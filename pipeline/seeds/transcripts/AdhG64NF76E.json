{"text": " Okay, so welcome back to Not welcome back to welcome to lesson 6 first time we've been in lesson 6. Welcome back to practical deep learning for coders We just started looking at tabular data Last time and for those of you who forgotten what we did was we We were looking at the Titanic data set and We were looking at creating binary splits by looking at Categorical variables or binary variables like sex and Continuous variables like the log of the fare that they paid and and using those You know what we also kind of came up with a score Which was basically how how good a job did that split to of grouping the Survival characteristics into two groups, you know All of nearly all of one of whom survived nearly all of whom the other didn't survive So they had like small standard deviation in each group and So then we created the world's simplest little UI to allow us to fiddle around and try to find a good binary split and we did We did come up with a very good binary split which was on Sex and actually we created this little Automated version and so this is I think the first time we can well not quite the first time is it no this is This is yet another time. I should say that we have successfully created a actual Machine learning algorithm from scratch. This one is about the world's simplest one. It's one are Creating the single rule which does a good job of splitting your data set into two parts Which differ as much as possible on the dependent variable One hour is probably not going to cut it for a lot of things though. It's surprisingly effective But it's so maybe we could go a step further And the other year step further we could go is we could create like a to our what if we took each of those groups males and females in the Titanic data set and Split each of those into two other groups. So split the males into two groups and split the females into two groups So To do that We can repeat the exact same piece of code we just did but let's remove Sex from it and then split the data set into males and females And run the same piece of code that we just did before but just for the males And so this is going to be like a 1r rule for how do we predict which males survive the Titanic and Let's have a look three eight three seven three eight three eight. Okay, so it's age Were they greater than or less than six? Turns out to be for the males the biggest predictor of whether they were going to survive That shipwreck and we can do the same thing females. So females There we go, no great supplies P class so whether they were in First class or not was the biggest predictor for females of whether they would survive the shipwreck So That has now given us a decision tree It is a series of A decision tree it is a series of binary splits which will gradually Split up our data more and more such that in the end These in the leaf nodes as we call them. We will hopefully get as you know much Stronger prediction as possible about survival So we could just repeat this step for each of the four groups we've now created males kids and older than six females first class and Everybody else and we could do it again and then we'd have eight groups We could do that manually with another couple of lines of code or we can just use Decision tree classifier, which is a class which does exactly that for us So there's no magic in here. It's just doing what we've just described And a decision tree classifier comes from a library called scikit-learn Scikit-learn is a fantastic library that focuses on kind of classical non deep learning ish machine learning methods like decision trees So we can so to create the exact same decision tree We can say please create a decision tree traffic Classifier with at most four leaf nodes And one very nice thing it has is it can draw the tree for us So here's a tiny little draw tree function And You can see here. It's going to first of all split on sex now It looks a bit weird to say sex is less than or equal to 0.5 But remember what our binary Characteristics are coded as zero one. So that's just how we you know easy way to Say males versus females And then here we've got for the females What class are they in and for the males what age are they and here's our four leaf nodes So for the females in first class Females in first class a 116 of them survived and four of them didn't so very good idea to be a well-to-do woman on the Titanic On the other hand males adults 68 survived 350 died so very bad idea to be a male adult on the Titanic So you can see you can kind of get a quick summary Of what's going on and one of the reasons people tend to like decision trees particularly for exploratory data analysis is it doesn't allow us to Get a quick picture of what are the key? driving variables in this data set and how much do they kind of predict what was happening in the data Okay, so it's around the same splits as us And it's got one additional piece of information we haven't seen before is this thing called Ginny Ginny is just another way of measuring how good a split is and I've Put the code to calculate Ginny here Here's how you can think of Ginny how likely is it that if you go into that sample and grab one item and Then go in again and grab another item. How likely is it that you're going to grab the same item each time? and so if that if the entire Leaf node is just people who survived or just people who didn't survive the probability would be one you get the same time same every time If it was an exactly equal mix the probability would be 0.5 so that's why we just Yeah, that's where this this formula comes from in the binary case And in fact, you can see it here right this group here is pretty much 50-50. So Ginny is 0.5 Or else this group here is nearly a hundred percent in one class. So Ginny is nearly Zero, so I had it backwards is one minus And I think I've written it backwards here as well so I've got to fix that So this decision tree is You know, we would expect it to be all accurate so we can calculate. It's been absolute error and for the 1r So just doing males versus females What was our score What was our score here we go 0.407 Actually, we have do we have an accuracy score somewhere here we are 0.336 That was for log-fair and for sex it was 0.215 Okay, so 0.215 so that was for the 1r version for the decision tree with four leaf nodes 0.224 so it's actually a little worse Right, and I think this just reflects the fact that this is such a small data set And the 1r Version was so good. We haven't really improved it that much I'm not enough to really see it amongst the randomness of such a small validation set We could go further To 50 a minimum of 50 Samples per leaf node. So that means that in each of these See how it says samples, which in this case is passengers on the Titanic. There's at least there's 67 people that were female first-class Less than 28 That's how you define that so this decision tree keeps building keeps splitting Until it gets to a point where there's going to be less than 50 at which point it stops splitting that that leaf so you can see They're all got at least 50 samples And so here's the decision tree that builds as you can see it doesn't have to be like constant depth, right? So this group here Which is males Who had cheaper fares? And who were older than 20 But younger than 32 Actually younger than 24 actually younger than 24 and actually Super cheap fares and so forth, right? So it keeps going down until we get to that group. So Let's try that decision tree. So that decision tree has an absolute error of point 183 So not surprisingly, you know, once we get there, it's starting to look like it's a little bit better So there's a model and This is a kaggle competition. So therefore we should submit it to the leaderboard and you know one of the you know biggest mistakes I see Not just beginners, but every level of practitioner make on kaggle is not to submit to the leaderboard Spend months making some perfect thing right, but you're actually going to see how you're going And you should try and submit something to the leaderboard every day So, you know regardless of how rubbish it is because You want to improve every day? So you want to keep iterating so to submit something to the leaderboard? You generally have to provide a csv file And so we're going to create a csv file And we're going to apply the category codes to get the the category for each one In our test set we're going to set the survived column to our predictions And then we're going to send that off to a csv And so yeah, so I submitted that And I got a score a little bit worse than most of our linear models and neural nets But not terrible. You know, it was uh, it's it's it's doing an okay job Now one interesting thing for the decision tree is there was a lot less pre-processing to do Did you notice that we didn't have to create any dummy variables for our categories? and Like you certainly can create dummy variables, but you often don't have to so for example You know for for for class You know, it's one two or three. You can just split on one two or three, you know, um, even for like What was that thing like the the embarkation? City code like we just convert them kind of arbitrarily to numbers one two and three and you can split on those numbers So with random forests, they're not random first not yet decision trees And so we're going to create a new tree So with random forests, they're not random first not yet decision trees. Um, yeah, you can generally get away with not doing stuff like dummy variables Uh, in fact even taking the log of fair We only did that to make our graph look better. But if you think about it splitting on log fair less than 2.7 It's exactly the same as splitting on fair is less than either the 2.7, you know Or whatever blog base we used. I can't remember so All that a decision tree cares about is the ordering of the data and this is another reason that decision tree based approaches are fantastic Because they don't care at all about outliers, you know long tail distributions Categorical variables, whatever you can throw it all in and it'll do a perfectly fine job so um for tabular data I would always start By using a decision tree based approach um And kind of create some baselines and so forth because it's it's really hard to mess it up And that's important So yeah, so here for example is embarked right it it was coded originally as the first letter of the city they embarked in But we turned it into a categorical variable and so pandas for us creates this this this vocab this list of all of the possible values and if you look at the codes Attribute you can see it's that s is the zero one two. So s has become two c Has become zero And so forth, right? So that's how we're converting the categories the strings The strings into numbers that we can sort and group by Um, so yeah So if we wanted to split c into one group and q and s and the other we can just do okay less than a recorder one point uh 0.5 Now of course if we wanted to split c and s into one group and q into the other we would need two binary splits first c On one side and q s q and s on the other and then q and s into q versus s And then the q and s leaf nodes could get similar Predictions so like you do have like sometimes it can take a little bit more messing around um, but Most of the time I find categorical variables work fine as numeric In decision tree based approaches and as I say here, I tend to use dummy variables only if there's like less than four levels Less than four levels Now what if we wanted to make this more accurate could we grow the tree further I mean we could but Um, you know, there's only 50 samples in these leaves right? It's it's not really um, You know if I keep splitting it the leaf nodes are going to have so little data that that's not really going to make Very useful predictions Now there are limitations to how accurate a decision tree can be um So what can we do? We can do something that's actually very I mean, I find it amazing and fascinating um Comes from a guy called leo bryman Uh and leo bryman, um came with this came up with this idea Called bagging and here's the basic idea of bagging Let's say we've got a model That's not very good Um, because let's say it's a decision tree. It's really small. We've hardly used any data for it Right. It's not very good. So it's got error. It's got errors on predictions Um, it's not a systematically biased error. It's not always predicting too high always predicting too low I mean decision trees, you know on average will predict the average right? Um, but it has errors Um, so what I could do is I could build another decision tree In some slightly different way That would have different splits And it would also be not a great model but Predicts the correct thing on average. It's not completely hopeless Um, and again, you know, some of the errors are a bit too high and some are a bit too low But I could keep doing this so if I could keep building lots and lots of slightly different decision trees Um, I'm going to end up with say a hundred different Models all of which are unbiased All of which are better than nothing and all of which have some errors a bit high some a bit low, whatever So what would happen if I averaged their predictions? Assuming that the models are not correlated with each other Then you're going to end up with errors On either side of the correct prediction some are a bit high. Some are a bit low There'll be this kind of distribution of errors Right and the average of those errors will be zero And so that means the average of the predictions of these multiple uncorrelated models each of which is unbiased will be The correct prediction because they have an error of zero and this is a mind-blowing insight It says that if we can generate a whole bunch of uncorrelated unbiased unbiased models We can average them and get something better than any of the individual models because the average of the error Will be zero So all we need Is a way to generate lots of models Well, we already have a great way to build models which is to create a decision tree How do we create lots of them? How do we create lots of unbiased but different models? Well Let's just grab a different subset of the data each time Let's just grab at random half the rows And build a decision tree and then grab another half the rows and build a decision tree And grab another half the rows and build a decision tree each of those decision trees is going to be not great It's only using half the data But it will be unbiased. It will be predicting the average on average. It will certainly be better than nothing because it's using You know some real data to try and create a real decision tree um The they won't be correlated with each other because they're each random subsets So that meets all of our criteria for bagging When you do this you create something called a random forest So let's create one In four lines of code so Here is a function to create a decision tree So let's say what this is just the proportion of data So let's say we put 75 of the data in each time or we could change it to 50 whatever So this is the number of samples in this subset i call it n and so let's at random choose Um n times the proportion we requested From the sample and build a decision tree from that And so now let's 100 times Get a tree and stick them all in a list using a list comprehension And now let's grab the predictions for each one of those trees And then let's stack all those predictions up together and take their mean And that is a random forest And what do we get one two, three, four, five, six, seven eight, that's seven lines of code so Random forests are very simple This is a slight simplification. There's one other difference that random forests do Which is when they build the decision tree They also randomly select a subset of the data that they want to build And they also randomly select a subset of columns And they select a different random subset of columns each time they do a split And so the idea is you kind of want it to be as random as possible, but also somewhat useful So we can do that by creating a random forest classifier Say how many trees do we want? How many samples per leaf? And then fit does what we just did And here's our mean absolute error rich Again, it's like you can see here. So we can do that by creating a random forest classifier Mean absolute error rich again It's like not as good as our decision tree, but it's still pretty good. And again, it's such a small data set It's hard to tell if that means anything And so we can submit that to kaggle. So earlier on I created a little function to submit to kaggle So now I just create some predictions and I submit to kaggle And yeah, looks like it gave nearly identical results to a single tree So now to one of my favorite things About random forests and I should say in most real world data sets of reasonable size random forests Basically always give you much better results than decision trees. This is just a small data set to Show you what to do One of my favorite things about Random forests is we can do something quite cool with them. What we can do is we can look at the underlying decision trees they create so we've now got 100 decision trees And we can see what columns Did it find a split on and so it's a here. Okay. Well the first thing it split on was six and it improved the gini from 0.47 To now just take the weighted average of 0.38 and 0.31 weighted by the samples So that's probably going to be about 0.33. So I would say okay. It's like 0.14 improvement in gini. Thanks to sex And we can do that again. Okay. Well within p-class, you know, how much did that improve gini? Again, we keep weighting it by the number of samples as well log fair. How much did that improve gini and we can keep track for each column of How much in gini? of how much in total did they improve the gini in this decision tree and then do that for every decision tree and then add them up per column and that gives you something called a feature importance plot and Here it is And a feature importance plot tells you how important is each feature How often did the trees picket and how much did it improve the gini when it did? And so we can see from the feature importance plot that sex was the most important and Class was the second most important and everything else was a long way back And this is another reason by the way why our random forest isn't really particularly helpful because It's just such a easy split to do right? Basically all that matters is You know what class you're in and whether you're male and female And these feature importance plots remember because they're built on random forests and Random forests don't care about really The distribution of your data and they can handle categorical variables and stuff like that That means that you can basically any tabular data set you have you can just plot this right away and Random forests, you know for most data sets only take a few seconds to train You know really at most of a minute or two And so if you've got a big data set and you know hundreds of columns do this first and find the 30 columns that might matter It's such a helpful Thing to do so i've done that for example. I did some work in credit scoring. So we're trying to find out which Things would predict who's going to default on a loan and I was given something like 7 000 columns from the database And I put it straight into a random forest and found I think there was about 30 columns that seemed Kind of interesting. I did that Like two hours after I started the job and I went to the Head of marketing and the head of risk and I told them here's the columns. I think that we should focus on And they were like Oh my god, we just finished a two-year consulting project with one of the big consultants Paid the millions of dollars And they came up with a subset of these There are a lot of things that you can do with um With random forests along this path. I'll touch on them briefly Um And specifically I'm going to look at chapter 8 of the book Which goes into this in a lot more detail and particularly interestingly chapter 8 of the book uses a Much bigger and more interesting Data set which is auction prices of heavy industrial equipment I mean, it's less interesting historically but more interestingly numerically And so some of the things I did there on this data set So this is from the data set this is from the scikit-learn documentation They looked at how as you increase the number of estimators So the number of estimates that you get from the data set As you increase the number of estimators, so the number of trees How much does the accuracy improve? so I then did the same thing on our data set so I actually just Added up to 40 more and more and more trees and you can see that basically as as predicted by that kind of an initial bit of Hand wavy theory I gave you that you would expect the more trees The lower the error because the more things you're averaging And that's exactly what we find the accuracy improves as we have more trees John what's up? Victor as possibly you might have just answered his question actually as he typed it But he's he's asking on the same theme the number of trees in a random forest Does increasing the number of trees always translate to a better error? Yes, it does always I mean tiny bumps, right? Tiny bumps, right? But yeah Once you smooth it out But um Decreasing returns and If you end up productionizing a random forest then of course every one of these trees you have to You know go through for at inference time So it's not that there's no cost. I mean having said that Zipping through a binary tree is the kind of thing you can really Do fast in fact, it's it's quite easy to like literally spit out C++ code With a bunch of if statements and compile it and get extremely fast performance I don't often use more than 100 trees. This is a rule of thumb This is a rule of thumb Is that the only one done? Okay So then there's another interesting feature of random forests, which is remember how in our example we trained with 75 of the data on each tree So that means for each tree there was 25 of the data we didn't train on Now this actually means if you don't have much data in some situations you can get away with not having a validation set and the reason why Is because for each tree we can pick the 25 percent of rows that weren't in that tree And see how accurate That tree was on those rows And we can average for each row Their accuracy on all of the trees in which they were not part of the training And that is called the out of bag error Or oob error and this is built in also to sklearn. You can ask for an oob prediction Um john Just before we move on Just before we move on um, zakiya has a question about bagging So we know that bagging is powerful as an ensemble approach to machine learning Would it be advisable to try out bagging then first when approaching a particular? Say tabular task Before deep learning so that's the first part of the question um And the second part is could we create a bagging model which includes fast ai? deep learning models Yes Absolutely. So to be clear, you know bagging is kind of like a meta method. It's not a prediction it's not a method of Modeling itself. It's just a method of combining other models um So random forests in particular as a particular approach to bagging Um is a you know, I would probably always start Personally a tabular project with a random forest because they're nearly impossible to mess up and they give good insight and they give a good base case But um, yeah your question then about can you bag? Other models is a very interesting one and the answer is you absolutely can and People very rarely do Um, but we will We will quite soon Uh, maybe even today Okay So I you know You might be getting the impression i'm a bit of a fan of random forests and before I was before you know people thought of me as The deep learning guy people thought of me as the random forests guy I used to go on about random forests all the time And one of the reasons i'm so enthused about them isn't just that they're very accurate or that they require You know that they're very hard to mess up and require a little processing pre-processing, but they give you a lot of uh, quick and easy insight Uh, and specifically these are the five things Which I think that we're interested in and all of which are things that random forests are good at they will tell us How confident are we in our predictions? On some particular row so when somebody you know when we're giving a loan to somebody We don't necessarily just want to know How likely are they to repay? But we'd also like to know How confident are we that we know? Because if we're if we're like well We think they'll repay But we're not confident of that. We would probably want to give them less of a loan And another thing that's very important is when we're then making a prediction so again, for example for for for credit Let's say you rejected that person's loan Why? And a random forest will tell us What what is the what is the reason that we made a prediction and you'll see why all these things? Which columns are the strongest predictors? You've already seen that one right? That's the feature importance plot Which columns are effectively redundant with each other i.e. They're basically highly correlated with each other Um And then one of the most important ones is you vary a column. How does it vary the predictions? So for example in your credit model How does your prediction of? Risk vary as you vary Well something that probably the regulator would want to know might be some you know, some protected variable like you know Race or some socio-demographic characteristics that you're not allowed to use in your model so they might check things like that For the first thing how confident are we in our predictions using a particular row of data? There's a really simple thing we can do which is Remember how when we calculated our predictions manually we stacked up the predictions together and took their mean Well, what if you took their standard deviation instead? So if you stack up your predictions and take their standard deviation And if that standard deviation is high That means all of them all of the trees are predicting something different And that suggests that we don't really know what we're doing And so that would happen if different subsets of the data end up giving completely different trees for this particular row So there's like a really simple thing you can do To get a sense of your prediction confidence Okay feature importance we've already discussed After I do feature importance, you know, like I said when I had the what 7 000 or so columns I got rid of like all but 30 That doesn't tend to improve the predictions of your random forest very much if at all, but it certainly helps like You know kind of logistically thinking about what you're doing You're all kind of logistically thinking about cleaning up the data You can focus on cleaning those 30 columns stuff like that. So I tend to remove the low importance variables I'm going to skip over this bit about removing redundant features because it's a little bit outside what we're talking about But definitely check it out in the book Something called a dendrogram But what I do want to mention is is the partial dependence this is the thing which says Um, what is the relationship? between a Column and the dependent variable And so this is something called a partial dependence plot. Now. This one's actually not specific to random forests Um a partial dependence plot is something you can do for basically any machine learning model Um, let's first of all look at one and then talk about how we make it So in this data set we're looking at the relationship we're looking at Uh the sale price at auction of heavy industrial equipment like bulldozers. This is specifically the blue books for bulldozers calc competition And a partial dependence plot between the year that the bulldozer or whatever was made And the price that was sold for this is actually the log price Is that it goes up? More recent bulldozers more recently made bulldozers are more expensive Um, and as you go back back to older and older build at bulldozers They're less and less expensive to a point and maybe these ones are some Old classic bulldozers you pay a bit extra for now You might think that you could easily create this plot by simply looking at your data at each year and taking the average sale price But that doesn't really work very well I mean it kind of does but it kind of doesn't let me give you an example It turns out that one of the biggest predictors of sale price for industrial equipment is whether it has air conditioning Um, and so air conditioning is you know, it's an expensive thing to add and it makes the equipment more expensive to buy And most things didn't have air conditioning back in the 60s and 70s and most of them do now So if you plot the relationship between year made and price You're actually going to be seeing a whole bunch of When you know how popular was air conditioning Right, so you get this this cross correlation going on that we just want to know No, what's what's just the impact of of the year? It was made all else being equal So There's actually a really easy way to do that which is we take our data set We take the we we leave it exactly as it is to just use the training data set But we take every single row and for the year made column. We set it to 1950 And so then we predict for every row. What would the sale price of that have been if it was made in 1950? And then we repeat it for 1951 and they repeat it for 1952 and so forth and then we plot the averages And that does exactly what I just said. Remember I said the special sale price was 1950 That's exactly what I just said. Remember I said the special words all else being equal This is setting everything else equal. It's the everything else is The data as it actually occurred And we're only varying year made And that's what a partial dependence plot is That works just as well for deep learning Or gradient boosting trees or logistic regressions or whatever It's a really cool thing you can do Um, and you can do more than one column at a time, you know, you can do two way partial dependence plots for example other one, um, okay, so then another one I mentioned was Can you describe why a particular prediction was made? So how did you decide for this particular row? to predict this particular value and um This is actually pretty easy to do there's a thing called tree interpreter But we could you could easily create this in about half a dozen lines of code All we do Is We're saying okay This customer's come in they've asked her alone We've put in all of their data through the random forest. It's about out of prediction We can actually have a look and say okay. Well that in tree number one What's the path that went down through the tree to get to the leaf node? And we can say oh well first of all it looked at sex and then it looked at postcode and then it looked at income and so we can see Exactly in tree number one which variables were used and what was the Change in jenny for each one And then we can do the same in tree two same in tree three time three four does this sound familiar? It's basically the same as our feature importance plot, right? But it's just for this one row of data And so that will tell you basically the feature importances for that one particular prediction And so then we can plot them Like this. So for example, this is an example of um an auction price prediction And according to this plot, you know, so we predicted that the net would be Uh, oh this is just a change from from uh, so I don't actually know what the price is but this is this is how much each one impacted the price so Year made I guess this must have been an older tractor. It caused a prediction of the price to go down But then it must have been a larger machine the product size caused it to go up Coupler system made it go up model id made it go up And so forth, right? So you can see the red Says that's made this made our prediction go down green made our prediction go up And so overall you can see Which things had the biggest impact on the prediction and what was the direction? For each one So it's basically a feature importance plot, but just for a single role for a single row Any questions john Yeah, there are a few questions Yeah, there are a couple that have that have sort of queued up this is a this is a good spot to um to jump to them, um So first of all andrew's asking uh jumping back to the um The oob era would you ever exclude a tree from a forest if had a if it had a bad out of bag era? Like if you if you had a I guess if you had a particularly bad Tree in your ensemble. Yeah, might you just Would you delete a tree that was not doing its thing? It's not playing its part. No, you wouldn't Um, if you start deleting trees then you are no longer Having a unbiased prediction of the dependent variable You are biasing it by making a choice. So even the bad ones Will be Improving the quality of the overall average All right. Thank you. Um, zakiya followed up with the question about bagging and we're just sort of going, you know layers and layers here Uh, you know, we could go on and create ensembles of bagged models Um, and you know, is it reasonable to assume that they would Continue so that's not going to make much difference right if they're all like You could take your 100 trees Split them into groups of 10 create 10 bagged ensembles and then average those but the average of an average is the same as the average Um You could like have a wider range of other kinds of models. You could have like neural nets trained on different subsets as well But again, it's just the average of an average will still give you the average Right. So there's not a lot of value in in kind of structuring the ensemble. You just I mean some some ensembles you can structure But but not bagging bagging is the simplest one. It's the one I mainly use There are more sophisticated approaches, but this one Is nice and easy All right, and there's there's one that um is a bit specific and it's referencing content. You haven't covered but we're here now. So Um, and it's on explainability uh, so feature importance of Random forest model sometimes has different results when you compare to other explainability techniques Um, like shap shap or lime Um, and we haven't covered these in the course, but um, amir is just curious about this If you've got any thoughts on which is more accurate or reliable Random forest feature importance or other techniques um I I would lean towards More immediately trusting random forest feature importances over other techniques on the whole On the basis that it's very hard to mess up a random forest um So Yeah, I feel like pretty confident that a random forest feature importance is going to Be pretty reasonable As long as this is the kind of data which a random forest is likely to be pretty good at You know doing you know, if it's like a computer vision model random forests aren't Particularly good at that and so one of the things that bryman talked about a lot was explainability He's got a great essay called the two cultures of statistics in which he talks about I guess what we're nowadays called kind of like data scientists and machine learning folks versus classic statisticians and um He he was you know, definitely a data scientist well before the Label existed and he pointed out. Yeah, you know first and foremost You need a model that's accurate. It needs to make good predictions A model that makes bad predictions will also be bad for making explanations because it doesn't actually know what's going on So if you know if you if you've got a deep learning model that's far more accurate than your random forest then it's You know explainability methods from the deep learning model will Probably be more useful because it's explaining a model. That's actually correct All right, let's take a um 10 minute break and we'll come back at five past seven Welcome back, um one person pointed out. I noticed I got the chapter wrong. It's chapter nine Not chapter eight in the book. I guess I can't read Somebody asked during the break about overfitting Um, can you overfit a random forest? Basically, no, not really Adding more trees will Make it more accurate It kind of asymptotes so you can't make it infinitely Accurate by using infinite trees, but it's certainly you know adding more trees won't make it worse If you don't have enough trees And you let the trees grow very deep that could overfit So you just have to make sure you have enough trees Radik told me about an experiment he did during that Radik told me during the break about an experiment he did But he didn't do it because he didn't know what was going on So he told me during the break about an experiment he did um, which is something i've done something similar, which is adding lots and lots of randomly generated columns to a data set and try to break the random forest and If you try it, it basically doesn't work. It's like it's really hard to confuse a random forest by giving it lots of Meaningless data it it does an amazingly good job of picking out The the useful stuff as I said, you know I had 30 useful columns out of 7 000 and it found them perfectly well And often, you know when you find those 30 columns You know you could go to you know I was doing consulting at the time go back to the client and say like tell me more about these columns And it's say like oh well that one there. We've actually got a better version of that now There's a new system, you know, we should grab that and oh this column Actually, that was because of this thing that happened last year, but we don't do it anymore Or you know, like you can really have this kind of discussion about the stuff you've zoomed into Um, you know, there are other things that you have to think about with lots of kinds of models like particularly regression models things like interactions You don't have to worry about that with random forests Like because you split on one column and then split on another column You get interactions for free as well Normalization you don't have to worry about you know, you don't have to have normally distributed columns So yeah, definitely worth a try now something I haven't gone into Um Is gradient boosting Um, but if you go to explain.ai You'll see that my friend Terrence and I have a three-part series about gradient boosting Including pictures of golf made by Terrence But to explain gradient boosting is a lot like random forests but rather than training a lot of training Fitting a tree again and again and again on different random subsets of the data Instead what we do is we fit very very very small trees to hardly ever any splits And we then say okay. Well, what's the error? So, you know, um, so imagine the simplest tree would be our one R rule tree of male versus female say And then you you take what's called the residual. That's the difference between the prediction and the actual The error and then you create another tree which attempts to predict that very small tree And then you create another very small tree which tries to predict the error from that And so forth each one is predicting the residual from all of the previous ones And so then to calculate a prediction Rather than taking the average of all the trees you take the sum of all the trees because each one has predicted the difference between the actual and All of the previous trees and that's called boosting Versus bagging so boosting and bagging are two kind of meta ensembling techniques And when bagging is applied to trees, it's called a random forest and when boosting is applied to trees It's called a gradient boosting machine or gradient booster decision tree Gradient boosting is generally speaking more accurate than random forests But you can absolutely overfit And so therefore It's not necessarily my first go-to thing Having said that there are ways to avoid overfitting But yeah, it's just it's it's not It's it you know because it's breakable it's not my first choice But yeah, check out our stuff here if you're interested and you know, you there is stuff which largely automates The process there's lots of hyper parameters. You have to select people generally just you know, try every combination of hyper parameters Um, and in the end you're generally should be able to get a more accurate gradient boosting model than random forest But not necessarily by much Okay, so that was the Kaggle notebook on random forests How random forests really work So What we've been doing is having this daily Walkthrough Where me and I don't know how many 20 or 30 folks get together on a zoom call and chat about you know getting through the course and setting up machines and Stuff like that. Um, and You know, we've been trying to kind of practice what you know things along the way And so a couple of weeks ago. I wanted to show like What does it look like to pick a Kaggle competition and just like Do the normal sensible Kind of mechanical steps that you're going to have to do Kind of mechanical steps that you would do for any computer vision model And so the Competition I picked was patty disease classification Which is about Recognizing diseases rice diseases and rice patties And yeah, I spent I don't know a couple of hours or three. I can't remember a few hours Throwing together something and um, I found that I was number one on the leaderboard And I thought oh, that's that's interesting like um, because you never quite have a sense of How well these things work And then I thought well, there's all these other things we should be doing as well and I tried Three more things and each time I tried another thing. I got further ahead at the top of the leaderboard So, um, I thought it'd be cool to take you through the process i'm going to do it reasonably quickly because um The walkthroughs are all available For you to see the entire thing in you know, seven hours of detail or however long we probably were six to seven hours of conversations Um, but I want to kind of take you through the basic process That I went through So since i've been starting to do more stuff on kaggle, um, you know, I realized there's some Kind of menial steps I have to do each time particularly because I like to run stuff on my own machine And then kind of upload it to kaggle So to do to make my life easier I created a little Module called fast kaggle, which you'll see in my notebooks from now on which you can download from pit or conda And as you'll see it makes some things a bit easier for example downloading the data for the patty disease classification If you just run setup comp And pass in the name of the competition If you are on kaggle It will return a path to On kaggle, it will return a path to that Competition data that's already on kaggle If you are not on kaggle and you haven't downloaded it it will download and unzip the data for you If you're not on kaggle and you have downloaded unzip the data, it will return a path to the one that you've already downloaded also, if you are on kaggle, you can ask it to make sure that Pip things are installed that might not be up to date. Otherwise Um, so this basically one line of code now gets us all set up and ready to go So this path So I ran this particular one on my own machine so it's downloaded and unzipped the data I've also got links to the Six walkthroughs so far into the videos Yes, and here's my result after these Four attempts that's a few fiddling around at the start So the overall approach at is well and this is not just to a kaggle competition right the reason I like looking at kaggle competitions is You can't hide from the truth In a kaggle competition, you know when you're working on some work project or something You might be able to convince yourself and everybody around you that you've done a fantastic job of not overfitting and your model's better than what anybody else could have made or whatever else but The brutal assessment of the private leaderboard Will tell you the truth Is your model actually predicting things correctly and is it overfit? Until you've been through that process You know, you're never going to know and a lot of people don't go through that process because at some level they don't want to know But it's okay, you know, nobody need you don't have to put your own name there I Always did right from the very first one. I wanted you know, if I was going to screw up really I wanted to have the pressure on myself of people seeing me in last place But you know, it's it's fine. You could do it all and honestly and you'll actually find As you improve you'll have so much self-confidence, you know and The stuff we're doing a Kaggle competition is indeed a subset of the things we need to do in real life but It's an important subset, you know Building a model that actually predicts things correctly and doesn't overfit is important and furthermore Structuring your code and analysis in such a way that you can keep improving over a three-month period without gradually getting into more and more of a tangled mess of impossible to understand code and Having no idea What untitled copy 13 was and why it was better than 25? Right. This is all Stuff you want to be practicing ideally Well away from the Customers or whatever, you know before you've kind of figured things out So the things I talk about here about doing things well in this Kaggle competition Should work, you know in other settings as well And so these are the two focuses that I recommend Get a really good validation set together. We've talked about that before right? And in a Kaggle competition, that's like it's very rare to see people do well in a Kaggle competition who don't have a good validation set sometimes that's easy in this competition actually it is easy because the The the test set seems to be a random sample But most of the time it's not actually I would say And then how quickly can you iterate? How quickly can you try things? How quickly can you iterate how quickly can you try things and find out what worked? So obviously you need a good validation set. Otherwise, it's impossible to iterate And so quickly iterating means not saying what is the biggest? You know open AI takes four months on 100 tpus model that I can train It's what can I do that's going to train in a minute or so? And will quickly give me a sense of like well I could try this I could try that what things going to work and then try You know 80 things It also doesn't mean that saying like oh, I heard this is amazing you Bayesian hyper parameter tuning approach. I'm going to spend three months implementing that Because that's going to like give you one thing But actually do well in in these competitions Or in machine learning in general you actually have to do everything reasonably well And doing just one thing really well Will still put you somewhere about last place I actually saw that a couple of years ago ozzy guy who's Very very distinguished machine learning practitioner Actually put together a team entered a caggle competition and literally came in last place Because they spent the entire three months trying to build this amazing new fancy thing and Never actually never actually iterated if you iterate I guarantee you won't be in last place Okay, so Here's how we can grab our data with fast caggle and it gives us tells us what path it's in And then I set my random seed And I only do this because i'm creating a notebook to share You know when I share a notebook, I like to be able to say as you can see this is 0.83 blah blah blah, right? And know that when you see it'll be 0.83 as well But when i'm doing stuff, otherwise, I would never set a random seed I want to be able to run things multiple times and see how much it's going to run So I'm going to set a random seed I want to be able to run things multiple times and see how much it changes each time because that'll give me a sense of like The modifications i'm making changing it because they're improving it making it worse or is it just random variation? So if you if you always set a random seed That's a bad idea because you won't be able to see the random variation. So this is just here for presenting a notebook Okay, so the data They've given us as usual. They've got a sample submission. They've got some test set images They've got some training set images a csv file about the training set Um, and then these other two you can ignore because I created them So let's grab a path To train images and so do you remember? Get image files so that gets us a list of the file names of all the images here recursively So we could just grab the first one And take a look so it's 480 by 640 Now we've got to be careful. Um, this is a pillow image python imaging library image Um in the imaging world, they generally say columns by rows In the array slash tensor world, we always say rows by columns So if you ask pytorch what the size of this is it'll say 640 by 480 And I guarantee at some point this is going to bite you so try to recognize it now Okay, so they're kind of taller than they are at least this one is taller than it is wide Um So I would actually actually know are they all this size because it's really helpful if they are all the same size or at least similar Um Believe it or not the amount of time it takes to decode a jpeg is actually quite significant Um, and so figuring out what size these things are is actually going to be pretty slow Um But my fast core library has a parallel sub module which can basically do anything that you can do in python It can do it in parallel So in this case we wanted to create a pillow image and get its size So if we create a function that does that and pass it to parallel passing in the function and the list of files It does it in parallel and that actually runs pretty fast And so here is the answer I don't know how this happened 10,403 images are indeed 480 by 640 and four of them aren't So basically what this says to me is that we should pre-process them or you know at some point process them So that they're probably all 480 by 640 or all basically the kind of same size. We'll pretend they're all this size um, but we can't Not do some initial resizing. Otherwise, this is going to screw up Um So like the probably the easiest way to do things the most common way to do things Is to either squish or crop every image to be a square So squishing is when you just In this case squish the aspect ratio down As opposed to cropping the aspect ratio down So we're going to do this in a little bit of a more complex way Um as opposed to cropping randomly a section out. So if we call resize squish it will squish it down Um, and so this is 480 by 480 squared. So this is what it's going to do to all of the images first on the cpu That allows them to be all batched together into a single mini batch. Everything in a mini batch has to be the same shape Otherwise the gpu won't like it And then that mini batch is put through data augmentation And it will grab a random subset of the image and make it a 128 by 128 pixel And here's what that looks like. Here's our data So show batch works for pretty much everything not just in the fast ai library But even for things like fast audio which are kind of community based things You should be able to use show batch on anything and and see or hear or whatever what your data looks like I don't know anything about rice disease, but apparently these are various rice diseases and this is what they look like So, um, I I jump into creating models much more quickly than most people Um, because I find model, you know models are a great way to understand my data as we've seen before So I basically build a model as soon as I can um and I want to Create a model That's going to let me iterate quickly So that means that i'm going to need a model that can train quickly so Thomas capel and I recently Um did this big project for best vision models for fine tuning Where we looked at nearly a hundred different architectures from from ross whiteman's tim library Py torch image model library and looked at Which ones could we fine tune which ones had the best transfer learning results And we tried two different data sets very different data sets. Um, one is the pets data set that we've seen before So trying to predict what breed of pet is from 37 different breeds and the other was a satellite imagery data set called planet So very very different data sets in terms of what they contain and also very different sizes The planet one's a lot smaller the pets one's a lot bigger Um, and so the main things we measured were how much memory did it use? How accurate was it and how long did it take to fit? And then I created this score which can which combines the fit time and error rate together And so this is a really useful table for picking a model And now in this case, I want to pick something That's really fast and there's one clear winner on speed, which is resnet 26 d And so its accuracy was six percent versus the best was like 4.1 percent So, okay, it's not amazingly accurate, but it's still pretty good and it's going to be really fast So that's why I picked resnet 26 d A lot of people think that When they do deep learning they're going to spend all of their time learning about exactly how a resnet 26 d is made and Convolutions and resnet blocks and transformers and blah blah blah. We will cover all that stuff In part two and a little bit of it next week But it almost never matters Right It's just it's just a function right and what matters is the inputs to it And the outputs to it and how fast it is and how accurate it is So let's create a learner which with a resnet 26 d from our data loaders and Let's run lr find so lr find Will put through one mini batch at a time Starting at a very very very low learning rate and gradually increase the learning rate and track the loss and Initially the learn the loss won't improve because the learning rate is so small It doesn't really do anything and at some point the learning rate is high enough that the loss will start coming down Then at some other point the load the learning rate is so high that it's going to start jumping past the answer and it's got to get worse And so somewhere around here is a learning rate where you're going to start to see Is a learning rate we'd want to pick We've got a couple of different ways of making suggestions I generally ignore them because these suggestions are specifically designed to be conservative They're a bit lower than perhaps an optimal in order to make sure we don't recommend something that totally screws up But I kind of like to say like well, how far right can I go and still see it like clearly? Really improving quickly and so i'd pick somewhere around 0.01 for this so I can now Fine tune our model with a learning rate of 0.01 Three epochs. So look the whole thing took a minute. That's what we want. Right? We want to be able to iterate Rapidly just a minute or so. So that's enough time for me to go and you know grab a glass of water or Do some reading like it's not going to get too distracted And What do we do before we submit Nothing we submit as soon as we can Okay, let's get our submission in so we've got a model Let's get it in So we read in our csv file of the sample submission And so the csv file basically looks like we're going to have to have a list of the image file names In order and then a column of labels So we can get all the image files in the test image Like so and we can sort them And so now we want is what we want is a data loader which Is exactly like the data loader we use to train the model Except pointing at the test set we want to use exactly the same transformations So there's actually a dl's dot test dl method which does that you just pass in The new set of items so the test set files So this is a data loader which we can use for our test set A test data loader has a key difference to a normal data loader, which is that it does not have any labels So that's a key distinction So we can get the predictions for our learner passing in that data loader And in the case of a classification problem, you can also ask for them to be decoded decoded means rather than just get returned the probability of every Rice disease where every plus it'll tell you what is the index of the most probable Rice disease, that's what decoded means so that'll return with probabilities, uh targets which obviously will be empty because it's a test set so throw them away and those decoded indexes Which look like this numbers from 0 to 9 because there's 10 possible rice diseases The caggle submission does not expect numbers from 0 to 9 it expects to see Strings like these So what do those numbers from 0 to 9 represent? We can look up our vocab to get a list So that's zero. That's one etc. That's nine So, um I realized later this is a slightly inefficient way to do it, but it does the job I need to be able to map these to strings so If I enumerate the vocab that gives me pairs of numbers zero bacterial leaf blight One bacterial leaf streak, etc. I could then create a dictionary out of that And then I can use pandas To look up each thing in a dictionary. They call that map If you're a pandas user, you've probably seen map used before being passed a function Which is really really slow But if you pass map addict, it's actually really really fast. So do it this way if you can So here's our predictions So we've got our Sample submission file ss. So if we replace this column label with our predictions like so Then we can turn that into a csv And remember this means Uh, this means run a bash command a shell command head is the first few rows. Let's just take a look That looks reasonable So we can now submit that to kaggle now Iterating rapidly means everything needs to be fast and easy Things that are slow and hard don't just take up your time But they're going to be a lot of work Things that are slow and hard don't just take up your time, but they take up your mental energy So even submitting to kaggle needs needs to be fast. So I put it into a cell So I can just run this cell Api.competition submit This csv file Give it a description So just run the cell and it submits to kaggle And as you can see it says here we go successfully submitted So that submission was terrible Top 80% also known as bottom 20% Which is not too surprising, right? I mean, it's it's one minute of training time But it's something that we can start with and that would be like However long it takes to get to this point that you put in our submission Now you've really started right because then tomorrow You can try to Make a slightly better one So I like to share my notebooks and so even sharing the notebook i've automated So part of fast kaggle is you can use this thing called push notebook And that sends it off to kaggle to create A notebook on kaggle There it is and there's my score As you can see it's exactly the same thing Why would you create public notebooks on kaggle well It's the same Brutal It's the same brutality of feedback That you get for entering a competition But this time rather than finding out In no uncertain terms whether you can predict things accurately This time you can find out no uncertain terms whether you can communicate things in a way that people find interesting and useful And if you get zero votes You know so be it right that's something to to know and then you know, ideally go and ask some friends like What do you think I could do to improve and if they say oh nothing it's fantastic you can tell no that's not true I didn't get any votes try again. This isn't good. How do I make it better? You know And you can try and improve because If you can create models that predict things well And you can communicate your results in a way that is clear and compelling You're a pretty good data scientist, you know, like they're they're two pretty important things and so here's a great way to Test yourself out on those things and improve. Yes, john Uh, yes, charme. We have a sort of a I think a timely question here from zakiya, um about your iterative approach And they're asking do you create different kaggle notebooks for each model that you create? For each model that you try. Yeah, so one kaggle book for the first one then separate notebooks subsequently Or do you do append to the bottom of a single notebook? What's your strategy? That's a great question and I know zakiya is going through the The daily walkthroughs but isn't quite caught up yet. So um, I will say keep keep it up because um In the six hours of going through this you'll see me create all the notebooks um but uh If I go to the actual directory I used Uh, you can see them so basically yeah, I started with You know what you just saw A bit messier without the pros but that same basic thing. I then duplicated it To create the next one Which is here and because I duplicated it, you know this stuff which I still need it's still there, right? And so I I run it and um I don't always know what i'm doing You know, and so at first if I don't really know what i'm doing next when I duplicate it It will be called, you know first steps on the road to the top part one dash copy one You know and that's okay And as soon as I can I'll try to rename that Once I know what i'm doing, you know um or if it doesn't say to go anywhere I rename it into something like you know Experiment blah blah blah and I'll put some notes at the bottom and I might put it into a failed folder or something But yeah, it's like It's a very low tech approach That I find works really well, which is just Duplicating notebooks and editing them and naming them carefully and putting them in order um And you know put the file name in when you submit as well um And then of course also if you've got things in git, you know You can have a link to the git commit so you'll know exactly what it is Generally speaking for me, you know, my notebooks will only have one submission in And then i'll move on and create a new notebook. So I don't really worry about versioning so much um But you can do that as well if that helps you Um, yeah, so that's basically what I do and and and I've worked with a lot of people who use much more sophisticated and complex processes and tools and stuff, but None of them seem to be able to stay as well organized as I am I think they kind of get a bit lost in their tools sometimes um and File systems and file names I think are Are good Um, great. Thanks. Um, so away from that kind of dev process more towards the The specifics of you know, finding the best model and all that sort of stuff we've got a couple of questions that are in the same space, which is You know, we've got some people here talking about auto ml frameworks, which you might want to you know touch on for people who haven't heard of those um, if you've got any particular auto ml frameworks you think are worth Recommending or just more generally, how do you go trying different models random forest gradient boosting neural network? So in that space if you couldn't comment sure um I use auto ml less than anybody I know I would guess um Which is to say never um Hyper parameter optimization never um And the reason why Is I like being highly intentional, you know, I like to think more like a scientist And have hypotheses And test them carefully And come up with conclusions which then I implement, you know, so for example um in this best vision models of fine tuning I didn't try a huge grid search of every possible Model every possible learning rate every possible pre-processing approach blah blah blah, right instead step one Was to find out Well, which things matter, right? so um For example does whether we squish or crop Make a difference, you know are some models better with squished and some models better with crop And so we just tested that for Again, not for every possible architecture But for one or two versions of each of the main families that took 20 minutes and the answer was no in every single case The same thing was better. So we don't need to do a grid search over that anymore, you know Or another classic one is like learning rates. Most people Do a kind of grid search over learning rates or they'll train a thousand models, you know with different learning rates but um This fantastic researcher named. Leslie smith invented the learning rate finder a few years ago We implemented it. I think within days of it first Coming out as a technical report and that's what i've used ever since Because it works Well and runs in a minute or so um Yeah, I mean and then like neural nets versus gbms versus random forests, I mean that's That shouldn't be too much of a question on the whole like they have pretty clear Places that they go um like If i'm doing computer vision, i'm obviously going to use a computer vision deep learning model Um, and which one I would use well if i'm transfer learning which hopefully is always I would look at The two tables here. This is my table for pets, which is which are the best at fine tuning to very similar things to what they're pre-trained on And then the same thing for planet Is which ones are best for fine tuning for two data sets that are very different to what they're trained on And as it happens in both case, they're very similar In particular convex is right up towards the top in both cases So I just like to have these rules of thumb and um Yeah, my rule of thumb for tabular is Random forests is going to be the fastest easiest way to get a pretty good result gbms Probably going to give me a slightly better result if I need it and can be bothered fussing around um Gbm I would probably yeah, actually I probably would run a hyper parameter sweep Because it is fiddly and and it's fast so you may as well Um, so yeah, so now so, you know, we were now going to make a slightly better submission a slightly better model um and so I had a couple of thoughts about this. The first thing was that thing trained in a minute On my home computer and I was like, okay, I'm going to run a hyper parameter sweep That thing trained in a minute on my home computer And then when I uploaded it to kaggle, it took about four minutes per epoch Which was horrifying and um kaggle's gpus are not amazing, but they're not that bad Um, so I knew something was up And what was up is I realized that they only have two Two virtual cpus which nowadays is tiny like, you know, you generally want as a rule of thumb about eight physical cpus per gpu Um And so spending all of its time just reading the damn data Now the data was 640 by 480 and we were ending up with any 128 pixel size bits for speed So there's no point doing that every epoch So step one was to make my kaggle iteration faster as well And so very simple thing to do Resize the images So fastai has a function called resize images And you say okay take all the train images and stick them in the destination Making them this size recursively And it will recreate the same folder structure over here And so that's why I called this the training path Because this is now my training data And so when I then trained on that on kaggle It went down to uh four times faster Um with no loss of accuracy so that was kind of step one was to actually get my fast iteration working um um now still I bet it's a long time And on kaggle you can actually see this little graph showing how much the cpu is being used how much the gpu is being used on Your own home machine you can um, there are tools free gp, you know free tools to do the same thing I saw that the gpu was still hardly being used So still cpu was being driven pretty hard Um, I wanted to use a better model anyway to move up the leaderboard so I moved from a um Oh, by the way, this graph is very useful so this is um This is speed versus error rate by family and so we're about to be looking at these um Convnext models Um, so we're going to be looking at this one Convnext tiny um Here it is Convnext tiny so we were looking at resnet 2060 which took this long on this data set But this one here is nearly the best. It's third best But it's still very fast And so it's the best overall score. So let's use this This particularly because you know, we're still spending all of our time waiting for the cpu anyway So it turned out that when I switched my architecture to Convnext It basically ran just as fast on kaggle So we can then Train that Let me switch to the kaggle version because my outputs are missing for some reason So, um Yeah, so I started out by running the resnet 2060 on the resized images and got Similar error rate, but I ran a few more epochs Got 12 error rate And so then I do exactly the same thing but with Convnext small And 4.5 error rate, so I don't think that different architectures are just Tiny little differences. This is over twice as good um and um A lot of folks you talk to will never have heard of this Convnext because it's very new and i've noticed a lot of people tend not to Keep up to date with new things. They kind of learn something at university and then they stop stop learning. So if somebody's still Just using resnets all the time You know, you can tell them we've we've actually we've moved on you know, um Resnets are still probably the fastest but For the mix of speed and performance you know Not so much Um Convnext, you know again you want these rules of thumb, right? If you're not sure what to do This Convnext, okay, and then like most things there's different sizes. There's a tiny There's a small there's a base. There's a large there's an extra large and you know, it's just uh, well, let's talk about the picture This is it here Right Large takes longer but lower error Tiny takes less time but higher error, right? So you you pick About your speed versus accuracy trade-off for you. So for us small is great And so yeah now we've got a 4.5 percent error that's that's terrific Um Now let's iterate Uh on kaggle. This is taking about a minute per epoch on my computer. It's probably taking about 20 seconds per epoch So not too bad um so You know one thing we could try Is instead of using squish As our pre-processing let's try using crop so that will randomly crop out an area Uh, and that's the default. So if I remove the method equals squish that will crop So you see how i've tried to get everything into a single Function, right this single function I can tell it let's go and find the definition What architecture do I want to train? How do I want to transform the items? How do I want to transform the batches and how many epochs do I want to do? That's basically it, right? Um, so this time I want to use the same architecture conf next I want to resize without cropping and then use the same data augmentation And okay error rates about the same So not particularly it's a tiny bit worse, but not enough to be interesting um Instead of cropping we can pad now padding is interesting. Do you see how these are all square? Right, but they've got black borders so Padding is interesting because it's the only way of pre-processing images which doesn't distort them and doesn't lose anything If you crop you lose things if you squish you distort things This does neither Now, of course the downside is that there's pixels that are literally pointless. They contain zeros So every way of getting this working It has its compromises, but this approach of resizing where we pad with zeros Is not used enough and it can actually often work quite well And in this case, it was about as good as our Best so far Um, but no not huge differences yet Um, what else could we do? uh well What we could do is um Through these pictures this is all the same picture But uh, it's gone through our data augmentation. So sometimes it's a bit darker Sometimes it's flipped horizontally. Sometimes it's slightly rotated. Sometimes it's slightly warped. Sometimes it's zooming into a slightly different section But this is all the same picture Maybe our model would like some of these versions better than others So what we can do is we can pass all of these to our model get predictions for all of them and Take the average Right. So it's our own kind of like little mini bagging approach and this is called test time augmentation Fast ai is very unusual in making that available in a single method You just pass tta and it will pass multiple augmented versions of the model tta and it will pass multiple augmented versions of the image and um and Average them for you And so This is the same model as before which had a 4.5 percent. So if instead if we get uh tta predictions Uh, and then get the error rate Um, wait, why does this say 4.8 last time I did this it was way better. Well, that's messing things up, isn't it? Uh So when I did this originally on my home computer it went from like 4.5 to 3.9. So possibly I Got a very bad luck this time. So this is the first time i've actually ever seen tta give it worse result. Um, So that's very weird I wonder if it's If I should do something other than the crop padding, all right I'll have to check that out and i'll try and come back to you and find out Why in this case? This one was worse um Anyway, take my word for it every other time i've tried it tta has been better um So then you know now that we've got a pretty good way of um resizing Um, we've got tta. We've got a good training process Let's just make bigger images And something that's really interesting and a lot of people don't realize is your images don't have to be square They just all have to be the same size And given that nearly all of our images are 640 by 480 We can just pick you know that aspect ratio So for example 256 by 192 and we'll resize everything To the same aspect ratio rectangular And that should work even better still so if we do that we do 12 epochs Okay, now our error rate's down to 2.2 percent And then we'll do tta Okay, this time you can see it actually improving down to under 2 percent So that's pretty cool, right? We've got our error rate at the start of this notebook. We were at 12 percent and by the time we've got through our little experiments We're down to under 2 percent And nothing about this is in any way specific to rice or this competition, you know, it's like this is a very Mechanistic, you know standardized approach which you can use for Certainly any kind of this type of computer vision competition and computer vision data set almost But you know, it looked very similar for a collaborative filtering model a tabular model nlp model whatever So, of course again, I want to submit as soon as I can so Just copy and paste the exact same steps. I took last time basically for creating a submission So as I said last time we did it using pandas, but there's actually an easier way So the step where here i've got the numbers from 0 to 9 Which is like which which rice disease is it? So here's a cute idea We can take our vocab And make it an array. So that's going to be a list of 10 things And then we can index into that vocab with our indices Which is kind of weird. This is a list of 10 things This is a list of other four or five thousand things So this will give me four or five thousand things So this will give me four or five thousand results, which is Each vocab item for that thing So this is another way of doing the same mapping and I would spend time playing with this code to understand what it does because it's the kind of like Very fast what you know, not just in terms of writing but this this the this would uh Optimize, you know on on the cpu Very very well. Um, so this is the kind of coding you want to get used to This kind of indexing Anyway, so then we can submit it just like last time and when I did that I got in the top 25 percent And that's that's where you want to be right? Like generally speaking. I find in cacl competitions the top 25 percent is like You're kind of like solid competitive level, you know, look just not to say like it's not easy You've got to know what you're doing But if you get in the top 25 percent, I think you can really feel like yeah, this is this is a you know very reasonable Attempt and so that's I think this is a very reasonable attempt Okay before we wrap up john any last questions Um, yeah, there's this there's two I think that would be good if we could touch on quickly before we wrap up Um one from victor asking about tta Um When I use tta during my training process do I need to do something special during inference or is this something you use only during Okay, so just explain Tta means test time augmentation. So specifically it means that you have to do something special during inference So yeah, so during training you basically always do augmentation, which means you're varying each image slightly So that the Model never seems the same image exactly the same twice and so it can't memorize it On fast ai and as I say, I don't think anybody else does this as far as I know if you call tta You can't memorize it The exact same augmentation approach on whatever data set you pass it And average out the prediction but but like multiple times on the same image and will average them out So you don't have to do anything different But if you didn't have any data augmentation in training, you can't use tta. It uses the same By default the same data augmentation you use for training Great. Thank you And then the last thing is that you can use the same data augmentation in training Great. Thank you. Um, and the other one is about how You know when you first started this example you squared the models and the images rather and you talked about squashing versus cropping versus, you know clipping and Scaling and so on but then you went on to say that These models can actually take rectangular inputs, right? So there's a question that's kind of probing it at that, you know If the if the models can take rectangular inputs, why would you ever? Even care as long as they're all the same size. So I find most of the time Data sets tend to have a wide variety of input sizes and aspect ratios so You know if there's just as many tall skinny ones as wide short ones You know, you doesn't make sense to create a rectangle because some of them you're going to really destroy them So a square is the kind of best compromise in some ways There are better things we can do Which We don't have any Off the shelf library support for yet and I don't think I don't know that anybody else has even published about this But we've experimented with kind of trying to Batch things that are similar aspect ratios together and use The kind of median rectangle for those and have had some good results with that. But honestly 99.999 percent of people given a wide variety of aspect ratios chuck everything into a square A follow-up just this is my own interest. Have you ever looked at You know, so the issue with um with padding as you say is that you're putting you know, big Black pixels there. Those are not nands. Those are black pixels. That's right. There's here and so there's there's something problematic To me, you know conceptually about that You know when you when you see uh, for example Four to three aspect ratio footage Presented for broadcast on 16 to 9 you get the kind of the blurred stretch that kind of stuff now We've played with that a lot. Yeah, I used to be really into it Actually and fastai still by default uses reflection padding Which means if this is I don't know that say this is a 20 pixel wide thing it takes the 20 pixels next to it and flips it over and sticks it here and It looks pretty good. You know, another one is copy which simply takes the outside pixel and it's a bit more like tv You know much to my chagrin It turns out none of them really help uh possibly, you know if anything they make it worse Because in the end The computer wants to know no, this is the end of the image. There's nothing else here. And if you reflect it, for example Then you're kind of creating weird spikes that didn't exist and the computer's got to be like, oh, I wonder what that spike is So yeah, it's a great question And I obviously spent like a couple of years assuming that We should be doing things that look more image like but actually The computer likes things to be presented to it in as straightforward a way as possible All right. Thanks everybody and uh, hope to see some of you in the walkthroughs and otherwise. See you next time", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 2.0, "text": " Okay, so welcome back to", "tokens": [1033, 11, 370, 2928, 646, 281], "temperature": 0.0, "avg_logprob": -0.2723050117492676, "compression_ratio": 1.5776397515527951, "no_speech_prob": 0.019679900258779526}, {"id": 1, "seek": 0, "start": 3.0, "end": 10.22, "text": " Not welcome back to welcome to lesson 6 first time we've been in lesson 6. Welcome back to practical deep learning for coders", "tokens": [1726, 2928, 646, 281, 2928, 281, 6898, 1386, 700, 565, 321, 600, 668, 294, 6898, 1386, 13, 4027, 646, 281, 8496, 2452, 2539, 337, 17656, 433], "temperature": 0.0, "avg_logprob": -0.2723050117492676, "compression_ratio": 1.5776397515527951, "no_speech_prob": 0.019679900258779526}, {"id": 2, "seek": 0, "start": 12.620000000000001, "end": 17.5, "text": " We just started looking at tabular data", "tokens": [492, 445, 1409, 1237, 412, 4421, 1040, 1412], "temperature": 0.0, "avg_logprob": -0.2723050117492676, "compression_ratio": 1.5776397515527951, "no_speech_prob": 0.019679900258779526}, {"id": 3, "seek": 0, "start": 19.52, "end": 26.82, "text": " Last time and for those of you who forgotten what we did was we", "tokens": [5264, 565, 293, 337, 729, 295, 291, 567, 11832, 437, 321, 630, 390, 321], "temperature": 0.0, "avg_logprob": -0.2723050117492676, "compression_ratio": 1.5776397515527951, "no_speech_prob": 0.019679900258779526}, {"id": 4, "seek": 2682, "start": 26.82, "end": 31.82, "text": " We were looking at the Titanic data set and", "tokens": [492, 645, 1237, 412, 264, 42183, 1412, 992, 293], "temperature": 0.0, "avg_logprob": -0.6373730040433114, "compression_ratio": 1.6846153846153846, "no_speech_prob": 7.023382931947708e-05}, {"id": 5, "seek": 2682, "start": 32.5, "end": 35.82, "text": " We were looking at creating binary splits", "tokens": [492, 645, 1237, 412, 4084, 17434, 37741], "temperature": 0.0, "avg_logprob": -0.6373730040433114, "compression_ratio": 1.6846153846153846, "no_speech_prob": 7.023382931947708e-05}, {"id": 6, "seek": 2682, "start": 36.42, "end": 38.42, "text": " by looking at", "tokens": [538, 1237, 412], "temperature": 0.0, "avg_logprob": -0.6373730040433114, "compression_ratio": 1.6846153846153846, "no_speech_prob": 7.023382931947708e-05}, {"id": 7, "seek": 2682, "start": 39.620000000000005, "end": 42.620000000000005, "text": " Categorical variables or binary variables like sex", "tokens": [383, 2968, 284, 804, 9102, 420, 17434, 9102, 411, 3260], "temperature": 0.0, "avg_logprob": -0.6373730040433114, "compression_ratio": 1.6846153846153846, "no_speech_prob": 7.023382931947708e-05}, {"id": 8, "seek": 2682, "start": 44.42, "end": 46.42, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.6373730040433114, "compression_ratio": 1.6846153846153846, "no_speech_prob": 7.023382931947708e-05}, {"id": 9, "seek": 2682, "start": 47.22, "end": 53.019999999999996, "text": " Continuous variables like the log of the fare that they paid and", "tokens": [14674, 12549, 9102, 411, 264, 3565, 295, 264, 11994, 300, 436, 4835, 293], "temperature": 0.0, "avg_logprob": -0.6373730040433114, "compression_ratio": 1.6846153846153846, "no_speech_prob": 7.023382931947708e-05}, {"id": 10, "seek": 5302, "start": 53.02, "end": 55.86, "text": " and using those", "tokens": [293, 1228, 729], "temperature": 0.0, "avg_logprob": -0.27445264089675175, "compression_ratio": 1.6076555023923444, "no_speech_prob": 4.607571827364154e-05}, {"id": 11, "seek": 5302, "start": 56.660000000000004, "end": 59.14, "text": " You know what we also kind of came up with a score", "tokens": [509, 458, 437, 321, 611, 733, 295, 1361, 493, 365, 257, 6175], "temperature": 0.0, "avg_logprob": -0.27445264089675175, "compression_ratio": 1.6076555023923444, "no_speech_prob": 4.607571827364154e-05}, {"id": 12, "seek": 5302, "start": 60.660000000000004, "end": 66.9, "text": " Which was basically how how good a job did that split to of grouping the", "tokens": [3013, 390, 1936, 577, 577, 665, 257, 1691, 630, 300, 7472, 281, 295, 40149, 264], "temperature": 0.0, "avg_logprob": -0.27445264089675175, "compression_ratio": 1.6076555023923444, "no_speech_prob": 4.607571827364154e-05}, {"id": 13, "seek": 5302, "start": 68.26, "end": 70.86, "text": " Survival characteristics into two groups, you know", "tokens": [40716, 3576, 10891, 666, 732, 3935, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.27445264089675175, "compression_ratio": 1.6076555023923444, "no_speech_prob": 4.607571827364154e-05}, {"id": 14, "seek": 5302, "start": 70.86, "end": 74.66, "text": " All of nearly all of one of whom survived nearly all of whom the other didn't survive", "tokens": [1057, 295, 6217, 439, 295, 472, 295, 7101, 14433, 6217, 439, 295, 7101, 264, 661, 994, 380, 7867], "temperature": 0.0, "avg_logprob": -0.27445264089675175, "compression_ratio": 1.6076555023923444, "no_speech_prob": 4.607571827364154e-05}, {"id": 15, "seek": 5302, "start": 74.66, "end": 77.30000000000001, "text": " So they had like small standard deviation in each group", "tokens": [407, 436, 632, 411, 1359, 3832, 25163, 294, 1184, 1594], "temperature": 0.0, "avg_logprob": -0.27445264089675175, "compression_ratio": 1.6076555023923444, "no_speech_prob": 4.607571827364154e-05}, {"id": 16, "seek": 5302, "start": 78.58000000000001, "end": 80.10000000000001, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.27445264089675175, "compression_ratio": 1.6076555023923444, "no_speech_prob": 4.607571827364154e-05}, {"id": 17, "seek": 8010, "start": 80.1, "end": 84.69999999999999, "text": " So then we created the world's simplest little UI to allow us to fiddle around and try to find a good", "tokens": [407, 550, 321, 2942, 264, 1002, 311, 22811, 707, 15682, 281, 2089, 505, 281, 24553, 2285, 926, 293, 853, 281, 915, 257, 665], "temperature": 0.0, "avg_logprob": -0.18591538790998788, "compression_ratio": 1.7526315789473683, "no_speech_prob": 5.5609292758163065e-05}, {"id": 18, "seek": 8010, "start": 85.74, "end": 88.05999999999999, "text": " binary split and we did", "tokens": [17434, 7472, 293, 321, 630], "temperature": 0.0, "avg_logprob": -0.18591538790998788, "compression_ratio": 1.7526315789473683, "no_speech_prob": 5.5609292758163065e-05}, {"id": 19, "seek": 8010, "start": 90.38, "end": 93.69999999999999, "text": " We did come up with a very good binary split", "tokens": [492, 630, 808, 493, 365, 257, 588, 665, 17434, 7472], "temperature": 0.0, "avg_logprob": -0.18591538790998788, "compression_ratio": 1.7526315789473683, "no_speech_prob": 5.5609292758163065e-05}, {"id": 20, "seek": 8010, "start": 94.97999999999999, "end": 96.97999999999999, "text": " which was on", "tokens": [597, 390, 322], "temperature": 0.0, "avg_logprob": -0.18591538790998788, "compression_ratio": 1.7526315789473683, "no_speech_prob": 5.5609292758163065e-05}, {"id": 21, "seek": 8010, "start": 97.61999999999999, "end": 99.61999999999999, "text": " Sex and actually we created this little", "tokens": [29037, 293, 767, 321, 2942, 341, 707], "temperature": 0.0, "avg_logprob": -0.18591538790998788, "compression_ratio": 1.7526315789473683, "no_speech_prob": 5.5609292758163065e-05}, {"id": 22, "seek": 8010, "start": 100.61999999999999, "end": 105.82, "text": " Automated version and so this is I think the first time we can well not quite the first time is it no this is", "tokens": [24619, 770, 3037, 293, 370, 341, 307, 286, 519, 264, 700, 565, 321, 393, 731, 406, 1596, 264, 700, 565, 307, 309, 572, 341, 307], "temperature": 0.0, "avg_logprob": -0.18591538790998788, "compression_ratio": 1.7526315789473683, "no_speech_prob": 5.5609292758163065e-05}, {"id": 23, "seek": 10582, "start": 105.82, "end": 109.33999999999999, "text": " This is yet another time. I should say that we have successfully created a", "tokens": [639, 307, 1939, 1071, 565, 13, 286, 820, 584, 300, 321, 362, 10727, 2942, 257], "temperature": 0.0, "avg_logprob": -0.28518936314533666, "compression_ratio": 1.5762081784386617, "no_speech_prob": 2.7966490961262025e-05}, {"id": 24, "seek": 10582, "start": 110.3, "end": 111.58, "text": " actual", "tokens": [3539], "temperature": 0.0, "avg_logprob": -0.28518936314533666, "compression_ratio": 1.5762081784386617, "no_speech_prob": 2.7966490961262025e-05}, {"id": 25, "seek": 10582, "start": 111.58, "end": 116.05999999999999, "text": " Machine learning algorithm from scratch. This one is about the world's simplest one. It's one are", "tokens": [22155, 2539, 9284, 490, 8459, 13, 639, 472, 307, 466, 264, 1002, 311, 22811, 472, 13, 467, 311, 472, 366], "temperature": 0.0, "avg_logprob": -0.28518936314533666, "compression_ratio": 1.5762081784386617, "no_speech_prob": 2.7966490961262025e-05}, {"id": 26, "seek": 10582, "start": 116.86, "end": 123.1, "text": " Creating the single rule which does a good job of splitting your data set into two parts", "tokens": [40002, 264, 2167, 4978, 597, 775, 257, 665, 1691, 295, 30348, 428, 1412, 992, 666, 732, 3166], "temperature": 0.0, "avg_logprob": -0.28518936314533666, "compression_ratio": 1.5762081784386617, "no_speech_prob": 2.7966490961262025e-05}, {"id": 27, "seek": 10582, "start": 123.1, "end": 127.02, "text": " Which differ as much as possible on the dependent variable", "tokens": [3013, 743, 382, 709, 382, 1944, 322, 264, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.28518936314533666, "compression_ratio": 1.5762081784386617, "no_speech_prob": 2.7966490961262025e-05}, {"id": 28, "seek": 10582, "start": 129.1, "end": 133.74, "text": " One hour is probably not going to cut it for a lot of things though. It's surprisingly effective", "tokens": [1485, 1773, 307, 1391, 406, 516, 281, 1723, 309, 337, 257, 688, 295, 721, 1673, 13, 467, 311, 17600, 4942], "temperature": 0.0, "avg_logprob": -0.28518936314533666, "compression_ratio": 1.5762081784386617, "no_speech_prob": 2.7966490961262025e-05}, {"id": 29, "seek": 13374, "start": 133.74, "end": 137.78, "text": " But it's so maybe we could go a step further", "tokens": [583, 309, 311, 370, 1310, 321, 727, 352, 257, 1823, 3052], "temperature": 0.0, "avg_logprob": -0.2127608809360238, "compression_ratio": 1.9085714285714286, "no_speech_prob": 2.0783940271940082e-05}, {"id": 30, "seek": 13374, "start": 138.78, "end": 144.02, "text": " And the other year step further we could go is we could create like a to our what if we took each of those", "tokens": [400, 264, 661, 1064, 1823, 3052, 321, 727, 352, 307, 321, 727, 1884, 411, 257, 281, 527, 437, 498, 321, 1890, 1184, 295, 729], "temperature": 0.0, "avg_logprob": -0.2127608809360238, "compression_ratio": 1.9085714285714286, "no_speech_prob": 2.0783940271940082e-05}, {"id": 31, "seek": 13374, "start": 145.10000000000002, "end": 149.06, "text": " groups males and females in the Titanic data set and", "tokens": [3935, 20776, 293, 21529, 294, 264, 42183, 1412, 992, 293], "temperature": 0.0, "avg_logprob": -0.2127608809360238, "compression_ratio": 1.9085714285714286, "no_speech_prob": 2.0783940271940082e-05}, {"id": 32, "seek": 13374, "start": 150.02, "end": 155.18, "text": " Split each of those into two other groups. So split the males into two groups and split the females into two groups", "tokens": [45111, 1184, 295, 729, 666, 732, 661, 3935, 13, 407, 7472, 264, 20776, 666, 732, 3935, 293, 7472, 264, 21529, 666, 732, 3935], "temperature": 0.0, "avg_logprob": -0.2127608809360238, "compression_ratio": 1.9085714285714286, "no_speech_prob": 2.0783940271940082e-05}, {"id": 33, "seek": 13374, "start": 157.06, "end": 159.06, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.2127608809360238, "compression_ratio": 1.9085714285714286, "no_speech_prob": 2.0783940271940082e-05}, {"id": 34, "seek": 13374, "start": 159.46, "end": 160.94, "text": " To do that", "tokens": [1407, 360, 300], "temperature": 0.0, "avg_logprob": -0.2127608809360238, "compression_ratio": 1.9085714285714286, "no_speech_prob": 2.0783940271940082e-05}, {"id": 35, "seek": 16094, "start": 160.94, "end": 166.3, "text": " We can repeat the exact same piece of code we just did but let's remove", "tokens": [492, 393, 7149, 264, 1900, 912, 2522, 295, 3089, 321, 445, 630, 457, 718, 311, 4159], "temperature": 0.0, "avg_logprob": -0.21293903302542772, "compression_ratio": 1.6595744680851063, "no_speech_prob": 9.222832886734977e-06}, {"id": 36, "seek": 16094, "start": 168.34, "end": 173.82, "text": " Sex from it and then split the data set into males and females", "tokens": [29037, 490, 309, 293, 550, 7472, 264, 1412, 992, 666, 20776, 293, 21529], "temperature": 0.0, "avg_logprob": -0.21293903302542772, "compression_ratio": 1.6595744680851063, "no_speech_prob": 9.222832886734977e-06}, {"id": 37, "seek": 16094, "start": 175.26, "end": 178.54, "text": " And run the same piece of code that we just did before but just for the males", "tokens": [400, 1190, 264, 912, 2522, 295, 3089, 300, 321, 445, 630, 949, 457, 445, 337, 264, 20776], "temperature": 0.0, "avg_logprob": -0.21293903302542772, "compression_ratio": 1.6595744680851063, "no_speech_prob": 9.222832886734977e-06}, {"id": 38, "seek": 16094, "start": 179.62, "end": 187.02, "text": " And so this is going to be like a 1r rule for how do we predict which males survive the Titanic and", "tokens": [400, 370, 341, 307, 516, 281, 312, 411, 257, 502, 81, 4978, 337, 577, 360, 321, 6069, 597, 20776, 7867, 264, 42183, 293], "temperature": 0.0, "avg_logprob": -0.21293903302542772, "compression_ratio": 1.6595744680851063, "no_speech_prob": 9.222832886734977e-06}, {"id": 39, "seek": 18702, "start": 187.02, "end": 193.22, "text": " Let's have a look three eight three seven three eight three eight. Okay, so it's age", "tokens": [961, 311, 362, 257, 574, 1045, 3180, 1045, 3407, 1045, 3180, 1045, 3180, 13, 1033, 11, 370, 309, 311, 3205], "temperature": 0.0, "avg_logprob": -0.2482635180155436, "compression_ratio": 1.5674157303370786, "no_speech_prob": 1.497000084782485e-05}, {"id": 40, "seek": 18702, "start": 194.62, "end": 196.62, "text": " Were they greater than or less than six?", "tokens": [12448, 436, 5044, 813, 420, 1570, 813, 2309, 30], "temperature": 0.0, "avg_logprob": -0.2482635180155436, "compression_ratio": 1.5674157303370786, "no_speech_prob": 1.497000084782485e-05}, {"id": 41, "seek": 18702, "start": 197.82000000000002, "end": 201.62, "text": " Turns out to be for the males the biggest predictor of whether they were going to survive", "tokens": [29524, 484, 281, 312, 337, 264, 20776, 264, 3880, 6069, 284, 295, 1968, 436, 645, 516, 281, 7867], "temperature": 0.0, "avg_logprob": -0.2482635180155436, "compression_ratio": 1.5674157303370786, "no_speech_prob": 1.497000084782485e-05}, {"id": 42, "seek": 18702, "start": 202.54000000000002, "end": 207.26000000000002, "text": " That shipwreck and we can do the same thing females. So females", "tokens": [663, 5374, 86, 14954, 293, 321, 393, 360, 264, 912, 551, 21529, 13, 407, 21529], "temperature": 0.0, "avg_logprob": -0.2482635180155436, "compression_ratio": 1.5674157303370786, "no_speech_prob": 1.497000084782485e-05}, {"id": 43, "seek": 20726, "start": 207.26, "end": 212.73999999999998, "text": " There we go, no great supplies P class so whether they were in", "tokens": [821, 321, 352, 11, 572, 869, 11768, 430, 1508, 370, 1968, 436, 645, 294], "temperature": 0.0, "avg_logprob": -0.573517091812626, "compression_ratio": 1.4605263157894737, "no_speech_prob": 5.861772478965577e-06}, {"id": 44, "seek": 20726, "start": 213.62, "end": 219.82, "text": " First class or not was the biggest predictor for females of whether they would survive the shipwreck", "tokens": [2386, 1508, 420, 406, 390, 264, 3880, 6069, 284, 337, 21529, 295, 1968, 436, 576, 7867, 264, 5374, 86, 14954], "temperature": 0.0, "avg_logprob": -0.573517091812626, "compression_ratio": 1.4605263157894737, "no_speech_prob": 5.861772478965577e-06}, {"id": 45, "seek": 20726, "start": 223.78, "end": 225.78, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.573517091812626, "compression_ratio": 1.4605263157894737, "no_speech_prob": 5.861772478965577e-06}, {"id": 46, "seek": 20726, "start": 225.14, "end": 227.73999999999998, "text": " That has now given us a", "tokens": [663, 575, 586, 2212, 505, 257], "temperature": 0.0, "avg_logprob": -0.573517091812626, "compression_ratio": 1.4605263157894737, "no_speech_prob": 5.861772478965577e-06}, {"id": 47, "seek": 20726, "start": 228.85999999999999, "end": 230.38, "text": " decision tree", "tokens": [3537, 4230], "temperature": 0.0, "avg_logprob": -0.573517091812626, "compression_ratio": 1.4605263157894737, "no_speech_prob": 5.861772478965577e-06}, {"id": 48, "seek": 20726, "start": 230.38, "end": 232.38, "text": " It is a series of", "tokens": [467, 307, 257, 2638, 295], "temperature": 0.0, "avg_logprob": -0.573517091812626, "compression_ratio": 1.4605263157894737, "no_speech_prob": 5.861772478965577e-06}, {"id": 49, "seek": 23238, "start": 232.38, "end": 236.78, "text": " A decision tree it is a series of binary splits", "tokens": [316, 3537, 4230, 309, 307, 257, 2638, 295, 17434, 37741], "temperature": 0.0, "avg_logprob": -0.3868380523309475, "compression_ratio": 1.5391705069124424, "no_speech_prob": 6.240537004487123e-06}, {"id": 50, "seek": 23238, "start": 237.34, "end": 239.34, "text": " which will gradually", "tokens": [597, 486, 13145], "temperature": 0.0, "avg_logprob": -0.3868380523309475, "compression_ratio": 1.5391705069124424, "no_speech_prob": 6.240537004487123e-06}, {"id": 51, "seek": 23238, "start": 239.85999999999999, "end": 243.5, "text": " Split up our data more and more such that in the end", "tokens": [45111, 493, 527, 1412, 544, 293, 544, 1270, 300, 294, 264, 917], "temperature": 0.0, "avg_logprob": -0.3868380523309475, "compression_ratio": 1.5391705069124424, "no_speech_prob": 6.240537004487123e-06}, {"id": 52, "seek": 23238, "start": 244.01999999999998, "end": 249.57999999999998, "text": " These in the leaf nodes as we call them. We will hopefully get as you know much", "tokens": [1981, 294, 264, 10871, 13891, 382, 321, 818, 552, 13, 492, 486, 4696, 483, 382, 291, 458, 709], "temperature": 0.0, "avg_logprob": -0.3868380523309475, "compression_ratio": 1.5391705069124424, "no_speech_prob": 6.240537004487123e-06}, {"id": 53, "seek": 23238, "start": 251.1, "end": 253.66, "text": " Stronger prediction as possible about survival", "tokens": [22792, 260, 17630, 382, 1944, 466, 12559], "temperature": 0.0, "avg_logprob": -0.3868380523309475, "compression_ratio": 1.5391705069124424, "no_speech_prob": 6.240537004487123e-06}, {"id": 54, "seek": 23238, "start": 255.85999999999999, "end": 260.62, "text": " So we could just repeat this step for each of the four groups we've now created males", "tokens": [407, 321, 727, 445, 7149, 341, 1823, 337, 1184, 295, 264, 1451, 3935, 321, 600, 586, 2942, 20776], "temperature": 0.0, "avg_logprob": -0.3868380523309475, "compression_ratio": 1.5391705069124424, "no_speech_prob": 6.240537004487123e-06}, {"id": 55, "seek": 26062, "start": 260.62, "end": 262.62, "text": " kids and older than six", "tokens": [2301, 293, 4906, 813, 2309], "temperature": 0.0, "avg_logprob": -0.3525656786831943, "compression_ratio": 1.5799086757990868, "no_speech_prob": 1.863074430730194e-05}, {"id": 56, "seek": 26062, "start": 263.62, "end": 265.62, "text": " females first class and", "tokens": [21529, 700, 1508, 293], "temperature": 0.0, "avg_logprob": -0.3525656786831943, "compression_ratio": 1.5799086757990868, "no_speech_prob": 1.863074430730194e-05}, {"id": 57, "seek": 26062, "start": 266.14, "end": 270.42, "text": " Everybody else and we could do it again and then we'd have eight groups", "tokens": [7646, 1646, 293, 321, 727, 360, 309, 797, 293, 550, 321, 1116, 362, 3180, 3935], "temperature": 0.0, "avg_logprob": -0.3525656786831943, "compression_ratio": 1.5799086757990868, "no_speech_prob": 1.863074430730194e-05}, {"id": 58, "seek": 26062, "start": 271.9, "end": 276.62, "text": " We could do that manually with another couple of lines of code or we can just use", "tokens": [492, 727, 360, 300, 16945, 365, 1071, 1916, 295, 3876, 295, 3089, 420, 321, 393, 445, 764], "temperature": 0.0, "avg_logprob": -0.3525656786831943, "compression_ratio": 1.5799086757990868, "no_speech_prob": 1.863074430730194e-05}, {"id": 59, "seek": 26062, "start": 277.14, "end": 280.86, "text": " Decision tree classifier, which is a class which does exactly that for us", "tokens": [12427, 1991, 4230, 1508, 9902, 11, 597, 307, 257, 1508, 597, 775, 2293, 300, 337, 505], "temperature": 0.0, "avg_logprob": -0.3525656786831943, "compression_ratio": 1.5799086757990868, "no_speech_prob": 1.863074430730194e-05}, {"id": 60, "seek": 26062, "start": 281.38, "end": 285.06, "text": " So there's no magic in here. It's just doing what we've just described", "tokens": [407, 456, 311, 572, 5585, 294, 510, 13, 467, 311, 445, 884, 437, 321, 600, 445, 7619], "temperature": 0.0, "avg_logprob": -0.3525656786831943, "compression_ratio": 1.5799086757990868, "no_speech_prob": 1.863074430730194e-05}, {"id": 61, "seek": 28506, "start": 285.06, "end": 290.98, "text": " And a decision tree classifier comes from a library called scikit-learn", "tokens": [400, 257, 3537, 4230, 1508, 9902, 1487, 490, 257, 6405, 1219, 2180, 22681, 12, 306, 1083], "temperature": 0.0, "avg_logprob": -0.4234662178235176, "compression_ratio": 1.8070175438596492, "no_speech_prob": 1.2804776815755758e-05}, {"id": 62, "seek": 28506, "start": 293.06, "end": 298.34000000000003, "text": " Scikit-learn is a fantastic library that focuses on kind of classical", "tokens": [16942, 22681, 12, 306, 1083, 307, 257, 5456, 6405, 300, 16109, 322, 733, 295, 13735], "temperature": 0.0, "avg_logprob": -0.4234662178235176, "compression_ratio": 1.8070175438596492, "no_speech_prob": 1.2804776815755758e-05}, {"id": 63, "seek": 28506, "start": 299.46, "end": 301.46, "text": " non deep learning ish", "tokens": [2107, 2452, 2539, 307, 71], "temperature": 0.0, "avg_logprob": -0.4234662178235176, "compression_ratio": 1.8070175438596492, "no_speech_prob": 1.2804776815755758e-05}, {"id": 64, "seek": 28506, "start": 301.74, "end": 303.58, "text": " machine learning methods", "tokens": [3479, 2539, 7150], "temperature": 0.0, "avg_logprob": -0.4234662178235176, "compression_ratio": 1.8070175438596492, "no_speech_prob": 1.2804776815755758e-05}, {"id": 65, "seek": 28506, "start": 303.58, "end": 305.58, "text": " like decision trees", "tokens": [411, 3537, 5852], "temperature": 0.0, "avg_logprob": -0.4234662178235176, "compression_ratio": 1.8070175438596492, "no_speech_prob": 1.2804776815755758e-05}, {"id": 66, "seek": 28506, "start": 306.5, "end": 309.3, "text": " So we can so to create the exact same decision tree", "tokens": [407, 321, 393, 370, 281, 1884, 264, 1900, 912, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.4234662178235176, "compression_ratio": 1.8070175438596492, "no_speech_prob": 1.2804776815755758e-05}, {"id": 67, "seek": 28506, "start": 309.3, "end": 311.94, "text": " We can say please create a decision tree traffic", "tokens": [492, 393, 584, 1767, 1884, 257, 3537, 4230, 6419], "temperature": 0.0, "avg_logprob": -0.4234662178235176, "compression_ratio": 1.8070175438596492, "no_speech_prob": 1.2804776815755758e-05}, {"id": 68, "seek": 31194, "start": 311.94, "end": 315.38, "text": " Classifier with at most four leaf nodes", "tokens": [9471, 9902, 365, 412, 881, 1451, 10871, 13891], "temperature": 0.0, "avg_logprob": -0.4243913847824623, "compression_ratio": 1.492537313432836, "no_speech_prob": 1.2606498785316944e-05}, {"id": 69, "seek": 31194, "start": 317.58, "end": 322.21999999999997, "text": " And one very nice thing it has is it can draw the tree for us", "tokens": [400, 472, 588, 1481, 551, 309, 575, 307, 309, 393, 2642, 264, 4230, 337, 505], "temperature": 0.0, "avg_logprob": -0.4243913847824623, "compression_ratio": 1.492537313432836, "no_speech_prob": 1.2606498785316944e-05}, {"id": 70, "seek": 31194, "start": 323.21999999999997, "end": 325.22, "text": " So here's a tiny little draw tree function", "tokens": [407, 510, 311, 257, 5870, 707, 2642, 4230, 2445], "temperature": 0.0, "avg_logprob": -0.4243913847824623, "compression_ratio": 1.492537313432836, "no_speech_prob": 1.2606498785316944e-05}, {"id": 71, "seek": 31194, "start": 327.94, "end": 329.34, "text": " And", "tokens": [400], "temperature": 0.0, "avg_logprob": -0.4243913847824623, "compression_ratio": 1.492537313432836, "no_speech_prob": 1.2606498785316944e-05}, {"id": 72, "seek": 31194, "start": 329.34, "end": 333.1, "text": " You can see here. It's going to first of all split on sex now", "tokens": [509, 393, 536, 510, 13, 467, 311, 516, 281, 700, 295, 439, 7472, 322, 3260, 586], "temperature": 0.0, "avg_logprob": -0.4243913847824623, "compression_ratio": 1.492537313432836, "no_speech_prob": 1.2606498785316944e-05}, {"id": 73, "seek": 31194, "start": 333.1, "end": 335.98, "text": " It looks a bit weird to say sex is less than or equal to 0.5", "tokens": [467, 1542, 257, 857, 3657, 281, 584, 3260, 307, 1570, 813, 420, 2681, 281, 1958, 13, 20], "temperature": 0.0, "avg_logprob": -0.4243913847824623, "compression_ratio": 1.492537313432836, "no_speech_prob": 1.2606498785316944e-05}, {"id": 74, "seek": 31194, "start": 336.3, "end": 338.42, "text": " But remember what our binary", "tokens": [583, 1604, 437, 527, 17434], "temperature": 0.0, "avg_logprob": -0.4243913847824623, "compression_ratio": 1.492537313432836, "no_speech_prob": 1.2606498785316944e-05}, {"id": 75, "seek": 33842, "start": 338.42, "end": 344.1, "text": " Characteristics are coded as zero one. So that's just how we you know easy way to", "tokens": [36786, 6006, 366, 34874, 382, 4018, 472, 13, 407, 300, 311, 445, 577, 321, 291, 458, 1858, 636, 281], "temperature": 0.0, "avg_logprob": -0.37135597124491654, "compression_ratio": 1.6167664670658684, "no_speech_prob": 5.954957487119827e-06}, {"id": 76, "seek": 33842, "start": 344.82, "end": 346.82, "text": " Say males versus females", "tokens": [6463, 20776, 5717, 21529], "temperature": 0.0, "avg_logprob": -0.37135597124491654, "compression_ratio": 1.6167664670658684, "no_speech_prob": 5.954957487119827e-06}, {"id": 77, "seek": 33842, "start": 349.14000000000004, "end": 351.78000000000003, "text": " And then here we've got for the females", "tokens": [400, 550, 510, 321, 600, 658, 337, 264, 21529], "temperature": 0.0, "avg_logprob": -0.37135597124491654, "compression_ratio": 1.6167664670658684, "no_speech_prob": 5.954957487119827e-06}, {"id": 78, "seek": 33842, "start": 353.38, "end": 359.06, "text": " What class are they in and for the males what age are they and here's our four leaf nodes", "tokens": [708, 1508, 366, 436, 294, 293, 337, 264, 20776, 437, 3205, 366, 436, 293, 510, 311, 527, 1451, 10871, 13891], "temperature": 0.0, "avg_logprob": -0.37135597124491654, "compression_ratio": 1.6167664670658684, "no_speech_prob": 5.954957487119827e-06}, {"id": 79, "seek": 33842, "start": 359.06, "end": 361.06, "text": " So for the females", "tokens": [407, 337, 264, 21529], "temperature": 0.0, "avg_logprob": -0.37135597124491654, "compression_ratio": 1.6167664670658684, "no_speech_prob": 5.954957487119827e-06}, {"id": 80, "seek": 33842, "start": 361.82, "end": 363.82, "text": " in first class", "tokens": [294, 700, 1508], "temperature": 0.0, "avg_logprob": -0.37135597124491654, "compression_ratio": 1.6167664670658684, "no_speech_prob": 5.954957487119827e-06}, {"id": 81, "seek": 36382, "start": 363.82, "end": 367.34, "text": " Females in first class a", "tokens": [479, 443, 4229, 294, 700, 1508, 257], "temperature": 0.0, "avg_logprob": -0.3149172562819261, "compression_ratio": 1.4225352112676057, "no_speech_prob": 2.19070898310747e-06}, {"id": 82, "seek": 36382, "start": 369.78, "end": 375.7, "text": " 116 of them survived and four of them didn't so very good idea to be a well-to-do", "tokens": [2975, 21, 295, 552, 14433, 293, 1451, 295, 552, 994, 380, 370, 588, 665, 1558, 281, 312, 257, 731, 12, 1353, 12, 2595], "temperature": 0.0, "avg_logprob": -0.3149172562819261, "compression_ratio": 1.4225352112676057, "no_speech_prob": 2.19070898310747e-06}, {"id": 83, "seek": 36382, "start": 376.62, "end": 378.62, "text": " woman on the Titanic", "tokens": [3059, 322, 264, 42183], "temperature": 0.0, "avg_logprob": -0.3149172562819261, "compression_ratio": 1.4225352112676057, "no_speech_prob": 2.19070898310747e-06}, {"id": 84, "seek": 36382, "start": 379.65999999999997, "end": 381.65999999999997, "text": " On the other hand", "tokens": [1282, 264, 661, 1011], "temperature": 0.0, "avg_logprob": -0.3149172562819261, "compression_ratio": 1.4225352112676057, "no_speech_prob": 2.19070898310747e-06}, {"id": 85, "seek": 36382, "start": 382.78, "end": 384.78, "text": " males", "tokens": [20776], "temperature": 0.0, "avg_logprob": -0.3149172562819261, "compression_ratio": 1.4225352112676057, "no_speech_prob": 2.19070898310747e-06}, {"id": 86, "seek": 36382, "start": 384.82, "end": 386.82, "text": " adults", "tokens": [8865], "temperature": 0.0, "avg_logprob": -0.3149172562819261, "compression_ratio": 1.4225352112676057, "no_speech_prob": 2.19070898310747e-06}, {"id": 87, "seek": 38682, "start": 386.82, "end": 395.58, "text": " 68 survived 350 died so very bad idea to be a male adult on the Titanic", "tokens": [23317, 14433, 18065, 4539, 370, 588, 1578, 1558, 281, 312, 257, 7133, 5075, 322, 264, 42183], "temperature": 0.0, "avg_logprob": -0.19882133159231632, "compression_ratio": 1.5541666666666667, "no_speech_prob": 4.4252660700294655e-06}, {"id": 88, "seek": 38682, "start": 397.09999999999997, "end": 399.1, "text": " So you can see you can kind of get a quick summary", "tokens": [407, 291, 393, 536, 291, 393, 733, 295, 483, 257, 1702, 12691], "temperature": 0.0, "avg_logprob": -0.19882133159231632, "compression_ratio": 1.5541666666666667, "no_speech_prob": 4.4252660700294655e-06}, {"id": 89, "seek": 38682, "start": 399.98, "end": 404.21999999999997, "text": " Of what's going on and one of the reasons people tend to like decision trees", "tokens": [2720, 437, 311, 516, 322, 293, 472, 295, 264, 4112, 561, 3928, 281, 411, 3537, 5852], "temperature": 0.0, "avg_logprob": -0.19882133159231632, "compression_ratio": 1.5541666666666667, "no_speech_prob": 4.4252660700294655e-06}, {"id": 90, "seek": 38682, "start": 404.34, "end": 407.38, "text": " particularly for exploratory data analysis is it doesn't allow us to", "tokens": [4098, 337, 24765, 4745, 1412, 5215, 307, 309, 1177, 380, 2089, 505, 281], "temperature": 0.0, "avg_logprob": -0.19882133159231632, "compression_ratio": 1.5541666666666667, "no_speech_prob": 4.4252660700294655e-06}, {"id": 91, "seek": 38682, "start": 408.14, "end": 411.14, "text": " Get a quick picture of what are the key?", "tokens": [3240, 257, 1702, 3036, 295, 437, 366, 264, 2141, 30], "temperature": 0.0, "avg_logprob": -0.19882133159231632, "compression_ratio": 1.5541666666666667, "no_speech_prob": 4.4252660700294655e-06}, {"id": 92, "seek": 38682, "start": 411.86, "end": 415.3, "text": " driving variables in this data set and how much do they kind of", "tokens": [4840, 9102, 294, 341, 1412, 992, 293, 577, 709, 360, 436, 733, 295], "temperature": 0.0, "avg_logprob": -0.19882133159231632, "compression_ratio": 1.5541666666666667, "no_speech_prob": 4.4252660700294655e-06}, {"id": 93, "seek": 41530, "start": 415.3, "end": 417.78000000000003, "text": " predict what was happening in the data", "tokens": [6069, 437, 390, 2737, 294, 264, 1412], "temperature": 0.0, "avg_logprob": -0.2947694405742075, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.1124501725134905e-05}, {"id": 94, "seek": 41530, "start": 419.22, "end": 421.22, "text": " Okay, so it's around the same splits as us", "tokens": [1033, 11, 370, 309, 311, 926, 264, 912, 37741, 382, 505], "temperature": 0.0, "avg_logprob": -0.2947694405742075, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.1124501725134905e-05}, {"id": 95, "seek": 41530, "start": 422.66, "end": 426.98, "text": " And it's got one additional piece of information we haven't seen before is this thing called Ginny", "tokens": [400, 309, 311, 658, 472, 4497, 2522, 295, 1589, 321, 2378, 380, 1612, 949, 307, 341, 551, 1219, 36846, 1634], "temperature": 0.0, "avg_logprob": -0.2947694405742075, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.1124501725134905e-05}, {"id": 96, "seek": 41530, "start": 428.18, "end": 432.82, "text": " Ginny is just another way of measuring how good a split is", "tokens": [36846, 1634, 307, 445, 1071, 636, 295, 13389, 577, 665, 257, 7472, 307], "temperature": 0.0, "avg_logprob": -0.2947694405742075, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.1124501725134905e-05}, {"id": 97, "seek": 41530, "start": 433.78000000000003, "end": 435.3, "text": " and I've", "tokens": [293, 286, 600], "temperature": 0.0, "avg_logprob": -0.2947694405742075, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.1124501725134905e-05}, {"id": 98, "seek": 41530, "start": 435.3, "end": 437.3, "text": " Put the code to calculate Ginny here", "tokens": [4935, 264, 3089, 281, 8873, 36846, 1634, 510], "temperature": 0.0, "avg_logprob": -0.2947694405742075, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.1124501725134905e-05}, {"id": 99, "seek": 41530, "start": 438.82, "end": 440.82, "text": " Here's how you can think of Ginny", "tokens": [1692, 311, 577, 291, 393, 519, 295, 36846, 1634], "temperature": 0.0, "avg_logprob": -0.2947694405742075, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.1124501725134905e-05}, {"id": 100, "seek": 44082, "start": 440.82, "end": 445.7, "text": " how likely is it that if you go into that sample and", "tokens": [577, 3700, 307, 309, 300, 498, 291, 352, 666, 300, 6889, 293], "temperature": 0.0, "avg_logprob": -0.2388304378209489, "compression_ratio": 1.7835051546391754, "no_speech_prob": 4.092751169082476e-06}, {"id": 101, "seek": 44082, "start": 446.34, "end": 448.46, "text": " grab one item and", "tokens": [4444, 472, 3174, 293], "temperature": 0.0, "avg_logprob": -0.2388304378209489, "compression_ratio": 1.7835051546391754, "no_speech_prob": 4.092751169082476e-06}, {"id": 102, "seek": 44082, "start": 448.98, "end": 454.98, "text": " Then go in again and grab another item. How likely is it that you're going to grab the same item each time?", "tokens": [1396, 352, 294, 797, 293, 4444, 1071, 3174, 13, 1012, 3700, 307, 309, 300, 291, 434, 516, 281, 4444, 264, 912, 3174, 1184, 565, 30], "temperature": 0.0, "avg_logprob": -0.2388304378209489, "compression_ratio": 1.7835051546391754, "no_speech_prob": 4.092751169082476e-06}, {"id": 103, "seek": 44082, "start": 455.54, "end": 457.53999999999996, "text": " and so", "tokens": [293, 370], "temperature": 0.0, "avg_logprob": -0.2388304378209489, "compression_ratio": 1.7835051546391754, "no_speech_prob": 4.092751169082476e-06}, {"id": 104, "seek": 44082, "start": 458.3, "end": 460.98, "text": " if that if the entire", "tokens": [498, 300, 498, 264, 2302], "temperature": 0.0, "avg_logprob": -0.2388304378209489, "compression_ratio": 1.7835051546391754, "no_speech_prob": 4.092751169082476e-06}, {"id": 105, "seek": 44082, "start": 461.5, "end": 468.5, "text": " Leaf node is just people who survived or just people who didn't survive the probability would be one you get the same time same every time", "tokens": [32290, 9984, 307, 445, 561, 567, 14433, 420, 445, 561, 567, 994, 380, 7867, 264, 8482, 576, 312, 472, 291, 483, 264, 912, 565, 912, 633, 565], "temperature": 0.0, "avg_logprob": -0.2388304378209489, "compression_ratio": 1.7835051546391754, "no_speech_prob": 4.092751169082476e-06}, {"id": 106, "seek": 46850, "start": 468.5, "end": 471.9, "text": " If it was an exactly equal mix the probability would be 0.5", "tokens": [759, 309, 390, 364, 2293, 2681, 2890, 264, 8482, 576, 312, 1958, 13, 20], "temperature": 0.0, "avg_logprob": -0.23074122575613168, "compression_ratio": 1.581896551724138, "no_speech_prob": 1.045043973135762e-05}, {"id": 107, "seek": 46850, "start": 472.42, "end": 474.42, "text": " so that's why we just", "tokens": [370, 300, 311, 983, 321, 445], "temperature": 0.0, "avg_logprob": -0.23074122575613168, "compression_ratio": 1.581896551724138, "no_speech_prob": 1.045043973135762e-05}, {"id": 108, "seek": 46850, "start": 475.18, "end": 478.9, "text": " Yeah, that's where this this formula comes from in the binary case", "tokens": [865, 11, 300, 311, 689, 341, 341, 8513, 1487, 490, 294, 264, 17434, 1389], "temperature": 0.0, "avg_logprob": -0.23074122575613168, "compression_ratio": 1.581896551724138, "no_speech_prob": 1.045043973135762e-05}, {"id": 109, "seek": 46850, "start": 480.02, "end": 485.3, "text": " And in fact, you can see it here right this group here is pretty much 50-50. So Ginny is 0.5", "tokens": [400, 294, 1186, 11, 291, 393, 536, 309, 510, 558, 341, 1594, 510, 307, 1238, 709, 2625, 12, 2803, 13, 407, 36846, 1634, 307, 1958, 13, 20], "temperature": 0.0, "avg_logprob": -0.23074122575613168, "compression_ratio": 1.581896551724138, "no_speech_prob": 1.045043973135762e-05}, {"id": 110, "seek": 46850, "start": 486.02, "end": 490.1, "text": " Or else this group here is nearly a hundred percent in one class. So Ginny is nearly", "tokens": [1610, 1646, 341, 1594, 510, 307, 6217, 257, 3262, 3043, 294, 472, 1508, 13, 407, 36846, 1634, 307, 6217], "temperature": 0.0, "avg_logprob": -0.23074122575613168, "compression_ratio": 1.581896551724138, "no_speech_prob": 1.045043973135762e-05}, {"id": 111, "seek": 46850, "start": 490.78, "end": 492.78, "text": " Zero, so I had it backwards is one minus", "tokens": [17182, 11, 370, 286, 632, 309, 12204, 307, 472, 3175], "temperature": 0.0, "avg_logprob": -0.23074122575613168, "compression_ratio": 1.581896551724138, "no_speech_prob": 1.045043973135762e-05}, {"id": 112, "seek": 49278, "start": 492.78, "end": 496.78, "text": " And I think I've written it backwards here as well so I've got to fix that", "tokens": [400, 286, 519, 286, 600, 3720, 309, 12204, 510, 382, 731, 370, 286, 600, 658, 281, 3191, 300], "temperature": 0.0, "avg_logprob": -0.3764589336556448, "compression_ratio": 1.423913043478261, "no_speech_prob": 5.173746103537269e-06}, {"id": 113, "seek": 49278, "start": 502.38, "end": 504.78, "text": " So this decision tree is", "tokens": [407, 341, 3537, 4230, 307], "temperature": 0.0, "avg_logprob": -0.3764589336556448, "compression_ratio": 1.423913043478261, "no_speech_prob": 5.173746103537269e-06}, {"id": 114, "seek": 49278, "start": 505.5, "end": 511.09999999999997, "text": " You know, we would expect it to be all accurate so we can calculate. It's been absolute error and for the 1r", "tokens": [509, 458, 11, 321, 576, 2066, 309, 281, 312, 439, 8559, 370, 321, 393, 8873, 13, 467, 311, 668, 8236, 6713, 293, 337, 264, 502, 81], "temperature": 0.0, "avg_logprob": -0.3764589336556448, "compression_ratio": 1.423913043478261, "no_speech_prob": 5.173746103537269e-06}, {"id": 115, "seek": 49278, "start": 511.09999999999997, "end": 513.1, "text": " So just doing males versus females", "tokens": [407, 445, 884, 20776, 5717, 21529], "temperature": 0.0, "avg_logprob": -0.3764589336556448, "compression_ratio": 1.423913043478261, "no_speech_prob": 5.173746103537269e-06}, {"id": 116, "seek": 49278, "start": 516.78, "end": 518.78, "text": " What was our score", "tokens": [708, 390, 527, 6175], "temperature": 0.0, "avg_logprob": -0.3764589336556448, "compression_ratio": 1.423913043478261, "no_speech_prob": 5.173746103537269e-06}, {"id": 117, "seek": 51878, "start": 518.78, "end": 523.02, "text": " What was our score here we go", "tokens": [708, 390, 527, 6175, 510, 321, 352], "temperature": 0.0, "avg_logprob": -0.49770652957078887, "compression_ratio": 1.5185185185185186, "no_speech_prob": 9.972571206162684e-06}, {"id": 118, "seek": 51878, "start": 524.06, "end": 526.06, "text": " 0.407", "tokens": [1958, 13, 5254, 22], "temperature": 0.0, "avg_logprob": -0.49770652957078887, "compression_ratio": 1.5185185185185186, "no_speech_prob": 9.972571206162684e-06}, {"id": 119, "seek": 51878, "start": 526.06, "end": 530.22, "text": " Actually, we have do we have an accuracy score somewhere here we are 0.336", "tokens": [5135, 11, 321, 362, 360, 321, 362, 364, 14170, 6175, 4079, 510, 321, 366, 1958, 13, 10191, 21], "temperature": 0.0, "avg_logprob": -0.49770652957078887, "compression_ratio": 1.5185185185185186, "no_speech_prob": 9.972571206162684e-06}, {"id": 120, "seek": 51878, "start": 533.18, "end": 537.98, "text": " That was for log-fair and for sex it was", "tokens": [663, 390, 337, 3565, 12, 69, 1246, 293, 337, 3260, 309, 390], "temperature": 0.0, "avg_logprob": -0.49770652957078887, "compression_ratio": 1.5185185185185186, "no_speech_prob": 9.972571206162684e-06}, {"id": 121, "seek": 51878, "start": 539.9, "end": 541.9, "text": " 0.215", "tokens": [1958, 13, 4436, 20], "temperature": 0.0, "avg_logprob": -0.49770652957078887, "compression_ratio": 1.5185185185185186, "no_speech_prob": 9.972571206162684e-06}, {"id": 122, "seek": 51878, "start": 541.9, "end": 547.1, "text": " Okay, so 0.215 so that was for the 1r version for the decision tree with four leaf nodes", "tokens": [1033, 11, 370, 1958, 13, 4436, 20, 370, 300, 390, 337, 264, 502, 81, 3037, 337, 264, 3537, 4230, 365, 1451, 10871, 13891], "temperature": 0.0, "avg_logprob": -0.49770652957078887, "compression_ratio": 1.5185185185185186, "no_speech_prob": 9.972571206162684e-06}, {"id": 123, "seek": 54710, "start": 547.1, "end": 549.34, "text": " 0.224 so it's actually a little worse", "tokens": [1958, 13, 7490, 19, 370, 309, 311, 767, 257, 707, 5324], "temperature": 0.0, "avg_logprob": -0.3219206704033746, "compression_ratio": 1.5047169811320755, "no_speech_prob": 9.665184734330978e-06}, {"id": 124, "seek": 54710, "start": 549.9, "end": 554.14, "text": " Right, and I think this just reflects the fact that this is such a small data set", "tokens": [1779, 11, 293, 286, 519, 341, 445, 18926, 264, 1186, 300, 341, 307, 1270, 257, 1359, 1412, 992], "temperature": 0.0, "avg_logprob": -0.3219206704033746, "compression_ratio": 1.5047169811320755, "no_speech_prob": 9.665184734330978e-06}, {"id": 125, "seek": 54710, "start": 555.9, "end": 557.9, "text": " And the 1r", "tokens": [400, 264, 502, 81], "temperature": 0.0, "avg_logprob": -0.3219206704033746, "compression_ratio": 1.5047169811320755, "no_speech_prob": 9.665184734330978e-06}, {"id": 126, "seek": 54710, "start": 558.86, "end": 562.46, "text": " Version was so good. We haven't really improved it that much", "tokens": [35965, 390, 370, 665, 13, 492, 2378, 380, 534, 9689, 309, 300, 709], "temperature": 0.0, "avg_logprob": -0.3219206704033746, "compression_ratio": 1.5047169811320755, "no_speech_prob": 9.665184734330978e-06}, {"id": 127, "seek": 54710, "start": 563.4200000000001, "end": 568.5400000000001, "text": " I'm not enough to really see it amongst the randomness of such a small validation set", "tokens": [286, 478, 406, 1547, 281, 534, 536, 309, 12918, 264, 4974, 1287, 295, 1270, 257, 1359, 24071, 992], "temperature": 0.0, "avg_logprob": -0.3219206704033746, "compression_ratio": 1.5047169811320755, "no_speech_prob": 9.665184734330978e-06}, {"id": 128, "seek": 54710, "start": 570.78, "end": 572.78, "text": " We could go further", "tokens": [492, 727, 352, 3052], "temperature": 0.0, "avg_logprob": -0.3219206704033746, "compression_ratio": 1.5047169811320755, "no_speech_prob": 9.665184734330978e-06}, {"id": 129, "seek": 54710, "start": 572.78, "end": 575.58, "text": " To 50 a minimum of 50", "tokens": [1407, 2625, 257, 7285, 295, 2625], "temperature": 0.0, "avg_logprob": -0.3219206704033746, "compression_ratio": 1.5047169811320755, "no_speech_prob": 9.665184734330978e-06}, {"id": 130, "seek": 57558, "start": 575.58, "end": 578.62, "text": " Samples per leaf node. So that means that in each of these", "tokens": [4832, 2622, 680, 10871, 9984, 13, 407, 300, 1355, 300, 294, 1184, 295, 613], "temperature": 0.0, "avg_logprob": -0.25272852460914685, "compression_ratio": 1.668, "no_speech_prob": 1.3845372450305149e-05}, {"id": 131, "seek": 57558, "start": 579.26, "end": 585.74, "text": " See how it says samples, which in this case is passengers on the Titanic. There's at least there's 67 people that", "tokens": [3008, 577, 309, 1619, 10938, 11, 597, 294, 341, 1389, 307, 18436, 322, 264, 42183, 13, 821, 311, 412, 1935, 456, 311, 23879, 561, 300], "temperature": 0.0, "avg_logprob": -0.25272852460914685, "compression_ratio": 1.668, "no_speech_prob": 1.3845372450305149e-05}, {"id": 132, "seek": 57558, "start": 586.62, "end": 587.9000000000001, "text": " were", "tokens": [645], "temperature": 0.0, "avg_logprob": -0.25272852460914685, "compression_ratio": 1.668, "no_speech_prob": 1.3845372450305149e-05}, {"id": 133, "seek": 57558, "start": 587.9000000000001, "end": 589.34, "text": " female", "tokens": [6556], "temperature": 0.0, "avg_logprob": -0.25272852460914685, "compression_ratio": 1.668, "no_speech_prob": 1.3845372450305149e-05}, {"id": 134, "seek": 57558, "start": 589.34, "end": 591.34, "text": " first-class", "tokens": [700, 12, 11665], "temperature": 0.0, "avg_logprob": -0.25272852460914685, "compression_ratio": 1.668, "no_speech_prob": 1.3845372450305149e-05}, {"id": 135, "seek": 57558, "start": 591.34, "end": 593.34, "text": " Less than 28", "tokens": [18649, 813, 7562], "temperature": 0.0, "avg_logprob": -0.25272852460914685, "compression_ratio": 1.668, "no_speech_prob": 1.3845372450305149e-05}, {"id": 136, "seek": 57558, "start": 593.6600000000001, "end": 597.34, "text": " That's how you define that so this decision tree keeps building keeps splitting", "tokens": [663, 311, 577, 291, 6964, 300, 370, 341, 3537, 4230, 5965, 2390, 5965, 30348], "temperature": 0.0, "avg_logprob": -0.25272852460914685, "compression_ratio": 1.668, "no_speech_prob": 1.3845372450305149e-05}, {"id": 137, "seek": 57558, "start": 597.58, "end": 602.86, "text": " Until it gets to a point where there's going to be less than 50 at which point it stops splitting that that leaf so you can see", "tokens": [9088, 309, 2170, 281, 257, 935, 689, 456, 311, 516, 281, 312, 1570, 813, 2625, 412, 597, 935, 309, 10094, 30348, 300, 300, 10871, 370, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.25272852460914685, "compression_ratio": 1.668, "no_speech_prob": 1.3845372450305149e-05}, {"id": 138, "seek": 60286, "start": 602.86, "end": 605.9, "text": " They're all got at least 50 samples", "tokens": [814, 434, 439, 658, 412, 1935, 2625, 10938], "temperature": 0.0, "avg_logprob": -0.284675071113988, "compression_ratio": 1.436842105263158, "no_speech_prob": 3.555905550456373e-06}, {"id": 139, "seek": 60286, "start": 607.42, "end": 613.82, "text": " And so here's the decision tree that builds as you can see it doesn't have to be like constant depth, right? So this group here", "tokens": [400, 370, 510, 311, 264, 3537, 4230, 300, 15182, 382, 291, 393, 536, 309, 1177, 380, 362, 281, 312, 411, 5754, 7161, 11, 558, 30, 407, 341, 1594, 510], "temperature": 0.0, "avg_logprob": -0.284675071113988, "compression_ratio": 1.436842105263158, "no_speech_prob": 3.555905550456373e-06}, {"id": 140, "seek": 60286, "start": 614.78, "end": 616.78, "text": " Which is males", "tokens": [3013, 307, 20776], "temperature": 0.0, "avg_logprob": -0.284675071113988, "compression_ratio": 1.436842105263158, "no_speech_prob": 3.555905550456373e-06}, {"id": 141, "seek": 60286, "start": 617.66, "end": 619.66, "text": " Who had cheaper fares?", "tokens": [2102, 632, 12284, 2050, 495, 30], "temperature": 0.0, "avg_logprob": -0.284675071113988, "compression_ratio": 1.436842105263158, "no_speech_prob": 3.555905550456373e-06}, {"id": 142, "seek": 60286, "start": 620.94, "end": 623.66, "text": " And who were older than 20", "tokens": [400, 567, 645, 4906, 813, 945], "temperature": 0.0, "avg_logprob": -0.284675071113988, "compression_ratio": 1.436842105263158, "no_speech_prob": 3.555905550456373e-06}, {"id": 143, "seek": 60286, "start": 624.94, "end": 626.94, "text": " But younger than 32", "tokens": [583, 7037, 813, 8858], "temperature": 0.0, "avg_logprob": -0.284675071113988, "compression_ratio": 1.436842105263158, "no_speech_prob": 3.555905550456373e-06}, {"id": 144, "seek": 60286, "start": 628.54, "end": 630.54, "text": " Actually younger than 24", "tokens": [5135, 7037, 813, 4022], "temperature": 0.0, "avg_logprob": -0.284675071113988, "compression_ratio": 1.436842105263158, "no_speech_prob": 3.555905550456373e-06}, {"id": 145, "seek": 63054, "start": 630.54, "end": 632.54, "text": " actually younger than 24", "tokens": [767, 7037, 813, 4022], "temperature": 0.0, "avg_logprob": -0.1540315876836362, "compression_ratio": 1.5833333333333333, "no_speech_prob": 8.267411431006622e-06}, {"id": 146, "seek": 63054, "start": 633.42, "end": 635.0999999999999, "text": " and actually", "tokens": [293, 767], "temperature": 0.0, "avg_logprob": -0.1540315876836362, "compression_ratio": 1.5833333333333333, "no_speech_prob": 8.267411431006622e-06}, {"id": 147, "seek": 63054, "start": 635.0999999999999, "end": 640.62, "text": " Super cheap fares and so forth, right? So it keeps going down until we get to that group. So", "tokens": [4548, 7084, 2050, 495, 293, 370, 5220, 11, 558, 30, 407, 309, 5965, 516, 760, 1826, 321, 483, 281, 300, 1594, 13, 407], "temperature": 0.0, "avg_logprob": -0.1540315876836362, "compression_ratio": 1.5833333333333333, "no_speech_prob": 8.267411431006622e-06}, {"id": 148, "seek": 63054, "start": 641.98, "end": 646.62, "text": " Let's try that decision tree. So that decision tree has an absolute error of point 183", "tokens": [961, 311, 853, 300, 3537, 4230, 13, 407, 300, 3537, 4230, 575, 364, 8236, 6713, 295, 935, 2443, 18], "temperature": 0.0, "avg_logprob": -0.1540315876836362, "compression_ratio": 1.5833333333333333, "no_speech_prob": 8.267411431006622e-06}, {"id": 149, "seek": 63054, "start": 647.42, "end": 651.3399999999999, "text": " So not surprisingly, you know, once we get there, it's starting to look like it's a little bit better", "tokens": [407, 406, 17600, 11, 291, 458, 11, 1564, 321, 483, 456, 11, 309, 311, 2891, 281, 574, 411, 309, 311, 257, 707, 857, 1101], "temperature": 0.0, "avg_logprob": -0.1540315876836362, "compression_ratio": 1.5833333333333333, "no_speech_prob": 8.267411431006622e-06}, {"id": 150, "seek": 63054, "start": 654.86, "end": 657.66, "text": " So there's a model and", "tokens": [407, 456, 311, 257, 2316, 293], "temperature": 0.0, "avg_logprob": -0.1540315876836362, "compression_ratio": 1.5833333333333333, "no_speech_prob": 8.267411431006622e-06}, {"id": 151, "seek": 65766, "start": 657.66, "end": 662.54, "text": " This is a kaggle competition. So therefore we should submit it to the leaderboard", "tokens": [639, 307, 257, 350, 559, 22631, 6211, 13, 407, 4412, 321, 820, 10315, 309, 281, 264, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.13510122393617535, "compression_ratio": 1.7685589519650655, "no_speech_prob": 4.710727807832882e-06}, {"id": 152, "seek": 65766, "start": 663.8199999999999, "end": 665.1, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.13510122393617535, "compression_ratio": 1.7685589519650655, "no_speech_prob": 4.710727807832882e-06}, {"id": 153, "seek": 65766, "start": 665.1, "end": 667.1, "text": " you know one of the you know", "tokens": [291, 458, 472, 295, 264, 291, 458], "temperature": 0.0, "avg_logprob": -0.13510122393617535, "compression_ratio": 1.7685589519650655, "no_speech_prob": 4.710727807832882e-06}, {"id": 154, "seek": 65766, "start": 667.9, "end": 669.9, "text": " biggest mistakes I see", "tokens": [3880, 8038, 286, 536], "temperature": 0.0, "avg_logprob": -0.13510122393617535, "compression_ratio": 1.7685589519650655, "no_speech_prob": 4.710727807832882e-06}, {"id": 155, "seek": 65766, "start": 670.86, "end": 675.74, "text": " Not just beginners, but every level of practitioner make on kaggle is not to submit to the leaderboard", "tokens": [1726, 445, 26992, 11, 457, 633, 1496, 295, 32125, 652, 322, 350, 559, 22631, 307, 406, 281, 10315, 281, 264, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.13510122393617535, "compression_ratio": 1.7685589519650655, "no_speech_prob": 4.710727807832882e-06}, {"id": 156, "seek": 65766, "start": 676.4599999999999, "end": 681.74, "text": " Spend months making some perfect thing right, but you're actually going to see how you're going", "tokens": [1738, 521, 2493, 1455, 512, 2176, 551, 558, 11, 457, 291, 434, 767, 516, 281, 536, 577, 291, 434, 516], "temperature": 0.0, "avg_logprob": -0.13510122393617535, "compression_ratio": 1.7685589519650655, "no_speech_prob": 4.710727807832882e-06}, {"id": 157, "seek": 65766, "start": 681.74, "end": 684.3, "text": " And you should try and submit something to the leaderboard every day", "tokens": [400, 291, 820, 853, 293, 10315, 746, 281, 264, 5263, 3787, 633, 786], "temperature": 0.0, "avg_logprob": -0.13510122393617535, "compression_ratio": 1.7685589519650655, "no_speech_prob": 4.710727807832882e-06}, {"id": 158, "seek": 68430, "start": 684.3, "end": 687.3399999999999, "text": " So, you know regardless of how rubbish it is because", "tokens": [407, 11, 291, 458, 10060, 295, 577, 29978, 309, 307, 570], "temperature": 0.0, "avg_logprob": -0.25265835032743567, "compression_ratio": 1.6736842105263159, "no_speech_prob": 8.397559213335626e-06}, {"id": 159, "seek": 68430, "start": 688.38, "end": 690.38, "text": " You want to improve every day?", "tokens": [509, 528, 281, 3470, 633, 786, 30], "temperature": 0.0, "avg_logprob": -0.25265835032743567, "compression_ratio": 1.6736842105263159, "no_speech_prob": 8.397559213335626e-06}, {"id": 160, "seek": 68430, "start": 690.38, "end": 696.06, "text": " So you want to keep iterating so to submit something to the leaderboard? You generally have to provide a", "tokens": [407, 291, 528, 281, 1066, 17138, 990, 370, 281, 10315, 746, 281, 264, 5263, 3787, 30, 509, 5101, 362, 281, 2893, 257], "temperature": 0.0, "avg_logprob": -0.25265835032743567, "compression_ratio": 1.6736842105263159, "no_speech_prob": 8.397559213335626e-06}, {"id": 161, "seek": 68430, "start": 697.02, "end": 699.02, "text": " csv file", "tokens": [28277, 85, 3991], "temperature": 0.0, "avg_logprob": -0.25265835032743567, "compression_ratio": 1.6736842105263159, "no_speech_prob": 8.397559213335626e-06}, {"id": 162, "seek": 68430, "start": 699.9, "end": 702.62, "text": " And so we're going to create a csv file", "tokens": [400, 370, 321, 434, 516, 281, 1884, 257, 28277, 85, 3991], "temperature": 0.0, "avg_logprob": -0.25265835032743567, "compression_ratio": 1.6736842105263159, "no_speech_prob": 8.397559213335626e-06}, {"id": 163, "seek": 68430, "start": 706.38, "end": 711.8199999999999, "text": " And we're going to apply the category codes to get the the category for each one", "tokens": [400, 321, 434, 516, 281, 3079, 264, 7719, 14211, 281, 483, 264, 264, 7719, 337, 1184, 472], "temperature": 0.0, "avg_logprob": -0.25265835032743567, "compression_ratio": 1.6736842105263159, "no_speech_prob": 8.397559213335626e-06}, {"id": 164, "seek": 71182, "start": 711.82, "end": 716.0600000000001, "text": " In our test set we're going to set the survived column to our predictions", "tokens": [682, 527, 1500, 992, 321, 434, 516, 281, 992, 264, 14433, 7738, 281, 527, 21264], "temperature": 0.0, "avg_logprob": -0.21333845730485587, "compression_ratio": 1.6010362694300517, "no_speech_prob": 8.529604201612528e-06}, {"id": 165, "seek": 71182, "start": 717.58, "end": 719.58, "text": " And then we're going to send that off to a csv", "tokens": [400, 550, 321, 434, 516, 281, 2845, 300, 766, 281, 257, 28277, 85], "temperature": 0.0, "avg_logprob": -0.21333845730485587, "compression_ratio": 1.6010362694300517, "no_speech_prob": 8.529604201612528e-06}, {"id": 166, "seek": 71182, "start": 722.5400000000001, "end": 724.5400000000001, "text": " And so yeah, so I submitted that", "tokens": [400, 370, 1338, 11, 370, 286, 14405, 300], "temperature": 0.0, "avg_logprob": -0.21333845730485587, "compression_ratio": 1.6010362694300517, "no_speech_prob": 8.529604201612528e-06}, {"id": 167, "seek": 71182, "start": 725.34, "end": 730.3000000000001, "text": " And I got a score a little bit worse than most of our linear models and neural nets", "tokens": [400, 286, 658, 257, 6175, 257, 707, 857, 5324, 813, 881, 295, 527, 8213, 5245, 293, 18161, 36170], "temperature": 0.0, "avg_logprob": -0.21333845730485587, "compression_ratio": 1.6010362694300517, "no_speech_prob": 8.529604201612528e-06}, {"id": 168, "seek": 71182, "start": 730.94, "end": 735.1, "text": " But not terrible. You know, it was uh, it's it's it's doing an okay job", "tokens": [583, 406, 6237, 13, 509, 458, 11, 309, 390, 2232, 11, 309, 311, 309, 311, 309, 311, 884, 364, 1392, 1691], "temperature": 0.0, "avg_logprob": -0.21333845730485587, "compression_ratio": 1.6010362694300517, "no_speech_prob": 8.529604201612528e-06}, {"id": 169, "seek": 73510, "start": 735.1, "end": 740.14, "text": " Now one interesting thing for the decision tree is there was a lot less pre-processing to do", "tokens": [823, 472, 1880, 551, 337, 264, 3537, 4230, 307, 456, 390, 257, 688, 1570, 659, 12, 41075, 278, 281, 360], "temperature": 0.0, "avg_logprob": -0.2890632474744642, "compression_ratio": 1.5851063829787233, "no_speech_prob": 1.2606396921910346e-05}, {"id": 170, "seek": 73510, "start": 740.14, "end": 745.1800000000001, "text": " Did you notice that we didn't have to create any dummy variables for our categories?", "tokens": [2589, 291, 3449, 300, 321, 994, 380, 362, 281, 1884, 604, 35064, 9102, 337, 527, 10479, 30], "temperature": 0.0, "avg_logprob": -0.2890632474744642, "compression_ratio": 1.5851063829787233, "no_speech_prob": 1.2606396921910346e-05}, {"id": 171, "seek": 73510, "start": 747.02, "end": 748.14, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.2890632474744642, "compression_ratio": 1.5851063829787233, "no_speech_prob": 1.2606396921910346e-05}, {"id": 172, "seek": 73510, "start": 748.14, "end": 753.9, "text": " Like you certainly can create dummy variables, but you often don't have to so for example", "tokens": [1743, 291, 3297, 393, 1884, 35064, 9102, 11, 457, 291, 2049, 500, 380, 362, 281, 370, 337, 1365], "temperature": 0.0, "avg_logprob": -0.2890632474744642, "compression_ratio": 1.5851063829787233, "no_speech_prob": 1.2606396921910346e-05}, {"id": 173, "seek": 73510, "start": 755.4200000000001, "end": 757.82, "text": " You know for for for class", "tokens": [509, 458, 337, 337, 337, 1508], "temperature": 0.0, "avg_logprob": -0.2890632474744642, "compression_ratio": 1.5851063829787233, "no_speech_prob": 1.2606396921910346e-05}, {"id": 174, "seek": 75782, "start": 757.82, "end": 764.22, "text": " You know, it's one two or three. You can just split on one two or three, you know, um, even for like", "tokens": [509, 458, 11, 309, 311, 472, 732, 420, 1045, 13, 509, 393, 445, 7472, 322, 472, 732, 420, 1045, 11, 291, 458, 11, 1105, 11, 754, 337, 411], "temperature": 0.0, "avg_logprob": -0.5147434156768176, "compression_ratio": 1.7027027027027026, "no_speech_prob": 7.182849458331475e-06}, {"id": 175, "seek": 75782, "start": 765.2600000000001, "end": 767.74, "text": " What was that thing like the the embarkation?", "tokens": [708, 390, 300, 551, 411, 264, 264, 29832, 399, 30], "temperature": 0.0, "avg_logprob": -0.5147434156768176, "compression_ratio": 1.7027027027027026, "no_speech_prob": 7.182849458331475e-06}, {"id": 176, "seek": 75782, "start": 768.5400000000001, "end": 775.1, "text": " City code like we just convert them kind of arbitrarily to numbers one two and three and you can split on those numbers", "tokens": [4392, 3089, 411, 321, 445, 7620, 552, 733, 295, 19071, 3289, 281, 3547, 472, 732, 293, 1045, 293, 291, 393, 7472, 322, 729, 3547], "temperature": 0.0, "avg_logprob": -0.5147434156768176, "compression_ratio": 1.7027027027027026, "no_speech_prob": 7.182849458331475e-06}, {"id": 177, "seek": 75782, "start": 775.9000000000001, "end": 779.58, "text": " So with random forests, they're not random first not yet decision trees", "tokens": [407, 365, 4974, 21700, 11, 436, 434, 406, 4974, 700, 406, 1939, 3537, 5852], "temperature": 0.0, "avg_logprob": -0.5147434156768176, "compression_ratio": 1.7027027027027026, "no_speech_prob": 7.182849458331475e-06}, {"id": 178, "seek": 75782, "start": 780.86, "end": 783.2600000000001, "text": " And so we're going to create a new tree", "tokens": [400, 370, 321, 434, 516, 281, 1884, 257, 777, 4230], "temperature": 0.0, "avg_logprob": -0.5147434156768176, "compression_ratio": 1.7027027027027026, "no_speech_prob": 7.182849458331475e-06}, {"id": 179, "seek": 78326, "start": 783.26, "end": 787.26, "text": " So with random forests, they're not random first not yet decision trees. Um,", "tokens": [407, 365, 4974, 21700, 11, 436, 434, 406, 4974, 700, 406, 1939, 3537, 5852, 13, 3301, 11], "temperature": 0.0, "avg_logprob": -0.1692009517124721, "compression_ratio": 1.5916666666666666, "no_speech_prob": 4.495113898883574e-06}, {"id": 180, "seek": 78326, "start": 787.9, "end": 789.9, "text": " yeah, you can generally get away with", "tokens": [1338, 11, 291, 393, 5101, 483, 1314, 365], "temperature": 0.0, "avg_logprob": -0.1692009517124721, "compression_ratio": 1.5916666666666666, "no_speech_prob": 4.495113898883574e-06}, {"id": 181, "seek": 78326, "start": 790.46, "end": 792.46, "text": " not doing stuff like", "tokens": [406, 884, 1507, 411], "temperature": 0.0, "avg_logprob": -0.1692009517124721, "compression_ratio": 1.5916666666666666, "no_speech_prob": 4.495113898883574e-06}, {"id": 182, "seek": 78326, "start": 792.9399999999999, "end": 794.46, "text": " dummy variables", "tokens": [35064, 9102], "temperature": 0.0, "avg_logprob": -0.1692009517124721, "compression_ratio": 1.5916666666666666, "no_speech_prob": 4.495113898883574e-06}, {"id": 183, "seek": 78326, "start": 794.46, "end": 796.86, "text": " Uh, in fact even taking the log of fair", "tokens": [4019, 11, 294, 1186, 754, 1940, 264, 3565, 295, 3143], "temperature": 0.0, "avg_logprob": -0.1692009517124721, "compression_ratio": 1.5916666666666666, "no_speech_prob": 4.495113898883574e-06}, {"id": 184, "seek": 78326, "start": 797.9, "end": 802.3, "text": " We only did that to make our graph look better. But if you think about it", "tokens": [492, 787, 630, 300, 281, 652, 527, 4295, 574, 1101, 13, 583, 498, 291, 519, 466, 309], "temperature": 0.0, "avg_logprob": -0.1692009517124721, "compression_ratio": 1.5916666666666666, "no_speech_prob": 4.495113898883574e-06}, {"id": 185, "seek": 78326, "start": 803.1, "end": 806.08, "text": " splitting on log fair less than 2.7", "tokens": [30348, 322, 3565, 3143, 1570, 813, 568, 13, 22], "temperature": 0.0, "avg_logprob": -0.1692009517124721, "compression_ratio": 1.5916666666666666, "no_speech_prob": 4.495113898883574e-06}, {"id": 186, "seek": 78326, "start": 807.18, "end": 811.34, "text": " It's exactly the same as splitting on fair is less than either the 2.7, you know", "tokens": [467, 311, 2293, 264, 912, 382, 30348, 322, 3143, 307, 1570, 813, 2139, 264, 568, 13, 22, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.1692009517124721, "compression_ratio": 1.5916666666666666, "no_speech_prob": 4.495113898883574e-06}, {"id": 187, "seek": 81134, "start": 811.34, "end": 813.74, "text": " Or whatever blog base we used. I can't remember", "tokens": [1610, 2035, 6968, 3096, 321, 1143, 13, 286, 393, 380, 1604], "temperature": 0.0, "avg_logprob": -0.16760986842466205, "compression_ratio": 1.5869565217391304, "no_speech_prob": 3.041520130864228e-06}, {"id": 188, "seek": 81134, "start": 815.26, "end": 817.26, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.16760986842466205, "compression_ratio": 1.5869565217391304, "no_speech_prob": 3.041520130864228e-06}, {"id": 189, "seek": 81134, "start": 817.26, "end": 824.5400000000001, "text": " All that a decision tree cares about is the ordering of the data and this is another reason that decision tree based approaches are fantastic", "tokens": [1057, 300, 257, 3537, 4230, 12310, 466, 307, 264, 21739, 295, 264, 1412, 293, 341, 307, 1071, 1778, 300, 3537, 4230, 2361, 11587, 366, 5456], "temperature": 0.0, "avg_logprob": -0.16760986842466205, "compression_ratio": 1.5869565217391304, "no_speech_prob": 3.041520130864228e-06}, {"id": 190, "seek": 81134, "start": 825.34, "end": 831.0400000000001, "text": " Because they don't care at all about outliers, you know long tail distributions", "tokens": [1436, 436, 500, 380, 1127, 412, 439, 466, 484, 23646, 11, 291, 458, 938, 6838, 37870], "temperature": 0.0, "avg_logprob": -0.16760986842466205, "compression_ratio": 1.5869565217391304, "no_speech_prob": 3.041520130864228e-06}, {"id": 191, "seek": 81134, "start": 832.2, "end": 838.46, "text": " Categorical variables, whatever you can throw it all in and it'll do a perfectly fine job", "tokens": [383, 2968, 284, 804, 9102, 11, 2035, 291, 393, 3507, 309, 439, 294, 293, 309, 603, 360, 257, 6239, 2489, 1691], "temperature": 0.0, "avg_logprob": -0.16760986842466205, "compression_ratio": 1.5869565217391304, "no_speech_prob": 3.041520130864228e-06}, {"id": 192, "seek": 81134, "start": 839.58, "end": 840.62, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.16760986842466205, "compression_ratio": 1.5869565217391304, "no_speech_prob": 3.041520130864228e-06}, {"id": 193, "seek": 84062, "start": 840.62, "end": 842.0600000000001, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.09031493942458908, "compression_ratio": 1.4945054945054945, "no_speech_prob": 3.6118233310844516e-06}, {"id": 194, "seek": 84062, "start": 842.0600000000001, "end": 843.9, "text": " for tabular data", "tokens": [337, 4421, 1040, 1412], "temperature": 0.0, "avg_logprob": -0.09031493942458908, "compression_ratio": 1.4945054945054945, "no_speech_prob": 3.6118233310844516e-06}, {"id": 195, "seek": 84062, "start": 843.9, "end": 845.9, "text": " I would always start", "tokens": [286, 576, 1009, 722], "temperature": 0.0, "avg_logprob": -0.09031493942458908, "compression_ratio": 1.4945054945054945, "no_speech_prob": 3.6118233310844516e-06}, {"id": 196, "seek": 84062, "start": 845.98, "end": 848.14, "text": " By using a decision tree based approach", "tokens": [3146, 1228, 257, 3537, 4230, 2361, 3109], "temperature": 0.0, "avg_logprob": -0.09031493942458908, "compression_ratio": 1.4945054945054945, "no_speech_prob": 3.6118233310844516e-06}, {"id": 197, "seek": 84062, "start": 849.18, "end": 850.54, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.09031493942458908, "compression_ratio": 1.4945054945054945, "no_speech_prob": 3.6118233310844516e-06}, {"id": 198, "seek": 84062, "start": 850.54, "end": 855.02, "text": " And kind of create some baselines and so forth because it's it's really hard to mess it up", "tokens": [400, 733, 295, 1884, 512, 987, 9173, 293, 370, 5220, 570, 309, 311, 309, 311, 534, 1152, 281, 2082, 309, 493], "temperature": 0.0, "avg_logprob": -0.09031493942458908, "compression_ratio": 1.4945054945054945, "no_speech_prob": 3.6118233310844516e-06}, {"id": 199, "seek": 84062, "start": 857.18, "end": 859.18, "text": " And that's important", "tokens": [400, 300, 311, 1021], "temperature": 0.0, "avg_logprob": -0.09031493942458908, "compression_ratio": 1.4945054945054945, "no_speech_prob": 3.6118233310844516e-06}, {"id": 200, "seek": 84062, "start": 861.42, "end": 868.14, "text": " So yeah, so here for example is embarked right it it was coded originally as", "tokens": [407, 1338, 11, 370, 510, 337, 1365, 307, 29832, 292, 558, 309, 309, 390, 34874, 7993, 382], "temperature": 0.0, "avg_logprob": -0.09031493942458908, "compression_ratio": 1.4945054945054945, "no_speech_prob": 3.6118233310844516e-06}, {"id": 201, "seek": 86814, "start": 868.14, "end": 871.18, "text": " the first letter of the city they embarked in", "tokens": [264, 700, 5063, 295, 264, 2307, 436, 29832, 292, 294], "temperature": 0.0, "avg_logprob": -0.3015303521786096, "compression_ratio": 1.6422413793103448, "no_speech_prob": 1.2605952179001179e-05}, {"id": 202, "seek": 86814, "start": 872.14, "end": 879.66, "text": " But we turned it into a categorical variable and so pandas for us creates this this this vocab this list of all of the possible values", "tokens": [583, 321, 3574, 309, 666, 257, 19250, 804, 7006, 293, 370, 4565, 296, 337, 505, 7829, 341, 341, 341, 2329, 455, 341, 1329, 295, 439, 295, 264, 1944, 4190], "temperature": 0.0, "avg_logprob": -0.3015303521786096, "compression_ratio": 1.6422413793103448, "no_speech_prob": 1.2605952179001179e-05}, {"id": 203, "seek": 86814, "start": 880.46, "end": 882.46, "text": " and if you look at the", "tokens": [293, 498, 291, 574, 412, 264], "temperature": 0.0, "avg_logprob": -0.3015303521786096, "compression_ratio": 1.6422413793103448, "no_speech_prob": 1.2605952179001179e-05}, {"id": 204, "seek": 86814, "start": 882.54, "end": 883.9, "text": " codes", "tokens": [14211], "temperature": 0.0, "avg_logprob": -0.3015303521786096, "compression_ratio": 1.6422413793103448, "no_speech_prob": 1.2605952179001179e-05}, {"id": 205, "seek": 86814, "start": 883.9, "end": 888.62, "text": " Attribute you can see it's that s is the zero one two. So s has become two", "tokens": [7298, 2024, 1169, 291, 393, 536, 309, 311, 300, 262, 307, 264, 4018, 472, 732, 13, 407, 262, 575, 1813, 732], "temperature": 0.0, "avg_logprob": -0.3015303521786096, "compression_ratio": 1.6422413793103448, "no_speech_prob": 1.2605952179001179e-05}, {"id": 206, "seek": 86814, "start": 889.26, "end": 890.14, "text": " c", "tokens": [269], "temperature": 0.0, "avg_logprob": -0.3015303521786096, "compression_ratio": 1.6422413793103448, "no_speech_prob": 1.2605952179001179e-05}, {"id": 207, "seek": 86814, "start": 890.14, "end": 892.14, "text": " Has become zero", "tokens": [8646, 1813, 4018], "temperature": 0.0, "avg_logprob": -0.3015303521786096, "compression_ratio": 1.6422413793103448, "no_speech_prob": 1.2605952179001179e-05}, {"id": 208, "seek": 86814, "start": 892.14, "end": 896.9399999999999, "text": " And so forth, right? So that's how we're converting the categories the strings", "tokens": [400, 370, 5220, 11, 558, 30, 407, 300, 311, 577, 321, 434, 29942, 264, 10479, 264, 13985], "temperature": 0.0, "avg_logprob": -0.3015303521786096, "compression_ratio": 1.6422413793103448, "no_speech_prob": 1.2605952179001179e-05}, {"id": 209, "seek": 89694, "start": 896.94, "end": 901.5, "text": " The strings into numbers that we can sort and group by", "tokens": [440, 13985, 666, 3547, 300, 321, 393, 1333, 293, 1594, 538], "temperature": 0.0, "avg_logprob": -0.16393577220828034, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.2029226127197035e-05}, {"id": 210, "seek": 89694, "start": 904.62, "end": 905.82, "text": " Um, so yeah", "tokens": [3301, 11, 370, 1338], "temperature": 0.0, "avg_logprob": -0.16393577220828034, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.2029226127197035e-05}, {"id": 211, "seek": 89694, "start": 905.82, "end": 911.2600000000001, "text": " So if we wanted to split c into one group and q and s and the other we can just do okay less than a recorder", "tokens": [407, 498, 321, 1415, 281, 7472, 269, 666, 472, 1594, 293, 9505, 293, 262, 293, 264, 661, 321, 393, 445, 360, 1392, 1570, 813, 257, 37744], "temperature": 0.0, "avg_logprob": -0.16393577220828034, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.2029226127197035e-05}, {"id": 212, "seek": 89694, "start": 911.2600000000001, "end": 913.2600000000001, "text": " one point uh 0.5", "tokens": [472, 935, 2232, 1958, 13, 20], "temperature": 0.0, "avg_logprob": -0.16393577220828034, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.2029226127197035e-05}, {"id": 213, "seek": 89694, "start": 914.1400000000001, "end": 920.94, "text": " Now of course if we wanted to split c and s into one group and q into the other we would need two binary splits first c", "tokens": [823, 295, 1164, 498, 321, 1415, 281, 7472, 269, 293, 262, 666, 472, 1594, 293, 9505, 666, 264, 661, 321, 576, 643, 732, 17434, 37741, 700, 269], "temperature": 0.0, "avg_logprob": -0.16393577220828034, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.2029226127197035e-05}, {"id": 214, "seek": 92094, "start": 920.94, "end": 925.9000000000001, "text": " On one side and q s q and s on the other and then q and s into q versus s", "tokens": [1282, 472, 1252, 293, 9505, 262, 9505, 293, 262, 322, 264, 661, 293, 550, 9505, 293, 262, 666, 9505, 5717, 262], "temperature": 0.0, "avg_logprob": -0.2502673674966687, "compression_ratio": 1.657370517928287, "no_speech_prob": 1.7501723050372675e-05}, {"id": 215, "seek": 92094, "start": 925.98, "end": 928.94, "text": " And then the q and s leaf nodes could get similar", "tokens": [400, 550, 264, 9505, 293, 262, 10871, 13891, 727, 483, 2531], "temperature": 0.0, "avg_logprob": -0.2502673674966687, "compression_ratio": 1.657370517928287, "no_speech_prob": 1.7501723050372675e-05}, {"id": 216, "seek": 92094, "start": 929.5, "end": 932.5400000000001, "text": " Predictions so like you do have like sometimes it can take a little bit more", "tokens": [32969, 15607, 370, 411, 291, 360, 362, 411, 2171, 309, 393, 747, 257, 707, 857, 544], "temperature": 0.0, "avg_logprob": -0.2502673674966687, "compression_ratio": 1.657370517928287, "no_speech_prob": 1.7501723050372675e-05}, {"id": 217, "seek": 92094, "start": 933.74, "end": 935.74, "text": " messing around um, but", "tokens": [23258, 926, 1105, 11, 457], "temperature": 0.0, "avg_logprob": -0.2502673674966687, "compression_ratio": 1.657370517928287, "no_speech_prob": 1.7501723050372675e-05}, {"id": 218, "seek": 92094, "start": 936.46, "end": 941.0200000000001, "text": " Most of the time I find categorical variables work fine as numeric", "tokens": [4534, 295, 264, 565, 286, 915, 19250, 804, 9102, 589, 2489, 382, 7866, 299], "temperature": 0.0, "avg_logprob": -0.2502673674966687, "compression_ratio": 1.657370517928287, "no_speech_prob": 1.7501723050372675e-05}, {"id": 219, "seek": 92094, "start": 941.58, "end": 947.6600000000001, "text": " In decision tree based approaches and as I say here, I tend to use dummy variables only if there's like less than four levels", "tokens": [682, 3537, 4230, 2361, 11587, 293, 382, 286, 584, 510, 11, 286, 3928, 281, 764, 35064, 9102, 787, 498, 456, 311, 411, 1570, 813, 1451, 4358], "temperature": 0.0, "avg_logprob": -0.2502673674966687, "compression_ratio": 1.657370517928287, "no_speech_prob": 1.7501723050372675e-05}, {"id": 220, "seek": 94766, "start": 947.66, "end": 949.66, "text": " Less than four levels", "tokens": [18649, 813, 1451, 4358], "temperature": 0.0, "avg_logprob": -0.08997678756713867, "compression_ratio": 1.348993288590604, "no_speech_prob": 1.844788243943185e-06}, {"id": 221, "seek": 94766, "start": 953.1, "end": 958.86, "text": " Now what if we wanted to make this more accurate could we grow the tree further", "tokens": [823, 437, 498, 321, 1415, 281, 652, 341, 544, 8559, 727, 321, 1852, 264, 4230, 3052], "temperature": 0.0, "avg_logprob": -0.08997678756713867, "compression_ratio": 1.348993288590604, "no_speech_prob": 1.844788243943185e-06}, {"id": 222, "seek": 94766, "start": 960.54, "end": 962.54, "text": " I mean we could", "tokens": [286, 914, 321, 727], "temperature": 0.0, "avg_logprob": -0.08997678756713867, "compression_ratio": 1.348993288590604, "no_speech_prob": 1.844788243943185e-06}, {"id": 223, "seek": 94766, "start": 963.5799999999999, "end": 965.5799999999999, "text": " but", "tokens": [457], "temperature": 0.0, "avg_logprob": -0.08997678756713867, "compression_ratio": 1.348993288590604, "no_speech_prob": 1.844788243943185e-06}, {"id": 224, "seek": 96558, "start": 965.58, "end": 977.26, "text": " Um, you know, there's only 50 samples in these leaves right? It's it's not really um,", "tokens": [3301, 11, 291, 458, 11, 456, 311, 787, 2625, 10938, 294, 613, 5510, 558, 30, 467, 311, 309, 311, 406, 534, 1105, 11], "temperature": 0.0, "avg_logprob": -0.1348658611899928, "compression_ratio": 1.5392670157068062, "no_speech_prob": 5.093463187222369e-06}, {"id": 225, "seek": 96558, "start": 978.46, "end": 983.1800000000001, "text": " You know if I keep splitting it the leaf nodes are going to have so little data that that's not really going to make", "tokens": [509, 458, 498, 286, 1066, 30348, 309, 264, 10871, 13891, 366, 516, 281, 362, 370, 707, 1412, 300, 300, 311, 406, 534, 516, 281, 652], "temperature": 0.0, "avg_logprob": -0.1348658611899928, "compression_ratio": 1.5392670157068062, "no_speech_prob": 5.093463187222369e-06}, {"id": 226, "seek": 96558, "start": 983.58, "end": 985.4200000000001, "text": " Very useful predictions", "tokens": [4372, 4420, 21264], "temperature": 0.0, "avg_logprob": -0.1348658611899928, "compression_ratio": 1.5392670157068062, "no_speech_prob": 5.093463187222369e-06}, {"id": 227, "seek": 96558, "start": 985.4200000000001, "end": 990.22, "text": " Now there are limitations to how accurate a decision tree can be", "tokens": [823, 456, 366, 15705, 281, 577, 8559, 257, 3537, 4230, 393, 312], "temperature": 0.0, "avg_logprob": -0.1348658611899928, "compression_ratio": 1.5392670157068062, "no_speech_prob": 5.093463187222369e-06}, {"id": 228, "seek": 96558, "start": 991.58, "end": 993.58, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.1348658611899928, "compression_ratio": 1.5392670157068062, "no_speech_prob": 5.093463187222369e-06}, {"id": 229, "seek": 99358, "start": 993.58, "end": 995.58, "text": " So what can we do?", "tokens": [407, 437, 393, 321, 360, 30], "temperature": 0.0, "avg_logprob": -0.1472137669722239, "compression_ratio": 1.5706806282722514, "no_speech_prob": 1.0615652172418777e-05}, {"id": 230, "seek": 99358, "start": 996.7, "end": 998.7, "text": " We can do something that's actually very", "tokens": [492, 393, 360, 746, 300, 311, 767, 588], "temperature": 0.0, "avg_logprob": -0.1472137669722239, "compression_ratio": 1.5706806282722514, "no_speech_prob": 1.0615652172418777e-05}, {"id": 231, "seek": 99358, "start": 999.74, "end": 1001.74, "text": " I mean, I find it amazing and fascinating", "tokens": [286, 914, 11, 286, 915, 309, 2243, 293, 10343], "temperature": 0.0, "avg_logprob": -0.1472137669722239, "compression_ratio": 1.5706806282722514, "no_speech_prob": 1.0615652172418777e-05}, {"id": 232, "seek": 99358, "start": 1002.38, "end": 1003.6600000000001, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.1472137669722239, "compression_ratio": 1.5706806282722514, "no_speech_prob": 1.0615652172418777e-05}, {"id": 233, "seek": 99358, "start": 1003.6600000000001, "end": 1005.6600000000001, "text": " Comes from a guy called leo bryman", "tokens": [47290, 490, 257, 2146, 1219, 476, 78, 272, 627, 1601], "temperature": 0.0, "avg_logprob": -0.1472137669722239, "compression_ratio": 1.5706806282722514, "no_speech_prob": 1.0615652172418777e-05}, {"id": 234, "seek": 99358, "start": 1006.7800000000001, "end": 1010.46, "text": " Uh and leo bryman, um came with this came up with this idea", "tokens": [4019, 293, 476, 78, 272, 627, 1601, 11, 1105, 1361, 365, 341, 1361, 493, 365, 341, 1558], "temperature": 0.0, "avg_logprob": -0.1472137669722239, "compression_ratio": 1.5706806282722514, "no_speech_prob": 1.0615652172418777e-05}, {"id": 235, "seek": 99358, "start": 1011.34, "end": 1014.7800000000001, "text": " Called bagging and here's the basic idea of bagging", "tokens": [45001, 3411, 3249, 293, 510, 311, 264, 3875, 1558, 295, 3411, 3249], "temperature": 0.0, "avg_logprob": -0.1472137669722239, "compression_ratio": 1.5706806282722514, "no_speech_prob": 1.0615652172418777e-05}, {"id": 236, "seek": 99358, "start": 1015.82, "end": 1017.82, "text": " Let's say we've got a model", "tokens": [961, 311, 584, 321, 600, 658, 257, 2316], "temperature": 0.0, "avg_logprob": -0.1472137669722239, "compression_ratio": 1.5706806282722514, "no_speech_prob": 1.0615652172418777e-05}, {"id": 237, "seek": 99358, "start": 1018.7800000000001, "end": 1020.7800000000001, "text": " That's not very good", "tokens": [663, 311, 406, 588, 665], "temperature": 0.0, "avg_logprob": -0.1472137669722239, "compression_ratio": 1.5706806282722514, "no_speech_prob": 1.0615652172418777e-05}, {"id": 238, "seek": 102078, "start": 1020.78, "end": 1026.46, "text": " Um, because let's say it's a decision tree. It's really small. We've hardly used any data for it", "tokens": [3301, 11, 570, 718, 311, 584, 309, 311, 257, 3537, 4230, 13, 467, 311, 534, 1359, 13, 492, 600, 13572, 1143, 604, 1412, 337, 309], "temperature": 0.0, "avg_logprob": -0.09198945916217306, "compression_ratio": 1.8481012658227849, "no_speech_prob": 7.183082288975129e-06}, {"id": 239, "seek": 102078, "start": 1026.94, "end": 1031.02, "text": " Right. It's not very good. So it's got error. It's got errors on predictions", "tokens": [1779, 13, 467, 311, 406, 588, 665, 13, 407, 309, 311, 658, 6713, 13, 467, 311, 658, 13603, 322, 21264], "temperature": 0.0, "avg_logprob": -0.09198945916217306, "compression_ratio": 1.8481012658227849, "no_speech_prob": 7.183082288975129e-06}, {"id": 240, "seek": 102078, "start": 1031.98, "end": 1036.86, "text": " Um, it's not a systematically biased error. It's not always predicting too high always predicting too low", "tokens": [3301, 11, 309, 311, 406, 257, 39531, 28035, 6713, 13, 467, 311, 406, 1009, 32884, 886, 1090, 1009, 32884, 886, 2295], "temperature": 0.0, "avg_logprob": -0.09198945916217306, "compression_ratio": 1.8481012658227849, "no_speech_prob": 7.183082288975129e-06}, {"id": 241, "seek": 102078, "start": 1036.94, "end": 1041.1, "text": " I mean decision trees, you know on average will predict the average right?", "tokens": [286, 914, 3537, 5852, 11, 291, 458, 322, 4274, 486, 6069, 264, 4274, 558, 30], "temperature": 0.0, "avg_logprob": -0.09198945916217306, "compression_ratio": 1.8481012658227849, "no_speech_prob": 7.183082288975129e-06}, {"id": 242, "seek": 102078, "start": 1041.82, "end": 1043.82, "text": " Um, but it has errors", "tokens": [3301, 11, 457, 309, 575, 13603], "temperature": 0.0, "avg_logprob": -0.09198945916217306, "compression_ratio": 1.8481012658227849, "no_speech_prob": 7.183082288975129e-06}, {"id": 243, "seek": 102078, "start": 1044.22, "end": 1048.06, "text": " Um, so what I could do is I could build another decision tree", "tokens": [3301, 11, 370, 437, 286, 727, 360, 307, 286, 727, 1322, 1071, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.09198945916217306, "compression_ratio": 1.8481012658227849, "no_speech_prob": 7.183082288975129e-06}, {"id": 244, "seek": 104806, "start": 1048.06, "end": 1050.06, "text": " In some slightly different way", "tokens": [682, 512, 4748, 819, 636], "temperature": 0.0, "avg_logprob": -0.2949030558268229, "compression_ratio": 1.7376425855513309, "no_speech_prob": 9.368259270559065e-06}, {"id": 245, "seek": 104806, "start": 1050.3799999999999, "end": 1052.3799999999999, "text": " That would have different splits", "tokens": [663, 576, 362, 819, 37741], "temperature": 0.0, "avg_logprob": -0.2949030558268229, "compression_ratio": 1.7376425855513309, "no_speech_prob": 9.368259270559065e-06}, {"id": 246, "seek": 104806, "start": 1052.86, "end": 1055.34, "text": " And it would also be not a great model", "tokens": [400, 309, 576, 611, 312, 406, 257, 869, 2316], "temperature": 0.0, "avg_logprob": -0.2949030558268229, "compression_ratio": 1.7376425855513309, "no_speech_prob": 9.368259270559065e-06}, {"id": 247, "seek": 104806, "start": 1055.8999999999999, "end": 1057.1, "text": " but", "tokens": [457], "temperature": 0.0, "avg_logprob": -0.2949030558268229, "compression_ratio": 1.7376425855513309, "no_speech_prob": 9.368259270559065e-06}, {"id": 248, "seek": 104806, "start": 1057.1, "end": 1060.1399999999999, "text": " Predicts the correct thing on average. It's not completely hopeless", "tokens": [430, 24945, 82, 264, 3006, 551, 322, 4274, 13, 467, 311, 406, 2584, 27317], "temperature": 0.0, "avg_logprob": -0.2949030558268229, "compression_ratio": 1.7376425855513309, "no_speech_prob": 9.368259270559065e-06}, {"id": 249, "seek": 104806, "start": 1060.78, "end": 1064.46, "text": " Um, and again, you know, some of the errors are a bit too high and some are a bit too low", "tokens": [3301, 11, 293, 797, 11, 291, 458, 11, 512, 295, 264, 13603, 366, 257, 857, 886, 1090, 293, 512, 366, 257, 857, 886, 2295], "temperature": 0.0, "avg_logprob": -0.2949030558268229, "compression_ratio": 1.7376425855513309, "no_speech_prob": 9.368259270559065e-06}, {"id": 250, "seek": 104806, "start": 1065.26, "end": 1070.06, "text": " But I could keep doing this so if I could keep building lots and lots of slightly different decision trees", "tokens": [583, 286, 727, 1066, 884, 341, 370, 498, 286, 727, 1066, 2390, 3195, 293, 3195, 295, 4748, 819, 3537, 5852], "temperature": 0.0, "avg_logprob": -0.2949030558268229, "compression_ratio": 1.7376425855513309, "no_speech_prob": 9.368259270559065e-06}, {"id": 251, "seek": 104806, "start": 1071.1799999999998, "end": 1073.82, "text": " Um, I'm going to end up with say a hundred different", "tokens": [3301, 11, 286, 478, 516, 281, 917, 493, 365, 584, 257, 3262, 819], "temperature": 0.0, "avg_logprob": -0.2949030558268229, "compression_ratio": 1.7376425855513309, "no_speech_prob": 9.368259270559065e-06}, {"id": 252, "seek": 104806, "start": 1074.3799999999999, "end": 1076.78, "text": " Models all of which are unbiased", "tokens": [6583, 1625, 439, 295, 597, 366, 517, 5614, 1937], "temperature": 0.0, "avg_logprob": -0.2949030558268229, "compression_ratio": 1.7376425855513309, "no_speech_prob": 9.368259270559065e-06}, {"id": 253, "seek": 107678, "start": 1076.78, "end": 1082.06, "text": " All of which are better than nothing and all of which have some errors a bit high some a bit low, whatever", "tokens": [1057, 295, 597, 366, 1101, 813, 1825, 293, 439, 295, 597, 362, 512, 13603, 257, 857, 1090, 512, 257, 857, 2295, 11, 2035], "temperature": 0.0, "avg_logprob": -0.2862488332420889, "compression_ratio": 1.7124463519313304, "no_speech_prob": 2.0261059034965e-06}, {"id": 254, "seek": 107678, "start": 1083.1, "end": 1085.1, "text": " So what would happen if I averaged their predictions?", "tokens": [407, 437, 576, 1051, 498, 286, 18247, 2980, 641, 21264, 30], "temperature": 0.0, "avg_logprob": -0.2862488332420889, "compression_ratio": 1.7124463519313304, "no_speech_prob": 2.0261059034965e-06}, {"id": 255, "seek": 107678, "start": 1086.94, "end": 1091.02, "text": " Assuming that the models are not correlated with each other", "tokens": [6281, 24919, 300, 264, 5245, 366, 406, 38574, 365, 1184, 661], "temperature": 0.0, "avg_logprob": -0.2862488332420889, "compression_ratio": 1.7124463519313304, "no_speech_prob": 2.0261059034965e-06}, {"id": 256, "seek": 107678, "start": 1091.98, "end": 1094.7, "text": " Then you're going to end up with errors", "tokens": [1396, 291, 434, 516, 281, 917, 493, 365, 13603], "temperature": 0.0, "avg_logprob": -0.2862488332420889, "compression_ratio": 1.7124463519313304, "no_speech_prob": 2.0261059034965e-06}, {"id": 257, "seek": 107678, "start": 1095.26, "end": 1100.54, "text": " On either side of the correct prediction some are a bit high. Some are a bit low", "tokens": [1282, 2139, 1252, 295, 264, 3006, 17630, 512, 366, 257, 857, 1090, 13, 2188, 366, 257, 857, 2295], "temperature": 0.0, "avg_logprob": -0.2862488332420889, "compression_ratio": 1.7124463519313304, "no_speech_prob": 2.0261059034965e-06}, {"id": 258, "seek": 107678, "start": 1101.1, "end": 1103.1, "text": " There'll be this kind of distribution of errors", "tokens": [821, 603, 312, 341, 733, 295, 7316, 295, 13603], "temperature": 0.0, "avg_logprob": -0.2862488332420889, "compression_ratio": 1.7124463519313304, "no_speech_prob": 2.0261059034965e-06}, {"id": 259, "seek": 107678, "start": 1103.42, "end": 1105.42, "text": " Right and", "tokens": [1779, 293], "temperature": 0.0, "avg_logprob": -0.2862488332420889, "compression_ratio": 1.7124463519313304, "no_speech_prob": 2.0261059034965e-06}, {"id": 260, "seek": 110542, "start": 1105.42, "end": 1107.42, "text": " the average of those errors will be", "tokens": [264, 4274, 295, 729, 13603, 486, 312], "temperature": 0.0, "avg_logprob": -0.30150533276934954, "compression_ratio": 1.6839378238341969, "no_speech_prob": 3.3931023608602118e-06}, {"id": 261, "seek": 110542, "start": 1108.3000000000002, "end": 1110.3000000000002, "text": " zero", "tokens": [4018], "temperature": 0.0, "avg_logprob": -0.30150533276934954, "compression_ratio": 1.6839378238341969, "no_speech_prob": 3.3931023608602118e-06}, {"id": 262, "seek": 110542, "start": 1110.7, "end": 1114.54, "text": " And so that means the average of the predictions of these multiple", "tokens": [400, 370, 300, 1355, 264, 4274, 295, 264, 21264, 295, 613, 3866], "temperature": 0.0, "avg_logprob": -0.30150533276934954, "compression_ratio": 1.6839378238341969, "no_speech_prob": 3.3931023608602118e-06}, {"id": 263, "seek": 110542, "start": 1115.02, "end": 1118.94, "text": " uncorrelated models each of which is unbiased will be", "tokens": [6219, 284, 12004, 5245, 1184, 295, 597, 307, 517, 5614, 1937, 486, 312], "temperature": 0.0, "avg_logprob": -0.30150533276934954, "compression_ratio": 1.6839378238341969, "no_speech_prob": 3.3931023608602118e-06}, {"id": 264, "seek": 110542, "start": 1119.66, "end": 1125.18, "text": " The correct prediction because they have an error of zero and this is a mind-blowing insight", "tokens": [440, 3006, 17630, 570, 436, 362, 364, 6713, 295, 4018, 293, 341, 307, 257, 1575, 12, 43788, 11269], "temperature": 0.0, "avg_logprob": -0.30150533276934954, "compression_ratio": 1.6839378238341969, "no_speech_prob": 3.3931023608602118e-06}, {"id": 265, "seek": 110542, "start": 1126.0600000000002, "end": 1128.0600000000002, "text": " It says that if we can generate", "tokens": [467, 1619, 300, 498, 321, 393, 8460], "temperature": 0.0, "avg_logprob": -0.30150533276934954, "compression_ratio": 1.6839378238341969, "no_speech_prob": 3.3931023608602118e-06}, {"id": 266, "seek": 110542, "start": 1128.8600000000001, "end": 1130.8600000000001, "text": " a whole bunch of uncorrelated", "tokens": [257, 1379, 3840, 295, 6219, 284, 12004], "temperature": 0.0, "avg_logprob": -0.30150533276934954, "compression_ratio": 1.6839378238341969, "no_speech_prob": 3.3931023608602118e-06}, {"id": 267, "seek": 110542, "start": 1131.9, "end": 1133.9, "text": " unbiased", "tokens": [517, 5614, 1937], "temperature": 0.0, "avg_logprob": -0.30150533276934954, "compression_ratio": 1.6839378238341969, "no_speech_prob": 3.3931023608602118e-06}, {"id": 268, "seek": 113390, "start": 1133.9, "end": 1135.9, "text": " unbiased", "tokens": [517, 5614, 1937], "temperature": 0.0, "avg_logprob": -0.06395753706344451, "compression_ratio": 1.797029702970297, "no_speech_prob": 3.138088914056425e-06}, {"id": 269, "seek": 113390, "start": 1135.98, "end": 1137.74, "text": " models", "tokens": [5245], "temperature": 0.0, "avg_logprob": -0.06395753706344451, "compression_ratio": 1.797029702970297, "no_speech_prob": 3.138088914056425e-06}, {"id": 270, "seek": 113390, "start": 1137.74, "end": 1144.7, "text": " We can average them and get something better than any of the individual models because the average of the error", "tokens": [492, 393, 4274, 552, 293, 483, 746, 1101, 813, 604, 295, 264, 2609, 5245, 570, 264, 4274, 295, 264, 6713], "temperature": 0.0, "avg_logprob": -0.06395753706344451, "compression_ratio": 1.797029702970297, "no_speech_prob": 3.138088914056425e-06}, {"id": 271, "seek": 113390, "start": 1145.3400000000001, "end": 1147.18, "text": " Will be zero", "tokens": [3099, 312, 4018], "temperature": 0.0, "avg_logprob": -0.06395753706344451, "compression_ratio": 1.797029702970297, "no_speech_prob": 3.138088914056425e-06}, {"id": 272, "seek": 113390, "start": 1147.18, "end": 1148.8600000000001, "text": " So all we need", "tokens": [407, 439, 321, 643], "temperature": 0.0, "avg_logprob": -0.06395753706344451, "compression_ratio": 1.797029702970297, "no_speech_prob": 3.138088914056425e-06}, {"id": 273, "seek": 113390, "start": 1148.8600000000001, "end": 1150.8600000000001, "text": " Is a way to generate", "tokens": [1119, 257, 636, 281, 8460], "temperature": 0.0, "avg_logprob": -0.06395753706344451, "compression_ratio": 1.797029702970297, "no_speech_prob": 3.138088914056425e-06}, {"id": 274, "seek": 113390, "start": 1150.8600000000001, "end": 1152.8600000000001, "text": " lots of models", "tokens": [3195, 295, 5245], "temperature": 0.0, "avg_logprob": -0.06395753706344451, "compression_ratio": 1.797029702970297, "no_speech_prob": 3.138088914056425e-06}, {"id": 275, "seek": 113390, "start": 1152.8600000000001, "end": 1156.22, "text": " Well, we already have a great way to build models which is to create a decision tree", "tokens": [1042, 11, 321, 1217, 362, 257, 869, 636, 281, 1322, 5245, 597, 307, 281, 1884, 257, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.06395753706344451, "compression_ratio": 1.797029702970297, "no_speech_prob": 3.138088914056425e-06}, {"id": 276, "seek": 113390, "start": 1157.42, "end": 1159.42, "text": " How do we create lots of them?", "tokens": [1012, 360, 321, 1884, 3195, 295, 552, 30], "temperature": 0.0, "avg_logprob": -0.06395753706344451, "compression_ratio": 1.797029702970297, "no_speech_prob": 3.138088914056425e-06}, {"id": 277, "seek": 115942, "start": 1159.42, "end": 1164.46, "text": " How do we create lots of unbiased but different models? Well", "tokens": [1012, 360, 321, 1884, 3195, 295, 517, 5614, 1937, 457, 819, 5245, 30, 1042], "temperature": 0.0, "avg_logprob": -0.06724846990484941, "compression_ratio": 2.097826086956522, "no_speech_prob": 7.183119123510551e-06}, {"id": 278, "seek": 115942, "start": 1166.0600000000002, "end": 1168.94, "text": " Let's just grab a different subset of the data each time", "tokens": [961, 311, 445, 4444, 257, 819, 25993, 295, 264, 1412, 1184, 565], "temperature": 0.0, "avg_logprob": -0.06724846990484941, "compression_ratio": 2.097826086956522, "no_speech_prob": 7.183119123510551e-06}, {"id": 279, "seek": 115942, "start": 1169.5, "end": 1172.38, "text": " Let's just grab at random half the rows", "tokens": [961, 311, 445, 4444, 412, 4974, 1922, 264, 13241], "temperature": 0.0, "avg_logprob": -0.06724846990484941, "compression_ratio": 2.097826086956522, "no_speech_prob": 7.183119123510551e-06}, {"id": 280, "seek": 115942, "start": 1173.26, "end": 1177.9, "text": " And build a decision tree and then grab another half the rows and build a decision tree", "tokens": [400, 1322, 257, 3537, 4230, 293, 550, 4444, 1071, 1922, 264, 13241, 293, 1322, 257, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.06724846990484941, "compression_ratio": 2.097826086956522, "no_speech_prob": 7.183119123510551e-06}, {"id": 281, "seek": 115942, "start": 1178.6200000000001, "end": 1182.94, "text": " And grab another half the rows and build a decision tree each of those decision trees is going to be not great", "tokens": [400, 4444, 1071, 1922, 264, 13241, 293, 1322, 257, 3537, 4230, 1184, 295, 729, 3537, 5852, 307, 516, 281, 312, 406, 869], "temperature": 0.0, "avg_logprob": -0.06724846990484941, "compression_ratio": 2.097826086956522, "no_speech_prob": 7.183119123510551e-06}, {"id": 282, "seek": 115942, "start": 1183.18, "end": 1185.18, "text": " It's only using half the data", "tokens": [467, 311, 787, 1228, 1922, 264, 1412], "temperature": 0.0, "avg_logprob": -0.06724846990484941, "compression_ratio": 2.097826086956522, "no_speech_prob": 7.183119123510551e-06}, {"id": 283, "seek": 118518, "start": 1185.18, "end": 1191.3400000000001, "text": " But it will be unbiased. It will be predicting the average on average. It will certainly be better than nothing because it's using", "tokens": [583, 309, 486, 312, 517, 5614, 1937, 13, 467, 486, 312, 32884, 264, 4274, 322, 4274, 13, 467, 486, 3297, 312, 1101, 813, 1825, 570, 309, 311, 1228], "temperature": 0.0, "avg_logprob": -0.06973789252486884, "compression_ratio": 1.6974789915966386, "no_speech_prob": 5.0936496336362325e-06}, {"id": 284, "seek": 118518, "start": 1191.74, "end": 1194.38, "text": " You know some real data to try and create a real decision tree", "tokens": [509, 458, 512, 957, 1412, 281, 853, 293, 1884, 257, 957, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.06973789252486884, "compression_ratio": 1.6974789915966386, "no_speech_prob": 5.0936496336362325e-06}, {"id": 285, "seek": 118518, "start": 1195.02, "end": 1196.14, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.06973789252486884, "compression_ratio": 1.6974789915966386, "no_speech_prob": 5.0936496336362325e-06}, {"id": 286, "seek": 118518, "start": 1196.14, "end": 1200.14, "text": " The they won't be correlated with each other because they're each random subsets", "tokens": [440, 436, 1582, 380, 312, 38574, 365, 1184, 661, 570, 436, 434, 1184, 4974, 2090, 1385], "temperature": 0.0, "avg_logprob": -0.06973789252486884, "compression_ratio": 1.6974789915966386, "no_speech_prob": 5.0936496336362325e-06}, {"id": 287, "seek": 118518, "start": 1200.8600000000001, "end": 1202.8600000000001, "text": " So that meets all of our criteria", "tokens": [407, 300, 13961, 439, 295, 527, 11101], "temperature": 0.0, "avg_logprob": -0.06973789252486884, "compression_ratio": 1.6974789915966386, "no_speech_prob": 5.0936496336362325e-06}, {"id": 288, "seek": 118518, "start": 1203.5800000000002, "end": 1205.5, "text": " for bagging", "tokens": [337, 3411, 3249], "temperature": 0.0, "avg_logprob": -0.06973789252486884, "compression_ratio": 1.6974789915966386, "no_speech_prob": 5.0936496336362325e-06}, {"id": 289, "seek": 118518, "start": 1205.5, "end": 1209.02, "text": " When you do this you create something called a random forest", "tokens": [1133, 291, 360, 341, 291, 1884, 746, 1219, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.06973789252486884, "compression_ratio": 1.6974789915966386, "no_speech_prob": 5.0936496336362325e-06}, {"id": 290, "seek": 118518, "start": 1210.8600000000001, "end": 1212.8600000000001, "text": " So let's create one", "tokens": [407, 718, 311, 1884, 472], "temperature": 0.0, "avg_logprob": -0.06973789252486884, "compression_ratio": 1.6974789915966386, "no_speech_prob": 5.0936496336362325e-06}, {"id": 291, "seek": 121286, "start": 1212.86, "end": 1214.86, "text": " In four lines of code", "tokens": [682, 1451, 3876, 295, 3089], "temperature": 0.0, "avg_logprob": -0.3351576471903238, "compression_ratio": 1.5935828877005347, "no_speech_prob": 9.222881089954171e-06}, {"id": 292, "seek": 121286, "start": 1215.58, "end": 1217.58, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.3351576471903238, "compression_ratio": 1.5935828877005347, "no_speech_prob": 9.222881089954171e-06}, {"id": 293, "seek": 121286, "start": 1217.58, "end": 1220.3799999999999, "text": " Here is a function to create a decision tree", "tokens": [1692, 307, 257, 2445, 281, 1884, 257, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.3351576471903238, "compression_ratio": 1.5935828877005347, "no_speech_prob": 9.222881089954171e-06}, {"id": 294, "seek": 121286, "start": 1220.3799999999999, "end": 1222.9399999999998, "text": " So let's say what this is just the proportion of data", "tokens": [407, 718, 311, 584, 437, 341, 307, 445, 264, 16068, 295, 1412], "temperature": 0.0, "avg_logprob": -0.3351576471903238, "compression_ratio": 1.5935828877005347, "no_speech_prob": 9.222881089954171e-06}, {"id": 295, "seek": 121286, "start": 1222.9399999999998, "end": 1228.86, "text": " So let's say we put 75 of the data in each time or we could change it to 50 whatever", "tokens": [407, 718, 311, 584, 321, 829, 9562, 295, 264, 1412, 294, 1184, 565, 420, 321, 727, 1319, 309, 281, 2625, 2035], "temperature": 0.0, "avg_logprob": -0.3351576471903238, "compression_ratio": 1.5935828877005347, "no_speech_prob": 9.222881089954171e-06}, {"id": 296, "seek": 121286, "start": 1230.3799999999999, "end": 1237.8999999999999, "text": " So this is the number of samples in this subset i call it n and so let's at random choose", "tokens": [407, 341, 307, 264, 1230, 295, 10938, 294, 341, 25993, 741, 818, 309, 297, 293, 370, 718, 311, 412, 4974, 2826], "temperature": 0.0, "avg_logprob": -0.3351576471903238, "compression_ratio": 1.5935828877005347, "no_speech_prob": 9.222881089954171e-06}, {"id": 297, "seek": 123790, "start": 1237.9, "end": 1241.1000000000001, "text": " Um n times the proportion we requested", "tokens": [3301, 297, 1413, 264, 16068, 321, 16436], "temperature": 0.0, "avg_logprob": -0.3174765381631972, "compression_ratio": 1.6770833333333333, "no_speech_prob": 9.36850301513914e-06}, {"id": 298, "seek": 123790, "start": 1241.8200000000002, "end": 1245.1000000000001, "text": " From the sample and build a decision tree from that", "tokens": [3358, 264, 6889, 293, 1322, 257, 3537, 4230, 490, 300], "temperature": 0.0, "avg_logprob": -0.3174765381631972, "compression_ratio": 1.6770833333333333, "no_speech_prob": 9.36850301513914e-06}, {"id": 299, "seek": 123790, "start": 1246.6200000000001, "end": 1249.5, "text": " And so now let's 100 times", "tokens": [400, 370, 586, 718, 311, 2319, 1413], "temperature": 0.0, "avg_logprob": -0.3174765381631972, "compression_ratio": 1.6770833333333333, "no_speech_prob": 9.36850301513914e-06}, {"id": 300, "seek": 123790, "start": 1250.6200000000001, "end": 1255.1000000000001, "text": " Get a tree and stick them all in a list using a list comprehension", "tokens": [3240, 257, 4230, 293, 2897, 552, 439, 294, 257, 1329, 1228, 257, 1329, 44991], "temperature": 0.0, "avg_logprob": -0.3174765381631972, "compression_ratio": 1.6770833333333333, "no_speech_prob": 9.36850301513914e-06}, {"id": 301, "seek": 123790, "start": 1257.42, "end": 1261.42, "text": " And now let's grab the predictions for each one of those trees", "tokens": [400, 586, 718, 311, 4444, 264, 21264, 337, 1184, 472, 295, 729, 5852], "temperature": 0.0, "avg_logprob": -0.3174765381631972, "compression_ratio": 1.6770833333333333, "no_speech_prob": 9.36850301513914e-06}, {"id": 302, "seek": 123790, "start": 1262.7, "end": 1265.66, "text": " And then let's stack all those predictions up together and take their mean", "tokens": [400, 550, 718, 311, 8630, 439, 729, 21264, 493, 1214, 293, 747, 641, 914], "temperature": 0.0, "avg_logprob": -0.3174765381631972, "compression_ratio": 1.6770833333333333, "no_speech_prob": 9.36850301513914e-06}, {"id": 303, "seek": 126566, "start": 1265.66, "end": 1268.14, "text": " And that is a random forest", "tokens": [400, 300, 307, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.4069792778937371, "compression_ratio": 1.6525821596244132, "no_speech_prob": 3.1380918699142057e-06}, {"id": 304, "seek": 126566, "start": 1269.74, "end": 1276.3000000000002, "text": " And what do we get one two, three, four, five, six, seven eight, that's seven lines of code so", "tokens": [400, 437, 360, 321, 483, 472, 732, 11, 1045, 11, 1451, 11, 1732, 11, 2309, 11, 3407, 3180, 11, 300, 311, 3407, 3876, 295, 3089, 370], "temperature": 0.0, "avg_logprob": -0.4069792778937371, "compression_ratio": 1.6525821596244132, "no_speech_prob": 3.1380918699142057e-06}, {"id": 305, "seek": 126566, "start": 1277.1000000000001, "end": 1279.1000000000001, "text": " Random forests are very", "tokens": [37603, 21700, 366, 588], "temperature": 0.0, "avg_logprob": -0.4069792778937371, "compression_ratio": 1.6525821596244132, "no_speech_prob": 3.1380918699142057e-06}, {"id": 306, "seek": 126566, "start": 1279.5800000000002, "end": 1281.1000000000001, "text": " simple", "tokens": [2199], "temperature": 0.0, "avg_logprob": -0.4069792778937371, "compression_ratio": 1.6525821596244132, "no_speech_prob": 3.1380918699142057e-06}, {"id": 307, "seek": 126566, "start": 1281.1000000000001, "end": 1285.66, "text": " This is a slight simplification. There's one other difference that random forests do", "tokens": [639, 307, 257, 4036, 6883, 3774, 13, 821, 311, 472, 661, 2649, 300, 4974, 21700, 360], "temperature": 0.0, "avg_logprob": -0.4069792778937371, "compression_ratio": 1.6525821596244132, "no_speech_prob": 3.1380918699142057e-06}, {"id": 308, "seek": 126566, "start": 1286.3000000000002, "end": 1288.3000000000002, "text": " Which is when they build the decision tree", "tokens": [3013, 307, 562, 436, 1322, 264, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.4069792778937371, "compression_ratio": 1.6525821596244132, "no_speech_prob": 3.1380918699142057e-06}, {"id": 309, "seek": 126566, "start": 1288.6200000000001, "end": 1292.78, "text": " They also randomly select a subset of the data that they want to build", "tokens": [814, 611, 16979, 3048, 257, 25993, 295, 264, 1412, 300, 436, 528, 281, 1322], "temperature": 0.0, "avg_logprob": -0.4069792778937371, "compression_ratio": 1.6525821596244132, "no_speech_prob": 3.1380918699142057e-06}, {"id": 310, "seek": 129278, "start": 1292.78, "end": 1296.22, "text": " And they also randomly select a subset of columns", "tokens": [400, 436, 611, 16979, 3048, 257, 25993, 295, 13766], "temperature": 0.0, "avg_logprob": -0.13787091862071643, "compression_ratio": 1.6646706586826348, "no_speech_prob": 1.028769383992767e-05}, {"id": 311, "seek": 129278, "start": 1297.1, "end": 1301.58, "text": " And they select a different random subset of columns each time they do a split", "tokens": [400, 436, 3048, 257, 819, 4974, 25993, 295, 13766, 1184, 565, 436, 360, 257, 7472], "temperature": 0.0, "avg_logprob": -0.13787091862071643, "compression_ratio": 1.6646706586826348, "no_speech_prob": 1.028769383992767e-05}, {"id": 312, "seek": 129278, "start": 1302.54, "end": 1307.68, "text": " And so the idea is you kind of want it to be as random as possible, but also somewhat useful", "tokens": [400, 370, 264, 1558, 307, 291, 733, 295, 528, 309, 281, 312, 382, 4974, 382, 1944, 11, 457, 611, 8344, 4420], "temperature": 0.0, "avg_logprob": -0.13787091862071643, "compression_ratio": 1.6646706586826348, "no_speech_prob": 1.028769383992767e-05}, {"id": 313, "seek": 130768, "start": 1307.68, "end": 1311.1200000000001, "text": " So we can do that by creating a random forest classifier", "tokens": [407, 321, 393, 360, 300, 538, 4084, 257, 4974, 6719, 1508, 9902], "temperature": 0.0, "avg_logprob": -0.6281639979435847, "compression_ratio": 1.6927710843373494, "no_speech_prob": 2.857119170585065e-06}, {"id": 314, "seek": 130768, "start": 1311.8400000000001, "end": 1313.8400000000001, "text": " Say how many trees do we want?", "tokens": [6463, 577, 867, 5852, 360, 321, 528, 30], "temperature": 0.0, "avg_logprob": -0.6281639979435847, "compression_ratio": 1.6927710843373494, "no_speech_prob": 2.857119170585065e-06}, {"id": 315, "seek": 130768, "start": 1314.3200000000002, "end": 1316.3200000000002, "text": " How many samples per leaf?", "tokens": [1012, 867, 10938, 680, 10871, 30], "temperature": 0.0, "avg_logprob": -0.6281639979435847, "compression_ratio": 1.6927710843373494, "no_speech_prob": 2.857119170585065e-06}, {"id": 316, "seek": 130768, "start": 1317.04, "end": 1319.04, "text": " And then fit does what we just did", "tokens": [400, 550, 3318, 775, 437, 321, 445, 630], "temperature": 0.0, "avg_logprob": -0.6281639979435847, "compression_ratio": 1.6927710843373494, "no_speech_prob": 2.857119170585065e-06}, {"id": 317, "seek": 130768, "start": 1319.6000000000001, "end": 1322.16, "text": " And here's our mean absolute error rich", "tokens": [400, 510, 311, 527, 914, 8236, 6713, 4593], "temperature": 0.0, "avg_logprob": -0.6281639979435847, "compression_ratio": 1.6927710843373494, "no_speech_prob": 2.857119170585065e-06}, {"id": 318, "seek": 130768, "start": 1325.44, "end": 1331.6000000000001, "text": " Again, it's like you can see here. So we can do that by creating a random forest classifier", "tokens": [3764, 11, 309, 311, 411, 291, 393, 536, 510, 13, 407, 321, 393, 360, 300, 538, 4084, 257, 4974, 6719, 1508, 9902], "temperature": 0.0, "avg_logprob": -0.6281639979435847, "compression_ratio": 1.6927710843373494, "no_speech_prob": 2.857119170585065e-06}, {"id": 319, "seek": 133160, "start": 1331.6, "end": 1337.1999999999998, "text": " Mean absolute error rich again", "tokens": [12302, 8236, 6713, 4593, 797], "temperature": 0.0, "avg_logprob": -0.08971787373954003, "compression_ratio": 1.6885245901639345, "no_speech_prob": 6.4388395912828855e-06}, {"id": 320, "seek": 133160, "start": 1337.1999999999998, "end": 1341.6, "text": " It's like not as good as our decision tree, but it's still pretty good. And again, it's such a small data set", "tokens": [467, 311, 411, 406, 382, 665, 382, 527, 3537, 4230, 11, 457, 309, 311, 920, 1238, 665, 13, 400, 797, 11, 309, 311, 1270, 257, 1359, 1412, 992], "temperature": 0.0, "avg_logprob": -0.08971787373954003, "compression_ratio": 1.6885245901639345, "no_speech_prob": 6.4388395912828855e-06}, {"id": 321, "seek": 133160, "start": 1341.6, "end": 1343.6, "text": " It's hard to tell if that means anything", "tokens": [467, 311, 1152, 281, 980, 498, 300, 1355, 1340], "temperature": 0.0, "avg_logprob": -0.08971787373954003, "compression_ratio": 1.6885245901639345, "no_speech_prob": 6.4388395912828855e-06}, {"id": 322, "seek": 133160, "start": 1344.08, "end": 1348.08, "text": " And so we can submit that to kaggle. So earlier on I created a little function to submit to kaggle", "tokens": [400, 370, 321, 393, 10315, 300, 281, 350, 559, 22631, 13, 407, 3071, 322, 286, 2942, 257, 707, 2445, 281, 10315, 281, 350, 559, 22631], "temperature": 0.0, "avg_logprob": -0.08971787373954003, "compression_ratio": 1.6885245901639345, "no_speech_prob": 6.4388395912828855e-06}, {"id": 323, "seek": 133160, "start": 1348.08, "end": 1351.12, "text": " So now I just create some predictions and I submit to kaggle", "tokens": [407, 586, 286, 445, 1884, 512, 21264, 293, 286, 10315, 281, 350, 559, 22631], "temperature": 0.0, "avg_logprob": -0.08971787373954003, "compression_ratio": 1.6885245901639345, "no_speech_prob": 6.4388395912828855e-06}, {"id": 324, "seek": 133160, "start": 1351.84, "end": 1355.28, "text": " And yeah, looks like it gave nearly identical results to a single tree", "tokens": [400, 1338, 11, 1542, 411, 309, 2729, 6217, 14800, 3542, 281, 257, 2167, 4230], "temperature": 0.0, "avg_logprob": -0.08971787373954003, "compression_ratio": 1.6885245901639345, "no_speech_prob": 6.4388395912828855e-06}, {"id": 325, "seek": 135528, "start": 1355.28, "end": 1360.0, "text": " So now to one of my favorite things", "tokens": [407, 586, 281, 472, 295, 452, 2954, 721], "temperature": 0.0, "avg_logprob": -0.2699265894682511, "compression_ratio": 1.7264573991031391, "no_speech_prob": 5.255094492895296e-06}, {"id": 326, "seek": 135528, "start": 1360.8, "end": 1366.8799999999999, "text": " About random forests and I should say in most real world data sets of reasonable size random forests", "tokens": [7769, 4974, 21700, 293, 286, 820, 584, 294, 881, 957, 1002, 1412, 6352, 295, 10585, 2744, 4974, 21700], "temperature": 0.0, "avg_logprob": -0.2699265894682511, "compression_ratio": 1.7264573991031391, "no_speech_prob": 5.255094492895296e-06}, {"id": 327, "seek": 135528, "start": 1367.44, "end": 1372.3999999999999, "text": " Basically always give you much better results than decision trees. This is just a small data set to", "tokens": [8537, 1009, 976, 291, 709, 1101, 3542, 813, 3537, 5852, 13, 639, 307, 445, 257, 1359, 1412, 992, 281], "temperature": 0.0, "avg_logprob": -0.2699265894682511, "compression_ratio": 1.7264573991031391, "no_speech_prob": 5.255094492895296e-06}, {"id": 328, "seek": 135528, "start": 1373.12, "end": 1375.12, "text": " Show you what to do", "tokens": [6895, 291, 437, 281, 360], "temperature": 0.0, "avg_logprob": -0.2699265894682511, "compression_ratio": 1.7264573991031391, "no_speech_prob": 5.255094492895296e-06}, {"id": 329, "seek": 135528, "start": 1375.12, "end": 1377.12, "text": " One of my favorite things about", "tokens": [1485, 295, 452, 2954, 721, 466], "temperature": 0.0, "avg_logprob": -0.2699265894682511, "compression_ratio": 1.7264573991031391, "no_speech_prob": 5.255094492895296e-06}, {"id": 330, "seek": 135528, "start": 1377.68, "end": 1382.6399999999999, "text": " Random forests is we can do something quite cool with them. What we can do is we can look at the", "tokens": [37603, 21700, 307, 321, 393, 360, 746, 1596, 1627, 365, 552, 13, 708, 321, 393, 360, 307, 321, 393, 574, 412, 264], "temperature": 0.0, "avg_logprob": -0.2699265894682511, "compression_ratio": 1.7264573991031391, "no_speech_prob": 5.255094492895296e-06}, {"id": 331, "seek": 138264, "start": 1382.64, "end": 1386.72, "text": " underlying decision trees they create so we've now got 100 decision trees", "tokens": [14217, 3537, 5852, 436, 1884, 370, 321, 600, 586, 658, 2319, 3537, 5852], "temperature": 0.0, "avg_logprob": -0.3363236050273097, "compression_ratio": 1.525, "no_speech_prob": 9.515985766483936e-06}, {"id": 332, "seek": 138264, "start": 1387.92, "end": 1390.4, "text": " And we can see what columns", "tokens": [400, 321, 393, 536, 437, 13766], "temperature": 0.0, "avg_logprob": -0.3363236050273097, "compression_ratio": 1.525, "no_speech_prob": 9.515985766483936e-06}, {"id": 333, "seek": 138264, "start": 1391.2, "end": 1395.92, "text": " Did it find a split on and so it's a here. Okay. Well the first thing it split on was six", "tokens": [2589, 309, 915, 257, 7472, 322, 293, 370, 309, 311, 257, 510, 13, 1033, 13, 1042, 264, 700, 551, 309, 7472, 322, 390, 2309], "temperature": 0.0, "avg_logprob": -0.3363236050273097, "compression_ratio": 1.525, "no_speech_prob": 9.515985766483936e-06}, {"id": 334, "seek": 138264, "start": 1396.88, "end": 1399.5200000000002, "text": " and it improved the gini from", "tokens": [293, 309, 9689, 264, 290, 3812, 490], "temperature": 0.0, "avg_logprob": -0.3363236050273097, "compression_ratio": 1.525, "no_speech_prob": 9.515985766483936e-06}, {"id": 335, "seek": 138264, "start": 1401.1200000000001, "end": 1402.64, "text": " 0.47", "tokens": [1958, 13, 14060], "temperature": 0.0, "avg_logprob": -0.3363236050273097, "compression_ratio": 1.525, "no_speech_prob": 9.515985766483936e-06}, {"id": 336, "seek": 138264, "start": 1402.64, "end": 1408.16, "text": " To now just take the weighted average of 0.38 and 0.31 weighted by the samples", "tokens": [1407, 586, 445, 747, 264, 32807, 4274, 295, 1958, 13, 12625, 293, 1958, 13, 12967, 32807, 538, 264, 10938], "temperature": 0.0, "avg_logprob": -0.3363236050273097, "compression_ratio": 1.525, "no_speech_prob": 9.515985766483936e-06}, {"id": 337, "seek": 140816, "start": 1408.16, "end": 1416.16, "text": " So that's probably going to be about 0.33. So I would say okay. It's like 0.14 improvement in gini. Thanks to sex", "tokens": [407, 300, 311, 1391, 516, 281, 312, 466, 1958, 13, 10191, 13, 407, 286, 576, 584, 1392, 13, 467, 311, 411, 1958, 13, 7271, 10444, 294, 290, 3812, 13, 2561, 281, 3260], "temperature": 0.0, "avg_logprob": -0.252029278956422, "compression_ratio": 1.5974025974025974, "no_speech_prob": 1.1125170203740709e-05}, {"id": 338, "seek": 140816, "start": 1417.52, "end": 1422.8000000000002, "text": " And we can do that again. Okay. Well within p-class, you know, how much did that improve gini?", "tokens": [400, 321, 393, 360, 300, 797, 13, 1033, 13, 1042, 1951, 280, 12, 11665, 11, 291, 458, 11, 577, 709, 630, 300, 3470, 290, 3812, 30], "temperature": 0.0, "avg_logprob": -0.252029278956422, "compression_ratio": 1.5974025974025974, "no_speech_prob": 1.1125170203740709e-05}, {"id": 339, "seek": 140816, "start": 1423.3600000000001, "end": 1429.6000000000001, "text": " Again, we keep weighting it by the number of samples as well log fair. How much did that improve gini and we can keep track", "tokens": [3764, 11, 321, 1066, 3364, 278, 309, 538, 264, 1230, 295, 10938, 382, 731, 3565, 3143, 13, 1012, 709, 630, 300, 3470, 290, 3812, 293, 321, 393, 1066, 2837], "temperature": 0.0, "avg_logprob": -0.252029278956422, "compression_ratio": 1.5974025974025974, "no_speech_prob": 1.1125170203740709e-05}, {"id": 340, "seek": 140816, "start": 1430.3200000000002, "end": 1432.3200000000002, "text": " for each column of", "tokens": [337, 1184, 7738, 295], "temperature": 0.0, "avg_logprob": -0.252029278956422, "compression_ratio": 1.5974025974025974, "no_speech_prob": 1.1125170203740709e-05}, {"id": 341, "seek": 140816, "start": 1433.8400000000001, "end": 1435.8400000000001, "text": " How much in gini?", "tokens": [1012, 709, 294, 290, 3812, 30], "temperature": 0.0, "avg_logprob": -0.252029278956422, "compression_ratio": 1.5974025974025974, "no_speech_prob": 1.1125170203740709e-05}, {"id": 342, "seek": 143584, "start": 1435.84, "end": 1441.6, "text": " of how much in total did they improve the gini in this decision tree and", "tokens": [295, 577, 709, 294, 3217, 630, 436, 3470, 264, 290, 3812, 294, 341, 3537, 4230, 293], "temperature": 0.0, "avg_logprob": -0.19022077672621784, "compression_ratio": 1.8461538461538463, "no_speech_prob": 4.637766778614605e-06}, {"id": 343, "seek": 143584, "start": 1442.24, "end": 1444.24, "text": " then do that for every decision tree and", "tokens": [550, 360, 300, 337, 633, 3537, 4230, 293], "temperature": 0.0, "avg_logprob": -0.19022077672621784, "compression_ratio": 1.8461538461538463, "no_speech_prob": 4.637766778614605e-06}, {"id": 344, "seek": 143584, "start": 1445.28, "end": 1451.76, "text": " then add them up per column and that gives you something called a feature importance plot and", "tokens": [550, 909, 552, 493, 680, 7738, 293, 300, 2709, 291, 746, 1219, 257, 4111, 7379, 7542, 293], "temperature": 0.0, "avg_logprob": -0.19022077672621784, "compression_ratio": 1.8461538461538463, "no_speech_prob": 4.637766778614605e-06}, {"id": 345, "seek": 143584, "start": 1452.9599999999998, "end": 1454.9599999999998, "text": " Here it is", "tokens": [1692, 309, 307], "temperature": 0.0, "avg_logprob": -0.19022077672621784, "compression_ratio": 1.8461538461538463, "no_speech_prob": 4.637766778614605e-06}, {"id": 346, "seek": 143584, "start": 1455.6799999999998, "end": 1460.32, "text": " And a feature importance plot tells you how important is each feature", "tokens": [400, 257, 4111, 7379, 7542, 5112, 291, 577, 1021, 307, 1184, 4111], "temperature": 0.0, "avg_logprob": -0.19022077672621784, "compression_ratio": 1.8461538461538463, "no_speech_prob": 4.637766778614605e-06}, {"id": 347, "seek": 146032, "start": 1460.32, "end": 1465.3999999999999, "text": " How often did the trees picket and how much did it improve the gini when it did?", "tokens": [1012, 2049, 630, 264, 5852, 1888, 302, 293, 577, 709, 630, 309, 3470, 264, 290, 3812, 562, 309, 630, 30], "temperature": 0.0, "avg_logprob": -0.31297192429051257, "compression_ratio": 1.6774193548387097, "no_speech_prob": 9.368292012368329e-06}, {"id": 348, "seek": 146032, "start": 1465.8799999999999, "end": 1468.9199999999998, "text": " And so we can see from the feature importance plot that sex", "tokens": [400, 370, 321, 393, 536, 490, 264, 4111, 7379, 7542, 300, 3260], "temperature": 0.0, "avg_logprob": -0.31297192429051257, "compression_ratio": 1.6774193548387097, "no_speech_prob": 9.368292012368329e-06}, {"id": 349, "seek": 146032, "start": 1469.6799999999998, "end": 1471.6799999999998, "text": " was the most important and", "tokens": [390, 264, 881, 1021, 293], "temperature": 0.0, "avg_logprob": -0.31297192429051257, "compression_ratio": 1.6774193548387097, "no_speech_prob": 9.368292012368329e-06}, {"id": 350, "seek": 146032, "start": 1473.4399999999998, "end": 1477.6799999999998, "text": " Class was the second most important and everything else was a long way back", "tokens": [9471, 390, 264, 1150, 881, 1021, 293, 1203, 1646, 390, 257, 938, 636, 646], "temperature": 0.0, "avg_logprob": -0.31297192429051257, "compression_ratio": 1.6774193548387097, "no_speech_prob": 9.368292012368329e-06}, {"id": 351, "seek": 146032, "start": 1478.72, "end": 1483.52, "text": " And this is another reason by the way why our random forest isn't really particularly helpful because", "tokens": [400, 341, 307, 1071, 1778, 538, 264, 636, 983, 527, 4974, 6719, 1943, 380, 534, 4098, 4961, 570], "temperature": 0.0, "avg_logprob": -0.31297192429051257, "compression_ratio": 1.6774193548387097, "no_speech_prob": 9.368292012368329e-06}, {"id": 352, "seek": 146032, "start": 1484.1599999999999, "end": 1488.24, "text": " It's just such a easy split to do right? Basically all that matters is", "tokens": [467, 311, 445, 1270, 257, 1858, 7472, 281, 360, 558, 30, 8537, 439, 300, 7001, 307], "temperature": 0.0, "avg_logprob": -0.31297192429051257, "compression_ratio": 1.6774193548387097, "no_speech_prob": 9.368292012368329e-06}, {"id": 353, "seek": 148824, "start": 1488.24, "end": 1491.68, "text": " You know what class you're in and whether you're male and female", "tokens": [509, 458, 437, 1508, 291, 434, 294, 293, 1968, 291, 434, 7133, 293, 6556], "temperature": 0.0, "avg_logprob": -0.31950624305081654, "compression_ratio": 1.651376146788991, "no_speech_prob": 3.611848569562426e-06}, {"id": 354, "seek": 148824, "start": 1494.0, "end": 1496.0, "text": " And these", "tokens": [400, 613], "temperature": 0.0, "avg_logprob": -0.31950624305081654, "compression_ratio": 1.651376146788991, "no_speech_prob": 3.611848569562426e-06}, {"id": 355, "seek": 148824, "start": 1496.08, "end": 1501.6, "text": " feature importance plots remember because they're built on random forests and", "tokens": [4111, 7379, 28609, 1604, 570, 436, 434, 3094, 322, 4974, 21700, 293], "temperature": 0.0, "avg_logprob": -0.31950624305081654, "compression_ratio": 1.651376146788991, "no_speech_prob": 3.611848569562426e-06}, {"id": 356, "seek": 148824, "start": 1502.4, "end": 1505.04, "text": " Random forests don't care about", "tokens": [37603, 21700, 500, 380, 1127, 466], "temperature": 0.0, "avg_logprob": -0.31950624305081654, "compression_ratio": 1.651376146788991, "no_speech_prob": 3.611848569562426e-06}, {"id": 357, "seek": 148824, "start": 1505.76, "end": 1506.72, "text": " really", "tokens": [534], "temperature": 0.0, "avg_logprob": -0.31950624305081654, "compression_ratio": 1.651376146788991, "no_speech_prob": 3.611848569562426e-06}, {"id": 358, "seek": 148824, "start": 1506.72, "end": 1510.96, "text": " The distribution of your data and they can handle categorical variables and stuff like that", "tokens": [440, 7316, 295, 428, 1412, 293, 436, 393, 4813, 19250, 804, 9102, 293, 1507, 411, 300], "temperature": 0.0, "avg_logprob": -0.31950624305081654, "compression_ratio": 1.651376146788991, "no_speech_prob": 3.611848569562426e-06}, {"id": 359, "seek": 148824, "start": 1511.28, "end": 1514.96, "text": " That means that you can basically any tabular data set you have you can just", "tokens": [663, 1355, 300, 291, 393, 1936, 604, 4421, 1040, 1412, 992, 291, 362, 291, 393, 445], "temperature": 0.0, "avg_logprob": -0.31950624305081654, "compression_ratio": 1.651376146788991, "no_speech_prob": 3.611848569562426e-06}, {"id": 360, "seek": 151496, "start": 1514.96, "end": 1517.68, "text": " plot this right away and", "tokens": [7542, 341, 558, 1314, 293], "temperature": 0.0, "avg_logprob": -0.3117115585892289, "compression_ratio": 1.5372340425531914, "no_speech_prob": 5.954595508228522e-06}, {"id": 361, "seek": 151496, "start": 1518.24, "end": 1522.48, "text": " Random forests, you know for most data sets only take a few seconds to train", "tokens": [37603, 21700, 11, 291, 458, 337, 881, 1412, 6352, 787, 747, 257, 1326, 3949, 281, 3847], "temperature": 0.0, "avg_logprob": -0.3117115585892289, "compression_ratio": 1.5372340425531914, "no_speech_prob": 5.954595508228522e-06}, {"id": 362, "seek": 151496, "start": 1523.04, "end": 1525.52, "text": " You know really at most of a minute or two", "tokens": [509, 458, 534, 412, 881, 295, 257, 3456, 420, 732], "temperature": 0.0, "avg_logprob": -0.3117115585892289, "compression_ratio": 1.5372340425531914, "no_speech_prob": 5.954595508228522e-06}, {"id": 363, "seek": 151496, "start": 1526.96, "end": 1531.52, "text": " And so if you've got a big data set and you know hundreds of columns", "tokens": [400, 370, 498, 291, 600, 658, 257, 955, 1412, 992, 293, 291, 458, 6779, 295, 13766], "temperature": 0.0, "avg_logprob": -0.3117115585892289, "compression_ratio": 1.5372340425531914, "no_speech_prob": 5.954595508228522e-06}, {"id": 364, "seek": 151496, "start": 1532.56, "end": 1534.56, "text": " do this first and", "tokens": [360, 341, 700, 293], "temperature": 0.0, "avg_logprob": -0.3117115585892289, "compression_ratio": 1.5372340425531914, "no_speech_prob": 5.954595508228522e-06}, {"id": 365, "seek": 151496, "start": 1534.8, "end": 1536.4, "text": " find the", "tokens": [915, 264], "temperature": 0.0, "avg_logprob": -0.3117115585892289, "compression_ratio": 1.5372340425531914, "no_speech_prob": 5.954595508228522e-06}, {"id": 366, "seek": 151496, "start": 1536.4, "end": 1538.8, "text": " 30 columns that might matter", "tokens": [2217, 13766, 300, 1062, 1871], "temperature": 0.0, "avg_logprob": -0.3117115585892289, "compression_ratio": 1.5372340425531914, "no_speech_prob": 5.954595508228522e-06}, {"id": 367, "seek": 151496, "start": 1539.92, "end": 1541.92, "text": " It's such a helpful", "tokens": [467, 311, 1270, 257, 4961], "temperature": 0.0, "avg_logprob": -0.3117115585892289, "compression_ratio": 1.5372340425531914, "no_speech_prob": 5.954595508228522e-06}, {"id": 368, "seek": 154192, "start": 1541.92, "end": 1547.3600000000001, "text": " Thing to do so i've done that for example. I did some work in credit scoring. So we're trying to find out which", "tokens": [30902, 281, 360, 370, 741, 600, 1096, 300, 337, 1365, 13, 286, 630, 512, 589, 294, 5397, 22358, 13, 407, 321, 434, 1382, 281, 915, 484, 597], "temperature": 0.0, "avg_logprob": -0.2563526153564453, "compression_ratio": 1.596, "no_speech_prob": 2.225186563009629e-06}, {"id": 369, "seek": 154192, "start": 1548.0800000000002, "end": 1550.8000000000002, "text": " Things would predict who's going to default on a loan", "tokens": [9514, 576, 6069, 567, 311, 516, 281, 7576, 322, 257, 10529], "temperature": 0.0, "avg_logprob": -0.2563526153564453, "compression_ratio": 1.596, "no_speech_prob": 2.225186563009629e-06}, {"id": 370, "seek": 154192, "start": 1551.44, "end": 1553.44, "text": " and I was given", "tokens": [293, 286, 390, 2212], "temperature": 0.0, "avg_logprob": -0.2563526153564453, "compression_ratio": 1.596, "no_speech_prob": 2.225186563009629e-06}, {"id": 371, "seek": 154192, "start": 1553.44, "end": 1554.54, "text": " something like", "tokens": [746, 411], "temperature": 0.0, "avg_logprob": -0.2563526153564453, "compression_ratio": 1.596, "no_speech_prob": 2.225186563009629e-06}, {"id": 372, "seek": 154192, "start": 1554.54, "end": 1556.16, "text": " 7 000", "tokens": [1614, 13711], "temperature": 0.0, "avg_logprob": -0.2563526153564453, "compression_ratio": 1.596, "no_speech_prob": 2.225186563009629e-06}, {"id": 373, "seek": 154192, "start": 1556.16, "end": 1558.16, "text": " columns from the database", "tokens": [13766, 490, 264, 8149], "temperature": 0.0, "avg_logprob": -0.2563526153564453, "compression_ratio": 1.596, "no_speech_prob": 2.225186563009629e-06}, {"id": 374, "seek": 154192, "start": 1558.64, "end": 1563.6000000000001, "text": " And I put it straight into a random forest and found I think there was about 30 columns that seemed", "tokens": [400, 286, 829, 309, 2997, 666, 257, 4974, 6719, 293, 1352, 286, 519, 456, 390, 466, 2217, 13766, 300, 6576], "temperature": 0.0, "avg_logprob": -0.2563526153564453, "compression_ratio": 1.596, "no_speech_prob": 2.225186563009629e-06}, {"id": 375, "seek": 154192, "start": 1564.3200000000002, "end": 1566.3200000000002, "text": " Kind of interesting. I did that", "tokens": [9242, 295, 1880, 13, 286, 630, 300], "temperature": 0.0, "avg_logprob": -0.2563526153564453, "compression_ratio": 1.596, "no_speech_prob": 2.225186563009629e-06}, {"id": 376, "seek": 154192, "start": 1566.88, "end": 1568.88, "text": " Like two hours after I started the job", "tokens": [1743, 732, 2496, 934, 286, 1409, 264, 1691], "temperature": 0.0, "avg_logprob": -0.2563526153564453, "compression_ratio": 1.596, "no_speech_prob": 2.225186563009629e-06}, {"id": 377, "seek": 156888, "start": 1568.88, "end": 1571.44, "text": " and I went to the", "tokens": [293, 286, 1437, 281, 264], "temperature": 0.0, "avg_logprob": -0.3237219574630901, "compression_ratio": 1.6150234741784038, "no_speech_prob": 1.95234297279967e-05}, {"id": 378, "seek": 156888, "start": 1571.92, "end": 1577.44, "text": " Head of marketing and the head of risk and I told them here's the columns. I think that we should focus on", "tokens": [11398, 295, 6370, 293, 264, 1378, 295, 3148, 293, 286, 1907, 552, 510, 311, 264, 13766, 13, 286, 519, 300, 321, 820, 1879, 322], "temperature": 0.0, "avg_logprob": -0.3237219574630901, "compression_ratio": 1.6150234741784038, "no_speech_prob": 1.95234297279967e-05}, {"id": 379, "seek": 156888, "start": 1578.0800000000002, "end": 1579.7600000000002, "text": " And they were like", "tokens": [400, 436, 645, 411], "temperature": 0.0, "avg_logprob": -0.3237219574630901, "compression_ratio": 1.6150234741784038, "no_speech_prob": 1.95234297279967e-05}, {"id": 380, "seek": 156888, "start": 1579.7600000000002, "end": 1584.5600000000002, "text": " Oh my god, we just finished a two-year consulting project with one of the big consultants", "tokens": [876, 452, 3044, 11, 321, 445, 4335, 257, 732, 12, 5294, 23682, 1716, 365, 472, 295, 264, 955, 38935], "temperature": 0.0, "avg_logprob": -0.3237219574630901, "compression_ratio": 1.6150234741784038, "no_speech_prob": 1.95234297279967e-05}, {"id": 381, "seek": 156888, "start": 1585.3600000000001, "end": 1587.3600000000001, "text": " Paid the millions of dollars", "tokens": [3426, 327, 264, 6803, 295, 3808], "temperature": 0.0, "avg_logprob": -0.3237219574630901, "compression_ratio": 1.6150234741784038, "no_speech_prob": 1.95234297279967e-05}, {"id": 382, "seek": 156888, "start": 1587.3600000000001, "end": 1589.3600000000001, "text": " And they came up with a subset of these", "tokens": [400, 436, 1361, 493, 365, 257, 25993, 295, 613], "temperature": 0.0, "avg_logprob": -0.3237219574630901, "compression_ratio": 1.6150234741784038, "no_speech_prob": 1.95234297279967e-05}, {"id": 383, "seek": 156888, "start": 1594.48, "end": 1596.48, "text": " There are a lot of things that you can do", "tokens": [821, 366, 257, 688, 295, 721, 300, 291, 393, 360], "temperature": 0.0, "avg_logprob": -0.3237219574630901, "compression_ratio": 1.6150234741784038, "no_speech_prob": 1.95234297279967e-05}, {"id": 384, "seek": 159648, "start": 1596.48, "end": 1598.48, "text": " with um", "tokens": [365, 1105], "temperature": 0.0, "avg_logprob": -0.39995704872020776, "compression_ratio": 1.5549132947976878, "no_speech_prob": 6.853884315205505e-06}, {"id": 385, "seek": 159648, "start": 1598.48, "end": 1602.24, "text": " With random forests along this path. I'll touch on them briefly", "tokens": [2022, 4974, 21700, 2051, 341, 3100, 13, 286, 603, 2557, 322, 552, 10515], "temperature": 0.0, "avg_logprob": -0.39995704872020776, "compression_ratio": 1.5549132947976878, "no_speech_prob": 6.853884315205505e-06}, {"id": 386, "seek": 159648, "start": 1603.84, "end": 1605.84, "text": " Um", "tokens": [3301], "temperature": 0.0, "avg_logprob": -0.39995704872020776, "compression_ratio": 1.5549132947976878, "no_speech_prob": 6.853884315205505e-06}, {"id": 387, "seek": 159648, "start": 1605.84, "end": 1607.84, "text": " And specifically", "tokens": [400, 4682], "temperature": 0.0, "avg_logprob": -0.39995704872020776, "compression_ratio": 1.5549132947976878, "no_speech_prob": 6.853884315205505e-06}, {"id": 388, "seek": 159648, "start": 1608.32, "end": 1612.48, "text": " I'm going to look at chapter 8 of the book", "tokens": [286, 478, 516, 281, 574, 412, 7187, 1649, 295, 264, 1446], "temperature": 0.0, "avg_logprob": -0.39995704872020776, "compression_ratio": 1.5549132947976878, "no_speech_prob": 6.853884315205505e-06}, {"id": 389, "seek": 159648, "start": 1614.24, "end": 1619.6, "text": " Which goes into this in a lot more detail and particularly interestingly chapter 8 of the book uses a", "tokens": [3013, 1709, 666, 341, 294, 257, 688, 544, 2607, 293, 4098, 25873, 7187, 1649, 295, 264, 1446, 4960, 257], "temperature": 0.0, "avg_logprob": -0.39995704872020776, "compression_ratio": 1.5549132947976878, "no_speech_prob": 6.853884315205505e-06}, {"id": 390, "seek": 159648, "start": 1621.28, "end": 1623.28, "text": " Much bigger and more interesting", "tokens": [12313, 3801, 293, 544, 1880], "temperature": 0.0, "avg_logprob": -0.39995704872020776, "compression_ratio": 1.5549132947976878, "no_speech_prob": 6.853884315205505e-06}, {"id": 391, "seek": 162328, "start": 1623.28, "end": 1627.2, "text": " Data set which is auction prices of heavy industrial equipment", "tokens": [11888, 992, 597, 307, 24139, 7901, 295, 4676, 9987, 5927], "temperature": 0.0, "avg_logprob": -0.3942905467945141, "compression_ratio": 1.7788018433179724, "no_speech_prob": 9.222177141054999e-06}, {"id": 392, "seek": 162328, "start": 1627.6, "end": 1631.2, "text": " I mean, it's less interesting historically but more interestingly numerically", "tokens": [286, 914, 11, 309, 311, 1570, 1880, 16180, 457, 544, 25873, 7866, 984], "temperature": 0.0, "avg_logprob": -0.3942905467945141, "compression_ratio": 1.7788018433179724, "no_speech_prob": 9.222177141054999e-06}, {"id": 393, "seek": 162328, "start": 1636.3999999999999, "end": 1639.92, "text": " And so some of the things I did there on this data set", "tokens": [400, 370, 512, 295, 264, 721, 286, 630, 456, 322, 341, 1412, 992], "temperature": 0.0, "avg_logprob": -0.3942905467945141, "compression_ratio": 1.7788018433179724, "no_speech_prob": 9.222177141054999e-06}, {"id": 394, "seek": 162328, "start": 1642.24, "end": 1645.28, "text": " So this is from the data set this is from the scikit-learn documentation", "tokens": [407, 341, 307, 490, 264, 1412, 992, 341, 307, 490, 264, 2180, 22681, 12, 306, 1083, 14333], "temperature": 0.0, "avg_logprob": -0.3942905467945141, "compression_ratio": 1.7788018433179724, "no_speech_prob": 9.222177141054999e-06}, {"id": 395, "seek": 162328, "start": 1646.24, "end": 1649.28, "text": " They looked at how as you increase the number of estimators", "tokens": [814, 2956, 412, 577, 382, 291, 3488, 264, 1230, 295, 8017, 3391], "temperature": 0.0, "avg_logprob": -0.3942905467945141, "compression_ratio": 1.7788018433179724, "no_speech_prob": 9.222177141054999e-06}, {"id": 396, "seek": 162328, "start": 1649.28, "end": 1651.28, "text": " So the number of estimates that you get from the data set", "tokens": [407, 264, 1230, 295, 20561, 300, 291, 483, 490, 264, 1412, 992], "temperature": 0.0, "avg_logprob": -0.3942905467945141, "compression_ratio": 1.7788018433179724, "no_speech_prob": 9.222177141054999e-06}, {"id": 397, "seek": 165128, "start": 1651.28, "end": 1654.24, "text": " As you increase the number of estimators, so the number of trees", "tokens": [1018, 291, 3488, 264, 1230, 295, 8017, 3391, 11, 370, 264, 1230, 295, 5852], "temperature": 0.0, "avg_logprob": -0.12948688216831372, "compression_ratio": 1.6857142857142857, "no_speech_prob": 2.6841189537663013e-06}, {"id": 398, "seek": 165128, "start": 1655.76, "end": 1658.56, "text": " How much does the accuracy improve?", "tokens": [1012, 709, 775, 264, 14170, 3470, 30], "temperature": 0.0, "avg_logprob": -0.12948688216831372, "compression_ratio": 1.6857142857142857, "no_speech_prob": 2.6841189537663013e-06}, {"id": 399, "seek": 165128, "start": 1659.12, "end": 1662.0, "text": " so I then did the same thing on our data set so I actually just", "tokens": [370, 286, 550, 630, 264, 912, 551, 322, 527, 1412, 992, 370, 286, 767, 445], "temperature": 0.0, "avg_logprob": -0.12948688216831372, "compression_ratio": 1.6857142857142857, "no_speech_prob": 2.6841189537663013e-06}, {"id": 400, "seek": 165128, "start": 1663.6, "end": 1666.8, "text": " Added up to 40 more and more and more trees", "tokens": [5349, 292, 493, 281, 3356, 544, 293, 544, 293, 544, 5852], "temperature": 0.0, "avg_logprob": -0.12948688216831372, "compression_ratio": 1.6857142857142857, "no_speech_prob": 2.6841189537663013e-06}, {"id": 401, "seek": 165128, "start": 1668.08, "end": 1673.84, "text": " and you can see that basically as as predicted by that kind of an initial bit of", "tokens": [293, 291, 393, 536, 300, 1936, 382, 382, 19147, 538, 300, 733, 295, 364, 5883, 857, 295], "temperature": 0.0, "avg_logprob": -0.12948688216831372, "compression_ratio": 1.6857142857142857, "no_speech_prob": 2.6841189537663013e-06}, {"id": 402, "seek": 165128, "start": 1675.2, "end": 1678.56, "text": " Hand wavy theory I gave you that you would expect the more trees", "tokens": [8854, 261, 15498, 5261, 286, 2729, 291, 300, 291, 576, 2066, 264, 544, 5852], "temperature": 0.0, "avg_logprob": -0.12948688216831372, "compression_ratio": 1.6857142857142857, "no_speech_prob": 2.6841189537663013e-06}, {"id": 403, "seek": 167856, "start": 1678.56, "end": 1681.36, "text": " The lower the error because the more things you're averaging", "tokens": [440, 3126, 264, 6713, 570, 264, 544, 721, 291, 434, 47308], "temperature": 0.0, "avg_logprob": -0.26447618787533766, "compression_ratio": 1.6590038314176245, "no_speech_prob": 8.013083970581647e-06}, {"id": 404, "seek": 167856, "start": 1681.9199999999998, "end": 1686.08, "text": " And that's exactly what we find the accuracy improves as we have more trees", "tokens": [400, 300, 311, 2293, 437, 321, 915, 264, 14170, 24771, 382, 321, 362, 544, 5852], "temperature": 0.0, "avg_logprob": -0.26447618787533766, "compression_ratio": 1.6590038314176245, "no_speech_prob": 8.013083970581647e-06}, {"id": 405, "seek": 167856, "start": 1686.8799999999999, "end": 1688.8799999999999, "text": " John what's up?", "tokens": [2619, 437, 311, 493, 30], "temperature": 0.0, "avg_logprob": -0.26447618787533766, "compression_ratio": 1.6590038314176245, "no_speech_prob": 8.013083970581647e-06}, {"id": 406, "seek": 167856, "start": 1689.44, "end": 1693.84, "text": " Victor as possibly you might have just answered his question actually as he typed it", "tokens": [15777, 382, 6264, 291, 1062, 362, 445, 10103, 702, 1168, 767, 382, 415, 33941, 309], "temperature": 0.0, "avg_logprob": -0.26447618787533766, "compression_ratio": 1.6590038314176245, "no_speech_prob": 8.013083970581647e-06}, {"id": 407, "seek": 167856, "start": 1693.84, "end": 1698.32, "text": " But he's he's asking on the same theme the number of trees in a random forest", "tokens": [583, 415, 311, 415, 311, 3365, 322, 264, 912, 6314, 264, 1230, 295, 5852, 294, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.26447618787533766, "compression_ratio": 1.6590038314176245, "no_speech_prob": 8.013083970581647e-06}, {"id": 408, "seek": 167856, "start": 1698.32, "end": 1702.3999999999999, "text": " Does increasing the number of trees always translate to a better error?", "tokens": [4402, 5662, 264, 1230, 295, 5852, 1009, 13799, 281, 257, 1101, 6713, 30], "temperature": 0.0, "avg_logprob": -0.26447618787533766, "compression_ratio": 1.6590038314176245, "no_speech_prob": 8.013083970581647e-06}, {"id": 409, "seek": 167856, "start": 1702.3999999999999, "end": 1704.3999999999999, "text": " Yes, it does always", "tokens": [1079, 11, 309, 775, 1009], "temperature": 0.0, "avg_logprob": -0.26447618787533766, "compression_ratio": 1.6590038314176245, "no_speech_prob": 8.013083970581647e-06}, {"id": 410, "seek": 167856, "start": 1705.2, "end": 1707.2, "text": " I mean tiny bumps, right?", "tokens": [286, 914, 5870, 27719, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.26447618787533766, "compression_ratio": 1.6590038314176245, "no_speech_prob": 8.013083970581647e-06}, {"id": 411, "seek": 170720, "start": 1707.2, "end": 1709.2, "text": " Tiny bumps, right? But yeah", "tokens": [39992, 27719, 11, 558, 30, 583, 1338], "temperature": 0.0, "avg_logprob": -0.12113629317865139, "compression_ratio": 1.4787234042553192, "no_speech_prob": 9.515479177935049e-06}, {"id": 412, "seek": 170720, "start": 1709.76, "end": 1711.76, "text": " Once you smooth it out", "tokens": [3443, 291, 5508, 309, 484], "temperature": 0.0, "avg_logprob": -0.12113629317865139, "compression_ratio": 1.4787234042553192, "no_speech_prob": 9.515479177935049e-06}, {"id": 413, "seek": 170720, "start": 1713.2, "end": 1715.2, "text": " But um", "tokens": [583, 1105], "temperature": 0.0, "avg_logprob": -0.12113629317865139, "compression_ratio": 1.4787234042553192, "no_speech_prob": 9.515479177935049e-06}, {"id": 414, "seek": 170720, "start": 1715.28, "end": 1717.28, "text": " Decreasing returns", "tokens": [12427, 265, 3349, 11247], "temperature": 0.0, "avg_logprob": -0.12113629317865139, "compression_ratio": 1.4787234042553192, "no_speech_prob": 9.515479177935049e-06}, {"id": 415, "seek": 170720, "start": 1717.92, "end": 1719.92, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.12113629317865139, "compression_ratio": 1.4787234042553192, "no_speech_prob": 9.515479177935049e-06}, {"id": 416, "seek": 170720, "start": 1720.72, "end": 1725.68, "text": " If you end up productionizing a random forest then of course every one of these trees you have to", "tokens": [759, 291, 917, 493, 4265, 3319, 257, 4974, 6719, 550, 295, 1164, 633, 472, 295, 613, 5852, 291, 362, 281], "temperature": 0.0, "avg_logprob": -0.12113629317865139, "compression_ratio": 1.4787234042553192, "no_speech_prob": 9.515479177935049e-06}, {"id": 417, "seek": 170720, "start": 1726.64, "end": 1728.48, "text": " You know go through", "tokens": [509, 458, 352, 807], "temperature": 0.0, "avg_logprob": -0.12113629317865139, "compression_ratio": 1.4787234042553192, "no_speech_prob": 9.515479177935049e-06}, {"id": 418, "seek": 170720, "start": 1728.48, "end": 1730.48, "text": " for at inference time", "tokens": [337, 412, 38253, 565], "temperature": 0.0, "avg_logprob": -0.12113629317865139, "compression_ratio": 1.4787234042553192, "no_speech_prob": 9.515479177935049e-06}, {"id": 419, "seek": 170720, "start": 1730.56, "end": 1733.8400000000001, "text": " So it's not that there's no cost. I mean having said that", "tokens": [407, 309, 311, 406, 300, 456, 311, 572, 2063, 13, 286, 914, 1419, 848, 300], "temperature": 0.0, "avg_logprob": -0.12113629317865139, "compression_ratio": 1.4787234042553192, "no_speech_prob": 9.515479177935049e-06}, {"id": 420, "seek": 173384, "start": 1733.84, "end": 1737.04, "text": " Zipping through a binary tree is the kind of thing you can", "tokens": [1176, 6297, 807, 257, 17434, 4230, 307, 264, 733, 295, 551, 291, 393], "temperature": 0.0, "avg_logprob": -0.3071928386446796, "compression_ratio": 1.4314720812182742, "no_speech_prob": 1.2028996025037486e-05}, {"id": 421, "seek": 173384, "start": 1737.84, "end": 1739.84, "text": " really", "tokens": [534], "temperature": 0.0, "avg_logprob": -0.3071928386446796, "compression_ratio": 1.4314720812182742, "no_speech_prob": 1.2028996025037486e-05}, {"id": 422, "seek": 173384, "start": 1739.84, "end": 1744.24, "text": " Do fast in fact, it's it's quite easy to like literally", "tokens": [1144, 2370, 294, 1186, 11, 309, 311, 309, 311, 1596, 1858, 281, 411, 3736], "temperature": 0.0, "avg_logprob": -0.3071928386446796, "compression_ratio": 1.4314720812182742, "no_speech_prob": 1.2028996025037486e-05}, {"id": 423, "seek": 173384, "start": 1744.8799999999999, "end": 1746.32, "text": " spit out", "tokens": [22127, 484], "temperature": 0.0, "avg_logprob": -0.3071928386446796, "compression_ratio": 1.4314720812182742, "no_speech_prob": 1.2028996025037486e-05}, {"id": 424, "seek": 173384, "start": 1746.32, "end": 1748.32, "text": " C++ code", "tokens": [383, 25472, 3089], "temperature": 0.0, "avg_logprob": -0.3071928386446796, "compression_ratio": 1.4314720812182742, "no_speech_prob": 1.2028996025037486e-05}, {"id": 425, "seek": 173384, "start": 1748.32, "end": 1754.1599999999999, "text": " With a bunch of if statements and compile it and get extremely fast performance", "tokens": [2022, 257, 3840, 295, 498, 12363, 293, 31413, 309, 293, 483, 4664, 2370, 3389], "temperature": 0.0, "avg_logprob": -0.3071928386446796, "compression_ratio": 1.4314720812182742, "no_speech_prob": 1.2028996025037486e-05}, {"id": 426, "seek": 173384, "start": 1756.9599999999998, "end": 1761.04, "text": " I don't often use more than 100 trees. This is a rule of thumb", "tokens": [286, 500, 380, 2049, 764, 544, 813, 2319, 5852, 13, 639, 307, 257, 4978, 295, 9298], "temperature": 0.0, "avg_logprob": -0.3071928386446796, "compression_ratio": 1.4314720812182742, "no_speech_prob": 1.2028996025037486e-05}, {"id": 427, "seek": 176104, "start": 1761.04, "end": 1763.04, "text": " This is a rule of thumb", "tokens": [639, 307, 257, 4978, 295, 9298], "temperature": 0.0, "avg_logprob": -0.17047200520833333, "compression_ratio": 1.5574712643678161, "no_speech_prob": 8.52962511999067e-06}, {"id": 428, "seek": 176104, "start": 1766.3999999999999, "end": 1768.3999999999999, "text": " Is that the only one done?", "tokens": [1119, 300, 264, 787, 472, 1096, 30], "temperature": 0.0, "avg_logprob": -0.17047200520833333, "compression_ratio": 1.5574712643678161, "no_speech_prob": 8.52962511999067e-06}, {"id": 429, "seek": 176104, "start": 1769.12, "end": 1771.12, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.17047200520833333, "compression_ratio": 1.5574712643678161, "no_speech_prob": 8.52962511999067e-06}, {"id": 430, "seek": 176104, "start": 1773.2, "end": 1780.0, "text": " So then there's another interesting feature of random forests, which is remember how in our example we trained with", "tokens": [407, 550, 456, 311, 1071, 1880, 4111, 295, 4974, 21700, 11, 597, 307, 1604, 577, 294, 527, 1365, 321, 8895, 365], "temperature": 0.0, "avg_logprob": -0.17047200520833333, "compression_ratio": 1.5574712643678161, "no_speech_prob": 8.52962511999067e-06}, {"id": 431, "seek": 176104, "start": 1780.78, "end": 1782.78, "text": " 75 of the data", "tokens": [9562, 295, 264, 1412], "temperature": 0.0, "avg_logprob": -0.17047200520833333, "compression_ratio": 1.5574712643678161, "no_speech_prob": 8.52962511999067e-06}, {"id": 432, "seek": 176104, "start": 1783.28, "end": 1784.8, "text": " on each tree", "tokens": [322, 1184, 4230], "temperature": 0.0, "avg_logprob": -0.17047200520833333, "compression_ratio": 1.5574712643678161, "no_speech_prob": 8.52962511999067e-06}, {"id": 433, "seek": 176104, "start": 1784.8, "end": 1787.92, "text": " So that means for each tree there was 25 of the data we didn't train on", "tokens": [407, 300, 1355, 337, 1184, 4230, 456, 390, 3552, 295, 264, 1412, 321, 994, 380, 3847, 322], "temperature": 0.0, "avg_logprob": -0.17047200520833333, "compression_ratio": 1.5574712643678161, "no_speech_prob": 8.52962511999067e-06}, {"id": 434, "seek": 178792, "start": 1787.92, "end": 1794.16, "text": " Now this actually means if you don't have much data in some situations you can get away with not having a validation set", "tokens": [823, 341, 767, 1355, 498, 291, 500, 380, 362, 709, 1412, 294, 512, 6851, 291, 393, 483, 1314, 365, 406, 1419, 257, 24071, 992], "temperature": 0.0, "avg_logprob": -0.2434231195694361, "compression_ratio": 1.5692307692307692, "no_speech_prob": 4.425430688570486e-06}, {"id": 435, "seek": 178792, "start": 1795.2, "end": 1797.2, "text": " and the reason why", "tokens": [293, 264, 1778, 983], "temperature": 0.0, "avg_logprob": -0.2434231195694361, "compression_ratio": 1.5692307692307692, "no_speech_prob": 4.425430688570486e-06}, {"id": 436, "seek": 178792, "start": 1797.52, "end": 1804.5600000000002, "text": " Is because for each tree we can pick the 25 percent of rows that weren't in that tree", "tokens": [1119, 570, 337, 1184, 4230, 321, 393, 1888, 264, 3552, 3043, 295, 13241, 300, 4999, 380, 294, 300, 4230], "temperature": 0.0, "avg_logprob": -0.2434231195694361, "compression_ratio": 1.5692307692307692, "no_speech_prob": 4.425430688570486e-06}, {"id": 437, "seek": 178792, "start": 1805.3600000000001, "end": 1807.3600000000001, "text": " And see how accurate", "tokens": [400, 536, 577, 8559], "temperature": 0.0, "avg_logprob": -0.2434231195694361, "compression_ratio": 1.5692307692307692, "no_speech_prob": 4.425430688570486e-06}, {"id": 438, "seek": 178792, "start": 1807.3600000000001, "end": 1809.3600000000001, "text": " That tree was on those rows", "tokens": [663, 4230, 390, 322, 729, 13241], "temperature": 0.0, "avg_logprob": -0.2434231195694361, "compression_ratio": 1.5692307692307692, "no_speech_prob": 4.425430688570486e-06}, {"id": 439, "seek": 178792, "start": 1810.0800000000002, "end": 1812.0800000000002, "text": " And we can average for each row", "tokens": [400, 321, 393, 4274, 337, 1184, 5386], "temperature": 0.0, "avg_logprob": -0.2434231195694361, "compression_ratio": 1.5692307692307692, "no_speech_prob": 4.425430688570486e-06}, {"id": 440, "seek": 181208, "start": 1812.08, "end": 1817.4399999999998, "text": " Their accuracy on all of the trees in which they were not part of the training", "tokens": [6710, 14170, 322, 439, 295, 264, 5852, 294, 597, 436, 645, 406, 644, 295, 264, 3097], "temperature": 0.0, "avg_logprob": -0.3292613098586815, "compression_ratio": 1.4904458598726114, "no_speech_prob": 3.6119088235864183e-06}, {"id": 441, "seek": 181208, "start": 1818.0, "end": 1820.3999999999999, "text": " And that is called the out of bag error", "tokens": [400, 300, 307, 1219, 264, 484, 295, 3411, 6713], "temperature": 0.0, "avg_logprob": -0.3292613098586815, "compression_ratio": 1.4904458598726114, "no_speech_prob": 3.6119088235864183e-06}, {"id": 442, "seek": 181208, "start": 1821.1999999999998, "end": 1827.52, "text": " Or oob error and this is built in also to sklearn. You can ask for an oob", "tokens": [1610, 277, 996, 6713, 293, 341, 307, 3094, 294, 611, 281, 1110, 306, 1083, 13, 509, 393, 1029, 337, 364, 277, 996], "temperature": 0.0, "avg_logprob": -0.3292613098586815, "compression_ratio": 1.4904458598726114, "no_speech_prob": 3.6119088235864183e-06}, {"id": 443, "seek": 181208, "start": 1828.8, "end": 1830.8, "text": " prediction", "tokens": [17630], "temperature": 0.0, "avg_logprob": -0.3292613098586815, "compression_ratio": 1.4904458598726114, "no_speech_prob": 3.6119088235864183e-06}, {"id": 444, "seek": 181208, "start": 1832.8, "end": 1834.8, "text": " Um john", "tokens": [3301, 35097], "temperature": 0.0, "avg_logprob": -0.3292613098586815, "compression_ratio": 1.4904458598726114, "no_speech_prob": 3.6119088235864183e-06}, {"id": 445, "seek": 181208, "start": 1836.32, "end": 1838.32, "text": " Just before we move on", "tokens": [1449, 949, 321, 1286, 322], "temperature": 0.0, "avg_logprob": -0.3292613098586815, "compression_ratio": 1.4904458598726114, "no_speech_prob": 3.6119088235864183e-06}, {"id": 446, "seek": 183832, "start": 1838.32, "end": 1843.6, "text": " Just before we move on um, zakiya has a question about bagging", "tokens": [1449, 949, 321, 1286, 322, 1105, 11, 710, 7421, 3016, 575, 257, 1168, 466, 3411, 3249], "temperature": 0.0, "avg_logprob": -0.13442525085137816, "compression_ratio": 1.685589519650655, "no_speech_prob": 8.93912510946393e-06}, {"id": 447, "seek": 183832, "start": 1844.6399999999999, "end": 1848.48, "text": " So we know that bagging is powerful as an ensemble approach to machine learning", "tokens": [407, 321, 458, 300, 3411, 3249, 307, 4005, 382, 364, 19492, 3109, 281, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.13442525085137816, "compression_ratio": 1.685589519650655, "no_speech_prob": 8.93912510946393e-06}, {"id": 448, "seek": 183832, "start": 1848.8, "end": 1853.36, "text": " Would it be advisable to try out bagging then first when approaching a particular?", "tokens": [6068, 309, 312, 10280, 712, 281, 853, 484, 3411, 3249, 550, 700, 562, 14908, 257, 1729, 30], "temperature": 0.0, "avg_logprob": -0.13442525085137816, "compression_ratio": 1.685589519650655, "no_speech_prob": 8.93912510946393e-06}, {"id": 449, "seek": 183832, "start": 1854.24, "end": 1856.24, "text": " Say tabular task", "tokens": [6463, 4421, 1040, 5633], "temperature": 0.0, "avg_logprob": -0.13442525085137816, "compression_ratio": 1.685589519650655, "no_speech_prob": 8.93912510946393e-06}, {"id": 450, "seek": 183832, "start": 1856.24, "end": 1859.36, "text": " Before deep learning so that's the first part of the question", "tokens": [4546, 2452, 2539, 370, 300, 311, 264, 700, 644, 295, 264, 1168], "temperature": 0.0, "avg_logprob": -0.13442525085137816, "compression_ratio": 1.685589519650655, "no_speech_prob": 8.93912510946393e-06}, {"id": 451, "seek": 183832, "start": 1860.56, "end": 1861.6, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.13442525085137816, "compression_ratio": 1.685589519650655, "no_speech_prob": 8.93912510946393e-06}, {"id": 452, "seek": 183832, "start": 1861.6, "end": 1866.56, "text": " And the second part is could we create a bagging model which includes fast ai?", "tokens": [400, 264, 1150, 644, 307, 727, 321, 1884, 257, 3411, 3249, 2316, 597, 5974, 2370, 9783, 30], "temperature": 0.0, "avg_logprob": -0.13442525085137816, "compression_ratio": 1.685589519650655, "no_speech_prob": 8.93912510946393e-06}, {"id": 453, "seek": 186656, "start": 1866.56, "end": 1868.56, "text": " deep learning models", "tokens": [2452, 2539, 5245], "temperature": 0.0, "avg_logprob": -0.18663428932107906, "compression_ratio": 1.6119402985074627, "no_speech_prob": 5.421816695161397e-06}, {"id": 454, "seek": 186656, "start": 1869.76, "end": 1871.04, "text": " Yes", "tokens": [1079], "temperature": 0.0, "avg_logprob": -0.18663428932107906, "compression_ratio": 1.6119402985074627, "no_speech_prob": 5.421816695161397e-06}, {"id": 455, "seek": 186656, "start": 1871.04, "end": 1874.08, "text": " Absolutely. So to be clear, you know bagging is kind of like", "tokens": [7021, 13, 407, 281, 312, 1850, 11, 291, 458, 3411, 3249, 307, 733, 295, 411], "temperature": 0.0, "avg_logprob": -0.18663428932107906, "compression_ratio": 1.6119402985074627, "no_speech_prob": 5.421816695161397e-06}, {"id": 456, "seek": 186656, "start": 1874.8799999999999, "end": 1876.8799999999999, "text": " a meta method. It's not a", "tokens": [257, 19616, 3170, 13, 467, 311, 406, 257], "temperature": 0.0, "avg_logprob": -0.18663428932107906, "compression_ratio": 1.6119402985074627, "no_speech_prob": 5.421816695161397e-06}, {"id": 457, "seek": 186656, "start": 1877.36, "end": 1879.36, "text": " prediction it's not a method of", "tokens": [17630, 309, 311, 406, 257, 3170, 295], "temperature": 0.0, "avg_logprob": -0.18663428932107906, "compression_ratio": 1.6119402985074627, "no_speech_prob": 5.421816695161397e-06}, {"id": 458, "seek": 186656, "start": 1879.36, "end": 1881.36, "text": " Modeling itself. It's just a method of", "tokens": [6583, 11031, 2564, 13, 467, 311, 445, 257, 3170, 295], "temperature": 0.0, "avg_logprob": -0.18663428932107906, "compression_ratio": 1.6119402985074627, "no_speech_prob": 5.421816695161397e-06}, {"id": 459, "seek": 186656, "start": 1882.3999999999999, "end": 1884.3999999999999, "text": " combining other models", "tokens": [21928, 661, 5245], "temperature": 0.0, "avg_logprob": -0.18663428932107906, "compression_ratio": 1.6119402985074627, "no_speech_prob": 5.421816695161397e-06}, {"id": 460, "seek": 186656, "start": 1884.3999999999999, "end": 1885.52, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.18663428932107906, "compression_ratio": 1.6119402985074627, "no_speech_prob": 5.421816695161397e-06}, {"id": 461, "seek": 186656, "start": 1885.52, "end": 1890.0, "text": " So random forests in particular as a particular approach to bagging", "tokens": [407, 4974, 21700, 294, 1729, 382, 257, 1729, 3109, 281, 3411, 3249], "temperature": 0.0, "avg_logprob": -0.18663428932107906, "compression_ratio": 1.6119402985074627, "no_speech_prob": 5.421816695161397e-06}, {"id": 462, "seek": 186656, "start": 1890.8, "end": 1894.08, "text": " Um is a you know, I would probably always start", "tokens": [3301, 307, 257, 291, 458, 11, 286, 576, 1391, 1009, 722], "temperature": 0.0, "avg_logprob": -0.18663428932107906, "compression_ratio": 1.6119402985074627, "no_speech_prob": 5.421816695161397e-06}, {"id": 463, "seek": 189408, "start": 1894.08, "end": 1901.1999999999998, "text": " Personally a tabular project with a random forest because they're nearly impossible to mess up and they give good insight and they give a good", "tokens": [21079, 257, 4421, 1040, 1716, 365, 257, 4974, 6719, 570, 436, 434, 6217, 6243, 281, 2082, 493, 293, 436, 976, 665, 11269, 293, 436, 976, 257, 665], "temperature": 0.0, "avg_logprob": -0.25492540318915186, "compression_ratio": 1.5833333333333333, "no_speech_prob": 8.529697879566811e-06}, {"id": 464, "seek": 189408, "start": 1901.1999999999998, "end": 1903.1999999999998, "text": " base case", "tokens": [3096, 1389], "temperature": 0.0, "avg_logprob": -0.25492540318915186, "compression_ratio": 1.5833333333333333, "no_speech_prob": 8.529697879566811e-06}, {"id": 465, "seek": 189408, "start": 1903.1999999999998, "end": 1907.04, "text": " But um, yeah your question then about can you bag?", "tokens": [583, 1105, 11, 1338, 428, 1168, 550, 466, 393, 291, 3411, 30], "temperature": 0.0, "avg_logprob": -0.25492540318915186, "compression_ratio": 1.5833333333333333, "no_speech_prob": 8.529697879566811e-06}, {"id": 466, "seek": 189408, "start": 1907.76, "end": 1912.3999999999999, "text": " Other models is a very interesting one and the answer is you absolutely can", "tokens": [5358, 5245, 307, 257, 588, 1880, 472, 293, 264, 1867, 307, 291, 3122, 393], "temperature": 0.0, "avg_logprob": -0.25492540318915186, "compression_ratio": 1.5833333333333333, "no_speech_prob": 8.529697879566811e-06}, {"id": 467, "seek": 189408, "start": 1913.1999999999998, "end": 1914.72, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.25492540318915186, "compression_ratio": 1.5833333333333333, "no_speech_prob": 8.529697879566811e-06}, {"id": 468, "seek": 189408, "start": 1914.72, "end": 1916.72, "text": " People very rarely do", "tokens": [3432, 588, 13752, 360], "temperature": 0.0, "avg_logprob": -0.25492540318915186, "compression_ratio": 1.5833333333333333, "no_speech_prob": 8.529697879566811e-06}, {"id": 469, "seek": 189408, "start": 1917.04, "end": 1919.04, "text": " Um, but we will", "tokens": [3301, 11, 457, 321, 486], "temperature": 0.0, "avg_logprob": -0.25492540318915186, "compression_ratio": 1.5833333333333333, "no_speech_prob": 8.529697879566811e-06}, {"id": 470, "seek": 189408, "start": 1919.04, "end": 1921.04, "text": " We will quite soon", "tokens": [492, 486, 1596, 2321], "temperature": 0.0, "avg_logprob": -0.25492540318915186, "compression_ratio": 1.5833333333333333, "no_speech_prob": 8.529697879566811e-06}, {"id": 471, "seek": 189408, "start": 1921.04, "end": 1923.04, "text": " Uh, maybe even today", "tokens": [4019, 11, 1310, 754, 965], "temperature": 0.0, "avg_logprob": -0.25492540318915186, "compression_ratio": 1.5833333333333333, "no_speech_prob": 8.529697879566811e-06}, {"id": 472, "seek": 192304, "start": 1923.04, "end": 1925.04, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.09593822216165476, "compression_ratio": 1.8943089430894309, "no_speech_prob": 6.53979304843233e-06}, {"id": 473, "seek": 192304, "start": 1928.96, "end": 1929.84, "text": " So I you know", "tokens": [407, 286, 291, 458], "temperature": 0.0, "avg_logprob": -0.09593822216165476, "compression_ratio": 1.8943089430894309, "no_speech_prob": 6.53979304843233e-06}, {"id": 474, "seek": 192304, "start": 1929.84, "end": 1936.24, "text": " You might be getting the impression i'm a bit of a fan of random forests and before I was before you know people thought of me as", "tokens": [509, 1062, 312, 1242, 264, 9995, 741, 478, 257, 857, 295, 257, 3429, 295, 4974, 21700, 293, 949, 286, 390, 949, 291, 458, 561, 1194, 295, 385, 382], "temperature": 0.0, "avg_logprob": -0.09593822216165476, "compression_ratio": 1.8943089430894309, "no_speech_prob": 6.53979304843233e-06}, {"id": 475, "seek": 192304, "start": 1936.8799999999999, "end": 1940.48, "text": " The deep learning guy people thought of me as the random forests guy", "tokens": [440, 2452, 2539, 2146, 561, 1194, 295, 385, 382, 264, 4974, 21700, 2146], "temperature": 0.0, "avg_logprob": -0.09593822216165476, "compression_ratio": 1.8943089430894309, "no_speech_prob": 6.53979304843233e-06}, {"id": 476, "seek": 192304, "start": 1940.56, "end": 1943.52, "text": " I used to go on about random forests all the time", "tokens": [286, 1143, 281, 352, 322, 466, 4974, 21700, 439, 264, 565], "temperature": 0.0, "avg_logprob": -0.09593822216165476, "compression_ratio": 1.8943089430894309, "no_speech_prob": 6.53979304843233e-06}, {"id": 477, "seek": 192304, "start": 1943.92, "end": 1949.12, "text": " And one of the reasons i'm so enthused about them isn't just that they're very accurate or that they require", "tokens": [400, 472, 295, 264, 4112, 741, 478, 370, 948, 71, 4717, 466, 552, 1943, 380, 445, 300, 436, 434, 588, 8559, 420, 300, 436, 3651], "temperature": 0.0, "avg_logprob": -0.09593822216165476, "compression_ratio": 1.8943089430894309, "no_speech_prob": 6.53979304843233e-06}, {"id": 478, "seek": 194912, "start": 1949.12, "end": 1955.1999999999998, "text": " You know that they're very hard to mess up and require a little processing pre-processing, but they give you a lot of uh,", "tokens": [509, 458, 300, 436, 434, 588, 1152, 281, 2082, 493, 293, 3651, 257, 707, 9007, 659, 12, 41075, 278, 11, 457, 436, 976, 291, 257, 688, 295, 2232, 11], "temperature": 0.0, "avg_logprob": -0.08765117811120075, "compression_ratio": 1.6928571428571428, "no_speech_prob": 7.888637810538057e-06}, {"id": 479, "seek": 194912, "start": 1955.6, "end": 1957.6, "text": " quick and easy insight", "tokens": [1702, 293, 1858, 11269], "temperature": 0.0, "avg_logprob": -0.08765117811120075, "compression_ratio": 1.6928571428571428, "no_speech_prob": 7.888637810538057e-06}, {"id": 480, "seek": 194912, "start": 1958.1599999999999, "end": 1960.6399999999999, "text": " Uh, and specifically these are the five things", "tokens": [4019, 11, 293, 4682, 613, 366, 264, 1732, 721], "temperature": 0.0, "avg_logprob": -0.08765117811120075, "compression_ratio": 1.6928571428571428, "no_speech_prob": 7.888637810538057e-06}, {"id": 481, "seek": 194912, "start": 1961.76, "end": 1966.56, "text": " Which I think that we're interested in and all of which are things that random forests are good at they will tell us", "tokens": [3013, 286, 519, 300, 321, 434, 3102, 294, 293, 439, 295, 597, 366, 721, 300, 4974, 21700, 366, 665, 412, 436, 486, 980, 505], "temperature": 0.0, "avg_logprob": -0.08765117811120075, "compression_ratio": 1.6928571428571428, "no_speech_prob": 7.888637810538057e-06}, {"id": 482, "seek": 194912, "start": 1966.6399999999999, "end": 1968.8799999999999, "text": " How confident are we in our predictions?", "tokens": [1012, 6679, 366, 321, 294, 527, 21264, 30], "temperature": 0.0, "avg_logprob": -0.08765117811120075, "compression_ratio": 1.6928571428571428, "no_speech_prob": 7.888637810538057e-06}, {"id": 483, "seek": 194912, "start": 1969.6, "end": 1974.4799999999998, "text": " On some particular row so when somebody you know when we're giving a loan to somebody", "tokens": [1282, 512, 1729, 5386, 370, 562, 2618, 291, 458, 562, 321, 434, 2902, 257, 10529, 281, 2618], "temperature": 0.0, "avg_logprob": -0.08765117811120075, "compression_ratio": 1.6928571428571428, "no_speech_prob": 7.888637810538057e-06}, {"id": 484, "seek": 194912, "start": 1975.1999999999998, "end": 1977.1999999999998, "text": " We don't necessarily just want to know", "tokens": [492, 500, 380, 4725, 445, 528, 281, 458], "temperature": 0.0, "avg_logprob": -0.08765117811120075, "compression_ratio": 1.6928571428571428, "no_speech_prob": 7.888637810538057e-06}, {"id": 485, "seek": 197720, "start": 1977.2, "end": 1979.2, "text": " How likely are they to repay?", "tokens": [1012, 3700, 366, 436, 281, 27522, 30], "temperature": 0.0, "avg_logprob": -0.09012692829348007, "compression_ratio": 1.6779661016949152, "no_speech_prob": 3.2376728995586745e-06}, {"id": 486, "seek": 197720, "start": 1979.68, "end": 1981.44, "text": " But we'd also like to know", "tokens": [583, 321, 1116, 611, 411, 281, 458], "temperature": 0.0, "avg_logprob": -0.09012692829348007, "compression_ratio": 1.6779661016949152, "no_speech_prob": 3.2376728995586745e-06}, {"id": 487, "seek": 197720, "start": 1981.44, "end": 1983.68, "text": " How confident are we that we know?", "tokens": [1012, 6679, 366, 321, 300, 321, 458, 30], "temperature": 0.0, "avg_logprob": -0.09012692829348007, "compression_ratio": 1.6779661016949152, "no_speech_prob": 3.2376728995586745e-06}, {"id": 488, "seek": 197720, "start": 1984.4, "end": 1986.4, "text": " Because if we're if we're like well", "tokens": [1436, 498, 321, 434, 498, 321, 434, 411, 731], "temperature": 0.0, "avg_logprob": -0.09012692829348007, "compression_ratio": 1.6779661016949152, "no_speech_prob": 3.2376728995586745e-06}, {"id": 489, "seek": 197720, "start": 1987.44, "end": 1989.28, "text": " We think they'll repay", "tokens": [492, 519, 436, 603, 27522], "temperature": 0.0, "avg_logprob": -0.09012692829348007, "compression_ratio": 1.6779661016949152, "no_speech_prob": 3.2376728995586745e-06}, {"id": 490, "seek": 197720, "start": 1989.28, "end": 1992.72, "text": " But we're not confident of that. We would probably want to give them less of a loan", "tokens": [583, 321, 434, 406, 6679, 295, 300, 13, 492, 576, 1391, 528, 281, 976, 552, 1570, 295, 257, 10529], "temperature": 0.0, "avg_logprob": -0.09012692829348007, "compression_ratio": 1.6779661016949152, "no_speech_prob": 3.2376728995586745e-06}, {"id": 491, "seek": 197720, "start": 1994.96, "end": 2001.3600000000001, "text": " And another thing that's very important is when we're then making a prediction so again, for example for for for credit", "tokens": [400, 1071, 551, 300, 311, 588, 1021, 307, 562, 321, 434, 550, 1455, 257, 17630, 370, 797, 11, 337, 1365, 337, 337, 337, 5397], "temperature": 0.0, "avg_logprob": -0.09012692829348007, "compression_ratio": 1.6779661016949152, "no_speech_prob": 3.2376728995586745e-06}, {"id": 492, "seek": 197720, "start": 2002.72, "end": 2005.2, "text": " Let's say you rejected that person's loan", "tokens": [961, 311, 584, 291, 15749, 300, 954, 311, 10529], "temperature": 0.0, "avg_logprob": -0.09012692829348007, "compression_ratio": 1.6779661016949152, "no_speech_prob": 3.2376728995586745e-06}, {"id": 493, "seek": 200520, "start": 2005.2, "end": 2006.48, "text": " Why?", "tokens": [1545, 30], "temperature": 0.0, "avg_logprob": -0.2639288035306064, "compression_ratio": 1.6757990867579908, "no_speech_prob": 9.972765838028863e-06}, {"id": 494, "seek": 200520, "start": 2006.48, "end": 2008.48, "text": " And a random forest will tell us", "tokens": [400, 257, 4974, 6719, 486, 980, 505], "temperature": 0.0, "avg_logprob": -0.2639288035306064, "compression_ratio": 1.6757990867579908, "no_speech_prob": 9.972765838028863e-06}, {"id": 495, "seek": 200520, "start": 2009.28, "end": 2013.76, "text": " What what is the what is the reason that we made a prediction and you'll see why all these things?", "tokens": [708, 437, 307, 264, 437, 307, 264, 1778, 300, 321, 1027, 257, 17630, 293, 291, 603, 536, 983, 439, 613, 721, 30], "temperature": 0.0, "avg_logprob": -0.2639288035306064, "compression_ratio": 1.6757990867579908, "no_speech_prob": 9.972765838028863e-06}, {"id": 496, "seek": 200520, "start": 2014.8, "end": 2019.28, "text": " Which columns are the strongest predictors? You've already seen that one right? That's the feature importance plot", "tokens": [3013, 13766, 366, 264, 16595, 6069, 830, 30, 509, 600, 1217, 1612, 300, 472, 558, 30, 663, 311, 264, 4111, 7379, 7542], "temperature": 0.0, "avg_logprob": -0.2639288035306064, "compression_ratio": 1.6757990867579908, "no_speech_prob": 9.972765838028863e-06}, {"id": 497, "seek": 200520, "start": 2020.4, "end": 2026.64, "text": " Which columns are effectively redundant with each other i.e. They're basically highly correlated with each other", "tokens": [3013, 13766, 366, 8659, 40997, 365, 1184, 661, 741, 13, 68, 13, 814, 434, 1936, 5405, 38574, 365, 1184, 661], "temperature": 0.0, "avg_logprob": -0.2639288035306064, "compression_ratio": 1.6757990867579908, "no_speech_prob": 9.972765838028863e-06}, {"id": 498, "seek": 200520, "start": 2029.04, "end": 2029.68, "text": " Um", "tokens": [3301], "temperature": 0.0, "avg_logprob": -0.2639288035306064, "compression_ratio": 1.6757990867579908, "no_speech_prob": 9.972765838028863e-06}, {"id": 499, "seek": 202968, "start": 2029.68, "end": 2036.8, "text": " And then one of the most important ones is you vary a column. How does it vary the predictions? So for example in your", "tokens": [400, 550, 472, 295, 264, 881, 1021, 2306, 307, 291, 10559, 257, 7738, 13, 1012, 775, 309, 10559, 264, 21264, 30, 407, 337, 1365, 294, 428], "temperature": 0.0, "avg_logprob": -0.14136557035808323, "compression_ratio": 1.627659574468085, "no_speech_prob": 1.012942630040925e-05}, {"id": 500, "seek": 202968, "start": 2037.76, "end": 2039.3600000000001, "text": " credit model", "tokens": [5397, 2316], "temperature": 0.0, "avg_logprob": -0.14136557035808323, "compression_ratio": 1.627659574468085, "no_speech_prob": 1.012942630040925e-05}, {"id": 501, "seek": 202968, "start": 2039.3600000000001, "end": 2041.3600000000001, "text": " How does your prediction of?", "tokens": [1012, 775, 428, 17630, 295, 30], "temperature": 0.0, "avg_logprob": -0.14136557035808323, "compression_ratio": 1.627659574468085, "no_speech_prob": 1.012942630040925e-05}, {"id": 502, "seek": 202968, "start": 2043.6000000000001, "end": 2044.88, "text": " Risk", "tokens": [45892], "temperature": 0.0, "avg_logprob": -0.14136557035808323, "compression_ratio": 1.627659574468085, "no_speech_prob": 1.012942630040925e-05}, {"id": 503, "seek": 202968, "start": 2044.88, "end": 2046.16, "text": " vary", "tokens": [10559], "temperature": 0.0, "avg_logprob": -0.14136557035808323, "compression_ratio": 1.627659574468085, "no_speech_prob": 1.012942630040925e-05}, {"id": 504, "seek": 202968, "start": 2046.16, "end": 2048.16, "text": " as you vary", "tokens": [382, 291, 10559], "temperature": 0.0, "avg_logprob": -0.14136557035808323, "compression_ratio": 1.627659574468085, "no_speech_prob": 1.012942630040925e-05}, {"id": 505, "seek": 202968, "start": 2048.2400000000002, "end": 2052.16, "text": " Well something that probably the regulator would want to know might be some you know, some protected", "tokens": [1042, 746, 300, 1391, 264, 36250, 576, 528, 281, 458, 1062, 312, 512, 291, 458, 11, 512, 10594], "temperature": 0.0, "avg_logprob": -0.14136557035808323, "compression_ratio": 1.627659574468085, "no_speech_prob": 1.012942630040925e-05}, {"id": 506, "seek": 202968, "start": 2052.96, "end": 2054.96, "text": " variable like you know", "tokens": [7006, 411, 291, 458], "temperature": 0.0, "avg_logprob": -0.14136557035808323, "compression_ratio": 1.627659574468085, "no_speech_prob": 1.012942630040925e-05}, {"id": 507, "seek": 205496, "start": 2054.96, "end": 2060.0, "text": " Race or some socio-demographic characteristics that you're not allowed to use in your model so they might check things like that", "tokens": [25908, 420, 512, 44303, 12, 10730, 12295, 10891, 300, 291, 434, 406, 4350, 281, 764, 294, 428, 2316, 370, 436, 1062, 1520, 721, 411, 300], "temperature": 0.0, "avg_logprob": -0.0860384420021293, "compression_ratio": 1.6742424242424243, "no_speech_prob": 1.0450867193867452e-05}, {"id": 508, "seek": 205496, "start": 2063.12, "end": 2068.08, "text": " For the first thing how confident are we in our predictions using a particular row of data?", "tokens": [1171, 264, 700, 551, 577, 6679, 366, 321, 294, 527, 21264, 1228, 257, 1729, 5386, 295, 1412, 30], "temperature": 0.0, "avg_logprob": -0.0860384420021293, "compression_ratio": 1.6742424242424243, "no_speech_prob": 1.0450867193867452e-05}, {"id": 509, "seek": 205496, "start": 2068.64, "end": 2070.64, "text": " There's a really simple thing we can do", "tokens": [821, 311, 257, 534, 2199, 551, 321, 393, 360], "temperature": 0.0, "avg_logprob": -0.0860384420021293, "compression_ratio": 1.6742424242424243, "no_speech_prob": 1.0450867193867452e-05}, {"id": 510, "seek": 205496, "start": 2071.04, "end": 2071.92, "text": " which is", "tokens": [597, 307], "temperature": 0.0, "avg_logprob": -0.0860384420021293, "compression_ratio": 1.6742424242424243, "no_speech_prob": 1.0450867193867452e-05}, {"id": 511, "seek": 205496, "start": 2071.92, "end": 2077.84, "text": " Remember how when we calculated our predictions manually we stacked up the predictions together and took their mean", "tokens": [5459, 577, 562, 321, 15598, 527, 21264, 16945, 321, 28867, 493, 264, 21264, 1214, 293, 1890, 641, 914], "temperature": 0.0, "avg_logprob": -0.0860384420021293, "compression_ratio": 1.6742424242424243, "no_speech_prob": 1.0450867193867452e-05}, {"id": 512, "seek": 205496, "start": 2079.04, "end": 2081.6, "text": " Well, what if you took their standard deviation instead?", "tokens": [1042, 11, 437, 498, 291, 1890, 641, 3832, 25163, 2602, 30], "temperature": 0.0, "avg_logprob": -0.0860384420021293, "compression_ratio": 1.6742424242424243, "no_speech_prob": 1.0450867193867452e-05}, {"id": 513, "seek": 208160, "start": 2081.6, "end": 2085.12, "text": " So if you stack up your predictions and take their standard deviation", "tokens": [407, 498, 291, 8630, 493, 428, 21264, 293, 747, 641, 3832, 25163], "temperature": 0.0, "avg_logprob": -0.24980692068735758, "compression_ratio": 1.8245614035087718, "no_speech_prob": 3.5007333281100728e-06}, {"id": 514, "seek": 208160, "start": 2086.3199999999997, "end": 2088.4, "text": " And if that standard deviation is high", "tokens": [400, 498, 300, 3832, 25163, 307, 1090], "temperature": 0.0, "avg_logprob": -0.24980692068735758, "compression_ratio": 1.8245614035087718, "no_speech_prob": 3.5007333281100728e-06}, {"id": 515, "seek": 208160, "start": 2088.88, "end": 2092.3199999999997, "text": " That means all of them all of the trees are predicting something different", "tokens": [663, 1355, 439, 295, 552, 439, 295, 264, 5852, 366, 32884, 746, 819], "temperature": 0.0, "avg_logprob": -0.24980692068735758, "compression_ratio": 1.8245614035087718, "no_speech_prob": 3.5007333281100728e-06}, {"id": 516, "seek": 208160, "start": 2093.2799999999997, "end": 2095.68, "text": " And that suggests that we don't really know what we're doing", "tokens": [400, 300, 13409, 300, 321, 500, 380, 534, 458, 437, 321, 434, 884], "temperature": 0.0, "avg_logprob": -0.24980692068735758, "compression_ratio": 1.8245614035087718, "no_speech_prob": 3.5007333281100728e-06}, {"id": 517, "seek": 208160, "start": 2096.08, "end": 2101.12, "text": " And so that would happen if different subsets of the data end up giving completely different trees", "tokens": [400, 370, 300, 576, 1051, 498, 819, 2090, 1385, 295, 264, 1412, 917, 493, 2902, 2584, 819, 5852], "temperature": 0.0, "avg_logprob": -0.24980692068735758, "compression_ratio": 1.8245614035087718, "no_speech_prob": 3.5007333281100728e-06}, {"id": 518, "seek": 208160, "start": 2101.8399999999997, "end": 2103.36, "text": " for this", "tokens": [337, 341], "temperature": 0.0, "avg_logprob": -0.24980692068735758, "compression_ratio": 1.8245614035087718, "no_speech_prob": 3.5007333281100728e-06}, {"id": 519, "seek": 208160, "start": 2103.36, "end": 2105.36, "text": " particular row", "tokens": [1729, 5386], "temperature": 0.0, "avg_logprob": -0.24980692068735758, "compression_ratio": 1.8245614035087718, "no_speech_prob": 3.5007333281100728e-06}, {"id": 520, "seek": 208160, "start": 2105.92, "end": 2108.96, "text": " So there's like a really simple thing you can do", "tokens": [407, 456, 311, 411, 257, 534, 2199, 551, 291, 393, 360], "temperature": 0.0, "avg_logprob": -0.24980692068735758, "compression_ratio": 1.8245614035087718, "no_speech_prob": 3.5007333281100728e-06}, {"id": 521, "seek": 210896, "start": 2108.96, "end": 2111.28, "text": " To get a sense of your prediction confidence", "tokens": [1407, 483, 257, 2020, 295, 428, 17630, 6687], "temperature": 0.0, "avg_logprob": -0.35826066199769363, "compression_ratio": 1.591093117408907, "no_speech_prob": 6.33907029623515e-06}, {"id": 522, "seek": 210896, "start": 2112.32, "end": 2114.7200000000003, "text": " Okay feature importance we've already discussed", "tokens": [1033, 4111, 7379, 321, 600, 1217, 7152], "temperature": 0.0, "avg_logprob": -0.35826066199769363, "compression_ratio": 1.591093117408907, "no_speech_prob": 6.33907029623515e-06}, {"id": 523, "seek": 210896, "start": 2118.7200000000003, "end": 2124.96, "text": " After I do feature importance, you know, like I said when I had the what 7 000 or so columns I got rid of like all but 30", "tokens": [2381, 286, 360, 4111, 7379, 11, 291, 458, 11, 411, 286, 848, 562, 286, 632, 264, 437, 1614, 13711, 420, 370, 13766, 286, 658, 3973, 295, 411, 439, 457, 2217], "temperature": 0.0, "avg_logprob": -0.35826066199769363, "compression_ratio": 1.591093117408907, "no_speech_prob": 6.33907029623515e-06}, {"id": 524, "seek": 210896, "start": 2126.08, "end": 2129.76, "text": " That doesn't tend to improve the predictions of your random forest very much", "tokens": [663, 1177, 380, 3928, 281, 3470, 264, 21264, 295, 428, 4974, 6719, 588, 709], "temperature": 0.0, "avg_logprob": -0.35826066199769363, "compression_ratio": 1.591093117408907, "no_speech_prob": 6.33907029623515e-06}, {"id": 525, "seek": 210896, "start": 2130.7200000000003, "end": 2133.76, "text": " if at all, but it certainly helps like", "tokens": [498, 412, 439, 11, 457, 309, 3297, 3665, 411], "temperature": 0.0, "avg_logprob": -0.35826066199769363, "compression_ratio": 1.591093117408907, "no_speech_prob": 6.33907029623515e-06}, {"id": 526, "seek": 210896, "start": 2135.2, "end": 2137.68, "text": " You know kind of logistically thinking about what you're doing", "tokens": [509, 458, 733, 295, 3565, 20458, 1953, 466, 437, 291, 434, 884], "temperature": 0.0, "avg_logprob": -0.35826066199769363, "compression_ratio": 1.591093117408907, "no_speech_prob": 6.33907029623515e-06}, {"id": 527, "seek": 213768, "start": 2137.68, "end": 2140.24, "text": " You're all kind of logistically thinking about cleaning up the data", "tokens": [509, 434, 439, 733, 295, 3565, 20458, 1953, 466, 8924, 493, 264, 1412], "temperature": 0.0, "avg_logprob": -0.1090927639523068, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.296196145034628e-06}, {"id": 528, "seek": 213768, "start": 2140.3199999999997, "end": 2145.14, "text": " You can focus on cleaning those 30 columns stuff like that. So I tend to remove the low importance variables", "tokens": [509, 393, 1879, 322, 8924, 729, 2217, 13766, 1507, 411, 300, 13, 407, 286, 3928, 281, 4159, 264, 2295, 7379, 9102], "temperature": 0.0, "avg_logprob": -0.1090927639523068, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.296196145034628e-06}, {"id": 529, "seek": 213768, "start": 2148.96, "end": 2153.9199999999996, "text": " I'm going to skip over this bit about removing redundant features because it's a little bit outside what we're talking about", "tokens": [286, 478, 516, 281, 10023, 670, 341, 857, 466, 12720, 40997, 4122, 570, 309, 311, 257, 707, 857, 2380, 437, 321, 434, 1417, 466], "temperature": 0.0, "avg_logprob": -0.1090927639523068, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.296196145034628e-06}, {"id": 530, "seek": 213768, "start": 2153.9199999999996, "end": 2155.9199999999996, "text": " But definitely check it out in the book", "tokens": [583, 2138, 1520, 309, 484, 294, 264, 1446], "temperature": 0.0, "avg_logprob": -0.1090927639523068, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.296196145034628e-06}, {"id": 531, "seek": 213768, "start": 2156.08, "end": 2158.08, "text": " Something called a dendrogram", "tokens": [6595, 1219, 257, 274, 521, 340, 1342], "temperature": 0.0, "avg_logprob": -0.1090927639523068, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.296196145034628e-06}, {"id": 532, "seek": 213768, "start": 2159.12, "end": 2163.52, "text": " But what I do want to mention is is the partial dependence this is the thing which says", "tokens": [583, 437, 286, 360, 528, 281, 2152, 307, 307, 264, 14641, 31704, 341, 307, 264, 551, 597, 1619], "temperature": 0.0, "avg_logprob": -0.1090927639523068, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.296196145034628e-06}, {"id": 533, "seek": 216352, "start": 2163.52, "end": 2166.56, "text": " Um, what is the relationship?", "tokens": [3301, 11, 437, 307, 264, 2480, 30], "temperature": 0.0, "avg_logprob": -0.11828791861440621, "compression_ratio": 1.7479338842975207, "no_speech_prob": 4.936865934723755e-06}, {"id": 534, "seek": 216352, "start": 2167.84, "end": 2169.84, "text": " between a", "tokens": [1296, 257], "temperature": 0.0, "avg_logprob": -0.11828791861440621, "compression_ratio": 1.7479338842975207, "no_speech_prob": 4.936865934723755e-06}, {"id": 535, "seek": 216352, "start": 2170.16, "end": 2172.16, "text": " Column and the dependent variable", "tokens": [4004, 16449, 293, 264, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.11828791861440621, "compression_ratio": 1.7479338842975207, "no_speech_prob": 4.936865934723755e-06}, {"id": 536, "seek": 216352, "start": 2172.8, "end": 2177.84, "text": " And so this is something called a partial dependence plot. Now. This one's actually not specific to random forests", "tokens": [400, 370, 341, 307, 746, 1219, 257, 14641, 31704, 7542, 13, 823, 13, 639, 472, 311, 767, 406, 2685, 281, 4974, 21700], "temperature": 0.0, "avg_logprob": -0.11828791861440621, "compression_ratio": 1.7479338842975207, "no_speech_prob": 4.936865934723755e-06}, {"id": 537, "seek": 216352, "start": 2178.56, "end": 2182.8, "text": " Um a partial dependence plot is something you can do for basically any machine learning model", "tokens": [3301, 257, 14641, 31704, 7542, 307, 746, 291, 393, 360, 337, 1936, 604, 3479, 2539, 2316], "temperature": 0.0, "avg_logprob": -0.11828791861440621, "compression_ratio": 1.7479338842975207, "no_speech_prob": 4.936865934723755e-06}, {"id": 538, "seek": 216352, "start": 2183.7599999999998, "end": 2187.04, "text": " Um, let's first of all look at one and then talk about how we make it", "tokens": [3301, 11, 718, 311, 700, 295, 439, 574, 412, 472, 293, 550, 751, 466, 577, 321, 652, 309], "temperature": 0.0, "avg_logprob": -0.11828791861440621, "compression_ratio": 1.7479338842975207, "no_speech_prob": 4.936865934723755e-06}, {"id": 539, "seek": 216352, "start": 2188.32, "end": 2192.0, "text": " So in this data set we're looking at the relationship we're looking at", "tokens": [407, 294, 341, 1412, 992, 321, 434, 1237, 412, 264, 2480, 321, 434, 1237, 412], "temperature": 0.0, "avg_logprob": -0.11828791861440621, "compression_ratio": 1.7479338842975207, "no_speech_prob": 4.936865934723755e-06}, {"id": 540, "seek": 219200, "start": 2192.0, "end": 2198.88, "text": " Uh the sale price at auction of heavy industrial equipment like bulldozers. This is specifically the", "tokens": [4019, 264, 8680, 3218, 412, 24139, 295, 4676, 9987, 5927, 411, 4693, 2595, 41698, 13, 639, 307, 4682, 264], "temperature": 0.0, "avg_logprob": -0.13567021618718686, "compression_ratio": 1.71875, "no_speech_prob": 3.50080631505989e-06}, {"id": 541, "seek": 219200, "start": 2199.6, "end": 2201.92, "text": " blue books for bulldozers calc competition", "tokens": [3344, 3642, 337, 4693, 2595, 41698, 2104, 66, 6211], "temperature": 0.0, "avg_logprob": -0.13567021618718686, "compression_ratio": 1.71875, "no_speech_prob": 3.50080631505989e-06}, {"id": 542, "seek": 219200, "start": 2203.12, "end": 2208.56, "text": " And a partial dependence plot between the year that the bulldozer or whatever was made", "tokens": [400, 257, 14641, 31704, 7542, 1296, 264, 1064, 300, 264, 4693, 2595, 4527, 420, 2035, 390, 1027], "temperature": 0.0, "avg_logprob": -0.13567021618718686, "compression_ratio": 1.71875, "no_speech_prob": 3.50080631505989e-06}, {"id": 543, "seek": 219200, "start": 2209.52, "end": 2212.48, "text": " And the price that was sold for this is actually the log price", "tokens": [400, 264, 3218, 300, 390, 3718, 337, 341, 307, 767, 264, 3565, 3218], "temperature": 0.0, "avg_logprob": -0.13567021618718686, "compression_ratio": 1.71875, "no_speech_prob": 3.50080631505989e-06}, {"id": 544, "seek": 219200, "start": 2213.36, "end": 2215.28, "text": " Is that it goes up?", "tokens": [1119, 300, 309, 1709, 493, 30], "temperature": 0.0, "avg_logprob": -0.13567021618718686, "compression_ratio": 1.71875, "no_speech_prob": 3.50080631505989e-06}, {"id": 545, "seek": 219200, "start": 2215.28, "end": 2219.2, "text": " More recent bulldozers more recently made bulldozers are more expensive", "tokens": [5048, 5162, 4693, 2595, 41698, 544, 3938, 1027, 4693, 2595, 41698, 366, 544, 5124], "temperature": 0.0, "avg_logprob": -0.13567021618718686, "compression_ratio": 1.71875, "no_speech_prob": 3.50080631505989e-06}, {"id": 546, "seek": 221920, "start": 2219.2, "end": 2222.48, "text": " Um, and as you go back back to older and older build at bulldozers", "tokens": [3301, 11, 293, 382, 291, 352, 646, 646, 281, 4906, 293, 4906, 1322, 412, 4693, 2595, 41698], "temperature": 0.0, "avg_logprob": -0.24640889321604081, "compression_ratio": 1.6017699115044248, "no_speech_prob": 3.5007512906304328e-06}, {"id": 547, "seek": 221920, "start": 2223.6, "end": 2227.7599999999998, "text": " They're less and less expensive to a point and maybe these ones are some", "tokens": [814, 434, 1570, 293, 1570, 5124, 281, 257, 935, 293, 1310, 613, 2306, 366, 512], "temperature": 0.0, "avg_logprob": -0.24640889321604081, "compression_ratio": 1.6017699115044248, "no_speech_prob": 3.5007512906304328e-06}, {"id": 548, "seek": 221920, "start": 2228.56, "end": 2230.7999999999997, "text": " Old classic bulldozers you pay a bit extra for", "tokens": [8633, 7230, 4693, 2595, 41698, 291, 1689, 257, 857, 2857, 337], "temperature": 0.0, "avg_logprob": -0.24640889321604081, "compression_ratio": 1.6017699115044248, "no_speech_prob": 3.5007512906304328e-06}, {"id": 549, "seek": 221920, "start": 2232.16, "end": 2233.2, "text": " now", "tokens": [586], "temperature": 0.0, "avg_logprob": -0.24640889321604081, "compression_ratio": 1.6017699115044248, "no_speech_prob": 3.5007512906304328e-06}, {"id": 550, "seek": 221920, "start": 2233.2, "end": 2239.04, "text": " You might think that you could easily create this plot by simply looking at your data at each year", "tokens": [509, 1062, 519, 300, 291, 727, 3612, 1884, 341, 7542, 538, 2935, 1237, 412, 428, 1412, 412, 1184, 1064], "temperature": 0.0, "avg_logprob": -0.24640889321604081, "compression_ratio": 1.6017699115044248, "no_speech_prob": 3.5007512906304328e-06}, {"id": 551, "seek": 221920, "start": 2239.52, "end": 2241.52, "text": " and taking the average sale price", "tokens": [293, 1940, 264, 4274, 8680, 3218], "temperature": 0.0, "avg_logprob": -0.24640889321604081, "compression_ratio": 1.6017699115044248, "no_speech_prob": 3.5007512906304328e-06}, {"id": 552, "seek": 221920, "start": 2242.64, "end": 2244.64, "text": " But that doesn't really work very well", "tokens": [583, 300, 1177, 380, 534, 589, 588, 731], "temperature": 0.0, "avg_logprob": -0.24640889321604081, "compression_ratio": 1.6017699115044248, "no_speech_prob": 3.5007512906304328e-06}, {"id": 553, "seek": 224464, "start": 2244.64, "end": 2249.44, "text": " I mean it kind of does but it kind of doesn't let me give you an example", "tokens": [286, 914, 309, 733, 295, 775, 457, 309, 733, 295, 1177, 380, 718, 385, 976, 291, 364, 1365], "temperature": 0.0, "avg_logprob": -0.17951836877939653, "compression_ratio": 1.7435897435897436, "no_speech_prob": 2.6425232135807164e-06}, {"id": 554, "seek": 224464, "start": 2249.44, "end": 2255.8399999999997, "text": " It turns out that one of the biggest predictors of sale price for industrial equipment is whether it has air conditioning", "tokens": [467, 4523, 484, 300, 472, 295, 264, 3880, 6069, 830, 295, 8680, 3218, 337, 9987, 5927, 307, 1968, 309, 575, 1988, 21901], "temperature": 0.0, "avg_logprob": -0.17951836877939653, "compression_ratio": 1.7435897435897436, "no_speech_prob": 2.6425232135807164e-06}, {"id": 555, "seek": 224464, "start": 2257.3599999999997, "end": 2263.68, "text": " Um, and so air conditioning is you know, it's an expensive thing to add and it makes the equipment more expensive to buy", "tokens": [3301, 11, 293, 370, 1988, 21901, 307, 291, 458, 11, 309, 311, 364, 5124, 551, 281, 909, 293, 309, 1669, 264, 5927, 544, 5124, 281, 2256], "temperature": 0.0, "avg_logprob": -0.17951836877939653, "compression_ratio": 1.7435897435897436, "no_speech_prob": 2.6425232135807164e-06}, {"id": 556, "seek": 224464, "start": 2264.64, "end": 2269.2, "text": " And most things didn't have air conditioning back in the 60s and 70s and most of them do now", "tokens": [400, 881, 721, 994, 380, 362, 1988, 21901, 646, 294, 264, 4060, 82, 293, 5285, 82, 293, 881, 295, 552, 360, 586], "temperature": 0.0, "avg_logprob": -0.17951836877939653, "compression_ratio": 1.7435897435897436, "no_speech_prob": 2.6425232135807164e-06}, {"id": 557, "seek": 226920, "start": 2269.2, "end": 2273.7599999999998, "text": " So if you plot the relationship between year made and price", "tokens": [407, 498, 291, 7542, 264, 2480, 1296, 1064, 1027, 293, 3218], "temperature": 0.0, "avg_logprob": -0.24106530582203584, "compression_ratio": 1.6300813008130082, "no_speech_prob": 2.857270374079235e-06}, {"id": 558, "seek": 226920, "start": 2274.0, "end": 2276.48, "text": " You're actually going to be seeing a whole bunch of", "tokens": [509, 434, 767, 516, 281, 312, 2577, 257, 1379, 3840, 295], "temperature": 0.0, "avg_logprob": -0.24106530582203584, "compression_ratio": 1.6300813008130082, "no_speech_prob": 2.857270374079235e-06}, {"id": 559, "seek": 226920, "start": 2277.2799999999997, "end": 2279.9199999999996, "text": " When you know how popular was air conditioning", "tokens": [1133, 291, 458, 577, 3743, 390, 1988, 21901], "temperature": 0.0, "avg_logprob": -0.24106530582203584, "compression_ratio": 1.6300813008130082, "no_speech_prob": 2.857270374079235e-06}, {"id": 560, "seek": 226920, "start": 2280.7999999999997, "end": 2284.64, "text": " Right, so you get this this cross correlation going on that we just want to know", "tokens": [1779, 11, 370, 291, 483, 341, 341, 3278, 20009, 516, 322, 300, 321, 445, 528, 281, 458], "temperature": 0.0, "avg_logprob": -0.24106530582203584, "compression_ratio": 1.6300813008130082, "no_speech_prob": 2.857270374079235e-06}, {"id": 561, "seek": 226920, "start": 2284.64, "end": 2289.2, "text": " No, what's what's just the impact of of the year? It was made all else being equal", "tokens": [883, 11, 437, 311, 437, 311, 445, 264, 2712, 295, 295, 264, 1064, 30, 467, 390, 1027, 439, 1646, 885, 2681], "temperature": 0.0, "avg_logprob": -0.24106530582203584, "compression_ratio": 1.6300813008130082, "no_speech_prob": 2.857270374079235e-06}, {"id": 562, "seek": 226920, "start": 2291.12, "end": 2292.08, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.24106530582203584, "compression_ratio": 1.6300813008130082, "no_speech_prob": 2.857270374079235e-06}, {"id": 563, "seek": 226920, "start": 2292.08, "end": 2295.52, "text": " There's actually a really easy way to do that which is we take our data set", "tokens": [821, 311, 767, 257, 534, 1858, 636, 281, 360, 300, 597, 307, 321, 747, 527, 1412, 992], "temperature": 0.0, "avg_logprob": -0.24106530582203584, "compression_ratio": 1.6300813008130082, "no_speech_prob": 2.857270374079235e-06}, {"id": 564, "seek": 229552, "start": 2295.52, "end": 2299.36, "text": " We take the we we leave it exactly as it is to just use the training data set", "tokens": [492, 747, 264, 321, 321, 1856, 309, 2293, 382, 309, 307, 281, 445, 764, 264, 3097, 1412, 992], "temperature": 0.0, "avg_logprob": -0.27835655212402344, "compression_ratio": 1.7857142857142858, "no_speech_prob": 1.5936020645312965e-05}, {"id": 565, "seek": 229552, "start": 2299.36, "end": 2304.56, "text": " But we take every single row and for the year made column. We set it to 1950", "tokens": [583, 321, 747, 633, 2167, 5386, 293, 337, 264, 1064, 1027, 7738, 13, 492, 992, 309, 281, 18141], "temperature": 0.0, "avg_logprob": -0.27835655212402344, "compression_ratio": 1.7857142857142858, "no_speech_prob": 1.5936020645312965e-05}, {"id": 566, "seek": 229552, "start": 2305.6, "end": 2312.08, "text": " And so then we predict for every row. What would the sale price of that have been if it was made in 1950?", "tokens": [400, 370, 550, 321, 6069, 337, 633, 5386, 13, 708, 576, 264, 8680, 3218, 295, 300, 362, 668, 498, 309, 390, 1027, 294, 18141, 30], "temperature": 0.0, "avg_logprob": -0.27835655212402344, "compression_ratio": 1.7857142857142858, "no_speech_prob": 1.5936020645312965e-05}, {"id": 567, "seek": 229552, "start": 2313.12, "end": 2319.2, "text": " And then we repeat it for 1951 and they repeat it for 1952 and so forth and then we plot the averages", "tokens": [400, 550, 321, 7149, 309, 337, 10858, 16, 293, 436, 7149, 309, 337, 10858, 17, 293, 370, 5220, 293, 550, 321, 7542, 264, 42257], "temperature": 0.0, "avg_logprob": -0.27835655212402344, "compression_ratio": 1.7857142857142858, "no_speech_prob": 1.5936020645312965e-05}, {"id": 568, "seek": 229552, "start": 2320.0, "end": 2323.84, "text": " And that does exactly what I just said. Remember I said the special sale price was 1950", "tokens": [400, 300, 775, 2293, 437, 286, 445, 848, 13, 5459, 286, 848, 264, 2121, 8680, 3218, 390, 1294, 2803], "temperature": 0.0, "avg_logprob": -0.27835655212402344, "compression_ratio": 1.7857142857142858, "no_speech_prob": 1.5936020645312965e-05}, {"id": 569, "seek": 232384, "start": 2323.84, "end": 2327.28, "text": " That's exactly what I just said. Remember I said the special words all else being equal", "tokens": [663, 311, 2293, 437, 286, 445, 848, 13, 5459, 286, 848, 264, 2121, 2283, 439, 1646, 885, 2681], "temperature": 0.0, "avg_logprob": -0.09270271600461473, "compression_ratio": 1.6707818930041152, "no_speech_prob": 8.013257684069686e-06}, {"id": 570, "seek": 232384, "start": 2328.0, "end": 2330.6400000000003, "text": " This is setting everything else equal. It's the everything else is", "tokens": [639, 307, 3287, 1203, 1646, 2681, 13, 467, 311, 264, 1203, 1646, 307], "temperature": 0.0, "avg_logprob": -0.09270271600461473, "compression_ratio": 1.6707818930041152, "no_speech_prob": 8.013257684069686e-06}, {"id": 571, "seek": 232384, "start": 2331.28, "end": 2333.28, "text": " The data as it actually occurred", "tokens": [440, 1412, 382, 309, 767, 11068], "temperature": 0.0, "avg_logprob": -0.09270271600461473, "compression_ratio": 1.6707818930041152, "no_speech_prob": 8.013257684069686e-06}, {"id": 572, "seek": 232384, "start": 2333.76, "end": 2335.76, "text": " And we're only varying year made", "tokens": [400, 321, 434, 787, 22984, 1064, 1027], "temperature": 0.0, "avg_logprob": -0.09270271600461473, "compression_ratio": 1.6707818930041152, "no_speech_prob": 8.013257684069686e-06}, {"id": 573, "seek": 232384, "start": 2336.56, "end": 2338.56, "text": " And that's what a partial dependence plot is", "tokens": [400, 300, 311, 437, 257, 14641, 31704, 7542, 307], "temperature": 0.0, "avg_logprob": -0.09270271600461473, "compression_ratio": 1.6707818930041152, "no_speech_prob": 8.013257684069686e-06}, {"id": 574, "seek": 232384, "start": 2339.6000000000004, "end": 2341.6000000000004, "text": " That works just as well for deep learning", "tokens": [663, 1985, 445, 382, 731, 337, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.09270271600461473, "compression_ratio": 1.6707818930041152, "no_speech_prob": 8.013257684069686e-06}, {"id": 575, "seek": 232384, "start": 2342.0, "end": 2346.08, "text": " Or gradient boosting trees or logistic regressions or whatever", "tokens": [1610, 16235, 43117, 5852, 420, 3565, 3142, 1121, 735, 626, 420, 2035], "temperature": 0.0, "avg_logprob": -0.09270271600461473, "compression_ratio": 1.6707818930041152, "no_speech_prob": 8.013257684069686e-06}, {"id": 576, "seek": 232384, "start": 2346.8, "end": 2348.56, "text": " It's a really", "tokens": [467, 311, 257, 534], "temperature": 0.0, "avg_logprob": -0.09270271600461473, "compression_ratio": 1.6707818930041152, "no_speech_prob": 8.013257684069686e-06}, {"id": 577, "seek": 232384, "start": 2348.56, "end": 2350.56, "text": " cool thing you can do", "tokens": [1627, 551, 291, 393, 360], "temperature": 0.0, "avg_logprob": -0.09270271600461473, "compression_ratio": 1.6707818930041152, "no_speech_prob": 8.013257684069686e-06}, {"id": 578, "seek": 235056, "start": 2350.56, "end": 2356.48, "text": " Um, and you can do more than one column at a time, you know, you can do two way", "tokens": [3301, 11, 293, 291, 393, 360, 544, 813, 472, 7738, 412, 257, 565, 11, 291, 458, 11, 291, 393, 360, 732, 636], "temperature": 0.0, "avg_logprob": -0.14452528953552246, "compression_ratio": 1.6578947368421053, "no_speech_prob": 1.0451077287143562e-05}, {"id": 579, "seek": 235056, "start": 2357.44, "end": 2359.44, "text": " partial dependence plots", "tokens": [14641, 31704, 28609], "temperature": 0.0, "avg_logprob": -0.14452528953552246, "compression_ratio": 1.6578947368421053, "no_speech_prob": 1.0451077287143562e-05}, {"id": 580, "seek": 235056, "start": 2359.52, "end": 2361.12, "text": " for example", "tokens": [337, 1365], "temperature": 0.0, "avg_logprob": -0.14452528953552246, "compression_ratio": 1.6578947368421053, "no_speech_prob": 1.0451077287143562e-05}, {"id": 581, "seek": 235056, "start": 2361.12, "end": 2364.4, "text": " other one, um, okay, so then another one I mentioned was", "tokens": [661, 472, 11, 1105, 11, 1392, 11, 370, 550, 1071, 472, 286, 2835, 390], "temperature": 0.0, "avg_logprob": -0.14452528953552246, "compression_ratio": 1.6578947368421053, "no_speech_prob": 1.0451077287143562e-05}, {"id": 582, "seek": 235056, "start": 2365.36, "end": 2373.6, "text": " Can you describe why a particular prediction was made? So how did you decide for this particular row?", "tokens": [1664, 291, 6786, 983, 257, 1729, 17630, 390, 1027, 30, 407, 577, 630, 291, 4536, 337, 341, 1729, 5386, 30], "temperature": 0.0, "avg_logprob": -0.14452528953552246, "compression_ratio": 1.6578947368421053, "no_speech_prob": 1.0451077287143562e-05}, {"id": 583, "seek": 235056, "start": 2374.56, "end": 2376.56, "text": " to predict this particular value", "tokens": [281, 6069, 341, 1729, 2158], "temperature": 0.0, "avg_logprob": -0.14452528953552246, "compression_ratio": 1.6578947368421053, "no_speech_prob": 1.0451077287143562e-05}, {"id": 584, "seek": 235056, "start": 2377.52, "end": 2379.04, "text": " and um", "tokens": [293, 1105], "temperature": 0.0, "avg_logprob": -0.14452528953552246, "compression_ratio": 1.6578947368421053, "no_speech_prob": 1.0451077287143562e-05}, {"id": 585, "seek": 237904, "start": 2379.04, "end": 2381.84, "text": " This is actually pretty easy to do there's a thing called tree interpreter", "tokens": [639, 307, 767, 1238, 1858, 281, 360, 456, 311, 257, 551, 1219, 4230, 34132], "temperature": 0.0, "avg_logprob": -0.09007735252380371, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.1658610674203373e-05}, {"id": 586, "seek": 237904, "start": 2381.84, "end": 2385.2799999999997, "text": " But we could you could easily create this in about half a dozen lines of code", "tokens": [583, 321, 727, 291, 727, 3612, 1884, 341, 294, 466, 1922, 257, 16654, 3876, 295, 3089], "temperature": 0.0, "avg_logprob": -0.09007735252380371, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.1658610674203373e-05}, {"id": 587, "seek": 237904, "start": 2385.92, "end": 2387.92, "text": " All we do", "tokens": [1057, 321, 360], "temperature": 0.0, "avg_logprob": -0.09007735252380371, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.1658610674203373e-05}, {"id": 588, "seek": 237904, "start": 2388.96, "end": 2390.32, "text": " Is", "tokens": [1119], "temperature": 0.0, "avg_logprob": -0.09007735252380371, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.1658610674203373e-05}, {"id": 589, "seek": 237904, "start": 2390.32, "end": 2392.24, "text": " We're saying okay", "tokens": [492, 434, 1566, 1392], "temperature": 0.0, "avg_logprob": -0.09007735252380371, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.1658610674203373e-05}, {"id": 590, "seek": 237904, "start": 2392.24, "end": 2394.64, "text": " This customer's come in they've asked her alone", "tokens": [639, 5474, 311, 808, 294, 436, 600, 2351, 720, 3312], "temperature": 0.0, "avg_logprob": -0.09007735252380371, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.1658610674203373e-05}, {"id": 591, "seek": 237904, "start": 2395.36, "end": 2399.2799999999997, "text": " We've put in all of their data through the random forest. It's about out of prediction", "tokens": [492, 600, 829, 294, 439, 295, 641, 1412, 807, 264, 4974, 6719, 13, 467, 311, 466, 484, 295, 17630], "temperature": 0.0, "avg_logprob": -0.09007735252380371, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.1658610674203373e-05}, {"id": 592, "seek": 237904, "start": 2400.24, "end": 2403.7599999999998, "text": " We can actually have a look and say okay. Well that in tree number one", "tokens": [492, 393, 767, 362, 257, 574, 293, 584, 1392, 13, 1042, 300, 294, 4230, 1230, 472], "temperature": 0.0, "avg_logprob": -0.09007735252380371, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.1658610674203373e-05}, {"id": 593, "seek": 237904, "start": 2404.64, "end": 2407.6, "text": " What's the path that went down through the tree to get to the leaf node?", "tokens": [708, 311, 264, 3100, 300, 1437, 760, 807, 264, 4230, 281, 483, 281, 264, 10871, 9984, 30], "temperature": 0.0, "avg_logprob": -0.09007735252380371, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.1658610674203373e-05}, {"id": 594, "seek": 240760, "start": 2407.6, "end": 2412.96, "text": " And we can say oh well first of all it looked at sex and then it looked at postcode and then it looked at income", "tokens": [400, 321, 393, 584, 1954, 731, 700, 295, 439, 309, 2956, 412, 3260, 293, 550, 309, 2956, 412, 2183, 22332, 293, 550, 309, 2956, 412, 5742], "temperature": 0.0, "avg_logprob": -0.2645919215571773, "compression_ratio": 1.7704918032786885, "no_speech_prob": 2.642555273268954e-06}, {"id": 595, "seek": 240760, "start": 2413.44, "end": 2415.44, "text": " and so we can see", "tokens": [293, 370, 321, 393, 536], "temperature": 0.0, "avg_logprob": -0.2645919215571773, "compression_ratio": 1.7704918032786885, "no_speech_prob": 2.642555273268954e-06}, {"id": 596, "seek": 240760, "start": 2416.48, "end": 2421.2, "text": " Exactly in tree number one which variables were used and what was the", "tokens": [7587, 294, 4230, 1230, 472, 597, 9102, 645, 1143, 293, 437, 390, 264], "temperature": 0.0, "avg_logprob": -0.2645919215571773, "compression_ratio": 1.7704918032786885, "no_speech_prob": 2.642555273268954e-06}, {"id": 597, "seek": 240760, "start": 2422.08, "end": 2424.08, "text": " Change in jenny for each one", "tokens": [15060, 294, 361, 268, 1634, 337, 1184, 472], "temperature": 0.0, "avg_logprob": -0.2645919215571773, "compression_ratio": 1.7704918032786885, "no_speech_prob": 2.642555273268954e-06}, {"id": 598, "seek": 240760, "start": 2424.16, "end": 2428.64, "text": " And then we can do the same in tree two same in tree three time three four does this sound familiar?", "tokens": [400, 550, 321, 393, 360, 264, 912, 294, 4230, 732, 912, 294, 4230, 1045, 565, 1045, 1451, 775, 341, 1626, 4963, 30], "temperature": 0.0, "avg_logprob": -0.2645919215571773, "compression_ratio": 1.7704918032786885, "no_speech_prob": 2.642555273268954e-06}, {"id": 599, "seek": 240760, "start": 2429.52, "end": 2434.0, "text": " It's basically the same as our feature importance plot, right? But it's just for this one row of data", "tokens": [467, 311, 1936, 264, 912, 382, 527, 4111, 7379, 7542, 11, 558, 30, 583, 309, 311, 445, 337, 341, 472, 5386, 295, 1412], "temperature": 0.0, "avg_logprob": -0.2645919215571773, "compression_ratio": 1.7704918032786885, "no_speech_prob": 2.642555273268954e-06}, {"id": 600, "seek": 243400, "start": 2434.0, "end": 2438.8, "text": " And so that will tell you basically the feature importances for that one particular prediction", "tokens": [400, 370, 300, 486, 980, 291, 1936, 264, 4111, 974, 2676, 337, 300, 472, 1729, 17630], "temperature": 0.0, "avg_logprob": -0.2771062159883803, "compression_ratio": 1.6127167630057804, "no_speech_prob": 2.902228288803599e-06}, {"id": 601, "seek": 243400, "start": 2439.44, "end": 2441.44, "text": " And so then we can plot them", "tokens": [400, 370, 550, 321, 393, 7542, 552], "temperature": 0.0, "avg_logprob": -0.2771062159883803, "compression_ratio": 1.6127167630057804, "no_speech_prob": 2.902228288803599e-06}, {"id": 602, "seek": 243400, "start": 2442.24, "end": 2445.36, "text": " Like this. So for example, this is an example of um", "tokens": [1743, 341, 13, 407, 337, 1365, 11, 341, 307, 364, 1365, 295, 1105], "temperature": 0.0, "avg_logprob": -0.2771062159883803, "compression_ratio": 1.6127167630057804, "no_speech_prob": 2.902228288803599e-06}, {"id": 603, "seek": 243400, "start": 2446.16, "end": 2448.16, "text": " an auction price prediction", "tokens": [364, 24139, 3218, 17630], "temperature": 0.0, "avg_logprob": -0.2771062159883803, "compression_ratio": 1.6127167630057804, "no_speech_prob": 2.902228288803599e-06}, {"id": 604, "seek": 243400, "start": 2449.52, "end": 2454.24, "text": " And according to this plot, you know, so we predicted that the net would be", "tokens": [400, 4650, 281, 341, 7542, 11, 291, 458, 11, 370, 321, 19147, 300, 264, 2533, 576, 312], "temperature": 0.0, "avg_logprob": -0.2771062159883803, "compression_ratio": 1.6127167630057804, "no_speech_prob": 2.902228288803599e-06}, {"id": 605, "seek": 245424, "start": 2454.24, "end": 2462.24, "text": " Uh, oh this is just a change from from uh, so I don't actually know what the price is but this is this is how much each one", "tokens": [4019, 11, 1954, 341, 307, 445, 257, 1319, 490, 490, 2232, 11, 370, 286, 500, 380, 767, 458, 437, 264, 3218, 307, 457, 341, 307, 341, 307, 577, 709, 1184, 472], "temperature": 0.0, "avg_logprob": -0.25825534246664134, "compression_ratio": 1.7448559670781894, "no_speech_prob": 5.862452326255152e-06}, {"id": 606, "seek": 245424, "start": 2462.3199999999997, "end": 2464.3199999999997, "text": " impacted the price so", "tokens": [15653, 264, 3218, 370], "temperature": 0.0, "avg_logprob": -0.25825534246664134, "compression_ratio": 1.7448559670781894, "no_speech_prob": 5.862452326255152e-06}, {"id": 607, "seek": 245424, "start": 2464.56, "end": 2469.7599999999998, "text": " Year made I guess this must have been an older tractor. It caused a prediction of the price to go down", "tokens": [10289, 1027, 286, 2041, 341, 1633, 362, 668, 364, 4906, 31857, 13, 467, 7008, 257, 17630, 295, 264, 3218, 281, 352, 760], "temperature": 0.0, "avg_logprob": -0.25825534246664134, "compression_ratio": 1.7448559670781894, "no_speech_prob": 5.862452326255152e-06}, {"id": 608, "seek": 245424, "start": 2470.3999999999996, "end": 2474.3199999999997, "text": " But then it must have been a larger machine the product size caused it to go up", "tokens": [583, 550, 309, 1633, 362, 668, 257, 4833, 3479, 264, 1674, 2744, 7008, 309, 281, 352, 493], "temperature": 0.0, "avg_logprob": -0.25825534246664134, "compression_ratio": 1.7448559670781894, "no_speech_prob": 5.862452326255152e-06}, {"id": 609, "seek": 245424, "start": 2475.2, "end": 2477.9199999999996, "text": " Coupler system made it go up model id made it go up", "tokens": [26180, 22732, 1185, 1027, 309, 352, 493, 2316, 4496, 1027, 309, 352, 493], "temperature": 0.0, "avg_logprob": -0.25825534246664134, "compression_ratio": 1.7448559670781894, "no_speech_prob": 5.862452326255152e-06}, {"id": 610, "seek": 245424, "start": 2478.8799999999997, "end": 2480.8799999999997, "text": " And so forth, right? So you can see the red", "tokens": [400, 370, 5220, 11, 558, 30, 407, 291, 393, 536, 264, 2182], "temperature": 0.0, "avg_logprob": -0.25825534246664134, "compression_ratio": 1.7448559670781894, "no_speech_prob": 5.862452326255152e-06}, {"id": 611, "seek": 248088, "start": 2480.88, "end": 2484.4, "text": " Says that's made this made our prediction go down green made our prediction go up", "tokens": [36780, 300, 311, 1027, 341, 1027, 527, 17630, 352, 760, 3092, 1027, 527, 17630, 352, 493], "temperature": 0.0, "avg_logprob": -0.34650183284983915, "compression_ratio": 1.6570048309178744, "no_speech_prob": 5.3378334996523336e-06}, {"id": 612, "seek": 248088, "start": 2485.44, "end": 2487.44, "text": " And so overall you can see", "tokens": [400, 370, 4787, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.34650183284983915, "compression_ratio": 1.6570048309178744, "no_speech_prob": 5.3378334996523336e-06}, {"id": 613, "seek": 248088, "start": 2488.08, "end": 2491.6, "text": " Which things had the biggest impact on the prediction and what was the direction?", "tokens": [3013, 721, 632, 264, 3880, 2712, 322, 264, 17630, 293, 437, 390, 264, 3513, 30], "temperature": 0.0, "avg_logprob": -0.34650183284983915, "compression_ratio": 1.6570048309178744, "no_speech_prob": 5.3378334996523336e-06}, {"id": 614, "seek": 248088, "start": 2492.6400000000003, "end": 2493.92, "text": " For each one", "tokens": [1171, 1184, 472], "temperature": 0.0, "avg_logprob": -0.34650183284983915, "compression_ratio": 1.6570048309178744, "no_speech_prob": 5.3378334996523336e-06}, {"id": 615, "seek": 248088, "start": 2493.92, "end": 2498.7200000000003, "text": " So it's basically a feature importance plot, but just for a single role for a single row", "tokens": [407, 309, 311, 1936, 257, 4111, 7379, 7542, 11, 457, 445, 337, 257, 2167, 3090, 337, 257, 2167, 5386], "temperature": 0.0, "avg_logprob": -0.34650183284983915, "compression_ratio": 1.6570048309178744, "no_speech_prob": 5.3378334996523336e-06}, {"id": 616, "seek": 248088, "start": 2502.1600000000003, "end": 2504.48, "text": " Any questions john", "tokens": [2639, 1651, 35097], "temperature": 0.0, "avg_logprob": -0.34650183284983915, "compression_ratio": 1.6570048309178744, "no_speech_prob": 5.3378334996523336e-06}, {"id": 617, "seek": 248088, "start": 2506.6400000000003, "end": 2508.6400000000003, "text": " Yeah, there are a few questions", "tokens": [865, 11, 456, 366, 257, 1326, 1651], "temperature": 0.0, "avg_logprob": -0.34650183284983915, "compression_ratio": 1.6570048309178744, "no_speech_prob": 5.3378334996523336e-06}, {"id": 618, "seek": 250864, "start": 2508.64, "end": 2516.0, "text": " Yeah, there are a couple that have that have sort of queued up this is a this is a good spot to um to jump to them, um", "tokens": [865, 11, 456, 366, 257, 1916, 300, 362, 300, 362, 1333, 295, 631, 5827, 493, 341, 307, 257, 341, 307, 257, 665, 4008, 281, 1105, 281, 3012, 281, 552, 11, 1105], "temperature": 0.0, "avg_logprob": -0.17130985260009765, "compression_ratio": 1.6740088105726871, "no_speech_prob": 1.1658871699182782e-05}, {"id": 619, "seek": 250864, "start": 2518.24, "end": 2519.52, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.17130985260009765, "compression_ratio": 1.6740088105726871, "no_speech_prob": 1.1658871699182782e-05}, {"id": 620, "seek": 250864, "start": 2519.52, "end": 2523.04, "text": " first of all andrew's asking uh jumping back to the um", "tokens": [700, 295, 439, 293, 2236, 311, 3365, 2232, 11233, 646, 281, 264, 1105], "temperature": 0.0, "avg_logprob": -0.17130985260009765, "compression_ratio": 1.6740088105726871, "no_speech_prob": 1.1658871699182782e-05}, {"id": 621, "seek": 250864, "start": 2523.68, "end": 2530.3199999999997, "text": " The oob era would you ever exclude a tree from a forest if had a if it had a bad out of bag era?", "tokens": [440, 277, 996, 4249, 576, 291, 1562, 33536, 257, 4230, 490, 257, 6719, 498, 632, 257, 498, 309, 632, 257, 1578, 484, 295, 3411, 4249, 30], "temperature": 0.0, "avg_logprob": -0.17130985260009765, "compression_ratio": 1.6740088105726871, "no_speech_prob": 1.1658871699182782e-05}, {"id": 622, "seek": 250864, "start": 2531.6, "end": 2534.08, "text": " Like if you if you had a I guess if you had a particularly bad", "tokens": [1743, 498, 291, 498, 291, 632, 257, 286, 2041, 498, 291, 632, 257, 4098, 1578], "temperature": 0.0, "avg_logprob": -0.17130985260009765, "compression_ratio": 1.6740088105726871, "no_speech_prob": 1.1658871699182782e-05}, {"id": 623, "seek": 250864, "start": 2535.04, "end": 2537.68, "text": " Tree in your ensemble. Yeah, might you just", "tokens": [22291, 294, 428, 19492, 13, 865, 11, 1062, 291, 445], "temperature": 0.0, "avg_logprob": -0.17130985260009765, "compression_ratio": 1.6740088105726871, "no_speech_prob": 1.1658871699182782e-05}, {"id": 624, "seek": 253768, "start": 2537.68, "end": 2543.2799999999997, "text": " Would you delete a tree that was not doing its thing? It's not playing its part. No, you wouldn't", "tokens": [6068, 291, 12097, 257, 4230, 300, 390, 406, 884, 1080, 551, 30, 467, 311, 406, 2433, 1080, 644, 13, 883, 11, 291, 2759, 380], "temperature": 0.0, "avg_logprob": -0.28310972994024103, "compression_ratio": 1.5686274509803921, "no_speech_prob": 5.771516043751035e-06}, {"id": 625, "seek": 253768, "start": 2544.64, "end": 2548.8799999999997, "text": " Um, if you start deleting trees then you are no longer", "tokens": [3301, 11, 498, 291, 722, 48946, 5852, 550, 291, 366, 572, 2854], "temperature": 0.0, "avg_logprob": -0.28310972994024103, "compression_ratio": 1.5686274509803921, "no_speech_prob": 5.771516043751035e-06}, {"id": 626, "seek": 253768, "start": 2550.0, "end": 2553.44, "text": " Having a unbiased prediction of the dependent variable", "tokens": [10222, 257, 517, 5614, 1937, 17630, 295, 264, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.28310972994024103, "compression_ratio": 1.5686274509803921, "no_speech_prob": 5.771516043751035e-06}, {"id": 627, "seek": 253768, "start": 2554.3199999999997, "end": 2558.64, "text": " You are biasing it by making a choice. So even the bad ones", "tokens": [509, 366, 3228, 3349, 309, 538, 1455, 257, 3922, 13, 407, 754, 264, 1578, 2306], "temperature": 0.0, "avg_logprob": -0.28310972994024103, "compression_ratio": 1.5686274509803921, "no_speech_prob": 5.771516043751035e-06}, {"id": 628, "seek": 253768, "start": 2559.6, "end": 2561.12, "text": " Will be", "tokens": [3099, 312], "temperature": 0.0, "avg_logprob": -0.28310972994024103, "compression_ratio": 1.5686274509803921, "no_speech_prob": 5.771516043751035e-06}, {"id": 629, "seek": 253768, "start": 2561.12, "end": 2563.44, "text": " Improving the quality of the overall", "tokens": [8270, 340, 798, 264, 3125, 295, 264, 4787], "temperature": 0.0, "avg_logprob": -0.28310972994024103, "compression_ratio": 1.5686274509803921, "no_speech_prob": 5.771516043751035e-06}, {"id": 630, "seek": 253768, "start": 2564.08, "end": 2565.3599999999997, "text": " average", "tokens": [4274], "temperature": 0.0, "avg_logprob": -0.28310972994024103, "compression_ratio": 1.5686274509803921, "no_speech_prob": 5.771516043751035e-06}, {"id": 631, "seek": 256536, "start": 2565.36, "end": 2574.2400000000002, "text": " All right. Thank you. Um, zakiya followed up with the question about bagging and we're just sort of going, you know layers and layers here", "tokens": [1057, 558, 13, 1044, 291, 13, 3301, 11, 710, 7421, 3016, 6263, 493, 365, 264, 1168, 466, 3411, 3249, 293, 321, 434, 445, 1333, 295, 516, 11, 291, 458, 7914, 293, 7914, 510], "temperature": 0.0, "avg_logprob": -0.26018179321289064, "compression_ratio": 1.7147887323943662, "no_speech_prob": 3.0239642001106404e-05}, {"id": 632, "seek": 256536, "start": 2575.04, "end": 2578.6400000000003, "text": " Uh, you know, we could go on and create ensembles of bagged models", "tokens": [4019, 11, 291, 458, 11, 321, 727, 352, 322, 293, 1884, 12567, 2504, 904, 295, 3411, 3004, 5245], "temperature": 0.0, "avg_logprob": -0.26018179321289064, "compression_ratio": 1.7147887323943662, "no_speech_prob": 3.0239642001106404e-05}, {"id": 633, "seek": 256536, "start": 2579.2000000000003, "end": 2582.08, "text": " Um, and you know, is it reasonable to assume that they would", "tokens": [3301, 11, 293, 291, 458, 11, 307, 309, 10585, 281, 6552, 300, 436, 576], "temperature": 0.0, "avg_logprob": -0.26018179321289064, "compression_ratio": 1.7147887323943662, "no_speech_prob": 3.0239642001106404e-05}, {"id": 634, "seek": 256536, "start": 2582.6400000000003, "end": 2585.36, "text": " Continue so that's not going to make much difference right if they're all like", "tokens": [24472, 370, 300, 311, 406, 516, 281, 652, 709, 2649, 558, 498, 436, 434, 439, 411], "temperature": 0.0, "avg_logprob": -0.26018179321289064, "compression_ratio": 1.7147887323943662, "no_speech_prob": 3.0239642001106404e-05}, {"id": 635, "seek": 256536, "start": 2585.92, "end": 2587.92, "text": " You could take your 100 trees", "tokens": [509, 727, 747, 428, 2319, 5852], "temperature": 0.0, "avg_logprob": -0.26018179321289064, "compression_ratio": 1.7147887323943662, "no_speech_prob": 3.0239642001106404e-05}, {"id": 636, "seek": 256536, "start": 2587.92, "end": 2594.0, "text": " Split them into groups of 10 create 10 bagged ensembles and then average those but the average of an average is", "tokens": [45111, 552, 666, 3935, 295, 1266, 1884, 1266, 3411, 3004, 12567, 2504, 904, 293, 550, 4274, 729, 457, 264, 4274, 295, 364, 4274, 307], "temperature": 0.0, "avg_logprob": -0.26018179321289064, "compression_ratio": 1.7147887323943662, "no_speech_prob": 3.0239642001106404e-05}, {"id": 637, "seek": 259400, "start": 2594.0, "end": 2596.0, "text": " the same as the average", "tokens": [264, 912, 382, 264, 4274], "temperature": 0.0, "avg_logprob": -0.20974597623271327, "compression_ratio": 1.7642857142857142, "no_speech_prob": 1.3844435670762323e-05}, {"id": 638, "seek": 259400, "start": 2596.0, "end": 2597.04, "text": " Um", "tokens": [3301], "temperature": 0.0, "avg_logprob": -0.20974597623271327, "compression_ratio": 1.7642857142857142, "no_speech_prob": 1.3844435670762323e-05}, {"id": 639, "seek": 259400, "start": 2597.04, "end": 2602.64, "text": " You could like have a wider range of other kinds of models. You could have like neural nets trained on different subsets as well", "tokens": [509, 727, 411, 362, 257, 11842, 3613, 295, 661, 3685, 295, 5245, 13, 509, 727, 362, 411, 18161, 36170, 8895, 322, 819, 2090, 1385, 382, 731], "temperature": 0.0, "avg_logprob": -0.20974597623271327, "compression_ratio": 1.7642857142857142, "no_speech_prob": 1.3844435670762323e-05}, {"id": 640, "seek": 259400, "start": 2602.64, "end": 2605.44, "text": " But again, it's just the average of an average will still give you the average", "tokens": [583, 797, 11, 309, 311, 445, 264, 4274, 295, 364, 4274, 486, 920, 976, 291, 264, 4274], "temperature": 0.0, "avg_logprob": -0.20974597623271327, "compression_ratio": 1.7642857142857142, "no_speech_prob": 1.3844435670762323e-05}, {"id": 641, "seek": 259400, "start": 2606.32, "end": 2614.0, "text": " Right. So there's not a lot of value in in kind of structuring the ensemble. You just I mean some some ensembles you can structure", "tokens": [1779, 13, 407, 456, 311, 406, 257, 688, 295, 2158, 294, 294, 733, 295, 6594, 1345, 264, 19492, 13, 509, 445, 286, 914, 512, 512, 12567, 2504, 904, 291, 393, 3877], "temperature": 0.0, "avg_logprob": -0.20974597623271327, "compression_ratio": 1.7642857142857142, "no_speech_prob": 1.3844435670762323e-05}, {"id": 642, "seek": 259400, "start": 2614.0, "end": 2618.08, "text": " But but not bagging bagging is the simplest one. It's the one I mainly use", "tokens": [583, 457, 406, 3411, 3249, 3411, 3249, 307, 264, 22811, 472, 13, 467, 311, 264, 472, 286, 8704, 764], "temperature": 0.0, "avg_logprob": -0.20974597623271327, "compression_ratio": 1.7642857142857142, "no_speech_prob": 1.3844435670762323e-05}, {"id": 643, "seek": 259400, "start": 2618.72, "end": 2621.28, "text": " There are more sophisticated approaches, but this one", "tokens": [821, 366, 544, 16950, 11587, 11, 457, 341, 472], "temperature": 0.0, "avg_logprob": -0.20974597623271327, "compression_ratio": 1.7642857142857142, "no_speech_prob": 1.3844435670762323e-05}, {"id": 644, "seek": 262128, "start": 2621.28, "end": 2623.28, "text": " Is nice and easy", "tokens": [1119, 1481, 293, 1858], "temperature": 0.0, "avg_logprob": -0.33076582636151997, "compression_ratio": 1.6563706563706564, "no_speech_prob": 2.1109475710545667e-05}, {"id": 645, "seek": 262128, "start": 2623.28, "end": 2630.0, "text": " All right, and there's there's one that um is a bit specific and it's referencing content. You haven't covered but we're here now. So", "tokens": [1057, 558, 11, 293, 456, 311, 456, 311, 472, 300, 1105, 307, 257, 857, 2685, 293, 309, 311, 40582, 2701, 13, 509, 2378, 380, 5343, 457, 321, 434, 510, 586, 13, 407], "temperature": 0.0, "avg_logprob": -0.33076582636151997, "compression_ratio": 1.6563706563706564, "no_speech_prob": 2.1109475710545667e-05}, {"id": 646, "seek": 262128, "start": 2630.7200000000003, "end": 2632.7200000000003, "text": " Um, and it's on explainability", "tokens": [3301, 11, 293, 309, 311, 322, 2903, 2310], "temperature": 0.0, "avg_logprob": -0.33076582636151997, "compression_ratio": 1.6563706563706564, "no_speech_prob": 2.1109475710545667e-05}, {"id": 647, "seek": 262128, "start": 2633.28, "end": 2635.28, "text": " uh, so feature importance of", "tokens": [2232, 11, 370, 4111, 7379, 295], "temperature": 0.0, "avg_logprob": -0.33076582636151997, "compression_ratio": 1.6563706563706564, "no_speech_prob": 2.1109475710545667e-05}, {"id": 648, "seek": 262128, "start": 2635.84, "end": 2641.36, "text": " Random forest model sometimes has different results when you compare to other explainability techniques", "tokens": [37603, 6719, 2316, 2171, 575, 819, 3542, 562, 291, 6794, 281, 661, 2903, 2310, 7512], "temperature": 0.0, "avg_logprob": -0.33076582636151997, "compression_ratio": 1.6563706563706564, "no_speech_prob": 2.1109475710545667e-05}, {"id": 649, "seek": 262128, "start": 2642.0800000000004, "end": 2645.2000000000003, "text": " Um, like shap shap or lime", "tokens": [3301, 11, 411, 402, 569, 402, 569, 420, 22035], "temperature": 0.0, "avg_logprob": -0.33076582636151997, "compression_ratio": 1.6563706563706564, "no_speech_prob": 2.1109475710545667e-05}, {"id": 650, "seek": 262128, "start": 2646.0800000000004, "end": 2649.76, "text": " Um, and we haven't covered these in the course, but um, amir is just curious about this", "tokens": [3301, 11, 293, 321, 2378, 380, 5343, 613, 294, 264, 1164, 11, 457, 1105, 11, 669, 347, 307, 445, 6369, 466, 341], "temperature": 0.0, "avg_logprob": -0.33076582636151997, "compression_ratio": 1.6563706563706564, "no_speech_prob": 2.1109475710545667e-05}, {"id": 651, "seek": 264976, "start": 2649.76, "end": 2652.48, "text": " If you've got any thoughts on which is more accurate or reliable", "tokens": [759, 291, 600, 658, 604, 4598, 322, 597, 307, 544, 8559, 420, 12924], "temperature": 0.0, "avg_logprob": -0.42483742596351937, "compression_ratio": 1.7005649717514124, "no_speech_prob": 2.2804351829108782e-05}, {"id": 652, "seek": 264976, "start": 2653.1200000000003, "end": 2655.6800000000003, "text": " Random forest feature importance or other techniques", "tokens": [37603, 6719, 4111, 7379, 420, 661, 7512], "temperature": 0.0, "avg_logprob": -0.42483742596351937, "compression_ratio": 1.7005649717514124, "no_speech_prob": 2.2804351829108782e-05}, {"id": 653, "seek": 264976, "start": 2656.5600000000004, "end": 2658.5600000000004, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.42483742596351937, "compression_ratio": 1.7005649717514124, "no_speech_prob": 2.2804351829108782e-05}, {"id": 654, "seek": 264976, "start": 2659.6000000000004, "end": 2661.6000000000004, "text": " I I would", "tokens": [286, 286, 576], "temperature": 0.0, "avg_logprob": -0.42483742596351937, "compression_ratio": 1.7005649717514124, "no_speech_prob": 2.2804351829108782e-05}, {"id": 655, "seek": 264976, "start": 2662.1600000000003, "end": 2664.1600000000003, "text": " lean towards", "tokens": [11659, 3030], "temperature": 0.0, "avg_logprob": -0.42483742596351937, "compression_ratio": 1.7005649717514124, "no_speech_prob": 2.2804351829108782e-05}, {"id": 656, "seek": 264976, "start": 2664.48, "end": 2670.7200000000003, "text": " More immediately trusting random forest feature importances over other techniques on the whole", "tokens": [5048, 4258, 28235, 4974, 6719, 4111, 974, 2676, 670, 661, 7512, 322, 264, 1379], "temperature": 0.0, "avg_logprob": -0.42483742596351937, "compression_ratio": 1.7005649717514124, "no_speech_prob": 2.2804351829108782e-05}, {"id": 657, "seek": 264976, "start": 2671.92, "end": 2674.8, "text": " On the basis that it's very hard to mess up a random forest", "tokens": [1282, 264, 5143, 300, 309, 311, 588, 1152, 281, 2082, 493, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.42483742596351937, "compression_ratio": 1.7005649717514124, "no_speech_prob": 2.2804351829108782e-05}, {"id": 658, "seek": 264976, "start": 2676.0, "end": 2678.0, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.42483742596351937, "compression_ratio": 1.7005649717514124, "no_speech_prob": 2.2804351829108782e-05}, {"id": 659, "seek": 267800, "start": 2678.0, "end": 2680.0, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.252261725805139, "compression_ratio": 1.6623376623376624, "no_speech_prob": 1.3417151421890594e-05}, {"id": 660, "seek": 267800, "start": 2680.4, "end": 2685.92, "text": " Yeah, I feel like pretty confident that a random forest feature importance is going to", "tokens": [865, 11, 286, 841, 411, 1238, 6679, 300, 257, 4974, 6719, 4111, 7379, 307, 516, 281], "temperature": 0.0, "avg_logprob": -0.252261725805139, "compression_ratio": 1.6623376623376624, "no_speech_prob": 1.3417151421890594e-05}, {"id": 661, "seek": 267800, "start": 2687.12, "end": 2688.56, "text": " Be pretty reasonable", "tokens": [879, 1238, 10585], "temperature": 0.0, "avg_logprob": -0.252261725805139, "compression_ratio": 1.6623376623376624, "no_speech_prob": 1.3417151421890594e-05}, {"id": 662, "seek": 267800, "start": 2688.56, "end": 2693.28, "text": " As long as this is the kind of data which a random forest is likely to be pretty good at", "tokens": [1018, 938, 382, 341, 307, 264, 733, 295, 1412, 597, 257, 4974, 6719, 307, 3700, 281, 312, 1238, 665, 412], "temperature": 0.0, "avg_logprob": -0.252261725805139, "compression_ratio": 1.6623376623376624, "no_speech_prob": 1.3417151421890594e-05}, {"id": 663, "seek": 267800, "start": 2693.84, "end": 2698.48, "text": " You know doing you know, if it's like a computer vision model random forests aren't", "tokens": [509, 458, 884, 291, 458, 11, 498, 309, 311, 411, 257, 3820, 5201, 2316, 4974, 21700, 3212, 380], "temperature": 0.0, "avg_logprob": -0.252261725805139, "compression_ratio": 1.6623376623376624, "no_speech_prob": 1.3417151421890594e-05}, {"id": 664, "seek": 267800, "start": 2699.04, "end": 2703.44, "text": " Particularly good at that and so one of the things that bryman talked about a lot was explainability", "tokens": [32281, 665, 412, 300, 293, 370, 472, 295, 264, 721, 300, 272, 627, 1601, 2825, 466, 257, 688, 390, 2903, 2310], "temperature": 0.0, "avg_logprob": -0.252261725805139, "compression_ratio": 1.6623376623376624, "no_speech_prob": 1.3417151421890594e-05}, {"id": 665, "seek": 270344, "start": 2703.44, "end": 2709.84, "text": " He's got a great essay called the two cultures of statistics in which he talks about I guess what we're nowadays called kind of like", "tokens": [634, 311, 658, 257, 869, 16238, 1219, 264, 732, 12951, 295, 12523, 294, 597, 415, 6686, 466, 286, 2041, 437, 321, 434, 13434, 1219, 733, 295, 411], "temperature": 0.0, "avg_logprob": -0.27546065481085524, "compression_ratio": 1.630952380952381, "no_speech_prob": 5.954834250587737e-06}, {"id": 666, "seek": 270344, "start": 2709.84, "end": 2713.04, "text": " data scientists and machine learning folks versus classic statisticians", "tokens": [1412, 7708, 293, 3479, 2539, 4024, 5717, 7230, 29588, 2567], "temperature": 0.0, "avg_logprob": -0.27546065481085524, "compression_ratio": 1.630952380952381, "no_speech_prob": 5.954834250587737e-06}, {"id": 667, "seek": 270344, "start": 2714.0, "end": 2715.6, "text": " and um", "tokens": [293, 1105], "temperature": 0.0, "avg_logprob": -0.27546065481085524, "compression_ratio": 1.630952380952381, "no_speech_prob": 5.954834250587737e-06}, {"id": 668, "seek": 270344, "start": 2715.6, "end": 2719.12, "text": " He he was you know, definitely a data scientist well before the", "tokens": [634, 415, 390, 291, 458, 11, 2138, 257, 1412, 12662, 731, 949, 264], "temperature": 0.0, "avg_logprob": -0.27546065481085524, "compression_ratio": 1.630952380952381, "no_speech_prob": 5.954834250587737e-06}, {"id": 669, "seek": 270344, "start": 2720.16, "end": 2724.4, "text": " Label existed and he pointed out. Yeah, you know first and foremost", "tokens": [10137, 338, 13135, 293, 415, 10932, 484, 13, 865, 11, 291, 458, 700, 293, 18864], "temperature": 0.0, "avg_logprob": -0.27546065481085524, "compression_ratio": 1.630952380952381, "no_speech_prob": 5.954834250587737e-06}, {"id": 670, "seek": 270344, "start": 2725.12, "end": 2728.48, "text": " You need a model that's accurate. It needs to make good predictions", "tokens": [509, 643, 257, 2316, 300, 311, 8559, 13, 467, 2203, 281, 652, 665, 21264], "temperature": 0.0, "avg_logprob": -0.27546065481085524, "compression_ratio": 1.630952380952381, "no_speech_prob": 5.954834250587737e-06}, {"id": 671, "seek": 272848, "start": 2728.48, "end": 2734.64, "text": " A model that makes bad predictions will also be bad for making explanations because it doesn't actually know what's going on", "tokens": [316, 2316, 300, 1669, 1578, 21264, 486, 611, 312, 1578, 337, 1455, 28708, 570, 309, 1177, 380, 767, 458, 437, 311, 516, 322], "temperature": 0.0, "avg_logprob": -0.2184126803749486, "compression_ratio": 1.7291666666666667, "no_speech_prob": 1.8341212125960737e-05}, {"id": 672, "seek": 272848, "start": 2735.68, "end": 2741.12, "text": " So if you know if you if you've got a deep learning model that's far more accurate than your random forest then it's", "tokens": [407, 498, 291, 458, 498, 291, 498, 291, 600, 658, 257, 2452, 2539, 2316, 300, 311, 1400, 544, 8559, 813, 428, 4974, 6719, 550, 309, 311], "temperature": 0.0, "avg_logprob": -0.2184126803749486, "compression_ratio": 1.7291666666666667, "no_speech_prob": 1.8341212125960737e-05}, {"id": 673, "seek": 272848, "start": 2741.76, "end": 2744.48, "text": " You know explainability methods from the deep learning model will", "tokens": [509, 458, 2903, 2310, 7150, 490, 264, 2452, 2539, 2316, 486], "temperature": 0.0, "avg_logprob": -0.2184126803749486, "compression_ratio": 1.7291666666666667, "no_speech_prob": 1.8341212125960737e-05}, {"id": 674, "seek": 272848, "start": 2745.12, "end": 2749.44, "text": " Probably be more useful because it's explaining a model. That's actually correct", "tokens": [9210, 312, 544, 4420, 570, 309, 311, 13468, 257, 2316, 13, 663, 311, 767, 3006], "temperature": 0.0, "avg_logprob": -0.2184126803749486, "compression_ratio": 1.7291666666666667, "no_speech_prob": 1.8341212125960737e-05}, {"id": 675, "seek": 272848, "start": 2752.96, "end": 2754.96, "text": " All right, let's take a um", "tokens": [1057, 558, 11, 718, 311, 747, 257, 1105], "temperature": 0.0, "avg_logprob": -0.2184126803749486, "compression_ratio": 1.7291666666666667, "no_speech_prob": 1.8341212125960737e-05}, {"id": 676, "seek": 275496, "start": 2754.96, "end": 2759.68, "text": " 10 minute break and we'll come back at five past seven", "tokens": [1266, 3456, 1821, 293, 321, 603, 808, 646, 412, 1732, 1791, 3407], "temperature": 0.0, "avg_logprob": -0.32136060274564304, "compression_ratio": 1.4792899408284024, "no_speech_prob": 4.61005256511271e-05}, {"id": 677, "seek": 275496, "start": 2765.44, "end": 2770.96, "text": " Welcome back, um one person pointed out. I noticed I got the chapter wrong. It's chapter nine", "tokens": [4027, 646, 11, 1105, 472, 954, 10932, 484, 13, 286, 5694, 286, 658, 264, 7187, 2085, 13, 467, 311, 7187, 4949], "temperature": 0.0, "avg_logprob": -0.32136060274564304, "compression_ratio": 1.4792899408284024, "no_speech_prob": 4.61005256511271e-05}, {"id": 678, "seek": 275496, "start": 2770.96, "end": 2773.84, "text": " Not chapter eight in the book. I guess I can't read", "tokens": [1726, 7187, 3180, 294, 264, 1446, 13, 286, 2041, 286, 393, 380, 1401], "temperature": 0.0, "avg_logprob": -0.32136060274564304, "compression_ratio": 1.4792899408284024, "no_speech_prob": 4.61005256511271e-05}, {"id": 679, "seek": 275496, "start": 2776.7200000000003, "end": 2780.48, "text": " Somebody asked during the break about overfitting", "tokens": [13463, 2351, 1830, 264, 1821, 466, 670, 69, 2414], "temperature": 0.0, "avg_logprob": -0.32136060274564304, "compression_ratio": 1.4792899408284024, "no_speech_prob": 4.61005256511271e-05}, {"id": 680, "seek": 278048, "start": 2780.48, "end": 2784.0, "text": " Um, can you overfit a random forest?", "tokens": [3301, 11, 393, 291, 670, 6845, 257, 4974, 6719, 30], "temperature": 0.0, "avg_logprob": -0.3056949138641357, "compression_ratio": 1.6312849162011174, "no_speech_prob": 2.4678438421688043e-05}, {"id": 681, "seek": 278048, "start": 2785.04, "end": 2787.04, "text": " Basically, no, not really", "tokens": [8537, 11, 572, 11, 406, 534], "temperature": 0.0, "avg_logprob": -0.3056949138641357, "compression_ratio": 1.6312849162011174, "no_speech_prob": 2.4678438421688043e-05}, {"id": 682, "seek": 278048, "start": 2787.68, "end": 2789.68, "text": " Adding more trees will", "tokens": [31204, 544, 5852, 486], "temperature": 0.0, "avg_logprob": -0.3056949138641357, "compression_ratio": 1.6312849162011174, "no_speech_prob": 2.4678438421688043e-05}, {"id": 683, "seek": 278048, "start": 2790.0, "end": 2792.0, "text": " Make it more accurate", "tokens": [4387, 309, 544, 8559], "temperature": 0.0, "avg_logprob": -0.3056949138641357, "compression_ratio": 1.6312849162011174, "no_speech_prob": 2.4678438421688043e-05}, {"id": 684, "seek": 278048, "start": 2794.08, "end": 2796.8, "text": " It kind of asymptotes so you can't make it infinitely", "tokens": [467, 733, 295, 35114, 17251, 370, 291, 393, 380, 652, 309, 36227], "temperature": 0.0, "avg_logprob": -0.3056949138641357, "compression_ratio": 1.6312849162011174, "no_speech_prob": 2.4678438421688043e-05}, {"id": 685, "seek": 278048, "start": 2797.52, "end": 2802.0, "text": " Accurate by using infinite trees, but it's certainly you know adding more trees won't make it worse", "tokens": [5725, 33144, 538, 1228, 13785, 5852, 11, 457, 309, 311, 3297, 291, 458, 5127, 544, 5852, 1582, 380, 652, 309, 5324], "temperature": 0.0, "avg_logprob": -0.3056949138641357, "compression_ratio": 1.6312849162011174, "no_speech_prob": 2.4678438421688043e-05}, {"id": 686, "seek": 278048, "start": 2803.92, "end": 2805.92, "text": " If you don't have enough trees", "tokens": [759, 291, 500, 380, 362, 1547, 5852], "temperature": 0.0, "avg_logprob": -0.3056949138641357, "compression_ratio": 1.6312849162011174, "no_speech_prob": 2.4678438421688043e-05}, {"id": 687, "seek": 280592, "start": 2805.92, "end": 2812.32, "text": " And you let the trees grow very deep that could overfit", "tokens": [400, 291, 718, 264, 5852, 1852, 588, 2452, 300, 727, 670, 6845], "temperature": 0.0, "avg_logprob": -0.5470393977753104, "compression_ratio": 1.7329192546583851, "no_speech_prob": 8.397492820222396e-06}, {"id": 688, "seek": 280592, "start": 2813.2000000000003, "end": 2815.2000000000003, "text": " So you just have to make sure you have enough trees", "tokens": [407, 291, 445, 362, 281, 652, 988, 291, 362, 1547, 5852], "temperature": 0.0, "avg_logprob": -0.5470393977753104, "compression_ratio": 1.7329192546583851, "no_speech_prob": 8.397492820222396e-06}, {"id": 689, "seek": 280592, "start": 2823.44, "end": 2826.2400000000002, "text": " Radik told me about an experiment he did during that", "tokens": [9654, 1035, 1907, 385, 466, 364, 5120, 415, 630, 1830, 300], "temperature": 0.0, "avg_logprob": -0.5470393977753104, "compression_ratio": 1.7329192546583851, "no_speech_prob": 8.397492820222396e-06}, {"id": 690, "seek": 280592, "start": 2826.96, "end": 2829.28, "text": " Radik told me during the break about an experiment he did", "tokens": [9654, 1035, 1907, 385, 1830, 264, 1821, 466, 364, 5120, 415, 630], "temperature": 0.0, "avg_logprob": -0.5470393977753104, "compression_ratio": 1.7329192546583851, "no_speech_prob": 8.397492820222396e-06}, {"id": 691, "seek": 280592, "start": 2830.0, "end": 2832.96, "text": " But he didn't do it because he didn't know what was going on", "tokens": [583, 415, 994, 380, 360, 309, 570, 415, 994, 380, 458, 437, 390, 516, 322], "temperature": 0.0, "avg_logprob": -0.5470393977753104, "compression_ratio": 1.7329192546583851, "no_speech_prob": 8.397492820222396e-06}, {"id": 692, "seek": 283296, "start": 2832.96, "end": 2840.0, "text": " So he told me during the break about an experiment he did um, which is something i've done something similar, which is adding lots and lots of", "tokens": [407, 415, 1907, 385, 1830, 264, 1821, 466, 364, 5120, 415, 630, 1105, 11, 597, 307, 746, 741, 600, 1096, 746, 2531, 11, 597, 307, 5127, 3195, 293, 3195, 295], "temperature": 0.0, "avg_logprob": -0.19208900295958228, "compression_ratio": 1.6903765690376569, "no_speech_prob": 1.4282398296927568e-05}, {"id": 693, "seek": 283296, "start": 2840.54, "end": 2846.48, "text": " randomly generated columns to a data set and try to break the random forest and", "tokens": [16979, 10833, 13766, 281, 257, 1412, 992, 293, 853, 281, 1821, 264, 4974, 6719, 293], "temperature": 0.0, "avg_logprob": -0.19208900295958228, "compression_ratio": 1.6903765690376569, "no_speech_prob": 1.4282398296927568e-05}, {"id": 694, "seek": 283296, "start": 2847.12, "end": 2850.8, "text": " If you try it, it basically doesn't work. It's like it's really hard", "tokens": [759, 291, 853, 309, 11, 309, 1936, 1177, 380, 589, 13, 467, 311, 411, 309, 311, 534, 1152], "temperature": 0.0, "avg_logprob": -0.19208900295958228, "compression_ratio": 1.6903765690376569, "no_speech_prob": 1.4282398296927568e-05}, {"id": 695, "seek": 283296, "start": 2851.92, "end": 2854.48, "text": " to confuse a random forest by giving it lots of", "tokens": [281, 28584, 257, 4974, 6719, 538, 2902, 309, 3195, 295], "temperature": 0.0, "avg_logprob": -0.19208900295958228, "compression_ratio": 1.6903765690376569, "no_speech_prob": 1.4282398296927568e-05}, {"id": 696, "seek": 283296, "start": 2855.36, "end": 2858.8, "text": " Meaningless data it it does an amazingly good job of picking out", "tokens": [19948, 1832, 1412, 309, 309, 775, 364, 31762, 665, 1691, 295, 8867, 484], "temperature": 0.0, "avg_logprob": -0.19208900295958228, "compression_ratio": 1.6903765690376569, "no_speech_prob": 1.4282398296927568e-05}, {"id": 697, "seek": 285880, "start": 2858.8, "end": 2864.5600000000004, "text": " The the useful stuff as I said, you know I had 30 useful columns out of 7 000 and it found them", "tokens": [440, 264, 4420, 1507, 382, 286, 848, 11, 291, 458, 286, 632, 2217, 4420, 13766, 484, 295, 1614, 13711, 293, 309, 1352, 552], "temperature": 0.0, "avg_logprob": -0.2173747475265611, "compression_ratio": 1.77491961414791, "no_speech_prob": 1.241003701579757e-05}, {"id": 698, "seek": 285880, "start": 2865.44, "end": 2867.44, "text": " perfectly well", "tokens": [6239, 731], "temperature": 0.0, "avg_logprob": -0.2173747475265611, "compression_ratio": 1.77491961414791, "no_speech_prob": 1.241003701579757e-05}, {"id": 699, "seek": 285880, "start": 2868.0, "end": 2871.2000000000003, "text": " And often, you know when you find those 30 columns", "tokens": [400, 2049, 11, 291, 458, 562, 291, 915, 729, 2217, 13766], "temperature": 0.0, "avg_logprob": -0.2173747475265611, "compression_ratio": 1.77491961414791, "no_speech_prob": 1.241003701579757e-05}, {"id": 700, "seek": 285880, "start": 2871.92, "end": 2873.28, "text": " You know you could go to you know", "tokens": [509, 458, 291, 727, 352, 281, 291, 458], "temperature": 0.0, "avg_logprob": -0.2173747475265611, "compression_ratio": 1.77491961414791, "no_speech_prob": 1.241003701579757e-05}, {"id": 701, "seek": 285880, "start": 2873.28, "end": 2877.28, "text": " I was doing consulting at the time go back to the client and say like tell me more about these columns", "tokens": [286, 390, 884, 23682, 412, 264, 565, 352, 646, 281, 264, 6423, 293, 584, 411, 980, 385, 544, 466, 613, 13766], "temperature": 0.0, "avg_logprob": -0.2173747475265611, "compression_ratio": 1.77491961414791, "no_speech_prob": 1.241003701579757e-05}, {"id": 702, "seek": 285880, "start": 2877.28, "end": 2880.8, "text": " And it's say like oh well that one there. We've actually got a better version of that now", "tokens": [400, 309, 311, 584, 411, 1954, 731, 300, 472, 456, 13, 492, 600, 767, 658, 257, 1101, 3037, 295, 300, 586], "temperature": 0.0, "avg_logprob": -0.2173747475265611, "compression_ratio": 1.77491961414791, "no_speech_prob": 1.241003701579757e-05}, {"id": 703, "seek": 285880, "start": 2880.8, "end": 2884.32, "text": " There's a new system, you know, we should grab that and oh this column", "tokens": [821, 311, 257, 777, 1185, 11, 291, 458, 11, 321, 820, 4444, 300, 293, 1954, 341, 7738], "temperature": 0.0, "avg_logprob": -0.2173747475265611, "compression_ratio": 1.77491961414791, "no_speech_prob": 1.241003701579757e-05}, {"id": 704, "seek": 285880, "start": 2884.32, "end": 2887.52, "text": " Actually, that was because of this thing that happened last year, but we don't do it anymore", "tokens": [5135, 11, 300, 390, 570, 295, 341, 551, 300, 2011, 1036, 1064, 11, 457, 321, 500, 380, 360, 309, 3602], "temperature": 0.0, "avg_logprob": -0.2173747475265611, "compression_ratio": 1.77491961414791, "no_speech_prob": 1.241003701579757e-05}, {"id": 705, "seek": 288752, "start": 2887.52, "end": 2893.22, "text": " Or you know, like you can really have this kind of discussion about the stuff you've zoomed into", "tokens": [1610, 291, 458, 11, 411, 291, 393, 534, 362, 341, 733, 295, 5017, 466, 264, 1507, 291, 600, 8863, 292, 666], "temperature": 0.0, "avg_logprob": -0.13660140598521514, "compression_ratio": 1.6594594594594594, "no_speech_prob": 1.8629014448379166e-05}, {"id": 706, "seek": 288752, "start": 2905.52, "end": 2912.02, "text": " Um, you know, there are other things that you have to think about with lots of kinds of models like particularly regression models things like interactions", "tokens": [3301, 11, 291, 458, 11, 456, 366, 661, 721, 300, 291, 362, 281, 519, 466, 365, 3195, 295, 3685, 295, 5245, 411, 4098, 24590, 5245, 721, 411, 13280], "temperature": 0.0, "avg_logprob": -0.13660140598521514, "compression_ratio": 1.6594594594594594, "no_speech_prob": 1.8629014448379166e-05}, {"id": 707, "seek": 288752, "start": 2912.8, "end": 2914.8, "text": " You don't have to worry about that with random forests", "tokens": [509, 500, 380, 362, 281, 3292, 466, 300, 365, 4974, 21700], "temperature": 0.0, "avg_logprob": -0.13660140598521514, "compression_ratio": 1.6594594594594594, "no_speech_prob": 1.8629014448379166e-05}, {"id": 708, "seek": 291480, "start": 2914.8, "end": 2918.8, "text": " Like because you split on one column and then split on another column", "tokens": [1743, 570, 291, 7472, 322, 472, 7738, 293, 550, 7472, 322, 1071, 7738], "temperature": 0.0, "avg_logprob": -0.16396453415138135, "compression_ratio": 1.5674157303370786, "no_speech_prob": 6.338946150208358e-06}, {"id": 709, "seek": 291480, "start": 2919.44, "end": 2921.44, "text": " You get interactions for free", "tokens": [509, 483, 13280, 337, 1737], "temperature": 0.0, "avg_logprob": -0.16396453415138135, "compression_ratio": 1.5674157303370786, "no_speech_prob": 6.338946150208358e-06}, {"id": 710, "seek": 291480, "start": 2922.0800000000004, "end": 2924.0800000000004, "text": " as well", "tokens": [382, 731], "temperature": 0.0, "avg_logprob": -0.16396453415138135, "compression_ratio": 1.5674157303370786, "no_speech_prob": 6.338946150208358e-06}, {"id": 711, "seek": 291480, "start": 2925.2000000000003, "end": 2928.8, "text": " Normalization you don't have to worry about you know, you don't have to have normally distributed", "tokens": [21277, 2144, 291, 500, 380, 362, 281, 3292, 466, 291, 458, 11, 291, 500, 380, 362, 281, 362, 5646, 12631], "temperature": 0.0, "avg_logprob": -0.16396453415138135, "compression_ratio": 1.5674157303370786, "no_speech_prob": 6.338946150208358e-06}, {"id": 712, "seek": 291480, "start": 2929.52, "end": 2931.52, "text": " columns", "tokens": [13766], "temperature": 0.0, "avg_logprob": -0.16396453415138135, "compression_ratio": 1.5674157303370786, "no_speech_prob": 6.338946150208358e-06}, {"id": 713, "seek": 291480, "start": 2932.0800000000004, "end": 2937.92, "text": " So yeah, definitely worth a try now something I haven't gone into", "tokens": [407, 1338, 11, 2138, 3163, 257, 853, 586, 746, 286, 2378, 380, 2780, 666], "temperature": 0.0, "avg_logprob": -0.16396453415138135, "compression_ratio": 1.5674157303370786, "no_speech_prob": 6.338946150208358e-06}, {"id": 714, "seek": 293792, "start": 2937.92, "end": 2939.92, "text": " Um", "tokens": [3301], "temperature": 0.0, "avg_logprob": -0.1641862898161917, "compression_ratio": 1.578616352201258, "no_speech_prob": 1.8333999832975678e-05}, {"id": 715, "seek": 293792, "start": 2944.16, "end": 2946.16, "text": " Is gradient boosting", "tokens": [1119, 16235, 43117], "temperature": 0.0, "avg_logprob": -0.1641862898161917, "compression_ratio": 1.578616352201258, "no_speech_prob": 1.8333999832975678e-05}, {"id": 716, "seek": 293792, "start": 2948.64, "end": 2950.7400000000002, "text": " Um, but if you go to explain.ai", "tokens": [3301, 11, 457, 498, 291, 352, 281, 2903, 13, 1301], "temperature": 0.0, "avg_logprob": -0.1641862898161917, "compression_ratio": 1.578616352201258, "no_speech_prob": 1.8333999832975678e-05}, {"id": 717, "seek": 293792, "start": 2952.32, "end": 2956.88, "text": " You'll see that my friend Terrence and I have a three-part series about gradient boosting", "tokens": [509, 603, 536, 300, 452, 1277, 6564, 10760, 293, 286, 362, 257, 1045, 12, 6971, 2638, 466, 16235, 43117], "temperature": 0.0, "avg_logprob": -0.1641862898161917, "compression_ratio": 1.578616352201258, "no_speech_prob": 1.8333999832975678e-05}, {"id": 718, "seek": 293792, "start": 2958.16, "end": 2960.32, "text": " Including pictures of golf made by Terrence", "tokens": [27137, 5242, 295, 12880, 1027, 538, 6564, 10760], "temperature": 0.0, "avg_logprob": -0.1641862898161917, "compression_ratio": 1.578616352201258, "no_speech_prob": 1.8333999832975678e-05}, {"id": 719, "seek": 293792, "start": 2962.48, "end": 2966.4, "text": " But to explain gradient boosting is a lot like random forests", "tokens": [583, 281, 2903, 16235, 43117, 307, 257, 688, 411, 4974, 21700], "temperature": 0.0, "avg_logprob": -0.1641862898161917, "compression_ratio": 1.578616352201258, "no_speech_prob": 1.8333999832975678e-05}, {"id": 720, "seek": 296640, "start": 2966.4, "end": 2968.4, "text": " but rather than", "tokens": [457, 2831, 813], "temperature": 0.0, "avg_logprob": -0.41681922661079157, "compression_ratio": 1.6019417475728155, "no_speech_prob": 1.3210096767579671e-05}, {"id": 721, "seek": 296640, "start": 2969.12, "end": 2971.12, "text": " training", "tokens": [3097], "temperature": 0.0, "avg_logprob": -0.41681922661079157, "compression_ratio": 1.6019417475728155, "no_speech_prob": 1.3210096767579671e-05}, {"id": 722, "seek": 296640, "start": 2971.12, "end": 2973.12, "text": " a lot of training", "tokens": [257, 688, 295, 3097], "temperature": 0.0, "avg_logprob": -0.41681922661079157, "compression_ratio": 1.6019417475728155, "no_speech_prob": 1.3210096767579671e-05}, {"id": 723, "seek": 296640, "start": 2973.12, "end": 2976.96, "text": " Fitting a tree again and again and again on different random subsets of the data", "tokens": [479, 2414, 257, 4230, 797, 293, 797, 293, 797, 322, 819, 4974, 2090, 1385, 295, 264, 1412], "temperature": 0.0, "avg_logprob": -0.41681922661079157, "compression_ratio": 1.6019417475728155, "no_speech_prob": 1.3210096767579671e-05}, {"id": 724, "seek": 296640, "start": 2978.4, "end": 2984.0, "text": " Instead what we do is we fit very very very small trees to hardly ever any splits", "tokens": [7156, 437, 321, 360, 307, 321, 3318, 588, 588, 588, 1359, 5852, 281, 13572, 1562, 604, 37741], "temperature": 0.0, "avg_logprob": -0.41681922661079157, "compression_ratio": 1.6019417475728155, "no_speech_prob": 1.3210096767579671e-05}, {"id": 725, "seek": 296640, "start": 2984.7200000000003, "end": 2987.36, "text": " And we then say okay. Well, what's the error?", "tokens": [400, 321, 550, 584, 1392, 13, 1042, 11, 437, 311, 264, 6713, 30], "temperature": 0.0, "avg_logprob": -0.41681922661079157, "compression_ratio": 1.6019417475728155, "no_speech_prob": 1.3210096767579671e-05}, {"id": 726, "seek": 296640, "start": 2987.84, "end": 2993.52, "text": " So, you know, um, so imagine the simplest tree would be our one R rule tree of", "tokens": [407, 11, 291, 458, 11, 1105, 11, 370, 3811, 264, 22811, 4230, 576, 312, 527, 472, 497, 4978, 4230, 295], "temperature": 0.0, "avg_logprob": -0.41681922661079157, "compression_ratio": 1.6019417475728155, "no_speech_prob": 1.3210096767579671e-05}, {"id": 727, "seek": 299352, "start": 2993.52, "end": 2995.52, "text": " male versus female say", "tokens": [7133, 5717, 6556, 584], "temperature": 0.0, "avg_logprob": -0.2736547603163608, "compression_ratio": 1.949748743718593, "no_speech_prob": 3.3404730857000686e-06}, {"id": 728, "seek": 299352, "start": 2996.24, "end": 3001.28, "text": " And then you you take what's called the residual. That's the difference between the prediction and the actual", "tokens": [400, 550, 291, 291, 747, 437, 311, 1219, 264, 27980, 13, 663, 311, 264, 2649, 1296, 264, 17630, 293, 264, 3539], "temperature": 0.0, "avg_logprob": -0.2736547603163608, "compression_ratio": 1.949748743718593, "no_speech_prob": 3.3404730857000686e-06}, {"id": 729, "seek": 299352, "start": 3001.84, "end": 3006.64, "text": " The error and then you create another tree which attempts to predict that", "tokens": [440, 6713, 293, 550, 291, 1884, 1071, 4230, 597, 15257, 281, 6069, 300], "temperature": 0.0, "avg_logprob": -0.2736547603163608, "compression_ratio": 1.949748743718593, "no_speech_prob": 3.3404730857000686e-06}, {"id": 730, "seek": 299352, "start": 3007.7599999999998, "end": 3009.2, "text": " very small tree", "tokens": [588, 1359, 4230], "temperature": 0.0, "avg_logprob": -0.2736547603163608, "compression_ratio": 1.949748743718593, "no_speech_prob": 3.3404730857000686e-06}, {"id": 731, "seek": 299352, "start": 3009.2, "end": 3014.96, "text": " And then you create another very small tree which tries to predict the error from that", "tokens": [400, 550, 291, 1884, 1071, 588, 1359, 4230, 597, 9898, 281, 6069, 264, 6713, 490, 300], "temperature": 0.0, "avg_logprob": -0.2736547603163608, "compression_ratio": 1.949748743718593, "no_speech_prob": 3.3404730857000686e-06}, {"id": 732, "seek": 299352, "start": 3015.92, "end": 3019.52, "text": " And so forth each one is predicting the residual from all of the previous ones", "tokens": [400, 370, 5220, 1184, 472, 307, 32884, 264, 27980, 490, 439, 295, 264, 3894, 2306], "temperature": 0.0, "avg_logprob": -0.2736547603163608, "compression_ratio": 1.949748743718593, "no_speech_prob": 3.3404730857000686e-06}, {"id": 733, "seek": 301952, "start": 3019.52, "end": 3023.04, "text": " And so then to calculate a prediction", "tokens": [400, 370, 550, 281, 8873, 257, 17630], "temperature": 0.0, "avg_logprob": -0.2925284735046991, "compression_ratio": 1.8484848484848484, "no_speech_prob": 4.495092980505433e-06}, {"id": 734, "seek": 301952, "start": 3023.36, "end": 3025.68, "text": " Rather than taking the average of all the trees", "tokens": [16571, 813, 1940, 264, 4274, 295, 439, 264, 5852], "temperature": 0.0, "avg_logprob": -0.2925284735046991, "compression_ratio": 1.8484848484848484, "no_speech_prob": 4.495092980505433e-06}, {"id": 735, "seek": 301952, "start": 3026.08, "end": 3031.28, "text": " you take the sum of all the trees because each one has predicted the difference between the actual", "tokens": [291, 747, 264, 2408, 295, 439, 264, 5852, 570, 1184, 472, 575, 19147, 264, 2649, 1296, 264, 3539], "temperature": 0.0, "avg_logprob": -0.2925284735046991, "compression_ratio": 1.8484848484848484, "no_speech_prob": 4.495092980505433e-06}, {"id": 736, "seek": 301952, "start": 3031.84, "end": 3032.88, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.2925284735046991, "compression_ratio": 1.8484848484848484, "no_speech_prob": 4.495092980505433e-06}, {"id": 737, "seek": 301952, "start": 3032.88, "end": 3035.7599999999998, "text": " All of the previous trees and that's called boosting", "tokens": [1057, 295, 264, 3894, 5852, 293, 300, 311, 1219, 43117], "temperature": 0.0, "avg_logprob": -0.2925284735046991, "compression_ratio": 1.8484848484848484, "no_speech_prob": 4.495092980505433e-06}, {"id": 738, "seek": 301952, "start": 3036.8, "end": 3041.68, "text": " Versus bagging so boosting and bagging are two kind of meta ensembling techniques", "tokens": [12226, 301, 3411, 3249, 370, 43117, 293, 3411, 3249, 366, 732, 733, 295, 19616, 12567, 2504, 1688, 7512], "temperature": 0.0, "avg_logprob": -0.2925284735046991, "compression_ratio": 1.8484848484848484, "no_speech_prob": 4.495092980505433e-06}, {"id": 739, "seek": 301952, "start": 3042.4, "end": 3048.64, "text": " And when bagging is applied to trees, it's called a random forest and when boosting is applied to trees", "tokens": [400, 562, 3411, 3249, 307, 6456, 281, 5852, 11, 309, 311, 1219, 257, 4974, 6719, 293, 562, 43117, 307, 6456, 281, 5852], "temperature": 0.0, "avg_logprob": -0.2925284735046991, "compression_ratio": 1.8484848484848484, "no_speech_prob": 4.495092980505433e-06}, {"id": 740, "seek": 304864, "start": 3048.64, "end": 3050.8799999999997, "text": " It's called a gradient boosting machine", "tokens": [467, 311, 1219, 257, 16235, 43117, 3479], "temperature": 0.0, "avg_logprob": -0.2993666607400645, "compression_ratio": 1.6022099447513811, "no_speech_prob": 4.494999757298501e-06}, {"id": 741, "seek": 304864, "start": 3051.44, "end": 3053.44, "text": " or gradient booster decision tree", "tokens": [420, 16235, 29275, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.2993666607400645, "compression_ratio": 1.6022099447513811, "no_speech_prob": 4.494999757298501e-06}, {"id": 742, "seek": 304864, "start": 3056.96, "end": 3062.48, "text": " Gradient boosting is generally speaking more accurate than random forests", "tokens": [16710, 1196, 43117, 307, 5101, 4124, 544, 8559, 813, 4974, 21700], "temperature": 0.0, "avg_logprob": -0.2993666607400645, "compression_ratio": 1.6022099447513811, "no_speech_prob": 4.494999757298501e-06}, {"id": 743, "seek": 304864, "start": 3063.8399999999997, "end": 3065.8399999999997, "text": " But you can absolutely overfit", "tokens": [583, 291, 393, 3122, 670, 6845], "temperature": 0.0, "avg_logprob": -0.2993666607400645, "compression_ratio": 1.6022099447513811, "no_speech_prob": 4.494999757298501e-06}, {"id": 744, "seek": 304864, "start": 3066.8799999999997, "end": 3068.8799999999997, "text": " And so therefore", "tokens": [400, 370, 4412], "temperature": 0.0, "avg_logprob": -0.2993666607400645, "compression_ratio": 1.6022099447513811, "no_speech_prob": 4.494999757298501e-06}, {"id": 745, "seek": 304864, "start": 3068.8799999999997, "end": 3071.12, "text": " It's not necessarily my first go-to thing", "tokens": [467, 311, 406, 4725, 452, 700, 352, 12, 1353, 551], "temperature": 0.0, "avg_logprob": -0.2993666607400645, "compression_ratio": 1.6022099447513811, "no_speech_prob": 4.494999757298501e-06}, {"id": 746, "seek": 304864, "start": 3071.68, "end": 3074.0, "text": " Having said that there are ways to avoid overfitting", "tokens": [10222, 848, 300, 456, 366, 2098, 281, 5042, 670, 69, 2414], "temperature": 0.0, "avg_logprob": -0.2993666607400645, "compression_ratio": 1.6022099447513811, "no_speech_prob": 4.494999757298501e-06}, {"id": 747, "seek": 307400, "start": 3074.0, "end": 3078.56, "text": " But yeah, it's just it's it's not", "tokens": [583, 1338, 11, 309, 311, 445, 309, 311, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.2626246075297511, "compression_ratio": 1.6908212560386473, "no_speech_prob": 6.747750376234762e-06}, {"id": 748, "seek": 307400, "start": 3080.32, "end": 3084.16, "text": " It's it you know because it's breakable it's not my first choice", "tokens": [467, 311, 309, 291, 458, 570, 309, 311, 1821, 712, 309, 311, 406, 452, 700, 3922], "temperature": 0.0, "avg_logprob": -0.2626246075297511, "compression_ratio": 1.6908212560386473, "no_speech_prob": 6.747750376234762e-06}, {"id": 749, "seek": 307400, "start": 3085.2, "end": 3091.36, "text": " But yeah, check out our stuff here if you're interested and you know, you there is stuff which largely automates", "tokens": [583, 1338, 11, 1520, 484, 527, 1507, 510, 498, 291, 434, 3102, 293, 291, 458, 11, 291, 456, 307, 1507, 597, 11611, 3553, 1024], "temperature": 0.0, "avg_logprob": -0.2626246075297511, "compression_ratio": 1.6908212560386473, "no_speech_prob": 6.747750376234762e-06}, {"id": 750, "seek": 307400, "start": 3091.92, "end": 3098.56, "text": " The process there's lots of hyper parameters. You have to select people generally just you know, try every combination of hyper parameters", "tokens": [440, 1399, 456, 311, 3195, 295, 9848, 9834, 13, 509, 362, 281, 3048, 561, 5101, 445, 291, 458, 11, 853, 633, 6562, 295, 9848, 9834], "temperature": 0.0, "avg_logprob": -0.2626246075297511, "compression_ratio": 1.6908212560386473, "no_speech_prob": 6.747750376234762e-06}, {"id": 751, "seek": 309856, "start": 3098.56, "end": 3104.32, "text": " Um, and in the end you're generally should be able to get a more accurate gradient boosting model than random forest", "tokens": [3301, 11, 293, 294, 264, 917, 291, 434, 5101, 820, 312, 1075, 281, 483, 257, 544, 8559, 16235, 43117, 2316, 813, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.3394334030151367, "compression_ratio": 1.3986013986013985, "no_speech_prob": 1.7500156900496222e-05}, {"id": 752, "seek": 309856, "start": 3105.84, "end": 3107.84, "text": " But not necessarily by much", "tokens": [583, 406, 4725, 538, 709], "temperature": 0.0, "avg_logprob": -0.3394334030151367, "compression_ratio": 1.3986013986013985, "no_speech_prob": 1.7500156900496222e-05}, {"id": 753, "seek": 309856, "start": 3112.7999999999997, "end": 3115.84, "text": " Okay, so that was the", "tokens": [1033, 11, 370, 300, 390, 264], "temperature": 0.0, "avg_logprob": -0.3394334030151367, "compression_ratio": 1.3986013986013985, "no_speech_prob": 1.7500156900496222e-05}, {"id": 754, "seek": 309856, "start": 3120.64, "end": 3125.12, "text": " Kaggle notebook on random forests", "tokens": [48751, 22631, 21060, 322, 4974, 21700], "temperature": 0.0, "avg_logprob": -0.3394334030151367, "compression_ratio": 1.3986013986013985, "no_speech_prob": 1.7500156900496222e-05}, {"id": 755, "seek": 312512, "start": 3125.12, "end": 3127.44, "text": " How random forests really work", "tokens": [1012, 4974, 21700, 534, 589], "temperature": 0.0, "avg_logprob": -0.3793547948201497, "compression_ratio": 1.4583333333333333, "no_speech_prob": 8.00622183305677e-06}, {"id": 756, "seek": 312512, "start": 3130.96, "end": 3132.48, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.3793547948201497, "compression_ratio": 1.4583333333333333, "no_speech_prob": 8.00622183305677e-06}, {"id": 757, "seek": 312512, "start": 3132.48, "end": 3136.56, "text": " What we've been doing is having this daily", "tokens": [708, 321, 600, 668, 884, 307, 1419, 341, 5212], "temperature": 0.0, "avg_logprob": -0.3793547948201497, "compression_ratio": 1.4583333333333333, "no_speech_prob": 8.00622183305677e-06}, {"id": 758, "seek": 312512, "start": 3137.52, "end": 3138.88, "text": " Walkthrough", "tokens": [10818, 11529], "temperature": 0.0, "avg_logprob": -0.3793547948201497, "compression_ratio": 1.4583333333333333, "no_speech_prob": 8.00622183305677e-06}, {"id": 759, "seek": 312512, "start": 3138.88, "end": 3145.2, "text": " Where me and I don't know how many 20 or 30 folks get together on a zoom call and chat about", "tokens": [2305, 385, 293, 286, 500, 380, 458, 577, 867, 945, 420, 2217, 4024, 483, 1214, 322, 257, 8863, 818, 293, 5081, 466], "temperature": 0.0, "avg_logprob": -0.3793547948201497, "compression_ratio": 1.4583333333333333, "no_speech_prob": 8.00622183305677e-06}, {"id": 760, "seek": 312512, "start": 3146.48, "end": 3148.48, "text": " you know getting through the course and", "tokens": [291, 458, 1242, 807, 264, 1164, 293], "temperature": 0.0, "avg_logprob": -0.3793547948201497, "compression_ratio": 1.4583333333333333, "no_speech_prob": 8.00622183305677e-06}, {"id": 761, "seek": 312512, "start": 3149.2799999999997, "end": 3151.2799999999997, "text": " setting up machines and", "tokens": [3287, 493, 8379, 293], "temperature": 0.0, "avg_logprob": -0.3793547948201497, "compression_ratio": 1.4583333333333333, "no_speech_prob": 8.00622183305677e-06}, {"id": 762, "seek": 315128, "start": 3151.28, "end": 3154.5600000000004, "text": " Stuff like that. Um, and", "tokens": [31347, 411, 300, 13, 3301, 11, 293], "temperature": 0.0, "avg_logprob": -0.3596226761980755, "compression_ratio": 1.5024630541871922, "no_speech_prob": 3.9663968891545665e-06}, {"id": 763, "seek": 315128, "start": 3155.84, "end": 3159.28, "text": " You know, we've been trying to kind of practice what you know things along the way", "tokens": [509, 458, 11, 321, 600, 668, 1382, 281, 733, 295, 3124, 437, 291, 458, 721, 2051, 264, 636], "temperature": 0.0, "avg_logprob": -0.3596226761980755, "compression_ratio": 1.5024630541871922, "no_speech_prob": 3.9663968891545665e-06}, {"id": 764, "seek": 315128, "start": 3160.48, "end": 3165.28, "text": " And so a couple of weeks ago. I wanted to show like", "tokens": [400, 370, 257, 1916, 295, 3259, 2057, 13, 286, 1415, 281, 855, 411], "temperature": 0.0, "avg_logprob": -0.3596226761980755, "compression_ratio": 1.5024630541871922, "no_speech_prob": 3.9663968891545665e-06}, {"id": 765, "seek": 315128, "start": 3166.2400000000002, "end": 3169.44, "text": " What does it look like to pick a Kaggle competition and just like", "tokens": [708, 775, 309, 574, 411, 281, 1888, 257, 48751, 22631, 6211, 293, 445, 411], "temperature": 0.0, "avg_logprob": -0.3596226761980755, "compression_ratio": 1.5024630541871922, "no_speech_prob": 3.9663968891545665e-06}, {"id": 766, "seek": 315128, "start": 3171.28, "end": 3173.28, "text": " Do the normal sensible", "tokens": [1144, 264, 2710, 25380], "temperature": 0.0, "avg_logprob": -0.3596226761980755, "compression_ratio": 1.5024630541871922, "no_speech_prob": 3.9663968891545665e-06}, {"id": 767, "seek": 315128, "start": 3175.44, "end": 3178.4, "text": " Kind of mechanical steps that you're going to have to do", "tokens": [9242, 295, 12070, 4439, 300, 291, 434, 516, 281, 362, 281, 360], "temperature": 0.0, "avg_logprob": -0.3596226761980755, "compression_ratio": 1.5024630541871922, "no_speech_prob": 3.9663968891545665e-06}, {"id": 768, "seek": 317840, "start": 3178.4, "end": 3182.4, "text": " Kind of mechanical steps that you would do for any computer vision model", "tokens": [9242, 295, 12070, 4439, 300, 291, 576, 360, 337, 604, 3820, 5201, 2316], "temperature": 0.0, "avg_logprob": -0.2717772858052314, "compression_ratio": 1.5, "no_speech_prob": 7.52628875488881e-06}, {"id": 769, "seek": 317840, "start": 3184.64, "end": 3186.64, "text": " And so the", "tokens": [400, 370, 264], "temperature": 0.0, "avg_logprob": -0.2717772858052314, "compression_ratio": 1.5, "no_speech_prob": 7.52628875488881e-06}, {"id": 770, "seek": 317840, "start": 3189.12, "end": 3192.08, "text": " Competition I picked was patty disease classification", "tokens": [43634, 286, 6183, 390, 1947, 874, 4752, 21538], "temperature": 0.0, "avg_logprob": -0.2717772858052314, "compression_ratio": 1.5, "no_speech_prob": 7.52628875488881e-06}, {"id": 771, "seek": 317840, "start": 3193.28, "end": 3195.28, "text": " Which is about", "tokens": [3013, 307, 466], "temperature": 0.0, "avg_logprob": -0.2717772858052314, "compression_ratio": 1.5, "no_speech_prob": 7.52628875488881e-06}, {"id": 772, "seek": 317840, "start": 3195.58, "end": 3198.64, "text": " Recognizing diseases rice diseases and rice patties", "tokens": [44682, 3319, 11044, 5090, 11044, 293, 5090, 49916, 530], "temperature": 0.0, "avg_logprob": -0.2717772858052314, "compression_ratio": 1.5, "no_speech_prob": 7.52628875488881e-06}, {"id": 773, "seek": 317840, "start": 3200.0, "end": 3203.6800000000003, "text": " And yeah, I spent I don't know a couple of hours or three. I can't remember a few hours", "tokens": [400, 1338, 11, 286, 4418, 286, 500, 380, 458, 257, 1916, 295, 2496, 420, 1045, 13, 286, 393, 380, 1604, 257, 1326, 2496], "temperature": 0.0, "avg_logprob": -0.2717772858052314, "compression_ratio": 1.5, "no_speech_prob": 7.52628875488881e-06}, {"id": 774, "seek": 320368, "start": 3203.68, "end": 3210.08, "text": " Throwing together something and um, I found that I was number one on the leaderboard", "tokens": [22228, 278, 1214, 746, 293, 1105, 11, 286, 1352, 300, 286, 390, 1230, 472, 322, 264, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.2684386474917633, "compression_ratio": 1.7719298245614035, "no_speech_prob": 8.800400792097207e-06}, {"id": 775, "seek": 320368, "start": 3211.04, "end": 3216.3199999999997, "text": " And I thought oh, that's that's interesting like um, because you never quite have a sense of", "tokens": [400, 286, 1194, 1954, 11, 300, 311, 300, 311, 1880, 411, 1105, 11, 570, 291, 1128, 1596, 362, 257, 2020, 295], "temperature": 0.0, "avg_logprob": -0.2684386474917633, "compression_ratio": 1.7719298245614035, "no_speech_prob": 8.800400792097207e-06}, {"id": 776, "seek": 320368, "start": 3217.6, "end": 3219.6, "text": " How well these things work", "tokens": [1012, 731, 613, 721, 589], "temperature": 0.0, "avg_logprob": -0.2684386474917633, "compression_ratio": 1.7719298245614035, "no_speech_prob": 8.800400792097207e-06}, {"id": 777, "seek": 320368, "start": 3220.64, "end": 3224.3199999999997, "text": " And then I thought well, there's all these other things we should be doing as well and I tried", "tokens": [400, 550, 286, 1194, 731, 11, 456, 311, 439, 613, 661, 721, 321, 820, 312, 884, 382, 731, 293, 286, 3031], "temperature": 0.0, "avg_logprob": -0.2684386474917633, "compression_ratio": 1.7719298245614035, "no_speech_prob": 8.800400792097207e-06}, {"id": 778, "seek": 320368, "start": 3225.2799999999997, "end": 3232.0, "text": " Three more things and each time I tried another thing. I got further ahead at the top of the leaderboard", "tokens": [6244, 544, 721, 293, 1184, 565, 286, 3031, 1071, 551, 13, 286, 658, 3052, 2286, 412, 264, 1192, 295, 264, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.2684386474917633, "compression_ratio": 1.7719298245614035, "no_speech_prob": 8.800400792097207e-06}, {"id": 779, "seek": 323200, "start": 3232.0, "end": 3235.52, "text": " So, um, I thought it'd be cool to take you through", "tokens": [407, 11, 1105, 11, 286, 1194, 309, 1116, 312, 1627, 281, 747, 291, 807], "temperature": 0.0, "avg_logprob": -0.3164268250161029, "compression_ratio": 1.6490384615384615, "no_speech_prob": 1.1123800504719839e-05}, {"id": 780, "seek": 323200, "start": 3236.32, "end": 3238.8, "text": " the process i'm going to do it", "tokens": [264, 1399, 741, 478, 516, 281, 360, 309], "temperature": 0.0, "avg_logprob": -0.3164268250161029, "compression_ratio": 1.6490384615384615, "no_speech_prob": 1.1123800504719839e-05}, {"id": 781, "seek": 323200, "start": 3239.28, "end": 3241.28, "text": " reasonably quickly", "tokens": [23551, 2661], "temperature": 0.0, "avg_logprob": -0.3164268250161029, "compression_ratio": 1.6490384615384615, "no_speech_prob": 1.1123800504719839e-05}, {"id": 782, "seek": 323200, "start": 3241.34, "end": 3243.34, "text": " because um", "tokens": [570, 1105], "temperature": 0.0, "avg_logprob": -0.3164268250161029, "compression_ratio": 1.6490384615384615, "no_speech_prob": 1.1123800504719839e-05}, {"id": 783, "seek": 323200, "start": 3244.16, "end": 3246.16, "text": " The walkthroughs are all available", "tokens": [440, 1792, 11529, 82, 366, 439, 2435], "temperature": 0.0, "avg_logprob": -0.3164268250161029, "compression_ratio": 1.6490384615384615, "no_speech_prob": 1.1123800504719839e-05}, {"id": 784, "seek": 323200, "start": 3246.72, "end": 3252.96, "text": " For you to see the entire thing in you know, seven hours of detail or however long we probably were six to seven hours", "tokens": [1171, 291, 281, 536, 264, 2302, 551, 294, 291, 458, 11, 3407, 2496, 295, 2607, 420, 4461, 938, 321, 1391, 645, 2309, 281, 3407, 2496], "temperature": 0.0, "avg_logprob": -0.3164268250161029, "compression_ratio": 1.6490384615384615, "no_speech_prob": 1.1123800504719839e-05}, {"id": 785, "seek": 323200, "start": 3253.44, "end": 3254.96, "text": " of conversations", "tokens": [295, 7315], "temperature": 0.0, "avg_logprob": -0.3164268250161029, "compression_ratio": 1.6490384615384615, "no_speech_prob": 1.1123800504719839e-05}, {"id": 786, "seek": 323200, "start": 3254.96, "end": 3257.44, "text": " Um, but I want to kind of take you through the basic", "tokens": [3301, 11, 457, 286, 528, 281, 733, 295, 747, 291, 807, 264, 3875], "temperature": 0.0, "avg_logprob": -0.3164268250161029, "compression_ratio": 1.6490384615384615, "no_speech_prob": 1.1123800504719839e-05}, {"id": 787, "seek": 323200, "start": 3258.16, "end": 3259.68, "text": " process", "tokens": [1399], "temperature": 0.0, "avg_logprob": -0.3164268250161029, "compression_ratio": 1.6490384615384615, "no_speech_prob": 1.1123800504719839e-05}, {"id": 788, "seek": 325968, "start": 3259.68, "end": 3261.68, "text": " That I went through", "tokens": [663, 286, 1437, 807], "temperature": 0.0, "avg_logprob": -0.26236758512609143, "compression_ratio": 1.504950495049505, "no_speech_prob": 1.0288334124197718e-05}, {"id": 789, "seek": 325968, "start": 3265.9199999999996, "end": 3271.7599999999998, "text": " So since i've been starting to do more stuff on kaggle, um, you know, I realized there's some", "tokens": [407, 1670, 741, 600, 668, 2891, 281, 360, 544, 1507, 322, 350, 559, 22631, 11, 1105, 11, 291, 458, 11, 286, 5334, 456, 311, 512], "temperature": 0.0, "avg_logprob": -0.26236758512609143, "compression_ratio": 1.504950495049505, "no_speech_prob": 1.0288334124197718e-05}, {"id": 790, "seek": 325968, "start": 3273.3599999999997, "end": 3278.96, "text": " Kind of menial steps I have to do each time particularly because I like to run stuff on my own machine", "tokens": [9242, 295, 1706, 831, 4439, 286, 362, 281, 360, 1184, 565, 4098, 570, 286, 411, 281, 1190, 1507, 322, 452, 1065, 3479], "temperature": 0.0, "avg_logprob": -0.26236758512609143, "compression_ratio": 1.504950495049505, "no_speech_prob": 1.0288334124197718e-05}, {"id": 791, "seek": 325968, "start": 3279.68, "end": 3281.68, "text": " And then kind of upload it to kaggle", "tokens": [400, 550, 733, 295, 6580, 309, 281, 350, 559, 22631], "temperature": 0.0, "avg_logprob": -0.26236758512609143, "compression_ratio": 1.504950495049505, "no_speech_prob": 1.0288334124197718e-05}, {"id": 792, "seek": 325968, "start": 3282.96, "end": 3286.48, "text": " So to do to make my life easier I created a little", "tokens": [407, 281, 360, 281, 652, 452, 993, 3571, 286, 2942, 257, 707], "temperature": 0.0, "avg_logprob": -0.26236758512609143, "compression_ratio": 1.504950495049505, "no_speech_prob": 1.0288334124197718e-05}, {"id": 793, "seek": 328648, "start": 3286.48, "end": 3292.0, "text": " Module called fast kaggle, which you'll see in my notebooks from now on which you can download from", "tokens": [48251, 1219, 2370, 350, 559, 22631, 11, 597, 291, 603, 536, 294, 452, 43782, 490, 586, 322, 597, 291, 393, 5484, 490], "temperature": 0.0, "avg_logprob": -0.31520332174098237, "compression_ratio": 1.6367924528301887, "no_speech_prob": 1.6696483726263978e-05}, {"id": 794, "seek": 328648, "start": 3292.56, "end": 3294.56, "text": " pit or conda", "tokens": [10147, 420, 2224, 64], "temperature": 0.0, "avg_logprob": -0.31520332174098237, "compression_ratio": 1.6367924528301887, "no_speech_prob": 1.6696483726263978e-05}, {"id": 795, "seek": 328648, "start": 3295.84, "end": 3298.8, "text": " And as you'll see it makes some things a bit easier for example", "tokens": [400, 382, 291, 603, 536, 309, 1669, 512, 721, 257, 857, 3571, 337, 1365], "temperature": 0.0, "avg_logprob": -0.31520332174098237, "compression_ratio": 1.6367924528301887, "no_speech_prob": 1.6696483726263978e-05}, {"id": 796, "seek": 328648, "start": 3299.5, "end": 3302.58, "text": " downloading the data for the patty disease classification", "tokens": [32529, 264, 1412, 337, 264, 1947, 874, 4752, 21538], "temperature": 0.0, "avg_logprob": -0.31520332174098237, "compression_ratio": 1.6367924528301887, "no_speech_prob": 1.6696483726263978e-05}, {"id": 797, "seek": 328648, "start": 3303.6, "end": 3305.6, "text": " If you just run setup comp", "tokens": [759, 291, 445, 1190, 8657, 715], "temperature": 0.0, "avg_logprob": -0.31520332174098237, "compression_ratio": 1.6367924528301887, "no_speech_prob": 1.6696483726263978e-05}, {"id": 798, "seek": 328648, "start": 3306.0, "end": 3308.16, "text": " And pass in the name of the competition", "tokens": [400, 1320, 294, 264, 1315, 295, 264, 6211], "temperature": 0.0, "avg_logprob": -0.31520332174098237, "compression_ratio": 1.6367924528301887, "no_speech_prob": 1.6696483726263978e-05}, {"id": 799, "seek": 328648, "start": 3308.8, "end": 3310.64, "text": " If you are on kaggle", "tokens": [759, 291, 366, 322, 350, 559, 22631], "temperature": 0.0, "avg_logprob": -0.31520332174098237, "compression_ratio": 1.6367924528301887, "no_speech_prob": 1.6696483726263978e-05}, {"id": 800, "seek": 328648, "start": 3310.64, "end": 3312.64, "text": " It will return a path to", "tokens": [467, 486, 2736, 257, 3100, 281], "temperature": 0.0, "avg_logprob": -0.31520332174098237, "compression_ratio": 1.6367924528301887, "no_speech_prob": 1.6696483726263978e-05}, {"id": 801, "seek": 331264, "start": 3312.64, "end": 3315.68, "text": " On kaggle, it will return a path to", "tokens": [1282, 350, 559, 22631, 11, 309, 486, 2736, 257, 3100, 281], "temperature": 0.0, "avg_logprob": -0.10957230896246238, "compression_ratio": 2.0813397129186604, "no_speech_prob": 7.29589964976185e-06}, {"id": 802, "seek": 331264, "start": 3316.72, "end": 3318.72, "text": " that", "tokens": [300], "temperature": 0.0, "avg_logprob": -0.10957230896246238, "compression_ratio": 2.0813397129186604, "no_speech_prob": 7.29589964976185e-06}, {"id": 803, "seek": 331264, "start": 3318.7999999999997, "end": 3320.7999999999997, "text": " Competition data that's already on kaggle", "tokens": [43634, 1412, 300, 311, 1217, 322, 350, 559, 22631], "temperature": 0.0, "avg_logprob": -0.10957230896246238, "compression_ratio": 2.0813397129186604, "no_speech_prob": 7.29589964976185e-06}, {"id": 804, "seek": 331264, "start": 3320.7999999999997, "end": 3325.44, "text": " If you are not on kaggle and you haven't downloaded it it will download and unzip the data for you", "tokens": [759, 291, 366, 406, 322, 350, 559, 22631, 293, 291, 2378, 380, 21748, 309, 309, 486, 5484, 293, 517, 27268, 264, 1412, 337, 291], "temperature": 0.0, "avg_logprob": -0.10957230896246238, "compression_ratio": 2.0813397129186604, "no_speech_prob": 7.29589964976185e-06}, {"id": 805, "seek": 331264, "start": 3325.8399999999997, "end": 3330.8799999999997, "text": " If you're not on kaggle and you have downloaded unzip the data, it will return a path to the one that you've already downloaded", "tokens": [759, 291, 434, 406, 322, 350, 559, 22631, 293, 291, 362, 21748, 517, 27268, 264, 1412, 11, 309, 486, 2736, 257, 3100, 281, 264, 472, 300, 291, 600, 1217, 21748], "temperature": 0.0, "avg_logprob": -0.10957230896246238, "compression_ratio": 2.0813397129186604, "no_speech_prob": 7.29589964976185e-06}, {"id": 806, "seek": 331264, "start": 3331.68, "end": 3334.64, "text": " also, if you are on kaggle, you can ask it to make sure that", "tokens": [611, 11, 498, 291, 366, 322, 350, 559, 22631, 11, 291, 393, 1029, 309, 281, 652, 988, 300], "temperature": 0.0, "avg_logprob": -0.10957230896246238, "compression_ratio": 2.0813397129186604, "no_speech_prob": 7.29589964976185e-06}, {"id": 807, "seek": 331264, "start": 3335.68, "end": 3339.12, "text": " Pip things are installed that might not be up to date. Otherwise", "tokens": [35396, 721, 366, 8899, 300, 1062, 406, 312, 493, 281, 4002, 13, 10328], "temperature": 0.0, "avg_logprob": -0.10957230896246238, "compression_ratio": 2.0813397129186604, "no_speech_prob": 7.29589964976185e-06}, {"id": 808, "seek": 333912, "start": 3339.12, "end": 3343.6, "text": " Um, so this basically one line of code now gets us all set up and ready to go", "tokens": [3301, 11, 370, 341, 1936, 472, 1622, 295, 3089, 586, 2170, 505, 439, 992, 493, 293, 1919, 281, 352], "temperature": 0.0, "avg_logprob": -0.12825368951868127, "compression_ratio": 1.5297297297297296, "no_speech_prob": 1.1124368029413745e-05}, {"id": 809, "seek": 333912, "start": 3344.96, "end": 3346.96, "text": " So this path", "tokens": [407, 341, 3100], "temperature": 0.0, "avg_logprob": -0.12825368951868127, "compression_ratio": 1.5297297297297296, "no_speech_prob": 1.1124368029413745e-05}, {"id": 810, "seek": 333912, "start": 3347.68, "end": 3352.88, "text": " So I ran this particular one on my own machine so it's downloaded and unzipped the data", "tokens": [407, 286, 5872, 341, 1729, 472, 322, 452, 1065, 3479, 370, 309, 311, 21748, 293, 517, 89, 5529, 264, 1412], "temperature": 0.0, "avg_logprob": -0.12825368951868127, "compression_ratio": 1.5297297297297296, "no_speech_prob": 1.1124368029413745e-05}, {"id": 811, "seek": 333912, "start": 3353.92, "end": 3355.92, "text": " I've also got links to the", "tokens": [286, 600, 611, 658, 6123, 281, 264], "temperature": 0.0, "avg_logprob": -0.12825368951868127, "compression_ratio": 1.5297297297297296, "no_speech_prob": 1.1124368029413745e-05}, {"id": 812, "seek": 333912, "start": 3356.7999999999997, "end": 3360.0, "text": " Six walkthroughs so far into the videos", "tokens": [11678, 1792, 11529, 82, 370, 1400, 666, 264, 2145], "temperature": 0.0, "avg_logprob": -0.12825368951868127, "compression_ratio": 1.5297297297297296, "no_speech_prob": 1.1124368029413745e-05}, {"id": 813, "seek": 333912, "start": 3362.24, "end": 3366.3199999999997, "text": " Yes, and here's my result after these", "tokens": [1079, 11, 293, 510, 311, 452, 1874, 934, 613], "temperature": 0.0, "avg_logprob": -0.12825368951868127, "compression_ratio": 1.5297297297297296, "no_speech_prob": 1.1124368029413745e-05}, {"id": 814, "seek": 336632, "start": 3366.32, "end": 3369.36, "text": " Four attempts that's a few fiddling around at the start", "tokens": [7451, 15257, 300, 311, 257, 1326, 283, 14273, 1688, 926, 412, 264, 722], "temperature": 0.0, "avg_logprob": -0.2590379507645317, "compression_ratio": 1.6891891891891893, "no_speech_prob": 1.0614260645525064e-05}, {"id": 815, "seek": 336632, "start": 3372.7200000000003, "end": 3381.84, "text": " So the overall approach at is well and this is not just to a kaggle competition right the reason I like looking at kaggle competitions is", "tokens": [407, 264, 4787, 3109, 412, 307, 731, 293, 341, 307, 406, 445, 281, 257, 350, 559, 22631, 6211, 558, 264, 1778, 286, 411, 1237, 412, 350, 559, 22631, 26185, 307], "temperature": 0.0, "avg_logprob": -0.2590379507645317, "compression_ratio": 1.6891891891891893, "no_speech_prob": 1.0614260645525064e-05}, {"id": 816, "seek": 336632, "start": 3383.6000000000004, "end": 3385.6000000000004, "text": " You can't hide from the truth", "tokens": [509, 393, 380, 6479, 490, 264, 3494], "temperature": 0.0, "avg_logprob": -0.2590379507645317, "compression_ratio": 1.6891891891891893, "no_speech_prob": 1.0614260645525064e-05}, {"id": 817, "seek": 336632, "start": 3386.32, "end": 3390.7200000000003, "text": " In a kaggle competition, you know when you're working on some work project or something", "tokens": [682, 257, 350, 559, 22631, 6211, 11, 291, 458, 562, 291, 434, 1364, 322, 512, 589, 1716, 420, 746], "temperature": 0.0, "avg_logprob": -0.2590379507645317, "compression_ratio": 1.6891891891891893, "no_speech_prob": 1.0614260645525064e-05}, {"id": 818, "seek": 336632, "start": 3391.28, "end": 3394.2400000000002, "text": " You might be able to convince yourself and everybody around you", "tokens": [509, 1062, 312, 1075, 281, 13447, 1803, 293, 2201, 926, 291], "temperature": 0.0, "avg_logprob": -0.2590379507645317, "compression_ratio": 1.6891891891891893, "no_speech_prob": 1.0614260645525064e-05}, {"id": 819, "seek": 339424, "start": 3394.24, "end": 3396.24, "text": " that you've done a fantastic job of", "tokens": [300, 291, 600, 1096, 257, 5456, 1691, 295], "temperature": 0.0, "avg_logprob": -0.3209383138020833, "compression_ratio": 1.5693069306930694, "no_speech_prob": 6.143654900370166e-06}, {"id": 820, "seek": 339424, "start": 3397.04, "end": 3402.24, "text": " not overfitting and your model's better than what anybody else could have made or whatever else but", "tokens": [406, 670, 69, 2414, 293, 428, 2316, 311, 1101, 813, 437, 4472, 1646, 727, 362, 1027, 420, 2035, 1646, 457], "temperature": 0.0, "avg_logprob": -0.3209383138020833, "compression_ratio": 1.5693069306930694, "no_speech_prob": 6.143654900370166e-06}, {"id": 821, "seek": 339424, "start": 3403.6, "end": 3406.64, "text": " The brutal assessment of the private leaderboard", "tokens": [440, 17878, 9687, 295, 264, 4551, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.3209383138020833, "compression_ratio": 1.5693069306930694, "no_speech_prob": 6.143654900370166e-06}, {"id": 822, "seek": 339424, "start": 3407.68, "end": 3409.68, "text": " Will tell you the truth", "tokens": [3099, 980, 291, 264, 3494], "temperature": 0.0, "avg_logprob": -0.3209383138020833, "compression_ratio": 1.5693069306930694, "no_speech_prob": 6.143654900370166e-06}, {"id": 823, "seek": 339424, "start": 3410.72, "end": 3415.8399999999997, "text": " Is your model actually predicting things correctly and is it overfit?", "tokens": [1119, 428, 2316, 767, 32884, 721, 8944, 293, 307, 309, 670, 6845, 30], "temperature": 0.0, "avg_logprob": -0.3209383138020833, "compression_ratio": 1.5693069306930694, "no_speech_prob": 6.143654900370166e-06}, {"id": 824, "seek": 339424, "start": 3420.0, "end": 3422.0, "text": " Until you've been through that process", "tokens": [9088, 291, 600, 668, 807, 300, 1399], "temperature": 0.0, "avg_logprob": -0.3209383138020833, "compression_ratio": 1.5693069306930694, "no_speech_prob": 6.143654900370166e-06}, {"id": 825, "seek": 342200, "start": 3422.0, "end": 3427.12, "text": " You know, you're never going to know and a lot of people don't go through that process because at some level they don't want to know", "tokens": [509, 458, 11, 291, 434, 1128, 516, 281, 458, 293, 257, 688, 295, 561, 500, 380, 352, 807, 300, 1399, 570, 412, 512, 1496, 436, 500, 380, 528, 281, 458], "temperature": 0.0, "avg_logprob": -0.21984234978170955, "compression_ratio": 1.73828125, "no_speech_prob": 1.1124174307042267e-05}, {"id": 826, "seek": 342200, "start": 3429.92, "end": 3434.08, "text": " But it's okay, you know, nobody need you don't have to put your own name there", "tokens": [583, 309, 311, 1392, 11, 291, 458, 11, 5079, 643, 291, 500, 380, 362, 281, 829, 428, 1065, 1315, 456], "temperature": 0.0, "avg_logprob": -0.21984234978170955, "compression_ratio": 1.73828125, "no_speech_prob": 1.1124174307042267e-05}, {"id": 827, "seek": 342200, "start": 3436.24, "end": 3436.72, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.21984234978170955, "compression_ratio": 1.73828125, "no_speech_prob": 1.1124174307042267e-05}, {"id": 828, "seek": 342200, "start": 3436.72, "end": 3441.04, "text": " Always did right from the very first one. I wanted you know, if I was going to screw up really", "tokens": [11270, 630, 558, 490, 264, 588, 700, 472, 13, 286, 1415, 291, 458, 11, 498, 286, 390, 516, 281, 5630, 493, 534], "temperature": 0.0, "avg_logprob": -0.21984234978170955, "compression_ratio": 1.73828125, "no_speech_prob": 1.1124174307042267e-05}, {"id": 829, "seek": 342200, "start": 3441.04, "end": 3444.4, "text": " I wanted to have the pressure on myself of people seeing me in last place", "tokens": [286, 1415, 281, 362, 264, 3321, 322, 2059, 295, 561, 2577, 385, 294, 1036, 1081], "temperature": 0.0, "avg_logprob": -0.21984234978170955, "compression_ratio": 1.73828125, "no_speech_prob": 1.1124174307042267e-05}, {"id": 830, "seek": 342200, "start": 3445.44, "end": 3448.0, "text": " But you know, it's it's fine. You could do it all and honestly", "tokens": [583, 291, 458, 11, 309, 311, 309, 311, 2489, 13, 509, 727, 360, 309, 439, 293, 6095], "temperature": 0.0, "avg_logprob": -0.21984234978170955, "compression_ratio": 1.73828125, "no_speech_prob": 1.1124174307042267e-05}, {"id": 831, "seek": 344800, "start": 3448.0, "end": 3450.0, "text": " and you'll actually find", "tokens": [293, 291, 603, 767, 915], "temperature": 0.0, "avg_logprob": -0.31432136447950343, "compression_ratio": 1.5896226415094339, "no_speech_prob": 1.0782747267512605e-05}, {"id": 832, "seek": 344800, "start": 3451.36, "end": 3455.92, "text": " As you improve you'll have so much self-confidence, you know", "tokens": [1018, 291, 3470, 291, 603, 362, 370, 709, 2698, 12, 47273, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.31432136447950343, "compression_ratio": 1.5896226415094339, "no_speech_prob": 1.0782747267512605e-05}, {"id": 833, "seek": 344800, "start": 3457.36, "end": 3458.24, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.31432136447950343, "compression_ratio": 1.5896226415094339, "no_speech_prob": 1.0782747267512605e-05}, {"id": 834, "seek": 344800, "start": 3458.24, "end": 3463.68, "text": " The stuff we're doing a Kaggle competition is indeed a subset of the things we need to do in real life", "tokens": [440, 1507, 321, 434, 884, 257, 48751, 22631, 6211, 307, 6451, 257, 25993, 295, 264, 721, 321, 643, 281, 360, 294, 957, 993], "temperature": 0.0, "avg_logprob": -0.31432136447950343, "compression_ratio": 1.5896226415094339, "no_speech_prob": 1.0782747267512605e-05}, {"id": 835, "seek": 344800, "start": 3464.48, "end": 3465.84, "text": " but", "tokens": [457], "temperature": 0.0, "avg_logprob": -0.31432136447950343, "compression_ratio": 1.5896226415094339, "no_speech_prob": 1.0782747267512605e-05}, {"id": 836, "seek": 344800, "start": 3465.84, "end": 3467.84, "text": " It's an important subset, you know", "tokens": [467, 311, 364, 1021, 25993, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.31432136447950343, "compression_ratio": 1.5896226415094339, "no_speech_prob": 1.0782747267512605e-05}, {"id": 837, "seek": 344800, "start": 3468.4, "end": 3473.6, "text": " Building a model that actually predicts things correctly and doesn't overfit is important and furthermore", "tokens": [18974, 257, 2316, 300, 767, 6069, 82, 721, 8944, 293, 1177, 380, 670, 6845, 307, 1021, 293, 3052, 3138], "temperature": 0.0, "avg_logprob": -0.31432136447950343, "compression_ratio": 1.5896226415094339, "no_speech_prob": 1.0782747267512605e-05}, {"id": 838, "seek": 347360, "start": 3473.6, "end": 3478.64, "text": " Structuring your code and analysis in such a way that you can keep improving over a three-month period", "tokens": [745, 1757, 1345, 428, 3089, 293, 5215, 294, 1270, 257, 636, 300, 291, 393, 1066, 11470, 670, 257, 1045, 12, 23534, 2896], "temperature": 0.0, "avg_logprob": -0.3268230983189174, "compression_ratio": 1.5172413793103448, "no_speech_prob": 3.905242010660004e-06}, {"id": 839, "seek": 347360, "start": 3478.64, "end": 3483.52, "text": " without gradually getting into more and more of a tangled mess of impossible to understand code and", "tokens": [1553, 13145, 1242, 666, 544, 293, 544, 295, 257, 47192, 2082, 295, 6243, 281, 1223, 3089, 293], "temperature": 0.0, "avg_logprob": -0.3268230983189174, "compression_ratio": 1.5172413793103448, "no_speech_prob": 3.905242010660004e-06}, {"id": 840, "seek": 347360, "start": 3484.16, "end": 3485.7599999999998, "text": " Having no idea", "tokens": [10222, 572, 1558], "temperature": 0.0, "avg_logprob": -0.3268230983189174, "compression_ratio": 1.5172413793103448, "no_speech_prob": 3.905242010660004e-06}, {"id": 841, "seek": 347360, "start": 3485.7599999999998, "end": 3491.12, "text": " What untitled copy 13 was and why it was better than 25?", "tokens": [708, 1701, 270, 1493, 5055, 3705, 390, 293, 983, 309, 390, 1101, 813, 3552, 30], "temperature": 0.0, "avg_logprob": -0.3268230983189174, "compression_ratio": 1.5172413793103448, "no_speech_prob": 3.905242010660004e-06}, {"id": 842, "seek": 347360, "start": 3491.92, "end": 3493.92, "text": " Right. This is all", "tokens": [1779, 13, 639, 307, 439], "temperature": 0.0, "avg_logprob": -0.3268230983189174, "compression_ratio": 1.5172413793103448, "no_speech_prob": 3.905242010660004e-06}, {"id": 843, "seek": 347360, "start": 3494.08, "end": 3496.08, "text": " Stuff you want to be practicing", "tokens": [31347, 291, 528, 281, 312, 11350], "temperature": 0.0, "avg_logprob": -0.3268230983189174, "compression_ratio": 1.5172413793103448, "no_speech_prob": 3.905242010660004e-06}, {"id": 844, "seek": 347360, "start": 3496.48, "end": 3498.0, "text": " ideally", "tokens": [22915], "temperature": 0.0, "avg_logprob": -0.3268230983189174, "compression_ratio": 1.5172413793103448, "no_speech_prob": 3.905242010660004e-06}, {"id": 845, "seek": 347360, "start": 3498.0, "end": 3498.96, "text": " Well away from the", "tokens": [1042, 1314, 490, 264], "temperature": 0.0, "avg_logprob": -0.3268230983189174, "compression_ratio": 1.5172413793103448, "no_speech_prob": 3.905242010660004e-06}, {"id": 846, "seek": 349896, "start": 3498.96, "end": 3504.0, "text": " Customers or whatever, you know before you've kind of figured things out", "tokens": [16649, 433, 420, 2035, 11, 291, 458, 949, 291, 600, 733, 295, 8932, 721, 484], "temperature": 0.0, "avg_logprob": -0.28120741059508503, "compression_ratio": 1.5829383886255923, "no_speech_prob": 9.570110250933794e-07}, {"id": 847, "seek": 349896, "start": 3505.12, "end": 3509.92, "text": " So the things I talk about here about doing things well in this Kaggle competition", "tokens": [407, 264, 721, 286, 751, 466, 510, 466, 884, 721, 731, 294, 341, 48751, 22631, 6211], "temperature": 0.0, "avg_logprob": -0.28120741059508503, "compression_ratio": 1.5829383886255923, "no_speech_prob": 9.570110250933794e-07}, {"id": 848, "seek": 349896, "start": 3511.92, "end": 3514.96, "text": " Should work, you know in other settings as well", "tokens": [6454, 589, 11, 291, 458, 294, 661, 6257, 382, 731], "temperature": 0.0, "avg_logprob": -0.28120741059508503, "compression_ratio": 1.5829383886255923, "no_speech_prob": 9.570110250933794e-07}, {"id": 849, "seek": 349896, "start": 3516.08, "end": 3519.28, "text": " And so these are the two focuses that I recommend", "tokens": [400, 370, 613, 366, 264, 732, 16109, 300, 286, 2748], "temperature": 0.0, "avg_logprob": -0.28120741059508503, "compression_ratio": 1.5829383886255923, "no_speech_prob": 9.570110250933794e-07}, {"id": 850, "seek": 349896, "start": 3521.12, "end": 3524.48, "text": " Get a really good validation set together. We've talked about that before right?", "tokens": [3240, 257, 534, 665, 24071, 992, 1214, 13, 492, 600, 2825, 466, 300, 949, 558, 30], "temperature": 0.0, "avg_logprob": -0.28120741059508503, "compression_ratio": 1.5829383886255923, "no_speech_prob": 9.570110250933794e-07}, {"id": 851, "seek": 352448, "start": 3524.48, "end": 3531.44, "text": " And in a Kaggle competition, that's like it's very rare to see people do well in a Kaggle competition who don't have a good validation set", "tokens": [400, 294, 257, 48751, 22631, 6211, 11, 300, 311, 411, 309, 311, 588, 5892, 281, 536, 561, 360, 731, 294, 257, 48751, 22631, 6211, 567, 500, 380, 362, 257, 665, 24071, 992], "temperature": 0.0, "avg_logprob": -0.27609028418858844, "compression_ratio": 1.726027397260274, "no_speech_prob": 8.800311661616433e-06}, {"id": 852, "seek": 352448, "start": 3532.4, "end": 3536.8, "text": " sometimes that's easy in this competition actually it is easy because the", "tokens": [2171, 300, 311, 1858, 294, 341, 6211, 767, 309, 307, 1858, 570, 264], "temperature": 0.0, "avg_logprob": -0.27609028418858844, "compression_ratio": 1.726027397260274, "no_speech_prob": 8.800311661616433e-06}, {"id": 853, "seek": 352448, "start": 3538.08, "end": 3540.96, "text": " The the test set seems to be a random sample", "tokens": [440, 264, 1500, 992, 2544, 281, 312, 257, 4974, 6889], "temperature": 0.0, "avg_logprob": -0.27609028418858844, "compression_ratio": 1.726027397260274, "no_speech_prob": 8.800311661616433e-06}, {"id": 854, "seek": 352448, "start": 3541.6, "end": 3544.0, "text": " But most of the time it's not actually I would say", "tokens": [583, 881, 295, 264, 565, 309, 311, 406, 767, 286, 576, 584], "temperature": 0.0, "avg_logprob": -0.27609028418858844, "compression_ratio": 1.726027397260274, "no_speech_prob": 8.800311661616433e-06}, {"id": 855, "seek": 352448, "start": 3544.96, "end": 3547.36, "text": " And then how quickly can you iterate?", "tokens": [400, 550, 577, 2661, 393, 291, 44497, 30], "temperature": 0.0, "avg_logprob": -0.27609028418858844, "compression_ratio": 1.726027397260274, "no_speech_prob": 8.800311661616433e-06}, {"id": 856, "seek": 352448, "start": 3548.2400000000002, "end": 3550.2400000000002, "text": " How quickly can you try things?", "tokens": [1012, 2661, 393, 291, 853, 721, 30], "temperature": 0.0, "avg_logprob": -0.27609028418858844, "compression_ratio": 1.726027397260274, "no_speech_prob": 8.800311661616433e-06}, {"id": 857, "seek": 355024, "start": 3550.24, "end": 3554.64, "text": " How quickly can you iterate how quickly can you try things and find out what worked?", "tokens": [1012, 2661, 393, 291, 44497, 577, 2661, 393, 291, 853, 721, 293, 915, 484, 437, 2732, 30], "temperature": 0.0, "avg_logprob": -0.26186364561646847, "compression_ratio": 1.5789473684210527, "no_speech_prob": 5.014235739508877e-06}, {"id": 858, "seek": 355024, "start": 3554.9599999999996, "end": 3558.24, "text": " So obviously you need a good validation set. Otherwise, it's impossible to iterate", "tokens": [407, 2745, 291, 643, 257, 665, 24071, 992, 13, 10328, 11, 309, 311, 6243, 281, 44497], "temperature": 0.0, "avg_logprob": -0.26186364561646847, "compression_ratio": 1.5789473684210527, "no_speech_prob": 5.014235739508877e-06}, {"id": 859, "seek": 355024, "start": 3559.8399999999997, "end": 3564.64, "text": " And so quickly iterating means not saying what is the biggest?", "tokens": [400, 370, 2661, 17138, 990, 1355, 406, 1566, 437, 307, 264, 3880, 30], "temperature": 0.0, "avg_logprob": -0.26186364561646847, "compression_ratio": 1.5789473684210527, "no_speech_prob": 5.014235739508877e-06}, {"id": 860, "seek": 355024, "start": 3566.24, "end": 3572.72, "text": " You know open AI takes four months on 100 tpus model that I can train", "tokens": [509, 458, 1269, 7318, 2516, 1451, 2493, 322, 2319, 256, 31624, 2316, 300, 286, 393, 3847], "temperature": 0.0, "avg_logprob": -0.26186364561646847, "compression_ratio": 1.5789473684210527, "no_speech_prob": 5.014235739508877e-06}, {"id": 861, "seek": 355024, "start": 3573.7599999999998, "end": 3576.72, "text": " It's what can I do that's going to train in a minute or so?", "tokens": [467, 311, 437, 393, 286, 360, 300, 311, 516, 281, 3847, 294, 257, 3456, 420, 370, 30], "temperature": 0.0, "avg_logprob": -0.26186364561646847, "compression_ratio": 1.5789473684210527, "no_speech_prob": 5.014235739508877e-06}, {"id": 862, "seek": 357672, "start": 3576.72, "end": 3581.4399999999996, "text": " And will quickly give me a sense of like well I could try this I could try that what things going to work", "tokens": [400, 486, 2661, 976, 385, 257, 2020, 295, 411, 731, 286, 727, 853, 341, 286, 727, 853, 300, 437, 721, 516, 281, 589], "temperature": 0.0, "avg_logprob": -0.3049527187736667, "compression_ratio": 1.6443514644351465, "no_speech_prob": 3.3402779990865383e-06}, {"id": 863, "seek": 357672, "start": 3582.08, "end": 3584.08, "text": " and then try", "tokens": [293, 550, 853], "temperature": 0.0, "avg_logprob": -0.3049527187736667, "compression_ratio": 1.6443514644351465, "no_speech_prob": 3.3402779990865383e-06}, {"id": 864, "seek": 357672, "start": 3584.08, "end": 3586.08, "text": " You know 80 things", "tokens": [509, 458, 4688, 721], "temperature": 0.0, "avg_logprob": -0.3049527187736667, "compression_ratio": 1.6443514644351465, "no_speech_prob": 3.3402779990865383e-06}, {"id": 865, "seek": 357672, "start": 3586.56, "end": 3590.48, "text": " It also doesn't mean that saying like oh, I heard this is amazing you", "tokens": [467, 611, 1177, 380, 914, 300, 1566, 411, 1954, 11, 286, 2198, 341, 307, 2243, 291], "temperature": 0.0, "avg_logprob": -0.3049527187736667, "compression_ratio": 1.6443514644351465, "no_speech_prob": 3.3402779990865383e-06}, {"id": 866, "seek": 357672, "start": 3591.68, "end": 3596.24, "text": " Bayesian hyper parameter tuning approach. I'm going to spend three months implementing that", "tokens": [7840, 42434, 9848, 13075, 15164, 3109, 13, 286, 478, 516, 281, 3496, 1045, 2493, 18114, 300], "temperature": 0.0, "avg_logprob": -0.3049527187736667, "compression_ratio": 1.6443514644351465, "no_speech_prob": 3.3402779990865383e-06}, {"id": 867, "seek": 357672, "start": 3596.7999999999997, "end": 3599.12, "text": " Because that's going to like give you one thing", "tokens": [1436, 300, 311, 516, 281, 411, 976, 291, 472, 551], "temperature": 0.0, "avg_logprob": -0.3049527187736667, "compression_ratio": 1.6443514644351465, "no_speech_prob": 3.3402779990865383e-06}, {"id": 868, "seek": 357672, "start": 3599.8399999999997, "end": 3601.8399999999997, "text": " But actually do well", "tokens": [583, 767, 360, 731], "temperature": 0.0, "avg_logprob": -0.3049527187736667, "compression_ratio": 1.6443514644351465, "no_speech_prob": 3.3402779990865383e-06}, {"id": 869, "seek": 357672, "start": 3602.08, "end": 3604.08, "text": " in in these competitions", "tokens": [294, 294, 613, 26185], "temperature": 0.0, "avg_logprob": -0.3049527187736667, "compression_ratio": 1.6443514644351465, "no_speech_prob": 3.3402779990865383e-06}, {"id": 870, "seek": 360408, "start": 3604.08, "end": 3607.2799999999997, "text": " Or in machine learning in general you actually have to do everything", "tokens": [1610, 294, 3479, 2539, 294, 2674, 291, 767, 362, 281, 360, 1203], "temperature": 0.0, "avg_logprob": -0.3360019235049977, "compression_ratio": 1.6849315068493151, "no_speech_prob": 1.147758484876249e-05}, {"id": 871, "seek": 360408, "start": 3607.92, "end": 3609.92, "text": " reasonably well", "tokens": [23551, 731], "temperature": 0.0, "avg_logprob": -0.3360019235049977, "compression_ratio": 1.6849315068493151, "no_speech_prob": 1.147758484876249e-05}, {"id": 872, "seek": 360408, "start": 3609.92, "end": 3611.92, "text": " And doing just one thing really well", "tokens": [400, 884, 445, 472, 551, 534, 731], "temperature": 0.0, "avg_logprob": -0.3360019235049977, "compression_ratio": 1.6849315068493151, "no_speech_prob": 1.147758484876249e-05}, {"id": 873, "seek": 360408, "start": 3612.24, "end": 3614.24, "text": " Will still put you somewhere about last place", "tokens": [3099, 920, 829, 291, 4079, 466, 1036, 1081], "temperature": 0.0, "avg_logprob": -0.3360019235049977, "compression_ratio": 1.6849315068493151, "no_speech_prob": 1.147758484876249e-05}, {"id": 874, "seek": 360408, "start": 3615.36, "end": 3618.3199999999997, "text": " I actually saw that a couple of years ago ozzy guy who's", "tokens": [286, 767, 1866, 300, 257, 1916, 295, 924, 2057, 277, 89, 1229, 2146, 567, 311], "temperature": 0.0, "avg_logprob": -0.3360019235049977, "compression_ratio": 1.6849315068493151, "no_speech_prob": 1.147758484876249e-05}, {"id": 875, "seek": 360408, "start": 3619.2799999999997, "end": 3622.0, "text": " Very very distinguished machine learning", "tokens": [4372, 588, 21702, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.3360019235049977, "compression_ratio": 1.6849315068493151, "no_speech_prob": 1.147758484876249e-05}, {"id": 876, "seek": 360408, "start": 3622.64, "end": 3624.64, "text": " practitioner", "tokens": [32125], "temperature": 0.0, "avg_logprob": -0.3360019235049977, "compression_ratio": 1.6849315068493151, "no_speech_prob": 1.147758484876249e-05}, {"id": 877, "seek": 360408, "start": 3626.24, "end": 3630.7999999999997, "text": " Actually put together a team entered a caggle competition and literally came in last place", "tokens": [5135, 829, 1214, 257, 1469, 9065, 257, 269, 559, 22631, 6211, 293, 3736, 1361, 294, 1036, 1081], "temperature": 0.0, "avg_logprob": -0.3360019235049977, "compression_ratio": 1.6849315068493151, "no_speech_prob": 1.147758484876249e-05}, {"id": 878, "seek": 363080, "start": 3630.8, "end": 3635.28, "text": " Because they spent the entire three months trying to build this amazing new", "tokens": [1436, 436, 4418, 264, 2302, 1045, 2493, 1382, 281, 1322, 341, 2243, 777], "temperature": 0.0, "avg_logprob": -0.2684539031982422, "compression_ratio": 1.5159574468085106, "no_speech_prob": 1.1842274034279399e-05}, {"id": 879, "seek": 363080, "start": 3636.32, "end": 3637.6800000000003, "text": " fancy", "tokens": [10247], "temperature": 0.0, "avg_logprob": -0.2684539031982422, "compression_ratio": 1.5159574468085106, "no_speech_prob": 1.1842274034279399e-05}, {"id": 880, "seek": 363080, "start": 3637.6800000000003, "end": 3639.6800000000003, "text": " thing and", "tokens": [551, 293], "temperature": 0.0, "avg_logprob": -0.2684539031982422, "compression_ratio": 1.5159574468085106, "no_speech_prob": 1.1842274034279399e-05}, {"id": 881, "seek": 363080, "start": 3639.76, "end": 3645.52, "text": " Never actually never actually iterated if you iterate I guarantee you won't be in last place", "tokens": [7344, 767, 1128, 767, 17138, 770, 498, 291, 44497, 286, 10815, 291, 1582, 380, 312, 294, 1036, 1081], "temperature": 0.0, "avg_logprob": -0.2684539031982422, "compression_ratio": 1.5159574468085106, "no_speech_prob": 1.1842274034279399e-05}, {"id": 882, "seek": 363080, "start": 3649.36, "end": 3650.88, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.2684539031982422, "compression_ratio": 1.5159574468085106, "no_speech_prob": 1.1842274034279399e-05}, {"id": 883, "seek": 363080, "start": 3650.88, "end": 3655.84, "text": " Here's how we can grab our data with fast caggle and it gives us tells us what path it's in", "tokens": [1692, 311, 577, 321, 393, 4444, 527, 1412, 365, 2370, 269, 559, 22631, 293, 309, 2709, 505, 5112, 505, 437, 3100, 309, 311, 294], "temperature": 0.0, "avg_logprob": -0.2684539031982422, "compression_ratio": 1.5159574468085106, "no_speech_prob": 1.1842274034279399e-05}, {"id": 884, "seek": 365584, "start": 3655.84, "end": 3659.1200000000003, "text": " And then I set my random seed", "tokens": [400, 550, 286, 992, 452, 4974, 8871], "temperature": 0.0, "avg_logprob": -0.3573856046122889, "compression_ratio": 1.7530364372469636, "no_speech_prob": 1.3006024346395861e-05}, {"id": 885, "seek": 365584, "start": 3659.6800000000003, "end": 3663.6800000000003, "text": " And I only do this because i'm creating a notebook to share", "tokens": [400, 286, 787, 360, 341, 570, 741, 478, 4084, 257, 21060, 281, 2073], "temperature": 0.0, "avg_logprob": -0.3573856046122889, "compression_ratio": 1.7530364372469636, "no_speech_prob": 1.3006024346395861e-05}, {"id": 886, "seek": 365584, "start": 3664.1600000000003, "end": 3668.96, "text": " You know when I share a notebook, I like to be able to say as you can see this is 0.83 blah blah blah, right?", "tokens": [509, 458, 562, 286, 2073, 257, 21060, 11, 286, 411, 281, 312, 1075, 281, 584, 382, 291, 393, 536, 341, 307, 1958, 13, 31849, 12288, 12288, 12288, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.3573856046122889, "compression_ratio": 1.7530364372469636, "no_speech_prob": 1.3006024346395861e-05}, {"id": 887, "seek": 365584, "start": 3669.28, "end": 3672.08, "text": " And know that when you see it'll be 0.83 as well", "tokens": [400, 458, 300, 562, 291, 536, 309, 603, 312, 1958, 13, 31849, 382, 731], "temperature": 0.0, "avg_logprob": -0.3573856046122889, "compression_ratio": 1.7530364372469636, "no_speech_prob": 1.3006024346395861e-05}, {"id": 888, "seek": 365584, "start": 3672.6400000000003, "end": 3675.6800000000003, "text": " But when i'm doing stuff, otherwise, I would never set a random seed", "tokens": [583, 562, 741, 478, 884, 1507, 11, 5911, 11, 286, 576, 1128, 992, 257, 4974, 8871], "temperature": 0.0, "avg_logprob": -0.3573856046122889, "compression_ratio": 1.7530364372469636, "no_speech_prob": 1.3006024346395861e-05}, {"id": 889, "seek": 365584, "start": 3675.6800000000003, "end": 3679.52, "text": " I want to be able to run things multiple times and see how much it's going to run", "tokens": [286, 528, 281, 312, 1075, 281, 1190, 721, 3866, 1413, 293, 536, 577, 709, 309, 311, 516, 281, 1190], "temperature": 0.0, "avg_logprob": -0.3573856046122889, "compression_ratio": 1.7530364372469636, "no_speech_prob": 1.3006024346395861e-05}, {"id": 890, "seek": 365584, "start": 3680.1600000000003, "end": 3682.1600000000003, "text": " So I'm going to set a random seed", "tokens": [407, 286, 478, 516, 281, 992, 257, 4974, 8871], "temperature": 0.0, "avg_logprob": -0.3573856046122889, "compression_ratio": 1.7530364372469636, "no_speech_prob": 1.3006024346395861e-05}, {"id": 891, "seek": 368216, "start": 3682.16, "end": 3686.72, "text": " I want to be able to run things multiple times and see how much it changes each time", "tokens": [286, 528, 281, 312, 1075, 281, 1190, 721, 3866, 1413, 293, 536, 577, 709, 309, 2962, 1184, 565], "temperature": 0.0, "avg_logprob": -0.09258098785693829, "compression_ratio": 1.708502024291498, "no_speech_prob": 3.0239289117162116e-05}, {"id": 892, "seek": 368216, "start": 3687.12, "end": 3689.12, "text": " because that'll give me a sense of like", "tokens": [570, 300, 603, 976, 385, 257, 2020, 295, 411], "temperature": 0.0, "avg_logprob": -0.09258098785693829, "compression_ratio": 1.708502024291498, "no_speech_prob": 3.0239289117162116e-05}, {"id": 893, "seek": 368216, "start": 3690.0, "end": 3695.2, "text": " The modifications i'm making changing it because they're improving it making it worse or is it just random variation?", "tokens": [440, 26881, 741, 478, 1455, 4473, 309, 570, 436, 434, 11470, 309, 1455, 309, 5324, 420, 307, 309, 445, 4974, 12990, 30], "temperature": 0.0, "avg_logprob": -0.09258098785693829, "compression_ratio": 1.708502024291498, "no_speech_prob": 3.0239289117162116e-05}, {"id": 894, "seek": 368216, "start": 3695.2799999999997, "end": 3698.08, "text": " So if you if you always set a random seed", "tokens": [407, 498, 291, 498, 291, 1009, 992, 257, 4974, 8871], "temperature": 0.0, "avg_logprob": -0.09258098785693829, "compression_ratio": 1.708502024291498, "no_speech_prob": 3.0239289117162116e-05}, {"id": 895, "seek": 368216, "start": 3698.7999999999997, "end": 3704.56, "text": " That's a bad idea because you won't be able to see the random variation. So this is just here for presenting a notebook", "tokens": [663, 311, 257, 1578, 1558, 570, 291, 1582, 380, 312, 1075, 281, 536, 264, 4974, 12990, 13, 407, 341, 307, 445, 510, 337, 15578, 257, 21060], "temperature": 0.0, "avg_logprob": -0.09258098785693829, "compression_ratio": 1.708502024291498, "no_speech_prob": 3.0239289117162116e-05}, {"id": 896, "seek": 368216, "start": 3706.64, "end": 3708.64, "text": " Okay, so the data", "tokens": [1033, 11, 370, 264, 1412], "temperature": 0.0, "avg_logprob": -0.09258098785693829, "compression_ratio": 1.708502024291498, "no_speech_prob": 3.0239289117162116e-05}, {"id": 897, "seek": 370864, "start": 3708.64, "end": 3713.3599999999997, "text": " They've given us as usual. They've got a sample submission. They've got some test set images", "tokens": [814, 600, 2212, 505, 382, 7713, 13, 814, 600, 658, 257, 6889, 23689, 13, 814, 600, 658, 512, 1500, 992, 5267], "temperature": 0.0, "avg_logprob": -0.09964500087322575, "compression_ratio": 1.7649769585253456, "no_speech_prob": 6.339032552205026e-06}, {"id": 898, "seek": 370864, "start": 3714.64, "end": 3716.64, "text": " They've got some training set images", "tokens": [814, 600, 658, 512, 3097, 992, 5267], "temperature": 0.0, "avg_logprob": -0.09964500087322575, "compression_ratio": 1.7649769585253456, "no_speech_prob": 6.339032552205026e-06}, {"id": 899, "seek": 370864, "start": 3717.04, "end": 3719.04, "text": " a csv file about the training set", "tokens": [257, 28277, 85, 3991, 466, 264, 3097, 992], "temperature": 0.0, "avg_logprob": -0.09964500087322575, "compression_ratio": 1.7649769585253456, "no_speech_prob": 6.339032552205026e-06}, {"id": 900, "seek": 370864, "start": 3720.4, "end": 3722.8799999999997, "text": " Um, and then these other two you can ignore because I created them", "tokens": [3301, 11, 293, 550, 613, 661, 732, 291, 393, 11200, 570, 286, 2942, 552], "temperature": 0.0, "avg_logprob": -0.09964500087322575, "compression_ratio": 1.7649769585253456, "no_speech_prob": 6.339032552205026e-06}, {"id": 901, "seek": 370864, "start": 3724.24, "end": 3726.24, "text": " So let's grab a path", "tokens": [407, 718, 311, 4444, 257, 3100], "temperature": 0.0, "avg_logprob": -0.09964500087322575, "compression_ratio": 1.7649769585253456, "no_speech_prob": 6.339032552205026e-06}, {"id": 902, "seek": 370864, "start": 3727.04, "end": 3729.44, "text": " To train images and so do you remember?", "tokens": [1407, 3847, 5267, 293, 370, 360, 291, 1604, 30], "temperature": 0.0, "avg_logprob": -0.09964500087322575, "compression_ratio": 1.7649769585253456, "no_speech_prob": 6.339032552205026e-06}, {"id": 903, "seek": 370864, "start": 3730.64, "end": 3736.9, "text": " Get image files so that gets us a list of the file names of all the images here recursively", "tokens": [3240, 3256, 7098, 370, 300, 2170, 505, 257, 1329, 295, 264, 3991, 5288, 295, 439, 264, 5267, 510, 20560, 3413], "temperature": 0.0, "avg_logprob": -0.09964500087322575, "compression_ratio": 1.7649769585253456, "no_speech_prob": 6.339032552205026e-06}, {"id": 904, "seek": 373690, "start": 3736.9, "end": 3738.9, "text": " So we could just grab the first one", "tokens": [407, 321, 727, 445, 4444, 264, 700, 472], "temperature": 0.0, "avg_logprob": -0.26776885986328125, "compression_ratio": 1.5377777777777777, "no_speech_prob": 4.157258445047773e-06}, {"id": 905, "seek": 373690, "start": 3739.7000000000003, "end": 3741.7000000000003, "text": " And take a look so it's 480", "tokens": [400, 747, 257, 574, 370, 309, 311, 1017, 4702], "temperature": 0.0, "avg_logprob": -0.26776885986328125, "compression_ratio": 1.5377777777777777, "no_speech_prob": 4.157258445047773e-06}, {"id": 906, "seek": 373690, "start": 3742.26, "end": 3744.26, "text": " by 640", "tokens": [538, 1386, 5254], "temperature": 0.0, "avg_logprob": -0.26776885986328125, "compression_ratio": 1.5377777777777777, "no_speech_prob": 4.157258445047773e-06}, {"id": 907, "seek": 373690, "start": 3744.26, "end": 3748.98, "text": " Now we've got to be careful. Um, this is a pillow image python imaging library image", "tokens": [823, 321, 600, 658, 281, 312, 5026, 13, 3301, 11, 341, 307, 257, 18581, 3256, 38797, 25036, 6405, 3256], "temperature": 0.0, "avg_logprob": -0.26776885986328125, "compression_ratio": 1.5377777777777777, "no_speech_prob": 4.157258445047773e-06}, {"id": 908, "seek": 373690, "start": 3749.78, "end": 3754.26, "text": " Um in the imaging world, they generally say columns by rows", "tokens": [3301, 294, 264, 25036, 1002, 11, 436, 5101, 584, 13766, 538, 13241], "temperature": 0.0, "avg_logprob": -0.26776885986328125, "compression_ratio": 1.5377777777777777, "no_speech_prob": 4.157258445047773e-06}, {"id": 909, "seek": 373690, "start": 3755.38, "end": 3759.78, "text": " In the array slash tensor world, we always say rows by columns", "tokens": [682, 264, 10225, 17330, 40863, 1002, 11, 321, 1009, 584, 13241, 538, 13766], "temperature": 0.0, "avg_logprob": -0.26776885986328125, "compression_ratio": 1.5377777777777777, "no_speech_prob": 4.157258445047773e-06}, {"id": 910, "seek": 373690, "start": 3760.26, "end": 3764.1, "text": " So if you ask pytorch what the size of this is it'll say 640 by 480", "tokens": [407, 498, 291, 1029, 25878, 284, 339, 437, 264, 2744, 295, 341, 307, 309, 603, 584, 1386, 5254, 538, 1017, 4702], "temperature": 0.0, "avg_logprob": -0.26776885986328125, "compression_ratio": 1.5377777777777777, "no_speech_prob": 4.157258445047773e-06}, {"id": 911, "seek": 376410, "start": 3764.1, "end": 3768.8199999999997, "text": " And I guarantee at some point this is going to bite you so try to recognize it now", "tokens": [400, 286, 10815, 412, 512, 935, 341, 307, 516, 281, 7988, 291, 370, 853, 281, 5521, 309, 586], "temperature": 0.0, "avg_logprob": -0.29045497560964045, "compression_ratio": 1.698744769874477, "no_speech_prob": 2.1111525711603463e-05}, {"id": 912, "seek": 376410, "start": 3769.94, "end": 3773.7, "text": " Okay, so they're kind of taller than they are at least this one is taller than it is wide", "tokens": [1033, 11, 370, 436, 434, 733, 295, 22406, 813, 436, 366, 412, 1935, 341, 472, 307, 22406, 813, 309, 307, 4874], "temperature": 0.0, "avg_logprob": -0.29045497560964045, "compression_ratio": 1.698744769874477, "no_speech_prob": 2.1111525711603463e-05}, {"id": 913, "seek": 376410, "start": 3776.42, "end": 3777.7, "text": " Um", "tokens": [3301], "temperature": 0.0, "avg_logprob": -0.29045497560964045, "compression_ratio": 1.698744769874477, "no_speech_prob": 2.1111525711603463e-05}, {"id": 914, "seek": 376410, "start": 3777.7, "end": 3782.42, "text": " So I would actually actually know are they all this size because it's really helpful if they are all the same size or at least similar", "tokens": [407, 286, 576, 767, 767, 458, 366, 436, 439, 341, 2744, 570, 309, 311, 534, 4961, 498, 436, 366, 439, 264, 912, 2744, 420, 412, 1935, 2531], "temperature": 0.0, "avg_logprob": -0.29045497560964045, "compression_ratio": 1.698744769874477, "no_speech_prob": 2.1111525711603463e-05}, {"id": 915, "seek": 376410, "start": 3784.74, "end": 3786.18, "text": " Um", "tokens": [3301], "temperature": 0.0, "avg_logprob": -0.29045497560964045, "compression_ratio": 1.698744769874477, "no_speech_prob": 2.1111525711603463e-05}, {"id": 916, "seek": 376410, "start": 3786.18, "end": 3791.22, "text": " Believe it or not the amount of time it takes to decode a jpeg is actually quite significant", "tokens": [21486, 309, 420, 406, 264, 2372, 295, 565, 309, 2516, 281, 979, 1429, 257, 361, 494, 70, 307, 767, 1596, 4776], "temperature": 0.0, "avg_logprob": -0.29045497560964045, "compression_ratio": 1.698744769874477, "no_speech_prob": 2.1111525711603463e-05}, {"id": 917, "seek": 379122, "start": 3791.22, "end": 3796.3399999999997, "text": " Um, and so figuring out what size these things are is actually going to be pretty slow", "tokens": [3301, 11, 293, 370, 15213, 484, 437, 2744, 613, 721, 366, 307, 767, 516, 281, 312, 1238, 2964], "temperature": 0.0, "avg_logprob": -0.2101023734152854, "compression_ratio": 1.8221343873517786, "no_speech_prob": 1.618675196368713e-05}, {"id": 918, "seek": 379122, "start": 3797.54, "end": 3798.2599999999998, "text": " Um", "tokens": [3301], "temperature": 0.0, "avg_logprob": -0.2101023734152854, "compression_ratio": 1.8221343873517786, "no_speech_prob": 1.618675196368713e-05}, {"id": 919, "seek": 379122, "start": 3798.2599999999998, "end": 3804.58, "text": " But my fast core library has a parallel sub module which can basically do anything that you can do in python", "tokens": [583, 452, 2370, 4965, 6405, 575, 257, 8952, 1422, 10088, 597, 393, 1936, 360, 1340, 300, 291, 393, 360, 294, 38797], "temperature": 0.0, "avg_logprob": -0.2101023734152854, "compression_ratio": 1.8221343873517786, "no_speech_prob": 1.618675196368713e-05}, {"id": 920, "seek": 379122, "start": 3804.58, "end": 3806.02, "text": " It can do it in parallel", "tokens": [467, 393, 360, 309, 294, 8952], "temperature": 0.0, "avg_logprob": -0.2101023734152854, "compression_ratio": 1.8221343873517786, "no_speech_prob": 1.618675196368713e-05}, {"id": 921, "seek": 379122, "start": 3806.02, "end": 3809.3799999999997, "text": " So in this case we wanted to create a pillow image and get its size", "tokens": [407, 294, 341, 1389, 321, 1415, 281, 1884, 257, 18581, 3256, 293, 483, 1080, 2744], "temperature": 0.0, "avg_logprob": -0.2101023734152854, "compression_ratio": 1.8221343873517786, "no_speech_prob": 1.618675196368713e-05}, {"id": 922, "seek": 379122, "start": 3810.1, "end": 3816.02, "text": " So if we create a function that does that and pass it to parallel passing in the function and the list of files", "tokens": [407, 498, 321, 1884, 257, 2445, 300, 775, 300, 293, 1320, 309, 281, 8952, 8437, 294, 264, 2445, 293, 264, 1329, 295, 7098], "temperature": 0.0, "avg_logprob": -0.2101023734152854, "compression_ratio": 1.8221343873517786, "no_speech_prob": 1.618675196368713e-05}, {"id": 923, "seek": 379122, "start": 3816.8199999999997, "end": 3819.3799999999997, "text": " It does it in parallel and that actually runs pretty fast", "tokens": [467, 775, 309, 294, 8952, 293, 300, 767, 6676, 1238, 2370], "temperature": 0.0, "avg_logprob": -0.2101023734152854, "compression_ratio": 1.8221343873517786, "no_speech_prob": 1.618675196368713e-05}, {"id": 924, "seek": 381938, "start": 3819.38, "end": 3821.38, "text": " And so here is the answer", "tokens": [400, 370, 510, 307, 264, 1867], "temperature": 0.0, "avg_logprob": -0.2376099113656693, "compression_ratio": 1.6307692307692307, "no_speech_prob": 8.139387318806257e-06}, {"id": 925, "seek": 381938, "start": 3822.42, "end": 3824.42, "text": " I don't know how this happened", "tokens": [286, 500, 380, 458, 577, 341, 2011], "temperature": 0.0, "avg_logprob": -0.2376099113656693, "compression_ratio": 1.6307692307692307, "no_speech_prob": 8.139387318806257e-06}, {"id": 926, "seek": 381938, "start": 3824.6400000000003, "end": 3829.46, "text": " 10,403 images are indeed 480 by 640 and four of them aren't", "tokens": [1266, 11, 5254, 18, 5267, 366, 6451, 1017, 4702, 538, 1386, 5254, 293, 1451, 295, 552, 3212, 380], "temperature": 0.0, "avg_logprob": -0.2376099113656693, "compression_ratio": 1.6307692307692307, "no_speech_prob": 8.139387318806257e-06}, {"id": 927, "seek": 381938, "start": 3830.26, "end": 3835.3, "text": " So basically what this says to me is that we should pre-process them or you know at some point process them", "tokens": [407, 1936, 437, 341, 1619, 281, 385, 307, 300, 321, 820, 659, 12, 41075, 552, 420, 291, 458, 412, 512, 935, 1399, 552], "temperature": 0.0, "avg_logprob": -0.2376099113656693, "compression_ratio": 1.6307692307692307, "no_speech_prob": 8.139387318806257e-06}, {"id": 928, "seek": 381938, "start": 3835.38, "end": 3841.1400000000003, "text": " So that they're probably all 480 by 640 or all basically the kind of same size. We'll pretend they're all this size", "tokens": [407, 300, 436, 434, 1391, 439, 1017, 4702, 538, 1386, 5254, 420, 439, 1936, 264, 733, 295, 912, 2744, 13, 492, 603, 11865, 436, 434, 439, 341, 2744], "temperature": 0.0, "avg_logprob": -0.2376099113656693, "compression_ratio": 1.6307692307692307, "no_speech_prob": 8.139387318806257e-06}, {"id": 929, "seek": 381938, "start": 3842.1800000000003, "end": 3844.1800000000003, "text": " um, but we can't", "tokens": [1105, 11, 457, 321, 393, 380], "temperature": 0.0, "avg_logprob": -0.2376099113656693, "compression_ratio": 1.6307692307692307, "no_speech_prob": 8.139387318806257e-06}, {"id": 930, "seek": 381938, "start": 3844.1800000000003, "end": 3847.1400000000003, "text": " Not do some initial resizing. Otherwise, this is going to screw up", "tokens": [1726, 360, 512, 5883, 725, 3319, 13, 10328, 11, 341, 307, 516, 281, 5630, 493], "temperature": 0.0, "avg_logprob": -0.2376099113656693, "compression_ratio": 1.6307692307692307, "no_speech_prob": 8.139387318806257e-06}, {"id": 931, "seek": 384714, "start": 3847.14, "end": 3849.14, "text": " Um", "tokens": [3301], "temperature": 0.0, "avg_logprob": -0.6949112027190453, "compression_ratio": 1.7258064516129032, "no_speech_prob": 1.6186353605007753e-05}, {"id": 932, "seek": 384714, "start": 3849.7799999999997, "end": 3853.2999999999997, "text": " So like the probably the easiest way to do things the most common way to do things", "tokens": [407, 411, 264, 1391, 264, 12889, 636, 281, 360, 721, 264, 881, 2689, 636, 281, 360, 721], "temperature": 0.0, "avg_logprob": -0.6949112027190453, "compression_ratio": 1.7258064516129032, "no_speech_prob": 1.6186353605007753e-05}, {"id": 933, "seek": 384714, "start": 3854.1, "end": 3859.06, "text": " Is to either squish or crop every image to be a square", "tokens": [1119, 281, 2139, 31379, 420, 9086, 633, 3256, 281, 312, 257, 3732], "temperature": 0.0, "avg_logprob": -0.6949112027190453, "compression_ratio": 1.7258064516129032, "no_speech_prob": 1.6186353605007753e-05}, {"id": 934, "seek": 384714, "start": 3860.2599999999998, "end": 3862.2599999999998, "text": " So squishing is when you just", "tokens": [407, 2339, 3807, 307, 562, 291, 445], "temperature": 0.0, "avg_logprob": -0.6949112027190453, "compression_ratio": 1.7258064516129032, "no_speech_prob": 1.6186353605007753e-05}, {"id": 935, "seek": 384714, "start": 3863.14, "end": 3865.46, "text": " In this case squish the aspect ratio down", "tokens": [682, 341, 1389, 31379, 264, 4171, 8509, 760], "temperature": 0.0, "avg_logprob": -0.6949112027190453, "compression_ratio": 1.7258064516129032, "no_speech_prob": 1.6186353605007753e-05}, {"id": 936, "seek": 384714, "start": 3868.1, "end": 3870.1, "text": " As opposed to cropping the aspect ratio down", "tokens": [1018, 8851, 281, 4848, 3759, 264, 4171, 8509, 760], "temperature": 0.0, "avg_logprob": -0.6949112027190453, "compression_ratio": 1.7258064516129032, "no_speech_prob": 1.6186353605007753e-05}, {"id": 937, "seek": 384714, "start": 3870.98, "end": 3873.62, "text": " So we're going to do this in a little bit of a more complex way", "tokens": [407, 321, 434, 516, 281, 360, 341, 294, 257, 707, 857, 295, 257, 544, 3997, 636], "temperature": 0.0, "avg_logprob": -0.6949112027190453, "compression_ratio": 1.7258064516129032, "no_speech_prob": 1.6186353605007753e-05}, {"id": 938, "seek": 387362, "start": 3873.62, "end": 3882.1, "text": " Um as opposed to cropping randomly a section out. So if we call resize squish it will squish it down", "tokens": [3301, 382, 8851, 281, 4848, 3759, 16979, 257, 3541, 484, 13, 407, 498, 321, 818, 50069, 31379, 309, 486, 31379, 309, 760], "temperature": 0.0, "avg_logprob": -0.1140111255645752, "compression_ratio": 1.5964912280701755, "no_speech_prob": 5.09356641487102e-06}, {"id": 939, "seek": 387362, "start": 3882.8199999999997, "end": 3888.74, "text": " Um, and so this is 480 by 480 squared. So this is what it's going to do to all of the images first", "tokens": [3301, 11, 293, 370, 341, 307, 1017, 4702, 538, 1017, 4702, 8889, 13, 407, 341, 307, 437, 309, 311, 516, 281, 360, 281, 439, 295, 264, 5267, 700], "temperature": 0.0, "avg_logprob": -0.1140111255645752, "compression_ratio": 1.5964912280701755, "no_speech_prob": 5.09356641487102e-06}, {"id": 940, "seek": 387362, "start": 3889.62, "end": 3891.62, "text": " on the cpu", "tokens": [322, 264, 269, 34859], "temperature": 0.0, "avg_logprob": -0.1140111255645752, "compression_ratio": 1.5964912280701755, "no_speech_prob": 5.09356641487102e-06}, {"id": 941, "seek": 387362, "start": 3891.94, "end": 3899.06, "text": " That allows them to be all batched together into a single mini batch. Everything in a mini batch has to be the same shape", "tokens": [663, 4045, 552, 281, 312, 439, 15245, 292, 1214, 666, 257, 2167, 8382, 15245, 13, 5471, 294, 257, 8382, 15245, 575, 281, 312, 264, 912, 3909], "temperature": 0.0, "avg_logprob": -0.1140111255645752, "compression_ratio": 1.5964912280701755, "no_speech_prob": 5.09356641487102e-06}, {"id": 942, "seek": 387362, "start": 3899.7, "end": 3902.02, "text": " Otherwise the gpu won't like it", "tokens": [10328, 264, 290, 34859, 1582, 380, 411, 309], "temperature": 0.0, "avg_logprob": -0.1140111255645752, "compression_ratio": 1.5964912280701755, "no_speech_prob": 5.09356641487102e-06}, {"id": 943, "seek": 390202, "start": 3902.02, "end": 3905.4, "text": " And then that mini batch is put through data augmentation", "tokens": [400, 550, 300, 8382, 15245, 307, 829, 807, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.29484851185868427, "compression_ratio": 1.5622119815668203, "no_speech_prob": 3.4465354019630468e-06}, {"id": 944, "seek": 390202, "start": 3906.74, "end": 3914.74, "text": " And it will grab a random subset of the image and make it a 128 by 128 pixel", "tokens": [400, 309, 486, 4444, 257, 4974, 25993, 295, 264, 3256, 293, 652, 309, 257, 29810, 538, 29810, 19261], "temperature": 0.0, "avg_logprob": -0.29484851185868427, "compression_ratio": 1.5622119815668203, "no_speech_prob": 3.4465354019630468e-06}, {"id": 945, "seek": 390202, "start": 3916.02, "end": 3918.74, "text": " And here's what that looks like. Here's our data", "tokens": [400, 510, 311, 437, 300, 1542, 411, 13, 1692, 311, 527, 1412], "temperature": 0.0, "avg_logprob": -0.29484851185868427, "compression_ratio": 1.5622119815668203, "no_speech_prob": 3.4465354019630468e-06}, {"id": 946, "seek": 390202, "start": 3920.2599999999998, "end": 3925.3, "text": " So show batch works for pretty much everything not just in the fast ai library", "tokens": [407, 855, 15245, 1985, 337, 1238, 709, 1203, 406, 445, 294, 264, 2370, 9783, 6405], "temperature": 0.0, "avg_logprob": -0.29484851185868427, "compression_ratio": 1.5622119815668203, "no_speech_prob": 3.4465354019630468e-06}, {"id": 947, "seek": 390202, "start": 3925.3, "end": 3929.94, "text": " But even for things like fast audio which are kind of community based things", "tokens": [583, 754, 337, 721, 411, 2370, 6278, 597, 366, 733, 295, 1768, 2361, 721], "temperature": 0.0, "avg_logprob": -0.29484851185868427, "compression_ratio": 1.5622119815668203, "no_speech_prob": 3.4465354019630468e-06}, {"id": 948, "seek": 392994, "start": 3929.94, "end": 3937.54, "text": " You should be able to use show batch on anything and and see or hear or whatever what your data looks like", "tokens": [509, 820, 312, 1075, 281, 764, 855, 15245, 322, 1340, 293, 293, 536, 420, 1568, 420, 2035, 437, 428, 1412, 1542, 411], "temperature": 0.0, "avg_logprob": -0.1830964827201736, "compression_ratio": 1.5625, "no_speech_prob": 1.2606279597093817e-05}, {"id": 949, "seek": 392994, "start": 3939.46, "end": 3946.1, "text": " I don't know anything about rice disease, but apparently these are various rice diseases and this is what they look like", "tokens": [286, 500, 380, 458, 1340, 466, 5090, 4752, 11, 457, 7970, 613, 366, 3683, 5090, 11044, 293, 341, 307, 437, 436, 574, 411], "temperature": 0.0, "avg_logprob": -0.1830964827201736, "compression_ratio": 1.5625, "no_speech_prob": 1.2606279597093817e-05}, {"id": 950, "seek": 392994, "start": 3951.46, "end": 3953.46, "text": " So, um, I", "tokens": [407, 11, 1105, 11, 286], "temperature": 0.0, "avg_logprob": -0.1830964827201736, "compression_ratio": 1.5625, "no_speech_prob": 1.2606279597093817e-05}, {"id": 951, "seek": 392994, "start": 3953.7000000000003, "end": 3958.34, "text": " I jump into creating models much more quickly than most people", "tokens": [286, 3012, 666, 4084, 5245, 709, 544, 2661, 813, 881, 561], "temperature": 0.0, "avg_logprob": -0.1830964827201736, "compression_ratio": 1.5625, "no_speech_prob": 1.2606279597093817e-05}, {"id": 952, "seek": 395834, "start": 3958.34, "end": 3963.7000000000003, "text": " Um, because I find model, you know models are a great way to understand my data as we've seen before", "tokens": [3301, 11, 570, 286, 915, 2316, 11, 291, 458, 5245, 366, 257, 869, 636, 281, 1223, 452, 1412, 382, 321, 600, 1612, 949], "temperature": 0.0, "avg_logprob": -0.14554730705592944, "compression_ratio": 1.6205128205128205, "no_speech_prob": 7.296176136151189e-06}, {"id": 953, "seek": 395834, "start": 3964.42, "end": 3966.42, "text": " So I basically build a model as soon as I can", "tokens": [407, 286, 1936, 1322, 257, 2316, 382, 2321, 382, 286, 393], "temperature": 0.0, "avg_logprob": -0.14554730705592944, "compression_ratio": 1.6205128205128205, "no_speech_prob": 7.296176136151189e-06}, {"id": 954, "seek": 395834, "start": 3967.78, "end": 3969.2200000000003, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.14554730705592944, "compression_ratio": 1.6205128205128205, "no_speech_prob": 7.296176136151189e-06}, {"id": 955, "seek": 395834, "start": 3969.2200000000003, "end": 3971.06, "text": " and I want to", "tokens": [293, 286, 528, 281], "temperature": 0.0, "avg_logprob": -0.14554730705592944, "compression_ratio": 1.6205128205128205, "no_speech_prob": 7.296176136151189e-06}, {"id": 956, "seek": 395834, "start": 3971.06, "end": 3972.82, "text": " Create a model", "tokens": [20248, 257, 2316], "temperature": 0.0, "avg_logprob": -0.14554730705592944, "compression_ratio": 1.6205128205128205, "no_speech_prob": 7.296176136151189e-06}, {"id": 957, "seek": 395834, "start": 3972.82, "end": 3974.82, "text": " That's going to let me iterate quickly", "tokens": [663, 311, 516, 281, 718, 385, 44497, 2661], "temperature": 0.0, "avg_logprob": -0.14554730705592944, "compression_ratio": 1.6205128205128205, "no_speech_prob": 7.296176136151189e-06}, {"id": 958, "seek": 395834, "start": 3974.98, "end": 3977.94, "text": " So that means that i'm going to need a model that can train quickly", "tokens": [407, 300, 1355, 300, 741, 478, 516, 281, 643, 257, 2316, 300, 393, 3847, 2661], "temperature": 0.0, "avg_logprob": -0.14554730705592944, "compression_ratio": 1.6205128205128205, "no_speech_prob": 7.296176136151189e-06}, {"id": 959, "seek": 395834, "start": 3978.82, "end": 3980.82, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.14554730705592944, "compression_ratio": 1.6205128205128205, "no_speech_prob": 7.296176136151189e-06}, {"id": 960, "seek": 395834, "start": 3981.6200000000003, "end": 3983.6200000000003, "text": " Thomas capel and I recently", "tokens": [8500, 1410, 338, 293, 286, 3938], "temperature": 0.0, "avg_logprob": -0.14554730705592944, "compression_ratio": 1.6205128205128205, "no_speech_prob": 7.296176136151189e-06}, {"id": 961, "seek": 398362, "start": 3983.62, "end": 3987.94, "text": " Um did this big project for best vision models for fine tuning", "tokens": [3301, 630, 341, 955, 1716, 337, 1151, 5201, 5245, 337, 2489, 15164], "temperature": 0.0, "avg_logprob": -0.4762095022892606, "compression_ratio": 1.5898876404494382, "no_speech_prob": 2.9478733267751522e-06}, {"id": 962, "seek": 398362, "start": 3989.14, "end": 3992.3399999999997, "text": " Where we looked at nearly a hundred different", "tokens": [2305, 321, 2956, 412, 6217, 257, 3262, 819], "temperature": 0.0, "avg_logprob": -0.4762095022892606, "compression_ratio": 1.5898876404494382, "no_speech_prob": 2.9478733267751522e-06}, {"id": 963, "seek": 398362, "start": 3993.38, "end": 3995.38, "text": " architectures", "tokens": [6331, 1303], "temperature": 0.0, "avg_logprob": -0.4762095022892606, "compression_ratio": 1.5898876404494382, "no_speech_prob": 2.9478733267751522e-06}, {"id": 964, "seek": 398362, "start": 3995.38, "end": 3998.66, "text": " from from ross whiteman's tim library", "tokens": [490, 490, 367, 772, 47548, 15023, 311, 524, 6405], "temperature": 0.0, "avg_logprob": -0.4762095022892606, "compression_ratio": 1.5898876404494382, "no_speech_prob": 2.9478733267751522e-06}, {"id": 965, "seek": 398362, "start": 3999.46, "end": 4001.46, "text": " Py torch image model library", "tokens": [9953, 27822, 3256, 2316, 6405], "temperature": 0.0, "avg_logprob": -0.4762095022892606, "compression_ratio": 1.5898876404494382, "no_speech_prob": 2.9478733267751522e-06}, {"id": 966, "seek": 398362, "start": 4002.5, "end": 4004.5, "text": " and looked at", "tokens": [293, 2956, 412], "temperature": 0.0, "avg_logprob": -0.4762095022892606, "compression_ratio": 1.5898876404494382, "no_speech_prob": 2.9478733267751522e-06}, {"id": 967, "seek": 398362, "start": 4005.7799999999997, "end": 4010.8199999999997, "text": " Which ones could we fine tune which ones had the best transfer learning results", "tokens": [3013, 2306, 727, 321, 2489, 10864, 597, 2306, 632, 264, 1151, 5003, 2539, 3542], "temperature": 0.0, "avg_logprob": -0.4762095022892606, "compression_ratio": 1.5898876404494382, "no_speech_prob": 2.9478733267751522e-06}, {"id": 968, "seek": 401082, "start": 4010.82, "end": 4016.5800000000004, "text": " And we tried two different data sets very different data sets. Um, one is the pets data set that we've seen before", "tokens": [400, 321, 3031, 732, 819, 1412, 6352, 588, 819, 1412, 6352, 13, 3301, 11, 472, 307, 264, 19897, 1412, 992, 300, 321, 600, 1612, 949], "temperature": 0.0, "avg_logprob": -0.271747406492842, "compression_ratio": 1.841121495327103, "no_speech_prob": 7.645960067748092e-06}, {"id": 969, "seek": 401082, "start": 4017.6200000000003, "end": 4022.5800000000004, "text": " So trying to predict what breed of pet is from 37 different breeds", "tokens": [407, 1382, 281, 6069, 437, 18971, 295, 3817, 307, 490, 13435, 819, 41609], "temperature": 0.0, "avg_logprob": -0.271747406492842, "compression_ratio": 1.841121495327103, "no_speech_prob": 7.645960067748092e-06}, {"id": 970, "seek": 401082, "start": 4023.54, "end": 4025.54, "text": " and the other was a", "tokens": [293, 264, 661, 390, 257], "temperature": 0.0, "avg_logprob": -0.271747406492842, "compression_ratio": 1.841121495327103, "no_speech_prob": 7.645960067748092e-06}, {"id": 971, "seek": 401082, "start": 4026.02, "end": 4028.34, "text": " satellite imagery data set called planet", "tokens": [16016, 24340, 1412, 992, 1219, 5054], "temperature": 0.0, "avg_logprob": -0.271747406492842, "compression_ratio": 1.841121495327103, "no_speech_prob": 7.645960067748092e-06}, {"id": 972, "seek": 401082, "start": 4028.82, "end": 4033.2200000000003, "text": " So very very different data sets in terms of what they contain and also very different sizes", "tokens": [407, 588, 588, 819, 1412, 6352, 294, 2115, 295, 437, 436, 5304, 293, 611, 588, 819, 11602], "temperature": 0.0, "avg_logprob": -0.271747406492842, "compression_ratio": 1.841121495327103, "no_speech_prob": 7.645960067748092e-06}, {"id": 973, "seek": 401082, "start": 4034.26, "end": 4036.5800000000004, "text": " The planet one's a lot smaller the pets one's a lot bigger", "tokens": [440, 5054, 472, 311, 257, 688, 4356, 264, 19897, 472, 311, 257, 688, 3801], "temperature": 0.0, "avg_logprob": -0.271747406492842, "compression_ratio": 1.841121495327103, "no_speech_prob": 7.645960067748092e-06}, {"id": 974, "seek": 403658, "start": 4036.58, "end": 4040.74, "text": " Um, and so the main things we measured were how much memory did it use?", "tokens": [3301, 11, 293, 370, 264, 2135, 721, 321, 12690, 645, 577, 709, 4675, 630, 309, 764, 30], "temperature": 0.0, "avg_logprob": -0.26829058624977287, "compression_ratio": 1.592039800995025, "no_speech_prob": 3.0415337732847547e-06}, {"id": 975, "seek": 403658, "start": 4041.54, "end": 4044.34, "text": " How accurate was it and how long did it take to fit?", "tokens": [1012, 8559, 390, 309, 293, 577, 938, 630, 309, 747, 281, 3318, 30], "temperature": 0.0, "avg_logprob": -0.26829058624977287, "compression_ratio": 1.592039800995025, "no_speech_prob": 3.0415337732847547e-06}, {"id": 976, "seek": 403658, "start": 4044.9, "end": 4049.7799999999997, "text": " And then I created this score which can which combines the fit time and error rate together", "tokens": [400, 550, 286, 2942, 341, 6175, 597, 393, 597, 29520, 264, 3318, 565, 293, 6713, 3314, 1214], "temperature": 0.0, "avg_logprob": -0.26829058624977287, "compression_ratio": 1.592039800995025, "no_speech_prob": 3.0415337732847547e-06}, {"id": 977, "seek": 403658, "start": 4052.18, "end": 4054.5, "text": " And so this is a really useful table", "tokens": [400, 370, 341, 307, 257, 534, 4420, 3199], "temperature": 0.0, "avg_logprob": -0.26829058624977287, "compression_ratio": 1.592039800995025, "no_speech_prob": 3.0415337732847547e-06}, {"id": 978, "seek": 403658, "start": 4055.7799999999997, "end": 4057.7799999999997, "text": " for picking", "tokens": [337, 8867], "temperature": 0.0, "avg_logprob": -0.26829058624977287, "compression_ratio": 1.592039800995025, "no_speech_prob": 3.0415337732847547e-06}, {"id": 979, "seek": 403658, "start": 4058.34, "end": 4059.7, "text": " a model", "tokens": [257, 2316], "temperature": 0.0, "avg_logprob": -0.26829058624977287, "compression_ratio": 1.592039800995025, "no_speech_prob": 3.0415337732847547e-06}, {"id": 980, "seek": 403658, "start": 4059.7, "end": 4062.34, "text": " And now in this case, I want to pick something", "tokens": [400, 586, 294, 341, 1389, 11, 286, 528, 281, 1888, 746], "temperature": 0.0, "avg_logprob": -0.26829058624977287, "compression_ratio": 1.592039800995025, "no_speech_prob": 3.0415337732847547e-06}, {"id": 981, "seek": 406234, "start": 4062.34, "end": 4068.98, "text": " That's really fast and there's one clear winner on speed, which is resnet 26 d", "tokens": [663, 311, 534, 2370, 293, 456, 311, 472, 1850, 8507, 322, 3073, 11, 597, 307, 725, 7129, 7551, 274], "temperature": 0.0, "avg_logprob": -0.24266928769229504, "compression_ratio": 1.5369458128078817, "no_speech_prob": 3.5007778933504596e-06}, {"id": 982, "seek": 406234, "start": 4070.26, "end": 4075.1400000000003, "text": " And so its accuracy was six percent versus the best was like 4.1 percent", "tokens": [400, 370, 1080, 14170, 390, 2309, 3043, 5717, 264, 1151, 390, 411, 1017, 13, 16, 3043], "temperature": 0.0, "avg_logprob": -0.24266928769229504, "compression_ratio": 1.5369458128078817, "no_speech_prob": 3.5007778933504596e-06}, {"id": 983, "seek": 406234, "start": 4075.6200000000003, "end": 4080.26, "text": " So, okay, it's not amazingly accurate, but it's still pretty good and it's going to be really fast", "tokens": [407, 11, 1392, 11, 309, 311, 406, 31762, 8559, 11, 457, 309, 311, 920, 1238, 665, 293, 309, 311, 516, 281, 312, 534, 2370], "temperature": 0.0, "avg_logprob": -0.24266928769229504, "compression_ratio": 1.5369458128078817, "no_speech_prob": 3.5007778933504596e-06}, {"id": 984, "seek": 406234, "start": 4081.2200000000003, "end": 4083.2200000000003, "text": " So that's why I picked resnet", "tokens": [407, 300, 311, 983, 286, 6183, 725, 7129], "temperature": 0.0, "avg_logprob": -0.24266928769229504, "compression_ratio": 1.5369458128078817, "no_speech_prob": 3.5007778933504596e-06}, {"id": 985, "seek": 406234, "start": 4083.94, "end": 4085.6200000000003, "text": " 26 d", "tokens": [7551, 274], "temperature": 0.0, "avg_logprob": -0.24266928769229504, "compression_ratio": 1.5369458128078817, "no_speech_prob": 3.5007778933504596e-06}, {"id": 986, "seek": 406234, "start": 4085.6200000000003, "end": 4087.6200000000003, "text": " A lot of people think that", "tokens": [316, 688, 295, 561, 519, 300], "temperature": 0.0, "avg_logprob": -0.24266928769229504, "compression_ratio": 1.5369458128078817, "no_speech_prob": 3.5007778933504596e-06}, {"id": 987, "seek": 408762, "start": 4087.62, "end": 4094.42, "text": " When they do deep learning they're going to spend all of their time learning about exactly how a resnet 26 d is made and", "tokens": [1133, 436, 360, 2452, 2539, 436, 434, 516, 281, 3496, 439, 295, 641, 565, 2539, 466, 2293, 577, 257, 725, 7129, 7551, 274, 307, 1027, 293], "temperature": 0.0, "avg_logprob": -0.27187746951454567, "compression_ratio": 1.6299559471365639, "no_speech_prob": 1.0952176126011182e-05}, {"id": 988, "seek": 408762, "start": 4095.3599999999997, "end": 4100.42, "text": " Convolutions and resnet blocks and transformers and blah blah blah. We will cover all that stuff", "tokens": [2656, 85, 15892, 293, 725, 7129, 8474, 293, 4088, 433, 293, 12288, 12288, 12288, 13, 492, 486, 2060, 439, 300, 1507], "temperature": 0.0, "avg_logprob": -0.27187746951454567, "compression_ratio": 1.6299559471365639, "no_speech_prob": 1.0952176126011182e-05}, {"id": 989, "seek": 408762, "start": 4101.22, "end": 4103.78, "text": " In part two and a little bit of it next week", "tokens": [682, 644, 732, 293, 257, 707, 857, 295, 309, 958, 1243], "temperature": 0.0, "avg_logprob": -0.27187746951454567, "compression_ratio": 1.6299559471365639, "no_speech_prob": 1.0952176126011182e-05}, {"id": 990, "seek": 408762, "start": 4104.42, "end": 4106.66, "text": " But it almost never matters", "tokens": [583, 309, 1920, 1128, 7001], "temperature": 0.0, "avg_logprob": -0.27187746951454567, "compression_ratio": 1.6299559471365639, "no_speech_prob": 1.0952176126011182e-05}, {"id": 991, "seek": 408762, "start": 4107.3, "end": 4108.26, "text": " Right", "tokens": [1779], "temperature": 0.0, "avg_logprob": -0.27187746951454567, "compression_ratio": 1.6299559471365639, "no_speech_prob": 1.0952176126011182e-05}, {"id": 992, "seek": 408762, "start": 4108.26, "end": 4112.34, "text": " It's just it's just a function right and what matters is the inputs to it", "tokens": [467, 311, 445, 309, 311, 445, 257, 2445, 558, 293, 437, 7001, 307, 264, 15743, 281, 309], "temperature": 0.0, "avg_logprob": -0.27187746951454567, "compression_ratio": 1.6299559471365639, "no_speech_prob": 1.0952176126011182e-05}, {"id": 993, "seek": 411234, "start": 4112.34, "end": 4117.38, "text": " And the outputs to it and how fast it is and how accurate it is", "tokens": [400, 264, 23930, 281, 309, 293, 577, 2370, 309, 307, 293, 577, 8559, 309, 307], "temperature": 0.0, "avg_logprob": -0.3218347502917778, "compression_ratio": 1.625, "no_speech_prob": 1.7329995216641692e-06}, {"id": 994, "seek": 411234, "start": 4117.7, "end": 4123.3, "text": " So let's create a learner which with a resnet 26 d from our data loaders", "tokens": [407, 718, 311, 1884, 257, 33347, 597, 365, 257, 725, 7129, 7551, 274, 490, 527, 1412, 3677, 433], "temperature": 0.0, "avg_logprob": -0.3218347502917778, "compression_ratio": 1.625, "no_speech_prob": 1.7329995216641692e-06}, {"id": 995, "seek": 411234, "start": 4124.82, "end": 4126.34, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.3218347502917778, "compression_ratio": 1.625, "no_speech_prob": 1.7329995216641692e-06}, {"id": 996, "seek": 411234, "start": 4126.34, "end": 4129.06, "text": " Let's run lr find so lr find", "tokens": [961, 311, 1190, 287, 81, 915, 370, 287, 81, 915], "temperature": 0.0, "avg_logprob": -0.3218347502917778, "compression_ratio": 1.625, "no_speech_prob": 1.7329995216641692e-06}, {"id": 997, "seek": 411234, "start": 4129.860000000001, "end": 4132.34, "text": " Will put through one mini batch at a time", "tokens": [3099, 829, 807, 472, 8382, 15245, 412, 257, 565], "temperature": 0.0, "avg_logprob": -0.3218347502917778, "compression_ratio": 1.625, "no_speech_prob": 1.7329995216641692e-06}, {"id": 998, "seek": 411234, "start": 4133.14, "end": 4136.900000000001, "text": " Starting at a very very very low learning rate and gradually increase the learning rate", "tokens": [16217, 412, 257, 588, 588, 588, 2295, 2539, 3314, 293, 13145, 3488, 264, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.3218347502917778, "compression_ratio": 1.625, "no_speech_prob": 1.7329995216641692e-06}, {"id": 999, "seek": 413690, "start": 4136.9, "end": 4142.42, "text": " and track the loss and", "tokens": [293, 2837, 264, 4470, 293], "temperature": 0.0, "avg_logprob": -0.3645789806659405, "compression_ratio": 2.0368663594470044, "no_speech_prob": 9.665413017501123e-06}, {"id": 1000, "seek": 413690, "start": 4143.94, "end": 4147.78, "text": " Initially the learn the loss won't improve because the learning rate is so small", "tokens": [29446, 264, 1466, 264, 4470, 1582, 380, 3470, 570, 264, 2539, 3314, 307, 370, 1359], "temperature": 0.0, "avg_logprob": -0.3645789806659405, "compression_ratio": 2.0368663594470044, "no_speech_prob": 9.665413017501123e-06}, {"id": 1001, "seek": 413690, "start": 4147.78, "end": 4152.82, "text": " It doesn't really do anything and at some point the learning rate is high enough that the loss will start coming down", "tokens": [467, 1177, 380, 534, 360, 1340, 293, 412, 512, 935, 264, 2539, 3314, 307, 1090, 1547, 300, 264, 4470, 486, 722, 1348, 760], "temperature": 0.0, "avg_logprob": -0.3645789806659405, "compression_ratio": 2.0368663594470044, "no_speech_prob": 9.665413017501123e-06}, {"id": 1002, "seek": 413690, "start": 4153.139999999999, "end": 4158.339999999999, "text": " Then at some other point the load the learning rate is so high that it's going to start jumping past the", "tokens": [1396, 412, 512, 661, 935, 264, 3677, 264, 2539, 3314, 307, 370, 1090, 300, 309, 311, 516, 281, 722, 11233, 1791, 264], "temperature": 0.0, "avg_logprob": -0.3645789806659405, "compression_ratio": 2.0368663594470044, "no_speech_prob": 9.665413017501123e-06}, {"id": 1003, "seek": 413690, "start": 4158.98, "end": 4160.98, "text": " answer and it's got to get worse", "tokens": [1867, 293, 309, 311, 658, 281, 483, 5324], "temperature": 0.0, "avg_logprob": -0.3645789806659405, "compression_ratio": 2.0368663594470044, "no_speech_prob": 9.665413017501123e-06}, {"id": 1004, "seek": 413690, "start": 4161.62, "end": 4165.46, "text": " And so somewhere around here is a learning rate where you're going to start to see", "tokens": [400, 370, 4079, 926, 510, 307, 257, 2539, 3314, 689, 291, 434, 516, 281, 722, 281, 536], "temperature": 0.0, "avg_logprob": -0.3645789806659405, "compression_ratio": 2.0368663594470044, "no_speech_prob": 9.665413017501123e-06}, {"id": 1005, "seek": 416546, "start": 4165.46, "end": 4167.46, "text": " Is a learning rate we'd want to pick", "tokens": [1119, 257, 2539, 3314, 321, 1116, 528, 281, 1888], "temperature": 0.0, "avg_logprob": -0.11643089042915093, "compression_ratio": 1.5647058823529412, "no_speech_prob": 4.4252738007344306e-06}, {"id": 1006, "seek": 416546, "start": 4170.82, "end": 4173.14, "text": " We've got a couple of different ways of making suggestions", "tokens": [492, 600, 658, 257, 1916, 295, 819, 2098, 295, 1455, 13396], "temperature": 0.0, "avg_logprob": -0.11643089042915093, "compression_ratio": 1.5647058823529412, "no_speech_prob": 4.4252738007344306e-06}, {"id": 1007, "seek": 416546, "start": 4176.18, "end": 4181.54, "text": " I generally ignore them because these suggestions are specifically designed to be conservative", "tokens": [286, 5101, 11200, 552, 570, 613, 13396, 366, 4682, 4761, 281, 312, 13780], "temperature": 0.0, "avg_logprob": -0.11643089042915093, "compression_ratio": 1.5647058823529412, "no_speech_prob": 4.4252738007344306e-06}, {"id": 1008, "seek": 416546, "start": 4181.62, "end": 4187.22, "text": " They're a bit lower than perhaps an optimal in order to make sure we don't recommend something that totally screws up", "tokens": [814, 434, 257, 857, 3126, 813, 4317, 364, 16252, 294, 1668, 281, 652, 988, 321, 500, 380, 2748, 746, 300, 3879, 13050, 493], "temperature": 0.0, "avg_logprob": -0.11643089042915093, "compression_ratio": 1.5647058823529412, "no_speech_prob": 4.4252738007344306e-06}, {"id": 1009, "seek": 416546, "start": 4188.26, "end": 4192.1, "text": " But I kind of like to say like well, how far right can I go and still see it like clearly?", "tokens": [583, 286, 733, 295, 411, 281, 584, 411, 731, 11, 577, 1400, 558, 393, 286, 352, 293, 920, 536, 309, 411, 4448, 30], "temperature": 0.0, "avg_logprob": -0.11643089042915093, "compression_ratio": 1.5647058823529412, "no_speech_prob": 4.4252738007344306e-06}, {"id": 1010, "seek": 419210, "start": 4192.1, "end": 4195.54, "text": " Really improving quickly and so i'd pick somewhere around", "tokens": [4083, 11470, 2661, 293, 370, 741, 1116, 1888, 4079, 926], "temperature": 0.0, "avg_logprob": -0.15756689763702122, "compression_ratio": 1.565891472868217, "no_speech_prob": 8.938476639741566e-06}, {"id": 1011, "seek": 419210, "start": 4197.38, "end": 4199.38, "text": " 0.01 for this", "tokens": [1958, 13, 10607, 337, 341], "temperature": 0.0, "avg_logprob": -0.15756689763702122, "compression_ratio": 1.565891472868217, "no_speech_prob": 8.938476639741566e-06}, {"id": 1012, "seek": 419210, "start": 4200.18, "end": 4201.9400000000005, "text": " so I can now", "tokens": [370, 286, 393, 586], "temperature": 0.0, "avg_logprob": -0.15756689763702122, "compression_ratio": 1.565891472868217, "no_speech_prob": 8.938476639741566e-06}, {"id": 1013, "seek": 419210, "start": 4201.9400000000005, "end": 4204.34, "text": " Fine tune our model with a learning rate of 0.01", "tokens": [12024, 10864, 527, 2316, 365, 257, 2539, 3314, 295, 1958, 13, 10607], "temperature": 0.0, "avg_logprob": -0.15756689763702122, "compression_ratio": 1.565891472868217, "no_speech_prob": 8.938476639741566e-06}, {"id": 1014, "seek": 419210, "start": 4205.3, "end": 4210.26, "text": " Three epochs. So look the whole thing took a minute. That's what we want. Right? We want to be able to iterate", "tokens": [6244, 30992, 28346, 13, 407, 574, 264, 1379, 551, 1890, 257, 3456, 13, 663, 311, 437, 321, 528, 13, 1779, 30, 492, 528, 281, 312, 1075, 281, 44497], "temperature": 0.0, "avg_logprob": -0.15756689763702122, "compression_ratio": 1.565891472868217, "no_speech_prob": 8.938476639741566e-06}, {"id": 1015, "seek": 419210, "start": 4210.88, "end": 4216.34, "text": " Rapidly just a minute or so. So that's enough time for me to go and you know grab a glass of water or", "tokens": [44580, 356, 445, 257, 3456, 420, 370, 13, 407, 300, 311, 1547, 565, 337, 385, 281, 352, 293, 291, 458, 4444, 257, 4276, 295, 1281, 420], "temperature": 0.0, "avg_logprob": -0.15756689763702122, "compression_ratio": 1.565891472868217, "no_speech_prob": 8.938476639741566e-06}, {"id": 1016, "seek": 419210, "start": 4217.3, "end": 4219.96, "text": " Do some reading like it's not going to get too distracted", "tokens": [1144, 512, 3760, 411, 309, 311, 406, 516, 281, 483, 886, 21658], "temperature": 0.0, "avg_logprob": -0.15756689763702122, "compression_ratio": 1.565891472868217, "no_speech_prob": 8.938476639741566e-06}, {"id": 1017, "seek": 421996, "start": 4219.96, "end": 4221.96, "text": " And", "tokens": [400], "temperature": 0.0, "avg_logprob": -0.13024722651431436, "compression_ratio": 1.6336633663366336, "no_speech_prob": 1.3844980458088685e-05}, {"id": 1018, "seek": 421996, "start": 4223.0, "end": 4225.0, "text": " What do we do before we submit", "tokens": [708, 360, 321, 360, 949, 321, 10315], "temperature": 0.0, "avg_logprob": -0.13024722651431436, "compression_ratio": 1.6336633663366336, "no_speech_prob": 1.3844980458088685e-05}, {"id": 1019, "seek": 421996, "start": 4225.56, "end": 4227.56, "text": " Nothing we submit as soon as we can", "tokens": [6693, 321, 10315, 382, 2321, 382, 321, 393], "temperature": 0.0, "avg_logprob": -0.13024722651431436, "compression_ratio": 1.6336633663366336, "no_speech_prob": 1.3844980458088685e-05}, {"id": 1020, "seek": 421996, "start": 4227.96, "end": 4230.76, "text": " Okay, let's get our submission in so we've got a model", "tokens": [1033, 11, 718, 311, 483, 527, 23689, 294, 370, 321, 600, 658, 257, 2316], "temperature": 0.0, "avg_logprob": -0.13024722651431436, "compression_ratio": 1.6336633663366336, "no_speech_prob": 1.3844980458088685e-05}, {"id": 1021, "seek": 421996, "start": 4231.32, "end": 4233.32, "text": " Let's get it in", "tokens": [961, 311, 483, 309, 294], "temperature": 0.0, "avg_logprob": -0.13024722651431436, "compression_ratio": 1.6336633663366336, "no_speech_prob": 1.3844980458088685e-05}, {"id": 1022, "seek": 421996, "start": 4234.44, "end": 4237.32, "text": " So we read in our csv file of the sample submission", "tokens": [407, 321, 1401, 294, 527, 28277, 85, 3991, 295, 264, 6889, 23689], "temperature": 0.0, "avg_logprob": -0.13024722651431436, "compression_ratio": 1.6336633663366336, "no_speech_prob": 1.3844980458088685e-05}, {"id": 1023, "seek": 421996, "start": 4238.36, "end": 4243.4, "text": " And so the csv file basically looks like we're going to have to have a list of the image file names", "tokens": [400, 370, 264, 28277, 85, 3991, 1936, 1542, 411, 321, 434, 516, 281, 362, 281, 362, 257, 1329, 295, 264, 3256, 3991, 5288], "temperature": 0.0, "avg_logprob": -0.13024722651431436, "compression_ratio": 1.6336633663366336, "no_speech_prob": 1.3844980458088685e-05}, {"id": 1024, "seek": 424340, "start": 4243.4, "end": 4249.4, "text": " In order and then a column of labels", "tokens": [682, 1668, 293, 550, 257, 7738, 295, 16949], "temperature": 0.0, "avg_logprob": -0.10136161131017349, "compression_ratio": 1.738888888888889, "no_speech_prob": 2.6841307771974243e-06}, {"id": 1025, "seek": 424340, "start": 4250.759999999999, "end": 4253.48, "text": " So we can get all the image files in the test image", "tokens": [407, 321, 393, 483, 439, 264, 3256, 7098, 294, 264, 1500, 3256], "temperature": 0.0, "avg_logprob": -0.10136161131017349, "compression_ratio": 1.738888888888889, "no_speech_prob": 2.6841307771974243e-06}, {"id": 1026, "seek": 424340, "start": 4254.839999999999, "end": 4256.839999999999, "text": " Like so and we can sort them", "tokens": [1743, 370, 293, 321, 393, 1333, 552], "temperature": 0.0, "avg_logprob": -0.10136161131017349, "compression_ratio": 1.738888888888889, "no_speech_prob": 2.6841307771974243e-06}, {"id": 1027, "seek": 424340, "start": 4258.04, "end": 4261.48, "text": " And so now we want is what we want is a data loader", "tokens": [400, 370, 586, 321, 528, 307, 437, 321, 528, 307, 257, 1412, 3677, 260], "temperature": 0.0, "avg_logprob": -0.10136161131017349, "compression_ratio": 1.738888888888889, "no_speech_prob": 2.6841307771974243e-06}, {"id": 1028, "seek": 424340, "start": 4262.679999999999, "end": 4263.96, "text": " which", "tokens": [597], "temperature": 0.0, "avg_logprob": -0.10136161131017349, "compression_ratio": 1.738888888888889, "no_speech_prob": 2.6841307771974243e-06}, {"id": 1029, "seek": 424340, "start": 4263.96, "end": 4267.4, "text": " Is exactly like the data loader we use to train the model", "tokens": [1119, 2293, 411, 264, 1412, 3677, 260, 321, 764, 281, 3847, 264, 2316], "temperature": 0.0, "avg_logprob": -0.10136161131017349, "compression_ratio": 1.738888888888889, "no_speech_prob": 2.6841307771974243e-06}, {"id": 1030, "seek": 424340, "start": 4268.2, "end": 4271.74, "text": " Except pointing at the test set we want to use exactly the same transformations", "tokens": [16192, 12166, 412, 264, 1500, 992, 321, 528, 281, 764, 2293, 264, 912, 34852], "temperature": 0.0, "avg_logprob": -0.10136161131017349, "compression_ratio": 1.738888888888889, "no_speech_prob": 2.6841307771974243e-06}, {"id": 1031, "seek": 427174, "start": 4271.74, "end": 4277.26, "text": " So there's actually a dl's dot test dl method which does that you just pass in", "tokens": [407, 456, 311, 767, 257, 37873, 311, 5893, 1500, 37873, 3170, 597, 775, 300, 291, 445, 1320, 294], "temperature": 0.0, "avg_logprob": -0.29583604600694446, "compression_ratio": 1.7119565217391304, "no_speech_prob": 5.594070898951031e-06}, {"id": 1032, "seek": 427174, "start": 4277.82, "end": 4280.46, "text": " The new set of items so the test set files", "tokens": [440, 777, 992, 295, 4754, 370, 264, 1500, 992, 7098], "temperature": 0.0, "avg_logprob": -0.29583604600694446, "compression_ratio": 1.7119565217391304, "no_speech_prob": 5.594070898951031e-06}, {"id": 1033, "seek": 427174, "start": 4281.58, "end": 4283.98, "text": " So this is a data loader which we can use", "tokens": [407, 341, 307, 257, 1412, 3677, 260, 597, 321, 393, 764], "temperature": 0.0, "avg_logprob": -0.29583604600694446, "compression_ratio": 1.7119565217391304, "no_speech_prob": 5.594070898951031e-06}, {"id": 1034, "seek": 427174, "start": 4284.62, "end": 4286.62, "text": " for our", "tokens": [337, 527], "temperature": 0.0, "avg_logprob": -0.29583604600694446, "compression_ratio": 1.7119565217391304, "no_speech_prob": 5.594070898951031e-06}, {"id": 1035, "seek": 427174, "start": 4286.62, "end": 4288.62, "text": " test set", "tokens": [1500, 992], "temperature": 0.0, "avg_logprob": -0.29583604600694446, "compression_ratio": 1.7119565217391304, "no_speech_prob": 5.594070898951031e-06}, {"id": 1036, "seek": 427174, "start": 4289.0199999999995, "end": 4295.5, "text": " A test data loader has a key difference to a normal data loader, which is that it does not have any labels", "tokens": [316, 1500, 1412, 3677, 260, 575, 257, 2141, 2649, 281, 257, 2710, 1412, 3677, 260, 11, 597, 307, 300, 309, 775, 406, 362, 604, 16949], "temperature": 0.0, "avg_logprob": -0.29583604600694446, "compression_ratio": 1.7119565217391304, "no_speech_prob": 5.594070898951031e-06}, {"id": 1037, "seek": 427174, "start": 4296.7, "end": 4298.7, "text": " So that's a key distinction", "tokens": [407, 300, 311, 257, 2141, 16844], "temperature": 0.0, "avg_logprob": -0.29583604600694446, "compression_ratio": 1.7119565217391304, "no_speech_prob": 5.594070898951031e-06}, {"id": 1038, "seek": 429870, "start": 4298.7, "end": 4301.98, "text": " So we can get the predictions for our learner passing in", "tokens": [407, 321, 393, 483, 264, 21264, 337, 527, 33347, 8437, 294], "temperature": 0.0, "avg_logprob": -0.271013947420342, "compression_ratio": 1.6956521739130435, "no_speech_prob": 6.4384830693597905e-06}, {"id": 1039, "seek": 429870, "start": 4302.78, "end": 4304.78, "text": " that data loader", "tokens": [300, 1412, 3677, 260], "temperature": 0.0, "avg_logprob": -0.271013947420342, "compression_ratio": 1.6956521739130435, "no_speech_prob": 6.4384830693597905e-06}, {"id": 1040, "seek": 429870, "start": 4305.0199999999995, "end": 4314.0599999999995, "text": " And in the case of a classification problem, you can also ask for them to be decoded decoded means rather than just get returned the probability", "tokens": [400, 294, 264, 1389, 295, 257, 21538, 1154, 11, 291, 393, 611, 1029, 337, 552, 281, 312, 979, 12340, 979, 12340, 1355, 2831, 813, 445, 483, 8752, 264, 8482], "temperature": 0.0, "avg_logprob": -0.271013947420342, "compression_ratio": 1.6956521739130435, "no_speech_prob": 6.4384830693597905e-06}, {"id": 1041, "seek": 429870, "start": 4314.62, "end": 4316.62, "text": " of every", "tokens": [295, 633], "temperature": 0.0, "avg_logprob": -0.271013947420342, "compression_ratio": 1.6956521739130435, "no_speech_prob": 6.4384830693597905e-06}, {"id": 1042, "seek": 429870, "start": 4316.62, "end": 4322.46, "text": " Rice disease where every plus it'll tell you what is the index of the most probable", "tokens": [19386, 4752, 689, 633, 1804, 309, 603, 980, 291, 437, 307, 264, 8186, 295, 264, 881, 21759], "temperature": 0.0, "avg_logprob": -0.271013947420342, "compression_ratio": 1.6956521739130435, "no_speech_prob": 6.4384830693597905e-06}, {"id": 1043, "seek": 429870, "start": 4323.099999999999, "end": 4325.099999999999, "text": " Rice disease, that's what decoded means", "tokens": [19386, 4752, 11, 300, 311, 437, 979, 12340, 1355], "temperature": 0.0, "avg_logprob": -0.271013947420342, "compression_ratio": 1.6956521739130435, "no_speech_prob": 6.4384830693597905e-06}, {"id": 1044, "seek": 432510, "start": 4325.1, "end": 4333.02, "text": " so that'll return with probabilities, uh targets which obviously will be empty because it's a test set so throw them away and those decoded indexes", "tokens": [370, 300, 603, 2736, 365, 33783, 11, 2232, 12911, 597, 2745, 486, 312, 6707, 570, 309, 311, 257, 1500, 992, 370, 3507, 552, 1314, 293, 729, 979, 12340, 8186, 279], "temperature": 0.0, "avg_logprob": -0.3054112328423394, "compression_ratio": 1.6577777777777778, "no_speech_prob": 2.8129766178608406e-06}, {"id": 1045, "seek": 432510, "start": 4333.660000000001, "end": 4337.900000000001, "text": " Which look like this numbers from 0 to 9 because there's 10 possible rice diseases", "tokens": [3013, 574, 411, 341, 3547, 490, 1958, 281, 1722, 570, 456, 311, 1266, 1944, 5090, 11044], "temperature": 0.0, "avg_logprob": -0.3054112328423394, "compression_ratio": 1.6577777777777778, "no_speech_prob": 2.8129766178608406e-06}, {"id": 1046, "seek": 432510, "start": 4339.660000000001, "end": 4343.660000000001, "text": " The caggle submission does not expect numbers from 0 to 9 it expects to see", "tokens": [440, 269, 559, 22631, 23689, 775, 406, 2066, 3547, 490, 1958, 281, 1722, 309, 33280, 281, 536], "temperature": 0.0, "avg_logprob": -0.3054112328423394, "compression_ratio": 1.6577777777777778, "no_speech_prob": 2.8129766178608406e-06}, {"id": 1047, "seek": 432510, "start": 4344.9400000000005, "end": 4346.9400000000005, "text": " Strings like these", "tokens": [8251, 1109, 411, 613], "temperature": 0.0, "avg_logprob": -0.3054112328423394, "compression_ratio": 1.6577777777777778, "no_speech_prob": 2.8129766178608406e-06}, {"id": 1048, "seek": 432510, "start": 4347.42, "end": 4350.700000000001, "text": " So what do those numbers from 0 to 9 represent?", "tokens": [407, 437, 360, 729, 3547, 490, 1958, 281, 1722, 2906, 30], "temperature": 0.0, "avg_logprob": -0.3054112328423394, "compression_ratio": 1.6577777777777778, "no_speech_prob": 2.8129766178608406e-06}, {"id": 1049, "seek": 435070, "start": 4350.7, "end": 4354.38, "text": " We can look up our vocab to get a list", "tokens": [492, 393, 574, 493, 527, 2329, 455, 281, 483, 257, 1329], "temperature": 0.0, "avg_logprob": -0.29604934860061816, "compression_ratio": 1.517766497461929, "no_speech_prob": 1.7329788306597038e-06}, {"id": 1050, "seek": 435070, "start": 4355.0199999999995, "end": 4357.74, "text": " So that's zero. That's one etc. That's nine", "tokens": [407, 300, 311, 4018, 13, 663, 311, 472, 5183, 13, 663, 311, 4949], "temperature": 0.0, "avg_logprob": -0.29604934860061816, "compression_ratio": 1.517766497461929, "no_speech_prob": 1.7329788306597038e-06}, {"id": 1051, "seek": 435070, "start": 4359.82, "end": 4361.26, "text": " So, um", "tokens": [407, 11, 1105], "temperature": 0.0, "avg_logprob": -0.29604934860061816, "compression_ratio": 1.517766497461929, "no_speech_prob": 1.7329788306597038e-06}, {"id": 1052, "seek": 435070, "start": 4361.26, "end": 4365.42, "text": " I realized later this is a slightly inefficient way to do it, but it does the job", "tokens": [286, 5334, 1780, 341, 307, 257, 4748, 43495, 636, 281, 360, 309, 11, 457, 309, 775, 264, 1691], "temperature": 0.0, "avg_logprob": -0.29604934860061816, "compression_ratio": 1.517766497461929, "no_speech_prob": 1.7329788306597038e-06}, {"id": 1053, "seek": 435070, "start": 4365.42, "end": 4368.46, "text": " I need to be able to map these to strings", "tokens": [286, 643, 281, 312, 1075, 281, 4471, 613, 281, 13985], "temperature": 0.0, "avg_logprob": -0.29604934860061816, "compression_ratio": 1.517766497461929, "no_speech_prob": 1.7329788306597038e-06}, {"id": 1054, "seek": 435070, "start": 4369.58, "end": 4370.78, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.29604934860061816, "compression_ratio": 1.517766497461929, "no_speech_prob": 1.7329788306597038e-06}, {"id": 1055, "seek": 435070, "start": 4370.78, "end": 4376.0599999999995, "text": " If I enumerate the vocab that gives me pairs of numbers zero bacterial leaf blight", "tokens": [759, 286, 465, 15583, 473, 264, 2329, 455, 300, 2709, 385, 15494, 295, 3547, 4018, 35632, 10871, 888, 397], "temperature": 0.0, "avg_logprob": -0.29604934860061816, "compression_ratio": 1.517766497461929, "no_speech_prob": 1.7329788306597038e-06}, {"id": 1056, "seek": 437606, "start": 4376.06, "end": 4380.46, "text": " One bacterial leaf streak, etc. I could then create a dictionary out of that", "tokens": [1485, 35632, 10871, 35634, 11, 5183, 13, 286, 727, 550, 1884, 257, 25890, 484, 295, 300], "temperature": 0.0, "avg_logprob": -0.24874685208002725, "compression_ratio": 1.6367713004484306, "no_speech_prob": 3.446494019954116e-06}, {"id": 1057, "seek": 437606, "start": 4381.18, "end": 4383.18, "text": " And then I can use pandas", "tokens": [400, 550, 286, 393, 764, 4565, 296], "temperature": 0.0, "avg_logprob": -0.24874685208002725, "compression_ratio": 1.6367713004484306, "no_speech_prob": 3.446494019954116e-06}, {"id": 1058, "seek": 437606, "start": 4383.820000000001, "end": 4387.42, "text": " To look up each thing in a dictionary. They call that map", "tokens": [1407, 574, 493, 1184, 551, 294, 257, 25890, 13, 814, 818, 300, 4471], "temperature": 0.0, "avg_logprob": -0.24874685208002725, "compression_ratio": 1.6367713004484306, "no_speech_prob": 3.446494019954116e-06}, {"id": 1059, "seek": 437606, "start": 4388.54, "end": 4393.26, "text": " If you're a pandas user, you've probably seen map used before being passed a function", "tokens": [759, 291, 434, 257, 4565, 296, 4195, 11, 291, 600, 1391, 1612, 4471, 1143, 949, 885, 4678, 257, 2445], "temperature": 0.0, "avg_logprob": -0.24874685208002725, "compression_ratio": 1.6367713004484306, "no_speech_prob": 3.446494019954116e-06}, {"id": 1060, "seek": 437606, "start": 4393.9800000000005, "end": 4395.9800000000005, "text": " Which is really really slow", "tokens": [3013, 307, 534, 534, 2964], "temperature": 0.0, "avg_logprob": -0.24874685208002725, "compression_ratio": 1.6367713004484306, "no_speech_prob": 3.446494019954116e-06}, {"id": 1061, "seek": 437606, "start": 4396.14, "end": 4400.860000000001, "text": " But if you pass map addict, it's actually really really fast. So do it this way if you can", "tokens": [583, 498, 291, 1320, 4471, 22072, 11, 309, 311, 767, 534, 534, 2370, 13, 407, 360, 309, 341, 636, 498, 291, 393], "temperature": 0.0, "avg_logprob": -0.24874685208002725, "compression_ratio": 1.6367713004484306, "no_speech_prob": 3.446494019954116e-06}, {"id": 1062, "seek": 440086, "start": 4400.86, "end": 4405.679999999999, "text": " So here's our predictions", "tokens": [407, 510, 311, 527, 21264], "temperature": 0.0, "avg_logprob": -0.4224319798605783, "compression_ratio": 1.4074074074074074, "no_speech_prob": 7.527620255132206e-06}, {"id": 1063, "seek": 440086, "start": 4406.86, "end": 4408.86, "text": " So we've got our", "tokens": [407, 321, 600, 658, 527], "temperature": 0.0, "avg_logprob": -0.4224319798605783, "compression_ratio": 1.4074074074074074, "no_speech_prob": 7.527620255132206e-06}, {"id": 1064, "seek": 440086, "start": 4411.58, "end": 4416.62, "text": " Sample submission file ss. So if we replace this column label with our predictions", "tokens": [4832, 781, 23689, 3991, 262, 82, 13, 407, 498, 321, 7406, 341, 7738, 7645, 365, 527, 21264], "temperature": 0.0, "avg_logprob": -0.4224319798605783, "compression_ratio": 1.4074074074074074, "no_speech_prob": 7.527620255132206e-06}, {"id": 1065, "seek": 440086, "start": 4417.66, "end": 4419.66, "text": " like so", "tokens": [411, 370], "temperature": 0.0, "avg_logprob": -0.4224319798605783, "compression_ratio": 1.4074074074074074, "no_speech_prob": 7.527620255132206e-06}, {"id": 1066, "seek": 440086, "start": 4419.9, "end": 4421.9, "text": " Then we can turn that into a csv", "tokens": [1396, 321, 393, 1261, 300, 666, 257, 28277, 85], "temperature": 0.0, "avg_logprob": -0.4224319798605783, "compression_ratio": 1.4074074074074074, "no_speech_prob": 7.527620255132206e-06}, {"id": 1067, "seek": 440086, "start": 4422.78, "end": 4424.78, "text": " And remember this means", "tokens": [400, 1604, 341, 1355], "temperature": 0.0, "avg_logprob": -0.4224319798605783, "compression_ratio": 1.4074074074074074, "no_speech_prob": 7.527620255132206e-06}, {"id": 1068, "seek": 442478, "start": 4424.78, "end": 4432.0599999999995, "text": " Uh, this means run a bash command a shell command head is the first few rows. Let's just take a look", "tokens": [4019, 11, 341, 1355, 1190, 257, 46183, 5622, 257, 8720, 5622, 1378, 307, 264, 700, 1326, 13241, 13, 961, 311, 445, 747, 257, 574], "temperature": 0.0, "avg_logprob": -0.39589716879169595, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.356733370514121e-06}, {"id": 1069, "seek": 442478, "start": 4432.38, "end": 4434.38, "text": " That looks reasonable", "tokens": [663, 1542, 10585], "temperature": 0.0, "avg_logprob": -0.39589716879169595, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.356733370514121e-06}, {"id": 1070, "seek": 442478, "start": 4435.259999999999, "end": 4437.58, "text": " So we can now submit that to kaggle now", "tokens": [407, 321, 393, 586, 10315, 300, 281, 350, 559, 22631, 586], "temperature": 0.0, "avg_logprob": -0.39589716879169595, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.356733370514121e-06}, {"id": 1071, "seek": 442478, "start": 4438.94, "end": 4442.54, "text": " Iterating rapidly means everything needs to be", "tokens": [286, 391, 990, 12910, 1355, 1203, 2203, 281, 312], "temperature": 0.0, "avg_logprob": -0.39589716879169595, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.356733370514121e-06}, {"id": 1072, "seek": 442478, "start": 4443.66, "end": 4445.259999999999, "text": " fast and easy", "tokens": [2370, 293, 1858], "temperature": 0.0, "avg_logprob": -0.39589716879169595, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.356733370514121e-06}, {"id": 1073, "seek": 442478, "start": 4445.259999999999, "end": 4448.62, "text": " Things that are slow and hard don't just take up your time", "tokens": [9514, 300, 366, 2964, 293, 1152, 500, 380, 445, 747, 493, 428, 565], "temperature": 0.0, "avg_logprob": -0.39589716879169595, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.356733370514121e-06}, {"id": 1074, "seek": 442478, "start": 4448.62, "end": 4450.62, "text": " But they're going to be a lot of work", "tokens": [583, 436, 434, 516, 281, 312, 257, 688, 295, 589], "temperature": 0.0, "avg_logprob": -0.39589716879169595, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.356733370514121e-06}, {"id": 1075, "seek": 445062, "start": 4450.62, "end": 4455.82, "text": " Things that are slow and hard don't just take up your time, but they take up your mental energy", "tokens": [9514, 300, 366, 2964, 293, 1152, 500, 380, 445, 747, 493, 428, 565, 11, 457, 436, 747, 493, 428, 4973, 2281], "temperature": 0.0, "avg_logprob": -0.11033795498035572, "compression_ratio": 1.6636363636363636, "no_speech_prob": 1.2028199307678733e-05}, {"id": 1076, "seek": 445062, "start": 4456.22, "end": 4458.78, "text": " So even submitting to kaggle needs needs to be fast. So", "tokens": [407, 754, 31836, 281, 350, 559, 22631, 2203, 2203, 281, 312, 2370, 13, 407], "temperature": 0.0, "avg_logprob": -0.11033795498035572, "compression_ratio": 1.6636363636363636, "no_speech_prob": 1.2028199307678733e-05}, {"id": 1077, "seek": 445062, "start": 4459.66, "end": 4461.66, "text": " I put it into a cell", "tokens": [286, 829, 309, 666, 257, 2815], "temperature": 0.0, "avg_logprob": -0.11033795498035572, "compression_ratio": 1.6636363636363636, "no_speech_prob": 1.2028199307678733e-05}, {"id": 1078, "seek": 445062, "start": 4461.66, "end": 4463.66, "text": " So I can just run this cell", "tokens": [407, 286, 393, 445, 1190, 341, 2815], "temperature": 0.0, "avg_logprob": -0.11033795498035572, "compression_ratio": 1.6636363636363636, "no_speech_prob": 1.2028199307678733e-05}, {"id": 1079, "seek": 445062, "start": 4466.62, "end": 4468.62, "text": " Api.competition submit", "tokens": [8723, 72, 13, 1112, 7275, 849, 10315], "temperature": 0.0, "avg_logprob": -0.11033795498035572, "compression_ratio": 1.6636363636363636, "no_speech_prob": 1.2028199307678733e-05}, {"id": 1080, "seek": 445062, "start": 4469.34, "end": 4471.18, "text": " This csv file", "tokens": [639, 28277, 85, 3991], "temperature": 0.0, "avg_logprob": -0.11033795498035572, "compression_ratio": 1.6636363636363636, "no_speech_prob": 1.2028199307678733e-05}, {"id": 1081, "seek": 445062, "start": 4471.18, "end": 4472.62, "text": " Give it a description", "tokens": [5303, 309, 257, 3855], "temperature": 0.0, "avg_logprob": -0.11033795498035572, "compression_ratio": 1.6636363636363636, "no_speech_prob": 1.2028199307678733e-05}, {"id": 1082, "seek": 445062, "start": 4472.62, "end": 4474.7, "text": " So just run the cell and it submits to kaggle", "tokens": [407, 445, 1190, 264, 2815, 293, 309, 8286, 1208, 281, 350, 559, 22631], "temperature": 0.0, "avg_logprob": -0.11033795498035572, "compression_ratio": 1.6636363636363636, "no_speech_prob": 1.2028199307678733e-05}, {"id": 1083, "seek": 445062, "start": 4475.34, "end": 4478.22, "text": " And as you can see it says here we go successfully submitted", "tokens": [400, 382, 291, 393, 536, 309, 1619, 510, 321, 352, 10727, 14405], "temperature": 0.0, "avg_logprob": -0.11033795498035572, "compression_ratio": 1.6636363636363636, "no_speech_prob": 1.2028199307678733e-05}, {"id": 1084, "seek": 447822, "start": 4478.22, "end": 4482.14, "text": " So that submission was terrible", "tokens": [407, 300, 23689, 390, 6237], "temperature": 0.0, "avg_logprob": -0.3100906638211982, "compression_ratio": 1.5201793721973094, "no_speech_prob": 3.446450818955782e-06}, {"id": 1085, "seek": 447822, "start": 4483.9800000000005, "end": 4486.38, "text": " Top 80% also known as bottom 20%", "tokens": [8840, 4688, 4, 611, 2570, 382, 2767, 945, 4], "temperature": 0.0, "avg_logprob": -0.3100906638211982, "compression_ratio": 1.5201793721973094, "no_speech_prob": 3.446450818955782e-06}, {"id": 1086, "seek": 447822, "start": 4487.18, "end": 4492.46, "text": " Which is not too surprising, right? I mean, it's it's one minute of training time", "tokens": [3013, 307, 406, 886, 8830, 11, 558, 30, 286, 914, 11, 309, 311, 309, 311, 472, 3456, 295, 3097, 565], "temperature": 0.0, "avg_logprob": -0.3100906638211982, "compression_ratio": 1.5201793721973094, "no_speech_prob": 3.446450818955782e-06}, {"id": 1087, "seek": 447822, "start": 4495.58, "end": 4499.34, "text": " But it's something that we can start with and that would be like", "tokens": [583, 309, 311, 746, 300, 321, 393, 722, 365, 293, 300, 576, 312, 411], "temperature": 0.0, "avg_logprob": -0.3100906638211982, "compression_ratio": 1.5201793721973094, "no_speech_prob": 3.446450818955782e-06}, {"id": 1088, "seek": 447822, "start": 4500.06, "end": 4503.02, "text": " However long it takes to get to this point that you put in our submission", "tokens": [2908, 938, 309, 2516, 281, 483, 281, 341, 935, 300, 291, 829, 294, 527, 23689], "temperature": 0.0, "avg_logprob": -0.3100906638211982, "compression_ratio": 1.5201793721973094, "no_speech_prob": 3.446450818955782e-06}, {"id": 1089, "seek": 447822, "start": 4503.66, "end": 4506.14, "text": " Now you've really started right because then tomorrow", "tokens": [823, 291, 600, 534, 1409, 558, 570, 550, 4153], "temperature": 0.0, "avg_logprob": -0.3100906638211982, "compression_ratio": 1.5201793721973094, "no_speech_prob": 3.446450818955782e-06}, {"id": 1090, "seek": 450614, "start": 4506.14, "end": 4507.900000000001, "text": " You can try to", "tokens": [509, 393, 853, 281], "temperature": 0.0, "avg_logprob": -0.21355181473952073, "compression_ratio": 1.5647058823529412, "no_speech_prob": 1.8922546587418765e-05}, {"id": 1091, "seek": 450614, "start": 4507.900000000001, "end": 4509.900000000001, "text": " Make a slightly better one", "tokens": [4387, 257, 4748, 1101, 472], "temperature": 0.0, "avg_logprob": -0.21355181473952073, "compression_ratio": 1.5647058823529412, "no_speech_prob": 1.8922546587418765e-05}, {"id": 1092, "seek": 450614, "start": 4513.34, "end": 4518.06, "text": " So I like to share my notebooks and so even sharing the notebook i've automated", "tokens": [407, 286, 411, 281, 2073, 452, 43782, 293, 370, 754, 5414, 264, 21060, 741, 600, 18473], "temperature": 0.0, "avg_logprob": -0.21355181473952073, "compression_ratio": 1.5647058823529412, "no_speech_prob": 1.8922546587418765e-05}, {"id": 1093, "seek": 450614, "start": 4518.54, "end": 4522.3, "text": " So part of fast kaggle is you can use this thing called push notebook", "tokens": [407, 644, 295, 2370, 350, 559, 22631, 307, 291, 393, 764, 341, 551, 1219, 2944, 21060], "temperature": 0.0, "avg_logprob": -0.21355181473952073, "compression_ratio": 1.5647058823529412, "no_speech_prob": 1.8922546587418765e-05}, {"id": 1094, "seek": 450614, "start": 4522.9400000000005, "end": 4524.9400000000005, "text": " And that sends it off to kaggle", "tokens": [400, 300, 14790, 309, 766, 281, 350, 559, 22631], "temperature": 0.0, "avg_logprob": -0.21355181473952073, "compression_ratio": 1.5647058823529412, "no_speech_prob": 1.8922546587418765e-05}, {"id": 1095, "seek": 450614, "start": 4525.5, "end": 4527.5, "text": " to create", "tokens": [281, 1884], "temperature": 0.0, "avg_logprob": -0.21355181473952073, "compression_ratio": 1.5647058823529412, "no_speech_prob": 1.8922546587418765e-05}, {"id": 1096, "seek": 450614, "start": 4530.3, "end": 4532.3, "text": " A notebook on kaggle", "tokens": [316, 21060, 322, 350, 559, 22631], "temperature": 0.0, "avg_logprob": -0.21355181473952073, "compression_ratio": 1.5647058823529412, "no_speech_prob": 1.8922546587418765e-05}, {"id": 1097, "seek": 453230, "start": 4532.3, "end": 4535.58, "text": " There it is and there's my score", "tokens": [821, 309, 307, 293, 456, 311, 452, 6175], "temperature": 0.0, "avg_logprob": -0.35575616359710693, "compression_ratio": 1.2844827586206897, "no_speech_prob": 1.384221286571119e-05}, {"id": 1098, "seek": 453230, "start": 4538.54, "end": 4540.54, "text": " As you can see it's exactly the same thing", "tokens": [1018, 291, 393, 536, 309, 311, 2293, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.35575616359710693, "compression_ratio": 1.2844827586206897, "no_speech_prob": 1.384221286571119e-05}, {"id": 1099, "seek": 453230, "start": 4548.3, "end": 4555.18, "text": " Why would you create public notebooks on kaggle well", "tokens": [1545, 576, 291, 1884, 1908, 43782, 322, 350, 559, 22631, 731], "temperature": 0.0, "avg_logprob": -0.35575616359710693, "compression_ratio": 1.2844827586206897, "no_speech_prob": 1.384221286571119e-05}, {"id": 1100, "seek": 453230, "start": 4557.26, "end": 4559.26, "text": " It's the same", "tokens": [467, 311, 264, 912], "temperature": 0.0, "avg_logprob": -0.35575616359710693, "compression_ratio": 1.2844827586206897, "no_speech_prob": 1.384221286571119e-05}, {"id": 1101, "seek": 453230, "start": 4559.26, "end": 4560.3, "text": " Brutal", "tokens": [1603, 325, 304], "temperature": 0.0, "avg_logprob": -0.35575616359710693, "compression_ratio": 1.2844827586206897, "no_speech_prob": 1.384221286571119e-05}, {"id": 1102, "seek": 456030, "start": 4560.3, "end": 4563.58, "text": " It's the same brutality of feedback", "tokens": [467, 311, 264, 912, 41745, 295, 5824], "temperature": 0.0, "avg_logprob": -0.12164593696594238, "compression_ratio": 1.7894736842105263, "no_speech_prob": 1.7775275409803726e-05}, {"id": 1103, "seek": 456030, "start": 4564.38, "end": 4566.38, "text": " That you get for entering a competition", "tokens": [663, 291, 483, 337, 11104, 257, 6211], "temperature": 0.0, "avg_logprob": -0.12164593696594238, "compression_ratio": 1.7894736842105263, "no_speech_prob": 1.7775275409803726e-05}, {"id": 1104, "seek": 456030, "start": 4567.18, "end": 4569.18, "text": " But this time rather than finding out", "tokens": [583, 341, 565, 2831, 813, 5006, 484], "temperature": 0.0, "avg_logprob": -0.12164593696594238, "compression_ratio": 1.7894736842105263, "no_speech_prob": 1.7775275409803726e-05}, {"id": 1105, "seek": 456030, "start": 4569.74, "end": 4573.02, "text": " In no uncertain terms whether you can predict things accurately", "tokens": [682, 572, 11308, 2115, 1968, 291, 393, 6069, 721, 20095], "temperature": 0.0, "avg_logprob": -0.12164593696594238, "compression_ratio": 1.7894736842105263, "no_speech_prob": 1.7775275409803726e-05}, {"id": 1106, "seek": 456030, "start": 4573.820000000001, "end": 4579.900000000001, "text": " This time you can find out no uncertain terms whether you can communicate things in a way that people find interesting and useful", "tokens": [639, 565, 291, 393, 915, 484, 572, 11308, 2115, 1968, 291, 393, 7890, 721, 294, 257, 636, 300, 561, 915, 1880, 293, 4420], "temperature": 0.0, "avg_logprob": -0.12164593696594238, "compression_ratio": 1.7894736842105263, "no_speech_prob": 1.7775275409803726e-05}, {"id": 1107, "seek": 456030, "start": 4581.02, "end": 4583.02, "text": " And if you get zero votes", "tokens": [400, 498, 291, 483, 4018, 12068], "temperature": 0.0, "avg_logprob": -0.12164593696594238, "compression_ratio": 1.7894736842105263, "no_speech_prob": 1.7775275409803726e-05}, {"id": 1108, "seek": 458302, "start": 4583.02, "end": 4589.42, "text": " You know so be it right that's something to to know and then you know, ideally go and ask some friends like", "tokens": [509, 458, 370, 312, 309, 558, 300, 311, 746, 281, 281, 458, 293, 550, 291, 458, 11, 22915, 352, 293, 1029, 512, 1855, 411], "temperature": 0.0, "avg_logprob": -0.28876373654320125, "compression_ratio": 1.6540084388185654, "no_speech_prob": 9.222709195455536e-06}, {"id": 1109, "seek": 458302, "start": 4590.700000000001, "end": 4595.5, "text": " What do you think I could do to improve and if they say oh nothing it's fantastic you can tell no that's not true", "tokens": [708, 360, 291, 519, 286, 727, 360, 281, 3470, 293, 498, 436, 584, 1954, 1825, 309, 311, 5456, 291, 393, 980, 572, 300, 311, 406, 2074], "temperature": 0.0, "avg_logprob": -0.28876373654320125, "compression_ratio": 1.6540084388185654, "no_speech_prob": 9.222709195455536e-06}, {"id": 1110, "seek": 458302, "start": 4595.9800000000005, "end": 4600.22, "text": " I didn't get any votes try again. This isn't good. How do I make it better?", "tokens": [286, 994, 380, 483, 604, 12068, 853, 797, 13, 639, 1943, 380, 665, 13, 1012, 360, 286, 652, 309, 1101, 30], "temperature": 0.0, "avg_logprob": -0.28876373654320125, "compression_ratio": 1.6540084388185654, "no_speech_prob": 9.222709195455536e-06}, {"id": 1111, "seek": 458302, "start": 4600.780000000001, "end": 4602.780000000001, "text": " You know", "tokens": [509, 458], "temperature": 0.0, "avg_logprob": -0.28876373654320125, "compression_ratio": 1.6540084388185654, "no_speech_prob": 9.222709195455536e-06}, {"id": 1112, "seek": 458302, "start": 4602.780000000001, "end": 4604.780000000001, "text": " And you can try and improve", "tokens": [400, 291, 393, 853, 293, 3470], "temperature": 0.0, "avg_logprob": -0.28876373654320125, "compression_ratio": 1.6540084388185654, "no_speech_prob": 9.222709195455536e-06}, {"id": 1113, "seek": 458302, "start": 4605.18, "end": 4607.18, "text": " because", "tokens": [570], "temperature": 0.0, "avg_logprob": -0.28876373654320125, "compression_ratio": 1.6540084388185654, "no_speech_prob": 9.222709195455536e-06}, {"id": 1114, "seek": 458302, "start": 4607.18, "end": 4609.5, "text": " If you can create models that predict things well", "tokens": [759, 291, 393, 1884, 5245, 300, 6069, 721, 731], "temperature": 0.0, "avg_logprob": -0.28876373654320125, "compression_ratio": 1.6540084388185654, "no_speech_prob": 9.222709195455536e-06}, {"id": 1115, "seek": 460950, "start": 4609.5, "end": 4613.02, "text": " And you can communicate your results in a way that is clear and compelling", "tokens": [400, 291, 393, 7890, 428, 3542, 294, 257, 636, 300, 307, 1850, 293, 20050], "temperature": 0.0, "avg_logprob": -0.2963099604067595, "compression_ratio": 1.65, "no_speech_prob": 1.2803814570361283e-05}, {"id": 1116, "seek": 460950, "start": 4614.62, "end": 4620.46, "text": " You're a pretty good data scientist, you know, like they're they're two pretty important things and so here's a great way to", "tokens": [509, 434, 257, 1238, 665, 1412, 12662, 11, 291, 458, 11, 411, 436, 434, 436, 434, 732, 1238, 1021, 721, 293, 370, 510, 311, 257, 869, 636, 281], "temperature": 0.0, "avg_logprob": -0.2963099604067595, "compression_ratio": 1.65, "no_speech_prob": 1.2803814570361283e-05}, {"id": 1117, "seek": 460950, "start": 4621.42, "end": 4624.78, "text": " Test yourself out on those things and improve. Yes, john", "tokens": [9279, 1803, 484, 322, 729, 721, 293, 3470, 13, 1079, 11, 35097], "temperature": 0.0, "avg_logprob": -0.2963099604067595, "compression_ratio": 1.65, "no_speech_prob": 1.2803814570361283e-05}, {"id": 1118, "seek": 460950, "start": 4625.5, "end": 4632.06, "text": " Uh, yes, charme. We have a sort of a I think a timely question here from zakiya, um about your iterative approach", "tokens": [4019, 11, 2086, 11, 1290, 1398, 13, 492, 362, 257, 1333, 295, 257, 286, 519, 257, 25150, 1168, 510, 490, 710, 7421, 3016, 11, 1105, 466, 428, 17138, 1166, 3109], "temperature": 0.0, "avg_logprob": -0.2963099604067595, "compression_ratio": 1.65, "no_speech_prob": 1.2803814570361283e-05}, {"id": 1119, "seek": 460950, "start": 4633.26, "end": 4638.3, "text": " And they're asking do you create different kaggle notebooks for each model that you create?", "tokens": [400, 436, 434, 3365, 360, 291, 1884, 819, 350, 559, 22631, 43782, 337, 1184, 2316, 300, 291, 1884, 30], "temperature": 0.0, "avg_logprob": -0.2963099604067595, "compression_ratio": 1.65, "no_speech_prob": 1.2803814570361283e-05}, {"id": 1120, "seek": 463830, "start": 4638.3, "end": 4644.62, "text": " For each model that you try. Yeah, so one kaggle book for the first one then separate notebooks subsequently", "tokens": [1171, 1184, 2316, 300, 291, 853, 13, 865, 11, 370, 472, 350, 559, 22631, 1446, 337, 264, 700, 472, 550, 4994, 43782, 26514], "temperature": 0.0, "avg_logprob": -0.12227762596947807, "compression_ratio": 1.6436781609195403, "no_speech_prob": 5.255133146420121e-06}, {"id": 1121, "seek": 463830, "start": 4645.1, "end": 4649.820000000001, "text": " Or do you do append to the bottom of a single notebook? What's your strategy? That's a great question", "tokens": [1610, 360, 291, 360, 34116, 281, 264, 2767, 295, 257, 2167, 21060, 30, 708, 311, 428, 5206, 30, 663, 311, 257, 869, 1168], "temperature": 0.0, "avg_logprob": -0.12227762596947807, "compression_ratio": 1.6436781609195403, "no_speech_prob": 5.255133146420121e-06}, {"id": 1122, "seek": 463830, "start": 4651.1, "end": 4653.1, "text": " and I know zakiya is going through the", "tokens": [293, 286, 458, 710, 7421, 3016, 307, 516, 807, 264], "temperature": 0.0, "avg_logprob": -0.12227762596947807, "compression_ratio": 1.6436781609195403, "no_speech_prob": 5.255133146420121e-06}, {"id": 1123, "seek": 463830, "start": 4654.06, "end": 4658.9400000000005, "text": " The daily walkthroughs but isn't quite caught up yet. So um, I will say keep keep it up because um", "tokens": [440, 5212, 1792, 11529, 82, 457, 1943, 380, 1596, 5415, 493, 1939, 13, 407, 1105, 11, 286, 486, 584, 1066, 1066, 309, 493, 570, 1105], "temperature": 0.0, "avg_logprob": -0.12227762596947807, "compression_ratio": 1.6436781609195403, "no_speech_prob": 5.255133146420121e-06}, {"id": 1124, "seek": 463830, "start": 4659.5, "end": 4664.0, "text": " In the six hours of going through this you'll see me create all the notebooks", "tokens": [682, 264, 2309, 2496, 295, 516, 807, 341, 291, 603, 536, 385, 1884, 439, 264, 43782], "temperature": 0.0, "avg_logprob": -0.12227762596947807, "compression_ratio": 1.6436781609195403, "no_speech_prob": 5.255133146420121e-06}, {"id": 1125, "seek": 463830, "start": 4664.860000000001, "end": 4666.62, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.12227762596947807, "compression_ratio": 1.6436781609195403, "no_speech_prob": 5.255133146420121e-06}, {"id": 1126, "seek": 466662, "start": 4666.62, "end": 4668.62, "text": " but uh", "tokens": [457, 2232], "temperature": 0.0, "avg_logprob": -0.11567567785580952, "compression_ratio": 1.5700934579439252, "no_speech_prob": 1.3210955330578145e-05}, {"id": 1127, "seek": 466662, "start": 4668.86, "end": 4671.9, "text": " If I go to the actual directory I used", "tokens": [759, 286, 352, 281, 264, 3539, 21120, 286, 1143], "temperature": 0.0, "avg_logprob": -0.11567567785580952, "compression_ratio": 1.5700934579439252, "no_speech_prob": 1.3210955330578145e-05}, {"id": 1128, "seek": 466662, "start": 4674.38, "end": 4677.82, "text": " Uh, you can see them so basically yeah, I started with", "tokens": [4019, 11, 291, 393, 536, 552, 370, 1936, 1338, 11, 286, 1409, 365], "temperature": 0.0, "avg_logprob": -0.11567567785580952, "compression_ratio": 1.5700934579439252, "no_speech_prob": 1.3210955330578145e-05}, {"id": 1129, "seek": 466662, "start": 4679.18, "end": 4681.18, "text": " You know what you just saw", "tokens": [509, 458, 437, 291, 445, 1866], "temperature": 0.0, "avg_logprob": -0.11567567785580952, "compression_ratio": 1.5700934579439252, "no_speech_prob": 1.3210955330578145e-05}, {"id": 1130, "seek": 466662, "start": 4682.14, "end": 4686.46, "text": " A bit messier without the pros but that same basic thing. I then duplicated it", "tokens": [316, 857, 2082, 811, 1553, 264, 6267, 457, 300, 912, 3875, 551, 13, 286, 550, 1581, 564, 3587, 309], "temperature": 0.0, "avg_logprob": -0.11567567785580952, "compression_ratio": 1.5700934579439252, "no_speech_prob": 1.3210955330578145e-05}, {"id": 1131, "seek": 466662, "start": 4687.82, "end": 4689.82, "text": " To create the next one", "tokens": [1407, 1884, 264, 958, 472], "temperature": 0.0, "avg_logprob": -0.11567567785580952, "compression_ratio": 1.5700934579439252, "no_speech_prob": 1.3210955330578145e-05}, {"id": 1132, "seek": 466662, "start": 4690.54, "end": 4695.82, "text": " Which is here and because I duplicated it, you know this stuff which I still need it's still there, right?", "tokens": [3013, 307, 510, 293, 570, 286, 1581, 564, 3587, 309, 11, 291, 458, 341, 1507, 597, 286, 920, 643, 309, 311, 920, 456, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11567567785580952, "compression_ratio": 1.5700934579439252, "no_speech_prob": 1.3210955330578145e-05}, {"id": 1133, "seek": 469582, "start": 4695.82, "end": 4698.46, "text": " And so I I run it and um", "tokens": [400, 370, 286, 286, 1190, 309, 293, 1105], "temperature": 0.0, "avg_logprob": -0.08353424072265625, "compression_ratio": 1.7551020408163265, "no_speech_prob": 2.625784145493526e-05}, {"id": 1134, "seek": 469582, "start": 4699.179999999999, "end": 4701.0199999999995, "text": " I don't always know what i'm doing", "tokens": [286, 500, 380, 1009, 458, 437, 741, 478, 884], "temperature": 0.0, "avg_logprob": -0.08353424072265625, "compression_ratio": 1.7551020408163265, "no_speech_prob": 2.625784145493526e-05}, {"id": 1135, "seek": 469582, "start": 4701.0199999999995, "end": 4705.179999999999, "text": " You know, and so at first if I don't really know what i'm doing next when I duplicate it", "tokens": [509, 458, 11, 293, 370, 412, 700, 498, 286, 500, 380, 534, 458, 437, 741, 478, 884, 958, 562, 286, 23976, 309], "temperature": 0.0, "avg_logprob": -0.08353424072265625, "compression_ratio": 1.7551020408163265, "no_speech_prob": 2.625784145493526e-05}, {"id": 1136, "seek": 469582, "start": 4705.74, "end": 4711.099999999999, "text": " It will be called, you know first steps on the road to the top part one dash copy one", "tokens": [467, 486, 312, 1219, 11, 291, 458, 700, 4439, 322, 264, 3060, 281, 264, 1192, 644, 472, 8240, 5055, 472], "temperature": 0.0, "avg_logprob": -0.08353424072265625, "compression_ratio": 1.7551020408163265, "no_speech_prob": 2.625784145493526e-05}, {"id": 1137, "seek": 469582, "start": 4711.9, "end": 4713.9, "text": " You know and that's okay", "tokens": [509, 458, 293, 300, 311, 1392], "temperature": 0.0, "avg_logprob": -0.08353424072265625, "compression_ratio": 1.7551020408163265, "no_speech_prob": 2.625784145493526e-05}, {"id": 1138, "seek": 469582, "start": 4715.259999999999, "end": 4717.259999999999, "text": " And as soon as I can", "tokens": [400, 382, 2321, 382, 286, 393], "temperature": 0.0, "avg_logprob": -0.08353424072265625, "compression_ratio": 1.7551020408163265, "no_speech_prob": 2.625784145493526e-05}, {"id": 1139, "seek": 469582, "start": 4717.74, "end": 4719.74, "text": " I'll try to rename that", "tokens": [286, 603, 853, 281, 36741, 300], "temperature": 0.0, "avg_logprob": -0.08353424072265625, "compression_ratio": 1.7551020408163265, "no_speech_prob": 2.625784145493526e-05}, {"id": 1140, "seek": 469582, "start": 4719.9, "end": 4721.9, "text": " Once I know what i'm doing, you know", "tokens": [3443, 286, 458, 437, 741, 478, 884, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.08353424072265625, "compression_ratio": 1.7551020408163265, "no_speech_prob": 2.625784145493526e-05}, {"id": 1141, "seek": 469582, "start": 4722.38, "end": 4723.98, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.08353424072265625, "compression_ratio": 1.7551020408163265, "no_speech_prob": 2.625784145493526e-05}, {"id": 1142, "seek": 472398, "start": 4723.98, "end": 4727.0199999999995, "text": " or if it doesn't say to go anywhere I rename it into something like", "tokens": [420, 498, 309, 1177, 380, 584, 281, 352, 4992, 286, 36741, 309, 666, 746, 411], "temperature": 0.0, "avg_logprob": -0.16288800758890587, "compression_ratio": 1.6394849785407726, "no_speech_prob": 2.1441062926896848e-05}, {"id": 1143, "seek": 472398, "start": 4727.5, "end": 4728.78, "text": " you know", "tokens": [291, 458], "temperature": 0.0, "avg_logprob": -0.16288800758890587, "compression_ratio": 1.6394849785407726, "no_speech_prob": 2.1441062926896848e-05}, {"id": 1144, "seek": 472398, "start": 4728.78, "end": 4733.98, "text": " Experiment blah blah blah and I'll put some notes at the bottom and I might put it into a failed folder or something", "tokens": [37933, 12288, 12288, 12288, 293, 286, 603, 829, 512, 5570, 412, 264, 2767, 293, 286, 1062, 829, 309, 666, 257, 7612, 10820, 420, 746], "temperature": 0.0, "avg_logprob": -0.16288800758890587, "compression_ratio": 1.6394849785407726, "no_speech_prob": 2.1441062926896848e-05}, {"id": 1145, "seek": 472398, "start": 4734.0599999999995, "end": 4736.0599999999995, "text": " But yeah, it's like", "tokens": [583, 1338, 11, 309, 311, 411], "temperature": 0.0, "avg_logprob": -0.16288800758890587, "compression_ratio": 1.6394849785407726, "no_speech_prob": 2.1441062926896848e-05}, {"id": 1146, "seek": 472398, "start": 4736.379999999999, "end": 4738.78, "text": " It's a very low tech", "tokens": [467, 311, 257, 588, 2295, 7553], "temperature": 0.0, "avg_logprob": -0.16288800758890587, "compression_ratio": 1.6394849785407726, "no_speech_prob": 2.1441062926896848e-05}, {"id": 1147, "seek": 472398, "start": 4739.9, "end": 4741.339999999999, "text": " approach", "tokens": [3109], "temperature": 0.0, "avg_logprob": -0.16288800758890587, "compression_ratio": 1.6394849785407726, "no_speech_prob": 2.1441062926896848e-05}, {"id": 1148, "seek": 472398, "start": 4741.339999999999, "end": 4743.82, "text": " That I find works really well, which is just", "tokens": [663, 286, 915, 1985, 534, 731, 11, 597, 307, 445], "temperature": 0.0, "avg_logprob": -0.16288800758890587, "compression_ratio": 1.6394849785407726, "no_speech_prob": 2.1441062926896848e-05}, {"id": 1149, "seek": 472398, "start": 4744.599999999999, "end": 4749.0199999999995, "text": " Duplicating notebooks and editing them and naming them carefully and putting them in order", "tokens": [5153, 4770, 990, 43782, 293, 10000, 552, 293, 25290, 552, 7500, 293, 3372, 552, 294, 1668], "temperature": 0.0, "avg_logprob": -0.16288800758890587, "compression_ratio": 1.6394849785407726, "no_speech_prob": 2.1441062926896848e-05}, {"id": 1150, "seek": 472398, "start": 4749.74, "end": 4751.5, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.16288800758890587, "compression_ratio": 1.6394849785407726, "no_speech_prob": 2.1441062926896848e-05}, {"id": 1151, "seek": 475150, "start": 4751.5, "end": 4755.02, "text": " And you know put the file name in when you submit as well", "tokens": [400, 291, 458, 829, 264, 3991, 1315, 294, 562, 291, 10315, 382, 731], "temperature": 0.0, "avg_logprob": -0.0747022297071374, "compression_ratio": 1.7096774193548387, "no_speech_prob": 1.61855059559457e-05}, {"id": 1152, "seek": 475150, "start": 4755.9, "end": 4757.58, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.0747022297071374, "compression_ratio": 1.7096774193548387, "no_speech_prob": 1.61855059559457e-05}, {"id": 1153, "seek": 475150, "start": 4757.58, "end": 4760.3, "text": " And then of course also if you've got things in git, you know", "tokens": [400, 550, 295, 1164, 611, 498, 291, 600, 658, 721, 294, 18331, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.0747022297071374, "compression_ratio": 1.7096774193548387, "no_speech_prob": 1.61855059559457e-05}, {"id": 1154, "seek": 475150, "start": 4760.3, "end": 4763.58, "text": " You can have a link to the git commit so you'll know exactly what it is", "tokens": [509, 393, 362, 257, 2113, 281, 264, 18331, 5599, 370, 291, 603, 458, 2293, 437, 309, 307], "temperature": 0.0, "avg_logprob": -0.0747022297071374, "compression_ratio": 1.7096774193548387, "no_speech_prob": 1.61855059559457e-05}, {"id": 1155, "seek": 475150, "start": 4764.06, "end": 4767.66, "text": " Generally speaking for me, you know, my notebooks will only have one submission in", "tokens": [21082, 4124, 337, 385, 11, 291, 458, 11, 452, 43782, 486, 787, 362, 472, 23689, 294], "temperature": 0.0, "avg_logprob": -0.0747022297071374, "compression_ratio": 1.7096774193548387, "no_speech_prob": 1.61855059559457e-05}, {"id": 1156, "seek": 475150, "start": 4768.3, "end": 4772.7, "text": " And then i'll move on and create a new notebook. So I don't really worry about versioning so much", "tokens": [400, 550, 741, 603, 1286, 322, 293, 1884, 257, 777, 21060, 13, 407, 286, 500, 380, 534, 3292, 466, 3037, 278, 370, 709], "temperature": 0.0, "avg_logprob": -0.0747022297071374, "compression_ratio": 1.7096774193548387, "no_speech_prob": 1.61855059559457e-05}, {"id": 1157, "seek": 475150, "start": 4773.58, "end": 4774.7, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.0747022297071374, "compression_ratio": 1.7096774193548387, "no_speech_prob": 1.61855059559457e-05}, {"id": 1158, "seek": 475150, "start": 4774.7, "end": 4776.7, "text": " But you can do that as well if that helps you", "tokens": [583, 291, 393, 360, 300, 382, 731, 498, 300, 3665, 291], "temperature": 0.0, "avg_logprob": -0.0747022297071374, "compression_ratio": 1.7096774193548387, "no_speech_prob": 1.61855059559457e-05}, {"id": 1159, "seek": 477670, "start": 4776.7, "end": 4781.42, "text": " Um, yeah, so that's basically what I do and and and", "tokens": [3301, 11, 1338, 11, 370, 300, 311, 1936, 437, 286, 360, 293, 293, 293], "temperature": 0.0, "avg_logprob": -0.09369769303695015, "compression_ratio": 1.559090909090909, "no_speech_prob": 1.7502026821603067e-05}, {"id": 1160, "seek": 477670, "start": 4782.54, "end": 4788.0599999999995, "text": " I've worked with a lot of people who use much more sophisticated and complex processes and tools and stuff, but", "tokens": [286, 600, 2732, 365, 257, 688, 295, 561, 567, 764, 709, 544, 16950, 293, 3997, 7555, 293, 3873, 293, 1507, 11, 457], "temperature": 0.0, "avg_logprob": -0.09369769303695015, "compression_ratio": 1.559090909090909, "no_speech_prob": 1.7502026821603067e-05}, {"id": 1161, "seek": 477670, "start": 4789.9, "end": 4793.26, "text": " None of them seem to be able to stay as well organized as I am", "tokens": [14492, 295, 552, 1643, 281, 312, 1075, 281, 1754, 382, 731, 9983, 382, 286, 669], "temperature": 0.0, "avg_logprob": -0.09369769303695015, "compression_ratio": 1.559090909090909, "no_speech_prob": 1.7502026821603067e-05}, {"id": 1162, "seek": 477670, "start": 4793.34, "end": 4796.0599999999995, "text": " I think they kind of get a bit lost in their tools sometimes", "tokens": [286, 519, 436, 733, 295, 483, 257, 857, 2731, 294, 641, 3873, 2171], "temperature": 0.0, "avg_logprob": -0.09369769303695015, "compression_ratio": 1.559090909090909, "no_speech_prob": 1.7502026821603067e-05}, {"id": 1163, "seek": 477670, "start": 4796.46, "end": 4797.98, "text": " um and", "tokens": [1105, 293], "temperature": 0.0, "avg_logprob": -0.09369769303695015, "compression_ratio": 1.559090909090909, "no_speech_prob": 1.7502026821603067e-05}, {"id": 1164, "seek": 477670, "start": 4797.98, "end": 4799.98, "text": " File systems and file names I think are", "tokens": [26196, 3652, 293, 3991, 5288, 286, 519, 366], "temperature": 0.0, "avg_logprob": -0.09369769303695015, "compression_ratio": 1.559090909090909, "no_speech_prob": 1.7502026821603067e-05}, {"id": 1165, "seek": 477670, "start": 4800.62, "end": 4802.62, "text": " Are good", "tokens": [2014, 665], "temperature": 0.0, "avg_logprob": -0.09369769303695015, "compression_ratio": 1.559090909090909, "no_speech_prob": 1.7502026821603067e-05}, {"id": 1166, "seek": 480262, "start": 4802.62, "end": 4806.3, "text": " Um, great. Thanks. Um, so away from that kind of", "tokens": [3301, 11, 869, 13, 2561, 13, 3301, 11, 370, 1314, 490, 300, 733, 295], "temperature": 0.0, "avg_logprob": -0.1526501304224918, "compression_ratio": 1.722007722007722, "no_speech_prob": 3.0237764804041944e-05}, {"id": 1167, "seek": 480262, "start": 4807.42, "end": 4809.42, "text": " dev process more towards the", "tokens": [1905, 1399, 544, 3030, 264], "temperature": 0.0, "avg_logprob": -0.1526501304224918, "compression_ratio": 1.722007722007722, "no_speech_prob": 3.0237764804041944e-05}, {"id": 1168, "seek": 480262, "start": 4810.62, "end": 4814.0599999999995, "text": " The specifics of you know, finding the best model and all that sort of stuff", "tokens": [440, 28454, 295, 291, 458, 11, 5006, 264, 1151, 2316, 293, 439, 300, 1333, 295, 1507], "temperature": 0.0, "avg_logprob": -0.1526501304224918, "compression_ratio": 1.722007722007722, "no_speech_prob": 3.0237764804041944e-05}, {"id": 1169, "seek": 480262, "start": 4814.0599999999995, "end": 4817.26, "text": " we've got a couple of questions that are in the same space, which is", "tokens": [321, 600, 658, 257, 1916, 295, 1651, 300, 366, 294, 264, 912, 1901, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.1526501304224918, "compression_ratio": 1.722007722007722, "no_speech_prob": 3.0237764804041944e-05}, {"id": 1170, "seek": 480262, "start": 4818.38, "end": 4824.14, "text": " You know, we've got some people here talking about auto ml frameworks, which you might want to you know touch on for people who haven't heard of those", "tokens": [509, 458, 11, 321, 600, 658, 512, 561, 510, 1417, 466, 8399, 23271, 29834, 11, 597, 291, 1062, 528, 281, 291, 458, 2557, 322, 337, 561, 567, 2378, 380, 2198, 295, 729], "temperature": 0.0, "avg_logprob": -0.1526501304224918, "compression_ratio": 1.722007722007722, "no_speech_prob": 3.0237764804041944e-05}, {"id": 1171, "seek": 480262, "start": 4824.7, "end": 4827.74, "text": " um, if you've got any particular auto ml frameworks you think are", "tokens": [1105, 11, 498, 291, 600, 658, 604, 1729, 8399, 23271, 29834, 291, 519, 366], "temperature": 0.0, "avg_logprob": -0.1526501304224918, "compression_ratio": 1.722007722007722, "no_speech_prob": 3.0237764804041944e-05}, {"id": 1172, "seek": 480262, "start": 4828.86, "end": 4830.2, "text": " worth", "tokens": [3163], "temperature": 0.0, "avg_logprob": -0.1526501304224918, "compression_ratio": 1.722007722007722, "no_speech_prob": 3.0237764804041944e-05}, {"id": 1173, "seek": 483020, "start": 4830.2, "end": 4836.86, "text": " Recommending or just more generally, how do you go trying different models random forest gradient boosting neural network?", "tokens": [49545, 2029, 420, 445, 544, 5101, 11, 577, 360, 291, 352, 1382, 819, 5245, 4974, 6719, 16235, 43117, 18161, 3209, 30], "temperature": 0.0, "avg_logprob": -0.19877759071245585, "compression_ratio": 1.471794871794872, "no_speech_prob": 6.4386554186057765e-06}, {"id": 1174, "seek": 483020, "start": 4837.98, "end": 4840.139999999999, "text": " So in that space if you couldn't comment sure", "tokens": [407, 294, 300, 1901, 498, 291, 2809, 380, 2871, 988], "temperature": 0.0, "avg_logprob": -0.19877759071245585, "compression_ratio": 1.471794871794872, "no_speech_prob": 6.4386554186057765e-06}, {"id": 1175, "seek": 483020, "start": 4840.78, "end": 4842.78, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.19877759071245585, "compression_ratio": 1.471794871794872, "no_speech_prob": 6.4386554186057765e-06}, {"id": 1176, "seek": 483020, "start": 4844.7, "end": 4848.0599999999995, "text": " I use auto ml less than anybody I know I would guess", "tokens": [286, 764, 8399, 23271, 1570, 813, 4472, 286, 458, 286, 576, 2041], "temperature": 0.0, "avg_logprob": -0.19877759071245585, "compression_ratio": 1.471794871794872, "no_speech_prob": 6.4386554186057765e-06}, {"id": 1177, "seek": 483020, "start": 4849.179999999999, "end": 4851.179999999999, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.19877759071245585, "compression_ratio": 1.471794871794872, "no_speech_prob": 6.4386554186057765e-06}, {"id": 1178, "seek": 483020, "start": 4851.42, "end": 4853.42, "text": " Which is to say never", "tokens": [3013, 307, 281, 584, 1128], "temperature": 0.0, "avg_logprob": -0.19877759071245585, "compression_ratio": 1.471794871794872, "no_speech_prob": 6.4386554186057765e-06}, {"id": 1179, "seek": 483020, "start": 4854.38, "end": 4856.0599999999995, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.19877759071245585, "compression_ratio": 1.471794871794872, "no_speech_prob": 6.4386554186057765e-06}, {"id": 1180, "seek": 483020, "start": 4856.0599999999995, "end": 4858.7, "text": " Hyper parameter optimization never", "tokens": [29592, 13075, 19618, 1128], "temperature": 0.0, "avg_logprob": -0.19877759071245585, "compression_ratio": 1.471794871794872, "no_speech_prob": 6.4386554186057765e-06}, {"id": 1181, "seek": 485870, "start": 4858.7, "end": 4860.7, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.13137007363234895, "compression_ratio": 1.5449735449735449, "no_speech_prob": 1.8447428828949342e-06}, {"id": 1182, "seek": 485870, "start": 4861.98, "end": 4863.98, "text": " And the reason why", "tokens": [400, 264, 1778, 983], "temperature": 0.0, "avg_logprob": -0.13137007363234895, "compression_ratio": 1.5449735449735449, "no_speech_prob": 1.8447428828949342e-06}, {"id": 1183, "seek": 485870, "start": 4864.46, "end": 4869.34, "text": " Is I like being highly intentional, you know, I like to think more like a scientist", "tokens": [1119, 286, 411, 885, 5405, 21935, 11, 291, 458, 11, 286, 411, 281, 519, 544, 411, 257, 12662], "temperature": 0.0, "avg_logprob": -0.13137007363234895, "compression_ratio": 1.5449735449735449, "no_speech_prob": 1.8447428828949342e-06}, {"id": 1184, "seek": 485870, "start": 4870.0599999999995, "end": 4871.98, "text": " And have hypotheses", "tokens": [400, 362, 49969], "temperature": 0.0, "avg_logprob": -0.13137007363234895, "compression_ratio": 1.5449735449735449, "no_speech_prob": 1.8447428828949342e-06}, {"id": 1185, "seek": 485870, "start": 4871.98, "end": 4873.98, "text": " And test them carefully", "tokens": [400, 1500, 552, 7500], "temperature": 0.0, "avg_logprob": -0.13137007363234895, "compression_ratio": 1.5449735449735449, "no_speech_prob": 1.8447428828949342e-06}, {"id": 1186, "seek": 485870, "start": 4875.099999999999, "end": 4879.98, "text": " And come up with conclusions which then I implement, you know, so for example", "tokens": [400, 808, 493, 365, 22865, 597, 550, 286, 4445, 11, 291, 458, 11, 370, 337, 1365], "temperature": 0.0, "avg_logprob": -0.13137007363234895, "compression_ratio": 1.5449735449735449, "no_speech_prob": 1.8447428828949342e-06}, {"id": 1187, "seek": 485870, "start": 4881.0199999999995, "end": 4883.5, "text": " um in this best vision models of fine tuning", "tokens": [1105, 294, 341, 1151, 5201, 5245, 295, 2489, 15164], "temperature": 0.0, "avg_logprob": -0.13137007363234895, "compression_ratio": 1.5449735449735449, "no_speech_prob": 1.8447428828949342e-06}, {"id": 1188, "seek": 485870, "start": 4885.099999999999, "end": 4887.099999999999, "text": " I didn't try a huge", "tokens": [286, 994, 380, 853, 257, 2603], "temperature": 0.0, "avg_logprob": -0.13137007363234895, "compression_ratio": 1.5449735449735449, "no_speech_prob": 1.8447428828949342e-06}, {"id": 1189, "seek": 488710, "start": 4887.1, "end": 4889.18, "text": " grid search of every possible", "tokens": [10748, 3164, 295, 633, 1944], "temperature": 0.0, "avg_logprob": -0.11266456180148654, "compression_ratio": 1.7023255813953488, "no_speech_prob": 2.769287902992801e-06}, {"id": 1190, "seek": 488710, "start": 4890.3, "end": 4895.820000000001, "text": " Model every possible learning rate every possible pre-processing approach blah blah blah, right instead step one", "tokens": [17105, 633, 1944, 2539, 3314, 633, 1944, 659, 12, 41075, 278, 3109, 12288, 12288, 12288, 11, 558, 2602, 1823, 472], "temperature": 0.0, "avg_logprob": -0.11266456180148654, "compression_ratio": 1.7023255813953488, "no_speech_prob": 2.769287902992801e-06}, {"id": 1191, "seek": 488710, "start": 4896.46, "end": 4897.9800000000005, "text": " Was to find out", "tokens": [3027, 281, 915, 484], "temperature": 0.0, "avg_logprob": -0.11266456180148654, "compression_ratio": 1.7023255813953488, "no_speech_prob": 2.769287902992801e-06}, {"id": 1192, "seek": 488710, "start": 4897.9800000000005, "end": 4899.9800000000005, "text": " Well, which things matter, right?", "tokens": [1042, 11, 597, 721, 1871, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11266456180148654, "compression_ratio": 1.7023255813953488, "no_speech_prob": 2.769287902992801e-06}, {"id": 1193, "seek": 488710, "start": 4900.3, "end": 4902.3, "text": " so um", "tokens": [370, 1105], "temperature": 0.0, "avg_logprob": -0.11266456180148654, "compression_ratio": 1.7023255813953488, "no_speech_prob": 2.769287902992801e-06}, {"id": 1194, "seek": 488710, "start": 4903.740000000001, "end": 4906.14, "text": " For example does whether we squish or crop", "tokens": [1171, 1365, 775, 1968, 321, 31379, 420, 9086], "temperature": 0.0, "avg_logprob": -0.11266456180148654, "compression_ratio": 1.7023255813953488, "no_speech_prob": 2.769287902992801e-06}, {"id": 1195, "seek": 488710, "start": 4907.900000000001, "end": 4912.620000000001, "text": " Make a difference, you know are some models better with squished and some models better with crop", "tokens": [4387, 257, 2649, 11, 291, 458, 366, 512, 5245, 1101, 365, 2339, 4729, 293, 512, 5245, 1101, 365, 9086], "temperature": 0.0, "avg_logprob": -0.11266456180148654, "compression_ratio": 1.7023255813953488, "no_speech_prob": 2.769287902992801e-06}, {"id": 1196, "seek": 488710, "start": 4913.820000000001, "end": 4915.820000000001, "text": " And so we just tested that", "tokens": [400, 370, 321, 445, 8246, 300], "temperature": 0.0, "avg_logprob": -0.11266456180148654, "compression_ratio": 1.7023255813953488, "no_speech_prob": 2.769287902992801e-06}, {"id": 1197, "seek": 491582, "start": 4915.82, "end": 4917.259999999999, "text": " for", "tokens": [337], "temperature": 0.0, "avg_logprob": -0.07626019237197448, "compression_ratio": 1.679245283018868, "no_speech_prob": 3.3929725304915337e-06}, {"id": 1198, "seek": 491582, "start": 4917.259999999999, "end": 4919.179999999999, "text": " Again, not for every possible architecture", "tokens": [3764, 11, 406, 337, 633, 1944, 9482], "temperature": 0.0, "avg_logprob": -0.07626019237197448, "compression_ratio": 1.679245283018868, "no_speech_prob": 3.3929725304915337e-06}, {"id": 1199, "seek": 491582, "start": 4919.179999999999, "end": 4925.74, "text": " But for one or two versions of each of the main families that took 20 minutes and the answer was no in every single case", "tokens": [583, 337, 472, 420, 732, 9606, 295, 1184, 295, 264, 2135, 4466, 300, 1890, 945, 2077, 293, 264, 1867, 390, 572, 294, 633, 2167, 1389], "temperature": 0.0, "avg_logprob": -0.07626019237197448, "compression_ratio": 1.679245283018868, "no_speech_prob": 3.3929725304915337e-06}, {"id": 1200, "seek": 491582, "start": 4926.94, "end": 4931.82, "text": " The same thing was better. So we don't need to do a grid search over that anymore, you know", "tokens": [440, 912, 551, 390, 1101, 13, 407, 321, 500, 380, 643, 281, 360, 257, 10748, 3164, 670, 300, 3602, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.07626019237197448, "compression_ratio": 1.679245283018868, "no_speech_prob": 3.3929725304915337e-06}, {"id": 1201, "seek": 491582, "start": 4932.38, "end": 4935.66, "text": " Or another classic one is like learning rates. Most people", "tokens": [1610, 1071, 7230, 472, 307, 411, 2539, 6846, 13, 4534, 561], "temperature": 0.0, "avg_logprob": -0.07626019237197448, "compression_ratio": 1.679245283018868, "no_speech_prob": 3.3929725304915337e-06}, {"id": 1202, "seek": 491582, "start": 4937.0199999999995, "end": 4941.74, "text": " Do a kind of grid search over learning rates or they'll train a thousand models, you know with different learning rates", "tokens": [1144, 257, 733, 295, 10748, 3164, 670, 2539, 6846, 420, 436, 603, 3847, 257, 4714, 5245, 11, 291, 458, 365, 819, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.07626019237197448, "compression_ratio": 1.679245283018868, "no_speech_prob": 3.3929725304915337e-06}, {"id": 1203, "seek": 491582, "start": 4942.78, "end": 4943.82, "text": " but um", "tokens": [457, 1105], "temperature": 0.0, "avg_logprob": -0.07626019237197448, "compression_ratio": 1.679245283018868, "no_speech_prob": 3.3929725304915337e-06}, {"id": 1204, "seek": 494382, "start": 4943.82, "end": 4947.82, "text": " This fantastic researcher named. Leslie smith invented the learning rate finder a few years ago", "tokens": [639, 5456, 21751, 4926, 13, 28140, 899, 355, 14479, 264, 2539, 3314, 915, 260, 257, 1326, 924, 2057], "temperature": 0.0, "avg_logprob": -0.11464591602702717, "compression_ratio": 1.5304347826086957, "no_speech_prob": 2.5864370400086045e-05}, {"id": 1205, "seek": 494382, "start": 4948.86, "end": 4951.98, "text": " We implemented it. I think within days of it first", "tokens": [492, 12270, 309, 13, 286, 519, 1951, 1708, 295, 309, 700], "temperature": 0.0, "avg_logprob": -0.11464591602702717, "compression_ratio": 1.5304347826086957, "no_speech_prob": 2.5864370400086045e-05}, {"id": 1206, "seek": 494382, "start": 4952.62, "end": 4955.74, "text": " Coming out as a technical report and that's what i've used ever since", "tokens": [12473, 484, 382, 257, 6191, 2275, 293, 300, 311, 437, 741, 600, 1143, 1562, 1670], "temperature": 0.0, "avg_logprob": -0.11464591602702717, "compression_ratio": 1.5304347826086957, "no_speech_prob": 2.5864370400086045e-05}, {"id": 1207, "seek": 494382, "start": 4956.46, "end": 4958.46, "text": " Because it works", "tokens": [1436, 309, 1985], "temperature": 0.0, "avg_logprob": -0.11464591602702717, "compression_ratio": 1.5304347826086957, "no_speech_prob": 2.5864370400086045e-05}, {"id": 1208, "seek": 494382, "start": 4958.62, "end": 4962.219999999999, "text": " Well and runs in a minute or so", "tokens": [1042, 293, 6676, 294, 257, 3456, 420, 370], "temperature": 0.0, "avg_logprob": -0.11464591602702717, "compression_ratio": 1.5304347826086957, "no_speech_prob": 2.5864370400086045e-05}, {"id": 1209, "seek": 494382, "start": 4963.58, "end": 4965.58, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.11464591602702717, "compression_ratio": 1.5304347826086957, "no_speech_prob": 2.5864370400086045e-05}, {"id": 1210, "seek": 496558, "start": 4965.58, "end": 4971.9, "text": " Yeah, I mean and then like neural nets versus gbms versus random forests, I mean that's", "tokens": [865, 11, 286, 914, 293, 550, 411, 18161, 36170, 5717, 290, 65, 2592, 5717, 4974, 21700, 11, 286, 914, 300, 311], "temperature": 0.0, "avg_logprob": -0.2815684700012207, "compression_ratio": 1.6909871244635193, "no_speech_prob": 2.5864021154120564e-05}, {"id": 1211, "seek": 496558, "start": 4972.86, "end": 4977.9, "text": " That shouldn't be too much of a question on the whole like they have pretty clear", "tokens": [663, 4659, 380, 312, 886, 709, 295, 257, 1168, 322, 264, 1379, 411, 436, 362, 1238, 1850], "temperature": 0.0, "avg_logprob": -0.2815684700012207, "compression_ratio": 1.6909871244635193, "no_speech_prob": 2.5864021154120564e-05}, {"id": 1212, "seek": 496558, "start": 4979.18, "end": 4981.18, "text": " Places that they go", "tokens": [2149, 2116, 300, 436, 352], "temperature": 0.0, "avg_logprob": -0.2815684700012207, "compression_ratio": 1.6909871244635193, "no_speech_prob": 2.5864021154120564e-05}, {"id": 1213, "seek": 496558, "start": 4981.58, "end": 4983.58, "text": " um like", "tokens": [1105, 411], "temperature": 0.0, "avg_logprob": -0.2815684700012207, "compression_ratio": 1.6909871244635193, "no_speech_prob": 2.5864021154120564e-05}, {"id": 1214, "seek": 496558, "start": 4984.62, "end": 4989.5, "text": " If i'm doing computer vision, i'm obviously going to use a computer vision deep learning model", "tokens": [759, 741, 478, 884, 3820, 5201, 11, 741, 478, 2745, 516, 281, 764, 257, 3820, 5201, 2452, 2539, 2316], "temperature": 0.0, "avg_logprob": -0.2815684700012207, "compression_ratio": 1.6909871244635193, "no_speech_prob": 2.5864021154120564e-05}, {"id": 1215, "seek": 496558, "start": 4990.0599999999995, "end": 4994.7, "text": " Um, and which one I would use well if i'm transfer learning which hopefully is always I would look at", "tokens": [3301, 11, 293, 597, 472, 286, 576, 764, 731, 498, 741, 478, 5003, 2539, 597, 4696, 307, 1009, 286, 576, 574, 412], "temperature": 0.0, "avg_logprob": -0.2815684700012207, "compression_ratio": 1.6909871244635193, "no_speech_prob": 2.5864021154120564e-05}, {"id": 1216, "seek": 499470, "start": 4994.7, "end": 5002.0599999999995, "text": " The two tables here. This is my table for pets, which is which are the best at fine tuning to very similar things to what they're pre-trained on", "tokens": [440, 732, 8020, 510, 13, 639, 307, 452, 3199, 337, 19897, 11, 597, 307, 597, 366, 264, 1151, 412, 2489, 15164, 281, 588, 2531, 721, 281, 437, 436, 434, 659, 12, 17227, 2001, 322], "temperature": 0.0, "avg_logprob": -0.24567083275836446, "compression_ratio": 1.8015873015873016, "no_speech_prob": 1.0450370609760284e-05}, {"id": 1217, "seek": 499470, "start": 5002.78, "end": 5004.78, "text": " And then the same thing for planet", "tokens": [400, 550, 264, 912, 551, 337, 5054], "temperature": 0.0, "avg_logprob": -0.24567083275836446, "compression_ratio": 1.8015873015873016, "no_speech_prob": 1.0450370609760284e-05}, {"id": 1218, "seek": 499470, "start": 5005.98, "end": 5010.38, "text": " Is which ones are best for fine tuning for two data sets that are very different to what they're trained on", "tokens": [1119, 597, 2306, 366, 1151, 337, 2489, 15164, 337, 732, 1412, 6352, 300, 366, 588, 819, 281, 437, 436, 434, 8895, 322], "temperature": 0.0, "avg_logprob": -0.24567083275836446, "compression_ratio": 1.8015873015873016, "no_speech_prob": 1.0450370609760284e-05}, {"id": 1219, "seek": 499470, "start": 5011.0199999999995, "end": 5013.0199999999995, "text": " And as it happens in both case, they're very similar", "tokens": [400, 382, 309, 2314, 294, 1293, 1389, 11, 436, 434, 588, 2531], "temperature": 0.0, "avg_logprob": -0.24567083275836446, "compression_ratio": 1.8015873015873016, "no_speech_prob": 1.0450370609760284e-05}, {"id": 1220, "seek": 499470, "start": 5013.58, "end": 5017.34, "text": " In particular convex is right up towards the top in both cases", "tokens": [682, 1729, 42432, 307, 558, 493, 3030, 264, 1192, 294, 1293, 3331], "temperature": 0.0, "avg_logprob": -0.24567083275836446, "compression_ratio": 1.8015873015873016, "no_speech_prob": 1.0450370609760284e-05}, {"id": 1221, "seek": 499470, "start": 5018.46, "end": 5020.46, "text": " So I just like to have these rules of thumb", "tokens": [407, 286, 445, 411, 281, 362, 613, 4474, 295, 9298], "temperature": 0.0, "avg_logprob": -0.24567083275836446, "compression_ratio": 1.8015873015873016, "no_speech_prob": 1.0450370609760284e-05}, {"id": 1222, "seek": 499470, "start": 5020.78, "end": 5022.38, "text": " and um", "tokens": [293, 1105], "temperature": 0.0, "avg_logprob": -0.24567083275836446, "compression_ratio": 1.8015873015873016, "no_speech_prob": 1.0450370609760284e-05}, {"id": 1223, "seek": 502238, "start": 5022.38, "end": 5024.7, "text": " Yeah, my rule of thumb for tabular is", "tokens": [865, 11, 452, 4978, 295, 9298, 337, 4421, 1040, 307], "temperature": 0.0, "avg_logprob": -0.287218325065844, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.0782715435198043e-05}, {"id": 1224, "seek": 502238, "start": 5025.02, "end": 5029.02, "text": " Random forests is going to be the fastest easiest way to get a pretty good result gbms", "tokens": [37603, 21700, 307, 516, 281, 312, 264, 14573, 12889, 636, 281, 483, 257, 1238, 665, 1874, 290, 65, 2592], "temperature": 0.0, "avg_logprob": -0.287218325065844, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.0782715435198043e-05}, {"id": 1225, "seek": 502238, "start": 5029.9800000000005, "end": 5034.9400000000005, "text": " Probably going to give me a slightly better result if I need it and can be bothered fussing around", "tokens": [9210, 516, 281, 976, 385, 257, 4748, 1101, 1874, 498, 286, 643, 309, 293, 393, 312, 22996, 34792, 278, 926], "temperature": 0.0, "avg_logprob": -0.287218325065844, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.0782715435198043e-05}, {"id": 1226, "seek": 502238, "start": 5035.26, "end": 5037.26, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.287218325065844, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.0782715435198043e-05}, {"id": 1227, "seek": 502238, "start": 5038.14, "end": 5043.82, "text": " Gbm I would probably yeah, actually I probably would run a hyper parameter", "tokens": [460, 65, 76, 286, 576, 1391, 1338, 11, 767, 286, 1391, 576, 1190, 257, 9848, 13075], "temperature": 0.0, "avg_logprob": -0.287218325065844, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.0782715435198043e-05}, {"id": 1228, "seek": 502238, "start": 5044.54, "end": 5045.66, "text": " sweep", "tokens": [22169], "temperature": 0.0, "avg_logprob": -0.287218325065844, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.0782715435198043e-05}, {"id": 1229, "seek": 502238, "start": 5045.66, "end": 5049.42, "text": " Because it is fiddly and and it's fast so you may as well", "tokens": [1436, 309, 307, 283, 14273, 356, 293, 293, 309, 311, 2370, 370, 291, 815, 382, 731], "temperature": 0.0, "avg_logprob": -0.287218325065844, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.0782715435198043e-05}, {"id": 1230, "seek": 504942, "start": 5049.42, "end": 5055.74, "text": " Um, so yeah, so now so, you know, we were now going to make a slightly better submission a slightly better model", "tokens": [3301, 11, 370, 1338, 11, 370, 586, 370, 11, 291, 458, 11, 321, 645, 586, 516, 281, 652, 257, 4748, 1101, 23689, 257, 4748, 1101, 2316], "temperature": 0.0, "avg_logprob": -0.5894006093343099, "compression_ratio": 1.5691489361702127, "no_speech_prob": 9.81560970103601e-06}, {"id": 1231, "seek": 504942, "start": 5056.78, "end": 5058.3, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.5894006093343099, "compression_ratio": 1.5691489361702127, "no_speech_prob": 9.81560970103601e-06}, {"id": 1232, "seek": 504942, "start": 5058.3, "end": 5060.3, "text": " and so", "tokens": [293, 370], "temperature": 0.0, "avg_logprob": -0.5894006093343099, "compression_ratio": 1.5691489361702127, "no_speech_prob": 9.81560970103601e-06}, {"id": 1233, "seek": 504942, "start": 5060.86, "end": 5063.26, "text": " I had a couple of thoughts about this. The first thing was", "tokens": [286, 632, 257, 1916, 295, 4598, 466, 341, 13, 440, 700, 551, 390], "temperature": 0.0, "avg_logprob": -0.5894006093343099, "compression_ratio": 1.5691489361702127, "no_speech_prob": 9.81560970103601e-06}, {"id": 1234, "seek": 504942, "start": 5064.38, "end": 5066.38, "text": " that thing trained", "tokens": [300, 551, 8895], "temperature": 0.0, "avg_logprob": -0.5894006093343099, "compression_ratio": 1.5691489361702127, "no_speech_prob": 9.81560970103601e-06}, {"id": 1235, "seek": 504942, "start": 5066.9400000000005, "end": 5068.46, "text": " in a minute", "tokens": [294, 257, 3456], "temperature": 0.0, "avg_logprob": -0.5894006093343099, "compression_ratio": 1.5691489361702127, "no_speech_prob": 9.81560970103601e-06}, {"id": 1236, "seek": 504942, "start": 5068.46, "end": 5073.9, "text": " On my home computer and I was like, okay, I'm going to run a hyper parameter sweep", "tokens": [1282, 452, 1280, 3820, 293, 286, 390, 411, 11, 1392, 11, 286, 478, 516, 281, 1190, 257, 9848, 13075, 22169], "temperature": 0.0, "avg_logprob": -0.5894006093343099, "compression_ratio": 1.5691489361702127, "no_speech_prob": 9.81560970103601e-06}, {"id": 1237, "seek": 507390, "start": 5073.9, "end": 5079.179999999999, "text": " That thing trained in a minute on my home computer", "tokens": [663, 551, 8895, 294, 257, 3456, 322, 452, 1280, 3820], "temperature": 0.0, "avg_logprob": -0.10161184129260835, "compression_ratio": 1.544502617801047, "no_speech_prob": 1.8447786942488165e-06}, {"id": 1238, "seek": 507390, "start": 5079.66, "end": 5083.82, "text": " And then when I uploaded it to kaggle, it took about four minutes per epoch", "tokens": [400, 550, 562, 286, 17135, 309, 281, 350, 559, 22631, 11, 309, 1890, 466, 1451, 2077, 680, 30992, 339], "temperature": 0.0, "avg_logprob": -0.10161184129260835, "compression_ratio": 1.544502617801047, "no_speech_prob": 1.8447786942488165e-06}, {"id": 1239, "seek": 507390, "start": 5084.46, "end": 5086.46, "text": " Which was horrifying", "tokens": [3013, 390, 40227], "temperature": 0.0, "avg_logprob": -0.10161184129260835, "compression_ratio": 1.544502617801047, "no_speech_prob": 1.8447786942488165e-06}, {"id": 1240, "seek": 507390, "start": 5086.62, "end": 5091.42, "text": " and um kaggle's gpus are not amazing, but they're not that bad", "tokens": [293, 1105, 350, 559, 22631, 311, 290, 31624, 366, 406, 2243, 11, 457, 436, 434, 406, 300, 1578], "temperature": 0.0, "avg_logprob": -0.10161184129260835, "compression_ratio": 1.544502617801047, "no_speech_prob": 1.8447786942488165e-06}, {"id": 1241, "seek": 507390, "start": 5091.98, "end": 5093.98, "text": " Um, so I knew something was up", "tokens": [3301, 11, 370, 286, 2586, 746, 390, 493], "temperature": 0.0, "avg_logprob": -0.10161184129260835, "compression_ratio": 1.544502617801047, "no_speech_prob": 1.8447786942488165e-06}, {"id": 1242, "seek": 507390, "start": 5094.62, "end": 5097.099999999999, "text": " And what was up is I realized that they only have two", "tokens": [400, 437, 390, 493, 307, 286, 5334, 300, 436, 787, 362, 732], "temperature": 0.0, "avg_logprob": -0.10161184129260835, "compression_ratio": 1.544502617801047, "no_speech_prob": 1.8447786942488165e-06}, {"id": 1243, "seek": 509710, "start": 5097.1, "end": 5104.72, "text": " Two virtual cpus which nowadays is tiny like, you know, you generally want as a rule of thumb about", "tokens": [4453, 6374, 269, 31624, 597, 13434, 307, 5870, 411, 11, 291, 458, 11, 291, 5101, 528, 382, 257, 4978, 295, 9298, 466], "temperature": 0.0, "avg_logprob": -0.14064106074246494, "compression_ratio": 1.4524886877828054, "no_speech_prob": 4.222629740979755e-06}, {"id": 1244, "seek": 509710, "start": 5105.280000000001, "end": 5108.56, "text": " eight physical cpus per gpu", "tokens": [3180, 4001, 269, 31624, 680, 290, 34859], "temperature": 0.0, "avg_logprob": -0.14064106074246494, "compression_ratio": 1.4524886877828054, "no_speech_prob": 4.222629740979755e-06}, {"id": 1245, "seek": 509710, "start": 5110.56, "end": 5112.240000000001, "text": " Um", "tokens": [3301], "temperature": 0.0, "avg_logprob": -0.14064106074246494, "compression_ratio": 1.4524886877828054, "no_speech_prob": 4.222629740979755e-06}, {"id": 1246, "seek": 509710, "start": 5112.240000000001, "end": 5114.88, "text": " And so spending all of its time just reading the damn data", "tokens": [400, 370, 6434, 439, 295, 1080, 565, 445, 3760, 264, 8151, 1412], "temperature": 0.0, "avg_logprob": -0.14064106074246494, "compression_ratio": 1.4524886877828054, "no_speech_prob": 4.222629740979755e-06}, {"id": 1247, "seek": 509710, "start": 5115.52, "end": 5120.88, "text": " Now the data was 640 by 480 and we were ending up with any 128 pixel size bits for speed", "tokens": [823, 264, 1412, 390, 1386, 5254, 538, 1017, 4702, 293, 321, 645, 8121, 493, 365, 604, 29810, 19261, 2744, 9239, 337, 3073], "temperature": 0.0, "avg_logprob": -0.14064106074246494, "compression_ratio": 1.4524886877828054, "no_speech_prob": 4.222629740979755e-06}, {"id": 1248, "seek": 509710, "start": 5121.200000000001, "end": 5124.0, "text": " So there's no point doing that every epoch", "tokens": [407, 456, 311, 572, 935, 884, 300, 633, 30992, 339], "temperature": 0.0, "avg_logprob": -0.14064106074246494, "compression_ratio": 1.4524886877828054, "no_speech_prob": 4.222629740979755e-06}, {"id": 1249, "seek": 512400, "start": 5124.0, "end": 5126.24, "text": " So step one was to make my", "tokens": [407, 1823, 472, 390, 281, 652, 452], "temperature": 0.0, "avg_logprob": -0.36817441041442167, "compression_ratio": 1.621212121212121, "no_speech_prob": 7.645525329280645e-06}, {"id": 1250, "seek": 512400, "start": 5126.88, "end": 5129.44, "text": " kaggle iteration faster as well", "tokens": [350, 559, 22631, 24784, 4663, 382, 731], "temperature": 0.0, "avg_logprob": -0.36817441041442167, "compression_ratio": 1.621212121212121, "no_speech_prob": 7.645525329280645e-06}, {"id": 1251, "seek": 512400, "start": 5130.0, "end": 5132.0, "text": " And so very simple thing to do", "tokens": [400, 370, 588, 2199, 551, 281, 360], "temperature": 0.0, "avg_logprob": -0.36817441041442167, "compression_ratio": 1.621212121212121, "no_speech_prob": 7.645525329280645e-06}, {"id": 1252, "seek": 512400, "start": 5132.32, "end": 5133.92, "text": " Resize the images", "tokens": [5015, 1125, 264, 5267], "temperature": 0.0, "avg_logprob": -0.36817441041442167, "compression_ratio": 1.621212121212121, "no_speech_prob": 7.645525329280645e-06}, {"id": 1253, "seek": 512400, "start": 5133.92, "end": 5137.28, "text": " So fastai has a function called resize images", "tokens": [407, 2370, 1301, 575, 257, 2445, 1219, 50069, 5267], "temperature": 0.0, "avg_logprob": -0.36817441041442167, "compression_ratio": 1.621212121212121, "no_speech_prob": 7.645525329280645e-06}, {"id": 1254, "seek": 512400, "start": 5137.92, "end": 5141.92, "text": " And you say okay take all the train images and stick them in", "tokens": [400, 291, 584, 1392, 747, 439, 264, 3847, 5267, 293, 2897, 552, 294], "temperature": 0.0, "avg_logprob": -0.36817441041442167, "compression_ratio": 1.621212121212121, "no_speech_prob": 7.645525329280645e-06}, {"id": 1255, "seek": 512400, "start": 5142.48, "end": 5144.48, "text": " the destination", "tokens": [264, 12236], "temperature": 0.0, "avg_logprob": -0.36817441041442167, "compression_ratio": 1.621212121212121, "no_speech_prob": 7.645525329280645e-06}, {"id": 1256, "seek": 512400, "start": 5144.48, "end": 5146.48, "text": " Making them this size", "tokens": [14595, 552, 341, 2744], "temperature": 0.0, "avg_logprob": -0.36817441041442167, "compression_ratio": 1.621212121212121, "no_speech_prob": 7.645525329280645e-06}, {"id": 1257, "seek": 512400, "start": 5146.72, "end": 5148.24, "text": " recursively", "tokens": [20560, 3413], "temperature": 0.0, "avg_logprob": -0.36817441041442167, "compression_ratio": 1.621212121212121, "no_speech_prob": 7.645525329280645e-06}, {"id": 1258, "seek": 512400, "start": 5148.24, "end": 5151.52, "text": " And it will recreate the same folder structure over here", "tokens": [400, 309, 486, 25833, 264, 912, 10820, 3877, 670, 510], "temperature": 0.0, "avg_logprob": -0.36817441041442167, "compression_ratio": 1.621212121212121, "no_speech_prob": 7.645525329280645e-06}, {"id": 1259, "seek": 515152, "start": 5151.52, "end": 5154.160000000001, "text": " And so that's why I called this the training path", "tokens": [400, 370, 300, 311, 983, 286, 1219, 341, 264, 3097, 3100], "temperature": 0.0, "avg_logprob": -0.3649609883626302, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.6279992678391864e-06}, {"id": 1260, "seek": 515152, "start": 5154.88, "end": 5156.88, "text": " Because this is now my training data", "tokens": [1436, 341, 307, 586, 452, 3097, 1412], "temperature": 0.0, "avg_logprob": -0.3649609883626302, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.6279992678391864e-06}, {"id": 1261, "seek": 515152, "start": 5158.4800000000005, "end": 5160.4800000000005, "text": " And so when I then", "tokens": [400, 370, 562, 286, 550], "temperature": 0.0, "avg_logprob": -0.3649609883626302, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.6279992678391864e-06}, {"id": 1262, "seek": 515152, "start": 5161.120000000001, "end": 5163.120000000001, "text": " trained on that", "tokens": [8895, 322, 300], "temperature": 0.0, "avg_logprob": -0.3649609883626302, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.6279992678391864e-06}, {"id": 1263, "seek": 515152, "start": 5163.120000000001, "end": 5164.4800000000005, "text": " on kaggle", "tokens": [322, 350, 559, 22631], "temperature": 0.0, "avg_logprob": -0.3649609883626302, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.6279992678391864e-06}, {"id": 1264, "seek": 515152, "start": 5164.4800000000005, "end": 5166.4800000000005, "text": " It went down to", "tokens": [467, 1437, 760, 281], "temperature": 0.0, "avg_logprob": -0.3649609883626302, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.6279992678391864e-06}, {"id": 1265, "seek": 515152, "start": 5166.4800000000005, "end": 5168.4800000000005, "text": " uh four times faster", "tokens": [2232, 1451, 1413, 4663], "temperature": 0.0, "avg_logprob": -0.3649609883626302, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.6279992678391864e-06}, {"id": 1266, "seek": 515152, "start": 5168.4800000000005, "end": 5174.56, "text": " Um with no loss of accuracy so that was kind of step one was to actually get my fast", "tokens": [3301, 365, 572, 4470, 295, 14170, 370, 300, 390, 733, 295, 1823, 472, 390, 281, 767, 483, 452, 2370], "temperature": 0.0, "avg_logprob": -0.3649609883626302, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.6279992678391864e-06}, {"id": 1267, "seek": 515152, "start": 5175.200000000001, "end": 5177.200000000001, "text": " iteration working", "tokens": [24784, 1364], "temperature": 0.0, "avg_logprob": -0.3649609883626302, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.6279992678391864e-06}, {"id": 1268, "seek": 515152, "start": 5177.200000000001, "end": 5179.200000000001, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.3649609883626302, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.6279992678391864e-06}, {"id": 1269, "seek": 517920, "start": 5179.2, "end": 5181.76, "text": " um now still", "tokens": [1105, 586, 920], "temperature": 0.0, "avg_logprob": -0.13734953520727938, "compression_ratio": 1.7764227642276422, "no_speech_prob": 2.1110407033120282e-05}, {"id": 1270, "seek": 517920, "start": 5182.48, "end": 5184.24, "text": " I bet it's a long time", "tokens": [286, 778, 309, 311, 257, 938, 565], "temperature": 0.0, "avg_logprob": -0.13734953520727938, "compression_ratio": 1.7764227642276422, "no_speech_prob": 2.1110407033120282e-05}, {"id": 1271, "seek": 517920, "start": 5184.24, "end": 5189.84, "text": " And on kaggle you can actually see this little graph showing how much the cpu is being used how much the gpu is being used on", "tokens": [400, 322, 350, 559, 22631, 291, 393, 767, 536, 341, 707, 4295, 4099, 577, 709, 264, 269, 34859, 307, 885, 1143, 577, 709, 264, 290, 34859, 307, 885, 1143, 322], "temperature": 0.0, "avg_logprob": -0.13734953520727938, "compression_ratio": 1.7764227642276422, "no_speech_prob": 2.1110407033120282e-05}, {"id": 1272, "seek": 517920, "start": 5189.84, "end": 5194.72, "text": " Your own home machine you can um, there are tools free gp, you know free tools to do the same thing", "tokens": [2260, 1065, 1280, 3479, 291, 393, 1105, 11, 456, 366, 3873, 1737, 290, 79, 11, 291, 458, 1737, 3873, 281, 360, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.13734953520727938, "compression_ratio": 1.7764227642276422, "no_speech_prob": 2.1110407033120282e-05}, {"id": 1273, "seek": 517920, "start": 5195.28, "end": 5197.76, "text": " I saw that the gpu was still hardly being used", "tokens": [286, 1866, 300, 264, 290, 34859, 390, 920, 13572, 885, 1143], "temperature": 0.0, "avg_logprob": -0.13734953520727938, "compression_ratio": 1.7764227642276422, "no_speech_prob": 2.1110407033120282e-05}, {"id": 1274, "seek": 517920, "start": 5198.32, "end": 5201.04, "text": " So still cpu was being driven pretty hard", "tokens": [407, 920, 269, 34859, 390, 885, 9555, 1238, 1152], "temperature": 0.0, "avg_logprob": -0.13734953520727938, "compression_ratio": 1.7764227642276422, "no_speech_prob": 2.1110407033120282e-05}, {"id": 1275, "seek": 517920, "start": 5201.76, "end": 5205.28, "text": " Um, I wanted to use a better model anyway to move up the leaderboard", "tokens": [3301, 11, 286, 1415, 281, 764, 257, 1101, 2316, 4033, 281, 1286, 493, 264, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.13734953520727938, "compression_ratio": 1.7764227642276422, "no_speech_prob": 2.1110407033120282e-05}, {"id": 1276, "seek": 520528, "start": 5205.28, "end": 5208.48, "text": " so I moved from a", "tokens": [370, 286, 4259, 490, 257], "temperature": 0.0, "avg_logprob": -0.24151745177151865, "compression_ratio": 1.3161764705882353, "no_speech_prob": 2.057995288851089e-06}, {"id": 1277, "seek": 520528, "start": 5209.679999999999, "end": 5211.679999999999, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.24151745177151865, "compression_ratio": 1.3161764705882353, "no_speech_prob": 2.057995288851089e-06}, {"id": 1278, "seek": 520528, "start": 5212.4, "end": 5215.44, "text": " Oh, by the way, this graph is very useful so this is um", "tokens": [876, 11, 538, 264, 636, 11, 341, 4295, 307, 588, 4420, 370, 341, 307, 1105], "temperature": 0.0, "avg_logprob": -0.24151745177151865, "compression_ratio": 1.3161764705882353, "no_speech_prob": 2.057995288851089e-06}, {"id": 1279, "seek": 520528, "start": 5219.5199999999995, "end": 5227.04, "text": " This is speed versus error rate by family and so we're about to be looking at these um", "tokens": [639, 307, 3073, 5717, 6713, 3314, 538, 1605, 293, 370, 321, 434, 466, 281, 312, 1237, 412, 613, 1105], "temperature": 0.0, "avg_logprob": -0.24151745177151865, "compression_ratio": 1.3161764705882353, "no_speech_prob": 2.057995288851089e-06}, {"id": 1280, "seek": 520528, "start": 5229.0199999999995, "end": 5231.0199999999995, "text": " Convnext models", "tokens": [2656, 85, 716, 734, 5245], "temperature": 0.0, "avg_logprob": -0.24151745177151865, "compression_ratio": 1.3161764705882353, "no_speech_prob": 2.057995288851089e-06}, {"id": 1281, "seek": 523102, "start": 5231.02, "end": 5234.9400000000005, "text": " Um, so we're going to be looking at this one Convnext tiny", "tokens": [3301, 11, 370, 321, 434, 516, 281, 312, 1237, 412, 341, 472, 2656, 85, 716, 734, 5870], "temperature": 0.0, "avg_logprob": -0.10500273271040483, "compression_ratio": 1.6566523605150214, "no_speech_prob": 2.994331680383766e-06}, {"id": 1282, "seek": 523102, "start": 5236.06, "end": 5238.06, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.10500273271040483, "compression_ratio": 1.6566523605150214, "no_speech_prob": 2.994331680383766e-06}, {"id": 1283, "seek": 523102, "start": 5239.18, "end": 5245.26, "text": " Here it is Convnext tiny so we were looking at resnet 2060 which took this long on this data set", "tokens": [1692, 309, 307, 2656, 85, 716, 734, 5870, 370, 321, 645, 1237, 412, 725, 7129, 945, 4550, 597, 1890, 341, 938, 322, 341, 1412, 992], "temperature": 0.0, "avg_logprob": -0.10500273271040483, "compression_ratio": 1.6566523605150214, "no_speech_prob": 2.994331680383766e-06}, {"id": 1284, "seek": 523102, "start": 5246.540000000001, "end": 5250.3, "text": " But this one here is nearly the best. It's third best", "tokens": [583, 341, 472, 510, 307, 6217, 264, 1151, 13, 467, 311, 2636, 1151], "temperature": 0.0, "avg_logprob": -0.10500273271040483, "compression_ratio": 1.6566523605150214, "no_speech_prob": 2.994331680383766e-06}, {"id": 1285, "seek": 523102, "start": 5250.9400000000005, "end": 5252.9400000000005, "text": " But it's still very fast", "tokens": [583, 309, 311, 920, 588, 2370], "temperature": 0.0, "avg_logprob": -0.10500273271040483, "compression_ratio": 1.6566523605150214, "no_speech_prob": 2.994331680383766e-06}, {"id": 1286, "seek": 523102, "start": 5253.5, "end": 5256.3, "text": " And so it's the best overall score. So let's use this", "tokens": [400, 370, 309, 311, 264, 1151, 4787, 6175, 13, 407, 718, 311, 764, 341], "temperature": 0.0, "avg_logprob": -0.10500273271040483, "compression_ratio": 1.6566523605150214, "no_speech_prob": 2.994331680383766e-06}, {"id": 1287, "seek": 525630, "start": 5256.3, "end": 5260.860000000001, "text": " This particularly because you know, we're still spending all of our time waiting for the cpu anyway", "tokens": [639, 4098, 570, 291, 458, 11, 321, 434, 920, 6434, 439, 295, 527, 565, 3806, 337, 264, 269, 34859, 4033], "temperature": 0.0, "avg_logprob": -0.128442435429014, "compression_ratio": 1.5213270142180095, "no_speech_prob": 8.013000297069084e-06}, {"id": 1288, "seek": 525630, "start": 5261.66, "end": 5265.5, "text": " So it turned out that when I switched my architecture to Convnext", "tokens": [407, 309, 3574, 484, 300, 562, 286, 16858, 452, 9482, 281, 2656, 85, 716, 734], "temperature": 0.0, "avg_logprob": -0.128442435429014, "compression_ratio": 1.5213270142180095, "no_speech_prob": 8.013000297069084e-06}, {"id": 1289, "seek": 525630, "start": 5266.54, "end": 5268.9400000000005, "text": " It basically ran just as fast on kaggle", "tokens": [467, 1936, 5872, 445, 382, 2370, 322, 350, 559, 22631], "temperature": 0.0, "avg_logprob": -0.128442435429014, "compression_ratio": 1.5213270142180095, "no_speech_prob": 8.013000297069084e-06}, {"id": 1290, "seek": 525630, "start": 5270.22, "end": 5272.14, "text": " So we can then", "tokens": [407, 321, 393, 550], "temperature": 0.0, "avg_logprob": -0.128442435429014, "compression_ratio": 1.5213270142180095, "no_speech_prob": 8.013000297069084e-06}, {"id": 1291, "seek": 525630, "start": 5272.14, "end": 5274.14, "text": " Train that", "tokens": [28029, 300], "temperature": 0.0, "avg_logprob": -0.128442435429014, "compression_ratio": 1.5213270142180095, "no_speech_prob": 8.013000297069084e-06}, {"id": 1292, "seek": 525630, "start": 5275.1, "end": 5278.78, "text": " Let me switch to the kaggle version because my outputs are missing for some reason", "tokens": [961, 385, 3679, 281, 264, 350, 559, 22631, 3037, 570, 452, 23930, 366, 5361, 337, 512, 1778], "temperature": 0.0, "avg_logprob": -0.128442435429014, "compression_ratio": 1.5213270142180095, "no_speech_prob": 8.013000297069084e-06}, {"id": 1293, "seek": 525630, "start": 5282.54, "end": 5283.9800000000005, "text": " So, um", "tokens": [407, 11, 1105], "temperature": 0.0, "avg_logprob": -0.128442435429014, "compression_ratio": 1.5213270142180095, "no_speech_prob": 8.013000297069084e-06}, {"id": 1294, "seek": 528398, "start": 5283.98, "end": 5288.219999999999, "text": " Yeah, so I started out by running the resnet 2060 on the resized images and got", "tokens": [865, 11, 370, 286, 1409, 484, 538, 2614, 264, 725, 7129, 945, 4550, 322, 264, 725, 1602, 5267, 293, 658], "temperature": 0.0, "avg_logprob": -0.10971751071438932, "compression_ratio": 1.5133928571428572, "no_speech_prob": 3.500661023281282e-06}, {"id": 1295, "seek": 528398, "start": 5288.94, "end": 5291.179999999999, "text": " Similar error rate, but I ran a few more epochs", "tokens": [10905, 6713, 3314, 11, 457, 286, 5872, 257, 1326, 544, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.10971751071438932, "compression_ratio": 1.5133928571428572, "no_speech_prob": 3.500661023281282e-06}, {"id": 1296, "seek": 528398, "start": 5292.0599999999995, "end": 5294.0599999999995, "text": " Got 12 error rate", "tokens": [5803, 2272, 6713, 3314], "temperature": 0.0, "avg_logprob": -0.10971751071438932, "compression_ratio": 1.5133928571428572, "no_speech_prob": 3.500661023281282e-06}, {"id": 1297, "seek": 528398, "start": 5294.379999999999, "end": 5296.379999999999, "text": " And so then I do exactly the same thing", "tokens": [400, 370, 550, 286, 360, 2293, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.10971751071438932, "compression_ratio": 1.5133928571428572, "no_speech_prob": 3.500661023281282e-06}, {"id": 1298, "seek": 528398, "start": 5296.86, "end": 5298.86, "text": " but with Convnext small", "tokens": [457, 365, 2656, 85, 716, 734, 1359], "temperature": 0.0, "avg_logprob": -0.10971751071438932, "compression_ratio": 1.5133928571428572, "no_speech_prob": 3.500661023281282e-06}, {"id": 1299, "seek": 528398, "start": 5299.66, "end": 5304.459999999999, "text": " And 4.5 error rate, so I don't think that different architectures are just", "tokens": [400, 1017, 13, 20, 6713, 3314, 11, 370, 286, 500, 380, 519, 300, 819, 6331, 1303, 366, 445], "temperature": 0.0, "avg_logprob": -0.10971751071438932, "compression_ratio": 1.5133928571428572, "no_speech_prob": 3.500661023281282e-06}, {"id": 1300, "seek": 528398, "start": 5305.339999999999, "end": 5307.74, "text": " Tiny little differences. This is", "tokens": [39992, 707, 7300, 13, 639, 307], "temperature": 0.0, "avg_logprob": -0.10971751071438932, "compression_ratio": 1.5133928571428572, "no_speech_prob": 3.500661023281282e-06}, {"id": 1301, "seek": 528398, "start": 5308.459999999999, "end": 5310.459999999999, "text": " over twice as good", "tokens": [670, 6091, 382, 665], "temperature": 0.0, "avg_logprob": -0.10971751071438932, "compression_ratio": 1.5133928571428572, "no_speech_prob": 3.500661023281282e-06}, {"id": 1302, "seek": 528398, "start": 5310.7, "end": 5312.299999999999, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.10971751071438932, "compression_ratio": 1.5133928571428572, "no_speech_prob": 3.500661023281282e-06}, {"id": 1303, "seek": 531230, "start": 5312.3, "end": 5314.3, "text": " and um", "tokens": [293, 1105], "temperature": 0.0, "avg_logprob": -0.09386334595856843, "compression_ratio": 1.6144067796610169, "no_speech_prob": 2.5068768081837334e-05}, {"id": 1304, "seek": 531230, "start": 5315.02, "end": 5317.74, "text": " A lot of folks you talk to will never have heard of this", "tokens": [316, 688, 295, 4024, 291, 751, 281, 486, 1128, 362, 2198, 295, 341], "temperature": 0.0, "avg_logprob": -0.09386334595856843, "compression_ratio": 1.6144067796610169, "no_speech_prob": 2.5068768081837334e-05}, {"id": 1305, "seek": 531230, "start": 5318.6, "end": 5320.6, "text": " Convnext because it's very new", "tokens": [2656, 85, 716, 734, 570, 309, 311, 588, 777], "temperature": 0.0, "avg_logprob": -0.09386334595856843, "compression_ratio": 1.6144067796610169, "no_speech_prob": 2.5068768081837334e-05}, {"id": 1306, "seek": 531230, "start": 5321.66, "end": 5324.06, "text": " and i've noticed a lot of people tend not to", "tokens": [293, 741, 600, 5694, 257, 688, 295, 561, 3928, 406, 281], "temperature": 0.0, "avg_logprob": -0.09386334595856843, "compression_ratio": 1.6144067796610169, "no_speech_prob": 2.5068768081837334e-05}, {"id": 1307, "seek": 531230, "start": 5325.58, "end": 5332.06, "text": " Keep up to date with new things. They kind of learn something at university and then they stop stop learning. So if somebody's still", "tokens": [5527, 493, 281, 4002, 365, 777, 721, 13, 814, 733, 295, 1466, 746, 412, 5454, 293, 550, 436, 1590, 1590, 2539, 13, 407, 498, 2618, 311, 920], "temperature": 0.0, "avg_logprob": -0.09386334595856843, "compression_ratio": 1.6144067796610169, "no_speech_prob": 2.5068768081837334e-05}, {"id": 1308, "seek": 531230, "start": 5332.860000000001, "end": 5334.860000000001, "text": " Just using resnets all the time", "tokens": [1449, 1228, 725, 77, 1385, 439, 264, 565], "temperature": 0.0, "avg_logprob": -0.09386334595856843, "compression_ratio": 1.6144067796610169, "no_speech_prob": 2.5068768081837334e-05}, {"id": 1309, "seek": 531230, "start": 5335.34, "end": 5338.54, "text": " You know, you can tell them we've we've actually we've moved on", "tokens": [509, 458, 11, 291, 393, 980, 552, 321, 600, 321, 600, 767, 321, 600, 4259, 322], "temperature": 0.0, "avg_logprob": -0.09386334595856843, "compression_ratio": 1.6144067796610169, "no_speech_prob": 2.5068768081837334e-05}, {"id": 1310, "seek": 531230, "start": 5339.26, "end": 5340.7, "text": " you know, um", "tokens": [291, 458, 11, 1105], "temperature": 0.0, "avg_logprob": -0.09386334595856843, "compression_ratio": 1.6144067796610169, "no_speech_prob": 2.5068768081837334e-05}, {"id": 1311, "seek": 534070, "start": 5340.7, "end": 5342.94, "text": " Resnets are still probably the fastest", "tokens": [5015, 77, 1385, 366, 920, 1391, 264, 14573], "temperature": 0.0, "avg_logprob": -0.11738490283004636, "compression_ratio": 1.6626506024096386, "no_speech_prob": 1.1841734703921247e-05}, {"id": 1312, "seek": 534070, "start": 5344.22, "end": 5345.099999999999, "text": " but", "tokens": [457], "temperature": 0.0, "avg_logprob": -0.11738490283004636, "compression_ratio": 1.6626506024096386, "no_speech_prob": 1.1841734703921247e-05}, {"id": 1313, "seek": 534070, "start": 5345.099999999999, "end": 5347.099999999999, "text": " For the mix of speed and performance", "tokens": [1171, 264, 2890, 295, 3073, 293, 3389], "temperature": 0.0, "avg_logprob": -0.11738490283004636, "compression_ratio": 1.6626506024096386, "no_speech_prob": 1.1841734703921247e-05}, {"id": 1314, "seek": 534070, "start": 5347.98, "end": 5349.099999999999, "text": " you know", "tokens": [291, 458], "temperature": 0.0, "avg_logprob": -0.11738490283004636, "compression_ratio": 1.6626506024096386, "no_speech_prob": 1.1841734703921247e-05}, {"id": 1315, "seek": 534070, "start": 5349.099999999999, "end": 5350.22, "text": " Not so much", "tokens": [1726, 370, 709], "temperature": 0.0, "avg_logprob": -0.11738490283004636, "compression_ratio": 1.6626506024096386, "no_speech_prob": 1.1841734703921247e-05}, {"id": 1316, "seek": 534070, "start": 5350.22, "end": 5353.98, "text": " Um Convnext, you know again you want these rules of thumb, right?", "tokens": [3301, 2656, 85, 716, 734, 11, 291, 458, 797, 291, 528, 613, 4474, 295, 9298, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11738490283004636, "compression_ratio": 1.6626506024096386, "no_speech_prob": 1.1841734703921247e-05}, {"id": 1317, "seek": 534070, "start": 5354.7, "end": 5356.7, "text": " If you're not sure what to do", "tokens": [759, 291, 434, 406, 988, 437, 281, 360], "temperature": 0.0, "avg_logprob": -0.11738490283004636, "compression_ratio": 1.6626506024096386, "no_speech_prob": 1.1841734703921247e-05}, {"id": 1318, "seek": 534070, "start": 5357.0199999999995, "end": 5362.7, "text": " This Convnext, okay, and then like most things there's different sizes. There's a tiny", "tokens": [639, 2656, 85, 716, 734, 11, 1392, 11, 293, 550, 411, 881, 721, 456, 311, 819, 11602, 13, 821, 311, 257, 5870], "temperature": 0.0, "avg_logprob": -0.11738490283004636, "compression_ratio": 1.6626506024096386, "no_speech_prob": 1.1841734703921247e-05}, {"id": 1319, "seek": 536270, "start": 5362.7, "end": 5370.3, "text": " There's a small there's a base. There's a large there's an extra large and you know, it's just uh, well, let's talk about the picture", "tokens": [821, 311, 257, 1359, 456, 311, 257, 3096, 13, 821, 311, 257, 2416, 456, 311, 364, 2857, 2416, 293, 291, 458, 11, 309, 311, 445, 2232, 11, 731, 11, 718, 311, 751, 466, 264, 3036], "temperature": 0.0, "avg_logprob": -0.12054024864645566, "compression_ratio": 1.578125, "no_speech_prob": 5.5074870033422485e-06}, {"id": 1320, "seek": 536270, "start": 5375.26, "end": 5377.26, "text": " This is it here", "tokens": [639, 307, 309, 510], "temperature": 0.0, "avg_logprob": -0.12054024864645566, "compression_ratio": 1.578125, "no_speech_prob": 5.5074870033422485e-06}, {"id": 1321, "seek": 536270, "start": 5377.98, "end": 5379.98, "text": " Right", "tokens": [1779], "temperature": 0.0, "avg_logprob": -0.12054024864645566, "compression_ratio": 1.578125, "no_speech_prob": 5.5074870033422485e-06}, {"id": 1322, "seek": 536270, "start": 5380.0599999999995, "end": 5383.179999999999, "text": " Large takes longer but lower error", "tokens": [33092, 2516, 2854, 457, 3126, 6713], "temperature": 0.0, "avg_logprob": -0.12054024864645566, "compression_ratio": 1.578125, "no_speech_prob": 5.5074870033422485e-06}, {"id": 1323, "seek": 536270, "start": 5384.22, "end": 5386.78, "text": " Tiny takes less time but higher error, right? So", "tokens": [39992, 2516, 1570, 565, 457, 2946, 6713, 11, 558, 30, 407], "temperature": 0.0, "avg_logprob": -0.12054024864645566, "compression_ratio": 1.578125, "no_speech_prob": 5.5074870033422485e-06}, {"id": 1324, "seek": 536270, "start": 5387.5, "end": 5389.179999999999, "text": " you you pick", "tokens": [291, 291, 1888], "temperature": 0.0, "avg_logprob": -0.12054024864645566, "compression_ratio": 1.578125, "no_speech_prob": 5.5074870033422485e-06}, {"id": 1325, "seek": 538918, "start": 5389.18, "end": 5394.860000000001, "text": " About your speed versus accuracy trade-off for you. So for us small is great", "tokens": [7769, 428, 3073, 5717, 14170, 4923, 12, 4506, 337, 291, 13, 407, 337, 505, 1359, 307, 869], "temperature": 0.0, "avg_logprob": -0.1064910327686983, "compression_ratio": 1.4977777777777779, "no_speech_prob": 1.078272453014506e-05}, {"id": 1326, "seek": 538918, "start": 5397.26, "end": 5401.18, "text": " And so yeah now we've got a 4.5 percent error that's that's terrific", "tokens": [400, 370, 1338, 586, 321, 600, 658, 257, 1017, 13, 20, 3043, 6713, 300, 311, 300, 311, 20899], "temperature": 0.0, "avg_logprob": -0.1064910327686983, "compression_ratio": 1.4977777777777779, "no_speech_prob": 1.078272453014506e-05}, {"id": 1327, "seek": 538918, "start": 5402.780000000001, "end": 5403.9800000000005, "text": " Um", "tokens": [3301], "temperature": 0.0, "avg_logprob": -0.1064910327686983, "compression_ratio": 1.4977777777777779, "no_speech_prob": 1.078272453014506e-05}, {"id": 1328, "seek": 538918, "start": 5403.9800000000005, "end": 5405.42, "text": " Now let's iterate", "tokens": [823, 718, 311, 44497], "temperature": 0.0, "avg_logprob": -0.1064910327686983, "compression_ratio": 1.4977777777777779, "no_speech_prob": 1.078272453014506e-05}, {"id": 1329, "seek": 538918, "start": 5405.42, "end": 5412.14, "text": " Uh on kaggle. This is taking about a minute per epoch on my computer. It's probably taking about 20 seconds per epoch", "tokens": [4019, 322, 350, 559, 22631, 13, 639, 307, 1940, 466, 257, 3456, 680, 30992, 339, 322, 452, 3820, 13, 467, 311, 1391, 1940, 466, 945, 3949, 680, 30992, 339], "temperature": 0.0, "avg_logprob": -0.1064910327686983, "compression_ratio": 1.4977777777777779, "no_speech_prob": 1.078272453014506e-05}, {"id": 1330, "seek": 538918, "start": 5412.22, "end": 5413.66, "text": " So not too bad", "tokens": [407, 406, 886, 1578], "temperature": 0.0, "avg_logprob": -0.1064910327686983, "compression_ratio": 1.4977777777777779, "no_speech_prob": 1.078272453014506e-05}, {"id": 1331, "seek": 538918, "start": 5413.66, "end": 5414.62, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.1064910327686983, "compression_ratio": 1.4977777777777779, "no_speech_prob": 1.078272453014506e-05}, {"id": 1332, "seek": 538918, "start": 5414.62, "end": 5415.66, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.1064910327686983, "compression_ratio": 1.4977777777777779, "no_speech_prob": 1.078272453014506e-05}, {"id": 1333, "seek": 538918, "start": 5415.66, "end": 5417.66, "text": " You know one thing we could try", "tokens": [509, 458, 472, 551, 321, 727, 853], "temperature": 0.0, "avg_logprob": -0.1064910327686983, "compression_ratio": 1.4977777777777779, "no_speech_prob": 1.078272453014506e-05}, {"id": 1334, "seek": 541766, "start": 5417.66, "end": 5419.66, "text": " Is instead of using squish", "tokens": [1119, 2602, 295, 1228, 31379], "temperature": 0.0, "avg_logprob": -0.14948107798894247, "compression_ratio": 1.6271929824561404, "no_speech_prob": 1.8341906979912892e-05}, {"id": 1335, "seek": 541766, "start": 5420.94, "end": 5425.9, "text": " As our pre-processing let's try using crop so that will randomly crop out an area", "tokens": [1018, 527, 659, 12, 41075, 278, 718, 311, 853, 1228, 9086, 370, 300, 486, 16979, 9086, 484, 364, 1859], "temperature": 0.0, "avg_logprob": -0.14948107798894247, "compression_ratio": 1.6271929824561404, "no_speech_prob": 1.8341906979912892e-05}, {"id": 1336, "seek": 541766, "start": 5426.62, "end": 5430.78, "text": " Uh, and that's the default. So if I remove the method equals squish that will crop", "tokens": [4019, 11, 293, 300, 311, 264, 7576, 13, 407, 498, 286, 4159, 264, 3170, 6915, 31379, 300, 486, 9086], "temperature": 0.0, "avg_logprob": -0.14948107798894247, "compression_ratio": 1.6271929824561404, "no_speech_prob": 1.8341906979912892e-05}, {"id": 1337, "seek": 541766, "start": 5431.74, "end": 5434.0599999999995, "text": " So you see how i've tried to get everything into a single", "tokens": [407, 291, 536, 577, 741, 600, 3031, 281, 483, 1203, 666, 257, 2167], "temperature": 0.0, "avg_logprob": -0.14948107798894247, "compression_ratio": 1.6271929824561404, "no_speech_prob": 1.8341906979912892e-05}, {"id": 1338, "seek": 541766, "start": 5434.7, "end": 5439.92, "text": " Function, right this single function I can tell it let's go and find the definition", "tokens": [11166, 882, 11, 558, 341, 2167, 2445, 286, 393, 980, 309, 718, 311, 352, 293, 915, 264, 7123], "temperature": 0.0, "avg_logprob": -0.14948107798894247, "compression_ratio": 1.6271929824561404, "no_speech_prob": 1.8341906979912892e-05}, {"id": 1339, "seek": 541766, "start": 5441.5, "end": 5443.5, "text": " What architecture do I want to train?", "tokens": [708, 9482, 360, 286, 528, 281, 3847, 30], "temperature": 0.0, "avg_logprob": -0.14948107798894247, "compression_ratio": 1.6271929824561404, "no_speech_prob": 1.8341906979912892e-05}, {"id": 1340, "seek": 544350, "start": 5443.5, "end": 5448.14, "text": " How do I want to transform the items? How do I want to transform the batches and how many epochs do I want to do?", "tokens": [1012, 360, 286, 528, 281, 4088, 264, 4754, 30, 1012, 360, 286, 528, 281, 4088, 264, 15245, 279, 293, 577, 867, 30992, 28346, 360, 286, 528, 281, 360, 30], "temperature": 0.0, "avg_logprob": -0.10595065525599888, "compression_ratio": 1.738938053097345, "no_speech_prob": 4.860265107708983e-06}, {"id": 1341, "seek": 544350, "start": 5448.86, "end": 5450.86, "text": " That's basically it, right?", "tokens": [663, 311, 1936, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.10595065525599888, "compression_ratio": 1.738938053097345, "no_speech_prob": 4.860265107708983e-06}, {"id": 1342, "seek": 544350, "start": 5451.26, "end": 5455.34, "text": " Um, so this time I want to use the same architecture conf next", "tokens": [3301, 11, 370, 341, 565, 286, 528, 281, 764, 264, 912, 9482, 1497, 958], "temperature": 0.0, "avg_logprob": -0.10595065525599888, "compression_ratio": 1.738938053097345, "no_speech_prob": 4.860265107708983e-06}, {"id": 1343, "seek": 544350, "start": 5455.82, "end": 5459.68, "text": " I want to resize without cropping and then use the same data augmentation", "tokens": [286, 528, 281, 50069, 1553, 4848, 3759, 293, 550, 764, 264, 912, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.10595065525599888, "compression_ratio": 1.738938053097345, "no_speech_prob": 4.860265107708983e-06}, {"id": 1344, "seek": 544350, "start": 5460.94, "end": 5463.1, "text": " And okay error rates about the same", "tokens": [400, 1392, 6713, 6846, 466, 264, 912], "temperature": 0.0, "avg_logprob": -0.10595065525599888, "compression_ratio": 1.738938053097345, "no_speech_prob": 4.860265107708983e-06}, {"id": 1345, "seek": 544350, "start": 5464.3, "end": 5468.3, "text": " So not particularly it's a tiny bit worse, but not enough to be interesting", "tokens": [407, 406, 4098, 309, 311, 257, 5870, 857, 5324, 11, 457, 406, 1547, 281, 312, 1880], "temperature": 0.0, "avg_logprob": -0.10595065525599888, "compression_ratio": 1.738938053097345, "no_speech_prob": 4.860265107708983e-06}, {"id": 1346, "seek": 544350, "start": 5469.18, "end": 5471.02, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.10595065525599888, "compression_ratio": 1.738938053097345, "no_speech_prob": 4.860265107708983e-06}, {"id": 1347, "seek": 547102, "start": 5471.02, "end": 5475.660000000001, "text": " Instead of cropping we can pad now padding is interesting. Do you see how these are all square?", "tokens": [7156, 295, 4848, 3759, 321, 393, 6887, 586, 39562, 307, 1880, 13, 1144, 291, 536, 577, 613, 366, 439, 3732, 30], "temperature": 0.0, "avg_logprob": -0.0658680223069101, "compression_ratio": 1.6768060836501901, "no_speech_prob": 9.818083526624832e-06}, {"id": 1348, "seek": 547102, "start": 5476.700000000001, "end": 5478.700000000001, "text": " Right, but they've got black borders", "tokens": [1779, 11, 457, 436, 600, 658, 2211, 16287], "temperature": 0.0, "avg_logprob": -0.0658680223069101, "compression_ratio": 1.6768060836501901, "no_speech_prob": 9.818083526624832e-06}, {"id": 1349, "seek": 547102, "start": 5479.820000000001, "end": 5480.780000000001, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.0658680223069101, "compression_ratio": 1.6768060836501901, "no_speech_prob": 9.818083526624832e-06}, {"id": 1350, "seek": 547102, "start": 5480.780000000001, "end": 5487.900000000001, "text": " Padding is interesting because it's the only way of pre-processing images which doesn't distort them and doesn't lose anything", "tokens": [18691, 3584, 307, 1880, 570, 309, 311, 264, 787, 636, 295, 659, 12, 41075, 278, 5267, 597, 1177, 380, 37555, 552, 293, 1177, 380, 3624, 1340], "temperature": 0.0, "avg_logprob": -0.0658680223069101, "compression_ratio": 1.6768060836501901, "no_speech_prob": 9.818083526624832e-06}, {"id": 1351, "seek": 547102, "start": 5488.3, "end": 5491.900000000001, "text": " If you crop you lose things if you squish you distort things", "tokens": [759, 291, 9086, 291, 3624, 721, 498, 291, 31379, 291, 37555, 721], "temperature": 0.0, "avg_logprob": -0.0658680223069101, "compression_ratio": 1.6768060836501901, "no_speech_prob": 9.818083526624832e-06}, {"id": 1352, "seek": 547102, "start": 5493.1, "end": 5494.46, "text": " This does neither", "tokens": [639, 775, 9662], "temperature": 0.0, "avg_logprob": -0.0658680223069101, "compression_ratio": 1.6768060836501901, "no_speech_prob": 9.818083526624832e-06}, {"id": 1353, "seek": 547102, "start": 5494.46, "end": 5499.820000000001, "text": " Now, of course the downside is that there's pixels that are literally pointless. They contain zeros", "tokens": [823, 11, 295, 1164, 264, 25060, 307, 300, 456, 311, 18668, 300, 366, 3736, 32824, 13, 814, 5304, 35193], "temperature": 0.0, "avg_logprob": -0.0658680223069101, "compression_ratio": 1.6768060836501901, "no_speech_prob": 9.818083526624832e-06}, {"id": 1354, "seek": 549982, "start": 5499.82, "end": 5502.0599999999995, "text": " So every way of getting this working", "tokens": [407, 633, 636, 295, 1242, 341, 1364], "temperature": 0.0, "avg_logprob": -0.3400139504290642, "compression_ratio": 1.542857142857143, "no_speech_prob": 6.240698439796688e-06}, {"id": 1355, "seek": 549982, "start": 5502.62, "end": 5507.42, "text": " It has its compromises, but this approach of resizing where we pad with zeros", "tokens": [467, 575, 1080, 11482, 3598, 11, 457, 341, 3109, 295, 725, 3319, 689, 321, 6887, 365, 35193], "temperature": 0.0, "avg_logprob": -0.3400139504290642, "compression_ratio": 1.542857142857143, "no_speech_prob": 6.240698439796688e-06}, {"id": 1356, "seek": 549982, "start": 5508.299999999999, "end": 5511.66, "text": " Is not used enough and it can actually often work quite well", "tokens": [1119, 406, 1143, 1547, 293, 309, 393, 767, 2049, 589, 1596, 731], "temperature": 0.0, "avg_logprob": -0.3400139504290642, "compression_ratio": 1.542857142857143, "no_speech_prob": 6.240698439796688e-06}, {"id": 1357, "seek": 549982, "start": 5512.7, "end": 5515.0199999999995, "text": " And in this case, it was about as good as our", "tokens": [400, 294, 341, 1389, 11, 309, 390, 466, 382, 665, 382, 527], "temperature": 0.0, "avg_logprob": -0.3400139504290642, "compression_ratio": 1.542857142857143, "no_speech_prob": 6.240698439796688e-06}, {"id": 1358, "seek": 549982, "start": 5515.74, "end": 5517.74, "text": " Best so far", "tokens": [9752, 370, 1400], "temperature": 0.0, "avg_logprob": -0.3400139504290642, "compression_ratio": 1.542857142857143, "no_speech_prob": 6.240698439796688e-06}, {"id": 1359, "seek": 549982, "start": 5518.0599999999995, "end": 5520.0599999999995, "text": " Um, but no not huge differences yet", "tokens": [3301, 11, 457, 572, 406, 2603, 7300, 1939], "temperature": 0.0, "avg_logprob": -0.3400139504290642, "compression_ratio": 1.542857142857143, "no_speech_prob": 6.240698439796688e-06}, {"id": 1360, "seek": 549982, "start": 5521.099999999999, "end": 5523.099999999999, "text": " Um, what else could we do?", "tokens": [3301, 11, 437, 1646, 727, 321, 360, 30], "temperature": 0.0, "avg_logprob": -0.3400139504290642, "compression_ratio": 1.542857142857143, "no_speech_prob": 6.240698439796688e-06}, {"id": 1361, "seek": 549982, "start": 5523.66, "end": 5525.66, "text": " uh well", "tokens": [2232, 731], "temperature": 0.0, "avg_logprob": -0.3400139504290642, "compression_ratio": 1.542857142857143, "no_speech_prob": 6.240698439796688e-06}, {"id": 1362, "seek": 549982, "start": 5526.94, "end": 5528.94, "text": " What we could do is", "tokens": [708, 321, 727, 360, 307], "temperature": 0.0, "avg_logprob": -0.3400139504290642, "compression_ratio": 1.542857142857143, "no_speech_prob": 6.240698439796688e-06}, {"id": 1363, "seek": 552894, "start": 5528.94, "end": 5530.94, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.28348994784884984, "compression_ratio": 1.880952380952381, "no_speech_prob": 1.3845105058862828e-05}, {"id": 1364, "seek": 552894, "start": 5531.259999999999, "end": 5534.86, "text": " Through these pictures this is all the same picture", "tokens": [8927, 613, 5242, 341, 307, 439, 264, 912, 3036], "temperature": 0.0, "avg_logprob": -0.28348994784884984, "compression_ratio": 1.880952380952381, "no_speech_prob": 1.3845105058862828e-05}, {"id": 1365, "seek": 552894, "start": 5535.74, "end": 5539.98, "text": " But uh, it's gone through our data augmentation. So sometimes it's a bit darker", "tokens": [583, 2232, 11, 309, 311, 2780, 807, 527, 1412, 14501, 19631, 13, 407, 2171, 309, 311, 257, 857, 12741], "temperature": 0.0, "avg_logprob": -0.28348994784884984, "compression_ratio": 1.880952380952381, "no_speech_prob": 1.3845105058862828e-05}, {"id": 1366, "seek": 552894, "start": 5539.98, "end": 5546.94, "text": " Sometimes it's flipped horizontally. Sometimes it's slightly rotated. Sometimes it's slightly warped. Sometimes it's zooming into a slightly different section", "tokens": [4803, 309, 311, 26273, 33796, 13, 4803, 309, 311, 4748, 42146, 13, 4803, 309, 311, 4748, 1516, 3452, 13, 4803, 309, 311, 48226, 666, 257, 4748, 819, 3541], "temperature": 0.0, "avg_logprob": -0.28348994784884984, "compression_ratio": 1.880952380952381, "no_speech_prob": 1.3845105058862828e-05}, {"id": 1367, "seek": 552894, "start": 5547.58, "end": 5549.58, "text": " But this is all the same picture", "tokens": [583, 341, 307, 439, 264, 912, 3036], "temperature": 0.0, "avg_logprob": -0.28348994784884984, "compression_ratio": 1.880952380952381, "no_speech_prob": 1.3845105058862828e-05}, {"id": 1368, "seek": 552894, "start": 5551.9, "end": 5554.94, "text": " Maybe our model would like some of these versions better than others", "tokens": [2704, 527, 2316, 576, 411, 512, 295, 613, 9606, 1101, 813, 2357], "temperature": 0.0, "avg_logprob": -0.28348994784884984, "compression_ratio": 1.880952380952381, "no_speech_prob": 1.3845105058862828e-05}, {"id": 1369, "seek": 555494, "start": 5554.94, "end": 5560.7, "text": " So what we can do is we can pass all of these to our model get predictions for all of them", "tokens": [407, 437, 321, 393, 360, 307, 321, 393, 1320, 439, 295, 613, 281, 527, 2316, 483, 21264, 337, 439, 295, 552], "temperature": 0.0, "avg_logprob": -0.2927021874321832, "compression_ratio": 1.6160714285714286, "no_speech_prob": 5.86262058277498e-06}, {"id": 1370, "seek": 555494, "start": 5561.74, "end": 5563.179999999999, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.2927021874321832, "compression_ratio": 1.6160714285714286, "no_speech_prob": 5.86262058277498e-06}, {"id": 1371, "seek": 555494, "start": 5563.179999999999, "end": 5564.379999999999, "text": " Take the average", "tokens": [3664, 264, 4274], "temperature": 0.0, "avg_logprob": -0.2927021874321832, "compression_ratio": 1.6160714285714286, "no_speech_prob": 5.86262058277498e-06}, {"id": 1372, "seek": 555494, "start": 5564.379999999999, "end": 5570.7, "text": " Right. So it's our own kind of like little mini bagging approach and this is called test time augmentation", "tokens": [1779, 13, 407, 309, 311, 527, 1065, 733, 295, 411, 707, 8382, 3411, 3249, 3109, 293, 341, 307, 1219, 1500, 565, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.2927021874321832, "compression_ratio": 1.6160714285714286, "no_speech_prob": 5.86262058277498e-06}, {"id": 1373, "seek": 555494, "start": 5572.219999999999, "end": 5575.98, "text": " Fast ai is very unusual in making that available in a single method", "tokens": [15968, 9783, 307, 588, 10901, 294, 1455, 300, 2435, 294, 257, 2167, 3170], "temperature": 0.0, "avg_logprob": -0.2927021874321832, "compression_ratio": 1.6160714285714286, "no_speech_prob": 5.86262058277498e-06}, {"id": 1374, "seek": 555494, "start": 5576.7, "end": 5581.66, "text": " You just pass tta and it will pass multiple augmented versions of the model", "tokens": [509, 445, 1320, 256, 1328, 293, 309, 486, 1320, 3866, 36155, 9606, 295, 264, 2316], "temperature": 0.0, "avg_logprob": -0.2927021874321832, "compression_ratio": 1.6160714285714286, "no_speech_prob": 5.86262058277498e-06}, {"id": 1375, "seek": 558166, "start": 5581.66, "end": 5585.58, "text": " tta and it will pass multiple augmented versions of the image", "tokens": [256, 1328, 293, 309, 486, 1320, 3866, 36155, 9606, 295, 264, 3256], "temperature": 0.0, "avg_logprob": -0.1264401011996799, "compression_ratio": 1.4851485148514851, "no_speech_prob": 1.3210975339461584e-05}, {"id": 1376, "seek": 558166, "start": 5586.46, "end": 5588.46, "text": " and um and", "tokens": [293, 1105, 293], "temperature": 0.0, "avg_logprob": -0.1264401011996799, "compression_ratio": 1.4851485148514851, "no_speech_prob": 1.3210975339461584e-05}, {"id": 1377, "seek": 558166, "start": 5588.94, "end": 5590.3, "text": " Average them", "tokens": [316, 3623, 552], "temperature": 0.0, "avg_logprob": -0.1264401011996799, "compression_ratio": 1.4851485148514851, "no_speech_prob": 1.3210975339461584e-05}, {"id": 1378, "seek": 558166, "start": 5590.3, "end": 5592.3, "text": " for you", "tokens": [337, 291], "temperature": 0.0, "avg_logprob": -0.1264401011996799, "compression_ratio": 1.4851485148514851, "no_speech_prob": 1.3210975339461584e-05}, {"id": 1379, "seek": 558166, "start": 5593.099999999999, "end": 5594.46, "text": " And so", "tokens": [400, 370], "temperature": 0.0, "avg_logprob": -0.1264401011996799, "compression_ratio": 1.4851485148514851, "no_speech_prob": 1.3210975339461584e-05}, {"id": 1380, "seek": 558166, "start": 5594.46, "end": 5602.08, "text": " This is the same model as before which had a 4.5 percent. So if instead if we get uh tta predictions", "tokens": [639, 307, 264, 912, 2316, 382, 949, 597, 632, 257, 1017, 13, 20, 3043, 13, 407, 498, 2602, 498, 321, 483, 2232, 256, 1328, 21264], "temperature": 0.0, "avg_logprob": -0.1264401011996799, "compression_ratio": 1.4851485148514851, "no_speech_prob": 1.3210975339461584e-05}, {"id": 1381, "seek": 558166, "start": 5604.0599999999995, "end": 5606.0599999999995, "text": " Uh, and then get the error rate", "tokens": [4019, 11, 293, 550, 483, 264, 6713, 3314], "temperature": 0.0, "avg_logprob": -0.1264401011996799, "compression_ratio": 1.4851485148514851, "no_speech_prob": 1.3210975339461584e-05}, {"id": 1382, "seek": 560606, "start": 5606.06, "end": 5613.18, "text": " Um, wait, why does this say 4.8 last time I did this it was way better. Well, that's messing things up, isn't it?", "tokens": [3301, 11, 1699, 11, 983, 775, 341, 584, 1017, 13, 23, 1036, 565, 286, 630, 341, 309, 390, 636, 1101, 13, 1042, 11, 300, 311, 23258, 721, 493, 11, 1943, 380, 309, 30], "temperature": 0.0, "avg_logprob": -0.12297034012643915, "compression_ratio": 1.4883720930232558, "no_speech_prob": 1.3210928045737091e-05}, {"id": 1383, "seek": 560606, "start": 5616.780000000001, "end": 5618.780000000001, "text": " Uh", "tokens": [4019], "temperature": 0.0, "avg_logprob": -0.12297034012643915, "compression_ratio": 1.4883720930232558, "no_speech_prob": 1.3210928045737091e-05}, {"id": 1384, "seek": 560606, "start": 5619.34, "end": 5625.26, "text": " So when I did this originally on my home computer it went from like 4.5 to 3.9. So possibly I", "tokens": [407, 562, 286, 630, 341, 7993, 322, 452, 1280, 3820, 309, 1437, 490, 411, 1017, 13, 20, 281, 805, 13, 24, 13, 407, 6264, 286], "temperature": 0.0, "avg_logprob": -0.12297034012643915, "compression_ratio": 1.4883720930232558, "no_speech_prob": 1.3210928045737091e-05}, {"id": 1385, "seek": 562526, "start": 5625.26, "end": 5634.3, "text": " Got a very bad luck this time. So this is the first time i've actually ever seen tta give it worse result. Um,", "tokens": [5803, 257, 588, 1578, 3668, 341, 565, 13, 407, 341, 307, 264, 700, 565, 741, 600, 767, 1562, 1612, 256, 1328, 976, 309, 5324, 1874, 13, 3301, 11], "temperature": 0.0, "avg_logprob": -0.0877180002173599, "compression_ratio": 1.5352112676056338, "no_speech_prob": 1.6963243979262188e-05}, {"id": 1386, "seek": 562526, "start": 5636.38, "end": 5638.38, "text": " So that's very weird", "tokens": [407, 300, 311, 588, 3657], "temperature": 0.0, "avg_logprob": -0.0877180002173599, "compression_ratio": 1.5352112676056338, "no_speech_prob": 1.6963243979262188e-05}, {"id": 1387, "seek": 562526, "start": 5638.860000000001, "end": 5640.860000000001, "text": " I wonder if it's", "tokens": [286, 2441, 498, 309, 311], "temperature": 0.0, "avg_logprob": -0.0877180002173599, "compression_ratio": 1.5352112676056338, "no_speech_prob": 1.6963243979262188e-05}, {"id": 1388, "seek": 562526, "start": 5642.22, "end": 5644.62, "text": " If I should do something other than the crop padding, all right", "tokens": [759, 286, 820, 360, 746, 661, 813, 264, 9086, 39562, 11, 439, 558], "temperature": 0.0, "avg_logprob": -0.0877180002173599, "compression_ratio": 1.5352112676056338, "no_speech_prob": 1.6963243979262188e-05}, {"id": 1389, "seek": 562526, "start": 5644.7, "end": 5647.02, "text": " I'll have to check that out and i'll try and come back to you and find out", "tokens": [286, 603, 362, 281, 1520, 300, 484, 293, 741, 603, 853, 293, 808, 646, 281, 291, 293, 915, 484], "temperature": 0.0, "avg_logprob": -0.0877180002173599, "compression_ratio": 1.5352112676056338, "no_speech_prob": 1.6963243979262188e-05}, {"id": 1390, "seek": 562526, "start": 5648.14, "end": 5650.06, "text": " Why in this case?", "tokens": [1545, 294, 341, 1389, 30], "temperature": 0.0, "avg_logprob": -0.0877180002173599, "compression_ratio": 1.5352112676056338, "no_speech_prob": 1.6963243979262188e-05}, {"id": 1391, "seek": 562526, "start": 5650.06, "end": 5652.06, "text": " This one was worse", "tokens": [639, 472, 390, 5324], "temperature": 0.0, "avg_logprob": -0.0877180002173599, "compression_ratio": 1.5352112676056338, "no_speech_prob": 1.6963243979262188e-05}, {"id": 1392, "seek": 562526, "start": 5652.22, "end": 5653.66, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.0877180002173599, "compression_ratio": 1.5352112676056338, "no_speech_prob": 1.6963243979262188e-05}, {"id": 1393, "seek": 565366, "start": 5653.66, "end": 5657.66, "text": " Anyway, take my word for it every other time i've tried it tta has been better", "tokens": [5684, 11, 747, 452, 1349, 337, 309, 633, 661, 565, 741, 600, 3031, 309, 256, 1328, 575, 668, 1101], "temperature": 0.0, "avg_logprob": -0.08235965511663174, "compression_ratio": 1.6541353383458646, "no_speech_prob": 2.0461289750528522e-05}, {"id": 1394, "seek": 565366, "start": 5658.22, "end": 5659.98, "text": " um", "tokens": [1105], "temperature": 0.0, "avg_logprob": -0.08235965511663174, "compression_ratio": 1.6541353383458646, "no_speech_prob": 2.0461289750528522e-05}, {"id": 1395, "seek": 565366, "start": 5659.98, "end": 5662.78, "text": " So then you know now that we've got a pretty good way of", "tokens": [407, 550, 291, 458, 586, 300, 321, 600, 658, 257, 1238, 665, 636, 295], "temperature": 0.0, "avg_logprob": -0.08235965511663174, "compression_ratio": 1.6541353383458646, "no_speech_prob": 2.0461289750528522e-05}, {"id": 1396, "seek": 565366, "start": 5663.32, "end": 5665.24, "text": " um resizing", "tokens": [1105, 725, 3319], "temperature": 0.0, "avg_logprob": -0.08235965511663174, "compression_ratio": 1.6541353383458646, "no_speech_prob": 2.0461289750528522e-05}, {"id": 1397, "seek": 565366, "start": 5665.24, "end": 5668.54, "text": " Um, we've got tta. We've got a good training process", "tokens": [3301, 11, 321, 600, 658, 256, 1328, 13, 492, 600, 658, 257, 665, 3097, 1399], "temperature": 0.0, "avg_logprob": -0.08235965511663174, "compression_ratio": 1.6541353383458646, "no_speech_prob": 2.0461289750528522e-05}, {"id": 1398, "seek": 565366, "start": 5669.58, "end": 5671.58, "text": " Let's just make bigger images", "tokens": [961, 311, 445, 652, 3801, 5267], "temperature": 0.0, "avg_logprob": -0.08235965511663174, "compression_ratio": 1.6541353383458646, "no_speech_prob": 2.0461289750528522e-05}, {"id": 1399, "seek": 565366, "start": 5671.58, "end": 5676.62, "text": " And something that's really interesting and a lot of people don't realize is your images don't have to be square", "tokens": [400, 746, 300, 311, 534, 1880, 293, 257, 688, 295, 561, 500, 380, 4325, 307, 428, 5267, 500, 380, 362, 281, 312, 3732], "temperature": 0.0, "avg_logprob": -0.08235965511663174, "compression_ratio": 1.6541353383458646, "no_speech_prob": 2.0461289750528522e-05}, {"id": 1400, "seek": 565366, "start": 5677.5, "end": 5679.5, "text": " They just all have to be the same size", "tokens": [814, 445, 439, 362, 281, 312, 264, 912, 2744], "temperature": 0.0, "avg_logprob": -0.08235965511663174, "compression_ratio": 1.6541353383458646, "no_speech_prob": 2.0461289750528522e-05}, {"id": 1401, "seek": 567950, "start": 5679.5, "end": 5683.26, "text": " And given that nearly all of our images are 640 by 480", "tokens": [400, 2212, 300, 6217, 439, 295, 527, 5267, 366, 1386, 5254, 538, 1017, 4702], "temperature": 0.0, "avg_logprob": -0.13133744270570816, "compression_ratio": 1.4798206278026906, "no_speech_prob": 6.438725449697813e-06}, {"id": 1402, "seek": 567950, "start": 5683.82, "end": 5686.22, "text": " We can just pick you know that aspect ratio", "tokens": [492, 393, 445, 1888, 291, 458, 300, 4171, 8509], "temperature": 0.0, "avg_logprob": -0.13133744270570816, "compression_ratio": 1.4798206278026906, "no_speech_prob": 6.438725449697813e-06}, {"id": 1403, "seek": 567950, "start": 5687.02, "end": 5690.7, "text": " So for example 256 by 192 and we'll resize everything", "tokens": [407, 337, 1365, 38882, 538, 1294, 17, 293, 321, 603, 50069, 1203], "temperature": 0.0, "avg_logprob": -0.13133744270570816, "compression_ratio": 1.4798206278026906, "no_speech_prob": 6.438725449697813e-06}, {"id": 1404, "seek": 567950, "start": 5691.58, "end": 5693.9, "text": " To the same aspect ratio rectangular", "tokens": [1407, 264, 912, 4171, 8509, 31167], "temperature": 0.0, "avg_logprob": -0.13133744270570816, "compression_ratio": 1.4798206278026906, "no_speech_prob": 6.438725449697813e-06}, {"id": 1405, "seek": 567950, "start": 5694.94, "end": 5698.48, "text": " And that should work even better still so if we do that we do 12 epochs", "tokens": [400, 300, 820, 589, 754, 1101, 920, 370, 498, 321, 360, 300, 321, 360, 2272, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.13133744270570816, "compression_ratio": 1.4798206278026906, "no_speech_prob": 6.438725449697813e-06}, {"id": 1406, "seek": 567950, "start": 5701.74, "end": 5704.3, "text": " Okay, now our error rate's down to 2.2 percent", "tokens": [1033, 11, 586, 527, 6713, 3314, 311, 760, 281, 568, 13, 17, 3043], "temperature": 0.0, "avg_logprob": -0.13133744270570816, "compression_ratio": 1.4798206278026906, "no_speech_prob": 6.438725449697813e-06}, {"id": 1407, "seek": 570430, "start": 5704.3, "end": 5707.68, "text": " And then we'll do tta", "tokens": [400, 550, 321, 603, 360, 256, 1328], "temperature": 0.0, "avg_logprob": -0.13282270016877548, "compression_ratio": 1.4364640883977902, "no_speech_prob": 4.860110038862331e-06}, {"id": 1408, "seek": 570430, "start": 5709.66, "end": 5713.26, "text": " Okay, this time you can see it actually improving down to under 2 percent", "tokens": [1033, 11, 341, 565, 291, 393, 536, 309, 767, 11470, 760, 281, 833, 568, 3043], "temperature": 0.0, "avg_logprob": -0.13282270016877548, "compression_ratio": 1.4364640883977902, "no_speech_prob": 4.860110038862331e-06}, {"id": 1409, "seek": 570430, "start": 5714.46, "end": 5718.9400000000005, "text": " So that's pretty cool, right? We've got our error rate at the start of this notebook. We were at", "tokens": [407, 300, 311, 1238, 1627, 11, 558, 30, 492, 600, 658, 527, 6713, 3314, 412, 264, 722, 295, 341, 21060, 13, 492, 645, 412], "temperature": 0.0, "avg_logprob": -0.13282270016877548, "compression_ratio": 1.4364640883977902, "no_speech_prob": 4.860110038862331e-06}, {"id": 1410, "seek": 570430, "start": 5723.34, "end": 5729.52, "text": " 12 percent and by the time we've got through our little experiments", "tokens": [2272, 3043, 293, 538, 264, 565, 321, 600, 658, 807, 527, 707, 12050], "temperature": 0.0, "avg_logprob": -0.13282270016877548, "compression_ratio": 1.4364640883977902, "no_speech_prob": 4.860110038862331e-06}, {"id": 1411, "seek": 572952, "start": 5729.52, "end": 5731.52, "text": " We're down to under 2 percent", "tokens": [492, 434, 760, 281, 833, 568, 3043], "temperature": 0.0, "avg_logprob": -0.38565577958759506, "compression_ratio": 1.5751295336787565, "no_speech_prob": 5.7714537433639634e-06}, {"id": 1412, "seek": 572952, "start": 5732.400000000001, "end": 5735.76, "text": " And nothing about this is in any way specific to", "tokens": [400, 1825, 466, 341, 307, 294, 604, 636, 2685, 281], "temperature": 0.0, "avg_logprob": -0.38565577958759506, "compression_ratio": 1.5751295336787565, "no_speech_prob": 5.7714537433639634e-06}, {"id": 1413, "seek": 572952, "start": 5736.72, "end": 5741.280000000001, "text": " rice or this competition, you know, it's like this is a very", "tokens": [5090, 420, 341, 6211, 11, 291, 458, 11, 309, 311, 411, 341, 307, 257, 588], "temperature": 0.0, "avg_logprob": -0.38565577958759506, "compression_ratio": 1.5751295336787565, "no_speech_prob": 5.7714537433639634e-06}, {"id": 1414, "seek": 572952, "start": 5743.1, "end": 5746.080000000001, "text": " Mechanistic, you know standardized", "tokens": [30175, 3142, 11, 291, 458, 31677], "temperature": 0.0, "avg_logprob": -0.38565577958759506, "compression_ratio": 1.5751295336787565, "no_speech_prob": 5.7714537433639634e-06}, {"id": 1415, "seek": 572952, "start": 5747.4400000000005, "end": 5748.88, "text": " approach", "tokens": [3109], "temperature": 0.0, "avg_logprob": -0.38565577958759506, "compression_ratio": 1.5751295336787565, "no_speech_prob": 5.7714537433639634e-06}, {"id": 1416, "seek": 572952, "start": 5748.88, "end": 5750.88, "text": " which you can use for", "tokens": [597, 291, 393, 764, 337], "temperature": 0.0, "avg_logprob": -0.38565577958759506, "compression_ratio": 1.5751295336787565, "no_speech_prob": 5.7714537433639634e-06}, {"id": 1417, "seek": 572952, "start": 5751.4400000000005, "end": 5756.320000000001, "text": " Certainly any kind of this type of computer vision competition and computer vision data set almost", "tokens": [16628, 604, 733, 295, 341, 2010, 295, 3820, 5201, 6211, 293, 3820, 5201, 1412, 992, 1920], "temperature": 0.0, "avg_logprob": -0.38565577958759506, "compression_ratio": 1.5751295336787565, "no_speech_prob": 5.7714537433639634e-06}, {"id": 1418, "seek": 575632, "start": 5756.32, "end": 5762.5599999999995, "text": " But you know, it looked very similar for a collaborative filtering model a tabular model nlp model whatever", "tokens": [583, 291, 458, 11, 309, 2956, 588, 2531, 337, 257, 16555, 30822, 2316, 257, 4421, 1040, 2316, 297, 75, 79, 2316, 2035], "temperature": 0.0, "avg_logprob": -0.24608228720870673, "compression_ratio": 1.5856573705179282, "no_speech_prob": 1.300474104937166e-05}, {"id": 1419, "seek": 575632, "start": 5764.5599999999995, "end": 5767.5199999999995, "text": " So, of course again, I want to submit as soon as I can so", "tokens": [407, 11, 295, 1164, 797, 11, 286, 528, 281, 10315, 382, 2321, 382, 286, 393, 370], "temperature": 0.0, "avg_logprob": -0.24608228720870673, "compression_ratio": 1.5856573705179282, "no_speech_prob": 1.300474104937166e-05}, {"id": 1420, "seek": 575632, "start": 5768.08, "end": 5772.96, "text": " Just copy and paste the exact same steps. I took last time basically for creating a submission", "tokens": [1449, 5055, 293, 9163, 264, 1900, 912, 4439, 13, 286, 1890, 1036, 565, 1936, 337, 4084, 257, 23689], "temperature": 0.0, "avg_logprob": -0.24608228720870673, "compression_ratio": 1.5856573705179282, "no_speech_prob": 1.300474104937166e-05}, {"id": 1421, "seek": 575632, "start": 5775.36, "end": 5778.719999999999, "text": " So as I said last time we did it using pandas, but there's actually an easier way", "tokens": [407, 382, 286, 848, 1036, 565, 321, 630, 309, 1228, 4565, 296, 11, 457, 456, 311, 767, 364, 3571, 636], "temperature": 0.0, "avg_logprob": -0.24608228720870673, "compression_ratio": 1.5856573705179282, "no_speech_prob": 1.300474104937166e-05}, {"id": 1422, "seek": 575632, "start": 5779.599999999999, "end": 5782.719999999999, "text": " So the step where here i've got the numbers from 0 to 9", "tokens": [407, 264, 1823, 689, 510, 741, 600, 658, 264, 3547, 490, 1958, 281, 1722], "temperature": 0.0, "avg_logprob": -0.24608228720870673, "compression_ratio": 1.5856573705179282, "no_speech_prob": 1.300474104937166e-05}, {"id": 1423, "seek": 578272, "start": 5782.72, "end": 5786.0, "text": " Which is like which which rice disease is it?", "tokens": [3013, 307, 411, 597, 597, 5090, 4752, 307, 309, 30], "temperature": 0.0, "avg_logprob": -0.27059797638828315, "compression_ratio": 1.8333333333333333, "no_speech_prob": 2.355146898480598e-05}, {"id": 1424, "seek": 578272, "start": 5786.8, "end": 5788.8, "text": " So here's a cute idea", "tokens": [407, 510, 311, 257, 4052, 1558], "temperature": 0.0, "avg_logprob": -0.27059797638828315, "compression_ratio": 1.8333333333333333, "no_speech_prob": 2.355146898480598e-05}, {"id": 1425, "seek": 578272, "start": 5788.88, "end": 5790.88, "text": " We can take our vocab", "tokens": [492, 393, 747, 527, 2329, 455], "temperature": 0.0, "avg_logprob": -0.27059797638828315, "compression_ratio": 1.8333333333333333, "no_speech_prob": 2.355146898480598e-05}, {"id": 1426, "seek": 578272, "start": 5791.2, "end": 5794.4800000000005, "text": " And make it an array. So that's going to be a list of 10 things", "tokens": [400, 652, 309, 364, 10225, 13, 407, 300, 311, 516, 281, 312, 257, 1329, 295, 1266, 721], "temperature": 0.0, "avg_logprob": -0.27059797638828315, "compression_ratio": 1.8333333333333333, "no_speech_prob": 2.355146898480598e-05}, {"id": 1427, "seek": 578272, "start": 5796.0, "end": 5800.4800000000005, "text": " And then we can index into that vocab with our indices", "tokens": [400, 550, 321, 393, 8186, 666, 300, 2329, 455, 365, 527, 43840], "temperature": 0.0, "avg_logprob": -0.27059797638828315, "compression_ratio": 1.8333333333333333, "no_speech_prob": 2.355146898480598e-05}, {"id": 1428, "seek": 578272, "start": 5801.04, "end": 5803.360000000001, "text": " Which is kind of weird. This is a list of 10 things", "tokens": [3013, 307, 733, 295, 3657, 13, 639, 307, 257, 1329, 295, 1266, 721], "temperature": 0.0, "avg_logprob": -0.27059797638828315, "compression_ratio": 1.8333333333333333, "no_speech_prob": 2.355146898480598e-05}, {"id": 1429, "seek": 578272, "start": 5804.08, "end": 5806.64, "text": " This is a list of other four or five thousand things", "tokens": [639, 307, 257, 1329, 295, 661, 1451, 420, 1732, 4714, 721], "temperature": 0.0, "avg_logprob": -0.27059797638828315, "compression_ratio": 1.8333333333333333, "no_speech_prob": 2.355146898480598e-05}, {"id": 1430, "seek": 578272, "start": 5807.04, "end": 5809.6, "text": " So this will give me four or five thousand things", "tokens": [407, 341, 486, 976, 385, 1451, 420, 1732, 4714, 721], "temperature": 0.0, "avg_logprob": -0.27059797638828315, "compression_ratio": 1.8333333333333333, "no_speech_prob": 2.355146898480598e-05}, {"id": 1431, "seek": 580960, "start": 5809.6, "end": 5813.120000000001, "text": " So this will give me four or five thousand results, which is", "tokens": [407, 341, 486, 976, 385, 1451, 420, 1732, 4714, 3542, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.3020069355867347, "compression_ratio": 1.5947136563876652, "no_speech_prob": 7.811212299202452e-07}, {"id": 1432, "seek": 580960, "start": 5814.0, "end": 5816.0, "text": " Each vocab item for that thing", "tokens": [6947, 2329, 455, 3174, 337, 300, 551], "temperature": 0.0, "avg_logprob": -0.3020069355867347, "compression_ratio": 1.5947136563876652, "no_speech_prob": 7.811212299202452e-07}, {"id": 1433, "seek": 580960, "start": 5816.400000000001, "end": 5820.240000000001, "text": " So this is another way of doing the same mapping and I would", "tokens": [407, 341, 307, 1071, 636, 295, 884, 264, 912, 18350, 293, 286, 576], "temperature": 0.0, "avg_logprob": -0.3020069355867347, "compression_ratio": 1.5947136563876652, "no_speech_prob": 7.811212299202452e-07}, {"id": 1434, "seek": 580960, "start": 5821.04, "end": 5822.08, "text": " spend time", "tokens": [3496, 565], "temperature": 0.0, "avg_logprob": -0.3020069355867347, "compression_ratio": 1.5947136563876652, "no_speech_prob": 7.811212299202452e-07}, {"id": 1435, "seek": 580960, "start": 5822.4800000000005, "end": 5826.240000000001, "text": " playing with this code to understand what it does because it's the kind of like", "tokens": [2433, 365, 341, 3089, 281, 1223, 437, 309, 775, 570, 309, 311, 264, 733, 295, 411], "temperature": 0.0, "avg_logprob": -0.3020069355867347, "compression_ratio": 1.5947136563876652, "no_speech_prob": 7.811212299202452e-07}, {"id": 1436, "seek": 580960, "start": 5827.280000000001, "end": 5831.6, "text": " Very fast what you know, not just in terms of writing but this this", "tokens": [4372, 2370, 437, 291, 458, 11, 406, 445, 294, 2115, 295, 3579, 457, 341, 341], "temperature": 0.0, "avg_logprob": -0.3020069355867347, "compression_ratio": 1.5947136563876652, "no_speech_prob": 7.811212299202452e-07}, {"id": 1437, "seek": 580960, "start": 5832.0, "end": 5834.0, "text": " the this would uh", "tokens": [264, 341, 576, 2232], "temperature": 0.0, "avg_logprob": -0.3020069355867347, "compression_ratio": 1.5947136563876652, "no_speech_prob": 7.811212299202452e-07}, {"id": 1438, "seek": 580960, "start": 5834.08, "end": 5836.64, "text": " Optimize, you know on on the cpu", "tokens": [35013, 1125, 11, 291, 458, 322, 322, 264, 269, 34859], "temperature": 0.0, "avg_logprob": -0.3020069355867347, "compression_ratio": 1.5947136563876652, "no_speech_prob": 7.811212299202452e-07}, {"id": 1439, "seek": 583664, "start": 5836.64, "end": 5840.160000000001, "text": " Very very well. Um, so this is the kind of coding you want to get used to", "tokens": [4372, 588, 731, 13, 3301, 11, 370, 341, 307, 264, 733, 295, 17720, 291, 528, 281, 483, 1143, 281], "temperature": 0.0, "avg_logprob": -0.3088035209506166, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.3081385077384766e-06}, {"id": 1440, "seek": 583664, "start": 5840.88, "end": 5842.88, "text": " This kind of indexing", "tokens": [639, 733, 295, 8186, 278], "temperature": 0.0, "avg_logprob": -0.3088035209506166, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.3081385077384766e-06}, {"id": 1441, "seek": 583664, "start": 5844.4800000000005, "end": 5847.12, "text": " Anyway, so then we can submit it just like last time", "tokens": [5684, 11, 370, 550, 321, 393, 10315, 309, 445, 411, 1036, 565], "temperature": 0.0, "avg_logprob": -0.3088035209506166, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.3081385077384766e-06}, {"id": 1442, "seek": 583664, "start": 5847.92, "end": 5849.92, "text": " and when I did that", "tokens": [293, 562, 286, 630, 300], "temperature": 0.0, "avg_logprob": -0.3088035209506166, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.3081385077384766e-06}, {"id": 1443, "seek": 583664, "start": 5849.92, "end": 5851.92, "text": " I got in the top 25 percent", "tokens": [286, 658, 294, 264, 1192, 3552, 3043], "temperature": 0.0, "avg_logprob": -0.3088035209506166, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.3081385077384766e-06}, {"id": 1444, "seek": 583664, "start": 5852.4800000000005, "end": 5858.8, "text": " And that's that's where you want to be right? Like generally speaking. I find in cacl competitions the top 25 percent", "tokens": [400, 300, 311, 300, 311, 689, 291, 528, 281, 312, 558, 30, 1743, 5101, 4124, 13, 286, 915, 294, 269, 326, 75, 26185, 264, 1192, 3552, 3043], "temperature": 0.0, "avg_logprob": -0.3088035209506166, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.3081385077384766e-06}, {"id": 1445, "seek": 583664, "start": 5859.6, "end": 5861.6, "text": " is like", "tokens": [307, 411], "temperature": 0.0, "avg_logprob": -0.3088035209506166, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.3081385077384766e-06}, {"id": 1446, "seek": 583664, "start": 5861.84, "end": 5864.72, "text": " You're kind of like solid competitive", "tokens": [509, 434, 733, 295, 411, 5100, 10043], "temperature": 0.0, "avg_logprob": -0.3088035209506166, "compression_ratio": 1.6216216216216217, "no_speech_prob": 1.3081385077384766e-06}, {"id": 1447, "seek": 586472, "start": 5864.72, "end": 5868.4800000000005, "text": " level, you know, look just not to say like it's not easy", "tokens": [1496, 11, 291, 458, 11, 574, 445, 406, 281, 584, 411, 309, 311, 406, 1858], "temperature": 0.0, "avg_logprob": -0.31385413368979653, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.961775852687424e-06}, {"id": 1448, "seek": 586472, "start": 5869.280000000001, "end": 5871.280000000001, "text": " You've got to know what you're doing", "tokens": [509, 600, 658, 281, 458, 437, 291, 434, 884], "temperature": 0.0, "avg_logprob": -0.31385413368979653, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.961775852687424e-06}, {"id": 1449, "seek": 586472, "start": 5871.280000000001, "end": 5876.08, "text": " But if you get in the top 25 percent, I think you can really feel like yeah, this is this is a", "tokens": [583, 498, 291, 483, 294, 264, 1192, 3552, 3043, 11, 286, 519, 291, 393, 534, 841, 411, 1338, 11, 341, 307, 341, 307, 257], "temperature": 0.0, "avg_logprob": -0.31385413368979653, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.961775852687424e-06}, {"id": 1450, "seek": 586472, "start": 5876.88, "end": 5878.88, "text": " you know very", "tokens": [291, 458, 588], "temperature": 0.0, "avg_logprob": -0.31385413368979653, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.961775852687424e-06}, {"id": 1451, "seek": 586472, "start": 5879.18, "end": 5880.400000000001, "text": " reasonable", "tokens": [10585], "temperature": 0.0, "avg_logprob": -0.31385413368979653, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.961775852687424e-06}, {"id": 1452, "seek": 586472, "start": 5880.400000000001, "end": 5883.76, "text": " Attempt and so that's I think this is a very reasonable attempt", "tokens": [7298, 4543, 293, 370, 300, 311, 286, 519, 341, 307, 257, 588, 10585, 5217], "temperature": 0.0, "avg_logprob": -0.31385413368979653, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.961775852687424e-06}, {"id": 1453, "seek": 586472, "start": 5885.360000000001, "end": 5887.84, "text": " Okay before we wrap up john any last questions", "tokens": [1033, 949, 321, 7019, 493, 35097, 604, 1036, 1651], "temperature": 0.0, "avg_logprob": -0.31385413368979653, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.961775852687424e-06}, {"id": 1454, "seek": 588784, "start": 5887.84, "end": 5893.4400000000005, "text": " Um, yeah, there's this there's two I think that would be good if we could touch on quickly before we wrap up", "tokens": [3301, 11, 1338, 11, 456, 311, 341, 456, 311, 732, 286, 519, 300, 576, 312, 665, 498, 321, 727, 2557, 322, 2661, 949, 321, 7019, 493], "temperature": 0.0, "avg_logprob": -0.4105686486936083, "compression_ratio": 1.759493670886076, "no_speech_prob": 2.7531876185094006e-05}, {"id": 1455, "seek": 588784, "start": 5893.4400000000005, "end": 5896.08, "text": " Um one from victor asking about tta", "tokens": [3301, 472, 490, 4403, 284, 3365, 466, 256, 1328], "temperature": 0.0, "avg_logprob": -0.4105686486936083, "compression_ratio": 1.759493670886076, "no_speech_prob": 2.7531876185094006e-05}, {"id": 1456, "seek": 588784, "start": 5897.4400000000005, "end": 5898.400000000001, "text": " Um", "tokens": [3301], "temperature": 0.0, "avg_logprob": -0.4105686486936083, "compression_ratio": 1.759493670886076, "no_speech_prob": 2.7531876185094006e-05}, {"id": 1457, "seek": 588784, "start": 5898.400000000001, "end": 5905.360000000001, "text": " When I use tta during my training process do I need to do something special during inference or is this something you use only during", "tokens": [1133, 286, 764, 256, 1328, 1830, 452, 3097, 1399, 360, 286, 643, 281, 360, 746, 2121, 1830, 38253, 420, 307, 341, 746, 291, 764, 787, 1830], "temperature": 0.0, "avg_logprob": -0.4105686486936083, "compression_ratio": 1.759493670886076, "no_speech_prob": 2.7531876185094006e-05}, {"id": 1458, "seek": 588784, "start": 5905.92, "end": 5907.92, "text": " Okay, so just explain", "tokens": [1033, 11, 370, 445, 2903], "temperature": 0.0, "avg_logprob": -0.4105686486936083, "compression_ratio": 1.759493670886076, "no_speech_prob": 2.7531876185094006e-05}, {"id": 1459, "seek": 588784, "start": 5908.4800000000005, "end": 5913.6, "text": " Tta means test time augmentation. So specifically it means that you have to do something special during inference", "tokens": [314, 1328, 1355, 1500, 565, 14501, 19631, 13, 407, 4682, 309, 1355, 300, 291, 362, 281, 360, 746, 2121, 1830, 38253], "temperature": 0.0, "avg_logprob": -0.4105686486936083, "compression_ratio": 1.759493670886076, "no_speech_prob": 2.7531876185094006e-05}, {"id": 1460, "seek": 591360, "start": 5913.6, "end": 5921.120000000001, "text": " So yeah, so during training you basically always do augmentation, which means you're varying each image slightly", "tokens": [407, 1338, 11, 370, 1830, 3097, 291, 1936, 1009, 360, 14501, 19631, 11, 597, 1355, 291, 434, 22984, 1184, 3256, 4748], "temperature": 0.0, "avg_logprob": -0.3276278404962449, "compression_ratio": 1.5853658536585367, "no_speech_prob": 5.50715958524961e-06}, {"id": 1461, "seek": 591360, "start": 5921.92, "end": 5923.68, "text": " So that the", "tokens": [407, 300, 264], "temperature": 0.0, "avg_logprob": -0.3276278404962449, "compression_ratio": 1.5853658536585367, "no_speech_prob": 5.50715958524961e-06}, {"id": 1462, "seek": 591360, "start": 5923.68, "end": 5928.320000000001, "text": " Model never seems the same image exactly the same twice and so it can't memorize it", "tokens": [17105, 1128, 2544, 264, 912, 3256, 2293, 264, 912, 6091, 293, 370, 309, 393, 380, 27478, 309], "temperature": 0.0, "avg_logprob": -0.3276278404962449, "compression_ratio": 1.5853658536585367, "no_speech_prob": 5.50715958524961e-06}, {"id": 1463, "seek": 591360, "start": 5930.320000000001, "end": 5935.76, "text": " On fast ai and as I say, I don't think anybody else does this as far as I know if you call tta", "tokens": [1282, 2370, 9783, 293, 382, 286, 584, 11, 286, 500, 380, 519, 4472, 1646, 775, 341, 382, 1400, 382, 286, 458, 498, 291, 818, 256, 1328], "temperature": 0.0, "avg_logprob": -0.3276278404962449, "compression_ratio": 1.5853658536585367, "no_speech_prob": 5.50715958524961e-06}, {"id": 1464, "seek": 591360, "start": 5936.56, "end": 5938.56, "text": " You can't memorize it", "tokens": [509, 393, 380, 27478, 309], "temperature": 0.0, "avg_logprob": -0.3276278404962449, "compression_ratio": 1.5853658536585367, "no_speech_prob": 5.50715958524961e-06}, {"id": 1465, "seek": 593856, "start": 5938.56, "end": 5943.68, "text": " The exact same augmentation approach on whatever data set you pass it", "tokens": [440, 1900, 912, 14501, 19631, 3109, 322, 2035, 1412, 992, 291, 1320, 309], "temperature": 0.0, "avg_logprob": -0.42236257232395946, "compression_ratio": 1.9259259259259258, "no_speech_prob": 8.267091288871597e-06}, {"id": 1466, "seek": 593856, "start": 5944.56, "end": 5949.76, "text": " And average out the prediction but but like multiple times on the same image and will average them out", "tokens": [400, 4274, 484, 264, 17630, 457, 457, 411, 3866, 1413, 322, 264, 912, 3256, 293, 486, 4274, 552, 484], "temperature": 0.0, "avg_logprob": -0.42236257232395946, "compression_ratio": 1.9259259259259258, "no_speech_prob": 8.267091288871597e-06}, {"id": 1467, "seek": 593856, "start": 5949.76, "end": 5951.76, "text": " So you don't have to do anything different", "tokens": [407, 291, 500, 380, 362, 281, 360, 1340, 819], "temperature": 0.0, "avg_logprob": -0.42236257232395946, "compression_ratio": 1.9259259259259258, "no_speech_prob": 8.267091288871597e-06}, {"id": 1468, "seek": 593856, "start": 5952.080000000001, "end": 5956.72, "text": " But if you didn't have any data augmentation in training, you can't use tta. It uses the same", "tokens": [583, 498, 291, 994, 380, 362, 604, 1412, 14501, 19631, 294, 3097, 11, 291, 393, 380, 764, 256, 1328, 13, 467, 4960, 264, 912], "temperature": 0.0, "avg_logprob": -0.42236257232395946, "compression_ratio": 1.9259259259259258, "no_speech_prob": 8.267091288871597e-06}, {"id": 1469, "seek": 593856, "start": 5957.280000000001, "end": 5959.76, "text": " By default the same data augmentation you use for training", "tokens": [3146, 7576, 264, 912, 1412, 14501, 19631, 291, 764, 337, 3097], "temperature": 0.0, "avg_logprob": -0.42236257232395946, "compression_ratio": 1.9259259259259258, "no_speech_prob": 8.267091288871597e-06}, {"id": 1470, "seek": 593856, "start": 5960.56, "end": 5961.68, "text": " Great. Thank you", "tokens": [3769, 13, 1044, 291], "temperature": 0.0, "avg_logprob": -0.42236257232395946, "compression_ratio": 1.9259259259259258, "no_speech_prob": 8.267091288871597e-06}, {"id": 1471, "seek": 593856, "start": 5961.68, "end": 5966.080000000001, "text": " And then the last thing is that you can use the same data augmentation in training", "tokens": [400, 550, 264, 1036, 551, 307, 300, 291, 393, 764, 264, 912, 1412, 14501, 19631, 294, 3097], "temperature": 0.0, "avg_logprob": -0.42236257232395946, "compression_ratio": 1.9259259259259258, "no_speech_prob": 8.267091288871597e-06}, {"id": 1472, "seek": 596608, "start": 5966.08, "end": 5969.76, "text": " Great. Thank you. Um, and the other one is about how", "tokens": [3769, 13, 1044, 291, 13, 3301, 11, 293, 264, 661, 472, 307, 466, 577], "temperature": 0.0, "avg_logprob": -0.26365561234323603, "compression_ratio": 1.676595744680851, "no_speech_prob": 1.644120493438095e-05}, {"id": 1473, "seek": 596608, "start": 5970.64, "end": 5978.88, "text": " You know when you first started this example you squared the models and the images rather and you talked about squashing versus cropping versus, you know clipping and", "tokens": [509, 458, 562, 291, 700, 1409, 341, 1365, 291, 8889, 264, 5245, 293, 264, 5267, 2831, 293, 291, 2825, 466, 2339, 11077, 5717, 4848, 3759, 5717, 11, 291, 458, 49320, 293], "temperature": 0.0, "avg_logprob": -0.26365561234323603, "compression_ratio": 1.676595744680851, "no_speech_prob": 1.644120493438095e-05}, {"id": 1474, "seek": 596608, "start": 5979.68, "end": 5982.88, "text": " Scaling and so on but then you went on to say that", "tokens": [2747, 4270, 293, 370, 322, 457, 550, 291, 1437, 322, 281, 584, 300], "temperature": 0.0, "avg_logprob": -0.26365561234323603, "compression_ratio": 1.676595744680851, "no_speech_prob": 1.644120493438095e-05}, {"id": 1475, "seek": 596608, "start": 5984.0, "end": 5991.44, "text": " These models can actually take rectangular inputs, right? So there's a question that's kind of probing it at that, you know", "tokens": [1981, 5245, 393, 767, 747, 31167, 15743, 11, 558, 30, 407, 456, 311, 257, 1168, 300, 311, 733, 295, 1239, 278, 309, 412, 300, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.26365561234323603, "compression_ratio": 1.676595744680851, "no_speech_prob": 1.644120493438095e-05}, {"id": 1476, "seek": 599144, "start": 5991.44, "end": 5996.48, "text": " If the if the models can take rectangular inputs, why would you ever?", "tokens": [759, 264, 498, 264, 5245, 393, 747, 31167, 15743, 11, 983, 576, 291, 1562, 30], "temperature": 0.0, "avg_logprob": -0.28704936504364015, "compression_ratio": 1.4793814432989691, "no_speech_prob": 6.438873697334202e-06}, {"id": 1477, "seek": 599144, "start": 5997.04, "end": 6000.96, "text": " Even care as long as they're all the same size. So", "tokens": [2754, 1127, 382, 938, 382, 436, 434, 439, 264, 912, 2744, 13, 407], "temperature": 0.0, "avg_logprob": -0.28704936504364015, "compression_ratio": 1.4793814432989691, "no_speech_prob": 6.438873697334202e-06}, {"id": 1478, "seek": 599144, "start": 6002.719999999999, "end": 6004.719999999999, "text": " I find most of the time", "tokens": [286, 915, 881, 295, 264, 565], "temperature": 0.0, "avg_logprob": -0.28704936504364015, "compression_ratio": 1.4793814432989691, "no_speech_prob": 6.438873697334202e-06}, {"id": 1479, "seek": 599144, "start": 6005.04, "end": 6009.919999999999, "text": " Data sets tend to have a wide variety of input sizes and aspect ratios", "tokens": [11888, 6352, 3928, 281, 362, 257, 4874, 5673, 295, 4846, 11602, 293, 4171, 32435], "temperature": 0.0, "avg_logprob": -0.28704936504364015, "compression_ratio": 1.4793814432989691, "no_speech_prob": 6.438873697334202e-06}, {"id": 1480, "seek": 599144, "start": 6010.879999999999, "end": 6012.4, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.28704936504364015, "compression_ratio": 1.4793814432989691, "no_speech_prob": 6.438873697334202e-06}, {"id": 1481, "seek": 599144, "start": 6012.4, "end": 6016.5599999999995, "text": " You know if there's just as many tall skinny ones as wide", "tokens": [509, 458, 498, 456, 311, 445, 382, 867, 6764, 25193, 2306, 382, 4874], "temperature": 0.0, "avg_logprob": -0.28704936504364015, "compression_ratio": 1.4793814432989691, "no_speech_prob": 6.438873697334202e-06}, {"id": 1482, "seek": 599144, "start": 6017.679999999999, "end": 6019.679999999999, "text": " short ones", "tokens": [2099, 2306], "temperature": 0.0, "avg_logprob": -0.28704936504364015, "compression_ratio": 1.4793814432989691, "no_speech_prob": 6.438873697334202e-06}, {"id": 1483, "seek": 601968, "start": 6019.68, "end": 6025.04, "text": " You know, you doesn't make sense to create a rectangle because some of them you're going to really destroy them", "tokens": [509, 458, 11, 291, 1177, 380, 652, 2020, 281, 1884, 257, 21930, 570, 512, 295, 552, 291, 434, 516, 281, 534, 5293, 552], "temperature": 0.0, "avg_logprob": -0.2618417926863128, "compression_ratio": 1.5772357723577235, "no_speech_prob": 2.4679506168467924e-05}, {"id": 1484, "seek": 601968, "start": 6025.04, "end": 6027.04, "text": " So a square is the kind of", "tokens": [407, 257, 3732, 307, 264, 733, 295], "temperature": 0.0, "avg_logprob": -0.2618417926863128, "compression_ratio": 1.5772357723577235, "no_speech_prob": 2.4679506168467924e-05}, {"id": 1485, "seek": 601968, "start": 6027.84, "end": 6029.84, "text": " best compromise in some ways", "tokens": [1151, 18577, 294, 512, 2098], "temperature": 0.0, "avg_logprob": -0.2618417926863128, "compression_ratio": 1.5772357723577235, "no_speech_prob": 2.4679506168467924e-05}, {"id": 1486, "seek": 601968, "start": 6031.12, "end": 6032.64, "text": " There are", "tokens": [821, 366], "temperature": 0.0, "avg_logprob": -0.2618417926863128, "compression_ratio": 1.5772357723577235, "no_speech_prob": 2.4679506168467924e-05}, {"id": 1487, "seek": 601968, "start": 6032.64, "end": 6034.64, "text": " better things we can do", "tokens": [1101, 721, 321, 393, 360], "temperature": 0.0, "avg_logprob": -0.2618417926863128, "compression_ratio": 1.5772357723577235, "no_speech_prob": 2.4679506168467924e-05}, {"id": 1488, "seek": 601968, "start": 6035.68, "end": 6036.72, "text": " Which", "tokens": [3013], "temperature": 0.0, "avg_logprob": -0.2618417926863128, "compression_ratio": 1.5772357723577235, "no_speech_prob": 2.4679506168467924e-05}, {"id": 1489, "seek": 601968, "start": 6036.72, "end": 6038.72, "text": " We don't have any", "tokens": [492, 500, 380, 362, 604], "temperature": 0.0, "avg_logprob": -0.2618417926863128, "compression_ratio": 1.5772357723577235, "no_speech_prob": 2.4679506168467924e-05}, {"id": 1490, "seek": 601968, "start": 6038.72, "end": 6043.84, "text": " Off the shelf library support for yet and I don't think I don't know that anybody else has even published about this", "tokens": [6318, 264, 15222, 6405, 1406, 337, 1939, 293, 286, 500, 380, 519, 286, 500, 380, 458, 300, 4472, 1646, 575, 754, 6572, 466, 341], "temperature": 0.0, "avg_logprob": -0.2618417926863128, "compression_ratio": 1.5772357723577235, "no_speech_prob": 2.4679506168467924e-05}, {"id": 1491, "seek": 601968, "start": 6043.84, "end": 6046.240000000001, "text": " But we've experimented with kind of trying to", "tokens": [583, 321, 600, 5120, 292, 365, 733, 295, 1382, 281], "temperature": 0.0, "avg_logprob": -0.2618417926863128, "compression_ratio": 1.5772357723577235, "no_speech_prob": 2.4679506168467924e-05}, {"id": 1492, "seek": 604624, "start": 6046.24, "end": 6049.92, "text": " Batch things that are similar aspect ratios together and use", "tokens": [363, 852, 721, 300, 366, 2531, 4171, 32435, 1214, 293, 764], "temperature": 0.0, "avg_logprob": -0.29473908414545746, "compression_ratio": 1.5797665369649805, "no_speech_prob": 2.0780329577974044e-05}, {"id": 1493, "seek": 604624, "start": 6050.4, "end": 6056.16, "text": " The kind of median rectangle for those and have had some good results with that. But honestly", "tokens": [440, 733, 295, 26779, 21930, 337, 729, 293, 362, 632, 512, 665, 3542, 365, 300, 13, 583, 6095], "temperature": 0.0, "avg_logprob": -0.29473908414545746, "compression_ratio": 1.5797665369649805, "no_speech_prob": 2.0780329577974044e-05}, {"id": 1494, "seek": 604624, "start": 6057.42, "end": 6062.48, "text": " 99.999 percent of people given a wide variety of aspect ratios chuck everything into a square", "tokens": [11803, 13, 49017, 3043, 295, 561, 2212, 257, 4874, 5673, 295, 4171, 32435, 20870, 1203, 666, 257, 3732], "temperature": 0.0, "avg_logprob": -0.29473908414545746, "compression_ratio": 1.5797665369649805, "no_speech_prob": 2.0780329577974044e-05}, {"id": 1495, "seek": 604624, "start": 6064.0, "end": 6067.5199999999995, "text": " A follow-up just this is my own interest. Have you ever looked at", "tokens": [316, 1524, 12, 1010, 445, 341, 307, 452, 1065, 1179, 13, 3560, 291, 1562, 2956, 412], "temperature": 0.0, "avg_logprob": -0.29473908414545746, "compression_ratio": 1.5797665369649805, "no_speech_prob": 2.0780329577974044e-05}, {"id": 1496, "seek": 604624, "start": 6068.5599999999995, "end": 6073.12, "text": " You know, so the issue with um with padding as you say is that you're putting you know, big", "tokens": [509, 458, 11, 370, 264, 2734, 365, 1105, 365, 39562, 382, 291, 584, 307, 300, 291, 434, 3372, 291, 458, 11, 955], "temperature": 0.0, "avg_logprob": -0.29473908414545746, "compression_ratio": 1.5797665369649805, "no_speech_prob": 2.0780329577974044e-05}, {"id": 1497, "seek": 607312, "start": 6073.12, "end": 6080.64, "text": " Black pixels there. Those are not nands. Those are black pixels. That's right. There's here and so there's there's something problematic", "tokens": [4076, 18668, 456, 13, 3950, 366, 406, 297, 2967, 13, 3950, 366, 2211, 18668, 13, 663, 311, 558, 13, 821, 311, 510, 293, 370, 456, 311, 456, 311, 746, 19011], "temperature": 0.0, "avg_logprob": -0.34692245059543186, "compression_ratio": 1.6470588235294117, "no_speech_prob": 4.610548785421997e-05}, {"id": 1498, "seek": 607312, "start": 6081.36, "end": 6083.36, "text": " To me, you know conceptually about that", "tokens": [1407, 385, 11, 291, 458, 3410, 671, 466, 300], "temperature": 0.0, "avg_logprob": -0.34692245059543186, "compression_ratio": 1.6470588235294117, "no_speech_prob": 4.610548785421997e-05}, {"id": 1499, "seek": 607312, "start": 6085.28, "end": 6088.08, "text": " You know when you when you see uh, for example", "tokens": [509, 458, 562, 291, 562, 291, 536, 2232, 11, 337, 1365], "temperature": 0.0, "avg_logprob": -0.34692245059543186, "compression_ratio": 1.6470588235294117, "no_speech_prob": 4.610548785421997e-05}, {"id": 1500, "seek": 607312, "start": 6088.8, "end": 6091.28, "text": " Four to three aspect ratio footage", "tokens": [7451, 281, 1045, 4171, 8509, 9556], "temperature": 0.0, "avg_logprob": -0.34692245059543186, "compression_ratio": 1.6470588235294117, "no_speech_prob": 4.610548785421997e-05}, {"id": 1501, "seek": 607312, "start": 6092.0, "end": 6097.28, "text": " Presented for broadcast on 16 to 9 you get the kind of the blurred stretch that kind of stuff now", "tokens": [2718, 6003, 337, 9975, 322, 3165, 281, 1722, 291, 483, 264, 733, 295, 264, 43525, 5985, 300, 733, 295, 1507, 586], "temperature": 0.0, "avg_logprob": -0.34692245059543186, "compression_ratio": 1.6470588235294117, "no_speech_prob": 4.610548785421997e-05}, {"id": 1502, "seek": 607312, "start": 6097.28, "end": 6100.08, "text": " We've played with that a lot. Yeah, I used to be really into it", "tokens": [492, 600, 3737, 365, 300, 257, 688, 13, 865, 11, 286, 1143, 281, 312, 534, 666, 309], "temperature": 0.0, "avg_logprob": -0.34692245059543186, "compression_ratio": 1.6470588235294117, "no_speech_prob": 4.610548785421997e-05}, {"id": 1503, "seek": 610008, "start": 6100.08, "end": 6104.8, "text": " Actually and fastai still by default uses reflection padding", "tokens": [5135, 293, 2370, 1301, 920, 538, 7576, 4960, 12914, 39562], "temperature": 0.0, "avg_logprob": -0.2631821055988689, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.9523507944541052e-05}, {"id": 1504, "seek": 610008, "start": 6105.12, "end": 6108.48, "text": " Which means if this is I don't know that say this is a 20 pixel wide thing", "tokens": [3013, 1355, 498, 341, 307, 286, 500, 380, 458, 300, 584, 341, 307, 257, 945, 19261, 4874, 551], "temperature": 0.0, "avg_logprob": -0.2631821055988689, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.9523507944541052e-05}, {"id": 1505, "seek": 610008, "start": 6108.72, "end": 6112.16, "text": " it takes the 20 pixels next to it and flips it over and sticks it here and", "tokens": [309, 2516, 264, 945, 18668, 958, 281, 309, 293, 40249, 309, 670, 293, 12518, 309, 510, 293], "temperature": 0.0, "avg_logprob": -0.2631821055988689, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.9523507944541052e-05}, {"id": 1506, "seek": 610008, "start": 6113.2, "end": 6120.48, "text": " It looks pretty good. You know, another one is copy which simply takes the outside pixel and it's a bit more like tv", "tokens": [467, 1542, 1238, 665, 13, 509, 458, 11, 1071, 472, 307, 5055, 597, 2935, 2516, 264, 2380, 19261, 293, 309, 311, 257, 857, 544, 411, 16364], "temperature": 0.0, "avg_logprob": -0.2631821055988689, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.9523507944541052e-05}, {"id": 1507, "seek": 610008, "start": 6124.24, "end": 6126.24, "text": " You know much to my chagrin", "tokens": [509, 458, 709, 281, 452, 417, 559, 12629], "temperature": 0.0, "avg_logprob": -0.2631821055988689, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.9523507944541052e-05}, {"id": 1508, "seek": 612624, "start": 6126.24, "end": 6130.24, "text": " It turns out none of them really help uh possibly, you know if anything they make it worse", "tokens": [467, 4523, 484, 6022, 295, 552, 534, 854, 2232, 6264, 11, 291, 458, 498, 1340, 436, 652, 309, 5324], "temperature": 0.0, "avg_logprob": -0.22730991296600878, "compression_ratio": 1.650735294117647, "no_speech_prob": 1.593509477970656e-05}, {"id": 1509, "seek": 612624, "start": 6132.08, "end": 6133.76, "text": " Because in the end", "tokens": [1436, 294, 264, 917], "temperature": 0.0, "avg_logprob": -0.22730991296600878, "compression_ratio": 1.650735294117647, "no_speech_prob": 1.593509477970656e-05}, {"id": 1510, "seek": 612624, "start": 6133.76, "end": 6139.84, "text": " The computer wants to know no, this is the end of the image. There's nothing else here. And if you reflect it, for example", "tokens": [440, 3820, 2738, 281, 458, 572, 11, 341, 307, 264, 917, 295, 264, 3256, 13, 821, 311, 1825, 1646, 510, 13, 400, 498, 291, 5031, 309, 11, 337, 1365], "temperature": 0.0, "avg_logprob": -0.22730991296600878, "compression_ratio": 1.650735294117647, "no_speech_prob": 1.593509477970656e-05}, {"id": 1511, "seek": 612624, "start": 6140.88, "end": 6145.92, "text": " Then you're kind of creating weird spikes that didn't exist and the computer's got to be like, oh, I wonder what that spike is", "tokens": [1396, 291, 434, 733, 295, 4084, 3657, 28997, 300, 994, 380, 2514, 293, 264, 3820, 311, 658, 281, 312, 411, 11, 1954, 11, 286, 2441, 437, 300, 21053, 307], "temperature": 0.0, "avg_logprob": -0.22730991296600878, "compression_ratio": 1.650735294117647, "no_speech_prob": 1.593509477970656e-05}, {"id": 1512, "seek": 612624, "start": 6146.48, "end": 6148.48, "text": " So yeah, it's a great question", "tokens": [407, 1338, 11, 309, 311, 257, 869, 1168], "temperature": 0.0, "avg_logprob": -0.22730991296600878, "compression_ratio": 1.650735294117647, "no_speech_prob": 1.593509477970656e-05}, {"id": 1513, "seek": 612624, "start": 6148.48, "end": 6152.48, "text": " And I obviously spent like a couple of years assuming that", "tokens": [400, 286, 2745, 4418, 411, 257, 1916, 295, 924, 11926, 300], "temperature": 0.0, "avg_logprob": -0.22730991296600878, "compression_ratio": 1.650735294117647, "no_speech_prob": 1.593509477970656e-05}, {"id": 1514, "seek": 615248, "start": 6152.48, "end": 6155.759999999999, "text": " We should be doing things that look more image like but actually", "tokens": [492, 820, 312, 884, 721, 300, 574, 544, 3256, 411, 457, 767], "temperature": 0.0, "avg_logprob": -0.21506211015044666, "compression_ratio": 1.4640883977900552, "no_speech_prob": 3.7616209738189355e-05}, {"id": 1515, "seek": 615248, "start": 6156.32, "end": 6160.48, "text": " The computer likes things to be presented to it in as straightforward a way as possible", "tokens": [440, 3820, 5902, 721, 281, 312, 8212, 281, 309, 294, 382, 15325, 257, 636, 382, 1944], "temperature": 0.0, "avg_logprob": -0.21506211015044666, "compression_ratio": 1.4640883977900552, "no_speech_prob": 3.7616209738189355e-05}, {"id": 1516, "seek": 616048, "start": 6160.48, "end": 6181.5199999999995, "text": " All right. Thanks everybody and uh, hope to see some of you in the walkthroughs and otherwise. See you next time", "tokens": [50364, 1057, 558, 13, 2561, 2201, 293, 2232, 11, 1454, 281, 536, 512, 295, 291, 294, 264, 1792, 11529, 82, 293, 5911, 13, 3008, 291, 958, 565, 51416], "temperature": 0.0, "avg_logprob": -0.20505428314208984, "compression_ratio": 1.1313131313131313, "no_speech_prob": 8.745256491238251e-05}], "language": "en"}