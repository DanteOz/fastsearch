{"text": " Okay, let me get a sip of water. I'm going to start with Notebook 2, topic modeling with NMF and SVD. So topic modeling, we're going to use two popular matrix decomposition techniques here. But the idea is that you have a collection of documents or a collection of text. And so here, this is an example showing some of the plays of Shakespeare. And you'll represent them as bags of words, just kind of taking the word count of different words. So here, Anthony appears a lot in Anthony and Cleopatra, but not in the Tempest or Hamlet. Let's see, Mercy, the word Mercy is in Hamlet. And so this is a way that you can kind of hear you're representing the plays of Shakespeare as a matrix. And the idea behind matrix decomposition is taking one matrix and representing it as a few matrices. And the ones we'll be looking at, these will be matrix products. So we'll take one matrix and decompose it into a few things that, multiplied together, give you your original matrix. And the reason this is useful is the factors that you're decomposing into have special properties that are nicer than your original matrix. And this is also sometimes called latent semantic analysis, LSA. And so kind of as a motivation, I give a case where what if you wanted to represent this matrix we have of the, and this is called the term document matrix, because here are the terms, these are the different documents. If you were going to represent that just as an outer product of two vectors, you're not going to do a great job. But one, kind of probably the best you could do would be to have one vector be the relative frequency of each vocabulary word out of the total word count. And together to have the average number of words per document. And that would give you as close as you could get, kind of taking information in. We'll do this then with, you know, multiple columns and multiple rows, and we can think about creating clusters of the documents. And so that's what's going to be happening where the different clusters of documents are groups. So let me start running these. So here we'll be using Scikit-learn and the 20 newsgroups data set, which is available in Scikit-learn data sets. Scikit-learn.datasets is kind of a nice, I think, collection of data sets that you can get kind of that come with Scikit-learn. So newsgroups were discussion groups on Newsnet, which was popular in the 80s and 90s. This is kind of, you hear about like bulletin boards from the earlier days of the web. This data set includes 18,000 newsgroups post with 20 topics. And then I link to some more tutorials on this. I really like there's a text analysis with topic models for the humanities and social sciences using British literature data set, which I'll show a tiny bit of later. So getting into this, we're going to we're just going to pick out four categories. So topic modeling is a unsupervised problem in that you don't know what the right answer is of, you know, like what are the best topics for your group of documents. That's not not something where you have a set of answers. And so to make this clear to us, if whether or not we're doing a good job, we're just going to pick out four topics and then not use that information, though, in our matrix decomposition and see how close what we get it comes to that. So here I've picked out atheism, religion, computer graphics and space. It's always nice to just check the shape of your data. So here this is 2034. Those are the different posts. Let's look at kind of what this looks like. So here I've just picked out one message. Hi, I've noticed that if you only save a model with all your mapping planes position carefully to a 3DS file that when you reload it after restarting. So what a what topic do you think this first message is? Computer graphics. Yes. Then the next one. And so here I was outputting the first just the first four text. We've got a seems to be barring evidence to the contrary that Koresh was simply another deranged fanatic. What what topic do we think this is? Yeah, probably religion. And so this is if you remember, this is a cult in Waco. This was in the 90s. Here's one. Let me see. So the figure seems unlikely to actually be anything but a perijove. I had to look this up. Perijove is the point in the orbit of a satellite of Jupiter nearest the planet center. So guesses about what that that topic is. Yeah, that's space. And so and then you can see that here when I checked. Yes, they were graphics, religion and space. And it's always good to look at your data just to get a sense of what is this what does this look like? The target attribute is the index of the category. So these are our labels or our wise, depending on how you how you refer to this. And so then something about these matrix decompositions and really, I think, most clustering analysis is you have to choose how many topics you want to create. And so that's something here. I've said six to see what I get. But you could try running this with different numbers of topics because, you know, you think about like if you're looking for clusters in a group, you don't know how many there truly are. And that they're often I mean, there is no true. There is no ground truth here. This is an unsupervised data set. So now I want to do a kind of a little aside about stop words stemming and lemmatization, because this is something you hear about a lot when talking to people about NLP as kind of what you need to do as part of your data processing. So from Christopher Manning intro to information retrieval, some extremely common words, which would be of little use in helping select documents match a user need are excluded from the vocabulary entirely. These are called stop words. This actually I saw a tweet of it's like bad, bad math graphics, and it said most popular word by state and it was a map of the US and every state had the word the written on it. So these is a stop word and you could say, okay, there's not that much information that people are using the word the a lot. It's it's popular. And then here the information retrieval book does note that the the general trend in informational retrieval systems over time has been from standard use of quite large stop list 200 to 300 terms to very small stop list to no stop list whatsoever. And web search engines generally do not use stop list, so they are they are falling out of favor, but I wanted to we're going to use one here since this is a pretty small data set and we're using a simple model. So from scikit learn feature extraction, you can import stop words. So from here I've just listed what the first 20 are. And so this is a longer list, but it has these words that were thought to not have not have much meaning, particularly something for here where we're talking about topics. You know these words would be more useful if we were doing language modeling, or you actually actually need them. And note that there's no single universal list of stop words. So, this really kind of depends on what library are using what they've included as the stop words or you could create your own list of stop words. So pause any question about stop words. Why are they sort of going out of style. So I think a lot of it is around neural networks can handle a lot more complexity. And I think it's if you had a simple model, you know you didn't really have enough complexity to learn about these kind of minor words when you probably wanted to learn about the more meaningful and rich words. Any other questions. And I forgot to do at that time I'm going to throw the catch box to, which has a microphone just so I can get that your questions in the recording, but that question was why are stop words going out of style. Okay, next up is stemming and lemmatization. So, are the below words the same organize organizes and organizing democracy democratic and democratic democratize democratization and stemming and lemmatization are both ways to get to kind of the root form of the words. Lemmatization uses rules about a language and the resulting tokens are at all actual words, quote stemming is the poor man's lemmatization. And that's where you kind of just chop the end off the word. It's a crude her heuristic, but it is it is faster is a benefit of stemming. And so here I illustrate this using an LTK. And this also some of these I was just curious of how it would do it. And there's not a there's not a canonical like this is what the stems have to be or this is what the lemmatization has to result in it's going to depend on the implementation. I looked at feet foot, foot and footing. So, here for lemmatization, the first three all went to foot. The last one went to footing for the stemming, it went to the first one went to feet and the last three went to foot. So this is something you can kind of think about with if you're doing a simple simple application. Do you want to have group together these kind of related related words. And so now, now I want you to take a few a few minutes, maybe longer to try lemmatizing and stemming the following following collections of words fly flies flying organize organizes organizing and universe and university. So let's take this five or 10 minutes we'll see how long it takes. Let me know if you have any problems kind of getting your, your notebook running or set up. And if you missed it earlier the GitHub repo is fast AI. So let me see if there's a board. Oh, so I'll just write it down here. So it's GitHub fast AI, and then core stash NLP. Back up. And so did you see anything interesting with the lemmatizing and stemming the collections collection of words. Yes, and let me throw. Yeah, so when looking at some of the stemming results. It resulted in words that are that have completely different meanings than the original word so like for for organize organizes organizing it all stemmed it to Oregon, which is very different than those three words. Yes, that's a good point. Yeah, so those all ended up with Oregon, I didn't have Oregon in there but if you did. Yeah, that's a very different meaning. Oh, and Jeremy's come to the future not directly over the computer setup. But does anyone want to say about any of the other ones. Anyone. What did you see with universe and university. Sorry, it's a little bit short universe without the E. Yeah, so those both go to the same word so that's another example of this, those getting mapped to something universe and university have different meanings better going to the same word. You want to toss it back. Thank you. And so I also wanted to note that stemming and lemon tization are their language dependent languages with more complex morphologies can show bigger benefits. For example, I read that Sanskrit has a very large number of verb forms. Actually does anyone here know Sanskrit. Do you want to do you want to say anything about it or. Okay, so this is they showed. I read this a few places, and there are other. There's a lot of different ways that you can modify a verb. I guess I shouldn't make a statement about every language languages have different ways of dealing with. Yeah, how they how they how they modify verbs what the different things they take into account are and this is this is a really kind of interesting topic. So I just wanted to say if you are working with different languages. This is something that could be more or less useful depending on the properties of the of the language you're working with. So I think they would potentially be an interesting blog post topic for anybody in the course that does does know a second language. I feel like there's a lot, a lot to be to be said there, depending on what the language is, but about some of the linguistic properties. So stemming and lemmatization are implement implementation dependent. So I tried running running some of these with spacey now. And, and we won't, we won't be using this in the kind of in the course but if you wanted to, to try it out. It's interesting. So here I lemmatized the the same set of words from before. So I used those at feet foot, foot and footing and they all they all ended up the same. I also thought it was interesting spacey doesn't offer a stemmer because it doesn't think you should be stemming. So that's a that's an example of being opinionated. And I use this to illustrate how stop words vary from library to library and you can see that the spacey stop words for English are different than psychic learn stop words from English. And so, sorry, that's the answer. Another exercise for you to try is what stop words appear in spacey but not in SK learn. So just take a few minutes to to compute that. And then, as well as the reverse what stop what stop words are in psychic learn but not in spacey. So some interesting ones that are in psychic learn but not spacey are like fire cry mill just some I don't know some words I wouldn't. Yeah, our stoppers, but it looks like spacey has a lot of like conjunctions or apostrophes. I got those. I got like apostrophe D apostrophe ll. I got this list for spacey but not psychic learn. Yeah, this. I don't know we could yeah look into it later although the point I was trying to illustrate yeah is that these are not consistent or standard so that that still fits with that. So I just I was just curious about this. And what yeah really wanted to illustrate that it's not always the same it's going to depend on the implementation and maybe even the version they are using. Yes. Do people trust one source over the other in industry like is our spacey stop words used more often than psychic learns or vice versa. That's a good question. I'm not sure that there's yeah that there's a standard. And then I haven't used it but I would imagine Jensen might have had some have it has its own as well. And that's something also that's probably just important to note what you're doing. So so people notice. But again as as people are moving towards more complex models the use of stop words is falling out of favor. Although here in this notebook we're going to be doing a pretty it's a simpler simpler model SVD so we will use stop where we will remove stop words. Thank you. Any other questions about stop words. Okay. And so then just as I mentioned mentioned earlier in in more complex models and particularly in deep learning this could hurt your performance because they are they are ways of throwing away information. Although that's you know can be useful when you have a simpler model and are not able to capture that complexity. And then another approach that I have not had a chance to try but that I have heard great things about and I'm interested in is sentence piece or these. I think there's some other techniques for forming sub word tokens. And so instead of we'll talk about tokenization in a moment. We're going to typically be thinking of our words as our units and sentence piece thinks about sub word units. So that's that could be something to look into or even something to try using in the project you do for your blog post. Okay so going back to the problem at hand of topic modeling looking at this collection of postings that we have on computer graphics space atheism and religion. So we're going to be using today we'll be using psychic learns count vectorizer. Although in the future we'll kind of dig more into how you would create tokens on your own or do this sort of work on your own. Let me run these. So. We're creating creating vectors. We have the shape of two thousand thirty four by twenty six thousand five hundred seventy six here the two thousand is the different the different postings. So those are you know separate. I guess the modern equivalent you could think of posting on Reddit or another kind of discussion site. So you know the two thousand different postings and then we have twenty six thousand. Words or tokens we can look at our vocab so vectorizer has this get feature names and this is a list of words. So here I was looking at words seven thousand to seven thousand and twenty you can see there in alphabetical art alphabetical order. We kind of have all these words that start with C.O. And I think. Actually before I go on to. Well yeah before I go on to using S.V.D. I'm going to show you what this looks like in a Excel spreadsheet because I think this can be a more visual way of seeing seeing what's happening. And so and this the spreadsheet is in the GitHub repo. So here I'm looking at kind of just a very small data set so I wanted to be able to put it into a spreadsheet but twenty seven British novels with just sixty four vocabulary words not. They had fifty five thousand vocabulary words altogether but we're just considering sixty four of them. And they I kind of did the I did the work for this in a in a Python notebook and then I extracted it just so you could visually see like what what does this look like. So here is the term document matrix. So along the side these are the different novels and it's the author's name and then the start of the title. So this is Jane Austen sense and sense of sensibility Charles Dickens Bleak House. And then along here we have the vocabulary words we're looking at in this case and they've been normalized using an approach called TFIDF which I'll talk about in a moment. But that just takes and takes into account kind of how many words are in each document and how how rare those words are. So you'll see Kathy does not show up in most of these books. It does show up in Wuthering Heights where Kathy is one of the main characters. And so this is this is how you are or how we are representing this collection of novels using the vocabulary words. And so I'm going to show you what SVD gives you SVD. It's going to give you three matrices back. So you is going to be the documents by the topics. S these are called the singular values but they are basically telling you kind of the importance or scale of each topic. You'll notice that S is a diagonal matrix matrix. So it has non diagonal values sorry non zero values down the diagonal. Everything else is zero. And then V relates topics to the vocabulary words. And so you can kind of look through actually I guess I should check does anyone have a favorite book among this collection that they they want to look at. OK so I was I was scrolling through yesterday just when I was preparing I think I was looking at the end of the list and I noticed that uncle was largest. Uncle's largest here in I guess this must be topic topic five. So then we can come over here and see OK topic five and I saw Stern Tristram has a high relatively high number point four seven for it. I actually was not familiar with this book so I had to look it up. Let me pull that up. Yeah so it's the life and opinions of Tristram Shandley gentlemen and it turns out that one of the main characters is the uncle so that that at least fits with when we find it. Here it is. Yeah. Apart from Tristram is narrator the most familiar and important characters in the book include his uncle Toby. And actually we could go back and let's see if Toby is a. Let's see if Toby is one of the vocabulary words may not have made the cut off. OK yeah so Toby's in here Toby is largest for topic five again so topic five involves uncle and Toby and when we go over one of the books that has a lot of topic five in it is the life and times. I've already forgotten the name Tristram Shandy. Any questions about this one so we'll see this from the kind of the Python point of view but I think there can be something nice about the visual and of where we're going kind of what these topics look like. Oh sorry. Just briefly explain again what each matrix are and how we got them. Sure so I am. Yeah I have not told you how we got them yet. So I kind of cheated. What did you just see where we were going. That's a fair question. So these these matrices are the result of SVD and we'll talk about that in a moment. But I just want to let you know what they are first so you know that kind of what we're producing and the first one is a matrix that is documents by topics. The middle one and we'll talk. Okay. I'll kind of do the spoiler as well. The columns of this matrix are worth the normal to each other which I haven't haven't showed you yet. Does anyone anyone remember what it means for vectors to be worth the normal set of vectors. Oh yeah. So I see. Oh is that a hand. I'm not going to be able to get it all the way back there. Yeah sorry. I believe that I believe that with normal vectors they're orthogonal to each other and also the magnitude is one. That's right. Their magnitudes one they're orthogonal to each other. What does it mean to be orthogonal. 90 degrees. Yeah 90 degrees. And so I thank you. Thank you. Yeah I saw several people kind of doing this handsome signal. Yeah they're they're perpendicular to each other which means their dot product. So for the set two different vectors are going to have a dot product of zero together and dotting one with itself will have a dot product of one, which is why it has magnitude one. So these these different topics are orthogonal to each other. These columns. Then we've got this matrix in the middle where these are called the singular values and they're going to be positive numbers and then everything that's not on the diagonal zero. And then over here we have our matrix V that is topics by vocabulary words or tokens. And here the rows are orthonormal to each other. So let's go back to the notebook. Sorry. Oops. OK. So Gilbert Strang who's the author of a classic linear algebra textbook said SVD is not nearly as famous as it should be. SVD is a super super useful matrix decomposition that shows up a lot of places and SVD algorithm factor factorizes a matrix into one matrix with orthogonal columns one with orthogonal rows and a diagonal matrix that contains the relative importance of each factor. SVD is an exact decomposition since the matrices it creates are big enough to fully cover the original matrix. And it's widely widely used in linear algebra, including for semantic analysis. So what we're doing here with the topic modeling collaborative filtering or recommendations. So you can see this in things like the Netflix Prize where it, you know, used in a more complicated way, but as a component. Calculating pseudo inverses for matrices that don't have a true inverse data compression or PCA. Sorry, I should correct this. We will not be covering PCA in here. So we're going to use scikit learns implementation of SVD today. So we give it vectors, which perhaps is a confusing name. This was the term document matrix that we had gotten for for our documents. So we're going to say full matrices equals false. Otherwise, what it does is we'll come up with extra extra columns for you and extra rows for V to fully make an orthonormal basis. But we're kind of never going to do that. So we run this and we get me. Sorry, hide the answer. We get back three matrices U, S, and V. And then I want I want you to. Sorry. I want you now to take a moment to do this exercise and confirm that the decomposition is the is the input. I'll show this one. So what you want to do is multiply the matrices together. I called that reconstructed vectors as you. So did you see that as as showed up as a vector, even though the middle the middle matrix is a matrix. So this is probably the trickiest part about this and actually can show this above. So you can see with S is shape, you know that it's 2034 comma, meaning it's just this one dimensional vector. So NumPy has the dot diagram and that turns. It's really neat if you put a one D array into it, it'll make it into a matrix. And if you put a matrix into it, it'll pull off the diagonal and give you back the one D array. Actually, let me show you that on maybe a smaller. So let's just use like the first five entries of S. Is there parentheses. Do just the four so it fits on a line. Okay, so here we were giving it. So let's print this out. So we've got this one D array with four numbers. If we put that into NP dot diag, we get back a four by four matrix where it's added zeros. And then if we apply NP dot diag to that again, it'll give us back just what was on the on the diagonal. So that's a, it can be a handy handy NumPy function. So we need to do that to do this multiplication U times the matrix form of S times V. And then I prefer, I think, using NumPy dot all close to check the reconstructed vectors and the vectors. Alternately, you could look at the norm of the difference of the two and see that it's zero. Any questions about this? Okay. So next I want you to confirm that U and V are orthonormal. Yes. Which we just had. Why do we subtract? So you don't need to. This is an alternate way of doing it. Oh, so either of those. Yeah. Sorry. Let me put that maybe separately. Yeah. Good question. In the alternate way, you would be checking that that difference was zero. Just take the dot product of U with the transpose of U and same thing for VH. That's right. And well, actually, say, and what are you looking for? The matrix of ones. Yes. Yeah. So take the dot product. It's pretty, it's pretty padded. So, yes, the take the transpose of U, multiply it by U. And here I've checked that it's all close to the identity matrix of the same shape. Ditto for V and V transpose. And the thing here is you do want to check that you're multiplying it on the correct side. So with U, it's the. Yeah, it's the columns that are orthonormal. So you put the transpose first to get rows by columns. V, it's the rows that are orthonormal. So you have that and then times the transpose to have the columns. Any questions? OK, and I think it's always just nice to check these things because, you know, you can know the definition of SVD that this is what it does. But I feel like this kind of can make it a little more a little more tangible of like, OK, it really is giving me a matrix that's orthonormal. And I really am getting back my original from it. And so let's let's see what can we say about the singular values. So here I've plotted the singular values. And again, remember the singular values. This is this one D array that you can make into a matrix by adding zeros. And I see that they are non-negative and they're decreasing. They're actually decreasing pretty quickly, it looks like. And the kind of meaning behind the singular values is telling you the importance of the topics. I should also note the reason that our other dimension is topics is because we're doing topic modeling. You could be doing SVD on a different application and assign that a different meaning. So this is the one setting the meaning of of that new dimension that we're adding and saying that it's topics. So here I wanted to see what are the topics. So I wrote a method that that goes through the vocab and is choosing the largest the largest values to be the top words for each topic. Because really each topic actually go back to the expel spreadsheet. I think you can see it better here like each topic involves a little bit of every single word in the vocabulary. Right. And so to get something meaningful out of this, I said, you know, I'll check, choose out what are the few words with the highest values here and assign those to be the words for the topic. And so that's that's what this method is doing. And calling those the topic words and then I'm I'm looking at them. The first the first one is pretty weird. Citrus ditto propagandists or name galactic centric kindergarten surreal imaginative. And that doesn't that doesn't seem to map to a topic. But the next one, JPEG gif file color quality image format. What topic do you think that is? Graphics. Yeah. Then we've got what looks like another graphics one. What's that? Because it's got graphic. Edu pub and mail are less meaningful, but the 3D ray FTP. Then we've got Jesus, God, Matthew, people, atheist, atheism. There is there is graphics in that last one. But overall, this seems like probably a religion one or an atheism one. Another another graphics one. Another atheism or religion one. Here we've got a space one. Space NASA lunar Mars missions probe. Actually, that's two space ones in a row. And so note that we're not. You know, we're not getting kind of definitive topics that exactly match what we what we thought the topics were of our source data. But this is these are sensical outputs. There are questions about this. So this also highlights that even though we kind of, you know, we were constructing, I would say the somewhat artificial data set that we were hoping would have, you know, for clusters. We wanted to look at more than four. We didn't just look at like the first four topics that we got because you can have some duplicates. And so here. Well, really, I guess we got a few graphics ones before, you know, we didn't get space until what is that topic? The seventh and eighth most important topics. Yes. If you decrease the number of clusters to four, would they be more precise? Well, so this is getting ahead of myself a bit. So here we've actually gotten two thousand topics and I was just looking at the top, the top ten. And we'll we'll get to this later. How you can do a truncated SVD. For SVD, you're basically kind of always imagining their two thousand topics, but your question will be very relevant when we talk about NMF. Yeah, so that's actually a good tie in to start talking about NMF. And so NMF is a different matrix factorization. So in practice, if you're doing a topic modeling problem, you would either use SVD or you would use NMF. I'm just showing both to kind of illustrate some of their properties and differences. The motivation for NMF is kind of wondering, OK, what do negative values mean in a lot of context and even you know, even here when we look at our words, what does it mean that you find a good negative like topic seven refers to negative point two nine atoms. How do we interpret that? And so a lot of people like NMF for its interpret ability of not having not having negatives, as the as the name suggests. And so here this is looking at an image problem of decomposing faces into different components. And this is also something that earlier I was saying, we're calling these components topics because we're doing topic modeling. If we were using SVD on face images, we would call our different components that we got, I guess, like different components of the face or different pieces of the face. And so this would be something kind of more akin to what you could get from SVD, whereas with NMF, because it's non-negative, it's like, oh, I can see how these are pieces of faces. So here, you know, here's kind of like underneath someone's eyes. This is the right side of someone's nose and kind of around the mouth. So NMF by giving you non-negatives. So what we liked about SVD decomposition is that the resulting matrices are orthogonal. NMF, we're saying, let's make them, let's constrain them to be non-negative. And so our data set V is going to be a product of just two matrices here, W and H, both that have non-negative entries. And so here they're showing a factorization of V where these are actual faces. And so each face has been kind of unwound, you know, to be a single array of pixel information. And they're representing that as facial features by the importance of the features in each image. And you can think of these. So matrix, matrix multiplication. Actually, this might be a good time to say a little bit about matrix multiplication. Matrix multiplication, one way to think about it is taking linear combinations. So typically, you know, a lot of people, when you talk about matrix multiplication, you might think of a song, row by column, row by column. I meant to look up the lyrics before this. I think it's multiply them, add them up one by one. But there's an alternate way to think about matrix multiplication, which is thinking about the columns of what you have first. And then actually, I should have these names. So say you have a column called A, B, C, and D. If you are multiplying that just by W, X, Y, C. Really, that's like taking this linear combination of W times column A plus X times column B plus Y times column C. Then I've run out of space. Plus Z times column D. And so the result of this is going to give you a column. And so this, a lot of, I think a lot of real world applications, this is actually kind of the meaning of what's going on with matrix multiplication, even though it's often not taught this way. But if you're doing, so this was a matrix times a vector. If you were doing a matrix times a matrix, what you're doing is taking a bunch of different linear combinations of the first matrix, where each column in your second matrix gives you the coefficients that you're using for this linear combination. Any questions about this interpretation of matrix multiplication? Okay, so this is, yeah, I think this is pretty useful and not, not talked about enough in linear algebra. And so that's what, that's what's going on here. In this picture is you have these facial features, and then you're taking a bunch of different linear combinations of facial features to reconstruct your original faces. Things to note about NMF is that it's non-unique. So SVD is giving you a unique decomposition, NMF does not. Here I list several, several applications, collaborative filtering, face decompositions, audio source separation, chemistry, bioinformatics. But going back to NLP and our topic at hand, if we were looking at, I should note this is transposed from a lot of the matrix, how the matrix is in Excel, where it's words by documents, then this is giving us topics and topic importance indicators. We're going to use Scikit-learns implementation of NMF, and here we have to set the number of topics. So that's setting the number of clusters that we expect or we think we might have. And so this is kind of a hyperparameter that you're having to guess at and that you might want to try different things with. So we'll use, yes? Oh wait, let me review. So you said number of topics is a hyperparameter? Yes. So how do you, do you have a metric or a graph that you can evaluate what number of topics is good for this? I mean this is hard because it's an unsupervised problem. Like we don't have, usually you don't have a ground truth of what the true topics are. So in this case, we don't know. Like in practice, if you were working on this type of problem, I think you would want to do some exploratory data analysis to see like how many topics does this look like to me? Try it and see like do these topics seem reasonable together? Thanks. You're welcome. Yeah, I mean I think there is something a little bit dissatisfying about the fact that you're kind of having to set this. So here I've run the decomposition, well I created a decomposition, fit the transform to vectors, which is our term document matrix from before. And then that returns the first matrix W and then this is how you access the second matrix H. And here I'm applying show topics on H and this is what I get back. What do you think of these topics? I thought they were pretty good. We've got computer images. This is actually very similar to one of the ones that showed up in SVD, the second one. Third is space, either religion or atheism, then other computer graphics. We might want to rerun this with more topics. Something to note is my show topics method, remember is only showing, and this is the method that I wrote, the end of the SVD section up here. I've manually chosen the number of topic words because, kind of same with SVD, you can have a value for every single word, you know, which in this case is 25,000 words, but many of those are going to be zero or close to zero for NMF. And so here I'm just choosing the largest to display as the topic words. And then we can see this in Excel, so I had the SVD, then I also did NMF on this collection of British novels. Here I did 10 topics for them. And it's something that I guess is nice is if you have lots of zeros, it's sparser, so like here, Kathy only shows up or sorry. Yes, sometimes the words are only showing up in one of the topics. Then we could go back and check. Let's see, so Kathy is in topic six. We would expect Wuthering Heights to have a fair amount of topic six, since Kathy is one of the main characters and it does, it's a 0.79. And so this is kind of how you can look at it again. We've got just two matrices here. Any questions? And let me. Okay, I'll repeat the question. How did I get the Excel file? So I cheated and I did this work in an IPython notebook. So I did this in Python. And then I think I saved them as CSVs maybe, the matrices, so that I could copy them into an Excel file. Yeah, so I don't think I saved that. Sorry, this is from a few years ago. Yeah, I kind of did it as scratch work, but I encourage you to try recreating it because I link. I link to the data source here so you can kind of download the British novel data. Oh no, I do have the Python code. Yeah, and I link to the Python code. Okay, so yeah, it is available. This is from the numerical linear algebra class. But so the Excel file is in the GitHub repo for this class, and then it's got the information on how you can recreate this. But here I really primarily just wanted to give you a way to visualize, okay, what are these factorizations look like? So I think when you're just reading the equations, they can sound more complicated. And then when you see it, it's like, okay, these are just matrices of numbers. Any other questions about this? Okay, so let's go back to. Back to the Jupyter notebook. Yeah, so next I was going to talk about TFIDF, which is Topic Frequency Inverse Document Frequency. And it's a way to normalize term counts by taking into account how often they appear in a document, how long the document is, and how common or rare the term is. So if there's a term that's extremely rare, it's more important. If a document super long, it kind of means less for each topic that appears in it, because you can assume a really long document just has more content in general. I'm going to use Scikit-Learns TFIDF Vectorizer. This is just another way of kind of processing our data. And so again, it's this newsgroup training data from from before that we had. And then here are the topics I get this time for for NMF. And here the fourth one is pretty weird, but the other. I think the other four are pretty good. And something to note about NMF is that it is not exact, so you may not get your original matrix back perfectly. And as I said before, it's also not unique. So NMF can be fast and easy to use. It did take years of research and expertise to create. Also for NMF, the matrix needs to be at least as tall as it is wide or you'll get an error with the fit transform, but you can always just transpose your matrix to get it in that format. So something that was nice about NMF is that we were only having to calculate a few topics and SVD, we were calculating 2000 topics, right? Whereas most of those were not very meaningful and we kind of would like that benefit for SVD of not having to calculate so many topics. And the way to do this is called truncated SVD. We're really just interested in the vectors corresponding to the largest singular values. And so this, there's a nice Facebook research, had a nice post on fast randomized SVD. So some shortcomings of class classical algorithms for decomposition are that matrices are often quotes, stupendously big. This is from a nice paper. Let me open this. About probabilistic algorithms for matrix decompositions. Data is also missing or inaccurate in the real world and it's kind of why use a ton of computational resources when you know that your input was somewhat imprecise. And so there's, you know, there's not a big benefit to being very precise with your calculation if your input data is not that precise. Data transfer now plays a major time in algorithms. And so this is moving, moving data from disk memory to, you know, registers or cache, moving it to the GPU. If you're doing GPU computations, but moving data around can be time consumed or very time consuming. And so randomized algorithms are a great, great solution to this. They're stable. The performance guarantees often don't depend on kind of these properties of the underlying matrices. They can be done in parallel. And so we're just going to look at randomized SVD as one example of a randomized algorithm and it's going to kind of let us get some of these benefits for SVD that we saw of NMF of, you know, we only had to calculate five topics for NMF. If that's the number we chose, whereas with the original SVD, we were throwing away a ton of data by only looking at, you know, a few singular values and a few topics. So I have a timing comparison of doing NumPy's linear algebra SVD that we did before, and it took me four seconds, which is kind of kind of slow, particularly we only had 2000 documents here, which is, I mean, depending on what you're doing, you could be doing something much larger. Then I used SKLearns implementation of randomized SVD, and it was three seconds. So that's a nice, I mean, it's a 25% speed up. And then I used randomized SVD from Facebook's library FBPCA, and that was just one second so that's significantly faster. So that's neat that it's a lot faster. I should note here I'm having to input how many singular values I want to calculate, which is going to map to, you're going to have the same number of columns of U and rows of V. It's going to correspond to your numbers of singular values, which is necessary for this matrix multiplication of U times S times V to work. So I chose 10. If I had chosen more, this would take longer. Yeah, and that's the end of this notebook. I'm going to talk more about this, some more about this next time, because I know this was probably a little bit quick. Are there any final questions for today, though? Yes, Quinn? So with the randomized SVD, is there still a way to determine the number of principal components based on the amount of explained variance that you're covering, or because it's randomized, do you lose some of that? The explained variance of? Yeah, like determining how many vectors you want based on how much variance is explained by the number of variables. So, the singular values, I think, give you that in terms of because the singular values, they're kind of telling you the magnitude, particularly since U and V are, you know, have orthogonal rows and orthogonal columns. That means that like each, you know, the pieces of U and V are just one and so all the magnitude is coming from the singular values. So yeah, you can look at, yeah, I think smaller singular values and there are techniques for looking at also like the ratio between the singular values of subsequent ones. That's a good question. Any other questions? Okay, we're about at, thank you, we're about at one o'clock, but yeah, feel free to email me if you have further questions and I will review this on Thursday at the start of class before we go on to the next application. Thanks.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.0, "text": " Okay, let me get a sip of water.", "tokens": [1033, 11, 718, 385, 483, 257, 29668, 295, 1281, 13], "temperature": 0.0, "avg_logprob": -0.20748150185362935, "compression_ratio": 1.4673913043478262, "no_speech_prob": 0.011153706349432468}, {"id": 1, "seek": 0, "start": 3.0, "end": 11.0, "text": " I'm going to start with Notebook 2, topic modeling with NMF and SVD.", "tokens": [286, 478, 516, 281, 722, 365, 11633, 2939, 568, 11, 4829, 15983, 365, 426, 44, 37, 293, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.20748150185362935, "compression_ratio": 1.4673913043478262, "no_speech_prob": 0.011153706349432468}, {"id": 2, "seek": 0, "start": 15.0, "end": 23.0, "text": " So topic modeling, we're going to use two popular matrix decomposition techniques here.", "tokens": [407, 4829, 15983, 11, 321, 434, 516, 281, 764, 732, 3743, 8141, 48356, 7512, 510, 13], "temperature": 0.0, "avg_logprob": -0.20748150185362935, "compression_ratio": 1.4673913043478262, "no_speech_prob": 0.011153706349432468}, {"id": 3, "seek": 0, "start": 23.0, "end": 29.0, "text": " But the idea is that you have a collection of documents or a collection of text.", "tokens": [583, 264, 1558, 307, 300, 291, 362, 257, 5765, 295, 8512, 420, 257, 5765, 295, 2487, 13], "temperature": 0.0, "avg_logprob": -0.20748150185362935, "compression_ratio": 1.4673913043478262, "no_speech_prob": 0.011153706349432468}, {"id": 4, "seek": 2900, "start": 29.0, "end": 33.0, "text": " And so here, this is an example showing some of the plays of Shakespeare.", "tokens": [400, 370, 510, 11, 341, 307, 364, 1365, 4099, 512, 295, 264, 5749, 295, 22825, 13], "temperature": 0.0, "avg_logprob": -0.08638201203457145, "compression_ratio": 1.55, "no_speech_prob": 0.00017393962480127811}, {"id": 5, "seek": 2900, "start": 33.0, "end": 40.0, "text": " And you'll represent them as bags of words, just kind of taking the word count of different words.", "tokens": [400, 291, 603, 2906, 552, 382, 10405, 295, 2283, 11, 445, 733, 295, 1940, 264, 1349, 1207, 295, 819, 2283, 13], "temperature": 0.0, "avg_logprob": -0.08638201203457145, "compression_ratio": 1.55, "no_speech_prob": 0.00017393962480127811}, {"id": 6, "seek": 2900, "start": 40.0, "end": 49.0, "text": " So here, Anthony appears a lot in Anthony and Cleopatra, but not in the Tempest or Hamlet.", "tokens": [407, 510, 11, 15853, 7038, 257, 688, 294, 15853, 293, 8834, 404, 33593, 11, 457, 406, 294, 264, 8095, 79, 377, 420, 8234, 2631, 13], "temperature": 0.0, "avg_logprob": -0.08638201203457145, "compression_ratio": 1.55, "no_speech_prob": 0.00017393962480127811}, {"id": 7, "seek": 2900, "start": 49.0, "end": 55.0, "text": " Let's see, Mercy, the word Mercy is in Hamlet.", "tokens": [961, 311, 536, 11, 35626, 11, 264, 1349, 35626, 307, 294, 8234, 2631, 13], "temperature": 0.0, "avg_logprob": -0.08638201203457145, "compression_ratio": 1.55, "no_speech_prob": 0.00017393962480127811}, {"id": 8, "seek": 5500, "start": 55.0, "end": 62.0, "text": " And so this is a way that you can kind of hear you're representing the plays of Shakespeare as a matrix.", "tokens": [400, 370, 341, 307, 257, 636, 300, 291, 393, 733, 295, 1568, 291, 434, 13460, 264, 5749, 295, 22825, 382, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.05915429041935848, "compression_ratio": 1.7716894977168949, "no_speech_prob": 4.757336500915699e-05}, {"id": 9, "seek": 5500, "start": 62.0, "end": 70.0, "text": " And the idea behind matrix decomposition is taking one matrix and representing it as a few matrices.", "tokens": [400, 264, 1558, 2261, 8141, 48356, 307, 1940, 472, 8141, 293, 13460, 309, 382, 257, 1326, 32284, 13], "temperature": 0.0, "avg_logprob": -0.05915429041935848, "compression_ratio": 1.7716894977168949, "no_speech_prob": 4.757336500915699e-05}, {"id": 10, "seek": 5500, "start": 70.0, "end": 73.0, "text": " And the ones we'll be looking at, these will be matrix products.", "tokens": [400, 264, 2306, 321, 603, 312, 1237, 412, 11, 613, 486, 312, 8141, 3383, 13], "temperature": 0.0, "avg_logprob": -0.05915429041935848, "compression_ratio": 1.7716894977168949, "no_speech_prob": 4.757336500915699e-05}, {"id": 11, "seek": 5500, "start": 73.0, "end": 81.0, "text": " So we'll take one matrix and decompose it into a few things that, multiplied together, give you your original matrix.", "tokens": [407, 321, 603, 747, 472, 8141, 293, 22867, 541, 309, 666, 257, 1326, 721, 300, 11, 17207, 1214, 11, 976, 291, 428, 3380, 8141, 13], "temperature": 0.0, "avg_logprob": -0.05915429041935848, "compression_ratio": 1.7716894977168949, "no_speech_prob": 4.757336500915699e-05}, {"id": 12, "seek": 8100, "start": 81.0, "end": 92.0, "text": " And the reason this is useful is the factors that you're decomposing into have special properties that are nicer than your original matrix.", "tokens": [400, 264, 1778, 341, 307, 4420, 307, 264, 6771, 300, 291, 434, 22867, 6110, 666, 362, 2121, 7221, 300, 366, 22842, 813, 428, 3380, 8141, 13], "temperature": 0.0, "avg_logprob": -0.07947671678331163, "compression_ratio": 1.3972602739726028, "no_speech_prob": 1.5688776329625398e-05}, {"id": 13, "seek": 8100, "start": 92.0, "end": 97.0, "text": " And this is also sometimes called latent semantic analysis, LSA.", "tokens": [400, 341, 307, 611, 2171, 1219, 48994, 47982, 5215, 11, 441, 8886, 13], "temperature": 0.0, "avg_logprob": -0.07947671678331163, "compression_ratio": 1.3972602739726028, "no_speech_prob": 1.5688776329625398e-05}, {"id": 14, "seek": 9700, "start": 97.0, "end": 111.0, "text": " And so kind of as a motivation, I give a case where what if you wanted to represent this matrix we have of the,", "tokens": [400, 370, 733, 295, 382, 257, 12335, 11, 286, 976, 257, 1389, 689, 437, 498, 291, 1415, 281, 2906, 341, 8141, 321, 362, 295, 264, 11], "temperature": 0.0, "avg_logprob": -0.09926637013753255, "compression_ratio": 1.6517412935323383, "no_speech_prob": 3.373345316504128e-05}, {"id": 15, "seek": 9700, "start": 111.0, "end": 118.0, "text": " and this is called the term document matrix, because here are the terms, these are the different documents.", "tokens": [293, 341, 307, 1219, 264, 1433, 4166, 8141, 11, 570, 510, 366, 264, 2115, 11, 613, 366, 264, 819, 8512, 13], "temperature": 0.0, "avg_logprob": -0.09926637013753255, "compression_ratio": 1.6517412935323383, "no_speech_prob": 3.373345316504128e-05}, {"id": 16, "seek": 9700, "start": 118.0, "end": 126.0, "text": " If you were going to represent that just as an outer product of two vectors, you're not going to do a great job.", "tokens": [759, 291, 645, 516, 281, 2906, 300, 445, 382, 364, 10847, 1674, 295, 732, 18875, 11, 291, 434, 406, 516, 281, 360, 257, 869, 1691, 13], "temperature": 0.0, "avg_logprob": -0.09926637013753255, "compression_ratio": 1.6517412935323383, "no_speech_prob": 3.373345316504128e-05}, {"id": 17, "seek": 12600, "start": 126.0, "end": 135.0, "text": " But one, kind of probably the best you could do would be to have one vector be the relative frequency of each vocabulary word out of the total word count.", "tokens": [583, 472, 11, 733, 295, 1391, 264, 1151, 291, 727, 360, 576, 312, 281, 362, 472, 8062, 312, 264, 4972, 7893, 295, 1184, 19864, 1349, 484, 295, 264, 3217, 1349, 1207, 13], "temperature": 0.0, "avg_logprob": -0.12765970230102539, "compression_ratio": 1.6611111111111112, "no_speech_prob": 1.983154652407393e-05}, {"id": 18, "seek": 12600, "start": 135.0, "end": 139.0, "text": " And together to have the average number of words per document.", "tokens": [400, 1214, 281, 362, 264, 4274, 1230, 295, 2283, 680, 4166, 13], "temperature": 0.0, "avg_logprob": -0.12765970230102539, "compression_ratio": 1.6611111111111112, "no_speech_prob": 1.983154652407393e-05}, {"id": 19, "seek": 12600, "start": 139.0, "end": 144.0, "text": " And that would give you as close as you could get, kind of taking information in.", "tokens": [400, 300, 576, 976, 291, 382, 1998, 382, 291, 727, 483, 11, 733, 295, 1940, 1589, 294, 13], "temperature": 0.0, "avg_logprob": -0.12765970230102539, "compression_ratio": 1.6611111111111112, "no_speech_prob": 1.983154652407393e-05}, {"id": 20, "seek": 14400, "start": 144.0, "end": 156.0, "text": " We'll do this then with, you know, multiple columns and multiple rows, and we can think about creating clusters of the documents.", "tokens": [492, 603, 360, 341, 550, 365, 11, 291, 458, 11, 3866, 13766, 293, 3866, 13241, 11, 293, 321, 393, 519, 466, 4084, 23313, 295, 264, 8512, 13], "temperature": 0.0, "avg_logprob": -0.092353883336802, "compression_ratio": 1.5389221556886228, "no_speech_prob": 1.0952848242595792e-05}, {"id": 21, "seek": 14400, "start": 156.0, "end": 165.0, "text": " And so that's what's going to be happening where the different clusters of documents are groups.", "tokens": [400, 370, 300, 311, 437, 311, 516, 281, 312, 2737, 689, 264, 819, 23313, 295, 8512, 366, 3935, 13], "temperature": 0.0, "avg_logprob": -0.092353883336802, "compression_ratio": 1.5389221556886228, "no_speech_prob": 1.0952848242595792e-05}, {"id": 22, "seek": 14400, "start": 165.0, "end": 171.0, "text": " So let me start running these.", "tokens": [407, 718, 385, 722, 2614, 613, 13], "temperature": 0.0, "avg_logprob": -0.092353883336802, "compression_ratio": 1.5389221556886228, "no_speech_prob": 1.0952848242595792e-05}, {"id": 23, "seek": 17100, "start": 171.0, "end": 180.0, "text": " So here we'll be using Scikit-learn and the 20 newsgroups data set, which is available in Scikit-learn data sets.", "tokens": [407, 510, 321, 603, 312, 1228, 16942, 22681, 12, 306, 1083, 293, 264, 945, 2583, 17377, 82, 1412, 992, 11, 597, 307, 2435, 294, 16942, 22681, 12, 306, 1083, 1412, 6352, 13], "temperature": 0.0, "avg_logprob": -0.1235052648216787, "compression_ratio": 1.6994818652849741, "no_speech_prob": 2.9308617740753107e-05}, {"id": 24, "seek": 17100, "start": 180.0, "end": 191.0, "text": " Scikit-learn.datasets is kind of a nice, I think, collection of data sets that you can get kind of that come with Scikit-learn.", "tokens": [16942, 22681, 12, 306, 1083, 13, 20367, 296, 1385, 307, 733, 295, 257, 1481, 11, 286, 519, 11, 5765, 295, 1412, 6352, 300, 291, 393, 483, 733, 295, 300, 808, 365, 16942, 22681, 12, 306, 1083, 13], "temperature": 0.0, "avg_logprob": -0.1235052648216787, "compression_ratio": 1.6994818652849741, "no_speech_prob": 2.9308617740753107e-05}, {"id": 25, "seek": 17100, "start": 191.0, "end": 198.0, "text": " So newsgroups were discussion groups on Newsnet, which was popular in the 80s and 90s.", "tokens": [407, 2583, 17377, 82, 645, 5017, 3935, 322, 7987, 7129, 11, 597, 390, 3743, 294, 264, 4688, 82, 293, 4289, 82, 13], "temperature": 0.0, "avg_logprob": -0.1235052648216787, "compression_ratio": 1.6994818652849741, "no_speech_prob": 2.9308617740753107e-05}, {"id": 26, "seek": 19800, "start": 198.0, "end": 206.0, "text": " This is kind of, you hear about like bulletin boards from the earlier days of the web.", "tokens": [639, 307, 733, 295, 11, 291, 1568, 466, 411, 11632, 259, 13293, 490, 264, 3071, 1708, 295, 264, 3670, 13], "temperature": 0.0, "avg_logprob": -0.13683826892406908, "compression_ratio": 1.4751131221719458, "no_speech_prob": 6.961699909879826e-06}, {"id": 27, "seek": 19800, "start": 206.0, "end": 213.0, "text": " This data set includes 18,000 newsgroups post with 20 topics.", "tokens": [639, 1412, 992, 5974, 2443, 11, 1360, 2583, 17377, 82, 2183, 365, 945, 8378, 13], "temperature": 0.0, "avg_logprob": -0.13683826892406908, "compression_ratio": 1.4751131221719458, "no_speech_prob": 6.961699909879826e-06}, {"id": 28, "seek": 19800, "start": 213.0, "end": 227.0, "text": " And then I link to some more tutorials on this. I really like there's a text analysis with topic models for the humanities and social sciences using British literature data set,", "tokens": [400, 550, 286, 2113, 281, 512, 544, 17616, 322, 341, 13, 286, 534, 411, 456, 311, 257, 2487, 5215, 365, 4829, 5245, 337, 264, 36140, 293, 2093, 17677, 1228, 6221, 10394, 1412, 992, 11], "temperature": 0.0, "avg_logprob": -0.13683826892406908, "compression_ratio": 1.4751131221719458, "no_speech_prob": 6.961699909879826e-06}, {"id": 29, "seek": 22700, "start": 227.0, "end": 236.0, "text": " which I'll show a tiny bit of later.", "tokens": [597, 286, 603, 855, 257, 5870, 857, 295, 1780, 13], "temperature": 0.0, "avg_logprob": -0.11797286368705132, "compression_ratio": 1.5268817204301075, "no_speech_prob": 7.071460458973888e-06}, {"id": 30, "seek": 22700, "start": 236.0, "end": 241.0, "text": " So getting into this, we're going to we're just going to pick out four categories.", "tokens": [407, 1242, 666, 341, 11, 321, 434, 516, 281, 321, 434, 445, 516, 281, 1888, 484, 1451, 10479, 13], "temperature": 0.0, "avg_logprob": -0.11797286368705132, "compression_ratio": 1.5268817204301075, "no_speech_prob": 7.071460458973888e-06}, {"id": 31, "seek": 22700, "start": 241.0, "end": 252.0, "text": " So topic modeling is a unsupervised problem in that you don't know what the right answer is of, you know, like what are the best topics for your group of documents.", "tokens": [407, 4829, 15983, 307, 257, 2693, 12879, 24420, 1154, 294, 300, 291, 500, 380, 458, 437, 264, 558, 1867, 307, 295, 11, 291, 458, 11, 411, 437, 366, 264, 1151, 8378, 337, 428, 1594, 295, 8512, 13], "temperature": 0.0, "avg_logprob": -0.11797286368705132, "compression_ratio": 1.5268817204301075, "no_speech_prob": 7.071460458973888e-06}, {"id": 32, "seek": 25200, "start": 252.0, "end": 265.0, "text": " That's not not something where you have a set of answers. And so to make this clear to us, if whether or not we're doing a good job, we're just going to pick out four topics and then", "tokens": [663, 311, 406, 406, 746, 689, 291, 362, 257, 992, 295, 6338, 13, 400, 370, 281, 652, 341, 1850, 281, 505, 11, 498, 1968, 420, 406, 321, 434, 884, 257, 665, 1691, 11, 321, 434, 445, 516, 281, 1888, 484, 1451, 8378, 293, 550], "temperature": 0.0, "avg_logprob": -0.08502612048632478, "compression_ratio": 1.5449735449735449, "no_speech_prob": 3.96669111069059e-06}, {"id": 33, "seek": 25200, "start": 265.0, "end": 272.0, "text": " not use that information, though, in our matrix decomposition and see how close what we get it comes to that.", "tokens": [406, 764, 300, 1589, 11, 1673, 11, 294, 527, 8141, 48356, 293, 536, 577, 1998, 437, 321, 483, 309, 1487, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.08502612048632478, "compression_ratio": 1.5449735449735449, "no_speech_prob": 3.96669111069059e-06}, {"id": 34, "seek": 27200, "start": 272.0, "end": 282.0, "text": " So here I've picked out atheism, religion, computer graphics and space.", "tokens": [407, 510, 286, 600, 6183, 484, 27033, 1434, 11, 7561, 11, 3820, 11837, 293, 1901, 13], "temperature": 0.0, "avg_logprob": -0.08205879909891478, "compression_ratio": 1.4480874316939891, "no_speech_prob": 4.7848716349108145e-06}, {"id": 35, "seek": 27200, "start": 282.0, "end": 292.0, "text": " It's always nice to just check the shape of your data. So here this is 2034. Those are the different posts.", "tokens": [467, 311, 1009, 1481, 281, 445, 1520, 264, 3909, 295, 428, 1412, 13, 407, 510, 341, 307, 945, 12249, 13, 3950, 366, 264, 819, 12300, 13], "temperature": 0.0, "avg_logprob": -0.08205879909891478, "compression_ratio": 1.4480874316939891, "no_speech_prob": 4.7848716349108145e-06}, {"id": 36, "seek": 27200, "start": 292.0, "end": 298.0, "text": " Let's look at kind of what this looks like. So here I've just picked out one message.", "tokens": [961, 311, 574, 412, 733, 295, 437, 341, 1542, 411, 13, 407, 510, 286, 600, 445, 6183, 484, 472, 3636, 13], "temperature": 0.0, "avg_logprob": -0.08205879909891478, "compression_ratio": 1.4480874316939891, "no_speech_prob": 4.7848716349108145e-06}, {"id": 37, "seek": 29800, "start": 298.0, "end": 308.0, "text": " Hi, I've noticed that if you only save a model with all your mapping planes position carefully to a 3DS file that when you reload it after restarting.", "tokens": [2421, 11, 286, 600, 5694, 300, 498, 291, 787, 3155, 257, 2316, 365, 439, 428, 18350, 14952, 2535, 7500, 281, 257, 805, 11844, 3991, 300, 562, 291, 25628, 309, 934, 21022, 278, 13], "temperature": 0.0, "avg_logprob": -0.15109304654396186, "compression_ratio": 1.3832335329341316, "no_speech_prob": 1.568730476719793e-05}, {"id": 38, "seek": 29800, "start": 308.0, "end": 314.0, "text": " So what a what topic do you think this first message is?", "tokens": [407, 437, 257, 437, 4829, 360, 291, 519, 341, 700, 3636, 307, 30], "temperature": 0.0, "avg_logprob": -0.15109304654396186, "compression_ratio": 1.3832335329341316, "no_speech_prob": 1.568730476719793e-05}, {"id": 39, "seek": 29800, "start": 314.0, "end": 318.0, "text": " Computer graphics. Yes.", "tokens": [22289, 11837, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.15109304654396186, "compression_ratio": 1.3832335329341316, "no_speech_prob": 1.568730476719793e-05}, {"id": 40, "seek": 31800, "start": 318.0, "end": 333.0, "text": " Then the next one. And so here I was outputting the first just the first four text. We've got a seems to be barring evidence to the contrary that Koresh was simply another deranged fanatic.", "tokens": [1396, 264, 958, 472, 13, 400, 370, 510, 286, 390, 5598, 783, 264, 700, 445, 264, 700, 1451, 2487, 13, 492, 600, 658, 257, 2544, 281, 312, 2159, 2937, 4467, 281, 264, 19506, 300, 591, 2706, 71, 390, 2935, 1071, 1163, 10296, 3429, 2399, 13], "temperature": 0.0, "avg_logprob": -0.14992629686991374, "compression_ratio": 1.4125, "no_speech_prob": 5.173609679332003e-06}, {"id": 41, "seek": 31800, "start": 333.0, "end": 339.0, "text": " What what topic do we think this is?", "tokens": [708, 437, 4829, 360, 321, 519, 341, 307, 30], "temperature": 0.0, "avg_logprob": -0.14992629686991374, "compression_ratio": 1.4125, "no_speech_prob": 5.173609679332003e-06}, {"id": 42, "seek": 33900, "start": 339.0, "end": 351.0, "text": " Yeah, probably religion. And so this is if you remember, this is a cult in Waco. This was in the 90s.", "tokens": [865, 11, 1391, 7561, 13, 400, 370, 341, 307, 498, 291, 1604, 11, 341, 307, 257, 2376, 294, 343, 11428, 13, 639, 390, 294, 264, 4289, 82, 13], "temperature": 0.0, "avg_logprob": -0.18077337741851807, "compression_ratio": 1.3312883435582823, "no_speech_prob": 4.637626261683181e-06}, {"id": 43, "seek": 33900, "start": 351.0, "end": 360.0, "text": " Here's one. Let me see. So the figure seems unlikely to actually be anything but a perijove. I had to look this up.", "tokens": [1692, 311, 472, 13, 961, 385, 536, 13, 407, 264, 2573, 2544, 17518, 281, 767, 312, 1340, 457, 257, 680, 1718, 1682, 13, 286, 632, 281, 574, 341, 493, 13], "temperature": 0.0, "avg_logprob": -0.18077337741851807, "compression_ratio": 1.3312883435582823, "no_speech_prob": 4.637626261683181e-06}, {"id": 44, "seek": 36000, "start": 360.0, "end": 370.0, "text": " Perijove is the point in the orbit of a satellite of Jupiter nearest the planet center. So guesses about what that that topic is.", "tokens": [3026, 1718, 1682, 307, 264, 935, 294, 264, 13991, 295, 257, 16016, 295, 24567, 23831, 264, 5054, 3056, 13, 407, 42703, 466, 437, 300, 300, 4829, 307, 13], "temperature": 0.0, "avg_logprob": -0.12955392201741536, "compression_ratio": 1.614678899082569, "no_speech_prob": 1.4063270100450609e-05}, {"id": 45, "seek": 36000, "start": 370.0, "end": 373.0, "text": " Yeah, that's space.", "tokens": [865, 11, 300, 311, 1901, 13], "temperature": 0.0, "avg_logprob": -0.12955392201741536, "compression_ratio": 1.614678899082569, "no_speech_prob": 1.4063270100450609e-05}, {"id": 46, "seek": 36000, "start": 373.0, "end": 387.0, "text": " And so and then you can see that here when I checked. Yes, they were graphics, religion and space. And it's always good to look at your data just to get a sense of what is this what does this look like?", "tokens": [400, 370, 293, 550, 291, 393, 536, 300, 510, 562, 286, 10033, 13, 1079, 11, 436, 645, 11837, 11, 7561, 293, 1901, 13, 400, 309, 311, 1009, 665, 281, 574, 412, 428, 1412, 445, 281, 483, 257, 2020, 295, 437, 307, 341, 437, 775, 341, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.12955392201741536, "compression_ratio": 1.614678899082569, "no_speech_prob": 1.4063270100450609e-05}, {"id": 47, "seek": 38700, "start": 387.0, "end": 400.0, "text": " The target attribute is the index of the category. So these are our labels or our wise, depending on how you how you refer to this.", "tokens": [440, 3779, 19667, 307, 264, 8186, 295, 264, 7719, 13, 407, 613, 366, 527, 16949, 420, 527, 10829, 11, 5413, 322, 577, 291, 577, 291, 2864, 281, 341, 13], "temperature": 0.0, "avg_logprob": -0.11823786006254308, "compression_ratio": 1.5235602094240839, "no_speech_prob": 0.00010389937233412638}, {"id": 48, "seek": 38700, "start": 400.0, "end": 412.0, "text": " And so then something about these matrix decompositions and really, I think, most clustering analysis is you have to choose how many topics you want to create.", "tokens": [400, 370, 550, 746, 466, 613, 8141, 22867, 329, 2451, 293, 534, 11, 286, 519, 11, 881, 596, 48673, 5215, 307, 291, 362, 281, 2826, 577, 867, 8378, 291, 528, 281, 1884, 13], "temperature": 0.0, "avg_logprob": -0.11823786006254308, "compression_ratio": 1.5235602094240839, "no_speech_prob": 0.00010389937233412638}, {"id": 49, "seek": 41200, "start": 412.0, "end": 426.0, "text": " And so that's something here. I've said six to see what I get. But you could try running this with different numbers of topics because, you know, you think about like if you're looking for clusters in a group, you don't know how many there truly are.", "tokens": [400, 370, 300, 311, 746, 510, 13, 286, 600, 848, 2309, 281, 536, 437, 286, 483, 13, 583, 291, 727, 853, 2614, 341, 365, 819, 3547, 295, 8378, 570, 11, 291, 458, 11, 291, 519, 466, 411, 498, 291, 434, 1237, 337, 23313, 294, 257, 1594, 11, 291, 500, 380, 458, 577, 867, 456, 4908, 366, 13], "temperature": 0.0, "avg_logprob": -0.08941049161164658, "compression_ratio": 1.5689655172413792, "no_speech_prob": 1.7230588127858937e-05}, {"id": 50, "seek": 41200, "start": 426.0, "end": 435.0, "text": " And that they're often I mean, there is no true. There is no ground truth here. This is an unsupervised data set.", "tokens": [400, 300, 436, 434, 2049, 286, 914, 11, 456, 307, 572, 2074, 13, 821, 307, 572, 2727, 3494, 510, 13, 639, 307, 364, 2693, 12879, 24420, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.08941049161164658, "compression_ratio": 1.5689655172413792, "no_speech_prob": 1.7230588127858937e-05}, {"id": 51, "seek": 43500, "start": 435.0, "end": 453.0, "text": " So now I want to do a kind of a little aside about stop words stemming and lemmatization, because this is something you hear about a lot when talking to people about NLP as kind of what you need to do as part of your data processing.", "tokens": [407, 586, 286, 528, 281, 360, 257, 733, 295, 257, 707, 7359, 466, 1590, 2283, 12312, 2810, 293, 7495, 15677, 2144, 11, 570, 341, 307, 746, 291, 1568, 466, 257, 688, 562, 1417, 281, 561, 466, 426, 45196, 382, 733, 295, 437, 291, 643, 281, 360, 382, 644, 295, 428, 1412, 9007, 13], "temperature": 0.0, "avg_logprob": -0.0793639902482953, "compression_ratio": 1.4935897435897436, "no_speech_prob": 2.1781266696052626e-05}, {"id": 52, "seek": 45300, "start": 453.0, "end": 468.0, "text": " So from Christopher Manning intro to information retrieval, some extremely common words, which would be of little use in helping select documents match a user need are excluded from the vocabulary entirely.", "tokens": [407, 490, 20649, 2458, 773, 12897, 281, 1589, 19817, 3337, 11, 512, 4664, 2689, 2283, 11, 597, 576, 312, 295, 707, 764, 294, 4315, 3048, 8512, 2995, 257, 4195, 643, 366, 29486, 490, 264, 19864, 7696, 13], "temperature": 0.0, "avg_logprob": -0.13139570050123262, "compression_ratio": 1.3733333333333333, "no_speech_prob": 1.8340164388064295e-05}, {"id": 53, "seek": 46800, "start": 468.0, "end": 483.0, "text": " These are called stop words. This actually I saw a tweet of it's like bad, bad math graphics, and it said most popular word by state and it was a map of the US and every state had the word the written on it.", "tokens": [1981, 366, 1219, 1590, 2283, 13, 639, 767, 286, 1866, 257, 15258, 295, 309, 311, 411, 1578, 11, 1578, 5221, 11837, 11, 293, 309, 848, 881, 3743, 1349, 538, 1785, 293, 309, 390, 257, 4471, 295, 264, 2546, 293, 633, 1785, 632, 264, 1349, 264, 3720, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.145734797583686, "compression_ratio": 1.6556603773584906, "no_speech_prob": 1.6699632396921515e-05}, {"id": 54, "seek": 46800, "start": 483.0, "end": 493.0, "text": " So these is a stop word and you could say, okay, there's not that much information that people are using the word the a lot. It's it's popular.", "tokens": [407, 613, 307, 257, 1590, 1349, 293, 291, 727, 584, 11, 1392, 11, 456, 311, 406, 300, 709, 1589, 300, 561, 366, 1228, 264, 1349, 264, 257, 688, 13, 467, 311, 309, 311, 3743, 13], "temperature": 0.0, "avg_logprob": -0.145734797583686, "compression_ratio": 1.6556603773584906, "no_speech_prob": 1.6699632396921515e-05}, {"id": 55, "seek": 49300, "start": 493.0, "end": 511.0, "text": " And then here the information retrieval book does note that the the general trend in informational retrieval systems over time has been from standard use of quite large stop list 200 to 300 terms to very small stop list to no stop list whatsoever.", "tokens": [400, 550, 510, 264, 1589, 19817, 3337, 1446, 775, 3637, 300, 264, 264, 2674, 6028, 294, 49391, 19817, 3337, 3652, 670, 565, 575, 668, 490, 3832, 764, 295, 1596, 2416, 1590, 1329, 2331, 281, 6641, 2115, 281, 588, 1359, 1590, 1329, 281, 572, 1590, 1329, 17076, 13], "temperature": 0.0, "avg_logprob": -0.13399508420158834, "compression_ratio": 1.6143790849673203, "no_speech_prob": 6.301557732513174e-05}, {"id": 56, "seek": 51100, "start": 511.0, "end": 526.0, "text": " And web search engines generally do not use stop list, so they are they are falling out of favor, but I wanted to we're going to use one here since this is a pretty small data set and we're using a simple model.", "tokens": [400, 3670, 3164, 12982, 5101, 360, 406, 764, 1590, 1329, 11, 370, 436, 366, 436, 366, 7440, 484, 295, 2294, 11, 457, 286, 1415, 281, 321, 434, 516, 281, 764, 472, 510, 1670, 341, 307, 257, 1238, 1359, 1412, 992, 293, 321, 434, 1228, 257, 2199, 2316, 13], "temperature": 0.0, "avg_logprob": -0.10944667984457578, "compression_ratio": 1.4840425531914894, "no_speech_prob": 8.800544492260087e-06}, {"id": 57, "seek": 51100, "start": 526.0, "end": 532.0, "text": " So from scikit learn feature extraction, you can import stop words.", "tokens": [407, 490, 2180, 22681, 1466, 4111, 30197, 11, 291, 393, 974, 1590, 2283, 13], "temperature": 0.0, "avg_logprob": -0.10944667984457578, "compression_ratio": 1.4840425531914894, "no_speech_prob": 8.800544492260087e-06}, {"id": 58, "seek": 53200, "start": 532.0, "end": 547.0, "text": " So from here I've just listed what the first 20 are. And so this is a longer list, but it has these words that were thought to not have not have much meaning, particularly something for here where we're talking about topics.", "tokens": [407, 490, 510, 286, 600, 445, 10052, 437, 264, 700, 945, 366, 13, 400, 370, 341, 307, 257, 2854, 1329, 11, 457, 309, 575, 613, 2283, 300, 645, 1194, 281, 406, 362, 406, 362, 709, 3620, 11, 4098, 746, 337, 510, 689, 321, 434, 1417, 466, 8378, 13], "temperature": 0.0, "avg_logprob": -0.12433781121906481, "compression_ratio": 1.5648148148148149, "no_speech_prob": 1.644162693992257e-05}, {"id": 59, "seek": 53200, "start": 547.0, "end": 555.0, "text": " You know these words would be more useful if we were doing language modeling, or you actually actually need them.", "tokens": [509, 458, 613, 2283, 576, 312, 544, 4420, 498, 321, 645, 884, 2856, 15983, 11, 420, 291, 767, 767, 643, 552, 13], "temperature": 0.0, "avg_logprob": -0.12433781121906481, "compression_ratio": 1.5648148148148149, "no_speech_prob": 1.644162693992257e-05}, {"id": 60, "seek": 55500, "start": 555.0, "end": 562.0, "text": " And note that there's no single universal list of stop words.", "tokens": [400, 3637, 300, 456, 311, 572, 2167, 11455, 1329, 295, 1590, 2283, 13], "temperature": 0.0, "avg_logprob": -0.12974939081403944, "compression_ratio": 1.6420454545454546, "no_speech_prob": 9.368352948513348e-06}, {"id": 61, "seek": 55500, "start": 562.0, "end": 574.0, "text": " So, this really kind of depends on what library are using what they've included as the stop words or you could create your own list of stop words.", "tokens": [407, 11, 341, 534, 733, 295, 5946, 322, 437, 6405, 366, 1228, 437, 436, 600, 5556, 382, 264, 1590, 2283, 420, 291, 727, 1884, 428, 1065, 1329, 295, 1590, 2283, 13], "temperature": 0.0, "avg_logprob": -0.12974939081403944, "compression_ratio": 1.6420454545454546, "no_speech_prob": 9.368352948513348e-06}, {"id": 62, "seek": 55500, "start": 574.0, "end": 578.0, "text": " So pause any question about stop words.", "tokens": [407, 10465, 604, 1168, 466, 1590, 2283, 13], "temperature": 0.0, "avg_logprob": -0.12974939081403944, "compression_ratio": 1.6420454545454546, "no_speech_prob": 9.368352948513348e-06}, {"id": 63, "seek": 55500, "start": 578.0, "end": 581.0, "text": " Why are they sort of going out of style.", "tokens": [1545, 366, 436, 1333, 295, 516, 484, 295, 3758, 13], "temperature": 0.0, "avg_logprob": -0.12974939081403944, "compression_ratio": 1.6420454545454546, "no_speech_prob": 9.368352948513348e-06}, {"id": 64, "seek": 58100, "start": 581.0, "end": 597.0, "text": " So I think a lot of it is around neural networks can handle a lot more complexity. And I think it's if you had a simple model, you know you didn't really have enough complexity to learn about these kind of minor words when you probably wanted to learn about", "tokens": [407, 286, 519, 257, 688, 295, 309, 307, 926, 18161, 9590, 393, 4813, 257, 688, 544, 14024, 13, 400, 286, 519, 309, 311, 498, 291, 632, 257, 2199, 2316, 11, 291, 458, 291, 994, 380, 534, 362, 1547, 14024, 281, 1466, 466, 613, 733, 295, 6696, 2283, 562, 291, 1391, 1415, 281, 1466, 466], "temperature": 0.0, "avg_logprob": -0.07924218373755886, "compression_ratio": 1.5858585858585859, "no_speech_prob": 8.664214874443132e-06}, {"id": 65, "seek": 58100, "start": 597.0, "end": 604.0, "text": " the more meaningful and rich words.", "tokens": [264, 544, 10995, 293, 4593, 2283, 13], "temperature": 0.0, "avg_logprob": -0.07924218373755886, "compression_ratio": 1.5858585858585859, "no_speech_prob": 8.664214874443132e-06}, {"id": 66, "seek": 58100, "start": 604.0, "end": 607.0, "text": " Any other questions.", "tokens": [2639, 661, 1651, 13], "temperature": 0.0, "avg_logprob": -0.07924218373755886, "compression_ratio": 1.5858585858585859, "no_speech_prob": 8.664214874443132e-06}, {"id": 67, "seek": 60700, "start": 607.0, "end": 623.0, "text": " And I forgot to do at that time I'm going to throw the catch box to, which has a microphone just so I can get that your questions in the recording, but that question was why are stop words going out of style.", "tokens": [400, 286, 5298, 281, 360, 412, 300, 565, 286, 478, 516, 281, 3507, 264, 3745, 2424, 281, 11, 597, 575, 257, 10952, 445, 370, 286, 393, 483, 300, 428, 1651, 294, 264, 6613, 11, 457, 300, 1168, 390, 983, 366, 1590, 2283, 516, 484, 295, 3758, 13], "temperature": 0.0, "avg_logprob": -0.13713700221135067, "compression_ratio": 1.4709302325581395, "no_speech_prob": 4.2643780034268275e-05}, {"id": 68, "seek": 60700, "start": 623.0, "end": 628.0, "text": " Okay, next up is stemming and lemmatization.", "tokens": [1033, 11, 958, 493, 307, 12312, 2810, 293, 7495, 15677, 2144, 13], "temperature": 0.0, "avg_logprob": -0.13713700221135067, "compression_ratio": 1.4709302325581395, "no_speech_prob": 4.2643780034268275e-05}, {"id": 69, "seek": 62800, "start": 628.0, "end": 647.0, "text": " So, are the below words the same organize organizes and organizing democracy democratic and democratic democratize democratization and stemming and lemmatization are both ways to get to kind of the root form of the words.", "tokens": [407, 11, 366, 264, 2507, 2283, 264, 912, 13859, 4645, 279, 293, 17608, 10528, 15337, 293, 15337, 37221, 1125, 37221, 2144, 293, 12312, 2810, 293, 7495, 15677, 2144, 366, 1293, 2098, 281, 483, 281, 733, 295, 264, 5593, 1254, 295, 264, 2283, 13], "temperature": 0.0, "avg_logprob": -0.17728590457997423, "compression_ratio": 1.768, "no_speech_prob": 1.6700148989912122e-05}, {"id": 70, "seek": 64700, "start": 647.0, "end": 661.0, "text": " Lemmatization uses rules about a language and the resulting tokens are at all actual words, quote stemming is the poor man's lemmatization. And that's where you kind of just chop the end off the word.", "tokens": [16905, 15677, 2144, 4960, 4474, 466, 257, 2856, 293, 264, 16505, 22667, 366, 412, 439, 3539, 2283, 11, 6513, 12312, 2810, 307, 264, 4716, 587, 311, 7495, 15677, 2144, 13, 400, 300, 311, 689, 291, 733, 295, 445, 7931, 264, 917, 766, 264, 1349, 13], "temperature": 0.0, "avg_logprob": -0.1312795461610306, "compression_ratio": 1.5735294117647058, "no_speech_prob": 7.646172889508307e-06}, {"id": 71, "seek": 64700, "start": 661.0, "end": 668.0, "text": " It's a crude her heuristic, but it is it is faster is a benefit of stemming.", "tokens": [467, 311, 257, 30796, 720, 415, 374, 3142, 11, 457, 309, 307, 309, 307, 4663, 307, 257, 5121, 295, 12312, 2810, 13], "temperature": 0.0, "avg_logprob": -0.1312795461610306, "compression_ratio": 1.5735294117647058, "no_speech_prob": 7.646172889508307e-06}, {"id": 72, "seek": 64700, "start": 668.0, "end": 675.0, "text": " And so here I illustrate this using an LTK.", "tokens": [400, 370, 510, 286, 23221, 341, 1228, 364, 42671, 42, 13], "temperature": 0.0, "avg_logprob": -0.1312795461610306, "compression_ratio": 1.5735294117647058, "no_speech_prob": 7.646172889508307e-06}, {"id": 73, "seek": 67500, "start": 675.0, "end": 679.0, "text": " And this also some of these I was just curious of how it would do it.", "tokens": [400, 341, 611, 512, 295, 613, 286, 390, 445, 6369, 295, 577, 309, 576, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.09948684692382813, "compression_ratio": 1.615819209039548, "no_speech_prob": 6.108406523708254e-05}, {"id": 74, "seek": 67500, "start": 679.0, "end": 689.0, "text": " And there's not a there's not a canonical like this is what the stems have to be or this is what the lemmatization has to result in it's going to depend on the implementation.", "tokens": [400, 456, 311, 406, 257, 456, 311, 406, 257, 46491, 411, 341, 307, 437, 264, 27600, 362, 281, 312, 420, 341, 307, 437, 264, 7495, 15677, 2144, 575, 281, 1874, 294, 309, 311, 516, 281, 5672, 322, 264, 11420, 13], "temperature": 0.0, "avg_logprob": -0.09948684692382813, "compression_ratio": 1.615819209039548, "no_speech_prob": 6.108406523708254e-05}, {"id": 75, "seek": 67500, "start": 689.0, "end": 693.0, "text": " I looked at feet foot, foot and footing.", "tokens": [286, 2956, 412, 3521, 2671, 11, 2671, 293, 45959, 13], "temperature": 0.0, "avg_logprob": -0.09948684692382813, "compression_ratio": 1.615819209039548, "no_speech_prob": 6.108406523708254e-05}, {"id": 76, "seek": 69300, "start": 693.0, "end": 707.0, "text": " So, here for lemmatization, the first three all went to foot. The last one went to footing for the stemming, it went to the first one went to feet and the last three went to foot.", "tokens": [407, 11, 510, 337, 7495, 15677, 2144, 11, 264, 700, 1045, 439, 1437, 281, 2671, 13, 440, 1036, 472, 1437, 281, 45959, 337, 264, 12312, 2810, 11, 309, 1437, 281, 264, 700, 472, 1437, 281, 3521, 293, 264, 1036, 1045, 1437, 281, 2671, 13], "temperature": 0.0, "avg_logprob": -0.20846547683080038, "compression_ratio": 1.6886792452830188, "no_speech_prob": 5.9550293372012675e-06}, {"id": 77, "seek": 70700, "start": 707.0, "end": 723.0, "text": " So this is something you can kind of think about with if you're doing a simple simple application. Do you want to have group together these kind of related related words.", "tokens": [407, 341, 307, 746, 291, 393, 733, 295, 519, 466, 365, 498, 291, 434, 884, 257, 2199, 2199, 3861, 13, 1144, 291, 528, 281, 362, 1594, 1214, 613, 733, 295, 4077, 4077, 2283, 13], "temperature": 0.0, "avg_logprob": -0.11359537275213945, "compression_ratio": 1.3821138211382114, "no_speech_prob": 1.6700398191460408e-05}, {"id": 78, "seek": 72300, "start": 723.0, "end": 742.0, "text": " And so now, now I want you to take a few a few minutes, maybe longer to try lemmatizing and stemming the following following collections of words fly flies flying organize organizes organizing and universe and university.", "tokens": [400, 370, 586, 11, 586, 286, 528, 291, 281, 747, 257, 1326, 257, 1326, 2077, 11, 1310, 2854, 281, 853, 7495, 15677, 3319, 293, 12312, 2810, 264, 3480, 3480, 16641, 295, 2283, 3603, 17414, 7137, 13859, 4645, 279, 17608, 293, 6445, 293, 5454, 13], "temperature": 0.0, "avg_logprob": -0.21735872824986777, "compression_ratio": 1.6131386861313868, "no_speech_prob": 9.515457350062206e-06}, {"id": 79, "seek": 74200, "start": 742.0, "end": 756.0, "text": " So let's take this five or 10 minutes we'll see how long it takes. Let me know if you have any problems kind of getting your, your notebook running or set up.", "tokens": [50364, 407, 718, 311, 747, 341, 1732, 420, 1266, 2077, 321, 603, 536, 577, 938, 309, 2516, 13, 961, 385, 458, 498, 291, 362, 604, 2740, 733, 295, 1242, 428, 11, 428, 21060, 2614, 420, 992, 493, 13, 51064], "temperature": 0.0, "avg_logprob": -0.18283498287200928, "compression_ratio": 1.2741935483870968, "no_speech_prob": 2.7530304578249343e-05}, {"id": 80, "seek": 77200, "start": 772.0, "end": 781.0, "text": " And if you missed it earlier the GitHub repo is fast AI.", "tokens": [400, 498, 291, 6721, 309, 3071, 264, 23331, 49040, 307, 2370, 7318, 13], "temperature": 0.0, "avg_logprob": -0.2588881944355212, "compression_ratio": 1.2941176470588236, "no_speech_prob": 0.4294453263282776}, {"id": 81, "seek": 77200, "start": 781.0, "end": 786.0, "text": " So let me see if there's a board. Oh, so I'll just write it down here.", "tokens": [407, 718, 385, 536, 498, 456, 311, 257, 3150, 13, 876, 11, 370, 286, 603, 445, 2464, 309, 760, 510, 13], "temperature": 0.0, "avg_logprob": -0.2588881944355212, "compression_ratio": 1.2941176470588236, "no_speech_prob": 0.4294453263282776}, {"id": 82, "seek": 77200, "start": 786.0, "end": 798.0, "text": " So it's GitHub fast AI, and then core stash NLP.", "tokens": [407, 309, 311, 23331, 2370, 7318, 11, 293, 550, 4965, 342, 1299, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.2588881944355212, "compression_ratio": 1.2941176470588236, "no_speech_prob": 0.4294453263282776}, {"id": 83, "seek": 79800, "start": 798.0, "end": 802.0, "text": " Back up.", "tokens": [5833, 493, 13], "temperature": 0.0, "avg_logprob": -0.15325129826863607, "compression_ratio": 1.2795698924731183, "no_speech_prob": 4.452391658560373e-05}, {"id": 84, "seek": 79800, "start": 802.0, "end": 814.0, "text": " And so did you see anything interesting with the lemmatizing and stemming the collections collection of words.", "tokens": [400, 370, 630, 291, 536, 1340, 1880, 365, 264, 7495, 15677, 3319, 293, 12312, 2810, 264, 16641, 5765, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.15325129826863607, "compression_ratio": 1.2795698924731183, "no_speech_prob": 4.452391658560373e-05}, {"id": 85, "seek": 81400, "start": 814.0, "end": 829.0, "text": " Yes, and let me throw.", "tokens": [1079, 11, 293, 718, 385, 3507, 13], "temperature": 0.0, "avg_logprob": -0.2983953302556818, "compression_ratio": 0.7333333333333333, "no_speech_prob": 2.368637524341466e-06}, {"id": 86, "seek": 82900, "start": 829.0, "end": 849.0, "text": " Yeah, so when looking at some of the stemming results. It resulted in words that are that have completely different meanings than the original word so like for for organize organizes organizing it all stemmed it to Oregon, which is very different than those three words.", "tokens": [865, 11, 370, 562, 1237, 412, 512, 295, 264, 12312, 2810, 3542, 13, 467, 18753, 294, 2283, 300, 366, 300, 362, 2584, 819, 28138, 813, 264, 3380, 1349, 370, 411, 337, 337, 13859, 4645, 279, 17608, 309, 439, 12312, 1912, 309, 281, 18664, 11, 597, 307, 588, 819, 813, 729, 1045, 2283, 13], "temperature": 0.0, "avg_logprob": -0.1785209811463648, "compression_ratio": 1.8189655172413792, "no_speech_prob": 1.544502993056085e-05}, {"id": 87, "seek": 82900, "start": 849.0, "end": 857.0, "text": " Yes, that's a good point. Yeah, so those all ended up with Oregon, I didn't have Oregon in there but if you did. Yeah, that's a very different meaning.", "tokens": [1079, 11, 300, 311, 257, 665, 935, 13, 865, 11, 370, 729, 439, 4590, 493, 365, 18664, 11, 286, 994, 380, 362, 18664, 294, 456, 457, 498, 291, 630, 13, 865, 11, 300, 311, 257, 588, 819, 3620, 13], "temperature": 0.0, "avg_logprob": -0.1785209811463648, "compression_ratio": 1.8189655172413792, "no_speech_prob": 1.544502993056085e-05}, {"id": 88, "seek": 85700, "start": 857.0, "end": 862.0, "text": " Oh, and Jeremy's come to", "tokens": [876, 11, 293, 17809, 311, 808, 281], "temperature": 0.0, "avg_logprob": -0.17408540772228706, "compression_ratio": 1.2432432432432432, "no_speech_prob": 3.3205371437361464e-05}, {"id": 89, "seek": 85700, "start": 862.0, "end": 867.0, "text": " the future not directly over the computer setup.", "tokens": [264, 2027, 406, 3838, 670, 264, 3820, 8657, 13], "temperature": 0.0, "avg_logprob": -0.17408540772228706, "compression_ratio": 1.2432432432432432, "no_speech_prob": 3.3205371437361464e-05}, {"id": 90, "seek": 85700, "start": 867.0, "end": 875.0, "text": " But does anyone want to say about any of the other ones.", "tokens": [583, 775, 2878, 528, 281, 584, 466, 604, 295, 264, 661, 2306, 13], "temperature": 0.0, "avg_logprob": -0.17408540772228706, "compression_ratio": 1.2432432432432432, "no_speech_prob": 3.3205371437361464e-05}, {"id": 91, "seek": 85700, "start": 875.0, "end": 882.0, "text": " Anyone.", "tokens": [14643, 13], "temperature": 0.0, "avg_logprob": -0.17408540772228706, "compression_ratio": 1.2432432432432432, "no_speech_prob": 3.3205371437361464e-05}, {"id": 92, "seek": 88200, "start": 882.0, "end": 893.0, "text": " What did you see with universe and university.", "tokens": [708, 630, 291, 536, 365, 6445, 293, 5454, 13], "temperature": 0.0, "avg_logprob": -0.14407207071781158, "compression_ratio": 1.660919540229885, "no_speech_prob": 2.71047065325547e-05}, {"id": 93, "seek": 88200, "start": 893.0, "end": 910.0, "text": " Sorry, it's a little bit short universe without the E. Yeah, so those both go to the same word so that's another example of this, those getting mapped to something universe and university have different meanings better going to the same word.", "tokens": [4919, 11, 309, 311, 257, 707, 857, 2099, 6445, 1553, 264, 462, 13, 865, 11, 370, 729, 1293, 352, 281, 264, 912, 1349, 370, 300, 311, 1071, 1365, 295, 341, 11, 729, 1242, 33318, 281, 746, 6445, 293, 5454, 362, 819, 28138, 1101, 516, 281, 264, 912, 1349, 13], "temperature": 0.0, "avg_logprob": -0.14407207071781158, "compression_ratio": 1.660919540229885, "no_speech_prob": 2.71047065325547e-05}, {"id": 94, "seek": 91000, "start": 910.0, "end": 912.0, "text": " You want to toss it back.", "tokens": [509, 528, 281, 14432, 309, 646, 13], "temperature": 0.0, "avg_logprob": -0.4050932824611664, "compression_ratio": 0.8571428571428571, "no_speech_prob": 5.06321266584564e-05}, {"id": 95, "seek": 91000, "start": 912.0, "end": 920.0, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.4050932824611664, "compression_ratio": 0.8571428571428571, "no_speech_prob": 5.06321266584564e-05}, {"id": 96, "seek": 92000, "start": 920.0, "end": 938.0, "text": " And so I also wanted to note that stemming and lemon tization are their language dependent languages with more complex morphologies can show bigger benefits. For example, I read that Sanskrit has a very large number of verb forms.", "tokens": [400, 370, 286, 611, 1415, 281, 3637, 300, 12312, 2810, 293, 11356, 256, 2144, 366, 641, 2856, 12334, 8650, 365, 544, 3997, 25778, 6204, 393, 855, 3801, 5311, 13, 1171, 1365, 11, 286, 1401, 300, 44392, 575, 257, 588, 2416, 1230, 295, 9595, 6422, 13], "temperature": 0.0, "avg_logprob": -0.17068111653230628, "compression_ratio": 1.393939393939394, "no_speech_prob": 2.2823924155090936e-05}, {"id": 97, "seek": 93800, "start": 938.0, "end": 944.0, "text": " Actually does anyone here know Sanskrit.", "tokens": [5135, 775, 2878, 510, 458, 44392, 13], "temperature": 0.0, "avg_logprob": -0.23830507419727467, "compression_ratio": 1.1428571428571428, "no_speech_prob": 1.3418106391327456e-05}, {"id": 98, "seek": 93800, "start": 944.0, "end": 952.0, "text": " Do you want to do you want to say anything about it or.", "tokens": [1144, 291, 528, 281, 360, 291, 528, 281, 584, 1340, 466, 309, 420, 13], "temperature": 0.0, "avg_logprob": -0.23830507419727467, "compression_ratio": 1.1428571428571428, "no_speech_prob": 1.3418106391327456e-05}, {"id": 99, "seek": 95200, "start": 952.0, "end": 971.0, "text": " Okay, so this is they showed. I read this a few places, and there are other.", "tokens": [1033, 11, 370, 341, 307, 436, 4712, 13, 286, 1401, 341, 257, 1326, 3190, 11, 293, 456, 366, 661, 13], "temperature": 0.0, "avg_logprob": -0.19943799575169882, "compression_ratio": 1.0555555555555556, "no_speech_prob": 1.669843913987279e-05}, {"id": 100, "seek": 97100, "start": 971.0, "end": 986.0, "text": " There's a lot of different ways that you can modify a verb.", "tokens": [821, 311, 257, 688, 295, 819, 2098, 300, 291, 393, 16927, 257, 9595, 13], "temperature": 0.2, "avg_logprob": -0.9964608086480035, "compression_ratio": 0.9365079365079365, "no_speech_prob": 6.201977521413937e-05}, {"id": 101, "seek": 98600, "start": 986.0, "end": 1001.0, "text": " I guess I shouldn't make a statement about every language languages have different ways of dealing with. Yeah, how they how they how they modify verbs what the different things they take into account are and this is this is a really kind of interesting topic.", "tokens": [286, 2041, 286, 4659, 380, 652, 257, 5629, 466, 633, 2856, 8650, 362, 819, 2098, 295, 6260, 365, 13, 865, 11, 577, 436, 577, 436, 577, 436, 16927, 30051, 437, 264, 819, 721, 436, 747, 666, 2696, 366, 293, 341, 307, 341, 307, 257, 534, 733, 295, 1880, 4829, 13], "temperature": 0.0, "avg_logprob": -0.13805870299643658, "compression_ratio": 1.8152610441767068, "no_speech_prob": 7.480221393052489e-05}, {"id": 102, "seek": 98600, "start": 1001.0, "end": 1012.0, "text": " So I just wanted to say if you are working with different languages. This is something that could be more or less useful depending on the properties of the of the language you're working with.", "tokens": [407, 286, 445, 1415, 281, 584, 498, 291, 366, 1364, 365, 819, 8650, 13, 639, 307, 746, 300, 727, 312, 544, 420, 1570, 4420, 5413, 322, 264, 7221, 295, 264, 295, 264, 2856, 291, 434, 1364, 365, 13], "temperature": 0.0, "avg_logprob": -0.13805870299643658, "compression_ratio": 1.8152610441767068, "no_speech_prob": 7.480221393052489e-05}, {"id": 103, "seek": 101200, "start": 1012.0, "end": 1021.0, "text": " So I think they would potentially be an interesting blog post topic for anybody in the course that does does know a second language.", "tokens": [407, 286, 519, 436, 576, 7263, 312, 364, 1880, 6968, 2183, 4829, 337, 4472, 294, 264, 1164, 300, 775, 775, 458, 257, 1150, 2856, 13], "temperature": 0.0, "avg_logprob": -0.18170701889764695, "compression_ratio": 1.5371428571428571, "no_speech_prob": 3.0708848498761654e-05}, {"id": 104, "seek": 101200, "start": 1021.0, "end": 1034.0, "text": " I feel like there's a lot, a lot to be to be said there, depending on what the language is, but about some of the linguistic properties.", "tokens": [286, 841, 411, 456, 311, 257, 688, 11, 257, 688, 281, 312, 281, 312, 848, 456, 11, 5413, 322, 437, 264, 2856, 307, 11, 457, 466, 512, 295, 264, 43002, 7221, 13], "temperature": 0.0, "avg_logprob": -0.18170701889764695, "compression_ratio": 1.5371428571428571, "no_speech_prob": 3.0708848498761654e-05}, {"id": 105, "seek": 103400, "start": 1034.0, "end": 1045.0, "text": " So stemming and lemmatization are implement implementation dependent. So I tried running running some of these with spacey now.", "tokens": [407, 12312, 2810, 293, 7495, 15677, 2144, 366, 4445, 11420, 12334, 13, 407, 286, 3031, 2614, 2614, 512, 295, 613, 365, 1901, 88, 586, 13], "temperature": 0.0, "avg_logprob": -0.1669870423681942, "compression_ratio": 1.6051282051282052, "no_speech_prob": 3.4263783163623884e-05}, {"id": 106, "seek": 103400, "start": 1045.0, "end": 1059.0, "text": " And, and we won't, we won't be using this in the kind of in the course but if you wanted to, to try it out. It's interesting. So here I lemmatized the the same set of words from before.", "tokens": [400, 11, 293, 321, 1582, 380, 11, 321, 1582, 380, 312, 1228, 341, 294, 264, 733, 295, 294, 264, 1164, 457, 498, 291, 1415, 281, 11, 281, 853, 309, 484, 13, 467, 311, 1880, 13, 407, 510, 286, 7495, 15677, 1602, 264, 264, 912, 992, 295, 2283, 490, 949, 13], "temperature": 0.0, "avg_logprob": -0.1669870423681942, "compression_ratio": 1.6051282051282052, "no_speech_prob": 3.4263783163623884e-05}, {"id": 107, "seek": 105900, "start": 1059.0, "end": 1071.0, "text": " So I used those at feet foot, foot and footing and they all they all ended up the same. I also thought it was interesting spacey doesn't offer a stemmer because it doesn't think you should be stemming.", "tokens": [407, 286, 1143, 729, 412, 3521, 2671, 11, 2671, 293, 45959, 293, 436, 439, 436, 439, 4590, 493, 264, 912, 13, 286, 611, 1194, 309, 390, 1880, 1901, 88, 1177, 380, 2626, 257, 12312, 936, 570, 309, 1177, 380, 519, 291, 820, 312, 12312, 2810, 13], "temperature": 0.0, "avg_logprob": -0.22708837068997897, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.540022695844527e-06}, {"id": 108, "seek": 105900, "start": 1071.0, "end": 1077.0, "text": " So that's a that's an example of being opinionated.", "tokens": [407, 300, 311, 257, 300, 311, 364, 1365, 295, 885, 4800, 770, 13], "temperature": 0.0, "avg_logprob": -0.22708837068997897, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.540022695844527e-06}, {"id": 109, "seek": 107700, "start": 1077.0, "end": 1090.0, "text": " And I use this to illustrate how stop words vary from library to library and you can see that the spacey stop words for English are different than psychic learn stop words from English.", "tokens": [400, 286, 764, 341, 281, 23221, 577, 1590, 2283, 10559, 490, 6405, 281, 6405, 293, 291, 393, 536, 300, 264, 1901, 88, 1590, 2283, 337, 3669, 366, 819, 813, 35406, 1466, 1590, 2283, 490, 3669, 13], "temperature": 0.0, "avg_logprob": -0.113727605342865, "compression_ratio": 1.4682539682539681, "no_speech_prob": 1.6441168554592878e-05}, {"id": 110, "seek": 109000, "start": 1090.0, "end": 1107.0, "text": " And so, sorry, that's the answer. Another exercise for you to try is what stop words appear in spacey but not in SK learn. So just take a few minutes to to compute that.", "tokens": [400, 370, 11, 2597, 11, 300, 311, 264, 1867, 13, 3996, 5380, 337, 291, 281, 853, 307, 437, 1590, 2283, 4204, 294, 1901, 88, 457, 406, 294, 21483, 1466, 13, 407, 445, 747, 257, 1326, 2077, 281, 281, 14722, 300, 13], "temperature": 0.0, "avg_logprob": -0.13336539798312716, "compression_ratio": 1.2900763358778626, "no_speech_prob": 8.012997568584979e-06}, {"id": 111, "seek": 110700, "start": 1107.0, "end": 1123.0, "text": " And then, as well as the reverse what stop what stop words are in psychic learn but not in spacey.", "tokens": [400, 550, 11, 382, 731, 382, 264, 9943, 437, 1590, 437, 1590, 2283, 366, 294, 35406, 1466, 457, 406, 294, 1901, 88, 13], "temperature": 0.0, "avg_logprob": -0.18401527404785156, "compression_ratio": 1.2098765432098766, "no_speech_prob": 3.5559078241931275e-06}, {"id": 112, "seek": 112300, "start": 1123.0, "end": 1145.0, "text": " So some interesting ones that are in psychic learn but not spacey are like fire cry mill just some I don't know some words I wouldn't. Yeah, our stoppers, but it looks like spacey has a lot of like conjunctions or apostrophes.", "tokens": [407, 512, 1880, 2306, 300, 366, 294, 35406, 1466, 457, 406, 1901, 88, 366, 411, 2610, 3305, 1728, 445, 512, 286, 500, 380, 458, 512, 2283, 286, 2759, 380, 13, 865, 11, 527, 1590, 21819, 11, 457, 309, 1542, 411, 1901, 88, 575, 257, 688, 295, 411, 18244, 3916, 420, 19484, 11741, 279, 13], "temperature": 0.0, "avg_logprob": -0.1858213030058762, "compression_ratio": 1.4675324675324675, "no_speech_prob": 7.527638899773592e-06}, {"id": 113, "seek": 114500, "start": 1145.0, "end": 1153.0, "text": " I got those.", "tokens": [286, 658, 729, 13], "temperature": 0.0, "avg_logprob": -0.44506034851074217, "compression_ratio": 1.1333333333333333, "no_speech_prob": 1.9524883100530133e-05}, {"id": 114, "seek": 114500, "start": 1153.0, "end": 1165.0, "text": " I got like apostrophe D apostrophe ll.", "tokens": [286, 658, 411, 19484, 27194, 413, 19484, 27194, 4849, 13], "temperature": 0.0, "avg_logprob": -0.44506034851074217, "compression_ratio": 1.1333333333333333, "no_speech_prob": 1.9524883100530133e-05}, {"id": 115, "seek": 116500, "start": 1165.0, "end": 1175.0, "text": " I got this list for spacey but not psychic learn.", "tokens": [286, 658, 341, 1329, 337, 1901, 88, 457, 406, 35406, 1466, 13], "temperature": 0.0, "avg_logprob": -0.21291767756144206, "compression_ratio": 1.4691358024691359, "no_speech_prob": 1.4063117305340711e-05}, {"id": 116, "seek": 116500, "start": 1175.0, "end": 1177.0, "text": " Yeah, this.", "tokens": [865, 11, 341, 13], "temperature": 0.0, "avg_logprob": -0.21291767756144206, "compression_ratio": 1.4691358024691359, "no_speech_prob": 1.4063117305340711e-05}, {"id": 117, "seek": 116500, "start": 1177.0, "end": 1190.0, "text": " I don't know we could yeah look into it later although the point I was trying to illustrate yeah is that these are not consistent or standard so that that still fits with that.", "tokens": [286, 500, 380, 458, 321, 727, 1338, 574, 666, 309, 1780, 4878, 264, 935, 286, 390, 1382, 281, 23221, 1338, 307, 300, 613, 366, 406, 8398, 420, 3832, 370, 300, 300, 920, 9001, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.21291767756144206, "compression_ratio": 1.4691358024691359, "no_speech_prob": 1.4063117305340711e-05}, {"id": 118, "seek": 119000, "start": 1190.0, "end": 1202.0, "text": " So I just I was just curious about this. And what yeah really wanted to illustrate that it's not always the same it's going to depend on the implementation and maybe even the version they are using.", "tokens": [407, 286, 445, 286, 390, 445, 6369, 466, 341, 13, 400, 437, 1338, 534, 1415, 281, 23221, 300, 309, 311, 406, 1009, 264, 912, 309, 311, 516, 281, 5672, 322, 264, 11420, 293, 1310, 754, 264, 3037, 436, 366, 1228, 13], "temperature": 0.0, "avg_logprob": -0.1607812667379574, "compression_ratio": 1.4295774647887325, "no_speech_prob": 4.756607086164877e-05}, {"id": 119, "seek": 119000, "start": 1202.0, "end": 1208.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.1607812667379574, "compression_ratio": 1.4295774647887325, "no_speech_prob": 4.756607086164877e-05}, {"id": 120, "seek": 120800, "start": 1208.0, "end": 1221.0, "text": " Do people trust one source over the other in industry like is our spacey stop words used more often than psychic learns or vice versa. That's a good question.", "tokens": [1144, 561, 3361, 472, 4009, 670, 264, 661, 294, 3518, 411, 307, 527, 1901, 88, 1590, 2283, 1143, 544, 2049, 813, 35406, 27152, 420, 11964, 25650, 13, 663, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.14433119297027588, "compression_ratio": 1.5240384615384615, "no_speech_prob": 1.5205885574687272e-05}, {"id": 121, "seek": 120800, "start": 1221.0, "end": 1226.0, "text": " I'm not sure that there's yeah that there's a standard.", "tokens": [286, 478, 406, 988, 300, 456, 311, 1338, 300, 456, 311, 257, 3832, 13], "temperature": 0.0, "avg_logprob": -0.14433119297027588, "compression_ratio": 1.5240384615384615, "no_speech_prob": 1.5205885574687272e-05}, {"id": 122, "seek": 120800, "start": 1226.0, "end": 1234.0, "text": " And then I haven't used it but I would imagine Jensen might have had some have it has its own as well.", "tokens": [400, 550, 286, 2378, 380, 1143, 309, 457, 286, 576, 3811, 508, 32934, 1062, 362, 632, 512, 362, 309, 575, 1080, 1065, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.14433119297027588, "compression_ratio": 1.5240384615384615, "no_speech_prob": 1.5205885574687272e-05}, {"id": 123, "seek": 123400, "start": 1234.0, "end": 1239.0, "text": " And that's something also that's probably just important to note what you're doing. So so people notice.", "tokens": [400, 300, 311, 746, 611, 300, 311, 1391, 445, 1021, 281, 3637, 437, 291, 434, 884, 13, 407, 370, 561, 3449, 13], "temperature": 0.0, "avg_logprob": -0.15938470503863167, "compression_ratio": 1.6425339366515836, "no_speech_prob": 1.1188903954462148e-06}, {"id": 124, "seek": 123400, "start": 1239.0, "end": 1247.0, "text": " But again as as people are moving towards more complex models the use of stop words is falling out of favor.", "tokens": [583, 797, 382, 382, 561, 366, 2684, 3030, 544, 3997, 5245, 264, 764, 295, 1590, 2283, 307, 7440, 484, 295, 2294, 13], "temperature": 0.0, "avg_logprob": -0.15938470503863167, "compression_ratio": 1.6425339366515836, "no_speech_prob": 1.1188903954462148e-06}, {"id": 125, "seek": 123400, "start": 1247.0, "end": 1261.0, "text": " Although here in this notebook we're going to be doing a pretty it's a simpler simpler model SVD so we will use stop where we will remove stop words.", "tokens": [5780, 510, 294, 341, 21060, 321, 434, 516, 281, 312, 884, 257, 1238, 309, 311, 257, 18587, 18587, 2316, 31910, 35, 370, 321, 486, 764, 1590, 689, 321, 486, 4159, 1590, 2283, 13], "temperature": 0.0, "avg_logprob": -0.15938470503863167, "compression_ratio": 1.6425339366515836, "no_speech_prob": 1.1188903954462148e-06}, {"id": 126, "seek": 126100, "start": 1261.0, "end": 1265.0, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.13342904177579012, "compression_ratio": 1.5176470588235293, "no_speech_prob": 1.0288595149177127e-05}, {"id": 127, "seek": 126100, "start": 1265.0, "end": 1270.0, "text": " Any other questions about stop words.", "tokens": [2639, 661, 1651, 466, 1590, 2283, 13], "temperature": 0.0, "avg_logprob": -0.13342904177579012, "compression_ratio": 1.5176470588235293, "no_speech_prob": 1.0288595149177127e-05}, {"id": 128, "seek": 126100, "start": 1270.0, "end": 1288.0, "text": " Okay. And so then just as I mentioned mentioned earlier in in more complex models and particularly in deep learning this could hurt your performance because they are they are ways of throwing away information.", "tokens": [1033, 13, 400, 370, 550, 445, 382, 286, 2835, 2835, 3071, 294, 294, 544, 3997, 5245, 293, 4098, 294, 2452, 2539, 341, 727, 4607, 428, 3389, 570, 436, 366, 436, 366, 2098, 295, 10238, 1314, 1589, 13], "temperature": 0.0, "avg_logprob": -0.13342904177579012, "compression_ratio": 1.5176470588235293, "no_speech_prob": 1.0288595149177127e-05}, {"id": 129, "seek": 128800, "start": 1288.0, "end": 1295.0, "text": " Although that's you know can be useful when you have a simpler model and are not able to capture that complexity.", "tokens": [5780, 300, 311, 291, 458, 393, 312, 4420, 562, 291, 362, 257, 18587, 2316, 293, 366, 406, 1075, 281, 7983, 300, 14024, 13], "temperature": 0.0, "avg_logprob": -0.07211080926363586, "compression_ratio": 1.5375722543352601, "no_speech_prob": 2.6270890884916298e-05}, {"id": 130, "seek": 128800, "start": 1295.0, "end": 1307.0, "text": " And then another approach that I have not had a chance to try but that I have heard great things about and I'm interested in is sentence piece or these.", "tokens": [400, 550, 1071, 3109, 300, 286, 362, 406, 632, 257, 2931, 281, 853, 457, 300, 286, 362, 2198, 869, 721, 466, 293, 286, 478, 3102, 294, 307, 8174, 2522, 420, 613, 13], "temperature": 0.0, "avg_logprob": -0.07211080926363586, "compression_ratio": 1.5375722543352601, "no_speech_prob": 2.6270890884916298e-05}, {"id": 131, "seek": 130700, "start": 1307.0, "end": 1319.0, "text": " I think there's some other techniques for forming sub word tokens. And so instead of we'll talk about tokenization in a moment.", "tokens": [286, 519, 456, 311, 512, 661, 7512, 337, 15745, 1422, 1349, 22667, 13, 400, 370, 2602, 295, 321, 603, 751, 466, 14862, 2144, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.08494501980868253, "compression_ratio": 1.5063291139240507, "no_speech_prob": 5.919819523114711e-05}, {"id": 132, "seek": 130700, "start": 1319.0, "end": 1330.0, "text": " We're going to typically be thinking of our words as our units and sentence piece thinks about sub word units.", "tokens": [492, 434, 516, 281, 5850, 312, 1953, 295, 527, 2283, 382, 527, 6815, 293, 8174, 2522, 7309, 466, 1422, 1349, 6815, 13], "temperature": 0.0, "avg_logprob": -0.08494501980868253, "compression_ratio": 1.5063291139240507, "no_speech_prob": 5.919819523114711e-05}, {"id": 133, "seek": 133000, "start": 1330.0, "end": 1340.0, "text": " So that's that could be something to look into or even something to try using in the project you do for your blog post.", "tokens": [407, 300, 311, 300, 727, 312, 746, 281, 574, 666, 420, 754, 746, 281, 853, 1228, 294, 264, 1716, 291, 360, 337, 428, 6968, 2183, 13], "temperature": 0.0, "avg_logprob": -0.10405731201171875, "compression_ratio": 1.5555555555555556, "no_speech_prob": 6.338695584418019e-06}, {"id": 134, "seek": 133000, "start": 1340.0, "end": 1356.0, "text": " Okay so going back to the problem at hand of topic modeling looking at this collection of postings that we have on computer graphics space atheism and religion.", "tokens": [1033, 370, 516, 646, 281, 264, 1154, 412, 1011, 295, 4829, 15983, 1237, 412, 341, 5765, 295, 2183, 1109, 300, 321, 362, 322, 3820, 11837, 1901, 27033, 1434, 293, 7561, 13], "temperature": 0.0, "avg_logprob": -0.10405731201171875, "compression_ratio": 1.5555555555555556, "no_speech_prob": 6.338695584418019e-06}, {"id": 135, "seek": 135600, "start": 1356.0, "end": 1362.0, "text": " So we're going to be using today we'll be using psychic learns count vectorizer.", "tokens": [407, 321, 434, 516, 281, 312, 1228, 965, 321, 603, 312, 1228, 35406, 27152, 1207, 8062, 6545, 13], "temperature": 0.0, "avg_logprob": -0.09997275190533332, "compression_ratio": 1.4647887323943662, "no_speech_prob": 1.3419341485132463e-05}, {"id": 136, "seek": 135600, "start": 1362.0, "end": 1370.0, "text": " Although in the future we'll kind of dig more into how you would create tokens on your own or do this sort of work on your own.", "tokens": [5780, 294, 264, 2027, 321, 603, 733, 295, 2528, 544, 666, 577, 291, 576, 1884, 22667, 322, 428, 1065, 420, 360, 341, 1333, 295, 589, 322, 428, 1065, 13], "temperature": 0.0, "avg_logprob": -0.09997275190533332, "compression_ratio": 1.4647887323943662, "no_speech_prob": 1.3419341485132463e-05}, {"id": 137, "seek": 137000, "start": 1370.0, "end": 1386.0, "text": " Let me run these. So.", "tokens": [961, 385, 1190, 613, 13, 407, 13], "temperature": 0.0, "avg_logprob": -0.22947114164179022, "compression_ratio": 0.7241379310344828, "no_speech_prob": 5.2248622523620725e-05}, {"id": 138, "seek": 138600, "start": 1386.0, "end": 1401.0, "text": " We're creating creating vectors. We have the shape of two thousand thirty four by twenty six thousand five hundred seventy six here the two thousand is the different the different postings.", "tokens": [492, 434, 4084, 4084, 18875, 13, 492, 362, 264, 3909, 295, 732, 4714, 11790, 1451, 538, 7699, 2309, 4714, 1732, 3262, 25662, 2309, 510, 264, 732, 4714, 307, 264, 819, 264, 819, 2183, 1109, 13], "temperature": 0.0, "avg_logprob": -0.15790550744355614, "compression_ratio": 1.6787564766839378, "no_speech_prob": 4.7568228183081374e-05}, {"id": 139, "seek": 138600, "start": 1401.0, "end": 1410.0, "text": " So those are you know separate. I guess the modern equivalent you could think of posting on Reddit or another kind of discussion site.", "tokens": [407, 729, 366, 291, 458, 4994, 13, 286, 2041, 264, 4363, 10344, 291, 727, 519, 295, 15978, 322, 32210, 420, 1071, 733, 295, 5017, 3621, 13], "temperature": 0.0, "avg_logprob": -0.15790550744355614, "compression_ratio": 1.6787564766839378, "no_speech_prob": 4.7568228183081374e-05}, {"id": 140, "seek": 141000, "start": 1410.0, "end": 1417.0, "text": " So you know the two thousand different postings and then we have twenty six thousand.", "tokens": [407, 291, 458, 264, 732, 4714, 819, 2183, 1109, 293, 550, 321, 362, 7699, 2309, 4714, 13], "temperature": 0.0, "avg_logprob": -0.12394524593742526, "compression_ratio": 1.4202898550724639, "no_speech_prob": 1.0952564480248839e-05}, {"id": 141, "seek": 141000, "start": 1417.0, "end": 1429.0, "text": " Words or tokens we can look at our vocab so vectorizer has this get feature names and this is a list of words.", "tokens": [32857, 420, 22667, 321, 393, 574, 412, 527, 2329, 455, 370, 8062, 6545, 575, 341, 483, 4111, 5288, 293, 341, 307, 257, 1329, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.12394524593742526, "compression_ratio": 1.4202898550724639, "no_speech_prob": 1.0952564480248839e-05}, {"id": 142, "seek": 142900, "start": 1429.0, "end": 1440.0, "text": " So here I was looking at words seven thousand to seven thousand and twenty you can see there in alphabetical art alphabetical order.", "tokens": [407, 510, 286, 390, 1237, 412, 2283, 3407, 4714, 281, 3407, 4714, 293, 7699, 291, 393, 536, 456, 294, 23339, 804, 1523, 23339, 804, 1668, 13], "temperature": 0.0, "avg_logprob": -0.17837450663248697, "compression_ratio": 1.608433734939759, "no_speech_prob": 5.0145886234531645e-06}, {"id": 143, "seek": 142900, "start": 1440.0, "end": 1445.0, "text": " We kind of have all these words that start with C.O.", "tokens": [492, 733, 295, 362, 439, 613, 2283, 300, 722, 365, 383, 13, 46, 13], "temperature": 0.0, "avg_logprob": -0.17837450663248697, "compression_ratio": 1.608433734939759, "no_speech_prob": 5.0145886234531645e-06}, {"id": 144, "seek": 142900, "start": 1445.0, "end": 1451.0, "text": " And I think. Actually before I go on to.", "tokens": [400, 286, 519, 13, 5135, 949, 286, 352, 322, 281, 13], "temperature": 0.0, "avg_logprob": -0.17837450663248697, "compression_ratio": 1.608433734939759, "no_speech_prob": 5.0145886234531645e-06}, {"id": 145, "seek": 142900, "start": 1451.0, "end": 1455.0, "text": " Well yeah before I go on to using S.V.D.", "tokens": [1042, 1338, 949, 286, 352, 322, 281, 1228, 318, 13, 53, 13, 35, 13], "temperature": 0.0, "avg_logprob": -0.17837450663248697, "compression_ratio": 1.608433734939759, "no_speech_prob": 5.0145886234531645e-06}, {"id": 146, "seek": 145500, "start": 1455.0, "end": 1465.0, "text": " I'm going to show you what this looks like in a Excel spreadsheet because I think this can be a more visual way of seeing seeing what's happening.", "tokens": [286, 478, 516, 281, 855, 291, 437, 341, 1542, 411, 294, 257, 19060, 27733, 570, 286, 519, 341, 393, 312, 257, 544, 5056, 636, 295, 2577, 2577, 437, 311, 2737, 13], "temperature": 0.0, "avg_logprob": -0.10440374880420919, "compression_ratio": 1.4055944055944056, "no_speech_prob": 2.0143328583799303e-05}, {"id": 147, "seek": 145500, "start": 1465.0, "end": 1471.0, "text": " And so and this the spreadsheet is in the GitHub repo.", "tokens": [400, 370, 293, 341, 264, 27733, 307, 294, 264, 23331, 49040, 13], "temperature": 0.0, "avg_logprob": -0.10440374880420919, "compression_ratio": 1.4055944055944056, "no_speech_prob": 2.0143328583799303e-05}, {"id": 148, "seek": 147100, "start": 1471.0, "end": 1487.0, "text": " So here I'm looking at kind of just a very small data set so I wanted to be able to put it into a spreadsheet but twenty seven British novels with just sixty four vocabulary words not.", "tokens": [407, 510, 286, 478, 1237, 412, 733, 295, 445, 257, 588, 1359, 1412, 992, 370, 286, 1415, 281, 312, 1075, 281, 829, 309, 666, 257, 27733, 457, 7699, 3407, 6221, 24574, 365, 445, 21390, 1451, 19864, 2283, 406, 13], "temperature": 0.0, "avg_logprob": -0.10161338533673968, "compression_ratio": 1.5238095238095237, "no_speech_prob": 2.2122960217529908e-05}, {"id": 149, "seek": 147100, "start": 1487.0, "end": 1493.0, "text": " They had fifty five thousand vocabulary words altogether but we're just considering sixty four of them.", "tokens": [814, 632, 13442, 1732, 4714, 19864, 2283, 19051, 457, 321, 434, 445, 8079, 21390, 1451, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.10161338533673968, "compression_ratio": 1.5238095238095237, "no_speech_prob": 2.2122960217529908e-05}, {"id": 150, "seek": 149300, "start": 1493.0, "end": 1505.0, "text": " And they I kind of did the I did the work for this in a in a Python notebook and then I extracted it just so you could visually see like what what does this look like.", "tokens": [400, 436, 286, 733, 295, 630, 264, 286, 630, 264, 589, 337, 341, 294, 257, 294, 257, 15329, 21060, 293, 550, 286, 34086, 309, 445, 370, 291, 727, 19622, 536, 411, 437, 437, 775, 341, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.09322347396459335, "compression_ratio": 1.621761658031088, "no_speech_prob": 1.0288731573382393e-05}, {"id": 151, "seek": 149300, "start": 1505.0, "end": 1509.0, "text": " So here is the term document matrix.", "tokens": [407, 510, 307, 264, 1433, 4166, 8141, 13], "temperature": 0.0, "avg_logprob": -0.09322347396459335, "compression_ratio": 1.621761658031088, "no_speech_prob": 1.0288731573382393e-05}, {"id": 152, "seek": 149300, "start": 1509.0, "end": 1516.0, "text": " So along the side these are the different novels and it's the author's name and then the start of the title.", "tokens": [407, 2051, 264, 1252, 613, 366, 264, 819, 24574, 293, 309, 311, 264, 3793, 311, 1315, 293, 550, 264, 722, 295, 264, 4876, 13], "temperature": 0.0, "avg_logprob": -0.09322347396459335, "compression_ratio": 1.621761658031088, "no_speech_prob": 1.0288731573382393e-05}, {"id": 153, "seek": 151600, "start": 1516.0, "end": 1523.0, "text": " So this is Jane Austen sense and sense of sensibility Charles Dickens Bleak House.", "tokens": [407, 341, 307, 13048, 4126, 268, 2020, 293, 2020, 295, 2923, 2841, 10523, 18754, 694, 30721, 514, 4928, 13], "temperature": 0.0, "avg_logprob": -0.12743794516231235, "compression_ratio": 1.5732217573221758, "no_speech_prob": 5.4219694902712945e-06}, {"id": 154, "seek": 151600, "start": 1523.0, "end": 1534.0, "text": " And then along here we have the vocabulary words we're looking at in this case and they've been normalized using an approach called TFIDF which I'll talk about in a moment.", "tokens": [400, 550, 2051, 510, 321, 362, 264, 19864, 2283, 321, 434, 1237, 412, 294, 341, 1389, 293, 436, 600, 668, 48704, 1228, 364, 3109, 1219, 40964, 2777, 37, 597, 286, 603, 751, 466, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.12743794516231235, "compression_ratio": 1.5732217573221758, "no_speech_prob": 5.4219694902712945e-06}, {"id": 155, "seek": 151600, "start": 1534.0, "end": 1545.0, "text": " But that just takes and takes into account kind of how many words are in each document and how how rare those words are.", "tokens": [583, 300, 445, 2516, 293, 2516, 666, 2696, 733, 295, 577, 867, 2283, 366, 294, 1184, 4166, 293, 577, 577, 5892, 729, 2283, 366, 13], "temperature": 0.0, "avg_logprob": -0.12743794516231235, "compression_ratio": 1.5732217573221758, "no_speech_prob": 5.4219694902712945e-06}, {"id": 156, "seek": 154500, "start": 1545.0, "end": 1549.0, "text": " So you'll see Kathy does not show up in most of these books.", "tokens": [407, 291, 603, 536, 30740, 775, 406, 855, 493, 294, 881, 295, 613, 3642, 13], "temperature": 0.0, "avg_logprob": -0.09320756047964096, "compression_ratio": 1.532934131736527, "no_speech_prob": 2.6270759917679243e-05}, {"id": 157, "seek": 154500, "start": 1549.0, "end": 1556.0, "text": " It does show up in Wuthering Heights where Kathy is one of the main characters.", "tokens": [467, 775, 855, 493, 294, 343, 17696, 278, 44039, 689, 30740, 307, 472, 295, 264, 2135, 4342, 13], "temperature": 0.0, "avg_logprob": -0.09320756047964096, "compression_ratio": 1.532934131736527, "no_speech_prob": 2.6270759917679243e-05}, {"id": 158, "seek": 154500, "start": 1556.0, "end": 1567.0, "text": " And so this is this is how you are or how we are representing this collection of novels using the vocabulary words.", "tokens": [400, 370, 341, 307, 341, 307, 577, 291, 366, 420, 577, 321, 366, 13460, 341, 5765, 295, 24574, 1228, 264, 19864, 2283, 13], "temperature": 0.0, "avg_logprob": -0.09320756047964096, "compression_ratio": 1.532934131736527, "no_speech_prob": 2.6270759917679243e-05}, {"id": 159, "seek": 156700, "start": 1567.0, "end": 1575.0, "text": " And so I'm going to show you what SVD gives you SVD.", "tokens": [400, 370, 286, 478, 516, 281, 855, 291, 437, 31910, 35, 2709, 291, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.14056455570718515, "compression_ratio": 1.3363636363636364, "no_speech_prob": 1.1300508958811406e-05}, {"id": 160, "seek": 156700, "start": 1575.0, "end": 1578.0, "text": " It's going to give you three matrices back.", "tokens": [467, 311, 516, 281, 976, 291, 1045, 32284, 646, 13], "temperature": 0.0, "avg_logprob": -0.14056455570718515, "compression_ratio": 1.3363636363636364, "no_speech_prob": 1.1300508958811406e-05}, {"id": 161, "seek": 156700, "start": 1578.0, "end": 1588.0, "text": " So you is going to be the documents by the topics.", "tokens": [407, 291, 307, 516, 281, 312, 264, 8512, 538, 264, 8378, 13], "temperature": 0.0, "avg_logprob": -0.14056455570718515, "compression_ratio": 1.3363636363636364, "no_speech_prob": 1.1300508958811406e-05}, {"id": 162, "seek": 158800, "start": 1588.0, "end": 1597.0, "text": " S these are called the singular values but they are basically telling you kind of the importance or scale of each topic.", "tokens": [318, 613, 366, 1219, 264, 20010, 4190, 457, 436, 366, 1936, 3585, 291, 733, 295, 264, 7379, 420, 4373, 295, 1184, 4829, 13], "temperature": 0.0, "avg_logprob": -0.1203442891438802, "compression_ratio": 1.592964824120603, "no_speech_prob": 2.796604167087935e-05}, {"id": 163, "seek": 158800, "start": 1597.0, "end": 1600.0, "text": " You'll notice that S is a diagonal matrix matrix.", "tokens": [509, 603, 3449, 300, 318, 307, 257, 21539, 8141, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1203442891438802, "compression_ratio": 1.592964824120603, "no_speech_prob": 2.796604167087935e-05}, {"id": 164, "seek": 158800, "start": 1600.0, "end": 1606.0, "text": " So it has non diagonal values sorry non zero values down the diagonal.", "tokens": [407, 309, 575, 2107, 21539, 4190, 2597, 2107, 4018, 4190, 760, 264, 21539, 13], "temperature": 0.0, "avg_logprob": -0.1203442891438802, "compression_ratio": 1.592964824120603, "no_speech_prob": 2.796604167087935e-05}, {"id": 165, "seek": 158800, "start": 1606.0, "end": 1608.0, "text": " Everything else is zero.", "tokens": [5471, 1646, 307, 4018, 13], "temperature": 0.0, "avg_logprob": -0.1203442891438802, "compression_ratio": 1.592964824120603, "no_speech_prob": 2.796604167087935e-05}, {"id": 166, "seek": 158800, "start": 1608.0, "end": 1613.0, "text": " And then V relates topics to the vocabulary words.", "tokens": [400, 550, 691, 16155, 8378, 281, 264, 19864, 2283, 13], "temperature": 0.0, "avg_logprob": -0.1203442891438802, "compression_ratio": 1.592964824120603, "no_speech_prob": 2.796604167087935e-05}, {"id": 167, "seek": 161300, "start": 1613.0, "end": 1629.0, "text": " And so you can kind of look through actually I guess I should check does anyone have a favorite book among this collection that they they want to look at.", "tokens": [400, 370, 291, 393, 733, 295, 574, 807, 767, 286, 2041, 286, 820, 1520, 775, 2878, 362, 257, 2954, 1446, 3654, 341, 5765, 300, 436, 436, 528, 281, 574, 412, 13], "temperature": 0.0, "avg_logprob": -0.12401479993547712, "compression_ratio": 1.3275862068965518, "no_speech_prob": 5.173725639906479e-06}, {"id": 168, "seek": 162900, "start": 1629.0, "end": 1643.0, "text": " OK so I was I was scrolling through yesterday just when I was preparing I think I was looking at the end of the list and I noticed that uncle was largest.", "tokens": [2264, 370, 286, 390, 286, 390, 29053, 807, 5186, 445, 562, 286, 390, 10075, 286, 519, 286, 390, 1237, 412, 264, 917, 295, 264, 1329, 293, 286, 5694, 300, 9153, 390, 6443, 13], "temperature": 0.0, "avg_logprob": -0.18586579808649026, "compression_ratio": 1.4863013698630136, "no_speech_prob": 2.931050767074339e-05}, {"id": 169, "seek": 162900, "start": 1643.0, "end": 1651.0, "text": " Uncle's largest here in I guess this must be topic topic five.", "tokens": [12347, 311, 6443, 510, 294, 286, 2041, 341, 1633, 312, 4829, 4829, 1732, 13], "temperature": 0.0, "avg_logprob": -0.18586579808649026, "compression_ratio": 1.4863013698630136, "no_speech_prob": 2.931050767074339e-05}, {"id": 170, "seek": 165100, "start": 1651.0, "end": 1665.0, "text": " So then we can come over here and see OK topic five and I saw Stern Tristram has a high relatively high number point four seven for it.", "tokens": [407, 550, 321, 393, 808, 670, 510, 293, 536, 2264, 4829, 1732, 293, 286, 1866, 39538, 1765, 468, 2356, 575, 257, 1090, 7226, 1090, 1230, 935, 1451, 3407, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.11632850035181586, "compression_ratio": 1.3741496598639455, "no_speech_prob": 7.071718300721841e-06}, {"id": 171, "seek": 165100, "start": 1665.0, "end": 1668.0, "text": " I actually was not familiar with this book so I had to look it up.", "tokens": [286, 767, 390, 406, 4963, 365, 341, 1446, 370, 286, 632, 281, 574, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.11632850035181586, "compression_ratio": 1.3741496598639455, "no_speech_prob": 7.071718300721841e-06}, {"id": 172, "seek": 166800, "start": 1668.0, "end": 1685.0, "text": " Let me pull that up.", "tokens": [961, 385, 2235, 300, 493, 13], "temperature": 0.0, "avg_logprob": -0.15765128135681153, "compression_ratio": 0.7142857142857143, "no_speech_prob": 5.9548128774622455e-06}, {"id": 173, "seek": 168500, "start": 1685.0, "end": 1701.0, "text": " Yeah so it's the life and opinions of Tristram Shandley gentlemen and it turns out that one of the main characters is the uncle so that that at least fits with when we find it.", "tokens": [865, 370, 309, 311, 264, 993, 293, 11819, 295, 1765, 468, 2356, 1160, 474, 3420, 11669, 293, 309, 4523, 484, 300, 472, 295, 264, 2135, 4342, 307, 264, 9153, 370, 300, 300, 412, 1935, 9001, 365, 562, 321, 915, 309, 13], "temperature": 0.0, "avg_logprob": -0.15054095418829666, "compression_ratio": 1.5968586387434556, "no_speech_prob": 7.0710229920223355e-06}, {"id": 174, "seek": 168500, "start": 1701.0, "end": 1702.0, "text": " Here it is. Yeah.", "tokens": [1692, 309, 307, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.15054095418829666, "compression_ratio": 1.5968586387434556, "no_speech_prob": 7.0710229920223355e-06}, {"id": 175, "seek": 168500, "start": 1702.0, "end": 1709.0, "text": " Apart from Tristram is narrator the most familiar and important characters in the book include his uncle Toby.", "tokens": [24111, 490, 1765, 468, 2356, 307, 32646, 264, 881, 4963, 293, 1021, 4342, 294, 264, 1446, 4090, 702, 9153, 40223, 13], "temperature": 0.0, "avg_logprob": -0.15054095418829666, "compression_ratio": 1.5968586387434556, "no_speech_prob": 7.0710229920223355e-06}, {"id": 176, "seek": 170900, "start": 1709.0, "end": 1717.0, "text": " And actually we could go back and let's see if Toby is a.", "tokens": [400, 767, 321, 727, 352, 646, 293, 718, 311, 536, 498, 40223, 307, 257, 13], "temperature": 0.0, "avg_logprob": -0.1305786371231079, "compression_ratio": 1.3047619047619048, "no_speech_prob": 2.443949142616475e-06}, {"id": 177, "seek": 170900, "start": 1717.0, "end": 1724.0, "text": " Let's see if Toby is one of the vocabulary words may not have made the cut off.", "tokens": [961, 311, 536, 498, 40223, 307, 472, 295, 264, 19864, 2283, 815, 406, 362, 1027, 264, 1723, 766, 13], "temperature": 0.0, "avg_logprob": -0.1305786371231079, "compression_ratio": 1.3047619047619048, "no_speech_prob": 2.443949142616475e-06}, {"id": 178, "seek": 172400, "start": 1724.0, "end": 1745.0, "text": " OK yeah so Toby's in here Toby is largest for topic five again so topic five involves uncle and Toby and when we go over one of the books that has a lot of topic five in it is the life and times.", "tokens": [2264, 1338, 370, 40223, 311, 294, 510, 40223, 307, 6443, 337, 4829, 1732, 797, 370, 4829, 1732, 11626, 9153, 293, 40223, 293, 562, 321, 352, 670, 472, 295, 264, 3642, 300, 575, 257, 688, 295, 4829, 1732, 294, 309, 307, 264, 993, 293, 1413, 13], "temperature": 0.0, "avg_logprob": -0.12851619720458984, "compression_ratio": 1.515527950310559, "no_speech_prob": 1.3210914403316565e-05}, {"id": 179, "seek": 172400, "start": 1745.0, "end": 1749.0, "text": " I've already forgotten the name Tristram Shandy.", "tokens": [286, 600, 1217, 11832, 264, 1315, 1765, 468, 2356, 1160, 11425, 13], "temperature": 0.0, "avg_logprob": -0.12851619720458984, "compression_ratio": 1.515527950310559, "no_speech_prob": 1.3210914403316565e-05}, {"id": 180, "seek": 174900, "start": 1749.0, "end": 1764.0, "text": " Any questions about this one so we'll see this from the kind of the Python point of view but I think there can be something nice about the visual and of where we're going kind of what these topics look like.", "tokens": [2639, 1651, 466, 341, 472, 370, 321, 603, 536, 341, 490, 264, 733, 295, 264, 15329, 935, 295, 1910, 457, 286, 519, 456, 393, 312, 746, 1481, 466, 264, 5056, 293, 295, 689, 321, 434, 516, 733, 295, 437, 613, 8378, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.08805421613297372, "compression_ratio": 1.4662162162162162, "no_speech_prob": 3.071183891734108e-05}, {"id": 181, "seek": 174900, "start": 1764.0, "end": 1769.0, "text": " Oh sorry.", "tokens": [876, 2597, 13], "temperature": 0.0, "avg_logprob": -0.08805421613297372, "compression_ratio": 1.4662162162162162, "no_speech_prob": 3.071183891734108e-05}, {"id": 182, "seek": 176900, "start": 1769.0, "end": 1779.0, "text": " Just briefly explain again what each matrix are and how we got them. Sure so I am. Yeah I have not told you how we got them yet. So I kind of cheated.", "tokens": [1449, 10515, 2903, 797, 437, 1184, 8141, 366, 293, 577, 321, 658, 552, 13, 4894, 370, 286, 669, 13, 865, 286, 362, 406, 1907, 291, 577, 321, 658, 552, 1939, 13, 407, 286, 733, 295, 28079, 13], "temperature": 0.0, "avg_logprob": -0.16595653340786318, "compression_ratio": 1.5252525252525253, "no_speech_prob": 2.1777557776658796e-05}, {"id": 183, "seek": 176900, "start": 1779.0, "end": 1790.0, "text": " What did you just see where we were going. That's a fair question. So these these matrices are the result of SVD and we'll talk about that in a moment.", "tokens": [708, 630, 291, 445, 536, 689, 321, 645, 516, 13, 663, 311, 257, 3143, 1168, 13, 407, 613, 613, 32284, 366, 264, 1874, 295, 31910, 35, 293, 321, 603, 751, 466, 300, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.16595653340786318, "compression_ratio": 1.5252525252525253, "no_speech_prob": 2.1777557776658796e-05}, {"id": 184, "seek": 179000, "start": 1790.0, "end": 1805.0, "text": " But I just want to let you know what they are first so you know that kind of what we're producing and the first one is a matrix that is documents by topics.", "tokens": [583, 286, 445, 528, 281, 718, 291, 458, 437, 436, 366, 700, 370, 291, 458, 300, 733, 295, 437, 321, 434, 10501, 293, 264, 700, 472, 307, 257, 8141, 300, 307, 8512, 538, 8378, 13], "temperature": 0.0, "avg_logprob": -0.06420274881216195, "compression_ratio": 1.3448275862068966, "no_speech_prob": 1.2218387382745277e-05}, {"id": 185, "seek": 180500, "start": 1805.0, "end": 1820.0, "text": " The middle one and we'll talk. Okay. I'll kind of do the spoiler as well. The columns of this matrix are worth the normal to each other which I haven't haven't showed you yet.", "tokens": [440, 2808, 472, 293, 321, 603, 751, 13, 1033, 13, 286, 603, 733, 295, 360, 264, 26927, 382, 731, 13, 440, 13766, 295, 341, 8141, 366, 3163, 264, 2710, 281, 1184, 661, 597, 286, 2378, 380, 2378, 380, 4712, 291, 1939, 13], "temperature": 0.0, "avg_logprob": -0.16826566060384116, "compression_ratio": 1.5402298850574712, "no_speech_prob": 3.647316771093756e-05}, {"id": 186, "seek": 180500, "start": 1820.0, "end": 1830.0, "text": " Does anyone anyone remember what it means for vectors to be worth the normal set of vectors.", "tokens": [4402, 2878, 2878, 1604, 437, 309, 1355, 337, 18875, 281, 312, 3163, 264, 2710, 992, 295, 18875, 13], "temperature": 0.0, "avg_logprob": -0.16826566060384116, "compression_ratio": 1.5402298850574712, "no_speech_prob": 3.647316771093756e-05}, {"id": 187, "seek": 183000, "start": 1830.0, "end": 1838.0, "text": " Oh yeah. So I see. Oh is that a hand.", "tokens": [876, 1338, 13, 407, 286, 536, 13, 876, 307, 300, 257, 1011, 13], "temperature": 0.0, "avg_logprob": -0.17800201177597047, "compression_ratio": 1.125, "no_speech_prob": 3.023554563696962e-05}, {"id": 188, "seek": 183000, "start": 1838.0, "end": 1843.0, "text": " I'm not going to be able to get it all the way back there.", "tokens": [286, 478, 406, 516, 281, 312, 1075, 281, 483, 309, 439, 264, 636, 646, 456, 13], "temperature": 0.0, "avg_logprob": -0.17800201177597047, "compression_ratio": 1.125, "no_speech_prob": 3.023554563696962e-05}, {"id": 189, "seek": 183000, "start": 1843.0, "end": 1848.0, "text": " Yeah sorry.", "tokens": [865, 2597, 13], "temperature": 0.0, "avg_logprob": -0.17800201177597047, "compression_ratio": 1.125, "no_speech_prob": 3.023554563696962e-05}, {"id": 190, "seek": 184800, "start": 1848.0, "end": 1860.0, "text": " I believe that I believe that with normal vectors they're orthogonal to each other and also the magnitude is one.", "tokens": [286, 1697, 300, 286, 1697, 300, 365, 2710, 18875, 436, 434, 41488, 281, 1184, 661, 293, 611, 264, 15668, 307, 472, 13], "temperature": 0.0, "avg_logprob": -0.2565469452829072, "compression_ratio": 1.7337662337662338, "no_speech_prob": 6.852827482362045e-06}, {"id": 191, "seek": 184800, "start": 1860.0, "end": 1865.0, "text": " That's right. Their magnitudes one they're orthogonal to each other. What does it mean to be orthogonal.", "tokens": [663, 311, 558, 13, 6710, 4944, 16451, 472, 436, 434, 41488, 281, 1184, 661, 13, 708, 775, 309, 914, 281, 312, 41488, 13], "temperature": 0.0, "avg_logprob": -0.2565469452829072, "compression_ratio": 1.7337662337662338, "no_speech_prob": 6.852827482362045e-06}, {"id": 192, "seek": 184800, "start": 1865.0, "end": 1874.0, "text": " 90 degrees. Yeah 90 degrees. And so I thank you.", "tokens": [4289, 5310, 13, 865, 4289, 5310, 13, 400, 370, 286, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.2565469452829072, "compression_ratio": 1.7337662337662338, "no_speech_prob": 6.852827482362045e-06}, {"id": 193, "seek": 187400, "start": 1874.0, "end": 1886.0, "text": " Thank you. Yeah I saw several people kind of doing this handsome signal. Yeah they're they're perpendicular to each other which means their dot product.", "tokens": [1044, 291, 13, 865, 286, 1866, 2940, 561, 733, 295, 884, 341, 13421, 6358, 13, 865, 436, 434, 436, 434, 26734, 281, 1184, 661, 597, 1355, 641, 5893, 1674, 13], "temperature": 0.0, "avg_logprob": -0.11131174723307291, "compression_ratio": 1.6323529411764706, "no_speech_prob": 1.9831841200357303e-05}, {"id": 194, "seek": 187400, "start": 1886.0, "end": 1900.0, "text": " So for the set two different vectors are going to have a dot product of zero together and dotting one with itself will have a dot product of one, which is why it has magnitude one.", "tokens": [407, 337, 264, 992, 732, 819, 18875, 366, 516, 281, 362, 257, 5893, 1674, 295, 4018, 1214, 293, 5893, 783, 472, 365, 2564, 486, 362, 257, 5893, 1674, 295, 472, 11, 597, 307, 983, 309, 575, 15668, 472, 13], "temperature": 0.0, "avg_logprob": -0.11131174723307291, "compression_ratio": 1.6323529411764706, "no_speech_prob": 1.9831841200357303e-05}, {"id": 195, "seek": 190000, "start": 1900.0, "end": 1909.0, "text": " So these these different topics are orthogonal to each other. These columns.", "tokens": [407, 613, 613, 819, 8378, 366, 41488, 281, 1184, 661, 13, 1981, 13766, 13], "temperature": 0.0, "avg_logprob": -0.07755182439630681, "compression_ratio": 1.5301204819277108, "no_speech_prob": 2.2473099306807853e-05}, {"id": 196, "seek": 190000, "start": 1909.0, "end": 1922.0, "text": " Then we've got this matrix in the middle where these are called the singular values and they're going to be positive numbers and then everything that's not on the diagonal zero.", "tokens": [1396, 321, 600, 658, 341, 8141, 294, 264, 2808, 689, 613, 366, 1219, 264, 20010, 4190, 293, 436, 434, 516, 281, 312, 3353, 3547, 293, 550, 1203, 300, 311, 406, 322, 264, 21539, 4018, 13], "temperature": 0.0, "avg_logprob": -0.07755182439630681, "compression_ratio": 1.5301204819277108, "no_speech_prob": 2.2473099306807853e-05}, {"id": 197, "seek": 192200, "start": 1922.0, "end": 1931.0, "text": " And then over here we have our matrix V that is topics by vocabulary words or tokens.", "tokens": [400, 550, 670, 510, 321, 362, 527, 8141, 691, 300, 307, 8378, 538, 19864, 2283, 420, 22667, 13], "temperature": 0.0, "avg_logprob": -0.10354941421084934, "compression_ratio": 1.2523364485981308, "no_speech_prob": 4.784968041349202e-06}, {"id": 198, "seek": 192200, "start": 1931.0, "end": 1937.0, "text": " And here the rows are orthonormal to each other.", "tokens": [400, 510, 264, 13241, 366, 420, 11943, 24440, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.10354941421084934, "compression_ratio": 1.2523364485981308, "no_speech_prob": 4.784968041349202e-06}, {"id": 199, "seek": 193700, "start": 1937.0, "end": 1956.0, "text": " So let's go back to the notebook.", "tokens": [407, 718, 311, 352, 646, 281, 264, 21060, 13], "temperature": 0.0, "avg_logprob": -0.12150188592764047, "compression_ratio": 0.8048780487804879, "no_speech_prob": 9.368457540404052e-06}, {"id": 200, "seek": 195600, "start": 1956.0, "end": 1972.0, "text": " Sorry. Oops. OK. So Gilbert Strang who's the author of a classic linear algebra textbook said SVD is not nearly as famous as it should be.", "tokens": [4919, 13, 21726, 13, 2264, 13, 407, 39003, 8251, 656, 567, 311, 264, 3793, 295, 257, 7230, 8213, 21989, 25591, 848, 31910, 35, 307, 406, 6217, 382, 4618, 382, 309, 820, 312, 13], "temperature": 0.0, "avg_logprob": -0.14149098782925992, "compression_ratio": 1.1794871794871795, "no_speech_prob": 1.341924780717818e-05}, {"id": 201, "seek": 197200, "start": 1972.0, "end": 1997.0, "text": " SVD is a super super useful matrix decomposition that shows up a lot of places and SVD algorithm factor factorizes a matrix into one matrix with orthogonal columns one with orthogonal rows and a diagonal matrix that contains the relative importance of each factor.", "tokens": [31910, 35, 307, 257, 1687, 1687, 4420, 8141, 48356, 300, 3110, 493, 257, 688, 295, 3190, 293, 31910, 35, 9284, 5952, 5952, 5660, 257, 8141, 666, 472, 8141, 365, 41488, 13766, 472, 365, 41488, 13241, 293, 257, 21539, 8141, 300, 8306, 264, 4972, 7379, 295, 1184, 5952, 13], "temperature": 0.0, "avg_logprob": -0.08946853417616624, "compression_ratio": 1.6196319018404908, "no_speech_prob": 3.535431460477412e-05}, {"id": 202, "seek": 199700, "start": 1997.0, "end": 2006.0, "text": " SVD is an exact decomposition since the matrices it creates are big enough to fully cover the original matrix.", "tokens": [31910, 35, 307, 364, 1900, 48356, 1670, 264, 32284, 309, 7829, 366, 955, 1547, 281, 4498, 2060, 264, 3380, 8141, 13], "temperature": 0.0, "avg_logprob": -0.10164291381835938, "compression_ratio": 1.1578947368421053, "no_speech_prob": 1.80577153514605e-05}, {"id": 203, "seek": 200600, "start": 2006.0, "end": 2027.0, "text": " And it's widely widely used in linear algebra, including for semantic analysis. So what we're doing here with the topic modeling collaborative filtering or recommendations. So you can see this in things like the Netflix Prize where it, you know, used in a more complicated way, but as a component.", "tokens": [400, 309, 311, 13371, 13371, 1143, 294, 8213, 21989, 11, 3009, 337, 47982, 5215, 13, 407, 437, 321, 434, 884, 510, 365, 264, 4829, 15983, 16555, 30822, 420, 10434, 13, 407, 291, 393, 536, 341, 294, 721, 411, 264, 12778, 22604, 689, 309, 11, 291, 458, 11, 1143, 294, 257, 544, 6179, 636, 11, 457, 382, 257, 6542, 13], "temperature": 0.0, "avg_logprob": -0.16043502565414186, "compression_ratio": 1.4924623115577889, "no_speech_prob": 9.817935278988443e-06}, {"id": 204, "seek": 202700, "start": 2027.0, "end": 2036.0, "text": " Calculating pseudo inverses for matrices that don't have a true inverse data compression or PCA.", "tokens": [3511, 2444, 990, 35899, 21378, 279, 337, 32284, 300, 500, 380, 362, 257, 2074, 17340, 1412, 19355, 420, 6465, 32, 13], "temperature": 0.0, "avg_logprob": -0.08999351479790428, "compression_ratio": 1.2538461538461538, "no_speech_prob": 5.390553633333184e-05}, {"id": 205, "seek": 202700, "start": 2036.0, "end": 2044.0, "text": " Sorry, I should correct this. We will not be covering PCA in here.", "tokens": [4919, 11, 286, 820, 3006, 341, 13, 492, 486, 406, 312, 10322, 6465, 32, 294, 510, 13], "temperature": 0.0, "avg_logprob": -0.08999351479790428, "compression_ratio": 1.2538461538461538, "no_speech_prob": 5.390553633333184e-05}, {"id": 206, "seek": 204400, "start": 2044.0, "end": 2059.0, "text": " So we're going to use scikit learns implementation of SVD today. So we give it vectors, which perhaps is a confusing name. This was the term document matrix that we had gotten for for our documents.", "tokens": [407, 321, 434, 516, 281, 764, 2180, 22681, 27152, 11420, 295, 31910, 35, 965, 13, 407, 321, 976, 309, 18875, 11, 597, 4317, 307, 257, 13181, 1315, 13, 639, 390, 264, 1433, 4166, 8141, 300, 321, 632, 5768, 337, 337, 527, 8512, 13], "temperature": 0.0, "avg_logprob": -0.08344171909575766, "compression_ratio": 1.346938775510204, "no_speech_prob": 2.7106596462544985e-05}, {"id": 207, "seek": 205900, "start": 2059.0, "end": 2075.0, "text": " So we're going to say full matrices equals false. Otherwise, what it does is we'll come up with extra extra columns for you and extra rows for V to fully make an orthonormal basis.", "tokens": [407, 321, 434, 516, 281, 584, 1577, 32284, 6915, 7908, 13, 10328, 11, 437, 309, 775, 307, 321, 603, 808, 493, 365, 2857, 2857, 13766, 337, 291, 293, 2857, 13241, 337, 691, 281, 4498, 652, 364, 420, 11943, 24440, 5143, 13], "temperature": 0.0, "avg_logprob": -0.21371736197636046, "compression_ratio": 1.4230769230769231, "no_speech_prob": 3.321257827337831e-05}, {"id": 208, "seek": 205900, "start": 2075.0, "end": 2079.0, "text": " But we're kind of never going to do that.", "tokens": [583, 321, 434, 733, 295, 1128, 516, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.21371736197636046, "compression_ratio": 1.4230769230769231, "no_speech_prob": 3.321257827337831e-05}, {"id": 209, "seek": 207900, "start": 2079.0, "end": 2096.0, "text": " So we run this and we get me. Sorry, hide the answer. We get back three matrices U, S, and V.", "tokens": [407, 321, 1190, 341, 293, 321, 483, 385, 13, 4919, 11, 6479, 264, 1867, 13, 492, 483, 646, 1045, 32284, 624, 11, 318, 11, 293, 691, 13], "temperature": 0.0, "avg_logprob": -0.26117813971734816, "compression_ratio": 1.1204819277108433, "no_speech_prob": 1.7777634639060125e-05}, {"id": 210, "seek": 209600, "start": 2096.0, "end": 2117.0, "text": " And then I want I want you to. Sorry. I want you now to take a moment to do this exercise and confirm that the decomposition is the is the input.", "tokens": [400, 550, 286, 528, 286, 528, 291, 281, 13, 4919, 13, 286, 528, 291, 586, 281, 747, 257, 1623, 281, 360, 341, 5380, 293, 9064, 300, 264, 48356, 307, 264, 307, 264, 4846, 13], "temperature": 0.0, "avg_logprob": -0.14753624012595729, "compression_ratio": 1.3679245283018868, "no_speech_prob": 1.1842808817164041e-05}, {"id": 211, "seek": 211700, "start": 2117.0, "end": 2126.0, "text": " I'll show this one. So what you want to do is multiply the matrices together.", "tokens": [286, 603, 855, 341, 472, 13, 407, 437, 291, 528, 281, 360, 307, 12972, 264, 32284, 1214, 13], "temperature": 0.0, "avg_logprob": -0.13079568111535275, "compression_ratio": 1.198019801980198, "no_speech_prob": 1.4509824723063502e-05}, {"id": 212, "seek": 211700, "start": 2126.0, "end": 2130.0, "text": " I called that reconstructed vectors as you.", "tokens": [286, 1219, 300, 31499, 292, 18875, 382, 291, 13], "temperature": 0.0, "avg_logprob": -0.13079568111535275, "compression_ratio": 1.198019801980198, "no_speech_prob": 1.4509824723063502e-05}, {"id": 213, "seek": 213000, "start": 2130.0, "end": 2148.0, "text": " So did you see that as as showed up as a vector, even though the middle the middle matrix is a matrix. So this is probably the trickiest part about this and actually can show this above.", "tokens": [407, 630, 291, 536, 300, 382, 382, 4712, 493, 382, 257, 8062, 11, 754, 1673, 264, 2808, 264, 2808, 8141, 307, 257, 8141, 13, 407, 341, 307, 1391, 264, 4282, 6495, 644, 466, 341, 293, 767, 393, 855, 341, 3673, 13], "temperature": 0.0, "avg_logprob": -0.12157574759589301, "compression_ratio": 1.4645669291338583, "no_speech_prob": 2.1780657334602438e-05}, {"id": 214, "seek": 214800, "start": 2148.0, "end": 2161.0, "text": " So you can see with S is shape, you know that it's 2034 comma, meaning it's just this one dimensional vector. So NumPy has the dot diagram and that turns.", "tokens": [407, 291, 393, 536, 365, 318, 307, 3909, 11, 291, 458, 300, 309, 311, 945, 12249, 22117, 11, 3620, 309, 311, 445, 341, 472, 18795, 8062, 13, 407, 22592, 47, 88, 575, 264, 5893, 10686, 293, 300, 4523, 13], "temperature": 0.0, "avg_logprob": -0.25717160868090255, "compression_ratio": 1.2520325203252032, "no_speech_prob": 1.1300270671199542e-05}, {"id": 215, "seek": 216100, "start": 2161.0, "end": 2184.0, "text": " It's really neat if you put a one D array into it, it'll make it into a matrix. And if you put a matrix into it, it'll pull off the diagonal and give you back the one D array. Actually, let me show you that on maybe a smaller.", "tokens": [467, 311, 534, 10654, 498, 291, 829, 257, 472, 413, 10225, 666, 309, 11, 309, 603, 652, 309, 666, 257, 8141, 13, 400, 498, 291, 829, 257, 8141, 666, 309, 11, 309, 603, 2235, 766, 264, 21539, 293, 976, 291, 646, 264, 472, 413, 10225, 13, 5135, 11, 718, 385, 855, 291, 300, 322, 1310, 257, 4356, 13], "temperature": 0.0, "avg_logprob": -0.14441091783585086, "compression_ratio": 1.5586206896551724, "no_speech_prob": 3.3930739391507814e-06}, {"id": 216, "seek": 218400, "start": 2184.0, "end": 2197.0, "text": " So let's just use like the first five entries of S.", "tokens": [407, 718, 311, 445, 764, 411, 264, 700, 1732, 23041, 295, 318, 13], "temperature": 0.0, "avg_logprob": -0.19311809539794922, "compression_ratio": 1.3385826771653544, "no_speech_prob": 1.1478683518362232e-05}, {"id": 217, "seek": 218400, "start": 2197.0, "end": 2201.0, "text": " Is there parentheses.", "tokens": [1119, 456, 34153, 13], "temperature": 0.0, "avg_logprob": -0.19311809539794922, "compression_ratio": 1.3385826771653544, "no_speech_prob": 1.1478683518362232e-05}, {"id": 218, "seek": 218400, "start": 2201.0, "end": 2212.0, "text": " Do just the four so it fits on a line. Okay, so here we were giving it. So let's print this out.", "tokens": [1144, 445, 264, 1451, 370, 309, 9001, 322, 257, 1622, 13, 1033, 11, 370, 510, 321, 645, 2902, 309, 13, 407, 718, 311, 4482, 341, 484, 13], "temperature": 0.0, "avg_logprob": -0.19311809539794922, "compression_ratio": 1.3385826771653544, "no_speech_prob": 1.1478683518362232e-05}, {"id": 219, "seek": 221200, "start": 2212.0, "end": 2225.0, "text": " So we've got this one D array with four numbers. If we put that into NP dot diag, we get back a four by four matrix where it's added zeros.", "tokens": [407, 321, 600, 658, 341, 472, 413, 10225, 365, 1451, 3547, 13, 759, 321, 829, 300, 666, 38611, 5893, 1026, 559, 11, 321, 483, 646, 257, 1451, 538, 1451, 8141, 689, 309, 311, 3869, 35193, 13], "temperature": 0.0, "avg_logprob": -0.12827986548928655, "compression_ratio": 1.5153061224489797, "no_speech_prob": 2.5464618374826387e-05}, {"id": 220, "seek": 221200, "start": 2225.0, "end": 2238.0, "text": " And then if we apply NP dot diag to that again, it'll give us back just what was on the on the diagonal. So that's a, it can be a handy handy NumPy function.", "tokens": [400, 550, 498, 321, 3079, 38611, 5893, 1026, 559, 281, 300, 797, 11, 309, 603, 976, 505, 646, 445, 437, 390, 322, 264, 322, 264, 21539, 13, 407, 300, 311, 257, 11, 309, 393, 312, 257, 13239, 13239, 22592, 47, 88, 2445, 13], "temperature": 0.0, "avg_logprob": -0.12827986548928655, "compression_ratio": 1.5153061224489797, "no_speech_prob": 2.5464618374826387e-05}, {"id": 221, "seek": 223800, "start": 2238.0, "end": 2247.0, "text": " So we need to do that to do this multiplication U times the matrix form of S times V.", "tokens": [407, 321, 643, 281, 360, 300, 281, 360, 341, 27290, 624, 1413, 264, 8141, 1254, 295, 318, 1413, 691, 13], "temperature": 0.0, "avg_logprob": -0.12263151804606119, "compression_ratio": 1.518716577540107, "no_speech_prob": 1.3419617971521802e-05}, {"id": 222, "seek": 223800, "start": 2247.0, "end": 2260.0, "text": " And then I prefer, I think, using NumPy dot all close to check the reconstructed vectors and the vectors.", "tokens": [400, 550, 286, 4382, 11, 286, 519, 11, 1228, 22592, 47, 88, 5893, 439, 1998, 281, 1520, 264, 31499, 292, 18875, 293, 264, 18875, 13], "temperature": 0.0, "avg_logprob": -0.12263151804606119, "compression_ratio": 1.518716577540107, "no_speech_prob": 1.3419617971521802e-05}, {"id": 223, "seek": 223800, "start": 2260.0, "end": 2266.0, "text": " Alternately, you could look at the norm of the difference of the two and see that it's zero.", "tokens": [23830, 1592, 11, 291, 727, 574, 412, 264, 2026, 295, 264, 2649, 295, 264, 732, 293, 536, 300, 309, 311, 4018, 13], "temperature": 0.0, "avg_logprob": -0.12263151804606119, "compression_ratio": 1.518716577540107, "no_speech_prob": 1.3419617971521802e-05}, {"id": 224, "seek": 226600, "start": 2266.0, "end": 2272.0, "text": " Any questions about this?", "tokens": [2639, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.1778500345018175, "compression_ratio": 0.7575757575757576, "no_speech_prob": 4.469039413379505e-05}, {"id": 225, "seek": 227200, "start": 2272.0, "end": 2300.0, "text": " Okay. So next I want you to confirm that U and V are orthonormal.", "tokens": [1033, 13, 407, 958, 286, 528, 291, 281, 9064, 300, 624, 293, 691, 366, 420, 11943, 24440, 13], "temperature": 0.0, "avg_logprob": -0.14337476817044345, "compression_ratio": 0.9420289855072463, "no_speech_prob": 2.753323678916786e-05}, {"id": 226, "seek": 230000, "start": 2300.0, "end": 2309.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.18832831723349436, "compression_ratio": 1.2608695652173914, "no_speech_prob": 1.4509757420455571e-05}, {"id": 227, "seek": 230000, "start": 2309.0, "end": 2313.0, "text": " Which we just had. Why do we subtract?", "tokens": [3013, 321, 445, 632, 13, 1545, 360, 321, 16390, 30], "temperature": 0.0, "avg_logprob": -0.18832831723349436, "compression_ratio": 1.2608695652173914, "no_speech_prob": 1.4509757420455571e-05}, {"id": 228, "seek": 230000, "start": 2313.0, "end": 2317.0, "text": " So you don't need to. This is an alternate way of doing it.", "tokens": [407, 291, 500, 380, 643, 281, 13, 639, 307, 364, 18873, 636, 295, 884, 309, 13], "temperature": 0.0, "avg_logprob": -0.18832831723349436, "compression_ratio": 1.2608695652173914, "no_speech_prob": 1.4509757420455571e-05}, {"id": 229, "seek": 230000, "start": 2317.0, "end": 2328.0, "text": " Oh, so either of those. Yeah. Sorry. Let me put that maybe separately.", "tokens": [876, 11, 370, 2139, 295, 729, 13, 865, 13, 4919, 13, 961, 385, 829, 300, 1310, 14759, 13], "temperature": 0.0, "avg_logprob": -0.18832831723349436, "compression_ratio": 1.2608695652173914, "no_speech_prob": 1.4509757420455571e-05}, {"id": 230, "seek": 232800, "start": 2328.0, "end": 2332.0, "text": " Yeah. Good question.", "tokens": [865, 13, 2205, 1168, 13], "temperature": 0.0, "avg_logprob": -0.19004864479178812, "compression_ratio": 1.4069767441860466, "no_speech_prob": 1.6441679690615274e-05}, {"id": 231, "seek": 232800, "start": 2332.0, "end": 2343.0, "text": " In the alternate way, you would be checking that that difference was zero.", "tokens": [682, 264, 18873, 636, 11, 291, 576, 312, 8568, 300, 300, 2649, 390, 4018, 13], "temperature": 0.0, "avg_logprob": -0.19004864479178812, "compression_ratio": 1.4069767441860466, "no_speech_prob": 1.6441679690615274e-05}, {"id": 232, "seek": 232800, "start": 2343.0, "end": 2349.0, "text": " Just take the dot product of U with the transpose of U and same thing for VH.", "tokens": [1449, 747, 264, 5893, 1674, 295, 624, 365, 264, 25167, 295, 624, 293, 912, 551, 337, 691, 39, 13], "temperature": 0.0, "avg_logprob": -0.19004864479178812, "compression_ratio": 1.4069767441860466, "no_speech_prob": 1.6441679690615274e-05}, {"id": 233, "seek": 232800, "start": 2349.0, "end": 2352.0, "text": " That's right. And well, actually, say, and what are you looking for?", "tokens": [663, 311, 558, 13, 400, 731, 11, 767, 11, 584, 11, 293, 437, 366, 291, 1237, 337, 30], "temperature": 0.0, "avg_logprob": -0.19004864479178812, "compression_ratio": 1.4069767441860466, "no_speech_prob": 1.6441679690615274e-05}, {"id": 234, "seek": 235200, "start": 2352.0, "end": 2370.0, "text": " The matrix of ones. Yes. Yeah. So take the dot product.", "tokens": [440, 8141, 295, 2306, 13, 1079, 13, 865, 13, 407, 747, 264, 5893, 1674, 13], "temperature": 0.0, "avg_logprob": -0.17631238506686303, "compression_ratio": 1.0864197530864197, "no_speech_prob": 6.401138671208173e-05}, {"id": 235, "seek": 235200, "start": 2370.0, "end": 2377.0, "text": " It's pretty, it's pretty padded.", "tokens": [467, 311, 1238, 11, 309, 311, 1238, 6887, 9207, 13], "temperature": 0.0, "avg_logprob": -0.17631238506686303, "compression_ratio": 1.0864197530864197, "no_speech_prob": 6.401138671208173e-05}, {"id": 236, "seek": 237700, "start": 2377.0, "end": 2383.0, "text": " So, yes, the take the transpose of U, multiply it by U.", "tokens": [407, 11, 2086, 11, 264, 747, 264, 25167, 295, 624, 11, 12972, 309, 538, 624, 13], "temperature": 0.0, "avg_logprob": -0.07808016424309717, "compression_ratio": 1.4971098265895955, "no_speech_prob": 3.8829384720884264e-05}, {"id": 237, "seek": 237700, "start": 2383.0, "end": 2390.0, "text": " And here I've checked that it's all close to the identity matrix of the same shape.", "tokens": [400, 510, 286, 600, 10033, 300, 309, 311, 439, 1998, 281, 264, 6575, 8141, 295, 264, 912, 3909, 13], "temperature": 0.0, "avg_logprob": -0.07808016424309717, "compression_ratio": 1.4971098265895955, "no_speech_prob": 3.8829384720884264e-05}, {"id": 238, "seek": 237700, "start": 2390.0, "end": 2394.0, "text": " Ditto for V and V transpose.", "tokens": [413, 34924, 337, 691, 293, 691, 25167, 13], "temperature": 0.0, "avg_logprob": -0.07808016424309717, "compression_ratio": 1.4971098265895955, "no_speech_prob": 3.8829384720884264e-05}, {"id": 239, "seek": 237700, "start": 2394.0, "end": 2400.0, "text": " And the thing here is you do want to check that you're multiplying it on the correct side.", "tokens": [400, 264, 551, 510, 307, 291, 360, 528, 281, 1520, 300, 291, 434, 30955, 309, 322, 264, 3006, 1252, 13], "temperature": 0.0, "avg_logprob": -0.07808016424309717, "compression_ratio": 1.4971098265895955, "no_speech_prob": 3.8829384720884264e-05}, {"id": 240, "seek": 240000, "start": 2400.0, "end": 2408.0, "text": " So with U, it's the. Yeah, it's the columns that are orthonormal.", "tokens": [407, 365, 624, 11, 309, 311, 264, 13, 865, 11, 309, 311, 264, 13766, 300, 366, 420, 11943, 24440, 13], "temperature": 0.0, "avg_logprob": -0.12388703520868866, "compression_ratio": 1.7338129496402879, "no_speech_prob": 1.1478542546683457e-05}, {"id": 241, "seek": 240000, "start": 2408.0, "end": 2414.0, "text": " So you put the transpose first to get rows by columns.", "tokens": [407, 291, 829, 264, 25167, 700, 281, 483, 13241, 538, 13766, 13], "temperature": 0.0, "avg_logprob": -0.12388703520868866, "compression_ratio": 1.7338129496402879, "no_speech_prob": 1.1478542546683457e-05}, {"id": 242, "seek": 240000, "start": 2414.0, "end": 2421.0, "text": " V, it's the rows that are orthonormal. So you have that and then times the transpose to have the columns.", "tokens": [691, 11, 309, 311, 264, 13241, 300, 366, 420, 11943, 24440, 13, 407, 291, 362, 300, 293, 550, 1413, 264, 25167, 281, 362, 264, 13766, 13], "temperature": 0.0, "avg_logprob": -0.12388703520868866, "compression_ratio": 1.7338129496402879, "no_speech_prob": 1.1478542546683457e-05}, {"id": 243, "seek": 240000, "start": 2421.0, "end": 2425.0, "text": " Any questions?", "tokens": [2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.12388703520868866, "compression_ratio": 1.7338129496402879, "no_speech_prob": 1.1478542546683457e-05}, {"id": 244, "seek": 242500, "start": 2425.0, "end": 2434.0, "text": " OK, and I think it's always just nice to check these things because, you know, you can know the definition of SVD that this is what it does.", "tokens": [2264, 11, 293, 286, 519, 309, 311, 1009, 445, 1481, 281, 1520, 613, 721, 570, 11, 291, 458, 11, 291, 393, 458, 264, 7123, 295, 31910, 35, 300, 341, 307, 437, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.06811488799329074, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.670101482886821e-05}, {"id": 245, "seek": 242500, "start": 2434.0, "end": 2441.0, "text": " But I feel like this kind of can make it a little more a little more tangible of like, OK, it really is giving me a matrix that's orthonormal.", "tokens": [583, 286, 841, 411, 341, 733, 295, 393, 652, 309, 257, 707, 544, 257, 707, 544, 27094, 295, 411, 11, 2264, 11, 309, 534, 307, 2902, 385, 257, 8141, 300, 311, 420, 11943, 24440, 13], "temperature": 0.0, "avg_logprob": -0.06811488799329074, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.670101482886821e-05}, {"id": 246, "seek": 242500, "start": 2441.0, "end": 2447.0, "text": " And I really am getting back my original from it.", "tokens": [400, 286, 534, 669, 1242, 646, 452, 3380, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.06811488799329074, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.670101482886821e-05}, {"id": 247, "seek": 242500, "start": 2447.0, "end": 2452.0, "text": " And so let's let's see what can we say about the singular values.", "tokens": [400, 370, 718, 311, 718, 311, 536, 437, 393, 321, 584, 466, 264, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.06811488799329074, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.670101482886821e-05}, {"id": 248, "seek": 245200, "start": 2452.0, "end": 2456.0, "text": " So here I've plotted the singular values. And again, remember the singular values.", "tokens": [407, 510, 286, 600, 43288, 264, 20010, 4190, 13, 400, 797, 11, 1604, 264, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.10666390827723912, "compression_ratio": 1.5164835164835164, "no_speech_prob": 1.2606671589310281e-05}, {"id": 249, "seek": 245200, "start": 2456.0, "end": 2463.0, "text": " This is this one D array that you can make into a matrix by adding zeros.", "tokens": [639, 307, 341, 472, 413, 10225, 300, 291, 393, 652, 666, 257, 8141, 538, 5127, 35193, 13], "temperature": 0.0, "avg_logprob": -0.10666390827723912, "compression_ratio": 1.5164835164835164, "no_speech_prob": 1.2606671589310281e-05}, {"id": 250, "seek": 245200, "start": 2463.0, "end": 2468.0, "text": " And I see that they are non-negative and they're decreasing.", "tokens": [400, 286, 536, 300, 436, 366, 2107, 12, 28561, 1166, 293, 436, 434, 23223, 13], "temperature": 0.0, "avg_logprob": -0.10666390827723912, "compression_ratio": 1.5164835164835164, "no_speech_prob": 1.2606671589310281e-05}, {"id": 251, "seek": 245200, "start": 2468.0, "end": 2473.0, "text": " They're actually decreasing pretty quickly, it looks like.", "tokens": [814, 434, 767, 23223, 1238, 2661, 11, 309, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.10666390827723912, "compression_ratio": 1.5164835164835164, "no_speech_prob": 1.2606671589310281e-05}, {"id": 252, "seek": 247300, "start": 2473.0, "end": 2485.0, "text": " And the kind of meaning behind the singular values is telling you the importance of the topics.", "tokens": [400, 264, 733, 295, 3620, 2261, 264, 20010, 4190, 307, 3585, 291, 264, 7379, 295, 264, 8378, 13], "temperature": 0.0, "avg_logprob": -0.08182779191032288, "compression_ratio": 1.5714285714285714, "no_speech_prob": 9.665633115218952e-06}, {"id": 253, "seek": 247300, "start": 2485.0, "end": 2492.0, "text": " I should also note the reason that our other dimension is topics is because we're doing topic modeling.", "tokens": [286, 820, 611, 3637, 264, 1778, 300, 527, 661, 10139, 307, 8378, 307, 570, 321, 434, 884, 4829, 15983, 13], "temperature": 0.0, "avg_logprob": -0.08182779191032288, "compression_ratio": 1.5714285714285714, "no_speech_prob": 9.665633115218952e-06}, {"id": 254, "seek": 247300, "start": 2492.0, "end": 2500.0, "text": " You could be doing SVD on a different application and assign that a different meaning.", "tokens": [509, 727, 312, 884, 31910, 35, 322, 257, 819, 3861, 293, 6269, 300, 257, 819, 3620, 13], "temperature": 0.0, "avg_logprob": -0.08182779191032288, "compression_ratio": 1.5714285714285714, "no_speech_prob": 9.665633115218952e-06}, {"id": 255, "seek": 250000, "start": 2500.0, "end": 2508.0, "text": " So this is the one setting the meaning of of that new dimension that we're adding and saying that it's topics.", "tokens": [407, 341, 307, 264, 472, 3287, 264, 3620, 295, 295, 300, 777, 10139, 300, 321, 434, 5127, 293, 1566, 300, 309, 311, 8378, 13], "temperature": 0.0, "avg_logprob": -0.21719906969768246, "compression_ratio": 1.3963963963963963, "no_speech_prob": 3.0240724299801514e-05}, {"id": 256, "seek": 250000, "start": 2508.0, "end": 2511.0, "text": " So here I wanted to see what are the topics.", "tokens": [407, 510, 286, 1415, 281, 536, 437, 366, 264, 8378, 13], "temperature": 0.0, "avg_logprob": -0.21719906969768246, "compression_ratio": 1.3963963963963963, "no_speech_prob": 3.0240724299801514e-05}, {"id": 257, "seek": 251100, "start": 2511.0, "end": 2531.0, "text": " So I wrote a method that that goes through the vocab and is choosing the largest the largest values to be the top words for each topic.", "tokens": [407, 286, 4114, 257, 3170, 300, 300, 1709, 807, 264, 2329, 455, 293, 307, 10875, 264, 6443, 264, 6443, 4190, 281, 312, 264, 1192, 2283, 337, 1184, 4829, 13], "temperature": 0.0, "avg_logprob": -0.1637729008992513, "compression_ratio": 1.5, "no_speech_prob": 6.747924544470152e-06}, {"id": 258, "seek": 251100, "start": 2531.0, "end": 2537.0, "text": " Because really each topic actually go back to the expel spreadsheet.", "tokens": [1436, 534, 1184, 4829, 767, 352, 646, 281, 264, 1278, 338, 27733, 13], "temperature": 0.0, "avg_logprob": -0.1637729008992513, "compression_ratio": 1.5, "no_speech_prob": 6.747924544470152e-06}, {"id": 259, "seek": 253700, "start": 2537.0, "end": 2544.0, "text": " I think you can see it better here like each topic involves a little bit of every single word in the vocabulary.", "tokens": [286, 519, 291, 393, 536, 309, 1101, 510, 411, 1184, 4829, 11626, 257, 707, 857, 295, 633, 2167, 1349, 294, 264, 19864, 13], "temperature": 0.0, "avg_logprob": -0.11798133633353493, "compression_ratio": 1.6036036036036037, "no_speech_prob": 6.604789814446121e-05}, {"id": 260, "seek": 253700, "start": 2544.0, "end": 2559.0, "text": " Right. And so to get something meaningful out of this, I said, you know, I'll check, choose out what are the few words with the highest values here and assign those to be the words for the topic.", "tokens": [1779, 13, 400, 370, 281, 483, 746, 10995, 484, 295, 341, 11, 286, 848, 11, 291, 458, 11, 286, 603, 1520, 11, 2826, 484, 437, 366, 264, 1326, 2283, 365, 264, 6343, 4190, 510, 293, 6269, 729, 281, 312, 264, 2283, 337, 264, 4829, 13], "temperature": 0.0, "avg_logprob": -0.11798133633353493, "compression_ratio": 1.6036036036036037, "no_speech_prob": 6.604789814446121e-05}, {"id": 261, "seek": 253700, "start": 2559.0, "end": 2563.0, "text": " And so that's that's what this method is doing.", "tokens": [400, 370, 300, 311, 300, 311, 437, 341, 3170, 307, 884, 13], "temperature": 0.0, "avg_logprob": -0.11798133633353493, "compression_ratio": 1.6036036036036037, "no_speech_prob": 6.604789814446121e-05}, {"id": 262, "seek": 256300, "start": 2563.0, "end": 2568.0, "text": " And calling those the topic words and then I'm I'm looking at them.", "tokens": [400, 5141, 729, 264, 4829, 2283, 293, 550, 286, 478, 286, 478, 1237, 412, 552, 13], "temperature": 0.0, "avg_logprob": -0.20174625738343197, "compression_ratio": 1.521472392638037, "no_speech_prob": 4.133206311962567e-05}, {"id": 263, "seek": 256300, "start": 2568.0, "end": 2573.0, "text": " The first the first one is pretty weird.", "tokens": [440, 700, 264, 700, 472, 307, 1238, 3657, 13], "temperature": 0.0, "avg_logprob": -0.20174625738343197, "compression_ratio": 1.521472392638037, "no_speech_prob": 4.133206311962567e-05}, {"id": 264, "seek": 256300, "start": 2573.0, "end": 2581.0, "text": " Citrus ditto propagandists or name galactic centric kindergarten surreal imaginative.", "tokens": [18435, 13783, 274, 34924, 12425, 474, 1751, 420, 1315, 7660, 19892, 1489, 1341, 26671, 32513, 23427, 1166, 13], "temperature": 0.0, "avg_logprob": -0.20174625738343197, "compression_ratio": 1.521472392638037, "no_speech_prob": 4.133206311962567e-05}, {"id": 265, "seek": 256300, "start": 2581.0, "end": 2586.0, "text": " And that doesn't that doesn't seem to map to a topic.", "tokens": [400, 300, 1177, 380, 300, 1177, 380, 1643, 281, 4471, 281, 257, 4829, 13], "temperature": 0.0, "avg_logprob": -0.20174625738343197, "compression_ratio": 1.521472392638037, "no_speech_prob": 4.133206311962567e-05}, {"id": 266, "seek": 258600, "start": 2586.0, "end": 2595.0, "text": " But the next one, JPEG gif file color quality image format. What topic do you think that is?", "tokens": [583, 264, 958, 472, 11, 508, 5208, 38, 290, 351, 3991, 2017, 3125, 3256, 7877, 13, 708, 4829, 360, 291, 519, 300, 307, 30], "temperature": 0.0, "avg_logprob": -0.15442251390026462, "compression_ratio": 1.3071895424836601, "no_speech_prob": 1.3845171451976057e-05}, {"id": 267, "seek": 258600, "start": 2595.0, "end": 2597.0, "text": " Graphics. Yeah.", "tokens": [21884, 1167, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.15442251390026462, "compression_ratio": 1.3071895424836601, "no_speech_prob": 1.3845171451976057e-05}, {"id": 268, "seek": 258600, "start": 2597.0, "end": 2603.0, "text": " Then we've got what looks like another graphics one.", "tokens": [1396, 321, 600, 658, 437, 1542, 411, 1071, 11837, 472, 13], "temperature": 0.0, "avg_logprob": -0.15442251390026462, "compression_ratio": 1.3071895424836601, "no_speech_prob": 1.3845171451976057e-05}, {"id": 269, "seek": 258600, "start": 2603.0, "end": 2605.0, "text": " What's that?", "tokens": [708, 311, 300, 30], "temperature": 0.0, "avg_logprob": -0.15442251390026462, "compression_ratio": 1.3071895424836601, "no_speech_prob": 1.3845171451976057e-05}, {"id": 270, "seek": 258600, "start": 2605.0, "end": 2608.0, "text": " Because it's got graphic.", "tokens": [1436, 309, 311, 658, 14089, 13], "temperature": 0.0, "avg_logprob": -0.15442251390026462, "compression_ratio": 1.3071895424836601, "no_speech_prob": 1.3845171451976057e-05}, {"id": 271, "seek": 260800, "start": 2608.0, "end": 2618.0, "text": " Edu pub and mail are less meaningful, but the 3D ray FTP. Then we've got Jesus, God, Matthew, people, atheist, atheism.", "tokens": [31900, 1535, 293, 10071, 366, 1570, 10995, 11, 457, 264, 805, 35, 18592, 479, 16804, 13, 1396, 321, 600, 658, 2705, 11, 1265, 11, 12434, 11, 561, 11, 43977, 11, 27033, 1434, 13], "temperature": 0.0, "avg_logprob": -0.1816582197553656, "compression_ratio": 1.6108374384236452, "no_speech_prob": 3.218867277610116e-05}, {"id": 272, "seek": 260800, "start": 2618.0, "end": 2626.0, "text": " There is there is graphics in that last one. But overall, this seems like probably a religion one or an atheism one.", "tokens": [821, 307, 456, 307, 11837, 294, 300, 1036, 472, 13, 583, 4787, 11, 341, 2544, 411, 1391, 257, 7561, 472, 420, 364, 27033, 1434, 472, 13], "temperature": 0.0, "avg_logprob": -0.1816582197553656, "compression_ratio": 1.6108374384236452, "no_speech_prob": 3.218867277610116e-05}, {"id": 273, "seek": 260800, "start": 2626.0, "end": 2629.0, "text": " Another another graphics one.", "tokens": [3996, 1071, 11837, 472, 13], "temperature": 0.0, "avg_logprob": -0.1816582197553656, "compression_ratio": 1.6108374384236452, "no_speech_prob": 3.218867277610116e-05}, {"id": 274, "seek": 260800, "start": 2629.0, "end": 2634.0, "text": " Another atheism or religion one. Here we've got a space one.", "tokens": [3996, 27033, 1434, 420, 7561, 472, 13, 1692, 321, 600, 658, 257, 1901, 472, 13], "temperature": 0.0, "avg_logprob": -0.1816582197553656, "compression_ratio": 1.6108374384236452, "no_speech_prob": 3.218867277610116e-05}, {"id": 275, "seek": 263400, "start": 2634.0, "end": 2639.0, "text": " Space NASA lunar Mars missions probe.", "tokens": [8705, 12077, 32581, 9692, 13744, 22715, 13], "temperature": 0.0, "avg_logprob": -0.0995333989461263, "compression_ratio": 1.5053763440860215, "no_speech_prob": 1.5445673852809705e-05}, {"id": 276, "seek": 263400, "start": 2639.0, "end": 2645.0, "text": " Actually, that's two space ones in a row. And so note that we're not.", "tokens": [5135, 11, 300, 311, 732, 1901, 2306, 294, 257, 5386, 13, 400, 370, 3637, 300, 321, 434, 406, 13], "temperature": 0.0, "avg_logprob": -0.0995333989461263, "compression_ratio": 1.5053763440860215, "no_speech_prob": 1.5445673852809705e-05}, {"id": 277, "seek": 263400, "start": 2645.0, "end": 2655.0, "text": " You know, we're not getting kind of definitive topics that exactly match what we what we thought the topics were of our source data.", "tokens": [509, 458, 11, 321, 434, 406, 1242, 733, 295, 28152, 8378, 300, 2293, 2995, 437, 321, 437, 321, 1194, 264, 8378, 645, 295, 527, 4009, 1412, 13], "temperature": 0.0, "avg_logprob": -0.0995333989461263, "compression_ratio": 1.5053763440860215, "no_speech_prob": 1.5445673852809705e-05}, {"id": 278, "seek": 263400, "start": 2655.0, "end": 2660.0, "text": " But this is these are sensical outputs.", "tokens": [583, 341, 307, 613, 366, 2923, 804, 23930, 13], "temperature": 0.0, "avg_logprob": -0.0995333989461263, "compression_ratio": 1.5053763440860215, "no_speech_prob": 1.5445673852809705e-05}, {"id": 279, "seek": 266000, "start": 2660.0, "end": 2666.0, "text": " There are questions about this.", "tokens": [821, 366, 1651, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.10681467056274414, "compression_ratio": 1.6026200873362446, "no_speech_prob": 8.139361852954607e-06}, {"id": 280, "seek": 266000, "start": 2666.0, "end": 2675.0, "text": " So this also highlights that even though we kind of, you know, we were constructing, I would say the somewhat artificial data set that we were hoping would have, you know, for clusters.", "tokens": [407, 341, 611, 14254, 300, 754, 1673, 321, 733, 295, 11, 291, 458, 11, 321, 645, 39969, 11, 286, 576, 584, 264, 8344, 11677, 1412, 992, 300, 321, 645, 7159, 576, 362, 11, 291, 458, 11, 337, 23313, 13], "temperature": 0.0, "avg_logprob": -0.10681467056274414, "compression_ratio": 1.6026200873362446, "no_speech_prob": 8.139361852954607e-06}, {"id": 281, "seek": 266000, "start": 2675.0, "end": 2678.0, "text": " We wanted to look at more than four.", "tokens": [492, 1415, 281, 574, 412, 544, 813, 1451, 13], "temperature": 0.0, "avg_logprob": -0.10681467056274414, "compression_ratio": 1.6026200873362446, "no_speech_prob": 8.139361852954607e-06}, {"id": 282, "seek": 266000, "start": 2678.0, "end": 2688.0, "text": " We didn't just look at like the first four topics that we got because you can have some duplicates. And so here.", "tokens": [492, 994, 380, 445, 574, 412, 411, 264, 700, 1451, 8378, 300, 321, 658, 570, 291, 393, 362, 512, 17154, 1024, 13, 400, 370, 510, 13], "temperature": 0.0, "avg_logprob": -0.10681467056274414, "compression_ratio": 1.6026200873362446, "no_speech_prob": 8.139361852954607e-06}, {"id": 283, "seek": 268800, "start": 2688.0, "end": 2696.0, "text": " Well, really, I guess we got a few graphics ones before, you know, we didn't get space until what is that topic?", "tokens": [1042, 11, 534, 11, 286, 2041, 321, 658, 257, 1326, 11837, 2306, 949, 11, 291, 458, 11, 321, 994, 380, 483, 1901, 1826, 437, 307, 300, 4829, 30], "temperature": 0.0, "avg_logprob": -0.10590606182813644, "compression_ratio": 1.3735632183908046, "no_speech_prob": 1.9524677554727532e-05}, {"id": 284, "seek": 268800, "start": 2696.0, "end": 2700.0, "text": " The seventh and eighth most important topics.", "tokens": [440, 17875, 293, 19495, 881, 1021, 8378, 13], "temperature": 0.0, "avg_logprob": -0.10590606182813644, "compression_ratio": 1.3735632183908046, "no_speech_prob": 1.9524677554727532e-05}, {"id": 285, "seek": 268800, "start": 2700.0, "end": 2703.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.10590606182813644, "compression_ratio": 1.3735632183908046, "no_speech_prob": 1.9524677554727532e-05}, {"id": 286, "seek": 268800, "start": 2703.0, "end": 2710.0, "text": " If you decrease the number of clusters to four, would they be more precise?", "tokens": [759, 291, 11514, 264, 1230, 295, 23313, 281, 1451, 11, 576, 436, 312, 544, 13600, 30], "temperature": 0.0, "avg_logprob": -0.10590606182813644, "compression_ratio": 1.3735632183908046, "no_speech_prob": 1.9524677554727532e-05}, {"id": 287, "seek": 271000, "start": 2710.0, "end": 2722.0, "text": " Well, so this is getting ahead of myself a bit. So here we've actually gotten two thousand topics and I was just looking at the top, the top ten.", "tokens": [1042, 11, 370, 341, 307, 1242, 2286, 295, 2059, 257, 857, 13, 407, 510, 321, 600, 767, 5768, 732, 4714, 8378, 293, 286, 390, 445, 1237, 412, 264, 1192, 11, 264, 1192, 2064, 13], "temperature": 0.0, "avg_logprob": -0.1753601412619314, "compression_ratio": 1.3766233766233766, "no_speech_prob": 4.3996078602503985e-05}, {"id": 288, "seek": 271000, "start": 2722.0, "end": 2729.0, "text": " And we'll we'll get to this later. How you can do a truncated SVD.", "tokens": [400, 321, 603, 321, 603, 483, 281, 341, 1780, 13, 1012, 291, 393, 360, 257, 504, 409, 66, 770, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.1753601412619314, "compression_ratio": 1.3766233766233766, "no_speech_prob": 4.3996078602503985e-05}, {"id": 289, "seek": 272900, "start": 2729.0, "end": 2745.0, "text": " For SVD, you're basically kind of always imagining their two thousand topics, but your question will be very relevant when we talk about NMF.", "tokens": [1171, 31910, 35, 11, 291, 434, 1936, 733, 295, 1009, 27798, 641, 732, 4714, 8378, 11, 457, 428, 1168, 486, 312, 588, 7340, 562, 321, 751, 466, 426, 44, 37, 13], "temperature": 0.0, "avg_logprob": -0.14387178421020508, "compression_ratio": 1.3419354838709678, "no_speech_prob": 1.2804658581444528e-05}, {"id": 290, "seek": 272900, "start": 2745.0, "end": 2750.0, "text": " Yeah, so that's actually a good tie in to start talking about NMF.", "tokens": [865, 11, 370, 300, 311, 767, 257, 665, 7582, 294, 281, 722, 1417, 466, 426, 44, 37, 13], "temperature": 0.0, "avg_logprob": -0.14387178421020508, "compression_ratio": 1.3419354838709678, "no_speech_prob": 1.2804658581444528e-05}, {"id": 291, "seek": 275000, "start": 2750.0, "end": 2761.0, "text": " And so NMF is a different matrix factorization. So in practice, if you're doing a topic modeling problem, you would either use SVD or you would use NMF.", "tokens": [400, 370, 426, 44, 37, 307, 257, 819, 8141, 5952, 2144, 13, 407, 294, 3124, 11, 498, 291, 434, 884, 257, 4829, 15983, 1154, 11, 291, 576, 2139, 764, 31910, 35, 420, 291, 576, 764, 426, 44, 37, 13], "temperature": 0.0, "avg_logprob": -0.054329242025102885, "compression_ratio": 1.4193548387096775, "no_speech_prob": 3.7851457364013186e-06}, {"id": 292, "seek": 275000, "start": 2761.0, "end": 2767.0, "text": " I'm just showing both to kind of illustrate some of their properties and differences.", "tokens": [286, 478, 445, 4099, 1293, 281, 733, 295, 23221, 512, 295, 641, 7221, 293, 7300, 13], "temperature": 0.0, "avg_logprob": -0.054329242025102885, "compression_ratio": 1.4193548387096775, "no_speech_prob": 3.7851457364013186e-06}, {"id": 293, "seek": 275000, "start": 2767.0, "end": 2773.0, "text": " The motivation for NMF is", "tokens": [440, 12335, 337, 426, 44, 37, 307], "temperature": 0.0, "avg_logprob": -0.054329242025102885, "compression_ratio": 1.4193548387096775, "no_speech_prob": 3.7851457364013186e-06}, {"id": 294, "seek": 277300, "start": 2773.0, "end": 2782.0, "text": " kind of wondering, OK, what do negative values mean in a lot of context and even", "tokens": [733, 295, 6359, 11, 2264, 11, 437, 360, 3671, 4190, 914, 294, 257, 688, 295, 4319, 293, 754], "temperature": 0.0, "avg_logprob": -0.12892711345966046, "compression_ratio": 1.5416666666666667, "no_speech_prob": 8.530116247129627e-06}, {"id": 295, "seek": 277300, "start": 2782.0, "end": 2789.0, "text": " you know, even here when we look at our words, what does it mean that you find a good negative", "tokens": [291, 458, 11, 754, 510, 562, 321, 574, 412, 527, 2283, 11, 437, 775, 309, 914, 300, 291, 915, 257, 665, 3671], "temperature": 0.0, "avg_logprob": -0.12892711345966046, "compression_ratio": 1.5416666666666667, "no_speech_prob": 8.530116247129627e-06}, {"id": 296, "seek": 277300, "start": 2789.0, "end": 2796.0, "text": " like topic seven refers to negative point two nine atoms. How do we interpret that?", "tokens": [411, 4829, 3407, 14942, 281, 3671, 935, 732, 4949, 16871, 13, 1012, 360, 321, 7302, 300, 30], "temperature": 0.0, "avg_logprob": -0.12892711345966046, "compression_ratio": 1.5416666666666667, "no_speech_prob": 8.530116247129627e-06}, {"id": 297, "seek": 279600, "start": 2796.0, "end": 2811.0, "text": " And so a lot of people like NMF for its interpret ability of not having not having negatives, as the as the name suggests.", "tokens": [400, 370, 257, 688, 295, 561, 411, 426, 44, 37, 337, 1080, 7302, 3485, 295, 406, 1419, 406, 1419, 40019, 11, 382, 264, 382, 264, 1315, 13409, 13], "temperature": 0.0, "avg_logprob": -0.0938797272168673, "compression_ratio": 1.4533333333333334, "no_speech_prob": 1.260590670426609e-05}, {"id": 298, "seek": 279600, "start": 2811.0, "end": 2819.0, "text": " And so here this is looking at an image problem of decomposing faces into different components.", "tokens": [400, 370, 510, 341, 307, 1237, 412, 364, 3256, 1154, 295, 22867, 6110, 8475, 666, 819, 6677, 13], "temperature": 0.0, "avg_logprob": -0.0938797272168673, "compression_ratio": 1.4533333333333334, "no_speech_prob": 1.260590670426609e-05}, {"id": 299, "seek": 281900, "start": 2819.0, "end": 2826.0, "text": " And this is also something that earlier I was saying, we're calling these components topics because we're doing topic modeling.", "tokens": [400, 341, 307, 611, 746, 300, 3071, 286, 390, 1566, 11, 321, 434, 5141, 613, 6677, 8378, 570, 321, 434, 884, 4829, 15983, 13], "temperature": 0.0, "avg_logprob": -0.07133438217807823, "compression_ratio": 1.711111111111111, "no_speech_prob": 1.670016899879556e-05}, {"id": 300, "seek": 281900, "start": 2826.0, "end": 2838.0, "text": " If we were using SVD on face images, we would call our different components that we got, I guess, like different components of the face or different pieces of the face.", "tokens": [759, 321, 645, 1228, 31910, 35, 322, 1851, 5267, 11, 321, 576, 818, 527, 819, 6677, 300, 321, 658, 11, 286, 2041, 11, 411, 819, 6677, 295, 264, 1851, 420, 819, 3755, 295, 264, 1851, 13], "temperature": 0.0, "avg_logprob": -0.07133438217807823, "compression_ratio": 1.711111111111111, "no_speech_prob": 1.670016899879556e-05}, {"id": 301, "seek": 281900, "start": 2838.0, "end": 2840.0, "text": " And so this", "tokens": [400, 370, 341], "temperature": 0.0, "avg_logprob": -0.07133438217807823, "compression_ratio": 1.711111111111111, "no_speech_prob": 1.670016899879556e-05}, {"id": 302, "seek": 284000, "start": 2840.0, "end": 2851.0, "text": " would be something kind of more akin to what you could get from SVD, whereas with NMF, because it's non-negative, it's like, oh, I can see how these are pieces of faces.", "tokens": [576, 312, 746, 733, 295, 544, 47540, 281, 437, 291, 727, 483, 490, 31910, 35, 11, 9735, 365, 426, 44, 37, 11, 570, 309, 311, 2107, 12, 28561, 1166, 11, 309, 311, 411, 11, 1954, 11, 286, 393, 536, 577, 613, 366, 3755, 295, 8475, 13], "temperature": 0.0, "avg_logprob": -0.08647404715072277, "compression_ratio": 1.5148514851485149, "no_speech_prob": 7.766652743157465e-06}, {"id": 303, "seek": 284000, "start": 2851.0, "end": 2856.0, "text": " So here, you know, here's kind of like underneath someone's eyes.", "tokens": [407, 510, 11, 291, 458, 11, 510, 311, 733, 295, 411, 7223, 1580, 311, 2575, 13], "temperature": 0.0, "avg_logprob": -0.08647404715072277, "compression_ratio": 1.5148514851485149, "no_speech_prob": 7.766652743157465e-06}, {"id": 304, "seek": 284000, "start": 2856.0, "end": 2863.0, "text": " This is the right side of someone's nose and kind of around the mouth.", "tokens": [639, 307, 264, 558, 1252, 295, 1580, 311, 6690, 293, 733, 295, 926, 264, 4525, 13], "temperature": 0.0, "avg_logprob": -0.08647404715072277, "compression_ratio": 1.5148514851485149, "no_speech_prob": 7.766652743157465e-06}, {"id": 305, "seek": 286300, "start": 2863.0, "end": 2870.0, "text": " So NMF by giving you non-negatives.", "tokens": [407, 426, 44, 37, 538, 2902, 291, 2107, 12, 28561, 4884, 13], "temperature": 0.0, "avg_logprob": -0.0809481183036429, "compression_ratio": 1.36986301369863, "no_speech_prob": 7.646282938367222e-06}, {"id": 306, "seek": 286300, "start": 2870.0, "end": 2880.0, "text": " So what we liked about SVD decomposition is that the resulting matrices are orthogonal.", "tokens": [407, 437, 321, 4501, 466, 31910, 35, 48356, 307, 300, 264, 16505, 32284, 366, 41488, 13], "temperature": 0.0, "avg_logprob": -0.0809481183036429, "compression_ratio": 1.36986301369863, "no_speech_prob": 7.646282938367222e-06}, {"id": 307, "seek": 286300, "start": 2880.0, "end": 2885.0, "text": " NMF, we're saying, let's make them, let's constrain them to be non-negative.", "tokens": [426, 44, 37, 11, 321, 434, 1566, 11, 718, 311, 652, 552, 11, 718, 311, 1817, 7146, 552, 281, 312, 2107, 12, 28561, 1166, 13], "temperature": 0.0, "avg_logprob": -0.0809481183036429, "compression_ratio": 1.36986301369863, "no_speech_prob": 7.646282938367222e-06}, {"id": 308, "seek": 288500, "start": 2885.0, "end": 2896.0, "text": " And so our data set V is going to be a product of just two matrices here, W and H, both that have non-negative entries.", "tokens": [400, 370, 527, 1412, 992, 691, 307, 516, 281, 312, 257, 1674, 295, 445, 732, 32284, 510, 11, 343, 293, 389, 11, 1293, 300, 362, 2107, 12, 28561, 1166, 23041, 13], "temperature": 0.0, "avg_logprob": -0.05470969336373466, "compression_ratio": 1.1782178217821782, "no_speech_prob": 4.565839390124893e-06}, {"id": 309, "seek": 289600, "start": 2896.0, "end": 2915.0, "text": " And so here they're showing a factorization of V where these are actual faces. And so each face has been kind of unwound, you know, to be a single array of pixel information.", "tokens": [400, 370, 510, 436, 434, 4099, 257, 5952, 2144, 295, 691, 689, 613, 366, 3539, 8475, 13, 400, 370, 1184, 1851, 575, 668, 733, 295, 14853, 554, 11, 291, 458, 11, 281, 312, 257, 2167, 10225, 295, 19261, 1589, 13], "temperature": 0.0, "avg_logprob": -0.08045140653848648, "compression_ratio": 1.5722543352601157, "no_speech_prob": 5.33788806933444e-06}, {"id": 310, "seek": 289600, "start": 2915.0, "end": 2922.0, "text": " And they're representing that as facial features by the importance of the features in each image.", "tokens": [400, 436, 434, 13460, 300, 382, 15642, 4122, 538, 264, 7379, 295, 264, 4122, 294, 1184, 3256, 13], "temperature": 0.0, "avg_logprob": -0.08045140653848648, "compression_ratio": 1.5722543352601157, "no_speech_prob": 5.33788806933444e-06}, {"id": 311, "seek": 292200, "start": 2922.0, "end": 2927.0, "text": " And you can think of these. So matrix, matrix multiplication.", "tokens": [400, 291, 393, 519, 295, 613, 13, 407, 8141, 11, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.09196959092066838, "compression_ratio": 1.6028368794326242, "no_speech_prob": 1.6964082533377223e-05}, {"id": 312, "seek": 292200, "start": 2927.0, "end": 2939.0, "text": " Actually, this might be a good time to say a little bit about matrix multiplication.", "tokens": [5135, 11, 341, 1062, 312, 257, 665, 565, 281, 584, 257, 707, 857, 466, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.09196959092066838, "compression_ratio": 1.6028368794326242, "no_speech_prob": 1.6964082533377223e-05}, {"id": 313, "seek": 292200, "start": 2939.0, "end": 2947.0, "text": " Matrix multiplication, one way to think about it is taking linear combinations.", "tokens": [36274, 27290, 11, 472, 636, 281, 519, 466, 309, 307, 1940, 8213, 21267, 13], "temperature": 0.0, "avg_logprob": -0.09196959092066838, "compression_ratio": 1.6028368794326242, "no_speech_prob": 1.6964082533377223e-05}, {"id": 314, "seek": 294700, "start": 2947.0, "end": 2955.0, "text": " So typically, you know, a lot of people, when you talk about matrix multiplication, you might think of a song, row by column, row by column.", "tokens": [407, 5850, 11, 291, 458, 11, 257, 688, 295, 561, 11, 562, 291, 751, 466, 8141, 27290, 11, 291, 1062, 519, 295, 257, 2153, 11, 5386, 538, 7738, 11, 5386, 538, 7738, 13], "temperature": 0.0, "avg_logprob": -0.07872153400035387, "compression_ratio": 1.7156398104265402, "no_speech_prob": 1.7230828234460205e-05}, {"id": 315, "seek": 294700, "start": 2955.0, "end": 2964.0, "text": " I meant to look up the lyrics before this. I think it's multiply them, add them up one by one.", "tokens": [286, 4140, 281, 574, 493, 264, 12189, 949, 341, 13, 286, 519, 309, 311, 12972, 552, 11, 909, 552, 493, 472, 538, 472, 13], "temperature": 0.0, "avg_logprob": -0.07872153400035387, "compression_ratio": 1.7156398104265402, "no_speech_prob": 1.7230828234460205e-05}, {"id": 316, "seek": 294700, "start": 2964.0, "end": 2974.0, "text": " But there's an alternate way to think about matrix multiplication, which is thinking about the columns of what you have first.", "tokens": [583, 456, 311, 364, 18873, 636, 281, 519, 466, 8141, 27290, 11, 597, 307, 1953, 466, 264, 13766, 295, 437, 291, 362, 700, 13], "temperature": 0.0, "avg_logprob": -0.07872153400035387, "compression_ratio": 1.7156398104265402, "no_speech_prob": 1.7230828234460205e-05}, {"id": 317, "seek": 297400, "start": 2974.0, "end": 2986.0, "text": " And then actually, I should have these names.", "tokens": [400, 550, 767, 11, 286, 820, 362, 613, 5288, 13], "temperature": 0.0, "avg_logprob": -0.3162461689540318, "compression_ratio": 0.8823529411764706, "no_speech_prob": 3.120148539892398e-05}, {"id": 318, "seek": 298600, "start": 2986.0, "end": 3006.0, "text": " So say you have a column called A, B, C, and D. If you are multiplying that just by", "tokens": [407, 584, 291, 362, 257, 7738, 1219, 316, 11, 363, 11, 383, 11, 293, 413, 13, 759, 291, 366, 30955, 300, 445, 538], "temperature": 0.0, "avg_logprob": -0.14053865381189296, "compression_ratio": 1.0326086956521738, "no_speech_prob": 1.5689338397351094e-05}, {"id": 319, "seek": 298600, "start": 3006.0, "end": 3011.0, "text": " W, X, Y, C.", "tokens": [343, 11, 1783, 11, 398, 11, 383, 13], "temperature": 0.0, "avg_logprob": -0.14053865381189296, "compression_ratio": 1.0326086956521738, "no_speech_prob": 1.5689338397351094e-05}, {"id": 320, "seek": 301100, "start": 3011.0, "end": 3020.0, "text": " Really, that's like taking this linear combination of W times column A", "tokens": [4083, 11, 300, 311, 411, 1940, 341, 8213, 6562, 295, 343, 1413, 7738, 316], "temperature": 0.0, "avg_logprob": -0.10734516236840225, "compression_ratio": 1.3240740740740742, "no_speech_prob": 1.952564525709022e-05}, {"id": 321, "seek": 301100, "start": 3020.0, "end": 3028.0, "text": " plus X times column B", "tokens": [1804, 1783, 1413, 7738, 363], "temperature": 0.0, "avg_logprob": -0.10734516236840225, "compression_ratio": 1.3240740740740742, "no_speech_prob": 1.952564525709022e-05}, {"id": 322, "seek": 301100, "start": 3028.0, "end": 3035.0, "text": " plus Y times column C. Then I've run out of space.", "tokens": [1804, 398, 1413, 7738, 383, 13, 1396, 286, 600, 1190, 484, 295, 1901, 13], "temperature": 0.0, "avg_logprob": -0.10734516236840225, "compression_ratio": 1.3240740740740742, "no_speech_prob": 1.952564525709022e-05}, {"id": 323, "seek": 303500, "start": 3035.0, "end": 3044.0, "text": " Plus Z times column D. And so the result of this is going to give you a column.", "tokens": [7721, 1176, 1413, 7738, 413, 13, 400, 370, 264, 1874, 295, 341, 307, 516, 281, 976, 291, 257, 7738, 13], "temperature": 0.0, "avg_logprob": -0.17275799184605695, "compression_ratio": 1.5722543352601157, "no_speech_prob": 4.133288166485727e-05}, {"id": 324, "seek": 303500, "start": 3044.0, "end": 3056.0, "text": " And so this, a lot of, I think a lot of real world applications, this is actually kind of the meaning of what's going on with matrix multiplication, even though it's often not taught this way.", "tokens": [400, 370, 341, 11, 257, 688, 295, 11, 286, 519, 257, 688, 295, 957, 1002, 5821, 11, 341, 307, 767, 733, 295, 264, 3620, 295, 437, 311, 516, 322, 365, 8141, 27290, 11, 754, 1673, 309, 311, 2049, 406, 5928, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.17275799184605695, "compression_ratio": 1.5722543352601157, "no_speech_prob": 4.133288166485727e-05}, {"id": 325, "seek": 305600, "start": 3056.0, "end": 3067.0, "text": " But if you're doing, so this was a matrix times a vector. If you were doing a matrix times a matrix, what you're doing is taking a bunch of different linear combinations of the first matrix,", "tokens": [583, 498, 291, 434, 884, 11, 370, 341, 390, 257, 8141, 1413, 257, 8062, 13, 759, 291, 645, 884, 257, 8141, 1413, 257, 8141, 11, 437, 291, 434, 884, 307, 1940, 257, 3840, 295, 819, 8213, 21267, 295, 264, 700, 8141, 11], "temperature": 0.0, "avg_logprob": -0.05819778804537616, "compression_ratio": 1.8226600985221675, "no_speech_prob": 2.078428224194795e-05}, {"id": 326, "seek": 305600, "start": 3067.0, "end": 3074.0, "text": " where each column in your second matrix gives you the coefficients that you're using for this linear combination.", "tokens": [689, 1184, 7738, 294, 428, 1150, 8141, 2709, 291, 264, 31994, 300, 291, 434, 1228, 337, 341, 8213, 6562, 13], "temperature": 0.0, "avg_logprob": -0.05819778804537616, "compression_ratio": 1.8226600985221675, "no_speech_prob": 2.078428224194795e-05}, {"id": 327, "seek": 305600, "start": 3074.0, "end": 3084.0, "text": " Any questions about this interpretation of matrix multiplication?", "tokens": [2639, 1651, 466, 341, 14174, 295, 8141, 27290, 30], "temperature": 0.0, "avg_logprob": -0.05819778804537616, "compression_ratio": 1.8226600985221675, "no_speech_prob": 2.078428224194795e-05}, {"id": 328, "seek": 308400, "start": 3084.0, "end": 3091.0, "text": " Okay, so this is, yeah, I think this is pretty useful and not, not talked about enough in linear algebra.", "tokens": [1033, 11, 370, 341, 307, 11, 1338, 11, 286, 519, 341, 307, 1238, 4420, 293, 406, 11, 406, 2825, 466, 1547, 294, 8213, 21989, 13], "temperature": 0.0, "avg_logprob": -0.10104108174641926, "compression_ratio": 1.6281407035175879, "no_speech_prob": 1.3006793778913561e-05}, {"id": 329, "seek": 308400, "start": 3091.0, "end": 3109.0, "text": " And so that's what, that's what's going on here. In this picture is you have these facial features, and then you're taking a bunch of different linear combinations of facial features to reconstruct your original faces.", "tokens": [400, 370, 300, 311, 437, 11, 300, 311, 437, 311, 516, 322, 510, 13, 682, 341, 3036, 307, 291, 362, 613, 15642, 4122, 11, 293, 550, 291, 434, 1940, 257, 3840, 295, 819, 8213, 21267, 295, 15642, 4122, 281, 31499, 428, 3380, 8475, 13], "temperature": 0.0, "avg_logprob": -0.10104108174641926, "compression_ratio": 1.6281407035175879, "no_speech_prob": 1.3006793778913561e-05}, {"id": 330, "seek": 310900, "start": 3109.0, "end": 3123.0, "text": " Things to note about NMF is that it's non-unique. So SVD is giving you a unique decomposition, NMF does not.", "tokens": [9514, 281, 3637, 466, 426, 44, 37, 307, 300, 309, 311, 2107, 12, 409, 1925, 13, 407, 31910, 35, 307, 2902, 291, 257, 3845, 48356, 11, 426, 44, 37, 775, 406, 13], "temperature": 0.0, "avg_logprob": -0.10236094146966934, "compression_ratio": 1.4310344827586208, "no_speech_prob": 1.5206276657409035e-05}, {"id": 331, "seek": 310900, "start": 3123.0, "end": 3136.0, "text": " Here I list several, several applications, collaborative filtering, face decompositions, audio source separation, chemistry, bioinformatics.", "tokens": [1692, 286, 1329, 2940, 11, 2940, 5821, 11, 16555, 30822, 11, 1851, 22867, 329, 2451, 11, 6278, 4009, 14634, 11, 12558, 11, 12198, 37811, 30292, 13], "temperature": 0.0, "avg_logprob": -0.10236094146966934, "compression_ratio": 1.4310344827586208, "no_speech_prob": 1.5206276657409035e-05}, {"id": 332, "seek": 313600, "start": 3136.0, "end": 3148.0, "text": " But going back to NLP and our topic at hand, if we were looking at, I should note this is transposed from a lot of the matrix, how the matrix is in Excel,", "tokens": [583, 516, 646, 281, 426, 45196, 293, 527, 4829, 412, 1011, 11, 498, 321, 645, 1237, 412, 11, 286, 820, 3637, 341, 307, 7132, 1744, 490, 257, 688, 295, 264, 8141, 11, 577, 264, 8141, 307, 294, 19060, 11], "temperature": 0.0, "avg_logprob": -0.08543118976411365, "compression_ratio": 1.4252873563218391, "no_speech_prob": 2.0144880181760527e-05}, {"id": 333, "seek": 313600, "start": 3148.0, "end": 3157.0, "text": " where it's words by documents, then this is giving us topics and topic importance indicators.", "tokens": [689, 309, 311, 2283, 538, 8512, 11, 550, 341, 307, 2902, 505, 8378, 293, 4829, 7379, 22176, 13], "temperature": 0.0, "avg_logprob": -0.08543118976411365, "compression_ratio": 1.4252873563218391, "no_speech_prob": 2.0144880181760527e-05}, {"id": 334, "seek": 315700, "start": 3157.0, "end": 3171.0, "text": " We're going to use Scikit-learns implementation of NMF, and here we have to set the number of topics. So that's setting the number of clusters that", "tokens": [492, 434, 516, 281, 764, 16942, 22681, 12, 306, 1083, 82, 11420, 295, 426, 44, 37, 11, 293, 510, 321, 362, 281, 992, 264, 1230, 295, 8378, 13, 407, 300, 311, 3287, 264, 1230, 295, 23313, 300], "temperature": 0.0, "avg_logprob": -0.08174860330275548, "compression_ratio": 1.5527638190954773, "no_speech_prob": 3.535345967975445e-05}, {"id": 335, "seek": 315700, "start": 3171.0, "end": 3180.0, "text": " we expect or we think we might have. And so this is kind of a hyperparameter that you're having to guess at and that you might want to try different things with.", "tokens": [321, 2066, 420, 321, 519, 321, 1062, 362, 13, 400, 370, 341, 307, 733, 295, 257, 9848, 2181, 335, 2398, 300, 291, 434, 1419, 281, 2041, 412, 293, 300, 291, 1062, 528, 281, 853, 819, 721, 365, 13], "temperature": 0.0, "avg_logprob": -0.08174860330275548, "compression_ratio": 1.5527638190954773, "no_speech_prob": 3.535345967975445e-05}, {"id": 336, "seek": 318000, "start": 3180.0, "end": 3191.0, "text": " So we'll use, yes? Oh wait, let me review.", "tokens": [407, 321, 603, 764, 11, 2086, 30, 876, 1699, 11, 718, 385, 3131, 13], "temperature": 0.0, "avg_logprob": -0.21352484776423528, "compression_ratio": 1.4405594405594406, "no_speech_prob": 7.253552757902071e-05}, {"id": 337, "seek": 318000, "start": 3191.0, "end": 3198.0, "text": " So you said number of topics is a hyperparameter?", "tokens": [407, 291, 848, 1230, 295, 8378, 307, 257, 9848, 2181, 335, 2398, 30], "temperature": 0.0, "avg_logprob": -0.21352484776423528, "compression_ratio": 1.4405594405594406, "no_speech_prob": 7.253552757902071e-05}, {"id": 338, "seek": 318000, "start": 3198.0, "end": 3199.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.21352484776423528, "compression_ratio": 1.4405594405594406, "no_speech_prob": 7.253552757902071e-05}, {"id": 339, "seek": 318000, "start": 3199.0, "end": 3209.0, "text": " So how do you, do you have a metric or a graph that you can evaluate what number of topics is good for this?", "tokens": [407, 577, 360, 291, 11, 360, 291, 362, 257, 20678, 420, 257, 4295, 300, 291, 393, 13059, 437, 1230, 295, 8378, 307, 665, 337, 341, 30], "temperature": 0.0, "avg_logprob": -0.21352484776423528, "compression_ratio": 1.4405594405594406, "no_speech_prob": 7.253552757902071e-05}, {"id": 340, "seek": 320900, "start": 3209.0, "end": 3221.0, "text": " I mean this is hard because it's an unsupervised problem. Like we don't have, usually you don't have a ground truth of what the true topics are.", "tokens": [286, 914, 341, 307, 1152, 570, 309, 311, 364, 2693, 12879, 24420, 1154, 13, 1743, 321, 500, 380, 362, 11, 2673, 291, 500, 380, 362, 257, 2727, 3494, 295, 437, 264, 2074, 8378, 366, 13], "temperature": 0.0, "avg_logprob": -0.1366072620253965, "compression_ratio": 1.587378640776699, "no_speech_prob": 3.822453072643839e-05}, {"id": 341, "seek": 320900, "start": 3221.0, "end": 3231.0, "text": " So in this case, we don't know. Like in practice, if you were working on this type of problem, I think you would want to do some exploratory data analysis to see like how many topics", "tokens": [407, 294, 341, 1389, 11, 321, 500, 380, 458, 13, 1743, 294, 3124, 11, 498, 291, 645, 1364, 322, 341, 2010, 295, 1154, 11, 286, 519, 291, 576, 528, 281, 360, 512, 24765, 4745, 1412, 5215, 281, 536, 411, 577, 867, 8378], "temperature": 0.0, "avg_logprob": -0.1366072620253965, "compression_ratio": 1.587378640776699, "no_speech_prob": 3.822453072643839e-05}, {"id": 342, "seek": 323100, "start": 3231.0, "end": 3239.0, "text": " does this look like to me? Try it and see like do these topics seem reasonable together?", "tokens": [775, 341, 574, 411, 281, 385, 30, 6526, 309, 293, 536, 411, 360, 613, 8378, 1643, 10585, 1214, 30], "temperature": 0.0, "avg_logprob": -0.11607534166366335, "compression_ratio": 1.453416149068323, "no_speech_prob": 1.6963847883744165e-05}, {"id": 343, "seek": 323100, "start": 3239.0, "end": 3240.0, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.11607534166366335, "compression_ratio": 1.453416149068323, "no_speech_prob": 1.6963847883744165e-05}, {"id": 344, "seek": 323100, "start": 3240.0, "end": 3243.0, "text": " You're welcome.", "tokens": [509, 434, 2928, 13], "temperature": 0.0, "avg_logprob": -0.11607534166366335, "compression_ratio": 1.453416149068323, "no_speech_prob": 1.6963847883744165e-05}, {"id": 345, "seek": 323100, "start": 3243.0, "end": 3254.0, "text": " Yeah, I mean I think there is something a little bit dissatisfying about the fact that you're kind of having to set this.", "tokens": [865, 11, 286, 914, 286, 519, 456, 307, 746, 257, 707, 857, 7802, 25239, 1840, 466, 264, 1186, 300, 291, 434, 733, 295, 1419, 281, 992, 341, 13], "temperature": 0.0, "avg_logprob": -0.11607534166366335, "compression_ratio": 1.453416149068323, "no_speech_prob": 1.6963847883744165e-05}, {"id": 346, "seek": 325400, "start": 3254.0, "end": 3266.0, "text": " So here I've run the decomposition, well I created a decomposition, fit the transform to vectors, which is our term document matrix from before.", "tokens": [407, 510, 286, 600, 1190, 264, 48356, 11, 731, 286, 2942, 257, 48356, 11, 3318, 264, 4088, 281, 18875, 11, 597, 307, 527, 1433, 4166, 8141, 490, 949, 13], "temperature": 0.0, "avg_logprob": -0.1411296844482422, "compression_ratio": 1.5354838709677419, "no_speech_prob": 5.6822032092895824e-06}, {"id": 347, "seek": 325400, "start": 3266.0, "end": 3276.0, "text": " And then that returns the first matrix W and then this is how you access the second matrix H.", "tokens": [400, 550, 300, 11247, 264, 700, 8141, 343, 293, 550, 341, 307, 577, 291, 2105, 264, 1150, 8141, 389, 13], "temperature": 0.0, "avg_logprob": -0.1411296844482422, "compression_ratio": 1.5354838709677419, "no_speech_prob": 5.6822032092895824e-06}, {"id": 348, "seek": 327600, "start": 3276.0, "end": 3285.0, "text": " And here I'm applying show topics on H and this is what I get back.", "tokens": [400, 510, 286, 478, 9275, 855, 8378, 322, 389, 293, 341, 307, 437, 286, 483, 646, 13], "temperature": 0.0, "avg_logprob": -0.1284250287867304, "compression_ratio": 1.4204545454545454, "no_speech_prob": 5.093598701932933e-06}, {"id": 349, "seek": 327600, "start": 3285.0, "end": 3289.0, "text": " What do you think of these topics?", "tokens": [708, 360, 291, 519, 295, 613, 8378, 30], "temperature": 0.0, "avg_logprob": -0.1284250287867304, "compression_ratio": 1.4204545454545454, "no_speech_prob": 5.093598701932933e-06}, {"id": 350, "seek": 327600, "start": 3289.0, "end": 3302.0, "text": " I thought they were pretty good. We've got computer images. This is actually very similar to one of the ones that showed up in SVD, the second one.", "tokens": [286, 1194, 436, 645, 1238, 665, 13, 492, 600, 658, 3820, 5267, 13, 639, 307, 767, 588, 2531, 281, 472, 295, 264, 2306, 300, 4712, 493, 294, 31910, 35, 11, 264, 1150, 472, 13], "temperature": 0.0, "avg_logprob": -0.1284250287867304, "compression_ratio": 1.4204545454545454, "no_speech_prob": 5.093598701932933e-06}, {"id": 351, "seek": 330200, "start": 3302.0, "end": 3311.0, "text": " Third is space, either religion or atheism, then other computer graphics. We might want to rerun this with more topics.", "tokens": [12548, 307, 1901, 11, 2139, 7561, 420, 27033, 1434, 11, 550, 661, 3820, 11837, 13, 492, 1062, 528, 281, 43819, 409, 341, 365, 544, 8378, 13], "temperature": 0.0, "avg_logprob": -0.12500509115365835, "compression_ratio": 1.5232558139534884, "no_speech_prob": 2.507038152543828e-05}, {"id": 352, "seek": 330200, "start": 3311.0, "end": 3326.0, "text": " Something to note is my show topics method, remember is only showing, and this is the method that I wrote, the end of the SVD section up here.", "tokens": [6595, 281, 3637, 307, 452, 855, 8378, 3170, 11, 1604, 307, 787, 4099, 11, 293, 341, 307, 264, 3170, 300, 286, 4114, 11, 264, 917, 295, 264, 31910, 35, 3541, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.12500509115365835, "compression_ratio": 1.5232558139534884, "no_speech_prob": 2.507038152543828e-05}, {"id": 353, "seek": 332600, "start": 3326.0, "end": 3345.0, "text": " I've manually chosen the number of topic words because, kind of same with SVD, you can have a value for every single word, you know, which in this case is 25,000 words, but many of those are going to be zero or close to zero for NMF.", "tokens": [286, 600, 16945, 8614, 264, 1230, 295, 4829, 2283, 570, 11, 733, 295, 912, 365, 31910, 35, 11, 291, 393, 362, 257, 2158, 337, 633, 2167, 1349, 11, 291, 458, 11, 597, 294, 341, 1389, 307, 3552, 11, 1360, 2283, 11, 457, 867, 295, 729, 366, 516, 281, 312, 4018, 420, 1998, 281, 4018, 337, 426, 44, 37, 13], "temperature": 0.0, "avg_logprob": -0.11763616255771966, "compression_ratio": 1.4366197183098592, "no_speech_prob": 4.75726010336075e-05}, {"id": 354, "seek": 332600, "start": 3345.0, "end": 3353.0, "text": " And so here I'm just choosing the largest to display as the topic words.", "tokens": [400, 370, 510, 286, 478, 445, 10875, 264, 6443, 281, 4674, 382, 264, 4829, 2283, 13], "temperature": 0.0, "avg_logprob": -0.11763616255771966, "compression_ratio": 1.4366197183098592, "no_speech_prob": 4.75726010336075e-05}, {"id": 355, "seek": 335300, "start": 3353.0, "end": 3368.0, "text": " And then we can see this in Excel, so I had the SVD, then I also did NMF on this collection of British novels.", "tokens": [400, 550, 321, 393, 536, 341, 294, 19060, 11, 370, 286, 632, 264, 31910, 35, 11, 550, 286, 611, 630, 426, 44, 37, 322, 341, 5765, 295, 6221, 24574, 13], "temperature": 0.0, "avg_logprob": -0.09110417085535386, "compression_ratio": 1.1578947368421053, "no_speech_prob": 2.014494748436846e-05}, {"id": 356, "seek": 336800, "start": 3368.0, "end": 3387.0, "text": " Here I did 10 topics for them. And it's something that I guess is nice is if you have lots of zeros, it's sparser, so like here, Kathy only shows up or sorry.", "tokens": [1692, 286, 630, 1266, 8378, 337, 552, 13, 400, 309, 311, 746, 300, 286, 2041, 307, 1481, 307, 498, 291, 362, 3195, 295, 35193, 11, 309, 311, 637, 685, 260, 11, 370, 411, 510, 11, 30740, 787, 3110, 493, 420, 2597, 13], "temperature": 0.0, "avg_logprob": -0.19953439546668011, "compression_ratio": 1.234375, "no_speech_prob": 1.6963769667199813e-05}, {"id": 357, "seek": 338700, "start": 3387.0, "end": 3398.0, "text": " Yes, sometimes the words are only showing up in one of the topics. Then we could go back and check. Let's see, so Kathy is in topic six.", "tokens": [1079, 11, 2171, 264, 2283, 366, 787, 4099, 493, 294, 472, 295, 264, 8378, 13, 1396, 321, 727, 352, 646, 293, 1520, 13, 961, 311, 536, 11, 370, 30740, 307, 294, 4829, 2309, 13], "temperature": 0.0, "avg_logprob": -0.1275117276894926, "compression_ratio": 1.5427350427350428, "no_speech_prob": 2.5070512492675334e-05}, {"id": 358, "seek": 338700, "start": 3398.0, "end": 3409.0, "text": " We would expect Wuthering Heights to have a fair amount of topic six, since Kathy is one of the main characters and it does, it's a 0.79.", "tokens": [492, 576, 2066, 343, 17696, 278, 44039, 281, 362, 257, 3143, 2372, 295, 4829, 2309, 11, 1670, 30740, 307, 472, 295, 264, 2135, 4342, 293, 309, 775, 11, 309, 311, 257, 1958, 13, 32042, 13], "temperature": 0.0, "avg_logprob": -0.1275117276894926, "compression_ratio": 1.5427350427350428, "no_speech_prob": 2.5070512492675334e-05}, {"id": 359, "seek": 338700, "start": 3409.0, "end": 3416.0, "text": " And so this is kind of how you can look at it again. We've got just two matrices here.", "tokens": [400, 370, 341, 307, 733, 295, 577, 291, 393, 574, 412, 309, 797, 13, 492, 600, 658, 445, 732, 32284, 510, 13], "temperature": 0.0, "avg_logprob": -0.1275117276894926, "compression_ratio": 1.5427350427350428, "no_speech_prob": 2.5070512492675334e-05}, {"id": 360, "seek": 341600, "start": 3416.0, "end": 3420.0, "text": " Any questions?", "tokens": [2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.1796322909268466, "compression_ratio": 1.3385826771653544, "no_speech_prob": 4.985285704606213e-05}, {"id": 361, "seek": 341600, "start": 3420.0, "end": 3422.0, "text": " And let me.", "tokens": [400, 718, 385, 13], "temperature": 0.0, "avg_logprob": -0.1796322909268466, "compression_ratio": 1.3385826771653544, "no_speech_prob": 4.985285704606213e-05}, {"id": 362, "seek": 341600, "start": 3422.0, "end": 3428.0, "text": " Okay, I'll repeat the question.", "tokens": [1033, 11, 286, 603, 7149, 264, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1796322909268466, "compression_ratio": 1.3385826771653544, "no_speech_prob": 4.985285704606213e-05}, {"id": 363, "seek": 341600, "start": 3428.0, "end": 3440.0, "text": " How did I get the Excel file? So I cheated and I did this work in an IPython notebook. So I did this in Python.", "tokens": [1012, 630, 286, 483, 264, 19060, 3991, 30, 407, 286, 28079, 293, 286, 630, 341, 589, 294, 364, 8671, 88, 11943, 21060, 13, 407, 286, 630, 341, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.1796322909268466, "compression_ratio": 1.3385826771653544, "no_speech_prob": 4.985285704606213e-05}, {"id": 364, "seek": 344000, "start": 3440.0, "end": 3449.0, "text": " And then I think I saved them as CSVs maybe, the matrices, so that I could copy them into an Excel file.", "tokens": [400, 550, 286, 519, 286, 6624, 552, 382, 48814, 82, 1310, 11, 264, 32284, 11, 370, 300, 286, 727, 5055, 552, 666, 364, 19060, 3991, 13], "temperature": 0.0, "avg_logprob": -0.12406261245925705, "compression_ratio": 1.4627659574468086, "no_speech_prob": 2.0143523215665482e-05}, {"id": 365, "seek": 344000, "start": 3449.0, "end": 3463.0, "text": " Yeah, so I don't think I saved that. Sorry, this is from a few years ago. Yeah, I kind of did it as scratch work, but I encourage you to try recreating it because I link.", "tokens": [865, 11, 370, 286, 500, 380, 519, 286, 6624, 300, 13, 4919, 11, 341, 307, 490, 257, 1326, 924, 2057, 13, 865, 11, 286, 733, 295, 630, 309, 382, 8459, 589, 11, 457, 286, 5373, 291, 281, 853, 850, 44613, 309, 570, 286, 2113, 13], "temperature": 0.0, "avg_logprob": -0.12406261245925705, "compression_ratio": 1.4627659574468086, "no_speech_prob": 2.0143523215665482e-05}, {"id": 366, "seek": 346300, "start": 3463.0, "end": 3472.0, "text": " I link to the data source here so you can kind of download the British novel data. Oh no, I do have the Python code.", "tokens": [286, 2113, 281, 264, 1412, 4009, 510, 370, 291, 393, 733, 295, 5484, 264, 6221, 7613, 1412, 13, 876, 572, 11, 286, 360, 362, 264, 15329, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1161011612933615, "compression_ratio": 1.5911111111111111, "no_speech_prob": 4.399610770633444e-05}, {"id": 367, "seek": 346300, "start": 3472.0, "end": 3481.0, "text": " Yeah, and I link to the Python code. Okay, so yeah, it is available. This is from the numerical linear algebra class.", "tokens": [865, 11, 293, 286, 2113, 281, 264, 15329, 3089, 13, 1033, 11, 370, 1338, 11, 309, 307, 2435, 13, 639, 307, 490, 264, 29054, 8213, 21989, 1508, 13], "temperature": 0.0, "avg_logprob": -0.1161011612933615, "compression_ratio": 1.5911111111111111, "no_speech_prob": 4.399610770633444e-05}, {"id": 368, "seek": 346300, "start": 3481.0, "end": 3490.0, "text": " But so the Excel file is in the GitHub repo for this class, and then it's got the information on how you can recreate this.", "tokens": [583, 370, 264, 19060, 3991, 307, 294, 264, 23331, 49040, 337, 341, 1508, 11, 293, 550, 309, 311, 658, 264, 1589, 322, 577, 291, 393, 25833, 341, 13], "temperature": 0.0, "avg_logprob": -0.1161011612933615, "compression_ratio": 1.5911111111111111, "no_speech_prob": 4.399610770633444e-05}, {"id": 369, "seek": 349000, "start": 3490.0, "end": 3497.0, "text": " But here I really primarily just wanted to give you a way to visualize, okay, what are these factorizations look like?", "tokens": [583, 510, 286, 534, 10029, 445, 1415, 281, 976, 291, 257, 636, 281, 23273, 11, 1392, 11, 437, 366, 613, 5952, 14455, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.10880154576794855, "compression_ratio": 1.5246636771300448, "no_speech_prob": 2.7967680580331944e-05}, {"id": 370, "seek": 349000, "start": 3497.0, "end": 3505.0, "text": " So I think when you're just reading the equations, they can sound more complicated. And then when you see it, it's like, okay, these are just matrices of numbers.", "tokens": [407, 286, 519, 562, 291, 434, 445, 3760, 264, 11787, 11, 436, 393, 1626, 544, 6179, 13, 400, 550, 562, 291, 536, 309, 11, 309, 311, 411, 11, 1392, 11, 613, 366, 445, 32284, 295, 3547, 13], "temperature": 0.0, "avg_logprob": -0.10880154576794855, "compression_ratio": 1.5246636771300448, "no_speech_prob": 2.7967680580331944e-05}, {"id": 371, "seek": 349000, "start": 3505.0, "end": 3511.0, "text": " Any other questions about this?", "tokens": [2639, 661, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.10880154576794855, "compression_ratio": 1.5246636771300448, "no_speech_prob": 2.7967680580331944e-05}, {"id": 372, "seek": 349000, "start": 3511.0, "end": 3515.0, "text": " Okay, so let's go back to.", "tokens": [1033, 11, 370, 718, 311, 352, 646, 281, 13], "temperature": 0.0, "avg_logprob": -0.10880154576794855, "compression_ratio": 1.5246636771300448, "no_speech_prob": 2.7967680580331944e-05}, {"id": 373, "seek": 351500, "start": 3515.0, "end": 3526.0, "text": " Back to the Jupyter notebook. Yeah, so next I was going to talk about TFIDF, which is Topic Frequency Inverse Document Frequency.", "tokens": [5833, 281, 264, 22125, 88, 391, 21060, 13, 865, 11, 370, 958, 286, 390, 516, 281, 751, 466, 40964, 2777, 37, 11, 597, 307, 8840, 299, 6142, 48154, 682, 4308, 37684, 6142, 48154, 13], "temperature": 0.0, "avg_logprob": -0.08820720771690468, "compression_ratio": 1.4623115577889447, "no_speech_prob": 9.609118569642305e-05}, {"id": 374, "seek": 351500, "start": 3526.0, "end": 3539.0, "text": " And it's a way to normalize term counts by taking into account how often they appear in a document, how long the document is, and how common or rare the term is.", "tokens": [400, 309, 311, 257, 636, 281, 2710, 1125, 1433, 14893, 538, 1940, 666, 2696, 577, 2049, 436, 4204, 294, 257, 4166, 11, 577, 938, 264, 4166, 307, 11, 293, 577, 2689, 420, 5892, 264, 1433, 307, 13], "temperature": 0.0, "avg_logprob": -0.08820720771690468, "compression_ratio": 1.4623115577889447, "no_speech_prob": 9.609118569642305e-05}, {"id": 375, "seek": 353900, "start": 3539.0, "end": 3554.0, "text": " So if there's a term that's extremely rare, it's more important. If a document super long, it kind of means less for each topic that appears in it, because you can assume a really long document just has more content in general.", "tokens": [407, 498, 456, 311, 257, 1433, 300, 311, 4664, 5892, 11, 309, 311, 544, 1021, 13, 759, 257, 4166, 1687, 938, 11, 309, 733, 295, 1355, 1570, 337, 1184, 4829, 300, 7038, 294, 309, 11, 570, 291, 393, 6552, 257, 534, 938, 4166, 445, 575, 544, 2701, 294, 2674, 13], "temperature": 0.0, "avg_logprob": -0.10286087603182406, "compression_ratio": 1.4375, "no_speech_prob": 1.7230910088983364e-05}, {"id": 376, "seek": 353900, "start": 3554.0, "end": 3559.0, "text": " I'm going to use Scikit-Learns TFIDF Vectorizer.", "tokens": [286, 478, 516, 281, 764, 16942, 22681, 12, 11020, 1083, 82, 40964, 2777, 37, 691, 20814, 6545, 13], "temperature": 0.0, "avg_logprob": -0.10286087603182406, "compression_ratio": 1.4375, "no_speech_prob": 1.7230910088983364e-05}, {"id": 377, "seek": 355900, "start": 3559.0, "end": 3572.0, "text": " This is just another way of kind of processing our data. And so again, it's this newsgroup training data from from before that we had.", "tokens": [639, 307, 445, 1071, 636, 295, 733, 295, 9007, 527, 1412, 13, 400, 370, 797, 11, 309, 311, 341, 2583, 17377, 3097, 1412, 490, 490, 949, 300, 321, 632, 13], "temperature": 0.0, "avg_logprob": -0.10142251161428598, "compression_ratio": 1.381294964028777, "no_speech_prob": 2.212483923358377e-05}, {"id": 378, "seek": 355900, "start": 3572.0, "end": 3581.0, "text": " And then here are the topics I get this time for for NMF.", "tokens": [400, 550, 510, 366, 264, 8378, 286, 483, 341, 565, 337, 337, 426, 44, 37, 13], "temperature": 0.0, "avg_logprob": -0.10142251161428598, "compression_ratio": 1.381294964028777, "no_speech_prob": 2.212483923358377e-05}, {"id": 379, "seek": 358100, "start": 3581.0, "end": 3589.0, "text": " And here the fourth one is pretty weird, but the other.", "tokens": [400, 510, 264, 6409, 472, 307, 1238, 3657, 11, 457, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.15338819367544992, "compression_ratio": 1.1875, "no_speech_prob": 1.8448024547979003e-06}, {"id": 380, "seek": 358100, "start": 3589.0, "end": 3601.0, "text": " I think the other four are pretty good.", "tokens": [286, 519, 264, 661, 1451, 366, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.15338819367544992, "compression_ratio": 1.1875, "no_speech_prob": 1.8448024547979003e-06}, {"id": 381, "seek": 360100, "start": 3601.0, "end": 3613.0, "text": " And something to note about NMF is that it is not exact, so you may not get your original matrix back perfectly.", "tokens": [400, 746, 281, 3637, 466, 426, 44, 37, 307, 300, 309, 307, 406, 1900, 11, 370, 291, 815, 406, 483, 428, 3380, 8141, 646, 6239, 13], "temperature": 0.0, "avg_logprob": -0.04051391644911333, "compression_ratio": 1.2380952380952381, "no_speech_prob": 2.178126851504203e-05}, {"id": 382, "seek": 360100, "start": 3613.0, "end": 3617.0, "text": " And as I said before, it's also not unique.", "tokens": [400, 382, 286, 848, 949, 11, 309, 311, 611, 406, 3845, 13], "temperature": 0.0, "avg_logprob": -0.04051391644911333, "compression_ratio": 1.2380952380952381, "no_speech_prob": 2.178126851504203e-05}, {"id": 383, "seek": 361700, "start": 3617.0, "end": 3642.0, "text": " So NMF can be fast and easy to use. It did take years of research and expertise to create. Also for NMF, the matrix needs to be at least as tall as it is wide or you'll get an error with the fit transform, but you can always just transpose your matrix to get it in that format.", "tokens": [407, 426, 44, 37, 393, 312, 2370, 293, 1858, 281, 764, 13, 467, 630, 747, 924, 295, 2132, 293, 11769, 281, 1884, 13, 2743, 337, 426, 44, 37, 11, 264, 8141, 2203, 281, 312, 412, 1935, 382, 6764, 382, 309, 307, 4874, 420, 291, 603, 483, 364, 6713, 365, 264, 3318, 4088, 11, 457, 291, 393, 1009, 445, 25167, 428, 8141, 281, 483, 309, 294, 300, 7877, 13], "temperature": 0.0, "avg_logprob": -0.07559098137749566, "compression_ratio": 1.5054347826086956, "no_speech_prob": 2.1443309378810227e-05}, {"id": 384, "seek": 364200, "start": 3642.0, "end": 3654.0, "text": " So something that was nice about NMF is that we were only having to calculate a few topics and SVD, we were calculating 2000 topics, right?", "tokens": [407, 746, 300, 390, 1481, 466, 426, 44, 37, 307, 300, 321, 645, 787, 1419, 281, 8873, 257, 1326, 8378, 293, 31910, 35, 11, 321, 645, 28258, 8132, 8378, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.09607774665556758, "compression_ratio": 1.5804878048780489, "no_speech_prob": 1.9832048565149307e-05}, {"id": 385, "seek": 364200, "start": 3654.0, "end": 3663.0, "text": " Whereas most of those were not very meaningful and we kind of would like that benefit for SVD of not having to calculate so many topics.", "tokens": [13813, 881, 295, 729, 645, 406, 588, 10995, 293, 321, 733, 295, 576, 411, 300, 5121, 337, 31910, 35, 295, 406, 1419, 281, 8873, 370, 867, 8378, 13], "temperature": 0.0, "avg_logprob": -0.09607774665556758, "compression_ratio": 1.5804878048780489, "no_speech_prob": 1.9832048565149307e-05}, {"id": 386, "seek": 364200, "start": 3663.0, "end": 3668.0, "text": " And the way to do this is called truncated SVD.", "tokens": [400, 264, 636, 281, 360, 341, 307, 1219, 504, 409, 66, 770, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.09607774665556758, "compression_ratio": 1.5804878048780489, "no_speech_prob": 1.9832048565149307e-05}, {"id": 387, "seek": 366800, "start": 3668.0, "end": 3676.0, "text": " We're really just interested in the vectors corresponding to the largest singular values.", "tokens": [492, 434, 534, 445, 3102, 294, 264, 18875, 11760, 281, 264, 6443, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.15787985211326963, "compression_ratio": 1.2846715328467153, "no_speech_prob": 4.198402530164458e-05}, {"id": 388, "seek": 366800, "start": 3676.0, "end": 3683.0, "text": " And so this, there's a nice Facebook research, had a nice post on fast randomized SVD.", "tokens": [400, 370, 341, 11, 456, 311, 257, 1481, 4384, 2132, 11, 632, 257, 1481, 2183, 322, 2370, 38513, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.15787985211326963, "compression_ratio": 1.2846715328467153, "no_speech_prob": 4.198402530164458e-05}, {"id": 389, "seek": 368300, "start": 3683.0, "end": 3700.0, "text": " So some shortcomings of class classical algorithms for decomposition are that matrices are often quotes, stupendously big. This is from a nice paper. Let me open this.", "tokens": [407, 512, 2099, 49886, 295, 1508, 13735, 14642, 337, 48356, 366, 300, 32284, 366, 2049, 19963, 11, 342, 1010, 521, 5098, 955, 13, 639, 307, 490, 257, 1481, 3035, 13, 961, 385, 1269, 341, 13], "temperature": 0.0, "avg_logprob": -0.1657110663021312, "compression_ratio": 1.490066225165563, "no_speech_prob": 1.7777505490812473e-05}, {"id": 390, "seek": 368300, "start": 3700.0, "end": 3707.0, "text": " About probabilistic algorithms for matrix decompositions.", "tokens": [7769, 31959, 3142, 14642, 337, 8141, 22867, 329, 2451, 13], "temperature": 0.0, "avg_logprob": -0.1657110663021312, "compression_ratio": 1.490066225165563, "no_speech_prob": 1.7777505490812473e-05}, {"id": 391, "seek": 370700, "start": 3707.0, "end": 3718.0, "text": " Data is also missing or inaccurate in the real world and it's kind of why use a ton of computational resources when you know that your input was somewhat imprecise.", "tokens": [11888, 307, 611, 5361, 420, 46443, 294, 264, 957, 1002, 293, 309, 311, 733, 295, 983, 764, 257, 2952, 295, 28270, 3593, 562, 291, 458, 300, 428, 4846, 390, 8344, 704, 13867, 908, 13], "temperature": 0.0, "avg_logprob": -0.0939971991947719, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.169072442688048e-05}, {"id": 392, "seek": 370700, "start": 3718.0, "end": 3729.0, "text": " And so there's, you know, there's not a big benefit to being very precise with your calculation if your input data is not that precise.", "tokens": [400, 370, 456, 311, 11, 291, 458, 11, 456, 311, 406, 257, 955, 5121, 281, 885, 588, 13600, 365, 428, 17108, 498, 428, 4846, 1412, 307, 406, 300, 13600, 13], "temperature": 0.0, "avg_logprob": -0.0939971991947719, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.169072442688048e-05}, {"id": 393, "seek": 372900, "start": 3729.0, "end": 3744.0, "text": " Data transfer now plays a major time in algorithms. And so this is moving, moving data from disk memory to, you know, registers or cache, moving it to the GPU.", "tokens": [11888, 5003, 586, 5749, 257, 2563, 565, 294, 14642, 13, 400, 370, 341, 307, 2684, 11, 2684, 1412, 490, 12355, 4675, 281, 11, 291, 458, 11, 38351, 420, 19459, 11, 2684, 309, 281, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.13759458632696242, "compression_ratio": 1.4662921348314606, "no_speech_prob": 1.5688763596699573e-05}, {"id": 394, "seek": 372900, "start": 3744.0, "end": 3754.0, "text": " If you're doing GPU computations, but moving data around can be time consumed or very time consuming.", "tokens": [759, 291, 434, 884, 18407, 2807, 763, 11, 457, 2684, 1412, 926, 393, 312, 565, 21226, 420, 588, 565, 19867, 13], "temperature": 0.0, "avg_logprob": -0.13759458632696242, "compression_ratio": 1.4662921348314606, "no_speech_prob": 1.5688763596699573e-05}, {"id": 395, "seek": 375400, "start": 3754.0, "end": 3759.0, "text": " And so randomized algorithms are a great, great solution to this.", "tokens": [400, 370, 38513, 14642, 366, 257, 869, 11, 869, 3827, 281, 341, 13], "temperature": 0.0, "avg_logprob": -0.07482741560254778, "compression_ratio": 1.3455882352941178, "no_speech_prob": 1.0782871868286747e-05}, {"id": 396, "seek": 375400, "start": 3759.0, "end": 3762.0, "text": " They're stable.", "tokens": [814, 434, 8351, 13], "temperature": 0.0, "avg_logprob": -0.07482741560254778, "compression_ratio": 1.3455882352941178, "no_speech_prob": 1.0782871868286747e-05}, {"id": 397, "seek": 375400, "start": 3762.0, "end": 3768.0, "text": " The performance guarantees often don't depend on kind of these properties of the underlying matrices.", "tokens": [440, 3389, 32567, 2049, 500, 380, 5672, 322, 733, 295, 613, 7221, 295, 264, 14217, 32284, 13], "temperature": 0.0, "avg_logprob": -0.07482741560254778, "compression_ratio": 1.3455882352941178, "no_speech_prob": 1.0782871868286747e-05}, {"id": 398, "seek": 376800, "start": 3768.0, "end": 3788.0, "text": " They can be done in parallel. And so we're just going to look at randomized SVD as one example of a randomized algorithm and it's going to kind of let us get some of these benefits for SVD that we saw of NMF of, you know, we only had to calculate five topics for NMF.", "tokens": [814, 393, 312, 1096, 294, 8952, 13, 400, 370, 321, 434, 445, 516, 281, 574, 412, 38513, 31910, 35, 382, 472, 1365, 295, 257, 38513, 9284, 293, 309, 311, 516, 281, 733, 295, 718, 505, 483, 512, 295, 613, 5311, 337, 31910, 35, 300, 321, 1866, 295, 426, 44, 37, 295, 11, 291, 458, 11, 321, 787, 632, 281, 8873, 1732, 8378, 337, 426, 44, 37, 13], "temperature": 0.0, "avg_logprob": -0.07282183203898684, "compression_ratio": 1.435483870967742, "no_speech_prob": 1.3630598004965577e-05}, {"id": 399, "seek": 378800, "start": 3788.0, "end": 3804.0, "text": " If that's the number we chose, whereas with the original SVD, we were throwing away a ton of data by only looking at, you know, a few singular values and a few topics.", "tokens": [759, 300, 311, 264, 1230, 321, 5111, 11, 9735, 365, 264, 3380, 31910, 35, 11, 321, 645, 10238, 1314, 257, 2952, 295, 1412, 538, 787, 1237, 412, 11, 291, 458, 11, 257, 1326, 20010, 4190, 293, 257, 1326, 8378, 13], "temperature": 0.0, "avg_logprob": -0.0567834268916737, "compression_ratio": 1.3046875, "no_speech_prob": 4.0928002817963716e-06}, {"id": 400, "seek": 380400, "start": 3804.0, "end": 3829.0, "text": " So I have a timing comparison of doing NumPy's linear algebra SVD that we did before, and it took me four seconds, which is kind of kind of slow, particularly we only had 2000 documents here, which is, I mean, depending on what you're doing, you could be doing something much larger.", "tokens": [407, 286, 362, 257, 10822, 9660, 295, 884, 22592, 47, 88, 311, 8213, 21989, 31910, 35, 300, 321, 630, 949, 11, 293, 309, 1890, 385, 1451, 3949, 11, 597, 307, 733, 295, 733, 295, 2964, 11, 4098, 321, 787, 632, 8132, 8512, 510, 11, 597, 307, 11, 286, 914, 11, 5413, 322, 437, 291, 434, 884, 11, 291, 727, 312, 884, 746, 709, 4833, 13], "temperature": 0.0, "avg_logprob": -0.10111212384873543, "compression_ratio": 1.4365482233502538, "no_speech_prob": 6.143979589978699e-06}, {"id": 401, "seek": 382900, "start": 3829.0, "end": 3840.0, "text": " Then I used SKLearns implementation of randomized SVD, and it was three seconds. So that's a nice, I mean, it's a 25% speed up.", "tokens": [1396, 286, 1143, 21483, 11020, 1083, 82, 11420, 295, 38513, 31910, 35, 11, 293, 309, 390, 1045, 3949, 13, 407, 300, 311, 257, 1481, 11, 286, 914, 11, 309, 311, 257, 3552, 4, 3073, 493, 13], "temperature": 0.0, "avg_logprob": -0.17497226338327668, "compression_ratio": 1.529100529100529, "no_speech_prob": 1.4970494703447912e-05}, {"id": 402, "seek": 382900, "start": 3840.0, "end": 3853.0, "text": " And then I used randomized SVD from Facebook's library FBPCA, and that was just one second so that's significantly faster. So that's neat that it's a lot faster.", "tokens": [400, 550, 286, 1143, 38513, 31910, 35, 490, 4384, 311, 6405, 479, 33, 12986, 32, 11, 293, 300, 390, 445, 472, 1150, 370, 300, 311, 10591, 4663, 13, 407, 300, 311, 10654, 300, 309, 311, 257, 688, 4663, 13], "temperature": 0.0, "avg_logprob": -0.17497226338327668, "compression_ratio": 1.529100529100529, "no_speech_prob": 1.4970494703447912e-05}, {"id": 403, "seek": 385300, "start": 3853.0, "end": 3868.0, "text": " I should note here I'm having to input how many singular values I want to calculate, which is going to map to, you're going to have the same number of columns of U and rows of V.", "tokens": [286, 820, 3637, 510, 286, 478, 1419, 281, 4846, 577, 867, 20010, 4190, 286, 528, 281, 8873, 11, 597, 307, 516, 281, 4471, 281, 11, 291, 434, 516, 281, 362, 264, 912, 1230, 295, 13766, 295, 624, 293, 13241, 295, 691, 13], "temperature": 0.0, "avg_logprob": -0.12132108729818593, "compression_ratio": 1.3587786259541985, "no_speech_prob": 1.5445828466909006e-05}, {"id": 404, "seek": 386800, "start": 3868.0, "end": 3884.0, "text": " It's going to correspond to your numbers of singular values, which is necessary for this matrix multiplication of U times S times V to work. So I chose 10. If I had chosen more, this would take longer.", "tokens": [467, 311, 516, 281, 6805, 281, 428, 3547, 295, 20010, 4190, 11, 597, 307, 4818, 337, 341, 8141, 27290, 295, 624, 1413, 318, 1413, 691, 281, 589, 13, 407, 286, 5111, 1266, 13, 759, 286, 632, 8614, 544, 11, 341, 576, 747, 2854, 13], "temperature": 0.0, "avg_logprob": -0.07616020242373149, "compression_ratio": 1.34, "no_speech_prob": 6.438837317546131e-06}, {"id": 405, "seek": 388400, "start": 3884.0, "end": 3902.0, "text": " Yeah, and that's the end of this notebook. I'm going to talk more about this, some more about this next time, because I know this was probably a little bit quick. Are there any final questions for today, though?", "tokens": [865, 11, 293, 300, 311, 264, 917, 295, 341, 21060, 13, 286, 478, 516, 281, 751, 544, 466, 341, 11, 512, 544, 466, 341, 958, 565, 11, 570, 286, 458, 341, 390, 1391, 257, 707, 857, 1702, 13, 2014, 456, 604, 2572, 1651, 337, 965, 11, 1673, 30], "temperature": 0.0, "avg_logprob": -0.12967646533045277, "compression_ratio": 1.4113924050632911, "no_speech_prob": 7.888884283602238e-06}, {"id": 406, "seek": 388400, "start": 3902.0, "end": 3907.0, "text": " Yes, Quinn?", "tokens": [1079, 11, 36723, 30], "temperature": 0.0, "avg_logprob": -0.12967646533045277, "compression_ratio": 1.4113924050632911, "no_speech_prob": 7.888884283602238e-06}, {"id": 407, "seek": 390700, "start": 3907.0, "end": 3921.0, "text": " So with the randomized SVD, is there still a way to determine the number of principal components based on the amount of explained variance that you're covering, or because it's randomized, do you lose some of that?", "tokens": [407, 365, 264, 38513, 31910, 35, 11, 307, 456, 920, 257, 636, 281, 6997, 264, 1230, 295, 9716, 6677, 2361, 322, 264, 2372, 295, 8825, 21977, 300, 291, 434, 10322, 11, 420, 570, 309, 311, 38513, 11, 360, 291, 3624, 512, 295, 300, 30], "temperature": 0.0, "avg_logprob": -0.2711861767346346, "compression_ratio": 1.6807511737089202, "no_speech_prob": 1.428441009920789e-05}, {"id": 408, "seek": 390700, "start": 3921.0, "end": 3923.0, "text": " The explained variance of?", "tokens": [440, 8825, 21977, 295, 30], "temperature": 0.0, "avg_logprob": -0.2711861767346346, "compression_ratio": 1.6807511737089202, "no_speech_prob": 1.428441009920789e-05}, {"id": 409, "seek": 390700, "start": 3923.0, "end": 3931.0, "text": " Yeah, like determining how many vectors you want based on how much variance is explained by the number of variables.", "tokens": [865, 11, 411, 23751, 577, 867, 18875, 291, 528, 2361, 322, 577, 709, 21977, 307, 8825, 538, 264, 1230, 295, 9102, 13], "temperature": 0.0, "avg_logprob": -0.2711861767346346, "compression_ratio": 1.6807511737089202, "no_speech_prob": 1.428441009920789e-05}, {"id": 410, "seek": 393100, "start": 3931.0, "end": 3949.0, "text": " So, the singular values, I think, give you that in terms of because the singular values, they're kind of telling you the magnitude, particularly since U and V are, you know, have orthogonal rows and orthogonal columns.", "tokens": [407, 11, 264, 20010, 4190, 11, 286, 519, 11, 976, 291, 300, 294, 2115, 295, 570, 264, 20010, 4190, 11, 436, 434, 733, 295, 3585, 291, 264, 15668, 11, 4098, 1670, 624, 293, 691, 366, 11, 291, 458, 11, 362, 41488, 13241, 293, 41488, 13766, 13], "temperature": 0.0, "avg_logprob": -0.0988880134210354, "compression_ratio": 1.7908163265306123, "no_speech_prob": 2.3922655600472353e-05}, {"id": 411, "seek": 393100, "start": 3949.0, "end": 3957.0, "text": " That means that like each, you know, the pieces of U and V are just one and so all the magnitude is coming from the singular values.", "tokens": [663, 1355, 300, 411, 1184, 11, 291, 458, 11, 264, 3755, 295, 624, 293, 691, 366, 445, 472, 293, 370, 439, 264, 15668, 307, 1348, 490, 264, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.0988880134210354, "compression_ratio": 1.7908163265306123, "no_speech_prob": 2.3922655600472353e-05}, {"id": 412, "seek": 395700, "start": 3957.0, "end": 3969.0, "text": " So yeah, you can look at, yeah, I think smaller singular values and there are techniques for looking at also like the ratio between the singular values of subsequent ones.", "tokens": [407, 1338, 11, 291, 393, 574, 412, 11, 1338, 11, 286, 519, 4356, 20010, 4190, 293, 456, 366, 7512, 337, 1237, 412, 611, 411, 264, 8509, 1296, 264, 20010, 4190, 295, 19962, 2306, 13], "temperature": 0.0, "avg_logprob": -0.12521590232849122, "compression_ratio": 1.4594594594594594, "no_speech_prob": 9.080216841539368e-06}, {"id": 413, "seek": 395700, "start": 3969.0, "end": 3981.0, "text": " That's a good question. Any other questions?", "tokens": [663, 311, 257, 665, 1168, 13, 2639, 661, 1651, 30], "temperature": 0.0, "avg_logprob": -0.12521590232849122, "compression_ratio": 1.4594594594594594, "no_speech_prob": 9.080216841539368e-06}, {"id": 414, "seek": 398100, "start": 3981.0, "end": 3997.0, "text": " Okay, we're about at, thank you, we're about at one o'clock, but yeah, feel free to email me if you have further questions and I will review this on Thursday at the start of class before we go on to the next application.", "tokens": [1033, 11, 321, 434, 466, 412, 11, 1309, 291, 11, 321, 434, 466, 412, 472, 277, 6, 9023, 11, 457, 1338, 11, 841, 1737, 281, 3796, 385, 498, 291, 362, 3052, 1651, 293, 286, 486, 3131, 341, 322, 10383, 412, 264, 722, 295, 1508, 949, 321, 352, 322, 281, 264, 958, 3861, 13], "temperature": 0.0, "avg_logprob": -0.1372773011525472, "compression_ratio": 1.4339622641509433, "no_speech_prob": 4.90640195494052e-05}, {"id": 415, "seek": 399700, "start": 3997.0, "end": 4012.0, "text": " Thanks.", "tokens": [50364, 2561, 13, 51114], "temperature": 0.0, "avg_logprob": -0.4357301712036133, "compression_ratio": 0.4666666666666667, "no_speech_prob": 0.0010224470170214772}], "language": "en"}