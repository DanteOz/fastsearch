{"text": " Hi everybody and welcome to lesson 3 of practical deep learning for coders we did a quick survey this week to see how people feel that the course is tracking and Over half of you think it's about right pace and of the rest who aren't Some of you think it's a bit slow and some of you think it's a bit sorry I don't think it's a bit slow and some of you think it's a bit fast So hopefully we're that's about the best we can do Generally speaking the first two lessons are a little more easy pacing for anybody who's already familiar with the kind of basic technology pieces and then the later lessons get you know more into kind of some of the foundations and today we're going to be talking about You know things like the matrix multiplications and gradients and calculus and stuff like that So for those of you who are more mathy and less computer you might find this one more comfortable and vice versa So remember that there is a Official course updates thread where you can see all of the up-to-date info about Everything you need to know and of course the course website As well, so by the time, you know you watch the video of the lesson It's pretty likely that if you come across a question or an issue somebody else will have so definitely search the forum and check the facts First and then of course feel free to ask a question yourself on the forum if you can't find your answer One thing I did want to point out which you'll see in the lesson thread and the course website is There is also a lesson zero Lesson zero is Based heavily on Radix book meta learning which internally is based heavily on all the things that I've said over the years about how to learn fast AI It's we try to make the course full of Tick bits about the science of learning itself and put them into the course It's a different course to probably any other you've taken and it's I strongly recommend watching lesson zero as Well, the last bit of lesson zero is about how to set up a Linux box from scratch which you can happily skip over unless That's of interest but the rest of it is Full of juicy information that I think you'll find useful So the basic idea of What to do to do a faster AI lesson is Watch the lecture And I generally you know on the video recommend watching it all the way through without stopping once and then go back and Watch it with lots of pauses Running the notebook as you go because otherwise you're kind of like running the notebook Without really knowing where it's heading if that makes sense And the idea of running the notebook is is you know, there's a few notebooks you could go through So obviously there's the book so going through chapter one of the book going through chapter two of the book as notebooks running every code cell and And experimenting with inputs and outputs to try and understand what's going on And then trying to reproduce those results And then trying to repeat the whole thing with a different data set and if you can do that last step, you know, that's Quite a stretch goal particularly at the start of the course because there's so many new concepts But that really shows that you you've got it sorted now first third bit reproduce results I recommend using you'll find in the fastbook repo. So the repository for the book. There is a special folder called Plane and plane contains all of the same chapters of the book But with all of the text removed except the headings and all the outputs removed and this is a great way for you to Test your understanding of the chapter is before you run each cell Try to say to yourself. Okay, what's this for? And what's it going to output if anything and if you kind of work through that slowly? That's a great way at any time You're not sure you can jump back to the the version of the notebook with the text to remind yourself And then head back over to the clean version So there's an idea for something which a lot of people find really useful for self-study I Say self-study, but of course as we've mentioned before the best kind of study is Study done to some extent with others for most people You know the research shows that you're more likely to stick with things if you're doing it as kind of a bit of a social activity There the forums are a great place to find and create study groups And you'll also find on the forums a link to our discord server So now that's our discord server and where there are some study groups there as well so I you know in-person study groups virtual study groups are a great way to You know really make good progress and find other people at a similar level to you if there's not a Study group coming at your level in your area in your time zone Create one so you just post something saying hey, let's create a study group So this week there's been a lot of fantastic activity I can't show all of it so what I did was I used the Summary functionality in the forums to grab all of the things with the highest votes and so I just quickly show a few Of those we have a Marvel detector created this week Identify your favorite Marvel character I love this a rock-paper-scissors game where you actually use pictures of the rock-paper-scissors Symbols and apparently the computer always loses. That's my favorite kind of game There is a lot of Elon around so very handy to have an Elon detector to you know Either find more of him if that's what you need or maybe less of him I thought this one is very interesting. I love these kind of really interesting ideas. There's like gee I wonder if this would work. Can you predict the average? temperature of an area based on a aerial photograph and And the eye and apparently the answer is yeah, actually you can predict it pretty well here in Brisbane It was predicted I believed within one and a half Celsius I Think this student is actually a genuine meteorologist if I remember correctly he built a cloud detector So then building on top of the what's your favorite Marvel character? There's now also an is it a Marvel character? My daughter loves this one. What dinosaur is this and I'm not as good about dinosaurs as I should be I feel like there's Ten times more dinosaurs than there was when I was a kid. So I never know their names. This is very handy This is cool. Choose your own adventure where you choose your path using facial expressions And I think this music genre classification Is also really cool And Brian Smith created a Microsoft power app Application that actually runs on a mobile phone That's pretty cool wouldn't be surprised to hear that Brian actually works at Microsoft. So also an opportunity to promote his own stuff there I thought this art movement classifier was interesting in that like there's a really interesting discussion on the forum about What it actually shows about similarities between different art movements And I thought this redaction Detector project was really was really cool As well, and there's a whole tweet thread and blog post and everything about this one particularly great piece of work Okay, so I'm going to Quickly show you a couple of little tips before we kind of jump into the mechanics of what's behind a neural network Which is I was playing a little bit with how do you make your Neural network more accurate during the week And so I created this pet detector and this pet detector is not just predicting Predicting dogs or cats, but what breed is it? That's obviously a much more difficult exercise now because I put this out on hiking face spaces you can download And look at my code because if you just click files and versions on the space Which you can find a link on the forum and the course website You can see them all here and you can download it to your own computer So I'll show you What I've got here now one thing I mentioned is today I'm using a different platform So in the past I've shown you colab and I've shown you kaggle And we've also looked at doing stuff on your own computer Not so much training models on your computer, but using the models you trained to create applications PaperSpace is a another website a bit like Kaggle and Google But in particular they have a product called gradient notebooks Which is at least as I speak and things change all the time to check the course website but as I speak in my opinion is is by far the best platform for Running this course and for you know doing experimentation I'll explain why as we go so why haven't I been using the past two weeks? Because I've been waiting for them to build some stuff for us to make it particularly good And they just they just finished so I've been using it all week, and it's totally amazing This is what it looks like So you've got a machine running in the cloud, but the thing that Was very special about it is it's a it's a real it's a real computer. You're using It's not like that kind of weird virtual version of things that Kaggle or colab has So if you whack on this button down here, you'll get a full version of Jupiter lab Or you can switch over to a full version of plastic Jupiter notebooks And I'm actually going to do stuff in Jupiter lab today because it's a pretty good environment for beginners who? Not familiar with the terminal which I know a lot of people in the course are in that situation you can do really everything Kind of graphically there's a file browser so here you can see I've got my pets repo It's got a git repository thing you can pull and push to git and Then you can also Open up a terminal create new notebooks And so forth so what I tend to do with this is I tend to go into a full screen. It's kind of like its own whole IDE And So you can see I've got here my my terminal Here's my notebook They have free GPUs And most importantly there's two good features one is that you can pay I think it's eight or nine dollars a month to get better GPUs and basically as many as you you know as many hours as you want And they have persistent storage so with CoLab if you've played with it you might have noticed It's annoying you have to muck around with saving things to Google Drive and stuff on Kaggle. There isn't really a way of Kind of having a persistent environment Where else some paper space you have you know whatever you save in your storage? It's going to be there the next time you come come back so I'm going to be adding Walkthroughs of all of this functionality so look at so if you're interested in really taking advantage of this check those out Okay, so I think the main thing that I wanted you to take away from lesson two isn't necessarily all the details of how do you use a particular platform to train models and Deploy them into applications through through JavaScript or online platforms But the key thing I wanted you to understand was the concept there's really two pieces There's the training piece and at the end of the training piece you end up with this model dot pickle file right And once you've got that That's now a thing where you feed it inputs, and it's spits out outputs Based on that model that you trained and then so you don't need You know because that happens pretty fast you generally don't need a GPU once you've got that trained And so then there's a separate step Which is deploying so I'll show you how I trained my pet classifier So you can see I've got two IPython notebooks One is app which is the one that's going to be doing the inference and production one is the one where I train the model So this first bit I'm going to skip over because you've seen it before I create my image data loaders Check that my data looks okay with show batch Train a resnet 34 and I get 7% accuracy So that's pretty good But check this out there's a link here To a notebook I created actually most of the work was done by Ross Whiteman Where we can try to improve this by finding a better architecture There are I think at the moment in the pytorch image models libraries over 500 Architectures, and we'll be learning over the course You know what they are how they differ, but you know broadly speaking. They're all mathematical functions, you know which are basically matrix multiplications and And these these non-linearities such as Relu's that we talk about today So most of the time those details don't matter what we care about is three things how fast are they? How much memory do they use and how accurate are they and? So what I've done here with Ross is we've grabbed all of the models from pytorch image Image models and you can see all the code. We've got is very very little code To create this this plot Now my screen resolution is a bit there we go let's do that and so on this plot On the x-axis we've got seconds per sample so how fast is it so? To the left is better is faster and on the right is how accurate is it so how how accurate was it on? Image net in particular and so generally speaking you want things that are up towards the top and left Now we've been mainly working with resnet and you can see down here Here's resnet 18 now resnet 18 is is a particularly small and fast version for prototyping we often use resnet 34 which is this one here And you can see this kind of like classic model that's very widely used actually nowadays isn't the state-of-the-art anymore So we can start to look up at these ones up here and find out some of these better models the ones that seem to be the most accurate and fast for these levitt models So I tried them out on my pets, and I found that they didn't work particularly well, so I thought okay Let's try something else out so next up. I tried these convexed models and This one in here was particularly interesting. It's kind of like super high accuracy. It's the you know if you want 0.001 seconds inference time it's the most accurate so I tried that so how do we try that? All we do is I can say So the high-touch image models is in the Tim module So at the very start I imported that And we can say list models and pass in a Glob a match and so this is going to show all the convex models and Here I can find the ones that I just saw and all I need to do is when I create the vision learner I just put the name of the model in as a string Okay, so you'll see earlier This one is not a string. That's because it's a model that fast AI provides the library Fast AI only provides a pretty small number So if you install Tim's you need to pip install Tim or conda install Tim You'll get hundreds more and you put that in a string So if I now train that the time for these epochs goes from 20 seconds to 27 seconds, so it is a little bit slower but the accuracy goes from 7.2 percent Down to 5.5 percent. So, you know, that's a pretty big relative difference 7.2 divided by 5.5. Yeah, it's about a 30% improvement. So that's pretty fantastic and you know, it's It's been a few years honestly Since we've seen anything Really beat resnet that's that's widely available and usable on regular GPUs So this is this is a big step and so this is a you know, there's a few architectures nowadays that really are Probably better choices a lot of the time and these cons so if you are not sure what to use Try these conv next architectures You might wonder what the names are about. Obviously Tiny is more large. Etc is how big is the model? So that'll be how much memory is it going to take up? How fast is it? and Then these ones here that say in 22 FT 1k These ones have been trained on more data. So image net there's two different image net data sets There's one that's got a thousand categories of pictures and there's another one that's got 22,000 categories of pictures So this is trained on the one with 22,000 categories of pictures So these are generally going to be more accurate on kind of standard photos of natural objects Okay, so from there I exported my model and that's the end okay, so now I've trained my model and I'm all done You know other things you could do obviously is add more epochs for example Add image augmentation. There's various things you can do, but you know, I found this this is actually pretty Pretty hard to beat this by much If any of you find you can do better, I'd love to hear about it So then to turn that into an application. I just did the same thing that we saw last week, which was to Load the learner As is something I did want to show you At the learner once we load it and call predict spits out a list of 37 numbers That's because there are 37 breeds of dog and cat. So these are the probability of each of those breeds What order they are they in? That's an important question The answer is that fast AI always stores this information about categories This is a category in this case of dog or cat breed in something called the vocab object and it's inside the data loaders So we can grab those categories and that's just a list of strings just tells us the order So if we now zip together the categories and the probabilities will get back a dictionary that tells you Well like so So here's that list of categories and here's the probability of each one and This was a basset hound so they can see yep almost certainly a basset hound So From there just like last week we can go and create our interface and then and then launch it And there we go, okay, so What did we just do really? What is this magic? model dot pickle file So we can take a look at the model dot pickle file It's an object type called a learner and a learner has two main things in it The first is the list of pre-processing steps that you did to turn your images into things of the model and That's basically This information here So it's your data blocks or your image data loaders or whatever and then the second thing most importantly is the trained model and So you can actually grab the trained model by just grabbing the dot model Attribute so I'm just going to call that M and then if I type M I can look at the model and so here it is Lots of stuff. So what is this stuff? Well, we'll learn about it all over time. But basically what you'll find is It contains lots of layers because this is a deep learning model and you can see it's kind of like a tree That's because lots of the layers themselves consist of layers so there's a whole layer called the Tim body which is most of it and then right at the end there's a second layer called sequential and then the Tim body contains Something called model and it can then it contains something called stem and something called stages and then stages contain zero one-two, etc So what is all this stuff? Well, let's take a look at one of them So to take a look at one of them. There's a really convenient method in pytorch called get sub module where we can pass in a kind of a dotted string Navigating through this hierarchy. So zero model stem one goes zero model stem one So this is going to return this layer norm 2d thing. So what is this layer norm 2d thing? Well, the key thing is It's got some code is the mathematical function that we talked about and then the other thing that we learned about is it has Parameters so we can list its parameters and look at this. It's just lots and lots and lots of numbers Let's Grab another example. We could have a look at zero dot model dot stages dot zero to blocks dot one dot MLP dot FC one and parameters another big bunch of numbers So what's going on here? What are these numbers and where it is did they come from? And how come these numbers can figure out whether something is a basset hound or not? Okay, so to Answer that question we're going to have a look at a Kaggle notebook How does a neural network really work I've got a local version of it here, which I'm going to take you through and the basic idea is Machine learning models are things that fit Functions to data so we start out with a very very flexible in fact an infinitely flexible as we've discussed function a neural network and We get it to do a particular thing Which is to recognize the patterns in the data examples we give it So let's do a much simpler example Than a neural network. Let's do a quadratic So let's create a function f which is 3x squared plus 2x Plus 1 ok so it's a quadratic with coefficients 3 2 and 1 So we can plot that function f and give it a title If you haven't seen this before things between dollar signs is what's called latech It's basically how we can create kind of typeset mathematical equations Okay, so let's run that and So here you can see the function here you can see the title I passed it and here is our quadratic Okay, so what we're going to do is we're going to Imagine that we don't know that's the true Mathematical function we're trying to find as it's obviously much simpler than the function that figures out whether an image is a Basset hound or not that we're just going to start super simple So this is the real function and we're going to try to recreate it from some data Now it's going to be very helpful if we have an easier way of creating different quadratics So I've defined a kind of a general form of a quadratic here If the with coefficients a B and C and at some particular point X it's going to be a x squared plus B X plus C And so let's test that Okay, so that's a for X equals one point five that's three X squared plus two X plus one Which is the quadratic we were did before? Now we're going to want to create lots of different quadratics to test them out and find out which one's best So this is a Somewhat advanced but very very helpful feature of Python that's worth mining if you're not familiar with it And it's used in a lot of programming languages It's called a partial application of a function basically. I want this exact function but I want to fix the values of a B and C to pick a particular quadratic and the way you fix the values of the function is you call this thing in Python called partial and you pass in the function and Then you pass in the values that you want to fix so for example If I now say make a quadratic three two one that's going to create a quadratic equation with coefficients three two and one and You can see if I then pass in so that's now F if I pass in one point five I get the exact same value I did before Okay, so we've now got an ability to create any quadratic Equation we want by passing in the parameters of the coefficients of the quadratic That gives us a function that we can then just call as just like any normal function So that only needs one thing now, which is the value of X because the other three a B and C are now fixed So if we plot that function We'll get exactly the same shape because it's the same coefficients Okay so Now I'm going to show an example of of some data some data that Matches the shape of this function, but in real life data is never exactly going to match the shape of a function It's going to have some noise. So here's a couple of Functions to add some noise So you can see I've still got the basic functional form here, but this data is a bit dotted around it The level to which you look at how I implemented these is entirely up to you It's not like super necessary, but it's all stuff which you know the kind of things we use quite a lot So this is to create normally distributed random numbers This is how we set the seed so that each time I run this I'm going to get the same random numbers This one is actually particularly helpful this creates a tensor so in this case a vector that goes from negative 2 to 2 in Equal steps and there's 20 of them. That's why there's 20 steps along here So then my y values is just f of X with This amount of noise added Okay, so as I say the details of that don't matter too much the main thing to know is we've got some Random data now and so this is the idea is now we're going to try to reconstruct the original quadratic equation find one which matches this data So how would we do that? Well what we can do is we can create a function called plot quadratic That first of all plots our data as a scatter plot and then it plots a function which is a quadratic The quadratic we pass in Now there's a very helpful thing for experimenting In Jupiter notebooks, which is the at interact Function if you add it on top of a function That it gives you these nice little sliders so here's an example of quadratic with coefficients 1.5 1.5 1.5 And it doesn't fit particularly well So how would we try to make this fit better? Well, I think what I'd do is I take the first slider and I would try moving it to the left and see if it looks better or worse that Looks worse to me. I think it needs to be more curvy. So let's try the other way Yeah, that doesn't look bad let's do the same thing for the next slider have it this way No, I think that's worse. Let's try the other way Okay, final slider try this way No, it's worse this way So you can see what we can do we can basically pick each of the coefficients One at a time try increasing a middle bit See if that improves it try decreasing it a little bit So if that improves it find the direction that improves it and then slide it in that direction a little bit And then when we're done, we can go back to the first one and see if We can make it any better Now we've done that And actually you can see that's not bad because I know the answers meant to be three two one, so they're pretty close And I wasn't shooting her promise That's basically What we're going to do that's basically how those parameters are created, but we obviously don't have time because the You know big fancy models have Often hundreds of millions of parameters. We don't have time to try a hundred hundred million sliders So we did something better Well, the first step is we need a better idea of like when I move it is it getting better or is it getting worse? so if you remember back to Arthur Samuels Description of machine learning that we learn about in chapter one of the book and in lesson one We need some Something we can measure which is a number that tells us how good is their model And if we had that then as we move these sliders, we could check to see whether it's getting better or worse So this is called a loss function. So there's lots of different loss functions you can pick but perhaps the most simple and common is Mean squared error which is going to be so it's going to get in our predictions and it's got the actuals And we're going to go predictions minus actuals squared and take them in so that's mean squared error so If I now rerun the exact same thing I had before but this time I'm going to calculate the loss the MSE between the values that we predict f of X Remember where F is the quadratic we created and the actuals why and this time I'm going to add a title to our function Which is the loss? So now Let's do this more rigorously We're starting at a mean squared error of 11.46. So let's try moving this to the left and see if it gets better No, wait, so we're over to the right All right, sorry around there, okay now let's try this one Okay, best when I go to the right Okay, what about C 3.91 getting worse so I keep going Sorry about that and so now we can repeat that process right so we've we've had each of a B and C move a little bit Let's go back to a can I get any better than 3.28? Let's try moving left Yeah, that was a bit better and for B. Let's try moving left worse right was better and Have it finally see move to the right. Oh Definitely better There we go Okay, so That's a more rigorous approach It's still manual but at least we can like we don't have to rely on us to kind of recognize. Does it look better or worse? So finally we're going to automate this so the key thing we need to know is for each parameter When we move it up Does the loss get better or when we move it down does the loss get better? What approach would be to try it right we could manually increase the parameter a bit and see if the loss improves and vice versa But there's a much faster way and the much faster way is to calculate its derivative So if you've forgotten what a derivative is, no problem. There's lots of tutorials out there You could go to Khan Academy or something like that But in short the derivative is what I just said the derivative is a function that tells you if you increase The input does the output increase or decrease and by how much so that's called the slope or the gradient Now the good news is Pie torch can automatically calculate that for you. So if you went through Horrifying months of learning derivative rules in year 11 and worried you're going to have to remember them all again. Don't worry you don't You don't have to calculate any of this yourself. It's all done for you. Watch this So the first thing to do is we need a function that takes the coefficients of the quadratic a B and C as inputs I Put them all on a list. You'll see why in a moment. I kind of call them parameters We create a quadratic passing in those parameters a B and C This star on the front is a very very common thing in Python Basically, it takes these parameters and spreads them out to turn them into a B and C and pass each of them to the function So I've now got a quadratic with those coefficients and And then we return the mean squared error of our predictions against our actions So this is a function that's going to take the coefficients of a quadratic and return the loss So let's try it Okay, so if we start with a B and C of 1.5 we get a mean squared error of 11 point four six It looks a bit weird it says it's a tensor So don't worry about that too much in short in pi torch Everything is a tensor a tensor just means that you don't it doesn't just work with numbers It also works with lists or vectors of numbers. That's got a 1d tensor Rectangles of numbers so tables of numbers. It's got a 2d tensor Layers of tables of numbers. That's got a 3d tensor and so forth. So in this case, this is a single number But it's still a tensor that means it's just wrapped up in the pi torch Machinery that allows it to do things like calculate derivatives But it's still just the number 11 point four six All right So what I'm going to do is I've got to create my parameters a B and C and I'm going to put them all in a single 1d tensor now 1d tensor is also known as a rank one tensor So this is a rank one tensor and it contains the list of numbers one point five one point five one point five and Then I've got to tell pi torch That I want you to calculate the gradient For these numbers whenever we use them in a calculation and the way we do that is we just say requires bread So here is our Tensor it contains one point five three times and it also tells us it's we flagged it to say please calculate gradients for this Particular tensor when we use it in calculations So let's now use it in the calculation we're going to pass it to that quad MSC that's the function We just created that gets the MSC a mean squared error for a set of coefficients And not surprisingly, it's the same number we saw before eleven point four six. Okay Not very exciting, but there is one thing that's very exciting It is added an extra thing to the end called grad function And this is the thing that tells us that if we wanted to high torch knows how to create calculate the gradients For our inputs and to tell pi torches, please go ahead and do that calculation You call backward On the result of your loss function now when I run it nothing happens It doesn't look like nothing happens. But what does happen is it's just added an attribute called grad Which is the gradient to our inputs ABC. So if we run this cell This tells me that if I increase a the loss will go down If I increase B the loss will go down a bit less And if I increase C the loss will go down Now we want the loss to go down Right. So that means we should increase a B and C Well, how much by well given that a is Says if you increase a even a little bit the loss improves a lot that suggests We're a long way away from the right answer. So we should probably increase this one a lot This one the second most and this one the third most. Okay, so this is saying when I increase This parameter the loss decreases So in other words, we want to adjust our parameters a B and C by the negative of these we want to increase increase increase So we can do that By saying okay, let's take our ABC minus equals so that means equals ABC minus the gradient But we're just going to like decrease it a bit. We don't want to jump too far. Okay, so just we're just going to go a small distance So we're going to just going to somewhat arbitrarily pick point. Oh one So that is now going to create a new set of parameters But you're going to be a little bit bigger than before because we subtracted negative numbers And we can now calculate the loss again So remember before It was eleven point four six So hopefully it's going to get better Yes, I did 10.11 There's one extra line of code which we didn't mention which is with torch dot no grad Remember earlier on we said that the parameter ABC requires grad and that means pytorch will automatically calculate Its derivative when it's used in a in a function Here it's being used in a function, but we don't want the derivative of this. This is not our loss Right. This is us updating the gradients. So this is basically The standard inner part of a pytorch loop and every neural net deep learning Pretty much every machine learning model at least of this style that you'll build basically looks like this If you look deep inside fast AI source code, you'll see something that basically looks like this So we could automate that right so let's just take those steps which is we're going to Calculate let's go back to here. We're going to calculate the mean squared error for our quadratic Call backward and then subtract the gradient times a small number from the gradient Let's do it five times So so far we're up to a loss of 10.1 So we're going to calculate our loss call dot backward to calculate the gradients and then with no grad subtract the gradients times a small number and print how we're going and There we go the loss keeps improving So we now have Some coefficients And There they are 3.2 1.9 2.0 so they're definitely heading in the right direction so That's basically how we do it's called optimization Okay, so you'll hear a lot in deep learning about optimizers. This is the most basic kind of Optimizer, but they're all built on this principle Of course, it's called gradient descent and you can see why it's called gradient descent. We calculate the gradients and Then do a descent which is we're trying to decrease the loss So Believe it or not. That's that's The entire foundations of how we create those parameters. So we need one more piece Which is what is the mathematical function that we're finding parameters for? We can't just use Quadratics right because it's pretty unlikely that the relationship between parameters and whether a pixel is part of a basset hound Is a quadratic it's going to be something much more complicated No problem It turns out that We can create an infinitely flexible function from this one tiny thing This is called a rectified linear, you know The first piece I'm sure you will recognize It's a linear function. We've got our output y our input x and coefficients m and b this is even simpler than our quadratic and This is a line And torch dot clip is a function that takes that output y and if it's greater than that number It turns it into that number. So in other words This is going to take anything that's negative and make it zero So this function is going to do two things Calculate the output of a line and if it is bigger than smaller than zero, it'll make it zero So that's rectified linear So let's use partial To take that function and set the m and b to 1 and 1 so this is now going to be this function here will be y equals x plus 1 followed by this torch dot clip and Here's the shape okay as we'd expect it's a line Until it gets under zero when it comes what's still a line. It's a becomes a horizontal line So we can now do the same thing we can take this plot function and make it interactive using interact and And we can see what happens when we change its two parameters m and b So we're now plotting the rectified linear and fixing m and b So m is the slope Okay, and B is the is the Intercept or the shift up and down Okay so that's how those Work now. Why is this interesting? Well, it's not interesting of itself but what we could do is we could take this rectified linear function and create a double revenue Which adds up to rectified linear functions together? so there's some slope and 1 b 1 some slack and slope and 2 b 2 we're going to calculate it at some point x and So let's take a look at What that function looks like if we plot it And you can see what happens is we get this downward slope and then a hook and then an upward slope so if I change them one it's going to change the slope of that first bit and B one is going to change its position Okay, and I'm sure you won't be surprised to hear that m2 changes the slope of the second bit and B to Changes that location now This is interesting why Because we don't just have to do a double relu we could add as many relu's together as we want and If we add as many relu's together as we want then we can have an arbitrarily squiggly function and with enough relu's We can match it as close as we want right, so you could imagine incredibly squiggly like I don't know like an audio waveform of me speaking and If I gave you a hundred million relu's to add together you could almost exactly match that Now we want functions that are not just That we've put in 2d we want things that can have more than one input But you can add these together across as many dimensions as you like and so exactly the same thing will give you a relu over surfaces or a relu over 3d 40 50 and so forth and it's the same idea with this incredibly simple foundation You can construct an arbitrarily accurate precise model a Problem is You need some numbers for them you need parameters. Oh No problem. We know how to get parameters We use gradient descent So believe it or not We have just derived deep learning everything from now on is Tweaks to make it faster and make it need less data You know this is this is it Now I remember a few years ago when I said something like this in a class Somebody on the forum was like this reminds me of that thing about how to draw an owl Jeremy's basically saying okay step one draw two circles Step to draw the rest of the owl the Thing I'm I find I have a lot of trouble explaining to students is when it comes to deep learning there's nothing between these two steps when you have values getting added together and gradient descent to optimize the parameters and samples of inputs and outputs that you want The computer draws the owl right? That's that's that's it So we're going to learn about all these other tweaks and they're all very important But when you come down to like trying to understand something in deep learning just try to keep coming back to remind yourself Of what it's doing Which it's using gradient descent to set some parameters to make a wiggly function Which is basically the addition of lots of rectified linear units or something very similar to that match your data Okay, so we've got some questions on the forum Okay, so question from Zakiya with six up votes so for those of you Watching the video what we do in the lesson is we want to make sure that the Questions that you hear answered are the ones that people really care about So we pick the ones which get the most up votes this question is Is there perhaps a way to try out all the different models and automatically find the best-performing one? Yes, absolutely you can do that so If we go back to our trading script remember there's this thing called best models and It's a list of strings so you can easily add a for loop around this So it basically goes, you know for Architecture in team dot list models and you could do the whole lot which would be like that and then you could Do that and away you go It's going to take a long time for 500 and something models So generally speaking like I've I've never done anything like that myself I would rather look at a picture like this and say like okay. Where am I in? the vast majority of the time this is something this would be the biggest Eric and number one mistake of Beginners I see is that they jump to these models From the start of a new project at the start of a new project. I pretty much only use resident 18 Because I want to spend all of my time Trying things out and I try different data augmentation. I'm going to try different ways of cleaning the data I'm going to try You know Different external data I can bring in and so I want to be trying lots of things and I want to be able to try it As fast as possible, right? So Trying better architectures is the very last thing that I do and What I do is once I've spent all this time and I've got to the point where I've got okay I've got my resident 18 or maybe you know resident 34 because it's nearly as fast And I'm like, okay. Well, how accurate is it? How fast is it? Do I need it more accurate for what I'm doing? Do I need it faster for what I'm doing? Could I accept some trade-off to make it a bit slower to make more accurate? And so then I'll have a look and I'll say okay. Well, I kind of need to be somewhere around point. No one seconds And so I try a few of these So that would be how I would think about that Okay next question from the forum Is around how do I know if I have enough data? What are some signs that indicate my problem needs more data? I think it's pretty similar to the architecture question. So you've got some amount of data Presumably you've you know, you've started using all the data that you have access to you've built your model. You've done your best Is it good enough? Do you have the accuracy that you need for whatever it is you're doing? You can't know until you've trained the model but as you've seen it only takes a few minutes to train a quick model So my very strong opinion is that the vast majority of Projects I see in industry wait far too long before they train their first model You know in my opinion you want to train your first model on day one with whatever CSB files or whatever that you can hack together and you might be surprised that None of the fancy stuff you're thinking of doing is necessary because you already have a good enough accuracy for what you need Or you might find quite the opposite you might find that oh my god with we're basically getting no accuracy at all Maybe it's impossible These are things you want to know at the start not at the end We'll learn lots of techniques both in this part of the course and in part two about ways to really get the most out of your data In particular, there's a reasonably recent technique called semi-supervised learning Which actually lets you get dramatically more out of your data and we've also started talking already about data augmentation Which is a classic technique you can use so you generally speaking It depends how expensive is it going to be to get the most out of your data? And also what do you mean when you say get more data? Do you mean more labeled data? Often it's easy to get lots of inputs and hard to get lots of outputs For example in medical imaging where I've spent a lot of time It's generally super easy to jump into the radiology archive and grab more CT scans But it's might be very difficult and expensive to You know draw segmentation masks and Pixel boundaries and so forth on them So often you can get more You know in this case images or text or whatever and maybe it's harder to get labels And again there's a lot of stuff you can do using things like we'll discuss semi-supervised learning To actually take advantage of unlabeled data as well Okay final question here in the quadratic example where we calculated the initial derivatives for a B and C We got values of minus 10.8 minus 2.4. Etc. What unit of these expressed in? Why don't we adjust our parameters by these values themselves? So I guess the question here is why are we multiplying it by a small number? Which in this case is 0.01? Okay, let's take those two parts of the question What's the unit here? The unit is for each increase in X of 1 How much does what I say in for each increase in in a of 1 so if I increase a from? In this case we have 1.5. So if we increase from 1.5 to 2.5 What would happen to the loss and the answer is it would go down by 10.9887 Now that's not exactly right because it's kind of like It's kind of like in an infinitely small space right because it's kind of like in a small space It's kind of like in an infinitely small space right because actually it's going to be curved right? But if it if it stays it stayed at that slope. That's what would happen So if we increased B by 1 the loss would decrease if it stayed constant You know if the slope stayed the same the loss would decrease by minus 2.122 Okay, so why would we not just change it directly by these numbers? Well the reason is The reason is that if we Have some function that we're fitting And there's some kind of interesting theory that says that once you get close enough to the the optimal value All functions look like quadratics anyway, right so we can kind of safely draw it in this kind of shape Because this is what they end up looking like if you get close enough And we're like let's say we're way out Over here. Okay, so we were measuring I'll use my daughter's favorite pens and I sparkly ones so we're measuring the slope here There's a very steep slope So that seems to suggest we should jump a really long way So we jump a really long way And what happened well we jumped way too far and the reason is that that slope decreased As we move along and so that's generally what's going to happen right particularly as you approach the optimal is generally the slopes going to decrease So that's why we multiply the gradient by a small number And that small number is a very very very important number it has a special name It's called the learning rate And this is an example of a hyper parameter it's not a parameter it's not one of the actual coefficients of your function But it's a parameter you use to calculate the parameters Pretty better right it's a hyper parameter and so it's something you have to pick Now we haven't picked any yet In any of the stuff we've done that I remember and that's because fast AI generally picks reasonable defaults For most things but later in the course we will learn about how to try and find really good learning rates And you will find sometimes you need to actually spend some time finding a good learning rate You could probably understand the intuition here if you pick a learning rate that's too big you'll jump too far And so you'll end up way over here and then you will try to then jump back again and you'll jump too far the other way And you'll actually diverge and so if you ever see when your model's training it's getting worse and worse Probably means your learning rates too big What would happen on the other hand if you pick a learning rate that's too small Then you're going to take tiny steps and of course the flatter it gets the smaller the steps are going to get And so you're going to get very very bored So finding the right learning rate is a compromise between the speed at which you find the answer And the possibility that you're actually going to shoot past it and get worse and worse Okay so one of the bits of feedback I got quite a lot in the survey is that people want a break half way through Which I think is a good idea so I think now is a good time to have a break So let's come back in 10 minutes at 25 past 7 Okay hope you had a good rest have a good break I should say So I want to now show you a really really important mathematical computational trick Which is we want to do a whole bunch of relu's Alright so we're going to be wanting to do a whole lot of mx plus b's And we don't just want to do mx plus b we're going to want to have like lots of variables So for example every single pixel of an image would be a separate variable So we're going to multiply every single one of those times some coefficient And then add them all together and then do the crop the relu And then we're going to do it a second time with a second bunch of parameters And then a third time and a fourth time and a fifth time It's going to be pretty inconvenient to write out a hundred million relu's But so happens there's a mathematical single mathematical operation that does all of those things for us Except for the final replace negatives with zeros and it's called matrix multiplication I expect everybody at some point did matrix multiplication at high school I suspect also a lot of you have forgotten how it works When people talk about linear algebra in deep learning They give the impression you need years of graduate school study to learn all this linear algebra You don't actually all you need almost all the time is matrix multiplication And it couldn't be simpler. I'm going to show you a couple of different ways The first is there's a really cool site called matrix multiplication xyz You can put in any matrix you want So I'm going to put in this one So this matrix is saying I've got three rows of data with three variables So maybe they're tiny images with three pixels And the value of the first one is 1 2 1 The second is 0 1 1 and the third is 2 3 1 So those are our three rows of data These are our three sets of coefficients So we've got a b and c in our data So I guess you'd call it x1 x2 and x3 And then here's our first set of coefficients a b and c 2 6 and 1 And then our second set is 5 7 and 8 So here's what happens when we do matrix multiplication That second this matrix here of coefficients gets flipped around And we do this is the multiplications and additions that I mentioned right So multiply add multiply add multiply add So that's going to give you the first number Because that is the left hand column of the second matrix times the first row So that gives you the top left result So the next one is going to give us two results right So we've got now the right hand one with the top row And the left hand one with the second row Keep going down Keep going down And that's it That's what matrix multiplication is It's modifying things together and adding them up So there'd be one more step to do to make this a layer of a neural network Which is if this had any negatives we would replace them with zeros That's why matrix multiplication is the critical foundational mathematical operation In basically all of deep learning So the GPUs that we use The thing that they are good at is this matrix multiplication They have special cores called tensor cores Which can basically only do one thing Which is to multiply together two four by four matrices And then they do that lots of times to bigger matrices So I'm going to show you an example of this We're actually going to build a complete machine learning model on real data In a spreadsheet So fast AI has become kind of famous for a number of things And one of them is using spreadsheets to create deep learning models We haven't done it for a couple of years so I'm pretty pumped to show this to you What I've done is I went over to Cable Where there's a competition I actually helped create many years ago called Titanic And it's like an ongoing competition So 14,000 people have entered it so far It's just a competition for a bit of fun There's no end date And the data for it is the data about Who survived and who didn't from the real Titanic disaster And so I clicked here on the download button To grab it on my computer That gave me a CSP Which I opened up in Excel The first thing I did then was I just removed a few columns that Clearly were not going to be important The name of the passengers, the passenger ID Just to try to make it a bit simpler And so I've ended up with Each row of this is one passenger The first column is the dependent variable The dependent variable is the thing we're trying to predict Did they survive? And the remaining are some information such as What class of the boat? First, second or third class? Their sex, their age How many siblings in the family? P. Arch, I think his parents or something? So you should always look for a data dictionary To find out what's what Number of parents and children What was their fare? And which of the three cities did they embark on? So there's our data Now when I first grabbed it I noticed that There were some people with no age Now there's all kinds of things we could do for that But for this purpose I just decided to remove them And I found the same thing for embarked I removed the blanks as well But that left me with nearly all of the data So then I've put that over here Here's our data with those rows removed And Okay, so these are the columns that came directly from Kaggle So basically what we now want to do Is we want to multiply each of these by a coefficient How do you multiply the word male by a coefficient? And how do you multiply S by a coefficient? You can't. So I converted all of these to numbers Male and female are very easy I created a column called is male And as you can see there's just an if statement That says if sex is male then it's one, otherwise it's zero And we can do something very similar for embarked We can have one column called did they embark in Southampton? Same deal And another column for did they, what's it called, Churberg? Did they embark in Churberg? And now P class is one, two or three, which is a number But it's not really a continuous measurement of something There isn't one or two or three things They're different levels So I decided to turn those into similar things, into these binary These are called binary categorical variables So are they first class? And are they second class? Okay, so that's all that The other thing that I was thinking, well, you know, then I kind of tried it And checked out what happened And what happened was the people with, so I created some random numbers So to create the random numbers I just went equals rand, right? And I copied those to the right And then I just went copy and I went paste values So that gave me some random numbers Before I said, oh, a, b, and c, let's just start them at 1.5, 1.5, 1.5 What we do in real life is we start our parameters at random numbers That are a bit more or a bit less than zero So these are random numbers Actually, sorry, I slightly lied I didn't use rand, I used rand minus 0.5 And that way I got small numbers that were on either side of zero So then when I took each of these And I multiplied them by Our fairs and ages and so forth What happened was that these numbers here Are way bigger than, you know, these numbers here And so in the end all that mattered was what was their fair Because they were just bigger than everything else So I wanted everything to basically go from 0 to 1 These numbers were too big So what I did up here is I just grabbed the maximum of this column The maximum of all the fairs is 512 And so then, actually I'll do age first I did maximum of age, because similar thing, right? There's 80 year olds and there's 2 year olds And so then over here I just did Okay, well what's their age divided by the maximum? And so that way all of these are between 0 and 1 Just like all of these are between 0 and 1 So that's how I fixed, this is called normalizing the data Now, we haven't done any of these things when we've done stuff with FastAI That's because FastAI does all of these things for you And we'll learn about how, right? But all these things are being done behind the scenes For fair, I did something a bit more Which is I noticed there's lots of very small fairs And there's also a few very big fairs So like $70 and then $7, $7 Generally speaking, when you have lots of really big numbers and a few small ones So generally speaking, when you've got a few really big numbers and lots of really small numbers This is really common with money Because money kind of follows this relationship where a few people have lots of it And they spend huge amounts of it and most people don't have heaps If you take the log of something that has that kind of extreme distribution You end up with something that's much more evenly distributed So I've added this here called log fair, as you can see And these are all around 1, which isn't bad I could have normalized that as well, but I was too lazy, I didn't bother Because it seemed okay So at this point you can now see that if we start from here All of these are all around the same kind of level So none of these columns are going to saturate the others So now I've got my coefficients, which are just as I said, they're just random And so now I need to basically calculate AX1 plus BX2 plus CX3 plus blah blah blah blah And so to do that, you can use some product in Excel I could have typed it out by hand, it could be very boring But some product is just going to multiply each of these This one will be multiplied by this one, this one will be multiplied by this one, so forth And then they get all added together Now one thing, if you're eagle-eyed, you might be wondering Is in a linear equation, we have Y equals MX plus B At the end there's this constant term And I do not have any constant term I've got something here called const, but I don't have any plus at the end How's that working? Well there's a nice trick that we pretty much always use in machine learning Which is to add a column of data just containing the number one every time If you have a column of data containing the number one every time Then that parameter becomes your constant term So you don't have to have a special constant term And so it makes our code a little bit simpler when you do it that way It's just a trick, but everybody does it So this is now the result of our linear model I'm not even going to do value, I'm just going to do the plain regression Now if you've done regression before, you might have learned about it as something you can solve with various metrics things But in fact, you can solve a regression using gradient descent So I've just gone ahead and created a loss for each row And so the loss is going to be equal to our prediction Minus whether they survived squared So this is going to be our squared error And there they all are, our squared errors And so here I've just summed them up I could have taken the mean, I guess that would have been a bit easier to think about But sum is going to give us the same result So here's our loss And so now we need to optimize that using gradient descent So Microsoft Excel has a gradient descent optimizer in it called solver So I'll click solver And it'll say, okay, what are you trying to optimize? It's this one here And I'm going to do it by changing these cells here And I'm trying to minimize it And so we're starting a loss of 55.78 Actually, let's change it to mean as well Is it called mean or average? Probably average Alright, so start at 1.03 So optimize that And there we go So it's gone from 1.03 to 0.1 And so we can check the predictions So the first one, it predicted exactly correctly It was, they didn't survive and we predict it wouldn't survive Ditto for this one It's very close And you can start to see, so this one, you can start to see a few issues here Which is like sometimes it's predicting less than one, sorry, less than zero And sometimes it's predicting more than one Wouldn't it be cool if we had some way of constraining it to between zero and one? And that's an example of some of the things we're going to learn about that make this stuff work a little bit better But you can see it's doing an okay job So this is not deep learning, this is not a neural net yet, this is just a regression So to make it into a neural net, we need to do it multiple times So I'm just going to do it twice So now rather than one set of coefficients, I've got two sets And again, I just put in random numbers Other than that, all the data is the same And so now I'm going to have my sum product again So the first sum product is with my first set of coefficients And my second sum product is with my second set of coefficients So I'm just calling them linear one and linear two Now there's no point adding those up together Because if you add up two linear functions together, you get another linear function We want to get all those wiggles, right? So that's why we have to do our ReLU So in Microsoft Excel, ReLU looks like this If the number is less than zero, use zero, otherwise use the number So that's how we're going to replace the negatives with zeros And then finally, if you remember from our spreadsheet, we have to add them together So we add the ReLUs together So that's going to be our prediction And then our loss is the same as the other sheet It's just survived minus prediction squared And let's change that to main Not main, average Okay, so let's try solving that Optimize, ah1 And this time we're changing all of those So this is using gradient descent Excel Solve is not the fastest anymore, but it gets the job done Okay, let's see how we went So 0.08 for our deep learning model versus 0.1 for our regression So it's a bit better So there you go So we've now created our first deep learning neural network from scratch And we did it in Microsoft Excel, everybody's favorite artificial intelligence tool So that was a bit slow and painful It would be a bit faster and easier if we used matrix multiplication So let's finally do that So this next one is going to be exactly the same as the last one, but with matrix multiplication So all that data looks the same You'll notice the key difference now is our parameters have been transposed So before I had the parameters matching the data in terms of being in columns For matrix multiplication, the expectation is the way matrix multiplication works is that you have to transpose this, so it goes The X and Y is kind of the opposite way around, the rows and columns are the opposite way around Other than that, it's the same, I've got the same I just copied and pasted the random numbers, so we had exactly the same starting point And so now, our entire, this entire thing here is a single function which is matrix multiply all of this by all of this And so when I run that, it fills in exactly the same numbers Make this average And so now we can optimize that So we can do that Like that, a minimum By changing these Solve It should get the same number Oh wait, wasn't it? Yep, and we do Okay, so that's just another way of doing the same thing You can see that matrix multiplication, it takes like a surprisingly long time, at least for me to get an intuitive feel For matrix multiplication, it's like a single mathematical operation So I still find it helpful to kind of remind myself as just doing these sum products and additions Okay, so that is That is a deep learning neural network in Microsoft Excel And the Titanic Kaggle competition, by the way is a pretty fun learning competition, if you haven't done much machine learning before then it's certainly worth trying out just to get the feel for how these all get put together So the chapter of the book that this lesson goes with is chapter 4 And chapter 4 of the book is the chapter where we lose the most people because it's, to be honest, it's hard But part of the reason it's hard is I couldn't put this into a book So we're teaching it a very different way in the course to what's in the book And you can use the two together, but if you've tried to read the book and been a bit disheartened try following through the spreadsheet instead Maybe try creating, like if you use Numbers or Google Sheets or something, you could try to create your own version of it on whatever spreadsheet platform you prefer Or you could try to do it yourself from scratch in Python, you know, if you want to really test yourself So there's some suggestions Okay, question from Victor Guerra In the Excel exercise when Jeremy is doing some feature engineering he comes up with two new columns, pclass1 and pclass2 That is true pclass1 and pclass2 Why is there no pclass3 column? Is it because if pclass1 is 0 and pclass2 is 0, then pclass3 must be 1 So in a way, two columns are enough to encode the input of the original column Yes, that's exactly the reason So there's no need to tell the computer about things it can kind of figure out for itself So when you create, these are called dummy variables So when you create dummy variables for a categorical variable with three levels like this one, you need two dummy variables So in general, a categorical variable with n levels needs n-1 columns Thanks for the good question So what we're going to be doing in our next lesson is looking at natural language processing So far we've looked at some computer vision and just now we've looked at some what we call tabular data, so kind of spreadsheet type data Next up, we're going to be looking at natural language processing So I'll give you a taste of it So you might want to open up the Getting Started with NLP for Absolute Beginners notebook So here's the Getting Started with NLP for Absolute Beginners notebook I will say as a notebook author, I may sound a bit lame but I always say when people have uploaded it, it always makes me really happy and it also helps other people find it So remember to upvote these notebooks or any other notebooks you like I also always read all the comments So if you want to ask any questions or make any comments, I enjoy those as well So natural language processing is about rather than taking, for example, image data and making predictions we take text data That text data most of the time is in the form of prose, so plain English text So English is the most common language used for NLP but there's NLP models in dozens of different languages nowadays and if you're a non-English speaker, you'll find that for many languages there's less resources in non-English languages and there's a great opportunity to provide NLP resources in your language This has actually been one of the things that the Fast.ai community has been fantastic at in the global community is building NLP resources For example, the first Fast.ai NLP resource was created by a student from the very first Fast.ai course The Indic languages, some of the best resources have come out of Fast.ai alumni and so forth So that's a particularly valuable thing you could look at So if your language is not well represented, that's an opportunity, not a problem So some examples of things you could use NLP for Perhaps the most common and practically useful in my opinion is classification Classification means you take a document Now when I say a document, that could be one or two words, it could be a book, it could be a Wikipedia page So it could be any length We use the word document, it sounds like that's a specific kind of length, but it can be a very short thing or a very long thing We take a document and we try to figure out a category for it Now that can cover many, many different kinds of applications So one common one that we'll look at a bit is sentiment analysis So for example, is this movie review positive or negative? Sentiment analysis is very helpful in things like marketing and product development You know, in big companies there's lots and lots of information coming in about your product It's very nice to be able to quickly sort it out and track metrics from week to week Something like figuring out what author wrote the document would be an example of a classification exercise Because you're trying to put it in a category, in this case is which author It gives a lot of opportunity in legal discovery, there's already some products in this area Where in this case the category is this legal document in scope or out of scope in the court case Just organizing documents, triaging inbound emails So like which part of the organization should it be sent to, is it urgent or not, stuff like that So these are examples of classification What you'll find is when we look at classification tasks in NLP It's going to look very, very similar to images But what we're going to do is we're going to use a different library The library we're going to use is called Hugging Face Transformers rather than Fast AI And there's two reasons for that The main reason why is because I think it's really helpful to see how things are done in more than one library And Hugging Face Transformers, Fast AI has a very layered architecture So you can do things at a very high level with very little code Or you can dig deeper and deeper and deeper, getting more and more fine-grained Hugging Face Transformers doesn't have the same high-level API at all that Fast AI has So you have to do more stuff manually And so at this point of the course, we're going to actually intentionally use a library Which is a little bit less user-friendly in order to see what extra steps you have to go through to use other libraries Having said that, the reason I picked this particular library is it is particularly good It has really good models in it It has a lot of really good techniques in it Not at all surprising because they have hired lots and lots of Fast AI alumni So they have very high-quality people working on it So before the next lesson, if you've got time, take a look at this notebook and take a look at the data The data we're going to be working with is quite interesting It's from a Kaggle competition, which is trying to figure out in patents Whether two concepts are referring to the same thing or not, whether those concepts are represented as English text And when you think about it, that is a classification task Because the document is basically text one, blah, text two, blah And then the category is similar or not similar And in fact, in this case, they actually have scores, it's either got to be basically 0, 0.25, 0.5, 0.75 or 1 How similar is it? But it's basically a classification task when you think of it that way So yeah, you can have a look at the data And next week we're going to go step-by-step through this notebook And we're going to take advantage of that as an opportunity also to talk about the really important topics Of validation sets and metrics, which are two of the most important topics in not just deep learning, but machine learning more generally Alright, thanks everybody, I'll see you next week, bye", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.62, "text": " Hi everybody and welcome to lesson 3 of practical deep learning for coders", "tokens": [2421, 2201, 293, 2928, 281, 6898, 805, 295, 8496, 2452, 2539, 337, 17656, 433], "temperature": 0.0, "avg_logprob": -0.1906223465911055, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.044641345739364624}, {"id": 1, "seek": 0, "start": 5.68, "end": 11.9, "text": " we did a quick survey this week to see how people feel that the course is tracking and", "tokens": [321, 630, 257, 1702, 8984, 341, 1243, 281, 536, 577, 561, 841, 300, 264, 1164, 307, 11603, 293], "temperature": 0.0, "avg_logprob": -0.1906223465911055, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.044641345739364624}, {"id": 2, "seek": 0, "start": 15.040000000000001, "end": 18.38, "text": " Over half of you think it's about right pace and of the rest who aren't", "tokens": [4886, 1922, 295, 291, 519, 309, 311, 466, 558, 11638, 293, 295, 264, 1472, 567, 3212, 380], "temperature": 0.0, "avg_logprob": -0.1906223465911055, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.044641345739364624}, {"id": 3, "seek": 0, "start": 19.16, "end": 22.0, "text": " Some of you think it's a bit slow and some of you think it's a bit sorry", "tokens": [2188, 295, 291, 519, 309, 311, 257, 857, 2964, 293, 512, 295, 291, 519, 309, 311, 257, 857, 2597], "temperature": 0.0, "avg_logprob": -0.1906223465911055, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.044641345739364624}, {"id": 4, "seek": 0, "start": 22.0, "end": 23.96, "text": " I don't think it's a bit slow and some of you think it's a bit fast", "tokens": [286, 500, 380, 519, 309, 311, 257, 857, 2964, 293, 512, 295, 291, 519, 309, 311, 257, 857, 2370], "temperature": 0.0, "avg_logprob": -0.1906223465911055, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.044641345739364624}, {"id": 5, "seek": 0, "start": 23.96, "end": 26.6, "text": " So hopefully we're that's about the best we can do", "tokens": [407, 4696, 321, 434, 300, 311, 466, 264, 1151, 321, 393, 360], "temperature": 0.0, "avg_logprob": -0.1906223465911055, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.044641345739364624}, {"id": 6, "seek": 2660, "start": 26.6, "end": 34.24, "text": " Generally speaking the first two lessons are a little more easy pacing for anybody who's already familiar with the kind of", "tokens": [21082, 4124, 264, 700, 732, 8820, 366, 257, 707, 544, 1858, 43285, 337, 4472, 567, 311, 1217, 4963, 365, 264, 733, 295], "temperature": 0.0, "avg_logprob": -0.237346134185791, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.00010886896779993549}, {"id": 7, "seek": 2660, "start": 35.08, "end": 42.160000000000004, "text": " basic technology pieces and then the later lessons get you know more into kind of some of the foundations and today we're going to", "tokens": [3875, 2899, 3755, 293, 550, 264, 1780, 8820, 483, 291, 458, 544, 666, 733, 295, 512, 295, 264, 22467, 293, 965, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.237346134185791, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.00010886896779993549}, {"id": 8, "seek": 2660, "start": 42.160000000000004, "end": 44.16, "text": " be talking about", "tokens": [312, 1417, 466], "temperature": 0.0, "avg_logprob": -0.237346134185791, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.00010886896779993549}, {"id": 9, "seek": 2660, "start": 44.2, "end": 50.28, "text": " You know things like the matrix multiplications and gradients and calculus and stuff like that", "tokens": [509, 458, 721, 411, 264, 8141, 17596, 763, 293, 2771, 2448, 293, 33400, 293, 1507, 411, 300], "temperature": 0.0, "avg_logprob": -0.237346134185791, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.00010886896779993549}, {"id": 10, "seek": 2660, "start": 50.84, "end": 55.96, "text": " So for those of you who are more mathy and less computer you might find this one more comfortable and", "tokens": [407, 337, 729, 295, 291, 567, 366, 544, 5221, 88, 293, 1570, 3820, 291, 1062, 915, 341, 472, 544, 4619, 293], "temperature": 0.0, "avg_logprob": -0.237346134185791, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.00010886896779993549}, {"id": 11, "seek": 5596, "start": 55.96, "end": 57.96, "text": " vice versa", "tokens": [11964, 25650], "temperature": 0.0, "avg_logprob": -0.1732576264275445, "compression_ratio": 1.5789473684210527, "no_speech_prob": 1.7880234963740804e-06}, {"id": 12, "seek": 5596, "start": 60.4, "end": 62.36, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.1732576264275445, "compression_ratio": 1.5789473684210527, "no_speech_prob": 1.7880234963740804e-06}, {"id": 13, "seek": 5596, "start": 62.36, "end": 64.36, "text": " remember that there is a", "tokens": [1604, 300, 456, 307, 257], "temperature": 0.0, "avg_logprob": -0.1732576264275445, "compression_ratio": 1.5789473684210527, "no_speech_prob": 1.7880234963740804e-06}, {"id": 14, "seek": 5596, "start": 64.6, "end": 69.84, "text": " Official course updates thread where you can see all of the up-to-date info about", "tokens": [38577, 1164, 9205, 7207, 689, 291, 393, 536, 439, 295, 264, 493, 12, 1353, 12, 17393, 13614, 466], "temperature": 0.0, "avg_logprob": -0.1732576264275445, "compression_ratio": 1.5789473684210527, "no_speech_prob": 1.7880234963740804e-06}, {"id": 15, "seek": 5596, "start": 70.4, "end": 73.44, "text": " Everything you need to know and of course the course website", "tokens": [5471, 291, 643, 281, 458, 293, 295, 1164, 264, 1164, 3144], "temperature": 0.0, "avg_logprob": -0.1732576264275445, "compression_ratio": 1.5789473684210527, "no_speech_prob": 1.7880234963740804e-06}, {"id": 16, "seek": 5596, "start": 74.08, "end": 79.64, "text": " As well, so by the time, you know you watch the video of the lesson", "tokens": [1018, 731, 11, 370, 538, 264, 565, 11, 291, 458, 291, 1159, 264, 960, 295, 264, 6898], "temperature": 0.0, "avg_logprob": -0.1732576264275445, "compression_ratio": 1.5789473684210527, "no_speech_prob": 1.7880234963740804e-06}, {"id": 17, "seek": 7964, "start": 79.64, "end": 87.16, "text": " It's pretty likely that if you come across a question or an issue somebody else will have so definitely search the forum and check the facts", "tokens": [467, 311, 1238, 3700, 300, 498, 291, 808, 2108, 257, 1168, 420, 364, 2734, 2618, 1646, 486, 362, 370, 2138, 3164, 264, 17542, 293, 1520, 264, 9130], "temperature": 0.0, "avg_logprob": -0.1411849051406703, "compression_ratio": 1.6861924686192469, "no_speech_prob": 1.3845406101609115e-05}, {"id": 18, "seek": 7964, "start": 88.4, "end": 93.04, "text": " First and then of course feel free to ask a question yourself on the forum if you can't find your answer", "tokens": [2386, 293, 550, 295, 1164, 841, 1737, 281, 1029, 257, 1168, 1803, 322, 264, 17542, 498, 291, 393, 380, 915, 428, 1867], "temperature": 0.0, "avg_logprob": -0.1411849051406703, "compression_ratio": 1.6861924686192469, "no_speech_prob": 1.3845406101609115e-05}, {"id": 19, "seek": 7964, "start": 95.32, "end": 99.88, "text": " One thing I did want to point out which you'll see in the lesson thread and the course website is", "tokens": [1485, 551, 286, 630, 528, 281, 935, 484, 597, 291, 603, 536, 294, 264, 6898, 7207, 293, 264, 1164, 3144, 307], "temperature": 0.0, "avg_logprob": -0.1411849051406703, "compression_ratio": 1.6861924686192469, "no_speech_prob": 1.3845406101609115e-05}, {"id": 20, "seek": 7964, "start": 100.88, "end": 102.88, "text": " There is also a lesson zero", "tokens": [821, 307, 611, 257, 6898, 4018], "temperature": 0.0, "avg_logprob": -0.1411849051406703, "compression_ratio": 1.6861924686192469, "no_speech_prob": 1.3845406101609115e-05}, {"id": 21, "seek": 7964, "start": 103.56, "end": 105.56, "text": " Lesson zero is", "tokens": [18649, 266, 4018, 307], "temperature": 0.0, "avg_logprob": -0.1411849051406703, "compression_ratio": 1.6861924686192469, "no_speech_prob": 1.3845406101609115e-05}, {"id": 22, "seek": 7964, "start": 106.96000000000001, "end": 108.96000000000001, "text": " Based heavily on", "tokens": [18785, 10950, 322], "temperature": 0.0, "avg_logprob": -0.1411849051406703, "compression_ratio": 1.6861924686192469, "no_speech_prob": 1.3845406101609115e-05}, {"id": 23, "seek": 10896, "start": 108.96, "end": 116.0, "text": " Radix book meta learning which internally is based heavily on all the things that I've said over the years about how to learn fast AI", "tokens": [9654, 970, 1446, 19616, 2539, 597, 19501, 307, 2361, 10950, 322, 439, 264, 721, 300, 286, 600, 848, 670, 264, 924, 466, 577, 281, 1466, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.19910661564316862, "compression_ratio": 1.6261261261261262, "no_speech_prob": 2.9309510864550248e-05}, {"id": 24, "seek": 10896, "start": 116.67999999999999, "end": 119.08, "text": " It's we try to make the course full of", "tokens": [467, 311, 321, 853, 281, 652, 264, 1164, 1577, 295], "temperature": 0.0, "avg_logprob": -0.19910661564316862, "compression_ratio": 1.6261261261261262, "no_speech_prob": 2.9309510864550248e-05}, {"id": 25, "seek": 10896, "start": 119.88, "end": 124.39999999999999, "text": " Tick bits about the science of learning itself and put them into the course", "tokens": [314, 618, 9239, 466, 264, 3497, 295, 2539, 2564, 293, 829, 552, 666, 264, 1164], "temperature": 0.0, "avg_logprob": -0.19910661564316862, "compression_ratio": 1.6261261261261262, "no_speech_prob": 2.9309510864550248e-05}, {"id": 26, "seek": 10896, "start": 125.16, "end": 130.6, "text": " It's a different course to probably any other you've taken and it's I strongly recommend", "tokens": [467, 311, 257, 819, 1164, 281, 1391, 604, 661, 291, 600, 2726, 293, 309, 311, 286, 10613, 2748], "temperature": 0.0, "avg_logprob": -0.19910661564316862, "compression_ratio": 1.6261261261261262, "no_speech_prob": 2.9309510864550248e-05}, {"id": 27, "seek": 10896, "start": 131.2, "end": 133.2, "text": " watching lesson zero as", "tokens": [1976, 6898, 4018, 382], "temperature": 0.0, "avg_logprob": -0.19910661564316862, "compression_ratio": 1.6261261261261262, "no_speech_prob": 2.9309510864550248e-05}, {"id": 28, "seek": 13320, "start": 133.2, "end": 139.72, "text": " Well, the last bit of lesson zero is about how to set up a Linux box from scratch which you can happily skip over unless", "tokens": [1042, 11, 264, 1036, 857, 295, 6898, 4018, 307, 466, 577, 281, 992, 493, 257, 18734, 2424, 490, 8459, 597, 291, 393, 19909, 10023, 670, 5969], "temperature": 0.0, "avg_logprob": -0.1968749761581421, "compression_ratio": 1.551440329218107, "no_speech_prob": 7.41097346690367e-06}, {"id": 29, "seek": 13320, "start": 139.72, "end": 141.72, "text": " That's of interest but the rest of it is", "tokens": [663, 311, 295, 1179, 457, 264, 1472, 295, 309, 307], "temperature": 0.0, "avg_logprob": -0.1968749761581421, "compression_ratio": 1.551440329218107, "no_speech_prob": 7.41097346690367e-06}, {"id": 30, "seek": 13320, "start": 142.48, "end": 145.48, "text": " Full of juicy information that I think you'll find useful", "tokens": [13841, 295, 24696, 1589, 300, 286, 519, 291, 603, 915, 4420], "temperature": 0.0, "avg_logprob": -0.1968749761581421, "compression_ratio": 1.551440329218107, "no_speech_prob": 7.41097346690367e-06}, {"id": 31, "seek": 13320, "start": 147.11999999999998, "end": 149.11999999999998, "text": " So the basic idea of", "tokens": [407, 264, 3875, 1558, 295], "temperature": 0.0, "avg_logprob": -0.1968749761581421, "compression_ratio": 1.551440329218107, "no_speech_prob": 7.41097346690367e-06}, {"id": 32, "seek": 13320, "start": 150.07999999999998, "end": 153.51999999999998, "text": " What to do to do a faster AI lesson is", "tokens": [708, 281, 360, 281, 360, 257, 4663, 7318, 6898, 307], "temperature": 0.0, "avg_logprob": -0.1968749761581421, "compression_ratio": 1.551440329218107, "no_speech_prob": 7.41097346690367e-06}, {"id": 33, "seek": 13320, "start": 155.0, "end": 156.72, "text": " Watch the lecture", "tokens": [7277, 264, 7991], "temperature": 0.0, "avg_logprob": -0.1968749761581421, "compression_ratio": 1.551440329218107, "no_speech_prob": 7.41097346690367e-06}, {"id": 34, "seek": 13320, "start": 156.72, "end": 160.95999999999998, "text": " And I generally you know on the video recommend watching it all the way through", "tokens": [400, 286, 5101, 291, 458, 322, 264, 960, 2748, 1976, 309, 439, 264, 636, 807], "temperature": 0.0, "avg_logprob": -0.1968749761581421, "compression_ratio": 1.551440329218107, "no_speech_prob": 7.41097346690367e-06}, {"id": 35, "seek": 16096, "start": 160.96, "end": 164.4, "text": " without stopping once and then go back and", "tokens": [1553, 12767, 1564, 293, 550, 352, 646, 293], "temperature": 0.0, "avg_logprob": -0.17274856567382812, "compression_ratio": 1.930327868852459, "no_speech_prob": 3.187521770087187e-06}, {"id": 36, "seek": 16096, "start": 165.04000000000002, "end": 167.04000000000002, "text": " Watch it with lots of pauses", "tokens": [7277, 309, 365, 3195, 295, 2502, 8355], "temperature": 0.0, "avg_logprob": -0.17274856567382812, "compression_ratio": 1.930327868852459, "no_speech_prob": 3.187521770087187e-06}, {"id": 37, "seek": 16096, "start": 167.08, "end": 170.88, "text": " Running the notebook as you go because otherwise you're kind of like running the notebook", "tokens": [28136, 264, 21060, 382, 291, 352, 570, 5911, 291, 434, 733, 295, 411, 2614, 264, 21060], "temperature": 0.0, "avg_logprob": -0.17274856567382812, "compression_ratio": 1.930327868852459, "no_speech_prob": 3.187521770087187e-06}, {"id": 38, "seek": 16096, "start": 171.76000000000002, "end": 175.20000000000002, "text": " Without really knowing where it's heading if that makes sense", "tokens": [9129, 534, 5276, 689, 309, 311, 9864, 498, 300, 1669, 2020], "temperature": 0.0, "avg_logprob": -0.17274856567382812, "compression_ratio": 1.930327868852459, "no_speech_prob": 3.187521770087187e-06}, {"id": 39, "seek": 16096, "start": 176.12, "end": 180.88, "text": " And the idea of running the notebook is is you know, there's a few notebooks you could go through", "tokens": [400, 264, 1558, 295, 2614, 264, 21060, 307, 307, 291, 458, 11, 456, 311, 257, 1326, 43782, 291, 727, 352, 807], "temperature": 0.0, "avg_logprob": -0.17274856567382812, "compression_ratio": 1.930327868852459, "no_speech_prob": 3.187521770087187e-06}, {"id": 40, "seek": 16096, "start": 180.88, "end": 186.68, "text": " So obviously there's the book so going through chapter one of the book going through chapter two of the book as notebooks", "tokens": [407, 2745, 456, 311, 264, 1446, 370, 516, 807, 7187, 472, 295, 264, 1446, 516, 807, 7187, 732, 295, 264, 1446, 382, 43782], "temperature": 0.0, "avg_logprob": -0.17274856567382812, "compression_ratio": 1.930327868852459, "no_speech_prob": 3.187521770087187e-06}, {"id": 41, "seek": 16096, "start": 187.36, "end": 189.36, "text": " running every code cell and", "tokens": [2614, 633, 3089, 2815, 293], "temperature": 0.0, "avg_logprob": -0.17274856567382812, "compression_ratio": 1.930327868852459, "no_speech_prob": 3.187521770087187e-06}, {"id": 42, "seek": 18936, "start": 189.36, "end": 193.4, "text": " And experimenting with inputs and outputs to try and understand what's going on", "tokens": [400, 29070, 365, 15743, 293, 23930, 281, 853, 293, 1223, 437, 311, 516, 322], "temperature": 0.0, "avg_logprob": -0.1510567916067023, "compression_ratio": 1.7298387096774193, "no_speech_prob": 1.593594060977921e-05}, {"id": 43, "seek": 18936, "start": 194.64000000000001, "end": 197.8, "text": " And then trying to reproduce those results", "tokens": [400, 550, 1382, 281, 29501, 729, 3542], "temperature": 0.0, "avg_logprob": -0.1510567916067023, "compression_ratio": 1.7298387096774193, "no_speech_prob": 1.593594060977921e-05}, {"id": 44, "seek": 18936, "start": 198.60000000000002, "end": 205.12, "text": " And then trying to repeat the whole thing with a different data set and if you can do that last step, you know, that's", "tokens": [400, 550, 1382, 281, 7149, 264, 1379, 551, 365, 257, 819, 1412, 992, 293, 498, 291, 393, 360, 300, 1036, 1823, 11, 291, 458, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.1510567916067023, "compression_ratio": 1.7298387096774193, "no_speech_prob": 1.593594060977921e-05}, {"id": 45, "seek": 18936, "start": 206.32000000000002, "end": 210.20000000000002, "text": " Quite a stretch goal particularly at the start of the course because there's so many new concepts", "tokens": [20464, 257, 5985, 3387, 4098, 412, 264, 722, 295, 264, 1164, 570, 456, 311, 370, 867, 777, 10392], "temperature": 0.0, "avg_logprob": -0.1510567916067023, "compression_ratio": 1.7298387096774193, "no_speech_prob": 1.593594060977921e-05}, {"id": 46, "seek": 18936, "start": 210.20000000000002, "end": 215.3, "text": " But that really shows that you you've got it sorted now first third bit reproduce results", "tokens": [583, 300, 534, 3110, 300, 291, 291, 600, 658, 309, 25462, 586, 700, 2636, 857, 29501, 3542], "temperature": 0.0, "avg_logprob": -0.1510567916067023, "compression_ratio": 1.7298387096774193, "no_speech_prob": 1.593594060977921e-05}, {"id": 47, "seek": 21530, "start": 215.3, "end": 223.4, "text": " I recommend using you'll find in the fastbook repo. So the repository for the book. There is a special folder called", "tokens": [286, 2748, 1228, 291, 603, 915, 294, 264, 2370, 2939, 49040, 13, 407, 264, 25841, 337, 264, 1446, 13, 821, 307, 257, 2121, 10820, 1219], "temperature": 0.0, "avg_logprob": -0.19659799575805664, "compression_ratio": 1.6653061224489796, "no_speech_prob": 7.411062142637093e-06}, {"id": 48, "seek": 21530, "start": 224.14000000000001, "end": 227.76000000000002, "text": " Plane and plane contains all of the same chapters of the book", "tokens": [2149, 1929, 293, 5720, 8306, 439, 295, 264, 912, 20013, 295, 264, 1446], "temperature": 0.0, "avg_logprob": -0.19659799575805664, "compression_ratio": 1.6653061224489796, "no_speech_prob": 7.411062142637093e-06}, {"id": 49, "seek": 21530, "start": 228.3, "end": 235.08, "text": " But with all of the text removed except the headings and all the outputs removed and this is a great way for you to", "tokens": [583, 365, 439, 295, 264, 2487, 7261, 3993, 264, 1378, 1109, 293, 439, 264, 23930, 7261, 293, 341, 307, 257, 869, 636, 337, 291, 281], "temperature": 0.0, "avg_logprob": -0.19659799575805664, "compression_ratio": 1.6653061224489796, "no_speech_prob": 7.411062142637093e-06}, {"id": 50, "seek": 21530, "start": 235.5, "end": 240.44, "text": " Test your understanding of the chapter is before you run each cell", "tokens": [9279, 428, 3701, 295, 264, 7187, 307, 949, 291, 1190, 1184, 2815], "temperature": 0.0, "avg_logprob": -0.19659799575805664, "compression_ratio": 1.6653061224489796, "no_speech_prob": 7.411062142637093e-06}, {"id": 51, "seek": 21530, "start": 241.34, "end": 243.34, "text": " Try to say to yourself. Okay, what's this for?", "tokens": [6526, 281, 584, 281, 1803, 13, 1033, 11, 437, 311, 341, 337, 30], "temperature": 0.0, "avg_logprob": -0.19659799575805664, "compression_ratio": 1.6653061224489796, "no_speech_prob": 7.411062142637093e-06}, {"id": 52, "seek": 24334, "start": 243.34, "end": 249.02, "text": " And what's it going to output if anything and if you kind of work through that slowly?", "tokens": [400, 437, 311, 309, 516, 281, 5598, 498, 1340, 293, 498, 291, 733, 295, 589, 807, 300, 5692, 30], "temperature": 0.0, "avg_logprob": -0.17905082342759618, "compression_ratio": 1.630952380952381, "no_speech_prob": 8.664400411362294e-06}, {"id": 53, "seek": 24334, "start": 249.58, "end": 251.38, "text": " That's a great way at any time", "tokens": [663, 311, 257, 869, 636, 412, 604, 565], "temperature": 0.0, "avg_logprob": -0.17905082342759618, "compression_ratio": 1.630952380952381, "no_speech_prob": 8.664400411362294e-06}, {"id": 54, "seek": 24334, "start": 251.38, "end": 256.06, "text": " You're not sure you can jump back to the the version of the notebook with the text to remind yourself", "tokens": [509, 434, 406, 988, 291, 393, 3012, 646, 281, 264, 264, 3037, 295, 264, 21060, 365, 264, 2487, 281, 4160, 1803], "temperature": 0.0, "avg_logprob": -0.17905082342759618, "compression_ratio": 1.630952380952381, "no_speech_prob": 8.664400411362294e-06}, {"id": 55, "seek": 24334, "start": 256.06, "end": 258.58, "text": " And then head back over to the clean version", "tokens": [400, 550, 1378, 646, 670, 281, 264, 2541, 3037], "temperature": 0.0, "avg_logprob": -0.17905082342759618, "compression_ratio": 1.630952380952381, "no_speech_prob": 8.664400411362294e-06}, {"id": 56, "seek": 24334, "start": 262.1, "end": 267.62, "text": " So there's an idea for something which a lot of people find really useful for self-study I", "tokens": [407, 456, 311, 364, 1558, 337, 746, 597, 257, 688, 295, 561, 915, 534, 4420, 337, 2698, 12, 28349, 88, 286], "temperature": 0.0, "avg_logprob": -0.17905082342759618, "compression_ratio": 1.630952380952381, "no_speech_prob": 8.664400411362294e-06}, {"id": 57, "seek": 24334, "start": 268.46, "end": 271.3, "text": " Say self-study, but of course as we've mentioned before", "tokens": [6463, 2698, 12, 28349, 88, 11, 457, 295, 1164, 382, 321, 600, 2835, 949], "temperature": 0.0, "avg_logprob": -0.17905082342759618, "compression_ratio": 1.630952380952381, "no_speech_prob": 8.664400411362294e-06}, {"id": 58, "seek": 27130, "start": 271.3, "end": 274.1, "text": " the best kind of study is", "tokens": [264, 1151, 733, 295, 2979, 307], "temperature": 0.0, "avg_logprob": -0.2424437864771429, "compression_ratio": 1.8085106382978724, "no_speech_prob": 2.046162808255758e-05}, {"id": 59, "seek": 27130, "start": 274.62, "end": 277.86, "text": " Study done to some extent with others for most people", "tokens": [27039, 1096, 281, 512, 8396, 365, 2357, 337, 881, 561], "temperature": 0.0, "avg_logprob": -0.2424437864771429, "compression_ratio": 1.8085106382978724, "no_speech_prob": 2.046162808255758e-05}, {"id": 60, "seek": 27130, "start": 278.34000000000003, "end": 284.82, "text": " You know the research shows that you're more likely to stick with things if you're doing it as kind of a bit of a social", "tokens": [509, 458, 264, 2132, 3110, 300, 291, 434, 544, 3700, 281, 2897, 365, 721, 498, 291, 434, 884, 309, 382, 733, 295, 257, 857, 295, 257, 2093], "temperature": 0.0, "avg_logprob": -0.2424437864771429, "compression_ratio": 1.8085106382978724, "no_speech_prob": 2.046162808255758e-05}, {"id": 61, "seek": 27130, "start": 284.82, "end": 285.90000000000003, "text": " activity", "tokens": [5191], "temperature": 0.0, "avg_logprob": -0.2424437864771429, "compression_ratio": 1.8085106382978724, "no_speech_prob": 2.046162808255758e-05}, {"id": 62, "seek": 27130, "start": 285.90000000000003, "end": 289.18, "text": " There the forums are a great place to find and create", "tokens": [821, 264, 26998, 366, 257, 869, 1081, 281, 915, 293, 1884], "temperature": 0.0, "avg_logprob": -0.2424437864771429, "compression_ratio": 1.8085106382978724, "no_speech_prob": 2.046162808255758e-05}, {"id": 63, "seek": 27130, "start": 290.1, "end": 291.54, "text": " study groups", "tokens": [2979, 3935], "temperature": 0.0, "avg_logprob": -0.2424437864771429, "compression_ratio": 1.8085106382978724, "no_speech_prob": 2.046162808255758e-05}, {"id": 64, "seek": 27130, "start": 291.54, "end": 296.14, "text": " And you'll also find on the forums a link to our discord server", "tokens": [400, 291, 603, 611, 915, 322, 264, 26998, 257, 2113, 281, 527, 32989, 7154], "temperature": 0.0, "avg_logprob": -0.2424437864771429, "compression_ratio": 1.8085106382978724, "no_speech_prob": 2.046162808255758e-05}, {"id": 65, "seek": 27130, "start": 297.02, "end": 300.96000000000004, "text": " So now that's our discord server and where there are some study groups there as well", "tokens": [407, 586, 300, 311, 527, 32989, 7154, 293, 689, 456, 366, 512, 2979, 3935, 456, 382, 731], "temperature": 0.0, "avg_logprob": -0.2424437864771429, "compression_ratio": 1.8085106382978724, "no_speech_prob": 2.046162808255758e-05}, {"id": 66, "seek": 30096, "start": 300.96, "end": 305.97999999999996, "text": " so I you know in-person study groups virtual study groups are a great way to", "tokens": [370, 286, 291, 458, 294, 12, 10813, 2979, 3935, 6374, 2979, 3935, 366, 257, 869, 636, 281], "temperature": 0.0, "avg_logprob": -0.1654214570016572, "compression_ratio": 1.720524017467249, "no_speech_prob": 5.507495643541915e-06}, {"id": 67, "seek": 30096, "start": 307.62, "end": 311.73999999999995, "text": " You know really make good progress and find other people at a similar level to you", "tokens": [509, 458, 534, 652, 665, 4205, 293, 915, 661, 561, 412, 257, 2531, 1496, 281, 291], "temperature": 0.0, "avg_logprob": -0.1654214570016572, "compression_ratio": 1.720524017467249, "no_speech_prob": 5.507495643541915e-06}, {"id": 68, "seek": 30096, "start": 312.65999999999997, "end": 314.58, "text": " if there's not a", "tokens": [498, 456, 311, 406, 257], "temperature": 0.0, "avg_logprob": -0.1654214570016572, "compression_ratio": 1.720524017467249, "no_speech_prob": 5.507495643541915e-06}, {"id": 69, "seek": 30096, "start": 314.58, "end": 318.34, "text": " Study group coming at your level in your area in your time zone", "tokens": [27039, 1594, 1348, 412, 428, 1496, 294, 428, 1859, 294, 428, 565, 6668], "temperature": 0.0, "avg_logprob": -0.1654214570016572, "compression_ratio": 1.720524017467249, "no_speech_prob": 5.507495643541915e-06}, {"id": 70, "seek": 30096, "start": 318.94, "end": 322.29999999999995, "text": " Create one so you just post something saying hey, let's create a study group", "tokens": [20248, 472, 370, 291, 445, 2183, 746, 1566, 4177, 11, 718, 311, 1884, 257, 2979, 1594], "temperature": 0.0, "avg_logprob": -0.1654214570016572, "compression_ratio": 1.720524017467249, "no_speech_prob": 5.507495643541915e-06}, {"id": 71, "seek": 30096, "start": 324.21999999999997, "end": 329.21999999999997, "text": " So this week there's been a lot of fantastic activity I can't show all of it", "tokens": [407, 341, 1243, 456, 311, 668, 257, 688, 295, 5456, 5191, 286, 393, 380, 855, 439, 295, 309], "temperature": 0.0, "avg_logprob": -0.1654214570016572, "compression_ratio": 1.720524017467249, "no_speech_prob": 5.507495643541915e-06}, {"id": 72, "seek": 32922, "start": 329.22, "end": 331.22, "text": " so what I did was I used the", "tokens": [370, 437, 286, 630, 390, 286, 1143, 264], "temperature": 0.0, "avg_logprob": -0.15824226163468272, "compression_ratio": 1.6177606177606179, "no_speech_prob": 1.5206010175461415e-05}, {"id": 73, "seek": 32922, "start": 331.54, "end": 336.70000000000005, "text": " Summary functionality in the forums to grab all of the things with the highest votes and so I just quickly show a few", "tokens": [8626, 76, 822, 14980, 294, 264, 26998, 281, 4444, 439, 295, 264, 721, 365, 264, 6343, 12068, 293, 370, 286, 445, 2661, 855, 257, 1326], "temperature": 0.0, "avg_logprob": -0.15824226163468272, "compression_ratio": 1.6177606177606179, "no_speech_prob": 1.5206010175461415e-05}, {"id": 74, "seek": 32922, "start": 336.70000000000005, "end": 338.22, "text": " Of those we have a", "tokens": [2720, 729, 321, 362, 257], "temperature": 0.0, "avg_logprob": -0.15824226163468272, "compression_ratio": 1.6177606177606179, "no_speech_prob": 1.5206010175461415e-05}, {"id": 75, "seek": 32922, "start": 338.22, "end": 340.70000000000005, "text": " Marvel detector created this week", "tokens": [13837, 25712, 2942, 341, 1243], "temperature": 0.0, "avg_logprob": -0.15824226163468272, "compression_ratio": 1.6177606177606179, "no_speech_prob": 1.5206010175461415e-05}, {"id": 76, "seek": 32922, "start": 342.5, "end": 344.5, "text": " Identify your favorite Marvel character", "tokens": [25905, 2505, 428, 2954, 13837, 2517], "temperature": 0.0, "avg_logprob": -0.15824226163468272, "compression_ratio": 1.6177606177606179, "no_speech_prob": 1.5206010175461415e-05}, {"id": 77, "seek": 32922, "start": 344.82000000000005, "end": 350.22, "text": " I love this a rock-paper-scissors game where you actually use pictures of the rock-paper-scissors", "tokens": [286, 959, 341, 257, 3727, 12, 22104, 12, 4417, 13839, 1216, 689, 291, 767, 764, 5242, 295, 264, 3727, 12, 22104, 12, 4417, 13839], "temperature": 0.0, "avg_logprob": -0.15824226163468272, "compression_ratio": 1.6177606177606179, "no_speech_prob": 1.5206010175461415e-05}, {"id": 78, "seek": 32922, "start": 350.70000000000005, "end": 355.38000000000005, "text": " Symbols and apparently the computer always loses. That's my favorite kind of game", "tokens": [3902, 5612, 82, 293, 7970, 264, 3820, 1009, 18293, 13, 663, 311, 452, 2954, 733, 295, 1216], "temperature": 0.0, "avg_logprob": -0.15824226163468272, "compression_ratio": 1.6177606177606179, "no_speech_prob": 1.5206010175461415e-05}, {"id": 79, "seek": 35538, "start": 355.38, "end": 362.1, "text": " There is a lot of Elon around so very handy to have an Elon detector to you know", "tokens": [821, 307, 257, 688, 295, 28498, 926, 370, 588, 13239, 281, 362, 364, 28498, 25712, 281, 291, 458], "temperature": 0.0, "avg_logprob": -0.20322803921169705, "compression_ratio": 1.581896551724138, "no_speech_prob": 7.410961188725196e-06}, {"id": 80, "seek": 35538, "start": 362.1, "end": 365.5, "text": " Either find more of him if that's what you need or maybe less of him", "tokens": [13746, 915, 544, 295, 796, 498, 300, 311, 437, 291, 643, 420, 1310, 1570, 295, 796], "temperature": 0.0, "avg_logprob": -0.20322803921169705, "compression_ratio": 1.581896551724138, "no_speech_prob": 7.410961188725196e-06}, {"id": 81, "seek": 35538, "start": 366.26, "end": 372.62, "text": " I thought this one is very interesting. I love these kind of really interesting ideas. There's like gee", "tokens": [286, 1194, 341, 472, 307, 588, 1880, 13, 286, 959, 613, 733, 295, 534, 1880, 3487, 13, 821, 311, 411, 24105], "temperature": 0.0, "avg_logprob": -0.20322803921169705, "compression_ratio": 1.581896551724138, "no_speech_prob": 7.410961188725196e-06}, {"id": 82, "seek": 35538, "start": 372.62, "end": 375.86, "text": " I wonder if this would work. Can you predict the average?", "tokens": [286, 2441, 498, 341, 576, 589, 13, 1664, 291, 6069, 264, 4274, 30], "temperature": 0.0, "avg_logprob": -0.20322803921169705, "compression_ratio": 1.581896551724138, "no_speech_prob": 7.410961188725196e-06}, {"id": 83, "seek": 35538, "start": 377.34, "end": 382.34, "text": " temperature of an area based on a aerial photograph and", "tokens": [4292, 295, 364, 1859, 2361, 322, 257, 31026, 8348, 293], "temperature": 0.0, "avg_logprob": -0.20322803921169705, "compression_ratio": 1.581896551724138, "no_speech_prob": 7.410961188725196e-06}, {"id": 84, "seek": 38234, "start": 382.34, "end": 388.35999999999996, "text": " And the eye and apparently the answer is yeah, actually you can predict it pretty well here in Brisbane", "tokens": [400, 264, 3313, 293, 7970, 264, 1867, 307, 1338, 11, 767, 291, 393, 6069, 309, 1238, 731, 510, 294, 32222], "temperature": 0.0, "avg_logprob": -0.2850440378939168, "compression_ratio": 1.5933609958506223, "no_speech_prob": 2.885548565245699e-05}, {"id": 85, "seek": 38234, "start": 388.35999999999996, "end": 390.65999999999997, "text": " It was predicted I believed within one and a half", "tokens": [467, 390, 19147, 286, 7847, 1951, 472, 293, 257, 1922], "temperature": 0.0, "avg_logprob": -0.2850440378939168, "compression_ratio": 1.5933609958506223, "no_speech_prob": 2.885548565245699e-05}, {"id": 86, "seek": 38234, "start": 391.38, "end": 393.38, "text": " Celsius", "tokens": [22658], "temperature": 0.0, "avg_logprob": -0.2850440378939168, "compression_ratio": 1.5933609958506223, "no_speech_prob": 2.885548565245699e-05}, {"id": 87, "seek": 38234, "start": 393.38, "end": 394.58, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.2850440378939168, "compression_ratio": 1.5933609958506223, "no_speech_prob": 2.885548565245699e-05}, {"id": 88, "seek": 38234, "start": 394.58, "end": 399.9, "text": " Think this student is actually a genuine meteorologist if I remember correctly he built a cloud detector", "tokens": [6557, 341, 3107, 307, 767, 257, 16699, 25313, 9201, 498, 286, 1604, 8944, 415, 3094, 257, 4588, 25712], "temperature": 0.0, "avg_logprob": -0.2850440378939168, "compression_ratio": 1.5933609958506223, "no_speech_prob": 2.885548565245699e-05}, {"id": 89, "seek": 38234, "start": 401.14, "end": 407.09999999999997, "text": " So then building on top of the what's your favorite Marvel character? There's now also an is it a Marvel character?", "tokens": [407, 550, 2390, 322, 1192, 295, 264, 437, 311, 428, 2954, 13837, 2517, 30, 821, 311, 586, 611, 364, 307, 309, 257, 13837, 2517, 30], "temperature": 0.0, "avg_logprob": -0.2850440378939168, "compression_ratio": 1.5933609958506223, "no_speech_prob": 2.885548565245699e-05}, {"id": 90, "seek": 40710, "start": 407.1, "end": 414.98, "text": " My daughter loves this one. What dinosaur is this and I'm not as good about dinosaurs as I should be I feel like there's", "tokens": [1222, 4653, 6752, 341, 472, 13, 708, 23627, 307, 341, 293, 286, 478, 406, 382, 665, 466, 25851, 382, 286, 820, 312, 286, 841, 411, 456, 311], "temperature": 0.0, "avg_logprob": -0.2023586908976237, "compression_ratio": 1.6050420168067228, "no_speech_prob": 1.473803695262177e-05}, {"id": 91, "seek": 40710, "start": 415.90000000000003, "end": 421.3, "text": " Ten times more dinosaurs than there was when I was a kid. So I never know their names. This is very handy", "tokens": [9380, 1413, 544, 25851, 813, 456, 390, 562, 286, 390, 257, 1636, 13, 407, 286, 1128, 458, 641, 5288, 13, 639, 307, 588, 13239], "temperature": 0.0, "avg_logprob": -0.2023586908976237, "compression_ratio": 1.6050420168067228, "no_speech_prob": 1.473803695262177e-05}, {"id": 92, "seek": 40710, "start": 422.22, "end": 426.86, "text": " This is cool. Choose your own adventure where you choose your path using facial expressions", "tokens": [639, 307, 1627, 13, 21661, 428, 1065, 9868, 689, 291, 2826, 428, 3100, 1228, 15642, 15277], "temperature": 0.0, "avg_logprob": -0.2023586908976237, "compression_ratio": 1.6050420168067228, "no_speech_prob": 1.473803695262177e-05}, {"id": 93, "seek": 40710, "start": 427.86, "end": 430.86, "text": " And I think this music genre classification", "tokens": [400, 286, 519, 341, 1318, 11022, 21538], "temperature": 0.0, "avg_logprob": -0.2023586908976237, "compression_ratio": 1.6050420168067228, "no_speech_prob": 1.473803695262177e-05}, {"id": 94, "seek": 40710, "start": 431.5, "end": 433.5, "text": " Is also really cool", "tokens": [1119, 611, 534, 1627], "temperature": 0.0, "avg_logprob": -0.2023586908976237, "compression_ratio": 1.6050420168067228, "no_speech_prob": 1.473803695262177e-05}, {"id": 95, "seek": 43350, "start": 433.5, "end": 435.5, "text": " And", "tokens": [400], "temperature": 0.0, "avg_logprob": -0.26468988644179475, "compression_ratio": 1.6717557251908397, "no_speech_prob": 3.943781848647632e-05}, {"id": 96, "seek": 43350, "start": 435.62, "end": 437.62, "text": " Brian Smith created a", "tokens": [10765, 8538, 2942, 257], "temperature": 0.0, "avg_logprob": -0.26468988644179475, "compression_ratio": 1.6717557251908397, "no_speech_prob": 3.943781848647632e-05}, {"id": 97, "seek": 43350, "start": 438.1, "end": 440.1, "text": " Microsoft power app", "tokens": [8116, 1347, 724], "temperature": 0.0, "avg_logprob": -0.26468988644179475, "compression_ratio": 1.6717557251908397, "no_speech_prob": 3.943781848647632e-05}, {"id": 98, "seek": 43350, "start": 440.26, "end": 442.26, "text": " Application that actually runs on a mobile phone", "tokens": [39512, 300, 767, 6676, 322, 257, 6013, 2593], "temperature": 0.0, "avg_logprob": -0.26468988644179475, "compression_ratio": 1.6717557251908397, "no_speech_prob": 3.943781848647632e-05}, {"id": 99, "seek": 43350, "start": 442.5, "end": 448.92, "text": " That's pretty cool wouldn't be surprised to hear that Brian actually works at Microsoft. So also an opportunity to promote", "tokens": [663, 311, 1238, 1627, 2759, 380, 312, 6100, 281, 1568, 300, 10765, 767, 1985, 412, 8116, 13, 407, 611, 364, 2650, 281, 9773], "temperature": 0.0, "avg_logprob": -0.26468988644179475, "compression_ratio": 1.6717557251908397, "no_speech_prob": 3.943781848647632e-05}, {"id": 100, "seek": 43350, "start": 449.82, "end": 451.7, "text": " his own stuff there", "tokens": [702, 1065, 1507, 456], "temperature": 0.0, "avg_logprob": -0.26468988644179475, "compression_ratio": 1.6717557251908397, "no_speech_prob": 3.943781848647632e-05}, {"id": 101, "seek": 43350, "start": 451.7, "end": 457.1, "text": " I thought this art movement classifier was interesting in that like there's a really interesting discussion on the forum about", "tokens": [286, 1194, 341, 1523, 3963, 1508, 9902, 390, 1880, 294, 300, 411, 456, 311, 257, 534, 1880, 5017, 322, 264, 17542, 466], "temperature": 0.0, "avg_logprob": -0.26468988644179475, "compression_ratio": 1.6717557251908397, "no_speech_prob": 3.943781848647632e-05}, {"id": 102, "seek": 43350, "start": 457.78, "end": 461.22, "text": " What it actually shows about similarities between different art movements", "tokens": [708, 309, 767, 3110, 466, 24197, 1296, 819, 1523, 9981], "temperature": 0.0, "avg_logprob": -0.26468988644179475, "compression_ratio": 1.6717557251908397, "no_speech_prob": 3.943781848647632e-05}, {"id": 103, "seek": 46122, "start": 461.22, "end": 464.34000000000003, "text": " And I thought this redaction", "tokens": [400, 286, 1194, 341, 2182, 2894], "temperature": 0.0, "avg_logprob": -0.20368175742066938, "compression_ratio": 1.5207373271889402, "no_speech_prob": 7.527718935307348e-06}, {"id": 104, "seek": 46122, "start": 464.94000000000005, "end": 467.34000000000003, "text": " Detector project was really was really cool", "tokens": [4237, 20814, 1716, 390, 534, 390, 534, 1627], "temperature": 0.0, "avg_logprob": -0.20368175742066938, "compression_ratio": 1.5207373271889402, "no_speech_prob": 7.527718935307348e-06}, {"id": 105, "seek": 46122, "start": 467.98, "end": 473.86, "text": " As well, and there's a whole tweet thread and blog post and everything about this one particularly great piece of work", "tokens": [1018, 731, 11, 293, 456, 311, 257, 1379, 15258, 7207, 293, 6968, 2183, 293, 1203, 466, 341, 472, 4098, 869, 2522, 295, 589], "temperature": 0.0, "avg_logprob": -0.20368175742066938, "compression_ratio": 1.5207373271889402, "no_speech_prob": 7.527718935307348e-06}, {"id": 106, "seek": 46122, "start": 475.3, "end": 477.3, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.20368175742066938, "compression_ratio": 1.5207373271889402, "no_speech_prob": 7.527718935307348e-06}, {"id": 107, "seek": 46122, "start": 478.5, "end": 480.18, "text": " I'm going to", "tokens": [286, 478, 516, 281], "temperature": 0.0, "avg_logprob": -0.20368175742066938, "compression_ratio": 1.5207373271889402, "no_speech_prob": 7.527718935307348e-06}, {"id": 108, "seek": 46122, "start": 480.18, "end": 486.12, "text": " Quickly show you a couple of little tips before we kind of jump into the mechanics of what's behind a neural network", "tokens": [31800, 855, 291, 257, 1916, 295, 707, 6082, 949, 321, 733, 295, 3012, 666, 264, 12939, 295, 437, 311, 2261, 257, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.20368175742066938, "compression_ratio": 1.5207373271889402, "no_speech_prob": 7.527718935307348e-06}, {"id": 109, "seek": 48612, "start": 486.12, "end": 491.36, "text": " Which is I was playing a little bit with how do you make your", "tokens": [3013, 307, 286, 390, 2433, 257, 707, 857, 365, 577, 360, 291, 652, 428], "temperature": 0.0, "avg_logprob": -0.2497964652187853, "compression_ratio": 1.5504587155963303, "no_speech_prob": 1.280491596844513e-05}, {"id": 110, "seek": 48612, "start": 492.08, "end": 494.88, "text": " Neural network more accurate during the week", "tokens": [1734, 1807, 3209, 544, 8559, 1830, 264, 1243], "temperature": 0.0, "avg_logprob": -0.2497964652187853, "compression_ratio": 1.5504587155963303, "no_speech_prob": 1.280491596844513e-05}, {"id": 111, "seek": 48612, "start": 494.88, "end": 500.4, "text": " And so I created this pet detector and this pet detector is not just predicting", "tokens": [400, 370, 286, 2942, 341, 3817, 25712, 293, 341, 3817, 25712, 307, 406, 445, 32884], "temperature": 0.0, "avg_logprob": -0.2497964652187853, "compression_ratio": 1.5504587155963303, "no_speech_prob": 1.280491596844513e-05}, {"id": 112, "seek": 48612, "start": 501.4, "end": 504.2, "text": " Predicting dogs or cats, but what breed is it?", "tokens": [32969, 21490, 7197, 420, 11111, 11, 457, 437, 18971, 307, 309, 30], "temperature": 0.0, "avg_logprob": -0.2497964652187853, "compression_ratio": 1.5504587155963303, "no_speech_prob": 1.280491596844513e-05}, {"id": 113, "seek": 48612, "start": 504.88, "end": 506.88, "text": " That's obviously a much more difficult", "tokens": [663, 311, 2745, 257, 709, 544, 2252], "temperature": 0.0, "avg_logprob": -0.2497964652187853, "compression_ratio": 1.5504587155963303, "no_speech_prob": 1.280491596844513e-05}, {"id": 114, "seek": 48612, "start": 507.64, "end": 513.84, "text": " exercise now because I put this out on hiking face spaces you can", "tokens": [5380, 586, 570, 286, 829, 341, 484, 322, 23784, 1851, 7673, 291, 393], "temperature": 0.0, "avg_logprob": -0.2497964652187853, "compression_ratio": 1.5504587155963303, "no_speech_prob": 1.280491596844513e-05}, {"id": 115, "seek": 51384, "start": 513.84, "end": 515.2800000000001, "text": " download", "tokens": [5484], "temperature": 0.0, "avg_logprob": -0.18209590230669295, "compression_ratio": 1.5812807881773399, "no_speech_prob": 1.1842757885460742e-05}, {"id": 116, "seek": 51384, "start": 515.2800000000001, "end": 520.0400000000001, "text": " And look at my code because if you just click files and versions on the space", "tokens": [400, 574, 412, 452, 3089, 570, 498, 291, 445, 2052, 7098, 293, 9606, 322, 264, 1901], "temperature": 0.0, "avg_logprob": -0.18209590230669295, "compression_ratio": 1.5812807881773399, "no_speech_prob": 1.1842757885460742e-05}, {"id": 117, "seek": 51384, "start": 520.0400000000001, "end": 523.0, "text": " Which you can find a link on the forum and the course website", "tokens": [3013, 291, 393, 915, 257, 2113, 322, 264, 17542, 293, 264, 1164, 3144], "temperature": 0.0, "avg_logprob": -0.18209590230669295, "compression_ratio": 1.5812807881773399, "no_speech_prob": 1.1842757885460742e-05}, {"id": 118, "seek": 51384, "start": 523.52, "end": 527.36, "text": " You can see them all here and you can download it to your own computer", "tokens": [509, 393, 536, 552, 439, 510, 293, 291, 393, 5484, 309, 281, 428, 1065, 3820], "temperature": 0.0, "avg_logprob": -0.18209590230669295, "compression_ratio": 1.5812807881773399, "no_speech_prob": 1.1842757885460742e-05}, {"id": 119, "seek": 51384, "start": 530.72, "end": 532.72, "text": " So I'll show you", "tokens": [407, 286, 603, 855, 291], "temperature": 0.0, "avg_logprob": -0.18209590230669295, "compression_ratio": 1.5812807881773399, "no_speech_prob": 1.1842757885460742e-05}, {"id": 120, "seek": 51384, "start": 533.52, "end": 538.24, "text": " What I've got here now one thing I mentioned is today", "tokens": [708, 286, 600, 658, 510, 586, 472, 551, 286, 2835, 307, 965], "temperature": 0.0, "avg_logprob": -0.18209590230669295, "compression_ratio": 1.5812807881773399, "no_speech_prob": 1.1842757885460742e-05}, {"id": 121, "seek": 51384, "start": 539.44, "end": 541.44, "text": " I'm using a different platform", "tokens": [286, 478, 1228, 257, 819, 3663], "temperature": 0.0, "avg_logprob": -0.18209590230669295, "compression_ratio": 1.5812807881773399, "no_speech_prob": 1.1842757885460742e-05}, {"id": 122, "seek": 54144, "start": 541.44, "end": 545.2, "text": " So in the past I've shown you colab and I've shown you kaggle", "tokens": [407, 294, 264, 1791, 286, 600, 4898, 291, 1173, 455, 293, 286, 600, 4898, 291, 350, 559, 22631], "temperature": 0.0, "avg_logprob": -0.24150442235610065, "compression_ratio": 1.5727272727272728, "no_speech_prob": 4.092828021384776e-06}, {"id": 123, "seek": 54144, "start": 546.2, "end": 549.72, "text": " And we've also looked at doing stuff on your own computer", "tokens": [400, 321, 600, 611, 2956, 412, 884, 1507, 322, 428, 1065, 3820], "temperature": 0.0, "avg_logprob": -0.24150442235610065, "compression_ratio": 1.5727272727272728, "no_speech_prob": 4.092828021384776e-06}, {"id": 124, "seek": 54144, "start": 549.72, "end": 554.5200000000001, "text": " Not so much training models on your computer, but using the models you trained to create applications", "tokens": [1726, 370, 709, 3097, 5245, 322, 428, 3820, 11, 457, 1228, 264, 5245, 291, 8895, 281, 1884, 5821], "temperature": 0.0, "avg_logprob": -0.24150442235610065, "compression_ratio": 1.5727272727272728, "no_speech_prob": 4.092828021384776e-06}, {"id": 125, "seek": 54144, "start": 556.72, "end": 560.6400000000001, "text": " PaperSpace is a another website a bit like", "tokens": [24990, 44306, 307, 257, 1071, 3144, 257, 857, 411], "temperature": 0.0, "avg_logprob": -0.24150442235610065, "compression_ratio": 1.5727272727272728, "no_speech_prob": 4.092828021384776e-06}, {"id": 126, "seek": 54144, "start": 561.8800000000001, "end": 563.8800000000001, "text": " Kaggle and Google", "tokens": [48751, 22631, 293, 3329], "temperature": 0.0, "avg_logprob": -0.24150442235610065, "compression_ratio": 1.5727272727272728, "no_speech_prob": 4.092828021384776e-06}, {"id": 127, "seek": 54144, "start": 564.2800000000001, "end": 567.36, "text": " But in particular they have a product called gradient notebooks", "tokens": [583, 294, 1729, 436, 362, 257, 1674, 1219, 16235, 43782], "temperature": 0.0, "avg_logprob": -0.24150442235610065, "compression_ratio": 1.5727272727272728, "no_speech_prob": 4.092828021384776e-06}, {"id": 128, "seek": 56736, "start": 567.36, "end": 572.76, "text": " Which is at least as I speak and things change all the time to check the course website", "tokens": [3013, 307, 412, 1935, 382, 286, 1710, 293, 721, 1319, 439, 264, 565, 281, 1520, 264, 1164, 3144], "temperature": 0.0, "avg_logprob": -0.18768543856484549, "compression_ratio": 1.684782608695652, "no_speech_prob": 5.827863424201496e-05}, {"id": 129, "seek": 56736, "start": 572.76, "end": 577.24, "text": " but as I speak in my opinion is is by far the best platform for", "tokens": [457, 382, 286, 1710, 294, 452, 4800, 307, 307, 538, 1400, 264, 1151, 3663, 337], "temperature": 0.0, "avg_logprob": -0.18768543856484549, "compression_ratio": 1.684782608695652, "no_speech_prob": 5.827863424201496e-05}, {"id": 130, "seek": 56736, "start": 578.5600000000001, "end": 582.52, "text": " Running this course and for you know doing experimentation", "tokens": [28136, 341, 1164, 293, 337, 291, 458, 884, 37142], "temperature": 0.0, "avg_logprob": -0.18768543856484549, "compression_ratio": 1.684782608695652, "no_speech_prob": 5.827863424201496e-05}, {"id": 131, "seek": 56736, "start": 583.52, "end": 586.6800000000001, "text": " I'll explain why as we go so why haven't I been using the past two weeks?", "tokens": [286, 603, 2903, 983, 382, 321, 352, 370, 983, 2378, 380, 286, 668, 1228, 264, 1791, 732, 3259, 30], "temperature": 0.0, "avg_logprob": -0.18768543856484549, "compression_ratio": 1.684782608695652, "no_speech_prob": 5.827863424201496e-05}, {"id": 132, "seek": 56736, "start": 587.2, "end": 592.6, "text": " Because I've been waiting for them to build some stuff for us to make it particularly good", "tokens": [1436, 286, 600, 668, 3806, 337, 552, 281, 1322, 512, 1507, 337, 505, 281, 652, 309, 4098, 665], "temperature": 0.0, "avg_logprob": -0.18768543856484549, "compression_ratio": 1.684782608695652, "no_speech_prob": 5.827863424201496e-05}, {"id": 133, "seek": 59260, "start": 592.6, "end": 597.44, "text": " And they just they just finished so I've been using it all week, and it's totally amazing", "tokens": [400, 436, 445, 436, 445, 4335, 370, 286, 600, 668, 1228, 309, 439, 1243, 11, 293, 309, 311, 3879, 2243], "temperature": 0.0, "avg_logprob": -0.17566817382286334, "compression_ratio": 1.6412213740458015, "no_speech_prob": 1.4737703168066218e-05}, {"id": 134, "seek": 59260, "start": 599.0, "end": 601.0, "text": " This is what it looks like", "tokens": [639, 307, 437, 309, 1542, 411], "temperature": 0.0, "avg_logprob": -0.17566817382286334, "compression_ratio": 1.6412213740458015, "no_speech_prob": 1.4737703168066218e-05}, {"id": 135, "seek": 59260, "start": 601.5600000000001, "end": 604.6800000000001, "text": " So you've got a machine running in the cloud, but the thing that", "tokens": [407, 291, 600, 658, 257, 3479, 2614, 294, 264, 4588, 11, 457, 264, 551, 300], "temperature": 0.0, "avg_logprob": -0.17566817382286334, "compression_ratio": 1.6412213740458015, "no_speech_prob": 1.4737703168066218e-05}, {"id": 136, "seek": 59260, "start": 605.8000000000001, "end": 610.96, "text": " Was very special about it is it's a it's a real it's a real computer. You're using", "tokens": [3027, 588, 2121, 466, 309, 307, 309, 311, 257, 309, 311, 257, 957, 309, 311, 257, 957, 3820, 13, 509, 434, 1228], "temperature": 0.0, "avg_logprob": -0.17566817382286334, "compression_ratio": 1.6412213740458015, "no_speech_prob": 1.4737703168066218e-05}, {"id": 137, "seek": 59260, "start": 610.96, "end": 615.8000000000001, "text": " It's not like that kind of weird virtual version of things that Kaggle or colab has", "tokens": [467, 311, 406, 411, 300, 733, 295, 3657, 6374, 3037, 295, 721, 300, 48751, 22631, 420, 1173, 455, 575], "temperature": 0.0, "avg_logprob": -0.17566817382286334, "compression_ratio": 1.6412213740458015, "no_speech_prob": 1.4737703168066218e-05}, {"id": 138, "seek": 61580, "start": 615.8, "end": 621.66, "text": " So if you whack on this button down here, you'll get a full version of Jupiter lab", "tokens": [407, 498, 291, 42877, 322, 341, 2960, 760, 510, 11, 291, 603, 483, 257, 1577, 3037, 295, 24567, 2715], "temperature": 0.0, "avg_logprob": -0.22319236498200493, "compression_ratio": 1.615702479338843, "no_speech_prob": 1.4509882021229714e-05}, {"id": 139, "seek": 61580, "start": 623.16, "end": 628.78, "text": " Or you can switch over to a full version of plastic Jupiter notebooks", "tokens": [1610, 291, 393, 3679, 670, 281, 257, 1577, 3037, 295, 5900, 24567, 43782], "temperature": 0.0, "avg_logprob": -0.22319236498200493, "compression_ratio": 1.615702479338843, "no_speech_prob": 1.4509882021229714e-05}, {"id": 140, "seek": 61580, "start": 628.78, "end": 636.04, "text": " And I'm actually going to do stuff in Jupiter lab today because it's a pretty good environment for beginners who?", "tokens": [400, 286, 478, 767, 516, 281, 360, 1507, 294, 24567, 2715, 965, 570, 309, 311, 257, 1238, 665, 2823, 337, 26992, 567, 30], "temperature": 0.0, "avg_logprob": -0.22319236498200493, "compression_ratio": 1.615702479338843, "no_speech_prob": 1.4509882021229714e-05}, {"id": 141, "seek": 61580, "start": 636.56, "end": 641.8399999999999, "text": " Not familiar with the terminal which I know a lot of people in the course are in that situation you can do really everything", "tokens": [1726, 4963, 365, 264, 14709, 597, 286, 458, 257, 688, 295, 561, 294, 264, 1164, 366, 294, 300, 2590, 291, 393, 360, 534, 1203], "temperature": 0.0, "avg_logprob": -0.22319236498200493, "compression_ratio": 1.615702479338843, "no_speech_prob": 1.4509882021229714e-05}, {"id": 142, "seek": 64184, "start": 641.84, "end": 649.08, "text": " Kind of graphically there's a file browser so here you can see I've got my pets repo", "tokens": [9242, 295, 4295, 984, 456, 311, 257, 3991, 11185, 370, 510, 291, 393, 536, 286, 600, 658, 452, 19897, 49040], "temperature": 0.0, "avg_logprob": -0.24828775813070575, "compression_ratio": 1.5714285714285714, "no_speech_prob": 7.646407539141364e-06}, {"id": 143, "seek": 64184, "start": 649.9200000000001, "end": 654.6, "text": " It's got a git repository thing you can pull and push to git", "tokens": [467, 311, 658, 257, 18331, 25841, 551, 291, 393, 2235, 293, 2944, 281, 18331], "temperature": 0.0, "avg_logprob": -0.24828775813070575, "compression_ratio": 1.5714285714285714, "no_speech_prob": 7.646407539141364e-06}, {"id": 144, "seek": 64184, "start": 655.6, "end": 657.6, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.24828775813070575, "compression_ratio": 1.5714285714285714, "no_speech_prob": 7.646407539141364e-06}, {"id": 145, "seek": 64184, "start": 657.96, "end": 659.96, "text": " Then you can also", "tokens": [1396, 291, 393, 611], "temperature": 0.0, "avg_logprob": -0.24828775813070575, "compression_ratio": 1.5714285714285714, "no_speech_prob": 7.646407539141364e-06}, {"id": 146, "seek": 64184, "start": 661.12, "end": 664.1600000000001, "text": " Open up a terminal create new notebooks", "tokens": [7238, 493, 257, 14709, 1884, 777, 43782], "temperature": 0.0, "avg_logprob": -0.24828775813070575, "compression_ratio": 1.5714285714285714, "no_speech_prob": 7.646407539141364e-06}, {"id": 147, "seek": 64184, "start": 665.2, "end": 670.52, "text": " And so forth so what I tend to do with this is I tend to go into a full screen. It's kind of like its own whole", "tokens": [400, 370, 5220, 370, 437, 286, 3928, 281, 360, 365, 341, 307, 286, 3928, 281, 352, 666, 257, 1577, 2568, 13, 467, 311, 733, 295, 411, 1080, 1065, 1379], "temperature": 0.0, "avg_logprob": -0.24828775813070575, "compression_ratio": 1.5714285714285714, "no_speech_prob": 7.646407539141364e-06}, {"id": 148, "seek": 67052, "start": 670.52, "end": 672.52, "text": " IDE", "tokens": [40930], "temperature": 0.0, "avg_logprob": -0.22550913799239927, "compression_ratio": 1.5265957446808511, "no_speech_prob": 3.785230546782259e-06}, {"id": 149, "seek": 67052, "start": 674.0, "end": 676.0, "text": " And", "tokens": [400], "temperature": 0.0, "avg_logprob": -0.22550913799239927, "compression_ratio": 1.5265957446808511, "no_speech_prob": 3.785230546782259e-06}, {"id": 150, "seek": 67052, "start": 676.1999999999999, "end": 679.12, "text": " So you can see I've got here my my terminal", "tokens": [407, 291, 393, 536, 286, 600, 658, 510, 452, 452, 14709], "temperature": 0.0, "avg_logprob": -0.22550913799239927, "compression_ratio": 1.5265957446808511, "no_speech_prob": 3.785230546782259e-06}, {"id": 151, "seek": 67052, "start": 680.24, "end": 682.24, "text": " Here's my notebook", "tokens": [1692, 311, 452, 21060], "temperature": 0.0, "avg_logprob": -0.22550913799239927, "compression_ratio": 1.5265957446808511, "no_speech_prob": 3.785230546782259e-06}, {"id": 152, "seek": 67052, "start": 682.68, "end": 684.88, "text": " They have free GPUs", "tokens": [814, 362, 1737, 18407, 82], "temperature": 0.0, "avg_logprob": -0.22550913799239927, "compression_ratio": 1.5265957446808511, "no_speech_prob": 3.785230546782259e-06}, {"id": 153, "seek": 67052, "start": 685.4, "end": 689.54, "text": " And most importantly there's two good features one is that you can pay", "tokens": [400, 881, 8906, 456, 311, 732, 665, 4122, 472, 307, 300, 291, 393, 1689], "temperature": 0.0, "avg_logprob": -0.22550913799239927, "compression_ratio": 1.5265957446808511, "no_speech_prob": 3.785230546782259e-06}, {"id": 154, "seek": 67052, "start": 689.54, "end": 695.36, "text": " I think it's eight or nine dollars a month to get better GPUs and basically as many as you you know as many hours as you", "tokens": [286, 519, 309, 311, 3180, 420, 4949, 3808, 257, 1618, 281, 483, 1101, 18407, 82, 293, 1936, 382, 867, 382, 291, 291, 458, 382, 867, 2496, 382, 291], "temperature": 0.0, "avg_logprob": -0.22550913799239927, "compression_ratio": 1.5265957446808511, "no_speech_prob": 3.785230546782259e-06}, {"id": 155, "seek": 67052, "start": 695.36, "end": 696.68, "text": " want", "tokens": [528], "temperature": 0.0, "avg_logprob": -0.22550913799239927, "compression_ratio": 1.5265957446808511, "no_speech_prob": 3.785230546782259e-06}, {"id": 156, "seek": 69668, "start": 696.68, "end": 701.3199999999999, "text": " And they have persistent storage so with CoLab if you've played with it you might have noticed", "tokens": [400, 436, 362, 24315, 6725, 370, 365, 3066, 37880, 498, 291, 600, 3737, 365, 309, 291, 1062, 362, 5694], "temperature": 0.0, "avg_logprob": -0.22187423706054688, "compression_ratio": 1.689516129032258, "no_speech_prob": 1.696379331406206e-05}, {"id": 157, "seek": 69668, "start": 701.3199999999999, "end": 707.4799999999999, "text": " It's annoying you have to muck around with saving things to Google Drive and stuff on Kaggle. There isn't really a way of", "tokens": [467, 311, 11304, 291, 362, 281, 275, 1134, 926, 365, 6816, 721, 281, 3329, 15622, 293, 1507, 322, 48751, 22631, 13, 821, 1943, 380, 534, 257, 636, 295], "temperature": 0.0, "avg_logprob": -0.22187423706054688, "compression_ratio": 1.689516129032258, "no_speech_prob": 1.696379331406206e-05}, {"id": 158, "seek": 69668, "start": 708.9599999999999, "end": 711.2399999999999, "text": " Kind of having a persistent environment", "tokens": [9242, 295, 1419, 257, 24315, 2823], "temperature": 0.0, "avg_logprob": -0.22187423706054688, "compression_ratio": 1.689516129032258, "no_speech_prob": 1.696379331406206e-05}, {"id": 159, "seek": 69668, "start": 711.76, "end": 716.1999999999999, "text": " Where else some paper space you have you know whatever you save in your storage?", "tokens": [2305, 1646, 512, 3035, 1901, 291, 362, 291, 458, 2035, 291, 3155, 294, 428, 6725, 30], "temperature": 0.0, "avg_logprob": -0.22187423706054688, "compression_ratio": 1.689516129032258, "no_speech_prob": 1.696379331406206e-05}, {"id": 160, "seek": 69668, "start": 716.1999999999999, "end": 718.3399999999999, "text": " It's going to be there the next time you come come back", "tokens": [467, 311, 516, 281, 312, 456, 264, 958, 565, 291, 808, 808, 646], "temperature": 0.0, "avg_logprob": -0.22187423706054688, "compression_ratio": 1.689516129032258, "no_speech_prob": 1.696379331406206e-05}, {"id": 161, "seek": 69668, "start": 718.92, "end": 720.92, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.22187423706054688, "compression_ratio": 1.689516129032258, "no_speech_prob": 1.696379331406206e-05}, {"id": 162, "seek": 69668, "start": 722.28, "end": 724.28, "text": " I'm going to be adding", "tokens": [286, 478, 516, 281, 312, 5127], "temperature": 0.0, "avg_logprob": -0.22187423706054688, "compression_ratio": 1.689516129032258, "no_speech_prob": 1.696379331406206e-05}, {"id": 163, "seek": 72428, "start": 724.28, "end": 731.04, "text": " Walkthroughs of all of this functionality so look at so if you're interested in really taking advantage of this check those out", "tokens": [10818, 11529, 82, 295, 439, 295, 341, 14980, 370, 574, 412, 370, 498, 291, 434, 3102, 294, 534, 1940, 5002, 295, 341, 1520, 729, 484], "temperature": 0.0, "avg_logprob": -0.1892818675321691, "compression_ratio": 1.6042553191489362, "no_speech_prob": 6.144030066934647e-06}, {"id": 164, "seek": 72428, "start": 732.56, "end": 734.56, "text": " Okay, so I", "tokens": [1033, 11, 370, 286], "temperature": 0.0, "avg_logprob": -0.1892818675321691, "compression_ratio": 1.6042553191489362, "no_speech_prob": 6.144030066934647e-06}, {"id": 165, "seek": 72428, "start": 734.9599999999999, "end": 736.9599999999999, "text": " think the main thing that", "tokens": [519, 264, 2135, 551, 300], "temperature": 0.0, "avg_logprob": -0.1892818675321691, "compression_ratio": 1.6042553191489362, "no_speech_prob": 6.144030066934647e-06}, {"id": 166, "seek": 72428, "start": 737.28, "end": 739.9599999999999, "text": " I wanted you to take away from lesson two", "tokens": [286, 1415, 291, 281, 747, 1314, 490, 6898, 732], "temperature": 0.0, "avg_logprob": -0.1892818675321691, "compression_ratio": 1.6042553191489362, "no_speech_prob": 6.144030066934647e-06}, {"id": 167, "seek": 72428, "start": 740.72, "end": 747.0, "text": " isn't necessarily all the details of how do you use a particular platform to train models and", "tokens": [1943, 380, 4725, 439, 264, 4365, 295, 577, 360, 291, 764, 257, 1729, 3663, 281, 3847, 5245, 293], "temperature": 0.0, "avg_logprob": -0.1892818675321691, "compression_ratio": 1.6042553191489362, "no_speech_prob": 6.144030066934647e-06}, {"id": 168, "seek": 74700, "start": 747.0, "end": 753.24, "text": " Deploy them into applications through through JavaScript or online platforms", "tokens": [1346, 2384, 552, 666, 5821, 807, 807, 15778, 420, 2950, 9473], "temperature": 0.0, "avg_logprob": -0.17083410614902533, "compression_ratio": 1.7410358565737052, "no_speech_prob": 7.071744676068192e-06}, {"id": 169, "seek": 74700, "start": 754.04, "end": 758.6, "text": " But the key thing I wanted you to understand was the concept there's really two pieces", "tokens": [583, 264, 2141, 551, 286, 1415, 291, 281, 1223, 390, 264, 3410, 456, 311, 534, 732, 3755], "temperature": 0.0, "avg_logprob": -0.17083410614902533, "compression_ratio": 1.7410358565737052, "no_speech_prob": 7.071744676068192e-06}, {"id": 170, "seek": 74700, "start": 759.08, "end": 765.32, "text": " There's the training piece and at the end of the training piece you end up with this model dot pickle file right", "tokens": [821, 311, 264, 3097, 2522, 293, 412, 264, 917, 295, 264, 3097, 2522, 291, 917, 493, 365, 341, 2316, 5893, 31433, 3991, 558], "temperature": 0.0, "avg_logprob": -0.17083410614902533, "compression_ratio": 1.7410358565737052, "no_speech_prob": 7.071744676068192e-06}, {"id": 171, "seek": 74700, "start": 766.0, "end": 767.92, "text": " And once you've got that", "tokens": [400, 1564, 291, 600, 658, 300], "temperature": 0.0, "avg_logprob": -0.17083410614902533, "compression_ratio": 1.7410358565737052, "no_speech_prob": 7.071744676068192e-06}, {"id": 172, "seek": 74700, "start": 767.92, "end": 772.4, "text": " That's now a thing where you feed it inputs, and it's spits out outputs", "tokens": [663, 311, 586, 257, 551, 689, 291, 3154, 309, 15743, 11, 293, 309, 311, 637, 1208, 484, 23930], "temperature": 0.0, "avg_logprob": -0.17083410614902533, "compression_ratio": 1.7410358565737052, "no_speech_prob": 7.071744676068192e-06}, {"id": 173, "seek": 74700, "start": 772.92, "end": 775.64, "text": " Based on that model that you trained and then so you don't need", "tokens": [18785, 322, 300, 2316, 300, 291, 8895, 293, 550, 370, 291, 500, 380, 643], "temperature": 0.0, "avg_logprob": -0.17083410614902533, "compression_ratio": 1.7410358565737052, "no_speech_prob": 7.071744676068192e-06}, {"id": 174, "seek": 77564, "start": 775.64, "end": 781.12, "text": " You know because that happens pretty fast you generally don't need a GPU once you've got that trained", "tokens": [509, 458, 570, 300, 2314, 1238, 2370, 291, 5101, 500, 380, 643, 257, 18407, 1564, 291, 600, 658, 300, 8895], "temperature": 0.0, "avg_logprob": -0.15793937014550277, "compression_ratio": 1.6103896103896105, "no_speech_prob": 5.0935936997120734e-06}, {"id": 175, "seek": 77564, "start": 781.68, "end": 783.68, "text": " And so then there's a separate step", "tokens": [400, 370, 550, 456, 311, 257, 4994, 1823], "temperature": 0.0, "avg_logprob": -0.15793937014550277, "compression_ratio": 1.6103896103896105, "no_speech_prob": 5.0935936997120734e-06}, {"id": 176, "seek": 77564, "start": 783.92, "end": 788.0, "text": " Which is deploying so I'll show you how I trained my", "tokens": [3013, 307, 34198, 370, 286, 603, 855, 291, 577, 286, 8895, 452], "temperature": 0.0, "avg_logprob": -0.15793937014550277, "compression_ratio": 1.6103896103896105, "no_speech_prob": 5.0935936997120734e-06}, {"id": 177, "seek": 77564, "start": 789.08, "end": 791.08, "text": " pet classifier", "tokens": [3817, 1508, 9902], "temperature": 0.0, "avg_logprob": -0.15793937014550277, "compression_ratio": 1.6103896103896105, "no_speech_prob": 5.0935936997120734e-06}, {"id": 178, "seek": 77564, "start": 792.72, "end": 794.72, "text": " So you can see I've got two", "tokens": [407, 291, 393, 536, 286, 600, 658, 732], "temperature": 0.0, "avg_logprob": -0.15793937014550277, "compression_ratio": 1.6103896103896105, "no_speech_prob": 5.0935936997120734e-06}, {"id": 179, "seek": 77564, "start": 795.36, "end": 797.36, "text": " IPython notebooks", "tokens": [8671, 88, 11943, 43782], "temperature": 0.0, "avg_logprob": -0.15793937014550277, "compression_ratio": 1.6103896103896105, "no_speech_prob": 5.0935936997120734e-06}, {"id": 180, "seek": 77564, "start": 797.56, "end": 802.9, "text": " One is app which is the one that's going to be doing the inference and production one is the one where I train the model", "tokens": [1485, 307, 724, 597, 307, 264, 472, 300, 311, 516, 281, 312, 884, 264, 38253, 293, 4265, 472, 307, 264, 472, 689, 286, 3847, 264, 2316], "temperature": 0.0, "avg_logprob": -0.15793937014550277, "compression_ratio": 1.6103896103896105, "no_speech_prob": 5.0935936997120734e-06}, {"id": 181, "seek": 80290, "start": 802.9, "end": 809.42, "text": " So this first bit I'm going to skip over because you've seen it before I create my image data loaders", "tokens": [407, 341, 700, 857, 286, 478, 516, 281, 10023, 670, 570, 291, 600, 1612, 309, 949, 286, 1884, 452, 3256, 1412, 3677, 433], "temperature": 0.0, "avg_logprob": -0.24340809255406476, "compression_ratio": 1.4090909090909092, "no_speech_prob": 8.939342478697654e-06}, {"id": 182, "seek": 80290, "start": 810.34, "end": 812.4599999999999, "text": " Check that my data looks okay with show batch", "tokens": [6881, 300, 452, 1412, 1542, 1392, 365, 855, 15245], "temperature": 0.0, "avg_logprob": -0.24340809255406476, "compression_ratio": 1.4090909090909092, "no_speech_prob": 8.939342478697654e-06}, {"id": 183, "seek": 80290, "start": 813.1, "end": 818.5, "text": " Train a resnet 34 and I get 7% accuracy", "tokens": [28029, 257, 725, 7129, 12790, 293, 286, 483, 1614, 4, 14170], "temperature": 0.0, "avg_logprob": -0.24340809255406476, "compression_ratio": 1.4090909090909092, "no_speech_prob": 8.939342478697654e-06}, {"id": 184, "seek": 80290, "start": 819.78, "end": 821.78, "text": " So that's pretty good", "tokens": [407, 300, 311, 1238, 665], "temperature": 0.0, "avg_logprob": -0.24340809255406476, "compression_ratio": 1.4090909090909092, "no_speech_prob": 8.939342478697654e-06}, {"id": 185, "seek": 80290, "start": 827.06, "end": 829.46, "text": " But check this out there's a link here", "tokens": [583, 1520, 341, 484, 456, 311, 257, 2113, 510], "temperature": 0.0, "avg_logprob": -0.24340809255406476, "compression_ratio": 1.4090909090909092, "no_speech_prob": 8.939342478697654e-06}, {"id": 186, "seek": 82946, "start": 829.46, "end": 835.74, "text": " To a notebook I created actually most of the work was done by Ross Whiteman", "tokens": [1407, 257, 21060, 286, 2942, 767, 881, 295, 264, 589, 390, 1096, 538, 16140, 21693, 15023], "temperature": 0.0, "avg_logprob": -0.19642137926678324, "compression_ratio": 1.5411255411255411, "no_speech_prob": 8.939467079471797e-06}, {"id": 187, "seek": 82946, "start": 837.58, "end": 840.94, "text": " Where we can try to improve this by finding a better architecture", "tokens": [2305, 321, 393, 853, 281, 3470, 341, 538, 5006, 257, 1101, 9482], "temperature": 0.0, "avg_logprob": -0.19642137926678324, "compression_ratio": 1.5411255411255411, "no_speech_prob": 8.939467079471797e-06}, {"id": 188, "seek": 82946, "start": 842.98, "end": 848.26, "text": " There are I think at the moment in the pytorch image models libraries over 500", "tokens": [821, 366, 286, 519, 412, 264, 1623, 294, 264, 25878, 284, 339, 3256, 5245, 15148, 670, 5923], "temperature": 0.0, "avg_logprob": -0.19642137926678324, "compression_ratio": 1.5411255411255411, "no_speech_prob": 8.939467079471797e-06}, {"id": 189, "seek": 82946, "start": 849.38, "end": 851.62, "text": " Architectures, and we'll be learning over the course", "tokens": [29306, 1303, 11, 293, 321, 603, 312, 2539, 670, 264, 1164], "temperature": 0.0, "avg_logprob": -0.19642137926678324, "compression_ratio": 1.5411255411255411, "no_speech_prob": 8.939467079471797e-06}, {"id": 190, "seek": 82946, "start": 852.26, "end": 857.5, "text": " You know what they are how they differ, but you know broadly speaking. They're all", "tokens": [509, 458, 437, 436, 366, 577, 436, 743, 11, 457, 291, 458, 19511, 4124, 13, 814, 434, 439], "temperature": 0.0, "avg_logprob": -0.19642137926678324, "compression_ratio": 1.5411255411255411, "no_speech_prob": 8.939467079471797e-06}, {"id": 191, "seek": 85750, "start": 857.5, "end": 862.26, "text": " mathematical functions, you know which are basically matrix multiplications and", "tokens": [18894, 6828, 11, 291, 458, 597, 366, 1936, 8141, 17596, 763, 293], "temperature": 0.0, "avg_logprob": -0.2550224342731514, "compression_ratio": 1.611336032388664, "no_speech_prob": 7.183145044109551e-06}, {"id": 192, "seek": 85750, "start": 863.1, "end": 865.78, "text": " And these these non-linearities such as", "tokens": [400, 613, 613, 2107, 12, 28263, 1088, 1270, 382], "temperature": 0.0, "avg_logprob": -0.2550224342731514, "compression_ratio": 1.611336032388664, "no_speech_prob": 7.183145044109551e-06}, {"id": 193, "seek": 85750, "start": 867.26, "end": 869.26, "text": " Relu's that we talk about today", "tokens": [8738, 84, 311, 300, 321, 751, 466, 965], "temperature": 0.0, "avg_logprob": -0.2550224342731514, "compression_ratio": 1.611336032388664, "no_speech_prob": 7.183145044109551e-06}, {"id": 194, "seek": 85750, "start": 870.54, "end": 875.3, "text": " So most of the time those details don't matter what we care about is three things how fast are they?", "tokens": [407, 881, 295, 264, 565, 729, 4365, 500, 380, 1871, 437, 321, 1127, 466, 307, 1045, 721, 577, 2370, 366, 436, 30], "temperature": 0.0, "avg_logprob": -0.2550224342731514, "compression_ratio": 1.611336032388664, "no_speech_prob": 7.183145044109551e-06}, {"id": 195, "seek": 85750, "start": 875.58, "end": 878.58, "text": " How much memory do they use and how accurate are they and?", "tokens": [1012, 709, 4675, 360, 436, 764, 293, 577, 8559, 366, 436, 293, 30], "temperature": 0.0, "avg_logprob": -0.2550224342731514, "compression_ratio": 1.611336032388664, "no_speech_prob": 7.183145044109551e-06}, {"id": 196, "seek": 85750, "start": 879.22, "end": 884.06, "text": " So what I've done here with Ross is we've grabbed all of the models from pytorch image", "tokens": [407, 437, 286, 600, 1096, 510, 365, 16140, 307, 321, 600, 18607, 439, 295, 264, 5245, 490, 25878, 284, 339, 3256], "temperature": 0.0, "avg_logprob": -0.2550224342731514, "compression_ratio": 1.611336032388664, "no_speech_prob": 7.183145044109551e-06}, {"id": 197, "seek": 88406, "start": 884.06, "end": 889.3, "text": " Image models and you can see all the code. We've got is very very little code", "tokens": [29903, 5245, 293, 291, 393, 536, 439, 264, 3089, 13, 492, 600, 658, 307, 588, 588, 707, 3089], "temperature": 0.0, "avg_logprob": -0.25539283752441405, "compression_ratio": 1.496969696969697, "no_speech_prob": 1.202935163746588e-05}, {"id": 198, "seek": 88406, "start": 890.6999999999999, "end": 892.6999999999999, "text": " To create this this plot", "tokens": [1407, 1884, 341, 341, 7542], "temperature": 0.0, "avg_logprob": -0.25539283752441405, "compression_ratio": 1.496969696969697, "no_speech_prob": 1.202935163746588e-05}, {"id": 199, "seek": 88406, "start": 895.42, "end": 902.5799999999999, "text": " Now my screen resolution is a bit there we go let's do that and so on this plot", "tokens": [823, 452, 2568, 8669, 307, 257, 857, 456, 321, 352, 718, 311, 360, 300, 293, 370, 322, 341, 7542], "temperature": 0.0, "avg_logprob": -0.25539283752441405, "compression_ratio": 1.496969696969697, "no_speech_prob": 1.202935163746588e-05}, {"id": 200, "seek": 88406, "start": 904.14, "end": 910.66, "text": " On the x-axis we've got seconds per sample so how fast is it so?", "tokens": [1282, 264, 2031, 12, 24633, 321, 600, 658, 3949, 680, 6889, 370, 577, 2370, 307, 309, 370, 30], "temperature": 0.0, "avg_logprob": -0.25539283752441405, "compression_ratio": 1.496969696969697, "no_speech_prob": 1.202935163746588e-05}, {"id": 201, "seek": 91066, "start": 910.66, "end": 917.2199999999999, "text": " To the left is better is faster and on the right is how accurate is it so how how accurate was it on?", "tokens": [1407, 264, 1411, 307, 1101, 307, 4663, 293, 322, 264, 558, 307, 577, 8559, 307, 309, 370, 577, 577, 8559, 390, 309, 322, 30], "temperature": 0.0, "avg_logprob": -0.21318700475600158, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.5445924873347394e-05}, {"id": 202, "seek": 91066, "start": 917.2199999999999, "end": 922.5799999999999, "text": " Image net in particular and so generally speaking you want things that are up towards the", "tokens": [29903, 2533, 294, 1729, 293, 370, 5101, 4124, 291, 528, 721, 300, 366, 493, 3030, 264], "temperature": 0.0, "avg_logprob": -0.21318700475600158, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.5445924873347394e-05}, {"id": 203, "seek": 91066, "start": 923.3399999999999, "end": 925.3399999999999, "text": " top and left", "tokens": [1192, 293, 1411], "temperature": 0.0, "avg_logprob": -0.21318700475600158, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.5445924873347394e-05}, {"id": 204, "seek": 91066, "start": 926.02, "end": 929.9399999999999, "text": " Now we've been mainly working with resnet and you can see down here", "tokens": [823, 321, 600, 668, 8704, 1364, 365, 725, 7129, 293, 291, 393, 536, 760, 510], "temperature": 0.0, "avg_logprob": -0.21318700475600158, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.5445924873347394e-05}, {"id": 205, "seek": 91066, "start": 929.9399999999999, "end": 936.6, "text": " Here's resnet 18 now resnet 18 is is a particularly small and fast version for prototyping we often use resnet", "tokens": [1692, 311, 725, 7129, 2443, 586, 725, 7129, 2443, 307, 307, 257, 4098, 1359, 293, 2370, 3037, 337, 46219, 3381, 321, 2049, 764, 725, 7129], "temperature": 0.0, "avg_logprob": -0.21318700475600158, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.5445924873347394e-05}, {"id": 206, "seek": 91066, "start": 936.86, "end": 938.86, "text": " 34 which is this one here", "tokens": [12790, 597, 307, 341, 472, 510], "temperature": 0.0, "avg_logprob": -0.21318700475600158, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.5445924873347394e-05}, {"id": 207, "seek": 93886, "start": 938.86, "end": 946.58, "text": " And you can see this kind of like classic model that's very widely used actually nowadays isn't the state-of-the-art anymore", "tokens": [400, 291, 393, 536, 341, 733, 295, 411, 7230, 2316, 300, 311, 588, 13371, 1143, 767, 13434, 1943, 380, 264, 1785, 12, 2670, 12, 3322, 12, 446, 3602], "temperature": 0.0, "avg_logprob": -0.14352477757276688, "compression_ratio": 1.6616541353383458, "no_speech_prob": 9.51578749663895e-06}, {"id": 208, "seek": 93886, "start": 948.14, "end": 952.46, "text": " So we can start to look up at these ones up here and find out some of these better models", "tokens": [407, 321, 393, 722, 281, 574, 493, 412, 613, 2306, 493, 510, 293, 915, 484, 512, 295, 613, 1101, 5245], "temperature": 0.0, "avg_logprob": -0.14352477757276688, "compression_ratio": 1.6616541353383458, "no_speech_prob": 9.51578749663895e-06}, {"id": 209, "seek": 93886, "start": 953.38, "end": 956.1, "text": " the ones that seem to be the most accurate and", "tokens": [264, 2306, 300, 1643, 281, 312, 264, 881, 8559, 293], "temperature": 0.0, "avg_logprob": -0.14352477757276688, "compression_ratio": 1.6616541353383458, "no_speech_prob": 9.51578749663895e-06}, {"id": 210, "seek": 93886, "start": 957.3000000000001, "end": 959.3000000000001, "text": " fast for these levitt models", "tokens": [2370, 337, 613, 20445, 593, 5245], "temperature": 0.0, "avg_logprob": -0.14352477757276688, "compression_ratio": 1.6616541353383458, "no_speech_prob": 9.51578749663895e-06}, {"id": 211, "seek": 93886, "start": 960.38, "end": 965.7, "text": " So I tried them out on my pets, and I found that they didn't work particularly well, so I thought okay", "tokens": [407, 286, 3031, 552, 484, 322, 452, 19897, 11, 293, 286, 1352, 300, 436, 994, 380, 589, 4098, 731, 11, 370, 286, 1194, 1392], "temperature": 0.0, "avg_logprob": -0.14352477757276688, "compression_ratio": 1.6616541353383458, "no_speech_prob": 9.51578749663895e-06}, {"id": 212, "seek": 96570, "start": 965.7, "end": 969.26, "text": " Let's try something else out so next up. I tried these", "tokens": [961, 311, 853, 746, 1646, 484, 370, 958, 493, 13, 286, 3031, 613], "temperature": 0.0, "avg_logprob": -0.28313378613404555, "compression_ratio": 1.5682819383259912, "no_speech_prob": 5.682337814505445e-06}, {"id": 213, "seek": 96570, "start": 970.38, "end": 972.38, "text": " convexed models and", "tokens": [42432, 292, 5245, 293], "temperature": 0.0, "avg_logprob": -0.28313378613404555, "compression_ratio": 1.5682819383259912, "no_speech_prob": 5.682337814505445e-06}, {"id": 214, "seek": 96570, "start": 972.6600000000001, "end": 979.0200000000001, "text": " This one in here was particularly interesting. It's kind of like super high accuracy. It's the you know if you want", "tokens": [639, 472, 294, 510, 390, 4098, 1880, 13, 467, 311, 733, 295, 411, 1687, 1090, 14170, 13, 467, 311, 264, 291, 458, 498, 291, 528], "temperature": 0.0, "avg_logprob": -0.28313378613404555, "compression_ratio": 1.5682819383259912, "no_speech_prob": 5.682337814505445e-06}, {"id": 215, "seek": 96570, "start": 981.1400000000001, "end": 985.82, "text": " 0.001 seconds inference time it's the most accurate so I tried that so how do we try that?", "tokens": [1958, 13, 628, 16, 3949, 38253, 565, 309, 311, 264, 881, 8559, 370, 286, 3031, 300, 370, 577, 360, 321, 853, 300, 30], "temperature": 0.0, "avg_logprob": -0.28313378613404555, "compression_ratio": 1.5682819383259912, "no_speech_prob": 5.682337814505445e-06}, {"id": 216, "seek": 96570, "start": 986.86, "end": 989.5400000000001, "text": " All we do is I can say", "tokens": [1057, 321, 360, 307, 286, 393, 584], "temperature": 0.0, "avg_logprob": -0.28313378613404555, "compression_ratio": 1.5682819383259912, "no_speech_prob": 5.682337814505445e-06}, {"id": 217, "seek": 96570, "start": 990.1800000000001, "end": 995.0, "text": " So the high-touch image models is in the Tim module", "tokens": [407, 264, 1090, 12, 83, 2220, 3256, 5245, 307, 294, 264, 7172, 10088], "temperature": 0.0, "avg_logprob": -0.28313378613404555, "compression_ratio": 1.5682819383259912, "no_speech_prob": 5.682337814505445e-06}, {"id": 218, "seek": 99500, "start": 995.0, "end": 997.0, "text": " So at the very start I imported that", "tokens": [407, 412, 264, 588, 722, 286, 25524, 300], "temperature": 0.0, "avg_logprob": -0.21394420706707498, "compression_ratio": 1.5792079207920793, "no_speech_prob": 1.202944986289367e-05}, {"id": 219, "seek": 99500, "start": 998.04, "end": 1000.72, "text": " And we can say list models and pass in a", "tokens": [400, 321, 393, 584, 1329, 5245, 293, 1320, 294, 257], "temperature": 0.0, "avg_logprob": -0.21394420706707498, "compression_ratio": 1.5792079207920793, "no_speech_prob": 1.202944986289367e-05}, {"id": 220, "seek": 99500, "start": 1002.24, "end": 1004.24, "text": " Glob a match and", "tokens": [10786, 65, 257, 2995, 293], "temperature": 0.0, "avg_logprob": -0.21394420706707498, "compression_ratio": 1.5792079207920793, "no_speech_prob": 1.202944986289367e-05}, {"id": 221, "seek": 99500, "start": 1004.28, "end": 1006.88, "text": " so this is going to show all the convex models and", "tokens": [370, 341, 307, 516, 281, 855, 439, 264, 42432, 5245, 293], "temperature": 0.0, "avg_logprob": -0.21394420706707498, "compression_ratio": 1.5792079207920793, "no_speech_prob": 1.202944986289367e-05}, {"id": 222, "seek": 99500, "start": 1007.48, "end": 1012.2, "text": " Here I can find the ones that I just saw and all I need to do is when I create the vision learner", "tokens": [1692, 286, 393, 915, 264, 2306, 300, 286, 445, 1866, 293, 439, 286, 643, 281, 360, 307, 562, 286, 1884, 264, 5201, 33347], "temperature": 0.0, "avg_logprob": -0.21394420706707498, "compression_ratio": 1.5792079207920793, "no_speech_prob": 1.202944986289367e-05}, {"id": 223, "seek": 99500, "start": 1012.2, "end": 1016.16, "text": " I just put the name of the model in as a string", "tokens": [286, 445, 829, 264, 1315, 295, 264, 2316, 294, 382, 257, 6798], "temperature": 0.0, "avg_logprob": -0.21394420706707498, "compression_ratio": 1.5792079207920793, "no_speech_prob": 1.202944986289367e-05}, {"id": 224, "seek": 99500, "start": 1016.84, "end": 1019.24, "text": " Okay, so you'll see earlier", "tokens": [1033, 11, 370, 291, 603, 536, 3071], "temperature": 0.0, "avg_logprob": -0.21394420706707498, "compression_ratio": 1.5792079207920793, "no_speech_prob": 1.202944986289367e-05}, {"id": 225, "seek": 101924, "start": 1019.24, "end": 1025.8, "text": " This one is not a string. That's because it's a model that fast AI provides the library", "tokens": [639, 472, 307, 406, 257, 6798, 13, 663, 311, 570, 309, 311, 257, 2316, 300, 2370, 7318, 6417, 264, 6405], "temperature": 0.0, "avg_logprob": -0.19308223327000937, "compression_ratio": 1.6387665198237886, "no_speech_prob": 7.183115030784393e-06}, {"id": 226, "seek": 101924, "start": 1027.8, "end": 1029.8, "text": " Fast AI only provides a pretty small number", "tokens": [15968, 7318, 787, 6417, 257, 1238, 1359, 1230], "temperature": 0.0, "avg_logprob": -0.19308223327000937, "compression_ratio": 1.6387665198237886, "no_speech_prob": 7.183115030784393e-06}, {"id": 227, "seek": 101924, "start": 1030.88, "end": 1034.8, "text": " So if you install Tim's you need to pip install Tim or conda install Tim", "tokens": [407, 498, 291, 3625, 7172, 311, 291, 643, 281, 8489, 3625, 7172, 420, 2224, 64, 3625, 7172], "temperature": 0.0, "avg_logprob": -0.19308223327000937, "compression_ratio": 1.6387665198237886, "no_speech_prob": 7.183115030784393e-06}, {"id": 228, "seek": 101924, "start": 1035.44, "end": 1038.28, "text": " You'll get hundreds more and you put that in a string", "tokens": [509, 603, 483, 6779, 544, 293, 291, 829, 300, 294, 257, 6798], "temperature": 0.0, "avg_logprob": -0.19308223327000937, "compression_ratio": 1.6387665198237886, "no_speech_prob": 7.183115030784393e-06}, {"id": 229, "seek": 101924, "start": 1039.08, "end": 1046.6, "text": " So if I now train that the time for these epochs goes from 20 seconds to 27 seconds, so it is a little bit slower", "tokens": [407, 498, 286, 586, 3847, 300, 264, 565, 337, 613, 30992, 28346, 1709, 490, 945, 3949, 281, 7634, 3949, 11, 370, 309, 307, 257, 707, 857, 14009], "temperature": 0.0, "avg_logprob": -0.19308223327000937, "compression_ratio": 1.6387665198237886, "no_speech_prob": 7.183115030784393e-06}, {"id": 230, "seek": 104660, "start": 1046.6, "end": 1049.52, "text": " but the accuracy goes from", "tokens": [457, 264, 14170, 1709, 490], "temperature": 0.0, "avg_logprob": -0.25240693773542133, "compression_ratio": 1.446808510638298, "no_speech_prob": 5.862713351234561e-06}, {"id": 231, "seek": 104660, "start": 1050.28, "end": 1051.84, "text": " 7.2 percent", "tokens": [1614, 13, 17, 3043], "temperature": 0.0, "avg_logprob": -0.25240693773542133, "compression_ratio": 1.446808510638298, "no_speech_prob": 5.862713351234561e-06}, {"id": 232, "seek": 104660, "start": 1051.84, "end": 1056.6, "text": " Down to 5.5 percent. So, you know, that's a pretty big relative difference", "tokens": [9506, 281, 1025, 13, 20, 3043, 13, 407, 11, 291, 458, 11, 300, 311, 257, 1238, 955, 4972, 2649], "temperature": 0.0, "avg_logprob": -0.25240693773542133, "compression_ratio": 1.446808510638298, "no_speech_prob": 5.862713351234561e-06}, {"id": 233, "seek": 104660, "start": 1059.1999999999998, "end": 1067.32, "text": " 7.2 divided by 5.5. Yeah, it's about a 30% improvement. So that's pretty fantastic and you know, it's", "tokens": [1614, 13, 17, 6666, 538, 1025, 13, 20, 13, 865, 11, 309, 311, 466, 257, 2217, 4, 10444, 13, 407, 300, 311, 1238, 5456, 293, 291, 458, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.25240693773542133, "compression_ratio": 1.446808510638298, "no_speech_prob": 5.862713351234561e-06}, {"id": 234, "seek": 104660, "start": 1068.8, "end": 1070.8, "text": " It's been a few years honestly", "tokens": [467, 311, 668, 257, 1326, 924, 6095], "temperature": 0.0, "avg_logprob": -0.25240693773542133, "compression_ratio": 1.446808510638298, "no_speech_prob": 5.862713351234561e-06}, {"id": 235, "seek": 104660, "start": 1071.32, "end": 1073.32, "text": " Since we've seen anything", "tokens": [4162, 321, 600, 1612, 1340], "temperature": 0.0, "avg_logprob": -0.25240693773542133, "compression_ratio": 1.446808510638298, "no_speech_prob": 5.862713351234561e-06}, {"id": 236, "seek": 107332, "start": 1073.32, "end": 1079.52, "text": " Really beat resnet that's that's widely available and usable on regular GPUs", "tokens": [4083, 4224, 725, 7129, 300, 311, 300, 311, 13371, 2435, 293, 29975, 322, 3890, 18407, 82], "temperature": 0.0, "avg_logprob": -0.23937729729546442, "compression_ratio": 1.6590909090909092, "no_speech_prob": 2.4298884454765357e-05}, {"id": 237, "seek": 107332, "start": 1080.12, "end": 1085.6, "text": " So this is this is a big step and so this is a you know, there's a few architectures nowadays that really are", "tokens": [407, 341, 307, 341, 307, 257, 955, 1823, 293, 370, 341, 307, 257, 291, 458, 11, 456, 311, 257, 1326, 6331, 1303, 13434, 300, 534, 366], "temperature": 0.0, "avg_logprob": -0.23937729729546442, "compression_ratio": 1.6590909090909092, "no_speech_prob": 2.4298884454765357e-05}, {"id": 238, "seek": 107332, "start": 1086.8799999999999, "end": 1091.9199999999998, "text": " Probably better choices a lot of the time and these cons so if you are not sure what to use", "tokens": [9210, 1101, 7994, 257, 688, 295, 264, 565, 293, 613, 1014, 370, 498, 291, 366, 406, 988, 437, 281, 764], "temperature": 0.0, "avg_logprob": -0.23937729729546442, "compression_ratio": 1.6590909090909092, "no_speech_prob": 2.4298884454765357e-05}, {"id": 239, "seek": 107332, "start": 1092.4399999999998, "end": 1094.4399999999998, "text": " Try these conv next architectures", "tokens": [6526, 613, 3754, 958, 6331, 1303], "temperature": 0.0, "avg_logprob": -0.23937729729546442, "compression_ratio": 1.6590909090909092, "no_speech_prob": 2.4298884454765357e-05}, {"id": 240, "seek": 107332, "start": 1095.04, "end": 1097.62, "text": " You might wonder what the names are about. Obviously", "tokens": [509, 1062, 2441, 437, 264, 5288, 366, 466, 13, 7580], "temperature": 0.0, "avg_logprob": -0.23937729729546442, "compression_ratio": 1.6590909090909092, "no_speech_prob": 2.4298884454765357e-05}, {"id": 241, "seek": 109762, "start": 1097.62, "end": 1103.54, "text": " Tiny is more large. Etc is how big is the model? So that'll be how much memory is it going to take up?", "tokens": [39992, 307, 544, 2416, 13, 3790, 66, 307, 577, 955, 307, 264, 2316, 30, 407, 300, 603, 312, 577, 709, 4675, 307, 309, 516, 281, 747, 493, 30], "temperature": 0.0, "avg_logprob": -0.17818062937157786, "compression_ratio": 1.8208333333333333, "no_speech_prob": 8.397814781346824e-06}, {"id": 242, "seek": 109762, "start": 1103.54, "end": 1105.54, "text": " How fast is it?", "tokens": [1012, 2370, 307, 309, 30], "temperature": 0.0, "avg_logprob": -0.17818062937157786, "compression_ratio": 1.8208333333333333, "no_speech_prob": 8.397814781346824e-06}, {"id": 243, "seek": 109762, "start": 1106.1, "end": 1107.6599999999999, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.17818062937157786, "compression_ratio": 1.8208333333333333, "no_speech_prob": 8.397814781346824e-06}, {"id": 244, "seek": 109762, "start": 1107.6599999999999, "end": 1111.9399999999998, "text": " Then these ones here that say in 22 FT 1k", "tokens": [1396, 613, 2306, 510, 300, 584, 294, 5853, 46675, 502, 74], "temperature": 0.0, "avg_logprob": -0.17818062937157786, "compression_ratio": 1.8208333333333333, "no_speech_prob": 8.397814781346824e-06}, {"id": 245, "seek": 109762, "start": 1112.54, "end": 1117.78, "text": " These ones have been trained on more data. So image net there's two different image net data sets", "tokens": [1981, 2306, 362, 668, 8895, 322, 544, 1412, 13, 407, 3256, 2533, 456, 311, 732, 819, 3256, 2533, 1412, 6352], "temperature": 0.0, "avg_logprob": -0.17818062937157786, "compression_ratio": 1.8208333333333333, "no_speech_prob": 8.397814781346824e-06}, {"id": 246, "seek": 109762, "start": 1117.78, "end": 1123.4799999999998, "text": " There's one that's got a thousand categories of pictures and there's another one that's got 22,000 categories of pictures", "tokens": [821, 311, 472, 300, 311, 658, 257, 4714, 10479, 295, 5242, 293, 456, 311, 1071, 472, 300, 311, 658, 5853, 11, 1360, 10479, 295, 5242], "temperature": 0.0, "avg_logprob": -0.17818062937157786, "compression_ratio": 1.8208333333333333, "no_speech_prob": 8.397814781346824e-06}, {"id": 247, "seek": 112348, "start": 1123.48, "end": 1128.04, "text": " So this is trained on the one with 22,000 categories of pictures", "tokens": [407, 341, 307, 8895, 322, 264, 472, 365, 5853, 11, 1360, 10479, 295, 5242], "temperature": 0.0, "avg_logprob": -0.14889184633890787, "compression_ratio": 1.524229074889868, "no_speech_prob": 9.080235031433403e-06}, {"id": 248, "seek": 112348, "start": 1129.68, "end": 1136.72, "text": " So these are generally going to be more accurate on kind of standard photos of natural objects", "tokens": [407, 613, 366, 5101, 516, 281, 312, 544, 8559, 322, 733, 295, 3832, 5787, 295, 3303, 6565], "temperature": 0.0, "avg_logprob": -0.14889184633890787, "compression_ratio": 1.524229074889868, "no_speech_prob": 9.080235031433403e-06}, {"id": 249, "seek": 112348, "start": 1138.52, "end": 1143.7, "text": " Okay, so from there I exported my model and that's the end okay, so now I've trained my model and I'm all done", "tokens": [1033, 11, 370, 490, 456, 286, 42055, 452, 2316, 293, 300, 311, 264, 917, 1392, 11, 370, 586, 286, 600, 8895, 452, 2316, 293, 286, 478, 439, 1096], "temperature": 0.0, "avg_logprob": -0.14889184633890787, "compression_ratio": 1.524229074889868, "no_speech_prob": 9.080235031433403e-06}, {"id": 250, "seek": 112348, "start": 1144.68, "end": 1149.0, "text": " You know other things you could do obviously is add more epochs for example", "tokens": [509, 458, 661, 721, 291, 727, 360, 2745, 307, 909, 544, 30992, 28346, 337, 1365], "temperature": 0.0, "avg_logprob": -0.14889184633890787, "compression_ratio": 1.524229074889868, "no_speech_prob": 9.080235031433403e-06}, {"id": 251, "seek": 114900, "start": 1149.0, "end": 1154.44, "text": " Add image augmentation. There's various things you can do, but you know, I found this this is actually pretty", "tokens": [5349, 3256, 14501, 19631, 13, 821, 311, 3683, 721, 291, 393, 360, 11, 457, 291, 458, 11, 286, 1352, 341, 341, 307, 767, 1238], "temperature": 0.0, "avg_logprob": -0.181035294824717, "compression_ratio": 1.5913043478260869, "no_speech_prob": 8.397685633099172e-06}, {"id": 252, "seek": 114900, "start": 1155.72, "end": 1157.72, "text": " Pretty hard to beat this by much", "tokens": [10693, 1152, 281, 4224, 341, 538, 709], "temperature": 0.0, "avg_logprob": -0.181035294824717, "compression_ratio": 1.5913043478260869, "no_speech_prob": 8.397685633099172e-06}, {"id": 253, "seek": 114900, "start": 1158.24, "end": 1161.2, "text": " If any of you find you can do better, I'd love to hear about it", "tokens": [759, 604, 295, 291, 915, 291, 393, 360, 1101, 11, 286, 1116, 959, 281, 1568, 466, 309], "temperature": 0.0, "avg_logprob": -0.181035294824717, "compression_ratio": 1.5913043478260869, "no_speech_prob": 8.397685633099172e-06}, {"id": 254, "seek": 114900, "start": 1162.16, "end": 1169.0, "text": " So then to turn that into an application. I just did the same thing that we saw last week, which was to", "tokens": [407, 550, 281, 1261, 300, 666, 364, 3861, 13, 286, 445, 630, 264, 912, 551, 300, 321, 1866, 1036, 1243, 11, 597, 390, 281], "temperature": 0.0, "avg_logprob": -0.181035294824717, "compression_ratio": 1.5913043478260869, "no_speech_prob": 8.397685633099172e-06}, {"id": 255, "seek": 114900, "start": 1170.04, "end": 1172.04, "text": " Load the learner", "tokens": [48408, 264, 33347], "temperature": 0.0, "avg_logprob": -0.181035294824717, "compression_ratio": 1.5913043478260869, "no_speech_prob": 8.397685633099172e-06}, {"id": 256, "seek": 114900, "start": 1173.12, "end": 1175.12, "text": " As is something I did want to show you", "tokens": [1018, 307, 746, 286, 630, 528, 281, 855, 291], "temperature": 0.0, "avg_logprob": -0.181035294824717, "compression_ratio": 1.5913043478260869, "no_speech_prob": 8.397685633099172e-06}, {"id": 257, "seek": 117512, "start": 1175.12, "end": 1180.6399999999999, "text": " At the learner once we load it and call predict spits out a list of 37 numbers", "tokens": [1711, 264, 33347, 1564, 321, 3677, 309, 293, 818, 6069, 637, 1208, 484, 257, 1329, 295, 13435, 3547], "temperature": 0.0, "avg_logprob": -0.13663657960437592, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.7107418645755388e-05}, {"id": 258, "seek": 117512, "start": 1180.76, "end": 1186.0, "text": " That's because there are 37 breeds of dog and cat. So these are the probability of each of those breeds", "tokens": [663, 311, 570, 456, 366, 13435, 41609, 295, 3000, 293, 3857, 13, 407, 613, 366, 264, 8482, 295, 1184, 295, 729, 41609], "temperature": 0.0, "avg_logprob": -0.13663657960437592, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.7107418645755388e-05}, {"id": 259, "seek": 117512, "start": 1186.32, "end": 1188.32, "text": " What order they are they in?", "tokens": [708, 1668, 436, 366, 436, 294, 30], "temperature": 0.0, "avg_logprob": -0.13663657960437592, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.7107418645755388e-05}, {"id": 260, "seek": 117512, "start": 1188.8, "end": 1190.8, "text": " That's an important question", "tokens": [663, 311, 364, 1021, 1168], "temperature": 0.0, "avg_logprob": -0.13663657960437592, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.7107418645755388e-05}, {"id": 261, "seek": 117512, "start": 1190.84, "end": 1195.8799999999999, "text": " The answer is that fast AI always stores this information about categories", "tokens": [440, 1867, 307, 300, 2370, 7318, 1009, 9512, 341, 1589, 466, 10479], "temperature": 0.0, "avg_logprob": -0.13663657960437592, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.7107418645755388e-05}, {"id": 262, "seek": 117512, "start": 1195.8799999999999, "end": 1202.56, "text": " This is a category in this case of dog or cat breed in something called the vocab object and it's inside the data loaders", "tokens": [639, 307, 257, 7719, 294, 341, 1389, 295, 3000, 420, 3857, 18971, 294, 746, 1219, 264, 2329, 455, 2657, 293, 309, 311, 1854, 264, 1412, 3677, 433], "temperature": 0.0, "avg_logprob": -0.13663657960437592, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.7107418645755388e-05}, {"id": 263, "seek": 120256, "start": 1202.56, "end": 1207.36, "text": " So we can grab those categories and that's just a list of strings just tells us the order", "tokens": [407, 321, 393, 4444, 729, 10479, 293, 300, 311, 445, 257, 1329, 295, 13985, 445, 5112, 505, 264, 1668], "temperature": 0.0, "avg_logprob": -0.2305133819580078, "compression_ratio": 1.8090452261306533, "no_speech_prob": 9.51587480813032e-06}, {"id": 264, "seek": 120256, "start": 1208.24, "end": 1215.62, "text": " So if we now zip together the categories and the probabilities will get back a dictionary that tells you", "tokens": [407, 498, 321, 586, 20730, 1214, 264, 10479, 293, 264, 33783, 486, 483, 646, 257, 25890, 300, 5112, 291], "temperature": 0.0, "avg_logprob": -0.2305133819580078, "compression_ratio": 1.8090452261306533, "no_speech_prob": 9.51587480813032e-06}, {"id": 265, "seek": 120256, "start": 1216.36, "end": 1218.08, "text": " Well like so", "tokens": [1042, 411, 370], "temperature": 0.0, "avg_logprob": -0.2305133819580078, "compression_ratio": 1.8090452261306533, "no_speech_prob": 9.51587480813032e-06}, {"id": 266, "seek": 120256, "start": 1218.08, "end": 1222.44, "text": " So here's that list of categories and here's the probability of each one", "tokens": [407, 510, 311, 300, 1329, 295, 10479, 293, 510, 311, 264, 8482, 295, 1184, 472], "temperature": 0.0, "avg_logprob": -0.2305133819580078, "compression_ratio": 1.8090452261306533, "no_speech_prob": 9.51587480813032e-06}, {"id": 267, "seek": 120256, "start": 1223.44, "end": 1224.96, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.2305133819580078, "compression_ratio": 1.8090452261306533, "no_speech_prob": 9.51587480813032e-06}, {"id": 268, "seek": 120256, "start": 1224.96, "end": 1228.96, "text": " This was a basset hound so they can see yep almost certainly a basset hound", "tokens": [639, 390, 257, 10136, 302, 276, 554, 370, 436, 393, 536, 18633, 1920, 3297, 257, 10136, 302, 276, 554], "temperature": 0.0, "avg_logprob": -0.2305133819580078, "compression_ratio": 1.8090452261306533, "no_speech_prob": 9.51587480813032e-06}, {"id": 269, "seek": 122896, "start": 1228.96, "end": 1230.96, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.2549964731389826, "compression_ratio": 1.6326530612244898, "no_speech_prob": 2.3687769044045126e-06}, {"id": 270, "seek": 122896, "start": 1231.0, "end": 1237.08, "text": " From there just like last week we can go and create our interface and then and then launch it", "tokens": [3358, 456, 445, 411, 1036, 1243, 321, 393, 352, 293, 1884, 527, 9226, 293, 550, 293, 550, 4025, 309], "temperature": 0.0, "avg_logprob": -0.2549964731389826, "compression_ratio": 1.6326530612244898, "no_speech_prob": 2.3687769044045126e-06}, {"id": 271, "seek": 122896, "start": 1238.32, "end": 1241.1200000000001, "text": " And there we go, okay, so", "tokens": [400, 456, 321, 352, 11, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.2549964731389826, "compression_ratio": 1.6326530612244898, "no_speech_prob": 2.3687769044045126e-06}, {"id": 272, "seek": 122896, "start": 1242.16, "end": 1245.72, "text": " What did we just do really? What is this magic?", "tokens": [708, 630, 321, 445, 360, 534, 30, 708, 307, 341, 5585, 30], "temperature": 0.0, "avg_logprob": -0.2549964731389826, "compression_ratio": 1.6326530612244898, "no_speech_prob": 2.3687769044045126e-06}, {"id": 273, "seek": 122896, "start": 1246.52, "end": 1248.52, "text": " model dot pickle file", "tokens": [2316, 5893, 31433, 3991], "temperature": 0.0, "avg_logprob": -0.2549964731389826, "compression_ratio": 1.6326530612244898, "no_speech_prob": 2.3687769044045126e-06}, {"id": 274, "seek": 122896, "start": 1248.76, "end": 1252.64, "text": " So we can take a look at the model dot pickle file", "tokens": [407, 321, 393, 747, 257, 574, 412, 264, 2316, 5893, 31433, 3991], "temperature": 0.0, "avg_logprob": -0.2549964731389826, "compression_ratio": 1.6326530612244898, "no_speech_prob": 2.3687769044045126e-06}, {"id": 275, "seek": 125264, "start": 1252.64, "end": 1258.6000000000001, "text": " It's an object type called a learner and a learner has two main things in it", "tokens": [467, 311, 364, 2657, 2010, 1219, 257, 33347, 293, 257, 33347, 575, 732, 2135, 721, 294, 309], "temperature": 0.0, "avg_logprob": -0.1448605548904603, "compression_ratio": 1.6875, "no_speech_prob": 3.1875465538178105e-06}, {"id": 276, "seek": 125264, "start": 1258.8400000000001, "end": 1266.16, "text": " The first is the list of pre-processing steps that you did to turn your images into things of the model and", "tokens": [440, 700, 307, 264, 1329, 295, 659, 12, 41075, 278, 4439, 300, 291, 630, 281, 1261, 428, 5267, 666, 721, 295, 264, 2316, 293], "temperature": 0.0, "avg_logprob": -0.1448605548904603, "compression_ratio": 1.6875, "no_speech_prob": 3.1875465538178105e-06}, {"id": 277, "seek": 125264, "start": 1266.5200000000002, "end": 1268.5200000000002, "text": " That's basically", "tokens": [663, 311, 1936], "temperature": 0.0, "avg_logprob": -0.1448605548904603, "compression_ratio": 1.6875, "no_speech_prob": 3.1875465538178105e-06}, {"id": 278, "seek": 125264, "start": 1271.96, "end": 1273.96, "text": " This information here", "tokens": [639, 1589, 510], "temperature": 0.0, "avg_logprob": -0.1448605548904603, "compression_ratio": 1.6875, "no_speech_prob": 3.1875465538178105e-06}, {"id": 279, "seek": 127396, "start": 1273.96, "end": 1282.4, "text": " So it's your data blocks or your image data loaders or whatever and then the second thing most importantly is the trained model and", "tokens": [407, 309, 311, 428, 1412, 8474, 420, 428, 3256, 1412, 3677, 433, 420, 2035, 293, 550, 264, 1150, 551, 881, 8906, 307, 264, 8895, 2316, 293], "temperature": 0.0, "avg_logprob": -0.15750950964811805, "compression_ratio": 1.7068273092369477, "no_speech_prob": 6.8542472035915125e-06}, {"id": 280, "seek": 127396, "start": 1283.1200000000001, "end": 1286.8400000000001, "text": " So you can actually grab the trained model by just grabbing the dot model", "tokens": [407, 291, 393, 767, 4444, 264, 8895, 2316, 538, 445, 23771, 264, 5893, 2316], "temperature": 0.0, "avg_logprob": -0.15750950964811805, "compression_ratio": 1.7068273092369477, "no_speech_prob": 6.8542472035915125e-06}, {"id": 281, "seek": 127396, "start": 1287.4, "end": 1293.6000000000001, "text": " Attribute so I'm just going to call that M and then if I type M I can look at the model and so here it is", "tokens": [7298, 2024, 1169, 370, 286, 478, 445, 516, 281, 818, 300, 376, 293, 550, 498, 286, 2010, 376, 286, 393, 574, 412, 264, 2316, 293, 370, 510, 309, 307], "temperature": 0.0, "avg_logprob": -0.15750950964811805, "compression_ratio": 1.7068273092369477, "no_speech_prob": 6.8542472035915125e-06}, {"id": 282, "seek": 127396, "start": 1295.24, "end": 1302.6000000000001, "text": " Lots of stuff. So what is this stuff? Well, we'll learn about it all over time. But basically what you'll find is", "tokens": [15908, 295, 1507, 13, 407, 437, 307, 341, 1507, 30, 1042, 11, 321, 603, 1466, 466, 309, 439, 670, 565, 13, 583, 1936, 437, 291, 603, 915, 307], "temperature": 0.0, "avg_logprob": -0.15750950964811805, "compression_ratio": 1.7068273092369477, "no_speech_prob": 6.8542472035915125e-06}, {"id": 283, "seek": 130260, "start": 1302.6, "end": 1309.6399999999999, "text": " It contains lots of layers because this is a deep learning model and you can see it's kind of like a tree", "tokens": [467, 8306, 3195, 295, 7914, 570, 341, 307, 257, 2452, 2539, 2316, 293, 291, 393, 536, 309, 311, 733, 295, 411, 257, 4230], "temperature": 0.0, "avg_logprob": -0.21359107494354249, "compression_ratio": 1.8186813186813187, "no_speech_prob": 7.766866474412382e-06}, {"id": 284, "seek": 130260, "start": 1309.6399999999999, "end": 1312.84, "text": " That's because lots of the layers themselves consist of layers", "tokens": [663, 311, 570, 3195, 295, 264, 7914, 2969, 4603, 295, 7914], "temperature": 0.0, "avg_logprob": -0.21359107494354249, "compression_ratio": 1.8186813186813187, "no_speech_prob": 7.766866474412382e-06}, {"id": 285, "seek": 130260, "start": 1313.9199999999998, "end": 1319.56, "text": " so there's a whole layer called the Tim body which is most of it and", "tokens": [370, 456, 311, 257, 1379, 4583, 1219, 264, 7172, 1772, 597, 307, 881, 295, 309, 293], "temperature": 0.0, "avg_logprob": -0.21359107494354249, "compression_ratio": 1.8186813186813187, "no_speech_prob": 7.766866474412382e-06}, {"id": 286, "seek": 130260, "start": 1320.1999999999998, "end": 1322.8799999999999, "text": " then right at the end there's a second layer called sequential and", "tokens": [550, 558, 412, 264, 917, 456, 311, 257, 1150, 4583, 1219, 42881, 293], "temperature": 0.0, "avg_logprob": -0.21359107494354249, "compression_ratio": 1.8186813186813187, "no_speech_prob": 7.766866474412382e-06}, {"id": 287, "seek": 130260, "start": 1323.9199999999998, "end": 1325.9199999999998, "text": " then the Tim body contains", "tokens": [550, 264, 7172, 1772, 8306], "temperature": 0.0, "avg_logprob": -0.21359107494354249, "compression_ratio": 1.8186813186813187, "no_speech_prob": 7.766866474412382e-06}, {"id": 288, "seek": 132592, "start": 1325.92, "end": 1333.72, "text": " Something called model and it can then it contains something called stem and something called stages and then stages contain zero", "tokens": [6595, 1219, 2316, 293, 309, 393, 550, 309, 8306, 746, 1219, 12312, 293, 746, 1219, 10232, 293, 550, 10232, 5304, 4018], "temperature": 0.0, "avg_logprob": -0.207087157874979, "compression_ratio": 1.785, "no_speech_prob": 6.962139650568133e-06}, {"id": 289, "seek": 132592, "start": 1334.28, "end": 1336.28, "text": " one-two, etc", "tokens": [472, 12, 20534, 11, 5183], "temperature": 0.0, "avg_logprob": -0.207087157874979, "compression_ratio": 1.785, "no_speech_prob": 6.962139650568133e-06}, {"id": 290, "seek": 132592, "start": 1336.92, "end": 1340.5600000000002, "text": " So what is all this stuff? Well, let's take a look at one of them", "tokens": [407, 437, 307, 439, 341, 1507, 30, 1042, 11, 718, 311, 747, 257, 574, 412, 472, 295, 552], "temperature": 0.0, "avg_logprob": -0.207087157874979, "compression_ratio": 1.785, "no_speech_prob": 6.962139650568133e-06}, {"id": 291, "seek": 132592, "start": 1341.68, "end": 1345.48, "text": " So to take a look at one of them. There's a really convenient", "tokens": [407, 281, 747, 257, 574, 412, 472, 295, 552, 13, 821, 311, 257, 534, 10851], "temperature": 0.0, "avg_logprob": -0.207087157874979, "compression_ratio": 1.785, "no_speech_prob": 6.962139650568133e-06}, {"id": 292, "seek": 132592, "start": 1346.24, "end": 1351.3200000000002, "text": " method in pytorch called get sub module where we can pass in a kind of a", "tokens": [3170, 294, 25878, 284, 339, 1219, 483, 1422, 10088, 689, 321, 393, 1320, 294, 257, 733, 295, 257], "temperature": 0.0, "avg_logprob": -0.207087157874979, "compression_ratio": 1.785, "no_speech_prob": 6.962139650568133e-06}, {"id": 293, "seek": 132592, "start": 1351.96, "end": 1353.88, "text": " dotted string", "tokens": [37459, 6798], "temperature": 0.0, "avg_logprob": -0.207087157874979, "compression_ratio": 1.785, "no_speech_prob": 6.962139650568133e-06}, {"id": 294, "seek": 135388, "start": 1353.88, "end": 1360.14, "text": " Navigating through this hierarchy. So zero model stem one goes zero model stem one", "tokens": [9219, 328, 990, 807, 341, 22333, 13, 407, 4018, 2316, 12312, 472, 1709, 4018, 2316, 12312, 472], "temperature": 0.0, "avg_logprob": -0.17065985386188215, "compression_ratio": 1.8197424892703862, "no_speech_prob": 1.2805237929569557e-05}, {"id": 295, "seek": 135388, "start": 1360.8400000000001, "end": 1368.1200000000001, "text": " So this is going to return this layer norm 2d thing. So what is this layer norm 2d thing? Well, the key thing is", "tokens": [407, 341, 307, 516, 281, 2736, 341, 4583, 2026, 568, 67, 551, 13, 407, 437, 307, 341, 4583, 2026, 568, 67, 551, 30, 1042, 11, 264, 2141, 551, 307], "temperature": 0.0, "avg_logprob": -0.17065985386188215, "compression_ratio": 1.8197424892703862, "no_speech_prob": 1.2805237929569557e-05}, {"id": 296, "seek": 135388, "start": 1369.6000000000001, "end": 1376.16, "text": " It's got some code is the mathematical function that we talked about and then the other thing that we learned about is it has", "tokens": [467, 311, 658, 512, 3089, 307, 264, 18894, 2445, 300, 321, 2825, 466, 293, 550, 264, 661, 551, 300, 321, 3264, 466, 307, 309, 575], "temperature": 0.0, "avg_logprob": -0.17065985386188215, "compression_ratio": 1.8197424892703862, "no_speech_prob": 1.2805237929569557e-05}, {"id": 297, "seek": 135388, "start": 1377.24, "end": 1381.5400000000002, "text": " Parameters so we can list its parameters and look at this. It's just lots and lots and lots of numbers", "tokens": [34882, 6202, 370, 321, 393, 1329, 1080, 9834, 293, 574, 412, 341, 13, 467, 311, 445, 3195, 293, 3195, 293, 3195, 295, 3547], "temperature": 0.0, "avg_logprob": -0.17065985386188215, "compression_ratio": 1.8197424892703862, "no_speech_prob": 1.2805237929569557e-05}, {"id": 298, "seek": 138154, "start": 1381.54, "end": 1383.46, "text": " Let's", "tokens": [961, 311], "temperature": 0.0, "avg_logprob": -0.284953373734669, "compression_ratio": 1.5727272727272728, "no_speech_prob": 2.123357035088702e-06}, {"id": 299, "seek": 138154, "start": 1383.46, "end": 1389.8999999999999, "text": " Grab another example. We could have a look at zero dot model dot stages dot zero to blocks dot one dot MLP dot FC one and", "tokens": [20357, 1071, 1365, 13, 492, 727, 362, 257, 574, 412, 4018, 5893, 2316, 5893, 10232, 5893, 4018, 281, 8474, 5893, 472, 5893, 21601, 47, 5893, 27168, 472, 293], "temperature": 0.0, "avg_logprob": -0.284953373734669, "compression_ratio": 1.5727272727272728, "no_speech_prob": 2.123357035088702e-06}, {"id": 300, "seek": 138154, "start": 1390.78, "end": 1392.22, "text": " parameters", "tokens": [9834], "temperature": 0.0, "avg_logprob": -0.284953373734669, "compression_ratio": 1.5727272727272728, "no_speech_prob": 2.123357035088702e-06}, {"id": 301, "seek": 138154, "start": 1392.22, "end": 1394.22, "text": " another big bunch of numbers", "tokens": [1071, 955, 3840, 295, 3547], "temperature": 0.0, "avg_logprob": -0.284953373734669, "compression_ratio": 1.5727272727272728, "no_speech_prob": 2.123357035088702e-06}, {"id": 302, "seek": 138154, "start": 1394.5, "end": 1400.94, "text": " So what's going on here? What are these numbers and where it is did they come from?", "tokens": [407, 437, 311, 516, 322, 510, 30, 708, 366, 613, 3547, 293, 689, 309, 307, 630, 436, 808, 490, 30], "temperature": 0.0, "avg_logprob": -0.284953373734669, "compression_ratio": 1.5727272727272728, "no_speech_prob": 2.123357035088702e-06}, {"id": 303, "seek": 138154, "start": 1400.94, "end": 1406.42, "text": " And how come these numbers can figure out whether something is a basset hound or not?", "tokens": [400, 577, 808, 613, 3547, 393, 2573, 484, 1968, 746, 307, 257, 10136, 302, 276, 554, 420, 406, 30], "temperature": 0.0, "avg_logprob": -0.284953373734669, "compression_ratio": 1.5727272727272728, "no_speech_prob": 2.123357035088702e-06}, {"id": 304, "seek": 140642, "start": 1406.42, "end": 1410.0600000000002, "text": " Okay, so to", "tokens": [1033, 11, 370, 281], "temperature": 0.0, "avg_logprob": -0.29646435721975856, "compression_ratio": 1.392156862745098, "no_speech_prob": 1.191100295727665e-06}, {"id": 305, "seek": 140642, "start": 1414.22, "end": 1416.94, "text": " Answer that question we're going to have a look at a", "tokens": [24545, 300, 1168, 321, 434, 516, 281, 362, 257, 574, 412, 257], "temperature": 0.0, "avg_logprob": -0.29646435721975856, "compression_ratio": 1.392156862745098, "no_speech_prob": 1.191100295727665e-06}, {"id": 306, "seek": 140642, "start": 1418.8200000000002, "end": 1420.8200000000002, "text": " Kaggle notebook", "tokens": [48751, 22631, 21060], "temperature": 0.0, "avg_logprob": -0.29646435721975856, "compression_ratio": 1.392156862745098, "no_speech_prob": 1.191100295727665e-06}, {"id": 307, "seek": 140642, "start": 1423.5, "end": 1425.74, "text": " How does a neural network really work", "tokens": [1012, 775, 257, 18161, 3209, 534, 589], "temperature": 0.0, "avg_logprob": -0.29646435721975856, "compression_ratio": 1.392156862745098, "no_speech_prob": 1.191100295727665e-06}, {"id": 308, "seek": 140642, "start": 1427.3000000000002, "end": 1432.7, "text": " I've got a local version of it here, which I'm going to take you through and the basic idea is", "tokens": [286, 600, 658, 257, 2654, 3037, 295, 309, 510, 11, 597, 286, 478, 516, 281, 747, 291, 807, 293, 264, 3875, 1558, 307], "temperature": 0.0, "avg_logprob": -0.29646435721975856, "compression_ratio": 1.392156862745098, "no_speech_prob": 1.191100295727665e-06}, {"id": 309, "seek": 143270, "start": 1432.7, "end": 1436.3, "text": " Machine learning models are things that fit", "tokens": [22155, 2539, 5245, 366, 721, 300, 3318], "temperature": 0.0, "avg_logprob": -0.23731347884254894, "compression_ratio": 1.6396396396396395, "no_speech_prob": 5.507511559699196e-06}, {"id": 310, "seek": 143270, "start": 1437.06, "end": 1444.38, "text": " Functions to data so we start out with a very very flexible in fact an infinitely flexible as we've discussed function a neural network and", "tokens": [11166, 3916, 281, 1412, 370, 321, 722, 484, 365, 257, 588, 588, 11358, 294, 1186, 364, 36227, 11358, 382, 321, 600, 7152, 2445, 257, 18161, 3209, 293], "temperature": 0.0, "avg_logprob": -0.23731347884254894, "compression_ratio": 1.6396396396396395, "no_speech_prob": 5.507511559699196e-06}, {"id": 311, "seek": 143270, "start": 1445.48, "end": 1447.48, "text": " We get it to do a particular thing", "tokens": [492, 483, 309, 281, 360, 257, 1729, 551], "temperature": 0.0, "avg_logprob": -0.23731347884254894, "compression_ratio": 1.6396396396396395, "no_speech_prob": 5.507511559699196e-06}, {"id": 312, "seek": 143270, "start": 1448.1000000000001, "end": 1451.68, "text": " Which is to recognize the patterns in the data examples we give it", "tokens": [3013, 307, 281, 5521, 264, 8294, 294, 264, 1412, 5110, 321, 976, 309], "temperature": 0.0, "avg_logprob": -0.23731347884254894, "compression_ratio": 1.6396396396396395, "no_speech_prob": 5.507511559699196e-06}, {"id": 313, "seek": 143270, "start": 1453.5, "end": 1455.5, "text": " So let's do a much simpler example", "tokens": [407, 718, 311, 360, 257, 709, 18587, 1365], "temperature": 0.0, "avg_logprob": -0.23731347884254894, "compression_ratio": 1.6396396396396395, "no_speech_prob": 5.507511559699196e-06}, {"id": 314, "seek": 143270, "start": 1456.54, "end": 1460.28, "text": " Than a neural network. Let's do a quadratic", "tokens": [18289, 257, 18161, 3209, 13, 961, 311, 360, 257, 37262], "temperature": 0.0, "avg_logprob": -0.23731347884254894, "compression_ratio": 1.6396396396396395, "no_speech_prob": 5.507511559699196e-06}, {"id": 315, "seek": 146028, "start": 1460.28, "end": 1464.32, "text": " So let's create a function f which is 3x squared", "tokens": [407, 718, 311, 1884, 257, 2445, 283, 597, 307, 805, 87, 8889], "temperature": 0.0, "avg_logprob": -0.18814503628274667, "compression_ratio": 1.5545454545454545, "no_speech_prob": 1.844808252826624e-06}, {"id": 316, "seek": 146028, "start": 1465.32, "end": 1466.92, "text": " plus 2x", "tokens": [1804, 568, 87], "temperature": 0.0, "avg_logprob": -0.18814503628274667, "compression_ratio": 1.5545454545454545, "no_speech_prob": 1.844808252826624e-06}, {"id": 317, "seek": 146028, "start": 1466.92, "end": 1471.62, "text": " Plus 1 ok so it's a quadratic with coefficients 3 2 and 1", "tokens": [7721, 502, 3133, 370, 309, 311, 257, 37262, 365, 31994, 805, 568, 293, 502], "temperature": 0.0, "avg_logprob": -0.18814503628274667, "compression_ratio": 1.5545454545454545, "no_speech_prob": 1.844808252826624e-06}, {"id": 318, "seek": 146028, "start": 1472.2, "end": 1475.18, "text": " So we can plot that function f and give it a title", "tokens": [407, 321, 393, 7542, 300, 2445, 283, 293, 976, 309, 257, 4876], "temperature": 0.0, "avg_logprob": -0.18814503628274667, "compression_ratio": 1.5545454545454545, "no_speech_prob": 1.844808252826624e-06}, {"id": 319, "seek": 146028, "start": 1475.48, "end": 1479.6, "text": " If you haven't seen this before things between dollar signs is what's called latech", "tokens": [759, 291, 2378, 380, 1612, 341, 949, 721, 1296, 7241, 7880, 307, 437, 311, 1219, 3469, 339], "temperature": 0.0, "avg_logprob": -0.18814503628274667, "compression_ratio": 1.5545454545454545, "no_speech_prob": 1.844808252826624e-06}, {"id": 320, "seek": 146028, "start": 1479.6, "end": 1482.96, "text": " It's basically how we can create kind of typeset mathematical equations", "tokens": [467, 311, 1936, 577, 321, 393, 1884, 733, 295, 3467, 302, 18894, 11787], "temperature": 0.0, "avg_logprob": -0.18814503628274667, "compression_ratio": 1.5545454545454545, "no_speech_prob": 1.844808252826624e-06}, {"id": 321, "seek": 148296, "start": 1482.96, "end": 1488.28, "text": " Okay, so let's run that and", "tokens": [1033, 11, 370, 718, 311, 1190, 300, 293], "temperature": 0.0, "avg_logprob": -0.18556252232304327, "compression_ratio": 1.7956521739130435, "no_speech_prob": 5.093650088383583e-06}, {"id": 322, "seek": 148296, "start": 1491.08, "end": 1496.48, "text": " So here you can see the function here you can see the title I passed it and here is our quadratic", "tokens": [407, 510, 291, 393, 536, 264, 2445, 510, 291, 393, 536, 264, 4876, 286, 4678, 309, 293, 510, 307, 527, 37262], "temperature": 0.0, "avg_logprob": -0.18556252232304327, "compression_ratio": 1.7956521739130435, "no_speech_prob": 5.093650088383583e-06}, {"id": 323, "seek": 148296, "start": 1497.1200000000001, "end": 1499.1200000000001, "text": " Okay, so what we're going to do is we're going to", "tokens": [1033, 11, 370, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.18556252232304327, "compression_ratio": 1.7956521739130435, "no_speech_prob": 5.093650088383583e-06}, {"id": 324, "seek": 148296, "start": 1500.16, "end": 1502.16, "text": " Imagine that we don't know that's the true", "tokens": [11739, 300, 321, 500, 380, 458, 300, 311, 264, 2074], "temperature": 0.0, "avg_logprob": -0.18556252232304327, "compression_ratio": 1.7956521739130435, "no_speech_prob": 5.093650088383583e-06}, {"id": 325, "seek": 148296, "start": 1502.96, "end": 1508.32, "text": " Mathematical function we're trying to find as it's obviously much simpler than the function that figures out whether an image is a", "tokens": [15776, 8615, 804, 2445, 321, 434, 1382, 281, 915, 382, 309, 311, 2745, 709, 18587, 813, 264, 2445, 300, 9624, 484, 1968, 364, 3256, 307, 257], "temperature": 0.0, "avg_logprob": -0.18556252232304327, "compression_ratio": 1.7956521739130435, "no_speech_prob": 5.093650088383583e-06}, {"id": 326, "seek": 148296, "start": 1509.28, "end": 1512.04, "text": " Basset hound or not that we're just going to start super simple", "tokens": [29626, 302, 276, 554, 420, 406, 300, 321, 434, 445, 516, 281, 722, 1687, 2199], "temperature": 0.0, "avg_logprob": -0.18556252232304327, "compression_ratio": 1.7956521739130435, "no_speech_prob": 5.093650088383583e-06}, {"id": 327, "seek": 151204, "start": 1512.04, "end": 1514.96, "text": " So this is the real function and we're going to try to", "tokens": [407, 341, 307, 264, 957, 2445, 293, 321, 434, 516, 281, 853, 281], "temperature": 0.0, "avg_logprob": -0.22276198261916036, "compression_ratio": 1.6283185840707965, "no_speech_prob": 2.0904371922370046e-06}, {"id": 328, "seek": 151204, "start": 1516.12, "end": 1518.12, "text": " recreate it from some data", "tokens": [25833, 309, 490, 512, 1412], "temperature": 0.0, "avg_logprob": -0.22276198261916036, "compression_ratio": 1.6283185840707965, "no_speech_prob": 2.0904371922370046e-06}, {"id": 329, "seek": 151204, "start": 1520.72, "end": 1525.3999999999999, "text": " Now it's going to be very helpful if we have an easier way of creating different quadratics", "tokens": [823, 309, 311, 516, 281, 312, 588, 4961, 498, 321, 362, 364, 3571, 636, 295, 4084, 819, 10787, 4481, 1167], "temperature": 0.0, "avg_logprob": -0.22276198261916036, "compression_ratio": 1.6283185840707965, "no_speech_prob": 2.0904371922370046e-06}, {"id": 330, "seek": 151204, "start": 1526.1599999999999, "end": 1529.36, "text": " So I've defined a kind of a general form of a quadratic here", "tokens": [407, 286, 600, 7642, 257, 733, 295, 257, 2674, 1254, 295, 257, 37262, 510], "temperature": 0.0, "avg_logprob": -0.22276198261916036, "compression_ratio": 1.6283185840707965, "no_speech_prob": 2.0904371922370046e-06}, {"id": 331, "seek": 151204, "start": 1529.72, "end": 1535.96, "text": " If the with coefficients a B and C and at some particular point X it's going to be a x squared plus B X plus C", "tokens": [759, 264, 365, 31994, 257, 363, 293, 383, 293, 412, 512, 1729, 935, 1783, 309, 311, 516, 281, 312, 257, 2031, 8889, 1804, 363, 1783, 1804, 383], "temperature": 0.0, "avg_logprob": -0.22276198261916036, "compression_ratio": 1.6283185840707965, "no_speech_prob": 2.0904371922370046e-06}, {"id": 332, "seek": 151204, "start": 1537.8, "end": 1539.8, "text": " And so let's test that", "tokens": [400, 370, 718, 311, 1500, 300], "temperature": 0.0, "avg_logprob": -0.22276198261916036, "compression_ratio": 1.6283185840707965, "no_speech_prob": 2.0904371922370046e-06}, {"id": 333, "seek": 153980, "start": 1539.8, "end": 1545.8799999999999, "text": " Okay, so that's a for X equals one point five that's three X squared plus two X plus one", "tokens": [1033, 11, 370, 300, 311, 257, 337, 1783, 6915, 472, 935, 1732, 300, 311, 1045, 1783, 8889, 1804, 732, 1783, 1804, 472], "temperature": 0.0, "avg_logprob": -0.1970916748046875, "compression_ratio": 1.5868725868725868, "no_speech_prob": 3.5559432944864966e-06}, {"id": 334, "seek": 153980, "start": 1545.8799999999999, "end": 1547.96, "text": " Which is the quadratic we were did before?", "tokens": [3013, 307, 264, 37262, 321, 645, 630, 949, 30], "temperature": 0.0, "avg_logprob": -0.1970916748046875, "compression_ratio": 1.5868725868725868, "no_speech_prob": 3.5559432944864966e-06}, {"id": 335, "seek": 153980, "start": 1551.8, "end": 1557.24, "text": " Now we're going to want to create lots of different quadratics to test them out and find out which one's best", "tokens": [823, 321, 434, 516, 281, 528, 281, 1884, 3195, 295, 819, 10787, 4481, 1167, 281, 1500, 552, 484, 293, 915, 484, 597, 472, 311, 1151], "temperature": 0.0, "avg_logprob": -0.1970916748046875, "compression_ratio": 1.5868725868725868, "no_speech_prob": 3.5559432944864966e-06}, {"id": 336, "seek": 153980, "start": 1558.76, "end": 1560.44, "text": " So this is a", "tokens": [407, 341, 307, 257], "temperature": 0.0, "avg_logprob": -0.1970916748046875, "compression_ratio": 1.5868725868725868, "no_speech_prob": 3.5559432944864966e-06}, {"id": 337, "seek": 153980, "start": 1560.44, "end": 1565.2, "text": " Somewhat advanced but very very helpful feature of Python that's worth mining if you're not familiar with it", "tokens": [2188, 5479, 7339, 457, 588, 588, 4961, 4111, 295, 15329, 300, 311, 3163, 15512, 498, 291, 434, 406, 4963, 365, 309], "temperature": 0.0, "avg_logprob": -0.1970916748046875, "compression_ratio": 1.5868725868725868, "no_speech_prob": 3.5559432944864966e-06}, {"id": 338, "seek": 153980, "start": 1565.2, "end": 1566.9199999999998, "text": " And it's used in a lot of programming languages", "tokens": [400, 309, 311, 1143, 294, 257, 688, 295, 9410, 8650], "temperature": 0.0, "avg_logprob": -0.1970916748046875, "compression_ratio": 1.5868725868725868, "no_speech_prob": 3.5559432944864966e-06}, {"id": 339, "seek": 156692, "start": 1566.92, "end": 1571.4, "text": " It's called a partial application of a function basically. I want this exact function", "tokens": [467, 311, 1219, 257, 14641, 3861, 295, 257, 2445, 1936, 13, 286, 528, 341, 1900, 2445], "temperature": 0.0, "avg_logprob": -0.1749548631555894, "compression_ratio": 1.8918918918918919, "no_speech_prob": 1.2029463505314197e-05}, {"id": 340, "seek": 156692, "start": 1571.88, "end": 1577.1200000000001, "text": " but I want to fix the values of a B and C to pick a particular quadratic and", "tokens": [457, 286, 528, 281, 3191, 264, 4190, 295, 257, 363, 293, 383, 281, 1888, 257, 1729, 37262, 293], "temperature": 0.0, "avg_logprob": -0.1749548631555894, "compression_ratio": 1.8918918918918919, "no_speech_prob": 1.2029463505314197e-05}, {"id": 341, "seek": 156692, "start": 1577.6000000000001, "end": 1584.6000000000001, "text": " the way you fix the values of the function is you call this thing in Python called partial and you pass in the function and", "tokens": [264, 636, 291, 3191, 264, 4190, 295, 264, 2445, 307, 291, 818, 341, 551, 294, 15329, 1219, 14641, 293, 291, 1320, 294, 264, 2445, 293], "temperature": 0.0, "avg_logprob": -0.1749548631555894, "compression_ratio": 1.8918918918918919, "no_speech_prob": 1.2029463505314197e-05}, {"id": 342, "seek": 156692, "start": 1584.92, "end": 1587.0800000000002, "text": " Then you pass in the values that you want to fix", "tokens": [1396, 291, 1320, 294, 264, 4190, 300, 291, 528, 281, 3191], "temperature": 0.0, "avg_logprob": -0.1749548631555894, "compression_ratio": 1.8918918918918919, "no_speech_prob": 1.2029463505314197e-05}, {"id": 343, "seek": 156692, "start": 1588.16, "end": 1590.16, "text": " so for example", "tokens": [370, 337, 1365], "temperature": 0.0, "avg_logprob": -0.1749548631555894, "compression_ratio": 1.8918918918918919, "no_speech_prob": 1.2029463505314197e-05}, {"id": 344, "seek": 159016, "start": 1590.16, "end": 1597.92, "text": " If I now say make a quadratic three two one that's going to create a quadratic equation with coefficients", "tokens": [759, 286, 586, 584, 652, 257, 37262, 1045, 732, 472, 300, 311, 516, 281, 1884, 257, 37262, 5367, 365, 31994], "temperature": 0.0, "avg_logprob": -0.21239391761490062, "compression_ratio": 1.6611111111111112, "no_speech_prob": 6.962131919863168e-06}, {"id": 345, "seek": 159016, "start": 1598.28, "end": 1600.52, "text": " three two and one and", "tokens": [1045, 732, 293, 472, 293], "temperature": 0.0, "avg_logprob": -0.21239391761490062, "compression_ratio": 1.6611111111111112, "no_speech_prob": 6.962131919863168e-06}, {"id": 346, "seek": 159016, "start": 1603.0, "end": 1607.52, "text": " You can see if I then pass in so that's now F if I pass in one point five", "tokens": [509, 393, 536, 498, 286, 550, 1320, 294, 370, 300, 311, 586, 479, 498, 286, 1320, 294, 472, 935, 1732], "temperature": 0.0, "avg_logprob": -0.21239391761490062, "compression_ratio": 1.6611111111111112, "no_speech_prob": 6.962131919863168e-06}, {"id": 347, "seek": 159016, "start": 1607.52, "end": 1609.92, "text": " I get the exact same value I did before", "tokens": [286, 483, 264, 1900, 912, 2158, 286, 630, 949], "temperature": 0.0, "avg_logprob": -0.21239391761490062, "compression_ratio": 1.6611111111111112, "no_speech_prob": 6.962131919863168e-06}, {"id": 348, "seek": 159016, "start": 1612.3600000000001, "end": 1615.4, "text": " Okay, so we've now got an ability to create any quadratic", "tokens": [1033, 11, 370, 321, 600, 586, 658, 364, 3485, 281, 1884, 604, 37262], "temperature": 0.0, "avg_logprob": -0.21239391761490062, "compression_ratio": 1.6611111111111112, "no_speech_prob": 6.962131919863168e-06}, {"id": 349, "seek": 161540, "start": 1615.4, "end": 1621.1200000000001, "text": " Equation we want by passing in the parameters of the coefficients of the quadratic", "tokens": [15624, 399, 321, 528, 538, 8437, 294, 264, 9834, 295, 264, 31994, 295, 264, 37262], "temperature": 0.0, "avg_logprob": -0.18502657464210023, "compression_ratio": 1.6755555555555555, "no_speech_prob": 1.9637907371361507e-06}, {"id": 350, "seek": 161540, "start": 1621.68, "end": 1625.64, "text": " That gives us a function that we can then just call as just like any normal function", "tokens": [663, 2709, 505, 257, 2445, 300, 321, 393, 550, 445, 818, 382, 445, 411, 604, 2710, 2445], "temperature": 0.0, "avg_logprob": -0.18502657464210023, "compression_ratio": 1.6755555555555555, "no_speech_prob": 1.9637907371361507e-06}, {"id": 351, "seek": 161540, "start": 1625.68, "end": 1631.0800000000002, "text": " So that only needs one thing now, which is the value of X because the other three a B and C are now fixed", "tokens": [407, 300, 787, 2203, 472, 551, 586, 11, 597, 307, 264, 2158, 295, 1783, 570, 264, 661, 1045, 257, 363, 293, 383, 366, 586, 6806], "temperature": 0.0, "avg_logprob": -0.18502657464210023, "compression_ratio": 1.6755555555555555, "no_speech_prob": 1.9637907371361507e-06}, {"id": 352, "seek": 161540, "start": 1633.5600000000002, "end": 1635.5600000000002, "text": " So if we plot that function", "tokens": [407, 498, 321, 7542, 300, 2445], "temperature": 0.0, "avg_logprob": -0.18502657464210023, "compression_ratio": 1.6755555555555555, "no_speech_prob": 1.9637907371361507e-06}, {"id": 353, "seek": 161540, "start": 1636.0800000000002, "end": 1639.98, "text": " We'll get exactly the same shape because it's the same coefficients", "tokens": [492, 603, 483, 2293, 264, 912, 3909, 570, 309, 311, 264, 912, 31994], "temperature": 0.0, "avg_logprob": -0.18502657464210023, "compression_ratio": 1.6755555555555555, "no_speech_prob": 1.9637907371361507e-06}, {"id": 354, "seek": 161540, "start": 1640.76, "end": 1642.2800000000002, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.18502657464210023, "compression_ratio": 1.6755555555555555, "no_speech_prob": 1.9637907371361507e-06}, {"id": 355, "seek": 161540, "start": 1642.2800000000002, "end": 1643.44, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.18502657464210023, "compression_ratio": 1.6755555555555555, "no_speech_prob": 1.9637907371361507e-06}, {"id": 356, "seek": 164344, "start": 1643.44, "end": 1647.24, "text": " Now I'm going to show an example of of some data", "tokens": [823, 286, 478, 516, 281, 855, 364, 1365, 295, 295, 512, 1412], "temperature": 0.0, "avg_logprob": -0.1319729955572831, "compression_ratio": 1.7647058823529411, "no_speech_prob": 1.3006934750592336e-05}, {"id": 357, "seek": 164344, "start": 1647.88, "end": 1649.88, "text": " some data that", "tokens": [512, 1412, 300], "temperature": 0.0, "avg_logprob": -0.1319729955572831, "compression_ratio": 1.7647058823529411, "no_speech_prob": 1.3006934750592336e-05}, {"id": 358, "seek": 164344, "start": 1650.04, "end": 1656.6000000000001, "text": " Matches the shape of this function, but in real life data is never exactly going to match the shape of a function", "tokens": [26178, 279, 264, 3909, 295, 341, 2445, 11, 457, 294, 957, 993, 1412, 307, 1128, 2293, 516, 281, 2995, 264, 3909, 295, 257, 2445], "temperature": 0.0, "avg_logprob": -0.1319729955572831, "compression_ratio": 1.7647058823529411, "no_speech_prob": 1.3006934750592336e-05}, {"id": 359, "seek": 164344, "start": 1657.56, "end": 1660.8, "text": " It's going to have some noise. So here's a couple of", "tokens": [467, 311, 516, 281, 362, 512, 5658, 13, 407, 510, 311, 257, 1916, 295], "temperature": 0.0, "avg_logprob": -0.1319729955572831, "compression_ratio": 1.7647058823529411, "no_speech_prob": 1.3006934750592336e-05}, {"id": 360, "seek": 164344, "start": 1661.8400000000001, "end": 1663.8400000000001, "text": " Functions to add some noise", "tokens": [11166, 3916, 281, 909, 512, 5658], "temperature": 0.0, "avg_logprob": -0.1319729955572831, "compression_ratio": 1.7647058823529411, "no_speech_prob": 1.3006934750592336e-05}, {"id": 361, "seek": 166384, "start": 1663.84, "end": 1673.04, "text": " So you can see I've still got the basic functional form here, but this data is a bit dotted around it", "tokens": [407, 291, 393, 536, 286, 600, 920, 658, 264, 3875, 11745, 1254, 510, 11, 457, 341, 1412, 307, 257, 857, 37459, 926, 309], "temperature": 0.0, "avg_logprob": -0.14116553562443432, "compression_ratio": 1.5363636363636364, "no_speech_prob": 6.962159659451572e-06}, {"id": 362, "seek": 166384, "start": 1674.84, "end": 1679.1999999999998, "text": " The level to which you look at how I implemented these is entirely up to you", "tokens": [440, 1496, 281, 597, 291, 574, 412, 577, 286, 12270, 613, 307, 7696, 493, 281, 291], "temperature": 0.0, "avg_logprob": -0.14116553562443432, "compression_ratio": 1.5363636363636364, "no_speech_prob": 6.962159659451572e-06}, {"id": 363, "seek": 166384, "start": 1679.1999999999998, "end": 1685.04, "text": " It's not like super necessary, but it's all stuff which you know the kind of things we use quite a lot", "tokens": [467, 311, 406, 411, 1687, 4818, 11, 457, 309, 311, 439, 1507, 597, 291, 458, 264, 733, 295, 721, 321, 764, 1596, 257, 688], "temperature": 0.0, "avg_logprob": -0.14116553562443432, "compression_ratio": 1.5363636363636364, "no_speech_prob": 6.962159659451572e-06}, {"id": 364, "seek": 166384, "start": 1685.04, "end": 1688.1599999999999, "text": " So this is to create normally distributed random numbers", "tokens": [407, 341, 307, 281, 1884, 5646, 12631, 4974, 3547], "temperature": 0.0, "avg_logprob": -0.14116553562443432, "compression_ratio": 1.5363636363636364, "no_speech_prob": 6.962159659451572e-06}, {"id": 365, "seek": 168816, "start": 1688.16, "end": 1694.6000000000001, "text": " This is how we set the seed so that each time I run this I'm going to get the same random numbers", "tokens": [639, 307, 577, 321, 992, 264, 8871, 370, 300, 1184, 565, 286, 1190, 341, 286, 478, 516, 281, 483, 264, 912, 4974, 3547], "temperature": 0.0, "avg_logprob": -0.23131847381591797, "compression_ratio": 1.5639810426540284, "no_speech_prob": 5.862741545570316e-06}, {"id": 366, "seek": 168816, "start": 1696.28, "end": 1698.98, "text": " This one is actually particularly helpful this creates a", "tokens": [639, 472, 307, 767, 4098, 4961, 341, 7829, 257], "temperature": 0.0, "avg_logprob": -0.23131847381591797, "compression_ratio": 1.5639810426540284, "no_speech_prob": 5.862741545570316e-06}, {"id": 367, "seek": 168816, "start": 1700.0800000000002, "end": 1705.96, "text": " tensor so in this case a vector that goes from negative 2 to 2 in", "tokens": [40863, 370, 294, 341, 1389, 257, 8062, 300, 1709, 490, 3671, 568, 281, 568, 294], "temperature": 0.0, "avg_logprob": -0.23131847381591797, "compression_ratio": 1.5639810426540284, "no_speech_prob": 5.862741545570316e-06}, {"id": 368, "seek": 168816, "start": 1706.6000000000001, "end": 1710.92, "text": " Equal steps and there's 20 of them. That's why there's 20 steps along here", "tokens": [15624, 304, 4439, 293, 456, 311, 945, 295, 552, 13, 663, 311, 983, 456, 311, 945, 4439, 2051, 510], "temperature": 0.0, "avg_logprob": -0.23131847381591797, "compression_ratio": 1.5639810426540284, "no_speech_prob": 5.862741545570316e-06}, {"id": 369, "seek": 168816, "start": 1712.44, "end": 1716.0, "text": " So then my y values is just f of X", "tokens": [407, 550, 452, 288, 4190, 307, 445, 283, 295, 1783], "temperature": 0.0, "avg_logprob": -0.23131847381591797, "compression_ratio": 1.5639810426540284, "no_speech_prob": 5.862741545570316e-06}, {"id": 370, "seek": 171600, "start": 1716.0, "end": 1718.0, "text": " with", "tokens": [365], "temperature": 0.0, "avg_logprob": -0.21495086916031375, "compression_ratio": 1.576036866359447, "no_speech_prob": 7.296271633094875e-06}, {"id": 371, "seek": 171600, "start": 1718.04, "end": 1720.04, "text": " This amount of noise added", "tokens": [639, 2372, 295, 5658, 3869], "temperature": 0.0, "avg_logprob": -0.21495086916031375, "compression_ratio": 1.576036866359447, "no_speech_prob": 7.296271633094875e-06}, {"id": 372, "seek": 171600, "start": 1720.36, "end": 1725.56, "text": " Okay, so as I say the details of that don't matter too much the main thing to know is we've got some", "tokens": [1033, 11, 370, 382, 286, 584, 264, 4365, 295, 300, 500, 380, 1871, 886, 709, 264, 2135, 551, 281, 458, 307, 321, 600, 658, 512], "temperature": 0.0, "avg_logprob": -0.21495086916031375, "compression_ratio": 1.576036866359447, "no_speech_prob": 7.296271633094875e-06}, {"id": 373, "seek": 171600, "start": 1726.2, "end": 1731.98, "text": " Random data now and so this is the idea is now we're going to try to reconstruct the original", "tokens": [37603, 1412, 586, 293, 370, 341, 307, 264, 1558, 307, 586, 321, 434, 516, 281, 853, 281, 31499, 264, 3380], "temperature": 0.0, "avg_logprob": -0.21495086916031375, "compression_ratio": 1.576036866359447, "no_speech_prob": 7.296271633094875e-06}, {"id": 374, "seek": 171600, "start": 1732.68, "end": 1734.96, "text": " quadratic equation find one which", "tokens": [37262, 5367, 915, 472, 597], "temperature": 0.0, "avg_logprob": -0.21495086916031375, "compression_ratio": 1.576036866359447, "no_speech_prob": 7.296271633094875e-06}, {"id": 375, "seek": 171600, "start": 1735.68, "end": 1737.28, "text": " matches this data", "tokens": [10676, 341, 1412], "temperature": 0.0, "avg_logprob": -0.21495086916031375, "compression_ratio": 1.576036866359447, "no_speech_prob": 7.296271633094875e-06}, {"id": 376, "seek": 171600, "start": 1737.28, "end": 1739.28, "text": " So how would we do that?", "tokens": [407, 577, 576, 321, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.21495086916031375, "compression_ratio": 1.576036866359447, "no_speech_prob": 7.296271633094875e-06}, {"id": 377, "seek": 173928, "start": 1739.28, "end": 1747.56, "text": " Well what we can do is we can create a function called plot quadratic", "tokens": [1042, 437, 321, 393, 360, 307, 321, 393, 1884, 257, 2445, 1219, 7542, 37262], "temperature": 0.0, "avg_logprob": -0.241595431071956, "compression_ratio": 1.693467336683417, "no_speech_prob": 4.425443421496311e-06}, {"id": 378, "seek": 173928, "start": 1748.12, "end": 1754.68, "text": " That first of all plots our data as a scatter plot and then it plots a function which is a quadratic", "tokens": [663, 700, 295, 439, 28609, 527, 1412, 382, 257, 34951, 7542, 293, 550, 309, 28609, 257, 2445, 597, 307, 257, 37262], "temperature": 0.0, "avg_logprob": -0.241595431071956, "compression_ratio": 1.693467336683417, "no_speech_prob": 4.425443421496311e-06}, {"id": 379, "seek": 173928, "start": 1755.08, "end": 1757.08, "text": " The quadratic we pass in", "tokens": [440, 37262, 321, 1320, 294], "temperature": 0.0, "avg_logprob": -0.241595431071956, "compression_ratio": 1.693467336683417, "no_speech_prob": 4.425443421496311e-06}, {"id": 380, "seek": 173928, "start": 1757.76, "end": 1760.24, "text": " Now there's a very helpful thing for experimenting", "tokens": [823, 456, 311, 257, 588, 4961, 551, 337, 29070], "temperature": 0.0, "avg_logprob": -0.241595431071956, "compression_ratio": 1.693467336683417, "no_speech_prob": 4.425443421496311e-06}, {"id": 381, "seek": 173928, "start": 1761.0, "end": 1764.68, "text": " In Jupiter notebooks, which is the at interact", "tokens": [682, 24567, 43782, 11, 597, 307, 264, 412, 4648], "temperature": 0.0, "avg_logprob": -0.241595431071956, "compression_ratio": 1.693467336683417, "no_speech_prob": 4.425443421496311e-06}, {"id": 382, "seek": 176468, "start": 1764.68, "end": 1768.64, "text": " Function if you add it on top of a function", "tokens": [11166, 882, 498, 291, 909, 309, 322, 1192, 295, 257, 2445], "temperature": 0.0, "avg_logprob": -0.18007881824786848, "compression_ratio": 1.5584415584415585, "no_speech_prob": 3.34051742356678e-06}, {"id": 383, "seek": 176468, "start": 1769.72, "end": 1771.72, "text": " That it gives you these nice little sliders", "tokens": [663, 309, 2709, 291, 613, 1481, 707, 1061, 6936], "temperature": 0.0, "avg_logprob": -0.18007881824786848, "compression_ratio": 1.5584415584415585, "no_speech_prob": 3.34051742356678e-06}, {"id": 384, "seek": 176468, "start": 1772.68, "end": 1774.68, "text": " so here's an example of", "tokens": [370, 510, 311, 364, 1365, 295], "temperature": 0.0, "avg_logprob": -0.18007881824786848, "compression_ratio": 1.5584415584415585, "no_speech_prob": 3.34051742356678e-06}, {"id": 385, "seek": 176468, "start": 1775.8400000000001, "end": 1777.48, "text": " quadratic with", "tokens": [37262, 365], "temperature": 0.0, "avg_logprob": -0.18007881824786848, "compression_ratio": 1.5584415584415585, "no_speech_prob": 3.34051742356678e-06}, {"id": 386, "seek": 176468, "start": 1777.48, "end": 1779.88, "text": " coefficients 1.5 1.5 1.5", "tokens": [31994, 502, 13, 20, 502, 13, 20, 502, 13, 20], "temperature": 0.0, "avg_logprob": -0.18007881824786848, "compression_ratio": 1.5584415584415585, "no_speech_prob": 3.34051742356678e-06}, {"id": 387, "seek": 176468, "start": 1781.72, "end": 1783.72, "text": " And it doesn't fit particularly well", "tokens": [400, 309, 1177, 380, 3318, 4098, 731], "temperature": 0.0, "avg_logprob": -0.18007881824786848, "compression_ratio": 1.5584415584415585, "no_speech_prob": 3.34051742356678e-06}, {"id": 388, "seek": 176468, "start": 1783.92, "end": 1789.3200000000002, "text": " So how would we try to make this fit better? Well, I think what I'd do is I take the first slider and", "tokens": [407, 577, 576, 321, 853, 281, 652, 341, 3318, 1101, 30, 1042, 11, 286, 519, 437, 286, 1116, 360, 307, 286, 747, 264, 700, 26046, 293], "temperature": 0.0, "avg_logprob": -0.18007881824786848, "compression_ratio": 1.5584415584415585, "no_speech_prob": 3.34051742356678e-06}, {"id": 389, "seek": 178932, "start": 1789.32, "end": 1794.52, "text": " I would try moving it to the left and see if it looks better or worse that", "tokens": [286, 576, 853, 2684, 309, 281, 264, 1411, 293, 536, 498, 309, 1542, 1101, 420, 5324, 300], "temperature": 0.0, "avg_logprob": -0.16666404873717064, "compression_ratio": 1.7676767676767677, "no_speech_prob": 2.1444539015647024e-05}, {"id": 390, "seek": 178932, "start": 1794.8799999999999, "end": 1798.3999999999999, "text": " Looks worse to me. I think it needs to be more curvy. So let's try the other way", "tokens": [10027, 5324, 281, 385, 13, 286, 519, 309, 2203, 281, 312, 544, 1262, 11869, 13, 407, 718, 311, 853, 264, 661, 636], "temperature": 0.0, "avg_logprob": -0.16666404873717064, "compression_ratio": 1.7676767676767677, "no_speech_prob": 2.1444539015647024e-05}, {"id": 391, "seek": 178932, "start": 1800.3999999999999, "end": 1804.9199999999998, "text": " Yeah, that doesn't look bad let's do the same thing for the next slider have it this way", "tokens": [865, 11, 300, 1177, 380, 574, 1578, 718, 311, 360, 264, 912, 551, 337, 264, 958, 26046, 362, 309, 341, 636], "temperature": 0.0, "avg_logprob": -0.16666404873717064, "compression_ratio": 1.7676767676767677, "no_speech_prob": 2.1444539015647024e-05}, {"id": 392, "seek": 178932, "start": 1806.6399999999999, "end": 1808.6399999999999, "text": " No, I think that's worse. Let's try the other way", "tokens": [883, 11, 286, 519, 300, 311, 5324, 13, 961, 311, 853, 264, 661, 636], "temperature": 0.0, "avg_logprob": -0.16666404873717064, "compression_ratio": 1.7676767676767677, "no_speech_prob": 2.1444539015647024e-05}, {"id": 393, "seek": 178932, "start": 1809.76, "end": 1812.5, "text": " Okay, final slider try this way", "tokens": [1033, 11, 2572, 26046, 853, 341, 636], "temperature": 0.0, "avg_logprob": -0.16666404873717064, "compression_ratio": 1.7676767676767677, "no_speech_prob": 2.1444539015647024e-05}, {"id": 394, "seek": 178932, "start": 1813.48, "end": 1815.48, "text": " No, it's worse this way", "tokens": [883, 11, 309, 311, 5324, 341, 636], "temperature": 0.0, "avg_logprob": -0.16666404873717064, "compression_ratio": 1.7676767676767677, "no_speech_prob": 2.1444539015647024e-05}, {"id": 395, "seek": 181548, "start": 1815.48, "end": 1820.72, "text": " So you can see what we can do we can basically pick each of the coefficients", "tokens": [407, 291, 393, 536, 437, 321, 393, 360, 321, 393, 1936, 1888, 1184, 295, 264, 31994], "temperature": 0.0, "avg_logprob": -0.17499443128997205, "compression_ratio": 1.8679245283018868, "no_speech_prob": 2.6425468604429625e-06}, {"id": 396, "seek": 181548, "start": 1821.44, "end": 1824.0, "text": " One at a time try increasing a middle bit", "tokens": [1485, 412, 257, 565, 853, 5662, 257, 2808, 857], "temperature": 0.0, "avg_logprob": -0.17499443128997205, "compression_ratio": 1.8679245283018868, "no_speech_prob": 2.6425468604429625e-06}, {"id": 397, "seek": 181548, "start": 1824.0, "end": 1826.4, "text": " See if that improves it try decreasing it a little bit", "tokens": [3008, 498, 300, 24771, 309, 853, 23223, 309, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.17499443128997205, "compression_ratio": 1.8679245283018868, "no_speech_prob": 2.6425468604429625e-06}, {"id": 398, "seek": 181548, "start": 1826.4, "end": 1832.1200000000001, "text": " So if that improves it find the direction that improves it and then slide it in that direction a little bit", "tokens": [407, 498, 300, 24771, 309, 915, 264, 3513, 300, 24771, 309, 293, 550, 4137, 309, 294, 300, 3513, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.17499443128997205, "compression_ratio": 1.8679245283018868, "no_speech_prob": 2.6425468604429625e-06}, {"id": 399, "seek": 181548, "start": 1832.1200000000001, "end": 1834.84, "text": " And then when we're done, we can go back to the first one and see if", "tokens": [400, 550, 562, 321, 434, 1096, 11, 321, 393, 352, 646, 281, 264, 700, 472, 293, 536, 498], "temperature": 0.0, "avg_logprob": -0.17499443128997205, "compression_ratio": 1.8679245283018868, "no_speech_prob": 2.6425468604429625e-06}, {"id": 400, "seek": 181548, "start": 1836.16, "end": 1838.16, "text": " We can make it any better", "tokens": [492, 393, 652, 309, 604, 1101], "temperature": 0.0, "avg_logprob": -0.17499443128997205, "compression_ratio": 1.8679245283018868, "no_speech_prob": 2.6425468604429625e-06}, {"id": 401, "seek": 181548, "start": 1838.4, "end": 1840.4, "text": " Now we've done that", "tokens": [823, 321, 600, 1096, 300], "temperature": 0.0, "avg_logprob": -0.17499443128997205, "compression_ratio": 1.8679245283018868, "no_speech_prob": 2.6425468604429625e-06}, {"id": 402, "seek": 184040, "start": 1840.4, "end": 1847.3200000000002, "text": " And actually you can see that's not bad because I know the answers meant to be three two one, so they're pretty close", "tokens": [400, 767, 291, 393, 536, 300, 311, 406, 1578, 570, 286, 458, 264, 6338, 4140, 281, 312, 1045, 732, 472, 11, 370, 436, 434, 1238, 1998], "temperature": 0.0, "avg_logprob": -0.2049574089050293, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.962151019251905e-06}, {"id": 403, "seek": 184040, "start": 1848.2, "end": 1850.2, "text": " And I wasn't shooting her promise", "tokens": [400, 286, 2067, 380, 5942, 720, 6228], "temperature": 0.0, "avg_logprob": -0.2049574089050293, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.962151019251905e-06}, {"id": 404, "seek": 184040, "start": 1852.2, "end": 1853.96, "text": " That's basically", "tokens": [663, 311, 1936], "temperature": 0.0, "avg_logprob": -0.2049574089050293, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.962151019251905e-06}, {"id": 405, "seek": 184040, "start": 1853.96, "end": 1861.2, "text": " What we're going to do that's basically how those parameters are created, but we obviously don't have time because the", "tokens": [708, 321, 434, 516, 281, 360, 300, 311, 1936, 577, 729, 9834, 366, 2942, 11, 457, 321, 2745, 500, 380, 362, 565, 570, 264], "temperature": 0.0, "avg_logprob": -0.2049574089050293, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.962151019251905e-06}, {"id": 406, "seek": 184040, "start": 1861.76, "end": 1863.76, "text": " You know big fancy models have", "tokens": [509, 458, 955, 10247, 5245, 362], "temperature": 0.0, "avg_logprob": -0.2049574089050293, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.962151019251905e-06}, {"id": 407, "seek": 184040, "start": 1864.88, "end": 1869.2800000000002, "text": " Often hundreds of millions of parameters. We don't have time to try a hundred hundred million sliders", "tokens": [20043, 6779, 295, 6803, 295, 9834, 13, 492, 500, 380, 362, 565, 281, 853, 257, 3262, 3262, 2459, 1061, 6936], "temperature": 0.0, "avg_logprob": -0.2049574089050293, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.962151019251905e-06}, {"id": 408, "seek": 186928, "start": 1869.28, "end": 1871.28, "text": " So we did something better", "tokens": [407, 321, 630, 746, 1101], "temperature": 0.0, "avg_logprob": -0.18042641878128052, "compression_ratio": 1.6359649122807018, "no_speech_prob": 4.157310740993125e-06}, {"id": 409, "seek": 186928, "start": 1871.72, "end": 1877.6, "text": " Well, the first step is we need a better idea of like when I move it is it getting better or is it getting worse?", "tokens": [1042, 11, 264, 700, 1823, 307, 321, 643, 257, 1101, 1558, 295, 411, 562, 286, 1286, 309, 307, 309, 1242, 1101, 420, 307, 309, 1242, 5324, 30], "temperature": 0.0, "avg_logprob": -0.18042641878128052, "compression_ratio": 1.6359649122807018, "no_speech_prob": 4.157310740993125e-06}, {"id": 410, "seek": 186928, "start": 1878.68, "end": 1880.68, "text": " so if you remember back to", "tokens": [370, 498, 291, 1604, 646, 281], "temperature": 0.0, "avg_logprob": -0.18042641878128052, "compression_ratio": 1.6359649122807018, "no_speech_prob": 4.157310740993125e-06}, {"id": 411, "seek": 186928, "start": 1881.92, "end": 1883.92, "text": " Arthur Samuels", "tokens": [19624, 4832, 84, 1625], "temperature": 0.0, "avg_logprob": -0.18042641878128052, "compression_ratio": 1.6359649122807018, "no_speech_prob": 4.157310740993125e-06}, {"id": 412, "seek": 186928, "start": 1884.52, "end": 1888.6399999999999, "text": " Description of machine learning that we learn about in chapter one of the book and in lesson one", "tokens": [3885, 12432, 295, 3479, 2539, 300, 321, 1466, 466, 294, 7187, 472, 295, 264, 1446, 293, 294, 6898, 472], "temperature": 0.0, "avg_logprob": -0.18042641878128052, "compression_ratio": 1.6359649122807018, "no_speech_prob": 4.157310740993125e-06}, {"id": 413, "seek": 186928, "start": 1890.08, "end": 1892.08, "text": " We need some", "tokens": [492, 643, 512], "temperature": 0.0, "avg_logprob": -0.18042641878128052, "compression_ratio": 1.6359649122807018, "no_speech_prob": 4.157310740993125e-06}, {"id": 414, "seek": 186928, "start": 1892.24, "end": 1896.36, "text": " Something we can measure which is a number that tells us how good is their model", "tokens": [6595, 321, 393, 3481, 597, 307, 257, 1230, 300, 5112, 505, 577, 665, 307, 641, 2316], "temperature": 0.0, "avg_logprob": -0.18042641878128052, "compression_ratio": 1.6359649122807018, "no_speech_prob": 4.157310740993125e-06}, {"id": 415, "seek": 189636, "start": 1896.36, "end": 1901.58, "text": " And if we had that then as we move these sliders, we could check to see whether it's getting better or worse", "tokens": [400, 498, 321, 632, 300, 550, 382, 321, 1286, 613, 1061, 6936, 11, 321, 727, 1520, 281, 536, 1968, 309, 311, 1242, 1101, 420, 5324], "temperature": 0.0, "avg_logprob": -0.19204444885253907, "compression_ratio": 1.7936507936507937, "no_speech_prob": 4.785007149621379e-06}, {"id": 416, "seek": 189636, "start": 1902.84, "end": 1909.82, "text": " So this is called a loss function. So there's lots of different loss functions you can pick but perhaps the most simple and common is", "tokens": [407, 341, 307, 1219, 257, 4470, 2445, 13, 407, 456, 311, 3195, 295, 819, 4470, 6828, 291, 393, 1888, 457, 4317, 264, 881, 2199, 293, 2689, 307], "temperature": 0.0, "avg_logprob": -0.19204444885253907, "compression_ratio": 1.7936507936507937, "no_speech_prob": 4.785007149621379e-06}, {"id": 417, "seek": 189636, "start": 1910.6, "end": 1916.1999999999998, "text": " Mean squared error which is going to be so it's going to get in our predictions and it's got the actuals", "tokens": [12302, 8889, 6713, 597, 307, 516, 281, 312, 370, 309, 311, 516, 281, 483, 294, 527, 21264, 293, 309, 311, 658, 264, 3539, 82], "temperature": 0.0, "avg_logprob": -0.19204444885253907, "compression_ratio": 1.7936507936507937, "no_speech_prob": 4.785007149621379e-06}, {"id": 418, "seek": 189636, "start": 1916.6, "end": 1923.08, "text": " And we're going to go predictions minus actuals squared and take them in so that's mean squared error", "tokens": [400, 321, 434, 516, 281, 352, 21264, 3175, 3539, 82, 8889, 293, 747, 552, 294, 370, 300, 311, 914, 8889, 6713], "temperature": 0.0, "avg_logprob": -0.19204444885253907, "compression_ratio": 1.7936507936507937, "no_speech_prob": 4.785007149621379e-06}, {"id": 419, "seek": 189636, "start": 1924.08, "end": 1925.28, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.19204444885253907, "compression_ratio": 1.7936507936507937, "no_speech_prob": 4.785007149621379e-06}, {"id": 420, "seek": 192528, "start": 1925.28, "end": 1932.76, "text": " If I now rerun the exact same thing I had before but this time I'm going to calculate the loss the MSE between", "tokens": [759, 286, 586, 43819, 409, 264, 1900, 912, 551, 286, 632, 949, 457, 341, 565, 286, 478, 516, 281, 8873, 264, 4470, 264, 376, 5879, 1296], "temperature": 0.0, "avg_logprob": -0.17768671295859598, "compression_ratio": 1.5841584158415842, "no_speech_prob": 9.972725820261985e-06}, {"id": 421, "seek": 192528, "start": 1933.3999999999999, "end": 1936.04, "text": " the values that we predict f of X", "tokens": [264, 4190, 300, 321, 6069, 283, 295, 1783], "temperature": 0.0, "avg_logprob": -0.17768671295859598, "compression_ratio": 1.5841584158415842, "no_speech_prob": 9.972725820261985e-06}, {"id": 422, "seek": 192528, "start": 1936.84, "end": 1943.68, "text": " Remember where F is the quadratic we created and the actuals why and this time I'm going to add a title to our function", "tokens": [5459, 689, 479, 307, 264, 37262, 321, 2942, 293, 264, 3539, 82, 983, 293, 341, 565, 286, 478, 516, 281, 909, 257, 4876, 281, 527, 2445], "temperature": 0.0, "avg_logprob": -0.17768671295859598, "compression_ratio": 1.5841584158415842, "no_speech_prob": 9.972725820261985e-06}, {"id": 423, "seek": 192528, "start": 1944.12, "end": 1946.12, "text": " Which is the loss?", "tokens": [3013, 307, 264, 4470, 30], "temperature": 0.0, "avg_logprob": -0.17768671295859598, "compression_ratio": 1.5841584158415842, "no_speech_prob": 9.972725820261985e-06}, {"id": 424, "seek": 192528, "start": 1947.8799999999999, "end": 1949.8799999999999, "text": " So now", "tokens": [407, 586], "temperature": 0.0, "avg_logprob": -0.17768671295859598, "compression_ratio": 1.5841584158415842, "no_speech_prob": 9.972725820261985e-06}, {"id": 425, "seek": 192528, "start": 1949.92, "end": 1951.92, "text": " Let's do this more rigorously", "tokens": [961, 311, 360, 341, 544, 42191, 5098], "temperature": 0.0, "avg_logprob": -0.17768671295859598, "compression_ratio": 1.5841584158415842, "no_speech_prob": 9.972725820261985e-06}, {"id": 426, "seek": 195192, "start": 1951.92, "end": 1958.1200000000001, "text": " We're starting at a mean squared error of 11.46. So let's try moving this to the left and see if it gets better", "tokens": [492, 434, 2891, 412, 257, 914, 8889, 6713, 295, 2975, 13, 16169, 13, 407, 718, 311, 853, 2684, 341, 281, 264, 1411, 293, 536, 498, 309, 2170, 1101], "temperature": 0.0, "avg_logprob": -0.2870649383181617, "compression_ratio": 1.4919786096256684, "no_speech_prob": 8.397944839089178e-06}, {"id": 427, "seek": 195192, "start": 1958.72, "end": 1960.72, "text": " No, wait, so we're over to the right", "tokens": [883, 11, 1699, 11, 370, 321, 434, 670, 281, 264, 558], "temperature": 0.0, "avg_logprob": -0.2870649383181617, "compression_ratio": 1.4919786096256684, "no_speech_prob": 8.397944839089178e-06}, {"id": 428, "seek": 195192, "start": 1963.8000000000002, "end": 1967.52, "text": " All right, sorry around there, okay now let's try this one", "tokens": [1057, 558, 11, 2597, 926, 456, 11, 1392, 586, 718, 311, 853, 341, 472], "temperature": 0.0, "avg_logprob": -0.2870649383181617, "compression_ratio": 1.4919786096256684, "no_speech_prob": 8.397944839089178e-06}, {"id": 429, "seek": 195192, "start": 1972.3200000000002, "end": 1975.0, "text": " Okay, best when I go to the right", "tokens": [1033, 11, 1151, 562, 286, 352, 281, 264, 558], "temperature": 0.0, "avg_logprob": -0.2870649383181617, "compression_ratio": 1.4919786096256684, "no_speech_prob": 8.397944839089178e-06}, {"id": 430, "seek": 195192, "start": 1976.52, "end": 1979.96, "text": " Okay, what about C 3.91 getting worse", "tokens": [1033, 11, 437, 466, 383, 805, 13, 29925, 1242, 5324], "temperature": 0.0, "avg_logprob": -0.2870649383181617, "compression_ratio": 1.4919786096256684, "no_speech_prob": 8.397944839089178e-06}, {"id": 431, "seek": 197996, "start": 1979.96, "end": 1981.96, "text": " so I keep going", "tokens": [370, 286, 1066, 516], "temperature": 0.0, "avg_logprob": -0.279020407765182, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.966898475482594e-06}, {"id": 432, "seek": 197996, "start": 1983.76, "end": 1990.28, "text": " Sorry about that and so now we can repeat that process right so we've we've had each of a B and C move a little bit", "tokens": [4919, 466, 300, 293, 370, 586, 321, 393, 7149, 300, 1399, 558, 370, 321, 600, 321, 600, 632, 1184, 295, 257, 363, 293, 383, 1286, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.279020407765182, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.966898475482594e-06}, {"id": 433, "seek": 197996, "start": 1990.3600000000001, "end": 1994.92, "text": " Let's go back to a can I get any better than 3.28? Let's try moving left", "tokens": [961, 311, 352, 646, 281, 257, 393, 286, 483, 604, 1101, 813, 805, 13, 11205, 30, 961, 311, 853, 2684, 1411], "temperature": 0.0, "avg_logprob": -0.279020407765182, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.966898475482594e-06}, {"id": 434, "seek": 197996, "start": 1995.88, "end": 1999.44, "text": " Yeah, that was a bit better and for B. Let's try moving left", "tokens": [865, 11, 300, 390, 257, 857, 1101, 293, 337, 363, 13, 961, 311, 853, 2684, 1411], "temperature": 0.0, "avg_logprob": -0.279020407765182, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.966898475482594e-06}, {"id": 435, "seek": 197996, "start": 2000.52, "end": 2002.32, "text": " worse", "tokens": [5324], "temperature": 0.0, "avg_logprob": -0.279020407765182, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.966898475482594e-06}, {"id": 436, "seek": 197996, "start": 2002.32, "end": 2004.32, "text": " right was better and", "tokens": [558, 390, 1101, 293], "temperature": 0.0, "avg_logprob": -0.279020407765182, "compression_ratio": 1.6417910447761195, "no_speech_prob": 3.966898475482594e-06}, {"id": 437, "seek": 200432, "start": 2004.32, "end": 2008.6799999999998, "text": " Have it finally see move to the right. Oh", "tokens": [3560, 309, 2721, 536, 1286, 281, 264, 558, 13, 876], "temperature": 0.0, "avg_logprob": -0.22966798503747146, "compression_ratio": 1.5097087378640777, "no_speech_prob": 2.1907683276367607e-06}, {"id": 438, "seek": 200432, "start": 2010.96, "end": 2012.96, "text": " Definitely better", "tokens": [12151, 1101], "temperature": 0.0, "avg_logprob": -0.22966798503747146, "compression_ratio": 1.5097087378640777, "no_speech_prob": 2.1907683276367607e-06}, {"id": 439, "seek": 200432, "start": 2013.6, "end": 2015.6, "text": " There we go", "tokens": [821, 321, 352], "temperature": 0.0, "avg_logprob": -0.22966798503747146, "compression_ratio": 1.5097087378640777, "no_speech_prob": 2.1907683276367607e-06}, {"id": 440, "seek": 200432, "start": 2015.6, "end": 2017.52, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.22966798503747146, "compression_ratio": 1.5097087378640777, "no_speech_prob": 2.1907683276367607e-06}, {"id": 441, "seek": 200432, "start": 2017.52, "end": 2019.4399999999998, "text": " That's a more rigorous approach", "tokens": [663, 311, 257, 544, 29882, 3109], "temperature": 0.0, "avg_logprob": -0.22966798503747146, "compression_ratio": 1.5097087378640777, "no_speech_prob": 2.1907683276367607e-06}, {"id": 442, "seek": 200432, "start": 2019.4399999999998, "end": 2026.12, "text": " It's still manual but at least we can like we don't have to rely on us to kind of recognize. Does it look better or worse?", "tokens": [467, 311, 920, 9688, 457, 412, 1935, 321, 393, 411, 321, 500, 380, 362, 281, 10687, 322, 505, 281, 733, 295, 5521, 13, 4402, 309, 574, 1101, 420, 5324, 30], "temperature": 0.0, "avg_logprob": -0.22966798503747146, "compression_ratio": 1.5097087378640777, "no_speech_prob": 2.1907683276367607e-06}, {"id": 443, "seek": 200432, "start": 2027.04, "end": 2029.96, "text": " So finally we're going to automate this", "tokens": [407, 2721, 321, 434, 516, 281, 31605, 341], "temperature": 0.0, "avg_logprob": -0.22966798503747146, "compression_ratio": 1.5097087378640777, "no_speech_prob": 2.1907683276367607e-06}, {"id": 444, "seek": 200432, "start": 2031.2, "end": 2033.2, "text": " so the key thing we need to know is", "tokens": [370, 264, 2141, 551, 321, 643, 281, 458, 307], "temperature": 0.0, "avg_logprob": -0.22966798503747146, "compression_ratio": 1.5097087378640777, "no_speech_prob": 2.1907683276367607e-06}, {"id": 445, "seek": 203320, "start": 2033.2, "end": 2035.2, "text": " for each parameter", "tokens": [337, 1184, 13075], "temperature": 0.0, "avg_logprob": -0.19792809063875222, "compression_ratio": 1.7675675675675675, "no_speech_prob": 3.966962594859069e-06}, {"id": 446, "seek": 203320, "start": 2036.04, "end": 2038.04, "text": " When we move it up", "tokens": [1133, 321, 1286, 309, 493], "temperature": 0.0, "avg_logprob": -0.19792809063875222, "compression_ratio": 1.7675675675675675, "no_speech_prob": 3.966962594859069e-06}, {"id": 447, "seek": 203320, "start": 2038.1200000000001, "end": 2043.04, "text": " Does the loss get better or when we move it down does the loss get better?", "tokens": [4402, 264, 4470, 483, 1101, 420, 562, 321, 1286, 309, 760, 775, 264, 4470, 483, 1101, 30], "temperature": 0.0, "avg_logprob": -0.19792809063875222, "compression_ratio": 1.7675675675675675, "no_speech_prob": 3.966962594859069e-06}, {"id": 448, "seek": 203320, "start": 2044.72, "end": 2052.2400000000002, "text": " What approach would be to try it right we could manually increase the parameter a bit and see if the loss improves and vice versa", "tokens": [708, 3109, 576, 312, 281, 853, 309, 558, 321, 727, 16945, 3488, 264, 13075, 257, 857, 293, 536, 498, 264, 4470, 24771, 293, 11964, 25650], "temperature": 0.0, "avg_logprob": -0.19792809063875222, "compression_ratio": 1.7675675675675675, "no_speech_prob": 3.966962594859069e-06}, {"id": 449, "seek": 203320, "start": 2053.2400000000002, "end": 2058.48, "text": " But there's a much faster way and the much faster way is to calculate its derivative", "tokens": [583, 456, 311, 257, 709, 4663, 636, 293, 264, 709, 4663, 636, 307, 281, 8873, 1080, 13760], "temperature": 0.0, "avg_logprob": -0.19792809063875222, "compression_ratio": 1.7675675675675675, "no_speech_prob": 3.966962594859069e-06}, {"id": 450, "seek": 205848, "start": 2058.48, "end": 2064.12, "text": " So if you've forgotten what a derivative is, no problem. There's lots of tutorials out there", "tokens": [407, 498, 291, 600, 11832, 437, 257, 13760, 307, 11, 572, 1154, 13, 821, 311, 3195, 295, 17616, 484, 456], "temperature": 0.0, "avg_logprob": -0.1952860328588593, "compression_ratio": 1.6406926406926408, "no_speech_prob": 4.425455244927434e-06}, {"id": 451, "seek": 205848, "start": 2064.12, "end": 2065.92, "text": " You could go to Khan Academy or something like that", "tokens": [509, 727, 352, 281, 18136, 11735, 420, 746, 411, 300], "temperature": 0.0, "avg_logprob": -0.1952860328588593, "compression_ratio": 1.6406926406926408, "no_speech_prob": 4.425455244927434e-06}, {"id": 452, "seek": 205848, "start": 2065.92, "end": 2073.56, "text": " But in short the derivative is what I just said the derivative is a function that tells you if you increase", "tokens": [583, 294, 2099, 264, 13760, 307, 437, 286, 445, 848, 264, 13760, 307, 257, 2445, 300, 5112, 291, 498, 291, 3488], "temperature": 0.0, "avg_logprob": -0.1952860328588593, "compression_ratio": 1.6406926406926408, "no_speech_prob": 4.425455244927434e-06}, {"id": 453, "seek": 205848, "start": 2073.96, "end": 2081.36, "text": " The input does the output increase or decrease and by how much so that's called the slope or the gradient", "tokens": [440, 4846, 775, 264, 5598, 3488, 420, 11514, 293, 538, 577, 709, 370, 300, 311, 1219, 264, 13525, 420, 264, 16235], "temperature": 0.0, "avg_logprob": -0.1952860328588593, "compression_ratio": 1.6406926406926408, "no_speech_prob": 4.425455244927434e-06}, {"id": 454, "seek": 205848, "start": 2082.76, "end": 2084.44, "text": " Now the good news is", "tokens": [823, 264, 665, 2583, 307], "temperature": 0.0, "avg_logprob": -0.1952860328588593, "compression_ratio": 1.6406926406926408, "no_speech_prob": 4.425455244927434e-06}, {"id": 455, "seek": 208444, "start": 2084.44, "end": 2089.76, "text": " Pie torch can automatically calculate that for you. So if you went through", "tokens": [22914, 27822, 393, 6772, 8873, 300, 337, 291, 13, 407, 498, 291, 1437, 807], "temperature": 0.0, "avg_logprob": -0.1857051078719322, "compression_ratio": 1.619607843137255, "no_speech_prob": 4.157304374530213e-06}, {"id": 456, "seek": 208444, "start": 2091.2400000000002, "end": 2098.2000000000003, "text": " Horrifying months of learning derivative rules in year 11 and worried you're going to have to remember them all again. Don't worry you don't", "tokens": [10691, 81, 5489, 2493, 295, 2539, 13760, 4474, 294, 1064, 2975, 293, 5804, 291, 434, 516, 281, 362, 281, 1604, 552, 439, 797, 13, 1468, 380, 3292, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.1857051078719322, "compression_ratio": 1.619607843137255, "no_speech_prob": 4.157304374530213e-06}, {"id": 457, "seek": 208444, "start": 2099.0, "end": 2103.04, "text": " You don't have to calculate any of this yourself. It's all done for you. Watch this", "tokens": [509, 500, 380, 362, 281, 8873, 604, 295, 341, 1803, 13, 467, 311, 439, 1096, 337, 291, 13, 7277, 341], "temperature": 0.0, "avg_logprob": -0.1857051078719322, "compression_ratio": 1.619607843137255, "no_speech_prob": 4.157304374530213e-06}, {"id": 458, "seek": 208444, "start": 2104.6, "end": 2111.64, "text": " So the first thing to do is we need a function that takes the coefficients of the quadratic a B and C as inputs I", "tokens": [407, 264, 700, 551, 281, 360, 307, 321, 643, 257, 2445, 300, 2516, 264, 31994, 295, 264, 37262, 257, 363, 293, 383, 382, 15743, 286], "temperature": 0.0, "avg_logprob": -0.1857051078719322, "compression_ratio": 1.619607843137255, "no_speech_prob": 4.157304374530213e-06}, {"id": 459, "seek": 211164, "start": 2111.64, "end": 2116.7599999999998, "text": " Put them all on a list. You'll see why in a moment. I kind of call them parameters", "tokens": [4935, 552, 439, 322, 257, 1329, 13, 509, 603, 536, 983, 294, 257, 1623, 13, 286, 733, 295, 818, 552, 9834], "temperature": 0.0, "avg_logprob": -0.1923958556820648, "compression_ratio": 1.6872246696035242, "no_speech_prob": 8.664431334182154e-06}, {"id": 460, "seek": 211164, "start": 2118.0, "end": 2120.0, "text": " We create a quadratic", "tokens": [492, 1884, 257, 37262], "temperature": 0.0, "avg_logprob": -0.1923958556820648, "compression_ratio": 1.6872246696035242, "no_speech_prob": 8.664431334182154e-06}, {"id": 461, "seek": 211164, "start": 2120.3199999999997, "end": 2122.48, "text": " passing in those parameters a B and C", "tokens": [8437, 294, 729, 9834, 257, 363, 293, 383], "temperature": 0.0, "avg_logprob": -0.1923958556820648, "compression_ratio": 1.6872246696035242, "no_speech_prob": 8.664431334182154e-06}, {"id": 462, "seek": 211164, "start": 2123.3599999999997, "end": 2126.52, "text": " This star on the front is a very very common thing in Python", "tokens": [639, 3543, 322, 264, 1868, 307, 257, 588, 588, 2689, 551, 294, 15329], "temperature": 0.0, "avg_logprob": -0.1923958556820648, "compression_ratio": 1.6872246696035242, "no_speech_prob": 8.664431334182154e-06}, {"id": 463, "seek": 211164, "start": 2127.64, "end": 2134.24, "text": " Basically, it takes these parameters and spreads them out to turn them into a B and C and pass each of them to the function", "tokens": [8537, 11, 309, 2516, 613, 9834, 293, 25728, 552, 484, 281, 1261, 552, 666, 257, 363, 293, 383, 293, 1320, 1184, 295, 552, 281, 264, 2445], "temperature": 0.0, "avg_logprob": -0.1923958556820648, "compression_ratio": 1.6872246696035242, "no_speech_prob": 8.664431334182154e-06}, {"id": 464, "seek": 211164, "start": 2134.24, "end": 2136.24, "text": " So I've now got a quadratic", "tokens": [407, 286, 600, 586, 658, 257, 37262], "temperature": 0.0, "avg_logprob": -0.1923958556820648, "compression_ratio": 1.6872246696035242, "no_speech_prob": 8.664431334182154e-06}, {"id": 465, "seek": 211164, "start": 2136.8799999999997, "end": 2138.8799999999997, "text": " with those coefficients and", "tokens": [365, 729, 31994, 293], "temperature": 0.0, "avg_logprob": -0.1923958556820648, "compression_ratio": 1.6872246696035242, "no_speech_prob": 8.664431334182154e-06}, {"id": 466, "seek": 213888, "start": 2138.88, "end": 2144.1600000000003, "text": " And then we return the mean squared error of our predictions against our actions", "tokens": [400, 550, 321, 2736, 264, 914, 8889, 6713, 295, 527, 21264, 1970, 527, 5909], "temperature": 0.0, "avg_logprob": -0.17205060761550378, "compression_ratio": 1.5980392156862746, "no_speech_prob": 2.6425489068060415e-06}, {"id": 467, "seek": 213888, "start": 2145.12, "end": 2150.2200000000003, "text": " So this is a function that's going to take the coefficients of a quadratic and return the loss", "tokens": [407, 341, 307, 257, 2445, 300, 311, 516, 281, 747, 264, 31994, 295, 257, 37262, 293, 2736, 264, 4470], "temperature": 0.0, "avg_logprob": -0.17205060761550378, "compression_ratio": 1.5980392156862746, "no_speech_prob": 2.6425489068060415e-06}, {"id": 468, "seek": 213888, "start": 2152.32, "end": 2154.32, "text": " So let's try it", "tokens": [407, 718, 311, 853, 309], "temperature": 0.0, "avg_logprob": -0.17205060761550378, "compression_ratio": 1.5980392156862746, "no_speech_prob": 2.6425489068060415e-06}, {"id": 469, "seek": 213888, "start": 2155.6, "end": 2162.6, "text": " Okay, so if we start with a B and C of 1.5 we get a mean squared error of 11 point four six", "tokens": [1033, 11, 370, 498, 321, 722, 365, 257, 363, 293, 383, 295, 502, 13, 20, 321, 483, 257, 914, 8889, 6713, 295, 2975, 935, 1451, 2309], "temperature": 0.0, "avg_logprob": -0.17205060761550378, "compression_ratio": 1.5980392156862746, "no_speech_prob": 2.6425489068060415e-06}, {"id": 470, "seek": 213888, "start": 2164.32, "end": 2166.4, "text": " It looks a bit weird it says it's a tensor", "tokens": [467, 1542, 257, 857, 3657, 309, 1619, 309, 311, 257, 40863], "temperature": 0.0, "avg_logprob": -0.17205060761550378, "compression_ratio": 1.5980392156862746, "no_speech_prob": 2.6425489068060415e-06}, {"id": 471, "seek": 216640, "start": 2166.4, "end": 2172.32, "text": " So don't worry about that too much in short in pi torch", "tokens": [407, 500, 380, 3292, 466, 300, 886, 709, 294, 2099, 294, 3895, 27822], "temperature": 0.0, "avg_logprob": -0.1835658171466578, "compression_ratio": 1.8309859154929577, "no_speech_prob": 7.07182061887579e-06}, {"id": 472, "seek": 216640, "start": 2173.04, "end": 2177.7200000000003, "text": " Everything is a tensor a tensor just means that you don't it doesn't just work with numbers", "tokens": [5471, 307, 257, 40863, 257, 40863, 445, 1355, 300, 291, 500, 380, 309, 1177, 380, 445, 589, 365, 3547], "temperature": 0.0, "avg_logprob": -0.1835658171466578, "compression_ratio": 1.8309859154929577, "no_speech_prob": 7.07182061887579e-06}, {"id": 473, "seek": 216640, "start": 2178.28, "end": 2182.6, "text": " It also works with lists or vectors of numbers. That's got a 1d tensor", "tokens": [467, 611, 1985, 365, 14511, 420, 18875, 295, 3547, 13, 663, 311, 658, 257, 502, 67, 40863], "temperature": 0.0, "avg_logprob": -0.1835658171466578, "compression_ratio": 1.8309859154929577, "no_speech_prob": 7.07182061887579e-06}, {"id": 474, "seek": 216640, "start": 2184.12, "end": 2187.4, "text": " Rectangles of numbers so tables of numbers. It's got a 2d tensor", "tokens": [497, 557, 656, 904, 295, 3547, 370, 8020, 295, 3547, 13, 467, 311, 658, 257, 568, 67, 40863], "temperature": 0.0, "avg_logprob": -0.1835658171466578, "compression_ratio": 1.8309859154929577, "no_speech_prob": 7.07182061887579e-06}, {"id": 475, "seek": 216640, "start": 2188.64, "end": 2194.76, "text": " Layers of tables of numbers. That's got a 3d tensor and so forth. So in this case, this is a single number", "tokens": [20084, 433, 295, 8020, 295, 3547, 13, 663, 311, 658, 257, 805, 67, 40863, 293, 370, 5220, 13, 407, 294, 341, 1389, 11, 341, 307, 257, 2167, 1230], "temperature": 0.0, "avg_logprob": -0.1835658171466578, "compression_ratio": 1.8309859154929577, "no_speech_prob": 7.07182061887579e-06}, {"id": 476, "seek": 219476, "start": 2194.76, "end": 2199.6000000000004, "text": " But it's still a tensor that means it's just wrapped up in the pi torch", "tokens": [583, 309, 311, 920, 257, 40863, 300, 1355, 309, 311, 445, 14226, 493, 294, 264, 3895, 27822], "temperature": 0.0, "avg_logprob": -0.23674578087352147, "compression_ratio": 1.6768558951965065, "no_speech_prob": 2.601601863716496e-06}, {"id": 477, "seek": 219476, "start": 2200.5600000000004, "end": 2203.48, "text": " Machinery that allows it to do things like calculate derivatives", "tokens": [12089, 23194, 300, 4045, 309, 281, 360, 721, 411, 8873, 33733], "temperature": 0.0, "avg_logprob": -0.23674578087352147, "compression_ratio": 1.6768558951965065, "no_speech_prob": 2.601601863716496e-06}, {"id": 478, "seek": 219476, "start": 2204.2400000000002, "end": 2206.36, "text": " But it's still just the number 11 point four six", "tokens": [583, 309, 311, 920, 445, 264, 1230, 2975, 935, 1451, 2309], "temperature": 0.0, "avg_logprob": -0.23674578087352147, "compression_ratio": 1.6768558951965065, "no_speech_prob": 2.601601863716496e-06}, {"id": 479, "seek": 219476, "start": 2210.48, "end": 2211.0, "text": " All right", "tokens": [1057, 558], "temperature": 0.0, "avg_logprob": -0.23674578087352147, "compression_ratio": 1.6768558951965065, "no_speech_prob": 2.601601863716496e-06}, {"id": 480, "seek": 219476, "start": 2211.0, "end": 2215.7200000000003, "text": " So what I'm going to do is I've got to create my parameters a B and C and I'm going to put them all in a single", "tokens": [407, 437, 286, 478, 516, 281, 360, 307, 286, 600, 658, 281, 1884, 452, 9834, 257, 363, 293, 383, 293, 286, 478, 516, 281, 829, 552, 439, 294, 257, 2167], "temperature": 0.0, "avg_logprob": -0.23674578087352147, "compression_ratio": 1.6768558951965065, "no_speech_prob": 2.601601863716496e-06}, {"id": 481, "seek": 219476, "start": 2215.96, "end": 2220.96, "text": " 1d tensor now 1d tensor is also known as a rank one tensor", "tokens": [502, 67, 40863, 586, 502, 67, 40863, 307, 611, 2570, 382, 257, 6181, 472, 40863], "temperature": 0.0, "avg_logprob": -0.23674578087352147, "compression_ratio": 1.6768558951965065, "no_speech_prob": 2.601601863716496e-06}, {"id": 482, "seek": 219476, "start": 2221.6000000000004, "end": 2223.6000000000004, "text": " So this is a rank", "tokens": [407, 341, 307, 257, 6181], "temperature": 0.0, "avg_logprob": -0.23674578087352147, "compression_ratio": 1.6768558951965065, "no_speech_prob": 2.601601863716496e-06}, {"id": 483, "seek": 222360, "start": 2223.6, "end": 2229.56, "text": " one tensor and it contains the list of numbers one point five one point five one point five and", "tokens": [472, 40863, 293, 309, 8306, 264, 1329, 295, 3547, 472, 935, 1732, 472, 935, 1732, 472, 935, 1732, 293], "temperature": 0.0, "avg_logprob": -0.20129602249354533, "compression_ratio": 1.6878612716763006, "no_speech_prob": 1.0953008313663304e-05}, {"id": 484, "seek": 222360, "start": 2230.44, "end": 2232.8399999999997, "text": " Then I've got to tell pi torch", "tokens": [1396, 286, 600, 658, 281, 980, 3895, 27822], "temperature": 0.0, "avg_logprob": -0.20129602249354533, "compression_ratio": 1.6878612716763006, "no_speech_prob": 1.0953008313663304e-05}, {"id": 485, "seek": 222360, "start": 2233.7999999999997, "end": 2235.92, "text": " That I want you to calculate the gradient", "tokens": [663, 286, 528, 291, 281, 8873, 264, 16235], "temperature": 0.0, "avg_logprob": -0.20129602249354533, "compression_ratio": 1.6878612716763006, "no_speech_prob": 1.0953008313663304e-05}, {"id": 486, "seek": 222360, "start": 2236.68, "end": 2242.7599999999998, "text": " For these numbers whenever we use them in a calculation and the way we do that is we just say requires bread", "tokens": [1171, 613, 3547, 5699, 321, 764, 552, 294, 257, 17108, 293, 264, 636, 321, 360, 300, 307, 321, 445, 584, 7029, 5961], "temperature": 0.0, "avg_logprob": -0.20129602249354533, "compression_ratio": 1.6878612716763006, "no_speech_prob": 1.0953008313663304e-05}, {"id": 487, "seek": 222360, "start": 2244.48, "end": 2246.24, "text": " So here is our", "tokens": [407, 510, 307, 527], "temperature": 0.0, "avg_logprob": -0.20129602249354533, "compression_ratio": 1.6878612716763006, "no_speech_prob": 1.0953008313663304e-05}, {"id": 488, "seek": 224624, "start": 2246.24, "end": 2254.3199999999997, "text": " Tensor it contains one point five three times and it also tells us it's we flagged it to say please calculate gradients for this", "tokens": [34306, 309, 8306, 472, 935, 1732, 1045, 1413, 293, 309, 611, 5112, 505, 309, 311, 321, 7166, 3004, 309, 281, 584, 1767, 8873, 2771, 2448, 337, 341], "temperature": 0.0, "avg_logprob": -0.20422327293539946, "compression_ratio": 1.655430711610487, "no_speech_prob": 6.643334472755669e-06}, {"id": 489, "seek": 224624, "start": 2255.3599999999997, "end": 2257.7599999999998, "text": " Particular tensor when we use it in calculations", "tokens": [4100, 14646, 40863, 562, 321, 764, 309, 294, 20448], "temperature": 0.0, "avg_logprob": -0.20422327293539946, "compression_ratio": 1.655430711610487, "no_speech_prob": 6.643334472755669e-06}, {"id": 490, "seek": 224624, "start": 2259.56, "end": 2265.12, "text": " So let's now use it in the calculation we're going to pass it to that quad MSC that's the function", "tokens": [407, 718, 311, 586, 764, 309, 294, 264, 17108, 321, 434, 516, 281, 1320, 309, 281, 300, 10787, 7395, 34, 300, 311, 264, 2445], "temperature": 0.0, "avg_logprob": -0.20422327293539946, "compression_ratio": 1.655430711610487, "no_speech_prob": 6.643334472755669e-06}, {"id": 491, "seek": 224624, "start": 2265.12, "end": 2269.8799999999997, "text": " We just created that gets the MSC a mean squared error for a set of coefficients", "tokens": [492, 445, 2942, 300, 2170, 264, 7395, 34, 257, 914, 8889, 6713, 337, 257, 992, 295, 31994], "temperature": 0.0, "avg_logprob": -0.20422327293539946, "compression_ratio": 1.655430711610487, "no_speech_prob": 6.643334472755669e-06}, {"id": 492, "seek": 226988, "start": 2269.88, "end": 2275.88, "text": " And not surprisingly, it's the same number we saw before eleven point four six. Okay", "tokens": [400, 406, 17600, 11, 309, 311, 264, 912, 1230, 321, 1866, 949, 21090, 935, 1451, 2309, 13, 1033], "temperature": 0.0, "avg_logprob": -0.24383344650268554, "compression_ratio": 1.66, "no_speech_prob": 5.0936732804984786e-06}, {"id": 493, "seek": 226988, "start": 2276.56, "end": 2279.6800000000003, "text": " Not very exciting, but there is one thing that's very exciting", "tokens": [1726, 588, 4670, 11, 457, 456, 307, 472, 551, 300, 311, 588, 4670], "temperature": 0.0, "avg_logprob": -0.24383344650268554, "compression_ratio": 1.66, "no_speech_prob": 5.0936732804984786e-06}, {"id": 494, "seek": 226988, "start": 2279.76, "end": 2282.88, "text": " It is added an extra thing to the end called grad function", "tokens": [467, 307, 3869, 364, 2857, 551, 281, 264, 917, 1219, 2771, 2445], "temperature": 0.0, "avg_logprob": -0.24383344650268554, "compression_ratio": 1.66, "no_speech_prob": 5.0936732804984786e-06}, {"id": 495, "seek": 226988, "start": 2282.88, "end": 2290.48, "text": " And this is the thing that tells us that if we wanted to high torch knows how to create calculate the gradients", "tokens": [400, 341, 307, 264, 551, 300, 5112, 505, 300, 498, 321, 1415, 281, 1090, 27822, 3255, 577, 281, 1884, 8873, 264, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.24383344650268554, "compression_ratio": 1.66, "no_speech_prob": 5.0936732804984786e-06}, {"id": 496, "seek": 226988, "start": 2291.36, "end": 2296.2400000000002, "text": " For our inputs and to tell pi torches, please go ahead and do that calculation", "tokens": [1171, 527, 15743, 293, 281, 980, 3895, 3930, 3781, 11, 1767, 352, 2286, 293, 360, 300, 17108], "temperature": 0.0, "avg_logprob": -0.24383344650268554, "compression_ratio": 1.66, "no_speech_prob": 5.0936732804984786e-06}, {"id": 497, "seek": 226988, "start": 2297.2000000000003, "end": 2299.2000000000003, "text": " You call backward", "tokens": [509, 818, 23897], "temperature": 0.0, "avg_logprob": -0.24383344650268554, "compression_ratio": 1.66, "no_speech_prob": 5.0936732804984786e-06}, {"id": 498, "seek": 229920, "start": 2299.2, "end": 2304.2799999999997, "text": " On the result of your loss function now when I run it nothing happens", "tokens": [1282, 264, 1874, 295, 428, 4470, 2445, 586, 562, 286, 1190, 309, 1825, 2314], "temperature": 0.0, "avg_logprob": -0.1892400061947176, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.6028062646000762e-06}, {"id": 499, "seek": 229920, "start": 2304.8799999999997, "end": 2310.2799999999997, "text": " It doesn't look like nothing happens. But what does happen is it's just added an attribute called grad", "tokens": [467, 1177, 380, 574, 411, 1825, 2314, 13, 583, 437, 775, 1051, 307, 309, 311, 445, 3869, 364, 19667, 1219, 2771], "temperature": 0.0, "avg_logprob": -0.1892400061947176, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.6028062646000762e-06}, {"id": 500, "seek": 229920, "start": 2310.7999999999997, "end": 2314.04, "text": " Which is the gradient to our inputs ABC. So if we run this cell", "tokens": [3013, 307, 264, 16235, 281, 527, 15743, 22342, 13, 407, 498, 321, 1190, 341, 2815], "temperature": 0.0, "avg_logprob": -0.1892400061947176, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.6028062646000762e-06}, {"id": 501, "seek": 229920, "start": 2315.4399999999996, "end": 2321.0, "text": " This tells me that if I increase a the loss will go down", "tokens": [639, 5112, 385, 300, 498, 286, 3488, 257, 264, 4470, 486, 352, 760], "temperature": 0.0, "avg_logprob": -0.1892400061947176, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.6028062646000762e-06}, {"id": 502, "seek": 229920, "start": 2322.12, "end": 2325.62, "text": " If I increase B the loss will go down a bit less", "tokens": [759, 286, 3488, 363, 264, 4470, 486, 352, 760, 257, 857, 1570], "temperature": 0.0, "avg_logprob": -0.1892400061947176, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.6028062646000762e-06}, {"id": 503, "seek": 232562, "start": 2325.62, "end": 2329.2999999999997, "text": " And if I increase C the loss will go down", "tokens": [400, 498, 286, 3488, 383, 264, 4470, 486, 352, 760], "temperature": 0.0, "avg_logprob": -0.20732990333012172, "compression_ratio": 1.775, "no_speech_prob": 2.4060682335402817e-06}, {"id": 504, "seek": 232562, "start": 2330.02, "end": 2332.02, "text": " Now we want the loss to go down", "tokens": [823, 321, 528, 264, 4470, 281, 352, 760], "temperature": 0.0, "avg_logprob": -0.20732990333012172, "compression_ratio": 1.775, "no_speech_prob": 2.4060682335402817e-06}, {"id": 505, "seek": 232562, "start": 2332.22, "end": 2335.88, "text": " Right. So that means we should increase a B and C", "tokens": [1779, 13, 407, 300, 1355, 321, 820, 3488, 257, 363, 293, 383], "temperature": 0.0, "avg_logprob": -0.20732990333012172, "compression_ratio": 1.775, "no_speech_prob": 2.4060682335402817e-06}, {"id": 506, "seek": 232562, "start": 2336.74, "end": 2339.7799999999997, "text": " Well, how much by well given that a is", "tokens": [1042, 11, 577, 709, 538, 731, 2212, 300, 257, 307], "temperature": 0.0, "avg_logprob": -0.20732990333012172, "compression_ratio": 1.775, "no_speech_prob": 2.4060682335402817e-06}, {"id": 507, "seek": 232562, "start": 2340.46, "end": 2345.58, "text": " Says if you increase a even a little bit the loss improves a lot that suggests", "tokens": [36780, 498, 291, 3488, 257, 754, 257, 707, 857, 264, 4470, 24771, 257, 688, 300, 13409], "temperature": 0.0, "avg_logprob": -0.20732990333012172, "compression_ratio": 1.775, "no_speech_prob": 2.4060682335402817e-06}, {"id": 508, "seek": 232562, "start": 2345.58, "end": 2349.02, "text": " We're a long way away from the right answer. So we should probably increase this one a lot", "tokens": [492, 434, 257, 938, 636, 1314, 490, 264, 558, 1867, 13, 407, 321, 820, 1391, 3488, 341, 472, 257, 688], "temperature": 0.0, "avg_logprob": -0.20732990333012172, "compression_ratio": 1.775, "no_speech_prob": 2.4060682335402817e-06}, {"id": 509, "seek": 234902, "start": 2349.02, "end": 2354.94, "text": " This one the second most and this one the third most. Okay, so this is saying when I increase", "tokens": [639, 472, 264, 1150, 881, 293, 341, 472, 264, 2636, 881, 13, 1033, 11, 370, 341, 307, 1566, 562, 286, 3488], "temperature": 0.0, "avg_logprob": -0.21677316029866536, "compression_ratio": 1.7107843137254901, "no_speech_prob": 1.5294045851987903e-06}, {"id": 510, "seek": 234902, "start": 2355.66, "end": 2358.1, "text": " This parameter the loss decreases", "tokens": [639, 13075, 264, 4470, 24108], "temperature": 0.0, "avg_logprob": -0.21677316029866536, "compression_ratio": 1.7107843137254901, "no_speech_prob": 1.5294045851987903e-06}, {"id": 511, "seek": 234902, "start": 2358.58, "end": 2365.82, "text": " So in other words, we want to adjust our parameters a B and C by the negative of these we want to increase", "tokens": [407, 294, 661, 2283, 11, 321, 528, 281, 4369, 527, 9834, 257, 363, 293, 383, 538, 264, 3671, 295, 613, 321, 528, 281, 3488], "temperature": 0.0, "avg_logprob": -0.21677316029866536, "compression_ratio": 1.7107843137254901, "no_speech_prob": 1.5294045851987903e-06}, {"id": 512, "seek": 234902, "start": 2366.38, "end": 2368.38, "text": " increase increase", "tokens": [3488, 3488], "temperature": 0.0, "avg_logprob": -0.21677316029866536, "compression_ratio": 1.7107843137254901, "no_speech_prob": 1.5294045851987903e-06}, {"id": 513, "seek": 234902, "start": 2368.7, "end": 2370.7, "text": " So we can do that", "tokens": [407, 321, 393, 360, 300], "temperature": 0.0, "avg_logprob": -0.21677316029866536, "compression_ratio": 1.7107843137254901, "no_speech_prob": 1.5294045851987903e-06}, {"id": 514, "seek": 234902, "start": 2371.3, "end": 2373.38, "text": " By saying okay, let's take our ABC", "tokens": [3146, 1566, 1392, 11, 718, 311, 747, 527, 22342], "temperature": 0.0, "avg_logprob": -0.21677316029866536, "compression_ratio": 1.7107843137254901, "no_speech_prob": 1.5294045851987903e-06}, {"id": 515, "seek": 237338, "start": 2373.38, "end": 2378.42, "text": " minus equals so that means equals ABC minus", "tokens": [3175, 6915, 370, 300, 1355, 6915, 22342, 3175], "temperature": 0.0, "avg_logprob": -0.21437461559589094, "compression_ratio": 1.6872427983539096, "no_speech_prob": 2.7693779429682763e-06}, {"id": 516, "seek": 237338, "start": 2379.58, "end": 2381.58, "text": " the gradient", "tokens": [264, 16235], "temperature": 0.0, "avg_logprob": -0.21437461559589094, "compression_ratio": 1.6872427983539096, "no_speech_prob": 2.7693779429682763e-06}, {"id": 517, "seek": 237338, "start": 2381.62, "end": 2387.02, "text": " But we're just going to like decrease it a bit. We don't want to jump too far. Okay, so just we're just going to go a small distance", "tokens": [583, 321, 434, 445, 516, 281, 411, 11514, 309, 257, 857, 13, 492, 500, 380, 528, 281, 3012, 886, 1400, 13, 1033, 11, 370, 445, 321, 434, 445, 516, 281, 352, 257, 1359, 4560], "temperature": 0.0, "avg_logprob": -0.21437461559589094, "compression_ratio": 1.6872427983539096, "no_speech_prob": 2.7693779429682763e-06}, {"id": 518, "seek": 237338, "start": 2387.7000000000003, "end": 2390.62, "text": " So we're going to just going to somewhat arbitrarily pick point. Oh one", "tokens": [407, 321, 434, 516, 281, 445, 516, 281, 8344, 19071, 3289, 1888, 935, 13, 876, 472], "temperature": 0.0, "avg_logprob": -0.21437461559589094, "compression_ratio": 1.6872427983539096, "no_speech_prob": 2.7693779429682763e-06}, {"id": 519, "seek": 237338, "start": 2392.7000000000003, "end": 2395.98, "text": " So that is now going to create a new set of parameters", "tokens": [407, 300, 307, 586, 516, 281, 1884, 257, 777, 992, 295, 9834], "temperature": 0.0, "avg_logprob": -0.21437461559589094, "compression_ratio": 1.6872427983539096, "no_speech_prob": 2.7693779429682763e-06}, {"id": 520, "seek": 237338, "start": 2396.34, "end": 2400.44, "text": " But you're going to be a little bit bigger than before because we subtracted negative numbers", "tokens": [583, 291, 434, 516, 281, 312, 257, 707, 857, 3801, 813, 949, 570, 321, 16390, 292, 3671, 3547], "temperature": 0.0, "avg_logprob": -0.21437461559589094, "compression_ratio": 1.6872427983539096, "no_speech_prob": 2.7693779429682763e-06}, {"id": 521, "seek": 240044, "start": 2400.44, "end": 2403.88, "text": " And we can now calculate the loss again", "tokens": [400, 321, 393, 586, 8873, 264, 4470, 797], "temperature": 0.0, "avg_logprob": -0.233580792887827, "compression_ratio": 1.5175438596491229, "no_speech_prob": 8.714292221156938e-07}, {"id": 522, "seek": 240044, "start": 2404.52, "end": 2406.52, "text": " So remember before", "tokens": [407, 1604, 949], "temperature": 0.0, "avg_logprob": -0.233580792887827, "compression_ratio": 1.5175438596491229, "no_speech_prob": 8.714292221156938e-07}, {"id": 523, "seek": 240044, "start": 2406.84, "end": 2408.84, "text": " It was eleven point four six", "tokens": [467, 390, 21090, 935, 1451, 2309], "temperature": 0.0, "avg_logprob": -0.233580792887827, "compression_ratio": 1.5175438596491229, "no_speech_prob": 8.714292221156938e-07}, {"id": 524, "seek": 240044, "start": 2409.36, "end": 2411.36, "text": " So hopefully it's going to get better", "tokens": [407, 4696, 309, 311, 516, 281, 483, 1101], "temperature": 0.0, "avg_logprob": -0.233580792887827, "compression_ratio": 1.5175438596491229, "no_speech_prob": 8.714292221156938e-07}, {"id": 525, "seek": 240044, "start": 2411.8, "end": 2413.52, "text": " Yes, I did", "tokens": [1079, 11, 286, 630], "temperature": 0.0, "avg_logprob": -0.233580792887827, "compression_ratio": 1.5175438596491229, "no_speech_prob": 8.714292221156938e-07}, {"id": 526, "seek": 240044, "start": 2413.52, "end": 2415.48, "text": " 10.11", "tokens": [1266, 13, 5348], "temperature": 0.0, "avg_logprob": -0.233580792887827, "compression_ratio": 1.5175438596491229, "no_speech_prob": 8.714292221156938e-07}, {"id": 527, "seek": 240044, "start": 2415.48, "end": 2419.92, "text": " There's one extra line of code which we didn't mention which is with torch dot no grad", "tokens": [821, 311, 472, 2857, 1622, 295, 3089, 597, 321, 994, 380, 2152, 597, 307, 365, 27822, 5893, 572, 2771], "temperature": 0.0, "avg_logprob": -0.233580792887827, "compression_ratio": 1.5175438596491229, "no_speech_prob": 8.714292221156938e-07}, {"id": 528, "seek": 240044, "start": 2421.4, "end": 2428.44, "text": " Remember earlier on we said that the parameter ABC requires grad and that means pytorch will automatically calculate", "tokens": [5459, 3071, 322, 321, 848, 300, 264, 13075, 22342, 7029, 2771, 293, 300, 1355, 25878, 284, 339, 486, 6772, 8873], "temperature": 0.0, "avg_logprob": -0.233580792887827, "compression_ratio": 1.5175438596491229, "no_speech_prob": 8.714292221156938e-07}, {"id": 529, "seek": 242844, "start": 2428.44, "end": 2432.4, "text": " Its derivative when it's used in a in a function", "tokens": [6953, 13760, 562, 309, 311, 1143, 294, 257, 294, 257, 2445], "temperature": 0.0, "avg_logprob": -0.20636361837387085, "compression_ratio": 1.7173913043478262, "no_speech_prob": 8.801000149105676e-06}, {"id": 530, "seek": 242844, "start": 2433.32, "end": 2437.96, "text": " Here it's being used in a function, but we don't want the derivative of this. This is not our loss", "tokens": [1692, 309, 311, 885, 1143, 294, 257, 2445, 11, 457, 321, 500, 380, 528, 264, 13760, 295, 341, 13, 639, 307, 406, 527, 4470], "temperature": 0.0, "avg_logprob": -0.20636361837387085, "compression_ratio": 1.7173913043478262, "no_speech_prob": 8.801000149105676e-06}, {"id": 531, "seek": 242844, "start": 2438.52, "end": 2441.96, "text": " Right. This is us updating the gradients. So this is basically", "tokens": [1779, 13, 639, 307, 505, 25113, 264, 2771, 2448, 13, 407, 341, 307, 1936], "temperature": 0.0, "avg_logprob": -0.20636361837387085, "compression_ratio": 1.7173913043478262, "no_speech_prob": 8.801000149105676e-06}, {"id": 532, "seek": 242844, "start": 2442.76, "end": 2448.84, "text": " The standard inner part of a pytorch loop and every neural net deep learning", "tokens": [440, 3832, 7284, 644, 295, 257, 25878, 284, 339, 6367, 293, 633, 18161, 2533, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.20636361837387085, "compression_ratio": 1.7173913043478262, "no_speech_prob": 8.801000149105676e-06}, {"id": 533, "seek": 242844, "start": 2449.16, "end": 2455.08, "text": " Pretty much every machine learning model at least of this style that you'll build basically looks like this", "tokens": [10693, 709, 633, 3479, 2539, 2316, 412, 1935, 295, 341, 3758, 300, 291, 603, 1322, 1936, 1542, 411, 341], "temperature": 0.0, "avg_logprob": -0.20636361837387085, "compression_ratio": 1.7173913043478262, "no_speech_prob": 8.801000149105676e-06}, {"id": 534, "seek": 245508, "start": 2455.08, "end": 2460.52, "text": " If you look deep inside fast AI source code, you'll see something that basically looks like this", "tokens": [759, 291, 574, 2452, 1854, 2370, 7318, 4009, 3089, 11, 291, 603, 536, 746, 300, 1936, 1542, 411, 341], "temperature": 0.0, "avg_logprob": -0.179734468460083, "compression_ratio": 1.5080213903743316, "no_speech_prob": 3.4465526823623804e-06}, {"id": 535, "seek": 245508, "start": 2465.84, "end": 2470.64, "text": " So we could automate that right so let's just take those steps which is we're going to", "tokens": [407, 321, 727, 31605, 300, 558, 370, 718, 311, 445, 747, 729, 4439, 597, 307, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.179734468460083, "compression_ratio": 1.5080213903743316, "no_speech_prob": 3.4465526823623804e-06}, {"id": 536, "seek": 245508, "start": 2473.24, "end": 2479.68, "text": " Calculate let's go back to here. We're going to calculate the mean squared error for our quadratic", "tokens": [3511, 2444, 473, 718, 311, 352, 646, 281, 510, 13, 492, 434, 516, 281, 8873, 264, 914, 8889, 6713, 337, 527, 37262], "temperature": 0.0, "avg_logprob": -0.179734468460083, "compression_ratio": 1.5080213903743316, "no_speech_prob": 3.4465526823623804e-06}, {"id": 537, "seek": 247968, "start": 2479.68, "end": 2487.3199999999997, "text": " Call backward and then subtract the gradient times a small number from the gradient", "tokens": [7807, 23897, 293, 550, 16390, 264, 16235, 1413, 257, 1359, 1230, 490, 264, 16235], "temperature": 0.0, "avg_logprob": -0.2581757264978745, "compression_ratio": 1.9221556886227544, "no_speech_prob": 7.411156730086077e-06}, {"id": 538, "seek": 247968, "start": 2488.56, "end": 2490.44, "text": " Let's do it five times", "tokens": [961, 311, 360, 309, 1732, 1413], "temperature": 0.0, "avg_logprob": -0.2581757264978745, "compression_ratio": 1.9221556886227544, "no_speech_prob": 7.411156730086077e-06}, {"id": 539, "seek": 247968, "start": 2490.44, "end": 2492.7799999999997, "text": " So so far we're up to a loss of 10.1", "tokens": [407, 370, 1400, 321, 434, 493, 281, 257, 4470, 295, 1266, 13, 16], "temperature": 0.0, "avg_logprob": -0.2581757264978745, "compression_ratio": 1.9221556886227544, "no_speech_prob": 7.411156730086077e-06}, {"id": 540, "seek": 247968, "start": 2493.3599999999997, "end": 2498.44, "text": " So we're going to calculate our loss call dot backward to calculate the gradients and", "tokens": [407, 321, 434, 516, 281, 8873, 527, 4470, 818, 5893, 23897, 281, 8873, 264, 2771, 2448, 293], "temperature": 0.0, "avg_logprob": -0.2581757264978745, "compression_ratio": 1.9221556886227544, "no_speech_prob": 7.411156730086077e-06}, {"id": 541, "seek": 247968, "start": 2499.44, "end": 2501.44, "text": " then with no grad", "tokens": [550, 365, 572, 2771], "temperature": 0.0, "avg_logprob": -0.2581757264978745, "compression_ratio": 1.9221556886227544, "no_speech_prob": 7.411156730086077e-06}, {"id": 542, "seek": 247968, "start": 2502.24, "end": 2504.6, "text": " subtract the gradients times a small number and", "tokens": [16390, 264, 2771, 2448, 1413, 257, 1359, 1230, 293], "temperature": 0.0, "avg_logprob": -0.2581757264978745, "compression_ratio": 1.9221556886227544, "no_speech_prob": 7.411156730086077e-06}, {"id": 543, "seek": 247968, "start": 2505.3999999999996, "end": 2507.3999999999996, "text": " print how we're going and", "tokens": [4482, 577, 321, 434, 516, 293], "temperature": 0.0, "avg_logprob": -0.2581757264978745, "compression_ratio": 1.9221556886227544, "no_speech_prob": 7.411156730086077e-06}, {"id": 544, "seek": 250740, "start": 2507.4, "end": 2512.52, "text": " There we go the loss keeps improving", "tokens": [821, 321, 352, 264, 4470, 5965, 11470], "temperature": 0.0, "avg_logprob": -0.3098538846385722, "compression_ratio": 1.2520325203252032, "no_speech_prob": 2.6425602754898136e-06}, {"id": 545, "seek": 250740, "start": 2514.08, "end": 2516.08, "text": " So we now have", "tokens": [407, 321, 586, 362], "temperature": 0.0, "avg_logprob": -0.3098538846385722, "compression_ratio": 1.2520325203252032, "no_speech_prob": 2.6425602754898136e-06}, {"id": 546, "seek": 250740, "start": 2525.08, "end": 2527.08, "text": " Some coefficients", "tokens": [2188, 31994], "temperature": 0.0, "avg_logprob": -0.3098538846385722, "compression_ratio": 1.2520325203252032, "no_speech_prob": 2.6425602754898136e-06}, {"id": 547, "seek": 250740, "start": 2528.28, "end": 2529.52, "text": " And", "tokens": [400], "temperature": 0.0, "avg_logprob": -0.3098538846385722, "compression_ratio": 1.2520325203252032, "no_speech_prob": 2.6425602754898136e-06}, {"id": 548, "seek": 250740, "start": 2529.52, "end": 2531.04, "text": " There they are", "tokens": [821, 436, 366], "temperature": 0.0, "avg_logprob": -0.3098538846385722, "compression_ratio": 1.2520325203252032, "no_speech_prob": 2.6425602754898136e-06}, {"id": 549, "seek": 253104, "start": 2531.04, "end": 2538.02, "text": " 3.2 1.9 2.0 so they're definitely heading in the right direction so", "tokens": [805, 13, 17, 502, 13, 24, 568, 13, 15, 370, 436, 434, 2138, 9864, 294, 264, 558, 3513, 370], "temperature": 0.0, "avg_logprob": -0.1943175383288451, "compression_ratio": 1.6916299559471366, "no_speech_prob": 3.288717152827303e-06}, {"id": 550, "seek": 253104, "start": 2540.48, "end": 2543.8, "text": " That's basically how we do it's called optimization", "tokens": [663, 311, 1936, 577, 321, 360, 309, 311, 1219, 19618], "temperature": 0.0, "avg_logprob": -0.1943175383288451, "compression_ratio": 1.6916299559471366, "no_speech_prob": 3.288717152827303e-06}, {"id": 551, "seek": 253104, "start": 2544.24, "end": 2549.44, "text": " Okay, so you'll hear a lot in deep learning about optimizers. This is the most basic kind of", "tokens": [1033, 11, 370, 291, 603, 1568, 257, 688, 294, 2452, 2539, 466, 5028, 22525, 13, 639, 307, 264, 881, 3875, 733, 295], "temperature": 0.0, "avg_logprob": -0.1943175383288451, "compression_ratio": 1.6916299559471366, "no_speech_prob": 3.288717152827303e-06}, {"id": 552, "seek": 253104, "start": 2550.12, "end": 2552.7599999999998, "text": " Optimizer, but they're all built on this principle", "tokens": [35013, 6545, 11, 457, 436, 434, 439, 3094, 322, 341, 8665], "temperature": 0.0, "avg_logprob": -0.1943175383288451, "compression_ratio": 1.6916299559471366, "no_speech_prob": 3.288717152827303e-06}, {"id": 553, "seek": 253104, "start": 2552.7599999999998, "end": 2558.96, "text": " Of course, it's called gradient descent and you can see why it's called gradient descent. We calculate the gradients and", "tokens": [2720, 1164, 11, 309, 311, 1219, 16235, 23475, 293, 291, 393, 536, 983, 309, 311, 1219, 16235, 23475, 13, 492, 8873, 264, 2771, 2448, 293], "temperature": 0.0, "avg_logprob": -0.1943175383288451, "compression_ratio": 1.6916299559471366, "no_speech_prob": 3.288717152827303e-06}, {"id": 554, "seek": 255896, "start": 2558.96, "end": 2563.12, "text": " Then do a descent which is we're trying to decrease the loss", "tokens": [1396, 360, 257, 23475, 597, 307, 321, 434, 1382, 281, 11514, 264, 4470], "temperature": 0.0, "avg_logprob": -0.19040323628319633, "compression_ratio": 1.5164835164835164, "no_speech_prob": 2.40608278545551e-06}, {"id": 555, "seek": 255896, "start": 2565.48, "end": 2567.4, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.19040323628319633, "compression_ratio": 1.5164835164835164, "no_speech_prob": 2.40608278545551e-06}, {"id": 556, "seek": 255896, "start": 2567.4, "end": 2569.4, "text": " Believe it or not. That's that's", "tokens": [21486, 309, 420, 406, 13, 663, 311, 300, 311], "temperature": 0.0, "avg_logprob": -0.19040323628319633, "compression_ratio": 1.5164835164835164, "no_speech_prob": 2.40608278545551e-06}, {"id": 557, "seek": 255896, "start": 2570.12, "end": 2575.92, "text": " The entire foundations of how we create those parameters. So we need one more piece", "tokens": [440, 2302, 22467, 295, 577, 321, 1884, 729, 9834, 13, 407, 321, 643, 472, 544, 2522], "temperature": 0.0, "avg_logprob": -0.19040323628319633, "compression_ratio": 1.5164835164835164, "no_speech_prob": 2.40608278545551e-06}, {"id": 558, "seek": 255896, "start": 2576.4, "end": 2579.96, "text": " Which is what is the mathematical function that we're finding parameters for?", "tokens": [3013, 307, 437, 307, 264, 18894, 2445, 300, 321, 434, 5006, 9834, 337, 30], "temperature": 0.0, "avg_logprob": -0.19040323628319633, "compression_ratio": 1.5164835164835164, "no_speech_prob": 2.40608278545551e-06}, {"id": 559, "seek": 255896, "start": 2581.28, "end": 2582.68, "text": " We can't just use", "tokens": [492, 393, 380, 445, 764], "temperature": 0.0, "avg_logprob": -0.19040323628319633, "compression_ratio": 1.5164835164835164, "no_speech_prob": 2.40608278545551e-06}, {"id": 560, "seek": 258268, "start": 2582.68, "end": 2590.52, "text": " Quadratics right because it's pretty unlikely that the relationship between parameters and whether a pixel is part of a basset hound", "tokens": [29619, 4481, 1167, 558, 570, 309, 311, 1238, 17518, 300, 264, 2480, 1296, 9834, 293, 1968, 257, 19261, 307, 644, 295, 257, 10136, 302, 276, 554], "temperature": 0.0, "avg_logprob": -0.2023942470550537, "compression_ratio": 1.5178571428571428, "no_speech_prob": 3.3405192425561836e-06}, {"id": 561, "seek": 258268, "start": 2590.8399999999997, "end": 2593.8399999999997, "text": " Is a quadratic it's going to be something much more complicated", "tokens": [1119, 257, 37262, 309, 311, 516, 281, 312, 746, 709, 544, 6179], "temperature": 0.0, "avg_logprob": -0.2023942470550537, "compression_ratio": 1.5178571428571428, "no_speech_prob": 3.3405192425561836e-06}, {"id": 562, "seek": 258268, "start": 2596.56, "end": 2598.56, "text": " No problem", "tokens": [883, 1154], "temperature": 0.0, "avg_logprob": -0.2023942470550537, "compression_ratio": 1.5178571428571428, "no_speech_prob": 3.3405192425561836e-06}, {"id": 563, "seek": 258268, "start": 2598.72, "end": 2600.72, "text": " It turns out that", "tokens": [467, 4523, 484, 300], "temperature": 0.0, "avg_logprob": -0.2023942470550537, "compression_ratio": 1.5178571428571428, "no_speech_prob": 3.3405192425561836e-06}, {"id": 564, "seek": 258268, "start": 2601.24, "end": 2606.18, "text": " We can create an infinitely flexible function from this one tiny thing", "tokens": [492, 393, 1884, 364, 36227, 11358, 2445, 490, 341, 472, 5870, 551], "temperature": 0.0, "avg_logprob": -0.2023942470550537, "compression_ratio": 1.5178571428571428, "no_speech_prob": 3.3405192425561836e-06}, {"id": 565, "seek": 258268, "start": 2607.3599999999997, "end": 2610.2, "text": " This is called a rectified linear, you know", "tokens": [639, 307, 1219, 257, 11048, 2587, 8213, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.2023942470550537, "compression_ratio": 1.5178571428571428, "no_speech_prob": 3.3405192425561836e-06}, {"id": 566, "seek": 261020, "start": 2610.2, "end": 2612.48, "text": " The first piece I'm sure you will recognize", "tokens": [440, 700, 2522, 286, 478, 988, 291, 486, 5521], "temperature": 0.0, "avg_logprob": -0.45471950010819867, "compression_ratio": 1.6127450980392157, "no_speech_prob": 5.422151389211649e-06}, {"id": 567, "seek": 261020, "start": 2613.04, "end": 2616.3599999999997, "text": " It's a linear function. We've got our output y", "tokens": [467, 311, 257, 8213, 2445, 13, 492, 600, 658, 527, 5598, 288], "temperature": 0.0, "avg_logprob": -0.45471950010819867, "compression_ratio": 1.6127450980392157, "no_speech_prob": 5.422151389211649e-06}, {"id": 568, "seek": 261020, "start": 2617.16, "end": 2619.16, "text": " our input x and", "tokens": [527, 4846, 2031, 293], "temperature": 0.0, "avg_logprob": -0.45471950010819867, "compression_ratio": 1.6127450980392157, "no_speech_prob": 5.422151389211649e-06}, {"id": 569, "seek": 261020, "start": 2619.3199999999997, "end": 2624.72, "text": " coefficients m and b this is even simpler than our quadratic and", "tokens": [31994, 275, 293, 272, 341, 307, 754, 18587, 813, 527, 37262, 293], "temperature": 0.0, "avg_logprob": -0.45471950010819867, "compression_ratio": 1.6127450980392157, "no_speech_prob": 5.422151389211649e-06}, {"id": 570, "seek": 261020, "start": 2625.48, "end": 2627.48, "text": " This is a line", "tokens": [639, 307, 257, 1622], "temperature": 0.0, "avg_logprob": -0.45471950010819867, "compression_ratio": 1.6127450980392157, "no_speech_prob": 5.422151389211649e-06}, {"id": 571, "seek": 261020, "start": 2628.8799999999997, "end": 2635.3599999999997, "text": " And torch dot clip is a function that takes that output y and if it's greater than that number", "tokens": [400, 27822, 5893, 7353, 307, 257, 2445, 300, 2516, 300, 5598, 288, 293, 498, 309, 311, 5044, 813, 300, 1230], "temperature": 0.0, "avg_logprob": -0.45471950010819867, "compression_ratio": 1.6127450980392157, "no_speech_prob": 5.422151389211649e-06}, {"id": 572, "seek": 261020, "start": 2635.8799999999997, "end": 2638.3199999999997, "text": " It turns it into that number. So in other words", "tokens": [467, 4523, 309, 666, 300, 1230, 13, 407, 294, 661, 2283], "temperature": 0.0, "avg_logprob": -0.45471950010819867, "compression_ratio": 1.6127450980392157, "no_speech_prob": 5.422151389211649e-06}, {"id": 573, "seek": 263832, "start": 2638.32, "end": 2641.6800000000003, "text": " This is going to take anything that's negative and make it zero", "tokens": [639, 307, 516, 281, 747, 1340, 300, 311, 3671, 293, 652, 309, 4018], "temperature": 0.0, "avg_logprob": -0.1715512956891741, "compression_ratio": 1.792929292929293, "no_speech_prob": 2.026126821874641e-06}, {"id": 574, "seek": 263832, "start": 2643.2000000000003, "end": 2645.2000000000003, "text": " So this function is going to do two things", "tokens": [407, 341, 2445, 307, 516, 281, 360, 732, 721], "temperature": 0.0, "avg_logprob": -0.1715512956891741, "compression_ratio": 1.792929292929293, "no_speech_prob": 2.026126821874641e-06}, {"id": 575, "seek": 263832, "start": 2646.84, "end": 2652.1600000000003, "text": " Calculate the output of a line and if it is bigger than smaller than zero, it'll make it zero", "tokens": [3511, 2444, 473, 264, 5598, 295, 257, 1622, 293, 498, 309, 307, 3801, 813, 4356, 813, 4018, 11, 309, 603, 652, 309, 4018], "temperature": 0.0, "avg_logprob": -0.1715512956891741, "compression_ratio": 1.792929292929293, "no_speech_prob": 2.026126821874641e-06}, {"id": 576, "seek": 263832, "start": 2653.6400000000003, "end": 2655.6400000000003, "text": " So that's rectified linear", "tokens": [407, 300, 311, 11048, 2587, 8213], "temperature": 0.0, "avg_logprob": -0.1715512956891741, "compression_ratio": 1.792929292929293, "no_speech_prob": 2.026126821874641e-06}, {"id": 577, "seek": 263832, "start": 2656.2000000000003, "end": 2658.2000000000003, "text": " So let's use partial", "tokens": [407, 718, 311, 764, 14641], "temperature": 0.0, "avg_logprob": -0.1715512956891741, "compression_ratio": 1.792929292929293, "no_speech_prob": 2.026126821874641e-06}, {"id": 578, "seek": 263832, "start": 2658.4, "end": 2664.44, "text": " To take that function and set the m and b to 1 and 1 so this is now going to be this function here", "tokens": [1407, 747, 300, 2445, 293, 992, 264, 275, 293, 272, 281, 502, 293, 502, 370, 341, 307, 586, 516, 281, 312, 341, 2445, 510], "temperature": 0.0, "avg_logprob": -0.1715512956891741, "compression_ratio": 1.792929292929293, "no_speech_prob": 2.026126821874641e-06}, {"id": 579, "seek": 263832, "start": 2665.0800000000004, "end": 2666.7200000000003, "text": " will be", "tokens": [486, 312], "temperature": 0.0, "avg_logprob": -0.1715512956891741, "compression_ratio": 1.792929292929293, "no_speech_prob": 2.026126821874641e-06}, {"id": 580, "seek": 266672, "start": 2666.72, "end": 2668.72, "text": " y equals x plus 1", "tokens": [288, 6915, 2031, 1804, 502], "temperature": 0.0, "avg_logprob": -0.2568369515334504, "compression_ratio": 1.5595854922279793, "no_speech_prob": 6.1440960053005256e-06}, {"id": 581, "seek": 266672, "start": 2669.16, "end": 2671.16, "text": " followed by this torch dot clip and", "tokens": [6263, 538, 341, 27822, 5893, 7353, 293], "temperature": 0.0, "avg_logprob": -0.2568369515334504, "compression_ratio": 1.5595854922279793, "no_speech_prob": 6.1440960053005256e-06}, {"id": 582, "seek": 266672, "start": 2673.0, "end": 2676.3999999999996, "text": " Here's the shape okay as we'd expect it's a line", "tokens": [1692, 311, 264, 3909, 1392, 382, 321, 1116, 2066, 309, 311, 257, 1622], "temperature": 0.0, "avg_logprob": -0.2568369515334504, "compression_ratio": 1.5595854922279793, "no_speech_prob": 6.1440960053005256e-06}, {"id": 583, "seek": 266672, "start": 2677.72, "end": 2683.52, "text": " Until it gets under zero when it comes what's still a line. It's a becomes a horizontal line", "tokens": [9088, 309, 2170, 833, 4018, 562, 309, 1487, 437, 311, 920, 257, 1622, 13, 467, 311, 257, 3643, 257, 12750, 1622], "temperature": 0.0, "avg_logprob": -0.2568369515334504, "compression_ratio": 1.5595854922279793, "no_speech_prob": 6.1440960053005256e-06}, {"id": 584, "seek": 266672, "start": 2685.9199999999996, "end": 2692.48, "text": " So we can now do the same thing we can take this plot function and make it interactive using interact and", "tokens": [407, 321, 393, 586, 360, 264, 912, 551, 321, 393, 747, 341, 7542, 2445, 293, 652, 309, 15141, 1228, 4648, 293], "temperature": 0.0, "avg_logprob": -0.2568369515334504, "compression_ratio": 1.5595854922279793, "no_speech_prob": 6.1440960053005256e-06}, {"id": 585, "seek": 269248, "start": 2692.48, "end": 2696.84, "text": " And we can see what happens when we change its two parameters m and b", "tokens": [400, 321, 393, 536, 437, 2314, 562, 321, 1319, 1080, 732, 9834, 275, 293, 272], "temperature": 0.0, "avg_logprob": -0.2805476833034206, "compression_ratio": 1.5161290322580645, "no_speech_prob": 7.889171683927998e-06}, {"id": 586, "seek": 269248, "start": 2697.2400000000002, "end": 2701.28, "text": " So we're now plotting the rectified linear and fixing m and b", "tokens": [407, 321, 434, 586, 41178, 264, 11048, 2587, 8213, 293, 19442, 275, 293, 272], "temperature": 0.0, "avg_logprob": -0.2805476833034206, "compression_ratio": 1.5161290322580645, "no_speech_prob": 7.889171683927998e-06}, {"id": 587, "seek": 269248, "start": 2702.2400000000002, "end": 2704.2400000000002, "text": " So m is the slope", "tokens": [407, 275, 307, 264, 13525], "temperature": 0.0, "avg_logprob": -0.2805476833034206, "compression_ratio": 1.5161290322580645, "no_speech_prob": 7.889171683927998e-06}, {"id": 588, "seek": 269248, "start": 2707.88, "end": 2710.16, "text": " Okay, and B is the is the", "tokens": [1033, 11, 293, 363, 307, 264, 307, 264], "temperature": 0.0, "avg_logprob": -0.2805476833034206, "compression_ratio": 1.5161290322580645, "no_speech_prob": 7.889171683927998e-06}, {"id": 589, "seek": 269248, "start": 2712.2400000000002, "end": 2714.34, "text": " Intercept or the shift up and down", "tokens": [5751, 1336, 420, 264, 5513, 493, 293, 760], "temperature": 0.0, "avg_logprob": -0.2805476833034206, "compression_ratio": 1.5161290322580645, "no_speech_prob": 7.889171683927998e-06}, {"id": 590, "seek": 269248, "start": 2715.84, "end": 2717.36, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2805476833034206, "compression_ratio": 1.5161290322580645, "no_speech_prob": 7.889171683927998e-06}, {"id": 591, "seek": 269248, "start": 2717.36, "end": 2718.92, "text": " so that's", "tokens": [370, 300, 311], "temperature": 0.0, "avg_logprob": -0.2805476833034206, "compression_ratio": 1.5161290322580645, "no_speech_prob": 7.889171683927998e-06}, {"id": 592, "seek": 269248, "start": 2718.92, "end": 2720.32, "text": " how those", "tokens": [577, 729], "temperature": 0.0, "avg_logprob": -0.2805476833034206, "compression_ratio": 1.5161290322580645, "no_speech_prob": 7.889171683927998e-06}, {"id": 593, "seek": 272032, "start": 2720.32, "end": 2724.6800000000003, "text": " Work now. Why is this interesting? Well, it's not interesting of itself", "tokens": [6603, 586, 13, 1545, 307, 341, 1880, 30, 1042, 11, 309, 311, 406, 1880, 295, 2564], "temperature": 0.0, "avg_logprob": -0.3043736898770896, "compression_ratio": 1.6857142857142857, "no_speech_prob": 1.1659292795229703e-05}, {"id": 594, "seek": 272032, "start": 2725.44, "end": 2730.32, "text": " but what we could do is we could take this rectified linear function and", "tokens": [457, 437, 321, 727, 360, 307, 321, 727, 747, 341, 11048, 2587, 8213, 2445, 293], "temperature": 0.0, "avg_logprob": -0.3043736898770896, "compression_ratio": 1.6857142857142857, "no_speech_prob": 1.1659292795229703e-05}, {"id": 595, "seek": 272032, "start": 2731.28, "end": 2733.28, "text": " create a double revenue", "tokens": [1884, 257, 3834, 9324], "temperature": 0.0, "avg_logprob": -0.3043736898770896, "compression_ratio": 1.6857142857142857, "no_speech_prob": 1.1659292795229703e-05}, {"id": 596, "seek": 272032, "start": 2733.6000000000004, "end": 2737.6800000000003, "text": " Which adds up to rectified linear functions together?", "tokens": [3013, 10860, 493, 281, 11048, 2587, 8213, 6828, 1214, 30], "temperature": 0.0, "avg_logprob": -0.3043736898770896, "compression_ratio": 1.6857142857142857, "no_speech_prob": 1.1659292795229703e-05}, {"id": 597, "seek": 272032, "start": 2738.4, "end": 2745.76, "text": " so there's some slope and 1 b 1 some slack and slope and 2 b 2 we're going to calculate it at some point x and", "tokens": [370, 456, 311, 512, 13525, 293, 502, 272, 502, 512, 29767, 293, 13525, 293, 568, 272, 568, 321, 434, 516, 281, 8873, 309, 412, 512, 935, 2031, 293], "temperature": 0.0, "avg_logprob": -0.3043736898770896, "compression_ratio": 1.6857142857142857, "no_speech_prob": 1.1659292795229703e-05}, {"id": 598, "seek": 272032, "start": 2747.56, "end": 2749.56, "text": " So let's take a look", "tokens": [407, 718, 311, 747, 257, 574], "temperature": 0.0, "avg_logprob": -0.3043736898770896, "compression_ratio": 1.6857142857142857, "no_speech_prob": 1.1659292795229703e-05}, {"id": 599, "seek": 274956, "start": 2749.56, "end": 2750.6, "text": " at", "tokens": [412], "temperature": 0.0, "avg_logprob": -0.23109955054063064, "compression_ratio": 1.812206572769953, "no_speech_prob": 1.723137393128127e-05}, {"id": 600, "seek": 274956, "start": 2750.6, "end": 2752.7599999999998, "text": " What that function looks like if we plot it", "tokens": [708, 300, 2445, 1542, 411, 498, 321, 7542, 309], "temperature": 0.0, "avg_logprob": -0.23109955054063064, "compression_ratio": 1.812206572769953, "no_speech_prob": 1.723137393128127e-05}, {"id": 601, "seek": 274956, "start": 2753.56, "end": 2758.72, "text": " And you can see what happens is we get this downward slope and then a hook and then an upward slope", "tokens": [400, 291, 393, 536, 437, 2314, 307, 321, 483, 341, 24805, 13525, 293, 550, 257, 6328, 293, 550, 364, 23452, 13525], "temperature": 0.0, "avg_logprob": -0.23109955054063064, "compression_ratio": 1.812206572769953, "no_speech_prob": 1.723137393128127e-05}, {"id": 602, "seek": 274956, "start": 2758.92, "end": 2762.6, "text": " so if I change them one it's going to change the slope of that first bit and", "tokens": [370, 498, 286, 1319, 552, 472, 309, 311, 516, 281, 1319, 264, 13525, 295, 300, 700, 857, 293], "temperature": 0.0, "avg_logprob": -0.23109955054063064, "compression_ratio": 1.812206572769953, "no_speech_prob": 1.723137393128127e-05}, {"id": 603, "seek": 274956, "start": 2764.4, "end": 2766.4, "text": " B one is going to change its position", "tokens": [363, 472, 307, 516, 281, 1319, 1080, 2535], "temperature": 0.0, "avg_logprob": -0.23109955054063064, "compression_ratio": 1.812206572769953, "no_speech_prob": 1.723137393128127e-05}, {"id": 604, "seek": 274956, "start": 2768.16, "end": 2773.72, "text": " Okay, and I'm sure you won't be surprised to hear that m2 changes the slope of the second bit and", "tokens": [1033, 11, 293, 286, 478, 988, 291, 1582, 380, 312, 6100, 281, 1568, 300, 275, 17, 2962, 264, 13525, 295, 264, 1150, 857, 293], "temperature": 0.0, "avg_logprob": -0.23109955054063064, "compression_ratio": 1.812206572769953, "no_speech_prob": 1.723137393128127e-05}, {"id": 605, "seek": 274956, "start": 2775.04, "end": 2776.72, "text": " B to", "tokens": [363, 281], "temperature": 0.0, "avg_logprob": -0.23109955054063064, "compression_ratio": 1.812206572769953, "no_speech_prob": 1.723137393128127e-05}, {"id": 606, "seek": 277672, "start": 2776.72, "end": 2779.8399999999997, "text": " Changes that location now", "tokens": [761, 10350, 300, 4914, 586], "temperature": 0.0, "avg_logprob": -0.2505014082964729, "compression_ratio": 1.7062146892655368, "no_speech_prob": 9.972854059014935e-06}, {"id": 607, "seek": 277672, "start": 2781.08, "end": 2783.24, "text": " This is interesting why", "tokens": [639, 307, 1880, 983], "temperature": 0.0, "avg_logprob": -0.2505014082964729, "compression_ratio": 1.7062146892655368, "no_speech_prob": 9.972854059014935e-06}, {"id": 608, "seek": 277672, "start": 2784.3999999999996, "end": 2786.68, "text": " Because we don't just have to do a double relu", "tokens": [1436, 321, 500, 380, 445, 362, 281, 360, 257, 3834, 1039, 84], "temperature": 0.0, "avg_logprob": -0.2505014082964729, "compression_ratio": 1.7062146892655368, "no_speech_prob": 9.972854059014935e-06}, {"id": 609, "seek": 277672, "start": 2787.2799999999997, "end": 2790.24, "text": " we could add as many relu's together as we want and", "tokens": [321, 727, 909, 382, 867, 1039, 84, 311, 1214, 382, 321, 528, 293], "temperature": 0.0, "avg_logprob": -0.2505014082964729, "compression_ratio": 1.7062146892655368, "no_speech_prob": 9.972854059014935e-06}, {"id": 610, "seek": 277672, "start": 2791.0, "end": 2799.08, "text": " If we add as many relu's together as we want then we can have an arbitrarily squiggly function and with enough relu's", "tokens": [759, 321, 909, 382, 867, 1039, 84, 311, 1214, 382, 321, 528, 550, 321, 393, 362, 364, 19071, 3289, 2339, 46737, 2445, 293, 365, 1547, 1039, 84, 311], "temperature": 0.0, "avg_logprob": -0.2505014082964729, "compression_ratio": 1.7062146892655368, "no_speech_prob": 9.972854059014935e-06}, {"id": 611, "seek": 277672, "start": 2799.72, "end": 2802.3599999999997, "text": " We can match it as close as we want", "tokens": [492, 393, 2995, 309, 382, 1998, 382, 321, 528], "temperature": 0.0, "avg_logprob": -0.2505014082964729, "compression_ratio": 1.7062146892655368, "no_speech_prob": 9.972854059014935e-06}, {"id": 612, "seek": 280236, "start": 2802.36, "end": 2808.88, "text": " right, so you could imagine incredibly squiggly like I don't know like an audio waveform of me speaking and", "tokens": [558, 11, 370, 291, 727, 3811, 6252, 2339, 46737, 411, 286, 500, 380, 458, 411, 364, 6278, 36512, 295, 385, 4124, 293], "temperature": 0.0, "avg_logprob": -0.1666834171001728, "compression_ratio": 1.5172413793103448, "no_speech_prob": 2.0904437860735925e-06}, {"id": 613, "seek": 280236, "start": 2809.6800000000003, "end": 2816.56, "text": " If I gave you a hundred million relu's to add together you could almost exactly match that", "tokens": [759, 286, 2729, 291, 257, 3262, 2459, 1039, 84, 311, 281, 909, 1214, 291, 727, 1920, 2293, 2995, 300], "temperature": 0.0, "avg_logprob": -0.1666834171001728, "compression_ratio": 1.5172413793103448, "no_speech_prob": 2.0904437860735925e-06}, {"id": 614, "seek": 280236, "start": 2818.56, "end": 2820.56, "text": " Now we want", "tokens": [823, 321, 528], "temperature": 0.0, "avg_logprob": -0.1666834171001728, "compression_ratio": 1.5172413793103448, "no_speech_prob": 2.0904437860735925e-06}, {"id": 615, "seek": 280236, "start": 2820.6, "end": 2822.6, "text": " functions that are not just", "tokens": [6828, 300, 366, 406, 445], "temperature": 0.0, "avg_logprob": -0.1666834171001728, "compression_ratio": 1.5172413793103448, "no_speech_prob": 2.0904437860735925e-06}, {"id": 616, "seek": 280236, "start": 2823.32, "end": 2826.6400000000003, "text": " That we've put in 2d we want things that can have more than one input", "tokens": [663, 321, 600, 829, 294, 568, 67, 321, 528, 721, 300, 393, 362, 544, 813, 472, 4846], "temperature": 0.0, "avg_logprob": -0.1666834171001728, "compression_ratio": 1.5172413793103448, "no_speech_prob": 2.0904437860735925e-06}, {"id": 617, "seek": 282664, "start": 2826.64, "end": 2832.6, "text": " But you can add these together across as many dimensions as you like and so exactly the same thing will give you a", "tokens": [583, 291, 393, 909, 613, 1214, 2108, 382, 867, 12819, 382, 291, 411, 293, 370, 2293, 264, 912, 551, 486, 976, 291, 257], "temperature": 0.0, "avg_logprob": -0.2724113464355469, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.453259177556902e-07}, {"id": 618, "seek": 282664, "start": 2833.56, "end": 2835.56, "text": " relu over surfaces or", "tokens": [1039, 84, 670, 16130, 420], "temperature": 0.0, "avg_logprob": -0.2724113464355469, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.453259177556902e-07}, {"id": 619, "seek": 282664, "start": 2836.08, "end": 2838.08, "text": " a relu over", "tokens": [257, 1039, 84, 670], "temperature": 0.0, "avg_logprob": -0.2724113464355469, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.453259177556902e-07}, {"id": 620, "seek": 282664, "start": 2838.4, "end": 2843.4, "text": " 3d 40 50 and so forth and it's the same idea with this", "tokens": [805, 67, 3356, 2625, 293, 370, 5220, 293, 309, 311, 264, 912, 1558, 365, 341], "temperature": 0.0, "avg_logprob": -0.2724113464355469, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.453259177556902e-07}, {"id": 621, "seek": 282664, "start": 2844.56, "end": 2846.56, "text": " incredibly simple foundation", "tokens": [6252, 2199, 7030], "temperature": 0.0, "avg_logprob": -0.2724113464355469, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.453259177556902e-07}, {"id": 622, "seek": 282664, "start": 2846.96, "end": 2850.24, "text": " You can construct an arbitrarily", "tokens": [509, 393, 7690, 364, 19071, 3289], "temperature": 0.0, "avg_logprob": -0.2724113464355469, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.453259177556902e-07}, {"id": 623, "seek": 282664, "start": 2851.3199999999997, "end": 2853.3199999999997, "text": " accurate precise", "tokens": [8559, 13600], "temperature": 0.0, "avg_logprob": -0.2724113464355469, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.453259177556902e-07}, {"id": 624, "seek": 282664, "start": 2853.68, "end": 2855.68, "text": " model", "tokens": [2316], "temperature": 0.0, "avg_logprob": -0.2724113464355469, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.453259177556902e-07}, {"id": 625, "seek": 285568, "start": 2855.68, "end": 2857.68, "text": " a", "tokens": [257], "temperature": 0.0, "avg_logprob": -0.2545824519923476, "compression_ratio": 1.4333333333333333, "no_speech_prob": 6.7479418248694856e-06}, {"id": 626, "seek": 285568, "start": 2859.08, "end": 2860.64, "text": " Problem is", "tokens": [11676, 307], "temperature": 0.0, "avg_logprob": -0.2545824519923476, "compression_ratio": 1.4333333333333333, "no_speech_prob": 6.7479418248694856e-06}, {"id": 627, "seek": 285568, "start": 2860.64, "end": 2863.08, "text": " You need some numbers for them you need parameters. Oh", "tokens": [509, 643, 512, 3547, 337, 552, 291, 643, 9834, 13, 876], "temperature": 0.0, "avg_logprob": -0.2545824519923476, "compression_ratio": 1.4333333333333333, "no_speech_prob": 6.7479418248694856e-06}, {"id": 628, "seek": 285568, "start": 2863.8399999999997, "end": 2866.2799999999997, "text": " No problem. We know how to get parameters", "tokens": [883, 1154, 13, 492, 458, 577, 281, 483, 9834], "temperature": 0.0, "avg_logprob": -0.2545824519923476, "compression_ratio": 1.4333333333333333, "no_speech_prob": 6.7479418248694856e-06}, {"id": 629, "seek": 285568, "start": 2866.8799999999997, "end": 2868.8799999999997, "text": " We use gradient descent", "tokens": [492, 764, 16235, 23475], "temperature": 0.0, "avg_logprob": -0.2545824519923476, "compression_ratio": 1.4333333333333333, "no_speech_prob": 6.7479418248694856e-06}, {"id": 630, "seek": 285568, "start": 2869.7999999999997, "end": 2871.7999999999997, "text": " So believe it or not", "tokens": [407, 1697, 309, 420, 406], "temperature": 0.0, "avg_logprob": -0.2545824519923476, "compression_ratio": 1.4333333333333333, "no_speech_prob": 6.7479418248694856e-06}, {"id": 631, "seek": 285568, "start": 2872.64, "end": 2875.68, "text": " We have just derived deep learning", "tokens": [492, 362, 445, 18949, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.2545824519923476, "compression_ratio": 1.4333333333333333, "no_speech_prob": 6.7479418248694856e-06}, {"id": 632, "seek": 285568, "start": 2877.12, "end": 2879.12, "text": " everything from now on is", "tokens": [1203, 490, 586, 322, 307], "temperature": 0.0, "avg_logprob": -0.2545824519923476, "compression_ratio": 1.4333333333333333, "no_speech_prob": 6.7479418248694856e-06}, {"id": 633, "seek": 287912, "start": 2879.12, "end": 2885.2799999999997, "text": " Tweaks to make it faster and make it need less data", "tokens": [47763, 5461, 281, 652, 309, 4663, 293, 652, 309, 643, 1570, 1412], "temperature": 0.0, "avg_logprob": -0.19716365858056079, "compression_ratio": 1.63681592039801, "no_speech_prob": 7.888992513471749e-06}, {"id": 634, "seek": 287912, "start": 2888.2, "end": 2890.52, "text": " You know this is this is it", "tokens": [509, 458, 341, 307, 341, 307, 309], "temperature": 0.0, "avg_logprob": -0.19716365858056079, "compression_ratio": 1.63681592039801, "no_speech_prob": 7.888992513471749e-06}, {"id": 635, "seek": 287912, "start": 2892.16, "end": 2895.3599999999997, "text": " Now I remember a few years ago when I said something like this in a class", "tokens": [823, 286, 1604, 257, 1326, 924, 2057, 562, 286, 848, 746, 411, 341, 294, 257, 1508], "temperature": 0.0, "avg_logprob": -0.19716365858056079, "compression_ratio": 1.63681592039801, "no_speech_prob": 7.888992513471749e-06}, {"id": 636, "seek": 287912, "start": 2895.6, "end": 2899.2799999999997, "text": " Somebody on the forum was like this reminds me of that thing about how to draw an owl", "tokens": [13463, 322, 264, 17542, 390, 411, 341, 12025, 385, 295, 300, 551, 466, 577, 281, 2642, 364, 34488], "temperature": 0.0, "avg_logprob": -0.19716365858056079, "compression_ratio": 1.63681592039801, "no_speech_prob": 7.888992513471749e-06}, {"id": 637, "seek": 287912, "start": 2899.96, "end": 2902.16, "text": " Jeremy's basically saying okay step one", "tokens": [17809, 311, 1936, 1566, 1392, 1823, 472], "temperature": 0.0, "avg_logprob": -0.19716365858056079, "compression_ratio": 1.63681592039801, "no_speech_prob": 7.888992513471749e-06}, {"id": 638, "seek": 287912, "start": 2902.7999999999997, "end": 2904.72, "text": " draw two circles", "tokens": [2642, 732, 13040], "temperature": 0.0, "avg_logprob": -0.19716365858056079, "compression_ratio": 1.63681592039801, "no_speech_prob": 7.888992513471749e-06}, {"id": 639, "seek": 287912, "start": 2904.72, "end": 2906.72, "text": " Step to draw the rest of the owl", "tokens": [5470, 281, 2642, 264, 1472, 295, 264, 34488], "temperature": 0.0, "avg_logprob": -0.19716365858056079, "compression_ratio": 1.63681592039801, "no_speech_prob": 7.888992513471749e-06}, {"id": 640, "seek": 290672, "start": 2906.72, "end": 2908.08, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.20600619174466275, "compression_ratio": 1.6852589641434264, "no_speech_prob": 5.422073172667297e-06}, {"id": 641, "seek": 290672, "start": 2908.08, "end": 2913.48, "text": " Thing I'm I find I have a lot of trouble explaining to students is when it comes to deep learning", "tokens": [30902, 286, 478, 286, 915, 286, 362, 257, 688, 295, 5253, 13468, 281, 1731, 307, 562, 309, 1487, 281, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.20600619174466275, "compression_ratio": 1.6852589641434264, "no_speech_prob": 5.422073172667297e-06}, {"id": 642, "seek": 290672, "start": 2913.48, "end": 2915.9199999999996, "text": " there's nothing between these two steps when you have", "tokens": [456, 311, 1825, 1296, 613, 732, 4439, 562, 291, 362], "temperature": 0.0, "avg_logprob": -0.20600619174466275, "compression_ratio": 1.6852589641434264, "no_speech_prob": 5.422073172667297e-06}, {"id": 643, "seek": 290672, "start": 2917.08, "end": 2919.08, "text": " values getting added together and", "tokens": [4190, 1242, 3869, 1214, 293], "temperature": 0.0, "avg_logprob": -0.20600619174466275, "compression_ratio": 1.6852589641434264, "no_speech_prob": 5.422073172667297e-06}, {"id": 644, "seek": 290672, "start": 2920.04, "end": 2922.24, "text": " gradient descent to optimize the parameters and", "tokens": [16235, 23475, 281, 19719, 264, 9834, 293], "temperature": 0.0, "avg_logprob": -0.20600619174466275, "compression_ratio": 1.6852589641434264, "no_speech_prob": 5.422073172667297e-06}, {"id": 645, "seek": 290672, "start": 2923.3599999999997, "end": 2925.3599999999997, "text": " samples of inputs and outputs that you want", "tokens": [10938, 295, 15743, 293, 23930, 300, 291, 528], "temperature": 0.0, "avg_logprob": -0.20600619174466275, "compression_ratio": 1.6852589641434264, "no_speech_prob": 5.422073172667297e-06}, {"id": 646, "seek": 290672, "start": 2926.3999999999996, "end": 2930.7999999999997, "text": " The computer draws the owl right? That's that's that's it", "tokens": [440, 3820, 20045, 264, 34488, 558, 30, 663, 311, 300, 311, 300, 311, 309], "temperature": 0.0, "avg_logprob": -0.20600619174466275, "compression_ratio": 1.6852589641434264, "no_speech_prob": 5.422073172667297e-06}, {"id": 647, "seek": 290672, "start": 2931.7999999999997, "end": 2935.52, "text": " So we're going to learn about all these other tweaks and they're all very important", "tokens": [407, 321, 434, 516, 281, 1466, 466, 439, 613, 661, 46664, 293, 436, 434, 439, 588, 1021], "temperature": 0.0, "avg_logprob": -0.20600619174466275, "compression_ratio": 1.6852589641434264, "no_speech_prob": 5.422073172667297e-06}, {"id": 648, "seek": 293552, "start": 2935.52, "end": 2942.8, "text": " But when you come down to like trying to understand something in deep learning just try to keep coming back to remind yourself", "tokens": [583, 562, 291, 808, 760, 281, 411, 1382, 281, 1223, 746, 294, 2452, 2539, 445, 853, 281, 1066, 1348, 646, 281, 4160, 1803], "temperature": 0.0, "avg_logprob": -0.16006224342946257, "compression_ratio": 1.6567796610169492, "no_speech_prob": 8.013343176571652e-06}, {"id": 649, "seek": 293552, "start": 2944.12, "end": 2945.96, "text": " Of what it's doing", "tokens": [2720, 437, 309, 311, 884], "temperature": 0.0, "avg_logprob": -0.16006224342946257, "compression_ratio": 1.6567796610169492, "no_speech_prob": 8.013343176571652e-06}, {"id": 650, "seek": 293552, "start": 2945.96, "end": 2951.1, "text": " Which it's using gradient descent to set some parameters to make a wiggly function", "tokens": [3013, 309, 311, 1228, 16235, 23475, 281, 992, 512, 9834, 281, 652, 257, 261, 46737, 2445], "temperature": 0.0, "avg_logprob": -0.16006224342946257, "compression_ratio": 1.6567796610169492, "no_speech_prob": 8.013343176571652e-06}, {"id": 651, "seek": 293552, "start": 2951.1, "end": 2955.6, "text": " Which is basically the addition of lots of rectified linear units or something very similar to that", "tokens": [3013, 307, 1936, 264, 4500, 295, 3195, 295, 11048, 2587, 8213, 6815, 420, 746, 588, 2531, 281, 300], "temperature": 0.0, "avg_logprob": -0.16006224342946257, "compression_ratio": 1.6567796610169492, "no_speech_prob": 8.013343176571652e-06}, {"id": 652, "seek": 293552, "start": 2956.36, "end": 2958.36, "text": " match your data", "tokens": [2995, 428, 1412], "temperature": 0.0, "avg_logprob": -0.16006224342946257, "compression_ratio": 1.6567796610169492, "no_speech_prob": 8.013343176571652e-06}, {"id": 653, "seek": 295836, "start": 2958.36, "end": 2964.7400000000002, "text": " Okay, so we've got some questions on the forum", "tokens": [1033, 11, 370, 321, 600, 658, 512, 1651, 322, 264, 17542], "temperature": 0.0, "avg_logprob": -0.24006925310407365, "compression_ratio": 1.6517412935323383, "no_speech_prob": 6.438845048251096e-06}, {"id": 654, "seek": 295836, "start": 2967.48, "end": 2973.1200000000003, "text": " Okay, so question from Zakiya with six up votes so for those of you", "tokens": [1033, 11, 370, 1168, 490, 1176, 7421, 3016, 365, 2309, 493, 12068, 370, 337, 729, 295, 291], "temperature": 0.0, "avg_logprob": -0.24006925310407365, "compression_ratio": 1.6517412935323383, "no_speech_prob": 6.438845048251096e-06}, {"id": 655, "seek": 295836, "start": 2974.08, "end": 2977.8, "text": " Watching the video what we do in the lesson is we want to make sure that the", "tokens": [28482, 264, 960, 437, 321, 360, 294, 264, 6898, 307, 321, 528, 281, 652, 988, 300, 264], "temperature": 0.0, "avg_logprob": -0.24006925310407365, "compression_ratio": 1.6517412935323383, "no_speech_prob": 6.438845048251096e-06}, {"id": 656, "seek": 295836, "start": 2978.1600000000003, "end": 2981.04, "text": " Questions that you hear answered are the ones that people really care about", "tokens": [27738, 300, 291, 1568, 10103, 366, 264, 2306, 300, 561, 534, 1127, 466], "temperature": 0.0, "avg_logprob": -0.24006925310407365, "compression_ratio": 1.6517412935323383, "no_speech_prob": 6.438845048251096e-06}, {"id": 657, "seek": 295836, "start": 2982.0, "end": 2986.04, "text": " So we pick the ones which get the most up votes this question is", "tokens": [407, 321, 1888, 264, 2306, 597, 483, 264, 881, 493, 12068, 341, 1168, 307], "temperature": 0.0, "avg_logprob": -0.24006925310407365, "compression_ratio": 1.6517412935323383, "no_speech_prob": 6.438845048251096e-06}, {"id": 658, "seek": 298604, "start": 2986.04, "end": 2993.2599999999998, "text": " Is there perhaps a way to try out all the different models and automatically find the best-performing one?", "tokens": [1119, 456, 4317, 257, 636, 281, 853, 484, 439, 264, 819, 5245, 293, 6772, 915, 264, 1151, 12, 26765, 278, 472, 30], "temperature": 0.0, "avg_logprob": -0.20126974129978614, "compression_ratio": 1.5776699029126213, "no_speech_prob": 2.2252656890486833e-06}, {"id": 659, "seek": 298604, "start": 2996.52, "end": 2999.92, "text": " Yes, absolutely you can do that so", "tokens": [1079, 11, 3122, 291, 393, 360, 300, 370], "temperature": 0.0, "avg_logprob": -0.20126974129978614, "compression_ratio": 1.5776699029126213, "no_speech_prob": 2.2252656890486833e-06}, {"id": 660, "seek": 298604, "start": 3003.68, "end": 3008.4, "text": " If we go back to our trading script remember there's this thing called best models and", "tokens": [759, 321, 352, 646, 281, 527, 9529, 5755, 1604, 456, 311, 341, 551, 1219, 1151, 5245, 293], "temperature": 0.0, "avg_logprob": -0.20126974129978614, "compression_ratio": 1.5776699029126213, "no_speech_prob": 2.2252656890486833e-06}, {"id": 661, "seek": 298604, "start": 3009.24, "end": 3013.0, "text": " It's a list of strings so you can easily add a for loop around this", "tokens": [467, 311, 257, 1329, 295, 13985, 370, 291, 393, 3612, 909, 257, 337, 6367, 926, 341], "temperature": 0.0, "avg_logprob": -0.20126974129978614, "compression_ratio": 1.5776699029126213, "no_speech_prob": 2.2252656890486833e-06}, {"id": 662, "seek": 301300, "start": 3013.0, "end": 3016.36, "text": " So it basically goes, you know for", "tokens": [407, 309, 1936, 1709, 11, 291, 458, 337], "temperature": 0.0, "avg_logprob": -0.21329429626464844, "compression_ratio": 1.5833333333333333, "no_speech_prob": 6.7479418248694856e-06}, {"id": 663, "seek": 301300, "start": 3018.56, "end": 3026.0, "text": " Architecture in team dot list models and you could do the whole lot which would be like that and then you could", "tokens": [43049, 294, 1469, 5893, 1329, 5245, 293, 291, 727, 360, 264, 1379, 688, 597, 576, 312, 411, 300, 293, 550, 291, 727], "temperature": 0.0, "avg_logprob": -0.21329429626464844, "compression_ratio": 1.5833333333333333, "no_speech_prob": 6.7479418248694856e-06}, {"id": 664, "seek": 301300, "start": 3028.64, "end": 3030.64, "text": " Do that and away you go", "tokens": [1144, 300, 293, 1314, 291, 352], "temperature": 0.0, "avg_logprob": -0.21329429626464844, "compression_ratio": 1.5833333333333333, "no_speech_prob": 6.7479418248694856e-06}, {"id": 665, "seek": 301300, "start": 3031.52, "end": 3034.62, "text": " It's going to take a long time for 500 and something models", "tokens": [467, 311, 516, 281, 747, 257, 938, 565, 337, 5923, 293, 746, 5245], "temperature": 0.0, "avg_logprob": -0.21329429626464844, "compression_ratio": 1.5833333333333333, "no_speech_prob": 6.7479418248694856e-06}, {"id": 666, "seek": 301300, "start": 3035.88, "end": 3039.92, "text": " So generally speaking like I've I've never done anything like that myself", "tokens": [407, 5101, 4124, 411, 286, 600, 286, 600, 1128, 1096, 1340, 411, 300, 2059], "temperature": 0.0, "avg_logprob": -0.21329429626464844, "compression_ratio": 1.5833333333333333, "no_speech_prob": 6.7479418248694856e-06}, {"id": 667, "seek": 303992, "start": 3039.92, "end": 3044.7200000000003, "text": " I would rather look at a picture like this and say like okay. Where am I in?", "tokens": [286, 576, 2831, 574, 412, 257, 3036, 411, 341, 293, 584, 411, 1392, 13, 2305, 669, 286, 294, 30], "temperature": 0.0, "avg_logprob": -0.19529703611968666, "compression_ratio": 1.6069868995633187, "no_speech_prob": 1.2029139725200366e-05}, {"id": 668, "seek": 303992, "start": 3045.7200000000003, "end": 3051.12, "text": " the vast majority of the time this is something this would be the biggest Eric and number one mistake of", "tokens": [264, 8369, 6286, 295, 264, 565, 341, 307, 746, 341, 576, 312, 264, 3880, 9336, 293, 1230, 472, 6146, 295], "temperature": 0.0, "avg_logprob": -0.19529703611968666, "compression_ratio": 1.6069868995633187, "no_speech_prob": 1.2029139725200366e-05}, {"id": 669, "seek": 303992, "start": 3052.6, "end": 3055.56, "text": " Beginners I see is that they jump to these models", "tokens": [20660, 2999, 286, 536, 307, 300, 436, 3012, 281, 613, 5245], "temperature": 0.0, "avg_logprob": -0.19529703611968666, "compression_ratio": 1.6069868995633187, "no_speech_prob": 1.2029139725200366e-05}, {"id": 670, "seek": 303992, "start": 3056.92, "end": 3061.76, "text": " From the start of a new project at the start of a new project. I pretty much only use resident 18", "tokens": [3358, 264, 722, 295, 257, 777, 1716, 412, 264, 722, 295, 257, 777, 1716, 13, 286, 1238, 709, 787, 764, 10832, 2443], "temperature": 0.0, "avg_logprob": -0.19529703611968666, "compression_ratio": 1.6069868995633187, "no_speech_prob": 1.2029139725200366e-05}, {"id": 671, "seek": 303992, "start": 3064.0, "end": 3066.0, "text": " Because I want to spend all of my time", "tokens": [1436, 286, 528, 281, 3496, 439, 295, 452, 565], "temperature": 0.0, "avg_logprob": -0.19529703611968666, "compression_ratio": 1.6069868995633187, "no_speech_prob": 1.2029139725200366e-05}, {"id": 672, "seek": 306600, "start": 3066.0, "end": 3071.0, "text": " Trying things out and I try different data augmentation. I'm going to try different ways of cleaning the data", "tokens": [20180, 721, 484, 293, 286, 853, 819, 1412, 14501, 19631, 13, 286, 478, 516, 281, 853, 819, 2098, 295, 8924, 264, 1412], "temperature": 0.0, "avg_logprob": -0.19672118292914498, "compression_ratio": 1.702970297029703, "no_speech_prob": 5.0146804824180435e-06}, {"id": 673, "seek": 306600, "start": 3071.96, "end": 3073.96, "text": " I'm going to try", "tokens": [286, 478, 516, 281, 853], "temperature": 0.0, "avg_logprob": -0.19672118292914498, "compression_ratio": 1.702970297029703, "no_speech_prob": 5.0146804824180435e-06}, {"id": 674, "seek": 306600, "start": 3074.48, "end": 3076.48, "text": " You know", "tokens": [509, 458], "temperature": 0.0, "avg_logprob": -0.19672118292914498, "compression_ratio": 1.702970297029703, "no_speech_prob": 5.0146804824180435e-06}, {"id": 675, "seek": 306600, "start": 3077.8, "end": 3082.92, "text": " Different external data I can bring in and so I want to be trying lots of things and I want to be able to try it", "tokens": [20825, 8320, 1412, 286, 393, 1565, 294, 293, 370, 286, 528, 281, 312, 1382, 3195, 295, 721, 293, 286, 528, 281, 312, 1075, 281, 853, 309], "temperature": 0.0, "avg_logprob": -0.19672118292914498, "compression_ratio": 1.702970297029703, "no_speech_prob": 5.0146804824180435e-06}, {"id": 676, "seek": 306600, "start": 3082.92, "end": 3085.4, "text": " As fast as possible, right? So", "tokens": [1018, 2370, 382, 1944, 11, 558, 30, 407], "temperature": 0.0, "avg_logprob": -0.19672118292914498, "compression_ratio": 1.702970297029703, "no_speech_prob": 5.0146804824180435e-06}, {"id": 677, "seek": 306600, "start": 3088.52, "end": 3093.28, "text": " Trying better architectures is the very last thing that I do and", "tokens": [20180, 1101, 6331, 1303, 307, 264, 588, 1036, 551, 300, 286, 360, 293], "temperature": 0.0, "avg_logprob": -0.19672118292914498, "compression_ratio": 1.702970297029703, "no_speech_prob": 5.0146804824180435e-06}, {"id": 678, "seek": 309328, "start": 3093.28, "end": 3097.6400000000003, "text": " What I do is once I've spent all this time and I've got to the point where I've got okay", "tokens": [708, 286, 360, 307, 1564, 286, 600, 4418, 439, 341, 565, 293, 286, 600, 658, 281, 264, 935, 689, 286, 600, 658, 1392], "temperature": 0.0, "avg_logprob": -0.19402994428362166, "compression_ratio": 1.7217391304347827, "no_speech_prob": 1.9832361431326717e-05}, {"id": 679, "seek": 309328, "start": 3097.6400000000003, "end": 3101.7200000000003, "text": " I've got my resident 18 or maybe you know resident 34 because it's nearly as fast", "tokens": [286, 600, 658, 452, 10832, 2443, 420, 1310, 291, 458, 10832, 12790, 570, 309, 311, 6217, 382, 2370], "temperature": 0.0, "avg_logprob": -0.19402994428362166, "compression_ratio": 1.7217391304347827, "no_speech_prob": 1.9832361431326717e-05}, {"id": 680, "seek": 309328, "start": 3103.52, "end": 3105.0400000000004, "text": " And", "tokens": [400], "temperature": 0.0, "avg_logprob": -0.19402994428362166, "compression_ratio": 1.7217391304347827, "no_speech_prob": 1.9832361431326717e-05}, {"id": 681, "seek": 309328, "start": 3105.0400000000004, "end": 3107.0400000000004, "text": " I'm like, okay. Well, how accurate is it?", "tokens": [286, 478, 411, 11, 1392, 13, 1042, 11, 577, 8559, 307, 309, 30], "temperature": 0.0, "avg_logprob": -0.19402994428362166, "compression_ratio": 1.7217391304347827, "no_speech_prob": 1.9832361431326717e-05}, {"id": 682, "seek": 309328, "start": 3107.5600000000004, "end": 3109.5600000000004, "text": " How fast is it?", "tokens": [1012, 2370, 307, 309, 30], "temperature": 0.0, "avg_logprob": -0.19402994428362166, "compression_ratio": 1.7217391304347827, "no_speech_prob": 1.9832361431326717e-05}, {"id": 683, "seek": 309328, "start": 3109.6800000000003, "end": 3112.2000000000003, "text": " Do I need it more accurate for what I'm doing?", "tokens": [1144, 286, 643, 309, 544, 8559, 337, 437, 286, 478, 884, 30], "temperature": 0.0, "avg_logprob": -0.19402994428362166, "compression_ratio": 1.7217391304347827, "no_speech_prob": 1.9832361431326717e-05}, {"id": 684, "seek": 309328, "start": 3112.6400000000003, "end": 3115.0400000000004, "text": " Do I need it faster for what I'm doing?", "tokens": [1144, 286, 643, 309, 4663, 337, 437, 286, 478, 884, 30], "temperature": 0.0, "avg_logprob": -0.19402994428362166, "compression_ratio": 1.7217391304347827, "no_speech_prob": 1.9832361431326717e-05}, {"id": 685, "seek": 309328, "start": 3115.0400000000004, "end": 3118.7200000000003, "text": " Could I accept some trade-off to make it a bit slower to make more accurate?", "tokens": [7497, 286, 3241, 512, 4923, 12, 4506, 281, 652, 309, 257, 857, 14009, 281, 652, 544, 8559, 30], "temperature": 0.0, "avg_logprob": -0.19402994428362166, "compression_ratio": 1.7217391304347827, "no_speech_prob": 1.9832361431326717e-05}, {"id": 686, "seek": 311872, "start": 3118.72, "end": 3123.4199999999996, "text": " And so then I'll have a look and I'll say okay. Well, I kind of need to be somewhere around point. No one seconds", "tokens": [400, 370, 550, 286, 603, 362, 257, 574, 293, 286, 603, 584, 1392, 13, 1042, 11, 286, 733, 295, 643, 281, 312, 4079, 926, 935, 13, 883, 472, 3949], "temperature": 0.0, "avg_logprob": -0.18930756128751314, "compression_ratio": 1.5959183673469388, "no_speech_prob": 7.527828984166263e-06}, {"id": 687, "seek": 311872, "start": 3123.4199999999996, "end": 3125.4199999999996, "text": " And so I try a few of these", "tokens": [400, 370, 286, 853, 257, 1326, 295, 613], "temperature": 0.0, "avg_logprob": -0.18930756128751314, "compression_ratio": 1.5959183673469388, "no_speech_prob": 7.527828984166263e-06}, {"id": 688, "seek": 311872, "start": 3126.6, "end": 3128.6, "text": " So that would be how I would think about", "tokens": [407, 300, 576, 312, 577, 286, 576, 519, 466], "temperature": 0.0, "avg_logprob": -0.18930756128751314, "compression_ratio": 1.5959183673469388, "no_speech_prob": 7.527828984166263e-06}, {"id": 689, "seek": 311872, "start": 3129.3999999999996, "end": 3131.3999999999996, "text": " that", "tokens": [300], "temperature": 0.0, "avg_logprob": -0.18930756128751314, "compression_ratio": 1.5959183673469388, "no_speech_prob": 7.527828984166263e-06}, {"id": 690, "seek": 311872, "start": 3133.2, "end": 3135.2, "text": " Okay next question from the forum", "tokens": [1033, 958, 1168, 490, 264, 17542], "temperature": 0.0, "avg_logprob": -0.18930756128751314, "compression_ratio": 1.5959183673469388, "no_speech_prob": 7.527828984166263e-06}, {"id": 691, "seek": 311872, "start": 3135.8399999999997, "end": 3142.8799999999997, "text": " Is around how do I know if I have enough data? What are some signs that indicate my problem needs more data?", "tokens": [1119, 926, 577, 360, 286, 458, 498, 286, 362, 1547, 1412, 30, 708, 366, 512, 7880, 300, 13330, 452, 1154, 2203, 544, 1412, 30], "temperature": 0.0, "avg_logprob": -0.18930756128751314, "compression_ratio": 1.5959183673469388, "no_speech_prob": 7.527828984166263e-06}, {"id": 692, "seek": 314288, "start": 3142.88, "end": 3149.8, "text": " I think it's pretty similar to the architecture question. So you've got some amount of data", "tokens": [286, 519, 309, 311, 1238, 2531, 281, 264, 9482, 1168, 13, 407, 291, 600, 658, 512, 2372, 295, 1412], "temperature": 0.0, "avg_logprob": -0.15518098996009355, "compression_ratio": 1.575, "no_speech_prob": 2.64256345872127e-06}, {"id": 693, "seek": 314288, "start": 3151.08, "end": 3157.88, "text": " Presumably you've you know, you've started using all the data that you have access to you've built your model. You've done your best", "tokens": [2718, 449, 1188, 291, 600, 291, 458, 11, 291, 600, 1409, 1228, 439, 264, 1412, 300, 291, 362, 2105, 281, 291, 600, 3094, 428, 2316, 13, 509, 600, 1096, 428, 1151], "temperature": 0.0, "avg_logprob": -0.15518098996009355, "compression_ratio": 1.575, "no_speech_prob": 2.64256345872127e-06}, {"id": 694, "seek": 314288, "start": 3159.4, "end": 3161.4, "text": " Is it good enough?", "tokens": [1119, 309, 665, 1547, 30], "temperature": 0.0, "avg_logprob": -0.15518098996009355, "compression_ratio": 1.575, "no_speech_prob": 2.64256345872127e-06}, {"id": 695, "seek": 314288, "start": 3161.84, "end": 3165.92, "text": " Do you have the accuracy that you need for whatever it is you're doing?", "tokens": [1144, 291, 362, 264, 14170, 300, 291, 643, 337, 2035, 309, 307, 291, 434, 884, 30], "temperature": 0.0, "avg_logprob": -0.15518098996009355, "compression_ratio": 1.575, "no_speech_prob": 2.64256345872127e-06}, {"id": 696, "seek": 316592, "start": 3165.92, "end": 3172.92, "text": " You can't know until you've trained the model but as you've seen it only takes a few minutes to train a quick model", "tokens": [509, 393, 380, 458, 1826, 291, 600, 8895, 264, 2316, 457, 382, 291, 600, 1612, 309, 787, 2516, 257, 1326, 2077, 281, 3847, 257, 1702, 2316], "temperature": 0.0, "avg_logprob": -0.3872966766357422, "compression_ratio": 1.645320197044335, "no_speech_prob": 2.225241360065411e-06}, {"id": 697, "seek": 316592, "start": 3172.92, "end": 3178.12, "text": " So my very strong opinion is that the vast majority of", "tokens": [407, 452, 588, 2068, 4800, 307, 300, 264, 8369, 6286, 295], "temperature": 0.0, "avg_logprob": -0.3872966766357422, "compression_ratio": 1.645320197044335, "no_speech_prob": 2.225241360065411e-06}, {"id": 698, "seek": 316592, "start": 3178.12, "end": 3184.12, "text": " Projects I see in industry wait far too long before they train their first model", "tokens": [9849, 82, 286, 536, 294, 3518, 1699, 1400, 886, 938, 949, 436, 3847, 641, 700, 2316], "temperature": 0.0, "avg_logprob": -0.3872966766357422, "compression_ratio": 1.645320197044335, "no_speech_prob": 2.225241360065411e-06}, {"id": 699, "seek": 316592, "start": 3184.12, "end": 3188.92, "text": " You know in my opinion you want to train your first model on day one with whatever", "tokens": [509, 458, 294, 452, 4800, 291, 528, 281, 3847, 428, 700, 2316, 322, 786, 472, 365, 2035], "temperature": 0.0, "avg_logprob": -0.3872966766357422, "compression_ratio": 1.645320197044335, "no_speech_prob": 2.225241360065411e-06}, {"id": 700, "seek": 318892, "start": 3188.92, "end": 3195.32, "text": " CSB files or whatever that you can hack together and you might be surprised that", "tokens": [9460, 33, 7098, 420, 2035, 300, 291, 393, 10339, 1214, 293, 291, 1062, 312, 6100, 300], "temperature": 0.0, "avg_logprob": -0.32341444364158056, "compression_ratio": 1.6818181818181819, "no_speech_prob": 2.3320458240050357e-06}, {"id": 701, "seek": 318892, "start": 3195.32, "end": 3200.28, "text": " None of the fancy stuff you're thinking of doing is necessary because you already have a good enough accuracy for what you need", "tokens": [14492, 295, 264, 10247, 1507, 291, 434, 1953, 295, 884, 307, 4818, 570, 291, 1217, 362, 257, 665, 1547, 14170, 337, 437, 291, 643], "temperature": 0.0, "avg_logprob": -0.32341444364158056, "compression_ratio": 1.6818181818181819, "no_speech_prob": 2.3320458240050357e-06}, {"id": 702, "seek": 318892, "start": 3200.28, "end": 3207.08, "text": " Or you might find quite the opposite you might find that oh my god with we're basically getting no accuracy at all", "tokens": [1610, 291, 1062, 915, 1596, 264, 6182, 291, 1062, 915, 300, 1954, 452, 3044, 365, 321, 434, 1936, 1242, 572, 14170, 412, 439], "temperature": 0.0, "avg_logprob": -0.32341444364158056, "compression_ratio": 1.6818181818181819, "no_speech_prob": 2.3320458240050357e-06}, {"id": 703, "seek": 318892, "start": 3207.08, "end": 3209.08, "text": " Maybe it's impossible", "tokens": [2704, 309, 311, 6243], "temperature": 0.0, "avg_logprob": -0.32341444364158056, "compression_ratio": 1.6818181818181819, "no_speech_prob": 2.3320458240050357e-06}, {"id": 704, "seek": 318892, "start": 3209.08, "end": 3213.48, "text": " These are things you want to know at the start not at the end", "tokens": [1981, 366, 721, 291, 528, 281, 458, 412, 264, 722, 406, 412, 264, 917], "temperature": 0.0, "avg_logprob": -0.32341444364158056, "compression_ratio": 1.6818181818181819, "no_speech_prob": 2.3320458240050357e-06}, {"id": 705, "seek": 321348, "start": 3213.48, "end": 3220.48, "text": " We'll learn lots of techniques both in this part of the course and in part two about ways to really get the most out of your data", "tokens": [492, 603, 1466, 3195, 295, 7512, 1293, 294, 341, 644, 295, 264, 1164, 293, 294, 644, 732, 466, 2098, 281, 534, 483, 264, 881, 484, 295, 428, 1412], "temperature": 0.0, "avg_logprob": -0.20949878870883834, "compression_ratio": 1.8195488721804511, "no_speech_prob": 1.0129743714060169e-05}, {"id": 706, "seek": 321348, "start": 3220.48, "end": 3225.48, "text": " In particular, there's a reasonably recent technique called semi-supervised learning", "tokens": [682, 1729, 11, 456, 311, 257, 23551, 5162, 6532, 1219, 12909, 12, 48172, 24420, 2539], "temperature": 0.0, "avg_logprob": -0.20949878870883834, "compression_ratio": 1.8195488721804511, "no_speech_prob": 1.0129743714060169e-05}, {"id": 707, "seek": 321348, "start": 3225.48, "end": 3231.48, "text": " Which actually lets you get dramatically more out of your data and we've also started talking already about data augmentation", "tokens": [3013, 767, 6653, 291, 483, 17548, 544, 484, 295, 428, 1412, 293, 321, 600, 611, 1409, 1417, 1217, 466, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.20949878870883834, "compression_ratio": 1.8195488721804511, "no_speech_prob": 1.0129743714060169e-05}, {"id": 708, "seek": 321348, "start": 3231.48, "end": 3235.48, "text": " Which is a classic technique you can use so you generally speaking", "tokens": [3013, 307, 257, 7230, 6532, 291, 393, 764, 370, 291, 5101, 4124], "temperature": 0.0, "avg_logprob": -0.20949878870883834, "compression_ratio": 1.8195488721804511, "no_speech_prob": 1.0129743714060169e-05}, {"id": 709, "seek": 321348, "start": 3235.48, "end": 3239.48, "text": " It depends how expensive is it going to be to get the most out of your data?", "tokens": [467, 5946, 577, 5124, 307, 309, 516, 281, 312, 281, 483, 264, 881, 484, 295, 428, 1412, 30], "temperature": 0.0, "avg_logprob": -0.20949878870883834, "compression_ratio": 1.8195488721804511, "no_speech_prob": 1.0129743714060169e-05}, {"id": 710, "seek": 323948, "start": 3239.48, "end": 3243.48, "text": " And also what do you mean when you say get more data? Do you mean more labeled data?", "tokens": [400, 611, 437, 360, 291, 914, 562, 291, 584, 483, 544, 1412, 30, 1144, 291, 914, 544, 21335, 1412, 30], "temperature": 0.0, "avg_logprob": -0.11852239825061917, "compression_ratio": 1.5991735537190082, "no_speech_prob": 2.4681516151758842e-05}, {"id": 711, "seek": 323948, "start": 3243.48, "end": 3248.48, "text": " Often it's easy to get lots of inputs and hard to get lots of outputs", "tokens": [20043, 309, 311, 1858, 281, 483, 3195, 295, 15743, 293, 1152, 281, 483, 3195, 295, 23930], "temperature": 0.0, "avg_logprob": -0.11852239825061917, "compression_ratio": 1.5991735537190082, "no_speech_prob": 2.4681516151758842e-05}, {"id": 712, "seek": 323948, "start": 3248.48, "end": 3251.48, "text": " For example in medical imaging where I've spent a lot of time", "tokens": [1171, 1365, 294, 4625, 25036, 689, 286, 600, 4418, 257, 688, 295, 565], "temperature": 0.0, "avg_logprob": -0.11852239825061917, "compression_ratio": 1.5991735537190082, "no_speech_prob": 2.4681516151758842e-05}, {"id": 713, "seek": 323948, "start": 3251.48, "end": 3257.48, "text": " It's generally super easy to jump into the radiology archive and grab more CT scans", "tokens": [467, 311, 5101, 1687, 1858, 281, 3012, 666, 264, 16335, 1793, 23507, 293, 4444, 544, 19529, 35116], "temperature": 0.0, "avg_logprob": -0.11852239825061917, "compression_ratio": 1.5991735537190082, "no_speech_prob": 2.4681516151758842e-05}, {"id": 714, "seek": 323948, "start": 3257.48, "end": 3261.48, "text": " But it's might be very difficult and expensive to", "tokens": [583, 309, 311, 1062, 312, 588, 2252, 293, 5124, 281], "temperature": 0.0, "avg_logprob": -0.11852239825061917, "compression_ratio": 1.5991735537190082, "no_speech_prob": 2.4681516151758842e-05}, {"id": 715, "seek": 323948, "start": 3261.48, "end": 3264.48, "text": " You know draw segmentation masks and", "tokens": [509, 458, 2642, 9469, 399, 11830, 293], "temperature": 0.0, "avg_logprob": -0.11852239825061917, "compression_ratio": 1.5991735537190082, "no_speech_prob": 2.4681516151758842e-05}, {"id": 716, "seek": 326448, "start": 3264.48, "end": 3269.48, "text": " Pixel boundaries and so forth on them", "tokens": [28323, 13180, 293, 370, 5220, 322, 552], "temperature": 0.0, "avg_logprob": -0.17765983782316508, "compression_ratio": 1.495049504950495, "no_speech_prob": 1.5688296116422862e-05}, {"id": 717, "seek": 326448, "start": 3269.48, "end": 3272.48, "text": " So often you can get more", "tokens": [407, 2049, 291, 393, 483, 544], "temperature": 0.0, "avg_logprob": -0.17765983782316508, "compression_ratio": 1.495049504950495, "no_speech_prob": 1.5688296116422862e-05}, {"id": 718, "seek": 326448, "start": 3272.48, "end": 3278.48, "text": " You know in this case images or text or whatever and maybe it's harder to get labels", "tokens": [509, 458, 294, 341, 1389, 5267, 420, 2487, 420, 2035, 293, 1310, 309, 311, 6081, 281, 483, 16949], "temperature": 0.0, "avg_logprob": -0.17765983782316508, "compression_ratio": 1.495049504950495, "no_speech_prob": 1.5688296116422862e-05}, {"id": 719, "seek": 326448, "start": 3278.48, "end": 3283.48, "text": " And again there's a lot of stuff you can do using things like we'll discuss semi-supervised learning", "tokens": [400, 797, 456, 311, 257, 688, 295, 1507, 291, 393, 360, 1228, 721, 411, 321, 603, 2248, 12909, 12, 48172, 24420, 2539], "temperature": 0.0, "avg_logprob": -0.17765983782316508, "compression_ratio": 1.495049504950495, "no_speech_prob": 1.5688296116422862e-05}, {"id": 720, "seek": 326448, "start": 3283.48, "end": 3287.48, "text": " To actually take advantage of unlabeled data as well", "tokens": [1407, 767, 747, 5002, 295, 32118, 18657, 292, 1412, 382, 731], "temperature": 0.0, "avg_logprob": -0.17765983782316508, "compression_ratio": 1.495049504950495, "no_speech_prob": 1.5688296116422862e-05}, {"id": 721, "seek": 328748, "start": 3287.48, "end": 3297.48, "text": " Okay final question here in the quadratic example where we calculated the initial derivatives for a B and C", "tokens": [1033, 2572, 1168, 510, 294, 264, 37262, 1365, 689, 321, 15598, 264, 5883, 33733, 337, 257, 363, 293, 383], "temperature": 0.0, "avg_logprob": -0.19550516294396442, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.9525101379258558e-05}, {"id": 722, "seek": 328748, "start": 3297.48, "end": 3302.48, "text": " We got values of minus 10.8 minus 2.4. Etc. What unit of these expressed in?", "tokens": [492, 658, 4190, 295, 3175, 1266, 13, 23, 3175, 568, 13, 19, 13, 3790, 66, 13, 708, 4985, 295, 613, 12675, 294, 30], "temperature": 0.0, "avg_logprob": -0.19550516294396442, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.9525101379258558e-05}, {"id": 723, "seek": 328748, "start": 3302.48, "end": 3305.48, "text": " Why don't we adjust our parameters by these values themselves?", "tokens": [1545, 500, 380, 321, 4369, 527, 9834, 538, 613, 4190, 2969, 30], "temperature": 0.0, "avg_logprob": -0.19550516294396442, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.9525101379258558e-05}, {"id": 724, "seek": 328748, "start": 3305.48, "end": 3309.48, "text": " So I guess the question here is why are we multiplying it by a small number?", "tokens": [407, 286, 2041, 264, 1168, 510, 307, 983, 366, 321, 30955, 309, 538, 257, 1359, 1230, 30], "temperature": 0.0, "avg_logprob": -0.19550516294396442, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.9525101379258558e-05}, {"id": 725, "seek": 328748, "start": 3309.48, "end": 3311.48, "text": " Which in this case is 0.01?", "tokens": [3013, 294, 341, 1389, 307, 1958, 13, 10607, 30], "temperature": 0.0, "avg_logprob": -0.19550516294396442, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.9525101379258558e-05}, {"id": 726, "seek": 331148, "start": 3311.48, "end": 3317.48, "text": " Okay, let's take those two parts of the question", "tokens": [1033, 11, 718, 311, 747, 729, 732, 3166, 295, 264, 1168], "temperature": 0.0, "avg_logprob": -0.2513544114969544, "compression_ratio": 1.4586466165413534, "no_speech_prob": 1.1842691492347512e-05}, {"id": 727, "seek": 331148, "start": 3317.48, "end": 3320.48, "text": " What's the unit here?", "tokens": [708, 311, 264, 4985, 510, 30], "temperature": 0.0, "avg_logprob": -0.2513544114969544, "compression_ratio": 1.4586466165413534, "no_speech_prob": 1.1842691492347512e-05}, {"id": 728, "seek": 331148, "start": 3320.48, "end": 3326.48, "text": " The unit is for each increase in X of 1", "tokens": [440, 4985, 307, 337, 1184, 3488, 294, 1783, 295, 502], "temperature": 0.0, "avg_logprob": -0.2513544114969544, "compression_ratio": 1.4586466165413534, "no_speech_prob": 1.1842691492347512e-05}, {"id": 729, "seek": 331148, "start": 3326.48, "end": 3333.48, "text": " How much does what I say in for each increase in in a of 1 so if I increase a from?", "tokens": [1012, 709, 775, 437, 286, 584, 294, 337, 1184, 3488, 294, 294, 257, 295, 502, 370, 498, 286, 3488, 257, 490, 30], "temperature": 0.0, "avg_logprob": -0.2513544114969544, "compression_ratio": 1.4586466165413534, "no_speech_prob": 1.1842691492347512e-05}, {"id": 730, "seek": 333348, "start": 3333.48, "end": 3342.48, "text": " In this case we have 1.5. So if we increase from 1.5 to 2.5", "tokens": [682, 341, 1389, 321, 362, 502, 13, 20, 13, 407, 498, 321, 3488, 490, 502, 13, 20, 281, 568, 13, 20], "temperature": 0.0, "avg_logprob": -0.2513824370970209, "compression_ratio": 1.625, "no_speech_prob": 1.2805125152226537e-05}, {"id": 731, "seek": 333348, "start": 3342.48, "end": 3348.48, "text": " What would happen to the loss and the answer is it would go down by 10.9887", "tokens": [708, 576, 1051, 281, 264, 4470, 293, 264, 1867, 307, 309, 576, 352, 760, 538, 1266, 13, 22516, 23853], "temperature": 0.0, "avg_logprob": -0.2513824370970209, "compression_ratio": 1.625, "no_speech_prob": 1.2805125152226537e-05}, {"id": 732, "seek": 333348, "start": 3348.48, "end": 3353.48, "text": " Now that's not exactly right because it's kind of like", "tokens": [823, 300, 311, 406, 2293, 558, 570, 309, 311, 733, 295, 411], "temperature": 0.0, "avg_logprob": -0.2513824370970209, "compression_ratio": 1.625, "no_speech_prob": 1.2805125152226537e-05}, {"id": 733, "seek": 333348, "start": 3353.48, "end": 3360.48, "text": " It's kind of like in an infinitely small space right because it's kind of like in a small space", "tokens": [467, 311, 733, 295, 411, 294, 364, 36227, 1359, 1901, 558, 570, 309, 311, 733, 295, 411, 294, 257, 1359, 1901], "temperature": 0.0, "avg_logprob": -0.2513824370970209, "compression_ratio": 1.625, "no_speech_prob": 1.2805125152226537e-05}, {"id": 734, "seek": 336048, "start": 3360.48, "end": 3364.48, "text": " It's kind of like in an infinitely small space right because actually it's going to be curved right?", "tokens": [467, 311, 733, 295, 411, 294, 364, 36227, 1359, 1901, 558, 570, 767, 309, 311, 516, 281, 312, 24991, 558, 30], "temperature": 0.0, "avg_logprob": -0.17547562681598428, "compression_ratio": 1.6632124352331605, "no_speech_prob": 1.1842672392958775e-05}, {"id": 735, "seek": 336048, "start": 3364.48, "end": 3368.48, "text": " But if it if it stays it stayed at that slope. That's what would happen", "tokens": [583, 498, 309, 498, 309, 10834, 309, 9181, 412, 300, 13525, 13, 663, 311, 437, 576, 1051], "temperature": 0.0, "avg_logprob": -0.17547562681598428, "compression_ratio": 1.6632124352331605, "no_speech_prob": 1.1842672392958775e-05}, {"id": 736, "seek": 336048, "start": 3368.48, "end": 3377.48, "text": " So if we increased B by 1 the loss would decrease if it stayed constant", "tokens": [407, 498, 321, 6505, 363, 538, 502, 264, 4470, 576, 11514, 498, 309, 9181, 5754], "temperature": 0.0, "avg_logprob": -0.17547562681598428, "compression_ratio": 1.6632124352331605, "no_speech_prob": 1.1842672392958775e-05}, {"id": 737, "seek": 336048, "start": 3377.48, "end": 3382.48, "text": " You know if the slope stayed the same the loss would decrease by minus 2.122", "tokens": [509, 458, 498, 264, 13525, 9181, 264, 912, 264, 4470, 576, 11514, 538, 3175, 568, 13, 4762, 17], "temperature": 0.0, "avg_logprob": -0.17547562681598428, "compression_ratio": 1.6632124352331605, "no_speech_prob": 1.1842672392958775e-05}, {"id": 738, "seek": 338248, "start": 3382.48, "end": 3390.48, "text": " Okay, so why would we not just change it directly by these numbers?", "tokens": [1033, 11, 370, 983, 576, 321, 406, 445, 1319, 309, 3838, 538, 613, 3547, 30], "temperature": 0.0, "avg_logprob": -0.12316119103204637, "compression_ratio": 1.2521008403361344, "no_speech_prob": 5.173842055228306e-06}, {"id": 739, "seek": 338248, "start": 3390.48, "end": 3395.48, "text": " Well the reason is", "tokens": [1042, 264, 1778, 307], "temperature": 0.0, "avg_logprob": -0.12316119103204637, "compression_ratio": 1.2521008403361344, "no_speech_prob": 5.173842055228306e-06}, {"id": 740, "seek": 338248, "start": 3395.48, "end": 3403.48, "text": " The reason is that if we", "tokens": [440, 1778, 307, 300, 498, 321], "temperature": 0.0, "avg_logprob": -0.12316119103204637, "compression_ratio": 1.2521008403361344, "no_speech_prob": 5.173842055228306e-06}, {"id": 741, "seek": 338248, "start": 3403.48, "end": 3411.48, "text": " Have some function that we're fitting", "tokens": [3560, 512, 2445, 300, 321, 434, 15669], "temperature": 0.0, "avg_logprob": -0.12316119103204637, "compression_ratio": 1.2521008403361344, "no_speech_prob": 5.173842055228306e-06}, {"id": 742, "seek": 341148, "start": 3411.48, "end": 3418.48, "text": " And there's some kind of interesting theory that says that once you get close enough to the the optimal value", "tokens": [400, 456, 311, 512, 733, 295, 1880, 5261, 300, 1619, 300, 1564, 291, 483, 1998, 1547, 281, 264, 264, 16252, 2158], "temperature": 0.0, "avg_logprob": -0.09230701658460828, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.972797670343425e-06}, {"id": 743, "seek": 341148, "start": 3418.48, "end": 3425.48, "text": " All functions look like quadratics anyway, right so we can kind of safely draw it in this kind of shape", "tokens": [1057, 6828, 574, 411, 10787, 4481, 1167, 4033, 11, 558, 370, 321, 393, 733, 295, 11750, 2642, 309, 294, 341, 733, 295, 3909], "temperature": 0.0, "avg_logprob": -0.09230701658460828, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.972797670343425e-06}, {"id": 744, "seek": 341148, "start": 3425.48, "end": 3428.48, "text": " Because this is what they end up looking like if you get close enough", "tokens": [1436, 341, 307, 437, 436, 917, 493, 1237, 411, 498, 291, 483, 1998, 1547], "temperature": 0.0, "avg_logprob": -0.09230701658460828, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.972797670343425e-06}, {"id": 745, "seek": 341148, "start": 3428.48, "end": 3433.48, "text": " And we're like let's say we're way out", "tokens": [400, 321, 434, 411, 718, 311, 584, 321, 434, 636, 484], "temperature": 0.0, "avg_logprob": -0.09230701658460828, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.972797670343425e-06}, {"id": 746, "seek": 341148, "start": 3433.48, "end": 3437.48, "text": " Over here. Okay, so we were measuring", "tokens": [4886, 510, 13, 1033, 11, 370, 321, 645, 13389], "temperature": 0.0, "avg_logprob": -0.09230701658460828, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.972797670343425e-06}, {"id": 747, "seek": 343748, "start": 3437.48, "end": 3444.48, "text": " I'll use my daughter's favorite pens and I sparkly ones so we're measuring the slope here", "tokens": [286, 603, 764, 452, 4653, 311, 2954, 6099, 293, 286, 9908, 356, 2306, 370, 321, 434, 13389, 264, 13525, 510], "temperature": 0.0, "avg_logprob": -0.09885280196731155, "compression_ratio": 1.6404494382022472, "no_speech_prob": 8.267724297184031e-06}, {"id": 748, "seek": 343748, "start": 3444.48, "end": 3447.48, "text": " There's a very steep slope", "tokens": [821, 311, 257, 588, 16841, 13525], "temperature": 0.0, "avg_logprob": -0.09885280196731155, "compression_ratio": 1.6404494382022472, "no_speech_prob": 8.267724297184031e-06}, {"id": 749, "seek": 343748, "start": 3447.48, "end": 3451.48, "text": " So that seems to suggest we should jump a really long way", "tokens": [407, 300, 2544, 281, 3402, 321, 820, 3012, 257, 534, 938, 636], "temperature": 0.0, "avg_logprob": -0.09885280196731155, "compression_ratio": 1.6404494382022472, "no_speech_prob": 8.267724297184031e-06}, {"id": 750, "seek": 343748, "start": 3451.48, "end": 3454.48, "text": " So we jump a really long way", "tokens": [407, 321, 3012, 257, 534, 938, 636], "temperature": 0.0, "avg_logprob": -0.09885280196731155, "compression_ratio": 1.6404494382022472, "no_speech_prob": 8.267724297184031e-06}, {"id": 751, "seek": 343748, "start": 3454.48, "end": 3462.48, "text": " And what happened well we jumped way too far and the reason is that that slope decreased", "tokens": [400, 437, 2011, 731, 321, 13864, 636, 886, 1400, 293, 264, 1778, 307, 300, 300, 13525, 24436], "temperature": 0.0, "avg_logprob": -0.09885280196731155, "compression_ratio": 1.6404494382022472, "no_speech_prob": 8.267724297184031e-06}, {"id": 752, "seek": 346248, "start": 3462.48, "end": 3471.48, "text": " As we move along and so that's generally what's going to happen right particularly as you approach the optimal is generally the slopes going to decrease", "tokens": [1018, 321, 1286, 2051, 293, 370, 300, 311, 5101, 437, 311, 516, 281, 1051, 558, 4098, 382, 291, 3109, 264, 16252, 307, 5101, 264, 37725, 516, 281, 11514], "temperature": 0.0, "avg_logprob": -0.10760907332102458, "compression_ratio": 1.7204301075268817, "no_speech_prob": 5.093661911814706e-06}, {"id": 753, "seek": 346248, "start": 3471.48, "end": 3475.48, "text": " So that's why we multiply the gradient by a small number", "tokens": [407, 300, 311, 983, 321, 12972, 264, 16235, 538, 257, 1359, 1230], "temperature": 0.0, "avg_logprob": -0.10760907332102458, "compression_ratio": 1.7204301075268817, "no_speech_prob": 5.093661911814706e-06}, {"id": 754, "seek": 346248, "start": 3475.48, "end": 3482.48, "text": " And that small number is a very very very important number it has a special name", "tokens": [400, 300, 1359, 1230, 307, 257, 588, 588, 588, 1021, 1230, 309, 575, 257, 2121, 1315], "temperature": 0.0, "avg_logprob": -0.10760907332102458, "compression_ratio": 1.7204301075268817, "no_speech_prob": 5.093661911814706e-06}, {"id": 755, "seek": 346248, "start": 3482.48, "end": 3489.48, "text": " It's called the learning rate", "tokens": [467, 311, 1219, 264, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.10760907332102458, "compression_ratio": 1.7204301075268817, "no_speech_prob": 5.093661911814706e-06}, {"id": 756, "seek": 348948, "start": 3489.48, "end": 3498.48, "text": " And this is an example of a hyper parameter it's not a parameter it's not one of the actual coefficients of your function", "tokens": [400, 341, 307, 364, 1365, 295, 257, 9848, 13075, 309, 311, 406, 257, 13075, 309, 311, 406, 472, 295, 264, 3539, 31994, 295, 428, 2445], "temperature": 0.0, "avg_logprob": -0.10288558107741336, "compression_ratio": 1.7777777777777777, "no_speech_prob": 5.093650997878285e-06}, {"id": 757, "seek": 348948, "start": 3498.48, "end": 3502.48, "text": " But it's a parameter you use to calculate the parameters", "tokens": [583, 309, 311, 257, 13075, 291, 764, 281, 8873, 264, 9834], "temperature": 0.0, "avg_logprob": -0.10288558107741336, "compression_ratio": 1.7777777777777777, "no_speech_prob": 5.093650997878285e-06}, {"id": 758, "seek": 348948, "start": 3502.48, "end": 3507.48, "text": " Pretty better right it's a hyper parameter and so it's something you have to pick", "tokens": [10693, 1101, 558, 309, 311, 257, 9848, 13075, 293, 370, 309, 311, 746, 291, 362, 281, 1888], "temperature": 0.0, "avg_logprob": -0.10288558107741336, "compression_ratio": 1.7777777777777777, "no_speech_prob": 5.093650997878285e-06}, {"id": 759, "seek": 348948, "start": 3507.48, "end": 3510.48, "text": " Now we haven't picked any yet", "tokens": [823, 321, 2378, 380, 6183, 604, 1939], "temperature": 0.0, "avg_logprob": -0.10288558107741336, "compression_ratio": 1.7777777777777777, "no_speech_prob": 5.093650997878285e-06}, {"id": 760, "seek": 348948, "start": 3510.48, "end": 3516.48, "text": " In any of the stuff we've done that I remember and that's because fast AI generally picks reasonable defaults", "tokens": [682, 604, 295, 264, 1507, 321, 600, 1096, 300, 286, 1604, 293, 300, 311, 570, 2370, 7318, 5101, 16137, 10585, 7576, 82], "temperature": 0.0, "avg_logprob": -0.10288558107741336, "compression_ratio": 1.7777777777777777, "no_speech_prob": 5.093650997878285e-06}, {"id": 761, "seek": 351648, "start": 3516.48, "end": 3524.48, "text": " For most things but later in the course we will learn about how to try and find really good learning rates", "tokens": [1171, 881, 721, 457, 1780, 294, 264, 1164, 321, 486, 1466, 466, 577, 281, 853, 293, 915, 534, 665, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.0661859722698436, "compression_ratio": 1.6684491978609626, "no_speech_prob": 3.822715734713711e-05}, {"id": 762, "seek": 351648, "start": 3524.48, "end": 3530.48, "text": " And you will find sometimes you need to actually spend some time finding a good learning rate", "tokens": [400, 291, 486, 915, 2171, 291, 643, 281, 767, 3496, 512, 565, 5006, 257, 665, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.0661859722698436, "compression_ratio": 1.6684491978609626, "no_speech_prob": 3.822715734713711e-05}, {"id": 763, "seek": 351648, "start": 3530.48, "end": 3538.48, "text": " You could probably understand the intuition here if you pick a learning rate that's too big you'll jump too far", "tokens": [509, 727, 1391, 1223, 264, 24002, 510, 498, 291, 1888, 257, 2539, 3314, 300, 311, 886, 955, 291, 603, 3012, 886, 1400], "temperature": 0.0, "avg_logprob": -0.0661859722698436, "compression_ratio": 1.6684491978609626, "no_speech_prob": 3.822715734713711e-05}, {"id": 764, "seek": 353848, "start": 3538.48, "end": 3547.48, "text": " And so you'll end up way over here and then you will try to then jump back again and you'll jump too far the other way", "tokens": [400, 370, 291, 603, 917, 493, 636, 670, 510, 293, 550, 291, 486, 853, 281, 550, 3012, 646, 797, 293, 291, 603, 3012, 886, 1400, 264, 661, 636], "temperature": 0.0, "avg_logprob": -0.06686314975514132, "compression_ratio": 1.745, "no_speech_prob": 1.5445752069354057e-05}, {"id": 765, "seek": 353848, "start": 3547.48, "end": 3554.48, "text": " And you'll actually diverge and so if you ever see when your model's training it's getting worse and worse", "tokens": [400, 291, 603, 767, 18558, 432, 293, 370, 498, 291, 1562, 536, 562, 428, 2316, 311, 3097, 309, 311, 1242, 5324, 293, 5324], "temperature": 0.0, "avg_logprob": -0.06686314975514132, "compression_ratio": 1.745, "no_speech_prob": 1.5445752069354057e-05}, {"id": 766, "seek": 353848, "start": 3554.48, "end": 3557.48, "text": " Probably means your learning rates too big", "tokens": [9210, 1355, 428, 2539, 6846, 886, 955], "temperature": 0.0, "avg_logprob": -0.06686314975514132, "compression_ratio": 1.745, "no_speech_prob": 1.5445752069354057e-05}, {"id": 767, "seek": 353848, "start": 3557.48, "end": 3562.48, "text": " What would happen on the other hand if you pick a learning rate that's too small", "tokens": [708, 576, 1051, 322, 264, 661, 1011, 498, 291, 1888, 257, 2539, 3314, 300, 311, 886, 1359], "temperature": 0.0, "avg_logprob": -0.06686314975514132, "compression_ratio": 1.745, "no_speech_prob": 1.5445752069354057e-05}, {"id": 768, "seek": 356248, "start": 3562.48, "end": 3571.48, "text": " Then you're going to take tiny steps and of course the flatter it gets the smaller the steps are going to get", "tokens": [1396, 291, 434, 516, 281, 747, 5870, 4439, 293, 295, 1164, 264, 41247, 309, 2170, 264, 4356, 264, 4439, 366, 516, 281, 483], "temperature": 0.0, "avg_logprob": -0.06287561930142917, "compression_ratio": 1.715736040609137, "no_speech_prob": 5.682271876139566e-06}, {"id": 769, "seek": 356248, "start": 3571.48, "end": 3574.48, "text": " And so you're going to get very very bored", "tokens": [400, 370, 291, 434, 516, 281, 483, 588, 588, 13521], "temperature": 0.0, "avg_logprob": -0.06287561930142917, "compression_ratio": 1.715736040609137, "no_speech_prob": 5.682271876139566e-06}, {"id": 770, "seek": 356248, "start": 3574.48, "end": 3580.48, "text": " So finding the right learning rate is a compromise between the speed at which you find the answer", "tokens": [407, 5006, 264, 558, 2539, 3314, 307, 257, 18577, 1296, 264, 3073, 412, 597, 291, 915, 264, 1867], "temperature": 0.0, "avg_logprob": -0.06287561930142917, "compression_ratio": 1.715736040609137, "no_speech_prob": 5.682271876139566e-06}, {"id": 771, "seek": 356248, "start": 3580.48, "end": 3585.48, "text": " And the possibility that you're actually going to shoot past it and get worse and worse", "tokens": [400, 264, 7959, 300, 291, 434, 767, 516, 281, 3076, 1791, 309, 293, 483, 5324, 293, 5324], "temperature": 0.0, "avg_logprob": -0.06287561930142917, "compression_ratio": 1.715736040609137, "no_speech_prob": 5.682271876139566e-06}, {"id": 772, "seek": 358548, "start": 3585.48, "end": 3592.48, "text": " Okay so one of the bits of feedback I got quite a lot in the survey is that people want a break half way through", "tokens": [1033, 370, 472, 295, 264, 9239, 295, 5824, 286, 658, 1596, 257, 688, 294, 264, 8984, 307, 300, 561, 528, 257, 1821, 1922, 636, 807], "temperature": 0.0, "avg_logprob": -0.11819527298212051, "compression_ratio": 1.4382716049382716, "no_speech_prob": 1.5688263374613598e-05}, {"id": 773, "seek": 358548, "start": 3592.48, "end": 3595.48, "text": " Which I think is a good idea so I think now is a good time to have a break", "tokens": [3013, 286, 519, 307, 257, 665, 1558, 370, 286, 519, 586, 307, 257, 665, 565, 281, 362, 257, 1821], "temperature": 0.0, "avg_logprob": -0.11819527298212051, "compression_ratio": 1.4382716049382716, "no_speech_prob": 1.5688263374613598e-05}, {"id": 774, "seek": 358548, "start": 3595.48, "end": 3600.48, "text": " So let's come back in 10 minutes at 25 past 7", "tokens": [407, 718, 311, 808, 646, 294, 1266, 2077, 412, 3552, 1791, 1614], "temperature": 0.0, "avg_logprob": -0.11819527298212051, "compression_ratio": 1.4382716049382716, "no_speech_prob": 1.5688263374613598e-05}, {"id": 775, "seek": 360048, "start": 3600.48, "end": 3617.48, "text": " Okay hope you had a good rest have a good break I should say", "tokens": [1033, 1454, 291, 632, 257, 665, 1472, 362, 257, 665, 1821, 286, 820, 584], "temperature": 0.0, "avg_logprob": -0.1362458116867963, "compression_ratio": 1.3425925925925926, "no_speech_prob": 6.400744314305484e-05}, {"id": 776, "seek": 360048, "start": 3617.48, "end": 3624.48, "text": " So I want to now show you a really really important mathematical computational trick", "tokens": [407, 286, 528, 281, 586, 855, 291, 257, 534, 534, 1021, 18894, 28270, 4282], "temperature": 0.0, "avg_logprob": -0.1362458116867963, "compression_ratio": 1.3425925925925926, "no_speech_prob": 6.400744314305484e-05}, {"id": 777, "seek": 362448, "start": 3624.48, "end": 3630.48, "text": " Which is we want to do a whole bunch of relu's", "tokens": [3013, 307, 321, 528, 281, 360, 257, 1379, 3840, 295, 1039, 84, 311], "temperature": 0.0, "avg_logprob": -0.1316741240651984, "compression_ratio": 1.8121827411167513, "no_speech_prob": 6.70865920255892e-05}, {"id": 778, "seek": 362448, "start": 3630.48, "end": 3639.48, "text": " Alright so we're going to be wanting to do a whole lot of mx plus b's", "tokens": [2798, 370, 321, 434, 516, 281, 312, 7935, 281, 360, 257, 1379, 688, 295, 275, 87, 1804, 272, 311], "temperature": 0.0, "avg_logprob": -0.1316741240651984, "compression_ratio": 1.8121827411167513, "no_speech_prob": 6.70865920255892e-05}, {"id": 779, "seek": 362448, "start": 3639.48, "end": 3645.48, "text": " And we don't just want to do mx plus b we're going to want to have like lots of variables", "tokens": [400, 321, 500, 380, 445, 528, 281, 360, 275, 87, 1804, 272, 321, 434, 516, 281, 528, 281, 362, 411, 3195, 295, 9102], "temperature": 0.0, "avg_logprob": -0.1316741240651984, "compression_ratio": 1.8121827411167513, "no_speech_prob": 6.70865920255892e-05}, {"id": 780, "seek": 362448, "start": 3645.48, "end": 3649.48, "text": " So for example every single pixel of an image would be a separate variable", "tokens": [407, 337, 1365, 633, 2167, 19261, 295, 364, 3256, 576, 312, 257, 4994, 7006], "temperature": 0.0, "avg_logprob": -0.1316741240651984, "compression_ratio": 1.8121827411167513, "no_speech_prob": 6.70865920255892e-05}, {"id": 781, "seek": 362448, "start": 3649.48, "end": 3653.48, "text": " So we're going to multiply every single one of those times some coefficient", "tokens": [407, 321, 434, 516, 281, 12972, 633, 2167, 472, 295, 729, 1413, 512, 17619], "temperature": 0.0, "avg_logprob": -0.1316741240651984, "compression_ratio": 1.8121827411167513, "no_speech_prob": 6.70865920255892e-05}, {"id": 782, "seek": 365348, "start": 3653.48, "end": 3661.48, "text": " And then add them all together and then do the crop the relu", "tokens": [400, 550, 909, 552, 439, 1214, 293, 550, 360, 264, 9086, 264, 1039, 84], "temperature": 0.0, "avg_logprob": -0.10780325995551215, "compression_ratio": 1.8115942028985508, "no_speech_prob": 1.3630988178192638e-05}, {"id": 783, "seek": 365348, "start": 3661.48, "end": 3665.48, "text": " And then we're going to do it a second time with a second bunch of parameters", "tokens": [400, 550, 321, 434, 516, 281, 360, 309, 257, 1150, 565, 365, 257, 1150, 3840, 295, 9834], "temperature": 0.0, "avg_logprob": -0.10780325995551215, "compression_ratio": 1.8115942028985508, "no_speech_prob": 1.3630988178192638e-05}, {"id": 784, "seek": 365348, "start": 3665.48, "end": 3668.48, "text": " And then a third time and a fourth time and a fifth time", "tokens": [400, 550, 257, 2636, 565, 293, 257, 6409, 565, 293, 257, 9266, 565], "temperature": 0.0, "avg_logprob": -0.10780325995551215, "compression_ratio": 1.8115942028985508, "no_speech_prob": 1.3630988178192638e-05}, {"id": 785, "seek": 365348, "start": 3668.48, "end": 3673.48, "text": " It's going to be pretty inconvenient to write out a hundred million relu's", "tokens": [467, 311, 516, 281, 312, 1238, 46196, 281, 2464, 484, 257, 3262, 2459, 1039, 84, 311], "temperature": 0.0, "avg_logprob": -0.10780325995551215, "compression_ratio": 1.8115942028985508, "no_speech_prob": 1.3630988178192638e-05}, {"id": 786, "seek": 365348, "start": 3673.48, "end": 3680.48, "text": " But so happens there's a mathematical single mathematical operation that does all of those things for us", "tokens": [583, 370, 2314, 456, 311, 257, 18894, 2167, 18894, 6916, 300, 775, 439, 295, 729, 721, 337, 505], "temperature": 0.0, "avg_logprob": -0.10780325995551215, "compression_ratio": 1.8115942028985508, "no_speech_prob": 1.3630988178192638e-05}, {"id": 787, "seek": 368048, "start": 3680.48, "end": 3685.48, "text": " Except for the final replace negatives with zeros and it's called matrix multiplication", "tokens": [16192, 337, 264, 2572, 7406, 40019, 365, 35193, 293, 309, 311, 1219, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.07190023597918059, "compression_ratio": 1.6371681415929205, "no_speech_prob": 1.9525428797351196e-05}, {"id": 788, "seek": 368048, "start": 3685.48, "end": 3690.48, "text": " I expect everybody at some point did matrix multiplication at high school", "tokens": [286, 2066, 2201, 412, 512, 935, 630, 8141, 27290, 412, 1090, 1395], "temperature": 0.0, "avg_logprob": -0.07190023597918059, "compression_ratio": 1.6371681415929205, "no_speech_prob": 1.9525428797351196e-05}, {"id": 789, "seek": 368048, "start": 3690.48, "end": 3693.48, "text": " I suspect also a lot of you have forgotten how it works", "tokens": [286, 9091, 611, 257, 688, 295, 291, 362, 11832, 577, 309, 1985], "temperature": 0.0, "avg_logprob": -0.07190023597918059, "compression_ratio": 1.6371681415929205, "no_speech_prob": 1.9525428797351196e-05}, {"id": 790, "seek": 368048, "start": 3693.48, "end": 3699.48, "text": " When people talk about linear algebra in deep learning", "tokens": [1133, 561, 751, 466, 8213, 21989, 294, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.07190023597918059, "compression_ratio": 1.6371681415929205, "no_speech_prob": 1.9525428797351196e-05}, {"id": 791, "seek": 368048, "start": 3699.48, "end": 3705.48, "text": " They give the impression you need years of graduate school study to learn all this linear algebra", "tokens": [814, 976, 264, 9995, 291, 643, 924, 295, 8080, 1395, 2979, 281, 1466, 439, 341, 8213, 21989], "temperature": 0.0, "avg_logprob": -0.07190023597918059, "compression_ratio": 1.6371681415929205, "no_speech_prob": 1.9525428797351196e-05}, {"id": 792, "seek": 370548, "start": 3705.48, "end": 3710.48, "text": " You don't actually all you need almost all the time is matrix multiplication", "tokens": [509, 500, 380, 767, 439, 291, 643, 1920, 439, 264, 565, 307, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.08906352001687755, "compression_ratio": 1.6681818181818182, "no_speech_prob": 1.2029368917865213e-05}, {"id": 793, "seek": 370548, "start": 3710.48, "end": 3713.48, "text": " And it couldn't be simpler. I'm going to show you a couple of different ways", "tokens": [400, 309, 2809, 380, 312, 18587, 13, 286, 478, 516, 281, 855, 291, 257, 1916, 295, 819, 2098], "temperature": 0.0, "avg_logprob": -0.08906352001687755, "compression_ratio": 1.6681818181818182, "no_speech_prob": 1.2029368917865213e-05}, {"id": 794, "seek": 370548, "start": 3713.48, "end": 3717.48, "text": " The first is there's a really cool site called matrix multiplication xyz", "tokens": [440, 700, 307, 456, 311, 257, 534, 1627, 3621, 1219, 8141, 27290, 2031, 37433], "temperature": 0.0, "avg_logprob": -0.08906352001687755, "compression_ratio": 1.6681818181818182, "no_speech_prob": 1.2029368917865213e-05}, {"id": 795, "seek": 370548, "start": 3717.48, "end": 3721.48, "text": " You can put in any matrix you want", "tokens": [509, 393, 829, 294, 604, 8141, 291, 528], "temperature": 0.0, "avg_logprob": -0.08906352001687755, "compression_ratio": 1.6681818181818182, "no_speech_prob": 1.2029368917865213e-05}, {"id": 796, "seek": 370548, "start": 3721.48, "end": 3727.48, "text": " So I'm going to put in this one", "tokens": [407, 286, 478, 516, 281, 829, 294, 341, 472], "temperature": 0.0, "avg_logprob": -0.08906352001687755, "compression_ratio": 1.6681818181818182, "no_speech_prob": 1.2029368917865213e-05}, {"id": 797, "seek": 370548, "start": 3727.48, "end": 3734.48, "text": " So this matrix is saying I've got three rows of data with three variables", "tokens": [407, 341, 8141, 307, 1566, 286, 600, 658, 1045, 13241, 295, 1412, 365, 1045, 9102], "temperature": 0.0, "avg_logprob": -0.08906352001687755, "compression_ratio": 1.6681818181818182, "no_speech_prob": 1.2029368917865213e-05}, {"id": 798, "seek": 373448, "start": 3734.48, "end": 3738.48, "text": " So maybe they're tiny images with three pixels", "tokens": [407, 1310, 436, 434, 5870, 5267, 365, 1045, 18668], "temperature": 0.0, "avg_logprob": -0.11207486932927913, "compression_ratio": 1.7079207920792079, "no_speech_prob": 6.604233203688636e-05}, {"id": 799, "seek": 373448, "start": 3738.48, "end": 3741.48, "text": " And the value of the first one is 1 2 1", "tokens": [400, 264, 2158, 295, 264, 700, 472, 307, 502, 568, 502], "temperature": 0.0, "avg_logprob": -0.11207486932927913, "compression_ratio": 1.7079207920792079, "no_speech_prob": 6.604233203688636e-05}, {"id": 800, "seek": 373448, "start": 3741.48, "end": 3745.48, "text": " The second is 0 1 1 and the third is 2 3 1", "tokens": [440, 1150, 307, 1958, 502, 502, 293, 264, 2636, 307, 568, 805, 502], "temperature": 0.0, "avg_logprob": -0.11207486932927913, "compression_ratio": 1.7079207920792079, "no_speech_prob": 6.604233203688636e-05}, {"id": 801, "seek": 373448, "start": 3745.48, "end": 3748.48, "text": " So those are our three rows of data", "tokens": [407, 729, 366, 527, 1045, 13241, 295, 1412], "temperature": 0.0, "avg_logprob": -0.11207486932927913, "compression_ratio": 1.7079207920792079, "no_speech_prob": 6.604233203688636e-05}, {"id": 802, "seek": 373448, "start": 3748.48, "end": 3750.48, "text": " These are our three sets of coefficients", "tokens": [1981, 366, 527, 1045, 6352, 295, 31994], "temperature": 0.0, "avg_logprob": -0.11207486932927913, "compression_ratio": 1.7079207920792079, "no_speech_prob": 6.604233203688636e-05}, {"id": 803, "seek": 373448, "start": 3750.48, "end": 3753.48, "text": " So we've got a b and c in our data", "tokens": [407, 321, 600, 658, 257, 272, 293, 269, 294, 527, 1412], "temperature": 0.0, "avg_logprob": -0.11207486932927913, "compression_ratio": 1.7079207920792079, "no_speech_prob": 6.604233203688636e-05}, {"id": 804, "seek": 373448, "start": 3753.48, "end": 3756.48, "text": " So I guess you'd call it x1 x2 and x3", "tokens": [407, 286, 2041, 291, 1116, 818, 309, 2031, 16, 2031, 17, 293, 2031, 18], "temperature": 0.0, "avg_logprob": -0.11207486932927913, "compression_ratio": 1.7079207920792079, "no_speech_prob": 6.604233203688636e-05}, {"id": 805, "seek": 373448, "start": 3756.48, "end": 3759.48, "text": " And then here's our first set of coefficients a b and c", "tokens": [400, 550, 510, 311, 527, 700, 992, 295, 31994, 257, 272, 293, 269], "temperature": 0.0, "avg_logprob": -0.11207486932927913, "compression_ratio": 1.7079207920792079, "no_speech_prob": 6.604233203688636e-05}, {"id": 806, "seek": 373448, "start": 3759.48, "end": 3762.48, "text": " 2 6 and 1", "tokens": [568, 1386, 293, 502], "temperature": 0.0, "avg_logprob": -0.11207486932927913, "compression_ratio": 1.7079207920792079, "no_speech_prob": 6.604233203688636e-05}, {"id": 807, "seek": 376248, "start": 3762.48, "end": 3765.48, "text": " And then our second set is 5 7 and 8", "tokens": [400, 550, 527, 1150, 992, 307, 1025, 1614, 293, 1649], "temperature": 0.0, "avg_logprob": -0.10796543121337891, "compression_ratio": 1.7150537634408602, "no_speech_prob": 1.8342376279179007e-05}, {"id": 808, "seek": 376248, "start": 3765.48, "end": 3768.48, "text": " So here's what happens when we do matrix multiplication", "tokens": [407, 510, 311, 437, 2314, 562, 321, 360, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.10796543121337891, "compression_ratio": 1.7150537634408602, "no_speech_prob": 1.8342376279179007e-05}, {"id": 809, "seek": 376248, "start": 3768.48, "end": 3776.48, "text": " That second this matrix here of coefficients gets flipped around", "tokens": [663, 1150, 341, 8141, 510, 295, 31994, 2170, 26273, 926], "temperature": 0.0, "avg_logprob": -0.10796543121337891, "compression_ratio": 1.7150537634408602, "no_speech_prob": 1.8342376279179007e-05}, {"id": 810, "seek": 376248, "start": 3776.48, "end": 3782.48, "text": " And we do this is the multiplications and additions that I mentioned right", "tokens": [400, 321, 360, 341, 307, 264, 17596, 763, 293, 35113, 300, 286, 2835, 558], "temperature": 0.0, "avg_logprob": -0.10796543121337891, "compression_ratio": 1.7150537634408602, "no_speech_prob": 1.8342376279179007e-05}, {"id": 811, "seek": 376248, "start": 3782.48, "end": 3785.48, "text": " So multiply add multiply add multiply add", "tokens": [407, 12972, 909, 12972, 909, 12972, 909], "temperature": 0.0, "avg_logprob": -0.10796543121337891, "compression_ratio": 1.7150537634408602, "no_speech_prob": 1.8342376279179007e-05}, {"id": 812, "seek": 376248, "start": 3785.48, "end": 3789.48, "text": " So that's going to give you the first number", "tokens": [407, 300, 311, 516, 281, 976, 291, 264, 700, 1230], "temperature": 0.0, "avg_logprob": -0.10796543121337891, "compression_ratio": 1.7150537634408602, "no_speech_prob": 1.8342376279179007e-05}, {"id": 813, "seek": 378948, "start": 3789.48, "end": 3796.48, "text": " Because that is the left hand column of the second matrix times the first row", "tokens": [1436, 300, 307, 264, 1411, 1011, 7738, 295, 264, 1150, 8141, 1413, 264, 700, 5386], "temperature": 0.0, "avg_logprob": -0.06671607073615579, "compression_ratio": 1.834319526627219, "no_speech_prob": 7.88918259786442e-06}, {"id": 814, "seek": 378948, "start": 3796.48, "end": 3801.48, "text": " So that gives you the top left result", "tokens": [407, 300, 2709, 291, 264, 1192, 1411, 1874], "temperature": 0.0, "avg_logprob": -0.06671607073615579, "compression_ratio": 1.834319526627219, "no_speech_prob": 7.88918259786442e-06}, {"id": 815, "seek": 378948, "start": 3801.48, "end": 3804.48, "text": " So the next one is going to give us two results right", "tokens": [407, 264, 958, 472, 307, 516, 281, 976, 505, 732, 3542, 558], "temperature": 0.0, "avg_logprob": -0.06671607073615579, "compression_ratio": 1.834319526627219, "no_speech_prob": 7.88918259786442e-06}, {"id": 816, "seek": 378948, "start": 3804.48, "end": 3807.48, "text": " So we've got now the right hand one with the top row", "tokens": [407, 321, 600, 658, 586, 264, 558, 1011, 472, 365, 264, 1192, 5386], "temperature": 0.0, "avg_logprob": -0.06671607073615579, "compression_ratio": 1.834319526627219, "no_speech_prob": 7.88918259786442e-06}, {"id": 817, "seek": 378948, "start": 3807.48, "end": 3810.48, "text": " And the left hand one with the second row", "tokens": [400, 264, 1411, 1011, 472, 365, 264, 1150, 5386], "temperature": 0.0, "avg_logprob": -0.06671607073615579, "compression_ratio": 1.834319526627219, "no_speech_prob": 7.88918259786442e-06}, {"id": 818, "seek": 378948, "start": 3810.48, "end": 3812.48, "text": " Keep going down", "tokens": [5527, 516, 760], "temperature": 0.0, "avg_logprob": -0.06671607073615579, "compression_ratio": 1.834319526627219, "no_speech_prob": 7.88918259786442e-06}, {"id": 819, "seek": 378948, "start": 3812.48, "end": 3815.48, "text": " Keep going down", "tokens": [5527, 516, 760], "temperature": 0.0, "avg_logprob": -0.06671607073615579, "compression_ratio": 1.834319526627219, "no_speech_prob": 7.88918259786442e-06}, {"id": 820, "seek": 378948, "start": 3815.48, "end": 3817.48, "text": " And that's it", "tokens": [400, 300, 311, 309], "temperature": 0.0, "avg_logprob": -0.06671607073615579, "compression_ratio": 1.834319526627219, "no_speech_prob": 7.88918259786442e-06}, {"id": 821, "seek": 381748, "start": 3817.48, "end": 3819.48, "text": " That's what matrix multiplication is", "tokens": [663, 311, 437, 8141, 27290, 307], "temperature": 0.0, "avg_logprob": -0.11028382944506268, "compression_ratio": 1.6607142857142858, "no_speech_prob": 1.260644057765603e-05}, {"id": 822, "seek": 381748, "start": 3819.48, "end": 3822.48, "text": " It's modifying things together and adding them up", "tokens": [467, 311, 42626, 721, 1214, 293, 5127, 552, 493], "temperature": 0.0, "avg_logprob": -0.11028382944506268, "compression_ratio": 1.6607142857142858, "no_speech_prob": 1.260644057765603e-05}, {"id": 823, "seek": 381748, "start": 3822.48, "end": 3825.48, "text": " So there'd be one more step to do to make this a layer of a neural network", "tokens": [407, 456, 1116, 312, 472, 544, 1823, 281, 360, 281, 652, 341, 257, 4583, 295, 257, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.11028382944506268, "compression_ratio": 1.6607142857142858, "no_speech_prob": 1.260644057765603e-05}, {"id": 824, "seek": 381748, "start": 3825.48, "end": 3829.48, "text": " Which is if this had any negatives we would replace them with zeros", "tokens": [3013, 307, 498, 341, 632, 604, 40019, 321, 576, 7406, 552, 365, 35193], "temperature": 0.0, "avg_logprob": -0.11028382944506268, "compression_ratio": 1.6607142857142858, "no_speech_prob": 1.260644057765603e-05}, {"id": 825, "seek": 381748, "start": 3829.48, "end": 3838.48, "text": " That's why matrix multiplication is the critical foundational mathematical operation", "tokens": [663, 311, 983, 8141, 27290, 307, 264, 4924, 32195, 18894, 6916], "temperature": 0.0, "avg_logprob": -0.11028382944506268, "compression_ratio": 1.6607142857142858, "no_speech_prob": 1.260644057765603e-05}, {"id": 826, "seek": 381748, "start": 3838.48, "end": 3841.48, "text": " In basically all of deep learning", "tokens": [682, 1936, 439, 295, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.11028382944506268, "compression_ratio": 1.6607142857142858, "no_speech_prob": 1.260644057765603e-05}, {"id": 827, "seek": 381748, "start": 3841.48, "end": 3844.48, "text": " So the GPUs that we use", "tokens": [407, 264, 18407, 82, 300, 321, 764], "temperature": 0.0, "avg_logprob": -0.11028382944506268, "compression_ratio": 1.6607142857142858, "no_speech_prob": 1.260644057765603e-05}, {"id": 828, "seek": 384448, "start": 3844.48, "end": 3849.48, "text": " The thing that they are good at is this matrix multiplication", "tokens": [440, 551, 300, 436, 366, 665, 412, 307, 341, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.08414162712535639, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.889071639510803e-06}, {"id": 829, "seek": 384448, "start": 3849.48, "end": 3852.48, "text": " They have special cores called tensor cores", "tokens": [814, 362, 2121, 24826, 1219, 40863, 24826], "temperature": 0.0, "avg_logprob": -0.08414162712535639, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.889071639510803e-06}, {"id": 830, "seek": 384448, "start": 3852.48, "end": 3855.48, "text": " Which can basically only do one thing", "tokens": [3013, 393, 1936, 787, 360, 472, 551], "temperature": 0.0, "avg_logprob": -0.08414162712535639, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.889071639510803e-06}, {"id": 831, "seek": 384448, "start": 3855.48, "end": 3859.48, "text": " Which is to multiply together two four by four matrices", "tokens": [3013, 307, 281, 12972, 1214, 732, 1451, 538, 1451, 32284], "temperature": 0.0, "avg_logprob": -0.08414162712535639, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.889071639510803e-06}, {"id": 832, "seek": 384448, "start": 3859.48, "end": 3862.48, "text": " And then they do that lots of times to bigger matrices", "tokens": [400, 550, 436, 360, 300, 3195, 295, 1413, 281, 3801, 32284], "temperature": 0.0, "avg_logprob": -0.08414162712535639, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.889071639510803e-06}, {"id": 833, "seek": 384448, "start": 3862.48, "end": 3865.48, "text": " So I'm going to show you an example of this", "tokens": [407, 286, 478, 516, 281, 855, 291, 364, 1365, 295, 341], "temperature": 0.0, "avg_logprob": -0.08414162712535639, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.889071639510803e-06}, {"id": 834, "seek": 384448, "start": 3865.48, "end": 3872.48, "text": " We're actually going to build a complete machine learning model on real data", "tokens": [492, 434, 767, 516, 281, 1322, 257, 3566, 3479, 2539, 2316, 322, 957, 1412], "temperature": 0.0, "avg_logprob": -0.08414162712535639, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.889071639510803e-06}, {"id": 835, "seek": 387248, "start": 3872.48, "end": 3875.48, "text": " In a spreadsheet", "tokens": [682, 257, 27733], "temperature": 0.0, "avg_logprob": -0.14470148086547852, "compression_ratio": 1.5375722543352601, "no_speech_prob": 2.1781126633868553e-05}, {"id": 836, "seek": 387248, "start": 3878.48, "end": 3882.48, "text": " So fast AI has become kind of famous for a number of things", "tokens": [407, 2370, 7318, 575, 1813, 733, 295, 4618, 337, 257, 1230, 295, 721], "temperature": 0.0, "avg_logprob": -0.14470148086547852, "compression_ratio": 1.5375722543352601, "no_speech_prob": 2.1781126633868553e-05}, {"id": 837, "seek": 387248, "start": 3882.48, "end": 3887.48, "text": " And one of them is using spreadsheets to create deep learning models", "tokens": [400, 472, 295, 552, 307, 1228, 23651, 1385, 281, 1884, 2452, 2539, 5245], "temperature": 0.0, "avg_logprob": -0.14470148086547852, "compression_ratio": 1.5375722543352601, "no_speech_prob": 2.1781126633868553e-05}, {"id": 838, "seek": 387248, "start": 3887.48, "end": 3890.48, "text": " We haven't done it for a couple of years so I'm pretty pumped to show this to you", "tokens": [492, 2378, 380, 1096, 309, 337, 257, 1916, 295, 924, 370, 286, 478, 1238, 27774, 281, 855, 341, 281, 291], "temperature": 0.0, "avg_logprob": -0.14470148086547852, "compression_ratio": 1.5375722543352601, "no_speech_prob": 2.1781126633868553e-05}, {"id": 839, "seek": 387248, "start": 3890.48, "end": 3897.48, "text": " What I've done is I went over to Cable", "tokens": [708, 286, 600, 1096, 307, 286, 1437, 670, 281, 383, 712], "temperature": 0.0, "avg_logprob": -0.14470148086547852, "compression_ratio": 1.5375722543352601, "no_speech_prob": 2.1781126633868553e-05}, {"id": 840, "seek": 389748, "start": 3897.48, "end": 3906.48, "text": " Where there's a competition I actually helped create many years ago called Titanic", "tokens": [2305, 456, 311, 257, 6211, 286, 767, 4254, 1884, 867, 924, 2057, 1219, 42183], "temperature": 0.0, "avg_logprob": -0.16483349731003027, "compression_ratio": 1.5087719298245614, "no_speech_prob": 2.5463939891778864e-05}, {"id": 841, "seek": 389748, "start": 3906.48, "end": 3909.48, "text": " And it's like an ongoing competition", "tokens": [400, 309, 311, 411, 364, 10452, 6211], "temperature": 0.0, "avg_logprob": -0.16483349731003027, "compression_ratio": 1.5087719298245614, "no_speech_prob": 2.5463939891778864e-05}, {"id": 842, "seek": 389748, "start": 3909.48, "end": 3912.48, "text": " So 14,000 people have entered it so far", "tokens": [407, 3499, 11, 1360, 561, 362, 9065, 309, 370, 1400], "temperature": 0.0, "avg_logprob": -0.16483349731003027, "compression_ratio": 1.5087719298245614, "no_speech_prob": 2.5463939891778864e-05}, {"id": 843, "seek": 389748, "start": 3912.48, "end": 3915.48, "text": " It's just a competition for a bit of fun", "tokens": [467, 311, 445, 257, 6211, 337, 257, 857, 295, 1019], "temperature": 0.0, "avg_logprob": -0.16483349731003027, "compression_ratio": 1.5087719298245614, "no_speech_prob": 2.5463939891778864e-05}, {"id": 844, "seek": 389748, "start": 3915.48, "end": 3917.48, "text": " There's no end date", "tokens": [821, 311, 572, 917, 4002], "temperature": 0.0, "avg_logprob": -0.16483349731003027, "compression_ratio": 1.5087719298245614, "no_speech_prob": 2.5463939891778864e-05}, {"id": 845, "seek": 389748, "start": 3917.48, "end": 3921.48, "text": " And the data for it is the data about", "tokens": [400, 264, 1412, 337, 309, 307, 264, 1412, 466], "temperature": 0.0, "avg_logprob": -0.16483349731003027, "compression_ratio": 1.5087719298245614, "no_speech_prob": 2.5463939891778864e-05}, {"id": 846, "seek": 392148, "start": 3921.48, "end": 3932.48, "text": " Who survived and who didn't from the real Titanic disaster", "tokens": [2102, 14433, 293, 567, 994, 380, 490, 264, 957, 42183, 11293], "temperature": 0.0, "avg_logprob": -0.10010162353515625, "compression_ratio": 1.416243654822335, "no_speech_prob": 4.264045492163859e-05}, {"id": 847, "seek": 392148, "start": 3932.48, "end": 3935.48, "text": " And so I clicked here on the download button", "tokens": [400, 370, 286, 23370, 510, 322, 264, 5484, 2960], "temperature": 0.0, "avg_logprob": -0.10010162353515625, "compression_ratio": 1.416243654822335, "no_speech_prob": 4.264045492163859e-05}, {"id": 848, "seek": 392148, "start": 3935.48, "end": 3937.48, "text": " To grab it on my computer", "tokens": [1407, 4444, 309, 322, 452, 3820], "temperature": 0.0, "avg_logprob": -0.10010162353515625, "compression_ratio": 1.416243654822335, "no_speech_prob": 4.264045492163859e-05}, {"id": 849, "seek": 392148, "start": 3937.48, "end": 3939.48, "text": " That gave me a CSP", "tokens": [663, 2729, 385, 257, 9460, 47], "temperature": 0.0, "avg_logprob": -0.10010162353515625, "compression_ratio": 1.416243654822335, "no_speech_prob": 4.264045492163859e-05}, {"id": 850, "seek": 392148, "start": 3939.48, "end": 3942.48, "text": " Which I opened up in Excel", "tokens": [3013, 286, 5625, 493, 294, 19060], "temperature": 0.0, "avg_logprob": -0.10010162353515625, "compression_ratio": 1.416243654822335, "no_speech_prob": 4.264045492163859e-05}, {"id": 851, "seek": 392148, "start": 3942.48, "end": 3947.48, "text": " The first thing I did then was I just removed a few columns that", "tokens": [440, 700, 551, 286, 630, 550, 390, 286, 445, 7261, 257, 1326, 13766, 300], "temperature": 0.0, "avg_logprob": -0.10010162353515625, "compression_ratio": 1.416243654822335, "no_speech_prob": 4.264045492163859e-05}, {"id": 852, "seek": 392148, "start": 3947.48, "end": 3949.48, "text": " Clearly were not going to be important", "tokens": [24120, 645, 406, 516, 281, 312, 1021], "temperature": 0.0, "avg_logprob": -0.10010162353515625, "compression_ratio": 1.416243654822335, "no_speech_prob": 4.264045492163859e-05}, {"id": 853, "seek": 394948, "start": 3949.48, "end": 3953.48, "text": " The name of the passengers, the passenger ID", "tokens": [440, 1315, 295, 264, 18436, 11, 264, 18707, 7348], "temperature": 0.0, "avg_logprob": -0.10956528809693483, "compression_ratio": 1.6547619047619047, "no_speech_prob": 3.8824317016405985e-05}, {"id": 854, "seek": 394948, "start": 3953.48, "end": 3955.48, "text": " Just to try to make it a bit simpler", "tokens": [1449, 281, 853, 281, 652, 309, 257, 857, 18587], "temperature": 0.0, "avg_logprob": -0.10956528809693483, "compression_ratio": 1.6547619047619047, "no_speech_prob": 3.8824317016405985e-05}, {"id": 855, "seek": 394948, "start": 3955.48, "end": 3957.48, "text": " And so I've ended up with", "tokens": [400, 370, 286, 600, 4590, 493, 365], "temperature": 0.0, "avg_logprob": -0.10956528809693483, "compression_ratio": 1.6547619047619047, "no_speech_prob": 3.8824317016405985e-05}, {"id": 856, "seek": 394948, "start": 3957.48, "end": 3960.48, "text": " Each row of this is one passenger", "tokens": [6947, 5386, 295, 341, 307, 472, 18707], "temperature": 0.0, "avg_logprob": -0.10956528809693483, "compression_ratio": 1.6547619047619047, "no_speech_prob": 3.8824317016405985e-05}, {"id": 857, "seek": 394948, "start": 3960.48, "end": 3962.48, "text": " The first column is the dependent variable", "tokens": [440, 700, 7738, 307, 264, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.10956528809693483, "compression_ratio": 1.6547619047619047, "no_speech_prob": 3.8824317016405985e-05}, {"id": 858, "seek": 394948, "start": 3962.48, "end": 3965.48, "text": " The dependent variable is the thing we're trying to predict", "tokens": [440, 12334, 7006, 307, 264, 551, 321, 434, 1382, 281, 6069], "temperature": 0.0, "avg_logprob": -0.10956528809693483, "compression_ratio": 1.6547619047619047, "no_speech_prob": 3.8824317016405985e-05}, {"id": 859, "seek": 394948, "start": 3965.48, "end": 3967.48, "text": " Did they survive?", "tokens": [2589, 436, 7867, 30], "temperature": 0.0, "avg_logprob": -0.10956528809693483, "compression_ratio": 1.6547619047619047, "no_speech_prob": 3.8824317016405985e-05}, {"id": 860, "seek": 394948, "start": 3967.48, "end": 3970.48, "text": " And the remaining are some information such as", "tokens": [400, 264, 8877, 366, 512, 1589, 1270, 382], "temperature": 0.0, "avg_logprob": -0.10956528809693483, "compression_ratio": 1.6547619047619047, "no_speech_prob": 3.8824317016405985e-05}, {"id": 861, "seek": 394948, "start": 3970.48, "end": 3973.48, "text": " What class of the boat? First, second or third class?", "tokens": [708, 1508, 295, 264, 6582, 30, 2386, 11, 1150, 420, 2636, 1508, 30], "temperature": 0.0, "avg_logprob": -0.10956528809693483, "compression_ratio": 1.6547619047619047, "no_speech_prob": 3.8824317016405985e-05}, {"id": 862, "seek": 394948, "start": 3973.48, "end": 3975.48, "text": " Their sex, their age", "tokens": [6710, 3260, 11, 641, 3205], "temperature": 0.0, "avg_logprob": -0.10956528809693483, "compression_ratio": 1.6547619047619047, "no_speech_prob": 3.8824317016405985e-05}, {"id": 863, "seek": 394948, "start": 3975.48, "end": 3977.48, "text": " How many siblings in the family?", "tokens": [1012, 867, 20571, 294, 264, 1605, 30], "temperature": 0.0, "avg_logprob": -0.10956528809693483, "compression_ratio": 1.6547619047619047, "no_speech_prob": 3.8824317016405985e-05}, {"id": 864, "seek": 397748, "start": 3977.48, "end": 3983.48, "text": " P. Arch, I think his parents or something?", "tokens": [430, 13, 10984, 11, 286, 519, 702, 3152, 420, 746, 30], "temperature": 0.0, "avg_logprob": -0.19185095657537013, "compression_ratio": 1.4381443298969072, "no_speech_prob": 0.00014192979142535478}, {"id": 865, "seek": 397748, "start": 3983.48, "end": 3985.48, "text": " So you should always look for a data dictionary", "tokens": [407, 291, 820, 1009, 574, 337, 257, 1412, 25890], "temperature": 0.0, "avg_logprob": -0.19185095657537013, "compression_ratio": 1.4381443298969072, "no_speech_prob": 0.00014192979142535478}, {"id": 866, "seek": 397748, "start": 3985.48, "end": 3987.48, "text": " To find out what's what", "tokens": [1407, 915, 484, 437, 311, 437], "temperature": 0.0, "avg_logprob": -0.19185095657537013, "compression_ratio": 1.4381443298969072, "no_speech_prob": 0.00014192979142535478}, {"id": 867, "seek": 397748, "start": 3987.48, "end": 3989.48, "text": " Number of parents and children", "tokens": [5118, 295, 3152, 293, 2227], "temperature": 0.0, "avg_logprob": -0.19185095657537013, "compression_ratio": 1.4381443298969072, "no_speech_prob": 0.00014192979142535478}, {"id": 868, "seek": 397748, "start": 3989.48, "end": 3992.48, "text": " What was their fare?", "tokens": [708, 390, 641, 11994, 30], "temperature": 0.0, "avg_logprob": -0.19185095657537013, "compression_ratio": 1.4381443298969072, "no_speech_prob": 0.00014192979142535478}, {"id": 869, "seek": 397748, "start": 3992.48, "end": 3995.48, "text": " And which of the three cities did they embark on?", "tokens": [400, 597, 295, 264, 1045, 6486, 630, 436, 29832, 322, 30], "temperature": 0.0, "avg_logprob": -0.19185095657537013, "compression_ratio": 1.4381443298969072, "no_speech_prob": 0.00014192979142535478}, {"id": 870, "seek": 397748, "start": 3995.48, "end": 3999.48, "text": " So there's our data", "tokens": [407, 456, 311, 527, 1412], "temperature": 0.0, "avg_logprob": -0.19185095657537013, "compression_ratio": 1.4381443298969072, "no_speech_prob": 0.00014192979142535478}, {"id": 871, "seek": 397748, "start": 3999.48, "end": 4002.48, "text": " Now when I first grabbed it", "tokens": [823, 562, 286, 700, 18607, 309], "temperature": 0.0, "avg_logprob": -0.19185095657537013, "compression_ratio": 1.4381443298969072, "no_speech_prob": 0.00014192979142535478}, {"id": 872, "seek": 397748, "start": 4002.48, "end": 4005.48, "text": " I noticed that", "tokens": [286, 5694, 300], "temperature": 0.0, "avg_logprob": -0.19185095657537013, "compression_ratio": 1.4381443298969072, "no_speech_prob": 0.00014192979142535478}, {"id": 873, "seek": 400548, "start": 4005.48, "end": 4007.48, "text": " There were some people with no age", "tokens": [821, 645, 512, 561, 365, 572, 3205], "temperature": 0.0, "avg_logprob": -0.1076097809866573, "compression_ratio": 1.6446700507614214, "no_speech_prob": 0.00011589688074309379}, {"id": 874, "seek": 400548, "start": 4007.48, "end": 4011.48, "text": " Now there's all kinds of things we could do for that", "tokens": [823, 456, 311, 439, 3685, 295, 721, 321, 727, 360, 337, 300], "temperature": 0.0, "avg_logprob": -0.1076097809866573, "compression_ratio": 1.6446700507614214, "no_speech_prob": 0.00011589688074309379}, {"id": 875, "seek": 400548, "start": 4011.48, "end": 4016.48, "text": " But for this purpose I just decided to remove them", "tokens": [583, 337, 341, 4334, 286, 445, 3047, 281, 4159, 552], "temperature": 0.0, "avg_logprob": -0.1076097809866573, "compression_ratio": 1.6446700507614214, "no_speech_prob": 0.00011589688074309379}, {"id": 876, "seek": 400548, "start": 4016.48, "end": 4019.48, "text": " And I found the same thing for embarked", "tokens": [400, 286, 1352, 264, 912, 551, 337, 29832, 292], "temperature": 0.0, "avg_logprob": -0.1076097809866573, "compression_ratio": 1.6446700507614214, "no_speech_prob": 0.00011589688074309379}, {"id": 877, "seek": 400548, "start": 4019.48, "end": 4022.48, "text": " I removed the blanks as well", "tokens": [286, 7261, 264, 8247, 82, 382, 731], "temperature": 0.0, "avg_logprob": -0.1076097809866573, "compression_ratio": 1.6446700507614214, "no_speech_prob": 0.00011589688074309379}, {"id": 878, "seek": 400548, "start": 4022.48, "end": 4025.48, "text": " But that left me with nearly all of the data", "tokens": [583, 300, 1411, 385, 365, 6217, 439, 295, 264, 1412], "temperature": 0.0, "avg_logprob": -0.1076097809866573, "compression_ratio": 1.6446700507614214, "no_speech_prob": 0.00011589688074309379}, {"id": 879, "seek": 400548, "start": 4025.48, "end": 4029.48, "text": " So then I've put that over here", "tokens": [407, 550, 286, 600, 829, 300, 670, 510], "temperature": 0.0, "avg_logprob": -0.1076097809866573, "compression_ratio": 1.6446700507614214, "no_speech_prob": 0.00011589688074309379}, {"id": 880, "seek": 400548, "start": 4029.48, "end": 4034.48, "text": " Here's our data with those rows removed", "tokens": [1692, 311, 527, 1412, 365, 729, 13241, 7261], "temperature": 0.0, "avg_logprob": -0.1076097809866573, "compression_ratio": 1.6446700507614214, "no_speech_prob": 0.00011589688074309379}, {"id": 881, "seek": 403448, "start": 4034.48, "end": 4036.48, "text": " And", "tokens": [400], "temperature": 0.0, "avg_logprob": -0.14372549337499282, "compression_ratio": 1.6513157894736843, "no_speech_prob": 3.821049540420063e-05}, {"id": 882, "seek": 403448, "start": 4040.48, "end": 4045.48, "text": " Okay, so these are the columns that came directly from Kaggle", "tokens": [1033, 11, 370, 613, 366, 264, 13766, 300, 1361, 3838, 490, 48751, 22631], "temperature": 0.0, "avg_logprob": -0.14372549337499282, "compression_ratio": 1.6513157894736843, "no_speech_prob": 3.821049540420063e-05}, {"id": 883, "seek": 403448, "start": 4045.48, "end": 4048.48, "text": " So basically what we now want to do", "tokens": [407, 1936, 437, 321, 586, 528, 281, 360], "temperature": 0.0, "avg_logprob": -0.14372549337499282, "compression_ratio": 1.6513157894736843, "no_speech_prob": 3.821049540420063e-05}, {"id": 884, "seek": 403448, "start": 4048.48, "end": 4051.48, "text": " Is we want to multiply each of these by a coefficient", "tokens": [1119, 321, 528, 281, 12972, 1184, 295, 613, 538, 257, 17619], "temperature": 0.0, "avg_logprob": -0.14372549337499282, "compression_ratio": 1.6513157894736843, "no_speech_prob": 3.821049540420063e-05}, {"id": 885, "seek": 403448, "start": 4051.48, "end": 4056.48, "text": " How do you multiply the word male by a coefficient?", "tokens": [1012, 360, 291, 12972, 264, 1349, 7133, 538, 257, 17619, 30], "temperature": 0.0, "avg_logprob": -0.14372549337499282, "compression_ratio": 1.6513157894736843, "no_speech_prob": 3.821049540420063e-05}, {"id": 886, "seek": 403448, "start": 4056.48, "end": 4061.48, "text": " And how do you multiply S by a coefficient?", "tokens": [400, 577, 360, 291, 12972, 318, 538, 257, 17619, 30], "temperature": 0.0, "avg_logprob": -0.14372549337499282, "compression_ratio": 1.6513157894736843, "no_speech_prob": 3.821049540420063e-05}, {"id": 887, "seek": 406148, "start": 4061.48, "end": 4064.48, "text": " You can't. So I converted all of these to numbers", "tokens": [509, 393, 380, 13, 407, 286, 16424, 439, 295, 613, 281, 3547], "temperature": 0.0, "avg_logprob": -0.13629273608722517, "compression_ratio": 1.662551440329218, "no_speech_prob": 4.068350972374901e-05}, {"id": 888, "seek": 406148, "start": 4064.48, "end": 4066.48, "text": " Male and female are very easy", "tokens": [21080, 293, 6556, 366, 588, 1858], "temperature": 0.0, "avg_logprob": -0.13629273608722517, "compression_ratio": 1.662551440329218, "no_speech_prob": 4.068350972374901e-05}, {"id": 889, "seek": 406148, "start": 4066.48, "end": 4069.48, "text": " I created a column called is male", "tokens": [286, 2942, 257, 7738, 1219, 307, 7133], "temperature": 0.0, "avg_logprob": -0.13629273608722517, "compression_ratio": 1.662551440329218, "no_speech_prob": 4.068350972374901e-05}, {"id": 890, "seek": 406148, "start": 4069.48, "end": 4071.48, "text": " And as you can see there's just an if statement", "tokens": [400, 382, 291, 393, 536, 456, 311, 445, 364, 498, 5629], "temperature": 0.0, "avg_logprob": -0.13629273608722517, "compression_ratio": 1.662551440329218, "no_speech_prob": 4.068350972374901e-05}, {"id": 891, "seek": 406148, "start": 4071.48, "end": 4075.48, "text": " That says if sex is male then it's one, otherwise it's zero", "tokens": [663, 1619, 498, 3260, 307, 7133, 550, 309, 311, 472, 11, 5911, 309, 311, 4018], "temperature": 0.0, "avg_logprob": -0.13629273608722517, "compression_ratio": 1.662551440329218, "no_speech_prob": 4.068350972374901e-05}, {"id": 892, "seek": 406148, "start": 4075.48, "end": 4077.48, "text": " And we can do something very similar for embarked", "tokens": [400, 321, 393, 360, 746, 588, 2531, 337, 29832, 292], "temperature": 0.0, "avg_logprob": -0.13629273608722517, "compression_ratio": 1.662551440329218, "no_speech_prob": 4.068350972374901e-05}, {"id": 893, "seek": 406148, "start": 4077.48, "end": 4082.48, "text": " We can have one column called did they embark in Southampton?", "tokens": [492, 393, 362, 472, 7738, 1219, 630, 436, 29832, 294, 4242, 335, 21987, 30], "temperature": 0.0, "avg_logprob": -0.13629273608722517, "compression_ratio": 1.662551440329218, "no_speech_prob": 4.068350972374901e-05}, {"id": 894, "seek": 406148, "start": 4082.48, "end": 4083.48, "text": " Same deal", "tokens": [10635, 2028], "temperature": 0.0, "avg_logprob": -0.13629273608722517, "compression_ratio": 1.662551440329218, "no_speech_prob": 4.068350972374901e-05}, {"id": 895, "seek": 406148, "start": 4083.48, "end": 4088.48, "text": " And another column for did they, what's it called, Churberg?", "tokens": [400, 1071, 7738, 337, 630, 436, 11, 437, 311, 309, 1219, 11, 761, 374, 6873, 30], "temperature": 0.0, "avg_logprob": -0.13629273608722517, "compression_ratio": 1.662551440329218, "no_speech_prob": 4.068350972374901e-05}, {"id": 896, "seek": 408848, "start": 4088.48, "end": 4091.48, "text": " Did they embark in Churberg?", "tokens": [2589, 436, 29832, 294, 761, 374, 6873, 30], "temperature": 0.0, "avg_logprob": -0.09776624368161571, "compression_ratio": 1.6228070175438596, "no_speech_prob": 9.223113011103123e-06}, {"id": 897, "seek": 408848, "start": 4091.48, "end": 4096.48, "text": " And now P class is one, two or three, which is a number", "tokens": [400, 586, 430, 1508, 307, 472, 11, 732, 420, 1045, 11, 597, 307, 257, 1230], "temperature": 0.0, "avg_logprob": -0.09776624368161571, "compression_ratio": 1.6228070175438596, "no_speech_prob": 9.223113011103123e-06}, {"id": 898, "seek": 408848, "start": 4096.48, "end": 4100.48, "text": " But it's not really a continuous measurement of something", "tokens": [583, 309, 311, 406, 534, 257, 10957, 13160, 295, 746], "temperature": 0.0, "avg_logprob": -0.09776624368161571, "compression_ratio": 1.6228070175438596, "no_speech_prob": 9.223113011103123e-06}, {"id": 899, "seek": 408848, "start": 4100.48, "end": 4103.48, "text": " There isn't one or two or three things", "tokens": [821, 1943, 380, 472, 420, 732, 420, 1045, 721], "temperature": 0.0, "avg_logprob": -0.09776624368161571, "compression_ratio": 1.6228070175438596, "no_speech_prob": 9.223113011103123e-06}, {"id": 900, "seek": 408848, "start": 4103.48, "end": 4105.48, "text": " They're different levels", "tokens": [814, 434, 819, 4358], "temperature": 0.0, "avg_logprob": -0.09776624368161571, "compression_ratio": 1.6228070175438596, "no_speech_prob": 9.223113011103123e-06}, {"id": 901, "seek": 408848, "start": 4105.48, "end": 4108.48, "text": " So I decided to turn those into similar things, into these binary", "tokens": [407, 286, 3047, 281, 1261, 729, 666, 2531, 721, 11, 666, 613, 17434], "temperature": 0.0, "avg_logprob": -0.09776624368161571, "compression_ratio": 1.6228070175438596, "no_speech_prob": 9.223113011103123e-06}, {"id": 902, "seek": 408848, "start": 4108.48, "end": 4111.48, "text": " These are called binary categorical variables", "tokens": [1981, 366, 1219, 17434, 19250, 804, 9102], "temperature": 0.0, "avg_logprob": -0.09776624368161571, "compression_ratio": 1.6228070175438596, "no_speech_prob": 9.223113011103123e-06}, {"id": 903, "seek": 408848, "start": 4111.48, "end": 4114.48, "text": " So are they first class?", "tokens": [407, 366, 436, 700, 1508, 30], "temperature": 0.0, "avg_logprob": -0.09776624368161571, "compression_ratio": 1.6228070175438596, "no_speech_prob": 9.223113011103123e-06}, {"id": 904, "seek": 408848, "start": 4114.48, "end": 4117.48, "text": " And are they second class?", "tokens": [400, 366, 436, 1150, 1508, 30], "temperature": 0.0, "avg_logprob": -0.09776624368161571, "compression_ratio": 1.6228070175438596, "no_speech_prob": 9.223113011103123e-06}, {"id": 905, "seek": 411748, "start": 4117.48, "end": 4121.48, "text": " Okay, so that's all that", "tokens": [1033, 11, 370, 300, 311, 439, 300], "temperature": 0.0, "avg_logprob": -0.15116250261347344, "compression_ratio": 1.7107843137254901, "no_speech_prob": 9.026315819937736e-05}, {"id": 906, "seek": 411748, "start": 4121.48, "end": 4124.48, "text": " The other thing that I was thinking, well, you know, then I kind of tried it", "tokens": [440, 661, 551, 300, 286, 390, 1953, 11, 731, 11, 291, 458, 11, 550, 286, 733, 295, 3031, 309], "temperature": 0.0, "avg_logprob": -0.15116250261347344, "compression_ratio": 1.7107843137254901, "no_speech_prob": 9.026315819937736e-05}, {"id": 907, "seek": 411748, "start": 4124.48, "end": 4126.48, "text": " And checked out what happened", "tokens": [400, 10033, 484, 437, 2011], "temperature": 0.0, "avg_logprob": -0.15116250261347344, "compression_ratio": 1.7107843137254901, "no_speech_prob": 9.026315819937736e-05}, {"id": 908, "seek": 411748, "start": 4126.48, "end": 4132.48, "text": " And what happened was the people with, so I created some random numbers", "tokens": [400, 437, 2011, 390, 264, 561, 365, 11, 370, 286, 2942, 512, 4974, 3547], "temperature": 0.0, "avg_logprob": -0.15116250261347344, "compression_ratio": 1.7107843137254901, "no_speech_prob": 9.026315819937736e-05}, {"id": 909, "seek": 411748, "start": 4132.48, "end": 4138.48, "text": " So to create the random numbers I just went equals rand, right?", "tokens": [407, 281, 1884, 264, 4974, 3547, 286, 445, 1437, 6915, 367, 474, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15116250261347344, "compression_ratio": 1.7107843137254901, "no_speech_prob": 9.026315819937736e-05}, {"id": 910, "seek": 411748, "start": 4138.48, "end": 4141.48, "text": " And I copied those to the right", "tokens": [400, 286, 25365, 729, 281, 264, 558], "temperature": 0.0, "avg_logprob": -0.15116250261347344, "compression_ratio": 1.7107843137254901, "no_speech_prob": 9.026315819937736e-05}, {"id": 911, "seek": 411748, "start": 4141.48, "end": 4144.48, "text": " And then I just went copy and I went paste values", "tokens": [400, 550, 286, 445, 1437, 5055, 293, 286, 1437, 9163, 4190], "temperature": 0.0, "avg_logprob": -0.15116250261347344, "compression_ratio": 1.7107843137254901, "no_speech_prob": 9.026315819937736e-05}, {"id": 912, "seek": 414448, "start": 4144.48, "end": 4147.48, "text": " So that gave me some random numbers", "tokens": [407, 300, 2729, 385, 512, 4974, 3547], "temperature": 0.0, "avg_logprob": -0.09901651272103806, "compression_ratio": 1.668103448275862, "no_speech_prob": 2.5465446015005e-05}, {"id": 913, "seek": 414448, "start": 4147.48, "end": 4152.48, "text": " Before I said, oh, a, b, and c, let's just start them at 1.5, 1.5, 1.5", "tokens": [4546, 286, 848, 11, 1954, 11, 257, 11, 272, 11, 293, 269, 11, 718, 311, 445, 722, 552, 412, 502, 13, 20, 11, 502, 13, 20, 11, 502, 13, 20], "temperature": 0.0, "avg_logprob": -0.09901651272103806, "compression_ratio": 1.668103448275862, "no_speech_prob": 2.5465446015005e-05}, {"id": 914, "seek": 414448, "start": 4152.48, "end": 4157.48, "text": " What we do in real life is we start our parameters at random numbers", "tokens": [708, 321, 360, 294, 957, 993, 307, 321, 722, 527, 9834, 412, 4974, 3547], "temperature": 0.0, "avg_logprob": -0.09901651272103806, "compression_ratio": 1.668103448275862, "no_speech_prob": 2.5465446015005e-05}, {"id": 915, "seek": 414448, "start": 4157.48, "end": 4160.48, "text": " That are a bit more or a bit less than zero", "tokens": [663, 366, 257, 857, 544, 420, 257, 857, 1570, 813, 4018], "temperature": 0.0, "avg_logprob": -0.09901651272103806, "compression_ratio": 1.668103448275862, "no_speech_prob": 2.5465446015005e-05}, {"id": 916, "seek": 414448, "start": 4160.48, "end": 4162.48, "text": " So these are random numbers", "tokens": [407, 613, 366, 4974, 3547], "temperature": 0.0, "avg_logprob": -0.09901651272103806, "compression_ratio": 1.668103448275862, "no_speech_prob": 2.5465446015005e-05}, {"id": 917, "seek": 414448, "start": 4162.48, "end": 4164.48, "text": " Actually, sorry, I slightly lied", "tokens": [5135, 11, 2597, 11, 286, 4748, 20101], "temperature": 0.0, "avg_logprob": -0.09901651272103806, "compression_ratio": 1.668103448275862, "no_speech_prob": 2.5465446015005e-05}, {"id": 918, "seek": 414448, "start": 4164.48, "end": 4168.48, "text": " I didn't use rand, I used rand minus 0.5", "tokens": [286, 994, 380, 764, 367, 474, 11, 286, 1143, 367, 474, 3175, 1958, 13, 20], "temperature": 0.0, "avg_logprob": -0.09901651272103806, "compression_ratio": 1.668103448275862, "no_speech_prob": 2.5465446015005e-05}, {"id": 919, "seek": 414448, "start": 4168.48, "end": 4173.48, "text": " And that way I got small numbers that were on either side of zero", "tokens": [400, 300, 636, 286, 658, 1359, 3547, 300, 645, 322, 2139, 1252, 295, 4018], "temperature": 0.0, "avg_logprob": -0.09901651272103806, "compression_ratio": 1.668103448275862, "no_speech_prob": 2.5465446015005e-05}, {"id": 920, "seek": 417348, "start": 4173.48, "end": 4178.48, "text": " So then when I took each of these", "tokens": [407, 550, 562, 286, 1890, 1184, 295, 613], "temperature": 0.0, "avg_logprob": -0.09839866663280286, "compression_ratio": 1.6647727272727273, "no_speech_prob": 1.6963227608357556e-05}, {"id": 921, "seek": 417348, "start": 4178.48, "end": 4181.48, "text": " And I multiplied them by", "tokens": [400, 286, 17207, 552, 538], "temperature": 0.0, "avg_logprob": -0.09839866663280286, "compression_ratio": 1.6647727272727273, "no_speech_prob": 1.6963227608357556e-05}, {"id": 922, "seek": 417348, "start": 4181.48, "end": 4185.48, "text": " Our fairs and ages and so forth", "tokens": [2621, 3143, 82, 293, 12357, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.09839866663280286, "compression_ratio": 1.6647727272727273, "no_speech_prob": 1.6963227608357556e-05}, {"id": 923, "seek": 417348, "start": 4185.48, "end": 4189.48, "text": " What happened was that these numbers here", "tokens": [708, 2011, 390, 300, 613, 3547, 510], "temperature": 0.0, "avg_logprob": -0.09839866663280286, "compression_ratio": 1.6647727272727273, "no_speech_prob": 1.6963227608357556e-05}, {"id": 924, "seek": 417348, "start": 4189.48, "end": 4194.48, "text": " Are way bigger than, you know, these numbers here", "tokens": [2014, 636, 3801, 813, 11, 291, 458, 11, 613, 3547, 510], "temperature": 0.0, "avg_logprob": -0.09839866663280286, "compression_ratio": 1.6647727272727273, "no_speech_prob": 1.6963227608357556e-05}, {"id": 925, "seek": 417348, "start": 4194.48, "end": 4198.48, "text": " And so in the end all that mattered was what was their fair", "tokens": [400, 370, 294, 264, 917, 439, 300, 44282, 390, 437, 390, 641, 3143], "temperature": 0.0, "avg_logprob": -0.09839866663280286, "compression_ratio": 1.6647727272727273, "no_speech_prob": 1.6963227608357556e-05}, {"id": 926, "seek": 417348, "start": 4198.48, "end": 4200.48, "text": " Because they were just bigger than everything else", "tokens": [1436, 436, 645, 445, 3801, 813, 1203, 1646], "temperature": 0.0, "avg_logprob": -0.09839866663280286, "compression_ratio": 1.6647727272727273, "no_speech_prob": 1.6963227608357556e-05}, {"id": 927, "seek": 420048, "start": 4200.48, "end": 4203.48, "text": " So I wanted everything to basically go from 0 to 1", "tokens": [407, 286, 1415, 1203, 281, 1936, 352, 490, 1958, 281, 502], "temperature": 0.0, "avg_logprob": -0.13345132794296533, "compression_ratio": 1.6611570247933884, "no_speech_prob": 3.5911656595999375e-05}, {"id": 928, "seek": 420048, "start": 4203.48, "end": 4205.48, "text": " These numbers were too big", "tokens": [1981, 3547, 645, 886, 955], "temperature": 0.0, "avg_logprob": -0.13345132794296533, "compression_ratio": 1.6611570247933884, "no_speech_prob": 3.5911656595999375e-05}, {"id": 929, "seek": 420048, "start": 4205.48, "end": 4209.48, "text": " So what I did up here is I just grabbed the maximum of this column", "tokens": [407, 437, 286, 630, 493, 510, 307, 286, 445, 18607, 264, 6674, 295, 341, 7738], "temperature": 0.0, "avg_logprob": -0.13345132794296533, "compression_ratio": 1.6611570247933884, "no_speech_prob": 3.5911656595999375e-05}, {"id": 930, "seek": 420048, "start": 4209.48, "end": 4213.48, "text": " The maximum of all the fairs is 512", "tokens": [440, 6674, 295, 439, 264, 3143, 82, 307, 1025, 4762], "temperature": 0.0, "avg_logprob": -0.13345132794296533, "compression_ratio": 1.6611570247933884, "no_speech_prob": 3.5911656595999375e-05}, {"id": 931, "seek": 420048, "start": 4213.48, "end": 4217.48, "text": " And so then, actually I'll do age first", "tokens": [400, 370, 550, 11, 767, 286, 603, 360, 3205, 700], "temperature": 0.0, "avg_logprob": -0.13345132794296533, "compression_ratio": 1.6611570247933884, "no_speech_prob": 3.5911656595999375e-05}, {"id": 932, "seek": 420048, "start": 4217.48, "end": 4220.48, "text": " I did maximum of age, because similar thing, right?", "tokens": [286, 630, 6674, 295, 3205, 11, 570, 2531, 551, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13345132794296533, "compression_ratio": 1.6611570247933884, "no_speech_prob": 3.5911656595999375e-05}, {"id": 933, "seek": 420048, "start": 4220.48, "end": 4222.48, "text": " There's 80 year olds and there's 2 year olds", "tokens": [821, 311, 4688, 1064, 41972, 293, 456, 311, 568, 1064, 41972], "temperature": 0.0, "avg_logprob": -0.13345132794296533, "compression_ratio": 1.6611570247933884, "no_speech_prob": 3.5911656595999375e-05}, {"id": 934, "seek": 420048, "start": 4222.48, "end": 4224.48, "text": " And so then over here I just did", "tokens": [400, 370, 550, 670, 510, 286, 445, 630], "temperature": 0.0, "avg_logprob": -0.13345132794296533, "compression_ratio": 1.6611570247933884, "no_speech_prob": 3.5911656595999375e-05}, {"id": 935, "seek": 420048, "start": 4224.48, "end": 4229.48, "text": " Okay, well what's their age divided by the maximum?", "tokens": [1033, 11, 731, 437, 311, 641, 3205, 6666, 538, 264, 6674, 30], "temperature": 0.0, "avg_logprob": -0.13345132794296533, "compression_ratio": 1.6611570247933884, "no_speech_prob": 3.5911656595999375e-05}, {"id": 936, "seek": 422948, "start": 4229.48, "end": 4233.48, "text": " And so that way all of these are between 0 and 1", "tokens": [400, 370, 300, 636, 439, 295, 613, 366, 1296, 1958, 293, 502], "temperature": 0.0, "avg_logprob": -0.08016442181019301, "compression_ratio": 1.6185567010309279, "no_speech_prob": 1.9832703401334584e-05}, {"id": 937, "seek": 422948, "start": 4233.48, "end": 4236.48, "text": " Just like all of these are between 0 and 1", "tokens": [1449, 411, 439, 295, 613, 366, 1296, 1958, 293, 502], "temperature": 0.0, "avg_logprob": -0.08016442181019301, "compression_ratio": 1.6185567010309279, "no_speech_prob": 1.9832703401334584e-05}, {"id": 938, "seek": 422948, "start": 4236.48, "end": 4241.48, "text": " So that's how I fixed, this is called normalizing the data", "tokens": [407, 300, 311, 577, 286, 6806, 11, 341, 307, 1219, 2710, 3319, 264, 1412], "temperature": 0.0, "avg_logprob": -0.08016442181019301, "compression_ratio": 1.6185567010309279, "no_speech_prob": 1.9832703401334584e-05}, {"id": 939, "seek": 422948, "start": 4241.48, "end": 4247.48, "text": " Now, we haven't done any of these things when we've done stuff with FastAI", "tokens": [823, 11, 321, 2378, 380, 1096, 604, 295, 613, 721, 562, 321, 600, 1096, 1507, 365, 15968, 48698], "temperature": 0.0, "avg_logprob": -0.08016442181019301, "compression_ratio": 1.6185567010309279, "no_speech_prob": 1.9832703401334584e-05}, {"id": 940, "seek": 422948, "start": 4247.48, "end": 4251.48, "text": " That's because FastAI does all of these things for you", "tokens": [663, 311, 570, 15968, 48698, 775, 439, 295, 613, 721, 337, 291], "temperature": 0.0, "avg_logprob": -0.08016442181019301, "compression_ratio": 1.6185567010309279, "no_speech_prob": 1.9832703401334584e-05}, {"id": 941, "seek": 422948, "start": 4251.48, "end": 4255.48, "text": " And we'll learn about how, right?", "tokens": [400, 321, 603, 1466, 466, 577, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.08016442181019301, "compression_ratio": 1.6185567010309279, "no_speech_prob": 1.9832703401334584e-05}, {"id": 942, "seek": 425548, "start": 4255.48, "end": 4260.48, "text": " But all these things are being done behind the scenes", "tokens": [583, 439, 613, 721, 366, 885, 1096, 2261, 264, 8026], "temperature": 0.0, "avg_logprob": -0.08968050503036351, "compression_ratio": 1.8605769230769231, "no_speech_prob": 7.483246736228466e-05}, {"id": 943, "seek": 425548, "start": 4260.48, "end": 4264.48, "text": " For fair, I did something a bit more", "tokens": [1171, 3143, 11, 286, 630, 746, 257, 857, 544], "temperature": 0.0, "avg_logprob": -0.08968050503036351, "compression_ratio": 1.8605769230769231, "no_speech_prob": 7.483246736228466e-05}, {"id": 944, "seek": 425548, "start": 4264.48, "end": 4267.48, "text": " Which is I noticed there's lots of very small fairs", "tokens": [3013, 307, 286, 5694, 456, 311, 3195, 295, 588, 1359, 3143, 82], "temperature": 0.0, "avg_logprob": -0.08968050503036351, "compression_ratio": 1.8605769230769231, "no_speech_prob": 7.483246736228466e-05}, {"id": 945, "seek": 425548, "start": 4267.48, "end": 4270.48, "text": " And there's also a few very big fairs", "tokens": [400, 456, 311, 611, 257, 1326, 588, 955, 3143, 82], "temperature": 0.0, "avg_logprob": -0.08968050503036351, "compression_ratio": 1.8605769230769231, "no_speech_prob": 7.483246736228466e-05}, {"id": 946, "seek": 425548, "start": 4270.48, "end": 4273.48, "text": " So like $70 and then $7, $7", "tokens": [407, 411, 1848, 5867, 293, 550, 1848, 22, 11, 1848, 22], "temperature": 0.0, "avg_logprob": -0.08968050503036351, "compression_ratio": 1.8605769230769231, "no_speech_prob": 7.483246736228466e-05}, {"id": 947, "seek": 425548, "start": 4273.48, "end": 4278.48, "text": " Generally speaking, when you have lots of really big numbers and a few small ones", "tokens": [21082, 4124, 11, 562, 291, 362, 3195, 295, 534, 955, 3547, 293, 257, 1326, 1359, 2306], "temperature": 0.0, "avg_logprob": -0.08968050503036351, "compression_ratio": 1.8605769230769231, "no_speech_prob": 7.483246736228466e-05}, {"id": 948, "seek": 425548, "start": 4278.48, "end": 4283.48, "text": " So generally speaking, when you've got a few really big numbers and lots of really small numbers", "tokens": [407, 5101, 4124, 11, 562, 291, 600, 658, 257, 1326, 534, 955, 3547, 293, 3195, 295, 534, 1359, 3547], "temperature": 0.0, "avg_logprob": -0.08968050503036351, "compression_ratio": 1.8605769230769231, "no_speech_prob": 7.483246736228466e-05}, {"id": 949, "seek": 428348, "start": 4283.48, "end": 4287.48, "text": " This is really common with money", "tokens": [639, 307, 534, 2689, 365, 1460], "temperature": 0.0, "avg_logprob": -0.10112672693589154, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.7534475520951673e-05}, {"id": 950, "seek": 428348, "start": 4287.48, "end": 4291.48, "text": " Because money kind of follows this relationship where a few people have lots of it", "tokens": [1436, 1460, 733, 295, 10002, 341, 2480, 689, 257, 1326, 561, 362, 3195, 295, 309], "temperature": 0.0, "avg_logprob": -0.10112672693589154, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.7534475520951673e-05}, {"id": 951, "seek": 428348, "start": 4291.48, "end": 4295.48, "text": " And they spend huge amounts of it and most people don't have heaps", "tokens": [400, 436, 3496, 2603, 11663, 295, 309, 293, 881, 561, 500, 380, 362, 415, 2382], "temperature": 0.0, "avg_logprob": -0.10112672693589154, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.7534475520951673e-05}, {"id": 952, "seek": 428348, "start": 4295.48, "end": 4300.48, "text": " If you take the log of something that has that kind of extreme distribution", "tokens": [759, 291, 747, 264, 3565, 295, 746, 300, 575, 300, 733, 295, 8084, 7316], "temperature": 0.0, "avg_logprob": -0.10112672693589154, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.7534475520951673e-05}, {"id": 953, "seek": 428348, "start": 4300.48, "end": 4303.48, "text": " You end up with something that's much more evenly distributed", "tokens": [509, 917, 493, 365, 746, 300, 311, 709, 544, 17658, 12631], "temperature": 0.0, "avg_logprob": -0.10112672693589154, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.7534475520951673e-05}, {"id": 954, "seek": 428348, "start": 4303.48, "end": 4308.48, "text": " So I've added this here called log fair, as you can see", "tokens": [407, 286, 600, 3869, 341, 510, 1219, 3565, 3143, 11, 382, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.10112672693589154, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.7534475520951673e-05}, {"id": 955, "seek": 428348, "start": 4308.48, "end": 4310.48, "text": " And these are all around 1, which isn't bad", "tokens": [400, 613, 366, 439, 926, 502, 11, 597, 1943, 380, 1578], "temperature": 0.0, "avg_logprob": -0.10112672693589154, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.7534475520951673e-05}, {"id": 956, "seek": 431048, "start": 4310.48, "end": 4313.48, "text": " I could have normalized that as well, but I was too lazy, I didn't bother", "tokens": [286, 727, 362, 48704, 300, 382, 731, 11, 457, 286, 390, 886, 14847, 11, 286, 994, 380, 8677], "temperature": 0.0, "avg_logprob": -0.11291498723237411, "compression_ratio": 1.5565610859728507, "no_speech_prob": 1.0289327292412054e-05}, {"id": 957, "seek": 431048, "start": 4313.48, "end": 4315.48, "text": " Because it seemed okay", "tokens": [1436, 309, 6576, 1392], "temperature": 0.0, "avg_logprob": -0.11291498723237411, "compression_ratio": 1.5565610859728507, "no_speech_prob": 1.0289327292412054e-05}, {"id": 958, "seek": 431048, "start": 4315.48, "end": 4322.48, "text": " So at this point you can now see that if we start from here", "tokens": [407, 412, 341, 935, 291, 393, 586, 536, 300, 498, 321, 722, 490, 510], "temperature": 0.0, "avg_logprob": -0.11291498723237411, "compression_ratio": 1.5565610859728507, "no_speech_prob": 1.0289327292412054e-05}, {"id": 959, "seek": 431048, "start": 4322.48, "end": 4326.48, "text": " All of these are all around the same kind of level", "tokens": [1057, 295, 613, 366, 439, 926, 264, 912, 733, 295, 1496], "temperature": 0.0, "avg_logprob": -0.11291498723237411, "compression_ratio": 1.5565610859728507, "no_speech_prob": 1.0289327292412054e-05}, {"id": 960, "seek": 431048, "start": 4326.48, "end": 4332.48, "text": " So none of these columns are going to saturate the others", "tokens": [407, 6022, 295, 613, 13766, 366, 516, 281, 21160, 473, 264, 2357], "temperature": 0.0, "avg_logprob": -0.11291498723237411, "compression_ratio": 1.5565610859728507, "no_speech_prob": 1.0289327292412054e-05}, {"id": 961, "seek": 431048, "start": 4332.48, "end": 4339.48, "text": " So now I've got my coefficients, which are just as I said, they're just random", "tokens": [407, 586, 286, 600, 658, 452, 31994, 11, 597, 366, 445, 382, 286, 848, 11, 436, 434, 445, 4974], "temperature": 0.0, "avg_logprob": -0.11291498723237411, "compression_ratio": 1.5565610859728507, "no_speech_prob": 1.0289327292412054e-05}, {"id": 962, "seek": 433948, "start": 4339.48, "end": 4347.48, "text": " And so now I need to basically calculate AX1 plus BX2 plus CX3 plus blah blah blah blah", "tokens": [400, 370, 586, 286, 643, 281, 1936, 8873, 316, 55, 16, 1804, 363, 55, 17, 1804, 383, 55, 18, 1804, 12288, 12288, 12288, 12288], "temperature": 0.0, "avg_logprob": -0.15329649713304308, "compression_ratio": 1.5058823529411764, "no_speech_prob": 8.939417966757901e-06}, {"id": 963, "seek": 433948, "start": 4347.48, "end": 4355.48, "text": " And so to do that, you can use some product in Excel", "tokens": [400, 370, 281, 360, 300, 11, 291, 393, 764, 512, 1674, 294, 19060], "temperature": 0.0, "avg_logprob": -0.15329649713304308, "compression_ratio": 1.5058823529411764, "no_speech_prob": 8.939417966757901e-06}, {"id": 964, "seek": 433948, "start": 4355.48, "end": 4357.48, "text": " I could have typed it out by hand, it could be very boring", "tokens": [286, 727, 362, 33941, 309, 484, 538, 1011, 11, 309, 727, 312, 588, 9989], "temperature": 0.0, "avg_logprob": -0.15329649713304308, "compression_ratio": 1.5058823529411764, "no_speech_prob": 8.939417966757901e-06}, {"id": 965, "seek": 433948, "start": 4357.48, "end": 4362.48, "text": " But some product is just going to multiply each of these", "tokens": [583, 512, 1674, 307, 445, 516, 281, 12972, 1184, 295, 613], "temperature": 0.0, "avg_logprob": -0.15329649713304308, "compression_ratio": 1.5058823529411764, "no_speech_prob": 8.939417966757901e-06}, {"id": 966, "seek": 436248, "start": 4362.48, "end": 4371.48, "text": " This one will be multiplied by this one, this one will be multiplied by this one, so forth", "tokens": [639, 472, 486, 312, 17207, 538, 341, 472, 11, 341, 472, 486, 312, 17207, 538, 341, 472, 11, 370, 5220], "temperature": 0.0, "avg_logprob": -0.09359282539004371, "compression_ratio": 1.7636363636363637, "no_speech_prob": 1.130059808929218e-05}, {"id": 967, "seek": 436248, "start": 4371.48, "end": 4373.48, "text": " And then they get all added together", "tokens": [400, 550, 436, 483, 439, 3869, 1214], "temperature": 0.0, "avg_logprob": -0.09359282539004371, "compression_ratio": 1.7636363636363637, "no_speech_prob": 1.130059808929218e-05}, {"id": 968, "seek": 436248, "start": 4373.48, "end": 4377.48, "text": " Now one thing, if you're eagle-eyed, you might be wondering", "tokens": [823, 472, 551, 11, 498, 291, 434, 30745, 12, 37860, 11, 291, 1062, 312, 6359], "temperature": 0.0, "avg_logprob": -0.09359282539004371, "compression_ratio": 1.7636363636363637, "no_speech_prob": 1.130059808929218e-05}, {"id": 969, "seek": 436248, "start": 4377.48, "end": 4381.48, "text": " Is in a linear equation, we have Y equals MX plus B", "tokens": [1119, 294, 257, 8213, 5367, 11, 321, 362, 398, 6915, 47509, 1804, 363], "temperature": 0.0, "avg_logprob": -0.09359282539004371, "compression_ratio": 1.7636363636363637, "no_speech_prob": 1.130059808929218e-05}, {"id": 970, "seek": 436248, "start": 4381.48, "end": 4384.48, "text": " At the end there's this constant term", "tokens": [1711, 264, 917, 456, 311, 341, 5754, 1433], "temperature": 0.0, "avg_logprob": -0.09359282539004371, "compression_ratio": 1.7636363636363637, "no_speech_prob": 1.130059808929218e-05}, {"id": 971, "seek": 436248, "start": 4384.48, "end": 4386.48, "text": " And I do not have any constant term", "tokens": [400, 286, 360, 406, 362, 604, 5754, 1433], "temperature": 0.0, "avg_logprob": -0.09359282539004371, "compression_ratio": 1.7636363636363637, "no_speech_prob": 1.130059808929218e-05}, {"id": 972, "seek": 436248, "start": 4386.48, "end": 4391.48, "text": " I've got something here called const, but I don't have any plus at the end", "tokens": [286, 600, 658, 746, 510, 1219, 1817, 11, 457, 286, 500, 380, 362, 604, 1804, 412, 264, 917], "temperature": 0.0, "avg_logprob": -0.09359282539004371, "compression_ratio": 1.7636363636363637, "no_speech_prob": 1.130059808929218e-05}, {"id": 973, "seek": 439148, "start": 4391.48, "end": 4394.48, "text": " How's that working?", "tokens": [1012, 311, 300, 1364, 30], "temperature": 0.0, "avg_logprob": -0.059102180052776725, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.5689056453993544e-05}, {"id": 974, "seek": 439148, "start": 4394.48, "end": 4398.48, "text": " Well there's a nice trick that we pretty much always use in machine learning", "tokens": [1042, 456, 311, 257, 1481, 4282, 300, 321, 1238, 709, 1009, 764, 294, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.059102180052776725, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.5689056453993544e-05}, {"id": 975, "seek": 439148, "start": 4398.48, "end": 4403.48, "text": " Which is to add a column of data just containing the number one every time", "tokens": [3013, 307, 281, 909, 257, 7738, 295, 1412, 445, 19273, 264, 1230, 472, 633, 565], "temperature": 0.0, "avg_logprob": -0.059102180052776725, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.5689056453993544e-05}, {"id": 976, "seek": 439148, "start": 4403.48, "end": 4406.48, "text": " If you have a column of data containing the number one every time", "tokens": [759, 291, 362, 257, 7738, 295, 1412, 19273, 264, 1230, 472, 633, 565], "temperature": 0.0, "avg_logprob": -0.059102180052776725, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.5689056453993544e-05}, {"id": 977, "seek": 439148, "start": 4406.48, "end": 4409.48, "text": " Then that parameter becomes your constant term", "tokens": [1396, 300, 13075, 3643, 428, 5754, 1433], "temperature": 0.0, "avg_logprob": -0.059102180052776725, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.5689056453993544e-05}, {"id": 978, "seek": 439148, "start": 4409.48, "end": 4413.48, "text": " So you don't have to have a special constant term", "tokens": [407, 291, 500, 380, 362, 281, 362, 257, 2121, 5754, 1433], "temperature": 0.0, "avg_logprob": -0.059102180052776725, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.5689056453993544e-05}, {"id": 979, "seek": 439148, "start": 4413.48, "end": 4419.48, "text": " And so it makes our code a little bit simpler when you do it that way", "tokens": [400, 370, 309, 1669, 527, 3089, 257, 707, 857, 18587, 562, 291, 360, 309, 300, 636], "temperature": 0.0, "avg_logprob": -0.059102180052776725, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.5689056453993544e-05}, {"id": 980, "seek": 441948, "start": 4419.48, "end": 4422.48, "text": " It's just a trick, but everybody does it", "tokens": [467, 311, 445, 257, 4282, 11, 457, 2201, 775, 309], "temperature": 0.0, "avg_logprob": -0.11398224830627442, "compression_ratio": 1.6833333333333333, "no_speech_prob": 6.502345786429942e-05}, {"id": 981, "seek": 441948, "start": 4422.48, "end": 4426.48, "text": " So this is now the result of our linear model", "tokens": [407, 341, 307, 586, 264, 1874, 295, 527, 8213, 2316], "temperature": 0.0, "avg_logprob": -0.11398224830627442, "compression_ratio": 1.6833333333333333, "no_speech_prob": 6.502345786429942e-05}, {"id": 982, "seek": 441948, "start": 4426.48, "end": 4434.48, "text": " I'm not even going to do value, I'm just going to do the plain regression", "tokens": [286, 478, 406, 754, 516, 281, 360, 2158, 11, 286, 478, 445, 516, 281, 360, 264, 11121, 24590], "temperature": 0.0, "avg_logprob": -0.11398224830627442, "compression_ratio": 1.6833333333333333, "no_speech_prob": 6.502345786429942e-05}, {"id": 983, "seek": 441948, "start": 4434.48, "end": 4441.48, "text": " Now if you've done regression before, you might have learned about it as something you can solve with various metrics things", "tokens": [823, 498, 291, 600, 1096, 24590, 949, 11, 291, 1062, 362, 3264, 466, 309, 382, 746, 291, 393, 5039, 365, 3683, 16367, 721], "temperature": 0.0, "avg_logprob": -0.11398224830627442, "compression_ratio": 1.6833333333333333, "no_speech_prob": 6.502345786429942e-05}, {"id": 984, "seek": 441948, "start": 4441.48, "end": 4445.48, "text": " But in fact, you can solve a regression using gradient descent", "tokens": [583, 294, 1186, 11, 291, 393, 5039, 257, 24590, 1228, 16235, 23475], "temperature": 0.0, "avg_logprob": -0.11398224830627442, "compression_ratio": 1.6833333333333333, "no_speech_prob": 6.502345786429942e-05}, {"id": 985, "seek": 441948, "start": 4445.48, "end": 4448.48, "text": " So I've just gone ahead and created a loss for each row", "tokens": [407, 286, 600, 445, 2780, 2286, 293, 2942, 257, 4470, 337, 1184, 5386], "temperature": 0.0, "avg_logprob": -0.11398224830627442, "compression_ratio": 1.6833333333333333, "no_speech_prob": 6.502345786429942e-05}, {"id": 986, "seek": 444848, "start": 4448.48, "end": 4455.48, "text": " And so the loss is going to be equal to our prediction", "tokens": [400, 370, 264, 4470, 307, 516, 281, 312, 2681, 281, 527, 17630], "temperature": 0.0, "avg_logprob": -0.0834871700831822, "compression_ratio": 1.7475728155339805, "no_speech_prob": 2.6687341232900508e-05}, {"id": 987, "seek": 444848, "start": 4455.48, "end": 4459.48, "text": " Minus whether they survived squared", "tokens": [2829, 301, 1968, 436, 14433, 8889], "temperature": 0.0, "avg_logprob": -0.0834871700831822, "compression_ratio": 1.7475728155339805, "no_speech_prob": 2.6687341232900508e-05}, {"id": 988, "seek": 444848, "start": 4459.48, "end": 4463.48, "text": " So this is going to be our squared error", "tokens": [407, 341, 307, 516, 281, 312, 527, 8889, 6713], "temperature": 0.0, "avg_logprob": -0.0834871700831822, "compression_ratio": 1.7475728155339805, "no_speech_prob": 2.6687341232900508e-05}, {"id": 989, "seek": 444848, "start": 4463.48, "end": 4465.48, "text": " And there they all are, our squared errors", "tokens": [400, 456, 436, 439, 366, 11, 527, 8889, 13603], "temperature": 0.0, "avg_logprob": -0.0834871700831822, "compression_ratio": 1.7475728155339805, "no_speech_prob": 2.6687341232900508e-05}, {"id": 990, "seek": 444848, "start": 4465.48, "end": 4468.48, "text": " And so here I've just summed them up", "tokens": [400, 370, 510, 286, 600, 445, 2408, 1912, 552, 493], "temperature": 0.0, "avg_logprob": -0.0834871700831822, "compression_ratio": 1.7475728155339805, "no_speech_prob": 2.6687341232900508e-05}, {"id": 991, "seek": 444848, "start": 4468.48, "end": 4472.48, "text": " I could have taken the mean, I guess that would have been a bit easier to think about", "tokens": [286, 727, 362, 2726, 264, 914, 11, 286, 2041, 300, 576, 362, 668, 257, 857, 3571, 281, 519, 466], "temperature": 0.0, "avg_logprob": -0.0834871700831822, "compression_ratio": 1.7475728155339805, "no_speech_prob": 2.6687341232900508e-05}, {"id": 992, "seek": 444848, "start": 4472.48, "end": 4474.48, "text": " But sum is going to give us the same result", "tokens": [583, 2408, 307, 516, 281, 976, 505, 264, 912, 1874], "temperature": 0.0, "avg_logprob": -0.0834871700831822, "compression_ratio": 1.7475728155339805, "no_speech_prob": 2.6687341232900508e-05}, {"id": 993, "seek": 444848, "start": 4474.48, "end": 4477.48, "text": " So here's our loss", "tokens": [407, 510, 311, 527, 4470], "temperature": 0.0, "avg_logprob": -0.0834871700831822, "compression_ratio": 1.7475728155339805, "no_speech_prob": 2.6687341232900508e-05}, {"id": 994, "seek": 447748, "start": 4477.48, "end": 4480.48, "text": " And so now we need to optimize that using gradient descent", "tokens": [400, 370, 586, 321, 643, 281, 19719, 300, 1228, 16235, 23475], "temperature": 0.0, "avg_logprob": -0.08950074139763327, "compression_ratio": 1.6363636363636365, "no_speech_prob": 8.939440704125445e-06}, {"id": 995, "seek": 447748, "start": 4480.48, "end": 4485.48, "text": " So Microsoft Excel has a gradient descent optimizer in it called solver", "tokens": [407, 8116, 19060, 575, 257, 16235, 23475, 5028, 6545, 294, 309, 1219, 1404, 331], "temperature": 0.0, "avg_logprob": -0.08950074139763327, "compression_ratio": 1.6363636363636365, "no_speech_prob": 8.939440704125445e-06}, {"id": 996, "seek": 447748, "start": 4485.48, "end": 4487.48, "text": " So I'll click solver", "tokens": [407, 286, 603, 2052, 1404, 331], "temperature": 0.0, "avg_logprob": -0.08950074139763327, "compression_ratio": 1.6363636363636365, "no_speech_prob": 8.939440704125445e-06}, {"id": 997, "seek": 447748, "start": 4487.48, "end": 4490.48, "text": " And it'll say, okay, what are you trying to optimize?", "tokens": [400, 309, 603, 584, 11, 1392, 11, 437, 366, 291, 1382, 281, 19719, 30], "temperature": 0.0, "avg_logprob": -0.08950074139763327, "compression_ratio": 1.6363636363636365, "no_speech_prob": 8.939440704125445e-06}, {"id": 998, "seek": 447748, "start": 4490.48, "end": 4492.48, "text": " It's this one here", "tokens": [467, 311, 341, 472, 510], "temperature": 0.0, "avg_logprob": -0.08950074139763327, "compression_ratio": 1.6363636363636365, "no_speech_prob": 8.939440704125445e-06}, {"id": 999, "seek": 447748, "start": 4492.48, "end": 4499.48, "text": " And I'm going to do it by changing these cells here", "tokens": [400, 286, 478, 516, 281, 360, 309, 538, 4473, 613, 5438, 510], "temperature": 0.0, "avg_logprob": -0.08950074139763327, "compression_ratio": 1.6363636363636365, "no_speech_prob": 8.939440704125445e-06}, {"id": 1000, "seek": 447748, "start": 4499.48, "end": 4503.48, "text": " And I'm trying to minimize it", "tokens": [400, 286, 478, 1382, 281, 17522, 309], "temperature": 0.0, "avg_logprob": -0.08950074139763327, "compression_ratio": 1.6363636363636365, "no_speech_prob": 8.939440704125445e-06}, {"id": 1001, "seek": 450348, "start": 4503.48, "end": 4508.48, "text": " And so we're starting a loss of 55.78", "tokens": [400, 370, 321, 434, 2891, 257, 4470, 295, 12330, 13, 30693], "temperature": 0.0, "avg_logprob": -0.12388100190596148, "compression_ratio": 1.2335766423357664, "no_speech_prob": 3.6687938518298324e-06}, {"id": 1002, "seek": 450348, "start": 4508.48, "end": 4512.48, "text": " Actually, let's change it to mean as well", "tokens": [5135, 11, 718, 311, 1319, 309, 281, 914, 382, 731], "temperature": 0.0, "avg_logprob": -0.12388100190596148, "compression_ratio": 1.2335766423357664, "no_speech_prob": 3.6687938518298324e-06}, {"id": 1003, "seek": 450348, "start": 4512.48, "end": 4515.48, "text": " Is it called mean or average?", "tokens": [1119, 309, 1219, 914, 420, 4274, 30], "temperature": 0.0, "avg_logprob": -0.12388100190596148, "compression_ratio": 1.2335766423357664, "no_speech_prob": 3.6687938518298324e-06}, {"id": 1004, "seek": 450348, "start": 4515.48, "end": 4521.48, "text": " Probably average", "tokens": [9210, 4274], "temperature": 0.0, "avg_logprob": -0.12388100190596148, "compression_ratio": 1.2335766423357664, "no_speech_prob": 3.6687938518298324e-06}, {"id": 1005, "seek": 450348, "start": 4521.48, "end": 4528.48, "text": " Alright, so start at 1.03", "tokens": [2798, 11, 370, 722, 412, 502, 13, 11592], "temperature": 0.0, "avg_logprob": -0.12388100190596148, "compression_ratio": 1.2335766423357664, "no_speech_prob": 3.6687938518298324e-06}, {"id": 1006, "seek": 450348, "start": 4528.48, "end": 4532.48, "text": " So optimize that", "tokens": [407, 19719, 300], "temperature": 0.0, "avg_logprob": -0.12388100190596148, "compression_ratio": 1.2335766423357664, "no_speech_prob": 3.6687938518298324e-06}, {"id": 1007, "seek": 453248, "start": 4532.48, "end": 4533.48, "text": " And there we go", "tokens": [400, 456, 321, 352], "temperature": 0.0, "avg_logprob": -0.1306221592533696, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.4063469279790297e-05}, {"id": 1008, "seek": 453248, "start": 4533.48, "end": 4536.48, "text": " So it's gone from 1.03 to 0.1", "tokens": [407, 309, 311, 2780, 490, 502, 13, 11592, 281, 1958, 13, 16], "temperature": 0.0, "avg_logprob": -0.1306221592533696, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.4063469279790297e-05}, {"id": 1009, "seek": 453248, "start": 4536.48, "end": 4538.48, "text": " And so we can check the predictions", "tokens": [400, 370, 321, 393, 1520, 264, 21264], "temperature": 0.0, "avg_logprob": -0.1306221592533696, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.4063469279790297e-05}, {"id": 1010, "seek": 453248, "start": 4538.48, "end": 4544.48, "text": " So the first one, it predicted exactly correctly", "tokens": [407, 264, 700, 472, 11, 309, 19147, 2293, 8944], "temperature": 0.0, "avg_logprob": -0.1306221592533696, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.4063469279790297e-05}, {"id": 1011, "seek": 453248, "start": 4544.48, "end": 4548.48, "text": " It was, they didn't survive and we predict it wouldn't survive", "tokens": [467, 390, 11, 436, 994, 380, 7867, 293, 321, 6069, 309, 2759, 380, 7867], "temperature": 0.0, "avg_logprob": -0.1306221592533696, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.4063469279790297e-05}, {"id": 1012, "seek": 453248, "start": 4548.48, "end": 4550.48, "text": " Ditto for this one", "tokens": [413, 34924, 337, 341, 472], "temperature": 0.0, "avg_logprob": -0.1306221592533696, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.4063469279790297e-05}, {"id": 1013, "seek": 453248, "start": 4550.48, "end": 4551.48, "text": " It's very close", "tokens": [467, 311, 588, 1998], "temperature": 0.0, "avg_logprob": -0.1306221592533696, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.4063469279790297e-05}, {"id": 1014, "seek": 453248, "start": 4551.48, "end": 4555.48, "text": " And you can start to see, so this one, you can start to see a few issues here", "tokens": [400, 291, 393, 722, 281, 536, 11, 370, 341, 472, 11, 291, 393, 722, 281, 536, 257, 1326, 2663, 510], "temperature": 0.0, "avg_logprob": -0.1306221592533696, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.4063469279790297e-05}, {"id": 1015, "seek": 453248, "start": 4555.48, "end": 4559.48, "text": " Which is like sometimes it's predicting less than one, sorry, less than zero", "tokens": [3013, 307, 411, 2171, 309, 311, 32884, 1570, 813, 472, 11, 2597, 11, 1570, 813, 4018], "temperature": 0.0, "avg_logprob": -0.1306221592533696, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.4063469279790297e-05}, {"id": 1016, "seek": 455948, "start": 4559.48, "end": 4562.48, "text": " And sometimes it's predicting more than one", "tokens": [400, 2171, 309, 311, 32884, 544, 813, 472], "temperature": 0.0, "avg_logprob": -0.05884275128764491, "compression_ratio": 1.7453183520599251, "no_speech_prob": 7.411078058794374e-06}, {"id": 1017, "seek": 455948, "start": 4562.48, "end": 4568.48, "text": " Wouldn't it be cool if we had some way of constraining it to between zero and one?", "tokens": [26291, 380, 309, 312, 1627, 498, 321, 632, 512, 636, 295, 11525, 1760, 309, 281, 1296, 4018, 293, 472, 30], "temperature": 0.0, "avg_logprob": -0.05884275128764491, "compression_ratio": 1.7453183520599251, "no_speech_prob": 7.411078058794374e-06}, {"id": 1018, "seek": 455948, "start": 4568.48, "end": 4573.48, "text": " And that's an example of some of the things we're going to learn about that make this stuff work a little bit better", "tokens": [400, 300, 311, 364, 1365, 295, 512, 295, 264, 721, 321, 434, 516, 281, 1466, 466, 300, 652, 341, 1507, 589, 257, 707, 857, 1101], "temperature": 0.0, "avg_logprob": -0.05884275128764491, "compression_ratio": 1.7453183520599251, "no_speech_prob": 7.411078058794374e-06}, {"id": 1019, "seek": 455948, "start": 4573.48, "end": 4575.48, "text": " But you can see it's doing an okay job", "tokens": [583, 291, 393, 536, 309, 311, 884, 364, 1392, 1691], "temperature": 0.0, "avg_logprob": -0.05884275128764491, "compression_ratio": 1.7453183520599251, "no_speech_prob": 7.411078058794374e-06}, {"id": 1020, "seek": 455948, "start": 4575.48, "end": 4579.48, "text": " So this is not deep learning, this is not a neural net yet, this is just a regression", "tokens": [407, 341, 307, 406, 2452, 2539, 11, 341, 307, 406, 257, 18161, 2533, 1939, 11, 341, 307, 445, 257, 24590], "temperature": 0.0, "avg_logprob": -0.05884275128764491, "compression_ratio": 1.7453183520599251, "no_speech_prob": 7.411078058794374e-06}, {"id": 1021, "seek": 455948, "start": 4579.48, "end": 4584.48, "text": " So to make it into a neural net, we need to do it multiple times", "tokens": [407, 281, 652, 309, 666, 257, 18161, 2533, 11, 321, 643, 281, 360, 309, 3866, 1413], "temperature": 0.0, "avg_logprob": -0.05884275128764491, "compression_ratio": 1.7453183520599251, "no_speech_prob": 7.411078058794374e-06}, {"id": 1022, "seek": 455948, "start": 4584.48, "end": 4586.48, "text": " So I'm just going to do it twice", "tokens": [407, 286, 478, 445, 516, 281, 360, 309, 6091], "temperature": 0.0, "avg_logprob": -0.05884275128764491, "compression_ratio": 1.7453183520599251, "no_speech_prob": 7.411078058794374e-06}, {"id": 1023, "seek": 458648, "start": 4586.48, "end": 4591.48, "text": " So now rather than one set of coefficients, I've got two sets", "tokens": [407, 586, 2831, 813, 472, 992, 295, 31994, 11, 286, 600, 658, 732, 6352], "temperature": 0.0, "avg_logprob": -0.06276872359126447, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.198173337499611e-05}, {"id": 1024, "seek": 458648, "start": 4591.48, "end": 4595.48, "text": " And again, I just put in random numbers", "tokens": [400, 797, 11, 286, 445, 829, 294, 4974, 3547], "temperature": 0.0, "avg_logprob": -0.06276872359126447, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.198173337499611e-05}, {"id": 1025, "seek": 458648, "start": 4595.48, "end": 4599.48, "text": " Other than that, all the data is the same", "tokens": [5358, 813, 300, 11, 439, 264, 1412, 307, 264, 912], "temperature": 0.0, "avg_logprob": -0.06276872359126447, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.198173337499611e-05}, {"id": 1026, "seek": 458648, "start": 4599.48, "end": 4604.48, "text": " And so now I'm going to have my sum product again", "tokens": [400, 370, 586, 286, 478, 516, 281, 362, 452, 2408, 1674, 797], "temperature": 0.0, "avg_logprob": -0.06276872359126447, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.198173337499611e-05}, {"id": 1027, "seek": 458648, "start": 4604.48, "end": 4609.48, "text": " So the first sum product is with my first set of coefficients", "tokens": [407, 264, 700, 2408, 1674, 307, 365, 452, 700, 992, 295, 31994], "temperature": 0.0, "avg_logprob": -0.06276872359126447, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.198173337499611e-05}, {"id": 1028, "seek": 458648, "start": 4609.48, "end": 4613.48, "text": " And my second sum product is with my second set of coefficients", "tokens": [400, 452, 1150, 2408, 1674, 307, 365, 452, 1150, 992, 295, 31994], "temperature": 0.0, "avg_logprob": -0.06276872359126447, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.198173337499611e-05}, {"id": 1029, "seek": 461348, "start": 4613.48, "end": 4616.48, "text": " So I'm just calling them linear one and linear two", "tokens": [407, 286, 478, 445, 5141, 552, 8213, 472, 293, 8213, 732], "temperature": 0.0, "avg_logprob": -0.07028228925622028, "compression_ratio": 1.69140625, "no_speech_prob": 3.0714487365912646e-05}, {"id": 1030, "seek": 461348, "start": 4616.48, "end": 4618.48, "text": " Now there's no point adding those up together", "tokens": [823, 456, 311, 572, 935, 5127, 729, 493, 1214], "temperature": 0.0, "avg_logprob": -0.07028228925622028, "compression_ratio": 1.69140625, "no_speech_prob": 3.0714487365912646e-05}, {"id": 1031, "seek": 461348, "start": 4618.48, "end": 4622.48, "text": " Because if you add up two linear functions together, you get another linear function", "tokens": [1436, 498, 291, 909, 493, 732, 8213, 6828, 1214, 11, 291, 483, 1071, 8213, 2445], "temperature": 0.0, "avg_logprob": -0.07028228925622028, "compression_ratio": 1.69140625, "no_speech_prob": 3.0714487365912646e-05}, {"id": 1032, "seek": 461348, "start": 4622.48, "end": 4624.48, "text": " We want to get all those wiggles, right?", "tokens": [492, 528, 281, 483, 439, 729, 261, 19469, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.07028228925622028, "compression_ratio": 1.69140625, "no_speech_prob": 3.0714487365912646e-05}, {"id": 1033, "seek": 461348, "start": 4624.48, "end": 4628.48, "text": " So that's why we have to do our ReLU", "tokens": [407, 300, 311, 983, 321, 362, 281, 360, 527, 1300, 43, 52], "temperature": 0.0, "avg_logprob": -0.07028228925622028, "compression_ratio": 1.69140625, "no_speech_prob": 3.0714487365912646e-05}, {"id": 1034, "seek": 461348, "start": 4628.48, "end": 4631.48, "text": " So in Microsoft Excel, ReLU looks like this", "tokens": [407, 294, 8116, 19060, 11, 1300, 43, 52, 1542, 411, 341], "temperature": 0.0, "avg_logprob": -0.07028228925622028, "compression_ratio": 1.69140625, "no_speech_prob": 3.0714487365912646e-05}, {"id": 1035, "seek": 461348, "start": 4631.48, "end": 4635.48, "text": " If the number is less than zero, use zero, otherwise use the number", "tokens": [759, 264, 1230, 307, 1570, 813, 4018, 11, 764, 4018, 11, 5911, 764, 264, 1230], "temperature": 0.0, "avg_logprob": -0.07028228925622028, "compression_ratio": 1.69140625, "no_speech_prob": 3.0714487365912646e-05}, {"id": 1036, "seek": 461348, "start": 4635.48, "end": 4640.48, "text": " So that's how we're going to replace the negatives with zeros", "tokens": [407, 300, 311, 577, 321, 434, 516, 281, 7406, 264, 40019, 365, 35193], "temperature": 0.0, "avg_logprob": -0.07028228925622028, "compression_ratio": 1.69140625, "no_speech_prob": 3.0714487365912646e-05}, {"id": 1037, "seek": 464048, "start": 4640.48, "end": 4647.48, "text": " And then finally, if you remember from our spreadsheet, we have to add them together", "tokens": [400, 550, 2721, 11, 498, 291, 1604, 490, 527, 27733, 11, 321, 362, 281, 909, 552, 1214], "temperature": 0.0, "avg_logprob": -0.0979901460500864, "compression_ratio": 1.598901098901099, "no_speech_prob": 1.496993991167983e-05}, {"id": 1038, "seek": 464048, "start": 4647.48, "end": 4650.48, "text": " So we add the ReLUs together", "tokens": [407, 321, 909, 264, 1300, 43, 29211, 1214], "temperature": 0.0, "avg_logprob": -0.0979901460500864, "compression_ratio": 1.598901098901099, "no_speech_prob": 1.496993991167983e-05}, {"id": 1039, "seek": 464048, "start": 4650.48, "end": 4652.48, "text": " So that's going to be our prediction", "tokens": [407, 300, 311, 516, 281, 312, 527, 17630], "temperature": 0.0, "avg_logprob": -0.0979901460500864, "compression_ratio": 1.598901098901099, "no_speech_prob": 1.496993991167983e-05}, {"id": 1040, "seek": 464048, "start": 4652.48, "end": 4654.48, "text": " And then our loss is the same as the other sheet", "tokens": [400, 550, 527, 4470, 307, 264, 912, 382, 264, 661, 8193], "temperature": 0.0, "avg_logprob": -0.0979901460500864, "compression_ratio": 1.598901098901099, "no_speech_prob": 1.496993991167983e-05}, {"id": 1041, "seek": 464048, "start": 4654.48, "end": 4658.48, "text": " It's just survived minus prediction squared", "tokens": [467, 311, 445, 14433, 3175, 17630, 8889], "temperature": 0.0, "avg_logprob": -0.0979901460500864, "compression_ratio": 1.598901098901099, "no_speech_prob": 1.496993991167983e-05}, {"id": 1042, "seek": 464048, "start": 4658.48, "end": 4662.48, "text": " And let's change that to main", "tokens": [400, 718, 311, 1319, 300, 281, 2135], "temperature": 0.0, "avg_logprob": -0.0979901460500864, "compression_ratio": 1.598901098901099, "no_speech_prob": 1.496993991167983e-05}, {"id": 1043, "seek": 464048, "start": 4664.48, "end": 4667.48, "text": " Not main, average", "tokens": [1726, 2135, 11, 4274], "temperature": 0.0, "avg_logprob": -0.0979901460500864, "compression_ratio": 1.598901098901099, "no_speech_prob": 1.496993991167983e-05}, {"id": 1044, "seek": 466748, "start": 4667.48, "end": 4671.48, "text": " Okay, so let's try solving that", "tokens": [1033, 11, 370, 718, 311, 853, 12606, 300], "temperature": 0.0, "avg_logprob": -0.17607081486628606, "compression_ratio": 1.3717948717948718, "no_speech_prob": 2.930960908997804e-05}, {"id": 1045, "seek": 466748, "start": 4673.48, "end": 4675.48, "text": " Optimize, ah1", "tokens": [35013, 1125, 11, 3716, 16], "temperature": 0.0, "avg_logprob": -0.17607081486628606, "compression_ratio": 1.3717948717948718, "no_speech_prob": 2.930960908997804e-05}, {"id": 1046, "seek": 466748, "start": 4675.48, "end": 4679.48, "text": " And this time we're changing all of those", "tokens": [400, 341, 565, 321, 434, 4473, 439, 295, 729], "temperature": 0.0, "avg_logprob": -0.17607081486628606, "compression_ratio": 1.3717948717948718, "no_speech_prob": 2.930960908997804e-05}, {"id": 1047, "seek": 466748, "start": 4681.48, "end": 4684.48, "text": " So this is using gradient descent", "tokens": [407, 341, 307, 1228, 16235, 23475], "temperature": 0.0, "avg_logprob": -0.17607081486628606, "compression_ratio": 1.3717948717948718, "no_speech_prob": 2.930960908997804e-05}, {"id": 1048, "seek": 466748, "start": 4685.48, "end": 4689.48, "text": " Excel Solve is not the fastest anymore, but it gets the job done", "tokens": [19060, 7026, 303, 307, 406, 264, 14573, 3602, 11, 457, 309, 2170, 264, 1691, 1096], "temperature": 0.0, "avg_logprob": -0.17607081486628606, "compression_ratio": 1.3717948717948718, "no_speech_prob": 2.930960908997804e-05}, {"id": 1049, "seek": 466748, "start": 4689.48, "end": 4691.48, "text": " Okay, let's see how we went", "tokens": [1033, 11, 718, 311, 536, 577, 321, 1437], "temperature": 0.0, "avg_logprob": -0.17607081486628606, "compression_ratio": 1.3717948717948718, "no_speech_prob": 2.930960908997804e-05}, {"id": 1050, "seek": 469148, "start": 4691.48, "end": 4698.48, "text": " So 0.08 for our deep learning model versus 0.1 for our regression", "tokens": [407, 1958, 13, 16133, 337, 527, 2452, 2539, 2316, 5717, 1958, 13, 16, 337, 527, 24590], "temperature": 0.0, "avg_logprob": -0.0951451671366789, "compression_ratio": 1.564516129032258, "no_speech_prob": 6.400995334843174e-05}, {"id": 1051, "seek": 469148, "start": 4698.48, "end": 4699.48, "text": " So it's a bit better", "tokens": [407, 309, 311, 257, 857, 1101], "temperature": 0.0, "avg_logprob": -0.0951451671366789, "compression_ratio": 1.564516129032258, "no_speech_prob": 6.400995334843174e-05}, {"id": 1052, "seek": 469148, "start": 4699.48, "end": 4700.48, "text": " So there you go", "tokens": [407, 456, 291, 352], "temperature": 0.0, "avg_logprob": -0.0951451671366789, "compression_ratio": 1.564516129032258, "no_speech_prob": 6.400995334843174e-05}, {"id": 1053, "seek": 469148, "start": 4700.48, "end": 4704.48, "text": " So we've now created our first deep learning neural network from scratch", "tokens": [407, 321, 600, 586, 2942, 527, 700, 2452, 2539, 18161, 3209, 490, 8459], "temperature": 0.0, "avg_logprob": -0.0951451671366789, "compression_ratio": 1.564516129032258, "no_speech_prob": 6.400995334843174e-05}, {"id": 1054, "seek": 469148, "start": 4704.48, "end": 4710.48, "text": " And we did it in Microsoft Excel, everybody's favorite artificial intelligence tool", "tokens": [400, 321, 630, 309, 294, 8116, 19060, 11, 2201, 311, 2954, 11677, 7599, 2290], "temperature": 0.0, "avg_logprob": -0.0951451671366789, "compression_ratio": 1.564516129032258, "no_speech_prob": 6.400995334843174e-05}, {"id": 1055, "seek": 469148, "start": 4711.48, "end": 4715.48, "text": " So that was a bit slow and painful", "tokens": [407, 300, 390, 257, 857, 2964, 293, 11697], "temperature": 0.0, "avg_logprob": -0.0951451671366789, "compression_ratio": 1.564516129032258, "no_speech_prob": 6.400995334843174e-05}, {"id": 1056, "seek": 469148, "start": 4715.48, "end": 4719.48, "text": " It would be a bit faster and easier if we used matrix multiplication", "tokens": [467, 576, 312, 257, 857, 4663, 293, 3571, 498, 321, 1143, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.0951451671366789, "compression_ratio": 1.564516129032258, "no_speech_prob": 6.400995334843174e-05}, {"id": 1057, "seek": 469148, "start": 4719.48, "end": 4720.48, "text": " So let's finally do that", "tokens": [407, 718, 311, 2721, 360, 300], "temperature": 0.0, "avg_logprob": -0.0951451671366789, "compression_ratio": 1.564516129032258, "no_speech_prob": 6.400995334843174e-05}, {"id": 1058, "seek": 472048, "start": 4720.48, "end": 4725.48, "text": " So this next one is going to be exactly the same as the last one, but with matrix multiplication", "tokens": [407, 341, 958, 472, 307, 516, 281, 312, 2293, 264, 912, 382, 264, 1036, 472, 11, 457, 365, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.1003717064857483, "compression_ratio": 1.7416267942583732, "no_speech_prob": 2.586554182926193e-05}, {"id": 1059, "seek": 472048, "start": 4725.48, "end": 4727.48, "text": " So all that data looks the same", "tokens": [407, 439, 300, 1412, 1542, 264, 912], "temperature": 0.0, "avg_logprob": -0.1003717064857483, "compression_ratio": 1.7416267942583732, "no_speech_prob": 2.586554182926193e-05}, {"id": 1060, "seek": 472048, "start": 4728.48, "end": 4732.48, "text": " You'll notice the key difference now is our parameters have been transposed", "tokens": [509, 603, 3449, 264, 2141, 2649, 586, 307, 527, 9834, 362, 668, 7132, 1744], "temperature": 0.0, "avg_logprob": -0.1003717064857483, "compression_ratio": 1.7416267942583732, "no_speech_prob": 2.586554182926193e-05}, {"id": 1061, "seek": 472048, "start": 4732.48, "end": 4740.48, "text": " So before I had the parameters matching the data in terms of being in columns", "tokens": [407, 949, 286, 632, 264, 9834, 14324, 264, 1412, 294, 2115, 295, 885, 294, 13766], "temperature": 0.0, "avg_logprob": -0.1003717064857483, "compression_ratio": 1.7416267942583732, "no_speech_prob": 2.586554182926193e-05}, {"id": 1062, "seek": 472048, "start": 4741.48, "end": 4747.48, "text": " For matrix multiplication, the expectation is the way matrix multiplication works", "tokens": [1171, 8141, 27290, 11, 264, 14334, 307, 264, 636, 8141, 27290, 1985], "temperature": 0.0, "avg_logprob": -0.1003717064857483, "compression_ratio": 1.7416267942583732, "no_speech_prob": 2.586554182926193e-05}, {"id": 1063, "seek": 474748, "start": 4747.48, "end": 4750.48, "text": " is that you have to transpose this, so it goes", "tokens": [307, 300, 291, 362, 281, 25167, 341, 11, 370, 309, 1709], "temperature": 0.0, "avg_logprob": -0.15458931765713535, "compression_ratio": 1.6730769230769231, "no_speech_prob": 1.5935585906845517e-05}, {"id": 1064, "seek": 474748, "start": 4751.48, "end": 4755.48, "text": " The X and Y is kind of the opposite way around, the rows and columns are the opposite way around", "tokens": [440, 1783, 293, 398, 307, 733, 295, 264, 6182, 636, 926, 11, 264, 13241, 293, 13766, 366, 264, 6182, 636, 926], "temperature": 0.0, "avg_logprob": -0.15458931765713535, "compression_ratio": 1.6730769230769231, "no_speech_prob": 1.5935585906845517e-05}, {"id": 1065, "seek": 474748, "start": 4755.48, "end": 4757.48, "text": " Other than that, it's the same, I've got the same", "tokens": [5358, 813, 300, 11, 309, 311, 264, 912, 11, 286, 600, 658, 264, 912], "temperature": 0.0, "avg_logprob": -0.15458931765713535, "compression_ratio": 1.6730769230769231, "no_speech_prob": 1.5935585906845517e-05}, {"id": 1066, "seek": 474748, "start": 4757.48, "end": 4761.48, "text": " I just copied and pasted the random numbers, so we had exactly the same starting point", "tokens": [286, 445, 25365, 293, 1791, 292, 264, 4974, 3547, 11, 370, 321, 632, 2293, 264, 912, 2891, 935], "temperature": 0.0, "avg_logprob": -0.15458931765713535, "compression_ratio": 1.6730769230769231, "no_speech_prob": 1.5935585906845517e-05}, {"id": 1067, "seek": 474748, "start": 4762.48, "end": 4769.48, "text": " And so now, our entire, this entire thing here is a single function", "tokens": [400, 370, 586, 11, 527, 2302, 11, 341, 2302, 551, 510, 307, 257, 2167, 2445], "temperature": 0.0, "avg_logprob": -0.15458931765713535, "compression_ratio": 1.6730769230769231, "no_speech_prob": 1.5935585906845517e-05}, {"id": 1068, "seek": 476948, "start": 4769.48, "end": 4777.48, "text": " which is matrix multiply all of this by all of this", "tokens": [597, 307, 8141, 12972, 439, 295, 341, 538, 439, 295, 341], "temperature": 0.0, "avg_logprob": -0.14387427435980904, "compression_ratio": 1.35, "no_speech_prob": 1.3006861991016194e-05}, {"id": 1069, "seek": 476948, "start": 4778.48, "end": 4783.48, "text": " And so when I run that, it fills in exactly the same numbers", "tokens": [400, 370, 562, 286, 1190, 300, 11, 309, 22498, 294, 2293, 264, 912, 3547], "temperature": 0.0, "avg_logprob": -0.14387427435980904, "compression_ratio": 1.35, "no_speech_prob": 1.3006861991016194e-05}, {"id": 1070, "seek": 476948, "start": 4784.48, "end": 4786.48, "text": " Make this average", "tokens": [4387, 341, 4274], "temperature": 0.0, "avg_logprob": -0.14387427435980904, "compression_ratio": 1.35, "no_speech_prob": 1.3006861991016194e-05}, {"id": 1071, "seek": 476948, "start": 4788.48, "end": 4791.48, "text": " And so now we can optimize that", "tokens": [400, 370, 586, 321, 393, 19719, 300], "temperature": 0.0, "avg_logprob": -0.14387427435980904, "compression_ratio": 1.35, "no_speech_prob": 1.3006861991016194e-05}, {"id": 1072, "seek": 479148, "start": 4791.48, "end": 4794.48, "text": " So we can do that", "tokens": [407, 321, 393, 360, 300], "temperature": 0.0, "avg_logprob": -0.39762935042381287, "compression_ratio": 1.3142857142857143, "no_speech_prob": 2.247129123134073e-05}, {"id": 1073, "seek": 479148, "start": 4797.48, "end": 4799.48, "text": " Like that, a minimum", "tokens": [1743, 300, 11, 257, 7285], "temperature": 0.0, "avg_logprob": -0.39762935042381287, "compression_ratio": 1.3142857142857143, "no_speech_prob": 2.247129123134073e-05}, {"id": 1074, "seek": 479148, "start": 4800.48, "end": 4803.48, "text": " By changing these", "tokens": [3146, 4473, 613], "temperature": 0.0, "avg_logprob": -0.39762935042381287, "compression_ratio": 1.3142857142857143, "no_speech_prob": 2.247129123134073e-05}, {"id": 1075, "seek": 479148, "start": 4806.48, "end": 4808.48, "text": " Solve", "tokens": [7026, 303], "temperature": 0.0, "avg_logprob": -0.39762935042381287, "compression_ratio": 1.3142857142857143, "no_speech_prob": 2.247129123134073e-05}, {"id": 1076, "seek": 479148, "start": 4809.48, "end": 4811.48, "text": " It should get the same number", "tokens": [467, 820, 483, 264, 912, 1230], "temperature": 0.0, "avg_logprob": -0.39762935042381287, "compression_ratio": 1.3142857142857143, "no_speech_prob": 2.247129123134073e-05}, {"id": 1077, "seek": 479148, "start": 4811.48, "end": 4813.48, "text": " Oh wait, wasn't it?", "tokens": [876, 1699, 11, 2067, 380, 309, 30], "temperature": 0.0, "avg_logprob": -0.39762935042381287, "compression_ratio": 1.3142857142857143, "no_speech_prob": 2.247129123134073e-05}, {"id": 1078, "seek": 479148, "start": 4814.48, "end": 4816.48, "text": " Yep, and we do", "tokens": [7010, 11, 293, 321, 360], "temperature": 0.0, "avg_logprob": -0.39762935042381287, "compression_ratio": 1.3142857142857143, "no_speech_prob": 2.247129123134073e-05}, {"id": 1079, "seek": 479148, "start": 4816.48, "end": 4819.48, "text": " Okay, so that's just another way of doing the same thing", "tokens": [1033, 11, 370, 300, 311, 445, 1071, 636, 295, 884, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.39762935042381287, "compression_ratio": 1.3142857142857143, "no_speech_prob": 2.247129123134073e-05}, {"id": 1080, "seek": 481948, "start": 4819.48, "end": 4826.48, "text": " You can see that matrix multiplication, it takes like a surprisingly long time, at least for me", "tokens": [509, 393, 536, 300, 8141, 27290, 11, 309, 2516, 411, 257, 17600, 938, 565, 11, 412, 1935, 337, 385], "temperature": 0.0, "avg_logprob": -0.1956504793728099, "compression_ratio": 1.5824175824175823, "no_speech_prob": 3.373371509951539e-05}, {"id": 1081, "seek": 481948, "start": 4827.48, "end": 4830.48, "text": " to get an intuitive feel", "tokens": [281, 483, 364, 21769, 841], "temperature": 0.0, "avg_logprob": -0.1956504793728099, "compression_ratio": 1.5824175824175823, "no_speech_prob": 3.373371509951539e-05}, {"id": 1082, "seek": 481948, "start": 4831.48, "end": 4835.48, "text": " For matrix multiplication, it's like a single mathematical operation", "tokens": [1171, 8141, 27290, 11, 309, 311, 411, 257, 2167, 18894, 6916], "temperature": 0.0, "avg_logprob": -0.1956504793728099, "compression_ratio": 1.5824175824175823, "no_speech_prob": 3.373371509951539e-05}, {"id": 1083, "seek": 481948, "start": 4835.48, "end": 4839.48, "text": " So I still find it helpful to kind of remind myself", "tokens": [407, 286, 920, 915, 309, 4961, 281, 733, 295, 4160, 2059], "temperature": 0.0, "avg_logprob": -0.1956504793728099, "compression_ratio": 1.5824175824175823, "no_speech_prob": 3.373371509951539e-05}, {"id": 1084, "seek": 481948, "start": 4839.48, "end": 4842.48, "text": " as just doing these sum products", "tokens": [382, 445, 884, 613, 2408, 3383], "temperature": 0.0, "avg_logprob": -0.1956504793728099, "compression_ratio": 1.5824175824175823, "no_speech_prob": 3.373371509951539e-05}, {"id": 1085, "seek": 481948, "start": 4844.48, "end": 4846.48, "text": " and additions", "tokens": [293, 35113], "temperature": 0.0, "avg_logprob": -0.1956504793728099, "compression_ratio": 1.5824175824175823, "no_speech_prob": 3.373371509951539e-05}, {"id": 1086, "seek": 484648, "start": 4846.48, "end": 4850.48, "text": " Okay, so that is", "tokens": [1033, 11, 370, 300, 307], "temperature": 0.0, "avg_logprob": -0.1305388562819537, "compression_ratio": 1.4246575342465753, "no_speech_prob": 2.9771355912089348e-05}, {"id": 1087, "seek": 484648, "start": 4854.48, "end": 4860.48, "text": " That is a deep learning neural network in Microsoft Excel", "tokens": [663, 307, 257, 2452, 2539, 18161, 3209, 294, 8116, 19060], "temperature": 0.0, "avg_logprob": -0.1305388562819537, "compression_ratio": 1.4246575342465753, "no_speech_prob": 2.9771355912089348e-05}, {"id": 1088, "seek": 484648, "start": 4861.48, "end": 4866.48, "text": " And the Titanic Kaggle competition, by the way", "tokens": [400, 264, 42183, 48751, 22631, 6211, 11, 538, 264, 636], "temperature": 0.0, "avg_logprob": -0.1305388562819537, "compression_ratio": 1.4246575342465753, "no_speech_prob": 2.9771355912089348e-05}, {"id": 1089, "seek": 484648, "start": 4867.48, "end": 4872.48, "text": " is a pretty fun learning competition, if you haven't done much machine learning before", "tokens": [307, 257, 1238, 1019, 2539, 6211, 11, 498, 291, 2378, 380, 1096, 709, 3479, 2539, 949], "temperature": 0.0, "avg_logprob": -0.1305388562819537, "compression_ratio": 1.4246575342465753, "no_speech_prob": 2.9771355912089348e-05}, {"id": 1090, "seek": 487248, "start": 4872.48, "end": 4878.48, "text": " then it's certainly worth trying out just to get the feel for how these all get put together", "tokens": [550, 309, 311, 3297, 3163, 1382, 484, 445, 281, 483, 264, 841, 337, 577, 613, 439, 483, 829, 1214], "temperature": 0.0, "avg_logprob": -0.14313191952912704, "compression_ratio": 1.6242424242424243, "no_speech_prob": 4.264277595211752e-05}, {"id": 1091, "seek": 487248, "start": 4882.48, "end": 4888.48, "text": " So the chapter of the book that this lesson goes with is chapter 4", "tokens": [407, 264, 7187, 295, 264, 1446, 300, 341, 6898, 1709, 365, 307, 7187, 1017], "temperature": 0.0, "avg_logprob": -0.14313191952912704, "compression_ratio": 1.6242424242424243, "no_speech_prob": 4.264277595211752e-05}, {"id": 1092, "seek": 487248, "start": 4888.48, "end": 4895.48, "text": " And chapter 4 of the book is the chapter where we lose the most people", "tokens": [400, 7187, 1017, 295, 264, 1446, 307, 264, 7187, 689, 321, 3624, 264, 881, 561], "temperature": 0.0, "avg_logprob": -0.14313191952912704, "compression_ratio": 1.6242424242424243, "no_speech_prob": 4.264277595211752e-05}, {"id": 1093, "seek": 487248, "start": 4895.48, "end": 4899.48, "text": " because it's, to be honest, it's hard", "tokens": [570, 309, 311, 11, 281, 312, 3245, 11, 309, 311, 1152], "temperature": 0.0, "avg_logprob": -0.14313191952912704, "compression_ratio": 1.6242424242424243, "no_speech_prob": 4.264277595211752e-05}, {"id": 1094, "seek": 489948, "start": 4899.48, "end": 4906.48, "text": " But part of the reason it's hard is I couldn't put this into a book", "tokens": [583, 644, 295, 264, 1778, 309, 311, 1152, 307, 286, 2809, 380, 829, 341, 666, 257, 1446], "temperature": 0.0, "avg_logprob": -0.10894049326578777, "compression_ratio": 1.5343915343915344, "no_speech_prob": 6.70783847454004e-05}, {"id": 1095, "seek": 489948, "start": 4907.48, "end": 4913.48, "text": " So we're teaching it a very different way in the course to what's in the book", "tokens": [407, 321, 434, 4571, 309, 257, 588, 819, 636, 294, 264, 1164, 281, 437, 311, 294, 264, 1446], "temperature": 0.0, "avg_logprob": -0.10894049326578777, "compression_ratio": 1.5343915343915344, "no_speech_prob": 6.70783847454004e-05}, {"id": 1096, "seek": 489948, "start": 4913.48, "end": 4919.48, "text": " And you can use the two together, but if you've tried to read the book and been a bit disheartened", "tokens": [400, 291, 393, 764, 264, 732, 1214, 11, 457, 498, 291, 600, 3031, 281, 1401, 264, 1446, 293, 668, 257, 857, 717, 12864, 5320], "temperature": 0.0, "avg_logprob": -0.10894049326578777, "compression_ratio": 1.5343915343915344, "no_speech_prob": 6.70783847454004e-05}, {"id": 1097, "seek": 489948, "start": 4919.48, "end": 4923.48, "text": " try following through the spreadsheet instead", "tokens": [853, 3480, 807, 264, 27733, 2602], "temperature": 0.0, "avg_logprob": -0.10894049326578777, "compression_ratio": 1.5343915343915344, "no_speech_prob": 6.70783847454004e-05}, {"id": 1098, "seek": 492348, "start": 4923.48, "end": 4930.48, "text": " Maybe try creating, like if you use Numbers or Google Sheets or something, you could try to create your own version of it", "tokens": [2704, 853, 4084, 11, 411, 498, 291, 764, 22592, 1616, 420, 3329, 1240, 1385, 420, 746, 11, 291, 727, 853, 281, 1884, 428, 1065, 3037, 295, 309], "temperature": 0.0, "avg_logprob": -0.16913509368896484, "compression_ratio": 1.5947368421052632, "no_speech_prob": 1.9831750250887126e-05}, {"id": 1099, "seek": 492348, "start": 4930.48, "end": 4933.48, "text": " on whatever spreadsheet platform you prefer", "tokens": [322, 2035, 27733, 3663, 291, 4382], "temperature": 0.0, "avg_logprob": -0.16913509368896484, "compression_ratio": 1.5947368421052632, "no_speech_prob": 1.9831750250887126e-05}, {"id": 1100, "seek": 492348, "start": 4933.48, "end": 4940.48, "text": " Or you could try to do it yourself from scratch in Python, you know, if you want to really test yourself", "tokens": [1610, 291, 727, 853, 281, 360, 309, 1803, 490, 8459, 294, 15329, 11, 291, 458, 11, 498, 291, 528, 281, 534, 1500, 1803], "temperature": 0.0, "avg_logprob": -0.16913509368896484, "compression_ratio": 1.5947368421052632, "no_speech_prob": 1.9831750250887126e-05}, {"id": 1101, "seek": 492348, "start": 4942.48, "end": 4944.48, "text": " So there's some suggestions", "tokens": [407, 456, 311, 512, 13396], "temperature": 0.0, "avg_logprob": -0.16913509368896484, "compression_ratio": 1.5947368421052632, "no_speech_prob": 1.9831750250887126e-05}, {"id": 1102, "seek": 494448, "start": 4944.48, "end": 4956.48, "text": " Okay, question from Victor Guerra", "tokens": [1033, 11, 1168, 490, 15777, 45725], "temperature": 0.0, "avg_logprob": -0.12416145618145283, "compression_ratio": 1.4569536423841059, "no_speech_prob": 2.6685394914238714e-05}, {"id": 1103, "seek": 494448, "start": 4956.48, "end": 4960.48, "text": " In the Excel exercise when Jeremy is doing some feature engineering", "tokens": [682, 264, 19060, 5380, 562, 17809, 307, 884, 512, 4111, 7043], "temperature": 0.0, "avg_logprob": -0.12416145618145283, "compression_ratio": 1.4569536423841059, "no_speech_prob": 2.6685394914238714e-05}, {"id": 1104, "seek": 494448, "start": 4960.48, "end": 4964.48, "text": " he comes up with two new columns, pclass1 and pclass2", "tokens": [415, 1487, 493, 365, 732, 777, 13766, 11, 280, 11665, 16, 293, 280, 11665, 17], "temperature": 0.0, "avg_logprob": -0.12416145618145283, "compression_ratio": 1.4569536423841059, "no_speech_prob": 2.6685394914238714e-05}, {"id": 1105, "seek": 494448, "start": 4964.48, "end": 4966.48, "text": " That is true", "tokens": [663, 307, 2074], "temperature": 0.0, "avg_logprob": -0.12416145618145283, "compression_ratio": 1.4569536423841059, "no_speech_prob": 2.6685394914238714e-05}, {"id": 1106, "seek": 494448, "start": 4966.48, "end": 4970.48, "text": " pclass1 and pclass2", "tokens": [280, 11665, 16, 293, 280, 11665, 17], "temperature": 0.0, "avg_logprob": -0.12416145618145283, "compression_ratio": 1.4569536423841059, "no_speech_prob": 2.6685394914238714e-05}, {"id": 1107, "seek": 494448, "start": 4970.48, "end": 4973.48, "text": " Why is there no pclass3 column?", "tokens": [1545, 307, 456, 572, 280, 11665, 18, 7738, 30], "temperature": 0.0, "avg_logprob": -0.12416145618145283, "compression_ratio": 1.4569536423841059, "no_speech_prob": 2.6685394914238714e-05}, {"id": 1108, "seek": 497348, "start": 4973.48, "end": 4980.48, "text": " Is it because if pclass1 is 0 and pclass2 is 0, then pclass3 must be 1", "tokens": [1119, 309, 570, 498, 280, 11665, 16, 307, 1958, 293, 280, 11665, 17, 307, 1958, 11, 550, 280, 11665, 18, 1633, 312, 502], "temperature": 0.0, "avg_logprob": -0.09497693379720053, "compression_ratio": 1.7296137339055795, "no_speech_prob": 1.805751890060492e-05}, {"id": 1109, "seek": 497348, "start": 4980.48, "end": 4983.48, "text": " So in a way, two columns are enough to encode the input of the original column", "tokens": [407, 294, 257, 636, 11, 732, 13766, 366, 1547, 281, 2058, 1429, 264, 4846, 295, 264, 3380, 7738], "temperature": 0.0, "avg_logprob": -0.09497693379720053, "compression_ratio": 1.7296137339055795, "no_speech_prob": 1.805751890060492e-05}, {"id": 1110, "seek": 497348, "start": 4983.48, "end": 4986.48, "text": " Yes, that's exactly the reason", "tokens": [1079, 11, 300, 311, 2293, 264, 1778], "temperature": 0.0, "avg_logprob": -0.09497693379720053, "compression_ratio": 1.7296137339055795, "no_speech_prob": 1.805751890060492e-05}, {"id": 1111, "seek": 497348, "start": 4986.48, "end": 4994.48, "text": " So there's no need to tell the computer about things it can kind of figure out for itself", "tokens": [407, 456, 311, 572, 643, 281, 980, 264, 3820, 466, 721, 309, 393, 733, 295, 2573, 484, 337, 2564], "temperature": 0.0, "avg_logprob": -0.09497693379720053, "compression_ratio": 1.7296137339055795, "no_speech_prob": 1.805751890060492e-05}, {"id": 1112, "seek": 497348, "start": 4994.48, "end": 4996.48, "text": " So when you create, these are called dummy variables", "tokens": [407, 562, 291, 1884, 11, 613, 366, 1219, 35064, 9102], "temperature": 0.0, "avg_logprob": -0.09497693379720053, "compression_ratio": 1.7296137339055795, "no_speech_prob": 1.805751890060492e-05}, {"id": 1113, "seek": 497348, "start": 4996.48, "end": 5002.48, "text": " So when you create dummy variables for a categorical variable with three levels", "tokens": [407, 562, 291, 1884, 35064, 9102, 337, 257, 19250, 804, 7006, 365, 1045, 4358], "temperature": 0.0, "avg_logprob": -0.09497693379720053, "compression_ratio": 1.7296137339055795, "no_speech_prob": 1.805751890060492e-05}, {"id": 1114, "seek": 500248, "start": 5002.48, "end": 5005.48, "text": " like this one, you need two dummy variables", "tokens": [411, 341, 472, 11, 291, 643, 732, 35064, 9102], "temperature": 0.0, "avg_logprob": -0.113149995389192, "compression_ratio": 1.6288209606986899, "no_speech_prob": 1.4509354514302686e-05}, {"id": 1115, "seek": 500248, "start": 5005.48, "end": 5011.48, "text": " So in general, a categorical variable with n levels needs n-1 columns", "tokens": [407, 294, 2674, 11, 257, 19250, 804, 7006, 365, 297, 4358, 2203, 297, 12, 16, 13766], "temperature": 0.0, "avg_logprob": -0.113149995389192, "compression_ratio": 1.6288209606986899, "no_speech_prob": 1.4509354514302686e-05}, {"id": 1116, "seek": 500248, "start": 5011.48, "end": 5013.48, "text": " Thanks for the good question", "tokens": [2561, 337, 264, 665, 1168], "temperature": 0.0, "avg_logprob": -0.113149995389192, "compression_ratio": 1.6288209606986899, "no_speech_prob": 1.4509354514302686e-05}, {"id": 1117, "seek": 500248, "start": 5016.48, "end": 5022.48, "text": " So what we're going to be doing in our next lesson is looking at natural language processing", "tokens": [407, 437, 321, 434, 516, 281, 312, 884, 294, 527, 958, 6898, 307, 1237, 412, 3303, 2856, 9007], "temperature": 0.0, "avg_logprob": -0.113149995389192, "compression_ratio": 1.6288209606986899, "no_speech_prob": 1.4509354514302686e-05}, {"id": 1118, "seek": 500248, "start": 5022.48, "end": 5025.48, "text": " So far we've looked at some computer vision", "tokens": [407, 1400, 321, 600, 2956, 412, 512, 3820, 5201], "temperature": 0.0, "avg_logprob": -0.113149995389192, "compression_ratio": 1.6288209606986899, "no_speech_prob": 1.4509354514302686e-05}, {"id": 1119, "seek": 500248, "start": 5025.48, "end": 5030.48, "text": " and just now we've looked at some what we call tabular data, so kind of spreadsheet type data", "tokens": [293, 445, 586, 321, 600, 2956, 412, 512, 437, 321, 818, 4421, 1040, 1412, 11, 370, 733, 295, 27733, 2010, 1412], "temperature": 0.0, "avg_logprob": -0.113149995389192, "compression_ratio": 1.6288209606986899, "no_speech_prob": 1.4509354514302686e-05}, {"id": 1120, "seek": 503048, "start": 5030.48, "end": 5033.48, "text": " Next up, we're going to be looking at natural language processing", "tokens": [3087, 493, 11, 321, 434, 516, 281, 312, 1237, 412, 3303, 2856, 9007], "temperature": 0.0, "avg_logprob": -0.09495188713073731, "compression_ratio": 1.7268722466960353, "no_speech_prob": 9.607572428649291e-05}, {"id": 1121, "seek": 503048, "start": 5033.48, "end": 5035.48, "text": " So I'll give you a taste of it", "tokens": [407, 286, 603, 976, 291, 257, 3939, 295, 309], "temperature": 0.0, "avg_logprob": -0.09495188713073731, "compression_ratio": 1.7268722466960353, "no_speech_prob": 9.607572428649291e-05}, {"id": 1122, "seek": 503048, "start": 5035.48, "end": 5041.48, "text": " So you might want to open up the Getting Started with NLP for Absolute Beginners notebook", "tokens": [407, 291, 1062, 528, 281, 1269, 493, 264, 13674, 39715, 365, 426, 45196, 337, 43965, 1169, 20660, 2999, 21060], "temperature": 0.0, "avg_logprob": -0.09495188713073731, "compression_ratio": 1.7268722466960353, "no_speech_prob": 9.607572428649291e-05}, {"id": 1123, "seek": 503048, "start": 5046.48, "end": 5050.48, "text": " So here's the Getting Started with NLP for Absolute Beginners notebook", "tokens": [407, 510, 311, 264, 13674, 39715, 365, 426, 45196, 337, 43965, 1169, 20660, 2999, 21060], "temperature": 0.0, "avg_logprob": -0.09495188713073731, "compression_ratio": 1.7268722466960353, "no_speech_prob": 9.607572428649291e-05}, {"id": 1124, "seek": 503048, "start": 5050.48, "end": 5055.48, "text": " I will say as a notebook author, I may sound a bit lame", "tokens": [286, 486, 584, 382, 257, 21060, 3793, 11, 286, 815, 1626, 257, 857, 27635], "temperature": 0.0, "avg_logprob": -0.09495188713073731, "compression_ratio": 1.7268722466960353, "no_speech_prob": 9.607572428649291e-05}, {"id": 1125, "seek": 503048, "start": 5055.48, "end": 5059.48, "text": " but I always say when people have uploaded it, it always makes me really happy", "tokens": [457, 286, 1009, 584, 562, 561, 362, 17135, 309, 11, 309, 1009, 1669, 385, 534, 2055], "temperature": 0.0, "avg_logprob": -0.09495188713073731, "compression_ratio": 1.7268722466960353, "no_speech_prob": 9.607572428649291e-05}, {"id": 1126, "seek": 505948, "start": 5059.48, "end": 5061.48, "text": " and it also helps other people find it", "tokens": [293, 309, 611, 3665, 661, 561, 915, 309], "temperature": 0.0, "avg_logprob": -0.09398938374346998, "compression_ratio": 1.6384976525821595, "no_speech_prob": 6.920176383573562e-05}, {"id": 1127, "seek": 505948, "start": 5061.48, "end": 5065.48, "text": " So remember to upvote these notebooks or any other notebooks you like", "tokens": [407, 1604, 281, 493, 85, 1370, 613, 43782, 420, 604, 661, 43782, 291, 411], "temperature": 0.0, "avg_logprob": -0.09398938374346998, "compression_ratio": 1.6384976525821595, "no_speech_prob": 6.920176383573562e-05}, {"id": 1128, "seek": 505948, "start": 5065.48, "end": 5067.48, "text": " I also always read all the comments", "tokens": [286, 611, 1009, 1401, 439, 264, 3053], "temperature": 0.0, "avg_logprob": -0.09398938374346998, "compression_ratio": 1.6384976525821595, "no_speech_prob": 6.920176383573562e-05}, {"id": 1129, "seek": 505948, "start": 5067.48, "end": 5072.48, "text": " So if you want to ask any questions or make any comments, I enjoy those as well", "tokens": [407, 498, 291, 528, 281, 1029, 604, 1651, 420, 652, 604, 3053, 11, 286, 2103, 729, 382, 731], "temperature": 0.0, "avg_logprob": -0.09398938374346998, "compression_ratio": 1.6384976525821595, "no_speech_prob": 6.920176383573562e-05}, {"id": 1130, "seek": 505948, "start": 5077.48, "end": 5085.48, "text": " So natural language processing is about rather than taking, for example, image data and making predictions", "tokens": [407, 3303, 2856, 9007, 307, 466, 2831, 813, 1940, 11, 337, 1365, 11, 3256, 1412, 293, 1455, 21264], "temperature": 0.0, "avg_logprob": -0.09398938374346998, "compression_ratio": 1.6384976525821595, "no_speech_prob": 6.920176383573562e-05}, {"id": 1131, "seek": 505948, "start": 5085.48, "end": 5087.48, "text": " we take text data", "tokens": [321, 747, 2487, 1412], "temperature": 0.0, "avg_logprob": -0.09398938374346998, "compression_ratio": 1.6384976525821595, "no_speech_prob": 6.920176383573562e-05}, {"id": 1132, "seek": 508748, "start": 5087.48, "end": 5094.48, "text": " That text data most of the time is in the form of prose, so plain English text", "tokens": [663, 2487, 1412, 881, 295, 264, 565, 307, 294, 264, 1254, 295, 12505, 11, 370, 11121, 3669, 2487], "temperature": 0.0, "avg_logprob": -0.1403911839360776, "compression_ratio": 1.5491329479768785, "no_speech_prob": 7.719378481851891e-05}, {"id": 1133, "seek": 508748, "start": 5094.48, "end": 5097.48, "text": " So English is the most common language used for NLP", "tokens": [407, 3669, 307, 264, 881, 2689, 2856, 1143, 337, 426, 45196], "temperature": 0.0, "avg_logprob": -0.1403911839360776, "compression_ratio": 1.5491329479768785, "no_speech_prob": 7.719378481851891e-05}, {"id": 1134, "seek": 508748, "start": 5097.48, "end": 5103.48, "text": " but there's NLP models in dozens of different languages nowadays", "tokens": [457, 456, 311, 426, 45196, 5245, 294, 18431, 295, 819, 8650, 13434], "temperature": 0.0, "avg_logprob": -0.1403911839360776, "compression_ratio": 1.5491329479768785, "no_speech_prob": 7.719378481851891e-05}, {"id": 1135, "seek": 508748, "start": 5103.48, "end": 5113.48, "text": " and if you're a non-English speaker, you'll find that for many languages", "tokens": [293, 498, 291, 434, 257, 2107, 12, 31254, 1933, 8145, 11, 291, 603, 915, 300, 337, 867, 8650], "temperature": 0.0, "avg_logprob": -0.1403911839360776, "compression_ratio": 1.5491329479768785, "no_speech_prob": 7.719378481851891e-05}, {"id": 1136, "seek": 511348, "start": 5113.48, "end": 5121.48, "text": " there's less resources in non-English languages and there's a great opportunity to provide NLP resources in your language", "tokens": [456, 311, 1570, 3593, 294, 2107, 12, 31254, 1933, 8650, 293, 456, 311, 257, 869, 2650, 281, 2893, 426, 45196, 3593, 294, 428, 2856], "temperature": 0.0, "avg_logprob": -0.09810218811035157, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.00015340540267061442}, {"id": 1137, "seek": 511348, "start": 5121.48, "end": 5126.48, "text": " This has actually been one of the things that the Fast.ai community has been fantastic at in the global community", "tokens": [639, 575, 767, 668, 472, 295, 264, 721, 300, 264, 15968, 13, 1301, 1768, 575, 668, 5456, 412, 294, 264, 4338, 1768], "temperature": 0.0, "avg_logprob": -0.09810218811035157, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.00015340540267061442}, {"id": 1138, "seek": 511348, "start": 5126.48, "end": 5131.48, "text": " is building NLP resources", "tokens": [307, 2390, 426, 45196, 3593], "temperature": 0.0, "avg_logprob": -0.09810218811035157, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.00015340540267061442}, {"id": 1139, "seek": 511348, "start": 5131.48, "end": 5141.48, "text": " For example, the first Fast.ai NLP resource was created by a student from the very first Fast.ai course", "tokens": [1171, 1365, 11, 264, 700, 15968, 13, 1301, 426, 45196, 7684, 390, 2942, 538, 257, 3107, 490, 264, 588, 700, 15968, 13, 1301, 1164], "temperature": 0.0, "avg_logprob": -0.09810218811035157, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.00015340540267061442}, {"id": 1140, "seek": 514148, "start": 5141.48, "end": 5148.48, "text": " The Indic languages, some of the best resources have come out of Fast.ai alumni and so forth", "tokens": [440, 2333, 299, 8650, 11, 512, 295, 264, 1151, 3593, 362, 808, 484, 295, 15968, 13, 1301, 16347, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.10338350521620884, "compression_ratio": 1.604, "no_speech_prob": 0.00012709260045085102}, {"id": 1141, "seek": 514148, "start": 5148.48, "end": 5151.48, "text": " So that's a particularly valuable thing you could look at", "tokens": [407, 300, 311, 257, 4098, 8263, 551, 291, 727, 574, 412], "temperature": 0.0, "avg_logprob": -0.10338350521620884, "compression_ratio": 1.604, "no_speech_prob": 0.00012709260045085102}, {"id": 1142, "seek": 514148, "start": 5151.48, "end": 5157.48, "text": " So if your language is not well represented, that's an opportunity, not a problem", "tokens": [407, 498, 428, 2856, 307, 406, 731, 10379, 11, 300, 311, 364, 2650, 11, 406, 257, 1154], "temperature": 0.0, "avg_logprob": -0.10338350521620884, "compression_ratio": 1.604, "no_speech_prob": 0.00012709260045085102}, {"id": 1143, "seek": 514148, "start": 5157.48, "end": 5161.48, "text": " So some examples of things you could use NLP for", "tokens": [407, 512, 5110, 295, 721, 291, 727, 764, 426, 45196, 337], "temperature": 0.0, "avg_logprob": -0.10338350521620884, "compression_ratio": 1.604, "no_speech_prob": 0.00012709260045085102}, {"id": 1144, "seek": 514148, "start": 5161.48, "end": 5166.48, "text": " Perhaps the most common and practically useful in my opinion is classification", "tokens": [10517, 264, 881, 2689, 293, 15667, 4420, 294, 452, 4800, 307, 21538], "temperature": 0.0, "avg_logprob": -0.10338350521620884, "compression_ratio": 1.604, "no_speech_prob": 0.00012709260045085102}, {"id": 1145, "seek": 514148, "start": 5166.48, "end": 5168.48, "text": " Classification means you take a document", "tokens": [9471, 3774, 1355, 291, 747, 257, 4166], "temperature": 0.0, "avg_logprob": -0.10338350521620884, "compression_ratio": 1.604, "no_speech_prob": 0.00012709260045085102}, {"id": 1146, "seek": 516848, "start": 5168.48, "end": 5174.48, "text": " Now when I say a document, that could be one or two words, it could be a book, it could be a Wikipedia page", "tokens": [823, 562, 286, 584, 257, 4166, 11, 300, 727, 312, 472, 420, 732, 2283, 11, 309, 727, 312, 257, 1446, 11, 309, 727, 312, 257, 28999, 3028], "temperature": 0.0, "avg_logprob": -0.10496869454017052, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.00018217097385786474}, {"id": 1147, "seek": 516848, "start": 5174.48, "end": 5175.48, "text": " So it could be any length", "tokens": [407, 309, 727, 312, 604, 4641], "temperature": 0.0, "avg_logprob": -0.10496869454017052, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.00018217097385786474}, {"id": 1148, "seek": 516848, "start": 5175.48, "end": 5181.48, "text": " We use the word document, it sounds like that's a specific kind of length, but it can be a very short thing or a very long thing", "tokens": [492, 764, 264, 1349, 4166, 11, 309, 3263, 411, 300, 311, 257, 2685, 733, 295, 4641, 11, 457, 309, 393, 312, 257, 588, 2099, 551, 420, 257, 588, 938, 551], "temperature": 0.0, "avg_logprob": -0.10496869454017052, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.00018217097385786474}, {"id": 1149, "seek": 516848, "start": 5181.48, "end": 5185.48, "text": " We take a document and we try to figure out a category for it", "tokens": [492, 747, 257, 4166, 293, 321, 853, 281, 2573, 484, 257, 7719, 337, 309], "temperature": 0.0, "avg_logprob": -0.10496869454017052, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.00018217097385786474}, {"id": 1150, "seek": 516848, "start": 5185.48, "end": 5188.48, "text": " Now that can cover many, many different kinds of applications", "tokens": [823, 300, 393, 2060, 867, 11, 867, 819, 3685, 295, 5821], "temperature": 0.0, "avg_logprob": -0.10496869454017052, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.00018217097385786474}, {"id": 1151, "seek": 516848, "start": 5188.48, "end": 5192.48, "text": " So one common one that we'll look at a bit is sentiment analysis", "tokens": [407, 472, 2689, 472, 300, 321, 603, 574, 412, 257, 857, 307, 16149, 5215], "temperature": 0.0, "avg_logprob": -0.10496869454017052, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.00018217097385786474}, {"id": 1152, "seek": 516848, "start": 5192.48, "end": 5195.48, "text": " So for example, is this movie review positive or negative?", "tokens": [407, 337, 1365, 11, 307, 341, 3169, 3131, 3353, 420, 3671, 30], "temperature": 0.0, "avg_logprob": -0.10496869454017052, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.00018217097385786474}, {"id": 1153, "seek": 519548, "start": 5195.48, "end": 5200.48, "text": " Sentiment analysis is very helpful in things like marketing and product development", "tokens": [23652, 2328, 5215, 307, 588, 4961, 294, 721, 411, 6370, 293, 1674, 3250], "temperature": 0.0, "avg_logprob": -0.10048390256947484, "compression_ratio": 1.718849840255591, "no_speech_prob": 2.9768103559035808e-05}, {"id": 1154, "seek": 519548, "start": 5200.48, "end": 5206.48, "text": " You know, in big companies there's lots and lots of information coming in about your product", "tokens": [509, 458, 11, 294, 955, 3431, 456, 311, 3195, 293, 3195, 295, 1589, 1348, 294, 466, 428, 1674], "temperature": 0.0, "avg_logprob": -0.10048390256947484, "compression_ratio": 1.718849840255591, "no_speech_prob": 2.9768103559035808e-05}, {"id": 1155, "seek": 519548, "start": 5206.48, "end": 5210.48, "text": " It's very nice to be able to quickly sort it out and track metrics from week to week", "tokens": [467, 311, 588, 1481, 281, 312, 1075, 281, 2661, 1333, 309, 484, 293, 2837, 16367, 490, 1243, 281, 1243], "temperature": 0.0, "avg_logprob": -0.10048390256947484, "compression_ratio": 1.718849840255591, "no_speech_prob": 2.9768103559035808e-05}, {"id": 1156, "seek": 519548, "start": 5210.48, "end": 5216.48, "text": " Something like figuring out what author wrote the document would be an example of a classification exercise", "tokens": [6595, 411, 15213, 484, 437, 3793, 4114, 264, 4166, 576, 312, 364, 1365, 295, 257, 21538, 5380], "temperature": 0.0, "avg_logprob": -0.10048390256947484, "compression_ratio": 1.718849840255591, "no_speech_prob": 2.9768103559035808e-05}, {"id": 1157, "seek": 519548, "start": 5216.48, "end": 5220.48, "text": " Because you're trying to put it in a category, in this case is which author", "tokens": [1436, 291, 434, 1382, 281, 829, 309, 294, 257, 7719, 11, 294, 341, 1389, 307, 597, 3793], "temperature": 0.0, "avg_logprob": -0.10048390256947484, "compression_ratio": 1.718849840255591, "no_speech_prob": 2.9768103559035808e-05}, {"id": 1158, "seek": 519548, "start": 5220.48, "end": 5224.48, "text": " It gives a lot of opportunity in legal discovery, there's already some products in this area", "tokens": [467, 2709, 257, 688, 295, 2650, 294, 5089, 12114, 11, 456, 311, 1217, 512, 3383, 294, 341, 1859], "temperature": 0.0, "avg_logprob": -0.10048390256947484, "compression_ratio": 1.718849840255591, "no_speech_prob": 2.9768103559035808e-05}, {"id": 1159, "seek": 522448, "start": 5224.48, "end": 5233.48, "text": " Where in this case the category is this legal document in scope or out of scope in the court case", "tokens": [2305, 294, 341, 1389, 264, 7719, 307, 341, 5089, 4166, 294, 11923, 420, 484, 295, 11923, 294, 264, 4753, 1389], "temperature": 0.0, "avg_logprob": -0.13433604453926656, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.0239692932809703e-05}, {"id": 1160, "seek": 522448, "start": 5233.48, "end": 5238.48, "text": " Just organizing documents, triaging inbound emails", "tokens": [1449, 17608, 8512, 11, 1376, 3568, 294, 18767, 12524], "temperature": 0.0, "avg_logprob": -0.13433604453926656, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.0239692932809703e-05}, {"id": 1161, "seek": 522448, "start": 5238.48, "end": 5244.48, "text": " So like which part of the organization should it be sent to, is it urgent or not, stuff like that", "tokens": [407, 411, 597, 644, 295, 264, 4475, 820, 309, 312, 2279, 281, 11, 307, 309, 19022, 420, 406, 11, 1507, 411, 300], "temperature": 0.0, "avg_logprob": -0.13433604453926656, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.0239692932809703e-05}, {"id": 1162, "seek": 522448, "start": 5244.48, "end": 5248.48, "text": " So these are examples of classification", "tokens": [407, 613, 366, 5110, 295, 21538], "temperature": 0.0, "avg_logprob": -0.13433604453926656, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.0239692932809703e-05}, {"id": 1163, "seek": 524848, "start": 5248.48, "end": 5255.48, "text": " What you'll find is when we look at classification tasks in NLP", "tokens": [708, 291, 603, 915, 307, 562, 321, 574, 412, 21538, 9608, 294, 426, 45196], "temperature": 0.0, "avg_logprob": -0.06633691417360768, "compression_ratio": 1.6639676113360324, "no_speech_prob": 3.4802120353560895e-05}, {"id": 1164, "seek": 524848, "start": 5255.48, "end": 5260.48, "text": " It's going to look very, very similar to images", "tokens": [467, 311, 516, 281, 574, 588, 11, 588, 2531, 281, 5267], "temperature": 0.0, "avg_logprob": -0.06633691417360768, "compression_ratio": 1.6639676113360324, "no_speech_prob": 3.4802120353560895e-05}, {"id": 1165, "seek": 524848, "start": 5260.48, "end": 5264.48, "text": " But what we're going to do is we're going to use a different library", "tokens": [583, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 764, 257, 819, 6405], "temperature": 0.0, "avg_logprob": -0.06633691417360768, "compression_ratio": 1.6639676113360324, "no_speech_prob": 3.4802120353560895e-05}, {"id": 1166, "seek": 524848, "start": 5264.48, "end": 5269.48, "text": " The library we're going to use is called Hugging Face Transformers rather than Fast AI", "tokens": [440, 6405, 321, 434, 516, 281, 764, 307, 1219, 46892, 3249, 4047, 27938, 433, 2831, 813, 15968, 7318], "temperature": 0.0, "avg_logprob": -0.06633691417360768, "compression_ratio": 1.6639676113360324, "no_speech_prob": 3.4802120353560895e-05}, {"id": 1167, "seek": 524848, "start": 5269.48, "end": 5271.48, "text": " And there's two reasons for that", "tokens": [400, 456, 311, 732, 4112, 337, 300], "temperature": 0.0, "avg_logprob": -0.06633691417360768, "compression_ratio": 1.6639676113360324, "no_speech_prob": 3.4802120353560895e-05}, {"id": 1168, "seek": 524848, "start": 5271.48, "end": 5277.48, "text": " The main reason why is because I think it's really helpful to see how things are done in more than one library", "tokens": [440, 2135, 1778, 983, 307, 570, 286, 519, 309, 311, 534, 4961, 281, 536, 577, 721, 366, 1096, 294, 544, 813, 472, 6405], "temperature": 0.0, "avg_logprob": -0.06633691417360768, "compression_ratio": 1.6639676113360324, "no_speech_prob": 3.4802120353560895e-05}, {"id": 1169, "seek": 527748, "start": 5277.48, "end": 5283.48, "text": " And Hugging Face Transformers, Fast AI has a very layered architecture", "tokens": [400, 46892, 3249, 4047, 27938, 433, 11, 15968, 7318, 575, 257, 588, 34666, 9482], "temperature": 0.0, "avg_logprob": -0.09013758065565577, "compression_ratio": 1.7357723577235773, "no_speech_prob": 4.6828045014990494e-05}, {"id": 1170, "seek": 527748, "start": 5283.48, "end": 5286.48, "text": " So you can do things at a very high level with very little code", "tokens": [407, 291, 393, 360, 721, 412, 257, 588, 1090, 1496, 365, 588, 707, 3089], "temperature": 0.0, "avg_logprob": -0.09013758065565577, "compression_ratio": 1.7357723577235773, "no_speech_prob": 4.6828045014990494e-05}, {"id": 1171, "seek": 527748, "start": 5286.48, "end": 5290.48, "text": " Or you can dig deeper and deeper and deeper, getting more and more fine-grained", "tokens": [1610, 291, 393, 2528, 7731, 293, 7731, 293, 7731, 11, 1242, 544, 293, 544, 2489, 12, 20735, 2001], "temperature": 0.0, "avg_logprob": -0.09013758065565577, "compression_ratio": 1.7357723577235773, "no_speech_prob": 4.6828045014990494e-05}, {"id": 1172, "seek": 527748, "start": 5290.48, "end": 5297.48, "text": " Hugging Face Transformers doesn't have the same high-level API at all that Fast AI has", "tokens": [46892, 3249, 4047, 27938, 433, 1177, 380, 362, 264, 912, 1090, 12, 12418, 9362, 412, 439, 300, 15968, 7318, 575], "temperature": 0.0, "avg_logprob": -0.09013758065565577, "compression_ratio": 1.7357723577235773, "no_speech_prob": 4.6828045014990494e-05}, {"id": 1173, "seek": 527748, "start": 5297.48, "end": 5299.48, "text": " So you have to do more stuff manually", "tokens": [407, 291, 362, 281, 360, 544, 1507, 16945], "temperature": 0.0, "avg_logprob": -0.09013758065565577, "compression_ratio": 1.7357723577235773, "no_speech_prob": 4.6828045014990494e-05}, {"id": 1174, "seek": 527748, "start": 5299.48, "end": 5304.48, "text": " And so at this point of the course, we're going to actually intentionally use a library", "tokens": [400, 370, 412, 341, 935, 295, 264, 1164, 11, 321, 434, 516, 281, 767, 22062, 764, 257, 6405], "temperature": 0.0, "avg_logprob": -0.09013758065565577, "compression_ratio": 1.7357723577235773, "no_speech_prob": 4.6828045014990494e-05}, {"id": 1175, "seek": 530448, "start": 5304.48, "end": 5311.48, "text": " Which is a little bit less user-friendly in order to see what extra steps you have to go through to use other libraries", "tokens": [3013, 307, 257, 707, 857, 1570, 4195, 12, 22864, 294, 1668, 281, 536, 437, 2857, 4439, 291, 362, 281, 352, 807, 281, 764, 661, 15148], "temperature": 0.0, "avg_logprob": -0.06850603648594447, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.3549722754978575e-05}, {"id": 1176, "seek": 530448, "start": 5311.48, "end": 5318.48, "text": " Having said that, the reason I picked this particular library is it is particularly good", "tokens": [10222, 848, 300, 11, 264, 1778, 286, 6183, 341, 1729, 6405, 307, 309, 307, 4098, 665], "temperature": 0.0, "avg_logprob": -0.06850603648594447, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.3549722754978575e-05}, {"id": 1177, "seek": 530448, "start": 5318.48, "end": 5321.48, "text": " It has really good models in it", "tokens": [467, 575, 534, 665, 5245, 294, 309], "temperature": 0.0, "avg_logprob": -0.06850603648594447, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.3549722754978575e-05}, {"id": 1178, "seek": 530448, "start": 5321.48, "end": 5324.48, "text": " It has a lot of really good techniques in it", "tokens": [467, 575, 257, 688, 295, 534, 665, 7512, 294, 309], "temperature": 0.0, "avg_logprob": -0.06850603648594447, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.3549722754978575e-05}, {"id": 1179, "seek": 530448, "start": 5324.48, "end": 5328.48, "text": " Not at all surprising because they have hired lots and lots of Fast AI alumni", "tokens": [1726, 412, 439, 8830, 570, 436, 362, 13144, 3195, 293, 3195, 295, 15968, 7318, 16347], "temperature": 0.0, "avg_logprob": -0.06850603648594447, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.3549722754978575e-05}, {"id": 1180, "seek": 530448, "start": 5328.48, "end": 5332.48, "text": " So they have very high-quality people working on it", "tokens": [407, 436, 362, 588, 1090, 12, 11286, 561, 1364, 322, 309], "temperature": 0.0, "avg_logprob": -0.06850603648594447, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.3549722754978575e-05}, {"id": 1181, "seek": 533248, "start": 5332.48, "end": 5343.48, "text": " So before the next lesson, if you've got time, take a look at this notebook and take a look at the data", "tokens": [407, 949, 264, 958, 6898, 11, 498, 291, 600, 658, 565, 11, 747, 257, 574, 412, 341, 21060, 293, 747, 257, 574, 412, 264, 1412], "temperature": 0.0, "avg_logprob": -0.10242481700709609, "compression_ratio": 1.5192307692307692, "no_speech_prob": 1.7776495951693505e-05}, {"id": 1182, "seek": 533248, "start": 5343.48, "end": 5348.48, "text": " The data we're going to be working with is quite interesting", "tokens": [440, 1412, 321, 434, 516, 281, 312, 1364, 365, 307, 1596, 1880], "temperature": 0.0, "avg_logprob": -0.10242481700709609, "compression_ratio": 1.5192307692307692, "no_speech_prob": 1.7776495951693505e-05}, {"id": 1183, "seek": 533248, "start": 5348.48, "end": 5355.48, "text": " It's from a Kaggle competition, which is trying to figure out in patents", "tokens": [467, 311, 490, 257, 48751, 22631, 6211, 11, 597, 307, 1382, 281, 2573, 484, 294, 38142], "temperature": 0.0, "avg_logprob": -0.10242481700709609, "compression_ratio": 1.5192307692307692, "no_speech_prob": 1.7776495951693505e-05}, {"id": 1184, "seek": 535548, "start": 5355.48, "end": 5363.48, "text": " Whether two concepts are referring to the same thing or not, whether those concepts are represented as English text", "tokens": [8503, 732, 10392, 366, 13761, 281, 264, 912, 551, 420, 406, 11, 1968, 729, 10392, 366, 10379, 382, 3669, 2487], "temperature": 0.0, "avg_logprob": -0.10107935391939603, "compression_ratio": 1.6214689265536724, "no_speech_prob": 2.354853495489806e-05}, {"id": 1185, "seek": 535548, "start": 5363.48, "end": 5366.48, "text": " And when you think about it, that is a classification task", "tokens": [400, 562, 291, 519, 466, 309, 11, 300, 307, 257, 21538, 5633], "temperature": 0.0, "avg_logprob": -0.10107935391939603, "compression_ratio": 1.6214689265536724, "no_speech_prob": 2.354853495489806e-05}, {"id": 1186, "seek": 535548, "start": 5366.48, "end": 5373.48, "text": " Because the document is basically text one, blah, text two, blah", "tokens": [1436, 264, 4166, 307, 1936, 2487, 472, 11, 12288, 11, 2487, 732, 11, 12288], "temperature": 0.0, "avg_logprob": -0.10107935391939603, "compression_ratio": 1.6214689265536724, "no_speech_prob": 2.354853495489806e-05}, {"id": 1187, "seek": 535548, "start": 5373.48, "end": 5377.48, "text": " And then the category is similar or not similar", "tokens": [400, 550, 264, 7719, 307, 2531, 420, 406, 2531], "temperature": 0.0, "avg_logprob": -0.10107935391939603, "compression_ratio": 1.6214689265536724, "no_speech_prob": 2.354853495489806e-05}, {"id": 1188, "seek": 537748, "start": 5377.48, "end": 5385.48, "text": " And in fact, in this case, they actually have scores, it's either got to be basically 0, 0.25, 0.5, 0.75 or 1", "tokens": [400, 294, 1186, 11, 294, 341, 1389, 11, 436, 767, 362, 13444, 11, 309, 311, 2139, 658, 281, 312, 1936, 1958, 11, 1958, 13, 6074, 11, 1958, 13, 20, 11, 1958, 13, 11901, 420, 502], "temperature": 0.0, "avg_logprob": -0.12612959768919818, "compression_ratio": 1.6046511627906976, "no_speech_prob": 4.132995309191756e-05}, {"id": 1189, "seek": 537748, "start": 5385.48, "end": 5391.48, "text": " How similar is it? But it's basically a classification task when you think of it that way", "tokens": [1012, 2531, 307, 309, 30, 583, 309, 311, 1936, 257, 21538, 5633, 562, 291, 519, 295, 309, 300, 636], "temperature": 0.0, "avg_logprob": -0.12612959768919818, "compression_ratio": 1.6046511627906976, "no_speech_prob": 4.132995309191756e-05}, {"id": 1190, "seek": 537748, "start": 5391.48, "end": 5394.48, "text": " So yeah, you can have a look at the data", "tokens": [407, 1338, 11, 291, 393, 362, 257, 574, 412, 264, 1412], "temperature": 0.0, "avg_logprob": -0.12612959768919818, "compression_ratio": 1.6046511627906976, "no_speech_prob": 4.132995309191756e-05}, {"id": 1191, "seek": 537748, "start": 5394.48, "end": 5399.48, "text": " And next week we're going to go step-by-step through this notebook", "tokens": [400, 958, 1243, 321, 434, 516, 281, 352, 1823, 12, 2322, 12, 16792, 807, 341, 21060], "temperature": 0.0, "avg_logprob": -0.12612959768919818, "compression_ratio": 1.6046511627906976, "no_speech_prob": 4.132995309191756e-05}, {"id": 1192, "seek": 537748, "start": 5399.48, "end": 5406.48, "text": " And we're going to take advantage of that as an opportunity also to talk about the really important topics", "tokens": [400, 321, 434, 516, 281, 747, 5002, 295, 300, 382, 364, 2650, 611, 281, 751, 466, 264, 534, 1021, 8378], "temperature": 0.0, "avg_logprob": -0.12612959768919818, "compression_ratio": 1.6046511627906976, "no_speech_prob": 4.132995309191756e-05}, {"id": 1193, "seek": 540648, "start": 5406.48, "end": 5417.48, "text": " Of validation sets and metrics, which are two of the most important topics in not just deep learning, but machine learning more generally", "tokens": [2720, 24071, 6352, 293, 16367, 11, 597, 366, 732, 295, 264, 881, 1021, 8378, 294, 406, 445, 2452, 2539, 11, 457, 3479, 2539, 544, 5101], "temperature": 0.0, "avg_logprob": -0.17488418623458507, "compression_ratio": 1.3426573426573427, "no_speech_prob": 9.303084516432136e-05}, {"id": 1194, "seek": 541748, "start": 5417.48, "end": 5436.48, "text": " Alright, thanks everybody, I'll see you next week, bye", "tokens": [50364, 2798, 11, 3231, 2201, 11, 286, 603, 536, 291, 958, 1243, 11, 6543, 51314], "temperature": 0.0, "avg_logprob": -0.26545894145965576, "compression_ratio": 0.8709677419354839, "no_speech_prob": 0.0003176518075633794}], "language": "en"}