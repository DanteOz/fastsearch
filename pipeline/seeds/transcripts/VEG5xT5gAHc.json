{"text": " Hi everybody and welcome to lesson 7. We're going to start by having a look at a kind of regularization called weight decay and the issue that we came to at the end of the last lesson is that we were training our simple dot product model with bias and our loss data going down and then it started going up again and so we have a problem that we are overfitting and remember in this case we're using mean squared error so try to recall why it is that we don't need a metric here because mean squared error is pretty much the thing we care about really or we could use mean absolute error if we like but either of those works fine as a loss function they don't have the problem of big flat areas like accuracy does for classification. So what we want to do is to make it less likely that we're going to overfit by doing something we call reducing the capacity of the model. The capacity of the model is basically how much space does it have to find answers and if it can kind of find any answer anywhere those answers can include basically memorizing the data set. So one way to handle this would be to decrease the number of latent factors but generally speaking reducing the number of parameters in a model particularly as we look at more deep learning style models ends up biasing the models towards very simple kind of shapes but there's a better way to do it rather than reducing the number of parameters instead we try to force the parameters to be smaller unless they're really required to be big and the way we do that is with weight decay weight decay is also known as L2 regularization they're very slightly different but we can think of them as the same thing and what we do is we change our loss function and specifically we change the loss function by adding to it the sum of all the weights squared in fact all of the parameters squared really should stay. Why do we do that? Well because if that's part of the loss function then one way to decrease the loss would be to decrease the weights one particular weight or all of the weights or something like that and so when we decrease the weights if you think about what that would do and think about for example the different possible values of a in y equals ax squared the larger a is for example a is 50 you get these very narrow peaks in general big coefficients are going to cause big swings big changes in the in the loss a small changes in the parameters and when you have these kind of sharp peaks or valleys it means that a small change through the parameter can make a very small change to the input and make a big change to the loss and so if you have if you're in that situation then you can basically fit all the data points close to exactly with a really complex jagged function with sharp changes which exactly tries to sit on each data point rather than finding a nice smooth surface which connects them all together or goes through them all. So if we limit our weights by adding in to the loss function the sum of the weight squared then what it's going to do is it's going to fit less well on the training set because we're giving it less room to try anything that it wants to but we're going to hope that it would result in a better loss on the validation set or the test set so that it will generalize better. One way to think about this is that the loss with weight decay is just the loss plus the sum of the parameter squared times some number we pick a hyper parameter sometimes it's like 0.1 or 0.01 or 0.001 and if region so this is basically what loss with weight decay looks like in his equation but remember when it actually comes to what's how is the loss used in stochastic gradient descent it's used by taking its gradient. So what's the gradient of this well if you remember back to when you first learned calculus it's okay if you don't the gradient of something squared is just two times that something we've changed from parameters to weight which is a bit confusing so let's use weight here to keep it consistent maybe parameters is better. So the derivative of weight squared is just two times weight so in other words to add in this term to the gradient we can just add to the gradients weight decay times two times weight and since weight decay is just a hyper parameter we can just replace it with weight decay times two so that would just give us weight decay times weight. So weight decay refers to adding on the to the gradients the weights times some hyper parameter and so that is going to try to create these kind of more shallow less bumpy surfaces. So to do that we can simply when we call fit or fit one cycle or whatever we can pass in a WD parameter and that's just this number here. So if we pass in point one then the training loss goes from point two nine to point four nine that's much worse right because we can't overfit anymore but the valid loss goes from point eight nine to point eight two much better. So this is an important thing to remember for those of you that have done a lot of more traditional statistical models is in kind of more traditional statistical models we try to avoid overfitting and we try to increase generalization by decreasing the number of parameters but in a lot of modern machine learning and certainly deep learning we tend to instead use regularization such as weight decay because it gives us more flexibility it lets us use more nonlinear functions and still avoid you know still reduces the capacity of the model. Great so we're down to point eight two three this is a good model this is really actually a very good model and so let's dig into actually what's going on here because in our in our architecture remember we basically just had four embedding layers so what's an embedding layer we've described it conceptually but let's write our own and remember we said that an embedding layer is just a computational shortcut for doing a matrix multiplication by a one-hot encoded matrix and that that is actually the same as just indexing into an array. So an embedding is just a indexing into an array and so it's nice to be able to create our own versions of things that exist in PyTorch and fast AI so let's do that for embedding. So if we're going to create our own kind of layer which is pretty cool we need to be aware of something which is normally a a layer is basically created by inheriting as we've discussed from module or nn.module so for example this is an example here of a module where we've created a class called t that inherits from module and when it's constructed remember that's what dunder in it does we're just going to sit this is just a dummy little module here we're going to set self.a to the number one repeated three times as a tensor. Now if you remember back to notebook four we talked about how the optimizers in PyTorch and fast AI rely on being able to grab the parameters attribute to find a list of all the parameters. Now if you want to be able to optimize self.a you would need to appear in parameters but actually there's nothing there why is that that's because PyTorch does not assume that everything that's in a module is something that needs to be learned to tell it that it's something that needs to be learned you have to wrap it with nn.parameter so here's exactly the same class but torch.ones which is just a list of three three ones in this case is wrapped in nn.parameter and now if I go parameters I see I have a parameter with three ones in it and that's going to automatically call requires grad underscore for us as well we haven't had to do that for things like nn.linear in the past because PyTorch automatically uses nn.parameter internally so if we have a look at the parameters for something that uses nn.linear linear with no bias layer you'll see again we have here a parameter with three things in it. So we want to in general be able to create a parameter so something with a tensor with a bunch of things in and generally we want to randomly initialize them so to randomly initialize we can pass in the size we want we can initialize a tensor of zeros of that size and then randomly generate some normal normally distributed random numbers with a mean of zero and a deviation of.01 no particular reason I'm picking those numbers just know how this works so here's something that will give us back a set of parameters of any size we want and so now we're going to replace everywhere that used to say embedding I'm going to replace it with create params everything else here is the same in the init under init and then the forward is very very similar to before as you can see I'm grabbing the zero index column from X that's my users and I just look it up as you see in that user factors array and the cool thing is I don't have to do anything with gradients myself for this manual embedding layer because PyTorch can figure out the gradients automatically as we've discussed so then I just got the dot product as before add on the bias as before to the sigmoid range as before and so here's a dot product bias without any special PyTorch layers and we fit and we get the same result so I think that is pretty amazingly cool we've really shown that the embedding layer is nothing fancy it's nothing magic right it's just indexing into an array so hopefully that removes a bit of the mystery for you so let's have a look at this model that we've created and we've trained and find out what it's learned that's already useful we've got something we can make pretty accurate predictions with but let's find out what those what the model looks like so remember when we have a question okay let's take a question before you can look at this what's the advantage of creating our own embedding layer over the stock PyTorch one oh nothing at all we're just showing that we can it's it's great to be able to dig under the surface because at some point you'll want to try doing new things so a good way to learn to do new things is to be able to replicate things that already exist and you can expect that you understand how they work it's also a great way to understand the foundations of what's going on is to actually create in code your own implementation but I wouldn't expect you to use this implementation in practice but basically it removes all the mystery so if you remember we've created a learner called learn and to get to the model that's inside it you can always call learn dot model and then inside that there's going to be automatically created for it well sorry not automatically we've created all these attributes movie factors movie bias bias and so forth so we can grab the learned up model dot movie bias and now what I'm going to do is I'm going to sort that vector and I'm going to print out the first five titles and so what this is going to do is it's going to print out though the movies with the smallest bias and here they are what does this mean well that kind of means these are the five movies that people really didn't like but it's more than that it's it's not only do people not like them but if we take account of the genre they're in the actors they have you know whatever the latent factors are people liked them a lot less than they expected so maybe for example people this is kind of I haven't seen any of these movies luckily but perhaps this is a sci-fi movie so people who kind of like these sci-fi movies found they're so bad they still didn't like it so we can do the exact opposite which is to sort sending and here are the top five movies and specifically they're the top five by bias right so these are the movies that even after you take account of the fact that la confidential I have seen all of these ones so la confidential is a kind of a murder mystery cop movie I guess and people who don't necessarily like that genre or I think guy Pierce was in it so maybe they don't like guy Pierce very much whatever people still liked this movie more than they expect so this is a kind of a nice thing that we can look inside our model and see what it's learned we can look at not only at the bias vector but we can also look at the factors now there are 50 factors which is too many to visualize so we could use a technique called PCA principal components now this the details don't matter but basically they're going to squish those 50 factors down to great and then we'll plot the top two as you can see here and what we see when we plot the top two is we can kind of see that the movies have been kind of spread out across a space of of some kind of latent factors and so if you look at the far right there's a whole bunch of kind of big budget actually things and on the far left there's more like cult kind of things Fargo Schindler's with Monty Python by the same token at the bottom we've got some English patient Harry and Harry Met Sally so kind of romance drama kind of stuff and at the top we've got action and sci-fi kind of stuff so you can see even though we haven't asked in any information about these movies all we've seen is who likes what these latent factors have automatically kind of figured out a space or a way of thinking about these movies based on what kinds of movies people like and what are other kinds of movies they like along with those but that's really interesting to kind of try and visualize what's going on inside your model now we don't have to do all this manually we can actually just say give me a collab learner using this set of data loaders with this number of factors and this y-range and it does everything we've just seen again about the same number okay so now you can see this is nice right we've actually been able to see right underneath inside the collab learner part of the fastai application the collaborative filtering application and we can build it all ourselves from scratch we know how to create the SGD know how to create the embedding layer we know how to create the the model the architecture so now you can see you know we've really and build up from scratch our own version of this so if we just type learn dot model you can see here the names are a bit more generic this is a user weight item weight user bias item bias but it's basically the same stuff we've seen before and we can replicate the exact analysis we saw before by using this same idea okay slightly different order this time because it's a bit random but pretty similar as well another interesting thing we can do is we can think about the distance between two movies so let's grab all the movie factors or just pop them into a variable and then let's pick some movie and then let's find the distance from that movie through every other movie and so one way of thinking about distance is you might recall the Pythagorean formula or the distance on a on the hypotenuse of a triangle which is also the distance to a point on it on a Cartesian plane on a chart which is root x squared plus y squared you might know doesn't matter if you don't but you can do exactly the same thing for 50 dimensions it doesn't just work for two dimensions there's a so that tells you how far away a point is from another point if you if x and y are actually differences between two movie vectors so then what gets interesting is you can actually then divide that kind of by the by the length to make all the lengths the same distance to find out how the angle in any two movies and that actually turns out to be a really good way to compare the similarity of two things that's called cosine similarity and so the details don't matter you can look them up if you're interested but the basic idea here is to see that we can actually pick a movie and find the movie that is the most similar to it based on these factors and of interest. I have a question all right what motivated learning at a 50 dimensional embedding and then using a to reduce the three versus just learning a three-dimensional oh because the purpose of this was actually to create a good model so the the visualization part is normally kind of the exploration of what's going in on in your model and so with a 50 with 50 latent factors you're going to get a more accurate so that's one approach is this dot product version there's another version we could use which is we could create a set of user factors and a set of item factors and just like before we could look them up but what we could then do instead of doing a dot product we could concatenate them together to a tensor that contains both the user and the movie factors next to each other and then we could pass them through a simple little neural network linear value linear and then sigmoid range as before so importantly here the first linear layer the number of inputs is equal to the number of user factors plus the number of item factors and the number of outputs is however many activations we have and then which we just default to a hundred here and then the final layer will go from a hundred to one because we're just making one prediction and so we could create or call that collab and then we can instantiate that to create a model we can create a learner and we can fit it's not going quite as well as before it's not terrible but it's not quite as good as our dot product version but the interesting thing here is it does give us some more flexibility which is that since we're not doing a dot product we can actually have a different embedding size for each of users versus items and actually fastai has a simple heuristic if you call get embedding size and pass in your data loaders it will suggest appropriate size embedding matrices for each of your categorical variables each of your user and item sensors so that's so if we pass in star M's settings that's going to pass in the user couple and the item and couple which we can then pass to embedding this is his star prefix we learned about in the last class in case you forgot so this is kind of interesting we can you know we can see here that there's two different architectures we could pick from it wouldn't be necessarily obvious ahead of time which one's going to work better I'm in this particular case the simplest one the dot product one actually turned out to be work a bit better which is interesting this particular version here if you call collab learner and pass use nn equals true then what that's going to do is it's going to use this version the version with the concatenation and the linear layers so collab learner use nn equals true again we get about the same result as you'd expect because it's just a shortcut for this version and it's interesting actually we have a look at collab learner it actually returns an object of type embedding and n and it's kind of cool if you look inside the fastai source code or use the double question mark trick to see the source code for embedding in n you'll see it's three lines of code how does that happen because we're using this thing called tabular model which we will learn about in a moment but basically this neural net version of collaborative filtering is literally just a tabular model in which we pass no continuous variables and some embedding sizes so we'll see that in a moment okay so that is collaborative filtering and again take a look at the further research section in particular after you finish the questionnaire because there's some really important next steps you can take to push your knowledge and your skills. So let's now move to notebook 9, tabular and we're going to look at tabular modeling and do a deep dive and let's start by talking about this idea that we were starting to see here which is embeddings and specifically let's move beyond just having embeddings for users and items but embeddings for any kind of categorical variable but really because we know an embedding is just a lookup into an array it can handle any kind of discrete categorical data so things like age are not discrete they're continuous numerical data but something like sex or postcode categorical variables they have a certain number of discrete levels the number of discrete levels they have is called their cardinality. So to have a look at an example of a data set that contains both categorical and continuous variables we're going to look at the Rossman sales competition that ran on Kaggle a few years ago and so basically what's going to happen is we're going to see a table that contains information about various stores in Germany and the goal will be to try and predict how many sales there's going to be for each day in a couple of week period for each store. One of the interesting things about this competition is that one of the gold medalists used deep learning and it was one of the earliest known examples of a state-of-the-art deep learning tabular model. I mean this is not long ago 2015 or something but really this idea of creating state-of-the-art tabular models with deep learning has not been very common and for not very long. You know interestingly compared to the other gold medalists in this competition the folks that use deep learning used a lot less feature engineering and a lot less domain expertise and so they wrote a paper called Entity Embeddings of Categorical Variables in which they basically described the exact thing that you saw in the in notebook 8 the way you can think of one-hot encodings as just being embeddings you can catenate them together and you can put them through a couple of layers they call them dense layers we've called them linear layers and create a neural network out of that. So this is really a neat you know kind of simple and obvious hindsight trick and they actually did exactly what we did in the paper which is to look at the results of the trained embeddings and so for example they had an embedding matrix for regions in Germany because there was what there wasn't really metadata about this these were just learned embeddings just like we learned embeddings about movies and so then they just created just like we did before a chart where they popped each region according to I think probably a PCA of their embeddings and then if you circle the ones that are close to each other in blue you'll see that they're actually close to each other in Germany and ditto for red and ditto for green and then here's the ground so this is like pretty amazing is the way that we can see that it's kind of a plant something about what Germany looks like based entirely on the purchasing behavior of people in those states. Something else they did was to look at every store and they looked at the distance between stores in practice like how many kilometers away they are and then they looked at the distance between stores in terms of their embedding distance just like we saw in the previous notebook and there was this very strong correlation that stores that were close to each other physically ended up having close embeddings as well even though the actual location of these stores in physical space was not part of the model. Ditto with days of the week so the days of the week or another embedding and the days of the week that were next to each other ended up next to each other in embedding space and ditto for months of the year so pretty fascinating the way kind of information about the world ends up captured just by looking at training embeddings which as we know are just index lookups into an array. So the way we then combine these categorical variables with these embeddings with continuous variables what was done in both the entity embedding paper that we just looked at and then also described in more detail by Google when they described how their recommendation system in Google Play works this is from Google's paper is they have the categorical features that go through the embeddings and then there are continuous features and then all the embedding results and the continuous features are just concatenated together into this big concatenated table that then goes through in this case three layers of a neural net and interestingly they also take the kind of collaborative filtering bit and do the dot product as well and combine the two so they use both of the tricks were used in the previous notebook and combine them together. So that's the basic idea we're going to be seeing for moving beyond just collaborative filtering which is just two categorical variables to as many categorical and as many continuous variables as we like. But before we do that let's take a step back and think about other approaches because as I mentioned the idea of deep learning as a kind of a best practice for tabular data is still pretty new and it's still kind of controversial. But certainly not always the case that it's the best approach so when we're not using deep learning what would we be using. Well what would probably be using is something called an ensemble of decision trees and the two most popular are random forests and gradient boosting machines or something similar. So basically between multi-layered neural networks like with SGD and ensembles of decision trees that kind of covers the vast majority of approaches that you're likely to see for tabular data and so we're going to make sure we cover them both of course today in fact. So although deep learning is nearly always clearly superior for stuff like images and audio and natural language text these two approaches tend to give somewhat similar results a lot of the time for tabular data. Let's take a look some you know you really should generally try both and see which works best for you for each problem you look at. Why does the range go from 0 to 5.5 if the maximum is 5. That's a great question. The reason is if you think about it for sigmoid it's actually impossible for a sigmoid to get all the way to the top or all the way to the bottom. Those are asymptotes so no matter how far how big your x is can never quite get to the top or no matter how small it is it can never quite get to the top. So if you want to be able to actually predict a rating of 5 then you need to use something higher than 5 your maximum. Are embeddings used only for highly cardinal categorical variables or is this approach you general for low cardinality can one use a one-hot encoding. I'll remind you cardinality is the number of discrete levels in a variable and and remember that an embedding is just a computational shortcut for a one-hot encoding. So there's really no reason to use a one-hot encoding because it's as long as you have more than two levels it's always going to be more memory and lower and give you exactly mathematically the same thing and if there's just two levels then it is basically identical so there isn't really any reason not to use it. Thank you for those great questions. Okay so one of the most important things about decision tree ensembles is that at the current state of the technology they do provide faster and easier ways of interpreting the model. I think that's rapidly improving for deep learning models on tabular data but that's where we are right now. They also require less hyper parameter tuning so they're easier to kind of get right the first time. So my first approach for analyzing a new data set is always an ensemble of decision trees and specifically I pretty much always start with a random forest because it's just so reliable. Yes. Your experience for highly imbalanced data such as broad or medical data what usually best out of random forest xgboost or neural network? I'm not sure that the whether the data is balanced or unbalanced is a key reason for choosing one of those about the others. I would try all of them and see which works best. So the exception to the guideline about start with decision tree ensembles is your first thing to try would be if there's some very high cardinality categorical variables then they can be a bit difficult to get to work really well in decision tree ensembles or if there's something like most importantly if it's like plain text data or image data or audio data or something like that then you're definitely going to need to use a neural net in there but you could actually ensemble it with a random forest as we'll see. Okay so really we're going to need to understand how decision tree ensembles work. So pytorch isn't a great choice for decision tree ensembles they're really designed for gradient based methods and random forests and decision tree growing are not really gradient based methods in the same way. So instead we're going to use a library called scikit-learn which is referred to as sklearn as a module. Scikit-learn does a lot of things we're only going to touch on a tiny piece of them stuff we need to do to train decision trees and random forests. We've already mentioned before Wes McKinney's book also a great book for understanding more about scikit-learn. So the data set for learning about decision tree ensembles is going to be another data set it's going to it's called the blue book for bulldozers data set and it's a Kaggle competition. So Kaggle competitions are fantastic they are machine learning competitions where you get interesting data sets you get feedback on whether your approach is any good or not you can see on a leaderboard what approaches are working best and then you can read blog posts from the winning contestants sharing tips and tricks. It's certainly not a substitute for actual practice doing end-to-end data science projects but for becoming good at creating predictive models that are predictive it's a really fantastic resource highly recommended and you can also submit to old most old competitions to see how you would have gone without having to worry about you know the kind of stress of like whether people will be looking at your results because they're not publicized or published if you do that. There's a question, can you comment on real-time applications of random forest? My experience they tend to be too slow for real-time use cases like a recommender system neural network is much faster when run on the right hardware. Let's get to that one thing what they are shall we. Now you can't just download an unta caggle data sets using the unta data thing that we have in fast AI so you actually have to sign up to caggle and then follow these instructions for how to download data from caggle. Make sure you replace creds here with what it describes you need to get a special API code and then run this one time to put that up on your server and now you can use caggle to download data using the API. So after we do that we're going to end up with a bunch of as you see CSV files so let's take a look at this data. So the main data the main table is train.csv remember that's comma separated values and the training set contains information such as unique identifier of a sale, the unique identifier of a machine, the sale price, the sale date. So what's going on here is one row of the data represents a single sale of a single piece of heavy machinery like a bulldozer at an auction. So it happens at a date as a price it's of some particular piece of equipment and so forth. So if we use pandas again to read in the CSV file let's combine training and valid together. We can then look at the columns to see there's a lot of columns there and many things which I don't know what the hell they mean like blade extension and pad type and ride control. But the good news is we're going to show you a way that you don't have to look at every single column and understand what they mean and random forests are going to help us with that as well. So once again we're going to be seeing this idea that models can actually help us with data understanding and data cleanup. One thing we can look at is ordinal columns, a good place to look at that now. If there's things there that you know are discrete values but have some order like product size it has medium and small and large medium and many these should not be in you know alphabetical order or some random order they should be in this specific order right they have a specific ordering. So we can use as type to turn it into a categorical variable and then we can say set categories ordered equals true to basically say this is an ordinal column so it's got discrete values but we actually want to define what the order of the classes are. We need to choose which is the dependent variable and we do that by looking on Kaggle and Kaggle will tell us that the thing we're meant to be predicting is sale price and actually specifically they'll tell us the thing we're meant to be predicting is the log of sale price because root mean squared log error is the is the what we're actually going to be judged on in the competition where we take the log. So we're going to replace sale price with its log and that's what we'll be using from now on. So a decision tree ensemble requires decision trees. So let's start by looking at decision trees. So a decision tree in in this case is a something that asks a series of binary that is yes or no questions about data. So such as is somebody less than greater than less than 30. Yes they are. Are they eating healthily. Yes they are. And so okay then we're going to say they are fit or unfit. So like there's an example of some arbitrary decision tree that somebody might have come up with it's a series of binary yes and no choices and at the bottom are leaf nodes that make some prediction. Now of course for our bulldozers competition we don't know what binary questions to ask about these things and in what order in order to make a prediction about sale price. So we're doing machine learning. So we're going to try and come up with some automated way to create the questions and there's actually a really simple procedure for doing that. You have a think about it. So if you want to kind of stretch yourself here have a think about what's an automatic procedure that you can come up with that would automatically build a decision tree where the final answer would do a you know significantly better than random job of estimating the sale price of one of these options. Right so here's here's the approach that we could use loop through each column of the data set. We're going to go through each of well obviously not sale price that's a dependent variable sale ID machine ID auctioneer year made etc. and so one of those will be for example product size. And so then what we're going to do is we're going to loop through each possible value of product size large large medium medium etc. and then we're going to do a split basically like where this comma is and we're going to say okay let's get all of the options of large equipment and put that into one group and everything that's smaller than that and put that into another group and so that's here split the data into two groups based on whether they're greater than or less than that value. If it's a categorical non-ordinal value a variable it'll be just whether it's equal or not equal to that level and then we're going to find the average sale price for each of the two groups so for the large group what was the average sale price for the smaller than large group what was the average sale price and that will be our model our prediction will simply be the average sale price for that group and so then you can say well how good is that model if our model was just to ask a single question with a yes no answer put things into two groups and take the average of the group as being our prediction and we can say how good would that model be what would be the root means grid error from that model and so we can then say all right how good it would it be if we use large as it and then let's try again what if we did large slash medium is a split what if we did medium is a split and so in each case we can find the root means grid error of that incredibly simple model and then once we've done that for all of the product size levels we can go to the next column and look at usage band and do every level of usage band and then state your level of state and so forth and so there'll be some variable and some split level which gives the best root means grid error of this really really simple model and so then we'll say okay that would be our first binary decision it gives us two groups and then we're going to take each one of those groups separately and create find another single binary decision for each of those two groups using exactly the same procedure so then we'll have four groups and then we'll do exactly the same thing again separately for each of those four groups and so forth so let's see what that looks like and in fact once we've gone through this you might even want to see if you can implement this algorithm yourself it's not trivial but it doesn't require any special coding skills so hopefully you can find you'll be able to do it there's a few things we have to do before we can actually create a decision tree in terms of just some basic data munging one is if we're going to take advantage of dates we actually want to call fastai's add date part function and what that does as you see after we call it as it creates a whole different a bunch of different bits of metadata from that data say all year say your months or weeks or day and so forth so say our date of itself doesn't have a whole lot of information directly but we can pull a lots of different information out of it and so this is an example of something called feature engineering which is where we take some piece some piece of data and we try to grab create lots other lots of other pieces of data from it so is this particular date the end of a month or not at the end of a year or not and so forth but that handles dates there's a bit more cleaning we want to do and fastai provides some things to make meaning easier we can use the tabular pandas class to create a tabular data set pandas and specifically we're going to use two tabular processes or tabular procs a tabular processor is basically just a transform and we've seen transforms before so go back and remind yourself what a transform is except it's just slightly different it's like three lines of code if you look at the code for it it's actually going to modify the object in place rather than creating a new object and giving it back to you and that's because often these tables of data are kind of really big and we don't want to waste lots of RAM and it's just going to run the transform once and save the result rather than doing it lazily when you access it for the same reason we just kind of make this a lot faster so you can just think of them as transforms really so one of them is called categorify and categorify is going to replace a column with numeric categories using the same basic idea of like a vocab like we've seen before fill missing is going to find any columns with missing data it's going to fill in the missing data with the median of the data and create a new column a Boolean column which is set to true for anything that was missing so these two things is basically enough to get you to a point where most of the time you'll be able to train a model now the next thing we need to do is think about our validation set as we discussed in lesson one a random validation set is not always appropriate and certainly for something like predicting auction results it almost certainly is not appropriate because we're going to be wanting to use a model in the future not at some random date in the past so the way this Kaggle competition was set up was that the test set the thing that you had to fill in and submit for the competition was two weeks of data that was after any of the training set so we should do the same thing for a validation set we should create something which is where the validation set is the last couple of weeks of data and and so then the training set will only be data before that so we basically can do that by grabbing everything before October 2011 create a training and validation set based on that condition and grabbing those bits so that's going to split our training set and validation set by date not randomly we're also going to need to tell when you create a tabular pandas object you're going to be passing in a data frame going to be passing in your tabular procs you also have to say what are my categorical and continuous variables we can use fastai's con cat split we automatically split a data frame to continuous and categorical variables for you so we can just pass those in tell it what what is the dependent variable you can have more than one and what are the indexes to split into training and valid and this is a tabular object so it's got all the information you need about the training set the validation set categorical and continuous variables and the dependent variable and any processes to run it looks a lot like a data set subject but has a dot train it has a dot valid and so if we have a look at dot show we can see the the data but dot show is going to show us the kind of the string data but if we look at dot items you can see internally it's actually stored these very compact numbers which we can use directly in a model so fastai has basically got us to a point here where we have our data into a format I'm ready for modeling and validation sets being created to see how these numbers relate to these strings we can again just like we saw last week use the classes attribute which is a dictionary which basically tells us the vocab so this is how we look up for example six is zero one two three four five six this is compact example that processing took takes a little while to run so you can go ahead and save the tabular object and so then you can load it back later without having to rerun all the processing so that's a nice kind of fast way to quickly get back up and running without having to reprocess your data so we've done the basic data managing we need so we can now create a decision tree and in scikit-learn a decision tree where the dependent variable is continuous is a decision tree regressor and let's start by telling it we just want a total of four leaf nodes we'll see what that means in a moment and in scikit-learn you generally call fit so it looks quite a lot like fast AI and you pass in your independent variables and your dependent variable and we can grab those straight from our tabular object training set is dot X's and dot Y and we can do the same thing for validation just to save us in typing okay question do you have any thoughts on what data augmentation for tabular data might look like I don't have a great sense of data augmentation for tabular data we'll be seeing later either in this course or in the next part drop out and mix up and stuff like that which they might be able to do that in later layers in a tabular model otherwise I think you'd need to think about kind of the semantics of the data and think about what are things you could do to change the data without changing the meaning sounds like a pretty tricky question this fast AI distinguish between ordered categories such as low medium high and unordered category variables yes that that was that ordinal thing I told you about before and all it really does is it ensures that your classes list has a specific order so then these numbers actually have a specific and as you'll see that's actually going to turn out to be pretty important for how we train our random forest okay so we can create a decision tree regressor we can fit it and then we can draw it the fast AI function and here is the decision tree we just trained and we and behind the scenes this actually used the basically the exact process that we described back here right so this is where you can like try and create your own decision tree implementation if you're interested in stretching yourself so we're going to use one that's already exists and the best way to understand what it's done is to look at this diagram from top to bottom so the first step is it says like okay the initial model it created is a model with no binary splits at all specifically it's always going to predict the value 10.1 for every single row why is that well because this is the simplest possible model is to take the average of the dependent variable and always predict that and so this is always should be your kind of pretty much your basic baseline for regression there are four hundred and four thousand seven hundred and ten rows auctions that we're averaging and the mean squared error of this incredibly simple model in which there are no rules at all no groups at all just a single average is a point for it so then the next most complex model is to take a single column coupler system and a single binary decision is coupler system less than or equal to point five true there are three hundred and sixty thousand eight hundred and forty seven auctions where it's true and forty-three thousand eight hundred and sixty-three where it's false and now interestingly in the false case you can see that there are no further binary decisions so this is called a leaf node it's a node where this is as far as you can get and so if your coupler system is not less than or equal to point five then the prediction this model makes for your sale price is nine point two one versus if it's true it's ten point two one so you can see it's actually found a very big difference here and that's why it picked this as the first binary split and so the mean squared error for this section here is point one two which is far better than we started out at point four eight this group still has three hundred sixty thousand in it and so it does another binary split this time is the year that this piece of equipment made was it less than or equal to nineteen ninety one and a half if it was if it's true then we get a leaf node and the prediction is nine point nine seven mean squared error point three seven if the value is false we don't have a leaf node and we have another binary split and you can see eventually we get down to here coupler system true year made false product size false mean squared error point one seven so all of these leaf nodes have MSC's that are smaller than that original baseline model of just taking the mean so this is how you can grow a decision tree and we only stopped here because we said max leaf nodes is four one two three four right and so if we want to keep training it further we can just use a higher number there's actually a very nice library by Terrence Park or D tree viz which can show us exactly the same information like so and so here are the same leaf nodes one two three four and you can see the kind of the chart of how many are there this is the split coupler system point five here are the two groups you can see the sale price in each of the two groups and then here's the leaf node and so then the second split was on year made and you can see here something weird's going on with year made there's a whole bunch of year made that are a thousand which is obviously not a sensible year for a bulldozer to be made so presumably that's some kind of missing value so when we look at the kind of the picture like this it can give us some insights about what's going on in our data and so maybe we should replace those thousands with 1950 because that's you know obviously a very very early year for a bulldozer so we can kind of pick it arbitrarily it's actually not really going to make any difference to the model that's created because all we care about is the order because we're just doing these binary splits that will make it easier to look at as you can see here's our 1950s now and so now it's much easier to see what's going on in that binary split so let's now get rid of max leaf nodes and build a bigger decision tree and then let's just for the rest of this notebook create a couple of little functions one to create the root mean squared error which is just here and another one to take a model and some independent independent variables predict from the model on the independent variables and then take the root mean squared error with a dependent variable so that's going to be our models root means squared error so for this decision tree in which we didn't have a stopping criteria so as many leaf nodes as you like the models root mean squared error is zero so we've just built the perfect model so this is great news right we've built the perfect auction trading system well remember we actually need to check the validation set so let's check the check mrmse with the validation set and oh it's worse than zero so our training set is zero our validation set is much worse than zero why has that happened well one of the things that a random forest in sklearn can do is it can tell you the number of leaf nodes number of leaves there are three hundred and forty one thousand number of data points four hundred and fifty thousand so in other words we have nearly as many leaf nodes as data points the most of our leaf nodes only have a single thing in so they're taking an average of a single thing clearly this makes no sense at all so what we should actually do is pick some different stopping criteria and let's say okay if you get a leaf node with 25 things or less in it don't don't split or don't split things to create a leaf node with less than 25 things in it and now if we fit and we look at the root mean squared error for the validation set it's going to go down from point three three to point three two so the training sets got worse from zero to point two four eight the validation sets got better and now we only have 12,000 leaf nodes so that is much more reasonable all right so let's take a five minute break and then we're going to come back and see how we get the best of both worlds how are we going to get something which has the kind of flexibility to get these you know what we're going to get down to zero but to get you know really deep trees but also without overfitting and the trick will be to use something called bagging we'll come back and talk about that in five minutes okay welcome back so we're going to look at how we can get the best of both worlds as we discussed and let's start by having a look at what we're doing with categorical variables festival and so you might notice that previously with categorical variables for example in collaborative filtering we had to you know kind of think about like how many embedding levels we have for example if you've used other modeling tools you might have doing things with creating dummy variables stuff like that random forests on the whole you don't have to the the reason is that as we've seen all of our categorical variables have been turned into numbers and so we can perfectly well have decision tree binary decisions which use those particular numbers now the numbers might not be ordered in any interesting way but if there's a particular level which kind of stands out as being important it only takes two binary splits to split out that level into a single you know into a single piece so generally speaking I don't normally worry too much about kind of encoding categorical variables in a special way as I mentioned I do try to encode ordinal variables by saying what the order of the levels is because often as you would expect sizes for example you know medium and small are going to be kind of next to each other and large and extra large would be next to each other that's good to have those as similar numbers having said that you can kind of one-hot encode a categorical variable if you want to using get dummies in pandas but there's not a lot of evidence that that actually helps there's actually that has been brought in a paper and so I would say in general for categorical variables don't worry about it too much just use what we've shown you have a question for for ordinal categorical variables how do you deal with when they have like an a or missing values where do you put that in the order so in fast AI and a missing values always appear as the first item they'll always be the zero in item and also if you get something in the validation or test set which is a level we haven't seen in training that will be considered to be that missing or in a value as well all right so what we're going to do to try and improve our random forest is we're going to use something called bagging and this was developed by a retired Berkeley professor named Leo Breiman in 1994 and he did a lot of great work and perhaps you could actually that most of it happened after he retired his technical report was called bagging predictors and he described how you could create multiple versions of a predictor so multiple different models and you could then aggregate them by averaging over the predictions and specifically the way he suggested doing this was to create what he called bootstrap replicates in other words randomly select different subsets of your data train a model on that subset and store it away as one of your predictors and then do it again a bunch of times and so each of these models is trained on a different random subset of your data and then you to predict you predict on all of those different versions of your model and average them and it turns out that bagging works really well so this the sequence of steps is basically randomly choose some subset of rows write in a model using that subset save that model and then return to step one do that a few times to train a few models and then to make a prediction predict with all the models and take the average that is bagging and it's very simple but it's astonishingly powerful and the reason why is that each of these models we've trained although they are not using all of the data so they're kind of less accurate than a model that use all of the data each of them is the errors are not correlated you know the errors because of using that smaller subset are not correlated with the errors of the other models because they're random subsets and so when you take the average of a bunch of kind of errors which are not correlated with each other the average of those errors is zero so therefore the average of the models should give us an accurate prediction of the thing we're actually trying to predict so as I say here it's an amazing result we can improve the accuracy of nearly any kind of algorithm by training it multiple times on different random subsets of data and then averaging the predictions so then Breiman in 2001 showed a way to do this specifically for decision trees where not only did he randomly choose a subset of rows for each model but then for each binary split he also randomly selected a subset of columns and this is called the random forest and it's perhaps the most widely used most practically important machine learning method and astonishingly simple to create a random forest regressor you use sklearns random forest regressor if you pass n jobs minus one it will use all of the CPU cause you have to run as fast as possible and estimators says how many trees how many models to train maxsamples says how many rows to use randomly chosen rows to use in each one maxfeatures is how many randomly chosen columns to use for each binary split point minsamplesleaf is the stopping criteria and we'll come back to so here's a little function that will create a random forest regressor and fit it to some set of independent variables and a dependent variable so we can give it a few default values and create a random forest and train and our validation set rmsc is 0.23 if we compare that of what we had before we had 0.32 so dramatically better by using a random forest so so what's happened when we called random forest regressor is it's just using that decision tree builder that we've already seen but it's building multiple versions with these different random subsets and for each binary split it does it's also randomly selecting a subset of columns and then when we create a prediction it is averaging the predictions of each of the trees and as you can see it's giving a really great result and one of the amazing things we'll find is that it's going to be hard for us to improve this very much you know the kind of the default starting point tends to turn out to be pretty great the the sklearn docs have lots of good information in one of the things it has is this nice picture that shows as you increase the number of estimators how does the accuracy improve error rate improves or different max features levels and in general the more trees you add the more accurate your model there it's not going to overfit right because it's it's averaging more of these these weak models more of these models that are trained on subsets of the data so train as many use as many estimators as you like really just a case of how much time do you have and whether you kind of reach a point where it's not really improving anymore you can actually get at the underlying decision trees in a model in a random forest model using estimators underscore so with a list comprehension we can call predict on each individual tree and so here's an array a numpy array containing the predictions from each individual tree for each row in our data so if we take the mean across the zero axis we'll get exactly the same number because remember that's what a random forest does is it takes the mean of the trees predictions so one cool thing we could do is we could look at the 40 estimators we have and grab the predictions or the first I of those trees and take their mean and then we can find the root means grid error and so in other words here is the accuracy when you've just got one tree two trees three trees four trees five trees etc and you can see so it's kind of nice right you can you can actually create your own and build your own tools to look inside these things and see what's going on and so we can see here that as you add more and more trees the accuracy did indeed keep improving or the root means grid error kept improving although it the improvement slowed down after a while the validation set is worse than the training set and there's a couple of reasons that could have happened the first reason could be because we're still overfitting which is not necessarily a problem this is something we could identify or maybe it's because the the fact that we're trying to predict the last two weeks is actually a problem and that the last two weeks are kind of different to the other options in our data set maybe something changed over time so how do we tell which of those two reasons there are what what is the reason that our validation set is worse we can actually find out using a very clever trick called out-of-bag error OOB error and we use OOB error for lots of things you can grab the OOB error or you can grab the OOB predictions from the model with OOB prediction and you can grab the RMSE and you can find that the OOB error RMSE is 0.21 which is quite a bit better than 0.23 so let me explain what OOB error is what OOB error is is we look at each row of the training set not the validation set each row of the training set and we say so fix a we say for row number one which trees included row number one in the training and we'll say okay let's not use those for calculating the error because that was part of those trees training I would just calculate the error for that row using the trees where that row was not included in training that tree because remember every tree is using only a subset of the data so we do that for every row we find the prediction using only the trees that were not used that that that row was not used and those are the OOB predictions but in other words this is like giving us a validation set result and without actually needing a validation but the thing is it's not with that time offset it's not looking at the last two weeks it's looking at the whole training set so this basically tells us how much of the error is due to overfitting versus jury to being the last couple of weeks so that's a cool trick OOB error is something that very quickly kind of gives us a sense of how much we're we're overfitting and we don't even need a validation set to do it but there's that OOB error so that's telling us a bit about what's going on in our model but then there's a lot of things we'd like to find out from our model and I've got five things in particular here which I generally find pretty interesting which is how confident are we about our predictions for some particular prediction we're making like we can say this is what we think the prediction is but how confident are we is that actually that or is just about that or we really have no idea and then for particular for predicting a particular item which factors were the most important in that prediction and how did they influence it overall which columns are making the biggest difference in amputal which ones could we maybe throw away and it wouldn't matter which columns are basically redundant with each other so we don't really need both of them and as we vary some column how does it change the prediction so those are the five things that we're that I'm interested in figuring out and we can do all of those things with a random forest let's start with the first one so the first one we've already seen that we can grab all of the predictions for all of the trees and take their mean to get the actual predictions of the model and then to get the RMSE but what if instead of saying mean we did exactly the same thing like so but instead said standard deviation this is going to tell us for every row in our data set how much did the trees vary and so if our model really had never seen kind of data like this before it was something where you know different trees were giving very different predictions it might give us a sense that maybe this is something that we're not at all confident about and as you can see when we look at the standard deviation of the trees for each prediction let's just look at the first five they vary a lot right point two point one point oh nine point nearly point three okay so this is a really interesting it's not something that a lot of people talk about but I think it's a really interesting approach to kind of figuring out whether we might want to be cautious about a particular prediction because maybe we're not very confident about it but there's one thing we can easily do with a random forest the next thing and this is I think the most important thing for me in terms of interpretation is feature importance here's what feature importance looks like we can call feature importance on a model with some independent variables let's say grab the first 10 this says these are the 10 most important features in this random forest these are the things that are the most strongly driving sale price or we could plot them and so you can see here there's just a few things that are by far the most important what year the equipment was made bulldozer or whatever how big is it a plus system whatever that means and the product class whatever that means and so you can get this by simply looking inside your train model and grabbing the feature importances attribute and so here for making it better to print out I'm just sticking that into a data frame and sorting the sending by importance so how is this actually being done it's it's actually really neat what scikit-learn does and Breiman the inventor of random forest described is that you can go through each tree and then start at the top of the tree and look at each branch and at each branch see what feature was used the split which binary which the binary split was based on which column and then how much better was the model after that split compared to beforehand and we basically then say okay that column was responsible for that amount of improvement and so you add that up across all of the splits across all of the trees for each column and then you normalize it so they all add to one and that's what gives you these numbers which we show the first few of them in this table and the first 30 of them here in this chart so this is something that's fast and it's easy and it kind of gives us a good sense of like well maybe the stuff that are less than 0.005 we could remove so if we did that that would leave us with only 21 columns so let's try that let's just let's just say okay x's which are important the x's which are in this list of ones to keep do the same they're valid retrain our random forest and have a look at the result and basically our accuracy is about the same but we've gone down from 78 columns the 21 column so I think this is really important it's not just about reading the most accurate model you can but you want to kind of be able to fit it in your head as best as possible and so 21 columns is going to be much easier for us to check for any data issues and understand what's going on and the accuracy is about the same of the RMSE so I would say okay let's do that let's just stick with x's important from now on and so here's this entire set of the 21 features and you can see it looks now like year made and product size of the two really important things and then there's a cluster of kind of mainly product related things that are kind of at the next level of importance one of the tricky things here is that we've got like product class desk model ID secondary desk model desk base model they modeled a script so they all look like there might be similar ways of saying the same thing so one thing that can help us to interpret the feature importance better and understand better what's happening the model is to remove redundant features so one way to do that is to call fast AI's cluster columns which is basically a thin wrapper for stuff that scikit-learn already provides and what that's going to do is it's going to find pairs of columns which are very similar you can see here sale year and sale elapsed see how this line is way out to the right or else machine ID and model ID is not at all it's way out to the left so that means that sale year and sale elapsed are very very similar when one is low the other tends to be low and vice versa here's a group of three which all seem to be much the same and then product group desk and product group and then fi best base model and fi model desk but these all seem like things where maybe we could remove one of each of these pairs because they're basically seem to be much the same you know they're you know they're when one is higher the other is high and vice versa so let's try removing one of each of these now it takes a little while to train a random forest and so for the just to see whether removing something makes it much worse we could just do a very fast version so we could just train something where we only have 50,000 rows per tree train for each tree and we'll just use 40 trees and let's then just get the OOB for and so for that fast simple version our basic OOB with our important X's is.877 and here for OOB a higher number is better so then let's try going through each of the things we thought we might not need and try dropping them and then getting the OOB error for our X's with that one column removed and so compared to.877 most of them don't seem to hurt very much they'll elapse hurt quite a bit right so for each of those groups let's go and see which one of the ones seems like we could remove it so here's the five I found let's remove the whole lot and see what happens and so the OOB went from 877 to 874 so hardly any difference at all despite the fact we managed to get rid of five of our variables so let's create something called X's final which is the X's important and then dropping those five save them for later we can always load them back again and then let's check our random forest using those and again.233 or.234 so we've got about the same thing but we've got even less columns now so we're getting a kind of a simpler and simpler model without hurting our accuracy it's great so the next thing we said we were interested in learning about is for the columns that are particularly the columns that are most important how does what's the relationship between that column and the dependent variable so example what's the relationship between product size and sale price so the first thing I would do would be just to look at a histogram so one way to do that is with a value counts in pandas and we can see here our different calls of product size and one thing to note here is actually missing is actually the most common and then next most is compact and small and then many is pretty tiny so we can do the same thing for year made now for year made we can't just see the the basic bar chart like here we recorded a histogram it's not it's a bar chart for year made we actually need a histogram which pandas has stuff like this built in so we can just call histogram and that 1950 you remember we created it that's kind of this missing value thing which used to be a thousand but most of them seem to have been well into the 90s and 2000s so let's now look at something called a partial dependence plot I'll show it to you first here is a partial dependence plot of year made against partial dependence and what does this mean well we should focus on the part where we actually have a reasonable end of data so at least well into the 80s go around here and so let's look at this bit here basically what this says is that as year made increases the predicted sale price log sale price of course also increases you can see and the log sale price is increasing linearly on other roughly so roughly then this is actually an exponential relationship between year made and sale price why do we call it a partial dependence are we just plotting the kind of the year against the average sale price well no we're not we can't do that because a lot of other things change from year to year example maybe more recently people tend to buy bigger bulldozers or bold more bulldozers with air conditioning or more expensive models of bulldozers and we really want to be able to say like no just what's the impact of year and nothing else and if you think about it from a kind of a inflation point of view you would expect that older bulldozers would be kind of that bulldozers would get a kind of a constant ratio cheaper the further you go back which is what we see so what we really want to say is all other things being equal what happens if only the year changes and there's a really cool way we can answer that question with a random forest so how does year made impact sale price all other things being equal so what we can do is we can go into our actual data set and replace every single value in the year made column with 1950 and then can calculate the predicted sale price for every single auction and then take the average over all the auctions and that's what gives us this value here and then we can do the same from 1951 1952 and so forth until eventually we get to our final year of 2011 so this isolates the effect of only year made so it's a kind of a bit of a curious thing to do but it's actually it's a pretty neat trick for trying to kind of pull apart and create this partial dependence to say what might be the impact of just changing year made and we can do the same thing for product size and one of the interesting things if we do it for product size is we see that the lowest value of predicted sale price log sale price is NA which is a bit of a worry because we kind of want to know well that means it's really important the question of whether or not the product size is labeled is really important and that is something that I would want to dig into before I actually use this model to find out well why is it that sometimes things aren't labeled and what does it mean you know why is it that that's actually a such as important predictor so that is the partial dependence plot and it's a really clever trick. So we have looked at four of the five questions we said we wanted to answer at the start of this section so the last one that we want to answer is one here we're predicting with a particular row of data what were the most important factors and how did they influence that predictor this is quite related to the very first thing we saw so it's like imagine you were using this auction price model in real life you had something on your tablet and you went into some auction and you looked up what the predicted auction price would be for this lot that's coming up to find out whether it seems like it's being under or overvalued and then you can decide what to do about that. So one thing we said we'd be interested to know is like well are we actually confident in our prediction and then we might be curious to find out like oh I'm really surprised it was predicting such a high value why was it predicting such a high value so to find the answer to that question we can use a module called tree interpreter and tree interpreter the way it works is that you pass in a single row so it's like here's the auction that's coming up here's the model here's the auctioneer ID etc etc please predict the value from the random forest what's the expected sale price and then what we can do is we can take that one row of data and put it through the first decision tree and we can see what's the first split that's selected and then based on that split does it end up increasing or decreasing the predicted price compared to that kind of raw baseline model of just take the average and then you can do that again at the next split and again at the next split and the next split so for each split we see what the increase or decrease in the addiction that's not right we see what the increase or decrease in the prediction is except while I'm here compared to the parent node and so then you can do that for every tree and then add up the total change in importance by split variable and that allows you to draw something like this so here's something that's looking at one particular row of data and overall we start at zero and so zero is the initial 10.1 do you remember this number 10.1 is the average log sale price of the whole data set they call it the bias re-interpret and so if we call that zero then for this particular row we're looking at year made as a negative 4.2 impact on the prediction and then product size has a positive 0.2 coupler system has a positive 0.046 model ID has a positive 0.127 and so forth right and so the red ones are negative and the green ones are positive and you can see how they all join up until eventually overall the prediction is that it's going to be negative 0.122 compared to 10.1 which is equal to 9.98 so this kind of plot is called a waterfall plot and so basically when we say tree interpreter dot predict it gives us back the prediction which is the actual number we get back from the random forest the bias which is just always this 10.1 for this data set and then the contributions which is all of these different values it's how much how important was each actor and here I've used a threshold which means anything that was less than 0.08 or gets thrown into this other category I think this is a really useful kind of thing to have in production because it can help you answer questions whether it will be for the customer or for you know whoever's using your model if they're surprised about some fiction why is that prediction so I'm going to show you something really interesting using some synthetic data and I want you to really have a think about why this is happening before I tell you and I pause the video if you're watching the video when I get to that point let's start by creating some synthetic data like so so we're going to grab 40 values evenly spaced between 0 and 20 and then we're just going to create the y equals x line and add some normally distributed random jitter on that here's this kind of plot so here's some data we want to try and predict and we're going to use a random forest in a kind of bit of a overkill here now in this case we only have one independent variable scikit-learn expects us to have more than one so we can use unsqueeze in PyTorch to add that go from a shape of 40 in other words a vector with 40 elements for a shape of 40 comma 1 in other words a matrix of 40 rows with one column so this unsqueeze one means add a unit axis here I don't use unsqueeze very often because I actually generally prefer the index with a special value none this works in PyTorch and numpy and this work the way it works is to say okay excellent remember that's size it's a vector of length 40 every row and then none means insert a unit axis here for the column so these are two ways of doing the same thing but this one is a little bit more flexible so that's what I use more often but now that we've got the shape that is expected which is a rank to tensor and a rare array with two dimensions or axes we can create a random forest we can fit it and let's just use the first 30 data points right so kind of stop here and then let's do a prediction right so let's plot the original data points and then also plot a prediction and look what happens on the prediction it acts it's kind of nice and accurate and then suddenly what happens so this is the bit where if you watch in the video I want you to pause and have a think biases flat so what's gone on here well remember a random forest is just taking the average of predictions of a bunch of trees and a tree the prediction of a tree is just the average of the values in a leaf node and remember we fitted using a training set containing only the first 30 so none of these appeared in the training set so the highest we could get would be the average of values that are inside the training set in other words there's this maximum we can get to so random forests cannot extrapolate outside of the bounds of the data that they're training set this is going to be a huge problem for things like time series prediction where there's like an underlying trend for instance but really it's more a more general issue than just time variables it's going to be hard for rent or impossible often for random forest to just extrapolate outside the types of data that it's seen in a general sense so we need to make sure that our validation set does not contain out of domain data so how do we find out of domain data so we might not even know if our test set is distributed in the same way as our training data so if they're from two different time periods how do you kind of tell how they vary right or if it's a Kaggle competition how do you tell if the test set and the training set which Kaggle gives you have some underlying differences there's actually a cool trick you can do which is you can create a column called is valid which contains zero for everything in the training set and one for everything in the validation set and it's concatenating all of the independent variables together so it's concatenating the independent variables of both the training and validation set together so this is our independent variable and this becomes our dependent variable and we're going to create a random forest not for predicting price but a random forest that predicts is this row from the validation set or the training set so if the validation set and the training set are from kind of the same distribution if they're not different then this random forest should basically have zero predictive power if it has any predictive power then it means that our training and validation set are different and to find out the source of that difference we can use feature importance and so you can see here that the difference between the validation set and the training set is not surprisingly sale elapsed so that's the number of days since I think like 1970 or something so it's basically the date so yes of course you can predict whether something is in the validation set or the training set by looking at the date because that's actually how we find them that makes sense this is interesting sales ID so it looks like the sales ID is not some random identifier but it increases over time and ditto for machine ID and then there's some other smaller ones here that kind of makes sense so I guess for something like model desk I guess there are certain models that were only made in later years for instance but you can see these top three columns are a bit of an issue so then we could say like okay what happens if we look at each one of those columns those first three and remove them and then see how it changes our RMSE on our sales price model on the validation set so we start from point two three two and removing sales ID actually makes it a bit better sale elapsed makes it a bit worse machine ID about the same so we can probably remove sales ID and machine ID without losing any accuracy and yep it's actually slightly improved but most importantly it's going to be more resilient over time right because we're trying to remove the time related features another thing to note is that since it seems that you know this kind of sale elapsed issue that maybe it's making a big difference is maybe looking at the sale year distribution this is the histogram most of the sales are in the last few years anyway so what happens if we only include the most recent few years so let's just include everything after 2004 so that is X is filtered and if I train on that subset then my accuracy goes improves a bit more from 231 to 30 so that's interesting right we're actually using less data less rows and getting a slightly better result because the more recent data is more representative so that's about as far as we can get with our random forest but what I will say is this this issue of extrapolation would not happen with a neural net would it because a neural net is using the kind of the underlying layers are linear layers and so linear layers can absolutely extrapolate so the obvious thing to think then at this point is well maybe what a neural net do a better job of this that's going to be the thing next up to this question question first how do how does feature importance relate to correlation feature importance doesn't particularly relate to correlation correlation is a concept for linear models and this is not a linear model so remember feature importance is calculated by looking at the improvement in accuracy as you go down each tree and you go down each binary split if you're used to linear regression then I guess our relations sometimes can be used as a measure of feature importance but this is a much more kind of direct version it's taking account of these nonlinearities and interactions as well so it's a much more flexible and reliable measure and release feature importance any more questions so I do the same thing with a neural network I'm going to just copy and paste the same lines of code that I had from before but this time I call it nn dfnn and these are the same lines of code and I'll grab the same list of columns we had before in the dependent variable to get the same data frame now as we've discussed for categorical columns we probably want to use embeddings so to create embeddings we need to know which columns should be treated as categorical variables and as we discussed we can use con cat split for that one of the useful things we can pass that is the maximum cardinality so max card equals 9000 means if there's a column with more than 9000 levels you should treat it as continuous and if it's got less than 9000 levels which it is categorical so that's you know it's a simple little function that just checks the cardinality and splits them based on how many discrete levels they have and of course that their data type if it's not actually a numeric data type it has to be categorical so there's our there's our split and then from there what we can do is we can say oh we've got to be a bit careful of sale elapsed because actually sale elapsed I think has less than 9000 categories but we definitely don't want to use that as a categorical variable the whole point was to make it that this is something that we can extrapolate so we certainly anything that's kind of time-dependent or we think that we might see things outside the range of inputs in the training data we should make them continuous variables so let's make sale elapsed put it in continuous neural net and remove it from categorical. So here's the number of unique levels this is from pandas for everything in our neural net data set for the categorical variables and I get a bit nervous when I see these really high numbers so I don't want to have too many things with like lots and lots of categories. The reason I don't want lots of things with lots and lots of categories is just they're going to take up a lot of parameters because in an embedding matrix this is you know every one of these is a row in an embedding matrix. In this case I notice model ID and model desk might be describing something very similar so I'd quite like to find out if I could get rid of one and an easy way to do that would be to use a random forest. So let's try removing the model desk and let's create a random forest and let's see what happens and oh it's actually a tiny bit better and certainly not worse so that suggests that we can actually get rid of one of these levels or one of these variables. So let's get rid of that one and so now we can create a tabular pandas object just like before but this time we're going to add one more processor which is normalize and the reason we need normalize so normalize is subtract the mean divide by the standard deviation. We didn't need that for a random forest because for a random forest we're just looking at less than or greater than through our binary splits. So all that matters is the order of things how they're sorted doesn't matter whether they're super big or super small but it definitely matters for neural nets because we have these linear layers so we don't want to have you know things with kind of crazy distributions with some super big numbers and super small numbers because it's not going to work. So it's always a good idea to normalize things neural nets where we can do that in a tabular neural net by using the normalize tabular proc. So we can do the same thing that we did before with creating our tabular pandas tabular object for the neural net and then we can create data loaders from that with a batch size and this is a large batch size because tabular models don't generally require nearly as much GPU RAM as a convolutional neural net or something or an RNN or something. Since the regression model we're going to want our range so let's find the minimum and maximum of our dependent variable. And we can now go ahead and create a tabular learner. So our tabular learner is going to take our data loaders our way range how many activations do you want in each of the linear layers and so you can have as many linear layers as you like here. How many outputs are there? So this is a regression with a single output and what loss function do you want? We can use lrFind and then we can go ahead and use fit1cycle. There's no pre-trained model obviously because this is not something where people have got pre-trained models for industrial equipment options. So we just use fit1cycle and train for a minute and then we can check and our RMSC is 0.226 which here was 0.230. So that's amazing we actually have you know straight away a better result and the random forest. It's a little more fussy it took some takes a little bit longer but as you can see you know for interesting data sets like this we can get some great results with neural nets. So here's something else we could do though. The random forest and the neural net they each have their own pros and cons. There's some things they're good at and there's some they're less good at. So maybe we can get the best of both worlds and a really easy way to do that is to use ensemble. So we've already seen that a random forest is a decision tree ensemble but now we can put that into another ensemble. We're going to have an ensemble of the random forest and a neural net. There's lots of super fancy ways you can do that but a really simple way is to take the average. So sum up the predictions from the two models divide by two and use that as prediction. So that's our ensemble prediction is just literally the average of the random forest prediction and the neural net prediction and that gives us 0.223 versus 0.226. So how good is that? Well it's a little hard to say because unfortunately this competition is old enough that we can't even submit to it and find out how we would have gone on Kaggle. So we don't really know and so we're relying on our own validation set but it's quite a bit better than even the first place score on the test set. So if the validation set is you know doing a good job then this is a good sign that this is a really really good model which wouldn't necessarily be that surprising because you know in the last few years I guess we've learned a lot about building these kinds of models and we're kind of taking advantage of a lot of the tricks that have that have appeared in recent years and yeah maybe this goes to show that well I think it certainly goes to show that both random forests and neural nets have a lot to offer and try both and maybe even find both. We've talked about an approach to ensembling called bagging which is where we train lots of models on different subsets of the data like the average of. Another approach to ensembling and particularly ensembling of trees is called boosting and boosting involves training a small model which underfits your data set so maybe like just have a very small number of beef nodes and then you calculate the predictions using the small model and then you subtract the predictions from the targets so these are kind of like the errors of your small underfit model we call them residual and then go back to step one but now instead of using the original targets use the residuals to train a small model which underfits your data set attempting to predict residuals and do that again and then until you reach some stopping criterion such as the maximum number of trees now you that will leave you with a bunch of models which you don't average but which you sum because each one is creating a model that's based on the residual of the previous one so we've subtracted the predictions of each new tree from the residuals of the previous tree so the residuals get smaller and smaller and then to make predictions we just have to do the opposite which is to add them all together so there's lots of variance of this but you'll see things like GBMs or gradient boosted machines or GPTTs or gradient boosted decision trees and there's lots of minor details around you know and are insignificant details but the basic idea is is what I've shown to question all right let's take the questions they're dropping features in a model as a way to reduce the complexity of the model and thus reduce overfitting is this better than adding some regularization like weight decay to tabular I didn't claim that removed columns to avoid overfitting we remove the columns to simplify fewer things to analyze and it should also mean we don't need as many trees but there's no particular reason to believe that this will regularize and well the idea of regularization doesn't necessarily make a lot of sense to random forests and always add more trees is there a good heuristic for picking the number of linear layers in the tabular model not really well if there is I don't know what it is I guess who through hidden layers works pretty well so you know what I showed those those numbers I showed her pretty good for a large ish model a default it uses 200 and 100 so maybe start with the default and then go up to 500 and 250 if that into an improvement and like just keep doubling them and improving or you run out of memory or time main thing to note about boosted models is that there's nothing to stop us from overfitting you add more and more trees a bagging model sort of a random first it's going to get going to should generalize better and better is your each time you're using your model which is based on a subset of the day but boosting each model will fit the training set better and better gradually overfit more and more so boosting methods do require generally hyperparameter tuning and fiddling around with you know you certainly have regularization boosting they're pretty sensitive to their hyper parameters which is why they're not normally my first go-to but they they often they more often win Kaggle competition random forests do like they tend to be good at getting that last little bit of performance the last thing I'm going to mention is something super neat which a lot of people don't seem to know exists is a shanks it's super cool which is something from the entity embeddings paper the table from it where what they did was they built a neural network they got the entity embeddings PE and then they tried a random forest using the entity embeddings as predictors rather than the approach I described with just the raw categorical variables and the the error for a random forest went from 0.16 to 0.11 a huge improvement and very simple method KNN went from 0.29 to 0.11 basically all of the methods when they used entity embeddings suddenly improved a lot the one thing you you you should try if you have a look at the further research section after the questionnaire is it asks to try to to do this actually take those entity embeddings that we trained in the neural net and use them in the random forest and then maybe try on some willing again and see if you can beat the point two two three that we had this is a really nice idea it's like you get you know all the benefits of the boosted decision trees but all of the nice features of entity embeddings and so this is something that not enough people seem to be playing with for some reason so overall you know random forests are nice and easy to train that you know they're very resilient they don't require much pre-processing they train quickly they don't overfit you know they can be a little less accurate and they can be a bit slow at inference time because the inference you have to go through every one of those trees having said that a binary tree can be pretty heavily optimized so you know it is something you can basically create a totally compiled version of a tree and they can certainly also be done entirely in parallel so that's something to consider gradient boosting machines are also fast to train on the whole but a little more fussy about hyperparameters you have to be careful about overfitting but a bit more accurate here or nets maybe the fussiest to deal with they've kind of got the least rules of thumb around or tutorials around saying this is kind of how to do it it's just a bit a bit newer a little bit less well understood but they can give better results in many situations than the other two approaches or at least with an ensemble can improve the other two approaches so I would always start with a random part and then see if you can beat it using these so yeah why don't you now see if you can find a Kaggle competition with tabular data whether it's running now it's a past one and see if you can repeat this process for that and see if you can get in the top 10% of the private leaderboard that would be a really great stretch goal at this point implement the decision tree algorithm yourself I think that's an important one we really understand it and then from there create your own random forest from scratch you might be surprised it's not that hard and then go and have a look at the tabular model source code and at this point this is pretty exciting you should find you pretty much know what all the lines do with two exceptions and if you don't you know dig around and explore and experiment and see if you can figure it out and with that we are I am very excited to say at a point where we've really dug all the way in to the end of these real valuable effective fast AI applications and we're understanding what's going on inside them. Now what should we expect for next week? So for next week we will set NLP and Qtvision and we'll do the same kind of ideas delve deep to see what's going on. Thanks everybody see you next week.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.5600000000000005, "text": " Hi everybody and welcome to lesson 7. We're going to start by having a look at a kind", "tokens": [2421, 2201, 293, 2928, 281, 6898, 1614, 13, 492, 434, 516, 281, 722, 538, 1419, 257, 574, 412, 257, 733], "temperature": 0.0, "avg_logprob": -0.16271232068538666, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.0010985414264723659}, {"id": 1, "seek": 0, "start": 7.5600000000000005, "end": 13.44, "text": " of regularization called weight decay and the issue that we came to at the end of the", "tokens": [295, 3890, 2144, 1219, 3364, 21039, 293, 264, 2734, 300, 321, 1361, 281, 412, 264, 917, 295, 264], "temperature": 0.0, "avg_logprob": -0.16271232068538666, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.0010985414264723659}, {"id": 2, "seek": 0, "start": 13.44, "end": 22.72, "text": " last lesson is that we were training our simple dot product model with bias and our loss data", "tokens": [1036, 6898, 307, 300, 321, 645, 3097, 527, 2199, 5893, 1674, 2316, 365, 12577, 293, 527, 4470, 1412], "temperature": 0.0, "avg_logprob": -0.16271232068538666, "compression_ratio": 1.5497076023391814, "no_speech_prob": 0.0010985414264723659}, {"id": 3, "seek": 2272, "start": 22.72, "end": 31.6, "text": " going down and then it started going up again and so we have a problem that we are overfitting", "tokens": [516, 760, 293, 550, 309, 1409, 516, 493, 797, 293, 370, 321, 362, 257, 1154, 300, 321, 366, 670, 69, 2414], "temperature": 0.0, "avg_logprob": -0.11559088285579237, "compression_ratio": 1.7403846153846154, "no_speech_prob": 7.030692358966917e-05}, {"id": 4, "seek": 2272, "start": 31.6, "end": 37.2, "text": " and remember in this case we're using mean squared error so try to recall why it is that", "tokens": [293, 1604, 294, 341, 1389, 321, 434, 1228, 914, 8889, 6713, 370, 853, 281, 9901, 983, 309, 307, 300], "temperature": 0.0, "avg_logprob": -0.11559088285579237, "compression_ratio": 1.7403846153846154, "no_speech_prob": 7.030692358966917e-05}, {"id": 5, "seek": 2272, "start": 37.2, "end": 44.44, "text": " we don't need a metric here because mean squared error is pretty much the thing we care about", "tokens": [321, 500, 380, 643, 257, 20678, 510, 570, 914, 8889, 6713, 307, 1238, 709, 264, 551, 321, 1127, 466], "temperature": 0.0, "avg_logprob": -0.11559088285579237, "compression_ratio": 1.7403846153846154, "no_speech_prob": 7.030692358966917e-05}, {"id": 6, "seek": 2272, "start": 44.44, "end": 49.0, "text": " really or we could use mean absolute error if we like but either of those works fine", "tokens": [534, 420, 321, 727, 764, 914, 8236, 6713, 498, 321, 411, 457, 2139, 295, 729, 1985, 2489], "temperature": 0.0, "avg_logprob": -0.11559088285579237, "compression_ratio": 1.7403846153846154, "no_speech_prob": 7.030692358966917e-05}, {"id": 7, "seek": 4900, "start": 49.0, "end": 56.56, "text": " as a loss function they don't have the problem of big flat areas like accuracy does for classification.", "tokens": [382, 257, 4470, 2445, 436, 500, 380, 362, 264, 1154, 295, 955, 4962, 3179, 411, 14170, 775, 337, 21538, 13], "temperature": 0.0, "avg_logprob": -0.07793897321854515, "compression_ratio": 1.7383177570093458, "no_speech_prob": 1.844814391915861e-06}, {"id": 8, "seek": 4900, "start": 56.56, "end": 63.28, "text": " So what we want to do is to make it less likely that we're going to overfit by doing something", "tokens": [407, 437, 321, 528, 281, 360, 307, 281, 652, 309, 1570, 3700, 300, 321, 434, 516, 281, 670, 6845, 538, 884, 746], "temperature": 0.0, "avg_logprob": -0.07793897321854515, "compression_ratio": 1.7383177570093458, "no_speech_prob": 1.844814391915861e-06}, {"id": 9, "seek": 4900, "start": 63.28, "end": 68.16, "text": " we call reducing the capacity of the model. The capacity of the model is basically how", "tokens": [321, 818, 12245, 264, 6042, 295, 264, 2316, 13, 440, 6042, 295, 264, 2316, 307, 1936, 577], "temperature": 0.0, "avg_logprob": -0.07793897321854515, "compression_ratio": 1.7383177570093458, "no_speech_prob": 1.844814391915861e-06}, {"id": 10, "seek": 4900, "start": 68.16, "end": 74.68, "text": " much space does it have to find answers and if it can kind of find any answer anywhere", "tokens": [709, 1901, 775, 309, 362, 281, 915, 6338, 293, 498, 309, 393, 733, 295, 915, 604, 1867, 4992], "temperature": 0.0, "avg_logprob": -0.07793897321854515, "compression_ratio": 1.7383177570093458, "no_speech_prob": 1.844814391915861e-06}, {"id": 11, "seek": 7468, "start": 74.68, "end": 82.48, "text": " those answers can include basically memorizing the data set. So one way to handle this would", "tokens": [729, 6338, 393, 4090, 1936, 10560, 3319, 264, 1412, 992, 13, 407, 472, 636, 281, 4813, 341, 576], "temperature": 0.0, "avg_logprob": -0.09130868786259701, "compression_ratio": 1.6418604651162791, "no_speech_prob": 4.0525358713239257e-07}, {"id": 12, "seek": 7468, "start": 82.48, "end": 90.58000000000001, "text": " be to decrease the number of latent factors but generally speaking reducing the number", "tokens": [312, 281, 11514, 264, 1230, 295, 48994, 6771, 457, 5101, 4124, 12245, 264, 1230], "temperature": 0.0, "avg_logprob": -0.09130868786259701, "compression_ratio": 1.6418604651162791, "no_speech_prob": 4.0525358713239257e-07}, {"id": 13, "seek": 7468, "start": 90.58000000000001, "end": 97.0, "text": " of parameters in a model particularly as we look at more deep learning style models ends", "tokens": [295, 9834, 294, 257, 2316, 4098, 382, 321, 574, 412, 544, 2452, 2539, 3758, 5245, 5314], "temperature": 0.0, "avg_logprob": -0.09130868786259701, "compression_ratio": 1.6418604651162791, "no_speech_prob": 4.0525358713239257e-07}, {"id": 14, "seek": 7468, "start": 97.0, "end": 103.56, "text": " up biasing the models towards very simple kind of shapes but there's a better way to", "tokens": [493, 3228, 3349, 264, 5245, 3030, 588, 2199, 733, 295, 10854, 457, 456, 311, 257, 1101, 636, 281], "temperature": 0.0, "avg_logprob": -0.09130868786259701, "compression_ratio": 1.6418604651162791, "no_speech_prob": 4.0525358713239257e-07}, {"id": 15, "seek": 10356, "start": 103.56, "end": 110.88, "text": " do it rather than reducing the number of parameters instead we try to force the parameters to", "tokens": [360, 309, 2831, 813, 12245, 264, 1230, 295, 9834, 2602, 321, 853, 281, 3464, 264, 9834, 281], "temperature": 0.0, "avg_logprob": -0.0795067548751831, "compression_ratio": 1.7045454545454546, "no_speech_prob": 7.571135256512207e-07}, {"id": 16, "seek": 10356, "start": 110.88, "end": 117.36, "text": " be smaller unless they're really required to be big and the way we do that is with weight", "tokens": [312, 4356, 5969, 436, 434, 534, 4739, 281, 312, 955, 293, 264, 636, 321, 360, 300, 307, 365, 3364], "temperature": 0.0, "avg_logprob": -0.0795067548751831, "compression_ratio": 1.7045454545454546, "no_speech_prob": 7.571135256512207e-07}, {"id": 17, "seek": 10356, "start": 117.36, "end": 122.92, "text": " decay weight decay is also known as L2 regularization they're very slightly different but we can", "tokens": [21039, 3364, 21039, 307, 611, 2570, 382, 441, 17, 3890, 2144, 436, 434, 588, 4748, 819, 457, 321, 393], "temperature": 0.0, "avg_logprob": -0.0795067548751831, "compression_ratio": 1.7045454545454546, "no_speech_prob": 7.571135256512207e-07}, {"id": 18, "seek": 10356, "start": 122.92, "end": 128.56, "text": " think of them as the same thing and what we do is we change our loss function and specifically", "tokens": [519, 295, 552, 382, 264, 912, 551, 293, 437, 321, 360, 307, 321, 1319, 527, 4470, 2445, 293, 4682], "temperature": 0.0, "avg_logprob": -0.0795067548751831, "compression_ratio": 1.7045454545454546, "no_speech_prob": 7.571135256512207e-07}, {"id": 19, "seek": 12856, "start": 128.56, "end": 135.76, "text": " we change the loss function by adding to it the sum of all the weights squared in fact", "tokens": [321, 1319, 264, 4470, 2445, 538, 5127, 281, 309, 264, 2408, 295, 439, 264, 17443, 8889, 294, 1186], "temperature": 0.0, "avg_logprob": -0.09173304357646424, "compression_ratio": 1.8134715025906736, "no_speech_prob": 4.888289595328388e-07}, {"id": 20, "seek": 12856, "start": 135.76, "end": 142.32, "text": " all of the parameters squared really should stay. Why do we do that? Well because if that's", "tokens": [439, 295, 264, 9834, 8889, 534, 820, 1754, 13, 1545, 360, 321, 360, 300, 30, 1042, 570, 498, 300, 311], "temperature": 0.0, "avg_logprob": -0.09173304357646424, "compression_ratio": 1.8134715025906736, "no_speech_prob": 4.888289595328388e-07}, {"id": 21, "seek": 12856, "start": 142.32, "end": 147.56, "text": " part of the loss function then one way to decrease the loss would be to decrease the", "tokens": [644, 295, 264, 4470, 2445, 550, 472, 636, 281, 11514, 264, 4470, 576, 312, 281, 11514, 264], "temperature": 0.0, "avg_logprob": -0.09173304357646424, "compression_ratio": 1.8134715025906736, "no_speech_prob": 4.888289595328388e-07}, {"id": 22, "seek": 12856, "start": 147.56, "end": 154.52, "text": " weights one particular weight or all of the weights or something like that and so when", "tokens": [17443, 472, 1729, 3364, 420, 439, 295, 264, 17443, 420, 746, 411, 300, 293, 370, 562], "temperature": 0.0, "avg_logprob": -0.09173304357646424, "compression_ratio": 1.8134715025906736, "no_speech_prob": 4.888289595328388e-07}, {"id": 23, "seek": 15452, "start": 154.52, "end": 163.92000000000002, "text": " we decrease the weights if you think about what that would do and think about for example", "tokens": [321, 11514, 264, 17443, 498, 291, 519, 466, 437, 300, 576, 360, 293, 519, 466, 337, 1365], "temperature": 0.0, "avg_logprob": -0.1659711618892482, "compression_ratio": 1.5872093023255813, "no_speech_prob": 3.4125818615393655e-07}, {"id": 24, "seek": 15452, "start": 163.92000000000002, "end": 174.8, "text": " the different possible values of a in y equals ax squared the larger a is for example a is", "tokens": [264, 819, 1944, 4190, 295, 257, 294, 288, 6915, 6360, 8889, 264, 4833, 257, 307, 337, 1365, 257, 307], "temperature": 0.0, "avg_logprob": -0.1659711618892482, "compression_ratio": 1.5872093023255813, "no_speech_prob": 3.4125818615393655e-07}, {"id": 25, "seek": 15452, "start": 174.8, "end": 184.48000000000002, "text": " 50 you get these very narrow peaks in general big coefficients are going to cause big swings", "tokens": [2625, 291, 483, 613, 588, 9432, 26897, 294, 2674, 955, 31994, 366, 516, 281, 3082, 955, 32386], "temperature": 0.0, "avg_logprob": -0.1659711618892482, "compression_ratio": 1.5872093023255813, "no_speech_prob": 3.4125818615393655e-07}, {"id": 26, "seek": 18448, "start": 184.48, "end": 191.28, "text": " big changes in the in the loss a small changes in the parameters and when you have these", "tokens": [955, 2962, 294, 264, 294, 264, 4470, 257, 1359, 2962, 294, 264, 9834, 293, 562, 291, 362, 613], "temperature": 0.0, "avg_logprob": -0.10441043263389951, "compression_ratio": 1.9358288770053476, "no_speech_prob": 8.990948003884114e-07}, {"id": 27, "seek": 18448, "start": 191.28, "end": 200.95999999999998, "text": " kind of sharp peaks or valleys it means that a small change through the parameter can make", "tokens": [733, 295, 8199, 26897, 420, 45614, 309, 1355, 300, 257, 1359, 1319, 807, 264, 13075, 393, 652], "temperature": 0.0, "avg_logprob": -0.10441043263389951, "compression_ratio": 1.9358288770053476, "no_speech_prob": 8.990948003884114e-07}, {"id": 28, "seek": 18448, "start": 200.95999999999998, "end": 207.16, "text": " a very small change to the input and make a big change to the loss and so if you have", "tokens": [257, 588, 1359, 1319, 281, 264, 4846, 293, 652, 257, 955, 1319, 281, 264, 4470, 293, 370, 498, 291, 362], "temperature": 0.0, "avg_logprob": -0.10441043263389951, "compression_ratio": 1.9358288770053476, "no_speech_prob": 8.990948003884114e-07}, {"id": 29, "seek": 18448, "start": 207.16, "end": 213.16, "text": " if you're in that situation then you can basically fit all the data points close to exactly with", "tokens": [498, 291, 434, 294, 300, 2590, 550, 291, 393, 1936, 3318, 439, 264, 1412, 2793, 1998, 281, 2293, 365], "temperature": 0.0, "avg_logprob": -0.10441043263389951, "compression_ratio": 1.9358288770053476, "no_speech_prob": 8.990948003884114e-07}, {"id": 30, "seek": 21316, "start": 213.16, "end": 220.04, "text": " a really complex jagged function with sharp changes which exactly tries to sit on each", "tokens": [257, 534, 3997, 6368, 3004, 2445, 365, 8199, 2962, 597, 2293, 9898, 281, 1394, 322, 1184], "temperature": 0.0, "avg_logprob": -0.09450390934944153, "compression_ratio": 1.6523809523809523, "no_speech_prob": 5.804993747915432e-07}, {"id": 31, "seek": 21316, "start": 220.04, "end": 226.28, "text": " data point rather than finding a nice smooth surface which connects them all together or", "tokens": [1412, 935, 2831, 813, 5006, 257, 1481, 5508, 3753, 597, 16967, 552, 439, 1214, 420], "temperature": 0.0, "avg_logprob": -0.09450390934944153, "compression_ratio": 1.6523809523809523, "no_speech_prob": 5.804993747915432e-07}, {"id": 32, "seek": 21316, "start": 226.28, "end": 233.84, "text": " goes through them all. So if we limit our weights by adding in to the loss function", "tokens": [1709, 807, 552, 439, 13, 407, 498, 321, 4948, 527, 17443, 538, 5127, 294, 281, 264, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.09450390934944153, "compression_ratio": 1.6523809523809523, "no_speech_prob": 5.804993747915432e-07}, {"id": 33, "seek": 21316, "start": 233.84, "end": 239.68, "text": " the sum of the weight squared then what it's going to do is it's going to fit less well", "tokens": [264, 2408, 295, 264, 3364, 8889, 550, 437, 309, 311, 516, 281, 360, 307, 309, 311, 516, 281, 3318, 1570, 731], "temperature": 0.0, "avg_logprob": -0.09450390934944153, "compression_ratio": 1.6523809523809523, "no_speech_prob": 5.804993747915432e-07}, {"id": 34, "seek": 23968, "start": 239.68, "end": 244.48000000000002, "text": " on the training set because we're giving it less room to try anything that it wants to", "tokens": [322, 264, 3097, 992, 570, 321, 434, 2902, 309, 1570, 1808, 281, 853, 1340, 300, 309, 2738, 281], "temperature": 0.0, "avg_logprob": -0.07188837948967429, "compression_ratio": 1.708133971291866, "no_speech_prob": 4.737899814699631e-07}, {"id": 35, "seek": 23968, "start": 244.48000000000002, "end": 249.64000000000001, "text": " but we're going to hope that it would result in a better loss on the validation set or", "tokens": [457, 321, 434, 516, 281, 1454, 300, 309, 576, 1874, 294, 257, 1101, 4470, 322, 264, 24071, 992, 420], "temperature": 0.0, "avg_logprob": -0.07188837948967429, "compression_ratio": 1.708133971291866, "no_speech_prob": 4.737899814699631e-07}, {"id": 36, "seek": 23968, "start": 249.64000000000001, "end": 256.08, "text": " the test set so that it will generalize better. One way to think about this is that the loss", "tokens": [264, 1500, 992, 370, 300, 309, 486, 2674, 1125, 1101, 13, 1485, 636, 281, 519, 466, 341, 307, 300, 264, 4470], "temperature": 0.0, "avg_logprob": -0.07188837948967429, "compression_ratio": 1.708133971291866, "no_speech_prob": 4.737899814699631e-07}, {"id": 37, "seek": 23968, "start": 256.08, "end": 265.56, "text": " with weight decay is just the loss plus the sum of the parameter squared times some number", "tokens": [365, 3364, 21039, 307, 445, 264, 4470, 1804, 264, 2408, 295, 264, 13075, 8889, 1413, 512, 1230], "temperature": 0.0, "avg_logprob": -0.07188837948967429, "compression_ratio": 1.708133971291866, "no_speech_prob": 4.737899814699631e-07}, {"id": 38, "seek": 26556, "start": 265.56, "end": 273.96, "text": " we pick a hyper parameter sometimes it's like 0.1 or 0.01 or 0.001 and if region so this", "tokens": [321, 1888, 257, 9848, 13075, 2171, 309, 311, 411, 1958, 13, 16, 420, 1958, 13, 10607, 420, 1958, 13, 628, 16, 293, 498, 4458, 370, 341], "temperature": 0.0, "avg_logprob": -0.14251082738240559, "compression_ratio": 1.6889952153110048, "no_speech_prob": 2.273269785746379e-07}, {"id": 39, "seek": 26556, "start": 273.96, "end": 279.4, "text": " is basically what loss with weight decay looks like in his equation but remember when it", "tokens": [307, 1936, 437, 4470, 365, 3364, 21039, 1542, 411, 294, 702, 5367, 457, 1604, 562, 309], "temperature": 0.0, "avg_logprob": -0.14251082738240559, "compression_ratio": 1.6889952153110048, "no_speech_prob": 2.273269785746379e-07}, {"id": 40, "seek": 26556, "start": 279.4, "end": 283.72, "text": " actually comes to what's how is the loss used in stochastic gradient descent it's used by", "tokens": [767, 1487, 281, 437, 311, 577, 307, 264, 4470, 1143, 294, 342, 8997, 2750, 16235, 23475, 309, 311, 1143, 538], "temperature": 0.0, "avg_logprob": -0.14251082738240559, "compression_ratio": 1.6889952153110048, "no_speech_prob": 2.273269785746379e-07}, {"id": 41, "seek": 26556, "start": 283.72, "end": 290.56, "text": " taking its gradient. So what's the gradient of this well if you remember back to when", "tokens": [1940, 1080, 16235, 13, 407, 437, 311, 264, 16235, 295, 341, 731, 498, 291, 1604, 646, 281, 562], "temperature": 0.0, "avg_logprob": -0.14251082738240559, "compression_ratio": 1.6889952153110048, "no_speech_prob": 2.273269785746379e-07}, {"id": 42, "seek": 29056, "start": 290.56, "end": 296.96, "text": " you first learned calculus it's okay if you don't the gradient of something squared is", "tokens": [291, 700, 3264, 33400, 309, 311, 1392, 498, 291, 500, 380, 264, 16235, 295, 746, 8889, 307], "temperature": 0.0, "avg_logprob": -0.15259482536786867, "compression_ratio": 1.7403846153846154, "no_speech_prob": 6.08358448062063e-07}, {"id": 43, "seek": 29056, "start": 296.96, "end": 302.72, "text": " just two times that something we've changed from parameters to weight which is a bit confusing", "tokens": [445, 732, 1413, 300, 746, 321, 600, 3105, 490, 9834, 281, 3364, 597, 307, 257, 857, 13181], "temperature": 0.0, "avg_logprob": -0.15259482536786867, "compression_ratio": 1.7403846153846154, "no_speech_prob": 6.08358448062063e-07}, {"id": 44, "seek": 29056, "start": 302.72, "end": 311.72, "text": " so let's use weight here to keep it consistent maybe parameters is better. So the derivative", "tokens": [370, 718, 311, 764, 3364, 510, 281, 1066, 309, 8398, 1310, 9834, 307, 1101, 13, 407, 264, 13760], "temperature": 0.0, "avg_logprob": -0.15259482536786867, "compression_ratio": 1.7403846153846154, "no_speech_prob": 6.08358448062063e-07}, {"id": 45, "seek": 29056, "start": 311.72, "end": 319.0, "text": " of weight squared is just two times weight so in other words to add in this term to the", "tokens": [295, 3364, 8889, 307, 445, 732, 1413, 3364, 370, 294, 661, 2283, 281, 909, 294, 341, 1433, 281, 264], "temperature": 0.0, "avg_logprob": -0.15259482536786867, "compression_ratio": 1.7403846153846154, "no_speech_prob": 6.08358448062063e-07}, {"id": 46, "seek": 31900, "start": 319.0, "end": 326.84, "text": " gradient we can just add to the gradients weight decay times two times weight and since", "tokens": [16235, 321, 393, 445, 909, 281, 264, 2771, 2448, 3364, 21039, 1413, 732, 1413, 3364, 293, 1670], "temperature": 0.0, "avg_logprob": -0.10792337319789788, "compression_ratio": 2.175, "no_speech_prob": 5.62639741019666e-07}, {"id": 47, "seek": 31900, "start": 326.84, "end": 331.08, "text": " weight decay is just a hyper parameter we can just replace it with weight decay times", "tokens": [3364, 21039, 307, 445, 257, 9848, 13075, 321, 393, 445, 7406, 309, 365, 3364, 21039, 1413], "temperature": 0.0, "avg_logprob": -0.10792337319789788, "compression_ratio": 2.175, "no_speech_prob": 5.62639741019666e-07}, {"id": 48, "seek": 31900, "start": 331.08, "end": 337.78, "text": " two so that would just give us weight decay times weight. So weight decay refers to adding", "tokens": [732, 370, 300, 576, 445, 976, 505, 3364, 21039, 1413, 3364, 13, 407, 3364, 21039, 14942, 281, 5127], "temperature": 0.0, "avg_logprob": -0.10792337319789788, "compression_ratio": 2.175, "no_speech_prob": 5.62639741019666e-07}, {"id": 49, "seek": 31900, "start": 337.78, "end": 347.76, "text": " on the to the gradients the weights times some hyper parameter and so that is going", "tokens": [322, 264, 281, 264, 2771, 2448, 264, 17443, 1413, 512, 9848, 13075, 293, 370, 300, 307, 516], "temperature": 0.0, "avg_logprob": -0.10792337319789788, "compression_ratio": 2.175, "no_speech_prob": 5.62639741019666e-07}, {"id": 50, "seek": 34776, "start": 347.76, "end": 357.28, "text": " to try to create these kind of more shallow less bumpy surfaces. So to do that we can", "tokens": [281, 853, 281, 1884, 613, 733, 295, 544, 20488, 1570, 49400, 16130, 13, 407, 281, 360, 300, 321, 393], "temperature": 0.0, "avg_logprob": -0.10912979894609594, "compression_ratio": 1.5144508670520231, "no_speech_prob": 2.15681325244077e-06}, {"id": 51, "seek": 34776, "start": 357.28, "end": 365.36, "text": " simply when we call fit or fit one cycle or whatever we can pass in a WD parameter and", "tokens": [2935, 562, 321, 818, 3318, 420, 3318, 472, 6586, 420, 2035, 321, 393, 1320, 294, 257, 343, 35, 13075, 293], "temperature": 0.0, "avg_logprob": -0.10912979894609594, "compression_ratio": 1.5144508670520231, "no_speech_prob": 2.15681325244077e-06}, {"id": 52, "seek": 34776, "start": 365.36, "end": 373.3, "text": " that's just this number here. So if we pass in point one then the training loss goes from", "tokens": [300, 311, 445, 341, 1230, 510, 13, 407, 498, 321, 1320, 294, 935, 472, 550, 264, 3097, 4470, 1709, 490], "temperature": 0.0, "avg_logprob": -0.10912979894609594, "compression_ratio": 1.5144508670520231, "no_speech_prob": 2.15681325244077e-06}, {"id": 53, "seek": 37330, "start": 373.3, "end": 380.56, "text": " point two nine to point four nine that's much worse right because we can't overfit anymore", "tokens": [935, 732, 4949, 281, 935, 1451, 4949, 300, 311, 709, 5324, 558, 570, 321, 393, 380, 670, 6845, 3602], "temperature": 0.0, "avg_logprob": -0.1272942031302103, "compression_ratio": 1.7980295566502462, "no_speech_prob": 8.990946867015737e-07}, {"id": 54, "seek": 37330, "start": 380.56, "end": 387.32, "text": " but the valid loss goes from point eight nine to point eight two much better. So this is", "tokens": [457, 264, 7363, 4470, 1709, 490, 935, 3180, 4949, 281, 935, 3180, 732, 709, 1101, 13, 407, 341, 307], "temperature": 0.0, "avg_logprob": -0.1272942031302103, "compression_ratio": 1.7980295566502462, "no_speech_prob": 8.990946867015737e-07}, {"id": 55, "seek": 37330, "start": 387.32, "end": 391.16, "text": " an important thing to remember for those of you that have done a lot of more traditional", "tokens": [364, 1021, 551, 281, 1604, 337, 729, 295, 291, 300, 362, 1096, 257, 688, 295, 544, 5164], "temperature": 0.0, "avg_logprob": -0.1272942031302103, "compression_ratio": 1.7980295566502462, "no_speech_prob": 8.990946867015737e-07}, {"id": 56, "seek": 37330, "start": 391.16, "end": 397.66, "text": " statistical models is in kind of more traditional statistical models we try to avoid overfitting", "tokens": [22820, 5245, 307, 294, 733, 295, 544, 5164, 22820, 5245, 321, 853, 281, 5042, 670, 69, 2414], "temperature": 0.0, "avg_logprob": -0.1272942031302103, "compression_ratio": 1.7980295566502462, "no_speech_prob": 8.990946867015737e-07}, {"id": 57, "seek": 39766, "start": 397.66, "end": 404.44, "text": " and we try to increase generalization by decreasing the number of parameters but in a lot of modern", "tokens": [293, 321, 853, 281, 3488, 2674, 2144, 538, 23223, 264, 1230, 295, 9834, 457, 294, 257, 688, 295, 4363], "temperature": 0.0, "avg_logprob": -0.11568888425827026, "compression_ratio": 1.644736842105263, "no_speech_prob": 7.112423077160202e-07}, {"id": 58, "seek": 39766, "start": 404.44, "end": 412.06, "text": " machine learning and certainly deep learning we tend to instead use regularization such", "tokens": [3479, 2539, 293, 3297, 2452, 2539, 321, 3928, 281, 2602, 764, 3890, 2144, 1270], "temperature": 0.0, "avg_logprob": -0.11568888425827026, "compression_ratio": 1.644736842105263, "no_speech_prob": 7.112423077160202e-07}, {"id": 59, "seek": 39766, "start": 412.06, "end": 418.36, "text": " as weight decay because it gives us more flexibility it lets us use more nonlinear functions and", "tokens": [382, 3364, 21039, 570, 309, 2709, 505, 544, 12635, 309, 6653, 505, 764, 544, 2107, 28263, 6828, 293], "temperature": 0.0, "avg_logprob": -0.11568888425827026, "compression_ratio": 1.644736842105263, "no_speech_prob": 7.112423077160202e-07}, {"id": 60, "seek": 39766, "start": 418.36, "end": 424.42, "text": " still avoid you know still reduces the capacity of the model. Great so we're down to point", "tokens": [920, 5042, 291, 458, 920, 18081, 264, 6042, 295, 264, 2316, 13, 3769, 370, 321, 434, 760, 281, 935], "temperature": 0.0, "avg_logprob": -0.11568888425827026, "compression_ratio": 1.644736842105263, "no_speech_prob": 7.112423077160202e-07}, {"id": 61, "seek": 42442, "start": 424.42, "end": 431.88, "text": " eight two three this is a good model this is really actually a very good model and so", "tokens": [3180, 732, 1045, 341, 307, 257, 665, 2316, 341, 307, 534, 767, 257, 588, 665, 2316, 293, 370], "temperature": 0.0, "avg_logprob": -0.09566200070264863, "compression_ratio": 1.8870967741935485, "no_speech_prob": 1.0511472510188469e-06}, {"id": 62, "seek": 42442, "start": 431.88, "end": 439.04, "text": " let's dig into actually what's going on here because in our in our architecture remember", "tokens": [718, 311, 2528, 666, 767, 437, 311, 516, 322, 510, 570, 294, 527, 294, 527, 9482, 1604], "temperature": 0.0, "avg_logprob": -0.09566200070264863, "compression_ratio": 1.8870967741935485, "no_speech_prob": 1.0511472510188469e-06}, {"id": 63, "seek": 42442, "start": 439.04, "end": 445.20000000000005, "text": " we basically just had four embedding layers so what's an embedding layer we've described", "tokens": [321, 1936, 445, 632, 1451, 12240, 3584, 7914, 370, 437, 311, 364, 12240, 3584, 4583, 321, 600, 7619], "temperature": 0.0, "avg_logprob": -0.09566200070264863, "compression_ratio": 1.8870967741935485, "no_speech_prob": 1.0511472510188469e-06}, {"id": 64, "seek": 42442, "start": 445.20000000000005, "end": 451.04, "text": " it conceptually but let's write our own and remember we said that an embedding layer is", "tokens": [309, 3410, 671, 457, 718, 311, 2464, 527, 1065, 293, 1604, 321, 848, 300, 364, 12240, 3584, 4583, 307], "temperature": 0.0, "avg_logprob": -0.09566200070264863, "compression_ratio": 1.8870967741935485, "no_speech_prob": 1.0511472510188469e-06}, {"id": 65, "seek": 45104, "start": 451.04, "end": 455.84000000000003, "text": " just a computational shortcut for doing a matrix multiplication by a one-hot encoded", "tokens": [445, 257, 28270, 24822, 337, 884, 257, 8141, 27290, 538, 257, 472, 12, 12194, 2058, 12340], "temperature": 0.0, "avg_logprob": -0.11999050430629564, "compression_ratio": 1.6587677725118484, "no_speech_prob": 4.965270363754826e-07}, {"id": 66, "seek": 45104, "start": 455.84000000000003, "end": 464.32000000000005, "text": " matrix and that that is actually the same as just indexing into an array. So an embedding", "tokens": [8141, 293, 300, 300, 307, 767, 264, 912, 382, 445, 8186, 278, 666, 364, 10225, 13, 407, 364, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.11999050430629564, "compression_ratio": 1.6587677725118484, "no_speech_prob": 4.965270363754826e-07}, {"id": 67, "seek": 45104, "start": 464.32000000000005, "end": 471.6, "text": " is just a indexing into an array and so it's nice to be able to create our own versions", "tokens": [307, 445, 257, 8186, 278, 666, 364, 10225, 293, 370, 309, 311, 1481, 281, 312, 1075, 281, 1884, 527, 1065, 9606], "temperature": 0.0, "avg_logprob": -0.11999050430629564, "compression_ratio": 1.6587677725118484, "no_speech_prob": 4.965270363754826e-07}, {"id": 68, "seek": 45104, "start": 471.6, "end": 477.04, "text": " of things that exist in PyTorch and fast AI so let's do that for embedding. So if we're", "tokens": [295, 721, 300, 2514, 294, 9953, 51, 284, 339, 293, 2370, 7318, 370, 718, 311, 360, 300, 337, 12240, 3584, 13, 407, 498, 321, 434], "temperature": 0.0, "avg_logprob": -0.11999050430629564, "compression_ratio": 1.6587677725118484, "no_speech_prob": 4.965270363754826e-07}, {"id": 69, "seek": 47704, "start": 477.04, "end": 483.64000000000004, "text": " going to create our own kind of layer which is pretty cool we need to be aware of something", "tokens": [516, 281, 1884, 527, 1065, 733, 295, 4583, 597, 307, 1238, 1627, 321, 643, 281, 312, 3650, 295, 746], "temperature": 0.0, "avg_logprob": -0.1451136589050293, "compression_ratio": 1.8641975308641976, "no_speech_prob": 1.1365616501279874e-06}, {"id": 70, "seek": 47704, "start": 483.64000000000004, "end": 490.92, "text": " which is normally a a layer is basically created by inheriting as we've discussed from module", "tokens": [597, 307, 5646, 257, 257, 4583, 307, 1936, 2942, 538, 9484, 1748, 382, 321, 600, 7152, 490, 10088], "temperature": 0.0, "avg_logprob": -0.1451136589050293, "compression_ratio": 1.8641975308641976, "no_speech_prob": 1.1365616501279874e-06}, {"id": 71, "seek": 47704, "start": 490.92, "end": 496.24, "text": " or nn.module so for example this is an example here of a module where we've created a class", "tokens": [420, 297, 77, 13, 8014, 2271, 370, 337, 1365, 341, 307, 364, 1365, 510, 295, 257, 10088, 689, 321, 600, 2942, 257, 1508], "temperature": 0.0, "avg_logprob": -0.1451136589050293, "compression_ratio": 1.8641975308641976, "no_speech_prob": 1.1365616501279874e-06}, {"id": 72, "seek": 47704, "start": 496.24, "end": 501.64000000000004, "text": " called t that inherits from module and when it's constructed remember that's what dunder", "tokens": [1219, 256, 300, 9484, 1208, 490, 10088, 293, 562, 309, 311, 17083, 1604, 300, 311, 437, 274, 6617], "temperature": 0.0, "avg_logprob": -0.1451136589050293, "compression_ratio": 1.8641975308641976, "no_speech_prob": 1.1365616501279874e-06}, {"id": 73, "seek": 47704, "start": 501.64000000000004, "end": 505.84000000000003, "text": " in it does we're just going to sit this is just a dummy little module here we're going", "tokens": [294, 309, 775, 321, 434, 445, 516, 281, 1394, 341, 307, 445, 257, 35064, 707, 10088, 510, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.1451136589050293, "compression_ratio": 1.8641975308641976, "no_speech_prob": 1.1365616501279874e-06}, {"id": 74, "seek": 50584, "start": 505.84, "end": 512.6, "text": " to set self.a to the number one repeated three times as a tensor. Now if you remember back", "tokens": [281, 992, 2698, 13, 64, 281, 264, 1230, 472, 10477, 1045, 1413, 382, 257, 40863, 13, 823, 498, 291, 1604, 646], "temperature": 0.0, "avg_logprob": -0.11968852149115669, "compression_ratio": 1.6712962962962963, "no_speech_prob": 4.313905606068147e-07}, {"id": 75, "seek": 50584, "start": 512.6, "end": 519.9599999999999, "text": " to notebook four we talked about how the optimizers in PyTorch and fast AI rely on being able", "tokens": [281, 21060, 1451, 321, 2825, 466, 577, 264, 5028, 22525, 294, 9953, 51, 284, 339, 293, 2370, 7318, 10687, 322, 885, 1075], "temperature": 0.0, "avg_logprob": -0.11968852149115669, "compression_ratio": 1.6712962962962963, "no_speech_prob": 4.313905606068147e-07}, {"id": 76, "seek": 50584, "start": 519.9599999999999, "end": 524.8199999999999, "text": " to grab the parameters attribute to find a list of all the parameters. Now if you want", "tokens": [281, 4444, 264, 9834, 19667, 281, 915, 257, 1329, 295, 439, 264, 9834, 13, 823, 498, 291, 528], "temperature": 0.0, "avg_logprob": -0.11968852149115669, "compression_ratio": 1.6712962962962963, "no_speech_prob": 4.313905606068147e-07}, {"id": 77, "seek": 50584, "start": 524.8199999999999, "end": 531.48, "text": " to be able to optimize self.a you would need to appear in parameters but actually there's", "tokens": [281, 312, 1075, 281, 19719, 2698, 13, 64, 291, 576, 643, 281, 4204, 294, 9834, 457, 767, 456, 311], "temperature": 0.0, "avg_logprob": -0.11968852149115669, "compression_ratio": 1.6712962962962963, "no_speech_prob": 4.313905606068147e-07}, {"id": 78, "seek": 53148, "start": 531.48, "end": 538.52, "text": " nothing there why is that that's because PyTorch does not assume that everything that's in", "tokens": [1825, 456, 983, 307, 300, 300, 311, 570, 9953, 51, 284, 339, 775, 406, 6552, 300, 1203, 300, 311, 294], "temperature": 0.0, "avg_logprob": -0.13643397498376592, "compression_ratio": 1.7864077669902914, "no_speech_prob": 1.248267608389142e-06}, {"id": 79, "seek": 53148, "start": 538.52, "end": 542.72, "text": " a module is something that needs to be learned to tell it that it's something that needs", "tokens": [257, 10088, 307, 746, 300, 2203, 281, 312, 3264, 281, 980, 309, 300, 309, 311, 746, 300, 2203], "temperature": 0.0, "avg_logprob": -0.13643397498376592, "compression_ratio": 1.7864077669902914, "no_speech_prob": 1.248267608389142e-06}, {"id": 80, "seek": 53148, "start": 542.72, "end": 550.04, "text": " to be learned you have to wrap it with nn.parameter so here's exactly the same class but torch.ones", "tokens": [281, 312, 3264, 291, 362, 281, 7019, 309, 365, 297, 77, 13, 2181, 335, 2398, 370, 510, 311, 2293, 264, 912, 1508, 457, 27822, 13, 2213], "temperature": 0.0, "avg_logprob": -0.13643397498376592, "compression_ratio": 1.7864077669902914, "no_speech_prob": 1.248267608389142e-06}, {"id": 81, "seek": 53148, "start": 550.04, "end": 555.5600000000001, "text": " which is just a list of three three ones in this case is wrapped in nn.parameter and now", "tokens": [597, 307, 445, 257, 1329, 295, 1045, 1045, 2306, 294, 341, 1389, 307, 14226, 294, 297, 77, 13, 2181, 335, 2398, 293, 586], "temperature": 0.0, "avg_logprob": -0.13643397498376592, "compression_ratio": 1.7864077669902914, "no_speech_prob": 1.248267608389142e-06}, {"id": 82, "seek": 55556, "start": 555.56, "end": 563.3199999999999, "text": " if I go parameters I see I have a parameter with three ones in it and that's going to", "tokens": [498, 286, 352, 9834, 286, 536, 286, 362, 257, 13075, 365, 1045, 2306, 294, 309, 293, 300, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.09664666250850378, "compression_ratio": 1.7425742574257426, "no_speech_prob": 1.0188080068473937e-06}, {"id": 83, "seek": 55556, "start": 563.3199999999999, "end": 569.28, "text": " automatically call requires grad underscore for us as well we haven't had to do that for", "tokens": [6772, 818, 7029, 2771, 37556, 337, 505, 382, 731, 321, 2378, 380, 632, 281, 360, 300, 337], "temperature": 0.0, "avg_logprob": -0.09664666250850378, "compression_ratio": 1.7425742574257426, "no_speech_prob": 1.0188080068473937e-06}, {"id": 84, "seek": 55556, "start": 569.28, "end": 576.02, "text": " things like nn.linear in the past because PyTorch automatically uses nn.parameter internally", "tokens": [721, 411, 297, 77, 13, 28263, 294, 264, 1791, 570, 9953, 51, 284, 339, 6772, 4960, 297, 77, 13, 2181, 335, 2398, 19501], "temperature": 0.0, "avg_logprob": -0.09664666250850378, "compression_ratio": 1.7425742574257426, "no_speech_prob": 1.0188080068473937e-06}, {"id": 85, "seek": 55556, "start": 576.02, "end": 581.1999999999999, "text": " so if we have a look at the parameters for something that uses nn.linear linear with", "tokens": [370, 498, 321, 362, 257, 574, 412, 264, 9834, 337, 746, 300, 4960, 297, 77, 13, 28263, 8213, 365], "temperature": 0.0, "avg_logprob": -0.09664666250850378, "compression_ratio": 1.7425742574257426, "no_speech_prob": 1.0188080068473937e-06}, {"id": 86, "seek": 58120, "start": 581.2, "end": 590.8000000000001, "text": " no bias layer you'll see again we have here a parameter with three things in it. So we", "tokens": [572, 12577, 4583, 291, 603, 536, 797, 321, 362, 510, 257, 13075, 365, 1045, 721, 294, 309, 13, 407, 321], "temperature": 0.0, "avg_logprob": -0.08005543412833378, "compression_ratio": 1.962162162162162, "no_speech_prob": 1.4593740615964634e-06}, {"id": 87, "seek": 58120, "start": 590.8000000000001, "end": 596.8000000000001, "text": " want to in general be able to create a parameter so something with a tensor with a bunch of", "tokens": [528, 281, 294, 2674, 312, 1075, 281, 1884, 257, 13075, 370, 746, 365, 257, 40863, 365, 257, 3840, 295], "temperature": 0.0, "avg_logprob": -0.08005543412833378, "compression_ratio": 1.962162162162162, "no_speech_prob": 1.4593740615964634e-06}, {"id": 88, "seek": 58120, "start": 596.8000000000001, "end": 601.48, "text": " things in and generally we want to randomly initialize them so to randomly initialize", "tokens": [721, 294, 293, 5101, 321, 528, 281, 16979, 5883, 1125, 552, 370, 281, 16979, 5883, 1125], "temperature": 0.0, "avg_logprob": -0.08005543412833378, "compression_ratio": 1.962162162162162, "no_speech_prob": 1.4593740615964634e-06}, {"id": 89, "seek": 58120, "start": 601.48, "end": 607.9000000000001, "text": " we can pass in the size we want we can initialize a tensor of zeros of that size and then randomly", "tokens": [321, 393, 1320, 294, 264, 2744, 321, 528, 321, 393, 5883, 1125, 257, 40863, 295, 35193, 295, 300, 2744, 293, 550, 16979], "temperature": 0.0, "avg_logprob": -0.08005543412833378, "compression_ratio": 1.962162162162162, "no_speech_prob": 1.4593740615964634e-06}, {"id": 90, "seek": 60790, "start": 607.9, "end": 613.12, "text": " generate some normal normally distributed random numbers with a mean of zero and a deviation", "tokens": [8460, 512, 2710, 5646, 12631, 4974, 3547, 365, 257, 914, 295, 4018, 293, 257, 25163], "temperature": 0.0, "avg_logprob": -0.16594095513372137, "compression_ratio": 1.6984732824427482, "no_speech_prob": 7.453759849340713e-07}, {"id": 91, "seek": 60790, "start": 613.12, "end": 619.52, "text": " of.01 no particular reason I'm picking those numbers just know how this works so here's", "tokens": [295, 2411, 10607, 572, 1729, 1778, 286, 478, 8867, 729, 3547, 445, 458, 577, 341, 1985, 370, 510, 311], "temperature": 0.0, "avg_logprob": -0.16594095513372137, "compression_ratio": 1.6984732824427482, "no_speech_prob": 7.453759849340713e-07}, {"id": 92, "seek": 60790, "start": 619.52, "end": 624.92, "text": " something that will give us back a set of parameters of any size we want and so now", "tokens": [746, 300, 486, 976, 505, 646, 257, 992, 295, 9834, 295, 604, 2744, 321, 528, 293, 370, 586], "temperature": 0.0, "avg_logprob": -0.16594095513372137, "compression_ratio": 1.6984732824427482, "no_speech_prob": 7.453759849340713e-07}, {"id": 93, "seek": 60790, "start": 624.92, "end": 629.0799999999999, "text": " we're going to replace everywhere that used to say embedding I'm going to replace it with", "tokens": [321, 434, 516, 281, 7406, 5315, 300, 1143, 281, 584, 12240, 3584, 286, 478, 516, 281, 7406, 309, 365], "temperature": 0.0, "avg_logprob": -0.16594095513372137, "compression_ratio": 1.6984732824427482, "no_speech_prob": 7.453759849340713e-07}, {"id": 94, "seek": 60790, "start": 629.0799999999999, "end": 637.0, "text": " create params everything else here is the same in the init under init and then the forward", "tokens": [1884, 971, 4070, 1203, 1646, 510, 307, 264, 912, 294, 264, 3157, 833, 3157, 293, 550, 264, 2128], "temperature": 0.0, "avg_logprob": -0.16594095513372137, "compression_ratio": 1.6984732824427482, "no_speech_prob": 7.453759849340713e-07}, {"id": 95, "seek": 63700, "start": 637.0, "end": 643.28, "text": " is very very similar to before as you can see I'm grabbing the zero index column from", "tokens": [307, 588, 588, 2531, 281, 949, 382, 291, 393, 536, 286, 478, 23771, 264, 4018, 8186, 7738, 490], "temperature": 0.0, "avg_logprob": -0.09158716201782227, "compression_ratio": 1.7054263565891472, "no_speech_prob": 3.7479887282643176e-07}, {"id": 96, "seek": 63700, "start": 643.28, "end": 652.36, "text": " X that's my users and I just look it up as you see in that user factors array and the", "tokens": [1783, 300, 311, 452, 5022, 293, 286, 445, 574, 309, 493, 382, 291, 536, 294, 300, 4195, 6771, 10225, 293, 264], "temperature": 0.0, "avg_logprob": -0.09158716201782227, "compression_ratio": 1.7054263565891472, "no_speech_prob": 3.7479887282643176e-07}, {"id": 97, "seek": 63700, "start": 652.36, "end": 656.24, "text": " cool thing is I don't have to do anything with gradients myself for this manual embedding", "tokens": [1627, 551, 307, 286, 500, 380, 362, 281, 360, 1340, 365, 2771, 2448, 2059, 337, 341, 9688, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.09158716201782227, "compression_ratio": 1.7054263565891472, "no_speech_prob": 3.7479887282643176e-07}, {"id": 98, "seek": 63700, "start": 656.24, "end": 661.0, "text": " layer because PyTorch can figure out the gradients automatically as we've discussed so then I", "tokens": [4583, 570, 9953, 51, 284, 339, 393, 2573, 484, 264, 2771, 2448, 6772, 382, 321, 600, 7152, 370, 550, 286], "temperature": 0.0, "avg_logprob": -0.09158716201782227, "compression_ratio": 1.7054263565891472, "no_speech_prob": 3.7479887282643176e-07}, {"id": 99, "seek": 63700, "start": 661.0, "end": 665.4, "text": " just got the dot product as before add on the bias as before to the sigmoid range as", "tokens": [445, 658, 264, 5893, 1674, 382, 949, 909, 322, 264, 12577, 382, 949, 281, 264, 4556, 3280, 327, 3613, 382], "temperature": 0.0, "avg_logprob": -0.09158716201782227, "compression_ratio": 1.7054263565891472, "no_speech_prob": 3.7479887282643176e-07}, {"id": 100, "seek": 66540, "start": 665.4, "end": 674.64, "text": " before and so here's a dot product bias without any special PyTorch layers and we fit and", "tokens": [949, 293, 370, 510, 311, 257, 5893, 1674, 12577, 1553, 604, 2121, 9953, 51, 284, 339, 7914, 293, 321, 3318, 293], "temperature": 0.0, "avg_logprob": -0.0772161812617861, "compression_ratio": 1.5954545454545455, "no_speech_prob": 7.45376098620909e-07}, {"id": 101, "seek": 66540, "start": 674.64, "end": 681.52, "text": " we get the same result so I think that is pretty amazingly cool we've really shown that", "tokens": [321, 483, 264, 912, 1874, 370, 286, 519, 300, 307, 1238, 31762, 1627, 321, 600, 534, 4898, 300], "temperature": 0.0, "avg_logprob": -0.0772161812617861, "compression_ratio": 1.5954545454545455, "no_speech_prob": 7.45376098620909e-07}, {"id": 102, "seek": 66540, "start": 681.52, "end": 687.16, "text": " the embedding layer is nothing fancy it's nothing magic right it's just indexing into", "tokens": [264, 12240, 3584, 4583, 307, 1825, 10247, 309, 311, 1825, 5585, 558, 309, 311, 445, 8186, 278, 666], "temperature": 0.0, "avg_logprob": -0.0772161812617861, "compression_ratio": 1.5954545454545455, "no_speech_prob": 7.45376098620909e-07}, {"id": 103, "seek": 66540, "start": 687.16, "end": 694.52, "text": " an array so hopefully that removes a bit of the mystery for you so let's have a look at", "tokens": [364, 10225, 370, 4696, 300, 30445, 257, 857, 295, 264, 11422, 337, 291, 370, 718, 311, 362, 257, 574, 412], "temperature": 0.0, "avg_logprob": -0.0772161812617861, "compression_ratio": 1.5954545454545455, "no_speech_prob": 7.45376098620909e-07}, {"id": 104, "seek": 69452, "start": 694.52, "end": 699.4399999999999, "text": " this model that we've created and we've trained and find out what it's learned that's already", "tokens": [341, 2316, 300, 321, 600, 2942, 293, 321, 600, 8895, 293, 915, 484, 437, 309, 311, 3264, 300, 311, 1217], "temperature": 0.0, "avg_logprob": -0.09729811409923518, "compression_ratio": 1.7857142857142858, "no_speech_prob": 9.570795782565256e-07}, {"id": 105, "seek": 69452, "start": 699.4399999999999, "end": 704.68, "text": " useful we've got something we can make pretty accurate predictions with but let's find out", "tokens": [4420, 321, 600, 658, 746, 321, 393, 652, 1238, 8559, 21264, 365, 457, 718, 311, 915, 484], "temperature": 0.0, "avg_logprob": -0.09729811409923518, "compression_ratio": 1.7857142857142858, "no_speech_prob": 9.570795782565256e-07}, {"id": 106, "seek": 69452, "start": 704.68, "end": 712.52, "text": " what those what the model looks like so remember when we have a question okay let's take a", "tokens": [437, 729, 437, 264, 2316, 1542, 411, 370, 1604, 562, 321, 362, 257, 1168, 1392, 718, 311, 747, 257], "temperature": 0.0, "avg_logprob": -0.09729811409923518, "compression_ratio": 1.7857142857142858, "no_speech_prob": 9.570795782565256e-07}, {"id": 107, "seek": 69452, "start": 712.52, "end": 717.8, "text": " question before you can look at this what's the advantage of creating our own embedding", "tokens": [1168, 949, 291, 393, 574, 412, 341, 437, 311, 264, 5002, 295, 4084, 527, 1065, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.09729811409923518, "compression_ratio": 1.7857142857142858, "no_speech_prob": 9.570795782565256e-07}, {"id": 108, "seek": 69452, "start": 717.8, "end": 724.1999999999999, "text": " layer over the stock PyTorch one oh nothing at all we're just showing that we can it's", "tokens": [4583, 670, 264, 4127, 9953, 51, 284, 339, 472, 1954, 1825, 412, 439, 321, 434, 445, 4099, 300, 321, 393, 309, 311], "temperature": 0.0, "avg_logprob": -0.09729811409923518, "compression_ratio": 1.7857142857142858, "no_speech_prob": 9.570795782565256e-07}, {"id": 109, "seek": 72420, "start": 724.2, "end": 728.32, "text": " it's great to be able to dig under the surface because at some point you'll want to try doing", "tokens": [309, 311, 869, 281, 312, 1075, 281, 2528, 833, 264, 3753, 570, 412, 512, 935, 291, 603, 528, 281, 853, 884], "temperature": 0.0, "avg_logprob": -0.08210521585801069, "compression_ratio": 1.865546218487395, "no_speech_prob": 2.4060891519184224e-06}, {"id": 110, "seek": 72420, "start": 728.32, "end": 733.08, "text": " new things so a good way to learn to do new things is to be able to replicate things that", "tokens": [777, 721, 370, 257, 665, 636, 281, 1466, 281, 360, 777, 721, 307, 281, 312, 1075, 281, 25356, 721, 300], "temperature": 0.0, "avg_logprob": -0.08210521585801069, "compression_ratio": 1.865546218487395, "no_speech_prob": 2.4060891519184224e-06}, {"id": 111, "seek": 72420, "start": 733.08, "end": 737.4000000000001, "text": " already exist and you can expect that you understand how they work it's also a great", "tokens": [1217, 2514, 293, 291, 393, 2066, 300, 291, 1223, 577, 436, 589, 309, 311, 611, 257, 869], "temperature": 0.0, "avg_logprob": -0.08210521585801069, "compression_ratio": 1.865546218487395, "no_speech_prob": 2.4060891519184224e-06}, {"id": 112, "seek": 72420, "start": 737.4000000000001, "end": 742.44, "text": " way to understand the foundations of what's going on is to actually create in code your", "tokens": [636, 281, 1223, 264, 22467, 295, 437, 311, 516, 322, 307, 281, 767, 1884, 294, 3089, 428], "temperature": 0.0, "avg_logprob": -0.08210521585801069, "compression_ratio": 1.865546218487395, "no_speech_prob": 2.4060891519184224e-06}, {"id": 113, "seek": 72420, "start": 742.44, "end": 748.08, "text": " own implementation but I wouldn't expect you to use this implementation in practice but", "tokens": [1065, 11420, 457, 286, 2759, 380, 2066, 291, 281, 764, 341, 11420, 294, 3124, 457], "temperature": 0.0, "avg_logprob": -0.08210521585801069, "compression_ratio": 1.865546218487395, "no_speech_prob": 2.4060891519184224e-06}, {"id": 114, "seek": 74808, "start": 748.08, "end": 754.88, "text": " basically it removes all the mystery so if you remember we've created a learner called", "tokens": [1936, 309, 30445, 439, 264, 11422, 370, 498, 291, 1604, 321, 600, 2942, 257, 33347, 1219], "temperature": 0.0, "avg_logprob": -0.13235792329039756, "compression_ratio": 1.8512820512820514, "no_speech_prob": 5.989262490402325e-07}, {"id": 115, "seek": 74808, "start": 754.88, "end": 762.32, "text": " learn and to get to the model that's inside it you can always call learn dot model and", "tokens": [1466, 293, 281, 483, 281, 264, 2316, 300, 311, 1854, 309, 291, 393, 1009, 818, 1466, 5893, 2316, 293], "temperature": 0.0, "avg_logprob": -0.13235792329039756, "compression_ratio": 1.8512820512820514, "no_speech_prob": 5.989262490402325e-07}, {"id": 116, "seek": 74808, "start": 762.32, "end": 766.8000000000001, "text": " then inside that there's going to be automatically created for it well sorry not automatically", "tokens": [550, 1854, 300, 456, 311, 516, 281, 312, 6772, 2942, 337, 309, 731, 2597, 406, 6772], "temperature": 0.0, "avg_logprob": -0.13235792329039756, "compression_ratio": 1.8512820512820514, "no_speech_prob": 5.989262490402325e-07}, {"id": 117, "seek": 74808, "start": 766.8000000000001, "end": 773.2, "text": " we've created all these attributes movie factors movie bias bias and so forth so we can grab", "tokens": [321, 600, 2942, 439, 613, 17212, 3169, 6771, 3169, 12577, 12577, 293, 370, 5220, 370, 321, 393, 4444], "temperature": 0.0, "avg_logprob": -0.13235792329039756, "compression_ratio": 1.8512820512820514, "no_speech_prob": 5.989262490402325e-07}, {"id": 118, "seek": 77320, "start": 773.2, "end": 781.12, "text": " the learned up model dot movie bias and now what I'm going to do is I'm going to sort", "tokens": [264, 3264, 493, 2316, 5893, 3169, 12577, 293, 586, 437, 286, 478, 516, 281, 360, 307, 286, 478, 516, 281, 1333], "temperature": 0.0, "avg_logprob": -0.12585812945698582, "compression_ratio": 1.9329608938547487, "no_speech_prob": 1.9142861162890767e-07}, {"id": 119, "seek": 77320, "start": 781.12, "end": 788.48, "text": " that vector and I'm going to print out the first five titles and so what this is going", "tokens": [300, 8062, 293, 286, 478, 516, 281, 4482, 484, 264, 700, 1732, 12992, 293, 370, 437, 341, 307, 516], "temperature": 0.0, "avg_logprob": -0.12585812945698582, "compression_ratio": 1.9329608938547487, "no_speech_prob": 1.9142861162890767e-07}, {"id": 120, "seek": 77320, "start": 788.48, "end": 795.36, "text": " to do is it's going to print out though the movies with the smallest bias and here they", "tokens": [281, 360, 307, 309, 311, 516, 281, 4482, 484, 1673, 264, 6233, 365, 264, 16998, 12577, 293, 510, 436], "temperature": 0.0, "avg_logprob": -0.12585812945698582, "compression_ratio": 1.9329608938547487, "no_speech_prob": 1.9142861162890767e-07}, {"id": 121, "seek": 77320, "start": 795.36, "end": 802.6800000000001, "text": " are what does this mean well that kind of means these are the five movies that people", "tokens": [366, 437, 775, 341, 914, 731, 300, 733, 295, 1355, 613, 366, 264, 1732, 6233, 300, 561], "temperature": 0.0, "avg_logprob": -0.12585812945698582, "compression_ratio": 1.9329608938547487, "no_speech_prob": 1.9142861162890767e-07}, {"id": 122, "seek": 80268, "start": 802.68, "end": 809.04, "text": " really didn't like but it's more than that it's it's not only do people not like them", "tokens": [534, 994, 380, 411, 457, 309, 311, 544, 813, 300, 309, 311, 309, 311, 406, 787, 360, 561, 406, 411, 552], "temperature": 0.0, "avg_logprob": -0.11345634006318592, "compression_ratio": 1.745, "no_speech_prob": 1.1365580121491803e-06}, {"id": 123, "seek": 80268, "start": 809.04, "end": 814.56, "text": " but if we take account of the genre they're in the actors they have you know whatever", "tokens": [457, 498, 321, 747, 2696, 295, 264, 11022, 436, 434, 294, 264, 10037, 436, 362, 291, 458, 2035], "temperature": 0.0, "avg_logprob": -0.11345634006318592, "compression_ratio": 1.745, "no_speech_prob": 1.1365580121491803e-06}, {"id": 124, "seek": 80268, "start": 814.56, "end": 820.9399999999999, "text": " the latent factors are people liked them a lot less than they expected so maybe for example", "tokens": [264, 48994, 6771, 366, 561, 4501, 552, 257, 688, 1570, 813, 436, 5176, 370, 1310, 337, 1365], "temperature": 0.0, "avg_logprob": -0.11345634006318592, "compression_ratio": 1.745, "no_speech_prob": 1.1365580121491803e-06}, {"id": 125, "seek": 80268, "start": 820.9399999999999, "end": 825.2399999999999, "text": " people this is kind of I haven't seen any of these movies luckily but perhaps this is", "tokens": [561, 341, 307, 733, 295, 286, 2378, 380, 1612, 604, 295, 613, 6233, 22880, 457, 4317, 341, 307], "temperature": 0.0, "avg_logprob": -0.11345634006318592, "compression_ratio": 1.745, "no_speech_prob": 1.1365580121491803e-06}, {"id": 126, "seek": 82524, "start": 825.24, "end": 833.6, "text": " a sci-fi movie so people who kind of like these sci-fi movies found they're so bad they", "tokens": [257, 2180, 12, 13325, 3169, 370, 561, 567, 733, 295, 411, 613, 2180, 12, 13325, 6233, 1352, 436, 434, 370, 1578, 436], "temperature": 0.0, "avg_logprob": -0.09024519698564397, "compression_ratio": 1.7339901477832513, "no_speech_prob": 2.8736826607200783e-07}, {"id": 127, "seek": 82524, "start": 833.6, "end": 841.48, "text": " still didn't like it so we can do the exact opposite which is to sort sending and here", "tokens": [920, 994, 380, 411, 309, 370, 321, 393, 360, 264, 1900, 6182, 597, 307, 281, 1333, 7750, 293, 510], "temperature": 0.0, "avg_logprob": -0.09024519698564397, "compression_ratio": 1.7339901477832513, "no_speech_prob": 2.8736826607200783e-07}, {"id": 128, "seek": 82524, "start": 841.48, "end": 847.12, "text": " are the top five movies and specifically they're the top five by bias right so these are the", "tokens": [366, 264, 1192, 1732, 6233, 293, 4682, 436, 434, 264, 1192, 1732, 538, 12577, 558, 370, 613, 366, 264], "temperature": 0.0, "avg_logprob": -0.09024519698564397, "compression_ratio": 1.7339901477832513, "no_speech_prob": 2.8736826607200783e-07}, {"id": 129, "seek": 82524, "start": 847.12, "end": 851.44, "text": " movies that even after you take account of the fact that la confidential I have seen", "tokens": [6233, 300, 754, 934, 291, 747, 2696, 295, 264, 1186, 300, 635, 27054, 286, 362, 1612], "temperature": 0.0, "avg_logprob": -0.09024519698564397, "compression_ratio": 1.7339901477832513, "no_speech_prob": 2.8736826607200783e-07}, {"id": 130, "seek": 85144, "start": 851.44, "end": 858.0400000000001, "text": " all of these ones so la confidential is a kind of a murder mystery cop movie I guess", "tokens": [439, 295, 613, 2306, 370, 635, 27054, 307, 257, 733, 295, 257, 6568, 11422, 2971, 3169, 286, 2041], "temperature": 0.0, "avg_logprob": -0.13872389566330684, "compression_ratio": 1.7053140096618358, "no_speech_prob": 5.714994131267304e-07}, {"id": 131, "seek": 85144, "start": 858.0400000000001, "end": 862.36, "text": " and people who don't necessarily like that genre or I think guy Pierce was in it so maybe", "tokens": [293, 561, 567, 500, 380, 4725, 411, 300, 11022, 420, 286, 519, 2146, 45432, 390, 294, 309, 370, 1310], "temperature": 0.0, "avg_logprob": -0.13872389566330684, "compression_ratio": 1.7053140096618358, "no_speech_prob": 5.714994131267304e-07}, {"id": 132, "seek": 85144, "start": 862.36, "end": 866.36, "text": " they don't like guy Pierce very much whatever people still liked this movie more than they", "tokens": [436, 500, 380, 411, 2146, 45432, 588, 709, 2035, 561, 920, 4501, 341, 3169, 544, 813, 436], "temperature": 0.0, "avg_logprob": -0.13872389566330684, "compression_ratio": 1.7053140096618358, "no_speech_prob": 5.714994131267304e-07}, {"id": 133, "seek": 85144, "start": 866.36, "end": 873.0400000000001, "text": " expect so this is a kind of a nice thing that we can look inside our model and see what", "tokens": [2066, 370, 341, 307, 257, 733, 295, 257, 1481, 551, 300, 321, 393, 574, 1854, 527, 2316, 293, 536, 437], "temperature": 0.0, "avg_logprob": -0.13872389566330684, "compression_ratio": 1.7053140096618358, "no_speech_prob": 5.714994131267304e-07}, {"id": 134, "seek": 87304, "start": 873.04, "end": 883.7199999999999, "text": " it's learned we can look at not only at the bias vector but we can also look at the factors", "tokens": [309, 311, 3264, 321, 393, 574, 412, 406, 787, 412, 264, 12577, 8062, 457, 321, 393, 611, 574, 412, 264, 6771], "temperature": 0.0, "avg_logprob": -0.1331178665161133, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.3925425719207851e-06}, {"id": 135, "seek": 87304, "start": 883.7199999999999, "end": 889.0799999999999, "text": " now there are 50 factors which is too many to visualize so we could use a technique called", "tokens": [586, 456, 366, 2625, 6771, 597, 307, 886, 867, 281, 23273, 370, 321, 727, 764, 257, 6532, 1219], "temperature": 0.0, "avg_logprob": -0.1331178665161133, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.3925425719207851e-06}, {"id": 136, "seek": 87304, "start": 889.0799999999999, "end": 893.04, "text": " PCA principal components now this the details don't matter but basically they're going to", "tokens": [6465, 32, 9716, 6677, 586, 341, 264, 4365, 500, 380, 1871, 457, 1936, 436, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.1331178665161133, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.3925425719207851e-06}, {"id": 137, "seek": 87304, "start": 893.04, "end": 901.92, "text": " squish those 50 factors down to great and then we'll plot the top two as you can see", "tokens": [31379, 729, 2625, 6771, 760, 281, 869, 293, 550, 321, 603, 7542, 264, 1192, 732, 382, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.1331178665161133, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.3925425719207851e-06}, {"id": 138, "seek": 90192, "start": 901.92, "end": 909.24, "text": " here and what we see when we plot the top two is we can kind of see that the movies", "tokens": [510, 293, 437, 321, 536, 562, 321, 7542, 264, 1192, 732, 307, 321, 393, 733, 295, 536, 300, 264, 6233], "temperature": 0.0, "avg_logprob": -0.11222056056676286, "compression_ratio": 1.72, "no_speech_prob": 1.1015903282896033e-06}, {"id": 139, "seek": 90192, "start": 909.24, "end": 917.0, "text": " have been kind of spread out across a space of of some kind of latent factors and so if", "tokens": [362, 668, 733, 295, 3974, 484, 2108, 257, 1901, 295, 295, 512, 733, 295, 48994, 6771, 293, 370, 498], "temperature": 0.0, "avg_logprob": -0.11222056056676286, "compression_ratio": 1.72, "no_speech_prob": 1.1015903282896033e-06}, {"id": 140, "seek": 90192, "start": 917.0, "end": 924.12, "text": " you look at the far right there's a whole bunch of kind of big budget actually things", "tokens": [291, 574, 412, 264, 1400, 558, 456, 311, 257, 1379, 3840, 295, 733, 295, 955, 4706, 767, 721], "temperature": 0.0, "avg_logprob": -0.11222056056676286, "compression_ratio": 1.72, "no_speech_prob": 1.1015903282896033e-06}, {"id": 141, "seek": 90192, "start": 924.12, "end": 930.24, "text": " and on the far left there's more like cult kind of things Fargo Schindler's with Monty", "tokens": [293, 322, 264, 1400, 1411, 456, 311, 544, 411, 2376, 733, 295, 721, 9067, 1571, 2065, 471, 1918, 311, 365, 4713, 874], "temperature": 0.0, "avg_logprob": -0.11222056056676286, "compression_ratio": 1.72, "no_speech_prob": 1.1015903282896033e-06}, {"id": 142, "seek": 93024, "start": 930.24, "end": 940.2, "text": " Python by the same token at the bottom we've got some English patient Harry and Harry Met", "tokens": [15329, 538, 264, 912, 14862, 412, 264, 2767, 321, 600, 658, 512, 3669, 4537, 9378, 293, 9378, 6377], "temperature": 0.0, "avg_logprob": -0.12843096806452825, "compression_ratio": 1.5465116279069768, "no_speech_prob": 5.368734150579257e-07}, {"id": 143, "seek": 93024, "start": 940.2, "end": 948.52, "text": " Sally so kind of romance drama kind of stuff and at the top we've got action and sci-fi", "tokens": [26385, 370, 733, 295, 19064, 9412, 733, 295, 1507, 293, 412, 264, 1192, 321, 600, 658, 3069, 293, 2180, 12, 13325], "temperature": 0.0, "avg_logprob": -0.12843096806452825, "compression_ratio": 1.5465116279069768, "no_speech_prob": 5.368734150579257e-07}, {"id": 144, "seek": 93024, "start": 948.52, "end": 954.32, "text": " kind of stuff so you can see even though we haven't asked in any information about these", "tokens": [733, 295, 1507, 370, 291, 393, 536, 754, 1673, 321, 2378, 380, 2351, 294, 604, 1589, 466, 613], "temperature": 0.0, "avg_logprob": -0.12843096806452825, "compression_ratio": 1.5465116279069768, "no_speech_prob": 5.368734150579257e-07}, {"id": 145, "seek": 95432, "start": 954.32, "end": 962.44, "text": " movies all we've seen is who likes what these latent factors have automatically kind of", "tokens": [6233, 439, 321, 600, 1612, 307, 567, 5902, 437, 613, 48994, 6771, 362, 6772, 733, 295], "temperature": 0.0, "avg_logprob": -0.0887659979455265, "compression_ratio": 1.748792270531401, "no_speech_prob": 1.191102342090744e-06}, {"id": 146, "seek": 95432, "start": 962.44, "end": 967.6, "text": " figured out a space or a way of thinking about these movies based on what kinds of movies", "tokens": [8932, 484, 257, 1901, 420, 257, 636, 295, 1953, 466, 613, 6233, 2361, 322, 437, 3685, 295, 6233], "temperature": 0.0, "avg_logprob": -0.0887659979455265, "compression_ratio": 1.748792270531401, "no_speech_prob": 1.191102342090744e-06}, {"id": 147, "seek": 95432, "start": 967.6, "end": 972.5200000000001, "text": " people like and what are other kinds of movies they like along with those but that's really", "tokens": [561, 411, 293, 437, 366, 661, 3685, 295, 6233, 436, 411, 2051, 365, 729, 457, 300, 311, 534], "temperature": 0.0, "avg_logprob": -0.0887659979455265, "compression_ratio": 1.748792270531401, "no_speech_prob": 1.191102342090744e-06}, {"id": 148, "seek": 95432, "start": 972.5200000000001, "end": 982.96, "text": " interesting to kind of try and visualize what's going on inside your model now we don't have", "tokens": [1880, 281, 733, 295, 853, 293, 23273, 437, 311, 516, 322, 1854, 428, 2316, 586, 321, 500, 380, 362], "temperature": 0.0, "avg_logprob": -0.0887659979455265, "compression_ratio": 1.748792270531401, "no_speech_prob": 1.191102342090744e-06}, {"id": 149, "seek": 98296, "start": 982.96, "end": 990.2, "text": " to do all this manually we can actually just say give me a collab learner using this set", "tokens": [281, 360, 439, 341, 16945, 321, 393, 767, 445, 584, 976, 385, 257, 44228, 33347, 1228, 341, 992], "temperature": 0.0, "avg_logprob": -0.10822896283082288, "compression_ratio": 1.8299595141700404, "no_speech_prob": 1.7880546465676161e-06}, {"id": 150, "seek": 98296, "start": 990.2, "end": 994.72, "text": " of data loaders with this number of factors and this y-range and it does everything we've", "tokens": [295, 1412, 3677, 433, 365, 341, 1230, 295, 6771, 293, 341, 288, 12, 14521, 293, 309, 775, 1203, 321, 600], "temperature": 0.0, "avg_logprob": -0.10822896283082288, "compression_ratio": 1.8299595141700404, "no_speech_prob": 1.7880546465676161e-06}, {"id": 151, "seek": 98296, "start": 994.72, "end": 1001.48, "text": " just seen again about the same number okay so now you can see this is nice right we've", "tokens": [445, 1612, 797, 466, 264, 912, 1230, 1392, 370, 586, 291, 393, 536, 341, 307, 1481, 558, 321, 600], "temperature": 0.0, "avg_logprob": -0.10822896283082288, "compression_ratio": 1.8299595141700404, "no_speech_prob": 1.7880546465676161e-06}, {"id": 152, "seek": 98296, "start": 1001.48, "end": 1006.76, "text": " actually been able to see right underneath inside the collab learner part of the fastai", "tokens": [767, 668, 1075, 281, 536, 558, 7223, 1854, 264, 44228, 33347, 644, 295, 264, 2370, 1301], "temperature": 0.0, "avg_logprob": -0.10822896283082288, "compression_ratio": 1.8299595141700404, "no_speech_prob": 1.7880546465676161e-06}, {"id": 153, "seek": 98296, "start": 1006.76, "end": 1011.44, "text": " application the collaborative filtering application and we can build it all ourselves from scratch", "tokens": [3861, 264, 16555, 30822, 3861, 293, 321, 393, 1322, 309, 439, 4175, 490, 8459], "temperature": 0.0, "avg_logprob": -0.10822896283082288, "compression_ratio": 1.8299595141700404, "no_speech_prob": 1.7880546465676161e-06}, {"id": 154, "seek": 101144, "start": 1011.44, "end": 1016.8000000000001, "text": " we know how to create the SGD know how to create the embedding layer we know how to", "tokens": [321, 458, 577, 281, 1884, 264, 34520, 35, 458, 577, 281, 1884, 264, 12240, 3584, 4583, 321, 458, 577, 281], "temperature": 0.0, "avg_logprob": -0.11316645005170037, "compression_ratio": 1.7927461139896372, "no_speech_prob": 1.5534911881331936e-06}, {"id": 155, "seek": 101144, "start": 1016.8000000000001, "end": 1024.64, "text": " create the the model the architecture so now you can see you know we've really and build", "tokens": [1884, 264, 264, 2316, 264, 9482, 370, 586, 291, 393, 536, 291, 458, 321, 600, 534, 293, 1322], "temperature": 0.0, "avg_logprob": -0.11316645005170037, "compression_ratio": 1.7927461139896372, "no_speech_prob": 1.5534911881331936e-06}, {"id": 156, "seek": 101144, "start": 1024.64, "end": 1030.74, "text": " up from scratch our own version of this so if we just type learn dot model you can see", "tokens": [493, 490, 8459, 527, 1065, 3037, 295, 341, 370, 498, 321, 445, 2010, 1466, 5893, 2316, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.11316645005170037, "compression_ratio": 1.7927461139896372, "no_speech_prob": 1.5534911881331936e-06}, {"id": 157, "seek": 101144, "start": 1030.74, "end": 1035.92, "text": " here the names are a bit more generic this is a user weight item weight user bias item", "tokens": [510, 264, 5288, 366, 257, 857, 544, 19577, 341, 307, 257, 4195, 3364, 3174, 3364, 4195, 12577, 3174], "temperature": 0.0, "avg_logprob": -0.11316645005170037, "compression_ratio": 1.7927461139896372, "no_speech_prob": 1.5534911881331936e-06}, {"id": 158, "seek": 103592, "start": 1035.92, "end": 1042.0, "text": " bias but it's basically the same stuff we've seen before and we can replicate the exact", "tokens": [12577, 457, 309, 311, 1936, 264, 912, 1507, 321, 600, 1612, 949, 293, 321, 393, 25356, 264, 1900], "temperature": 0.0, "avg_logprob": -0.1187902647873451, "compression_ratio": 1.603658536585366, "no_speech_prob": 1.7061788639693987e-06}, {"id": 159, "seek": 103592, "start": 1042.0, "end": 1051.0800000000002, "text": " analysis we saw before by using this same idea okay slightly different order this time", "tokens": [5215, 321, 1866, 949, 538, 1228, 341, 912, 1558, 1392, 4748, 819, 1668, 341, 565], "temperature": 0.0, "avg_logprob": -0.1187902647873451, "compression_ratio": 1.603658536585366, "no_speech_prob": 1.7061788639693987e-06}, {"id": 160, "seek": 103592, "start": 1051.0800000000002, "end": 1059.4, "text": " because it's a bit random but pretty similar as well another interesting thing we can do", "tokens": [570, 309, 311, 257, 857, 4974, 457, 1238, 2531, 382, 731, 1071, 1880, 551, 321, 393, 360], "temperature": 0.0, "avg_logprob": -0.1187902647873451, "compression_ratio": 1.603658536585366, "no_speech_prob": 1.7061788639693987e-06}, {"id": 161, "seek": 105940, "start": 1059.4, "end": 1067.8400000000001, "text": " is we can think about the distance between two movies so let's grab all the movie factors", "tokens": [307, 321, 393, 519, 466, 264, 4560, 1296, 732, 6233, 370, 718, 311, 4444, 439, 264, 3169, 6771], "temperature": 0.0, "avg_logprob": -0.09335321676535685, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.0676995998437633e-06}, {"id": 162, "seek": 105940, "start": 1067.8400000000001, "end": 1079.3600000000001, "text": " or just pop them into a variable and then let's pick some movie and then let's find", "tokens": [420, 445, 1665, 552, 666, 257, 7006, 293, 550, 718, 311, 1888, 512, 3169, 293, 550, 718, 311, 915], "temperature": 0.0, "avg_logprob": -0.09335321676535685, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.0676995998437633e-06}, {"id": 163, "seek": 105940, "start": 1079.3600000000001, "end": 1087.48, "text": " the distance from that movie through every other movie and so one way of thinking about", "tokens": [264, 4560, 490, 300, 3169, 807, 633, 661, 3169, 293, 370, 472, 636, 295, 1953, 466], "temperature": 0.0, "avg_logprob": -0.09335321676535685, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.0676995998437633e-06}, {"id": 164, "seek": 108748, "start": 1087.48, "end": 1095.22, "text": " distance is you might recall the Pythagorean formula or the distance on a on the hypotenuse", "tokens": [4560, 307, 291, 1062, 9901, 264, 9953, 392, 559, 25885, 8513, 420, 264, 4560, 322, 257, 322, 264, 7420, 21990, 438], "temperature": 0.0, "avg_logprob": -0.1250187276483892, "compression_ratio": 1.7311320754716981, "no_speech_prob": 1.6536797602384468e-06}, {"id": 165, "seek": 108748, "start": 1095.22, "end": 1102.32, "text": " of a triangle which is also the distance to a point on it on a Cartesian plane on a chart", "tokens": [295, 257, 13369, 597, 307, 611, 264, 4560, 281, 257, 935, 322, 309, 322, 257, 22478, 42434, 5720, 322, 257, 6927], "temperature": 0.0, "avg_logprob": -0.1250187276483892, "compression_ratio": 1.7311320754716981, "no_speech_prob": 1.6536797602384468e-06}, {"id": 166, "seek": 108748, "start": 1102.32, "end": 1107.08, "text": " which is root x squared plus y squared you might know doesn't matter if you don't but", "tokens": [597, 307, 5593, 2031, 8889, 1804, 288, 8889, 291, 1062, 458, 1177, 380, 1871, 498, 291, 500, 380, 457], "temperature": 0.0, "avg_logprob": -0.1250187276483892, "compression_ratio": 1.7311320754716981, "no_speech_prob": 1.6536797602384468e-06}, {"id": 167, "seek": 108748, "start": 1107.08, "end": 1114.0, "text": " you can do exactly the same thing for 50 dimensions it doesn't just work for two dimensions there's", "tokens": [291, 393, 360, 2293, 264, 912, 551, 337, 2625, 12819, 309, 1177, 380, 445, 589, 337, 732, 12819, 456, 311], "temperature": 0.0, "avg_logprob": -0.1250187276483892, "compression_ratio": 1.7311320754716981, "no_speech_prob": 1.6536797602384468e-06}, {"id": 168, "seek": 111400, "start": 1114.0, "end": 1122.36, "text": " a so that tells you how far away a point is from another point if you if x and y are actually", "tokens": [257, 370, 300, 5112, 291, 577, 1400, 1314, 257, 935, 307, 490, 1071, 935, 498, 291, 498, 2031, 293, 288, 366, 767], "temperature": 0.0, "avg_logprob": -0.09394289601233698, "compression_ratio": 1.68125, "no_speech_prob": 4.247027334258746e-07}, {"id": 169, "seek": 111400, "start": 1122.36, "end": 1134.86, "text": " differences between two movie vectors so then what gets interesting is you can actually", "tokens": [7300, 1296, 732, 3169, 18875, 370, 550, 437, 2170, 1880, 307, 291, 393, 767], "temperature": 0.0, "avg_logprob": -0.09394289601233698, "compression_ratio": 1.68125, "no_speech_prob": 4.247027334258746e-07}, {"id": 170, "seek": 111400, "start": 1134.86, "end": 1141.56, "text": " then divide that kind of by the by the length to make all the lengths the same distance", "tokens": [550, 9845, 300, 733, 295, 538, 264, 538, 264, 4641, 281, 652, 439, 264, 26329, 264, 912, 4560], "temperature": 0.0, "avg_logprob": -0.09394289601233698, "compression_ratio": 1.68125, "no_speech_prob": 4.247027334258746e-07}, {"id": 171, "seek": 114156, "start": 1141.56, "end": 1146.32, "text": " to find out how the angle in any two movies and that actually turns out to be a really", "tokens": [281, 915, 484, 577, 264, 5802, 294, 604, 732, 6233, 293, 300, 767, 4523, 484, 281, 312, 257, 534], "temperature": 0.0, "avg_logprob": -0.1230677654868678, "compression_ratio": 1.7972972972972974, "no_speech_prob": 8.446205015388841e-07}, {"id": 172, "seek": 114156, "start": 1146.32, "end": 1151.04, "text": " good way to compare the similarity of two things that's called cosine similarity and", "tokens": [665, 636, 281, 6794, 264, 32194, 295, 732, 721, 300, 311, 1219, 23565, 32194, 293], "temperature": 0.0, "avg_logprob": -0.1230677654868678, "compression_ratio": 1.7972972972972974, "no_speech_prob": 8.446205015388841e-07}, {"id": 173, "seek": 114156, "start": 1151.04, "end": 1155.1599999999999, "text": " so the details don't matter you can look them up if you're interested but the basic idea", "tokens": [370, 264, 4365, 500, 380, 1871, 291, 393, 574, 552, 493, 498, 291, 434, 3102, 457, 264, 3875, 1558], "temperature": 0.0, "avg_logprob": -0.1230677654868678, "compression_ratio": 1.7972972972972974, "no_speech_prob": 8.446205015388841e-07}, {"id": 174, "seek": 114156, "start": 1155.1599999999999, "end": 1160.6, "text": " here is to see that we can actually pick a movie and find the movie that is the most", "tokens": [510, 307, 281, 536, 300, 321, 393, 767, 1888, 257, 3169, 293, 915, 264, 3169, 300, 307, 264, 881], "temperature": 0.0, "avg_logprob": -0.1230677654868678, "compression_ratio": 1.7972972972972974, "no_speech_prob": 8.446205015388841e-07}, {"id": 175, "seek": 114156, "start": 1160.6, "end": 1165.24, "text": " similar to it based on these factors and of interest.", "tokens": [2531, 281, 309, 2361, 322, 613, 6771, 293, 295, 1179, 13], "temperature": 0.0, "avg_logprob": -0.1230677654868678, "compression_ratio": 1.7972972972972974, "no_speech_prob": 8.446205015388841e-07}, {"id": 176, "seek": 116524, "start": 1165.24, "end": 1171.6, "text": " I have a question all right what motivated learning at a 50 dimensional embedding and", "tokens": [286, 362, 257, 1168, 439, 558, 437, 14515, 2539, 412, 257, 2625, 18795, 12240, 3584, 293], "temperature": 0.0, "avg_logprob": -0.17071137428283692, "compression_ratio": 1.6572769953051643, "no_speech_prob": 5.093661002320005e-06}, {"id": 177, "seek": 116524, "start": 1171.6, "end": 1177.6, "text": " then using a to reduce the three versus just learning a three-dimensional oh because the", "tokens": [550, 1228, 257, 281, 5407, 264, 1045, 5717, 445, 2539, 257, 1045, 12, 18759, 1954, 570, 264], "temperature": 0.0, "avg_logprob": -0.17071137428283692, "compression_ratio": 1.6572769953051643, "no_speech_prob": 5.093661002320005e-06}, {"id": 178, "seek": 116524, "start": 1177.6, "end": 1183.84, "text": " purpose of this was actually to create a good model so the the visualization part is normally", "tokens": [4334, 295, 341, 390, 767, 281, 1884, 257, 665, 2316, 370, 264, 264, 25801, 644, 307, 5646], "temperature": 0.0, "avg_logprob": -0.17071137428283692, "compression_ratio": 1.6572769953051643, "no_speech_prob": 5.093661002320005e-06}, {"id": 179, "seek": 116524, "start": 1183.84, "end": 1190.32, "text": " kind of the exploration of what's going in on in your model and so with a 50 with 50", "tokens": [733, 295, 264, 16197, 295, 437, 311, 516, 294, 322, 294, 428, 2316, 293, 370, 365, 257, 2625, 365, 2625], "temperature": 0.0, "avg_logprob": -0.17071137428283692, "compression_ratio": 1.6572769953051643, "no_speech_prob": 5.093661002320005e-06}, {"id": 180, "seek": 119032, "start": 1190.32, "end": 1197.1599999999999, "text": " latent factors you're going to get a more accurate so that's one approach is this dot", "tokens": [48994, 6771, 291, 434, 516, 281, 483, 257, 544, 8559, 370, 300, 311, 472, 3109, 307, 341, 5893], "temperature": 0.0, "avg_logprob": -0.1076642613352081, "compression_ratio": 1.8804347826086956, "no_speech_prob": 3.632672189723962e-07}, {"id": 181, "seek": 119032, "start": 1197.1599999999999, "end": 1206.2, "text": " product version there's another version we could use which is we could create a set of", "tokens": [1674, 3037, 456, 311, 1071, 3037, 321, 727, 764, 597, 307, 321, 727, 1884, 257, 992, 295], "temperature": 0.0, "avg_logprob": -0.1076642613352081, "compression_ratio": 1.8804347826086956, "no_speech_prob": 3.632672189723962e-07}, {"id": 182, "seek": 119032, "start": 1206.2, "end": 1215.12, "text": " user factors and a set of item factors and just like before we could look them up but", "tokens": [4195, 6771, 293, 257, 992, 295, 3174, 6771, 293, 445, 411, 949, 321, 727, 574, 552, 493, 457], "temperature": 0.0, "avg_logprob": -0.1076642613352081, "compression_ratio": 1.8804347826086956, "no_speech_prob": 3.632672189723962e-07}, {"id": 183, "seek": 119032, "start": 1215.12, "end": 1220.24, "text": " what we could then do instead of doing a dot product we could concatenate them together", "tokens": [437, 321, 727, 550, 360, 2602, 295, 884, 257, 5893, 1674, 321, 727, 1588, 7186, 473, 552, 1214], "temperature": 0.0, "avg_logprob": -0.1076642613352081, "compression_ratio": 1.8804347826086956, "no_speech_prob": 3.632672189723962e-07}, {"id": 184, "seek": 122024, "start": 1220.24, "end": 1227.96, "text": " to a tensor that contains both the user and the movie factors next to each other and then", "tokens": [281, 257, 40863, 300, 8306, 1293, 264, 4195, 293, 264, 3169, 6771, 958, 281, 1184, 661, 293, 550], "temperature": 0.0, "avg_logprob": -0.11266321950144582, "compression_ratio": 1.8518518518518519, "no_speech_prob": 3.2563082186243264e-07}, {"id": 185, "seek": 122024, "start": 1227.96, "end": 1236.52, "text": " we could pass them through a simple little neural network linear value linear and then", "tokens": [321, 727, 1320, 552, 807, 257, 2199, 707, 18161, 3209, 8213, 2158, 8213, 293, 550], "temperature": 0.0, "avg_logprob": -0.11266321950144582, "compression_ratio": 1.8518518518518519, "no_speech_prob": 3.2563082186243264e-07}, {"id": 186, "seek": 122024, "start": 1236.52, "end": 1242.64, "text": " sigmoid range as before so importantly here the first linear layer the number of inputs", "tokens": [4556, 3280, 327, 3613, 382, 949, 370, 8906, 510, 264, 700, 8213, 4583, 264, 1230, 295, 15743], "temperature": 0.0, "avg_logprob": -0.11266321950144582, "compression_ratio": 1.8518518518518519, "no_speech_prob": 3.2563082186243264e-07}, {"id": 187, "seek": 122024, "start": 1242.64, "end": 1249.52, "text": " is equal to the number of user factors plus the number of item factors and the number", "tokens": [307, 2681, 281, 264, 1230, 295, 4195, 6771, 1804, 264, 1230, 295, 3174, 6771, 293, 264, 1230], "temperature": 0.0, "avg_logprob": -0.11266321950144582, "compression_ratio": 1.8518518518518519, "no_speech_prob": 3.2563082186243264e-07}, {"id": 188, "seek": 124952, "start": 1249.52, "end": 1258.4, "text": " of outputs is however many activations we have and then which we just default to a hundred", "tokens": [295, 23930, 307, 4461, 867, 2430, 763, 321, 362, 293, 550, 597, 321, 445, 7576, 281, 257, 3262], "temperature": 0.0, "avg_logprob": -0.12270372254507882, "compression_ratio": 1.756218905472637, "no_speech_prob": 5.539163794310298e-07}, {"id": 189, "seek": 124952, "start": 1258.4, "end": 1262.34, "text": " here and then the final layer will go from a hundred to one because we're just making", "tokens": [510, 293, 550, 264, 2572, 4583, 486, 352, 490, 257, 3262, 281, 472, 570, 321, 434, 445, 1455], "temperature": 0.0, "avg_logprob": -0.12270372254507882, "compression_ratio": 1.756218905472637, "no_speech_prob": 5.539163794310298e-07}, {"id": 190, "seek": 124952, "start": 1262.34, "end": 1268.36, "text": " one prediction and so we could create or call that collab and then we can instantiate that", "tokens": [472, 17630, 293, 370, 321, 727, 1884, 420, 818, 300, 44228, 293, 550, 321, 393, 9836, 13024, 300], "temperature": 0.0, "avg_logprob": -0.12270372254507882, "compression_ratio": 1.756218905472637, "no_speech_prob": 5.539163794310298e-07}, {"id": 191, "seek": 124952, "start": 1268.36, "end": 1275.08, "text": " to create a model we can create a learner and we can fit it's not going quite as well", "tokens": [281, 1884, 257, 2316, 321, 393, 1884, 257, 33347, 293, 321, 393, 3318, 309, 311, 406, 516, 1596, 382, 731], "temperature": 0.0, "avg_logprob": -0.12270372254507882, "compression_ratio": 1.756218905472637, "no_speech_prob": 5.539163794310298e-07}, {"id": 192, "seek": 127508, "start": 1275.08, "end": 1280.6799999999998, "text": " as before it's not terrible but it's not quite as good as our dot product version but the", "tokens": [382, 949, 309, 311, 406, 6237, 457, 309, 311, 406, 1596, 382, 665, 382, 527, 5893, 1674, 3037, 457, 264], "temperature": 0.0, "avg_logprob": -0.10652946564088385, "compression_ratio": 1.6875, "no_speech_prob": 7.002159918556572e-07}, {"id": 193, "seek": 127508, "start": 1280.6799999999998, "end": 1285.04, "text": " interesting thing here is it does give us some more flexibility which is that since", "tokens": [1880, 551, 510, 307, 309, 775, 976, 505, 512, 544, 12635, 597, 307, 300, 1670], "temperature": 0.0, "avg_logprob": -0.10652946564088385, "compression_ratio": 1.6875, "no_speech_prob": 7.002159918556572e-07}, {"id": 194, "seek": 127508, "start": 1285.04, "end": 1292.1999999999998, "text": " we're not doing a dot product we can actually have a different embedding size for each of", "tokens": [321, 434, 406, 884, 257, 5893, 1674, 321, 393, 767, 362, 257, 819, 12240, 3584, 2744, 337, 1184, 295], "temperature": 0.0, "avg_logprob": -0.10652946564088385, "compression_ratio": 1.6875, "no_speech_prob": 7.002159918556572e-07}, {"id": 195, "seek": 127508, "start": 1292.1999999999998, "end": 1297.36, "text": " users versus items and actually fastai has a simple heuristic if you call get embedding", "tokens": [5022, 5717, 4754, 293, 767, 2370, 1301, 575, 257, 2199, 415, 374, 3142, 498, 291, 818, 483, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.10652946564088385, "compression_ratio": 1.6875, "no_speech_prob": 7.002159918556572e-07}, {"id": 196, "seek": 129736, "start": 1297.36, "end": 1305.56, "text": " size and pass in your data loaders it will suggest appropriate size embedding matrices", "tokens": [2744, 293, 1320, 294, 428, 1412, 3677, 433, 309, 486, 3402, 6854, 2744, 12240, 3584, 32284], "temperature": 0.0, "avg_logprob": -0.2637801700168186, "compression_ratio": 1.6477987421383649, "no_speech_prob": 2.457990433413215e-07}, {"id": 197, "seek": 129736, "start": 1305.56, "end": 1316.6399999999999, "text": " for each of your categorical variables each of your user and item sensors so that's so", "tokens": [337, 1184, 295, 428, 19250, 804, 9102, 1184, 295, 428, 4195, 293, 3174, 14840, 370, 300, 311, 370], "temperature": 0.0, "avg_logprob": -0.2637801700168186, "compression_ratio": 1.6477987421383649, "no_speech_prob": 2.457990433413215e-07}, {"id": 198, "seek": 129736, "start": 1316.6399999999999, "end": 1327.28, "text": " if we pass in star M's settings that's going to pass in the user couple and the item and", "tokens": [498, 321, 1320, 294, 3543, 376, 311, 6257, 300, 311, 516, 281, 1320, 294, 264, 4195, 1916, 293, 264, 3174, 293], "temperature": 0.0, "avg_logprob": -0.2637801700168186, "compression_ratio": 1.6477987421383649, "no_speech_prob": 2.457990433413215e-07}, {"id": 199, "seek": 132728, "start": 1327.28, "end": 1333.36, "text": " couple which we can then pass to embedding this is his star prefix we learned about in", "tokens": [1916, 597, 321, 393, 550, 1320, 281, 12240, 3584, 341, 307, 702, 3543, 46969, 321, 3264, 466, 294], "temperature": 0.0, "avg_logprob": -0.13362664802401675, "compression_ratio": 1.73046875, "no_speech_prob": 1.003010879685462e-06}, {"id": 200, "seek": 132728, "start": 1333.36, "end": 1341.16, "text": " the last class in case you forgot so this is kind of interesting we can you know we", "tokens": [264, 1036, 1508, 294, 1389, 291, 5298, 370, 341, 307, 733, 295, 1880, 321, 393, 291, 458, 321], "temperature": 0.0, "avg_logprob": -0.13362664802401675, "compression_ratio": 1.73046875, "no_speech_prob": 1.003010879685462e-06}, {"id": 201, "seek": 132728, "start": 1341.16, "end": 1345.34, "text": " can see here that there's two different architectures we could pick from it wouldn't be necessarily", "tokens": [393, 536, 510, 300, 456, 311, 732, 819, 6331, 1303, 321, 727, 1888, 490, 309, 2759, 380, 312, 4725], "temperature": 0.0, "avg_logprob": -0.13362664802401675, "compression_ratio": 1.73046875, "no_speech_prob": 1.003010879685462e-06}, {"id": 202, "seek": 132728, "start": 1345.34, "end": 1349.58, "text": " obvious ahead of time which one's going to work better I'm in this particular case the", "tokens": [6322, 2286, 295, 565, 597, 472, 311, 516, 281, 589, 1101, 286, 478, 294, 341, 1729, 1389, 264], "temperature": 0.0, "avg_logprob": -0.13362664802401675, "compression_ratio": 1.73046875, "no_speech_prob": 1.003010879685462e-06}, {"id": 203, "seek": 132728, "start": 1349.58, "end": 1354.0, "text": " simplest one the dot product one actually turned out to be work a bit better which is", "tokens": [22811, 472, 264, 5893, 1674, 472, 767, 3574, 484, 281, 312, 589, 257, 857, 1101, 597, 307], "temperature": 0.0, "avg_logprob": -0.13362664802401675, "compression_ratio": 1.73046875, "no_speech_prob": 1.003010879685462e-06}, {"id": 204, "seek": 135400, "start": 1354.0, "end": 1361.2, "text": " interesting this particular version here if you call collab learner and pass use nn equals", "tokens": [1880, 341, 1729, 3037, 510, 498, 291, 818, 44228, 33347, 293, 1320, 764, 297, 77, 6915], "temperature": 0.0, "avg_logprob": -0.19422957337932822, "compression_ratio": 1.8368421052631578, "no_speech_prob": 2.0061534655724245e-07}, {"id": 205, "seek": 135400, "start": 1361.2, "end": 1366.64, "text": " true then what that's going to do is it's going to use this version the version with", "tokens": [2074, 550, 437, 300, 311, 516, 281, 360, 307, 309, 311, 516, 281, 764, 341, 3037, 264, 3037, 365], "temperature": 0.0, "avg_logprob": -0.19422957337932822, "compression_ratio": 1.8368421052631578, "no_speech_prob": 2.0061534655724245e-07}, {"id": 206, "seek": 135400, "start": 1366.64, "end": 1377.84, "text": " the concatenation and the linear layers so collab learner use nn equals true again we", "tokens": [264, 1588, 7186, 399, 293, 264, 8213, 7914, 370, 44228, 33347, 764, 297, 77, 6915, 2074, 797, 321], "temperature": 0.0, "avg_logprob": -0.19422957337932822, "compression_ratio": 1.8368421052631578, "no_speech_prob": 2.0061534655724245e-07}, {"id": 207, "seek": 135400, "start": 1377.84, "end": 1383.68, "text": " get about the same result as you'd expect because it's just a shortcut for this version", "tokens": [483, 466, 264, 912, 1874, 382, 291, 1116, 2066, 570, 309, 311, 445, 257, 24822, 337, 341, 3037], "temperature": 0.0, "avg_logprob": -0.19422957337932822, "compression_ratio": 1.8368421052631578, "no_speech_prob": 2.0061534655724245e-07}, {"id": 208, "seek": 138368, "start": 1383.68, "end": 1390.04, "text": " and it's interesting actually we have a look at collab learner it actually returns an object", "tokens": [293, 309, 311, 1880, 767, 321, 362, 257, 574, 412, 44228, 33347, 309, 767, 11247, 364, 2657], "temperature": 0.0, "avg_logprob": -0.09738402183239277, "compression_ratio": 1.7658730158730158, "no_speech_prob": 1.287892814616498e-06}, {"id": 209, "seek": 138368, "start": 1390.04, "end": 1395.0, "text": " of type embedding and n and it's kind of cool if you look inside the fastai source code", "tokens": [295, 2010, 12240, 3584, 293, 297, 293, 309, 311, 733, 295, 1627, 498, 291, 574, 1854, 264, 2370, 1301, 4009, 3089], "temperature": 0.0, "avg_logprob": -0.09738402183239277, "compression_ratio": 1.7658730158730158, "no_speech_prob": 1.287892814616498e-06}, {"id": 210, "seek": 138368, "start": 1395.0, "end": 1399.0, "text": " or use the double question mark trick to see the source code for embedding in n you'll", "tokens": [420, 764, 264, 3834, 1168, 1491, 4282, 281, 536, 264, 4009, 3089, 337, 12240, 3584, 294, 297, 291, 603], "temperature": 0.0, "avg_logprob": -0.09738402183239277, "compression_ratio": 1.7658730158730158, "no_speech_prob": 1.287892814616498e-06}, {"id": 211, "seek": 138368, "start": 1399.0, "end": 1404.8400000000001, "text": " see it's three lines of code how does that happen because we're using this thing called", "tokens": [536, 309, 311, 1045, 3876, 295, 3089, 577, 775, 300, 1051, 570, 321, 434, 1228, 341, 551, 1219], "temperature": 0.0, "avg_logprob": -0.09738402183239277, "compression_ratio": 1.7658730158730158, "no_speech_prob": 1.287892814616498e-06}, {"id": 212, "seek": 138368, "start": 1404.8400000000001, "end": 1413.44, "text": " tabular model which we will learn about in a moment but basically this neural net version", "tokens": [4421, 1040, 2316, 597, 321, 486, 1466, 466, 294, 257, 1623, 457, 1936, 341, 18161, 2533, 3037], "temperature": 0.0, "avg_logprob": -0.09738402183239277, "compression_ratio": 1.7658730158730158, "no_speech_prob": 1.287892814616498e-06}, {"id": 213, "seek": 141344, "start": 1413.44, "end": 1419.68, "text": " of collaborative filtering is literally just a tabular model in which we pass no continuous", "tokens": [295, 16555, 30822, 307, 3736, 445, 257, 4421, 1040, 2316, 294, 597, 321, 1320, 572, 10957], "temperature": 0.0, "avg_logprob": -0.08446423630965383, "compression_ratio": 1.6926605504587156, "no_speech_prob": 1.392543822476e-06}, {"id": 214, "seek": 141344, "start": 1419.68, "end": 1432.16, "text": " variables and some embedding sizes so we'll see that in a moment okay so that is collaborative", "tokens": [9102, 293, 512, 12240, 3584, 11602, 370, 321, 603, 536, 300, 294, 257, 1623, 1392, 370, 300, 307, 16555], "temperature": 0.0, "avg_logprob": -0.08446423630965383, "compression_ratio": 1.6926605504587156, "no_speech_prob": 1.392543822476e-06}, {"id": 215, "seek": 141344, "start": 1432.16, "end": 1436.8, "text": " filtering and again take a look at the further research section in particular after you finish", "tokens": [30822, 293, 797, 747, 257, 574, 412, 264, 3052, 2132, 3541, 294, 1729, 934, 291, 2413], "temperature": 0.0, "avg_logprob": -0.08446423630965383, "compression_ratio": 1.6926605504587156, "no_speech_prob": 1.392543822476e-06}, {"id": 216, "seek": 141344, "start": 1436.8, "end": 1442.0, "text": " the questionnaire because there's some really important next steps you can take to push", "tokens": [264, 44702, 570, 456, 311, 512, 534, 1021, 958, 4439, 291, 393, 747, 281, 2944], "temperature": 0.0, "avg_logprob": -0.08446423630965383, "compression_ratio": 1.6926605504587156, "no_speech_prob": 1.392543822476e-06}, {"id": 217, "seek": 144200, "start": 1442.0, "end": 1446.64, "text": " your knowledge and your skills.", "tokens": [428, 3601, 293, 428, 3942, 13], "temperature": 0.0, "avg_logprob": -0.11036518828509605, "compression_ratio": 1.5978260869565217, "no_speech_prob": 6.681505624328565e-07}, {"id": 218, "seek": 144200, "start": 1446.64, "end": 1454.28, "text": " So let's now move to notebook 9, tabular and we're going to look at tabular modeling and", "tokens": [407, 718, 311, 586, 1286, 281, 21060, 1722, 11, 4421, 1040, 293, 321, 434, 516, 281, 574, 412, 4421, 1040, 15983, 293], "temperature": 0.0, "avg_logprob": -0.11036518828509605, "compression_ratio": 1.5978260869565217, "no_speech_prob": 6.681505624328565e-07}, {"id": 219, "seek": 144200, "start": 1454.28, "end": 1459.2, "text": " do a deep dive and let's start by talking about this idea that we were starting to see", "tokens": [360, 257, 2452, 9192, 293, 718, 311, 722, 538, 1417, 466, 341, 1558, 300, 321, 645, 2891, 281, 536], "temperature": 0.0, "avg_logprob": -0.11036518828509605, "compression_ratio": 1.5978260869565217, "no_speech_prob": 6.681505624328565e-07}, {"id": 220, "seek": 144200, "start": 1459.2, "end": 1468.12, "text": " here which is embeddings and specifically let's move beyond just having embeddings for", "tokens": [510, 597, 307, 12240, 29432, 293, 4682, 718, 311, 1286, 4399, 445, 1419, 12240, 29432, 337], "temperature": 0.0, "avg_logprob": -0.11036518828509605, "compression_ratio": 1.5978260869565217, "no_speech_prob": 6.681505624328565e-07}, {"id": 221, "seek": 146812, "start": 1468.12, "end": 1474.9199999999998, "text": " users and items but embeddings for any kind of categorical variable but really because", "tokens": [5022, 293, 4754, 457, 12240, 29432, 337, 604, 733, 295, 19250, 804, 7006, 457, 534, 570], "temperature": 0.0, "avg_logprob": -0.11776029146634616, "compression_ratio": 1.8350515463917525, "no_speech_prob": 3.6898796906825737e-07}, {"id": 222, "seek": 146812, "start": 1474.9199999999998, "end": 1483.7199999999998, "text": " we know an embedding is just a lookup into an array it can handle any kind of discrete", "tokens": [321, 458, 364, 12240, 3584, 307, 445, 257, 574, 1010, 666, 364, 10225, 309, 393, 4813, 604, 733, 295, 27706], "temperature": 0.0, "avg_logprob": -0.11776029146634616, "compression_ratio": 1.8350515463917525, "no_speech_prob": 3.6898796906825737e-07}, {"id": 223, "seek": 146812, "start": 1483.7199999999998, "end": 1489.04, "text": " categorical data so things like age are not discrete they're continuous numerical data", "tokens": [19250, 804, 1412, 370, 721, 411, 3205, 366, 406, 27706, 436, 434, 10957, 29054, 1412], "temperature": 0.0, "avg_logprob": -0.11776029146634616, "compression_ratio": 1.8350515463917525, "no_speech_prob": 3.6898796906825737e-07}, {"id": 224, "seek": 146812, "start": 1489.04, "end": 1496.4799999999998, "text": " but something like sex or postcode categorical variables they have a certain number of discrete", "tokens": [457, 746, 411, 3260, 420, 2183, 22332, 19250, 804, 9102, 436, 362, 257, 1629, 1230, 295, 27706], "temperature": 0.0, "avg_logprob": -0.11776029146634616, "compression_ratio": 1.8350515463917525, "no_speech_prob": 3.6898796906825737e-07}, {"id": 225, "seek": 149648, "start": 1496.48, "end": 1502.2, "text": " levels the number of discrete levels they have is called their cardinality.", "tokens": [4358, 264, 1230, 295, 27706, 4358, 436, 362, 307, 1219, 641, 2920, 259, 1860, 13], "temperature": 0.0, "avg_logprob": -0.09372926962496055, "compression_ratio": 1.7418032786885247, "no_speech_prob": 1.3081720453556045e-06}, {"id": 226, "seek": 149648, "start": 1502.2, "end": 1508.8, "text": " So to have a look at an example of a data set that contains both categorical and continuous", "tokens": [407, 281, 362, 257, 574, 412, 364, 1365, 295, 257, 1412, 992, 300, 8306, 1293, 19250, 804, 293, 10957], "temperature": 0.0, "avg_logprob": -0.09372926962496055, "compression_ratio": 1.7418032786885247, "no_speech_prob": 1.3081720453556045e-06}, {"id": 227, "seek": 149648, "start": 1508.8, "end": 1515.24, "text": " variables we're going to look at the Rossman sales competition that ran on Kaggle a few", "tokens": [9102, 321, 434, 516, 281, 574, 412, 264, 16140, 1601, 5763, 6211, 300, 5872, 322, 48751, 22631, 257, 1326], "temperature": 0.0, "avg_logprob": -0.09372926962496055, "compression_ratio": 1.7418032786885247, "no_speech_prob": 1.3081720453556045e-06}, {"id": 228, "seek": 149648, "start": 1515.24, "end": 1520.3600000000001, "text": " years ago and so basically what's going to happen is we're going to see a table that", "tokens": [924, 2057, 293, 370, 1936, 437, 311, 516, 281, 1051, 307, 321, 434, 516, 281, 536, 257, 3199, 300], "temperature": 0.0, "avg_logprob": -0.09372926962496055, "compression_ratio": 1.7418032786885247, "no_speech_prob": 1.3081720453556045e-06}, {"id": 229, "seek": 149648, "start": 1520.3600000000001, "end": 1525.56, "text": " contains information about various stores in Germany and the goal will be to try and", "tokens": [8306, 1589, 466, 3683, 9512, 294, 7244, 293, 264, 3387, 486, 312, 281, 853, 293], "temperature": 0.0, "avg_logprob": -0.09372926962496055, "compression_ratio": 1.7418032786885247, "no_speech_prob": 1.3081720453556045e-06}, {"id": 230, "seek": 152556, "start": 1525.56, "end": 1531.24, "text": " predict how many sales there's going to be for each day in a couple of week period for", "tokens": [6069, 577, 867, 5763, 456, 311, 516, 281, 312, 337, 1184, 786, 294, 257, 1916, 295, 1243, 2896, 337], "temperature": 0.0, "avg_logprob": -0.12890846589032343, "compression_ratio": 1.7112068965517242, "no_speech_prob": 3.6688338695967104e-06}, {"id": 231, "seek": 152556, "start": 1531.24, "end": 1534.76, "text": " each store.", "tokens": [1184, 3531, 13], "temperature": 0.0, "avg_logprob": -0.12890846589032343, "compression_ratio": 1.7112068965517242, "no_speech_prob": 3.6688338695967104e-06}, {"id": 232, "seek": 152556, "start": 1534.76, "end": 1540.04, "text": " One of the interesting things about this competition is that one of the gold medalists used deep", "tokens": [1485, 295, 264, 1880, 721, 466, 341, 6211, 307, 300, 472, 295, 264, 3821, 21364, 1751, 1143, 2452], "temperature": 0.0, "avg_logprob": -0.12890846589032343, "compression_ratio": 1.7112068965517242, "no_speech_prob": 3.6688338695967104e-06}, {"id": 233, "seek": 152556, "start": 1540.04, "end": 1546.04, "text": " learning and it was one of the earliest known examples of a state-of-the-art deep learning", "tokens": [2539, 293, 309, 390, 472, 295, 264, 20573, 2570, 5110, 295, 257, 1785, 12, 2670, 12, 3322, 12, 446, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.12890846589032343, "compression_ratio": 1.7112068965517242, "no_speech_prob": 3.6688338695967104e-06}, {"id": 234, "seek": 152556, "start": 1546.04, "end": 1547.04, "text": " tabular model.", "tokens": [4421, 1040, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12890846589032343, "compression_ratio": 1.7112068965517242, "no_speech_prob": 3.6688338695967104e-06}, {"id": 235, "seek": 152556, "start": 1547.04, "end": 1554.04, "text": " I mean this is not long ago 2015 or something but really this idea of creating state-of-the-art", "tokens": [286, 914, 341, 307, 406, 938, 2057, 7546, 420, 746, 457, 534, 341, 1558, 295, 4084, 1785, 12, 2670, 12, 3322, 12, 446], "temperature": 0.0, "avg_logprob": -0.12890846589032343, "compression_ratio": 1.7112068965517242, "no_speech_prob": 3.6688338695967104e-06}, {"id": 236, "seek": 155404, "start": 1554.04, "end": 1561.44, "text": " tabular models with deep learning has not been very common and for not very long.", "tokens": [4421, 1040, 5245, 365, 2452, 2539, 575, 406, 668, 588, 2689, 293, 337, 406, 588, 938, 13], "temperature": 0.0, "avg_logprob": -0.10940369521037187, "compression_ratio": 1.6804511278195489, "no_speech_prob": 1.733039994178398e-06}, {"id": 237, "seek": 155404, "start": 1561.44, "end": 1565.84, "text": " You know interestingly compared to the other gold medalists in this competition the folks", "tokens": [509, 458, 25873, 5347, 281, 264, 661, 3821, 21364, 1751, 294, 341, 6211, 264, 4024], "temperature": 0.0, "avg_logprob": -0.10940369521037187, "compression_ratio": 1.6804511278195489, "no_speech_prob": 1.733039994178398e-06}, {"id": 238, "seek": 155404, "start": 1565.84, "end": 1571.0, "text": " that use deep learning used a lot less feature engineering and a lot less domain expertise", "tokens": [300, 764, 2452, 2539, 1143, 257, 688, 1570, 4111, 7043, 293, 257, 688, 1570, 9274, 11769], "temperature": 0.0, "avg_logprob": -0.10940369521037187, "compression_ratio": 1.6804511278195489, "no_speech_prob": 1.733039994178398e-06}, {"id": 239, "seek": 155404, "start": 1571.0, "end": 1577.08, "text": " and so they wrote a paper called Entity Embeddings of Categorical Variables in which they basically", "tokens": [293, 370, 436, 4114, 257, 3035, 1219, 3951, 507, 24234, 292, 29432, 295, 383, 2968, 284, 804, 32511, 2965, 294, 597, 436, 1936], "temperature": 0.0, "avg_logprob": -0.10940369521037187, "compression_ratio": 1.6804511278195489, "no_speech_prob": 1.733039994178398e-06}, {"id": 240, "seek": 155404, "start": 1577.08, "end": 1583.36, "text": " described the exact thing that you saw in the in notebook 8 the way you can think of", "tokens": [7619, 264, 1900, 551, 300, 291, 1866, 294, 264, 294, 21060, 1649, 264, 636, 291, 393, 519, 295], "temperature": 0.0, "avg_logprob": -0.10940369521037187, "compression_ratio": 1.6804511278195489, "no_speech_prob": 1.733039994178398e-06}, {"id": 241, "seek": 158336, "start": 1583.36, "end": 1588.84, "text": " one-hot encodings as just being embeddings you can catenate them together and you can", "tokens": [472, 12, 12194, 2058, 378, 1109, 382, 445, 885, 12240, 29432, 291, 393, 3857, 268, 473, 552, 1214, 293, 291, 393], "temperature": 0.0, "avg_logprob": -0.12075947721799214, "compression_ratio": 1.7136752136752136, "no_speech_prob": 1.816209305616212e-06}, {"id": 242, "seek": 158336, "start": 1588.84, "end": 1593.8, "text": " put them through a couple of layers they call them dense layers we've called them linear", "tokens": [829, 552, 807, 257, 1916, 295, 7914, 436, 818, 552, 18011, 7914, 321, 600, 1219, 552, 8213], "temperature": 0.0, "avg_logprob": -0.12075947721799214, "compression_ratio": 1.7136752136752136, "no_speech_prob": 1.816209305616212e-06}, {"id": 243, "seek": 158336, "start": 1593.8, "end": 1598.36, "text": " layers and create a neural network out of that.", "tokens": [7914, 293, 1884, 257, 18161, 3209, 484, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.12075947721799214, "compression_ratio": 1.7136752136752136, "no_speech_prob": 1.816209305616212e-06}, {"id": 244, "seek": 158336, "start": 1598.36, "end": 1605.1599999999999, "text": " So this is really a neat you know kind of simple and obvious hindsight trick and they", "tokens": [407, 341, 307, 534, 257, 10654, 291, 458, 733, 295, 2199, 293, 6322, 44357, 4282, 293, 436], "temperature": 0.0, "avg_logprob": -0.12075947721799214, "compression_ratio": 1.7136752136752136, "no_speech_prob": 1.816209305616212e-06}, {"id": 245, "seek": 158336, "start": 1605.1599999999999, "end": 1611.4799999999998, "text": " actually did exactly what we did in the paper which is to look at the results of the trained", "tokens": [767, 630, 2293, 437, 321, 630, 294, 264, 3035, 597, 307, 281, 574, 412, 264, 3542, 295, 264, 8895], "temperature": 0.0, "avg_logprob": -0.12075947721799214, "compression_ratio": 1.7136752136752136, "no_speech_prob": 1.816209305616212e-06}, {"id": 246, "seek": 161148, "start": 1611.48, "end": 1621.48, "text": " embeddings and so for example they had an embedding matrix for regions in Germany because", "tokens": [12240, 29432, 293, 370, 337, 1365, 436, 632, 364, 12240, 3584, 8141, 337, 10682, 294, 7244, 570], "temperature": 0.0, "avg_logprob": -0.08971763586069083, "compression_ratio": 1.7512437810945274, "no_speech_prob": 4.5209262111711723e-07}, {"id": 247, "seek": 161148, "start": 1621.48, "end": 1625.0, "text": " there was what there wasn't really metadata about this these were just learned embeddings", "tokens": [456, 390, 437, 456, 2067, 380, 534, 26603, 466, 341, 613, 645, 445, 3264, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.08971763586069083, "compression_ratio": 1.7512437810945274, "no_speech_prob": 4.5209262111711723e-07}, {"id": 248, "seek": 161148, "start": 1625.0, "end": 1629.92, "text": " just like we learned embeddings about movies and so then they just created just like we", "tokens": [445, 411, 321, 3264, 12240, 29432, 466, 6233, 293, 370, 550, 436, 445, 2942, 445, 411, 321], "temperature": 0.0, "avg_logprob": -0.08971763586069083, "compression_ratio": 1.7512437810945274, "no_speech_prob": 4.5209262111711723e-07}, {"id": 249, "seek": 161148, "start": 1629.92, "end": 1636.68, "text": " did before a chart where they popped each region according to I think probably a PCA", "tokens": [630, 949, 257, 6927, 689, 436, 21545, 1184, 4458, 4650, 281, 286, 519, 1391, 257, 6465, 32], "temperature": 0.0, "avg_logprob": -0.08971763586069083, "compression_ratio": 1.7512437810945274, "no_speech_prob": 4.5209262111711723e-07}, {"id": 250, "seek": 163668, "start": 1636.68, "end": 1643.24, "text": " of their embeddings and then if you circle the ones that are close to each other in blue", "tokens": [295, 641, 12240, 29432, 293, 550, 498, 291, 6329, 264, 2306, 300, 366, 1998, 281, 1184, 661, 294, 3344], "temperature": 0.0, "avg_logprob": -0.11572506848503561, "compression_ratio": 1.7794871794871794, "no_speech_prob": 1.2289098094697692e-06}, {"id": 251, "seek": 163668, "start": 1643.24, "end": 1648.48, "text": " you'll see that they're actually close to each other in Germany and ditto for red and", "tokens": [291, 603, 536, 300, 436, 434, 767, 1998, 281, 1184, 661, 294, 7244, 293, 274, 34924, 337, 2182, 293], "temperature": 0.0, "avg_logprob": -0.11572506848503561, "compression_ratio": 1.7794871794871794, "no_speech_prob": 1.2289098094697692e-06}, {"id": 252, "seek": 163668, "start": 1648.48, "end": 1656.5600000000002, "text": " ditto for green and then here's the ground so this is like pretty amazing is the way", "tokens": [274, 34924, 337, 3092, 293, 550, 510, 311, 264, 2727, 370, 341, 307, 411, 1238, 2243, 307, 264, 636], "temperature": 0.0, "avg_logprob": -0.11572506848503561, "compression_ratio": 1.7794871794871794, "no_speech_prob": 1.2289098094697692e-06}, {"id": 253, "seek": 163668, "start": 1656.5600000000002, "end": 1662.28, "text": " that we can see that it's kind of a plant something about what Germany looks like based", "tokens": [300, 321, 393, 536, 300, 309, 311, 733, 295, 257, 3709, 746, 466, 437, 7244, 1542, 411, 2361], "temperature": 0.0, "avg_logprob": -0.11572506848503561, "compression_ratio": 1.7794871794871794, "no_speech_prob": 1.2289098094697692e-06}, {"id": 254, "seek": 166228, "start": 1662.28, "end": 1666.84, "text": " entirely on the purchasing behavior of people in those states.", "tokens": [7696, 322, 264, 20906, 5223, 295, 561, 294, 729, 4368, 13], "temperature": 0.0, "avg_logprob": -0.0819302381471146, "compression_ratio": 1.8944954128440368, "no_speech_prob": 1.5056934898893815e-06}, {"id": 255, "seek": 166228, "start": 1666.84, "end": 1673.08, "text": " Something else they did was to look at every store and they looked at the distance between", "tokens": [6595, 1646, 436, 630, 390, 281, 574, 412, 633, 3531, 293, 436, 2956, 412, 264, 4560, 1296], "temperature": 0.0, "avg_logprob": -0.0819302381471146, "compression_ratio": 1.8944954128440368, "no_speech_prob": 1.5056934898893815e-06}, {"id": 256, "seek": 166228, "start": 1673.08, "end": 1678.96, "text": " stores in practice like how many kilometers away they are and then they looked at the", "tokens": [9512, 294, 3124, 411, 577, 867, 13904, 1314, 436, 366, 293, 550, 436, 2956, 412, 264], "temperature": 0.0, "avg_logprob": -0.0819302381471146, "compression_ratio": 1.8944954128440368, "no_speech_prob": 1.5056934898893815e-06}, {"id": 257, "seek": 166228, "start": 1678.96, "end": 1684.8, "text": " distance between stores in terms of their embedding distance just like we saw in the", "tokens": [4560, 1296, 9512, 294, 2115, 295, 641, 12240, 3584, 4560, 445, 411, 321, 1866, 294, 264], "temperature": 0.0, "avg_logprob": -0.0819302381471146, "compression_ratio": 1.8944954128440368, "no_speech_prob": 1.5056934898893815e-06}, {"id": 258, "seek": 166228, "start": 1684.8, "end": 1690.56, "text": " previous notebook and there was this very strong correlation that stores that were close", "tokens": [3894, 21060, 293, 456, 390, 341, 588, 2068, 20009, 300, 9512, 300, 645, 1998], "temperature": 0.0, "avg_logprob": -0.0819302381471146, "compression_ratio": 1.8944954128440368, "no_speech_prob": 1.5056934898893815e-06}, {"id": 259, "seek": 169056, "start": 1690.56, "end": 1700.3999999999999, "text": " to each other physically ended up having close embeddings as well even though the actual", "tokens": [281, 1184, 661, 9762, 4590, 493, 1419, 1998, 12240, 29432, 382, 731, 754, 1673, 264, 3539], "temperature": 0.0, "avg_logprob": -0.06684152483940124, "compression_ratio": 1.9649122807017543, "no_speech_prob": 3.747984180790809e-07}, {"id": 260, "seek": 169056, "start": 1700.3999999999999, "end": 1706.28, "text": " location of these stores in physical space was not part of the model.", "tokens": [4914, 295, 613, 9512, 294, 4001, 1901, 390, 406, 644, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.06684152483940124, "compression_ratio": 1.9649122807017543, "no_speech_prob": 3.747984180790809e-07}, {"id": 261, "seek": 169056, "start": 1706.28, "end": 1712.1599999999999, "text": " Ditto with days of the week so the days of the week or another embedding and the days", "tokens": [413, 34924, 365, 1708, 295, 264, 1243, 370, 264, 1708, 295, 264, 1243, 420, 1071, 12240, 3584, 293, 264, 1708], "temperature": 0.0, "avg_logprob": -0.06684152483940124, "compression_ratio": 1.9649122807017543, "no_speech_prob": 3.747984180790809e-07}, {"id": 262, "seek": 169056, "start": 1712.1599999999999, "end": 1717.8, "text": " of the week that were next to each other ended up next to each other in embedding space and", "tokens": [295, 264, 1243, 300, 645, 958, 281, 1184, 661, 4590, 493, 958, 281, 1184, 661, 294, 12240, 3584, 1901, 293], "temperature": 0.0, "avg_logprob": -0.06684152483940124, "compression_ratio": 1.9649122807017543, "no_speech_prob": 3.747984180790809e-07}, {"id": 263, "seek": 171780, "start": 1717.8, "end": 1725.52, "text": " ditto for months of the year so pretty fascinating the way kind of information about the world", "tokens": [274, 34924, 337, 2493, 295, 264, 1064, 370, 1238, 10343, 264, 636, 733, 295, 1589, 466, 264, 1002], "temperature": 0.0, "avg_logprob": -0.14289504378589232, "compression_ratio": 1.6021505376344085, "no_speech_prob": 1.7603349533601431e-06}, {"id": 264, "seek": 171780, "start": 1725.52, "end": 1732.6399999999999, "text": " ends up captured just by looking at training embeddings which as we know are just index", "tokens": [5314, 493, 11828, 445, 538, 1237, 412, 3097, 12240, 29432, 597, 382, 321, 458, 366, 445, 8186], "temperature": 0.0, "avg_logprob": -0.14289504378589232, "compression_ratio": 1.6021505376344085, "no_speech_prob": 1.7603349533601431e-06}, {"id": 265, "seek": 171780, "start": 1732.6399999999999, "end": 1736.44, "text": " lookups into an array.", "tokens": [574, 7528, 666, 364, 10225, 13], "temperature": 0.0, "avg_logprob": -0.14289504378589232, "compression_ratio": 1.6021505376344085, "no_speech_prob": 1.7603349533601431e-06}, {"id": 266, "seek": 171780, "start": 1736.44, "end": 1742.2, "text": " So the way we then combine these categorical variables with these embeddings with continuous", "tokens": [407, 264, 636, 321, 550, 10432, 613, 19250, 804, 9102, 365, 613, 12240, 29432, 365, 10957], "temperature": 0.0, "avg_logprob": -0.14289504378589232, "compression_ratio": 1.6021505376344085, "no_speech_prob": 1.7603349533601431e-06}, {"id": 267, "seek": 174220, "start": 1742.2, "end": 1749.2, "text": " variables what was done in both the entity embedding paper that we just looked at and", "tokens": [9102, 437, 390, 1096, 294, 1293, 264, 13977, 12240, 3584, 3035, 300, 321, 445, 2956, 412, 293], "temperature": 0.0, "avg_logprob": -0.07859024405479431, "compression_ratio": 1.9327731092436975, "no_speech_prob": 1.2098627166778897e-06}, {"id": 268, "seek": 174220, "start": 1749.2, "end": 1754.52, "text": " then also described in more detail by Google when they described how their recommendation", "tokens": [550, 611, 7619, 294, 544, 2607, 538, 3329, 562, 436, 7619, 577, 641, 11879], "temperature": 0.0, "avg_logprob": -0.07859024405479431, "compression_ratio": 1.9327731092436975, "no_speech_prob": 1.2098627166778897e-06}, {"id": 269, "seek": 174220, "start": 1754.52, "end": 1761.0800000000002, "text": " system in Google Play works this is from Google's paper is they have the categorical features", "tokens": [1185, 294, 3329, 5506, 1985, 341, 307, 490, 3329, 311, 3035, 307, 436, 362, 264, 19250, 804, 4122], "temperature": 0.0, "avg_logprob": -0.07859024405479431, "compression_ratio": 1.9327731092436975, "no_speech_prob": 1.2098627166778897e-06}, {"id": 270, "seek": 174220, "start": 1761.0800000000002, "end": 1766.42, "text": " that go through the embeddings and then there are continuous features and then all the embedding", "tokens": [300, 352, 807, 264, 12240, 29432, 293, 550, 456, 366, 10957, 4122, 293, 550, 439, 264, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.07859024405479431, "compression_ratio": 1.9327731092436975, "no_speech_prob": 1.2098627166778897e-06}, {"id": 271, "seek": 174220, "start": 1766.42, "end": 1771.04, "text": " results and the continuous features are just concatenated together into this big concatenated", "tokens": [3542, 293, 264, 10957, 4122, 366, 445, 1588, 7186, 770, 1214, 666, 341, 955, 1588, 7186, 770], "temperature": 0.0, "avg_logprob": -0.07859024405479431, "compression_ratio": 1.9327731092436975, "no_speech_prob": 1.2098627166778897e-06}, {"id": 272, "seek": 177104, "start": 1771.04, "end": 1776.52, "text": " table that then goes through in this case three layers of a neural net and interestingly", "tokens": [3199, 300, 550, 1709, 807, 294, 341, 1389, 1045, 7914, 295, 257, 18161, 2533, 293, 25873], "temperature": 0.0, "avg_logprob": -0.11280929611389895, "compression_ratio": 1.7235023041474655, "no_speech_prob": 5.804990905744489e-07}, {"id": 273, "seek": 177104, "start": 1776.52, "end": 1784.12, "text": " they also take the kind of collaborative filtering bit and do the dot product as well and combine", "tokens": [436, 611, 747, 264, 733, 295, 16555, 30822, 857, 293, 360, 264, 5893, 1674, 382, 731, 293, 10432], "temperature": 0.0, "avg_logprob": -0.11280929611389895, "compression_ratio": 1.7235023041474655, "no_speech_prob": 5.804990905744489e-07}, {"id": 274, "seek": 177104, "start": 1784.12, "end": 1788.6, "text": " the two so they use both of the tricks were used in the previous notebook and combine", "tokens": [264, 732, 370, 436, 764, 1293, 295, 264, 11733, 645, 1143, 294, 264, 3894, 21060, 293, 10432], "temperature": 0.0, "avg_logprob": -0.11280929611389895, "compression_ratio": 1.7235023041474655, "no_speech_prob": 5.804990905744489e-07}, {"id": 275, "seek": 177104, "start": 1788.6, "end": 1791.6, "text": " them together.", "tokens": [552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.11280929611389895, "compression_ratio": 1.7235023041474655, "no_speech_prob": 5.804990905744489e-07}, {"id": 276, "seek": 177104, "start": 1791.6, "end": 1798.28, "text": " So that's the basic idea we're going to be seeing for moving beyond just collaborative", "tokens": [407, 300, 311, 264, 3875, 1558, 321, 434, 516, 281, 312, 2577, 337, 2684, 4399, 445, 16555], "temperature": 0.0, "avg_logprob": -0.11280929611389895, "compression_ratio": 1.7235023041474655, "no_speech_prob": 5.804990905744489e-07}, {"id": 277, "seek": 179828, "start": 1798.28, "end": 1804.48, "text": " filtering which is just two categorical variables to as many categorical and as many continuous", "tokens": [30822, 597, 307, 445, 732, 19250, 804, 9102, 281, 382, 867, 19250, 804, 293, 382, 867, 10957], "temperature": 0.0, "avg_logprob": -0.11632797194690239, "compression_ratio": 1.6338028169014085, "no_speech_prob": 1.2679242900048848e-06}, {"id": 278, "seek": 179828, "start": 1804.48, "end": 1807.36, "text": " variables as we like.", "tokens": [9102, 382, 321, 411, 13], "temperature": 0.0, "avg_logprob": -0.11632797194690239, "compression_ratio": 1.6338028169014085, "no_speech_prob": 1.2679242900048848e-06}, {"id": 279, "seek": 179828, "start": 1807.36, "end": 1813.18, "text": " But before we do that let's take a step back and think about other approaches because as", "tokens": [583, 949, 321, 360, 300, 718, 311, 747, 257, 1823, 646, 293, 519, 466, 661, 11587, 570, 382], "temperature": 0.0, "avg_logprob": -0.11632797194690239, "compression_ratio": 1.6338028169014085, "no_speech_prob": 1.2679242900048848e-06}, {"id": 280, "seek": 179828, "start": 1813.18, "end": 1820.08, "text": " I mentioned the idea of deep learning as a kind of a best practice for tabular data is", "tokens": [286, 2835, 264, 1558, 295, 2452, 2539, 382, 257, 733, 295, 257, 1151, 3124, 337, 4421, 1040, 1412, 307], "temperature": 0.0, "avg_logprob": -0.11632797194690239, "compression_ratio": 1.6338028169014085, "no_speech_prob": 1.2679242900048848e-06}, {"id": 281, "seek": 179828, "start": 1820.08, "end": 1824.72, "text": " still pretty new and it's still kind of controversial.", "tokens": [920, 1238, 777, 293, 309, 311, 920, 733, 295, 17323, 13], "temperature": 0.0, "avg_logprob": -0.11632797194690239, "compression_ratio": 1.6338028169014085, "no_speech_prob": 1.2679242900048848e-06}, {"id": 282, "seek": 182472, "start": 1824.72, "end": 1829.96, "text": " But certainly not always the case that it's the best approach so when we're not using", "tokens": [583, 3297, 406, 1009, 264, 1389, 300, 309, 311, 264, 1151, 3109, 370, 562, 321, 434, 406, 1228], "temperature": 0.0, "avg_logprob": -0.14634169655284662, "compression_ratio": 1.6596638655462186, "no_speech_prob": 1.0676994861569256e-06}, {"id": 283, "seek": 182472, "start": 1829.96, "end": 1832.68, "text": " deep learning what would we be using.", "tokens": [2452, 2539, 437, 576, 321, 312, 1228, 13], "temperature": 0.0, "avg_logprob": -0.14634169655284662, "compression_ratio": 1.6596638655462186, "no_speech_prob": 1.0676994861569256e-06}, {"id": 284, "seek": 182472, "start": 1832.68, "end": 1838.04, "text": " Well what would probably be using is something called an ensemble of decision trees and the", "tokens": [1042, 437, 576, 1391, 312, 1228, 307, 746, 1219, 364, 19492, 295, 3537, 5852, 293, 264], "temperature": 0.0, "avg_logprob": -0.14634169655284662, "compression_ratio": 1.6596638655462186, "no_speech_prob": 1.0676994861569256e-06}, {"id": 285, "seek": 182472, "start": 1838.04, "end": 1846.16, "text": " two most popular are random forests and gradient boosting machines or something similar.", "tokens": [732, 881, 3743, 366, 4974, 21700, 293, 16235, 43117, 8379, 420, 746, 2531, 13], "temperature": 0.0, "avg_logprob": -0.14634169655284662, "compression_ratio": 1.6596638655462186, "no_speech_prob": 1.0676994861569256e-06}, {"id": 286, "seek": 182472, "start": 1846.16, "end": 1850.56, "text": " So basically between multi-layered neural networks like with SGD and ensembles of decision", "tokens": [407, 1936, 1296, 4825, 12, 8376, 4073, 18161, 9590, 411, 365, 34520, 35, 293, 12567, 2504, 904, 295, 3537], "temperature": 0.0, "avg_logprob": -0.14634169655284662, "compression_ratio": 1.6596638655462186, "no_speech_prob": 1.0676994861569256e-06}, {"id": 287, "seek": 185056, "start": 1850.56, "end": 1857.08, "text": " trees that kind of covers the vast majority of approaches that you're likely to see for", "tokens": [5852, 300, 733, 295, 10538, 264, 8369, 6286, 295, 11587, 300, 291, 434, 3700, 281, 536, 337], "temperature": 0.0, "avg_logprob": -0.10812733047886898, "compression_ratio": 1.6267281105990783, "no_speech_prob": 3.340524017403368e-06}, {"id": 288, "seek": 185056, "start": 1857.08, "end": 1864.48, "text": " tabular data and so we're going to make sure we cover them both of course today in fact.", "tokens": [4421, 1040, 1412, 293, 370, 321, 434, 516, 281, 652, 988, 321, 2060, 552, 1293, 295, 1164, 965, 294, 1186, 13], "temperature": 0.0, "avg_logprob": -0.10812733047886898, "compression_ratio": 1.6267281105990783, "no_speech_prob": 3.340524017403368e-06}, {"id": 289, "seek": 185056, "start": 1864.48, "end": 1869.96, "text": " So although deep learning is nearly always clearly superior for stuff like images and", "tokens": [407, 4878, 2452, 2539, 307, 6217, 1009, 4448, 13028, 337, 1507, 411, 5267, 293], "temperature": 0.0, "avg_logprob": -0.10812733047886898, "compression_ratio": 1.6267281105990783, "no_speech_prob": 3.340524017403368e-06}, {"id": 290, "seek": 185056, "start": 1869.96, "end": 1876.3999999999999, "text": " audio and natural language text these two approaches tend to give somewhat similar results", "tokens": [6278, 293, 3303, 2856, 2487, 613, 732, 11587, 3928, 281, 976, 8344, 2531, 3542], "temperature": 0.0, "avg_logprob": -0.10812733047886898, "compression_ratio": 1.6267281105990783, "no_speech_prob": 3.340524017403368e-06}, {"id": 291, "seek": 187640, "start": 1876.4, "end": 1880.6000000000001, "text": " a lot of the time for tabular data.", "tokens": [257, 688, 295, 264, 565, 337, 4421, 1040, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1615751404123208, "compression_ratio": 1.5767441860465117, "no_speech_prob": 7.52789446778479e-06}, {"id": 292, "seek": 187640, "start": 1880.6000000000001, "end": 1883.8400000000001, "text": " Let's take a look some you know you really should generally try both and see which works", "tokens": [961, 311, 747, 257, 574, 512, 291, 458, 291, 534, 820, 5101, 853, 1293, 293, 536, 597, 1985], "temperature": 0.0, "avg_logprob": -0.1615751404123208, "compression_ratio": 1.5767441860465117, "no_speech_prob": 7.52789446778479e-06}, {"id": 293, "seek": 187640, "start": 1883.8400000000001, "end": 1890.72, "text": " best for you for each problem you look at.", "tokens": [1151, 337, 291, 337, 1184, 1154, 291, 574, 412, 13], "temperature": 0.0, "avg_logprob": -0.1615751404123208, "compression_ratio": 1.5767441860465117, "no_speech_prob": 7.52789446778479e-06}, {"id": 294, "seek": 187640, "start": 1890.72, "end": 1898.2, "text": " Why does the range go from 0 to 5.5 if the maximum is 5.", "tokens": [1545, 775, 264, 3613, 352, 490, 1958, 281, 1025, 13, 20, 498, 264, 6674, 307, 1025, 13], "temperature": 0.0, "avg_logprob": -0.1615751404123208, "compression_ratio": 1.5767441860465117, "no_speech_prob": 7.52789446778479e-06}, {"id": 295, "seek": 187640, "start": 1898.2, "end": 1899.68, "text": " That's a great question.", "tokens": [663, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1615751404123208, "compression_ratio": 1.5767441860465117, "no_speech_prob": 7.52789446778479e-06}, {"id": 296, "seek": 187640, "start": 1899.68, "end": 1906.0400000000002, "text": " The reason is if you think about it for sigmoid it's actually impossible for a sigmoid to", "tokens": [440, 1778, 307, 498, 291, 519, 466, 309, 337, 4556, 3280, 327, 309, 311, 767, 6243, 337, 257, 4556, 3280, 327, 281], "temperature": 0.0, "avg_logprob": -0.1615751404123208, "compression_ratio": 1.5767441860465117, "no_speech_prob": 7.52789446778479e-06}, {"id": 297, "seek": 190604, "start": 1906.04, "end": 1909.2, "text": " get all the way to the top or all the way to the bottom.", "tokens": [483, 439, 264, 636, 281, 264, 1192, 420, 439, 264, 636, 281, 264, 2767, 13], "temperature": 0.0, "avg_logprob": -0.14210600323147243, "compression_ratio": 1.8130434782608695, "no_speech_prob": 5.771773430751637e-06}, {"id": 298, "seek": 190604, "start": 1909.2, "end": 1914.84, "text": " Those are asymptotes so no matter how far how big your x is can never quite get to the", "tokens": [3950, 366, 35114, 17251, 370, 572, 1871, 577, 1400, 577, 955, 428, 2031, 307, 393, 1128, 1596, 483, 281, 264], "temperature": 0.0, "avg_logprob": -0.14210600323147243, "compression_ratio": 1.8130434782608695, "no_speech_prob": 5.771773430751637e-06}, {"id": 299, "seek": 190604, "start": 1914.84, "end": 1918.2, "text": " top or no matter how small it is it can never quite get to the top.", "tokens": [1192, 420, 572, 1871, 577, 1359, 309, 307, 309, 393, 1128, 1596, 483, 281, 264, 1192, 13], "temperature": 0.0, "avg_logprob": -0.14210600323147243, "compression_ratio": 1.8130434782608695, "no_speech_prob": 5.771773430751637e-06}, {"id": 300, "seek": 190604, "start": 1918.2, "end": 1922.6399999999999, "text": " So if you want to be able to actually predict a rating of 5 then you need to use something", "tokens": [407, 498, 291, 528, 281, 312, 1075, 281, 767, 6069, 257, 10990, 295, 1025, 550, 291, 643, 281, 764, 746], "temperature": 0.0, "avg_logprob": -0.14210600323147243, "compression_ratio": 1.8130434782608695, "no_speech_prob": 5.771773430751637e-06}, {"id": 301, "seek": 190604, "start": 1922.6399999999999, "end": 1927.36, "text": " higher than 5 your maximum.", "tokens": [2946, 813, 1025, 428, 6674, 13], "temperature": 0.0, "avg_logprob": -0.14210600323147243, "compression_ratio": 1.8130434782608695, "no_speech_prob": 5.771773430751637e-06}, {"id": 302, "seek": 190604, "start": 1927.36, "end": 1932.44, "text": " Are embeddings used only for highly cardinal categorical variables or is this approach", "tokens": [2014, 12240, 29432, 1143, 787, 337, 5405, 2920, 2071, 19250, 804, 9102, 420, 307, 341, 3109], "temperature": 0.0, "avg_logprob": -0.14210600323147243, "compression_ratio": 1.8130434782608695, "no_speech_prob": 5.771773430751637e-06}, {"id": 303, "seek": 193244, "start": 1932.44, "end": 1938.96, "text": " you general for low cardinality can one use a one-hot encoding.", "tokens": [291, 2674, 337, 2295, 2920, 259, 1860, 393, 472, 764, 257, 472, 12, 12194, 43430, 13], "temperature": 0.0, "avg_logprob": -0.21386625708603277, "compression_ratio": 1.695187165775401, "no_speech_prob": 8.315242894241237e-07}, {"id": 304, "seek": 193244, "start": 1938.96, "end": 1949.24, "text": " I'll remind you cardinality is the number of discrete levels in a variable and and remember", "tokens": [286, 603, 4160, 291, 2920, 259, 1860, 307, 264, 1230, 295, 27706, 4358, 294, 257, 7006, 293, 293, 1604], "temperature": 0.0, "avg_logprob": -0.21386625708603277, "compression_ratio": 1.695187165775401, "no_speech_prob": 8.315242894241237e-07}, {"id": 305, "seek": 193244, "start": 1949.24, "end": 1955.96, "text": " that an embedding is just a computational shortcut for a one-hot encoding.", "tokens": [300, 364, 12240, 3584, 307, 445, 257, 28270, 24822, 337, 257, 472, 12, 12194, 43430, 13], "temperature": 0.0, "avg_logprob": -0.21386625708603277, "compression_ratio": 1.695187165775401, "no_speech_prob": 8.315242894241237e-07}, {"id": 306, "seek": 193244, "start": 1955.96, "end": 1962.0, "text": " So there's really no reason to use a one-hot encoding because it's as long as you have", "tokens": [407, 456, 311, 534, 572, 1778, 281, 764, 257, 472, 12, 12194, 43430, 570, 309, 311, 382, 938, 382, 291, 362], "temperature": 0.0, "avg_logprob": -0.21386625708603277, "compression_ratio": 1.695187165775401, "no_speech_prob": 8.315242894241237e-07}, {"id": 307, "seek": 196200, "start": 1962.0, "end": 1967.46, "text": " more than two levels it's always going to be more memory and lower and give you exactly", "tokens": [544, 813, 732, 4358, 309, 311, 1009, 516, 281, 312, 544, 4675, 293, 3126, 293, 976, 291, 2293], "temperature": 0.0, "avg_logprob": -0.17533553592742435, "compression_ratio": 1.5808383233532934, "no_speech_prob": 1.9333451746206265e-06}, {"id": 308, "seek": 196200, "start": 1967.46, "end": 1972.68, "text": " mathematically the same thing and if there's just two levels then it is basically identical", "tokens": [44003, 264, 912, 551, 293, 498, 456, 311, 445, 732, 4358, 550, 309, 307, 1936, 14800], "temperature": 0.0, "avg_logprob": -0.17533553592742435, "compression_ratio": 1.5808383233532934, "no_speech_prob": 1.9333451746206265e-06}, {"id": 309, "seek": 196200, "start": 1972.68, "end": 1978.22, "text": " so there isn't really any reason not to use it.", "tokens": [370, 456, 1943, 380, 534, 604, 1778, 406, 281, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.17533553592742435, "compression_ratio": 1.5808383233532934, "no_speech_prob": 1.9333451746206265e-06}, {"id": 310, "seek": 196200, "start": 1978.22, "end": 1980.96, "text": " Thank you for those great questions.", "tokens": [1044, 291, 337, 729, 869, 1651, 13], "temperature": 0.0, "avg_logprob": -0.17533553592742435, "compression_ratio": 1.5808383233532934, "no_speech_prob": 1.9333451746206265e-06}, {"id": 311, "seek": 198096, "start": 1980.96, "end": 1992.32, "text": " Okay so one of the most important things about decision tree ensembles is that at the current", "tokens": [1033, 370, 472, 295, 264, 881, 1021, 721, 466, 3537, 4230, 12567, 2504, 904, 307, 300, 412, 264, 2190], "temperature": 0.0, "avg_logprob": -0.11719478731570036, "compression_ratio": 1.6375, "no_speech_prob": 9.665807738201693e-06}, {"id": 312, "seek": 198096, "start": 1992.32, "end": 1997.72, "text": " state of the technology they do provide faster and easier ways of interpreting the model.", "tokens": [1785, 295, 264, 2899, 436, 360, 2893, 4663, 293, 3571, 2098, 295, 37395, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11719478731570036, "compression_ratio": 1.6375, "no_speech_prob": 9.665807738201693e-06}, {"id": 313, "seek": 198096, "start": 1997.72, "end": 2001.6000000000001, "text": " I think that's rapidly improving for deep learning models on tabular data but that's", "tokens": [286, 519, 300, 311, 12910, 11470, 337, 2452, 2539, 5245, 322, 4421, 1040, 1412, 457, 300, 311], "temperature": 0.0, "avg_logprob": -0.11719478731570036, "compression_ratio": 1.6375, "no_speech_prob": 9.665807738201693e-06}, {"id": 314, "seek": 198096, "start": 2001.6000000000001, "end": 2003.52, "text": " where we are right now.", "tokens": [689, 321, 366, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.11719478731570036, "compression_ratio": 1.6375, "no_speech_prob": 9.665807738201693e-06}, {"id": 315, "seek": 198096, "start": 2003.52, "end": 2008.56, "text": " They also require less hyper parameter tuning so they're easier to kind of get right the", "tokens": [814, 611, 3651, 1570, 9848, 13075, 15164, 370, 436, 434, 3571, 281, 733, 295, 483, 558, 264], "temperature": 0.0, "avg_logprob": -0.11719478731570036, "compression_ratio": 1.6375, "no_speech_prob": 9.665807738201693e-06}, {"id": 316, "seek": 198096, "start": 2008.56, "end": 2009.56, "text": " first time.", "tokens": [700, 565, 13], "temperature": 0.0, "avg_logprob": -0.11719478731570036, "compression_ratio": 1.6375, "no_speech_prob": 9.665807738201693e-06}, {"id": 317, "seek": 200956, "start": 2009.56, "end": 2015.44, "text": " So my first approach for analyzing a new data set is always an ensemble of decision trees", "tokens": [407, 452, 700, 3109, 337, 23663, 257, 777, 1412, 992, 307, 1009, 364, 19492, 295, 3537, 5852], "temperature": 0.0, "avg_logprob": -0.25143381915514984, "compression_ratio": 1.5592417061611374, "no_speech_prob": 1.4737844139744993e-05}, {"id": 318, "seek": 200956, "start": 2015.44, "end": 2019.36, "text": " and specifically I pretty much always start with a random forest because it's just so", "tokens": [293, 4682, 286, 1238, 709, 1009, 722, 365, 257, 4974, 6719, 570, 309, 311, 445, 370], "temperature": 0.0, "avg_logprob": -0.25143381915514984, "compression_ratio": 1.5592417061611374, "no_speech_prob": 1.4737844139744993e-05}, {"id": 319, "seek": 200956, "start": 2019.36, "end": 2020.36, "text": " reliable.", "tokens": [12924, 13], "temperature": 0.0, "avg_logprob": -0.25143381915514984, "compression_ratio": 1.5592417061611374, "no_speech_prob": 1.4737844139744993e-05}, {"id": 320, "seek": 200956, "start": 2020.36, "end": 2021.36, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.25143381915514984, "compression_ratio": 1.5592417061611374, "no_speech_prob": 1.4737844139744993e-05}, {"id": 321, "seek": 200956, "start": 2021.36, "end": 2030.48, "text": " Your experience for highly imbalanced data such as broad or medical data what usually", "tokens": [2260, 1752, 337, 5405, 566, 40251, 1412, 1270, 382, 4152, 420, 4625, 1412, 437, 2673], "temperature": 0.0, "avg_logprob": -0.25143381915514984, "compression_ratio": 1.5592417061611374, "no_speech_prob": 1.4737844139744993e-05}, {"id": 322, "seek": 200956, "start": 2030.48, "end": 2035.48, "text": " best out of random forest xgboost or neural network?", "tokens": [1151, 484, 295, 4974, 6719, 2031, 70, 1763, 555, 420, 18161, 3209, 30], "temperature": 0.0, "avg_logprob": -0.25143381915514984, "compression_ratio": 1.5592417061611374, "no_speech_prob": 1.4737844139744993e-05}, {"id": 323, "seek": 203548, "start": 2035.48, "end": 2042.92, "text": " I'm not sure that the whether the data is balanced or unbalanced is a key reason for", "tokens": [286, 478, 406, 988, 300, 264, 1968, 264, 1412, 307, 13902, 420, 517, 40251, 307, 257, 2141, 1778, 337], "temperature": 0.0, "avg_logprob": -0.0751854192430728, "compression_ratio": 1.7469879518072289, "no_speech_prob": 1.628034169698367e-06}, {"id": 324, "seek": 203548, "start": 2042.92, "end": 2044.44, "text": " choosing one of those about the others.", "tokens": [10875, 472, 295, 729, 466, 264, 2357, 13], "temperature": 0.0, "avg_logprob": -0.0751854192430728, "compression_ratio": 1.7469879518072289, "no_speech_prob": 1.628034169698367e-06}, {"id": 325, "seek": 203548, "start": 2044.44, "end": 2049.56, "text": " I would try all of them and see which works best.", "tokens": [286, 576, 853, 439, 295, 552, 293, 536, 597, 1985, 1151, 13], "temperature": 0.0, "avg_logprob": -0.0751854192430728, "compression_ratio": 1.7469879518072289, "no_speech_prob": 1.628034169698367e-06}, {"id": 326, "seek": 203548, "start": 2049.56, "end": 2053.76, "text": " So the exception to the guideline about start with decision tree ensembles is your first", "tokens": [407, 264, 11183, 281, 264, 41653, 466, 722, 365, 3537, 4230, 12567, 2504, 904, 307, 428, 700], "temperature": 0.0, "avg_logprob": -0.0751854192430728, "compression_ratio": 1.7469879518072289, "no_speech_prob": 1.628034169698367e-06}, {"id": 327, "seek": 203548, "start": 2053.76, "end": 2058.56, "text": " thing to try would be if there's some very high cardinality categorical variables then", "tokens": [551, 281, 853, 576, 312, 498, 456, 311, 512, 588, 1090, 2920, 259, 1860, 19250, 804, 9102, 550], "temperature": 0.0, "avg_logprob": -0.0751854192430728, "compression_ratio": 1.7469879518072289, "no_speech_prob": 1.628034169698367e-06}, {"id": 328, "seek": 203548, "start": 2058.56, "end": 2065.2, "text": " they can be a bit difficult to get to work really well in decision tree ensembles or", "tokens": [436, 393, 312, 257, 857, 2252, 281, 483, 281, 589, 534, 731, 294, 3537, 4230, 12567, 2504, 904, 420], "temperature": 0.0, "avg_logprob": -0.0751854192430728, "compression_ratio": 1.7469879518072289, "no_speech_prob": 1.628034169698367e-06}, {"id": 329, "seek": 206520, "start": 2065.2, "end": 2068.96, "text": " if there's something like most importantly if it's like plain text data or image data", "tokens": [498, 456, 311, 746, 411, 881, 8906, 498, 309, 311, 411, 11121, 2487, 1412, 420, 3256, 1412], "temperature": 0.0, "avg_logprob": -0.1411064759041499, "compression_ratio": 1.8025210084033614, "no_speech_prob": 9.721505875859293e-07}, {"id": 330, "seek": 206520, "start": 2068.96, "end": 2074.3599999999997, "text": " or audio data or something like that then you're definitely going to need to use a neural", "tokens": [420, 6278, 1412, 420, 746, 411, 300, 550, 291, 434, 2138, 516, 281, 643, 281, 764, 257, 18161], "temperature": 0.0, "avg_logprob": -0.1411064759041499, "compression_ratio": 1.8025210084033614, "no_speech_prob": 9.721505875859293e-07}, {"id": 331, "seek": 206520, "start": 2074.3599999999997, "end": 2082.52, "text": " net in there but you could actually ensemble it with a random forest as we'll see.", "tokens": [2533, 294, 456, 457, 291, 727, 767, 19492, 309, 365, 257, 4974, 6719, 382, 321, 603, 536, 13], "temperature": 0.0, "avg_logprob": -0.1411064759041499, "compression_ratio": 1.8025210084033614, "no_speech_prob": 9.721505875859293e-07}, {"id": 332, "seek": 206520, "start": 2082.52, "end": 2089.4399999999996, "text": " Okay so really we're going to need to understand how decision tree ensembles work.", "tokens": [1033, 370, 534, 321, 434, 516, 281, 643, 281, 1223, 577, 3537, 4230, 12567, 2504, 904, 589, 13], "temperature": 0.0, "avg_logprob": -0.1411064759041499, "compression_ratio": 1.8025210084033614, "no_speech_prob": 9.721505875859293e-07}, {"id": 333, "seek": 206520, "start": 2089.4399999999996, "end": 2094.7599999999998, "text": " So pytorch isn't a great choice for decision tree ensembles they're really designed for", "tokens": [407, 25878, 284, 339, 1943, 380, 257, 869, 3922, 337, 3537, 4230, 12567, 2504, 904, 436, 434, 534, 4761, 337], "temperature": 0.0, "avg_logprob": -0.1411064759041499, "compression_ratio": 1.8025210084033614, "no_speech_prob": 9.721505875859293e-07}, {"id": 334, "seek": 209476, "start": 2094.76, "end": 2101.2000000000003, "text": " gradient based methods and random forests and decision tree growing are not really gradient", "tokens": [16235, 2361, 7150, 293, 4974, 21700, 293, 3537, 4230, 4194, 366, 406, 534, 16235], "temperature": 0.0, "avg_logprob": -0.19976602141390143, "compression_ratio": 1.8028846153846154, "no_speech_prob": 2.4824682895996375e-06}, {"id": 335, "seek": 209476, "start": 2101.2000000000003, "end": 2102.84, "text": " based methods in the same way.", "tokens": [2361, 7150, 294, 264, 912, 636, 13], "temperature": 0.0, "avg_logprob": -0.19976602141390143, "compression_ratio": 1.8028846153846154, "no_speech_prob": 2.4824682895996375e-06}, {"id": 336, "seek": 209476, "start": 2102.84, "end": 2110.1200000000003, "text": " So instead we're going to use a library called scikit-learn which is referred to as sklearn", "tokens": [407, 2602, 321, 434, 516, 281, 764, 257, 6405, 1219, 2180, 22681, 12, 306, 1083, 597, 307, 10839, 281, 382, 1110, 306, 1083], "temperature": 0.0, "avg_logprob": -0.19976602141390143, "compression_ratio": 1.8028846153846154, "no_speech_prob": 2.4824682895996375e-06}, {"id": 337, "seek": 209476, "start": 2110.1200000000003, "end": 2113.28, "text": " as a module.", "tokens": [382, 257, 10088, 13], "temperature": 0.0, "avg_logprob": -0.19976602141390143, "compression_ratio": 1.8028846153846154, "no_speech_prob": 2.4824682895996375e-06}, {"id": 338, "seek": 209476, "start": 2113.28, "end": 2119.0, "text": " Scikit-learn does a lot of things we're only going to touch on a tiny piece of them stuff", "tokens": [16942, 22681, 12, 306, 1083, 775, 257, 688, 295, 721, 321, 434, 787, 516, 281, 2557, 322, 257, 5870, 2522, 295, 552, 1507], "temperature": 0.0, "avg_logprob": -0.19976602141390143, "compression_ratio": 1.8028846153846154, "no_speech_prob": 2.4824682895996375e-06}, {"id": 339, "seek": 209476, "start": 2119.0, "end": 2124.5600000000004, "text": " we need to do to train decision trees and random forests.", "tokens": [321, 643, 281, 360, 281, 3847, 3537, 5852, 293, 4974, 21700, 13], "temperature": 0.0, "avg_logprob": -0.19976602141390143, "compression_ratio": 1.8028846153846154, "no_speech_prob": 2.4824682895996375e-06}, {"id": 340, "seek": 212456, "start": 2124.56, "end": 2128.7599999999998, "text": " We've already mentioned before Wes McKinney's book also a great book for understanding more", "tokens": [492, 600, 1217, 2835, 949, 23843, 21765, 259, 2397, 311, 1446, 611, 257, 869, 1446, 337, 3701, 544], "temperature": 0.0, "avg_logprob": -0.1202583112214741, "compression_ratio": 1.7813953488372094, "no_speech_prob": 9.276342325392761e-07}, {"id": 341, "seek": 212456, "start": 2128.7599999999998, "end": 2131.64, "text": " about scikit-learn.", "tokens": [466, 2180, 22681, 12, 306, 1083, 13], "temperature": 0.0, "avg_logprob": -0.1202583112214741, "compression_ratio": 1.7813953488372094, "no_speech_prob": 9.276342325392761e-07}, {"id": 342, "seek": 212456, "start": 2131.64, "end": 2136.68, "text": " So the data set for learning about decision tree ensembles is going to be another data", "tokens": [407, 264, 1412, 992, 337, 2539, 466, 3537, 4230, 12567, 2504, 904, 307, 516, 281, 312, 1071, 1412], "temperature": 0.0, "avg_logprob": -0.1202583112214741, "compression_ratio": 1.7813953488372094, "no_speech_prob": 9.276342325392761e-07}, {"id": 343, "seek": 212456, "start": 2136.68, "end": 2143.2799999999997, "text": " set it's going to it's called the blue book for bulldozers data set and it's a Kaggle", "tokens": [992, 309, 311, 516, 281, 309, 311, 1219, 264, 3344, 1446, 337, 4693, 2595, 41698, 1412, 992, 293, 309, 311, 257, 48751, 22631], "temperature": 0.0, "avg_logprob": -0.1202583112214741, "compression_ratio": 1.7813953488372094, "no_speech_prob": 9.276342325392761e-07}, {"id": 344, "seek": 212456, "start": 2143.2799999999997, "end": 2145.2799999999997, "text": " competition.", "tokens": [6211, 13], "temperature": 0.0, "avg_logprob": -0.1202583112214741, "compression_ratio": 1.7813953488372094, "no_speech_prob": 9.276342325392761e-07}, {"id": 345, "seek": 212456, "start": 2145.2799999999997, "end": 2153.0, "text": " So Kaggle competitions are fantastic they are machine learning competitions where you", "tokens": [407, 48751, 22631, 26185, 366, 5456, 436, 366, 3479, 2539, 26185, 689, 291], "temperature": 0.0, "avg_logprob": -0.1202583112214741, "compression_ratio": 1.7813953488372094, "no_speech_prob": 9.276342325392761e-07}, {"id": 346, "seek": 215300, "start": 2153.0, "end": 2157.6, "text": " get interesting data sets you get feedback on whether your approach is any good or not", "tokens": [483, 1880, 1412, 6352, 291, 483, 5824, 322, 1968, 428, 3109, 307, 604, 665, 420, 406], "temperature": 0.0, "avg_logprob": -0.08030358950297038, "compression_ratio": 1.5902439024390245, "no_speech_prob": 4.092839844815899e-06}, {"id": 347, "seek": 215300, "start": 2157.6, "end": 2161.56, "text": " you can see on a leaderboard what approaches are working best and then you can read blog", "tokens": [291, 393, 536, 322, 257, 5263, 3787, 437, 11587, 366, 1364, 1151, 293, 550, 291, 393, 1401, 6968], "temperature": 0.0, "avg_logprob": -0.08030358950297038, "compression_ratio": 1.5902439024390245, "no_speech_prob": 4.092839844815899e-06}, {"id": 348, "seek": 215300, "start": 2161.56, "end": 2166.7, "text": " posts from the winning contestants sharing tips and tricks.", "tokens": [12300, 490, 264, 8224, 39676, 5414, 6082, 293, 11733, 13], "temperature": 0.0, "avg_logprob": -0.08030358950297038, "compression_ratio": 1.5902439024390245, "no_speech_prob": 4.092839844815899e-06}, {"id": 349, "seek": 215300, "start": 2166.7, "end": 2177.4, "text": " It's certainly not a substitute for actual practice doing end-to-end data science projects", "tokens": [467, 311, 3297, 406, 257, 15802, 337, 3539, 3124, 884, 917, 12, 1353, 12, 521, 1412, 3497, 4455], "temperature": 0.0, "avg_logprob": -0.08030358950297038, "compression_ratio": 1.5902439024390245, "no_speech_prob": 4.092839844815899e-06}, {"id": 350, "seek": 217740, "start": 2177.4, "end": 2183.7000000000003, "text": " but for becoming good at creating predictive models that are predictive it's a really fantastic", "tokens": [457, 337, 5617, 665, 412, 4084, 35521, 5245, 300, 366, 35521, 309, 311, 257, 534, 5456], "temperature": 0.0, "avg_logprob": -0.10773440105159109, "compression_ratio": 1.6593886462882097, "no_speech_prob": 2.3687744032940827e-06}, {"id": 351, "seek": 217740, "start": 2183.7000000000003, "end": 2190.0, "text": " resource highly recommended and you can also submit to old most old competitions to see", "tokens": [7684, 5405, 9628, 293, 291, 393, 611, 10315, 281, 1331, 881, 1331, 26185, 281, 536], "temperature": 0.0, "avg_logprob": -0.10773440105159109, "compression_ratio": 1.6593886462882097, "no_speech_prob": 2.3687744032940827e-06}, {"id": 352, "seek": 217740, "start": 2190.0, "end": 2193.84, "text": " how you would have gone without having to worry about you know the kind of stress of", "tokens": [577, 291, 576, 362, 2780, 1553, 1419, 281, 3292, 466, 291, 458, 264, 733, 295, 4244, 295], "temperature": 0.0, "avg_logprob": -0.10773440105159109, "compression_ratio": 1.6593886462882097, "no_speech_prob": 2.3687744032940827e-06}, {"id": 353, "seek": 217740, "start": 2193.84, "end": 2200.36, "text": " like whether people will be looking at your results because they're not publicized or", "tokens": [411, 1968, 561, 486, 312, 1237, 412, 428, 3542, 570, 436, 434, 406, 1908, 1602, 420], "temperature": 0.0, "avg_logprob": -0.10773440105159109, "compression_ratio": 1.6593886462882097, "no_speech_prob": 2.3687744032940827e-06}, {"id": 354, "seek": 217740, "start": 2200.36, "end": 2201.92, "text": " published if you do that.", "tokens": [6572, 498, 291, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.10773440105159109, "compression_ratio": 1.6593886462882097, "no_speech_prob": 2.3687744032940827e-06}, {"id": 355, "seek": 220192, "start": 2201.92, "end": 2208.48, "text": " There's a question, can you comment on real-time applications of random forest?", "tokens": [821, 311, 257, 1168, 11, 393, 291, 2871, 322, 957, 12, 3766, 5821, 295, 4974, 6719, 30], "temperature": 0.0, "avg_logprob": -0.21468137015759106, "compression_ratio": 1.46875, "no_speech_prob": 6.339104402286466e-06}, {"id": 356, "seek": 220192, "start": 2208.48, "end": 2215.16, "text": " My experience they tend to be too slow for real-time use cases like a recommender system", "tokens": [1222, 1752, 436, 3928, 281, 312, 886, 2964, 337, 957, 12, 3766, 764, 3331, 411, 257, 2748, 260, 1185], "temperature": 0.0, "avg_logprob": -0.21468137015759106, "compression_ratio": 1.46875, "no_speech_prob": 6.339104402286466e-06}, {"id": 357, "seek": 220192, "start": 2215.16, "end": 2218.92, "text": " neural network is much faster when run on the right hardware.", "tokens": [18161, 3209, 307, 709, 4663, 562, 1190, 322, 264, 558, 8837, 13], "temperature": 0.0, "avg_logprob": -0.21468137015759106, "compression_ratio": 1.46875, "no_speech_prob": 6.339104402286466e-06}, {"id": 358, "seek": 220192, "start": 2218.92, "end": 2224.96, "text": " Let's get to that one thing what they are shall we.", "tokens": [961, 311, 483, 281, 300, 472, 551, 437, 436, 366, 4393, 321, 13], "temperature": 0.0, "avg_logprob": -0.21468137015759106, "compression_ratio": 1.46875, "no_speech_prob": 6.339104402286466e-06}, {"id": 359, "seek": 222496, "start": 2224.96, "end": 2232.2400000000002, "text": " Now you can't just download an unta caggle data sets using the unta data thing that we", "tokens": [823, 291, 393, 380, 445, 5484, 364, 517, 1328, 269, 559, 22631, 1412, 6352, 1228, 264, 517, 1328, 1412, 551, 300, 321], "temperature": 0.0, "avg_logprob": -0.15312634981595552, "compression_ratio": 1.7117903930131004, "no_speech_prob": 1.1365594900780707e-06}, {"id": 360, "seek": 222496, "start": 2232.2400000000002, "end": 2236.86, "text": " have in fast AI so you actually have to sign up to caggle and then follow these instructions", "tokens": [362, 294, 2370, 7318, 370, 291, 767, 362, 281, 1465, 493, 281, 269, 559, 22631, 293, 550, 1524, 613, 9415], "temperature": 0.0, "avg_logprob": -0.15312634981595552, "compression_ratio": 1.7117903930131004, "no_speech_prob": 1.1365594900780707e-06}, {"id": 361, "seek": 222496, "start": 2236.86, "end": 2241.36, "text": " for how to download data from caggle.", "tokens": [337, 577, 281, 5484, 1412, 490, 269, 559, 22631, 13], "temperature": 0.0, "avg_logprob": -0.15312634981595552, "compression_ratio": 1.7117903930131004, "no_speech_prob": 1.1365594900780707e-06}, {"id": 362, "seek": 222496, "start": 2241.36, "end": 2246.56, "text": " Make sure you replace creds here with what it describes you need to get a special API", "tokens": [4387, 988, 291, 7406, 3864, 82, 510, 365, 437, 309, 15626, 291, 643, 281, 483, 257, 2121, 9362], "temperature": 0.0, "avg_logprob": -0.15312634981595552, "compression_ratio": 1.7117903930131004, "no_speech_prob": 1.1365594900780707e-06}, {"id": 363, "seek": 222496, "start": 2246.56, "end": 2254.76, "text": " code and then run this one time to put that up on your server and now you can use caggle", "tokens": [3089, 293, 550, 1190, 341, 472, 565, 281, 829, 300, 493, 322, 428, 7154, 293, 586, 291, 393, 764, 269, 559, 22631], "temperature": 0.0, "avg_logprob": -0.15312634981595552, "compression_ratio": 1.7117903930131004, "no_speech_prob": 1.1365594900780707e-06}, {"id": 364, "seek": 225476, "start": 2254.76, "end": 2259.32, "text": " to download data using the API.", "tokens": [281, 5484, 1412, 1228, 264, 9362, 13], "temperature": 0.0, "avg_logprob": -0.1624431839908462, "compression_ratio": 1.5263157894736843, "no_speech_prob": 8.764243375480874e-08}, {"id": 365, "seek": 225476, "start": 2259.32, "end": 2266.0400000000004, "text": " So after we do that we're going to end up with a bunch of as you see CSV files so let's", "tokens": [407, 934, 321, 360, 300, 321, 434, 516, 281, 917, 493, 365, 257, 3840, 295, 382, 291, 536, 48814, 7098, 370, 718, 311], "temperature": 0.0, "avg_logprob": -0.1624431839908462, "compression_ratio": 1.5263157894736843, "no_speech_prob": 8.764243375480874e-08}, {"id": 366, "seek": 225476, "start": 2266.0400000000004, "end": 2269.36, "text": " take a look at this data.", "tokens": [747, 257, 574, 412, 341, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1624431839908462, "compression_ratio": 1.5263157894736843, "no_speech_prob": 8.764243375480874e-08}, {"id": 367, "seek": 225476, "start": 2269.36, "end": 2276.0, "text": " So the main data the main table is train.csv remember that's comma separated values and", "tokens": [407, 264, 2135, 1412, 264, 2135, 3199, 307, 3847, 13, 14368, 85, 1604, 300, 311, 22117, 12005, 4190, 293], "temperature": 0.0, "avg_logprob": -0.1624431839908462, "compression_ratio": 1.5263157894736843, "no_speech_prob": 8.764243375480874e-08}, {"id": 368, "seek": 225476, "start": 2276.0, "end": 2281.0, "text": " the training set contains information such as unique identifier of a sale, the unique", "tokens": [264, 3097, 992, 8306, 1589, 1270, 382, 3845, 45690, 295, 257, 8680, 11, 264, 3845], "temperature": 0.0, "avg_logprob": -0.1624431839908462, "compression_ratio": 1.5263157894736843, "no_speech_prob": 8.764243375480874e-08}, {"id": 369, "seek": 228100, "start": 2281.0, "end": 2285.08, "text": " identifier of a machine, the sale price, the sale date.", "tokens": [45690, 295, 257, 3479, 11, 264, 8680, 3218, 11, 264, 8680, 4002, 13], "temperature": 0.0, "avg_logprob": -0.14699117263945022, "compression_ratio": 1.6493506493506493, "no_speech_prob": 4.4508442442747764e-07}, {"id": 370, "seek": 228100, "start": 2285.08, "end": 2290.6, "text": " So what's going on here is one row of the data represents a single sale of a single", "tokens": [407, 437, 311, 516, 322, 510, 307, 472, 5386, 295, 264, 1412, 8855, 257, 2167, 8680, 295, 257, 2167], "temperature": 0.0, "avg_logprob": -0.14699117263945022, "compression_ratio": 1.6493506493506493, "no_speech_prob": 4.4508442442747764e-07}, {"id": 371, "seek": 228100, "start": 2290.6, "end": 2295.16, "text": " piece of heavy machinery like a bulldozer at an auction.", "tokens": [2522, 295, 4676, 27302, 411, 257, 4693, 2595, 4527, 412, 364, 24139, 13], "temperature": 0.0, "avg_logprob": -0.14699117263945022, "compression_ratio": 1.6493506493506493, "no_speech_prob": 4.4508442442747764e-07}, {"id": 372, "seek": 228100, "start": 2295.16, "end": 2301.2, "text": " So it happens at a date as a price it's of some particular piece of equipment and so", "tokens": [407, 309, 2314, 412, 257, 4002, 382, 257, 3218, 309, 311, 295, 512, 1729, 2522, 295, 5927, 293, 370], "temperature": 0.0, "avg_logprob": -0.14699117263945022, "compression_ratio": 1.6493506493506493, "no_speech_prob": 4.4508442442747764e-07}, {"id": 373, "seek": 228100, "start": 2301.2, "end": 2303.04, "text": " forth.", "tokens": [5220, 13], "temperature": 0.0, "avg_logprob": -0.14699117263945022, "compression_ratio": 1.6493506493506493, "no_speech_prob": 4.4508442442747764e-07}, {"id": 374, "seek": 228100, "start": 2303.04, "end": 2310.12, "text": " So if we use pandas again to read in the CSV file let's combine training and valid together.", "tokens": [407, 498, 321, 764, 4565, 296, 797, 281, 1401, 294, 264, 48814, 3991, 718, 311, 10432, 3097, 293, 7363, 1214, 13], "temperature": 0.0, "avg_logprob": -0.14699117263945022, "compression_ratio": 1.6493506493506493, "no_speech_prob": 4.4508442442747764e-07}, {"id": 375, "seek": 231012, "start": 2310.12, "end": 2315.24, "text": " We can then look at the columns to see there's a lot of columns there and many things which", "tokens": [492, 393, 550, 574, 412, 264, 13766, 281, 536, 456, 311, 257, 688, 295, 13766, 456, 293, 867, 721, 597], "temperature": 0.0, "avg_logprob": -0.0912373992037182, "compression_ratio": 1.8609022556390977, "no_speech_prob": 1.1911060937563889e-06}, {"id": 376, "seek": 231012, "start": 2315.24, "end": 2320.92, "text": " I don't know what the hell they mean like blade extension and pad type and ride control.", "tokens": [286, 500, 380, 458, 437, 264, 4921, 436, 914, 411, 10959, 10320, 293, 6887, 2010, 293, 5077, 1969, 13], "temperature": 0.0, "avg_logprob": -0.0912373992037182, "compression_ratio": 1.8609022556390977, "no_speech_prob": 1.1911060937563889e-06}, {"id": 377, "seek": 231012, "start": 2320.92, "end": 2324.74, "text": " But the good news is we're going to show you a way that you don't have to look at every", "tokens": [583, 264, 665, 2583, 307, 321, 434, 516, 281, 855, 291, 257, 636, 300, 291, 500, 380, 362, 281, 574, 412, 633], "temperature": 0.0, "avg_logprob": -0.0912373992037182, "compression_ratio": 1.8609022556390977, "no_speech_prob": 1.1911060937563889e-06}, {"id": 378, "seek": 231012, "start": 2324.74, "end": 2328.72, "text": " single column and understand what they mean and random forests are going to help us with", "tokens": [2167, 7738, 293, 1223, 437, 436, 914, 293, 4974, 21700, 366, 516, 281, 854, 505, 365], "temperature": 0.0, "avg_logprob": -0.0912373992037182, "compression_ratio": 1.8609022556390977, "no_speech_prob": 1.1911060937563889e-06}, {"id": 379, "seek": 231012, "start": 2328.72, "end": 2331.08, "text": " that as well.", "tokens": [300, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.0912373992037182, "compression_ratio": 1.8609022556390977, "no_speech_prob": 1.1911060937563889e-06}, {"id": 380, "seek": 231012, "start": 2331.08, "end": 2334.96, "text": " So once again we're going to be seeing this idea that models can actually help us with", "tokens": [407, 1564, 797, 321, 434, 516, 281, 312, 2577, 341, 1558, 300, 5245, 393, 767, 854, 505, 365], "temperature": 0.0, "avg_logprob": -0.0912373992037182, "compression_ratio": 1.8609022556390977, "no_speech_prob": 1.1911060937563889e-06}, {"id": 381, "seek": 231012, "start": 2334.96, "end": 2338.68, "text": " data understanding and data cleanup.", "tokens": [1412, 3701, 293, 1412, 40991, 13], "temperature": 0.0, "avg_logprob": -0.0912373992037182, "compression_ratio": 1.8609022556390977, "no_speech_prob": 1.1911060937563889e-06}, {"id": 382, "seek": 233868, "start": 2338.68, "end": 2342.8399999999997, "text": " One thing we can look at is ordinal columns, a good place to look at that now.", "tokens": [1485, 551, 321, 393, 574, 412, 307, 4792, 2071, 13766, 11, 257, 665, 1081, 281, 574, 412, 300, 586, 13], "temperature": 0.0, "avg_logprob": -0.16491971232674338, "compression_ratio": 1.7417840375586855, "no_speech_prob": 9.42242479595734e-07}, {"id": 383, "seek": 233868, "start": 2342.8399999999997, "end": 2348.74, "text": " If there's things there that you know are discrete values but have some order like product", "tokens": [759, 456, 311, 721, 456, 300, 291, 458, 366, 27706, 4190, 457, 362, 512, 1668, 411, 1674], "temperature": 0.0, "avg_logprob": -0.16491971232674338, "compression_ratio": 1.7417840375586855, "no_speech_prob": 9.42242479595734e-07}, {"id": 384, "seek": 233868, "start": 2348.74, "end": 2357.3599999999997, "text": " size it has medium and small and large medium and many these should not be in you know alphabetical", "tokens": [2744, 309, 575, 6399, 293, 1359, 293, 2416, 6399, 293, 867, 613, 820, 406, 312, 294, 291, 458, 23339, 804], "temperature": 0.0, "avg_logprob": -0.16491971232674338, "compression_ratio": 1.7417840375586855, "no_speech_prob": 9.42242479595734e-07}, {"id": 385, "seek": 233868, "start": 2357.3599999999997, "end": 2363.3199999999997, "text": " order or some random order they should be in this specific order right they have a specific", "tokens": [1668, 420, 512, 4974, 1668, 436, 820, 312, 294, 341, 2685, 1668, 558, 436, 362, 257, 2685], "temperature": 0.0, "avg_logprob": -0.16491971232674338, "compression_ratio": 1.7417840375586855, "no_speech_prob": 9.42242479595734e-07}, {"id": 386, "seek": 233868, "start": 2363.3199999999997, "end": 2364.7799999999997, "text": " ordering.", "tokens": [21739, 13], "temperature": 0.0, "avg_logprob": -0.16491971232674338, "compression_ratio": 1.7417840375586855, "no_speech_prob": 9.42242479595734e-07}, {"id": 387, "seek": 236478, "start": 2364.78, "end": 2373.8, "text": " So we can use as type to turn it into a categorical variable and then we can say set categories", "tokens": [407, 321, 393, 764, 382, 2010, 281, 1261, 309, 666, 257, 19250, 804, 7006, 293, 550, 321, 393, 584, 992, 10479], "temperature": 0.0, "avg_logprob": -0.08261911641983759, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.496698243703577e-07}, {"id": 388, "seek": 236478, "start": 2373.8, "end": 2378.8, "text": " ordered equals true to basically say this is an ordinal column so it's got discrete", "tokens": [8866, 6915, 2074, 281, 1936, 584, 341, 307, 364, 4792, 2071, 7738, 370, 309, 311, 658, 27706], "temperature": 0.0, "avg_logprob": -0.08261911641983759, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.496698243703577e-07}, {"id": 389, "seek": 236478, "start": 2378.8, "end": 2386.76, "text": " values but we actually want to define what the order of the classes are.", "tokens": [4190, 457, 321, 767, 528, 281, 6964, 437, 264, 1668, 295, 264, 5359, 366, 13], "temperature": 0.0, "avg_logprob": -0.08261911641983759, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.496698243703577e-07}, {"id": 390, "seek": 236478, "start": 2386.76, "end": 2391.1200000000003, "text": " We need to choose which is the dependent variable and we do that by looking on Kaggle and Kaggle", "tokens": [492, 643, 281, 2826, 597, 307, 264, 12334, 7006, 293, 321, 360, 300, 538, 1237, 322, 48751, 22631, 293, 48751, 22631], "temperature": 0.0, "avg_logprob": -0.08261911641983759, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.496698243703577e-07}, {"id": 391, "seek": 239112, "start": 2391.12, "end": 2396.12, "text": " will tell us that the thing we're meant to be predicting is sale price and actually specifically", "tokens": [486, 980, 505, 300, 264, 551, 321, 434, 4140, 281, 312, 32884, 307, 8680, 3218, 293, 767, 4682], "temperature": 0.0, "avg_logprob": -0.11777945927211217, "compression_ratio": 2.013215859030837, "no_speech_prob": 3.520909785947879e-07}, {"id": 392, "seek": 239112, "start": 2396.12, "end": 2399.7599999999998, "text": " they'll tell us the thing we're meant to be predicting is the log of sale price because", "tokens": [436, 603, 980, 505, 264, 551, 321, 434, 4140, 281, 312, 32884, 307, 264, 3565, 295, 8680, 3218, 570], "temperature": 0.0, "avg_logprob": -0.11777945927211217, "compression_ratio": 2.013215859030837, "no_speech_prob": 3.520909785947879e-07}, {"id": 393, "seek": 239112, "start": 2399.7599999999998, "end": 2405.62, "text": " root mean squared log error is the is the what we're actually going to be judged on", "tokens": [5593, 914, 8889, 3565, 6713, 307, 264, 307, 264, 437, 321, 434, 767, 516, 281, 312, 27485, 322], "temperature": 0.0, "avg_logprob": -0.11777945927211217, "compression_ratio": 2.013215859030837, "no_speech_prob": 3.520909785947879e-07}, {"id": 394, "seek": 239112, "start": 2405.62, "end": 2409.04, "text": " in the competition where we take the log.", "tokens": [294, 264, 6211, 689, 321, 747, 264, 3565, 13], "temperature": 0.0, "avg_logprob": -0.11777945927211217, "compression_ratio": 2.013215859030837, "no_speech_prob": 3.520909785947879e-07}, {"id": 395, "seek": 239112, "start": 2409.04, "end": 2412.96, "text": " So we're going to replace sale price with its log and that's what we'll be using from", "tokens": [407, 321, 434, 516, 281, 7406, 8680, 3218, 365, 1080, 3565, 293, 300, 311, 437, 321, 603, 312, 1228, 490], "temperature": 0.0, "avg_logprob": -0.11777945927211217, "compression_ratio": 2.013215859030837, "no_speech_prob": 3.520909785947879e-07}, {"id": 396, "seek": 239112, "start": 2412.96, "end": 2415.2, "text": " now on.", "tokens": [586, 322, 13], "temperature": 0.0, "avg_logprob": -0.11777945927211217, "compression_ratio": 2.013215859030837, "no_speech_prob": 3.520909785947879e-07}, {"id": 397, "seek": 239112, "start": 2415.2, "end": 2419.04, "text": " So a decision tree ensemble requires decision trees.", "tokens": [407, 257, 3537, 4230, 19492, 7029, 3537, 5852, 13], "temperature": 0.0, "avg_logprob": -0.11777945927211217, "compression_ratio": 2.013215859030837, "no_speech_prob": 3.520909785947879e-07}, {"id": 398, "seek": 241904, "start": 2419.04, "end": 2421.6, "text": " So let's start by looking at decision trees.", "tokens": [407, 718, 311, 722, 538, 1237, 412, 3537, 5852, 13], "temperature": 0.0, "avg_logprob": -0.17398519101350204, "compression_ratio": 1.775, "no_speech_prob": 8.446212405033293e-07}, {"id": 399, "seek": 241904, "start": 2421.6, "end": 2429.32, "text": " So a decision tree in in this case is a something that asks a series of binary that is yes or", "tokens": [407, 257, 3537, 4230, 294, 294, 341, 1389, 307, 257, 746, 300, 8962, 257, 2638, 295, 17434, 300, 307, 2086, 420], "temperature": 0.0, "avg_logprob": -0.17398519101350204, "compression_ratio": 1.775, "no_speech_prob": 8.446212405033293e-07}, {"id": 400, "seek": 241904, "start": 2429.32, "end": 2431.4, "text": " no questions about data.", "tokens": [572, 1651, 466, 1412, 13], "temperature": 0.0, "avg_logprob": -0.17398519101350204, "compression_ratio": 1.775, "no_speech_prob": 8.446212405033293e-07}, {"id": 401, "seek": 241904, "start": 2431.4, "end": 2435.52, "text": " So such as is somebody less than greater than less than 30.", "tokens": [407, 1270, 382, 307, 2618, 1570, 813, 5044, 813, 1570, 813, 2217, 13], "temperature": 0.0, "avg_logprob": -0.17398519101350204, "compression_ratio": 1.775, "no_speech_prob": 8.446212405033293e-07}, {"id": 402, "seek": 241904, "start": 2435.52, "end": 2436.52, "text": " Yes they are.", "tokens": [1079, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.17398519101350204, "compression_ratio": 1.775, "no_speech_prob": 8.446212405033293e-07}, {"id": 403, "seek": 241904, "start": 2436.52, "end": 2437.52, "text": " Are they eating healthily.", "tokens": [2014, 436, 3936, 1585, 953, 13], "temperature": 0.0, "avg_logprob": -0.17398519101350204, "compression_ratio": 1.775, "no_speech_prob": 8.446212405033293e-07}, {"id": 404, "seek": 241904, "start": 2437.52, "end": 2438.52, "text": " Yes they are.", "tokens": [1079, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.17398519101350204, "compression_ratio": 1.775, "no_speech_prob": 8.446212405033293e-07}, {"id": 405, "seek": 241904, "start": 2438.52, "end": 2441.8, "text": " And so okay then we're going to say they are fit or unfit.", "tokens": [400, 370, 1392, 550, 321, 434, 516, 281, 584, 436, 366, 3318, 420, 3971, 270, 13], "temperature": 0.0, "avg_logprob": -0.17398519101350204, "compression_ratio": 1.775, "no_speech_prob": 8.446212405033293e-07}, {"id": 406, "seek": 241904, "start": 2441.8, "end": 2447.88, "text": " So like there's an example of some arbitrary decision tree that somebody might have come", "tokens": [407, 411, 456, 311, 364, 1365, 295, 512, 23211, 3537, 4230, 300, 2618, 1062, 362, 808], "temperature": 0.0, "avg_logprob": -0.17398519101350204, "compression_ratio": 1.775, "no_speech_prob": 8.446212405033293e-07}, {"id": 407, "seek": 244788, "start": 2447.88, "end": 2453.36, "text": " up with it's a series of binary yes and no choices and at the bottom are leaf nodes that", "tokens": [493, 365, 309, 311, 257, 2638, 295, 17434, 2086, 293, 572, 7994, 293, 412, 264, 2767, 366, 10871, 13891, 300], "temperature": 0.0, "avg_logprob": -0.17799235612918168, "compression_ratio": 1.6205128205128205, "no_speech_prob": 1.93334949472046e-06}, {"id": 408, "seek": 244788, "start": 2453.36, "end": 2455.4, "text": " make some prediction.", "tokens": [652, 512, 17630, 13], "temperature": 0.0, "avg_logprob": -0.17799235612918168, "compression_ratio": 1.6205128205128205, "no_speech_prob": 1.93334949472046e-06}, {"id": 409, "seek": 244788, "start": 2455.4, "end": 2466.92, "text": " Now of course for our bulldozers competition we don't know what binary questions to ask", "tokens": [823, 295, 1164, 337, 527, 4693, 2595, 41698, 6211, 321, 500, 380, 458, 437, 17434, 1651, 281, 1029], "temperature": 0.0, "avg_logprob": -0.17799235612918168, "compression_ratio": 1.6205128205128205, "no_speech_prob": 1.93334949472046e-06}, {"id": 410, "seek": 244788, "start": 2466.92, "end": 2472.4, "text": " about these things and in what order in order to make a prediction about sale price.", "tokens": [466, 613, 721, 293, 294, 437, 1668, 294, 1668, 281, 652, 257, 17630, 466, 8680, 3218, 13], "temperature": 0.0, "avg_logprob": -0.17799235612918168, "compression_ratio": 1.6205128205128205, "no_speech_prob": 1.93334949472046e-06}, {"id": 411, "seek": 244788, "start": 2472.4, "end": 2474.52, "text": " So we're doing machine learning.", "tokens": [407, 321, 434, 884, 3479, 2539, 13], "temperature": 0.0, "avg_logprob": -0.17799235612918168, "compression_ratio": 1.6205128205128205, "no_speech_prob": 1.93334949472046e-06}, {"id": 412, "seek": 247452, "start": 2474.52, "end": 2480.44, "text": " So we're going to try and come up with some automated way to create the questions and", "tokens": [407, 321, 434, 516, 281, 853, 293, 808, 493, 365, 512, 18473, 636, 281, 1884, 264, 1651, 293], "temperature": 0.0, "avg_logprob": -0.09386131551006052, "compression_ratio": 1.776, "no_speech_prob": 1.602804331923835e-06}, {"id": 413, "seek": 247452, "start": 2480.44, "end": 2483.4, "text": " there's actually a really simple procedure for doing that.", "tokens": [456, 311, 767, 257, 534, 2199, 10747, 337, 884, 300, 13], "temperature": 0.0, "avg_logprob": -0.09386131551006052, "compression_ratio": 1.776, "no_speech_prob": 1.602804331923835e-06}, {"id": 414, "seek": 247452, "start": 2483.4, "end": 2484.4, "text": " You have a think about it.", "tokens": [509, 362, 257, 519, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.09386131551006052, "compression_ratio": 1.776, "no_speech_prob": 1.602804331923835e-06}, {"id": 415, "seek": 247452, "start": 2484.4, "end": 2488.92, "text": " So if you want to kind of stretch yourself here have a think about what's an automatic", "tokens": [407, 498, 291, 528, 281, 733, 295, 5985, 1803, 510, 362, 257, 519, 466, 437, 311, 364, 12509], "temperature": 0.0, "avg_logprob": -0.09386131551006052, "compression_ratio": 1.776, "no_speech_prob": 1.602804331923835e-06}, {"id": 416, "seek": 247452, "start": 2488.92, "end": 2495.08, "text": " procedure that you can come up with that would automatically build a decision tree where", "tokens": [10747, 300, 291, 393, 808, 493, 365, 300, 576, 6772, 1322, 257, 3537, 4230, 689], "temperature": 0.0, "avg_logprob": -0.09386131551006052, "compression_ratio": 1.776, "no_speech_prob": 1.602804331923835e-06}, {"id": 417, "seek": 247452, "start": 2495.08, "end": 2501.84, "text": " the final answer would do a you know significantly better than random job of estimating the sale", "tokens": [264, 2572, 1867, 576, 360, 257, 291, 458, 10591, 1101, 813, 4974, 1691, 295, 8017, 990, 264, 8680], "temperature": 0.0, "avg_logprob": -0.09386131551006052, "compression_ratio": 1.776, "no_speech_prob": 1.602804331923835e-06}, {"id": 418, "seek": 250184, "start": 2501.84, "end": 2507.04, "text": " price of one of these options.", "tokens": [3218, 295, 472, 295, 613, 3956, 13], "temperature": 0.0, "avg_logprob": -0.25260516247117376, "compression_ratio": 1.5771144278606966, "no_speech_prob": 1.1015935115210596e-06}, {"id": 419, "seek": 250184, "start": 2507.04, "end": 2514.8, "text": " Right so here's here's the approach that we could use loop through each column of the", "tokens": [1779, 370, 510, 311, 510, 311, 264, 3109, 300, 321, 727, 764, 6367, 807, 1184, 7738, 295, 264], "temperature": 0.0, "avg_logprob": -0.25260516247117376, "compression_ratio": 1.5771144278606966, "no_speech_prob": 1.1015935115210596e-06}, {"id": 420, "seek": 250184, "start": 2514.8, "end": 2515.8, "text": " data set.", "tokens": [1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.25260516247117376, "compression_ratio": 1.5771144278606966, "no_speech_prob": 1.1015935115210596e-06}, {"id": 421, "seek": 250184, "start": 2515.8, "end": 2520.1200000000003, "text": " We're going to go through each of well obviously not sale price that's a dependent variable", "tokens": [492, 434, 516, 281, 352, 807, 1184, 295, 731, 2745, 406, 8680, 3218, 300, 311, 257, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.25260516247117376, "compression_ratio": 1.5771144278606966, "no_speech_prob": 1.1015935115210596e-06}, {"id": 422, "seek": 250184, "start": 2520.1200000000003, "end": 2527.32, "text": " sale ID machine ID auctioneer year made etc. and so one of those will be for example product", "tokens": [8680, 7348, 3479, 7348, 24139, 68, 260, 1064, 1027, 5183, 13, 293, 370, 472, 295, 729, 486, 312, 337, 1365, 1674], "temperature": 0.0, "avg_logprob": -0.25260516247117376, "compression_ratio": 1.5771144278606966, "no_speech_prob": 1.1015935115210596e-06}, {"id": 423, "seek": 250184, "start": 2527.32, "end": 2529.2400000000002, "text": " size.", "tokens": [2744, 13], "temperature": 0.0, "avg_logprob": -0.25260516247117376, "compression_ratio": 1.5771144278606966, "no_speech_prob": 1.1015935115210596e-06}, {"id": 424, "seek": 252924, "start": 2529.24, "end": 2536.68, "text": " And so then what we're going to do is we're going to loop through each possible value", "tokens": [400, 370, 550, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 6367, 807, 1184, 1944, 2158], "temperature": 0.0, "avg_logprob": -0.10549760706284467, "compression_ratio": 1.7611940298507462, "no_speech_prob": 3.927856937480101e-07}, {"id": 425, "seek": 252924, "start": 2536.68, "end": 2543.24, "text": " of product size large large medium medium etc. and then we're going to do a split basically", "tokens": [295, 1674, 2744, 2416, 2416, 6399, 6399, 5183, 13, 293, 550, 321, 434, 516, 281, 360, 257, 7472, 1936], "temperature": 0.0, "avg_logprob": -0.10549760706284467, "compression_ratio": 1.7611940298507462, "no_speech_prob": 3.927856937480101e-07}, {"id": 426, "seek": 252924, "start": 2543.24, "end": 2549.16, "text": " like where this comma is and we're going to say okay let's get all of the options of large", "tokens": [411, 689, 341, 22117, 307, 293, 321, 434, 516, 281, 584, 1392, 718, 311, 483, 439, 295, 264, 3956, 295, 2416], "temperature": 0.0, "avg_logprob": -0.10549760706284467, "compression_ratio": 1.7611940298507462, "no_speech_prob": 3.927856937480101e-07}, {"id": 427, "seek": 252924, "start": 2549.16, "end": 2554.68, "text": " equipment and put that into one group and everything that's smaller than that and put", "tokens": [5927, 293, 829, 300, 666, 472, 1594, 293, 1203, 300, 311, 4356, 813, 300, 293, 829], "temperature": 0.0, "avg_logprob": -0.10549760706284467, "compression_ratio": 1.7611940298507462, "no_speech_prob": 3.927856937480101e-07}, {"id": 428, "seek": 255468, "start": 2554.68, "end": 2562.56, "text": " that into another group and so that's here split the data into two groups based on whether", "tokens": [300, 666, 1071, 1594, 293, 370, 300, 311, 510, 7472, 264, 1412, 666, 732, 3935, 2361, 322, 1968], "temperature": 0.0, "avg_logprob": -0.09739653918207908, "compression_ratio": 1.837962962962963, "no_speech_prob": 1.197930288299176e-07}, {"id": 429, "seek": 255468, "start": 2562.56, "end": 2565.7599999999998, "text": " they're greater than or less than that value.", "tokens": [436, 434, 5044, 813, 420, 1570, 813, 300, 2158, 13], "temperature": 0.0, "avg_logprob": -0.09739653918207908, "compression_ratio": 1.837962962962963, "no_speech_prob": 1.197930288299176e-07}, {"id": 430, "seek": 255468, "start": 2565.7599999999998, "end": 2569.7599999999998, "text": " If it's a categorical non-ordinal value a variable it'll be just whether it's equal", "tokens": [759, 309, 311, 257, 19250, 804, 2107, 12, 765, 2071, 2158, 257, 7006, 309, 603, 312, 445, 1968, 309, 311, 2681], "temperature": 0.0, "avg_logprob": -0.09739653918207908, "compression_ratio": 1.837962962962963, "no_speech_prob": 1.197930288299176e-07}, {"id": 431, "seek": 255468, "start": 2569.7599999999998, "end": 2575.68, "text": " or not equal to that level and then we're going to find the average sale price for each", "tokens": [420, 406, 2681, 281, 300, 1496, 293, 550, 321, 434, 516, 281, 915, 264, 4274, 8680, 3218, 337, 1184], "temperature": 0.0, "avg_logprob": -0.09739653918207908, "compression_ratio": 1.837962962962963, "no_speech_prob": 1.197930288299176e-07}, {"id": 432, "seek": 255468, "start": 2575.68, "end": 2581.0, "text": " of the two groups so for the large group what was the average sale price for the smaller", "tokens": [295, 264, 732, 3935, 370, 337, 264, 2416, 1594, 437, 390, 264, 4274, 8680, 3218, 337, 264, 4356], "temperature": 0.0, "avg_logprob": -0.09739653918207908, "compression_ratio": 1.837962962962963, "no_speech_prob": 1.197930288299176e-07}, {"id": 433, "seek": 258100, "start": 2581.0, "end": 2587.04, "text": " than large group what was the average sale price and that will be our model our prediction", "tokens": [813, 2416, 1594, 437, 390, 264, 4274, 8680, 3218, 293, 300, 486, 312, 527, 2316, 527, 17630], "temperature": 0.0, "avg_logprob": -0.10496732300403071, "compression_ratio": 2.0, "no_speech_prob": 1.308174660152872e-06}, {"id": 434, "seek": 258100, "start": 2587.04, "end": 2592.52, "text": " will simply be the average sale price for that group and so then you can say well how", "tokens": [486, 2935, 312, 264, 4274, 8680, 3218, 337, 300, 1594, 293, 370, 550, 291, 393, 584, 731, 577], "temperature": 0.0, "avg_logprob": -0.10496732300403071, "compression_ratio": 2.0, "no_speech_prob": 1.308174660152872e-06}, {"id": 435, "seek": 258100, "start": 2592.52, "end": 2597.32, "text": " good is that model if our model was just to ask a single question with a yes no answer", "tokens": [665, 307, 300, 2316, 498, 527, 2316, 390, 445, 281, 1029, 257, 2167, 1168, 365, 257, 2086, 572, 1867], "temperature": 0.0, "avg_logprob": -0.10496732300403071, "compression_ratio": 2.0, "no_speech_prob": 1.308174660152872e-06}, {"id": 436, "seek": 258100, "start": 2597.32, "end": 2602.28, "text": " put things into two groups and take the average of the group as being our prediction and we", "tokens": [829, 721, 666, 732, 3935, 293, 747, 264, 4274, 295, 264, 1594, 382, 885, 527, 17630, 293, 321], "temperature": 0.0, "avg_logprob": -0.10496732300403071, "compression_ratio": 2.0, "no_speech_prob": 1.308174660152872e-06}, {"id": 437, "seek": 258100, "start": 2602.28, "end": 2606.28, "text": " can say how good would that model be what would be the root means grid error from that", "tokens": [393, 584, 577, 665, 576, 300, 2316, 312, 437, 576, 312, 264, 5593, 1355, 10748, 6713, 490, 300], "temperature": 0.0, "avg_logprob": -0.10496732300403071, "compression_ratio": 2.0, "no_speech_prob": 1.308174660152872e-06}, {"id": 438, "seek": 260628, "start": 2606.28, "end": 2613.2400000000002, "text": " model and so we can then say all right how good it would it be if we use large as it", "tokens": [2316, 293, 370, 321, 393, 550, 584, 439, 558, 577, 665, 309, 576, 309, 312, 498, 321, 764, 2416, 382, 309], "temperature": 0.0, "avg_logprob": -0.12481520149145234, "compression_ratio": 1.7989690721649485, "no_speech_prob": 1.003012130240677e-06}, {"id": 439, "seek": 260628, "start": 2613.2400000000002, "end": 2618.84, "text": " and then let's try again what if we did large slash medium is a split what if we did medium", "tokens": [293, 550, 718, 311, 853, 797, 437, 498, 321, 630, 2416, 17330, 6399, 307, 257, 7472, 437, 498, 321, 630, 6399], "temperature": 0.0, "avg_logprob": -0.12481520149145234, "compression_ratio": 1.7989690721649485, "no_speech_prob": 1.003012130240677e-06}, {"id": 440, "seek": 260628, "start": 2618.84, "end": 2622.7000000000003, "text": " is a split and so in each case we can find the root means grid error of that incredibly", "tokens": [307, 257, 7472, 293, 370, 294, 1184, 1389, 321, 393, 915, 264, 5593, 1355, 10748, 6713, 295, 300, 6252], "temperature": 0.0, "avg_logprob": -0.12481520149145234, "compression_ratio": 1.7989690721649485, "no_speech_prob": 1.003012130240677e-06}, {"id": 441, "seek": 260628, "start": 2622.7000000000003, "end": 2627.36, "text": " simple model and then once we've done that for all of the product size levels we can", "tokens": [2199, 2316, 293, 550, 1564, 321, 600, 1096, 300, 337, 439, 295, 264, 1674, 2744, 4358, 321, 393], "temperature": 0.0, "avg_logprob": -0.12481520149145234, "compression_ratio": 1.7989690721649485, "no_speech_prob": 1.003012130240677e-06}, {"id": 442, "seek": 262736, "start": 2627.36, "end": 2636.8, "text": " go to the next column and look at usage band and do every level of usage band and then", "tokens": [352, 281, 264, 958, 7738, 293, 574, 412, 14924, 4116, 293, 360, 633, 1496, 295, 14924, 4116, 293, 550], "temperature": 0.0, "avg_logprob": -0.10053087453373143, "compression_ratio": 1.7058823529411764, "no_speech_prob": 4.520937011420756e-07}, {"id": 443, "seek": 262736, "start": 2636.8, "end": 2644.56, "text": " state your level of state and so forth and so there'll be some variable and some split", "tokens": [1785, 428, 1496, 295, 1785, 293, 370, 5220, 293, 370, 456, 603, 312, 512, 7006, 293, 512, 7472], "temperature": 0.0, "avg_logprob": -0.10053087453373143, "compression_ratio": 1.7058823529411764, "no_speech_prob": 4.520937011420756e-07}, {"id": 444, "seek": 262736, "start": 2644.56, "end": 2651.1200000000003, "text": " level which gives the best root means grid error of this really really simple model and", "tokens": [1496, 597, 2709, 264, 1151, 5593, 1355, 10748, 6713, 295, 341, 534, 534, 2199, 2316, 293], "temperature": 0.0, "avg_logprob": -0.10053087453373143, "compression_ratio": 1.7058823529411764, "no_speech_prob": 4.520937011420756e-07}, {"id": 445, "seek": 265112, "start": 2651.12, "end": 2658.08, "text": " so then we'll say okay that would be our first binary decision it gives us two groups and", "tokens": [370, 550, 321, 603, 584, 1392, 300, 576, 312, 527, 700, 17434, 3537, 309, 2709, 505, 732, 3935, 293], "temperature": 0.0, "avg_logprob": -0.09499790142108867, "compression_ratio": 1.9120879120879122, "no_speech_prob": 4.592130551372975e-07}, {"id": 446, "seek": 265112, "start": 2658.08, "end": 2663.16, "text": " then we're going to take each one of those groups separately and create find another", "tokens": [550, 321, 434, 516, 281, 747, 1184, 472, 295, 729, 3935, 14759, 293, 1884, 915, 1071], "temperature": 0.0, "avg_logprob": -0.09499790142108867, "compression_ratio": 1.9120879120879122, "no_speech_prob": 4.592130551372975e-07}, {"id": 447, "seek": 265112, "start": 2663.16, "end": 2669.72, "text": " single binary decision for each of those two groups using exactly the same procedure so", "tokens": [2167, 17434, 3537, 337, 1184, 295, 729, 732, 3935, 1228, 2293, 264, 912, 10747, 370], "temperature": 0.0, "avg_logprob": -0.09499790142108867, "compression_ratio": 1.9120879120879122, "no_speech_prob": 4.592130551372975e-07}, {"id": 448, "seek": 265112, "start": 2669.72, "end": 2673.48, "text": " then we'll have four groups and then we'll do exactly the same thing again separately", "tokens": [550, 321, 603, 362, 1451, 3935, 293, 550, 321, 603, 360, 2293, 264, 912, 551, 797, 14759], "temperature": 0.0, "avg_logprob": -0.09499790142108867, "compression_ratio": 1.9120879120879122, "no_speech_prob": 4.592130551372975e-07}, {"id": 449, "seek": 267348, "start": 2673.48, "end": 2684.2400000000002, "text": " for each of those four groups and so forth so let's see what that looks like and in fact", "tokens": [337, 1184, 295, 729, 1451, 3935, 293, 370, 5220, 370, 718, 311, 536, 437, 300, 1542, 411, 293, 294, 1186], "temperature": 0.0, "avg_logprob": -0.08752487993788445, "compression_ratio": 1.6425339366515836, "no_speech_prob": 5.453287599266332e-07}, {"id": 450, "seek": 267348, "start": 2684.2400000000002, "end": 2687.56, "text": " once we've gone through this you might even want to see if you can implement this algorithm", "tokens": [1564, 321, 600, 2780, 807, 341, 291, 1062, 754, 528, 281, 536, 498, 291, 393, 4445, 341, 9284], "temperature": 0.0, "avg_logprob": -0.08752487993788445, "compression_ratio": 1.6425339366515836, "no_speech_prob": 5.453287599266332e-07}, {"id": 451, "seek": 267348, "start": 2687.56, "end": 2695.2, "text": " yourself it's not trivial but it doesn't require any special coding skills so hopefully you", "tokens": [1803, 309, 311, 406, 26703, 457, 309, 1177, 380, 3651, 604, 2121, 17720, 3942, 370, 4696, 291], "temperature": 0.0, "avg_logprob": -0.08752487993788445, "compression_ratio": 1.6425339366515836, "no_speech_prob": 5.453287599266332e-07}, {"id": 452, "seek": 267348, "start": 2695.2, "end": 2700.88, "text": " can find you'll be able to do it there's a few things we have to do before we can actually", "tokens": [393, 915, 291, 603, 312, 1075, 281, 360, 309, 456, 311, 257, 1326, 721, 321, 362, 281, 360, 949, 321, 393, 767], "temperature": 0.0, "avg_logprob": -0.08752487993788445, "compression_ratio": 1.6425339366515836, "no_speech_prob": 5.453287599266332e-07}, {"id": 453, "seek": 270088, "start": 2700.88, "end": 2706.6, "text": " create a decision tree in terms of just some basic data munging one is if we're going to", "tokens": [1884, 257, 3537, 4230, 294, 2115, 295, 445, 512, 3875, 1412, 275, 1063, 278, 472, 307, 498, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.17535593317843032, "compression_ratio": 1.7177033492822966, "no_speech_prob": 1.1365588079570443e-06}, {"id": 454, "seek": 270088, "start": 2706.6, "end": 2713.88, "text": " take advantage of dates we actually want to call fastai's add date part function and what", "tokens": [747, 5002, 295, 11691, 321, 767, 528, 281, 818, 2370, 1301, 311, 909, 4002, 644, 2445, 293, 437], "temperature": 0.0, "avg_logprob": -0.17535593317843032, "compression_ratio": 1.7177033492822966, "no_speech_prob": 1.1365588079570443e-06}, {"id": 455, "seek": 270088, "start": 2713.88, "end": 2719.48, "text": " that does as you see after we call it as it creates a whole different a bunch of different", "tokens": [300, 775, 382, 291, 536, 934, 321, 818, 309, 382, 309, 7829, 257, 1379, 819, 257, 3840, 295, 819], "temperature": 0.0, "avg_logprob": -0.17535593317843032, "compression_ratio": 1.7177033492822966, "no_speech_prob": 1.1365588079570443e-06}, {"id": 456, "seek": 270088, "start": 2719.48, "end": 2729.1600000000003, "text": " bits of metadata from that data say all year say your months or weeks or day and so forth", "tokens": [9239, 295, 26603, 490, 300, 1412, 584, 439, 1064, 584, 428, 2493, 420, 3259, 420, 786, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.17535593317843032, "compression_ratio": 1.7177033492822966, "no_speech_prob": 1.1365588079570443e-06}, {"id": 457, "seek": 272916, "start": 2729.16, "end": 2735.68, "text": " so say our date of itself doesn't have a whole lot of information directly but we can pull", "tokens": [370, 584, 527, 4002, 295, 2564, 1177, 380, 362, 257, 1379, 688, 295, 1589, 3838, 457, 321, 393, 2235], "temperature": 0.0, "avg_logprob": -0.10423479728328372, "compression_ratio": 1.8632478632478633, "no_speech_prob": 1.1365572163413162e-06}, {"id": 458, "seek": 272916, "start": 2735.68, "end": 2739.6, "text": " a lots of different information out of it and so this is an example of something called", "tokens": [257, 3195, 295, 819, 1589, 484, 295, 309, 293, 370, 341, 307, 364, 1365, 295, 746, 1219], "temperature": 0.0, "avg_logprob": -0.10423479728328372, "compression_ratio": 1.8632478632478633, "no_speech_prob": 1.1365572163413162e-06}, {"id": 459, "seek": 272916, "start": 2739.6, "end": 2744.6, "text": " feature engineering which is where we take some piece some piece of data and we try to", "tokens": [4111, 7043, 597, 307, 689, 321, 747, 512, 2522, 512, 2522, 295, 1412, 293, 321, 853, 281], "temperature": 0.0, "avg_logprob": -0.10423479728328372, "compression_ratio": 1.8632478632478633, "no_speech_prob": 1.1365572163413162e-06}, {"id": 460, "seek": 272916, "start": 2744.6, "end": 2750.2, "text": " grab create lots other lots of other pieces of data from it so is this particular date", "tokens": [4444, 1884, 3195, 661, 3195, 295, 661, 3755, 295, 1412, 490, 309, 370, 307, 341, 1729, 4002], "temperature": 0.0, "avg_logprob": -0.10423479728328372, "compression_ratio": 1.8632478632478633, "no_speech_prob": 1.1365572163413162e-06}, {"id": 461, "seek": 272916, "start": 2750.2, "end": 2757.02, "text": " the end of a month or not at the end of a year or not and so forth but that handles", "tokens": [264, 917, 295, 257, 1618, 420, 406, 412, 264, 917, 295, 257, 1064, 420, 406, 293, 370, 5220, 457, 300, 18722], "temperature": 0.0, "avg_logprob": -0.10423479728328372, "compression_ratio": 1.8632478632478633, "no_speech_prob": 1.1365572163413162e-06}, {"id": 462, "seek": 275702, "start": 2757.02, "end": 2763.7599999999998, "text": " dates there's a bit more cleaning we want to do and fastai provides some things to make", "tokens": [11691, 456, 311, 257, 857, 544, 8924, 321, 528, 281, 360, 293, 2370, 1301, 6417, 512, 721, 281, 652], "temperature": 0.0, "avg_logprob": -0.12210931334384652, "compression_ratio": 1.797029702970297, "no_speech_prob": 8.446198194178578e-07}, {"id": 463, "seek": 275702, "start": 2763.7599999999998, "end": 2773.52, "text": " meaning easier we can use the tabular pandas class to create a tabular data set pandas", "tokens": [3620, 3571, 321, 393, 764, 264, 4421, 1040, 4565, 296, 1508, 281, 1884, 257, 4421, 1040, 1412, 992, 4565, 296], "temperature": 0.0, "avg_logprob": -0.12210931334384652, "compression_ratio": 1.797029702970297, "no_speech_prob": 8.446198194178578e-07}, {"id": 464, "seek": 275702, "start": 2773.52, "end": 2780.0, "text": " and specifically we're going to use two tabular processes or tabular procs a tabular processor", "tokens": [293, 4682, 321, 434, 516, 281, 764, 732, 4421, 1040, 7555, 420, 4421, 1040, 9510, 82, 257, 4421, 1040, 15321], "temperature": 0.0, "avg_logprob": -0.12210931334384652, "compression_ratio": 1.797029702970297, "no_speech_prob": 8.446198194178578e-07}, {"id": 465, "seek": 275702, "start": 2780.0, "end": 2784.6, "text": " is basically just a transform and we've seen transforms before so go back and remind yourself", "tokens": [307, 1936, 445, 257, 4088, 293, 321, 600, 1612, 35592, 949, 370, 352, 646, 293, 4160, 1803], "temperature": 0.0, "avg_logprob": -0.12210931334384652, "compression_ratio": 1.797029702970297, "no_speech_prob": 8.446198194178578e-07}, {"id": 466, "seek": 278460, "start": 2784.6, "end": 2790.68, "text": " what a transform is except it's just slightly different it's like three lines of code if", "tokens": [437, 257, 4088, 307, 3993, 309, 311, 445, 4748, 819, 309, 311, 411, 1045, 3876, 295, 3089, 498], "temperature": 0.0, "avg_logprob": -0.048029184341430664, "compression_ratio": 1.7689243027888446, "no_speech_prob": 7.811448767824913e-07}, {"id": 467, "seek": 278460, "start": 2790.68, "end": 2796.3199999999997, "text": " you look at the code for it it's actually going to modify the object in place rather", "tokens": [291, 574, 412, 264, 3089, 337, 309, 309, 311, 767, 516, 281, 16927, 264, 2657, 294, 1081, 2831], "temperature": 0.0, "avg_logprob": -0.048029184341430664, "compression_ratio": 1.7689243027888446, "no_speech_prob": 7.811448767824913e-07}, {"id": 468, "seek": 278460, "start": 2796.3199999999997, "end": 2800.48, "text": " than creating a new object and giving it back to you and that's because often these tables", "tokens": [813, 4084, 257, 777, 2657, 293, 2902, 309, 646, 281, 291, 293, 300, 311, 570, 2049, 613, 8020], "temperature": 0.0, "avg_logprob": -0.048029184341430664, "compression_ratio": 1.7689243027888446, "no_speech_prob": 7.811448767824913e-07}, {"id": 469, "seek": 278460, "start": 2800.48, "end": 2806.36, "text": " of data are kind of really big and we don't want to waste lots of RAM and it's just going", "tokens": [295, 1412, 366, 733, 295, 534, 955, 293, 321, 500, 380, 528, 281, 5964, 3195, 295, 14561, 293, 309, 311, 445, 516], "temperature": 0.0, "avg_logprob": -0.048029184341430664, "compression_ratio": 1.7689243027888446, "no_speech_prob": 7.811448767824913e-07}, {"id": 470, "seek": 278460, "start": 2806.36, "end": 2811.12, "text": " to run the transform once and save the result rather than doing it lazily when you access", "tokens": [281, 1190, 264, 4088, 1564, 293, 3155, 264, 1874, 2831, 813, 884, 309, 19320, 953, 562, 291, 2105], "temperature": 0.0, "avg_logprob": -0.048029184341430664, "compression_ratio": 1.7689243027888446, "no_speech_prob": 7.811448767824913e-07}, {"id": 471, "seek": 281112, "start": 2811.12, "end": 2817.3199999999997, "text": " it for the same reason we just kind of make this a lot faster so you can just think of", "tokens": [309, 337, 264, 912, 1778, 321, 445, 733, 295, 652, 341, 257, 688, 4663, 370, 291, 393, 445, 519, 295], "temperature": 0.0, "avg_logprob": -0.13405192041971598, "compression_ratio": 1.7653061224489797, "no_speech_prob": 1.4593695141229546e-06}, {"id": 472, "seek": 281112, "start": 2817.3199999999997, "end": 2822.08, "text": " them as transforms really so one of them is called categorify and categorify is going", "tokens": [552, 382, 35592, 534, 370, 472, 295, 552, 307, 1219, 19250, 2505, 293, 19250, 2505, 307, 516], "temperature": 0.0, "avg_logprob": -0.13405192041971598, "compression_ratio": 1.7653061224489797, "no_speech_prob": 1.4593695141229546e-06}, {"id": 473, "seek": 281112, "start": 2822.08, "end": 2829.4, "text": " to replace a column with numeric categories using the same basic idea of like a vocab", "tokens": [281, 7406, 257, 7738, 365, 7866, 299, 10479, 1228, 264, 912, 3875, 1558, 295, 411, 257, 2329, 455], "temperature": 0.0, "avg_logprob": -0.13405192041971598, "compression_ratio": 1.7653061224489797, "no_speech_prob": 1.4593695141229546e-06}, {"id": 474, "seek": 281112, "start": 2829.4, "end": 2836.3199999999997, "text": " like we've seen before fill missing is going to find any columns with missing data it's", "tokens": [411, 321, 600, 1612, 949, 2836, 5361, 307, 516, 281, 915, 604, 13766, 365, 5361, 1412, 309, 311], "temperature": 0.0, "avg_logprob": -0.13405192041971598, "compression_ratio": 1.7653061224489797, "no_speech_prob": 1.4593695141229546e-06}, {"id": 475, "seek": 283632, "start": 2836.32, "end": 2841.1600000000003, "text": " going to fill in the missing data with the median of the data and create a new column", "tokens": [516, 281, 2836, 294, 264, 5361, 1412, 365, 264, 26779, 295, 264, 1412, 293, 1884, 257, 777, 7738], "temperature": 0.0, "avg_logprob": -0.06464397206025965, "compression_ratio": 1.78, "no_speech_prob": 3.632673895026528e-07}, {"id": 476, "seek": 283632, "start": 2841.1600000000003, "end": 2845.84, "text": " a Boolean column which is set to true for anything that was missing so these two things", "tokens": [257, 23351, 28499, 7738, 597, 307, 992, 281, 2074, 337, 1340, 300, 390, 5361, 370, 613, 732, 721], "temperature": 0.0, "avg_logprob": -0.06464397206025965, "compression_ratio": 1.78, "no_speech_prob": 3.632673895026528e-07}, {"id": 477, "seek": 283632, "start": 2845.84, "end": 2849.32, "text": " is basically enough to get you to a point where most of the time you'll be able to train", "tokens": [307, 1936, 1547, 281, 483, 291, 281, 257, 935, 689, 881, 295, 264, 565, 291, 603, 312, 1075, 281, 3847], "temperature": 0.0, "avg_logprob": -0.06464397206025965, "compression_ratio": 1.78, "no_speech_prob": 3.632673895026528e-07}, {"id": 478, "seek": 283632, "start": 2849.32, "end": 2857.4, "text": " a model now the next thing we need to do is think about our validation set as we discussed", "tokens": [257, 2316, 586, 264, 958, 551, 321, 643, 281, 360, 307, 519, 466, 527, 24071, 992, 382, 321, 7152], "temperature": 0.0, "avg_logprob": -0.06464397206025965, "compression_ratio": 1.78, "no_speech_prob": 3.632673895026528e-07}, {"id": 479, "seek": 283632, "start": 2857.4, "end": 2864.0800000000004, "text": " in lesson one a random validation set is not always appropriate and certainly for something", "tokens": [294, 6898, 472, 257, 4974, 24071, 992, 307, 406, 1009, 6854, 293, 3297, 337, 746], "temperature": 0.0, "avg_logprob": -0.06464397206025965, "compression_ratio": 1.78, "no_speech_prob": 3.632673895026528e-07}, {"id": 480, "seek": 286408, "start": 2864.08, "end": 2869.3199999999997, "text": " like predicting auction results it almost certainly is not appropriate because we're", "tokens": [411, 32884, 24139, 3542, 309, 1920, 3297, 307, 406, 6854, 570, 321, 434], "temperature": 0.0, "avg_logprob": -0.08969643115997314, "compression_ratio": 1.7386934673366834, "no_speech_prob": 4.812507086171536e-07}, {"id": 481, "seek": 286408, "start": 2869.3199999999997, "end": 2875.04, "text": " going to be wanting to use a model in the future not at some random date in the past", "tokens": [516, 281, 312, 7935, 281, 764, 257, 2316, 294, 264, 2027, 406, 412, 512, 4974, 4002, 294, 264, 1791], "temperature": 0.0, "avg_logprob": -0.08969643115997314, "compression_ratio": 1.7386934673366834, "no_speech_prob": 4.812507086171536e-07}, {"id": 482, "seek": 286408, "start": 2875.04, "end": 2880.7999999999997, "text": " so the way this Kaggle competition was set up was that the test set the thing that you", "tokens": [370, 264, 636, 341, 48751, 22631, 6211, 390, 992, 493, 390, 300, 264, 1500, 992, 264, 551, 300, 291], "temperature": 0.0, "avg_logprob": -0.08969643115997314, "compression_ratio": 1.7386934673366834, "no_speech_prob": 4.812507086171536e-07}, {"id": 483, "seek": 286408, "start": 2880.7999999999997, "end": 2888.92, "text": " had to fill in and submit for the competition was two weeks of data that was after any of", "tokens": [632, 281, 2836, 294, 293, 10315, 337, 264, 6211, 390, 732, 3259, 295, 1412, 300, 390, 934, 604, 295], "temperature": 0.0, "avg_logprob": -0.08969643115997314, "compression_ratio": 1.7386934673366834, "no_speech_prob": 4.812507086171536e-07}, {"id": 484, "seek": 288892, "start": 2888.92, "end": 2894.6, "text": " the training set so we should do the same thing for a validation set we should create", "tokens": [264, 3097, 992, 370, 321, 820, 360, 264, 912, 551, 337, 257, 24071, 992, 321, 820, 1884], "temperature": 0.0, "avg_logprob": -0.07719683647155762, "compression_ratio": 1.8918918918918919, "no_speech_prob": 6.083553785174445e-07}, {"id": 485, "seek": 288892, "start": 2894.6, "end": 2902.76, "text": " something which is where the validation set is the last couple of weeks of data and and", "tokens": [746, 597, 307, 689, 264, 24071, 992, 307, 264, 1036, 1916, 295, 3259, 295, 1412, 293, 293], "temperature": 0.0, "avg_logprob": -0.07719683647155762, "compression_ratio": 1.8918918918918919, "no_speech_prob": 6.083553785174445e-07}, {"id": 486, "seek": 288892, "start": 2902.76, "end": 2908.12, "text": " so then the training set will only be data before that so we basically can do that by", "tokens": [370, 550, 264, 3097, 992, 486, 787, 312, 1412, 949, 300, 370, 321, 1936, 393, 360, 300, 538], "temperature": 0.0, "avg_logprob": -0.07719683647155762, "compression_ratio": 1.8918918918918919, "no_speech_prob": 6.083553785174445e-07}, {"id": 487, "seek": 288892, "start": 2908.12, "end": 2913.96, "text": " grabbing everything before October 2011 create a training and validation set based on that", "tokens": [23771, 1203, 949, 7617, 10154, 1884, 257, 3097, 293, 24071, 992, 2361, 322, 300], "temperature": 0.0, "avg_logprob": -0.07719683647155762, "compression_ratio": 1.8918918918918919, "no_speech_prob": 6.083553785174445e-07}, {"id": 488, "seek": 291396, "start": 2913.96, "end": 2923.16, "text": " condition and grabbing those bits so that's going to split our training set and validation", "tokens": [4188, 293, 23771, 729, 9239, 370, 300, 311, 516, 281, 7472, 527, 3097, 992, 293, 24071], "temperature": 0.0, "avg_logprob": -0.1302098679816586, "compression_ratio": 1.7794117647058822, "no_speech_prob": 1.118939735533786e-06}, {"id": 489, "seek": 291396, "start": 2923.16, "end": 2930.2, "text": " set by date not randomly we're also going to need to tell when you create a tabular", "tokens": [992, 538, 4002, 406, 16979, 321, 434, 611, 516, 281, 643, 281, 980, 562, 291, 1884, 257, 4421, 1040], "temperature": 0.0, "avg_logprob": -0.1302098679816586, "compression_ratio": 1.7794117647058822, "no_speech_prob": 1.118939735533786e-06}, {"id": 490, "seek": 291396, "start": 2930.2, "end": 2935.0, "text": " pandas object you're going to be passing in a data frame going to be passing in your tabular", "tokens": [4565, 296, 2657, 291, 434, 516, 281, 312, 8437, 294, 257, 1412, 3920, 516, 281, 312, 8437, 294, 428, 4421, 1040], "temperature": 0.0, "avg_logprob": -0.1302098679816586, "compression_ratio": 1.7794117647058822, "no_speech_prob": 1.118939735533786e-06}, {"id": 491, "seek": 291396, "start": 2935.0, "end": 2941.68, "text": " procs you also have to say what are my categorical and continuous variables we can use fastai's", "tokens": [9510, 82, 291, 611, 362, 281, 584, 437, 366, 452, 19250, 804, 293, 10957, 9102, 321, 393, 764, 2370, 1301, 311], "temperature": 0.0, "avg_logprob": -0.1302098679816586, "compression_ratio": 1.7794117647058822, "no_speech_prob": 1.118939735533786e-06}, {"id": 492, "seek": 294168, "start": 2941.68, "end": 2948.72, "text": " con cat split we automatically split a data frame to continuous and categorical variables", "tokens": [416, 3857, 7472, 321, 6772, 7472, 257, 1412, 3920, 281, 10957, 293, 19250, 804, 9102], "temperature": 0.0, "avg_logprob": -0.12050908925581952, "compression_ratio": 1.9316239316239316, "no_speech_prob": 1.4823488072579494e-06}, {"id": 493, "seek": 294168, "start": 2948.72, "end": 2954.96, "text": " for you so we can just pass those in tell it what what is the dependent variable you", "tokens": [337, 291, 370, 321, 393, 445, 1320, 729, 294, 980, 309, 437, 437, 307, 264, 12334, 7006, 291], "temperature": 0.0, "avg_logprob": -0.12050908925581952, "compression_ratio": 1.9316239316239316, "no_speech_prob": 1.4823488072579494e-06}, {"id": 494, "seek": 294168, "start": 2954.96, "end": 2960.7999999999997, "text": " can have more than one and what are the indexes to split into training and valid and this", "tokens": [393, 362, 544, 813, 472, 293, 437, 366, 264, 8186, 279, 281, 7472, 666, 3097, 293, 7363, 293, 341], "temperature": 0.0, "avg_logprob": -0.12050908925581952, "compression_ratio": 1.9316239316239316, "no_speech_prob": 1.4823488072579494e-06}, {"id": 495, "seek": 294168, "start": 2960.7999999999997, "end": 2965.64, "text": " is a tabular object so it's got all the information you need about the training set the validation", "tokens": [307, 257, 4421, 1040, 2657, 370, 309, 311, 658, 439, 264, 1589, 291, 643, 466, 264, 3097, 992, 264, 24071], "temperature": 0.0, "avg_logprob": -0.12050908925581952, "compression_ratio": 1.9316239316239316, "no_speech_prob": 1.4823488072579494e-06}, {"id": 496, "seek": 294168, "start": 2965.64, "end": 2970.8399999999997, "text": " set categorical and continuous variables and the dependent variable and any processes to", "tokens": [992, 19250, 804, 293, 10957, 9102, 293, 264, 12334, 7006, 293, 604, 7555, 281], "temperature": 0.0, "avg_logprob": -0.12050908925581952, "compression_ratio": 1.9316239316239316, "no_speech_prob": 1.4823488072579494e-06}, {"id": 497, "seek": 297084, "start": 2970.84, "end": 2981.1600000000003, "text": " run it looks a lot like a data set subject but has a dot train it has a dot valid and", "tokens": [1190, 309, 1542, 257, 688, 411, 257, 1412, 992, 3983, 457, 575, 257, 5893, 3847, 309, 575, 257, 5893, 7363, 293], "temperature": 0.0, "avg_logprob": -0.08739829725689358, "compression_ratio": 1.7866666666666666, "no_speech_prob": 6.893600925650389e-07}, {"id": 498, "seek": 297084, "start": 2981.1600000000003, "end": 2990.0, "text": " so if we have a look at dot show we can see the the data but dot show is going to show", "tokens": [370, 498, 321, 362, 257, 574, 412, 5893, 855, 321, 393, 536, 264, 264, 1412, 457, 5893, 855, 307, 516, 281, 855], "temperature": 0.0, "avg_logprob": -0.08739829725689358, "compression_ratio": 1.7866666666666666, "no_speech_prob": 6.893600925650389e-07}, {"id": 499, "seek": 297084, "start": 2990.0, "end": 2996.84, "text": " us the kind of the string data but if we look at dot items you can see internally it's actually", "tokens": [505, 264, 733, 295, 264, 6798, 1412, 457, 498, 321, 574, 412, 5893, 4754, 291, 393, 536, 19501, 309, 311, 767], "temperature": 0.0, "avg_logprob": -0.08739829725689358, "compression_ratio": 1.7866666666666666, "no_speech_prob": 6.893600925650389e-07}, {"id": 500, "seek": 299684, "start": 2996.84, "end": 3006.1600000000003, "text": " stored these very compact numbers which we can use directly in a model so fastai has", "tokens": [12187, 613, 588, 14679, 3547, 597, 321, 393, 764, 3838, 294, 257, 2316, 370, 2370, 1301, 575], "temperature": 0.0, "avg_logprob": -0.10283724466959636, "compression_ratio": 1.6571428571428573, "no_speech_prob": 1.101593738894735e-06}, {"id": 501, "seek": 299684, "start": 3006.1600000000003, "end": 3011.08, "text": " basically got us to a point here where we have our data into a format I'm ready for", "tokens": [1936, 658, 505, 281, 257, 935, 510, 689, 321, 362, 527, 1412, 666, 257, 7877, 286, 478, 1919, 337], "temperature": 0.0, "avg_logprob": -0.10283724466959636, "compression_ratio": 1.6571428571428573, "no_speech_prob": 1.101593738894735e-06}, {"id": 502, "seek": 299684, "start": 3011.08, "end": 3019.6000000000004, "text": " modeling and validation sets being created to see how these numbers relate to these strings", "tokens": [15983, 293, 24071, 6352, 885, 2942, 281, 536, 577, 613, 3547, 10961, 281, 613, 13985], "temperature": 0.0, "avg_logprob": -0.10283724466959636, "compression_ratio": 1.6571428571428573, "no_speech_prob": 1.101593738894735e-06}, {"id": 503, "seek": 299684, "start": 3019.6000000000004, "end": 3025.2400000000002, "text": " we can again just like we saw last week use the classes attribute which is a dictionary", "tokens": [321, 393, 797, 445, 411, 321, 1866, 1036, 1243, 764, 264, 5359, 19667, 597, 307, 257, 25890], "temperature": 0.0, "avg_logprob": -0.10283724466959636, "compression_ratio": 1.6571428571428573, "no_speech_prob": 1.101593738894735e-06}, {"id": 504, "seek": 302524, "start": 3025.24, "end": 3030.4399999999996, "text": " which basically tells us the vocab so this is how we look up for example six is zero", "tokens": [597, 1936, 5112, 505, 264, 2329, 455, 370, 341, 307, 577, 321, 574, 493, 337, 1365, 2309, 307, 4018], "temperature": 0.0, "avg_logprob": -0.0842866613751366, "compression_ratio": 1.6829268292682926, "no_speech_prob": 3.6688493310066406e-06}, {"id": 505, "seek": 302524, "start": 3030.4399999999996, "end": 3038.08, "text": " one two three four five six this is compact example that processing took takes a little", "tokens": [472, 732, 1045, 1451, 1732, 2309, 341, 307, 14679, 1365, 300, 9007, 1890, 2516, 257, 707], "temperature": 0.0, "avg_logprob": -0.0842866613751366, "compression_ratio": 1.6829268292682926, "no_speech_prob": 3.6688493310066406e-06}, {"id": 506, "seek": 302524, "start": 3038.08, "end": 3045.04, "text": " while to run so you can go ahead and save the tabular object and so then you can load", "tokens": [1339, 281, 1190, 370, 291, 393, 352, 2286, 293, 3155, 264, 4421, 1040, 2657, 293, 370, 550, 291, 393, 3677], "temperature": 0.0, "avg_logprob": -0.0842866613751366, "compression_ratio": 1.6829268292682926, "no_speech_prob": 3.6688493310066406e-06}, {"id": 507, "seek": 302524, "start": 3045.04, "end": 3050.74, "text": " it back later without having to rerun all the processing so that's a nice kind of fast", "tokens": [309, 646, 1780, 1553, 1419, 281, 43819, 409, 439, 264, 9007, 370, 300, 311, 257, 1481, 733, 295, 2370], "temperature": 0.0, "avg_logprob": -0.0842866613751366, "compression_ratio": 1.6829268292682926, "no_speech_prob": 3.6688493310066406e-06}, {"id": 508, "seek": 305074, "start": 3050.74, "end": 3058.2, "text": " way to quickly get back up and running without having to reprocess your data so we've done", "tokens": [636, 281, 2661, 483, 646, 493, 293, 2614, 1553, 1419, 281, 35257, 780, 428, 1412, 370, 321, 600, 1096], "temperature": 0.0, "avg_logprob": -0.06133362304332644, "compression_ratio": 1.6842105263157894, "no_speech_prob": 5.714994699701492e-07}, {"id": 509, "seek": 305074, "start": 3058.2, "end": 3063.24, "text": " the basic data managing we need so we can now create a decision tree and in scikit-learn", "tokens": [264, 3875, 1412, 11642, 321, 643, 370, 321, 393, 586, 1884, 257, 3537, 4230, 293, 294, 2180, 22681, 12, 306, 1083], "temperature": 0.0, "avg_logprob": -0.06133362304332644, "compression_ratio": 1.6842105263157894, "no_speech_prob": 5.714994699701492e-07}, {"id": 510, "seek": 305074, "start": 3063.24, "end": 3069.9599999999996, "text": " a decision tree where the dependent variable is continuous is a decision tree regressor", "tokens": [257, 3537, 4230, 689, 264, 12334, 7006, 307, 10957, 307, 257, 3537, 4230, 1121, 735, 284], "temperature": 0.0, "avg_logprob": -0.06133362304332644, "compression_ratio": 1.6842105263157894, "no_speech_prob": 5.714994699701492e-07}, {"id": 511, "seek": 305074, "start": 3069.9599999999996, "end": 3075.16, "text": " and let's start by telling it we just want a total of four leaf nodes we'll see what", "tokens": [293, 718, 311, 722, 538, 3585, 309, 321, 445, 528, 257, 3217, 295, 1451, 10871, 13891, 321, 603, 536, 437], "temperature": 0.0, "avg_logprob": -0.06133362304332644, "compression_ratio": 1.6842105263157894, "no_speech_prob": 5.714994699701492e-07}, {"id": 512, "seek": 307516, "start": 3075.16, "end": 3081.16, "text": " that means in a moment and in scikit-learn you generally call fit so it looks quite a", "tokens": [300, 1355, 294, 257, 1623, 293, 294, 2180, 22681, 12, 306, 1083, 291, 5101, 818, 3318, 370, 309, 1542, 1596, 257], "temperature": 0.0, "avg_logprob": -0.14529489629408893, "compression_ratio": 1.6418604651162791, "no_speech_prob": 2.74208503014961e-07}, {"id": 513, "seek": 307516, "start": 3081.16, "end": 3087.12, "text": " lot like fast AI and you pass in your independent variables and your dependent variable and", "tokens": [688, 411, 2370, 7318, 293, 291, 1320, 294, 428, 6695, 9102, 293, 428, 12334, 7006, 293], "temperature": 0.0, "avg_logprob": -0.14529489629408893, "compression_ratio": 1.6418604651162791, "no_speech_prob": 2.74208503014961e-07}, {"id": 514, "seek": 307516, "start": 3087.12, "end": 3093.7599999999998, "text": " we can grab those straight from our tabular object training set is dot X's and dot Y and", "tokens": [321, 393, 4444, 729, 2997, 490, 527, 4421, 1040, 2657, 3097, 992, 307, 5893, 1783, 311, 293, 5893, 398, 293], "temperature": 0.0, "avg_logprob": -0.14529489629408893, "compression_ratio": 1.6418604651162791, "no_speech_prob": 2.74208503014961e-07}, {"id": 515, "seek": 307516, "start": 3093.7599999999998, "end": 3099.7999999999997, "text": " we can do the same thing for validation just to save us in typing okay question do you", "tokens": [321, 393, 360, 264, 912, 551, 337, 24071, 445, 281, 3155, 505, 294, 18444, 1392, 1168, 360, 291], "temperature": 0.0, "avg_logprob": -0.14529489629408893, "compression_ratio": 1.6418604651162791, "no_speech_prob": 2.74208503014961e-07}, {"id": 516, "seek": 309980, "start": 3099.8, "end": 3110.0800000000004, "text": " have any thoughts on what data augmentation for tabular data might look like I don't have", "tokens": [362, 604, 4598, 322, 437, 1412, 14501, 19631, 337, 4421, 1040, 1412, 1062, 574, 411, 286, 500, 380, 362], "temperature": 0.0, "avg_logprob": -0.1200405634366549, "compression_ratio": 1.6878980891719746, "no_speech_prob": 3.138115062029101e-06}, {"id": 517, "seek": 309980, "start": 3110.0800000000004, "end": 3119.5600000000004, "text": " a great sense of data augmentation for tabular data we'll be seeing later either in this", "tokens": [257, 869, 2020, 295, 1412, 14501, 19631, 337, 4421, 1040, 1412, 321, 603, 312, 2577, 1780, 2139, 294, 341], "temperature": 0.0, "avg_logprob": -0.1200405634366549, "compression_ratio": 1.6878980891719746, "no_speech_prob": 3.138115062029101e-06}, {"id": 518, "seek": 309980, "start": 3119.5600000000004, "end": 3125.84, "text": " course or in the next part drop out and mix up and stuff like that which they might be", "tokens": [1164, 420, 294, 264, 958, 644, 3270, 484, 293, 2890, 493, 293, 1507, 411, 300, 597, 436, 1062, 312], "temperature": 0.0, "avg_logprob": -0.1200405634366549, "compression_ratio": 1.6878980891719746, "no_speech_prob": 3.138115062029101e-06}, {"id": 519, "seek": 312584, "start": 3125.84, "end": 3133.32, "text": " able to do that in later layers in a tabular model otherwise I think you'd need to think", "tokens": [1075, 281, 360, 300, 294, 1780, 7914, 294, 257, 4421, 1040, 2316, 5911, 286, 519, 291, 1116, 643, 281, 519], "temperature": 0.0, "avg_logprob": -0.12380383889886397, "compression_ratio": 1.6839622641509433, "no_speech_prob": 2.156801429009647e-06}, {"id": 520, "seek": 312584, "start": 3133.32, "end": 3138.1200000000003, "text": " about kind of the semantics of the data and think about what are things you could do to", "tokens": [466, 733, 295, 264, 4361, 45298, 295, 264, 1412, 293, 519, 466, 437, 366, 721, 291, 727, 360, 281], "temperature": 0.0, "avg_logprob": -0.12380383889886397, "compression_ratio": 1.6839622641509433, "no_speech_prob": 2.156801429009647e-06}, {"id": 521, "seek": 312584, "start": 3138.1200000000003, "end": 3145.6400000000003, "text": " change the data without changing the meaning sounds like a pretty tricky question this", "tokens": [1319, 264, 1412, 1553, 4473, 264, 3620, 3263, 411, 257, 1238, 12414, 1168, 341], "temperature": 0.0, "avg_logprob": -0.12380383889886397, "compression_ratio": 1.6839622641509433, "no_speech_prob": 2.156801429009647e-06}, {"id": 522, "seek": 312584, "start": 3145.6400000000003, "end": 3153.36, "text": " fast AI distinguish between ordered categories such as low medium high and unordered category", "tokens": [2370, 7318, 20206, 1296, 8866, 10479, 1270, 382, 2295, 6399, 1090, 293, 517, 765, 4073, 7719], "temperature": 0.0, "avg_logprob": -0.12380383889886397, "compression_ratio": 1.6839622641509433, "no_speech_prob": 2.156801429009647e-06}, {"id": 523, "seek": 315336, "start": 3153.36, "end": 3158.28, "text": " variables yes that that was that ordinal thing I told you about before and all it really", "tokens": [9102, 2086, 300, 300, 390, 300, 4792, 2071, 551, 286, 1907, 291, 466, 949, 293, 439, 309, 534], "temperature": 0.0, "avg_logprob": -0.07672740773456853, "compression_ratio": 1.6872037914691944, "no_speech_prob": 1.2679219025812927e-06}, {"id": 524, "seek": 315336, "start": 3158.28, "end": 3165.86, "text": " does is it ensures that your classes list has a specific order so then these numbers", "tokens": [775, 307, 309, 28111, 300, 428, 5359, 1329, 575, 257, 2685, 1668, 370, 550, 613, 3547], "temperature": 0.0, "avg_logprob": -0.07672740773456853, "compression_ratio": 1.6872037914691944, "no_speech_prob": 1.2679219025812927e-06}, {"id": 525, "seek": 315336, "start": 3165.86, "end": 3169.56, "text": " actually have a specific and as you'll see that's actually going to turn out to be pretty", "tokens": [767, 362, 257, 2685, 293, 382, 291, 603, 536, 300, 311, 767, 516, 281, 1261, 484, 281, 312, 1238], "temperature": 0.0, "avg_logprob": -0.07672740773456853, "compression_ratio": 1.6872037914691944, "no_speech_prob": 1.2679219025812927e-06}, {"id": 526, "seek": 315336, "start": 3169.56, "end": 3177.54, "text": " important for how we train our random forest okay so we can create a decision tree regressor", "tokens": [1021, 337, 577, 321, 3847, 527, 4974, 6719, 1392, 370, 321, 393, 1884, 257, 3537, 4230, 1121, 735, 284], "temperature": 0.0, "avg_logprob": -0.07672740773456853, "compression_ratio": 1.6872037914691944, "no_speech_prob": 1.2679219025812927e-06}, {"id": 527, "seek": 317754, "start": 3177.54, "end": 3186.7599999999998, "text": " we can fit it and then we can draw it the fast AI function and here is the decision", "tokens": [321, 393, 3318, 309, 293, 550, 321, 393, 2642, 309, 264, 2370, 7318, 2445, 293, 510, 307, 264, 3537], "temperature": 0.0, "avg_logprob": -0.07441872755686442, "compression_ratio": 1.641025641025641, "no_speech_prob": 2.918933432738413e-07}, {"id": 528, "seek": 317754, "start": 3186.7599999999998, "end": 3195.92, "text": " tree we just trained and we and behind the scenes this actually used the basically the", "tokens": [4230, 321, 445, 8895, 293, 321, 293, 2261, 264, 8026, 341, 767, 1143, 264, 1936, 264], "temperature": 0.0, "avg_logprob": -0.07441872755686442, "compression_ratio": 1.641025641025641, "no_speech_prob": 2.918933432738413e-07}, {"id": 529, "seek": 317754, "start": 3195.92, "end": 3202.38, "text": " exact process that we described back here right so this is where you can like try and", "tokens": [1900, 1399, 300, 321, 7619, 646, 510, 558, 370, 341, 307, 689, 291, 393, 411, 853, 293], "temperature": 0.0, "avg_logprob": -0.07441872755686442, "compression_ratio": 1.641025641025641, "no_speech_prob": 2.918933432738413e-07}, {"id": 530, "seek": 320238, "start": 3202.38, "end": 3209.96, "text": " create your own decision tree implementation if you're interested in stretching yourself", "tokens": [1884, 428, 1065, 3537, 4230, 11420, 498, 291, 434, 3102, 294, 19632, 1803], "temperature": 0.0, "avg_logprob": -0.07789048442134151, "compression_ratio": 1.6869158878504673, "no_speech_prob": 9.47640046433662e-08}, {"id": 531, "seek": 320238, "start": 3209.96, "end": 3213.6400000000003, "text": " so we're going to use one that's already exists and the best way to understand what it's done", "tokens": [370, 321, 434, 516, 281, 764, 472, 300, 311, 1217, 8198, 293, 264, 1151, 636, 281, 1223, 437, 309, 311, 1096], "temperature": 0.0, "avg_logprob": -0.07789048442134151, "compression_ratio": 1.6869158878504673, "no_speech_prob": 9.47640046433662e-08}, {"id": 532, "seek": 320238, "start": 3213.6400000000003, "end": 3220.8, "text": " is to look at this diagram from top to bottom so the first step is it says like okay the", "tokens": [307, 281, 574, 412, 341, 10686, 490, 1192, 281, 2767, 370, 264, 700, 1823, 307, 309, 1619, 411, 1392, 264], "temperature": 0.0, "avg_logprob": -0.07789048442134151, "compression_ratio": 1.6869158878504673, "no_speech_prob": 9.47640046433662e-08}, {"id": 533, "seek": 320238, "start": 3220.8, "end": 3227.9, "text": " initial model it created is a model with no binary splits at all specifically it's always", "tokens": [5883, 2316, 309, 2942, 307, 257, 2316, 365, 572, 17434, 37741, 412, 439, 4682, 309, 311, 1009], "temperature": 0.0, "avg_logprob": -0.07789048442134151, "compression_ratio": 1.6869158878504673, "no_speech_prob": 9.47640046433662e-08}, {"id": 534, "seek": 322790, "start": 3227.9, "end": 3234.96, "text": " going to predict the value 10.1 for every single row why is that well because this is", "tokens": [516, 281, 6069, 264, 2158, 1266, 13, 16, 337, 633, 2167, 5386, 983, 307, 300, 731, 570, 341, 307], "temperature": 0.0, "avg_logprob": -0.08491809242650082, "compression_ratio": 1.783132530120482, "no_speech_prob": 3.5763551409218053e-07}, {"id": 535, "seek": 322790, "start": 3234.96, "end": 3240.2000000000003, "text": " the simplest possible model is to take the average of the dependent variable and always", "tokens": [264, 22811, 1944, 2316, 307, 281, 747, 264, 4274, 295, 264, 12334, 7006, 293, 1009], "temperature": 0.0, "avg_logprob": -0.08491809242650082, "compression_ratio": 1.783132530120482, "no_speech_prob": 3.5763551409218053e-07}, {"id": 536, "seek": 322790, "start": 3240.2000000000003, "end": 3245.92, "text": " predict that and so this is always should be your kind of pretty much your basic baseline", "tokens": [6069, 300, 293, 370, 341, 307, 1009, 820, 312, 428, 733, 295, 1238, 709, 428, 3875, 20518], "temperature": 0.0, "avg_logprob": -0.08491809242650082, "compression_ratio": 1.783132530120482, "no_speech_prob": 3.5763551409218053e-07}, {"id": 537, "seek": 322790, "start": 3245.92, "end": 3252.58, "text": " for regression there are four hundred and four thousand seven hundred and ten rows auctions", "tokens": [337, 24590, 456, 366, 1451, 3262, 293, 1451, 4714, 3407, 3262, 293, 2064, 13241, 1609, 3916], "temperature": 0.0, "avg_logprob": -0.08491809242650082, "compression_ratio": 1.783132530120482, "no_speech_prob": 3.5763551409218053e-07}, {"id": 538, "seek": 322790, "start": 3252.58, "end": 3257.4, "text": " that we're averaging and the mean squared error of this incredibly simple model in which", "tokens": [300, 321, 434, 47308, 293, 264, 914, 8889, 6713, 295, 341, 6252, 2199, 2316, 294, 597], "temperature": 0.0, "avg_logprob": -0.08491809242650082, "compression_ratio": 1.783132530120482, "no_speech_prob": 3.5763551409218053e-07}, {"id": 539, "seek": 325740, "start": 3257.4, "end": 3265.9, "text": " there are no rules at all no groups at all just a single average is a point for it so", "tokens": [456, 366, 572, 4474, 412, 439, 572, 3935, 412, 439, 445, 257, 2167, 4274, 307, 257, 935, 337, 309, 370], "temperature": 0.0, "avg_logprob": -0.1392060991317507, "compression_ratio": 1.6948051948051948, "no_speech_prob": 7.690365464441129e-07}, {"id": 540, "seek": 325740, "start": 3265.9, "end": 3273.32, "text": " then the next most complex model is to take a single column coupler system and a single", "tokens": [550, 264, 958, 881, 3997, 2316, 307, 281, 747, 257, 2167, 7738, 1384, 22732, 1185, 293, 257, 2167], "temperature": 0.0, "avg_logprob": -0.1392060991317507, "compression_ratio": 1.6948051948051948, "no_speech_prob": 7.690365464441129e-07}, {"id": 541, "seek": 325740, "start": 3273.32, "end": 3280.5, "text": " binary decision is coupler system less than or equal to point five true there are three", "tokens": [17434, 3537, 307, 1384, 22732, 1185, 1570, 813, 420, 2681, 281, 935, 1732, 2074, 456, 366, 1045], "temperature": 0.0, "avg_logprob": -0.1392060991317507, "compression_ratio": 1.6948051948051948, "no_speech_prob": 7.690365464441129e-07}, {"id": 542, "seek": 328050, "start": 3280.5, "end": 3288.4, "text": " hundred and sixty thousand eight hundred and forty seven auctions where it's true and forty-three", "tokens": [3262, 293, 21390, 4714, 3180, 3262, 293, 15815, 3407, 1609, 3916, 689, 309, 311, 2074, 293, 15815, 12, 27583], "temperature": 0.0, "avg_logprob": -0.1243987175134512, "compression_ratio": 1.9350649350649352, "no_speech_prob": 1.267926222681126e-06}, {"id": 543, "seek": 328050, "start": 3288.4, "end": 3290.96, "text": " thousand eight hundred and sixty-three where it's false and now interestingly in the false", "tokens": [4714, 3180, 3262, 293, 21390, 12, 27583, 689, 309, 311, 7908, 293, 586, 25873, 294, 264, 7908], "temperature": 0.0, "avg_logprob": -0.1243987175134512, "compression_ratio": 1.9350649350649352, "no_speech_prob": 1.267926222681126e-06}, {"id": 544, "seek": 328050, "start": 3290.96, "end": 3297.36, "text": " case you can see that there are no further binary decisions so this is called a leaf", "tokens": [1389, 291, 393, 536, 300, 456, 366, 572, 3052, 17434, 5327, 370, 341, 307, 1219, 257, 10871], "temperature": 0.0, "avg_logprob": -0.1243987175134512, "compression_ratio": 1.9350649350649352, "no_speech_prob": 1.267926222681126e-06}, {"id": 545, "seek": 328050, "start": 3297.36, "end": 3304.52, "text": " node it's a node where this is as far as you can get and so if your coupler system is not", "tokens": [9984, 309, 311, 257, 9984, 689, 341, 307, 382, 1400, 382, 291, 393, 483, 293, 370, 498, 428, 1384, 22732, 1185, 307, 406], "temperature": 0.0, "avg_logprob": -0.1243987175134512, "compression_ratio": 1.9350649350649352, "no_speech_prob": 1.267926222681126e-06}, {"id": 546, "seek": 328050, "start": 3304.52, "end": 3309.82, "text": " less than or equal to point five then the prediction this model makes for your sale", "tokens": [1570, 813, 420, 2681, 281, 935, 1732, 550, 264, 17630, 341, 2316, 1669, 337, 428, 8680], "temperature": 0.0, "avg_logprob": -0.1243987175134512, "compression_ratio": 1.9350649350649352, "no_speech_prob": 1.267926222681126e-06}, {"id": 547, "seek": 330982, "start": 3309.82, "end": 3316.54, "text": " price is nine point two one versus if it's true it's ten point two one so you can see", "tokens": [3218, 307, 4949, 935, 732, 472, 5717, 498, 309, 311, 2074, 309, 311, 2064, 935, 732, 472, 370, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.11422456479540058, "compression_ratio": 1.8109243697478992, "no_speech_prob": 1.5056966731208377e-06}, {"id": 548, "seek": 330982, "start": 3316.54, "end": 3319.7000000000003, "text": " it's actually found a very big difference here and that's why it picked this as the", "tokens": [309, 311, 767, 1352, 257, 588, 955, 2649, 510, 293, 300, 311, 983, 309, 6183, 341, 382, 264], "temperature": 0.0, "avg_logprob": -0.11422456479540058, "compression_ratio": 1.8109243697478992, "no_speech_prob": 1.5056966731208377e-06}, {"id": 549, "seek": 330982, "start": 3319.7000000000003, "end": 3325.6400000000003, "text": " first binary split and so the mean squared error for this section here is point one two", "tokens": [700, 17434, 7472, 293, 370, 264, 914, 8889, 6713, 337, 341, 3541, 510, 307, 935, 472, 732], "temperature": 0.0, "avg_logprob": -0.11422456479540058, "compression_ratio": 1.8109243697478992, "no_speech_prob": 1.5056966731208377e-06}, {"id": 550, "seek": 330982, "start": 3325.6400000000003, "end": 3331.02, "text": " which is far better than we started out at point four eight this group still has three", "tokens": [597, 307, 1400, 1101, 813, 321, 1409, 484, 412, 935, 1451, 3180, 341, 1594, 920, 575, 1045], "temperature": 0.0, "avg_logprob": -0.11422456479540058, "compression_ratio": 1.8109243697478992, "no_speech_prob": 1.5056966731208377e-06}, {"id": 551, "seek": 330982, "start": 3331.02, "end": 3336.6600000000003, "text": " hundred sixty thousand in it and so it does another binary split this time is the year", "tokens": [3262, 21390, 4714, 294, 309, 293, 370, 309, 775, 1071, 17434, 7472, 341, 565, 307, 264, 1064], "temperature": 0.0, "avg_logprob": -0.11422456479540058, "compression_ratio": 1.8109243697478992, "no_speech_prob": 1.5056966731208377e-06}, {"id": 552, "seek": 333666, "start": 3336.66, "end": 3341.96, "text": " that this piece of equipment made was it less than or equal to nineteen ninety one and a", "tokens": [300, 341, 2522, 295, 5927, 1027, 390, 309, 1570, 813, 420, 2681, 281, 31555, 25063, 472, 293, 257], "temperature": 0.0, "avg_logprob": -0.12865656497431735, "compression_ratio": 1.9086956521739131, "no_speech_prob": 1.8162178321290412e-06}, {"id": 553, "seek": 333666, "start": 3341.96, "end": 3348.56, "text": " half if it was if it's true then we get a leaf node and the prediction is nine point", "tokens": [1922, 498, 309, 390, 498, 309, 311, 2074, 550, 321, 483, 257, 10871, 9984, 293, 264, 17630, 307, 4949, 935], "temperature": 0.0, "avg_logprob": -0.12865656497431735, "compression_ratio": 1.9086956521739131, "no_speech_prob": 1.8162178321290412e-06}, {"id": 554, "seek": 333666, "start": 3348.56, "end": 3353.44, "text": " nine seven mean squared error point three seven if the value is false we don't have", "tokens": [4949, 3407, 914, 8889, 6713, 935, 1045, 3407, 498, 264, 2158, 307, 7908, 321, 500, 380, 362], "temperature": 0.0, "avg_logprob": -0.12865656497431735, "compression_ratio": 1.9086956521739131, "no_speech_prob": 1.8162178321290412e-06}, {"id": 555, "seek": 333666, "start": 3353.44, "end": 3357.7799999999997, "text": " a leaf node and we have another binary split and you can see eventually we get down to", "tokens": [257, 10871, 9984, 293, 321, 362, 1071, 17434, 7472, 293, 291, 393, 536, 4728, 321, 483, 760, 281], "temperature": 0.0, "avg_logprob": -0.12865656497431735, "compression_ratio": 1.9086956521739131, "no_speech_prob": 1.8162178321290412e-06}, {"id": 556, "seek": 333666, "start": 3357.7799999999997, "end": 3365.7599999999998, "text": " here coupler system true year made false product size false mean squared error point one seven", "tokens": [510, 1384, 22732, 1185, 2074, 1064, 1027, 7908, 1674, 2744, 7908, 914, 8889, 6713, 935, 472, 3407], "temperature": 0.0, "avg_logprob": -0.12865656497431735, "compression_ratio": 1.9086956521739131, "no_speech_prob": 1.8162178321290412e-06}, {"id": 557, "seek": 336576, "start": 3365.76, "end": 3373.82, "text": " so all of these leaf nodes have MSC's that are smaller than that original baseline model", "tokens": [370, 439, 295, 613, 10871, 13891, 362, 7395, 34, 311, 300, 366, 4356, 813, 300, 3380, 20518, 2316], "temperature": 0.0, "avg_logprob": -0.10480751310076032, "compression_ratio": 1.6, "no_speech_prob": 4.247027334258746e-07}, {"id": 558, "seek": 336576, "start": 3373.82, "end": 3379.6600000000003, "text": " of just taking the mean so this is how you can grow a decision tree and we only stopped", "tokens": [295, 445, 1940, 264, 914, 370, 341, 307, 577, 291, 393, 1852, 257, 3537, 4230, 293, 321, 787, 5936], "temperature": 0.0, "avg_logprob": -0.10480751310076032, "compression_ratio": 1.6, "no_speech_prob": 4.247027334258746e-07}, {"id": 559, "seek": 336576, "start": 3379.6600000000003, "end": 3386.6000000000004, "text": " here because we said max leaf nodes is four one two three four right and so if we want", "tokens": [510, 570, 321, 848, 11469, 10871, 13891, 307, 1451, 472, 732, 1045, 1451, 558, 293, 370, 498, 321, 528], "temperature": 0.0, "avg_logprob": -0.10480751310076032, "compression_ratio": 1.6, "no_speech_prob": 4.247027334258746e-07}, {"id": 560, "seek": 336576, "start": 3386.6000000000004, "end": 3393.9, "text": " to keep training it further we can just use a higher number there's actually a very nice", "tokens": [281, 1066, 3097, 309, 3052, 321, 393, 445, 764, 257, 2946, 1230, 456, 311, 767, 257, 588, 1481], "temperature": 0.0, "avg_logprob": -0.10480751310076032, "compression_ratio": 1.6, "no_speech_prob": 4.247027334258746e-07}, {"id": 561, "seek": 339390, "start": 3393.9, "end": 3402.26, "text": " library by Terrence Park or D tree viz which can show us exactly the same information like", "tokens": [6405, 538, 6564, 10760, 4964, 420, 413, 4230, 371, 590, 597, 393, 855, 505, 2293, 264, 912, 1589, 411], "temperature": 0.0, "avg_logprob": -0.13944946635853162, "compression_ratio": 1.7352941176470589, "no_speech_prob": 5.043467012910696e-07}, {"id": 562, "seek": 339390, "start": 3402.26, "end": 3409.46, "text": " so and so here are the same leaf nodes one two three four and you can see the kind of", "tokens": [370, 293, 370, 510, 366, 264, 912, 10871, 13891, 472, 732, 1045, 1451, 293, 291, 393, 536, 264, 733, 295], "temperature": 0.0, "avg_logprob": -0.13944946635853162, "compression_ratio": 1.7352941176470589, "no_speech_prob": 5.043467012910696e-07}, {"id": 563, "seek": 339390, "start": 3409.46, "end": 3414.7400000000002, "text": " the chart of how many are there this is the split coupler system point five here are the", "tokens": [264, 6927, 295, 577, 867, 366, 456, 341, 307, 264, 7472, 1384, 22732, 1185, 935, 1732, 510, 366, 264], "temperature": 0.0, "avg_logprob": -0.13944946635853162, "compression_ratio": 1.7352941176470589, "no_speech_prob": 5.043467012910696e-07}, {"id": 564, "seek": 339390, "start": 3414.7400000000002, "end": 3419.7400000000002, "text": " two groups you can see the sale price in each of the two groups and then here's the leaf", "tokens": [732, 3935, 291, 393, 536, 264, 8680, 3218, 294, 1184, 295, 264, 732, 3935, 293, 550, 510, 311, 264, 10871], "temperature": 0.0, "avg_logprob": -0.13944946635853162, "compression_ratio": 1.7352941176470589, "no_speech_prob": 5.043467012910696e-07}, {"id": 565, "seek": 341974, "start": 3419.74, "end": 3425.1, "text": " node and so then the second split was on year made and you can see here something weird's", "tokens": [9984, 293, 370, 550, 264, 1150, 7472, 390, 322, 1064, 1027, 293, 291, 393, 536, 510, 746, 3657, 311], "temperature": 0.0, "avg_logprob": -0.09981257960481464, "compression_ratio": 1.7729083665338645, "no_speech_prob": 1.4823544915998355e-06}, {"id": 566, "seek": 341974, "start": 3425.1, "end": 3429.2999999999997, "text": " going on with year made there's a whole bunch of year made that are a thousand which is", "tokens": [516, 322, 365, 1064, 1027, 456, 311, 257, 1379, 3840, 295, 1064, 1027, 300, 366, 257, 4714, 597, 307], "temperature": 0.0, "avg_logprob": -0.09981257960481464, "compression_ratio": 1.7729083665338645, "no_speech_prob": 1.4823544915998355e-06}, {"id": 567, "seek": 341974, "start": 3429.2999999999997, "end": 3434.7799999999997, "text": " obviously not a sensible year for a bulldozer to be made so presumably that's some kind", "tokens": [2745, 406, 257, 25380, 1064, 337, 257, 4693, 2595, 4527, 281, 312, 1027, 370, 26742, 300, 311, 512, 733], "temperature": 0.0, "avg_logprob": -0.09981257960481464, "compression_ratio": 1.7729083665338645, "no_speech_prob": 1.4823544915998355e-06}, {"id": 568, "seek": 341974, "start": 3434.7799999999997, "end": 3440.9799999999996, "text": " of missing value so when we look at the kind of the picture like this it can give us some", "tokens": [295, 5361, 2158, 370, 562, 321, 574, 412, 264, 733, 295, 264, 3036, 411, 341, 309, 393, 976, 505, 512], "temperature": 0.0, "avg_logprob": -0.09981257960481464, "compression_ratio": 1.7729083665338645, "no_speech_prob": 1.4823544915998355e-06}, {"id": 569, "seek": 341974, "start": 3440.9799999999996, "end": 3448.4599999999996, "text": " insights about what's going on in our data and so maybe we should replace those thousands", "tokens": [14310, 466, 437, 311, 516, 322, 294, 527, 1412, 293, 370, 1310, 321, 820, 7406, 729, 5383], "temperature": 0.0, "avg_logprob": -0.09981257960481464, "compression_ratio": 1.7729083665338645, "no_speech_prob": 1.4823544915998355e-06}, {"id": 570, "seek": 344846, "start": 3448.46, "end": 3454.78, "text": " with 1950 because that's you know obviously a very very early year for a bulldozer so", "tokens": [365, 18141, 570, 300, 311, 291, 458, 2745, 257, 588, 588, 2440, 1064, 337, 257, 4693, 2595, 4527, 370], "temperature": 0.0, "avg_logprob": -0.08589153289794922, "compression_ratio": 1.7509881422924902, "no_speech_prob": 7.453757575603959e-07}, {"id": 571, "seek": 344846, "start": 3454.78, "end": 3459.62, "text": " we can kind of pick it arbitrarily it's actually not really going to make any difference to", "tokens": [321, 393, 733, 295, 1888, 309, 19071, 3289, 309, 311, 767, 406, 534, 516, 281, 652, 604, 2649, 281], "temperature": 0.0, "avg_logprob": -0.08589153289794922, "compression_ratio": 1.7509881422924902, "no_speech_prob": 7.453757575603959e-07}, {"id": 572, "seek": 344846, "start": 3459.62, "end": 3464.78, "text": " the model that's created because all we care about is the order because we're just doing", "tokens": [264, 2316, 300, 311, 2942, 570, 439, 321, 1127, 466, 307, 264, 1668, 570, 321, 434, 445, 884], "temperature": 0.0, "avg_logprob": -0.08589153289794922, "compression_ratio": 1.7509881422924902, "no_speech_prob": 7.453757575603959e-07}, {"id": 573, "seek": 344846, "start": 3464.78, "end": 3470.5, "text": " these binary splits that will make it easier to look at as you can see here's our 1950s", "tokens": [613, 17434, 37741, 300, 486, 652, 309, 3571, 281, 574, 412, 382, 291, 393, 536, 510, 311, 527, 18141, 82], "temperature": 0.0, "avg_logprob": -0.08589153289794922, "compression_ratio": 1.7509881422924902, "no_speech_prob": 7.453757575603959e-07}, {"id": 574, "seek": 344846, "start": 3470.5, "end": 3478.44, "text": " now and so now it's much easier to see what's going on in that binary split so let's now", "tokens": [586, 293, 370, 586, 309, 311, 709, 3571, 281, 536, 437, 311, 516, 322, 294, 300, 17434, 7472, 370, 718, 311, 586], "temperature": 0.0, "avg_logprob": -0.08589153289794922, "compression_ratio": 1.7509881422924902, "no_speech_prob": 7.453757575603959e-07}, {"id": 575, "seek": 347844, "start": 3478.44, "end": 3485.1, "text": " get rid of max leaf nodes and build a bigger decision tree and then let's just for the", "tokens": [483, 3973, 295, 11469, 10871, 13891, 293, 1322, 257, 3801, 3537, 4230, 293, 550, 718, 311, 445, 337, 264], "temperature": 0.0, "avg_logprob": -0.12573868036270142, "compression_ratio": 2.017857142857143, "no_speech_prob": 1.0845142242033035e-06}, {"id": 576, "seek": 347844, "start": 3485.1, "end": 3490.26, "text": " rest of this notebook create a couple of little functions one to create the root mean squared", "tokens": [1472, 295, 341, 21060, 1884, 257, 1916, 295, 707, 6828, 472, 281, 1884, 264, 5593, 914, 8889], "temperature": 0.0, "avg_logprob": -0.12573868036270142, "compression_ratio": 2.017857142857143, "no_speech_prob": 1.0845142242033035e-06}, {"id": 577, "seek": 347844, "start": 3490.26, "end": 3496.94, "text": " error which is just here and another one to take a model and some independent independent", "tokens": [6713, 597, 307, 445, 510, 293, 1071, 472, 281, 747, 257, 2316, 293, 512, 6695, 6695], "temperature": 0.0, "avg_logprob": -0.12573868036270142, "compression_ratio": 2.017857142857143, "no_speech_prob": 1.0845142242033035e-06}, {"id": 578, "seek": 347844, "start": 3496.94, "end": 3503.2200000000003, "text": " variables predict from the model on the independent variables and then take the root mean squared", "tokens": [9102, 6069, 490, 264, 2316, 322, 264, 6695, 9102, 293, 550, 747, 264, 5593, 914, 8889], "temperature": 0.0, "avg_logprob": -0.12573868036270142, "compression_ratio": 2.017857142857143, "no_speech_prob": 1.0845142242033035e-06}, {"id": 579, "seek": 347844, "start": 3503.2200000000003, "end": 3507.9, "text": " error with a dependent variable so that's going to be our models root means squared", "tokens": [6713, 365, 257, 12334, 7006, 370, 300, 311, 516, 281, 312, 527, 5245, 5593, 1355, 8889], "temperature": 0.0, "avg_logprob": -0.12573868036270142, "compression_ratio": 2.017857142857143, "no_speech_prob": 1.0845142242033035e-06}, {"id": 580, "seek": 350790, "start": 3507.9, "end": 3513.6600000000003, "text": " error so for this decision tree in which we didn't have a stopping criteria so as many", "tokens": [6713, 370, 337, 341, 3537, 4230, 294, 597, 321, 994, 380, 362, 257, 12767, 11101, 370, 382, 867], "temperature": 0.0, "avg_logprob": -0.12923620968330196, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.8162201058657956e-06}, {"id": 581, "seek": 350790, "start": 3513.6600000000003, "end": 3521.1800000000003, "text": " leaf nodes as you like the models root mean squared error is zero so we've just built", "tokens": [10871, 13891, 382, 291, 411, 264, 5245, 5593, 914, 8889, 6713, 307, 4018, 370, 321, 600, 445, 3094], "temperature": 0.0, "avg_logprob": -0.12923620968330196, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.8162201058657956e-06}, {"id": 582, "seek": 350790, "start": 3521.1800000000003, "end": 3529.54, "text": " the perfect model so this is great news right we've built the perfect auction trading system", "tokens": [264, 2176, 2316, 370, 341, 307, 869, 2583, 558, 321, 600, 3094, 264, 2176, 24139, 9529, 1185], "temperature": 0.0, "avg_logprob": -0.12923620968330196, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.8162201058657956e-06}, {"id": 583, "seek": 350790, "start": 3529.54, "end": 3534.02, "text": " well remember we actually need to check the validation set so let's check the check mrmse", "tokens": [731, 1604, 321, 767, 643, 281, 1520, 264, 24071, 992, 370, 718, 311, 1520, 264, 1520, 33660, 76, 405], "temperature": 0.0, "avg_logprob": -0.12923620968330196, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.8162201058657956e-06}, {"id": 584, "seek": 353402, "start": 3534.02, "end": 3542.06, "text": " with the validation set and oh it's worse than zero so our training set is zero our", "tokens": [365, 264, 24071, 992, 293, 1954, 309, 311, 5324, 813, 4018, 370, 527, 3097, 992, 307, 4018, 527], "temperature": 0.0, "avg_logprob": -0.13090372674259138, "compression_ratio": 1.8177083333333333, "no_speech_prob": 7.002158213254006e-07}, {"id": 585, "seek": 353402, "start": 3542.06, "end": 3548.3, "text": " validation set is much worse than zero why has that happened well one of the things that", "tokens": [24071, 992, 307, 709, 5324, 813, 4018, 983, 575, 300, 2011, 731, 472, 295, 264, 721, 300], "temperature": 0.0, "avg_logprob": -0.13090372674259138, "compression_ratio": 1.8177083333333333, "no_speech_prob": 7.002158213254006e-07}, {"id": 586, "seek": 353402, "start": 3548.3, "end": 3553.34, "text": " a random forest in sklearn can do is it can tell you the number of leaf nodes number of", "tokens": [257, 4974, 6719, 294, 1110, 306, 1083, 393, 360, 307, 309, 393, 980, 291, 264, 1230, 295, 10871, 13891, 1230, 295], "temperature": 0.0, "avg_logprob": -0.13090372674259138, "compression_ratio": 1.8177083333333333, "no_speech_prob": 7.002158213254006e-07}, {"id": 587, "seek": 353402, "start": 3553.34, "end": 3559.46, "text": " leaves there are three hundred and forty one thousand number of data points four hundred", "tokens": [5510, 456, 366, 1045, 3262, 293, 15815, 472, 4714, 1230, 295, 1412, 2793, 1451, 3262], "temperature": 0.0, "avg_logprob": -0.13090372674259138, "compression_ratio": 1.8177083333333333, "no_speech_prob": 7.002158213254006e-07}, {"id": 588, "seek": 355946, "start": 3559.46, "end": 3564.3, "text": " and fifty thousand so in other words we have nearly as many leaf nodes as data points the", "tokens": [293, 13442, 4714, 370, 294, 661, 2283, 321, 362, 6217, 382, 867, 10871, 13891, 382, 1412, 2793, 264], "temperature": 0.0, "avg_logprob": -0.12629657421471938, "compression_ratio": 1.8375, "no_speech_prob": 7.571133551209641e-07}, {"id": 589, "seek": 355946, "start": 3564.3, "end": 3567.66, "text": " most of our leaf nodes only have a single thing in so they're taking an average of a", "tokens": [881, 295, 527, 10871, 13891, 787, 362, 257, 2167, 551, 294, 370, 436, 434, 1940, 364, 4274, 295, 257], "temperature": 0.0, "avg_logprob": -0.12629657421471938, "compression_ratio": 1.8375, "no_speech_prob": 7.571133551209641e-07}, {"id": 590, "seek": 355946, "start": 3567.66, "end": 3574.14, "text": " single thing clearly this makes no sense at all so what we should actually do is pick", "tokens": [2167, 551, 4448, 341, 1669, 572, 2020, 412, 439, 370, 437, 321, 820, 767, 360, 307, 1888], "temperature": 0.0, "avg_logprob": -0.12629657421471938, "compression_ratio": 1.8375, "no_speech_prob": 7.571133551209641e-07}, {"id": 591, "seek": 355946, "start": 3574.14, "end": 3580.76, "text": " some different stopping criteria and let's say okay if you get a leaf node with 25 things", "tokens": [512, 819, 12767, 11101, 293, 718, 311, 584, 1392, 498, 291, 483, 257, 10871, 9984, 365, 3552, 721], "temperature": 0.0, "avg_logprob": -0.12629657421471938, "compression_ratio": 1.8375, "no_speech_prob": 7.571133551209641e-07}, {"id": 592, "seek": 355946, "start": 3580.76, "end": 3586.06, "text": " or less in it don't don't split or don't split things to create a leaf node with less than", "tokens": [420, 1570, 294, 309, 500, 380, 500, 380, 7472, 420, 500, 380, 7472, 721, 281, 1884, 257, 10871, 9984, 365, 1570, 813], "temperature": 0.0, "avg_logprob": -0.12629657421471938, "compression_ratio": 1.8375, "no_speech_prob": 7.571133551209641e-07}, {"id": 593, "seek": 358606, "start": 3586.06, "end": 3592.22, "text": " 25 things in it and now if we fit and we look at the root mean squared error for the validation", "tokens": [3552, 721, 294, 309, 293, 586, 498, 321, 3318, 293, 321, 574, 412, 264, 5593, 914, 8889, 6713, 337, 264, 24071], "temperature": 0.0, "avg_logprob": -0.09344300456430721, "compression_ratio": 1.7115384615384615, "no_speech_prob": 6.083583912186441e-07}, {"id": 594, "seek": 358606, "start": 3592.22, "end": 3599.34, "text": " set it's going to go down from point three three to point three two so the training sets", "tokens": [992, 309, 311, 516, 281, 352, 760, 490, 935, 1045, 1045, 281, 935, 1045, 732, 370, 264, 3097, 6352], "temperature": 0.0, "avg_logprob": -0.09344300456430721, "compression_ratio": 1.7115384615384615, "no_speech_prob": 6.083583912186441e-07}, {"id": 595, "seek": 358606, "start": 3599.34, "end": 3604.34, "text": " got worse from zero to point two four eight the validation sets got better and now we", "tokens": [658, 5324, 490, 4018, 281, 935, 732, 1451, 3180, 264, 24071, 6352, 658, 1101, 293, 586, 321], "temperature": 0.0, "avg_logprob": -0.09344300456430721, "compression_ratio": 1.7115384615384615, "no_speech_prob": 6.083583912186441e-07}, {"id": 596, "seek": 358606, "start": 3604.34, "end": 3612.7799999999997, "text": " only have 12,000 leaf nodes so that is much more reasonable all right so let's take a", "tokens": [787, 362, 2272, 11, 1360, 10871, 13891, 370, 300, 307, 709, 544, 10585, 439, 558, 370, 718, 311, 747, 257], "temperature": 0.0, "avg_logprob": -0.09344300456430721, "compression_ratio": 1.7115384615384615, "no_speech_prob": 6.083583912186441e-07}, {"id": 597, "seek": 361278, "start": 3612.78, "end": 3616.52, "text": " five minute break and then we're going to come back and see how we get the best of both", "tokens": [1732, 3456, 1821, 293, 550, 321, 434, 516, 281, 808, 646, 293, 536, 577, 321, 483, 264, 1151, 295, 1293], "temperature": 0.0, "avg_logprob": -0.1042210667632347, "compression_ratio": 1.7738693467336684, "no_speech_prob": 9.570793508828501e-07}, {"id": 598, "seek": 361278, "start": 3616.52, "end": 3623.5400000000004, "text": " worlds how are we going to get something which has the kind of flexibility to get these you", "tokens": [13401, 577, 366, 321, 516, 281, 483, 746, 597, 575, 264, 733, 295, 12635, 281, 483, 613, 291], "temperature": 0.0, "avg_logprob": -0.1042210667632347, "compression_ratio": 1.7738693467336684, "no_speech_prob": 9.570793508828501e-07}, {"id": 599, "seek": 361278, "start": 3623.5400000000004, "end": 3629.5, "text": " know what we're going to get down to zero but to get you know really deep trees but", "tokens": [458, 437, 321, 434, 516, 281, 483, 760, 281, 4018, 457, 281, 483, 291, 458, 534, 2452, 5852, 457], "temperature": 0.0, "avg_logprob": -0.1042210667632347, "compression_ratio": 1.7738693467336684, "no_speech_prob": 9.570793508828501e-07}, {"id": 600, "seek": 361278, "start": 3629.5, "end": 3634.94, "text": " also without overfitting and the trick will be to use something called bagging we'll come", "tokens": [611, 1553, 670, 69, 2414, 293, 264, 4282, 486, 312, 281, 764, 746, 1219, 3411, 3249, 321, 603, 808], "temperature": 0.0, "avg_logprob": -0.1042210667632347, "compression_ratio": 1.7738693467336684, "no_speech_prob": 9.570793508828501e-07}, {"id": 601, "seek": 363494, "start": 3634.94, "end": 3648.1, "text": " back and talk about that in five minutes okay welcome back so we're going to look at how", "tokens": [646, 293, 751, 466, 300, 294, 1732, 2077, 1392, 2928, 646, 370, 321, 434, 516, 281, 574, 412, 577], "temperature": 0.0, "avg_logprob": -0.06582378596067429, "compression_ratio": 1.56, "no_speech_prob": 6.681508466499508e-07}, {"id": 602, "seek": 363494, "start": 3648.1, "end": 3655.58, "text": " we can get the best of both worlds as we discussed and let's start by having a look at what we're", "tokens": [321, 393, 483, 264, 1151, 295, 1293, 13401, 382, 321, 7152, 293, 718, 311, 722, 538, 1419, 257, 574, 412, 437, 321, 434], "temperature": 0.0, "avg_logprob": -0.06582378596067429, "compression_ratio": 1.56, "no_speech_prob": 6.681508466499508e-07}, {"id": 603, "seek": 363494, "start": 3655.58, "end": 3662.34, "text": " doing with categorical variables festival and so you might notice that previously with", "tokens": [884, 365, 19250, 804, 9102, 12091, 293, 370, 291, 1062, 3449, 300, 8046, 365], "temperature": 0.0, "avg_logprob": -0.06582378596067429, "compression_ratio": 1.56, "no_speech_prob": 6.681508466499508e-07}, {"id": 604, "seek": 366234, "start": 3662.34, "end": 3669.6600000000003, "text": " categorical variables for example in collaborative filtering we had to you know kind of think", "tokens": [19250, 804, 9102, 337, 1365, 294, 16555, 30822, 321, 632, 281, 291, 458, 733, 295, 519], "temperature": 0.0, "avg_logprob": -0.11731559038162231, "compression_ratio": 1.7102803738317758, "no_speech_prob": 1.8162121477871551e-06}, {"id": 605, "seek": 366234, "start": 3669.6600000000003, "end": 3675.1400000000003, "text": " about like how many embedding levels we have for example if you've used other modeling", "tokens": [466, 411, 577, 867, 12240, 3584, 4358, 321, 362, 337, 1365, 498, 291, 600, 1143, 661, 15983], "temperature": 0.0, "avg_logprob": -0.11731559038162231, "compression_ratio": 1.7102803738317758, "no_speech_prob": 1.8162121477871551e-06}, {"id": 606, "seek": 366234, "start": 3675.1400000000003, "end": 3681.6400000000003, "text": " tools you might have doing things with creating dummy variables stuff like that random forests", "tokens": [3873, 291, 1062, 362, 884, 721, 365, 4084, 35064, 9102, 1507, 411, 300, 4974, 21700], "temperature": 0.0, "avg_logprob": -0.11731559038162231, "compression_ratio": 1.7102803738317758, "no_speech_prob": 1.8162121477871551e-06}, {"id": 607, "seek": 366234, "start": 3681.6400000000003, "end": 3690.6200000000003, "text": " on the whole you don't have to the the reason is that as we've seen all of our categorical", "tokens": [322, 264, 1379, 291, 500, 380, 362, 281, 264, 264, 1778, 307, 300, 382, 321, 600, 1612, 439, 295, 527, 19250, 804], "temperature": 0.0, "avg_logprob": -0.11731559038162231, "compression_ratio": 1.7102803738317758, "no_speech_prob": 1.8162121477871551e-06}, {"id": 608, "seek": 369062, "start": 3690.62, "end": 3699.22, "text": " variables have been turned into numbers and so we can perfectly well have decision tree", "tokens": [9102, 362, 668, 3574, 666, 3547, 293, 370, 321, 393, 6239, 731, 362, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.0829747234072004, "compression_ratio": 1.6390532544378698, "no_speech_prob": 1.5381711193640513e-07}, {"id": 609, "seek": 369062, "start": 3699.22, "end": 3708.4, "text": " binary decisions which use those particular numbers now the numbers might not be ordered", "tokens": [17434, 5327, 597, 764, 729, 1729, 3547, 586, 264, 3547, 1062, 406, 312, 8866], "temperature": 0.0, "avg_logprob": -0.0829747234072004, "compression_ratio": 1.6390532544378698, "no_speech_prob": 1.5381711193640513e-07}, {"id": 610, "seek": 369062, "start": 3708.4, "end": 3716.2599999999998, "text": " in any interesting way but if there's a particular level which kind of stands out as being important", "tokens": [294, 604, 1880, 636, 457, 498, 456, 311, 257, 1729, 1496, 597, 733, 295, 7382, 484, 382, 885, 1021], "temperature": 0.0, "avg_logprob": -0.0829747234072004, "compression_ratio": 1.6390532544378698, "no_speech_prob": 1.5381711193640513e-07}, {"id": 611, "seek": 371626, "start": 3716.26, "end": 3724.78, "text": " it only takes two binary splits to split out that level into a single you know into a single", "tokens": [309, 787, 2516, 732, 17434, 37741, 281, 7472, 484, 300, 1496, 666, 257, 2167, 291, 458, 666, 257, 2167], "temperature": 0.0, "avg_logprob": -0.08077444349016462, "compression_ratio": 1.652694610778443, "no_speech_prob": 2.7852664175043174e-07}, {"id": 612, "seek": 371626, "start": 3724.78, "end": 3734.1800000000003, "text": " piece so generally speaking I don't normally worry too much about kind of encoding categorical", "tokens": [2522, 370, 5101, 4124, 286, 500, 380, 5646, 3292, 886, 709, 466, 733, 295, 43430, 19250, 804], "temperature": 0.0, "avg_logprob": -0.08077444349016462, "compression_ratio": 1.652694610778443, "no_speech_prob": 2.7852664175043174e-07}, {"id": 613, "seek": 371626, "start": 3734.1800000000003, "end": 3740.46, "text": " variables in a special way as I mentioned I do try to encode ordinal variables by saying", "tokens": [9102, 294, 257, 2121, 636, 382, 286, 2835, 286, 360, 853, 281, 2058, 1429, 4792, 2071, 9102, 538, 1566], "temperature": 0.0, "avg_logprob": -0.08077444349016462, "compression_ratio": 1.652694610778443, "no_speech_prob": 2.7852664175043174e-07}, {"id": 614, "seek": 374046, "start": 3740.46, "end": 3747.26, "text": " what the order of the levels is because often as you would expect sizes for example you", "tokens": [437, 264, 1668, 295, 264, 4358, 307, 570, 2049, 382, 291, 576, 2066, 11602, 337, 1365, 291], "temperature": 0.0, "avg_logprob": -0.08579249101526597, "compression_ratio": 1.6985645933014355, "no_speech_prob": 3.5209046700401814e-07}, {"id": 615, "seek": 374046, "start": 3747.26, "end": 3751.62, "text": " know medium and small are going to be kind of next to each other and large and extra", "tokens": [458, 6399, 293, 1359, 366, 516, 281, 312, 733, 295, 958, 281, 1184, 661, 293, 2416, 293, 2857], "temperature": 0.0, "avg_logprob": -0.08579249101526597, "compression_ratio": 1.6985645933014355, "no_speech_prob": 3.5209046700401814e-07}, {"id": 616, "seek": 374046, "start": 3751.62, "end": 3757.82, "text": " large would be next to each other that's good to have those as similar numbers having said", "tokens": [2416, 576, 312, 958, 281, 1184, 661, 300, 311, 665, 281, 362, 729, 382, 2531, 3547, 1419, 848], "temperature": 0.0, "avg_logprob": -0.08579249101526597, "compression_ratio": 1.6985645933014355, "no_speech_prob": 3.5209046700401814e-07}, {"id": 617, "seek": 374046, "start": 3757.82, "end": 3765.9, "text": " that you can kind of one-hot encode a categorical variable if you want to using get dummies", "tokens": [300, 291, 393, 733, 295, 472, 12, 12194, 2058, 1429, 257, 19250, 804, 7006, 498, 291, 528, 281, 1228, 483, 16784, 38374], "temperature": 0.0, "avg_logprob": -0.08579249101526597, "compression_ratio": 1.6985645933014355, "no_speech_prob": 3.5209046700401814e-07}, {"id": 618, "seek": 376590, "start": 3765.9, "end": 3771.78, "text": " in pandas but there's not a lot of evidence that that actually helps there's actually", "tokens": [294, 4565, 296, 457, 456, 311, 406, 257, 688, 295, 4467, 300, 300, 767, 3665, 456, 311, 767], "temperature": 0.0, "avg_logprob": -0.15121279444013322, "compression_ratio": 1.6761904761904762, "no_speech_prob": 3.520896143527352e-07}, {"id": 619, "seek": 376590, "start": 3771.78, "end": 3778.42, "text": " that has been brought in a paper and so I would say in general for categorical variables", "tokens": [300, 575, 668, 3038, 294, 257, 3035, 293, 370, 286, 576, 584, 294, 2674, 337, 19250, 804, 9102], "temperature": 0.0, "avg_logprob": -0.15121279444013322, "compression_ratio": 1.6761904761904762, "no_speech_prob": 3.520896143527352e-07}, {"id": 620, "seek": 376590, "start": 3778.42, "end": 3785.46, "text": " don't worry about it too much just use what we've shown you have a question for for ordinal", "tokens": [500, 380, 3292, 466, 309, 886, 709, 445, 764, 437, 321, 600, 4898, 291, 362, 257, 1168, 337, 337, 4792, 2071], "temperature": 0.0, "avg_logprob": -0.15121279444013322, "compression_ratio": 1.6761904761904762, "no_speech_prob": 3.520896143527352e-07}, {"id": 621, "seek": 376590, "start": 3785.46, "end": 3793.14, "text": " categorical variables how do you deal with when they have like an a or missing values", "tokens": [19250, 804, 9102, 577, 360, 291, 2028, 365, 562, 436, 362, 411, 364, 257, 420, 5361, 4190], "temperature": 0.0, "avg_logprob": -0.15121279444013322, "compression_ratio": 1.6761904761904762, "no_speech_prob": 3.520896143527352e-07}, {"id": 622, "seek": 379314, "start": 3793.14, "end": 3802.7, "text": " where do you put that in the order so in fast AI and a missing values always appear as the", "tokens": [689, 360, 291, 829, 300, 294, 264, 1668, 370, 294, 2370, 7318, 293, 257, 5361, 4190, 1009, 4204, 382, 264], "temperature": 0.0, "avg_logprob": -0.11679555057139879, "compression_ratio": 1.7028301886792452, "no_speech_prob": 4.81250140182965e-07}, {"id": 623, "seek": 379314, "start": 3802.7, "end": 3809.1, "text": " first item they'll always be the zero in item and also if you get something in the validation", "tokens": [700, 3174, 436, 603, 1009, 312, 264, 4018, 294, 3174, 293, 611, 498, 291, 483, 746, 294, 264, 24071], "temperature": 0.0, "avg_logprob": -0.11679555057139879, "compression_ratio": 1.7028301886792452, "no_speech_prob": 4.81250140182965e-07}, {"id": 624, "seek": 379314, "start": 3809.1, "end": 3813.18, "text": " or test set which is a level we haven't seen in training that will be considered to be", "tokens": [420, 1500, 992, 597, 307, 257, 1496, 321, 2378, 380, 1612, 294, 3097, 300, 486, 312, 4888, 281, 312], "temperature": 0.0, "avg_logprob": -0.11679555057139879, "compression_ratio": 1.7028301886792452, "no_speech_prob": 4.81250140182965e-07}, {"id": 625, "seek": 379314, "start": 3813.18, "end": 3822.3799999999997, "text": " that missing or in a value as well all right so what we're going to do to try and improve", "tokens": [300, 5361, 420, 294, 257, 2158, 382, 731, 439, 558, 370, 437, 321, 434, 516, 281, 360, 281, 853, 293, 3470], "temperature": 0.0, "avg_logprob": -0.11679555057139879, "compression_ratio": 1.7028301886792452, "no_speech_prob": 4.81250140182965e-07}, {"id": 626, "seek": 382238, "start": 3822.38, "end": 3828.6600000000003, "text": " our random forest is we're going to use something called bagging and this was developed by a", "tokens": [527, 4974, 6719, 307, 321, 434, 516, 281, 764, 746, 1219, 3411, 3249, 293, 341, 390, 4743, 538, 257], "temperature": 0.0, "avg_logprob": -0.08363624719473031, "compression_ratio": 1.6318181818181818, "no_speech_prob": 3.2377222396462457e-06}, {"id": 627, "seek": 382238, "start": 3828.6600000000003, "end": 3835.1800000000003, "text": " retired Berkeley professor named Leo Breiman in 1994 and he did a lot of great work and", "tokens": [16776, 23684, 8304, 4926, 19344, 7090, 25504, 294, 22736, 293, 415, 630, 257, 688, 295, 869, 589, 293], "temperature": 0.0, "avg_logprob": -0.08363624719473031, "compression_ratio": 1.6318181818181818, "no_speech_prob": 3.2377222396462457e-06}, {"id": 628, "seek": 382238, "start": 3835.1800000000003, "end": 3840.82, "text": " perhaps you could actually that most of it happened after he retired his technical report", "tokens": [4317, 291, 727, 767, 300, 881, 295, 309, 2011, 934, 415, 16776, 702, 6191, 2275], "temperature": 0.0, "avg_logprob": -0.08363624719473031, "compression_ratio": 1.6318181818181818, "no_speech_prob": 3.2377222396462457e-06}, {"id": 629, "seek": 382238, "start": 3840.82, "end": 3845.9, "text": " was called bagging predictors and he described how you could create multiple versions of", "tokens": [390, 1219, 3411, 3249, 6069, 830, 293, 415, 7619, 577, 291, 727, 1884, 3866, 9606, 295], "temperature": 0.0, "avg_logprob": -0.08363624719473031, "compression_ratio": 1.6318181818181818, "no_speech_prob": 3.2377222396462457e-06}, {"id": 630, "seek": 384590, "start": 3845.9, "end": 3852.82, "text": " a predictor so multiple different models and you could then aggregate them by averaging", "tokens": [257, 6069, 284, 370, 3866, 819, 5245, 293, 291, 727, 550, 26118, 552, 538, 47308], "temperature": 0.0, "avg_logprob": -0.08316754039965178, "compression_ratio": 1.7474747474747474, "no_speech_prob": 1.3496953670255607e-06}, {"id": 631, "seek": 384590, "start": 3852.82, "end": 3860.58, "text": " over the predictions and specifically the way he suggested doing this was to create", "tokens": [670, 264, 21264, 293, 4682, 264, 636, 415, 10945, 884, 341, 390, 281, 1884], "temperature": 0.0, "avg_logprob": -0.08316754039965178, "compression_ratio": 1.7474747474747474, "no_speech_prob": 1.3496953670255607e-06}, {"id": 632, "seek": 384590, "start": 3860.58, "end": 3865.94, "text": " what he called bootstrap replicates in other words randomly select different subsets of", "tokens": [437, 415, 1219, 11450, 372, 4007, 3248, 299, 1024, 294, 661, 2283, 16979, 3048, 819, 2090, 1385, 295], "temperature": 0.0, "avg_logprob": -0.08316754039965178, "compression_ratio": 1.7474747474747474, "no_speech_prob": 1.3496953670255607e-06}, {"id": 633, "seek": 384590, "start": 3865.94, "end": 3871.86, "text": " your data train a model on that subset and store it away as one of your predictors and", "tokens": [428, 1412, 3847, 257, 2316, 322, 300, 25993, 293, 3531, 309, 1314, 382, 472, 295, 428, 6069, 830, 293], "temperature": 0.0, "avg_logprob": -0.08316754039965178, "compression_ratio": 1.7474747474747474, "no_speech_prob": 1.3496953670255607e-06}, {"id": 634, "seek": 387186, "start": 3871.86, "end": 3876.5, "text": " then do it again a bunch of times and so each of these models is trained on a different", "tokens": [550, 360, 309, 797, 257, 3840, 295, 1413, 293, 370, 1184, 295, 613, 5245, 307, 8895, 322, 257, 819], "temperature": 0.0, "avg_logprob": -0.109161927149846, "compression_ratio": 1.780612244897959, "no_speech_prob": 3.8669625723741774e-07}, {"id": 635, "seek": 387186, "start": 3876.5, "end": 3883.38, "text": " random subset of your data and then you to predict you predict on all of those different", "tokens": [4974, 25993, 295, 428, 1412, 293, 550, 291, 281, 6069, 291, 6069, 322, 439, 295, 729, 819], "temperature": 0.0, "avg_logprob": -0.109161927149846, "compression_ratio": 1.780612244897959, "no_speech_prob": 3.8669625723741774e-07}, {"id": 636, "seek": 387186, "start": 3883.38, "end": 3892.1800000000003, "text": " versions of your model and average them and it turns out that bagging works really well", "tokens": [9606, 295, 428, 2316, 293, 4274, 552, 293, 309, 4523, 484, 300, 3411, 3249, 1985, 534, 731], "temperature": 0.0, "avg_logprob": -0.109161927149846, "compression_ratio": 1.780612244897959, "no_speech_prob": 3.8669625723741774e-07}, {"id": 637, "seek": 387186, "start": 3892.1800000000003, "end": 3898.5, "text": " so this the sequence of steps is basically randomly choose some subset of rows write", "tokens": [370, 341, 264, 8310, 295, 4439, 307, 1936, 16979, 2826, 512, 25993, 295, 13241, 2464], "temperature": 0.0, "avg_logprob": -0.109161927149846, "compression_ratio": 1.780612244897959, "no_speech_prob": 3.8669625723741774e-07}, {"id": 638, "seek": 389850, "start": 3898.5, "end": 3904.18, "text": " in a model using that subset save that model and then return to step one do that a few", "tokens": [294, 257, 2316, 1228, 300, 25993, 3155, 300, 2316, 293, 550, 2736, 281, 1823, 472, 360, 300, 257, 1326], "temperature": 0.0, "avg_logprob": -0.0978893601750753, "compression_ratio": 1.8144329896907216, "no_speech_prob": 1.0845137694559526e-06}, {"id": 639, "seek": 389850, "start": 3904.18, "end": 3910.46, "text": " times to train a few models and then to make a prediction predict with all the models and", "tokens": [1413, 281, 3847, 257, 1326, 5245, 293, 550, 281, 652, 257, 17630, 6069, 365, 439, 264, 5245, 293], "temperature": 0.0, "avg_logprob": -0.0978893601750753, "compression_ratio": 1.8144329896907216, "no_speech_prob": 1.0845137694559526e-06}, {"id": 640, "seek": 389850, "start": 3910.46, "end": 3918.22, "text": " take the average that is bagging and it's very simple but it's astonishingly powerful", "tokens": [747, 264, 4274, 300, 307, 3411, 3249, 293, 309, 311, 588, 2199, 457, 309, 311, 35264, 356, 4005], "temperature": 0.0, "avg_logprob": -0.0978893601750753, "compression_ratio": 1.8144329896907216, "no_speech_prob": 1.0845137694559526e-06}, {"id": 641, "seek": 389850, "start": 3918.22, "end": 3925.5, "text": " and the reason why is that each of these models we've trained although they are not using", "tokens": [293, 264, 1778, 983, 307, 300, 1184, 295, 613, 5245, 321, 600, 8895, 4878, 436, 366, 406, 1228], "temperature": 0.0, "avg_logprob": -0.0978893601750753, "compression_ratio": 1.8144329896907216, "no_speech_prob": 1.0845137694559526e-06}, {"id": 642, "seek": 392550, "start": 3925.5, "end": 3932.02, "text": " all of the data so they're kind of less accurate than a model that use all of the data each", "tokens": [439, 295, 264, 1412, 370, 436, 434, 733, 295, 1570, 8559, 813, 257, 2316, 300, 764, 439, 295, 264, 1412, 1184], "temperature": 0.0, "avg_logprob": -0.09298721566257706, "compression_ratio": 2.022346368715084, "no_speech_prob": 1.760334157552279e-06}, {"id": 643, "seek": 392550, "start": 3932.02, "end": 3940.24, "text": " of them is the errors are not correlated you know the errors because of using that smaller", "tokens": [295, 552, 307, 264, 13603, 366, 406, 38574, 291, 458, 264, 13603, 570, 295, 1228, 300, 4356], "temperature": 0.0, "avg_logprob": -0.09298721566257706, "compression_ratio": 2.022346368715084, "no_speech_prob": 1.760334157552279e-06}, {"id": 644, "seek": 392550, "start": 3940.24, "end": 3944.66, "text": " subset are not correlated with the errors of the other models because they're random", "tokens": [25993, 366, 406, 38574, 365, 264, 13603, 295, 264, 661, 5245, 570, 436, 434, 4974], "temperature": 0.0, "avg_logprob": -0.09298721566257706, "compression_ratio": 2.022346368715084, "no_speech_prob": 1.760334157552279e-06}, {"id": 645, "seek": 392550, "start": 3944.66, "end": 3954.64, "text": " subsets and so when you take the average of a bunch of kind of errors which are not correlated", "tokens": [2090, 1385, 293, 370, 562, 291, 747, 264, 4274, 295, 257, 3840, 295, 733, 295, 13603, 597, 366, 406, 38574], "temperature": 0.0, "avg_logprob": -0.09298721566257706, "compression_ratio": 2.022346368715084, "no_speech_prob": 1.760334157552279e-06}, {"id": 646, "seek": 395464, "start": 3954.64, "end": 3963.3399999999997, "text": " with each other the average of those errors is zero so therefore the average of the models", "tokens": [365, 1184, 661, 264, 4274, 295, 729, 13603, 307, 4018, 370, 4412, 264, 4274, 295, 264, 5245], "temperature": 0.0, "avg_logprob": -0.10015994232970399, "compression_ratio": 1.6698564593301435, "no_speech_prob": 1.37094946239813e-06}, {"id": 647, "seek": 395464, "start": 3963.3399999999997, "end": 3968.74, "text": " should give us an accurate prediction of the thing we're actually trying to predict so", "tokens": [820, 976, 505, 364, 8559, 17630, 295, 264, 551, 321, 434, 767, 1382, 281, 6069, 370], "temperature": 0.0, "avg_logprob": -0.10015994232970399, "compression_ratio": 1.6698564593301435, "no_speech_prob": 1.37094946239813e-06}, {"id": 648, "seek": 395464, "start": 3968.74, "end": 3972.98, "text": " as I say here it's an amazing result we can improve the accuracy of nearly any kind of", "tokens": [382, 286, 584, 510, 309, 311, 364, 2243, 1874, 321, 393, 3470, 264, 14170, 295, 6217, 604, 733, 295], "temperature": 0.0, "avg_logprob": -0.10015994232970399, "compression_ratio": 1.6698564593301435, "no_speech_prob": 1.37094946239813e-06}, {"id": 649, "seek": 395464, "start": 3972.98, "end": 3978.7, "text": " algorithm by training it multiple times on different random subsets of data and then", "tokens": [9284, 538, 3097, 309, 3866, 1413, 322, 819, 4974, 2090, 1385, 295, 1412, 293, 550], "temperature": 0.0, "avg_logprob": -0.10015994232970399, "compression_ratio": 1.6698564593301435, "no_speech_prob": 1.37094946239813e-06}, {"id": 650, "seek": 397870, "start": 3978.7, "end": 3987.22, "text": " averaging the predictions so then Breiman in 2001 showed a way to do this specifically", "tokens": [47308, 264, 21264, 370, 550, 7090, 25504, 294, 16382, 4712, 257, 636, 281, 360, 341, 4682], "temperature": 0.0, "avg_logprob": -0.07205636877762644, "compression_ratio": 1.6605504587155964, "no_speech_prob": 1.0511442951610661e-06}, {"id": 651, "seek": 397870, "start": 3987.22, "end": 3993.74, "text": " for decision trees where not only did he randomly choose a subset of rows for each model but", "tokens": [337, 3537, 5852, 689, 406, 787, 630, 415, 16979, 2826, 257, 25993, 295, 13241, 337, 1184, 2316, 457], "temperature": 0.0, "avg_logprob": -0.07205636877762644, "compression_ratio": 1.6605504587155964, "no_speech_prob": 1.0511442951610661e-06}, {"id": 652, "seek": 397870, "start": 3993.74, "end": 4000.46, "text": " then for each binary split he also randomly selected a subset of columns and this is called", "tokens": [550, 337, 1184, 17434, 7472, 415, 611, 16979, 8209, 257, 25993, 295, 13766, 293, 341, 307, 1219], "temperature": 0.0, "avg_logprob": -0.07205636877762644, "compression_ratio": 1.6605504587155964, "no_speech_prob": 1.0511442951610661e-06}, {"id": 653, "seek": 397870, "start": 4000.46, "end": 4006.22, "text": " the random forest and it's perhaps the most widely used most practically important machine", "tokens": [264, 4974, 6719, 293, 309, 311, 4317, 264, 881, 13371, 1143, 881, 15667, 1021, 3479], "temperature": 0.0, "avg_logprob": -0.07205636877762644, "compression_ratio": 1.6605504587155964, "no_speech_prob": 1.0511442951610661e-06}, {"id": 654, "seek": 400622, "start": 4006.22, "end": 4015.4599999999996, "text": " learning method and astonishingly simple to create a random forest regressor you use sklearns", "tokens": [2539, 3170, 293, 35264, 356, 2199, 281, 1884, 257, 4974, 6719, 1121, 735, 284, 291, 764, 1110, 306, 1083, 82], "temperature": 0.0, "avg_logprob": -0.17618388288161335, "compression_ratio": 1.6176470588235294, "no_speech_prob": 9.42242479595734e-07}, {"id": 655, "seek": 400622, "start": 4015.4599999999996, "end": 4021.9399999999996, "text": " random forest regressor if you pass n jobs minus one it will use all of the CPU cause", "tokens": [4974, 6719, 1121, 735, 284, 498, 291, 1320, 297, 4782, 3175, 472, 309, 486, 764, 439, 295, 264, 13199, 3082], "temperature": 0.0, "avg_logprob": -0.17618388288161335, "compression_ratio": 1.6176470588235294, "no_speech_prob": 9.42242479595734e-07}, {"id": 656, "seek": 400622, "start": 4021.9399999999996, "end": 4030.62, "text": " you have to run as fast as possible and estimators says how many trees how many models to train", "tokens": [291, 362, 281, 1190, 382, 2370, 382, 1944, 293, 8017, 3391, 1619, 577, 867, 5852, 577, 867, 5245, 281, 3847], "temperature": 0.0, "avg_logprob": -0.17618388288161335, "compression_ratio": 1.6176470588235294, "no_speech_prob": 9.42242479595734e-07}, {"id": 657, "seek": 403062, "start": 4030.62, "end": 4037.54, "text": " maxsamples says how many rows to use randomly chosen rows to use in each one maxfeatures", "tokens": [11469, 19988, 2622, 1619, 577, 867, 13241, 281, 764, 16979, 8614, 13241, 281, 764, 294, 1184, 472, 11469, 2106, 3377], "temperature": 0.0, "avg_logprob": -0.11891195633832147, "compression_ratio": 1.7586206896551724, "no_speech_prob": 7.934480663607246e-07}, {"id": 658, "seek": 403062, "start": 4037.54, "end": 4044.9, "text": " is how many randomly chosen columns to use for each binary split point minsamplesleaf", "tokens": [307, 577, 867, 16979, 8614, 13766, 281, 764, 337, 1184, 17434, 7472, 935, 923, 19988, 2622, 306, 2792], "temperature": 0.0, "avg_logprob": -0.11891195633832147, "compression_ratio": 1.7586206896551724, "no_speech_prob": 7.934480663607246e-07}, {"id": 659, "seek": 403062, "start": 4044.9, "end": 4052.1, "text": " is the stopping criteria and we'll come back to so here's a little function that will create", "tokens": [307, 264, 12767, 11101, 293, 321, 603, 808, 646, 281, 370, 510, 311, 257, 707, 2445, 300, 486, 1884], "temperature": 0.0, "avg_logprob": -0.11891195633832147, "compression_ratio": 1.7586206896551724, "no_speech_prob": 7.934480663607246e-07}, {"id": 660, "seek": 403062, "start": 4052.1, "end": 4057.58, "text": " a random forest regressor and fit it to some set of independent variables and a dependent", "tokens": [257, 4974, 6719, 1121, 735, 284, 293, 3318, 309, 281, 512, 992, 295, 6695, 9102, 293, 257, 12334], "temperature": 0.0, "avg_logprob": -0.11891195633832147, "compression_ratio": 1.7586206896551724, "no_speech_prob": 7.934480663607246e-07}, {"id": 661, "seek": 405758, "start": 4057.58, "end": 4068.5, "text": " variable so we can give it a few default values and create a random forest and train and our", "tokens": [7006, 370, 321, 393, 976, 309, 257, 1326, 7576, 4190, 293, 1884, 257, 4974, 6719, 293, 3847, 293, 527], "temperature": 0.0, "avg_logprob": -0.14571529210999953, "compression_ratio": 1.4, "no_speech_prob": 5.203566502132162e-07}, {"id": 662, "seek": 405758, "start": 4068.5, "end": 4082.06, "text": " validation set rmsc is 0.23 if we compare that of what we had before", "tokens": [24071, 992, 367, 2592, 66, 307, 1958, 13, 9356, 498, 321, 6794, 300, 295, 437, 321, 632, 949], "temperature": 0.0, "avg_logprob": -0.14571529210999953, "compression_ratio": 1.4, "no_speech_prob": 5.203566502132162e-07}, {"id": 663, "seek": 408206, "start": 4082.06, "end": 4097.08, "text": " we had 0.32 so dramatically better by using a random forest so so what's happened when", "tokens": [321, 632, 1958, 13, 11440, 370, 17548, 1101, 538, 1228, 257, 4974, 6719, 370, 370, 437, 311, 2011, 562], "temperature": 0.0, "avg_logprob": -0.10607844988505045, "compression_ratio": 1.5411764705882354, "no_speech_prob": 5.62638888368383e-07}, {"id": 664, "seek": 408206, "start": 4097.08, "end": 4102.78, "text": " we called random forest regressor is it's just using that decision tree builder that", "tokens": [321, 1219, 4974, 6719, 1121, 735, 284, 307, 309, 311, 445, 1228, 300, 3537, 4230, 27377, 300], "temperature": 0.0, "avg_logprob": -0.10607844988505045, "compression_ratio": 1.5411764705882354, "no_speech_prob": 5.62638888368383e-07}, {"id": 665, "seek": 408206, "start": 4102.78, "end": 4108.36, "text": " we've already seen but it's building multiple versions with these different random subsets", "tokens": [321, 600, 1217, 1612, 457, 309, 311, 2390, 3866, 9606, 365, 613, 819, 4974, 2090, 1385], "temperature": 0.0, "avg_logprob": -0.10607844988505045, "compression_ratio": 1.5411764705882354, "no_speech_prob": 5.62638888368383e-07}, {"id": 666, "seek": 410836, "start": 4108.36, "end": 4115.38, "text": " and for each binary split it does it's also randomly selecting a subset of columns and", "tokens": [293, 337, 1184, 17434, 7472, 309, 775, 309, 311, 611, 16979, 18182, 257, 25993, 295, 13766, 293], "temperature": 0.0, "avg_logprob": -0.07931800221287927, "compression_ratio": 1.723809523809524, "no_speech_prob": 1.4144699207463418e-06}, {"id": 667, "seek": 410836, "start": 4115.38, "end": 4121.86, "text": " then when we create a prediction it is averaging the predictions of each of the trees and as", "tokens": [550, 562, 321, 1884, 257, 17630, 309, 307, 47308, 264, 21264, 295, 1184, 295, 264, 5852, 293, 382], "temperature": 0.0, "avg_logprob": -0.07931800221287927, "compression_ratio": 1.723809523809524, "no_speech_prob": 1.4144699207463418e-06}, {"id": 668, "seek": 410836, "start": 4121.86, "end": 4126.98, "text": " you can see it's giving a really great result and one of the amazing things we'll find is", "tokens": [291, 393, 536, 309, 311, 2902, 257, 534, 869, 1874, 293, 472, 295, 264, 2243, 721, 321, 603, 915, 307], "temperature": 0.0, "avg_logprob": -0.07931800221287927, "compression_ratio": 1.723809523809524, "no_speech_prob": 1.4144699207463418e-06}, {"id": 669, "seek": 410836, "start": 4126.98, "end": 4131.92, "text": " that it's going to be hard for us to improve this very much you know the kind of the default", "tokens": [300, 309, 311, 516, 281, 312, 1152, 337, 505, 281, 3470, 341, 588, 709, 291, 458, 264, 733, 295, 264, 7576], "temperature": 0.0, "avg_logprob": -0.07931800221287927, "compression_ratio": 1.723809523809524, "no_speech_prob": 1.4144699207463418e-06}, {"id": 670, "seek": 413192, "start": 4131.92, "end": 4141.06, "text": " starting point tends to turn out to be pretty great the the sklearn docs have lots of good", "tokens": [2891, 935, 12258, 281, 1261, 484, 281, 312, 1238, 869, 264, 264, 1110, 306, 1083, 45623, 362, 3195, 295, 665], "temperature": 0.0, "avg_logprob": -0.09371075082997807, "compression_ratio": 1.5976331360946745, "no_speech_prob": 5.989259648231382e-07}, {"id": 671, "seek": 413192, "start": 4141.06, "end": 4145.52, "text": " information in one of the things it has is this nice picture that shows as you increase", "tokens": [1589, 294, 472, 295, 264, 721, 309, 575, 307, 341, 1481, 3036, 300, 3110, 382, 291, 3488], "temperature": 0.0, "avg_logprob": -0.09371075082997807, "compression_ratio": 1.5976331360946745, "no_speech_prob": 5.989259648231382e-07}, {"id": 672, "seek": 413192, "start": 4145.52, "end": 4153.92, "text": " the number of estimators how does the accuracy improve error rate improves or different max", "tokens": [264, 1230, 295, 8017, 3391, 577, 775, 264, 14170, 3470, 6713, 3314, 24771, 420, 819, 11469], "temperature": 0.0, "avg_logprob": -0.09371075082997807, "compression_ratio": 1.5976331360946745, "no_speech_prob": 5.989259648231382e-07}, {"id": 673, "seek": 415392, "start": 4153.92, "end": 4162.54, "text": " features levels and in general the more trees you add the more accurate your model there", "tokens": [4122, 4358, 293, 294, 2674, 264, 544, 5852, 291, 909, 264, 544, 8559, 428, 2316, 456], "temperature": 0.0, "avg_logprob": -0.11668946526267311, "compression_ratio": 1.8097560975609757, "no_speech_prob": 2.9649027055711485e-07}, {"id": 674, "seek": 415392, "start": 4162.54, "end": 4171.86, "text": " it's not going to overfit right because it's it's averaging more of these these weak models", "tokens": [309, 311, 406, 516, 281, 670, 6845, 558, 570, 309, 311, 309, 311, 47308, 544, 295, 613, 613, 5336, 5245], "temperature": 0.0, "avg_logprob": -0.11668946526267311, "compression_ratio": 1.8097560975609757, "no_speech_prob": 2.9649027055711485e-07}, {"id": 675, "seek": 415392, "start": 4171.86, "end": 4177.6, "text": " more of these models that are trained on subsets of the data so train as many use as many estimators", "tokens": [544, 295, 613, 5245, 300, 366, 8895, 322, 2090, 1385, 295, 264, 1412, 370, 3847, 382, 867, 764, 382, 867, 8017, 3391], "temperature": 0.0, "avg_logprob": -0.11668946526267311, "compression_ratio": 1.8097560975609757, "no_speech_prob": 2.9649027055711485e-07}, {"id": 676, "seek": 415392, "start": 4177.6, "end": 4182.32, "text": " as you like really just a case of how much time do you have and whether you kind of reach", "tokens": [382, 291, 411, 534, 445, 257, 1389, 295, 577, 709, 565, 360, 291, 362, 293, 1968, 291, 733, 295, 2524], "temperature": 0.0, "avg_logprob": -0.11668946526267311, "compression_ratio": 1.8097560975609757, "no_speech_prob": 2.9649027055711485e-07}, {"id": 677, "seek": 418232, "start": 4182.32, "end": 4187.759999999999, "text": " a point where it's not really improving anymore you can actually get at the underlying decision", "tokens": [257, 935, 689, 309, 311, 406, 534, 11470, 3602, 291, 393, 767, 483, 412, 264, 14217, 3537], "temperature": 0.0, "avg_logprob": -0.12067839387175325, "compression_ratio": 1.7623762376237624, "no_speech_prob": 4.6644444751109404e-07}, {"id": 678, "seek": 418232, "start": 4187.759999999999, "end": 4193.46, "text": " trees in a model in a random forest model using estimators underscore so with a list", "tokens": [5852, 294, 257, 2316, 294, 257, 4974, 6719, 2316, 1228, 8017, 3391, 37556, 370, 365, 257, 1329], "temperature": 0.0, "avg_logprob": -0.12067839387175325, "compression_ratio": 1.7623762376237624, "no_speech_prob": 4.6644444751109404e-07}, {"id": 679, "seek": 418232, "start": 4193.46, "end": 4199.66, "text": " comprehension we can call predict on each individual tree and so here's an array a numpy", "tokens": [44991, 321, 393, 818, 6069, 322, 1184, 2609, 4230, 293, 370, 510, 311, 364, 10225, 257, 1031, 8200], "temperature": 0.0, "avg_logprob": -0.12067839387175325, "compression_ratio": 1.7623762376237624, "no_speech_prob": 4.6644444751109404e-07}, {"id": 680, "seek": 418232, "start": 4199.66, "end": 4206.679999999999, "text": " array containing the predictions from each individual tree for each row in our data so", "tokens": [10225, 19273, 264, 21264, 490, 1184, 2609, 4230, 337, 1184, 5386, 294, 527, 1412, 370], "temperature": 0.0, "avg_logprob": -0.12067839387175325, "compression_ratio": 1.7623762376237624, "no_speech_prob": 4.6644444751109404e-07}, {"id": 681, "seek": 420668, "start": 4206.68, "end": 4219.16, "text": " if we take the mean across the zero axis we'll get exactly the same number because remember", "tokens": [498, 321, 747, 264, 914, 2108, 264, 4018, 10298, 321, 603, 483, 2293, 264, 912, 1230, 570, 1604], "temperature": 0.0, "avg_logprob": -0.12095096499420875, "compression_ratio": 1.4918032786885247, "no_speech_prob": 1.1430727653305439e-07}, {"id": 682, "seek": 420668, "start": 4219.16, "end": 4227.92, "text": " that's what a random forest does is it takes the mean of the trees predictions so one cool", "tokens": [300, 311, 437, 257, 4974, 6719, 775, 307, 309, 2516, 264, 914, 295, 264, 5852, 21264, 370, 472, 1627], "temperature": 0.0, "avg_logprob": -0.12095096499420875, "compression_ratio": 1.4918032786885247, "no_speech_prob": 1.1430727653305439e-07}, {"id": 683, "seek": 422792, "start": 4227.92, "end": 4236.72, "text": " thing we could do is we could look at the 40 estimators we have and grab the predictions", "tokens": [551, 321, 727, 360, 307, 321, 727, 574, 412, 264, 3356, 8017, 3391, 321, 362, 293, 4444, 264, 21264], "temperature": 0.0, "avg_logprob": -0.11451040808834247, "compression_ratio": 1.5930232558139534, "no_speech_prob": 5.626393431157339e-07}, {"id": 684, "seek": 422792, "start": 4236.72, "end": 4245.4800000000005, "text": " or the first I of those trees and take their mean and then we can find the root means grid", "tokens": [420, 264, 700, 286, 295, 729, 5852, 293, 747, 641, 914, 293, 550, 321, 393, 915, 264, 5593, 1355, 10748], "temperature": 0.0, "avg_logprob": -0.11451040808834247, "compression_ratio": 1.5930232558139534, "no_speech_prob": 5.626393431157339e-07}, {"id": 685, "seek": 422792, "start": 4245.4800000000005, "end": 4253.68, "text": " error and so in other words here is the accuracy when you've just got one tree two trees three", "tokens": [6713, 293, 370, 294, 661, 2283, 510, 307, 264, 14170, 562, 291, 600, 445, 658, 472, 4230, 732, 5852, 1045], "temperature": 0.0, "avg_logprob": -0.11451040808834247, "compression_ratio": 1.5930232558139534, "no_speech_prob": 5.626393431157339e-07}, {"id": 686, "seek": 425368, "start": 4253.68, "end": 4258.72, "text": " trees four trees five trees etc and you can see so it's kind of nice right you can you", "tokens": [5852, 1451, 5852, 1732, 5852, 5183, 293, 291, 393, 536, 370, 309, 311, 733, 295, 1481, 558, 291, 393, 291], "temperature": 0.0, "avg_logprob": -0.11827779397731875, "compression_ratio": 1.7857142857142858, "no_speech_prob": 1.8738688822850236e-06}, {"id": 687, "seek": 425368, "start": 4258.72, "end": 4265.04, "text": " can actually create your own and build your own tools to look inside these things and", "tokens": [393, 767, 1884, 428, 1065, 293, 1322, 428, 1065, 3873, 281, 574, 1854, 613, 721, 293], "temperature": 0.0, "avg_logprob": -0.11827779397731875, "compression_ratio": 1.7857142857142858, "no_speech_prob": 1.8738688822850236e-06}, {"id": 688, "seek": 425368, "start": 4265.04, "end": 4270.6, "text": " see what's going on and so we can see here that as you add more and more trees the accuracy", "tokens": [536, 437, 311, 516, 322, 293, 370, 321, 393, 536, 510, 300, 382, 291, 909, 544, 293, 544, 5852, 264, 14170], "temperature": 0.0, "avg_logprob": -0.11827779397731875, "compression_ratio": 1.7857142857142858, "no_speech_prob": 1.8738688822850236e-06}, {"id": 689, "seek": 425368, "start": 4270.6, "end": 4276.0, "text": " did indeed keep improving or the root means grid error kept improving although it the", "tokens": [630, 6451, 1066, 11470, 420, 264, 5593, 1355, 10748, 6713, 4305, 11470, 4878, 309, 264], "temperature": 0.0, "avg_logprob": -0.11827779397731875, "compression_ratio": 1.7857142857142858, "no_speech_prob": 1.8738688822850236e-06}, {"id": 690, "seek": 427600, "start": 4276.0, "end": 4286.44, "text": " improvement slowed down after a while the validation set is worse than the training", "tokens": [10444, 32057, 760, 934, 257, 1339, 264, 24071, 992, 307, 5324, 813, 264, 3097], "temperature": 0.0, "avg_logprob": -0.10030428171157837, "compression_ratio": 1.719626168224299, "no_speech_prob": 1.9444304655280575e-07}, {"id": 691, "seek": 427600, "start": 4286.44, "end": 4292.6, "text": " set and there's a couple of reasons that could have happened the first reason could be because", "tokens": [992, 293, 456, 311, 257, 1916, 295, 4112, 300, 727, 362, 2011, 264, 700, 1778, 727, 312, 570], "temperature": 0.0, "avg_logprob": -0.10030428171157837, "compression_ratio": 1.719626168224299, "no_speech_prob": 1.9444304655280575e-07}, {"id": 692, "seek": 427600, "start": 4292.6, "end": 4298.6, "text": " we're still overfitting which is not necessarily a problem this is something we could identify", "tokens": [321, 434, 920, 670, 69, 2414, 597, 307, 406, 4725, 257, 1154, 341, 307, 746, 321, 727, 5876], "temperature": 0.0, "avg_logprob": -0.10030428171157837, "compression_ratio": 1.719626168224299, "no_speech_prob": 1.9444304655280575e-07}, {"id": 693, "seek": 427600, "start": 4298.6, "end": 4304.1, "text": " or maybe it's because the the fact that we're trying to predict the last two weeks is actually", "tokens": [420, 1310, 309, 311, 570, 264, 264, 1186, 300, 321, 434, 1382, 281, 6069, 264, 1036, 732, 3259, 307, 767], "temperature": 0.0, "avg_logprob": -0.10030428171157837, "compression_ratio": 1.719626168224299, "no_speech_prob": 1.9444304655280575e-07}, {"id": 694, "seek": 430410, "start": 4304.1, "end": 4309.52, "text": " a problem and that the last two weeks are kind of different to the other options in", "tokens": [257, 1154, 293, 300, 264, 1036, 732, 3259, 366, 733, 295, 819, 281, 264, 661, 3956, 294], "temperature": 0.0, "avg_logprob": -0.11542056588565602, "compression_ratio": 1.6442307692307692, "no_speech_prob": 1.2679221299549681e-06}, {"id": 695, "seek": 430410, "start": 4309.52, "end": 4314.240000000001, "text": " our data set maybe something changed over time so how do we tell which of those two", "tokens": [527, 1412, 992, 1310, 746, 3105, 670, 565, 370, 577, 360, 321, 980, 597, 295, 729, 732], "temperature": 0.0, "avg_logprob": -0.11542056588565602, "compression_ratio": 1.6442307692307692, "no_speech_prob": 1.2679221299549681e-06}, {"id": 696, "seek": 430410, "start": 4314.240000000001, "end": 4320.8, "text": " reasons there are what what is the reason that our validation set is worse we can actually", "tokens": [4112, 456, 366, 437, 437, 307, 264, 1778, 300, 527, 24071, 992, 307, 5324, 321, 393, 767], "temperature": 0.0, "avg_logprob": -0.11542056588565602, "compression_ratio": 1.6442307692307692, "no_speech_prob": 1.2679221299549681e-06}, {"id": 697, "seek": 430410, "start": 4320.8, "end": 4326.4400000000005, "text": " find out using a very clever trick called out-of-bag error OOB error and we use OOB", "tokens": [915, 484, 1228, 257, 588, 13494, 4282, 1219, 484, 12, 2670, 12, 17282, 6713, 422, 46, 33, 6713, 293, 321, 764, 422, 46, 33], "temperature": 0.0, "avg_logprob": -0.11542056588565602, "compression_ratio": 1.6442307692307692, "no_speech_prob": 1.2679221299549681e-06}, {"id": 698, "seek": 432644, "start": 4326.44, "end": 4335.639999999999, "text": " error for lots of things you can grab the OOB error or you can grab the OOB predictions", "tokens": [6713, 337, 3195, 295, 721, 291, 393, 4444, 264, 422, 46, 33, 6713, 420, 291, 393, 4444, 264, 422, 46, 33, 21264], "temperature": 0.0, "avg_logprob": -0.1245717881601068, "compression_ratio": 1.7236842105263157, "no_speech_prob": 1.79830450974805e-07}, {"id": 699, "seek": 432644, "start": 4335.639999999999, "end": 4341.28, "text": " from the model with OOB prediction and you can grab the RMSE and you can find that the", "tokens": [490, 264, 2316, 365, 422, 46, 33, 17630, 293, 291, 393, 4444, 264, 23790, 5879, 293, 291, 393, 915, 300, 264], "temperature": 0.0, "avg_logprob": -0.1245717881601068, "compression_ratio": 1.7236842105263157, "no_speech_prob": 1.79830450974805e-07}, {"id": 700, "seek": 432644, "start": 4341.28, "end": 4353.28, "text": " OOB error RMSE is 0.21 which is quite a bit better than 0.23 so let me explain what OOB", "tokens": [422, 46, 33, 6713, 23790, 5879, 307, 1958, 13, 4436, 597, 307, 1596, 257, 857, 1101, 813, 1958, 13, 9356, 370, 718, 385, 2903, 437, 422, 46, 33], "temperature": 0.0, "avg_logprob": -0.1245717881601068, "compression_ratio": 1.7236842105263157, "no_speech_prob": 1.79830450974805e-07}, {"id": 701, "seek": 435328, "start": 4353.28, "end": 4364.84, "text": " error is what OOB error is is we look at each row of the training set not the validation", "tokens": [6713, 307, 437, 422, 46, 33, 6713, 307, 307, 321, 574, 412, 1184, 5386, 295, 264, 3097, 992, 406, 264, 24071], "temperature": 0.0, "avg_logprob": -0.09744112733481587, "compression_ratio": 1.7908496732026145, "no_speech_prob": 1.1544576636879356e-06}, {"id": 702, "seek": 435328, "start": 4364.84, "end": 4372.719999999999, "text": " set each row of the training set and we say so fix a we say for row number one which trees", "tokens": [992, 1184, 5386, 295, 264, 3097, 992, 293, 321, 584, 370, 3191, 257, 321, 584, 337, 5386, 1230, 472, 597, 5852], "temperature": 0.0, "avg_logprob": -0.09744112733481587, "compression_ratio": 1.7908496732026145, "no_speech_prob": 1.1544576636879356e-06}, {"id": 703, "seek": 435328, "start": 4372.719999999999, "end": 4378.719999999999, "text": " included row number one in the training and we'll say okay let's not use those for calculating", "tokens": [5556, 5386, 1230, 472, 294, 264, 3097, 293, 321, 603, 584, 1392, 718, 311, 406, 764, 729, 337, 28258], "temperature": 0.0, "avg_logprob": -0.09744112733481587, "compression_ratio": 1.7908496732026145, "no_speech_prob": 1.1544576636879356e-06}, {"id": 704, "seek": 437872, "start": 4378.72, "end": 4384.2, "text": " the error because that was part of those trees training I would just calculate the error", "tokens": [264, 6713, 570, 300, 390, 644, 295, 729, 5852, 3097, 286, 576, 445, 8873, 264, 6713], "temperature": 0.0, "avg_logprob": -0.11521915197372437, "compression_ratio": 1.9508196721311475, "no_speech_prob": 7.112417392818315e-07}, {"id": 705, "seek": 437872, "start": 4384.2, "end": 4390.96, "text": " for that row using the trees where that row was not included in training that tree because", "tokens": [337, 300, 5386, 1228, 264, 5852, 689, 300, 5386, 390, 406, 5556, 294, 3097, 300, 4230, 570], "temperature": 0.0, "avg_logprob": -0.11521915197372437, "compression_ratio": 1.9508196721311475, "no_speech_prob": 7.112417392818315e-07}, {"id": 706, "seek": 437872, "start": 4390.96, "end": 4395.92, "text": " remember every tree is using only a subset of the data so we do that for every row we", "tokens": [1604, 633, 4230, 307, 1228, 787, 257, 25993, 295, 264, 1412, 370, 321, 360, 300, 337, 633, 5386, 321], "temperature": 0.0, "avg_logprob": -0.11521915197372437, "compression_ratio": 1.9508196721311475, "no_speech_prob": 7.112417392818315e-07}, {"id": 707, "seek": 437872, "start": 4395.92, "end": 4405.88, "text": " find the prediction using only the trees that were not used that that that row was not used", "tokens": [915, 264, 17630, 1228, 787, 264, 5852, 300, 645, 406, 1143, 300, 300, 300, 5386, 390, 406, 1143], "temperature": 0.0, "avg_logprob": -0.11521915197372437, "compression_ratio": 1.9508196721311475, "no_speech_prob": 7.112417392818315e-07}, {"id": 708, "seek": 440588, "start": 4405.88, "end": 4411.6, "text": " and those are the OOB predictions but in other words this is like giving us a validation", "tokens": [293, 729, 366, 264, 422, 46, 33, 21264, 457, 294, 661, 2283, 341, 307, 411, 2902, 505, 257, 24071], "temperature": 0.0, "avg_logprob": -0.10681576099035875, "compression_ratio": 1.8181818181818181, "no_speech_prob": 3.2377488423662726e-06}, {"id": 709, "seek": 440588, "start": 4411.6, "end": 4419.16, "text": " set result and without actually needing a validation but the thing is it's not with", "tokens": [992, 1874, 293, 1553, 767, 18006, 257, 24071, 457, 264, 551, 307, 309, 311, 406, 365], "temperature": 0.0, "avg_logprob": -0.10681576099035875, "compression_ratio": 1.8181818181818181, "no_speech_prob": 3.2377488423662726e-06}, {"id": 710, "seek": 440588, "start": 4419.16, "end": 4422.74, "text": " that time offset it's not looking at the last two weeks it's looking at the whole training", "tokens": [300, 565, 18687, 309, 311, 406, 1237, 412, 264, 1036, 732, 3259, 309, 311, 1237, 412, 264, 1379, 3097], "temperature": 0.0, "avg_logprob": -0.10681576099035875, "compression_ratio": 1.8181818181818181, "no_speech_prob": 3.2377488423662726e-06}, {"id": 711, "seek": 440588, "start": 4422.74, "end": 4430.24, "text": " set so this basically tells us how much of the error is due to overfitting versus jury", "tokens": [992, 370, 341, 1936, 5112, 505, 577, 709, 295, 264, 6713, 307, 3462, 281, 670, 69, 2414, 5717, 19516], "temperature": 0.0, "avg_logprob": -0.10681576099035875, "compression_ratio": 1.8181818181818181, "no_speech_prob": 3.2377488423662726e-06}, {"id": 712, "seek": 440588, "start": 4430.24, "end": 4435.36, "text": " to being the last couple of weeks so that's a cool trick OOB error is something that very", "tokens": [281, 885, 264, 1036, 1916, 295, 3259, 370, 300, 311, 257, 1627, 4282, 422, 46, 33, 6713, 307, 746, 300, 588], "temperature": 0.0, "avg_logprob": -0.10681576099035875, "compression_ratio": 1.8181818181818181, "no_speech_prob": 3.2377488423662726e-06}, {"id": 713, "seek": 443536, "start": 4435.36, "end": 4440.04, "text": " quickly kind of gives us a sense of how much we're we're overfitting and we don't even", "tokens": [2661, 733, 295, 2709, 505, 257, 2020, 295, 577, 709, 321, 434, 321, 434, 670, 69, 2414, 293, 321, 500, 380, 754], "temperature": 0.0, "avg_logprob": -0.0770384117409035, "compression_ratio": 1.757085020242915, "no_speech_prob": 5.014699581806781e-06}, {"id": 714, "seek": 443536, "start": 4440.04, "end": 4446.36, "text": " need a validation set to do it but there's that OOB error so that's telling us a bit", "tokens": [643, 257, 24071, 992, 281, 360, 309, 457, 456, 311, 300, 422, 46, 33, 6713, 370, 300, 311, 3585, 505, 257, 857], "temperature": 0.0, "avg_logprob": -0.0770384117409035, "compression_ratio": 1.757085020242915, "no_speech_prob": 5.014699581806781e-06}, {"id": 715, "seek": 443536, "start": 4446.36, "end": 4452.08, "text": " about what's going on in our model but then there's a lot of things we'd like to find", "tokens": [466, 437, 311, 516, 322, 294, 527, 2316, 457, 550, 456, 311, 257, 688, 295, 721, 321, 1116, 411, 281, 915], "temperature": 0.0, "avg_logprob": -0.0770384117409035, "compression_ratio": 1.757085020242915, "no_speech_prob": 5.014699581806781e-06}, {"id": 716, "seek": 443536, "start": 4452.08, "end": 4456.88, "text": " out from our model and I've got five things in particular here which I generally find", "tokens": [484, 490, 527, 2316, 293, 286, 600, 658, 1732, 721, 294, 1729, 510, 597, 286, 5101, 915], "temperature": 0.0, "avg_logprob": -0.0770384117409035, "compression_ratio": 1.757085020242915, "no_speech_prob": 5.014699581806781e-06}, {"id": 717, "seek": 443536, "start": 4456.88, "end": 4465.12, "text": " pretty interesting which is how confident are we about our predictions for some particular", "tokens": [1238, 1880, 597, 307, 577, 6679, 366, 321, 466, 527, 21264, 337, 512, 1729], "temperature": 0.0, "avg_logprob": -0.0770384117409035, "compression_ratio": 1.757085020242915, "no_speech_prob": 5.014699581806781e-06}, {"id": 718, "seek": 446512, "start": 4465.12, "end": 4470.68, "text": " prediction we're making like we can say this is what we think the prediction is but how", "tokens": [17630, 321, 434, 1455, 411, 321, 393, 584, 341, 307, 437, 321, 519, 264, 17630, 307, 457, 577], "temperature": 0.0, "avg_logprob": -0.12821980265827923, "compression_ratio": 1.7707317073170732, "no_speech_prob": 3.187534048265661e-06}, {"id": 719, "seek": 446512, "start": 4470.68, "end": 4476.98, "text": " confident are we is that actually that or is just about that or we really have no idea", "tokens": [6679, 366, 321, 307, 300, 767, 300, 420, 307, 445, 466, 300, 420, 321, 534, 362, 572, 1558], "temperature": 0.0, "avg_logprob": -0.12821980265827923, "compression_ratio": 1.7707317073170732, "no_speech_prob": 3.187534048265661e-06}, {"id": 720, "seek": 446512, "start": 4476.98, "end": 4483.96, "text": " and then for particular for predicting a particular item which factors were the most important", "tokens": [293, 550, 337, 1729, 337, 32884, 257, 1729, 3174, 597, 6771, 645, 264, 881, 1021], "temperature": 0.0, "avg_logprob": -0.12821980265827923, "compression_ratio": 1.7707317073170732, "no_speech_prob": 3.187534048265661e-06}, {"id": 721, "seek": 446512, "start": 4483.96, "end": 4490.14, "text": " in that prediction and how did they influence it overall which columns are making the biggest", "tokens": [294, 300, 17630, 293, 577, 630, 436, 6503, 309, 4787, 597, 13766, 366, 1455, 264, 3880], "temperature": 0.0, "avg_logprob": -0.12821980265827923, "compression_ratio": 1.7707317073170732, "no_speech_prob": 3.187534048265661e-06}, {"id": 722, "seek": 449014, "start": 4490.14, "end": 4495.360000000001, "text": " difference in amputal which ones could we maybe throw away and it wouldn't matter which", "tokens": [2649, 294, 669, 2582, 304, 597, 2306, 727, 321, 1310, 3507, 1314, 293, 309, 2759, 380, 1871, 597], "temperature": 0.0, "avg_logprob": -0.11275670557846258, "compression_ratio": 1.6571428571428573, "no_speech_prob": 1.0845143378901412e-06}, {"id": 723, "seek": 449014, "start": 4495.360000000001, "end": 4503.240000000001, "text": " columns are basically redundant with each other so we don't really need both of them", "tokens": [13766, 366, 1936, 40997, 365, 1184, 661, 370, 321, 500, 380, 534, 643, 1293, 295, 552], "temperature": 0.0, "avg_logprob": -0.11275670557846258, "compression_ratio": 1.6571428571428573, "no_speech_prob": 1.0845143378901412e-06}, {"id": 724, "seek": 449014, "start": 4503.240000000001, "end": 4508.820000000001, "text": " and as we vary some column how does it change the prediction so those are the five things", "tokens": [293, 382, 321, 10559, 512, 7738, 577, 775, 309, 1319, 264, 17630, 370, 729, 366, 264, 1732, 721], "temperature": 0.0, "avg_logprob": -0.11275670557846258, "compression_ratio": 1.6571428571428573, "no_speech_prob": 1.0845143378901412e-06}, {"id": 725, "seek": 449014, "start": 4508.820000000001, "end": 4513.200000000001, "text": " that we're that I'm interested in figuring out and we can do all of those things with", "tokens": [300, 321, 434, 300, 286, 478, 3102, 294, 15213, 484, 293, 321, 393, 360, 439, 295, 729, 721, 365], "temperature": 0.0, "avg_logprob": -0.11275670557846258, "compression_ratio": 1.6571428571428573, "no_speech_prob": 1.0845143378901412e-06}, {"id": 726, "seek": 451320, "start": 4513.2, "end": 4521.36, "text": " a random forest let's start with the first one so the first one we've already seen that", "tokens": [257, 4974, 6719, 718, 311, 722, 365, 264, 700, 472, 370, 264, 700, 472, 321, 600, 1217, 1612, 300], "temperature": 0.0, "avg_logprob": -0.10036367177963257, "compression_ratio": 1.6838709677419355, "no_speech_prob": 1.2482680631364929e-06}, {"id": 727, "seek": 451320, "start": 4521.36, "end": 4530.5199999999995, "text": " we can grab all of the predictions for all of the trees and take their mean to get the", "tokens": [321, 393, 4444, 439, 295, 264, 21264, 337, 439, 295, 264, 5852, 293, 747, 641, 914, 281, 483, 264], "temperature": 0.0, "avg_logprob": -0.10036367177963257, "compression_ratio": 1.6838709677419355, "no_speech_prob": 1.2482680631364929e-06}, {"id": 728, "seek": 451320, "start": 4530.5199999999995, "end": 4534.679999999999, "text": " actual predictions of the model and then to get the RMSE but what if instead of saying", "tokens": [3539, 21264, 295, 264, 2316, 293, 550, 281, 483, 264, 23790, 5879, 457, 437, 498, 2602, 295, 1566], "temperature": 0.0, "avg_logprob": -0.10036367177963257, "compression_ratio": 1.6838709677419355, "no_speech_prob": 1.2482680631364929e-06}, {"id": 729, "seek": 453468, "start": 4534.68, "end": 4544.72, "text": " mean we did exactly the same thing like so but instead said standard deviation this is", "tokens": [914, 321, 630, 2293, 264, 912, 551, 411, 370, 457, 2602, 848, 3832, 25163, 341, 307], "temperature": 0.0, "avg_logprob": -0.12125084048411885, "compression_ratio": 1.5853658536585367, "no_speech_prob": 7.224429623420292e-07}, {"id": 730, "seek": 453468, "start": 4544.72, "end": 4554.76, "text": " going to tell us for every row in our data set how much did the trees vary and so if", "tokens": [516, 281, 980, 505, 337, 633, 5386, 294, 527, 1412, 992, 577, 709, 630, 264, 5852, 10559, 293, 370, 498], "temperature": 0.0, "avg_logprob": -0.12125084048411885, "compression_ratio": 1.5853658536585367, "no_speech_prob": 7.224429623420292e-07}, {"id": 731, "seek": 453468, "start": 4554.76, "end": 4561.200000000001, "text": " our model really had never seen kind of data like this before it was something where you", "tokens": [527, 2316, 534, 632, 1128, 1612, 733, 295, 1412, 411, 341, 949, 309, 390, 746, 689, 291], "temperature": 0.0, "avg_logprob": -0.12125084048411885, "compression_ratio": 1.5853658536585367, "no_speech_prob": 7.224429623420292e-07}, {"id": 732, "seek": 456120, "start": 4561.2, "end": 4564.88, "text": " know different trees were giving very different predictions it might give us a sense that", "tokens": [458, 819, 5852, 645, 2902, 588, 819, 21264, 309, 1062, 976, 505, 257, 2020, 300], "temperature": 0.0, "avg_logprob": -0.1004785085335756, "compression_ratio": 1.7727272727272727, "no_speech_prob": 4.88829698497284e-07}, {"id": 733, "seek": 456120, "start": 4564.88, "end": 4571.24, "text": " maybe this is something that we're not at all confident about and as you can see when", "tokens": [1310, 341, 307, 746, 300, 321, 434, 406, 412, 439, 6679, 466, 293, 382, 291, 393, 536, 562], "temperature": 0.0, "avg_logprob": -0.1004785085335756, "compression_ratio": 1.7727272727272727, "no_speech_prob": 4.88829698497284e-07}, {"id": 734, "seek": 456120, "start": 4571.24, "end": 4575.92, "text": " we look at the standard deviation of the trees for each prediction let's just look at the", "tokens": [321, 574, 412, 264, 3832, 25163, 295, 264, 5852, 337, 1184, 17630, 718, 311, 445, 574, 412, 264], "temperature": 0.0, "avg_logprob": -0.1004785085335756, "compression_ratio": 1.7727272727272727, "no_speech_prob": 4.88829698497284e-07}, {"id": 735, "seek": 456120, "start": 4575.92, "end": 4581.679999999999, "text": " first five they vary a lot right point two point one point oh nine point nearly point", "tokens": [700, 1732, 436, 10559, 257, 688, 558, 935, 732, 935, 472, 935, 1954, 4949, 935, 6217, 935], "temperature": 0.0, "avg_logprob": -0.1004785085335756, "compression_ratio": 1.7727272727272727, "no_speech_prob": 4.88829698497284e-07}, {"id": 736, "seek": 458168, "start": 4581.68, "end": 4591.6, "text": " three okay so this is a really interesting it's not something that a lot of people talk", "tokens": [1045, 1392, 370, 341, 307, 257, 534, 1880, 309, 311, 406, 746, 300, 257, 688, 295, 561, 751], "temperature": 0.0, "avg_logprob": -0.08049129843711852, "compression_ratio": 1.7294685990338163, "no_speech_prob": 1.0677006230253028e-06}, {"id": 737, "seek": 458168, "start": 4591.6, "end": 4596.68, "text": " about but I think it's a really interesting approach to kind of figuring out whether we", "tokens": [466, 457, 286, 519, 309, 311, 257, 534, 1880, 3109, 281, 733, 295, 15213, 484, 1968, 321], "temperature": 0.0, "avg_logprob": -0.08049129843711852, "compression_ratio": 1.7294685990338163, "no_speech_prob": 1.0677006230253028e-06}, {"id": 738, "seek": 458168, "start": 4596.68, "end": 4602.58, "text": " might want to be cautious about a particular prediction because maybe we're not very confident", "tokens": [1062, 528, 281, 312, 25278, 466, 257, 1729, 17630, 570, 1310, 321, 434, 406, 588, 6679], "temperature": 0.0, "avg_logprob": -0.08049129843711852, "compression_ratio": 1.7294685990338163, "no_speech_prob": 1.0677006230253028e-06}, {"id": 739, "seek": 458168, "start": 4602.58, "end": 4609.0, "text": " about it but there's one thing we can easily do with a random forest the next thing and", "tokens": [466, 309, 457, 456, 311, 472, 551, 321, 393, 3612, 360, 365, 257, 4974, 6719, 264, 958, 551, 293], "temperature": 0.0, "avg_logprob": -0.08049129843711852, "compression_ratio": 1.7294685990338163, "no_speech_prob": 1.0677006230253028e-06}, {"id": 740, "seek": 460900, "start": 4609.0, "end": 4615.72, "text": " this is I think the most important thing for me in terms of interpretation is feature importance", "tokens": [341, 307, 286, 519, 264, 881, 1021, 551, 337, 385, 294, 2115, 295, 14174, 307, 4111, 7379], "temperature": 0.0, "avg_logprob": -0.13996315002441406, "compression_ratio": 1.8823529411764706, "no_speech_prob": 4.425464794621803e-06}, {"id": 741, "seek": 460900, "start": 4615.72, "end": 4619.84, "text": " here's what feature importance looks like we can call feature importance on a model", "tokens": [510, 311, 437, 4111, 7379, 1542, 411, 321, 393, 818, 4111, 7379, 322, 257, 2316], "temperature": 0.0, "avg_logprob": -0.13996315002441406, "compression_ratio": 1.8823529411764706, "no_speech_prob": 4.425464794621803e-06}, {"id": 742, "seek": 460900, "start": 4619.84, "end": 4624.32, "text": " with some independent variables let's say grab the first 10 this says these are the", "tokens": [365, 512, 6695, 9102, 718, 311, 584, 4444, 264, 700, 1266, 341, 1619, 613, 366, 264], "temperature": 0.0, "avg_logprob": -0.13996315002441406, "compression_ratio": 1.8823529411764706, "no_speech_prob": 4.425464794621803e-06}, {"id": 743, "seek": 460900, "start": 4624.32, "end": 4631.16, "text": " 10 most important features in this random forest these are the things that are the most", "tokens": [1266, 881, 1021, 4122, 294, 341, 4974, 6719, 613, 366, 264, 721, 300, 366, 264, 881], "temperature": 0.0, "avg_logprob": -0.13996315002441406, "compression_ratio": 1.8823529411764706, "no_speech_prob": 4.425464794621803e-06}, {"id": 744, "seek": 463116, "start": 4631.16, "end": 4639.16, "text": " strongly driving sale price or we could plot them and so you can see here there's just", "tokens": [10613, 4840, 8680, 3218, 420, 321, 727, 7542, 552, 293, 370, 291, 393, 536, 510, 456, 311, 445], "temperature": 0.0, "avg_logprob": -0.11313023105744392, "compression_ratio": 1.5952380952380953, "no_speech_prob": 5.255373253021389e-06}, {"id": 745, "seek": 463116, "start": 4639.16, "end": 4648.0, "text": " a few things that are by far the most important what year the equipment was made bulldozer", "tokens": [257, 1326, 721, 300, 366, 538, 1400, 264, 881, 1021, 437, 1064, 264, 5927, 390, 1027, 4693, 2595, 4527], "temperature": 0.0, "avg_logprob": -0.11313023105744392, "compression_ratio": 1.5952380952380953, "no_speech_prob": 5.255373253021389e-06}, {"id": 746, "seek": 463116, "start": 4648.0, "end": 4655.04, "text": " or whatever how big is it a plus system whatever that means and the product class whatever", "tokens": [420, 2035, 577, 955, 307, 309, 257, 1804, 1185, 2035, 300, 1355, 293, 264, 1674, 1508, 2035], "temperature": 0.0, "avg_logprob": -0.11313023105744392, "compression_ratio": 1.5952380952380953, "no_speech_prob": 5.255373253021389e-06}, {"id": 747, "seek": 465504, "start": 4655.04, "end": 4664.4, "text": " that means and so you can get this by simply looking inside your train model and grabbing", "tokens": [300, 1355, 293, 370, 291, 393, 483, 341, 538, 2935, 1237, 1854, 428, 3847, 2316, 293, 23771], "temperature": 0.0, "avg_logprob": -0.10175360043843587, "compression_ratio": 1.6094674556213018, "no_speech_prob": 9.276328682972235e-07}, {"id": 748, "seek": 465504, "start": 4664.4, "end": 4670.12, "text": " the feature importances attribute and so here for making it better to print out I'm just", "tokens": [264, 4111, 974, 2676, 19667, 293, 370, 510, 337, 1455, 309, 1101, 281, 4482, 484, 286, 478, 445], "temperature": 0.0, "avg_logprob": -0.10175360043843587, "compression_ratio": 1.6094674556213018, "no_speech_prob": 9.276328682972235e-07}, {"id": 749, "seek": 465504, "start": 4670.12, "end": 4678.76, "text": " sticking that into a data frame and sorting the sending by importance so how is this actually", "tokens": [13465, 300, 666, 257, 1412, 3920, 293, 32411, 264, 7750, 538, 7379, 370, 577, 307, 341, 767], "temperature": 0.0, "avg_logprob": -0.10175360043843587, "compression_ratio": 1.6094674556213018, "no_speech_prob": 9.276328682972235e-07}, {"id": 750, "seek": 467876, "start": 4678.76, "end": 4685.76, "text": " being done it's it's actually really neat what scikit-learn does and Breiman the inventor", "tokens": [885, 1096, 309, 311, 309, 311, 767, 534, 10654, 437, 2180, 22681, 12, 306, 1083, 775, 293, 7090, 25504, 264, 41593], "temperature": 0.0, "avg_logprob": -0.10355987171135327, "compression_ratio": 1.83402489626556, "no_speech_prob": 9.276335504182498e-07}, {"id": 751, "seek": 467876, "start": 4685.76, "end": 4691.08, "text": " of random forest described is that you can go through each tree and then start at the", "tokens": [295, 4974, 6719, 7619, 307, 300, 291, 393, 352, 807, 1184, 4230, 293, 550, 722, 412, 264], "temperature": 0.0, "avg_logprob": -0.10355987171135327, "compression_ratio": 1.83402489626556, "no_speech_prob": 9.276335504182498e-07}, {"id": 752, "seek": 467876, "start": 4691.08, "end": 4697.12, "text": " top of the tree and look at each branch and at each branch see what feature was used the", "tokens": [1192, 295, 264, 4230, 293, 574, 412, 1184, 9819, 293, 412, 1184, 9819, 536, 437, 4111, 390, 1143, 264], "temperature": 0.0, "avg_logprob": -0.10355987171135327, "compression_ratio": 1.83402489626556, "no_speech_prob": 9.276335504182498e-07}, {"id": 753, "seek": 467876, "start": 4697.12, "end": 4702.76, "text": " split which binary which the binary split was based on which column and then how much", "tokens": [7472, 597, 17434, 597, 264, 17434, 7472, 390, 2361, 322, 597, 7738, 293, 550, 577, 709], "temperature": 0.0, "avg_logprob": -0.10355987171135327, "compression_ratio": 1.83402489626556, "no_speech_prob": 9.276335504182498e-07}, {"id": 754, "seek": 467876, "start": 4702.76, "end": 4708.62, "text": " better was the model after that split compared to beforehand and we basically then say okay", "tokens": [1101, 390, 264, 2316, 934, 300, 7472, 5347, 281, 22893, 293, 321, 1936, 550, 584, 1392], "temperature": 0.0, "avg_logprob": -0.10355987171135327, "compression_ratio": 1.83402489626556, "no_speech_prob": 9.276335504182498e-07}, {"id": 755, "seek": 470862, "start": 4708.62, "end": 4714.5599999999995, "text": " that column was responsible for that amount of improvement and so you add that up across", "tokens": [300, 7738, 390, 6250, 337, 300, 2372, 295, 10444, 293, 370, 291, 909, 300, 493, 2108], "temperature": 0.0, "avg_logprob": -0.07931619092642543, "compression_ratio": 1.7766497461928934, "no_speech_prob": 4.247028186910029e-07}, {"id": 756, "seek": 470862, "start": 4714.5599999999995, "end": 4721.92, "text": " all of the splits across all of the trees for each column and then you normalize it", "tokens": [439, 295, 264, 37741, 2108, 439, 295, 264, 5852, 337, 1184, 7738, 293, 550, 291, 2710, 1125, 309], "temperature": 0.0, "avg_logprob": -0.07931619092642543, "compression_ratio": 1.7766497461928934, "no_speech_prob": 4.247028186910029e-07}, {"id": 757, "seek": 470862, "start": 4721.92, "end": 4727.48, "text": " so they all add to one and that's what gives you these numbers which we show the first", "tokens": [370, 436, 439, 909, 281, 472, 293, 300, 311, 437, 2709, 291, 613, 3547, 597, 321, 855, 264, 700], "temperature": 0.0, "avg_logprob": -0.07931619092642543, "compression_ratio": 1.7766497461928934, "no_speech_prob": 4.247028186910029e-07}, {"id": 758, "seek": 470862, "start": 4727.48, "end": 4734.36, "text": " few of them in this table and the first 30 of them here in this chart so this is something", "tokens": [1326, 295, 552, 294, 341, 3199, 293, 264, 700, 2217, 295, 552, 510, 294, 341, 6927, 370, 341, 307, 746], "temperature": 0.0, "avg_logprob": -0.07931619092642543, "compression_ratio": 1.7766497461928934, "no_speech_prob": 4.247028186910029e-07}, {"id": 759, "seek": 473436, "start": 4734.36, "end": 4740.599999999999, "text": " that's fast and it's easy and it kind of gives us a good sense of like well maybe the stuff", "tokens": [300, 311, 2370, 293, 309, 311, 1858, 293, 309, 733, 295, 2709, 505, 257, 665, 2020, 295, 411, 731, 1310, 264, 1507], "temperature": 0.0, "avg_logprob": -0.10523046206121575, "compression_ratio": 1.576470588235294, "no_speech_prob": 5.626392294288962e-07}, {"id": 760, "seek": 473436, "start": 4740.599999999999, "end": 4749.08, "text": " that are less than 0.005 we could remove so if we did that that would leave us with only", "tokens": [300, 366, 1570, 813, 1958, 13, 628, 20, 321, 727, 4159, 370, 498, 321, 630, 300, 300, 576, 1856, 505, 365, 787], "temperature": 0.0, "avg_logprob": -0.10523046206121575, "compression_ratio": 1.576470588235294, "no_speech_prob": 5.626392294288962e-07}, {"id": 761, "seek": 473436, "start": 4749.08, "end": 4757.96, "text": " 21 columns so let's try that let's just let's just say okay x's which are important the", "tokens": [5080, 13766, 370, 718, 311, 853, 300, 718, 311, 445, 718, 311, 445, 584, 1392, 2031, 311, 597, 366, 1021, 264], "temperature": 0.0, "avg_logprob": -0.10523046206121575, "compression_ratio": 1.576470588235294, "no_speech_prob": 5.626392294288962e-07}, {"id": 762, "seek": 475796, "start": 4757.96, "end": 4765.56, "text": " x's which are in this list of ones to keep do the same they're valid retrain our random", "tokens": [2031, 311, 597, 366, 294, 341, 1329, 295, 2306, 281, 1066, 360, 264, 912, 436, 434, 7363, 1533, 7146, 527, 4974], "temperature": 0.0, "avg_logprob": -0.11789385477701823, "compression_ratio": 1.6294642857142858, "no_speech_prob": 2.829130778536637e-07}, {"id": 763, "seek": 475796, "start": 4765.56, "end": 4774.66, "text": " forest and have a look at the result and basically our accuracy is about the same but we've gone", "tokens": [6719, 293, 362, 257, 574, 412, 264, 1874, 293, 1936, 527, 14170, 307, 466, 264, 912, 457, 321, 600, 2780], "temperature": 0.0, "avg_logprob": -0.11789385477701823, "compression_ratio": 1.6294642857142858, "no_speech_prob": 2.829130778536637e-07}, {"id": 764, "seek": 475796, "start": 4774.66, "end": 4782.32, "text": " down from 78 columns the 21 column so I think this is really important it's not just about", "tokens": [760, 490, 26369, 13766, 264, 5080, 7738, 370, 286, 519, 341, 307, 534, 1021, 309, 311, 406, 445, 466], "temperature": 0.0, "avg_logprob": -0.11789385477701823, "compression_ratio": 1.6294642857142858, "no_speech_prob": 2.829130778536637e-07}, {"id": 765, "seek": 475796, "start": 4782.32, "end": 4785.8, "text": " reading the most accurate model you can but you want to kind of be able to fit it in your", "tokens": [3760, 264, 881, 8559, 2316, 291, 393, 457, 291, 528, 281, 733, 295, 312, 1075, 281, 3318, 309, 294, 428], "temperature": 0.0, "avg_logprob": -0.11789385477701823, "compression_ratio": 1.6294642857142858, "no_speech_prob": 2.829130778536637e-07}, {"id": 766, "seek": 478580, "start": 4785.8, "end": 4791.08, "text": " head as best as possible and so 21 columns is going to be much easier for us to check", "tokens": [1378, 382, 1151, 382, 1944, 293, 370, 5080, 13766, 307, 516, 281, 312, 709, 3571, 337, 505, 281, 1520], "temperature": 0.0, "avg_logprob": -0.11953069922629367, "compression_ratio": 1.5963302752293578, "no_speech_prob": 4.2470256289561803e-07}, {"id": 767, "seek": 478580, "start": 4791.08, "end": 4795.92, "text": " for any data issues and understand what's going on and the accuracy is about the same", "tokens": [337, 604, 1412, 2663, 293, 1223, 437, 311, 516, 322, 293, 264, 14170, 307, 466, 264, 912], "temperature": 0.0, "avg_logprob": -0.11953069922629367, "compression_ratio": 1.5963302752293578, "no_speech_prob": 4.2470256289561803e-07}, {"id": 768, "seek": 478580, "start": 4795.92, "end": 4804.2, "text": " of the RMSE so I would say okay let's do that let's just stick with x's important from now", "tokens": [295, 264, 23790, 5879, 370, 286, 576, 584, 1392, 718, 311, 360, 300, 718, 311, 445, 2897, 365, 2031, 311, 1021, 490, 586], "temperature": 0.0, "avg_logprob": -0.11953069922629367, "compression_ratio": 1.5963302752293578, "no_speech_prob": 4.2470256289561803e-07}, {"id": 769, "seek": 478580, "start": 4804.2, "end": 4812.56, "text": " on and so here's this entire set of the 21 features and you can see it looks now like", "tokens": [322, 293, 370, 510, 311, 341, 2302, 992, 295, 264, 5080, 4122, 293, 291, 393, 536, 309, 1542, 586, 411], "temperature": 0.0, "avg_logprob": -0.11953069922629367, "compression_ratio": 1.5963302752293578, "no_speech_prob": 4.2470256289561803e-07}, {"id": 770, "seek": 481256, "start": 4812.56, "end": 4818.120000000001, "text": " year made and product size of the two really important things and then there's a cluster", "tokens": [1064, 1027, 293, 1674, 2744, 295, 264, 732, 534, 1021, 721, 293, 550, 456, 311, 257, 13630], "temperature": 0.0, "avg_logprob": -0.11129265797289112, "compression_ratio": 1.775, "no_speech_prob": 9.87462158263952e-07}, {"id": 771, "seek": 481256, "start": 4818.120000000001, "end": 4825.4400000000005, "text": " of kind of mainly product related things that are kind of at the next level of importance", "tokens": [295, 733, 295, 8704, 1674, 4077, 721, 300, 366, 733, 295, 412, 264, 958, 1496, 295, 7379], "temperature": 0.0, "avg_logprob": -0.11129265797289112, "compression_ratio": 1.775, "no_speech_prob": 9.87462158263952e-07}, {"id": 772, "seek": 481256, "start": 4825.4400000000005, "end": 4835.080000000001, "text": " one of the tricky things here is that we've got like product class desk model ID secondary", "tokens": [472, 295, 264, 12414, 721, 510, 307, 300, 321, 600, 658, 411, 1674, 1508, 10026, 2316, 7348, 11396], "temperature": 0.0, "avg_logprob": -0.11129265797289112, "compression_ratio": 1.775, "no_speech_prob": 9.87462158263952e-07}, {"id": 773, "seek": 481256, "start": 4835.080000000001, "end": 4839.4400000000005, "text": " desk model desk base model they modeled a script so they all look like there might be", "tokens": [10026, 2316, 10026, 3096, 2316, 436, 37140, 257, 5755, 370, 436, 439, 574, 411, 456, 1062, 312], "temperature": 0.0, "avg_logprob": -0.11129265797289112, "compression_ratio": 1.775, "no_speech_prob": 9.87462158263952e-07}, {"id": 774, "seek": 483944, "start": 4839.44, "end": 4844.719999999999, "text": " similar ways of saying the same thing so one thing that can help us to interpret the feature", "tokens": [2531, 2098, 295, 1566, 264, 912, 551, 370, 472, 551, 300, 393, 854, 505, 281, 7302, 264, 4111], "temperature": 0.0, "avg_logprob": -0.10051267961912518, "compression_ratio": 1.69377990430622, "no_speech_prob": 6.786721655771544e-07}, {"id": 775, "seek": 483944, "start": 4844.719999999999, "end": 4853.679999999999, "text": " importance better and understand better what's happening the model is to remove redundant", "tokens": [7379, 1101, 293, 1223, 1101, 437, 311, 2737, 264, 2316, 307, 281, 4159, 40997], "temperature": 0.0, "avg_logprob": -0.10051267961912518, "compression_ratio": 1.69377990430622, "no_speech_prob": 6.786721655771544e-07}, {"id": 776, "seek": 483944, "start": 4853.679999999999, "end": 4860.48, "text": " features so one way to do that is to call fast AI's cluster columns which is basically", "tokens": [4122, 370, 472, 636, 281, 360, 300, 307, 281, 818, 2370, 7318, 311, 13630, 13766, 597, 307, 1936], "temperature": 0.0, "avg_logprob": -0.10051267961912518, "compression_ratio": 1.69377990430622, "no_speech_prob": 6.786721655771544e-07}, {"id": 777, "seek": 483944, "start": 4860.48, "end": 4865.639999999999, "text": " a thin wrapper for stuff that scikit-learn already provides and what that's going to", "tokens": [257, 5862, 46906, 337, 1507, 300, 2180, 22681, 12, 306, 1083, 1217, 6417, 293, 437, 300, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.10051267961912518, "compression_ratio": 1.69377990430622, "no_speech_prob": 6.786721655771544e-07}, {"id": 778, "seek": 486564, "start": 4865.64, "end": 4871.68, "text": " do is it's going to find pairs of columns which are very similar you can see here sale", "tokens": [360, 307, 309, 311, 516, 281, 915, 15494, 295, 13766, 597, 366, 588, 2531, 291, 393, 536, 510, 8680], "temperature": 0.0, "avg_logprob": -0.09847077301570348, "compression_ratio": 1.8583690987124464, "no_speech_prob": 3.288737616458093e-06}, {"id": 779, "seek": 486564, "start": 4871.68, "end": 4877.200000000001, "text": " year and sale elapsed see how this line is way out to the right or else machine ID and", "tokens": [1064, 293, 8680, 806, 2382, 292, 536, 577, 341, 1622, 307, 636, 484, 281, 264, 558, 420, 1646, 3479, 7348, 293], "temperature": 0.0, "avg_logprob": -0.09847077301570348, "compression_ratio": 1.8583690987124464, "no_speech_prob": 3.288737616458093e-06}, {"id": 780, "seek": 486564, "start": 4877.200000000001, "end": 4881.56, "text": " model ID is not at all it's way out to the left so that means that sale year and sale", "tokens": [2316, 7348, 307, 406, 412, 439, 309, 311, 636, 484, 281, 264, 1411, 370, 300, 1355, 300, 8680, 1064, 293, 8680], "temperature": 0.0, "avg_logprob": -0.09847077301570348, "compression_ratio": 1.8583690987124464, "no_speech_prob": 3.288737616458093e-06}, {"id": 781, "seek": 486564, "start": 4881.56, "end": 4887.64, "text": " elapsed are very very similar when one is low the other tends to be low and vice versa", "tokens": [806, 2382, 292, 366, 588, 588, 2531, 562, 472, 307, 2295, 264, 661, 12258, 281, 312, 2295, 293, 11964, 25650], "temperature": 0.0, "avg_logprob": -0.09847077301570348, "compression_ratio": 1.8583690987124464, "no_speech_prob": 3.288737616458093e-06}, {"id": 782, "seek": 486564, "start": 4887.64, "end": 4893.4800000000005, "text": " here's a group of three which all seem to be much the same and then product group desk", "tokens": [510, 311, 257, 1594, 295, 1045, 597, 439, 1643, 281, 312, 709, 264, 912, 293, 550, 1674, 1594, 10026], "temperature": 0.0, "avg_logprob": -0.09847077301570348, "compression_ratio": 1.8583690987124464, "no_speech_prob": 3.288737616458093e-06}, {"id": 783, "seek": 489348, "start": 4893.48, "end": 4899.5199999999995, "text": " and product group and then fi best base model and fi model desk but these all seem like", "tokens": [293, 1674, 1594, 293, 550, 15848, 1151, 3096, 2316, 293, 15848, 2316, 10026, 457, 613, 439, 1643, 411], "temperature": 0.0, "avg_logprob": -0.1803923422290433, "compression_ratio": 1.6540880503144655, "no_speech_prob": 7.571118771920737e-07}, {"id": 784, "seek": 489348, "start": 4899.5199999999995, "end": 4905.919999999999, "text": " things where maybe we could remove one of each of these pairs because they're basically", "tokens": [721, 689, 1310, 321, 727, 4159, 472, 295, 1184, 295, 613, 15494, 570, 436, 434, 1936], "temperature": 0.0, "avg_logprob": -0.1803923422290433, "compression_ratio": 1.6540880503144655, "no_speech_prob": 7.571118771920737e-07}, {"id": 785, "seek": 489348, "start": 4905.919999999999, "end": 4911.4, "text": " seem to be much the same you know they're you know they're when one is higher the other", "tokens": [1643, 281, 312, 709, 264, 912, 291, 458, 436, 434, 291, 458, 436, 434, 562, 472, 307, 2946, 264, 661], "temperature": 0.0, "avg_logprob": -0.1803923422290433, "compression_ratio": 1.6540880503144655, "no_speech_prob": 7.571118771920737e-07}, {"id": 786, "seek": 491140, "start": 4911.4, "end": 4925.799999999999, "text": " is high and vice versa so let's try removing one of each of these now it takes a little", "tokens": [307, 1090, 293, 11964, 25650, 370, 718, 311, 853, 12720, 472, 295, 1184, 295, 613, 586, 309, 2516, 257, 707], "temperature": 0.0, "avg_logprob": -0.07177292916082567, "compression_ratio": 1.6687898089171975, "no_speech_prob": 1.2679244036917225e-06}, {"id": 787, "seek": 491140, "start": 4925.799999999999, "end": 4932.2, "text": " while to train a random forest and so for the just to see whether removing something", "tokens": [1339, 281, 3847, 257, 4974, 6719, 293, 370, 337, 264, 445, 281, 536, 1968, 12720, 746], "temperature": 0.0, "avg_logprob": -0.07177292916082567, "compression_ratio": 1.6687898089171975, "no_speech_prob": 1.2679244036917225e-06}, {"id": 788, "seek": 491140, "start": 4932.2, "end": 4937.82, "text": " makes it much worse we could just do a very fast version so we could just train something", "tokens": [1669, 309, 709, 5324, 321, 727, 445, 360, 257, 588, 2370, 3037, 370, 321, 727, 445, 3847, 746], "temperature": 0.0, "avg_logprob": -0.07177292916082567, "compression_ratio": 1.6687898089171975, "no_speech_prob": 1.2679244036917225e-06}, {"id": 789, "seek": 493782, "start": 4937.82, "end": 4950.799999999999, "text": " where we only have 50,000 rows per tree train for each tree and we'll just use 40 trees", "tokens": [689, 321, 787, 362, 2625, 11, 1360, 13241, 680, 4230, 3847, 337, 1184, 4230, 293, 321, 603, 445, 764, 3356, 5852], "temperature": 0.0, "avg_logprob": -0.14463109016418457, "compression_ratio": 1.3410852713178294, "no_speech_prob": 1.1015922609658446e-06}, {"id": 790, "seek": 493782, "start": 4950.799999999999, "end": 4960.24, "text": " and let's then just get the OOB for and so for that fast simple version our basic OOB", "tokens": [293, 718, 311, 550, 445, 483, 264, 422, 46, 33, 337, 293, 370, 337, 300, 2370, 2199, 3037, 527, 3875, 422, 46, 33], "temperature": 0.0, "avg_logprob": -0.14463109016418457, "compression_ratio": 1.3410852713178294, "no_speech_prob": 1.1015922609658446e-06}, {"id": 791, "seek": 496024, "start": 4960.24, "end": 4970.76, "text": " with our important X's is.877 and here for OOB a higher number is better so then let's", "tokens": [365, 527, 1021, 1783, 311, 307, 2411, 23, 17512, 293, 510, 337, 422, 46, 33, 257, 2946, 1230, 307, 1101, 370, 550, 718, 311], "temperature": 0.0, "avg_logprob": -0.11576268332345145, "compression_ratio": 1.5497076023391814, "no_speech_prob": 5.714995268135681e-07}, {"id": 792, "seek": 496024, "start": 4970.76, "end": 4976.639999999999, "text": " try going through each of the things we thought we might not need and try dropping them and", "tokens": [853, 516, 807, 1184, 295, 264, 721, 321, 1194, 321, 1062, 406, 643, 293, 853, 13601, 552, 293], "temperature": 0.0, "avg_logprob": -0.11576268332345145, "compression_ratio": 1.5497076023391814, "no_speech_prob": 5.714995268135681e-07}, {"id": 793, "seek": 496024, "start": 4976.639999999999, "end": 4983.44, "text": " then getting the OOB error for our X's with that one column removed and so compared to", "tokens": [550, 1242, 264, 422, 46, 33, 6713, 337, 527, 1783, 311, 365, 300, 472, 7738, 7261, 293, 370, 5347, 281], "temperature": 0.0, "avg_logprob": -0.11576268332345145, "compression_ratio": 1.5497076023391814, "no_speech_prob": 5.714995268135681e-07}, {"id": 794, "seek": 498344, "start": 4983.44, "end": 4992.719999999999, "text": ".877 most of them don't seem to hurt very much they'll elapse hurt quite a bit right", "tokens": [2411, 23, 17512, 881, 295, 552, 500, 380, 1643, 281, 4607, 588, 709, 436, 603, 806, 11145, 4607, 1596, 257, 857, 558], "temperature": 0.0, "avg_logprob": -0.11137332086977751, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.2289153801248176e-06}, {"id": 795, "seek": 498344, "start": 4992.719999999999, "end": 4999.44, "text": " so for each of those groups let's go and see which one of the ones seems like we could", "tokens": [370, 337, 1184, 295, 729, 3935, 718, 311, 352, 293, 536, 597, 472, 295, 264, 2306, 2544, 411, 321, 727], "temperature": 0.0, "avg_logprob": -0.11137332086977751, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.2289153801248176e-06}, {"id": 796, "seek": 498344, "start": 4999.44, "end": 5007.48, "text": " remove it so here's the five I found let's remove the whole lot and see what happens", "tokens": [4159, 309, 370, 510, 311, 264, 1732, 286, 1352, 718, 311, 4159, 264, 1379, 688, 293, 536, 437, 2314], "temperature": 0.0, "avg_logprob": -0.11137332086977751, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.2289153801248176e-06}, {"id": 797, "seek": 500748, "start": 5007.48, "end": 5016.5199999999995, "text": " and so the OOB went from 877 to 874 so hardly any difference at all despite the fact we", "tokens": [293, 370, 264, 422, 46, 33, 1437, 490, 1649, 17512, 281, 27990, 19, 370, 13572, 604, 2649, 412, 439, 7228, 264, 1186, 321], "temperature": 0.0, "avg_logprob": -0.09508540737095164, "compression_ratio": 1.4505494505494505, "no_speech_prob": 3.5209075122111244e-07}, {"id": 798, "seek": 500748, "start": 5016.5199999999995, "end": 5024.62, "text": " managed to get rid of five of our variables so let's create something called X's final", "tokens": [6453, 281, 483, 3973, 295, 1732, 295, 527, 9102, 370, 718, 311, 1884, 746, 1219, 1783, 311, 2572], "temperature": 0.0, "avg_logprob": -0.09508540737095164, "compression_ratio": 1.4505494505494505, "no_speech_prob": 3.5209075122111244e-07}, {"id": 799, "seek": 500748, "start": 5024.62, "end": 5032.679999999999, "text": " which is the X's important and then dropping those five save them for later we can always", "tokens": [597, 307, 264, 1783, 311, 1021, 293, 550, 13601, 729, 1732, 3155, 552, 337, 1780, 321, 393, 1009], "temperature": 0.0, "avg_logprob": -0.09508540737095164, "compression_ratio": 1.4505494505494505, "no_speech_prob": 3.5209075122111244e-07}, {"id": 800, "seek": 503268, "start": 5032.68, "end": 5039.360000000001, "text": " load them back again and then let's check our random forest using those and again.233", "tokens": [3677, 552, 646, 797, 293, 550, 718, 311, 1520, 527, 4974, 6719, 1228, 729, 293, 797, 2411, 9356, 18], "temperature": 0.0, "avg_logprob": -0.12360161052030676, "compression_ratio": 1.6862745098039216, "no_speech_prob": 6.083578796278744e-07}, {"id": 801, "seek": 503268, "start": 5039.360000000001, "end": 5047.8, "text": " or.234 so we've got about the same thing but we've got even less columns now so we're", "tokens": [420, 2411, 9356, 19, 370, 321, 600, 658, 466, 264, 912, 551, 457, 321, 600, 658, 754, 1570, 13766, 586, 370, 321, 434], "temperature": 0.0, "avg_logprob": -0.12360161052030676, "compression_ratio": 1.6862745098039216, "no_speech_prob": 6.083578796278744e-07}, {"id": 802, "seek": 503268, "start": 5047.8, "end": 5055.16, "text": " getting a kind of a simpler and simpler model without hurting our accuracy it's great so", "tokens": [1242, 257, 733, 295, 257, 18587, 293, 18587, 2316, 1553, 17744, 527, 14170, 309, 311, 869, 370], "temperature": 0.0, "avg_logprob": -0.12360161052030676, "compression_ratio": 1.6862745098039216, "no_speech_prob": 6.083578796278744e-07}, {"id": 803, "seek": 503268, "start": 5055.16, "end": 5059.84, "text": " the next thing we said we were interested in learning about is for the columns that", "tokens": [264, 958, 551, 321, 848, 321, 645, 3102, 294, 2539, 466, 307, 337, 264, 13766, 300], "temperature": 0.0, "avg_logprob": -0.12360161052030676, "compression_ratio": 1.6862745098039216, "no_speech_prob": 6.083578796278744e-07}, {"id": 804, "seek": 505984, "start": 5059.84, "end": 5065.84, "text": " are particularly the columns that are most important how does what's the relationship", "tokens": [366, 4098, 264, 13766, 300, 366, 881, 1021, 577, 775, 437, 311, 264, 2480], "temperature": 0.0, "avg_logprob": -0.09050116961515403, "compression_ratio": 1.7177033492822966, "no_speech_prob": 2.726458887991612e-06}, {"id": 805, "seek": 505984, "start": 5065.84, "end": 5069.72, "text": " between that column and the dependent variable so example what's the relationship between", "tokens": [1296, 300, 7738, 293, 264, 12334, 7006, 370, 1365, 437, 311, 264, 2480, 1296], "temperature": 0.0, "avg_logprob": -0.09050116961515403, "compression_ratio": 1.7177033492822966, "no_speech_prob": 2.726458887991612e-06}, {"id": 806, "seek": 505984, "start": 5069.72, "end": 5078.64, "text": " product size and sale price so the first thing I would do would be just to look at a histogram", "tokens": [1674, 2744, 293, 8680, 3218, 370, 264, 700, 551, 286, 576, 360, 576, 312, 445, 281, 574, 412, 257, 49816], "temperature": 0.0, "avg_logprob": -0.09050116961515403, "compression_ratio": 1.7177033492822966, "no_speech_prob": 2.726458887991612e-06}, {"id": 807, "seek": 505984, "start": 5078.64, "end": 5089.6, "text": " so one way to do that is with a value counts in pandas and we can see here our different", "tokens": [370, 472, 636, 281, 360, 300, 307, 365, 257, 2158, 14893, 294, 4565, 296, 293, 321, 393, 536, 510, 527, 819], "temperature": 0.0, "avg_logprob": -0.09050116961515403, "compression_ratio": 1.7177033492822966, "no_speech_prob": 2.726458887991612e-06}, {"id": 808, "seek": 508960, "start": 5089.6, "end": 5096.4400000000005, "text": " calls of product size and one thing to note here is actually missing is actually the most", "tokens": [5498, 295, 1674, 2744, 293, 472, 551, 281, 3637, 510, 307, 767, 5361, 307, 767, 264, 881], "temperature": 0.0, "avg_logprob": -0.19623260498046874, "compression_ratio": 1.8677248677248677, "no_speech_prob": 7.18323144610622e-06}, {"id": 809, "seek": 508960, "start": 5096.4400000000005, "end": 5104.72, "text": " common and then next most is compact and small and then many is pretty tiny so we can do", "tokens": [2689, 293, 550, 958, 881, 307, 14679, 293, 1359, 293, 550, 867, 307, 1238, 5870, 370, 321, 393, 360], "temperature": 0.0, "avg_logprob": -0.19623260498046874, "compression_ratio": 1.8677248677248677, "no_speech_prob": 7.18323144610622e-06}, {"id": 810, "seek": 508960, "start": 5104.72, "end": 5112.04, "text": " the same thing for year made now for year made we can't just see the the basic bar chart", "tokens": [264, 912, 551, 337, 1064, 1027, 586, 337, 1064, 1027, 321, 393, 380, 445, 536, 264, 264, 3875, 2159, 6927], "temperature": 0.0, "avg_logprob": -0.19623260498046874, "compression_ratio": 1.8677248677248677, "no_speech_prob": 7.18323144610622e-06}, {"id": 811, "seek": 508960, "start": 5112.04, "end": 5116.92, "text": " like here we recorded a histogram it's not it's a bar chart for year made we actually", "tokens": [411, 510, 321, 8287, 257, 49816, 309, 311, 406, 309, 311, 257, 2159, 6927, 337, 1064, 1027, 321, 767], "temperature": 0.0, "avg_logprob": -0.19623260498046874, "compression_ratio": 1.8677248677248677, "no_speech_prob": 7.18323144610622e-06}, {"id": 812, "seek": 511692, "start": 5116.92, "end": 5123.4800000000005, "text": " need a histogram which pandas has stuff like this built in so we can just call histogram", "tokens": [643, 257, 49816, 597, 4565, 296, 575, 1507, 411, 341, 3094, 294, 370, 321, 393, 445, 818, 49816], "temperature": 0.0, "avg_logprob": -0.08709286036116354, "compression_ratio": 1.6069868995633187, "no_speech_prob": 6.577921567441081e-07}, {"id": 813, "seek": 511692, "start": 5123.4800000000005, "end": 5128.6, "text": " and that 1950 you remember we created it that's kind of this missing value thing which used", "tokens": [293, 300, 18141, 291, 1604, 321, 2942, 309, 300, 311, 733, 295, 341, 5361, 2158, 551, 597, 1143], "temperature": 0.0, "avg_logprob": -0.08709286036116354, "compression_ratio": 1.6069868995633187, "no_speech_prob": 6.577921567441081e-07}, {"id": 814, "seek": 511692, "start": 5128.6, "end": 5136.04, "text": " to be a thousand but most of them seem to have been well into the 90s and 2000s so let's", "tokens": [281, 312, 257, 4714, 457, 881, 295, 552, 1643, 281, 362, 668, 731, 666, 264, 4289, 82, 293, 8132, 82, 370, 718, 311], "temperature": 0.0, "avg_logprob": -0.08709286036116354, "compression_ratio": 1.6069868995633187, "no_speech_prob": 6.577921567441081e-07}, {"id": 815, "seek": 511692, "start": 5136.04, "end": 5144.04, "text": " now look at something called a partial dependence plot I'll show it to you first here is a partial", "tokens": [586, 574, 412, 746, 1219, 257, 14641, 31704, 7542, 286, 603, 855, 309, 281, 291, 700, 510, 307, 257, 14641], "temperature": 0.0, "avg_logprob": -0.08709286036116354, "compression_ratio": 1.6069868995633187, "no_speech_prob": 6.577921567441081e-07}, {"id": 816, "seek": 514404, "start": 5144.04, "end": 5156.92, "text": " dependence plot of year made against partial dependence and what does this mean well we", "tokens": [31704, 7542, 295, 1064, 1027, 1970, 14641, 31704, 293, 437, 775, 341, 914, 731, 321], "temperature": 0.0, "avg_logprob": -0.1019446849822998, "compression_ratio": 1.5853658536585367, "no_speech_prob": 2.9189365591264504e-07}, {"id": 817, "seek": 514404, "start": 5156.92, "end": 5161.56, "text": " should focus on the part where we actually have a reasonable end of data so at least", "tokens": [820, 1879, 322, 264, 644, 689, 321, 767, 362, 257, 10585, 917, 295, 1412, 370, 412, 1935], "temperature": 0.0, "avg_logprob": -0.1019446849822998, "compression_ratio": 1.5853658536585367, "no_speech_prob": 2.9189365591264504e-07}, {"id": 818, "seek": 514404, "start": 5161.56, "end": 5168.42, "text": " well into the 80s go around here and so let's look at this bit here basically what this", "tokens": [731, 666, 264, 4688, 82, 352, 926, 510, 293, 370, 718, 311, 574, 412, 341, 857, 510, 1936, 437, 341], "temperature": 0.0, "avg_logprob": -0.1019446849822998, "compression_ratio": 1.5853658536585367, "no_speech_prob": 2.9189365591264504e-07}, {"id": 819, "seek": 516842, "start": 5168.42, "end": 5181.84, "text": " says is that as year made increases the predicted sale price log sale price of course also increases", "tokens": [1619, 307, 300, 382, 1064, 1027, 8637, 264, 19147, 8680, 3218, 3565, 8680, 3218, 295, 1164, 611, 8637], "temperature": 0.0, "avg_logprob": -0.1503648593507964, "compression_ratio": 1.8, "no_speech_prob": 8.851558277456206e-07}, {"id": 820, "seek": 516842, "start": 5181.84, "end": 5188.12, "text": " you can see and the log sale price is increasing linearly on other roughly so roughly then", "tokens": [291, 393, 536, 293, 264, 3565, 8680, 3218, 307, 5662, 43586, 322, 661, 9810, 370, 9810, 550], "temperature": 0.0, "avg_logprob": -0.1503648593507964, "compression_ratio": 1.8, "no_speech_prob": 8.851558277456206e-07}, {"id": 821, "seek": 516842, "start": 5188.12, "end": 5196.16, "text": " this is actually an exponential relationship between year made and sale price why do we", "tokens": [341, 307, 767, 364, 21510, 2480, 1296, 1064, 1027, 293, 8680, 3218, 983, 360, 321], "temperature": 0.0, "avg_logprob": -0.1503648593507964, "compression_ratio": 1.8, "no_speech_prob": 8.851558277456206e-07}, {"id": 822, "seek": 519616, "start": 5196.16, "end": 5201.12, "text": " call it a partial dependence are we just plotting the kind of the year against the average sale", "tokens": [818, 309, 257, 14641, 31704, 366, 321, 445, 41178, 264, 733, 295, 264, 1064, 1970, 264, 4274, 8680], "temperature": 0.0, "avg_logprob": -0.1060657168543616, "compression_ratio": 1.6787330316742082, "no_speech_prob": 6.083579364712932e-07}, {"id": 823, "seek": 519616, "start": 5201.12, "end": 5207.599999999999, "text": " price well no we're not we can't do that because a lot of other things change from year to", "tokens": [3218, 731, 572, 321, 434, 406, 321, 393, 380, 360, 300, 570, 257, 688, 295, 661, 721, 1319, 490, 1064, 281], "temperature": 0.0, "avg_logprob": -0.1060657168543616, "compression_ratio": 1.6787330316742082, "no_speech_prob": 6.083579364712932e-07}, {"id": 824, "seek": 519616, "start": 5207.599999999999, "end": 5217.04, "text": " year example maybe more recently people tend to buy bigger bulldozers or bold more bulldozers", "tokens": [1064, 1365, 1310, 544, 3938, 561, 3928, 281, 2256, 3801, 4693, 2595, 41698, 420, 11928, 544, 4693, 2595, 41698], "temperature": 0.0, "avg_logprob": -0.1060657168543616, "compression_ratio": 1.6787330316742082, "no_speech_prob": 6.083579364712932e-07}, {"id": 825, "seek": 519616, "start": 5217.04, "end": 5223.76, "text": " with air conditioning or more expensive models of bulldozers and we really want to be able", "tokens": [365, 1988, 21901, 420, 544, 5124, 5245, 295, 4693, 2595, 41698, 293, 321, 534, 528, 281, 312, 1075], "temperature": 0.0, "avg_logprob": -0.1060657168543616, "compression_ratio": 1.6787330316742082, "no_speech_prob": 6.083579364712932e-07}, {"id": 826, "seek": 522376, "start": 5223.76, "end": 5228.88, "text": " to say like no just what's the impact of year and nothing else and if you think about it", "tokens": [281, 584, 411, 572, 445, 437, 311, 264, 2712, 295, 1064, 293, 1825, 1646, 293, 498, 291, 519, 466, 309], "temperature": 0.0, "avg_logprob": -0.08348396454734364, "compression_ratio": 1.7389162561576355, "no_speech_prob": 1.6536843077119556e-06}, {"id": 827, "seek": 522376, "start": 5228.88, "end": 5238.320000000001, "text": " from a kind of a inflation point of view you would expect that older bulldozers would be", "tokens": [490, 257, 733, 295, 257, 15860, 935, 295, 1910, 291, 576, 2066, 300, 4906, 4693, 2595, 41698, 576, 312], "temperature": 0.0, "avg_logprob": -0.08348396454734364, "compression_ratio": 1.7389162561576355, "no_speech_prob": 1.6536843077119556e-06}, {"id": 828, "seek": 522376, "start": 5238.320000000001, "end": 5247.400000000001, "text": " kind of that bulldozers would get a kind of a constant ratio cheaper the further you go", "tokens": [733, 295, 300, 4693, 2595, 41698, 576, 483, 257, 733, 295, 257, 5754, 8509, 12284, 264, 3052, 291, 352], "temperature": 0.0, "avg_logprob": -0.08348396454734364, "compression_ratio": 1.7389162561576355, "no_speech_prob": 1.6536843077119556e-06}, {"id": 829, "seek": 522376, "start": 5247.400000000001, "end": 5253.0, "text": " back which is what we see so what we really want to say is all other things being equal", "tokens": [646, 597, 307, 437, 321, 536, 370, 437, 321, 534, 528, 281, 584, 307, 439, 661, 721, 885, 2681], "temperature": 0.0, "avg_logprob": -0.08348396454734364, "compression_ratio": 1.7389162561576355, "no_speech_prob": 1.6536843077119556e-06}, {"id": 830, "seek": 525300, "start": 5253.0, "end": 5259.92, "text": " what happens if only the year changes and there's a really cool way we can answer that", "tokens": [437, 2314, 498, 787, 264, 1064, 2962, 293, 456, 311, 257, 534, 1627, 636, 321, 393, 1867, 300], "temperature": 0.0, "avg_logprob": -0.08403872220944135, "compression_ratio": 1.6333333333333333, "no_speech_prob": 2.6016070933110313e-06}, {"id": 831, "seek": 525300, "start": 5259.92, "end": 5266.1, "text": " question with a random forest so how does year made impact sale price all other things", "tokens": [1168, 365, 257, 4974, 6719, 370, 577, 775, 1064, 1027, 2712, 8680, 3218, 439, 661, 721], "temperature": 0.0, "avg_logprob": -0.08403872220944135, "compression_ratio": 1.6333333333333333, "no_speech_prob": 2.6016070933110313e-06}, {"id": 832, "seek": 525300, "start": 5266.1, "end": 5272.54, "text": " being equal so what we can do is we can go into our actual data set and replace every", "tokens": [885, 2681, 370, 437, 321, 393, 360, 307, 321, 393, 352, 666, 527, 3539, 1412, 992, 293, 7406, 633], "temperature": 0.0, "avg_logprob": -0.08403872220944135, "compression_ratio": 1.6333333333333333, "no_speech_prob": 2.6016070933110313e-06}, {"id": 833, "seek": 525300, "start": 5272.54, "end": 5278.44, "text": " single value in the year made column with 1950 and then can calculate the predicted", "tokens": [2167, 2158, 294, 264, 1064, 1027, 7738, 365, 18141, 293, 550, 393, 8873, 264, 19147], "temperature": 0.0, "avg_logprob": -0.08403872220944135, "compression_ratio": 1.6333333333333333, "no_speech_prob": 2.6016070933110313e-06}, {"id": 834, "seek": 527844, "start": 5278.44, "end": 5283.599999999999, "text": " sale price for every single auction and then take the average over all the auctions and", "tokens": [8680, 3218, 337, 633, 2167, 24139, 293, 550, 747, 264, 4274, 670, 439, 264, 1609, 3916, 293], "temperature": 0.0, "avg_logprob": -0.08893619161663634, "compression_ratio": 1.5647058823529412, "no_speech_prob": 1.7330478385702008e-06}, {"id": 835, "seek": 527844, "start": 5283.599999999999, "end": 5290.919999999999, "text": " that's what gives us this value here and then we can do the same from 1951 1952 and so forth", "tokens": [300, 311, 437, 2709, 505, 341, 2158, 510, 293, 550, 321, 393, 360, 264, 912, 490, 10858, 16, 10858, 17, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.08893619161663634, "compression_ratio": 1.5647058823529412, "no_speech_prob": 1.7330478385702008e-06}, {"id": 836, "seek": 527844, "start": 5290.919999999999, "end": 5300.04, "text": " until eventually we get to our final year of 2011 so this isolates the effect of only", "tokens": [1826, 4728, 321, 483, 281, 527, 2572, 1064, 295, 10154, 370, 341, 7381, 1024, 264, 1802, 295, 787], "temperature": 0.0, "avg_logprob": -0.08893619161663634, "compression_ratio": 1.5647058823529412, "no_speech_prob": 1.7330478385702008e-06}, {"id": 837, "seek": 530004, "start": 5300.04, "end": 5308.62, "text": " year made so it's a kind of a bit of a curious thing to do but it's actually it's a pretty", "tokens": [1064, 1027, 370, 309, 311, 257, 733, 295, 257, 857, 295, 257, 6369, 551, 281, 360, 457, 309, 311, 767, 309, 311, 257, 1238], "temperature": 0.0, "avg_logprob": -0.0947409772324836, "compression_ratio": 1.7868020304568528, "no_speech_prob": 1.2878906545665814e-06}, {"id": 838, "seek": 530004, "start": 5308.62, "end": 5315.08, "text": " neat trick for trying to kind of pull apart and create this partial dependence to say", "tokens": [10654, 4282, 337, 1382, 281, 733, 295, 2235, 4936, 293, 1884, 341, 14641, 31704, 281, 584], "temperature": 0.0, "avg_logprob": -0.0947409772324836, "compression_ratio": 1.7868020304568528, "no_speech_prob": 1.2878906545665814e-06}, {"id": 839, "seek": 530004, "start": 5315.08, "end": 5322.12, "text": " what might be the impact of just changing year made and we can do the same thing for", "tokens": [437, 1062, 312, 264, 2712, 295, 445, 4473, 1064, 1027, 293, 321, 393, 360, 264, 912, 551, 337], "temperature": 0.0, "avg_logprob": -0.0947409772324836, "compression_ratio": 1.7868020304568528, "no_speech_prob": 1.2878906545665814e-06}, {"id": 840, "seek": 530004, "start": 5322.12, "end": 5326.64, "text": " product size and one of the interesting things if we do it for product size is we see that", "tokens": [1674, 2744, 293, 472, 295, 264, 1880, 721, 498, 321, 360, 309, 337, 1674, 2744, 307, 321, 536, 300], "temperature": 0.0, "avg_logprob": -0.0947409772324836, "compression_ratio": 1.7868020304568528, "no_speech_prob": 1.2878906545665814e-06}, {"id": 841, "seek": 532664, "start": 5326.64, "end": 5338.72, "text": " the lowest value of predicted sale price log sale price is NA which is a bit of a worry", "tokens": [264, 12437, 2158, 295, 19147, 8680, 3218, 3565, 8680, 3218, 307, 16585, 597, 307, 257, 857, 295, 257, 3292], "temperature": 0.0, "avg_logprob": -0.08179596324025849, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.9033785747524234e-06}, {"id": 842, "seek": 532664, "start": 5338.72, "end": 5342.280000000001, "text": " because we kind of want to know well that means it's really important the question of", "tokens": [570, 321, 733, 295, 528, 281, 458, 731, 300, 1355, 309, 311, 534, 1021, 264, 1168, 295], "temperature": 0.0, "avg_logprob": -0.08179596324025849, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.9033785747524234e-06}, {"id": 843, "seek": 532664, "start": 5342.280000000001, "end": 5348.200000000001, "text": " whether or not the product size is labeled is really important and that is something", "tokens": [1968, 420, 406, 264, 1674, 2744, 307, 21335, 307, 534, 1021, 293, 300, 307, 746], "temperature": 0.0, "avg_logprob": -0.08179596324025849, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.9033785747524234e-06}, {"id": 844, "seek": 532664, "start": 5348.200000000001, "end": 5352.72, "text": " that I would want to dig into before I actually use this model to find out well why is it", "tokens": [300, 286, 576, 528, 281, 2528, 666, 949, 286, 767, 764, 341, 2316, 281, 915, 484, 731, 983, 307, 309], "temperature": 0.0, "avg_logprob": -0.08179596324025849, "compression_ratio": 1.681159420289855, "no_speech_prob": 1.9033785747524234e-06}, {"id": 845, "seek": 535272, "start": 5352.72, "end": 5356.68, "text": " that sometimes things aren't labeled and what does it mean you know why is it that that's", "tokens": [300, 2171, 721, 3212, 380, 21335, 293, 437, 775, 309, 914, 291, 458, 983, 307, 309, 300, 300, 311], "temperature": 0.0, "avg_logprob": -0.12329796382359096, "compression_ratio": 1.674641148325359, "no_speech_prob": 1.9033774378840462e-06}, {"id": 846, "seek": 535272, "start": 5356.68, "end": 5363.76, "text": " actually a such as important predictor so that is the partial dependence plot and it's", "tokens": [767, 257, 1270, 382, 1021, 6069, 284, 370, 300, 307, 264, 14641, 31704, 7542, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.12329796382359096, "compression_ratio": 1.674641148325359, "no_speech_prob": 1.9033774378840462e-06}, {"id": 847, "seek": 535272, "start": 5363.76, "end": 5374.12, "text": " a really clever trick. So we have looked at four of the five questions we said we wanted", "tokens": [257, 534, 13494, 4282, 13, 407, 321, 362, 2956, 412, 1451, 295, 264, 1732, 1651, 321, 848, 321, 1415], "temperature": 0.0, "avg_logprob": -0.12329796382359096, "compression_ratio": 1.674641148325359, "no_speech_prob": 1.9033774378840462e-06}, {"id": 848, "seek": 535272, "start": 5374.12, "end": 5381.84, "text": " to answer at the start of this section so the last one that we want to answer is one", "tokens": [281, 1867, 412, 264, 722, 295, 341, 3541, 370, 264, 1036, 472, 300, 321, 528, 281, 1867, 307, 472], "temperature": 0.0, "avg_logprob": -0.12329796382359096, "compression_ratio": 1.674641148325359, "no_speech_prob": 1.9033774378840462e-06}, {"id": 849, "seek": 538184, "start": 5381.84, "end": 5387.0, "text": " here we're predicting with a particular row of data what were the most important factors", "tokens": [510, 321, 434, 32884, 365, 257, 1729, 5386, 295, 1412, 437, 645, 264, 881, 1021, 6771], "temperature": 0.0, "avg_logprob": -0.08548513392812197, "compression_ratio": 1.778688524590164, "no_speech_prob": 7.76690194470575e-06}, {"id": 850, "seek": 538184, "start": 5387.0, "end": 5391.4800000000005, "text": " and how did they influence that predictor this is quite related to the very first thing", "tokens": [293, 577, 630, 436, 6503, 300, 6069, 284, 341, 307, 1596, 4077, 281, 264, 588, 700, 551], "temperature": 0.0, "avg_logprob": -0.08548513392812197, "compression_ratio": 1.778688524590164, "no_speech_prob": 7.76690194470575e-06}, {"id": 851, "seek": 538184, "start": 5391.4800000000005, "end": 5397.400000000001, "text": " we saw so it's like imagine you were using this auction price model in real life you", "tokens": [321, 1866, 370, 309, 311, 411, 3811, 291, 645, 1228, 341, 24139, 3218, 2316, 294, 957, 993, 291], "temperature": 0.0, "avg_logprob": -0.08548513392812197, "compression_ratio": 1.778688524590164, "no_speech_prob": 7.76690194470575e-06}, {"id": 852, "seek": 538184, "start": 5397.400000000001, "end": 5402.360000000001, "text": " had something on your tablet and you went into some auction and you looked up what the", "tokens": [632, 746, 322, 428, 14136, 293, 291, 1437, 666, 512, 24139, 293, 291, 2956, 493, 437, 264], "temperature": 0.0, "avg_logprob": -0.08548513392812197, "compression_ratio": 1.778688524590164, "no_speech_prob": 7.76690194470575e-06}, {"id": 853, "seek": 538184, "start": 5402.360000000001, "end": 5409.96, "text": " predicted auction price would be for this lot that's coming up to find out whether it", "tokens": [19147, 24139, 3218, 576, 312, 337, 341, 688, 300, 311, 1348, 493, 281, 915, 484, 1968, 309], "temperature": 0.0, "avg_logprob": -0.08548513392812197, "compression_ratio": 1.778688524590164, "no_speech_prob": 7.76690194470575e-06}, {"id": 854, "seek": 540996, "start": 5409.96, "end": 5415.76, "text": " seems like it's being under or overvalued and then you can decide what to do about that.", "tokens": [2544, 411, 309, 311, 885, 833, 420, 670, 3337, 5827, 293, 550, 291, 393, 4536, 437, 281, 360, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.10650957462399505, "compression_ratio": 1.710144927536232, "no_speech_prob": 1.9033758462683181e-06}, {"id": 855, "seek": 540996, "start": 5415.76, "end": 5420.04, "text": " So one thing we said we'd be interested to know is like well are we actually confident", "tokens": [407, 472, 551, 321, 848, 321, 1116, 312, 3102, 281, 458, 307, 411, 731, 366, 321, 767, 6679], "temperature": 0.0, "avg_logprob": -0.10650957462399505, "compression_ratio": 1.710144927536232, "no_speech_prob": 1.9033758462683181e-06}, {"id": 856, "seek": 540996, "start": 5420.04, "end": 5425.24, "text": " in our prediction and then we might be curious to find out like oh I'm really surprised it", "tokens": [294, 527, 17630, 293, 550, 321, 1062, 312, 6369, 281, 915, 484, 411, 1954, 286, 478, 534, 6100, 309], "temperature": 0.0, "avg_logprob": -0.10650957462399505, "compression_ratio": 1.710144927536232, "no_speech_prob": 1.9033758462683181e-06}, {"id": 857, "seek": 540996, "start": 5425.24, "end": 5432.14, "text": " was predicting such a high value why was it predicting such a high value so to find the", "tokens": [390, 32884, 1270, 257, 1090, 2158, 983, 390, 309, 32884, 1270, 257, 1090, 2158, 370, 281, 915, 264], "temperature": 0.0, "avg_logprob": -0.10650957462399505, "compression_ratio": 1.710144927536232, "no_speech_prob": 1.9033758462683181e-06}, {"id": 858, "seek": 543214, "start": 5432.14, "end": 5441.280000000001, "text": " answer to that question we can use a module called tree interpreter and tree interpreter", "tokens": [1867, 281, 300, 1168, 321, 393, 764, 257, 10088, 1219, 4230, 34132, 293, 4230, 34132], "temperature": 0.0, "avg_logprob": -0.11530800299210982, "compression_ratio": 1.6832298136645962, "no_speech_prob": 2.8130016289651394e-06}, {"id": 859, "seek": 543214, "start": 5441.280000000001, "end": 5447.68, "text": " the way it works is that you pass in a single row so it's like here's the auction that's", "tokens": [264, 636, 309, 1985, 307, 300, 291, 1320, 294, 257, 2167, 5386, 370, 309, 311, 411, 510, 311, 264, 24139, 300, 311], "temperature": 0.0, "avg_logprob": -0.11530800299210982, "compression_ratio": 1.6832298136645962, "no_speech_prob": 2.8130016289651394e-06}, {"id": 860, "seek": 543214, "start": 5447.68, "end": 5458.160000000001, "text": " coming up here's the model here's the auctioneer ID etc etc please predict the value from the", "tokens": [1348, 493, 510, 311, 264, 2316, 510, 311, 264, 24139, 68, 260, 7348, 5183, 5183, 1767, 6069, 264, 2158, 490, 264], "temperature": 0.0, "avg_logprob": -0.11530800299210982, "compression_ratio": 1.6832298136645962, "no_speech_prob": 2.8130016289651394e-06}, {"id": 861, "seek": 545816, "start": 5458.16, "end": 5463.08, "text": " random forest what's the expected sale price and then what we can do is we can take that", "tokens": [4974, 6719, 437, 311, 264, 5176, 8680, 3218, 293, 550, 437, 321, 393, 360, 307, 321, 393, 747, 300], "temperature": 0.0, "avg_logprob": -0.07930775984977055, "compression_ratio": 2.018181818181818, "no_speech_prob": 3.3931216876226244e-06}, {"id": 862, "seek": 545816, "start": 5463.08, "end": 5468.12, "text": " one row of data and put it through the first decision tree and we can see what's the first", "tokens": [472, 5386, 295, 1412, 293, 829, 309, 807, 264, 700, 3537, 4230, 293, 321, 393, 536, 437, 311, 264, 700], "temperature": 0.0, "avg_logprob": -0.07930775984977055, "compression_ratio": 2.018181818181818, "no_speech_prob": 3.3931216876226244e-06}, {"id": 863, "seek": 545816, "start": 5468.12, "end": 5474.28, "text": " split that's selected and then based on that split does it end up increasing or decreasing", "tokens": [7472, 300, 311, 8209, 293, 550, 2361, 322, 300, 7472, 775, 309, 917, 493, 5662, 420, 23223], "temperature": 0.0, "avg_logprob": -0.07930775984977055, "compression_ratio": 2.018181818181818, "no_speech_prob": 3.3931216876226244e-06}, {"id": 864, "seek": 545816, "start": 5474.28, "end": 5480.44, "text": " the predicted price compared to that kind of raw baseline model of just take the average", "tokens": [264, 19147, 3218, 5347, 281, 300, 733, 295, 8936, 20518, 2316, 295, 445, 747, 264, 4274], "temperature": 0.0, "avg_logprob": -0.07930775984977055, "compression_ratio": 2.018181818181818, "no_speech_prob": 3.3931216876226244e-06}, {"id": 865, "seek": 545816, "start": 5480.44, "end": 5483.5199999999995, "text": " and then you can do that again at the next split and again at the next split and the", "tokens": [293, 550, 291, 393, 360, 300, 797, 412, 264, 958, 7472, 293, 797, 412, 264, 958, 7472, 293, 264], "temperature": 0.0, "avg_logprob": -0.07930775984977055, "compression_ratio": 2.018181818181818, "no_speech_prob": 3.3931216876226244e-06}, {"id": 866, "seek": 548352, "start": 5483.52, "end": 5491.76, "text": " next split so for each split we see what the increase or decrease in the addiction that's", "tokens": [958, 7472, 370, 337, 1184, 7472, 321, 536, 437, 264, 3488, 420, 11514, 294, 264, 16835, 300, 311], "temperature": 0.0, "avg_logprob": -0.1294171923682803, "compression_ratio": 1.8472222222222223, "no_speech_prob": 4.888291300630954e-07}, {"id": 867, "seek": 548352, "start": 5491.76, "end": 5506.120000000001, "text": " not right we see what the increase or decrease in the prediction is except while I'm here", "tokens": [406, 558, 321, 536, 437, 264, 3488, 420, 11514, 294, 264, 17630, 307, 3993, 1339, 286, 478, 510], "temperature": 0.0, "avg_logprob": -0.1294171923682803, "compression_ratio": 1.8472222222222223, "no_speech_prob": 4.888291300630954e-07}, {"id": 868, "seek": 548352, "start": 5506.120000000001, "end": 5510.56, "text": " compared to the parent node and so then you can do that for every tree and then add up", "tokens": [5347, 281, 264, 2596, 9984, 293, 370, 550, 291, 393, 360, 300, 337, 633, 4230, 293, 550, 909, 493], "temperature": 0.0, "avg_logprob": -0.1294171923682803, "compression_ratio": 1.8472222222222223, "no_speech_prob": 4.888291300630954e-07}, {"id": 869, "seek": 551056, "start": 5510.56, "end": 5518.400000000001, "text": " the total change in importance by split variable and that allows you to draw something like", "tokens": [264, 3217, 1319, 294, 7379, 538, 7472, 7006, 293, 300, 4045, 291, 281, 2642, 746, 411], "temperature": 0.0, "avg_logprob": -0.1314777284860611, "compression_ratio": 1.569767441860465, "no_speech_prob": 1.1911058663827134e-06}, {"id": 870, "seek": 551056, "start": 5518.400000000001, "end": 5526.400000000001, "text": " this so here's something that's looking at one particular row of data and overall we", "tokens": [341, 370, 510, 311, 746, 300, 311, 1237, 412, 472, 1729, 5386, 295, 1412, 293, 4787, 321], "temperature": 0.0, "avg_logprob": -0.1314777284860611, "compression_ratio": 1.569767441860465, "no_speech_prob": 1.1911058663827134e-06}, {"id": 871, "seek": 551056, "start": 5526.400000000001, "end": 5536.360000000001, "text": " start at zero and so zero is the initial 10.1 do you remember this number 10.1 is the average", "tokens": [722, 412, 4018, 293, 370, 4018, 307, 264, 5883, 1266, 13, 16, 360, 291, 1604, 341, 1230, 1266, 13, 16, 307, 264, 4274], "temperature": 0.0, "avg_logprob": -0.1314777284860611, "compression_ratio": 1.569767441860465, "no_speech_prob": 1.1911058663827134e-06}, {"id": 872, "seek": 553636, "start": 5536.36, "end": 5542.2, "text": " log sale price of the whole data set they call it the bias re-interpret and so if we", "tokens": [3565, 8680, 3218, 295, 264, 1379, 1412, 992, 436, 818, 309, 264, 12577, 319, 12, 41935, 293, 370, 498, 321], "temperature": 0.0, "avg_logprob": -0.19827758904659387, "compression_ratio": 1.5, "no_speech_prob": 5.896412744732515e-07}, {"id": 873, "seek": 553636, "start": 5542.2, "end": 5551.16, "text": " call that zero then for this particular row we're looking at year made as a negative 4.2", "tokens": [818, 300, 4018, 550, 337, 341, 1729, 5386, 321, 434, 1237, 412, 1064, 1027, 382, 257, 3671, 1017, 13, 17], "temperature": 0.0, "avg_logprob": -0.19827758904659387, "compression_ratio": 1.5, "no_speech_prob": 5.896412744732515e-07}, {"id": 874, "seek": 553636, "start": 5551.16, "end": 5557.799999999999, "text": " impact on the prediction and then product size has a positive 0.2 coupler system has", "tokens": [2712, 322, 264, 17630, 293, 550, 1674, 2744, 575, 257, 3353, 1958, 13, 17, 1384, 22732, 1185, 575], "temperature": 0.0, "avg_logprob": -0.19827758904659387, "compression_ratio": 1.5, "no_speech_prob": 5.896412744732515e-07}, {"id": 875, "seek": 555780, "start": 5557.8, "end": 5566.8, "text": " a positive 0.046 model ID has a positive 0.127 and so forth right and so the red ones are", "tokens": [257, 3353, 1958, 13, 15, 16169, 2316, 7348, 575, 257, 3353, 1958, 13, 4762, 22, 293, 370, 5220, 558, 293, 370, 264, 2182, 2306, 366], "temperature": 0.0, "avg_logprob": -0.11487178007761638, "compression_ratio": 1.5789473684210527, "no_speech_prob": 1.1430726232219968e-07}, {"id": 876, "seek": 555780, "start": 5566.8, "end": 5571.64, "text": " negative and the green ones are positive and you can see how they all join up until eventually", "tokens": [3671, 293, 264, 3092, 2306, 366, 3353, 293, 291, 393, 536, 577, 436, 439, 3917, 493, 1826, 4728], "temperature": 0.0, "avg_logprob": -0.11487178007761638, "compression_ratio": 1.5789473684210527, "no_speech_prob": 1.1430726232219968e-07}, {"id": 877, "seek": 555780, "start": 5571.64, "end": 5581.12, "text": " overall the prediction is that it's going to be negative 0.122 compared to 10.1 which", "tokens": [4787, 264, 17630, 307, 300, 309, 311, 516, 281, 312, 3671, 1958, 13, 4762, 17, 5347, 281, 1266, 13, 16, 597], "temperature": 0.0, "avg_logprob": -0.11487178007761638, "compression_ratio": 1.5789473684210527, "no_speech_prob": 1.1430726232219968e-07}, {"id": 878, "seek": 558112, "start": 5581.12, "end": 5592.4, "text": " is equal to 9.98 so this kind of plot is called a waterfall plot and so basically when we", "tokens": [307, 2681, 281, 1722, 13, 22516, 370, 341, 733, 295, 7542, 307, 1219, 257, 27848, 7542, 293, 370, 1936, 562, 321], "temperature": 0.0, "avg_logprob": -0.10546639608958411, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.1253511900122248e-07}, {"id": 879, "seek": 558112, "start": 5592.4, "end": 5600.84, "text": " say tree interpreter dot predict it gives us back the prediction which is the actual", "tokens": [584, 4230, 34132, 5893, 6069, 309, 2709, 505, 646, 264, 17630, 597, 307, 264, 3539], "temperature": 0.0, "avg_logprob": -0.10546639608958411, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.1253511900122248e-07}, {"id": 880, "seek": 558112, "start": 5600.84, "end": 5605.92, "text": " number we get back from the random forest the bias which is just always this 10.1 for", "tokens": [1230, 321, 483, 646, 490, 264, 4974, 6719, 264, 12577, 597, 307, 445, 1009, 341, 1266, 13, 16, 337], "temperature": 0.0, "avg_logprob": -0.10546639608958411, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.1253511900122248e-07}, {"id": 881, "seek": 560592, "start": 5605.92, "end": 5612.88, "text": " this data set and then the contributions which is all of these different values it's how", "tokens": [341, 1412, 992, 293, 550, 264, 15725, 597, 307, 439, 295, 613, 819, 4190, 309, 311, 577], "temperature": 0.0, "avg_logprob": -0.09452135955231099, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.0511470236451714e-06}, {"id": 882, "seek": 560592, "start": 5612.88, "end": 5621.56, "text": " much how important was each actor and here I've used a threshold which means anything", "tokens": [709, 577, 1021, 390, 1184, 8747, 293, 510, 286, 600, 1143, 257, 14678, 597, 1355, 1340], "temperature": 0.0, "avg_logprob": -0.09452135955231099, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.0511470236451714e-06}, {"id": 883, "seek": 560592, "start": 5621.56, "end": 5628.58, "text": " that was less than 0.08 or gets thrown into this other category I think this is a really", "tokens": [300, 390, 1570, 813, 1958, 13, 16133, 420, 2170, 11732, 666, 341, 661, 7719, 286, 519, 341, 307, 257, 534], "temperature": 0.0, "avg_logprob": -0.09452135955231099, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.0511470236451714e-06}, {"id": 884, "seek": 560592, "start": 5628.58, "end": 5634.24, "text": " useful kind of thing to have in production because it can help you answer questions whether", "tokens": [4420, 733, 295, 551, 281, 362, 294, 4265, 570, 309, 393, 854, 291, 1867, 1651, 1968], "temperature": 0.0, "avg_logprob": -0.09452135955231099, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.0511470236451714e-06}, {"id": 885, "seek": 563424, "start": 5634.24, "end": 5639.36, "text": " it will be for the customer or for you know whoever's using your model if they're surprised", "tokens": [309, 486, 312, 337, 264, 5474, 420, 337, 291, 458, 11387, 311, 1228, 428, 2316, 498, 436, 434, 6100], "temperature": 0.0, "avg_logprob": -0.13945767849306517, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.5612625904614106e-06}, {"id": 886, "seek": 563424, "start": 5639.36, "end": 5648.84, "text": " about some fiction why is that prediction so I'm going to show you something really", "tokens": [466, 512, 13266, 983, 307, 300, 17630, 370, 286, 478, 516, 281, 855, 291, 746, 534], "temperature": 0.0, "avg_logprob": -0.13945767849306517, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.5612625904614106e-06}, {"id": 887, "seek": 563424, "start": 5648.84, "end": 5655.04, "text": " interesting using some synthetic data and I want you to really have a think about why", "tokens": [1880, 1228, 512, 23420, 1412, 293, 286, 528, 291, 281, 534, 362, 257, 519, 466, 983], "temperature": 0.0, "avg_logprob": -0.13945767849306517, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.5612625904614106e-06}, {"id": 888, "seek": 563424, "start": 5655.04, "end": 5660.9, "text": " this is happening before I tell you and I pause the video if you're watching the video", "tokens": [341, 307, 2737, 949, 286, 980, 291, 293, 286, 10465, 264, 960, 498, 291, 434, 1976, 264, 960], "temperature": 0.0, "avg_logprob": -0.13945767849306517, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.5612625904614106e-06}, {"id": 889, "seek": 566090, "start": 5660.9, "end": 5666.799999999999, "text": " when I get to that point let's start by creating some synthetic data like so so we're going", "tokens": [562, 286, 483, 281, 300, 935, 718, 311, 722, 538, 4084, 512, 23420, 1412, 411, 370, 370, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.11734713207591664, "compression_ratio": 1.6481481481481481, "no_speech_prob": 3.3931253256014315e-06}, {"id": 890, "seek": 566090, "start": 5666.799999999999, "end": 5673.44, "text": " to grab 40 values evenly spaced between 0 and 20 and then we're just going to create", "tokens": [281, 4444, 3356, 4190, 17658, 43766, 1296, 1958, 293, 945, 293, 550, 321, 434, 445, 516, 281, 1884], "temperature": 0.0, "avg_logprob": -0.11734713207591664, "compression_ratio": 1.6481481481481481, "no_speech_prob": 3.3931253256014315e-06}, {"id": 891, "seek": 566090, "start": 5673.44, "end": 5683.44, "text": " the y equals x line and add some normally distributed random jitter on that here's this", "tokens": [264, 288, 6915, 2031, 1622, 293, 909, 512, 5646, 12631, 4974, 361, 3904, 322, 300, 510, 311, 341], "temperature": 0.0, "avg_logprob": -0.11734713207591664, "compression_ratio": 1.6481481481481481, "no_speech_prob": 3.3931253256014315e-06}, {"id": 892, "seek": 566090, "start": 5683.44, "end": 5688.879999999999, "text": " kind of plot so here's some data we want to try and predict and we're going to use a random", "tokens": [733, 295, 7542, 370, 510, 311, 512, 1412, 321, 528, 281, 853, 293, 6069, 293, 321, 434, 516, 281, 764, 257, 4974], "temperature": 0.0, "avg_logprob": -0.11734713207591664, "compression_ratio": 1.6481481481481481, "no_speech_prob": 3.3931253256014315e-06}, {"id": 893, "seek": 568888, "start": 5688.88, "end": 5698.2, "text": " forest in a kind of bit of a overkill here now in this case we only have one independent", "tokens": [6719, 294, 257, 733, 295, 857, 295, 257, 670, 34213, 510, 586, 294, 341, 1389, 321, 787, 362, 472, 6695], "temperature": 0.0, "avg_logprob": -0.11937255075533096, "compression_ratio": 1.4722222222222223, "no_speech_prob": 3.7853017147426726e-06}, {"id": 894, "seek": 568888, "start": 5698.2, "end": 5708.72, "text": " variable scikit-learn expects us to have more than one so we can use unsqueeze in PyTorch", "tokens": [7006, 2180, 22681, 12, 306, 1083, 33280, 505, 281, 362, 544, 813, 472, 370, 321, 393, 764, 2693, 1077, 10670, 294, 9953, 51, 284, 339], "temperature": 0.0, "avg_logprob": -0.11937255075533096, "compression_ratio": 1.4722222222222223, "no_speech_prob": 3.7853017147426726e-06}, {"id": 895, "seek": 568888, "start": 5708.72, "end": 5714.68, "text": " to add that go from a shape of 40 in other words a vector with 40 elements for a shape", "tokens": [281, 909, 300, 352, 490, 257, 3909, 295, 3356, 294, 661, 2283, 257, 8062, 365, 3356, 4959, 337, 257, 3909], "temperature": 0.0, "avg_logprob": -0.11937255075533096, "compression_ratio": 1.4722222222222223, "no_speech_prob": 3.7853017147426726e-06}, {"id": 896, "seek": 571468, "start": 5714.68, "end": 5721.52, "text": " of 40 comma 1 in other words a matrix of 40 rows with one column so this unsqueeze one", "tokens": [295, 3356, 22117, 502, 294, 661, 2283, 257, 8141, 295, 3356, 13241, 365, 472, 7738, 370, 341, 2693, 1077, 10670, 472], "temperature": 0.0, "avg_logprob": -0.1808888808540676, "compression_ratio": 1.5789473684210527, "no_speech_prob": 7.934480095173058e-07}, {"id": 897, "seek": 571468, "start": 5721.52, "end": 5729.400000000001, "text": " means add a unit axis here I don't use unsqueeze very often because I actually generally prefer", "tokens": [1355, 909, 257, 4985, 10298, 510, 286, 500, 380, 764, 2693, 1077, 10670, 588, 2049, 570, 286, 767, 5101, 4382], "temperature": 0.0, "avg_logprob": -0.1808888808540676, "compression_ratio": 1.5789473684210527, "no_speech_prob": 7.934480095173058e-07}, {"id": 898, "seek": 571468, "start": 5729.400000000001, "end": 5736.240000000001, "text": " the index with a special value none this works in PyTorch and numpy and this work the way", "tokens": [264, 8186, 365, 257, 2121, 2158, 6022, 341, 1985, 294, 9953, 51, 284, 339, 293, 1031, 8200, 293, 341, 589, 264, 636], "temperature": 0.0, "avg_logprob": -0.1808888808540676, "compression_ratio": 1.5789473684210527, "no_speech_prob": 7.934480095173058e-07}, {"id": 899, "seek": 571468, "start": 5736.240000000001, "end": 5741.9400000000005, "text": " it works is to say okay excellent remember that's size it's a vector of length 40 every", "tokens": [309, 1985, 307, 281, 584, 1392, 7103, 1604, 300, 311, 2744, 309, 311, 257, 8062, 295, 4641, 3356, 633], "temperature": 0.0, "avg_logprob": -0.1808888808540676, "compression_ratio": 1.5789473684210527, "no_speech_prob": 7.934480095173058e-07}, {"id": 900, "seek": 574194, "start": 5741.94, "end": 5749.96, "text": " row and then none means insert a unit axis here for the column so these are two ways", "tokens": [5386, 293, 550, 6022, 1355, 8969, 257, 4985, 10298, 510, 337, 264, 7738, 370, 613, 366, 732, 2098], "temperature": 0.0, "avg_logprob": -0.18100260552905856, "compression_ratio": 1.6798029556650247, "no_speech_prob": 3.059018922613177e-07}, {"id": 901, "seek": 574194, "start": 5749.96, "end": 5753.719999999999, "text": " of doing the same thing but this one is a little bit more flexible so that's what I", "tokens": [295, 884, 264, 912, 551, 457, 341, 472, 307, 257, 707, 857, 544, 11358, 370, 300, 311, 437, 286], "temperature": 0.0, "avg_logprob": -0.18100260552905856, "compression_ratio": 1.6798029556650247, "no_speech_prob": 3.059018922613177e-07}, {"id": 902, "seek": 574194, "start": 5753.719999999999, "end": 5759.759999999999, "text": " use more often but now that we've got the shape that is expected which is a rank to", "tokens": [764, 544, 2049, 457, 586, 300, 321, 600, 658, 264, 3909, 300, 307, 5176, 597, 307, 257, 6181, 281], "temperature": 0.0, "avg_logprob": -0.18100260552905856, "compression_ratio": 1.6798029556650247, "no_speech_prob": 3.059018922613177e-07}, {"id": 903, "seek": 574194, "start": 5759.759999999999, "end": 5766.16, "text": " tensor and a rare array with two dimensions or axes we can create a random forest we can", "tokens": [40863, 293, 257, 5892, 10225, 365, 732, 12819, 420, 35387, 321, 393, 1884, 257, 4974, 6719, 321, 393], "temperature": 0.0, "avg_logprob": -0.18100260552905856, "compression_ratio": 1.6798029556650247, "no_speech_prob": 3.059018922613177e-07}, {"id": 904, "seek": 576616, "start": 5766.16, "end": 5774.32, "text": " fit it and let's just use the first 30 data points right so kind of stop here and then", "tokens": [3318, 309, 293, 718, 311, 445, 764, 264, 700, 2217, 1412, 2793, 558, 370, 733, 295, 1590, 510, 293, 550], "temperature": 0.0, "avg_logprob": -0.11919291814168294, "compression_ratio": 1.86096256684492, "no_speech_prob": 3.555959892764804e-06}, {"id": 905, "seek": 576616, "start": 5774.32, "end": 5778.84, "text": " let's do a prediction right so let's plot the original data points and then also plot", "tokens": [718, 311, 360, 257, 17630, 558, 370, 718, 311, 7542, 264, 3380, 1412, 2793, 293, 550, 611, 7542], "temperature": 0.0, "avg_logprob": -0.11919291814168294, "compression_ratio": 1.86096256684492, "no_speech_prob": 3.555959892764804e-06}, {"id": 906, "seek": 576616, "start": 5778.84, "end": 5784.8, "text": " a prediction and look what happens on the prediction it acts it's kind of nice and accurate", "tokens": [257, 17630, 293, 574, 437, 2314, 322, 264, 17630, 309, 10672, 309, 311, 733, 295, 1481, 293, 8559], "temperature": 0.0, "avg_logprob": -0.11919291814168294, "compression_ratio": 1.86096256684492, "no_speech_prob": 3.555959892764804e-06}, {"id": 907, "seek": 576616, "start": 5784.8, "end": 5789.04, "text": " and then suddenly what happens so this is the bit where if you watch in the video I", "tokens": [293, 550, 5800, 437, 2314, 370, 341, 307, 264, 857, 689, 498, 291, 1159, 294, 264, 960, 286], "temperature": 0.0, "avg_logprob": -0.11919291814168294, "compression_ratio": 1.86096256684492, "no_speech_prob": 3.555959892764804e-06}, {"id": 908, "seek": 578904, "start": 5789.04, "end": 5796.88, "text": " want you to pause and have a think biases flat so what's gone on here well remember", "tokens": [528, 291, 281, 10465, 293, 362, 257, 519, 32152, 4962, 370, 437, 311, 2780, 322, 510, 731, 1604], "temperature": 0.0, "avg_logprob": -0.08161454200744629, "compression_ratio": 1.7164179104477613, "no_speech_prob": 2.742086735452176e-07}, {"id": 909, "seek": 578904, "start": 5796.88, "end": 5802.28, "text": " a random forest is just taking the average of predictions of a bunch of trees and a tree", "tokens": [257, 4974, 6719, 307, 445, 1940, 264, 4274, 295, 21264, 295, 257, 3840, 295, 5852, 293, 257, 4230], "temperature": 0.0, "avg_logprob": -0.08161454200744629, "compression_ratio": 1.7164179104477613, "no_speech_prob": 2.742086735452176e-07}, {"id": 910, "seek": 578904, "start": 5802.28, "end": 5809.5199999999995, "text": " the prediction of a tree is just the average of the values in a leaf node and remember", "tokens": [264, 17630, 295, 257, 4230, 307, 445, 264, 4274, 295, 264, 4190, 294, 257, 10871, 9984, 293, 1604], "temperature": 0.0, "avg_logprob": -0.08161454200744629, "compression_ratio": 1.7164179104477613, "no_speech_prob": 2.742086735452176e-07}, {"id": 911, "seek": 578904, "start": 5809.5199999999995, "end": 5816.2, "text": " we fitted using a training set containing only the first 30 so none of these appeared", "tokens": [321, 26321, 1228, 257, 3097, 992, 19273, 787, 264, 700, 2217, 370, 6022, 295, 613, 8516], "temperature": 0.0, "avg_logprob": -0.08161454200744629, "compression_ratio": 1.7164179104477613, "no_speech_prob": 2.742086735452176e-07}, {"id": 912, "seek": 581620, "start": 5816.2, "end": 5822.44, "text": " in the training set so the highest we could get would be the average of values that are", "tokens": [294, 264, 3097, 992, 370, 264, 6343, 321, 727, 483, 576, 312, 264, 4274, 295, 4190, 300, 366], "temperature": 0.0, "avg_logprob": -0.1204014548772498, "compression_ratio": 1.781725888324873, "no_speech_prob": 1.6280462205031654e-06}, {"id": 913, "seek": 581620, "start": 5822.44, "end": 5829.36, "text": " inside the training set in other words there's this maximum we can get to so random forests", "tokens": [1854, 264, 3097, 992, 294, 661, 2283, 456, 311, 341, 6674, 321, 393, 483, 281, 370, 4974, 21700], "temperature": 0.0, "avg_logprob": -0.1204014548772498, "compression_ratio": 1.781725888324873, "no_speech_prob": 1.6280462205031654e-06}, {"id": 914, "seek": 581620, "start": 5829.36, "end": 5835.5599999999995, "text": " cannot extrapolate outside of the bounds of the data that they're training set this is", "tokens": [2644, 48224, 473, 2380, 295, 264, 29905, 295, 264, 1412, 300, 436, 434, 3097, 992, 341, 307], "temperature": 0.0, "avg_logprob": -0.1204014548772498, "compression_ratio": 1.781725888324873, "no_speech_prob": 1.6280462205031654e-06}, {"id": 915, "seek": 581620, "start": 5835.5599999999995, "end": 5838.48, "text": " going to be a huge problem for things like time series prediction where there's like", "tokens": [516, 281, 312, 257, 2603, 1154, 337, 721, 411, 565, 2638, 17630, 689, 456, 311, 411], "temperature": 0.0, "avg_logprob": -0.1204014548772498, "compression_ratio": 1.781725888324873, "no_speech_prob": 1.6280462205031654e-06}, {"id": 916, "seek": 583848, "start": 5838.48, "end": 5846.44, "text": " an underlying trend for instance but really it's more a more general issue than just time", "tokens": [364, 14217, 6028, 337, 5197, 457, 534, 309, 311, 544, 257, 544, 2674, 2734, 813, 445, 565], "temperature": 0.0, "avg_logprob": -0.11236834820405936, "compression_ratio": 1.7156862745098038, "no_speech_prob": 4.1811810547187633e-07}, {"id": 917, "seek": 583848, "start": 5846.44, "end": 5851.04, "text": " variables it's going to be hard for rent or impossible often for random forest to just", "tokens": [9102, 309, 311, 516, 281, 312, 1152, 337, 6214, 420, 6243, 2049, 337, 4974, 6719, 281, 445], "temperature": 0.0, "avg_logprob": -0.11236834820405936, "compression_ratio": 1.7156862745098038, "no_speech_prob": 4.1811810547187633e-07}, {"id": 918, "seek": 583848, "start": 5851.04, "end": 5857.04, "text": " extrapolate outside the types of data that it's seen in a general sense so we need to", "tokens": [48224, 473, 2380, 264, 3467, 295, 1412, 300, 309, 311, 1612, 294, 257, 2674, 2020, 370, 321, 643, 281], "temperature": 0.0, "avg_logprob": -0.11236834820405936, "compression_ratio": 1.7156862745098038, "no_speech_prob": 4.1811810547187633e-07}, {"id": 919, "seek": 583848, "start": 5857.04, "end": 5864.799999999999, "text": " make sure that our validation set does not contain out of domain data so how do we find", "tokens": [652, 988, 300, 527, 24071, 992, 775, 406, 5304, 484, 295, 9274, 1412, 370, 577, 360, 321, 915], "temperature": 0.0, "avg_logprob": -0.11236834820405936, "compression_ratio": 1.7156862745098038, "no_speech_prob": 4.1811810547187633e-07}, {"id": 920, "seek": 586480, "start": 5864.8, "end": 5872.12, "text": " out of domain data so we might not even know if our test set is distributed in the same", "tokens": [484, 295, 9274, 1412, 370, 321, 1062, 406, 754, 458, 498, 527, 1500, 992, 307, 12631, 294, 264, 912], "temperature": 0.0, "avg_logprob": -0.08872141557581284, "compression_ratio": 1.7391304347826086, "no_speech_prob": 5.896409334127384e-07}, {"id": 921, "seek": 586480, "start": 5872.12, "end": 5875.9800000000005, "text": " way as our training data so if they're from two different time periods how do you kind", "tokens": [636, 382, 527, 3097, 1412, 370, 498, 436, 434, 490, 732, 819, 565, 13804, 577, 360, 291, 733], "temperature": 0.0, "avg_logprob": -0.08872141557581284, "compression_ratio": 1.7391304347826086, "no_speech_prob": 5.896409334127384e-07}, {"id": 922, "seek": 586480, "start": 5875.9800000000005, "end": 5883.28, "text": " of tell how they vary right or if it's a Kaggle competition how do you tell if the test set", "tokens": [295, 980, 577, 436, 10559, 558, 420, 498, 309, 311, 257, 48751, 22631, 6211, 577, 360, 291, 980, 498, 264, 1500, 992], "temperature": 0.0, "avg_logprob": -0.08872141557581284, "compression_ratio": 1.7391304347826086, "no_speech_prob": 5.896409334127384e-07}, {"id": 923, "seek": 586480, "start": 5883.28, "end": 5889.84, "text": " and the training set which Kaggle gives you have some underlying differences there's actually", "tokens": [293, 264, 3097, 992, 597, 48751, 22631, 2709, 291, 362, 512, 14217, 7300, 456, 311, 767], "temperature": 0.0, "avg_logprob": -0.08872141557581284, "compression_ratio": 1.7391304347826086, "no_speech_prob": 5.896409334127384e-07}, {"id": 924, "seek": 588984, "start": 5889.84, "end": 5896.92, "text": " a cool trick you can do which is you can create a column called is valid which contains zero", "tokens": [257, 1627, 4282, 291, 393, 360, 597, 307, 291, 393, 1884, 257, 7738, 1219, 307, 7363, 597, 8306, 4018], "temperature": 0.0, "avg_logprob": -0.0990440050760905, "compression_ratio": 2.1637426900584797, "no_speech_prob": 5.989263627270702e-07}, {"id": 925, "seek": 588984, "start": 5896.92, "end": 5903.6, "text": " for everything in the training set and one for everything in the validation set and it's", "tokens": [337, 1203, 294, 264, 3097, 992, 293, 472, 337, 1203, 294, 264, 24071, 992, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.0990440050760905, "compression_ratio": 2.1637426900584797, "no_speech_prob": 5.989263627270702e-07}, {"id": 926, "seek": 588984, "start": 5903.6, "end": 5909.24, "text": " concatenating all of the independent variables together so it's concatenating the independent", "tokens": [1588, 7186, 990, 439, 295, 264, 6695, 9102, 1214, 370, 309, 311, 1588, 7186, 990, 264, 6695], "temperature": 0.0, "avg_logprob": -0.0990440050760905, "compression_ratio": 2.1637426900584797, "no_speech_prob": 5.989263627270702e-07}, {"id": 927, "seek": 588984, "start": 5909.24, "end": 5916.400000000001, "text": " variables of both the training and validation set together so this is our independent variable", "tokens": [9102, 295, 1293, 264, 3097, 293, 24071, 992, 1214, 370, 341, 307, 527, 6695, 7006], "temperature": 0.0, "avg_logprob": -0.0990440050760905, "compression_ratio": 2.1637426900584797, "no_speech_prob": 5.989263627270702e-07}, {"id": 928, "seek": 591640, "start": 5916.4, "end": 5920.74, "text": " and this becomes our dependent variable and we're going to create a random forest not", "tokens": [293, 341, 3643, 527, 12334, 7006, 293, 321, 434, 516, 281, 1884, 257, 4974, 6719, 406], "temperature": 0.0, "avg_logprob": -0.10696576231269427, "compression_ratio": 1.9775784753363228, "no_speech_prob": 1.6028039908633218e-06}, {"id": 929, "seek": 591640, "start": 5920.74, "end": 5928.16, "text": " for predicting price but a random forest that predicts is this row from the validation set", "tokens": [337, 32884, 3218, 457, 257, 4974, 6719, 300, 6069, 82, 307, 341, 5386, 490, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.10696576231269427, "compression_ratio": 1.9775784753363228, "no_speech_prob": 1.6028039908633218e-06}, {"id": 930, "seek": 591640, "start": 5928.16, "end": 5933.759999999999, "text": " or the training set so if the validation set and the training set are from kind of the", "tokens": [420, 264, 3097, 992, 370, 498, 264, 24071, 992, 293, 264, 3097, 992, 366, 490, 733, 295, 264], "temperature": 0.0, "avg_logprob": -0.10696576231269427, "compression_ratio": 1.9775784753363228, "no_speech_prob": 1.6028039908633218e-06}, {"id": 931, "seek": 591640, "start": 5933.759999999999, "end": 5938.679999999999, "text": " same distribution if they're not different then this random forest should basically have", "tokens": [912, 7316, 498, 436, 434, 406, 819, 550, 341, 4974, 6719, 820, 1936, 362], "temperature": 0.0, "avg_logprob": -0.10696576231269427, "compression_ratio": 1.9775784753363228, "no_speech_prob": 1.6028039908633218e-06}, {"id": 932, "seek": 591640, "start": 5938.679999999999, "end": 5945.92, "text": " zero predictive power if it has any predictive power then it means that our training and", "tokens": [4018, 35521, 1347, 498, 309, 575, 604, 35521, 1347, 550, 309, 1355, 300, 527, 3097, 293], "temperature": 0.0, "avg_logprob": -0.10696576231269427, "compression_ratio": 1.9775784753363228, "no_speech_prob": 1.6028039908633218e-06}, {"id": 933, "seek": 594592, "start": 5945.92, "end": 5952.36, "text": " validation set are different and to find out the source of that difference we can use feature", "tokens": [24071, 992, 366, 819, 293, 281, 915, 484, 264, 4009, 295, 300, 2649, 321, 393, 764, 4111], "temperature": 0.0, "avg_logprob": -0.09540390062935744, "compression_ratio": 1.6985645933014355, "no_speech_prob": 1.505699401604943e-06}, {"id": 934, "seek": 594592, "start": 5952.36, "end": 5960.62, "text": " importance and so you can see here that the difference between the validation set and", "tokens": [7379, 293, 370, 291, 393, 536, 510, 300, 264, 2649, 1296, 264, 24071, 992, 293], "temperature": 0.0, "avg_logprob": -0.09540390062935744, "compression_ratio": 1.6985645933014355, "no_speech_prob": 1.505699401604943e-06}, {"id": 935, "seek": 594592, "start": 5960.62, "end": 5968.78, "text": " the training set is not surprisingly sale elapsed so that's the number of days since", "tokens": [264, 3097, 992, 307, 406, 17600, 8680, 806, 2382, 292, 370, 300, 311, 264, 1230, 295, 1708, 1670], "temperature": 0.0, "avg_logprob": -0.09540390062935744, "compression_ratio": 1.6985645933014355, "no_speech_prob": 1.505699401604943e-06}, {"id": 936, "seek": 594592, "start": 5968.78, "end": 5974.0, "text": " I think like 1970 or something so it's basically the date so yes of course you can predict", "tokens": [286, 519, 411, 14577, 420, 746, 370, 309, 311, 1936, 264, 4002, 370, 2086, 295, 1164, 291, 393, 6069], "temperature": 0.0, "avg_logprob": -0.09540390062935744, "compression_ratio": 1.6985645933014355, "no_speech_prob": 1.505699401604943e-06}, {"id": 937, "seek": 597400, "start": 5974.0, "end": 5978.76, "text": " whether something is in the validation set or the training set by looking at the date", "tokens": [1968, 746, 307, 294, 264, 24071, 992, 420, 264, 3097, 992, 538, 1237, 412, 264, 4002], "temperature": 0.0, "avg_logprob": -0.1139788528283437, "compression_ratio": 1.8235294117647058, "no_speech_prob": 6.681505055894377e-07}, {"id": 938, "seek": 597400, "start": 5978.76, "end": 5984.0, "text": " because that's actually how we find them that makes sense this is interesting sales ID so", "tokens": [570, 300, 311, 767, 577, 321, 915, 552, 300, 1669, 2020, 341, 307, 1880, 5763, 7348, 370], "temperature": 0.0, "avg_logprob": -0.1139788528283437, "compression_ratio": 1.8235294117647058, "no_speech_prob": 6.681505055894377e-07}, {"id": 939, "seek": 597400, "start": 5984.0, "end": 5989.32, "text": " it looks like the sales ID is not some random identifier but it increases over time and", "tokens": [309, 1542, 411, 264, 5763, 7348, 307, 406, 512, 4974, 45690, 457, 309, 8637, 670, 565, 293], "temperature": 0.0, "avg_logprob": -0.1139788528283437, "compression_ratio": 1.8235294117647058, "no_speech_prob": 6.681505055894377e-07}, {"id": 940, "seek": 597400, "start": 5989.32, "end": 5995.44, "text": " ditto for machine ID and then there's some other smaller ones here that kind of makes", "tokens": [274, 34924, 337, 3479, 7348, 293, 550, 456, 311, 512, 661, 4356, 2306, 510, 300, 733, 295, 1669], "temperature": 0.0, "avg_logprob": -0.1139788528283437, "compression_ratio": 1.8235294117647058, "no_speech_prob": 6.681505055894377e-07}, {"id": 941, "seek": 597400, "start": 5995.44, "end": 6000.44, "text": " sense so I guess for something like model desk I guess there are certain models that", "tokens": [2020, 370, 286, 2041, 337, 746, 411, 2316, 10026, 286, 2041, 456, 366, 1629, 5245, 300], "temperature": 0.0, "avg_logprob": -0.1139788528283437, "compression_ratio": 1.8235294117647058, "no_speech_prob": 6.681505055894377e-07}, {"id": 942, "seek": 600044, "start": 6000.44, "end": 6007.799999999999, "text": " were only made in later years for instance but you can see these top three columns are", "tokens": [645, 787, 1027, 294, 1780, 924, 337, 5197, 457, 291, 393, 536, 613, 1192, 1045, 13766, 366], "temperature": 0.0, "avg_logprob": -0.10032186810932463, "compression_ratio": 1.532544378698225, "no_speech_prob": 1.1726381217158632e-06}, {"id": 943, "seek": 600044, "start": 6007.799999999999, "end": 6015.36, "text": " a bit of an issue so then we could say like okay what happens if we look at each one of", "tokens": [257, 857, 295, 364, 2734, 370, 550, 321, 727, 584, 411, 1392, 437, 2314, 498, 321, 574, 412, 1184, 472, 295], "temperature": 0.0, "avg_logprob": -0.10032186810932463, "compression_ratio": 1.532544378698225, "no_speech_prob": 1.1726381217158632e-06}, {"id": 944, "seek": 600044, "start": 6015.36, "end": 6027.48, "text": " those columns those first three and remove them and then see how it changes our RMSE", "tokens": [729, 13766, 729, 700, 1045, 293, 4159, 552, 293, 550, 536, 577, 309, 2962, 527, 23790, 5879], "temperature": 0.0, "avg_logprob": -0.10032186810932463, "compression_ratio": 1.532544378698225, "no_speech_prob": 1.1726381217158632e-06}, {"id": 945, "seek": 602748, "start": 6027.48, "end": 6037.799999999999, "text": " on our sales price model on the validation set so we start from point two three two and", "tokens": [322, 527, 5763, 3218, 2316, 322, 264, 24071, 992, 370, 321, 722, 490, 935, 732, 1045, 732, 293], "temperature": 0.0, "avg_logprob": -0.11550763212604287, "compression_ratio": 1.733009708737864, "no_speech_prob": 1.7061779544746969e-06}, {"id": 946, "seek": 602748, "start": 6037.799999999999, "end": 6044.2, "text": " removing sales ID actually makes it a bit better sale elapsed makes it a bit worse machine", "tokens": [12720, 5763, 7348, 767, 1669, 309, 257, 857, 1101, 8680, 806, 2382, 292, 1669, 309, 257, 857, 5324, 3479], "temperature": 0.0, "avg_logprob": -0.11550763212604287, "compression_ratio": 1.733009708737864, "no_speech_prob": 1.7061779544746969e-06}, {"id": 947, "seek": 602748, "start": 6044.2, "end": 6049.639999999999, "text": " ID about the same so we can probably remove sales ID and machine ID without losing any", "tokens": [7348, 466, 264, 912, 370, 321, 393, 1391, 4159, 5763, 7348, 293, 3479, 7348, 1553, 7027, 604], "temperature": 0.0, "avg_logprob": -0.11550763212604287, "compression_ratio": 1.733009708737864, "no_speech_prob": 1.7061779544746969e-06}, {"id": 948, "seek": 602748, "start": 6049.639999999999, "end": 6055.04, "text": " accuracy and yep it's actually slightly improved but most importantly it's going to be more", "tokens": [14170, 293, 18633, 309, 311, 767, 4748, 9689, 457, 881, 8906, 309, 311, 516, 281, 312, 544], "temperature": 0.0, "avg_logprob": -0.11550763212604287, "compression_ratio": 1.733009708737864, "no_speech_prob": 1.7061779544746969e-06}, {"id": 949, "seek": 605504, "start": 6055.04, "end": 6064.32, "text": " resilient over time right because we're trying to remove the time related features another", "tokens": [23699, 670, 565, 558, 570, 321, 434, 1382, 281, 4159, 264, 565, 4077, 4122, 1071], "temperature": 0.0, "avg_logprob": -0.0896896032186655, "compression_ratio": 1.736318407960199, "no_speech_prob": 2.406092562523554e-06}, {"id": 950, "seek": 605504, "start": 6064.32, "end": 6069.96, "text": " thing to note is that since it seems that you know this kind of sale elapsed issue that", "tokens": [551, 281, 3637, 307, 300, 1670, 309, 2544, 300, 291, 458, 341, 733, 295, 8680, 806, 2382, 292, 2734, 300], "temperature": 0.0, "avg_logprob": -0.0896896032186655, "compression_ratio": 1.736318407960199, "no_speech_prob": 2.406092562523554e-06}, {"id": 951, "seek": 605504, "start": 6069.96, "end": 6076.56, "text": " maybe it's making a big difference is maybe looking at the sale year distribution this", "tokens": [1310, 309, 311, 1455, 257, 955, 2649, 307, 1310, 1237, 412, 264, 8680, 1064, 7316, 341], "temperature": 0.0, "avg_logprob": -0.0896896032186655, "compression_ratio": 1.736318407960199, "no_speech_prob": 2.406092562523554e-06}, {"id": 952, "seek": 605504, "start": 6076.56, "end": 6081.4, "text": " is the histogram most of the sales are in the last few years anyway so what happens", "tokens": [307, 264, 49816, 881, 295, 264, 5763, 366, 294, 264, 1036, 1326, 924, 4033, 370, 437, 2314], "temperature": 0.0, "avg_logprob": -0.0896896032186655, "compression_ratio": 1.736318407960199, "no_speech_prob": 2.406092562523554e-06}, {"id": 953, "seek": 608140, "start": 6081.4, "end": 6089.92, "text": " if we only include the most recent few years so let's just include everything after 2004", "tokens": [498, 321, 787, 4090, 264, 881, 5162, 1326, 924, 370, 718, 311, 445, 4090, 1203, 934, 15817], "temperature": 0.0, "avg_logprob": -0.114793028150286, "compression_ratio": 1.4666666666666666, "no_speech_prob": 1.7880602172226645e-06}, {"id": 954, "seek": 608140, "start": 6089.92, "end": 6098.08, "text": " so that is X is filtered and if I train on that subset then my accuracy goes improves", "tokens": [370, 300, 307, 1783, 307, 37111, 293, 498, 286, 3847, 322, 300, 25993, 550, 452, 14170, 1709, 24771], "temperature": 0.0, "avg_logprob": -0.114793028150286, "compression_ratio": 1.4666666666666666, "no_speech_prob": 1.7880602172226645e-06}, {"id": 955, "seek": 608140, "start": 6098.08, "end": 6106.28, "text": " a bit more from 231 to 30 so that's interesting right we're actually using less data less", "tokens": [257, 857, 544, 490, 6673, 16, 281, 2217, 370, 300, 311, 1880, 558, 321, 434, 767, 1228, 1570, 1412, 1570], "temperature": 0.0, "avg_logprob": -0.114793028150286, "compression_ratio": 1.4666666666666666, "no_speech_prob": 1.7880602172226645e-06}, {"id": 956, "seek": 610628, "start": 6106.28, "end": 6114.0199999999995, "text": " rows and getting a slightly better result because the more recent data is more representative", "tokens": [13241, 293, 1242, 257, 4748, 1101, 1874, 570, 264, 544, 5162, 1412, 307, 544, 12424], "temperature": 0.0, "avg_logprob": -0.07350638508796692, "compression_ratio": 1.7475728155339805, "no_speech_prob": 1.5779536397531047e-06}, {"id": 957, "seek": 610628, "start": 6114.0199999999995, "end": 6122.28, "text": " so that's about as far as we can get with our random forest but what I will say is this", "tokens": [370, 300, 311, 466, 382, 1400, 382, 321, 393, 483, 365, 527, 4974, 6719, 457, 437, 286, 486, 584, 307, 341], "temperature": 0.0, "avg_logprob": -0.07350638508796692, "compression_ratio": 1.7475728155339805, "no_speech_prob": 1.5779536397531047e-06}, {"id": 958, "seek": 610628, "start": 6122.28, "end": 6129.28, "text": " this issue of extrapolation would not happen with a neural net would it because a neural", "tokens": [341, 2734, 295, 48224, 399, 576, 406, 1051, 365, 257, 18161, 2533, 576, 309, 570, 257, 18161], "temperature": 0.0, "avg_logprob": -0.07350638508796692, "compression_ratio": 1.7475728155339805, "no_speech_prob": 1.5779536397531047e-06}, {"id": 959, "seek": 610628, "start": 6129.28, "end": 6134.08, "text": " net is using the kind of the underlying layers are linear layers and so linear layers can", "tokens": [2533, 307, 1228, 264, 733, 295, 264, 14217, 7914, 366, 8213, 7914, 293, 370, 8213, 7914, 393], "temperature": 0.0, "avg_logprob": -0.07350638508796692, "compression_ratio": 1.7475728155339805, "no_speech_prob": 1.5779536397531047e-06}, {"id": 960, "seek": 613408, "start": 6134.08, "end": 6140.5199999999995, "text": " absolutely extrapolate so the obvious thing to think then at this point is well maybe", "tokens": [3122, 48224, 473, 370, 264, 6322, 551, 281, 519, 550, 412, 341, 935, 307, 731, 1310], "temperature": 0.0, "avg_logprob": -0.15069223774804008, "compression_ratio": 1.8594594594594596, "no_speech_prob": 2.8572987957886653e-06}, {"id": 961, "seek": 613408, "start": 6140.5199999999995, "end": 6145.84, "text": " what a neural net do a better job of this that's going to be the thing next up to this", "tokens": [437, 257, 18161, 2533, 360, 257, 1101, 1691, 295, 341, 300, 311, 516, 281, 312, 264, 551, 958, 493, 281, 341], "temperature": 0.0, "avg_logprob": -0.15069223774804008, "compression_ratio": 1.8594594594594596, "no_speech_prob": 2.8572987957886653e-06}, {"id": 962, "seek": 613408, "start": 6145.84, "end": 6157.08, "text": " question question first how do how does feature importance relate to correlation", "tokens": [1168, 1168, 700, 577, 360, 577, 775, 4111, 7379, 10961, 281, 20009], "temperature": 0.0, "avg_logprob": -0.15069223774804008, "compression_ratio": 1.8594594594594596, "no_speech_prob": 2.8572987957886653e-06}, {"id": 963, "seek": 613408, "start": 6157.08, "end": 6162.76, "text": " feature importance doesn't particularly relate to correlation correlation is a concept for", "tokens": [4111, 7379, 1177, 380, 4098, 10961, 281, 20009, 20009, 307, 257, 3410, 337], "temperature": 0.0, "avg_logprob": -0.15069223774804008, "compression_ratio": 1.8594594594594596, "no_speech_prob": 2.8572987957886653e-06}, {"id": 964, "seek": 616276, "start": 6162.76, "end": 6167.8, "text": " linear models and this is not a linear model so remember feature importance is calculated", "tokens": [8213, 5245, 293, 341, 307, 406, 257, 8213, 2316, 370, 1604, 4111, 7379, 307, 15598], "temperature": 0.0, "avg_logprob": -0.08272376553765659, "compression_ratio": 1.6071428571428572, "no_speech_prob": 2.0904442408209434e-06}, {"id": 965, "seek": 616276, "start": 6167.8, "end": 6176.72, "text": " by looking at the improvement in accuracy as you go down each tree and you go down each", "tokens": [538, 1237, 412, 264, 10444, 294, 14170, 382, 291, 352, 760, 1184, 4230, 293, 291, 352, 760, 1184], "temperature": 0.0, "avg_logprob": -0.08272376553765659, "compression_ratio": 1.6071428571428572, "no_speech_prob": 2.0904442408209434e-06}, {"id": 966, "seek": 616276, "start": 6176.72, "end": 6185.68, "text": " binary split if you're used to linear regression then I guess our relations sometimes can be", "tokens": [17434, 7472, 498, 291, 434, 1143, 281, 8213, 24590, 550, 286, 2041, 527, 2299, 2171, 393, 312], "temperature": 0.0, "avg_logprob": -0.08272376553765659, "compression_ratio": 1.6071428571428572, "no_speech_prob": 2.0904442408209434e-06}, {"id": 967, "seek": 618568, "start": 6185.68, "end": 6193.72, "text": " used as a measure of feature importance but this is a much more kind of direct version", "tokens": [1143, 382, 257, 3481, 295, 4111, 7379, 457, 341, 307, 257, 709, 544, 733, 295, 2047, 3037], "temperature": 0.0, "avg_logprob": -0.16994033715663812, "compression_ratio": 1.7575757575757576, "no_speech_prob": 2.64256095761084e-06}, {"id": 968, "seek": 618568, "start": 6193.72, "end": 6201.88, "text": " it's taking account of these nonlinearities and interactions as well so it's a much more", "tokens": [309, 311, 1940, 2696, 295, 613, 2107, 28263, 1088, 293, 13280, 382, 731, 370, 309, 311, 257, 709, 544], "temperature": 0.0, "avg_logprob": -0.16994033715663812, "compression_ratio": 1.7575757575757576, "no_speech_prob": 2.64256095761084e-06}, {"id": 969, "seek": 618568, "start": 6201.88, "end": 6211.08, "text": " flexible and reliable measure and release feature importance any more questions so I", "tokens": [11358, 293, 12924, 3481, 293, 4374, 4111, 7379, 604, 544, 1651, 370, 286], "temperature": 0.0, "avg_logprob": -0.16994033715663812, "compression_ratio": 1.7575757575757576, "no_speech_prob": 2.64256095761084e-06}, {"id": 970, "seek": 618568, "start": 6211.08, "end": 6214.96, "text": " do the same thing with a neural network I'm going to just copy and paste the same lines", "tokens": [360, 264, 912, 551, 365, 257, 18161, 3209, 286, 478, 516, 281, 445, 5055, 293, 9163, 264, 912, 3876], "temperature": 0.0, "avg_logprob": -0.16994033715663812, "compression_ratio": 1.7575757575757576, "no_speech_prob": 2.64256095761084e-06}, {"id": 971, "seek": 621496, "start": 6214.96, "end": 6221.12, "text": " of code that I had from before but this time I call it nn dfnn and these are the same lines", "tokens": [295, 3089, 300, 286, 632, 490, 949, 457, 341, 565, 286, 818, 309, 297, 77, 274, 69, 26384, 293, 613, 366, 264, 912, 3876], "temperature": 0.0, "avg_logprob": -0.13761567631992724, "compression_ratio": 1.8865546218487395, "no_speech_prob": 3.0415792480198434e-06}, {"id": 972, "seek": 621496, "start": 6221.12, "end": 6225.96, "text": " of code and I'll grab the same list of columns we had before in the dependent variable to", "tokens": [295, 3089, 293, 286, 603, 4444, 264, 912, 1329, 295, 13766, 321, 632, 949, 294, 264, 12334, 7006, 281], "temperature": 0.0, "avg_logprob": -0.13761567631992724, "compression_ratio": 1.8865546218487395, "no_speech_prob": 3.0415792480198434e-06}, {"id": 973, "seek": 621496, "start": 6225.96, "end": 6233.0, "text": " get the same data frame now as we've discussed for categorical columns we probably want to", "tokens": [483, 264, 912, 1412, 3920, 586, 382, 321, 600, 7152, 337, 19250, 804, 13766, 321, 1391, 528, 281], "temperature": 0.0, "avg_logprob": -0.13761567631992724, "compression_ratio": 1.8865546218487395, "no_speech_prob": 3.0415792480198434e-06}, {"id": 974, "seek": 621496, "start": 6233.0, "end": 6238.2, "text": " use embeddings so to create embeddings we need to know which columns should be treated", "tokens": [764, 12240, 29432, 370, 281, 1884, 12240, 29432, 321, 643, 281, 458, 597, 13766, 820, 312, 8668], "temperature": 0.0, "avg_logprob": -0.13761567631992724, "compression_ratio": 1.8865546218487395, "no_speech_prob": 3.0415792480198434e-06}, {"id": 975, "seek": 621496, "start": 6238.2, "end": 6244.12, "text": " as categorical variables and as we discussed we can use con cat split for that one of the", "tokens": [382, 19250, 804, 9102, 293, 382, 321, 7152, 321, 393, 764, 416, 3857, 7472, 337, 300, 472, 295, 264], "temperature": 0.0, "avg_logprob": -0.13761567631992724, "compression_ratio": 1.8865546218487395, "no_speech_prob": 3.0415792480198434e-06}, {"id": 976, "seek": 624412, "start": 6244.12, "end": 6250.72, "text": " useful things we can pass that is the maximum cardinality so max card equals 9000 means", "tokens": [4420, 721, 321, 393, 1320, 300, 307, 264, 6674, 2920, 259, 1860, 370, 11469, 2920, 6915, 1722, 1360, 1355], "temperature": 0.0, "avg_logprob": -0.09527520016506985, "compression_ratio": 1.77734375, "no_speech_prob": 3.807009534284589e-07}, {"id": 977, "seek": 624412, "start": 6250.72, "end": 6256.28, "text": " if there's a column with more than 9000 levels you should treat it as continuous and if it's", "tokens": [498, 456, 311, 257, 7738, 365, 544, 813, 1722, 1360, 4358, 291, 820, 2387, 309, 382, 10957, 293, 498, 309, 311], "temperature": 0.0, "avg_logprob": -0.09527520016506985, "compression_ratio": 1.77734375, "no_speech_prob": 3.807009534284589e-07}, {"id": 978, "seek": 624412, "start": 6256.28, "end": 6262.64, "text": " got less than 9000 levels which it is categorical so that's you know it's a simple little function", "tokens": [658, 1570, 813, 1722, 1360, 4358, 597, 309, 307, 19250, 804, 370, 300, 311, 291, 458, 309, 311, 257, 2199, 707, 2445], "temperature": 0.0, "avg_logprob": -0.09527520016506985, "compression_ratio": 1.77734375, "no_speech_prob": 3.807009534284589e-07}, {"id": 979, "seek": 624412, "start": 6262.64, "end": 6266.96, "text": " that just checks the cardinality and splits them based on how many discrete levels they", "tokens": [300, 445, 13834, 264, 2920, 259, 1860, 293, 37741, 552, 2361, 322, 577, 867, 27706, 4358, 436], "temperature": 0.0, "avg_logprob": -0.09527520016506985, "compression_ratio": 1.77734375, "no_speech_prob": 3.807009534284589e-07}, {"id": 980, "seek": 624412, "start": 6266.96, "end": 6272.5599999999995, "text": " have and of course that their data type if it's not actually a numeric data type it has", "tokens": [362, 293, 295, 1164, 300, 641, 1412, 2010, 498, 309, 311, 406, 767, 257, 7866, 299, 1412, 2010, 309, 575], "temperature": 0.0, "avg_logprob": -0.09527520016506985, "compression_ratio": 1.77734375, "no_speech_prob": 3.807009534284589e-07}, {"id": 981, "seek": 627256, "start": 6272.56, "end": 6286.56, "text": " to be categorical so there's our there's our split and then from there what we can do is", "tokens": [281, 312, 19250, 804, 370, 456, 311, 527, 456, 311, 527, 7472, 293, 550, 490, 456, 437, 321, 393, 360, 307], "temperature": 0.0, "avg_logprob": -0.09209267298380534, "compression_ratio": 1.745, "no_speech_prob": 1.2098635124857537e-06}, {"id": 982, "seek": 627256, "start": 6286.56, "end": 6290.360000000001, "text": " we can say oh we've got to be a bit careful of sale elapsed because actually sale elapsed", "tokens": [321, 393, 584, 1954, 321, 600, 658, 281, 312, 257, 857, 5026, 295, 8680, 806, 2382, 292, 570, 767, 8680, 806, 2382, 292], "temperature": 0.0, "avg_logprob": -0.09209267298380534, "compression_ratio": 1.745, "no_speech_prob": 1.2098635124857537e-06}, {"id": 983, "seek": 627256, "start": 6290.360000000001, "end": 6294.96, "text": " I think has less than 9000 categories but we definitely don't want to use that as a", "tokens": [286, 519, 575, 1570, 813, 1722, 1360, 10479, 457, 321, 2138, 500, 380, 528, 281, 764, 300, 382, 257], "temperature": 0.0, "avg_logprob": -0.09209267298380534, "compression_ratio": 1.745, "no_speech_prob": 1.2098635124857537e-06}, {"id": 984, "seek": 627256, "start": 6294.96, "end": 6298.6, "text": " categorical variable the whole point was to make it that this is something that we can", "tokens": [19250, 804, 7006, 264, 1379, 935, 390, 281, 652, 309, 300, 341, 307, 746, 300, 321, 393], "temperature": 0.0, "avg_logprob": -0.09209267298380534, "compression_ratio": 1.745, "no_speech_prob": 1.2098635124857537e-06}, {"id": 985, "seek": 629860, "start": 6298.6, "end": 6303.92, "text": " extrapolate so we certainly anything that's kind of time-dependent or we think that we", "tokens": [48224, 473, 370, 321, 3297, 1340, 300, 311, 733, 295, 565, 12, 36763, 317, 420, 321, 519, 300, 321], "temperature": 0.0, "avg_logprob": -0.10904144379029791, "compression_ratio": 1.6451612903225807, "no_speech_prob": 1.1365599448254216e-06}, {"id": 986, "seek": 629860, "start": 6303.92, "end": 6310.96, "text": " might see things outside the range of inputs in the training data we should make them continuous", "tokens": [1062, 536, 721, 2380, 264, 3613, 295, 15743, 294, 264, 3097, 1412, 321, 820, 652, 552, 10957], "temperature": 0.0, "avg_logprob": -0.10904144379029791, "compression_ratio": 1.6451612903225807, "no_speech_prob": 1.1365599448254216e-06}, {"id": 987, "seek": 629860, "start": 6310.96, "end": 6316.280000000001, "text": " variables so let's make sale elapsed put it in continuous neural net and remove it from", "tokens": [9102, 370, 718, 311, 652, 8680, 806, 2382, 292, 829, 309, 294, 10957, 18161, 2533, 293, 4159, 309, 490], "temperature": 0.0, "avg_logprob": -0.10904144379029791, "compression_ratio": 1.6451612903225807, "no_speech_prob": 1.1365599448254216e-06}, {"id": 988, "seek": 629860, "start": 6316.280000000001, "end": 6324.4400000000005, "text": " categorical. So here's the number of unique levels this is from pandas for everything", "tokens": [19250, 804, 13, 407, 510, 311, 264, 1230, 295, 3845, 4358, 341, 307, 490, 4565, 296, 337, 1203], "temperature": 0.0, "avg_logprob": -0.10904144379029791, "compression_ratio": 1.6451612903225807, "no_speech_prob": 1.1365599448254216e-06}, {"id": 989, "seek": 632444, "start": 6324.44, "end": 6329.839999999999, "text": " in our neural net data set for the categorical variables and I get a bit nervous when I see", "tokens": [294, 527, 18161, 2533, 1412, 992, 337, 264, 19250, 804, 9102, 293, 286, 483, 257, 857, 6296, 562, 286, 536], "temperature": 0.0, "avg_logprob": -0.10377743922242331, "compression_ratio": 1.8458333333333334, "no_speech_prob": 8.579216910220566e-07}, {"id": 990, "seek": 632444, "start": 6329.839999999999, "end": 6334.32, "text": " these really high numbers so I don't want to have too many things with like lots and", "tokens": [613, 534, 1090, 3547, 370, 286, 500, 380, 528, 281, 362, 886, 867, 721, 365, 411, 3195, 293], "temperature": 0.0, "avg_logprob": -0.10377743922242331, "compression_ratio": 1.8458333333333334, "no_speech_prob": 8.579216910220566e-07}, {"id": 991, "seek": 632444, "start": 6334.32, "end": 6341.2, "text": " lots of categories. The reason I don't want lots of things with lots and lots of categories", "tokens": [3195, 295, 10479, 13, 440, 1778, 286, 500, 380, 528, 3195, 295, 721, 365, 3195, 293, 3195, 295, 10479], "temperature": 0.0, "avg_logprob": -0.10377743922242331, "compression_ratio": 1.8458333333333334, "no_speech_prob": 8.579216910220566e-07}, {"id": 992, "seek": 632444, "start": 6341.2, "end": 6345.759999999999, "text": " is just they're going to take up a lot of parameters because in an embedding matrix", "tokens": [307, 445, 436, 434, 516, 281, 747, 493, 257, 688, 295, 9834, 570, 294, 364, 12240, 3584, 8141], "temperature": 0.0, "avg_logprob": -0.10377743922242331, "compression_ratio": 1.8458333333333334, "no_speech_prob": 8.579216910220566e-07}, {"id": 993, "seek": 632444, "start": 6345.759999999999, "end": 6349.96, "text": " this is you know every one of these is a row in an embedding matrix. In this case I notice", "tokens": [341, 307, 291, 458, 633, 472, 295, 613, 307, 257, 5386, 294, 364, 12240, 3584, 8141, 13, 682, 341, 1389, 286, 3449], "temperature": 0.0, "avg_logprob": -0.10377743922242331, "compression_ratio": 1.8458333333333334, "no_speech_prob": 8.579216910220566e-07}, {"id": 994, "seek": 634996, "start": 6349.96, "end": 6355.52, "text": " model ID and model desk might be describing something very similar so I'd quite like to", "tokens": [2316, 7348, 293, 2316, 10026, 1062, 312, 16141, 746, 588, 2531, 370, 286, 1116, 1596, 411, 281], "temperature": 0.0, "avg_logprob": -0.08683637640942102, "compression_ratio": 1.674641148325359, "no_speech_prob": 1.5056970141813508e-06}, {"id": 995, "seek": 634996, "start": 6355.52, "end": 6359.4800000000005, "text": " find out if I could get rid of one and an easy way to do that would be to use a random", "tokens": [915, 484, 498, 286, 727, 483, 3973, 295, 472, 293, 364, 1858, 636, 281, 360, 300, 576, 312, 281, 764, 257, 4974], "temperature": 0.0, "avg_logprob": -0.08683637640942102, "compression_ratio": 1.674641148325359, "no_speech_prob": 1.5056970141813508e-06}, {"id": 996, "seek": 634996, "start": 6359.4800000000005, "end": 6371.04, "text": " forest. So let's try removing the model desk and let's create a random forest and let's", "tokens": [6719, 13, 407, 718, 311, 853, 12720, 264, 2316, 10026, 293, 718, 311, 1884, 257, 4974, 6719, 293, 718, 311], "temperature": 0.0, "avg_logprob": -0.08683637640942102, "compression_ratio": 1.674641148325359, "no_speech_prob": 1.5056970141813508e-06}, {"id": 997, "seek": 634996, "start": 6371.04, "end": 6377.08, "text": " see what happens and oh it's actually a tiny bit better and certainly not worse so that", "tokens": [536, 437, 2314, 293, 1954, 309, 311, 767, 257, 5870, 857, 1101, 293, 3297, 406, 5324, 370, 300], "temperature": 0.0, "avg_logprob": -0.08683637640942102, "compression_ratio": 1.674641148325359, "no_speech_prob": 1.5056970141813508e-06}, {"id": 998, "seek": 637708, "start": 6377.08, "end": 6382.84, "text": " suggests that we can actually get rid of one of these levels or one of these variables.", "tokens": [13409, 300, 321, 393, 767, 483, 3973, 295, 472, 295, 613, 4358, 420, 472, 295, 613, 9102, 13], "temperature": 0.0, "avg_logprob": -0.11267952607056805, "compression_ratio": 1.776, "no_speech_prob": 1.2289161759326817e-06}, {"id": 999, "seek": 637708, "start": 6382.84, "end": 6387.24, "text": " So let's get rid of that one and so now we can create a tabular pandas object just like", "tokens": [407, 718, 311, 483, 3973, 295, 300, 472, 293, 370, 586, 321, 393, 1884, 257, 4421, 1040, 4565, 296, 2657, 445, 411], "temperature": 0.0, "avg_logprob": -0.11267952607056805, "compression_ratio": 1.776, "no_speech_prob": 1.2289161759326817e-06}, {"id": 1000, "seek": 637708, "start": 6387.24, "end": 6394.68, "text": " before but this time we're going to add one more processor which is normalize and the", "tokens": [949, 457, 341, 565, 321, 434, 516, 281, 909, 472, 544, 15321, 597, 307, 2710, 1125, 293, 264], "temperature": 0.0, "avg_logprob": -0.11267952607056805, "compression_ratio": 1.776, "no_speech_prob": 1.2289161759326817e-06}, {"id": 1001, "seek": 637708, "start": 6394.68, "end": 6401.32, "text": " reason we need normalize so normalize is subtract the mean divide by the standard deviation.", "tokens": [1778, 321, 643, 2710, 1125, 370, 2710, 1125, 307, 16390, 264, 914, 9845, 538, 264, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.11267952607056805, "compression_ratio": 1.776, "no_speech_prob": 1.2289161759326817e-06}, {"id": 1002, "seek": 637708, "start": 6401.32, "end": 6405.76, "text": " We didn't need that for a random forest because for a random forest we're just looking at", "tokens": [492, 994, 380, 643, 300, 337, 257, 4974, 6719, 570, 337, 257, 4974, 6719, 321, 434, 445, 1237, 412], "temperature": 0.0, "avg_logprob": -0.11267952607056805, "compression_ratio": 1.776, "no_speech_prob": 1.2289161759326817e-06}, {"id": 1003, "seek": 640576, "start": 6405.76, "end": 6410.8, "text": " less than or greater than through our binary splits. So all that matters is the order of", "tokens": [1570, 813, 420, 5044, 813, 807, 527, 17434, 37741, 13, 407, 439, 300, 7001, 307, 264, 1668, 295], "temperature": 0.0, "avg_logprob": -0.11132042080748315, "compression_ratio": 1.8266129032258065, "no_speech_prob": 2.642571644173586e-06}, {"id": 1004, "seek": 640576, "start": 6410.8, "end": 6415.84, "text": " things how they're sorted doesn't matter whether they're super big or super small but it definitely", "tokens": [721, 577, 436, 434, 25462, 1177, 380, 1871, 1968, 436, 434, 1687, 955, 420, 1687, 1359, 457, 309, 2138], "temperature": 0.0, "avg_logprob": -0.11132042080748315, "compression_ratio": 1.8266129032258065, "no_speech_prob": 2.642571644173586e-06}, {"id": 1005, "seek": 640576, "start": 6415.84, "end": 6423.84, "text": " matters for neural nets because we have these linear layers so we don't want to have you", "tokens": [7001, 337, 18161, 36170, 570, 321, 362, 613, 8213, 7914, 370, 321, 500, 380, 528, 281, 362, 291], "temperature": 0.0, "avg_logprob": -0.11132042080748315, "compression_ratio": 1.8266129032258065, "no_speech_prob": 2.642571644173586e-06}, {"id": 1006, "seek": 640576, "start": 6423.84, "end": 6427.400000000001, "text": " know things with kind of crazy distributions with some super big numbers and super small", "tokens": [458, 721, 365, 733, 295, 3219, 37870, 365, 512, 1687, 955, 3547, 293, 1687, 1359], "temperature": 0.0, "avg_logprob": -0.11132042080748315, "compression_ratio": 1.8266129032258065, "no_speech_prob": 2.642571644173586e-06}, {"id": 1007, "seek": 640576, "start": 6427.400000000001, "end": 6432.280000000001, "text": " numbers because it's not going to work. So it's always a good idea to normalize things", "tokens": [3547, 570, 309, 311, 406, 516, 281, 589, 13, 407, 309, 311, 1009, 257, 665, 1558, 281, 2710, 1125, 721], "temperature": 0.0, "avg_logprob": -0.11132042080748315, "compression_ratio": 1.8266129032258065, "no_speech_prob": 2.642571644173586e-06}, {"id": 1008, "seek": 643228, "start": 6432.28, "end": 6439.84, "text": " neural nets where we can do that in a tabular neural net by using the normalize tabular", "tokens": [18161, 36170, 689, 321, 393, 360, 300, 294, 257, 4421, 1040, 18161, 2533, 538, 1228, 264, 2710, 1125, 4421, 1040], "temperature": 0.0, "avg_logprob": -0.14823557609735533, "compression_ratio": 1.8272251308900525, "no_speech_prob": 1.5294111790353782e-06}, {"id": 1009, "seek": 643228, "start": 6439.84, "end": 6444.5599999999995, "text": " proc. So we can do the same thing that we did before with creating our tabular pandas", "tokens": [9510, 13, 407, 321, 393, 360, 264, 912, 551, 300, 321, 630, 949, 365, 4084, 527, 4421, 1040, 4565, 296], "temperature": 0.0, "avg_logprob": -0.14823557609735533, "compression_ratio": 1.8272251308900525, "no_speech_prob": 1.5294111790353782e-06}, {"id": 1010, "seek": 643228, "start": 6444.5599999999995, "end": 6452.639999999999, "text": " tabular object for the neural net and then we can create data loaders from that with", "tokens": [4421, 1040, 2657, 337, 264, 18161, 2533, 293, 550, 321, 393, 1884, 1412, 3677, 433, 490, 300, 365], "temperature": 0.0, "avg_logprob": -0.14823557609735533, "compression_ratio": 1.8272251308900525, "no_speech_prob": 1.5294111790353782e-06}, {"id": 1011, "seek": 643228, "start": 6452.639999999999, "end": 6457.4, "text": " a batch size and this is a large batch size because tabular models don't generally require", "tokens": [257, 15245, 2744, 293, 341, 307, 257, 2416, 15245, 2744, 570, 4421, 1040, 5245, 500, 380, 5101, 3651], "temperature": 0.0, "avg_logprob": -0.14823557609735533, "compression_ratio": 1.8272251308900525, "no_speech_prob": 1.5294111790353782e-06}, {"id": 1012, "seek": 645740, "start": 6457.4, "end": 6469.24, "text": " nearly as much GPU RAM as a convolutional neural net or something or an RNN or something.", "tokens": [6217, 382, 709, 18407, 14561, 382, 257, 45216, 304, 18161, 2533, 420, 746, 420, 364, 45702, 45, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.18379610501802884, "compression_ratio": 1.4480874316939891, "no_speech_prob": 1.1079033868099941e-07}, {"id": 1013, "seek": 645740, "start": 6469.24, "end": 6472.92, "text": " Since the regression model we're going to want our range so let's find the minimum and", "tokens": [4162, 264, 24590, 2316, 321, 434, 516, 281, 528, 527, 3613, 370, 718, 311, 915, 264, 7285, 293], "temperature": 0.0, "avg_logprob": -0.18379610501802884, "compression_ratio": 1.4480874316939891, "no_speech_prob": 1.1079033868099941e-07}, {"id": 1014, "seek": 645740, "start": 6472.92, "end": 6480.48, "text": " maximum of our dependent variable. And we can now go ahead and create a tabular learner.", "tokens": [6674, 295, 527, 12334, 7006, 13, 400, 321, 393, 586, 352, 2286, 293, 1884, 257, 4421, 1040, 33347, 13], "temperature": 0.0, "avg_logprob": -0.18379610501802884, "compression_ratio": 1.4480874316939891, "no_speech_prob": 1.1079033868099941e-07}, {"id": 1015, "seek": 648048, "start": 6480.48, "end": 6488.36, "text": " So our tabular learner is going to take our data loaders our way range how many activations", "tokens": [407, 527, 4421, 1040, 33347, 307, 516, 281, 747, 527, 1412, 3677, 433, 527, 636, 3613, 577, 867, 2430, 763], "temperature": 0.0, "avg_logprob": -0.15413305261632898, "compression_ratio": 1.6761904761904762, "no_speech_prob": 1.467734449533964e-07}, {"id": 1016, "seek": 648048, "start": 6488.36, "end": 6493.16, "text": " do you want in each of the linear layers and so you can have as many linear layers as you", "tokens": [360, 291, 528, 294, 1184, 295, 264, 8213, 7914, 293, 370, 291, 393, 362, 382, 867, 8213, 7914, 382, 291], "temperature": 0.0, "avg_logprob": -0.15413305261632898, "compression_ratio": 1.6761904761904762, "no_speech_prob": 1.467734449533964e-07}, {"id": 1017, "seek": 648048, "start": 6493.16, "end": 6500.16, "text": " like here. How many outputs are there? So this is a regression with a single output", "tokens": [411, 510, 13, 1012, 867, 23930, 366, 456, 30, 407, 341, 307, 257, 24590, 365, 257, 2167, 5598], "temperature": 0.0, "avg_logprob": -0.15413305261632898, "compression_ratio": 1.6761904761904762, "no_speech_prob": 1.467734449533964e-07}, {"id": 1018, "seek": 648048, "start": 6500.16, "end": 6508.32, "text": " and what loss function do you want? We can use lrFind and then we can go ahead and use", "tokens": [293, 437, 4470, 2445, 360, 291, 528, 30, 492, 393, 764, 287, 81, 37, 471, 293, 550, 321, 393, 352, 2286, 293, 764], "temperature": 0.0, "avg_logprob": -0.15413305261632898, "compression_ratio": 1.6761904761904762, "no_speech_prob": 1.467734449533964e-07}, {"id": 1019, "seek": 650832, "start": 6508.32, "end": 6513.599999999999, "text": " fit1cycle. There's no pre-trained model obviously because this is not something where people", "tokens": [3318, 16, 14796, 13, 821, 311, 572, 659, 12, 17227, 2001, 2316, 2745, 570, 341, 307, 406, 746, 689, 561], "temperature": 0.0, "avg_logprob": -0.19220367431640625, "compression_ratio": 1.46448087431694, "no_speech_prob": 2.026137735811062e-06}, {"id": 1020, "seek": 650832, "start": 6513.599999999999, "end": 6521.799999999999, "text": " have got pre-trained models for industrial equipment options. So we just use fit1cycle", "tokens": [362, 658, 659, 12, 17227, 2001, 5245, 337, 9987, 5927, 3956, 13, 407, 321, 445, 764, 3318, 16, 14796], "temperature": 0.0, "avg_logprob": -0.19220367431640625, "compression_ratio": 1.46448087431694, "no_speech_prob": 2.026137735811062e-06}, {"id": 1021, "seek": 650832, "start": 6521.799999999999, "end": 6535.44, "text": " and train for a minute and then we can check and our RMSC is 0.226 which here was 0.230.", "tokens": [293, 3847, 337, 257, 3456, 293, 550, 321, 393, 1520, 293, 527, 497, 10288, 34, 307, 1958, 13, 7490, 21, 597, 510, 390, 1958, 13, 17, 3446, 13], "temperature": 0.0, "avg_logprob": -0.19220367431640625, "compression_ratio": 1.46448087431694, "no_speech_prob": 2.026137735811062e-06}, {"id": 1022, "seek": 653544, "start": 6535.44, "end": 6540.48, "text": " So that's amazing we actually have you know straight away a better result and the random", "tokens": [407, 300, 311, 2243, 321, 767, 362, 291, 458, 2997, 1314, 257, 1101, 1874, 293, 264, 4974], "temperature": 0.0, "avg_logprob": -0.16635688145955405, "compression_ratio": 1.69377990430622, "no_speech_prob": 5.682373284798814e-06}, {"id": 1023, "seek": 653544, "start": 6540.48, "end": 6546.04, "text": " forest. It's a little more fussy it took some takes a little bit longer but as you can see", "tokens": [6719, 13, 467, 311, 257, 707, 544, 283, 26394, 309, 1890, 512, 2516, 257, 707, 857, 2854, 457, 382, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.16635688145955405, "compression_ratio": 1.69377990430622, "no_speech_prob": 5.682373284798814e-06}, {"id": 1024, "seek": 653544, "start": 6546.04, "end": 6553.24, "text": " you know for interesting data sets like this we can get some great results with neural", "tokens": [291, 458, 337, 1880, 1412, 6352, 411, 341, 321, 393, 483, 512, 869, 3542, 365, 18161], "temperature": 0.0, "avg_logprob": -0.16635688145955405, "compression_ratio": 1.69377990430622, "no_speech_prob": 5.682373284798814e-06}, {"id": 1025, "seek": 653544, "start": 6553.24, "end": 6564.599999999999, "text": " nets. So here's something else we could do though. The random forest and the neural net", "tokens": [36170, 13, 407, 510, 311, 746, 1646, 321, 727, 360, 1673, 13, 440, 4974, 6719, 293, 264, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.16635688145955405, "compression_ratio": 1.69377990430622, "no_speech_prob": 5.682373284798814e-06}, {"id": 1026, "seek": 656460, "start": 6564.6, "end": 6569.04, "text": " they each have their own pros and cons. There's some things they're good at and there's some", "tokens": [436, 1184, 362, 641, 1065, 6267, 293, 1014, 13, 821, 311, 512, 721, 436, 434, 665, 412, 293, 456, 311, 512], "temperature": 0.0, "avg_logprob": -0.1539342110617119, "compression_ratio": 1.879668049792531, "no_speech_prob": 1.6536858993276837e-06}, {"id": 1027, "seek": 656460, "start": 6569.04, "end": 6575.160000000001, "text": " they're less good at. So maybe we can get the best of both worlds and a really easy", "tokens": [436, 434, 1570, 665, 412, 13, 407, 1310, 321, 393, 483, 264, 1151, 295, 1293, 13401, 293, 257, 534, 1858], "temperature": 0.0, "avg_logprob": -0.1539342110617119, "compression_ratio": 1.879668049792531, "no_speech_prob": 1.6536858993276837e-06}, {"id": 1028, "seek": 656460, "start": 6575.160000000001, "end": 6580.400000000001, "text": " way to do that is to use ensemble. So we've already seen that a random forest is a decision", "tokens": [636, 281, 360, 300, 307, 281, 764, 19492, 13, 407, 321, 600, 1217, 1612, 300, 257, 4974, 6719, 307, 257, 3537], "temperature": 0.0, "avg_logprob": -0.1539342110617119, "compression_ratio": 1.879668049792531, "no_speech_prob": 1.6536858993276837e-06}, {"id": 1029, "seek": 656460, "start": 6580.400000000001, "end": 6584.8, "text": " tree ensemble but now we can put that into another ensemble. We're going to have an ensemble", "tokens": [4230, 19492, 457, 586, 321, 393, 829, 300, 666, 1071, 19492, 13, 492, 434, 516, 281, 362, 364, 19492], "temperature": 0.0, "avg_logprob": -0.1539342110617119, "compression_ratio": 1.879668049792531, "no_speech_prob": 1.6536858993276837e-06}, {"id": 1030, "seek": 656460, "start": 6584.8, "end": 6591.200000000001, "text": " of the random forest and a neural net. There's lots of super fancy ways you can do that but", "tokens": [295, 264, 4974, 6719, 293, 257, 18161, 2533, 13, 821, 311, 3195, 295, 1687, 10247, 2098, 291, 393, 360, 300, 457], "temperature": 0.0, "avg_logprob": -0.1539342110617119, "compression_ratio": 1.879668049792531, "no_speech_prob": 1.6536858993276837e-06}, {"id": 1031, "seek": 659120, "start": 6591.2, "end": 6597.24, "text": " a really simple way is to take the average. So sum up the predictions from the two models", "tokens": [257, 534, 2199, 636, 307, 281, 747, 264, 4274, 13, 407, 2408, 493, 264, 21264, 490, 264, 732, 5245], "temperature": 0.0, "avg_logprob": -0.1438957959756084, "compression_ratio": 1.7178217821782178, "no_speech_prob": 9.422431617167604e-07}, {"id": 1032, "seek": 659120, "start": 6597.24, "end": 6603.0, "text": " divide by two and use that as prediction. So that's our ensemble prediction is just", "tokens": [9845, 538, 732, 293, 764, 300, 382, 17630, 13, 407, 300, 311, 527, 19492, 17630, 307, 445], "temperature": 0.0, "avg_logprob": -0.1438957959756084, "compression_ratio": 1.7178217821782178, "no_speech_prob": 9.422431617167604e-07}, {"id": 1033, "seek": 659120, "start": 6603.0, "end": 6608.24, "text": " literally the average of the random forest prediction and the neural net prediction and", "tokens": [3736, 264, 4274, 295, 264, 4974, 6719, 17630, 293, 264, 18161, 2533, 17630, 293], "temperature": 0.0, "avg_logprob": -0.1438957959756084, "compression_ratio": 1.7178217821782178, "no_speech_prob": 9.422431617167604e-07}, {"id": 1034, "seek": 659120, "start": 6608.24, "end": 6621.12, "text": " that gives us 0.223 versus 0.226. So how good is that? Well it's a little hard to say", "tokens": [300, 2709, 505, 1958, 13, 7490, 18, 5717, 1958, 13, 7490, 21, 13, 407, 577, 665, 307, 300, 30, 1042, 309, 311, 257, 707, 1152, 281, 584], "temperature": 0.0, "avg_logprob": -0.1438957959756084, "compression_ratio": 1.7178217821782178, "no_speech_prob": 9.422431617167604e-07}, {"id": 1035, "seek": 662112, "start": 6621.12, "end": 6626.48, "text": " because unfortunately this competition is old enough that we can't even submit to it", "tokens": [570, 7015, 341, 6211, 307, 1331, 1547, 300, 321, 393, 380, 754, 10315, 281, 309], "temperature": 0.0, "avg_logprob": -0.07704026754512343, "compression_ratio": 1.624413145539906, "no_speech_prob": 7.071849267958896e-06}, {"id": 1036, "seek": 662112, "start": 6626.48, "end": 6632.16, "text": " and find out how we would have gone on Kaggle. So we don't really know and so we're relying", "tokens": [293, 915, 484, 577, 321, 576, 362, 2780, 322, 48751, 22631, 13, 407, 321, 500, 380, 534, 458, 293, 370, 321, 434, 24140], "temperature": 0.0, "avg_logprob": -0.07704026754512343, "compression_ratio": 1.624413145539906, "no_speech_prob": 7.071849267958896e-06}, {"id": 1037, "seek": 662112, "start": 6632.16, "end": 6638.36, "text": " on our own validation set but it's quite a bit better than even the first place score", "tokens": [322, 527, 1065, 24071, 992, 457, 309, 311, 1596, 257, 857, 1101, 813, 754, 264, 700, 1081, 6175], "temperature": 0.0, "avg_logprob": -0.07704026754512343, "compression_ratio": 1.624413145539906, "no_speech_prob": 7.071849267958896e-06}, {"id": 1038, "seek": 662112, "start": 6638.36, "end": 6646.5599999999995, "text": " on the test set. So if the validation set is you know doing a good job then this is", "tokens": [322, 264, 1500, 992, 13, 407, 498, 264, 24071, 992, 307, 291, 458, 884, 257, 665, 1691, 550, 341, 307], "temperature": 0.0, "avg_logprob": -0.07704026754512343, "compression_ratio": 1.624413145539906, "no_speech_prob": 7.071849267958896e-06}, {"id": 1039, "seek": 664656, "start": 6646.56, "end": 6652.76, "text": " a good sign that this is a really really good model which wouldn't necessarily be that surprising", "tokens": [257, 665, 1465, 300, 341, 307, 257, 534, 534, 665, 2316, 597, 2759, 380, 4725, 312, 300, 8830], "temperature": 0.0, "avg_logprob": -0.11788650880376976, "compression_ratio": 1.6727272727272726, "no_speech_prob": 4.425447059475118e-06}, {"id": 1040, "seek": 664656, "start": 6652.76, "end": 6660.8, "text": " because you know in the last few years I guess we've learned a lot about building these kinds", "tokens": [570, 291, 458, 294, 264, 1036, 1326, 924, 286, 2041, 321, 600, 3264, 257, 688, 466, 2390, 613, 3685], "temperature": 0.0, "avg_logprob": -0.11788650880376976, "compression_ratio": 1.6727272727272726, "no_speech_prob": 4.425447059475118e-06}, {"id": 1041, "seek": 664656, "start": 6660.8, "end": 6666.96, "text": " of models and we're kind of taking advantage of a lot of the tricks that have that have", "tokens": [295, 5245, 293, 321, 434, 733, 295, 1940, 5002, 295, 257, 688, 295, 264, 11733, 300, 362, 300, 362], "temperature": 0.0, "avg_logprob": -0.11788650880376976, "compression_ratio": 1.6727272727272726, "no_speech_prob": 4.425447059475118e-06}, {"id": 1042, "seek": 664656, "start": 6666.96, "end": 6672.360000000001, "text": " appeared in recent years and yeah maybe this goes to show that well I think it certainly", "tokens": [8516, 294, 5162, 924, 293, 1338, 1310, 341, 1709, 281, 855, 300, 731, 286, 519, 309, 3297], "temperature": 0.0, "avg_logprob": -0.11788650880376976, "compression_ratio": 1.6727272727272726, "no_speech_prob": 4.425447059475118e-06}, {"id": 1043, "seek": 667236, "start": 6672.36, "end": 6682.32, "text": " goes to show that both random forests and neural nets have a lot to offer and try both", "tokens": [1709, 281, 855, 300, 1293, 4974, 21700, 293, 18161, 36170, 362, 257, 688, 281, 2626, 293, 853, 1293], "temperature": 0.0, "avg_logprob": -0.10035625371066006, "compression_ratio": 1.5423728813559323, "no_speech_prob": 1.4367418543770327e-06}, {"id": 1044, "seek": 667236, "start": 6682.32, "end": 6692.32, "text": " and maybe even find both. We've talked about an approach to ensembling called bagging which", "tokens": [293, 1310, 754, 915, 1293, 13, 492, 600, 2825, 466, 364, 3109, 281, 12567, 2504, 1688, 1219, 3411, 3249, 597], "temperature": 0.0, "avg_logprob": -0.10035625371066006, "compression_ratio": 1.5423728813559323, "no_speech_prob": 1.4367418543770327e-06}, {"id": 1045, "seek": 667236, "start": 6692.32, "end": 6699.32, "text": " is where we train lots of models on different subsets of the data like the average of. Another", "tokens": [307, 689, 321, 3847, 3195, 295, 5245, 322, 819, 2090, 1385, 295, 264, 1412, 411, 264, 4274, 295, 13, 3996], "temperature": 0.0, "avg_logprob": -0.10035625371066006, "compression_ratio": 1.5423728813559323, "no_speech_prob": 1.4367418543770327e-06}, {"id": 1046, "seek": 669932, "start": 6699.32, "end": 6706.679999999999, "text": " approach to ensembling and particularly ensembling of trees is called boosting and boosting involves", "tokens": [3109, 281, 12567, 2504, 1688, 293, 4098, 12567, 2504, 1688, 295, 5852, 307, 1219, 43117, 293, 43117, 11626], "temperature": 0.0, "avg_logprob": -0.14920645874816102, "compression_ratio": 1.82, "no_speech_prob": 1.370948666590266e-06}, {"id": 1047, "seek": 669932, "start": 6706.679999999999, "end": 6713.04, "text": " training a small model which underfits your data set so maybe like just have a very small", "tokens": [3097, 257, 1359, 2316, 597, 833, 13979, 428, 1412, 992, 370, 1310, 411, 445, 362, 257, 588, 1359], "temperature": 0.0, "avg_logprob": -0.14920645874816102, "compression_ratio": 1.82, "no_speech_prob": 1.370948666590266e-06}, {"id": 1048, "seek": 669932, "start": 6713.04, "end": 6720.759999999999, "text": " number of beef nodes and then you calculate the predictions using the small model and", "tokens": [1230, 295, 9256, 13891, 293, 550, 291, 8873, 264, 21264, 1228, 264, 1359, 2316, 293], "temperature": 0.0, "avg_logprob": -0.14920645874816102, "compression_ratio": 1.82, "no_speech_prob": 1.370948666590266e-06}, {"id": 1049, "seek": 669932, "start": 6720.759999999999, "end": 6725.0, "text": " then you subtract the predictions from the targets so these are kind of like the errors", "tokens": [550, 291, 16390, 264, 21264, 490, 264, 12911, 370, 613, 366, 733, 295, 411, 264, 13603], "temperature": 0.0, "avg_logprob": -0.14920645874816102, "compression_ratio": 1.82, "no_speech_prob": 1.370948666590266e-06}, {"id": 1050, "seek": 672500, "start": 6725.0, "end": 6732.56, "text": " of your small underfit model we call them residual and then go back to step one but", "tokens": [295, 428, 1359, 833, 6845, 2316, 321, 818, 552, 27980, 293, 550, 352, 646, 281, 1823, 472, 457], "temperature": 0.0, "avg_logprob": -0.11460104966774964, "compression_ratio": 1.7626262626262625, "no_speech_prob": 3.340510147609166e-06}, {"id": 1051, "seek": 672500, "start": 6732.56, "end": 6738.64, "text": " now instead of using the original targets use the residuals to train a small model which", "tokens": [586, 2602, 295, 1228, 264, 3380, 12911, 764, 264, 27980, 82, 281, 3847, 257, 1359, 2316, 597], "temperature": 0.0, "avg_logprob": -0.11460104966774964, "compression_ratio": 1.7626262626262625, "no_speech_prob": 3.340510147609166e-06}, {"id": 1052, "seek": 672500, "start": 6738.64, "end": 6747.36, "text": " underfits your data set attempting to predict residuals and do that again and then until", "tokens": [833, 13979, 428, 1412, 992, 22001, 281, 6069, 27980, 82, 293, 360, 300, 797, 293, 550, 1826], "temperature": 0.0, "avg_logprob": -0.11460104966774964, "compression_ratio": 1.7626262626262625, "no_speech_prob": 3.340510147609166e-06}, {"id": 1053, "seek": 672500, "start": 6747.36, "end": 6753.36, "text": " you reach some stopping criterion such as the maximum number of trees now you that will", "tokens": [291, 2524, 512, 12767, 46691, 1270, 382, 264, 6674, 1230, 295, 5852, 586, 291, 300, 486], "temperature": 0.0, "avg_logprob": -0.11460104966774964, "compression_ratio": 1.7626262626262625, "no_speech_prob": 3.340510147609166e-06}, {"id": 1054, "seek": 675336, "start": 6753.36, "end": 6760.5, "text": " leave you with a bunch of models which you don't average but which you sum because each", "tokens": [1856, 291, 365, 257, 3840, 295, 5245, 597, 291, 500, 380, 4274, 457, 597, 291, 2408, 570, 1184], "temperature": 0.0, "avg_logprob": -0.08881124608656939, "compression_ratio": 1.8917525773195876, "no_speech_prob": 1.0511449772820924e-06}, {"id": 1055, "seek": 675336, "start": 6760.5, "end": 6766.5199999999995, "text": " one is creating a model that's based on the residual of the previous one so we've subtracted", "tokens": [472, 307, 4084, 257, 2316, 300, 311, 2361, 322, 264, 27980, 295, 264, 3894, 472, 370, 321, 600, 16390, 292], "temperature": 0.0, "avg_logprob": -0.08881124608656939, "compression_ratio": 1.8917525773195876, "no_speech_prob": 1.0511449772820924e-06}, {"id": 1056, "seek": 675336, "start": 6766.5199999999995, "end": 6771.2, "text": " the predictions of each new tree from the residuals of the previous tree so the residuals", "tokens": [264, 21264, 295, 1184, 777, 4230, 490, 264, 27980, 82, 295, 264, 3894, 4230, 370, 264, 27980, 82], "temperature": 0.0, "avg_logprob": -0.08881124608656939, "compression_ratio": 1.8917525773195876, "no_speech_prob": 1.0511449772820924e-06}, {"id": 1057, "seek": 675336, "start": 6771.2, "end": 6776.36, "text": " get smaller and smaller and then to make predictions we just have to do the opposite which is to", "tokens": [483, 4356, 293, 4356, 293, 550, 281, 652, 21264, 321, 445, 362, 281, 360, 264, 6182, 597, 307, 281], "temperature": 0.0, "avg_logprob": -0.08881124608656939, "compression_ratio": 1.8917525773195876, "no_speech_prob": 1.0511449772820924e-06}, {"id": 1058, "seek": 677636, "start": 6776.36, "end": 6786.44, "text": " add them all together so there's lots of variance of this but you'll see things like GBMs or", "tokens": [909, 552, 439, 1214, 370, 456, 311, 3195, 295, 21977, 295, 341, 457, 291, 603, 536, 721, 411, 460, 18345, 82, 420], "temperature": 0.0, "avg_logprob": -0.22006674510676685, "compression_ratio": 1.6367924528301887, "no_speech_prob": 1.0348496743972646e-06}, {"id": 1059, "seek": 677636, "start": 6786.44, "end": 6794.16, "text": " gradient boosted machines or GPTTs or gradient boosted decision trees and there's lots of", "tokens": [16235, 9194, 292, 8379, 420, 26039, 28178, 82, 420, 16235, 9194, 292, 3537, 5852, 293, 456, 311, 3195, 295], "temperature": 0.0, "avg_logprob": -0.22006674510676685, "compression_ratio": 1.6367924528301887, "no_speech_prob": 1.0348496743972646e-06}, {"id": 1060, "seek": 677636, "start": 6794.16, "end": 6800.839999999999, "text": " minor details around you know and are insignificant details but the basic idea is is what I've", "tokens": [6696, 4365, 926, 291, 458, 293, 366, 43685, 4365, 457, 264, 3875, 1558, 307, 307, 437, 286, 600], "temperature": 0.0, "avg_logprob": -0.22006674510676685, "compression_ratio": 1.6367924528301887, "no_speech_prob": 1.0348496743972646e-06}, {"id": 1061, "seek": 677636, "start": 6800.839999999999, "end": 6805.839999999999, "text": " shown to question all right let's take the questions they're dropping", "tokens": [4898, 281, 1168, 439, 558, 718, 311, 747, 264, 1651, 436, 434, 13601], "temperature": 0.0, "avg_logprob": -0.22006674510676685, "compression_ratio": 1.6367924528301887, "no_speech_prob": 1.0348496743972646e-06}, {"id": 1062, "seek": 680584, "start": 6805.84, "end": 6811.400000000001, "text": " features in a model as a way to reduce the complexity of the model and thus reduce overfitting", "tokens": [4122, 294, 257, 2316, 382, 257, 636, 281, 5407, 264, 14024, 295, 264, 2316, 293, 8807, 5407, 670, 69, 2414], "temperature": 0.0, "avg_logprob": -0.20024949532968025, "compression_ratio": 1.523489932885906, "no_speech_prob": 2.710704575292766e-05}, {"id": 1063, "seek": 680584, "start": 6811.400000000001, "end": 6815.64, "text": " is this better than adding some regularization like weight decay to tabular", "tokens": [307, 341, 1101, 813, 5127, 512, 3890, 2144, 411, 3364, 21039, 281, 4421, 1040], "temperature": 0.0, "avg_logprob": -0.20024949532968025, "compression_ratio": 1.523489932885906, "no_speech_prob": 2.710704575292766e-05}, {"id": 1064, "seek": 680584, "start": 6815.64, "end": 6828.4800000000005, "text": " I didn't claim that removed columns to avoid overfitting", "tokens": [286, 994, 380, 3932, 300, 7261, 13766, 281, 5042, 670, 69, 2414], "temperature": 0.0, "avg_logprob": -0.20024949532968025, "compression_ratio": 1.523489932885906, "no_speech_prob": 2.710704575292766e-05}, {"id": 1065, "seek": 682848, "start": 6828.48, "end": 6840.0, "text": " we remove the columns to simplify fewer things to analyze and it should also mean we don't", "tokens": [321, 4159, 264, 13766, 281, 20460, 13366, 721, 281, 12477, 293, 309, 820, 611, 914, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.11627315282821656, "compression_ratio": 1.647887323943662, "no_speech_prob": 3.2886948702071095e-06}, {"id": 1066, "seek": 682848, "start": 6840.0, "end": 6844.48, "text": " need as many trees but there's no particular reason to believe that this will regularize", "tokens": [643, 382, 867, 5852, 457, 456, 311, 572, 1729, 1778, 281, 1697, 300, 341, 486, 3890, 1125], "temperature": 0.0, "avg_logprob": -0.11627315282821656, "compression_ratio": 1.647887323943662, "no_speech_prob": 3.2886948702071095e-06}, {"id": 1067, "seek": 682848, "start": 6844.48, "end": 6849.48, "text": " and well the idea of regularization doesn't necessarily make a lot of sense to random", "tokens": [293, 731, 264, 1558, 295, 3890, 2144, 1177, 380, 4725, 652, 257, 688, 295, 2020, 281, 4974], "temperature": 0.0, "avg_logprob": -0.11627315282821656, "compression_ratio": 1.647887323943662, "no_speech_prob": 3.2886948702071095e-06}, {"id": 1068, "seek": 682848, "start": 6849.48, "end": 6857.48, "text": " forests and always add more trees is there a good heuristic for picking the number of", "tokens": [21700, 293, 1009, 909, 544, 5852, 307, 456, 257, 665, 415, 374, 3142, 337, 8867, 264, 1230, 295], "temperature": 0.0, "avg_logprob": -0.11627315282821656, "compression_ratio": 1.647887323943662, "no_speech_prob": 3.2886948702071095e-06}, {"id": 1069, "seek": 685748, "start": 6857.48, "end": 6869.08, "text": " linear layers in the tabular model not really well if there is I don't know what it is I", "tokens": [8213, 7914, 294, 264, 4421, 1040, 2316, 406, 534, 731, 498, 456, 307, 286, 500, 380, 458, 437, 309, 307, 286], "temperature": 0.0, "avg_logprob": -0.2092178491445688, "compression_ratio": 1.6, "no_speech_prob": 2.726456386881182e-06}, {"id": 1070, "seek": 685748, "start": 6869.08, "end": 6878.08, "text": " guess who through hidden layers works pretty well so you know what I showed those those", "tokens": [2041, 567, 807, 7633, 7914, 1985, 1238, 731, 370, 291, 458, 437, 286, 4712, 729, 729], "temperature": 0.0, "avg_logprob": -0.2092178491445688, "compression_ratio": 1.6, "no_speech_prob": 2.726456386881182e-06}, {"id": 1071, "seek": 685748, "start": 6878.08, "end": 6886.799999999999, "text": " numbers I showed her pretty good for a large ish model a default it uses 200 and 100 so", "tokens": [3547, 286, 4712, 720, 1238, 665, 337, 257, 2416, 307, 71, 2316, 257, 7576, 309, 4960, 2331, 293, 2319, 370], "temperature": 0.0, "avg_logprob": -0.2092178491445688, "compression_ratio": 1.6, "no_speech_prob": 2.726456386881182e-06}, {"id": 1072, "seek": 688680, "start": 6886.8, "end": 6892.400000000001, "text": " maybe start with the default and then go up to 500 and 250 if that into an improvement", "tokens": [1310, 722, 365, 264, 7576, 293, 550, 352, 493, 281, 5923, 293, 11650, 498, 300, 666, 364, 10444], "temperature": 0.0, "avg_logprob": -0.1758723992567796, "compression_ratio": 1.7580645161290323, "no_speech_prob": 1.1478618034743704e-05}, {"id": 1073, "seek": 688680, "start": 6892.400000000001, "end": 6900.4800000000005, "text": " and like just keep doubling them and improving or you run out of memory or time", "tokens": [293, 411, 445, 1066, 33651, 552, 293, 11470, 420, 291, 1190, 484, 295, 4675, 420, 565], "temperature": 0.0, "avg_logprob": -0.1758723992567796, "compression_ratio": 1.7580645161290323, "no_speech_prob": 1.1478618034743704e-05}, {"id": 1074, "seek": 688680, "start": 6900.4800000000005, "end": 6905.12, "text": " main thing to note about boosted models is that there's nothing to stop us from overfitting", "tokens": [2135, 551, 281, 3637, 466, 9194, 292, 5245, 307, 300, 456, 311, 1825, 281, 1590, 505, 490, 670, 69, 2414], "temperature": 0.0, "avg_logprob": -0.1758723992567796, "compression_ratio": 1.7580645161290323, "no_speech_prob": 1.1478618034743704e-05}, {"id": 1075, "seek": 688680, "start": 6905.12, "end": 6911.400000000001, "text": " you add more and more trees a bagging model sort of a random first it's going to get going", "tokens": [291, 909, 544, 293, 544, 5852, 257, 3411, 3249, 2316, 1333, 295, 257, 4974, 700, 309, 311, 516, 281, 483, 516], "temperature": 0.0, "avg_logprob": -0.1758723992567796, "compression_ratio": 1.7580645161290323, "no_speech_prob": 1.1478618034743704e-05}, {"id": 1076, "seek": 688680, "start": 6911.400000000001, "end": 6916.66, "text": " to should generalize better and better is your each time you're using your model which", "tokens": [281, 820, 2674, 1125, 1101, 293, 1101, 307, 428, 1184, 565, 291, 434, 1228, 428, 2316, 597], "temperature": 0.0, "avg_logprob": -0.1758723992567796, "compression_ratio": 1.7580645161290323, "no_speech_prob": 1.1478618034743704e-05}, {"id": 1077, "seek": 691666, "start": 6916.66, "end": 6924.639999999999, "text": " is based on a subset of the day but boosting each model will fit the training set better", "tokens": [307, 2361, 322, 257, 25993, 295, 264, 786, 457, 43117, 1184, 2316, 486, 3318, 264, 3097, 992, 1101], "temperature": 0.0, "avg_logprob": -0.13837095166816085, "compression_ratio": 1.631578947368421, "no_speech_prob": 4.157328021392459e-06}, {"id": 1078, "seek": 691666, "start": 6924.639999999999, "end": 6933.0599999999995, "text": " and better gradually overfit more and more so boosting methods do require generally hyperparameter", "tokens": [293, 1101, 13145, 670, 6845, 544, 293, 544, 370, 43117, 7150, 360, 3651, 5101, 9848, 2181, 335, 2398], "temperature": 0.0, "avg_logprob": -0.13837095166816085, "compression_ratio": 1.631578947368421, "no_speech_prob": 4.157328021392459e-06}, {"id": 1079, "seek": 691666, "start": 6933.0599999999995, "end": 6943.5599999999995, "text": " tuning and fiddling around with you know you certainly have regularization boosting they're", "tokens": [15164, 293, 283, 14273, 1688, 926, 365, 291, 458, 291, 3297, 362, 3890, 2144, 43117, 436, 434], "temperature": 0.0, "avg_logprob": -0.13837095166816085, "compression_ratio": 1.631578947368421, "no_speech_prob": 4.157328021392459e-06}, {"id": 1080, "seek": 694356, "start": 6943.56, "end": 6947.92, "text": " pretty sensitive to their hyper parameters which is why they're not normally my first", "tokens": [1238, 9477, 281, 641, 9848, 9834, 597, 307, 983, 436, 434, 406, 5646, 452, 700], "temperature": 0.0, "avg_logprob": -0.1560402157940442, "compression_ratio": 1.6232558139534883, "no_speech_prob": 3.089474375883583e-06}, {"id": 1081, "seek": 694356, "start": 6947.92, "end": 6956.68, "text": " go-to but they they often they more often win Kaggle competition random forests do like", "tokens": [352, 12, 1353, 457, 436, 436, 2049, 436, 544, 2049, 1942, 48751, 22631, 6211, 4974, 21700, 360, 411], "temperature": 0.0, "avg_logprob": -0.1560402157940442, "compression_ratio": 1.6232558139534883, "no_speech_prob": 3.089474375883583e-06}, {"id": 1082, "seek": 694356, "start": 6956.68, "end": 6964.8, "text": " they tend to be good at getting that last little bit of performance the last thing I'm", "tokens": [436, 3928, 281, 312, 665, 412, 1242, 300, 1036, 707, 857, 295, 3389, 264, 1036, 551, 286, 478], "temperature": 0.0, "avg_logprob": -0.1560402157940442, "compression_ratio": 1.6232558139534883, "no_speech_prob": 3.089474375883583e-06}, {"id": 1083, "seek": 694356, "start": 6964.8, "end": 6971.4400000000005, "text": " going to mention is something super neat which a lot of people don't seem to know exists", "tokens": [516, 281, 2152, 307, 746, 1687, 10654, 597, 257, 688, 295, 561, 500, 380, 1643, 281, 458, 8198], "temperature": 0.0, "avg_logprob": -0.1560402157940442, "compression_ratio": 1.6232558139534883, "no_speech_prob": 3.089474375883583e-06}, {"id": 1084, "seek": 697144, "start": 6971.44, "end": 6977.16, "text": " is a shanks it's super cool which is something from the entity embeddings paper the table", "tokens": [307, 257, 402, 14592, 309, 311, 1687, 1627, 597, 307, 746, 490, 264, 13977, 12240, 29432, 3035, 264, 3199], "temperature": 0.0, "avg_logprob": -0.1560054902107485, "compression_ratio": 1.7179487179487178, "no_speech_prob": 5.626390588986396e-07}, {"id": 1085, "seek": 697144, "start": 6977.16, "end": 6983.919999999999, "text": " from it where what they did was they built a neural network they got the entity embeddings", "tokens": [490, 309, 689, 437, 436, 630, 390, 436, 3094, 257, 18161, 3209, 436, 658, 264, 13977, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.1560054902107485, "compression_ratio": 1.7179487179487178, "no_speech_prob": 5.626390588986396e-07}, {"id": 1086, "seek": 697144, "start": 6983.919999999999, "end": 6995.24, "text": " PE and then they tried a random forest using the entity embeddings as predictors rather", "tokens": [24346, 293, 550, 436, 3031, 257, 4974, 6719, 1228, 264, 13977, 12240, 29432, 382, 6069, 830, 2831], "temperature": 0.0, "avg_logprob": -0.1560054902107485, "compression_ratio": 1.7179487179487178, "no_speech_prob": 5.626390588986396e-07}, {"id": 1087, "seek": 699524, "start": 6995.24, "end": 7003.08, "text": " than the approach I described with just the raw categorical variables and the the error", "tokens": [813, 264, 3109, 286, 7619, 365, 445, 264, 8936, 19250, 804, 9102, 293, 264, 264, 6713], "temperature": 0.0, "avg_logprob": -0.1476763802013178, "compression_ratio": 1.6186046511627907, "no_speech_prob": 1.414465373272833e-06}, {"id": 1088, "seek": 699524, "start": 7003.08, "end": 7011.16, "text": " for a random forest went from 0.16 to 0.11 a huge improvement and very simple method", "tokens": [337, 257, 4974, 6719, 1437, 490, 1958, 13, 6866, 281, 1958, 13, 5348, 257, 2603, 10444, 293, 588, 2199, 3170], "temperature": 0.0, "avg_logprob": -0.1476763802013178, "compression_ratio": 1.6186046511627907, "no_speech_prob": 1.414465373272833e-06}, {"id": 1089, "seek": 699524, "start": 7011.16, "end": 7019.04, "text": " KNN went from 0.29 to 0.11 basically all of the methods when they used entity embeddings", "tokens": [26967, 45, 1437, 490, 1958, 13, 11871, 281, 1958, 13, 5348, 1936, 439, 295, 264, 7150, 562, 436, 1143, 13977, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.1476763802013178, "compression_ratio": 1.6186046511627907, "no_speech_prob": 1.414465373272833e-06}, {"id": 1090, "seek": 699524, "start": 7019.04, "end": 7024.08, "text": " suddenly improved a lot the one thing you you you should try if you have a look at the", "tokens": [5800, 9689, 257, 688, 264, 472, 551, 291, 291, 291, 820, 853, 498, 291, 362, 257, 574, 412, 264], "temperature": 0.0, "avg_logprob": -0.1476763802013178, "compression_ratio": 1.6186046511627907, "no_speech_prob": 1.414465373272833e-06}, {"id": 1091, "seek": 702408, "start": 7024.08, "end": 7030.08, "text": " further research section after the questionnaire is it asks to try to to do this actually take", "tokens": [3052, 2132, 3541, 934, 264, 44702, 307, 309, 8962, 281, 853, 281, 281, 360, 341, 767, 747], "temperature": 0.0, "avg_logprob": -0.16686038511345186, "compression_ratio": 1.6966824644549763, "no_speech_prob": 2.857287881852244e-06}, {"id": 1092, "seek": 702408, "start": 7030.08, "end": 7033.68, "text": " those entity embeddings that we trained in the neural net and use them in the random", "tokens": [729, 13977, 12240, 29432, 300, 321, 8895, 294, 264, 18161, 2533, 293, 764, 552, 294, 264, 4974], "temperature": 0.0, "avg_logprob": -0.16686038511345186, "compression_ratio": 1.6966824644549763, "no_speech_prob": 2.857287881852244e-06}, {"id": 1093, "seek": 702408, "start": 7033.68, "end": 7041.76, "text": " forest and then maybe try on some willing again and see if you can beat the point two", "tokens": [6719, 293, 550, 1310, 853, 322, 512, 4950, 797, 293, 536, 498, 291, 393, 4224, 264, 935, 732], "temperature": 0.0, "avg_logprob": -0.16686038511345186, "compression_ratio": 1.6966824644549763, "no_speech_prob": 2.857287881852244e-06}, {"id": 1094, "seek": 702408, "start": 7041.76, "end": 7049.5199999999995, "text": " two three that we had this is a really nice idea it's like you get you know all the benefits", "tokens": [732, 1045, 300, 321, 632, 341, 307, 257, 534, 1481, 1558, 309, 311, 411, 291, 483, 291, 458, 439, 264, 5311], "temperature": 0.0, "avg_logprob": -0.16686038511345186, "compression_ratio": 1.6966824644549763, "no_speech_prob": 2.857287881852244e-06}, {"id": 1095, "seek": 704952, "start": 7049.52, "end": 7058.280000000001, "text": " of the boosted decision trees but all of the nice features of entity embeddings and so", "tokens": [295, 264, 9194, 292, 3537, 5852, 457, 439, 295, 264, 1481, 4122, 295, 13977, 12240, 29432, 293, 370], "temperature": 0.0, "avg_logprob": -0.15668716430664062, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.0676959618649562e-06}, {"id": 1096, "seek": 704952, "start": 7058.280000000001, "end": 7068.240000000001, "text": " this is something that not enough people seem to be playing with for some reason so overall", "tokens": [341, 307, 746, 300, 406, 1547, 561, 1643, 281, 312, 2433, 365, 337, 512, 1778, 370, 4787], "temperature": 0.0, "avg_logprob": -0.15668716430664062, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.0676959618649562e-06}, {"id": 1097, "seek": 704952, "start": 7068.240000000001, "end": 7072.52, "text": " you know random forests are nice and easy to train that you know they're very resilient", "tokens": [291, 458, 4974, 21700, 366, 1481, 293, 1858, 281, 3847, 300, 291, 458, 436, 434, 588, 23699], "temperature": 0.0, "avg_logprob": -0.15668716430664062, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.0676959618649562e-06}, {"id": 1098, "seek": 707252, "start": 7072.52, "end": 7079.64, "text": " they don't require much pre-processing they train quickly they don't overfit you know", "tokens": [436, 500, 380, 3651, 709, 659, 12, 41075, 278, 436, 3847, 2661, 436, 500, 380, 670, 6845, 291, 458], "temperature": 0.0, "avg_logprob": -0.10422358512878419, "compression_ratio": 1.755, "no_speech_prob": 9.721505875859293e-07}, {"id": 1099, "seek": 707252, "start": 7079.64, "end": 7085.4800000000005, "text": " they can be a little less accurate and they can be a bit slow at inference time because", "tokens": [436, 393, 312, 257, 707, 1570, 8559, 293, 436, 393, 312, 257, 857, 2964, 412, 38253, 565, 570], "temperature": 0.0, "avg_logprob": -0.10422358512878419, "compression_ratio": 1.755, "no_speech_prob": 9.721505875859293e-07}, {"id": 1100, "seek": 707252, "start": 7085.4800000000005, "end": 7091.080000000001, "text": " the inference you have to go through every one of those trees having said that a binary", "tokens": [264, 38253, 291, 362, 281, 352, 807, 633, 472, 295, 729, 5852, 1419, 848, 300, 257, 17434], "temperature": 0.0, "avg_logprob": -0.10422358512878419, "compression_ratio": 1.755, "no_speech_prob": 9.721505875859293e-07}, {"id": 1101, "seek": 707252, "start": 7091.080000000001, "end": 7100.540000000001, "text": " tree can be pretty heavily optimized so you know it is something you can basically create", "tokens": [4230, 393, 312, 1238, 10950, 26941, 370, 291, 458, 309, 307, 746, 291, 393, 1936, 1884], "temperature": 0.0, "avg_logprob": -0.10422358512878419, "compression_ratio": 1.755, "no_speech_prob": 9.721505875859293e-07}, {"id": 1102, "seek": 710054, "start": 7100.54, "end": 7110.6, "text": " a totally compiled version of a tree and they can certainly also be done entirely in parallel", "tokens": [257, 3879, 36548, 3037, 295, 257, 4230, 293, 436, 393, 3297, 611, 312, 1096, 7696, 294, 8952], "temperature": 0.0, "avg_logprob": -0.15696605803474548, "compression_ratio": 1.576271186440678, "no_speech_prob": 3.9278546637433465e-07}, {"id": 1103, "seek": 710054, "start": 7110.6, "end": 7118.54, "text": " so that's something to consider gradient boosting machines are also fast to train on the whole", "tokens": [370, 300, 311, 746, 281, 1949, 16235, 43117, 8379, 366, 611, 2370, 281, 3847, 322, 264, 1379], "temperature": 0.0, "avg_logprob": -0.15696605803474548, "compression_ratio": 1.576271186440678, "no_speech_prob": 3.9278546637433465e-07}, {"id": 1104, "seek": 710054, "start": 7118.54, "end": 7123.92, "text": " but a little more fussy about hyperparameters you have to be careful about overfitting but", "tokens": [457, 257, 707, 544, 283, 26394, 466, 9848, 2181, 335, 6202, 291, 362, 281, 312, 5026, 466, 670, 69, 2414, 457], "temperature": 0.0, "avg_logprob": -0.15696605803474548, "compression_ratio": 1.576271186440678, "no_speech_prob": 3.9278546637433465e-07}, {"id": 1105, "seek": 712392, "start": 7123.92, "end": 7132.28, "text": " a bit more accurate here or nets maybe the fussiest to deal with they've kind of got", "tokens": [257, 857, 544, 8559, 510, 420, 36170, 1310, 264, 34792, 6495, 281, 2028, 365, 436, 600, 733, 295, 658], "temperature": 0.0, "avg_logprob": -0.1416261082603818, "compression_ratio": 1.6712962962962963, "no_speech_prob": 1.816212602534506e-06}, {"id": 1106, "seek": 712392, "start": 7132.28, "end": 7138.2, "text": " the least rules of thumb around or tutorials around saying this is kind of how to do it", "tokens": [264, 1935, 4474, 295, 9298, 926, 420, 17616, 926, 1566, 341, 307, 733, 295, 577, 281, 360, 309], "temperature": 0.0, "avg_logprob": -0.1416261082603818, "compression_ratio": 1.6712962962962963, "no_speech_prob": 1.816212602534506e-06}, {"id": 1107, "seek": 712392, "start": 7138.2, "end": 7144.4800000000005, "text": " it's just a bit a bit newer a little bit less well understood but they can give better results", "tokens": [309, 311, 445, 257, 857, 257, 857, 17628, 257, 707, 857, 1570, 731, 7320, 457, 436, 393, 976, 1101, 3542], "temperature": 0.0, "avg_logprob": -0.1416261082603818, "compression_ratio": 1.6712962962962963, "no_speech_prob": 1.816212602534506e-06}, {"id": 1108, "seek": 712392, "start": 7144.4800000000005, "end": 7148.8, "text": " in many situations than the other two approaches or at least with an ensemble can improve the", "tokens": [294, 867, 6851, 813, 264, 661, 732, 11587, 420, 412, 1935, 365, 364, 19492, 393, 3470, 264], "temperature": 0.0, "avg_logprob": -0.1416261082603818, "compression_ratio": 1.6712962962962963, "no_speech_prob": 1.816212602534506e-06}, {"id": 1109, "seek": 714880, "start": 7148.8, "end": 7154.76, "text": " other two approaches so I would always start with a random part and then see if you can", "tokens": [661, 732, 11587, 370, 286, 576, 1009, 722, 365, 257, 4974, 644, 293, 550, 536, 498, 291, 393], "temperature": 0.0, "avg_logprob": -0.09634433927990141, "compression_ratio": 1.7695473251028806, "no_speech_prob": 2.8130027658335166e-06}, {"id": 1110, "seek": 714880, "start": 7154.76, "end": 7161.68, "text": " beat it using these so yeah why don't you now see if you can find a Kaggle competition", "tokens": [4224, 309, 1228, 613, 370, 1338, 983, 500, 380, 291, 586, 536, 498, 291, 393, 915, 257, 48751, 22631, 6211], "temperature": 0.0, "avg_logprob": -0.09634433927990141, "compression_ratio": 1.7695473251028806, "no_speech_prob": 2.8130027658335166e-06}, {"id": 1111, "seek": 714880, "start": 7161.68, "end": 7166.2, "text": " with tabular data whether it's running now it's a past one and see if you can repeat", "tokens": [365, 4421, 1040, 1412, 1968, 309, 311, 2614, 586, 309, 311, 257, 1791, 472, 293, 536, 498, 291, 393, 7149], "temperature": 0.0, "avg_logprob": -0.09634433927990141, "compression_ratio": 1.7695473251028806, "no_speech_prob": 2.8130027658335166e-06}, {"id": 1112, "seek": 714880, "start": 7166.2, "end": 7171.96, "text": " this process for that and see if you can get in the top 10% of the private leaderboard", "tokens": [341, 1399, 337, 300, 293, 536, 498, 291, 393, 483, 294, 264, 1192, 1266, 4, 295, 264, 4551, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.09634433927990141, "compression_ratio": 1.7695473251028806, "no_speech_prob": 2.8130027658335166e-06}, {"id": 1113, "seek": 714880, "start": 7171.96, "end": 7178.400000000001, "text": " that would be a really great stretch goal at this point implement the decision tree", "tokens": [300, 576, 312, 257, 534, 869, 5985, 3387, 412, 341, 935, 4445, 264, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.09634433927990141, "compression_ratio": 1.7695473251028806, "no_speech_prob": 2.8130027658335166e-06}, {"id": 1114, "seek": 717840, "start": 7178.4, "end": 7182.08, "text": " algorithm yourself I think that's an important one we really understand it and then from", "tokens": [9284, 1803, 286, 519, 300, 311, 364, 1021, 472, 321, 534, 1223, 309, 293, 550, 490], "temperature": 0.0, "avg_logprob": -0.07308341007606656, "compression_ratio": 1.7734375, "no_speech_prob": 3.1381255212181713e-06}, {"id": 1115, "seek": 717840, "start": 7182.08, "end": 7191.32, "text": " there create your own random forest from scratch you might be surprised it's not that hard", "tokens": [456, 1884, 428, 1065, 4974, 6719, 490, 8459, 291, 1062, 312, 6100, 309, 311, 406, 300, 1152], "temperature": 0.0, "avg_logprob": -0.07308341007606656, "compression_ratio": 1.7734375, "no_speech_prob": 3.1381255212181713e-06}, {"id": 1116, "seek": 717840, "start": 7191.32, "end": 7195.48, "text": " and then go and have a look at the tabular model source code and at this point this is", "tokens": [293, 550, 352, 293, 362, 257, 574, 412, 264, 4421, 1040, 2316, 4009, 3089, 293, 412, 341, 935, 341, 307], "temperature": 0.0, "avg_logprob": -0.07308341007606656, "compression_ratio": 1.7734375, "no_speech_prob": 3.1381255212181713e-06}, {"id": 1117, "seek": 717840, "start": 7195.48, "end": 7202.44, "text": " pretty exciting you should find you pretty much know what all the lines do with two exceptions", "tokens": [1238, 4670, 291, 820, 915, 291, 1238, 709, 458, 437, 439, 264, 3876, 360, 365, 732, 22847], "temperature": 0.0, "avg_logprob": -0.07308341007606656, "compression_ratio": 1.7734375, "no_speech_prob": 3.1381255212181713e-06}, {"id": 1118, "seek": 717840, "start": 7202.44, "end": 7206.4, "text": " and if you don't you know dig around and explore and experiment and see if you can figure it", "tokens": [293, 498, 291, 500, 380, 291, 458, 2528, 926, 293, 6839, 293, 5120, 293, 536, 498, 291, 393, 2573, 309], "temperature": 0.0, "avg_logprob": -0.07308341007606656, "compression_ratio": 1.7734375, "no_speech_prob": 3.1381255212181713e-06}, {"id": 1119, "seek": 720640, "start": 7206.4, "end": 7216.16, "text": " out and with that we are I am very excited to say at a point where we've really dug all", "tokens": [484, 293, 365, 300, 321, 366, 286, 669, 588, 2919, 281, 584, 412, 257, 935, 689, 321, 600, 534, 22954, 439], "temperature": 0.0, "avg_logprob": -0.19731019827035756, "compression_ratio": 1.4602272727272727, "no_speech_prob": 4.637833171727834e-06}, {"id": 1120, "seek": 720640, "start": 7216.16, "end": 7224.799999999999, "text": " the way in to the end of these real valuable effective fast AI applications and we're understanding", "tokens": [264, 636, 294, 281, 264, 917, 295, 613, 957, 8263, 4942, 2370, 7318, 5821, 293, 321, 434, 3701], "temperature": 0.0, "avg_logprob": -0.19731019827035756, "compression_ratio": 1.4602272727272727, "no_speech_prob": 4.637833171727834e-06}, {"id": 1121, "seek": 720640, "start": 7224.799999999999, "end": 7226.36, "text": " what's going on inside them.", "tokens": [437, 311, 516, 322, 1854, 552, 13], "temperature": 0.0, "avg_logprob": -0.19731019827035756, "compression_ratio": 1.4602272727272727, "no_speech_prob": 4.637833171727834e-06}, {"id": 1122, "seek": 720640, "start": 7226.36, "end": 7229.36, "text": " Now what should we expect for next week?", "tokens": [823, 437, 820, 321, 2066, 337, 958, 1243, 30], "temperature": 0.0, "avg_logprob": -0.19731019827035756, "compression_ratio": 1.4602272727272727, "no_speech_prob": 4.637833171727834e-06}, {"id": 1123, "seek": 722936, "start": 7229.36, "end": 7238.599999999999, "text": " So for next week we will set NLP and Qtvision and we'll do the same kind of ideas delve", "tokens": [407, 337, 958, 1243, 321, 486, 992, 426, 45196, 293, 1249, 83, 6763, 293, 321, 603, 360, 264, 912, 733, 295, 3487, 43098], "temperature": 0.0, "avg_logprob": -0.4303805457221137, "compression_ratio": 1.256198347107438, "no_speech_prob": 7.482500222977251e-05}, {"id": 1124, "seek": 722936, "start": 7238.599999999999, "end": 7244.04, "text": " deep to see what's going on.", "tokens": [2452, 281, 536, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.4303805457221137, "compression_ratio": 1.256198347107438, "no_speech_prob": 7.482500222977251e-05}, {"id": 1125, "seek": 724404, "start": 7244.04, "end": 7259.68, "text": " Thanks everybody see you next week.", "tokens": [50364, 2561, 2201, 536, 291, 958, 1243, 13, 51146], "temperature": 0.0, "avg_logprob": -0.5574791908264161, "compression_ratio": 0.813953488372093, "no_speech_prob": 0.00011924482532776892}], "language": "en"}