{"text": " So, we're going to be talking about GANs today. Who has heard of GANs? Very hot technology, but definitely deserving to be in the cutting edge deep learning part of the course because they're not quite proven to be necessarily useful for anything, but they're nearly there. They're definitely going to get there and we're going to focus on the things where they're definitely going to be useful in practice. There's a number of areas where they may turn out to be useful in practice, but we don't know yet. So, I think the area that they're definitely going to be useful in practice is the kind of thing you see on the left here, which is for example turning drawings into rendered pictures. This comes from a paper that just came out two days ago. So there's a very active research going on right now. Before we get there though, let's talk about some interesting stuff from the last class. This is an interesting thing that one of our diversity fellows, Christine Payne, did. Christine has a master's in medicine from Stanford and so she obviously had an interest in thinking what would it look like if we built a language model of medicine. One of the things that we briefly touched on back in Lesson 4, but didn't really talk much about last time, is this idea you can actually seed a generative language model, which basically means you've trained a language model on some corpus and then you're going to generate some text from that language model. So you can start off by feeding it a few words, to basically say here's the first few words to create the hidden state in the language model and then generate from there, please. So Christine did something clever, which was to kind of pick a, was to seed it with a question and then repeat the question three times, Christine, three times, and then let it generate from there. And so she fed a language model lots of different medical texts and then fed it this question, what is the prevalence of malaria? And the model said in the US about 10% of the population has the virus, but only about 1% is infected with the virus, about 50 to 80 million infected. She said what's the treatment for ectopic pregnancy? And it said it's a safe and safe treatment for women with a history of symptoms that may have a significant impact on clinical response. What's an important factor, development and management of ectopic pregnancies, etc. And so what I find interesting about this is, you know, it's pretty close to being a, to me as somebody who doesn't have a master's in medicine from Stanford, it's pretty close to being a believable answer to the question. But it really has no bearing on reality whatsoever and I kind of think it's an interesting kind of ethical and user experience quandary. So actually I'm involved also in a company called doc.ai that's trying to basically do a number of things, but in the end provide an app for doctors and patients which can help kind of create a conversational user interface around helping them with their medical issues. And I've been kind of continually saying to the software engineers on that team, please don't try to create a generative model using like an LSTM or something because they're going to be really good at creating bad advice that sounds impressive. You know, kind of like, you know, political pundits or tenured professors, you know, people who can say bullshit with great authority. So I think, yeah, I thought it was a really interesting experiment. And great to see what our diversity fellows are doing. I mean this is why we have this program. I suppose I shouldn't just say master's in medicine, actually a Julliard-trained classical musician and actually also a Princeton Ballot, Victoria and Physics, so also a high-performance computing expert. Yeah, okay, so she does a bit of everything. So yeah, really impressive group of people and great to see such exciting kind of ideas coming out. And if you're wondering, you know, I've done some interesting experiments. Should I let people know about it? Well I, Christine mentioned this in the forum, I went on to mention it on Twitter, to which I got this response. If you're looking for a job, you may be wondering who Xavier Ameritrain is. Well he is the founder of a hot new medical AI startup. He was previously the head of engineering at Quora. Before that he was the guy at Netflix who ran the data science team and built their recommender systems. And so this is what happens if you do something cool. Let people know about it and get noticed by awesome people like Xavier. So let's talk about Sci-Fi 10. And the reason I'm going to talk about Sci-Fi 10 is that we're going to be looking at some more bare-bones PyTorch stuff today to build these generative adversarial models. There's no really fast AI support to speak of at all for GANs at the moment. I'm sure there will be soon enough, but currently there isn't. So we're going to be building a lot of models from scratch. It's been a while since we've done much, you know, serious model building. A little bit of model building I guess for our bounding box stuff, but really all the interesting stuff there was the loss function. So we looked at Sci-Fi 10 in the part 1 of the course and we built something which was getting about 85% accuracy and I can't remember a couple of hours to train. Interestingly there's a competition going on now to see who can actually train Sci-Fi 10 the fastest going through this Stanford Dawn bench. So the goal is to get it to train to 94% accuracy. So it'll be interesting to see if we can build an architecture that can get to 94% accuracy because that's a lot better than our previous attempt. And so hopefully in doing so we'll learn something about creating good architectures. That'll be then useful for looking at these GANs today. But I think also it's useful because I've been kind of looking much more deeply into the last few years' papers about different kinds of CNN architectures and realized that a lot of the insights in those papers are not being widely leveraged and clearly not widely understood. So I want to show you what happens if we can leverage some of that understanding. So I've got this notebook called Sci-Fi 10 Darknet. That's because the particular architecture we're going to look at is really very close to the Darknet architecture. But you'll see in the process that the Darknet architecture, as in not the whole YOLO version 3 end-to-end thing, but just the part of it that they pre-trained on ImageNet to do classification, it's almost like the most generic, simple architecture you could come up with. And so it's a really great starting point for experiments. So we're going to call it Darknet, but it's not quite Darknet and you can fiddle around with it to create things that definitely aren't Darknet. It's really just the basis of nearly any modern ResNet-based architecture. So Sci-Fi 10, remember, is a fairly small dataset, the images are only 32x32 in size. And I think it's a really great dataset to work with because you can train it relatively quickly unlike ImageNet. It's a relatively small amount of data unlike ImageNet. And it's actually quite hard to recognize the images because 32x32 is kind of too small to easily see what's going on. So it's somewhat challenging. So I think it's a really underappreciated dataset because it's old. And who at DeepMind or OpenAI wants to work with a small old dataset when they could use their entire server room to process something much bigger. But to me, I think this is a really great dataset to focus on. So we'll go ahead and import our usual stuff. And we're going to try and build a network from scratch to train this with. One thing that I think is a really good exercise for anybody who's not 100% confident with their kind of broadcasting and PyTorch and so forth basic skills is figure out how I came up with these numbers. So these numbers are the averages for each channel and the standard deviations for each channel in Sci-Fi 10. So try and, that's a bit of homework, just make sure you can recreate those numbers and see if you can do it in no more than a couple of lines of code, no loops. Ideally, you want to do it in one go if you can. Because these are fairly small, we can use a larger batch size than usual, 256, and the size of these images is 32. Normally, we have this standard set of side-on transformations we use for photos of normal objects. We're not going to use that here though because these images are so small that trying to rotate a 32x32 image a bit is going to introduce a lot of blocky kind of distortions. So the kind of standard transforms that people tend to use is a random horizontal flip, and then we add size divided by 8. So 4 pixels of padding on each side. And one thing which I find works really well is by default, Fast.ai doesn't add black padding, which basically every other library does. We actually take the last 4 pixels of the existing photo and flip it and reflect it. And we find that we get much better results by using this reflection padding by default. So now that we've got a 36x36 image, this set of transforms in training will randomly pick a 32x32 crop. So we get a little bit of variation, but not heaps. So we can use our normal from paths to grab our data. So we now need an architecture. And what we're going to do is we're going to create an architecture which fits in one screen. So this is from scratch. As you can see, I'm using the predefined comp2d, batchnorm2d, leakyvalue modules, but I'm not using any blocks or anything. They're all being defined. So the entire thing is here on one screen. So if you're ever wondering, can I understand a modern, good quality architecture? Absolutely. Let's study this one. So my basic starting point with an architecture is to say, okay, it's a stacked bunch of layers. And generally speaking, there's going to be some kind of hierarchy of layers. So at the very bottom level, there's things like a convolutional layer and a batchnorm layer. But generally speaking, any time you have a convolution, you're probably going to have some standard sequence. And normally it's going to be conv, batchnorm, then a nonlinear activation like a value. So I try to start right from the top by saying, okay, what are my basic units going to be? And so by defining it here, that way I don't have to worry about trying to keep everything consistent and it's going to make everything a lot simpler. So here's my conv layer. So any time I say conv layer, I mean conv, batchnorm, leakyvalue. Now I'm not quite saying leakyvalue. I'm saying leakyvalue. And that's, I think we briefly mentioned it before, but the basic idea is that normally a value looks like that, right? Hopefully you all know that now. A leakyvalue looks like that. So this part, as before, has a gradient of 1 and this part has a gradient of, it can vary, but something around.1 or.01 is common. And the idea behind it is that when you're in this negative zone here, you don't end up with a 0 gradient, which makes it very hard to update it. In practice, people have found leakyvalue more useful on smaller datasets and less useful on big datasets, but it's interesting that for the YOLO version 3 paper, they did use a leakyvalue and got great performance from it. So it rarely makes things worse and it often makes things better. So it's probably not bad if you need to create your own architecture to make that your default go-to is to use leakyvalue. You'll notice I don't define a PyTorch module here, I just go ahead and go sequential. This is something that if you read other people's PyTorch code, it's really underutilized. People tend to write everything as a PyTorch module with an init and a forward. But if the thing you want is just a sequence of things one after the other, it's much more concise and easy to understand to just make it a sequential. So I've just got a simple plain function that just returns a sequential model. So I mentioned that there's generally a number of hierarchies of units in most modern networks. And I think we know now that the next level in this unit hierarchy for resnets, and this is a type of resnet, is the res block or the residual block. I call it here a res layer. And back when we last did CyPhar 10, I oversimplified this, I cheated a little bit. We had x coming in and we put that through a conv and then we added it back up to x to go out. So in general, we've got your output is equal to your input plus some function of your input. And the thing we did last year was we made f was a 2D conv. And actually in the real res block, there's actually two of them. So it's actually conv of conv of x. And when I say conv, I'm using this as a shortcut for our conv layer. In other words, conv batch norm relu. So you can see here I've created two convs and here it is. I take my x, put it through the first conv, put it through the second conv, and add it back up to my input again to get my basic res block. So one kind of interesting approach or one interesting insight here is kind of what are the number of channels in these convolutions. So we've got coming in some ni, some number of input channels, number of inputs, or number of input filters. The way that the darknet folks set things up is they said, okay, we're going to make every one of these res layers spit out the same number of channels that came in. And I kind of like that. That's why I used it here, because it makes life simpler. And so what they did is they said, okay, let's have the first conv half the number of channels and then the second conv double it again. So ni goes to ni over 2 and then ni over 2 goes to ni. So you've kind of got this funneling thing where if you've got 64 channels coming in, you kind of get squished down with a first conv down to 32 channels and then taken back up again to 64 channels coming out. Yes, Rachel? Why is inplace equals true in the leaky Raleigh? Oh, thanks for asking. A lot of people forget this or don't know about it, but this is a really important memory technique. If you think about it, this conv layer, it's like the lowest level thing. So pretty much everything in our resnet, once it's all put together, is going to be conv layers, conv layers, conv layers. If you don't have inplace equals true, it's going to create a whole separate piece of memory for the output of the value. So it's going to allocate a whole bunch of memory that's totally unnecessary. And actually, since I wrote this, I came up with another idea the other day, which I'll now implement, which is you could do the same thing for the res layer. Rather than going, let's just reorder this to say, x plus that, you could actually do the same thing here. Hopefully some of you might remember that in PyTorch, pretty much every function has an underscore suffix version, which says do that inplace. So plus, there's also an add, and so that's add inplace, and so that's now suddenly reduced my memory there as well. So these are really handy little tricks. And I actually forgot the inplace equals true at first for this, and I literally was having to decrease my batch size to much lower amounts than I knew should be possible, and it was driving me crazy, and then I realized that that was missing. You can also do that with dropout, by the way, if you have dropout. So dropout and all the activation functions you can do inplace, and then generally any arithmetic operation you can do inplace as well. Why is bias usually, like in ResNet, set to false in the conv layer? So if you're watching the video, pause now and see if you can figure this out, because this is a really interesting question. Why don't we need bias? So I'll wait for you to pause. Welcome back. So if you figured it out, here's the thing, immediately after the conv, there's a batch norm, and remember batch norm has two learnable parameters for each activation, the kind of the thing you multiply by and the thing you add. So since we're, if we had bias here to add, and then we add another thing here, we're adding two things, which is totally pointless, like that's two weights where one would do. So if you have a batch norm after a conv, then you can either say in the batch norm, don't include the add bit there please, or easier is just to say don't include the bias in the conv. There's no particular harm, but again it's going to take more memory because that's more gradients that it has to keep track of. So best to avoid. Also another thing, a little trick, is most people's conv layers have padding as a parameter, but generally speaking you should be able to calculate the padding easily enough. And I see people try to implement special same padding modules and all kinds of stuff like that, but if you've got a stride1, or pretty much any stride actually, and you've got a kernel size of 3, then obviously that's going to overlap by one unit on each side, so we want padding of 1. Or else if it's stride1, then we don't need any padding. So in general, padding of kernel size integer divided by 2 is what you need. There's some tweaks sometimes, but in this case this works perfectly well. So again, trying to simplify my code by having the computer calculate stuff for me rather than me having to do it myself. Another thing here with the 2 conv layers, so we have this idea of a bottleneck, this idea of reducing the channels and then increasing them again, is also what kernel size we use. So here's a 1x1 conv, and so this is again something you might want to pause the video now and think about, what's a 1x1 conv really? What actually happens in a 1x1 conv? So if we've got a little 4x4 grid here, and of course there's a filters or channels access as well, maybe that's like 32, and we're going to do a 1x1 conv. So what's the kernel for a 1x1 conv going to look like? It's going to be 1x32. So remember when we talk about the kernel size, we never mention that last piece, we don't say it's 1x1x32 because that's part of the filters in and filters out. So in other words then, what happens is this one thing gets placed first of all here on the first cell, and we basically get a dot product of that 32-bit with this 32-bit, and that's going to give us our first output. And then we're going to take that 32-bit bit and put it with the second one to get the second output. So it's basically going to be a bunch of little dot products for each point in the grid. So what it basically is then, it's basically something which is allowing us to kind of change the dimensionality in whatever way we want in the channel dimension. And so that would be one of our filters. And so in this case, we're creating ni divided by 2 of these, so we're going to have ni divided by 2 of these dot products all with different weighted averages of the input channels. So it basically lets us with very little computation add this additional step of calculations and non-linearities. So that's a cool trick, this idea of taking advantage of these 1x1 cons, creating this bottleneck and then pulling it out again with 3x3 cons, so that's actually going to take advantage of the 2D nature of the input properly. So 1x1 cons doesn't take advantage of that at all. So these two lines of code, there's not much in it, but it's a really great test of your understanding and intuition about what's going on. Why is it that a 1x1 con going from ni to ni over 2 channels followed by a 3x3 con going from ni over 2 to ni channels, why does it work, why do the tensor ranks line up, why do the dimensions all line up nicely, why is it a good idea, what's it really doing, it's a really good thing to fiddle around with, maybe create some small ones in Jupyter Notebook, run them yourself, see what inputs and outputs come in and out, really get a feel for that. Once you've done so, you can then play around with different things. One of the really unappreciated papers is this one, Wide Residual Networks. And it's really quite a simple paper, but what they do is they basically fiddle around with these two lines of code. And what they do is they say, well what if this wasn't divided by 2, but what if it was times 2. That would be totally allowable. That's going to line up nicely. Or what if we had another com3 after this, and so this was actually ni over 2 to ni over 2, and then this is ni over 2. Again that's going to work, right, kernel size 1, 3, 1, going to half the number of kernels, leave it at half, and then double it again at the end. And so they come up with this simple notation for basically defining what this can look like. And then they show lots of experiments. And basically what they show is that this approach of bottlenecking, of decreasing the number of channels, which is almost universal in ResNets, is probably not a good idea. In fact from the experiments, definitely not a good idea. Because what happens is it lets you create really deep networks. The guys who created ResNets got particularly famous creating a 1001 layer network. But the thing about 1001 layers is you can't calculate layer 2 until you finish layer 1. You can't calculate layer 3 until you finish layer 2. So it's sequential. GPUs don't like sequential. So what they showed is that if you have less layers but with more calculations per layer, so one easy way to do that would be to remove the divider by 2. No other changes. Like try this at home. Try running sci-fi and see what happens. Or maybe even multiply it by 2, or fiddle around. And that basically lets your GPU do more work. And it's very interesting because the vast majority of papers that talk about performance of different architectures never actually time how long it takes to run a batch through it. Like they literally say this one requires x number of floating point operations per batch. But then they never actually bother to run the damn thing like a proper experimentalist and find out whether it's faster or slower. And so a lot of the architectures that are really famous now turn out to be slow as molasses and take crap loads of memory and are just totally useless. Because the researchers never actually bother to see whether they're fast and to actually see whether they fit in RAM with normal batch sizes. So the wide resnet paper is unusual in that it actually times how long it takes. As does the YOLO version 3 paper, which made the same insight. I'm not sure they might have missed the wide resnets paper because the YOLO version 3 paper came to a lot of the same conclusions, but I'm not even sure they cited the wide resnets paper so they might not be aware that all that work's been done. But they're both great to see people actually timing things and noticing what actually makes sense. Yes, Rich? Question from the audience. SelU looked really hot in the paper which came out, but I noticed that you don't use it. What's your opinion on SelU? So SelU is something largely for fully connected layers which allows you to get rid of batch norm. The basic idea is that if you use this different activation function, it's kind of self-normalizing. That's what the S in SelU stands for. So self-normalizing means it'll always remain at a unit standard deviation and zero mean and therefore you don't need that batch norm. It hasn't really gone anywhere, and the reason it hasn't really gone anywhere is because it's incredibly finicky. You have to use a very specific initialization otherwise it doesn't start with exactly the right standard deviation and mean. Very hard to use it with things like embeddings. If you do, then you have to use a particular kind of embedding. Initialization which doesn't necessarily actually make sense for embeddings. So you do all this work, very hard to get it right, and if you do finally get it right, what's the point where you've managed to get rid of some batch norm layers which weren't really hurting you anyway. It's interesting because that SelU paper, I think one of the reasons people noticed it, or in my experience the main reason people noticed it, was because it was created by the inventor of LSTMs. And also it had a huge mathematical appendix and people were like, lots of maths from a famous guy, this must be great. But in practice I don't see anybody using it to get any state of the art results or win any competitions or anything like that. So this is like some of the tiniest bits of code we've seen, but there's so much here and it's fascinating to play with. So now we've got this block which is built on this block, and then we're going to create another block on top of that block. So we're going to call this a group layer, and it's going to contain a bunch of res layers. And so a group layer is going to have some number of channels or filters coming in. And what we're going to do is we're going to double the number of channels coming in by just using a standard conv layer. Additionally we'll halve the grid size by using a stride of 2. And then we're going to do a whole bunch of res blocks, a whole bunch of res layers. We can pick how many, that could be 2 or 3 or 8. Because remember these res layers don't change the grid size and they don't change the number of channels. So you can add as many as you like, anywhere you like, without causing any problems. It's just going to use more computation and more RAM, but there's no reason other than that you can't add as many as you like. So a group layer therefore is going to end up doubling the number of channels because of this initial convolution which doubles the number of channels. And depending on what we pass in a stride, it may also halve the grid size if we put stride equals 2. And then we can do a whole bunch of res block computations, as many as we like. So then to define our dark net or whatever we want to call this thing, we're just going to pass in something that looks like this. And what this says is create 5 group layers. The first one will contain 1 of these extra res layers. The second will contain 2, then 4, then 6, then 3. And I want you to start with 32 filters. So the first one of these res layers will contain 32 filters and there will just be one extra res layer. The second one is going to double the number of filters because that's what we do each time we have a new group layer, we double the number. So the second one will have 64, then 128, then 256, then 512, and then that'll be it. So that's going to be like nearly all of the network is going to be those bunches of layers. And remember every one of those group layers also has one convolution at the start. And so then all we have is before that all happens, we're going to have one convolutional layer at the very start and at the very end we're going to do our standard adaptive average pooling, flatten, and a linear layer to create the number of classes out at the end. So one convolution at the end, adaptive pooling, and one linear layer at the other end, and then in the middle these group layers, each one consisting of a convolutional layer followed by n number of res layers. That's it. Again I think we've mentioned this a few times, but I'm yet to see any code out there, any examples, anything anywhere that uses adaptive average pooling. Everyone I've seen writes it like this, and then bits a particular number here, which means that it's now tied to a particular image size, which definitely isn't what you want. So most people, even the top researchers I speak to, most of them are still under the impression that a specific architecture is tied to a specific size, and that's a huge problem when people think that because it really limits their ability to use smaller sizes to kickstart their modeling or to use smaller sizes for doing experiments and stuff like that. Again you'll notice I'm using sequential here, but a nice way to create architectures is to start out by creating a list, in this case this is a list with just one comp layer in, and then my function here, make group layer, it just returns another list. So then I can just go plus equals, appending that list to the previous list, and then I could go plus equals to append this bunch of things to that list, and then finally sequential of all those layers. So that's a very nice thing, so now my forward is just self.layers. So here's a nice picture of how to make your architectures as simple as possible. So you can now go ahead and create this, and as I say, you could even parameterize this to make it a number that you pass in here, to pass in different numbers, so it's not 2, maybe it's times 2 instead, you could pass in things that change the kernel size or change the number of convolutional layers, fiddle around with it, and maybe you can create something, I've actually got a version of this which I'm about to run for you, which kind of implements all of the different parameters that's in that wide resnet paper, so I could fiddle around to see what worked well. So once we've got that, we can use conv learner from model data to take our PyTorch model, module, and the model data object and turn them into a learner, give it a criterion, that's a metrics if we like, and then we can call fit and away we go. Could you please explain adaptive average pooling, how does setting to one work? Sure, before I do, I just want to, since we've only got a certain amount of time in this class, I wanted to see, I do want to see how we go, you know, with this simple network against these state of the art results. So to make life a little easier, we can start it running now and see how it looks later. So I've got the command ready to go. So we've basically taken all that stuff and put it into a simple little Python script, and I've modified some of those parameters I mentioned to create something I've called a WRN22 network, which doesn't officially exist, but it's got a bunch of changes to the parameters we talked about based on my experiments. We're going to use the new Leslie Smith one cycle thing. So there's quite a bunch of cool stuff here. So the one cycle implementation was done by our student Sylvain Gugger, I think, I don't know how to pronounce his name exactly, Sylvain. The trained Cypher experiments were largely done by Brett Kearns, and stuff like getting the half-position floating point implementation integrated into Fast.ai was done by Andrew Shaw. So it's been a cool kind of bunch of different student projects coming together to allow us to run this. So this is going to run actually on a AWS, Amazon AWS P3, which has 8 GPUs. The P3 has these newer Volta architecture GPUs, which actually have special support for half-precision floating point. Fast.ai is the first library I know of to actually integrate the Volta optimized half-precision floating point into the library. So we can just go learn.half now and get that support automatically. And is also the first one to integrate one cycle. So these are the parameters for the one cycle. So we can go ahead and get this running. So what this actually does is it's using PyTorch's multi-GPU support. Since there are 8 GPUs, it's actually going to fire off 8 separate Python processes, and each one's going to train on a little bit. And then at the end, it's going to pass the gradient updates back to kind of the master process that's going to integrate them all together. So you'll see lots of progress bars all pop up together. And you can see it's training 3 or 4 seconds when you do it this way. Where else when I was training earlier, I was getting about 30 seconds per epoch. So doing it this way, we can kind of train things 10 times faster or so, which is pretty cool. Okay, so we'll leave that running. So you were asking about adaptive average pooling, and I think specifically is what's the number 1 doing? So normally when we're doing average pooling, let's say we've got 4x4. Let's say we did average pooling 2,2. Then that creates a 2x2 area and takes the average of those 4. And then we can pass in the stride. So if we said stride 1, then the next one is we look at this block of 2x2 and take that average and so forth. So that's like what a normal 2x2 average pooling would be. And so in that case, if we didn't have any padding, that would spit out a 3x3, because it's 2 here, 2 here, 2 here. And if we added padding, we can make it 4x4. So if we wanted to spit out something, we didn't want 3x3, what if we wanted 1x1? Then we could say average pool 4,4. And so that's going to do 4,4 and average the whole lot. That would spit out 1x1. So that's just one way to do it. Rather than saying the size of the pooling filter, why don't we instead say, well I don't care what the size of the input grid is, I always want 1x1. So that's where then you say adaptive average pool. And now you don't say what's the size of the pooling filter, you instead say what's the size of the output I want. And so I want something that's 1x1. And if you only put a single int, it assumes you mean 1x1. So in this case, adaptive average pooling 1 with a 4x4 grid coming in is the same as average pooling 4,4. If it was a 7x7 grid coming in, it would be the same as 7,7. So it's the same operation, it's just expressing it in a way that says regardless of the input, I want something of that sized output. Well we got to 94, and it took 3 minutes and 11 seconds, and the previous state of the art was 1 hour and 7 minutes. So was it worth fiddling around with those parameters and learning a little bit about how these architectures actually work and not just using what came out of the box? Well holy shit, we just used a publicly available instance, we used a spot instance, so that cost us like $8 per hour for 3 minutes, cost us a few cents to train this from scratch 20 times faster than anybody's ever done it before. So that's like the most crazy state of the art result I think we've ever seen, we've seen many, but this one just blew it out of the water. And so this is partly thanks to just fiddling around with those parameters of the architecture, mainly frankly about using Leslie Smith's one cycle thing and Sobhav's implementation of that. Remember, not only, so just to remind you of what that's doing, it's basically saying this is batches, and this is learning rate. It creates an upward path that's equally long as the downward path, so it's a true CLR, triangular cyclical learning rate, as per usual, you can pick the ratio between those two numbers, so x divided by y in this case is a number that you get to pick, in this case we picked 50, so we started out with a much smaller one here. And then it's got this cool idea which is you get to say what percentage of your epochs then is spent going from the bottom of this down all the way down pretty much to 0. And that's what this second number here is, so 15% of the batches are spent going from the bottom of our triangle even further. So importantly though, that's not the only thing one cycle does, we also have momentum, and momentum goes from 0.95 to 0.85 like this. In other words, when the learning rate is really low, we use a lot of momentum, and when the learning rate is really high, we use very little momentum, which makes a lot of sense. But until Leslie Smith showed this in that paper, I've never seen anybody do it before, so it's a really cool trick. So you can now use that by using the useCLRBeta parameter in FastAI, and you should be able to basically replicate this data-the-art result. You can use it on your own computer or your paper space, obviously the only thing you won't get is the multi-GPU piece, but that makes it a bit easier to train anyway. So on a single GPU, you should be able to beat this on a single GPU. Yeah. Make group layer contains stride equals 2, so this means stride is 1 for layer 1 and 2 for everything else. What's the logic behind it? Usually the strides I have seen are odd. No, strides are either 1 or 2, I think you're thinking of kernel sizes. So stride equals 2 means that I jump 2 across, and so stride of 2 means that you halve your grid size. So I think you might have just got confused between stride and kernel size there. And so if we have a stride of 1, the grid size doesn't change. If we have a stride of 2, then it does. And so in this case, because this is for CIFAR10, 32x32 is small, and we don't get to halve the grid size very often, because pretty quickly we're going to run out of cells. And so that's why the first layer has a stride of 1, so we don't decrease the grid size straight away basically. And it's kind of a nice way of doing it because that's why we kind of have a low number here, so we can start out with not too much computation on the big grid, and then we can gradually do more and more computation as the grids get smaller and smaller. Because the smaller grid, the computation will take less time. So I think so that we can do all of our ganning in one go, let's take a slightly early break and come back at 7.30. So we're going to talk about Generative Adversarial Networks, also known as GANs. And specifically we're going to focus on the Wasserstein GAN paper, which included some guy called Sumit Chintala who went on to create some piece of software called Hightorch. The Wasserstein GAN was heavily influenced by the DC GAN, or Deep Convolutional Generative Adversarial Networks paper, which also Sumit was involved with. So it's a really interesting paper to read. A lot of it looks like this. The good news is you can skip those bits, because there's also a bit that looks like this which says, do these things. Now I will say though that a lot of papers have a theoretical section which seems to be there entirely to get past the reviewers' need for theory. That's not true of the WGAN paper. The theory bit is actually really interesting. You don't need to know it to use it, but if you want to learn about some cool ideas and see the thinking behind why this particular algorithm, it's absolutely fascinating. And almost nobody before this paper came out, I didn't know, literally I knew nobody who had studied the math that it's based on. So like everybody had to learn the math it was based on. And so the paper does a pretty good job of laying out all the pieces. You'll have to do a bunch of reading yourself. So if you're interested in digging into the deeper math behind some paper to see what it's like to study it, I would pick this one, because at the end of that theory section you'll come away saying, okay, I can see now why they made this algorithm the way it is. And then having come up with that idea, the other thing is often these theoretical sections are very clearly added after they come up with the algorithm. They'll come up with the algorithm based on intuition and experiments and then later on post hoc justify it. Whereas this one you can clearly see it's like, okay, let's actually think about what's going on in GANs and think about what they need to do and then come up with the algorithm. So the basic idea of a GAN is it's a generative model. So it's something that is going to create sentences or create images, it's going to generate stuff. And it's going to try and create stuff which is very hard to tell the difference between generated stuff and real stuff. So a generative model could be used to face swap a video, a very well-known controversial thing of deep fakes and fake pornography and stuff happening at the moment. It could be used to fake somebody's voice. It could be used to fake the answer to a medical question. But in that case, it's not really a fake. It could be a generative answer to a medical question that's actually a good answer. So you're generating language, you could generate a caption to an image, for example. So generative models have lots of interesting applications. But generally speaking, they need to be good enough that for example, if you're using it to automatically create a news scene for Carrie Fisher in the next Star Wars movie, since she's not around to play that part anymore, you want to try and generate an image of her that looks the same, then it has to fool the Star Wars audience into thinking, okay, that doesn't look like some weird Carrie Fisher, that looks like the real Carrie Fisher. Or if you're trying to generate an answer to a medical question, you want to generate English that reads nicely and clearly and sounds authoritative and meaningful. So the idea of a generative adversarial network is we're going to create not just a generative model to create, say, the generated image, but a second model that's going to try to pick which ones are real and which ones are generated. We're going to call them fake. So which ones are real and which ones are fake. So we've got a generator that's going to create our fake content and a discriminator that's going to try to get good at recognizing which ones are real and which ones are fake. So there's going to be two models. And then there's going to be adversarial, meaning the generator is going to try to keep getting better at fooling the discriminator into thinking that fake is real, and the discriminator is going to try to keep getting better at discriminating between the real and the fake. And they're going to go head-to-head like that. And it's basically as easy as I just described. It really is. We're just going to build two models in PyTorch. We're going to create a training loop that first of all says the loss function for the discriminator is can you tell the difference between real and fake, and then update the weights of that. And then we're going to create a loss function for the generator which is going to say can you generate something which pulls the discriminator and update the weights from that loss. And we're going to loop through that a few times and see what happens. And so let's come back to the pseudocode here of the algorithm and let's read the real code first. So there's lots of different things you can do with GANs. And we're going to do something that's kind of boring but easy to understand and it's kind of cool that it's even possible. Which is we're just going to generate some pictures from nothing. We're just going to get it to draw some pictures. And specifically we're going to get it to draw pictures of bedrooms. You'll find if you hopefully get a chance to play around with this during the week with your own datasets, if you pick a dataset that's very varied like ImageNet and then get a GAN to try and create ImageNet pictures, it tends not to do so well because it's not really clear enough what you want a picture of. So it's better to give it, for example, there's a dataset called CelebA which is pictures of celebrities' faces. That works great with GANs. You create really clear celebrity faces that don't actually exist. The bedroom dataset, also a good one, lots of pictures of the same kind of thing. So that's just a suggestion. So there's something called the Lsun scene classification dataset. You can download it using these steps. It's pretty huge. So I've actually created a Kaggle dataset of a 20% sample. So unless you're really excited about generating bedroom images, you might prefer to grab the 20% sample. So then we do the normal steps of creating some different paths. And in this case, as we did before, I find it much easier to kind of go the CSV route when it comes to handling our data. So I just generate a CSV with the list of files that we want and a fake label of 0 because we don't really have labels for these at all. And I actually create two CSV files, one that contains everything in that bedroom dataset and one that just contains a random 10%. It's just nice to do that because then I can most of the time use the sample when I'm experimenting. Because there's well over a million files, even just reading in the list takes a while. So this will look pretty familiar. So here's a conv block. This is before I realized that sequential models are much better. So if you compare this to my previous conv block with a sequential model, there's just a lot more lines of code here. But it does the same thing of doing conv value batch norm. And we calculate our padding, and here's the bias-false. So this is the same as before, basically, but with a little bit more code. So the first thing we're going to do is we're going to build a discriminator. So a discriminator is going to receive as input an image. And it's going to spit out a number. And the number is meant to be lower if it thinks this image is real. Now of course, what does it do for a lower number thing doesn't appear in the architecture. That'll be in the loss function. So all we have to do is create something that takes an image and spits out a number. So a lot of this code is borrowed from the original authors of the paper. So some of the naming scheme and stuff is different to what we're used to. So sorry about that. But I've tried to make it look at least somewhat familiar. I probably should have renamed things a little bit. But it looks very similar to actually what we had before. We start out with a convolution. So remember, conv block is conv value batch norm. And then we have a bunch of extra conv layers. This is not going to use a residual. So it looks very similar to before, a bunch of extra layers, but these are going to be conv layers rather than res layers. And then at the end, we need to append enough stride2 conv layers that we decrease the grid size down to be no bigger than 4x4. So it's going to keep using stride2, divide the size by 2, stride2, divide by size by 2 until our grid size is no bigger than 4. And so this is quite a nice way of creating as many layers as you need in a network to handle arbitrary sized images and turn them into a fixed known grid size. Yes, Rachel. Does a GAN need a lot more data than say dogs versus cats or NLP or is it comparable? You know, honestly, I'm kind of embarrassed to say I am not an expert practitioner in GANs. So the stuff I teach in part 1 is stuff I'm happy to say I know the best way to do these things and so I can show you state of the art results like I just did with Sci-Fi 10 with the help of some of my students, of course. I'm not there at all with GANs. So I'm not quite sure how much you need. Like in general, it seems you need quite a lot. Remember the only reason we didn't need too much in dogs and cats is because we had a pre-trained model and could we leverage pre-trained GAN models and fine-tune them? Probably. I don't think anybody's done it as far as I know. That could be a really interesting thing for people to kind of think about and experiment with. Maybe people have done it and there's some literature there I haven't come across. So I'm somewhat familiar with the main pieces of literature in GANs, but I don't know all of it. So maybe I've missed something about transfer learning in GANs, but that would be the trick to not needing too much data. So it's the huge speed-up combination of one cycle learning rate and momentum annealing plus the 8 GPU parallel training and the half precision. Is that only possible to do the half precision calculation with consumer GPU? Another question, why is the calculation eight times faster from single to half precision while from double to single is only two times faster? So the CyPhar 10 result, it's not eight times faster from single to half. It's about two or three times as fast from single to half. The Nvidia claims about the flop's performance of the tensor cores are academically correct, but in practice meaningless because it really depends on what cores you need for what pieces. So about two or three X improvement for half. So the half precision helps a bit, the extra GPUs helps a bit, the one cycle helps an enormous amount. Then another key piece was the playing around with the parameters that I told you about. So reading the wide ResNet paper carefully, identifying the kinds of things that they found there, and then writing a version of the architecture you just saw that made it really easy for me to fiddle around with parameters. Staying up all night trying every possible combination of different kernel sizes and numbers of kernels and numbers of layer groups and size of layer groups. Remember we did a bottleneck, but actually we tended to focus not on bottlenecks but instead on widening. So we actually like things that increase the size and then decrease it because it takes better advantage of the GPU. So all those things combined together. I'd say the one cycle was perhaps the most critical, but every one of those resulted in a big speed up. That's why we were able to get this 30X improvement over the state of the art sci-fi 10. And we got some ideas for other things, like after this Dawnbench finishes. Maybe we'll try and go even further and see if we can beat 1 minute one day. That'll be fun. So here's our discriminator. The important thing to remember about an architecture is it doesn't do anything other than have some input tensor size and rank and some output tensor size and rank. So this is going to spit out, you see the last conv here has one channel. This is a bit different to what we're used to, because normally our last thing is a linear block. But our last thing here is a conv block. And it's only got one channel, but it's got a grid size of something around 4x4. It's no more than 4x4. So we're going to spit out a 4x4x1 tensor. So what we then do is we then take the mean of that. So it goes from 4x4x1 to the scalar. So this is kind of like the ultimate adaptive average pooling, because we've got something with just one channel, we take the mean. So this is a bit different. Normally we first do average pooling and then we put it through a fully connected layer to get our one thing out. In this case though, we're getting one channel out and then taking the mean of that. I haven't fiddled around with why did we do it that way, what would instead happen if we did the usual average pooling followed by a fully connected layer. Would it work better, would it not, I don't know. I rather suspect it would work better if we did it the normal way, but I haven't tried it and I don't really have a good enough intuition to know whether I'm missing something. It's an interesting experiment to try, if somebody wants to stick an adaptive average pooling layer here and a fully connected layer afterwards with a single output. It should keep working, it should do something. The loss will go down, just see whether it works. So that's the discriminator. So there's going to be a training loop. Let's assume we've already got a generator. Somebody says, okay Jeremy, here's a generator, it generates bedrooms. I want you to build a model that can figure out which ones are real and which ones aren't. So I'm going to take the dataset and basically label a bunch of images which are fake bedrooms from the generator and a bunch of images of real bedrooms from my Lsun dataset, just stick a 1 or a 0 and an H1, and then I'll try to get the discriminator to tell the difference. So that's going to be simple enough. But I haven't been given a generator, I need to build one. So a generator, and we haven't talked about the loss function yet, we're just going to assume that there's some loss function that does this thing. So a generator is also an architecture which doesn't do anything by itself until we have a loss function and data. But what are the ranks and sizes of the tensors? Well the input to the generator is going to be a vector of random numbers. And in the paper they call that the prior. It's going to be a vector of random numbers. How big? I don't know. Some big. 64, 128. And the idea is that a different bunch of random numbers will generate a different bedroom. So that's the idea. So our GAN generator has to take as input a vector, and it's going to take that vector, so here's our input, and it's going to stick it through, in this case a sequential model. And the sequential model is going to take that vector and it's going to turn it into a rank 4 tensor, or if you take off the batch bit, a rank 3 tensor, height by width by 3. So you can see at the end here, our final step, nc number of channels. So I think that's going to have to end up being 3 because we're going to create a 3-channel image of some size. Yes, Richard? In comf block forward, is there a reason why batch norm comes after relu, i.e. self.batchnorm.self.relu? No, there's not. It's just what they had in the code I borrowed from, I think. In ResNet, the order is reversed. So again, unless my intuition about GANs is all wrong and for some reason need to be different to what I'm used to, I would normally expect to go relu then batch norm. This is actually the order that makes more sense to me. But I think the order I had in the darknet was what they used in the darknet paper. So I don't know, everybody seems to have a different order of these things. And in fact, most people for CyPhar10 have a different order again, which is they actually go bn then relu then conv, which is kind of a quirky way of thinking about it. But it turns out that often for residual blocks, that works better. That's called a pre-activation ResNet. So if you Google for pre-activation ResNet, you can see that. So yeah, there's a few, not so much papers, but more blog posts out there where people have experimented with different orders of those things, and it seems to depend a lot on what specific dataset it is and what you're doing with, although in general the difference in performance is small enough you won't care unless it's through a competition. So the generator needs to start with a vector and end up with a rank 3 tensor. We don't really know how to do that yet. So how do we do that? How do we start with a vector and turn it into a rank 3 tensor? We need to use something called a deconvolution. And a deconvolution is, or as they call it in PyTorch, a transposed convolution. Same thing, different name. And so a deconvolution is something which rather than decreasing the grid size, it increases the grid size. So as with all things, it's easiest to see in an Excel spreadsheet. So here's a convolution. We start, let's say, with a 4x4 grid cell with a single channel, a single filter. And let's put it through a 3x3 kernel, again with a single output filter. So we've got a single channel in, a single filter kernel, and so if we don't add any padding we're going to end up with 2x2, because that 3x3 can go in 1, 2, 3, 4 places. It can go in 1 of 2 places across and 1 of 2 places down if there's no padding. So there's our convolution. Remember the convolution is just the sum of the product of the kernel and the appropriate grid cell. So there's our standard 3x3 upon 1 channel, 1 filter. So the idea now is I want to go the opposite direction. I want to start with my 2x2 and I want to create a 4x4. And specifically I want to create the same 4x4 that I started with. And I want to do that by using a convolution. So how would I do that? Well if I have a 3x3 convolution, then if I want to create a 4x4 output, I'm going to need to create this much padding. Because with this much padding, I'm going to end up with 1, 2, 3, 4 by 1, 2, 3, 4. You see why that is? So this filter can go in any one of 4 places across and 4 places up and down. So let's say my convolutional filter was just a bunch of zeros. And I can calculate my error for each cell by taking this attraction. And then I can get the sum of absolute values, the L1 loss, by just summing up the absolute values of those errors. So now I could use optimization, so in Excel that's called solver, to do a gradient descent. So I'm going to set that cell equal to a minimum, I'm going to try and reduce my loss, by changing my filter. And I'll go solve. And you can see it's come up with a filter such that 15.7 compared to 16, 17 is right, 17.8, 19.8. So it's not perfect, and in general you can't assume that a deconvolution can exactly create the same, the exact thing that you want, because there's just not enough, there's only 9 things here and 16 things you're trying to create. But it's made a pretty good attempt. So this is what a deconvolution looks like, a stride 1, 3x3 deconvolution on a 2x2 grid cell input. How difficult is it to create a discriminator to identify fake news versus real news? You don't need anything special, that's just a classifier. So you would just use the NLP classifier from previous to previous class and lesson 4. In that case, there's no generative piece, so you just need the data set that says these are the things that we believe are fake news and these are the things we consider to be real news. It should actually work very well. To the best of my knowledge, if you try it, you should get as good a result as anybody else has got. Whether it's good enough to be useful in practice, I don't know. Question asked. I don't think anybody in our course has tried and nobody else outside our course knows of this technique. So as we've learned, we've just had a very significant jump in NLP classification capabilities and I think the best you could do at this stage would be to generate a triage that says these things look pretty sketchy based on how they're written and then some human could go and fact check them. An NLP classifier, an RNN can't fact check things, but it could recognize these are written in that kind of highly popularized style which often fake news is written in, so maybe these ones are worth paying attention to. I think that would probably be the best you could hope for without drawing on some kind of external data sources. It's important to remember that a discriminator is basically just a classifier and you don't need any special techniques beyond what we've already learned to do in NLP classification. So to do that kind of deconvolution in PyTorch, just say conv transpose 2D and in the normal way you say the number of input channels, the number of output channels, the kernel size, the stride, the padding, the bias, so these parameters are all the same. And the reason it's called a conv transpose is because actually it turns out that this is the same as the calculation of the gradient of convolution. That's basically why they call it that. So this is a really nice example back on the old Theano website that comes from a really nice paper which actually shows you some visualizations. So this is actually the one we just saw of doing a 2x2 deconvolution. If there's a stride 2, then you don't just have padding around the outside, but you actually have to put padding in the middle as well. They're not actually quite implemented this way because this is slow to do in practice. They implement them a different way, but it all happens behind the scenes. We don't have to worry about it. We've talked about this convolution arithmetic tutorial before, and if you're still not comfortable with convolutions and in order to get comfortable with deconvolutions, this is a great site to go to. If you want to see the paper, just google for convolution arithmetic. That'll be the first thing that comes up. And so that Theano tutorial actually comes from this paper. The paper doesn't have the animated GIFs. So it's interesting then, a deconv block looks identical to a conv block except it's got the word transpose written here. We just go conv relu batch norm as before. It's got input filters, output filters. The only difference is that stride 2 means that the grid size will double rather than half. Both nn conv transpose 2d and nn.upsample seem to do the same thing, ie. expand grid size height and width from the previous layer. Can we say that conv transpose 2d is always better than upsample since upsample is merely resizing and filling unknowns by zeros or interpolation? No you can't. So there's a fantastic interactive paper on distil.pub called Deconvolution and Checkerboard Artifacts which points out that what we're doing right now is extremely suboptimal, but the good news is everybody else does it. If you have a look here, can you see these checkerboard artifacts? It's all like dark blue, light blue, dark blue, light blue. So these are all from actual papers. Basically they noticed every one of these papers with generative models has these checkerboard artifacts. What they realized is it's because when you have a stride 2 convolution of size 3 kernel, they overlap. And so you basically get some grid cells get twice as much activation. And so even if you start with random weights, you end up with a checkerboard artifact. You can kind of see it here. And so the deeper you get, the worse it gets. Another advice is actually less direct than it ought to be. I found that for most generative models, upsampling is better. If you do nm.upsample, then all it does is it's basically doing pooling. It's kind of the opposite of pooling. It says let's replace this one pixel, or this one grid cell, with 4, 2x2. And there's a number of ways to upsample. One is just to copy it across to those 4. Another is to use a bilinear or bicubic interpolation. There are various techniques to try and create a smooth upsampled version. And you can pretty much choose any of them in PyTorch. So if you do a 2x2 upsample and then a regular stride 1 3x3 conv, that's like another way of doing the same kind of thing as a conv transpose. It's doubling the grid size and doing some convolutional arithmetic on it. And I found for generative models, it pretty much always works better. And in that distilled up publication, they kind of indicate that maybe that's a good approach but they don't just come out and say just do this, or else I would just say just do this. Having said that, for GANs, I haven't had that much success with it yet. And I think it probably requires some tweaking to get it to work. I'm sure some people have got it to work. The issue I think is that in the early stages, it doesn't create enough noise. I had a version actually where I tried to do it with an upsample. And you can kind of see that the noise didn't look very noisy. So anyway, it's an interesting question. But next week when we look at style transfer and super resolution and stuff, I think you'll see an upsample really comes into its own. So the generator, we can now basically start with the vector. We can decide and say, okay, let's not think of it as a vector, but actually it's a 1x1 grid cell. We can turn it into a 4x4, 8x8, and so forth. And so that's why we have to make sure it's a suitable multiple so that we can actually create something of the right size. And so you can see it's doing the exact opposite as before. It's making the cell size smaller and smaller by 2 at a time, as long as it can. Sorry, bigger and bigger, the cell size bigger and bigger as long as it can until it gets to half the size that we want. And then finally we add one more on at the end, sorry, we add n more on at the end with no stride. And then we add one more, conv transpose, to finally get to the size that we wanted. And we're done. Finally we put that through a THAN, and that's going to force us to be in the 0-1 range, because of course we don't want to spit out arbitrary size pixel values. So we've got a generator architecture which spits out an image of some given size with the correct number of channels, and with values between 0 and 1. So at this point we can now create our model data object. These things take a while to train, so I just made it 128x128, so this is just a convenient way to make it a bit faster. And that's going to be the size of the input, but then we're going to use transformations to turn it into 64x64. There's been more recent advances which have attempted to really increase this up to kind of like high resolution sizes, but they still tend to require either a batch size of 1, or lots and lots of GPUs or whatever. So we're kind of trying to do things that we can do on single consumer GPUs here. So here's an example of one of the 64x64 petroms. So we're going to do pretty much everything manually, so let's go ahead and create our two models, our generator and our discriminator. And as you can see, they're DCGAN, so in other words they're the same modules that came up were appeared in this paper. So if you're interested in reading the papers, it's well worth going back and looking at the DCGAN paper to see what these architectures are, because it's assumed that when you read the WassersteinGAN paper that you already know that. Yes? Question from the audience. Shouldn't we use a sigmoid if we want values between 0 and 1? I always forget which one's which. So sigmoid is 0 to 1, than is 1 to minus 1. I think what will happen is, I'm going to have to check that. I vaguely remember thinking about this when I was writing this notebook and realizing that 1 to minus 1 made sense for some reason, but I can't remember what that reason was now. Let me get back to you about that during the week and remind me if I forget. So we've got our generator and our discriminator. So we need a function that returns a prior vector, so a bunch of noise. So we do that by creating a bunch of zeros. nz is the size of z, so like very often in our code if you see a mysterious letter, it's because that's the letter they used in the paper. So z is the size of our noise vector. So there's the size of our noise vector, and then we use a normal distribution to generate random numbers inside that. And that needs to be a variable because it's going to be participating in the gradient updates. So here's an example of creating some noise. And so here are 4 different pieces of noise. So we need an optimizer in order to update our gradients. In the Vossestein-Gann paper, they told us to use rmsprop. So that's fine. So when you see this thing saying do an rmsprop update in a paper, that's nice. We can just do an rmsprop update with PyTorch. And they suggested a learning rate of 5e-5. I think I found 1e-4 seemed to work, so I just made it a bit bigger. So now we need a training loop. And so this is the thing that's going to implement this algorithm. So a training loop is going to go through some number of epochs that we get to pick. So that's going to be a parameter. And so remember, when you do everything manually, you've got to remember all the manual steps to do. So one is that you have to set your modules into training mode when you're training them and into evaluation mode when you're evaluating them. Because in training mode, batch norm updates happen and dropout happens. In evaluation mode, those 2 things get turned off. That's basically the difference. So put it into training mode. We're going to grab an iterator from our training data loader. We're going to see how many steps we have to go through. And then we'll use tqdm to give us a progress bar. And then we're going to go through that many steps. So the first step of this algorithm is to update the discriminator. So in this one, they don't call it a discriminator, they call it a critic. So w are the weights of the critic. So the first step is to train our critic a little bit, and then we're going to train our generator a little bit, and then we're going to go back to the top of the loop. So we've got a while loop on the outside, and then inside that, there's another loop for the critic, and so here's our little loop inside that for the critic. We call it a discriminator. So what we're going to do now is we've got a generator, and at the moment it's random. So our generator is going to generate stuff that looks something like this. And so we need to first of all teach our discriminator to tell the difference between that and a bedroom, which shouldn't be too hard, you would hope. So we just do it in basically the usual way, but there's a few little tweaks. So first of all, we're going to grab a mini batch of real bedroom photos. So we can just grab the next batch from our iterator, turn it into a variable. Then we're going to calculate the loss for that. So this is going to be how much does the discriminator think this looks fake. This looks fake, the real ones look fake. And then we're going to create some fake images, and to do that we'll create some random noise, and we'll stick it through our generator, which at this stage is just a bunch of random weights. And that's going to create a mini batch of fake images. And so then we'll put that through the same discriminator module as before to get the loss for that, so how fake do the fake ones look. Remember when you do everything manually, you have to zero the gradients in your loop. If you've forgotten about that, go back to the part 1 lesson where we do everything from scratch. So now finally, the total discriminator loss is equal to the real loss minus the fake loss. And so you can see that here, they don't talk about the loss, they actually just talk about what are the gradient updates. So this here is the symbol for get the gradients. So inside here is the loss. And try to learn to throw away in your head all of the boring stuff. So when you see sum over m divided by m, that means take the average. So just throw that away and replace it with np.mean in your head. There's another np.mean. So you want to get quick at being able to see these common idioms. So anytime you see 1 over m sum over m, you go, okay, np.mean. So we're taking the mean of, and we're taking the mean of, so that's all fine. Xi, what's Xi? It looks like it's x to the power of i, but it's not. The math notation is very overloaded. So it's showed us here what Xi is, and it's a set of m samples from a batch of the real data. So in other words, this is a mini-batch. So when you see something saying sample, it means just grab a row, grab a row, grab a row, and you can see here, grab it m times, and we'll call the first row x parenthesis 1, the second row x parenthesis 2. One of the annoying things about math notation is the way that we index into arrays is everybody uses different approaches, subscripts, superscripts, things in brackets, combinations, commas, square brackets, whatever. So you've just got to look in the paper and be like, okay, at some point they're going to say take the i-th row from this matrix, or the i-th image in this batch, how are they going to do it? In this case, it's a superscript in parentheses. So that's all sample means, and curly brackets means it's just a set of them. This little squiggle followed by something here means according to some probability distribution. And so in this case, and very, very often in papers, it simply means, hey, you've got a bunch of data, grab a bit from it at random. So that's the probability distribution of the data you have is the data you have. So this says grab m things at random from your real data. This says grab m things at random from your prior samples, and so that means, in other words, call create noise to create m random vectors. So now we've got m real images. Each one gets put through our discriminator. We've got m bits of noise. Each one gets put through our generator to create m generated images. Each one of those gets put through, look, FW, that's the same thing, so each one of those gets put through our discriminator to try and figure out whether they're fake or not. So then it's this minus this and the mean of that, and then finally get the gradient of that in order to figure out how to use RMSProp to update our weights using some learning rate. So in PyTorch, we don't have to worry about getting the gradients. We can just specify the loss bit and then just say loss.backward, discriminator.optimizer.step. There's one key step, which is that we have to keep all of our activations, sorry, all of our weights, which are the parameters in a PyTorch module, in this small range between negative.01 and.01. Why? Because the mathematical assumptions that make this algorithm work only apply in like a small ball. So I'm not going to tell, I think it's kind of interesting to understand the math of why that's the case, but it's very specific to this one paper and understanding it won't help you understand any other paper. So only study it if you're interested in, I think it's nicely explained, I think it's fun, but it won't be information that you'll reuse elsewhere unless you get super into GANs. I'll also mention, after the paper came out, an improved Frossestein GAN came out that said, hey, there are better ways to ensure that your weight space is in this tight ball, which is basically to kind of penalize gradients that are too high. So actually nowadays there are slightly different ways to do this. That's why this line of code there, it's kind of the key contribution. This one line of code actually is the one line of code you add to make a Frossestein GAN. But the work was all in knowing that that's the thing that you can do that makes everything work better. So at the end of this, we've got a discriminator that can recognize it in real bedrooms in our totally random crappy generated images. So let's now try and create some better images. So now set trainable discriminator to false, set trainable the generator to true, zero route the gradients of the generator, and now our loss again is FW, that's the discriminator of the generator applied to some more random noise. So here's our random noise, here's our generator, and here's our discriminator. I think I can remove that now because I think I've put it inside the discriminator, but I won't change it now because it's going to confuse me. So it's exactly the same as before where we did generator on the noise and then pass that to discriminator, but this time the thing that's trainable is the generator, not the discriminator. So in other words, in this pseudocode, the thing they update is theta, which is the generator's parameters rather than W, which is the discriminator's parameters. And so hopefully you'll see now that this W down here is telling you these are the parameters of the discriminator. This theta down here is telling you these are the parameters of the generator. Again, it's not a universal mathematical notation, it's a thing they're doing in this particular paper, but it's kind of nice when you see some suffix like that, try to think about what it's telling you. So we take some noise, generate some images, try and figure out if they're fake or real, and use that to get gradients with respect to the generator as opposed to earlier, we got them with respect to the discriminator, and use that to update our weights with rms prop with an alpha learning rate. You'll see that it's kind of unfair that the discriminator is getting trained n critic times, which they set to 5, for every time that we train the generator once. And the paper talks a bit about this, but the basic idea is like there's no point making the generator better if the discriminator doesn't know how to discriminate yet. So that's why we've got this while loop. And here's that 5. And actually something which was added I think in the later paper, maybe a supplementary material, is the idea that from time to time and a bunch of times at the start, you should do more steps of the discriminator. So make sure that the discriminator is pretty capable from time to time. So do a bunch of epochs of training the discriminator a bunch of times to get better at telling the difference between real and fake, and then do one step of making the generator being better at generating, and that is an epoch. And so let's train that for one epoch. And then let's create some noise so we can generate some examples. Actually we're going to do that later. Let's first of all decrease the learning rate by 10 and do one more pass. So we've now done two epochs. And now let's use our noise to pass it to our generator. And then put it through our denormalization to turn it back into something we can see. And then plot it. And we have some bedrooms. Okay, there's not real bedrooms and some of them don't look particularly like bedrooms, but some of them look a lot like bedrooms. So that's the idea. That's a GAN. And I think the best way to think about a GAN is it's like an underlying technology that you'll probably never use like this, but you'll use in lots of interesting ways. For example, we're going to use it to create now a cycle GAN. And we're going to use a cycle GAN to turn horses into zebras. You could also use it to turn Monet prints into photos or to turn photos of Yosemite in summer into winter. So it's going to be pretty... yes, Rachel. Two questions. One, is there any reason for using RMS props specifically as the optimizer as opposed to Atom? I don't remember it being explicitly discussed in the paper. I don't know if it's just experimental or the theoretical reason. Have a look in the paper and see what it says. And which could be a reasonable way of detecting overfitting while training or evaluating the performance of one of these GAN models once we are done training? In other words, how does the notion of train, validation, test sets translate to GANs? That's an awesome question. And there's a lot of people who make jokes about how GANs is the one field where you don't need a test set and people take advantage of that by making stuff up and saying it looks great. There are some pretty famous problems with GANs. One of the famous problems with GANs is called mode collapse. And mode collapse happens where you look at your bedrooms and it turns out that there's basically only 3 kinds of bedrooms that every possible noise vector mapped to. You look at your gallery and it turns out they're all just the same thing. There's just 3 different things. Mode collapse is easy to see if you collapse down to a small number of modes, like 3 or 4. But what if you have a mode collapse down to 10,000 modes? So there's only 10,000 possible bedrooms that all of your noise vectors collapse to. That's not, you wouldn't be able to see it here, because it's pretty unlikely you would have 2 identical bedrooms out of 10,000. Or what if every one of these bedrooms is basically a direct copy, basically it memorized some input? Could that be happening? And the truth is, most papers don't do a good job or sometimes any job of checking those things. So the question of how do we evaluate GANs, and even the point of maybe we should actually evaluate GANs properly, is something that is not widely enough understood even now. And some people are trying to really push. So Ian Goodfellow, who a lot of you will know because he came and spoke here at a lot of the book club meetings last year, and of course was the first author on the most famous deep learning book. He's the inventor of GANs and he's been sending a continuous stream of tweets reminding people about the importance of testing GANs properly. So if you see a paper that claims exceptional GAN results, then this is definitely something to look at. Have they talked about mode collapse? Have they talked about memorization? So this is going to be really straightforward because it's just a neural net. So all we're going to do is we're going to create an input containing lots of zebra photos, and with each one we'll pair it with an equivalent horse photo, and we'll just train a neural net that goes from one to the other. Or you could do the same thing for every Monet painting, create a dataset containing the photo of the place. Oh wait, that's not possible because the places that Monet painted aren't there anymore, and there aren't exact zebra versions of horses. And oh wait, how the hell is this going to work? This seems to break everything we know about what neural nets can do and how they do them. Alright, Rachel, you're going to ask me a question to spoil our whole train of thought. Come on, better be good. Can GANs be used for data augmentation? Yeah, absolutely. You can use a GAN for data augmentation. Should you? I don't know. Like there are some papers that try to do semi-supervised learning with GANs. I haven't found any that are particularly compelling, showing state-of-the-art results on really interesting datasets that have been widely studied. I'm a little skeptical. The reason I'm a little skeptical is because in my experience, if you train a model with synthetic data, the neural net will become fantastically good at recognizing the specific problems of your synthetic data and that will end up what it's learning from. And there are lots of other ways of doing semi-supervised models which do work well. There are some places it can work. For example, you might remember Ottavio Good created that fantastic visualization in part one of the zooming conv net where he showed a letter going through MNIST. He at least at that time was the number one autonomous remote-controlled car guy in autonomous remote-controlled car competitions. And he trained his model using synthetically augmented data where he basically took real videos of a car driving around a circuit and added fake people and fake other cars and stuff like that. I think that worked well because he's kind of a genius and because I think he had a well-defined little subset that he had to work in. In general, it's really hard. It's really, really hard to use synthetic data. I've tried using synthetic data in models for decades now, obviously not GANs because they're pretty new, but in general it's very hard to do. Very interesting research question. So somehow these folks at Berkeley created a model that can turn a horse into a zebra despite not having any photos, unless they went out there and painted horses and took before and after shots, but I believe they didn't. So how the hell did they do this? It's kind of genius. I will say the person I know who's doing the most interesting practice of CycleGAN right now is one of our students, Helena Sarin. She's the only artist I know of who is a CycleGAN artist. Here's an example I love. She created this little doodle in the top left and then trained a CycleGAN to turn it into this beautiful painting in the bottom right. Here are some more of her amazing works. And I think it's really interesting. I mentioned at the start of this class that GANs are in the category of stuff that's not there yet, but it's nearly there. And in this case, there's at least one person in the world now who's creating beautiful and extraordinary artworks using GANs. And there's lots of specifically CycleGANs. And there's actually at least maybe a dozen people I know of who are just doing interesting creative work with neural nets more generally. And the field of creative AI is going to expand dramatically. And I think it's interesting with Helena, I don't know her personally, but from what I understand of her background, she's a software developer as her full-time job and an artist as her hobby. And she's kind of started combining these two by saying, gosh, I wonder what this particular tool could bring to my art. And so if you follow her Twitter account, we'll make sure we add it on the Wiki. If somebody can find it, it's Helena Sarin. She basically posts a new work almost every day. And they're always pretty amazing. So here's the basic trick. And this is from the CycleGAN paper. We're going to have two images, assuming we're doing this with images. But the key thing is they're not paired images. So we don't have a data set of horses and the equivalent zebras. We've got a bunch of horses, bunch of zebras. Grab one horse, grab one zebra. We've now got an X. So let's say X is horse and Y is zebra. We're going to train a generator, what they call here a mapping function, that turns horse into zebra. We'll call that mapping function G. And we'll create one mapping function generator that turns a zebra into a horse. And we'll call that F. We'll create a discriminator, just like we did before, which is going to get as good as possible at recognizing real from fake horses. So that'll be DX. And then another discriminator which is going to be as good as possible at recognizing real from fake zebras. We'll call that DY. So that's kind of our starting point. But then the key thing to making this work, so we're kind of generating a loss function here. Here's one bit of the loss function, here's a second bit of the loss function. We're going to create something called cycle consistency loss, which says after you turn your horse into a zebra with your G generator, and check whether or not I can recognize that it's real. I keep forgetting which one is horse and which one is zebra. I apologize if I get my X's and Y's backwards. I turn my horse into a zebra, and then I'm going to try and turn that zebra back into the same horse that I started with. And so then I'm going to have another function that's going to check whether this horse, which I generated knowing nothing about X, generated entirely from this zebra, is similar to the original horse or not. So the idea would be, if your generated zebra doesn't look anything like your original horse, you've got no chance of turning it back into the original horse. So a loss which compares X hat to X is going to be really bad unless you can go into Y and back out again, and you're probably only going to be able to do that if you're able to create a zebra that looks like the original horse so that you know what the original horse looked like. And vice versa, take your zebra, turn it into a fake horse, and check that you can recognize that, and then try and turn it back into the original zebra and check that it looks like the original. So notice here, this F is our zebra to horse, this G is our horse to zebra, so the G and the F are kind of doing two things. They're both turning the original horse into the zebra and then turning the zebra back into the original horse. So notice that there's only two generators. There isn't a separate generator for the reverse mapping. You have to use the same generator that was used for the original mapping. So this is the cycle consistency loss, and I just think this is genius. The idea that this is a thing that could even be possible, honestly when this came out, it just never occurred to me as a thing that I could even try and solve. It seems so obviously impossible. And then the idea that you can solve it like this, I just think it's so damn smart. So it's good to look at the equations in this paper because they're just good examples. They're written pretty simply. It's not like some of the stuff in the Wasserstein-Gann paper, which is like lots of theoretical proofs and whatever else. In this case, they're just equations that just lay out what's going on, and you really want to get to a point where you can read them and understand them. So let's kind of start talking through them. So we've got a horse and a zebra. So for some mapping function G, which is our horse to zebra mapping function, then there's a GAN loss, which is the bit we're already familiar with, that says I've got a horse, a zebra, a fake zebra recognizer, and a horse to zebra generator, and the loss is what we saw before. It's our ability to draw one zebra out of our zebras and recognize whether it's real or fake. And then take a horse and turn it into a zebra and recognize whether that's real or fake. And then you do one minus the other. In this case, they've got a log in there. The log's not terribly important. So this is the thing we just saw. That's why we did Wasserstein GAN first, this is just a standard GAN loss in math form. All of this sounds awfully like translating in one language to another, then back to the original. Have GANs or any equivalent been tried in translation? Not that I know of. There's unsupervised machine translation, which does kind of do something like this, but I haven't looked at it closely enough to know if it's nearly identical or if it's just vaguely similar. So, to kind of back up to what I do know, normally with translation, you require this kind of paired input, you require parallel texts. This is the French translation of this English sentence. I do know there's been a couple of recent papers that show the ability to create good quality translation models without paired data. I haven't implemented them and I don't understand anything I haven't implemented, but yeah, they may well be doing the same basic idea. We'll look at it during the week and get back to you. So we've got our GAN loss, the next piece is the cycle consistency loss. And so the basic idea here is that we start with our horse, use our zebra generator on that to create a zebra, use our horse generator on that to create a horse, and then compare that to the original horse, and this double lines with the 1, we've seen this before, this is the L1 loss. So this is the sum of the absolute value of differences. Or else if this was a 2, it would be the L2 loss or the 2 norm, which would be the sum of square root of it actually. And again, we now know this squiggle idea, which is from our horses, grab a horse. So this is what we mean by sample from a distribution. There's all kinds of distributions, but most commonly in these papers, we're using an empirical distribution. In other words, we've got some rows of data, grab a row. So when you see this thing, squiggle, other thing, this thing here, when it says p data, that means grab something from the data, and we're going to call that thing x. So from our horses, pictures, grab a horse, turn it into a zebra, turn it back into a horse, compare it to the original, and sum up the absolute values. Do that for horse to zebra, do it for zebra to horse as well, add the 2 together, and that is our cycle consistency loss. So now we get our loss function, and the whole loss function depends on our horse generator, our zebra generator, our horse recognizer, our zebra recognizer discriminator, and we're going to add up the GAN loss for recognizing horses, the GAN loss for recognizing zebras, and the cycle consistency loss for our 2 generators. And then we've got a lambda here, which hopefully we're kind of used to this idea now, that is when you've got 2 different kinds of loss, you chuck in a parameter there that you can multiply them by so that they're about the same scale. We did a similar thing with our bounding box loss compared to our classifier loss when we did that localization stuff. So then we're going to try to, for this loss function, maximize the capability of the discriminators at discriminating whilst minimizing that for the generators. So the generators and the discriminators are going to be facing off against each other. So when you see this min-max thing in papers, you'll see it a lot, it basically means this idea that in your training loop, one thing is trying to make something better, the other is trying to make something worse, and there's lots of ways to do it, but most commonly you alternate between the two. And you'll often see this just referred to in math papers as min-max. So when you see min-max, you should immediately think, okay, adversarial training. So let's look at the code. And we're only going to, we probably won't be able to finish this today, but we're going to do something almost unheard of, which is I started looking at somebody else's code, and I was not so disgusted that I threw the whole thing away and did it myself. I actually said, I quite like this, I like it enough I'm going to show it to my students. So this is where the code comes from, so this is one of the people that created the original code for CycleGANs, and they've created a PyTorch version. And I had to clean it up a little bit, but it's actually pretty damn good. I think the first time I found code that I didn't feel the need to rewrite from scratch before I showed it to you. And so the cool thing about this is one of the reasons I like doing it this way, like finally finding something that's not awful, is that you're now going to get to see almost all the bits of fast AI, or like all the relevant bits of fast AI written in a different way by somebody else. And so you're going to get to see like, oh how they do datasets and data loaders and models and training loops and so forth. So you'll find there's a CycleGAN directory, which is basically nearly this, with some cleanups which I hope to submit as a PR sometime. It was written in a way that unfortunately made it a bit overconnected to how they were using it as a script, so I cleaned it up a little bit so I could use it as a module. But other than that, it's pretty similar. So CycleGAN is basically their code, copied from their GitHub repo with some minor changes. So the way the CycleGAN mini library has been set up is that the configuration options they're assuming are being passed into like a script. So they've got this trainOptionsParser method, and so you can see I'm basically passing in an array of script options. Where's my data, how many threads do I want to drop out, how many iterations, what am I going to call this model, which GPU do I want to run it on. So that gives me an opt object, which you can then see what it contains. You'll see it contains some things I didn't mention, and that's because it's got defaults for everything else that I didn't mention. So we're going to, rather than using fastai stuff, we're going to actually use CycleGAN stuff. So the first thing we're going to need is a data loader. And so this is also a great opportunity for you again to practice your ability to navigate through code with your editor or IDE of choice. So we're going to start with create data loader. So you should be able to go find symbol or in vim tag to jump straight to create data loader. And we can see that's creating a custom data set loader. And then we can see custom data set loader is a base data loader. So that doesn't really do anything. So basically we can see that it's going to use a standard PyTorch data loader. So that's good. And so we know if you're going to use a standard PyTorch data loader, you have to pass it a data set. And we know that a data set is something that contains a length and an indexer. So presumably when we look at create data set, it's going to do that. Here is create data set. So this library actually does more than just cycleGAN. It handles both aligned and unaligned image pairs. We know that our image pairs are unaligned. So we've got an unaligned data set. Here it is. And as expected, it has a get item and a length. And so the length is just whatever of our... so A and B is our horses and zebras. We've got two sets. So whichever one is longer is the length of the data loader. And so get item is just going to go ahead and randomly grab something from each of our two horses and zebras, open them up with PIL, or P-I-L, run them through some transformations. And then we could either be turning horses into zebras or zebras into horses, so there's some direction. And then it will just go ahead and return our horse and our zebra and our path to the horse and the path to the zebra. So hopefully you can kind of see that this is looking pretty similar to the kind of stuff that Fast.ai does. Fast.ai obviously does quite a lot more when it comes to transforms and performance and stuff like this. But you know, remember this is like research code for this one thing. Like it's pretty cool that they did all this work. So we've got a data loader. So we can go and load our data into it. And so that will tell us how many mini-batches are in it. That's the length of the data loader in PyTorch. Next step, we've got a data loader, is to create a model. So you can go tag for create model. There it is. Okay, same idea. We've got different kinds of models. So we're going to be doing a cycleGAN. So here's our cycleGAN model. Okay, so there's quite a lot of stuff in a cycleGAN model. So let's go through and find out what's going to be used. But basically, at this stage, we've just called initializer, and so when we initialize it, you can see it's going to go through and it's going to define two generators, which is not surprising, a generator for our horses and a generator for our zebras. Let's see what else we've got here. Okay, there's some way for it to generate a pool of fake data. And then here we're going to grab our GAN loss, and as we talked about, our cycle consistency loss is an L1 loss. That's interesting. They're going to use Adam. So obviously for cycleGANs, they found Adam works pretty well. And so then we're going to have an optimizer for our horse discriminator, an optimizer for our zebra discriminator, and an optimizer for our generator. The optimizer for the generator is going to contain the parameters both for the horse generator and the zebra generator all in one place. So the initializer is going to set up all of the different networks and loss functions we need and they're all going to be stored inside this model. And so then it prints out and shows us exactly the PyTorch bottles we have. And so it's interesting to see that they're using ResNets. And so you can see the ResNets look pretty familiar. We've got conv batchNorm relu, conv batchNorm. So instanceNorm is just the same as batchNorm basically, but it applies it to one image at a time. The difference isn't particularly important. And you can see they're doing reflection padding just like we are. You can kind of see when you try to build everything from scratch like this, it is a lot of work. And you can kind of get the nice little things that FastAI does automatically for you, you kind of have to do all of them by hand and only end up with a subset of them. So over time, hopefully soon we'll get all of this GAN stuff into FastAI and it'll be nice and easy. So we've got our model. And remember the model contains the loss functions, it contains the generators, it contains the discriminators all in one convenient place. So I've gone ahead and kind of copied and pasted and slightly refactored the training loop from their code so that we can run it inside the notebook. So this is all pretty familiar, right? A loop to go through each epoch and a loop to go through the data. Before we did this, we set up our, this is actually not a PyTorch dataset, I think this is what they used, slightly confusingly, to talk about their combined, what we would call a model data object, I guess, all the data that they need. Look through that with TQDM to get a progress bar. And so now we can go through and see what happens in the model. So set input, so set input. So it's kind of a different approach to what we do in FastAI. This is kind of neat, you know, it's quite specific to CycleGANs, but basically internally inside this model is this idea that we're going to go into our data and grab, you know, we're either going horse to zebra or zebra to horse, depending on which way we go. We either, you know, A is either the horse or the zebra and vice versa, and if necessary, put it on the appropriate GPU and then grab the appropriate paths. Okay, so the model now has a mini batch of horses and a mini batch of zebras. And so now we optimize the parameters. Okay, so it's kind of nice to see it like this. You can see each step, right? So first of all, try to optimize the generators, then try to optimize the horse discriminator, then try to optimize the zebra discriminator. Zero grad is part of PyTorch, step is part of PyTorch. So the interesting bit is the actual thing that does the back propagation on the generator. So here it is. And let's jump to the key pieces. There's all the bits, all the formula that we basically just saw from the paper. So let's take a horse and generate a zebra. So we've now got a fake zebra. And let's now use the discriminator to see if we can tell whether it's fake or not. And then let's pop that into our loss function, which we set up earlier, to see if we can basically get a loss function based on that prediction. Then let's do the same thing to do the GAN loss. So go in the opposite direction, and then we need to use the opposite discriminator, and then put that through the loss function again. And then let's do the cycle consistency loss. So again, we take our fake, which we created up here, and try and turn it back again into the original. And then let's use that loss function, cycle consistency loss function we created earlier to compare it to the real original. And here's that lambda. So there's some weight that we used, and that was set up actually, we just used the default that they suggested in their options. And then do the same for the opposite direction, and then add them all together. Do the backward step, and that's it. So we can then do the same thing for the first discriminator. And since basically all the work's been done now, there's much less to do here. So I won't step all through it, but it's basically the same basic stuff that we've already seen. So optimize parameters basically is calculating the losses and doing the optimizer step from time to time, save and print out some results. And then from time to time, update the learning rate, so they've got some learning rate annealing built in here as well. It isn't very exciting, but you can take a look at it. So they've basically got some kind of fast AI, they've got this idea of schedulers, which you can then use to update your learning rates. So I think for those of you who are interested in better understanding deep learning APIs or interested in contributing more to fast AI, or interested in creating your own version of some of this stuff in some different backend, it's cool to look at a second kind of API that covers some subset of some of the similar things to get a sense of how are they solving some of these problems and what are the similarities and what are the differences. So we train that for a little while, and then we can just grab a few examples, and here we have them. So here are our horses, here they are as zebras, and here they are back as horses again. Here's a zebra into a horse, back into a zebra, it's kind of thrown away its head for some reason but not so much it couldn't get it back again. This is a really interesting one, this is obviously not what zebras look like, but if it's going to be a zebra version of that horse. It's also interesting to see its failure situations, I guess it doesn't very often see basically just an eyeball, it has no idea how to do that one. So some of them don't work very well, this one's done a pretty good job. This one's interesting, it's done a good job with that one and that one, but for some reason the one in the middle didn't get to go. This one's a really weird shape, but it's done a reasonable job of it. This one looks good. This one's pretty sloppy. Again, the fork just ahead, it's not bad. So you know, it took me like 24 hours to train it even that far, so it's kind of slow. And I know Helena is constantly complaining on Twitter about how long these things take, I don't know how she's so productive with them. So yeah, I will mention one more thing that just came out yesterday, which is there's now a multimodal image-to-image translation of unpaird, and so you can basically now create different cats, for instance, from this doc. So this is basically not just creating one example of the output that you want, but creating multiple ones. So here's house cat to big cat, and here's big cat to house cat. This is the paper, so this came out like yesterday or the day before, I think. I think it's pretty amazing cat to dog. So you can kind of see how this technology is developing, and I think there's so many opportunities to maybe do this with music or speech or writing or to create kind of tools for artists or whatever. All right, thanks everybody, and see you next week.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.78, "text": " So, we're going to be talking about GANs today.", "tokens": [407, 11, 321, 434, 516, 281, 312, 1417, 466, 460, 1770, 82, 965, 13], "temperature": 0.0, "avg_logprob": -0.23011048634847006, "compression_ratio": 1.483695652173913, "no_speech_prob": 0.017436131834983826}, {"id": 1, "seek": 0, "start": 5.78, "end": 9.84, "text": " Who has heard of GANs?", "tokens": [2102, 575, 2198, 295, 460, 1770, 82, 30], "temperature": 0.0, "avg_logprob": -0.23011048634847006, "compression_ratio": 1.483695652173913, "no_speech_prob": 0.017436131834983826}, {"id": 2, "seek": 0, "start": 9.84, "end": 20.66, "text": " Very hot technology, but definitely deserving to be in the cutting edge deep learning part", "tokens": [4372, 2368, 2899, 11, 457, 2138, 48781, 281, 312, 294, 264, 6492, 4691, 2452, 2539, 644], "temperature": 0.0, "avg_logprob": -0.23011048634847006, "compression_ratio": 1.483695652173913, "no_speech_prob": 0.017436131834983826}, {"id": 3, "seek": 0, "start": 20.66, "end": 28.32, "text": " of the course because they're not quite proven to be necessarily useful for anything, but", "tokens": [295, 264, 1164, 570, 436, 434, 406, 1596, 12785, 281, 312, 4725, 4420, 337, 1340, 11, 457], "temperature": 0.0, "avg_logprob": -0.23011048634847006, "compression_ratio": 1.483695652173913, "no_speech_prob": 0.017436131834983826}, {"id": 4, "seek": 0, "start": 28.32, "end": 29.72, "text": " they're nearly there.", "tokens": [436, 434, 6217, 456, 13], "temperature": 0.0, "avg_logprob": -0.23011048634847006, "compression_ratio": 1.483695652173913, "no_speech_prob": 0.017436131834983826}, {"id": 5, "seek": 2972, "start": 29.72, "end": 35.36, "text": " They're definitely going to get there and we're going to focus on the things where they're", "tokens": [814, 434, 2138, 516, 281, 483, 456, 293, 321, 434, 516, 281, 1879, 322, 264, 721, 689, 436, 434], "temperature": 0.0, "avg_logprob": -0.1736703872680664, "compression_ratio": 1.966804979253112, "no_speech_prob": 4.985046689398587e-05}, {"id": 6, "seek": 2972, "start": 35.36, "end": 38.8, "text": " definitely going to be useful in practice.", "tokens": [2138, 516, 281, 312, 4420, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.1736703872680664, "compression_ratio": 1.966804979253112, "no_speech_prob": 4.985046689398587e-05}, {"id": 7, "seek": 2972, "start": 38.8, "end": 41.8, "text": " There's a number of areas where they may turn out to be useful in practice, but we don't", "tokens": [821, 311, 257, 1230, 295, 3179, 689, 436, 815, 1261, 484, 281, 312, 4420, 294, 3124, 11, 457, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.1736703872680664, "compression_ratio": 1.966804979253112, "no_speech_prob": 4.985046689398587e-05}, {"id": 8, "seek": 2972, "start": 41.8, "end": 42.8, "text": " know yet.", "tokens": [458, 1939, 13], "temperature": 0.0, "avg_logprob": -0.1736703872680664, "compression_ratio": 1.966804979253112, "no_speech_prob": 4.985046689398587e-05}, {"id": 9, "seek": 2972, "start": 42.8, "end": 48.099999999999994, "text": " So, I think the area that they're definitely going to be useful in practice is the kind", "tokens": [407, 11, 286, 519, 264, 1859, 300, 436, 434, 2138, 516, 281, 312, 4420, 294, 3124, 307, 264, 733], "temperature": 0.0, "avg_logprob": -0.1736703872680664, "compression_ratio": 1.966804979253112, "no_speech_prob": 4.985046689398587e-05}, {"id": 10, "seek": 2972, "start": 48.099999999999994, "end": 53.44, "text": " of thing you see on the left here, which is for example turning drawings into rendered", "tokens": [295, 551, 291, 536, 322, 264, 1411, 510, 11, 597, 307, 337, 1365, 6246, 18618, 666, 28748], "temperature": 0.0, "avg_logprob": -0.1736703872680664, "compression_ratio": 1.966804979253112, "no_speech_prob": 4.985046689398587e-05}, {"id": 11, "seek": 2972, "start": 53.44, "end": 55.0, "text": " pictures.", "tokens": [5242, 13], "temperature": 0.0, "avg_logprob": -0.1736703872680664, "compression_ratio": 1.966804979253112, "no_speech_prob": 4.985046689398587e-05}, {"id": 12, "seek": 2972, "start": 55.0, "end": 59.519999999999996, "text": " This comes from a paper that just came out two days ago.", "tokens": [639, 1487, 490, 257, 3035, 300, 445, 1361, 484, 732, 1708, 2057, 13], "temperature": 0.0, "avg_logprob": -0.1736703872680664, "compression_ratio": 1.966804979253112, "no_speech_prob": 4.985046689398587e-05}, {"id": 13, "seek": 5952, "start": 59.52, "end": 65.56, "text": " So there's a very active research going on right now.", "tokens": [407, 456, 311, 257, 588, 4967, 2132, 516, 322, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.1648377598942937, "compression_ratio": 1.572139303482587, "no_speech_prob": 1.5936229829094373e-05}, {"id": 14, "seek": 5952, "start": 65.56, "end": 72.42, "text": " Before we get there though, let's talk about some interesting stuff from the last class.", "tokens": [4546, 321, 483, 456, 1673, 11, 718, 311, 751, 466, 512, 1880, 1507, 490, 264, 1036, 1508, 13], "temperature": 0.0, "avg_logprob": -0.1648377598942937, "compression_ratio": 1.572139303482587, "no_speech_prob": 1.5936229829094373e-05}, {"id": 15, "seek": 5952, "start": 72.42, "end": 77.52000000000001, "text": " This is an interesting thing that one of our diversity fellows, Christine Payne, did.", "tokens": [639, 307, 364, 1880, 551, 300, 472, 295, 527, 8811, 35595, 11, 24038, 11431, 716, 11, 630, 13], "temperature": 0.0, "avg_logprob": -0.1648377598942937, "compression_ratio": 1.572139303482587, "no_speech_prob": 1.5936229829094373e-05}, {"id": 16, "seek": 5952, "start": 77.52000000000001, "end": 84.04, "text": " Christine has a master's in medicine from Stanford and so she obviously had an interest", "tokens": [24038, 575, 257, 4505, 311, 294, 7195, 490, 20374, 293, 370, 750, 2745, 632, 364, 1179], "temperature": 0.0, "avg_logprob": -0.1648377598942937, "compression_ratio": 1.572139303482587, "no_speech_prob": 1.5936229829094373e-05}, {"id": 17, "seek": 8404, "start": 84.04, "end": 90.84, "text": " in thinking what would it look like if we built a language model of medicine.", "tokens": [294, 1953, 437, 576, 309, 574, 411, 498, 321, 3094, 257, 2856, 2316, 295, 7195, 13], "temperature": 0.0, "avg_logprob": -0.1262506361930601, "compression_ratio": 1.6943231441048034, "no_speech_prob": 2.058028712781379e-06}, {"id": 18, "seek": 8404, "start": 90.84, "end": 95.60000000000001, "text": " One of the things that we briefly touched on back in Lesson 4, but didn't really talk", "tokens": [1485, 295, 264, 721, 300, 321, 10515, 9828, 322, 646, 294, 18649, 266, 1017, 11, 457, 994, 380, 534, 751], "temperature": 0.0, "avg_logprob": -0.1262506361930601, "compression_ratio": 1.6943231441048034, "no_speech_prob": 2.058028712781379e-06}, {"id": 19, "seek": 8404, "start": 95.60000000000001, "end": 101.76, "text": " much about last time, is this idea you can actually seed a generative language model,", "tokens": [709, 466, 1036, 565, 11, 307, 341, 1558, 291, 393, 767, 8871, 257, 1337, 1166, 2856, 2316, 11], "temperature": 0.0, "avg_logprob": -0.1262506361930601, "compression_ratio": 1.6943231441048034, "no_speech_prob": 2.058028712781379e-06}, {"id": 20, "seek": 8404, "start": 101.76, "end": 106.08000000000001, "text": " which basically means you've trained a language model on some corpus and then you're going", "tokens": [597, 1936, 1355, 291, 600, 8895, 257, 2856, 2316, 322, 512, 1181, 31624, 293, 550, 291, 434, 516], "temperature": 0.0, "avg_logprob": -0.1262506361930601, "compression_ratio": 1.6943231441048034, "no_speech_prob": 2.058028712781379e-06}, {"id": 21, "seek": 8404, "start": 106.08000000000001, "end": 109.64000000000001, "text": " to generate some text from that language model.", "tokens": [281, 8460, 512, 2487, 490, 300, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1262506361930601, "compression_ratio": 1.6943231441048034, "no_speech_prob": 2.058028712781379e-06}, {"id": 22, "seek": 10964, "start": 109.64, "end": 115.72, "text": " So you can start off by feeding it a few words, to basically say here's the first few words", "tokens": [407, 291, 393, 722, 766, 538, 12919, 309, 257, 1326, 2283, 11, 281, 1936, 584, 510, 311, 264, 700, 1326, 2283], "temperature": 0.0, "avg_logprob": -0.1993924223858377, "compression_ratio": 1.7488372093023257, "no_speech_prob": 3.393109182070475e-06}, {"id": 23, "seek": 10964, "start": 115.72, "end": 121.12, "text": " to create the hidden state in the language model and then generate from there, please.", "tokens": [281, 1884, 264, 7633, 1785, 294, 264, 2856, 2316, 293, 550, 8460, 490, 456, 11, 1767, 13], "temperature": 0.0, "avg_logprob": -0.1993924223858377, "compression_ratio": 1.7488372093023257, "no_speech_prob": 3.393109182070475e-06}, {"id": 24, "seek": 10964, "start": 121.12, "end": 128.2, "text": " So Christine did something clever, which was to kind of pick a, was to seed it with a question", "tokens": [407, 24038, 630, 746, 13494, 11, 597, 390, 281, 733, 295, 1888, 257, 11, 390, 281, 8871, 309, 365, 257, 1168], "temperature": 0.0, "avg_logprob": -0.1993924223858377, "compression_ratio": 1.7488372093023257, "no_speech_prob": 3.393109182070475e-06}, {"id": 25, "seek": 10964, "start": 128.2, "end": 133.56, "text": " and then repeat the question three times, Christine, three times, and then let it generate", "tokens": [293, 550, 7149, 264, 1168, 1045, 1413, 11, 24038, 11, 1045, 1413, 11, 293, 550, 718, 309, 8460], "temperature": 0.0, "avg_logprob": -0.1993924223858377, "compression_ratio": 1.7488372093023257, "no_speech_prob": 3.393109182070475e-06}, {"id": 26, "seek": 10964, "start": 133.56, "end": 135.84, "text": " from there.", "tokens": [490, 456, 13], "temperature": 0.0, "avg_logprob": -0.1993924223858377, "compression_ratio": 1.7488372093023257, "no_speech_prob": 3.393109182070475e-06}, {"id": 27, "seek": 13584, "start": 135.84, "end": 141.4, "text": " And so she fed a language model lots of different medical texts and then fed it this question,", "tokens": [400, 370, 750, 4636, 257, 2856, 2316, 3195, 295, 819, 4625, 15765, 293, 550, 4636, 309, 341, 1168, 11], "temperature": 0.0, "avg_logprob": -0.1913334090134193, "compression_ratio": 1.6906474820143884, "no_speech_prob": 1.2805084224964958e-05}, {"id": 28, "seek": 13584, "start": 141.4, "end": 143.52, "text": " what is the prevalence of malaria?", "tokens": [437, 307, 264, 42583, 295, 45182, 30], "temperature": 0.0, "avg_logprob": -0.1913334090134193, "compression_ratio": 1.6906474820143884, "no_speech_prob": 1.2805084224964958e-05}, {"id": 29, "seek": 13584, "start": 143.52, "end": 149.36, "text": " And the model said in the US about 10% of the population has the virus, but only about", "tokens": [400, 264, 2316, 848, 294, 264, 2546, 466, 1266, 4, 295, 264, 4415, 575, 264, 5752, 11, 457, 787, 466], "temperature": 0.0, "avg_logprob": -0.1913334090134193, "compression_ratio": 1.6906474820143884, "no_speech_prob": 1.2805084224964958e-05}, {"id": 30, "seek": 13584, "start": 149.36, "end": 152.8, "text": " 1% is infected with the virus, about 50 to 80 million infected.", "tokens": [502, 4, 307, 15414, 365, 264, 5752, 11, 466, 2625, 281, 4688, 2459, 15414, 13], "temperature": 0.0, "avg_logprob": -0.1913334090134193, "compression_ratio": 1.6906474820143884, "no_speech_prob": 1.2805084224964958e-05}, {"id": 31, "seek": 13584, "start": 152.8, "end": 156.12, "text": " She said what's the treatment for ectopic pregnancy?", "tokens": [1240, 848, 437, 311, 264, 5032, 337, 308, 349, 40216, 16120, 30], "temperature": 0.0, "avg_logprob": -0.1913334090134193, "compression_ratio": 1.6906474820143884, "no_speech_prob": 1.2805084224964958e-05}, {"id": 32, "seek": 13584, "start": 156.12, "end": 160.28, "text": " And it said it's a safe and safe treatment for women with a history of symptoms that", "tokens": [400, 309, 848, 309, 311, 257, 3273, 293, 3273, 5032, 337, 2266, 365, 257, 2503, 295, 8332, 300], "temperature": 0.0, "avg_logprob": -0.1913334090134193, "compression_ratio": 1.6906474820143884, "no_speech_prob": 1.2805084224964958e-05}, {"id": 33, "seek": 13584, "start": 160.28, "end": 162.36, "text": " may have a significant impact on clinical response.", "tokens": [815, 362, 257, 4776, 2712, 322, 9115, 4134, 13], "temperature": 0.0, "avg_logprob": -0.1913334090134193, "compression_ratio": 1.6906474820143884, "no_speech_prob": 1.2805084224964958e-05}, {"id": 34, "seek": 16236, "start": 162.36, "end": 166.76000000000002, "text": " What's an important factor, development and management of ectopic pregnancies, etc.", "tokens": [708, 311, 364, 1021, 5952, 11, 3250, 293, 4592, 295, 308, 349, 40216, 7681, 32286, 11, 5183, 13], "temperature": 0.0, "avg_logprob": -0.21967205047607422, "compression_ratio": 1.6611570247933884, "no_speech_prob": 9.665957804827485e-06}, {"id": 35, "seek": 16236, "start": 166.76000000000002, "end": 175.76000000000002, "text": " And so what I find interesting about this is, you know, it's pretty close to being a, to", "tokens": [400, 370, 437, 286, 915, 1880, 466, 341, 307, 11, 291, 458, 11, 309, 311, 1238, 1998, 281, 885, 257, 11, 281], "temperature": 0.0, "avg_logprob": -0.21967205047607422, "compression_ratio": 1.6611570247933884, "no_speech_prob": 9.665957804827485e-06}, {"id": 36, "seek": 16236, "start": 175.76000000000002, "end": 179.24, "text": " me as somebody who doesn't have a master's in medicine from Stanford, it's pretty close", "tokens": [385, 382, 2618, 567, 1177, 380, 362, 257, 4505, 311, 294, 7195, 490, 20374, 11, 309, 311, 1238, 1998], "temperature": 0.0, "avg_logprob": -0.21967205047607422, "compression_ratio": 1.6611570247933884, "no_speech_prob": 9.665957804827485e-06}, {"id": 37, "seek": 16236, "start": 179.24, "end": 183.52, "text": " to being a believable answer to the question.", "tokens": [281, 885, 257, 1351, 17915, 1867, 281, 264, 1168, 13], "temperature": 0.0, "avg_logprob": -0.21967205047607422, "compression_ratio": 1.6611570247933884, "no_speech_prob": 9.665957804827485e-06}, {"id": 38, "seek": 16236, "start": 183.52, "end": 189.8, "text": " But it really has no bearing on reality whatsoever and I kind of think it's an interesting kind", "tokens": [583, 309, 534, 575, 572, 17350, 322, 4103, 17076, 293, 286, 733, 295, 519, 309, 311, 364, 1880, 733], "temperature": 0.0, "avg_logprob": -0.21967205047607422, "compression_ratio": 1.6611570247933884, "no_speech_prob": 9.665957804827485e-06}, {"id": 39, "seek": 18980, "start": 189.8, "end": 193.56, "text": " of ethical and user experience quandary.", "tokens": [295, 18890, 293, 4195, 1752, 6932, 822, 13], "temperature": 0.0, "avg_logprob": -0.13009787118563088, "compression_ratio": 1.5770750988142292, "no_speech_prob": 2.796515400405042e-05}, {"id": 40, "seek": 18980, "start": 193.56, "end": 200.68, "text": " So actually I'm involved also in a company called doc.ai that's trying to basically do", "tokens": [407, 767, 286, 478, 3288, 611, 294, 257, 2237, 1219, 3211, 13, 1301, 300, 311, 1382, 281, 1936, 360], "temperature": 0.0, "avg_logprob": -0.13009787118563088, "compression_ratio": 1.5770750988142292, "no_speech_prob": 2.796515400405042e-05}, {"id": 41, "seek": 18980, "start": 200.68, "end": 205.60000000000002, "text": " a number of things, but in the end provide an app for doctors and patients which can", "tokens": [257, 1230, 295, 721, 11, 457, 294, 264, 917, 2893, 364, 724, 337, 8778, 293, 4209, 597, 393], "temperature": 0.0, "avg_logprob": -0.13009787118563088, "compression_ratio": 1.5770750988142292, "no_speech_prob": 2.796515400405042e-05}, {"id": 42, "seek": 18980, "start": 205.60000000000002, "end": 210.4, "text": " help kind of create a conversational user interface around helping them with their medical", "tokens": [854, 733, 295, 1884, 257, 2615, 1478, 4195, 9226, 926, 4315, 552, 365, 641, 4625], "temperature": 0.0, "avg_logprob": -0.13009787118563088, "compression_ratio": 1.5770750988142292, "no_speech_prob": 2.796515400405042e-05}, {"id": 43, "seek": 18980, "start": 210.4, "end": 211.54000000000002, "text": " issues.", "tokens": [2663, 13], "temperature": 0.0, "avg_logprob": -0.13009787118563088, "compression_ratio": 1.5770750988142292, "no_speech_prob": 2.796515400405042e-05}, {"id": 44, "seek": 18980, "start": 211.54000000000002, "end": 217.08, "text": " And I've been kind of continually saying to the software engineers on that team, please", "tokens": [400, 286, 600, 668, 733, 295, 22277, 1566, 281, 264, 4722, 11955, 322, 300, 1469, 11, 1767], "temperature": 0.0, "avg_logprob": -0.13009787118563088, "compression_ratio": 1.5770750988142292, "no_speech_prob": 2.796515400405042e-05}, {"id": 45, "seek": 21708, "start": 217.08, "end": 222.84, "text": " don't try to create a generative model using like an LSTM or something because they're", "tokens": [500, 380, 853, 281, 1884, 257, 1337, 1166, 2316, 1228, 411, 364, 441, 6840, 44, 420, 746, 570, 436, 434], "temperature": 0.0, "avg_logprob": -0.14676493567389412, "compression_ratio": 1.4974358974358974, "no_speech_prob": 6.0486372603918426e-06}, {"id": 46, "seek": 21708, "start": 222.84, "end": 228.76000000000002, "text": " going to be really good at creating bad advice that sounds impressive.", "tokens": [516, 281, 312, 534, 665, 412, 4084, 1578, 5192, 300, 3263, 8992, 13], "temperature": 0.0, "avg_logprob": -0.14676493567389412, "compression_ratio": 1.4974358974358974, "no_speech_prob": 6.0486372603918426e-06}, {"id": 47, "seek": 21708, "start": 228.76000000000002, "end": 235.52, "text": " You know, kind of like, you know, political pundits or tenured professors, you know, people", "tokens": [509, 458, 11, 733, 295, 411, 11, 291, 458, 11, 3905, 280, 997, 1208, 420, 2064, 3831, 15924, 11, 291, 458, 11, 561], "temperature": 0.0, "avg_logprob": -0.14676493567389412, "compression_ratio": 1.4974358974358974, "no_speech_prob": 6.0486372603918426e-06}, {"id": 48, "seek": 21708, "start": 235.52, "end": 240.56, "text": " who can say bullshit with great authority.", "tokens": [567, 393, 584, 22676, 365, 869, 8281, 13], "temperature": 0.0, "avg_logprob": -0.14676493567389412, "compression_ratio": 1.4974358974358974, "no_speech_prob": 6.0486372603918426e-06}, {"id": 49, "seek": 24056, "start": 240.56, "end": 249.12, "text": " So I think, yeah, I thought it was a really interesting experiment.", "tokens": [407, 286, 519, 11, 1338, 11, 286, 1194, 309, 390, 257, 534, 1880, 5120, 13], "temperature": 0.0, "avg_logprob": -0.3685099013308261, "compression_ratio": 1.5206611570247934, "no_speech_prob": 3.373389699845575e-05}, {"id": 50, "seek": 24056, "start": 249.12, "end": 252.68, "text": " And great to see what our diversity fellows are doing.", "tokens": [400, 869, 281, 536, 437, 527, 8811, 35595, 366, 884, 13], "temperature": 0.0, "avg_logprob": -0.3685099013308261, "compression_ratio": 1.5206611570247934, "no_speech_prob": 3.373389699845575e-05}, {"id": 51, "seek": 24056, "start": 252.68, "end": 254.92000000000002, "text": " I mean this is why we have this program.", "tokens": [286, 914, 341, 307, 983, 321, 362, 341, 1461, 13], "temperature": 0.0, "avg_logprob": -0.3685099013308261, "compression_ratio": 1.5206611570247934, "no_speech_prob": 3.373389699845575e-05}, {"id": 52, "seek": 24056, "start": 254.92000000000002, "end": 261.1, "text": " I suppose I shouldn't just say master's in medicine, actually a Julliard-trained classical", "tokens": [286, 7297, 286, 4659, 380, 445, 584, 4505, 311, 294, 7195, 11, 767, 257, 508, 858, 72, 515, 12, 17227, 2001, 13735], "temperature": 0.0, "avg_logprob": -0.3685099013308261, "compression_ratio": 1.5206611570247934, "no_speech_prob": 3.373389699845575e-05}, {"id": 53, "seek": 24056, "start": 261.1, "end": 266.68, "text": " musician and actually also a Princeton Ballot, Victoria and Physics, so also a high-performance", "tokens": [19570, 293, 767, 611, 257, 36592, 10744, 310, 11, 16656, 293, 38355, 11, 370, 611, 257, 1090, 12, 50242], "temperature": 0.0, "avg_logprob": -0.3685099013308261, "compression_ratio": 1.5206611570247934, "no_speech_prob": 3.373389699845575e-05}, {"id": 54, "seek": 24056, "start": 266.68, "end": 267.68, "text": " computing expert.", "tokens": [15866, 5844, 13], "temperature": 0.0, "avg_logprob": -0.3685099013308261, "compression_ratio": 1.5206611570247934, "no_speech_prob": 3.373389699845575e-05}, {"id": 55, "seek": 26768, "start": 267.68, "end": 271.12, "text": " Yeah, okay, so she does a bit of everything.", "tokens": [865, 11, 1392, 11, 370, 750, 775, 257, 857, 295, 1203, 13], "temperature": 0.0, "avg_logprob": -0.21366131062410315, "compression_ratio": 1.5355648535564854, "no_speech_prob": 1.4738223399035633e-05}, {"id": 56, "seek": 26768, "start": 271.12, "end": 276.44, "text": " So yeah, really impressive group of people and great to see such exciting kind of ideas", "tokens": [407, 1338, 11, 534, 8992, 1594, 295, 561, 293, 869, 281, 536, 1270, 4670, 733, 295, 3487], "temperature": 0.0, "avg_logprob": -0.21366131062410315, "compression_ratio": 1.5355648535564854, "no_speech_prob": 1.4738223399035633e-05}, {"id": 57, "seek": 26768, "start": 276.44, "end": 277.44, "text": " coming out.", "tokens": [1348, 484, 13], "temperature": 0.0, "avg_logprob": -0.21366131062410315, "compression_ratio": 1.5355648535564854, "no_speech_prob": 1.4738223399035633e-05}, {"id": 58, "seek": 26768, "start": 277.44, "end": 283.92, "text": " And if you're wondering, you know, I've done some interesting experiments.", "tokens": [400, 498, 291, 434, 6359, 11, 291, 458, 11, 286, 600, 1096, 512, 1880, 12050, 13], "temperature": 0.0, "avg_logprob": -0.21366131062410315, "compression_ratio": 1.5355648535564854, "no_speech_prob": 1.4738223399035633e-05}, {"id": 59, "seek": 26768, "start": 283.92, "end": 286.8, "text": " Should I let people know about it?", "tokens": [6454, 286, 718, 561, 458, 466, 309, 30], "temperature": 0.0, "avg_logprob": -0.21366131062410315, "compression_ratio": 1.5355648535564854, "no_speech_prob": 1.4738223399035633e-05}, {"id": 60, "seek": 26768, "start": 286.8, "end": 292.96000000000004, "text": " Well I, Christine mentioned this in the forum, I went on to mention it on Twitter, to which", "tokens": [1042, 286, 11, 24038, 2835, 341, 294, 264, 17542, 11, 286, 1437, 322, 281, 2152, 309, 322, 5794, 11, 281, 597], "temperature": 0.0, "avg_logprob": -0.21366131062410315, "compression_ratio": 1.5355648535564854, "no_speech_prob": 1.4738223399035633e-05}, {"id": 61, "seek": 26768, "start": 292.96000000000004, "end": 295.08, "text": " I got this response.", "tokens": [286, 658, 341, 4134, 13], "temperature": 0.0, "avg_logprob": -0.21366131062410315, "compression_ratio": 1.5355648535564854, "no_speech_prob": 1.4738223399035633e-05}, {"id": 62, "seek": 29508, "start": 295.08, "end": 299.2, "text": " If you're looking for a job, you may be wondering who Xavier Ameritrain is.", "tokens": [759, 291, 434, 1237, 337, 257, 1691, 11, 291, 815, 312, 6359, 567, 44653, 22597, 270, 7146, 307, 13], "temperature": 0.0, "avg_logprob": -0.20503192477756077, "compression_ratio": 1.5, "no_speech_prob": 6.401599239325151e-05}, {"id": 63, "seek": 29508, "start": 299.2, "end": 304.52, "text": " Well he is the founder of a hot new medical AI startup.", "tokens": [1042, 415, 307, 264, 14917, 295, 257, 2368, 777, 4625, 7318, 18578, 13], "temperature": 0.0, "avg_logprob": -0.20503192477756077, "compression_ratio": 1.5, "no_speech_prob": 6.401599239325151e-05}, {"id": 64, "seek": 29508, "start": 304.52, "end": 307.88, "text": " He was previously the head of engineering at Quora.", "tokens": [634, 390, 8046, 264, 1378, 295, 7043, 412, 2326, 3252, 13], "temperature": 0.0, "avg_logprob": -0.20503192477756077, "compression_ratio": 1.5, "no_speech_prob": 6.401599239325151e-05}, {"id": 65, "seek": 29508, "start": 307.88, "end": 311.52, "text": " Before that he was the guy at Netflix who ran the data science team and built their", "tokens": [4546, 300, 415, 390, 264, 2146, 412, 12778, 567, 5872, 264, 1412, 3497, 1469, 293, 3094, 641], "temperature": 0.0, "avg_logprob": -0.20503192477756077, "compression_ratio": 1.5, "no_speech_prob": 6.401599239325151e-05}, {"id": 66, "seek": 29508, "start": 311.52, "end": 313.59999999999997, "text": " recommender systems.", "tokens": [2748, 260, 3652, 13], "temperature": 0.0, "avg_logprob": -0.20503192477756077, "compression_ratio": 1.5, "no_speech_prob": 6.401599239325151e-05}, {"id": 67, "seek": 29508, "start": 313.59999999999997, "end": 317.64, "text": " And so this is what happens if you do something cool.", "tokens": [400, 370, 341, 307, 437, 2314, 498, 291, 360, 746, 1627, 13], "temperature": 0.0, "avg_logprob": -0.20503192477756077, "compression_ratio": 1.5, "no_speech_prob": 6.401599239325151e-05}, {"id": 68, "seek": 31764, "start": 317.64, "end": 326.56, "text": " Let people know about it and get noticed by awesome people like Xavier.", "tokens": [961, 561, 458, 466, 309, 293, 483, 5694, 538, 3476, 561, 411, 44653, 13], "temperature": 0.0, "avg_logprob": -0.1358675580275686, "compression_ratio": 1.478494623655914, "no_speech_prob": 1.0289400051988196e-05}, {"id": 69, "seek": 31764, "start": 326.56, "end": 332.56, "text": " So let's talk about Sci-Fi 10.", "tokens": [407, 718, 311, 751, 466, 16942, 12, 13229, 1266, 13], "temperature": 0.0, "avg_logprob": -0.1358675580275686, "compression_ratio": 1.478494623655914, "no_speech_prob": 1.0289400051988196e-05}, {"id": 70, "seek": 31764, "start": 332.56, "end": 339.59999999999997, "text": " And the reason I'm going to talk about Sci-Fi 10 is that we're going to be looking at some", "tokens": [400, 264, 1778, 286, 478, 516, 281, 751, 466, 16942, 12, 13229, 1266, 307, 300, 321, 434, 516, 281, 312, 1237, 412, 512], "temperature": 0.0, "avg_logprob": -0.1358675580275686, "compression_ratio": 1.478494623655914, "no_speech_prob": 1.0289400051988196e-05}, {"id": 71, "seek": 31764, "start": 339.59999999999997, "end": 347.08, "text": " more bare-bones PyTorch stuff today to build these generative adversarial models.", "tokens": [544, 6949, 12, 44954, 9953, 51, 284, 339, 1507, 965, 281, 1322, 613, 1337, 1166, 17641, 44745, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1358675580275686, "compression_ratio": 1.478494623655914, "no_speech_prob": 1.0289400051988196e-05}, {"id": 72, "seek": 34708, "start": 347.08, "end": 352.64, "text": " There's no really fast AI support to speak of at all for GANs at the moment.", "tokens": [821, 311, 572, 534, 2370, 7318, 1406, 281, 1710, 295, 412, 439, 337, 460, 1770, 82, 412, 264, 1623, 13], "temperature": 0.0, "avg_logprob": -0.1640026878764611, "compression_ratio": 1.6632996632996633, "no_speech_prob": 4.006176823168062e-05}, {"id": 73, "seek": 34708, "start": 352.64, "end": 355.12, "text": " I'm sure there will be soon enough, but currently there isn't.", "tokens": [286, 478, 988, 456, 486, 312, 2321, 1547, 11, 457, 4362, 456, 1943, 380, 13], "temperature": 0.0, "avg_logprob": -0.1640026878764611, "compression_ratio": 1.6632996632996633, "no_speech_prob": 4.006176823168062e-05}, {"id": 74, "seek": 34708, "start": 355.12, "end": 357.0, "text": " So we're going to be building a lot of models from scratch.", "tokens": [407, 321, 434, 516, 281, 312, 2390, 257, 688, 295, 5245, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.1640026878764611, "compression_ratio": 1.6632996632996633, "no_speech_prob": 4.006176823168062e-05}, {"id": 75, "seek": 34708, "start": 357.0, "end": 362.15999999999997, "text": " It's been a while since we've done much, you know, serious model building.", "tokens": [467, 311, 668, 257, 1339, 1670, 321, 600, 1096, 709, 11, 291, 458, 11, 3156, 2316, 2390, 13], "temperature": 0.0, "avg_logprob": -0.1640026878764611, "compression_ratio": 1.6632996632996633, "no_speech_prob": 4.006176823168062e-05}, {"id": 76, "seek": 34708, "start": 362.15999999999997, "end": 367.59999999999997, "text": " A little bit of model building I guess for our bounding box stuff, but really all the", "tokens": [316, 707, 857, 295, 2316, 2390, 286, 2041, 337, 527, 5472, 278, 2424, 1507, 11, 457, 534, 439, 264], "temperature": 0.0, "avg_logprob": -0.1640026878764611, "compression_ratio": 1.6632996632996633, "no_speech_prob": 4.006176823168062e-05}, {"id": 77, "seek": 34708, "start": 367.59999999999997, "end": 370.28, "text": " interesting stuff there was the loss function.", "tokens": [1880, 1507, 456, 390, 264, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1640026878764611, "compression_ratio": 1.6632996632996633, "no_speech_prob": 4.006176823168062e-05}, {"id": 78, "seek": 34708, "start": 370.28, "end": 375.12, "text": " So we looked at Sci-Fi 10 in the part 1 of the course and we built something which was", "tokens": [407, 321, 2956, 412, 16942, 12, 13229, 1266, 294, 264, 644, 502, 295, 264, 1164, 293, 321, 3094, 746, 597, 390], "temperature": 0.0, "avg_logprob": -0.1640026878764611, "compression_ratio": 1.6632996632996633, "no_speech_prob": 4.006176823168062e-05}, {"id": 79, "seek": 37512, "start": 375.12, "end": 381.76, "text": " getting about 85% accuracy and I can't remember a couple of hours to train.", "tokens": [1242, 466, 14695, 4, 14170, 293, 286, 393, 380, 1604, 257, 1916, 295, 2496, 281, 3847, 13], "temperature": 0.0, "avg_logprob": -0.18892917818236118, "compression_ratio": 1.649402390438247, "no_speech_prob": 3.138090050924802e-06}, {"id": 80, "seek": 37512, "start": 381.76, "end": 385.96, "text": " Interestingly there's a competition going on now to see who can actually train Sci-Fi", "tokens": [30564, 456, 311, 257, 6211, 516, 322, 586, 281, 536, 567, 393, 767, 3847, 16942, 12, 13229], "temperature": 0.0, "avg_logprob": -0.18892917818236118, "compression_ratio": 1.649402390438247, "no_speech_prob": 3.138090050924802e-06}, {"id": 81, "seek": 37512, "start": 385.96, "end": 390.72, "text": " 10 the fastest going through this Stanford Dawn bench.", "tokens": [1266, 264, 14573, 516, 807, 341, 20374, 26001, 10638, 13], "temperature": 0.0, "avg_logprob": -0.18892917818236118, "compression_ratio": 1.649402390438247, "no_speech_prob": 3.138090050924802e-06}, {"id": 82, "seek": 37512, "start": 390.72, "end": 394.2, "text": " So the goal is to get it to train to 94% accuracy.", "tokens": [407, 264, 3387, 307, 281, 483, 309, 281, 3847, 281, 30849, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.18892917818236118, "compression_ratio": 1.649402390438247, "no_speech_prob": 3.138090050924802e-06}, {"id": 83, "seek": 37512, "start": 394.2, "end": 399.76, "text": " So it'll be interesting to see if we can build an architecture that can get to 94% accuracy", "tokens": [407, 309, 603, 312, 1880, 281, 536, 498, 321, 393, 1322, 364, 9482, 300, 393, 483, 281, 30849, 4, 14170], "temperature": 0.0, "avg_logprob": -0.18892917818236118, "compression_ratio": 1.649402390438247, "no_speech_prob": 3.138090050924802e-06}, {"id": 84, "seek": 37512, "start": 399.76, "end": 403.28000000000003, "text": " because that's a lot better than our previous attempt.", "tokens": [570, 300, 311, 257, 688, 1101, 813, 527, 3894, 5217, 13], "temperature": 0.0, "avg_logprob": -0.18892917818236118, "compression_ratio": 1.649402390438247, "no_speech_prob": 3.138090050924802e-06}, {"id": 85, "seek": 40328, "start": 403.28, "end": 407.76, "text": " And so hopefully in doing so we'll learn something about creating good architectures.", "tokens": [400, 370, 4696, 294, 884, 370, 321, 603, 1466, 746, 466, 4084, 665, 6331, 1303, 13], "temperature": 0.0, "avg_logprob": -0.12317638981099031, "compression_ratio": 1.6328125, "no_speech_prob": 2.2125052055343986e-05}, {"id": 86, "seek": 40328, "start": 407.76, "end": 412.79999999999995, "text": " That'll be then useful for looking at these GANs today.", "tokens": [663, 603, 312, 550, 4420, 337, 1237, 412, 613, 460, 1770, 82, 965, 13], "temperature": 0.0, "avg_logprob": -0.12317638981099031, "compression_ratio": 1.6328125, "no_speech_prob": 2.2125052055343986e-05}, {"id": 87, "seek": 40328, "start": 412.79999999999995, "end": 419.64, "text": " But I think also it's useful because I've been kind of looking much more deeply into", "tokens": [583, 286, 519, 611, 309, 311, 4420, 570, 286, 600, 668, 733, 295, 1237, 709, 544, 8760, 666], "temperature": 0.0, "avg_logprob": -0.12317638981099031, "compression_ratio": 1.6328125, "no_speech_prob": 2.2125052055343986e-05}, {"id": 88, "seek": 40328, "start": 419.64, "end": 425.03999999999996, "text": " the last few years' papers about different kinds of CNN architectures and realized that", "tokens": [264, 1036, 1326, 924, 6, 10577, 466, 819, 3685, 295, 24859, 6331, 1303, 293, 5334, 300], "temperature": 0.0, "avg_logprob": -0.12317638981099031, "compression_ratio": 1.6328125, "no_speech_prob": 2.2125052055343986e-05}, {"id": 89, "seek": 40328, "start": 425.03999999999996, "end": 429.76, "text": " a lot of the insights in those papers are not being widely leveraged and clearly not", "tokens": [257, 688, 295, 264, 14310, 294, 729, 10577, 366, 406, 885, 13371, 12451, 2980, 293, 4448, 406], "temperature": 0.0, "avg_logprob": -0.12317638981099031, "compression_ratio": 1.6328125, "no_speech_prob": 2.2125052055343986e-05}, {"id": 90, "seek": 40328, "start": 429.76, "end": 431.59999999999997, "text": " widely understood.", "tokens": [13371, 7320, 13], "temperature": 0.0, "avg_logprob": -0.12317638981099031, "compression_ratio": 1.6328125, "no_speech_prob": 2.2125052055343986e-05}, {"id": 91, "seek": 43160, "start": 431.6, "end": 437.24, "text": " So I want to show you what happens if we can leverage some of that understanding.", "tokens": [407, 286, 528, 281, 855, 291, 437, 2314, 498, 321, 393, 13982, 512, 295, 300, 3701, 13], "temperature": 0.0, "avg_logprob": -0.14760282121855636, "compression_ratio": 1.6167883211678833, "no_speech_prob": 8.139458259392995e-06}, {"id": 92, "seek": 43160, "start": 437.24, "end": 443.24, "text": " So I've got this notebook called Sci-Fi 10 Darknet.", "tokens": [407, 286, 600, 658, 341, 21060, 1219, 16942, 12, 13229, 1266, 9563, 7129, 13], "temperature": 0.0, "avg_logprob": -0.14760282121855636, "compression_ratio": 1.6167883211678833, "no_speech_prob": 8.139458259392995e-06}, {"id": 93, "seek": 43160, "start": 443.24, "end": 447.6, "text": " That's because the particular architecture we're going to look at is really very close", "tokens": [663, 311, 570, 264, 1729, 9482, 321, 434, 516, 281, 574, 412, 307, 534, 588, 1998], "temperature": 0.0, "avg_logprob": -0.14760282121855636, "compression_ratio": 1.6167883211678833, "no_speech_prob": 8.139458259392995e-06}, {"id": 94, "seek": 43160, "start": 447.6, "end": 449.08000000000004, "text": " to the Darknet architecture.", "tokens": [281, 264, 9563, 7129, 9482, 13], "temperature": 0.0, "avg_logprob": -0.14760282121855636, "compression_ratio": 1.6167883211678833, "no_speech_prob": 8.139458259392995e-06}, {"id": 95, "seek": 43160, "start": 449.08000000000004, "end": 454.72, "text": " But you'll see in the process that the Darknet architecture, as in not the whole YOLO version", "tokens": [583, 291, 603, 536, 294, 264, 1399, 300, 264, 9563, 7129, 9482, 11, 382, 294, 406, 264, 1379, 398, 5046, 46, 3037], "temperature": 0.0, "avg_logprob": -0.14760282121855636, "compression_ratio": 1.6167883211678833, "no_speech_prob": 8.139458259392995e-06}, {"id": 96, "seek": 43160, "start": 454.72, "end": 461.52000000000004, "text": " 3 end-to-end thing, but just the part of it that they pre-trained on ImageNet to do classification,", "tokens": [805, 917, 12, 1353, 12, 521, 551, 11, 457, 445, 264, 644, 295, 309, 300, 436, 659, 12, 17227, 2001, 322, 29903, 31890, 281, 360, 21538, 11], "temperature": 0.0, "avg_logprob": -0.14760282121855636, "compression_ratio": 1.6167883211678833, "no_speech_prob": 8.139458259392995e-06}, {"id": 97, "seek": 46152, "start": 461.52, "end": 468.2, "text": " it's almost like the most generic, simple architecture you could come up with.", "tokens": [309, 311, 1920, 411, 264, 881, 19577, 11, 2199, 9482, 291, 727, 808, 493, 365, 13], "temperature": 0.0, "avg_logprob": -0.12995760781424387, "compression_ratio": 1.5589519650655022, "no_speech_prob": 1.593621891515795e-05}, {"id": 98, "seek": 46152, "start": 468.2, "end": 472.4, "text": " And so it's a really great starting point for experiments.", "tokens": [400, 370, 309, 311, 257, 534, 869, 2891, 935, 337, 12050, 13], "temperature": 0.0, "avg_logprob": -0.12995760781424387, "compression_ratio": 1.5589519650655022, "no_speech_prob": 1.593621891515795e-05}, {"id": 99, "seek": 46152, "start": 472.4, "end": 476.28, "text": " So we're going to call it Darknet, but it's not quite Darknet and you can fiddle around", "tokens": [407, 321, 434, 516, 281, 818, 309, 9563, 7129, 11, 457, 309, 311, 406, 1596, 9563, 7129, 293, 291, 393, 24553, 2285, 926], "temperature": 0.0, "avg_logprob": -0.12995760781424387, "compression_ratio": 1.5589519650655022, "no_speech_prob": 1.593621891515795e-05}, {"id": 100, "seek": 46152, "start": 476.28, "end": 478.88, "text": " with it to create things that definitely aren't Darknet.", "tokens": [365, 309, 281, 1884, 721, 300, 2138, 3212, 380, 9563, 7129, 13], "temperature": 0.0, "avg_logprob": -0.12995760781424387, "compression_ratio": 1.5589519650655022, "no_speech_prob": 1.593621891515795e-05}, {"id": 101, "seek": 46152, "start": 478.88, "end": 486.52, "text": " It's really just the basis of nearly any modern ResNet-based architecture.", "tokens": [467, 311, 534, 445, 264, 5143, 295, 6217, 604, 4363, 5015, 31890, 12, 6032, 9482, 13], "temperature": 0.0, "avg_logprob": -0.12995760781424387, "compression_ratio": 1.5589519650655022, "no_speech_prob": 1.593621891515795e-05}, {"id": 102, "seek": 48652, "start": 486.52, "end": 494.44, "text": " So Sci-Fi 10, remember, is a fairly small dataset, the images are only 32x32 in size.", "tokens": [407, 16942, 12, 13229, 1266, 11, 1604, 11, 307, 257, 6457, 1359, 28872, 11, 264, 5267, 366, 787, 8858, 87, 11440, 294, 2744, 13], "temperature": 0.0, "avg_logprob": -0.14124920633104113, "compression_ratio": 1.6557377049180328, "no_speech_prob": 1.0952900993288495e-05}, {"id": 103, "seek": 48652, "start": 494.44, "end": 501.4, "text": " And I think it's a really great dataset to work with because you can train it relatively", "tokens": [400, 286, 519, 309, 311, 257, 534, 869, 28872, 281, 589, 365, 570, 291, 393, 3847, 309, 7226], "temperature": 0.0, "avg_logprob": -0.14124920633104113, "compression_ratio": 1.6557377049180328, "no_speech_prob": 1.0952900993288495e-05}, {"id": 104, "seek": 48652, "start": 501.4, "end": 503.32, "text": " quickly unlike ImageNet.", "tokens": [2661, 8343, 29903, 31890, 13], "temperature": 0.0, "avg_logprob": -0.14124920633104113, "compression_ratio": 1.6557377049180328, "no_speech_prob": 1.0952900993288495e-05}, {"id": 105, "seek": 48652, "start": 503.32, "end": 506.76, "text": " It's a relatively small amount of data unlike ImageNet.", "tokens": [467, 311, 257, 7226, 1359, 2372, 295, 1412, 8343, 29903, 31890, 13], "temperature": 0.0, "avg_logprob": -0.14124920633104113, "compression_ratio": 1.6557377049180328, "no_speech_prob": 1.0952900993288495e-05}, {"id": 106, "seek": 48652, "start": 506.76, "end": 511.4, "text": " And it's actually quite hard to recognize the images because 32x32 is kind of too small", "tokens": [400, 309, 311, 767, 1596, 1152, 281, 5521, 264, 5267, 570, 8858, 87, 11440, 307, 733, 295, 886, 1359], "temperature": 0.0, "avg_logprob": -0.14124920633104113, "compression_ratio": 1.6557377049180328, "no_speech_prob": 1.0952900993288495e-05}, {"id": 107, "seek": 48652, "start": 511.4, "end": 512.88, "text": " to easily see what's going on.", "tokens": [281, 3612, 536, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.14124920633104113, "compression_ratio": 1.6557377049180328, "no_speech_prob": 1.0952900993288495e-05}, {"id": 108, "seek": 48652, "start": 512.88, "end": 515.48, "text": " So it's somewhat challenging.", "tokens": [407, 309, 311, 8344, 7595, 13], "temperature": 0.0, "avg_logprob": -0.14124920633104113, "compression_ratio": 1.6557377049180328, "no_speech_prob": 1.0952900993288495e-05}, {"id": 109, "seek": 51548, "start": 515.48, "end": 520.36, "text": " So I think it's a really underappreciated dataset because it's old.", "tokens": [407, 286, 519, 309, 311, 257, 534, 833, 1746, 3326, 770, 28872, 570, 309, 311, 1331, 13], "temperature": 0.0, "avg_logprob": -0.1800205296483533, "compression_ratio": 1.514018691588785, "no_speech_prob": 1.3845638932252768e-05}, {"id": 110, "seek": 51548, "start": 520.36, "end": 526.84, "text": " And who at DeepMind or OpenAI wants to work with a small old dataset when they could use", "tokens": [400, 567, 412, 14895, 44, 471, 420, 7238, 48698, 2738, 281, 589, 365, 257, 1359, 1331, 28872, 562, 436, 727, 764], "temperature": 0.0, "avg_logprob": -0.1800205296483533, "compression_ratio": 1.514018691588785, "no_speech_prob": 1.3845638932252768e-05}, {"id": 111, "seek": 51548, "start": 526.84, "end": 530.48, "text": " their entire server room to process something much bigger.", "tokens": [641, 2302, 7154, 1808, 281, 1399, 746, 709, 3801, 13], "temperature": 0.0, "avg_logprob": -0.1800205296483533, "compression_ratio": 1.514018691588785, "no_speech_prob": 1.3845638932252768e-05}, {"id": 112, "seek": 51548, "start": 530.48, "end": 536.6800000000001, "text": " But to me, I think this is a really great dataset to focus on.", "tokens": [583, 281, 385, 11, 286, 519, 341, 307, 257, 534, 869, 28872, 281, 1879, 322, 13], "temperature": 0.0, "avg_logprob": -0.1800205296483533, "compression_ratio": 1.514018691588785, "no_speech_prob": 1.3845638932252768e-05}, {"id": 113, "seek": 51548, "start": 536.6800000000001, "end": 541.6800000000001, "text": " So we'll go ahead and import our usual stuff.", "tokens": [407, 321, 603, 352, 2286, 293, 974, 527, 7713, 1507, 13], "temperature": 0.0, "avg_logprob": -0.1800205296483533, "compression_ratio": 1.514018691588785, "no_speech_prob": 1.3845638932252768e-05}, {"id": 114, "seek": 54168, "start": 541.68, "end": 548.8, "text": " And we're going to try and build a network from scratch to train this with.", "tokens": [400, 321, 434, 516, 281, 853, 293, 1322, 257, 3209, 490, 8459, 281, 3847, 341, 365, 13], "temperature": 0.0, "avg_logprob": -0.11186083968804807, "compression_ratio": 1.5748987854251013, "no_speech_prob": 1.0952942830044776e-05}, {"id": 115, "seek": 54168, "start": 548.8, "end": 554.16, "text": " One thing that I think is a really good exercise for anybody who's not 100% confident with", "tokens": [1485, 551, 300, 286, 519, 307, 257, 534, 665, 5380, 337, 4472, 567, 311, 406, 2319, 4, 6679, 365], "temperature": 0.0, "avg_logprob": -0.11186083968804807, "compression_ratio": 1.5748987854251013, "no_speech_prob": 1.0952942830044776e-05}, {"id": 116, "seek": 54168, "start": 554.16, "end": 561.42, "text": " their kind of broadcasting and PyTorch and so forth basic skills is figure out how I", "tokens": [641, 733, 295, 30024, 293, 9953, 51, 284, 339, 293, 370, 5220, 3875, 3942, 307, 2573, 484, 577, 286], "temperature": 0.0, "avg_logprob": -0.11186083968804807, "compression_ratio": 1.5748987854251013, "no_speech_prob": 1.0952942830044776e-05}, {"id": 117, "seek": 54168, "start": 561.42, "end": 564.28, "text": " came up with these numbers.", "tokens": [1361, 493, 365, 613, 3547, 13], "temperature": 0.0, "avg_logprob": -0.11186083968804807, "compression_ratio": 1.5748987854251013, "no_speech_prob": 1.0952942830044776e-05}, {"id": 118, "seek": 54168, "start": 564.28, "end": 568.92, "text": " So these numbers are the averages for each channel and the standard deviations for each", "tokens": [407, 613, 3547, 366, 264, 42257, 337, 1184, 2269, 293, 264, 3832, 31219, 763, 337, 1184], "temperature": 0.0, "avg_logprob": -0.11186083968804807, "compression_ratio": 1.5748987854251013, "no_speech_prob": 1.0952942830044776e-05}, {"id": 119, "seek": 54168, "start": 568.92, "end": 570.7199999999999, "text": " channel in Sci-Fi 10.", "tokens": [2269, 294, 16942, 12, 13229, 1266, 13], "temperature": 0.0, "avg_logprob": -0.11186083968804807, "compression_ratio": 1.5748987854251013, "no_speech_prob": 1.0952942830044776e-05}, {"id": 120, "seek": 57072, "start": 570.72, "end": 575.44, "text": " So try and, that's a bit of homework, just make sure you can recreate those numbers and", "tokens": [407, 853, 293, 11, 300, 311, 257, 857, 295, 14578, 11, 445, 652, 988, 291, 393, 25833, 729, 3547, 293], "temperature": 0.0, "avg_logprob": -0.22650735096264912, "compression_ratio": 1.5523809523809524, "no_speech_prob": 1.670122037467081e-05}, {"id": 121, "seek": 57072, "start": 575.44, "end": 582.12, "text": " see if you can do it in no more than a couple of lines of code, no loops.", "tokens": [536, 498, 291, 393, 360, 309, 294, 572, 544, 813, 257, 1916, 295, 3876, 295, 3089, 11, 572, 16121, 13], "temperature": 0.0, "avg_logprob": -0.22650735096264912, "compression_ratio": 1.5523809523809524, "no_speech_prob": 1.670122037467081e-05}, {"id": 122, "seek": 57072, "start": 582.12, "end": 589.2, "text": " Ideally, you want to do it in one go if you can.", "tokens": [40817, 11, 291, 528, 281, 360, 309, 294, 472, 352, 498, 291, 393, 13], "temperature": 0.0, "avg_logprob": -0.22650735096264912, "compression_ratio": 1.5523809523809524, "no_speech_prob": 1.670122037467081e-05}, {"id": 123, "seek": 57072, "start": 589.2, "end": 594.1600000000001, "text": " Because these are fairly small, we can use a larger batch size than usual, 256, and the", "tokens": [1436, 613, 366, 6457, 1359, 11, 321, 393, 764, 257, 4833, 15245, 2744, 813, 7713, 11, 38882, 11, 293, 264], "temperature": 0.0, "avg_logprob": -0.22650735096264912, "compression_ratio": 1.5523809523809524, "no_speech_prob": 1.670122037467081e-05}, {"id": 124, "seek": 57072, "start": 594.1600000000001, "end": 599.0, "text": " size of these images is 32.", "tokens": [2744, 295, 613, 5267, 307, 8858, 13], "temperature": 0.0, "avg_logprob": -0.22650735096264912, "compression_ratio": 1.5523809523809524, "no_speech_prob": 1.670122037467081e-05}, {"id": 125, "seek": 59900, "start": 599.0, "end": 606.56, "text": " Normally, we have this standard set of side-on transformations we use for photos of normal", "tokens": [17424, 11, 321, 362, 341, 3832, 992, 295, 1252, 12, 266, 34852, 321, 764, 337, 5787, 295, 2710], "temperature": 0.0, "avg_logprob": -0.16948274612426759, "compression_ratio": 1.6541666666666666, "no_speech_prob": 2.9022921808063984e-06}, {"id": 126, "seek": 59900, "start": 606.56, "end": 607.56, "text": " objects.", "tokens": [6565, 13], "temperature": 0.0, "avg_logprob": -0.16948274612426759, "compression_ratio": 1.6541666666666666, "no_speech_prob": 2.9022921808063984e-06}, {"id": 127, "seek": 59900, "start": 607.56, "end": 611.56, "text": " We're not going to use that here though because these images are so small that trying to rotate", "tokens": [492, 434, 406, 516, 281, 764, 300, 510, 1673, 570, 613, 5267, 366, 370, 1359, 300, 1382, 281, 13121], "temperature": 0.0, "avg_logprob": -0.16948274612426759, "compression_ratio": 1.6541666666666666, "no_speech_prob": 2.9022921808063984e-06}, {"id": 128, "seek": 59900, "start": 611.56, "end": 619.04, "text": " a 32x32 image a bit is going to introduce a lot of blocky kind of distortions.", "tokens": [257, 8858, 87, 11440, 3256, 257, 857, 307, 516, 281, 5366, 257, 688, 295, 3461, 88, 733, 295, 37555, 626, 13], "temperature": 0.0, "avg_logprob": -0.16948274612426759, "compression_ratio": 1.6541666666666666, "no_speech_prob": 2.9022921808063984e-06}, {"id": 129, "seek": 59900, "start": 619.04, "end": 625.12, "text": " So the kind of standard transforms that people tend to use is a random horizontal flip, and", "tokens": [407, 264, 733, 295, 3832, 35592, 300, 561, 3928, 281, 764, 307, 257, 4974, 12750, 7929, 11, 293], "temperature": 0.0, "avg_logprob": -0.16948274612426759, "compression_ratio": 1.6541666666666666, "no_speech_prob": 2.9022921808063984e-06}, {"id": 130, "seek": 59900, "start": 625.12, "end": 628.64, "text": " then we add size divided by 8.", "tokens": [550, 321, 909, 2744, 6666, 538, 1649, 13], "temperature": 0.0, "avg_logprob": -0.16948274612426759, "compression_ratio": 1.6541666666666666, "no_speech_prob": 2.9022921808063984e-06}, {"id": 131, "seek": 62864, "start": 628.64, "end": 632.16, "text": " So 4 pixels of padding on each side.", "tokens": [407, 1017, 18668, 295, 39562, 322, 1184, 1252, 13], "temperature": 0.0, "avg_logprob": -0.10076893601462106, "compression_ratio": 1.6653846153846155, "no_speech_prob": 7.411227215925464e-06}, {"id": 132, "seek": 62864, "start": 632.16, "end": 637.08, "text": " And one thing which I find works really well is by default, Fast.ai doesn't add black padding,", "tokens": [400, 472, 551, 597, 286, 915, 1985, 534, 731, 307, 538, 7576, 11, 15968, 13, 1301, 1177, 380, 909, 2211, 39562, 11], "temperature": 0.0, "avg_logprob": -0.10076893601462106, "compression_ratio": 1.6653846153846155, "no_speech_prob": 7.411227215925464e-06}, {"id": 133, "seek": 62864, "start": 637.08, "end": 639.12, "text": " which basically every other library does.", "tokens": [597, 1936, 633, 661, 6405, 775, 13], "temperature": 0.0, "avg_logprob": -0.10076893601462106, "compression_ratio": 1.6653846153846155, "no_speech_prob": 7.411227215925464e-06}, {"id": 134, "seek": 62864, "start": 639.12, "end": 644.72, "text": " We actually take the last 4 pixels of the existing photo and flip it and reflect it.", "tokens": [492, 767, 747, 264, 1036, 1017, 18668, 295, 264, 6741, 5052, 293, 7929, 309, 293, 5031, 309, 13], "temperature": 0.0, "avg_logprob": -0.10076893601462106, "compression_ratio": 1.6653846153846155, "no_speech_prob": 7.411227215925464e-06}, {"id": 135, "seek": 62864, "start": 644.72, "end": 651.4399999999999, "text": " And we find that we get much better results by using this reflection padding by default.", "tokens": [400, 321, 915, 300, 321, 483, 709, 1101, 3542, 538, 1228, 341, 12914, 39562, 538, 7576, 13], "temperature": 0.0, "avg_logprob": -0.10076893601462106, "compression_ratio": 1.6653846153846155, "no_speech_prob": 7.411227215925464e-06}, {"id": 136, "seek": 62864, "start": 651.4399999999999, "end": 658.2, "text": " So now that we've got a 36x36 image, this set of transforms in training will randomly", "tokens": [407, 586, 300, 321, 600, 658, 257, 8652, 87, 11309, 3256, 11, 341, 992, 295, 35592, 294, 3097, 486, 16979], "temperature": 0.0, "avg_logprob": -0.10076893601462106, "compression_ratio": 1.6653846153846155, "no_speech_prob": 7.411227215925464e-06}, {"id": 137, "seek": 65820, "start": 658.2, "end": 660.2800000000001, "text": " pick a 32x32 crop.", "tokens": [1888, 257, 8858, 87, 11440, 9086, 13], "temperature": 0.0, "avg_logprob": -0.1753525279817127, "compression_ratio": 1.5988372093023255, "no_speech_prob": 1.0783139259729069e-05}, {"id": 138, "seek": 65820, "start": 660.2800000000001, "end": 664.36, "text": " So we get a little bit of variation, but not heaps.", "tokens": [407, 321, 483, 257, 707, 857, 295, 12990, 11, 457, 406, 415, 2382, 13], "temperature": 0.0, "avg_logprob": -0.1753525279817127, "compression_ratio": 1.5988372093023255, "no_speech_prob": 1.0783139259729069e-05}, {"id": 139, "seek": 65820, "start": 664.36, "end": 667.44, "text": " So we can use our normal from paths to grab our data.", "tokens": [407, 321, 393, 764, 527, 2710, 490, 14518, 281, 4444, 527, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1753525279817127, "compression_ratio": 1.5988372093023255, "no_speech_prob": 1.0783139259729069e-05}, {"id": 140, "seek": 65820, "start": 667.44, "end": 670.44, "text": " So we now need an architecture.", "tokens": [407, 321, 586, 643, 364, 9482, 13], "temperature": 0.0, "avg_logprob": -0.1753525279817127, "compression_ratio": 1.5988372093023255, "no_speech_prob": 1.0783139259729069e-05}, {"id": 141, "seek": 65820, "start": 670.44, "end": 676.2, "text": " And what we're going to do is we're going to create an architecture which fits in one", "tokens": [400, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 1884, 364, 9482, 597, 9001, 294, 472], "temperature": 0.0, "avg_logprob": -0.1753525279817127, "compression_ratio": 1.5988372093023255, "no_speech_prob": 1.0783139259729069e-05}, {"id": 142, "seek": 65820, "start": 676.2, "end": 678.12, "text": " screen.", "tokens": [2568, 13], "temperature": 0.0, "avg_logprob": -0.1753525279817127, "compression_ratio": 1.5988372093023255, "no_speech_prob": 1.0783139259729069e-05}, {"id": 143, "seek": 65820, "start": 678.12, "end": 681.6400000000001, "text": " So this is from scratch.", "tokens": [407, 341, 307, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.1753525279817127, "compression_ratio": 1.5988372093023255, "no_speech_prob": 1.0783139259729069e-05}, {"id": 144, "seek": 68164, "start": 681.64, "end": 692.36, "text": " As you can see, I'm using the predefined comp2d, batchnorm2d, leakyvalue modules, but I'm not", "tokens": [1018, 291, 393, 536, 11, 286, 478, 1228, 264, 659, 37716, 715, 17, 67, 11, 15245, 13403, 17, 67, 11, 476, 15681, 29155, 16679, 11, 457, 286, 478, 406], "temperature": 0.0, "avg_logprob": -0.21049026820970618, "compression_ratio": 1.4832535885167464, "no_speech_prob": 6.747975476173451e-06}, {"id": 145, "seek": 68164, "start": 692.36, "end": 695.12, "text": " using any blocks or anything.", "tokens": [1228, 604, 8474, 420, 1340, 13], "temperature": 0.0, "avg_logprob": -0.21049026820970618, "compression_ratio": 1.4832535885167464, "no_speech_prob": 6.747975476173451e-06}, {"id": 146, "seek": 68164, "start": 695.12, "end": 696.12, "text": " They're all being defined.", "tokens": [814, 434, 439, 885, 7642, 13], "temperature": 0.0, "avg_logprob": -0.21049026820970618, "compression_ratio": 1.4832535885167464, "no_speech_prob": 6.747975476173451e-06}, {"id": 147, "seek": 68164, "start": 696.12, "end": 697.88, "text": " So the entire thing is here on one screen.", "tokens": [407, 264, 2302, 551, 307, 510, 322, 472, 2568, 13], "temperature": 0.0, "avg_logprob": -0.21049026820970618, "compression_ratio": 1.4832535885167464, "no_speech_prob": 6.747975476173451e-06}, {"id": 148, "seek": 68164, "start": 697.88, "end": 703.8, "text": " So if you're ever wondering, can I understand a modern, good quality architecture?", "tokens": [407, 498, 291, 434, 1562, 6359, 11, 393, 286, 1223, 257, 4363, 11, 665, 3125, 9482, 30], "temperature": 0.0, "avg_logprob": -0.21049026820970618, "compression_ratio": 1.4832535885167464, "no_speech_prob": 6.747975476173451e-06}, {"id": 149, "seek": 68164, "start": 703.8, "end": 704.8, "text": " Absolutely.", "tokens": [7021, 13], "temperature": 0.0, "avg_logprob": -0.21049026820970618, "compression_ratio": 1.4832535885167464, "no_speech_prob": 6.747975476173451e-06}, {"id": 150, "seek": 68164, "start": 704.8, "end": 708.52, "text": " Let's study this one.", "tokens": [961, 311, 2979, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.21049026820970618, "compression_ratio": 1.4832535885167464, "no_speech_prob": 6.747975476173451e-06}, {"id": 151, "seek": 70852, "start": 708.52, "end": 718.0, "text": " So my basic starting point with an architecture is to say, okay, it's a stacked bunch of layers.", "tokens": [407, 452, 3875, 2891, 935, 365, 364, 9482, 307, 281, 584, 11, 1392, 11, 309, 311, 257, 28867, 3840, 295, 7914, 13], "temperature": 0.0, "avg_logprob": -0.15244735000479934, "compression_ratio": 1.806949806949807, "no_speech_prob": 2.332064013899071e-06}, {"id": 152, "seek": 70852, "start": 718.0, "end": 720.72, "text": " And generally speaking, there's going to be some kind of hierarchy of layers.", "tokens": [400, 5101, 4124, 11, 456, 311, 516, 281, 312, 512, 733, 295, 22333, 295, 7914, 13], "temperature": 0.0, "avg_logprob": -0.15244735000479934, "compression_ratio": 1.806949806949807, "no_speech_prob": 2.332064013899071e-06}, {"id": 153, "seek": 70852, "start": 720.72, "end": 724.28, "text": " So at the very bottom level, there's things like a convolutional layer and a batchnorm", "tokens": [407, 412, 264, 588, 2767, 1496, 11, 456, 311, 721, 411, 257, 45216, 304, 4583, 293, 257, 15245, 13403], "temperature": 0.0, "avg_logprob": -0.15244735000479934, "compression_ratio": 1.806949806949807, "no_speech_prob": 2.332064013899071e-06}, {"id": 154, "seek": 70852, "start": 724.28, "end": 725.28, "text": " layer.", "tokens": [4583, 13], "temperature": 0.0, "avg_logprob": -0.15244735000479934, "compression_ratio": 1.806949806949807, "no_speech_prob": 2.332064013899071e-06}, {"id": 155, "seek": 70852, "start": 725.28, "end": 730.24, "text": " But generally speaking, any time you have a convolution, you're probably going to have", "tokens": [583, 5101, 4124, 11, 604, 565, 291, 362, 257, 45216, 11, 291, 434, 1391, 516, 281, 362], "temperature": 0.0, "avg_logprob": -0.15244735000479934, "compression_ratio": 1.806949806949807, "no_speech_prob": 2.332064013899071e-06}, {"id": 156, "seek": 70852, "start": 730.24, "end": 731.24, "text": " some standard sequence.", "tokens": [512, 3832, 8310, 13], "temperature": 0.0, "avg_logprob": -0.15244735000479934, "compression_ratio": 1.806949806949807, "no_speech_prob": 2.332064013899071e-06}, {"id": 157, "seek": 70852, "start": 731.24, "end": 737.76, "text": " And normally it's going to be conv, batchnorm, then a nonlinear activation like a value.", "tokens": [400, 5646, 309, 311, 516, 281, 312, 3754, 11, 15245, 13403, 11, 550, 257, 2107, 28263, 24433, 411, 257, 2158, 13], "temperature": 0.0, "avg_logprob": -0.15244735000479934, "compression_ratio": 1.806949806949807, "no_speech_prob": 2.332064013899071e-06}, {"id": 158, "seek": 73776, "start": 737.76, "end": 745.64, "text": " So I try to start right from the top by saying, okay, what are my basic units going to be?", "tokens": [407, 286, 853, 281, 722, 558, 490, 264, 1192, 538, 1566, 11, 1392, 11, 437, 366, 452, 3875, 6815, 516, 281, 312, 30], "temperature": 0.0, "avg_logprob": -0.14844391232445125, "compression_ratio": 1.6327433628318584, "no_speech_prob": 4.637847723643063e-06}, {"id": 159, "seek": 73776, "start": 745.64, "end": 754.56, "text": " And so by defining it here, that way I don't have to worry about trying to keep everything", "tokens": [400, 370, 538, 17827, 309, 510, 11, 300, 636, 286, 500, 380, 362, 281, 3292, 466, 1382, 281, 1066, 1203], "temperature": 0.0, "avg_logprob": -0.14844391232445125, "compression_ratio": 1.6327433628318584, "no_speech_prob": 4.637847723643063e-06}, {"id": 160, "seek": 73776, "start": 754.56, "end": 757.08, "text": " consistent and it's going to make everything a lot simpler.", "tokens": [8398, 293, 309, 311, 516, 281, 652, 1203, 257, 688, 18587, 13], "temperature": 0.0, "avg_logprob": -0.14844391232445125, "compression_ratio": 1.6327433628318584, "no_speech_prob": 4.637847723643063e-06}, {"id": 161, "seek": 73776, "start": 757.08, "end": 758.88, "text": " So here's my conv layer.", "tokens": [407, 510, 311, 452, 3754, 4583, 13], "temperature": 0.0, "avg_logprob": -0.14844391232445125, "compression_ratio": 1.6327433628318584, "no_speech_prob": 4.637847723643063e-06}, {"id": 162, "seek": 73776, "start": 758.88, "end": 763.92, "text": " So any time I say conv layer, I mean conv, batchnorm, leakyvalue.", "tokens": [407, 604, 565, 286, 584, 3754, 4583, 11, 286, 914, 3754, 11, 15245, 13403, 11, 476, 15681, 29155, 13], "temperature": 0.0, "avg_logprob": -0.14844391232445125, "compression_ratio": 1.6327433628318584, "no_speech_prob": 4.637847723643063e-06}, {"id": 163, "seek": 73776, "start": 763.92, "end": 766.72, "text": " Now I'm not quite saying leakyvalue.", "tokens": [823, 286, 478, 406, 1596, 1566, 476, 15681, 29155, 13], "temperature": 0.0, "avg_logprob": -0.14844391232445125, "compression_ratio": 1.6327433628318584, "no_speech_prob": 4.637847723643063e-06}, {"id": 164, "seek": 76672, "start": 766.72, "end": 769.88, "text": " I'm saying leakyvalue.", "tokens": [286, 478, 1566, 476, 15681, 29155, 13], "temperature": 0.0, "avg_logprob": -0.24641927083333334, "compression_ratio": 1.3059701492537314, "no_speech_prob": 6.439008757297415e-06}, {"id": 165, "seek": 76672, "start": 769.88, "end": 776.9200000000001, "text": " And that's, I think we briefly mentioned it before, but the basic idea is that normally", "tokens": [400, 300, 311, 11, 286, 519, 321, 10515, 2835, 309, 949, 11, 457, 264, 3875, 1558, 307, 300, 5646], "temperature": 0.0, "avg_logprob": -0.24641927083333334, "compression_ratio": 1.3059701492537314, "no_speech_prob": 6.439008757297415e-06}, {"id": 166, "seek": 76672, "start": 776.9200000000001, "end": 786.8000000000001, "text": " a value looks like that, right?", "tokens": [257, 2158, 1542, 411, 300, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.24641927083333334, "compression_ratio": 1.3059701492537314, "no_speech_prob": 6.439008757297415e-06}, {"id": 167, "seek": 76672, "start": 786.8000000000001, "end": 790.76, "text": " Hopefully you all know that now.", "tokens": [10429, 291, 439, 458, 300, 586, 13], "temperature": 0.0, "avg_logprob": -0.24641927083333334, "compression_ratio": 1.3059701492537314, "no_speech_prob": 6.439008757297415e-06}, {"id": 168, "seek": 79076, "start": 790.76, "end": 798.76, "text": " A leakyvalue looks like that.", "tokens": [316, 476, 15681, 29155, 1542, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.1331415918138292, "compression_ratio": 1.5454545454545454, "no_speech_prob": 9.276345167563704e-07}, {"id": 169, "seek": 79076, "start": 798.76, "end": 803.4399999999999, "text": " So this part, as before, has a gradient of 1 and this part has a gradient of, it can", "tokens": [407, 341, 644, 11, 382, 949, 11, 575, 257, 16235, 295, 502, 293, 341, 644, 575, 257, 16235, 295, 11, 309, 393], "temperature": 0.0, "avg_logprob": -0.1331415918138292, "compression_ratio": 1.5454545454545454, "no_speech_prob": 9.276345167563704e-07}, {"id": 170, "seek": 79076, "start": 803.4399999999999, "end": 808.68, "text": " vary, but something around.1 or.01 is common.", "tokens": [10559, 11, 457, 746, 926, 2411, 16, 420, 2411, 10607, 307, 2689, 13], "temperature": 0.0, "avg_logprob": -0.1331415918138292, "compression_ratio": 1.5454545454545454, "no_speech_prob": 9.276345167563704e-07}, {"id": 171, "seek": 79076, "start": 808.68, "end": 814.4, "text": " And the idea behind it is that when you're in this negative zone here, you don't end", "tokens": [400, 264, 1558, 2261, 309, 307, 300, 562, 291, 434, 294, 341, 3671, 6668, 510, 11, 291, 500, 380, 917], "temperature": 0.0, "avg_logprob": -0.1331415918138292, "compression_ratio": 1.5454545454545454, "no_speech_prob": 9.276345167563704e-07}, {"id": 172, "seek": 79076, "start": 814.4, "end": 819.08, "text": " up with a 0 gradient, which makes it very hard to update it.", "tokens": [493, 365, 257, 1958, 16235, 11, 597, 1669, 309, 588, 1152, 281, 5623, 309, 13], "temperature": 0.0, "avg_logprob": -0.1331415918138292, "compression_ratio": 1.5454545454545454, "no_speech_prob": 9.276345167563704e-07}, {"id": 173, "seek": 81908, "start": 819.08, "end": 825.5200000000001, "text": " In practice, people have found leakyvalue more useful on smaller datasets and less useful", "tokens": [682, 3124, 11, 561, 362, 1352, 476, 15681, 29155, 544, 4420, 322, 4356, 42856, 293, 1570, 4420], "temperature": 0.0, "avg_logprob": -0.13546495891752697, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.7061773860405083e-06}, {"id": 174, "seek": 81908, "start": 825.5200000000001, "end": 829.76, "text": " on big datasets, but it's interesting that for the YOLO version 3 paper, they did use", "tokens": [322, 955, 42856, 11, 457, 309, 311, 1880, 300, 337, 264, 398, 5046, 46, 3037, 805, 3035, 11, 436, 630, 764], "temperature": 0.0, "avg_logprob": -0.13546495891752697, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.7061773860405083e-06}, {"id": 175, "seek": 81908, "start": 829.76, "end": 833.12, "text": " a leakyvalue and got great performance from it.", "tokens": [257, 476, 15681, 29155, 293, 658, 869, 3389, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.13546495891752697, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.7061773860405083e-06}, {"id": 176, "seek": 81908, "start": 833.12, "end": 837.36, "text": " So it rarely makes things worse and it often makes things better.", "tokens": [407, 309, 13752, 1669, 721, 5324, 293, 309, 2049, 1669, 721, 1101, 13], "temperature": 0.0, "avg_logprob": -0.13546495891752697, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.7061773860405083e-06}, {"id": 177, "seek": 81908, "start": 837.36, "end": 842.44, "text": " So it's probably not bad if you need to create your own architecture to make that your default", "tokens": [407, 309, 311, 1391, 406, 1578, 498, 291, 643, 281, 1884, 428, 1065, 9482, 281, 652, 300, 428, 7576], "temperature": 0.0, "avg_logprob": -0.13546495891752697, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.7061773860405083e-06}, {"id": 178, "seek": 81908, "start": 842.44, "end": 846.32, "text": " go-to is to use leakyvalue.", "tokens": [352, 12, 1353, 307, 281, 764, 476, 15681, 29155, 13], "temperature": 0.0, "avg_logprob": -0.13546495891752697, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.7061773860405083e-06}, {"id": 179, "seek": 84632, "start": 846.32, "end": 853.6800000000001, "text": " You'll notice I don't define a PyTorch module here, I just go ahead and go sequential.", "tokens": [509, 603, 3449, 286, 500, 380, 6964, 257, 9953, 51, 284, 339, 10088, 510, 11, 286, 445, 352, 2286, 293, 352, 42881, 13], "temperature": 0.0, "avg_logprob": -0.0994533278725364, "compression_ratio": 1.6970954356846473, "no_speech_prob": 2.1444753656396642e-05}, {"id": 180, "seek": 84632, "start": 853.6800000000001, "end": 859.5200000000001, "text": " This is something that if you read other people's PyTorch code, it's really underutilized.", "tokens": [639, 307, 746, 300, 498, 291, 1401, 661, 561, 311, 9953, 51, 284, 339, 3089, 11, 309, 311, 534, 833, 20835, 1602, 13], "temperature": 0.0, "avg_logprob": -0.0994533278725364, "compression_ratio": 1.6970954356846473, "no_speech_prob": 2.1444753656396642e-05}, {"id": 181, "seek": 84632, "start": 859.5200000000001, "end": 864.08, "text": " People tend to write everything as a PyTorch module with an init and a forward.", "tokens": [3432, 3928, 281, 2464, 1203, 382, 257, 9953, 51, 284, 339, 10088, 365, 364, 3157, 293, 257, 2128, 13], "temperature": 0.0, "avg_logprob": -0.0994533278725364, "compression_ratio": 1.6970954356846473, "no_speech_prob": 2.1444753656396642e-05}, {"id": 182, "seek": 84632, "start": 864.08, "end": 869.2800000000001, "text": " But if the thing you want is just a sequence of things one after the other, it's much more", "tokens": [583, 498, 264, 551, 291, 528, 307, 445, 257, 8310, 295, 721, 472, 934, 264, 661, 11, 309, 311, 709, 544], "temperature": 0.0, "avg_logprob": -0.0994533278725364, "compression_ratio": 1.6970954356846473, "no_speech_prob": 2.1444753656396642e-05}, {"id": 183, "seek": 84632, "start": 869.2800000000001, "end": 872.6800000000001, "text": " concise and easy to understand to just make it a sequential.", "tokens": [44882, 293, 1858, 281, 1223, 281, 445, 652, 309, 257, 42881, 13], "temperature": 0.0, "avg_logprob": -0.0994533278725364, "compression_ratio": 1.6970954356846473, "no_speech_prob": 2.1444753656396642e-05}, {"id": 184, "seek": 87268, "start": 872.68, "end": 880.68, "text": " So I've just got a simple plain function that just returns a sequential model.", "tokens": [407, 286, 600, 445, 658, 257, 2199, 11121, 2445, 300, 445, 11247, 257, 42881, 2316, 13], "temperature": 0.0, "avg_logprob": -0.18222229182720184, "compression_ratio": 1.502857142857143, "no_speech_prob": 1.750278170220554e-05}, {"id": 185, "seek": 87268, "start": 880.68, "end": 890.0, "text": " So I mentioned that there's generally a number of hierarchies of units in most modern networks.", "tokens": [407, 286, 2835, 300, 456, 311, 5101, 257, 1230, 295, 35250, 530, 295, 6815, 294, 881, 4363, 9590, 13], "temperature": 0.0, "avg_logprob": -0.18222229182720184, "compression_ratio": 1.502857142857143, "no_speech_prob": 1.750278170220554e-05}, {"id": 186, "seek": 87268, "start": 890.0, "end": 898.4399999999999, "text": " And I think we know now that the next level in this unit hierarchy for resnets, and this", "tokens": [400, 286, 519, 321, 458, 586, 300, 264, 958, 1496, 294, 341, 4985, 22333, 337, 725, 77, 1385, 11, 293, 341], "temperature": 0.0, "avg_logprob": -0.18222229182720184, "compression_ratio": 1.502857142857143, "no_speech_prob": 1.750278170220554e-05}, {"id": 187, "seek": 89844, "start": 898.44, "end": 903.6800000000001, "text": " is a type of resnet, is the res block or the residual block.", "tokens": [307, 257, 2010, 295, 725, 7129, 11, 307, 264, 725, 3461, 420, 264, 27980, 3461, 13], "temperature": 0.0, "avg_logprob": -0.16804962158203124, "compression_ratio": 1.4550561797752808, "no_speech_prob": 1.0129942893399857e-05}, {"id": 188, "seek": 89844, "start": 903.6800000000001, "end": 910.5600000000001, "text": " I call it here a res layer.", "tokens": [286, 818, 309, 510, 257, 725, 4583, 13], "temperature": 0.0, "avg_logprob": -0.16804962158203124, "compression_ratio": 1.4550561797752808, "no_speech_prob": 1.0129942893399857e-05}, {"id": 189, "seek": 89844, "start": 910.5600000000001, "end": 918.6, "text": " And back when we last did CyPhar 10, I oversimplified this, I cheated a little bit.", "tokens": [400, 646, 562, 321, 1036, 630, 10295, 47, 5854, 1266, 11, 286, 15488, 332, 564, 2587, 341, 11, 286, 28079, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.16804962158203124, "compression_ratio": 1.4550561797752808, "no_speech_prob": 1.0129942893399857e-05}, {"id": 190, "seek": 89844, "start": 918.6, "end": 927.5600000000001, "text": " We had x coming in and we put that through a conv and then we added it back up to x to", "tokens": [492, 632, 2031, 1348, 294, 293, 321, 829, 300, 807, 257, 3754, 293, 550, 321, 3869, 309, 646, 493, 281, 2031, 281], "temperature": 0.0, "avg_logprob": -0.16804962158203124, "compression_ratio": 1.4550561797752808, "no_speech_prob": 1.0129942893399857e-05}, {"id": 191, "seek": 92756, "start": 927.56, "end": 930.8, "text": " go out.", "tokens": [352, 484, 13], "temperature": 0.0, "avg_logprob": -0.2182826801222198, "compression_ratio": 1.2983870967741935, "no_speech_prob": 3.1381409826281015e-06}, {"id": 192, "seek": 92756, "start": 930.8, "end": 945.16, "text": " So in general, we've got your output is equal to your input plus some function of your input.", "tokens": [407, 294, 2674, 11, 321, 600, 658, 428, 5598, 307, 2681, 281, 428, 4846, 1804, 512, 2445, 295, 428, 4846, 13], "temperature": 0.0, "avg_logprob": -0.2182826801222198, "compression_ratio": 1.2983870967741935, "no_speech_prob": 3.1381409826281015e-06}, {"id": 193, "seek": 92756, "start": 945.16, "end": 952.28, "text": " And the thing we did last year was we made f was a 2D conv.", "tokens": [400, 264, 551, 321, 630, 1036, 1064, 390, 321, 1027, 283, 390, 257, 568, 35, 3754, 13], "temperature": 0.0, "avg_logprob": -0.2182826801222198, "compression_ratio": 1.2983870967741935, "no_speech_prob": 3.1381409826281015e-06}, {"id": 194, "seek": 95228, "start": 952.28, "end": 965.0799999999999, "text": " And actually in the real res block, there's actually two of them.", "tokens": [400, 767, 294, 264, 957, 725, 3461, 11, 456, 311, 767, 732, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.21595539766199448, "compression_ratio": 1.401639344262295, "no_speech_prob": 4.936977802572073e-06}, {"id": 195, "seek": 95228, "start": 965.0799999999999, "end": 974.52, "text": " So it's actually conv of conv of x.", "tokens": [407, 309, 311, 767, 3754, 295, 3754, 295, 2031, 13], "temperature": 0.0, "avg_logprob": -0.21595539766199448, "compression_ratio": 1.401639344262295, "no_speech_prob": 4.936977802572073e-06}, {"id": 196, "seek": 95228, "start": 974.52, "end": 978.04, "text": " And when I say conv, I'm using this as a shortcut for our conv layer.", "tokens": [400, 562, 286, 584, 3754, 11, 286, 478, 1228, 341, 382, 257, 24822, 337, 527, 3754, 4583, 13], "temperature": 0.0, "avg_logprob": -0.21595539766199448, "compression_ratio": 1.401639344262295, "no_speech_prob": 4.936977802572073e-06}, {"id": 197, "seek": 97804, "start": 978.04, "end": 984.8, "text": " In other words, conv batch norm relu.", "tokens": [682, 661, 2283, 11, 3754, 15245, 2026, 1039, 84, 13], "temperature": 0.0, "avg_logprob": -0.16257858276367188, "compression_ratio": 1.5324675324675325, "no_speech_prob": 8.013445949472953e-06}, {"id": 198, "seek": 97804, "start": 984.8, "end": 988.68, "text": " So you can see here I've created two convs and here it is.", "tokens": [407, 291, 393, 536, 510, 286, 600, 2942, 732, 3754, 82, 293, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.16257858276367188, "compression_ratio": 1.5324675324675325, "no_speech_prob": 8.013445949472953e-06}, {"id": 199, "seek": 97804, "start": 988.68, "end": 993.12, "text": " I take my x, put it through the first conv, put it through the second conv, and add it", "tokens": [286, 747, 452, 2031, 11, 829, 309, 807, 264, 700, 3754, 11, 829, 309, 807, 264, 1150, 3754, 11, 293, 909, 309], "temperature": 0.0, "avg_logprob": -0.16257858276367188, "compression_ratio": 1.5324675324675325, "no_speech_prob": 8.013445949472953e-06}, {"id": 200, "seek": 97804, "start": 993.12, "end": 1006.56, "text": " back up to my input again to get my basic res block.", "tokens": [646, 493, 281, 452, 4846, 797, 281, 483, 452, 3875, 725, 3461, 13], "temperature": 0.0, "avg_logprob": -0.16257858276367188, "compression_ratio": 1.5324675324675325, "no_speech_prob": 8.013445949472953e-06}, {"id": 201, "seek": 100656, "start": 1006.56, "end": 1015.16, "text": " So one kind of interesting approach or one interesting insight here is kind of what are", "tokens": [407, 472, 733, 295, 1880, 3109, 420, 472, 1880, 11269, 510, 307, 733, 295, 437, 366], "temperature": 0.0, "avg_logprob": -0.15885202568697643, "compression_ratio": 1.7486631016042782, "no_speech_prob": 2.295913418493001e-06}, {"id": 202, "seek": 100656, "start": 1015.16, "end": 1020.0, "text": " the number of channels in these convolutions.", "tokens": [264, 1230, 295, 9235, 294, 613, 3754, 15892, 13], "temperature": 0.0, "avg_logprob": -0.15885202568697643, "compression_ratio": 1.7486631016042782, "no_speech_prob": 2.295913418493001e-06}, {"id": 203, "seek": 100656, "start": 1020.0, "end": 1025.96, "text": " So we've got coming in some ni, some number of input channels, number of inputs, or number", "tokens": [407, 321, 600, 658, 1348, 294, 512, 3867, 11, 512, 1230, 295, 4846, 9235, 11, 1230, 295, 15743, 11, 420, 1230], "temperature": 0.0, "avg_logprob": -0.15885202568697643, "compression_ratio": 1.7486631016042782, "no_speech_prob": 2.295913418493001e-06}, {"id": 204, "seek": 100656, "start": 1025.96, "end": 1029.08, "text": " of input filters.", "tokens": [295, 4846, 15995, 13], "temperature": 0.0, "avg_logprob": -0.15885202568697643, "compression_ratio": 1.7486631016042782, "no_speech_prob": 2.295913418493001e-06}, {"id": 205, "seek": 100656, "start": 1029.08, "end": 1032.96, "text": " The way that the darknet folks set things up is they said, okay, we're going to make", "tokens": [440, 636, 300, 264, 2877, 7129, 4024, 992, 721, 493, 307, 436, 848, 11, 1392, 11, 321, 434, 516, 281, 652], "temperature": 0.0, "avg_logprob": -0.15885202568697643, "compression_ratio": 1.7486631016042782, "no_speech_prob": 2.295913418493001e-06}, {"id": 206, "seek": 103296, "start": 1032.96, "end": 1038.88, "text": " every one of these res layers spit out the same number of channels that came in.", "tokens": [633, 472, 295, 613, 725, 7914, 22127, 484, 264, 912, 1230, 295, 9235, 300, 1361, 294, 13], "temperature": 0.0, "avg_logprob": -0.13582737277252505, "compression_ratio": 1.7067307692307692, "no_speech_prob": 3.446572463872144e-06}, {"id": 207, "seek": 103296, "start": 1038.88, "end": 1040.04, "text": " And I kind of like that.", "tokens": [400, 286, 733, 295, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.13582737277252505, "compression_ratio": 1.7067307692307692, "no_speech_prob": 3.446572463872144e-06}, {"id": 208, "seek": 103296, "start": 1040.04, "end": 1043.16, "text": " That's why I used it here, because it makes life simpler.", "tokens": [663, 311, 983, 286, 1143, 309, 510, 11, 570, 309, 1669, 993, 18587, 13], "temperature": 0.0, "avg_logprob": -0.13582737277252505, "compression_ratio": 1.7067307692307692, "no_speech_prob": 3.446572463872144e-06}, {"id": 209, "seek": 103296, "start": 1043.16, "end": 1049.0, "text": " And so what they did is they said, okay, let's have the first conv half the number of channels", "tokens": [400, 370, 437, 436, 630, 307, 436, 848, 11, 1392, 11, 718, 311, 362, 264, 700, 3754, 1922, 264, 1230, 295, 9235], "temperature": 0.0, "avg_logprob": -0.13582737277252505, "compression_ratio": 1.7067307692307692, "no_speech_prob": 3.446572463872144e-06}, {"id": 210, "seek": 103296, "start": 1049.0, "end": 1051.52, "text": " and then the second conv double it again.", "tokens": [293, 550, 264, 1150, 3754, 3834, 309, 797, 13], "temperature": 0.0, "avg_logprob": -0.13582737277252505, "compression_ratio": 1.7067307692307692, "no_speech_prob": 3.446572463872144e-06}, {"id": 211, "seek": 103296, "start": 1051.52, "end": 1056.24, "text": " So ni goes to ni over 2 and then ni over 2 goes to ni.", "tokens": [407, 3867, 1709, 281, 3867, 670, 568, 293, 550, 3867, 670, 568, 1709, 281, 3867, 13], "temperature": 0.0, "avg_logprob": -0.13582737277252505, "compression_ratio": 1.7067307692307692, "no_speech_prob": 3.446572463872144e-06}, {"id": 212, "seek": 105624, "start": 1056.24, "end": 1063.32, "text": " So you've kind of got this funneling thing where if you've got 64 channels coming in,", "tokens": [407, 291, 600, 733, 295, 658, 341, 24515, 278, 551, 689, 498, 291, 600, 658, 12145, 9235, 1348, 294, 11], "temperature": 0.0, "avg_logprob": -0.2669245102826287, "compression_ratio": 1.5154639175257731, "no_speech_prob": 1.209863398798916e-06}, {"id": 213, "seek": 105624, "start": 1063.32, "end": 1069.32, "text": " you kind of get squished down with a first conv down to 32 channels and then taken back", "tokens": [291, 733, 295, 483, 2339, 4729, 760, 365, 257, 700, 3754, 760, 281, 8858, 9235, 293, 550, 2726, 646], "temperature": 0.0, "avg_logprob": -0.2669245102826287, "compression_ratio": 1.5154639175257731, "no_speech_prob": 1.209863398798916e-06}, {"id": 214, "seek": 105624, "start": 1069.32, "end": 1073.16, "text": " up again to 64 channels coming out.", "tokens": [493, 797, 281, 12145, 9235, 1348, 484, 13], "temperature": 0.0, "avg_logprob": -0.2669245102826287, "compression_ratio": 1.5154639175257731, "no_speech_prob": 1.209863398798916e-06}, {"id": 215, "seek": 105624, "start": 1073.16, "end": 1076.08, "text": " Yes, Rachel?", "tokens": [1079, 11, 14246, 30], "temperature": 0.0, "avg_logprob": -0.2669245102826287, "compression_ratio": 1.5154639175257731, "no_speech_prob": 1.209863398798916e-06}, {"id": 216, "seek": 105624, "start": 1076.08, "end": 1079.08, "text": " Why is inplace equals true in the leaky Raleigh?", "tokens": [1545, 307, 294, 6742, 6915, 2074, 294, 264, 476, 15681, 497, 1220, 910, 30], "temperature": 0.0, "avg_logprob": -0.2669245102826287, "compression_ratio": 1.5154639175257731, "no_speech_prob": 1.209863398798916e-06}, {"id": 217, "seek": 105624, "start": 1079.08, "end": 1081.64, "text": " Oh, thanks for asking.", "tokens": [876, 11, 3231, 337, 3365, 13], "temperature": 0.0, "avg_logprob": -0.2669245102826287, "compression_ratio": 1.5154639175257731, "no_speech_prob": 1.209863398798916e-06}, {"id": 218, "seek": 108164, "start": 1081.64, "end": 1089.6000000000001, "text": " A lot of people forget this or don't know about it, but this is a really important memory", "tokens": [316, 688, 295, 561, 2870, 341, 420, 500, 380, 458, 466, 309, 11, 457, 341, 307, 257, 534, 1021, 4675], "temperature": 0.0, "avg_logprob": -0.12397050857543945, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.6536874909434118e-06}, {"id": 219, "seek": 108164, "start": 1089.6000000000001, "end": 1091.16, "text": " technique.", "tokens": [6532, 13], "temperature": 0.0, "avg_logprob": -0.12397050857543945, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.6536874909434118e-06}, {"id": 220, "seek": 108164, "start": 1091.16, "end": 1094.92, "text": " If you think about it, this conv layer, it's like the lowest level thing.", "tokens": [759, 291, 519, 466, 309, 11, 341, 3754, 4583, 11, 309, 311, 411, 264, 12437, 1496, 551, 13], "temperature": 0.0, "avg_logprob": -0.12397050857543945, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.6536874909434118e-06}, {"id": 221, "seek": 108164, "start": 1094.92, "end": 1098.92, "text": " So pretty much everything in our resnet, once it's all put together, is going to be conv", "tokens": [407, 1238, 709, 1203, 294, 527, 725, 7129, 11, 1564, 309, 311, 439, 829, 1214, 11, 307, 516, 281, 312, 3754], "temperature": 0.0, "avg_logprob": -0.12397050857543945, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.6536874909434118e-06}, {"id": 222, "seek": 108164, "start": 1098.92, "end": 1103.72, "text": " layers, conv layers, conv layers.", "tokens": [7914, 11, 3754, 7914, 11, 3754, 7914, 13], "temperature": 0.0, "avg_logprob": -0.12397050857543945, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.6536874909434118e-06}, {"id": 223, "seek": 108164, "start": 1103.72, "end": 1110.24, "text": " If you don't have inplace equals true, it's going to create a whole separate piece of memory", "tokens": [759, 291, 500, 380, 362, 294, 6742, 6915, 2074, 11, 309, 311, 516, 281, 1884, 257, 1379, 4994, 2522, 295, 4675], "temperature": 0.0, "avg_logprob": -0.12397050857543945, "compression_ratio": 1.7333333333333334, "no_speech_prob": 1.6536874909434118e-06}, {"id": 224, "seek": 111024, "start": 1110.24, "end": 1114.72, "text": " for the output of the value.", "tokens": [337, 264, 5598, 295, 264, 2158, 13], "temperature": 0.0, "avg_logprob": -0.16324630860359438, "compression_ratio": 1.610091743119266, "no_speech_prob": 7.07185654391651e-06}, {"id": 225, "seek": 111024, "start": 1114.72, "end": 1119.28, "text": " So it's going to allocate a whole bunch of memory that's totally unnecessary.", "tokens": [407, 309, 311, 516, 281, 35713, 257, 1379, 3840, 295, 4675, 300, 311, 3879, 19350, 13], "temperature": 0.0, "avg_logprob": -0.16324630860359438, "compression_ratio": 1.610091743119266, "no_speech_prob": 7.07185654391651e-06}, {"id": 226, "seek": 111024, "start": 1119.28, "end": 1125.8, "text": " And actually, since I wrote this, I came up with another idea the other day, which I'll", "tokens": [400, 767, 11, 1670, 286, 4114, 341, 11, 286, 1361, 493, 365, 1071, 1558, 264, 661, 786, 11, 597, 286, 603], "temperature": 0.0, "avg_logprob": -0.16324630860359438, "compression_ratio": 1.610091743119266, "no_speech_prob": 7.07185654391651e-06}, {"id": 227, "seek": 111024, "start": 1125.8, "end": 1129.76, "text": " now implement, which is you could do the same thing for the res layer.", "tokens": [586, 4445, 11, 597, 307, 291, 727, 360, 264, 912, 551, 337, 264, 725, 4583, 13], "temperature": 0.0, "avg_logprob": -0.16324630860359438, "compression_ratio": 1.610091743119266, "no_speech_prob": 7.07185654391651e-06}, {"id": 228, "seek": 111024, "start": 1129.76, "end": 1139.68, "text": " Rather than going, let's just reorder this to say, x plus that, you could actually do", "tokens": [16571, 813, 516, 11, 718, 311, 445, 319, 4687, 341, 281, 584, 11, 2031, 1804, 300, 11, 291, 727, 767, 360], "temperature": 0.0, "avg_logprob": -0.16324630860359438, "compression_ratio": 1.610091743119266, "no_speech_prob": 7.07185654391651e-06}, {"id": 229, "seek": 113968, "start": 1139.68, "end": 1141.72, "text": " the same thing here.", "tokens": [264, 912, 551, 510, 13], "temperature": 0.0, "avg_logprob": -0.1847499500621449, "compression_ratio": 1.5358851674641147, "no_speech_prob": 1.994727199416957e-06}, {"id": 230, "seek": 113968, "start": 1141.72, "end": 1146.44, "text": " Hopefully some of you might remember that in PyTorch, pretty much every function has", "tokens": [10429, 512, 295, 291, 1062, 1604, 300, 294, 9953, 51, 284, 339, 11, 1238, 709, 633, 2445, 575], "temperature": 0.0, "avg_logprob": -0.1847499500621449, "compression_ratio": 1.5358851674641147, "no_speech_prob": 1.994727199416957e-06}, {"id": 231, "seek": 113968, "start": 1146.44, "end": 1151.2, "text": " an underscore suffix version, which says do that inplace.", "tokens": [364, 37556, 3889, 970, 3037, 11, 597, 1619, 360, 300, 294, 6742, 13], "temperature": 0.0, "avg_logprob": -0.1847499500621449, "compression_ratio": 1.5358851674641147, "no_speech_prob": 1.994727199416957e-06}, {"id": 232, "seek": 113968, "start": 1151.2, "end": 1162.44, "text": " So plus, there's also an add, and so that's add inplace, and so that's now suddenly reduced", "tokens": [407, 1804, 11, 456, 311, 611, 364, 909, 11, 293, 370, 300, 311, 909, 294, 6742, 11, 293, 370, 300, 311, 586, 5800, 9212], "temperature": 0.0, "avg_logprob": -0.1847499500621449, "compression_ratio": 1.5358851674641147, "no_speech_prob": 1.994727199416957e-06}, {"id": 233, "seek": 113968, "start": 1162.44, "end": 1164.68, "text": " my memory there as well.", "tokens": [452, 4675, 456, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1847499500621449, "compression_ratio": 1.5358851674641147, "no_speech_prob": 1.994727199416957e-06}, {"id": 234, "seek": 113968, "start": 1164.68, "end": 1167.1200000000001, "text": " So these are really handy little tricks.", "tokens": [407, 613, 366, 534, 13239, 707, 11733, 13], "temperature": 0.0, "avg_logprob": -0.1847499500621449, "compression_ratio": 1.5358851674641147, "no_speech_prob": 1.994727199416957e-06}, {"id": 235, "seek": 116712, "start": 1167.12, "end": 1171.04, "text": " And I actually forgot the inplace equals true at first for this, and I literally was having", "tokens": [400, 286, 767, 5298, 264, 294, 6742, 6915, 2074, 412, 700, 337, 341, 11, 293, 286, 3736, 390, 1419], "temperature": 0.0, "avg_logprob": -0.135837265423366, "compression_ratio": 1.7325581395348837, "no_speech_prob": 9.516122190689202e-06}, {"id": 236, "seek": 116712, "start": 1171.04, "end": 1174.76, "text": " to decrease my batch size to much lower amounts than I knew should be possible, and it was", "tokens": [281, 11514, 452, 15245, 2744, 281, 709, 3126, 11663, 813, 286, 2586, 820, 312, 1944, 11, 293, 309, 390], "temperature": 0.0, "avg_logprob": -0.135837265423366, "compression_ratio": 1.7325581395348837, "no_speech_prob": 9.516122190689202e-06}, {"id": 237, "seek": 116712, "start": 1174.76, "end": 1179.6, "text": " driving me crazy, and then I realized that that was missing.", "tokens": [4840, 385, 3219, 11, 293, 550, 286, 5334, 300, 300, 390, 5361, 13], "temperature": 0.0, "avg_logprob": -0.135837265423366, "compression_ratio": 1.7325581395348837, "no_speech_prob": 9.516122190689202e-06}, {"id": 238, "seek": 116712, "start": 1179.6, "end": 1182.9199999999998, "text": " You can also do that with dropout, by the way, if you have dropout.", "tokens": [509, 393, 611, 360, 300, 365, 3270, 346, 11, 538, 264, 636, 11, 498, 291, 362, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.135837265423366, "compression_ratio": 1.7325581395348837, "no_speech_prob": 9.516122190689202e-06}, {"id": 239, "seek": 116712, "start": 1182.9199999999998, "end": 1190.32, "text": " So dropout and all the activation functions you can do inplace, and then generally any", "tokens": [407, 3270, 346, 293, 439, 264, 24433, 6828, 291, 393, 360, 294, 6742, 11, 293, 550, 5101, 604], "temperature": 0.0, "avg_logprob": -0.135837265423366, "compression_ratio": 1.7325581395348837, "no_speech_prob": 9.516122190689202e-06}, {"id": 240, "seek": 116712, "start": 1190.32, "end": 1194.3999999999999, "text": " arithmetic operation you can do inplace as well.", "tokens": [42973, 6916, 291, 393, 360, 294, 6742, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.135837265423366, "compression_ratio": 1.7325581395348837, "no_speech_prob": 9.516122190689202e-06}, {"id": 241, "seek": 119440, "start": 1194.4, "end": 1202.0800000000002, "text": " Why is bias usually, like in ResNet, set to false in the conv layer?", "tokens": [1545, 307, 12577, 2673, 11, 411, 294, 5015, 31890, 11, 992, 281, 7908, 294, 264, 3754, 4583, 30], "temperature": 0.0, "avg_logprob": -0.23211471633155747, "compression_ratio": 1.5810810810810811, "no_speech_prob": 6.401881546480581e-05}, {"id": 242, "seek": 119440, "start": 1202.0800000000002, "end": 1208.0400000000002, "text": " So if you're watching the video, pause now and see if you can figure this out, because", "tokens": [407, 498, 291, 434, 1976, 264, 960, 11, 10465, 586, 293, 536, 498, 291, 393, 2573, 341, 484, 11, 570], "temperature": 0.0, "avg_logprob": -0.23211471633155747, "compression_ratio": 1.5810810810810811, "no_speech_prob": 6.401881546480581e-05}, {"id": 243, "seek": 119440, "start": 1208.0400000000002, "end": 1210.3200000000002, "text": " this is a really interesting question.", "tokens": [341, 307, 257, 534, 1880, 1168, 13], "temperature": 0.0, "avg_logprob": -0.23211471633155747, "compression_ratio": 1.5810810810810811, "no_speech_prob": 6.401881546480581e-05}, {"id": 244, "seek": 119440, "start": 1210.3200000000002, "end": 1212.2800000000002, "text": " Why don't we need bias?", "tokens": [1545, 500, 380, 321, 643, 12577, 30], "temperature": 0.0, "avg_logprob": -0.23211471633155747, "compression_ratio": 1.5810810810810811, "no_speech_prob": 6.401881546480581e-05}, {"id": 245, "seek": 119440, "start": 1212.2800000000002, "end": 1214.96, "text": " So I'll wait for you to pause.", "tokens": [407, 286, 603, 1699, 337, 291, 281, 10465, 13], "temperature": 0.0, "avg_logprob": -0.23211471633155747, "compression_ratio": 1.5810810810810811, "no_speech_prob": 6.401881546480581e-05}, {"id": 246, "seek": 119440, "start": 1214.96, "end": 1215.96, "text": " Welcome back.", "tokens": [4027, 646, 13], "temperature": 0.0, "avg_logprob": -0.23211471633155747, "compression_ratio": 1.5810810810810811, "no_speech_prob": 6.401881546480581e-05}, {"id": 247, "seek": 119440, "start": 1215.96, "end": 1222.8000000000002, "text": " So if you figured it out, here's the thing, immediately after the conv, there's a batch", "tokens": [407, 498, 291, 8932, 309, 484, 11, 510, 311, 264, 551, 11, 4258, 934, 264, 3754, 11, 456, 311, 257, 15245], "temperature": 0.0, "avg_logprob": -0.23211471633155747, "compression_ratio": 1.5810810810810811, "no_speech_prob": 6.401881546480581e-05}, {"id": 248, "seek": 122280, "start": 1222.8, "end": 1231.32, "text": " norm, and remember batch norm has two learnable parameters for each activation, the kind of", "tokens": [2026, 11, 293, 1604, 15245, 2026, 575, 732, 1466, 712, 9834, 337, 1184, 24433, 11, 264, 733, 295], "temperature": 0.0, "avg_logprob": -0.15772021733797514, "compression_ratio": 1.7402597402597402, "no_speech_prob": 2.046275039901957e-05}, {"id": 249, "seek": 122280, "start": 1231.32, "end": 1234.8799999999999, "text": " the thing you multiply by and the thing you add.", "tokens": [264, 551, 291, 12972, 538, 293, 264, 551, 291, 909, 13], "temperature": 0.0, "avg_logprob": -0.15772021733797514, "compression_ratio": 1.7402597402597402, "no_speech_prob": 2.046275039901957e-05}, {"id": 250, "seek": 122280, "start": 1234.8799999999999, "end": 1240.24, "text": " So since we're, if we had bias here to add, and then we add another thing here, we're", "tokens": [407, 1670, 321, 434, 11, 498, 321, 632, 12577, 510, 281, 909, 11, 293, 550, 321, 909, 1071, 551, 510, 11, 321, 434], "temperature": 0.0, "avg_logprob": -0.15772021733797514, "compression_ratio": 1.7402597402597402, "no_speech_prob": 2.046275039901957e-05}, {"id": 251, "seek": 122280, "start": 1240.24, "end": 1244.52, "text": " adding two things, which is totally pointless, like that's two weights where one would do.", "tokens": [5127, 732, 721, 11, 597, 307, 3879, 32824, 11, 411, 300, 311, 732, 17443, 689, 472, 576, 360, 13], "temperature": 0.0, "avg_logprob": -0.15772021733797514, "compression_ratio": 1.7402597402597402, "no_speech_prob": 2.046275039901957e-05}, {"id": 252, "seek": 122280, "start": 1244.52, "end": 1251.9199999999998, "text": " So if you have a batch norm after a conv, then you can either say in the batch norm,", "tokens": [407, 498, 291, 362, 257, 15245, 2026, 934, 257, 3754, 11, 550, 291, 393, 2139, 584, 294, 264, 15245, 2026, 11], "temperature": 0.0, "avg_logprob": -0.15772021733797514, "compression_ratio": 1.7402597402597402, "no_speech_prob": 2.046275039901957e-05}, {"id": 253, "seek": 125192, "start": 1251.92, "end": 1256.5600000000002, "text": " don't include the add bit there please, or easier is just to say don't include the bias", "tokens": [500, 380, 4090, 264, 909, 857, 456, 1767, 11, 420, 3571, 307, 445, 281, 584, 500, 380, 4090, 264, 12577], "temperature": 0.0, "avg_logprob": -0.17872171816618546, "compression_ratio": 1.5934579439252337, "no_speech_prob": 9.080413292394951e-06}, {"id": 254, "seek": 125192, "start": 1256.5600000000002, "end": 1260.24, "text": " in the conv.", "tokens": [294, 264, 3754, 13], "temperature": 0.0, "avg_logprob": -0.17872171816618546, "compression_ratio": 1.5934579439252337, "no_speech_prob": 9.080413292394951e-06}, {"id": 255, "seek": 125192, "start": 1260.24, "end": 1264.92, "text": " There's no particular harm, but again it's going to take more memory because that's more", "tokens": [821, 311, 572, 1729, 6491, 11, 457, 797, 309, 311, 516, 281, 747, 544, 4675, 570, 300, 311, 544], "temperature": 0.0, "avg_logprob": -0.17872171816618546, "compression_ratio": 1.5934579439252337, "no_speech_prob": 9.080413292394951e-06}, {"id": 256, "seek": 125192, "start": 1264.92, "end": 1268.96, "text": " gradients that it has to keep track of.", "tokens": [2771, 2448, 300, 309, 575, 281, 1066, 2837, 295, 13], "temperature": 0.0, "avg_logprob": -0.17872171816618546, "compression_ratio": 1.5934579439252337, "no_speech_prob": 9.080413292394951e-06}, {"id": 257, "seek": 125192, "start": 1268.96, "end": 1272.44, "text": " So best to avoid.", "tokens": [407, 1151, 281, 5042, 13], "temperature": 0.0, "avg_logprob": -0.17872171816618546, "compression_ratio": 1.5934579439252337, "no_speech_prob": 9.080413292394951e-06}, {"id": 258, "seek": 125192, "start": 1272.44, "end": 1279.3600000000001, "text": " Also another thing, a little trick, is most people's conv layers have padding as a parameter,", "tokens": [2743, 1071, 551, 11, 257, 707, 4282, 11, 307, 881, 561, 311, 3754, 7914, 362, 39562, 382, 257, 13075, 11], "temperature": 0.0, "avg_logprob": -0.17872171816618546, "compression_ratio": 1.5934579439252337, "no_speech_prob": 9.080413292394951e-06}, {"id": 259, "seek": 127936, "start": 1279.36, "end": 1283.6399999999999, "text": " but generally speaking you should be able to calculate the padding easily enough.", "tokens": [457, 5101, 4124, 291, 820, 312, 1075, 281, 8873, 264, 39562, 3612, 1547, 13], "temperature": 0.0, "avg_logprob": -0.1705018772798426, "compression_ratio": 1.5520361990950227, "no_speech_prob": 1.3631252841150854e-05}, {"id": 260, "seek": 127936, "start": 1283.6399999999999, "end": 1289.8, "text": " And I see people try to implement special same padding modules and all kinds of stuff", "tokens": [400, 286, 536, 561, 853, 281, 4445, 2121, 912, 39562, 16679, 293, 439, 3685, 295, 1507], "temperature": 0.0, "avg_logprob": -0.1705018772798426, "compression_ratio": 1.5520361990950227, "no_speech_prob": 1.3631252841150854e-05}, {"id": 261, "seek": 127936, "start": 1289.8, "end": 1296.7199999999998, "text": " like that, but if you've got a stride1, or pretty much any stride actually, and you've", "tokens": [411, 300, 11, 457, 498, 291, 600, 658, 257, 1056, 482, 16, 11, 420, 1238, 709, 604, 1056, 482, 767, 11, 293, 291, 600], "temperature": 0.0, "avg_logprob": -0.1705018772798426, "compression_ratio": 1.5520361990950227, "no_speech_prob": 1.3631252841150854e-05}, {"id": 262, "seek": 127936, "start": 1296.7199999999998, "end": 1308.1999999999998, "text": " got a kernel size of 3, then obviously that's going to overlap by one unit on each side,", "tokens": [658, 257, 28256, 2744, 295, 805, 11, 550, 2745, 300, 311, 516, 281, 19959, 538, 472, 4985, 322, 1184, 1252, 11], "temperature": 0.0, "avg_logprob": -0.1705018772798426, "compression_ratio": 1.5520361990950227, "no_speech_prob": 1.3631252841150854e-05}, {"id": 263, "seek": 130820, "start": 1308.2, "end": 1309.92, "text": " so we want padding of 1.", "tokens": [370, 321, 528, 39562, 295, 502, 13], "temperature": 0.0, "avg_logprob": -0.18557829605905632, "compression_ratio": 1.5434782608695652, "no_speech_prob": 4.0294403333973605e-06}, {"id": 264, "seek": 130820, "start": 1309.92, "end": 1314.52, "text": " Or else if it's stride1, then we don't need any padding.", "tokens": [1610, 1646, 498, 309, 311, 1056, 482, 16, 11, 550, 321, 500, 380, 643, 604, 39562, 13], "temperature": 0.0, "avg_logprob": -0.18557829605905632, "compression_ratio": 1.5434782608695652, "no_speech_prob": 4.0294403333973605e-06}, {"id": 265, "seek": 130820, "start": 1314.52, "end": 1320.64, "text": " So in general, padding of kernel size integer divided by 2 is what you need.", "tokens": [407, 294, 2674, 11, 39562, 295, 28256, 2744, 24922, 6666, 538, 568, 307, 437, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.18557829605905632, "compression_ratio": 1.5434782608695652, "no_speech_prob": 4.0294403333973605e-06}, {"id": 266, "seek": 130820, "start": 1320.64, "end": 1325.44, "text": " There's some tweaks sometimes, but in this case this works perfectly well.", "tokens": [821, 311, 512, 46664, 2171, 11, 457, 294, 341, 1389, 341, 1985, 6239, 731, 13], "temperature": 0.0, "avg_logprob": -0.18557829605905632, "compression_ratio": 1.5434782608695652, "no_speech_prob": 4.0294403333973605e-06}, {"id": 267, "seek": 130820, "start": 1325.44, "end": 1330.8400000000001, "text": " So again, trying to simplify my code by having the computer calculate stuff for me rather", "tokens": [407, 797, 11, 1382, 281, 20460, 452, 3089, 538, 1419, 264, 3820, 8873, 1507, 337, 385, 2831], "temperature": 0.0, "avg_logprob": -0.18557829605905632, "compression_ratio": 1.5434782608695652, "no_speech_prob": 4.0294403333973605e-06}, {"id": 268, "seek": 130820, "start": 1330.8400000000001, "end": 1335.16, "text": " than me having to do it myself.", "tokens": [813, 385, 1419, 281, 360, 309, 2059, 13], "temperature": 0.0, "avg_logprob": -0.18557829605905632, "compression_ratio": 1.5434782608695652, "no_speech_prob": 4.0294403333973605e-06}, {"id": 269, "seek": 133516, "start": 1335.16, "end": 1340.96, "text": " Another thing here with the 2 conv layers, so we have this idea of a bottleneck, this", "tokens": [3996, 551, 510, 365, 264, 568, 3754, 7914, 11, 370, 321, 362, 341, 1558, 295, 257, 44641, 547, 11, 341], "temperature": 0.0, "avg_logprob": -0.14959321562776862, "compression_ratio": 1.6401869158878504, "no_speech_prob": 9.666056939749978e-06}, {"id": 270, "seek": 133516, "start": 1340.96, "end": 1346.3600000000001, "text": " idea of reducing the channels and then increasing them again, is also what kernel size we use.", "tokens": [1558, 295, 12245, 264, 9235, 293, 550, 5662, 552, 797, 11, 307, 611, 437, 28256, 2744, 321, 764, 13], "temperature": 0.0, "avg_logprob": -0.14959321562776862, "compression_ratio": 1.6401869158878504, "no_speech_prob": 9.666056939749978e-06}, {"id": 271, "seek": 133516, "start": 1346.3600000000001, "end": 1351.5800000000002, "text": " So here's a 1x1 conv, and so this is again something you might want to pause the video", "tokens": [407, 510, 311, 257, 502, 87, 16, 3754, 11, 293, 370, 341, 307, 797, 746, 291, 1062, 528, 281, 10465, 264, 960], "temperature": 0.0, "avg_logprob": -0.14959321562776862, "compression_ratio": 1.6401869158878504, "no_speech_prob": 9.666056939749978e-06}, {"id": 272, "seek": 133516, "start": 1351.5800000000002, "end": 1356.1200000000001, "text": " now and think about, what's a 1x1 conv really?", "tokens": [586, 293, 519, 466, 11, 437, 311, 257, 502, 87, 16, 3754, 534, 30], "temperature": 0.0, "avg_logprob": -0.14959321562776862, "compression_ratio": 1.6401869158878504, "no_speech_prob": 9.666056939749978e-06}, {"id": 273, "seek": 133516, "start": 1356.1200000000001, "end": 1362.96, "text": " What actually happens in a 1x1 conv?", "tokens": [708, 767, 2314, 294, 257, 502, 87, 16, 3754, 30], "temperature": 0.0, "avg_logprob": -0.14959321562776862, "compression_ratio": 1.6401869158878504, "no_speech_prob": 9.666056939749978e-06}, {"id": 274, "seek": 136296, "start": 1362.96, "end": 1375.24, "text": " So if we've got a little 4x4 grid here, and of course there's a filters or channels access", "tokens": [407, 498, 321, 600, 658, 257, 707, 1017, 87, 19, 10748, 510, 11, 293, 295, 1164, 456, 311, 257, 15995, 420, 9235, 2105], "temperature": 0.0, "avg_logprob": -0.10308452914742862, "compression_ratio": 1.435374149659864, "no_speech_prob": 2.2252786493481835e-06}, {"id": 275, "seek": 136296, "start": 1375.24, "end": 1381.2, "text": " as well, maybe that's like 32, and we're going to do a 1x1 conv.", "tokens": [382, 731, 11, 1310, 300, 311, 411, 8858, 11, 293, 321, 434, 516, 281, 360, 257, 502, 87, 16, 3754, 13], "temperature": 0.0, "avg_logprob": -0.10308452914742862, "compression_ratio": 1.435374149659864, "no_speech_prob": 2.2252786493481835e-06}, {"id": 276, "seek": 136296, "start": 1381.2, "end": 1386.68, "text": " So what's the kernel for a 1x1 conv going to look like?", "tokens": [407, 437, 311, 264, 28256, 337, 257, 502, 87, 16, 3754, 516, 281, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.10308452914742862, "compression_ratio": 1.435374149659864, "no_speech_prob": 2.2252786493481835e-06}, {"id": 277, "seek": 138668, "start": 1386.68, "end": 1394.8400000000001, "text": " It's going to be 1x32.", "tokens": [467, 311, 516, 281, 312, 502, 87, 11440, 13], "temperature": 0.0, "avg_logprob": -0.14397612596169496, "compression_ratio": 1.4863387978142077, "no_speech_prob": 2.2252775124798063e-06}, {"id": 278, "seek": 138668, "start": 1394.8400000000001, "end": 1400.44, "text": " So remember when we talk about the kernel size, we never mention that last piece, we", "tokens": [407, 1604, 562, 321, 751, 466, 264, 28256, 2744, 11, 321, 1128, 2152, 300, 1036, 2522, 11, 321], "temperature": 0.0, "avg_logprob": -0.14397612596169496, "compression_ratio": 1.4863387978142077, "no_speech_prob": 2.2252775124798063e-06}, {"id": 279, "seek": 138668, "start": 1400.44, "end": 1404.92, "text": " don't say it's 1x1x32 because that's part of the filters in and filters out.", "tokens": [500, 380, 584, 309, 311, 502, 87, 16, 87, 11440, 570, 300, 311, 644, 295, 264, 15995, 294, 293, 15995, 484, 13], "temperature": 0.0, "avg_logprob": -0.14397612596169496, "compression_ratio": 1.4863387978142077, "no_speech_prob": 2.2252775124798063e-06}, {"id": 280, "seek": 138668, "start": 1404.92, "end": 1411.5600000000002, "text": " So in other words then, what happens is this one thing gets placed first of all here on", "tokens": [407, 294, 661, 2283, 550, 11, 437, 2314, 307, 341, 472, 551, 2170, 7074, 700, 295, 439, 510, 322], "temperature": 0.0, "avg_logprob": -0.14397612596169496, "compression_ratio": 1.4863387978142077, "no_speech_prob": 2.2252775124798063e-06}, {"id": 281, "seek": 141156, "start": 1411.56, "end": 1425.48, "text": " the first cell, and we basically get a dot product of that 32-bit with this 32-bit, and", "tokens": [264, 700, 2815, 11, 293, 321, 1936, 483, 257, 5893, 1674, 295, 300, 8858, 12, 5260, 365, 341, 8858, 12, 5260, 11, 293], "temperature": 0.0, "avg_logprob": -0.1774398379855686, "compression_ratio": 1.7679558011049723, "no_speech_prob": 7.88921897765249e-06}, {"id": 282, "seek": 141156, "start": 1425.48, "end": 1427.96, "text": " that's going to give us our first output.", "tokens": [300, 311, 516, 281, 976, 505, 527, 700, 5598, 13], "temperature": 0.0, "avg_logprob": -0.1774398379855686, "compression_ratio": 1.7679558011049723, "no_speech_prob": 7.88921897765249e-06}, {"id": 283, "seek": 141156, "start": 1427.96, "end": 1431.08, "text": " And then we're going to take that 32-bit bit and put it with the second one to get the", "tokens": [400, 550, 321, 434, 516, 281, 747, 300, 8858, 12, 5260, 857, 293, 829, 309, 365, 264, 1150, 472, 281, 483, 264], "temperature": 0.0, "avg_logprob": -0.1774398379855686, "compression_ratio": 1.7679558011049723, "no_speech_prob": 7.88921897765249e-06}, {"id": 284, "seek": 141156, "start": 1431.08, "end": 1432.08, "text": " second output.", "tokens": [1150, 5598, 13], "temperature": 0.0, "avg_logprob": -0.1774398379855686, "compression_ratio": 1.7679558011049723, "no_speech_prob": 7.88921897765249e-06}, {"id": 285, "seek": 141156, "start": 1432.08, "end": 1440.48, "text": " So it's basically going to be a bunch of little dot products for each point in the grid.", "tokens": [407, 309, 311, 1936, 516, 281, 312, 257, 3840, 295, 707, 5893, 3383, 337, 1184, 935, 294, 264, 10748, 13], "temperature": 0.0, "avg_logprob": -0.1774398379855686, "compression_ratio": 1.7679558011049723, "no_speech_prob": 7.88921897765249e-06}, {"id": 286, "seek": 144048, "start": 1440.48, "end": 1454.0, "text": " So what it basically is then, it's basically something which is allowing us to kind of", "tokens": [407, 437, 309, 1936, 307, 550, 11, 309, 311, 1936, 746, 597, 307, 8293, 505, 281, 733, 295], "temperature": 0.0, "avg_logprob": -0.15740613937377929, "compression_ratio": 1.460431654676259, "no_speech_prob": 1.4593737205359503e-06}, {"id": 287, "seek": 144048, "start": 1454.0, "end": 1463.32, "text": " change the dimensionality in whatever way we want in the channel dimension.", "tokens": [1319, 264, 10139, 1860, 294, 2035, 636, 321, 528, 294, 264, 2269, 10139, 13], "temperature": 0.0, "avg_logprob": -0.15740613937377929, "compression_ratio": 1.460431654676259, "no_speech_prob": 1.4593737205359503e-06}, {"id": 288, "seek": 144048, "start": 1463.32, "end": 1468.76, "text": " And so that would be one of our filters.", "tokens": [400, 370, 300, 576, 312, 472, 295, 527, 15995, 13], "temperature": 0.0, "avg_logprob": -0.15740613937377929, "compression_ratio": 1.460431654676259, "no_speech_prob": 1.4593737205359503e-06}, {"id": 289, "seek": 146876, "start": 1468.76, "end": 1475.92, "text": " And so in this case, we're creating ni divided by 2 of these, so we're going to have ni divided", "tokens": [400, 370, 294, 341, 1389, 11, 321, 434, 4084, 3867, 6666, 538, 568, 295, 613, 11, 370, 321, 434, 516, 281, 362, 3867, 6666], "temperature": 0.0, "avg_logprob": -0.22205855141223316, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.902296046158881e-06}, {"id": 290, "seek": 146876, "start": 1475.92, "end": 1482.68, "text": " by 2 of these dot products all with different weighted averages of the input channels.", "tokens": [538, 568, 295, 613, 5893, 3383, 439, 365, 819, 32807, 42257, 295, 264, 4846, 9235, 13], "temperature": 0.0, "avg_logprob": -0.22205855141223316, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.902296046158881e-06}, {"id": 291, "seek": 146876, "start": 1482.68, "end": 1492.8, "text": " So it basically lets us with very little computation add this additional step of calculations and", "tokens": [407, 309, 1936, 6653, 505, 365, 588, 707, 24903, 909, 341, 4497, 1823, 295, 20448, 293], "temperature": 0.0, "avg_logprob": -0.22205855141223316, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.902296046158881e-06}, {"id": 292, "seek": 146876, "start": 1492.8, "end": 1495.72, "text": " non-linearities.", "tokens": [2107, 12, 28263, 1088, 13], "temperature": 0.0, "avg_logprob": -0.22205855141223316, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.902296046158881e-06}, {"id": 293, "seek": 149572, "start": 1495.72, "end": 1501.2, "text": " So that's a cool trick, this idea of taking advantage of these 1x1 cons, creating this", "tokens": [407, 300, 311, 257, 1627, 4282, 11, 341, 1558, 295, 1940, 5002, 295, 613, 502, 87, 16, 1014, 11, 4084, 341], "temperature": 0.0, "avg_logprob": -0.14876360986746995, "compression_ratio": 1.705607476635514, "no_speech_prob": 3.393131692064344e-06}, {"id": 294, "seek": 149572, "start": 1501.2, "end": 1506.08, "text": " bottleneck and then pulling it out again with 3x3 cons, so that's actually going to take", "tokens": [44641, 547, 293, 550, 8407, 309, 484, 797, 365, 805, 87, 18, 1014, 11, 370, 300, 311, 767, 516, 281, 747], "temperature": 0.0, "avg_logprob": -0.14876360986746995, "compression_ratio": 1.705607476635514, "no_speech_prob": 3.393131692064344e-06}, {"id": 295, "seek": 149572, "start": 1506.08, "end": 1511.56, "text": " advantage of the 2D nature of the input properly.", "tokens": [5002, 295, 264, 568, 35, 3687, 295, 264, 4846, 6108, 13], "temperature": 0.0, "avg_logprob": -0.14876360986746995, "compression_ratio": 1.705607476635514, "no_speech_prob": 3.393131692064344e-06}, {"id": 296, "seek": 149572, "start": 1511.56, "end": 1517.8, "text": " So 1x1 cons doesn't take advantage of that at all.", "tokens": [407, 502, 87, 16, 1014, 1177, 380, 747, 5002, 295, 300, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.14876360986746995, "compression_ratio": 1.705607476635514, "no_speech_prob": 3.393131692064344e-06}, {"id": 297, "seek": 149572, "start": 1517.8, "end": 1524.76, "text": " So these two lines of code, there's not much in it, but it's a really great test of your", "tokens": [407, 613, 732, 3876, 295, 3089, 11, 456, 311, 406, 709, 294, 309, 11, 457, 309, 311, 257, 534, 869, 1500, 295, 428], "temperature": 0.0, "avg_logprob": -0.14876360986746995, "compression_ratio": 1.705607476635514, "no_speech_prob": 3.393131692064344e-06}, {"id": 298, "seek": 152476, "start": 1524.76, "end": 1529.18, "text": " understanding and intuition about what's going on.", "tokens": [3701, 293, 24002, 466, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.15568122863769532, "compression_ratio": 1.7304347826086957, "no_speech_prob": 1.0289334568369668e-05}, {"id": 299, "seek": 152476, "start": 1529.18, "end": 1536.64, "text": " Why is it that a 1x1 con going from ni to ni over 2 channels followed by a 3x3 con going", "tokens": [1545, 307, 309, 300, 257, 502, 87, 16, 416, 516, 490, 3867, 281, 3867, 670, 568, 9235, 6263, 538, 257, 805, 87, 18, 416, 516], "temperature": 0.0, "avg_logprob": -0.15568122863769532, "compression_ratio": 1.7304347826086957, "no_speech_prob": 1.0289334568369668e-05}, {"id": 300, "seek": 152476, "start": 1536.64, "end": 1542.8, "text": " from ni over 2 to ni channels, why does it work, why do the tensor ranks line up, why", "tokens": [490, 3867, 670, 568, 281, 3867, 9235, 11, 983, 775, 309, 589, 11, 983, 360, 264, 40863, 21406, 1622, 493, 11, 983], "temperature": 0.0, "avg_logprob": -0.15568122863769532, "compression_ratio": 1.7304347826086957, "no_speech_prob": 1.0289334568369668e-05}, {"id": 301, "seek": 152476, "start": 1542.8, "end": 1549.32, "text": " do the dimensions all line up nicely, why is it a good idea, what's it really doing,", "tokens": [360, 264, 12819, 439, 1622, 493, 9594, 11, 983, 307, 309, 257, 665, 1558, 11, 437, 311, 309, 534, 884, 11], "temperature": 0.0, "avg_logprob": -0.15568122863769532, "compression_ratio": 1.7304347826086957, "no_speech_prob": 1.0289334568369668e-05}, {"id": 302, "seek": 152476, "start": 1549.32, "end": 1553.8799999999999, "text": " it's a really good thing to fiddle around with, maybe create some small ones in Jupyter", "tokens": [309, 311, 257, 534, 665, 551, 281, 24553, 2285, 926, 365, 11, 1310, 1884, 512, 1359, 2306, 294, 22125, 88, 391], "temperature": 0.0, "avg_logprob": -0.15568122863769532, "compression_ratio": 1.7304347826086957, "no_speech_prob": 1.0289334568369668e-05}, {"id": 303, "seek": 155388, "start": 1553.88, "end": 1558.88, "text": " Notebook, run them yourself, see what inputs and outputs come in and out, really get a", "tokens": [11633, 2939, 11, 1190, 552, 1803, 11, 536, 437, 15743, 293, 23930, 808, 294, 293, 484, 11, 534, 483, 257], "temperature": 0.0, "avg_logprob": -0.17211893330449643, "compression_ratio": 1.473053892215569, "no_speech_prob": 5.368743245526275e-07}, {"id": 304, "seek": 155388, "start": 1558.88, "end": 1561.44, "text": " feel for that.", "tokens": [841, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.17211893330449643, "compression_ratio": 1.473053892215569, "no_speech_prob": 5.368743245526275e-07}, {"id": 305, "seek": 155388, "start": 1561.44, "end": 1569.88, "text": " Once you've done so, you can then play around with different things.", "tokens": [3443, 291, 600, 1096, 370, 11, 291, 393, 550, 862, 926, 365, 819, 721, 13], "temperature": 0.0, "avg_logprob": -0.17211893330449643, "compression_ratio": 1.473053892215569, "no_speech_prob": 5.368743245526275e-07}, {"id": 306, "seek": 155388, "start": 1569.88, "end": 1582.0800000000002, "text": " One of the really unappreciated papers is this one, Wide Residual Networks.", "tokens": [1485, 295, 264, 534, 517, 1746, 3326, 770, 10577, 307, 341, 472, 11, 42543, 5015, 327, 901, 12640, 82, 13], "temperature": 0.0, "avg_logprob": -0.17211893330449643, "compression_ratio": 1.473053892215569, "no_speech_prob": 5.368743245526275e-07}, {"id": 307, "seek": 158208, "start": 1582.08, "end": 1588.08, "text": " And it's really quite a simple paper, but what they do is they basically fiddle around", "tokens": [400, 309, 311, 534, 1596, 257, 2199, 3035, 11, 457, 437, 436, 360, 307, 436, 1936, 24553, 2285, 926], "temperature": 0.0, "avg_logprob": -0.22250952544035735, "compression_ratio": 1.5706214689265536, "no_speech_prob": 2.190777649957454e-06}, {"id": 308, "seek": 158208, "start": 1588.08, "end": 1592.9199999999998, "text": " with these two lines of code.", "tokens": [365, 613, 732, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.22250952544035735, "compression_ratio": 1.5706214689265536, "no_speech_prob": 2.190777649957454e-06}, {"id": 309, "seek": 158208, "start": 1592.9199999999998, "end": 1597.6399999999999, "text": " And what they do is they say, well what if this wasn't divided by 2, but what if it was", "tokens": [400, 437, 436, 360, 307, 436, 584, 11, 731, 437, 498, 341, 2067, 380, 6666, 538, 568, 11, 457, 437, 498, 309, 390], "temperature": 0.0, "avg_logprob": -0.22250952544035735, "compression_ratio": 1.5706214689265536, "no_speech_prob": 2.190777649957454e-06}, {"id": 310, "seek": 158208, "start": 1597.6399999999999, "end": 1598.6399999999999, "text": " times 2.", "tokens": [1413, 568, 13], "temperature": 0.0, "avg_logprob": -0.22250952544035735, "compression_ratio": 1.5706214689265536, "no_speech_prob": 2.190777649957454e-06}, {"id": 311, "seek": 158208, "start": 1598.6399999999999, "end": 1602.24, "text": " That would be totally allowable.", "tokens": [663, 576, 312, 3879, 2089, 712, 13], "temperature": 0.0, "avg_logprob": -0.22250952544035735, "compression_ratio": 1.5706214689265536, "no_speech_prob": 2.190777649957454e-06}, {"id": 312, "seek": 158208, "start": 1602.24, "end": 1604.56, "text": " That's going to line up nicely.", "tokens": [663, 311, 516, 281, 1622, 493, 9594, 13], "temperature": 0.0, "avg_logprob": -0.22250952544035735, "compression_ratio": 1.5706214689265536, "no_speech_prob": 2.190777649957454e-06}, {"id": 313, "seek": 160456, "start": 1604.56, "end": 1618.56, "text": " Or what if we had another com3 after this, and so this was actually ni over 2 to ni over", "tokens": [1610, 437, 498, 321, 632, 1071, 395, 18, 934, 341, 11, 293, 370, 341, 390, 767, 3867, 670, 568, 281, 3867, 670], "temperature": 0.0, "avg_logprob": -0.21894431695705507, "compression_ratio": 1.5892857142857142, "no_speech_prob": 5.453280778056069e-07}, {"id": 314, "seek": 160456, "start": 1618.56, "end": 1621.8, "text": " 2, and then this is ni over 2.", "tokens": [568, 11, 293, 550, 341, 307, 3867, 670, 568, 13], "temperature": 0.0, "avg_logprob": -0.21894431695705507, "compression_ratio": 1.5892857142857142, "no_speech_prob": 5.453280778056069e-07}, {"id": 315, "seek": 160456, "start": 1621.8, "end": 1626.52, "text": " Again that's going to work, right, kernel size 1, 3, 1, going to half the number of", "tokens": [3764, 300, 311, 516, 281, 589, 11, 558, 11, 28256, 2744, 502, 11, 805, 11, 502, 11, 516, 281, 1922, 264, 1230, 295], "temperature": 0.0, "avg_logprob": -0.21894431695705507, "compression_ratio": 1.5892857142857142, "no_speech_prob": 5.453280778056069e-07}, {"id": 316, "seek": 160456, "start": 1626.52, "end": 1630.24, "text": " kernels, leave it at half, and then double it again at the end.", "tokens": [23434, 1625, 11, 1856, 309, 412, 1922, 11, 293, 550, 3834, 309, 797, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.21894431695705507, "compression_ratio": 1.5892857142857142, "no_speech_prob": 5.453280778056069e-07}, {"id": 317, "seek": 163024, "start": 1630.24, "end": 1638.44, "text": " And so they come up with this simple notation for basically defining what this can look", "tokens": [400, 370, 436, 808, 493, 365, 341, 2199, 24657, 337, 1936, 17827, 437, 341, 393, 574], "temperature": 0.0, "avg_logprob": -0.1639101722023704, "compression_ratio": 1.6820276497695852, "no_speech_prob": 2.026135689447983e-06}, {"id": 318, "seek": 163024, "start": 1638.44, "end": 1639.44, "text": " like.", "tokens": [411, 13], "temperature": 0.0, "avg_logprob": -0.1639101722023704, "compression_ratio": 1.6820276497695852, "no_speech_prob": 2.026135689447983e-06}, {"id": 319, "seek": 163024, "start": 1639.44, "end": 1642.24, "text": " And then they show lots of experiments.", "tokens": [400, 550, 436, 855, 3195, 295, 12050, 13], "temperature": 0.0, "avg_logprob": -0.1639101722023704, "compression_ratio": 1.6820276497695852, "no_speech_prob": 2.026135689447983e-06}, {"id": 320, "seek": 163024, "start": 1642.24, "end": 1650.96, "text": " And basically what they show is that this approach of bottlenecking, of decreasing the", "tokens": [400, 1936, 437, 436, 855, 307, 300, 341, 3109, 295, 44641, 25723, 11, 295, 23223, 264], "temperature": 0.0, "avg_logprob": -0.1639101722023704, "compression_ratio": 1.6820276497695852, "no_speech_prob": 2.026135689447983e-06}, {"id": 321, "seek": 163024, "start": 1650.96, "end": 1656.32, "text": " number of channels, which is almost universal in ResNets, is probably not a good idea.", "tokens": [1230, 295, 9235, 11, 597, 307, 1920, 11455, 294, 5015, 45, 1385, 11, 307, 1391, 406, 257, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.1639101722023704, "compression_ratio": 1.6820276497695852, "no_speech_prob": 2.026135689447983e-06}, {"id": 322, "seek": 163024, "start": 1656.32, "end": 1659.48, "text": " In fact from the experiments, definitely not a good idea.", "tokens": [682, 1186, 490, 264, 12050, 11, 2138, 406, 257, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.1639101722023704, "compression_ratio": 1.6820276497695852, "no_speech_prob": 2.026135689447983e-06}, {"id": 323, "seek": 165948, "start": 1659.48, "end": 1663.1200000000001, "text": " Because what happens is it lets you create really deep networks.", "tokens": [1436, 437, 2314, 307, 309, 6653, 291, 1884, 534, 2452, 9590, 13], "temperature": 0.0, "avg_logprob": -0.19403725108881106, "compression_ratio": 1.7035175879396984, "no_speech_prob": 9.368643077323213e-06}, {"id": 324, "seek": 165948, "start": 1663.1200000000001, "end": 1669.92, "text": " The guys who created ResNets got particularly famous creating a 1001 layer network.", "tokens": [440, 1074, 567, 2942, 5015, 45, 1385, 658, 4098, 4618, 4084, 257, 2319, 16, 4583, 3209, 13], "temperature": 0.0, "avg_logprob": -0.19403725108881106, "compression_ratio": 1.7035175879396984, "no_speech_prob": 9.368643077323213e-06}, {"id": 325, "seek": 165948, "start": 1669.92, "end": 1675.24, "text": " But the thing about 1001 layers is you can't calculate layer 2 until you finish layer 1.", "tokens": [583, 264, 551, 466, 2319, 16, 7914, 307, 291, 393, 380, 8873, 4583, 568, 1826, 291, 2413, 4583, 502, 13], "temperature": 0.0, "avg_logprob": -0.19403725108881106, "compression_ratio": 1.7035175879396984, "no_speech_prob": 9.368643077323213e-06}, {"id": 326, "seek": 165948, "start": 1675.24, "end": 1678.16, "text": " You can't calculate layer 3 until you finish layer 2.", "tokens": [509, 393, 380, 8873, 4583, 805, 1826, 291, 2413, 4583, 568, 13], "temperature": 0.0, "avg_logprob": -0.19403725108881106, "compression_ratio": 1.7035175879396984, "no_speech_prob": 9.368643077323213e-06}, {"id": 327, "seek": 165948, "start": 1678.16, "end": 1679.16, "text": " So it's sequential.", "tokens": [407, 309, 311, 42881, 13], "temperature": 0.0, "avg_logprob": -0.19403725108881106, "compression_ratio": 1.7035175879396984, "no_speech_prob": 9.368643077323213e-06}, {"id": 328, "seek": 165948, "start": 1679.16, "end": 1682.44, "text": " GPUs don't like sequential.", "tokens": [18407, 82, 500, 380, 411, 42881, 13], "temperature": 0.0, "avg_logprob": -0.19403725108881106, "compression_ratio": 1.7035175879396984, "no_speech_prob": 9.368643077323213e-06}, {"id": 329, "seek": 168244, "start": 1682.44, "end": 1690.48, "text": " So what they showed is that if you have less layers but with more calculations per layer,", "tokens": [407, 437, 436, 4712, 307, 300, 498, 291, 362, 1570, 7914, 457, 365, 544, 20448, 680, 4583, 11], "temperature": 0.0, "avg_logprob": -0.20662070609427788, "compression_ratio": 1.5457875457875458, "no_speech_prob": 4.092895323992707e-06}, {"id": 330, "seek": 168244, "start": 1690.48, "end": 1694.68, "text": " so one easy way to do that would be to remove the divider by 2.", "tokens": [370, 472, 1858, 636, 281, 360, 300, 576, 312, 281, 4159, 264, 3414, 1438, 538, 568, 13], "temperature": 0.0, "avg_logprob": -0.20662070609427788, "compression_ratio": 1.5457875457875458, "no_speech_prob": 4.092895323992707e-06}, {"id": 331, "seek": 168244, "start": 1694.68, "end": 1695.68, "text": " No other changes.", "tokens": [883, 661, 2962, 13], "temperature": 0.0, "avg_logprob": -0.20662070609427788, "compression_ratio": 1.5457875457875458, "no_speech_prob": 4.092895323992707e-06}, {"id": 332, "seek": 168244, "start": 1695.68, "end": 1697.76, "text": " Like try this at home.", "tokens": [1743, 853, 341, 412, 1280, 13], "temperature": 0.0, "avg_logprob": -0.20662070609427788, "compression_ratio": 1.5457875457875458, "no_speech_prob": 4.092895323992707e-06}, {"id": 333, "seek": 168244, "start": 1697.76, "end": 1700.68, "text": " Try running sci-fi and see what happens.", "tokens": [6526, 2614, 2180, 12, 13325, 293, 536, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.20662070609427788, "compression_ratio": 1.5457875457875458, "no_speech_prob": 4.092895323992707e-06}, {"id": 334, "seek": 168244, "start": 1700.68, "end": 1703.4, "text": " Or maybe even multiply it by 2, or fiddle around.", "tokens": [1610, 1310, 754, 12972, 309, 538, 568, 11, 420, 24553, 2285, 926, 13], "temperature": 0.0, "avg_logprob": -0.20662070609427788, "compression_ratio": 1.5457875457875458, "no_speech_prob": 4.092895323992707e-06}, {"id": 335, "seek": 168244, "start": 1703.4, "end": 1707.2, "text": " And that basically lets your GPU do more work.", "tokens": [400, 300, 1936, 6653, 428, 18407, 360, 544, 589, 13], "temperature": 0.0, "avg_logprob": -0.20662070609427788, "compression_ratio": 1.5457875457875458, "no_speech_prob": 4.092895323992707e-06}, {"id": 336, "seek": 168244, "start": 1707.2, "end": 1712.2, "text": " And it's very interesting because the vast majority of papers that talk about performance", "tokens": [400, 309, 311, 588, 1880, 570, 264, 8369, 6286, 295, 10577, 300, 751, 466, 3389], "temperature": 0.0, "avg_logprob": -0.20662070609427788, "compression_ratio": 1.5457875457875458, "no_speech_prob": 4.092895323992707e-06}, {"id": 337, "seek": 171220, "start": 1712.2, "end": 1718.88, "text": " of different architectures never actually time how long it takes to run a batch through", "tokens": [295, 819, 6331, 1303, 1128, 767, 565, 577, 938, 309, 2516, 281, 1190, 257, 15245, 807], "temperature": 0.0, "avg_logprob": -0.15402447808649122, "compression_ratio": 1.6887966804979253, "no_speech_prob": 1.3419828974292614e-05}, {"id": 338, "seek": 171220, "start": 1718.88, "end": 1719.88, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.15402447808649122, "compression_ratio": 1.6887966804979253, "no_speech_prob": 1.3419828974292614e-05}, {"id": 339, "seek": 171220, "start": 1719.88, "end": 1725.16, "text": " Like they literally say this one requires x number of floating point operations per", "tokens": [1743, 436, 3736, 584, 341, 472, 7029, 2031, 1230, 295, 12607, 935, 7705, 680], "temperature": 0.0, "avg_logprob": -0.15402447808649122, "compression_ratio": 1.6887966804979253, "no_speech_prob": 1.3419828974292614e-05}, {"id": 340, "seek": 171220, "start": 1725.16, "end": 1726.16, "text": " batch.", "tokens": [15245, 13], "temperature": 0.0, "avg_logprob": -0.15402447808649122, "compression_ratio": 1.6887966804979253, "no_speech_prob": 1.3419828974292614e-05}, {"id": 341, "seek": 171220, "start": 1726.16, "end": 1730.64, "text": " But then they never actually bother to run the damn thing like a proper experimentalist", "tokens": [583, 550, 436, 1128, 767, 8677, 281, 1190, 264, 8151, 551, 411, 257, 2296, 17069, 468], "temperature": 0.0, "avg_logprob": -0.15402447808649122, "compression_ratio": 1.6887966804979253, "no_speech_prob": 1.3419828974292614e-05}, {"id": 342, "seek": 171220, "start": 1730.64, "end": 1733.04, "text": " and find out whether it's faster or slower.", "tokens": [293, 915, 484, 1968, 309, 311, 4663, 420, 14009, 13], "temperature": 0.0, "avg_logprob": -0.15402447808649122, "compression_ratio": 1.6887966804979253, "no_speech_prob": 1.3419828974292614e-05}, {"id": 343, "seek": 171220, "start": 1733.04, "end": 1739.28, "text": " And so a lot of the architectures that are really famous now turn out to be slow as molasses", "tokens": [400, 370, 257, 688, 295, 264, 6331, 1303, 300, 366, 534, 4618, 586, 1261, 484, 281, 312, 2964, 382, 8015, 26615], "temperature": 0.0, "avg_logprob": -0.15402447808649122, "compression_ratio": 1.6887966804979253, "no_speech_prob": 1.3419828974292614e-05}, {"id": 344, "seek": 173928, "start": 1739.28, "end": 1743.8, "text": " and take crap loads of memory and are just totally useless.", "tokens": [293, 747, 12426, 12668, 295, 4675, 293, 366, 445, 3879, 14115, 13], "temperature": 0.0, "avg_logprob": -0.19949634944167094, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.714986741622852e-07}, {"id": 345, "seek": 173928, "start": 1743.8, "end": 1748.48, "text": " Because the researchers never actually bother to see whether they're fast and to actually", "tokens": [1436, 264, 10309, 1128, 767, 8677, 281, 536, 1968, 436, 434, 2370, 293, 281, 767], "temperature": 0.0, "avg_logprob": -0.19949634944167094, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.714986741622852e-07}, {"id": 346, "seek": 173928, "start": 1748.48, "end": 1752.22, "text": " see whether they fit in RAM with normal batch sizes.", "tokens": [536, 1968, 436, 3318, 294, 14561, 365, 2710, 15245, 11602, 13], "temperature": 0.0, "avg_logprob": -0.19949634944167094, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.714986741622852e-07}, {"id": 347, "seek": 173928, "start": 1752.22, "end": 1758.28, "text": " So the wide resnet paper is unusual in that it actually times how long it takes.", "tokens": [407, 264, 4874, 725, 7129, 3035, 307, 10901, 294, 300, 309, 767, 1413, 577, 938, 309, 2516, 13], "temperature": 0.0, "avg_logprob": -0.19949634944167094, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.714986741622852e-07}, {"id": 348, "seek": 173928, "start": 1758.28, "end": 1762.08, "text": " As does the YOLO version 3 paper, which made the same insight.", "tokens": [1018, 775, 264, 398, 5046, 46, 3037, 805, 3035, 11, 597, 1027, 264, 912, 11269, 13], "temperature": 0.0, "avg_logprob": -0.19949634944167094, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.714986741622852e-07}, {"id": 349, "seek": 173928, "start": 1762.08, "end": 1766.28, "text": " I'm not sure they might have missed the wide resnets paper because the YOLO version 3", "tokens": [286, 478, 406, 988, 436, 1062, 362, 6721, 264, 4874, 725, 77, 1385, 3035, 570, 264, 398, 5046, 46, 3037, 805], "temperature": 0.0, "avg_logprob": -0.19949634944167094, "compression_ratio": 1.7142857142857142, "no_speech_prob": 5.714986741622852e-07}, {"id": 350, "seek": 176628, "start": 1766.28, "end": 1771.28, "text": " paper came to a lot of the same conclusions, but I'm not even sure they cited the wide", "tokens": [3035, 1361, 281, 257, 688, 295, 264, 912, 22865, 11, 457, 286, 478, 406, 754, 988, 436, 30134, 264, 4874], "temperature": 0.0, "avg_logprob": -0.24954769300377888, "compression_ratio": 1.6186770428015564, "no_speech_prob": 5.0146486501034815e-06}, {"id": 351, "seek": 176628, "start": 1771.28, "end": 1775.6399999999999, "text": " resnets paper so they might not be aware that all that work's been done.", "tokens": [725, 77, 1385, 3035, 370, 436, 1062, 406, 312, 3650, 300, 439, 300, 589, 311, 668, 1096, 13], "temperature": 0.0, "avg_logprob": -0.24954769300377888, "compression_ratio": 1.6186770428015564, "no_speech_prob": 5.0146486501034815e-06}, {"id": 352, "seek": 176628, "start": 1775.6399999999999, "end": 1781.48, "text": " But they're both great to see people actually timing things and noticing what actually makes", "tokens": [583, 436, 434, 1293, 869, 281, 536, 561, 767, 10822, 721, 293, 21814, 437, 767, 1669], "temperature": 0.0, "avg_logprob": -0.24954769300377888, "compression_ratio": 1.6186770428015564, "no_speech_prob": 5.0146486501034815e-06}, {"id": 353, "seek": 176628, "start": 1781.48, "end": 1782.48, "text": " sense.", "tokens": [2020, 13], "temperature": 0.0, "avg_logprob": -0.24954769300377888, "compression_ratio": 1.6186770428015564, "no_speech_prob": 5.0146486501034815e-06}, {"id": 354, "seek": 176628, "start": 1782.48, "end": 1783.48, "text": " Yes, Rich?", "tokens": [1079, 11, 6781, 30], "temperature": 0.0, "avg_logprob": -0.24954769300377888, "compression_ratio": 1.6186770428015564, "no_speech_prob": 5.0146486501034815e-06}, {"id": 355, "seek": 176628, "start": 1783.48, "end": 1785.6, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.24954769300377888, "compression_ratio": 1.6186770428015564, "no_speech_prob": 5.0146486501034815e-06}, {"id": 356, "seek": 176628, "start": 1785.6, "end": 1789.48, "text": " SelU looked really hot in the paper which came out, but I noticed that you don't use", "tokens": [10736, 52, 2956, 534, 2368, 294, 264, 3035, 597, 1361, 484, 11, 457, 286, 5694, 300, 291, 500, 380, 764], "temperature": 0.0, "avg_logprob": -0.24954769300377888, "compression_ratio": 1.6186770428015564, "no_speech_prob": 5.0146486501034815e-06}, {"id": 357, "seek": 176628, "start": 1789.48, "end": 1790.48, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.24954769300377888, "compression_ratio": 1.6186770428015564, "no_speech_prob": 5.0146486501034815e-06}, {"id": 358, "seek": 176628, "start": 1790.48, "end": 1792.68, "text": " What's your opinion on SelU?", "tokens": [708, 311, 428, 4800, 322, 10736, 52, 30], "temperature": 0.0, "avg_logprob": -0.24954769300377888, "compression_ratio": 1.6186770428015564, "no_speech_prob": 5.0146486501034815e-06}, {"id": 359, "seek": 179268, "start": 1792.68, "end": 1799.88, "text": " So SelU is something largely for fully connected layers which allows you to get rid of batch", "tokens": [407, 10736, 52, 307, 746, 11611, 337, 4498, 4582, 7914, 597, 4045, 291, 281, 483, 3973, 295, 15245], "temperature": 0.0, "avg_logprob": -0.17738267998946342, "compression_ratio": 1.6607142857142858, "no_speech_prob": 5.338057690096321e-06}, {"id": 360, "seek": 179268, "start": 1799.88, "end": 1800.88, "text": " norm.", "tokens": [2026, 13], "temperature": 0.0, "avg_logprob": -0.17738267998946342, "compression_ratio": 1.6607142857142858, "no_speech_prob": 5.338057690096321e-06}, {"id": 361, "seek": 179268, "start": 1800.88, "end": 1808.0800000000002, "text": " The basic idea is that if you use this different activation function, it's kind of self-normalizing.", "tokens": [440, 3875, 1558, 307, 300, 498, 291, 764, 341, 819, 24433, 2445, 11, 309, 311, 733, 295, 2698, 12, 23157, 3319, 13], "temperature": 0.0, "avg_logprob": -0.17738267998946342, "compression_ratio": 1.6607142857142858, "no_speech_prob": 5.338057690096321e-06}, {"id": 362, "seek": 179268, "start": 1808.0800000000002, "end": 1811.1200000000001, "text": " That's what the S in SelU stands for.", "tokens": [663, 311, 437, 264, 318, 294, 10736, 52, 7382, 337, 13], "temperature": 0.0, "avg_logprob": -0.17738267998946342, "compression_ratio": 1.6607142857142858, "no_speech_prob": 5.338057690096321e-06}, {"id": 363, "seek": 179268, "start": 1811.1200000000001, "end": 1815.9, "text": " So self-normalizing means it'll always remain at a unit standard deviation and zero mean", "tokens": [407, 2698, 12, 23157, 3319, 1355, 309, 603, 1009, 6222, 412, 257, 4985, 3832, 25163, 293, 4018, 914], "temperature": 0.0, "avg_logprob": -0.17738267998946342, "compression_ratio": 1.6607142857142858, "no_speech_prob": 5.338057690096321e-06}, {"id": 364, "seek": 179268, "start": 1815.9, "end": 1819.68, "text": " and therefore you don't need that batch norm.", "tokens": [293, 4412, 291, 500, 380, 643, 300, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.17738267998946342, "compression_ratio": 1.6607142857142858, "no_speech_prob": 5.338057690096321e-06}, {"id": 365, "seek": 181968, "start": 1819.68, "end": 1823.4, "text": " It hasn't really gone anywhere, and the reason it hasn't really gone anywhere is because", "tokens": [467, 6132, 380, 534, 2780, 4992, 11, 293, 264, 1778, 309, 6132, 380, 534, 2780, 4992, 307, 570], "temperature": 0.0, "avg_logprob": -0.17632743028494027, "compression_ratio": 1.8025210084033614, "no_speech_prob": 1.1842977073683869e-05}, {"id": 366, "seek": 181968, "start": 1823.4, "end": 1825.6000000000001, "text": " it's incredibly finicky.", "tokens": [309, 311, 6252, 962, 20539, 13], "temperature": 0.0, "avg_logprob": -0.17632743028494027, "compression_ratio": 1.8025210084033614, "no_speech_prob": 1.1842977073683869e-05}, {"id": 367, "seek": 181968, "start": 1825.6000000000001, "end": 1830.04, "text": " You have to use a very specific initialization otherwise it doesn't start with exactly the", "tokens": [509, 362, 281, 764, 257, 588, 2685, 5883, 2144, 5911, 309, 1177, 380, 722, 365, 2293, 264], "temperature": 0.0, "avg_logprob": -0.17632743028494027, "compression_ratio": 1.8025210084033614, "no_speech_prob": 1.1842977073683869e-05}, {"id": 368, "seek": 181968, "start": 1830.04, "end": 1835.1200000000001, "text": " right standard deviation and mean.", "tokens": [558, 3832, 25163, 293, 914, 13], "temperature": 0.0, "avg_logprob": -0.17632743028494027, "compression_ratio": 1.8025210084033614, "no_speech_prob": 1.1842977073683869e-05}, {"id": 369, "seek": 181968, "start": 1835.1200000000001, "end": 1836.88, "text": " Very hard to use it with things like embeddings.", "tokens": [4372, 1152, 281, 764, 309, 365, 721, 411, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.17632743028494027, "compression_ratio": 1.8025210084033614, "no_speech_prob": 1.1842977073683869e-05}, {"id": 370, "seek": 181968, "start": 1836.88, "end": 1841.04, "text": " If you do, then you have to use a particular kind of embedding.", "tokens": [759, 291, 360, 11, 550, 291, 362, 281, 764, 257, 1729, 733, 295, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.17632743028494027, "compression_ratio": 1.8025210084033614, "no_speech_prob": 1.1842977073683869e-05}, {"id": 371, "seek": 181968, "start": 1841.04, "end": 1845.3200000000002, "text": " Initialization which doesn't necessarily actually make sense for embeddings.", "tokens": [22937, 831, 2144, 597, 1177, 380, 4725, 767, 652, 2020, 337, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.17632743028494027, "compression_ratio": 1.8025210084033614, "no_speech_prob": 1.1842977073683869e-05}, {"id": 372, "seek": 184532, "start": 1845.32, "end": 1852.08, "text": " So you do all this work, very hard to get it right, and if you do finally get it right,", "tokens": [407, 291, 360, 439, 341, 589, 11, 588, 1152, 281, 483, 309, 558, 11, 293, 498, 291, 360, 2721, 483, 309, 558, 11], "temperature": 0.0, "avg_logprob": -0.18053298950195312, "compression_ratio": 1.6909722222222223, "no_speech_prob": 1.7330437458440429e-06}, {"id": 373, "seek": 184532, "start": 1852.08, "end": 1856.08, "text": " what's the point where you've managed to get rid of some batch norm layers which weren't", "tokens": [437, 311, 264, 935, 689, 291, 600, 6453, 281, 483, 3973, 295, 512, 15245, 2026, 7914, 597, 4999, 380], "temperature": 0.0, "avg_logprob": -0.18053298950195312, "compression_ratio": 1.6909722222222223, "no_speech_prob": 1.7330437458440429e-06}, {"id": 374, "seek": 184532, "start": 1856.08, "end": 1858.48, "text": " really hurting you anyway.", "tokens": [534, 17744, 291, 4033, 13], "temperature": 0.0, "avg_logprob": -0.18053298950195312, "compression_ratio": 1.6909722222222223, "no_speech_prob": 1.7330437458440429e-06}, {"id": 375, "seek": 184532, "start": 1858.48, "end": 1863.24, "text": " It's interesting because that SelU paper, I think one of the reasons people noticed it,", "tokens": [467, 311, 1880, 570, 300, 10736, 52, 3035, 11, 286, 519, 472, 295, 264, 4112, 561, 5694, 309, 11], "temperature": 0.0, "avg_logprob": -0.18053298950195312, "compression_ratio": 1.6909722222222223, "no_speech_prob": 1.7330437458440429e-06}, {"id": 376, "seek": 184532, "start": 1863.24, "end": 1866.6, "text": " or in my experience the main reason people noticed it, was because it was created by", "tokens": [420, 294, 452, 1752, 264, 2135, 1778, 561, 5694, 309, 11, 390, 570, 309, 390, 2942, 538], "temperature": 0.0, "avg_logprob": -0.18053298950195312, "compression_ratio": 1.6909722222222223, "no_speech_prob": 1.7330437458440429e-06}, {"id": 377, "seek": 184532, "start": 1866.6, "end": 1869.2, "text": " the inventor of LSTMs.", "tokens": [264, 41593, 295, 441, 6840, 26386, 13], "temperature": 0.0, "avg_logprob": -0.18053298950195312, "compression_ratio": 1.6909722222222223, "no_speech_prob": 1.7330437458440429e-06}, {"id": 378, "seek": 184532, "start": 1869.2, "end": 1874.08, "text": " And also it had a huge mathematical appendix and people were like, lots of maths from a", "tokens": [400, 611, 309, 632, 257, 2603, 18894, 34116, 970, 293, 561, 645, 411, 11, 3195, 295, 36287, 490, 257], "temperature": 0.0, "avg_logprob": -0.18053298950195312, "compression_ratio": 1.6909722222222223, "no_speech_prob": 1.7330437458440429e-06}, {"id": 379, "seek": 187408, "start": 1874.08, "end": 1877.52, "text": " famous guy, this must be great.", "tokens": [4618, 2146, 11, 341, 1633, 312, 869, 13], "temperature": 0.0, "avg_logprob": -0.13116751490412532, "compression_ratio": 1.7088607594936709, "no_speech_prob": 1.663157576103913e-07}, {"id": 380, "seek": 187408, "start": 1877.52, "end": 1883.6799999999998, "text": " But in practice I don't see anybody using it to get any state of the art results or", "tokens": [583, 294, 3124, 286, 500, 380, 536, 4472, 1228, 309, 281, 483, 604, 1785, 295, 264, 1523, 3542, 420], "temperature": 0.0, "avg_logprob": -0.13116751490412532, "compression_ratio": 1.7088607594936709, "no_speech_prob": 1.663157576103913e-07}, {"id": 381, "seek": 187408, "start": 1883.6799999999998, "end": 1889.52, "text": " win any competitions or anything like that.", "tokens": [1942, 604, 26185, 420, 1340, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.13116751490412532, "compression_ratio": 1.7088607594936709, "no_speech_prob": 1.663157576103913e-07}, {"id": 382, "seek": 187408, "start": 1889.52, "end": 1893.3999999999999, "text": " So this is like some of the tiniest bits of code we've seen, but there's so much here", "tokens": [407, 341, 307, 411, 512, 295, 264, 256, 3812, 377, 9239, 295, 3089, 321, 600, 1612, 11, 457, 456, 311, 370, 709, 510], "temperature": 0.0, "avg_logprob": -0.13116751490412532, "compression_ratio": 1.7088607594936709, "no_speech_prob": 1.663157576103913e-07}, {"id": 383, "seek": 187408, "start": 1893.3999999999999, "end": 1896.6399999999999, "text": " and it's fascinating to play with.", "tokens": [293, 309, 311, 10343, 281, 862, 365, 13], "temperature": 0.0, "avg_logprob": -0.13116751490412532, "compression_ratio": 1.7088607594936709, "no_speech_prob": 1.663157576103913e-07}, {"id": 384, "seek": 187408, "start": 1896.6399999999999, "end": 1900.8, "text": " So now we've got this block which is built on this block, and then we're going to create", "tokens": [407, 586, 321, 600, 658, 341, 3461, 597, 307, 3094, 322, 341, 3461, 11, 293, 550, 321, 434, 516, 281, 1884], "temperature": 0.0, "avg_logprob": -0.13116751490412532, "compression_ratio": 1.7088607594936709, "no_speech_prob": 1.663157576103913e-07}, {"id": 385, "seek": 187408, "start": 1900.8, "end": 1903.8, "text": " another block on top of that block.", "tokens": [1071, 3461, 322, 1192, 295, 300, 3461, 13], "temperature": 0.0, "avg_logprob": -0.13116751490412532, "compression_ratio": 1.7088607594936709, "no_speech_prob": 1.663157576103913e-07}, {"id": 386, "seek": 190380, "start": 1903.8, "end": 1911.56, "text": " So we're going to call this a group layer, and it's going to contain a bunch of res layers.", "tokens": [407, 321, 434, 516, 281, 818, 341, 257, 1594, 4583, 11, 293, 309, 311, 516, 281, 5304, 257, 3840, 295, 725, 7914, 13], "temperature": 0.0, "avg_logprob": -0.10677853608742738, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.0952982847811654e-05}, {"id": 387, "seek": 190380, "start": 1911.56, "end": 1921.02, "text": " And so a group layer is going to have some number of channels or filters coming in.", "tokens": [400, 370, 257, 1594, 4583, 307, 516, 281, 362, 512, 1230, 295, 9235, 420, 15995, 1348, 294, 13], "temperature": 0.0, "avg_logprob": -0.10677853608742738, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.0952982847811654e-05}, {"id": 388, "seek": 190380, "start": 1921.02, "end": 1926.08, "text": " And what we're going to do is we're going to double the number of channels coming in", "tokens": [400, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 3834, 264, 1230, 295, 9235, 1348, 294], "temperature": 0.0, "avg_logprob": -0.10677853608742738, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.0952982847811654e-05}, {"id": 389, "seek": 190380, "start": 1926.08, "end": 1930.6399999999999, "text": " by just using a standard conv layer.", "tokens": [538, 445, 1228, 257, 3832, 3754, 4583, 13], "temperature": 0.0, "avg_logprob": -0.10677853608742738, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.0952982847811654e-05}, {"id": 390, "seek": 193064, "start": 1930.64, "end": 1936.16, "text": " Additionally we'll halve the grid size by using a stride of 2.", "tokens": [19927, 321, 603, 7523, 303, 264, 10748, 2744, 538, 1228, 257, 1056, 482, 295, 568, 13], "temperature": 0.0, "avg_logprob": -0.17565583283046507, "compression_ratio": 1.7092511013215859, "no_speech_prob": 5.594308731815545e-06}, {"id": 391, "seek": 193064, "start": 1936.16, "end": 1941.8400000000001, "text": " And then we're going to do a whole bunch of res blocks, a whole bunch of res layers.", "tokens": [400, 550, 321, 434, 516, 281, 360, 257, 1379, 3840, 295, 725, 8474, 11, 257, 1379, 3840, 295, 725, 7914, 13], "temperature": 0.0, "avg_logprob": -0.17565583283046507, "compression_ratio": 1.7092511013215859, "no_speech_prob": 5.594308731815545e-06}, {"id": 392, "seek": 193064, "start": 1941.8400000000001, "end": 1945.92, "text": " We can pick how many, that could be 2 or 3 or 8.", "tokens": [492, 393, 1888, 577, 867, 11, 300, 727, 312, 568, 420, 805, 420, 1649, 13], "temperature": 0.0, "avg_logprob": -0.17565583283046507, "compression_ratio": 1.7092511013215859, "no_speech_prob": 5.594308731815545e-06}, {"id": 393, "seek": 193064, "start": 1945.92, "end": 1951.24, "text": " Because remember these res layers don't change the grid size and they don't change the number", "tokens": [1436, 1604, 613, 725, 7914, 500, 380, 1319, 264, 10748, 2744, 293, 436, 500, 380, 1319, 264, 1230], "temperature": 0.0, "avg_logprob": -0.17565583283046507, "compression_ratio": 1.7092511013215859, "no_speech_prob": 5.594308731815545e-06}, {"id": 394, "seek": 193064, "start": 1951.24, "end": 1952.24, "text": " of channels.", "tokens": [295, 9235, 13], "temperature": 0.0, "avg_logprob": -0.17565583283046507, "compression_ratio": 1.7092511013215859, "no_speech_prob": 5.594308731815545e-06}, {"id": 395, "seek": 193064, "start": 1952.24, "end": 1956.76, "text": " So you can add as many as you like, anywhere you like, without causing any problems.", "tokens": [407, 291, 393, 909, 382, 867, 382, 291, 411, 11, 4992, 291, 411, 11, 1553, 9853, 604, 2740, 13], "temperature": 0.0, "avg_logprob": -0.17565583283046507, "compression_ratio": 1.7092511013215859, "no_speech_prob": 5.594308731815545e-06}, {"id": 396, "seek": 195676, "start": 1956.76, "end": 1962.44, "text": " It's just going to use more computation and more RAM, but there's no reason other than", "tokens": [467, 311, 445, 516, 281, 764, 544, 24903, 293, 544, 14561, 11, 457, 456, 311, 572, 1778, 661, 813], "temperature": 0.0, "avg_logprob": -0.136838183111074, "compression_ratio": 1.623931623931624, "no_speech_prob": 3.3931305551959667e-06}, {"id": 397, "seek": 195676, "start": 1962.44, "end": 1964.32, "text": " that you can't add as many as you like.", "tokens": [300, 291, 393, 380, 909, 382, 867, 382, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.136838183111074, "compression_ratio": 1.623931623931624, "no_speech_prob": 3.3931305551959667e-06}, {"id": 398, "seek": 195676, "start": 1964.32, "end": 1971.12, "text": " So a group layer therefore is going to end up doubling the number of channels because", "tokens": [407, 257, 1594, 4583, 4412, 307, 516, 281, 917, 493, 33651, 264, 1230, 295, 9235, 570], "temperature": 0.0, "avg_logprob": -0.136838183111074, "compression_ratio": 1.623931623931624, "no_speech_prob": 3.3931305551959667e-06}, {"id": 399, "seek": 195676, "start": 1971.12, "end": 1978.6, "text": " of this initial convolution which doubles the number of channels.", "tokens": [295, 341, 5883, 45216, 597, 31634, 264, 1230, 295, 9235, 13], "temperature": 0.0, "avg_logprob": -0.136838183111074, "compression_ratio": 1.623931623931624, "no_speech_prob": 3.3931305551959667e-06}, {"id": 400, "seek": 195676, "start": 1978.6, "end": 1983.58, "text": " And depending on what we pass in a stride, it may also halve the grid size if we put", "tokens": [400, 5413, 322, 437, 321, 1320, 294, 257, 1056, 482, 11, 309, 815, 611, 7523, 303, 264, 10748, 2744, 498, 321, 829], "temperature": 0.0, "avg_logprob": -0.136838183111074, "compression_ratio": 1.623931623931624, "no_speech_prob": 3.3931305551959667e-06}, {"id": 401, "seek": 195676, "start": 1983.58, "end": 1985.52, "text": " stride equals 2.", "tokens": [1056, 482, 6915, 568, 13], "temperature": 0.0, "avg_logprob": -0.136838183111074, "compression_ratio": 1.623931623931624, "no_speech_prob": 3.3931305551959667e-06}, {"id": 402, "seek": 198552, "start": 1985.52, "end": 1994.0, "text": " And then we can do a whole bunch of res block computations, as many as we like.", "tokens": [400, 550, 321, 393, 360, 257, 1379, 3840, 295, 725, 3461, 2807, 763, 11, 382, 867, 382, 321, 411, 13], "temperature": 0.0, "avg_logprob": -0.11385900428496211, "compression_ratio": 1.502415458937198, "no_speech_prob": 2.3320696982409572e-06}, {"id": 403, "seek": 198552, "start": 1994.0, "end": 2001.28, "text": " So then to define our dark net or whatever we want to call this thing, we're just going", "tokens": [407, 550, 281, 6964, 527, 2877, 2533, 420, 2035, 321, 528, 281, 818, 341, 551, 11, 321, 434, 445, 516], "temperature": 0.0, "avg_logprob": -0.11385900428496211, "compression_ratio": 1.502415458937198, "no_speech_prob": 2.3320696982409572e-06}, {"id": 404, "seek": 198552, "start": 2001.28, "end": 2006.44, "text": " to pass in something that looks like this.", "tokens": [281, 1320, 294, 746, 300, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.11385900428496211, "compression_ratio": 1.502415458937198, "no_speech_prob": 2.3320696982409572e-06}, {"id": 405, "seek": 198552, "start": 2006.44, "end": 2010.44, "text": " And what this says is create 5 group layers.", "tokens": [400, 437, 341, 1619, 307, 1884, 1025, 1594, 7914, 13], "temperature": 0.0, "avg_logprob": -0.11385900428496211, "compression_ratio": 1.502415458937198, "no_speech_prob": 2.3320696982409572e-06}, {"id": 406, "seek": 198552, "start": 2010.44, "end": 2014.36, "text": " The first one will contain 1 of these extra res layers.", "tokens": [440, 700, 472, 486, 5304, 502, 295, 613, 2857, 725, 7914, 13], "temperature": 0.0, "avg_logprob": -0.11385900428496211, "compression_ratio": 1.502415458937198, "no_speech_prob": 2.3320696982409572e-06}, {"id": 407, "seek": 201436, "start": 2014.36, "end": 2019.3999999999999, "text": " The second will contain 2, then 4, then 6, then 3.", "tokens": [440, 1150, 486, 5304, 568, 11, 550, 1017, 11, 550, 1386, 11, 550, 805, 13], "temperature": 0.0, "avg_logprob": -0.1170280629938299, "compression_ratio": 1.6453488372093024, "no_speech_prob": 3.1875574677542318e-06}, {"id": 408, "seek": 201436, "start": 2019.3999999999999, "end": 2027.6, "text": " And I want you to start with 32 filters.", "tokens": [400, 286, 528, 291, 281, 722, 365, 8858, 15995, 13], "temperature": 0.0, "avg_logprob": -0.1170280629938299, "compression_ratio": 1.6453488372093024, "no_speech_prob": 3.1875574677542318e-06}, {"id": 409, "seek": 201436, "start": 2027.6, "end": 2037.1599999999999, "text": " So the first one of these res layers will contain 32 filters and there will just be", "tokens": [407, 264, 700, 472, 295, 613, 725, 7914, 486, 5304, 8858, 15995, 293, 456, 486, 445, 312], "temperature": 0.0, "avg_logprob": -0.1170280629938299, "compression_ratio": 1.6453488372093024, "no_speech_prob": 3.1875574677542318e-06}, {"id": 410, "seek": 201436, "start": 2037.1599999999999, "end": 2039.28, "text": " one extra res layer.", "tokens": [472, 2857, 725, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1170280629938299, "compression_ratio": 1.6453488372093024, "no_speech_prob": 3.1875574677542318e-06}, {"id": 411, "seek": 201436, "start": 2039.28, "end": 2043.84, "text": " The second one is going to double the number of filters because that's what we do each", "tokens": [440, 1150, 472, 307, 516, 281, 3834, 264, 1230, 295, 15995, 570, 300, 311, 437, 321, 360, 1184], "temperature": 0.0, "avg_logprob": -0.1170280629938299, "compression_ratio": 1.6453488372093024, "no_speech_prob": 3.1875574677542318e-06}, {"id": 412, "seek": 204384, "start": 2043.84, "end": 2046.28, "text": " time we have a new group layer, we double the number.", "tokens": [565, 321, 362, 257, 777, 1594, 4583, 11, 321, 3834, 264, 1230, 13], "temperature": 0.0, "avg_logprob": -0.13020892496462222, "compression_ratio": 1.7652173913043478, "no_speech_prob": 5.862799298483878e-06}, {"id": 413, "seek": 204384, "start": 2046.28, "end": 2054.68, "text": " So the second one will have 64, then 128, then 256, then 512, and then that'll be it.", "tokens": [407, 264, 1150, 472, 486, 362, 12145, 11, 550, 29810, 11, 550, 38882, 11, 550, 1025, 4762, 11, 293, 550, 300, 603, 312, 309, 13], "temperature": 0.0, "avg_logprob": -0.13020892496462222, "compression_ratio": 1.7652173913043478, "no_speech_prob": 5.862799298483878e-06}, {"id": 414, "seek": 204384, "start": 2054.68, "end": 2060.36, "text": " So that's going to be like nearly all of the network is going to be those bunches of layers.", "tokens": [407, 300, 311, 516, 281, 312, 411, 6217, 439, 295, 264, 3209, 307, 516, 281, 312, 729, 3840, 279, 295, 7914, 13], "temperature": 0.0, "avg_logprob": -0.13020892496462222, "compression_ratio": 1.7652173913043478, "no_speech_prob": 5.862799298483878e-06}, {"id": 415, "seek": 204384, "start": 2060.36, "end": 2067.52, "text": " And remember every one of those group layers also has one convolution at the start.", "tokens": [400, 1604, 633, 472, 295, 729, 1594, 7914, 611, 575, 472, 45216, 412, 264, 722, 13], "temperature": 0.0, "avg_logprob": -0.13020892496462222, "compression_ratio": 1.7652173913043478, "no_speech_prob": 5.862799298483878e-06}, {"id": 416, "seek": 204384, "start": 2067.52, "end": 2072.92, "text": " And so then all we have is before that all happens, we're going to have one convolutional", "tokens": [400, 370, 550, 439, 321, 362, 307, 949, 300, 439, 2314, 11, 321, 434, 516, 281, 362, 472, 45216, 304], "temperature": 0.0, "avg_logprob": -0.13020892496462222, "compression_ratio": 1.7652173913043478, "no_speech_prob": 5.862799298483878e-06}, {"id": 417, "seek": 207292, "start": 2072.92, "end": 2078.7200000000003, "text": " layer at the very start and at the very end we're going to do our standard adaptive average", "tokens": [4583, 412, 264, 588, 722, 293, 412, 264, 588, 917, 321, 434, 516, 281, 360, 527, 3832, 27912, 4274], "temperature": 0.0, "avg_logprob": -0.1664655605951945, "compression_ratio": 1.883495145631068, "no_speech_prob": 8.139626515912823e-06}, {"id": 418, "seek": 207292, "start": 2078.7200000000003, "end": 2085.12, "text": " pooling, flatten, and a linear layer to create the number of classes out at the end.", "tokens": [7005, 278, 11, 24183, 11, 293, 257, 8213, 4583, 281, 1884, 264, 1230, 295, 5359, 484, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.1664655605951945, "compression_ratio": 1.883495145631068, "no_speech_prob": 8.139626515912823e-06}, {"id": 419, "seek": 207292, "start": 2085.12, "end": 2091.52, "text": " So one convolution at the end, adaptive pooling, and one linear layer at the other end, and", "tokens": [407, 472, 45216, 412, 264, 917, 11, 27912, 7005, 278, 11, 293, 472, 8213, 4583, 412, 264, 661, 917, 11, 293], "temperature": 0.0, "avg_logprob": -0.1664655605951945, "compression_ratio": 1.883495145631068, "no_speech_prob": 8.139626515912823e-06}, {"id": 420, "seek": 207292, "start": 2091.52, "end": 2097.7200000000003, "text": " then in the middle these group layers, each one consisting of a convolutional layer followed", "tokens": [550, 294, 264, 2808, 613, 1594, 7914, 11, 1184, 472, 33921, 295, 257, 45216, 304, 4583, 6263], "temperature": 0.0, "avg_logprob": -0.1664655605951945, "compression_ratio": 1.883495145631068, "no_speech_prob": 8.139626515912823e-06}, {"id": 421, "seek": 207292, "start": 2097.7200000000003, "end": 2101.8, "text": " by n number of res layers.", "tokens": [538, 297, 1230, 295, 725, 7914, 13], "temperature": 0.0, "avg_logprob": -0.1664655605951945, "compression_ratio": 1.883495145631068, "no_speech_prob": 8.139626515912823e-06}, {"id": 422, "seek": 210180, "start": 2101.8, "end": 2103.92, "text": " That's it.", "tokens": [663, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.15674808892336758, "compression_ratio": 1.5616438356164384, "no_speech_prob": 1.1365599448254216e-06}, {"id": 423, "seek": 210180, "start": 2103.92, "end": 2110.92, "text": " Again I think we've mentioned this a few times, but I'm yet to see any code out there, any", "tokens": [3764, 286, 519, 321, 600, 2835, 341, 257, 1326, 1413, 11, 457, 286, 478, 1939, 281, 536, 604, 3089, 484, 456, 11, 604], "temperature": 0.0, "avg_logprob": -0.15674808892336758, "compression_ratio": 1.5616438356164384, "no_speech_prob": 1.1365599448254216e-06}, {"id": 424, "seek": 210180, "start": 2110.92, "end": 2116.4, "text": " examples, anything anywhere that uses adaptive average pooling.", "tokens": [5110, 11, 1340, 4992, 300, 4960, 27912, 4274, 7005, 278, 13], "temperature": 0.0, "avg_logprob": -0.15674808892336758, "compression_ratio": 1.5616438356164384, "no_speech_prob": 1.1365599448254216e-06}, {"id": 425, "seek": 210180, "start": 2116.4, "end": 2122.8, "text": " Everyone I've seen writes it like this, and then bits a particular number here, which", "tokens": [5198, 286, 600, 1612, 13657, 309, 411, 341, 11, 293, 550, 9239, 257, 1729, 1230, 510, 11, 597], "temperature": 0.0, "avg_logprob": -0.15674808892336758, "compression_ratio": 1.5616438356164384, "no_speech_prob": 1.1365599448254216e-06}, {"id": 426, "seek": 210180, "start": 2122.8, "end": 2127.6400000000003, "text": " means that it's now tied to a particular image size, which definitely isn't what you want.", "tokens": [1355, 300, 309, 311, 586, 9601, 281, 257, 1729, 3256, 2744, 11, 597, 2138, 1943, 380, 437, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.15674808892336758, "compression_ratio": 1.5616438356164384, "no_speech_prob": 1.1365599448254216e-06}, {"id": 427, "seek": 212764, "start": 2127.64, "end": 2132.5, "text": " So most people, even the top researchers I speak to, most of them are still under the", "tokens": [407, 881, 561, 11, 754, 264, 1192, 10309, 286, 1710, 281, 11, 881, 295, 552, 366, 920, 833, 264], "temperature": 0.0, "avg_logprob": -0.13371221518810886, "compression_ratio": 1.7, "no_speech_prob": 1.10159248833952e-06}, {"id": 428, "seek": 212764, "start": 2132.5, "end": 2139.96, "text": " impression that a specific architecture is tied to a specific size, and that's a huge", "tokens": [9995, 300, 257, 2685, 9482, 307, 9601, 281, 257, 2685, 2744, 11, 293, 300, 311, 257, 2603], "temperature": 0.0, "avg_logprob": -0.13371221518810886, "compression_ratio": 1.7, "no_speech_prob": 1.10159248833952e-06}, {"id": 429, "seek": 212764, "start": 2139.96, "end": 2145.3199999999997, "text": " problem when people think that because it really limits their ability to use smaller", "tokens": [1154, 562, 561, 519, 300, 570, 309, 534, 10406, 641, 3485, 281, 764, 4356], "temperature": 0.0, "avg_logprob": -0.13371221518810886, "compression_ratio": 1.7, "no_speech_prob": 1.10159248833952e-06}, {"id": 430, "seek": 212764, "start": 2145.3199999999997, "end": 2149.8399999999997, "text": " sizes to kickstart their modeling or to use smaller sizes for doing experiments and stuff", "tokens": [11602, 281, 4437, 24419, 641, 15983, 420, 281, 764, 4356, 11602, 337, 884, 12050, 293, 1507], "temperature": 0.0, "avg_logprob": -0.13371221518810886, "compression_ratio": 1.7, "no_speech_prob": 1.10159248833952e-06}, {"id": 431, "seek": 212764, "start": 2149.8399999999997, "end": 2153.8799999999997, "text": " like that.", "tokens": [411, 300, 13], "temperature": 0.0, "avg_logprob": -0.13371221518810886, "compression_ratio": 1.7, "no_speech_prob": 1.10159248833952e-06}, {"id": 432, "seek": 215388, "start": 2153.88, "end": 2158.4, "text": " Again you'll notice I'm using sequential here, but a nice way to create architectures is", "tokens": [3764, 291, 603, 3449, 286, 478, 1228, 42881, 510, 11, 457, 257, 1481, 636, 281, 1884, 6331, 1303, 307], "temperature": 0.0, "avg_logprob": -0.12001626387886379, "compression_ratio": 1.8095238095238095, "no_speech_prob": 1.406391220371006e-05}, {"id": 433, "seek": 215388, "start": 2158.4, "end": 2163.2000000000003, "text": " to start out by creating a list, in this case this is a list with just one comp layer in,", "tokens": [281, 722, 484, 538, 4084, 257, 1329, 11, 294, 341, 1389, 341, 307, 257, 1329, 365, 445, 472, 715, 4583, 294, 11], "temperature": 0.0, "avg_logprob": -0.12001626387886379, "compression_ratio": 1.8095238095238095, "no_speech_prob": 1.406391220371006e-05}, {"id": 434, "seek": 215388, "start": 2163.2000000000003, "end": 2168.5, "text": " and then my function here, make group layer, it just returns another list.", "tokens": [293, 550, 452, 2445, 510, 11, 652, 1594, 4583, 11, 309, 445, 11247, 1071, 1329, 13], "temperature": 0.0, "avg_logprob": -0.12001626387886379, "compression_ratio": 1.8095238095238095, "no_speech_prob": 1.406391220371006e-05}, {"id": 435, "seek": 215388, "start": 2168.5, "end": 2173.6400000000003, "text": " So then I can just go plus equals, appending that list to the previous list, and then I", "tokens": [407, 550, 286, 393, 445, 352, 1804, 6915, 11, 724, 2029, 300, 1329, 281, 264, 3894, 1329, 11, 293, 550, 286], "temperature": 0.0, "avg_logprob": -0.12001626387886379, "compression_ratio": 1.8095238095238095, "no_speech_prob": 1.406391220371006e-05}, {"id": 436, "seek": 215388, "start": 2173.6400000000003, "end": 2178.8, "text": " could go plus equals to append this bunch of things to that list, and then finally sequential", "tokens": [727, 352, 1804, 6915, 281, 34116, 341, 3840, 295, 721, 281, 300, 1329, 11, 293, 550, 2721, 42881], "temperature": 0.0, "avg_logprob": -0.12001626387886379, "compression_ratio": 1.8095238095238095, "no_speech_prob": 1.406391220371006e-05}, {"id": 437, "seek": 215388, "start": 2178.8, "end": 2179.8, "text": " of all those layers.", "tokens": [295, 439, 729, 7914, 13], "temperature": 0.0, "avg_logprob": -0.12001626387886379, "compression_ratio": 1.8095238095238095, "no_speech_prob": 1.406391220371006e-05}, {"id": 438, "seek": 217980, "start": 2179.8, "end": 2185.4, "text": " So that's a very nice thing, so now my forward is just self.layers.", "tokens": [407, 300, 311, 257, 588, 1481, 551, 11, 370, 586, 452, 2128, 307, 445, 2698, 13, 8376, 433, 13], "temperature": 0.0, "avg_logprob": -0.1733949425515164, "compression_ratio": 1.6180904522613064, "no_speech_prob": 8.664621418574825e-06}, {"id": 439, "seek": 217980, "start": 2185.4, "end": 2194.0, "text": " So here's a nice picture of how to make your architectures as simple as possible.", "tokens": [407, 510, 311, 257, 1481, 3036, 295, 577, 281, 652, 428, 6331, 1303, 382, 2199, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.1733949425515164, "compression_ratio": 1.6180904522613064, "no_speech_prob": 8.664621418574825e-06}, {"id": 440, "seek": 217980, "start": 2194.0, "end": 2199.7200000000003, "text": " So you can now go ahead and create this, and as I say, you could even parameterize this", "tokens": [407, 291, 393, 586, 352, 2286, 293, 1884, 341, 11, 293, 382, 286, 584, 11, 291, 727, 754, 13075, 1125, 341], "temperature": 0.0, "avg_logprob": -0.1733949425515164, "compression_ratio": 1.6180904522613064, "no_speech_prob": 8.664621418574825e-06}, {"id": 441, "seek": 217980, "start": 2199.7200000000003, "end": 2204.88, "text": " to make it a number that you pass in here, to pass in different numbers, so it's not", "tokens": [281, 652, 309, 257, 1230, 300, 291, 1320, 294, 510, 11, 281, 1320, 294, 819, 3547, 11, 370, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.1733949425515164, "compression_ratio": 1.6180904522613064, "no_speech_prob": 8.664621418574825e-06}, {"id": 442, "seek": 220488, "start": 2204.88, "end": 2210.28, "text": " 2, maybe it's times 2 instead, you could pass in things that change the kernel size or change", "tokens": [568, 11, 1310, 309, 311, 1413, 568, 2602, 11, 291, 727, 1320, 294, 721, 300, 1319, 264, 28256, 2744, 420, 1319], "temperature": 0.0, "avg_logprob": -0.15602786146749661, "compression_ratio": 1.6655290102389078, "no_speech_prob": 5.955100277788006e-06}, {"id": 443, "seek": 220488, "start": 2210.28, "end": 2214.92, "text": " the number of convolutional layers, fiddle around with it, and maybe you can create something,", "tokens": [264, 1230, 295, 45216, 304, 7914, 11, 24553, 2285, 926, 365, 309, 11, 293, 1310, 291, 393, 1884, 746, 11], "temperature": 0.0, "avg_logprob": -0.15602786146749661, "compression_ratio": 1.6655290102389078, "no_speech_prob": 5.955100277788006e-06}, {"id": 444, "seek": 220488, "start": 2214.92, "end": 2221.36, "text": " I've actually got a version of this which I'm about to run for you, which kind of implements", "tokens": [286, 600, 767, 658, 257, 3037, 295, 341, 597, 286, 478, 466, 281, 1190, 337, 291, 11, 597, 733, 295, 704, 17988], "temperature": 0.0, "avg_logprob": -0.15602786146749661, "compression_ratio": 1.6655290102389078, "no_speech_prob": 5.955100277788006e-06}, {"id": 445, "seek": 220488, "start": 2221.36, "end": 2226.48, "text": " all of the different parameters that's in that wide resnet paper, so I could fiddle", "tokens": [439, 295, 264, 819, 9834, 300, 311, 294, 300, 4874, 725, 7129, 3035, 11, 370, 286, 727, 24553, 2285], "temperature": 0.0, "avg_logprob": -0.15602786146749661, "compression_ratio": 1.6655290102389078, "no_speech_prob": 5.955100277788006e-06}, {"id": 446, "seek": 220488, "start": 2226.48, "end": 2229.2400000000002, "text": " around to see what worked well.", "tokens": [926, 281, 536, 437, 2732, 731, 13], "temperature": 0.0, "avg_logprob": -0.15602786146749661, "compression_ratio": 1.6655290102389078, "no_speech_prob": 5.955100277788006e-06}, {"id": 447, "seek": 220488, "start": 2229.2400000000002, "end": 2234.8, "text": " So once we've got that, we can use conv learner from model data to take our PyTorch model,", "tokens": [407, 1564, 321, 600, 658, 300, 11, 321, 393, 764, 3754, 33347, 490, 2316, 1412, 281, 747, 527, 9953, 51, 284, 339, 2316, 11], "temperature": 0.0, "avg_logprob": -0.15602786146749661, "compression_ratio": 1.6655290102389078, "no_speech_prob": 5.955100277788006e-06}, {"id": 448, "seek": 223480, "start": 2234.8, "end": 2241.1200000000003, "text": " module, and the model data object and turn them into a learner, give it a criterion,", "tokens": [10088, 11, 293, 264, 2316, 1412, 2657, 293, 1261, 552, 666, 257, 33347, 11, 976, 309, 257, 46691, 11], "temperature": 0.0, "avg_logprob": -0.22822239755213947, "compression_ratio": 1.5069767441860464, "no_speech_prob": 1.3631222827825695e-05}, {"id": 449, "seek": 223480, "start": 2241.1200000000003, "end": 2246.5600000000004, "text": " that's a metrics if we like, and then we can call fit and away we go.", "tokens": [300, 311, 257, 16367, 498, 321, 411, 11, 293, 550, 321, 393, 818, 3318, 293, 1314, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.22822239755213947, "compression_ratio": 1.5069767441860464, "no_speech_prob": 1.3631222827825695e-05}, {"id": 450, "seek": 223480, "start": 2246.5600000000004, "end": 2251.84, "text": " Could you please explain adaptive average pooling, how does setting to one work?", "tokens": [7497, 291, 1767, 2903, 27912, 4274, 7005, 278, 11, 577, 775, 3287, 281, 472, 589, 30], "temperature": 0.0, "avg_logprob": -0.22822239755213947, "compression_ratio": 1.5069767441860464, "no_speech_prob": 1.3631222827825695e-05}, {"id": 451, "seek": 223480, "start": 2251.84, "end": 2258.8, "text": " Sure, before I do, I just want to, since we've only got a certain amount of time in this", "tokens": [4894, 11, 949, 286, 360, 11, 286, 445, 528, 281, 11, 1670, 321, 600, 787, 658, 257, 1629, 2372, 295, 565, 294, 341], "temperature": 0.0, "avg_logprob": -0.22822239755213947, "compression_ratio": 1.5069767441860464, "no_speech_prob": 1.3631222827825695e-05}, {"id": 452, "seek": 225880, "start": 2258.8, "end": 2271.1800000000003, "text": " class, I wanted to see, I do want to see how we go, you know, with this simple network", "tokens": [1508, 11, 286, 1415, 281, 536, 11, 286, 360, 528, 281, 536, 577, 321, 352, 11, 291, 458, 11, 365, 341, 2199, 3209], "temperature": 0.0, "avg_logprob": -0.12177448687346085, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.1478650776552968e-05}, {"id": 453, "seek": 225880, "start": 2271.1800000000003, "end": 2274.0800000000004, "text": " against these state of the art results.", "tokens": [1970, 613, 1785, 295, 264, 1523, 3542, 13], "temperature": 0.0, "avg_logprob": -0.12177448687346085, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.1478650776552968e-05}, {"id": 454, "seek": 225880, "start": 2274.0800000000004, "end": 2279.28, "text": " So to make life a little easier, we can start it running now and see how it looks later.", "tokens": [407, 281, 652, 993, 257, 707, 3571, 11, 321, 393, 722, 309, 2614, 586, 293, 536, 577, 309, 1542, 1780, 13], "temperature": 0.0, "avg_logprob": -0.12177448687346085, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.1478650776552968e-05}, {"id": 455, "seek": 225880, "start": 2279.28, "end": 2282.7400000000002, "text": " So I've got the command ready to go.", "tokens": [407, 286, 600, 658, 264, 5622, 1919, 281, 352, 13], "temperature": 0.0, "avg_logprob": -0.12177448687346085, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.1478650776552968e-05}, {"id": 456, "seek": 225880, "start": 2282.7400000000002, "end": 2287.7400000000002, "text": " So we've basically taken all that stuff and put it into a simple little Python script,", "tokens": [407, 321, 600, 1936, 2726, 439, 300, 1507, 293, 829, 309, 666, 257, 2199, 707, 15329, 5755, 11], "temperature": 0.0, "avg_logprob": -0.12177448687346085, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.1478650776552968e-05}, {"id": 457, "seek": 228774, "start": 2287.74, "end": 2291.4399999999996, "text": " and I've modified some of those parameters I mentioned to create something I've called", "tokens": [293, 286, 600, 15873, 512, 295, 729, 9834, 286, 2835, 281, 1884, 746, 286, 600, 1219], "temperature": 0.0, "avg_logprob": -0.21481975180203797, "compression_ratio": 1.6124567474048443, "no_speech_prob": 3.2191375794354826e-05}, {"id": 458, "seek": 228774, "start": 2291.4399999999996, "end": 2296.68, "text": " a WRN22 network, which doesn't officially exist, but it's got a bunch of changes to", "tokens": [257, 44175, 45, 7490, 3209, 11, 597, 1177, 380, 12053, 2514, 11, 457, 309, 311, 658, 257, 3840, 295, 2962, 281], "temperature": 0.0, "avg_logprob": -0.21481975180203797, "compression_ratio": 1.6124567474048443, "no_speech_prob": 3.2191375794354826e-05}, {"id": 459, "seek": 228774, "start": 2296.68, "end": 2300.3199999999997, "text": " the parameters we talked about based on my experiments.", "tokens": [264, 9834, 321, 2825, 466, 2361, 322, 452, 12050, 13], "temperature": 0.0, "avg_logprob": -0.21481975180203797, "compression_ratio": 1.6124567474048443, "no_speech_prob": 3.2191375794354826e-05}, {"id": 460, "seek": 228774, "start": 2300.3199999999997, "end": 2304.9199999999996, "text": " We're going to use the new Leslie Smith one cycle thing.", "tokens": [492, 434, 516, 281, 764, 264, 777, 28140, 8538, 472, 6586, 551, 13], "temperature": 0.0, "avg_logprob": -0.21481975180203797, "compression_ratio": 1.6124567474048443, "no_speech_prob": 3.2191375794354826e-05}, {"id": 461, "seek": 228774, "start": 2304.9199999999996, "end": 2307.0, "text": " So there's quite a bunch of cool stuff here.", "tokens": [407, 456, 311, 1596, 257, 3840, 295, 1627, 1507, 510, 13], "temperature": 0.0, "avg_logprob": -0.21481975180203797, "compression_ratio": 1.6124567474048443, "no_speech_prob": 3.2191375794354826e-05}, {"id": 462, "seek": 228774, "start": 2307.0, "end": 2311.08, "text": " So the one cycle implementation was done by our student Sylvain Gugger, I think, I don't", "tokens": [407, 264, 472, 6586, 11420, 390, 1096, 538, 527, 3107, 3902, 14574, 491, 460, 697, 1321, 11, 286, 519, 11, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.21481975180203797, "compression_ratio": 1.6124567474048443, "no_speech_prob": 3.2191375794354826e-05}, {"id": 463, "seek": 228774, "start": 2311.08, "end": 2315.3999999999996, "text": " know how to pronounce his name exactly, Sylvain.", "tokens": [458, 577, 281, 19567, 702, 1315, 2293, 11, 3902, 14574, 491, 13], "temperature": 0.0, "avg_logprob": -0.21481975180203797, "compression_ratio": 1.6124567474048443, "no_speech_prob": 3.2191375794354826e-05}, {"id": 464, "seek": 231540, "start": 2315.4, "end": 2320.44, "text": " The trained Cypher experiments were largely done by Brett Kearns, and stuff like getting", "tokens": [440, 8895, 10295, 79, 511, 12050, 645, 11611, 1096, 538, 29447, 3189, 1083, 82, 11, 293, 1507, 411, 1242], "temperature": 0.0, "avg_logprob": -0.20153384961579976, "compression_ratio": 1.4698795180722892, "no_speech_prob": 3.0415628771152114e-06}, {"id": 465, "seek": 231540, "start": 2320.44, "end": 2327.36, "text": " the half-position floating point implementation integrated into Fast.ai was done by Andrew", "tokens": [264, 1922, 12, 38078, 12607, 935, 11420, 10919, 666, 15968, 13, 1301, 390, 1096, 538, 10110], "temperature": 0.0, "avg_logprob": -0.20153384961579976, "compression_ratio": 1.4698795180722892, "no_speech_prob": 3.0415628771152114e-06}, {"id": 466, "seek": 231540, "start": 2327.36, "end": 2328.8, "text": " Shaw.", "tokens": [27132, 13], "temperature": 0.0, "avg_logprob": -0.20153384961579976, "compression_ratio": 1.4698795180722892, "no_speech_prob": 3.0415628771152114e-06}, {"id": 467, "seek": 231540, "start": 2328.8, "end": 2333.12, "text": " So it's been a cool kind of bunch of different student projects coming together to allow", "tokens": [407, 309, 311, 668, 257, 1627, 733, 295, 3840, 295, 819, 3107, 4455, 1348, 1214, 281, 2089], "temperature": 0.0, "avg_logprob": -0.20153384961579976, "compression_ratio": 1.4698795180722892, "no_speech_prob": 3.0415628771152114e-06}, {"id": 468, "seek": 231540, "start": 2333.12, "end": 2335.2400000000002, "text": " us to run this.", "tokens": [505, 281, 1190, 341, 13], "temperature": 0.0, "avg_logprob": -0.20153384961579976, "compression_ratio": 1.4698795180722892, "no_speech_prob": 3.0415628771152114e-06}, {"id": 469, "seek": 231540, "start": 2335.2400000000002, "end": 2343.0, "text": " So this is going to run actually on a AWS, Amazon AWS P3, which has 8 GPUs.", "tokens": [407, 341, 307, 516, 281, 1190, 767, 322, 257, 17650, 11, 6795, 17650, 430, 18, 11, 597, 575, 1649, 18407, 82, 13], "temperature": 0.0, "avg_logprob": -0.20153384961579976, "compression_ratio": 1.4698795180722892, "no_speech_prob": 3.0415628771152114e-06}, {"id": 470, "seek": 234300, "start": 2343.0, "end": 2349.0, "text": " The P3 has these newer Volta architecture GPUs, which actually have special support", "tokens": [440, 430, 18, 575, 613, 17628, 8911, 1328, 9482, 18407, 82, 11, 597, 767, 362, 2121, 1406], "temperature": 0.0, "avg_logprob": -0.15977862606877866, "compression_ratio": 1.6788990825688073, "no_speech_prob": 3.844905222649686e-06}, {"id": 471, "seek": 234300, "start": 2349.0, "end": 2351.44, "text": " for half-precision floating point.", "tokens": [337, 1922, 12, 3712, 40832, 12607, 935, 13], "temperature": 0.0, "avg_logprob": -0.15977862606877866, "compression_ratio": 1.6788990825688073, "no_speech_prob": 3.844905222649686e-06}, {"id": 472, "seek": 234300, "start": 2351.44, "end": 2358.88, "text": " Fast.ai is the first library I know of to actually integrate the Volta optimized half-precision", "tokens": [15968, 13, 1301, 307, 264, 700, 6405, 286, 458, 295, 281, 767, 13365, 264, 8911, 1328, 26941, 1922, 12, 3712, 40832], "temperature": 0.0, "avg_logprob": -0.15977862606877866, "compression_ratio": 1.6788990825688073, "no_speech_prob": 3.844905222649686e-06}, {"id": 473, "seek": 234300, "start": 2358.88, "end": 2360.82, "text": " floating point into the library.", "tokens": [12607, 935, 666, 264, 6405, 13], "temperature": 0.0, "avg_logprob": -0.15977862606877866, "compression_ratio": 1.6788990825688073, "no_speech_prob": 3.844905222649686e-06}, {"id": 474, "seek": 234300, "start": 2360.82, "end": 2366.92, "text": " So we can just go learn.half now and get that support automatically.", "tokens": [407, 321, 393, 445, 352, 1466, 13, 25461, 586, 293, 483, 300, 1406, 6772, 13], "temperature": 0.0, "avg_logprob": -0.15977862606877866, "compression_ratio": 1.6788990825688073, "no_speech_prob": 3.844905222649686e-06}, {"id": 475, "seek": 234300, "start": 2366.92, "end": 2369.84, "text": " And is also the first one to integrate one cycle.", "tokens": [400, 307, 611, 264, 700, 472, 281, 13365, 472, 6586, 13], "temperature": 0.0, "avg_logprob": -0.15977862606877866, "compression_ratio": 1.6788990825688073, "no_speech_prob": 3.844905222649686e-06}, {"id": 476, "seek": 236984, "start": 2369.84, "end": 2373.54, "text": " So these are the parameters for the one cycle.", "tokens": [407, 613, 366, 264, 9834, 337, 264, 472, 6586, 13], "temperature": 0.0, "avg_logprob": -0.12176593552287827, "compression_ratio": 1.699604743083004, "no_speech_prob": 7.071817890391685e-06}, {"id": 477, "seek": 236984, "start": 2373.54, "end": 2376.1200000000003, "text": " So we can go ahead and get this running.", "tokens": [407, 321, 393, 352, 2286, 293, 483, 341, 2614, 13], "temperature": 0.0, "avg_logprob": -0.12176593552287827, "compression_ratio": 1.699604743083004, "no_speech_prob": 7.071817890391685e-06}, {"id": 478, "seek": 236984, "start": 2376.1200000000003, "end": 2382.48, "text": " So what this actually does is it's using PyTorch's multi-GPU support.", "tokens": [407, 437, 341, 767, 775, 307, 309, 311, 1228, 9953, 51, 284, 339, 311, 4825, 12, 38, 8115, 1406, 13], "temperature": 0.0, "avg_logprob": -0.12176593552287827, "compression_ratio": 1.699604743083004, "no_speech_prob": 7.071817890391685e-06}, {"id": 479, "seek": 236984, "start": 2382.48, "end": 2387.76, "text": " Since there are 8 GPUs, it's actually going to fire off 8 separate Python processes, and", "tokens": [4162, 456, 366, 1649, 18407, 82, 11, 309, 311, 767, 516, 281, 2610, 766, 1649, 4994, 15329, 7555, 11, 293], "temperature": 0.0, "avg_logprob": -0.12176593552287827, "compression_ratio": 1.699604743083004, "no_speech_prob": 7.071817890391685e-06}, {"id": 480, "seek": 236984, "start": 2387.76, "end": 2390.6400000000003, "text": " each one's going to train on a little bit.", "tokens": [1184, 472, 311, 516, 281, 3847, 322, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.12176593552287827, "compression_ratio": 1.699604743083004, "no_speech_prob": 7.071817890391685e-06}, {"id": 481, "seek": 236984, "start": 2390.6400000000003, "end": 2395.76, "text": " And then at the end, it's going to pass the gradient updates back to kind of the master", "tokens": [400, 550, 412, 264, 917, 11, 309, 311, 516, 281, 1320, 264, 16235, 9205, 646, 281, 733, 295, 264, 4505], "temperature": 0.0, "avg_logprob": -0.12176593552287827, "compression_ratio": 1.699604743083004, "no_speech_prob": 7.071817890391685e-06}, {"id": 482, "seek": 236984, "start": 2395.76, "end": 2399.52, "text": " process that's going to integrate them all together.", "tokens": [1399, 300, 311, 516, 281, 13365, 552, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.12176593552287827, "compression_ratio": 1.699604743083004, "no_speech_prob": 7.071817890391685e-06}, {"id": 483, "seek": 239952, "start": 2399.52, "end": 2406.8, "text": " So you'll see lots of progress bars all pop up together.", "tokens": [407, 291, 603, 536, 3195, 295, 4205, 10228, 439, 1665, 493, 1214, 13], "temperature": 0.0, "avg_logprob": -0.2095287557233844, "compression_ratio": 1.4344827586206896, "no_speech_prob": 2.078483703371603e-05}, {"id": 484, "seek": 239952, "start": 2406.8, "end": 2414.48, "text": " And you can see it's training 3 or 4 seconds when you do it this way.", "tokens": [400, 291, 393, 536, 309, 311, 3097, 805, 420, 1017, 3949, 562, 291, 360, 309, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.2095287557233844, "compression_ratio": 1.4344827586206896, "no_speech_prob": 2.078483703371603e-05}, {"id": 485, "seek": 239952, "start": 2414.48, "end": 2426.32, "text": " Where else when I was training earlier, I was getting about 30 seconds per epoch.", "tokens": [2305, 1646, 562, 286, 390, 3097, 3071, 11, 286, 390, 1242, 466, 2217, 3949, 680, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.2095287557233844, "compression_ratio": 1.4344827586206896, "no_speech_prob": 2.078483703371603e-05}, {"id": 486, "seek": 242632, "start": 2426.32, "end": 2432.6000000000004, "text": " So doing it this way, we can kind of train things 10 times faster or so, which is pretty", "tokens": [407, 884, 309, 341, 636, 11, 321, 393, 733, 295, 3847, 721, 1266, 1413, 4663, 420, 370, 11, 597, 307, 1238], "temperature": 0.0, "avg_logprob": -0.19765585119074042, "compression_ratio": 1.5073891625615763, "no_speech_prob": 5.25536870554788e-06}, {"id": 487, "seek": 242632, "start": 2432.6000000000004, "end": 2433.6000000000004, "text": " cool.", "tokens": [1627, 13], "temperature": 0.0, "avg_logprob": -0.19765585119074042, "compression_ratio": 1.5073891625615763, "no_speech_prob": 5.25536870554788e-06}, {"id": 488, "seek": 242632, "start": 2433.6000000000004, "end": 2435.92, "text": " Okay, so we'll leave that running.", "tokens": [1033, 11, 370, 321, 603, 1856, 300, 2614, 13], "temperature": 0.0, "avg_logprob": -0.19765585119074042, "compression_ratio": 1.5073891625615763, "no_speech_prob": 5.25536870554788e-06}, {"id": 489, "seek": 242632, "start": 2435.92, "end": 2440.44, "text": " So you were asking about adaptive average pooling, and I think specifically is what's", "tokens": [407, 291, 645, 3365, 466, 27912, 4274, 7005, 278, 11, 293, 286, 519, 4682, 307, 437, 311], "temperature": 0.0, "avg_logprob": -0.19765585119074042, "compression_ratio": 1.5073891625615763, "no_speech_prob": 5.25536870554788e-06}, {"id": 490, "seek": 242632, "start": 2440.44, "end": 2442.8, "text": " the number 1 doing?", "tokens": [264, 1230, 502, 884, 30], "temperature": 0.0, "avg_logprob": -0.19765585119074042, "compression_ratio": 1.5073891625615763, "no_speech_prob": 5.25536870554788e-06}, {"id": 491, "seek": 242632, "start": 2442.8, "end": 2456.0, "text": " So normally when we're doing average pooling, let's say we've got 4x4.", "tokens": [407, 5646, 562, 321, 434, 884, 4274, 7005, 278, 11, 718, 311, 584, 321, 600, 658, 1017, 87, 19, 13], "temperature": 0.0, "avg_logprob": -0.19765585119074042, "compression_ratio": 1.5073891625615763, "no_speech_prob": 5.25536870554788e-06}, {"id": 492, "seek": 245600, "start": 2456.0, "end": 2465.88, "text": " Let's say we did average pooling 2,2.", "tokens": [961, 311, 584, 321, 630, 4274, 7005, 278, 568, 11, 17, 13], "temperature": 0.0, "avg_logprob": -0.13724052387735117, "compression_ratio": 1.2252252252252251, "no_speech_prob": 4.6378604565688875e-06}, {"id": 493, "seek": 245600, "start": 2465.88, "end": 2476.18, "text": " Then that creates a 2x2 area and takes the average of those 4.", "tokens": [1396, 300, 7829, 257, 568, 87, 17, 1859, 293, 2516, 264, 4274, 295, 729, 1017, 13], "temperature": 0.0, "avg_logprob": -0.13724052387735117, "compression_ratio": 1.2252252252252251, "no_speech_prob": 4.6378604565688875e-06}, {"id": 494, "seek": 245600, "start": 2476.18, "end": 2482.08, "text": " And then we can pass in the stride.", "tokens": [400, 550, 321, 393, 1320, 294, 264, 1056, 482, 13], "temperature": 0.0, "avg_logprob": -0.13724052387735117, "compression_ratio": 1.2252252252252251, "no_speech_prob": 4.6378604565688875e-06}, {"id": 495, "seek": 248208, "start": 2482.08, "end": 2487.52, "text": " So if we said stride 1, then the next one is we look at this block of 2x2 and take that", "tokens": [407, 498, 321, 848, 1056, 482, 502, 11, 550, 264, 958, 472, 307, 321, 574, 412, 341, 3461, 295, 568, 87, 17, 293, 747, 300], "temperature": 0.0, "avg_logprob": -0.15244705446304813, "compression_ratio": 1.5604395604395604, "no_speech_prob": 1.7061789776562364e-06}, {"id": 496, "seek": 248208, "start": 2487.52, "end": 2489.7599999999998, "text": " average and so forth.", "tokens": [4274, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.15244705446304813, "compression_ratio": 1.5604395604395604, "no_speech_prob": 1.7061789776562364e-06}, {"id": 497, "seek": 248208, "start": 2489.7599999999998, "end": 2495.36, "text": " So that's like what a normal 2x2 average pooling would be.", "tokens": [407, 300, 311, 411, 437, 257, 2710, 568, 87, 17, 4274, 7005, 278, 576, 312, 13], "temperature": 0.0, "avg_logprob": -0.15244705446304813, "compression_ratio": 1.5604395604395604, "no_speech_prob": 1.7061789776562364e-06}, {"id": 498, "seek": 248208, "start": 2495.36, "end": 2502.68, "text": " And so in that case, if we didn't have any padding, that would spit out a 3x3, because", "tokens": [400, 370, 294, 300, 1389, 11, 498, 321, 994, 380, 362, 604, 39562, 11, 300, 576, 22127, 484, 257, 805, 87, 18, 11, 570], "temperature": 0.0, "avg_logprob": -0.15244705446304813, "compression_ratio": 1.5604395604395604, "no_speech_prob": 1.7061789776562364e-06}, {"id": 499, "seek": 248208, "start": 2502.68, "end": 2507.48, "text": " it's 2 here, 2 here, 2 here.", "tokens": [309, 311, 568, 510, 11, 568, 510, 11, 568, 510, 13], "temperature": 0.0, "avg_logprob": -0.15244705446304813, "compression_ratio": 1.5604395604395604, "no_speech_prob": 1.7061789776562364e-06}, {"id": 500, "seek": 250748, "start": 2507.48, "end": 2512.16, "text": " And if we added padding, we can make it 4x4.", "tokens": [400, 498, 321, 3869, 39562, 11, 321, 393, 652, 309, 1017, 87, 19, 13], "temperature": 0.0, "avg_logprob": -0.14064449983484606, "compression_ratio": 1.525, "no_speech_prob": 7.811477757968532e-07}, {"id": 501, "seek": 250748, "start": 2512.16, "end": 2518.6, "text": " So if we wanted to spit out something, we didn't want 3x3, what if we wanted 1x1?", "tokens": [407, 498, 321, 1415, 281, 22127, 484, 746, 11, 321, 994, 380, 528, 805, 87, 18, 11, 437, 498, 321, 1415, 502, 87, 16, 30], "temperature": 0.0, "avg_logprob": -0.14064449983484606, "compression_ratio": 1.525, "no_speech_prob": 7.811477757968532e-07}, {"id": 502, "seek": 250748, "start": 2518.6, "end": 2524.78, "text": " Then we could say average pool 4,4.", "tokens": [1396, 321, 727, 584, 4274, 7005, 1017, 11, 19, 13], "temperature": 0.0, "avg_logprob": -0.14064449983484606, "compression_ratio": 1.525, "no_speech_prob": 7.811477757968532e-07}, {"id": 503, "seek": 250748, "start": 2524.78, "end": 2533.04, "text": " And so that's going to do 4,4 and average the whole lot.", "tokens": [400, 370, 300, 311, 516, 281, 360, 1017, 11, 19, 293, 4274, 264, 1379, 688, 13], "temperature": 0.0, "avg_logprob": -0.14064449983484606, "compression_ratio": 1.525, "no_speech_prob": 7.811477757968532e-07}, {"id": 504, "seek": 250748, "start": 2533.04, "end": 2537.4, "text": " That would spit out 1x1.", "tokens": [663, 576, 22127, 484, 502, 87, 16, 13], "temperature": 0.0, "avg_logprob": -0.14064449983484606, "compression_ratio": 1.525, "no_speech_prob": 7.811477757968532e-07}, {"id": 505, "seek": 253740, "start": 2537.4, "end": 2539.52, "text": " So that's just one way to do it.", "tokens": [407, 300, 311, 445, 472, 636, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.1079000639683992, "compression_ratio": 1.8368421052631578, "no_speech_prob": 1.2098639672331046e-06}, {"id": 506, "seek": 253740, "start": 2539.52, "end": 2547.4, "text": " Rather than saying the size of the pooling filter, why don't we instead say, well I don't", "tokens": [16571, 813, 1566, 264, 2744, 295, 264, 7005, 278, 6608, 11, 983, 500, 380, 321, 2602, 584, 11, 731, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.1079000639683992, "compression_ratio": 1.8368421052631578, "no_speech_prob": 1.2098639672331046e-06}, {"id": 507, "seek": 253740, "start": 2547.4, "end": 2552.96, "text": " care what the size of the input grid is, I always want 1x1.", "tokens": [1127, 437, 264, 2744, 295, 264, 4846, 10748, 307, 11, 286, 1009, 528, 502, 87, 16, 13], "temperature": 0.0, "avg_logprob": -0.1079000639683992, "compression_ratio": 1.8368421052631578, "no_speech_prob": 1.2098639672331046e-06}, {"id": 508, "seek": 253740, "start": 2552.96, "end": 2560.04, "text": " So that's where then you say adaptive average pool.", "tokens": [407, 300, 311, 689, 550, 291, 584, 27912, 4274, 7005, 13], "temperature": 0.0, "avg_logprob": -0.1079000639683992, "compression_ratio": 1.8368421052631578, "no_speech_prob": 1.2098639672331046e-06}, {"id": 509, "seek": 253740, "start": 2560.04, "end": 2563.6, "text": " And now you don't say what's the size of the pooling filter, you instead say what's the", "tokens": [400, 586, 291, 500, 380, 584, 437, 311, 264, 2744, 295, 264, 7005, 278, 6608, 11, 291, 2602, 584, 437, 311, 264], "temperature": 0.0, "avg_logprob": -0.1079000639683992, "compression_ratio": 1.8368421052631578, "no_speech_prob": 1.2098639672331046e-06}, {"id": 510, "seek": 253740, "start": 2563.6, "end": 2565.48, "text": " size of the output I want.", "tokens": [2744, 295, 264, 5598, 286, 528, 13], "temperature": 0.0, "avg_logprob": -0.1079000639683992, "compression_ratio": 1.8368421052631578, "no_speech_prob": 1.2098639672331046e-06}, {"id": 511, "seek": 256548, "start": 2565.48, "end": 2568.2400000000002, "text": " And so I want something that's 1x1.", "tokens": [400, 370, 286, 528, 746, 300, 311, 502, 87, 16, 13], "temperature": 0.0, "avg_logprob": -0.12055267367446632, "compression_ratio": 1.63013698630137, "no_speech_prob": 1.2679263363679638e-06}, {"id": 512, "seek": 256548, "start": 2568.2400000000002, "end": 2572.2, "text": " And if you only put a single int, it assumes you mean 1x1.", "tokens": [400, 498, 291, 787, 829, 257, 2167, 560, 11, 309, 37808, 291, 914, 502, 87, 16, 13], "temperature": 0.0, "avg_logprob": -0.12055267367446632, "compression_ratio": 1.63013698630137, "no_speech_prob": 1.2679263363679638e-06}, {"id": 513, "seek": 256548, "start": 2572.2, "end": 2579.92, "text": " So in this case, adaptive average pooling 1 with a 4x4 grid coming in is the same as", "tokens": [407, 294, 341, 1389, 11, 27912, 4274, 7005, 278, 502, 365, 257, 1017, 87, 19, 10748, 1348, 294, 307, 264, 912, 382], "temperature": 0.0, "avg_logprob": -0.12055267367446632, "compression_ratio": 1.63013698630137, "no_speech_prob": 1.2679263363679638e-06}, {"id": 514, "seek": 256548, "start": 2579.92, "end": 2582.36, "text": " average pooling 4,4.", "tokens": [4274, 7005, 278, 1017, 11, 19, 13], "temperature": 0.0, "avg_logprob": -0.12055267367446632, "compression_ratio": 1.63013698630137, "no_speech_prob": 1.2679263363679638e-06}, {"id": 515, "seek": 256548, "start": 2582.36, "end": 2586.9, "text": " If it was a 7x7 grid coming in, it would be the same as 7,7.", "tokens": [759, 309, 390, 257, 1614, 87, 22, 10748, 1348, 294, 11, 309, 576, 312, 264, 912, 382, 1614, 11, 22, 13], "temperature": 0.0, "avg_logprob": -0.12055267367446632, "compression_ratio": 1.63013698630137, "no_speech_prob": 1.2679263363679638e-06}, {"id": 516, "seek": 256548, "start": 2586.9, "end": 2591.56, "text": " So it's the same operation, it's just expressing it in a way that says regardless of the input,", "tokens": [407, 309, 311, 264, 912, 6916, 11, 309, 311, 445, 22171, 309, 294, 257, 636, 300, 1619, 10060, 295, 264, 4846, 11], "temperature": 0.0, "avg_logprob": -0.12055267367446632, "compression_ratio": 1.63013698630137, "no_speech_prob": 1.2679263363679638e-06}, {"id": 517, "seek": 259156, "start": 2591.56, "end": 2606.12, "text": " I want something of that sized output.", "tokens": [286, 528, 746, 295, 300, 20004, 5598, 13], "temperature": 0.0, "avg_logprob": -0.188469797372818, "compression_ratio": 1.4431137724550898, "no_speech_prob": 8.939657163864467e-06}, {"id": 518, "seek": 259156, "start": 2606.12, "end": 2613.92, "text": " Well we got to 94, and it took 3 minutes and 11 seconds, and the previous state of the", "tokens": [1042, 321, 658, 281, 30849, 11, 293, 309, 1890, 805, 2077, 293, 2975, 3949, 11, 293, 264, 3894, 1785, 295, 264], "temperature": 0.0, "avg_logprob": -0.188469797372818, "compression_ratio": 1.4431137724550898, "no_speech_prob": 8.939657163864467e-06}, {"id": 519, "seek": 259156, "start": 2613.92, "end": 2617.4, "text": " art was 1 hour and 7 minutes.", "tokens": [1523, 390, 502, 1773, 293, 1614, 2077, 13], "temperature": 0.0, "avg_logprob": -0.188469797372818, "compression_ratio": 1.4431137724550898, "no_speech_prob": 8.939657163864467e-06}, {"id": 520, "seek": 259156, "start": 2617.4, "end": 2620.66, "text": " So was it worth fiddling around with those parameters and learning a little bit about", "tokens": [407, 390, 309, 3163, 283, 14273, 1688, 926, 365, 729, 9834, 293, 2539, 257, 707, 857, 466], "temperature": 0.0, "avg_logprob": -0.188469797372818, "compression_ratio": 1.4431137724550898, "no_speech_prob": 8.939657163864467e-06}, {"id": 521, "seek": 262066, "start": 2620.66, "end": 2624.7999999999997, "text": " how these architectures actually work and not just using what came out of the box?", "tokens": [577, 613, 6331, 1303, 767, 589, 293, 406, 445, 1228, 437, 1361, 484, 295, 264, 2424, 30], "temperature": 0.0, "avg_logprob": -0.18149962606309336, "compression_ratio": 1.5096153846153846, "no_speech_prob": 1.6187364963116124e-05}, {"id": 522, "seek": 262066, "start": 2624.7999999999997, "end": 2631.48, "text": " Well holy shit, we just used a publicly available instance, we used a spot instance, so that", "tokens": [1042, 10622, 4611, 11, 321, 445, 1143, 257, 14843, 2435, 5197, 11, 321, 1143, 257, 4008, 5197, 11, 370, 300], "temperature": 0.0, "avg_logprob": -0.18149962606309336, "compression_ratio": 1.5096153846153846, "no_speech_prob": 1.6187364963116124e-05}, {"id": 523, "seek": 262066, "start": 2631.48, "end": 2641.54, "text": " cost us like $8 per hour for 3 minutes, cost us a few cents to train this from scratch", "tokens": [2063, 505, 411, 1848, 23, 680, 1773, 337, 805, 2077, 11, 2063, 505, 257, 1326, 14941, 281, 3847, 341, 490, 8459], "temperature": 0.0, "avg_logprob": -0.18149962606309336, "compression_ratio": 1.5096153846153846, "no_speech_prob": 1.6187364963116124e-05}, {"id": 524, "seek": 262066, "start": 2641.54, "end": 2647.0, "text": " 20 times faster than anybody's ever done it before.", "tokens": [945, 1413, 4663, 813, 4472, 311, 1562, 1096, 309, 949, 13], "temperature": 0.0, "avg_logprob": -0.18149962606309336, "compression_ratio": 1.5096153846153846, "no_speech_prob": 1.6187364963116124e-05}, {"id": 525, "seek": 264700, "start": 2647.0, "end": 2650.88, "text": " So that's like the most crazy state of the art result I think we've ever seen, we've", "tokens": [407, 300, 311, 411, 264, 881, 3219, 1785, 295, 264, 1523, 1874, 286, 519, 321, 600, 1562, 1612, 11, 321, 600], "temperature": 0.0, "avg_logprob": -0.27928130571232285, "compression_ratio": 1.5348837209302326, "no_speech_prob": 3.3405208341719117e-06}, {"id": 526, "seek": 264700, "start": 2650.88, "end": 2654.52, "text": " seen many, but this one just blew it out of the water.", "tokens": [1612, 867, 11, 457, 341, 472, 445, 19075, 309, 484, 295, 264, 1281, 13], "temperature": 0.0, "avg_logprob": -0.27928130571232285, "compression_ratio": 1.5348837209302326, "no_speech_prob": 3.3405208341719117e-06}, {"id": 527, "seek": 264700, "start": 2654.52, "end": 2663.52, "text": " And so this is partly thanks to just fiddling around with those parameters of the architecture,", "tokens": [400, 370, 341, 307, 17031, 3231, 281, 445, 283, 14273, 1688, 926, 365, 729, 9834, 295, 264, 9482, 11], "temperature": 0.0, "avg_logprob": -0.27928130571232285, "compression_ratio": 1.5348837209302326, "no_speech_prob": 3.3405208341719117e-06}, {"id": 528, "seek": 264700, "start": 2663.52, "end": 2669.04, "text": " mainly frankly about using Leslie Smith's one cycle thing and Sobhav's implementation", "tokens": [8704, 11939, 466, 1228, 28140, 8538, 311, 472, 6586, 551, 293, 407, 65, 71, 706, 311, 11420], "temperature": 0.0, "avg_logprob": -0.27928130571232285, "compression_ratio": 1.5348837209302326, "no_speech_prob": 3.3405208341719117e-06}, {"id": 529, "seek": 264700, "start": 2669.04, "end": 2670.04, "text": " of that.", "tokens": [295, 300, 13], "temperature": 0.0, "avg_logprob": -0.27928130571232285, "compression_ratio": 1.5348837209302326, "no_speech_prob": 3.3405208341719117e-06}, {"id": 530, "seek": 267004, "start": 2670.04, "end": 2678.7799999999997, "text": " Remember, not only, so just to remind you of what that's doing, it's basically saying", "tokens": [5459, 11, 406, 787, 11, 370, 445, 281, 4160, 291, 295, 437, 300, 311, 884, 11, 309, 311, 1936, 1566], "temperature": 0.0, "avg_logprob": -0.20974414075007203, "compression_ratio": 1.4370860927152318, "no_speech_prob": 1.8738672906692955e-06}, {"id": 531, "seek": 267004, "start": 2678.7799999999997, "end": 2685.36, "text": " this is batches, and this is learning rate.", "tokens": [341, 307, 15245, 279, 11, 293, 341, 307, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.20974414075007203, "compression_ratio": 1.4370860927152318, "no_speech_prob": 1.8738672906692955e-06}, {"id": 532, "seek": 267004, "start": 2685.36, "end": 2694.6, "text": " It creates an upward path that's equally long as the downward path, so it's a true CLR,", "tokens": [467, 7829, 364, 23452, 3100, 300, 311, 12309, 938, 382, 264, 24805, 3100, 11, 370, 309, 311, 257, 2074, 12855, 49, 11], "temperature": 0.0, "avg_logprob": -0.20974414075007203, "compression_ratio": 1.4370860927152318, "no_speech_prob": 1.8738672906692955e-06}, {"id": 533, "seek": 269460, "start": 2694.6, "end": 2703.2799999999997, "text": " triangular cyclical learning rate, as per usual, you can pick the ratio between those", "tokens": [38190, 19474, 804, 2539, 3314, 11, 382, 680, 7713, 11, 291, 393, 1888, 264, 8509, 1296, 729], "temperature": 0.0, "avg_logprob": -0.13642039579503676, "compression_ratio": 1.6059113300492611, "no_speech_prob": 6.3391871663043275e-06}, {"id": 534, "seek": 269460, "start": 2703.2799999999997, "end": 2710.08, "text": " two numbers, so x divided by y in this case is a number that you get to pick, in this", "tokens": [732, 3547, 11, 370, 2031, 6666, 538, 288, 294, 341, 1389, 307, 257, 1230, 300, 291, 483, 281, 1888, 11, 294, 341], "temperature": 0.0, "avg_logprob": -0.13642039579503676, "compression_ratio": 1.6059113300492611, "no_speech_prob": 6.3391871663043275e-06}, {"id": 535, "seek": 269460, "start": 2710.08, "end": 2720.04, "text": " case we picked 50, so we started out with a much smaller one here.", "tokens": [1389, 321, 6183, 2625, 11, 370, 321, 1409, 484, 365, 257, 709, 4356, 472, 510, 13], "temperature": 0.0, "avg_logprob": -0.13642039579503676, "compression_ratio": 1.6059113300492611, "no_speech_prob": 6.3391871663043275e-06}, {"id": 536, "seek": 269460, "start": 2720.04, "end": 2724.08, "text": " And then it's got this cool idea which is you get to say what percentage of your epochs", "tokens": [400, 550, 309, 311, 658, 341, 1627, 1558, 597, 307, 291, 483, 281, 584, 437, 9668, 295, 428, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.13642039579503676, "compression_ratio": 1.6059113300492611, "no_speech_prob": 6.3391871663043275e-06}, {"id": 537, "seek": 272408, "start": 2724.08, "end": 2730.64, "text": " then is spent going from the bottom of this down all the way down pretty much to 0.", "tokens": [550, 307, 4418, 516, 490, 264, 2767, 295, 341, 760, 439, 264, 636, 760, 1238, 709, 281, 1958, 13], "temperature": 0.0, "avg_logprob": -0.13612726137235567, "compression_ratio": 1.5904255319148937, "no_speech_prob": 3.138128931823303e-06}, {"id": 538, "seek": 272408, "start": 2730.64, "end": 2738.04, "text": " And that's what this second number here is, so 15% of the batches are spent going from", "tokens": [400, 300, 311, 437, 341, 1150, 1230, 510, 307, 11, 370, 2119, 4, 295, 264, 15245, 279, 366, 4418, 516, 490], "temperature": 0.0, "avg_logprob": -0.13612726137235567, "compression_ratio": 1.5904255319148937, "no_speech_prob": 3.138128931823303e-06}, {"id": 539, "seek": 272408, "start": 2738.04, "end": 2742.7599999999998, "text": " the bottom of our triangle even further.", "tokens": [264, 2767, 295, 527, 13369, 754, 3052, 13], "temperature": 0.0, "avg_logprob": -0.13612726137235567, "compression_ratio": 1.5904255319148937, "no_speech_prob": 3.138128931823303e-06}, {"id": 540, "seek": 272408, "start": 2742.7599999999998, "end": 2751.0, "text": " So importantly though, that's not the only thing one cycle does, we also have momentum,", "tokens": [407, 8906, 1673, 11, 300, 311, 406, 264, 787, 551, 472, 6586, 775, 11, 321, 611, 362, 11244, 11], "temperature": 0.0, "avg_logprob": -0.13612726137235567, "compression_ratio": 1.5904255319148937, "no_speech_prob": 3.138128931823303e-06}, {"id": 541, "seek": 275100, "start": 2751.0, "end": 2765.48, "text": " and momentum goes from 0.95 to 0.85 like this.", "tokens": [293, 11244, 1709, 490, 1958, 13, 15718, 281, 1958, 13, 19287, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.11206196629723837, "compression_ratio": 1.6102564102564103, "no_speech_prob": 3.555964440238313e-06}, {"id": 542, "seek": 275100, "start": 2765.48, "end": 2770.16, "text": " In other words, when the learning rate is really low, we use a lot of momentum, and", "tokens": [682, 661, 2283, 11, 562, 264, 2539, 3314, 307, 534, 2295, 11, 321, 764, 257, 688, 295, 11244, 11, 293], "temperature": 0.0, "avg_logprob": -0.11206196629723837, "compression_ratio": 1.6102564102564103, "no_speech_prob": 3.555964440238313e-06}, {"id": 543, "seek": 275100, "start": 2770.16, "end": 2772.92, "text": " when the learning rate is really high, we use very little momentum, which makes a lot", "tokens": [562, 264, 2539, 3314, 307, 534, 1090, 11, 321, 764, 588, 707, 11244, 11, 597, 1669, 257, 688], "temperature": 0.0, "avg_logprob": -0.11206196629723837, "compression_ratio": 1.6102564102564103, "no_speech_prob": 3.555964440238313e-06}, {"id": 544, "seek": 275100, "start": 2772.92, "end": 2773.92, "text": " of sense.", "tokens": [295, 2020, 13], "temperature": 0.0, "avg_logprob": -0.11206196629723837, "compression_ratio": 1.6102564102564103, "no_speech_prob": 3.555964440238313e-06}, {"id": 545, "seek": 275100, "start": 2773.92, "end": 2780.04, "text": " But until Leslie Smith showed this in that paper, I've never seen anybody do it before,", "tokens": [583, 1826, 28140, 8538, 4712, 341, 294, 300, 3035, 11, 286, 600, 1128, 1612, 4472, 360, 309, 949, 11], "temperature": 0.0, "avg_logprob": -0.11206196629723837, "compression_ratio": 1.6102564102564103, "no_speech_prob": 3.555964440238313e-06}, {"id": 546, "seek": 278004, "start": 2780.04, "end": 2781.92, "text": " so it's a really cool trick.", "tokens": [370, 309, 311, 257, 534, 1627, 4282, 13], "temperature": 0.0, "avg_logprob": -0.16949968839946547, "compression_ratio": 1.4845814977973568, "no_speech_prob": 1.3631103684019763e-05}, {"id": 547, "seek": 278004, "start": 2781.92, "end": 2790.88, "text": " So you can now use that by using the useCLRBeta parameter in FastAI, and you should be able", "tokens": [407, 291, 393, 586, 764, 300, 538, 1228, 264, 764, 22458, 49, 33, 7664, 13075, 294, 15968, 48698, 11, 293, 291, 820, 312, 1075], "temperature": 0.0, "avg_logprob": -0.16949968839946547, "compression_ratio": 1.4845814977973568, "no_speech_prob": 1.3631103684019763e-05}, {"id": 548, "seek": 278004, "start": 2790.88, "end": 2794.44, "text": " to basically replicate this data-the-art result.", "tokens": [281, 1936, 25356, 341, 1412, 12, 3322, 12, 446, 1874, 13], "temperature": 0.0, "avg_logprob": -0.16949968839946547, "compression_ratio": 1.4845814977973568, "no_speech_prob": 1.3631103684019763e-05}, {"id": 549, "seek": 278004, "start": 2794.44, "end": 2798.12, "text": " You can use it on your own computer or your paper space, obviously the only thing you", "tokens": [509, 393, 764, 309, 322, 428, 1065, 3820, 420, 428, 3035, 1901, 11, 2745, 264, 787, 551, 291], "temperature": 0.0, "avg_logprob": -0.16949968839946547, "compression_ratio": 1.4845814977973568, "no_speech_prob": 1.3631103684019763e-05}, {"id": 550, "seek": 278004, "start": 2798.12, "end": 2803.62, "text": " won't get is the multi-GPU piece, but that makes it a bit easier to train anyway.", "tokens": [1582, 380, 483, 307, 264, 4825, 12, 38, 8115, 2522, 11, 457, 300, 1669, 309, 257, 857, 3571, 281, 3847, 4033, 13], "temperature": 0.0, "avg_logprob": -0.16949968839946547, "compression_ratio": 1.4845814977973568, "no_speech_prob": 1.3631103684019763e-05}, {"id": 551, "seek": 280362, "start": 2803.62, "end": 2812.0, "text": " So on a single GPU, you should be able to beat this on a single GPU.", "tokens": [407, 322, 257, 2167, 18407, 11, 291, 820, 312, 1075, 281, 4224, 341, 322, 257, 2167, 18407, 13], "temperature": 0.0, "avg_logprob": -0.2249043258195071, "compression_ratio": 1.5380952380952382, "no_speech_prob": 6.144126928120386e-06}, {"id": 552, "seek": 280362, "start": 2812.0, "end": 2813.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2249043258195071, "compression_ratio": 1.5380952380952382, "no_speech_prob": 6.144126928120386e-06}, {"id": 553, "seek": 280362, "start": 2813.0, "end": 2819.7599999999998, "text": " Make group layer contains stride equals 2, so this means stride is 1 for layer 1 and", "tokens": [4387, 1594, 4583, 8306, 1056, 482, 6915, 568, 11, 370, 341, 1355, 1056, 482, 307, 502, 337, 4583, 502, 293], "temperature": 0.0, "avg_logprob": -0.2249043258195071, "compression_ratio": 1.5380952380952382, "no_speech_prob": 6.144126928120386e-06}, {"id": 554, "seek": 280362, "start": 2819.7599999999998, "end": 2821.16, "text": " 2 for everything else.", "tokens": [568, 337, 1203, 1646, 13], "temperature": 0.0, "avg_logprob": -0.2249043258195071, "compression_ratio": 1.5380952380952382, "no_speech_prob": 6.144126928120386e-06}, {"id": 555, "seek": 280362, "start": 2821.16, "end": 2823.04, "text": " What's the logic behind it?", "tokens": [708, 311, 264, 9952, 2261, 309, 30], "temperature": 0.0, "avg_logprob": -0.2249043258195071, "compression_ratio": 1.5380952380952382, "no_speech_prob": 6.144126928120386e-06}, {"id": 556, "seek": 280362, "start": 2823.04, "end": 2826.4, "text": " Usually the strides I have seen are odd.", "tokens": [11419, 264, 1056, 1875, 286, 362, 1612, 366, 7401, 13], "temperature": 0.0, "avg_logprob": -0.2249043258195071, "compression_ratio": 1.5380952380952382, "no_speech_prob": 6.144126928120386e-06}, {"id": 557, "seek": 280362, "start": 2826.4, "end": 2832.4, "text": " No, strides are either 1 or 2, I think you're thinking of kernel sizes.", "tokens": [883, 11, 1056, 1875, 366, 2139, 502, 420, 568, 11, 286, 519, 291, 434, 1953, 295, 28256, 11602, 13], "temperature": 0.0, "avg_logprob": -0.2249043258195071, "compression_ratio": 1.5380952380952382, "no_speech_prob": 6.144126928120386e-06}, {"id": 558, "seek": 283240, "start": 2832.4, "end": 2838.52, "text": " So stride equals 2 means that I jump 2 across, and so stride of 2 means that you halve your", "tokens": [407, 1056, 482, 6915, 568, 1355, 300, 286, 3012, 568, 2108, 11, 293, 370, 1056, 482, 295, 568, 1355, 300, 291, 7523, 303, 428], "temperature": 0.0, "avg_logprob": -0.14587355468232752, "compression_ratio": 1.6995515695067265, "no_speech_prob": 8.13959377410356e-06}, {"id": 559, "seek": 283240, "start": 2838.52, "end": 2839.52, "text": " grid size.", "tokens": [10748, 2744, 13], "temperature": 0.0, "avg_logprob": -0.14587355468232752, "compression_ratio": 1.6995515695067265, "no_speech_prob": 8.13959377410356e-06}, {"id": 560, "seek": 283240, "start": 2839.52, "end": 2844.04, "text": " So I think you might have just got confused between stride and kernel size there.", "tokens": [407, 286, 519, 291, 1062, 362, 445, 658, 9019, 1296, 1056, 482, 293, 28256, 2744, 456, 13], "temperature": 0.0, "avg_logprob": -0.14587355468232752, "compression_ratio": 1.6995515695067265, "no_speech_prob": 8.13959377410356e-06}, {"id": 561, "seek": 283240, "start": 2844.04, "end": 2849.92, "text": " And so if we have a stride of 1, the grid size doesn't change.", "tokens": [400, 370, 498, 321, 362, 257, 1056, 482, 295, 502, 11, 264, 10748, 2744, 1177, 380, 1319, 13], "temperature": 0.0, "avg_logprob": -0.14587355468232752, "compression_ratio": 1.6995515695067265, "no_speech_prob": 8.13959377410356e-06}, {"id": 562, "seek": 283240, "start": 2849.92, "end": 2854.1600000000003, "text": " If we have a stride of 2, then it does.", "tokens": [759, 321, 362, 257, 1056, 482, 295, 568, 11, 550, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.14587355468232752, "compression_ratio": 1.6995515695067265, "no_speech_prob": 8.13959377410356e-06}, {"id": 563, "seek": 283240, "start": 2854.1600000000003, "end": 2860.88, "text": " And so in this case, because this is for CIFAR10, 32x32 is small, and we don't get to halve", "tokens": [400, 370, 294, 341, 1389, 11, 570, 341, 307, 337, 383, 12775, 1899, 3279, 11, 8858, 87, 11440, 307, 1359, 11, 293, 321, 500, 380, 483, 281, 7523, 303], "temperature": 0.0, "avg_logprob": -0.14587355468232752, "compression_ratio": 1.6995515695067265, "no_speech_prob": 8.13959377410356e-06}, {"id": 564, "seek": 286088, "start": 2860.88, "end": 2865.8, "text": " the grid size very often, because pretty quickly we're going to run out of cells.", "tokens": [264, 10748, 2744, 588, 2049, 11, 570, 1238, 2661, 321, 434, 516, 281, 1190, 484, 295, 5438, 13], "temperature": 0.0, "avg_logprob": -0.1295962756193137, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.8162181731895544e-06}, {"id": 565, "seek": 286088, "start": 2865.8, "end": 2875.96, "text": " And so that's why the first layer has a stride of 1, so we don't decrease the grid size straight", "tokens": [400, 370, 300, 311, 983, 264, 700, 4583, 575, 257, 1056, 482, 295, 502, 11, 370, 321, 500, 380, 11514, 264, 10748, 2744, 2997], "temperature": 0.0, "avg_logprob": -0.1295962756193137, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.8162181731895544e-06}, {"id": 566, "seek": 286088, "start": 2875.96, "end": 2878.96, "text": " away basically.", "tokens": [1314, 1936, 13], "temperature": 0.0, "avg_logprob": -0.1295962756193137, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.8162181731895544e-06}, {"id": 567, "seek": 286088, "start": 2878.96, "end": 2884.08, "text": " And it's kind of a nice way of doing it because that's why we kind of have a low number here,", "tokens": [400, 309, 311, 733, 295, 257, 1481, 636, 295, 884, 309, 570, 300, 311, 983, 321, 733, 295, 362, 257, 2295, 1230, 510, 11], "temperature": 0.0, "avg_logprob": -0.1295962756193137, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.8162181731895544e-06}, {"id": 568, "seek": 288408, "start": 2884.08, "end": 2891.56, "text": " so we can start out with not too much computation on the big grid, and then we can gradually", "tokens": [370, 321, 393, 722, 484, 365, 406, 886, 709, 24903, 322, 264, 955, 10748, 11, 293, 550, 321, 393, 13145], "temperature": 0.0, "avg_logprob": -0.09827722311019897, "compression_ratio": 1.675531914893617, "no_speech_prob": 6.540411959576886e-06}, {"id": 569, "seek": 288408, "start": 2891.56, "end": 2895.7999999999997, "text": " do more and more computation as the grids get smaller and smaller.", "tokens": [360, 544, 293, 544, 24903, 382, 264, 677, 3742, 483, 4356, 293, 4356, 13], "temperature": 0.0, "avg_logprob": -0.09827722311019897, "compression_ratio": 1.675531914893617, "no_speech_prob": 6.540411959576886e-06}, {"id": 570, "seek": 288408, "start": 2895.7999999999997, "end": 2907.36, "text": " Because the smaller grid, the computation will take less time.", "tokens": [1436, 264, 4356, 10748, 11, 264, 24903, 486, 747, 1570, 565, 13], "temperature": 0.0, "avg_logprob": -0.09827722311019897, "compression_ratio": 1.675531914893617, "no_speech_prob": 6.540411959576886e-06}, {"id": 571, "seek": 288408, "start": 2907.36, "end": 2912.92, "text": " So I think so that we can do all of our ganning in one go, let's take a slightly early break", "tokens": [407, 286, 519, 370, 300, 321, 393, 360, 439, 295, 527, 7574, 773, 294, 472, 352, 11, 718, 311, 747, 257, 4748, 2440, 1821], "temperature": 0.0, "avg_logprob": -0.09827722311019897, "compression_ratio": 1.675531914893617, "no_speech_prob": 6.540411959576886e-06}, {"id": 572, "seek": 291292, "start": 2912.92, "end": 2930.44, "text": " and come back at 7.30.", "tokens": [293, 808, 646, 412, 1614, 13, 3446, 13], "temperature": 0.0, "avg_logprob": -0.1206153498755561, "compression_ratio": 1.0505050505050506, "no_speech_prob": 6.048811428627232e-06}, {"id": 573, "seek": 291292, "start": 2930.44, "end": 2936.76, "text": " So we're going to talk about Generative Adversarial Networks, also known as GANs.", "tokens": [407, 321, 434, 516, 281, 751, 466, 15409, 1166, 1999, 840, 44745, 12640, 82, 11, 611, 2570, 382, 460, 1770, 82, 13], "temperature": 0.0, "avg_logprob": -0.1206153498755561, "compression_ratio": 1.0505050505050506, "no_speech_prob": 6.048811428627232e-06}, {"id": 574, "seek": 293676, "start": 2936.76, "end": 2943.0800000000004, "text": " And specifically we're going to focus on the Wasserstein GAN paper, which included some", "tokens": [400, 4682, 321, 434, 516, 281, 1879, 322, 264, 17351, 9089, 460, 1770, 3035, 11, 597, 5556, 512], "temperature": 0.0, "avg_logprob": -0.19869464567337913, "compression_ratio": 1.5324074074074074, "no_speech_prob": 2.111160210915841e-05}, {"id": 575, "seek": 293676, "start": 2943.0800000000004, "end": 2950.88, "text": " guy called Sumit Chintala who went on to create some piece of software called Hightorch.", "tokens": [2146, 1219, 8626, 270, 761, 686, 5159, 567, 1437, 322, 281, 1884, 512, 2522, 295, 4722, 1219, 389, 397, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.19869464567337913, "compression_ratio": 1.5324074074074074, "no_speech_prob": 2.111160210915841e-05}, {"id": 576, "seek": 293676, "start": 2950.88, "end": 2959.32, "text": " The Wasserstein GAN was heavily influenced by the DC GAN, or Deep Convolutional Generative", "tokens": [440, 17351, 9089, 460, 1770, 390, 10950, 15269, 538, 264, 9114, 460, 1770, 11, 420, 14895, 2656, 85, 3386, 304, 15409, 1166], "temperature": 0.0, "avg_logprob": -0.19869464567337913, "compression_ratio": 1.5324074074074074, "no_speech_prob": 2.111160210915841e-05}, {"id": 577, "seek": 293676, "start": 2959.32, "end": 2965.82, "text": " Adversarial Networks paper, which also Sumit was involved with.", "tokens": [1999, 840, 44745, 12640, 82, 3035, 11, 597, 611, 8626, 270, 390, 3288, 365, 13], "temperature": 0.0, "avg_logprob": -0.19869464567337913, "compression_ratio": 1.5324074074074074, "no_speech_prob": 2.111160210915841e-05}, {"id": 578, "seek": 296582, "start": 2965.82, "end": 2973.56, "text": " So it's a really interesting paper to read.", "tokens": [407, 309, 311, 257, 534, 1880, 3035, 281, 1401, 13], "temperature": 0.0, "avg_logprob": -0.15576912273060192, "compression_ratio": 1.4253731343283582, "no_speech_prob": 2.684168521227548e-06}, {"id": 579, "seek": 296582, "start": 2973.56, "end": 2979.28, "text": " A lot of it looks like this.", "tokens": [316, 688, 295, 309, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.15576912273060192, "compression_ratio": 1.4253731343283582, "no_speech_prob": 2.684168521227548e-06}, {"id": 580, "seek": 296582, "start": 2979.28, "end": 2985.4, "text": " The good news is you can skip those bits, because there's also a bit that looks like", "tokens": [440, 665, 2583, 307, 291, 393, 10023, 729, 9239, 11, 570, 456, 311, 611, 257, 857, 300, 1542, 411], "temperature": 0.0, "avg_logprob": -0.15576912273060192, "compression_ratio": 1.4253731343283582, "no_speech_prob": 2.684168521227548e-06}, {"id": 581, "seek": 296582, "start": 2985.4, "end": 2990.4, "text": " this which says, do these things.", "tokens": [341, 597, 1619, 11, 360, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.15576912273060192, "compression_ratio": 1.4253731343283582, "no_speech_prob": 2.684168521227548e-06}, {"id": 582, "seek": 299040, "start": 2990.4, "end": 2996.6800000000003, "text": " Now I will say though that a lot of papers have a theoretical section which seems to", "tokens": [823, 286, 486, 584, 1673, 300, 257, 688, 295, 10577, 362, 257, 20864, 3541, 597, 2544, 281], "temperature": 0.0, "avg_logprob": -0.1382864251428721, "compression_ratio": 1.5625, "no_speech_prob": 4.637833626475185e-06}, {"id": 583, "seek": 299040, "start": 2996.6800000000003, "end": 3003.28, "text": " be there entirely to get past the reviewers' need for theory.", "tokens": [312, 456, 7696, 281, 483, 1791, 264, 45837, 6, 643, 337, 5261, 13], "temperature": 0.0, "avg_logprob": -0.1382864251428721, "compression_ratio": 1.5625, "no_speech_prob": 4.637833626475185e-06}, {"id": 584, "seek": 299040, "start": 3003.28, "end": 3004.96, "text": " That's not true of the WGAN paper.", "tokens": [663, 311, 406, 2074, 295, 264, 343, 27699, 3035, 13], "temperature": 0.0, "avg_logprob": -0.1382864251428721, "compression_ratio": 1.5625, "no_speech_prob": 4.637833626475185e-06}, {"id": 585, "seek": 299040, "start": 3004.96, "end": 3007.88, "text": " The theory bit is actually really interesting.", "tokens": [440, 5261, 857, 307, 767, 534, 1880, 13], "temperature": 0.0, "avg_logprob": -0.1382864251428721, "compression_ratio": 1.5625, "no_speech_prob": 4.637833626475185e-06}, {"id": 586, "seek": 299040, "start": 3007.88, "end": 3014.36, "text": " You don't need to know it to use it, but if you want to learn about some cool ideas and", "tokens": [509, 500, 380, 643, 281, 458, 309, 281, 764, 309, 11, 457, 498, 291, 528, 281, 1466, 466, 512, 1627, 3487, 293], "temperature": 0.0, "avg_logprob": -0.1382864251428721, "compression_ratio": 1.5625, "no_speech_prob": 4.637833626475185e-06}, {"id": 587, "seek": 299040, "start": 3014.36, "end": 3019.96, "text": " see the thinking behind why this particular algorithm, it's absolutely fascinating.", "tokens": [536, 264, 1953, 2261, 983, 341, 1729, 9284, 11, 309, 311, 3122, 10343, 13], "temperature": 0.0, "avg_logprob": -0.1382864251428721, "compression_ratio": 1.5625, "no_speech_prob": 4.637833626475185e-06}, {"id": 588, "seek": 301996, "start": 3019.96, "end": 3025.88, "text": " And almost nobody before this paper came out, I didn't know, literally I knew nobody who", "tokens": [400, 1920, 5079, 949, 341, 3035, 1361, 484, 11, 286, 994, 380, 458, 11, 3736, 286, 2586, 5079, 567], "temperature": 0.0, "avg_logprob": -0.15693436622619628, "compression_ratio": 1.6276150627615062, "no_speech_prob": 1.6442372725578025e-05}, {"id": 589, "seek": 301996, "start": 3025.88, "end": 3028.8, "text": " had studied the math that it's based on.", "tokens": [632, 9454, 264, 5221, 300, 309, 311, 2361, 322, 13], "temperature": 0.0, "avg_logprob": -0.15693436622619628, "compression_ratio": 1.6276150627615062, "no_speech_prob": 1.6442372725578025e-05}, {"id": 590, "seek": 301996, "start": 3028.8, "end": 3032.12, "text": " So like everybody had to learn the math it was based on.", "tokens": [407, 411, 2201, 632, 281, 1466, 264, 5221, 309, 390, 2361, 322, 13], "temperature": 0.0, "avg_logprob": -0.15693436622619628, "compression_ratio": 1.6276150627615062, "no_speech_prob": 1.6442372725578025e-05}, {"id": 591, "seek": 301996, "start": 3032.12, "end": 3035.7200000000003, "text": " And so the paper does a pretty good job of laying out all the pieces.", "tokens": [400, 370, 264, 3035, 775, 257, 1238, 665, 1691, 295, 14903, 484, 439, 264, 3755, 13], "temperature": 0.0, "avg_logprob": -0.15693436622619628, "compression_ratio": 1.6276150627615062, "no_speech_prob": 1.6442372725578025e-05}, {"id": 592, "seek": 301996, "start": 3035.7200000000003, "end": 3037.48, "text": " You'll have to do a bunch of reading yourself.", "tokens": [509, 603, 362, 281, 360, 257, 3840, 295, 3760, 1803, 13], "temperature": 0.0, "avg_logprob": -0.15693436622619628, "compression_ratio": 1.6276150627615062, "no_speech_prob": 1.6442372725578025e-05}, {"id": 593, "seek": 301996, "start": 3037.48, "end": 3045.48, "text": " So if you're interested in digging into the deeper math behind some paper to see what", "tokens": [407, 498, 291, 434, 3102, 294, 17343, 666, 264, 7731, 5221, 2261, 512, 3035, 281, 536, 437], "temperature": 0.0, "avg_logprob": -0.15693436622619628, "compression_ratio": 1.6276150627615062, "no_speech_prob": 1.6442372725578025e-05}, {"id": 594, "seek": 304548, "start": 3045.48, "end": 3051.2400000000002, "text": " it's like to study it, I would pick this one, because at the end of that theory section", "tokens": [309, 311, 411, 281, 2979, 309, 11, 286, 576, 1888, 341, 472, 11, 570, 412, 264, 917, 295, 300, 5261, 3541], "temperature": 0.0, "avg_logprob": -0.14065371718362113, "compression_ratio": 1.7630522088353413, "no_speech_prob": 7.07185654391651e-06}, {"id": 595, "seek": 304548, "start": 3051.2400000000002, "end": 3061.16, "text": " you'll come away saying, okay, I can see now why they made this algorithm the way it is.", "tokens": [291, 603, 808, 1314, 1566, 11, 1392, 11, 286, 393, 536, 586, 983, 436, 1027, 341, 9284, 264, 636, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.14065371718362113, "compression_ratio": 1.7630522088353413, "no_speech_prob": 7.07185654391651e-06}, {"id": 596, "seek": 304548, "start": 3061.16, "end": 3065.28, "text": " And then having come up with that idea, the other thing is often these theoretical sections", "tokens": [400, 550, 1419, 808, 493, 365, 300, 1558, 11, 264, 661, 551, 307, 2049, 613, 20864, 10863], "temperature": 0.0, "avg_logprob": -0.14065371718362113, "compression_ratio": 1.7630522088353413, "no_speech_prob": 7.07185654391651e-06}, {"id": 597, "seek": 304548, "start": 3065.28, "end": 3067.8, "text": " are very clearly added after they come up with the algorithm.", "tokens": [366, 588, 4448, 3869, 934, 436, 808, 493, 365, 264, 9284, 13], "temperature": 0.0, "avg_logprob": -0.14065371718362113, "compression_ratio": 1.7630522088353413, "no_speech_prob": 7.07185654391651e-06}, {"id": 598, "seek": 304548, "start": 3067.8, "end": 3072.2, "text": " They'll come up with the algorithm based on intuition and experiments and then later on", "tokens": [814, 603, 808, 493, 365, 264, 9284, 2361, 322, 24002, 293, 12050, 293, 550, 1780, 322], "temperature": 0.0, "avg_logprob": -0.14065371718362113, "compression_ratio": 1.7630522088353413, "no_speech_prob": 7.07185654391651e-06}, {"id": 599, "seek": 304548, "start": 3072.2, "end": 3073.72, "text": " post hoc justify it.", "tokens": [2183, 16708, 20833, 309, 13], "temperature": 0.0, "avg_logprob": -0.14065371718362113, "compression_ratio": 1.7630522088353413, "no_speech_prob": 7.07185654391651e-06}, {"id": 600, "seek": 307372, "start": 3073.72, "end": 3078.68, "text": " Whereas this one you can clearly see it's like, okay, let's actually think about what's", "tokens": [13813, 341, 472, 291, 393, 4448, 536, 309, 311, 411, 11, 1392, 11, 718, 311, 767, 519, 466, 437, 311], "temperature": 0.0, "avg_logprob": -0.13550495315384078, "compression_ratio": 1.6485148514851484, "no_speech_prob": 1.3497000281859073e-06}, {"id": 601, "seek": 307372, "start": 3078.68, "end": 3084.3199999999997, "text": " going on in GANs and think about what they need to do and then come up with the algorithm.", "tokens": [516, 322, 294, 460, 1770, 82, 293, 519, 466, 437, 436, 643, 281, 360, 293, 550, 808, 493, 365, 264, 9284, 13], "temperature": 0.0, "avg_logprob": -0.13550495315384078, "compression_ratio": 1.6485148514851484, "no_speech_prob": 1.3497000281859073e-06}, {"id": 602, "seek": 307372, "start": 3084.3199999999997, "end": 3090.3199999999997, "text": " So the basic idea of a GAN is it's a generative model.", "tokens": [407, 264, 3875, 1558, 295, 257, 460, 1770, 307, 309, 311, 257, 1337, 1166, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13550495315384078, "compression_ratio": 1.6485148514851484, "no_speech_prob": 1.3497000281859073e-06}, {"id": 603, "seek": 307372, "start": 3090.3199999999997, "end": 3097.56, "text": " So it's something that is going to create sentences or create images, it's going to", "tokens": [407, 309, 311, 746, 300, 307, 516, 281, 1884, 16579, 420, 1884, 5267, 11, 309, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.13550495315384078, "compression_ratio": 1.6485148514851484, "no_speech_prob": 1.3497000281859073e-06}, {"id": 604, "seek": 307372, "start": 3097.56, "end": 3101.04, "text": " generate stuff.", "tokens": [8460, 1507, 13], "temperature": 0.0, "avg_logprob": -0.13550495315384078, "compression_ratio": 1.6485148514851484, "no_speech_prob": 1.3497000281859073e-06}, {"id": 605, "seek": 310104, "start": 3101.04, "end": 3107.92, "text": " And it's going to try and create stuff which is very hard to tell the difference between", "tokens": [400, 309, 311, 516, 281, 853, 293, 1884, 1507, 597, 307, 588, 1152, 281, 980, 264, 2649, 1296], "temperature": 0.0, "avg_logprob": -0.15044709193853684, "compression_ratio": 1.6127450980392157, "no_speech_prob": 3.785304897974129e-06}, {"id": 606, "seek": 310104, "start": 3107.92, "end": 3110.72, "text": " generated stuff and real stuff.", "tokens": [10833, 1507, 293, 957, 1507, 13], "temperature": 0.0, "avg_logprob": -0.15044709193853684, "compression_ratio": 1.6127450980392157, "no_speech_prob": 3.785304897974129e-06}, {"id": 607, "seek": 310104, "start": 3110.72, "end": 3118.64, "text": " So a generative model could be used to face swap a video, a very well-known controversial", "tokens": [407, 257, 1337, 1166, 2316, 727, 312, 1143, 281, 1851, 18135, 257, 960, 11, 257, 588, 731, 12, 6861, 17323], "temperature": 0.0, "avg_logprob": -0.15044709193853684, "compression_ratio": 1.6127450980392157, "no_speech_prob": 3.785304897974129e-06}, {"id": 608, "seek": 310104, "start": 3118.64, "end": 3123.68, "text": " thing of deep fakes and fake pornography and stuff happening at the moment.", "tokens": [551, 295, 2452, 283, 3419, 293, 7592, 49936, 293, 1507, 2737, 412, 264, 1623, 13], "temperature": 0.0, "avg_logprob": -0.15044709193853684, "compression_ratio": 1.6127450980392157, "no_speech_prob": 3.785304897974129e-06}, {"id": 609, "seek": 310104, "start": 3123.68, "end": 3129.44, "text": " It could be used to fake somebody's voice.", "tokens": [467, 727, 312, 1143, 281, 7592, 2618, 311, 3177, 13], "temperature": 0.0, "avg_logprob": -0.15044709193853684, "compression_ratio": 1.6127450980392157, "no_speech_prob": 3.785304897974129e-06}, {"id": 610, "seek": 312944, "start": 3129.44, "end": 3133.92, "text": " It could be used to fake the answer to a medical question.", "tokens": [467, 727, 312, 1143, 281, 7592, 264, 1867, 281, 257, 4625, 1168, 13], "temperature": 0.0, "avg_logprob": -0.13171085679387473, "compression_ratio": 1.7606382978723405, "no_speech_prob": 1.5779562545503723e-06}, {"id": 611, "seek": 312944, "start": 3133.92, "end": 3135.64, "text": " But in that case, it's not really a fake.", "tokens": [583, 294, 300, 1389, 11, 309, 311, 406, 534, 257, 7592, 13], "temperature": 0.0, "avg_logprob": -0.13171085679387473, "compression_ratio": 1.7606382978723405, "no_speech_prob": 1.5779562545503723e-06}, {"id": 612, "seek": 312944, "start": 3135.64, "end": 3140.36, "text": " It could be a generative answer to a medical question that's actually a good answer.", "tokens": [467, 727, 312, 257, 1337, 1166, 1867, 281, 257, 4625, 1168, 300, 311, 767, 257, 665, 1867, 13], "temperature": 0.0, "avg_logprob": -0.13171085679387473, "compression_ratio": 1.7606382978723405, "no_speech_prob": 1.5779562545503723e-06}, {"id": 613, "seek": 312944, "start": 3140.36, "end": 3149.12, "text": " So you're generating language, you could generate a caption to an image, for example.", "tokens": [407, 291, 434, 17746, 2856, 11, 291, 727, 8460, 257, 31974, 281, 364, 3256, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.13171085679387473, "compression_ratio": 1.7606382978723405, "no_speech_prob": 1.5779562545503723e-06}, {"id": 614, "seek": 312944, "start": 3149.12, "end": 3155.96, "text": " So generative models have lots of interesting applications.", "tokens": [407, 1337, 1166, 5245, 362, 3195, 295, 1880, 5821, 13], "temperature": 0.0, "avg_logprob": -0.13171085679387473, "compression_ratio": 1.7606382978723405, "no_speech_prob": 1.5779562545503723e-06}, {"id": 615, "seek": 315596, "start": 3155.96, "end": 3162.0, "text": " But generally speaking, they need to be good enough that for example, if you're using it", "tokens": [583, 5101, 4124, 11, 436, 643, 281, 312, 665, 1547, 300, 337, 1365, 11, 498, 291, 434, 1228, 309], "temperature": 0.0, "avg_logprob": -0.15500348409016926, "compression_ratio": 1.743083003952569, "no_speech_prob": 9.223391316481866e-06}, {"id": 616, "seek": 315596, "start": 3162.0, "end": 3169.0, "text": " to automatically create a news scene for Carrie Fisher in the next Star Wars movie, since", "tokens": [281, 6772, 1884, 257, 2583, 4145, 337, 34654, 26676, 294, 264, 958, 5705, 9818, 3169, 11, 1670], "temperature": 0.0, "avg_logprob": -0.15500348409016926, "compression_ratio": 1.743083003952569, "no_speech_prob": 9.223391316481866e-06}, {"id": 617, "seek": 315596, "start": 3169.0, "end": 3174.6, "text": " she's not around to play that part anymore, you want to try and generate an image of her", "tokens": [750, 311, 406, 926, 281, 862, 300, 644, 3602, 11, 291, 528, 281, 853, 293, 8460, 364, 3256, 295, 720], "temperature": 0.0, "avg_logprob": -0.15500348409016926, "compression_ratio": 1.743083003952569, "no_speech_prob": 9.223391316481866e-06}, {"id": 618, "seek": 315596, "start": 3174.6, "end": 3180.16, "text": " that looks the same, then it has to fool the Star Wars audience into thinking, okay, that", "tokens": [300, 1542, 264, 912, 11, 550, 309, 575, 281, 7979, 264, 5705, 9818, 4034, 666, 1953, 11, 1392, 11, 300], "temperature": 0.0, "avg_logprob": -0.15500348409016926, "compression_ratio": 1.743083003952569, "no_speech_prob": 9.223391316481866e-06}, {"id": 619, "seek": 315596, "start": 3180.16, "end": 3185.44, "text": " doesn't look like some weird Carrie Fisher, that looks like the real Carrie Fisher.", "tokens": [1177, 380, 574, 411, 512, 3657, 34654, 26676, 11, 300, 1542, 411, 264, 957, 34654, 26676, 13], "temperature": 0.0, "avg_logprob": -0.15500348409016926, "compression_ratio": 1.743083003952569, "no_speech_prob": 9.223391316481866e-06}, {"id": 620, "seek": 318544, "start": 3185.44, "end": 3189.96, "text": " Or if you're trying to generate an answer to a medical question, you want to generate", "tokens": [1610, 498, 291, 434, 1382, 281, 8460, 364, 1867, 281, 257, 4625, 1168, 11, 291, 528, 281, 8460], "temperature": 0.0, "avg_logprob": -0.13425317061574835, "compression_ratio": 1.7577092511013215, "no_speech_prob": 3.5559658044803655e-06}, {"id": 621, "seek": 318544, "start": 3189.96, "end": 3196.84, "text": " English that reads nicely and clearly and sounds authoritative and meaningful.", "tokens": [3669, 300, 15700, 9594, 293, 4448, 293, 3263, 3793, 14275, 293, 10995, 13], "temperature": 0.0, "avg_logprob": -0.13425317061574835, "compression_ratio": 1.7577092511013215, "no_speech_prob": 3.5559658044803655e-06}, {"id": 622, "seek": 318544, "start": 3196.84, "end": 3203.76, "text": " So the idea of a generative adversarial network is we're going to create not just a generative", "tokens": [407, 264, 1558, 295, 257, 1337, 1166, 17641, 44745, 3209, 307, 321, 434, 516, 281, 1884, 406, 445, 257, 1337, 1166], "temperature": 0.0, "avg_logprob": -0.13425317061574835, "compression_ratio": 1.7577092511013215, "no_speech_prob": 3.5559658044803655e-06}, {"id": 623, "seek": 318544, "start": 3203.76, "end": 3211.56, "text": " model to create, say, the generated image, but a second model that's going to try to", "tokens": [2316, 281, 1884, 11, 584, 11, 264, 10833, 3256, 11, 457, 257, 1150, 2316, 300, 311, 516, 281, 853, 281], "temperature": 0.0, "avg_logprob": -0.13425317061574835, "compression_ratio": 1.7577092511013215, "no_speech_prob": 3.5559658044803655e-06}, {"id": 624, "seek": 318544, "start": 3211.56, "end": 3214.84, "text": " pick which ones are real and which ones are generated.", "tokens": [1888, 597, 2306, 366, 957, 293, 597, 2306, 366, 10833, 13], "temperature": 0.0, "avg_logprob": -0.13425317061574835, "compression_ratio": 1.7577092511013215, "no_speech_prob": 3.5559658044803655e-06}, {"id": 625, "seek": 321484, "start": 3214.84, "end": 3217.6400000000003, "text": " We're going to call them fake.", "tokens": [492, 434, 516, 281, 818, 552, 7592, 13], "temperature": 0.0, "avg_logprob": -0.16198399861653645, "compression_ratio": 2.23943661971831, "no_speech_prob": 9.666061487223487e-06}, {"id": 626, "seek": 321484, "start": 3217.6400000000003, "end": 3219.0, "text": " So which ones are real and which ones are fake.", "tokens": [407, 597, 2306, 366, 957, 293, 597, 2306, 366, 7592, 13], "temperature": 0.0, "avg_logprob": -0.16198399861653645, "compression_ratio": 2.23943661971831, "no_speech_prob": 9.666061487223487e-06}, {"id": 627, "seek": 321484, "start": 3219.0, "end": 3225.6800000000003, "text": " So we've got a generator that's going to create our fake content and a discriminator that's", "tokens": [407, 321, 600, 658, 257, 19265, 300, 311, 516, 281, 1884, 527, 7592, 2701, 293, 257, 20828, 1639, 300, 311], "temperature": 0.0, "avg_logprob": -0.16198399861653645, "compression_ratio": 2.23943661971831, "no_speech_prob": 9.666061487223487e-06}, {"id": 628, "seek": 321484, "start": 3225.6800000000003, "end": 3229.84, "text": " going to try to get good at recognizing which ones are real and which ones are fake.", "tokens": [516, 281, 853, 281, 483, 665, 412, 18538, 597, 2306, 366, 957, 293, 597, 2306, 366, 7592, 13], "temperature": 0.0, "avg_logprob": -0.16198399861653645, "compression_ratio": 2.23943661971831, "no_speech_prob": 9.666061487223487e-06}, {"id": 629, "seek": 321484, "start": 3229.84, "end": 3232.44, "text": " So there's going to be two models.", "tokens": [407, 456, 311, 516, 281, 312, 732, 5245, 13], "temperature": 0.0, "avg_logprob": -0.16198399861653645, "compression_ratio": 2.23943661971831, "no_speech_prob": 9.666061487223487e-06}, {"id": 630, "seek": 321484, "start": 3232.44, "end": 3236.1600000000003, "text": " And then there's going to be adversarial, meaning the generator is going to try to keep", "tokens": [400, 550, 456, 311, 516, 281, 312, 17641, 44745, 11, 3620, 264, 19265, 307, 516, 281, 853, 281, 1066], "temperature": 0.0, "avg_logprob": -0.16198399861653645, "compression_ratio": 2.23943661971831, "no_speech_prob": 9.666061487223487e-06}, {"id": 631, "seek": 321484, "start": 3236.1600000000003, "end": 3241.6800000000003, "text": " getting better at fooling the discriminator into thinking that fake is real, and the discriminator", "tokens": [1242, 1101, 412, 7979, 278, 264, 20828, 1639, 666, 1953, 300, 7592, 307, 957, 11, 293, 264, 20828, 1639], "temperature": 0.0, "avg_logprob": -0.16198399861653645, "compression_ratio": 2.23943661971831, "no_speech_prob": 9.666061487223487e-06}, {"id": 632, "seek": 324168, "start": 3241.68, "end": 3245.9199999999996, "text": " is going to try to keep getting better at discriminating between the real and the fake.", "tokens": [307, 516, 281, 853, 281, 1066, 1242, 1101, 412, 20828, 990, 1296, 264, 957, 293, 264, 7592, 13], "temperature": 0.0, "avg_logprob": -0.14947312565173132, "compression_ratio": 1.7732793522267207, "no_speech_prob": 4.222816187393619e-06}, {"id": 633, "seek": 324168, "start": 3245.9199999999996, "end": 3249.64, "text": " And they're going to go head-to-head like that.", "tokens": [400, 436, 434, 516, 281, 352, 1378, 12, 1353, 12, 1934, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.14947312565173132, "compression_ratio": 1.7732793522267207, "no_speech_prob": 4.222816187393619e-06}, {"id": 634, "seek": 324168, "start": 3249.64, "end": 3255.08, "text": " And it's basically as easy as I just described.", "tokens": [400, 309, 311, 1936, 382, 1858, 382, 286, 445, 7619, 13], "temperature": 0.0, "avg_logprob": -0.14947312565173132, "compression_ratio": 1.7732793522267207, "no_speech_prob": 4.222816187393619e-06}, {"id": 635, "seek": 324168, "start": 3255.08, "end": 3256.08, "text": " It really is.", "tokens": [467, 534, 307, 13], "temperature": 0.0, "avg_logprob": -0.14947312565173132, "compression_ratio": 1.7732793522267207, "no_speech_prob": 4.222816187393619e-06}, {"id": 636, "seek": 324168, "start": 3256.08, "end": 3258.72, "text": " We're just going to build two models in PyTorch.", "tokens": [492, 434, 445, 516, 281, 1322, 732, 5245, 294, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.14947312565173132, "compression_ratio": 1.7732793522267207, "no_speech_prob": 4.222816187393619e-06}, {"id": 637, "seek": 324168, "start": 3258.72, "end": 3263.16, "text": " We're going to create a training loop that first of all says the loss function for the", "tokens": [492, 434, 516, 281, 1884, 257, 3097, 6367, 300, 700, 295, 439, 1619, 264, 4470, 2445, 337, 264], "temperature": 0.0, "avg_logprob": -0.14947312565173132, "compression_ratio": 1.7732793522267207, "no_speech_prob": 4.222816187393619e-06}, {"id": 638, "seek": 324168, "start": 3263.16, "end": 3266.9199999999996, "text": " discriminator is can you tell the difference between real and fake, and then update the", "tokens": [20828, 1639, 307, 393, 291, 980, 264, 2649, 1296, 957, 293, 7592, 11, 293, 550, 5623, 264], "temperature": 0.0, "avg_logprob": -0.14947312565173132, "compression_ratio": 1.7732793522267207, "no_speech_prob": 4.222816187393619e-06}, {"id": 639, "seek": 324168, "start": 3266.9199999999996, "end": 3268.16, "text": " weights of that.", "tokens": [17443, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.14947312565173132, "compression_ratio": 1.7732793522267207, "no_speech_prob": 4.222816187393619e-06}, {"id": 640, "seek": 326816, "start": 3268.16, "end": 3271.68, "text": " And then we're going to create a loss function for the generator which is going to say can", "tokens": [400, 550, 321, 434, 516, 281, 1884, 257, 4470, 2445, 337, 264, 19265, 597, 307, 516, 281, 584, 393], "temperature": 0.0, "avg_logprob": -0.10718384697323753, "compression_ratio": 1.715481171548117, "no_speech_prob": 1.2098639672331046e-06}, {"id": 641, "seek": 326816, "start": 3271.68, "end": 3277.04, "text": " you generate something which pulls the discriminator and update the weights from that loss.", "tokens": [291, 8460, 746, 597, 16982, 264, 20828, 1639, 293, 5623, 264, 17443, 490, 300, 4470, 13], "temperature": 0.0, "avg_logprob": -0.10718384697323753, "compression_ratio": 1.715481171548117, "no_speech_prob": 1.2098639672331046e-06}, {"id": 642, "seek": 326816, "start": 3277.04, "end": 3280.8799999999997, "text": " And we're going to loop through that a few times and see what happens.", "tokens": [400, 321, 434, 516, 281, 6367, 807, 300, 257, 1326, 1413, 293, 536, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.10718384697323753, "compression_ratio": 1.715481171548117, "no_speech_prob": 1.2098639672331046e-06}, {"id": 643, "seek": 326816, "start": 3280.8799999999997, "end": 3287.7999999999997, "text": " And so let's come back to the pseudocode here of the algorithm and let's read the real code", "tokens": [400, 370, 718, 311, 808, 646, 281, 264, 25505, 532, 905, 1429, 510, 295, 264, 9284, 293, 718, 311, 1401, 264, 957, 3089], "temperature": 0.0, "avg_logprob": -0.10718384697323753, "compression_ratio": 1.715481171548117, "no_speech_prob": 1.2098639672331046e-06}, {"id": 644, "seek": 326816, "start": 3287.7999999999997, "end": 3292.6, "text": " first.", "tokens": [700, 13], "temperature": 0.0, "avg_logprob": -0.10718384697323753, "compression_ratio": 1.715481171548117, "no_speech_prob": 1.2098639672331046e-06}, {"id": 645, "seek": 326816, "start": 3292.6, "end": 3296.8399999999997, "text": " So there's lots of different things you can do with GANs.", "tokens": [407, 456, 311, 3195, 295, 819, 721, 291, 393, 360, 365, 460, 1770, 82, 13], "temperature": 0.0, "avg_logprob": -0.10718384697323753, "compression_ratio": 1.715481171548117, "no_speech_prob": 1.2098639672331046e-06}, {"id": 646, "seek": 329684, "start": 3296.84, "end": 3302.28, "text": " And we're going to do something that's kind of boring but easy to understand and it's", "tokens": [400, 321, 434, 516, 281, 360, 746, 300, 311, 733, 295, 9989, 457, 1858, 281, 1223, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.15639336128545, "compression_ratio": 1.8383458646616542, "no_speech_prob": 6.854279945400776e-06}, {"id": 647, "seek": 329684, "start": 3302.28, "end": 3304.1200000000003, "text": " kind of cool that it's even possible.", "tokens": [733, 295, 1627, 300, 309, 311, 754, 1944, 13], "temperature": 0.0, "avg_logprob": -0.15639336128545, "compression_ratio": 1.8383458646616542, "no_speech_prob": 6.854279945400776e-06}, {"id": 648, "seek": 329684, "start": 3304.1200000000003, "end": 3307.88, "text": " Which is we're just going to generate some pictures from nothing.", "tokens": [3013, 307, 321, 434, 445, 516, 281, 8460, 512, 5242, 490, 1825, 13], "temperature": 0.0, "avg_logprob": -0.15639336128545, "compression_ratio": 1.8383458646616542, "no_speech_prob": 6.854279945400776e-06}, {"id": 649, "seek": 329684, "start": 3307.88, "end": 3311.02, "text": " We're just going to get it to draw some pictures.", "tokens": [492, 434, 445, 516, 281, 483, 309, 281, 2642, 512, 5242, 13], "temperature": 0.0, "avg_logprob": -0.15639336128545, "compression_ratio": 1.8383458646616542, "no_speech_prob": 6.854279945400776e-06}, {"id": 650, "seek": 329684, "start": 3311.02, "end": 3315.44, "text": " And specifically we're going to get it to draw pictures of bedrooms.", "tokens": [400, 4682, 321, 434, 516, 281, 483, 309, 281, 2642, 5242, 295, 39955, 13], "temperature": 0.0, "avg_logprob": -0.15639336128545, "compression_ratio": 1.8383458646616542, "no_speech_prob": 6.854279945400776e-06}, {"id": 651, "seek": 329684, "start": 3315.44, "end": 3319.6800000000003, "text": " You'll find if you hopefully get a chance to play around with this during the week with", "tokens": [509, 603, 915, 498, 291, 4696, 483, 257, 2931, 281, 862, 926, 365, 341, 1830, 264, 1243, 365], "temperature": 0.0, "avg_logprob": -0.15639336128545, "compression_ratio": 1.8383458646616542, "no_speech_prob": 6.854279945400776e-06}, {"id": 652, "seek": 329684, "start": 3319.6800000000003, "end": 3326.6400000000003, "text": " your own datasets, if you pick a dataset that's very varied like ImageNet and then get a GAN", "tokens": [428, 1065, 42856, 11, 498, 291, 1888, 257, 28872, 300, 311, 588, 22877, 411, 29903, 31890, 293, 550, 483, 257, 460, 1770], "temperature": 0.0, "avg_logprob": -0.15639336128545, "compression_ratio": 1.8383458646616542, "no_speech_prob": 6.854279945400776e-06}, {"id": 653, "seek": 332664, "start": 3326.64, "end": 3332.7599999999998, "text": " to try and create ImageNet pictures, it tends not to do so well because it's not really", "tokens": [281, 853, 293, 1884, 29903, 31890, 5242, 11, 309, 12258, 406, 281, 360, 370, 731, 570, 309, 311, 406, 534], "temperature": 0.0, "avg_logprob": -0.14167064030965168, "compression_ratio": 1.6555555555555554, "no_speech_prob": 4.936915502185002e-06}, {"id": 654, "seek": 332664, "start": 3332.7599999999998, "end": 3335.68, "text": " clear enough what you want a picture of.", "tokens": [1850, 1547, 437, 291, 528, 257, 3036, 295, 13], "temperature": 0.0, "avg_logprob": -0.14167064030965168, "compression_ratio": 1.6555555555555554, "no_speech_prob": 4.936915502185002e-06}, {"id": 655, "seek": 332664, "start": 3335.68, "end": 3340.44, "text": " So it's better to give it, for example, there's a dataset called CelebA which is pictures", "tokens": [407, 309, 311, 1101, 281, 976, 309, 11, 337, 1365, 11, 456, 311, 257, 28872, 1219, 8257, 28512, 32, 597, 307, 5242], "temperature": 0.0, "avg_logprob": -0.14167064030965168, "compression_ratio": 1.6555555555555554, "no_speech_prob": 4.936915502185002e-06}, {"id": 656, "seek": 332664, "start": 3340.44, "end": 3342.2799999999997, "text": " of celebrities' faces.", "tokens": [295, 23200, 6, 8475, 13], "temperature": 0.0, "avg_logprob": -0.14167064030965168, "compression_ratio": 1.6555555555555554, "no_speech_prob": 4.936915502185002e-06}, {"id": 657, "seek": 332664, "start": 3342.2799999999997, "end": 3343.56, "text": " That works great with GANs.", "tokens": [663, 1985, 869, 365, 460, 1770, 82, 13], "temperature": 0.0, "avg_logprob": -0.14167064030965168, "compression_ratio": 1.6555555555555554, "no_speech_prob": 4.936915502185002e-06}, {"id": 658, "seek": 332664, "start": 3343.56, "end": 3348.08, "text": " You create really clear celebrity faces that don't actually exist.", "tokens": [509, 1884, 534, 1850, 18597, 8475, 300, 500, 380, 767, 2514, 13], "temperature": 0.0, "avg_logprob": -0.14167064030965168, "compression_ratio": 1.6555555555555554, "no_speech_prob": 4.936915502185002e-06}, {"id": 659, "seek": 332664, "start": 3348.08, "end": 3353.08, "text": " The bedroom dataset, also a good one, lots of pictures of the same kind of thing.", "tokens": [440, 11211, 28872, 11, 611, 257, 665, 472, 11, 3195, 295, 5242, 295, 264, 912, 733, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.14167064030965168, "compression_ratio": 1.6555555555555554, "no_speech_prob": 4.936915502185002e-06}, {"id": 660, "seek": 332664, "start": 3353.08, "end": 3355.7, "text": " So that's just a suggestion.", "tokens": [407, 300, 311, 445, 257, 16541, 13], "temperature": 0.0, "avg_logprob": -0.14167064030965168, "compression_ratio": 1.6555555555555554, "no_speech_prob": 4.936915502185002e-06}, {"id": 661, "seek": 335570, "start": 3355.7, "end": 3360.6, "text": " So there's something called the Lsun scene classification dataset.", "tokens": [407, 456, 311, 746, 1219, 264, 441, 11314, 4145, 21538, 28872, 13], "temperature": 0.0, "avg_logprob": -0.15103093253241645, "compression_ratio": 1.5550660792951543, "no_speech_prob": 2.857299705283367e-06}, {"id": 662, "seek": 335570, "start": 3360.6, "end": 3366.64, "text": " You can download it using these steps.", "tokens": [509, 393, 5484, 309, 1228, 613, 4439, 13], "temperature": 0.0, "avg_logprob": -0.15103093253241645, "compression_ratio": 1.5550660792951543, "no_speech_prob": 2.857299705283367e-06}, {"id": 663, "seek": 335570, "start": 3366.64, "end": 3368.24, "text": " It's pretty huge.", "tokens": [467, 311, 1238, 2603, 13], "temperature": 0.0, "avg_logprob": -0.15103093253241645, "compression_ratio": 1.5550660792951543, "no_speech_prob": 2.857299705283367e-06}, {"id": 664, "seek": 335570, "start": 3368.24, "end": 3373.14, "text": " So I've actually created a Kaggle dataset of a 20% sample.", "tokens": [407, 286, 600, 767, 2942, 257, 48751, 22631, 28872, 295, 257, 945, 4, 6889, 13], "temperature": 0.0, "avg_logprob": -0.15103093253241645, "compression_ratio": 1.5550660792951543, "no_speech_prob": 2.857299705283367e-06}, {"id": 665, "seek": 335570, "start": 3373.14, "end": 3378.2799999999997, "text": " So unless you're really excited about generating bedroom images, you might prefer to grab the", "tokens": [407, 5969, 291, 434, 534, 2919, 466, 17746, 11211, 5267, 11, 291, 1062, 4382, 281, 4444, 264], "temperature": 0.0, "avg_logprob": -0.15103093253241645, "compression_ratio": 1.5550660792951543, "no_speech_prob": 2.857299705283367e-06}, {"id": 666, "seek": 335570, "start": 3378.2799999999997, "end": 3380.98, "text": " 20% sample.", "tokens": [945, 4, 6889, 13], "temperature": 0.0, "avg_logprob": -0.15103093253241645, "compression_ratio": 1.5550660792951543, "no_speech_prob": 2.857299705283367e-06}, {"id": 667, "seek": 335570, "start": 3380.98, "end": 3385.46, "text": " So then we do the normal steps of creating some different paths.", "tokens": [407, 550, 321, 360, 264, 2710, 4439, 295, 4084, 512, 819, 14518, 13], "temperature": 0.0, "avg_logprob": -0.15103093253241645, "compression_ratio": 1.5550660792951543, "no_speech_prob": 2.857299705283367e-06}, {"id": 668, "seek": 338546, "start": 3385.46, "end": 3391.58, "text": " And in this case, as we did before, I find it much easier to kind of go the CSV route", "tokens": [400, 294, 341, 1389, 11, 382, 321, 630, 949, 11, 286, 915, 309, 709, 3571, 281, 733, 295, 352, 264, 48814, 7955], "temperature": 0.0, "avg_logprob": -0.16353922949896918, "compression_ratio": 1.5675675675675675, "no_speech_prob": 5.093659183330601e-06}, {"id": 669, "seek": 338546, "start": 3391.58, "end": 3394.38, "text": " when it comes to handling our data.", "tokens": [562, 309, 1487, 281, 13175, 527, 1412, 13], "temperature": 0.0, "avg_logprob": -0.16353922949896918, "compression_ratio": 1.5675675675675675, "no_speech_prob": 5.093659183330601e-06}, {"id": 670, "seek": 338546, "start": 3394.38, "end": 3401.84, "text": " So I just generate a CSV with the list of files that we want and a fake label of 0 because", "tokens": [407, 286, 445, 8460, 257, 48814, 365, 264, 1329, 295, 7098, 300, 321, 528, 293, 257, 7592, 7645, 295, 1958, 570], "temperature": 0.0, "avg_logprob": -0.16353922949896918, "compression_ratio": 1.5675675675675675, "no_speech_prob": 5.093659183330601e-06}, {"id": 671, "seek": 338546, "start": 3401.84, "end": 3405.12, "text": " we don't really have labels for these at all.", "tokens": [321, 500, 380, 534, 362, 16949, 337, 613, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.16353922949896918, "compression_ratio": 1.5675675675675675, "no_speech_prob": 5.093659183330601e-06}, {"id": 672, "seek": 338546, "start": 3405.12, "end": 3411.76, "text": " And I actually create two CSV files, one that contains everything in that bedroom dataset", "tokens": [400, 286, 767, 1884, 732, 48814, 7098, 11, 472, 300, 8306, 1203, 294, 300, 11211, 28872], "temperature": 0.0, "avg_logprob": -0.16353922949896918, "compression_ratio": 1.5675675675675675, "no_speech_prob": 5.093659183330601e-06}, {"id": 673, "seek": 341176, "start": 3411.76, "end": 3416.0, "text": " and one that just contains a random 10%.", "tokens": [293, 472, 300, 445, 8306, 257, 4974, 1266, 6856], "temperature": 0.0, "avg_logprob": -0.1649855427120043, "compression_ratio": 1.5594713656387664, "no_speech_prob": 1.4144706028673681e-06}, {"id": 674, "seek": 341176, "start": 3416.0, "end": 3422.88, "text": " It's just nice to do that because then I can most of the time use the sample when I'm experimenting.", "tokens": [467, 311, 445, 1481, 281, 360, 300, 570, 550, 286, 393, 881, 295, 264, 565, 764, 264, 6889, 562, 286, 478, 29070, 13], "temperature": 0.0, "avg_logprob": -0.1649855427120043, "compression_ratio": 1.5594713656387664, "no_speech_prob": 1.4144706028673681e-06}, {"id": 675, "seek": 341176, "start": 3422.88, "end": 3431.5, "text": " Because there's well over a million files, even just reading in the list takes a while.", "tokens": [1436, 456, 311, 731, 670, 257, 2459, 7098, 11, 754, 445, 3760, 294, 264, 1329, 2516, 257, 1339, 13], "temperature": 0.0, "avg_logprob": -0.1649855427120043, "compression_ratio": 1.5594713656387664, "no_speech_prob": 1.4144706028673681e-06}, {"id": 676, "seek": 341176, "start": 3431.5, "end": 3433.9, "text": " So this will look pretty familiar.", "tokens": [407, 341, 486, 574, 1238, 4963, 13], "temperature": 0.0, "avg_logprob": -0.1649855427120043, "compression_ratio": 1.5594713656387664, "no_speech_prob": 1.4144706028673681e-06}, {"id": 677, "seek": 341176, "start": 3433.9, "end": 3434.9, "text": " So here's a conv block.", "tokens": [407, 510, 311, 257, 416, 85, 3461, 13], "temperature": 0.0, "avg_logprob": -0.1649855427120043, "compression_ratio": 1.5594713656387664, "no_speech_prob": 1.4144706028673681e-06}, {"id": 678, "seek": 341176, "start": 3434.9, "end": 3440.34, "text": " This is before I realized that sequential models are much better.", "tokens": [639, 307, 949, 286, 5334, 300, 42881, 5245, 366, 709, 1101, 13], "temperature": 0.0, "avg_logprob": -0.1649855427120043, "compression_ratio": 1.5594713656387664, "no_speech_prob": 1.4144706028673681e-06}, {"id": 679, "seek": 344034, "start": 3440.34, "end": 3444.38, "text": " So if you compare this to my previous conv block with a sequential model, there's just", "tokens": [407, 498, 291, 6794, 341, 281, 452, 3894, 416, 85, 3461, 365, 257, 42881, 2316, 11, 456, 311, 445], "temperature": 0.0, "avg_logprob": -0.24056201757386672, "compression_ratio": 1.5634517766497462, "no_speech_prob": 5.955087544862181e-06}, {"id": 680, "seek": 344034, "start": 3444.38, "end": 3447.94, "text": " a lot more lines of code here.", "tokens": [257, 688, 544, 3876, 295, 3089, 510, 13], "temperature": 0.0, "avg_logprob": -0.24056201757386672, "compression_ratio": 1.5634517766497462, "no_speech_prob": 5.955087544862181e-06}, {"id": 681, "seek": 344034, "start": 3447.94, "end": 3456.48, "text": " But it does the same thing of doing conv value batch norm.", "tokens": [583, 309, 775, 264, 912, 551, 295, 884, 416, 85, 2158, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.24056201757386672, "compression_ratio": 1.5634517766497462, "no_speech_prob": 5.955087544862181e-06}, {"id": 682, "seek": 344034, "start": 3456.48, "end": 3459.2200000000003, "text": " And we calculate our padding, and here's the bias-false.", "tokens": [400, 321, 8873, 527, 39562, 11, 293, 510, 311, 264, 12577, 12, 36474, 405, 13], "temperature": 0.0, "avg_logprob": -0.24056201757386672, "compression_ratio": 1.5634517766497462, "no_speech_prob": 5.955087544862181e-06}, {"id": 683, "seek": 344034, "start": 3459.2200000000003, "end": 3468.5, "text": " So this is the same as before, basically, but with a little bit more code.", "tokens": [407, 341, 307, 264, 912, 382, 949, 11, 1936, 11, 457, 365, 257, 707, 857, 544, 3089, 13], "temperature": 0.0, "avg_logprob": -0.24056201757386672, "compression_ratio": 1.5634517766497462, "no_speech_prob": 5.955087544862181e-06}, {"id": 684, "seek": 346850, "start": 3468.5, "end": 3472.06, "text": " So the first thing we're going to do is we're going to build a discriminator.", "tokens": [407, 264, 700, 551, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 1322, 257, 20828, 1639, 13], "temperature": 0.0, "avg_logprob": -0.1486055374145508, "compression_ratio": 1.7511961722488039, "no_speech_prob": 6.748033229087014e-06}, {"id": 685, "seek": 346850, "start": 3472.06, "end": 3477.94, "text": " So a discriminator is going to receive as input an image.", "tokens": [407, 257, 20828, 1639, 307, 516, 281, 4774, 382, 4846, 364, 3256, 13], "temperature": 0.0, "avg_logprob": -0.1486055374145508, "compression_ratio": 1.7511961722488039, "no_speech_prob": 6.748033229087014e-06}, {"id": 686, "seek": 346850, "start": 3477.94, "end": 3481.14, "text": " And it's going to spit out a number.", "tokens": [400, 309, 311, 516, 281, 22127, 484, 257, 1230, 13], "temperature": 0.0, "avg_logprob": -0.1486055374145508, "compression_ratio": 1.7511961722488039, "no_speech_prob": 6.748033229087014e-06}, {"id": 687, "seek": 346850, "start": 3481.14, "end": 3487.42, "text": " And the number is meant to be lower if it thinks this image is real.", "tokens": [400, 264, 1230, 307, 4140, 281, 312, 3126, 498, 309, 7309, 341, 3256, 307, 957, 13], "temperature": 0.0, "avg_logprob": -0.1486055374145508, "compression_ratio": 1.7511961722488039, "no_speech_prob": 6.748033229087014e-06}, {"id": 688, "seek": 346850, "start": 3487.42, "end": 3493.1, "text": " Now of course, what does it do for a lower number thing doesn't appear in the architecture.", "tokens": [823, 295, 1164, 11, 437, 775, 309, 360, 337, 257, 3126, 1230, 551, 1177, 380, 4204, 294, 264, 9482, 13], "temperature": 0.0, "avg_logprob": -0.1486055374145508, "compression_ratio": 1.7511961722488039, "no_speech_prob": 6.748033229087014e-06}, {"id": 689, "seek": 346850, "start": 3493.1, "end": 3494.82, "text": " That'll be in the loss function.", "tokens": [663, 603, 312, 294, 264, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1486055374145508, "compression_ratio": 1.7511961722488039, "no_speech_prob": 6.748033229087014e-06}, {"id": 690, "seek": 349482, "start": 3494.82, "end": 3503.54, "text": " So all we have to do is create something that takes an image and spits out a number.", "tokens": [407, 439, 321, 362, 281, 360, 307, 1884, 746, 300, 2516, 364, 3256, 293, 637, 1208, 484, 257, 1230, 13], "temperature": 0.0, "avg_logprob": -0.10881780175601735, "compression_ratio": 1.548780487804878, "no_speech_prob": 1.7880597624753136e-06}, {"id": 691, "seek": 349482, "start": 3503.54, "end": 3513.7000000000003, "text": " So a lot of this code is borrowed from the original authors of the paper.", "tokens": [407, 257, 688, 295, 341, 3089, 307, 26805, 490, 264, 3380, 16552, 295, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.10881780175601735, "compression_ratio": 1.548780487804878, "no_speech_prob": 1.7880597624753136e-06}, {"id": 692, "seek": 349482, "start": 3513.7000000000003, "end": 3517.86, "text": " So some of the naming scheme and stuff is different to what we're used to.", "tokens": [407, 512, 295, 264, 25290, 12232, 293, 1507, 307, 819, 281, 437, 321, 434, 1143, 281, 13], "temperature": 0.0, "avg_logprob": -0.10881780175601735, "compression_ratio": 1.548780487804878, "no_speech_prob": 1.7880597624753136e-06}, {"id": 693, "seek": 349482, "start": 3517.86, "end": 3523.5800000000004, "text": " So sorry about that.", "tokens": [407, 2597, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.10881780175601735, "compression_ratio": 1.548780487804878, "no_speech_prob": 1.7880597624753136e-06}, {"id": 694, "seek": 352358, "start": 3523.58, "end": 3525.58, "text": " But I've tried to make it look at least somewhat familiar.", "tokens": [583, 286, 600, 3031, 281, 652, 309, 574, 412, 1935, 8344, 4963, 13], "temperature": 0.0, "avg_logprob": -0.203812313079834, "compression_ratio": 1.7751937984496124, "no_speech_prob": 8.664612323627807e-06}, {"id": 695, "seek": 352358, "start": 3525.58, "end": 3527.9, "text": " I probably should have renamed things a little bit.", "tokens": [286, 1391, 820, 362, 40949, 721, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.203812313079834, "compression_ratio": 1.7751937984496124, "no_speech_prob": 8.664612323627807e-06}, {"id": 696, "seek": 352358, "start": 3527.9, "end": 3530.2, "text": " But it looks very similar to actually what we had before.", "tokens": [583, 309, 1542, 588, 2531, 281, 767, 437, 321, 632, 949, 13], "temperature": 0.0, "avg_logprob": -0.203812313079834, "compression_ratio": 1.7751937984496124, "no_speech_prob": 8.664612323627807e-06}, {"id": 697, "seek": 352358, "start": 3530.2, "end": 3532.34, "text": " We start out with a convolution.", "tokens": [492, 722, 484, 365, 257, 45216, 13], "temperature": 0.0, "avg_logprob": -0.203812313079834, "compression_ratio": 1.7751937984496124, "no_speech_prob": 8.664612323627807e-06}, {"id": 698, "seek": 352358, "start": 3532.34, "end": 3538.02, "text": " So remember, conv block is conv value batch norm.", "tokens": [407, 1604, 11, 3754, 3461, 307, 3754, 2158, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.203812313079834, "compression_ratio": 1.7751937984496124, "no_speech_prob": 8.664612323627807e-06}, {"id": 699, "seek": 352358, "start": 3538.02, "end": 3542.18, "text": " And then we have a bunch of extra conv layers.", "tokens": [400, 550, 321, 362, 257, 3840, 295, 2857, 3754, 7914, 13], "temperature": 0.0, "avg_logprob": -0.203812313079834, "compression_ratio": 1.7751937984496124, "no_speech_prob": 8.664612323627807e-06}, {"id": 700, "seek": 352358, "start": 3542.18, "end": 3543.74, "text": " This is not going to use a residual.", "tokens": [639, 307, 406, 516, 281, 764, 257, 27980, 13], "temperature": 0.0, "avg_logprob": -0.203812313079834, "compression_ratio": 1.7751937984496124, "no_speech_prob": 8.664612323627807e-06}, {"id": 701, "seek": 352358, "start": 3543.74, "end": 3547.2599999999998, "text": " So it looks very similar to before, a bunch of extra layers, but these are going to be", "tokens": [407, 309, 1542, 588, 2531, 281, 949, 11, 257, 3840, 295, 2857, 7914, 11, 457, 613, 366, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.203812313079834, "compression_ratio": 1.7751937984496124, "no_speech_prob": 8.664612323627807e-06}, {"id": 702, "seek": 352358, "start": 3547.2599999999998, "end": 3552.2599999999998, "text": " conv layers rather than res layers.", "tokens": [3754, 7914, 2831, 813, 725, 7914, 13], "temperature": 0.0, "avg_logprob": -0.203812313079834, "compression_ratio": 1.7751937984496124, "no_speech_prob": 8.664612323627807e-06}, {"id": 703, "seek": 355226, "start": 3552.26, "end": 3564.2200000000003, "text": " And then at the end, we need to append enough stride2 conv layers that we decrease the grid", "tokens": [400, 550, 412, 264, 917, 11, 321, 643, 281, 34116, 1547, 1056, 482, 17, 3754, 7914, 300, 321, 11514, 264, 10748], "temperature": 0.0, "avg_logprob": -0.1509704834375626, "compression_ratio": 1.6305732484076434, "no_speech_prob": 2.994420356117189e-06}, {"id": 704, "seek": 355226, "start": 3564.2200000000003, "end": 3568.7000000000003, "text": " size down to be no bigger than 4x4.", "tokens": [2744, 760, 281, 312, 572, 3801, 813, 1017, 87, 19, 13], "temperature": 0.0, "avg_logprob": -0.1509704834375626, "compression_ratio": 1.6305732484076434, "no_speech_prob": 2.994420356117189e-06}, {"id": 705, "seek": 355226, "start": 3568.7000000000003, "end": 3573.46, "text": " So it's going to keep using stride2, divide the size by 2, stride2, divide by size by", "tokens": [407, 309, 311, 516, 281, 1066, 1228, 1056, 482, 17, 11, 9845, 264, 2744, 538, 568, 11, 1056, 482, 17, 11, 9845, 538, 2744, 538], "temperature": 0.0, "avg_logprob": -0.1509704834375626, "compression_ratio": 1.6305732484076434, "no_speech_prob": 2.994420356117189e-06}, {"id": 706, "seek": 355226, "start": 3573.46, "end": 3576.98, "text": " 2 until our grid size is no bigger than 4.", "tokens": [568, 1826, 527, 10748, 2744, 307, 572, 3801, 813, 1017, 13], "temperature": 0.0, "avg_logprob": -0.1509704834375626, "compression_ratio": 1.6305732484076434, "no_speech_prob": 2.994420356117189e-06}, {"id": 707, "seek": 357698, "start": 3576.98, "end": 3582.3, "text": " And so this is quite a nice way of creating as many layers as you need in a network to", "tokens": [400, 370, 341, 307, 1596, 257, 1481, 636, 295, 4084, 382, 867, 7914, 382, 291, 643, 294, 257, 3209, 281], "temperature": 0.0, "avg_logprob": -0.16980029627219917, "compression_ratio": 1.4504132231404958, "no_speech_prob": 5.50756976736011e-06}, {"id": 708, "seek": 357698, "start": 3582.3, "end": 3587.22, "text": " handle arbitrary sized images and turn them into a fixed known grid size.", "tokens": [4813, 23211, 20004, 5267, 293, 1261, 552, 666, 257, 6806, 2570, 10748, 2744, 13], "temperature": 0.0, "avg_logprob": -0.16980029627219917, "compression_ratio": 1.4504132231404958, "no_speech_prob": 5.50756976736011e-06}, {"id": 709, "seek": 357698, "start": 3587.22, "end": 3588.22, "text": " Yes, Rachel.", "tokens": [1079, 11, 14246, 13], "temperature": 0.0, "avg_logprob": -0.16980029627219917, "compression_ratio": 1.4504132231404958, "no_speech_prob": 5.50756976736011e-06}, {"id": 710, "seek": 357698, "start": 3588.22, "end": 3596.1, "text": " Does a GAN need a lot more data than say dogs versus cats or NLP or is it comparable?", "tokens": [4402, 257, 460, 1770, 643, 257, 688, 544, 1412, 813, 584, 7197, 5717, 11111, 420, 426, 45196, 420, 307, 309, 25323, 30], "temperature": 0.0, "avg_logprob": -0.16980029627219917, "compression_ratio": 1.4504132231404958, "no_speech_prob": 5.50756976736011e-06}, {"id": 711, "seek": 357698, "start": 3596.1, "end": 3602.42, "text": " You know, honestly, I'm kind of embarrassed to say I am not an expert practitioner in", "tokens": [509, 458, 11, 6095, 11, 286, 478, 733, 295, 16843, 281, 584, 286, 669, 406, 364, 5844, 32125, 294], "temperature": 0.0, "avg_logprob": -0.16980029627219917, "compression_ratio": 1.4504132231404958, "no_speech_prob": 5.50756976736011e-06}, {"id": 712, "seek": 357698, "start": 3602.42, "end": 3604.02, "text": " GANs.", "tokens": [460, 1770, 82, 13], "temperature": 0.0, "avg_logprob": -0.16980029627219917, "compression_ratio": 1.4504132231404958, "no_speech_prob": 5.50756976736011e-06}, {"id": 713, "seek": 360402, "start": 3604.02, "end": 3613.42, "text": " So the stuff I teach in part 1 is stuff I'm happy to say I know the best way to do these", "tokens": [407, 264, 1507, 286, 2924, 294, 644, 502, 307, 1507, 286, 478, 2055, 281, 584, 286, 458, 264, 1151, 636, 281, 360, 613], "temperature": 0.0, "avg_logprob": -0.19035623623774603, "compression_ratio": 1.559090909090909, "no_speech_prob": 7.646412996109575e-06}, {"id": 714, "seek": 360402, "start": 3613.42, "end": 3617.9, "text": " things and so I can show you state of the art results like I just did with Sci-Fi 10", "tokens": [721, 293, 370, 286, 393, 855, 291, 1785, 295, 264, 1523, 3542, 411, 286, 445, 630, 365, 16942, 12, 13229, 1266], "temperature": 0.0, "avg_logprob": -0.19035623623774603, "compression_ratio": 1.559090909090909, "no_speech_prob": 7.646412996109575e-06}, {"id": 715, "seek": 360402, "start": 3617.9, "end": 3621.34, "text": " with the help of some of my students, of course.", "tokens": [365, 264, 854, 295, 512, 295, 452, 1731, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.19035623623774603, "compression_ratio": 1.559090909090909, "no_speech_prob": 7.646412996109575e-06}, {"id": 716, "seek": 360402, "start": 3621.34, "end": 3626.48, "text": " I'm not there at all with GANs.", "tokens": [286, 478, 406, 456, 412, 439, 365, 460, 1770, 82, 13], "temperature": 0.0, "avg_logprob": -0.19035623623774603, "compression_ratio": 1.559090909090909, "no_speech_prob": 7.646412996109575e-06}, {"id": 717, "seek": 360402, "start": 3626.48, "end": 3628.22, "text": " So I'm not quite sure how much you need.", "tokens": [407, 286, 478, 406, 1596, 988, 577, 709, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.19035623623774603, "compression_ratio": 1.559090909090909, "no_speech_prob": 7.646412996109575e-06}, {"id": 718, "seek": 360402, "start": 3628.22, "end": 3633.14, "text": " Like in general, it seems you need quite a lot.", "tokens": [1743, 294, 2674, 11, 309, 2544, 291, 643, 1596, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.19035623623774603, "compression_ratio": 1.559090909090909, "no_speech_prob": 7.646412996109575e-06}, {"id": 719, "seek": 363314, "start": 3633.14, "end": 3636.42, "text": " Remember the only reason we didn't need too much in dogs and cats is because we had a", "tokens": [5459, 264, 787, 1778, 321, 994, 380, 643, 886, 709, 294, 7197, 293, 11111, 307, 570, 321, 632, 257], "temperature": 0.0, "avg_logprob": -0.1645116114961928, "compression_ratio": 1.6688741721854305, "no_speech_prob": 6.144090093584964e-06}, {"id": 720, "seek": 363314, "start": 3636.42, "end": 3642.42, "text": " pre-trained model and could we leverage pre-trained GAN models and fine-tune them?", "tokens": [659, 12, 17227, 2001, 2316, 293, 727, 321, 13982, 659, 12, 17227, 2001, 460, 1770, 5245, 293, 2489, 12, 83, 2613, 552, 30], "temperature": 0.0, "avg_logprob": -0.1645116114961928, "compression_ratio": 1.6688741721854305, "no_speech_prob": 6.144090093584964e-06}, {"id": 721, "seek": 363314, "start": 3642.42, "end": 3643.7799999999997, "text": " Probably.", "tokens": [9210, 13], "temperature": 0.0, "avg_logprob": -0.1645116114961928, "compression_ratio": 1.6688741721854305, "no_speech_prob": 6.144090093584964e-06}, {"id": 722, "seek": 363314, "start": 3643.7799999999997, "end": 3646.7799999999997, "text": " I don't think anybody's done it as far as I know.", "tokens": [286, 500, 380, 519, 4472, 311, 1096, 309, 382, 1400, 382, 286, 458, 13], "temperature": 0.0, "avg_logprob": -0.1645116114961928, "compression_ratio": 1.6688741721854305, "no_speech_prob": 6.144090093584964e-06}, {"id": 723, "seek": 363314, "start": 3646.7799999999997, "end": 3650.7799999999997, "text": " That could be a really interesting thing for people to kind of think about and experiment", "tokens": [663, 727, 312, 257, 534, 1880, 551, 337, 561, 281, 733, 295, 519, 466, 293, 5120], "temperature": 0.0, "avg_logprob": -0.1645116114961928, "compression_ratio": 1.6688741721854305, "no_speech_prob": 6.144090093584964e-06}, {"id": 724, "seek": 363314, "start": 3650.7799999999997, "end": 3651.7799999999997, "text": " with.", "tokens": [365, 13], "temperature": 0.0, "avg_logprob": -0.1645116114961928, "compression_ratio": 1.6688741721854305, "no_speech_prob": 6.144090093584964e-06}, {"id": 725, "seek": 363314, "start": 3651.7799999999997, "end": 3654.8599999999997, "text": " Maybe people have done it and there's some literature there I haven't come across.", "tokens": [2704, 561, 362, 1096, 309, 293, 456, 311, 512, 10394, 456, 286, 2378, 380, 808, 2108, 13], "temperature": 0.0, "avg_logprob": -0.1645116114961928, "compression_ratio": 1.6688741721854305, "no_speech_prob": 6.144090093584964e-06}, {"id": 726, "seek": 363314, "start": 3654.8599999999997, "end": 3661.14, "text": " So I'm somewhat familiar with the main pieces of literature in GANs, but I don't know all", "tokens": [407, 286, 478, 8344, 4963, 365, 264, 2135, 3755, 295, 10394, 294, 460, 1770, 82, 11, 457, 286, 500, 380, 458, 439], "temperature": 0.0, "avg_logprob": -0.1645116114961928, "compression_ratio": 1.6688741721854305, "no_speech_prob": 6.144090093584964e-06}, {"id": 727, "seek": 363314, "start": 3661.14, "end": 3662.62, "text": " of it.", "tokens": [295, 309, 13], "temperature": 0.0, "avg_logprob": -0.1645116114961928, "compression_ratio": 1.6688741721854305, "no_speech_prob": 6.144090093584964e-06}, {"id": 728, "seek": 366262, "start": 3662.62, "end": 3666.2999999999997, "text": " So maybe I've missed something about transfer learning in GANs, but that would be the trick", "tokens": [407, 1310, 286, 600, 6721, 746, 466, 5003, 2539, 294, 460, 1770, 82, 11, 457, 300, 576, 312, 264, 4282], "temperature": 0.0, "avg_logprob": -0.14600059239551275, "compression_ratio": 1.643939393939394, "no_speech_prob": 3.533802373567596e-05}, {"id": 729, "seek": 366262, "start": 3666.2999999999997, "end": 3669.5, "text": " to not needing too much data.", "tokens": [281, 406, 18006, 886, 709, 1412, 13], "temperature": 0.0, "avg_logprob": -0.14600059239551275, "compression_ratio": 1.643939393939394, "no_speech_prob": 3.533802373567596e-05}, {"id": 730, "seek": 366262, "start": 3669.5, "end": 3674.66, "text": " So it's the huge speed-up combination of one cycle learning rate and momentum annealing", "tokens": [407, 309, 311, 264, 2603, 3073, 12, 1010, 6562, 295, 472, 6586, 2539, 3314, 293, 11244, 22256, 4270], "temperature": 0.0, "avg_logprob": -0.14600059239551275, "compression_ratio": 1.643939393939394, "no_speech_prob": 3.533802373567596e-05}, {"id": 731, "seek": 366262, "start": 3674.66, "end": 3679.06, "text": " plus the 8 GPU parallel training and the half precision.", "tokens": [1804, 264, 1649, 18407, 8952, 3097, 293, 264, 1922, 18356, 13], "temperature": 0.0, "avg_logprob": -0.14600059239551275, "compression_ratio": 1.643939393939394, "no_speech_prob": 3.533802373567596e-05}, {"id": 732, "seek": 366262, "start": 3679.06, "end": 3684.8199999999997, "text": " Is that only possible to do the half precision calculation with consumer GPU?", "tokens": [1119, 300, 787, 1944, 281, 360, 264, 1922, 18356, 17108, 365, 9711, 18407, 30], "temperature": 0.0, "avg_logprob": -0.14600059239551275, "compression_ratio": 1.643939393939394, "no_speech_prob": 3.533802373567596e-05}, {"id": 733, "seek": 366262, "start": 3684.8199999999997, "end": 3689.18, "text": " Another question, why is the calculation eight times faster from single to half precision", "tokens": [3996, 1168, 11, 983, 307, 264, 17108, 3180, 1413, 4663, 490, 2167, 281, 1922, 18356], "temperature": 0.0, "avg_logprob": -0.14600059239551275, "compression_ratio": 1.643939393939394, "no_speech_prob": 3.533802373567596e-05}, {"id": 734, "seek": 368918, "start": 3689.18, "end": 3693.14, "text": " while from double to single is only two times faster?", "tokens": [1339, 490, 3834, 281, 2167, 307, 787, 732, 1413, 4663, 30], "temperature": 0.0, "avg_logprob": -0.1958208619878533, "compression_ratio": 1.6504424778761062, "no_speech_prob": 5.338107712304918e-06}, {"id": 735, "seek": 368918, "start": 3693.14, "end": 3698.7, "text": " So the CyPhar 10 result, it's not eight times faster from single to half.", "tokens": [407, 264, 10295, 47, 5854, 1266, 1874, 11, 309, 311, 406, 3180, 1413, 4663, 490, 2167, 281, 1922, 13], "temperature": 0.0, "avg_logprob": -0.1958208619878533, "compression_ratio": 1.6504424778761062, "no_speech_prob": 5.338107712304918e-06}, {"id": 736, "seek": 368918, "start": 3698.7, "end": 3702.62, "text": " It's about two or three times as fast from single to half.", "tokens": [467, 311, 466, 732, 420, 1045, 1413, 382, 2370, 490, 2167, 281, 1922, 13], "temperature": 0.0, "avg_logprob": -0.1958208619878533, "compression_ratio": 1.6504424778761062, "no_speech_prob": 5.338107712304918e-06}, {"id": 737, "seek": 368918, "start": 3702.62, "end": 3711.2599999999998, "text": " The Nvidia claims about the flop's performance of the tensor cores are academically correct,", "tokens": [440, 46284, 9441, 466, 264, 25343, 311, 3389, 295, 264, 40863, 24826, 366, 48944, 3006, 11], "temperature": 0.0, "avg_logprob": -0.1958208619878533, "compression_ratio": 1.6504424778761062, "no_speech_prob": 5.338107712304918e-06}, {"id": 738, "seek": 368918, "start": 3711.2599999999998, "end": 3717.5, "text": " but in practice meaningless because it really depends on what cores you need for what pieces.", "tokens": [457, 294, 3124, 33232, 570, 309, 534, 5946, 322, 437, 24826, 291, 643, 337, 437, 3755, 13], "temperature": 0.0, "avg_logprob": -0.1958208619878533, "compression_ratio": 1.6504424778761062, "no_speech_prob": 5.338107712304918e-06}, {"id": 739, "seek": 371750, "start": 3717.5, "end": 3722.7, "text": " So about two or three X improvement for half.", "tokens": [407, 466, 732, 420, 1045, 1783, 10444, 337, 1922, 13], "temperature": 0.0, "avg_logprob": -0.18861421754088584, "compression_ratio": 1.5833333333333333, "no_speech_prob": 4.565903054754017e-06}, {"id": 740, "seek": 371750, "start": 3722.7, "end": 3730.74, "text": " So the half precision helps a bit, the extra GPUs helps a bit, the one cycle helps an enormous", "tokens": [407, 264, 1922, 18356, 3665, 257, 857, 11, 264, 2857, 18407, 82, 3665, 257, 857, 11, 264, 472, 6586, 3665, 364, 11322], "temperature": 0.0, "avg_logprob": -0.18861421754088584, "compression_ratio": 1.5833333333333333, "no_speech_prob": 4.565903054754017e-06}, {"id": 741, "seek": 371750, "start": 3730.74, "end": 3731.74, "text": " amount.", "tokens": [2372, 13], "temperature": 0.0, "avg_logprob": -0.18861421754088584, "compression_ratio": 1.5833333333333333, "no_speech_prob": 4.565903054754017e-06}, {"id": 742, "seek": 371750, "start": 3731.74, "end": 3736.82, "text": " Then another key piece was the playing around with the parameters that I told you about.", "tokens": [1396, 1071, 2141, 2522, 390, 264, 2433, 926, 365, 264, 9834, 300, 286, 1907, 291, 466, 13], "temperature": 0.0, "avg_logprob": -0.18861421754088584, "compression_ratio": 1.5833333333333333, "no_speech_prob": 4.565903054754017e-06}, {"id": 743, "seek": 371750, "start": 3736.82, "end": 3743.1, "text": " So reading the wide ResNet paper carefully, identifying the kinds of things that they", "tokens": [407, 3760, 264, 4874, 5015, 31890, 3035, 7500, 11, 16696, 264, 3685, 295, 721, 300, 436], "temperature": 0.0, "avg_logprob": -0.18861421754088584, "compression_ratio": 1.5833333333333333, "no_speech_prob": 4.565903054754017e-06}, {"id": 744, "seek": 374310, "start": 3743.1, "end": 3749.1, "text": " found there, and then writing a version of the architecture you just saw that made it", "tokens": [1352, 456, 11, 293, 550, 3579, 257, 3037, 295, 264, 9482, 291, 445, 1866, 300, 1027, 309], "temperature": 0.0, "avg_logprob": -0.16210167983482623, "compression_ratio": 1.7239819004524888, "no_speech_prob": 1.670109668339137e-05}, {"id": 745, "seek": 374310, "start": 3749.1, "end": 3755.74, "text": " really easy for me to fiddle around with parameters.", "tokens": [534, 1858, 337, 385, 281, 24553, 2285, 926, 365, 9834, 13], "temperature": 0.0, "avg_logprob": -0.16210167983482623, "compression_ratio": 1.7239819004524888, "no_speech_prob": 1.670109668339137e-05}, {"id": 746, "seek": 374310, "start": 3755.74, "end": 3761.14, "text": " Staying up all night trying every possible combination of different kernel sizes and", "tokens": [8691, 278, 493, 439, 1818, 1382, 633, 1944, 6562, 295, 819, 28256, 11602, 293], "temperature": 0.0, "avg_logprob": -0.16210167983482623, "compression_ratio": 1.7239819004524888, "no_speech_prob": 1.670109668339137e-05}, {"id": 747, "seek": 374310, "start": 3761.14, "end": 3768.5, "text": " numbers of kernels and numbers of layer groups and size of layer groups.", "tokens": [3547, 295, 23434, 1625, 293, 3547, 295, 4583, 3935, 293, 2744, 295, 4583, 3935, 13], "temperature": 0.0, "avg_logprob": -0.16210167983482623, "compression_ratio": 1.7239819004524888, "no_speech_prob": 1.670109668339137e-05}, {"id": 748, "seek": 374310, "start": 3768.5, "end": 3772.7799999999997, "text": " Remember we did a bottleneck, but actually we tended to focus not on bottlenecks but", "tokens": [5459, 321, 630, 257, 44641, 547, 11, 457, 767, 321, 34732, 281, 1879, 406, 322, 44641, 2761, 457], "temperature": 0.0, "avg_logprob": -0.16210167983482623, "compression_ratio": 1.7239819004524888, "no_speech_prob": 1.670109668339137e-05}, {"id": 749, "seek": 377278, "start": 3772.78, "end": 3774.1400000000003, "text": " instead on widening.", "tokens": [2602, 322, 32552, 278, 13], "temperature": 0.0, "avg_logprob": -0.17135779390630035, "compression_ratio": 1.550420168067227, "no_speech_prob": 1.6442189007648267e-05}, {"id": 750, "seek": 377278, "start": 3774.1400000000003, "end": 3777.78, "text": " So we actually like things that increase the size and then decrease it because it takes", "tokens": [407, 321, 767, 411, 721, 300, 3488, 264, 2744, 293, 550, 11514, 309, 570, 309, 2516], "temperature": 0.0, "avg_logprob": -0.17135779390630035, "compression_ratio": 1.550420168067227, "no_speech_prob": 1.6442189007648267e-05}, {"id": 751, "seek": 377278, "start": 3777.78, "end": 3779.46, "text": " better advantage of the GPU.", "tokens": [1101, 5002, 295, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.17135779390630035, "compression_ratio": 1.550420168067227, "no_speech_prob": 1.6442189007648267e-05}, {"id": 752, "seek": 377278, "start": 3779.46, "end": 3782.86, "text": " So all those things combined together.", "tokens": [407, 439, 729, 721, 9354, 1214, 13], "temperature": 0.0, "avg_logprob": -0.17135779390630035, "compression_ratio": 1.550420168067227, "no_speech_prob": 1.6442189007648267e-05}, {"id": 753, "seek": 377278, "start": 3782.86, "end": 3789.2200000000003, "text": " I'd say the one cycle was perhaps the most critical, but every one of those resulted", "tokens": [286, 1116, 584, 264, 472, 6586, 390, 4317, 264, 881, 4924, 11, 457, 633, 472, 295, 729, 18753], "temperature": 0.0, "avg_logprob": -0.17135779390630035, "compression_ratio": 1.550420168067227, "no_speech_prob": 1.6442189007648267e-05}, {"id": 754, "seek": 377278, "start": 3789.2200000000003, "end": 3790.2200000000003, "text": " in a big speed up.", "tokens": [294, 257, 955, 3073, 493, 13], "temperature": 0.0, "avg_logprob": -0.17135779390630035, "compression_ratio": 1.550420168067227, "no_speech_prob": 1.6442189007648267e-05}, {"id": 755, "seek": 377278, "start": 3790.2200000000003, "end": 3800.6200000000003, "text": " That's why we were able to get this 30X improvement over the state of the art sci-fi 10.", "tokens": [663, 311, 983, 321, 645, 1075, 281, 483, 341, 2217, 55, 10444, 670, 264, 1785, 295, 264, 1523, 2180, 12, 13325, 1266, 13], "temperature": 0.0, "avg_logprob": -0.17135779390630035, "compression_ratio": 1.550420168067227, "no_speech_prob": 1.6442189007648267e-05}, {"id": 756, "seek": 380062, "start": 3800.62, "end": 3806.42, "text": " And we got some ideas for other things, like after this Dawnbench finishes.", "tokens": [400, 321, 658, 512, 3487, 337, 661, 721, 11, 411, 934, 341, 26001, 47244, 23615, 13], "temperature": 0.0, "avg_logprob": -0.2897847009741742, "compression_ratio": 1.6133333333333333, "no_speech_prob": 2.3187076294561848e-05}, {"id": 757, "seek": 380062, "start": 3806.42, "end": 3813.5, "text": " Maybe we'll try and go even further and see if we can beat 1 minute one day.", "tokens": [2704, 321, 603, 853, 293, 352, 754, 3052, 293, 536, 498, 321, 393, 4224, 502, 3456, 472, 786, 13], "temperature": 0.0, "avg_logprob": -0.2897847009741742, "compression_ratio": 1.6133333333333333, "no_speech_prob": 2.3187076294561848e-05}, {"id": 758, "seek": 380062, "start": 3813.5, "end": 3816.5, "text": " That'll be fun.", "tokens": [663, 603, 312, 1019, 13], "temperature": 0.0, "avg_logprob": -0.2897847009741742, "compression_ratio": 1.6133333333333333, "no_speech_prob": 2.3187076294561848e-05}, {"id": 759, "seek": 380062, "start": 3816.5, "end": 3819.38, "text": " So here's our discriminator.", "tokens": [407, 510, 311, 527, 20828, 1639, 13], "temperature": 0.0, "avg_logprob": -0.2897847009741742, "compression_ratio": 1.6133333333333333, "no_speech_prob": 2.3187076294561848e-05}, {"id": 760, "seek": 380062, "start": 3819.38, "end": 3824.7, "text": " The important thing to remember about an architecture is it doesn't do anything other than have", "tokens": [440, 1021, 551, 281, 1604, 466, 364, 9482, 307, 309, 1177, 380, 360, 1340, 661, 813, 362], "temperature": 0.0, "avg_logprob": -0.2897847009741742, "compression_ratio": 1.6133333333333333, "no_speech_prob": 2.3187076294561848e-05}, {"id": 761, "seek": 380062, "start": 3824.7, "end": 3829.5, "text": " some input tensor size and rank and some output tensor size and rank.", "tokens": [512, 4846, 40863, 2744, 293, 6181, 293, 512, 5598, 40863, 2744, 293, 6181, 13], "temperature": 0.0, "avg_logprob": -0.2897847009741742, "compression_ratio": 1.6133333333333333, "no_speech_prob": 2.3187076294561848e-05}, {"id": 762, "seek": 382950, "start": 3829.5, "end": 3835.3, "text": " So this is going to spit out, you see the last conv here has one channel.", "tokens": [407, 341, 307, 516, 281, 22127, 484, 11, 291, 536, 264, 1036, 3754, 510, 575, 472, 2269, 13], "temperature": 0.0, "avg_logprob": -0.1783789907182966, "compression_ratio": 1.669767441860465, "no_speech_prob": 1.4738878235220909e-05}, {"id": 763, "seek": 382950, "start": 3835.3, "end": 3838.58, "text": " This is a bit different to what we're used to, because normally our last thing is a linear", "tokens": [639, 307, 257, 857, 819, 281, 437, 321, 434, 1143, 281, 11, 570, 5646, 527, 1036, 551, 307, 257, 8213], "temperature": 0.0, "avg_logprob": -0.1783789907182966, "compression_ratio": 1.669767441860465, "no_speech_prob": 1.4738878235220909e-05}, {"id": 764, "seek": 382950, "start": 3838.58, "end": 3839.58, "text": " block.", "tokens": [3461, 13], "temperature": 0.0, "avg_logprob": -0.1783789907182966, "compression_ratio": 1.669767441860465, "no_speech_prob": 1.4738878235220909e-05}, {"id": 765, "seek": 382950, "start": 3839.58, "end": 3843.26, "text": " But our last thing here is a conv block.", "tokens": [583, 527, 1036, 551, 510, 307, 257, 3754, 3461, 13], "temperature": 0.0, "avg_logprob": -0.1783789907182966, "compression_ratio": 1.669767441860465, "no_speech_prob": 1.4738878235220909e-05}, {"id": 766, "seek": 382950, "start": 3843.26, "end": 3849.74, "text": " And it's only got one channel, but it's got a grid size of something around 4x4.", "tokens": [400, 309, 311, 787, 658, 472, 2269, 11, 457, 309, 311, 658, 257, 10748, 2744, 295, 746, 926, 1017, 87, 19, 13], "temperature": 0.0, "avg_logprob": -0.1783789907182966, "compression_ratio": 1.669767441860465, "no_speech_prob": 1.4738878235220909e-05}, {"id": 767, "seek": 382950, "start": 3849.74, "end": 3850.74, "text": " It's no more than 4x4.", "tokens": [467, 311, 572, 544, 813, 1017, 87, 19, 13], "temperature": 0.0, "avg_logprob": -0.1783789907182966, "compression_ratio": 1.669767441860465, "no_speech_prob": 1.4738878235220909e-05}, {"id": 768, "seek": 382950, "start": 3850.74, "end": 3857.18, "text": " So we're going to spit out a 4x4x1 tensor.", "tokens": [407, 321, 434, 516, 281, 22127, 484, 257, 1017, 87, 19, 87, 16, 40863, 13], "temperature": 0.0, "avg_logprob": -0.1783789907182966, "compression_ratio": 1.669767441860465, "no_speech_prob": 1.4738878235220909e-05}, {"id": 769, "seek": 385718, "start": 3857.18, "end": 3863.8999999999996, "text": " So what we then do is we then take the mean of that.", "tokens": [407, 437, 321, 550, 360, 307, 321, 550, 747, 264, 914, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.14039247176226446, "compression_ratio": 1.6278026905829597, "no_speech_prob": 5.771896667283727e-06}, {"id": 770, "seek": 385718, "start": 3863.8999999999996, "end": 3868.06, "text": " So it goes from 4x4x1 to the scalar.", "tokens": [407, 309, 1709, 490, 1017, 87, 19, 87, 16, 281, 264, 39684, 13], "temperature": 0.0, "avg_logprob": -0.14039247176226446, "compression_ratio": 1.6278026905829597, "no_speech_prob": 5.771896667283727e-06}, {"id": 771, "seek": 385718, "start": 3868.06, "end": 3872.2599999999998, "text": " So this is kind of like the ultimate adaptive average pooling, because we've got something", "tokens": [407, 341, 307, 733, 295, 411, 264, 9705, 27912, 4274, 7005, 278, 11, 570, 321, 600, 658, 746], "temperature": 0.0, "avg_logprob": -0.14039247176226446, "compression_ratio": 1.6278026905829597, "no_speech_prob": 5.771896667283727e-06}, {"id": 772, "seek": 385718, "start": 3872.2599999999998, "end": 3874.46, "text": " with just one channel, we take the mean.", "tokens": [365, 445, 472, 2269, 11, 321, 747, 264, 914, 13], "temperature": 0.0, "avg_logprob": -0.14039247176226446, "compression_ratio": 1.6278026905829597, "no_speech_prob": 5.771896667283727e-06}, {"id": 773, "seek": 385718, "start": 3874.46, "end": 3876.74, "text": " So this is a bit different.", "tokens": [407, 341, 307, 257, 857, 819, 13], "temperature": 0.0, "avg_logprob": -0.14039247176226446, "compression_ratio": 1.6278026905829597, "no_speech_prob": 5.771896667283727e-06}, {"id": 774, "seek": 385718, "start": 3876.74, "end": 3881.2999999999997, "text": " Normally we first do average pooling and then we put it through a fully connected layer", "tokens": [17424, 321, 700, 360, 4274, 7005, 278, 293, 550, 321, 829, 309, 807, 257, 4498, 4582, 4583], "temperature": 0.0, "avg_logprob": -0.14039247176226446, "compression_ratio": 1.6278026905829597, "no_speech_prob": 5.771896667283727e-06}, {"id": 775, "seek": 385718, "start": 3881.2999999999997, "end": 3882.8599999999997, "text": " to get our one thing out.", "tokens": [281, 483, 527, 472, 551, 484, 13], "temperature": 0.0, "avg_logprob": -0.14039247176226446, "compression_ratio": 1.6278026905829597, "no_speech_prob": 5.771896667283727e-06}, {"id": 776, "seek": 388286, "start": 3882.86, "end": 3888.78, "text": " In this case though, we're getting one channel out and then taking the mean of that.", "tokens": [682, 341, 1389, 1673, 11, 321, 434, 1242, 472, 2269, 484, 293, 550, 1940, 264, 914, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.14803170763756618, "compression_ratio": 1.7296296296296296, "no_speech_prob": 1.2805334336007945e-05}, {"id": 777, "seek": 388286, "start": 3888.78, "end": 3893.2200000000003, "text": " I haven't fiddled around with why did we do it that way, what would instead happen if", "tokens": [286, 2378, 380, 283, 14273, 1493, 926, 365, 983, 630, 321, 360, 309, 300, 636, 11, 437, 576, 2602, 1051, 498], "temperature": 0.0, "avg_logprob": -0.14803170763756618, "compression_ratio": 1.7296296296296296, "no_speech_prob": 1.2805334336007945e-05}, {"id": 778, "seek": 388286, "start": 3893.2200000000003, "end": 3897.82, "text": " we did the usual average pooling followed by a fully connected layer.", "tokens": [321, 630, 264, 7713, 4274, 7005, 278, 6263, 538, 257, 4498, 4582, 4583, 13], "temperature": 0.0, "avg_logprob": -0.14803170763756618, "compression_ratio": 1.7296296296296296, "no_speech_prob": 1.2805334336007945e-05}, {"id": 779, "seek": 388286, "start": 3897.82, "end": 3901.94, "text": " Would it work better, would it not, I don't know.", "tokens": [6068, 309, 589, 1101, 11, 576, 309, 406, 11, 286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.14803170763756618, "compression_ratio": 1.7296296296296296, "no_speech_prob": 1.2805334336007945e-05}, {"id": 780, "seek": 388286, "start": 3901.94, "end": 3907.54, "text": " I rather suspect it would work better if we did it the normal way, but I haven't tried", "tokens": [286, 2831, 9091, 309, 576, 589, 1101, 498, 321, 630, 309, 264, 2710, 636, 11, 457, 286, 2378, 380, 3031], "temperature": 0.0, "avg_logprob": -0.14803170763756618, "compression_ratio": 1.7296296296296296, "no_speech_prob": 1.2805334336007945e-05}, {"id": 781, "seek": 388286, "start": 3907.54, "end": 3912.38, "text": " it and I don't really have a good enough intuition to know whether I'm missing something.", "tokens": [309, 293, 286, 500, 380, 534, 362, 257, 665, 1547, 24002, 281, 458, 1968, 286, 478, 5361, 746, 13], "temperature": 0.0, "avg_logprob": -0.14803170763756618, "compression_ratio": 1.7296296296296296, "no_speech_prob": 1.2805334336007945e-05}, {"id": 782, "seek": 391238, "start": 3912.38, "end": 3915.42, "text": " It's an interesting experiment to try, if somebody wants to stick an adaptive average", "tokens": [467, 311, 364, 1880, 5120, 281, 853, 11, 498, 2618, 2738, 281, 2897, 364, 27912, 4274], "temperature": 0.0, "avg_logprob": -0.20261084332185633, "compression_ratio": 1.718849840255591, "no_speech_prob": 9.972883162845392e-06}, {"id": 783, "seek": 391238, "start": 3915.42, "end": 3919.78, "text": " pooling layer here and a fully connected layer afterwards with a single output.", "tokens": [7005, 278, 4583, 510, 293, 257, 4498, 4582, 4583, 10543, 365, 257, 2167, 5598, 13], "temperature": 0.0, "avg_logprob": -0.20261084332185633, "compression_ratio": 1.718849840255591, "no_speech_prob": 9.972883162845392e-06}, {"id": 784, "seek": 391238, "start": 3919.78, "end": 3923.7000000000003, "text": " It should keep working, it should do something.", "tokens": [467, 820, 1066, 1364, 11, 309, 820, 360, 746, 13], "temperature": 0.0, "avg_logprob": -0.20261084332185633, "compression_ratio": 1.718849840255591, "no_speech_prob": 9.972883162845392e-06}, {"id": 785, "seek": 391238, "start": 3923.7000000000003, "end": 3927.38, "text": " The loss will go down, just see whether it works.", "tokens": [440, 4470, 486, 352, 760, 11, 445, 536, 1968, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.20261084332185633, "compression_ratio": 1.718849840255591, "no_speech_prob": 9.972883162845392e-06}, {"id": 786, "seek": 391238, "start": 3927.38, "end": 3928.38, "text": " So that's the discriminator.", "tokens": [407, 300, 311, 264, 20828, 1639, 13], "temperature": 0.0, "avg_logprob": -0.20261084332185633, "compression_ratio": 1.718849840255591, "no_speech_prob": 9.972883162845392e-06}, {"id": 787, "seek": 391238, "start": 3928.38, "end": 3931.3, "text": " So there's going to be a training loop.", "tokens": [407, 456, 311, 516, 281, 312, 257, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.20261084332185633, "compression_ratio": 1.718849840255591, "no_speech_prob": 9.972883162845392e-06}, {"id": 788, "seek": 391238, "start": 3931.3, "end": 3933.1, "text": " Let's assume we've already got a generator.", "tokens": [961, 311, 6552, 321, 600, 1217, 658, 257, 19265, 13], "temperature": 0.0, "avg_logprob": -0.20261084332185633, "compression_ratio": 1.718849840255591, "no_speech_prob": 9.972883162845392e-06}, {"id": 789, "seek": 391238, "start": 3933.1, "end": 3937.86, "text": " Somebody says, okay Jeremy, here's a generator, it generates bedrooms.", "tokens": [13463, 1619, 11, 1392, 17809, 11, 510, 311, 257, 19265, 11, 309, 23815, 39955, 13], "temperature": 0.0, "avg_logprob": -0.20261084332185633, "compression_ratio": 1.718849840255591, "no_speech_prob": 9.972883162845392e-06}, {"id": 790, "seek": 391238, "start": 3937.86, "end": 3941.2400000000002, "text": " I want you to build a model that can figure out which ones are real and which ones aren't.", "tokens": [286, 528, 291, 281, 1322, 257, 2316, 300, 393, 2573, 484, 597, 2306, 366, 957, 293, 597, 2306, 3212, 380, 13], "temperature": 0.0, "avg_logprob": -0.20261084332185633, "compression_ratio": 1.718849840255591, "no_speech_prob": 9.972883162845392e-06}, {"id": 791, "seek": 394124, "start": 3941.24, "end": 3946.7799999999997, "text": " So I'm going to take the dataset and basically label a bunch of images which are fake bedrooms", "tokens": [407, 286, 478, 516, 281, 747, 264, 28872, 293, 1936, 7645, 257, 3840, 295, 5267, 597, 366, 7592, 39955], "temperature": 0.0, "avg_logprob": -0.1873717067217586, "compression_ratio": 1.6444444444444444, "no_speech_prob": 1.9637973309727386e-06}, {"id": 792, "seek": 394124, "start": 3946.7799999999997, "end": 3952.18, "text": " from the generator and a bunch of images of real bedrooms from my Lsun dataset, just stick", "tokens": [490, 264, 19265, 293, 257, 3840, 295, 5267, 295, 957, 39955, 490, 452, 441, 11314, 28872, 11, 445, 2897], "temperature": 0.0, "avg_logprob": -0.1873717067217586, "compression_ratio": 1.6444444444444444, "no_speech_prob": 1.9637973309727386e-06}, {"id": 793, "seek": 394124, "start": 3952.18, "end": 3957.7999999999997, "text": " a 1 or a 0 and an H1, and then I'll try to get the discriminator to tell the difference.", "tokens": [257, 502, 420, 257, 1958, 293, 364, 389, 16, 11, 293, 550, 286, 603, 853, 281, 483, 264, 20828, 1639, 281, 980, 264, 2649, 13], "temperature": 0.0, "avg_logprob": -0.1873717067217586, "compression_ratio": 1.6444444444444444, "no_speech_prob": 1.9637973309727386e-06}, {"id": 794, "seek": 394124, "start": 3957.7999999999997, "end": 3964.06, "text": " So that's going to be simple enough.", "tokens": [407, 300, 311, 516, 281, 312, 2199, 1547, 13], "temperature": 0.0, "avg_logprob": -0.1873717067217586, "compression_ratio": 1.6444444444444444, "no_speech_prob": 1.9637973309727386e-06}, {"id": 795, "seek": 394124, "start": 3964.06, "end": 3967.8999999999996, "text": " But I haven't been given a generator, I need to build one.", "tokens": [583, 286, 2378, 380, 668, 2212, 257, 19265, 11, 286, 643, 281, 1322, 472, 13], "temperature": 0.0, "avg_logprob": -0.1873717067217586, "compression_ratio": 1.6444444444444444, "no_speech_prob": 1.9637973309727386e-06}, {"id": 796, "seek": 396790, "start": 3967.9, "end": 3972.02, "text": " So a generator, and we haven't talked about the loss function yet, we're just going to", "tokens": [407, 257, 19265, 11, 293, 321, 2378, 380, 2825, 466, 264, 4470, 2445, 1939, 11, 321, 434, 445, 516, 281], "temperature": 0.0, "avg_logprob": -0.15643718264518527, "compression_ratio": 1.7581967213114753, "no_speech_prob": 4.965273205925769e-07}, {"id": 797, "seek": 396790, "start": 3972.02, "end": 3975.6600000000003, "text": " assume that there's some loss function that does this thing.", "tokens": [6552, 300, 456, 311, 512, 4470, 2445, 300, 775, 341, 551, 13], "temperature": 0.0, "avg_logprob": -0.15643718264518527, "compression_ratio": 1.7581967213114753, "no_speech_prob": 4.965273205925769e-07}, {"id": 798, "seek": 396790, "start": 3975.6600000000003, "end": 3981.06, "text": " So a generator is also an architecture which doesn't do anything by itself until we have", "tokens": [407, 257, 19265, 307, 611, 364, 9482, 597, 1177, 380, 360, 1340, 538, 2564, 1826, 321, 362], "temperature": 0.0, "avg_logprob": -0.15643718264518527, "compression_ratio": 1.7581967213114753, "no_speech_prob": 4.965273205925769e-07}, {"id": 799, "seek": 396790, "start": 3981.06, "end": 3983.38, "text": " a loss function and data.", "tokens": [257, 4470, 2445, 293, 1412, 13], "temperature": 0.0, "avg_logprob": -0.15643718264518527, "compression_ratio": 1.7581967213114753, "no_speech_prob": 4.965273205925769e-07}, {"id": 800, "seek": 396790, "start": 3983.38, "end": 3987.26, "text": " But what are the ranks and sizes of the tensors?", "tokens": [583, 437, 366, 264, 21406, 293, 11602, 295, 264, 10688, 830, 30], "temperature": 0.0, "avg_logprob": -0.15643718264518527, "compression_ratio": 1.7581967213114753, "no_speech_prob": 4.965273205925769e-07}, {"id": 801, "seek": 396790, "start": 3987.26, "end": 3994.1800000000003, "text": " Well the input to the generator is going to be a vector of random numbers.", "tokens": [1042, 264, 4846, 281, 264, 19265, 307, 516, 281, 312, 257, 8062, 295, 4974, 3547, 13], "temperature": 0.0, "avg_logprob": -0.15643718264518527, "compression_ratio": 1.7581967213114753, "no_speech_prob": 4.965273205925769e-07}, {"id": 802, "seek": 396790, "start": 3994.1800000000003, "end": 3995.82, "text": " And in the paper they call that the prior.", "tokens": [400, 294, 264, 3035, 436, 818, 300, 264, 4059, 13], "temperature": 0.0, "avg_logprob": -0.15643718264518527, "compression_ratio": 1.7581967213114753, "no_speech_prob": 4.965273205925769e-07}, {"id": 803, "seek": 399582, "start": 3995.82, "end": 3998.34, "text": " It's going to be a vector of random numbers.", "tokens": [467, 311, 516, 281, 312, 257, 8062, 295, 4974, 3547, 13], "temperature": 0.0, "avg_logprob": -0.1771034605047676, "compression_ratio": 1.6067415730337078, "no_speech_prob": 5.014718681195518e-06}, {"id": 804, "seek": 399582, "start": 3998.34, "end": 3999.34, "text": " How big?", "tokens": [1012, 955, 30], "temperature": 0.0, "avg_logprob": -0.1771034605047676, "compression_ratio": 1.6067415730337078, "no_speech_prob": 5.014718681195518e-06}, {"id": 805, "seek": 399582, "start": 3999.34, "end": 4000.5800000000004, "text": " I don't know.", "tokens": [286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.1771034605047676, "compression_ratio": 1.6067415730337078, "no_speech_prob": 5.014718681195518e-06}, {"id": 806, "seek": 399582, "start": 4000.5800000000004, "end": 4001.5800000000004, "text": " Some big.", "tokens": [2188, 955, 13], "temperature": 0.0, "avg_logprob": -0.1771034605047676, "compression_ratio": 1.6067415730337078, "no_speech_prob": 5.014718681195518e-06}, {"id": 807, "seek": 399582, "start": 4001.5800000000004, "end": 4002.5800000000004, "text": " 64, 128.", "tokens": [12145, 11, 29810, 13], "temperature": 0.0, "avg_logprob": -0.1771034605047676, "compression_ratio": 1.6067415730337078, "no_speech_prob": 5.014718681195518e-06}, {"id": 808, "seek": 399582, "start": 4002.5800000000004, "end": 4008.46, "text": " And the idea is that a different bunch of random numbers will generate a different bedroom.", "tokens": [400, 264, 1558, 307, 300, 257, 819, 3840, 295, 4974, 3547, 486, 8460, 257, 819, 11211, 13], "temperature": 0.0, "avg_logprob": -0.1771034605047676, "compression_ratio": 1.6067415730337078, "no_speech_prob": 5.014718681195518e-06}, {"id": 809, "seek": 399582, "start": 4008.46, "end": 4010.6400000000003, "text": " So that's the idea.", "tokens": [407, 300, 311, 264, 1558, 13], "temperature": 0.0, "avg_logprob": -0.1771034605047676, "compression_ratio": 1.6067415730337078, "no_speech_prob": 5.014718681195518e-06}, {"id": 810, "seek": 399582, "start": 4010.6400000000003, "end": 4022.6600000000003, "text": " So our GAN generator has to take as input a vector, and it's going to take that vector,", "tokens": [407, 527, 460, 1770, 19265, 575, 281, 747, 382, 4846, 257, 8062, 11, 293, 309, 311, 516, 281, 747, 300, 8062, 11], "temperature": 0.0, "avg_logprob": -0.1771034605047676, "compression_ratio": 1.6067415730337078, "no_speech_prob": 5.014718681195518e-06}, {"id": 811, "seek": 402266, "start": 4022.66, "end": 4029.3399999999997, "text": " so here's our input, and it's going to stick it through, in this case a sequential model.", "tokens": [370, 510, 311, 527, 4846, 11, 293, 309, 311, 516, 281, 2897, 309, 807, 11, 294, 341, 1389, 257, 42881, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1764599525765197, "compression_ratio": 1.639751552795031, "no_speech_prob": 4.289318439987255e-06}, {"id": 812, "seek": 402266, "start": 4029.3399999999997, "end": 4040.5, "text": " And the sequential model is going to take that vector and it's going to turn it into", "tokens": [400, 264, 42881, 2316, 307, 516, 281, 747, 300, 8062, 293, 309, 311, 516, 281, 1261, 309, 666], "temperature": 0.0, "avg_logprob": -0.1764599525765197, "compression_ratio": 1.639751552795031, "no_speech_prob": 4.289318439987255e-06}, {"id": 813, "seek": 402266, "start": 4040.5, "end": 4050.74, "text": " a rank 4 tensor, or if you take off the batch bit, a rank 3 tensor, height by width by 3.", "tokens": [257, 6181, 1017, 40863, 11, 420, 498, 291, 747, 766, 264, 15245, 857, 11, 257, 6181, 805, 40863, 11, 6681, 538, 11402, 538, 805, 13], "temperature": 0.0, "avg_logprob": -0.1764599525765197, "compression_ratio": 1.639751552795031, "no_speech_prob": 4.289318439987255e-06}, {"id": 814, "seek": 405074, "start": 4050.74, "end": 4061.02, "text": " So you can see at the end here, our final step, nc number of channels.", "tokens": [407, 291, 393, 536, 412, 264, 917, 510, 11, 527, 2572, 1823, 11, 297, 66, 1230, 295, 9235, 13], "temperature": 0.0, "avg_logprob": -0.3418831579463998, "compression_ratio": 1.4682926829268292, "no_speech_prob": 2.6016070933110313e-06}, {"id": 815, "seek": 405074, "start": 4061.02, "end": 4065.02, "text": " So I think that's going to have to end up being 3 because we're going to create a 3-channel", "tokens": [407, 286, 519, 300, 311, 516, 281, 362, 281, 917, 493, 885, 805, 570, 321, 434, 516, 281, 1884, 257, 805, 12, 339, 11444], "temperature": 0.0, "avg_logprob": -0.3418831579463998, "compression_ratio": 1.4682926829268292, "no_speech_prob": 2.6016070933110313e-06}, {"id": 816, "seek": 405074, "start": 4065.02, "end": 4068.8599999999997, "text": " image of some size.", "tokens": [3256, 295, 512, 2744, 13], "temperature": 0.0, "avg_logprob": -0.3418831579463998, "compression_ratio": 1.4682926829268292, "no_speech_prob": 2.6016070933110313e-06}, {"id": 817, "seek": 405074, "start": 4068.8599999999997, "end": 4071.3799999999997, "text": " Yes, Richard?", "tokens": [1079, 11, 9809, 30], "temperature": 0.0, "avg_logprob": -0.3418831579463998, "compression_ratio": 1.4682926829268292, "no_speech_prob": 2.6016070933110313e-06}, {"id": 818, "seek": 405074, "start": 4071.3799999999997, "end": 4078.14, "text": " In comf block forward, is there a reason why batch norm comes after relu, i.e. self.batchnorm.self.relu?", "tokens": [682, 395, 69, 3461, 2128, 11, 307, 456, 257, 1778, 983, 15245, 2026, 1487, 934, 1039, 84, 11, 741, 13, 68, 13, 2698, 13, 65, 852, 13403, 13, 927, 13, 4419, 84, 30], "temperature": 0.0, "avg_logprob": -0.3418831579463998, "compression_ratio": 1.4682926829268292, "no_speech_prob": 2.6016070933110313e-06}, {"id": 819, "seek": 407814, "start": 4078.14, "end": 4080.98, "text": " No, there's not.", "tokens": [883, 11, 456, 311, 406, 13], "temperature": 0.0, "avg_logprob": -0.2238553518272308, "compression_ratio": 1.443298969072165, "no_speech_prob": 8.139656529237982e-06}, {"id": 820, "seek": 407814, "start": 4080.98, "end": 4084.66, "text": " It's just what they had in the code I borrowed from, I think.", "tokens": [467, 311, 445, 437, 436, 632, 294, 264, 3089, 286, 26805, 490, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.2238553518272308, "compression_ratio": 1.443298969072165, "no_speech_prob": 8.139656529237982e-06}, {"id": 821, "seek": 407814, "start": 4084.66, "end": 4088.22, "text": " In ResNet, the order is reversed.", "tokens": [682, 5015, 31890, 11, 264, 1668, 307, 30563, 13], "temperature": 0.0, "avg_logprob": -0.2238553518272308, "compression_ratio": 1.443298969072165, "no_speech_prob": 8.139656529237982e-06}, {"id": 822, "seek": 407814, "start": 4088.22, "end": 4097.7, "text": " So again, unless my intuition about GANs is all wrong and for some reason need to be different", "tokens": [407, 797, 11, 5969, 452, 24002, 466, 460, 1770, 82, 307, 439, 2085, 293, 337, 512, 1778, 643, 281, 312, 819], "temperature": 0.0, "avg_logprob": -0.2238553518272308, "compression_ratio": 1.443298969072165, "no_speech_prob": 8.139656529237982e-06}, {"id": 823, "seek": 407814, "start": 4097.7, "end": 4107.98, "text": " to what I'm used to, I would normally expect to go relu then batch norm.", "tokens": [281, 437, 286, 478, 1143, 281, 11, 286, 576, 5646, 2066, 281, 352, 1039, 84, 550, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.2238553518272308, "compression_ratio": 1.443298969072165, "no_speech_prob": 8.139656529237982e-06}, {"id": 824, "seek": 410798, "start": 4107.98, "end": 4113.66, "text": " This is actually the order that makes more sense to me.", "tokens": [639, 307, 767, 264, 1668, 300, 1669, 544, 2020, 281, 385, 13], "temperature": 0.0, "avg_logprob": -0.13901997789924528, "compression_ratio": 1.577319587628866, "no_speech_prob": 1.1300734513497446e-05}, {"id": 825, "seek": 410798, "start": 4113.66, "end": 4120.259999999999, "text": " But I think the order I had in the darknet was what they used in the darknet paper.", "tokens": [583, 286, 519, 264, 1668, 286, 632, 294, 264, 2877, 7129, 390, 437, 436, 1143, 294, 264, 2877, 7129, 3035, 13], "temperature": 0.0, "avg_logprob": -0.13901997789924528, "compression_ratio": 1.577319587628866, "no_speech_prob": 1.1300734513497446e-05}, {"id": 826, "seek": 410798, "start": 4120.259999999999, "end": 4125.219999999999, "text": " So I don't know, everybody seems to have a different order of these things.", "tokens": [407, 286, 500, 380, 458, 11, 2201, 2544, 281, 362, 257, 819, 1668, 295, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.13901997789924528, "compression_ratio": 1.577319587628866, "no_speech_prob": 1.1300734513497446e-05}, {"id": 827, "seek": 410798, "start": 4125.219999999999, "end": 4132.419999999999, "text": " And in fact, most people for CyPhar10 have a different order again, which is they actually", "tokens": [400, 294, 1186, 11, 881, 561, 337, 10295, 47, 5854, 3279, 362, 257, 819, 1668, 797, 11, 597, 307, 436, 767], "temperature": 0.0, "avg_logprob": -0.13901997789924528, "compression_ratio": 1.577319587628866, "no_speech_prob": 1.1300734513497446e-05}, {"id": 828, "seek": 413242, "start": 4132.42, "end": 4142.18, "text": " go bn then relu then conv, which is kind of a quirky way of thinking about it.", "tokens": [352, 272, 77, 550, 1039, 84, 550, 3754, 11, 597, 307, 733, 295, 257, 49515, 636, 295, 1953, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.1658464272816976, "compression_ratio": 1.560747663551402, "no_speech_prob": 6.962200131965801e-06}, {"id": 829, "seek": 413242, "start": 4142.18, "end": 4146.74, "text": " But it turns out that often for residual blocks, that works better.", "tokens": [583, 309, 4523, 484, 300, 2049, 337, 27980, 8474, 11, 300, 1985, 1101, 13], "temperature": 0.0, "avg_logprob": -0.1658464272816976, "compression_ratio": 1.560747663551402, "no_speech_prob": 6.962200131965801e-06}, {"id": 830, "seek": 413242, "start": 4146.74, "end": 4148.46, "text": " That's called a pre-activation ResNet.", "tokens": [663, 311, 1219, 257, 659, 12, 23397, 399, 5015, 31890, 13], "temperature": 0.0, "avg_logprob": -0.1658464272816976, "compression_ratio": 1.560747663551402, "no_speech_prob": 6.962200131965801e-06}, {"id": 831, "seek": 413242, "start": 4148.46, "end": 4153.58, "text": " So if you Google for pre-activation ResNet, you can see that.", "tokens": [407, 498, 291, 3329, 337, 659, 12, 23397, 399, 5015, 31890, 11, 291, 393, 536, 300, 13], "temperature": 0.0, "avg_logprob": -0.1658464272816976, "compression_ratio": 1.560747663551402, "no_speech_prob": 6.962200131965801e-06}, {"id": 832, "seek": 413242, "start": 4153.58, "end": 4158.34, "text": " So yeah, there's a few, not so much papers, but more blog posts out there where people", "tokens": [407, 1338, 11, 456, 311, 257, 1326, 11, 406, 370, 709, 10577, 11, 457, 544, 6968, 12300, 484, 456, 689, 561], "temperature": 0.0, "avg_logprob": -0.1658464272816976, "compression_ratio": 1.560747663551402, "no_speech_prob": 6.962200131965801e-06}, {"id": 833, "seek": 415834, "start": 4158.34, "end": 4163.58, "text": " have experimented with different orders of those things, and it seems to depend a lot", "tokens": [362, 5120, 292, 365, 819, 9470, 295, 729, 721, 11, 293, 309, 2544, 281, 5672, 257, 688], "temperature": 0.0, "avg_logprob": -0.1444928438767143, "compression_ratio": 1.645021645021645, "no_speech_prob": 4.029433057439746e-06}, {"id": 834, "seek": 415834, "start": 4163.58, "end": 4169.38, "text": " on what specific dataset it is and what you're doing with, although in general the difference", "tokens": [322, 437, 2685, 28872, 309, 307, 293, 437, 291, 434, 884, 365, 11, 4878, 294, 2674, 264, 2649], "temperature": 0.0, "avg_logprob": -0.1444928438767143, "compression_ratio": 1.645021645021645, "no_speech_prob": 4.029433057439746e-06}, {"id": 835, "seek": 415834, "start": 4169.38, "end": 4176.9800000000005, "text": " in performance is small enough you won't care unless it's through a competition.", "tokens": [294, 3389, 307, 1359, 1547, 291, 1582, 380, 1127, 5969, 309, 311, 807, 257, 6211, 13], "temperature": 0.0, "avg_logprob": -0.1444928438767143, "compression_ratio": 1.645021645021645, "no_speech_prob": 4.029433057439746e-06}, {"id": 836, "seek": 415834, "start": 4176.9800000000005, "end": 4184.860000000001, "text": " So the generator needs to start with a vector and end up with a rank 3 tensor.", "tokens": [407, 264, 19265, 2203, 281, 722, 365, 257, 8062, 293, 917, 493, 365, 257, 6181, 805, 40863, 13], "temperature": 0.0, "avg_logprob": -0.1444928438767143, "compression_ratio": 1.645021645021645, "no_speech_prob": 4.029433057439746e-06}, {"id": 837, "seek": 415834, "start": 4184.860000000001, "end": 4187.18, "text": " We don't really know how to do that yet.", "tokens": [492, 500, 380, 534, 458, 577, 281, 360, 300, 1939, 13], "temperature": 0.0, "avg_logprob": -0.1444928438767143, "compression_ratio": 1.645021645021645, "no_speech_prob": 4.029433057439746e-06}, {"id": 838, "seek": 418718, "start": 4187.18, "end": 4188.780000000001, "text": " So how do we do that?", "tokens": [407, 577, 360, 321, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.15838267586447977, "compression_ratio": 1.5093167701863355, "no_speech_prob": 1.1125483069918118e-05}, {"id": 839, "seek": 418718, "start": 4188.780000000001, "end": 4194.900000000001, "text": " How do we start with a vector and turn it into a rank 3 tensor?", "tokens": [1012, 360, 321, 722, 365, 257, 8062, 293, 1261, 309, 666, 257, 6181, 805, 40863, 30], "temperature": 0.0, "avg_logprob": -0.15838267586447977, "compression_ratio": 1.5093167701863355, "no_speech_prob": 1.1125483069918118e-05}, {"id": 840, "seek": 418718, "start": 4194.900000000001, "end": 4199.26, "text": " We need to use something called a deconvolution.", "tokens": [492, 643, 281, 764, 746, 1219, 257, 979, 266, 85, 3386, 13], "temperature": 0.0, "avg_logprob": -0.15838267586447977, "compression_ratio": 1.5093167701863355, "no_speech_prob": 1.1125483069918118e-05}, {"id": 841, "seek": 418718, "start": 4199.26, "end": 4208.02, "text": " And a deconvolution is, or as they call it in PyTorch, a transposed convolution.", "tokens": [400, 257, 979, 266, 85, 3386, 307, 11, 420, 382, 436, 818, 309, 294, 9953, 51, 284, 339, 11, 257, 7132, 1744, 45216, 13], "temperature": 0.0, "avg_logprob": -0.15838267586447977, "compression_ratio": 1.5093167701863355, "no_speech_prob": 1.1125483069918118e-05}, {"id": 842, "seek": 418718, "start": 4208.02, "end": 4212.14, "text": " Same thing, different name.", "tokens": [10635, 551, 11, 819, 1315, 13], "temperature": 0.0, "avg_logprob": -0.15838267586447977, "compression_ratio": 1.5093167701863355, "no_speech_prob": 1.1125483069918118e-05}, {"id": 843, "seek": 421214, "start": 4212.14, "end": 4220.820000000001, "text": " And so a deconvolution is something which rather than decreasing the grid size, it increases", "tokens": [400, 370, 257, 979, 266, 85, 3386, 307, 746, 597, 2831, 813, 23223, 264, 10748, 2744, 11, 309, 8637], "temperature": 0.0, "avg_logprob": -0.12306023836135864, "compression_ratio": 1.6114285714285714, "no_speech_prob": 4.029441697639413e-06}, {"id": 844, "seek": 421214, "start": 4220.820000000001, "end": 4222.96, "text": " the grid size.", "tokens": [264, 10748, 2744, 13], "temperature": 0.0, "avg_logprob": -0.12306023836135864, "compression_ratio": 1.6114285714285714, "no_speech_prob": 4.029441697639413e-06}, {"id": 845, "seek": 421214, "start": 4222.96, "end": 4228.820000000001, "text": " So as with all things, it's easiest to see in an Excel spreadsheet.", "tokens": [407, 382, 365, 439, 721, 11, 309, 311, 12889, 281, 536, 294, 364, 19060, 27733, 13], "temperature": 0.0, "avg_logprob": -0.12306023836135864, "compression_ratio": 1.6114285714285714, "no_speech_prob": 4.029441697639413e-06}, {"id": 846, "seek": 421214, "start": 4228.820000000001, "end": 4230.700000000001, "text": " So here's a convolution.", "tokens": [407, 510, 311, 257, 45216, 13], "temperature": 0.0, "avg_logprob": -0.12306023836135864, "compression_ratio": 1.6114285714285714, "no_speech_prob": 4.029441697639413e-06}, {"id": 847, "seek": 421214, "start": 4230.700000000001, "end": 4238.3, "text": " We start, let's say, with a 4x4 grid cell with a single channel, a single filter.", "tokens": [492, 722, 11, 718, 311, 584, 11, 365, 257, 1017, 87, 19, 10748, 2815, 365, 257, 2167, 2269, 11, 257, 2167, 6608, 13], "temperature": 0.0, "avg_logprob": -0.12306023836135864, "compression_ratio": 1.6114285714285714, "no_speech_prob": 4.029441697639413e-06}, {"id": 848, "seek": 423830, "start": 4238.3, "end": 4244.26, "text": " And let's put it through a 3x3 kernel, again with a single output filter.", "tokens": [400, 718, 311, 829, 309, 807, 257, 805, 87, 18, 28256, 11, 797, 365, 257, 2167, 5598, 6608, 13], "temperature": 0.0, "avg_logprob": -0.1429351013485748, "compression_ratio": 1.653061224489796, "no_speech_prob": 5.771900760009885e-06}, {"id": 849, "seek": 423830, "start": 4244.26, "end": 4251.9800000000005, "text": " So we've got a single channel in, a single filter kernel, and so if we don't add any", "tokens": [407, 321, 600, 658, 257, 2167, 2269, 294, 11, 257, 2167, 6608, 28256, 11, 293, 370, 498, 321, 500, 380, 909, 604], "temperature": 0.0, "avg_logprob": -0.1429351013485748, "compression_ratio": 1.653061224489796, "no_speech_prob": 5.771900760009885e-06}, {"id": 850, "seek": 423830, "start": 4251.9800000000005, "end": 4260.06, "text": " padding we're going to end up with 2x2, because that 3x3 can go in 1, 2, 3, 4 places.", "tokens": [39562, 321, 434, 516, 281, 917, 493, 365, 568, 87, 17, 11, 570, 300, 805, 87, 18, 393, 352, 294, 502, 11, 568, 11, 805, 11, 1017, 3190, 13], "temperature": 0.0, "avg_logprob": -0.1429351013485748, "compression_ratio": 1.653061224489796, "no_speech_prob": 5.771900760009885e-06}, {"id": 851, "seek": 423830, "start": 4260.06, "end": 4266.9400000000005, "text": " It can go in 1 of 2 places across and 1 of 2 places down if there's no padding.", "tokens": [467, 393, 352, 294, 502, 295, 568, 3190, 2108, 293, 502, 295, 568, 3190, 760, 498, 456, 311, 572, 39562, 13], "temperature": 0.0, "avg_logprob": -0.1429351013485748, "compression_ratio": 1.653061224489796, "no_speech_prob": 5.771900760009885e-06}, {"id": 852, "seek": 426694, "start": 4266.94, "end": 4270.5, "text": " So there's our convolution.", "tokens": [407, 456, 311, 527, 45216, 13], "temperature": 0.0, "avg_logprob": -0.15756004040057842, "compression_ratio": 1.5354838709677419, "no_speech_prob": 8.851557140587829e-07}, {"id": 853, "seek": 426694, "start": 4270.5, "end": 4276.099999999999, "text": " Remember the convolution is just the sum of the product of the kernel and the appropriate", "tokens": [5459, 264, 45216, 307, 445, 264, 2408, 295, 264, 1674, 295, 264, 28256, 293, 264, 6854], "temperature": 0.0, "avg_logprob": -0.15756004040057842, "compression_ratio": 1.5354838709677419, "no_speech_prob": 8.851557140587829e-07}, {"id": 854, "seek": 426694, "start": 4276.099999999999, "end": 4277.339999999999, "text": " grid cell.", "tokens": [10748, 2815, 13], "temperature": 0.0, "avg_logprob": -0.15756004040057842, "compression_ratio": 1.5354838709677419, "no_speech_prob": 8.851557140587829e-07}, {"id": 855, "seek": 426694, "start": 4277.339999999999, "end": 4285.5199999999995, "text": " So there's our standard 3x3 upon 1 channel, 1 filter.", "tokens": [407, 456, 311, 527, 3832, 805, 87, 18, 3564, 502, 2269, 11, 502, 6608, 13], "temperature": 0.0, "avg_logprob": -0.15756004040057842, "compression_ratio": 1.5354838709677419, "no_speech_prob": 8.851557140587829e-07}, {"id": 856, "seek": 426694, "start": 4285.5199999999995, "end": 4288.62, "text": " So the idea now is I want to go the opposite direction.", "tokens": [407, 264, 1558, 586, 307, 286, 528, 281, 352, 264, 6182, 3513, 13], "temperature": 0.0, "avg_logprob": -0.15756004040057842, "compression_ratio": 1.5354838709677419, "no_speech_prob": 8.851557140587829e-07}, {"id": 857, "seek": 428862, "start": 4288.62, "end": 4298.0599999999995, "text": " I want to start with my 2x2 and I want to create a 4x4.", "tokens": [286, 528, 281, 722, 365, 452, 568, 87, 17, 293, 286, 528, 281, 1884, 257, 1017, 87, 19, 13], "temperature": 0.0, "avg_logprob": -0.08985345442216475, "compression_ratio": 1.6706586826347305, "no_speech_prob": 7.453759849340713e-07}, {"id": 858, "seek": 428862, "start": 4298.0599999999995, "end": 4302.66, "text": " And specifically I want to create the same 4x4 that I started with.", "tokens": [400, 4682, 286, 528, 281, 1884, 264, 912, 1017, 87, 19, 300, 286, 1409, 365, 13], "temperature": 0.0, "avg_logprob": -0.08985345442216475, "compression_ratio": 1.6706586826347305, "no_speech_prob": 7.453759849340713e-07}, {"id": 859, "seek": 428862, "start": 4302.66, "end": 4305.88, "text": " And I want to do that by using a convolution.", "tokens": [400, 286, 528, 281, 360, 300, 538, 1228, 257, 45216, 13], "temperature": 0.0, "avg_logprob": -0.08985345442216475, "compression_ratio": 1.6706586826347305, "no_speech_prob": 7.453759849340713e-07}, {"id": 860, "seek": 428862, "start": 4305.88, "end": 4307.599999999999, "text": " So how would I do that?", "tokens": [407, 577, 576, 286, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.08985345442216475, "compression_ratio": 1.6706586826347305, "no_speech_prob": 7.453759849340713e-07}, {"id": 861, "seek": 428862, "start": 4307.599999999999, "end": 4313.9, "text": " Well if I have a 3x3 convolution, then if I want to create a 4x4 output, I'm going to", "tokens": [1042, 498, 286, 362, 257, 805, 87, 18, 45216, 11, 550, 498, 286, 528, 281, 1884, 257, 1017, 87, 19, 5598, 11, 286, 478, 516, 281], "temperature": 0.0, "avg_logprob": -0.08985345442216475, "compression_ratio": 1.6706586826347305, "no_speech_prob": 7.453759849340713e-07}, {"id": 862, "seek": 431390, "start": 4313.9, "end": 4319.86, "text": " need to create this much padding.", "tokens": [643, 281, 1884, 341, 709, 39562, 13], "temperature": 0.0, "avg_logprob": -0.16836522925983777, "compression_ratio": 1.516304347826087, "no_speech_prob": 7.690367169743695e-07}, {"id": 863, "seek": 431390, "start": 4319.86, "end": 4330.42, "text": " Because with this much padding, I'm going to end up with 1, 2, 3, 4 by 1, 2, 3, 4.", "tokens": [1436, 365, 341, 709, 39562, 11, 286, 478, 516, 281, 917, 493, 365, 502, 11, 568, 11, 805, 11, 1017, 538, 502, 11, 568, 11, 805, 11, 1017, 13], "temperature": 0.0, "avg_logprob": -0.16836522925983777, "compression_ratio": 1.516304347826087, "no_speech_prob": 7.690367169743695e-07}, {"id": 864, "seek": 431390, "start": 4330.42, "end": 4331.42, "text": " You see why that is?", "tokens": [509, 536, 983, 300, 307, 30], "temperature": 0.0, "avg_logprob": -0.16836522925983777, "compression_ratio": 1.516304347826087, "no_speech_prob": 7.690367169743695e-07}, {"id": 865, "seek": 431390, "start": 4331.42, "end": 4338.46, "text": " So this filter can go in any one of 4 places across and 4 places up and down.", "tokens": [407, 341, 6608, 393, 352, 294, 604, 472, 295, 1017, 3190, 2108, 293, 1017, 3190, 493, 293, 760, 13], "temperature": 0.0, "avg_logprob": -0.16836522925983777, "compression_ratio": 1.516304347826087, "no_speech_prob": 7.690367169743695e-07}, {"id": 866, "seek": 431390, "start": 4338.46, "end": 4342.219999999999, "text": " So let's say my convolutional filter was just a bunch of zeros.", "tokens": [407, 718, 311, 584, 452, 45216, 304, 6608, 390, 445, 257, 3840, 295, 35193, 13], "temperature": 0.0, "avg_logprob": -0.16836522925983777, "compression_ratio": 1.516304347826087, "no_speech_prob": 7.690367169743695e-07}, {"id": 867, "seek": 434222, "start": 4342.22, "end": 4349.3, "text": " And I can calculate my error for each cell by taking this attraction.", "tokens": [400, 286, 393, 8873, 452, 6713, 337, 1184, 2815, 538, 1940, 341, 17672, 13], "temperature": 0.0, "avg_logprob": -0.15584525546512087, "compression_ratio": 1.470899470899471, "no_speech_prob": 2.225278421974508e-06}, {"id": 868, "seek": 434222, "start": 4349.3, "end": 4355.740000000001, "text": " And then I can get the sum of absolute values, the L1 loss, by just summing up the absolute", "tokens": [400, 550, 286, 393, 483, 264, 2408, 295, 8236, 4190, 11, 264, 441, 16, 4470, 11, 538, 445, 2408, 2810, 493, 264, 8236], "temperature": 0.0, "avg_logprob": -0.15584525546512087, "compression_ratio": 1.470899470899471, "no_speech_prob": 2.225278421974508e-06}, {"id": 869, "seek": 434222, "start": 4355.740000000001, "end": 4358.34, "text": " values of those errors.", "tokens": [4190, 295, 729, 13603, 13], "temperature": 0.0, "avg_logprob": -0.15584525546512087, "compression_ratio": 1.470899470899471, "no_speech_prob": 2.225278421974508e-06}, {"id": 870, "seek": 434222, "start": 4358.34, "end": 4368.9400000000005, "text": " So now I could use optimization, so in Excel that's called solver, to do a gradient descent.", "tokens": [407, 586, 286, 727, 764, 19618, 11, 370, 294, 19060, 300, 311, 1219, 1404, 331, 11, 281, 360, 257, 16235, 23475, 13], "temperature": 0.0, "avg_logprob": -0.15584525546512087, "compression_ratio": 1.470899470899471, "no_speech_prob": 2.225278421974508e-06}, {"id": 871, "seek": 436894, "start": 4368.94, "end": 4376.419999999999, "text": " So I'm going to set that cell equal to a minimum, I'm going to try and reduce my loss, by changing", "tokens": [407, 286, 478, 516, 281, 992, 300, 2815, 2681, 281, 257, 7285, 11, 286, 478, 516, 281, 853, 293, 5407, 452, 4470, 11, 538, 4473], "temperature": 0.0, "avg_logprob": -0.2201404571533203, "compression_ratio": 1.4953271028037383, "no_speech_prob": 3.844929324259283e-06}, {"id": 872, "seek": 436894, "start": 4376.419999999999, "end": 4377.419999999999, "text": " my filter.", "tokens": [452, 6608, 13], "temperature": 0.0, "avg_logprob": -0.2201404571533203, "compression_ratio": 1.4953271028037383, "no_speech_prob": 3.844929324259283e-06}, {"id": 873, "seek": 436894, "start": 4377.419999999999, "end": 4382.419999999999, "text": " And I'll go solve.", "tokens": [400, 286, 603, 352, 5039, 13], "temperature": 0.0, "avg_logprob": -0.2201404571533203, "compression_ratio": 1.4953271028037383, "no_speech_prob": 3.844929324259283e-06}, {"id": 874, "seek": 436894, "start": 4382.419999999999, "end": 4388.74, "text": " And you can see it's come up with a filter such that 15.7 compared to 16, 17 is right,", "tokens": [400, 291, 393, 536, 309, 311, 808, 493, 365, 257, 6608, 1270, 300, 2119, 13, 22, 5347, 281, 3165, 11, 3282, 307, 558, 11], "temperature": 0.0, "avg_logprob": -0.2201404571533203, "compression_ratio": 1.4953271028037383, "no_speech_prob": 3.844929324259283e-06}, {"id": 875, "seek": 436894, "start": 4388.74, "end": 4389.74, "text": " 17.8, 19.8.", "tokens": [3282, 13, 23, 11, 1294, 13, 23, 13], "temperature": 0.0, "avg_logprob": -0.2201404571533203, "compression_ratio": 1.4953271028037383, "no_speech_prob": 3.844929324259283e-06}, {"id": 876, "seek": 436894, "start": 4389.74, "end": 4396.62, "text": " So it's not perfect, and in general you can't assume that a deconvolution can exactly create", "tokens": [407, 309, 311, 406, 2176, 11, 293, 294, 2674, 291, 393, 380, 6552, 300, 257, 979, 266, 85, 3386, 393, 2293, 1884], "temperature": 0.0, "avg_logprob": -0.2201404571533203, "compression_ratio": 1.4953271028037383, "no_speech_prob": 3.844929324259283e-06}, {"id": 877, "seek": 439662, "start": 4396.62, "end": 4403.46, "text": " the same, the exact thing that you want, because there's just not enough, there's only 9 things", "tokens": [264, 912, 11, 264, 1900, 551, 300, 291, 528, 11, 570, 456, 311, 445, 406, 1547, 11, 456, 311, 787, 1722, 721], "temperature": 0.0, "avg_logprob": -0.17721122919127, "compression_ratio": 1.4759358288770053, "no_speech_prob": 5.014710495743202e-06}, {"id": 878, "seek": 439662, "start": 4403.46, "end": 4406.54, "text": " here and 16 things you're trying to create.", "tokens": [510, 293, 3165, 721, 291, 434, 1382, 281, 1884, 13], "temperature": 0.0, "avg_logprob": -0.17721122919127, "compression_ratio": 1.4759358288770053, "no_speech_prob": 5.014710495743202e-06}, {"id": 879, "seek": 439662, "start": 4406.54, "end": 4409.9, "text": " But it's made a pretty good attempt.", "tokens": [583, 309, 311, 1027, 257, 1238, 665, 5217, 13], "temperature": 0.0, "avg_logprob": -0.17721122919127, "compression_ratio": 1.4759358288770053, "no_speech_prob": 5.014710495743202e-06}, {"id": 880, "seek": 439662, "start": 4409.9, "end": 4420.0199999999995, "text": " So this is what a deconvolution looks like, a stride 1, 3x3 deconvolution on a 2x2 grid", "tokens": [407, 341, 307, 437, 257, 979, 266, 85, 3386, 1542, 411, 11, 257, 1056, 482, 502, 11, 805, 87, 18, 979, 266, 85, 3386, 322, 257, 568, 87, 17, 10748], "temperature": 0.0, "avg_logprob": -0.17721122919127, "compression_ratio": 1.4759358288770053, "no_speech_prob": 5.014710495743202e-06}, {"id": 881, "seek": 439662, "start": 4420.0199999999995, "end": 4424.099999999999, "text": " cell input.", "tokens": [2815, 4846, 13], "temperature": 0.0, "avg_logprob": -0.17721122919127, "compression_ratio": 1.4759358288770053, "no_speech_prob": 5.014710495743202e-06}, {"id": 882, "seek": 442410, "start": 4424.1, "end": 4431.700000000001, "text": " How difficult is it to create a discriminator to identify fake news versus real news?", "tokens": [1012, 2252, 307, 309, 281, 1884, 257, 20828, 1639, 281, 5876, 7592, 2583, 5717, 957, 2583, 30], "temperature": 0.0, "avg_logprob": -0.18855340981189114, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.4970711163186934e-05}, {"id": 883, "seek": 442410, "start": 4431.700000000001, "end": 4435.26, "text": " You don't need anything special, that's just a classifier.", "tokens": [509, 500, 380, 643, 1340, 2121, 11, 300, 311, 445, 257, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.18855340981189114, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.4970711163186934e-05}, {"id": 884, "seek": 442410, "start": 4435.26, "end": 4443.02, "text": " So you would just use the NLP classifier from previous to previous class and lesson 4.", "tokens": [407, 291, 576, 445, 764, 264, 426, 45196, 1508, 9902, 490, 3894, 281, 3894, 1508, 293, 6898, 1017, 13], "temperature": 0.0, "avg_logprob": -0.18855340981189114, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.4970711163186934e-05}, {"id": 885, "seek": 442410, "start": 4443.02, "end": 4451.42, "text": " In that case, there's no generative piece, so you just need the data set that says these", "tokens": [682, 300, 1389, 11, 456, 311, 572, 1337, 1166, 2522, 11, 370, 291, 445, 643, 264, 1412, 992, 300, 1619, 613], "temperature": 0.0, "avg_logprob": -0.18855340981189114, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.4970711163186934e-05}, {"id": 886, "seek": 445142, "start": 4451.42, "end": 4454.46, "text": " are the things that we believe are fake news and these are the things we consider to be", "tokens": [366, 264, 721, 300, 321, 1697, 366, 7592, 2583, 293, 613, 366, 264, 721, 321, 1949, 281, 312], "temperature": 0.0, "avg_logprob": -0.22776633572865682, "compression_ratio": 1.5654450261780104, "no_speech_prob": 2.7108439098810777e-05}, {"id": 887, "seek": 445142, "start": 4454.46, "end": 4456.46, "text": " real news.", "tokens": [957, 2583, 13], "temperature": 0.0, "avg_logprob": -0.22776633572865682, "compression_ratio": 1.5654450261780104, "no_speech_prob": 2.7108439098810777e-05}, {"id": 888, "seek": 445142, "start": 4456.46, "end": 4461.02, "text": " It should actually work very well.", "tokens": [467, 820, 767, 589, 588, 731, 13], "temperature": 0.0, "avg_logprob": -0.22776633572865682, "compression_ratio": 1.5654450261780104, "no_speech_prob": 2.7108439098810777e-05}, {"id": 889, "seek": 445142, "start": 4461.02, "end": 4466.46, "text": " To the best of my knowledge, if you try it, you should get as good a result as anybody", "tokens": [1407, 264, 1151, 295, 452, 3601, 11, 498, 291, 853, 309, 11, 291, 820, 483, 382, 665, 257, 1874, 382, 4472], "temperature": 0.0, "avg_logprob": -0.22776633572865682, "compression_ratio": 1.5654450261780104, "no_speech_prob": 2.7108439098810777e-05}, {"id": 890, "seek": 445142, "start": 4466.46, "end": 4467.7, "text": " else has got.", "tokens": [1646, 575, 658, 13], "temperature": 0.0, "avg_logprob": -0.22776633572865682, "compression_ratio": 1.5654450261780104, "no_speech_prob": 2.7108439098810777e-05}, {"id": 891, "seek": 445142, "start": 4467.7, "end": 4470.22, "text": " Whether it's good enough to be useful in practice, I don't know.", "tokens": [8503, 309, 311, 665, 1547, 281, 312, 4420, 294, 3124, 11, 286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.22776633572865682, "compression_ratio": 1.5654450261780104, "no_speech_prob": 2.7108439098810777e-05}, {"id": 892, "seek": 447022, "start": 4470.22, "end": 4483.66, "text": " Question asked.", "tokens": [14464, 2351, 13], "temperature": 0.0, "avg_logprob": -0.2871722635233177, "compression_ratio": 1.375796178343949, "no_speech_prob": 4.9858139391290024e-05}, {"id": 893, "seek": 447022, "start": 4483.66, "end": 4487.46, "text": " I don't think anybody in our course has tried and nobody else outside our course knows of", "tokens": [286, 500, 380, 519, 4472, 294, 527, 1164, 575, 3031, 293, 5079, 1646, 2380, 527, 1164, 3255, 295], "temperature": 0.0, "avg_logprob": -0.2871722635233177, "compression_ratio": 1.375796178343949, "no_speech_prob": 4.9858139391290024e-05}, {"id": 894, "seek": 447022, "start": 4487.46, "end": 4488.740000000001, "text": " this technique.", "tokens": [341, 6532, 13], "temperature": 0.0, "avg_logprob": -0.2871722635233177, "compression_ratio": 1.375796178343949, "no_speech_prob": 4.9858139391290024e-05}, {"id": 895, "seek": 447022, "start": 4488.740000000001, "end": 4499.62, "text": " So as we've learned, we've just had a very significant jump in NLP classification capabilities", "tokens": [407, 382, 321, 600, 3264, 11, 321, 600, 445, 632, 257, 588, 4776, 3012, 294, 426, 45196, 21538, 10862], "temperature": 0.0, "avg_logprob": -0.2871722635233177, "compression_ratio": 1.375796178343949, "no_speech_prob": 4.9858139391290024e-05}, {"id": 896, "seek": 449962, "start": 4499.62, "end": 4509.0599999999995, "text": " and I think the best you could do at this stage would be to generate a triage that says", "tokens": [293, 286, 519, 264, 1151, 291, 727, 360, 412, 341, 3233, 576, 312, 281, 8460, 257, 1376, 609, 300, 1619], "temperature": 0.0, "avg_logprob": -0.191997922261556, "compression_ratio": 1.5561497326203209, "no_speech_prob": 3.321275653433986e-05}, {"id": 897, "seek": 449962, "start": 4509.0599999999995, "end": 4515.38, "text": " these things look pretty sketchy based on how they're written and then some human could", "tokens": [613, 721, 574, 1238, 12325, 88, 2361, 322, 577, 436, 434, 3720, 293, 550, 512, 1952, 727], "temperature": 0.0, "avg_logprob": -0.191997922261556, "compression_ratio": 1.5561497326203209, "no_speech_prob": 3.321275653433986e-05}, {"id": 898, "seek": 449962, "start": 4515.38, "end": 4518.18, "text": " go and fact check them.", "tokens": [352, 293, 1186, 1520, 552, 13], "temperature": 0.0, "avg_logprob": -0.191997922261556, "compression_ratio": 1.5561497326203209, "no_speech_prob": 3.321275653433986e-05}, {"id": 899, "seek": 449962, "start": 4518.18, "end": 4524.9, "text": " An NLP classifier, an RNN can't fact check things, but it could recognize these are written", "tokens": [1107, 426, 45196, 1508, 9902, 11, 364, 45702, 45, 393, 380, 1186, 1520, 721, 11, 457, 309, 727, 5521, 613, 366, 3720], "temperature": 0.0, "avg_logprob": -0.191997922261556, "compression_ratio": 1.5561497326203209, "no_speech_prob": 3.321275653433986e-05}, {"id": 900, "seek": 452490, "start": 4524.9, "end": 4533.62, "text": " in that kind of highly popularized style which often fake news is written in, so maybe these", "tokens": [294, 300, 733, 295, 5405, 3743, 1602, 3758, 597, 2049, 7592, 2583, 307, 3720, 294, 11, 370, 1310, 613], "temperature": 0.0, "avg_logprob": -0.18303087693226489, "compression_ratio": 1.536697247706422, "no_speech_prob": 6.540315098391147e-06}, {"id": 901, "seek": 452490, "start": 4533.62, "end": 4536.0199999999995, "text": " ones are worth paying attention to.", "tokens": [2306, 366, 3163, 6229, 3202, 281, 13], "temperature": 0.0, "avg_logprob": -0.18303087693226489, "compression_ratio": 1.536697247706422, "no_speech_prob": 6.540315098391147e-06}, {"id": 902, "seek": 452490, "start": 4536.0199999999995, "end": 4542.139999999999, "text": " I think that would probably be the best you could hope for without drawing on some kind", "tokens": [286, 519, 300, 576, 1391, 312, 264, 1151, 291, 727, 1454, 337, 1553, 6316, 322, 512, 733], "temperature": 0.0, "avg_logprob": -0.18303087693226489, "compression_ratio": 1.536697247706422, "no_speech_prob": 6.540315098391147e-06}, {"id": 903, "seek": 452490, "start": 4542.139999999999, "end": 4546.379999999999, "text": " of external data sources.", "tokens": [295, 8320, 1412, 7139, 13], "temperature": 0.0, "avg_logprob": -0.18303087693226489, "compression_ratio": 1.536697247706422, "no_speech_prob": 6.540315098391147e-06}, {"id": 904, "seek": 452490, "start": 4546.379999999999, "end": 4551.46, "text": " It's important to remember that a discriminator is basically just a classifier and you don't", "tokens": [467, 311, 1021, 281, 1604, 300, 257, 20828, 1639, 307, 1936, 445, 257, 1508, 9902, 293, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.18303087693226489, "compression_ratio": 1.536697247706422, "no_speech_prob": 6.540315098391147e-06}, {"id": 905, "seek": 455146, "start": 4551.46, "end": 4561.34, "text": " need any special techniques beyond what we've already learned to do in NLP classification.", "tokens": [643, 604, 2121, 7512, 4399, 437, 321, 600, 1217, 3264, 281, 360, 294, 426, 45196, 21538, 13], "temperature": 0.0, "avg_logprob": -0.14686023924085828, "compression_ratio": 1.6018518518518519, "no_speech_prob": 3.089471874773153e-06}, {"id": 906, "seek": 455146, "start": 4561.34, "end": 4568.78, "text": " So to do that kind of deconvolution in PyTorch, just say conv transpose 2D and in the normal", "tokens": [407, 281, 360, 300, 733, 295, 979, 266, 85, 3386, 294, 9953, 51, 284, 339, 11, 445, 584, 3754, 25167, 568, 35, 293, 294, 264, 2710], "temperature": 0.0, "avg_logprob": -0.14686023924085828, "compression_ratio": 1.6018518518518519, "no_speech_prob": 3.089471874773153e-06}, {"id": 907, "seek": 455146, "start": 4568.78, "end": 4573.82, "text": " way you say the number of input channels, the number of output channels, the kernel", "tokens": [636, 291, 584, 264, 1230, 295, 4846, 9235, 11, 264, 1230, 295, 5598, 9235, 11, 264, 28256], "temperature": 0.0, "avg_logprob": -0.14686023924085828, "compression_ratio": 1.6018518518518519, "no_speech_prob": 3.089471874773153e-06}, {"id": 908, "seek": 455146, "start": 4573.82, "end": 4579.06, "text": " size, the stride, the padding, the bias, so these parameters are all the same.", "tokens": [2744, 11, 264, 1056, 482, 11, 264, 39562, 11, 264, 12577, 11, 370, 613, 9834, 366, 439, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.14686023924085828, "compression_ratio": 1.6018518518518519, "no_speech_prob": 3.089471874773153e-06}, {"id": 909, "seek": 457906, "start": 4579.06, "end": 4583.860000000001, "text": " And the reason it's called a conv transpose is because actually it turns out that this", "tokens": [400, 264, 1778, 309, 311, 1219, 257, 3754, 25167, 307, 570, 767, 309, 4523, 484, 300, 341], "temperature": 0.0, "avg_logprob": -0.13113068117953763, "compression_ratio": 1.738197424892704, "no_speech_prob": 7.889221706136595e-06}, {"id": 910, "seek": 457906, "start": 4583.860000000001, "end": 4589.780000000001, "text": " is the same as the calculation of the gradient of convolution.", "tokens": [307, 264, 912, 382, 264, 17108, 295, 264, 16235, 295, 45216, 13], "temperature": 0.0, "avg_logprob": -0.13113068117953763, "compression_ratio": 1.738197424892704, "no_speech_prob": 7.889221706136595e-06}, {"id": 911, "seek": 457906, "start": 4589.780000000001, "end": 4593.780000000001, "text": " That's basically why they call it that.", "tokens": [663, 311, 1936, 983, 436, 818, 309, 300, 13], "temperature": 0.0, "avg_logprob": -0.13113068117953763, "compression_ratio": 1.738197424892704, "no_speech_prob": 7.889221706136595e-06}, {"id": 912, "seek": 457906, "start": 4593.780000000001, "end": 4598.46, "text": " So this is a really nice example back on the old Theano website that comes from a really", "tokens": [407, 341, 307, 257, 534, 1481, 1365, 646, 322, 264, 1331, 440, 3730, 3144, 300, 1487, 490, 257, 534], "temperature": 0.0, "avg_logprob": -0.13113068117953763, "compression_ratio": 1.738197424892704, "no_speech_prob": 7.889221706136595e-06}, {"id": 913, "seek": 457906, "start": 4598.46, "end": 4602.780000000001, "text": " nice paper which actually shows you some visualizations.", "tokens": [1481, 3035, 597, 767, 3110, 291, 512, 5056, 14455, 13], "temperature": 0.0, "avg_logprob": -0.13113068117953763, "compression_ratio": 1.738197424892704, "no_speech_prob": 7.889221706136595e-06}, {"id": 914, "seek": 457906, "start": 4602.780000000001, "end": 4607.660000000001, "text": " So this is actually the one we just saw of doing a 2x2 deconvolution.", "tokens": [407, 341, 307, 767, 264, 472, 321, 445, 1866, 295, 884, 257, 568, 87, 17, 979, 266, 85, 3386, 13], "temperature": 0.0, "avg_logprob": -0.13113068117953763, "compression_ratio": 1.738197424892704, "no_speech_prob": 7.889221706136595e-06}, {"id": 915, "seek": 460766, "start": 4607.66, "end": 4612.22, "text": " If there's a stride 2, then you don't just have padding around the outside, but you actually", "tokens": [759, 456, 311, 257, 1056, 482, 568, 11, 550, 291, 500, 380, 445, 362, 39562, 926, 264, 2380, 11, 457, 291, 767], "temperature": 0.0, "avg_logprob": -0.20546793937683105, "compression_ratio": 1.6968503937007875, "no_speech_prob": 1.3211836630944163e-05}, {"id": 916, "seek": 460766, "start": 4612.22, "end": 4616.82, "text": " have to put padding in the middle as well.", "tokens": [362, 281, 829, 39562, 294, 264, 2808, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.20546793937683105, "compression_ratio": 1.6968503937007875, "no_speech_prob": 1.3211836630944163e-05}, {"id": 917, "seek": 460766, "start": 4616.82, "end": 4621.139999999999, "text": " They're not actually quite implemented this way because this is slow to do in practice.", "tokens": [814, 434, 406, 767, 1596, 12270, 341, 636, 570, 341, 307, 2964, 281, 360, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.20546793937683105, "compression_ratio": 1.6968503937007875, "no_speech_prob": 1.3211836630944163e-05}, {"id": 918, "seek": 460766, "start": 4621.139999999999, "end": 4624.46, "text": " They implement them a different way, but it all happens behind the scenes.", "tokens": [814, 4445, 552, 257, 819, 636, 11, 457, 309, 439, 2314, 2261, 264, 8026, 13], "temperature": 0.0, "avg_logprob": -0.20546793937683105, "compression_ratio": 1.6968503937007875, "no_speech_prob": 1.3211836630944163e-05}, {"id": 919, "seek": 460766, "start": 4624.46, "end": 4628.62, "text": " We don't have to worry about it.", "tokens": [492, 500, 380, 362, 281, 3292, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.20546793937683105, "compression_ratio": 1.6968503937007875, "no_speech_prob": 1.3211836630944163e-05}, {"id": 920, "seek": 460766, "start": 4628.62, "end": 4633.58, "text": " We've talked about this convolution arithmetic tutorial before, and if you're still not comfortable", "tokens": [492, 600, 2825, 466, 341, 45216, 42973, 7073, 949, 11, 293, 498, 291, 434, 920, 406, 4619], "temperature": 0.0, "avg_logprob": -0.20546793937683105, "compression_ratio": 1.6968503937007875, "no_speech_prob": 1.3211836630944163e-05}, {"id": 921, "seek": 463358, "start": 4633.58, "end": 4639.62, "text": " with convolutions and in order to get comfortable with deconvolutions, this is a great site", "tokens": [365, 3754, 15892, 293, 294, 1668, 281, 483, 4619, 365, 979, 266, 85, 15892, 11, 341, 307, 257, 869, 3621], "temperature": 0.0, "avg_logprob": -0.22035868962605795, "compression_ratio": 1.5511363636363635, "no_speech_prob": 1.241131940332707e-05}, {"id": 922, "seek": 463358, "start": 4639.62, "end": 4640.98, "text": " to go to.", "tokens": [281, 352, 281, 13], "temperature": 0.0, "avg_logprob": -0.22035868962605795, "compression_ratio": 1.5511363636363635, "no_speech_prob": 1.241131940332707e-05}, {"id": 923, "seek": 463358, "start": 4640.98, "end": 4644.18, "text": " If you want to see the paper, just google for convolution arithmetic.", "tokens": [759, 291, 528, 281, 536, 264, 3035, 11, 445, 20742, 337, 45216, 42973, 13], "temperature": 0.0, "avg_logprob": -0.22035868962605795, "compression_ratio": 1.5511363636363635, "no_speech_prob": 1.241131940332707e-05}, {"id": 924, "seek": 463358, "start": 4644.18, "end": 4656.78, "text": " That'll be the first thing that comes up.", "tokens": [663, 603, 312, 264, 700, 551, 300, 1487, 493, 13], "temperature": 0.0, "avg_logprob": -0.22035868962605795, "compression_ratio": 1.5511363636363635, "no_speech_prob": 1.241131940332707e-05}, {"id": 925, "seek": 463358, "start": 4656.78, "end": 4661.46, "text": " And so that Theano tutorial actually comes from this paper.", "tokens": [400, 370, 300, 440, 3730, 7073, 767, 1487, 490, 341, 3035, 13], "temperature": 0.0, "avg_logprob": -0.22035868962605795, "compression_ratio": 1.5511363636363635, "no_speech_prob": 1.241131940332707e-05}, {"id": 926, "seek": 466146, "start": 4661.46, "end": 4668.74, "text": " The paper doesn't have the animated GIFs.", "tokens": [440, 3035, 1177, 380, 362, 264, 18947, 460, 12775, 82, 13], "temperature": 0.0, "avg_logprob": -0.25276977785172, "compression_ratio": 1.54337899543379, "no_speech_prob": 1.497075572842732e-05}, {"id": 927, "seek": 466146, "start": 4668.74, "end": 4673.02, "text": " So it's interesting then, a deconv block looks identical to a conv block except it's got", "tokens": [407, 309, 311, 1880, 550, 11, 257, 979, 266, 85, 3461, 1542, 14800, 281, 257, 3754, 3461, 3993, 309, 311, 658], "temperature": 0.0, "avg_logprob": -0.25276977785172, "compression_ratio": 1.54337899543379, "no_speech_prob": 1.497075572842732e-05}, {"id": 928, "seek": 466146, "start": 4673.02, "end": 4675.58, "text": " the word transpose written here.", "tokens": [264, 1349, 25167, 3720, 510, 13], "temperature": 0.0, "avg_logprob": -0.25276977785172, "compression_ratio": 1.54337899543379, "no_speech_prob": 1.497075572842732e-05}, {"id": 929, "seek": 466146, "start": 4675.58, "end": 4678.1, "text": " We just go conv relu batch norm as before.", "tokens": [492, 445, 352, 3754, 1039, 84, 15245, 2026, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.25276977785172, "compression_ratio": 1.54337899543379, "no_speech_prob": 1.497075572842732e-05}, {"id": 930, "seek": 466146, "start": 4678.1, "end": 4681.02, "text": " It's got input filters, output filters.", "tokens": [467, 311, 658, 4846, 15995, 11, 5598, 15995, 13], "temperature": 0.0, "avg_logprob": -0.25276977785172, "compression_ratio": 1.54337899543379, "no_speech_prob": 1.497075572842732e-05}, {"id": 931, "seek": 466146, "start": 4681.02, "end": 4688.3, "text": " The only difference is that stride 2 means that the grid size will double rather than", "tokens": [440, 787, 2649, 307, 300, 1056, 482, 568, 1355, 300, 264, 10748, 2744, 486, 3834, 2831, 813], "temperature": 0.0, "avg_logprob": -0.25276977785172, "compression_ratio": 1.54337899543379, "no_speech_prob": 1.497075572842732e-05}, {"id": 932, "seek": 466146, "start": 4688.3, "end": 4689.3, "text": " half.", "tokens": [1922, 13], "temperature": 0.0, "avg_logprob": -0.25276977785172, "compression_ratio": 1.54337899543379, "no_speech_prob": 1.497075572842732e-05}, {"id": 933, "seek": 468930, "start": 4689.3, "end": 4698.18, "text": " Both nn conv transpose 2d and nn.upsample seem to do the same thing, ie. expand grid", "tokens": [6767, 297, 77, 3754, 25167, 568, 67, 293, 297, 77, 13, 7528, 335, 781, 1643, 281, 360, 264, 912, 551, 11, 43203, 13, 5268, 10748], "temperature": 0.0, "avg_logprob": -0.20463904415268497, "compression_ratio": 1.5449735449735449, "no_speech_prob": 5.255289124761475e-06}, {"id": 934, "seek": 468930, "start": 4698.18, "end": 4700.66, "text": " size height and width from the previous layer.", "tokens": [2744, 6681, 293, 11402, 490, 264, 3894, 4583, 13], "temperature": 0.0, "avg_logprob": -0.20463904415268497, "compression_ratio": 1.5449735449735449, "no_speech_prob": 5.255289124761475e-06}, {"id": 935, "seek": 468930, "start": 4700.66, "end": 4706.18, "text": " Can we say that conv transpose 2d is always better than upsample since upsample is merely", "tokens": [1664, 321, 584, 300, 3754, 25167, 568, 67, 307, 1009, 1101, 813, 15497, 335, 781, 1670, 15497, 335, 781, 307, 17003], "temperature": 0.0, "avg_logprob": -0.20463904415268497, "compression_ratio": 1.5449735449735449, "no_speech_prob": 5.255289124761475e-06}, {"id": 936, "seek": 468930, "start": 4706.18, "end": 4711.42, "text": " resizing and filling unknowns by zeros or interpolation?", "tokens": [725, 3319, 293, 10623, 46048, 538, 35193, 420, 44902, 399, 30], "temperature": 0.0, "avg_logprob": -0.20463904415268497, "compression_ratio": 1.5449735449735449, "no_speech_prob": 5.255289124761475e-06}, {"id": 937, "seek": 468930, "start": 4711.42, "end": 4716.5, "text": " No you can't.", "tokens": [883, 291, 393, 380, 13], "temperature": 0.0, "avg_logprob": -0.20463904415268497, "compression_ratio": 1.5449735449735449, "no_speech_prob": 5.255289124761475e-06}, {"id": 938, "seek": 471650, "start": 4716.5, "end": 4725.94, "text": " So there's a fantastic interactive paper on distil.pub called Deconvolution and Checkerboard", "tokens": [407, 456, 311, 257, 5456, 15141, 3035, 322, 1483, 388, 13, 79, 836, 1219, 1346, 1671, 85, 3386, 293, 6881, 260, 3787], "temperature": 0.0, "avg_logprob": -0.16197928378456516, "compression_ratio": 1.5675675675675675, "no_speech_prob": 1.3631209185405169e-05}, {"id": 939, "seek": 471650, "start": 4725.94, "end": 4733.1, "text": " Artifacts which points out that what we're doing right now is extremely suboptimal, but", "tokens": [5735, 351, 15295, 597, 2793, 484, 300, 437, 321, 434, 884, 558, 586, 307, 4664, 1422, 5747, 10650, 11, 457], "temperature": 0.0, "avg_logprob": -0.16197928378456516, "compression_ratio": 1.5675675675675675, "no_speech_prob": 1.3631209185405169e-05}, {"id": 940, "seek": 471650, "start": 4733.1, "end": 4736.86, "text": " the good news is everybody else does it.", "tokens": [264, 665, 2583, 307, 2201, 1646, 775, 309, 13], "temperature": 0.0, "avg_logprob": -0.16197928378456516, "compression_ratio": 1.5675675675675675, "no_speech_prob": 1.3631209185405169e-05}, {"id": 941, "seek": 471650, "start": 4736.86, "end": 4742.42, "text": " If you have a look here, can you see these checkerboard artifacts?", "tokens": [759, 291, 362, 257, 574, 510, 11, 393, 291, 536, 613, 1520, 260, 3787, 24617, 30], "temperature": 0.0, "avg_logprob": -0.16197928378456516, "compression_ratio": 1.5675675675675675, "no_speech_prob": 1.3631209185405169e-05}, {"id": 942, "seek": 471650, "start": 4742.42, "end": 4745.86, "text": " It's all like dark blue, light blue, dark blue, light blue.", "tokens": [467, 311, 439, 411, 2877, 3344, 11, 1442, 3344, 11, 2877, 3344, 11, 1442, 3344, 13], "temperature": 0.0, "avg_logprob": -0.16197928378456516, "compression_ratio": 1.5675675675675675, "no_speech_prob": 1.3631209185405169e-05}, {"id": 943, "seek": 474586, "start": 4745.86, "end": 4751.82, "text": " So these are all from actual papers.", "tokens": [407, 613, 366, 439, 490, 3539, 10577, 13], "temperature": 0.0, "avg_logprob": -0.2046280522500315, "compression_ratio": 1.430232558139535, "no_speech_prob": 6.048901013855357e-06}, {"id": 944, "seek": 474586, "start": 4751.82, "end": 4755.62, "text": " Basically they noticed every one of these papers with generative models has these checkerboard", "tokens": [8537, 436, 5694, 633, 472, 295, 613, 10577, 365, 1337, 1166, 5245, 575, 613, 1520, 260, 3787], "temperature": 0.0, "avg_logprob": -0.2046280522500315, "compression_ratio": 1.430232558139535, "no_speech_prob": 6.048901013855357e-06}, {"id": 945, "seek": 474586, "start": 4755.62, "end": 4757.62, "text": " artifacts.", "tokens": [24617, 13], "temperature": 0.0, "avg_logprob": -0.2046280522500315, "compression_ratio": 1.430232558139535, "no_speech_prob": 6.048901013855357e-06}, {"id": 946, "seek": 474586, "start": 4757.62, "end": 4766.5, "text": " What they realized is it's because when you have a stride 2 convolution of size 3 kernel,", "tokens": [708, 436, 5334, 307, 309, 311, 570, 562, 291, 362, 257, 1056, 482, 568, 45216, 295, 2744, 805, 28256, 11], "temperature": 0.0, "avg_logprob": -0.2046280522500315, "compression_ratio": 1.430232558139535, "no_speech_prob": 6.048901013855357e-06}, {"id": 947, "seek": 474586, "start": 4766.5, "end": 4769.299999999999, "text": " they overlap.", "tokens": [436, 19959, 13], "temperature": 0.0, "avg_logprob": -0.2046280522500315, "compression_ratio": 1.430232558139535, "no_speech_prob": 6.048901013855357e-06}, {"id": 948, "seek": 476930, "start": 4769.3, "end": 4777.9400000000005, "text": " And so you basically get some grid cells get twice as much activation.", "tokens": [400, 370, 291, 1936, 483, 512, 10748, 5438, 483, 6091, 382, 709, 24433, 13], "temperature": 0.0, "avg_logprob": -0.20216751098632812, "compression_ratio": 1.5064935064935066, "no_speech_prob": 2.9023065053479513e-06}, {"id": 949, "seek": 476930, "start": 4777.9400000000005, "end": 4784.66, "text": " And so even if you start with random weights, you end up with a checkerboard artifact.", "tokens": [400, 370, 754, 498, 291, 722, 365, 4974, 17443, 11, 291, 917, 493, 365, 257, 1520, 260, 3787, 34806, 13], "temperature": 0.0, "avg_logprob": -0.20216751098632812, "compression_ratio": 1.5064935064935066, "no_speech_prob": 2.9023065053479513e-06}, {"id": 950, "seek": 476930, "start": 4784.66, "end": 4788.5, "text": " You can kind of see it here.", "tokens": [509, 393, 733, 295, 536, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.20216751098632812, "compression_ratio": 1.5064935064935066, "no_speech_prob": 2.9023065053479513e-06}, {"id": 951, "seek": 476930, "start": 4788.5, "end": 4794.3, "text": " And so the deeper you get, the worse it gets.", "tokens": [400, 370, 264, 7731, 291, 483, 11, 264, 5324, 309, 2170, 13], "temperature": 0.0, "avg_logprob": -0.20216751098632812, "compression_ratio": 1.5064935064935066, "no_speech_prob": 2.9023065053479513e-06}, {"id": 952, "seek": 479430, "start": 4794.3, "end": 4799.9400000000005, "text": " Another advice is actually less direct than it ought to be.", "tokens": [3996, 5192, 307, 767, 1570, 2047, 813, 309, 13416, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.2191603260655557, "compression_ratio": 1.535, "no_speech_prob": 2.9023067327216268e-06}, {"id": 953, "seek": 479430, "start": 4799.9400000000005, "end": 4806.42, "text": " I found that for most generative models, upsampling is better.", "tokens": [286, 1352, 300, 337, 881, 1337, 1166, 5245, 11, 15497, 335, 11970, 307, 1101, 13], "temperature": 0.0, "avg_logprob": -0.2191603260655557, "compression_ratio": 1.535, "no_speech_prob": 2.9023067327216268e-06}, {"id": 954, "seek": 479430, "start": 4806.42, "end": 4815.1, "text": " If you do nm.upsample, then all it does is it's basically doing pooling.", "tokens": [759, 291, 360, 297, 76, 13, 7528, 335, 781, 11, 550, 439, 309, 775, 307, 309, 311, 1936, 884, 7005, 278, 13], "temperature": 0.0, "avg_logprob": -0.2191603260655557, "compression_ratio": 1.535, "no_speech_prob": 2.9023067327216268e-06}, {"id": 955, "seek": 479430, "start": 4815.1, "end": 4816.7, "text": " It's kind of the opposite of pooling.", "tokens": [467, 311, 733, 295, 264, 6182, 295, 7005, 278, 13], "temperature": 0.0, "avg_logprob": -0.2191603260655557, "compression_ratio": 1.535, "no_speech_prob": 2.9023067327216268e-06}, {"id": 956, "seek": 479430, "start": 4816.7, "end": 4823.14, "text": " It says let's replace this one pixel, or this one grid cell, with 4, 2x2.", "tokens": [467, 1619, 718, 311, 7406, 341, 472, 19261, 11, 420, 341, 472, 10748, 2815, 11, 365, 1017, 11, 568, 87, 17, 13], "temperature": 0.0, "avg_logprob": -0.2191603260655557, "compression_ratio": 1.535, "no_speech_prob": 2.9023067327216268e-06}, {"id": 957, "seek": 482314, "start": 4823.14, "end": 4824.740000000001, "text": " And there's a number of ways to upsample.", "tokens": [400, 456, 311, 257, 1230, 295, 2098, 281, 15497, 335, 781, 13], "temperature": 0.0, "avg_logprob": -0.1351058613170277, "compression_ratio": 1.5169491525423728, "no_speech_prob": 2.9944212656118907e-06}, {"id": 958, "seek": 482314, "start": 4824.740000000001, "end": 4828.14, "text": " One is just to copy it across to those 4.", "tokens": [1485, 307, 445, 281, 5055, 309, 2108, 281, 729, 1017, 13], "temperature": 0.0, "avg_logprob": -0.1351058613170277, "compression_ratio": 1.5169491525423728, "no_speech_prob": 2.9944212656118907e-06}, {"id": 959, "seek": 482314, "start": 4828.14, "end": 4831.9800000000005, "text": " Another is to use a bilinear or bicubic interpolation.", "tokens": [3996, 307, 281, 764, 257, 8588, 533, 289, 420, 34472, 836, 299, 44902, 399, 13], "temperature": 0.0, "avg_logprob": -0.1351058613170277, "compression_ratio": 1.5169491525423728, "no_speech_prob": 2.9944212656118907e-06}, {"id": 960, "seek": 482314, "start": 4831.9800000000005, "end": 4836.22, "text": " There are various techniques to try and create a smooth upsampled version.", "tokens": [821, 366, 3683, 7512, 281, 853, 293, 1884, 257, 5508, 15497, 335, 15551, 3037, 13], "temperature": 0.0, "avg_logprob": -0.1351058613170277, "compression_ratio": 1.5169491525423728, "no_speech_prob": 2.9944212656118907e-06}, {"id": 961, "seek": 482314, "start": 4836.22, "end": 4839.660000000001, "text": " And you can pretty much choose any of them in PyTorch.", "tokens": [400, 291, 393, 1238, 709, 2826, 604, 295, 552, 294, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.1351058613170277, "compression_ratio": 1.5169491525423728, "no_speech_prob": 2.9944212656118907e-06}, {"id": 962, "seek": 482314, "start": 4839.660000000001, "end": 4848.820000000001, "text": " So if you do a 2x2 upsample and then a regular stride 1 3x3 conv, that's like another way", "tokens": [407, 498, 291, 360, 257, 568, 87, 17, 15497, 335, 781, 293, 550, 257, 3890, 1056, 482, 502, 805, 87, 18, 3754, 11, 300, 311, 411, 1071, 636], "temperature": 0.0, "avg_logprob": -0.1351058613170277, "compression_ratio": 1.5169491525423728, "no_speech_prob": 2.9944212656118907e-06}, {"id": 963, "seek": 484882, "start": 4848.82, "end": 4855.2, "text": " of doing the same kind of thing as a conv transpose.", "tokens": [295, 884, 264, 912, 733, 295, 551, 382, 257, 3754, 25167, 13], "temperature": 0.0, "avg_logprob": -0.18206144838917013, "compression_ratio": 1.6536796536796536, "no_speech_prob": 4.785057626577327e-06}, {"id": 964, "seek": 484882, "start": 4855.2, "end": 4861.9, "text": " It's doubling the grid size and doing some convolutional arithmetic on it.", "tokens": [467, 311, 33651, 264, 10748, 2744, 293, 884, 512, 45216, 304, 42973, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.18206144838917013, "compression_ratio": 1.6536796536796536, "no_speech_prob": 4.785057626577327e-06}, {"id": 965, "seek": 484882, "start": 4861.9, "end": 4866.38, "text": " And I found for generative models, it pretty much always works better.", "tokens": [400, 286, 1352, 337, 1337, 1166, 5245, 11, 309, 1238, 709, 1009, 1985, 1101, 13], "temperature": 0.0, "avg_logprob": -0.18206144838917013, "compression_ratio": 1.6536796536796536, "no_speech_prob": 4.785057626577327e-06}, {"id": 966, "seek": 484882, "start": 4866.38, "end": 4872.299999999999, "text": " And in that distilled up publication, they kind of indicate that maybe that's a good", "tokens": [400, 294, 300, 1483, 6261, 493, 19953, 11, 436, 733, 295, 13330, 300, 1310, 300, 311, 257, 665], "temperature": 0.0, "avg_logprob": -0.18206144838917013, "compression_ratio": 1.6536796536796536, "no_speech_prob": 4.785057626577327e-06}, {"id": 967, "seek": 484882, "start": 4872.299999999999, "end": 4875.66, "text": " approach but they don't just come out and say just do this, or else I would just say", "tokens": [3109, 457, 436, 500, 380, 445, 808, 484, 293, 584, 445, 360, 341, 11, 420, 1646, 286, 576, 445, 584], "temperature": 0.0, "avg_logprob": -0.18206144838917013, "compression_ratio": 1.6536796536796536, "no_speech_prob": 4.785057626577327e-06}, {"id": 968, "seek": 484882, "start": 4875.66, "end": 4878.299999999999, "text": " just do this.", "tokens": [445, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.18206144838917013, "compression_ratio": 1.6536796536796536, "no_speech_prob": 4.785057626577327e-06}, {"id": 969, "seek": 487830, "start": 4878.3, "end": 4883.66, "text": " Having said that, for GANs, I haven't had that much success with it yet.", "tokens": [10222, 848, 300, 11, 337, 460, 1770, 82, 11, 286, 2378, 380, 632, 300, 709, 2245, 365, 309, 1939, 13], "temperature": 0.0, "avg_logprob": -0.1369395717497795, "compression_ratio": 1.5402843601895735, "no_speech_prob": 1.6442383639514446e-05}, {"id": 970, "seek": 487830, "start": 4883.66, "end": 4886.58, "text": " And I think it probably requires some tweaking to get it to work.", "tokens": [400, 286, 519, 309, 1391, 7029, 512, 6986, 2456, 281, 483, 309, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.1369395717497795, "compression_ratio": 1.5402843601895735, "no_speech_prob": 1.6442383639514446e-05}, {"id": 971, "seek": 487830, "start": 4886.58, "end": 4890.06, "text": " I'm sure some people have got it to work.", "tokens": [286, 478, 988, 512, 561, 362, 658, 309, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.1369395717497795, "compression_ratio": 1.5402843601895735, "no_speech_prob": 1.6442383639514446e-05}, {"id": 972, "seek": 487830, "start": 4890.06, "end": 4902.9400000000005, "text": " The issue I think is that in the early stages, it doesn't create enough noise.", "tokens": [440, 2734, 286, 519, 307, 300, 294, 264, 2440, 10232, 11, 309, 1177, 380, 1884, 1547, 5658, 13], "temperature": 0.0, "avg_logprob": -0.1369395717497795, "compression_ratio": 1.5402843601895735, "no_speech_prob": 1.6442383639514446e-05}, {"id": 973, "seek": 487830, "start": 4902.9400000000005, "end": 4907.18, "text": " I had a version actually where I tried to do it with an upsample.", "tokens": [286, 632, 257, 3037, 767, 689, 286, 3031, 281, 360, 309, 365, 364, 15497, 335, 781, 13], "temperature": 0.0, "avg_logprob": -0.1369395717497795, "compression_ratio": 1.5402843601895735, "no_speech_prob": 1.6442383639514446e-05}, {"id": 974, "seek": 490718, "start": 4907.18, "end": 4910.860000000001, "text": " And you can kind of see that the noise didn't look very noisy.", "tokens": [400, 291, 393, 733, 295, 536, 300, 264, 5658, 994, 380, 574, 588, 24518, 13], "temperature": 0.0, "avg_logprob": -0.19423285552433558, "compression_ratio": 1.5896414342629481, "no_speech_prob": 8.267773409897927e-06}, {"id": 975, "seek": 490718, "start": 4910.860000000001, "end": 4913.9400000000005, "text": " So anyway, it's an interesting question.", "tokens": [407, 4033, 11, 309, 311, 364, 1880, 1168, 13], "temperature": 0.0, "avg_logprob": -0.19423285552433558, "compression_ratio": 1.5896414342629481, "no_speech_prob": 8.267773409897927e-06}, {"id": 976, "seek": 490718, "start": 4913.9400000000005, "end": 4919.02, "text": " But next week when we look at style transfer and super resolution and stuff, I think you'll", "tokens": [583, 958, 1243, 562, 321, 574, 412, 3758, 5003, 293, 1687, 8669, 293, 1507, 11, 286, 519, 291, 603], "temperature": 0.0, "avg_logprob": -0.19423285552433558, "compression_ratio": 1.5896414342629481, "no_speech_prob": 8.267773409897927e-06}, {"id": 977, "seek": 490718, "start": 4919.02, "end": 4925.1, "text": " see an upsample really comes into its own.", "tokens": [536, 364, 15497, 335, 781, 534, 1487, 666, 1080, 1065, 13], "temperature": 0.0, "avg_logprob": -0.19423285552433558, "compression_ratio": 1.5896414342629481, "no_speech_prob": 8.267773409897927e-06}, {"id": 978, "seek": 490718, "start": 4925.1, "end": 4928.740000000001, "text": " So the generator, we can now basically start with the vector.", "tokens": [407, 264, 19265, 11, 321, 393, 586, 1936, 722, 365, 264, 8062, 13], "temperature": 0.0, "avg_logprob": -0.19423285552433558, "compression_ratio": 1.5896414342629481, "no_speech_prob": 8.267773409897927e-06}, {"id": 979, "seek": 490718, "start": 4928.740000000001, "end": 4932.860000000001, "text": " We can decide and say, okay, let's not think of it as a vector, but actually it's a 1x1", "tokens": [492, 393, 4536, 293, 584, 11, 1392, 11, 718, 311, 406, 519, 295, 309, 382, 257, 8062, 11, 457, 767, 309, 311, 257, 502, 87, 16], "temperature": 0.0, "avg_logprob": -0.19423285552433558, "compression_ratio": 1.5896414342629481, "no_speech_prob": 8.267773409897927e-06}, {"id": 980, "seek": 490718, "start": 4932.860000000001, "end": 4933.860000000001, "text": " grid cell.", "tokens": [10748, 2815, 13], "temperature": 0.0, "avg_logprob": -0.19423285552433558, "compression_ratio": 1.5896414342629481, "no_speech_prob": 8.267773409897927e-06}, {"id": 981, "seek": 493386, "start": 4933.86, "end": 4937.139999999999, "text": " We can turn it into a 4x4, 8x8, and so forth.", "tokens": [492, 393, 1261, 309, 666, 257, 1017, 87, 19, 11, 1649, 87, 23, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1954068722932235, "compression_ratio": 1.55, "no_speech_prob": 1.078321656677872e-05}, {"id": 982, "seek": 493386, "start": 4937.139999999999, "end": 4942.78, "text": " And so that's why we have to make sure it's a suitable multiple so that we can actually", "tokens": [400, 370, 300, 311, 983, 321, 362, 281, 652, 988, 309, 311, 257, 12873, 3866, 370, 300, 321, 393, 767], "temperature": 0.0, "avg_logprob": -0.1954068722932235, "compression_ratio": 1.55, "no_speech_prob": 1.078321656677872e-05}, {"id": 983, "seek": 493386, "start": 4942.78, "end": 4945.7, "text": " create something of the right size.", "tokens": [1884, 746, 295, 264, 558, 2744, 13], "temperature": 0.0, "avg_logprob": -0.1954068722932235, "compression_ratio": 1.55, "no_speech_prob": 1.078321656677872e-05}, {"id": 984, "seek": 493386, "start": 4945.7, "end": 4948.66, "text": " And so you can see it's doing the exact opposite as before.", "tokens": [400, 370, 291, 393, 536, 309, 311, 884, 264, 1900, 6182, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.1954068722932235, "compression_ratio": 1.55, "no_speech_prob": 1.078321656677872e-05}, {"id": 985, "seek": 493386, "start": 4948.66, "end": 4956.82, "text": " It's making the cell size smaller and smaller by 2 at a time, as long as it can.", "tokens": [467, 311, 1455, 264, 2815, 2744, 4356, 293, 4356, 538, 568, 412, 257, 565, 11, 382, 938, 382, 309, 393, 13], "temperature": 0.0, "avg_logprob": -0.1954068722932235, "compression_ratio": 1.55, "no_speech_prob": 1.078321656677872e-05}, {"id": 986, "seek": 495682, "start": 4956.82, "end": 4964.9, "text": " Sorry, bigger and bigger, the cell size bigger and bigger as long as it can until it gets", "tokens": [4919, 11, 3801, 293, 3801, 11, 264, 2815, 2744, 3801, 293, 3801, 382, 938, 382, 309, 393, 1826, 309, 2170], "temperature": 0.0, "avg_logprob": -0.2167558341190733, "compression_ratio": 1.9, "no_speech_prob": 8.939654435380362e-06}, {"id": 987, "seek": 495682, "start": 4964.9, "end": 4968.219999999999, "text": " to half the size that we want.", "tokens": [281, 1922, 264, 2744, 300, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.2167558341190733, "compression_ratio": 1.9, "no_speech_prob": 8.939654435380362e-06}, {"id": 988, "seek": 495682, "start": 4968.219999999999, "end": 4976.38, "text": " And then finally we add one more on at the end, sorry, we add n more on at the end with", "tokens": [400, 550, 2721, 321, 909, 472, 544, 322, 412, 264, 917, 11, 2597, 11, 321, 909, 297, 544, 322, 412, 264, 917, 365], "temperature": 0.0, "avg_logprob": -0.2167558341190733, "compression_ratio": 1.9, "no_speech_prob": 8.939654435380362e-06}, {"id": 989, "seek": 495682, "start": 4976.38, "end": 4978.139999999999, "text": " no stride.", "tokens": [572, 1056, 482, 13], "temperature": 0.0, "avg_logprob": -0.2167558341190733, "compression_ratio": 1.9, "no_speech_prob": 8.939654435380362e-06}, {"id": 990, "seek": 495682, "start": 4978.139999999999, "end": 4985.98, "text": " And then we add one more, conv transpose, to finally get to the size that we wanted.", "tokens": [400, 550, 321, 909, 472, 544, 11, 3754, 25167, 11, 281, 2721, 483, 281, 264, 2744, 300, 321, 1415, 13], "temperature": 0.0, "avg_logprob": -0.2167558341190733, "compression_ratio": 1.9, "no_speech_prob": 8.939654435380362e-06}, {"id": 991, "seek": 498598, "start": 4985.98, "end": 4988.0199999999995, "text": " And we're done.", "tokens": [400, 321, 434, 1096, 13], "temperature": 0.0, "avg_logprob": -0.2230267198118445, "compression_ratio": 1.4042553191489362, "no_speech_prob": 2.36878850046196e-06}, {"id": 992, "seek": 498598, "start": 4988.0199999999995, "end": 4995.419999999999, "text": " Finally we put that through a THAN, and that's going to force us to be in the 0-1 range,", "tokens": [6288, 321, 829, 300, 807, 257, 3578, 1770, 11, 293, 300, 311, 516, 281, 3464, 505, 281, 312, 294, 264, 1958, 12, 16, 3613, 11], "temperature": 0.0, "avg_logprob": -0.2230267198118445, "compression_ratio": 1.4042553191489362, "no_speech_prob": 2.36878850046196e-06}, {"id": 993, "seek": 498598, "start": 4995.419999999999, "end": 5004.9, "text": " because of course we don't want to spit out arbitrary size pixel values.", "tokens": [570, 295, 1164, 321, 500, 380, 528, 281, 22127, 484, 23211, 2744, 19261, 4190, 13], "temperature": 0.0, "avg_logprob": -0.2230267198118445, "compression_ratio": 1.4042553191489362, "no_speech_prob": 2.36878850046196e-06}, {"id": 994, "seek": 498598, "start": 5004.9, "end": 5009.62, "text": " So we've got a generator architecture which spits out an image of some given size with", "tokens": [407, 321, 600, 658, 257, 19265, 9482, 597, 637, 1208, 484, 364, 3256, 295, 512, 2212, 2744, 365], "temperature": 0.0, "avg_logprob": -0.2230267198118445, "compression_ratio": 1.4042553191489362, "no_speech_prob": 2.36878850046196e-06}, {"id": 995, "seek": 500962, "start": 5009.62, "end": 5019.46, "text": " the correct number of channels, and with values between 0 and 1.", "tokens": [264, 3006, 1230, 295, 9235, 11, 293, 365, 4190, 1296, 1958, 293, 502, 13], "temperature": 0.0, "avg_logprob": -0.13511041281879813, "compression_ratio": 1.4210526315789473, "no_speech_prob": 5.804980105494906e-07}, {"id": 996, "seek": 500962, "start": 5019.46, "end": 5026.22, "text": " So at this point we can now create our model data object.", "tokens": [407, 412, 341, 935, 321, 393, 586, 1884, 527, 2316, 1412, 2657, 13], "temperature": 0.0, "avg_logprob": -0.13511041281879813, "compression_ratio": 1.4210526315789473, "no_speech_prob": 5.804980105494906e-07}, {"id": 997, "seek": 500962, "start": 5026.22, "end": 5031.5, "text": " These things take a while to train, so I just made it 128x128, so this is just a convenient", "tokens": [1981, 721, 747, 257, 1339, 281, 3847, 11, 370, 286, 445, 1027, 309, 29810, 87, 4762, 23, 11, 370, 341, 307, 445, 257, 10851], "temperature": 0.0, "avg_logprob": -0.13511041281879813, "compression_ratio": 1.4210526315789473, "no_speech_prob": 5.804980105494906e-07}, {"id": 998, "seek": 500962, "start": 5031.5, "end": 5037.94, "text": " way to make it a bit faster.", "tokens": [636, 281, 652, 309, 257, 857, 4663, 13], "temperature": 0.0, "avg_logprob": -0.13511041281879813, "compression_ratio": 1.4210526315789473, "no_speech_prob": 5.804980105494906e-07}, {"id": 999, "seek": 503794, "start": 5037.94, "end": 5041.24, "text": " And that's going to be the size of the input, but then we're going to use transformations", "tokens": [400, 300, 311, 516, 281, 312, 264, 2744, 295, 264, 4846, 11, 457, 550, 321, 434, 516, 281, 764, 34852], "temperature": 0.0, "avg_logprob": -0.14534509961850176, "compression_ratio": 1.607843137254902, "no_speech_prob": 1.2411362149578054e-05}, {"id": 1000, "seek": 503794, "start": 5041.24, "end": 5046.379999999999, "text": " to turn it into 64x64.", "tokens": [281, 1261, 309, 666, 12145, 87, 19395, 13], "temperature": 0.0, "avg_logprob": -0.14534509961850176, "compression_ratio": 1.607843137254902, "no_speech_prob": 1.2411362149578054e-05}, {"id": 1001, "seek": 503794, "start": 5046.379999999999, "end": 5051.0, "text": " There's been more recent advances which have attempted to really increase this up to kind", "tokens": [821, 311, 668, 544, 5162, 25297, 597, 362, 18997, 281, 534, 3488, 341, 493, 281, 733], "temperature": 0.0, "avg_logprob": -0.14534509961850176, "compression_ratio": 1.607843137254902, "no_speech_prob": 1.2411362149578054e-05}, {"id": 1002, "seek": 503794, "start": 5051.0, "end": 5056.46, "text": " of like high resolution sizes, but they still tend to require either a batch size of 1,", "tokens": [295, 411, 1090, 8669, 11602, 11, 457, 436, 920, 3928, 281, 3651, 2139, 257, 15245, 2744, 295, 502, 11], "temperature": 0.0, "avg_logprob": -0.14534509961850176, "compression_ratio": 1.607843137254902, "no_speech_prob": 1.2411362149578054e-05}, {"id": 1003, "seek": 503794, "start": 5056.46, "end": 5058.66, "text": " or lots and lots of GPUs or whatever.", "tokens": [420, 3195, 293, 3195, 295, 18407, 82, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.14534509961850176, "compression_ratio": 1.607843137254902, "no_speech_prob": 1.2411362149578054e-05}, {"id": 1004, "seek": 503794, "start": 5058.66, "end": 5064.74, "text": " So we're kind of trying to do things that we can do on single consumer GPUs here.", "tokens": [407, 321, 434, 733, 295, 1382, 281, 360, 721, 300, 321, 393, 360, 322, 2167, 9711, 18407, 82, 510, 13], "temperature": 0.0, "avg_logprob": -0.14534509961850176, "compression_ratio": 1.607843137254902, "no_speech_prob": 1.2411362149578054e-05}, {"id": 1005, "seek": 506474, "start": 5064.74, "end": 5071.32, "text": " So here's an example of one of the 64x64 petroms.", "tokens": [407, 510, 311, 364, 1365, 295, 472, 295, 264, 12145, 87, 19395, 3817, 4397, 82, 13], "temperature": 0.0, "avg_logprob": -0.15224946931351063, "compression_ratio": 1.4734299516908214, "no_speech_prob": 6.962223324080696e-06}, {"id": 1006, "seek": 506474, "start": 5071.32, "end": 5076.78, "text": " So we're going to do pretty much everything manually, so let's go ahead and create our", "tokens": [407, 321, 434, 516, 281, 360, 1238, 709, 1203, 16945, 11, 370, 718, 311, 352, 2286, 293, 1884, 527], "temperature": 0.0, "avg_logprob": -0.15224946931351063, "compression_ratio": 1.4734299516908214, "no_speech_prob": 6.962223324080696e-06}, {"id": 1007, "seek": 506474, "start": 5076.78, "end": 5081.9, "text": " two models, our generator and our discriminator.", "tokens": [732, 5245, 11, 527, 19265, 293, 527, 20828, 1639, 13], "temperature": 0.0, "avg_logprob": -0.15224946931351063, "compression_ratio": 1.4734299516908214, "no_speech_prob": 6.962223324080696e-06}, {"id": 1008, "seek": 506474, "start": 5081.9, "end": 5090.3, "text": " And as you can see, they're DCGAN, so in other words they're the same modules that came up", "tokens": [400, 382, 291, 393, 536, 11, 436, 434, 9114, 27699, 11, 370, 294, 661, 2283, 436, 434, 264, 912, 16679, 300, 1361, 493], "temperature": 0.0, "avg_logprob": -0.15224946931351063, "compression_ratio": 1.4734299516908214, "no_speech_prob": 6.962223324080696e-06}, {"id": 1009, "seek": 506474, "start": 5090.3, "end": 5091.78, "text": " were appeared in this paper.", "tokens": [645, 8516, 294, 341, 3035, 13], "temperature": 0.0, "avg_logprob": -0.15224946931351063, "compression_ratio": 1.4734299516908214, "no_speech_prob": 6.962223324080696e-06}, {"id": 1010, "seek": 509178, "start": 5091.78, "end": 5096.54, "text": " So if you're interested in reading the papers, it's well worth going back and looking at", "tokens": [407, 498, 291, 434, 3102, 294, 3760, 264, 10577, 11, 309, 311, 731, 3163, 516, 646, 293, 1237, 412], "temperature": 0.0, "avg_logprob": -0.2722481124255122, "compression_ratio": 1.5166666666666666, "no_speech_prob": 1.4738810023118276e-05}, {"id": 1011, "seek": 509178, "start": 5096.54, "end": 5101.74, "text": " the DCGAN paper to see what these architectures are, because it's assumed that when you read", "tokens": [264, 9114, 27699, 3035, 281, 536, 437, 613, 6331, 1303, 366, 11, 570, 309, 311, 15895, 300, 562, 291, 1401], "temperature": 0.0, "avg_logprob": -0.2722481124255122, "compression_ratio": 1.5166666666666666, "no_speech_prob": 1.4738810023118276e-05}, {"id": 1012, "seek": 509178, "start": 5101.74, "end": 5104.98, "text": " the WassersteinGAN paper that you already know that.", "tokens": [264, 17351, 9089, 27699, 3035, 300, 291, 1217, 458, 300, 13], "temperature": 0.0, "avg_logprob": -0.2722481124255122, "compression_ratio": 1.5166666666666666, "no_speech_prob": 1.4738810023118276e-05}, {"id": 1013, "seek": 509178, "start": 5104.98, "end": 5105.98, "text": " Yes?", "tokens": [1079, 30], "temperature": 0.0, "avg_logprob": -0.2722481124255122, "compression_ratio": 1.5166666666666666, "no_speech_prob": 1.4738810023118276e-05}, {"id": 1014, "seek": 509178, "start": 5105.98, "end": 5107.94, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.2722481124255122, "compression_ratio": 1.5166666666666666, "no_speech_prob": 1.4738810023118276e-05}, {"id": 1015, "seek": 509178, "start": 5107.94, "end": 5113.5, "text": " Shouldn't we use a sigmoid if we want values between 0 and 1?", "tokens": [34170, 380, 321, 764, 257, 4556, 3280, 327, 498, 321, 528, 4190, 1296, 1958, 293, 502, 30], "temperature": 0.0, "avg_logprob": -0.2722481124255122, "compression_ratio": 1.5166666666666666, "no_speech_prob": 1.4738810023118276e-05}, {"id": 1016, "seek": 509178, "start": 5113.5, "end": 5115.34, "text": " I always forget which one's which.", "tokens": [286, 1009, 2870, 597, 472, 311, 597, 13], "temperature": 0.0, "avg_logprob": -0.2722481124255122, "compression_ratio": 1.5166666666666666, "no_speech_prob": 1.4738810023118276e-05}, {"id": 1017, "seek": 511534, "start": 5115.34, "end": 5127.1, "text": " So sigmoid is 0 to 1, than is 1 to minus 1.", "tokens": [407, 4556, 3280, 327, 307, 1958, 281, 502, 11, 813, 307, 502, 281, 3175, 502, 13], "temperature": 0.0, "avg_logprob": -0.19780606399347753, "compression_ratio": 1.5777777777777777, "no_speech_prob": 2.1782283511129208e-05}, {"id": 1018, "seek": 511534, "start": 5127.1, "end": 5132.7, "text": " I think what will happen is, I'm going to have to check that.", "tokens": [286, 519, 437, 486, 1051, 307, 11, 286, 478, 516, 281, 362, 281, 1520, 300, 13], "temperature": 0.0, "avg_logprob": -0.19780606399347753, "compression_ratio": 1.5777777777777777, "no_speech_prob": 2.1782283511129208e-05}, {"id": 1019, "seek": 511534, "start": 5132.7, "end": 5136.82, "text": " I vaguely remember thinking about this when I was writing this notebook and realizing", "tokens": [286, 13501, 48863, 1604, 1953, 466, 341, 562, 286, 390, 3579, 341, 21060, 293, 16734], "temperature": 0.0, "avg_logprob": -0.19780606399347753, "compression_ratio": 1.5777777777777777, "no_speech_prob": 2.1782283511129208e-05}, {"id": 1020, "seek": 511534, "start": 5136.82, "end": 5140.82, "text": " that 1 to minus 1 made sense for some reason, but I can't remember what that reason was", "tokens": [300, 502, 281, 3175, 502, 1027, 2020, 337, 512, 1778, 11, 457, 286, 393, 380, 1604, 437, 300, 1778, 390], "temperature": 0.0, "avg_logprob": -0.19780606399347753, "compression_ratio": 1.5777777777777777, "no_speech_prob": 2.1782283511129208e-05}, {"id": 1021, "seek": 511534, "start": 5140.82, "end": 5141.82, "text": " now.", "tokens": [586, 13], "temperature": 0.0, "avg_logprob": -0.19780606399347753, "compression_ratio": 1.5777777777777777, "no_speech_prob": 2.1782283511129208e-05}, {"id": 1022, "seek": 514182, "start": 5141.82, "end": 5150.259999999999, "text": " Let me get back to you about that during the week and remind me if I forget.", "tokens": [961, 385, 483, 646, 281, 291, 466, 300, 1830, 264, 1243, 293, 4160, 385, 498, 286, 2870, 13], "temperature": 0.0, "avg_logprob": -0.1794393806047337, "compression_ratio": 1.5990338164251208, "no_speech_prob": 1.1659313713607844e-05}, {"id": 1023, "seek": 514182, "start": 5150.259999999999, "end": 5152.42, "text": " So we've got our generator and our discriminator.", "tokens": [407, 321, 600, 658, 527, 19265, 293, 527, 20828, 1639, 13], "temperature": 0.0, "avg_logprob": -0.1794393806047337, "compression_ratio": 1.5990338164251208, "no_speech_prob": 1.1659313713607844e-05}, {"id": 1024, "seek": 514182, "start": 5152.42, "end": 5159.96, "text": " So we need a function that returns a prior vector, so a bunch of noise.", "tokens": [407, 321, 643, 257, 2445, 300, 11247, 257, 4059, 8062, 11, 370, 257, 3840, 295, 5658, 13], "temperature": 0.0, "avg_logprob": -0.1794393806047337, "compression_ratio": 1.5990338164251208, "no_speech_prob": 1.1659313713607844e-05}, {"id": 1025, "seek": 514182, "start": 5159.96, "end": 5163.299999999999, "text": " So we do that by creating a bunch of zeros.", "tokens": [407, 321, 360, 300, 538, 4084, 257, 3840, 295, 35193, 13], "temperature": 0.0, "avg_logprob": -0.1794393806047337, "compression_ratio": 1.5990338164251208, "no_speech_prob": 1.1659313713607844e-05}, {"id": 1026, "seek": 514182, "start": 5163.299999999999, "end": 5169.74, "text": " nz is the size of z, so like very often in our code if you see a mysterious letter, it's", "tokens": [297, 89, 307, 264, 2744, 295, 710, 11, 370, 411, 588, 2049, 294, 527, 3089, 498, 291, 536, 257, 13831, 5063, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.1794393806047337, "compression_ratio": 1.5990338164251208, "no_speech_prob": 1.1659313713607844e-05}, {"id": 1027, "seek": 516974, "start": 5169.74, "end": 5172.179999999999, "text": " because that's the letter they used in the paper.", "tokens": [570, 300, 311, 264, 5063, 436, 1143, 294, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.18023328220143037, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.1300637197564356e-05}, {"id": 1028, "seek": 516974, "start": 5172.179999999999, "end": 5177.0199999999995, "text": " So z is the size of our noise vector.", "tokens": [407, 710, 307, 264, 2744, 295, 527, 5658, 8062, 13], "temperature": 0.0, "avg_logprob": -0.18023328220143037, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.1300637197564356e-05}, {"id": 1029, "seek": 516974, "start": 5177.0199999999995, "end": 5182.36, "text": " So there's the size of our noise vector, and then we use a normal distribution to generate", "tokens": [407, 456, 311, 264, 2744, 295, 527, 5658, 8062, 11, 293, 550, 321, 764, 257, 2710, 7316, 281, 8460], "temperature": 0.0, "avg_logprob": -0.18023328220143037, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.1300637197564356e-05}, {"id": 1030, "seek": 516974, "start": 5182.36, "end": 5185.219999999999, "text": " random numbers inside that.", "tokens": [4974, 3547, 1854, 300, 13], "temperature": 0.0, "avg_logprob": -0.18023328220143037, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.1300637197564356e-05}, {"id": 1031, "seek": 516974, "start": 5185.219999999999, "end": 5189.38, "text": " And that needs to be a variable because it's going to be participating in the gradient", "tokens": [400, 300, 2203, 281, 312, 257, 7006, 570, 309, 311, 516, 281, 312, 13950, 294, 264, 16235], "temperature": 0.0, "avg_logprob": -0.18023328220143037, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.1300637197564356e-05}, {"id": 1032, "seek": 516974, "start": 5189.38, "end": 5192.82, "text": " updates.", "tokens": [9205, 13], "temperature": 0.0, "avg_logprob": -0.18023328220143037, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.1300637197564356e-05}, {"id": 1033, "seek": 516974, "start": 5192.82, "end": 5195.58, "text": " So here's an example of creating some noise.", "tokens": [407, 510, 311, 364, 1365, 295, 4084, 512, 5658, 13], "temperature": 0.0, "avg_logprob": -0.18023328220143037, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.1300637197564356e-05}, {"id": 1034, "seek": 516974, "start": 5195.58, "end": 5198.58, "text": " And so here are 4 different pieces of noise.", "tokens": [400, 370, 510, 366, 1017, 819, 3755, 295, 5658, 13], "temperature": 0.0, "avg_logprob": -0.18023328220143037, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.1300637197564356e-05}, {"id": 1035, "seek": 519858, "start": 5198.58, "end": 5207.46, "text": " So we need an optimizer in order to update our gradients.", "tokens": [407, 321, 643, 364, 5028, 6545, 294, 1668, 281, 5623, 527, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.16745057635837132, "compression_ratio": 1.5508982035928143, "no_speech_prob": 4.710887878900394e-06}, {"id": 1036, "seek": 519858, "start": 5207.46, "end": 5213.5, "text": " In the Vossestein-Gann paper, they told us to use rmsprop.", "tokens": [682, 264, 691, 772, 8887, 259, 12, 38, 969, 3035, 11, 436, 1907, 505, 281, 764, 367, 2592, 79, 1513, 13], "temperature": 0.0, "avg_logprob": -0.16745057635837132, "compression_ratio": 1.5508982035928143, "no_speech_prob": 4.710887878900394e-06}, {"id": 1037, "seek": 519858, "start": 5213.5, "end": 5214.78, "text": " So that's fine.", "tokens": [407, 300, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.16745057635837132, "compression_ratio": 1.5508982035928143, "no_speech_prob": 4.710887878900394e-06}, {"id": 1038, "seek": 519858, "start": 5214.78, "end": 5219.34, "text": " So when you see this thing saying do an rmsprop update in a paper, that's nice.", "tokens": [407, 562, 291, 536, 341, 551, 1566, 360, 364, 367, 2592, 79, 1513, 5623, 294, 257, 3035, 11, 300, 311, 1481, 13], "temperature": 0.0, "avg_logprob": -0.16745057635837132, "compression_ratio": 1.5508982035928143, "no_speech_prob": 4.710887878900394e-06}, {"id": 1039, "seek": 519858, "start": 5219.34, "end": 5224.86, "text": " We can just do an rmsprop update with PyTorch.", "tokens": [492, 393, 445, 360, 364, 367, 2592, 79, 1513, 5623, 365, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.16745057635837132, "compression_ratio": 1.5508982035928143, "no_speech_prob": 4.710887878900394e-06}, {"id": 1040, "seek": 522486, "start": 5224.86, "end": 5230.099999999999, "text": " And they suggested a learning rate of 5e-5.", "tokens": [400, 436, 10945, 257, 2539, 3314, 295, 1025, 68, 12, 20, 13], "temperature": 0.0, "avg_logprob": -0.08972431688892599, "compression_ratio": 1.6067961165048543, "no_speech_prob": 4.6644481699331664e-07}, {"id": 1041, "seek": 522486, "start": 5230.099999999999, "end": 5234.86, "text": " I think I found 1e-4 seemed to work, so I just made it a bit bigger.", "tokens": [286, 519, 286, 1352, 502, 68, 12, 19, 6576, 281, 589, 11, 370, 286, 445, 1027, 309, 257, 857, 3801, 13], "temperature": 0.0, "avg_logprob": -0.08972431688892599, "compression_ratio": 1.6067961165048543, "no_speech_prob": 4.6644481699331664e-07}, {"id": 1042, "seek": 522486, "start": 5234.86, "end": 5237.0199999999995, "text": " So now we need a training loop.", "tokens": [407, 586, 321, 643, 257, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.08972431688892599, "compression_ratio": 1.6067961165048543, "no_speech_prob": 4.6644481699331664e-07}, {"id": 1043, "seek": 522486, "start": 5237.0199999999995, "end": 5241.58, "text": " And so this is the thing that's going to implement this algorithm.", "tokens": [400, 370, 341, 307, 264, 551, 300, 311, 516, 281, 4445, 341, 9284, 13], "temperature": 0.0, "avg_logprob": -0.08972431688892599, "compression_ratio": 1.6067961165048543, "no_speech_prob": 4.6644481699331664e-07}, {"id": 1044, "seek": 522486, "start": 5241.58, "end": 5247.5599999999995, "text": " So a training loop is going to go through some number of epochs that we get to pick.", "tokens": [407, 257, 3097, 6367, 307, 516, 281, 352, 807, 512, 1230, 295, 30992, 28346, 300, 321, 483, 281, 1888, 13], "temperature": 0.0, "avg_logprob": -0.08972431688892599, "compression_ratio": 1.6067961165048543, "no_speech_prob": 4.6644481699331664e-07}, {"id": 1045, "seek": 522486, "start": 5247.5599999999995, "end": 5251.179999999999, "text": " So that's going to be a parameter.", "tokens": [407, 300, 311, 516, 281, 312, 257, 13075, 13], "temperature": 0.0, "avg_logprob": -0.08972431688892599, "compression_ratio": 1.6067961165048543, "no_speech_prob": 4.6644481699331664e-07}, {"id": 1046, "seek": 525118, "start": 5251.18, "end": 5255.14, "text": " And so remember, when you do everything manually, you've got to remember all the manual steps", "tokens": [400, 370, 1604, 11, 562, 291, 360, 1203, 16945, 11, 291, 600, 658, 281, 1604, 439, 264, 9688, 4439], "temperature": 0.0, "avg_logprob": -0.13382326881840545, "compression_ratio": 1.8376068376068375, "no_speech_prob": 9.66604784480296e-06}, {"id": 1047, "seek": 525118, "start": 5255.14, "end": 5256.14, "text": " to do.", "tokens": [281, 360, 13], "temperature": 0.0, "avg_logprob": -0.13382326881840545, "compression_ratio": 1.8376068376068375, "no_speech_prob": 9.66604784480296e-06}, {"id": 1048, "seek": 525118, "start": 5256.14, "end": 5261.38, "text": " So one is that you have to set your modules into training mode when you're training them", "tokens": [407, 472, 307, 300, 291, 362, 281, 992, 428, 16679, 666, 3097, 4391, 562, 291, 434, 3097, 552], "temperature": 0.0, "avg_logprob": -0.13382326881840545, "compression_ratio": 1.8376068376068375, "no_speech_prob": 9.66604784480296e-06}, {"id": 1049, "seek": 525118, "start": 5261.38, "end": 5264.54, "text": " and into evaluation mode when you're evaluating them.", "tokens": [293, 666, 13344, 4391, 562, 291, 434, 27479, 552, 13], "temperature": 0.0, "avg_logprob": -0.13382326881840545, "compression_ratio": 1.8376068376068375, "no_speech_prob": 9.66604784480296e-06}, {"id": 1050, "seek": 525118, "start": 5264.54, "end": 5269.9800000000005, "text": " Because in training mode, batch norm updates happen and dropout happens.", "tokens": [1436, 294, 3097, 4391, 11, 15245, 2026, 9205, 1051, 293, 3270, 346, 2314, 13], "temperature": 0.0, "avg_logprob": -0.13382326881840545, "compression_ratio": 1.8376068376068375, "no_speech_prob": 9.66604784480296e-06}, {"id": 1051, "seek": 525118, "start": 5269.9800000000005, "end": 5273.3, "text": " In evaluation mode, those 2 things get turned off.", "tokens": [682, 13344, 4391, 11, 729, 568, 721, 483, 3574, 766, 13], "temperature": 0.0, "avg_logprob": -0.13382326881840545, "compression_ratio": 1.8376068376068375, "no_speech_prob": 9.66604784480296e-06}, {"id": 1052, "seek": 525118, "start": 5273.3, "end": 5274.3, "text": " That's basically the difference.", "tokens": [663, 311, 1936, 264, 2649, 13], "temperature": 0.0, "avg_logprob": -0.13382326881840545, "compression_ratio": 1.8376068376068375, "no_speech_prob": 9.66604784480296e-06}, {"id": 1053, "seek": 525118, "start": 5274.3, "end": 5278.22, "text": " So put it into training mode.", "tokens": [407, 829, 309, 666, 3097, 4391, 13], "temperature": 0.0, "avg_logprob": -0.13382326881840545, "compression_ratio": 1.8376068376068375, "no_speech_prob": 9.66604784480296e-06}, {"id": 1054, "seek": 527822, "start": 5278.22, "end": 5281.9400000000005, "text": " We're going to grab an iterator from our training data loader.", "tokens": [492, 434, 516, 281, 4444, 364, 17138, 1639, 490, 527, 3097, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.12101302929778597, "compression_ratio": 1.6323529411764706, "no_speech_prob": 9.080428753804881e-06}, {"id": 1055, "seek": 527822, "start": 5281.9400000000005, "end": 5284.7, "text": " We're going to see how many steps we have to go through.", "tokens": [492, 434, 516, 281, 536, 577, 867, 4439, 321, 362, 281, 352, 807, 13], "temperature": 0.0, "avg_logprob": -0.12101302929778597, "compression_ratio": 1.6323529411764706, "no_speech_prob": 9.080428753804881e-06}, {"id": 1056, "seek": 527822, "start": 5284.7, "end": 5290.06, "text": " And then we'll use tqdm to give us a progress bar.", "tokens": [400, 550, 321, 603, 764, 256, 80, 67, 76, 281, 976, 505, 257, 4205, 2159, 13], "temperature": 0.0, "avg_logprob": -0.12101302929778597, "compression_ratio": 1.6323529411764706, "no_speech_prob": 9.080428753804881e-06}, {"id": 1057, "seek": 527822, "start": 5290.06, "end": 5294.1, "text": " And then we're going to go through that many steps.", "tokens": [400, 550, 321, 434, 516, 281, 352, 807, 300, 867, 4439, 13], "temperature": 0.0, "avg_logprob": -0.12101302929778597, "compression_ratio": 1.6323529411764706, "no_speech_prob": 9.080428753804881e-06}, {"id": 1058, "seek": 529410, "start": 5294.1, "end": 5310.08, "text": " So the first step of this algorithm is to update the discriminator.", "tokens": [407, 264, 700, 1823, 295, 341, 9284, 307, 281, 5623, 264, 20828, 1639, 13], "temperature": 0.0, "avg_logprob": -0.10519557840683881, "compression_ratio": 1.5344827586206897, "no_speech_prob": 1.6536862403881969e-06}, {"id": 1059, "seek": 529410, "start": 5310.08, "end": 5319.9800000000005, "text": " So in this one, they don't call it a discriminator, they call it a critic.", "tokens": [407, 294, 341, 472, 11, 436, 500, 380, 818, 309, 257, 20828, 1639, 11, 436, 818, 309, 257, 7850, 13], "temperature": 0.0, "avg_logprob": -0.10519557840683881, "compression_ratio": 1.5344827586206897, "no_speech_prob": 1.6536862403881969e-06}, {"id": 1060, "seek": 529410, "start": 5319.9800000000005, "end": 5323.42, "text": " So w are the weights of the critic.", "tokens": [407, 261, 366, 264, 17443, 295, 264, 7850, 13], "temperature": 0.0, "avg_logprob": -0.10519557840683881, "compression_ratio": 1.5344827586206897, "no_speech_prob": 1.6536862403881969e-06}, {"id": 1061, "seek": 532342, "start": 5323.42, "end": 5328.7, "text": " So the first step is to train our critic a little bit, and then we're going to train", "tokens": [407, 264, 700, 1823, 307, 281, 3847, 527, 7850, 257, 707, 857, 11, 293, 550, 321, 434, 516, 281, 3847], "temperature": 0.0, "avg_logprob": -0.141344211140617, "compression_ratio": 2.1148325358851676, "no_speech_prob": 1.2029548088321462e-05}, {"id": 1062, "seek": 532342, "start": 5328.7, "end": 5332.26, "text": " our generator a little bit, and then we're going to go back to the top of the loop.", "tokens": [527, 19265, 257, 707, 857, 11, 293, 550, 321, 434, 516, 281, 352, 646, 281, 264, 1192, 295, 264, 6367, 13], "temperature": 0.0, "avg_logprob": -0.141344211140617, "compression_ratio": 2.1148325358851676, "no_speech_prob": 1.2029548088321462e-05}, {"id": 1063, "seek": 532342, "start": 5332.26, "end": 5340.06, "text": " So we've got a while loop on the outside, and then inside that, there's another loop", "tokens": [407, 321, 600, 658, 257, 1339, 6367, 322, 264, 2380, 11, 293, 550, 1854, 300, 11, 456, 311, 1071, 6367], "temperature": 0.0, "avg_logprob": -0.141344211140617, "compression_ratio": 2.1148325358851676, "no_speech_prob": 1.2029548088321462e-05}, {"id": 1064, "seek": 532342, "start": 5340.06, "end": 5344.9400000000005, "text": " for the critic, and so here's our little loop inside that for the critic.", "tokens": [337, 264, 7850, 11, 293, 370, 510, 311, 527, 707, 6367, 1854, 300, 337, 264, 7850, 13], "temperature": 0.0, "avg_logprob": -0.141344211140617, "compression_ratio": 2.1148325358851676, "no_speech_prob": 1.2029548088321462e-05}, {"id": 1065, "seek": 532342, "start": 5344.9400000000005, "end": 5347.1, "text": " We call it a discriminator.", "tokens": [492, 818, 309, 257, 20828, 1639, 13], "temperature": 0.0, "avg_logprob": -0.141344211140617, "compression_ratio": 2.1148325358851676, "no_speech_prob": 1.2029548088321462e-05}, {"id": 1066, "seek": 532342, "start": 5347.1, "end": 5353.38, "text": " So what we're going to do now is we've got a generator, and at the moment it's random.", "tokens": [407, 437, 321, 434, 516, 281, 360, 586, 307, 321, 600, 658, 257, 19265, 11, 293, 412, 264, 1623, 309, 311, 4974, 13], "temperature": 0.0, "avg_logprob": -0.141344211140617, "compression_ratio": 2.1148325358851676, "no_speech_prob": 1.2029548088321462e-05}, {"id": 1067, "seek": 535338, "start": 5353.38, "end": 5359.14, "text": " So our generator is going to generate stuff that looks something like this.", "tokens": [407, 527, 19265, 307, 516, 281, 8460, 1507, 300, 1542, 746, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.10320929686228435, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.4682716684765182e-05}, {"id": 1068, "seek": 535338, "start": 5359.14, "end": 5364.14, "text": " And so we need to first of all teach our discriminator to tell the difference between that and a", "tokens": [400, 370, 321, 643, 281, 700, 295, 439, 2924, 527, 20828, 1639, 281, 980, 264, 2649, 1296, 300, 293, 257], "temperature": 0.0, "avg_logprob": -0.10320929686228435, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.4682716684765182e-05}, {"id": 1069, "seek": 535338, "start": 5364.14, "end": 5369.38, "text": " bedroom, which shouldn't be too hard, you would hope.", "tokens": [11211, 11, 597, 4659, 380, 312, 886, 1152, 11, 291, 576, 1454, 13], "temperature": 0.0, "avg_logprob": -0.10320929686228435, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.4682716684765182e-05}, {"id": 1070, "seek": 535338, "start": 5369.38, "end": 5374.82, "text": " So we just do it in basically the usual way, but there's a few little tweaks.", "tokens": [407, 321, 445, 360, 309, 294, 1936, 264, 7713, 636, 11, 457, 456, 311, 257, 1326, 707, 46664, 13], "temperature": 0.0, "avg_logprob": -0.10320929686228435, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.4682716684765182e-05}, {"id": 1071, "seek": 535338, "start": 5374.82, "end": 5380.58, "text": " So first of all, we're going to grab a mini batch of real bedroom photos.", "tokens": [407, 700, 295, 439, 11, 321, 434, 516, 281, 4444, 257, 8382, 15245, 295, 957, 11211, 5787, 13], "temperature": 0.0, "avg_logprob": -0.10320929686228435, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.4682716684765182e-05}, {"id": 1072, "seek": 538058, "start": 5380.58, "end": 5391.0599999999995, "text": " So we can just grab the next batch from our iterator, turn it into a variable.", "tokens": [407, 321, 393, 445, 4444, 264, 958, 15245, 490, 527, 17138, 1639, 11, 1261, 309, 666, 257, 7006, 13], "temperature": 0.0, "avg_logprob": -0.14764966097745028, "compression_ratio": 1.4137931034482758, "no_speech_prob": 1.602807628842129e-06}, {"id": 1073, "seek": 538058, "start": 5391.0599999999995, "end": 5396.94, "text": " Then we're going to calculate the loss for that.", "tokens": [1396, 321, 434, 516, 281, 8873, 264, 4470, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.14764966097745028, "compression_ratio": 1.4137931034482758, "no_speech_prob": 1.602807628842129e-06}, {"id": 1074, "seek": 538058, "start": 5396.94, "end": 5408.26, "text": " So this is going to be how much does the discriminator think this looks fake.", "tokens": [407, 341, 307, 516, 281, 312, 577, 709, 775, 264, 20828, 1639, 519, 341, 1542, 7592, 13], "temperature": 0.0, "avg_logprob": -0.14764966097745028, "compression_ratio": 1.4137931034482758, "no_speech_prob": 1.602807628842129e-06}, {"id": 1075, "seek": 540826, "start": 5408.26, "end": 5412.34, "text": " This looks fake, the real ones look fake.", "tokens": [639, 1542, 7592, 11, 264, 957, 2306, 574, 7592, 13], "temperature": 0.0, "avg_logprob": -0.12097847218416174, "compression_ratio": 1.799043062200957, "no_speech_prob": 5.1739002628892194e-06}, {"id": 1076, "seek": 540826, "start": 5412.34, "end": 5418.26, "text": " And then we're going to create some fake images, and to do that we'll create some random noise,", "tokens": [400, 550, 321, 434, 516, 281, 1884, 512, 7592, 5267, 11, 293, 281, 360, 300, 321, 603, 1884, 512, 4974, 5658, 11], "temperature": 0.0, "avg_logprob": -0.12097847218416174, "compression_ratio": 1.799043062200957, "no_speech_prob": 5.1739002628892194e-06}, {"id": 1077, "seek": 540826, "start": 5418.26, "end": 5421.900000000001, "text": " and we'll stick it through our generator, which at this stage is just a bunch of random", "tokens": [293, 321, 603, 2897, 309, 807, 527, 19265, 11, 597, 412, 341, 3233, 307, 445, 257, 3840, 295, 4974], "temperature": 0.0, "avg_logprob": -0.12097847218416174, "compression_ratio": 1.799043062200957, "no_speech_prob": 5.1739002628892194e-06}, {"id": 1078, "seek": 540826, "start": 5421.900000000001, "end": 5423.62, "text": " weights.", "tokens": [17443, 13], "temperature": 0.0, "avg_logprob": -0.12097847218416174, "compression_ratio": 1.799043062200957, "no_speech_prob": 5.1739002628892194e-06}, {"id": 1079, "seek": 540826, "start": 5423.62, "end": 5427.06, "text": " And that's going to create a mini batch of fake images.", "tokens": [400, 300, 311, 516, 281, 1884, 257, 8382, 15245, 295, 7592, 5267, 13], "temperature": 0.0, "avg_logprob": -0.12097847218416174, "compression_ratio": 1.799043062200957, "no_speech_prob": 5.1739002628892194e-06}, {"id": 1080, "seek": 540826, "start": 5427.06, "end": 5432.900000000001, "text": " And so then we'll put that through the same discriminator module as before to get the", "tokens": [400, 370, 550, 321, 603, 829, 300, 807, 264, 912, 20828, 1639, 10088, 382, 949, 281, 483, 264], "temperature": 0.0, "avg_logprob": -0.12097847218416174, "compression_ratio": 1.799043062200957, "no_speech_prob": 5.1739002628892194e-06}, {"id": 1081, "seek": 543290, "start": 5432.9, "end": 5438.58, "text": " loss for that, so how fake do the fake ones look.", "tokens": [4470, 337, 300, 11, 370, 577, 7592, 360, 264, 7592, 2306, 574, 13], "temperature": 0.0, "avg_logprob": -0.16735713822501047, "compression_ratio": 1.5980392156862746, "no_speech_prob": 6.748025498382049e-06}, {"id": 1082, "seek": 543290, "start": 5438.58, "end": 5443.86, "text": " Remember when you do everything manually, you have to zero the gradients in your loop.", "tokens": [5459, 562, 291, 360, 1203, 16945, 11, 291, 362, 281, 4018, 264, 2771, 2448, 294, 428, 6367, 13], "temperature": 0.0, "avg_logprob": -0.16735713822501047, "compression_ratio": 1.5980392156862746, "no_speech_prob": 6.748025498382049e-06}, {"id": 1083, "seek": 543290, "start": 5443.86, "end": 5448.9, "text": " If you've forgotten about that, go back to the part 1 lesson where we do everything from", "tokens": [759, 291, 600, 11832, 466, 300, 11, 352, 646, 281, 264, 644, 502, 6898, 689, 321, 360, 1203, 490], "temperature": 0.0, "avg_logprob": -0.16735713822501047, "compression_ratio": 1.5980392156862746, "no_speech_prob": 6.748025498382049e-06}, {"id": 1084, "seek": 543290, "start": 5448.9, "end": 5449.9, "text": " scratch.", "tokens": [8459, 13], "temperature": 0.0, "avg_logprob": -0.16735713822501047, "compression_ratio": 1.5980392156862746, "no_speech_prob": 6.748025498382049e-06}, {"id": 1085, "seek": 543290, "start": 5449.9, "end": 5459.0, "text": " So now finally, the total discriminator loss is equal to the real loss minus the fake loss.", "tokens": [407, 586, 2721, 11, 264, 3217, 20828, 1639, 4470, 307, 2681, 281, 264, 957, 4470, 3175, 264, 7592, 4470, 13], "temperature": 0.0, "avg_logprob": -0.16735713822501047, "compression_ratio": 1.5980392156862746, "no_speech_prob": 6.748025498382049e-06}, {"id": 1086, "seek": 545900, "start": 5459.0, "end": 5465.7, "text": " And so you can see that here, they don't talk about the loss, they actually just talk about", "tokens": [400, 370, 291, 393, 536, 300, 510, 11, 436, 500, 380, 751, 466, 264, 4470, 11, 436, 767, 445, 751, 466], "temperature": 0.0, "avg_logprob": -0.11954735006604876, "compression_ratio": 1.7841409691629957, "no_speech_prob": 2.3320703803619836e-06}, {"id": 1087, "seek": 545900, "start": 5465.7, "end": 5467.78, "text": " what are the gradient updates.", "tokens": [437, 366, 264, 16235, 9205, 13], "temperature": 0.0, "avg_logprob": -0.11954735006604876, "compression_ratio": 1.7841409691629957, "no_speech_prob": 2.3320703803619836e-06}, {"id": 1088, "seek": 545900, "start": 5467.78, "end": 5472.26, "text": " So this here is the symbol for get the gradients.", "tokens": [407, 341, 510, 307, 264, 5986, 337, 483, 264, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.11954735006604876, "compression_ratio": 1.7841409691629957, "no_speech_prob": 2.3320703803619836e-06}, {"id": 1089, "seek": 545900, "start": 5472.26, "end": 5475.58, "text": " So inside here is the loss.", "tokens": [407, 1854, 510, 307, 264, 4470, 13], "temperature": 0.0, "avg_logprob": -0.11954735006604876, "compression_ratio": 1.7841409691629957, "no_speech_prob": 2.3320703803619836e-06}, {"id": 1090, "seek": 545900, "start": 5475.58, "end": 5479.72, "text": " And try to learn to throw away in your head all of the boring stuff.", "tokens": [400, 853, 281, 1466, 281, 3507, 1314, 294, 428, 1378, 439, 295, 264, 9989, 1507, 13], "temperature": 0.0, "avg_logprob": -0.11954735006604876, "compression_ratio": 1.7841409691629957, "no_speech_prob": 2.3320703803619836e-06}, {"id": 1091, "seek": 545900, "start": 5479.72, "end": 5485.46, "text": " So when you see sum over m divided by m, that means take the average.", "tokens": [407, 562, 291, 536, 2408, 670, 275, 6666, 538, 275, 11, 300, 1355, 747, 264, 4274, 13], "temperature": 0.0, "avg_logprob": -0.11954735006604876, "compression_ratio": 1.7841409691629957, "no_speech_prob": 2.3320703803619836e-06}, {"id": 1092, "seek": 545900, "start": 5485.46, "end": 5488.86, "text": " So just throw that away and replace it with np.mean in your head.", "tokens": [407, 445, 3507, 300, 1314, 293, 7406, 309, 365, 33808, 13, 1398, 282, 294, 428, 1378, 13], "temperature": 0.0, "avg_logprob": -0.11954735006604876, "compression_ratio": 1.7841409691629957, "no_speech_prob": 2.3320703803619836e-06}, {"id": 1093, "seek": 548886, "start": 5488.86, "end": 5489.86, "text": " There's another np.mean.", "tokens": [821, 311, 1071, 33808, 13, 1398, 282, 13], "temperature": 0.0, "avg_logprob": -0.17398857652095326, "compression_ratio": 1.6346153846153846, "no_speech_prob": 8.801067451713607e-06}, {"id": 1094, "seek": 548886, "start": 5489.86, "end": 5494.74, "text": " So you want to get quick at being able to see these common idioms.", "tokens": [407, 291, 528, 281, 483, 1702, 412, 885, 1075, 281, 536, 613, 2689, 18014, 4785, 13], "temperature": 0.0, "avg_logprob": -0.17398857652095326, "compression_ratio": 1.6346153846153846, "no_speech_prob": 8.801067451713607e-06}, {"id": 1095, "seek": 548886, "start": 5494.74, "end": 5499.98, "text": " So anytime you see 1 over m sum over m, you go, okay, np.mean.", "tokens": [407, 13038, 291, 536, 502, 670, 275, 2408, 670, 275, 11, 291, 352, 11, 1392, 11, 33808, 13, 1398, 282, 13], "temperature": 0.0, "avg_logprob": -0.17398857652095326, "compression_ratio": 1.6346153846153846, "no_speech_prob": 8.801067451713607e-06}, {"id": 1096, "seek": 548886, "start": 5499.98, "end": 5505.66, "text": " So we're taking the mean of, and we're taking the mean of, so that's all fine.", "tokens": [407, 321, 434, 1940, 264, 914, 295, 11, 293, 321, 434, 1940, 264, 914, 295, 11, 370, 300, 311, 439, 2489, 13], "temperature": 0.0, "avg_logprob": -0.17398857652095326, "compression_ratio": 1.6346153846153846, "no_speech_prob": 8.801067451713607e-06}, {"id": 1097, "seek": 548886, "start": 5505.66, "end": 5507.42, "text": " Xi, what's Xi?", "tokens": [15712, 11, 437, 311, 15712, 30], "temperature": 0.0, "avg_logprob": -0.17398857652095326, "compression_ratio": 1.6346153846153846, "no_speech_prob": 8.801067451713607e-06}, {"id": 1098, "seek": 548886, "start": 5507.42, "end": 5511.179999999999, "text": " It looks like it's x to the power of i, but it's not.", "tokens": [467, 1542, 411, 309, 311, 2031, 281, 264, 1347, 295, 741, 11, 457, 309, 311, 406, 13], "temperature": 0.0, "avg_logprob": -0.17398857652095326, "compression_ratio": 1.6346153846153846, "no_speech_prob": 8.801067451713607e-06}, {"id": 1099, "seek": 548886, "start": 5511.179999999999, "end": 5513.179999999999, "text": " The math notation is very overloaded.", "tokens": [440, 5221, 24657, 307, 588, 28777, 292, 13], "temperature": 0.0, "avg_logprob": -0.17398857652095326, "compression_ratio": 1.6346153846153846, "no_speech_prob": 8.801067451713607e-06}, {"id": 1100, "seek": 551318, "start": 5513.18, "end": 5522.38, "text": " So it's showed us here what Xi is, and it's a set of m samples from a batch of the real", "tokens": [407, 309, 311, 4712, 505, 510, 437, 15712, 307, 11, 293, 309, 311, 257, 992, 295, 275, 10938, 490, 257, 15245, 295, 264, 957], "temperature": 0.0, "avg_logprob": -0.14858802011079877, "compression_ratio": 1.7014925373134329, "no_speech_prob": 1.118940986089001e-06}, {"id": 1101, "seek": 551318, "start": 5522.38, "end": 5523.38, "text": " data.", "tokens": [1412, 13], "temperature": 0.0, "avg_logprob": -0.14858802011079877, "compression_ratio": 1.7014925373134329, "no_speech_prob": 1.118940986089001e-06}, {"id": 1102, "seek": 551318, "start": 5523.38, "end": 5525.780000000001, "text": " So in other words, this is a mini-batch.", "tokens": [407, 294, 661, 2283, 11, 341, 307, 257, 8382, 12, 65, 852, 13], "temperature": 0.0, "avg_logprob": -0.14858802011079877, "compression_ratio": 1.7014925373134329, "no_speech_prob": 1.118940986089001e-06}, {"id": 1103, "seek": 551318, "start": 5525.780000000001, "end": 5530.740000000001, "text": " So when you see something saying sample, it means just grab a row, grab a row, grab a", "tokens": [407, 562, 291, 536, 746, 1566, 6889, 11, 309, 1355, 445, 4444, 257, 5386, 11, 4444, 257, 5386, 11, 4444, 257], "temperature": 0.0, "avg_logprob": -0.14858802011079877, "compression_ratio": 1.7014925373134329, "no_speech_prob": 1.118940986089001e-06}, {"id": 1104, "seek": 551318, "start": 5530.740000000001, "end": 5536.740000000001, "text": " row, and you can see here, grab it m times, and we'll call the first row x parenthesis", "tokens": [5386, 11, 293, 291, 393, 536, 510, 11, 4444, 309, 275, 1413, 11, 293, 321, 603, 818, 264, 700, 5386, 2031, 23350, 9374], "temperature": 0.0, "avg_logprob": -0.14858802011079877, "compression_ratio": 1.7014925373134329, "no_speech_prob": 1.118940986089001e-06}, {"id": 1105, "seek": 551318, "start": 5536.740000000001, "end": 5538.66, "text": " 1, the second row x parenthesis 2.", "tokens": [502, 11, 264, 1150, 5386, 2031, 23350, 9374, 568, 13], "temperature": 0.0, "avg_logprob": -0.14858802011079877, "compression_ratio": 1.7014925373134329, "no_speech_prob": 1.118940986089001e-06}, {"id": 1106, "seek": 553866, "start": 5538.66, "end": 5548.58, "text": " One of the annoying things about math notation is the way that we index into arrays is everybody", "tokens": [1485, 295, 264, 11304, 721, 466, 5221, 24657, 307, 264, 636, 300, 321, 8186, 666, 41011, 307, 2201], "temperature": 0.0, "avg_logprob": -0.1374160171648778, "compression_ratio": 1.6599190283400809, "no_speech_prob": 6.439009212044766e-06}, {"id": 1107, "seek": 553866, "start": 5548.58, "end": 5553.78, "text": " uses different approaches, subscripts, superscripts, things in brackets, combinations, commas,", "tokens": [4960, 819, 11587, 11, 2325, 39280, 11, 37906, 5944, 82, 11, 721, 294, 26179, 11, 21267, 11, 800, 296, 11], "temperature": 0.0, "avg_logprob": -0.1374160171648778, "compression_ratio": 1.6599190283400809, "no_speech_prob": 6.439009212044766e-06}, {"id": 1108, "seek": 553866, "start": 5553.78, "end": 5555.34, "text": " square brackets, whatever.", "tokens": [3732, 26179, 11, 2035, 13], "temperature": 0.0, "avg_logprob": -0.1374160171648778, "compression_ratio": 1.6599190283400809, "no_speech_prob": 6.439009212044766e-06}, {"id": 1109, "seek": 553866, "start": 5555.34, "end": 5559.599999999999, "text": " So you've just got to look in the paper and be like, okay, at some point they're going", "tokens": [407, 291, 600, 445, 658, 281, 574, 294, 264, 3035, 293, 312, 411, 11, 1392, 11, 412, 512, 935, 436, 434, 516], "temperature": 0.0, "avg_logprob": -0.1374160171648778, "compression_ratio": 1.6599190283400809, "no_speech_prob": 6.439009212044766e-06}, {"id": 1110, "seek": 553866, "start": 5559.599999999999, "end": 5565.54, "text": " to say take the i-th row from this matrix, or the i-th image in this batch, how are they", "tokens": [281, 584, 747, 264, 741, 12, 392, 5386, 490, 341, 8141, 11, 420, 264, 741, 12, 392, 3256, 294, 341, 15245, 11, 577, 366, 436], "temperature": 0.0, "avg_logprob": -0.1374160171648778, "compression_ratio": 1.6599190283400809, "no_speech_prob": 6.439009212044766e-06}, {"id": 1111, "seek": 553866, "start": 5565.54, "end": 5566.54, "text": " going to do it?", "tokens": [516, 281, 360, 309, 30], "temperature": 0.0, "avg_logprob": -0.1374160171648778, "compression_ratio": 1.6599190283400809, "no_speech_prob": 6.439009212044766e-06}, {"id": 1112, "seek": 556654, "start": 5566.54, "end": 5571.06, "text": " In this case, it's a superscript in parentheses.", "tokens": [682, 341, 1389, 11, 309, 311, 257, 37906, 5944, 294, 34153, 13], "temperature": 0.0, "avg_logprob": -0.11198745889866606, "compression_ratio": 1.5657894736842106, "no_speech_prob": 2.8573090276040602e-06}, {"id": 1113, "seek": 556654, "start": 5571.06, "end": 5575.9, "text": " So that's all sample means, and curly brackets means it's just a set of them.", "tokens": [407, 300, 311, 439, 6889, 1355, 11, 293, 32066, 26179, 1355, 309, 311, 445, 257, 992, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.11198745889866606, "compression_ratio": 1.5657894736842106, "no_speech_prob": 2.8573090276040602e-06}, {"id": 1114, "seek": 556654, "start": 5575.9, "end": 5584.58, "text": " This little squiggle followed by something here means according to some probability distribution.", "tokens": [639, 707, 2339, 19694, 6263, 538, 746, 510, 1355, 4650, 281, 512, 8482, 7316, 13], "temperature": 0.0, "avg_logprob": -0.11198745889866606, "compression_ratio": 1.5657894736842106, "no_speech_prob": 2.8573090276040602e-06}, {"id": 1115, "seek": 556654, "start": 5584.58, "end": 5589.38, "text": " And so in this case, and very, very often in papers, it simply means, hey, you've got", "tokens": [400, 370, 294, 341, 1389, 11, 293, 588, 11, 588, 2049, 294, 10577, 11, 309, 2935, 1355, 11, 4177, 11, 291, 600, 658], "temperature": 0.0, "avg_logprob": -0.11198745889866606, "compression_ratio": 1.5657894736842106, "no_speech_prob": 2.8573090276040602e-06}, {"id": 1116, "seek": 556654, "start": 5589.38, "end": 5593.78, "text": " a bunch of data, grab a bit from it at random.", "tokens": [257, 3840, 295, 1412, 11, 4444, 257, 857, 490, 309, 412, 4974, 13], "temperature": 0.0, "avg_logprob": -0.11198745889866606, "compression_ratio": 1.5657894736842106, "no_speech_prob": 2.8573090276040602e-06}, {"id": 1117, "seek": 559378, "start": 5593.78, "end": 5601.0599999999995, "text": " So that's the probability distribution of the data you have is the data you have.", "tokens": [407, 300, 311, 264, 8482, 7316, 295, 264, 1412, 291, 362, 307, 264, 1412, 291, 362, 13], "temperature": 0.0, "avg_logprob": -0.121569333757673, "compression_ratio": 1.8, "no_speech_prob": 5.422199592430843e-06}, {"id": 1118, "seek": 559378, "start": 5601.0599999999995, "end": 5607.099999999999, "text": " So this says grab m things at random from your real data.", "tokens": [407, 341, 1619, 4444, 275, 721, 412, 4974, 490, 428, 957, 1412, 13], "temperature": 0.0, "avg_logprob": -0.121569333757673, "compression_ratio": 1.8, "no_speech_prob": 5.422199592430843e-06}, {"id": 1119, "seek": 559378, "start": 5607.099999999999, "end": 5614.38, "text": " This says grab m things at random from your prior samples, and so that means, in other", "tokens": [639, 1619, 4444, 275, 721, 412, 4974, 490, 428, 4059, 10938, 11, 293, 370, 300, 1355, 11, 294, 661], "temperature": 0.0, "avg_logprob": -0.121569333757673, "compression_ratio": 1.8, "no_speech_prob": 5.422199592430843e-06}, {"id": 1120, "seek": 559378, "start": 5614.38, "end": 5622.62, "text": " words, call create noise to create m random vectors.", "tokens": [2283, 11, 818, 1884, 5658, 281, 1884, 275, 4974, 18875, 13], "temperature": 0.0, "avg_logprob": -0.121569333757673, "compression_ratio": 1.8, "no_speech_prob": 5.422199592430843e-06}, {"id": 1121, "seek": 562262, "start": 5622.62, "end": 5626.66, "text": " So now we've got m real images.", "tokens": [407, 586, 321, 600, 658, 275, 957, 5267, 13], "temperature": 0.0, "avg_logprob": -0.12056574870630637, "compression_ratio": 1.9829545454545454, "no_speech_prob": 5.771904852736043e-06}, {"id": 1122, "seek": 562262, "start": 5626.66, "end": 5632.94, "text": " Each one gets put through our discriminator.", "tokens": [6947, 472, 2170, 829, 807, 527, 20828, 1639, 13], "temperature": 0.0, "avg_logprob": -0.12056574870630637, "compression_ratio": 1.9829545454545454, "no_speech_prob": 5.771904852736043e-06}, {"id": 1123, "seek": 562262, "start": 5632.94, "end": 5636.58, "text": " We've got m bits of noise.", "tokens": [492, 600, 658, 275, 9239, 295, 5658, 13], "temperature": 0.0, "avg_logprob": -0.12056574870630637, "compression_ratio": 1.9829545454545454, "no_speech_prob": 5.771904852736043e-06}, {"id": 1124, "seek": 562262, "start": 5636.58, "end": 5643.5199999999995, "text": " Each one gets put through our generator to create m generated images.", "tokens": [6947, 472, 2170, 829, 807, 527, 19265, 281, 1884, 275, 10833, 5267, 13], "temperature": 0.0, "avg_logprob": -0.12056574870630637, "compression_ratio": 1.9829545454545454, "no_speech_prob": 5.771904852736043e-06}, {"id": 1125, "seek": 562262, "start": 5643.5199999999995, "end": 5647.26, "text": " Each one of those gets put through, look, FW, that's the same thing, so each one of", "tokens": [6947, 472, 295, 729, 2170, 829, 807, 11, 574, 11, 479, 54, 11, 300, 311, 264, 912, 551, 11, 370, 1184, 472, 295], "temperature": 0.0, "avg_logprob": -0.12056574870630637, "compression_ratio": 1.9829545454545454, "no_speech_prob": 5.771904852736043e-06}, {"id": 1126, "seek": 562262, "start": 5647.26, "end": 5650.94, "text": " those gets put through our discriminator to try and figure out whether they're fake or", "tokens": [729, 2170, 829, 807, 527, 20828, 1639, 281, 853, 293, 2573, 484, 1968, 436, 434, 7592, 420], "temperature": 0.0, "avg_logprob": -0.12056574870630637, "compression_ratio": 1.9829545454545454, "no_speech_prob": 5.771904852736043e-06}, {"id": 1127, "seek": 562262, "start": 5650.94, "end": 5651.94, "text": " not.", "tokens": [406, 13], "temperature": 0.0, "avg_logprob": -0.12056574870630637, "compression_ratio": 1.9829545454545454, "no_speech_prob": 5.771904852736043e-06}, {"id": 1128, "seek": 565194, "start": 5651.94, "end": 5658.099999999999, "text": " So then it's this minus this and the mean of that, and then finally get the gradient", "tokens": [407, 550, 309, 311, 341, 3175, 341, 293, 264, 914, 295, 300, 11, 293, 550, 2721, 483, 264, 16235], "temperature": 0.0, "avg_logprob": -0.17738427921217315, "compression_ratio": 1.5898617511520738, "no_speech_prob": 2.994428541569505e-06}, {"id": 1129, "seek": 565194, "start": 5658.099999999999, "end": 5665.259999999999, "text": " of that in order to figure out how to use RMSProp to update our weights using some learning", "tokens": [295, 300, 294, 1668, 281, 2573, 484, 577, 281, 764, 497, 10288, 47, 1513, 281, 5623, 527, 17443, 1228, 512, 2539], "temperature": 0.0, "avg_logprob": -0.17738427921217315, "compression_ratio": 1.5898617511520738, "no_speech_prob": 2.994428541569505e-06}, {"id": 1130, "seek": 565194, "start": 5665.259999999999, "end": 5667.219999999999, "text": " rate.", "tokens": [3314, 13], "temperature": 0.0, "avg_logprob": -0.17738427921217315, "compression_ratio": 1.5898617511520738, "no_speech_prob": 2.994428541569505e-06}, {"id": 1131, "seek": 565194, "start": 5667.219999999999, "end": 5673.86, "text": " So in PyTorch, we don't have to worry about getting the gradients.", "tokens": [407, 294, 9953, 51, 284, 339, 11, 321, 500, 380, 362, 281, 3292, 466, 1242, 264, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.17738427921217315, "compression_ratio": 1.5898617511520738, "no_speech_prob": 2.994428541569505e-06}, {"id": 1132, "seek": 565194, "start": 5673.86, "end": 5681.9, "text": " We can just specify the loss bit and then just say loss.backward, discriminator.optimizer.step.", "tokens": [492, 393, 445, 16500, 264, 4470, 857, 293, 550, 445, 584, 4470, 13, 3207, 1007, 11, 20828, 1639, 13, 5747, 332, 6545, 13, 16792, 13], "temperature": 0.0, "avg_logprob": -0.17738427921217315, "compression_ratio": 1.5898617511520738, "no_speech_prob": 2.994428541569505e-06}, {"id": 1133, "seek": 568190, "start": 5681.9, "end": 5694.62, "text": " There's one key step, which is that we have to keep all of our activations, sorry, all", "tokens": [821, 311, 472, 2141, 1823, 11, 597, 307, 300, 321, 362, 281, 1066, 439, 295, 527, 2430, 763, 11, 2597, 11, 439], "temperature": 0.0, "avg_logprob": -0.2330757264168032, "compression_ratio": 1.348993288590604, "no_speech_prob": 2.3687932753091445e-06}, {"id": 1134, "seek": 568190, "start": 5694.62, "end": 5701.62, "text": " of our weights, which are the parameters in a PyTorch module, in this small range between", "tokens": [295, 527, 17443, 11, 597, 366, 264, 9834, 294, 257, 9953, 51, 284, 339, 10088, 11, 294, 341, 1359, 3613, 1296], "temperature": 0.0, "avg_logprob": -0.2330757264168032, "compression_ratio": 1.348993288590604, "no_speech_prob": 2.3687932753091445e-06}, {"id": 1135, "seek": 568190, "start": 5701.62, "end": 5705.7, "text": " negative.01 and.01.", "tokens": [3671, 2411, 10607, 293, 2411, 10607, 13], "temperature": 0.0, "avg_logprob": -0.2330757264168032, "compression_ratio": 1.348993288590604, "no_speech_prob": 2.3687932753091445e-06}, {"id": 1136, "seek": 568190, "start": 5705.7, "end": 5707.54, "text": " Why?", "tokens": [1545, 30], "temperature": 0.0, "avg_logprob": -0.2330757264168032, "compression_ratio": 1.348993288590604, "no_speech_prob": 2.3687932753091445e-06}, {"id": 1137, "seek": 570754, "start": 5707.54, "end": 5714.78, "text": " Because the mathematical assumptions that make this algorithm work only apply in like", "tokens": [1436, 264, 18894, 17695, 300, 652, 341, 9284, 589, 787, 3079, 294, 411], "temperature": 0.0, "avg_logprob": -0.14013607333404848, "compression_ratio": 1.752212389380531, "no_speech_prob": 8.267830708064139e-06}, {"id": 1138, "seek": 570754, "start": 5714.78, "end": 5717.7, "text": " a small ball.", "tokens": [257, 1359, 2594, 13], "temperature": 0.0, "avg_logprob": -0.14013607333404848, "compression_ratio": 1.752212389380531, "no_speech_prob": 8.267830708064139e-06}, {"id": 1139, "seek": 570754, "start": 5717.7, "end": 5722.38, "text": " So I'm not going to tell, I think it's kind of interesting to understand the math of why", "tokens": [407, 286, 478, 406, 516, 281, 980, 11, 286, 519, 309, 311, 733, 295, 1880, 281, 1223, 264, 5221, 295, 983], "temperature": 0.0, "avg_logprob": -0.14013607333404848, "compression_ratio": 1.752212389380531, "no_speech_prob": 8.267830708064139e-06}, {"id": 1140, "seek": 570754, "start": 5722.38, "end": 5727.74, "text": " that's the case, but it's very specific to this one paper and understanding it won't", "tokens": [300, 311, 264, 1389, 11, 457, 309, 311, 588, 2685, 281, 341, 472, 3035, 293, 3701, 309, 1582, 380], "temperature": 0.0, "avg_logprob": -0.14013607333404848, "compression_ratio": 1.752212389380531, "no_speech_prob": 8.267830708064139e-06}, {"id": 1141, "seek": 570754, "start": 5727.74, "end": 5729.54, "text": " help you understand any other paper.", "tokens": [854, 291, 1223, 604, 661, 3035, 13], "temperature": 0.0, "avg_logprob": -0.14013607333404848, "compression_ratio": 1.752212389380531, "no_speech_prob": 8.267830708064139e-06}, {"id": 1142, "seek": 570754, "start": 5729.54, "end": 5734.46, "text": " So only study it if you're interested in, I think it's nicely explained, I think it's", "tokens": [407, 787, 2979, 309, 498, 291, 434, 3102, 294, 11, 286, 519, 309, 311, 9594, 8825, 11, 286, 519, 309, 311], "temperature": 0.0, "avg_logprob": -0.14013607333404848, "compression_ratio": 1.752212389380531, "no_speech_prob": 8.267830708064139e-06}, {"id": 1143, "seek": 573446, "start": 5734.46, "end": 5741.42, "text": " fun, but it won't be information that you'll reuse elsewhere unless you get super into", "tokens": [1019, 11, 457, 309, 1582, 380, 312, 1589, 300, 291, 603, 26225, 14517, 5969, 291, 483, 1687, 666], "temperature": 0.0, "avg_logprob": -0.15293328840653975, "compression_ratio": 1.5342465753424657, "no_speech_prob": 9.51608035393292e-06}, {"id": 1144, "seek": 573446, "start": 5741.42, "end": 5742.42, "text": " GANs.", "tokens": [460, 1770, 82, 13], "temperature": 0.0, "avg_logprob": -0.15293328840653975, "compression_ratio": 1.5342465753424657, "no_speech_prob": 9.51608035393292e-06}, {"id": 1145, "seek": 573446, "start": 5742.42, "end": 5747.94, "text": " I'll also mention, after the paper came out, an improved Frossestein GAN came out that", "tokens": [286, 603, 611, 2152, 11, 934, 264, 3035, 1361, 484, 11, 364, 9689, 1526, 772, 8887, 259, 460, 1770, 1361, 484, 300], "temperature": 0.0, "avg_logprob": -0.15293328840653975, "compression_ratio": 1.5342465753424657, "no_speech_prob": 9.51608035393292e-06}, {"id": 1146, "seek": 573446, "start": 5747.94, "end": 5754.38, "text": " said, hey, there are better ways to ensure that your weight space is in this tight ball,", "tokens": [848, 11, 4177, 11, 456, 366, 1101, 2098, 281, 5586, 300, 428, 3364, 1901, 307, 294, 341, 4524, 2594, 11], "temperature": 0.0, "avg_logprob": -0.15293328840653975, "compression_ratio": 1.5342465753424657, "no_speech_prob": 9.51608035393292e-06}, {"id": 1147, "seek": 573446, "start": 5754.38, "end": 5759.3, "text": " which is basically to kind of penalize gradients that are too high.", "tokens": [597, 307, 1936, 281, 733, 295, 13661, 1125, 2771, 2448, 300, 366, 886, 1090, 13], "temperature": 0.0, "avg_logprob": -0.15293328840653975, "compression_ratio": 1.5342465753424657, "no_speech_prob": 9.51608035393292e-06}, {"id": 1148, "seek": 575930, "start": 5759.3, "end": 5765.22, "text": " So actually nowadays there are slightly different ways to do this.", "tokens": [407, 767, 13434, 456, 366, 4748, 819, 2098, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.17213702201843262, "compression_ratio": 1.7195121951219512, "no_speech_prob": 8.664566848892719e-06}, {"id": 1149, "seek": 575930, "start": 5765.22, "end": 5768.74, "text": " That's why this line of code there, it's kind of the key contribution.", "tokens": [663, 311, 983, 341, 1622, 295, 3089, 456, 11, 309, 311, 733, 295, 264, 2141, 13150, 13], "temperature": 0.0, "avg_logprob": -0.17213702201843262, "compression_ratio": 1.7195121951219512, "no_speech_prob": 8.664566848892719e-06}, {"id": 1150, "seek": 575930, "start": 5768.74, "end": 5773.14, "text": " This one line of code actually is the one line of code you add to make a Frossestein", "tokens": [639, 472, 1622, 295, 3089, 767, 307, 264, 472, 1622, 295, 3089, 291, 909, 281, 652, 257, 1526, 772, 8887, 259], "temperature": 0.0, "avg_logprob": -0.17213702201843262, "compression_ratio": 1.7195121951219512, "no_speech_prob": 8.664566848892719e-06}, {"id": 1151, "seek": 575930, "start": 5773.14, "end": 5774.14, "text": " GAN.", "tokens": [460, 1770, 13], "temperature": 0.0, "avg_logprob": -0.17213702201843262, "compression_ratio": 1.7195121951219512, "no_speech_prob": 8.664566848892719e-06}, {"id": 1152, "seek": 575930, "start": 5774.14, "end": 5779.72, "text": " But the work was all in knowing that that's the thing that you can do that makes everything", "tokens": [583, 264, 589, 390, 439, 294, 5276, 300, 300, 311, 264, 551, 300, 291, 393, 360, 300, 1669, 1203], "temperature": 0.0, "avg_logprob": -0.17213702201843262, "compression_ratio": 1.7195121951219512, "no_speech_prob": 8.664566848892719e-06}, {"id": 1153, "seek": 575930, "start": 5779.72, "end": 5780.72, "text": " work better.", "tokens": [589, 1101, 13], "temperature": 0.0, "avg_logprob": -0.17213702201843262, "compression_ratio": 1.7195121951219512, "no_speech_prob": 8.664566848892719e-06}, {"id": 1154, "seek": 575930, "start": 5780.72, "end": 5785.58, "text": " So at the end of this, we've got a discriminator that can recognize it in real bedrooms in", "tokens": [407, 412, 264, 917, 295, 341, 11, 321, 600, 658, 257, 20828, 1639, 300, 393, 5521, 309, 294, 957, 39955, 294], "temperature": 0.0, "avg_logprob": -0.17213702201843262, "compression_ratio": 1.7195121951219512, "no_speech_prob": 8.664566848892719e-06}, {"id": 1155, "seek": 578558, "start": 5785.58, "end": 5790.78, "text": " our totally random crappy generated images.", "tokens": [527, 3879, 4974, 36531, 10833, 5267, 13], "temperature": 0.0, "avg_logprob": -0.17768622135770493, "compression_ratio": 1.69375, "no_speech_prob": 6.540410140587483e-06}, {"id": 1156, "seek": 578558, "start": 5790.78, "end": 5793.18, "text": " So let's now try and create some better images.", "tokens": [407, 718, 311, 586, 853, 293, 1884, 512, 1101, 5267, 13], "temperature": 0.0, "avg_logprob": -0.17768622135770493, "compression_ratio": 1.69375, "no_speech_prob": 6.540410140587483e-06}, {"id": 1157, "seek": 578558, "start": 5793.18, "end": 5799.74, "text": " So now set trainable discriminator to false, set trainable the generator to true, zero", "tokens": [407, 586, 992, 3847, 712, 20828, 1639, 281, 7908, 11, 992, 3847, 712, 264, 19265, 281, 2074, 11, 4018], "temperature": 0.0, "avg_logprob": -0.17768622135770493, "compression_ratio": 1.69375, "no_speech_prob": 6.540410140587483e-06}, {"id": 1158, "seek": 578558, "start": 5799.74, "end": 5812.0599999999995, "text": " route the gradients of the generator, and now our loss again is FW, that's the discriminator", "tokens": [7955, 264, 2771, 2448, 295, 264, 19265, 11, 293, 586, 527, 4470, 797, 307, 479, 54, 11, 300, 311, 264, 20828, 1639], "temperature": 0.0, "avg_logprob": -0.17768622135770493, "compression_ratio": 1.69375, "no_speech_prob": 6.540410140587483e-06}, {"id": 1159, "seek": 581206, "start": 5812.06, "end": 5819.780000000001, "text": " of the generator applied to some more random noise.", "tokens": [295, 264, 19265, 6456, 281, 512, 544, 4974, 5658, 13], "temperature": 0.0, "avg_logprob": -0.12227563988672543, "compression_ratio": 1.729559748427673, "no_speech_prob": 6.439024673454696e-06}, {"id": 1160, "seek": 581206, "start": 5819.780000000001, "end": 5829.3, "text": " So here's our random noise, here's our generator, and here's our discriminator.", "tokens": [407, 510, 311, 527, 4974, 5658, 11, 510, 311, 527, 19265, 11, 293, 510, 311, 527, 20828, 1639, 13], "temperature": 0.0, "avg_logprob": -0.12227563988672543, "compression_ratio": 1.729559748427673, "no_speech_prob": 6.439024673454696e-06}, {"id": 1161, "seek": 581206, "start": 5829.3, "end": 5836.3, "text": " I think I can remove that now because I think I've put it inside the discriminator, but", "tokens": [286, 519, 286, 393, 4159, 300, 586, 570, 286, 519, 286, 600, 829, 309, 1854, 264, 20828, 1639, 11, 457], "temperature": 0.0, "avg_logprob": -0.12227563988672543, "compression_ratio": 1.729559748427673, "no_speech_prob": 6.439024673454696e-06}, {"id": 1162, "seek": 581206, "start": 5836.3, "end": 5839.900000000001, "text": " I won't change it now because it's going to confuse me.", "tokens": [286, 1582, 380, 1319, 309, 586, 570, 309, 311, 516, 281, 28584, 385, 13], "temperature": 0.0, "avg_logprob": -0.12227563988672543, "compression_ratio": 1.729559748427673, "no_speech_prob": 6.439024673454696e-06}, {"id": 1163, "seek": 583990, "start": 5839.9, "end": 5846.0199999999995, "text": " So it's exactly the same as before where we did generator on the noise and then pass that", "tokens": [407, 309, 311, 2293, 264, 912, 382, 949, 689, 321, 630, 19265, 322, 264, 5658, 293, 550, 1320, 300], "temperature": 0.0, "avg_logprob": -0.15148958630032008, "compression_ratio": 1.8624338624338623, "no_speech_prob": 4.785074906976661e-06}, {"id": 1164, "seek": 583990, "start": 5846.0199999999995, "end": 5850.74, "text": " to discriminator, but this time the thing that's trainable is the generator, not the", "tokens": [281, 20828, 1639, 11, 457, 341, 565, 264, 551, 300, 311, 3847, 712, 307, 264, 19265, 11, 406, 264], "temperature": 0.0, "avg_logprob": -0.15148958630032008, "compression_ratio": 1.8624338624338623, "no_speech_prob": 4.785074906976661e-06}, {"id": 1165, "seek": 583990, "start": 5850.74, "end": 5851.98, "text": " discriminator.", "tokens": [20828, 1639, 13], "temperature": 0.0, "avg_logprob": -0.15148958630032008, "compression_ratio": 1.8624338624338623, "no_speech_prob": 4.785074906976661e-06}, {"id": 1166, "seek": 583990, "start": 5851.98, "end": 5859.66, "text": " So in other words, in this pseudocode, the thing they update is theta, which is the generator's", "tokens": [407, 294, 661, 2283, 11, 294, 341, 25505, 532, 905, 1429, 11, 264, 551, 436, 5623, 307, 9725, 11, 597, 307, 264, 19265, 311], "temperature": 0.0, "avg_logprob": -0.15148958630032008, "compression_ratio": 1.8624338624338623, "no_speech_prob": 4.785074906976661e-06}, {"id": 1167, "seek": 583990, "start": 5859.66, "end": 5865.9, "text": " parameters rather than W, which is the discriminator's parameters.", "tokens": [9834, 2831, 813, 343, 11, 597, 307, 264, 20828, 1639, 311, 9834, 13], "temperature": 0.0, "avg_logprob": -0.15148958630032008, "compression_ratio": 1.8624338624338623, "no_speech_prob": 4.785074906976661e-06}, {"id": 1168, "seek": 586590, "start": 5865.9, "end": 5872.78, "text": " And so hopefully you'll see now that this W down here is telling you these are the parameters", "tokens": [400, 370, 4696, 291, 603, 536, 586, 300, 341, 343, 760, 510, 307, 3585, 291, 613, 366, 264, 9834], "temperature": 0.0, "avg_logprob": -0.15630318461984827, "compression_ratio": 1.7962962962962963, "no_speech_prob": 2.090455836878391e-06}, {"id": 1169, "seek": 586590, "start": 5872.78, "end": 5875.0199999999995, "text": " of the discriminator.", "tokens": [295, 264, 20828, 1639, 13], "temperature": 0.0, "avg_logprob": -0.15630318461984827, "compression_ratio": 1.7962962962962963, "no_speech_prob": 2.090455836878391e-06}, {"id": 1170, "seek": 586590, "start": 5875.0199999999995, "end": 5883.82, "text": " This theta down here is telling you these are the parameters of the generator.", "tokens": [639, 9725, 760, 510, 307, 3585, 291, 613, 366, 264, 9834, 295, 264, 19265, 13], "temperature": 0.0, "avg_logprob": -0.15630318461984827, "compression_ratio": 1.7962962962962963, "no_speech_prob": 2.090455836878391e-06}, {"id": 1171, "seek": 586590, "start": 5883.82, "end": 5891.259999999999, "text": " Again, it's not a universal mathematical notation, it's a thing they're doing in this particular", "tokens": [3764, 11, 309, 311, 406, 257, 11455, 18894, 24657, 11, 309, 311, 257, 551, 436, 434, 884, 294, 341, 1729], "temperature": 0.0, "avg_logprob": -0.15630318461984827, "compression_ratio": 1.7962962962962963, "no_speech_prob": 2.090455836878391e-06}, {"id": 1172, "seek": 589126, "start": 5891.26, "end": 5898.46, "text": " paper, but it's kind of nice when you see some suffix like that, try to think about", "tokens": [3035, 11, 457, 309, 311, 733, 295, 1481, 562, 291, 536, 512, 3889, 970, 411, 300, 11, 853, 281, 519, 466], "temperature": 0.0, "avg_logprob": -0.1403828355454907, "compression_ratio": 1.7488151658767772, "no_speech_prob": 1.1659458323265426e-05}, {"id": 1173, "seek": 589126, "start": 5898.46, "end": 5901.820000000001, "text": " what it's telling you.", "tokens": [437, 309, 311, 3585, 291, 13], "temperature": 0.0, "avg_logprob": -0.1403828355454907, "compression_ratio": 1.7488151658767772, "no_speech_prob": 1.1659458323265426e-05}, {"id": 1174, "seek": 589126, "start": 5901.820000000001, "end": 5908.5, "text": " So we take some noise, generate some images, try and figure out if they're fake or real,", "tokens": [407, 321, 747, 512, 5658, 11, 8460, 512, 5267, 11, 853, 293, 2573, 484, 498, 436, 434, 7592, 420, 957, 11], "temperature": 0.0, "avg_logprob": -0.1403828355454907, "compression_ratio": 1.7488151658767772, "no_speech_prob": 1.1659458323265426e-05}, {"id": 1175, "seek": 589126, "start": 5908.5, "end": 5915.46, "text": " and use that to get gradients with respect to the generator as opposed to earlier, we", "tokens": [293, 764, 300, 281, 483, 2771, 2448, 365, 3104, 281, 264, 19265, 382, 8851, 281, 3071, 11, 321], "temperature": 0.0, "avg_logprob": -0.1403828355454907, "compression_ratio": 1.7488151658767772, "no_speech_prob": 1.1659458323265426e-05}, {"id": 1176, "seek": 589126, "start": 5915.46, "end": 5920.46, "text": " got them with respect to the discriminator, and use that to update our weights with rms", "tokens": [658, 552, 365, 3104, 281, 264, 20828, 1639, 11, 293, 764, 300, 281, 5623, 527, 17443, 365, 367, 2592], "temperature": 0.0, "avg_logprob": -0.1403828355454907, "compression_ratio": 1.7488151658767772, "no_speech_prob": 1.1659458323265426e-05}, {"id": 1177, "seek": 592046, "start": 5920.46, "end": 5928.06, "text": " prop with an alpha learning rate.", "tokens": [2365, 365, 364, 8961, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.13511030641320634, "compression_ratio": 1.5212765957446808, "no_speech_prob": 5.093685103929602e-06}, {"id": 1178, "seek": 592046, "start": 5928.06, "end": 5935.46, "text": " You'll see that it's kind of unfair that the discriminator is getting trained n critic", "tokens": [509, 603, 536, 300, 309, 311, 733, 295, 17019, 300, 264, 20828, 1639, 307, 1242, 8895, 297, 7850], "temperature": 0.0, "avg_logprob": -0.13511030641320634, "compression_ratio": 1.5212765957446808, "no_speech_prob": 5.093685103929602e-06}, {"id": 1179, "seek": 592046, "start": 5935.46, "end": 5944.74, "text": " times, which they set to 5, for every time that we train the generator once.", "tokens": [1413, 11, 597, 436, 992, 281, 1025, 11, 337, 633, 565, 300, 321, 3847, 264, 19265, 1564, 13], "temperature": 0.0, "avg_logprob": -0.13511030641320634, "compression_ratio": 1.5212765957446808, "no_speech_prob": 5.093685103929602e-06}, {"id": 1180, "seek": 592046, "start": 5944.74, "end": 5949.54, "text": " And the paper talks a bit about this, but the basic idea is like there's no point making", "tokens": [400, 264, 3035, 6686, 257, 857, 466, 341, 11, 457, 264, 3875, 1558, 307, 411, 456, 311, 572, 935, 1455], "temperature": 0.0, "avg_logprob": -0.13511030641320634, "compression_ratio": 1.5212765957446808, "no_speech_prob": 5.093685103929602e-06}, {"id": 1181, "seek": 594954, "start": 5949.54, "end": 5955.06, "text": " the generator better if the discriminator doesn't know how to discriminate yet.", "tokens": [264, 19265, 1101, 498, 264, 20828, 1639, 1177, 380, 458, 577, 281, 47833, 1939, 13], "temperature": 0.0, "avg_logprob": -0.14253941977896342, "compression_ratio": 1.5485436893203883, "no_speech_prob": 1.497091943747364e-05}, {"id": 1182, "seek": 594954, "start": 5955.06, "end": 5958.5, "text": " So that's why we've got this while loop.", "tokens": [407, 300, 311, 983, 321, 600, 658, 341, 1339, 6367, 13], "temperature": 0.0, "avg_logprob": -0.14253941977896342, "compression_ratio": 1.5485436893203883, "no_speech_prob": 1.497091943747364e-05}, {"id": 1183, "seek": 594954, "start": 5958.5, "end": 5960.9, "text": " And here's that 5.", "tokens": [400, 510, 311, 300, 1025, 13], "temperature": 0.0, "avg_logprob": -0.14253941977896342, "compression_ratio": 1.5485436893203883, "no_speech_prob": 1.497091943747364e-05}, {"id": 1184, "seek": 594954, "start": 5960.9, "end": 5967.7, "text": " And actually something which was added I think in the later paper, maybe a supplementary", "tokens": [400, 767, 746, 597, 390, 3869, 286, 519, 294, 264, 1780, 3035, 11, 1310, 257, 15436, 822], "temperature": 0.0, "avg_logprob": -0.14253941977896342, "compression_ratio": 1.5485436893203883, "no_speech_prob": 1.497091943747364e-05}, {"id": 1185, "seek": 594954, "start": 5967.7, "end": 5977.64, "text": " material, is the idea that from time to time and a bunch of times at the start, you should", "tokens": [2527, 11, 307, 264, 1558, 300, 490, 565, 281, 565, 293, 257, 3840, 295, 1413, 412, 264, 722, 11, 291, 820], "temperature": 0.0, "avg_logprob": -0.14253941977896342, "compression_ratio": 1.5485436893203883, "no_speech_prob": 1.497091943747364e-05}, {"id": 1186, "seek": 597764, "start": 5977.64, "end": 5980.1, "text": " do more steps of the discriminator.", "tokens": [360, 544, 4439, 295, 264, 20828, 1639, 13], "temperature": 0.0, "avg_logprob": -0.14718742960507109, "compression_ratio": 1.8894472361809045, "no_speech_prob": 8.139618330460507e-06}, {"id": 1187, "seek": 597764, "start": 5980.1, "end": 5986.46, "text": " So make sure that the discriminator is pretty capable from time to time.", "tokens": [407, 652, 988, 300, 264, 20828, 1639, 307, 1238, 8189, 490, 565, 281, 565, 13], "temperature": 0.0, "avg_logprob": -0.14718742960507109, "compression_ratio": 1.8894472361809045, "no_speech_prob": 8.139618330460507e-06}, {"id": 1188, "seek": 597764, "start": 5986.46, "end": 5991.9400000000005, "text": " So do a bunch of epochs of training the discriminator a bunch of times to get better at telling", "tokens": [407, 360, 257, 3840, 295, 30992, 28346, 295, 3097, 264, 20828, 1639, 257, 3840, 295, 1413, 281, 483, 1101, 412, 3585], "temperature": 0.0, "avg_logprob": -0.14718742960507109, "compression_ratio": 1.8894472361809045, "no_speech_prob": 8.139618330460507e-06}, {"id": 1189, "seek": 597764, "start": 5991.9400000000005, "end": 5997.08, "text": " the difference between real and fake, and then do one step of making the generator being", "tokens": [264, 2649, 1296, 957, 293, 7592, 11, 293, 550, 360, 472, 1823, 295, 1455, 264, 19265, 885], "temperature": 0.0, "avg_logprob": -0.14718742960507109, "compression_ratio": 1.8894472361809045, "no_speech_prob": 8.139618330460507e-06}, {"id": 1190, "seek": 597764, "start": 5997.08, "end": 6001.96, "text": " better at generating, and that is an epoch.", "tokens": [1101, 412, 17746, 11, 293, 300, 307, 364, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.14718742960507109, "compression_ratio": 1.8894472361809045, "no_speech_prob": 8.139618330460507e-06}, {"id": 1191, "seek": 597764, "start": 6001.96, "end": 6007.26, "text": " And so let's train that for one epoch.", "tokens": [400, 370, 718, 311, 3847, 300, 337, 472, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.14718742960507109, "compression_ratio": 1.8894472361809045, "no_speech_prob": 8.139618330460507e-06}, {"id": 1192, "seek": 600726, "start": 6007.26, "end": 6016.06, "text": " And then let's create some noise so we can generate some examples.", "tokens": [400, 550, 718, 311, 1884, 512, 5658, 370, 321, 393, 8460, 512, 5110, 13], "temperature": 0.0, "avg_logprob": -0.14176221211751303, "compression_ratio": 1.5375722543352601, "no_speech_prob": 4.86042745251325e-06}, {"id": 1193, "seek": 600726, "start": 6016.06, "end": 6017.06, "text": " Actually we're going to do that later.", "tokens": [5135, 321, 434, 516, 281, 360, 300, 1780, 13], "temperature": 0.0, "avg_logprob": -0.14176221211751303, "compression_ratio": 1.5375722543352601, "no_speech_prob": 4.86042745251325e-06}, {"id": 1194, "seek": 600726, "start": 6017.06, "end": 6020.900000000001, "text": " Let's first of all decrease the learning rate by 10 and do one more pass.", "tokens": [961, 311, 700, 295, 439, 11514, 264, 2539, 3314, 538, 1266, 293, 360, 472, 544, 1320, 13], "temperature": 0.0, "avg_logprob": -0.14176221211751303, "compression_ratio": 1.5375722543352601, "no_speech_prob": 4.86042745251325e-06}, {"id": 1195, "seek": 600726, "start": 6020.900000000001, "end": 6023.1, "text": " So we've now done two epochs.", "tokens": [407, 321, 600, 586, 1096, 732, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.14176221211751303, "compression_ratio": 1.5375722543352601, "no_speech_prob": 4.86042745251325e-06}, {"id": 1196, "seek": 600726, "start": 6023.1, "end": 6030.54, "text": " And now let's use our noise to pass it to our generator.", "tokens": [400, 586, 718, 311, 764, 527, 5658, 281, 1320, 309, 281, 527, 19265, 13], "temperature": 0.0, "avg_logprob": -0.14176221211751303, "compression_ratio": 1.5375722543352601, "no_speech_prob": 4.86042745251325e-06}, {"id": 1197, "seek": 603054, "start": 6030.54, "end": 6037.58, "text": " And then put it through our denormalization to turn it back into something we can see.", "tokens": [400, 550, 829, 309, 807, 527, 1441, 24440, 2144, 281, 1261, 309, 646, 666, 746, 321, 393, 536, 13], "temperature": 0.0, "avg_logprob": -0.20212013776912247, "compression_ratio": 1.608695652173913, "no_speech_prob": 5.1738998081418686e-06}, {"id": 1198, "seek": 603054, "start": 6037.58, "end": 6041.54, "text": " And then plot it.", "tokens": [400, 550, 7542, 309, 13], "temperature": 0.0, "avg_logprob": -0.20212013776912247, "compression_ratio": 1.608695652173913, "no_speech_prob": 5.1738998081418686e-06}, {"id": 1199, "seek": 603054, "start": 6041.54, "end": 6042.54, "text": " And we have some bedrooms.", "tokens": [400, 321, 362, 512, 39955, 13], "temperature": 0.0, "avg_logprob": -0.20212013776912247, "compression_ratio": 1.608695652173913, "no_speech_prob": 5.1738998081418686e-06}, {"id": 1200, "seek": 603054, "start": 6042.54, "end": 6046.58, "text": " Okay, there's not real bedrooms and some of them don't look particularly like bedrooms,", "tokens": [1033, 11, 456, 311, 406, 957, 39955, 293, 512, 295, 552, 500, 380, 574, 4098, 411, 39955, 11], "temperature": 0.0, "avg_logprob": -0.20212013776912247, "compression_ratio": 1.608695652173913, "no_speech_prob": 5.1738998081418686e-06}, {"id": 1201, "seek": 603054, "start": 6046.58, "end": 6049.56, "text": " but some of them look a lot like bedrooms.", "tokens": [457, 512, 295, 552, 574, 257, 688, 411, 39955, 13], "temperature": 0.0, "avg_logprob": -0.20212013776912247, "compression_ratio": 1.608695652173913, "no_speech_prob": 5.1738998081418686e-06}, {"id": 1202, "seek": 603054, "start": 6049.56, "end": 6052.14, "text": " So that's the idea.", "tokens": [407, 300, 311, 264, 1558, 13], "temperature": 0.0, "avg_logprob": -0.20212013776912247, "compression_ratio": 1.608695652173913, "no_speech_prob": 5.1738998081418686e-06}, {"id": 1203, "seek": 603054, "start": 6052.14, "end": 6053.64, "text": " That's a GAN.", "tokens": [663, 311, 257, 460, 1770, 13], "temperature": 0.0, "avg_logprob": -0.20212013776912247, "compression_ratio": 1.608695652173913, "no_speech_prob": 5.1738998081418686e-06}, {"id": 1204, "seek": 605364, "start": 6053.64, "end": 6060.860000000001, "text": " And I think the best way to think about a GAN is it's like an underlying technology", "tokens": [400, 286, 519, 264, 1151, 636, 281, 519, 466, 257, 460, 1770, 307, 309, 311, 411, 364, 14217, 2899], "temperature": 0.0, "avg_logprob": -0.12654735171605672, "compression_ratio": 1.4171779141104295, "no_speech_prob": 8.186352715711109e-07}, {"id": 1205, "seek": 605364, "start": 6060.860000000001, "end": 6068.860000000001, "text": " that you'll probably never use like this, but you'll use in lots of interesting ways.", "tokens": [300, 291, 603, 1391, 1128, 764, 411, 341, 11, 457, 291, 603, 764, 294, 3195, 295, 1880, 2098, 13], "temperature": 0.0, "avg_logprob": -0.12654735171605672, "compression_ratio": 1.4171779141104295, "no_speech_prob": 8.186352715711109e-07}, {"id": 1206, "seek": 605364, "start": 6068.860000000001, "end": 6080.34, "text": " For example, we're going to use it to create now a cycle GAN.", "tokens": [1171, 1365, 11, 321, 434, 516, 281, 764, 309, 281, 1884, 586, 257, 6586, 460, 1770, 13], "temperature": 0.0, "avg_logprob": -0.12654735171605672, "compression_ratio": 1.4171779141104295, "no_speech_prob": 8.186352715711109e-07}, {"id": 1207, "seek": 608034, "start": 6080.34, "end": 6088.06, "text": " And we're going to use a cycle GAN to turn horses into zebras.", "tokens": [400, 321, 434, 516, 281, 764, 257, 6586, 460, 1770, 281, 1261, 13112, 666, 5277, 38182, 13], "temperature": 0.0, "avg_logprob": -0.1821258544921875, "compression_ratio": 1.516431924882629, "no_speech_prob": 2.332063331778045e-06}, {"id": 1208, "seek": 608034, "start": 6088.06, "end": 6093.04, "text": " You could also use it to turn Monet prints into photos or to turn photos of Yosemite", "tokens": [509, 727, 611, 764, 309, 281, 1261, 47871, 22305, 666, 5787, 420, 281, 1261, 5787, 295, 398, 329, 443, 642], "temperature": 0.0, "avg_logprob": -0.1821258544921875, "compression_ratio": 1.516431924882629, "no_speech_prob": 2.332063331778045e-06}, {"id": 1209, "seek": 608034, "start": 6093.04, "end": 6096.22, "text": " in summer into winter.", "tokens": [294, 4266, 666, 6355, 13], "temperature": 0.0, "avg_logprob": -0.1821258544921875, "compression_ratio": 1.516431924882629, "no_speech_prob": 2.332063331778045e-06}, {"id": 1210, "seek": 608034, "start": 6096.22, "end": 6098.860000000001, "text": " So it's going to be pretty... yes, Rachel.", "tokens": [407, 309, 311, 516, 281, 312, 1238, 1097, 2086, 11, 14246, 13], "temperature": 0.0, "avg_logprob": -0.1821258544921875, "compression_ratio": 1.516431924882629, "no_speech_prob": 2.332063331778045e-06}, {"id": 1211, "seek": 608034, "start": 6098.860000000001, "end": 6099.860000000001, "text": " Two questions.", "tokens": [4453, 1651, 13], "temperature": 0.0, "avg_logprob": -0.1821258544921875, "compression_ratio": 1.516431924882629, "no_speech_prob": 2.332063331778045e-06}, {"id": 1212, "seek": 608034, "start": 6099.860000000001, "end": 6105.82, "text": " One, is there any reason for using RMS props specifically as the optimizer as opposed to", "tokens": [1485, 11, 307, 456, 604, 1778, 337, 1228, 497, 10288, 26173, 4682, 382, 264, 5028, 6545, 382, 8851, 281], "temperature": 0.0, "avg_logprob": -0.1821258544921875, "compression_ratio": 1.516431924882629, "no_speech_prob": 2.332063331778045e-06}, {"id": 1213, "seek": 608034, "start": 6105.82, "end": 6106.82, "text": " Atom?", "tokens": [1711, 298, 30], "temperature": 0.0, "avg_logprob": -0.1821258544921875, "compression_ratio": 1.516431924882629, "no_speech_prob": 2.332063331778045e-06}, {"id": 1214, "seek": 610682, "start": 6106.82, "end": 6111.5, "text": " I don't remember it being explicitly discussed in the paper.", "tokens": [286, 500, 380, 1604, 309, 885, 20803, 7152, 294, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.16536791508014387, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.4063836715649813e-05}, {"id": 1215, "seek": 610682, "start": 6111.5, "end": 6114.259999999999, "text": " I don't know if it's just experimental or the theoretical reason.", "tokens": [286, 500, 380, 458, 498, 309, 311, 445, 17069, 420, 264, 20864, 1778, 13], "temperature": 0.0, "avg_logprob": -0.16536791508014387, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.4063836715649813e-05}, {"id": 1216, "seek": 610682, "start": 6114.259999999999, "end": 6117.98, "text": " Have a look in the paper and see what it says.", "tokens": [3560, 257, 574, 294, 264, 3035, 293, 536, 437, 309, 1619, 13], "temperature": 0.0, "avg_logprob": -0.16536791508014387, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.4063836715649813e-05}, {"id": 1217, "seek": 610682, "start": 6117.98, "end": 6122.58, "text": " And which could be a reasonable way of detecting overfitting while training or evaluating the", "tokens": [400, 597, 727, 312, 257, 10585, 636, 295, 40237, 670, 69, 2414, 1339, 3097, 420, 27479, 264], "temperature": 0.0, "avg_logprob": -0.16536791508014387, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.4063836715649813e-05}, {"id": 1218, "seek": 610682, "start": 6122.58, "end": 6126.099999999999, "text": " performance of one of these GAN models once we are done training?", "tokens": [3389, 295, 472, 295, 613, 460, 1770, 5245, 1564, 321, 366, 1096, 3097, 30], "temperature": 0.0, "avg_logprob": -0.16536791508014387, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.4063836715649813e-05}, {"id": 1219, "seek": 610682, "start": 6126.099999999999, "end": 6135.38, "text": " In other words, how does the notion of train, validation, test sets translate to GANs?", "tokens": [682, 661, 2283, 11, 577, 775, 264, 10710, 295, 3847, 11, 24071, 11, 1500, 6352, 13799, 281, 460, 1770, 82, 30], "temperature": 0.0, "avg_logprob": -0.16536791508014387, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.4063836715649813e-05}, {"id": 1220, "seek": 613538, "start": 6135.38, "end": 6138.82, "text": " That's an awesome question.", "tokens": [663, 311, 364, 3476, 1168, 13], "temperature": 0.0, "avg_logprob": -0.11113064311375127, "compression_ratio": 1.7208333333333334, "no_speech_prob": 7.296327567019034e-06}, {"id": 1221, "seek": 613538, "start": 6138.82, "end": 6144.46, "text": " And there's a lot of people who make jokes about how GANs is the one field where you", "tokens": [400, 456, 311, 257, 688, 295, 561, 567, 652, 14439, 466, 577, 460, 1770, 82, 307, 264, 472, 2519, 689, 291], "temperature": 0.0, "avg_logprob": -0.11113064311375127, "compression_ratio": 1.7208333333333334, "no_speech_prob": 7.296327567019034e-06}, {"id": 1222, "seek": 613538, "start": 6144.46, "end": 6150.66, "text": " don't need a test set and people take advantage of that by making stuff up and saying it looks", "tokens": [500, 380, 643, 257, 1500, 992, 293, 561, 747, 5002, 295, 300, 538, 1455, 1507, 493, 293, 1566, 309, 1542], "temperature": 0.0, "avg_logprob": -0.11113064311375127, "compression_ratio": 1.7208333333333334, "no_speech_prob": 7.296327567019034e-06}, {"id": 1223, "seek": 613538, "start": 6150.66, "end": 6153.14, "text": " great.", "tokens": [869, 13], "temperature": 0.0, "avg_logprob": -0.11113064311375127, "compression_ratio": 1.7208333333333334, "no_speech_prob": 7.296327567019034e-06}, {"id": 1224, "seek": 613538, "start": 6153.14, "end": 6156.5, "text": " There are some pretty famous problems with GANs.", "tokens": [821, 366, 512, 1238, 4618, 2740, 365, 460, 1770, 82, 13], "temperature": 0.0, "avg_logprob": -0.11113064311375127, "compression_ratio": 1.7208333333333334, "no_speech_prob": 7.296327567019034e-06}, {"id": 1225, "seek": 613538, "start": 6156.5, "end": 6160.1, "text": " One of the famous problems with GANs is called mode collapse.", "tokens": [1485, 295, 264, 4618, 2740, 365, 460, 1770, 82, 307, 1219, 4391, 15584, 13], "temperature": 0.0, "avg_logprob": -0.11113064311375127, "compression_ratio": 1.7208333333333334, "no_speech_prob": 7.296327567019034e-06}, {"id": 1226, "seek": 613538, "start": 6160.1, "end": 6164.96, "text": " And mode collapse happens where you look at your bedrooms and it turns out that there's", "tokens": [400, 4391, 15584, 2314, 689, 291, 574, 412, 428, 39955, 293, 309, 4523, 484, 300, 456, 311], "temperature": 0.0, "avg_logprob": -0.11113064311375127, "compression_ratio": 1.7208333333333334, "no_speech_prob": 7.296327567019034e-06}, {"id": 1227, "seek": 616496, "start": 6164.96, "end": 6170.74, "text": " basically only 3 kinds of bedrooms that every possible noise vector mapped to.", "tokens": [1936, 787, 805, 3685, 295, 39955, 300, 633, 1944, 5658, 8062, 33318, 281, 13], "temperature": 0.0, "avg_logprob": -0.2436236985232852, "compression_ratio": 1.7254098360655739, "no_speech_prob": 6.1441282923624385e-06}, {"id": 1228, "seek": 616496, "start": 6170.74, "end": 6174.26, "text": " You look at your gallery and it turns out they're all just the same thing.", "tokens": [509, 574, 412, 428, 18378, 293, 309, 4523, 484, 436, 434, 439, 445, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.2436236985232852, "compression_ratio": 1.7254098360655739, "no_speech_prob": 6.1441282923624385e-06}, {"id": 1229, "seek": 616496, "start": 6174.26, "end": 6177.06, "text": " There's just 3 different things.", "tokens": [821, 311, 445, 805, 819, 721, 13], "temperature": 0.0, "avg_logprob": -0.2436236985232852, "compression_ratio": 1.7254098360655739, "no_speech_prob": 6.1441282923624385e-06}, {"id": 1230, "seek": 616496, "start": 6177.06, "end": 6182.58, "text": " Mode collapse is easy to see if you collapse down to a small number of modes, like 3 or", "tokens": [20500, 15584, 307, 1858, 281, 536, 498, 291, 15584, 760, 281, 257, 1359, 1230, 295, 14068, 11, 411, 805, 420], "temperature": 0.0, "avg_logprob": -0.2436236985232852, "compression_ratio": 1.7254098360655739, "no_speech_prob": 6.1441282923624385e-06}, {"id": 1231, "seek": 616496, "start": 6182.58, "end": 6183.58, "text": " 4.", "tokens": [1017, 13], "temperature": 0.0, "avg_logprob": -0.2436236985232852, "compression_ratio": 1.7254098360655739, "no_speech_prob": 6.1441282923624385e-06}, {"id": 1232, "seek": 616496, "start": 6183.58, "end": 6186.46, "text": " But what if you have a mode collapse down to 10,000 modes?", "tokens": [583, 437, 498, 291, 362, 257, 4391, 15584, 760, 281, 1266, 11, 1360, 14068, 30], "temperature": 0.0, "avg_logprob": -0.2436236985232852, "compression_ratio": 1.7254098360655739, "no_speech_prob": 6.1441282923624385e-06}, {"id": 1233, "seek": 616496, "start": 6186.46, "end": 6191.9800000000005, "text": " So there's only 10,000 possible bedrooms that all of your noise vectors collapse to.", "tokens": [407, 456, 311, 787, 1266, 11, 1360, 1944, 39955, 300, 439, 295, 428, 5658, 18875, 15584, 281, 13], "temperature": 0.0, "avg_logprob": -0.2436236985232852, "compression_ratio": 1.7254098360655739, "no_speech_prob": 6.1441282923624385e-06}, {"id": 1234, "seek": 619198, "start": 6191.98, "end": 6195.28, "text": " That's not, you wouldn't be able to see it here, because it's pretty unlikely you would", "tokens": [663, 311, 406, 11, 291, 2759, 380, 312, 1075, 281, 536, 309, 510, 11, 570, 309, 311, 1238, 17518, 291, 576], "temperature": 0.0, "avg_logprob": -0.21838385264078777, "compression_ratio": 1.5087719298245614, "no_speech_prob": 2.5215674668288557e-06}, {"id": 1235, "seek": 619198, "start": 6195.28, "end": 6198.54, "text": " have 2 identical bedrooms out of 10,000.", "tokens": [362, 568, 14800, 39955, 484, 295, 1266, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.21838385264078777, "compression_ratio": 1.5087719298245614, "no_speech_prob": 2.5215674668288557e-06}, {"id": 1236, "seek": 619198, "start": 6198.54, "end": 6206.459999999999, "text": " Or what if every one of these bedrooms is basically a direct copy, basically it memorized", "tokens": [1610, 437, 498, 633, 472, 295, 613, 39955, 307, 1936, 257, 2047, 5055, 11, 1936, 309, 46677], "temperature": 0.0, "avg_logprob": -0.21838385264078777, "compression_ratio": 1.5087719298245614, "no_speech_prob": 2.5215674668288557e-06}, {"id": 1237, "seek": 619198, "start": 6206.459999999999, "end": 6209.94, "text": " some input?", "tokens": [512, 4846, 30], "temperature": 0.0, "avg_logprob": -0.21838385264078777, "compression_ratio": 1.5087719298245614, "no_speech_prob": 2.5215674668288557e-06}, {"id": 1238, "seek": 619198, "start": 6209.94, "end": 6211.74, "text": " Could that be happening?", "tokens": [7497, 300, 312, 2737, 30], "temperature": 0.0, "avg_logprob": -0.21838385264078777, "compression_ratio": 1.5087719298245614, "no_speech_prob": 2.5215674668288557e-06}, {"id": 1239, "seek": 619198, "start": 6211.74, "end": 6219.54, "text": " And the truth is, most papers don't do a good job or sometimes any job of checking those", "tokens": [400, 264, 3494, 307, 11, 881, 10577, 500, 380, 360, 257, 665, 1691, 420, 2171, 604, 1691, 295, 8568, 729], "temperature": 0.0, "avg_logprob": -0.21838385264078777, "compression_ratio": 1.5087719298245614, "no_speech_prob": 2.5215674668288557e-06}, {"id": 1240, "seek": 621954, "start": 6219.54, "end": 6222.46, "text": " things.", "tokens": [721, 13], "temperature": 0.0, "avg_logprob": -0.16071655790684586, "compression_ratio": 1.4610389610389611, "no_speech_prob": 8.013467777345795e-06}, {"id": 1241, "seek": 621954, "start": 6222.46, "end": 6230.14, "text": " So the question of how do we evaluate GANs, and even the point of maybe we should actually", "tokens": [407, 264, 1168, 295, 577, 360, 321, 13059, 460, 1770, 82, 11, 293, 754, 264, 935, 295, 1310, 321, 820, 767], "temperature": 0.0, "avg_logprob": -0.16071655790684586, "compression_ratio": 1.4610389610389611, "no_speech_prob": 8.013467777345795e-06}, {"id": 1242, "seek": 621954, "start": 6230.14, "end": 6239.82, "text": " evaluate GANs properly, is something that is not widely enough understood even now.", "tokens": [13059, 460, 1770, 82, 6108, 11, 307, 746, 300, 307, 406, 13371, 1547, 7320, 754, 586, 13], "temperature": 0.0, "avg_logprob": -0.16071655790684586, "compression_ratio": 1.4610389610389611, "no_speech_prob": 8.013467777345795e-06}, {"id": 1243, "seek": 621954, "start": 6239.82, "end": 6242.5, "text": " And some people are trying to really push.", "tokens": [400, 512, 561, 366, 1382, 281, 534, 2944, 13], "temperature": 0.0, "avg_logprob": -0.16071655790684586, "compression_ratio": 1.4610389610389611, "no_speech_prob": 8.013467777345795e-06}, {"id": 1244, "seek": 624250, "start": 6242.5, "end": 6249.58, "text": " So Ian Goodfellow, who a lot of you will know because he came and spoke here at a lot of", "tokens": [407, 19595, 2205, 69, 21348, 11, 567, 257, 688, 295, 291, 486, 458, 570, 415, 1361, 293, 7179, 510, 412, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.12480554909541688, "compression_ratio": 1.572093023255814, "no_speech_prob": 5.955082542641321e-06}, {"id": 1245, "seek": 624250, "start": 6249.58, "end": 6255.62, "text": " the book club meetings last year, and of course was the first author on the most famous deep", "tokens": [264, 1446, 6482, 8410, 1036, 1064, 11, 293, 295, 1164, 390, 264, 700, 3793, 322, 264, 881, 4618, 2452], "temperature": 0.0, "avg_logprob": -0.12480554909541688, "compression_ratio": 1.572093023255814, "no_speech_prob": 5.955082542641321e-06}, {"id": 1246, "seek": 624250, "start": 6255.62, "end": 6256.62, "text": " learning book.", "tokens": [2539, 1446, 13], "temperature": 0.0, "avg_logprob": -0.12480554909541688, "compression_ratio": 1.572093023255814, "no_speech_prob": 5.955082542641321e-06}, {"id": 1247, "seek": 624250, "start": 6256.62, "end": 6264.94, "text": " He's the inventor of GANs and he's been sending a continuous stream of tweets reminding people", "tokens": [634, 311, 264, 41593, 295, 460, 1770, 82, 293, 415, 311, 668, 7750, 257, 10957, 4309, 295, 25671, 27639, 561], "temperature": 0.0, "avg_logprob": -0.12480554909541688, "compression_ratio": 1.572093023255814, "no_speech_prob": 5.955082542641321e-06}, {"id": 1248, "seek": 624250, "start": 6264.94, "end": 6270.54, "text": " about the importance of testing GANs properly.", "tokens": [466, 264, 7379, 295, 4997, 460, 1770, 82, 6108, 13], "temperature": 0.0, "avg_logprob": -0.12480554909541688, "compression_ratio": 1.572093023255814, "no_speech_prob": 5.955082542641321e-06}, {"id": 1249, "seek": 627054, "start": 6270.54, "end": 6277.18, "text": " So if you see a paper that claims exceptional GAN results, then this is definitely something", "tokens": [407, 498, 291, 536, 257, 3035, 300, 9441, 19279, 460, 1770, 3542, 11, 550, 341, 307, 2138, 746], "temperature": 0.0, "avg_logprob": -0.15908234694908405, "compression_ratio": 1.6009174311926606, "no_speech_prob": 4.637847723643063e-06}, {"id": 1250, "seek": 627054, "start": 6277.18, "end": 6278.82, "text": " to look at.", "tokens": [281, 574, 412, 13], "temperature": 0.0, "avg_logprob": -0.15908234694908405, "compression_ratio": 1.6009174311926606, "no_speech_prob": 4.637847723643063e-06}, {"id": 1251, "seek": 627054, "start": 6278.82, "end": 6280.42, "text": " Have they talked about mode collapse?", "tokens": [3560, 436, 2825, 466, 4391, 15584, 30], "temperature": 0.0, "avg_logprob": -0.15908234694908405, "compression_ratio": 1.6009174311926606, "no_speech_prob": 4.637847723643063e-06}, {"id": 1252, "seek": 627054, "start": 6280.42, "end": 6287.14, "text": " Have they talked about memorization?", "tokens": [3560, 436, 2825, 466, 10560, 2144, 30], "temperature": 0.0, "avg_logprob": -0.15908234694908405, "compression_ratio": 1.6009174311926606, "no_speech_prob": 4.637847723643063e-06}, {"id": 1253, "seek": 627054, "start": 6287.14, "end": 6291.3, "text": " So this is going to be really straightforward because it's just a neural net.", "tokens": [407, 341, 307, 516, 281, 312, 534, 15325, 570, 309, 311, 445, 257, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.15908234694908405, "compression_ratio": 1.6009174311926606, "no_speech_prob": 4.637847723643063e-06}, {"id": 1254, "seek": 627054, "start": 6291.3, "end": 6298.82, "text": " So all we're going to do is we're going to create an input containing lots of zebra photos,", "tokens": [407, 439, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 1884, 364, 4846, 19273, 3195, 295, 47060, 5787, 11], "temperature": 0.0, "avg_logprob": -0.15908234694908405, "compression_ratio": 1.6009174311926606, "no_speech_prob": 4.637847723643063e-06}, {"id": 1255, "seek": 629882, "start": 6298.82, "end": 6306.74, "text": " and with each one we'll pair it with an equivalent horse photo, and we'll just train a neural", "tokens": [293, 365, 1184, 472, 321, 603, 6119, 309, 365, 364, 10344, 6832, 5052, 11, 293, 321, 603, 445, 3847, 257, 18161], "temperature": 0.0, "avg_logprob": -0.1672901538533902, "compression_ratio": 1.6904761904761905, "no_speech_prob": 8.800954674370587e-06}, {"id": 1256, "seek": 629882, "start": 6306.74, "end": 6308.38, "text": " net that goes from one to the other.", "tokens": [2533, 300, 1709, 490, 472, 281, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.1672901538533902, "compression_ratio": 1.6904761904761905, "no_speech_prob": 8.800954674370587e-06}, {"id": 1257, "seek": 629882, "start": 6308.38, "end": 6313.0599999999995, "text": " Or you could do the same thing for every Monet painting, create a dataset containing the", "tokens": [1610, 291, 727, 360, 264, 912, 551, 337, 633, 47871, 5370, 11, 1884, 257, 28872, 19273, 264], "temperature": 0.0, "avg_logprob": -0.1672901538533902, "compression_ratio": 1.6904761904761905, "no_speech_prob": 8.800954674370587e-06}, {"id": 1258, "seek": 629882, "start": 6313.0599999999995, "end": 6314.86, "text": " photo of the place.", "tokens": [5052, 295, 264, 1081, 13], "temperature": 0.0, "avg_logprob": -0.1672901538533902, "compression_ratio": 1.6904761904761905, "no_speech_prob": 8.800954674370587e-06}, {"id": 1259, "seek": 629882, "start": 6314.86, "end": 6320.9, "text": " Oh wait, that's not possible because the places that Monet painted aren't there anymore, and", "tokens": [876, 1699, 11, 300, 311, 406, 1944, 570, 264, 3190, 300, 47871, 11797, 3212, 380, 456, 3602, 11, 293], "temperature": 0.0, "avg_logprob": -0.1672901538533902, "compression_ratio": 1.6904761904761905, "no_speech_prob": 8.800954674370587e-06}, {"id": 1260, "seek": 629882, "start": 6320.9, "end": 6323.74, "text": " there aren't exact zebra versions of horses.", "tokens": [456, 3212, 380, 1900, 47060, 9606, 295, 13112, 13], "temperature": 0.0, "avg_logprob": -0.1672901538533902, "compression_ratio": 1.6904761904761905, "no_speech_prob": 8.800954674370587e-06}, {"id": 1261, "seek": 629882, "start": 6323.74, "end": 6327.94, "text": " And oh wait, how the hell is this going to work?", "tokens": [400, 1954, 1699, 11, 577, 264, 4921, 307, 341, 516, 281, 589, 30], "temperature": 0.0, "avg_logprob": -0.1672901538533902, "compression_ratio": 1.6904761904761905, "no_speech_prob": 8.800954674370587e-06}, {"id": 1262, "seek": 632794, "start": 6327.94, "end": 6332.86, "text": " This seems to break everything we know about what neural nets can do and how they do them.", "tokens": [639, 2544, 281, 1821, 1203, 321, 458, 466, 437, 18161, 36170, 393, 360, 293, 577, 436, 360, 552, 13], "temperature": 0.0, "avg_logprob": -0.21645331182399719, "compression_ratio": 1.543726235741445, "no_speech_prob": 9.368618520966265e-06}, {"id": 1263, "seek": 632794, "start": 6332.86, "end": 6336.94, "text": " Alright, Rachel, you're going to ask me a question to spoil our whole train of thought.", "tokens": [2798, 11, 14246, 11, 291, 434, 516, 281, 1029, 385, 257, 1168, 281, 18630, 527, 1379, 3847, 295, 1194, 13], "temperature": 0.0, "avg_logprob": -0.21645331182399719, "compression_ratio": 1.543726235741445, "no_speech_prob": 9.368618520966265e-06}, {"id": 1264, "seek": 632794, "start": 6336.94, "end": 6338.54, "text": " Come on, better be good.", "tokens": [2492, 322, 11, 1101, 312, 665, 13], "temperature": 0.0, "avg_logprob": -0.21645331182399719, "compression_ratio": 1.543726235741445, "no_speech_prob": 9.368618520966265e-06}, {"id": 1265, "seek": 632794, "start": 6338.54, "end": 6341.139999999999, "text": " Can GANs be used for data augmentation?", "tokens": [1664, 460, 1770, 82, 312, 1143, 337, 1412, 14501, 19631, 30], "temperature": 0.0, "avg_logprob": -0.21645331182399719, "compression_ratio": 1.543726235741445, "no_speech_prob": 9.368618520966265e-06}, {"id": 1266, "seek": 632794, "start": 6341.139999999999, "end": 6343.179999999999, "text": " Yeah, absolutely.", "tokens": [865, 11, 3122, 13], "temperature": 0.0, "avg_logprob": -0.21645331182399719, "compression_ratio": 1.543726235741445, "no_speech_prob": 9.368618520966265e-06}, {"id": 1267, "seek": 632794, "start": 6343.179999999999, "end": 6345.58, "text": " You can use a GAN for data augmentation.", "tokens": [509, 393, 764, 257, 460, 1770, 337, 1412, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.21645331182399719, "compression_ratio": 1.543726235741445, "no_speech_prob": 9.368618520966265e-06}, {"id": 1268, "seek": 632794, "start": 6345.58, "end": 6346.58, "text": " Should you?", "tokens": [6454, 291, 30], "temperature": 0.0, "avg_logprob": -0.21645331182399719, "compression_ratio": 1.543726235741445, "no_speech_prob": 9.368618520966265e-06}, {"id": 1269, "seek": 632794, "start": 6346.58, "end": 6349.219999999999, "text": " I don't know.", "tokens": [286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.21645331182399719, "compression_ratio": 1.543726235741445, "no_speech_prob": 9.368618520966265e-06}, {"id": 1270, "seek": 632794, "start": 6349.219999999999, "end": 6353.66, "text": " Like there are some papers that try to do semi-supervised learning with GANs.", "tokens": [1743, 456, 366, 512, 10577, 300, 853, 281, 360, 12909, 12, 48172, 24420, 2539, 365, 460, 1770, 82, 13], "temperature": 0.0, "avg_logprob": -0.21645331182399719, "compression_ratio": 1.543726235741445, "no_speech_prob": 9.368618520966265e-06}, {"id": 1271, "seek": 635366, "start": 6353.66, "end": 6360.4, "text": " I haven't found any that are particularly compelling, showing state-of-the-art results on really", "tokens": [286, 2378, 380, 1352, 604, 300, 366, 4098, 20050, 11, 4099, 1785, 12, 2670, 12, 3322, 12, 446, 3542, 322, 534], "temperature": 0.0, "avg_logprob": -0.10036052892237533, "compression_ratio": 1.5810810810810811, "no_speech_prob": 3.041583113372326e-06}, {"id": 1272, "seek": 635366, "start": 6360.4, "end": 6364.98, "text": " interesting datasets that have been widely studied.", "tokens": [1880, 42856, 300, 362, 668, 13371, 9454, 13], "temperature": 0.0, "avg_logprob": -0.10036052892237533, "compression_ratio": 1.5810810810810811, "no_speech_prob": 3.041583113372326e-06}, {"id": 1273, "seek": 635366, "start": 6364.98, "end": 6368.139999999999, "text": " I'm a little skeptical.", "tokens": [286, 478, 257, 707, 28601, 13], "temperature": 0.0, "avg_logprob": -0.10036052892237533, "compression_ratio": 1.5810810810810811, "no_speech_prob": 3.041583113372326e-06}, {"id": 1274, "seek": 635366, "start": 6368.139999999999, "end": 6372.7, "text": " The reason I'm a little skeptical is because in my experience, if you train a model with", "tokens": [440, 1778, 286, 478, 257, 707, 28601, 307, 570, 294, 452, 1752, 11, 498, 291, 3847, 257, 2316, 365], "temperature": 0.0, "avg_logprob": -0.10036052892237533, "compression_ratio": 1.5810810810810811, "no_speech_prob": 3.041583113372326e-06}, {"id": 1275, "seek": 635366, "start": 6372.7, "end": 6380.0599999999995, "text": " synthetic data, the neural net will become fantastically good at recognizing the specific", "tokens": [23420, 1412, 11, 264, 18161, 2533, 486, 1813, 4115, 22808, 665, 412, 18538, 264, 2685], "temperature": 0.0, "avg_logprob": -0.10036052892237533, "compression_ratio": 1.5810810810810811, "no_speech_prob": 3.041583113372326e-06}, {"id": 1276, "seek": 638006, "start": 6380.06, "end": 6386.860000000001, "text": " problems of your synthetic data and that will end up what it's learning from.", "tokens": [2740, 295, 428, 23420, 1412, 293, 300, 486, 917, 493, 437, 309, 311, 2539, 490, 13], "temperature": 0.0, "avg_logprob": -0.21631316705183548, "compression_ratio": 1.4979253112033195, "no_speech_prob": 1.8631522834766656e-05}, {"id": 1277, "seek": 638006, "start": 6386.860000000001, "end": 6392.38, "text": " And there are lots of other ways of doing semi-supervised models which do work well.", "tokens": [400, 456, 366, 3195, 295, 661, 2098, 295, 884, 12909, 12, 48172, 24420, 5245, 597, 360, 589, 731, 13], "temperature": 0.0, "avg_logprob": -0.21631316705183548, "compression_ratio": 1.4979253112033195, "no_speech_prob": 1.8631522834766656e-05}, {"id": 1278, "seek": 638006, "start": 6392.38, "end": 6394.3, "text": " There are some places it can work.", "tokens": [821, 366, 512, 3190, 309, 393, 589, 13], "temperature": 0.0, "avg_logprob": -0.21631316705183548, "compression_ratio": 1.4979253112033195, "no_speech_prob": 1.8631522834766656e-05}, {"id": 1279, "seek": 638006, "start": 6394.3, "end": 6399.900000000001, "text": " For example, you might remember Ottavio Good created that fantastic visualization in part", "tokens": [1171, 1365, 11, 291, 1062, 1604, 24243, 706, 1004, 2205, 2942, 300, 5456, 25801, 294, 644], "temperature": 0.0, "avg_logprob": -0.21631316705183548, "compression_ratio": 1.4979253112033195, "no_speech_prob": 1.8631522834766656e-05}, {"id": 1280, "seek": 638006, "start": 6399.900000000001, "end": 6405.740000000001, "text": " one of the zooming conv net where he showed a letter going through MNIST.", "tokens": [472, 295, 264, 48226, 3754, 2533, 689, 415, 4712, 257, 5063, 516, 807, 376, 45, 19756, 13], "temperature": 0.0, "avg_logprob": -0.21631316705183548, "compression_ratio": 1.4979253112033195, "no_speech_prob": 1.8631522834766656e-05}, {"id": 1281, "seek": 640574, "start": 6405.74, "end": 6416.26, "text": " He at least at that time was the number one autonomous remote-controlled car guy in autonomous", "tokens": [634, 412, 1935, 412, 300, 565, 390, 264, 1230, 472, 23797, 8607, 12, 49344, 1032, 2146, 294, 23797], "temperature": 0.0, "avg_logprob": -0.149944093492296, "compression_ratio": 1.6649484536082475, "no_speech_prob": 7.2962579906743485e-06}, {"id": 1282, "seek": 640574, "start": 6416.26, "end": 6418.46, "text": " remote-controlled car competitions.", "tokens": [8607, 12, 49344, 1032, 26185, 13], "temperature": 0.0, "avg_logprob": -0.149944093492296, "compression_ratio": 1.6649484536082475, "no_speech_prob": 7.2962579906743485e-06}, {"id": 1283, "seek": 640574, "start": 6418.46, "end": 6426.0599999999995, "text": " And he trained his model using synthetically augmented data where he basically took real", "tokens": [400, 415, 8895, 702, 2316, 1228, 10657, 22652, 36155, 1412, 689, 415, 1936, 1890, 957], "temperature": 0.0, "avg_logprob": -0.149944093492296, "compression_ratio": 1.6649484536082475, "no_speech_prob": 7.2962579906743485e-06}, {"id": 1284, "seek": 640574, "start": 6426.0599999999995, "end": 6433.9, "text": " videos of a car driving around a circuit and added fake people and fake other cars and", "tokens": [2145, 295, 257, 1032, 4840, 926, 257, 9048, 293, 3869, 7592, 561, 293, 7592, 661, 5163, 293], "temperature": 0.0, "avg_logprob": -0.149944093492296, "compression_ratio": 1.6649484536082475, "no_speech_prob": 7.2962579906743485e-06}, {"id": 1285, "seek": 640574, "start": 6433.9, "end": 6435.08, "text": " stuff like that.", "tokens": [1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.149944093492296, "compression_ratio": 1.6649484536082475, "no_speech_prob": 7.2962579906743485e-06}, {"id": 1286, "seek": 643508, "start": 6435.08, "end": 6445.42, "text": " I think that worked well because he's kind of a genius and because I think he had a well-defined", "tokens": [286, 519, 300, 2732, 731, 570, 415, 311, 733, 295, 257, 14017, 293, 570, 286, 519, 415, 632, 257, 731, 12, 37716], "temperature": 0.0, "avg_logprob": -0.24941906929016114, "compression_ratio": 1.6195652173913044, "no_speech_prob": 6.747986844857223e-06}, {"id": 1287, "seek": 643508, "start": 6445.42, "end": 6453.34, "text": " little subset that he had to work in.", "tokens": [707, 25993, 300, 415, 632, 281, 589, 294, 13], "temperature": 0.0, "avg_logprob": -0.24941906929016114, "compression_ratio": 1.6195652173913044, "no_speech_prob": 6.747986844857223e-06}, {"id": 1288, "seek": 643508, "start": 6453.34, "end": 6454.74, "text": " In general, it's really hard.", "tokens": [682, 2674, 11, 309, 311, 534, 1152, 13], "temperature": 0.0, "avg_logprob": -0.24941906929016114, "compression_ratio": 1.6195652173913044, "no_speech_prob": 6.747986844857223e-06}, {"id": 1289, "seek": 643508, "start": 6454.74, "end": 6456.66, "text": " It's really, really hard to use synthetic data.", "tokens": [467, 311, 534, 11, 534, 1152, 281, 764, 23420, 1412, 13], "temperature": 0.0, "avg_logprob": -0.24941906929016114, "compression_ratio": 1.6195652173913044, "no_speech_prob": 6.747986844857223e-06}, {"id": 1290, "seek": 643508, "start": 6456.66, "end": 6462.26, "text": " I've tried using synthetic data in models for decades now, obviously not GANs because", "tokens": [286, 600, 3031, 1228, 23420, 1412, 294, 5245, 337, 7878, 586, 11, 2745, 406, 460, 1770, 82, 570], "temperature": 0.0, "avg_logprob": -0.24941906929016114, "compression_ratio": 1.6195652173913044, "no_speech_prob": 6.747986844857223e-06}, {"id": 1291, "seek": 646226, "start": 6462.26, "end": 6467.02, "text": " they're pretty new, but in general it's very hard to do.", "tokens": [436, 434, 1238, 777, 11, 457, 294, 2674, 309, 311, 588, 1152, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.12789296346997459, "compression_ratio": 1.4585635359116023, "no_speech_prob": 2.52155427915568e-06}, {"id": 1292, "seek": 646226, "start": 6467.02, "end": 6473.42, "text": " Very interesting research question.", "tokens": [4372, 1880, 2132, 1168, 13], "temperature": 0.0, "avg_logprob": -0.12789296346997459, "compression_ratio": 1.4585635359116023, "no_speech_prob": 2.52155427915568e-06}, {"id": 1293, "seek": 646226, "start": 6473.42, "end": 6483.74, "text": " So somehow these folks at Berkeley created a model that can turn a horse into a zebra", "tokens": [407, 6063, 613, 4024, 412, 23684, 2942, 257, 2316, 300, 393, 1261, 257, 6832, 666, 257, 47060], "temperature": 0.0, "avg_logprob": -0.12789296346997459, "compression_ratio": 1.4585635359116023, "no_speech_prob": 2.52155427915568e-06}, {"id": 1294, "seek": 646226, "start": 6483.74, "end": 6488.860000000001, "text": " despite not having any photos, unless they went out there and painted horses and took", "tokens": [7228, 406, 1419, 604, 5787, 11, 5969, 436, 1437, 484, 456, 293, 11797, 13112, 293, 1890], "temperature": 0.0, "avg_logprob": -0.12789296346997459, "compression_ratio": 1.4585635359116023, "no_speech_prob": 2.52155427915568e-06}, {"id": 1295, "seek": 648886, "start": 6488.86, "end": 6494.9, "text": " before and after shots, but I believe they didn't.", "tokens": [949, 293, 934, 8305, 11, 457, 286, 1697, 436, 994, 380, 13], "temperature": 0.0, "avg_logprob": -0.1777253772901452, "compression_ratio": 1.372093023255814, "no_speech_prob": 1.2805257938452996e-05}, {"id": 1296, "seek": 648886, "start": 6494.9, "end": 6498.9, "text": " So how the hell did they do this?", "tokens": [407, 577, 264, 4921, 630, 436, 360, 341, 30], "temperature": 0.0, "avg_logprob": -0.1777253772901452, "compression_ratio": 1.372093023255814, "no_speech_prob": 1.2805257938452996e-05}, {"id": 1297, "seek": 648886, "start": 6498.9, "end": 6504.339999999999, "text": " It's kind of genius.", "tokens": [467, 311, 733, 295, 14017, 13], "temperature": 0.0, "avg_logprob": -0.1777253772901452, "compression_ratio": 1.372093023255814, "no_speech_prob": 1.2805257938452996e-05}, {"id": 1298, "seek": 648886, "start": 6504.339999999999, "end": 6511.0599999999995, "text": " I will say the person I know who's doing the most interesting practice of CycleGAN right", "tokens": [286, 486, 584, 264, 954, 286, 458, 567, 311, 884, 264, 881, 1880, 3124, 295, 10295, 2160, 27699, 558], "temperature": 0.0, "avg_logprob": -0.1777253772901452, "compression_ratio": 1.372093023255814, "no_speech_prob": 1.2805257938452996e-05}, {"id": 1299, "seek": 648886, "start": 6511.0599999999995, "end": 6515.94, "text": " now is one of our students, Helena Sarin.", "tokens": [586, 307, 472, 295, 527, 1731, 11, 49294, 6894, 259, 13], "temperature": 0.0, "avg_logprob": -0.1777253772901452, "compression_ratio": 1.372093023255814, "no_speech_prob": 1.2805257938452996e-05}, {"id": 1300, "seek": 651594, "start": 6515.94, "end": 6521.259999999999, "text": " She's the only artist I know of who is a CycleGAN artist.", "tokens": [1240, 311, 264, 787, 5748, 286, 458, 295, 567, 307, 257, 10295, 2160, 27699, 5748, 13], "temperature": 0.0, "avg_logprob": -0.17331874801451902, "compression_ratio": 1.4924623115577889, "no_speech_prob": 1.0289277270203456e-05}, {"id": 1301, "seek": 651594, "start": 6521.259999999999, "end": 6522.74, "text": " Here's an example I love.", "tokens": [1692, 311, 364, 1365, 286, 959, 13], "temperature": 0.0, "avg_logprob": -0.17331874801451902, "compression_ratio": 1.4924623115577889, "no_speech_prob": 1.0289277270203456e-05}, {"id": 1302, "seek": 651594, "start": 6522.74, "end": 6529.299999999999, "text": " She created this little doodle in the top left and then trained a CycleGAN to turn it", "tokens": [1240, 2942, 341, 707, 360, 30013, 294, 264, 1192, 1411, 293, 550, 8895, 257, 10295, 2160, 27699, 281, 1261, 309], "temperature": 0.0, "avg_logprob": -0.17331874801451902, "compression_ratio": 1.4924623115577889, "no_speech_prob": 1.0289277270203456e-05}, {"id": 1303, "seek": 651594, "start": 6529.299999999999, "end": 6534.259999999999, "text": " into this beautiful painting in the bottom right.", "tokens": [666, 341, 2238, 5370, 294, 264, 2767, 558, 13], "temperature": 0.0, "avg_logprob": -0.17331874801451902, "compression_ratio": 1.4924623115577889, "no_speech_prob": 1.0289277270203456e-05}, {"id": 1304, "seek": 651594, "start": 6534.259999999999, "end": 6538.86, "text": " Here are some more of her amazing works.", "tokens": [1692, 366, 512, 544, 295, 720, 2243, 1985, 13], "temperature": 0.0, "avg_logprob": -0.17331874801451902, "compression_ratio": 1.4924623115577889, "no_speech_prob": 1.0289277270203456e-05}, {"id": 1305, "seek": 651594, "start": 6538.86, "end": 6540.379999999999, "text": " And I think it's really interesting.", "tokens": [400, 286, 519, 309, 311, 534, 1880, 13], "temperature": 0.0, "avg_logprob": -0.17331874801451902, "compression_ratio": 1.4924623115577889, "no_speech_prob": 1.0289277270203456e-05}, {"id": 1306, "seek": 654038, "start": 6540.38, "end": 6548.02, "text": " I mentioned at the start of this class that GANs are in the category of stuff that's not", "tokens": [286, 2835, 412, 264, 722, 295, 341, 1508, 300, 460, 1770, 82, 366, 294, 264, 7719, 295, 1507, 300, 311, 406], "temperature": 0.0, "avg_logprob": -0.15195294973012563, "compression_ratio": 1.6941176470588235, "no_speech_prob": 1.3211695659265388e-05}, {"id": 1307, "seek": 654038, "start": 6548.02, "end": 6550.7, "text": " there yet, but it's nearly there.", "tokens": [456, 1939, 11, 457, 309, 311, 6217, 456, 13], "temperature": 0.0, "avg_logprob": -0.15195294973012563, "compression_ratio": 1.6941176470588235, "no_speech_prob": 1.3211695659265388e-05}, {"id": 1308, "seek": 654038, "start": 6550.7, "end": 6555.58, "text": " And in this case, there's at least one person in the world now who's creating beautiful", "tokens": [400, 294, 341, 1389, 11, 456, 311, 412, 1935, 472, 954, 294, 264, 1002, 586, 567, 311, 4084, 2238], "temperature": 0.0, "avg_logprob": -0.15195294973012563, "compression_ratio": 1.6941176470588235, "no_speech_prob": 1.3211695659265388e-05}, {"id": 1309, "seek": 654038, "start": 6555.58, "end": 6558.5, "text": " and extraordinary artworks using GANs.", "tokens": [293, 10581, 15829, 82, 1228, 460, 1770, 82, 13], "temperature": 0.0, "avg_logprob": -0.15195294973012563, "compression_ratio": 1.6941176470588235, "no_speech_prob": 1.3211695659265388e-05}, {"id": 1310, "seek": 654038, "start": 6558.5, "end": 6560.74, "text": " And there's lots of specifically CycleGANs.", "tokens": [400, 456, 311, 3195, 295, 4682, 10295, 2160, 27699, 82, 13], "temperature": 0.0, "avg_logprob": -0.15195294973012563, "compression_ratio": 1.6941176470588235, "no_speech_prob": 1.3211695659265388e-05}, {"id": 1311, "seek": 654038, "start": 6560.74, "end": 6565.74, "text": " And there's actually at least maybe a dozen people I know of who are just doing interesting", "tokens": [400, 456, 311, 767, 412, 1935, 1310, 257, 16654, 561, 286, 458, 295, 567, 366, 445, 884, 1880], "temperature": 0.0, "avg_logprob": -0.15195294973012563, "compression_ratio": 1.6941176470588235, "no_speech_prob": 1.3211695659265388e-05}, {"id": 1312, "seek": 654038, "start": 6565.74, "end": 6569.9400000000005, "text": " creative work with neural nets more generally.", "tokens": [5880, 589, 365, 18161, 36170, 544, 5101, 13], "temperature": 0.0, "avg_logprob": -0.15195294973012563, "compression_ratio": 1.6941176470588235, "no_speech_prob": 1.3211695659265388e-05}, {"id": 1313, "seek": 656994, "start": 6569.94, "end": 6575.0599999999995, "text": " And the field of creative AI is going to expand dramatically.", "tokens": [400, 264, 2519, 295, 5880, 7318, 307, 516, 281, 5268, 17548, 13], "temperature": 0.0, "avg_logprob": -0.18475219530936998, "compression_ratio": 1.6056338028169015, "no_speech_prob": 4.2645755456760526e-05}, {"id": 1314, "seek": 656994, "start": 6575.0599999999995, "end": 6578.339999999999, "text": " And I think it's interesting with Helena, I don't know her personally, but from what", "tokens": [400, 286, 519, 309, 311, 1880, 365, 49294, 11, 286, 500, 380, 458, 720, 5665, 11, 457, 490, 437], "temperature": 0.0, "avg_logprob": -0.18475219530936998, "compression_ratio": 1.6056338028169015, "no_speech_prob": 4.2645755456760526e-05}, {"id": 1315, "seek": 656994, "start": 6578.339999999999, "end": 6585.08, "text": " I understand of her background, she's a software developer as her full-time job and an artist", "tokens": [286, 1223, 295, 720, 3678, 11, 750, 311, 257, 4722, 10754, 382, 720, 1577, 12, 3766, 1691, 293, 364, 5748], "temperature": 0.0, "avg_logprob": -0.18475219530936998, "compression_ratio": 1.6056338028169015, "no_speech_prob": 4.2645755456760526e-05}, {"id": 1316, "seek": 656994, "start": 6585.08, "end": 6586.08, "text": " as her hobby.", "tokens": [382, 720, 18240, 13], "temperature": 0.0, "avg_logprob": -0.18475219530936998, "compression_ratio": 1.6056338028169015, "no_speech_prob": 4.2645755456760526e-05}, {"id": 1317, "seek": 656994, "start": 6586.08, "end": 6591.62, "text": " And she's kind of started combining these two by saying, gosh, I wonder what this particular", "tokens": [400, 750, 311, 733, 295, 1409, 21928, 613, 732, 538, 1566, 11, 6502, 11, 286, 2441, 437, 341, 1729], "temperature": 0.0, "avg_logprob": -0.18475219530936998, "compression_ratio": 1.6056338028169015, "no_speech_prob": 4.2645755456760526e-05}, {"id": 1318, "seek": 656994, "start": 6591.62, "end": 6593.7, "text": " tool could bring to my art.", "tokens": [2290, 727, 1565, 281, 452, 1523, 13], "temperature": 0.0, "avg_logprob": -0.18475219530936998, "compression_ratio": 1.6056338028169015, "no_speech_prob": 4.2645755456760526e-05}, {"id": 1319, "seek": 656994, "start": 6593.7, "end": 6598.099999999999, "text": " And so if you follow her Twitter account, we'll make sure we add it on the Wiki.", "tokens": [400, 370, 498, 291, 1524, 720, 5794, 2696, 11, 321, 603, 652, 988, 321, 909, 309, 322, 264, 35892, 13], "temperature": 0.0, "avg_logprob": -0.18475219530936998, "compression_ratio": 1.6056338028169015, "no_speech_prob": 4.2645755456760526e-05}, {"id": 1320, "seek": 659810, "start": 6598.1, "end": 6603.1, "text": " If somebody can find it, it's Helena Sarin.", "tokens": [759, 2618, 393, 915, 309, 11, 309, 311, 49294, 6894, 259, 13], "temperature": 0.0, "avg_logprob": -0.2211471105876722, "compression_ratio": 1.423913043478261, "no_speech_prob": 6.240798029466532e-06}, {"id": 1321, "seek": 659810, "start": 6603.1, "end": 6605.900000000001, "text": " She basically posts a new work almost every day.", "tokens": [1240, 1936, 12300, 257, 777, 589, 1920, 633, 786, 13], "temperature": 0.0, "avg_logprob": -0.2211471105876722, "compression_ratio": 1.423913043478261, "no_speech_prob": 6.240798029466532e-06}, {"id": 1322, "seek": 659810, "start": 6605.900000000001, "end": 6611.900000000001, "text": " And they're always pretty amazing.", "tokens": [400, 436, 434, 1009, 1238, 2243, 13], "temperature": 0.0, "avg_logprob": -0.2211471105876722, "compression_ratio": 1.423913043478261, "no_speech_prob": 6.240798029466532e-06}, {"id": 1323, "seek": 659810, "start": 6611.900000000001, "end": 6614.660000000001, "text": " So here's the basic trick.", "tokens": [407, 510, 311, 264, 3875, 4282, 13], "temperature": 0.0, "avg_logprob": -0.2211471105876722, "compression_ratio": 1.423913043478261, "no_speech_prob": 6.240798029466532e-06}, {"id": 1324, "seek": 659810, "start": 6614.660000000001, "end": 6618.700000000001, "text": " And this is from the CycleGAN paper.", "tokens": [400, 341, 307, 490, 264, 10295, 2160, 27699, 3035, 13], "temperature": 0.0, "avg_logprob": -0.2211471105876722, "compression_ratio": 1.423913043478261, "no_speech_prob": 6.240798029466532e-06}, {"id": 1325, "seek": 659810, "start": 6618.700000000001, "end": 6626.700000000001, "text": " We're going to have two images, assuming we're doing this with images.", "tokens": [492, 434, 516, 281, 362, 732, 5267, 11, 11926, 321, 434, 884, 341, 365, 5267, 13], "temperature": 0.0, "avg_logprob": -0.2211471105876722, "compression_ratio": 1.423913043478261, "no_speech_prob": 6.240798029466532e-06}, {"id": 1326, "seek": 662670, "start": 6626.7, "end": 6629.099999999999, "text": " But the key thing is they're not paired images.", "tokens": [583, 264, 2141, 551, 307, 436, 434, 406, 25699, 5267, 13], "temperature": 0.0, "avg_logprob": -0.15164957500639417, "compression_ratio": 1.680952380952381, "no_speech_prob": 1.2411325769789983e-05}, {"id": 1327, "seek": 662670, "start": 6629.099999999999, "end": 6634.66, "text": " So we don't have a data set of horses and the equivalent zebras.", "tokens": [407, 321, 500, 380, 362, 257, 1412, 992, 295, 13112, 293, 264, 10344, 5277, 38182, 13], "temperature": 0.0, "avg_logprob": -0.15164957500639417, "compression_ratio": 1.680952380952381, "no_speech_prob": 1.2411325769789983e-05}, {"id": 1328, "seek": 662670, "start": 6634.66, "end": 6638.48, "text": " We've got a bunch of horses, bunch of zebras.", "tokens": [492, 600, 658, 257, 3840, 295, 13112, 11, 3840, 295, 5277, 38182, 13], "temperature": 0.0, "avg_logprob": -0.15164957500639417, "compression_ratio": 1.680952380952381, "no_speech_prob": 1.2411325769789983e-05}, {"id": 1329, "seek": 662670, "start": 6638.48, "end": 6641.82, "text": " Grab one horse, grab one zebra.", "tokens": [20357, 472, 6832, 11, 4444, 472, 47060, 13], "temperature": 0.0, "avg_logprob": -0.15164957500639417, "compression_ratio": 1.680952380952381, "no_speech_prob": 1.2411325769789983e-05}, {"id": 1330, "seek": 662670, "start": 6641.82, "end": 6647.099999999999, "text": " We've now got an X. So let's say X is horse and Y is zebra.", "tokens": [492, 600, 586, 658, 364, 1783, 13, 407, 718, 311, 584, 1783, 307, 6832, 293, 398, 307, 47060, 13], "temperature": 0.0, "avg_logprob": -0.15164957500639417, "compression_ratio": 1.680952380952381, "no_speech_prob": 1.2411325769789983e-05}, {"id": 1331, "seek": 662670, "start": 6647.099999999999, "end": 6653.78, "text": " We're going to train a generator, what they call here a mapping function, that turns horse", "tokens": [492, 434, 516, 281, 3847, 257, 19265, 11, 437, 436, 818, 510, 257, 18350, 2445, 11, 300, 4523, 6832], "temperature": 0.0, "avg_logprob": -0.15164957500639417, "compression_ratio": 1.680952380952381, "no_speech_prob": 1.2411325769789983e-05}, {"id": 1332, "seek": 662670, "start": 6653.78, "end": 6654.78, "text": " into zebra.", "tokens": [666, 47060, 13], "temperature": 0.0, "avg_logprob": -0.15164957500639417, "compression_ratio": 1.680952380952381, "no_speech_prob": 1.2411325769789983e-05}, {"id": 1333, "seek": 665478, "start": 6654.78, "end": 6659.7, "text": " We'll call that mapping function G. And we'll create one mapping function generator that", "tokens": [492, 603, 818, 300, 18350, 2445, 460, 13, 400, 321, 603, 1884, 472, 18350, 2445, 19265, 300], "temperature": 0.0, "avg_logprob": -0.2033900363104684, "compression_ratio": 1.97196261682243, "no_speech_prob": 6.540407412103377e-06}, {"id": 1334, "seek": 665478, "start": 6659.7, "end": 6661.98, "text": " turns a zebra into a horse.", "tokens": [4523, 257, 47060, 666, 257, 6832, 13], "temperature": 0.0, "avg_logprob": -0.2033900363104684, "compression_ratio": 1.97196261682243, "no_speech_prob": 6.540407412103377e-06}, {"id": 1335, "seek": 665478, "start": 6661.98, "end": 6669.219999999999, "text": " And we'll call that F. We'll create a discriminator, just like we did before, which is going to", "tokens": [400, 321, 603, 818, 300, 479, 13, 492, 603, 1884, 257, 20828, 1639, 11, 445, 411, 321, 630, 949, 11, 597, 307, 516, 281], "temperature": 0.0, "avg_logprob": -0.2033900363104684, "compression_ratio": 1.97196261682243, "no_speech_prob": 6.540407412103377e-06}, {"id": 1336, "seek": 665478, "start": 6669.219999999999, "end": 6674.099999999999, "text": " get as good as possible at recognizing real from fake horses.", "tokens": [483, 382, 665, 382, 1944, 412, 18538, 957, 490, 7592, 13112, 13], "temperature": 0.0, "avg_logprob": -0.2033900363104684, "compression_ratio": 1.97196261682243, "no_speech_prob": 6.540407412103377e-06}, {"id": 1337, "seek": 665478, "start": 6674.099999999999, "end": 6675.86, "text": " So that'll be DX.", "tokens": [407, 300, 603, 312, 48817, 13], "temperature": 0.0, "avg_logprob": -0.2033900363104684, "compression_ratio": 1.97196261682243, "no_speech_prob": 6.540407412103377e-06}, {"id": 1338, "seek": 665478, "start": 6675.86, "end": 6680.3, "text": " And then another discriminator which is going to be as good as possible at recognizing real", "tokens": [400, 550, 1071, 20828, 1639, 597, 307, 516, 281, 312, 382, 665, 382, 1944, 412, 18538, 957], "temperature": 0.0, "avg_logprob": -0.2033900363104684, "compression_ratio": 1.97196261682243, "no_speech_prob": 6.540407412103377e-06}, {"id": 1339, "seek": 665478, "start": 6680.3, "end": 6681.54, "text": " from fake zebras.", "tokens": [490, 7592, 5277, 38182, 13], "temperature": 0.0, "avg_logprob": -0.2033900363104684, "compression_ratio": 1.97196261682243, "no_speech_prob": 6.540407412103377e-06}, {"id": 1340, "seek": 665478, "start": 6681.54, "end": 6684.74, "text": " We'll call that DY.", "tokens": [492, 603, 818, 300, 48427, 13], "temperature": 0.0, "avg_logprob": -0.2033900363104684, "compression_ratio": 1.97196261682243, "no_speech_prob": 6.540407412103377e-06}, {"id": 1341, "seek": 668474, "start": 6684.74, "end": 6687.78, "text": " So that's kind of our starting point.", "tokens": [407, 300, 311, 733, 295, 527, 2891, 935, 13], "temperature": 0.0, "avg_logprob": -0.15966151310847357, "compression_ratio": 1.6861924686192469, "no_speech_prob": 2.0785026208614e-05}, {"id": 1342, "seek": 668474, "start": 6687.78, "end": 6692.94, "text": " But then the key thing to making this work, so we're kind of generating a loss function", "tokens": [583, 550, 264, 2141, 551, 281, 1455, 341, 589, 11, 370, 321, 434, 733, 295, 17746, 257, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.15966151310847357, "compression_ratio": 1.6861924686192469, "no_speech_prob": 2.0785026208614e-05}, {"id": 1343, "seek": 668474, "start": 6692.94, "end": 6693.94, "text": " here.", "tokens": [510, 13], "temperature": 0.0, "avg_logprob": -0.15966151310847357, "compression_ratio": 1.6861924686192469, "no_speech_prob": 2.0785026208614e-05}, {"id": 1344, "seek": 668474, "start": 6693.94, "end": 6696.62, "text": " Here's one bit of the loss function, here's a second bit of the loss function.", "tokens": [1692, 311, 472, 857, 295, 264, 4470, 2445, 11, 510, 311, 257, 1150, 857, 295, 264, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.15966151310847357, "compression_ratio": 1.6861924686192469, "no_speech_prob": 2.0785026208614e-05}, {"id": 1345, "seek": 668474, "start": 6696.62, "end": 6701.219999999999, "text": " We're going to create something called cycle consistency loss, which says after you turn", "tokens": [492, 434, 516, 281, 1884, 746, 1219, 6586, 14416, 4470, 11, 597, 1619, 934, 291, 1261], "temperature": 0.0, "avg_logprob": -0.15966151310847357, "compression_ratio": 1.6861924686192469, "no_speech_prob": 2.0785026208614e-05}, {"id": 1346, "seek": 668474, "start": 6701.219999999999, "end": 6710.3, "text": " your horse into a zebra with your G generator, and check whether or not I can recognize that", "tokens": [428, 6832, 666, 257, 47060, 365, 428, 460, 19265, 11, 293, 1520, 1968, 420, 406, 286, 393, 5521, 300], "temperature": 0.0, "avg_logprob": -0.15966151310847357, "compression_ratio": 1.6861924686192469, "no_speech_prob": 2.0785026208614e-05}, {"id": 1347, "seek": 668474, "start": 6710.3, "end": 6711.3, "text": " it's real.", "tokens": [309, 311, 957, 13], "temperature": 0.0, "avg_logprob": -0.15966151310847357, "compression_ratio": 1.6861924686192469, "no_speech_prob": 2.0785026208614e-05}, {"id": 1348, "seek": 671130, "start": 6711.3, "end": 6715.18, "text": " I keep forgetting which one is horse and which one is zebra.", "tokens": [286, 1066, 25428, 597, 472, 307, 6832, 293, 597, 472, 307, 47060, 13], "temperature": 0.0, "avg_logprob": -0.19978556522103244, "compression_ratio": 1.7096774193548387, "no_speech_prob": 9.368691280542407e-06}, {"id": 1349, "seek": 671130, "start": 6715.18, "end": 6717.74, "text": " I apologize if I get my X's and Y's backwards.", "tokens": [286, 12328, 498, 286, 483, 452, 1783, 311, 293, 398, 311, 12204, 13], "temperature": 0.0, "avg_logprob": -0.19978556522103244, "compression_ratio": 1.7096774193548387, "no_speech_prob": 9.368691280542407e-06}, {"id": 1350, "seek": 671130, "start": 6717.74, "end": 6723.1, "text": " I turn my horse into a zebra, and then I'm going to try and turn that zebra back into", "tokens": [286, 1261, 452, 6832, 666, 257, 47060, 11, 293, 550, 286, 478, 516, 281, 853, 293, 1261, 300, 47060, 646, 666], "temperature": 0.0, "avg_logprob": -0.19978556522103244, "compression_ratio": 1.7096774193548387, "no_speech_prob": 9.368691280542407e-06}, {"id": 1351, "seek": 671130, "start": 6723.1, "end": 6727.1, "text": " the same horse that I started with.", "tokens": [264, 912, 6832, 300, 286, 1409, 365, 13], "temperature": 0.0, "avg_logprob": -0.19978556522103244, "compression_ratio": 1.7096774193548387, "no_speech_prob": 9.368691280542407e-06}, {"id": 1352, "seek": 671130, "start": 6727.1, "end": 6736.42, "text": " And so then I'm going to have another function that's going to check whether this horse,", "tokens": [400, 370, 550, 286, 478, 516, 281, 362, 1071, 2445, 300, 311, 516, 281, 1520, 1968, 341, 6832, 11], "temperature": 0.0, "avg_logprob": -0.19978556522103244, "compression_ratio": 1.7096774193548387, "no_speech_prob": 9.368691280542407e-06}, {"id": 1353, "seek": 673642, "start": 6736.42, "end": 6742.26, "text": " which I generated knowing nothing about X, generated entirely from this zebra, is similar", "tokens": [597, 286, 10833, 5276, 1825, 466, 1783, 11, 10833, 7696, 490, 341, 47060, 11, 307, 2531], "temperature": 0.0, "avg_logprob": -0.110836929149842, "compression_ratio": 1.7136150234741785, "no_speech_prob": 4.888297553407028e-07}, {"id": 1354, "seek": 673642, "start": 6742.26, "end": 6745.06, "text": " to the original horse or not.", "tokens": [281, 264, 3380, 6832, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.110836929149842, "compression_ratio": 1.7136150234741785, "no_speech_prob": 4.888297553407028e-07}, {"id": 1355, "seek": 673642, "start": 6745.06, "end": 6752.46, "text": " So the idea would be, if your generated zebra doesn't look anything like your original horse,", "tokens": [407, 264, 1558, 576, 312, 11, 498, 428, 10833, 47060, 1177, 380, 574, 1340, 411, 428, 3380, 6832, 11], "temperature": 0.0, "avg_logprob": -0.110836929149842, "compression_ratio": 1.7136150234741785, "no_speech_prob": 4.888297553407028e-07}, {"id": 1356, "seek": 673642, "start": 6752.46, "end": 6756.46, "text": " you've got no chance of turning it back into the original horse.", "tokens": [291, 600, 658, 572, 2931, 295, 6246, 309, 646, 666, 264, 3380, 6832, 13], "temperature": 0.0, "avg_logprob": -0.110836929149842, "compression_ratio": 1.7136150234741785, "no_speech_prob": 4.888297553407028e-07}, {"id": 1357, "seek": 673642, "start": 6756.46, "end": 6764.26, "text": " So a loss which compares X hat to X is going to be really bad unless you can go into Y", "tokens": [407, 257, 4470, 597, 38334, 1783, 2385, 281, 1783, 307, 516, 281, 312, 534, 1578, 5969, 291, 393, 352, 666, 398], "temperature": 0.0, "avg_logprob": -0.110836929149842, "compression_ratio": 1.7136150234741785, "no_speech_prob": 4.888297553407028e-07}, {"id": 1358, "seek": 676426, "start": 6764.26, "end": 6768.5, "text": " and back out again, and you're probably only going to be able to do that if you're able", "tokens": [293, 646, 484, 797, 11, 293, 291, 434, 1391, 787, 516, 281, 312, 1075, 281, 360, 300, 498, 291, 434, 1075], "temperature": 0.0, "avg_logprob": -0.13056368779654454, "compression_ratio": 1.95, "no_speech_prob": 6.240911261556903e-06}, {"id": 1359, "seek": 676426, "start": 6768.5, "end": 6773.74, "text": " to create a zebra that looks like the original horse so that you know what the original horse", "tokens": [281, 1884, 257, 47060, 300, 1542, 411, 264, 3380, 6832, 370, 300, 291, 458, 437, 264, 3380, 6832], "temperature": 0.0, "avg_logprob": -0.13056368779654454, "compression_ratio": 1.95, "no_speech_prob": 6.240911261556903e-06}, {"id": 1360, "seek": 676426, "start": 6773.74, "end": 6775.46, "text": " looked like.", "tokens": [2956, 411, 13], "temperature": 0.0, "avg_logprob": -0.13056368779654454, "compression_ratio": 1.95, "no_speech_prob": 6.240911261556903e-06}, {"id": 1361, "seek": 676426, "start": 6775.46, "end": 6783.3, "text": " And vice versa, take your zebra, turn it into a fake horse, and check that you can recognize", "tokens": [400, 11964, 25650, 11, 747, 428, 47060, 11, 1261, 309, 666, 257, 7592, 6832, 11, 293, 1520, 300, 291, 393, 5521], "temperature": 0.0, "avg_logprob": -0.13056368779654454, "compression_ratio": 1.95, "no_speech_prob": 6.240911261556903e-06}, {"id": 1362, "seek": 676426, "start": 6783.3, "end": 6788.42, "text": " that, and then try and turn it back into the original zebra and check that it looks like", "tokens": [300, 11, 293, 550, 853, 293, 1261, 309, 646, 666, 264, 3380, 47060, 293, 1520, 300, 309, 1542, 411], "temperature": 0.0, "avg_logprob": -0.13056368779654454, "compression_ratio": 1.95, "no_speech_prob": 6.240911261556903e-06}, {"id": 1363, "seek": 676426, "start": 6788.42, "end": 6789.42, "text": " the original.", "tokens": [264, 3380, 13], "temperature": 0.0, "avg_logprob": -0.13056368779654454, "compression_ratio": 1.95, "no_speech_prob": 6.240911261556903e-06}, {"id": 1364, "seek": 678942, "start": 6789.42, "end": 6798.22, "text": " So notice here, this F is our zebra to horse, this G is our horse to zebra, so the G and", "tokens": [407, 3449, 510, 11, 341, 479, 307, 527, 47060, 281, 6832, 11, 341, 460, 307, 527, 6832, 281, 47060, 11, 370, 264, 460, 293], "temperature": 0.0, "avg_logprob": -0.13416802542550224, "compression_ratio": 1.903225806451613, "no_speech_prob": 4.637862275558291e-06}, {"id": 1365, "seek": 678942, "start": 6798.22, "end": 6800.78, "text": " the F are kind of doing two things.", "tokens": [264, 479, 366, 733, 295, 884, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.13416802542550224, "compression_ratio": 1.903225806451613, "no_speech_prob": 4.637862275558291e-06}, {"id": 1366, "seek": 678942, "start": 6800.78, "end": 6807.1, "text": " They're both turning the original horse into the zebra and then turning the zebra back", "tokens": [814, 434, 1293, 6246, 264, 3380, 6832, 666, 264, 47060, 293, 550, 6246, 264, 47060, 646], "temperature": 0.0, "avg_logprob": -0.13416802542550224, "compression_ratio": 1.903225806451613, "no_speech_prob": 4.637862275558291e-06}, {"id": 1367, "seek": 678942, "start": 6807.1, "end": 6808.86, "text": " into the original horse.", "tokens": [666, 264, 3380, 6832, 13], "temperature": 0.0, "avg_logprob": -0.13416802542550224, "compression_ratio": 1.903225806451613, "no_speech_prob": 4.637862275558291e-06}, {"id": 1368, "seek": 678942, "start": 6808.86, "end": 6811.62, "text": " So notice that there's only two generators.", "tokens": [407, 3449, 300, 456, 311, 787, 732, 38662, 13], "temperature": 0.0, "avg_logprob": -0.13416802542550224, "compression_ratio": 1.903225806451613, "no_speech_prob": 4.637862275558291e-06}, {"id": 1369, "seek": 678942, "start": 6811.62, "end": 6815.04, "text": " There isn't a separate generator for the reverse mapping.", "tokens": [821, 1943, 380, 257, 4994, 19265, 337, 264, 9943, 18350, 13], "temperature": 0.0, "avg_logprob": -0.13416802542550224, "compression_ratio": 1.903225806451613, "no_speech_prob": 4.637862275558291e-06}, {"id": 1370, "seek": 678942, "start": 6815.04, "end": 6819.2, "text": " You have to use the same generator that was used for the original mapping.", "tokens": [509, 362, 281, 764, 264, 912, 19265, 300, 390, 1143, 337, 264, 3380, 18350, 13], "temperature": 0.0, "avg_logprob": -0.13416802542550224, "compression_ratio": 1.903225806451613, "no_speech_prob": 4.637862275558291e-06}, {"id": 1371, "seek": 681920, "start": 6819.2, "end": 6826.58, "text": " So this is the cycle consistency loss, and I just think this is genius.", "tokens": [407, 341, 307, 264, 6586, 14416, 4470, 11, 293, 286, 445, 519, 341, 307, 14017, 13], "temperature": 0.0, "avg_logprob": -0.13583682133601263, "compression_ratio": 1.7412935323383085, "no_speech_prob": 6.144151029729983e-06}, {"id": 1372, "seek": 681920, "start": 6826.58, "end": 6832.66, "text": " The idea that this is a thing that could even be possible, honestly when this came out,", "tokens": [440, 1558, 300, 341, 307, 257, 551, 300, 727, 754, 312, 1944, 11, 6095, 562, 341, 1361, 484, 11], "temperature": 0.0, "avg_logprob": -0.13583682133601263, "compression_ratio": 1.7412935323383085, "no_speech_prob": 6.144151029729983e-06}, {"id": 1373, "seek": 681920, "start": 6832.66, "end": 6836.58, "text": " it just never occurred to me as a thing that I could even try and solve.", "tokens": [309, 445, 1128, 11068, 281, 385, 382, 257, 551, 300, 286, 727, 754, 853, 293, 5039, 13], "temperature": 0.0, "avg_logprob": -0.13583682133601263, "compression_ratio": 1.7412935323383085, "no_speech_prob": 6.144151029729983e-06}, {"id": 1374, "seek": 681920, "start": 6836.58, "end": 6838.94, "text": " It seems so obviously impossible.", "tokens": [467, 2544, 370, 2745, 6243, 13], "temperature": 0.0, "avg_logprob": -0.13583682133601263, "compression_ratio": 1.7412935323383085, "no_speech_prob": 6.144151029729983e-06}, {"id": 1375, "seek": 681920, "start": 6838.94, "end": 6846.0199999999995, "text": " And then the idea that you can solve it like this, I just think it's so damn smart.", "tokens": [400, 550, 264, 1558, 300, 291, 393, 5039, 309, 411, 341, 11, 286, 445, 519, 309, 311, 370, 8151, 4069, 13], "temperature": 0.0, "avg_logprob": -0.13583682133601263, "compression_ratio": 1.7412935323383085, "no_speech_prob": 6.144151029729983e-06}, {"id": 1376, "seek": 684602, "start": 6846.02, "end": 6854.780000000001, "text": " So it's good to look at the equations in this paper because they're just good examples.", "tokens": [407, 309, 311, 665, 281, 574, 412, 264, 11787, 294, 341, 3035, 570, 436, 434, 445, 665, 5110, 13], "temperature": 0.0, "avg_logprob": -0.15228715979534646, "compression_ratio": 1.7093023255813953, "no_speech_prob": 6.962214229133679e-06}, {"id": 1377, "seek": 684602, "start": 6854.780000000001, "end": 6856.740000000001, "text": " They're written pretty simply.", "tokens": [814, 434, 3720, 1238, 2935, 13], "temperature": 0.0, "avg_logprob": -0.15228715979534646, "compression_ratio": 1.7093023255813953, "no_speech_prob": 6.962214229133679e-06}, {"id": 1378, "seek": 684602, "start": 6856.740000000001, "end": 6862.740000000001, "text": " It's not like some of the stuff in the Wasserstein-Gann paper, which is like lots of theoretical", "tokens": [467, 311, 406, 411, 512, 295, 264, 1507, 294, 264, 17351, 9089, 12, 38, 969, 3035, 11, 597, 307, 411, 3195, 295, 20864], "temperature": 0.0, "avg_logprob": -0.15228715979534646, "compression_ratio": 1.7093023255813953, "no_speech_prob": 6.962214229133679e-06}, {"id": 1379, "seek": 684602, "start": 6862.740000000001, "end": 6864.18, "text": " proofs and whatever else.", "tokens": [8177, 82, 293, 2035, 1646, 13], "temperature": 0.0, "avg_logprob": -0.15228715979534646, "compression_ratio": 1.7093023255813953, "no_speech_prob": 6.962214229133679e-06}, {"id": 1380, "seek": 684602, "start": 6864.18, "end": 6868.860000000001, "text": " In this case, they're just equations that just lay out what's going on, and you really", "tokens": [682, 341, 1389, 11, 436, 434, 445, 11787, 300, 445, 2360, 484, 437, 311, 516, 322, 11, 293, 291, 534], "temperature": 0.0, "avg_logprob": -0.15228715979534646, "compression_ratio": 1.7093023255813953, "no_speech_prob": 6.962214229133679e-06}, {"id": 1381, "seek": 684602, "start": 6868.860000000001, "end": 6872.14, "text": " want to get to a point where you can read them and understand them.", "tokens": [528, 281, 483, 281, 257, 935, 689, 291, 393, 1401, 552, 293, 1223, 552, 13], "temperature": 0.0, "avg_logprob": -0.15228715979534646, "compression_ratio": 1.7093023255813953, "no_speech_prob": 6.962214229133679e-06}, {"id": 1382, "seek": 684602, "start": 6872.14, "end": 6875.14, "text": " So let's kind of start talking through them.", "tokens": [407, 718, 311, 733, 295, 722, 1417, 807, 552, 13], "temperature": 0.0, "avg_logprob": -0.15228715979534646, "compression_ratio": 1.7093023255813953, "no_speech_prob": 6.962214229133679e-06}, {"id": 1383, "seek": 687514, "start": 6875.14, "end": 6882.54, "text": " So we've got a horse and a zebra.", "tokens": [407, 321, 600, 658, 257, 6832, 293, 257, 47060, 13], "temperature": 0.0, "avg_logprob": -0.17003658612569172, "compression_ratio": 1.4788732394366197, "no_speech_prob": 9.818285434448626e-06}, {"id": 1384, "seek": 687514, "start": 6882.54, "end": 6891.34, "text": " So for some mapping function G, which is our horse to zebra mapping function, then there's", "tokens": [407, 337, 512, 18350, 2445, 460, 11, 597, 307, 527, 6832, 281, 47060, 18350, 2445, 11, 550, 456, 311], "temperature": 0.0, "avg_logprob": -0.17003658612569172, "compression_ratio": 1.4788732394366197, "no_speech_prob": 9.818285434448626e-06}, {"id": 1385, "seek": 687514, "start": 6891.34, "end": 6896.34, "text": " a GAN loss, which is the bit we're already familiar with, that says I've got a horse,", "tokens": [257, 460, 1770, 4470, 11, 597, 307, 264, 857, 321, 434, 1217, 4963, 365, 11, 300, 1619, 286, 600, 658, 257, 6832, 11], "temperature": 0.0, "avg_logprob": -0.17003658612569172, "compression_ratio": 1.4788732394366197, "no_speech_prob": 9.818285434448626e-06}, {"id": 1386, "seek": 689634, "start": 6896.34, "end": 6906.14, "text": " a zebra, a fake zebra recognizer, and a horse to zebra generator, and the loss is what we", "tokens": [257, 47060, 11, 257, 7592, 47060, 3068, 6545, 11, 293, 257, 6832, 281, 47060, 19265, 11, 293, 264, 4470, 307, 437, 321], "temperature": 0.0, "avg_logprob": -0.1271161112868995, "compression_ratio": 1.5116279069767442, "no_speech_prob": 9.422435596206924e-07}, {"id": 1387, "seek": 689634, "start": 6906.14, "end": 6907.42, "text": " saw before.", "tokens": [1866, 949, 13], "temperature": 0.0, "avg_logprob": -0.1271161112868995, "compression_ratio": 1.5116279069767442, "no_speech_prob": 9.422435596206924e-07}, {"id": 1388, "seek": 689634, "start": 6907.42, "end": 6919.58, "text": " It's our ability to draw one zebra out of our zebras and recognize whether it's real", "tokens": [467, 311, 527, 3485, 281, 2642, 472, 47060, 484, 295, 527, 5277, 38182, 293, 5521, 1968, 309, 311, 957], "temperature": 0.0, "avg_logprob": -0.1271161112868995, "compression_ratio": 1.5116279069767442, "no_speech_prob": 9.422435596206924e-07}, {"id": 1389, "seek": 689634, "start": 6919.58, "end": 6922.3, "text": " or fake.", "tokens": [420, 7592, 13], "temperature": 0.0, "avg_logprob": -0.1271161112868995, "compression_ratio": 1.5116279069767442, "no_speech_prob": 9.422435596206924e-07}, {"id": 1390, "seek": 692230, "start": 6922.3, "end": 6934.46, "text": " And then take a horse and turn it into a zebra and recognize whether that's real or fake.", "tokens": [400, 550, 747, 257, 6832, 293, 1261, 309, 666, 257, 47060, 293, 5521, 1968, 300, 311, 957, 420, 7592, 13], "temperature": 0.0, "avg_logprob": -0.19433463829151099, "compression_ratio": 1.4567901234567902, "no_speech_prob": 3.555978992153541e-06}, {"id": 1391, "seek": 692230, "start": 6934.46, "end": 6938.900000000001, "text": " And then you do one minus the other.", "tokens": [400, 550, 291, 360, 472, 3175, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.19433463829151099, "compression_ratio": 1.4567901234567902, "no_speech_prob": 3.555978992153541e-06}, {"id": 1392, "seek": 692230, "start": 6938.900000000001, "end": 6941.18, "text": " In this case, they've got a log in there.", "tokens": [682, 341, 1389, 11, 436, 600, 658, 257, 3565, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.19433463829151099, "compression_ratio": 1.4567901234567902, "no_speech_prob": 3.555978992153541e-06}, {"id": 1393, "seek": 692230, "start": 6941.18, "end": 6943.1, "text": " The log's not terribly important.", "tokens": [440, 3565, 311, 406, 22903, 1021, 13], "temperature": 0.0, "avg_logprob": -0.19433463829151099, "compression_ratio": 1.4567901234567902, "no_speech_prob": 3.555978992153541e-06}, {"id": 1394, "seek": 692230, "start": 6943.1, "end": 6945.62, "text": " So this is the thing we just saw.", "tokens": [407, 341, 307, 264, 551, 321, 445, 1866, 13], "temperature": 0.0, "avg_logprob": -0.19433463829151099, "compression_ratio": 1.4567901234567902, "no_speech_prob": 3.555978992153541e-06}, {"id": 1395, "seek": 694562, "start": 6945.62, "end": 6954.82, "text": " That's why we did Wasserstein GAN first, this is just a standard GAN loss in math form.", "tokens": [663, 311, 983, 321, 630, 17351, 9089, 460, 1770, 700, 11, 341, 307, 445, 257, 3832, 460, 1770, 4470, 294, 5221, 1254, 13], "temperature": 0.0, "avg_logprob": -0.2353469580411911, "compression_ratio": 1.393063583815029, "no_speech_prob": 2.7968584618065506e-05}, {"id": 1396, "seek": 694562, "start": 6954.82, "end": 6958.66, "text": " All of this sounds awfully like translating in one language to another, then back to the", "tokens": [1057, 295, 341, 3263, 47976, 411, 35030, 294, 472, 2856, 281, 1071, 11, 550, 646, 281, 264], "temperature": 0.0, "avg_logprob": -0.2353469580411911, "compression_ratio": 1.393063583815029, "no_speech_prob": 2.7968584618065506e-05}, {"id": 1397, "seek": 694562, "start": 6958.66, "end": 6959.66, "text": " original.", "tokens": [3380, 13], "temperature": 0.0, "avg_logprob": -0.2353469580411911, "compression_ratio": 1.393063583815029, "no_speech_prob": 2.7968584618065506e-05}, {"id": 1398, "seek": 694562, "start": 6959.66, "end": 6964.5, "text": " Have GANs or any equivalent been tried in translation?", "tokens": [3560, 460, 1770, 82, 420, 604, 10344, 668, 3031, 294, 12853, 30], "temperature": 0.0, "avg_logprob": -0.2353469580411911, "compression_ratio": 1.393063583815029, "no_speech_prob": 2.7968584618065506e-05}, {"id": 1399, "seek": 696450, "start": 6964.5, "end": 6977.7, "text": " Not that I know of.", "tokens": [1726, 300, 286, 458, 295, 13], "temperature": 0.0, "avg_logprob": -0.21740398735835634, "compression_ratio": 1.3766233766233766, "no_speech_prob": 6.401809514500201e-05}, {"id": 1400, "seek": 696450, "start": 6977.7, "end": 6985.66, "text": " There's unsupervised machine translation, which does kind of do something like this,", "tokens": [821, 311, 2693, 12879, 24420, 3479, 12853, 11, 597, 775, 733, 295, 360, 746, 411, 341, 11], "temperature": 0.0, "avg_logprob": -0.21740398735835634, "compression_ratio": 1.3766233766233766, "no_speech_prob": 6.401809514500201e-05}, {"id": 1401, "seek": 696450, "start": 6985.66, "end": 6990.86, "text": " but I haven't looked at it closely enough to know if it's nearly identical or if it's", "tokens": [457, 286, 2378, 380, 2956, 412, 309, 8185, 1547, 281, 458, 498, 309, 311, 6217, 14800, 420, 498, 309, 311], "temperature": 0.0, "avg_logprob": -0.21740398735835634, "compression_ratio": 1.3766233766233766, "no_speech_prob": 6.401809514500201e-05}, {"id": 1402, "seek": 696450, "start": 6990.86, "end": 6993.86, "text": " just vaguely similar.", "tokens": [445, 13501, 48863, 2531, 13], "temperature": 0.0, "avg_logprob": -0.21740398735835634, "compression_ratio": 1.3766233766233766, "no_speech_prob": 6.401809514500201e-05}, {"id": 1403, "seek": 699386, "start": 6993.86, "end": 6999.9, "text": " So, to kind of back up to what I do know, normally with translation, you require this", "tokens": [407, 11, 281, 733, 295, 646, 493, 281, 437, 286, 360, 458, 11, 5646, 365, 12853, 11, 291, 3651, 341], "temperature": 0.0, "avg_logprob": -0.1348322362315898, "compression_ratio": 1.771186440677966, "no_speech_prob": 4.092887593287742e-06}, {"id": 1404, "seek": 699386, "start": 6999.9, "end": 7003.099999999999, "text": " kind of paired input, you require parallel texts.", "tokens": [733, 295, 25699, 4846, 11, 291, 3651, 8952, 15765, 13], "temperature": 0.0, "avg_logprob": -0.1348322362315898, "compression_ratio": 1.771186440677966, "no_speech_prob": 4.092887593287742e-06}, {"id": 1405, "seek": 699386, "start": 7003.099999999999, "end": 7007.86, "text": " This is the French translation of this English sentence.", "tokens": [639, 307, 264, 5522, 12853, 295, 341, 3669, 8174, 13], "temperature": 0.0, "avg_logprob": -0.1348322362315898, "compression_ratio": 1.771186440677966, "no_speech_prob": 4.092887593287742e-06}, {"id": 1406, "seek": 699386, "start": 7007.86, "end": 7013.099999999999, "text": " I do know there's been a couple of recent papers that show the ability to create good", "tokens": [286, 360, 458, 456, 311, 668, 257, 1916, 295, 5162, 10577, 300, 855, 264, 3485, 281, 1884, 665], "temperature": 0.0, "avg_logprob": -0.1348322362315898, "compression_ratio": 1.771186440677966, "no_speech_prob": 4.092887593287742e-06}, {"id": 1407, "seek": 699386, "start": 7013.099999999999, "end": 7018.82, "text": " quality translation models without paired data.", "tokens": [3125, 12853, 5245, 1553, 25699, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1348322362315898, "compression_ratio": 1.771186440677966, "no_speech_prob": 4.092887593287742e-06}, {"id": 1408, "seek": 699386, "start": 7018.82, "end": 7023.099999999999, "text": " I haven't implemented them and I don't understand anything I haven't implemented, but yeah,", "tokens": [286, 2378, 380, 12270, 552, 293, 286, 500, 380, 1223, 1340, 286, 2378, 380, 12270, 11, 457, 1338, 11], "temperature": 0.0, "avg_logprob": -0.1348322362315898, "compression_ratio": 1.771186440677966, "no_speech_prob": 4.092887593287742e-06}, {"id": 1409, "seek": 702310, "start": 7023.1, "end": 7025.900000000001, "text": " they may well be doing the same basic idea.", "tokens": [436, 815, 731, 312, 884, 264, 912, 3875, 1558, 13], "temperature": 0.0, "avg_logprob": -0.16554305922817175, "compression_ratio": 1.4855491329479769, "no_speech_prob": 9.080341442313511e-06}, {"id": 1410, "seek": 702310, "start": 7025.900000000001, "end": 7036.02, "text": " We'll look at it during the week and get back to you.", "tokens": [492, 603, 574, 412, 309, 1830, 264, 1243, 293, 483, 646, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.16554305922817175, "compression_ratio": 1.4855491329479769, "no_speech_prob": 9.080341442313511e-06}, {"id": 1411, "seek": 702310, "start": 7036.02, "end": 7041.5, "text": " So we've got our GAN loss, the next piece is the cycle consistency loss.", "tokens": [407, 321, 600, 658, 527, 460, 1770, 4470, 11, 264, 958, 2522, 307, 264, 6586, 14416, 4470, 13], "temperature": 0.0, "avg_logprob": -0.16554305922817175, "compression_ratio": 1.4855491329479769, "no_speech_prob": 9.080341442313511e-06}, {"id": 1412, "seek": 702310, "start": 7041.5, "end": 7047.860000000001, "text": " And so the basic idea here is that we start with our horse, use our zebra generator on", "tokens": [400, 370, 264, 3875, 1558, 510, 307, 300, 321, 722, 365, 527, 6832, 11, 764, 527, 47060, 19265, 322], "temperature": 0.0, "avg_logprob": -0.16554305922817175, "compression_ratio": 1.4855491329479769, "no_speech_prob": 9.080341442313511e-06}, {"id": 1413, "seek": 704786, "start": 7047.86, "end": 7053.9, "text": " that to create a zebra, use our horse generator on that to create a horse, and then compare", "tokens": [300, 281, 1884, 257, 47060, 11, 764, 527, 6832, 19265, 322, 300, 281, 1884, 257, 6832, 11, 293, 550, 6794], "temperature": 0.0, "avg_logprob": -0.185267820812407, "compression_ratio": 1.7268518518518519, "no_speech_prob": 5.682386017724639e-06}, {"id": 1414, "seek": 704786, "start": 7053.9, "end": 7060.66, "text": " that to the original horse, and this double lines with the 1, we've seen this before,", "tokens": [300, 281, 264, 3380, 6832, 11, 293, 341, 3834, 3876, 365, 264, 502, 11, 321, 600, 1612, 341, 949, 11], "temperature": 0.0, "avg_logprob": -0.185267820812407, "compression_ratio": 1.7268518518518519, "no_speech_prob": 5.682386017724639e-06}, {"id": 1415, "seek": 704786, "start": 7060.66, "end": 7062.66, "text": " this is the L1 loss.", "tokens": [341, 307, 264, 441, 16, 4470, 13], "temperature": 0.0, "avg_logprob": -0.185267820812407, "compression_ratio": 1.7268518518518519, "no_speech_prob": 5.682386017724639e-06}, {"id": 1416, "seek": 704786, "start": 7062.66, "end": 7065.46, "text": " So this is the sum of the absolute value of differences.", "tokens": [407, 341, 307, 264, 2408, 295, 264, 8236, 2158, 295, 7300, 13], "temperature": 0.0, "avg_logprob": -0.185267820812407, "compression_ratio": 1.7268518518518519, "no_speech_prob": 5.682386017724639e-06}, {"id": 1417, "seek": 704786, "start": 7065.46, "end": 7071.82, "text": " Or else if this was a 2, it would be the L2 loss or the 2 norm, which would be the sum", "tokens": [1610, 1646, 498, 341, 390, 257, 568, 11, 309, 576, 312, 264, 441, 17, 4470, 420, 264, 568, 2026, 11, 597, 576, 312, 264, 2408], "temperature": 0.0, "avg_logprob": -0.185267820812407, "compression_ratio": 1.7268518518518519, "no_speech_prob": 5.682386017724639e-06}, {"id": 1418, "seek": 704786, "start": 7071.82, "end": 7077.5, "text": " of square root of it actually.", "tokens": [295, 3732, 5593, 295, 309, 767, 13], "temperature": 0.0, "avg_logprob": -0.185267820812407, "compression_ratio": 1.7268518518518519, "no_speech_prob": 5.682386017724639e-06}, {"id": 1419, "seek": 707750, "start": 7077.5, "end": 7087.34, "text": " And again, we now know this squiggle idea, which is from our horses, grab a horse.", "tokens": [400, 797, 11, 321, 586, 458, 341, 2339, 19694, 1558, 11, 597, 307, 490, 527, 13112, 11, 4444, 257, 6832, 13], "temperature": 0.0, "avg_logprob": -0.13750901623306988, "compression_ratio": 1.7312775330396475, "no_speech_prob": 1.7502748960396275e-05}, {"id": 1420, "seek": 707750, "start": 7087.34, "end": 7090.62, "text": " So this is what we mean by sample from a distribution.", "tokens": [407, 341, 307, 437, 321, 914, 538, 6889, 490, 257, 7316, 13], "temperature": 0.0, "avg_logprob": -0.13750901623306988, "compression_ratio": 1.7312775330396475, "no_speech_prob": 1.7502748960396275e-05}, {"id": 1421, "seek": 707750, "start": 7090.62, "end": 7095.36, "text": " There's all kinds of distributions, but most commonly in these papers, we're using an empirical", "tokens": [821, 311, 439, 3685, 295, 37870, 11, 457, 881, 12719, 294, 613, 10577, 11, 321, 434, 1228, 364, 31886], "temperature": 0.0, "avg_logprob": -0.13750901623306988, "compression_ratio": 1.7312775330396475, "no_speech_prob": 1.7502748960396275e-05}, {"id": 1422, "seek": 707750, "start": 7095.36, "end": 7096.42, "text": " distribution.", "tokens": [7316, 13], "temperature": 0.0, "avg_logprob": -0.13750901623306988, "compression_ratio": 1.7312775330396475, "no_speech_prob": 1.7502748960396275e-05}, {"id": 1423, "seek": 707750, "start": 7096.42, "end": 7100.1, "text": " In other words, we've got some rows of data, grab a row.", "tokens": [682, 661, 2283, 11, 321, 600, 658, 512, 13241, 295, 1412, 11, 4444, 257, 5386, 13], "temperature": 0.0, "avg_logprob": -0.13750901623306988, "compression_ratio": 1.7312775330396475, "no_speech_prob": 1.7502748960396275e-05}, {"id": 1424, "seek": 707750, "start": 7100.1, "end": 7107.34, "text": " So when you see this thing, squiggle, other thing, this thing here, when it says p data,", "tokens": [407, 562, 291, 536, 341, 551, 11, 2339, 19694, 11, 661, 551, 11, 341, 551, 510, 11, 562, 309, 1619, 280, 1412, 11], "temperature": 0.0, "avg_logprob": -0.13750901623306988, "compression_ratio": 1.7312775330396475, "no_speech_prob": 1.7502748960396275e-05}, {"id": 1425, "seek": 710734, "start": 7107.34, "end": 7113.3, "text": " that means grab something from the data, and we're going to call that thing x.", "tokens": [300, 1355, 4444, 746, 490, 264, 1412, 11, 293, 321, 434, 516, 281, 818, 300, 551, 2031, 13], "temperature": 0.0, "avg_logprob": -0.1608752210934957, "compression_ratio": 1.6971153846153846, "no_speech_prob": 1.7880602172226645e-06}, {"id": 1426, "seek": 710734, "start": 7113.3, "end": 7118.54, "text": " So from our horses, pictures, grab a horse, turn it into a zebra, turn it back into a", "tokens": [407, 490, 527, 13112, 11, 5242, 11, 4444, 257, 6832, 11, 1261, 309, 666, 257, 47060, 11, 1261, 309, 646, 666, 257], "temperature": 0.0, "avg_logprob": -0.1608752210934957, "compression_ratio": 1.6971153846153846, "no_speech_prob": 1.7880602172226645e-06}, {"id": 1427, "seek": 710734, "start": 7118.54, "end": 7123.38, "text": " horse, compare it to the original, and sum up the absolute values.", "tokens": [6832, 11, 6794, 309, 281, 264, 3380, 11, 293, 2408, 493, 264, 8236, 4190, 13], "temperature": 0.0, "avg_logprob": -0.1608752210934957, "compression_ratio": 1.6971153846153846, "no_speech_prob": 1.7880602172226645e-06}, {"id": 1428, "seek": 710734, "start": 7123.38, "end": 7128.26, "text": " Do that for horse to zebra, do it for zebra to horse as well, add the 2 together, and", "tokens": [1144, 300, 337, 6832, 281, 47060, 11, 360, 309, 337, 47060, 281, 6832, 382, 731, 11, 909, 264, 568, 1214, 11, 293], "temperature": 0.0, "avg_logprob": -0.1608752210934957, "compression_ratio": 1.6971153846153846, "no_speech_prob": 1.7880602172226645e-06}, {"id": 1429, "seek": 710734, "start": 7128.26, "end": 7136.46, "text": " that is our cycle consistency loss.", "tokens": [300, 307, 527, 6586, 14416, 4470, 13], "temperature": 0.0, "avg_logprob": -0.1608752210934957, "compression_ratio": 1.6971153846153846, "no_speech_prob": 1.7880602172226645e-06}, {"id": 1430, "seek": 713646, "start": 7136.46, "end": 7143.54, "text": " So now we get our loss function, and the whole loss function depends on our horse generator,", "tokens": [407, 586, 321, 483, 527, 4470, 2445, 11, 293, 264, 1379, 4470, 2445, 5946, 322, 527, 6832, 19265, 11], "temperature": 0.0, "avg_logprob": -0.10564025444320485, "compression_ratio": 1.9636363636363636, "no_speech_prob": 5.122893185216526e-07}, {"id": 1431, "seek": 713646, "start": 7143.54, "end": 7149.38, "text": " our zebra generator, our horse recognizer, our zebra recognizer discriminator, and we're", "tokens": [527, 47060, 19265, 11, 527, 6832, 3068, 6545, 11, 527, 47060, 3068, 6545, 20828, 1639, 11, 293, 321, 434], "temperature": 0.0, "avg_logprob": -0.10564025444320485, "compression_ratio": 1.9636363636363636, "no_speech_prob": 5.122893185216526e-07}, {"id": 1432, "seek": 713646, "start": 7149.38, "end": 7158.06, "text": " going to add up the GAN loss for recognizing horses, the GAN loss for recognizing zebras,", "tokens": [516, 281, 909, 493, 264, 460, 1770, 4470, 337, 18538, 13112, 11, 264, 460, 1770, 4470, 337, 18538, 5277, 38182, 11], "temperature": 0.0, "avg_logprob": -0.10564025444320485, "compression_ratio": 1.9636363636363636, "no_speech_prob": 5.122893185216526e-07}, {"id": 1433, "seek": 713646, "start": 7158.06, "end": 7163.66, "text": " and the cycle consistency loss for our 2 generators.", "tokens": [293, 264, 6586, 14416, 4470, 337, 527, 568, 38662, 13], "temperature": 0.0, "avg_logprob": -0.10564025444320485, "compression_ratio": 1.9636363636363636, "no_speech_prob": 5.122893185216526e-07}, {"id": 1434, "seek": 716366, "start": 7163.66, "end": 7167.66, "text": " And then we've got a lambda here, which hopefully we're kind of used to this idea now, that", "tokens": [400, 550, 321, 600, 658, 257, 13607, 510, 11, 597, 4696, 321, 434, 733, 295, 1143, 281, 341, 1558, 586, 11, 300], "temperature": 0.0, "avg_logprob": -0.13829881018334692, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.4510400433209725e-05}, {"id": 1435, "seek": 716366, "start": 7167.66, "end": 7172.78, "text": " is when you've got 2 different kinds of loss, you chuck in a parameter there that you can", "tokens": [307, 562, 291, 600, 658, 568, 819, 3685, 295, 4470, 11, 291, 20870, 294, 257, 13075, 456, 300, 291, 393], "temperature": 0.0, "avg_logprob": -0.13829881018334692, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.4510400433209725e-05}, {"id": 1436, "seek": 716366, "start": 7172.78, "end": 7176.38, "text": " multiply them by so that they're about the same scale.", "tokens": [12972, 552, 538, 370, 300, 436, 434, 466, 264, 912, 4373, 13], "temperature": 0.0, "avg_logprob": -0.13829881018334692, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.4510400433209725e-05}, {"id": 1437, "seek": 716366, "start": 7176.38, "end": 7183.42, "text": " We did a similar thing with our bounding box loss compared to our classifier loss when", "tokens": [492, 630, 257, 2531, 551, 365, 527, 5472, 278, 2424, 4470, 5347, 281, 527, 1508, 9902, 4470, 562], "temperature": 0.0, "avg_logprob": -0.13829881018334692, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.4510400433209725e-05}, {"id": 1438, "seek": 716366, "start": 7183.42, "end": 7189.22, "text": " we did that localization stuff.", "tokens": [321, 630, 300, 2654, 2144, 1507, 13], "temperature": 0.0, "avg_logprob": -0.13829881018334692, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.4510400433209725e-05}, {"id": 1439, "seek": 718922, "start": 7189.22, "end": 7196.18, "text": " So then we're going to try to, for this loss function, maximize the capability of the discriminators", "tokens": [407, 550, 321, 434, 516, 281, 853, 281, 11, 337, 341, 4470, 2445, 11, 19874, 264, 13759, 295, 264, 20828, 3391], "temperature": 0.0, "avg_logprob": -0.11117665561628931, "compression_ratio": 1.761658031088083, "no_speech_prob": 1.287893269363849e-06}, {"id": 1440, "seek": 718922, "start": 7196.18, "end": 7203.46, "text": " at discriminating whilst minimizing that for the generators.", "tokens": [412, 20828, 990, 18534, 46608, 300, 337, 264, 38662, 13], "temperature": 0.0, "avg_logprob": -0.11117665561628931, "compression_ratio": 1.761658031088083, "no_speech_prob": 1.287893269363849e-06}, {"id": 1441, "seek": 718922, "start": 7203.46, "end": 7208.9400000000005, "text": " So the generators and the discriminators are going to be facing off against each other.", "tokens": [407, 264, 38662, 293, 264, 20828, 3391, 366, 516, 281, 312, 7170, 766, 1970, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.11117665561628931, "compression_ratio": 1.761658031088083, "no_speech_prob": 1.287893269363849e-06}, {"id": 1442, "seek": 718922, "start": 7208.9400000000005, "end": 7215.58, "text": " So when you see this min-max thing in papers, you'll see it a lot, it basically means this", "tokens": [407, 562, 291, 536, 341, 923, 12, 41167, 551, 294, 10577, 11, 291, 603, 536, 309, 257, 688, 11, 309, 1936, 1355, 341], "temperature": 0.0, "avg_logprob": -0.11117665561628931, "compression_ratio": 1.761658031088083, "no_speech_prob": 1.287893269363849e-06}, {"id": 1443, "seek": 721558, "start": 7215.58, "end": 7220.22, "text": " idea that in your training loop, one thing is trying to make something better, the other", "tokens": [1558, 300, 294, 428, 3097, 6367, 11, 472, 551, 307, 1382, 281, 652, 746, 1101, 11, 264, 661], "temperature": 0.0, "avg_logprob": -0.15840766009162455, "compression_ratio": 1.6973684210526316, "no_speech_prob": 6.439008757297415e-06}, {"id": 1444, "seek": 721558, "start": 7220.22, "end": 7224.74, "text": " is trying to make something worse, and there's lots of ways to do it, but most commonly you", "tokens": [307, 1382, 281, 652, 746, 5324, 11, 293, 456, 311, 3195, 295, 2098, 281, 360, 309, 11, 457, 881, 12719, 291], "temperature": 0.0, "avg_logprob": -0.15840766009162455, "compression_ratio": 1.6973684210526316, "no_speech_prob": 6.439008757297415e-06}, {"id": 1445, "seek": 721558, "start": 7224.74, "end": 7227.22, "text": " alternate between the two.", "tokens": [18873, 1296, 264, 732, 13], "temperature": 0.0, "avg_logprob": -0.15840766009162455, "compression_ratio": 1.6973684210526316, "no_speech_prob": 6.439008757297415e-06}, {"id": 1446, "seek": 721558, "start": 7227.22, "end": 7230.82, "text": " And you'll often see this just referred to in math papers as min-max.", "tokens": [400, 291, 603, 2049, 536, 341, 445, 10839, 281, 294, 5221, 10577, 382, 923, 12, 41167, 13], "temperature": 0.0, "avg_logprob": -0.15840766009162455, "compression_ratio": 1.6973684210526316, "no_speech_prob": 6.439008757297415e-06}, {"id": 1447, "seek": 721558, "start": 7230.82, "end": 7242.36, "text": " So when you see min-max, you should immediately think, okay, adversarial training.", "tokens": [407, 562, 291, 536, 923, 12, 41167, 11, 291, 820, 4258, 519, 11, 1392, 11, 17641, 44745, 3097, 13], "temperature": 0.0, "avg_logprob": -0.15840766009162455, "compression_ratio": 1.6973684210526316, "no_speech_prob": 6.439008757297415e-06}, {"id": 1448, "seek": 721558, "start": 7242.36, "end": 7244.22, "text": " So let's look at the code.", "tokens": [407, 718, 311, 574, 412, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.15840766009162455, "compression_ratio": 1.6973684210526316, "no_speech_prob": 6.439008757297415e-06}, {"id": 1449, "seek": 724422, "start": 7244.22, "end": 7249.06, "text": " And we're only going to, we probably won't be able to finish this today, but we're going", "tokens": [400, 321, 434, 787, 516, 281, 11, 321, 1391, 1582, 380, 312, 1075, 281, 2413, 341, 965, 11, 457, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.11936518966510731, "compression_ratio": 1.6046511627906976, "no_speech_prob": 2.8856824428658e-05}, {"id": 1450, "seek": 724422, "start": 7249.06, "end": 7256.7, "text": " to do something almost unheard of, which is I started looking at somebody else's code,", "tokens": [281, 360, 746, 1920, 517, 42915, 295, 11, 597, 307, 286, 1409, 1237, 412, 2618, 1646, 311, 3089, 11], "temperature": 0.0, "avg_logprob": -0.11936518966510731, "compression_ratio": 1.6046511627906976, "no_speech_prob": 2.8856824428658e-05}, {"id": 1451, "seek": 724422, "start": 7256.7, "end": 7260.7, "text": " and I was not so disgusted that I threw the whole thing away and did it myself.", "tokens": [293, 286, 390, 406, 370, 14116, 6589, 300, 286, 11918, 264, 1379, 551, 1314, 293, 630, 309, 2059, 13], "temperature": 0.0, "avg_logprob": -0.11936518966510731, "compression_ratio": 1.6046511627906976, "no_speech_prob": 2.8856824428658e-05}, {"id": 1452, "seek": 724422, "start": 7260.7, "end": 7267.18, "text": " I actually said, I quite like this, I like it enough I'm going to show it to my students.", "tokens": [286, 767, 848, 11, 286, 1596, 411, 341, 11, 286, 411, 309, 1547, 286, 478, 516, 281, 855, 309, 281, 452, 1731, 13], "temperature": 0.0, "avg_logprob": -0.11936518966510731, "compression_ratio": 1.6046511627906976, "no_speech_prob": 2.8856824428658e-05}, {"id": 1453, "seek": 726718, "start": 7267.18, "end": 7277.38, "text": " So this is where the code comes from, so this is one of the people that created the original", "tokens": [407, 341, 307, 689, 264, 3089, 1487, 490, 11, 370, 341, 307, 472, 295, 264, 561, 300, 2942, 264, 3380], "temperature": 0.0, "avg_logprob": -0.15697662646953875, "compression_ratio": 1.4037267080745341, "no_speech_prob": 5.014697308070026e-06}, {"id": 1454, "seek": 726718, "start": 7277.38, "end": 7285.38, "text": " code for CycleGANs, and they've created a PyTorch version.", "tokens": [3089, 337, 10295, 2160, 27699, 82, 11, 293, 436, 600, 2942, 257, 9953, 51, 284, 339, 3037, 13], "temperature": 0.0, "avg_logprob": -0.15697662646953875, "compression_ratio": 1.4037267080745341, "no_speech_prob": 5.014697308070026e-06}, {"id": 1455, "seek": 726718, "start": 7285.38, "end": 7293.900000000001, "text": " And I had to clean it up a little bit, but it's actually pretty damn good.", "tokens": [400, 286, 632, 281, 2541, 309, 493, 257, 707, 857, 11, 457, 309, 311, 767, 1238, 8151, 665, 13], "temperature": 0.0, "avg_logprob": -0.15697662646953875, "compression_ratio": 1.4037267080745341, "no_speech_prob": 5.014697308070026e-06}, {"id": 1456, "seek": 729390, "start": 7293.9, "end": 7300.58, "text": " I think the first time I found code that I didn't feel the need to rewrite from scratch", "tokens": [286, 519, 264, 700, 565, 286, 1352, 3089, 300, 286, 994, 380, 841, 264, 643, 281, 28132, 490, 8459], "temperature": 0.0, "avg_logprob": -0.1504686428950383, "compression_ratio": 1.6991525423728813, "no_speech_prob": 4.0294366954185534e-06}, {"id": 1457, "seek": 729390, "start": 7300.58, "end": 7304.379999999999, "text": " before I showed it to you.", "tokens": [949, 286, 4712, 309, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.1504686428950383, "compression_ratio": 1.6991525423728813, "no_speech_prob": 4.0294366954185534e-06}, {"id": 1458, "seek": 729390, "start": 7304.379999999999, "end": 7309.179999999999, "text": " And so the cool thing about this is one of the reasons I like doing it this way, like", "tokens": [400, 370, 264, 1627, 551, 466, 341, 307, 472, 295, 264, 4112, 286, 411, 884, 309, 341, 636, 11, 411], "temperature": 0.0, "avg_logprob": -0.1504686428950383, "compression_ratio": 1.6991525423728813, "no_speech_prob": 4.0294366954185534e-06}, {"id": 1459, "seek": 729390, "start": 7309.179999999999, "end": 7315.98, "text": " finally finding something that's not awful, is that you're now going to get to see almost", "tokens": [2721, 5006, 746, 300, 311, 406, 11232, 11, 307, 300, 291, 434, 586, 516, 281, 483, 281, 536, 1920], "temperature": 0.0, "avg_logprob": -0.1504686428950383, "compression_ratio": 1.6991525423728813, "no_speech_prob": 4.0294366954185534e-06}, {"id": 1460, "seek": 729390, "start": 7315.98, "end": 7321.0199999999995, "text": " all the bits of fast AI, or like all the relevant bits of fast AI written in a different way", "tokens": [439, 264, 9239, 295, 2370, 7318, 11, 420, 411, 439, 264, 7340, 9239, 295, 2370, 7318, 3720, 294, 257, 819, 636], "temperature": 0.0, "avg_logprob": -0.1504686428950383, "compression_ratio": 1.6991525423728813, "no_speech_prob": 4.0294366954185534e-06}, {"id": 1461, "seek": 729390, "start": 7321.0199999999995, "end": 7322.7, "text": " by somebody else.", "tokens": [538, 2618, 1646, 13], "temperature": 0.0, "avg_logprob": -0.1504686428950383, "compression_ratio": 1.6991525423728813, "no_speech_prob": 4.0294366954185534e-06}, {"id": 1462, "seek": 732270, "start": 7322.7, "end": 7327.0199999999995, "text": " And so you're going to get to see like, oh how they do datasets and data loaders and", "tokens": [400, 370, 291, 434, 516, 281, 483, 281, 536, 411, 11, 1954, 577, 436, 360, 42856, 293, 1412, 3677, 433, 293], "temperature": 0.0, "avg_logprob": -0.17075374852056088, "compression_ratio": 1.5442477876106195, "no_speech_prob": 6.144117378426017e-06}, {"id": 1463, "seek": 732270, "start": 7327.0199999999995, "end": 7331.46, "text": " models and training loops and so forth.", "tokens": [5245, 293, 3097, 16121, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.17075374852056088, "compression_ratio": 1.5442477876106195, "no_speech_prob": 6.144117378426017e-06}, {"id": 1464, "seek": 732270, "start": 7331.46, "end": 7342.42, "text": " So you'll find there's a CycleGAN directory, which is basically nearly this, with some", "tokens": [407, 291, 603, 915, 456, 311, 257, 10295, 2160, 27699, 21120, 11, 597, 307, 1936, 6217, 341, 11, 365, 512], "temperature": 0.0, "avg_logprob": -0.17075374852056088, "compression_ratio": 1.5442477876106195, "no_speech_prob": 6.144117378426017e-06}, {"id": 1465, "seek": 732270, "start": 7342.42, "end": 7346.78, "text": " cleanups which I hope to submit as a PR sometime.", "tokens": [2541, 7528, 597, 286, 1454, 281, 10315, 382, 257, 11568, 15053, 13], "temperature": 0.0, "avg_logprob": -0.17075374852056088, "compression_ratio": 1.5442477876106195, "no_speech_prob": 6.144117378426017e-06}, {"id": 1466, "seek": 732270, "start": 7346.78, "end": 7350.3, "text": " It was written in a way that unfortunately made it a bit overconnected to how they were", "tokens": [467, 390, 3720, 294, 257, 636, 300, 7015, 1027, 309, 257, 857, 670, 9826, 292, 281, 577, 436, 645], "temperature": 0.0, "avg_logprob": -0.17075374852056088, "compression_ratio": 1.5442477876106195, "no_speech_prob": 6.144117378426017e-06}, {"id": 1467, "seek": 735030, "start": 7350.3, "end": 7354.22, "text": " using it as a script, so I cleaned it up a little bit so I could use it as a module.", "tokens": [1228, 309, 382, 257, 5755, 11, 370, 286, 16146, 309, 493, 257, 707, 857, 370, 286, 727, 764, 309, 382, 257, 10088, 13], "temperature": 0.0, "avg_logprob": -0.14469217484997166, "compression_ratio": 1.5745614035087718, "no_speech_prob": 7.646416634088382e-06}, {"id": 1468, "seek": 735030, "start": 7354.22, "end": 7357.14, "text": " But other than that, it's pretty similar.", "tokens": [583, 661, 813, 300, 11, 309, 311, 1238, 2531, 13], "temperature": 0.0, "avg_logprob": -0.14469217484997166, "compression_ratio": 1.5745614035087718, "no_speech_prob": 7.646416634088382e-06}, {"id": 1469, "seek": 735030, "start": 7357.14, "end": 7368.320000000001, "text": " So CycleGAN is basically their code, copied from their GitHub repo with some minor changes.", "tokens": [407, 10295, 2160, 27699, 307, 1936, 641, 3089, 11, 25365, 490, 641, 23331, 49040, 365, 512, 6696, 2962, 13], "temperature": 0.0, "avg_logprob": -0.14469217484997166, "compression_ratio": 1.5745614035087718, "no_speech_prob": 7.646416634088382e-06}, {"id": 1470, "seek": 735030, "start": 7368.320000000001, "end": 7375.9400000000005, "text": " So the way the CycleGAN mini library has been set up is that the configuration options they're", "tokens": [407, 264, 636, 264, 10295, 2160, 27699, 8382, 6405, 575, 668, 992, 493, 307, 300, 264, 11694, 3956, 436, 434], "temperature": 0.0, "avg_logprob": -0.14469217484997166, "compression_ratio": 1.5745614035087718, "no_speech_prob": 7.646416634088382e-06}, {"id": 1471, "seek": 735030, "start": 7375.9400000000005, "end": 7379.400000000001, "text": " assuming are being passed into like a script.", "tokens": [11926, 366, 885, 4678, 666, 411, 257, 5755, 13], "temperature": 0.0, "avg_logprob": -0.14469217484997166, "compression_ratio": 1.5745614035087718, "no_speech_prob": 7.646416634088382e-06}, {"id": 1472, "seek": 737940, "start": 7379.4, "end": 7386.7, "text": " So they've got this trainOptionsParser method, and so you can see I'm basically passing in", "tokens": [407, 436, 600, 658, 341, 3847, 46, 9799, 47, 685, 260, 3170, 11, 293, 370, 291, 393, 536, 286, 478, 1936, 8437, 294], "temperature": 0.0, "avg_logprob": -0.14067550758262734, "compression_ratio": 1.4777777777777779, "no_speech_prob": 8.800973773759324e-06}, {"id": 1473, "seek": 737940, "start": 7386.7, "end": 7391.219999999999, "text": " an array of script options.", "tokens": [364, 10225, 295, 5755, 3956, 13], "temperature": 0.0, "avg_logprob": -0.14067550758262734, "compression_ratio": 1.4777777777777779, "no_speech_prob": 8.800973773759324e-06}, {"id": 1474, "seek": 737940, "start": 7391.219999999999, "end": 7397.179999999999, "text": " Where's my data, how many threads do I want to drop out, how many iterations, what am", "tokens": [2305, 311, 452, 1412, 11, 577, 867, 19314, 360, 286, 528, 281, 3270, 484, 11, 577, 867, 36540, 11, 437, 669], "temperature": 0.0, "avg_logprob": -0.14067550758262734, "compression_ratio": 1.4777777777777779, "no_speech_prob": 8.800973773759324e-06}, {"id": 1475, "seek": 737940, "start": 7397.179999999999, "end": 7402.299999999999, "text": " I going to call this model, which GPU do I want to run it on.", "tokens": [286, 516, 281, 818, 341, 2316, 11, 597, 18407, 360, 286, 528, 281, 1190, 309, 322, 13], "temperature": 0.0, "avg_logprob": -0.14067550758262734, "compression_ratio": 1.4777777777777779, "no_speech_prob": 8.800973773759324e-06}, {"id": 1476, "seek": 740230, "start": 7402.3, "end": 7411.7, "text": " So that gives me an opt object, which you can then see what it contains.", "tokens": [407, 300, 2709, 385, 364, 2427, 2657, 11, 597, 291, 393, 550, 536, 437, 309, 8306, 13], "temperature": 0.0, "avg_logprob": -0.17468870162963868, "compression_ratio": 1.6839622641509433, "no_speech_prob": 2.368787363593583e-06}, {"id": 1477, "seek": 740230, "start": 7411.7, "end": 7415.78, "text": " You'll see it contains some things I didn't mention, and that's because it's got defaults", "tokens": [509, 603, 536, 309, 8306, 512, 721, 286, 994, 380, 2152, 11, 293, 300, 311, 570, 309, 311, 658, 7576, 82], "temperature": 0.0, "avg_logprob": -0.17468870162963868, "compression_ratio": 1.6839622641509433, "no_speech_prob": 2.368787363593583e-06}, {"id": 1478, "seek": 740230, "start": 7415.78, "end": 7419.58, "text": " for everything else that I didn't mention.", "tokens": [337, 1203, 1646, 300, 286, 994, 380, 2152, 13], "temperature": 0.0, "avg_logprob": -0.17468870162963868, "compression_ratio": 1.6839622641509433, "no_speech_prob": 2.368787363593583e-06}, {"id": 1479, "seek": 740230, "start": 7419.58, "end": 7425.34, "text": " So we're going to, rather than using fastai stuff, we're going to actually use CycleGAN", "tokens": [407, 321, 434, 516, 281, 11, 2831, 813, 1228, 2370, 1301, 1507, 11, 321, 434, 516, 281, 767, 764, 10295, 2160, 27699], "temperature": 0.0, "avg_logprob": -0.17468870162963868, "compression_ratio": 1.6839622641509433, "no_speech_prob": 2.368787363593583e-06}, {"id": 1480, "seek": 740230, "start": 7425.34, "end": 7426.34, "text": " stuff.", "tokens": [1507, 13], "temperature": 0.0, "avg_logprob": -0.17468870162963868, "compression_ratio": 1.6839622641509433, "no_speech_prob": 2.368787363593583e-06}, {"id": 1481, "seek": 740230, "start": 7426.34, "end": 7430.24, "text": " So the first thing we're going to need is a data loader.", "tokens": [407, 264, 700, 551, 321, 434, 516, 281, 643, 307, 257, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.17468870162963868, "compression_ratio": 1.6839622641509433, "no_speech_prob": 2.368787363593583e-06}, {"id": 1482, "seek": 743024, "start": 7430.24, "end": 7435.62, "text": " And so this is also a great opportunity for you again to practice your ability to navigate", "tokens": [400, 370, 341, 307, 611, 257, 869, 2650, 337, 291, 797, 281, 3124, 428, 3485, 281, 12350], "temperature": 0.0, "avg_logprob": -0.1504413922627767, "compression_ratio": 1.625, "no_speech_prob": 3.0415817491302732e-06}, {"id": 1483, "seek": 743024, "start": 7435.62, "end": 7441.0599999999995, "text": " through code with your editor or IDE of choice.", "tokens": [807, 3089, 365, 428, 9839, 420, 40930, 295, 3922, 13], "temperature": 0.0, "avg_logprob": -0.1504413922627767, "compression_ratio": 1.625, "no_speech_prob": 3.0415817491302732e-06}, {"id": 1484, "seek": 743024, "start": 7441.0599999999995, "end": 7443.82, "text": " So we're going to start with create data loader.", "tokens": [407, 321, 434, 516, 281, 722, 365, 1884, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.1504413922627767, "compression_ratio": 1.625, "no_speech_prob": 3.0415817491302732e-06}, {"id": 1485, "seek": 743024, "start": 7443.82, "end": 7450.219999999999, "text": " So you should be able to go find symbol or in vim tag to jump straight to create data", "tokens": [407, 291, 820, 312, 1075, 281, 352, 915, 5986, 420, 294, 371, 332, 6162, 281, 3012, 2997, 281, 1884, 1412], "temperature": 0.0, "avg_logprob": -0.1504413922627767, "compression_ratio": 1.625, "no_speech_prob": 3.0415817491302732e-06}, {"id": 1486, "seek": 743024, "start": 7450.219999999999, "end": 7451.8, "text": " loader.", "tokens": [3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.1504413922627767, "compression_ratio": 1.625, "no_speech_prob": 3.0415817491302732e-06}, {"id": 1487, "seek": 743024, "start": 7451.8, "end": 7456.219999999999, "text": " And we can see that's creating a custom data set loader.", "tokens": [400, 321, 393, 536, 300, 311, 4084, 257, 2375, 1412, 992, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.1504413922627767, "compression_ratio": 1.625, "no_speech_prob": 3.0415817491302732e-06}, {"id": 1488, "seek": 745622, "start": 7456.22, "end": 7462.1, "text": " And then we can see custom data set loader is a base data loader.", "tokens": [400, 550, 321, 393, 536, 2375, 1412, 992, 3677, 260, 307, 257, 3096, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.11348816386440344, "compression_ratio": 1.899497487437186, "no_speech_prob": 4.157327111897757e-06}, {"id": 1489, "seek": 745622, "start": 7462.1, "end": 7465.7, "text": " So that doesn't really do anything.", "tokens": [407, 300, 1177, 380, 534, 360, 1340, 13], "temperature": 0.0, "avg_logprob": -0.11348816386440344, "compression_ratio": 1.899497487437186, "no_speech_prob": 4.157327111897757e-06}, {"id": 1490, "seek": 745622, "start": 7465.7, "end": 7471.16, "text": " So basically we can see that it's going to use a standard PyTorch data loader.", "tokens": [407, 1936, 321, 393, 536, 300, 309, 311, 516, 281, 764, 257, 3832, 9953, 51, 284, 339, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.11348816386440344, "compression_ratio": 1.899497487437186, "no_speech_prob": 4.157327111897757e-06}, {"id": 1491, "seek": 745622, "start": 7471.16, "end": 7472.360000000001, "text": " So that's good.", "tokens": [407, 300, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.11348816386440344, "compression_ratio": 1.899497487437186, "no_speech_prob": 4.157327111897757e-06}, {"id": 1492, "seek": 745622, "start": 7472.360000000001, "end": 7476.58, "text": " And so we know if you're going to use a standard PyTorch data loader, you have to pass it a", "tokens": [400, 370, 321, 458, 498, 291, 434, 516, 281, 764, 257, 3832, 9953, 51, 284, 339, 1412, 3677, 260, 11, 291, 362, 281, 1320, 309, 257], "temperature": 0.0, "avg_logprob": -0.11348816386440344, "compression_ratio": 1.899497487437186, "no_speech_prob": 4.157327111897757e-06}, {"id": 1493, "seek": 745622, "start": 7476.58, "end": 7477.58, "text": " data set.", "tokens": [1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.11348816386440344, "compression_ratio": 1.899497487437186, "no_speech_prob": 4.157327111897757e-06}, {"id": 1494, "seek": 745622, "start": 7477.58, "end": 7483.92, "text": " And we know that a data set is something that contains a length and an indexer.", "tokens": [400, 321, 458, 300, 257, 1412, 992, 307, 746, 300, 8306, 257, 4641, 293, 364, 8186, 260, 13], "temperature": 0.0, "avg_logprob": -0.11348816386440344, "compression_ratio": 1.899497487437186, "no_speech_prob": 4.157327111897757e-06}, {"id": 1495, "seek": 748392, "start": 7483.92, "end": 7487.7, "text": " So presumably when we look at create data set, it's going to do that.", "tokens": [407, 26742, 562, 321, 574, 412, 1884, 1412, 992, 11, 309, 311, 516, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.17054215599508846, "compression_ratio": 1.6238095238095238, "no_speech_prob": 9.721528613226837e-07}, {"id": 1496, "seek": 748392, "start": 7487.7, "end": 7490.02, "text": " Here is create data set.", "tokens": [1692, 307, 1884, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.17054215599508846, "compression_ratio": 1.6238095238095238, "no_speech_prob": 9.721528613226837e-07}, {"id": 1497, "seek": 748392, "start": 7490.02, "end": 7493.42, "text": " So this library actually does more than just cycleGAN.", "tokens": [407, 341, 6405, 767, 775, 544, 813, 445, 6586, 27699, 13], "temperature": 0.0, "avg_logprob": -0.17054215599508846, "compression_ratio": 1.6238095238095238, "no_speech_prob": 9.721528613226837e-07}, {"id": 1498, "seek": 748392, "start": 7493.42, "end": 7497.06, "text": " It handles both aligned and unaligned image pairs.", "tokens": [467, 18722, 1293, 17962, 293, 517, 304, 16690, 3256, 15494, 13], "temperature": 0.0, "avg_logprob": -0.17054215599508846, "compression_ratio": 1.6238095238095238, "no_speech_prob": 9.721528613226837e-07}, {"id": 1499, "seek": 748392, "start": 7497.06, "end": 7500.54, "text": " We know that our image pairs are unaligned.", "tokens": [492, 458, 300, 527, 3256, 15494, 366, 517, 304, 16690, 13], "temperature": 0.0, "avg_logprob": -0.17054215599508846, "compression_ratio": 1.6238095238095238, "no_speech_prob": 9.721528613226837e-07}, {"id": 1500, "seek": 748392, "start": 7500.54, "end": 7502.9400000000005, "text": " So we've got an unaligned data set.", "tokens": [407, 321, 600, 658, 364, 517, 304, 16690, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.17054215599508846, "compression_ratio": 1.6238095238095238, "no_speech_prob": 9.721528613226837e-07}, {"id": 1501, "seek": 748392, "start": 7502.9400000000005, "end": 7503.9400000000005, "text": " Here it is.", "tokens": [1692, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.17054215599508846, "compression_ratio": 1.6238095238095238, "no_speech_prob": 9.721528613226837e-07}, {"id": 1502, "seek": 748392, "start": 7503.9400000000005, "end": 7511.46, "text": " And as expected, it has a get item and a length.", "tokens": [400, 382, 5176, 11, 309, 575, 257, 483, 3174, 293, 257, 4641, 13], "temperature": 0.0, "avg_logprob": -0.17054215599508846, "compression_ratio": 1.6238095238095238, "no_speech_prob": 9.721528613226837e-07}, {"id": 1503, "seek": 751146, "start": 7511.46, "end": 7519.62, "text": " And so the length is just whatever of our... so A and B is our horses and zebras.", "tokens": [400, 370, 264, 4641, 307, 445, 2035, 295, 527, 1097, 370, 316, 293, 363, 307, 527, 13112, 293, 5277, 38182, 13], "temperature": 0.0, "avg_logprob": -0.22930493305638894, "compression_ratio": 1.6, "no_speech_prob": 1.994722197196097e-06}, {"id": 1504, "seek": 751146, "start": 7519.62, "end": 7521.86, "text": " We've got two sets.", "tokens": [492, 600, 658, 732, 6352, 13], "temperature": 0.0, "avg_logprob": -0.22930493305638894, "compression_ratio": 1.6, "no_speech_prob": 1.994722197196097e-06}, {"id": 1505, "seek": 751146, "start": 7521.86, "end": 7524.96, "text": " So whichever one is longer is the length of the data loader.", "tokens": [407, 24123, 472, 307, 2854, 307, 264, 4641, 295, 264, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.22930493305638894, "compression_ratio": 1.6, "no_speech_prob": 1.994722197196097e-06}, {"id": 1506, "seek": 751146, "start": 7524.96, "end": 7533.26, "text": " And so get item is just going to go ahead and randomly grab something from each of our", "tokens": [400, 370, 483, 3174, 307, 445, 516, 281, 352, 2286, 293, 16979, 4444, 746, 490, 1184, 295, 527], "temperature": 0.0, "avg_logprob": -0.22930493305638894, "compression_ratio": 1.6, "no_speech_prob": 1.994722197196097e-06}, {"id": 1507, "seek": 751146, "start": 7533.26, "end": 7541.3, "text": " two horses and zebras, open them up with PIL, or P-I-L, run them through some transformations.", "tokens": [732, 13112, 293, 5277, 38182, 11, 1269, 552, 493, 365, 430, 4620, 11, 420, 430, 12, 40, 12, 43, 11, 1190, 552, 807, 512, 34852, 13], "temperature": 0.0, "avg_logprob": -0.22930493305638894, "compression_ratio": 1.6, "no_speech_prob": 1.994722197196097e-06}, {"id": 1508, "seek": 754130, "start": 7541.3, "end": 7546.14, "text": " And then we could either be turning horses into zebras or zebras into horses, so there's", "tokens": [400, 550, 321, 727, 2139, 312, 6246, 13112, 666, 5277, 38182, 420, 5277, 38182, 666, 13112, 11, 370, 456, 311], "temperature": 0.0, "avg_logprob": -0.19251771099799503, "compression_ratio": 1.8132780082987552, "no_speech_prob": 4.7108851504162885e-06}, {"id": 1509, "seek": 754130, "start": 7546.14, "end": 7548.18, "text": " some direction.", "tokens": [512, 3513, 13], "temperature": 0.0, "avg_logprob": -0.19251771099799503, "compression_ratio": 1.8132780082987552, "no_speech_prob": 4.7108851504162885e-06}, {"id": 1510, "seek": 754130, "start": 7548.18, "end": 7553.62, "text": " And then it will just go ahead and return our horse and our zebra and our path to the", "tokens": [400, 550, 309, 486, 445, 352, 2286, 293, 2736, 527, 6832, 293, 527, 47060, 293, 527, 3100, 281, 264], "temperature": 0.0, "avg_logprob": -0.19251771099799503, "compression_ratio": 1.8132780082987552, "no_speech_prob": 4.7108851504162885e-06}, {"id": 1511, "seek": 754130, "start": 7553.62, "end": 7555.3, "text": " horse and the path to the zebra.", "tokens": [6832, 293, 264, 3100, 281, 264, 47060, 13], "temperature": 0.0, "avg_logprob": -0.19251771099799503, "compression_ratio": 1.8132780082987552, "no_speech_prob": 4.7108851504162885e-06}, {"id": 1512, "seek": 754130, "start": 7555.3, "end": 7560.9800000000005, "text": " So hopefully you can kind of see that this is looking pretty similar to the kind of stuff", "tokens": [407, 4696, 291, 393, 733, 295, 536, 300, 341, 307, 1237, 1238, 2531, 281, 264, 733, 295, 1507], "temperature": 0.0, "avg_logprob": -0.19251771099799503, "compression_ratio": 1.8132780082987552, "no_speech_prob": 4.7108851504162885e-06}, {"id": 1513, "seek": 754130, "start": 7560.9800000000005, "end": 7562.9800000000005, "text": " that Fast.ai does.", "tokens": [300, 15968, 13, 1301, 775, 13], "temperature": 0.0, "avg_logprob": -0.19251771099799503, "compression_ratio": 1.8132780082987552, "no_speech_prob": 4.7108851504162885e-06}, {"id": 1514, "seek": 754130, "start": 7562.9800000000005, "end": 7568.900000000001, "text": " Fast.ai obviously does quite a lot more when it comes to transforms and performance and", "tokens": [15968, 13, 1301, 2745, 775, 1596, 257, 688, 544, 562, 309, 1487, 281, 35592, 293, 3389, 293], "temperature": 0.0, "avg_logprob": -0.19251771099799503, "compression_ratio": 1.8132780082987552, "no_speech_prob": 4.7108851504162885e-06}, {"id": 1515, "seek": 754130, "start": 7568.900000000001, "end": 7569.900000000001, "text": " stuff like this.", "tokens": [1507, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.19251771099799503, "compression_ratio": 1.8132780082987552, "no_speech_prob": 4.7108851504162885e-06}, {"id": 1516, "seek": 756990, "start": 7569.9, "end": 7574.0199999999995, "text": " But you know, remember this is like research code for this one thing.", "tokens": [583, 291, 458, 11, 1604, 341, 307, 411, 2132, 3089, 337, 341, 472, 551, 13], "temperature": 0.0, "avg_logprob": -0.14919018745422363, "compression_ratio": 1.6118721461187215, "no_speech_prob": 3.6119699871051125e-06}, {"id": 1517, "seek": 756990, "start": 7574.0199999999995, "end": 7577.98, "text": " Like it's pretty cool that they did all this work.", "tokens": [1743, 309, 311, 1238, 1627, 300, 436, 630, 439, 341, 589, 13], "temperature": 0.0, "avg_logprob": -0.14919018745422363, "compression_ratio": 1.6118721461187215, "no_speech_prob": 3.6119699871051125e-06}, {"id": 1518, "seek": 756990, "start": 7577.98, "end": 7580.32, "text": " So we've got a data loader.", "tokens": [407, 321, 600, 658, 257, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.14919018745422363, "compression_ratio": 1.6118721461187215, "no_speech_prob": 3.6119699871051125e-06}, {"id": 1519, "seek": 756990, "start": 7580.32, "end": 7584.44, "text": " So we can go and load our data into it.", "tokens": [407, 321, 393, 352, 293, 3677, 527, 1412, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.14919018745422363, "compression_ratio": 1.6118721461187215, "no_speech_prob": 3.6119699871051125e-06}, {"id": 1520, "seek": 756990, "start": 7584.44, "end": 7586.94, "text": " And so that will tell us how many mini-batches are in it.", "tokens": [400, 370, 300, 486, 980, 505, 577, 867, 8382, 12, 65, 852, 279, 366, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.14919018745422363, "compression_ratio": 1.6118721461187215, "no_speech_prob": 3.6119699871051125e-06}, {"id": 1521, "seek": 756990, "start": 7586.94, "end": 7591.2, "text": " That's the length of the data loader in PyTorch.", "tokens": [663, 311, 264, 4641, 295, 264, 1412, 3677, 260, 294, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.14919018745422363, "compression_ratio": 1.6118721461187215, "no_speech_prob": 3.6119699871051125e-06}, {"id": 1522, "seek": 756990, "start": 7591.2, "end": 7597.46, "text": " Next step, we've got a data loader, is to create a model.", "tokens": [3087, 1823, 11, 321, 600, 658, 257, 1412, 3677, 260, 11, 307, 281, 1884, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14919018745422363, "compression_ratio": 1.6118721461187215, "no_speech_prob": 3.6119699871051125e-06}, {"id": 1523, "seek": 759746, "start": 7597.46, "end": 7602.34, "text": " So you can go tag for create model.", "tokens": [407, 291, 393, 352, 6162, 337, 1884, 2316, 13], "temperature": 0.0, "avg_logprob": -0.2203711043012903, "compression_ratio": 1.591160220994475, "no_speech_prob": 2.090455836878391e-06}, {"id": 1524, "seek": 759746, "start": 7602.34, "end": 7604.46, "text": " There it is.", "tokens": [821, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.2203711043012903, "compression_ratio": 1.591160220994475, "no_speech_prob": 2.090455836878391e-06}, {"id": 1525, "seek": 759746, "start": 7604.46, "end": 7606.02, "text": " Okay, same idea.", "tokens": [1033, 11, 912, 1558, 13], "temperature": 0.0, "avg_logprob": -0.2203711043012903, "compression_ratio": 1.591160220994475, "no_speech_prob": 2.090455836878391e-06}, {"id": 1526, "seek": 759746, "start": 7606.02, "end": 7607.7, "text": " We've got different kinds of models.", "tokens": [492, 600, 658, 819, 3685, 295, 5245, 13], "temperature": 0.0, "avg_logprob": -0.2203711043012903, "compression_ratio": 1.591160220994475, "no_speech_prob": 2.090455836878391e-06}, {"id": 1527, "seek": 759746, "start": 7607.7, "end": 7611.1, "text": " So we're going to be doing a cycleGAN.", "tokens": [407, 321, 434, 516, 281, 312, 884, 257, 6586, 27699, 13], "temperature": 0.0, "avg_logprob": -0.2203711043012903, "compression_ratio": 1.591160220994475, "no_speech_prob": 2.090455836878391e-06}, {"id": 1528, "seek": 759746, "start": 7611.1, "end": 7612.74, "text": " So here's our cycleGAN model.", "tokens": [407, 510, 311, 527, 6586, 27699, 2316, 13], "temperature": 0.0, "avg_logprob": -0.2203711043012903, "compression_ratio": 1.591160220994475, "no_speech_prob": 2.090455836878391e-06}, {"id": 1529, "seek": 759746, "start": 7612.74, "end": 7615.66, "text": " Okay, so there's quite a lot of stuff in a cycleGAN model.", "tokens": [1033, 11, 370, 456, 311, 1596, 257, 688, 295, 1507, 294, 257, 6586, 27699, 2316, 13], "temperature": 0.0, "avg_logprob": -0.2203711043012903, "compression_ratio": 1.591160220994475, "no_speech_prob": 2.090455836878391e-06}, {"id": 1530, "seek": 759746, "start": 7615.66, "end": 7619.86, "text": " So let's go through and find out what's going to be used.", "tokens": [407, 718, 311, 352, 807, 293, 915, 484, 437, 311, 516, 281, 312, 1143, 13], "temperature": 0.0, "avg_logprob": -0.2203711043012903, "compression_ratio": 1.591160220994475, "no_speech_prob": 2.090455836878391e-06}, {"id": 1531, "seek": 761986, "start": 7619.86, "end": 7630.219999999999, "text": " But basically, at this stage, we've just called initializer, and so when we initialize it,", "tokens": [583, 1936, 11, 412, 341, 3233, 11, 321, 600, 445, 1219, 5883, 6545, 11, 293, 370, 562, 321, 5883, 1125, 309, 11], "temperature": 0.0, "avg_logprob": -0.23938125219100562, "compression_ratio": 1.6457142857142857, "no_speech_prob": 1.5056984921102412e-06}, {"id": 1532, "seek": 761986, "start": 7630.219999999999, "end": 7635.94, "text": " you can see it's going to go through and it's going to define two generators, which is not", "tokens": [291, 393, 536, 309, 311, 516, 281, 352, 807, 293, 309, 311, 516, 281, 6964, 732, 38662, 11, 597, 307, 406], "temperature": 0.0, "avg_logprob": -0.23938125219100562, "compression_ratio": 1.6457142857142857, "no_speech_prob": 1.5056984921102412e-06}, {"id": 1533, "seek": 761986, "start": 7635.94, "end": 7641.219999999999, "text": " surprising, a generator for our horses and a generator for our zebras.", "tokens": [8830, 11, 257, 19265, 337, 527, 13112, 293, 257, 19265, 337, 527, 5277, 38182, 13], "temperature": 0.0, "avg_logprob": -0.23938125219100562, "compression_ratio": 1.6457142857142857, "no_speech_prob": 1.5056984921102412e-06}, {"id": 1534, "seek": 761986, "start": 7641.219999999999, "end": 7646.139999999999, "text": " Let's see what else we've got here.", "tokens": [961, 311, 536, 437, 1646, 321, 600, 658, 510, 13], "temperature": 0.0, "avg_logprob": -0.23938125219100562, "compression_ratio": 1.6457142857142857, "no_speech_prob": 1.5056984921102412e-06}, {"id": 1535, "seek": 764614, "start": 7646.14, "end": 7651.9800000000005, "text": " Okay, there's some way for it to generate a pool of fake data.", "tokens": [1033, 11, 456, 311, 512, 636, 337, 309, 281, 8460, 257, 7005, 295, 7592, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1556322801680792, "compression_ratio": 1.4818652849740932, "no_speech_prob": 3.785312173931743e-06}, {"id": 1536, "seek": 764614, "start": 7651.9800000000005, "end": 7659.160000000001, "text": " And then here we're going to grab our GAN loss, and as we talked about, our cycle consistency", "tokens": [400, 550, 510, 321, 434, 516, 281, 4444, 527, 460, 1770, 4470, 11, 293, 382, 321, 2825, 466, 11, 527, 6586, 14416], "temperature": 0.0, "avg_logprob": -0.1556322801680792, "compression_ratio": 1.4818652849740932, "no_speech_prob": 3.785312173931743e-06}, {"id": 1537, "seek": 764614, "start": 7659.160000000001, "end": 7663.54, "text": " loss is an L1 loss.", "tokens": [4470, 307, 364, 441, 16, 4470, 13], "temperature": 0.0, "avg_logprob": -0.1556322801680792, "compression_ratio": 1.4818652849740932, "no_speech_prob": 3.785312173931743e-06}, {"id": 1538, "seek": 764614, "start": 7663.54, "end": 7664.54, "text": " That's interesting.", "tokens": [663, 311, 1880, 13], "temperature": 0.0, "avg_logprob": -0.1556322801680792, "compression_ratio": 1.4818652849740932, "no_speech_prob": 3.785312173931743e-06}, {"id": 1539, "seek": 764614, "start": 7664.54, "end": 7665.700000000001, "text": " They're going to use Adam.", "tokens": [814, 434, 516, 281, 764, 7938, 13], "temperature": 0.0, "avg_logprob": -0.1556322801680792, "compression_ratio": 1.4818652849740932, "no_speech_prob": 3.785312173931743e-06}, {"id": 1540, "seek": 764614, "start": 7665.700000000001, "end": 7672.1, "text": " So obviously for cycleGANs, they found Adam works pretty well.", "tokens": [407, 2745, 337, 6586, 27699, 82, 11, 436, 1352, 7938, 1985, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.1556322801680792, "compression_ratio": 1.4818652849740932, "no_speech_prob": 3.785312173931743e-06}, {"id": 1541, "seek": 767210, "start": 7672.1, "end": 7677.34, "text": " And so then we're going to have an optimizer for our horse discriminator, an optimizer for", "tokens": [400, 370, 550, 321, 434, 516, 281, 362, 364, 5028, 6545, 337, 527, 6832, 20828, 1639, 11, 364, 5028, 6545, 337], "temperature": 0.0, "avg_logprob": -0.09715194916457273, "compression_ratio": 2.021505376344086, "no_speech_prob": 4.222818006383022e-06}, {"id": 1542, "seek": 767210, "start": 7677.34, "end": 7685.900000000001, "text": " our zebra discriminator, and an optimizer for our generator.", "tokens": [527, 47060, 20828, 1639, 11, 293, 364, 5028, 6545, 337, 527, 19265, 13], "temperature": 0.0, "avg_logprob": -0.09715194916457273, "compression_ratio": 2.021505376344086, "no_speech_prob": 4.222818006383022e-06}, {"id": 1543, "seek": 767210, "start": 7685.900000000001, "end": 7690.620000000001, "text": " The optimizer for the generator is going to contain the parameters both for the horse", "tokens": [440, 5028, 6545, 337, 264, 19265, 307, 516, 281, 5304, 264, 9834, 1293, 337, 264, 6832], "temperature": 0.0, "avg_logprob": -0.09715194916457273, "compression_ratio": 2.021505376344086, "no_speech_prob": 4.222818006383022e-06}, {"id": 1544, "seek": 767210, "start": 7690.620000000001, "end": 7695.9800000000005, "text": " generator and the zebra generator all in one place.", "tokens": [19265, 293, 264, 47060, 19265, 439, 294, 472, 1081, 13], "temperature": 0.0, "avg_logprob": -0.09715194916457273, "compression_ratio": 2.021505376344086, "no_speech_prob": 4.222818006383022e-06}, {"id": 1545, "seek": 767210, "start": 7695.9800000000005, "end": 7700.3, "text": " So the initializer is going to set up all of the different networks and loss functions", "tokens": [407, 264, 5883, 6545, 307, 516, 281, 992, 493, 439, 295, 264, 819, 9590, 293, 4470, 6828], "temperature": 0.0, "avg_logprob": -0.09715194916457273, "compression_ratio": 2.021505376344086, "no_speech_prob": 4.222818006383022e-06}, {"id": 1546, "seek": 770030, "start": 7700.3, "end": 7706.22, "text": " we need and they're all going to be stored inside this model.", "tokens": [321, 643, 293, 436, 434, 439, 516, 281, 312, 12187, 1854, 341, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1493380611593073, "compression_ratio": 1.5828877005347595, "no_speech_prob": 1.788062036212068e-06}, {"id": 1547, "seek": 770030, "start": 7706.22, "end": 7712.18, "text": " And so then it prints out and shows us exactly the PyTorch bottles we have.", "tokens": [400, 370, 550, 309, 22305, 484, 293, 3110, 505, 2293, 264, 9953, 51, 284, 339, 15923, 321, 362, 13], "temperature": 0.0, "avg_logprob": -0.1493380611593073, "compression_ratio": 1.5828877005347595, "no_speech_prob": 1.788062036212068e-06}, {"id": 1548, "seek": 770030, "start": 7712.18, "end": 7715.900000000001, "text": " And so it's interesting to see that they're using ResNets.", "tokens": [400, 370, 309, 311, 1880, 281, 536, 300, 436, 434, 1228, 5015, 45, 1385, 13], "temperature": 0.0, "avg_logprob": -0.1493380611593073, "compression_ratio": 1.5828877005347595, "no_speech_prob": 1.788062036212068e-06}, {"id": 1549, "seek": 770030, "start": 7715.900000000001, "end": 7719.62, "text": " And so you can see the ResNets look pretty familiar.", "tokens": [400, 370, 291, 393, 536, 264, 5015, 45, 1385, 574, 1238, 4963, 13], "temperature": 0.0, "avg_logprob": -0.1493380611593073, "compression_ratio": 1.5828877005347595, "no_speech_prob": 1.788062036212068e-06}, {"id": 1550, "seek": 770030, "start": 7719.62, "end": 7726.02, "text": " We've got conv batchNorm relu, conv batchNorm.", "tokens": [492, 600, 658, 3754, 15245, 45, 687, 1039, 84, 11, 3754, 15245, 45, 687, 13], "temperature": 0.0, "avg_logprob": -0.1493380611593073, "compression_ratio": 1.5828877005347595, "no_speech_prob": 1.788062036212068e-06}, {"id": 1551, "seek": 772602, "start": 7726.02, "end": 7732.26, "text": " So instanceNorm is just the same as batchNorm basically, but it applies it to one image", "tokens": [407, 5197, 45, 687, 307, 445, 264, 912, 382, 15245, 45, 687, 1936, 11, 457, 309, 13165, 309, 281, 472, 3256], "temperature": 0.0, "avg_logprob": -0.13721512024661145, "compression_ratio": 1.4807692307692308, "no_speech_prob": 5.014710040995851e-06}, {"id": 1552, "seek": 772602, "start": 7732.26, "end": 7733.26, "text": " at a time.", "tokens": [412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.13721512024661145, "compression_ratio": 1.4807692307692308, "no_speech_prob": 5.014710040995851e-06}, {"id": 1553, "seek": 772602, "start": 7733.26, "end": 7739.34, "text": " The difference isn't particularly important.", "tokens": [440, 2649, 1943, 380, 4098, 1021, 13], "temperature": 0.0, "avg_logprob": -0.13721512024661145, "compression_ratio": 1.4807692307692308, "no_speech_prob": 5.014710040995851e-06}, {"id": 1554, "seek": 772602, "start": 7739.34, "end": 7744.860000000001, "text": " And you can see they're doing reflection padding just like we are.", "tokens": [400, 291, 393, 536, 436, 434, 884, 12914, 39562, 445, 411, 321, 366, 13], "temperature": 0.0, "avg_logprob": -0.13721512024661145, "compression_ratio": 1.4807692307692308, "no_speech_prob": 5.014710040995851e-06}, {"id": 1555, "seek": 772602, "start": 7744.860000000001, "end": 7749.700000000001, "text": " You can kind of see when you try to build everything from scratch like this, it is a", "tokens": [509, 393, 733, 295, 536, 562, 291, 853, 281, 1322, 1203, 490, 8459, 411, 341, 11, 309, 307, 257], "temperature": 0.0, "avg_logprob": -0.13721512024661145, "compression_ratio": 1.4807692307692308, "no_speech_prob": 5.014710040995851e-06}, {"id": 1556, "seek": 772602, "start": 7749.700000000001, "end": 7750.700000000001, "text": " lot of work.", "tokens": [688, 295, 589, 13], "temperature": 0.0, "avg_logprob": -0.13721512024661145, "compression_ratio": 1.4807692307692308, "no_speech_prob": 5.014710040995851e-06}, {"id": 1557, "seek": 775070, "start": 7750.7, "end": 7759.94, "text": " And you can kind of get the nice little things that FastAI does automatically for you, you", "tokens": [400, 291, 393, 733, 295, 483, 264, 1481, 707, 721, 300, 15968, 48698, 775, 6772, 337, 291, 11, 291], "temperature": 0.0, "avg_logprob": -0.19996313446933783, "compression_ratio": 1.683982683982684, "no_speech_prob": 1.6187341316253878e-05}, {"id": 1558, "seek": 775070, "start": 7759.94, "end": 7765.98, "text": " kind of have to do all of them by hand and only end up with a subset of them.", "tokens": [733, 295, 362, 281, 360, 439, 295, 552, 538, 1011, 293, 787, 917, 493, 365, 257, 25993, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.19996313446933783, "compression_ratio": 1.683982683982684, "no_speech_prob": 1.6187341316253878e-05}, {"id": 1559, "seek": 775070, "start": 7765.98, "end": 7771.179999999999, "text": " So over time, hopefully soon we'll get all of this GAN stuff into FastAI and it'll be", "tokens": [407, 670, 565, 11, 4696, 2321, 321, 603, 483, 439, 295, 341, 460, 1770, 1507, 666, 15968, 48698, 293, 309, 603, 312], "temperature": 0.0, "avg_logprob": -0.19996313446933783, "compression_ratio": 1.683982683982684, "no_speech_prob": 1.6187341316253878e-05}, {"id": 1560, "seek": 775070, "start": 7771.179999999999, "end": 7774.179999999999, "text": " nice and easy.", "tokens": [1481, 293, 1858, 13], "temperature": 0.0, "avg_logprob": -0.19996313446933783, "compression_ratio": 1.683982683982684, "no_speech_prob": 1.6187341316253878e-05}, {"id": 1561, "seek": 775070, "start": 7774.179999999999, "end": 7775.179999999999, "text": " So we've got our model.", "tokens": [407, 321, 600, 658, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.19996313446933783, "compression_ratio": 1.683982683982684, "no_speech_prob": 1.6187341316253878e-05}, {"id": 1562, "seek": 775070, "start": 7775.179999999999, "end": 7779.54, "text": " And remember the model contains the loss functions, it contains the generators, it contains the", "tokens": [400, 1604, 264, 2316, 8306, 264, 4470, 6828, 11, 309, 8306, 264, 38662, 11, 309, 8306, 264], "temperature": 0.0, "avg_logprob": -0.19996313446933783, "compression_ratio": 1.683982683982684, "no_speech_prob": 1.6187341316253878e-05}, {"id": 1563, "seek": 777954, "start": 7779.54, "end": 7782.98, "text": " discriminators all in one convenient place.", "tokens": [20828, 3391, 439, 294, 472, 10851, 1081, 13], "temperature": 0.0, "avg_logprob": -0.12634524816199194, "compression_ratio": 1.582010582010582, "no_speech_prob": 6.339177616609959e-06}, {"id": 1564, "seek": 777954, "start": 7782.98, "end": 7788.5, "text": " So I've gone ahead and kind of copied and pasted and slightly refactored the training", "tokens": [407, 286, 600, 2780, 2286, 293, 733, 295, 25365, 293, 1791, 292, 293, 4748, 1895, 578, 2769, 264, 3097], "temperature": 0.0, "avg_logprob": -0.12634524816199194, "compression_ratio": 1.582010582010582, "no_speech_prob": 6.339177616609959e-06}, {"id": 1565, "seek": 777954, "start": 7788.5, "end": 7793.9, "text": " loop from their code so that we can run it inside the notebook.", "tokens": [6367, 490, 641, 3089, 370, 300, 321, 393, 1190, 309, 1854, 264, 21060, 13], "temperature": 0.0, "avg_logprob": -0.12634524816199194, "compression_ratio": 1.582010582010582, "no_speech_prob": 6.339177616609959e-06}, {"id": 1566, "seek": 777954, "start": 7793.9, "end": 7795.9, "text": " So this is all pretty familiar, right?", "tokens": [407, 341, 307, 439, 1238, 4963, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12634524816199194, "compression_ratio": 1.582010582010582, "no_speech_prob": 6.339177616609959e-06}, {"id": 1567, "seek": 777954, "start": 7795.9, "end": 7803.86, "text": " A loop to go through each epoch and a loop to go through the data.", "tokens": [316, 6367, 281, 352, 807, 1184, 30992, 339, 293, 257, 6367, 281, 352, 807, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.12634524816199194, "compression_ratio": 1.582010582010582, "no_speech_prob": 6.339177616609959e-06}, {"id": 1568, "seek": 780386, "start": 7803.86, "end": 7809.86, "text": " Before we did this, we set up our, this is actually not a PyTorch dataset, I think this", "tokens": [4546, 321, 630, 341, 11, 321, 992, 493, 527, 11, 341, 307, 767, 406, 257, 9953, 51, 284, 339, 28872, 11, 286, 519, 341], "temperature": 0.0, "avg_logprob": -0.16664508858112373, "compression_ratio": 1.588235294117647, "no_speech_prob": 5.771883479610551e-06}, {"id": 1569, "seek": 780386, "start": 7809.86, "end": 7816.0599999999995, "text": " is what they used, slightly confusingly, to talk about their combined, what we would call", "tokens": [307, 437, 436, 1143, 11, 4748, 13181, 356, 11, 281, 751, 466, 641, 9354, 11, 437, 321, 576, 818], "temperature": 0.0, "avg_logprob": -0.16664508858112373, "compression_ratio": 1.588235294117647, "no_speech_prob": 5.771883479610551e-06}, {"id": 1570, "seek": 780386, "start": 7816.0599999999995, "end": 7820.86, "text": " a model data object, I guess, all the data that they need.", "tokens": [257, 2316, 1412, 2657, 11, 286, 2041, 11, 439, 264, 1412, 300, 436, 643, 13], "temperature": 0.0, "avg_logprob": -0.16664508858112373, "compression_ratio": 1.588235294117647, "no_speech_prob": 5.771883479610551e-06}, {"id": 1571, "seek": 780386, "start": 7820.86, "end": 7825.179999999999, "text": " Look through that with TQDM to get a progress bar.", "tokens": [2053, 807, 300, 365, 314, 48, 35, 44, 281, 483, 257, 4205, 2159, 13], "temperature": 0.0, "avg_logprob": -0.16664508858112373, "compression_ratio": 1.588235294117647, "no_speech_prob": 5.771883479610551e-06}, {"id": 1572, "seek": 780386, "start": 7825.179999999999, "end": 7828.759999999999, "text": " And so now we can go through and see what happens in the model.", "tokens": [400, 370, 586, 321, 393, 352, 807, 293, 536, 437, 2314, 294, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16664508858112373, "compression_ratio": 1.588235294117647, "no_speech_prob": 5.771883479610551e-06}, {"id": 1573, "seek": 782876, "start": 7828.76, "end": 7837.02, "text": " So set input, so set input.", "tokens": [407, 992, 4846, 11, 370, 992, 4846, 13], "temperature": 0.0, "avg_logprob": -0.2052962050145986, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.10159248833952e-06}, {"id": 1574, "seek": 782876, "start": 7837.02, "end": 7842.18, "text": " So it's kind of a different approach to what we do in FastAI.", "tokens": [407, 309, 311, 733, 295, 257, 819, 3109, 281, 437, 321, 360, 294, 15968, 48698, 13], "temperature": 0.0, "avg_logprob": -0.2052962050145986, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.10159248833952e-06}, {"id": 1575, "seek": 782876, "start": 7842.18, "end": 7846.76, "text": " This is kind of neat, you know, it's quite specific to CycleGANs, but basically internally", "tokens": [639, 307, 733, 295, 10654, 11, 291, 458, 11, 309, 311, 1596, 2685, 281, 10295, 2160, 27699, 82, 11, 457, 1936, 19501], "temperature": 0.0, "avg_logprob": -0.2052962050145986, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.10159248833952e-06}, {"id": 1576, "seek": 782876, "start": 7846.76, "end": 7853.42, "text": " inside this model is this idea that we're going to go into our data and grab, you know,", "tokens": [1854, 341, 2316, 307, 341, 1558, 300, 321, 434, 516, 281, 352, 666, 527, 1412, 293, 4444, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.2052962050145986, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.10159248833952e-06}, {"id": 1577, "seek": 782876, "start": 7853.42, "end": 7858.74, "text": " we're either going horse to zebra or zebra to horse, depending on which way we go.", "tokens": [321, 434, 2139, 516, 6832, 281, 47060, 420, 47060, 281, 6832, 11, 5413, 322, 597, 636, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.2052962050145986, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.10159248833952e-06}, {"id": 1578, "seek": 785874, "start": 7858.74, "end": 7864.98, "text": " We either, you know, A is either the horse or the zebra and vice versa, and if necessary,", "tokens": [492, 2139, 11, 291, 458, 11, 316, 307, 2139, 264, 6832, 420, 264, 47060, 293, 11964, 25650, 11, 293, 498, 4818, 11], "temperature": 0.0, "avg_logprob": -0.20765699659075057, "compression_ratio": 1.5810810810810811, "no_speech_prob": 3.34049946104642e-06}, {"id": 1579, "seek": 785874, "start": 7864.98, "end": 7870.5, "text": " put it on the appropriate GPU and then grab the appropriate paths.", "tokens": [829, 309, 322, 264, 6854, 18407, 293, 550, 4444, 264, 6854, 14518, 13], "temperature": 0.0, "avg_logprob": -0.20765699659075057, "compression_ratio": 1.5810810810810811, "no_speech_prob": 3.34049946104642e-06}, {"id": 1580, "seek": 785874, "start": 7870.5, "end": 7880.04, "text": " Okay, so the model now has a mini batch of horses and a mini batch of zebras.", "tokens": [1033, 11, 370, 264, 2316, 586, 575, 257, 8382, 15245, 295, 13112, 293, 257, 8382, 15245, 295, 5277, 38182, 13], "temperature": 0.0, "avg_logprob": -0.20765699659075057, "compression_ratio": 1.5810810810810811, "no_speech_prob": 3.34049946104642e-06}, {"id": 1581, "seek": 788004, "start": 7880.04, "end": 7889.42, "text": " And so now we optimize the parameters.", "tokens": [400, 370, 586, 321, 19719, 264, 9834, 13], "temperature": 0.0, "avg_logprob": -0.1715512342855964, "compression_ratio": 1.7133333333333334, "no_speech_prob": 1.3925410939918947e-06}, {"id": 1582, "seek": 788004, "start": 7889.42, "end": 7894.46, "text": " Okay, so it's kind of nice to see it like this.", "tokens": [1033, 11, 370, 309, 311, 733, 295, 1481, 281, 536, 309, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.1715512342855964, "compression_ratio": 1.7133333333333334, "no_speech_prob": 1.3925410939918947e-06}, {"id": 1583, "seek": 788004, "start": 7894.46, "end": 7896.36, "text": " You can see each step, right?", "tokens": [509, 393, 536, 1184, 1823, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1715512342855964, "compression_ratio": 1.7133333333333334, "no_speech_prob": 1.3925410939918947e-06}, {"id": 1584, "seek": 788004, "start": 7896.36, "end": 7905.94, "text": " So first of all, try to optimize the generators, then try to optimize the horse discriminator,", "tokens": [407, 700, 295, 439, 11, 853, 281, 19719, 264, 38662, 11, 550, 853, 281, 19719, 264, 6832, 20828, 1639, 11], "temperature": 0.0, "avg_logprob": -0.1715512342855964, "compression_ratio": 1.7133333333333334, "no_speech_prob": 1.3925410939918947e-06}, {"id": 1585, "seek": 788004, "start": 7905.94, "end": 7909.94, "text": " then try to optimize the zebra discriminator.", "tokens": [550, 853, 281, 19719, 264, 47060, 20828, 1639, 13], "temperature": 0.0, "avg_logprob": -0.1715512342855964, "compression_ratio": 1.7133333333333334, "no_speech_prob": 1.3925410939918947e-06}, {"id": 1586, "seek": 790994, "start": 7909.94, "end": 7915.5, "text": " Zero grad is part of PyTorch, step is part of PyTorch.", "tokens": [17182, 2771, 307, 644, 295, 9953, 51, 284, 339, 11, 1823, 307, 644, 295, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.1295044779777527, "compression_ratio": 1.5681818181818181, "no_speech_prob": 2.05804212782823e-06}, {"id": 1587, "seek": 790994, "start": 7915.5, "end": 7924.639999999999, "text": " So the interesting bit is the actual thing that does the back propagation on the generator.", "tokens": [407, 264, 1880, 857, 307, 264, 3539, 551, 300, 775, 264, 646, 38377, 322, 264, 19265, 13], "temperature": 0.0, "avg_logprob": -0.1295044779777527, "compression_ratio": 1.5681818181818181, "no_speech_prob": 2.05804212782823e-06}, {"id": 1588, "seek": 790994, "start": 7924.639999999999, "end": 7927.94, "text": " So here it is.", "tokens": [407, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1295044779777527, "compression_ratio": 1.5681818181818181, "no_speech_prob": 2.05804212782823e-06}, {"id": 1589, "seek": 790994, "start": 7927.94, "end": 7930.139999999999, "text": " And let's jump to the key pieces.", "tokens": [400, 718, 311, 3012, 281, 264, 2141, 3755, 13], "temperature": 0.0, "avg_logprob": -0.1295044779777527, "compression_ratio": 1.5681818181818181, "no_speech_prob": 2.05804212782823e-06}, {"id": 1590, "seek": 790994, "start": 7930.139999999999, "end": 7936.179999999999, "text": " There's all the bits, all the formula that we basically just saw from the paper.", "tokens": [821, 311, 439, 264, 9239, 11, 439, 264, 8513, 300, 321, 1936, 445, 1866, 490, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.1295044779777527, "compression_ratio": 1.5681818181818181, "no_speech_prob": 2.05804212782823e-06}, {"id": 1591, "seek": 793618, "start": 7936.18, "end": 7944.9400000000005, "text": " So let's take a horse and generate a zebra.", "tokens": [407, 718, 311, 747, 257, 6832, 293, 8460, 257, 47060, 13], "temperature": 0.0, "avg_logprob": -0.13572935862083957, "compression_ratio": 1.5696202531645569, "no_speech_prob": 1.8448191667630454e-06}, {"id": 1592, "seek": 793618, "start": 7944.9400000000005, "end": 7947.22, "text": " So we've now got a fake zebra.", "tokens": [407, 321, 600, 586, 658, 257, 7592, 47060, 13], "temperature": 0.0, "avg_logprob": -0.13572935862083957, "compression_ratio": 1.5696202531645569, "no_speech_prob": 1.8448191667630454e-06}, {"id": 1593, "seek": 793618, "start": 7947.22, "end": 7953.54, "text": " And let's now use the discriminator to see if we can tell whether it's fake or not.", "tokens": [400, 718, 311, 586, 764, 264, 20828, 1639, 281, 536, 498, 321, 393, 980, 1968, 309, 311, 7592, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.13572935862083957, "compression_ratio": 1.5696202531645569, "no_speech_prob": 1.8448191667630454e-06}, {"id": 1594, "seek": 793618, "start": 7953.54, "end": 7964.5, "text": " And then let's pop that into our loss function, which we set up earlier, to see if we can", "tokens": [400, 550, 718, 311, 1665, 300, 666, 527, 4470, 2445, 11, 597, 321, 992, 493, 3071, 11, 281, 536, 498, 321, 393], "temperature": 0.0, "avg_logprob": -0.13572935862083957, "compression_ratio": 1.5696202531645569, "no_speech_prob": 1.8448191667630454e-06}, {"id": 1595, "seek": 796450, "start": 7964.5, "end": 7971.42, "text": " basically get a loss function based on that prediction.", "tokens": [1936, 483, 257, 4470, 2445, 2361, 322, 300, 17630, 13], "temperature": 0.0, "avg_logprob": -0.15161955034410632, "compression_ratio": 1.7023809523809523, "no_speech_prob": 5.093672825751128e-06}, {"id": 1596, "seek": 796450, "start": 7971.42, "end": 7974.5, "text": " Then let's do the same thing to do the GAN loss.", "tokens": [1396, 718, 311, 360, 264, 912, 551, 281, 360, 264, 460, 1770, 4470, 13], "temperature": 0.0, "avg_logprob": -0.15161955034410632, "compression_ratio": 1.7023809523809523, "no_speech_prob": 5.093672825751128e-06}, {"id": 1597, "seek": 796450, "start": 7974.5, "end": 7981.0, "text": " So go in the opposite direction, and then we need to use the opposite discriminator,", "tokens": [407, 352, 294, 264, 6182, 3513, 11, 293, 550, 321, 643, 281, 764, 264, 6182, 20828, 1639, 11], "temperature": 0.0, "avg_logprob": -0.15161955034410632, "compression_ratio": 1.7023809523809523, "no_speech_prob": 5.093672825751128e-06}, {"id": 1598, "seek": 796450, "start": 7981.0, "end": 7985.14, "text": " and then put that through the loss function again.", "tokens": [293, 550, 829, 300, 807, 264, 4470, 2445, 797, 13], "temperature": 0.0, "avg_logprob": -0.15161955034410632, "compression_ratio": 1.7023809523809523, "no_speech_prob": 5.093672825751128e-06}, {"id": 1599, "seek": 796450, "start": 7985.14, "end": 7989.04, "text": " And then let's do the cycle consistency loss.", "tokens": [400, 550, 718, 311, 360, 264, 6586, 14416, 4470, 13], "temperature": 0.0, "avg_logprob": -0.15161955034410632, "compression_ratio": 1.7023809523809523, "no_speech_prob": 5.093672825751128e-06}, {"id": 1600, "seek": 798904, "start": 7989.04, "end": 7998.0199999999995, "text": " So again, we take our fake, which we created up here, and try and turn it back again into", "tokens": [407, 797, 11, 321, 747, 527, 7592, 11, 597, 321, 2942, 493, 510, 11, 293, 853, 293, 1261, 309, 646, 797, 666], "temperature": 0.0, "avg_logprob": -0.14320007960001627, "compression_ratio": 1.725, "no_speech_prob": 1.6280492900477839e-06}, {"id": 1601, "seek": 798904, "start": 7998.0199999999995, "end": 8000.38, "text": " the original.", "tokens": [264, 3380, 13], "temperature": 0.0, "avg_logprob": -0.14320007960001627, "compression_ratio": 1.725, "no_speech_prob": 1.6280492900477839e-06}, {"id": 1602, "seek": 798904, "start": 8000.38, "end": 8005.66, "text": " And then let's use that loss function, cycle consistency loss function we created earlier", "tokens": [400, 550, 718, 311, 764, 300, 4470, 2445, 11, 6586, 14416, 4470, 2445, 321, 2942, 3071], "temperature": 0.0, "avg_logprob": -0.14320007960001627, "compression_ratio": 1.725, "no_speech_prob": 1.6280492900477839e-06}, {"id": 1603, "seek": 798904, "start": 8005.66, "end": 8009.06, "text": " to compare it to the real original.", "tokens": [281, 6794, 309, 281, 264, 957, 3380, 13], "temperature": 0.0, "avg_logprob": -0.14320007960001627, "compression_ratio": 1.725, "no_speech_prob": 1.6280492900477839e-06}, {"id": 1604, "seek": 798904, "start": 8009.06, "end": 8010.64, "text": " And here's that lambda.", "tokens": [400, 510, 311, 300, 13607, 13], "temperature": 0.0, "avg_logprob": -0.14320007960001627, "compression_ratio": 1.725, "no_speech_prob": 1.6280492900477839e-06}, {"id": 1605, "seek": 798904, "start": 8010.64, "end": 8017.5, "text": " So there's some weight that we used, and that was set up actually, we just used the default", "tokens": [407, 456, 311, 512, 3364, 300, 321, 1143, 11, 293, 300, 390, 992, 493, 767, 11, 321, 445, 1143, 264, 7576], "temperature": 0.0, "avg_logprob": -0.14320007960001627, "compression_ratio": 1.725, "no_speech_prob": 1.6280492900477839e-06}, {"id": 1606, "seek": 801750, "start": 8017.5, "end": 8019.66, "text": " that they suggested in their options.", "tokens": [300, 436, 10945, 294, 641, 3956, 13], "temperature": 0.0, "avg_logprob": -0.1128970507917733, "compression_ratio": 1.5390070921985815, "no_speech_prob": 4.565952167467913e-06}, {"id": 1607, "seek": 801750, "start": 8019.66, "end": 8026.7, "text": " And then do the same for the opposite direction, and then add them all together.", "tokens": [400, 550, 360, 264, 912, 337, 264, 6182, 3513, 11, 293, 550, 909, 552, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.1128970507917733, "compression_ratio": 1.5390070921985815, "no_speech_prob": 4.565952167467913e-06}, {"id": 1608, "seek": 801750, "start": 8026.7, "end": 8030.8, "text": " Do the backward step, and that's it.", "tokens": [1144, 264, 23897, 1823, 11, 293, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.1128970507917733, "compression_ratio": 1.5390070921985815, "no_speech_prob": 4.565952167467913e-06}, {"id": 1609, "seek": 801750, "start": 8030.8, "end": 8038.02, "text": " So we can then do the same thing for the first discriminator.", "tokens": [407, 321, 393, 550, 360, 264, 912, 551, 337, 264, 700, 20828, 1639, 13], "temperature": 0.0, "avg_logprob": -0.1128970507917733, "compression_ratio": 1.5390070921985815, "no_speech_prob": 4.565952167467913e-06}, {"id": 1610, "seek": 803802, "start": 8038.02, "end": 8050.34, "text": " And since basically all the work's been done now, there's much less to do here.", "tokens": [400, 1670, 1936, 439, 264, 589, 311, 668, 1096, 586, 11, 456, 311, 709, 1570, 281, 360, 510, 13], "temperature": 0.0, "avg_logprob": -0.1370711399958684, "compression_ratio": 1.563953488372093, "no_speech_prob": 4.222815277898917e-06}, {"id": 1611, "seek": 803802, "start": 8050.34, "end": 8057.34, "text": " So I won't step all through it, but it's basically the same basic stuff that we've already seen.", "tokens": [407, 286, 1582, 380, 1823, 439, 807, 309, 11, 457, 309, 311, 1936, 264, 912, 3875, 1507, 300, 321, 600, 1217, 1612, 13], "temperature": 0.0, "avg_logprob": -0.1370711399958684, "compression_ratio": 1.563953488372093, "no_speech_prob": 4.222815277898917e-06}, {"id": 1612, "seek": 803802, "start": 8057.34, "end": 8065.5, "text": " So optimize parameters basically is calculating the losses and doing the optimizer step from", "tokens": [407, 19719, 9834, 1936, 307, 28258, 264, 15352, 293, 884, 264, 5028, 6545, 1823, 490], "temperature": 0.0, "avg_logprob": -0.1370711399958684, "compression_ratio": 1.563953488372093, "no_speech_prob": 4.222815277898917e-06}, {"id": 1613, "seek": 806550, "start": 8065.5, "end": 8070.5, "text": " time to time, save and print out some results.", "tokens": [565, 281, 565, 11, 3155, 293, 4482, 484, 512, 3542, 13], "temperature": 0.0, "avg_logprob": -0.2009372267612191, "compression_ratio": 1.6335078534031413, "no_speech_prob": 8.397962119488511e-06}, {"id": 1614, "seek": 806550, "start": 8070.5, "end": 8075.78, "text": " And then from time to time, update the learning rate, so they've got some learning rate annealing", "tokens": [400, 550, 490, 565, 281, 565, 11, 5623, 264, 2539, 3314, 11, 370, 436, 600, 658, 512, 2539, 3314, 22256, 4270], "temperature": 0.0, "avg_logprob": -0.2009372267612191, "compression_ratio": 1.6335078534031413, "no_speech_prob": 8.397962119488511e-06}, {"id": 1615, "seek": 806550, "start": 8075.78, "end": 8077.86, "text": " built in here as well.", "tokens": [3094, 294, 510, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.2009372267612191, "compression_ratio": 1.6335078534031413, "no_speech_prob": 8.397962119488511e-06}, {"id": 1616, "seek": 806550, "start": 8077.86, "end": 8089.58, "text": " It isn't very exciting, but you can take a look at it.", "tokens": [467, 1943, 380, 588, 4670, 11, 457, 291, 393, 747, 257, 574, 412, 309, 13], "temperature": 0.0, "avg_logprob": -0.2009372267612191, "compression_ratio": 1.6335078534031413, "no_speech_prob": 8.397962119488511e-06}, {"id": 1617, "seek": 806550, "start": 8089.58, "end": 8094.76, "text": " So they've basically got some kind of fast AI, they've got this idea of schedulers, which", "tokens": [407, 436, 600, 1936, 658, 512, 733, 295, 2370, 7318, 11, 436, 600, 658, 341, 1558, 295, 12000, 433, 11, 597], "temperature": 0.0, "avg_logprob": -0.2009372267612191, "compression_ratio": 1.6335078534031413, "no_speech_prob": 8.397962119488511e-06}, {"id": 1618, "seek": 809476, "start": 8094.76, "end": 8098.76, "text": " you can then use to update your learning rates.", "tokens": [291, 393, 550, 764, 281, 5623, 428, 2539, 6846, 13], "temperature": 0.0, "avg_logprob": -0.12755973288353453, "compression_ratio": 1.7341772151898733, "no_speech_prob": 1.8058173736790195e-05}, {"id": 1619, "seek": 809476, "start": 8098.76, "end": 8106.9400000000005, "text": " So I think for those of you who are interested in better understanding deep learning APIs", "tokens": [407, 286, 519, 337, 729, 295, 291, 567, 366, 3102, 294, 1101, 3701, 2452, 2539, 21445], "temperature": 0.0, "avg_logprob": -0.12755973288353453, "compression_ratio": 1.7341772151898733, "no_speech_prob": 1.8058173736790195e-05}, {"id": 1620, "seek": 809476, "start": 8106.9400000000005, "end": 8113.5, "text": " or interested in contributing more to fast AI, or interested in creating your own version", "tokens": [420, 3102, 294, 19270, 544, 281, 2370, 7318, 11, 420, 3102, 294, 4084, 428, 1065, 3037], "temperature": 0.0, "avg_logprob": -0.12755973288353453, "compression_ratio": 1.7341772151898733, "no_speech_prob": 1.8058173736790195e-05}, {"id": 1621, "seek": 809476, "start": 8113.5, "end": 8119.58, "text": " of some of this stuff in some different backend, it's cool to look at a second kind of API", "tokens": [295, 512, 295, 341, 1507, 294, 512, 819, 38087, 11, 309, 311, 1627, 281, 574, 412, 257, 1150, 733, 295, 9362], "temperature": 0.0, "avg_logprob": -0.12755973288353453, "compression_ratio": 1.7341772151898733, "no_speech_prob": 1.8058173736790195e-05}, {"id": 1622, "seek": 809476, "start": 8119.58, "end": 8124.46, "text": " that covers some subset of some of the similar things to get a sense of how are they solving", "tokens": [300, 10538, 512, 25993, 295, 512, 295, 264, 2531, 721, 281, 483, 257, 2020, 295, 577, 366, 436, 12606], "temperature": 0.0, "avg_logprob": -0.12755973288353453, "compression_ratio": 1.7341772151898733, "no_speech_prob": 1.8058173736790195e-05}, {"id": 1623, "seek": 812446, "start": 8124.46, "end": 8130.46, "text": " some of these problems and what are the similarities and what are the differences.", "tokens": [512, 295, 613, 2740, 293, 437, 366, 264, 24197, 293, 437, 366, 264, 7300, 13], "temperature": 0.0, "avg_logprob": -0.1565239306577702, "compression_ratio": 1.7584541062801933, "no_speech_prob": 1.6701083950465545e-05}, {"id": 1624, "seek": 812446, "start": 8130.46, "end": 8139.26, "text": " So we train that for a little while, and then we can just grab a few examples, and here", "tokens": [407, 321, 3847, 300, 337, 257, 707, 1339, 11, 293, 550, 321, 393, 445, 4444, 257, 1326, 5110, 11, 293, 510], "temperature": 0.0, "avg_logprob": -0.1565239306577702, "compression_ratio": 1.7584541062801933, "no_speech_prob": 1.6701083950465545e-05}, {"id": 1625, "seek": 812446, "start": 8139.26, "end": 8140.88, "text": " we have them.", "tokens": [321, 362, 552, 13], "temperature": 0.0, "avg_logprob": -0.1565239306577702, "compression_ratio": 1.7584541062801933, "no_speech_prob": 1.6701083950465545e-05}, {"id": 1626, "seek": 812446, "start": 8140.88, "end": 8148.02, "text": " So here are our horses, here they are as zebras, and here they are back as horses again.", "tokens": [407, 510, 366, 527, 13112, 11, 510, 436, 366, 382, 5277, 38182, 11, 293, 510, 436, 366, 646, 382, 13112, 797, 13], "temperature": 0.0, "avg_logprob": -0.1565239306577702, "compression_ratio": 1.7584541062801933, "no_speech_prob": 1.6701083950465545e-05}, {"id": 1627, "seek": 812446, "start": 8148.02, "end": 8151.9, "text": " Here's a zebra into a horse, back into a zebra, it's kind of thrown away its head for some", "tokens": [1692, 311, 257, 47060, 666, 257, 6832, 11, 646, 666, 257, 47060, 11, 309, 311, 733, 295, 11732, 1314, 1080, 1378, 337, 512], "temperature": 0.0, "avg_logprob": -0.1565239306577702, "compression_ratio": 1.7584541062801933, "no_speech_prob": 1.6701083950465545e-05}, {"id": 1628, "seek": 815190, "start": 8151.9, "end": 8156.82, "text": " reason but not so much it couldn't get it back again.", "tokens": [1778, 457, 406, 370, 709, 309, 2809, 380, 483, 309, 646, 797, 13], "temperature": 0.0, "avg_logprob": -0.14325224463619404, "compression_ratio": 1.773851590106007, "no_speech_prob": 1.5445868484675884e-05}, {"id": 1629, "seek": 815190, "start": 8156.82, "end": 8160.179999999999, "text": " This is a really interesting one, this is obviously not what zebras look like, but if", "tokens": [639, 307, 257, 534, 1880, 472, 11, 341, 307, 2745, 406, 437, 5277, 38182, 574, 411, 11, 457, 498], "temperature": 0.0, "avg_logprob": -0.14325224463619404, "compression_ratio": 1.773851590106007, "no_speech_prob": 1.5445868484675884e-05}, {"id": 1630, "seek": 815190, "start": 8160.179999999999, "end": 8163.42, "text": " it's going to be a zebra version of that horse.", "tokens": [309, 311, 516, 281, 312, 257, 47060, 3037, 295, 300, 6832, 13], "temperature": 0.0, "avg_logprob": -0.14325224463619404, "compression_ratio": 1.773851590106007, "no_speech_prob": 1.5445868484675884e-05}, {"id": 1631, "seek": 815190, "start": 8163.42, "end": 8167.379999999999, "text": " It's also interesting to see its failure situations, I guess it doesn't very often see basically", "tokens": [467, 311, 611, 1880, 281, 536, 1080, 7763, 6851, 11, 286, 2041, 309, 1177, 380, 588, 2049, 536, 1936], "temperature": 0.0, "avg_logprob": -0.14325224463619404, "compression_ratio": 1.773851590106007, "no_speech_prob": 1.5445868484675884e-05}, {"id": 1632, "seek": 815190, "start": 8167.379999999999, "end": 8172.86, "text": " just an eyeball, it has no idea how to do that one.", "tokens": [445, 364, 38868, 11, 309, 575, 572, 1558, 577, 281, 360, 300, 472, 13], "temperature": 0.0, "avg_logprob": -0.14325224463619404, "compression_ratio": 1.773851590106007, "no_speech_prob": 1.5445868484675884e-05}, {"id": 1633, "seek": 815190, "start": 8172.86, "end": 8177.599999999999, "text": " So some of them don't work very well, this one's done a pretty good job.", "tokens": [407, 512, 295, 552, 500, 380, 589, 588, 731, 11, 341, 472, 311, 1096, 257, 1238, 665, 1691, 13], "temperature": 0.0, "avg_logprob": -0.14325224463619404, "compression_ratio": 1.773851590106007, "no_speech_prob": 1.5445868484675884e-05}, {"id": 1634, "seek": 815190, "start": 8177.599999999999, "end": 8180.62, "text": " This one's interesting, it's done a good job with that one and that one, but for some reason", "tokens": [639, 472, 311, 1880, 11, 309, 311, 1096, 257, 665, 1691, 365, 300, 472, 293, 300, 472, 11, 457, 337, 512, 1778], "temperature": 0.0, "avg_logprob": -0.14325224463619404, "compression_ratio": 1.773851590106007, "no_speech_prob": 1.5445868484675884e-05}, {"id": 1635, "seek": 818062, "start": 8180.62, "end": 8184.66, "text": " the one in the middle didn't get to go.", "tokens": [264, 472, 294, 264, 2808, 994, 380, 483, 281, 352, 13], "temperature": 0.0, "avg_logprob": -0.2521939277648926, "compression_ratio": 1.5560165975103735, "no_speech_prob": 8.267790690297261e-06}, {"id": 1636, "seek": 818062, "start": 8184.66, "end": 8188.0599999999995, "text": " This one's a really weird shape, but it's done a reasonable job of it.", "tokens": [639, 472, 311, 257, 534, 3657, 3909, 11, 457, 309, 311, 1096, 257, 10585, 1691, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.2521939277648926, "compression_ratio": 1.5560165975103735, "no_speech_prob": 8.267790690297261e-06}, {"id": 1637, "seek": 818062, "start": 8188.0599999999995, "end": 8190.98, "text": " This one looks good.", "tokens": [639, 472, 1542, 665, 13], "temperature": 0.0, "avg_logprob": -0.2521939277648926, "compression_ratio": 1.5560165975103735, "no_speech_prob": 8.267790690297261e-06}, {"id": 1638, "seek": 818062, "start": 8190.98, "end": 8192.98, "text": " This one's pretty sloppy.", "tokens": [639, 472, 311, 1238, 43684, 13], "temperature": 0.0, "avg_logprob": -0.2521939277648926, "compression_ratio": 1.5560165975103735, "no_speech_prob": 8.267790690297261e-06}, {"id": 1639, "seek": 818062, "start": 8192.98, "end": 8196.96, "text": " Again, the fork just ahead, it's not bad.", "tokens": [3764, 11, 264, 17716, 445, 2286, 11, 309, 311, 406, 1578, 13], "temperature": 0.0, "avg_logprob": -0.2521939277648926, "compression_ratio": 1.5560165975103735, "no_speech_prob": 8.267790690297261e-06}, {"id": 1640, "seek": 818062, "start": 8196.96, "end": 8204.3, "text": " So you know, it took me like 24 hours to train it even that far, so it's kind of slow.", "tokens": [407, 291, 458, 11, 309, 1890, 385, 411, 4022, 2496, 281, 3847, 309, 754, 300, 1400, 11, 370, 309, 311, 733, 295, 2964, 13], "temperature": 0.0, "avg_logprob": -0.2521939277648926, "compression_ratio": 1.5560165975103735, "no_speech_prob": 8.267790690297261e-06}, {"id": 1641, "seek": 818062, "start": 8204.3, "end": 8209.5, "text": " And I know Helena is constantly complaining on Twitter about how long these things take,", "tokens": [400, 286, 458, 49294, 307, 6460, 20740, 322, 5794, 466, 577, 938, 613, 721, 747, 11], "temperature": 0.0, "avg_logprob": -0.2521939277648926, "compression_ratio": 1.5560165975103735, "no_speech_prob": 8.267790690297261e-06}, {"id": 1642, "seek": 820950, "start": 8209.5, "end": 8213.38, "text": " I don't know how she's so productive with them.", "tokens": [286, 500, 380, 458, 577, 750, 311, 370, 13304, 365, 552, 13], "temperature": 0.0, "avg_logprob": -0.16398198553856383, "compression_ratio": 1.6061946902654867, "no_speech_prob": 2.5465664293733425e-05}, {"id": 1643, "seek": 820950, "start": 8213.38, "end": 8219.9, "text": " So yeah, I will mention one more thing that just came out yesterday, which is there's", "tokens": [407, 1338, 11, 286, 486, 2152, 472, 544, 551, 300, 445, 1361, 484, 5186, 11, 597, 307, 456, 311], "temperature": 0.0, "avg_logprob": -0.16398198553856383, "compression_ratio": 1.6061946902654867, "no_speech_prob": 2.5465664293733425e-05}, {"id": 1644, "seek": 820950, "start": 8219.9, "end": 8227.58, "text": " now a multimodal image-to-image translation of unpaird, and so you can basically now create", "tokens": [586, 257, 32972, 378, 304, 3256, 12, 1353, 12, 26624, 12853, 295, 20994, 1246, 67, 11, 293, 370, 291, 393, 1936, 586, 1884], "temperature": 0.0, "avg_logprob": -0.16398198553856383, "compression_ratio": 1.6061946902654867, "no_speech_prob": 2.5465664293733425e-05}, {"id": 1645, "seek": 820950, "start": 8227.58, "end": 8233.44, "text": " different cats, for instance, from this doc.", "tokens": [819, 11111, 11, 337, 5197, 11, 490, 341, 3211, 13], "temperature": 0.0, "avg_logprob": -0.16398198553856383, "compression_ratio": 1.6061946902654867, "no_speech_prob": 2.5465664293733425e-05}, {"id": 1646, "seek": 820950, "start": 8233.44, "end": 8239.02, "text": " So this is basically not just creating one example of the output that you want, but creating", "tokens": [407, 341, 307, 1936, 406, 445, 4084, 472, 1365, 295, 264, 5598, 300, 291, 528, 11, 457, 4084], "temperature": 0.0, "avg_logprob": -0.16398198553856383, "compression_ratio": 1.6061946902654867, "no_speech_prob": 2.5465664293733425e-05}, {"id": 1647, "seek": 823902, "start": 8239.02, "end": 8240.02, "text": " multiple ones.", "tokens": [3866, 2306, 13], "temperature": 0.0, "avg_logprob": -0.21547974480523002, "compression_ratio": 1.688073394495413, "no_speech_prob": 3.4267741284566e-05}, {"id": 1648, "seek": 823902, "start": 8240.02, "end": 8246.1, "text": " So here's house cat to big cat, and here's big cat to house cat.", "tokens": [407, 510, 311, 1782, 3857, 281, 955, 3857, 11, 293, 510, 311, 955, 3857, 281, 1782, 3857, 13], "temperature": 0.0, "avg_logprob": -0.21547974480523002, "compression_ratio": 1.688073394495413, "no_speech_prob": 3.4267741284566e-05}, {"id": 1649, "seek": 823902, "start": 8246.1, "end": 8250.300000000001, "text": " This is the paper, so this came out like yesterday or the day before, I think.", "tokens": [639, 307, 264, 3035, 11, 370, 341, 1361, 484, 411, 5186, 420, 264, 786, 949, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.21547974480523002, "compression_ratio": 1.688073394495413, "no_speech_prob": 3.4267741284566e-05}, {"id": 1650, "seek": 823902, "start": 8250.300000000001, "end": 8255.66, "text": " I think it's pretty amazing cat to dog.", "tokens": [286, 519, 309, 311, 1238, 2243, 3857, 281, 3000, 13], "temperature": 0.0, "avg_logprob": -0.21547974480523002, "compression_ratio": 1.688073394495413, "no_speech_prob": 3.4267741284566e-05}, {"id": 1651, "seek": 823902, "start": 8255.66, "end": 8259.94, "text": " So you can kind of see how this technology is developing, and I think there's so many", "tokens": [407, 291, 393, 733, 295, 536, 577, 341, 2899, 307, 6416, 11, 293, 286, 519, 456, 311, 370, 867], "temperature": 0.0, "avg_logprob": -0.21547974480523002, "compression_ratio": 1.688073394495413, "no_speech_prob": 3.4267741284566e-05}, {"id": 1652, "seek": 823902, "start": 8259.94, "end": 8266.9, "text": " opportunities to maybe do this with music or speech or writing or to create kind of", "tokens": [4786, 281, 1310, 360, 341, 365, 1318, 420, 6218, 420, 3579, 420, 281, 1884, 733, 295], "temperature": 0.0, "avg_logprob": -0.21547974480523002, "compression_ratio": 1.688073394495413, "no_speech_prob": 3.4267741284566e-05}, {"id": 1653, "seek": 826690, "start": 8266.9, "end": 8269.22, "text": " tools for artists or whatever.", "tokens": [3873, 337, 6910, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.3286419998515736, "compression_ratio": 1.0389610389610389, "no_speech_prob": 4.754458859679289e-05}, {"id": 1654, "seek": 826922, "start": 8269.22, "end": 8297.14, "text": " All right, thanks everybody, and see you next week.", "tokens": [50364, 1057, 558, 11, 3231, 2201, 11, 293, 536, 291, 958, 1243, 13, 51760], "temperature": 0.0, "avg_logprob": -0.36030505498250326, "compression_ratio": 0.864406779661017, "no_speech_prob": 8.754496957408264e-05}], "language": "en"}