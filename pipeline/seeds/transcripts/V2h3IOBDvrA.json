{"text": " I guess I noticed during the week from some of the questions I've been seeing that the idea of what a convolution is is still a little counterintuitive or surprising to some people. So I feel like the only way I know to teach things effectively is by creating a spreadsheet. So here we are. This is the famous number 7 from Lesson 0. I wanted to show you exactly what a convolution does. And specifically what a convolution does in a deep learning neural network. So we are generally using modern convolutions, and that means a 3x3 convolution. So here is a 3x3 convolution. And I have just randomly generated 9 random numbers. So that is a filter. There's one filter. Here is my second filter. It is 9 more random numbers. This is what we do in Keras when we ask for a convolutional layer. We tell it, the first thing we pass it is how many filters do we want. And that's how many of these random matrices do we want it to build for us. So in this case, it's as if I passed convolution2d, the first parameter would be 2, and the second parameter would be 3,3, because it's a 3x3. So what happens to this little random matrix? In order to calculate the very first item, it takes the sum of the blue stuff times the red stuff, those 9, all added together. So let's go down here and do where it gets a bit darker. This is equal to these 9 times these 9, and when I say times, I mean element-wise times, so the top left by the top left, the middle by the middle, and so forth, and add them all together. That's all a convolution is. So it's just as you go through, we take the corresponding 3x3 area in the image and we multiply each of those 9 things by each of these 9 things, and then we add those 9 products together. That's it, that's a convolution. So there's really nothing particularly weird or confusing about it, and I'll make this available in class so you can have a look. You can see that when I get to the top left corner, I can't move further left and up because I've reached the edge, and this is why when you do a 3x3 convolution without zero padding, you lose one pixel on each edge because you can't push this 3x3 any further. So if we go down to the bottom left, you can see again the same thing. So that's why you can see that my result is one row less than my starting point. So I did this for 2 different filters. So here's my second filter, and you can see when I calculate this one, it's exactly the same thing. It's these 9 times each of these 9 added together. These are just 9 other random numbers. So that's how we start with our first. In this case, I've created 2 convolutional filters, and this is the output of those 2 convolutional filters. So my second layer. Now my second layer is no longer enough just to have a 3x3 matrix, and I now need a 3x3x2 tensor because to calculate my top left of my second convolutional layer, I need these 9 by these 9 added together plus these 9 by these 9 added together. At this point, my previous layer is no longer just one thing, but it's 2 things. Now indeed, if our original picture was a 3-channel color picture, our very first convolutional layer would have had to have been 3x3x3 tensors. So all of the convolutional layers from now on are going to be 3x3x number of filters in the previous layer convolution matrices. So here's my first 3x3x2 tensor, and you can see it's taking 9 from here, 9 from here, and adding those 2 together. And so then for my second filter in my second layer, it's exactly the same thing. I've created 2 more random matrices, or 1 more random 3x3x2 tensor, and here again I have those 3 by these 9, by these 9, sum, plus those 9 by those 9, sum. And that gives me that one. So that gives me my first 2 layers of my convolutional neural network. Then I do max pooling. Max pooling is slightly more awkward to do in Excel, but that's fine, we can still handle it. So here's max pooling. So max pooling is now going to decrease the resolution of my image by 2 on each axis. So how do we calculate that number? That number is simply the maximum of those 4. And then that number is the maximum of those 4, and so forth. So with max pooling, we had 2 filters in the previous layer, so we still have 2 filters, but now our filters have half the resolution in each of the x and y axes. And so then I thought, okay, we've done 2 convolutional layers. Question from the audience. How did you go from one matrix to 2 matrices in the second layer? How did I go from one matrix to 2 matrices, as in how did I go from just this one thing to these 2 things? So the answer to that is I just created 2 random 3x3 filters. This is my first random 3x3 filter, this is my second random 3x3 filter. So each output then was simply equal to each corresponding 9-element section multiplied by each other and added together. So because I had 2 random 3x3 matrices, I ended up with 2 outputs. So 2 filters means 2 sets of outputs. So now that we've got our max pooling layer, let's use a dense layer to turn it into our output. So a dense layer means that every single one of our activations from our max pooling layer needs a random weight. So these are a whole bunch of random numbers. So what I do is I take every one of those random numbers and multiply each one by a corresponding input. And MNIST, we would have 10 activations because we need an activation for 0, 1, 2, 3, so forth. So for MNIST, we would need 10 sets of these dense weight matrices so that we could calculate the 10 outputs. If we were only calculating one output, this would be a perfectly reasonable way to do it. For one output, it's just a sum product of everything from our final layer with a weight for everything in that final layer added together. So that's all a dense layer is. So really both dense layers and convolutional layers couldn't be easier mathematically. I think the surprising thing is what happens when you then say, rather than using random weights, let's calculate the derivative of what happens if we were to change that weight up by a bit or down by a bit, and how would it impact our loss. In this case, I haven't actually got as far as calculating a loss function, but we could add over here a sigmoid loss for example. And so we can calculate the derivative of the loss with respect to every single weight in the dense layer and every single weight in all of our filters in that layer and every single weight in all of our filters in this layer. And then with all of those derivatives, we can calculate how to optimize all of these weights. And the surprising thing is that when we optimize all of these weights, we end up with these incredibly powerful models like those visualizations that we saw. So I'm not quite sure where the disconnect between the incredibly simple math and the outcome is, I think it might be that it's so easy, it's hard to believe that that's all it is. But I'm not skipping over anything. That really is it. And so to help you really understand this, I'm going to talk more about SGD. Why would you use a sigmoid function here? So the loss function we generally use is the softmax, so e to the xi divided by sum of e to the xi. If it's just binary, that's just the equivalent of having just 1 over 1 plus e to the xi. So softmax in the binary case simplifies into a sigmoid function. Thank you for clarifying that question. So I think this is super fun. We're going to talk about not just SGD, but every variant of SGD, including one invented just a week ago. So we've already talked about SGD. Question. Does SGD happen for all layers at once? Answer. SGD happens for all layers at once. Yes, we calculate the derivative of all the weights with respect to the loss. And when to have a max pool after convolution vs when not to? When to have a max pool after a convolution, who knows. This is a very controversial question and indeed some people now are saying never use max pool. Instead of using max pool when you're doing the convolutions, don't do a convolution over every set of 9 pixels, but instead skip a pixel each time. And so that's another way of downsampling. Jeffrey Hinton, who's kind of the father of deep learning, has gone as far as saying that the extremely great success of max pooling has been the greatest problem deep learning has faced. Because to him, it really stops us from going further. I don't know whether that's true or not. I assume it is because he's Jeffrey Hinton and I'm not. For now, we use max pooling every time we're doing fine-tuning because we need to make sure that our architecture is identical to the original VGG's architecture, so we have to put max pooling wherever they do. Why do we want max pooling or downsampling? Are we just trying to look at bigger features at the input? Why use max pooling at all? There's a couple of reasons. The first is that max pooling helps with translation invariance. So it basically says if this feature is here or here or here or here, I don't care, it's kind of roughly in the right spot. And so that seems to work well. And the second is exactly what you said. Every time we max pool, we end up with a smaller grid, which means that our 3x3 convolutions are effectively covering a larger part of the original image, which means that our convolutions can find larger and more complex features. I think they would be the two main reasons. Is Jeffrey Hinton cool with the idea of doing the skipping? The capsule architecture, CAPSULE. You can learn all about the things that he thinks we ought to have but don't yet have. He did point out that one of the key pieces of deep learning that he invented took 17 years from conception to working. So he is somebody who sticks to these things and makes it work. Is max pooling unique to image processing? Max pooling is not unique to image processing. It's likely to be useful for any kind of convolutional neural network. And a convolutional neural network can be used for any kind of data that has some kind of consistent ordering. So things like speech, or any kind of audio, or some kind of consistent time series, all of these things have some kind of ordering to them, and therefore you can use a CNN and therefore you can use max pooling. And as we look at NLP, we will be looking more at convolutional neural networks for other data types. And interestingly, the author of Keras last week or maybe the week before made the contention that perhaps it will turn out that CNNs are the architecture that will be used for every type of ordered data. And this was just after one of the leading NLP researchers released a paper basically showing a state-of-the-art result in NLP using convolutional neural networks. So although we'll start learning about recurrent neural networks next week, I have to be open to the possibility that they'll become redundant by the end of the year. So SGD, we looked at the SGD intro notebook, but I think things are a little more clear sometimes when you can see it all in front of you. So here is basically the identical thing that we saw in the SGD notebook in Excel. So we are going to start by creating a line. We create 29 random numbers, and then we say, OK, let's create something that is equal to 2 times x plus 30. And so here is 2 times x plus 30. So that's my input data. So I am trying to again create something that can find the parameters of a line. Now the important thing, and this is the leap, which requires not thinking too hard lest you realize how surprising and amazing this is. Everything we learn about how to fit a line is identical to how to fit filters and weights in a convolutional neural network. And so everything we learn about calculating the slope and the intercept, we will then use to let computers see. And so the answer to any question which is basically why, is why not. This is a function that takes some inputs and calculates an output. This is a function that takes some inputs and calculates an output. So why not? The only reason it wouldn't work would be because it was too slow, for example. We know it's not too slow because we tried it and it works pretty well. So everything we're about to learn works for any kind of function which has the appropriate types of gradients. And we can talk more about that later. But neural nets have the appropriate kinds of gradients. So SGD, we start with a guess. What do we think the parameters of our function are? In this case, the intercept and the slope. And with Keras, they will be randomized using the chloro-initialization procedure we learned about, which is 6 divided by n in plus n out. But let's assume they're both 1. We are going to use very, very small mini-batches here. Mini-batches are going to be of size 1, basically because it's easier to do in Excel and it's easier to see. But everything we're going to see would work equally well for a mini-batch of size 4 or 64 or 128. So here's our first row, our first mini-batch. Our input is 14 and our desired output is 58. And so our guesses to our parameters are 1 and 1. And therefore our predicted Y value is equal to 1 plus 1 times 14, which is normally 15. So if we're doing root-mean-squared error, our error squared is prediction minus actual squared. So the next thing we do is we want to calculate the derivative with respect to each of our two inputs. One really easy way to do that is to add a tiny amount to each of the two inputs and see how the output varies. So let's start by doing that. So let's add 0.01 to our intercept and calculate the line and then calculate the loss squared. So this is the error if b is increased by 0.01. And then let's calculate the difference between that error and the actual error and then divide that by our change, which is 0.01. And that gives us our estimated gradient. I'm using de for de-error, db. It should have probably been dl for de-loss, db. So this is the change in loss with respect to b is negative 85.99. We can do the same thing for a. So we can add 0.01 to a and then calculate our line, subtract our actual, take the square, and so there is our value of estimated loss da, subtract it from the actual loss, divide it by 0.01. And so there are two estimates as the derivative. This approach to estimating the derivative is called finite differencing. At any time you calculate a derivative by hand, you should always use finite differencing to make sure your calculation is correct. You're not very likely to ever have to do that, however, because all of the libraries do derivatives for you. And they do them analytically, not using finite derivatives. And so here are the derivatives calculated analytically, which you can do by going to Wolfram Alpha and typing in your formula and getting the derivative back. So this is the analytical derivative of the loss with respect to b and the analytical derivative of a. And so you can see that our analytical and our finite difference are very similar for b and they are very similar for a. So that makes me feel comfortable that we got the calculation correct. So all SGD does is it says, okay, this tells us if we change our weights by a little bit, this is the change in our loss function, we know that increasing our value of b by a bit will decrease the loss function, and we know that increasing our value of a by a little bit will decrease the loss function. So therefore, let's decrease both of them by a little bit. And the way we do that is to multiply the derivative times a learning rate, that's the value of a little bit, and subtract that from our previous guess. So we do that for a, and we do that for b, and here are our new guesses. Now we're at 1.12 and 1.01. And so let's copy them over here, 1.12 and 1.01. And then we do the same thing, and that gives us a new a and a b. We keep doing that again and again and again until we've gone through the whole data set. At the end of which, we have a guess of a of 2.61 and a guess of b of 1.07. So that's one epoch. Now in real life, we would be having shuffle equals true, which means that these would be randomized. So this isn't quite perfect, but apart from that, this is SGD with a mini-batch size of 1. So at the end of the epoch, we say, this is our new slope, so let's copy 2.61 over here. And this is our new intercept, so let's copy 1.06 over here. And so now, it starts again. So we can keep doing that again and again and again. Copy the stuff from the bottom, stick it back at the top, and each one of these is going to be an epoch. So I recorded a macro with me copying this to the bottom and pasting it at the top and added something that says for i equals 1 to 5 around it. So now if I click run, it will copy and paste it 5 times. And so you can see it's gradually getting closer. And we know that our goal is that it should be a, a equals 2 and b equals 30. So we've got as far as a equals 2.5 and b equals 1.3. So they're better than our starting point. And you can see our gradually improving loss function. But it's going to take a long time. Yes, Rachel? Question from the audience. Can we still do analytic derivatives when we are using non-linear activation functions? Yes, we can use analytical derivatives as long as we're using a function that has an analytical derivative, which is pretty much every useful function you can think of, except ones that you can't have something that has an if-then statement in it because it kind of jumps from here to here, but even those you can approximate. So a good example would be ReLU. So ReLU, which is max of 0,x strictly speaking doesn't really have a derivative at every point, or at least not a well-defined one, because this is what ReLU looks like. And so its derivative here is 0 and its derivative here is 1. What is its derivative exactly here? Who knows? But the thing is, mathematicians care about that kind of thing, we don't. Like in real life, this is a computer, and computers are never exactly anything. We can either assume that it's like an infinite amount to this side or an infinite amount to this side, and who cares. So as long as it has a derivative that you can calculate in a meaningful way in practice on a computer, then it'll be fine. So one thing you might have noticed about this is it's going to take an awfully long time to get anywhere. And so you might think, okay, let's increase the learning rate. Fine, let's increase the learning rate. So let's get rid of one of these zeros. Oh dear, something went crazy. What went crazy? I'll tell you what went crazy. Our A's and B's started to go out to like 11 million, which is not the correct answer. So how did it go ahead and do that? Well here's the problem. Let's say this was the shape of our loss function. And this was our initial guess. We figured out the derivative is going this way. Well actually the derivative is positive, so we want to go the opposite direction. And so we step a little bit over here. And then that leaves us to here. We step a little bit further. And this looks good. But then we increase the learning rate. So rather than stepping a little bit, we stepped a long way. And that put us here. And then we stepped a long way again. And that put us here. If your learning rate is too high, you're going to get worse and worse. And that's what happened. So getting your learning rate right is critical to getting your thing to train at all. Exploding gradients, or you can even have gradients that do the opposite. Exploding gradients is something a little bit different, but it's a similar idea. So it looks like.001 is the best we can do. And that's a bit sad because this is really slow. So let's try and improve it. So one thing we could do is to say, well, given that every time we've been, actually let me do this in a few more dimensions. So let's say we had a 3-dimensional set of axes now, and we kind of had a loss function that looks like this kind of valley. And let's say our initial guess was somewhere over here. So over here, the gradient is pointing in this direction. So we might make a step and end up there. And then we might make another step which would put us there. And this is actually the most common thing that happens in neural networks. Something that's kind of flat in one dimension like this is called a saddle point. And it's actually been proved that the vast majority of the space of a loss function in a neural network is pretty much all saddle points. So when you look at this, it's pretty obvious what should be done. Which is if we go to here and then we go to here, we can say, well on average, we're kind of obviously heading in this direction. Especially when we do it again, we're obviously heading in this direction. So let's take the average of how we've been going so far and do a bit of that. And that's exactly what momentum does. Question. If relu isn't the cost function, why are we concerned with its differentiability? We care about the derivative of the output with respect to the inputs. The inputs are the filters. And remember the loss function consists of a function of a function of a function of a function. So it is categorical cross-entropy loss applied to softmax, applied to relu, applied to dense layer, applied to max pooling, applied to relu, applied to convolutions, etc. So in other words, to calculate the derivative of the loss with respect to the inputs, you have to calculate the derivative through that whole function. And this is what's called backpropagation. With backpropagation, it's easy to calculate that derivative because we know that from the chain rule, the derivative of a function of a function is simply equal to the product of the derivatives of those functions. So in practice, all we do is we calculate the derivative of every layer with respect to its inputs and then we just multiply them all together. And so that's why we need to know the derivative of the activation layers as well as the loss layer and everything else. So here's the trick. What we're going to do is we're going to say every time we take a step, we're going to also calculate the average of the last few steps. So after these two steps, the average is this direction. So the next step, we're going to take our gradient step as usual and we're going to add on our average of the last few steps. And that means that we end up actually going to here. And then we do the same thing again. So we find the average of the last few steps and it's now even further in this direction. Question. What is that surface? This is the surface of the loss function with respect to some of the parameters, in this case just a couple of parameters. It's just an example of what a loss function might look like. So this is the loss and this is some weight number 1 and this is some weight number 2. So we're trying to get our little, if you can imagine this is like gravity, we're trying to get this little ball to travel down this valley as far down to the bottom as possible. And so the trick is that we're going to keep taking a step, not just the gradient step, but also the average of the last few steps. And so in practice, this is going to end up going, donk, donk, donk, donk, donk. That's the idea. So to do that in Excel is pretty straightforward. To make things simpler, I have removed the finite differencing based derivatives here, so we just have the analytical derivatives. But other than that, this is identical to the previous spreadsheet. Same data, same predictions, same derivatives, except we've done one extra thing, which is that when we calculate our new B, we say it's our previous B minus our learning rate times our gradient times this cell. What is that cell? That cell is equal to our gradient times.1 plus the thing just above it times.9. And the thing just above it is equal to its gradient times.1 plus the thing just above it times.9. So in other words, this column is keeping track of an average derivative of the last few steps that we've taken, which is exactly what we want. And we do that for both of our two parameters. So this.9 is our momentum parameter. So in Keras, when you use momentum, you can say momentum equals and you say how much momentum you want. You just pick it. So you just pick that parameter. Just like your learning rate, you pick it. Your momentum factor, you pick it. It's something you get to choose. And you choose it by trying a few and find out what works best. So let's try running this. And you can see it is still not exactly zipping along. Why is it not exactly zipping along? The reason when we look at it is that we know that the constant term needs to get all the way up to 30. And it's still way down at 1.5. It's not moving fast enough, whereas the slope term moved very quickly to where we want it to be. So what we really want is we need different learning rates for different parameters. And doing this is called dynamic learning rates. And the first really effective dynamic learning rate approaches have just appeared in the last 3 years or so. And one very popular one is called Adagrad. And it's very simple. All of these dynamic learning rate approaches have the same insight, which is this. If the parameter that I'm changing, if the derivative of that parameter is consistently of a very low magnitude, then if the derivative of this mini-batch is higher than that, then what I really care about is the relative difference between how much this variable tends to change and how much it's going to change this time around. So in other words, we don't just care about what's the gradient, but is the magnitude of the gradient a lot more or a lot less than it has tended to be recently? So the easy way to calculate the overall amount of change of the gradient recently is to keep track of the square of the gradient. So what we do with Adagrad is you can see at the bottom of my epoch here, I have got a sum of squares of all of my gradients. And then I have taken the square root, so I've got the roots on the squares, and then I've just divided it by the count to get the average. So this is the average of the roots of the squares of my gradients. So this number here will be high if the magnitudes of my gradients is high. And because it's squared, it will be particularly high if sometimes they're really high. So why is it okay to just use a mini-batch since the surface is going to depend on what points are in your mini-batch? It's not ideal to just use a mini-batch, and we will learn about a better approach to this in a moment. But for now, let's look at this. In fact, there are two approaches very related, Adagrad and Adadelta. And one of them actually does this for all of the gradients so far, and one of them uses a slightly more sophisticated approach. This approach of doing it on a mini-batch-by-mini-batch basis is slightly different either, but it's similar enough to explain the concept. So what I do is I... Question from the audience. Does this mean for a CNN, would dynamic learning rates mean that each filter would have its own learning rate? It would mean that every parameter has its own learning rate. So this is one parameter, that's a parameter, that's a parameter, that's a parameter, and then in our dense layer, that's a parameter, that's a parameter, that's a parameter. So when you go model.summary in Keras, it shows you for every layer how many parameters there are. So anytime you're unclear on how many parameters there are, you can go back and have a look at these spreadsheets and you can also look at the Keras model.summary and make sure you understand how they turn out. So for the first layer, it's going to be the size of your filter times the number of your filters, if it's just grayscale, and then after that the number of parameters will be equal to the size of your filter times the number of filters coming in times the number of filters coming out. And then of course your dense layers will be every input goes to every output, so number of inputs times number of outputs. A parameter to the function that is calculating whether it's a cat or a dog. So what we do now is we say, this number here, 1857, this is saying that the derivative of the loss with respect to the slope varies a lot, whereas the derivative of the loss with respect to the intercept doesn't vary much at all. So at the end of every epoch, I copy that up to here, and then I take my learning rate and I divide it by that. And so now for each of my parameters, I now have this adjusted learning rate, which is the learning rate divided by the recent sum of squareds average gradient. And so you can see that now one of my learning rates is 100 times faster than the other one. And so let's see what happens when I run this. Question from the audience. There's not really a relationship with normalizing the input data, because it can help, but still if your inputs are of very different scales, it's still a lot more work for it to do. So yes it helps, but it doesn't help so much that it makes it useless. And in fact, it turns out that even with dynamic learning rates, having not just normalized inputs, but batch normalized activations is extremely helpful. And so the thing about when you're using AdaGrad or any kind of dynamic learning rates is generally you'll set the learning rate quite a lot higher, because remember you're dividing it by this recent average. So if I set it to.1, oh too far, so that's no good. So let's try.05. You can see after just 5 steps, I'm already halfway there. Another 5 steps, getting very close, another 5 steps, and it's exploded. Now why did that happen? Because as we get closer and closer to where we want to be, you can see that you need to take smaller and smaller steps. And by keeping the learning rates the same, it meant that eventually we went too far. So this is still something you have to be very careful of. A more elegant, in my opinion, approach to the same thing that AdaGrad is doing is something called RMSProp. RMSProp was first introduced in Jeffrey Hinton's Coursera course. So if you go to the Coursera course, called Neural Networks, you'll find it. And in one of those classes he introduces RMSProp. So it's quite funny nowadays, because this comes up in academic papers a lot, when people cite it, they have to cite Coursera course, Chapter 6, at minute 14 and 30 seconds. But Hinton has asked that this be the official way that it is cited. So there you go. So here's what RMSProp does. What RMSProp does is exactly the same thing as Momentum, but instead of keeping track of the weighted running average of the gradients, we keep track of the weighted running average of the square of the gradients. So here it is. Everything here is the same as Momentum so far, except that I take my gradient squared, multiply it by my.1 and add it to my previous cell times.9. So this is keeping track of the recent running average of the squareds of the gradients. And when I have that, I then do exactly the same thing that I did in AdaGrad, which is to divide the learning rate by it. So I take my previous guess as to b and then I subtract from it my derivative times the learning rate, divide it by the square root of the recent running weighted average of the squared gradients. So it's doing basically the same thing as AdaGrad, but in a way that's doing it continuously. Question- So these are all different types of learning rate optimization? Answer- These last two are different types of dynamic learning rate approaches. So let's try this one. We'll run it for a few steps, and again we'll have to guess what learning rate to start with. So as you can see, this is going pretty well. And I'll show you something really nice about RMSProp, which is what happens as we get very close. We know the right answer is 2 and 30. Is it about to explode? No, it doesn't explode. And the reason it doesn't explode is because it's recalculating that running average every single minibatch. And so rather than waiting until the end of the epoch, by which date it's gone so far that it can't come back again, it just jumps a little bit too far, and then it recalculates the dynamic learning rates and tries again. So what happens with RMSProp is if your learning rate is too high, then it doesn't explode, it just ends up going around the right answer. And so when you use RMSProp, as soon as you see your validation scores flatten out, you know this is what's going on, and so therefore you should probably divide your learning rate by 10. And you see me doing this all the time. When I'm running Keras stuff, you'll keep seeing me run a few steps, divide the learning rate by 10, run a few steps. So you don't see that my loss function explodes, you just see that it flattens out. Question. Do you want your learning rate to get smaller and smaller? Answer. Yes, you do. Your very first learning rate often has to start small, and we'll talk about that in a moment, but once you've kind of got started, you generally have to gradually decrease the learning rate. That's called learning rate annealing. Question. Can you repeat what you said earlier that something does the same thing as AdaGrad? So RMSProp, which we're looking at now, does exactly the same thing as AdaGrad, which is divide the learning rate by the root sum of squared of the gradients. But rather than doing it since the beginning of time, or every minibatch, or every epoch, RMSProp does it continuously using the same technique that we learned from momentum, which is take the squared of this gradient, multiply it by 0.1 and add it to 0.9 times the last calculation. Question. That's called a moving average. Answer. It's a weighted moving average, where we're weighting it such that the more recent squared gradients are weighted higher. I think it's actually an exponentially weighted moving average, to be more precise. So there's something pretty obvious we could do here, which is momentum seems like a good idea, RMSProp seems like a good idea, why not do both? And that is called Adam. And so Adam was invented like, I don't know, last year, 18 months ago. And hopefully one of the things you see from these spreadsheets is that these recently invented things are still at the ridiculously extremely simple end of the spectrum. So the stuff that people are discovering in deep learning is a long, long, long, long way away from being incredibly complex or sophisticated. And so hopefully you'll find this very encouraging, which is if you want to play at the state of the art of deep learning, that's not at all hard to do. So let's look at Adam, which I remember it coming out 12-18 months ago, and everybody was so excited because suddenly it became so much easier and faster to train neural nets. But once I actually tried to create an Excel spreadsheet out of it, I realized, oh my god, it's just RMSProp to plus momentum. And so literally all I did was I copied my momentum page and then I copied across my RMSProp columns and combined them. So you can see here I have my exponentially weighted moving average of the gradients. Here is my exponentially weighted moving average of the squareds of the gradients. And so then when I calculate my new parameters, I take my old parameter and I subtract my derivative times the learning rate, but my momentum factor. So in other words, the recent weighted moving average of the gradients, multiplied by the learning rate, divided by the recent moving average of the square of the derivatives, or the root of them anyway. So it's literally just combining momentum plus RMSProp. And so let's see how that goes. Let's run 5epox, and we can use a pretty high learning rate now because it's really handling a lot of stuff for us. And 5epox, we're almost perfect. And so another 5epox, it does exactly the same thing that RMSProp does, which is it goes too far and tries to come back. So we need to do the same thing when we use Adam. And Adam is what I use all the time now. I just divide by 10 every time I see it flatten out. So a week ago, somebody came out with something that they called not Adam, but Eve. And Eve is an addition to Adam which attempts to deal with this learning rate annealing automatically. And so all of this is exactly the same as my Adam page. But at the bottom I've added some extra stuff. I have kept track of the root mean squared error, this is just my loss function. And then I copy across my loss function from my previous epoch and from the epoch before that. And what Eve does is it says how much has the loss function changed. And so it's got this ratio between the previous loss function and the loss function before that. So you can see it's the absolute value of the last one minus the one before, divided by whichever one is smaller. And what it says is, let's then adjust the learning rate such that instead of just using the learning rate we're given, let's adjust the learning rate that we're given by taking the exponentially weighted moving average of these ratios. You can see another of these beta's appearing here. So this thing here is equal to our last ratio times 0.9 plus our new ratio times 0.1. And so then for our learning rate, we divide the learning rate from Adam by this. So what that says is, if the learning rate is moving around a lot, if it's very bumpy, we should probably decrease the learning rate because it's going all over the place. Remember how we saw before, if we've kind of gone past where we want to get to, it just jumps up and down. On the other hand, if the loss function is staying pretty constant, then we probably want to increase the learning rate. So that all seems like a good idea. And so again, let's try it. Not bad, right? So after 5 epochs, it's kind of gone a little bit too far. After a week of playing with it, I use this on State Farm a lot during the week. I grabbed a Keras implementation which somebody wrote like a day after the paper came out. The problem is that because it can both decrease and increase the learning rate, sometimes as it gets down to the flat bottom point where it's pretty much optimal, it'll often be the case that the loss gets pretty constant at that point. And so therefore, Eve will try to increase the learning rate. And so what I tend to find happen is that it would very quickly get pretty close to the answer, and then suddenly it would jump to somewhere really awful. And then it would start to get to the answer again and jump somewhere really awful. Question from the audience. Generally, for the exit condition, we give a delta that the change in this gradient should be below a certain delta, then we just stop doing that, right? We have not done any such thing, no. We have always said run for a specific number of epochs. We have not defined any kind of stopping criterion. It is possible to define such a stopping criterion, but nobody's really come up with one that's remotely reliable. And the reason why is that when you look at the graph of loss over time, it doesn't tend to look like that, it tends to look like this. And so in practice, it's very hard to know when to stop. It's kind of still a human judgment thing. Oh yeah, that's definitely true. And particularly with a type of architecture called ResNet that we'll look at next week, the authors showed that it kind of tends to go like this. In practice, you kind of have to run your training for as long as you have patience for at whatever the best learning rate you can come up with is. So something I actually came up with 6 or 12 months ago, but we kind of re-stimulated my interest after I read this Adam paper, is something which dynamically updates learning rates in such a way that they only go down, and rather than using the loss function, which as I just said is incredibly bumpy, there's something else which is less bumpy, which is the average sum of squareds gradients. So I actually created a little spreadsheet of my idea and I hope to prototype it in Python maybe this week or the next week after. And the idea is basically this, keep track of the sum of the squareds of the derivatives and compare the sum of the squareds of the derivatives from the last epoch to the sum of the squareds of the derivatives of this epoch and look at the ratio of the two. The derivatives should keep going down. If they ever go up by too much, that would strongly suggest that you've kind of jumped out of the good part of the function. So any time they go up too much, you should decrease the learning rate. So I literally added like two lines of code to my incredibly simple VBA, Adam with a kneeling here. If the gradient ratio is greater than 2, so if it doubles, divide the learning rate by 4. Here's what happens when I run that. So I'm pretty interested in this idea. I think it's going to work super well because it allows me to focus on just running stuff without ever worrying about setting learning rates. So I'm hopeful that this approach to automatic learning rate and kneeling is something that we can have in our toolbox by the end of this course. One thing that happened to me today is I tried a lot of different learning rates and I didn't get anywhere. But I was working with the whole data set. Would trying with sample will actually, I'm trying to understand, if I try with a sample and I find something, would that apply to the whole data set or how do I go about investigating this? Answer that question. Here is the answer to that question. The question was, it takes a long time to figure out the optimal learning rate. Can we calculate it using just a sample? And to answer that question, I'm going to show you how I entered Statefarm. And indeed, when I started entering Statefarm, I started by using a sample. And so step 1 was to think, what insights can we gain from using a sample which can still apply when we move to the whole data set? Running stuff in a sample took 10 or 20 seconds, running stuff in the full data set took 2 to 10 minutes for an epoch. So after I created my sample, which I just created randomly, I first of all wanted to find out what does it take to create a better than random model here. So I always start with the simplest possible model. And so the simplest possible model has a single dense layer. Now here's a handy trick. Rather than worrying about calculating the average and standard deviation of the input and subtracting it all out in order to normalize your input layer, you can just start with a batch norm layer. And so if you start with a batch norm layer, it's going to do that for you. So anytime you create a Keras model from scratch, I would recommend making your first layer a batch norm layer. And so this is going to normalize the data for me. So that's a cool little trick which I haven't actually seen anybody use elsewhere, but I think it's a good default starting point all the time. If I'm going to use a dense layer, then obviously I have to flatten everything into a single vector first. So this is really a most minimal model. So I tried fitting it. I compiled it, fit it, and nothing happened. Not only did nothing happen to my validation, but really nothing happened to my training. It's only taking 7 seconds per epoch to find this out, so that's okay. So what might be going on? So I look at model.summary and I see that there's 1.5 million parameters. And that makes me think, okay, it's probably not under-fitting. It's probably unlikely that with 1.5 million parameters, there's really nothing useful it can do whatsoever. It's only a linear model, true, but I still think it should be able to do something. So that makes me think that what must be going on is it must be doing that thing where it jumps too far. And it's particularly easy to jump too far at the very start of training. Let me explain why. It turns out that there are often reasonably good answers that are way too easy to find. So one reasonably good answer would be always predict 0. Because there are 10 output classes in the state farm competition, there's one of 10 different types of distracted driving, and you are scored based on the cross-entropy loss. And what that's looking at is how accurate are each of your 10 predictions. So rather than trying to predict something well, what if we just always predict 0.01. Nine times out of 10, you're going to be right. Because 9 out of the 10 categories, it's not that. It's only one of the 10 categories. So actually always predicting 0.01 would be pretty good. Now it turns out it's not possible to do that because we have a softmax layer. And a softmax layer, remember, is e to the xi divided by sum of e to the xi's. And so in a softmax layer, everything has to add to 1. So therefore, if it makes one of the classes really high and all of the other ones really low, then 9 times out of 10, it is going to be right 9 times out of 10. So in other words, it's a pretty good answer for it to always predict some random class, class 8, close to 100% certainty. And that's what happens. So anybody who tried this, and I saw a lot of people on the forums this week saying, I tried to train it and nothing happened, and the folks who got the really interesting insight were the ones who then went on to say, and then I looked at my predictions and it kept predicting the same class with great confidence again and again and again. That's why I did that. So our next step then is to try decreasing the learning rate. So here is exactly the same model, but I'm now using a much lower learning rate. And when I run that, it's actually moving. So it's only 12 seconds of compute time to figure out that I'm going to have to start with a lower learning rate. Once we've got to a point where the accuracy is reasonably better than random, we're well away from that part of the loss function now that says always predict everything as the same class, and therefore we can now increase the learning rate back up again. So generally speaking, for these harder problems, you'll need to start at an epoch or two at a lower learning rate, and then you can increase it back up again. So you can see now I can put it back up to.01 and very quickly increasing my accuracy. So you can see here my accuracy on my validation set is.5 using a linear model. And this is a good starting point because it kind of says to me, anytime that my validation accuracy is worse than about.5, this is really no better than even a linear model, so this is not worth spending more time on. One obvious question would be, how do you decide how big a sample to use? And what I did was I tried a few different sizes of sample for my validation set, and I then said, evaluate the model, so in other words, calculate loss function, on the validation set, but for a whole bunch of randomly sampled batches, so do it 10 times. And so then I looked and I saw how the accuracy changed. With a validation set set at 1000 images, my accuracy changed from.48 or.47 to.51, so it's not changing too much. It's small enough that I think I can make useful insights using a sample size of this size. So what else can we learn from a sample? Well one is, are there other architectures that work well? So the obvious thing to do with a computer vision problem is to try a convolutional neural network. And here's one of the most simple convolutional neural networks. Two convolutional layers, each one with a max pooling layer, and then one dense layer followed by my dense output layer. So again I tried that and found that it very quickly got to an accuracy of 100% on the training set, but only 24% on the validation set. And that's because I was very careful to make sure my validation set included different drivers to my training set, because Oncago told us that the test set has different drivers. And so it's much harder to recognize what a driver is doing if we've never seen that driver before. So I could see that convolutional neural networks clearly are a great way to model this kind of data, but I've got to think very carefully about overfitting. So step 1 to avoiding overfitting is data augmentation, as we learned in our data augmentation class. So here's the exact same model, and I tried every type of data augmentation. So I tried shifting it left and right a bit. I tried shifting it up and down a bit. I tried sharing it a bit. I tried rotating it a bit. I tried shifting the channels, the colors a bit. And for each of those I tried 4 different levels, and I found in each case what was the best. And then I combined them all together. So here are my best data augmentation amounts. So on 1560 images, a very small set, this is just my sample, I then ran my very simple two convolutional layer model with this data augmentation at these optimized parameters. And it didn't look very good. After 5 epochs I only had 0.1 accuracy on my validation set. But I can see that my training set is continuing to improve. And so that makes me think, don't give up yet, try decreasing the learning rate and do a few more. And lo and behold, it started improving. So this is where you've got to be careful not to jump to conclusions too soon. So I ran a few more and it's improving well, so I ran a few more. Another 25. And look at what happened. It kept getting better and better and better until we were getting 67% accuracy. So this 1.15 validation loss is well within the top 50% in this competition. So using an incredibly simple model on just a sample, we can get in the top half of this Kaggle competition simply by using the right kind of data augmentation. So I think this is a really interesting insight about the power of this incredibly useful tool. Let's have a 5 minute break. And we'll do your question first. Question from the audience. Would a class imbalance affect? It's unlikely that there's going to be a class imbalance in my sample unless there was an equivalent class imbalance in the real data, because I've got 1000 examples. And statistically speaking, that's unlikely. If there was a class imbalance in my original data, then I want my sample to have that class imbalance too. So at this point, I felt pretty good that I knew that we should be using a convolutional neural network, which is obviously a very strong hypothesis to start with anyway. And also felt pretty confident when you knew what kind of learning rate to start with and then how to change it, and also what data augmentation to do. The next thing I wanted to wonder about was how else do I handle overfitting, because although I'm getting some pretty good results, I'm still overfitting hugely,.6 vs..9. So the next thing in our list of ways to avoid overfitting, and I hope you guys all remember that we have that list in Lesson 3. The 5 steps. Let's go and have a look at it now to remind ourselves. This is to reducing overfitting. These are the 5 steps. We can't add more data. We've tried using data augmentation. We're already using batch norm and conv nets. So the next step is to add regularization. And dropout is our kind of favored regularization technique. So I was thinking, okay, can we... Actually, before we do that, I'll just mention one more thing about this data augmentation approach. I have literally never seen anybody write down a process as to how to figure out what kind of data augmentation to use and the amount. The only posts I've seen on it always rely on intuition, which is basically like look at the images and think about how much they seem like they should be able to move around or rotate. I really tried this week to come up with a rigorous repeatable process that you could use. And that process is go through each data augmentation type one at a time, try 3 or 4 different levels of it on a sample with a big enough validation set that it's pretty stable, to find the best value of each of the data augmentation parameters, and then try combining them all together. So I hope you kind of come away with this as a practical message, which probably your colleagues, even if some of them claim to be deep learning experts, I doubt that they're doing this. So this is something you can hopefully get people into the practice of doing. Regularization however, we cannot do on a sample. And the reason why is that step one, add more data, that step is very correlated with add regularization. As we add more data, we need less regularization. So as we move from a sample to the full data set, we're going to need less regularization. So to figure out how much regularization to use, we have to use the whole data set. So at this point I changed it to use the whole data set, not the sample, and I started using dropout. So you can see that I started with my data augmentation amounts that you've already seen, and I started adding in some dropout and ran it for a few epochs to see what would happen. And you can see it's worked pretty well. So we're getting up into the 75% now and before we were in the 64%. So I haven't checked, once we add clipping, which is very important for getting the best cross-entropy loss function, I haven't checked where that would get us on the Kaggle leaderboard, but I'm pretty sure it would be at least in the top third based on this accuracy. I ran a few more epochs with an even lower learning rate and got 0.79. So this is going to be well up into the top third, maybe even the top quarter, probably the top third of the leaderboard. I got to this point by just trying out a couple of different levels of dropout, and I just put them in my dense layers. There's no rule of thumb here. A lot of people put small amounts of dropout in their convolutional layers as well. All I can say is to try things. But what VGG does is to put 50% dropout after each of its dense layers, and that doesn't seem like a bad rule of thumb. So that's what I was doing here, and then trying around a few different sizes of dense layers to try and find something reasonable. I didn't spend a heap of time on this, so there's probably better architectures, but as you can see, this is still a pretty good one. So that was my step 2. So far, we have not used a pre-trained network at all. So this is getting into the top third of the leaderboard without even using any ImageNet features. So that's pretty damn cool. But we're pretty sure that ImageNet features would be helpful. So that was the next step, to use ImageNet features, so VGG features. Specifically, I was reasonably confident that all of the convolutional layers of VGG are probably pretty much good enough. I didn't expect I would have to fine-tune them much, if at all, because the convolutional layers are the things which really look at the shape and structure of things rather than how they fit together. And these are photos of the real world, just like ImageNet are photos of the real world. So I really felt like most of the time, if not all of it, was likely to be spent on the dense layers. So therefore, because calculating the convolutional layers takes nearly all the time, because that's where all the computation is, I pre-computed the output of the convolutional layers. And we've done this before, you might remember. When we looked at dropout, we did exactly this. We figured out what was the last convolutional layer's ID, we grabbed all of the layers up to that ID, we built a model out of them, and then we calculated the output of that model. That told us the value of those features, those activations from VGG's last convolutional layer. So I did exactly the same thing. I basically copied and pasted that code. So I said, grab VGG16, find the last convolutional layer, build a model that contains everything up to and including that layer, predict the output of that model. So predicting the output means calculate the activations of that last convolutional layer. And since that takes some time, then save that, so I never have to do it again. So then in the future, I can just load that array. And so have a think about what would you expect the shape of this to be. And you can figure out what you would expect the shape to be by looking at model.summary and finding the last convolutional layer. Here it is. And we can see it is 512 filters by 14 by 14. So let's have a look, and we'll find our conv-val-fit.shape, 512 by 14 by 14, as expected. Is there a reason you chose to leave out the max pooling and flatten layers? Basically because it takes zero time to calculate them, and the max pooling layer loses information. So I thought, given that I might want to play around with other types of pooling or other types of convolutions, I thought precalculating this layer is the last one that takes a lot of computation time. Having said that, the first thing I did with it in my new model was to max pool it and flatten it. So now that I have the output of VGG for the last conv layer, I can now build a model that has dense layers on top of that. And so the input to this model will be the output of those conv layers. And the nice thing is it won't take long to run this, even on the whole dataset, because the dense layers don't take much computation time. So here's my model. By making p a parameter, I could try a wide range of dropout amounts. And I fit it, and one epoch takes 5 seconds on the entire dataset. So this is a super good way to play around. You can see one epoch gets me 0.65, three epochs gets me 0.75. So this is pretty cool. I have something that in 15 seconds can get me 0.75 accuracy. And notice here, I'm not using any data augmentation. Why aren't I using data augmentation? Because you can't precompute the output of convolutional layers if you're using data augmentation. Because with data augmentation, your convolutional layers give you a different output every time. So that's just a bit of a bummer. You can't use data augmentation if you are precomputing the output of a layer. Because every time it sees the same cat photo, it's rotating it by a different amount. So it gives a different output of the convolutional layer, so you can't precompute it. There is something you can do, which I played with a little bit, which is you could precompute something that's 10 times bigger than your dataset consisting of 10 different data augmented versions of it, which is why I actually had this data generator with augmentations. And I created something called data-augmented convolutional features in which I predicted 5 times the amount of data, and so that basically gave me a dataset 5 times bigger. And that actually worked pretty well. It's not as good as having a whole new sample every time, but it's kind of a compromise. So once I played around with these dense layers, I then did some more fine-tuning and found out that... I went basically here, I then tried saying, let's go through all of my layers in my model from 16 onwards and set them to trainable and see what happens. So I tried retraining, fine-tuning some of the convolutional layers as well. It basically didn't help. So I experimented with my hypothesis and I found it was correct, which is it seems that for this particular model, coming up with the right set of dense layers is what it's all about. Yes, Rachel? Question from the audience. There's a question, if we want rotational invariance, should we keep the max pooling or can another layer do it as well? Max pooling doesn't really have anything to do with rotational invariance. Max pooling does translation invariance. So I'm going to show you one more cool trick. I'm going to show you a little bit of State Farm every week from now on because there's so many cool things to try. And I want to keep reviewing CNNs because convolutional neural nets really are becoming what deep learning is all about. I'm going to show you one really cool trick. It's actually a combination of two tricks. The two tricks are called pseudo-labeling and knowledge distillation. If you Google for pseudo-labeling semi-supervised learning, you can see the original paper that came out with pseudo-labeling. I guess that's 2013. And then knowledge distillation. This is a Jeffrey Hinton paper, Distilling the Knowledge in a Neural Network. This is from 2015. So these are a couple of really cool techniques. We're going to combine them together. And they're kind of crazy. What we're going to do is we're going to use the test set to give us more information. In State Farm, the test set has 80,000 images in it, and the training set has 20,000 images in it. So what could we do with those 80,000 images which we don't have labels for? It seems a shame to waste them. It seems like we should be able to do something with them. And there's a great little picture here. Imagine we only had two points, and we knew their labels, white and black. And then somebody said, how would you label this? And then they told you that there's a whole lot of other unlabeled data. Notice this is all gray. It's not labeled. But it's helped us, hasn't it? It's helped us because it's told us how the data is structured. This is what semi-supervised learning is all about. It's all about using the unlabeled data to try and understand something about the structure of it and use that to help you, just like in this picture. Pseudo-labeling and knowledge distillation are a way to do this. And I'm not going to do it on the test set, I'm going to do it on the validation set because it's a little bit easier to see the impact of it. And maybe next week we'll look at the test set, because that's going to be much cooler when you're doing the test set. It's this simple. What we do is we take our model, some model we've already built, and we predict the outputs from that model for our unlabeled set. In this case I'm using the validation set as if it was unlabeled. So I'm ignoring the labels. And those things we call the pseudolabels. So now that we have predictions for the test set or the validation set, it's not that they're true, but we can pretend they're true. We can say they're some label, they're not correct labels, but they're labels nonetheless. So what we then do is we take our training labels and we concatenate them with our validation or test set pseudolabels. And so we now have a bunch of labels for all of our data. And so we can now also concatenate our convolutional features with the convolutional features of the validation set or test set. And we now use these to train a model. So the model we use is exactly the same model we had before, and we train it in exactly the same way as before. And our loss goes up from.75 to.82. So our error has dropped by like 25%. And the reason why is just because we use this additional unlabeled data to try to figure out the structure of it. Question about model choice. How do you learn how to design a model and when to stop messing with them? It seems like you've taken a few initial ideas, tweaked them to get higher accuracy, but unless your initial guesses are amazing, there should be plenty of architectures that would also work. If and when you figure out how to find an architecture and stop fucking with it, please tell me, because I don't sleep. We all want to know this. I look back at these models I'm showing you, and I'm thinking, I bet there's something like twice as good. I don't know what it is. There are all kinds of ways of optimizing other hyperparameters of deep learning. For example, there's something called Spearmint, which is a Bayesian optimization hyperparameter tuning thing. In fact just last week a new paper came out for hyperparameter tuning, but this is all about tuning things like the learning rate and stuff like that. Coming up with architectures, there are some people who have tried to come up with some kind of more general architectures, and we're going to look at one next week called Resnets, which seem to be pretty encouraging in that direction. I'll give you an example. Resnet is an architecture which won ImageNet in 2015. The author of Resnet, super smart guy, Kai Ming He from Microsoft said, the reason Resnet is so great is it lets us build very very very very deep networks. Indeed he showed a network with over 1000 layers and it was totally state of the art. Somebody else came along a few months ago and built wide Resnets with like 50 layers and easily beat Kai Ming He's best results. So the very author of the ImageNet winner completely got wrong the reason why his invention was good. So the idea that any of us have any idea how to create optimal architectures is totally totally wrong. We don't. So that's why I'm trying to show you what we know so far, which is like the processes you can use to build them without waiting forever. So in this case, doing your data augmentation on the small sample in a rigorous way, figuring out that probably the dense layers are where the action is at and pre-computing the input to them. These are the kinds of things that can keep you sane. I'm showing you the outcome of my last weeks playing with this. I can tell you that during this time I continually fell into the trap of running stuff on the whole network and all the way through and fiddling around with hyperparameters. And I'd have to stop myself and have a cup of tea and say, okay, is this really a good idea, is this really a good use of time. So we all do it. But not you anymore because you've been to this class. Can you run us through this one more time? I'm just a little confused. It feels like maybe we're using our validation set as part of our training program. I'm confused how it's not true. But look, we're not using the validation labels. Nowhere here does it say val underscore labels. So yeah, we are absolutely using our validation set, but we're using the validation set's inputs. And for our test set, we have the inputs. So next week I will show you this page again, and this time I'm going to use the test set. I just didn't have enough time to do it this time around. And hopefully we're going to see some great results. And when we do it on the test set, then you'll be really convinced that it's not using the labels because we don't have any labels. But you can see here, all it's doing is it's creating pseudo-labels by calculating what it thinks it ought to be based on the model that we just built with that 75% accuracy. And so then it's able to use the input data for the validation set in an intelligent way and therefore improve the accuracy. Question asked. What do you mean the same? Question asked. Yeah, it's using bn underscore model, and bn underscore model is the thing that we just fitted by using the training labels. This is bn model, this thing with this 0.755 accuracy. Semi-supervised works because you're giving it a model which already knows about a bunch of labels, but unsupervised wouldn't know. Unsupervised has nothing, that's right. Unsupervised learning is where you're trying to build a model when you have no labels at all. How many people here would be interested in hearing about unsupervised learning during this class? Okay, enough people, I should do that. I will add it. During the week, perhaps we can create a forum thread about unsupervised learning and I can learn about what you're interested in doing with it. Because many things that people think of as unsupervised problems actually aren't. So pseudo-labeling is insane and awesome. And we need the green box back. Question asked. There are a number of questions. So one is earlier you talked about learning about the structure of the data that you can learn from the validation set. Can you say more about that? I don't know, not really. Other than that picture I showed you before with the two little spirally things. And that picture was kind of showing how they clustered in a way that was higher dimension than what you can see when you just had to. So think about that Matt Zyler paper we saw or the Jason Yosinski visualization toolbox we saw. The layers learn shapes and textures and concepts. In that 80,000 test images of people driving in different distracted ways, there are lots of concepts there to learn about ways in which people drive in distracted ways, even though they're not labeled. So what we're doing is we're trying to learn better convolutional or dense features. That's what I mean by learning more. So the structure of the data here is basically what these pictures tend to look like. More importantly, in what ways do they differ? Because it's the ways that they differ that therefore must be related to how they're labeled. Question- Can you use your updated model to make new labels for the validation set? Answer- Yes, you can absolutely do pseudo-labeling on pseudo-labeling. And you should. And if I don't get sick of running this code, I will try it next week. Question- Could that introduce bias towards your validation set? Answer- No, because we don't have any validation labels. One of the tricky parameters in pseudo-labeling is in each batch, how much do I make it a mix of training versus pseudo. One of the big things that stopped me from getting the test set in this week is that Keras doesn't have a way of creating batches which have like 80% of this set and 20% of that set, which is really what I want. If I just pseudo-labeled the whole test set and then concatenated it, then 80% of my batches are going to be pseudo-labels. And generally speaking, the rule of thumb I've read is that somewhere around a quarter to a third of your mini-batches should be pseudo-labels. So I need to write some code basically to get Keras to generate batches which are a mix from two different places before I can do this properly. Question- Are your pseudo-labels only as good as the initial model you're beginning from? So do you need to have a particular accuracy? Answer- Yeah, your pseudo-labels are indeed as good as your model you're starting from. People have not studied this enough to know how sensitive it is to those initial labels. Question- Is there a rule of thumb about what accuracy level? Answer- No, this is too new. Just try it. My guess is that pseudo-labels will be useful regardless of what accuracy level you're at because it will make it better. As long as you are in a semi-supervised learning context, i.e. you have a lot of unlabeled data that you want to take advantage of. I really want to move on because I told you I wanted to get us down the path to NLP this week. It turns out that the path to NLP, strange as it sounds, starts with collaborative filtering. You will learn why next week. This week we are going to learn about collaborative filtering. Collaborative filtering is a way of doing recommender systems. I sent you guys an email today with a link to more information about collaborative filtering and recommender systems, so please read those links if you haven't already just to get a sense of what the problem we're solving here is. In short, what we're trying to do is to learn to predict who is going to like what and how much. For example, the $1 million Netflix price. What rating level will this person give this movie? If you're writing Amazon's recommender system to figure out what to show you on their home page, which products is this person likely to rate highly? If you're trying to figure out what stuff to show on a news feed, which articles is this person likely to enjoy reading? There's a lot of different ways of doing this, but broadly speaking, there are two main classifications of recommender systems. One is based on metadata, which is for example, this guy filled out a survey in which they said they liked action movies and sci-fi. We also have taken all of our movies and put them into genres, and here are all of our action sci-fi movies, so we'll use them. Broadly speaking, that would be a metadata-based approach. A collaborative filtering-based approach is very different. It says, let's find other people like you and find out what they liked and assume that you will like the same stuff. And specifically when we say people like you, we mean people who rated the same movies you've watched in a similar way. And that's called collaborative filtering. It turns out that in a large enough dataset, collaborative filtering is so much better than the metadata-based approaches that adding metadata doesn't even improve it at all. So when people in the Netflix prize actually went out to IMDB and sucked in additional data and tried to use that to make it better, at a certain point it didn't help. Once their collaborative filtering models were good enough, it didn't help. And that's because it's something I learned about 20 years ago when I used to do a lot of surveys and consulting. It turns out that asking people about their behavior is crap compared to actually looking at people's behavior. So let me show you what collaborative filtering looks like. What we're going to do is we're going to use a dataset called MovieLens. So you guys hopefully will be able to play around with this this week. Unfortunately, Rachel and I could not find any Kaggle competitions that were about recommend agenda systems and where the competitions were still open for entries. However there is something called MovieLens, which is a widely studied dataset in academia. Perhaps surprisingly, approaching or beating an academic state of the art is way easier than winning a Kaggle competition. In Kaggle competitions, lots and lots and lots of people look at that data and they try lots and lots and lots of things and they use a really pragmatic approach, whereas academic state of the arts are done by academics. So with that said, the MovieLens benchmarks are going to be much easier to beat than any Kaggle competition, but it's still interesting. So you can download MovieLens dataset from the MovieLens dataset website, and you'll see that there's one here recommended for new research with 20 million items in. Also conveniently, they have a small one with only 100,000 ratings. So you don't have to build a sample, they have already built a sample for you. So I am of course going to use a sample. So what I do is I read in ratings.csv. And as you'll see here, I've started using pandas. PD is PD for pandas. How many people here have tried pandas? Awesome. So those of you that don't, hopefully the peer-group pressure is kicking in. So pandas is a great way of dealing with structured data and you should use it. Reading a CSV file is this easy, showing the first few items is this easy, finding out how big it is, finding out how many users and movies there are, are all this easy. I wanted to play with this in Excel, because that's the only way I know how to teach. What I did was I grabbed the user ID by rating and grabbed the top 15 busiest movie-watching users, and then I grabbed the 15 most-watched movies, and then I created a crosstab of the two. And then I copied that into Excel. Here is the table I downloaded from MovieLens for the 15 busiest movie-watching users and the 15 most widely-watched movies. And here are the ratings. Here's the rating of user 14 for movie 27. Look at these guys. These 3 users have watched every single one of these movies. I'm probably one of them. I love movies. And these have been watched by every single one of these users. So user 14 kind of liked movie 27, loved movie 49, hated movie 51. So this guy really liked movie 49, didn't much like movie 57, so they may feel the same way about movie 27 as that user. That's the basic essence of collaborative filtering. We're going to try and automate it a little bit. And the way we're going to automate it is we're going to say, let's pretend for each movie we had 5 characteristics, which is like, is it sci-fi, is it action, is it dialogue heavy, is it new, and does it have Bruce Willis? And then we could have those 5 things for every user as well. Is this user somebody who likes sci-fi, action, dialogue, new movies, and Bruce Willis? And so what we could then do is multiply that set of user features with that set of movie features. If this person likes sci-fi and it's sci-fi, and they like action and it is action, then a high number will appear in here for this matrix product of these two vectors. And so this would be a cool way to build up a collaborative filtering system if only we could create these 5 items for every movie and for every user. Now because we don't actually know what 5 things are most important for users and what 5 things are most important for movies, we're instead going to learn them. And the way we learn them is the way we learn everything, which is we start by randomizing them and then we use gradient descent. So here are 5 random numbers for every movie, and here are 5 random numbers for every user, and in the middle is the dot product of that movie with that user. Once we have a good set of movie factors and user factors for each one, then each of these ratings will be similar to each of the observed ratings. And therefore this sum of squared errors will be low. Currently it is high. So we start with our random numbers, we start with a loss function of 40. So we now want to use gradient descent, and it turns out that every copy of Excel has a gradient descent solver in it. So we're going to go ahead and use it. It's called solver. And so we have to tell it what thing to minimize, so it's saying minimize this, and which things do we want to change, which is all of our factors, and then we set it to a minimum and we say solve. And then we can see in the bottom left, it is trying to make this better and better and better using gradient descent. Notice I'm not saying stochastic gradient descent. Stochastic gradient descent means it's doing a mini-batch at a mini-batch time. Gradient descent means it's doing the whole dataset each time. Excel uses gradient descent, not stochastic gradient descent. They give the same answer. You might also wonder why it's so damn slow. It's so damn slow because it doesn't know how to create analytical derivatives, so it's having to calculate the derivatives with finite differencing, which is slow. So here we've got a solution. It got it down to 5. That's pretty good. So we can see here that it predicted 5.14 and it was actually 5. It predicted 3.05 and it was actually 3. So it's done a really, really good job. It's a little bit too easy because there are 5 times that many user factors, movie factors and 5 times that many user factors. We've got nearly as many factors as we have things to calculate, so it's kind of over-specified, but the idea is there. There's one piece missing. The piece we're missing is that some users probably just like movies more than others, and some movies are probably just more liked than others. This.product does not allow us in any way to say this is an enthusiastic user or this is a popular movie. To do that, we have to add bias terms. So here is exactly the same spreadsheet, but I've added one more row to the movies part and one more column to the users part for our biases. And I've updated the formula so that as well as the matrix multiplication, it also is adding the user bias and the movie bias. So this is saying this is a very popular movie and this is a very enthusiastic user, for example. So now that we have a collaborative filtering plus bias, we can do gradient descent on that. So previously our gradient descent loss function was 5.6. We would expect it to be better with bias because we can really better specify what's going on. Let's try it. So again, we run solver, solve, and we let that zip along and we see what happens. So these things we're calculating are called latent factors. A latent factor is some factor that is influencing the outcome, but we don't quite know what it is. We're just assuming it's there. In fact, what happens is when people do collaborative filtering, they then go back and they draw graphs where they say, here are the movies that are scored highly on this latent factor and low on that latent factor. So they'll discover the Bruce Willis factor and the sci-fi factor and so forth. And so if you look at the Netflix Prize visualizations, you'll see these graphs people do. The way they do them is they literally do this. Not in Excel, because they're not that cool, but they calculate these latent factors and then they draw pictures of them and then they actually write the name of the movie on the graph. Anyway, 4.6, even better. So you can see that, in fact I also have an error here, because any time that my rating is empty, I really want to be setting this to empty as well, which means my parenthesis was in the wrong place. So I'm going to recalculate this with my error fixed up and see if we get a better answer. Question from the audience. Question from the audience. So we can look at some pictures next week, but during the week, Google for Netflix Prize visualizations and you'll see these pictures. And it really does work the way I described. It figures out what are the most interesting dimensions on which we can rate a movie. And things like level of action and sci-fi and dialogue driven are very important features it turns out. But rather than pre-specifying those features, we have definitely learned from this class that calculating features using gradient descent is going to give us better features than trying to engineer them by hand. Interesting that it feels crazy. Tell me next week if you find some particularly interesting things or if it still seems crazy and we can try to decrysify it a little bit. So let's do this in Keras. Now there's really only one main new concept we have to learn, which is we started out with data not in a crosstab form, but in this form. We have user ID, movie ID, rating triplets, and I crosstab them. Each of these rows is one feature of a movie, and each of these columns is one feature of a user. And so one of these sets of 5 is one set of features for a user. This is this user's latent factors. This is the thing I said at the start of class, which is there's nothing mathematically complicated about gradient descent. The hard part is unlearning the idea that this should be hard. Gradient descent just figures it out. I just wanted to point out that this you can think of as a smaller, more concise way to represent the movies and the users. In math, there's the concept of a matrix factorization. An SVD for example, which is where you basically take a big matrix and turn it into a small narrow one and a small thin one and multiply the two together. This is exactly what we're doing. Instead of having how user 14 rated every single movie, we just have 5 numbers that represents it, which is pretty cool. So earlier did you say that both the user features were random as well as the... Yes. I guess I'm in trouble relating to you. I thought usually we run gradient descent on something that has inputs that you know and here what do you know? What we know. That's what we know, the resulting ratings. So can you perhaps come up with the wrong... If one of the numbers was in the wrong spot, our loss function would be less good and therefore there would be a gradient from that weight to say you should make this weight a little higher or a little lower. So all the gradient descent is saying for every weight, if we make it a little higher, does it get better, or if we make it a little bit lower, does it get better. And then we keep making them a little bit higher and lower until we can't go any better. And we had to decide how to combine the weights. This was our architecture. So our architecture was, let's take a dot product of some assumed user feature and some assumed movie feature. And let's add in the second case some assumed bias term. So we had to build an architecture. And we built the architecture using common sense, which is to say this seems like a reasonable way of thinking about this. I'm going to show you a better architecture in a moment. In fact, we're running out of time, so let me jump into the better architecture. So I wanted to point out that there is something new we're going to have to learn here, which is how do you start with a numeric user ID and look up to find what is their 5 element latent factor matrix. Now remember, when we have user IDs like 1, 2 and 3, one way to specify them is using one-hot encoding. So one way to handle this situation would be if this was our user matrix, it was one-hot encoded, and then we had a factor matrix containing a whole bunch of random numbers. One way to do it would be to take a dot product or a matrix product of this and this. And what that would do would be for this one here, it would basically say, let's multiply that by this, it would grab the first column of the matrix. And this here would grab the second column of the matrix. And this here would grab the third column of the matrix. So one way to do this in Keras would be to represent our user IDs as one-hot encodings and to create a user factor matrix just as a regular matrix like this, and then take a matrix product. That's horribly slow because if we have 10,000 users, then this thing is 10,000 wide, and that's a really big matrix multiplication. All we're actually doing is saying for user ID number 1, take the first column. For user ID number 2, take the second column. For user ID number 3, take the third column. And so Keras has something which does this for us, and it's called an embedding layer. And embedding is literally something which takes an integer as an input and looks up and grabs the corresponding column as output. So it's doing exactly what we're seeing in this spreadsheet. Question 1. How do you deal with missing values? So if a user has not rated a particular movie? That's no problem. So missing values are just ignored. So if it's missing, I just set the loss to 0. And then how do you break up a training and test set? I broke up the training and test set randomly by grabbing random numbers and saying are they greater or less than 0.8 and then split my ratings into two groups based on that. So here it is. Here's our dot product. In Keras, there's one other thing. I'm going to stop using the sequential model in Keras and start using the functional model in Keras. I'll talk more about this next week, but you can read about it later in the week. There are two ways of creating models in Keras, the sequential and the functional. They do similar things, but the functional is much more flexible and it's going to be what we're going to need to use from now on. So this is going to look slightly unfamiliar, but the ideas are the same. So we create an input layer for a user, and then we say now create an embedding layer for N users, which is 671, and we want to create how many latent factors. I decided not to create 5, but to create 50. And then I create a movie input, and then I create a movie embedding with 50 factors, and then I say take the dot product of those, and that's our model. So now please compile the model, and now train it, taking the user ID and movie ID as input, the rating as the target, and run it for 6 epochs, and I get a 1.27 loss. This is with an RMSE loss. Notice that I'm not doing anything else clever, it's just that simple dot product. That gives me a 1.27. Here's how I add the bias. I use exactly the same kind of embedding inputs as before, and I've encapsulated them in a function. So my user and movie embeddings are the same. And then I create bias by simply creating an embedding with just a single output. And so then my new model is do a dot product, and then add the user bias, and add the movie bias, and try fitting that, and it takes me to a validation loss of 1.1. How is that going? Well, there are lots of sites on the Internet where you can find out benchmarks for movie lens, and on the 100,000 dataset, we're generally looking for RMSE of about 0.89. The best one here is 0.89, and this one, RMSE, and that's on the 1 million dataset. High 0.89s, low 0.9s would be state of the art according to these benchmarks. So we're on the right track, but we're not there yet. So let's try something better. Let's create a neural net. And a neural net does the same thing. We create a movie embedding and a user embedding, again with 50 factors, and this time we don't take a dot product, we just concatenate the two vectors together, stick one on the end or the other. And because we now have one big vector, we can create a neural net. Create a dense layer, add dropout, create an activation, compile it, and fit it. And after 5 epochs, we get something way better than state of the art. So we couldn't find anything better than about 0.89. So this whole notebook took me like half an hour to write, so I don't claim to be a collaborative filtering expert. But I think it's pretty cool that these things that were written by people that write collaborative filtering software for a living, that's what these websites are basically coming from, places that use LensKit, so LensKit is a piece of software for recommender systems, we have just killed their benchmark. And it took us 10 seconds to train. So I think that's pretty neat. We're right on time, so we're going to take one last question. So that was a very, very quick introduction to embeddings. As per usual in this class, I kind of stick the new stuff in at the end and say, go study it. So your job this week is to keep improving State Farm, hopefully win the new fisheries competition. By the way, in the last half hour, I just created this little notebook in which I basically copied the Dogs and Cats Redux competition into something which does the same thing with the fish data, and I quickly submitted a result. So we currently have one of us in 18th place, so hopefully you can beat that tomorrow. Most importantly, download the MovieLens data and have a play with that, and we'll talk more about embeddings next week. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.48, "text": " I guess I noticed during the week from some of the questions I've been seeing that the", "tokens": [286, 2041, 286, 5694, 1830, 264, 1243, 490, 512, 295, 264, 1651, 286, 600, 668, 2577, 300, 264], "temperature": 0.0, "avg_logprob": -0.1358037465055224, "compression_ratio": 1.5591397849462365, "no_speech_prob": 0.0061912755481898785}, {"id": 1, "seek": 0, "start": 6.48, "end": 15.4, "text": " idea of what a convolution is is still a little counterintuitive or surprising to some people.", "tokens": [1558, 295, 437, 257, 45216, 307, 307, 920, 257, 707, 5682, 686, 48314, 420, 8830, 281, 512, 561, 13], "temperature": 0.0, "avg_logprob": -0.1358037465055224, "compression_ratio": 1.5591397849462365, "no_speech_prob": 0.0061912755481898785}, {"id": 2, "seek": 0, "start": 15.4, "end": 20.78, "text": " So I feel like the only way I know to teach things effectively is by creating a spreadsheet.", "tokens": [407, 286, 841, 411, 264, 787, 636, 286, 458, 281, 2924, 721, 8659, 307, 538, 4084, 257, 27733, 13], "temperature": 0.0, "avg_logprob": -0.1358037465055224, "compression_ratio": 1.5591397849462365, "no_speech_prob": 0.0061912755481898785}, {"id": 3, "seek": 0, "start": 20.78, "end": 23.44, "text": " So here we are.", "tokens": [407, 510, 321, 366, 13], "temperature": 0.0, "avg_logprob": -0.1358037465055224, "compression_ratio": 1.5591397849462365, "no_speech_prob": 0.0061912755481898785}, {"id": 4, "seek": 2344, "start": 23.44, "end": 47.52, "text": " This is the famous number 7 from Lesson 0.", "tokens": [639, 307, 264, 4618, 1230, 1614, 490, 18649, 266, 1958, 13], "temperature": 0.0, "avg_logprob": -0.24971657139914377, "compression_ratio": 1.0666666666666667, "no_speech_prob": 5.4754887969465926e-05}, {"id": 5, "seek": 2344, "start": 47.52, "end": 50.24, "text": " I wanted to show you exactly what a convolution does.", "tokens": [286, 1415, 281, 855, 291, 2293, 437, 257, 45216, 775, 13], "temperature": 0.0, "avg_logprob": -0.24971657139914377, "compression_ratio": 1.0666666666666667, "no_speech_prob": 5.4754887969465926e-05}, {"id": 6, "seek": 5024, "start": 50.24, "end": 57.68, "text": " And specifically what a convolution does in a deep learning neural network.", "tokens": [400, 4682, 437, 257, 45216, 775, 294, 257, 2452, 2539, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.14385543961122813, "compression_ratio": 1.6721311475409837, "no_speech_prob": 4.133315815124661e-05}, {"id": 7, "seek": 5024, "start": 57.68, "end": 64.74000000000001, "text": " So we are generally using modern convolutions, and that means a 3x3 convolution.", "tokens": [407, 321, 366, 5101, 1228, 4363, 3754, 15892, 11, 293, 300, 1355, 257, 805, 87, 18, 45216, 13], "temperature": 0.0, "avg_logprob": -0.14385543961122813, "compression_ratio": 1.6721311475409837, "no_speech_prob": 4.133315815124661e-05}, {"id": 8, "seek": 5024, "start": 64.74000000000001, "end": 68.24000000000001, "text": " So here is a 3x3 convolution.", "tokens": [407, 510, 307, 257, 805, 87, 18, 45216, 13], "temperature": 0.0, "avg_logprob": -0.14385543961122813, "compression_ratio": 1.6721311475409837, "no_speech_prob": 4.133315815124661e-05}, {"id": 9, "seek": 5024, "start": 68.24000000000001, "end": 73.78, "text": " And I have just randomly generated 9 random numbers.", "tokens": [400, 286, 362, 445, 16979, 10833, 1722, 4974, 3547, 13], "temperature": 0.0, "avg_logprob": -0.14385543961122813, "compression_ratio": 1.6721311475409837, "no_speech_prob": 4.133315815124661e-05}, {"id": 10, "seek": 5024, "start": 73.78, "end": 76.76, "text": " So that is a filter.", "tokens": [407, 300, 307, 257, 6608, 13], "temperature": 0.0, "avg_logprob": -0.14385543961122813, "compression_ratio": 1.6721311475409837, "no_speech_prob": 4.133315815124661e-05}, {"id": 11, "seek": 5024, "start": 76.76, "end": 77.76, "text": " There's one filter.", "tokens": [821, 311, 472, 6608, 13], "temperature": 0.0, "avg_logprob": -0.14385543961122813, "compression_ratio": 1.6721311475409837, "no_speech_prob": 4.133315815124661e-05}, {"id": 12, "seek": 5024, "start": 77.76, "end": 79.4, "text": " Here is my second filter.", "tokens": [1692, 307, 452, 1150, 6608, 13], "temperature": 0.0, "avg_logprob": -0.14385543961122813, "compression_ratio": 1.6721311475409837, "no_speech_prob": 4.133315815124661e-05}, {"id": 13, "seek": 7940, "start": 79.4, "end": 85.12, "text": " It is 9 more random numbers.", "tokens": [467, 307, 1722, 544, 4974, 3547, 13], "temperature": 0.0, "avg_logprob": -0.20310307532241664, "compression_ratio": 1.6238095238095238, "no_speech_prob": 3.944149284507148e-05}, {"id": 14, "seek": 7940, "start": 85.12, "end": 90.08000000000001, "text": " This is what we do in Keras when we ask for a convolutional layer.", "tokens": [639, 307, 437, 321, 360, 294, 591, 6985, 562, 321, 1029, 337, 257, 45216, 304, 4583, 13], "temperature": 0.0, "avg_logprob": -0.20310307532241664, "compression_ratio": 1.6238095238095238, "no_speech_prob": 3.944149284507148e-05}, {"id": 15, "seek": 7940, "start": 90.08000000000001, "end": 94.80000000000001, "text": " We tell it, the first thing we pass it is how many filters do we want.", "tokens": [492, 980, 309, 11, 264, 700, 551, 321, 1320, 309, 307, 577, 867, 15995, 360, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.20310307532241664, "compression_ratio": 1.6238095238095238, "no_speech_prob": 3.944149284507148e-05}, {"id": 16, "seek": 7940, "start": 94.80000000000001, "end": 100.96000000000001, "text": " And that's how many of these random matrices do we want it to build for us.", "tokens": [400, 300, 311, 577, 867, 295, 613, 4974, 32284, 360, 321, 528, 309, 281, 1322, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.20310307532241664, "compression_ratio": 1.6238095238095238, "no_speech_prob": 3.944149284507148e-05}, {"id": 17, "seek": 7940, "start": 100.96000000000001, "end": 106.76, "text": " So in this case, it's as if I passed convolution2d, the first parameter would be 2, and the second", "tokens": [407, 294, 341, 1389, 11, 309, 311, 382, 498, 286, 4678, 45216, 17, 67, 11, 264, 700, 13075, 576, 312, 568, 11, 293, 264, 1150], "temperature": 0.0, "avg_logprob": -0.20310307532241664, "compression_ratio": 1.6238095238095238, "no_speech_prob": 3.944149284507148e-05}, {"id": 18, "seek": 10676, "start": 106.76, "end": 111.32000000000001, "text": " parameter would be 3,3, because it's a 3x3.", "tokens": [13075, 576, 312, 805, 11, 18, 11, 570, 309, 311, 257, 805, 87, 18, 13], "temperature": 0.0, "avg_logprob": -0.15255255926223027, "compression_ratio": 1.3734177215189873, "no_speech_prob": 8.398003046750091e-06}, {"id": 19, "seek": 10676, "start": 111.32000000000001, "end": 115.16000000000001, "text": " So what happens to this little random matrix?", "tokens": [407, 437, 2314, 281, 341, 707, 4974, 8141, 30], "temperature": 0.0, "avg_logprob": -0.15255255926223027, "compression_ratio": 1.3734177215189873, "no_speech_prob": 8.398003046750091e-06}, {"id": 20, "seek": 10676, "start": 115.16000000000001, "end": 128.4, "text": " In order to calculate the very first item, it takes the sum of the blue stuff times the", "tokens": [682, 1668, 281, 8873, 264, 588, 700, 3174, 11, 309, 2516, 264, 2408, 295, 264, 3344, 1507, 1413, 264], "temperature": 0.0, "avg_logprob": -0.15255255926223027, "compression_ratio": 1.3734177215189873, "no_speech_prob": 8.398003046750091e-06}, {"id": 21, "seek": 10676, "start": 128.4, "end": 133.48000000000002, "text": " red stuff, those 9, all added together.", "tokens": [2182, 1507, 11, 729, 1722, 11, 439, 3869, 1214, 13], "temperature": 0.0, "avg_logprob": -0.15255255926223027, "compression_ratio": 1.3734177215189873, "no_speech_prob": 8.398003046750091e-06}, {"id": 22, "seek": 13348, "start": 133.48, "end": 138.0, "text": " So let's go down here and do where it gets a bit darker.", "tokens": [407, 718, 311, 352, 760, 510, 293, 360, 689, 309, 2170, 257, 857, 12741, 13], "temperature": 0.0, "avg_logprob": -0.18965062555277123, "compression_ratio": 1.6484018264840183, "no_speech_prob": 8.397994861297775e-06}, {"id": 23, "seek": 13348, "start": 138.0, "end": 145.07999999999998, "text": " This is equal to these 9 times these 9, and when I say times, I mean element-wise times,", "tokens": [639, 307, 2681, 281, 613, 1722, 1413, 613, 1722, 11, 293, 562, 286, 584, 1413, 11, 286, 914, 4478, 12, 3711, 1413, 11], "temperature": 0.0, "avg_logprob": -0.18965062555277123, "compression_ratio": 1.6484018264840183, "no_speech_prob": 8.397994861297775e-06}, {"id": 24, "seek": 13348, "start": 145.07999999999998, "end": 149.35999999999999, "text": " so the top left by the top left, the middle by the middle, and so forth, and add them", "tokens": [370, 264, 1192, 1411, 538, 264, 1192, 1411, 11, 264, 2808, 538, 264, 2808, 11, 293, 370, 5220, 11, 293, 909, 552], "temperature": 0.0, "avg_logprob": -0.18965062555277123, "compression_ratio": 1.6484018264840183, "no_speech_prob": 8.397994861297775e-06}, {"id": 25, "seek": 13348, "start": 149.35999999999999, "end": 152.95999999999998, "text": " all together.", "tokens": [439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.18965062555277123, "compression_ratio": 1.6484018264840183, "no_speech_prob": 8.397994861297775e-06}, {"id": 26, "seek": 13348, "start": 152.95999999999998, "end": 153.95999999999998, "text": " That's all a convolution is.", "tokens": [663, 311, 439, 257, 45216, 307, 13], "temperature": 0.0, "avg_logprob": -0.18965062555277123, "compression_ratio": 1.6484018264840183, "no_speech_prob": 8.397994861297775e-06}, {"id": 27, "seek": 13348, "start": 153.95999999999998, "end": 162.44, "text": " So it's just as you go through, we take the corresponding 3x3 area in the image and we", "tokens": [407, 309, 311, 445, 382, 291, 352, 807, 11, 321, 747, 264, 11760, 805, 87, 18, 1859, 294, 264, 3256, 293, 321], "temperature": 0.0, "avg_logprob": -0.18965062555277123, "compression_ratio": 1.6484018264840183, "no_speech_prob": 8.397994861297775e-06}, {"id": 28, "seek": 16244, "start": 162.44, "end": 167.96, "text": " multiply each of those 9 things by each of these 9 things, and then we add those 9 products", "tokens": [12972, 1184, 295, 729, 1722, 721, 538, 1184, 295, 613, 1722, 721, 11, 293, 550, 321, 909, 729, 1722, 3383], "temperature": 0.0, "avg_logprob": -0.1430168050400754, "compression_ratio": 1.6081081081081081, "no_speech_prob": 8.139646524796262e-06}, {"id": 29, "seek": 16244, "start": 167.96, "end": 168.96, "text": " together.", "tokens": [1214, 13], "temperature": 0.0, "avg_logprob": -0.1430168050400754, "compression_ratio": 1.6081081081081081, "no_speech_prob": 8.139646524796262e-06}, {"id": 30, "seek": 16244, "start": 168.96, "end": 172.92, "text": " That's it, that's a convolution.", "tokens": [663, 311, 309, 11, 300, 311, 257, 45216, 13], "temperature": 0.0, "avg_logprob": -0.1430168050400754, "compression_ratio": 1.6081081081081081, "no_speech_prob": 8.139646524796262e-06}, {"id": 31, "seek": 16244, "start": 172.92, "end": 179.2, "text": " So there's really nothing particularly weird or confusing about it, and I'll make this", "tokens": [407, 456, 311, 534, 1825, 4098, 3657, 420, 13181, 466, 309, 11, 293, 286, 603, 652, 341], "temperature": 0.0, "avg_logprob": -0.1430168050400754, "compression_ratio": 1.6081081081081081, "no_speech_prob": 8.139646524796262e-06}, {"id": 32, "seek": 16244, "start": 179.2, "end": 181.88, "text": " available in class so you can have a look.", "tokens": [2435, 294, 1508, 370, 291, 393, 362, 257, 574, 13], "temperature": 0.0, "avg_logprob": -0.1430168050400754, "compression_ratio": 1.6081081081081081, "no_speech_prob": 8.139646524796262e-06}, {"id": 33, "seek": 16244, "start": 181.88, "end": 190.0, "text": " You can see that when I get to the top left corner, I can't move further left and up because", "tokens": [509, 393, 536, 300, 562, 286, 483, 281, 264, 1192, 1411, 4538, 11, 286, 393, 380, 1286, 3052, 1411, 293, 493, 570], "temperature": 0.0, "avg_logprob": -0.1430168050400754, "compression_ratio": 1.6081081081081081, "no_speech_prob": 8.139646524796262e-06}, {"id": 34, "seek": 19000, "start": 190.0, "end": 197.08, "text": " I've reached the edge, and this is why when you do a 3x3 convolution without zero padding,", "tokens": [286, 600, 6488, 264, 4691, 11, 293, 341, 307, 983, 562, 291, 360, 257, 805, 87, 18, 45216, 1553, 4018, 39562, 11], "temperature": 0.0, "avg_logprob": -0.12090041420676491, "compression_ratio": 1.5637254901960784, "no_speech_prob": 1.0289424608345143e-05}, {"id": 35, "seek": 19000, "start": 197.08, "end": 204.64, "text": " you lose one pixel on each edge because you can't push this 3x3 any further.", "tokens": [291, 3624, 472, 19261, 322, 1184, 4691, 570, 291, 393, 380, 2944, 341, 805, 87, 18, 604, 3052, 13], "temperature": 0.0, "avg_logprob": -0.12090041420676491, "compression_ratio": 1.5637254901960784, "no_speech_prob": 1.0289424608345143e-05}, {"id": 36, "seek": 19000, "start": 204.64, "end": 211.44, "text": " So if we go down to the bottom left, you can see again the same thing.", "tokens": [407, 498, 321, 352, 760, 281, 264, 2767, 1411, 11, 291, 393, 536, 797, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.12090041420676491, "compression_ratio": 1.5637254901960784, "no_speech_prob": 1.0289424608345143e-05}, {"id": 37, "seek": 19000, "start": 211.44, "end": 219.28, "text": " So that's why you can see that my result is one row less than my starting point.", "tokens": [407, 300, 311, 983, 291, 393, 536, 300, 452, 1874, 307, 472, 5386, 1570, 813, 452, 2891, 935, 13], "temperature": 0.0, "avg_logprob": -0.12090041420676491, "compression_ratio": 1.5637254901960784, "no_speech_prob": 1.0289424608345143e-05}, {"id": 38, "seek": 21928, "start": 219.28, "end": 221.8, "text": " So I did this for 2 different filters.", "tokens": [407, 286, 630, 341, 337, 568, 819, 15995, 13], "temperature": 0.0, "avg_logprob": -0.182992742519186, "compression_ratio": 1.5972850678733033, "no_speech_prob": 6.7479904828360304e-06}, {"id": 39, "seek": 21928, "start": 221.8, "end": 228.68, "text": " So here's my second filter, and you can see when I calculate this one, it's exactly the", "tokens": [407, 510, 311, 452, 1150, 6608, 11, 293, 291, 393, 536, 562, 286, 8873, 341, 472, 11, 309, 311, 2293, 264], "temperature": 0.0, "avg_logprob": -0.182992742519186, "compression_ratio": 1.5972850678733033, "no_speech_prob": 6.7479904828360304e-06}, {"id": 40, "seek": 21928, "start": 228.68, "end": 229.68, "text": " same thing.", "tokens": [912, 551, 13], "temperature": 0.0, "avg_logprob": -0.182992742519186, "compression_ratio": 1.5972850678733033, "no_speech_prob": 6.7479904828360304e-06}, {"id": 41, "seek": 21928, "start": 229.68, "end": 235.6, "text": " It's these 9 times each of these 9 added together.", "tokens": [467, 311, 613, 1722, 1413, 1184, 295, 613, 1722, 3869, 1214, 13], "temperature": 0.0, "avg_logprob": -0.182992742519186, "compression_ratio": 1.5972850678733033, "no_speech_prob": 6.7479904828360304e-06}, {"id": 42, "seek": 21928, "start": 235.6, "end": 239.28, "text": " These are just 9 other random numbers.", "tokens": [1981, 366, 445, 1722, 661, 4974, 3547, 13], "temperature": 0.0, "avg_logprob": -0.182992742519186, "compression_ratio": 1.5972850678733033, "no_speech_prob": 6.7479904828360304e-06}, {"id": 43, "seek": 21928, "start": 239.28, "end": 242.36, "text": " So that's how we start with our first.", "tokens": [407, 300, 311, 577, 321, 722, 365, 527, 700, 13], "temperature": 0.0, "avg_logprob": -0.182992742519186, "compression_ratio": 1.5972850678733033, "no_speech_prob": 6.7479904828360304e-06}, {"id": 44, "seek": 21928, "start": 242.36, "end": 247.84, "text": " In this case, I've created 2 convolutional filters, and this is the output of those 2", "tokens": [682, 341, 1389, 11, 286, 600, 2942, 568, 45216, 304, 15995, 11, 293, 341, 307, 264, 5598, 295, 729, 568], "temperature": 0.0, "avg_logprob": -0.182992742519186, "compression_ratio": 1.5972850678733033, "no_speech_prob": 6.7479904828360304e-06}, {"id": 45, "seek": 24784, "start": 247.84, "end": 251.36, "text": " convolutional filters.", "tokens": [45216, 304, 15995, 13], "temperature": 0.0, "avg_logprob": -0.22121265956333705, "compression_ratio": 1.5, "no_speech_prob": 8.801061085250694e-06}, {"id": 46, "seek": 24784, "start": 251.36, "end": 253.04, "text": " So my second layer.", "tokens": [407, 452, 1150, 4583, 13], "temperature": 0.0, "avg_logprob": -0.22121265956333705, "compression_ratio": 1.5, "no_speech_prob": 8.801061085250694e-06}, {"id": 47, "seek": 24784, "start": 253.04, "end": 260.32, "text": " Now my second layer is no longer enough just to have a 3x3 matrix, and I now need a 3x3x2", "tokens": [823, 452, 1150, 4583, 307, 572, 2854, 1547, 445, 281, 362, 257, 805, 87, 18, 8141, 11, 293, 286, 586, 643, 257, 805, 87, 18, 87, 17], "temperature": 0.0, "avg_logprob": -0.22121265956333705, "compression_ratio": 1.5, "no_speech_prob": 8.801061085250694e-06}, {"id": 48, "seek": 24784, "start": 260.32, "end": 271.4, "text": " tensor because to calculate my top left of my second convolutional layer, I need these", "tokens": [40863, 570, 281, 8873, 452, 1192, 1411, 295, 452, 1150, 45216, 304, 4583, 11, 286, 643, 613], "temperature": 0.0, "avg_logprob": -0.22121265956333705, "compression_ratio": 1.5, "no_speech_prob": 8.801061085250694e-06}, {"id": 49, "seek": 27140, "start": 271.4, "end": 281.35999999999996, "text": " 9 by these 9 added together plus these 9 by these 9 added together.", "tokens": [1722, 538, 613, 1722, 3869, 1214, 1804, 613, 1722, 538, 613, 1722, 3869, 1214, 13], "temperature": 0.0, "avg_logprob": -0.15198425010398584, "compression_ratio": 1.5668449197860963, "no_speech_prob": 3.4465629141777754e-06}, {"id": 50, "seek": 27140, "start": 281.35999999999996, "end": 287.4, "text": " At this point, my previous layer is no longer just one thing, but it's 2 things.", "tokens": [1711, 341, 935, 11, 452, 3894, 4583, 307, 572, 2854, 445, 472, 551, 11, 457, 309, 311, 568, 721, 13], "temperature": 0.0, "avg_logprob": -0.15198425010398584, "compression_ratio": 1.5668449197860963, "no_speech_prob": 3.4465629141777754e-06}, {"id": 51, "seek": 27140, "start": 287.4, "end": 294.91999999999996, "text": " Now indeed, if our original picture was a 3-channel color picture, our very first convolutional", "tokens": [823, 6451, 11, 498, 527, 3380, 3036, 390, 257, 805, 12, 339, 11444, 2017, 3036, 11, 527, 588, 700, 45216, 304], "temperature": 0.0, "avg_logprob": -0.15198425010398584, "compression_ratio": 1.5668449197860963, "no_speech_prob": 3.4465629141777754e-06}, {"id": 52, "seek": 27140, "start": 294.91999999999996, "end": 301.08, "text": " layer would have had to have been 3x3x3 tensors.", "tokens": [4583, 576, 362, 632, 281, 362, 668, 805, 87, 18, 87, 18, 10688, 830, 13], "temperature": 0.0, "avg_logprob": -0.15198425010398584, "compression_ratio": 1.5668449197860963, "no_speech_prob": 3.4465629141777754e-06}, {"id": 53, "seek": 30108, "start": 301.08, "end": 308.84, "text": " So all of the convolutional layers from now on are going to be 3x3x number of filters", "tokens": [407, 439, 295, 264, 45216, 304, 7914, 490, 586, 322, 366, 516, 281, 312, 805, 87, 18, 87, 1230, 295, 15995], "temperature": 0.0, "avg_logprob": -0.13432098442400006, "compression_ratio": 1.53125, "no_speech_prob": 1.1843021638924256e-05}, {"id": 54, "seek": 30108, "start": 308.84, "end": 313.96, "text": " in the previous layer convolution matrices.", "tokens": [294, 264, 3894, 4583, 45216, 32284, 13], "temperature": 0.0, "avg_logprob": -0.13432098442400006, "compression_ratio": 1.53125, "no_speech_prob": 1.1843021638924256e-05}, {"id": 55, "seek": 30108, "start": 313.96, "end": 324.32, "text": " So here's my first 3x3x2 tensor, and you can see it's taking 9 from here, 9 from here,", "tokens": [407, 510, 311, 452, 700, 805, 87, 18, 87, 17, 40863, 11, 293, 291, 393, 536, 309, 311, 1940, 1722, 490, 510, 11, 1722, 490, 510, 11], "temperature": 0.0, "avg_logprob": -0.13432098442400006, "compression_ratio": 1.53125, "no_speech_prob": 1.1843021638924256e-05}, {"id": 56, "seek": 30108, "start": 324.32, "end": 326.91999999999996, "text": " and adding those 2 together.", "tokens": [293, 5127, 729, 568, 1214, 13], "temperature": 0.0, "avg_logprob": -0.13432098442400006, "compression_ratio": 1.53125, "no_speech_prob": 1.1843021638924256e-05}, {"id": 57, "seek": 32692, "start": 326.92, "end": 333.2, "text": " And so then for my second filter in my second layer, it's exactly the same thing.", "tokens": [400, 370, 550, 337, 452, 1150, 6608, 294, 452, 1150, 4583, 11, 309, 311, 2293, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.24448839823404947, "compression_ratio": 1.5739644970414202, "no_speech_prob": 1.0289425517839845e-05}, {"id": 58, "seek": 32692, "start": 333.2, "end": 340.44, "text": " I've created 2 more random matrices, or 1 more random 3x3x2 tensor, and here again I", "tokens": [286, 600, 2942, 568, 544, 4974, 32284, 11, 420, 502, 544, 4974, 805, 87, 18, 87, 17, 40863, 11, 293, 510, 797, 286], "temperature": 0.0, "avg_logprob": -0.24448839823404947, "compression_ratio": 1.5739644970414202, "no_speech_prob": 1.0289425517839845e-05}, {"id": 59, "seek": 32692, "start": 340.44, "end": 350.84000000000003, "text": " have those 3 by these 9, by these 9, sum, plus those 9 by those 9, sum.", "tokens": [362, 729, 805, 538, 613, 1722, 11, 538, 613, 1722, 11, 2408, 11, 1804, 729, 1722, 538, 729, 1722, 11, 2408, 13], "temperature": 0.0, "avg_logprob": -0.24448839823404947, "compression_ratio": 1.5739644970414202, "no_speech_prob": 1.0289425517839845e-05}, {"id": 60, "seek": 32692, "start": 350.84000000000003, "end": 354.04, "text": " And that gives me that one.", "tokens": [400, 300, 2709, 385, 300, 472, 13], "temperature": 0.0, "avg_logprob": -0.24448839823404947, "compression_ratio": 1.5739644970414202, "no_speech_prob": 1.0289425517839845e-05}, {"id": 61, "seek": 35404, "start": 354.04, "end": 360.48, "text": " So that gives me my first 2 layers of my convolutional neural network.", "tokens": [407, 300, 2709, 385, 452, 700, 568, 7914, 295, 452, 45216, 304, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.15187600601551143, "compression_ratio": 1.5257731958762886, "no_speech_prob": 2.9023040042375214e-06}, {"id": 62, "seek": 35404, "start": 360.48, "end": 363.04, "text": " Then I do max pooling.", "tokens": [1396, 286, 360, 11469, 7005, 278, 13], "temperature": 0.0, "avg_logprob": -0.15187600601551143, "compression_ratio": 1.5257731958762886, "no_speech_prob": 2.9023040042375214e-06}, {"id": 63, "seek": 35404, "start": 363.04, "end": 368.0, "text": " Max pooling is slightly more awkward to do in Excel, but that's fine, we can still handle", "tokens": [7402, 7005, 278, 307, 4748, 544, 11411, 281, 360, 294, 19060, 11, 457, 300, 311, 2489, 11, 321, 393, 920, 4813], "temperature": 0.0, "avg_logprob": -0.15187600601551143, "compression_ratio": 1.5257731958762886, "no_speech_prob": 2.9023040042375214e-06}, {"id": 64, "seek": 35404, "start": 368.0, "end": 369.88, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.15187600601551143, "compression_ratio": 1.5257731958762886, "no_speech_prob": 2.9023040042375214e-06}, {"id": 65, "seek": 35404, "start": 369.88, "end": 371.24, "text": " So here's max pooling.", "tokens": [407, 510, 311, 11469, 7005, 278, 13], "temperature": 0.0, "avg_logprob": -0.15187600601551143, "compression_ratio": 1.5257731958762886, "no_speech_prob": 2.9023040042375214e-06}, {"id": 66, "seek": 35404, "start": 371.24, "end": 380.40000000000003, "text": " So max pooling is now going to decrease the resolution of my image by 2 on each axis.", "tokens": [407, 11469, 7005, 278, 307, 586, 516, 281, 11514, 264, 8669, 295, 452, 3256, 538, 568, 322, 1184, 10298, 13], "temperature": 0.0, "avg_logprob": -0.15187600601551143, "compression_ratio": 1.5257731958762886, "no_speech_prob": 2.9023040042375214e-06}, {"id": 67, "seek": 38040, "start": 380.4, "end": 384.2, "text": " So how do we calculate that number?", "tokens": [407, 577, 360, 321, 8873, 300, 1230, 30], "temperature": 0.0, "avg_logprob": -0.11968910510723407, "compression_ratio": 1.5570469798657718, "no_speech_prob": 3.5008379200007766e-06}, {"id": 68, "seek": 38040, "start": 384.2, "end": 389.32, "text": " That number is simply the maximum of those 4.", "tokens": [663, 1230, 307, 2935, 264, 6674, 295, 729, 1017, 13], "temperature": 0.0, "avg_logprob": -0.11968910510723407, "compression_ratio": 1.5570469798657718, "no_speech_prob": 3.5008379200007766e-06}, {"id": 69, "seek": 38040, "start": 389.32, "end": 394.96, "text": " And then that number is the maximum of those 4, and so forth.", "tokens": [400, 550, 300, 1230, 307, 264, 6674, 295, 729, 1017, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.11968910510723407, "compression_ratio": 1.5570469798657718, "no_speech_prob": 3.5008379200007766e-06}, {"id": 70, "seek": 38040, "start": 394.96, "end": 403.52, "text": " So with max pooling, we had 2 filters in the previous layer, so we still have 2 filters,", "tokens": [407, 365, 11469, 7005, 278, 11, 321, 632, 568, 15995, 294, 264, 3894, 4583, 11, 370, 321, 920, 362, 568, 15995, 11], "temperature": 0.0, "avg_logprob": -0.11968910510723407, "compression_ratio": 1.5570469798657718, "no_speech_prob": 3.5008379200007766e-06}, {"id": 71, "seek": 40352, "start": 403.52, "end": 411.91999999999996, "text": " but now our filters have half the resolution in each of the x and y axes.", "tokens": [457, 586, 527, 15995, 362, 1922, 264, 8669, 294, 1184, 295, 264, 2031, 293, 288, 35387, 13], "temperature": 0.0, "avg_logprob": -0.2698229427995353, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.093667823530268e-06}, {"id": 72, "seek": 40352, "start": 411.91999999999996, "end": 415.4, "text": " And so then I thought, okay, we've done 2 convolutional layers.", "tokens": [400, 370, 550, 286, 1194, 11, 1392, 11, 321, 600, 1096, 568, 45216, 304, 7914, 13], "temperature": 0.0, "avg_logprob": -0.2698229427995353, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.093667823530268e-06}, {"id": 73, "seek": 40352, "start": 415.4, "end": 419.44, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.2698229427995353, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.093667823530268e-06}, {"id": 74, "seek": 40352, "start": 419.44, "end": 423.28, "text": " How did you go from one matrix to 2 matrices in the second layer?", "tokens": [1012, 630, 291, 352, 490, 472, 8141, 281, 568, 32284, 294, 264, 1150, 4583, 30], "temperature": 0.0, "avg_logprob": -0.2698229427995353, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.093667823530268e-06}, {"id": 75, "seek": 40352, "start": 423.28, "end": 431.0, "text": " How did I go from one matrix to 2 matrices, as in how did I go from just this one thing", "tokens": [1012, 630, 286, 352, 490, 472, 8141, 281, 568, 32284, 11, 382, 294, 577, 630, 286, 352, 490, 445, 341, 472, 551], "temperature": 0.0, "avg_logprob": -0.2698229427995353, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.093667823530268e-06}, {"id": 76, "seek": 43100, "start": 431.0, "end": 434.32, "text": " to these 2 things?", "tokens": [281, 613, 568, 721, 30], "temperature": 0.0, "avg_logprob": -0.12607430156908536, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.8627588259696495e-06}, {"id": 77, "seek": 43100, "start": 434.32, "end": 439.24, "text": " So the answer to that is I just created 2 random 3x3 filters.", "tokens": [407, 264, 1867, 281, 300, 307, 286, 445, 2942, 568, 4974, 805, 87, 18, 15995, 13], "temperature": 0.0, "avg_logprob": -0.12607430156908536, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.8627588259696495e-06}, {"id": 78, "seek": 43100, "start": 439.24, "end": 445.06, "text": " This is my first random 3x3 filter, this is my second random 3x3 filter.", "tokens": [639, 307, 452, 700, 4974, 805, 87, 18, 6608, 11, 341, 307, 452, 1150, 4974, 805, 87, 18, 6608, 13], "temperature": 0.0, "avg_logprob": -0.12607430156908536, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.8627588259696495e-06}, {"id": 79, "seek": 43100, "start": 445.06, "end": 454.2, "text": " So each output then was simply equal to each corresponding 9-element section multiplied", "tokens": [407, 1184, 5598, 550, 390, 2935, 2681, 281, 1184, 11760, 1722, 12, 68, 3054, 3541, 17207], "temperature": 0.0, "avg_logprob": -0.12607430156908536, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.8627588259696495e-06}, {"id": 80, "seek": 43100, "start": 454.2, "end": 455.72, "text": " by each other and added together.", "tokens": [538, 1184, 661, 293, 3869, 1214, 13], "temperature": 0.0, "avg_logprob": -0.12607430156908536, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.8627588259696495e-06}, {"id": 81, "seek": 45572, "start": 455.72, "end": 462.52000000000004, "text": " So because I had 2 random 3x3 matrices, I ended up with 2 outputs.", "tokens": [407, 570, 286, 632, 568, 4974, 805, 87, 18, 32284, 11, 286, 4590, 493, 365, 568, 23930, 13], "temperature": 0.0, "avg_logprob": -0.15514190735355501, "compression_ratio": 1.3958333333333333, "no_speech_prob": 4.7108769649639726e-06}, {"id": 82, "seek": 45572, "start": 462.52000000000004, "end": 473.8, "text": " So 2 filters means 2 sets of outputs.", "tokens": [407, 568, 15995, 1355, 568, 6352, 295, 23930, 13], "temperature": 0.0, "avg_logprob": -0.15514190735355501, "compression_ratio": 1.3958333333333333, "no_speech_prob": 4.7108769649639726e-06}, {"id": 83, "seek": 45572, "start": 473.8, "end": 480.6, "text": " So now that we've got our max pooling layer, let's use a dense layer to turn it into our", "tokens": [407, 586, 300, 321, 600, 658, 527, 11469, 7005, 278, 4583, 11, 718, 311, 764, 257, 18011, 4583, 281, 1261, 309, 666, 527], "temperature": 0.0, "avg_logprob": -0.15514190735355501, "compression_ratio": 1.3958333333333333, "no_speech_prob": 4.7108769649639726e-06}, {"id": 84, "seek": 45572, "start": 480.6, "end": 483.18, "text": " output.", "tokens": [5598, 13], "temperature": 0.0, "avg_logprob": -0.15514190735355501, "compression_ratio": 1.3958333333333333, "no_speech_prob": 4.7108769649639726e-06}, {"id": 85, "seek": 48318, "start": 483.18, "end": 491.12, "text": " So a dense layer means that every single one of our activations from our max pooling layer", "tokens": [407, 257, 18011, 4583, 1355, 300, 633, 2167, 472, 295, 527, 2430, 763, 490, 527, 11469, 7005, 278, 4583], "temperature": 0.0, "avg_logprob": -0.11394136092242073, "compression_ratio": 1.5808383233532934, "no_speech_prob": 8.800974683254026e-06}, {"id": 86, "seek": 48318, "start": 491.12, "end": 493.5, "text": " needs a random weight.", "tokens": [2203, 257, 4974, 3364, 13], "temperature": 0.0, "avg_logprob": -0.11394136092242073, "compression_ratio": 1.5808383233532934, "no_speech_prob": 8.800974683254026e-06}, {"id": 87, "seek": 48318, "start": 493.5, "end": 497.58, "text": " So these are a whole bunch of random numbers.", "tokens": [407, 613, 366, 257, 1379, 3840, 295, 4974, 3547, 13], "temperature": 0.0, "avg_logprob": -0.11394136092242073, "compression_ratio": 1.5808383233532934, "no_speech_prob": 8.800974683254026e-06}, {"id": 88, "seek": 48318, "start": 497.58, "end": 504.16, "text": " So what I do is I take every one of those random numbers and multiply each one by a", "tokens": [407, 437, 286, 360, 307, 286, 747, 633, 472, 295, 729, 4974, 3547, 293, 12972, 1184, 472, 538, 257], "temperature": 0.0, "avg_logprob": -0.11394136092242073, "compression_ratio": 1.5808383233532934, "no_speech_prob": 8.800974683254026e-06}, {"id": 89, "seek": 48318, "start": 504.16, "end": 512.92, "text": " corresponding input.", "tokens": [11760, 4846, 13], "temperature": 0.0, "avg_logprob": -0.11394136092242073, "compression_ratio": 1.5808383233532934, "no_speech_prob": 8.800974683254026e-06}, {"id": 90, "seek": 51292, "start": 512.92, "end": 524.64, "text": " And MNIST, we would have 10 activations because we need an activation for 0, 1, 2, 3, so forth.", "tokens": [400, 376, 45, 19756, 11, 321, 576, 362, 1266, 2430, 763, 570, 321, 643, 364, 24433, 337, 1958, 11, 502, 11, 568, 11, 805, 11, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.18848802222580205, "compression_ratio": 1.4539007092198581, "no_speech_prob": 6.64334538669209e-06}, {"id": 91, "seek": 51292, "start": 524.64, "end": 532.88, "text": " So for MNIST, we would need 10 sets of these dense weight matrices so that we could calculate", "tokens": [407, 337, 376, 45, 19756, 11, 321, 576, 643, 1266, 6352, 295, 613, 18011, 3364, 32284, 370, 300, 321, 727, 8873], "temperature": 0.0, "avg_logprob": -0.18848802222580205, "compression_ratio": 1.4539007092198581, "no_speech_prob": 6.64334538669209e-06}, {"id": 92, "seek": 51292, "start": 532.88, "end": 536.64, "text": " the 10 outputs.", "tokens": [264, 1266, 23930, 13], "temperature": 0.0, "avg_logprob": -0.18848802222580205, "compression_ratio": 1.4539007092198581, "no_speech_prob": 6.64334538669209e-06}, {"id": 93, "seek": 53664, "start": 536.64, "end": 543.3199999999999, "text": " If we were only calculating one output, this would be a perfectly reasonable way to do", "tokens": [759, 321, 645, 787, 28258, 472, 5598, 11, 341, 576, 312, 257, 6239, 10585, 636, 281, 360], "temperature": 0.0, "avg_logprob": -0.18087591844446518, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.5294065178750316e-06}, {"id": 94, "seek": 53664, "start": 543.3199999999999, "end": 544.3199999999999, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.18087591844446518, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.5294065178750316e-06}, {"id": 95, "seek": 53664, "start": 544.3199999999999, "end": 551.6, "text": " For one output, it's just a sum product of everything from our final layer with a weight", "tokens": [1171, 472, 5598, 11, 309, 311, 445, 257, 2408, 1674, 295, 1203, 490, 527, 2572, 4583, 365, 257, 3364], "temperature": 0.0, "avg_logprob": -0.18087591844446518, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.5294065178750316e-06}, {"id": 96, "seek": 53664, "start": 551.6, "end": 557.8, "text": " for everything in that final layer added together.", "tokens": [337, 1203, 294, 300, 2572, 4583, 3869, 1214, 13], "temperature": 0.0, "avg_logprob": -0.18087591844446518, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.5294065178750316e-06}, {"id": 97, "seek": 53664, "start": 557.8, "end": 561.84, "text": " So that's all a dense layer is.", "tokens": [407, 300, 311, 439, 257, 18011, 4583, 307, 13], "temperature": 0.0, "avg_logprob": -0.18087591844446518, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.5294065178750316e-06}, {"id": 98, "seek": 56184, "start": 561.84, "end": 573.2, "text": " So really both dense layers and convolutional layers couldn't be easier mathematically.", "tokens": [407, 534, 1293, 18011, 7914, 293, 45216, 304, 7914, 2809, 380, 312, 3571, 44003, 13], "temperature": 0.0, "avg_logprob": -0.1382019227011162, "compression_ratio": 1.5317919075144508, "no_speech_prob": 1.9333492673467845e-06}, {"id": 99, "seek": 56184, "start": 573.2, "end": 580.24, "text": " I think the surprising thing is what happens when you then say, rather than using random", "tokens": [286, 519, 264, 8830, 551, 307, 437, 2314, 562, 291, 550, 584, 11, 2831, 813, 1228, 4974], "temperature": 0.0, "avg_logprob": -0.1382019227011162, "compression_ratio": 1.5317919075144508, "no_speech_prob": 1.9333492673467845e-06}, {"id": 100, "seek": 56184, "start": 580.24, "end": 585.5600000000001, "text": " weights, let's calculate the derivative of what happens if we were to change that weight", "tokens": [17443, 11, 718, 311, 8873, 264, 13760, 295, 437, 2314, 498, 321, 645, 281, 1319, 300, 3364], "temperature": 0.0, "avg_logprob": -0.1382019227011162, "compression_ratio": 1.5317919075144508, "no_speech_prob": 1.9333492673467845e-06}, {"id": 101, "seek": 58556, "start": 585.56, "end": 592.9599999999999, "text": " up by a bit or down by a bit, and how would it impact our loss.", "tokens": [493, 538, 257, 857, 420, 760, 538, 257, 857, 11, 293, 577, 576, 309, 2712, 527, 4470, 13], "temperature": 0.0, "avg_logprob": -0.19101293463456004, "compression_ratio": 1.6972477064220184, "no_speech_prob": 8.530202649126295e-06}, {"id": 102, "seek": 58556, "start": 592.9599999999999, "end": 596.16, "text": " In this case, I haven't actually got as far as calculating a loss function, but we could", "tokens": [682, 341, 1389, 11, 286, 2378, 380, 767, 658, 382, 1400, 382, 28258, 257, 4470, 2445, 11, 457, 321, 727], "temperature": 0.0, "avg_logprob": -0.19101293463456004, "compression_ratio": 1.6972477064220184, "no_speech_prob": 8.530202649126295e-06}, {"id": 103, "seek": 58556, "start": 596.16, "end": 602.04, "text": " add over here a sigmoid loss for example.", "tokens": [909, 670, 510, 257, 4556, 3280, 327, 4470, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.19101293463456004, "compression_ratio": 1.6972477064220184, "no_speech_prob": 8.530202649126295e-06}, {"id": 104, "seek": 58556, "start": 602.04, "end": 606.0, "text": " And so we can calculate the derivative of the loss with respect to every single weight", "tokens": [400, 370, 321, 393, 8873, 264, 13760, 295, 264, 4470, 365, 3104, 281, 633, 2167, 3364], "temperature": 0.0, "avg_logprob": -0.19101293463456004, "compression_ratio": 1.6972477064220184, "no_speech_prob": 8.530202649126295e-06}, {"id": 105, "seek": 58556, "start": 606.0, "end": 613.56, "text": " in the dense layer and every single weight in all of our filters in that layer and every", "tokens": [294, 264, 18011, 4583, 293, 633, 2167, 3364, 294, 439, 295, 527, 15995, 294, 300, 4583, 293, 633], "temperature": 0.0, "avg_logprob": -0.19101293463456004, "compression_ratio": 1.6972477064220184, "no_speech_prob": 8.530202649126295e-06}, {"id": 106, "seek": 61356, "start": 613.56, "end": 618.16, "text": " single weight in all of our filters in this layer.", "tokens": [2167, 3364, 294, 439, 295, 527, 15995, 294, 341, 4583, 13], "temperature": 0.0, "avg_logprob": -0.15729593700832792, "compression_ratio": 1.748878923766816, "no_speech_prob": 4.029415777040413e-06}, {"id": 107, "seek": 61356, "start": 618.16, "end": 621.4799999999999, "text": " And then with all of those derivatives, we can calculate how to optimize all of these", "tokens": [400, 550, 365, 439, 295, 729, 33733, 11, 321, 393, 8873, 577, 281, 19719, 439, 295, 613], "temperature": 0.0, "avg_logprob": -0.15729593700832792, "compression_ratio": 1.748878923766816, "no_speech_prob": 4.029415777040413e-06}, {"id": 108, "seek": 61356, "start": 621.4799999999999, "end": 622.92, "text": " weights.", "tokens": [17443, 13], "temperature": 0.0, "avg_logprob": -0.15729593700832792, "compression_ratio": 1.748878923766816, "no_speech_prob": 4.029415777040413e-06}, {"id": 109, "seek": 61356, "start": 622.92, "end": 627.76, "text": " And the surprising thing is that when we optimize all of these weights, we end up with these", "tokens": [400, 264, 8830, 551, 307, 300, 562, 321, 19719, 439, 295, 613, 17443, 11, 321, 917, 493, 365, 613], "temperature": 0.0, "avg_logprob": -0.15729593700832792, "compression_ratio": 1.748878923766816, "no_speech_prob": 4.029415777040413e-06}, {"id": 110, "seek": 61356, "start": 627.76, "end": 632.56, "text": " incredibly powerful models like those visualizations that we saw.", "tokens": [6252, 4005, 5245, 411, 729, 5056, 14455, 300, 321, 1866, 13], "temperature": 0.0, "avg_logprob": -0.15729593700832792, "compression_ratio": 1.748878923766816, "no_speech_prob": 4.029415777040413e-06}, {"id": 111, "seek": 61356, "start": 632.56, "end": 639.16, "text": " So I'm not quite sure where the disconnect between the incredibly simple math and the", "tokens": [407, 286, 478, 406, 1596, 988, 689, 264, 14299, 1296, 264, 6252, 2199, 5221, 293, 264], "temperature": 0.0, "avg_logprob": -0.15729593700832792, "compression_ratio": 1.748878923766816, "no_speech_prob": 4.029415777040413e-06}, {"id": 112, "seek": 63916, "start": 639.16, "end": 644.4, "text": " outcome is, I think it might be that it's so easy, it's hard to believe that that's", "tokens": [9700, 307, 11, 286, 519, 309, 1062, 312, 300, 309, 311, 370, 1858, 11, 309, 311, 1152, 281, 1697, 300, 300, 311], "temperature": 0.0, "avg_logprob": -0.1752362701128114, "compression_ratio": 1.5278969957081545, "no_speech_prob": 1.0783194738905877e-05}, {"id": 113, "seek": 63916, "start": 644.4, "end": 645.4, "text": " all it is.", "tokens": [439, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1752362701128114, "compression_ratio": 1.5278969957081545, "no_speech_prob": 1.0783194738905877e-05}, {"id": 114, "seek": 63916, "start": 645.4, "end": 648.16, "text": " But I'm not skipping over anything.", "tokens": [583, 286, 478, 406, 31533, 670, 1340, 13], "temperature": 0.0, "avg_logprob": -0.1752362701128114, "compression_ratio": 1.5278969957081545, "no_speech_prob": 1.0783194738905877e-05}, {"id": 115, "seek": 63916, "start": 648.16, "end": 649.56, "text": " That really is it.", "tokens": [663, 534, 307, 309, 13], "temperature": 0.0, "avg_logprob": -0.1752362701128114, "compression_ratio": 1.5278969957081545, "no_speech_prob": 1.0783194738905877e-05}, {"id": 116, "seek": 63916, "start": 649.56, "end": 656.6, "text": " And so to help you really understand this, I'm going to talk more about SGD.", "tokens": [400, 370, 281, 854, 291, 534, 1223, 341, 11, 286, 478, 516, 281, 751, 544, 466, 34520, 35, 13], "temperature": 0.0, "avg_logprob": -0.1752362701128114, "compression_ratio": 1.5278969957081545, "no_speech_prob": 1.0783194738905877e-05}, {"id": 117, "seek": 63916, "start": 656.6, "end": 660.88, "text": " Why would you use a sigmoid function here?", "tokens": [1545, 576, 291, 764, 257, 4556, 3280, 327, 2445, 510, 30], "temperature": 0.0, "avg_logprob": -0.1752362701128114, "compression_ratio": 1.5278969957081545, "no_speech_prob": 1.0783194738905877e-05}, {"id": 118, "seek": 63916, "start": 660.88, "end": 667.0, "text": " So the loss function we generally use is the softmax, so e to the xi divided by sum of", "tokens": [407, 264, 4470, 2445, 321, 5101, 764, 307, 264, 2787, 41167, 11, 370, 308, 281, 264, 36800, 6666, 538, 2408, 295], "temperature": 0.0, "avg_logprob": -0.1752362701128114, "compression_ratio": 1.5278969957081545, "no_speech_prob": 1.0783194738905877e-05}, {"id": 119, "seek": 66700, "start": 667.0, "end": 669.48, "text": " e to the xi.", "tokens": [308, 281, 264, 36800, 13], "temperature": 0.0, "avg_logprob": -0.1472500032848782, "compression_ratio": 1.4782608695652173, "no_speech_prob": 5.507521109393565e-06}, {"id": 120, "seek": 66700, "start": 669.48, "end": 676.0, "text": " If it's just binary, that's just the equivalent of having just 1 over 1 plus e to the xi.", "tokens": [759, 309, 311, 445, 17434, 11, 300, 311, 445, 264, 10344, 295, 1419, 445, 502, 670, 502, 1804, 308, 281, 264, 36800, 13], "temperature": 0.0, "avg_logprob": -0.1472500032848782, "compression_ratio": 1.4782608695652173, "no_speech_prob": 5.507521109393565e-06}, {"id": 121, "seek": 66700, "start": 676.0, "end": 685.56, "text": " So softmax in the binary case simplifies into a sigmoid function.", "tokens": [407, 2787, 41167, 294, 264, 17434, 1389, 6883, 11221, 666, 257, 4556, 3280, 327, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1472500032848782, "compression_ratio": 1.4782608695652173, "no_speech_prob": 5.507521109393565e-06}, {"id": 122, "seek": 66700, "start": 685.56, "end": 688.84, "text": " Thank you for clarifying that question.", "tokens": [1044, 291, 337, 6093, 5489, 300, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1472500032848782, "compression_ratio": 1.4782608695652173, "no_speech_prob": 5.507521109393565e-06}, {"id": 123, "seek": 66700, "start": 688.84, "end": 691.32, "text": " So I think this is super fun.", "tokens": [407, 286, 519, 341, 307, 1687, 1019, 13], "temperature": 0.0, "avg_logprob": -0.1472500032848782, "compression_ratio": 1.4782608695652173, "no_speech_prob": 5.507521109393565e-06}, {"id": 124, "seek": 69132, "start": 691.32, "end": 698.12, "text": " We're going to talk about not just SGD, but every variant of SGD, including one invented", "tokens": [492, 434, 516, 281, 751, 466, 406, 445, 34520, 35, 11, 457, 633, 17501, 295, 34520, 35, 11, 3009, 472, 14479], "temperature": 0.0, "avg_logprob": -0.23278866873847115, "compression_ratio": 1.5982905982905984, "no_speech_prob": 7.411167189275147e-06}, {"id": 125, "seek": 69132, "start": 698.12, "end": 700.84, "text": " just a week ago.", "tokens": [445, 257, 1243, 2057, 13], "temperature": 0.0, "avg_logprob": -0.23278866873847115, "compression_ratio": 1.5982905982905984, "no_speech_prob": 7.411167189275147e-06}, {"id": 126, "seek": 69132, "start": 700.84, "end": 703.4000000000001, "text": " So we've already talked about SGD.", "tokens": [407, 321, 600, 1217, 2825, 466, 34520, 35, 13], "temperature": 0.0, "avg_logprob": -0.23278866873847115, "compression_ratio": 1.5982905982905984, "no_speech_prob": 7.411167189275147e-06}, {"id": 127, "seek": 69132, "start": 703.4000000000001, "end": 704.4000000000001, "text": " Question.", "tokens": [14464, 13], "temperature": 0.0, "avg_logprob": -0.23278866873847115, "compression_ratio": 1.5982905982905984, "no_speech_prob": 7.411167189275147e-06}, {"id": 128, "seek": 69132, "start": 704.4000000000001, "end": 708.6, "text": " Does SGD happen for all layers at once?", "tokens": [4402, 34520, 35, 1051, 337, 439, 7914, 412, 1564, 30], "temperature": 0.0, "avg_logprob": -0.23278866873847115, "compression_ratio": 1.5982905982905984, "no_speech_prob": 7.411167189275147e-06}, {"id": 129, "seek": 69132, "start": 708.6, "end": 709.6, "text": " Answer.", "tokens": [24545, 13], "temperature": 0.0, "avg_logprob": -0.23278866873847115, "compression_ratio": 1.5982905982905984, "no_speech_prob": 7.411167189275147e-06}, {"id": 130, "seek": 69132, "start": 709.6, "end": 710.6, "text": " SGD happens for all layers at once.", "tokens": [34520, 35, 2314, 337, 439, 7914, 412, 1564, 13], "temperature": 0.0, "avg_logprob": -0.23278866873847115, "compression_ratio": 1.5982905982905984, "no_speech_prob": 7.411167189275147e-06}, {"id": 131, "seek": 69132, "start": 710.6, "end": 715.24, "text": " Yes, we calculate the derivative of all the weights with respect to the loss.", "tokens": [1079, 11, 321, 8873, 264, 13760, 295, 439, 264, 17443, 365, 3104, 281, 264, 4470, 13], "temperature": 0.0, "avg_logprob": -0.23278866873847115, "compression_ratio": 1.5982905982905984, "no_speech_prob": 7.411167189275147e-06}, {"id": 132, "seek": 69132, "start": 715.24, "end": 719.9200000000001, "text": " And when to have a max pool after convolution vs when not to?", "tokens": [400, 562, 281, 362, 257, 11469, 7005, 934, 45216, 12041, 562, 406, 281, 30], "temperature": 0.0, "avg_logprob": -0.23278866873847115, "compression_ratio": 1.5982905982905984, "no_speech_prob": 7.411167189275147e-06}, {"id": 133, "seek": 71992, "start": 719.92, "end": 726.56, "text": " When to have a max pool after a convolution, who knows.", "tokens": [1133, 281, 362, 257, 11469, 7005, 934, 257, 45216, 11, 567, 3255, 13], "temperature": 0.0, "avg_logprob": -0.13866463460420309, "compression_ratio": 1.5789473684210527, "no_speech_prob": 3.705178460222669e-05}, {"id": 134, "seek": 71992, "start": 726.56, "end": 731.04, "text": " This is a very controversial question and indeed some people now are saying never use", "tokens": [639, 307, 257, 588, 17323, 1168, 293, 6451, 512, 561, 586, 366, 1566, 1128, 764], "temperature": 0.0, "avg_logprob": -0.13866463460420309, "compression_ratio": 1.5789473684210527, "no_speech_prob": 3.705178460222669e-05}, {"id": 135, "seek": 71992, "start": 731.04, "end": 733.16, "text": " max pool.", "tokens": [11469, 7005, 13], "temperature": 0.0, "avg_logprob": -0.13866463460420309, "compression_ratio": 1.5789473684210527, "no_speech_prob": 3.705178460222669e-05}, {"id": 136, "seek": 71992, "start": 733.16, "end": 740.52, "text": " Instead of using max pool when you're doing the convolutions, don't do a convolution over", "tokens": [7156, 295, 1228, 11469, 7005, 562, 291, 434, 884, 264, 3754, 15892, 11, 500, 380, 360, 257, 45216, 670], "temperature": 0.0, "avg_logprob": -0.13866463460420309, "compression_ratio": 1.5789473684210527, "no_speech_prob": 3.705178460222669e-05}, {"id": 137, "seek": 71992, "start": 740.52, "end": 748.7199999999999, "text": " every set of 9 pixels, but instead skip a pixel each time.", "tokens": [633, 992, 295, 1722, 18668, 11, 457, 2602, 10023, 257, 19261, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.13866463460420309, "compression_ratio": 1.5789473684210527, "no_speech_prob": 3.705178460222669e-05}, {"id": 138, "seek": 74872, "start": 748.72, "end": 753.76, "text": " And so that's another way of downsampling.", "tokens": [400, 370, 300, 311, 1071, 636, 295, 760, 19988, 11970, 13], "temperature": 0.0, "avg_logprob": -0.16325552710171404, "compression_ratio": 1.5673076923076923, "no_speech_prob": 2.212475737906061e-05}, {"id": 139, "seek": 74872, "start": 753.76, "end": 758.6, "text": " Jeffrey Hinton, who's kind of the father of deep learning, has gone as far as saying that", "tokens": [28721, 389, 12442, 11, 567, 311, 733, 295, 264, 3086, 295, 2452, 2539, 11, 575, 2780, 382, 1400, 382, 1566, 300], "temperature": 0.0, "avg_logprob": -0.16325552710171404, "compression_ratio": 1.5673076923076923, "no_speech_prob": 2.212475737906061e-05}, {"id": 140, "seek": 74872, "start": 758.6, "end": 767.8000000000001, "text": " the extremely great success of max pooling has been the greatest problem deep learning", "tokens": [264, 4664, 869, 2245, 295, 11469, 7005, 278, 575, 668, 264, 6636, 1154, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.16325552710171404, "compression_ratio": 1.5673076923076923, "no_speech_prob": 2.212475737906061e-05}, {"id": 141, "seek": 74872, "start": 767.8000000000001, "end": 768.8000000000001, "text": " has faced.", "tokens": [575, 11446, 13], "temperature": 0.0, "avg_logprob": -0.16325552710171404, "compression_ratio": 1.5673076923076923, "no_speech_prob": 2.212475737906061e-05}, {"id": 142, "seek": 74872, "start": 768.8000000000001, "end": 776.08, "text": " Because to him, it really stops us from going further.", "tokens": [1436, 281, 796, 11, 309, 534, 10094, 505, 490, 516, 3052, 13], "temperature": 0.0, "avg_logprob": -0.16325552710171404, "compression_ratio": 1.5673076923076923, "no_speech_prob": 2.212475737906061e-05}, {"id": 143, "seek": 74872, "start": 776.08, "end": 777.48, "text": " I don't know whether that's true or not.", "tokens": [286, 500, 380, 458, 1968, 300, 311, 2074, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.16325552710171404, "compression_ratio": 1.5673076923076923, "no_speech_prob": 2.212475737906061e-05}, {"id": 144, "seek": 77748, "start": 777.48, "end": 784.08, "text": " I assume it is because he's Jeffrey Hinton and I'm not.", "tokens": [286, 6552, 309, 307, 570, 415, 311, 28721, 389, 12442, 293, 286, 478, 406, 13], "temperature": 0.0, "avg_logprob": -0.2033846051085229, "compression_ratio": 1.6017316017316017, "no_speech_prob": 1.8631306375027634e-05}, {"id": 145, "seek": 77748, "start": 784.08, "end": 791.5, "text": " For now, we use max pooling every time we're doing fine-tuning because we need to make", "tokens": [1171, 586, 11, 321, 764, 11469, 7005, 278, 633, 565, 321, 434, 884, 2489, 12, 83, 37726, 570, 321, 643, 281, 652], "temperature": 0.0, "avg_logprob": -0.2033846051085229, "compression_ratio": 1.6017316017316017, "no_speech_prob": 1.8631306375027634e-05}, {"id": 146, "seek": 77748, "start": 791.5, "end": 796.52, "text": " sure that our architecture is identical to the original VGG's architecture, so we have", "tokens": [988, 300, 527, 9482, 307, 14800, 281, 264, 3380, 691, 27561, 311, 9482, 11, 370, 321, 362], "temperature": 0.0, "avg_logprob": -0.2033846051085229, "compression_ratio": 1.6017316017316017, "no_speech_prob": 1.8631306375027634e-05}, {"id": 147, "seek": 77748, "start": 796.52, "end": 800.44, "text": " to put max pooling wherever they do.", "tokens": [281, 829, 11469, 7005, 278, 8660, 436, 360, 13], "temperature": 0.0, "avg_logprob": -0.2033846051085229, "compression_ratio": 1.6017316017316017, "no_speech_prob": 1.8631306375027634e-05}, {"id": 148, "seek": 77748, "start": 800.44, "end": 803.08, "text": " Why do we want max pooling or downsampling?", "tokens": [1545, 360, 321, 528, 11469, 7005, 278, 420, 760, 19988, 11970, 30], "temperature": 0.0, "avg_logprob": -0.2033846051085229, "compression_ratio": 1.6017316017316017, "no_speech_prob": 1.8631306375027634e-05}, {"id": 149, "seek": 77748, "start": 803.08, "end": 806.88, "text": " Are we just trying to look at bigger features at the input?", "tokens": [2014, 321, 445, 1382, 281, 574, 412, 3801, 4122, 412, 264, 4846, 30], "temperature": 0.0, "avg_logprob": -0.2033846051085229, "compression_ratio": 1.6017316017316017, "no_speech_prob": 1.8631306375027634e-05}, {"id": 150, "seek": 80688, "start": 806.88, "end": 811.28, "text": " Why use max pooling at all?", "tokens": [1545, 764, 11469, 7005, 278, 412, 439, 30], "temperature": 0.0, "avg_logprob": -0.11368066021519849, "compression_ratio": 1.60546875, "no_speech_prob": 1.1300563528493512e-05}, {"id": 151, "seek": 80688, "start": 811.28, "end": 812.68, "text": " There's a couple of reasons.", "tokens": [821, 311, 257, 1916, 295, 4112, 13], "temperature": 0.0, "avg_logprob": -0.11368066021519849, "compression_ratio": 1.60546875, "no_speech_prob": 1.1300563528493512e-05}, {"id": 152, "seek": 80688, "start": 812.68, "end": 817.8, "text": " The first is that max pooling helps with translation invariance.", "tokens": [440, 700, 307, 300, 11469, 7005, 278, 3665, 365, 12853, 33270, 719, 13], "temperature": 0.0, "avg_logprob": -0.11368066021519849, "compression_ratio": 1.60546875, "no_speech_prob": 1.1300563528493512e-05}, {"id": 153, "seek": 80688, "start": 817.8, "end": 822.6, "text": " So it basically says if this feature is here or here or here or here, I don't care, it's", "tokens": [407, 309, 1936, 1619, 498, 341, 4111, 307, 510, 420, 510, 420, 510, 420, 510, 11, 286, 500, 380, 1127, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.11368066021519849, "compression_ratio": 1.60546875, "no_speech_prob": 1.1300563528493512e-05}, {"id": 154, "seek": 80688, "start": 822.6, "end": 824.68, "text": " kind of roughly in the right spot.", "tokens": [733, 295, 9810, 294, 264, 558, 4008, 13], "temperature": 0.0, "avg_logprob": -0.11368066021519849, "compression_ratio": 1.60546875, "no_speech_prob": 1.1300563528493512e-05}, {"id": 155, "seek": 80688, "start": 824.68, "end": 826.28, "text": " And so that seems to work well.", "tokens": [400, 370, 300, 2544, 281, 589, 731, 13], "temperature": 0.0, "avg_logprob": -0.11368066021519849, "compression_ratio": 1.60546875, "no_speech_prob": 1.1300563528493512e-05}, {"id": 156, "seek": 80688, "start": 826.28, "end": 827.9, "text": " And the second is exactly what you said.", "tokens": [400, 264, 1150, 307, 2293, 437, 291, 848, 13], "temperature": 0.0, "avg_logprob": -0.11368066021519849, "compression_ratio": 1.60546875, "no_speech_prob": 1.1300563528493512e-05}, {"id": 157, "seek": 80688, "start": 827.9, "end": 833.64, "text": " Every time we max pool, we end up with a smaller grid, which means that our 3x3 convolutions", "tokens": [2048, 565, 321, 11469, 7005, 11, 321, 917, 493, 365, 257, 4356, 10748, 11, 597, 1355, 300, 527, 805, 87, 18, 3754, 15892], "temperature": 0.0, "avg_logprob": -0.11368066021519849, "compression_ratio": 1.60546875, "no_speech_prob": 1.1300563528493512e-05}, {"id": 158, "seek": 83364, "start": 833.64, "end": 838.3199999999999, "text": " are effectively covering a larger part of the original image, which means that our convolutions", "tokens": [366, 8659, 10322, 257, 4833, 644, 295, 264, 3380, 3256, 11, 597, 1355, 300, 527, 3754, 15892], "temperature": 0.0, "avg_logprob": -0.18638606404149255, "compression_ratio": 1.3582089552238805, "no_speech_prob": 7.254125375766307e-05}, {"id": 159, "seek": 83364, "start": 838.3199999999999, "end": 842.64, "text": " can find larger and more complex features.", "tokens": [393, 915, 4833, 293, 544, 3997, 4122, 13], "temperature": 0.0, "avg_logprob": -0.18638606404149255, "compression_ratio": 1.3582089552238805, "no_speech_prob": 7.254125375766307e-05}, {"id": 160, "seek": 83364, "start": 842.64, "end": 848.3199999999999, "text": " I think they would be the two main reasons.", "tokens": [286, 519, 436, 576, 312, 264, 732, 2135, 4112, 13], "temperature": 0.0, "avg_logprob": -0.18638606404149255, "compression_ratio": 1.3582089552238805, "no_speech_prob": 7.254125375766307e-05}, {"id": 161, "seek": 84832, "start": 848.32, "end": 873.0, "text": " Is Jeffrey Hinton cool with the idea of doing the skipping?", "tokens": [1119, 28721, 389, 12442, 1627, 365, 264, 1558, 295, 884, 264, 31533, 30], "temperature": 0.0, "avg_logprob": -0.3610628913430607, "compression_ratio": 0.9516129032258065, "no_speech_prob": 4.683548104367219e-05}, {"id": 162, "seek": 87300, "start": 873.0, "end": 881.68, "text": " The capsule architecture, CAPSULE.", "tokens": [440, 29247, 9482, 11, 33636, 20214, 2634, 13], "temperature": 0.0, "avg_logprob": -0.2743940842457307, "compression_ratio": 1.4593301435406698, "no_speech_prob": 2.178147951781284e-05}, {"id": 163, "seek": 87300, "start": 881.68, "end": 885.84, "text": " You can learn all about the things that he thinks we ought to have but don't yet have.", "tokens": [509, 393, 1466, 439, 466, 264, 721, 300, 415, 7309, 321, 13416, 281, 362, 457, 500, 380, 1939, 362, 13], "temperature": 0.0, "avg_logprob": -0.2743940842457307, "compression_ratio": 1.4593301435406698, "no_speech_prob": 2.178147951781284e-05}, {"id": 164, "seek": 87300, "start": 885.84, "end": 895.68, "text": " He did point out that one of the key pieces of deep learning that he invented took 17", "tokens": [634, 630, 935, 484, 300, 472, 295, 264, 2141, 3755, 295, 2452, 2539, 300, 415, 14479, 1890, 3282], "temperature": 0.0, "avg_logprob": -0.2743940842457307, "compression_ratio": 1.4593301435406698, "no_speech_prob": 2.178147951781284e-05}, {"id": 165, "seek": 87300, "start": 895.68, "end": 898.04, "text": " years from conception to working.", "tokens": [924, 490, 30698, 281, 1364, 13], "temperature": 0.0, "avg_logprob": -0.2743940842457307, "compression_ratio": 1.4593301435406698, "no_speech_prob": 2.178147951781284e-05}, {"id": 166, "seek": 87300, "start": 898.04, "end": 901.32, "text": " So he is somebody who sticks to these things and makes it work.", "tokens": [407, 415, 307, 2618, 567, 12518, 281, 613, 721, 293, 1669, 309, 589, 13], "temperature": 0.0, "avg_logprob": -0.2743940842457307, "compression_ratio": 1.4593301435406698, "no_speech_prob": 2.178147951781284e-05}, {"id": 167, "seek": 90132, "start": 901.32, "end": 905.0, "text": " Is max pooling unique to image processing?", "tokens": [1119, 11469, 7005, 278, 3845, 281, 3256, 9007, 30], "temperature": 0.0, "avg_logprob": -0.1306608131339958, "compression_ratio": 2.008968609865471, "no_speech_prob": 3.169158662785776e-05}, {"id": 168, "seek": 90132, "start": 905.0, "end": 908.12, "text": " Max pooling is not unique to image processing.", "tokens": [7402, 7005, 278, 307, 406, 3845, 281, 3256, 9007, 13], "temperature": 0.0, "avg_logprob": -0.1306608131339958, "compression_ratio": 2.008968609865471, "no_speech_prob": 3.169158662785776e-05}, {"id": 169, "seek": 90132, "start": 908.12, "end": 911.84, "text": " It's likely to be useful for any kind of convolutional neural network.", "tokens": [467, 311, 3700, 281, 312, 4420, 337, 604, 733, 295, 45216, 304, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.1306608131339958, "compression_ratio": 2.008968609865471, "no_speech_prob": 3.169158662785776e-05}, {"id": 170, "seek": 90132, "start": 911.84, "end": 915.96, "text": " And a convolutional neural network can be used for any kind of data that has some kind", "tokens": [400, 257, 45216, 304, 18161, 3209, 393, 312, 1143, 337, 604, 733, 295, 1412, 300, 575, 512, 733], "temperature": 0.0, "avg_logprob": -0.1306608131339958, "compression_ratio": 2.008968609865471, "no_speech_prob": 3.169158662785776e-05}, {"id": 171, "seek": 90132, "start": 915.96, "end": 917.6800000000001, "text": " of consistent ordering.", "tokens": [295, 8398, 21739, 13], "temperature": 0.0, "avg_logprob": -0.1306608131339958, "compression_ratio": 2.008968609865471, "no_speech_prob": 3.169158662785776e-05}, {"id": 172, "seek": 90132, "start": 917.6800000000001, "end": 924.9200000000001, "text": " So things like speech, or any kind of audio, or some kind of consistent time series, all", "tokens": [407, 721, 411, 6218, 11, 420, 604, 733, 295, 6278, 11, 420, 512, 733, 295, 8398, 565, 2638, 11, 439], "temperature": 0.0, "avg_logprob": -0.1306608131339958, "compression_ratio": 2.008968609865471, "no_speech_prob": 3.169158662785776e-05}, {"id": 173, "seek": 90132, "start": 924.9200000000001, "end": 928.8000000000001, "text": " of these things have some kind of ordering to them, and therefore you can use a CNN and", "tokens": [295, 613, 721, 362, 512, 733, 295, 21739, 281, 552, 11, 293, 4412, 291, 393, 764, 257, 24859, 293], "temperature": 0.0, "avg_logprob": -0.1306608131339958, "compression_ratio": 2.008968609865471, "no_speech_prob": 3.169158662785776e-05}, {"id": 174, "seek": 92880, "start": 928.8, "end": 931.8, "text": " therefore you can use max pooling.", "tokens": [4412, 291, 393, 764, 11469, 7005, 278, 13], "temperature": 0.0, "avg_logprob": -0.12204288614207301, "compression_ratio": 1.6084905660377358, "no_speech_prob": 1.1478370652184822e-05}, {"id": 175, "seek": 92880, "start": 931.8, "end": 937.3199999999999, "text": " And as we look at NLP, we will be looking more at convolutional neural networks for", "tokens": [400, 382, 321, 574, 412, 426, 45196, 11, 321, 486, 312, 1237, 544, 412, 45216, 304, 18161, 9590, 337], "temperature": 0.0, "avg_logprob": -0.12204288614207301, "compression_ratio": 1.6084905660377358, "no_speech_prob": 1.1478370652184822e-05}, {"id": 176, "seek": 92880, "start": 937.3199999999999, "end": 938.8, "text": " other data types.", "tokens": [661, 1412, 3467, 13], "temperature": 0.0, "avg_logprob": -0.12204288614207301, "compression_ratio": 1.6084905660377358, "no_speech_prob": 1.1478370652184822e-05}, {"id": 177, "seek": 92880, "start": 938.8, "end": 946.64, "text": " And interestingly, the author of Keras last week or maybe the week before made the contention", "tokens": [400, 25873, 11, 264, 3793, 295, 591, 6985, 1036, 1243, 420, 1310, 264, 1243, 949, 1027, 264, 660, 1251], "temperature": 0.0, "avg_logprob": -0.12204288614207301, "compression_ratio": 1.6084905660377358, "no_speech_prob": 1.1478370652184822e-05}, {"id": 178, "seek": 92880, "start": 946.64, "end": 952.5999999999999, "text": " that perhaps it will turn out that CNNs are the architecture that will be used for every", "tokens": [300, 4317, 309, 486, 1261, 484, 300, 24859, 82, 366, 264, 9482, 300, 486, 312, 1143, 337, 633], "temperature": 0.0, "avg_logprob": -0.12204288614207301, "compression_ratio": 1.6084905660377358, "no_speech_prob": 1.1478370652184822e-05}, {"id": 179, "seek": 92880, "start": 952.5999999999999, "end": 956.12, "text": " type of ordered data.", "tokens": [2010, 295, 8866, 1412, 13], "temperature": 0.0, "avg_logprob": -0.12204288614207301, "compression_ratio": 1.6084905660377358, "no_speech_prob": 1.1478370652184822e-05}, {"id": 180, "seek": 95612, "start": 956.12, "end": 961.08, "text": " And this was just after one of the leading NLP researchers released a paper basically", "tokens": [400, 341, 390, 445, 934, 472, 295, 264, 5775, 426, 45196, 10309, 4736, 257, 3035, 1936], "temperature": 0.0, "avg_logprob": -0.11698376377926598, "compression_ratio": 1.5207373271889402, "no_speech_prob": 1.5206002899503801e-05}, {"id": 181, "seek": 95612, "start": 961.08, "end": 967.5600000000001, "text": " showing a state-of-the-art result in NLP using convolutional neural networks.", "tokens": [4099, 257, 1785, 12, 2670, 12, 3322, 12, 446, 1874, 294, 426, 45196, 1228, 45216, 304, 18161, 9590, 13], "temperature": 0.0, "avg_logprob": -0.11698376377926598, "compression_ratio": 1.5207373271889402, "no_speech_prob": 1.5206002899503801e-05}, {"id": 182, "seek": 95612, "start": 967.5600000000001, "end": 973.64, "text": " So although we'll start learning about recurrent neural networks next week, I have to be open", "tokens": [407, 4878, 321, 603, 722, 2539, 466, 18680, 1753, 18161, 9590, 958, 1243, 11, 286, 362, 281, 312, 1269], "temperature": 0.0, "avg_logprob": -0.11698376377926598, "compression_ratio": 1.5207373271889402, "no_speech_prob": 1.5206002899503801e-05}, {"id": 183, "seek": 95612, "start": 973.64, "end": 982.84, "text": " to the possibility that they'll become redundant by the end of the year.", "tokens": [281, 264, 7959, 300, 436, 603, 1813, 40997, 538, 264, 917, 295, 264, 1064, 13], "temperature": 0.0, "avg_logprob": -0.11698376377926598, "compression_ratio": 1.5207373271889402, "no_speech_prob": 1.5206002899503801e-05}, {"id": 184, "seek": 98284, "start": 982.84, "end": 988.76, "text": " So SGD, we looked at the SGD intro notebook, but I think things are a little more clear", "tokens": [407, 34520, 35, 11, 321, 2956, 412, 264, 34520, 35, 12897, 21060, 11, 457, 286, 519, 721, 366, 257, 707, 544, 1850], "temperature": 0.0, "avg_logprob": -0.11570241053899129, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.586618393252138e-05}, {"id": 185, "seek": 98284, "start": 988.76, "end": 990.72, "text": " sometimes when you can see it all in front of you.", "tokens": [2171, 562, 291, 393, 536, 309, 439, 294, 1868, 295, 291, 13], "temperature": 0.0, "avg_logprob": -0.11570241053899129, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.586618393252138e-05}, {"id": 186, "seek": 98284, "start": 990.72, "end": 996.72, "text": " So here is basically the identical thing that we saw in the SGD notebook in Excel.", "tokens": [407, 510, 307, 1936, 264, 14800, 551, 300, 321, 1866, 294, 264, 34520, 35, 21060, 294, 19060, 13], "temperature": 0.0, "avg_logprob": -0.11570241053899129, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.586618393252138e-05}, {"id": 187, "seek": 98284, "start": 996.72, "end": 1002.0400000000001, "text": " So we are going to start by creating a line.", "tokens": [407, 321, 366, 516, 281, 722, 538, 4084, 257, 1622, 13], "temperature": 0.0, "avg_logprob": -0.11570241053899129, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.586618393252138e-05}, {"id": 188, "seek": 98284, "start": 1002.0400000000001, "end": 1008.84, "text": " We create 29 random numbers, and then we say, OK, let's create something that is equal to", "tokens": [492, 1884, 9413, 4974, 3547, 11, 293, 550, 321, 584, 11, 2264, 11, 718, 311, 1884, 746, 300, 307, 2681, 281], "temperature": 0.0, "avg_logprob": -0.11570241053899129, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.586618393252138e-05}, {"id": 189, "seek": 100884, "start": 1008.84, "end": 1020.84, "text": " 2 times x plus 30.", "tokens": [568, 1413, 2031, 1804, 2217, 13], "temperature": 0.0, "avg_logprob": -0.16189071655273438, "compression_ratio": 1.3504273504273505, "no_speech_prob": 2.994418764501461e-06}, {"id": 190, "seek": 100884, "start": 1020.84, "end": 1023.9200000000001, "text": " And so here is 2 times x plus 30.", "tokens": [400, 370, 510, 307, 568, 1413, 2031, 1804, 2217, 13], "temperature": 0.0, "avg_logprob": -0.16189071655273438, "compression_ratio": 1.3504273504273505, "no_speech_prob": 2.994418764501461e-06}, {"id": 191, "seek": 100884, "start": 1023.9200000000001, "end": 1027.6000000000001, "text": " So that's my input data.", "tokens": [407, 300, 311, 452, 4846, 1412, 13], "temperature": 0.0, "avg_logprob": -0.16189071655273438, "compression_ratio": 1.3504273504273505, "no_speech_prob": 2.994418764501461e-06}, {"id": 192, "seek": 100884, "start": 1027.6000000000001, "end": 1033.08, "text": " So I am trying to again create something that can find the parameters of a line.", "tokens": [407, 286, 669, 1382, 281, 797, 1884, 746, 300, 393, 915, 264, 9834, 295, 257, 1622, 13], "temperature": 0.0, "avg_logprob": -0.16189071655273438, "compression_ratio": 1.3504273504273505, "no_speech_prob": 2.994418764501461e-06}, {"id": 193, "seek": 103308, "start": 1033.08, "end": 1042.12, "text": " Now the important thing, and this is the leap, which requires not thinking too hard lest", "tokens": [823, 264, 1021, 551, 11, 293, 341, 307, 264, 19438, 11, 597, 7029, 406, 1953, 886, 1152, 287, 377], "temperature": 0.0, "avg_logprob": -0.11147945309862678, "compression_ratio": 1.6859903381642511, "no_speech_prob": 4.860393801209284e-06}, {"id": 194, "seek": 103308, "start": 1042.12, "end": 1045.48, "text": " you realize how surprising and amazing this is.", "tokens": [291, 4325, 577, 8830, 293, 2243, 341, 307, 13], "temperature": 0.0, "avg_logprob": -0.11147945309862678, "compression_ratio": 1.6859903381642511, "no_speech_prob": 4.860393801209284e-06}, {"id": 195, "seek": 103308, "start": 1045.48, "end": 1052.32, "text": " Everything we learn about how to fit a line is identical to how to fit filters and weights", "tokens": [5471, 321, 1466, 466, 577, 281, 3318, 257, 1622, 307, 14800, 281, 577, 281, 3318, 15995, 293, 17443], "temperature": 0.0, "avg_logprob": -0.11147945309862678, "compression_ratio": 1.6859903381642511, "no_speech_prob": 4.860393801209284e-06}, {"id": 196, "seek": 103308, "start": 1052.32, "end": 1054.22, "text": " in a convolutional neural network.", "tokens": [294, 257, 45216, 304, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.11147945309862678, "compression_ratio": 1.6859903381642511, "no_speech_prob": 4.860393801209284e-06}, {"id": 197, "seek": 103308, "start": 1054.22, "end": 1059.04, "text": " And so everything we learn about calculating the slope and the intercept, we will then", "tokens": [400, 370, 1203, 321, 1466, 466, 28258, 264, 13525, 293, 264, 24700, 11, 321, 486, 550], "temperature": 0.0, "avg_logprob": -0.11147945309862678, "compression_ratio": 1.6859903381642511, "no_speech_prob": 4.860393801209284e-06}, {"id": 198, "seek": 105904, "start": 1059.04, "end": 1063.28, "text": " use to let computers see.", "tokens": [764, 281, 718, 10807, 536, 13], "temperature": 0.0, "avg_logprob": -0.1622128441220238, "compression_ratio": 1.8632075471698113, "no_speech_prob": 3.3931310099433176e-06}, {"id": 199, "seek": 105904, "start": 1063.28, "end": 1070.04, "text": " And so the answer to any question which is basically why, is why not.", "tokens": [400, 370, 264, 1867, 281, 604, 1168, 597, 307, 1936, 983, 11, 307, 983, 406, 13], "temperature": 0.0, "avg_logprob": -0.1622128441220238, "compression_ratio": 1.8632075471698113, "no_speech_prob": 3.3931310099433176e-06}, {"id": 200, "seek": 105904, "start": 1070.04, "end": 1073.24, "text": " This is a function that takes some inputs and calculates an output.", "tokens": [639, 307, 257, 2445, 300, 2516, 512, 15743, 293, 4322, 1024, 364, 5598, 13], "temperature": 0.0, "avg_logprob": -0.1622128441220238, "compression_ratio": 1.8632075471698113, "no_speech_prob": 3.3931310099433176e-06}, {"id": 201, "seek": 105904, "start": 1073.24, "end": 1077.04, "text": " This is a function that takes some inputs and calculates an output.", "tokens": [639, 307, 257, 2445, 300, 2516, 512, 15743, 293, 4322, 1024, 364, 5598, 13], "temperature": 0.0, "avg_logprob": -0.1622128441220238, "compression_ratio": 1.8632075471698113, "no_speech_prob": 3.3931310099433176e-06}, {"id": 202, "seek": 105904, "start": 1077.04, "end": 1078.92, "text": " So why not?", "tokens": [407, 983, 406, 30], "temperature": 0.0, "avg_logprob": -0.1622128441220238, "compression_ratio": 1.8632075471698113, "no_speech_prob": 3.3931310099433176e-06}, {"id": 203, "seek": 105904, "start": 1078.92, "end": 1082.8799999999999, "text": " The only reason it wouldn't work would be because it was too slow, for example.", "tokens": [440, 787, 1778, 309, 2759, 380, 589, 576, 312, 570, 309, 390, 886, 2964, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.1622128441220238, "compression_ratio": 1.8632075471698113, "no_speech_prob": 3.3931310099433176e-06}, {"id": 204, "seek": 105904, "start": 1082.8799999999999, "end": 1086.54, "text": " We know it's not too slow because we tried it and it works pretty well.", "tokens": [492, 458, 309, 311, 406, 886, 2964, 570, 321, 3031, 309, 293, 309, 1985, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.1622128441220238, "compression_ratio": 1.8632075471698113, "no_speech_prob": 3.3931310099433176e-06}, {"id": 205, "seek": 108654, "start": 1086.54, "end": 1096.28, "text": " So everything we're about to learn works for any kind of function which has the appropriate", "tokens": [407, 1203, 321, 434, 466, 281, 1466, 1985, 337, 604, 733, 295, 2445, 597, 575, 264, 6854], "temperature": 0.0, "avg_logprob": -0.16857927300956813, "compression_ratio": 1.5980861244019138, "no_speech_prob": 6.438963282562327e-06}, {"id": 206, "seek": 108654, "start": 1096.28, "end": 1097.84, "text": " types of gradients.", "tokens": [3467, 295, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.16857927300956813, "compression_ratio": 1.5980861244019138, "no_speech_prob": 6.438963282562327e-06}, {"id": 207, "seek": 108654, "start": 1097.84, "end": 1101.1599999999999, "text": " And we can talk more about that later.", "tokens": [400, 321, 393, 751, 544, 466, 300, 1780, 13], "temperature": 0.0, "avg_logprob": -0.16857927300956813, "compression_ratio": 1.5980861244019138, "no_speech_prob": 6.438963282562327e-06}, {"id": 208, "seek": 108654, "start": 1101.1599999999999, "end": 1103.94, "text": " But neural nets have the appropriate kinds of gradients.", "tokens": [583, 18161, 36170, 362, 264, 6854, 3685, 295, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.16857927300956813, "compression_ratio": 1.5980861244019138, "no_speech_prob": 6.438963282562327e-06}, {"id": 209, "seek": 108654, "start": 1103.94, "end": 1107.52, "text": " So SGD, we start with a guess.", "tokens": [407, 34520, 35, 11, 321, 722, 365, 257, 2041, 13], "temperature": 0.0, "avg_logprob": -0.16857927300956813, "compression_ratio": 1.5980861244019138, "no_speech_prob": 6.438963282562327e-06}, {"id": 210, "seek": 108654, "start": 1107.52, "end": 1109.44, "text": " What do we think the parameters of our function are?", "tokens": [708, 360, 321, 519, 264, 9834, 295, 527, 2445, 366, 30], "temperature": 0.0, "avg_logprob": -0.16857927300956813, "compression_ratio": 1.5980861244019138, "no_speech_prob": 6.438963282562327e-06}, {"id": 211, "seek": 108654, "start": 1109.44, "end": 1111.3999999999999, "text": " In this case, the intercept and the slope.", "tokens": [682, 341, 1389, 11, 264, 24700, 293, 264, 13525, 13], "temperature": 0.0, "avg_logprob": -0.16857927300956813, "compression_ratio": 1.5980861244019138, "no_speech_prob": 6.438963282562327e-06}, {"id": 212, "seek": 111140, "start": 1111.4, "end": 1117.16, "text": " And with Keras, they will be randomized using the chloro-initialization procedure we learned", "tokens": [400, 365, 591, 6985, 11, 436, 486, 312, 38513, 1228, 264, 18178, 78, 12, 259, 270, 831, 2144, 10747, 321, 3264], "temperature": 0.0, "avg_logprob": -0.22440717734542548, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.5206645002763253e-05}, {"id": 213, "seek": 111140, "start": 1117.16, "end": 1121.76, "text": " about, which is 6 divided by n in plus n out.", "tokens": [466, 11, 597, 307, 1386, 6666, 538, 297, 294, 1804, 297, 484, 13], "temperature": 0.0, "avg_logprob": -0.22440717734542548, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.5206645002763253e-05}, {"id": 214, "seek": 111140, "start": 1121.76, "end": 1127.6000000000001, "text": " But let's assume they're both 1.", "tokens": [583, 718, 311, 6552, 436, 434, 1293, 502, 13], "temperature": 0.0, "avg_logprob": -0.22440717734542548, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.5206645002763253e-05}, {"id": 215, "seek": 111140, "start": 1127.6000000000001, "end": 1131.0400000000002, "text": " We are going to use very, very small mini-batches here.", "tokens": [492, 366, 516, 281, 764, 588, 11, 588, 1359, 8382, 12, 65, 852, 279, 510, 13], "temperature": 0.0, "avg_logprob": -0.22440717734542548, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.5206645002763253e-05}, {"id": 216, "seek": 111140, "start": 1131.0400000000002, "end": 1136.1200000000001, "text": " Mini-batches are going to be of size 1, basically because it's easier to do in Excel and it's", "tokens": [18239, 12, 65, 852, 279, 366, 516, 281, 312, 295, 2744, 502, 11, 1936, 570, 309, 311, 3571, 281, 360, 294, 19060, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.22440717734542548, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.5206645002763253e-05}, {"id": 217, "seek": 111140, "start": 1136.1200000000001, "end": 1137.72, "text": " easier to see.", "tokens": [3571, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.22440717734542548, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.5206645002763253e-05}, {"id": 218, "seek": 113772, "start": 1137.72, "end": 1141.44, "text": " But everything we're going to see would work equally well for a mini-batch of size 4 or", "tokens": [583, 1203, 321, 434, 516, 281, 536, 576, 589, 12309, 731, 337, 257, 8382, 12, 65, 852, 295, 2744, 1017, 420], "temperature": 0.0, "avg_logprob": -0.1802878975868225, "compression_ratio": 1.5207373271889402, "no_speech_prob": 2.4824610136420233e-06}, {"id": 219, "seek": 113772, "start": 1141.44, "end": 1144.52, "text": " 64 or 128.", "tokens": [12145, 420, 29810, 13], "temperature": 0.0, "avg_logprob": -0.1802878975868225, "compression_ratio": 1.5207373271889402, "no_speech_prob": 2.4824610136420233e-06}, {"id": 220, "seek": 113772, "start": 1144.52, "end": 1147.56, "text": " So here's our first row, our first mini-batch.", "tokens": [407, 510, 311, 527, 700, 5386, 11, 527, 700, 8382, 12, 65, 852, 13], "temperature": 0.0, "avg_logprob": -0.1802878975868225, "compression_ratio": 1.5207373271889402, "no_speech_prob": 2.4824610136420233e-06}, {"id": 221, "seek": 113772, "start": 1147.56, "end": 1151.56, "text": " Our input is 14 and our desired output is 58.", "tokens": [2621, 4846, 307, 3499, 293, 527, 14721, 5598, 307, 21786, 13], "temperature": 0.0, "avg_logprob": -0.1802878975868225, "compression_ratio": 1.5207373271889402, "no_speech_prob": 2.4824610136420233e-06}, {"id": 222, "seek": 113772, "start": 1151.56, "end": 1154.96, "text": " And so our guesses to our parameters are 1 and 1.", "tokens": [400, 370, 527, 42703, 281, 527, 9834, 366, 502, 293, 502, 13], "temperature": 0.0, "avg_logprob": -0.1802878975868225, "compression_ratio": 1.5207373271889402, "no_speech_prob": 2.4824610136420233e-06}, {"id": 223, "seek": 113772, "start": 1154.96, "end": 1167.64, "text": " And therefore our predicted Y value is equal to 1 plus 1 times 14, which is normally 15.", "tokens": [400, 4412, 527, 19147, 398, 2158, 307, 2681, 281, 502, 1804, 502, 1413, 3499, 11, 597, 307, 5646, 2119, 13], "temperature": 0.0, "avg_logprob": -0.1802878975868225, "compression_ratio": 1.5207373271889402, "no_speech_prob": 2.4824610136420233e-06}, {"id": 224, "seek": 116764, "start": 1167.64, "end": 1172.3600000000001, "text": " So if we're doing root-mean-squared error, our error squared is prediction minus actual", "tokens": [407, 498, 321, 434, 884, 5593, 12, 1398, 282, 12, 33292, 1642, 6713, 11, 527, 6713, 8889, 307, 17630, 3175, 3539], "temperature": 0.0, "avg_logprob": -0.13893499876323498, "compression_ratio": 1.6473429951690821, "no_speech_prob": 9.080403287953231e-06}, {"id": 225, "seek": 116764, "start": 1172.3600000000001, "end": 1175.16, "text": " squared.", "tokens": [8889, 13], "temperature": 0.0, "avg_logprob": -0.13893499876323498, "compression_ratio": 1.6473429951690821, "no_speech_prob": 9.080403287953231e-06}, {"id": 226, "seek": 116764, "start": 1175.16, "end": 1179.24, "text": " So the next thing we do is we want to calculate the derivative with respect to each of our", "tokens": [407, 264, 958, 551, 321, 360, 307, 321, 528, 281, 8873, 264, 13760, 365, 3104, 281, 1184, 295, 527], "temperature": 0.0, "avg_logprob": -0.13893499876323498, "compression_ratio": 1.6473429951690821, "no_speech_prob": 9.080403287953231e-06}, {"id": 227, "seek": 116764, "start": 1179.24, "end": 1182.16, "text": " two inputs.", "tokens": [732, 15743, 13], "temperature": 0.0, "avg_logprob": -0.13893499876323498, "compression_ratio": 1.6473429951690821, "no_speech_prob": 9.080403287953231e-06}, {"id": 228, "seek": 116764, "start": 1182.16, "end": 1188.0400000000002, "text": " One really easy way to do that is to add a tiny amount to each of the two inputs and", "tokens": [1485, 534, 1858, 636, 281, 360, 300, 307, 281, 909, 257, 5870, 2372, 281, 1184, 295, 264, 732, 15743, 293], "temperature": 0.0, "avg_logprob": -0.13893499876323498, "compression_ratio": 1.6473429951690821, "no_speech_prob": 9.080403287953231e-06}, {"id": 229, "seek": 116764, "start": 1188.0400000000002, "end": 1190.4, "text": " see how the output varies.", "tokens": [536, 577, 264, 5598, 21716, 13], "temperature": 0.0, "avg_logprob": -0.13893499876323498, "compression_ratio": 1.6473429951690821, "no_speech_prob": 9.080403287953231e-06}, {"id": 230, "seek": 116764, "start": 1190.4, "end": 1191.9, "text": " So let's start by doing that.", "tokens": [407, 718, 311, 722, 538, 884, 300, 13], "temperature": 0.0, "avg_logprob": -0.13893499876323498, "compression_ratio": 1.6473429951690821, "no_speech_prob": 9.080403287953231e-06}, {"id": 231, "seek": 119190, "start": 1191.9, "end": 1204.48, "text": " So let's add 0.01 to our intercept and calculate the line and then calculate the loss squared.", "tokens": [407, 718, 311, 909, 1958, 13, 10607, 281, 527, 24700, 293, 8873, 264, 1622, 293, 550, 8873, 264, 4470, 8889, 13], "temperature": 0.0, "avg_logprob": -0.11685520769601845, "compression_ratio": 1.7027027027027026, "no_speech_prob": 1.3081729548503063e-06}, {"id": 232, "seek": 119190, "start": 1204.48, "end": 1210.22, "text": " So this is the error if b is increased by 0.01.", "tokens": [407, 341, 307, 264, 6713, 498, 272, 307, 6505, 538, 1958, 13, 10607, 13], "temperature": 0.0, "avg_logprob": -0.11685520769601845, "compression_ratio": 1.7027027027027026, "no_speech_prob": 1.3081729548503063e-06}, {"id": 233, "seek": 119190, "start": 1210.22, "end": 1214.48, "text": " And then let's calculate the difference between that error and the actual error and then divide", "tokens": [400, 550, 718, 311, 8873, 264, 2649, 1296, 300, 6713, 293, 264, 3539, 6713, 293, 550, 9845], "temperature": 0.0, "avg_logprob": -0.11685520769601845, "compression_ratio": 1.7027027027027026, "no_speech_prob": 1.3081729548503063e-06}, {"id": 234, "seek": 119190, "start": 1214.48, "end": 1218.42, "text": " that by our change, which is 0.01.", "tokens": [300, 538, 527, 1319, 11, 597, 307, 1958, 13, 10607, 13], "temperature": 0.0, "avg_logprob": -0.11685520769601845, "compression_ratio": 1.7027027027027026, "no_speech_prob": 1.3081729548503063e-06}, {"id": 235, "seek": 119190, "start": 1218.42, "end": 1221.2, "text": " And that gives us our estimated gradient.", "tokens": [400, 300, 2709, 505, 527, 14109, 16235, 13], "temperature": 0.0, "avg_logprob": -0.11685520769601845, "compression_ratio": 1.7027027027027026, "no_speech_prob": 1.3081729548503063e-06}, {"id": 236, "seek": 122120, "start": 1221.2, "end": 1223.44, "text": " I'm using de for de-error, db.", "tokens": [286, 478, 1228, 368, 337, 368, 12, 260, 2874, 11, 274, 65, 13], "temperature": 0.0, "avg_logprob": -0.23021614417601166, "compression_ratio": 1.4308510638297873, "no_speech_prob": 5.422198682936141e-06}, {"id": 237, "seek": 122120, "start": 1223.44, "end": 1226.6000000000001, "text": " It should have probably been dl for de-loss, db.", "tokens": [467, 820, 362, 1391, 668, 37873, 337, 368, 12, 75, 772, 11, 274, 65, 13], "temperature": 0.0, "avg_logprob": -0.23021614417601166, "compression_ratio": 1.4308510638297873, "no_speech_prob": 5.422198682936141e-06}, {"id": 238, "seek": 122120, "start": 1226.6000000000001, "end": 1232.2, "text": " So this is the change in loss with respect to b is negative 85.99.", "tokens": [407, 341, 307, 264, 1319, 294, 4470, 365, 3104, 281, 272, 307, 3671, 14695, 13, 8494, 13], "temperature": 0.0, "avg_logprob": -0.23021614417601166, "compression_ratio": 1.4308510638297873, "no_speech_prob": 5.422198682936141e-06}, {"id": 239, "seek": 122120, "start": 1232.2, "end": 1233.9, "text": " We can do the same thing for a.", "tokens": [492, 393, 360, 264, 912, 551, 337, 257, 13], "temperature": 0.0, "avg_logprob": -0.23021614417601166, "compression_ratio": 1.4308510638297873, "no_speech_prob": 5.422198682936141e-06}, {"id": 240, "seek": 122120, "start": 1233.9, "end": 1243.92, "text": " So we can add 0.01 to a and then calculate our line, subtract our actual, take the square,", "tokens": [407, 321, 393, 909, 1958, 13, 10607, 281, 257, 293, 550, 8873, 527, 1622, 11, 16390, 527, 3539, 11, 747, 264, 3732, 11], "temperature": 0.0, "avg_logprob": -0.23021614417601166, "compression_ratio": 1.4308510638297873, "no_speech_prob": 5.422198682936141e-06}, {"id": 241, "seek": 124392, "start": 1243.92, "end": 1252.16, "text": " and so there is our value of estimated loss da, subtract it from the actual loss, divide", "tokens": [293, 370, 456, 307, 527, 2158, 295, 14109, 4470, 1120, 11, 16390, 309, 490, 264, 3539, 4470, 11, 9845], "temperature": 0.0, "avg_logprob": -0.17428947117017662, "compression_ratio": 1.7786259541984732, "no_speech_prob": 1.529408791611786e-06}, {"id": 242, "seek": 124392, "start": 1252.16, "end": 1254.16, "text": " it by 0.01.", "tokens": [309, 538, 1958, 13, 10607, 13], "temperature": 0.0, "avg_logprob": -0.17428947117017662, "compression_ratio": 1.7786259541984732, "no_speech_prob": 1.529408791611786e-06}, {"id": 243, "seek": 124392, "start": 1254.16, "end": 1256.46, "text": " And so there are two estimates as the derivative.", "tokens": [400, 370, 456, 366, 732, 20561, 382, 264, 13760, 13], "temperature": 0.0, "avg_logprob": -0.17428947117017662, "compression_ratio": 1.7786259541984732, "no_speech_prob": 1.529408791611786e-06}, {"id": 244, "seek": 124392, "start": 1256.46, "end": 1260.92, "text": " This approach to estimating the derivative is called finite differencing.", "tokens": [639, 3109, 281, 8017, 990, 264, 13760, 307, 1219, 19362, 743, 13644, 13], "temperature": 0.0, "avg_logprob": -0.17428947117017662, "compression_ratio": 1.7786259541984732, "no_speech_prob": 1.529408791611786e-06}, {"id": 245, "seek": 124392, "start": 1260.92, "end": 1265.0, "text": " At any time you calculate a derivative by hand, you should always use finite differencing", "tokens": [1711, 604, 565, 291, 8873, 257, 13760, 538, 1011, 11, 291, 820, 1009, 764, 19362, 743, 13644], "temperature": 0.0, "avg_logprob": -0.17428947117017662, "compression_ratio": 1.7786259541984732, "no_speech_prob": 1.529408791611786e-06}, {"id": 246, "seek": 124392, "start": 1265.0, "end": 1267.68, "text": " to make sure your calculation is correct.", "tokens": [281, 652, 988, 428, 17108, 307, 3006, 13], "temperature": 0.0, "avg_logprob": -0.17428947117017662, "compression_ratio": 1.7786259541984732, "no_speech_prob": 1.529408791611786e-06}, {"id": 247, "seek": 124392, "start": 1267.68, "end": 1271.6000000000001, "text": " You're not very likely to ever have to do that, however, because all of the libraries", "tokens": [509, 434, 406, 588, 3700, 281, 1562, 362, 281, 360, 300, 11, 4461, 11, 570, 439, 295, 264, 15148], "temperature": 0.0, "avg_logprob": -0.17428947117017662, "compression_ratio": 1.7786259541984732, "no_speech_prob": 1.529408791611786e-06}, {"id": 248, "seek": 124392, "start": 1271.6000000000001, "end": 1273.68, "text": " do derivatives for you.", "tokens": [360, 33733, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.17428947117017662, "compression_ratio": 1.7786259541984732, "no_speech_prob": 1.529408791611786e-06}, {"id": 249, "seek": 127368, "start": 1273.68, "end": 1277.72, "text": " And they do them analytically, not using finite derivatives.", "tokens": [400, 436, 360, 552, 10783, 984, 11, 406, 1228, 19362, 33733, 13], "temperature": 0.0, "avg_logprob": -0.16670248585362588, "compression_ratio": 1.92018779342723, "no_speech_prob": 1.2218970368849114e-05}, {"id": 250, "seek": 127368, "start": 1277.72, "end": 1283.48, "text": " And so here are the derivatives calculated analytically, which you can do by going to", "tokens": [400, 370, 510, 366, 264, 33733, 15598, 10783, 984, 11, 597, 291, 393, 360, 538, 516, 281], "temperature": 0.0, "avg_logprob": -0.16670248585362588, "compression_ratio": 1.92018779342723, "no_speech_prob": 1.2218970368849114e-05}, {"id": 251, "seek": 127368, "start": 1283.48, "end": 1287.22, "text": " Wolfram Alpha and typing in your formula and getting the derivative back.", "tokens": [16634, 2356, 20588, 293, 18444, 294, 428, 8513, 293, 1242, 264, 13760, 646, 13], "temperature": 0.0, "avg_logprob": -0.16670248585362588, "compression_ratio": 1.92018779342723, "no_speech_prob": 1.2218970368849114e-05}, {"id": 252, "seek": 127368, "start": 1287.22, "end": 1291.04, "text": " So this is the analytical derivative of the loss with respect to b and the analytical", "tokens": [407, 341, 307, 264, 29579, 13760, 295, 264, 4470, 365, 3104, 281, 272, 293, 264, 29579], "temperature": 0.0, "avg_logprob": -0.16670248585362588, "compression_ratio": 1.92018779342723, "no_speech_prob": 1.2218970368849114e-05}, {"id": 253, "seek": 127368, "start": 1291.04, "end": 1294.52, "text": " derivative of a.", "tokens": [13760, 295, 257, 13], "temperature": 0.0, "avg_logprob": -0.16670248585362588, "compression_ratio": 1.92018779342723, "no_speech_prob": 1.2218970368849114e-05}, {"id": 254, "seek": 127368, "start": 1294.52, "end": 1298.76, "text": " And so you can see that our analytical and our finite difference are very similar for", "tokens": [400, 370, 291, 393, 536, 300, 527, 29579, 293, 527, 19362, 2649, 366, 588, 2531, 337], "temperature": 0.0, "avg_logprob": -0.16670248585362588, "compression_ratio": 1.92018779342723, "no_speech_prob": 1.2218970368849114e-05}, {"id": 255, "seek": 129876, "start": 1298.76, "end": 1303.64, "text": " b and they are very similar for a.", "tokens": [272, 293, 436, 366, 588, 2531, 337, 257, 13], "temperature": 0.0, "avg_logprob": -0.13100855549176535, "compression_ratio": 1.8058252427184467, "no_speech_prob": 6.8542790359060746e-06}, {"id": 256, "seek": 129876, "start": 1303.64, "end": 1307.16, "text": " So that makes me feel comfortable that we got the calculation correct.", "tokens": [407, 300, 1669, 385, 841, 4619, 300, 321, 658, 264, 17108, 3006, 13], "temperature": 0.0, "avg_logprob": -0.13100855549176535, "compression_ratio": 1.8058252427184467, "no_speech_prob": 6.8542790359060746e-06}, {"id": 257, "seek": 129876, "start": 1307.16, "end": 1315.82, "text": " So all SGD does is it says, okay, this tells us if we change our weights by a little bit,", "tokens": [407, 439, 34520, 35, 775, 307, 309, 1619, 11, 1392, 11, 341, 5112, 505, 498, 321, 1319, 527, 17443, 538, 257, 707, 857, 11], "temperature": 0.0, "avg_logprob": -0.13100855549176535, "compression_ratio": 1.8058252427184467, "no_speech_prob": 6.8542790359060746e-06}, {"id": 258, "seek": 129876, "start": 1315.82, "end": 1322.04, "text": " this is the change in our loss function, we know that increasing our value of b by a bit", "tokens": [341, 307, 264, 1319, 294, 527, 4470, 2445, 11, 321, 458, 300, 5662, 527, 2158, 295, 272, 538, 257, 857], "temperature": 0.0, "avg_logprob": -0.13100855549176535, "compression_ratio": 1.8058252427184467, "no_speech_prob": 6.8542790359060746e-06}, {"id": 259, "seek": 129876, "start": 1322.04, "end": 1327.16, "text": " will decrease the loss function, and we know that increasing our value of a by a little", "tokens": [486, 11514, 264, 4470, 2445, 11, 293, 321, 458, 300, 5662, 527, 2158, 295, 257, 538, 257, 707], "temperature": 0.0, "avg_logprob": -0.13100855549176535, "compression_ratio": 1.8058252427184467, "no_speech_prob": 6.8542790359060746e-06}, {"id": 260, "seek": 132716, "start": 1327.16, "end": 1329.6000000000001, "text": " bit will decrease the loss function.", "tokens": [857, 486, 11514, 264, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.12738612146660833, "compression_ratio": 1.6540284360189574, "no_speech_prob": 9.666038749855943e-06}, {"id": 261, "seek": 132716, "start": 1329.6000000000001, "end": 1332.8000000000002, "text": " So therefore, let's decrease both of them by a little bit.", "tokens": [407, 4412, 11, 718, 311, 11514, 1293, 295, 552, 538, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.12738612146660833, "compression_ratio": 1.6540284360189574, "no_speech_prob": 9.666038749855943e-06}, {"id": 262, "seek": 132716, "start": 1332.8000000000002, "end": 1337.64, "text": " And the way we do that is to multiply the derivative times a learning rate, that's the", "tokens": [400, 264, 636, 321, 360, 300, 307, 281, 12972, 264, 13760, 1413, 257, 2539, 3314, 11, 300, 311, 264], "temperature": 0.0, "avg_logprob": -0.12738612146660833, "compression_ratio": 1.6540284360189574, "no_speech_prob": 9.666038749855943e-06}, {"id": 263, "seek": 132716, "start": 1337.64, "end": 1343.0800000000002, "text": " value of a little bit, and subtract that from our previous guess.", "tokens": [2158, 295, 257, 707, 857, 11, 293, 16390, 300, 490, 527, 3894, 2041, 13], "temperature": 0.0, "avg_logprob": -0.12738612146660833, "compression_ratio": 1.6540284360189574, "no_speech_prob": 9.666038749855943e-06}, {"id": 264, "seek": 132716, "start": 1343.0800000000002, "end": 1347.94, "text": " So we do that for a, and we do that for b, and here are our new guesses.", "tokens": [407, 321, 360, 300, 337, 257, 11, 293, 321, 360, 300, 337, 272, 11, 293, 510, 366, 527, 777, 42703, 13], "temperature": 0.0, "avg_logprob": -0.12738612146660833, "compression_ratio": 1.6540284360189574, "no_speech_prob": 9.666038749855943e-06}, {"id": 265, "seek": 132716, "start": 1347.94, "end": 1351.1200000000001, "text": " Now we're at 1.12 and 1.01.", "tokens": [823, 321, 434, 412, 502, 13, 4762, 293, 502, 13, 10607, 13], "temperature": 0.0, "avg_logprob": -0.12738612146660833, "compression_ratio": 1.6540284360189574, "no_speech_prob": 9.666038749855943e-06}, {"id": 266, "seek": 135112, "start": 1351.12, "end": 1358.52, "text": " And so let's copy them over here, 1.12 and 1.01.", "tokens": [400, 370, 718, 311, 5055, 552, 670, 510, 11, 502, 13, 4762, 293, 502, 13, 10607, 13], "temperature": 0.0, "avg_logprob": -0.12939561843872072, "compression_ratio": 1.5759162303664922, "no_speech_prob": 4.710874236479867e-06}, {"id": 267, "seek": 135112, "start": 1358.52, "end": 1364.12, "text": " And then we do the same thing, and that gives us a new a and a b.", "tokens": [400, 550, 321, 360, 264, 912, 551, 11, 293, 300, 2709, 505, 257, 777, 257, 293, 257, 272, 13], "temperature": 0.0, "avg_logprob": -0.12939561843872072, "compression_ratio": 1.5759162303664922, "no_speech_prob": 4.710874236479867e-06}, {"id": 268, "seek": 135112, "start": 1364.12, "end": 1369.76, "text": " We keep doing that again and again and again until we've gone through the whole data set.", "tokens": [492, 1066, 884, 300, 797, 293, 797, 293, 797, 1826, 321, 600, 2780, 807, 264, 1379, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.12939561843872072, "compression_ratio": 1.5759162303664922, "no_speech_prob": 4.710874236479867e-06}, {"id": 269, "seek": 135112, "start": 1369.76, "end": 1376.04, "text": " At the end of which, we have a guess of a of 2.61 and a guess of b of 1.07.", "tokens": [1711, 264, 917, 295, 597, 11, 321, 362, 257, 2041, 295, 257, 295, 568, 13, 31537, 293, 257, 2041, 295, 272, 295, 502, 13, 16231, 13], "temperature": 0.0, "avg_logprob": -0.12939561843872072, "compression_ratio": 1.5759162303664922, "no_speech_prob": 4.710874236479867e-06}, {"id": 270, "seek": 135112, "start": 1376.04, "end": 1378.3999999999999, "text": " So that's one epoch.", "tokens": [407, 300, 311, 472, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.12939561843872072, "compression_ratio": 1.5759162303664922, "no_speech_prob": 4.710874236479867e-06}, {"id": 271, "seek": 137840, "start": 1378.4, "end": 1383.5600000000002, "text": " Now in real life, we would be having shuffle equals true, which means that these would", "tokens": [823, 294, 957, 993, 11, 321, 576, 312, 1419, 39426, 6915, 2074, 11, 597, 1355, 300, 613, 576], "temperature": 0.0, "avg_logprob": -0.15329994552436915, "compression_ratio": 1.4484536082474226, "no_speech_prob": 3.84491750082816e-06}, {"id": 272, "seek": 137840, "start": 1383.5600000000002, "end": 1385.1200000000001, "text": " be randomized.", "tokens": [312, 38513, 13], "temperature": 0.0, "avg_logprob": -0.15329994552436915, "compression_ratio": 1.4484536082474226, "no_speech_prob": 3.84491750082816e-06}, {"id": 273, "seek": 137840, "start": 1385.1200000000001, "end": 1391.2800000000002, "text": " So this isn't quite perfect, but apart from that, this is SGD with a mini-batch size of", "tokens": [407, 341, 1943, 380, 1596, 2176, 11, 457, 4936, 490, 300, 11, 341, 307, 34520, 35, 365, 257, 8382, 12, 65, 852, 2744, 295], "temperature": 0.0, "avg_logprob": -0.15329994552436915, "compression_ratio": 1.4484536082474226, "no_speech_prob": 3.84491750082816e-06}, {"id": 274, "seek": 137840, "start": 1391.2800000000002, "end": 1392.7, "text": " 1.", "tokens": [502, 13], "temperature": 0.0, "avg_logprob": -0.15329994552436915, "compression_ratio": 1.4484536082474226, "no_speech_prob": 3.84491750082816e-06}, {"id": 275, "seek": 137840, "start": 1392.7, "end": 1404.5600000000002, "text": " So at the end of the epoch, we say, this is our new slope, so let's copy 2.61 over here.", "tokens": [407, 412, 264, 917, 295, 264, 30992, 339, 11, 321, 584, 11, 341, 307, 527, 777, 13525, 11, 370, 718, 311, 5055, 568, 13, 31537, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.15329994552436915, "compression_ratio": 1.4484536082474226, "no_speech_prob": 3.84491750082816e-06}, {"id": 276, "seek": 140456, "start": 1404.56, "end": 1412.72, "text": " And this is our new intercept, so let's copy 1.06 over here.", "tokens": [400, 341, 307, 527, 777, 24700, 11, 370, 718, 311, 5055, 502, 13, 12791, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.11772549882227061, "compression_ratio": 1.6135265700483092, "no_speech_prob": 2.368788955209311e-06}, {"id": 277, "seek": 140456, "start": 1412.72, "end": 1417.76, "text": " And so now, it starts again.", "tokens": [400, 370, 586, 11, 309, 3719, 797, 13], "temperature": 0.0, "avg_logprob": -0.11772549882227061, "compression_ratio": 1.6135265700483092, "no_speech_prob": 2.368788955209311e-06}, {"id": 278, "seek": 140456, "start": 1417.76, "end": 1420.76, "text": " So we can keep doing that again and again and again.", "tokens": [407, 321, 393, 1066, 884, 300, 797, 293, 797, 293, 797, 13], "temperature": 0.0, "avg_logprob": -0.11772549882227061, "compression_ratio": 1.6135265700483092, "no_speech_prob": 2.368788955209311e-06}, {"id": 279, "seek": 140456, "start": 1420.76, "end": 1424.24, "text": " Copy the stuff from the bottom, stick it back at the top, and each one of these is going", "tokens": [25653, 264, 1507, 490, 264, 2767, 11, 2897, 309, 646, 412, 264, 1192, 11, 293, 1184, 472, 295, 613, 307, 516], "temperature": 0.0, "avg_logprob": -0.11772549882227061, "compression_ratio": 1.6135265700483092, "no_speech_prob": 2.368788955209311e-06}, {"id": 280, "seek": 140456, "start": 1424.24, "end": 1425.4199999999998, "text": " to be an epoch.", "tokens": [281, 312, 364, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.11772549882227061, "compression_ratio": 1.6135265700483092, "no_speech_prob": 2.368788955209311e-06}, {"id": 281, "seek": 140456, "start": 1425.4199999999998, "end": 1430.82, "text": " So I recorded a macro with me copying this to the bottom and pasting it at the top and", "tokens": [407, 286, 8287, 257, 18887, 365, 385, 27976, 341, 281, 264, 2767, 293, 1791, 278, 309, 412, 264, 1192, 293], "temperature": 0.0, "avg_logprob": -0.11772549882227061, "compression_ratio": 1.6135265700483092, "no_speech_prob": 2.368788955209311e-06}, {"id": 282, "seek": 143082, "start": 1430.82, "end": 1434.6399999999999, "text": " added something that says for i equals 1 to 5 around it.", "tokens": [3869, 746, 300, 1619, 337, 741, 6915, 502, 281, 1025, 926, 309, 13], "temperature": 0.0, "avg_logprob": -0.16964110587407083, "compression_ratio": 1.5841121495327102, "no_speech_prob": 8.267803423223086e-06}, {"id": 283, "seek": 143082, "start": 1434.6399999999999, "end": 1441.9199999999998, "text": " So now if I click run, it will copy and paste it 5 times.", "tokens": [407, 586, 498, 286, 2052, 1190, 11, 309, 486, 5055, 293, 9163, 309, 1025, 1413, 13], "temperature": 0.0, "avg_logprob": -0.16964110587407083, "compression_ratio": 1.5841121495327102, "no_speech_prob": 8.267803423223086e-06}, {"id": 284, "seek": 143082, "start": 1441.9199999999998, "end": 1444.08, "text": " And so you can see it's gradually getting closer.", "tokens": [400, 370, 291, 393, 536, 309, 311, 13145, 1242, 4966, 13], "temperature": 0.0, "avg_logprob": -0.16964110587407083, "compression_ratio": 1.5841121495327102, "no_speech_prob": 8.267803423223086e-06}, {"id": 285, "seek": 143082, "start": 1444.08, "end": 1451.8799999999999, "text": " And we know that our goal is that it should be a, a equals 2 and b equals 30.", "tokens": [400, 321, 458, 300, 527, 3387, 307, 300, 309, 820, 312, 257, 11, 257, 6915, 568, 293, 272, 6915, 2217, 13], "temperature": 0.0, "avg_logprob": -0.16964110587407083, "compression_ratio": 1.5841121495327102, "no_speech_prob": 8.267803423223086e-06}, {"id": 286, "seek": 143082, "start": 1451.8799999999999, "end": 1458.6599999999999, "text": " So we've got as far as a equals 2.5 and b equals 1.3.", "tokens": [407, 321, 600, 658, 382, 1400, 382, 257, 6915, 568, 13, 20, 293, 272, 6915, 502, 13, 18, 13], "temperature": 0.0, "avg_logprob": -0.16964110587407083, "compression_ratio": 1.5841121495327102, "no_speech_prob": 8.267803423223086e-06}, {"id": 287, "seek": 143082, "start": 1458.6599999999999, "end": 1460.2, "text": " So they're better than our starting point.", "tokens": [407, 436, 434, 1101, 813, 527, 2891, 935, 13], "temperature": 0.0, "avg_logprob": -0.16964110587407083, "compression_ratio": 1.5841121495327102, "no_speech_prob": 8.267803423223086e-06}, {"id": 288, "seek": 146020, "start": 1460.2, "end": 1466.2, "text": " And you can see our gradually improving loss function.", "tokens": [400, 291, 393, 536, 527, 13145, 11470, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.23162687201248972, "compression_ratio": 1.7025862068965518, "no_speech_prob": 9.223350389220286e-06}, {"id": 289, "seek": 146020, "start": 1466.2, "end": 1467.88, "text": " But it's going to take a long time.", "tokens": [583, 309, 311, 516, 281, 747, 257, 938, 565, 13], "temperature": 0.0, "avg_logprob": -0.23162687201248972, "compression_ratio": 1.7025862068965518, "no_speech_prob": 9.223350389220286e-06}, {"id": 290, "seek": 146020, "start": 1467.88, "end": 1468.88, "text": " Yes, Rachel?", "tokens": [1079, 11, 14246, 30], "temperature": 0.0, "avg_logprob": -0.23162687201248972, "compression_ratio": 1.7025862068965518, "no_speech_prob": 9.223350389220286e-06}, {"id": 291, "seek": 146020, "start": 1468.88, "end": 1469.88, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.23162687201248972, "compression_ratio": 1.7025862068965518, "no_speech_prob": 9.223350389220286e-06}, {"id": 292, "seek": 146020, "start": 1469.88, "end": 1474.52, "text": " Can we still do analytic derivatives when we are using non-linear activation functions?", "tokens": [1664, 321, 920, 360, 40358, 33733, 562, 321, 366, 1228, 2107, 12, 28263, 24433, 6828, 30], "temperature": 0.0, "avg_logprob": -0.23162687201248972, "compression_ratio": 1.7025862068965518, "no_speech_prob": 9.223350389220286e-06}, {"id": 293, "seek": 146020, "start": 1474.52, "end": 1480.52, "text": " Yes, we can use analytical derivatives as long as we're using a function that has an", "tokens": [1079, 11, 321, 393, 764, 29579, 33733, 382, 938, 382, 321, 434, 1228, 257, 2445, 300, 575, 364], "temperature": 0.0, "avg_logprob": -0.23162687201248972, "compression_ratio": 1.7025862068965518, "no_speech_prob": 9.223350389220286e-06}, {"id": 294, "seek": 146020, "start": 1480.52, "end": 1487.48, "text": " analytical derivative, which is pretty much every useful function you can think of, except", "tokens": [29579, 13760, 11, 597, 307, 1238, 709, 633, 4420, 2445, 291, 393, 519, 295, 11, 3993], "temperature": 0.0, "avg_logprob": -0.23162687201248972, "compression_ratio": 1.7025862068965518, "no_speech_prob": 9.223350389220286e-06}, {"id": 295, "seek": 148748, "start": 1487.48, "end": 1491.72, "text": " ones that you can't have something that has an if-then statement in it because it kind", "tokens": [2306, 300, 291, 393, 380, 362, 746, 300, 575, 364, 498, 12, 19096, 5629, 294, 309, 570, 309, 733], "temperature": 0.0, "avg_logprob": -0.21814371744791666, "compression_ratio": 1.4545454545454546, "no_speech_prob": 3.966946223954437e-06}, {"id": 296, "seek": 148748, "start": 1491.72, "end": 1495.16, "text": " of jumps from here to here, but even those you can approximate.", "tokens": [295, 16704, 490, 510, 281, 510, 11, 457, 754, 729, 291, 393, 30874, 13], "temperature": 0.0, "avg_logprob": -0.21814371744791666, "compression_ratio": 1.4545454545454546, "no_speech_prob": 3.966946223954437e-06}, {"id": 297, "seek": 148748, "start": 1495.16, "end": 1499.84, "text": " So a good example would be ReLU.", "tokens": [407, 257, 665, 1365, 576, 312, 1300, 43, 52, 13], "temperature": 0.0, "avg_logprob": -0.21814371744791666, "compression_ratio": 1.4545454545454546, "no_speech_prob": 3.966946223954437e-06}, {"id": 298, "seek": 148748, "start": 1499.84, "end": 1508.8, "text": " So ReLU, which is max of 0,x strictly speaking doesn't really have a derivative at every", "tokens": [407, 1300, 43, 52, 11, 597, 307, 11469, 295, 1958, 11, 87, 20792, 4124, 1177, 380, 534, 362, 257, 13760, 412, 633], "temperature": 0.0, "avg_logprob": -0.21814371744791666, "compression_ratio": 1.4545454545454546, "no_speech_prob": 3.966946223954437e-06}, {"id": 299, "seek": 150880, "start": 1508.8, "end": 1523.12, "text": " point, or at least not a well-defined one, because this is what ReLU looks like.", "tokens": [935, 11, 420, 412, 1935, 406, 257, 731, 12, 37716, 472, 11, 570, 341, 307, 437, 1300, 43, 52, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.1588925804410662, "compression_ratio": 1.450381679389313, "no_speech_prob": 2.9023035494901706e-06}, {"id": 300, "seek": 150880, "start": 1523.12, "end": 1532.28, "text": " And so its derivative here is 0 and its derivative here is 1.", "tokens": [400, 370, 1080, 13760, 510, 307, 1958, 293, 1080, 13760, 510, 307, 502, 13], "temperature": 0.0, "avg_logprob": -0.1588925804410662, "compression_ratio": 1.450381679389313, "no_speech_prob": 2.9023035494901706e-06}, {"id": 301, "seek": 150880, "start": 1532.28, "end": 1535.8799999999999, "text": " What is its derivative exactly here?", "tokens": [708, 307, 1080, 13760, 2293, 510, 30], "temperature": 0.0, "avg_logprob": -0.1588925804410662, "compression_ratio": 1.450381679389313, "no_speech_prob": 2.9023035494901706e-06}, {"id": 302, "seek": 150880, "start": 1535.8799999999999, "end": 1537.3999999999999, "text": " Who knows?", "tokens": [2102, 3255, 30], "temperature": 0.0, "avg_logprob": -0.1588925804410662, "compression_ratio": 1.450381679389313, "no_speech_prob": 2.9023035494901706e-06}, {"id": 303, "seek": 153740, "start": 1537.4, "end": 1541.68, "text": " But the thing is, mathematicians care about that kind of thing, we don't.", "tokens": [583, 264, 551, 307, 11, 32811, 2567, 1127, 466, 300, 733, 295, 551, 11, 321, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.12675447371399517, "compression_ratio": 1.7260869565217392, "no_speech_prob": 8.53024630487198e-06}, {"id": 304, "seek": 153740, "start": 1541.68, "end": 1547.0800000000002, "text": " Like in real life, this is a computer, and computers are never exactly anything.", "tokens": [1743, 294, 957, 993, 11, 341, 307, 257, 3820, 11, 293, 10807, 366, 1128, 2293, 1340, 13], "temperature": 0.0, "avg_logprob": -0.12675447371399517, "compression_ratio": 1.7260869565217392, "no_speech_prob": 8.53024630487198e-06}, {"id": 305, "seek": 153740, "start": 1547.0800000000002, "end": 1550.52, "text": " We can either assume that it's like an infinite amount to this side or an infinite amount", "tokens": [492, 393, 2139, 6552, 300, 309, 311, 411, 364, 13785, 2372, 281, 341, 1252, 420, 364, 13785, 2372], "temperature": 0.0, "avg_logprob": -0.12675447371399517, "compression_ratio": 1.7260869565217392, "no_speech_prob": 8.53024630487198e-06}, {"id": 306, "seek": 153740, "start": 1550.52, "end": 1552.9, "text": " to this side, and who cares.", "tokens": [281, 341, 1252, 11, 293, 567, 12310, 13], "temperature": 0.0, "avg_logprob": -0.12675447371399517, "compression_ratio": 1.7260869565217392, "no_speech_prob": 8.53024630487198e-06}, {"id": 307, "seek": 153740, "start": 1552.9, "end": 1559.88, "text": " So as long as it has a derivative that you can calculate in a meaningful way in practice", "tokens": [407, 382, 938, 382, 309, 575, 257, 13760, 300, 291, 393, 8873, 294, 257, 10995, 636, 294, 3124], "temperature": 0.0, "avg_logprob": -0.12675447371399517, "compression_ratio": 1.7260869565217392, "no_speech_prob": 8.53024630487198e-06}, {"id": 308, "seek": 153740, "start": 1559.88, "end": 1566.76, "text": " on a computer, then it'll be fine.", "tokens": [322, 257, 3820, 11, 550, 309, 603, 312, 2489, 13], "temperature": 0.0, "avg_logprob": -0.12675447371399517, "compression_ratio": 1.7260869565217392, "no_speech_prob": 8.53024630487198e-06}, {"id": 309, "seek": 156676, "start": 1566.76, "end": 1569.44, "text": " So one thing you might have noticed about this is it's going to take an awfully long", "tokens": [407, 472, 551, 291, 1062, 362, 5694, 466, 341, 307, 309, 311, 516, 281, 747, 364, 47976, 938], "temperature": 0.0, "avg_logprob": -0.20258406769457482, "compression_ratio": 1.7220216606498195, "no_speech_prob": 2.3187170882010832e-05}, {"id": 310, "seek": 156676, "start": 1569.44, "end": 1572.12, "text": " time to get anywhere.", "tokens": [565, 281, 483, 4992, 13], "temperature": 0.0, "avg_logprob": -0.20258406769457482, "compression_ratio": 1.7220216606498195, "no_speech_prob": 2.3187170882010832e-05}, {"id": 311, "seek": 156676, "start": 1572.12, "end": 1575.48, "text": " And so you might think, okay, let's increase the learning rate.", "tokens": [400, 370, 291, 1062, 519, 11, 1392, 11, 718, 311, 3488, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.20258406769457482, "compression_ratio": 1.7220216606498195, "no_speech_prob": 2.3187170882010832e-05}, {"id": 312, "seek": 156676, "start": 1575.48, "end": 1577.8799999999999, "text": " Fine, let's increase the learning rate.", "tokens": [12024, 11, 718, 311, 3488, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.20258406769457482, "compression_ratio": 1.7220216606498195, "no_speech_prob": 2.3187170882010832e-05}, {"id": 313, "seek": 156676, "start": 1577.8799999999999, "end": 1580.8799999999999, "text": " So let's get rid of one of these zeros.", "tokens": [407, 718, 311, 483, 3973, 295, 472, 295, 613, 35193, 13], "temperature": 0.0, "avg_logprob": -0.20258406769457482, "compression_ratio": 1.7220216606498195, "no_speech_prob": 2.3187170882010832e-05}, {"id": 314, "seek": 156676, "start": 1580.8799999999999, "end": 1584.04, "text": " Oh dear, something went crazy.", "tokens": [876, 6875, 11, 746, 1437, 3219, 13], "temperature": 0.0, "avg_logprob": -0.20258406769457482, "compression_ratio": 1.7220216606498195, "no_speech_prob": 2.3187170882010832e-05}, {"id": 315, "seek": 156676, "start": 1584.04, "end": 1585.04, "text": " What went crazy?", "tokens": [708, 1437, 3219, 30], "temperature": 0.0, "avg_logprob": -0.20258406769457482, "compression_ratio": 1.7220216606498195, "no_speech_prob": 2.3187170882010832e-05}, {"id": 316, "seek": 156676, "start": 1585.04, "end": 1587.08, "text": " I'll tell you what went crazy.", "tokens": [286, 603, 980, 291, 437, 1437, 3219, 13], "temperature": 0.0, "avg_logprob": -0.20258406769457482, "compression_ratio": 1.7220216606498195, "no_speech_prob": 2.3187170882010832e-05}, {"id": 317, "seek": 156676, "start": 1587.08, "end": 1592.84, "text": " Our A's and B's started to go out to like 11 million, which is not the correct answer.", "tokens": [2621, 316, 311, 293, 363, 311, 1409, 281, 352, 484, 281, 411, 2975, 2459, 11, 597, 307, 406, 264, 3006, 1867, 13], "temperature": 0.0, "avg_logprob": -0.20258406769457482, "compression_ratio": 1.7220216606498195, "no_speech_prob": 2.3187170882010832e-05}, {"id": 318, "seek": 156676, "start": 1592.84, "end": 1594.72, "text": " So how did it go ahead and do that?", "tokens": [407, 577, 630, 309, 352, 2286, 293, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.20258406769457482, "compression_ratio": 1.7220216606498195, "no_speech_prob": 2.3187170882010832e-05}, {"id": 319, "seek": 156676, "start": 1594.72, "end": 1596.34, "text": " Well here's the problem.", "tokens": [1042, 510, 311, 264, 1154, 13], "temperature": 0.0, "avg_logprob": -0.20258406769457482, "compression_ratio": 1.7220216606498195, "no_speech_prob": 2.3187170882010832e-05}, {"id": 320, "seek": 159634, "start": 1596.34, "end": 1600.4399999999998, "text": " Let's say this was the shape of our loss function.", "tokens": [961, 311, 584, 341, 390, 264, 3909, 295, 527, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.26256203882902573, "compression_ratio": 1.8038277511961722, "no_speech_prob": 7.76691649662098e-06}, {"id": 321, "seek": 159634, "start": 1600.4399999999998, "end": 1603.32, "text": " And this was our initial guess.", "tokens": [400, 341, 390, 527, 5883, 2041, 13], "temperature": 0.0, "avg_logprob": -0.26256203882902573, "compression_ratio": 1.8038277511961722, "no_speech_prob": 7.76691649662098e-06}, {"id": 322, "seek": 159634, "start": 1603.32, "end": 1606.72, "text": " We figured out the derivative is going this way.", "tokens": [492, 8932, 484, 264, 13760, 307, 516, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.26256203882902573, "compression_ratio": 1.8038277511961722, "no_speech_prob": 7.76691649662098e-06}, {"id": 323, "seek": 159634, "start": 1606.72, "end": 1610.76, "text": " Well actually the derivative is positive, so we want to go the opposite direction.", "tokens": [1042, 767, 264, 13760, 307, 3353, 11, 370, 321, 528, 281, 352, 264, 6182, 3513, 13], "temperature": 0.0, "avg_logprob": -0.26256203882902573, "compression_ratio": 1.8038277511961722, "no_speech_prob": 7.76691649662098e-06}, {"id": 324, "seek": 159634, "start": 1610.76, "end": 1614.24, "text": " And so we step a little bit over here.", "tokens": [400, 370, 321, 1823, 257, 707, 857, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.26256203882902573, "compression_ratio": 1.8038277511961722, "no_speech_prob": 7.76691649662098e-06}, {"id": 325, "seek": 159634, "start": 1614.24, "end": 1616.1999999999998, "text": " And then that leaves us to here.", "tokens": [400, 550, 300, 5510, 505, 281, 510, 13], "temperature": 0.0, "avg_logprob": -0.26256203882902573, "compression_ratio": 1.8038277511961722, "no_speech_prob": 7.76691649662098e-06}, {"id": 326, "seek": 159634, "start": 1616.1999999999998, "end": 1618.6, "text": " We step a little bit further.", "tokens": [492, 1823, 257, 707, 857, 3052, 13], "temperature": 0.0, "avg_logprob": -0.26256203882902573, "compression_ratio": 1.8038277511961722, "no_speech_prob": 7.76691649662098e-06}, {"id": 327, "seek": 159634, "start": 1618.6, "end": 1619.6, "text": " And this looks good.", "tokens": [400, 341, 1542, 665, 13], "temperature": 0.0, "avg_logprob": -0.26256203882902573, "compression_ratio": 1.8038277511961722, "no_speech_prob": 7.76691649662098e-06}, {"id": 328, "seek": 159634, "start": 1619.6, "end": 1623.5, "text": " But then we increase the learning rate.", "tokens": [583, 550, 321, 3488, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.26256203882902573, "compression_ratio": 1.8038277511961722, "no_speech_prob": 7.76691649662098e-06}, {"id": 329, "seek": 162350, "start": 1623.5, "end": 1628.12, "text": " So rather than stepping a little bit, we stepped a long way.", "tokens": [407, 2831, 813, 16821, 257, 707, 857, 11, 321, 15251, 257, 938, 636, 13], "temperature": 0.0, "avg_logprob": -0.17430834717802948, "compression_ratio": 1.9011627906976745, "no_speech_prob": 2.1233724964986322e-06}, {"id": 330, "seek": 162350, "start": 1628.12, "end": 1630.36, "text": " And that put us here.", "tokens": [400, 300, 829, 505, 510, 13], "temperature": 0.0, "avg_logprob": -0.17430834717802948, "compression_ratio": 1.9011627906976745, "no_speech_prob": 2.1233724964986322e-06}, {"id": 331, "seek": 162350, "start": 1630.36, "end": 1632.32, "text": " And then we stepped a long way again.", "tokens": [400, 550, 321, 15251, 257, 938, 636, 797, 13], "temperature": 0.0, "avg_logprob": -0.17430834717802948, "compression_ratio": 1.9011627906976745, "no_speech_prob": 2.1233724964986322e-06}, {"id": 332, "seek": 162350, "start": 1632.32, "end": 1635.2, "text": " And that put us here.", "tokens": [400, 300, 829, 505, 510, 13], "temperature": 0.0, "avg_logprob": -0.17430834717802948, "compression_ratio": 1.9011627906976745, "no_speech_prob": 2.1233724964986322e-06}, {"id": 333, "seek": 162350, "start": 1635.2, "end": 1639.56, "text": " If your learning rate is too high, you're going to get worse and worse.", "tokens": [759, 428, 2539, 3314, 307, 886, 1090, 11, 291, 434, 516, 281, 483, 5324, 293, 5324, 13], "temperature": 0.0, "avg_logprob": -0.17430834717802948, "compression_ratio": 1.9011627906976745, "no_speech_prob": 2.1233724964986322e-06}, {"id": 334, "seek": 162350, "start": 1639.56, "end": 1641.7, "text": " And that's what happened.", "tokens": [400, 300, 311, 437, 2011, 13], "temperature": 0.0, "avg_logprob": -0.17430834717802948, "compression_ratio": 1.9011627906976745, "no_speech_prob": 2.1233724964986322e-06}, {"id": 335, "seek": 162350, "start": 1641.7, "end": 1650.8, "text": " So getting your learning rate right is critical to getting your thing to train at all.", "tokens": [407, 1242, 428, 2539, 3314, 558, 307, 4924, 281, 1242, 428, 551, 281, 3847, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.17430834717802948, "compression_ratio": 1.9011627906976745, "no_speech_prob": 2.1233724964986322e-06}, {"id": 336, "seek": 165080, "start": 1650.8, "end": 1655.6399999999999, "text": " Exploding gradients, or you can even have gradients that do the opposite.", "tokens": [12514, 8616, 2771, 2448, 11, 420, 291, 393, 754, 362, 2771, 2448, 300, 360, 264, 6182, 13], "temperature": 0.0, "avg_logprob": -0.2022226590376634, "compression_ratio": 1.6205357142857142, "no_speech_prob": 2.1782123440061696e-05}, {"id": 337, "seek": 165080, "start": 1655.6399999999999, "end": 1660.3999999999999, "text": " Exploding gradients is something a little bit different, but it's a similar idea.", "tokens": [12514, 8616, 2771, 2448, 307, 746, 257, 707, 857, 819, 11, 457, 309, 311, 257, 2531, 1558, 13], "temperature": 0.0, "avg_logprob": -0.2022226590376634, "compression_ratio": 1.6205357142857142, "no_speech_prob": 2.1782123440061696e-05}, {"id": 338, "seek": 165080, "start": 1660.3999999999999, "end": 1664.6399999999999, "text": " So it looks like.001 is the best we can do.", "tokens": [407, 309, 1542, 411, 2411, 628, 16, 307, 264, 1151, 321, 393, 360, 13], "temperature": 0.0, "avg_logprob": -0.2022226590376634, "compression_ratio": 1.6205357142857142, "no_speech_prob": 2.1782123440061696e-05}, {"id": 339, "seek": 165080, "start": 1664.6399999999999, "end": 1667.3999999999999, "text": " And that's a bit sad because this is really slow.", "tokens": [400, 300, 311, 257, 857, 4227, 570, 341, 307, 534, 2964, 13], "temperature": 0.0, "avg_logprob": -0.2022226590376634, "compression_ratio": 1.6205357142857142, "no_speech_prob": 2.1782123440061696e-05}, {"id": 340, "seek": 165080, "start": 1667.3999999999999, "end": 1669.6399999999999, "text": " So let's try and improve it.", "tokens": [407, 718, 311, 853, 293, 3470, 309, 13], "temperature": 0.0, "avg_logprob": -0.2022226590376634, "compression_ratio": 1.6205357142857142, "no_speech_prob": 2.1782123440061696e-05}, {"id": 341, "seek": 165080, "start": 1669.6399999999999, "end": 1677.08, "text": " So one thing we could do is to say, well, given that every time we've been, actually", "tokens": [407, 472, 551, 321, 727, 360, 307, 281, 584, 11, 731, 11, 2212, 300, 633, 565, 321, 600, 668, 11, 767], "temperature": 0.0, "avg_logprob": -0.2022226590376634, "compression_ratio": 1.6205357142857142, "no_speech_prob": 2.1782123440061696e-05}, {"id": 342, "seek": 167708, "start": 1677.08, "end": 1681.12, "text": " let me do this in a few more dimensions.", "tokens": [718, 385, 360, 341, 294, 257, 1326, 544, 12819, 13], "temperature": 0.0, "avg_logprob": -0.11444808112250435, "compression_ratio": 1.61, "no_speech_prob": 7.64645665185526e-06}, {"id": 343, "seek": 167708, "start": 1681.12, "end": 1688.3, "text": " So let's say we had a 3-dimensional set of axes now, and we kind of had a loss function", "tokens": [407, 718, 311, 584, 321, 632, 257, 805, 12, 18759, 992, 295, 35387, 586, 11, 293, 321, 733, 295, 632, 257, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.11444808112250435, "compression_ratio": 1.61, "no_speech_prob": 7.64645665185526e-06}, {"id": 344, "seek": 167708, "start": 1688.3, "end": 1692.1999999999998, "text": " that looks like this kind of valley.", "tokens": [300, 1542, 411, 341, 733, 295, 17636, 13], "temperature": 0.0, "avg_logprob": -0.11444808112250435, "compression_ratio": 1.61, "no_speech_prob": 7.64645665185526e-06}, {"id": 345, "seek": 167708, "start": 1692.1999999999998, "end": 1695.6399999999999, "text": " And let's say our initial guess was somewhere over here.", "tokens": [400, 718, 311, 584, 527, 5883, 2041, 390, 4079, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.11444808112250435, "compression_ratio": 1.61, "no_speech_prob": 7.64645665185526e-06}, {"id": 346, "seek": 167708, "start": 1695.6399999999999, "end": 1701.0, "text": " So over here, the gradient is pointing in this direction.", "tokens": [407, 670, 510, 11, 264, 16235, 307, 12166, 294, 341, 3513, 13], "temperature": 0.0, "avg_logprob": -0.11444808112250435, "compression_ratio": 1.61, "no_speech_prob": 7.64645665185526e-06}, {"id": 347, "seek": 167708, "start": 1701.0, "end": 1705.28, "text": " So we might make a step and end up there.", "tokens": [407, 321, 1062, 652, 257, 1823, 293, 917, 493, 456, 13], "temperature": 0.0, "avg_logprob": -0.11444808112250435, "compression_ratio": 1.61, "no_speech_prob": 7.64645665185526e-06}, {"id": 348, "seek": 170528, "start": 1705.28, "end": 1710.48, "text": " And then we might make another step which would put us there.", "tokens": [400, 550, 321, 1062, 652, 1071, 1823, 597, 576, 829, 505, 456, 13], "temperature": 0.0, "avg_logprob": -0.18326042947315035, "compression_ratio": 1.669767441860465, "no_speech_prob": 1.6701280401321128e-05}, {"id": 349, "seek": 170528, "start": 1710.48, "end": 1715.2, "text": " And this is actually the most common thing that happens in neural networks.", "tokens": [400, 341, 307, 767, 264, 881, 2689, 551, 300, 2314, 294, 18161, 9590, 13], "temperature": 0.0, "avg_logprob": -0.18326042947315035, "compression_ratio": 1.669767441860465, "no_speech_prob": 1.6701280401321128e-05}, {"id": 350, "seek": 170528, "start": 1715.2, "end": 1721.16, "text": " Something that's kind of flat in one dimension like this is called a saddle point.", "tokens": [6595, 300, 311, 733, 295, 4962, 294, 472, 10139, 411, 341, 307, 1219, 257, 30459, 935, 13], "temperature": 0.0, "avg_logprob": -0.18326042947315035, "compression_ratio": 1.669767441860465, "no_speech_prob": 1.6701280401321128e-05}, {"id": 351, "seek": 170528, "start": 1721.16, "end": 1726.0, "text": " And it's actually been proved that the vast majority of the space of a loss function in", "tokens": [400, 309, 311, 767, 668, 14617, 300, 264, 8369, 6286, 295, 264, 1901, 295, 257, 4470, 2445, 294], "temperature": 0.0, "avg_logprob": -0.18326042947315035, "compression_ratio": 1.669767441860465, "no_speech_prob": 1.6701280401321128e-05}, {"id": 352, "seek": 170528, "start": 1726.0, "end": 1729.6, "text": " a neural network is pretty much all saddle points.", "tokens": [257, 18161, 3209, 307, 1238, 709, 439, 30459, 2793, 13], "temperature": 0.0, "avg_logprob": -0.18326042947315035, "compression_ratio": 1.669767441860465, "no_speech_prob": 1.6701280401321128e-05}, {"id": 353, "seek": 172960, "start": 1729.6, "end": 1738.6, "text": " So when you look at this, it's pretty obvious what should be done.", "tokens": [407, 562, 291, 574, 412, 341, 11, 309, 311, 1238, 6322, 437, 820, 312, 1096, 13], "temperature": 0.0, "avg_logprob": -0.1917033465403431, "compression_ratio": 1.7366071428571428, "no_speech_prob": 5.422178219305351e-06}, {"id": 354, "seek": 172960, "start": 1738.6, "end": 1744.86, "text": " Which is if we go to here and then we go to here, we can say, well on average, we're kind", "tokens": [3013, 307, 498, 321, 352, 281, 510, 293, 550, 321, 352, 281, 510, 11, 321, 393, 584, 11, 731, 322, 4274, 11, 321, 434, 733], "temperature": 0.0, "avg_logprob": -0.1917033465403431, "compression_ratio": 1.7366071428571428, "no_speech_prob": 5.422178219305351e-06}, {"id": 355, "seek": 172960, "start": 1744.86, "end": 1746.3999999999999, "text": " of obviously heading in this direction.", "tokens": [295, 2745, 9864, 294, 341, 3513, 13], "temperature": 0.0, "avg_logprob": -0.1917033465403431, "compression_ratio": 1.7366071428571428, "no_speech_prob": 5.422178219305351e-06}, {"id": 356, "seek": 172960, "start": 1746.3999999999999, "end": 1749.6399999999999, "text": " Especially when we do it again, we're obviously heading in this direction.", "tokens": [8545, 562, 321, 360, 309, 797, 11, 321, 434, 2745, 9864, 294, 341, 3513, 13], "temperature": 0.0, "avg_logprob": -0.1917033465403431, "compression_ratio": 1.7366071428571428, "no_speech_prob": 5.422178219305351e-06}, {"id": 357, "seek": 172960, "start": 1749.6399999999999, "end": 1755.12, "text": " So let's take the average of how we've been going so far and do a bit of that.", "tokens": [407, 718, 311, 747, 264, 4274, 295, 577, 321, 600, 668, 516, 370, 1400, 293, 360, 257, 857, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.1917033465403431, "compression_ratio": 1.7366071428571428, "no_speech_prob": 5.422178219305351e-06}, {"id": 358, "seek": 172960, "start": 1755.12, "end": 1757.9199999999998, "text": " And that's exactly what momentum does.", "tokens": [400, 300, 311, 2293, 437, 11244, 775, 13], "temperature": 0.0, "avg_logprob": -0.1917033465403431, "compression_ratio": 1.7366071428571428, "no_speech_prob": 5.422178219305351e-06}, {"id": 359, "seek": 175792, "start": 1757.92, "end": 1759.96, "text": " Question.", "tokens": [14464, 13], "temperature": 0.0, "avg_logprob": -0.1973867416381836, "compression_ratio": 1.7393939393939395, "no_speech_prob": 1.0451410162204411e-05}, {"id": 360, "seek": 175792, "start": 1759.96, "end": 1767.92, "text": " If relu isn't the cost function, why are we concerned with its differentiability?", "tokens": [759, 1039, 84, 1943, 380, 264, 2063, 2445, 11, 983, 366, 321, 5922, 365, 1080, 27372, 2310, 30], "temperature": 0.0, "avg_logprob": -0.1973867416381836, "compression_ratio": 1.7393939393939395, "no_speech_prob": 1.0451410162204411e-05}, {"id": 361, "seek": 175792, "start": 1767.92, "end": 1774.16, "text": " We care about the derivative of the output with respect to the inputs.", "tokens": [492, 1127, 466, 264, 13760, 295, 264, 5598, 365, 3104, 281, 264, 15743, 13], "temperature": 0.0, "avg_logprob": -0.1973867416381836, "compression_ratio": 1.7393939393939395, "no_speech_prob": 1.0451410162204411e-05}, {"id": 362, "seek": 175792, "start": 1774.16, "end": 1776.28, "text": " The inputs are the filters.", "tokens": [440, 15743, 366, 264, 15995, 13], "temperature": 0.0, "avg_logprob": -0.1973867416381836, "compression_ratio": 1.7393939393939395, "no_speech_prob": 1.0451410162204411e-05}, {"id": 363, "seek": 175792, "start": 1776.28, "end": 1780.96, "text": " And remember the loss function consists of a function of a function of a function of", "tokens": [400, 1604, 264, 4470, 2445, 14689, 295, 257, 2445, 295, 257, 2445, 295, 257, 2445, 295], "temperature": 0.0, "avg_logprob": -0.1973867416381836, "compression_ratio": 1.7393939393939395, "no_speech_prob": 1.0451410162204411e-05}, {"id": 364, "seek": 175792, "start": 1780.96, "end": 1781.96, "text": " a function.", "tokens": [257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1973867416381836, "compression_ratio": 1.7393939393939395, "no_speech_prob": 1.0451410162204411e-05}, {"id": 365, "seek": 178196, "start": 1781.96, "end": 1793.72, "text": " So it is categorical cross-entropy loss applied to softmax, applied to relu, applied to dense", "tokens": [407, 309, 307, 19250, 804, 3278, 12, 317, 27514, 4470, 6456, 281, 2787, 41167, 11, 6456, 281, 1039, 84, 11, 6456, 281, 18011], "temperature": 0.0, "avg_logprob": -0.15687620639801025, "compression_ratio": 1.816831683168317, "no_speech_prob": 9.570796919433633e-07}, {"id": 366, "seek": 178196, "start": 1793.72, "end": 1799.2, "text": " layer, applied to max pooling, applied to relu, applied to convolutions, etc.", "tokens": [4583, 11, 6456, 281, 11469, 7005, 278, 11, 6456, 281, 1039, 84, 11, 6456, 281, 3754, 15892, 11, 5183, 13], "temperature": 0.0, "avg_logprob": -0.15687620639801025, "compression_ratio": 1.816831683168317, "no_speech_prob": 9.570796919433633e-07}, {"id": 367, "seek": 178196, "start": 1799.2, "end": 1803.64, "text": " So in other words, to calculate the derivative of the loss with respect to the inputs, you", "tokens": [407, 294, 661, 2283, 11, 281, 8873, 264, 13760, 295, 264, 4470, 365, 3104, 281, 264, 15743, 11, 291], "temperature": 0.0, "avg_logprob": -0.15687620639801025, "compression_ratio": 1.816831683168317, "no_speech_prob": 9.570796919433633e-07}, {"id": 368, "seek": 178196, "start": 1803.64, "end": 1807.24, "text": " have to calculate the derivative through that whole function.", "tokens": [362, 281, 8873, 264, 13760, 807, 300, 1379, 2445, 13], "temperature": 0.0, "avg_logprob": -0.15687620639801025, "compression_ratio": 1.816831683168317, "no_speech_prob": 9.570796919433633e-07}, {"id": 369, "seek": 178196, "start": 1807.24, "end": 1808.8, "text": " And this is what's called backpropagation.", "tokens": [400, 341, 307, 437, 311, 1219, 646, 79, 1513, 559, 399, 13], "temperature": 0.0, "avg_logprob": -0.15687620639801025, "compression_ratio": 1.816831683168317, "no_speech_prob": 9.570796919433633e-07}, {"id": 370, "seek": 180880, "start": 1808.8, "end": 1814.48, "text": " With backpropagation, it's easy to calculate that derivative because we know that from", "tokens": [2022, 646, 79, 1513, 559, 399, 11, 309, 311, 1858, 281, 8873, 300, 13760, 570, 321, 458, 300, 490], "temperature": 0.0, "avg_logprob": -0.13179496515577085, "compression_ratio": 1.8489795918367347, "no_speech_prob": 1.4593704236176563e-06}, {"id": 371, "seek": 180880, "start": 1814.48, "end": 1820.32, "text": " the chain rule, the derivative of a function of a function is simply equal to the product", "tokens": [264, 5021, 4978, 11, 264, 13760, 295, 257, 2445, 295, 257, 2445, 307, 2935, 2681, 281, 264, 1674], "temperature": 0.0, "avg_logprob": -0.13179496515577085, "compression_ratio": 1.8489795918367347, "no_speech_prob": 1.4593704236176563e-06}, {"id": 372, "seek": 180880, "start": 1820.32, "end": 1822.52, "text": " of the derivatives of those functions.", "tokens": [295, 264, 33733, 295, 729, 6828, 13], "temperature": 0.0, "avg_logprob": -0.13179496515577085, "compression_ratio": 1.8489795918367347, "no_speech_prob": 1.4593704236176563e-06}, {"id": 373, "seek": 180880, "start": 1822.52, "end": 1827.1399999999999, "text": " So in practice, all we do is we calculate the derivative of every layer with respect", "tokens": [407, 294, 3124, 11, 439, 321, 360, 307, 321, 8873, 264, 13760, 295, 633, 4583, 365, 3104], "temperature": 0.0, "avg_logprob": -0.13179496515577085, "compression_ratio": 1.8489795918367347, "no_speech_prob": 1.4593704236176563e-06}, {"id": 374, "seek": 180880, "start": 1827.1399999999999, "end": 1830.74, "text": " to its inputs and then we just multiply them all together.", "tokens": [281, 1080, 15743, 293, 550, 321, 445, 12972, 552, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.13179496515577085, "compression_ratio": 1.8489795918367347, "no_speech_prob": 1.4593704236176563e-06}, {"id": 375, "seek": 180880, "start": 1830.74, "end": 1837.58, "text": " And so that's why we need to know the derivative of the activation layers as well as the loss", "tokens": [400, 370, 300, 311, 983, 321, 643, 281, 458, 264, 13760, 295, 264, 24433, 7914, 382, 731, 382, 264, 4470], "temperature": 0.0, "avg_logprob": -0.13179496515577085, "compression_ratio": 1.8489795918367347, "no_speech_prob": 1.4593704236176563e-06}, {"id": 376, "seek": 183758, "start": 1837.58, "end": 1843.76, "text": " layer and everything else.", "tokens": [4583, 293, 1203, 1646, 13], "temperature": 0.0, "avg_logprob": -0.14615280487958124, "compression_ratio": 1.6506849315068493, "no_speech_prob": 9.368652172270231e-06}, {"id": 377, "seek": 183758, "start": 1843.76, "end": 1845.86, "text": " So here's the trick.", "tokens": [407, 510, 311, 264, 4282, 13], "temperature": 0.0, "avg_logprob": -0.14615280487958124, "compression_ratio": 1.6506849315068493, "no_speech_prob": 9.368652172270231e-06}, {"id": 378, "seek": 183758, "start": 1845.86, "end": 1857.04, "text": " What we're going to do is we're going to say every time we take a step, we're going to", "tokens": [708, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 584, 633, 565, 321, 747, 257, 1823, 11, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.14615280487958124, "compression_ratio": 1.6506849315068493, "no_speech_prob": 9.368652172270231e-06}, {"id": 379, "seek": 183758, "start": 1857.04, "end": 1860.96, "text": " also calculate the average of the last few steps.", "tokens": [611, 8873, 264, 4274, 295, 264, 1036, 1326, 4439, 13], "temperature": 0.0, "avg_logprob": -0.14615280487958124, "compression_ratio": 1.6506849315068493, "no_speech_prob": 9.368652172270231e-06}, {"id": 380, "seek": 183758, "start": 1860.96, "end": 1864.76, "text": " So after these two steps, the average is this direction.", "tokens": [407, 934, 613, 732, 4439, 11, 264, 4274, 307, 341, 3513, 13], "temperature": 0.0, "avg_logprob": -0.14615280487958124, "compression_ratio": 1.6506849315068493, "no_speech_prob": 9.368652172270231e-06}, {"id": 381, "seek": 186476, "start": 1864.76, "end": 1869.0, "text": " So the next step, we're going to take our gradient step as usual and we're going to", "tokens": [407, 264, 958, 1823, 11, 321, 434, 516, 281, 747, 527, 16235, 1823, 382, 7713, 293, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.15863003647118284, "compression_ratio": 1.8728070175438596, "no_speech_prob": 7.183207344496623e-06}, {"id": 382, "seek": 186476, "start": 1869.0, "end": 1874.76, "text": " add on our average of the last few steps.", "tokens": [909, 322, 527, 4274, 295, 264, 1036, 1326, 4439, 13], "temperature": 0.0, "avg_logprob": -0.15863003647118284, "compression_ratio": 1.8728070175438596, "no_speech_prob": 7.183207344496623e-06}, {"id": 383, "seek": 186476, "start": 1874.76, "end": 1877.76, "text": " And that means that we end up actually going to here.", "tokens": [400, 300, 1355, 300, 321, 917, 493, 767, 516, 281, 510, 13], "temperature": 0.0, "avg_logprob": -0.15863003647118284, "compression_ratio": 1.8728070175438596, "no_speech_prob": 7.183207344496623e-06}, {"id": 384, "seek": 186476, "start": 1877.76, "end": 1879.3799999999999, "text": " And then we do the same thing again.", "tokens": [400, 550, 321, 360, 264, 912, 551, 797, 13], "temperature": 0.0, "avg_logprob": -0.15863003647118284, "compression_ratio": 1.8728070175438596, "no_speech_prob": 7.183207344496623e-06}, {"id": 385, "seek": 186476, "start": 1879.3799999999999, "end": 1884.4, "text": " So we find the average of the last few steps and it's now even further in this direction.", "tokens": [407, 321, 915, 264, 4274, 295, 264, 1036, 1326, 4439, 293, 309, 311, 586, 754, 3052, 294, 341, 3513, 13], "temperature": 0.0, "avg_logprob": -0.15863003647118284, "compression_ratio": 1.8728070175438596, "no_speech_prob": 7.183207344496623e-06}, {"id": 386, "seek": 186476, "start": 1884.4, "end": 1885.4, "text": " Question.", "tokens": [14464, 13], "temperature": 0.0, "avg_logprob": -0.15863003647118284, "compression_ratio": 1.8728070175438596, "no_speech_prob": 7.183207344496623e-06}, {"id": 387, "seek": 186476, "start": 1885.4, "end": 1887.44, "text": " What is that surface?", "tokens": [708, 307, 300, 3753, 30], "temperature": 0.0, "avg_logprob": -0.15863003647118284, "compression_ratio": 1.8728070175438596, "no_speech_prob": 7.183207344496623e-06}, {"id": 388, "seek": 186476, "start": 1887.44, "end": 1894.12, "text": " This is the surface of the loss function with respect to some of the parameters, in this", "tokens": [639, 307, 264, 3753, 295, 264, 4470, 2445, 365, 3104, 281, 512, 295, 264, 9834, 11, 294, 341], "temperature": 0.0, "avg_logprob": -0.15863003647118284, "compression_ratio": 1.8728070175438596, "no_speech_prob": 7.183207344496623e-06}, {"id": 389, "seek": 189412, "start": 1894.12, "end": 1895.12, "text": " case just a couple of parameters.", "tokens": [1389, 445, 257, 1916, 295, 9834, 13], "temperature": 0.0, "avg_logprob": -0.15606722267725134, "compression_ratio": 1.7960199004975124, "no_speech_prob": 8.801062904240098e-06}, {"id": 390, "seek": 189412, "start": 1895.12, "end": 1899.0, "text": " It's just an example of what a loss function might look like.", "tokens": [467, 311, 445, 364, 1365, 295, 437, 257, 4470, 2445, 1062, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.15606722267725134, "compression_ratio": 1.7960199004975124, "no_speech_prob": 8.801062904240098e-06}, {"id": 391, "seek": 189412, "start": 1899.0, "end": 1911.76, "text": " So this is the loss and this is some weight number 1 and this is some weight number 2.", "tokens": [407, 341, 307, 264, 4470, 293, 341, 307, 512, 3364, 1230, 502, 293, 341, 307, 512, 3364, 1230, 568, 13], "temperature": 0.0, "avg_logprob": -0.15606722267725134, "compression_ratio": 1.7960199004975124, "no_speech_prob": 8.801062904240098e-06}, {"id": 392, "seek": 189412, "start": 1911.76, "end": 1915.8799999999999, "text": " So we're trying to get our little, if you can imagine this is like gravity, we're trying", "tokens": [407, 321, 434, 1382, 281, 483, 527, 707, 11, 498, 291, 393, 3811, 341, 307, 411, 12110, 11, 321, 434, 1382], "temperature": 0.0, "avg_logprob": -0.15606722267725134, "compression_ratio": 1.7960199004975124, "no_speech_prob": 8.801062904240098e-06}, {"id": 393, "seek": 189412, "start": 1915.8799999999999, "end": 1921.08, "text": " to get this little ball to travel down this valley as far down to the bottom as possible.", "tokens": [281, 483, 341, 707, 2594, 281, 3147, 760, 341, 17636, 382, 1400, 760, 281, 264, 2767, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.15606722267725134, "compression_ratio": 1.7960199004975124, "no_speech_prob": 8.801062904240098e-06}, {"id": 394, "seek": 192108, "start": 1921.08, "end": 1928.0, "text": " And so the trick is that we're going to keep taking a step, not just the gradient step,", "tokens": [400, 370, 264, 4282, 307, 300, 321, 434, 516, 281, 1066, 1940, 257, 1823, 11, 406, 445, 264, 16235, 1823, 11], "temperature": 0.0, "avg_logprob": -0.16772483115972475, "compression_ratio": 1.6127167630057804, "no_speech_prob": 2.76939522336761e-06}, {"id": 395, "seek": 192108, "start": 1928.0, "end": 1931.72, "text": " but also the average of the last few steps.", "tokens": [457, 611, 264, 4274, 295, 264, 1036, 1326, 4439, 13], "temperature": 0.0, "avg_logprob": -0.16772483115972475, "compression_ratio": 1.6127167630057804, "no_speech_prob": 2.76939522336761e-06}, {"id": 396, "seek": 192108, "start": 1931.72, "end": 1940.32, "text": " And so in practice, this is going to end up going, donk, donk, donk, donk, donk.", "tokens": [400, 370, 294, 3124, 11, 341, 307, 516, 281, 917, 493, 516, 11, 500, 74, 11, 500, 74, 11, 500, 74, 11, 500, 74, 11, 500, 74, 13], "temperature": 0.0, "avg_logprob": -0.16772483115972475, "compression_ratio": 1.6127167630057804, "no_speech_prob": 2.76939522336761e-06}, {"id": 397, "seek": 192108, "start": 1940.32, "end": 1942.0, "text": " That's the idea.", "tokens": [663, 311, 264, 1558, 13], "temperature": 0.0, "avg_logprob": -0.16772483115972475, "compression_ratio": 1.6127167630057804, "no_speech_prob": 2.76939522336761e-06}, {"id": 398, "seek": 192108, "start": 1942.0, "end": 1946.6799999999998, "text": " So to do that in Excel is pretty straightforward.", "tokens": [407, 281, 360, 300, 294, 19060, 307, 1238, 15325, 13], "temperature": 0.0, "avg_logprob": -0.16772483115972475, "compression_ratio": 1.6127167630057804, "no_speech_prob": 2.76939522336761e-06}, {"id": 399, "seek": 194668, "start": 1946.68, "end": 1951.04, "text": " To make things simpler, I have removed the finite differencing based derivatives here,", "tokens": [1407, 652, 721, 18587, 11, 286, 362, 7261, 264, 19362, 743, 13644, 2361, 33733, 510, 11], "temperature": 0.0, "avg_logprob": -0.18592710928483444, "compression_ratio": 1.645021645021645, "no_speech_prob": 5.86276928515872e-06}, {"id": 400, "seek": 194668, "start": 1951.04, "end": 1952.8400000000001, "text": " so we just have the analytical derivatives.", "tokens": [370, 321, 445, 362, 264, 29579, 33733, 13], "temperature": 0.0, "avg_logprob": -0.18592710928483444, "compression_ratio": 1.645021645021645, "no_speech_prob": 5.86276928515872e-06}, {"id": 401, "seek": 194668, "start": 1952.8400000000001, "end": 1957.52, "text": " But other than that, this is identical to the previous spreadsheet.", "tokens": [583, 661, 813, 300, 11, 341, 307, 14800, 281, 264, 3894, 27733, 13], "temperature": 0.0, "avg_logprob": -0.18592710928483444, "compression_ratio": 1.645021645021645, "no_speech_prob": 5.86276928515872e-06}, {"id": 402, "seek": 194668, "start": 1957.52, "end": 1963.96, "text": " Same data, same predictions, same derivatives, except we've done one extra thing, which is", "tokens": [10635, 1412, 11, 912, 21264, 11, 912, 33733, 11, 3993, 321, 600, 1096, 472, 2857, 551, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.18592710928483444, "compression_ratio": 1.645021645021645, "no_speech_prob": 5.86276928515872e-06}, {"id": 403, "seek": 194668, "start": 1963.96, "end": 1972.76, "text": " that when we calculate our new B, we say it's our previous B minus our learning rate times", "tokens": [300, 562, 321, 8873, 527, 777, 363, 11, 321, 584, 309, 311, 527, 3894, 363, 3175, 527, 2539, 3314, 1413], "temperature": 0.0, "avg_logprob": -0.18592710928483444, "compression_ratio": 1.645021645021645, "no_speech_prob": 5.86276928515872e-06}, {"id": 404, "seek": 197276, "start": 1972.76, "end": 1977.44, "text": " our gradient times this cell.", "tokens": [527, 16235, 1413, 341, 2815, 13], "temperature": 0.0, "avg_logprob": -0.22340320473286643, "compression_ratio": 2.111111111111111, "no_speech_prob": 6.1441405705409124e-06}, {"id": 405, "seek": 197276, "start": 1977.44, "end": 1979.24, "text": " What is that cell?", "tokens": [708, 307, 300, 2815, 30], "temperature": 0.0, "avg_logprob": -0.22340320473286643, "compression_ratio": 2.111111111111111, "no_speech_prob": 6.1441405705409124e-06}, {"id": 406, "seek": 197276, "start": 1979.24, "end": 1991.96, "text": " That cell is equal to our gradient times.1 plus the thing just above it times.9.", "tokens": [663, 2815, 307, 2681, 281, 527, 16235, 1413, 2411, 16, 1804, 264, 551, 445, 3673, 309, 1413, 2411, 24, 13], "temperature": 0.0, "avg_logprob": -0.22340320473286643, "compression_ratio": 2.111111111111111, "no_speech_prob": 6.1441405705409124e-06}, {"id": 407, "seek": 197276, "start": 1991.96, "end": 1997.96, "text": " And the thing just above it is equal to its gradient times.1 plus the thing just above", "tokens": [400, 264, 551, 445, 3673, 309, 307, 2681, 281, 1080, 16235, 1413, 2411, 16, 1804, 264, 551, 445, 3673], "temperature": 0.0, "avg_logprob": -0.22340320473286643, "compression_ratio": 2.111111111111111, "no_speech_prob": 6.1441405705409124e-06}, {"id": 408, "seek": 197276, "start": 1997.96, "end": 1999.96, "text": " it times.9.", "tokens": [309, 1413, 2411, 24, 13], "temperature": 0.0, "avg_logprob": -0.22340320473286643, "compression_ratio": 2.111111111111111, "no_speech_prob": 6.1441405705409124e-06}, {"id": 409, "seek": 199996, "start": 1999.96, "end": 2007.8400000000001, "text": " So in other words, this column is keeping track of an average derivative of the last", "tokens": [407, 294, 661, 2283, 11, 341, 7738, 307, 5145, 2837, 295, 364, 4274, 13760, 295, 264, 1036], "temperature": 0.0, "avg_logprob": -0.1278628445743175, "compression_ratio": 1.6225490196078431, "no_speech_prob": 3.6119618016527966e-06}, {"id": 410, "seek": 199996, "start": 2007.8400000000001, "end": 2011.64, "text": " few steps that we've taken, which is exactly what we want.", "tokens": [1326, 4439, 300, 321, 600, 2726, 11, 597, 307, 2293, 437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.1278628445743175, "compression_ratio": 1.6225490196078431, "no_speech_prob": 3.6119618016527966e-06}, {"id": 411, "seek": 199996, "start": 2011.64, "end": 2015.28, "text": " And we do that for both of our two parameters.", "tokens": [400, 321, 360, 300, 337, 1293, 295, 527, 732, 9834, 13], "temperature": 0.0, "avg_logprob": -0.1278628445743175, "compression_ratio": 1.6225490196078431, "no_speech_prob": 3.6119618016527966e-06}, {"id": 412, "seek": 199996, "start": 2015.28, "end": 2020.58, "text": " So this.9 is our momentum parameter.", "tokens": [407, 341, 2411, 24, 307, 527, 11244, 13075, 13], "temperature": 0.0, "avg_logprob": -0.1278628445743175, "compression_ratio": 1.6225490196078431, "no_speech_prob": 3.6119618016527966e-06}, {"id": 413, "seek": 199996, "start": 2020.58, "end": 2026.08, "text": " So in Keras, when you use momentum, you can say momentum equals and you say how much momentum", "tokens": [407, 294, 591, 6985, 11, 562, 291, 764, 11244, 11, 291, 393, 584, 11244, 6915, 293, 291, 584, 577, 709, 11244], "temperature": 0.0, "avg_logprob": -0.1278628445743175, "compression_ratio": 1.6225490196078431, "no_speech_prob": 3.6119618016527966e-06}, {"id": 414, "seek": 199996, "start": 2026.08, "end": 2027.08, "text": " you want.", "tokens": [291, 528, 13], "temperature": 0.0, "avg_logprob": -0.1278628445743175, "compression_ratio": 1.6225490196078431, "no_speech_prob": 3.6119618016527966e-06}, {"id": 415, "seek": 202708, "start": 2027.08, "end": 2031.1599999999999, "text": " You just pick it.", "tokens": [509, 445, 1888, 309, 13], "temperature": 0.0, "avg_logprob": -0.19644735147664835, "compression_ratio": 1.6210526315789473, "no_speech_prob": 6.7479941208148375e-06}, {"id": 416, "seek": 202708, "start": 2031.1599999999999, "end": 2033.04, "text": " So you just pick that parameter.", "tokens": [407, 291, 445, 1888, 300, 13075, 13], "temperature": 0.0, "avg_logprob": -0.19644735147664835, "compression_ratio": 1.6210526315789473, "no_speech_prob": 6.7479941208148375e-06}, {"id": 417, "seek": 202708, "start": 2033.04, "end": 2035.56, "text": " Just like your learning rate, you pick it.", "tokens": [1449, 411, 428, 2539, 3314, 11, 291, 1888, 309, 13], "temperature": 0.0, "avg_logprob": -0.19644735147664835, "compression_ratio": 1.6210526315789473, "no_speech_prob": 6.7479941208148375e-06}, {"id": 418, "seek": 202708, "start": 2035.56, "end": 2037.52, "text": " Your momentum factor, you pick it.", "tokens": [2260, 11244, 5952, 11, 291, 1888, 309, 13], "temperature": 0.0, "avg_logprob": -0.19644735147664835, "compression_ratio": 1.6210526315789473, "no_speech_prob": 6.7479941208148375e-06}, {"id": 419, "seek": 202708, "start": 2037.52, "end": 2039.1599999999999, "text": " It's something you get to choose.", "tokens": [467, 311, 746, 291, 483, 281, 2826, 13], "temperature": 0.0, "avg_logprob": -0.19644735147664835, "compression_ratio": 1.6210526315789473, "no_speech_prob": 6.7479941208148375e-06}, {"id": 420, "seek": 202708, "start": 2039.1599999999999, "end": 2043.3999999999999, "text": " And you choose it by trying a few and find out what works best.", "tokens": [400, 291, 2826, 309, 538, 1382, 257, 1326, 293, 915, 484, 437, 1985, 1151, 13], "temperature": 0.0, "avg_logprob": -0.19644735147664835, "compression_ratio": 1.6210526315789473, "no_speech_prob": 6.7479941208148375e-06}, {"id": 421, "seek": 202708, "start": 2043.3999999999999, "end": 2049.64, "text": " So let's try running this.", "tokens": [407, 718, 311, 853, 2614, 341, 13], "temperature": 0.0, "avg_logprob": -0.19644735147664835, "compression_ratio": 1.6210526315789473, "no_speech_prob": 6.7479941208148375e-06}, {"id": 422, "seek": 202708, "start": 2049.64, "end": 2055.56, "text": " And you can see it is still not exactly zipping along.", "tokens": [400, 291, 393, 536, 309, 307, 920, 406, 2293, 710, 6297, 2051, 13], "temperature": 0.0, "avg_logprob": -0.19644735147664835, "compression_ratio": 1.6210526315789473, "no_speech_prob": 6.7479941208148375e-06}, {"id": 423, "seek": 205556, "start": 2055.56, "end": 2057.68, "text": " Why is it not exactly zipping along?", "tokens": [1545, 307, 309, 406, 2293, 710, 6297, 2051, 30], "temperature": 0.0, "avg_logprob": -0.12195286458852339, "compression_ratio": 1.5638766519823788, "no_speech_prob": 1.0451447451487184e-05}, {"id": 424, "seek": 205556, "start": 2057.68, "end": 2062.4, "text": " The reason when we look at it is that we know that the constant term needs to get all the", "tokens": [440, 1778, 562, 321, 574, 412, 309, 307, 300, 321, 458, 300, 264, 5754, 1433, 2203, 281, 483, 439, 264], "temperature": 0.0, "avg_logprob": -0.12195286458852339, "compression_ratio": 1.5638766519823788, "no_speech_prob": 1.0451447451487184e-05}, {"id": 425, "seek": 205556, "start": 2062.4, "end": 2065.0, "text": " way up to 30.", "tokens": [636, 493, 281, 2217, 13], "temperature": 0.0, "avg_logprob": -0.12195286458852339, "compression_ratio": 1.5638766519823788, "no_speech_prob": 1.0451447451487184e-05}, {"id": 426, "seek": 205556, "start": 2065.0, "end": 2068.84, "text": " And it's still way down at 1.5.", "tokens": [400, 309, 311, 920, 636, 760, 412, 502, 13, 20, 13], "temperature": 0.0, "avg_logprob": -0.12195286458852339, "compression_ratio": 1.5638766519823788, "no_speech_prob": 1.0451447451487184e-05}, {"id": 427, "seek": 205556, "start": 2068.84, "end": 2075.72, "text": " It's not moving fast enough, whereas the slope term moved very quickly to where we want it", "tokens": [467, 311, 406, 2684, 2370, 1547, 11, 9735, 264, 13525, 1433, 4259, 588, 2661, 281, 689, 321, 528, 309], "temperature": 0.0, "avg_logprob": -0.12195286458852339, "compression_ratio": 1.5638766519823788, "no_speech_prob": 1.0451447451487184e-05}, {"id": 428, "seek": 205556, "start": 2075.72, "end": 2076.72, "text": " to be.", "tokens": [281, 312, 13], "temperature": 0.0, "avg_logprob": -0.12195286458852339, "compression_ratio": 1.5638766519823788, "no_speech_prob": 1.0451447451487184e-05}, {"id": 429, "seek": 205556, "start": 2076.72, "end": 2082.64, "text": " So what we really want is we need different learning rates for different parameters.", "tokens": [407, 437, 321, 534, 528, 307, 321, 643, 819, 2539, 6846, 337, 819, 9834, 13], "temperature": 0.0, "avg_logprob": -0.12195286458852339, "compression_ratio": 1.5638766519823788, "no_speech_prob": 1.0451447451487184e-05}, {"id": 430, "seek": 208264, "start": 2082.64, "end": 2085.6, "text": " And doing this is called dynamic learning rates.", "tokens": [400, 884, 341, 307, 1219, 8546, 2539, 6846, 13], "temperature": 0.0, "avg_logprob": -0.1683616512700131, "compression_ratio": 1.7906976744186047, "no_speech_prob": 3.500829961922136e-06}, {"id": 431, "seek": 208264, "start": 2085.6, "end": 2091.3599999999997, "text": " And the first really effective dynamic learning rate approaches have just appeared in the", "tokens": [400, 264, 700, 534, 4942, 8546, 2539, 3314, 11587, 362, 445, 8516, 294, 264], "temperature": 0.0, "avg_logprob": -0.1683616512700131, "compression_ratio": 1.7906976744186047, "no_speech_prob": 3.500829961922136e-06}, {"id": 432, "seek": 208264, "start": 2091.3599999999997, "end": 2095.12, "text": " last 3 years or so.", "tokens": [1036, 805, 924, 420, 370, 13], "temperature": 0.0, "avg_logprob": -0.1683616512700131, "compression_ratio": 1.7906976744186047, "no_speech_prob": 3.500829961922136e-06}, {"id": 433, "seek": 208264, "start": 2095.12, "end": 2098.8399999999997, "text": " And one very popular one is called Adagrad.", "tokens": [400, 472, 588, 3743, 472, 307, 1219, 1999, 559, 6206, 13], "temperature": 0.0, "avg_logprob": -0.1683616512700131, "compression_ratio": 1.7906976744186047, "no_speech_prob": 3.500829961922136e-06}, {"id": 434, "seek": 208264, "start": 2098.8399999999997, "end": 2099.8399999999997, "text": " And it's very simple.", "tokens": [400, 309, 311, 588, 2199, 13], "temperature": 0.0, "avg_logprob": -0.1683616512700131, "compression_ratio": 1.7906976744186047, "no_speech_prob": 3.500829961922136e-06}, {"id": 435, "seek": 208264, "start": 2099.8399999999997, "end": 2105.3599999999997, "text": " All of these dynamic learning rate approaches have the same insight, which is this.", "tokens": [1057, 295, 613, 8546, 2539, 3314, 11587, 362, 264, 912, 11269, 11, 597, 307, 341, 13], "temperature": 0.0, "avg_logprob": -0.1683616512700131, "compression_ratio": 1.7906976744186047, "no_speech_prob": 3.500829961922136e-06}, {"id": 436, "seek": 210536, "start": 2105.36, "end": 2112.6800000000003, "text": " If the parameter that I'm changing, if the derivative of that parameter is consistently", "tokens": [759, 264, 13075, 300, 286, 478, 4473, 11, 498, 264, 13760, 295, 300, 13075, 307, 14961], "temperature": 0.0, "avg_logprob": -0.10410002918986531, "compression_ratio": 1.7700534759358288, "no_speech_prob": 6.6433794927434064e-06}, {"id": 437, "seek": 210536, "start": 2112.6800000000003, "end": 2120.48, "text": " of a very low magnitude, then if the derivative of this mini-batch is higher than that, then", "tokens": [295, 257, 588, 2295, 15668, 11, 550, 498, 264, 13760, 295, 341, 8382, 12, 65, 852, 307, 2946, 813, 300, 11, 550], "temperature": 0.0, "avg_logprob": -0.10410002918986531, "compression_ratio": 1.7700534759358288, "no_speech_prob": 6.6433794927434064e-06}, {"id": 438, "seek": 210536, "start": 2120.48, "end": 2126.56, "text": " what I really care about is the relative difference between how much this variable tends to change", "tokens": [437, 286, 534, 1127, 466, 307, 264, 4972, 2649, 1296, 577, 709, 341, 7006, 12258, 281, 1319], "temperature": 0.0, "avg_logprob": -0.10410002918986531, "compression_ratio": 1.7700534759358288, "no_speech_prob": 6.6433794927434064e-06}, {"id": 439, "seek": 210536, "start": 2126.56, "end": 2130.1600000000003, "text": " and how much it's going to change this time around.", "tokens": [293, 577, 709, 309, 311, 516, 281, 1319, 341, 565, 926, 13], "temperature": 0.0, "avg_logprob": -0.10410002918986531, "compression_ratio": 1.7700534759358288, "no_speech_prob": 6.6433794927434064e-06}, {"id": 440, "seek": 213016, "start": 2130.16, "end": 2136.04, "text": " So in other words, we don't just care about what's the gradient, but is the magnitude", "tokens": [407, 294, 661, 2283, 11, 321, 500, 380, 445, 1127, 466, 437, 311, 264, 16235, 11, 457, 307, 264, 15668], "temperature": 0.0, "avg_logprob": -0.08281017303466796, "compression_ratio": 1.7136363636363636, "no_speech_prob": 1.9333517684572143e-06}, {"id": 441, "seek": 213016, "start": 2136.04, "end": 2141.3599999999997, "text": " of the gradient a lot more or a lot less than it has tended to be recently?", "tokens": [295, 264, 16235, 257, 688, 544, 420, 257, 688, 1570, 813, 309, 575, 34732, 281, 312, 3938, 30], "temperature": 0.0, "avg_logprob": -0.08281017303466796, "compression_ratio": 1.7136363636363636, "no_speech_prob": 1.9333517684572143e-06}, {"id": 442, "seek": 213016, "start": 2141.3599999999997, "end": 2147.7999999999997, "text": " So the easy way to calculate the overall amount of change of the gradient recently is to keep", "tokens": [407, 264, 1858, 636, 281, 8873, 264, 4787, 2372, 295, 1319, 295, 264, 16235, 3938, 307, 281, 1066], "temperature": 0.0, "avg_logprob": -0.08281017303466796, "compression_ratio": 1.7136363636363636, "no_speech_prob": 1.9333517684572143e-06}, {"id": 443, "seek": 213016, "start": 2147.7999999999997, "end": 2151.44, "text": " track of the square of the gradient.", "tokens": [2837, 295, 264, 3732, 295, 264, 16235, 13], "temperature": 0.0, "avg_logprob": -0.08281017303466796, "compression_ratio": 1.7136363636363636, "no_speech_prob": 1.9333517684572143e-06}, {"id": 444, "seek": 213016, "start": 2151.44, "end": 2159.08, "text": " So what we do with Adagrad is you can see at the bottom of my epoch here, I have got", "tokens": [407, 437, 321, 360, 365, 1999, 559, 6206, 307, 291, 393, 536, 412, 264, 2767, 295, 452, 30992, 339, 510, 11, 286, 362, 658], "temperature": 0.0, "avg_logprob": -0.08281017303466796, "compression_ratio": 1.7136363636363636, "no_speech_prob": 1.9333517684572143e-06}, {"id": 445, "seek": 215908, "start": 2159.08, "end": 2163.04, "text": " a sum of squares of all of my gradients.", "tokens": [257, 2408, 295, 19368, 295, 439, 295, 452, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.1422516934506528, "compression_ratio": 1.8944954128440368, "no_speech_prob": 3.555962393875234e-06}, {"id": 446, "seek": 215908, "start": 2163.04, "end": 2166.92, "text": " And then I have taken the square root, so I've got the roots on the squares, and then", "tokens": [400, 550, 286, 362, 2726, 264, 3732, 5593, 11, 370, 286, 600, 658, 264, 10669, 322, 264, 19368, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.1422516934506528, "compression_ratio": 1.8944954128440368, "no_speech_prob": 3.555962393875234e-06}, {"id": 447, "seek": 215908, "start": 2166.92, "end": 2169.3199999999997, "text": " I've just divided it by the count to get the average.", "tokens": [286, 600, 445, 6666, 309, 538, 264, 1207, 281, 483, 264, 4274, 13], "temperature": 0.0, "avg_logprob": -0.1422516934506528, "compression_ratio": 1.8944954128440368, "no_speech_prob": 3.555962393875234e-06}, {"id": 448, "seek": 215908, "start": 2169.3199999999997, "end": 2173.16, "text": " So this is the average of the roots of the squares of my gradients.", "tokens": [407, 341, 307, 264, 4274, 295, 264, 10669, 295, 264, 19368, 295, 452, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.1422516934506528, "compression_ratio": 1.8944954128440368, "no_speech_prob": 3.555962393875234e-06}, {"id": 449, "seek": 215908, "start": 2173.16, "end": 2178.3199999999997, "text": " So this number here will be high if the magnitudes of my gradients is high.", "tokens": [407, 341, 1230, 510, 486, 312, 1090, 498, 264, 4944, 16451, 295, 452, 2771, 2448, 307, 1090, 13], "temperature": 0.0, "avg_logprob": -0.1422516934506528, "compression_ratio": 1.8944954128440368, "no_speech_prob": 3.555962393875234e-06}, {"id": 450, "seek": 215908, "start": 2178.3199999999997, "end": 2184.68, "text": " And because it's squared, it will be particularly high if sometimes they're really high.", "tokens": [400, 570, 309, 311, 8889, 11, 309, 486, 312, 4098, 1090, 498, 2171, 436, 434, 534, 1090, 13], "temperature": 0.0, "avg_logprob": -0.1422516934506528, "compression_ratio": 1.8944954128440368, "no_speech_prob": 3.555962393875234e-06}, {"id": 451, "seek": 218468, "start": 2184.68, "end": 2190.2, "text": " So why is it okay to just use a mini-batch since the surface is going to depend on what", "tokens": [407, 983, 307, 309, 1392, 281, 445, 764, 257, 8382, 12, 65, 852, 1670, 264, 3753, 307, 516, 281, 5672, 322, 437], "temperature": 0.0, "avg_logprob": -0.18216325308530384, "compression_ratio": 1.7328244274809161, "no_speech_prob": 2.111197136400733e-05}, {"id": 452, "seek": 218468, "start": 2190.2, "end": 2192.8799999999997, "text": " points are in your mini-batch?", "tokens": [2793, 366, 294, 428, 8382, 12, 65, 852, 30], "temperature": 0.0, "avg_logprob": -0.18216325308530384, "compression_ratio": 1.7328244274809161, "no_speech_prob": 2.111197136400733e-05}, {"id": 453, "seek": 218468, "start": 2192.8799999999997, "end": 2197.16, "text": " It's not ideal to just use a mini-batch, and we will learn about a better approach to this", "tokens": [467, 311, 406, 7157, 281, 445, 764, 257, 8382, 12, 65, 852, 11, 293, 321, 486, 1466, 466, 257, 1101, 3109, 281, 341], "temperature": 0.0, "avg_logprob": -0.18216325308530384, "compression_ratio": 1.7328244274809161, "no_speech_prob": 2.111197136400733e-05}, {"id": 454, "seek": 218468, "start": 2197.16, "end": 2198.16, "text": " in a moment.", "tokens": [294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.18216325308530384, "compression_ratio": 1.7328244274809161, "no_speech_prob": 2.111197136400733e-05}, {"id": 455, "seek": 218468, "start": 2198.16, "end": 2199.3999999999996, "text": " But for now, let's look at this.", "tokens": [583, 337, 586, 11, 718, 311, 574, 412, 341, 13], "temperature": 0.0, "avg_logprob": -0.18216325308530384, "compression_ratio": 1.7328244274809161, "no_speech_prob": 2.111197136400733e-05}, {"id": 456, "seek": 218468, "start": 2199.3999999999996, "end": 2205.12, "text": " In fact, there are two approaches very related, Adagrad and Adadelta.", "tokens": [682, 1186, 11, 456, 366, 732, 11587, 588, 4077, 11, 1999, 559, 6206, 293, 1999, 16325, 1328, 13], "temperature": 0.0, "avg_logprob": -0.18216325308530384, "compression_ratio": 1.7328244274809161, "no_speech_prob": 2.111197136400733e-05}, {"id": 457, "seek": 218468, "start": 2205.12, "end": 2211.7599999999998, "text": " And one of them actually does this for all of the gradients so far, and one of them uses", "tokens": [400, 472, 295, 552, 767, 775, 341, 337, 439, 295, 264, 2771, 2448, 370, 1400, 11, 293, 472, 295, 552, 4960], "temperature": 0.0, "avg_logprob": -0.18216325308530384, "compression_ratio": 1.7328244274809161, "no_speech_prob": 2.111197136400733e-05}, {"id": 458, "seek": 218468, "start": 2211.7599999999998, "end": 2214.64, "text": " a slightly more sophisticated approach.", "tokens": [257, 4748, 544, 16950, 3109, 13], "temperature": 0.0, "avg_logprob": -0.18216325308530384, "compression_ratio": 1.7328244274809161, "no_speech_prob": 2.111197136400733e-05}, {"id": 459, "seek": 221464, "start": 2214.64, "end": 2219.64, "text": " This approach of doing it on a mini-batch-by-mini-batch basis is slightly different either, but it's", "tokens": [639, 3109, 295, 884, 309, 322, 257, 8382, 12, 65, 852, 12, 2322, 12, 2367, 72, 12, 65, 852, 5143, 307, 4748, 819, 2139, 11, 457, 309, 311], "temperature": 0.0, "avg_logprob": -0.23310724238759464, "compression_ratio": 1.6188340807174888, "no_speech_prob": 7.966968405526131e-05}, {"id": 460, "seek": 221464, "start": 2219.64, "end": 2226.3599999999997, "text": " similar enough to explain the concept.", "tokens": [2531, 1547, 281, 2903, 264, 3410, 13], "temperature": 0.0, "avg_logprob": -0.23310724238759464, "compression_ratio": 1.6188340807174888, "no_speech_prob": 7.966968405526131e-05}, {"id": 461, "seek": 221464, "start": 2226.3599999999997, "end": 2227.3599999999997, "text": " So what I do is I...", "tokens": [407, 437, 286, 360, 307, 286, 1097], "temperature": 0.0, "avg_logprob": -0.23310724238759464, "compression_ratio": 1.6188340807174888, "no_speech_prob": 7.966968405526131e-05}, {"id": 462, "seek": 221464, "start": 2227.3599999999997, "end": 2228.3599999999997, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.23310724238759464, "compression_ratio": 1.6188340807174888, "no_speech_prob": 7.966968405526131e-05}, {"id": 463, "seek": 221464, "start": 2228.3599999999997, "end": 2234.12, "text": " Does this mean for a CNN, would dynamic learning rates mean that each filter would have its", "tokens": [4402, 341, 914, 337, 257, 24859, 11, 576, 8546, 2539, 6846, 914, 300, 1184, 6608, 576, 362, 1080], "temperature": 0.0, "avg_logprob": -0.23310724238759464, "compression_ratio": 1.6188340807174888, "no_speech_prob": 7.966968405526131e-05}, {"id": 464, "seek": 221464, "start": 2234.12, "end": 2236.58, "text": " own learning rate?", "tokens": [1065, 2539, 3314, 30], "temperature": 0.0, "avg_logprob": -0.23310724238759464, "compression_ratio": 1.6188340807174888, "no_speech_prob": 7.966968405526131e-05}, {"id": 465, "seek": 221464, "start": 2236.58, "end": 2240.3599999999997, "text": " It would mean that every parameter has its own learning rate.", "tokens": [467, 576, 914, 300, 633, 13075, 575, 1080, 1065, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.23310724238759464, "compression_ratio": 1.6188340807174888, "no_speech_prob": 7.966968405526131e-05}, {"id": 466, "seek": 224036, "start": 2240.36, "end": 2244.96, "text": " So this is one parameter, that's a parameter, that's a parameter, that's a parameter, and", "tokens": [407, 341, 307, 472, 13075, 11, 300, 311, 257, 13075, 11, 300, 311, 257, 13075, 11, 300, 311, 257, 13075, 11, 293], "temperature": 0.0, "avg_logprob": -0.117416258781187, "compression_ratio": 2.316326530612245, "no_speech_prob": 2.468168895575218e-05}, {"id": 467, "seek": 224036, "start": 2244.96, "end": 2253.92, "text": " then in our dense layer, that's a parameter, that's a parameter, that's a parameter.", "tokens": [550, 294, 527, 18011, 4583, 11, 300, 311, 257, 13075, 11, 300, 311, 257, 13075, 11, 300, 311, 257, 13075, 13], "temperature": 0.0, "avg_logprob": -0.117416258781187, "compression_ratio": 2.316326530612245, "no_speech_prob": 2.468168895575218e-05}, {"id": 468, "seek": 224036, "start": 2253.92, "end": 2259.96, "text": " So when you go model.summary in Keras, it shows you for every layer how many parameters", "tokens": [407, 562, 291, 352, 2316, 13, 82, 40879, 822, 294, 591, 6985, 11, 309, 3110, 291, 337, 633, 4583, 577, 867, 9834], "temperature": 0.0, "avg_logprob": -0.117416258781187, "compression_ratio": 2.316326530612245, "no_speech_prob": 2.468168895575218e-05}, {"id": 469, "seek": 224036, "start": 2259.96, "end": 2260.96, "text": " there are.", "tokens": [456, 366, 13], "temperature": 0.0, "avg_logprob": -0.117416258781187, "compression_ratio": 2.316326530612245, "no_speech_prob": 2.468168895575218e-05}, {"id": 470, "seek": 224036, "start": 2260.96, "end": 2264.9, "text": " So anytime you're unclear on how many parameters there are, you can go back and have a look", "tokens": [407, 13038, 291, 434, 25636, 322, 577, 867, 9834, 456, 366, 11, 291, 393, 352, 646, 293, 362, 257, 574], "temperature": 0.0, "avg_logprob": -0.117416258781187, "compression_ratio": 2.316326530612245, "no_speech_prob": 2.468168895575218e-05}, {"id": 471, "seek": 224036, "start": 2264.9, "end": 2270.32, "text": " at these spreadsheets and you can also look at the Keras model.summary and make sure you", "tokens": [412, 613, 23651, 1385, 293, 291, 393, 611, 574, 412, 264, 591, 6985, 2316, 13, 82, 40879, 822, 293, 652, 988, 291], "temperature": 0.0, "avg_logprob": -0.117416258781187, "compression_ratio": 2.316326530612245, "no_speech_prob": 2.468168895575218e-05}, {"id": 472, "seek": 227032, "start": 2270.32, "end": 2273.28, "text": " understand how they turn out.", "tokens": [1223, 577, 436, 1261, 484, 13], "temperature": 0.0, "avg_logprob": -0.10817140635877552, "compression_ratio": 2.066326530612245, "no_speech_prob": 1.0615948667691555e-05}, {"id": 473, "seek": 227032, "start": 2273.28, "end": 2279.1200000000003, "text": " So for the first layer, it's going to be the size of your filter times the number of your", "tokens": [407, 337, 264, 700, 4583, 11, 309, 311, 516, 281, 312, 264, 2744, 295, 428, 6608, 1413, 264, 1230, 295, 428], "temperature": 0.0, "avg_logprob": -0.10817140635877552, "compression_ratio": 2.066326530612245, "no_speech_prob": 1.0615948667691555e-05}, {"id": 474, "seek": 227032, "start": 2279.1200000000003, "end": 2284.48, "text": " filters, if it's just grayscale, and then after that the number of parameters will be", "tokens": [15995, 11, 498, 309, 311, 445, 677, 3772, 37088, 11, 293, 550, 934, 300, 264, 1230, 295, 9834, 486, 312], "temperature": 0.0, "avg_logprob": -0.10817140635877552, "compression_ratio": 2.066326530612245, "no_speech_prob": 1.0615948667691555e-05}, {"id": 475, "seek": 227032, "start": 2284.48, "end": 2292.0800000000004, "text": " equal to the size of your filter times the number of filters coming in times the number", "tokens": [2681, 281, 264, 2744, 295, 428, 6608, 1413, 264, 1230, 295, 15995, 1348, 294, 1413, 264, 1230], "temperature": 0.0, "avg_logprob": -0.10817140635877552, "compression_ratio": 2.066326530612245, "no_speech_prob": 1.0615948667691555e-05}, {"id": 476, "seek": 227032, "start": 2292.0800000000004, "end": 2294.6800000000003, "text": " of filters coming out.", "tokens": [295, 15995, 1348, 484, 13], "temperature": 0.0, "avg_logprob": -0.10817140635877552, "compression_ratio": 2.066326530612245, "no_speech_prob": 1.0615948667691555e-05}, {"id": 477, "seek": 227032, "start": 2294.6800000000003, "end": 2299.0, "text": " And then of course your dense layers will be every input goes to every output, so number", "tokens": [400, 550, 295, 1164, 428, 18011, 7914, 486, 312, 633, 4846, 1709, 281, 633, 5598, 11, 370, 1230], "temperature": 0.0, "avg_logprob": -0.10817140635877552, "compression_ratio": 2.066326530612245, "no_speech_prob": 1.0615948667691555e-05}, {"id": 478, "seek": 229900, "start": 2299.0, "end": 2301.28, "text": " of inputs times number of outputs.", "tokens": [295, 15743, 1413, 1230, 295, 23930, 13], "temperature": 0.0, "avg_logprob": -0.16071381440033783, "compression_ratio": 1.6067415730337078, "no_speech_prob": 3.2377502066083252e-06}, {"id": 479, "seek": 229900, "start": 2301.28, "end": 2311.52, "text": " A parameter to the function that is calculating whether it's a cat or a dog.", "tokens": [316, 13075, 281, 264, 2445, 300, 307, 28258, 1968, 309, 311, 257, 3857, 420, 257, 3000, 13], "temperature": 0.0, "avg_logprob": -0.16071381440033783, "compression_ratio": 1.6067415730337078, "no_speech_prob": 3.2377502066083252e-06}, {"id": 480, "seek": 229900, "start": 2311.52, "end": 2320.48, "text": " So what we do now is we say, this number here, 1857, this is saying that the derivative of", "tokens": [407, 437, 321, 360, 586, 307, 321, 584, 11, 341, 1230, 510, 11, 2443, 19004, 11, 341, 307, 1566, 300, 264, 13760, 295], "temperature": 0.0, "avg_logprob": -0.16071381440033783, "compression_ratio": 1.6067415730337078, "no_speech_prob": 3.2377502066083252e-06}, {"id": 481, "seek": 229900, "start": 2320.48, "end": 2327.04, "text": " the loss with respect to the slope varies a lot, whereas the derivative of the loss", "tokens": [264, 4470, 365, 3104, 281, 264, 13525, 21716, 257, 688, 11, 9735, 264, 13760, 295, 264, 4470], "temperature": 0.0, "avg_logprob": -0.16071381440033783, "compression_ratio": 1.6067415730337078, "no_speech_prob": 3.2377502066083252e-06}, {"id": 482, "seek": 232704, "start": 2327.04, "end": 2330.68, "text": " with respect to the intercept doesn't vary much at all.", "tokens": [365, 3104, 281, 264, 24700, 1177, 380, 10559, 709, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.11323590114198882, "compression_ratio": 1.6, "no_speech_prob": 1.5056972415550263e-06}, {"id": 483, "seek": 232704, "start": 2330.68, "end": 2339.44, "text": " So at the end of every epoch, I copy that up to here, and then I take my learning rate", "tokens": [407, 412, 264, 917, 295, 633, 30992, 339, 11, 286, 5055, 300, 493, 281, 510, 11, 293, 550, 286, 747, 452, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.11323590114198882, "compression_ratio": 1.6, "no_speech_prob": 1.5056972415550263e-06}, {"id": 484, "seek": 232704, "start": 2339.44, "end": 2341.84, "text": " and I divide it by that.", "tokens": [293, 286, 9845, 309, 538, 300, 13], "temperature": 0.0, "avg_logprob": -0.11323590114198882, "compression_ratio": 1.6, "no_speech_prob": 1.5056972415550263e-06}, {"id": 485, "seek": 232704, "start": 2341.84, "end": 2348.32, "text": " And so now for each of my parameters, I now have this adjusted learning rate, which is", "tokens": [400, 370, 586, 337, 1184, 295, 452, 9834, 11, 286, 586, 362, 341, 19871, 2539, 3314, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.11323590114198882, "compression_ratio": 1.6, "no_speech_prob": 1.5056972415550263e-06}, {"id": 486, "seek": 232704, "start": 2348.32, "end": 2355.12, "text": " the learning rate divided by the recent sum of squareds average gradient.", "tokens": [264, 2539, 3314, 6666, 538, 264, 5162, 2408, 295, 8889, 82, 4274, 16235, 13], "temperature": 0.0, "avg_logprob": -0.11323590114198882, "compression_ratio": 1.6, "no_speech_prob": 1.5056972415550263e-06}, {"id": 487, "seek": 235512, "start": 2355.12, "end": 2360.88, "text": " And so you can see that now one of my learning rates is 100 times faster than the other one.", "tokens": [400, 370, 291, 393, 536, 300, 586, 472, 295, 452, 2539, 6846, 307, 2319, 1413, 4663, 813, 264, 661, 472, 13], "temperature": 0.0, "avg_logprob": -0.29291144539328184, "compression_ratio": 1.456043956043956, "no_speech_prob": 6.747999577783048e-06}, {"id": 488, "seek": 235512, "start": 2360.88, "end": 2363.7999999999997, "text": " And so let's see what happens when I run this.", "tokens": [400, 370, 718, 311, 536, 437, 2314, 562, 286, 1190, 341, 13], "temperature": 0.0, "avg_logprob": -0.29291144539328184, "compression_ratio": 1.456043956043956, "no_speech_prob": 6.747999577783048e-06}, {"id": 489, "seek": 235512, "start": 2363.7999999999997, "end": 2370.68, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.29291144539328184, "compression_ratio": 1.456043956043956, "no_speech_prob": 6.747999577783048e-06}, {"id": 490, "seek": 235512, "start": 2370.68, "end": 2380.66, "text": " There's not really a relationship with normalizing the input data, because it can help, but still", "tokens": [821, 311, 406, 534, 257, 2480, 365, 2710, 3319, 264, 4846, 1412, 11, 570, 309, 393, 854, 11, 457, 920], "temperature": 0.0, "avg_logprob": -0.29291144539328184, "compression_ratio": 1.456043956043956, "no_speech_prob": 6.747999577783048e-06}, {"id": 491, "seek": 238066, "start": 2380.66, "end": 2390.8599999999997, "text": " if your inputs are of very different scales, it's still a lot more work for it to do.", "tokens": [498, 428, 15743, 366, 295, 588, 819, 17408, 11, 309, 311, 920, 257, 688, 544, 589, 337, 309, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1468513073065342, "compression_ratio": 1.5396039603960396, "no_speech_prob": 2.5215529149136273e-06}, {"id": 492, "seek": 238066, "start": 2390.8599999999997, "end": 2394.48, "text": " So yes it helps, but it doesn't help so much that it makes it useless.", "tokens": [407, 2086, 309, 3665, 11, 457, 309, 1177, 380, 854, 370, 709, 300, 309, 1669, 309, 14115, 13], "temperature": 0.0, "avg_logprob": -0.1468513073065342, "compression_ratio": 1.5396039603960396, "no_speech_prob": 2.5215529149136273e-06}, {"id": 493, "seek": 238066, "start": 2394.48, "end": 2401.0, "text": " And in fact, it turns out that even with dynamic learning rates, having not just normalized", "tokens": [400, 294, 1186, 11, 309, 4523, 484, 300, 754, 365, 8546, 2539, 6846, 11, 1419, 406, 445, 48704], "temperature": 0.0, "avg_logprob": -0.1468513073065342, "compression_ratio": 1.5396039603960396, "no_speech_prob": 2.5215529149136273e-06}, {"id": 494, "seek": 238066, "start": 2401.0, "end": 2407.3599999999997, "text": " inputs, but batch normalized activations is extremely helpful.", "tokens": [15743, 11, 457, 15245, 48704, 2430, 763, 307, 4664, 4961, 13], "temperature": 0.0, "avg_logprob": -0.1468513073065342, "compression_ratio": 1.5396039603960396, "no_speech_prob": 2.5215529149136273e-06}, {"id": 495, "seek": 240736, "start": 2407.36, "end": 2412.1600000000003, "text": " And so the thing about when you're using AdaGrad or any kind of dynamic learning rates is generally", "tokens": [400, 370, 264, 551, 466, 562, 291, 434, 1228, 32276, 38, 6206, 420, 604, 733, 295, 8546, 2539, 6846, 307, 5101], "temperature": 0.0, "avg_logprob": -0.2486511042088638, "compression_ratio": 1.481283422459893, "no_speech_prob": 7.967093551997095e-05}, {"id": 496, "seek": 240736, "start": 2412.1600000000003, "end": 2415.78, "text": " you'll set the learning rate quite a lot higher, because remember you're dividing it by this", "tokens": [291, 603, 992, 264, 2539, 3314, 1596, 257, 688, 2946, 11, 570, 1604, 291, 434, 26764, 309, 538, 341], "temperature": 0.0, "avg_logprob": -0.2486511042088638, "compression_ratio": 1.481283422459893, "no_speech_prob": 7.967093551997095e-05}, {"id": 497, "seek": 240736, "start": 2415.78, "end": 2416.78, "text": " recent average.", "tokens": [5162, 4274, 13], "temperature": 0.0, "avg_logprob": -0.2486511042088638, "compression_ratio": 1.481283422459893, "no_speech_prob": 7.967093551997095e-05}, {"id": 498, "seek": 240736, "start": 2416.78, "end": 2424.44, "text": " So if I set it to.1, oh too far, so that's no good.", "tokens": [407, 498, 286, 992, 309, 281, 2411, 16, 11, 1954, 886, 1400, 11, 370, 300, 311, 572, 665, 13], "temperature": 0.0, "avg_logprob": -0.2486511042088638, "compression_ratio": 1.481283422459893, "no_speech_prob": 7.967093551997095e-05}, {"id": 499, "seek": 240736, "start": 2424.44, "end": 2433.44, "text": " So let's try.05.", "tokens": [407, 718, 311, 853, 2411, 13328, 13], "temperature": 0.0, "avg_logprob": -0.2486511042088638, "compression_ratio": 1.481283422459893, "no_speech_prob": 7.967093551997095e-05}, {"id": 500, "seek": 243344, "start": 2433.44, "end": 2437.6, "text": " You can see after just 5 steps, I'm already halfway there.", "tokens": [509, 393, 536, 934, 445, 1025, 4439, 11, 286, 478, 1217, 15461, 456, 13], "temperature": 0.0, "avg_logprob": -0.17650447393718519, "compression_ratio": 1.5919540229885059, "no_speech_prob": 4.860406534135109e-06}, {"id": 501, "seek": 243344, "start": 2437.6, "end": 2447.2400000000002, "text": " Another 5 steps, getting very close, another 5 steps, and it's exploded.", "tokens": [3996, 1025, 4439, 11, 1242, 588, 1998, 11, 1071, 1025, 4439, 11, 293, 309, 311, 27049, 13], "temperature": 0.0, "avg_logprob": -0.17650447393718519, "compression_ratio": 1.5919540229885059, "no_speech_prob": 4.860406534135109e-06}, {"id": 502, "seek": 243344, "start": 2447.2400000000002, "end": 2449.7200000000003, "text": " Now why did that happen?", "tokens": [823, 983, 630, 300, 1051, 30], "temperature": 0.0, "avg_logprob": -0.17650447393718519, "compression_ratio": 1.5919540229885059, "no_speech_prob": 4.860406534135109e-06}, {"id": 503, "seek": 243344, "start": 2449.7200000000003, "end": 2457.92, "text": " Because as we get closer and closer to where we want to be, you can see that you need to", "tokens": [1436, 382, 321, 483, 4966, 293, 4966, 281, 689, 321, 528, 281, 312, 11, 291, 393, 536, 300, 291, 643, 281], "temperature": 0.0, "avg_logprob": -0.17650447393718519, "compression_ratio": 1.5919540229885059, "no_speech_prob": 4.860406534135109e-06}, {"id": 504, "seek": 243344, "start": 2457.92, "end": 2460.8, "text": " take smaller and smaller steps.", "tokens": [747, 4356, 293, 4356, 4439, 13], "temperature": 0.0, "avg_logprob": -0.17650447393718519, "compression_ratio": 1.5919540229885059, "no_speech_prob": 4.860406534135109e-06}, {"id": 505, "seek": 246080, "start": 2460.8, "end": 2467.96, "text": " And by keeping the learning rates the same, it meant that eventually we went too far.", "tokens": [400, 538, 5145, 264, 2539, 6846, 264, 912, 11, 309, 4140, 300, 4728, 321, 1437, 886, 1400, 13], "temperature": 0.0, "avg_logprob": -0.13082946031943135, "compression_ratio": 1.5118483412322274, "no_speech_prob": 4.425452061695978e-06}, {"id": 506, "seek": 246080, "start": 2467.96, "end": 2475.32, "text": " So this is still something you have to be very careful of.", "tokens": [407, 341, 307, 920, 746, 291, 362, 281, 312, 588, 5026, 295, 13], "temperature": 0.0, "avg_logprob": -0.13082946031943135, "compression_ratio": 1.5118483412322274, "no_speech_prob": 4.425452061695978e-06}, {"id": 507, "seek": 246080, "start": 2475.32, "end": 2480.44, "text": " A more elegant, in my opinion, approach to the same thing that AdaGrad is doing is something", "tokens": [316, 544, 21117, 11, 294, 452, 4800, 11, 3109, 281, 264, 912, 551, 300, 32276, 38, 6206, 307, 884, 307, 746], "temperature": 0.0, "avg_logprob": -0.13082946031943135, "compression_ratio": 1.5118483412322274, "no_speech_prob": 4.425452061695978e-06}, {"id": 508, "seek": 246080, "start": 2480.44, "end": 2481.44, "text": " called RMSProp.", "tokens": [1219, 497, 10288, 47, 1513, 13], "temperature": 0.0, "avg_logprob": -0.13082946031943135, "compression_ratio": 1.5118483412322274, "no_speech_prob": 4.425452061695978e-06}, {"id": 509, "seek": 246080, "start": 2481.44, "end": 2486.88, "text": " RMSProp was first introduced in Jeffrey Hinton's Coursera course.", "tokens": [497, 10288, 47, 1513, 390, 700, 7268, 294, 28721, 389, 12442, 311, 383, 5067, 1663, 1164, 13], "temperature": 0.0, "avg_logprob": -0.13082946031943135, "compression_ratio": 1.5118483412322274, "no_speech_prob": 4.425452061695978e-06}, {"id": 510, "seek": 248688, "start": 2486.88, "end": 2497.08, "text": " So if you go to the Coursera course, called Neural Networks, you'll find it.", "tokens": [407, 498, 291, 352, 281, 264, 383, 5067, 1663, 1164, 11, 1219, 1734, 1807, 12640, 82, 11, 291, 603, 915, 309, 13], "temperature": 0.0, "avg_logprob": -0.24487513762253982, "compression_ratio": 1.4979757085020242, "no_speech_prob": 3.219027348677628e-05}, {"id": 511, "seek": 248688, "start": 2497.08, "end": 2499.7200000000003, "text": " And in one of those classes he introduces RMSProp.", "tokens": [400, 294, 472, 295, 729, 5359, 415, 31472, 497, 10288, 47, 1513, 13], "temperature": 0.0, "avg_logprob": -0.24487513762253982, "compression_ratio": 1.4979757085020242, "no_speech_prob": 3.219027348677628e-05}, {"id": 512, "seek": 248688, "start": 2499.7200000000003, "end": 2503.88, "text": " So it's quite funny nowadays, because this comes up in academic papers a lot, when people", "tokens": [407, 309, 311, 1596, 4074, 13434, 11, 570, 341, 1487, 493, 294, 7778, 10577, 257, 688, 11, 562, 561], "temperature": 0.0, "avg_logprob": -0.24487513762253982, "compression_ratio": 1.4979757085020242, "no_speech_prob": 3.219027348677628e-05}, {"id": 513, "seek": 248688, "start": 2503.88, "end": 2511.0, "text": " cite it, they have to cite Coursera course, Chapter 6, at minute 14 and 30 seconds.", "tokens": [37771, 309, 11, 436, 362, 281, 37771, 383, 5067, 1663, 1164, 11, 18874, 1386, 11, 412, 3456, 3499, 293, 2217, 3949, 13], "temperature": 0.0, "avg_logprob": -0.24487513762253982, "compression_ratio": 1.4979757085020242, "no_speech_prob": 3.219027348677628e-05}, {"id": 514, "seek": 248688, "start": 2511.0, "end": 2514.32, "text": " But Hinton has asked that this be the official way that it is cited.", "tokens": [583, 389, 12442, 575, 2351, 300, 341, 312, 264, 4783, 636, 300, 309, 307, 30134, 13], "temperature": 0.0, "avg_logprob": -0.24487513762253982, "compression_ratio": 1.4979757085020242, "no_speech_prob": 3.219027348677628e-05}, {"id": 515, "seek": 251432, "start": 2514.32, "end": 2519.32, "text": " So there you go.", "tokens": [407, 456, 291, 352, 13], "temperature": 0.0, "avg_logprob": -0.1365033507347107, "compression_ratio": 1.8026315789473684, "no_speech_prob": 5.143754606251605e-05}, {"id": 516, "seek": 251432, "start": 2519.32, "end": 2520.96, "text": " So here's what RMSProp does.", "tokens": [407, 510, 311, 437, 497, 10288, 47, 1513, 775, 13], "temperature": 0.0, "avg_logprob": -0.1365033507347107, "compression_ratio": 1.8026315789473684, "no_speech_prob": 5.143754606251605e-05}, {"id": 517, "seek": 251432, "start": 2520.96, "end": 2526.96, "text": " What RMSProp does is exactly the same thing as Momentum, but instead of keeping track", "tokens": [708, 497, 10288, 47, 1513, 775, 307, 2293, 264, 912, 551, 382, 19093, 449, 11, 457, 2602, 295, 5145, 2837], "temperature": 0.0, "avg_logprob": -0.1365033507347107, "compression_ratio": 1.8026315789473684, "no_speech_prob": 5.143754606251605e-05}, {"id": 518, "seek": 251432, "start": 2526.96, "end": 2533.0, "text": " of the weighted running average of the gradients, we keep track of the weighted running average", "tokens": [295, 264, 32807, 2614, 4274, 295, 264, 2771, 2448, 11, 321, 1066, 2837, 295, 264, 32807, 2614, 4274], "temperature": 0.0, "avg_logprob": -0.1365033507347107, "compression_ratio": 1.8026315789473684, "no_speech_prob": 5.143754606251605e-05}, {"id": 519, "seek": 251432, "start": 2533.0, "end": 2534.98, "text": " of the square of the gradients.", "tokens": [295, 264, 3732, 295, 264, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.1365033507347107, "compression_ratio": 1.8026315789473684, "no_speech_prob": 5.143754606251605e-05}, {"id": 520, "seek": 251432, "start": 2534.98, "end": 2537.96, "text": " So here it is.", "tokens": [407, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1365033507347107, "compression_ratio": 1.8026315789473684, "no_speech_prob": 5.143754606251605e-05}, {"id": 521, "seek": 253796, "start": 2537.96, "end": 2547.2, "text": " Everything here is the same as Momentum so far, except that I take my gradient squared,", "tokens": [5471, 510, 307, 264, 912, 382, 19093, 449, 370, 1400, 11, 3993, 300, 286, 747, 452, 16235, 8889, 11], "temperature": 0.0, "avg_logprob": -0.12876546382904053, "compression_ratio": 1.5235849056603774, "no_speech_prob": 5.771863470727112e-06}, {"id": 522, "seek": 253796, "start": 2547.2, "end": 2555.64, "text": " multiply it by my.1 and add it to my previous cell times.9.", "tokens": [12972, 309, 538, 452, 2411, 16, 293, 909, 309, 281, 452, 3894, 2815, 1413, 2411, 24, 13], "temperature": 0.0, "avg_logprob": -0.12876546382904053, "compression_ratio": 1.5235849056603774, "no_speech_prob": 5.771863470727112e-06}, {"id": 523, "seek": 253796, "start": 2555.64, "end": 2561.96, "text": " So this is keeping track of the recent running average of the squareds of the gradients.", "tokens": [407, 341, 307, 5145, 2837, 295, 264, 5162, 2614, 4274, 295, 264, 8889, 82, 295, 264, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.12876546382904053, "compression_ratio": 1.5235849056603774, "no_speech_prob": 5.771863470727112e-06}, {"id": 524, "seek": 253796, "start": 2561.96, "end": 2566.2, "text": " And when I have that, I then do exactly the same thing that I did in AdaGrad, which is", "tokens": [400, 562, 286, 362, 300, 11, 286, 550, 360, 2293, 264, 912, 551, 300, 286, 630, 294, 32276, 38, 6206, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.12876546382904053, "compression_ratio": 1.5235849056603774, "no_speech_prob": 5.771863470727112e-06}, {"id": 525, "seek": 256620, "start": 2566.2, "end": 2568.24, "text": " to divide the learning rate by it.", "tokens": [281, 9845, 264, 2539, 3314, 538, 309, 13], "temperature": 0.0, "avg_logprob": -0.15280743104865752, "compression_ratio": 1.6169154228855722, "no_speech_prob": 2.9944253583380487e-06}, {"id": 526, "seek": 256620, "start": 2568.24, "end": 2576.7999999999997, "text": " So I take my previous guess as to b and then I subtract from it my derivative times the", "tokens": [407, 286, 747, 452, 3894, 2041, 382, 281, 272, 293, 550, 286, 16390, 490, 309, 452, 13760, 1413, 264], "temperature": 0.0, "avg_logprob": -0.15280743104865752, "compression_ratio": 1.6169154228855722, "no_speech_prob": 2.9944253583380487e-06}, {"id": 527, "seek": 256620, "start": 2576.7999999999997, "end": 2583.6, "text": " learning rate, divide it by the square root of the recent running weighted average of", "tokens": [2539, 3314, 11, 9845, 309, 538, 264, 3732, 5593, 295, 264, 5162, 2614, 32807, 4274, 295], "temperature": 0.0, "avg_logprob": -0.15280743104865752, "compression_ratio": 1.6169154228855722, "no_speech_prob": 2.9944253583380487e-06}, {"id": 528, "seek": 256620, "start": 2583.6, "end": 2585.16, "text": " the squared gradients.", "tokens": [264, 8889, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.15280743104865752, "compression_ratio": 1.6169154228855722, "no_speech_prob": 2.9944253583380487e-06}, {"id": 529, "seek": 256620, "start": 2585.16, "end": 2590.48, "text": " So it's doing basically the same thing as AdaGrad, but in a way that's doing it continuously.", "tokens": [407, 309, 311, 884, 1936, 264, 912, 551, 382, 32276, 38, 6206, 11, 457, 294, 257, 636, 300, 311, 884, 309, 15684, 13], "temperature": 0.0, "avg_logprob": -0.15280743104865752, "compression_ratio": 1.6169154228855722, "no_speech_prob": 2.9944253583380487e-06}, {"id": 530, "seek": 259048, "start": 2590.48, "end": 2596.44, "text": " Question- So these are all different types of learning rate optimization?", "tokens": [14464, 12, 407, 613, 366, 439, 819, 3467, 295, 2539, 3314, 19618, 30], "temperature": 0.0, "avg_logprob": -0.24597613016764322, "compression_ratio": 1.4193548387096775, "no_speech_prob": 7.411211299768183e-06}, {"id": 531, "seek": 259048, "start": 2596.44, "end": 2603.6, "text": " Answer- These last two are different types of dynamic learning rate approaches.", "tokens": [24545, 12, 1981, 1036, 732, 366, 819, 3467, 295, 8546, 2539, 3314, 11587, 13], "temperature": 0.0, "avg_logprob": -0.24597613016764322, "compression_ratio": 1.4193548387096775, "no_speech_prob": 7.411211299768183e-06}, {"id": 532, "seek": 259048, "start": 2603.6, "end": 2604.6, "text": " So let's try this one.", "tokens": [407, 718, 311, 853, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.24597613016764322, "compression_ratio": 1.4193548387096775, "no_speech_prob": 7.411211299768183e-06}, {"id": 533, "seek": 260460, "start": 2604.6, "end": 2623.7599999999998, "text": " We'll run it for a few steps, and again we'll have to guess what learning rate to start", "tokens": [492, 603, 1190, 309, 337, 257, 1326, 4439, 11, 293, 797, 321, 603, 362, 281, 2041, 437, 2539, 3314, 281, 722], "temperature": 0.0, "avg_logprob": -0.364147097565407, "compression_ratio": 1.2636363636363637, "no_speech_prob": 6.85426130075939e-06}, {"id": 534, "seek": 260460, "start": 2623.7599999999998, "end": 2629.7599999999998, "text": " with.", "tokens": [365, 13], "temperature": 0.0, "avg_logprob": -0.364147097565407, "compression_ratio": 1.2636363636363637, "no_speech_prob": 6.85426130075939e-06}, {"id": 535, "seek": 260460, "start": 2629.7599999999998, "end": 2631.86, "text": " So as you can see, this is going pretty well.", "tokens": [407, 382, 291, 393, 536, 11, 341, 307, 516, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.364147097565407, "compression_ratio": 1.2636363636363637, "no_speech_prob": 6.85426130075939e-06}, {"id": 536, "seek": 263186, "start": 2631.86, "end": 2636.42, "text": " And I'll show you something really nice about RMSProp, which is what happens as we get very", "tokens": [400, 286, 603, 855, 291, 746, 534, 1481, 466, 497, 10288, 47, 1513, 11, 597, 307, 437, 2314, 382, 321, 483, 588], "temperature": 0.0, "avg_logprob": -0.14956202226526596, "compression_ratio": 1.6468531468531469, "no_speech_prob": 2.1444493540911935e-05}, {"id": 537, "seek": 263186, "start": 2636.42, "end": 2637.42, "text": " close.", "tokens": [1998, 13], "temperature": 0.0, "avg_logprob": -0.14956202226526596, "compression_ratio": 1.6468531468531469, "no_speech_prob": 2.1444493540911935e-05}, {"id": 538, "seek": 263186, "start": 2637.42, "end": 2639.2000000000003, "text": " We know the right answer is 2 and 30.", "tokens": [492, 458, 264, 558, 1867, 307, 568, 293, 2217, 13], "temperature": 0.0, "avg_logprob": -0.14956202226526596, "compression_ratio": 1.6468531468531469, "no_speech_prob": 2.1444493540911935e-05}, {"id": 539, "seek": 263186, "start": 2639.2000000000003, "end": 2641.2000000000003, "text": " Is it about to explode?", "tokens": [1119, 309, 466, 281, 21411, 30], "temperature": 0.0, "avg_logprob": -0.14956202226526596, "compression_ratio": 1.6468531468531469, "no_speech_prob": 2.1444493540911935e-05}, {"id": 540, "seek": 263186, "start": 2641.2000000000003, "end": 2643.88, "text": " No, it doesn't explode.", "tokens": [883, 11, 309, 1177, 380, 21411, 13], "temperature": 0.0, "avg_logprob": -0.14956202226526596, "compression_ratio": 1.6468531468531469, "no_speech_prob": 2.1444493540911935e-05}, {"id": 541, "seek": 263186, "start": 2643.88, "end": 2649.1600000000003, "text": " And the reason it doesn't explode is because it's recalculating that running average every", "tokens": [400, 264, 1778, 309, 1177, 380, 21411, 307, 570, 309, 311, 850, 304, 2444, 990, 300, 2614, 4274, 633], "temperature": 0.0, "avg_logprob": -0.14956202226526596, "compression_ratio": 1.6468531468531469, "no_speech_prob": 2.1444493540911935e-05}, {"id": 542, "seek": 263186, "start": 2649.1600000000003, "end": 2650.92, "text": " single minibatch.", "tokens": [2167, 923, 897, 852, 13], "temperature": 0.0, "avg_logprob": -0.14956202226526596, "compression_ratio": 1.6468531468531469, "no_speech_prob": 2.1444493540911935e-05}, {"id": 543, "seek": 263186, "start": 2650.92, "end": 2654.76, "text": " And so rather than waiting until the end of the epoch, by which date it's gone so far", "tokens": [400, 370, 2831, 813, 3806, 1826, 264, 917, 295, 264, 30992, 339, 11, 538, 597, 4002, 309, 311, 2780, 370, 1400], "temperature": 0.0, "avg_logprob": -0.14956202226526596, "compression_ratio": 1.6468531468531469, "no_speech_prob": 2.1444493540911935e-05}, {"id": 544, "seek": 263186, "start": 2654.76, "end": 2660.04, "text": " that it can't come back again, it just jumps a little bit too far, and then it recalculates", "tokens": [300, 309, 393, 380, 808, 646, 797, 11, 309, 445, 16704, 257, 707, 857, 886, 1400, 11, 293, 550, 309, 850, 304, 2444, 1024], "temperature": 0.0, "avg_logprob": -0.14956202226526596, "compression_ratio": 1.6468531468531469, "no_speech_prob": 2.1444493540911935e-05}, {"id": 545, "seek": 266004, "start": 2660.04, "end": 2663.08, "text": " the dynamic learning rates and tries again.", "tokens": [264, 8546, 2539, 6846, 293, 9898, 797, 13], "temperature": 0.0, "avg_logprob": -0.14990105306295523, "compression_ratio": 1.7147766323024054, "no_speech_prob": 3.726582463059458e-06}, {"id": 546, "seek": 266004, "start": 2663.08, "end": 2667.8, "text": " So what happens with RMSProp is if your learning rate is too high, then it doesn't explode,", "tokens": [407, 437, 2314, 365, 497, 10288, 47, 1513, 307, 498, 428, 2539, 3314, 307, 886, 1090, 11, 550, 309, 1177, 380, 21411, 11], "temperature": 0.0, "avg_logprob": -0.14990105306295523, "compression_ratio": 1.7147766323024054, "no_speech_prob": 3.726582463059458e-06}, {"id": 547, "seek": 266004, "start": 2667.8, "end": 2671.38, "text": " it just ends up going around the right answer.", "tokens": [309, 445, 5314, 493, 516, 926, 264, 558, 1867, 13], "temperature": 0.0, "avg_logprob": -0.14990105306295523, "compression_ratio": 1.7147766323024054, "no_speech_prob": 3.726582463059458e-06}, {"id": 548, "seek": 266004, "start": 2671.38, "end": 2678.36, "text": " And so when you use RMSProp, as soon as you see your validation scores flatten out, you", "tokens": [400, 370, 562, 291, 764, 497, 10288, 47, 1513, 11, 382, 2321, 382, 291, 536, 428, 24071, 13444, 24183, 484, 11, 291], "temperature": 0.0, "avg_logprob": -0.14990105306295523, "compression_ratio": 1.7147766323024054, "no_speech_prob": 3.726582463059458e-06}, {"id": 549, "seek": 266004, "start": 2678.36, "end": 2682.36, "text": " know this is what's going on, and so therefore you should probably divide your learning rate", "tokens": [458, 341, 307, 437, 311, 516, 322, 11, 293, 370, 4412, 291, 820, 1391, 9845, 428, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.14990105306295523, "compression_ratio": 1.7147766323024054, "no_speech_prob": 3.726582463059458e-06}, {"id": 550, "seek": 266004, "start": 2682.36, "end": 2683.36, "text": " by 10.", "tokens": [538, 1266, 13], "temperature": 0.0, "avg_logprob": -0.14990105306295523, "compression_ratio": 1.7147766323024054, "no_speech_prob": 3.726582463059458e-06}, {"id": 551, "seek": 266004, "start": 2683.36, "end": 2684.8, "text": " And you see me doing this all the time.", "tokens": [400, 291, 536, 385, 884, 341, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.14990105306295523, "compression_ratio": 1.7147766323024054, "no_speech_prob": 3.726582463059458e-06}, {"id": 552, "seek": 266004, "start": 2684.8, "end": 2689.0, "text": " When I'm running Keras stuff, you'll keep seeing me run a few steps, divide the learning", "tokens": [1133, 286, 478, 2614, 591, 6985, 1507, 11, 291, 603, 1066, 2577, 385, 1190, 257, 1326, 4439, 11, 9845, 264, 2539], "temperature": 0.0, "avg_logprob": -0.14990105306295523, "compression_ratio": 1.7147766323024054, "no_speech_prob": 3.726582463059458e-06}, {"id": 553, "seek": 268900, "start": 2689.0, "end": 2690.28, "text": " rate by 10, run a few steps.", "tokens": [3314, 538, 1266, 11, 1190, 257, 1326, 4439, 13], "temperature": 0.0, "avg_logprob": -0.22050782775878905, "compression_ratio": 1.7440944881889764, "no_speech_prob": 3.219204882043414e-05}, {"id": 554, "seek": 268900, "start": 2690.28, "end": 2695.0, "text": " So you don't see that my loss function explodes, you just see that it flattens out.", "tokens": [407, 291, 500, 380, 536, 300, 452, 4470, 2445, 42610, 11, 291, 445, 536, 300, 309, 932, 1591, 694, 484, 13], "temperature": 0.0, "avg_logprob": -0.22050782775878905, "compression_ratio": 1.7440944881889764, "no_speech_prob": 3.219204882043414e-05}, {"id": 555, "seek": 268900, "start": 2695.0, "end": 2696.0, "text": " Question.", "tokens": [14464, 13], "temperature": 0.0, "avg_logprob": -0.22050782775878905, "compression_ratio": 1.7440944881889764, "no_speech_prob": 3.219204882043414e-05}, {"id": 556, "seek": 268900, "start": 2696.0, "end": 2698.12, "text": " Do you want your learning rate to get smaller and smaller?", "tokens": [1144, 291, 528, 428, 2539, 3314, 281, 483, 4356, 293, 4356, 30], "temperature": 0.0, "avg_logprob": -0.22050782775878905, "compression_ratio": 1.7440944881889764, "no_speech_prob": 3.219204882043414e-05}, {"id": 557, "seek": 268900, "start": 2698.12, "end": 2699.12, "text": " Answer.", "tokens": [24545, 13], "temperature": 0.0, "avg_logprob": -0.22050782775878905, "compression_ratio": 1.7440944881889764, "no_speech_prob": 3.219204882043414e-05}, {"id": 558, "seek": 268900, "start": 2699.12, "end": 2700.92, "text": " Yes, you do.", "tokens": [1079, 11, 291, 360, 13], "temperature": 0.0, "avg_logprob": -0.22050782775878905, "compression_ratio": 1.7440944881889764, "no_speech_prob": 3.219204882043414e-05}, {"id": 559, "seek": 268900, "start": 2700.92, "end": 2705.04, "text": " Your very first learning rate often has to start small, and we'll talk about that in", "tokens": [2260, 588, 700, 2539, 3314, 2049, 575, 281, 722, 1359, 11, 293, 321, 603, 751, 466, 300, 294], "temperature": 0.0, "avg_logprob": -0.22050782775878905, "compression_ratio": 1.7440944881889764, "no_speech_prob": 3.219204882043414e-05}, {"id": 560, "seek": 268900, "start": 2705.04, "end": 2710.8, "text": " a moment, but once you've kind of got started, you generally have to gradually decrease the", "tokens": [257, 1623, 11, 457, 1564, 291, 600, 733, 295, 658, 1409, 11, 291, 5101, 362, 281, 13145, 11514, 264], "temperature": 0.0, "avg_logprob": -0.22050782775878905, "compression_ratio": 1.7440944881889764, "no_speech_prob": 3.219204882043414e-05}, {"id": 561, "seek": 268900, "start": 2710.8, "end": 2711.8, "text": " learning rate.", "tokens": [2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.22050782775878905, "compression_ratio": 1.7440944881889764, "no_speech_prob": 3.219204882043414e-05}, {"id": 562, "seek": 268900, "start": 2711.8, "end": 2712.8, "text": " That's called learning rate annealing.", "tokens": [663, 311, 1219, 2539, 3314, 22256, 4270, 13], "temperature": 0.0, "avg_logprob": -0.22050782775878905, "compression_ratio": 1.7440944881889764, "no_speech_prob": 3.219204882043414e-05}, {"id": 563, "seek": 268900, "start": 2712.8, "end": 2713.8, "text": " Question.", "tokens": [14464, 13], "temperature": 0.0, "avg_logprob": -0.22050782775878905, "compression_ratio": 1.7440944881889764, "no_speech_prob": 3.219204882043414e-05}, {"id": 564, "seek": 271380, "start": 2713.8, "end": 2719.92, "text": " Can you repeat what you said earlier that something does the same thing as AdaGrad?", "tokens": [1664, 291, 7149, 437, 291, 848, 3071, 300, 746, 775, 264, 912, 551, 382, 32276, 38, 6206, 30], "temperature": 0.0, "avg_logprob": -0.20781345367431642, "compression_ratio": 1.6274509803921569, "no_speech_prob": 9.972847692552023e-06}, {"id": 565, "seek": 271380, "start": 2719.92, "end": 2725.0800000000004, "text": " So RMSProp, which we're looking at now, does exactly the same thing as AdaGrad, which is", "tokens": [407, 497, 10288, 47, 1513, 11, 597, 321, 434, 1237, 412, 586, 11, 775, 2293, 264, 912, 551, 382, 32276, 38, 6206, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.20781345367431642, "compression_ratio": 1.6274509803921569, "no_speech_prob": 9.972847692552023e-06}, {"id": 566, "seek": 271380, "start": 2725.0800000000004, "end": 2731.1200000000003, "text": " divide the learning rate by the root sum of squared of the gradients.", "tokens": [9845, 264, 2539, 3314, 538, 264, 5593, 2408, 295, 8889, 295, 264, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.20781345367431642, "compression_ratio": 1.6274509803921569, "no_speech_prob": 9.972847692552023e-06}, {"id": 567, "seek": 271380, "start": 2731.1200000000003, "end": 2738.4, "text": " But rather than doing it since the beginning of time, or every minibatch, or every epoch,", "tokens": [583, 2831, 813, 884, 309, 1670, 264, 2863, 295, 565, 11, 420, 633, 923, 897, 852, 11, 420, 633, 30992, 339, 11], "temperature": 0.0, "avg_logprob": -0.20781345367431642, "compression_ratio": 1.6274509803921569, "no_speech_prob": 9.972847692552023e-06}, {"id": 568, "seek": 273840, "start": 2738.4, "end": 2744.32, "text": " RMSProp does it continuously using the same technique that we learned from momentum, which", "tokens": [497, 10288, 47, 1513, 775, 309, 15684, 1228, 264, 912, 6532, 300, 321, 3264, 490, 11244, 11, 597], "temperature": 0.0, "avg_logprob": -0.1725348154703776, "compression_ratio": 1.51131221719457, "no_speech_prob": 6.747978659404907e-06}, {"id": 569, "seek": 273840, "start": 2744.32, "end": 2754.12, "text": " is take the squared of this gradient, multiply it by 0.1 and add it to 0.9 times the last", "tokens": [307, 747, 264, 8889, 295, 341, 16235, 11, 12972, 309, 538, 1958, 13, 16, 293, 909, 309, 281, 1958, 13, 24, 1413, 264, 1036], "temperature": 0.0, "avg_logprob": -0.1725348154703776, "compression_ratio": 1.51131221719457, "no_speech_prob": 6.747978659404907e-06}, {"id": 570, "seek": 273840, "start": 2754.12, "end": 2755.12, "text": " calculation.", "tokens": [17108, 13], "temperature": 0.0, "avg_logprob": -0.1725348154703776, "compression_ratio": 1.51131221719457, "no_speech_prob": 6.747978659404907e-06}, {"id": 571, "seek": 273840, "start": 2755.12, "end": 2756.12, "text": " Question.", "tokens": [14464, 13], "temperature": 0.0, "avg_logprob": -0.1725348154703776, "compression_ratio": 1.51131221719457, "no_speech_prob": 6.747978659404907e-06}, {"id": 572, "seek": 273840, "start": 2756.12, "end": 2757.6800000000003, "text": " That's called a moving average.", "tokens": [663, 311, 1219, 257, 2684, 4274, 13], "temperature": 0.0, "avg_logprob": -0.1725348154703776, "compression_ratio": 1.51131221719457, "no_speech_prob": 6.747978659404907e-06}, {"id": 573, "seek": 273840, "start": 2757.6800000000003, "end": 2758.6800000000003, "text": " Answer.", "tokens": [24545, 13], "temperature": 0.0, "avg_logprob": -0.1725348154703776, "compression_ratio": 1.51131221719457, "no_speech_prob": 6.747978659404907e-06}, {"id": 574, "seek": 273840, "start": 2758.6800000000003, "end": 2766.48, "text": " It's a weighted moving average, where we're weighting it such that the more recent squared", "tokens": [467, 311, 257, 32807, 2684, 4274, 11, 689, 321, 434, 3364, 278, 309, 1270, 300, 264, 544, 5162, 8889], "temperature": 0.0, "avg_logprob": -0.1725348154703776, "compression_ratio": 1.51131221719457, "no_speech_prob": 6.747978659404907e-06}, {"id": 575, "seek": 276648, "start": 2766.48, "end": 2769.2, "text": " gradients are weighted higher.", "tokens": [2771, 2448, 366, 32807, 2946, 13], "temperature": 0.0, "avg_logprob": -0.16854282220204672, "compression_ratio": 1.5367965367965368, "no_speech_prob": 3.668847057269886e-06}, {"id": 576, "seek": 276648, "start": 2769.2, "end": 2775.4, "text": " I think it's actually an exponentially weighted moving average, to be more precise.", "tokens": [286, 519, 309, 311, 767, 364, 37330, 32807, 2684, 4274, 11, 281, 312, 544, 13600, 13], "temperature": 0.0, "avg_logprob": -0.16854282220204672, "compression_ratio": 1.5367965367965368, "no_speech_prob": 3.668847057269886e-06}, {"id": 577, "seek": 276648, "start": 2775.4, "end": 2778.84, "text": " So there's something pretty obvious we could do here, which is momentum seems like a good", "tokens": [407, 456, 311, 746, 1238, 6322, 321, 727, 360, 510, 11, 597, 307, 11244, 2544, 411, 257, 665], "temperature": 0.0, "avg_logprob": -0.16854282220204672, "compression_ratio": 1.5367965367965368, "no_speech_prob": 3.668847057269886e-06}, {"id": 578, "seek": 276648, "start": 2778.84, "end": 2785.56, "text": " idea, RMSProp seems like a good idea, why not do both?", "tokens": [1558, 11, 497, 10288, 47, 1513, 2544, 411, 257, 665, 1558, 11, 983, 406, 360, 1293, 30], "temperature": 0.0, "avg_logprob": -0.16854282220204672, "compression_ratio": 1.5367965367965368, "no_speech_prob": 3.668847057269886e-06}, {"id": 579, "seek": 276648, "start": 2785.56, "end": 2788.16, "text": " And that is called Adam.", "tokens": [400, 300, 307, 1219, 7938, 13], "temperature": 0.0, "avg_logprob": -0.16854282220204672, "compression_ratio": 1.5367965367965368, "no_speech_prob": 3.668847057269886e-06}, {"id": 580, "seek": 276648, "start": 2788.16, "end": 2793.28, "text": " And so Adam was invented like, I don't know, last year, 18 months ago.", "tokens": [400, 370, 7938, 390, 14479, 411, 11, 286, 500, 380, 458, 11, 1036, 1064, 11, 2443, 2493, 2057, 13], "temperature": 0.0, "avg_logprob": -0.16854282220204672, "compression_ratio": 1.5367965367965368, "no_speech_prob": 3.668847057269886e-06}, {"id": 581, "seek": 279328, "start": 2793.28, "end": 2797.76, "text": " And hopefully one of the things you see from these spreadsheets is that these recently", "tokens": [400, 4696, 472, 295, 264, 721, 291, 536, 490, 613, 23651, 1385, 307, 300, 613, 3938], "temperature": 0.0, "avg_logprob": -0.14519179308855976, "compression_ratio": 1.7718631178707225, "no_speech_prob": 4.637842266674852e-06}, {"id": 582, "seek": 279328, "start": 2797.76, "end": 2804.1200000000003, "text": " invented things are still at the ridiculously extremely simple end of the spectrum.", "tokens": [14479, 721, 366, 920, 412, 264, 41358, 4664, 2199, 917, 295, 264, 11143, 13], "temperature": 0.0, "avg_logprob": -0.14519179308855976, "compression_ratio": 1.7718631178707225, "no_speech_prob": 4.637842266674852e-06}, {"id": 583, "seek": 279328, "start": 2804.1200000000003, "end": 2808.6400000000003, "text": " So the stuff that people are discovering in deep learning is a long, long, long, long", "tokens": [407, 264, 1507, 300, 561, 366, 24773, 294, 2452, 2539, 307, 257, 938, 11, 938, 11, 938, 11, 938], "temperature": 0.0, "avg_logprob": -0.14519179308855976, "compression_ratio": 1.7718631178707225, "no_speech_prob": 4.637842266674852e-06}, {"id": 584, "seek": 279328, "start": 2808.6400000000003, "end": 2813.2000000000003, "text": " way away from being incredibly complex or sophisticated.", "tokens": [636, 1314, 490, 885, 6252, 3997, 420, 16950, 13], "temperature": 0.0, "avg_logprob": -0.14519179308855976, "compression_ratio": 1.7718631178707225, "no_speech_prob": 4.637842266674852e-06}, {"id": 585, "seek": 279328, "start": 2813.2000000000003, "end": 2817.5, "text": " And so hopefully you'll find this very encouraging, which is if you want to play at the state", "tokens": [400, 370, 4696, 291, 603, 915, 341, 588, 14580, 11, 597, 307, 498, 291, 528, 281, 862, 412, 264, 1785], "temperature": 0.0, "avg_logprob": -0.14519179308855976, "compression_ratio": 1.7718631178707225, "no_speech_prob": 4.637842266674852e-06}, {"id": 586, "seek": 279328, "start": 2817.5, "end": 2822.88, "text": " of the art of deep learning, that's not at all hard to do.", "tokens": [295, 264, 1523, 295, 2452, 2539, 11, 300, 311, 406, 412, 439, 1152, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.14519179308855976, "compression_ratio": 1.7718631178707225, "no_speech_prob": 4.637842266674852e-06}, {"id": 587, "seek": 282288, "start": 2822.88, "end": 2829.12, "text": " So let's look at Adam, which I remember it coming out 12-18 months ago, and everybody", "tokens": [407, 718, 311, 574, 412, 7938, 11, 597, 286, 1604, 309, 1348, 484, 2272, 12, 6494, 2493, 2057, 11, 293, 2201], "temperature": 0.0, "avg_logprob": -0.1480605032591693, "compression_ratio": 1.5401459854014599, "no_speech_prob": 1.341985807812307e-05}, {"id": 588, "seek": 282288, "start": 2829.12, "end": 2834.1600000000003, "text": " was so excited because suddenly it became so much easier and faster to train neural", "tokens": [390, 370, 2919, 570, 5800, 309, 3062, 370, 709, 3571, 293, 4663, 281, 3847, 18161], "temperature": 0.0, "avg_logprob": -0.1480605032591693, "compression_ratio": 1.5401459854014599, "no_speech_prob": 1.341985807812307e-05}, {"id": 589, "seek": 282288, "start": 2834.1600000000003, "end": 2835.1600000000003, "text": " nets.", "tokens": [36170, 13], "temperature": 0.0, "avg_logprob": -0.1480605032591693, "compression_ratio": 1.5401459854014599, "no_speech_prob": 1.341985807812307e-05}, {"id": 590, "seek": 282288, "start": 2835.1600000000003, "end": 2839.2200000000003, "text": " But once I actually tried to create an Excel spreadsheet out of it, I realized, oh my god,", "tokens": [583, 1564, 286, 767, 3031, 281, 1884, 364, 19060, 27733, 484, 295, 309, 11, 286, 5334, 11, 1954, 452, 3044, 11], "temperature": 0.0, "avg_logprob": -0.1480605032591693, "compression_ratio": 1.5401459854014599, "no_speech_prob": 1.341985807812307e-05}, {"id": 591, "seek": 282288, "start": 2839.2200000000003, "end": 2842.6400000000003, "text": " it's just RMSProp to plus momentum.", "tokens": [309, 311, 445, 497, 10288, 47, 1513, 281, 1804, 11244, 13], "temperature": 0.0, "avg_logprob": -0.1480605032591693, "compression_ratio": 1.5401459854014599, "no_speech_prob": 1.341985807812307e-05}, {"id": 592, "seek": 282288, "start": 2842.6400000000003, "end": 2847.48, "text": " And so literally all I did was I copied my momentum page and then I copied across my", "tokens": [400, 370, 3736, 439, 286, 630, 390, 286, 25365, 452, 11244, 3028, 293, 550, 286, 25365, 2108, 452], "temperature": 0.0, "avg_logprob": -0.1480605032591693, "compression_ratio": 1.5401459854014599, "no_speech_prob": 1.341985807812307e-05}, {"id": 593, "seek": 282288, "start": 2847.48, "end": 2850.96, "text": " RMSProp columns and combined them.", "tokens": [497, 10288, 47, 1513, 13766, 293, 9354, 552, 13], "temperature": 0.0, "avg_logprob": -0.1480605032591693, "compression_ratio": 1.5401459854014599, "no_speech_prob": 1.341985807812307e-05}, {"id": 594, "seek": 285096, "start": 2850.96, "end": 2862.32, "text": " So you can see here I have my exponentially weighted moving average of the gradients.", "tokens": [407, 291, 393, 536, 510, 286, 362, 452, 37330, 32807, 2684, 4274, 295, 264, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.15812187194824218, "compression_ratio": 1.7551020408163265, "no_speech_prob": 7.889191692811437e-06}, {"id": 595, "seek": 285096, "start": 2862.32, "end": 2868.08, "text": " Here is my exponentially weighted moving average of the squareds of the gradients.", "tokens": [1692, 307, 452, 37330, 32807, 2684, 4274, 295, 264, 8889, 82, 295, 264, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.15812187194824218, "compression_ratio": 1.7551020408163265, "no_speech_prob": 7.889191692811437e-06}, {"id": 596, "seek": 285096, "start": 2868.08, "end": 2878.28, "text": " And so then when I calculate my new parameters, I take my old parameter and I subtract my", "tokens": [400, 370, 550, 562, 286, 8873, 452, 777, 9834, 11, 286, 747, 452, 1331, 13075, 293, 286, 16390, 452], "temperature": 0.0, "avg_logprob": -0.15812187194824218, "compression_ratio": 1.7551020408163265, "no_speech_prob": 7.889191692811437e-06}, {"id": 597, "seek": 287828, "start": 2878.28, "end": 2883.5600000000004, "text": " derivative times the learning rate, but my momentum factor.", "tokens": [13760, 1413, 264, 2539, 3314, 11, 457, 452, 11244, 5952, 13], "temperature": 0.0, "avg_logprob": -0.220333718634271, "compression_ratio": 1.733695652173913, "no_speech_prob": 3.4465579119569156e-06}, {"id": 598, "seek": 287828, "start": 2883.5600000000004, "end": 2890.6000000000004, "text": " So in other words, the recent weighted moving average of the gradients, multiplied by the", "tokens": [407, 294, 661, 2283, 11, 264, 5162, 32807, 2684, 4274, 295, 264, 2771, 2448, 11, 17207, 538, 264], "temperature": 0.0, "avg_logprob": -0.220333718634271, "compression_ratio": 1.733695652173913, "no_speech_prob": 3.4465579119569156e-06}, {"id": 599, "seek": 287828, "start": 2890.6000000000004, "end": 2897.1200000000003, "text": " learning rate, divided by the recent moving average of the square of the derivatives,", "tokens": [2539, 3314, 11, 6666, 538, 264, 5162, 2684, 4274, 295, 264, 3732, 295, 264, 33733, 11], "temperature": 0.0, "avg_logprob": -0.220333718634271, "compression_ratio": 1.733695652173913, "no_speech_prob": 3.4465579119569156e-06}, {"id": 600, "seek": 287828, "start": 2897.1200000000003, "end": 2898.8, "text": " or the root of them anyway.", "tokens": [420, 264, 5593, 295, 552, 4033, 13], "temperature": 0.0, "avg_logprob": -0.220333718634271, "compression_ratio": 1.733695652173913, "no_speech_prob": 3.4465579119569156e-06}, {"id": 601, "seek": 287828, "start": 2898.8, "end": 2906.6400000000003, "text": " So it's literally just combining momentum plus RMSProp.", "tokens": [407, 309, 311, 3736, 445, 21928, 11244, 1804, 497, 10288, 47, 1513, 13], "temperature": 0.0, "avg_logprob": -0.220333718634271, "compression_ratio": 1.733695652173913, "no_speech_prob": 3.4465579119569156e-06}, {"id": 602, "seek": 290664, "start": 2906.64, "end": 2908.24, "text": " And so let's see how that goes.", "tokens": [400, 370, 718, 311, 536, 577, 300, 1709, 13], "temperature": 0.0, "avg_logprob": -0.15208299042748624, "compression_ratio": 1.6401673640167365, "no_speech_prob": 1.8924916730611585e-05}, {"id": 603, "seek": 290664, "start": 2908.24, "end": 2915.04, "text": " Let's run 5epox, and we can use a pretty high learning rate now because it's really handling", "tokens": [961, 311, 1190, 1025, 595, 5230, 11, 293, 321, 393, 764, 257, 1238, 1090, 2539, 3314, 586, 570, 309, 311, 534, 13175], "temperature": 0.0, "avg_logprob": -0.15208299042748624, "compression_ratio": 1.6401673640167365, "no_speech_prob": 1.8924916730611585e-05}, {"id": 604, "seek": 290664, "start": 2915.04, "end": 2916.8399999999997, "text": " a lot of stuff for us.", "tokens": [257, 688, 295, 1507, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.15208299042748624, "compression_ratio": 1.6401673640167365, "no_speech_prob": 1.8924916730611585e-05}, {"id": 605, "seek": 290664, "start": 2916.8399999999997, "end": 2920.62, "text": " And 5epox, we're almost perfect.", "tokens": [400, 1025, 595, 5230, 11, 321, 434, 1920, 2176, 13], "temperature": 0.0, "avg_logprob": -0.15208299042748624, "compression_ratio": 1.6401673640167365, "no_speech_prob": 1.8924916730611585e-05}, {"id": 606, "seek": 290664, "start": 2920.62, "end": 2926.4, "text": " And so another 5epox, it does exactly the same thing that RMSProp does, which is it", "tokens": [400, 370, 1071, 1025, 595, 5230, 11, 309, 775, 2293, 264, 912, 551, 300, 497, 10288, 47, 1513, 775, 11, 597, 307, 309], "temperature": 0.0, "avg_logprob": -0.15208299042748624, "compression_ratio": 1.6401673640167365, "no_speech_prob": 1.8924916730611585e-05}, {"id": 607, "seek": 290664, "start": 2926.4, "end": 2929.2799999999997, "text": " goes too far and tries to come back.", "tokens": [1709, 886, 1400, 293, 9898, 281, 808, 646, 13], "temperature": 0.0, "avg_logprob": -0.15208299042748624, "compression_ratio": 1.6401673640167365, "no_speech_prob": 1.8924916730611585e-05}, {"id": 608, "seek": 290664, "start": 2929.2799999999997, "end": 2931.92, "text": " So we need to do the same thing when we use Adam.", "tokens": [407, 321, 643, 281, 360, 264, 912, 551, 562, 321, 764, 7938, 13], "temperature": 0.0, "avg_logprob": -0.15208299042748624, "compression_ratio": 1.6401673640167365, "no_speech_prob": 1.8924916730611585e-05}, {"id": 609, "seek": 290664, "start": 2931.92, "end": 2935.08, "text": " And Adam is what I use all the time now.", "tokens": [400, 7938, 307, 437, 286, 764, 439, 264, 565, 586, 13], "temperature": 0.0, "avg_logprob": -0.15208299042748624, "compression_ratio": 1.6401673640167365, "no_speech_prob": 1.8924916730611585e-05}, {"id": 610, "seek": 293508, "start": 2935.08, "end": 2941.2799999999997, "text": " I just divide by 10 every time I see it flatten out.", "tokens": [286, 445, 9845, 538, 1266, 633, 565, 286, 536, 309, 24183, 484, 13], "temperature": 0.0, "avg_logprob": -0.1369917509985752, "compression_ratio": 1.4142011834319526, "no_speech_prob": 5.862790203536861e-06}, {"id": 611, "seek": 293508, "start": 2941.2799999999997, "end": 2948.84, "text": " So a week ago, somebody came out with something that they called not Adam, but Eve.", "tokens": [407, 257, 1243, 2057, 11, 2618, 1361, 484, 365, 746, 300, 436, 1219, 406, 7938, 11, 457, 15544, 13], "temperature": 0.0, "avg_logprob": -0.1369917509985752, "compression_ratio": 1.4142011834319526, "no_speech_prob": 5.862790203536861e-06}, {"id": 612, "seek": 293508, "start": 2948.84, "end": 2956.6, "text": " And Eve is an addition to Adam which attempts to deal with this learning rate annealing", "tokens": [400, 15544, 307, 364, 4500, 281, 7938, 597, 15257, 281, 2028, 365, 341, 2539, 3314, 22256, 4270], "temperature": 0.0, "avg_logprob": -0.1369917509985752, "compression_ratio": 1.4142011834319526, "no_speech_prob": 5.862790203536861e-06}, {"id": 613, "seek": 293508, "start": 2956.6, "end": 2959.3199999999997, "text": " automatically.", "tokens": [6772, 13], "temperature": 0.0, "avg_logprob": -0.1369917509985752, "compression_ratio": 1.4142011834319526, "no_speech_prob": 5.862790203536861e-06}, {"id": 614, "seek": 295932, "start": 2959.32, "end": 2965.2400000000002, "text": " And so all of this is exactly the same as my Adam page.", "tokens": [400, 370, 439, 295, 341, 307, 2293, 264, 912, 382, 452, 7938, 3028, 13], "temperature": 0.0, "avg_logprob": -0.14385074697515016, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.994718104469939e-06}, {"id": 615, "seek": 295932, "start": 2965.2400000000002, "end": 2967.76, "text": " But at the bottom I've added some extra stuff.", "tokens": [583, 412, 264, 2767, 286, 600, 3869, 512, 2857, 1507, 13], "temperature": 0.0, "avg_logprob": -0.14385074697515016, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.994718104469939e-06}, {"id": 616, "seek": 295932, "start": 2967.76, "end": 2974.6400000000003, "text": " I have kept track of the root mean squared error, this is just my loss function.", "tokens": [286, 362, 4305, 2837, 295, 264, 5593, 914, 8889, 6713, 11, 341, 307, 445, 452, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.14385074697515016, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.994718104469939e-06}, {"id": 617, "seek": 295932, "start": 2974.6400000000003, "end": 2981.4, "text": " And then I copy across my loss function from my previous epoch and from the epoch before", "tokens": [400, 550, 286, 5055, 2108, 452, 4470, 2445, 490, 452, 3894, 30992, 339, 293, 490, 264, 30992, 339, 949], "temperature": 0.0, "avg_logprob": -0.14385074697515016, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.994718104469939e-06}, {"id": 618, "seek": 295932, "start": 2981.4, "end": 2982.6400000000003, "text": " that.", "tokens": [300, 13], "temperature": 0.0, "avg_logprob": -0.14385074697515016, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.994718104469939e-06}, {"id": 619, "seek": 295932, "start": 2982.6400000000003, "end": 2988.1600000000003, "text": " And what Eve does is it says how much has the loss function changed.", "tokens": [400, 437, 15544, 775, 307, 309, 1619, 577, 709, 575, 264, 4470, 2445, 3105, 13], "temperature": 0.0, "avg_logprob": -0.14385074697515016, "compression_ratio": 1.613953488372093, "no_speech_prob": 1.994718104469939e-06}, {"id": 620, "seek": 298816, "start": 2988.16, "end": 2994.68, "text": " And so it's got this ratio between the previous loss function and the loss function before", "tokens": [400, 370, 309, 311, 658, 341, 8509, 1296, 264, 3894, 4470, 2445, 293, 264, 4470, 2445, 949], "temperature": 0.0, "avg_logprob": -0.12694783528645834, "compression_ratio": 1.5989304812834224, "no_speech_prob": 4.9369455155101605e-06}, {"id": 621, "seek": 298816, "start": 2994.68, "end": 2995.68, "text": " that.", "tokens": [300, 13], "temperature": 0.0, "avg_logprob": -0.12694783528645834, "compression_ratio": 1.5989304812834224, "no_speech_prob": 4.9369455155101605e-06}, {"id": 622, "seek": 298816, "start": 2995.68, "end": 3000.3599999999997, "text": " So you can see it's the absolute value of the last one minus the one before, divided", "tokens": [407, 291, 393, 536, 309, 311, 264, 8236, 2158, 295, 264, 1036, 472, 3175, 264, 472, 949, 11, 6666], "temperature": 0.0, "avg_logprob": -0.12694783528645834, "compression_ratio": 1.5989304812834224, "no_speech_prob": 4.9369455155101605e-06}, {"id": 623, "seek": 298816, "start": 3000.3599999999997, "end": 3002.92, "text": " by whichever one is smaller.", "tokens": [538, 24123, 472, 307, 4356, 13], "temperature": 0.0, "avg_logprob": -0.12694783528645834, "compression_ratio": 1.5989304812834224, "no_speech_prob": 4.9369455155101605e-06}, {"id": 624, "seek": 298816, "start": 3002.92, "end": 3010.7999999999997, "text": " And what it says is, let's then adjust the learning rate such that instead of just using", "tokens": [400, 437, 309, 1619, 307, 11, 718, 311, 550, 4369, 264, 2539, 3314, 1270, 300, 2602, 295, 445, 1228], "temperature": 0.0, "avg_logprob": -0.12694783528645834, "compression_ratio": 1.5989304812834224, "no_speech_prob": 4.9369455155101605e-06}, {"id": 625, "seek": 301080, "start": 3010.8, "end": 3026.5600000000004, "text": " the learning rate we're given, let's adjust the learning rate that we're given by taking", "tokens": [264, 2539, 3314, 321, 434, 2212, 11, 718, 311, 4369, 264, 2539, 3314, 300, 321, 434, 2212, 538, 1940], "temperature": 0.0, "avg_logprob": -0.2908036455195001, "compression_ratio": 1.5546875, "no_speech_prob": 5.25537370776874e-06}, {"id": 626, "seek": 301080, "start": 3026.5600000000004, "end": 3033.88, "text": " the exponentially weighted moving average of these ratios.", "tokens": [264, 37330, 32807, 2684, 4274, 295, 613, 32435, 13], "temperature": 0.0, "avg_logprob": -0.2908036455195001, "compression_ratio": 1.5546875, "no_speech_prob": 5.25537370776874e-06}, {"id": 627, "seek": 301080, "start": 3033.88, "end": 3036.1600000000003, "text": " You can see another of these beta's appearing here.", "tokens": [509, 393, 536, 1071, 295, 613, 9861, 311, 19870, 510, 13], "temperature": 0.0, "avg_logprob": -0.2908036455195001, "compression_ratio": 1.5546875, "no_speech_prob": 5.25537370776874e-06}, {"id": 628, "seek": 303616, "start": 3036.16, "end": 3050.2, "text": " So this thing here is equal to our last ratio times 0.9 plus our new ratio times 0.1.", "tokens": [407, 341, 551, 510, 307, 2681, 281, 527, 1036, 8509, 1413, 1958, 13, 24, 1804, 527, 777, 8509, 1413, 1958, 13, 16, 13], "temperature": 0.0, "avg_logprob": -0.13595022039210541, "compression_ratio": 1.403361344537815, "no_speech_prob": 3.3931305551959667e-06}, {"id": 629, "seek": 303616, "start": 3050.2, "end": 3064.72, "text": " And so then for our learning rate, we divide the learning rate from Adam by this.", "tokens": [400, 370, 550, 337, 527, 2539, 3314, 11, 321, 9845, 264, 2539, 3314, 490, 7938, 538, 341, 13], "temperature": 0.0, "avg_logprob": -0.13595022039210541, "compression_ratio": 1.403361344537815, "no_speech_prob": 3.3931305551959667e-06}, {"id": 630, "seek": 306472, "start": 3064.72, "end": 3071.9199999999996, "text": " So what that says is, if the learning rate is moving around a lot, if it's very bumpy,", "tokens": [407, 437, 300, 1619, 307, 11, 498, 264, 2539, 3314, 307, 2684, 926, 257, 688, 11, 498, 309, 311, 588, 49400, 11], "temperature": 0.0, "avg_logprob": -0.10321885661074989, "compression_ratio": 1.7469879518072289, "no_speech_prob": 4.737899814699631e-07}, {"id": 631, "seek": 306472, "start": 3071.9199999999996, "end": 3076.7999999999997, "text": " we should probably decrease the learning rate because it's going all over the place.", "tokens": [321, 820, 1391, 11514, 264, 2539, 3314, 570, 309, 311, 516, 439, 670, 264, 1081, 13], "temperature": 0.0, "avg_logprob": -0.10321885661074989, "compression_ratio": 1.7469879518072289, "no_speech_prob": 4.737899814699631e-07}, {"id": 632, "seek": 306472, "start": 3076.7999999999997, "end": 3080.68, "text": " Remember how we saw before, if we've kind of gone past where we want to get to, it just", "tokens": [5459, 577, 321, 1866, 949, 11, 498, 321, 600, 733, 295, 2780, 1791, 689, 321, 528, 281, 483, 281, 11, 309, 445], "temperature": 0.0, "avg_logprob": -0.10321885661074989, "compression_ratio": 1.7469879518072289, "no_speech_prob": 4.737899814699631e-07}, {"id": 633, "seek": 306472, "start": 3080.68, "end": 3082.9199999999996, "text": " jumps up and down.", "tokens": [16704, 493, 293, 760, 13], "temperature": 0.0, "avg_logprob": -0.10321885661074989, "compression_ratio": 1.7469879518072289, "no_speech_prob": 4.737899814699631e-07}, {"id": 634, "seek": 306472, "start": 3082.9199999999996, "end": 3087.9199999999996, "text": " On the other hand, if the loss function is staying pretty constant, then we probably", "tokens": [1282, 264, 661, 1011, 11, 498, 264, 4470, 2445, 307, 7939, 1238, 5754, 11, 550, 321, 1391], "temperature": 0.0, "avg_logprob": -0.10321885661074989, "compression_ratio": 1.7469879518072289, "no_speech_prob": 4.737899814699631e-07}, {"id": 635, "seek": 306472, "start": 3087.9199999999996, "end": 3090.64, "text": " want to increase the learning rate.", "tokens": [528, 281, 3488, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.10321885661074989, "compression_ratio": 1.7469879518072289, "no_speech_prob": 4.737899814699631e-07}, {"id": 636, "seek": 306472, "start": 3090.64, "end": 3092.66, "text": " So that all seems like a good idea.", "tokens": [407, 300, 439, 2544, 411, 257, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.10321885661074989, "compression_ratio": 1.7469879518072289, "no_speech_prob": 4.737899814699631e-07}, {"id": 637, "seek": 309266, "start": 3092.66, "end": 3101.04, "text": " And so again, let's try it.", "tokens": [400, 370, 797, 11, 718, 311, 853, 309, 13], "temperature": 0.0, "avg_logprob": -0.18669911372808762, "compression_ratio": 1.405128205128205, "no_speech_prob": 1.0952966476907022e-05}, {"id": 638, "seek": 309266, "start": 3101.04, "end": 3102.04, "text": " Not bad, right?", "tokens": [1726, 1578, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18669911372808762, "compression_ratio": 1.405128205128205, "no_speech_prob": 1.0952966476907022e-05}, {"id": 639, "seek": 309266, "start": 3102.04, "end": 3106.2799999999997, "text": " So after 5 epochs, it's kind of gone a little bit too far.", "tokens": [407, 934, 1025, 30992, 28346, 11, 309, 311, 733, 295, 2780, 257, 707, 857, 886, 1400, 13], "temperature": 0.0, "avg_logprob": -0.18669911372808762, "compression_ratio": 1.405128205128205, "no_speech_prob": 1.0952966476907022e-05}, {"id": 640, "seek": 309266, "start": 3106.2799999999997, "end": 3109.6, "text": " After a week of playing with it, I use this on State Farm a lot during the week.", "tokens": [2381, 257, 1243, 295, 2433, 365, 309, 11, 286, 764, 341, 322, 4533, 19991, 257, 688, 1830, 264, 1243, 13], "temperature": 0.0, "avg_logprob": -0.18669911372808762, "compression_ratio": 1.405128205128205, "no_speech_prob": 1.0952966476907022e-05}, {"id": 641, "seek": 309266, "start": 3109.6, "end": 3116.6, "text": " I grabbed a Keras implementation which somebody wrote like a day after the paper came out.", "tokens": [286, 18607, 257, 591, 6985, 11420, 597, 2618, 4114, 411, 257, 786, 934, 264, 3035, 1361, 484, 13], "temperature": 0.0, "avg_logprob": -0.18669911372808762, "compression_ratio": 1.405128205128205, "no_speech_prob": 1.0952966476907022e-05}, {"id": 642, "seek": 311660, "start": 3116.6, "end": 3124.8399999999997, "text": " The problem is that because it can both decrease and increase the learning rate, sometimes", "tokens": [440, 1154, 307, 300, 570, 309, 393, 1293, 11514, 293, 3488, 264, 2539, 3314, 11, 2171], "temperature": 0.0, "avg_logprob": -0.11059096161748322, "compression_ratio": 1.6519337016574585, "no_speech_prob": 5.771857559011551e-06}, {"id": 643, "seek": 311660, "start": 3124.8399999999997, "end": 3131.92, "text": " as it gets down to the flat bottom point where it's pretty much optimal, it'll often be the", "tokens": [382, 309, 2170, 760, 281, 264, 4962, 2767, 935, 689, 309, 311, 1238, 709, 16252, 11, 309, 603, 2049, 312, 264], "temperature": 0.0, "avg_logprob": -0.11059096161748322, "compression_ratio": 1.6519337016574585, "no_speech_prob": 5.771857559011551e-06}, {"id": 644, "seek": 311660, "start": 3131.92, "end": 3138.7599999999998, "text": " case that the loss gets pretty constant at that point.", "tokens": [1389, 300, 264, 4470, 2170, 1238, 5754, 412, 300, 935, 13], "temperature": 0.0, "avg_logprob": -0.11059096161748322, "compression_ratio": 1.6519337016574585, "no_speech_prob": 5.771857559011551e-06}, {"id": 645, "seek": 311660, "start": 3138.7599999999998, "end": 3142.2, "text": " And so therefore, Eve will try to increase the learning rate.", "tokens": [400, 370, 4412, 11, 15544, 486, 853, 281, 3488, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.11059096161748322, "compression_ratio": 1.6519337016574585, "no_speech_prob": 5.771857559011551e-06}, {"id": 646, "seek": 314220, "start": 3142.2, "end": 3146.7599999999998, "text": " And so what I tend to find happen is that it would very quickly get pretty close to", "tokens": [400, 370, 437, 286, 3928, 281, 915, 1051, 307, 300, 309, 576, 588, 2661, 483, 1238, 1998, 281], "temperature": 0.0, "avg_logprob": -0.31282749007233474, "compression_ratio": 1.718045112781955, "no_speech_prob": 1.834254726418294e-05}, {"id": 647, "seek": 314220, "start": 3146.7599999999998, "end": 3149.52, "text": " the answer, and then suddenly it would jump to somewhere really awful.", "tokens": [264, 1867, 11, 293, 550, 5800, 309, 576, 3012, 281, 4079, 534, 11232, 13], "temperature": 0.0, "avg_logprob": -0.31282749007233474, "compression_ratio": 1.718045112781955, "no_speech_prob": 1.834254726418294e-05}, {"id": 648, "seek": 314220, "start": 3149.52, "end": 3153.12, "text": " And then it would start to get to the answer again and jump somewhere really awful.", "tokens": [400, 550, 309, 576, 722, 281, 483, 281, 264, 1867, 797, 293, 3012, 4079, 534, 11232, 13], "temperature": 0.0, "avg_logprob": -0.31282749007233474, "compression_ratio": 1.718045112781955, "no_speech_prob": 1.834254726418294e-05}, {"id": 649, "seek": 314220, "start": 3153.12, "end": 3156.12, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.31282749007233474, "compression_ratio": 1.718045112781955, "no_speech_prob": 1.834254726418294e-05}, {"id": 650, "seek": 314220, "start": 3156.12, "end": 3163.0, "text": " Generally, for the exit condition, we give a delta that the change in this gradient should", "tokens": [21082, 11, 337, 264, 11043, 4188, 11, 321, 976, 257, 8289, 300, 264, 1319, 294, 341, 16235, 820], "temperature": 0.0, "avg_logprob": -0.31282749007233474, "compression_ratio": 1.718045112781955, "no_speech_prob": 1.834254726418294e-05}, {"id": 651, "seek": 314220, "start": 3163.0, "end": 3167.72, "text": " be below a certain delta, then we just stop doing that, right?", "tokens": [312, 2507, 257, 1629, 8289, 11, 550, 321, 445, 1590, 884, 300, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.31282749007233474, "compression_ratio": 1.718045112781955, "no_speech_prob": 1.834254726418294e-05}, {"id": 652, "seek": 314220, "start": 3167.72, "end": 3169.48, "text": " We have not done any such thing, no.", "tokens": [492, 362, 406, 1096, 604, 1270, 551, 11, 572, 13], "temperature": 0.0, "avg_logprob": -0.31282749007233474, "compression_ratio": 1.718045112781955, "no_speech_prob": 1.834254726418294e-05}, {"id": 653, "seek": 316948, "start": 3169.48, "end": 3173.72, "text": " We have always said run for a specific number of epochs.", "tokens": [492, 362, 1009, 848, 1190, 337, 257, 2685, 1230, 295, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.09479104416279853, "compression_ratio": 1.5445544554455446, "no_speech_prob": 4.93689913128037e-06}, {"id": 654, "seek": 316948, "start": 3173.72, "end": 3179.56, "text": " We have not defined any kind of stopping criterion.", "tokens": [492, 362, 406, 7642, 604, 733, 295, 12767, 46691, 13], "temperature": 0.0, "avg_logprob": -0.09479104416279853, "compression_ratio": 1.5445544554455446, "no_speech_prob": 4.93689913128037e-06}, {"id": 655, "seek": 316948, "start": 3179.56, "end": 3184.44, "text": " It is possible to define such a stopping criterion, but nobody's really come up with one that's", "tokens": [467, 307, 1944, 281, 6964, 1270, 257, 12767, 46691, 11, 457, 5079, 311, 534, 808, 493, 365, 472, 300, 311], "temperature": 0.0, "avg_logprob": -0.09479104416279853, "compression_ratio": 1.5445544554455446, "no_speech_prob": 4.93689913128037e-06}, {"id": 656, "seek": 316948, "start": 3184.44, "end": 3186.28, "text": " remotely reliable.", "tokens": [20824, 12924, 13], "temperature": 0.0, "avg_logprob": -0.09479104416279853, "compression_ratio": 1.5445544554455446, "no_speech_prob": 4.93689913128037e-06}, {"id": 657, "seek": 316948, "start": 3186.28, "end": 3194.18, "text": " And the reason why is that when you look at the graph of loss over time, it doesn't tend", "tokens": [400, 264, 1778, 983, 307, 300, 562, 291, 574, 412, 264, 4295, 295, 4470, 670, 565, 11, 309, 1177, 380, 3928], "temperature": 0.0, "avg_logprob": -0.09479104416279853, "compression_ratio": 1.5445544554455446, "no_speech_prob": 4.93689913128037e-06}, {"id": 658, "seek": 319418, "start": 3194.18, "end": 3199.94, "text": " to look like that, it tends to look like this.", "tokens": [281, 574, 411, 300, 11, 309, 12258, 281, 574, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.19937373267279732, "compression_ratio": 1.5902439024390245, "no_speech_prob": 3.0415587843890535e-06}, {"id": 659, "seek": 319418, "start": 3199.94, "end": 3205.56, "text": " And so in practice, it's very hard to know when to stop.", "tokens": [400, 370, 294, 3124, 11, 309, 311, 588, 1152, 281, 458, 562, 281, 1590, 13], "temperature": 0.0, "avg_logprob": -0.19937373267279732, "compression_ratio": 1.5902439024390245, "no_speech_prob": 3.0415587843890535e-06}, {"id": 660, "seek": 319418, "start": 3205.56, "end": 3207.44, "text": " It's kind of still a human judgment thing.", "tokens": [467, 311, 733, 295, 920, 257, 1952, 12216, 551, 13], "temperature": 0.0, "avg_logprob": -0.19937373267279732, "compression_ratio": 1.5902439024390245, "no_speech_prob": 3.0415587843890535e-06}, {"id": 661, "seek": 319418, "start": 3207.44, "end": 3211.12, "text": " Oh yeah, that's definitely true.", "tokens": [876, 1338, 11, 300, 311, 2138, 2074, 13], "temperature": 0.0, "avg_logprob": -0.19937373267279732, "compression_ratio": 1.5902439024390245, "no_speech_prob": 3.0415587843890535e-06}, {"id": 662, "seek": 319418, "start": 3211.12, "end": 3216.58, "text": " And particularly with a type of architecture called ResNet that we'll look at next week,", "tokens": [400, 4098, 365, 257, 2010, 295, 9482, 1219, 5015, 31890, 300, 321, 603, 574, 412, 958, 1243, 11], "temperature": 0.0, "avg_logprob": -0.19937373267279732, "compression_ratio": 1.5902439024390245, "no_speech_prob": 3.0415587843890535e-06}, {"id": 663, "seek": 319418, "start": 3216.58, "end": 3223.2799999999997, "text": " the authors showed that it kind of tends to go like this.", "tokens": [264, 16552, 4712, 300, 309, 733, 295, 12258, 281, 352, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.19937373267279732, "compression_ratio": 1.5902439024390245, "no_speech_prob": 3.0415587843890535e-06}, {"id": 664, "seek": 322328, "start": 3223.28, "end": 3228.2400000000002, "text": " In practice, you kind of have to run your training for as long as you have patience", "tokens": [682, 3124, 11, 291, 733, 295, 362, 281, 1190, 428, 3097, 337, 382, 938, 382, 291, 362, 14826], "temperature": 0.0, "avg_logprob": -0.12566172925731803, "compression_ratio": 1.5911330049261083, "no_speech_prob": 4.495087068789871e-06}, {"id": 665, "seek": 322328, "start": 3228.2400000000002, "end": 3233.44, "text": " for at whatever the best learning rate you can come up with is.", "tokens": [337, 412, 2035, 264, 1151, 2539, 3314, 291, 393, 808, 493, 365, 307, 13], "temperature": 0.0, "avg_logprob": -0.12566172925731803, "compression_ratio": 1.5911330049261083, "no_speech_prob": 4.495087068789871e-06}, {"id": 666, "seek": 322328, "start": 3233.44, "end": 3241.0400000000004, "text": " So something I actually came up with 6 or 12 months ago, but we kind of re-stimulated", "tokens": [407, 746, 286, 767, 1361, 493, 365, 1386, 420, 2272, 2493, 2057, 11, 457, 321, 733, 295, 319, 12, 372, 332, 6987], "temperature": 0.0, "avg_logprob": -0.12566172925731803, "compression_ratio": 1.5911330049261083, "no_speech_prob": 4.495087068789871e-06}, {"id": 667, "seek": 322328, "start": 3241.0400000000004, "end": 3247.6800000000003, "text": " my interest after I read this Adam paper, is something which dynamically updates learning", "tokens": [452, 1179, 934, 286, 1401, 341, 7938, 3035, 11, 307, 746, 597, 43492, 9205, 2539], "temperature": 0.0, "avg_logprob": -0.12566172925731803, "compression_ratio": 1.5911330049261083, "no_speech_prob": 4.495087068789871e-06}, {"id": 668, "seek": 324768, "start": 3247.68, "end": 3254.44, "text": " rates in such a way that they only go down, and rather than using the loss function, which", "tokens": [6846, 294, 1270, 257, 636, 300, 436, 787, 352, 760, 11, 293, 2831, 813, 1228, 264, 4470, 2445, 11, 597], "temperature": 0.0, "avg_logprob": -0.14422483553831605, "compression_ratio": 1.551111111111111, "no_speech_prob": 2.0462377506191842e-05}, {"id": 669, "seek": 324768, "start": 3254.44, "end": 3259.8799999999997, "text": " as I just said is incredibly bumpy, there's something else which is less bumpy, which", "tokens": [382, 286, 445, 848, 307, 6252, 49400, 11, 456, 311, 746, 1646, 597, 307, 1570, 49400, 11, 597], "temperature": 0.0, "avg_logprob": -0.14422483553831605, "compression_ratio": 1.551111111111111, "no_speech_prob": 2.0462377506191842e-05}, {"id": 670, "seek": 324768, "start": 3259.8799999999997, "end": 3264.44, "text": " is the average sum of squareds gradients.", "tokens": [307, 264, 4274, 2408, 295, 8889, 82, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.14422483553831605, "compression_ratio": 1.551111111111111, "no_speech_prob": 2.0462377506191842e-05}, {"id": 671, "seek": 324768, "start": 3264.44, "end": 3269.0, "text": " So I actually created a little spreadsheet of my idea and I hope to prototype it in Python", "tokens": [407, 286, 767, 2942, 257, 707, 27733, 295, 452, 1558, 293, 286, 1454, 281, 19475, 309, 294, 15329], "temperature": 0.0, "avg_logprob": -0.14422483553831605, "compression_ratio": 1.551111111111111, "no_speech_prob": 2.0462377506191842e-05}, {"id": 672, "seek": 324768, "start": 3269.0, "end": 3272.2799999999997, "text": " maybe this week or the next week after.", "tokens": [1310, 341, 1243, 420, 264, 958, 1243, 934, 13], "temperature": 0.0, "avg_logprob": -0.14422483553831605, "compression_ratio": 1.551111111111111, "no_speech_prob": 2.0462377506191842e-05}, {"id": 673, "seek": 327228, "start": 3272.28, "end": 3280.88, "text": " And the idea is basically this, keep track of the sum of the squareds of the derivatives", "tokens": [400, 264, 1558, 307, 1936, 341, 11, 1066, 2837, 295, 264, 2408, 295, 264, 8889, 82, 295, 264, 33733], "temperature": 0.0, "avg_logprob": -0.1060800195854401, "compression_ratio": 2.0288461538461537, "no_speech_prob": 1.3211556506576017e-05}, {"id": 674, "seek": 327228, "start": 3280.88, "end": 3285.0400000000004, "text": " and compare the sum of the squareds of the derivatives from the last epoch to the sum", "tokens": [293, 6794, 264, 2408, 295, 264, 8889, 82, 295, 264, 33733, 490, 264, 1036, 30992, 339, 281, 264, 2408], "temperature": 0.0, "avg_logprob": -0.1060800195854401, "compression_ratio": 2.0288461538461537, "no_speech_prob": 1.3211556506576017e-05}, {"id": 675, "seek": 327228, "start": 3285.0400000000004, "end": 3289.96, "text": " of the squareds of the derivatives of this epoch and look at the ratio of the two.", "tokens": [295, 264, 8889, 82, 295, 264, 33733, 295, 341, 30992, 339, 293, 574, 412, 264, 8509, 295, 264, 732, 13], "temperature": 0.0, "avg_logprob": -0.1060800195854401, "compression_ratio": 2.0288461538461537, "no_speech_prob": 1.3211556506576017e-05}, {"id": 676, "seek": 327228, "start": 3289.96, "end": 3293.6400000000003, "text": " The derivatives should keep going down.", "tokens": [440, 33733, 820, 1066, 516, 760, 13], "temperature": 0.0, "avg_logprob": -0.1060800195854401, "compression_ratio": 2.0288461538461537, "no_speech_prob": 1.3211556506576017e-05}, {"id": 677, "seek": 327228, "start": 3293.6400000000003, "end": 3298.86, "text": " If they ever go up by too much, that would strongly suggest that you've kind of jumped", "tokens": [759, 436, 1562, 352, 493, 538, 886, 709, 11, 300, 576, 10613, 3402, 300, 291, 600, 733, 295, 13864], "temperature": 0.0, "avg_logprob": -0.1060800195854401, "compression_ratio": 2.0288461538461537, "no_speech_prob": 1.3211556506576017e-05}, {"id": 678, "seek": 327228, "start": 3298.86, "end": 3301.8, "text": " out of the good part of the function.", "tokens": [484, 295, 264, 665, 644, 295, 264, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1060800195854401, "compression_ratio": 2.0288461538461537, "no_speech_prob": 1.3211556506576017e-05}, {"id": 679, "seek": 330180, "start": 3301.8, "end": 3307.4, "text": " So any time they go up too much, you should decrease the learning rate.", "tokens": [407, 604, 565, 436, 352, 493, 886, 709, 11, 291, 820, 11514, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.1976595852110121, "compression_ratio": 1.458100558659218, "no_speech_prob": 9.223392225976568e-06}, {"id": 680, "seek": 330180, "start": 3307.4, "end": 3314.7200000000003, "text": " So I literally added like two lines of code to my incredibly simple VBA, Adam with a kneeling", "tokens": [407, 286, 3736, 3869, 411, 732, 3876, 295, 3089, 281, 452, 6252, 2199, 691, 9295, 11, 7938, 365, 257, 32704, 11031], "temperature": 0.0, "avg_logprob": -0.1976595852110121, "compression_ratio": 1.458100558659218, "no_speech_prob": 9.223392225976568e-06}, {"id": 681, "seek": 330180, "start": 3314.7200000000003, "end": 3315.7200000000003, "text": " here.", "tokens": [510, 13], "temperature": 0.0, "avg_logprob": -0.1976595852110121, "compression_ratio": 1.458100558659218, "no_speech_prob": 9.223392225976568e-06}, {"id": 682, "seek": 330180, "start": 3315.7200000000003, "end": 3322.04, "text": " If the gradient ratio is greater than 2, so if it doubles, divide the learning rate by", "tokens": [759, 264, 16235, 8509, 307, 5044, 813, 568, 11, 370, 498, 309, 31634, 11, 9845, 264, 2539, 3314, 538], "temperature": 0.0, "avg_logprob": -0.1976595852110121, "compression_ratio": 1.458100558659218, "no_speech_prob": 9.223392225976568e-06}, {"id": 683, "seek": 330180, "start": 3322.04, "end": 3323.04, "text": " 4.", "tokens": [1017, 13], "temperature": 0.0, "avg_logprob": -0.1976595852110121, "compression_ratio": 1.458100558659218, "no_speech_prob": 9.223392225976568e-06}, {"id": 684, "seek": 332304, "start": 3323.04, "end": 3347.52, "text": " Here's what happens when I run that.", "tokens": [1692, 311, 437, 2314, 562, 286, 1190, 300, 13], "temperature": 0.0, "avg_logprob": -0.25541619459788006, "compression_ratio": 1.0, "no_speech_prob": 2.726452748902375e-06}, {"id": 685, "seek": 332304, "start": 3347.52, "end": 3349.2799999999997, "text": " So I'm pretty interested in this idea.", "tokens": [407, 286, 478, 1238, 3102, 294, 341, 1558, 13], "temperature": 0.0, "avg_logprob": -0.25541619459788006, "compression_ratio": 1.0, "no_speech_prob": 2.726452748902375e-06}, {"id": 686, "seek": 334928, "start": 3349.28, "end": 3355.48, "text": " I think it's going to work super well because it allows me to focus on just running stuff", "tokens": [286, 519, 309, 311, 516, 281, 589, 1687, 731, 570, 309, 4045, 385, 281, 1879, 322, 445, 2614, 1507], "temperature": 0.0, "avg_logprob": -0.16577824549888498, "compression_ratio": 1.543010752688172, "no_speech_prob": 1.5445724784513004e-05}, {"id": 687, "seek": 334928, "start": 3355.48, "end": 3357.84, "text": " without ever worrying about setting learning rates.", "tokens": [1553, 1562, 18788, 466, 3287, 2539, 6846, 13], "temperature": 0.0, "avg_logprob": -0.16577824549888498, "compression_ratio": 1.543010752688172, "no_speech_prob": 1.5445724784513004e-05}, {"id": 688, "seek": 334928, "start": 3357.84, "end": 3362.6000000000004, "text": " So I'm hopeful that this approach to automatic learning rate and kneeling is something that", "tokens": [407, 286, 478, 20531, 300, 341, 3109, 281, 12509, 2539, 3314, 293, 32704, 11031, 307, 746, 300], "temperature": 0.0, "avg_logprob": -0.16577824549888498, "compression_ratio": 1.543010752688172, "no_speech_prob": 1.5445724784513004e-05}, {"id": 689, "seek": 334928, "start": 3362.6000000000004, "end": 3366.88, "text": " we can have in our toolbox by the end of this course.", "tokens": [321, 393, 362, 294, 527, 44593, 538, 264, 917, 295, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.16577824549888498, "compression_ratio": 1.543010752688172, "no_speech_prob": 1.5445724784513004e-05}, {"id": 690, "seek": 336688, "start": 3366.88, "end": 3379.44, "text": " One thing that happened to me today is I tried a lot of different learning rates and I didn't", "tokens": [1485, 551, 300, 2011, 281, 385, 965, 307, 286, 3031, 257, 688, 295, 819, 2539, 6846, 293, 286, 994, 380], "temperature": 0.0, "avg_logprob": -0.287356694539388, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.00026494538178667426}, {"id": 691, "seek": 336688, "start": 3379.44, "end": 3380.44, "text": " get anywhere.", "tokens": [483, 4992, 13], "temperature": 0.0, "avg_logprob": -0.287356694539388, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.00026494538178667426}, {"id": 692, "seek": 336688, "start": 3380.44, "end": 3384.12, "text": " But I was working with the whole data set.", "tokens": [583, 286, 390, 1364, 365, 264, 1379, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.287356694539388, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.00026494538178667426}, {"id": 693, "seek": 336688, "start": 3384.12, "end": 3390.04, "text": " Would trying with sample will actually, I'm trying to understand, if I try with a sample", "tokens": [6068, 1382, 365, 6889, 486, 767, 11, 286, 478, 1382, 281, 1223, 11, 498, 286, 853, 365, 257, 6889], "temperature": 0.0, "avg_logprob": -0.287356694539388, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.00026494538178667426}, {"id": 694, "seek": 336688, "start": 3390.04, "end": 3395.96, "text": " and I find something, would that apply to the whole data set or how do I go about investigating", "tokens": [293, 286, 915, 746, 11, 576, 300, 3079, 281, 264, 1379, 1412, 992, 420, 577, 360, 286, 352, 466, 22858], "temperature": 0.0, "avg_logprob": -0.287356694539388, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.00026494538178667426}, {"id": 695, "seek": 339596, "start": 3395.96, "end": 3396.96, "text": " this?", "tokens": [341, 30], "temperature": 0.0, "avg_logprob": -0.25147623558566995, "compression_ratio": 1.572289156626506, "no_speech_prob": 1.3630684406962246e-05}, {"id": 696, "seek": 339596, "start": 3396.96, "end": 3397.96, "text": " Answer that question.", "tokens": [24545, 300, 1168, 13], "temperature": 0.0, "avg_logprob": -0.25147623558566995, "compression_ratio": 1.572289156626506, "no_speech_prob": 1.3630684406962246e-05}, {"id": 697, "seek": 339596, "start": 3397.96, "end": 3408.8, "text": " Here is the answer to that question.", "tokens": [1692, 307, 264, 1867, 281, 300, 1168, 13], "temperature": 0.0, "avg_logprob": -0.25147623558566995, "compression_ratio": 1.572289156626506, "no_speech_prob": 1.3630684406962246e-05}, {"id": 698, "seek": 339596, "start": 3408.8, "end": 3415.0, "text": " The question was, it takes a long time to figure out the optimal learning rate.", "tokens": [440, 1168, 390, 11, 309, 2516, 257, 938, 565, 281, 2573, 484, 264, 16252, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.25147623558566995, "compression_ratio": 1.572289156626506, "no_speech_prob": 1.3630684406962246e-05}, {"id": 699, "seek": 339596, "start": 3415.0, "end": 3418.84, "text": " Can we calculate it using just a sample?", "tokens": [1664, 321, 8873, 309, 1228, 445, 257, 6889, 30], "temperature": 0.0, "avg_logprob": -0.25147623558566995, "compression_ratio": 1.572289156626506, "no_speech_prob": 1.3630684406962246e-05}, {"id": 700, "seek": 339596, "start": 3418.84, "end": 3425.48, "text": " And to answer that question, I'm going to show you how I entered Statefarm.", "tokens": [400, 281, 1867, 300, 1168, 11, 286, 478, 516, 281, 855, 291, 577, 286, 9065, 4533, 69, 4452, 13], "temperature": 0.0, "avg_logprob": -0.25147623558566995, "compression_ratio": 1.572289156626506, "no_speech_prob": 1.3630684406962246e-05}, {"id": 701, "seek": 342548, "start": 3425.48, "end": 3431.92, "text": " And indeed, when I started entering Statefarm, I started by using a sample.", "tokens": [400, 6451, 11, 562, 286, 1409, 11104, 4533, 69, 4452, 11, 286, 1409, 538, 1228, 257, 6889, 13], "temperature": 0.0, "avg_logprob": -0.18549310077320447, "compression_ratio": 1.6548223350253808, "no_speech_prob": 1.1300681762804743e-05}, {"id": 702, "seek": 342548, "start": 3431.92, "end": 3439.94, "text": " And so step 1 was to think, what insights can we gain from using a sample which can", "tokens": [400, 370, 1823, 502, 390, 281, 519, 11, 437, 14310, 393, 321, 6052, 490, 1228, 257, 6889, 597, 393], "temperature": 0.0, "avg_logprob": -0.18549310077320447, "compression_ratio": 1.6548223350253808, "no_speech_prob": 1.1300681762804743e-05}, {"id": 703, "seek": 342548, "start": 3439.94, "end": 3443.32, "text": " still apply when we move to the whole data set?", "tokens": [920, 3079, 562, 321, 1286, 281, 264, 1379, 1412, 992, 30], "temperature": 0.0, "avg_logprob": -0.18549310077320447, "compression_ratio": 1.6548223350253808, "no_speech_prob": 1.1300681762804743e-05}, {"id": 704, "seek": 342548, "start": 3443.32, "end": 3449.64, "text": " Running stuff in a sample took 10 or 20 seconds, running stuff in the full data set took 2", "tokens": [28136, 1507, 294, 257, 6889, 1890, 1266, 420, 945, 3949, 11, 2614, 1507, 294, 264, 1577, 1412, 992, 1890, 568], "temperature": 0.0, "avg_logprob": -0.18549310077320447, "compression_ratio": 1.6548223350253808, "no_speech_prob": 1.1300681762804743e-05}, {"id": 705, "seek": 342548, "start": 3449.64, "end": 3453.6, "text": " to 10 minutes for an epoch.", "tokens": [281, 1266, 2077, 337, 364, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.18549310077320447, "compression_ratio": 1.6548223350253808, "no_speech_prob": 1.1300681762804743e-05}, {"id": 706, "seek": 345360, "start": 3453.6, "end": 3463.3199999999997, "text": " So after I created my sample, which I just created randomly, I first of all wanted to", "tokens": [407, 934, 286, 2942, 452, 6889, 11, 597, 286, 445, 2942, 16979, 11, 286, 700, 295, 439, 1415, 281], "temperature": 0.0, "avg_logprob": -0.13595793463967062, "compression_ratio": 1.6341463414634145, "no_speech_prob": 4.860411081608618e-06}, {"id": 707, "seek": 345360, "start": 3463.3199999999997, "end": 3471.92, "text": " find out what does it take to create a better than random model here.", "tokens": [915, 484, 437, 775, 309, 747, 281, 1884, 257, 1101, 813, 4974, 2316, 510, 13], "temperature": 0.0, "avg_logprob": -0.13595793463967062, "compression_ratio": 1.6341463414634145, "no_speech_prob": 4.860411081608618e-06}, {"id": 708, "seek": 345360, "start": 3471.92, "end": 3475.64, "text": " So I always start with the simplest possible model.", "tokens": [407, 286, 1009, 722, 365, 264, 22811, 1944, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13595793463967062, "compression_ratio": 1.6341463414634145, "no_speech_prob": 4.860411081608618e-06}, {"id": 709, "seek": 345360, "start": 3475.64, "end": 3482.08, "text": " And so the simplest possible model has a single dense layer.", "tokens": [400, 370, 264, 22811, 1944, 2316, 575, 257, 2167, 18011, 4583, 13], "temperature": 0.0, "avg_logprob": -0.13595793463967062, "compression_ratio": 1.6341463414634145, "no_speech_prob": 4.860411081608618e-06}, {"id": 710, "seek": 348208, "start": 3482.08, "end": 3484.0, "text": " Now here's a handy trick.", "tokens": [823, 510, 311, 257, 13239, 4282, 13], "temperature": 0.0, "avg_logprob": -0.12817607206456802, "compression_ratio": 1.811764705882353, "no_speech_prob": 1.2029497156618163e-05}, {"id": 711, "seek": 348208, "start": 3484.0, "end": 3487.68, "text": " Rather than worrying about calculating the average and standard deviation of the input", "tokens": [16571, 813, 18788, 466, 28258, 264, 4274, 293, 3832, 25163, 295, 264, 4846], "temperature": 0.0, "avg_logprob": -0.12817607206456802, "compression_ratio": 1.811764705882353, "no_speech_prob": 1.2029497156618163e-05}, {"id": 712, "seek": 348208, "start": 3487.68, "end": 3492.84, "text": " and subtracting it all out in order to normalize your input layer, you can just start with", "tokens": [293, 16390, 278, 309, 439, 484, 294, 1668, 281, 2710, 1125, 428, 4846, 4583, 11, 291, 393, 445, 722, 365], "temperature": 0.0, "avg_logprob": -0.12817607206456802, "compression_ratio": 1.811764705882353, "no_speech_prob": 1.2029497156618163e-05}, {"id": 713, "seek": 348208, "start": 3492.84, "end": 3495.68, "text": " a batch norm layer.", "tokens": [257, 15245, 2026, 4583, 13], "temperature": 0.0, "avg_logprob": -0.12817607206456802, "compression_ratio": 1.811764705882353, "no_speech_prob": 1.2029497156618163e-05}, {"id": 714, "seek": 348208, "start": 3495.68, "end": 3498.72, "text": " And so if you start with a batch norm layer, it's going to do that for you.", "tokens": [400, 370, 498, 291, 722, 365, 257, 15245, 2026, 4583, 11, 309, 311, 516, 281, 360, 300, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.12817607206456802, "compression_ratio": 1.811764705882353, "no_speech_prob": 1.2029497156618163e-05}, {"id": 715, "seek": 348208, "start": 3498.72, "end": 3503.6, "text": " So anytime you create a Keras model from scratch, I would recommend making your first layer", "tokens": [407, 13038, 291, 1884, 257, 591, 6985, 2316, 490, 8459, 11, 286, 576, 2748, 1455, 428, 700, 4583], "temperature": 0.0, "avg_logprob": -0.12817607206456802, "compression_ratio": 1.811764705882353, "no_speech_prob": 1.2029497156618163e-05}, {"id": 716, "seek": 348208, "start": 3503.6, "end": 3505.04, "text": " a batch norm layer.", "tokens": [257, 15245, 2026, 4583, 13], "temperature": 0.0, "avg_logprob": -0.12817607206456802, "compression_ratio": 1.811764705882353, "no_speech_prob": 1.2029497156618163e-05}, {"id": 717, "seek": 348208, "start": 3505.04, "end": 3509.36, "text": " And so this is going to normalize the data for me.", "tokens": [400, 370, 341, 307, 516, 281, 2710, 1125, 264, 1412, 337, 385, 13], "temperature": 0.0, "avg_logprob": -0.12817607206456802, "compression_ratio": 1.811764705882353, "no_speech_prob": 1.2029497156618163e-05}, {"id": 718, "seek": 350936, "start": 3509.36, "end": 3512.84, "text": " So that's a cool little trick which I haven't actually seen anybody use elsewhere, but I", "tokens": [407, 300, 311, 257, 1627, 707, 4282, 597, 286, 2378, 380, 767, 1612, 4472, 764, 14517, 11, 457, 286], "temperature": 0.0, "avg_logprob": -0.1549370087772967, "compression_ratio": 1.526829268292683, "no_speech_prob": 9.972859515983146e-06}, {"id": 719, "seek": 350936, "start": 3512.84, "end": 3518.48, "text": " think it's a good default starting point all the time.", "tokens": [519, 309, 311, 257, 665, 7576, 2891, 935, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.1549370087772967, "compression_ratio": 1.526829268292683, "no_speech_prob": 9.972859515983146e-06}, {"id": 720, "seek": 350936, "start": 3518.48, "end": 3524.04, "text": " If I'm going to use a dense layer, then obviously I have to flatten everything into a single", "tokens": [759, 286, 478, 516, 281, 764, 257, 18011, 4583, 11, 550, 2745, 286, 362, 281, 24183, 1203, 666, 257, 2167], "temperature": 0.0, "avg_logprob": -0.1549370087772967, "compression_ratio": 1.526829268292683, "no_speech_prob": 9.972859515983146e-06}, {"id": 721, "seek": 350936, "start": 3524.04, "end": 3525.6400000000003, "text": " vector first.", "tokens": [8062, 700, 13], "temperature": 0.0, "avg_logprob": -0.1549370087772967, "compression_ratio": 1.526829268292683, "no_speech_prob": 9.972859515983146e-06}, {"id": 722, "seek": 350936, "start": 3525.6400000000003, "end": 3533.84, "text": " So this is really a most minimal model.", "tokens": [407, 341, 307, 534, 257, 881, 13206, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1549370087772967, "compression_ratio": 1.526829268292683, "no_speech_prob": 9.972859515983146e-06}, {"id": 723, "seek": 350936, "start": 3533.84, "end": 3535.1600000000003, "text": " So I tried fitting it.", "tokens": [407, 286, 3031, 15669, 309, 13], "temperature": 0.0, "avg_logprob": -0.1549370087772967, "compression_ratio": 1.526829268292683, "no_speech_prob": 9.972859515983146e-06}, {"id": 724, "seek": 353516, "start": 3535.16, "end": 3541.0, "text": " I compiled it, fit it, and nothing happened.", "tokens": [286, 36548, 309, 11, 3318, 309, 11, 293, 1825, 2011, 13], "temperature": 0.0, "avg_logprob": -0.1608971469807175, "compression_ratio": 1.6017316017316017, "no_speech_prob": 6.048867362551391e-06}, {"id": 725, "seek": 353516, "start": 3541.0, "end": 3546.08, "text": " Not only did nothing happen to my validation, but really nothing happened to my training.", "tokens": [1726, 787, 630, 1825, 1051, 281, 452, 24071, 11, 457, 534, 1825, 2011, 281, 452, 3097, 13], "temperature": 0.0, "avg_logprob": -0.1608971469807175, "compression_ratio": 1.6017316017316017, "no_speech_prob": 6.048867362551391e-06}, {"id": 726, "seek": 353516, "start": 3546.08, "end": 3552.7999999999997, "text": " It's only taking 7 seconds per epoch to find this out, so that's okay.", "tokens": [467, 311, 787, 1940, 1614, 3949, 680, 30992, 339, 281, 915, 341, 484, 11, 370, 300, 311, 1392, 13], "temperature": 0.0, "avg_logprob": -0.1608971469807175, "compression_ratio": 1.6017316017316017, "no_speech_prob": 6.048867362551391e-06}, {"id": 727, "seek": 353516, "start": 3552.7999999999997, "end": 3553.8399999999997, "text": " So what might be going on?", "tokens": [407, 437, 1062, 312, 516, 322, 30], "temperature": 0.0, "avg_logprob": -0.1608971469807175, "compression_ratio": 1.6017316017316017, "no_speech_prob": 6.048867362551391e-06}, {"id": 728, "seek": 353516, "start": 3553.8399999999997, "end": 3558.7999999999997, "text": " So I look at model.summary and I see that there's 1.5 million parameters.", "tokens": [407, 286, 574, 412, 2316, 13, 82, 40879, 822, 293, 286, 536, 300, 456, 311, 502, 13, 20, 2459, 9834, 13], "temperature": 0.0, "avg_logprob": -0.1608971469807175, "compression_ratio": 1.6017316017316017, "no_speech_prob": 6.048867362551391e-06}, {"id": 729, "seek": 353516, "start": 3558.7999999999997, "end": 3561.68, "text": " And that makes me think, okay, it's probably not under-fitting.", "tokens": [400, 300, 1669, 385, 519, 11, 1392, 11, 309, 311, 1391, 406, 833, 12, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.1608971469807175, "compression_ratio": 1.6017316017316017, "no_speech_prob": 6.048867362551391e-06}, {"id": 730, "seek": 356168, "start": 3561.68, "end": 3566.3199999999997, "text": " It's probably unlikely that with 1.5 million parameters, there's really nothing useful", "tokens": [467, 311, 1391, 17518, 300, 365, 502, 13, 20, 2459, 9834, 11, 456, 311, 534, 1825, 4420], "temperature": 0.0, "avg_logprob": -0.10582103179051326, "compression_ratio": 1.6065573770491803, "no_speech_prob": 4.092885319550987e-06}, {"id": 731, "seek": 356168, "start": 3566.3199999999997, "end": 3567.3199999999997, "text": " it can do whatsoever.", "tokens": [309, 393, 360, 17076, 13], "temperature": 0.0, "avg_logprob": -0.10582103179051326, "compression_ratio": 1.6065573770491803, "no_speech_prob": 4.092885319550987e-06}, {"id": 732, "seek": 356168, "start": 3567.3199999999997, "end": 3571.8799999999997, "text": " It's only a linear model, true, but I still think it should be able to do something.", "tokens": [467, 311, 787, 257, 8213, 2316, 11, 2074, 11, 457, 286, 920, 519, 309, 820, 312, 1075, 281, 360, 746, 13], "temperature": 0.0, "avg_logprob": -0.10582103179051326, "compression_ratio": 1.6065573770491803, "no_speech_prob": 4.092885319550987e-06}, {"id": 733, "seek": 356168, "start": 3571.8799999999997, "end": 3575.9199999999996, "text": " So that makes me think that what must be going on is it must be doing that thing where it", "tokens": [407, 300, 1669, 385, 519, 300, 437, 1633, 312, 516, 322, 307, 309, 1633, 312, 884, 300, 551, 689, 309], "temperature": 0.0, "avg_logprob": -0.10582103179051326, "compression_ratio": 1.6065573770491803, "no_speech_prob": 4.092885319550987e-06}, {"id": 734, "seek": 356168, "start": 3575.9199999999996, "end": 3578.8799999999997, "text": " jumps too far.", "tokens": [16704, 886, 1400, 13], "temperature": 0.0, "avg_logprob": -0.10582103179051326, "compression_ratio": 1.6065573770491803, "no_speech_prob": 4.092885319550987e-06}, {"id": 735, "seek": 356168, "start": 3578.8799999999997, "end": 3585.48, "text": " And it's particularly easy to jump too far at the very start of training.", "tokens": [400, 309, 311, 4098, 1858, 281, 3012, 886, 1400, 412, 264, 588, 722, 295, 3097, 13], "temperature": 0.0, "avg_logprob": -0.10582103179051326, "compression_ratio": 1.6065573770491803, "no_speech_prob": 4.092885319550987e-06}, {"id": 736, "seek": 356168, "start": 3585.48, "end": 3589.52, "text": " Let me explain why.", "tokens": [961, 385, 2903, 983, 13], "temperature": 0.0, "avg_logprob": -0.10582103179051326, "compression_ratio": 1.6065573770491803, "no_speech_prob": 4.092885319550987e-06}, {"id": 737, "seek": 358952, "start": 3589.52, "end": 3600.0, "text": " It turns out that there are often reasonably good answers that are way too easy to find.", "tokens": [467, 4523, 484, 300, 456, 366, 2049, 23551, 665, 6338, 300, 366, 636, 886, 1858, 281, 915, 13], "temperature": 0.0, "avg_logprob": -0.20715128933941876, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.553488573335926e-06}, {"id": 738, "seek": 358952, "start": 3600.0, "end": 3606.62, "text": " So one reasonably good answer would be always predict 0.", "tokens": [407, 472, 23551, 665, 1867, 576, 312, 1009, 6069, 1958, 13], "temperature": 0.0, "avg_logprob": -0.20715128933941876, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.553488573335926e-06}, {"id": 739, "seek": 358952, "start": 3606.62, "end": 3612.96, "text": " Because there are 10 output classes in the state farm competition, there's one of 10", "tokens": [1436, 456, 366, 1266, 5598, 5359, 294, 264, 1785, 5421, 6211, 11, 456, 311, 472, 295, 1266], "temperature": 0.0, "avg_logprob": -0.20715128933941876, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.553488573335926e-06}, {"id": 740, "seek": 361296, "start": 3612.96, "end": 3619.84, "text": " different types of distracted driving, and you are scored based on the cross-entropy", "tokens": [819, 3467, 295, 21658, 4840, 11, 293, 291, 366, 18139, 2361, 322, 264, 3278, 12, 317, 27514], "temperature": 0.0, "avg_logprob": -0.18658550749433803, "compression_ratio": 1.5394736842105263, "no_speech_prob": 9.080396921490319e-06}, {"id": 741, "seek": 361296, "start": 3619.84, "end": 3621.64, "text": " loss.", "tokens": [4470, 13], "temperature": 0.0, "avg_logprob": -0.18658550749433803, "compression_ratio": 1.5394736842105263, "no_speech_prob": 9.080396921490319e-06}, {"id": 742, "seek": 361296, "start": 3621.64, "end": 3626.04, "text": " And what that's looking at is how accurate are each of your 10 predictions.", "tokens": [400, 437, 300, 311, 1237, 412, 307, 577, 8559, 366, 1184, 295, 428, 1266, 21264, 13], "temperature": 0.0, "avg_logprob": -0.18658550749433803, "compression_ratio": 1.5394736842105263, "no_speech_prob": 9.080396921490319e-06}, {"id": 743, "seek": 361296, "start": 3626.04, "end": 3635.16, "text": " So rather than trying to predict something well, what if we just always predict 0.01.", "tokens": [407, 2831, 813, 1382, 281, 6069, 746, 731, 11, 437, 498, 321, 445, 1009, 6069, 1958, 13, 10607, 13], "temperature": 0.0, "avg_logprob": -0.18658550749433803, "compression_ratio": 1.5394736842105263, "no_speech_prob": 9.080396921490319e-06}, {"id": 744, "seek": 361296, "start": 3635.16, "end": 3637.5, "text": " Nine times out of 10, you're going to be right.", "tokens": [18939, 1413, 484, 295, 1266, 11, 291, 434, 516, 281, 312, 558, 13], "temperature": 0.0, "avg_logprob": -0.18658550749433803, "compression_ratio": 1.5394736842105263, "no_speech_prob": 9.080396921490319e-06}, {"id": 745, "seek": 361296, "start": 3637.5, "end": 3641.44, "text": " Because 9 out of the 10 categories, it's not that.", "tokens": [1436, 1722, 484, 295, 264, 1266, 10479, 11, 309, 311, 406, 300, 13], "temperature": 0.0, "avg_logprob": -0.18658550749433803, "compression_ratio": 1.5394736842105263, "no_speech_prob": 9.080396921490319e-06}, {"id": 746, "seek": 364144, "start": 3641.44, "end": 3643.2000000000003, "text": " It's only one of the 10 categories.", "tokens": [467, 311, 787, 472, 295, 264, 1266, 10479, 13], "temperature": 0.0, "avg_logprob": -0.1516385808721319, "compression_ratio": 1.6808510638297873, "no_speech_prob": 2.0261363715690095e-06}, {"id": 747, "seek": 364144, "start": 3643.2000000000003, "end": 3647.56, "text": " So actually always predicting 0.01 would be pretty good.", "tokens": [407, 767, 1009, 32884, 1958, 13, 10607, 576, 312, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.1516385808721319, "compression_ratio": 1.6808510638297873, "no_speech_prob": 2.0261363715690095e-06}, {"id": 748, "seek": 364144, "start": 3647.56, "end": 3651.48, "text": " Now it turns out it's not possible to do that because we have a softmax layer.", "tokens": [823, 309, 4523, 484, 309, 311, 406, 1944, 281, 360, 300, 570, 321, 362, 257, 2787, 41167, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1516385808721319, "compression_ratio": 1.6808510638297873, "no_speech_prob": 2.0261363715690095e-06}, {"id": 749, "seek": 364144, "start": 3651.48, "end": 3656.7200000000003, "text": " And a softmax layer, remember, is e to the xi divided by sum of e to the xi's.", "tokens": [400, 257, 2787, 41167, 4583, 11, 1604, 11, 307, 308, 281, 264, 36800, 6666, 538, 2408, 295, 308, 281, 264, 36800, 311, 13], "temperature": 0.0, "avg_logprob": -0.1516385808721319, "compression_ratio": 1.6808510638297873, "no_speech_prob": 2.0261363715690095e-06}, {"id": 750, "seek": 364144, "start": 3656.7200000000003, "end": 3662.3, "text": " And so in a softmax layer, everything has to add to 1.", "tokens": [400, 370, 294, 257, 2787, 41167, 4583, 11, 1203, 575, 281, 909, 281, 502, 13], "temperature": 0.0, "avg_logprob": -0.1516385808721319, "compression_ratio": 1.6808510638297873, "no_speech_prob": 2.0261363715690095e-06}, {"id": 751, "seek": 364144, "start": 3662.3, "end": 3668.52, "text": " So therefore, if it makes one of the classes really high and all of the other ones really", "tokens": [407, 4412, 11, 498, 309, 1669, 472, 295, 264, 5359, 534, 1090, 293, 439, 295, 264, 661, 2306, 534], "temperature": 0.0, "avg_logprob": -0.1516385808721319, "compression_ratio": 1.6808510638297873, "no_speech_prob": 2.0261363715690095e-06}, {"id": 752, "seek": 366852, "start": 3668.52, "end": 3675.04, "text": " low, then 9 times out of 10, it is going to be right 9 times out of 10.", "tokens": [2295, 11, 550, 1722, 1413, 484, 295, 1266, 11, 309, 307, 516, 281, 312, 558, 1722, 1413, 484, 295, 1266, 13], "temperature": 0.0, "avg_logprob": -0.13948527006345374, "compression_ratio": 1.6291666666666667, "no_speech_prob": 5.33812408320955e-06}, {"id": 753, "seek": 366852, "start": 3675.04, "end": 3681.92, "text": " So in other words, it's a pretty good answer for it to always predict some random class,", "tokens": [407, 294, 661, 2283, 11, 309, 311, 257, 1238, 665, 1867, 337, 309, 281, 1009, 6069, 512, 4974, 1508, 11], "temperature": 0.0, "avg_logprob": -0.13948527006345374, "compression_ratio": 1.6291666666666667, "no_speech_prob": 5.33812408320955e-06}, {"id": 754, "seek": 366852, "start": 3681.92, "end": 3686.48, "text": " class 8, close to 100% certainty.", "tokens": [1508, 1649, 11, 1998, 281, 2319, 4, 27022, 13], "temperature": 0.0, "avg_logprob": -0.13948527006345374, "compression_ratio": 1.6291666666666667, "no_speech_prob": 5.33812408320955e-06}, {"id": 755, "seek": 366852, "start": 3686.48, "end": 3687.48, "text": " And that's what happens.", "tokens": [400, 300, 311, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.13948527006345374, "compression_ratio": 1.6291666666666667, "no_speech_prob": 5.33812408320955e-06}, {"id": 756, "seek": 366852, "start": 3687.48, "end": 3691.32, "text": " So anybody who tried this, and I saw a lot of people on the forums this week saying,", "tokens": [407, 4472, 567, 3031, 341, 11, 293, 286, 1866, 257, 688, 295, 561, 322, 264, 26998, 341, 1243, 1566, 11], "temperature": 0.0, "avg_logprob": -0.13948527006345374, "compression_ratio": 1.6291666666666667, "no_speech_prob": 5.33812408320955e-06}, {"id": 757, "seek": 366852, "start": 3691.32, "end": 3697.4, "text": " I tried to train it and nothing happened, and the folks who got the really interesting", "tokens": [286, 3031, 281, 3847, 309, 293, 1825, 2011, 11, 293, 264, 4024, 567, 658, 264, 534, 1880], "temperature": 0.0, "avg_logprob": -0.13948527006345374, "compression_ratio": 1.6291666666666667, "no_speech_prob": 5.33812408320955e-06}, {"id": 758, "seek": 369740, "start": 3697.4, "end": 3701.84, "text": " insight were the ones who then went on to say, and then I looked at my predictions and", "tokens": [11269, 645, 264, 2306, 567, 550, 1437, 322, 281, 584, 11, 293, 550, 286, 2956, 412, 452, 21264, 293], "temperature": 0.0, "avg_logprob": -0.21088682115077972, "compression_ratio": 1.5679012345679013, "no_speech_prob": 2.2473577701020986e-05}, {"id": 759, "seek": 369740, "start": 3701.84, "end": 3708.08, "text": " it kept predicting the same class with great confidence again and again and again.", "tokens": [309, 4305, 32884, 264, 912, 1508, 365, 869, 6687, 797, 293, 797, 293, 797, 13], "temperature": 0.0, "avg_logprob": -0.21088682115077972, "compression_ratio": 1.5679012345679013, "no_speech_prob": 2.2473577701020986e-05}, {"id": 760, "seek": 369740, "start": 3708.08, "end": 3716.92, "text": " That's why I did that.", "tokens": [663, 311, 983, 286, 630, 300, 13], "temperature": 0.0, "avg_logprob": -0.21088682115077972, "compression_ratio": 1.5679012345679013, "no_speech_prob": 2.2473577701020986e-05}, {"id": 761, "seek": 369740, "start": 3716.92, "end": 3724.28, "text": " So our next step then is to try decreasing the learning rate.", "tokens": [407, 527, 958, 1823, 550, 307, 281, 853, 23223, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.21088682115077972, "compression_ratio": 1.5679012345679013, "no_speech_prob": 2.2473577701020986e-05}, {"id": 762, "seek": 372428, "start": 3724.28, "end": 3733.48, "text": " So here is exactly the same model, but I'm now using a much lower learning rate.", "tokens": [407, 510, 307, 2293, 264, 912, 2316, 11, 457, 286, 478, 586, 1228, 257, 709, 3126, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.12129041518288097, "compression_ratio": 1.5424528301886793, "no_speech_prob": 3.611955435189884e-06}, {"id": 763, "seek": 372428, "start": 3733.48, "end": 3738.42, "text": " And when I run that, it's actually moving.", "tokens": [400, 562, 286, 1190, 300, 11, 309, 311, 767, 2684, 13], "temperature": 0.0, "avg_logprob": -0.12129041518288097, "compression_ratio": 1.5424528301886793, "no_speech_prob": 3.611955435189884e-06}, {"id": 764, "seek": 372428, "start": 3738.42, "end": 3742.84, "text": " So it's only 12 seconds of compute time to figure out that I'm going to have to start", "tokens": [407, 309, 311, 787, 2272, 3949, 295, 14722, 565, 281, 2573, 484, 300, 286, 478, 516, 281, 362, 281, 722], "temperature": 0.0, "avg_logprob": -0.12129041518288097, "compression_ratio": 1.5424528301886793, "no_speech_prob": 3.611955435189884e-06}, {"id": 765, "seek": 372428, "start": 3742.84, "end": 3744.6400000000003, "text": " with a lower learning rate.", "tokens": [365, 257, 3126, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.12129041518288097, "compression_ratio": 1.5424528301886793, "no_speech_prob": 3.611955435189884e-06}, {"id": 766, "seek": 372428, "start": 3744.6400000000003, "end": 3752.86, "text": " Once we've got to a point where the accuracy is reasonably better than random, we're well", "tokens": [3443, 321, 600, 658, 281, 257, 935, 689, 264, 14170, 307, 23551, 1101, 813, 4974, 11, 321, 434, 731], "temperature": 0.0, "avg_logprob": -0.12129041518288097, "compression_ratio": 1.5424528301886793, "no_speech_prob": 3.611955435189884e-06}, {"id": 767, "seek": 375286, "start": 3752.86, "end": 3757.92, "text": " away from that part of the loss function now that says always predict everything as the", "tokens": [1314, 490, 300, 644, 295, 264, 4470, 2445, 586, 300, 1619, 1009, 6069, 1203, 382, 264], "temperature": 0.0, "avg_logprob": -0.16793911999995165, "compression_ratio": 1.764957264957265, "no_speech_prob": 1.733040789986262e-06}, {"id": 768, "seek": 375286, "start": 3757.92, "end": 3763.1400000000003, "text": " same class, and therefore we can now increase the learning rate back up again.", "tokens": [912, 1508, 11, 293, 4412, 321, 393, 586, 3488, 264, 2539, 3314, 646, 493, 797, 13], "temperature": 0.0, "avg_logprob": -0.16793911999995165, "compression_ratio": 1.764957264957265, "no_speech_prob": 1.733040789986262e-06}, {"id": 769, "seek": 375286, "start": 3763.1400000000003, "end": 3767.52, "text": " So generally speaking, for these harder problems, you'll need to start at an epoch or two at", "tokens": [407, 5101, 4124, 11, 337, 613, 6081, 2740, 11, 291, 603, 643, 281, 722, 412, 364, 30992, 339, 420, 732, 412], "temperature": 0.0, "avg_logprob": -0.16793911999995165, "compression_ratio": 1.764957264957265, "no_speech_prob": 1.733040789986262e-06}, {"id": 770, "seek": 375286, "start": 3767.52, "end": 3773.6800000000003, "text": " a lower learning rate, and then you can increase it back up again.", "tokens": [257, 3126, 2539, 3314, 11, 293, 550, 291, 393, 3488, 309, 646, 493, 797, 13], "temperature": 0.0, "avg_logprob": -0.16793911999995165, "compression_ratio": 1.764957264957265, "no_speech_prob": 1.733040789986262e-06}, {"id": 771, "seek": 375286, "start": 3773.6800000000003, "end": 3780.96, "text": " So you can see now I can put it back up to.01 and very quickly increasing my accuracy.", "tokens": [407, 291, 393, 536, 586, 286, 393, 829, 309, 646, 493, 281, 2411, 10607, 293, 588, 2661, 5662, 452, 14170, 13], "temperature": 0.0, "avg_logprob": -0.16793911999995165, "compression_ratio": 1.764957264957265, "no_speech_prob": 1.733040789986262e-06}, {"id": 772, "seek": 378096, "start": 3780.96, "end": 3787.76, "text": " So you can see here my accuracy on my validation set is.5 using a linear model.", "tokens": [407, 291, 393, 536, 510, 452, 14170, 322, 452, 24071, 992, 307, 2411, 20, 1228, 257, 8213, 2316, 13], "temperature": 0.0, "avg_logprob": -0.15371383229891458, "compression_ratio": 1.6607142857142858, "no_speech_prob": 2.1907687823841115e-06}, {"id": 773, "seek": 378096, "start": 3787.76, "end": 3792.44, "text": " And this is a good starting point because it kind of says to me, anytime that my validation", "tokens": [400, 341, 307, 257, 665, 2891, 935, 570, 309, 733, 295, 1619, 281, 385, 11, 13038, 300, 452, 24071], "temperature": 0.0, "avg_logprob": -0.15371383229891458, "compression_ratio": 1.6607142857142858, "no_speech_prob": 2.1907687823841115e-06}, {"id": 774, "seek": 378096, "start": 3792.44, "end": 3797.44, "text": " accuracy is worse than about.5, this is really no better than even a linear model, so this", "tokens": [14170, 307, 5324, 813, 466, 2411, 20, 11, 341, 307, 534, 572, 1101, 813, 754, 257, 8213, 2316, 11, 370, 341], "temperature": 0.0, "avg_logprob": -0.15371383229891458, "compression_ratio": 1.6607142857142858, "no_speech_prob": 2.1907687823841115e-06}, {"id": 775, "seek": 378096, "start": 3797.44, "end": 3801.32, "text": " is not worth spending more time on.", "tokens": [307, 406, 3163, 6434, 544, 565, 322, 13], "temperature": 0.0, "avg_logprob": -0.15371383229891458, "compression_ratio": 1.6607142857142858, "no_speech_prob": 2.1907687823841115e-06}, {"id": 776, "seek": 378096, "start": 3801.32, "end": 3806.0, "text": " One obvious question would be, how do you decide how big a sample to use?", "tokens": [1485, 6322, 1168, 576, 312, 11, 577, 360, 291, 4536, 577, 955, 257, 6889, 281, 764, 30], "temperature": 0.0, "avg_logprob": -0.15371383229891458, "compression_ratio": 1.6607142857142858, "no_speech_prob": 2.1907687823841115e-06}, {"id": 777, "seek": 380600, "start": 3806.0, "end": 3811.08, "text": " And what I did was I tried a few different sizes of sample for my validation set, and", "tokens": [400, 437, 286, 630, 390, 286, 3031, 257, 1326, 819, 11602, 295, 6889, 337, 452, 24071, 992, 11, 293], "temperature": 0.0, "avg_logprob": -0.19936061486965273, "compression_ratio": 1.5678391959798994, "no_speech_prob": 1.7603360902285203e-06}, {"id": 778, "seek": 380600, "start": 3811.08, "end": 3818.36, "text": " I then said, evaluate the model, so in other words, calculate loss function, on the validation", "tokens": [286, 550, 848, 11, 13059, 264, 2316, 11, 370, 294, 661, 2283, 11, 8873, 4470, 2445, 11, 322, 264, 24071], "temperature": 0.0, "avg_logprob": -0.19936061486965273, "compression_ratio": 1.5678391959798994, "no_speech_prob": 1.7603360902285203e-06}, {"id": 779, "seek": 380600, "start": 3818.36, "end": 3826.44, "text": " set, but for a whole bunch of randomly sampled batches, so do it 10 times.", "tokens": [992, 11, 457, 337, 257, 1379, 3840, 295, 16979, 3247, 15551, 15245, 279, 11, 370, 360, 309, 1266, 1413, 13], "temperature": 0.0, "avg_logprob": -0.19936061486965273, "compression_ratio": 1.5678391959798994, "no_speech_prob": 1.7603360902285203e-06}, {"id": 780, "seek": 380600, "start": 3826.44, "end": 3829.84, "text": " And so then I looked and I saw how the accuracy changed.", "tokens": [400, 370, 550, 286, 2956, 293, 286, 1866, 577, 264, 14170, 3105, 13], "temperature": 0.0, "avg_logprob": -0.19936061486965273, "compression_ratio": 1.5678391959798994, "no_speech_prob": 1.7603360902285203e-06}, {"id": 781, "seek": 382984, "start": 3829.84, "end": 3838.92, "text": " With a validation set set at 1000 images, my accuracy changed from.48 or.47 to.51,", "tokens": [2022, 257, 24071, 992, 992, 412, 9714, 5267, 11, 452, 14170, 3105, 490, 2411, 13318, 420, 2411, 14060, 281, 2411, 18682, 11], "temperature": 0.0, "avg_logprob": -0.20009382565816244, "compression_ratio": 1.3820224719101124, "no_speech_prob": 2.1907744667259976e-06}, {"id": 782, "seek": 382984, "start": 3838.92, "end": 3841.32, "text": " so it's not changing too much.", "tokens": [370, 309, 311, 406, 4473, 886, 709, 13], "temperature": 0.0, "avg_logprob": -0.20009382565816244, "compression_ratio": 1.3820224719101124, "no_speech_prob": 2.1907744667259976e-06}, {"id": 783, "seek": 382984, "start": 3841.32, "end": 3846.92, "text": " It's small enough that I think I can make useful insights using a sample size of this", "tokens": [467, 311, 1359, 1547, 300, 286, 519, 286, 393, 652, 4420, 14310, 1228, 257, 6889, 2744, 295, 341], "temperature": 0.0, "avg_logprob": -0.20009382565816244, "compression_ratio": 1.3820224719101124, "no_speech_prob": 2.1907744667259976e-06}, {"id": 784, "seek": 382984, "start": 3846.92, "end": 3850.04, "text": " size.", "tokens": [2744, 13], "temperature": 0.0, "avg_logprob": -0.20009382565816244, "compression_ratio": 1.3820224719101124, "no_speech_prob": 2.1907744667259976e-06}, {"id": 785, "seek": 382984, "start": 3850.04, "end": 3858.8, "text": " So what else can we learn from a sample?", "tokens": [407, 437, 1646, 393, 321, 1466, 490, 257, 6889, 30], "temperature": 0.0, "avg_logprob": -0.20009382565816244, "compression_ratio": 1.3820224719101124, "no_speech_prob": 2.1907744667259976e-06}, {"id": 786, "seek": 385880, "start": 3858.8, "end": 3862.2400000000002, "text": " Well one is, are there other architectures that work well?", "tokens": [1042, 472, 307, 11, 366, 456, 661, 6331, 1303, 300, 589, 731, 30], "temperature": 0.0, "avg_logprob": -0.13798346630362576, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.6187386790988967e-05}, {"id": 787, "seek": 385880, "start": 3862.2400000000002, "end": 3866.48, "text": " So the obvious thing to do with a computer vision problem is to try a convolutional neural", "tokens": [407, 264, 6322, 551, 281, 360, 365, 257, 3820, 5201, 1154, 307, 281, 853, 257, 45216, 304, 18161], "temperature": 0.0, "avg_logprob": -0.13798346630362576, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.6187386790988967e-05}, {"id": 788, "seek": 385880, "start": 3866.48, "end": 3868.92, "text": " network.", "tokens": [3209, 13], "temperature": 0.0, "avg_logprob": -0.13798346630362576, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.6187386790988967e-05}, {"id": 789, "seek": 385880, "start": 3868.92, "end": 3871.98, "text": " And here's one of the most simple convolutional neural networks.", "tokens": [400, 510, 311, 472, 295, 264, 881, 2199, 45216, 304, 18161, 9590, 13], "temperature": 0.0, "avg_logprob": -0.13798346630362576, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.6187386790988967e-05}, {"id": 790, "seek": 385880, "start": 3871.98, "end": 3880.2400000000002, "text": " Two convolutional layers, each one with a max pooling layer, and then one dense layer", "tokens": [4453, 45216, 304, 7914, 11, 1184, 472, 365, 257, 11469, 7005, 278, 4583, 11, 293, 550, 472, 18011, 4583], "temperature": 0.0, "avg_logprob": -0.13798346630362576, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.6187386790988967e-05}, {"id": 791, "seek": 385880, "start": 3880.2400000000002, "end": 3883.6800000000003, "text": " followed by my dense output layer.", "tokens": [6263, 538, 452, 18011, 5598, 4583, 13], "temperature": 0.0, "avg_logprob": -0.13798346630362576, "compression_ratio": 1.702970297029703, "no_speech_prob": 1.6187386790988967e-05}, {"id": 792, "seek": 388368, "start": 3883.68, "end": 3891.96, "text": " So again I tried that and found that it very quickly got to an accuracy of 100% on the", "tokens": [407, 797, 286, 3031, 300, 293, 1352, 300, 309, 588, 2661, 658, 281, 364, 14170, 295, 2319, 4, 322, 264], "temperature": 0.0, "avg_logprob": -0.1450259410417997, "compression_ratio": 1.7261410788381744, "no_speech_prob": 4.860402896156302e-06}, {"id": 793, "seek": 388368, "start": 3891.96, "end": 3896.24, "text": " training set, but only 24% on the validation set.", "tokens": [3097, 992, 11, 457, 787, 4022, 4, 322, 264, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.1450259410417997, "compression_ratio": 1.7261410788381744, "no_speech_prob": 4.860402896156302e-06}, {"id": 794, "seek": 388368, "start": 3896.24, "end": 3900.7999999999997, "text": " And that's because I was very careful to make sure my validation set included different", "tokens": [400, 300, 311, 570, 286, 390, 588, 5026, 281, 652, 988, 452, 24071, 992, 5556, 819], "temperature": 0.0, "avg_logprob": -0.1450259410417997, "compression_ratio": 1.7261410788381744, "no_speech_prob": 4.860402896156302e-06}, {"id": 795, "seek": 388368, "start": 3900.7999999999997, "end": 3907.52, "text": " drivers to my training set, because Oncago told us that the test set has different drivers.", "tokens": [11590, 281, 452, 3097, 992, 11, 570, 1282, 496, 1571, 1907, 505, 300, 264, 1500, 992, 575, 819, 11590, 13], "temperature": 0.0, "avg_logprob": -0.1450259410417997, "compression_ratio": 1.7261410788381744, "no_speech_prob": 4.860402896156302e-06}, {"id": 796, "seek": 388368, "start": 3907.52, "end": 3911.7599999999998, "text": " And so it's much harder to recognize what a driver is doing if we've never seen that", "tokens": [400, 370, 309, 311, 709, 6081, 281, 5521, 437, 257, 6787, 307, 884, 498, 321, 600, 1128, 1612, 300], "temperature": 0.0, "avg_logprob": -0.1450259410417997, "compression_ratio": 1.7261410788381744, "no_speech_prob": 4.860402896156302e-06}, {"id": 797, "seek": 388368, "start": 3911.7599999999998, "end": 3913.3799999999997, "text": " driver before.", "tokens": [6787, 949, 13], "temperature": 0.0, "avg_logprob": -0.1450259410417997, "compression_ratio": 1.7261410788381744, "no_speech_prob": 4.860402896156302e-06}, {"id": 798, "seek": 391338, "start": 3913.38, "end": 3919.62, "text": " So I could see that convolutional neural networks clearly are a great way to model this kind", "tokens": [407, 286, 727, 536, 300, 45216, 304, 18161, 9590, 4448, 366, 257, 869, 636, 281, 2316, 341, 733], "temperature": 0.0, "avg_logprob": -0.11079753875732422, "compression_ratio": 1.6933333333333334, "no_speech_prob": 1.5689343854319304e-05}, {"id": 799, "seek": 391338, "start": 3919.62, "end": 3924.2400000000002, "text": " of data, but I've got to think very carefully about overfitting.", "tokens": [295, 1412, 11, 457, 286, 600, 658, 281, 519, 588, 7500, 466, 670, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.11079753875732422, "compression_ratio": 1.6933333333333334, "no_speech_prob": 1.5689343854319304e-05}, {"id": 800, "seek": 391338, "start": 3924.2400000000002, "end": 3930.2000000000003, "text": " So step 1 to avoiding overfitting is data augmentation, as we learned in our data augmentation", "tokens": [407, 1823, 502, 281, 20220, 670, 69, 2414, 307, 1412, 14501, 19631, 11, 382, 321, 3264, 294, 527, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.11079753875732422, "compression_ratio": 1.6933333333333334, "no_speech_prob": 1.5689343854319304e-05}, {"id": 801, "seek": 391338, "start": 3930.2000000000003, "end": 3931.7200000000003, "text": " class.", "tokens": [1508, 13], "temperature": 0.0, "avg_logprob": -0.11079753875732422, "compression_ratio": 1.6933333333333334, "no_speech_prob": 1.5689343854319304e-05}, {"id": 802, "seek": 391338, "start": 3931.7200000000003, "end": 3939.0, "text": " So here's the exact same model, and I tried every type of data augmentation.", "tokens": [407, 510, 311, 264, 1900, 912, 2316, 11, 293, 286, 3031, 633, 2010, 295, 1412, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.11079753875732422, "compression_ratio": 1.6933333333333334, "no_speech_prob": 1.5689343854319304e-05}, {"id": 803, "seek": 391338, "start": 3939.0, "end": 3941.6800000000003, "text": " So I tried shifting it left and right a bit.", "tokens": [407, 286, 3031, 17573, 309, 1411, 293, 558, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.11079753875732422, "compression_ratio": 1.6933333333333334, "no_speech_prob": 1.5689343854319304e-05}, {"id": 804, "seek": 394168, "start": 3941.68, "end": 3944.3599999999997, "text": " I tried shifting it up and down a bit.", "tokens": [286, 3031, 17573, 309, 493, 293, 760, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.14283116658528647, "compression_ratio": 1.7582417582417582, "no_speech_prob": 1.1300648111500777e-05}, {"id": 805, "seek": 394168, "start": 3944.3599999999997, "end": 3947.08, "text": " I tried sharing it a bit.", "tokens": [286, 3031, 5414, 309, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.14283116658528647, "compression_ratio": 1.7582417582417582, "no_speech_prob": 1.1300648111500777e-05}, {"id": 806, "seek": 394168, "start": 3947.08, "end": 3950.9199999999996, "text": " I tried rotating it a bit.", "tokens": [286, 3031, 19627, 309, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.14283116658528647, "compression_ratio": 1.7582417582417582, "no_speech_prob": 1.1300648111500777e-05}, {"id": 807, "seek": 394168, "start": 3950.9199999999996, "end": 3955.64, "text": " I tried shifting the channels, the colors a bit.", "tokens": [286, 3031, 17573, 264, 9235, 11, 264, 4577, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.14283116658528647, "compression_ratio": 1.7582417582417582, "no_speech_prob": 1.1300648111500777e-05}, {"id": 808, "seek": 394168, "start": 3955.64, "end": 3960.3199999999997, "text": " And for each of those I tried 4 different levels, and I found in each case what was", "tokens": [400, 337, 1184, 295, 729, 286, 3031, 1017, 819, 4358, 11, 293, 286, 1352, 294, 1184, 1389, 437, 390], "temperature": 0.0, "avg_logprob": -0.14283116658528647, "compression_ratio": 1.7582417582417582, "no_speech_prob": 1.1300648111500777e-05}, {"id": 809, "seek": 394168, "start": 3960.3199999999997, "end": 3963.08, "text": " the best.", "tokens": [264, 1151, 13], "temperature": 0.0, "avg_logprob": -0.14283116658528647, "compression_ratio": 1.7582417582417582, "no_speech_prob": 1.1300648111500777e-05}, {"id": 810, "seek": 394168, "start": 3963.08, "end": 3966.24, "text": " And then I combined them all together.", "tokens": [400, 550, 286, 9354, 552, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.14283116658528647, "compression_ratio": 1.7582417582417582, "no_speech_prob": 1.1300648111500777e-05}, {"id": 811, "seek": 394168, "start": 3966.24, "end": 3971.24, "text": " So here are my best data augmentation amounts.", "tokens": [407, 510, 366, 452, 1151, 1412, 14501, 19631, 11663, 13], "temperature": 0.0, "avg_logprob": -0.14283116658528647, "compression_ratio": 1.7582417582417582, "no_speech_prob": 1.1300648111500777e-05}, {"id": 812, "seek": 397124, "start": 3971.24, "end": 3978.2, "text": " So on 1560 images, a very small set, this is just my sample, I then ran my very simple", "tokens": [407, 322, 2119, 4550, 5267, 11, 257, 588, 1359, 992, 11, 341, 307, 445, 452, 6889, 11, 286, 550, 5872, 452, 588, 2199], "temperature": 0.0, "avg_logprob": -0.1841310218528465, "compression_ratio": 1.5807692307692307, "no_speech_prob": 7.527912657678826e-06}, {"id": 813, "seek": 397124, "start": 3978.2, "end": 3985.56, "text": " two convolutional layer model with this data augmentation at these optimized parameters.", "tokens": [732, 45216, 304, 4583, 2316, 365, 341, 1412, 14501, 19631, 412, 613, 26941, 9834, 13], "temperature": 0.0, "avg_logprob": -0.1841310218528465, "compression_ratio": 1.5807692307692307, "no_speech_prob": 7.527912657678826e-06}, {"id": 814, "seek": 397124, "start": 3985.56, "end": 3987.3199999999997, "text": " And it didn't look very good.", "tokens": [400, 309, 994, 380, 574, 588, 665, 13], "temperature": 0.0, "avg_logprob": -0.1841310218528465, "compression_ratio": 1.5807692307692307, "no_speech_prob": 7.527912657678826e-06}, {"id": 815, "seek": 397124, "start": 3987.3199999999997, "end": 3992.2, "text": " After 5 epochs I only had 0.1 accuracy on my validation set.", "tokens": [2381, 1025, 30992, 28346, 286, 787, 632, 1958, 13, 16, 14170, 322, 452, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.1841310218528465, "compression_ratio": 1.5807692307692307, "no_speech_prob": 7.527912657678826e-06}, {"id": 816, "seek": 397124, "start": 3992.2, "end": 3996.2599999999998, "text": " But I can see that my training set is continuing to improve.", "tokens": [583, 286, 393, 536, 300, 452, 3097, 992, 307, 9289, 281, 3470, 13], "temperature": 0.0, "avg_logprob": -0.1841310218528465, "compression_ratio": 1.5807692307692307, "no_speech_prob": 7.527912657678826e-06}, {"id": 817, "seek": 397124, "start": 3996.2599999999998, "end": 4000.2, "text": " And so that makes me think, don't give up yet, try decreasing the learning rate and", "tokens": [400, 370, 300, 1669, 385, 519, 11, 500, 380, 976, 493, 1939, 11, 853, 23223, 264, 2539, 3314, 293], "temperature": 0.0, "avg_logprob": -0.1841310218528465, "compression_ratio": 1.5807692307692307, "no_speech_prob": 7.527912657678826e-06}, {"id": 818, "seek": 400020, "start": 4000.2, "end": 4001.68, "text": " do a few more.", "tokens": [360, 257, 1326, 544, 13], "temperature": 0.0, "avg_logprob": -0.19437599182128906, "compression_ratio": 1.49375, "no_speech_prob": 8.939601684687659e-06}, {"id": 819, "seek": 400020, "start": 4001.68, "end": 4004.2799999999997, "text": " And lo and behold, it started improving.", "tokens": [400, 450, 293, 27234, 11, 309, 1409, 11470, 13], "temperature": 0.0, "avg_logprob": -0.19437599182128906, "compression_ratio": 1.49375, "no_speech_prob": 8.939601684687659e-06}, {"id": 820, "seek": 400020, "start": 4004.2799999999997, "end": 4009.4399999999996, "text": " So this is where you've got to be careful not to jump to conclusions too soon.", "tokens": [407, 341, 307, 689, 291, 600, 658, 281, 312, 5026, 406, 281, 3012, 281, 22865, 886, 2321, 13], "temperature": 0.0, "avg_logprob": -0.19437599182128906, "compression_ratio": 1.49375, "no_speech_prob": 8.939601684687659e-06}, {"id": 821, "seek": 400020, "start": 4009.4399999999996, "end": 4014.7599999999998, "text": " So I ran a few more and it's improving well, so I ran a few more.", "tokens": [407, 286, 5872, 257, 1326, 544, 293, 309, 311, 11470, 731, 11, 370, 286, 5872, 257, 1326, 544, 13], "temperature": 0.0, "avg_logprob": -0.19437599182128906, "compression_ratio": 1.49375, "no_speech_prob": 8.939601684687659e-06}, {"id": 822, "seek": 400020, "start": 4014.7599999999998, "end": 4015.7599999999998, "text": " Another 25.", "tokens": [3996, 3552, 13], "temperature": 0.0, "avg_logprob": -0.19437599182128906, "compression_ratio": 1.49375, "no_speech_prob": 8.939601684687659e-06}, {"id": 823, "seek": 400020, "start": 4015.7599999999998, "end": 4017.8799999999997, "text": " And look at what happened.", "tokens": [400, 574, 412, 437, 2011, 13], "temperature": 0.0, "avg_logprob": -0.19437599182128906, "compression_ratio": 1.49375, "no_speech_prob": 8.939601684687659e-06}, {"id": 824, "seek": 401788, "start": 4017.88, "end": 4030.92, "text": " It kept getting better and better and better until we were getting 67% accuracy.", "tokens": [467, 4305, 1242, 1101, 293, 1101, 293, 1101, 1826, 321, 645, 1242, 23879, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.10407152483540197, "compression_ratio": 1.5, "no_speech_prob": 6.339141236821888e-06}, {"id": 825, "seek": 401788, "start": 4030.92, "end": 4039.6600000000003, "text": " So this 1.15 validation loss is well within the top 50% in this competition.", "tokens": [407, 341, 502, 13, 5211, 24071, 4470, 307, 731, 1951, 264, 1192, 2625, 4, 294, 341, 6211, 13], "temperature": 0.0, "avg_logprob": -0.10407152483540197, "compression_ratio": 1.5, "no_speech_prob": 6.339141236821888e-06}, {"id": 826, "seek": 401788, "start": 4039.6600000000003, "end": 4044.88, "text": " So using an incredibly simple model on just a sample, we can get in the top half of this", "tokens": [407, 1228, 364, 6252, 2199, 2316, 322, 445, 257, 6889, 11, 321, 393, 483, 294, 264, 1192, 1922, 295, 341], "temperature": 0.0, "avg_logprob": -0.10407152483540197, "compression_ratio": 1.5, "no_speech_prob": 6.339141236821888e-06}, {"id": 827, "seek": 404488, "start": 4044.88, "end": 4050.4, "text": " Kaggle competition simply by using the right kind of data augmentation.", "tokens": [48751, 22631, 6211, 2935, 538, 1228, 264, 558, 733, 295, 1412, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.33876920064290367, "compression_ratio": 1.4285714285714286, "no_speech_prob": 2.24733284994727e-05}, {"id": 828, "seek": 404488, "start": 4050.4, "end": 4055.56, "text": " So I think this is a really interesting insight about the power of this incredibly useful", "tokens": [407, 286, 519, 341, 307, 257, 534, 1880, 11269, 466, 264, 1347, 295, 341, 6252, 4420], "temperature": 0.0, "avg_logprob": -0.33876920064290367, "compression_ratio": 1.4285714285714286, "no_speech_prob": 2.24733284994727e-05}, {"id": 829, "seek": 404488, "start": 4055.56, "end": 4056.56, "text": " tool.", "tokens": [2290, 13], "temperature": 0.0, "avg_logprob": -0.33876920064290367, "compression_ratio": 1.4285714285714286, "no_speech_prob": 2.24733284994727e-05}, {"id": 830, "seek": 404488, "start": 4056.56, "end": 4058.56, "text": " Let's have a 5 minute break.", "tokens": [961, 311, 362, 257, 1025, 3456, 1821, 13], "temperature": 0.0, "avg_logprob": -0.33876920064290367, "compression_ratio": 1.4285714285714286, "no_speech_prob": 2.24733284994727e-05}, {"id": 831, "seek": 404488, "start": 4058.56, "end": 4062.2400000000002, "text": " And we'll do your question first.", "tokens": [400, 321, 603, 360, 428, 1168, 700, 13], "temperature": 0.0, "avg_logprob": -0.33876920064290367, "compression_ratio": 1.4285714285714286, "no_speech_prob": 2.24733284994727e-05}, {"id": 832, "seek": 404488, "start": 4062.2400000000002, "end": 4067.84, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.33876920064290367, "compression_ratio": 1.4285714285714286, "no_speech_prob": 2.24733284994727e-05}, {"id": 833, "seek": 404488, "start": 4067.84, "end": 4073.08, "text": " Would a class imbalance affect?", "tokens": [6068, 257, 1508, 43007, 3345, 30], "temperature": 0.0, "avg_logprob": -0.33876920064290367, "compression_ratio": 1.4285714285714286, "no_speech_prob": 2.24733284994727e-05}, {"id": 834, "seek": 407308, "start": 4073.08, "end": 4078.7999999999997, "text": " It's unlikely that there's going to be a class imbalance in my sample unless there was an", "tokens": [467, 311, 17518, 300, 456, 311, 516, 281, 312, 257, 1508, 43007, 294, 452, 6889, 5969, 456, 390, 364], "temperature": 0.0, "avg_logprob": -0.13709867155397093, "compression_ratio": 1.7722222222222221, "no_speech_prob": 1.723109198792372e-05}, {"id": 835, "seek": 407308, "start": 4078.7999999999997, "end": 4085.02, "text": " equivalent class imbalance in the real data, because I've got 1000 examples.", "tokens": [10344, 1508, 43007, 294, 264, 957, 1412, 11, 570, 286, 600, 658, 9714, 5110, 13], "temperature": 0.0, "avg_logprob": -0.13709867155397093, "compression_ratio": 1.7722222222222221, "no_speech_prob": 1.723109198792372e-05}, {"id": 836, "seek": 407308, "start": 4085.02, "end": 4087.84, "text": " And statistically speaking, that's unlikely.", "tokens": [400, 36478, 4124, 11, 300, 311, 17518, 13], "temperature": 0.0, "avg_logprob": -0.13709867155397093, "compression_ratio": 1.7722222222222221, "no_speech_prob": 1.723109198792372e-05}, {"id": 837, "seek": 407308, "start": 4087.84, "end": 4091.44, "text": " If there was a class imbalance in my original data, then I want my sample to have that class", "tokens": [759, 456, 390, 257, 1508, 43007, 294, 452, 3380, 1412, 11, 550, 286, 528, 452, 6889, 281, 362, 300, 1508], "temperature": 0.0, "avg_logprob": -0.13709867155397093, "compression_ratio": 1.7722222222222221, "no_speech_prob": 1.723109198792372e-05}, {"id": 838, "seek": 407308, "start": 4091.44, "end": 4095.0, "text": " imbalance too.", "tokens": [43007, 886, 13], "temperature": 0.0, "avg_logprob": -0.13709867155397093, "compression_ratio": 1.7722222222222221, "no_speech_prob": 1.723109198792372e-05}, {"id": 839, "seek": 409500, "start": 4095.0, "end": 4104.2, "text": " So at this point, I felt pretty good that I knew that we should be using a convolutional", "tokens": [407, 412, 341, 935, 11, 286, 2762, 1238, 665, 300, 286, 2586, 300, 321, 820, 312, 1228, 257, 45216, 304], "temperature": 0.0, "avg_logprob": -0.1630506639356737, "compression_ratio": 1.5940594059405941, "no_speech_prob": 7.766802809783258e-06}, {"id": 840, "seek": 409500, "start": 4104.2, "end": 4109.96, "text": " neural network, which is obviously a very strong hypothesis to start with anyway.", "tokens": [18161, 3209, 11, 597, 307, 2745, 257, 588, 2068, 17291, 281, 722, 365, 4033, 13], "temperature": 0.0, "avg_logprob": -0.1630506639356737, "compression_ratio": 1.5940594059405941, "no_speech_prob": 7.766802809783258e-06}, {"id": 841, "seek": 409500, "start": 4109.96, "end": 4116.76, "text": " And also felt pretty confident when you knew what kind of learning rate to start with and", "tokens": [400, 611, 2762, 1238, 6679, 562, 291, 2586, 437, 733, 295, 2539, 3314, 281, 722, 365, 293], "temperature": 0.0, "avg_logprob": -0.1630506639356737, "compression_ratio": 1.5940594059405941, "no_speech_prob": 7.766802809783258e-06}, {"id": 842, "seek": 409500, "start": 4116.76, "end": 4124.28, "text": " then how to change it, and also what data augmentation to do.", "tokens": [550, 577, 281, 1319, 309, 11, 293, 611, 437, 1412, 14501, 19631, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1630506639356737, "compression_ratio": 1.5940594059405941, "no_speech_prob": 7.766802809783258e-06}, {"id": 843, "seek": 412428, "start": 4124.28, "end": 4129.5199999999995, "text": " The next thing I wanted to wonder about was how else do I handle overfitting, because", "tokens": [440, 958, 551, 286, 1415, 281, 2441, 466, 390, 577, 1646, 360, 286, 4813, 670, 69, 2414, 11, 570], "temperature": 0.0, "avg_logprob": -0.1585782309558904, "compression_ratio": 1.5836909871244635, "no_speech_prob": 2.1444435333251022e-05}, {"id": 844, "seek": 412428, "start": 4129.5199999999995, "end": 4137.599999999999, "text": " although I'm getting some pretty good results, I'm still overfitting hugely,.6 vs..9.", "tokens": [4878, 286, 478, 1242, 512, 1238, 665, 3542, 11, 286, 478, 920, 670, 69, 2414, 27417, 11, 2411, 21, 12041, 13, 2411, 24, 13], "temperature": 0.0, "avg_logprob": -0.1585782309558904, "compression_ratio": 1.5836909871244635, "no_speech_prob": 2.1444435333251022e-05}, {"id": 845, "seek": 412428, "start": 4137.599999999999, "end": 4143.08, "text": " So the next thing in our list of ways to avoid overfitting, and I hope you guys all remember", "tokens": [407, 264, 958, 551, 294, 527, 1329, 295, 2098, 281, 5042, 670, 69, 2414, 11, 293, 286, 1454, 291, 1074, 439, 1604], "temperature": 0.0, "avg_logprob": -0.1585782309558904, "compression_ratio": 1.5836909871244635, "no_speech_prob": 2.1444435333251022e-05}, {"id": 846, "seek": 412428, "start": 4143.08, "end": 4146.679999999999, "text": " that we have that list in Lesson 3.", "tokens": [300, 321, 362, 300, 1329, 294, 18649, 266, 805, 13], "temperature": 0.0, "avg_logprob": -0.1585782309558904, "compression_ratio": 1.5836909871244635, "no_speech_prob": 2.1444435333251022e-05}, {"id": 847, "seek": 412428, "start": 4146.679999999999, "end": 4148.84, "text": " The 5 steps.", "tokens": [440, 1025, 4439, 13], "temperature": 0.0, "avg_logprob": -0.1585782309558904, "compression_ratio": 1.5836909871244635, "no_speech_prob": 2.1444435333251022e-05}, {"id": 848, "seek": 412428, "start": 4148.84, "end": 4154.04, "text": " Let's go and have a look at it now to remind ourselves.", "tokens": [961, 311, 352, 293, 362, 257, 574, 412, 309, 586, 281, 4160, 4175, 13], "temperature": 0.0, "avg_logprob": -0.1585782309558904, "compression_ratio": 1.5836909871244635, "no_speech_prob": 2.1444435333251022e-05}, {"id": 849, "seek": 415404, "start": 4154.04, "end": 4155.04, "text": " This is to reducing overfitting.", "tokens": [639, 307, 281, 12245, 670, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.3179744586609958, "compression_ratio": 1.6408163265306122, "no_speech_prob": 3.426713010412641e-05}, {"id": 850, "seek": 415404, "start": 4155.04, "end": 4156.04, "text": " These are the 5 steps.", "tokens": [1981, 366, 264, 1025, 4439, 13], "temperature": 0.0, "avg_logprob": -0.3179744586609958, "compression_ratio": 1.6408163265306122, "no_speech_prob": 3.426713010412641e-05}, {"id": 851, "seek": 415404, "start": 4156.04, "end": 4160.2, "text": " We can't add more data.", "tokens": [492, 393, 380, 909, 544, 1412, 13], "temperature": 0.0, "avg_logprob": -0.3179744586609958, "compression_ratio": 1.6408163265306122, "no_speech_prob": 3.426713010412641e-05}, {"id": 852, "seek": 415404, "start": 4160.2, "end": 4162.88, "text": " We've tried using data augmentation.", "tokens": [492, 600, 3031, 1228, 1412, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.3179744586609958, "compression_ratio": 1.6408163265306122, "no_speech_prob": 3.426713010412641e-05}, {"id": 853, "seek": 415404, "start": 4162.88, "end": 4165.84, "text": " We're already using batch norm and conv nets.", "tokens": [492, 434, 1217, 1228, 15245, 2026, 293, 3754, 36170, 13], "temperature": 0.0, "avg_logprob": -0.3179744586609958, "compression_ratio": 1.6408163265306122, "no_speech_prob": 3.426713010412641e-05}, {"id": 854, "seek": 415404, "start": 4165.84, "end": 4168.64, "text": " So the next step is to add regularization.", "tokens": [407, 264, 958, 1823, 307, 281, 909, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.3179744586609958, "compression_ratio": 1.6408163265306122, "no_speech_prob": 3.426713010412641e-05}, {"id": 855, "seek": 415404, "start": 4168.64, "end": 4172.2, "text": " And dropout is our kind of favored regularization technique.", "tokens": [400, 3270, 346, 307, 527, 733, 295, 44420, 3890, 2144, 6532, 13], "temperature": 0.0, "avg_logprob": -0.3179744586609958, "compression_ratio": 1.6408163265306122, "no_speech_prob": 3.426713010412641e-05}, {"id": 856, "seek": 415404, "start": 4172.2, "end": 4174.68, "text": " So I was thinking, okay, can we...", "tokens": [407, 286, 390, 1953, 11, 1392, 11, 393, 321, 1097], "temperature": 0.0, "avg_logprob": -0.3179744586609958, "compression_ratio": 1.6408163265306122, "no_speech_prob": 3.426713010412641e-05}, {"id": 857, "seek": 415404, "start": 4174.68, "end": 4179.5199999999995, "text": " Actually, before we do that, I'll just mention one more thing about this data augmentation", "tokens": [5135, 11, 949, 321, 360, 300, 11, 286, 603, 445, 2152, 472, 544, 551, 466, 341, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.3179744586609958, "compression_ratio": 1.6408163265306122, "no_speech_prob": 3.426713010412641e-05}, {"id": 858, "seek": 415404, "start": 4179.5199999999995, "end": 4182.24, "text": " approach.", "tokens": [3109, 13], "temperature": 0.0, "avg_logprob": -0.3179744586609958, "compression_ratio": 1.6408163265306122, "no_speech_prob": 3.426713010412641e-05}, {"id": 859, "seek": 418224, "start": 4182.24, "end": 4191.04, "text": " I have literally never seen anybody write down a process as to how to figure out what kind", "tokens": [286, 362, 3736, 1128, 1612, 4472, 2464, 760, 257, 1399, 382, 281, 577, 281, 2573, 484, 437, 733], "temperature": 0.0, "avg_logprob": -0.10402693929551524, "compression_ratio": 1.5458937198067633, "no_speech_prob": 3.785293756664032e-06}, {"id": 860, "seek": 418224, "start": 4191.04, "end": 4193.96, "text": " of data augmentation to use and the amount.", "tokens": [295, 1412, 14501, 19631, 281, 764, 293, 264, 2372, 13], "temperature": 0.0, "avg_logprob": -0.10402693929551524, "compression_ratio": 1.5458937198067633, "no_speech_prob": 3.785293756664032e-06}, {"id": 861, "seek": 418224, "start": 4193.96, "end": 4200.719999999999, "text": " The only posts I've seen on it always rely on intuition, which is basically like look", "tokens": [440, 787, 12300, 286, 600, 1612, 322, 309, 1009, 10687, 322, 24002, 11, 597, 307, 1936, 411, 574], "temperature": 0.0, "avg_logprob": -0.10402693929551524, "compression_ratio": 1.5458937198067633, "no_speech_prob": 3.785293756664032e-06}, {"id": 862, "seek": 418224, "start": 4200.719999999999, "end": 4204.28, "text": " at the images and think about how much they seem like they should be able to move around", "tokens": [412, 264, 5267, 293, 519, 466, 577, 709, 436, 1643, 411, 436, 820, 312, 1075, 281, 1286, 926], "temperature": 0.0, "avg_logprob": -0.10402693929551524, "compression_ratio": 1.5458937198067633, "no_speech_prob": 3.785293756664032e-06}, {"id": 863, "seek": 418224, "start": 4204.28, "end": 4206.28, "text": " or rotate.", "tokens": [420, 13121, 13], "temperature": 0.0, "avg_logprob": -0.10402693929551524, "compression_ratio": 1.5458937198067633, "no_speech_prob": 3.785293756664032e-06}, {"id": 864, "seek": 420628, "start": 4206.28, "end": 4213.719999999999, "text": " I really tried this week to come up with a rigorous repeatable process that you could", "tokens": [286, 534, 3031, 341, 1243, 281, 808, 493, 365, 257, 29882, 7149, 712, 1399, 300, 291, 727], "temperature": 0.0, "avg_logprob": -0.13871916135152182, "compression_ratio": 1.5240641711229947, "no_speech_prob": 3.2887037377804518e-06}, {"id": 865, "seek": 420628, "start": 4213.719999999999, "end": 4215.599999999999, "text": " use.", "tokens": [764, 13], "temperature": 0.0, "avg_logprob": -0.13871916135152182, "compression_ratio": 1.5240641711229947, "no_speech_prob": 3.2887037377804518e-06}, {"id": 866, "seek": 420628, "start": 4215.599999999999, "end": 4221.44, "text": " And that process is go through each data augmentation type one at a time, try 3 or 4 different levels", "tokens": [400, 300, 1399, 307, 352, 807, 1184, 1412, 14501, 19631, 2010, 472, 412, 257, 565, 11, 853, 805, 420, 1017, 819, 4358], "temperature": 0.0, "avg_logprob": -0.13871916135152182, "compression_ratio": 1.5240641711229947, "no_speech_prob": 3.2887037377804518e-06}, {"id": 867, "seek": 420628, "start": 4221.44, "end": 4229.599999999999, "text": " of it on a sample with a big enough validation set that it's pretty stable, to find the best", "tokens": [295, 309, 322, 257, 6889, 365, 257, 955, 1547, 24071, 992, 300, 309, 311, 1238, 8351, 11, 281, 915, 264, 1151], "temperature": 0.0, "avg_logprob": -0.13871916135152182, "compression_ratio": 1.5240641711229947, "no_speech_prob": 3.2887037377804518e-06}, {"id": 868, "seek": 422960, "start": 4229.6, "end": 4240.68, "text": " value of each of the data augmentation parameters, and then try combining them all together.", "tokens": [2158, 295, 1184, 295, 264, 1412, 14501, 19631, 9834, 11, 293, 550, 853, 21928, 552, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.14982273999382467, "compression_ratio": 1.4761904761904763, "no_speech_prob": 3.6688006730400957e-06}, {"id": 869, "seek": 422960, "start": 4240.68, "end": 4249.96, "text": " So I hope you kind of come away with this as a practical message, which probably your", "tokens": [407, 286, 1454, 291, 733, 295, 808, 1314, 365, 341, 382, 257, 8496, 3636, 11, 597, 1391, 428], "temperature": 0.0, "avg_logprob": -0.14982273999382467, "compression_ratio": 1.4761904761904763, "no_speech_prob": 3.6688006730400957e-06}, {"id": 870, "seek": 422960, "start": 4249.96, "end": 4253.56, "text": " colleagues, even if some of them claim to be deep learning experts, I doubt that they're", "tokens": [7734, 11, 754, 498, 512, 295, 552, 3932, 281, 312, 2452, 2539, 8572, 11, 286, 6385, 300, 436, 434], "temperature": 0.0, "avg_logprob": -0.14982273999382467, "compression_ratio": 1.4761904761904763, "no_speech_prob": 3.6688006730400957e-06}, {"id": 871, "seek": 422960, "start": 4253.56, "end": 4254.84, "text": " doing this.", "tokens": [884, 341, 13], "temperature": 0.0, "avg_logprob": -0.14982273999382467, "compression_ratio": 1.4761904761904763, "no_speech_prob": 3.6688006730400957e-06}, {"id": 872, "seek": 425484, "start": 4254.84, "end": 4260.8, "text": " So this is something you can hopefully get people into the practice of doing.", "tokens": [407, 341, 307, 746, 291, 393, 4696, 483, 561, 666, 264, 3124, 295, 884, 13], "temperature": 0.0, "avg_logprob": -0.14762826161841824, "compression_ratio": 1.6647058823529413, "no_speech_prob": 3.844892489723861e-06}, {"id": 873, "seek": 425484, "start": 4260.8, "end": 4264.64, "text": " Regularization however, we cannot do on a sample.", "tokens": [45659, 2144, 4461, 11, 321, 2644, 360, 322, 257, 6889, 13], "temperature": 0.0, "avg_logprob": -0.14762826161841824, "compression_ratio": 1.6647058823529413, "no_speech_prob": 3.844892489723861e-06}, {"id": 874, "seek": 425484, "start": 4264.64, "end": 4275.2, "text": " And the reason why is that step one, add more data, that step is very correlated with add", "tokens": [400, 264, 1778, 983, 307, 300, 1823, 472, 11, 909, 544, 1412, 11, 300, 1823, 307, 588, 38574, 365, 909], "temperature": 0.0, "avg_logprob": -0.14762826161841824, "compression_ratio": 1.6647058823529413, "no_speech_prob": 3.844892489723861e-06}, {"id": 875, "seek": 425484, "start": 4275.2, "end": 4276.2, "text": " regularization.", "tokens": [3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.14762826161841824, "compression_ratio": 1.6647058823529413, "no_speech_prob": 3.844892489723861e-06}, {"id": 876, "seek": 425484, "start": 4276.2, "end": 4280.06, "text": " As we add more data, we need less regularization.", "tokens": [1018, 321, 909, 544, 1412, 11, 321, 643, 1570, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.14762826161841824, "compression_ratio": 1.6647058823529413, "no_speech_prob": 3.844892489723861e-06}, {"id": 877, "seek": 428006, "start": 4280.06, "end": 4285.9800000000005, "text": " So as we move from a sample to the full data set, we're going to need less regularization.", "tokens": [407, 382, 321, 1286, 490, 257, 6889, 281, 264, 1577, 1412, 992, 11, 321, 434, 516, 281, 643, 1570, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.09265122121694136, "compression_ratio": 1.7766990291262137, "no_speech_prob": 2.2252663711697096e-06}, {"id": 878, "seek": 428006, "start": 4285.9800000000005, "end": 4293.1, "text": " So to figure out how much regularization to use, we have to use the whole data set.", "tokens": [407, 281, 2573, 484, 577, 709, 3890, 2144, 281, 764, 11, 321, 362, 281, 764, 264, 1379, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.09265122121694136, "compression_ratio": 1.7766990291262137, "no_speech_prob": 2.2252663711697096e-06}, {"id": 879, "seek": 428006, "start": 4293.1, "end": 4298.400000000001, "text": " So at this point I changed it to use the whole data set, not the sample, and I started using", "tokens": [407, 412, 341, 935, 286, 3105, 309, 281, 764, 264, 1379, 1412, 992, 11, 406, 264, 6889, 11, 293, 286, 1409, 1228], "temperature": 0.0, "avg_logprob": -0.09265122121694136, "compression_ratio": 1.7766990291262137, "no_speech_prob": 2.2252663711697096e-06}, {"id": 880, "seek": 428006, "start": 4298.400000000001, "end": 4299.400000000001, "text": " dropout.", "tokens": [3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.09265122121694136, "compression_ratio": 1.7766990291262137, "no_speech_prob": 2.2252663711697096e-06}, {"id": 881, "seek": 428006, "start": 4299.400000000001, "end": 4305.96, "text": " So you can see that I started with my data augmentation amounts that you've already seen,", "tokens": [407, 291, 393, 536, 300, 286, 1409, 365, 452, 1412, 14501, 19631, 11663, 300, 291, 600, 1217, 1612, 11], "temperature": 0.0, "avg_logprob": -0.09265122121694136, "compression_ratio": 1.7766990291262137, "no_speech_prob": 2.2252663711697096e-06}, {"id": 882, "seek": 430596, "start": 4305.96, "end": 4313.52, "text": " and I started adding in some dropout and ran it for a few epochs to see what would happen.", "tokens": [293, 286, 1409, 5127, 294, 512, 3270, 346, 293, 5872, 309, 337, 257, 1326, 30992, 28346, 281, 536, 437, 576, 1051, 13], "temperature": 0.0, "avg_logprob": -0.15764663769648626, "compression_ratio": 1.562753036437247, "no_speech_prob": 7.889156222518068e-06}, {"id": 883, "seek": 430596, "start": 4313.52, "end": 4317.28, "text": " And you can see it's worked pretty well.", "tokens": [400, 291, 393, 536, 309, 311, 2732, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.15764663769648626, "compression_ratio": 1.562753036437247, "no_speech_prob": 7.889156222518068e-06}, {"id": 884, "seek": 430596, "start": 4317.28, "end": 4322.4, "text": " So we're getting up into the 75% now and before we were in the 64%.", "tokens": [407, 321, 434, 1242, 493, 666, 264, 9562, 4, 586, 293, 949, 321, 645, 294, 264, 12145, 6856], "temperature": 0.0, "avg_logprob": -0.15764663769648626, "compression_ratio": 1.562753036437247, "no_speech_prob": 7.889156222518068e-06}, {"id": 885, "seek": 430596, "start": 4322.4, "end": 4328.26, "text": " So I haven't checked, once we add clipping, which is very important for getting the best", "tokens": [407, 286, 2378, 380, 10033, 11, 1564, 321, 909, 49320, 11, 597, 307, 588, 1021, 337, 1242, 264, 1151], "temperature": 0.0, "avg_logprob": -0.15764663769648626, "compression_ratio": 1.562753036437247, "no_speech_prob": 7.889156222518068e-06}, {"id": 886, "seek": 430596, "start": 4328.26, "end": 4335.4800000000005, "text": " cross-entropy loss function, I haven't checked where that would get us on the Kaggle leaderboard,", "tokens": [3278, 12, 317, 27514, 4470, 2445, 11, 286, 2378, 380, 10033, 689, 300, 576, 483, 505, 322, 264, 48751, 22631, 5263, 3787, 11], "temperature": 0.0, "avg_logprob": -0.15764663769648626, "compression_ratio": 1.562753036437247, "no_speech_prob": 7.889156222518068e-06}, {"id": 887, "seek": 433548, "start": 4335.48, "end": 4346.4, "text": " but I'm pretty sure it would be at least in the top third based on this accuracy.", "tokens": [457, 286, 478, 1238, 988, 309, 576, 312, 412, 1935, 294, 264, 1192, 2636, 2361, 322, 341, 14170, 13], "temperature": 0.0, "avg_logprob": -0.21197273856715151, "compression_ratio": 1.5307262569832403, "no_speech_prob": 1.2606800737557933e-05}, {"id": 888, "seek": 433548, "start": 4346.4, "end": 4356.12, "text": " I ran a few more epochs with an even lower learning rate and got 0.79.", "tokens": [286, 5872, 257, 1326, 544, 30992, 28346, 365, 364, 754, 3126, 2539, 3314, 293, 658, 1958, 13, 32042, 13], "temperature": 0.0, "avg_logprob": -0.21197273856715151, "compression_ratio": 1.5307262569832403, "no_speech_prob": 1.2606800737557933e-05}, {"id": 889, "seek": 433548, "start": 4356.12, "end": 4359.719999999999, "text": " So this is going to be well up into the top third, maybe even the top quarter, probably", "tokens": [407, 341, 307, 516, 281, 312, 731, 493, 666, 264, 1192, 2636, 11, 1310, 754, 264, 1192, 6555, 11, 1391], "temperature": 0.0, "avg_logprob": -0.21197273856715151, "compression_ratio": 1.5307262569832403, "no_speech_prob": 1.2606800737557933e-05}, {"id": 890, "seek": 433548, "start": 4359.719999999999, "end": 4363.4, "text": " the top third of the leaderboard.", "tokens": [264, 1192, 2636, 295, 264, 5263, 3787, 13], "temperature": 0.0, "avg_logprob": -0.21197273856715151, "compression_ratio": 1.5307262569832403, "no_speech_prob": 1.2606800737557933e-05}, {"id": 891, "seek": 436340, "start": 4363.4, "end": 4369.599999999999, "text": " I got to this point by just trying out a couple of different levels of dropout, and I just", "tokens": [286, 658, 281, 341, 935, 538, 445, 1382, 484, 257, 1916, 295, 819, 4358, 295, 3270, 346, 11, 293, 286, 445], "temperature": 0.0, "avg_logprob": -0.14740841605446556, "compression_ratio": 1.6355932203389831, "no_speech_prob": 1.497083758295048e-05}, {"id": 892, "seek": 436340, "start": 4369.599999999999, "end": 4371.96, "text": " put them in my dense layers.", "tokens": [829, 552, 294, 452, 18011, 7914, 13], "temperature": 0.0, "avg_logprob": -0.14740841605446556, "compression_ratio": 1.6355932203389831, "no_speech_prob": 1.497083758295048e-05}, {"id": 893, "seek": 436340, "start": 4371.96, "end": 4374.28, "text": " There's no rule of thumb here.", "tokens": [821, 311, 572, 4978, 295, 9298, 510, 13], "temperature": 0.0, "avg_logprob": -0.14740841605446556, "compression_ratio": 1.6355932203389831, "no_speech_prob": 1.497083758295048e-05}, {"id": 894, "seek": 436340, "start": 4374.28, "end": 4379.679999999999, "text": " A lot of people put small amounts of dropout in their convolutional layers as well.", "tokens": [316, 688, 295, 561, 829, 1359, 11663, 295, 3270, 346, 294, 641, 45216, 304, 7914, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.14740841605446556, "compression_ratio": 1.6355932203389831, "no_speech_prob": 1.497083758295048e-05}, {"id": 895, "seek": 436340, "start": 4379.679999999999, "end": 4383.24, "text": " All I can say is to try things.", "tokens": [1057, 286, 393, 584, 307, 281, 853, 721, 13], "temperature": 0.0, "avg_logprob": -0.14740841605446556, "compression_ratio": 1.6355932203389831, "no_speech_prob": 1.497083758295048e-05}, {"id": 896, "seek": 436340, "start": 4383.24, "end": 4389.74, "text": " But what VGG does is to put 50% dropout after each of its dense layers, and that doesn't", "tokens": [583, 437, 691, 27561, 775, 307, 281, 829, 2625, 4, 3270, 346, 934, 1184, 295, 1080, 18011, 7914, 11, 293, 300, 1177, 380], "temperature": 0.0, "avg_logprob": -0.14740841605446556, "compression_ratio": 1.6355932203389831, "no_speech_prob": 1.497083758295048e-05}, {"id": 897, "seek": 436340, "start": 4389.74, "end": 4390.92, "text": " seem like a bad rule of thumb.", "tokens": [1643, 411, 257, 1578, 4978, 295, 9298, 13], "temperature": 0.0, "avg_logprob": -0.14740841605446556, "compression_ratio": 1.6355932203389831, "no_speech_prob": 1.497083758295048e-05}, {"id": 898, "seek": 439092, "start": 4390.92, "end": 4395.12, "text": " So that's what I was doing here, and then trying around a few different sizes of dense", "tokens": [407, 300, 311, 437, 286, 390, 884, 510, 11, 293, 550, 1382, 926, 257, 1326, 819, 11602, 295, 18011], "temperature": 0.0, "avg_logprob": -0.15890083965073284, "compression_ratio": 1.6, "no_speech_prob": 9.721495644043898e-07}, {"id": 899, "seek": 439092, "start": 4395.12, "end": 4397.2, "text": " layers to try and find something reasonable.", "tokens": [7914, 281, 853, 293, 915, 746, 10585, 13], "temperature": 0.0, "avg_logprob": -0.15890083965073284, "compression_ratio": 1.6, "no_speech_prob": 9.721495644043898e-07}, {"id": 900, "seek": 439092, "start": 4397.2, "end": 4401.2, "text": " I didn't spend a heap of time on this, so there's probably better architectures, but", "tokens": [286, 994, 380, 3496, 257, 33591, 295, 565, 322, 341, 11, 370, 456, 311, 1391, 1101, 6331, 1303, 11, 457], "temperature": 0.0, "avg_logprob": -0.15890083965073284, "compression_ratio": 1.6, "no_speech_prob": 9.721495644043898e-07}, {"id": 901, "seek": 439092, "start": 4401.2, "end": 4404.16, "text": " as you can see, this is still a pretty good one.", "tokens": [382, 291, 393, 536, 11, 341, 307, 920, 257, 1238, 665, 472, 13], "temperature": 0.0, "avg_logprob": -0.15890083965073284, "compression_ratio": 1.6, "no_speech_prob": 9.721495644043898e-07}, {"id": 902, "seek": 439092, "start": 4404.16, "end": 4407.16, "text": " So that was my step 2.", "tokens": [407, 300, 390, 452, 1823, 568, 13], "temperature": 0.0, "avg_logprob": -0.15890083965073284, "compression_ratio": 1.6, "no_speech_prob": 9.721495644043898e-07}, {"id": 903, "seek": 439092, "start": 4407.16, "end": 4415.16, "text": " So far, we have not used a pre-trained network at all.", "tokens": [407, 1400, 11, 321, 362, 406, 1143, 257, 659, 12, 17227, 2001, 3209, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.15890083965073284, "compression_ratio": 1.6, "no_speech_prob": 9.721495644043898e-07}, {"id": 904, "seek": 439092, "start": 4415.16, "end": 4420.4, "text": " So this is getting into the top third of the leaderboard without even using any ImageNet", "tokens": [407, 341, 307, 1242, 666, 264, 1192, 2636, 295, 264, 5263, 3787, 1553, 754, 1228, 604, 29903, 31890], "temperature": 0.0, "avg_logprob": -0.15890083965073284, "compression_ratio": 1.6, "no_speech_prob": 9.721495644043898e-07}, {"id": 905, "seek": 442040, "start": 4420.4, "end": 4421.4, "text": " features.", "tokens": [4122, 13], "temperature": 0.0, "avg_logprob": -0.17221931382721545, "compression_ratio": 1.6872246696035242, "no_speech_prob": 6.54038831271464e-06}, {"id": 906, "seek": 442040, "start": 4421.4, "end": 4424.16, "text": " So that's pretty damn cool.", "tokens": [407, 300, 311, 1238, 8151, 1627, 13], "temperature": 0.0, "avg_logprob": -0.17221931382721545, "compression_ratio": 1.6872246696035242, "no_speech_prob": 6.54038831271464e-06}, {"id": 907, "seek": 442040, "start": 4424.16, "end": 4427.679999999999, "text": " But we're pretty sure that ImageNet features would be helpful.", "tokens": [583, 321, 434, 1238, 988, 300, 29903, 31890, 4122, 576, 312, 4961, 13], "temperature": 0.0, "avg_logprob": -0.17221931382721545, "compression_ratio": 1.6872246696035242, "no_speech_prob": 6.54038831271464e-06}, {"id": 908, "seek": 442040, "start": 4427.679999999999, "end": 4432.5599999999995, "text": " So that was the next step, to use ImageNet features, so VGG features.", "tokens": [407, 300, 390, 264, 958, 1823, 11, 281, 764, 29903, 31890, 4122, 11, 370, 691, 27561, 4122, 13], "temperature": 0.0, "avg_logprob": -0.17221931382721545, "compression_ratio": 1.6872246696035242, "no_speech_prob": 6.54038831271464e-06}, {"id": 909, "seek": 442040, "start": 4432.5599999999995, "end": 4439.719999999999, "text": " Specifically, I was reasonably confident that all of the convolutional layers of VGG are", "tokens": [26058, 11, 286, 390, 23551, 6679, 300, 439, 295, 264, 45216, 304, 7914, 295, 691, 27561, 366], "temperature": 0.0, "avg_logprob": -0.17221931382721545, "compression_ratio": 1.6872246696035242, "no_speech_prob": 6.54038831271464e-06}, {"id": 910, "seek": 442040, "start": 4439.719999999999, "end": 4441.5599999999995, "text": " probably pretty much good enough.", "tokens": [1391, 1238, 709, 665, 1547, 13], "temperature": 0.0, "avg_logprob": -0.17221931382721545, "compression_ratio": 1.6872246696035242, "no_speech_prob": 6.54038831271464e-06}, {"id": 911, "seek": 442040, "start": 4441.5599999999995, "end": 4446.12, "text": " I didn't expect I would have to fine-tune them much, if at all, because the convolutional", "tokens": [286, 994, 380, 2066, 286, 576, 362, 281, 2489, 12, 83, 2613, 552, 709, 11, 498, 412, 439, 11, 570, 264, 45216, 304], "temperature": 0.0, "avg_logprob": -0.17221931382721545, "compression_ratio": 1.6872246696035242, "no_speech_prob": 6.54038831271464e-06}, {"id": 912, "seek": 444612, "start": 4446.12, "end": 4451.32, "text": " layers are the things which really look at the shape and structure of things rather than", "tokens": [7914, 366, 264, 721, 597, 534, 574, 412, 264, 3909, 293, 3877, 295, 721, 2831, 813], "temperature": 0.0, "avg_logprob": -0.11620748657541177, "compression_ratio": 1.76, "no_speech_prob": 9.665963261795696e-06}, {"id": 913, "seek": 444612, "start": 4451.32, "end": 4453.0, "text": " how they fit together.", "tokens": [577, 436, 3318, 1214, 13], "temperature": 0.0, "avg_logprob": -0.11620748657541177, "compression_ratio": 1.76, "no_speech_prob": 9.665963261795696e-06}, {"id": 914, "seek": 444612, "start": 4453.0, "end": 4458.64, "text": " And these are photos of the real world, just like ImageNet are photos of the real world.", "tokens": [400, 613, 366, 5787, 295, 264, 957, 1002, 11, 445, 411, 29903, 31890, 366, 5787, 295, 264, 957, 1002, 13], "temperature": 0.0, "avg_logprob": -0.11620748657541177, "compression_ratio": 1.76, "no_speech_prob": 9.665963261795696e-06}, {"id": 915, "seek": 444612, "start": 4458.64, "end": 4463.5199999999995, "text": " So I really felt like most of the time, if not all of it, was likely to be spent on the", "tokens": [407, 286, 534, 2762, 411, 881, 295, 264, 565, 11, 498, 406, 439, 295, 309, 11, 390, 3700, 281, 312, 4418, 322, 264], "temperature": 0.0, "avg_logprob": -0.11620748657541177, "compression_ratio": 1.76, "no_speech_prob": 9.665963261795696e-06}, {"id": 916, "seek": 444612, "start": 4463.5199999999995, "end": 4465.26, "text": " dense layers.", "tokens": [18011, 7914, 13], "temperature": 0.0, "avg_logprob": -0.11620748657541177, "compression_ratio": 1.76, "no_speech_prob": 9.665963261795696e-06}, {"id": 917, "seek": 444612, "start": 4465.26, "end": 4470.5599999999995, "text": " So therefore, because calculating the convolutional layers takes nearly all the time, because", "tokens": [407, 4412, 11, 570, 28258, 264, 45216, 304, 7914, 2516, 6217, 439, 264, 565, 11, 570], "temperature": 0.0, "avg_logprob": -0.11620748657541177, "compression_ratio": 1.76, "no_speech_prob": 9.665963261795696e-06}, {"id": 918, "seek": 447056, "start": 4470.56, "end": 4477.280000000001, "text": " that's where all the computation is, I pre-computed the output of the convolutional layers.", "tokens": [300, 311, 689, 439, 264, 24903, 307, 11, 286, 659, 12, 1112, 2582, 292, 264, 5598, 295, 264, 45216, 304, 7914, 13], "temperature": 0.0, "avg_logprob": -0.1102084477742513, "compression_ratio": 1.553072625698324, "no_speech_prob": 6.5403542066633236e-06}, {"id": 919, "seek": 447056, "start": 4477.280000000001, "end": 4481.04, "text": " And we've done this before, you might remember.", "tokens": [400, 321, 600, 1096, 341, 949, 11, 291, 1062, 1604, 13], "temperature": 0.0, "avg_logprob": -0.1102084477742513, "compression_ratio": 1.553072625698324, "no_speech_prob": 6.5403542066633236e-06}, {"id": 920, "seek": 447056, "start": 4481.04, "end": 4490.88, "text": " When we looked at dropout, we did exactly this.", "tokens": [1133, 321, 2956, 412, 3270, 346, 11, 321, 630, 2293, 341, 13], "temperature": 0.0, "avg_logprob": -0.1102084477742513, "compression_ratio": 1.553072625698324, "no_speech_prob": 6.5403542066633236e-06}, {"id": 921, "seek": 447056, "start": 4490.88, "end": 4497.52, "text": " We figured out what was the last convolutional layer's ID, we grabbed all of the layers up", "tokens": [492, 8932, 484, 437, 390, 264, 1036, 45216, 304, 4583, 311, 7348, 11, 321, 18607, 439, 295, 264, 7914, 493], "temperature": 0.0, "avg_logprob": -0.1102084477742513, "compression_ratio": 1.553072625698324, "no_speech_prob": 6.5403542066633236e-06}, {"id": 922, "seek": 449752, "start": 4497.52, "end": 4504.92, "text": " to that ID, we built a model out of them, and then we calculated the output of that", "tokens": [281, 300, 7348, 11, 321, 3094, 257, 2316, 484, 295, 552, 11, 293, 550, 321, 15598, 264, 5598, 295, 300], "temperature": 0.0, "avg_logprob": -0.12918913121126135, "compression_ratio": 1.6728971962616823, "no_speech_prob": 9.818202670430765e-06}, {"id": 923, "seek": 449752, "start": 4504.92, "end": 4505.92, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.12918913121126135, "compression_ratio": 1.6728971962616823, "no_speech_prob": 9.818202670430765e-06}, {"id": 924, "seek": 449752, "start": 4505.92, "end": 4512.4800000000005, "text": " That told us the value of those features, those activations from VGG's last convolutional", "tokens": [663, 1907, 505, 264, 2158, 295, 729, 4122, 11, 729, 2430, 763, 490, 691, 27561, 311, 1036, 45216, 304], "temperature": 0.0, "avg_logprob": -0.12918913121126135, "compression_ratio": 1.6728971962616823, "no_speech_prob": 9.818202670430765e-06}, {"id": 925, "seek": 449752, "start": 4512.4800000000005, "end": 4513.6, "text": " layer.", "tokens": [4583, 13], "temperature": 0.0, "avg_logprob": -0.12918913121126135, "compression_ratio": 1.6728971962616823, "no_speech_prob": 9.818202670430765e-06}, {"id": 926, "seek": 449752, "start": 4513.6, "end": 4515.0, "text": " So I did exactly the same thing.", "tokens": [407, 286, 630, 2293, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.12918913121126135, "compression_ratio": 1.6728971962616823, "no_speech_prob": 9.818202670430765e-06}, {"id": 927, "seek": 449752, "start": 4515.0, "end": 4518.580000000001, "text": " I basically copied and pasted that code.", "tokens": [286, 1936, 25365, 293, 1791, 292, 300, 3089, 13], "temperature": 0.0, "avg_logprob": -0.12918913121126135, "compression_ratio": 1.6728971962616823, "no_speech_prob": 9.818202670430765e-06}, {"id": 928, "seek": 449752, "start": 4518.580000000001, "end": 4524.4800000000005, "text": " So I said, grab VGG16, find the last convolutional layer, build a model that contains everything", "tokens": [407, 286, 848, 11, 4444, 691, 27561, 6866, 11, 915, 264, 1036, 45216, 304, 4583, 11, 1322, 257, 2316, 300, 8306, 1203], "temperature": 0.0, "avg_logprob": -0.12918913121126135, "compression_ratio": 1.6728971962616823, "no_speech_prob": 9.818202670430765e-06}, {"id": 929, "seek": 452448, "start": 4524.48, "end": 4534.379999999999, "text": " up to and including that layer, predict the output of that model.", "tokens": [493, 281, 293, 3009, 300, 4583, 11, 6069, 264, 5598, 295, 300, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12848663330078125, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.0129930160474032e-05}, {"id": 930, "seek": 452448, "start": 4534.379999999999, "end": 4542.139999999999, "text": " So predicting the output means calculate the activations of that last convolutional layer.", "tokens": [407, 32884, 264, 5598, 1355, 8873, 264, 2430, 763, 295, 300, 1036, 45216, 304, 4583, 13], "temperature": 0.0, "avg_logprob": -0.12848663330078125, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.0129930160474032e-05}, {"id": 931, "seek": 452448, "start": 4542.139999999999, "end": 4549.679999999999, "text": " And since that takes some time, then save that, so I never have to do it again.", "tokens": [400, 1670, 300, 2516, 512, 565, 11, 550, 3155, 300, 11, 370, 286, 1128, 362, 281, 360, 309, 797, 13], "temperature": 0.0, "avg_logprob": -0.12848663330078125, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.0129930160474032e-05}, {"id": 932, "seek": 454968, "start": 4549.68, "end": 4567.92, "text": " So then in the future, I can just load that array.", "tokens": [407, 550, 294, 264, 2027, 11, 286, 393, 445, 3677, 300, 10225, 13], "temperature": 0.0, "avg_logprob": -0.16430633597903782, "compression_ratio": 1.1941747572815533, "no_speech_prob": 1.1478668966447003e-05}, {"id": 933, "seek": 454968, "start": 4567.92, "end": 4573.64, "text": " And so have a think about what would you expect the shape of this to be.", "tokens": [400, 370, 362, 257, 519, 466, 437, 576, 291, 2066, 264, 3909, 295, 341, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.16430633597903782, "compression_ratio": 1.1941747572815533, "no_speech_prob": 1.1478668966447003e-05}, {"id": 934, "seek": 457364, "start": 4573.64, "end": 4581.4800000000005, "text": " And you can figure out what you would expect the shape to be by looking at model.summary", "tokens": [400, 291, 393, 2573, 484, 437, 291, 576, 2066, 264, 3909, 281, 312, 538, 1237, 412, 2316, 13, 82, 40879, 822], "temperature": 0.0, "avg_logprob": -0.18313623729505038, "compression_ratio": 1.323943661971831, "no_speech_prob": 8.398013960686512e-06}, {"id": 935, "seek": 457364, "start": 4581.4800000000005, "end": 4584.76, "text": " and finding the last convolutional layer.", "tokens": [293, 5006, 264, 1036, 45216, 304, 4583, 13], "temperature": 0.0, "avg_logprob": -0.18313623729505038, "compression_ratio": 1.323943661971831, "no_speech_prob": 8.398013960686512e-06}, {"id": 936, "seek": 457364, "start": 4584.76, "end": 4585.76, "text": " Here it is.", "tokens": [1692, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.18313623729505038, "compression_ratio": 1.323943661971831, "no_speech_prob": 8.398013960686512e-06}, {"id": 937, "seek": 457364, "start": 4585.76, "end": 4591.4800000000005, "text": " And we can see it is 512 filters by 14 by 14.", "tokens": [400, 321, 393, 536, 309, 307, 1025, 4762, 15995, 538, 3499, 538, 3499, 13], "temperature": 0.0, "avg_logprob": -0.18313623729505038, "compression_ratio": 1.323943661971831, "no_speech_prob": 8.398013960686512e-06}, {"id": 938, "seek": 459148, "start": 4591.48, "end": 4614.12, "text": " So let's have a look, and we'll find our conv-val-fit.shape, 512 by 14 by 14, as expected.", "tokens": [407, 718, 311, 362, 257, 574, 11, 293, 321, 603, 915, 527, 3754, 12, 3337, 12, 6845, 13, 82, 42406, 11, 1025, 4762, 538, 3499, 538, 3499, 11, 382, 5176, 13], "temperature": 0.0, "avg_logprob": -0.3559676306588309, "compression_ratio": 1.0112359550561798, "no_speech_prob": 3.705234848894179e-05}, {"id": 939, "seek": 461412, "start": 4614.12, "end": 4625.32, "text": " Is there a reason you chose to leave out the max pooling and flatten layers?", "tokens": [1119, 456, 257, 1778, 291, 5111, 281, 1856, 484, 264, 11469, 7005, 278, 293, 24183, 7914, 30], "temperature": 0.0, "avg_logprob": -0.154362753033638, "compression_ratio": 1.5257142857142858, "no_speech_prob": 3.071749961236492e-05}, {"id": 940, "seek": 461412, "start": 4625.32, "end": 4633.599999999999, "text": " Basically because it takes zero time to calculate them, and the max pooling layer loses information.", "tokens": [8537, 570, 309, 2516, 4018, 565, 281, 8873, 552, 11, 293, 264, 11469, 7005, 278, 4583, 18293, 1589, 13], "temperature": 0.0, "avg_logprob": -0.154362753033638, "compression_ratio": 1.5257142857142858, "no_speech_prob": 3.071749961236492e-05}, {"id": 941, "seek": 461412, "start": 4633.599999999999, "end": 4639.4, "text": " So I thought, given that I might want to play around with other types of pooling or other", "tokens": [407, 286, 1194, 11, 2212, 300, 286, 1062, 528, 281, 862, 926, 365, 661, 3467, 295, 7005, 278, 420, 661], "temperature": 0.0, "avg_logprob": -0.154362753033638, "compression_ratio": 1.5257142857142858, "no_speech_prob": 3.071749961236492e-05}, {"id": 942, "seek": 463940, "start": 4639.4, "end": 4645.2, "text": " types of convolutions, I thought precalculating this layer is the last one that takes a lot", "tokens": [3467, 295, 3754, 15892, 11, 286, 1194, 4346, 304, 2444, 990, 341, 4583, 307, 264, 1036, 472, 300, 2516, 257, 688], "temperature": 0.0, "avg_logprob": -0.1703579652877081, "compression_ratio": 1.5487179487179488, "no_speech_prob": 9.8182208603248e-06}, {"id": 943, "seek": 463940, "start": 4645.2, "end": 4648.36, "text": " of computation time.", "tokens": [295, 24903, 565, 13], "temperature": 0.0, "avg_logprob": -0.1703579652877081, "compression_ratio": 1.5487179487179488, "no_speech_prob": 9.8182208603248e-06}, {"id": 944, "seek": 463940, "start": 4648.36, "end": 4653.839999999999, "text": " Having said that, the first thing I did with it in my new model was to max pool it and", "tokens": [10222, 848, 300, 11, 264, 700, 551, 286, 630, 365, 309, 294, 452, 777, 2316, 390, 281, 11469, 7005, 309, 293], "temperature": 0.0, "avg_logprob": -0.1703579652877081, "compression_ratio": 1.5487179487179488, "no_speech_prob": 9.8182208603248e-06}, {"id": 945, "seek": 463940, "start": 4653.839999999999, "end": 4660.679999999999, "text": " flatten it.", "tokens": [24183, 309, 13], "temperature": 0.0, "avg_logprob": -0.1703579652877081, "compression_ratio": 1.5487179487179488, "no_speech_prob": 9.8182208603248e-06}, {"id": 946, "seek": 463940, "start": 4660.679999999999, "end": 4668.4, "text": " So now that I have the output of VGG for the last conv layer, I can now build a model that", "tokens": [407, 586, 300, 286, 362, 264, 5598, 295, 691, 27561, 337, 264, 1036, 3754, 4583, 11, 286, 393, 586, 1322, 257, 2316, 300], "temperature": 0.0, "avg_logprob": -0.1703579652877081, "compression_ratio": 1.5487179487179488, "no_speech_prob": 9.8182208603248e-06}, {"id": 947, "seek": 466840, "start": 4668.4, "end": 4671.5599999999995, "text": " has dense layers on top of that.", "tokens": [575, 18011, 7914, 322, 1192, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.17723124736064189, "compression_ratio": 1.6639004149377594, "no_speech_prob": 9.223360393662006e-06}, {"id": 948, "seek": 466840, "start": 4671.5599999999995, "end": 4674.799999999999, "text": " And so the input to this model will be the output of those conv layers.", "tokens": [400, 370, 264, 4846, 281, 341, 2316, 486, 312, 264, 5598, 295, 729, 3754, 7914, 13], "temperature": 0.0, "avg_logprob": -0.17723124736064189, "compression_ratio": 1.6639004149377594, "no_speech_prob": 9.223360393662006e-06}, {"id": 949, "seek": 466840, "start": 4674.799999999999, "end": 4678.4, "text": " And the nice thing is it won't take long to run this, even on the whole dataset, because", "tokens": [400, 264, 1481, 551, 307, 309, 1582, 380, 747, 938, 281, 1190, 341, 11, 754, 322, 264, 1379, 28872, 11, 570], "temperature": 0.0, "avg_logprob": -0.17723124736064189, "compression_ratio": 1.6639004149377594, "no_speech_prob": 9.223360393662006e-06}, {"id": 950, "seek": 466840, "start": 4678.4, "end": 4681.62, "text": " the dense layers don't take much computation time.", "tokens": [264, 18011, 7914, 500, 380, 747, 709, 24903, 565, 13], "temperature": 0.0, "avg_logprob": -0.17723124736064189, "compression_ratio": 1.6639004149377594, "no_speech_prob": 9.223360393662006e-06}, {"id": 951, "seek": 466840, "start": 4681.62, "end": 4684.24, "text": " So here's my model.", "tokens": [407, 510, 311, 452, 2316, 13], "temperature": 0.0, "avg_logprob": -0.17723124736064189, "compression_ratio": 1.6639004149377594, "no_speech_prob": 9.223360393662006e-06}, {"id": 952, "seek": 466840, "start": 4684.24, "end": 4691.24, "text": " By making p a parameter, I could try a wide range of dropout amounts.", "tokens": [3146, 1455, 280, 257, 13075, 11, 286, 727, 853, 257, 4874, 3613, 295, 3270, 346, 11663, 13], "temperature": 0.0, "avg_logprob": -0.17723124736064189, "compression_ratio": 1.6639004149377594, "no_speech_prob": 9.223360393662006e-06}, {"id": 953, "seek": 466840, "start": 4691.24, "end": 4697.58, "text": " And I fit it, and one epoch takes 5 seconds on the entire dataset.", "tokens": [400, 286, 3318, 309, 11, 293, 472, 30992, 339, 2516, 1025, 3949, 322, 264, 2302, 28872, 13], "temperature": 0.0, "avg_logprob": -0.17723124736064189, "compression_ratio": 1.6639004149377594, "no_speech_prob": 9.223360393662006e-06}, {"id": 954, "seek": 469758, "start": 4697.58, "end": 4700.44, "text": " So this is a super good way to play around.", "tokens": [407, 341, 307, 257, 1687, 665, 636, 281, 862, 926, 13], "temperature": 0.0, "avg_logprob": -0.15121821071324723, "compression_ratio": 1.5351351351351352, "no_speech_prob": 1.5206457646854687e-05}, {"id": 955, "seek": 469758, "start": 4700.44, "end": 4715.36, "text": " You can see one epoch gets me 0.65, three epochs gets me 0.75.", "tokens": [509, 393, 536, 472, 30992, 339, 2170, 385, 1958, 13, 16824, 11, 1045, 30992, 28346, 2170, 385, 1958, 13, 11901, 13], "temperature": 0.0, "avg_logprob": -0.15121821071324723, "compression_ratio": 1.5351351351351352, "no_speech_prob": 1.5206457646854687e-05}, {"id": 956, "seek": 469758, "start": 4715.36, "end": 4716.36, "text": " So this is pretty cool.", "tokens": [407, 341, 307, 1238, 1627, 13], "temperature": 0.0, "avg_logprob": -0.15121821071324723, "compression_ratio": 1.5351351351351352, "no_speech_prob": 1.5206457646854687e-05}, {"id": 957, "seek": 469758, "start": 4716.36, "end": 4720.0, "text": " I have something that in 15 seconds can get me 0.75 accuracy.", "tokens": [286, 362, 746, 300, 294, 2119, 3949, 393, 483, 385, 1958, 13, 11901, 14170, 13], "temperature": 0.0, "avg_logprob": -0.15121821071324723, "compression_ratio": 1.5351351351351352, "no_speech_prob": 1.5206457646854687e-05}, {"id": 958, "seek": 469758, "start": 4720.0, "end": 4725.12, "text": " And notice here, I'm not using any data augmentation.", "tokens": [400, 3449, 510, 11, 286, 478, 406, 1228, 604, 1412, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.15121821071324723, "compression_ratio": 1.5351351351351352, "no_speech_prob": 1.5206457646854687e-05}, {"id": 959, "seek": 469758, "start": 4725.12, "end": 4727.2, "text": " Why aren't I using data augmentation?", "tokens": [1545, 3212, 380, 286, 1228, 1412, 14501, 19631, 30], "temperature": 0.0, "avg_logprob": -0.15121821071324723, "compression_ratio": 1.5351351351351352, "no_speech_prob": 1.5206457646854687e-05}, {"id": 960, "seek": 472720, "start": 4727.2, "end": 4731.5199999999995, "text": " Because you can't precompute the output of convolutional layers if you're using data", "tokens": [1436, 291, 393, 380, 659, 21541, 1169, 264, 5598, 295, 45216, 304, 7914, 498, 291, 434, 1228, 1412], "temperature": 0.0, "avg_logprob": -0.1127684523419636, "compression_ratio": 1.8284023668639053, "no_speech_prob": 9.516145837551448e-06}, {"id": 961, "seek": 472720, "start": 4731.5199999999995, "end": 4732.5199999999995, "text": " augmentation.", "tokens": [14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.1127684523419636, "compression_ratio": 1.8284023668639053, "no_speech_prob": 9.516145837551448e-06}, {"id": 962, "seek": 472720, "start": 4732.5199999999995, "end": 4740.2, "text": " Because with data augmentation, your convolutional layers give you a different output every time.", "tokens": [1436, 365, 1412, 14501, 19631, 11, 428, 45216, 304, 7914, 976, 291, 257, 819, 5598, 633, 565, 13], "temperature": 0.0, "avg_logprob": -0.1127684523419636, "compression_ratio": 1.8284023668639053, "no_speech_prob": 9.516145837551448e-06}, {"id": 963, "seek": 472720, "start": 4740.2, "end": 4745.04, "text": " So that's just a bit of a bummer.", "tokens": [407, 300, 311, 445, 257, 857, 295, 257, 13309, 936, 13], "temperature": 0.0, "avg_logprob": -0.1127684523419636, "compression_ratio": 1.8284023668639053, "no_speech_prob": 9.516145837551448e-06}, {"id": 964, "seek": 472720, "start": 4745.04, "end": 4750.88, "text": " You can't use data augmentation if you are precomputing the output of a layer.", "tokens": [509, 393, 380, 764, 1412, 14501, 19631, 498, 291, 366, 659, 1112, 2582, 278, 264, 5598, 295, 257, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1127684523419636, "compression_ratio": 1.8284023668639053, "no_speech_prob": 9.516145837551448e-06}, {"id": 965, "seek": 475088, "start": 4750.88, "end": 4762.28, "text": " Because every time it sees the same cat photo, it's rotating it by a different amount.", "tokens": [1436, 633, 565, 309, 8194, 264, 912, 3857, 5052, 11, 309, 311, 19627, 309, 538, 257, 819, 2372, 13], "temperature": 0.0, "avg_logprob": -0.14351517813546316, "compression_ratio": 1.5314285714285714, "no_speech_prob": 4.157343482802389e-06}, {"id": 966, "seek": 475088, "start": 4762.28, "end": 4771.56, "text": " So it gives a different output of the convolutional layer, so you can't precompute it.", "tokens": [407, 309, 2709, 257, 819, 5598, 295, 264, 45216, 304, 4583, 11, 370, 291, 393, 380, 659, 21541, 1169, 309, 13], "temperature": 0.0, "avg_logprob": -0.14351517813546316, "compression_ratio": 1.5314285714285714, "no_speech_prob": 4.157343482802389e-06}, {"id": 967, "seek": 475088, "start": 4771.56, "end": 4776.56, "text": " There is something you can do, which I played with a little bit, which is you could precompute", "tokens": [821, 307, 746, 291, 393, 360, 11, 597, 286, 3737, 365, 257, 707, 857, 11, 597, 307, 291, 727, 659, 21541, 1169], "temperature": 0.0, "avg_logprob": -0.14351517813546316, "compression_ratio": 1.5314285714285714, "no_speech_prob": 4.157343482802389e-06}, {"id": 968, "seek": 477656, "start": 4776.56, "end": 4784.52, "text": " something that's 10 times bigger than your dataset consisting of 10 different data augmented", "tokens": [746, 300, 311, 1266, 1413, 3801, 813, 428, 28872, 33921, 295, 1266, 819, 1412, 36155], "temperature": 0.0, "avg_logprob": -0.2478360448564802, "compression_ratio": 1.52, "no_speech_prob": 1.7502665286883712e-05}, {"id": 969, "seek": 477656, "start": 4784.52, "end": 4797.56, "text": " versions of it, which is why I actually had this data generator with augmentations.", "tokens": [9606, 295, 309, 11, 597, 307, 983, 286, 767, 632, 341, 1412, 19265, 365, 29919, 763, 13], "temperature": 0.0, "avg_logprob": -0.2478360448564802, "compression_ratio": 1.52, "no_speech_prob": 1.7502665286883712e-05}, {"id": 970, "seek": 477656, "start": 4797.56, "end": 4804.0, "text": " And I created something called data-augmented convolutional features in which I predicted", "tokens": [400, 286, 2942, 746, 1219, 1412, 12, 20056, 14684, 45216, 304, 4122, 294, 597, 286, 19147], "temperature": 0.0, "avg_logprob": -0.2478360448564802, "compression_ratio": 1.52, "no_speech_prob": 1.7502665286883712e-05}, {"id": 971, "seek": 480400, "start": 4804.0, "end": 4814.0, "text": " 5 times the amount of data, and so that basically gave me a dataset 5 times bigger.", "tokens": [1025, 1413, 264, 2372, 295, 1412, 11, 293, 370, 300, 1936, 2729, 385, 257, 28872, 1025, 1413, 3801, 13], "temperature": 0.0, "avg_logprob": -0.16874009086972191, "compression_ratio": 1.5096153846153846, "no_speech_prob": 8.530229933967348e-06}, {"id": 972, "seek": 480400, "start": 4814.0, "end": 4816.32, "text": " And that actually worked pretty well.", "tokens": [400, 300, 767, 2732, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.16874009086972191, "compression_ratio": 1.5096153846153846, "no_speech_prob": 8.530229933967348e-06}, {"id": 973, "seek": 480400, "start": 4816.32, "end": 4822.24, "text": " It's not as good as having a whole new sample every time, but it's kind of a compromise.", "tokens": [467, 311, 406, 382, 665, 382, 1419, 257, 1379, 777, 6889, 633, 565, 11, 457, 309, 311, 733, 295, 257, 18577, 13], "temperature": 0.0, "avg_logprob": -0.16874009086972191, "compression_ratio": 1.5096153846153846, "no_speech_prob": 8.530229933967348e-06}, {"id": 974, "seek": 480400, "start": 4822.24, "end": 4830.46, "text": " So once I played around with these dense layers, I then did some more fine-tuning and found", "tokens": [407, 1564, 286, 3737, 926, 365, 613, 18011, 7914, 11, 286, 550, 630, 512, 544, 2489, 12, 83, 37726, 293, 1352], "temperature": 0.0, "avg_logprob": -0.16874009086972191, "compression_ratio": 1.5096153846153846, "no_speech_prob": 8.530229933967348e-06}, {"id": 975, "seek": 480400, "start": 4830.46, "end": 4831.46, "text": " out that...", "tokens": [484, 300, 1097], "temperature": 0.0, "avg_logprob": -0.16874009086972191, "compression_ratio": 1.5096153846153846, "no_speech_prob": 8.530229933967348e-06}, {"id": 976, "seek": 483146, "start": 4831.46, "end": 4837.84, "text": " I went basically here, I then tried saying, let's go through all of my layers in my model", "tokens": [286, 1437, 1936, 510, 11, 286, 550, 3031, 1566, 11, 718, 311, 352, 807, 439, 295, 452, 7914, 294, 452, 2316], "temperature": 0.0, "avg_logprob": -0.18280671990436057, "compression_ratio": 1.5426008968609866, "no_speech_prob": 7.889196240284946e-06}, {"id": 977, "seek": 483146, "start": 4837.84, "end": 4844.16, "text": " from 16 onwards and set them to trainable and see what happens.", "tokens": [490, 3165, 34230, 293, 992, 552, 281, 3847, 712, 293, 536, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.18280671990436057, "compression_ratio": 1.5426008968609866, "no_speech_prob": 7.889196240284946e-06}, {"id": 978, "seek": 483146, "start": 4844.16, "end": 4848.56, "text": " So I tried retraining, fine-tuning some of the convolutional layers as well.", "tokens": [407, 286, 3031, 1533, 424, 1760, 11, 2489, 12, 83, 37726, 512, 295, 264, 45216, 304, 7914, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.18280671990436057, "compression_ratio": 1.5426008968609866, "no_speech_prob": 7.889196240284946e-06}, {"id": 979, "seek": 483146, "start": 4848.56, "end": 4850.44, "text": " It basically didn't help.", "tokens": [467, 1936, 994, 380, 854, 13], "temperature": 0.0, "avg_logprob": -0.18280671990436057, "compression_ratio": 1.5426008968609866, "no_speech_prob": 7.889196240284946e-06}, {"id": 980, "seek": 483146, "start": 4850.44, "end": 4855.16, "text": " So I experimented with my hypothesis and I found it was correct, which is it seems that", "tokens": [407, 286, 5120, 292, 365, 452, 17291, 293, 286, 1352, 309, 390, 3006, 11, 597, 307, 309, 2544, 300], "temperature": 0.0, "avg_logprob": -0.18280671990436057, "compression_ratio": 1.5426008968609866, "no_speech_prob": 7.889196240284946e-06}, {"id": 981, "seek": 485516, "start": 4855.16, "end": 4861.5599999999995, "text": " for this particular model, coming up with the right set of dense layers is what it's", "tokens": [337, 341, 1729, 2316, 11, 1348, 493, 365, 264, 558, 992, 295, 18011, 7914, 307, 437, 309, 311], "temperature": 0.0, "avg_logprob": -0.19833438316088045, "compression_ratio": 1.5586854460093897, "no_speech_prob": 7.071699201333104e-06}, {"id": 982, "seek": 485516, "start": 4861.5599999999995, "end": 4862.5599999999995, "text": " all about.", "tokens": [439, 466, 13], "temperature": 0.0, "avg_logprob": -0.19833438316088045, "compression_ratio": 1.5586854460093897, "no_speech_prob": 7.071699201333104e-06}, {"id": 983, "seek": 485516, "start": 4862.5599999999995, "end": 4863.5599999999995, "text": " Yes, Rachel?", "tokens": [1079, 11, 14246, 30], "temperature": 0.0, "avg_logprob": -0.19833438316088045, "compression_ratio": 1.5586854460093897, "no_speech_prob": 7.071699201333104e-06}, {"id": 984, "seek": 485516, "start": 4863.5599999999995, "end": 4864.5599999999995, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.19833438316088045, "compression_ratio": 1.5586854460093897, "no_speech_prob": 7.071699201333104e-06}, {"id": 985, "seek": 485516, "start": 4864.5599999999995, "end": 4868.72, "text": " There's a question, if we want rotational invariance, should we keep the max pooling", "tokens": [821, 311, 257, 1168, 11, 498, 321, 528, 45420, 33270, 719, 11, 820, 321, 1066, 264, 11469, 7005, 278], "temperature": 0.0, "avg_logprob": -0.19833438316088045, "compression_ratio": 1.5586854460093897, "no_speech_prob": 7.071699201333104e-06}, {"id": 986, "seek": 485516, "start": 4868.72, "end": 4875.16, "text": " or can another layer do it as well?", "tokens": [420, 393, 1071, 4583, 360, 309, 382, 731, 30], "temperature": 0.0, "avg_logprob": -0.19833438316088045, "compression_ratio": 1.5586854460093897, "no_speech_prob": 7.071699201333104e-06}, {"id": 987, "seek": 485516, "start": 4875.16, "end": 4879.12, "text": " Max pooling doesn't really have anything to do with rotational invariance.", "tokens": [7402, 7005, 278, 1177, 380, 534, 362, 1340, 281, 360, 365, 45420, 33270, 719, 13], "temperature": 0.0, "avg_logprob": -0.19833438316088045, "compression_ratio": 1.5586854460093897, "no_speech_prob": 7.071699201333104e-06}, {"id": 988, "seek": 487912, "start": 4879.12, "end": 4887.32, "text": " Max pooling does translation invariance.", "tokens": [7402, 7005, 278, 775, 12853, 33270, 719, 13], "temperature": 0.0, "avg_logprob": -0.16755985330652307, "compression_ratio": 1.7341772151898733, "no_speech_prob": 6.962186944292625e-06}, {"id": 989, "seek": 487912, "start": 4887.32, "end": 4889.12, "text": " So I'm going to show you one more cool trick.", "tokens": [407, 286, 478, 516, 281, 855, 291, 472, 544, 1627, 4282, 13], "temperature": 0.0, "avg_logprob": -0.16755985330652307, "compression_ratio": 1.7341772151898733, "no_speech_prob": 6.962186944292625e-06}, {"id": 990, "seek": 487912, "start": 4889.12, "end": 4893.24, "text": " I'm going to show you a little bit of State Farm every week from now on because there's", "tokens": [286, 478, 516, 281, 855, 291, 257, 707, 857, 295, 4533, 19991, 633, 1243, 490, 586, 322, 570, 456, 311], "temperature": 0.0, "avg_logprob": -0.16755985330652307, "compression_ratio": 1.7341772151898733, "no_speech_prob": 6.962186944292625e-06}, {"id": 991, "seek": 487912, "start": 4893.24, "end": 4896.08, "text": " so many cool things to try.", "tokens": [370, 867, 1627, 721, 281, 853, 13], "temperature": 0.0, "avg_logprob": -0.16755985330652307, "compression_ratio": 1.7341772151898733, "no_speech_prob": 6.962186944292625e-06}, {"id": 992, "seek": 487912, "start": 4896.08, "end": 4901.86, "text": " And I want to keep reviewing CNNs because convolutional neural nets really are becoming", "tokens": [400, 286, 528, 281, 1066, 19576, 24859, 82, 570, 45216, 304, 18161, 36170, 534, 366, 5617], "temperature": 0.0, "avg_logprob": -0.16755985330652307, "compression_ratio": 1.7341772151898733, "no_speech_prob": 6.962186944292625e-06}, {"id": 993, "seek": 487912, "start": 4901.86, "end": 4903.36, "text": " what deep learning is all about.", "tokens": [437, 2452, 2539, 307, 439, 466, 13], "temperature": 0.0, "avg_logprob": -0.16755985330652307, "compression_ratio": 1.7341772151898733, "no_speech_prob": 6.962186944292625e-06}, {"id": 994, "seek": 487912, "start": 4903.36, "end": 4905.76, "text": " I'm going to show you one really cool trick.", "tokens": [286, 478, 516, 281, 855, 291, 472, 534, 1627, 4282, 13], "temperature": 0.0, "avg_logprob": -0.16755985330652307, "compression_ratio": 1.7341772151898733, "no_speech_prob": 6.962186944292625e-06}, {"id": 995, "seek": 487912, "start": 4905.76, "end": 4907.66, "text": " It's actually a combination of two tricks.", "tokens": [467, 311, 767, 257, 6562, 295, 732, 11733, 13], "temperature": 0.0, "avg_logprob": -0.16755985330652307, "compression_ratio": 1.7341772151898733, "no_speech_prob": 6.962186944292625e-06}, {"id": 996, "seek": 490766, "start": 4907.66, "end": 4917.8, "text": " The two tricks are called pseudo-labeling and knowledge distillation.", "tokens": [440, 732, 11733, 366, 1219, 35899, 12, 44990, 11031, 293, 3601, 42923, 399, 13], "temperature": 0.0, "avg_logprob": -0.2049645171768364, "compression_ratio": 1.6, "no_speech_prob": 9.0803869170486e-06}, {"id": 997, "seek": 490766, "start": 4917.8, "end": 4922.5199999999995, "text": " If you Google for pseudo-labeling semi-supervised learning, you can see the original paper that", "tokens": [759, 291, 3329, 337, 35899, 12, 44990, 11031, 12909, 12, 48172, 24420, 2539, 11, 291, 393, 536, 264, 3380, 3035, 300], "temperature": 0.0, "avg_logprob": -0.2049645171768364, "compression_ratio": 1.6, "no_speech_prob": 9.0803869170486e-06}, {"id": 998, "seek": 490766, "start": 4922.5199999999995, "end": 4923.5199999999995, "text": " came out with pseudo-labeling.", "tokens": [1361, 484, 365, 35899, 12, 44990, 11031, 13], "temperature": 0.0, "avg_logprob": -0.2049645171768364, "compression_ratio": 1.6, "no_speech_prob": 9.0803869170486e-06}, {"id": 999, "seek": 490766, "start": 4923.5199999999995, "end": 4929.639999999999, "text": " I guess that's 2013.", "tokens": [286, 2041, 300, 311, 9012, 13], "temperature": 0.0, "avg_logprob": -0.2049645171768364, "compression_ratio": 1.6, "no_speech_prob": 9.0803869170486e-06}, {"id": 1000, "seek": 490766, "start": 4929.639999999999, "end": 4933.2, "text": " And then knowledge distillation.", "tokens": [400, 550, 3601, 42923, 399, 13], "temperature": 0.0, "avg_logprob": -0.2049645171768364, "compression_ratio": 1.6, "no_speech_prob": 9.0803869170486e-06}, {"id": 1001, "seek": 490766, "start": 4933.2, "end": 4937.599999999999, "text": " This is a Jeffrey Hinton paper, Distilling the Knowledge in a Neural Network.", "tokens": [639, 307, 257, 28721, 389, 12442, 3035, 11, 9840, 7345, 264, 32906, 294, 257, 1734, 1807, 12640, 13], "temperature": 0.0, "avg_logprob": -0.2049645171768364, "compression_ratio": 1.6, "no_speech_prob": 9.0803869170486e-06}, {"id": 1002, "seek": 493760, "start": 4937.6, "end": 4940.08, "text": " This is from 2015.", "tokens": [639, 307, 490, 7546, 13], "temperature": 0.0, "avg_logprob": -0.16682882157583084, "compression_ratio": 1.4503311258278146, "no_speech_prob": 2.0904426492052153e-06}, {"id": 1003, "seek": 493760, "start": 4940.08, "end": 4949.280000000001, "text": " So these are a couple of really cool techniques.", "tokens": [407, 613, 366, 257, 1916, 295, 534, 1627, 7512, 13], "temperature": 0.0, "avg_logprob": -0.16682882157583084, "compression_ratio": 1.4503311258278146, "no_speech_prob": 2.0904426492052153e-06}, {"id": 1004, "seek": 493760, "start": 4949.280000000001, "end": 4952.4800000000005, "text": " We're going to combine them together.", "tokens": [492, 434, 516, 281, 10432, 552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.16682882157583084, "compression_ratio": 1.4503311258278146, "no_speech_prob": 2.0904426492052153e-06}, {"id": 1005, "seek": 493760, "start": 4952.4800000000005, "end": 4954.700000000001, "text": " And they're kind of crazy.", "tokens": [400, 436, 434, 733, 295, 3219, 13], "temperature": 0.0, "avg_logprob": -0.16682882157583084, "compression_ratio": 1.4503311258278146, "no_speech_prob": 2.0904426492052153e-06}, {"id": 1006, "seek": 493760, "start": 4954.700000000001, "end": 4960.4800000000005, "text": " What we're going to do is we're going to use the test set to give us more information.", "tokens": [708, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 764, 264, 1500, 992, 281, 976, 505, 544, 1589, 13], "temperature": 0.0, "avg_logprob": -0.16682882157583084, "compression_ratio": 1.4503311258278146, "no_speech_prob": 2.0904426492052153e-06}, {"id": 1007, "seek": 496048, "start": 4960.48, "end": 4968.08, "text": " In State Farm, the test set has 80,000 images in it, and the training set has 20,000 images", "tokens": [682, 4533, 19991, 11, 264, 1500, 992, 575, 4688, 11, 1360, 5267, 294, 309, 11, 293, 264, 3097, 992, 575, 945, 11, 1360, 5267], "temperature": 0.0, "avg_logprob": -0.1761178970336914, "compression_ratio": 1.6243386243386244, "no_speech_prob": 3.0415758374147117e-06}, {"id": 1008, "seek": 496048, "start": 4968.08, "end": 4969.959999999999, "text": " in it.", "tokens": [294, 309, 13], "temperature": 0.0, "avg_logprob": -0.1761178970336914, "compression_ratio": 1.6243386243386244, "no_speech_prob": 3.0415758374147117e-06}, {"id": 1009, "seek": 496048, "start": 4969.959999999999, "end": 4981.759999999999, "text": " So what could we do with those 80,000 images which we don't have labels for?", "tokens": [407, 437, 727, 321, 360, 365, 729, 4688, 11, 1360, 5267, 597, 321, 500, 380, 362, 16949, 337, 30], "temperature": 0.0, "avg_logprob": -0.1761178970336914, "compression_ratio": 1.6243386243386244, "no_speech_prob": 3.0415758374147117e-06}, {"id": 1010, "seek": 496048, "start": 4981.759999999999, "end": 4983.879999999999, "text": " It seems a shame to waste them.", "tokens": [467, 2544, 257, 10069, 281, 5964, 552, 13], "temperature": 0.0, "avg_logprob": -0.1761178970336914, "compression_ratio": 1.6243386243386244, "no_speech_prob": 3.0415758374147117e-06}, {"id": 1011, "seek": 496048, "start": 4983.879999999999, "end": 4986.16, "text": " It seems like we should be able to do something with them.", "tokens": [467, 2544, 411, 321, 820, 312, 1075, 281, 360, 746, 365, 552, 13], "temperature": 0.0, "avg_logprob": -0.1761178970336914, "compression_ratio": 1.6243386243386244, "no_speech_prob": 3.0415758374147117e-06}, {"id": 1012, "seek": 496048, "start": 4986.16, "end": 4987.959999999999, "text": " And there's a great little picture here.", "tokens": [400, 456, 311, 257, 869, 707, 3036, 510, 13], "temperature": 0.0, "avg_logprob": -0.1761178970336914, "compression_ratio": 1.6243386243386244, "no_speech_prob": 3.0415758374147117e-06}, {"id": 1013, "seek": 498796, "start": 4987.96, "end": 4994.44, "text": " Imagine we only had two points, and we knew their labels, white and black.", "tokens": [11739, 321, 787, 632, 732, 2793, 11, 293, 321, 2586, 641, 16949, 11, 2418, 293, 2211, 13], "temperature": 0.0, "avg_logprob": -0.13268726212637766, "compression_ratio": 1.646808510638298, "no_speech_prob": 4.289302069082623e-06}, {"id": 1014, "seek": 498796, "start": 4994.44, "end": 4998.0, "text": " And then somebody said, how would you label this?", "tokens": [400, 550, 2618, 848, 11, 577, 576, 291, 7645, 341, 30], "temperature": 0.0, "avg_logprob": -0.13268726212637766, "compression_ratio": 1.646808510638298, "no_speech_prob": 4.289302069082623e-06}, {"id": 1015, "seek": 498796, "start": 4998.0, "end": 5004.02, "text": " And then they told you that there's a whole lot of other unlabeled data.", "tokens": [400, 550, 436, 1907, 291, 300, 456, 311, 257, 1379, 688, 295, 661, 32118, 18657, 292, 1412, 13], "temperature": 0.0, "avg_logprob": -0.13268726212637766, "compression_ratio": 1.646808510638298, "no_speech_prob": 4.289302069082623e-06}, {"id": 1016, "seek": 498796, "start": 5004.02, "end": 5005.96, "text": " Notice this is all gray.", "tokens": [13428, 341, 307, 439, 10855, 13], "temperature": 0.0, "avg_logprob": -0.13268726212637766, "compression_ratio": 1.646808510638298, "no_speech_prob": 4.289302069082623e-06}, {"id": 1017, "seek": 498796, "start": 5005.96, "end": 5007.92, "text": " It's not labeled.", "tokens": [467, 311, 406, 21335, 13], "temperature": 0.0, "avg_logprob": -0.13268726212637766, "compression_ratio": 1.646808510638298, "no_speech_prob": 4.289302069082623e-06}, {"id": 1018, "seek": 498796, "start": 5007.92, "end": 5009.74, "text": " But it's helped us, hasn't it?", "tokens": [583, 309, 311, 4254, 505, 11, 6132, 380, 309, 30], "temperature": 0.0, "avg_logprob": -0.13268726212637766, "compression_ratio": 1.646808510638298, "no_speech_prob": 4.289302069082623e-06}, {"id": 1019, "seek": 498796, "start": 5009.74, "end": 5014.88, "text": " It's helped us because it's told us how the data is structured.", "tokens": [467, 311, 4254, 505, 570, 309, 311, 1907, 505, 577, 264, 1412, 307, 18519, 13], "temperature": 0.0, "avg_logprob": -0.13268726212637766, "compression_ratio": 1.646808510638298, "no_speech_prob": 4.289302069082623e-06}, {"id": 1020, "seek": 498796, "start": 5014.88, "end": 5016.96, "text": " This is what semi-supervised learning is all about.", "tokens": [639, 307, 437, 12909, 12, 48172, 24420, 2539, 307, 439, 466, 13], "temperature": 0.0, "avg_logprob": -0.13268726212637766, "compression_ratio": 1.646808510638298, "no_speech_prob": 4.289302069082623e-06}, {"id": 1021, "seek": 501696, "start": 5016.96, "end": 5021.2, "text": " It's all about using the unlabeled data to try and understand something about the structure", "tokens": [467, 311, 439, 466, 1228, 264, 32118, 18657, 292, 1412, 281, 853, 293, 1223, 746, 466, 264, 3877], "temperature": 0.0, "avg_logprob": -0.14471024957321982, "compression_ratio": 1.8068181818181819, "no_speech_prob": 1.0783163816086017e-05}, {"id": 1022, "seek": 501696, "start": 5021.2, "end": 5028.2, "text": " of it and use that to help you, just like in this picture.", "tokens": [295, 309, 293, 764, 300, 281, 854, 291, 11, 445, 411, 294, 341, 3036, 13], "temperature": 0.0, "avg_logprob": -0.14471024957321982, "compression_ratio": 1.8068181818181819, "no_speech_prob": 1.0783163816086017e-05}, {"id": 1023, "seek": 501696, "start": 5028.2, "end": 5033.96, "text": " Pseudo-labeling and knowledge distillation are a way to do this.", "tokens": [430, 405, 6207, 12, 44990, 11031, 293, 3601, 42923, 399, 366, 257, 636, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.14471024957321982, "compression_ratio": 1.8068181818181819, "no_speech_prob": 1.0783163816086017e-05}, {"id": 1024, "seek": 501696, "start": 5033.96, "end": 5037.36, "text": " And I'm not going to do it on the test set, I'm going to do it on the validation set because", "tokens": [400, 286, 478, 406, 516, 281, 360, 309, 322, 264, 1500, 992, 11, 286, 478, 516, 281, 360, 309, 322, 264, 24071, 992, 570], "temperature": 0.0, "avg_logprob": -0.14471024957321982, "compression_ratio": 1.8068181818181819, "no_speech_prob": 1.0783163816086017e-05}, {"id": 1025, "seek": 501696, "start": 5037.36, "end": 5040.8, "text": " it's a little bit easier to see the impact of it.", "tokens": [309, 311, 257, 707, 857, 3571, 281, 536, 264, 2712, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.14471024957321982, "compression_ratio": 1.8068181818181819, "no_speech_prob": 1.0783163816086017e-05}, {"id": 1026, "seek": 501696, "start": 5040.8, "end": 5044.56, "text": " And maybe next week we'll look at the test set, because that's going to be much cooler", "tokens": [400, 1310, 958, 1243, 321, 603, 574, 412, 264, 1500, 992, 11, 570, 300, 311, 516, 281, 312, 709, 15566], "temperature": 0.0, "avg_logprob": -0.14471024957321982, "compression_ratio": 1.8068181818181819, "no_speech_prob": 1.0783163816086017e-05}, {"id": 1027, "seek": 501696, "start": 5044.56, "end": 5046.76, "text": " when you're doing the test set.", "tokens": [562, 291, 434, 884, 264, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.14471024957321982, "compression_ratio": 1.8068181818181819, "no_speech_prob": 1.0783163816086017e-05}, {"id": 1028, "seek": 504676, "start": 5046.76, "end": 5047.76, "text": " It's this simple.", "tokens": [467, 311, 341, 2199, 13], "temperature": 0.0, "avg_logprob": -0.12862970021145403, "compression_ratio": 1.7637130801687764, "no_speech_prob": 1.130069085775176e-05}, {"id": 1029, "seek": 504676, "start": 5047.76, "end": 5054.400000000001, "text": " What we do is we take our model, some model we've already built, and we predict the outputs", "tokens": [708, 321, 360, 307, 321, 747, 527, 2316, 11, 512, 2316, 321, 600, 1217, 3094, 11, 293, 321, 6069, 264, 23930], "temperature": 0.0, "avg_logprob": -0.12862970021145403, "compression_ratio": 1.7637130801687764, "no_speech_prob": 1.130069085775176e-05}, {"id": 1030, "seek": 504676, "start": 5054.400000000001, "end": 5057.64, "text": " from that model for our unlabeled set.", "tokens": [490, 300, 2316, 337, 527, 32118, 18657, 292, 992, 13], "temperature": 0.0, "avg_logprob": -0.12862970021145403, "compression_ratio": 1.7637130801687764, "no_speech_prob": 1.130069085775176e-05}, {"id": 1031, "seek": 504676, "start": 5057.64, "end": 5061.04, "text": " In this case I'm using the validation set as if it was unlabeled.", "tokens": [682, 341, 1389, 286, 478, 1228, 264, 24071, 992, 382, 498, 309, 390, 32118, 18657, 292, 13], "temperature": 0.0, "avg_logprob": -0.12862970021145403, "compression_ratio": 1.7637130801687764, "no_speech_prob": 1.130069085775176e-05}, {"id": 1032, "seek": 504676, "start": 5061.04, "end": 5063.84, "text": " So I'm ignoring the labels.", "tokens": [407, 286, 478, 26258, 264, 16949, 13], "temperature": 0.0, "avg_logprob": -0.12862970021145403, "compression_ratio": 1.7637130801687764, "no_speech_prob": 1.130069085775176e-05}, {"id": 1033, "seek": 504676, "start": 5063.84, "end": 5066.4800000000005, "text": " And those things we call the pseudolabels.", "tokens": [400, 729, 721, 321, 818, 264, 25505, 532, 401, 455, 1625, 13], "temperature": 0.0, "avg_logprob": -0.12862970021145403, "compression_ratio": 1.7637130801687764, "no_speech_prob": 1.130069085775176e-05}, {"id": 1034, "seek": 504676, "start": 5066.4800000000005, "end": 5071.52, "text": " So now that we have predictions for the test set or the validation set, it's not that they're", "tokens": [407, 586, 300, 321, 362, 21264, 337, 264, 1500, 992, 420, 264, 24071, 992, 11, 309, 311, 406, 300, 436, 434], "temperature": 0.0, "avg_logprob": -0.12862970021145403, "compression_ratio": 1.7637130801687764, "no_speech_prob": 1.130069085775176e-05}, {"id": 1035, "seek": 504676, "start": 5071.52, "end": 5075.56, "text": " true, but we can pretend they're true.", "tokens": [2074, 11, 457, 321, 393, 11865, 436, 434, 2074, 13], "temperature": 0.0, "avg_logprob": -0.12862970021145403, "compression_ratio": 1.7637130801687764, "no_speech_prob": 1.130069085775176e-05}, {"id": 1036, "seek": 507556, "start": 5075.56, "end": 5080.160000000001, "text": " We can say they're some label, they're not correct labels, but they're labels nonetheless.", "tokens": [492, 393, 584, 436, 434, 512, 7645, 11, 436, 434, 406, 3006, 16949, 11, 457, 436, 434, 16949, 26756, 13], "temperature": 0.0, "avg_logprob": -0.10966464189382699, "compression_ratio": 1.8995215311004785, "no_speech_prob": 5.338104983820813e-06}, {"id": 1037, "seek": 507556, "start": 5080.160000000001, "end": 5087.120000000001, "text": " So what we then do is we take our training labels and we concatenate them with our validation", "tokens": [407, 437, 321, 550, 360, 307, 321, 747, 527, 3097, 16949, 293, 321, 1588, 7186, 473, 552, 365, 527, 24071], "temperature": 0.0, "avg_logprob": -0.10966464189382699, "compression_ratio": 1.8995215311004785, "no_speech_prob": 5.338104983820813e-06}, {"id": 1038, "seek": 507556, "start": 5087.120000000001, "end": 5089.860000000001, "text": " or test set pseudolabels.", "tokens": [420, 1500, 992, 25505, 532, 401, 455, 1625, 13], "temperature": 0.0, "avg_logprob": -0.10966464189382699, "compression_ratio": 1.8995215311004785, "no_speech_prob": 5.338104983820813e-06}, {"id": 1039, "seek": 507556, "start": 5089.860000000001, "end": 5093.92, "text": " And so we now have a bunch of labels for all of our data.", "tokens": [400, 370, 321, 586, 362, 257, 3840, 295, 16949, 337, 439, 295, 527, 1412, 13], "temperature": 0.0, "avg_logprob": -0.10966464189382699, "compression_ratio": 1.8995215311004785, "no_speech_prob": 5.338104983820813e-06}, {"id": 1040, "seek": 507556, "start": 5093.92, "end": 5100.0, "text": " And so we can now also concatenate our convolutional features with the convolutional features of", "tokens": [400, 370, 321, 393, 586, 611, 1588, 7186, 473, 527, 45216, 304, 4122, 365, 264, 45216, 304, 4122, 295], "temperature": 0.0, "avg_logprob": -0.10966464189382699, "compression_ratio": 1.8995215311004785, "no_speech_prob": 5.338104983820813e-06}, {"id": 1041, "seek": 507556, "start": 5100.0, "end": 5104.0, "text": " the validation set or test set.", "tokens": [264, 24071, 992, 420, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.10966464189382699, "compression_ratio": 1.8995215311004785, "no_speech_prob": 5.338104983820813e-06}, {"id": 1042, "seek": 510400, "start": 5104.0, "end": 5109.44, "text": " And we now use these to train a model.", "tokens": [400, 321, 586, 764, 613, 281, 3847, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.129817437434542, "compression_ratio": 1.5033557046979866, "no_speech_prob": 3.1875551940174773e-06}, {"id": 1043, "seek": 510400, "start": 5109.44, "end": 5115.16, "text": " So the model we use is exactly the same model we had before, and we train it in exactly", "tokens": [407, 264, 2316, 321, 764, 307, 2293, 264, 912, 2316, 321, 632, 949, 11, 293, 321, 3847, 309, 294, 2293], "temperature": 0.0, "avg_logprob": -0.129817437434542, "compression_ratio": 1.5033557046979866, "no_speech_prob": 3.1875551940174773e-06}, {"id": 1044, "seek": 510400, "start": 5115.16, "end": 5117.68, "text": " the same way as before.", "tokens": [264, 912, 636, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.129817437434542, "compression_ratio": 1.5033557046979866, "no_speech_prob": 3.1875551940174773e-06}, {"id": 1045, "seek": 510400, "start": 5117.68, "end": 5122.38, "text": " And our loss goes up from.75 to.82.", "tokens": [400, 527, 4470, 1709, 493, 490, 2411, 11901, 281, 2411, 32848, 13], "temperature": 0.0, "avg_logprob": -0.129817437434542, "compression_ratio": 1.5033557046979866, "no_speech_prob": 3.1875551940174773e-06}, {"id": 1046, "seek": 510400, "start": 5122.38, "end": 5127.36, "text": " So our error has dropped by like 25%.", "tokens": [407, 527, 6713, 575, 8119, 538, 411, 3552, 6856], "temperature": 0.0, "avg_logprob": -0.129817437434542, "compression_ratio": 1.5033557046979866, "no_speech_prob": 3.1875551940174773e-06}, {"id": 1047, "seek": 512736, "start": 5127.36, "end": 5134.719999999999, "text": " And the reason why is just because we use this additional unlabeled data to try to figure", "tokens": [400, 264, 1778, 983, 307, 445, 570, 321, 764, 341, 4497, 32118, 18657, 292, 1412, 281, 853, 281, 2573], "temperature": 0.0, "avg_logprob": -0.1619902218089384, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.3631238289235625e-05}, {"id": 1048, "seek": 512736, "start": 5134.719999999999, "end": 5136.2, "text": " out the structure of it.", "tokens": [484, 264, 3877, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.1619902218089384, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.3631238289235625e-05}, {"id": 1049, "seek": 512736, "start": 5136.2, "end": 5139.88, "text": " Question about model choice.", "tokens": [14464, 466, 2316, 3922, 13], "temperature": 0.0, "avg_logprob": -0.1619902218089384, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.3631238289235625e-05}, {"id": 1050, "seek": 512736, "start": 5139.88, "end": 5143.28, "text": " How do you learn how to design a model and when to stop messing with them?", "tokens": [1012, 360, 291, 1466, 577, 281, 1715, 257, 2316, 293, 562, 281, 1590, 23258, 365, 552, 30], "temperature": 0.0, "avg_logprob": -0.1619902218089384, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.3631238289235625e-05}, {"id": 1051, "seek": 512736, "start": 5143.28, "end": 5148.139999999999, "text": " It seems like you've taken a few initial ideas, tweaked them to get higher accuracy, but unless", "tokens": [467, 2544, 411, 291, 600, 2726, 257, 1326, 5883, 3487, 11, 6986, 7301, 552, 281, 483, 2946, 14170, 11, 457, 5969], "temperature": 0.0, "avg_logprob": -0.1619902218089384, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.3631238289235625e-05}, {"id": 1052, "seek": 512736, "start": 5148.139999999999, "end": 5152.24, "text": " your initial guesses are amazing, there should be plenty of architectures that would also", "tokens": [428, 5883, 42703, 366, 2243, 11, 456, 820, 312, 7140, 295, 6331, 1303, 300, 576, 611], "temperature": 0.0, "avg_logprob": -0.1619902218089384, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.3631238289235625e-05}, {"id": 1053, "seek": 512736, "start": 5152.24, "end": 5153.24, "text": " work.", "tokens": [589, 13], "temperature": 0.0, "avg_logprob": -0.1619902218089384, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.3631238289235625e-05}, {"id": 1054, "seek": 515324, "start": 5153.24, "end": 5159.719999999999, "text": " If and when you figure out how to find an architecture and stop fucking with it, please", "tokens": [759, 293, 562, 291, 2573, 484, 577, 281, 915, 364, 9482, 293, 1590, 5546, 365, 309, 11, 1767], "temperature": 0.0, "avg_logprob": -0.16220796399000215, "compression_ratio": 1.5136612021857923, "no_speech_prob": 3.8227375625865534e-05}, {"id": 1055, "seek": 515324, "start": 5159.719999999999, "end": 5167.08, "text": " tell me, because I don't sleep.", "tokens": [980, 385, 11, 570, 286, 500, 380, 2817, 13], "temperature": 0.0, "avg_logprob": -0.16220796399000215, "compression_ratio": 1.5136612021857923, "no_speech_prob": 3.8227375625865534e-05}, {"id": 1056, "seek": 515324, "start": 5167.08, "end": 5168.719999999999, "text": " We all want to know this.", "tokens": [492, 439, 528, 281, 458, 341, 13], "temperature": 0.0, "avg_logprob": -0.16220796399000215, "compression_ratio": 1.5136612021857923, "no_speech_prob": 3.8227375625865534e-05}, {"id": 1057, "seek": 515324, "start": 5168.719999999999, "end": 5174.96, "text": " I look back at these models I'm showing you, and I'm thinking, I bet there's something", "tokens": [286, 574, 646, 412, 613, 5245, 286, 478, 4099, 291, 11, 293, 286, 478, 1953, 11, 286, 778, 456, 311, 746], "temperature": 0.0, "avg_logprob": -0.16220796399000215, "compression_ratio": 1.5136612021857923, "no_speech_prob": 3.8227375625865534e-05}, {"id": 1058, "seek": 515324, "start": 5174.96, "end": 5177.2, "text": " like twice as good.", "tokens": [411, 6091, 382, 665, 13], "temperature": 0.0, "avg_logprob": -0.16220796399000215, "compression_ratio": 1.5136612021857923, "no_speech_prob": 3.8227375625865534e-05}, {"id": 1059, "seek": 515324, "start": 5177.2, "end": 5179.4, "text": " I don't know what it is.", "tokens": [286, 500, 380, 458, 437, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.16220796399000215, "compression_ratio": 1.5136612021857923, "no_speech_prob": 3.8227375625865534e-05}, {"id": 1060, "seek": 517940, "start": 5179.4, "end": 5186.08, "text": " There are all kinds of ways of optimizing other hyperparameters of deep learning.", "tokens": [821, 366, 439, 3685, 295, 2098, 295, 40425, 661, 9848, 2181, 335, 6202, 295, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.11665829022725423, "compression_ratio": 1.558659217877095, "no_speech_prob": 4.68379475933034e-05}, {"id": 1061, "seek": 517940, "start": 5186.08, "end": 5196.16, "text": " For example, there's something called Spearmint, which is a Bayesian optimization hyperparameter", "tokens": [1171, 1365, 11, 456, 311, 746, 1219, 3550, 4452, 686, 11, 597, 307, 257, 7840, 42434, 19618, 9848, 2181, 335, 2398], "temperature": 0.0, "avg_logprob": -0.11665829022725423, "compression_ratio": 1.558659217877095, "no_speech_prob": 4.68379475933034e-05}, {"id": 1062, "seek": 517940, "start": 5196.16, "end": 5199.4, "text": " tuning thing.", "tokens": [15164, 551, 13], "temperature": 0.0, "avg_logprob": -0.11665829022725423, "compression_ratio": 1.558659217877095, "no_speech_prob": 4.68379475933034e-05}, {"id": 1063, "seek": 517940, "start": 5199.4, "end": 5203.879999999999, "text": " In fact just last week a new paper came out for hyperparameter tuning, but this is all", "tokens": [682, 1186, 445, 1036, 1243, 257, 777, 3035, 1361, 484, 337, 9848, 2181, 335, 2398, 15164, 11, 457, 341, 307, 439], "temperature": 0.0, "avg_logprob": -0.11665829022725423, "compression_ratio": 1.558659217877095, "no_speech_prob": 4.68379475933034e-05}, {"id": 1064, "seek": 520388, "start": 5203.88, "end": 5210.32, "text": " about tuning things like the learning rate and stuff like that.", "tokens": [466, 15164, 721, 411, 264, 2539, 3314, 293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.15915870666503906, "compression_ratio": 1.5345911949685536, "no_speech_prob": 7.527884918090422e-06}, {"id": 1065, "seek": 520388, "start": 5210.32, "end": 5223.88, "text": " Coming up with architectures, there are some people who have tried to come up with some", "tokens": [12473, 493, 365, 6331, 1303, 11, 456, 366, 512, 561, 567, 362, 3031, 281, 808, 493, 365, 512], "temperature": 0.0, "avg_logprob": -0.15915870666503906, "compression_ratio": 1.5345911949685536, "no_speech_prob": 7.527884918090422e-06}, {"id": 1066, "seek": 520388, "start": 5223.88, "end": 5229.88, "text": " kind of more general architectures, and we're going to look at one next week called Resnets,", "tokens": [733, 295, 544, 2674, 6331, 1303, 11, 293, 321, 434, 516, 281, 574, 412, 472, 958, 1243, 1219, 5015, 77, 1385, 11], "temperature": 0.0, "avg_logprob": -0.15915870666503906, "compression_ratio": 1.5345911949685536, "no_speech_prob": 7.527884918090422e-06}, {"id": 1067, "seek": 522988, "start": 5229.88, "end": 5234.92, "text": " which seem to be pretty encouraging in that direction.", "tokens": [597, 1643, 281, 312, 1238, 14580, 294, 300, 3513, 13], "temperature": 0.0, "avg_logprob": -0.3336598284832843, "compression_ratio": 1.4477611940298507, "no_speech_prob": 2.2124970200820826e-05}, {"id": 1068, "seek": 522988, "start": 5234.92, "end": 5239.0, "text": " I'll give you an example.", "tokens": [286, 603, 976, 291, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.3336598284832843, "compression_ratio": 1.4477611940298507, "no_speech_prob": 2.2124970200820826e-05}, {"id": 1069, "seek": 522988, "start": 5239.0, "end": 5247.4400000000005, "text": " Resnet is an architecture which won ImageNet in 2015.", "tokens": [5015, 7129, 307, 364, 9482, 597, 1582, 29903, 31890, 294, 7546, 13], "temperature": 0.0, "avg_logprob": -0.3336598284832843, "compression_ratio": 1.4477611940298507, "no_speech_prob": 2.2124970200820826e-05}, {"id": 1070, "seek": 522988, "start": 5247.4400000000005, "end": 5255.16, "text": " The author of Resnet, super smart guy, Kai Ming He from Microsoft said, the reason Resnet", "tokens": [440, 3793, 295, 5015, 7129, 11, 1687, 4069, 2146, 11, 20753, 19352, 634, 490, 8116, 848, 11, 264, 1778, 5015, 7129], "temperature": 0.0, "avg_logprob": -0.3336598284832843, "compression_ratio": 1.4477611940298507, "no_speech_prob": 2.2124970200820826e-05}, {"id": 1071, "seek": 522988, "start": 5255.16, "end": 5259.8, "text": " is so great is it lets us build very very very very deep networks.", "tokens": [307, 370, 869, 307, 309, 6653, 505, 1322, 588, 588, 588, 588, 2452, 9590, 13], "temperature": 0.0, "avg_logprob": -0.3336598284832843, "compression_ratio": 1.4477611940298507, "no_speech_prob": 2.2124970200820826e-05}, {"id": 1072, "seek": 525980, "start": 5259.8, "end": 5265.84, "text": " Indeed he showed a network with over 1000 layers and it was totally state of the art.", "tokens": [15061, 415, 4712, 257, 3209, 365, 670, 9714, 7914, 293, 309, 390, 3879, 1785, 295, 264, 1523, 13], "temperature": 0.0, "avg_logprob": -0.12520930705926356, "compression_ratio": 1.4976303317535544, "no_speech_prob": 7.183129582699621e-06}, {"id": 1073, "seek": 525980, "start": 5265.84, "end": 5274.360000000001, "text": " Somebody else came along a few months ago and built wide Resnets with like 50 layers", "tokens": [13463, 1646, 1361, 2051, 257, 1326, 2493, 2057, 293, 3094, 4874, 5015, 77, 1385, 365, 411, 2625, 7914], "temperature": 0.0, "avg_logprob": -0.12520930705926356, "compression_ratio": 1.4976303317535544, "no_speech_prob": 7.183129582699621e-06}, {"id": 1074, "seek": 525980, "start": 5274.360000000001, "end": 5277.92, "text": " and easily beat Kai Ming He's best results.", "tokens": [293, 3612, 4224, 20753, 19352, 634, 311, 1151, 3542, 13], "temperature": 0.0, "avg_logprob": -0.12520930705926356, "compression_ratio": 1.4976303317535544, "no_speech_prob": 7.183129582699621e-06}, {"id": 1075, "seek": 525980, "start": 5277.92, "end": 5283.52, "text": " So the very author of the ImageNet winner completely got wrong the reason why his invention", "tokens": [407, 264, 588, 3793, 295, 264, 29903, 31890, 8507, 2584, 658, 2085, 264, 1778, 983, 702, 22265], "temperature": 0.0, "avg_logprob": -0.12520930705926356, "compression_ratio": 1.4976303317535544, "no_speech_prob": 7.183129582699621e-06}, {"id": 1076, "seek": 525980, "start": 5283.52, "end": 5284.900000000001, "text": " was good.", "tokens": [390, 665, 13], "temperature": 0.0, "avg_logprob": -0.12520930705926356, "compression_ratio": 1.4976303317535544, "no_speech_prob": 7.183129582699621e-06}, {"id": 1077, "seek": 528490, "start": 5284.9, "end": 5290.799999999999, "text": " So the idea that any of us have any idea how to create optimal architectures is totally", "tokens": [407, 264, 1558, 300, 604, 295, 505, 362, 604, 1558, 577, 281, 1884, 16252, 6331, 1303, 307, 3879], "temperature": 0.0, "avg_logprob": -0.10952585171430539, "compression_ratio": 1.6309963099630995, "no_speech_prob": 6.747938641638029e-06}, {"id": 1078, "seek": 528490, "start": 5290.799999999999, "end": 5291.799999999999, "text": " totally wrong.", "tokens": [3879, 2085, 13], "temperature": 0.0, "avg_logprob": -0.10952585171430539, "compression_ratio": 1.6309963099630995, "no_speech_prob": 6.747938641638029e-06}, {"id": 1079, "seek": 528490, "start": 5291.799999999999, "end": 5292.799999999999, "text": " We don't.", "tokens": [492, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.10952585171430539, "compression_ratio": 1.6309963099630995, "no_speech_prob": 6.747938641638029e-06}, {"id": 1080, "seek": 528490, "start": 5292.799999999999, "end": 5298.0, "text": " So that's why I'm trying to show you what we know so far, which is like the processes", "tokens": [407, 300, 311, 983, 286, 478, 1382, 281, 855, 291, 437, 321, 458, 370, 1400, 11, 597, 307, 411, 264, 7555], "temperature": 0.0, "avg_logprob": -0.10952585171430539, "compression_ratio": 1.6309963099630995, "no_speech_prob": 6.747938641638029e-06}, {"id": 1081, "seek": 528490, "start": 5298.0, "end": 5301.36, "text": " you can use to build them without waiting forever.", "tokens": [291, 393, 764, 281, 1322, 552, 1553, 3806, 5680, 13], "temperature": 0.0, "avg_logprob": -0.10952585171430539, "compression_ratio": 1.6309963099630995, "no_speech_prob": 6.747938641638029e-06}, {"id": 1082, "seek": 528490, "start": 5301.36, "end": 5307.0, "text": " So in this case, doing your data augmentation on the small sample in a rigorous way, figuring", "tokens": [407, 294, 341, 1389, 11, 884, 428, 1412, 14501, 19631, 322, 264, 1359, 6889, 294, 257, 29882, 636, 11, 15213], "temperature": 0.0, "avg_logprob": -0.10952585171430539, "compression_ratio": 1.6309963099630995, "no_speech_prob": 6.747938641638029e-06}, {"id": 1083, "seek": 528490, "start": 5307.0, "end": 5311.32, "text": " out that probably the dense layers are where the action is at and pre-computing the input", "tokens": [484, 300, 1391, 264, 18011, 7914, 366, 689, 264, 3069, 307, 412, 293, 659, 12, 1112, 2582, 278, 264, 4846], "temperature": 0.0, "avg_logprob": -0.10952585171430539, "compression_ratio": 1.6309963099630995, "no_speech_prob": 6.747938641638029e-06}, {"id": 1084, "seek": 528490, "start": 5311.32, "end": 5312.58, "text": " to them.", "tokens": [281, 552, 13], "temperature": 0.0, "avg_logprob": -0.10952585171430539, "compression_ratio": 1.6309963099630995, "no_speech_prob": 6.747938641638029e-06}, {"id": 1085, "seek": 531258, "start": 5312.58, "end": 5316.26, "text": " These are the kinds of things that can keep you sane.", "tokens": [1981, 366, 264, 3685, 295, 721, 300, 393, 1066, 291, 45610, 13], "temperature": 0.0, "avg_logprob": -0.17809980055865118, "compression_ratio": 1.7370517928286853, "no_speech_prob": 1.3845691682945471e-05}, {"id": 1086, "seek": 531258, "start": 5316.26, "end": 5321.5599999999995, "text": " I'm showing you the outcome of my last weeks playing with this.", "tokens": [286, 478, 4099, 291, 264, 9700, 295, 452, 1036, 3259, 2433, 365, 341, 13], "temperature": 0.0, "avg_logprob": -0.17809980055865118, "compression_ratio": 1.7370517928286853, "no_speech_prob": 1.3845691682945471e-05}, {"id": 1087, "seek": 531258, "start": 5321.5599999999995, "end": 5327.5199999999995, "text": " I can tell you that during this time I continually fell into the trap of running stuff on the", "tokens": [286, 393, 980, 291, 300, 1830, 341, 565, 286, 22277, 5696, 666, 264, 11487, 295, 2614, 1507, 322, 264], "temperature": 0.0, "avg_logprob": -0.17809980055865118, "compression_ratio": 1.7370517928286853, "no_speech_prob": 1.3845691682945471e-05}, {"id": 1088, "seek": 531258, "start": 5327.5199999999995, "end": 5333.28, "text": " whole network and all the way through and fiddling around with hyperparameters.", "tokens": [1379, 3209, 293, 439, 264, 636, 807, 293, 283, 14273, 1688, 926, 365, 9848, 2181, 335, 6202, 13], "temperature": 0.0, "avg_logprob": -0.17809980055865118, "compression_ratio": 1.7370517928286853, "no_speech_prob": 1.3845691682945471e-05}, {"id": 1089, "seek": 531258, "start": 5333.28, "end": 5336.8, "text": " And I'd have to stop myself and have a cup of tea and say, okay, is this really a good", "tokens": [400, 286, 1116, 362, 281, 1590, 2059, 293, 362, 257, 4414, 295, 5817, 293, 584, 11, 1392, 11, 307, 341, 534, 257, 665], "temperature": 0.0, "avg_logprob": -0.17809980055865118, "compression_ratio": 1.7370517928286853, "no_speech_prob": 1.3845691682945471e-05}, {"id": 1090, "seek": 531258, "start": 5336.8, "end": 5339.1, "text": " idea, is this really a good use of time.", "tokens": [1558, 11, 307, 341, 534, 257, 665, 764, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.17809980055865118, "compression_ratio": 1.7370517928286853, "no_speech_prob": 1.3845691682945471e-05}, {"id": 1091, "seek": 531258, "start": 5339.1, "end": 5341.32, "text": " So we all do it.", "tokens": [407, 321, 439, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.17809980055865118, "compression_ratio": 1.7370517928286853, "no_speech_prob": 1.3845691682945471e-05}, {"id": 1092, "seek": 534132, "start": 5341.32, "end": 5351.48, "text": " But not you anymore because you've been to this class.", "tokens": [583, 406, 291, 3602, 570, 291, 600, 668, 281, 341, 1508, 13], "temperature": 0.0, "avg_logprob": -0.2852585834005605, "compression_ratio": 1.5794392523364487, "no_speech_prob": 2.5866376745398156e-05}, {"id": 1093, "seek": 534132, "start": 5351.48, "end": 5353.12, "text": " Can you run us through this one more time?", "tokens": [1664, 291, 1190, 505, 807, 341, 472, 544, 565, 30], "temperature": 0.0, "avg_logprob": -0.2852585834005605, "compression_ratio": 1.5794392523364487, "no_speech_prob": 2.5866376745398156e-05}, {"id": 1094, "seek": 534132, "start": 5353.12, "end": 5354.88, "text": " I'm just a little confused.", "tokens": [286, 478, 445, 257, 707, 9019, 13], "temperature": 0.0, "avg_logprob": -0.2852585834005605, "compression_ratio": 1.5794392523364487, "no_speech_prob": 2.5866376745398156e-05}, {"id": 1095, "seek": 534132, "start": 5354.88, "end": 5360.719999999999, "text": " It feels like maybe we're using our validation set as part of our training program.", "tokens": [467, 3417, 411, 1310, 321, 434, 1228, 527, 24071, 992, 382, 644, 295, 527, 3097, 1461, 13], "temperature": 0.0, "avg_logprob": -0.2852585834005605, "compression_ratio": 1.5794392523364487, "no_speech_prob": 2.5866376745398156e-05}, {"id": 1096, "seek": 534132, "start": 5360.719999999999, "end": 5362.24, "text": " I'm confused how it's not true.", "tokens": [286, 478, 9019, 577, 309, 311, 406, 2074, 13], "temperature": 0.0, "avg_logprob": -0.2852585834005605, "compression_ratio": 1.5794392523364487, "no_speech_prob": 2.5866376745398156e-05}, {"id": 1097, "seek": 534132, "start": 5362.24, "end": 5366.679999999999, "text": " But look, we're not using the validation labels.", "tokens": [583, 574, 11, 321, 434, 406, 1228, 264, 24071, 16949, 13], "temperature": 0.0, "avg_logprob": -0.2852585834005605, "compression_ratio": 1.5794392523364487, "no_speech_prob": 2.5866376745398156e-05}, {"id": 1098, "seek": 534132, "start": 5366.679999999999, "end": 5370.2, "text": " Nowhere here does it say val underscore labels.", "tokens": [823, 6703, 510, 775, 309, 584, 1323, 37556, 16949, 13], "temperature": 0.0, "avg_logprob": -0.2852585834005605, "compression_ratio": 1.5794392523364487, "no_speech_prob": 2.5866376745398156e-05}, {"id": 1099, "seek": 537020, "start": 5370.2, "end": 5374.88, "text": " So yeah, we are absolutely using our validation set, but we're using the validation set's", "tokens": [407, 1338, 11, 321, 366, 3122, 1228, 527, 24071, 992, 11, 457, 321, 434, 1228, 264, 24071, 992, 311], "temperature": 0.0, "avg_logprob": -0.14008834010870883, "compression_ratio": 1.8262548262548262, "no_speech_prob": 1.4510255823552143e-05}, {"id": 1100, "seek": 537020, "start": 5374.88, "end": 5377.32, "text": " inputs.", "tokens": [15743, 13], "temperature": 0.0, "avg_logprob": -0.14008834010870883, "compression_ratio": 1.8262548262548262, "no_speech_prob": 1.4510255823552143e-05}, {"id": 1101, "seek": 537020, "start": 5377.32, "end": 5381.32, "text": " And for our test set, we have the inputs.", "tokens": [400, 337, 527, 1500, 992, 11, 321, 362, 264, 15743, 13], "temperature": 0.0, "avg_logprob": -0.14008834010870883, "compression_ratio": 1.8262548262548262, "no_speech_prob": 1.4510255823552143e-05}, {"id": 1102, "seek": 537020, "start": 5381.32, "end": 5386.72, "text": " So next week I will show you this page again, and this time I'm going to use the test set.", "tokens": [407, 958, 1243, 286, 486, 855, 291, 341, 3028, 797, 11, 293, 341, 565, 286, 478, 516, 281, 764, 264, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.14008834010870883, "compression_ratio": 1.8262548262548262, "no_speech_prob": 1.4510255823552143e-05}, {"id": 1103, "seek": 537020, "start": 5386.72, "end": 5389.599999999999, "text": " I just didn't have enough time to do it this time around.", "tokens": [286, 445, 994, 380, 362, 1547, 565, 281, 360, 309, 341, 565, 926, 13], "temperature": 0.0, "avg_logprob": -0.14008834010870883, "compression_ratio": 1.8262548262548262, "no_speech_prob": 1.4510255823552143e-05}, {"id": 1104, "seek": 537020, "start": 5389.599999999999, "end": 5391.599999999999, "text": " And hopefully we're going to see some great results.", "tokens": [400, 4696, 321, 434, 516, 281, 536, 512, 869, 3542, 13], "temperature": 0.0, "avg_logprob": -0.14008834010870883, "compression_ratio": 1.8262548262548262, "no_speech_prob": 1.4510255823552143e-05}, {"id": 1105, "seek": 537020, "start": 5391.599999999999, "end": 5394.84, "text": " And when we do it on the test set, then you'll be really convinced that it's not using the", "tokens": [400, 562, 321, 360, 309, 322, 264, 1500, 992, 11, 550, 291, 603, 312, 534, 12561, 300, 309, 311, 406, 1228, 264], "temperature": 0.0, "avg_logprob": -0.14008834010870883, "compression_ratio": 1.8262548262548262, "no_speech_prob": 1.4510255823552143e-05}, {"id": 1106, "seek": 537020, "start": 5394.84, "end": 5396.44, "text": " labels because we don't have any labels.", "tokens": [16949, 570, 321, 500, 380, 362, 604, 16949, 13], "temperature": 0.0, "avg_logprob": -0.14008834010870883, "compression_ratio": 1.8262548262548262, "no_speech_prob": 1.4510255823552143e-05}, {"id": 1107, "seek": 539644, "start": 5396.44, "end": 5402.32, "text": " But you can see here, all it's doing is it's creating pseudo-labels by calculating what", "tokens": [583, 291, 393, 536, 510, 11, 439, 309, 311, 884, 307, 309, 311, 4084, 35899, 12, 44990, 1625, 538, 28258, 437], "temperature": 0.0, "avg_logprob": -0.13187340686195775, "compression_ratio": 1.549222797927461, "no_speech_prob": 6.708745786454529e-05}, {"id": 1108, "seek": 539644, "start": 5402.32, "end": 5410.5599999999995, "text": " it thinks it ought to be based on the model that we just built with that 75% accuracy.", "tokens": [309, 7309, 309, 13416, 281, 312, 2361, 322, 264, 2316, 300, 321, 445, 3094, 365, 300, 9562, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.13187340686195775, "compression_ratio": 1.549222797927461, "no_speech_prob": 6.708745786454529e-05}, {"id": 1109, "seek": 539644, "start": 5410.5599999999995, "end": 5416.32, "text": " And so then it's able to use the input data for the validation set in an intelligent way", "tokens": [400, 370, 550, 309, 311, 1075, 281, 764, 264, 4846, 1412, 337, 264, 24071, 992, 294, 364, 13232, 636], "temperature": 0.0, "avg_logprob": -0.13187340686195775, "compression_ratio": 1.549222797927461, "no_speech_prob": 6.708745786454529e-05}, {"id": 1110, "seek": 539644, "start": 5416.32, "end": 5418.12, "text": " and therefore improve the accuracy.", "tokens": [293, 4412, 3470, 264, 14170, 13], "temperature": 0.0, "avg_logprob": -0.13187340686195775, "compression_ratio": 1.549222797927461, "no_speech_prob": 6.708745786454529e-05}, {"id": 1111, "seek": 541812, "start": 5418.12, "end": 5428.72, "text": " Question asked.", "tokens": [14464, 2351, 13], "temperature": 0.0, "avg_logprob": -0.3607042269273238, "compression_ratio": 1.4038461538461537, "no_speech_prob": 3.269712397013791e-05}, {"id": 1112, "seek": 541812, "start": 5428.72, "end": 5429.72, "text": " What do you mean the same?", "tokens": [708, 360, 291, 914, 264, 912, 30], "temperature": 0.0, "avg_logprob": -0.3607042269273238, "compression_ratio": 1.4038461538461537, "no_speech_prob": 3.269712397013791e-05}, {"id": 1113, "seek": 541812, "start": 5429.72, "end": 5430.72, "text": " Question asked.", "tokens": [14464, 2351, 13], "temperature": 0.0, "avg_logprob": -0.3607042269273238, "compression_ratio": 1.4038461538461537, "no_speech_prob": 3.269712397013791e-05}, {"id": 1114, "seek": 541812, "start": 5430.72, "end": 5445.72, "text": " Yeah, it's using bn underscore model, and bn underscore model is the thing that we just", "tokens": [865, 11, 309, 311, 1228, 272, 77, 37556, 2316, 11, 293, 272, 77, 37556, 2316, 307, 264, 551, 300, 321, 445], "temperature": 0.0, "avg_logprob": -0.3607042269273238, "compression_ratio": 1.4038461538461537, "no_speech_prob": 3.269712397013791e-05}, {"id": 1115, "seek": 544572, "start": 5445.72, "end": 5450.12, "text": " fitted by using the training labels.", "tokens": [26321, 538, 1228, 264, 3097, 16949, 13], "temperature": 0.0, "avg_logprob": -0.38869273473346044, "compression_ratio": 1.39375, "no_speech_prob": 3.0715851607965305e-05}, {"id": 1116, "seek": 544572, "start": 5450.12, "end": 5452.72, "text": " This is bn model, this thing with this 0.755 accuracy.", "tokens": [639, 307, 272, 77, 2316, 11, 341, 551, 365, 341, 1958, 13, 11901, 20, 14170, 13], "temperature": 0.0, "avg_logprob": -0.38869273473346044, "compression_ratio": 1.39375, "no_speech_prob": 3.0715851607965305e-05}, {"id": 1117, "seek": 544572, "start": 5452.72, "end": 5469.64, "text": " Semi-supervised works because you're giving it a model which already knows about a bunch", "tokens": [318, 13372, 12, 48172, 24420, 1985, 570, 291, 434, 2902, 309, 257, 2316, 597, 1217, 3255, 466, 257, 3840], "temperature": 0.0, "avg_logprob": -0.38869273473346044, "compression_ratio": 1.39375, "no_speech_prob": 3.0715851607965305e-05}, {"id": 1118, "seek": 544572, "start": 5469.64, "end": 5472.4400000000005, "text": " of labels, but unsupervised wouldn't know.", "tokens": [295, 16949, 11, 457, 2693, 12879, 24420, 2759, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.38869273473346044, "compression_ratio": 1.39375, "no_speech_prob": 3.0715851607965305e-05}, {"id": 1119, "seek": 547244, "start": 5472.44, "end": 5481.36, "text": " Unsupervised has nothing, that's right.", "tokens": [25017, 12879, 24420, 575, 1825, 11, 300, 311, 558, 13], "temperature": 0.0, "avg_logprob": -0.23382165431976318, "compression_ratio": 1.5543478260869565, "no_speech_prob": 1.922267438203562e-05}, {"id": 1120, "seek": 547244, "start": 5481.36, "end": 5486.639999999999, "text": " Unsupervised learning is where you're trying to build a model when you have no labels at", "tokens": [25017, 12879, 24420, 2539, 307, 689, 291, 434, 1382, 281, 1322, 257, 2316, 562, 291, 362, 572, 16949, 412], "temperature": 0.0, "avg_logprob": -0.23382165431976318, "compression_ratio": 1.5543478260869565, "no_speech_prob": 1.922267438203562e-05}, {"id": 1121, "seek": 547244, "start": 5486.639999999999, "end": 5487.639999999999, "text": " all.", "tokens": [439, 13], "temperature": 0.0, "avg_logprob": -0.23382165431976318, "compression_ratio": 1.5543478260869565, "no_speech_prob": 1.922267438203562e-05}, {"id": 1122, "seek": 547244, "start": 5487.639999999999, "end": 5491.36, "text": " How many people here would be interested in hearing about unsupervised learning during", "tokens": [1012, 867, 561, 510, 576, 312, 3102, 294, 4763, 466, 2693, 12879, 24420, 2539, 1830], "temperature": 0.0, "avg_logprob": -0.23382165431976318, "compression_ratio": 1.5543478260869565, "no_speech_prob": 1.922267438203562e-05}, {"id": 1123, "seek": 547244, "start": 5491.36, "end": 5492.36, "text": " this class?", "tokens": [341, 1508, 30], "temperature": 0.0, "avg_logprob": -0.23382165431976318, "compression_ratio": 1.5543478260869565, "no_speech_prob": 1.922267438203562e-05}, {"id": 1124, "seek": 547244, "start": 5492.36, "end": 5495.0, "text": " Okay, enough people, I should do that.", "tokens": [1033, 11, 1547, 561, 11, 286, 820, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.23382165431976318, "compression_ratio": 1.5543478260869565, "no_speech_prob": 1.922267438203562e-05}, {"id": 1125, "seek": 547244, "start": 5495.0, "end": 5502.28, "text": " I will add it.", "tokens": [286, 486, 909, 309, 13], "temperature": 0.0, "avg_logprob": -0.23382165431976318, "compression_ratio": 1.5543478260869565, "no_speech_prob": 1.922267438203562e-05}, {"id": 1126, "seek": 550228, "start": 5502.28, "end": 5506.5599999999995, "text": " During the week, perhaps we can create a forum thread about unsupervised learning and I can", "tokens": [6842, 264, 1243, 11, 4317, 321, 393, 1884, 257, 17542, 7207, 466, 2693, 12879, 24420, 2539, 293, 286, 393], "temperature": 0.0, "avg_logprob": -0.2517262988620334, "compression_ratio": 1.5350877192982457, "no_speech_prob": 1.0952870979963336e-05}, {"id": 1127, "seek": 550228, "start": 5506.5599999999995, "end": 5509.5199999999995, "text": " learn about what you're interested in doing with it.", "tokens": [1466, 466, 437, 291, 434, 3102, 294, 884, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.2517262988620334, "compression_ratio": 1.5350877192982457, "no_speech_prob": 1.0952870979963336e-05}, {"id": 1128, "seek": 550228, "start": 5509.5199999999995, "end": 5516.719999999999, "text": " Because many things that people think of as unsupervised problems actually aren't.", "tokens": [1436, 867, 721, 300, 561, 519, 295, 382, 2693, 12879, 24420, 2740, 767, 3212, 380, 13], "temperature": 0.0, "avg_logprob": -0.2517262988620334, "compression_ratio": 1.5350877192982457, "no_speech_prob": 1.0952870979963336e-05}, {"id": 1129, "seek": 550228, "start": 5516.719999999999, "end": 5521.5599999999995, "text": " So pseudo-labeling is insane and awesome.", "tokens": [407, 35899, 12, 44990, 11031, 307, 10838, 293, 3476, 13], "temperature": 0.0, "avg_logprob": -0.2517262988620334, "compression_ratio": 1.5350877192982457, "no_speech_prob": 1.0952870979963336e-05}, {"id": 1130, "seek": 550228, "start": 5521.5599999999995, "end": 5523.5599999999995, "text": " And we need the green box back.", "tokens": [400, 321, 643, 264, 3092, 2424, 646, 13], "temperature": 0.0, "avg_logprob": -0.2517262988620334, "compression_ratio": 1.5350877192982457, "no_speech_prob": 1.0952870979963336e-05}, {"id": 1131, "seek": 550228, "start": 5523.5599999999995, "end": 5525.5599999999995, "text": " Question asked.", "tokens": [14464, 2351, 13], "temperature": 0.0, "avg_logprob": -0.2517262988620334, "compression_ratio": 1.5350877192982457, "no_speech_prob": 1.0952870979963336e-05}, {"id": 1132, "seek": 550228, "start": 5525.5599999999995, "end": 5530.5599999999995, "text": " There are a number of questions.", "tokens": [821, 366, 257, 1230, 295, 1651, 13], "temperature": 0.0, "avg_logprob": -0.2517262988620334, "compression_ratio": 1.5350877192982457, "no_speech_prob": 1.0952870979963336e-05}, {"id": 1133, "seek": 553056, "start": 5530.56, "end": 5534.4800000000005, "text": " So one is earlier you talked about learning about the structure of the data that you can", "tokens": [407, 472, 307, 3071, 291, 2825, 466, 2539, 466, 264, 3877, 295, 264, 1412, 300, 291, 393], "temperature": 0.0, "avg_logprob": -0.3009096374511719, "compression_ratio": 1.6993006993006994, "no_speech_prob": 3.94392991438508e-05}, {"id": 1134, "seek": 553056, "start": 5534.4800000000005, "end": 5535.88, "text": " learn from the validation set.", "tokens": [1466, 490, 264, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.3009096374511719, "compression_ratio": 1.6993006993006994, "no_speech_prob": 3.94392991438508e-05}, {"id": 1135, "seek": 553056, "start": 5535.88, "end": 5538.360000000001, "text": " Can you say more about that?", "tokens": [1664, 291, 584, 544, 466, 300, 30], "temperature": 0.0, "avg_logprob": -0.3009096374511719, "compression_ratio": 1.6993006993006994, "no_speech_prob": 3.94392991438508e-05}, {"id": 1136, "seek": 553056, "start": 5538.360000000001, "end": 5540.64, "text": " I don't know, not really.", "tokens": [286, 500, 380, 458, 11, 406, 534, 13], "temperature": 0.0, "avg_logprob": -0.3009096374511719, "compression_ratio": 1.6993006993006994, "no_speech_prob": 3.94392991438508e-05}, {"id": 1137, "seek": 553056, "start": 5540.64, "end": 5544.240000000001, "text": " Other than that picture I showed you before with the two little spirally things.", "tokens": [5358, 813, 300, 3036, 286, 4712, 291, 949, 365, 264, 732, 707, 10733, 379, 721, 13], "temperature": 0.0, "avg_logprob": -0.3009096374511719, "compression_ratio": 1.6993006993006994, "no_speech_prob": 3.94392991438508e-05}, {"id": 1138, "seek": 553056, "start": 5544.240000000001, "end": 5549.160000000001, "text": " And that picture was kind of showing how they clustered in a way that was higher dimension", "tokens": [400, 300, 3036, 390, 733, 295, 4099, 577, 436, 596, 38624, 294, 257, 636, 300, 390, 2946, 10139], "temperature": 0.0, "avg_logprob": -0.3009096374511719, "compression_ratio": 1.6993006993006994, "no_speech_prob": 3.94392991438508e-05}, {"id": 1139, "seek": 553056, "start": 5549.160000000001, "end": 5551.4400000000005, "text": " than what you can see when you just had to.", "tokens": [813, 437, 291, 393, 536, 562, 291, 445, 632, 281, 13], "temperature": 0.0, "avg_logprob": -0.3009096374511719, "compression_ratio": 1.6993006993006994, "no_speech_prob": 3.94392991438508e-05}, {"id": 1140, "seek": 553056, "start": 5551.4400000000005, "end": 5556.84, "text": " So think about that Matt Zyler paper we saw or the Jason Yosinski visualization toolbox", "tokens": [407, 519, 466, 300, 7397, 1176, 88, 1918, 3035, 321, 1866, 420, 264, 11181, 398, 329, 38984, 25801, 44593], "temperature": 0.0, "avg_logprob": -0.3009096374511719, "compression_ratio": 1.6993006993006994, "no_speech_prob": 3.94392991438508e-05}, {"id": 1141, "seek": 553056, "start": 5556.84, "end": 5558.52, "text": " we saw.", "tokens": [321, 1866, 13], "temperature": 0.0, "avg_logprob": -0.3009096374511719, "compression_ratio": 1.6993006993006994, "no_speech_prob": 3.94392991438508e-05}, {"id": 1142, "seek": 555852, "start": 5558.52, "end": 5566.320000000001, "text": " The layers learn shapes and textures and concepts.", "tokens": [440, 7914, 1466, 10854, 293, 24501, 293, 10392, 13], "temperature": 0.0, "avg_logprob": -0.11579783488128145, "compression_ratio": 1.5333333333333334, "no_speech_prob": 4.6378004299185704e-06}, {"id": 1143, "seek": 555852, "start": 5566.320000000001, "end": 5573.0, "text": " In that 80,000 test images of people driving in different distracted ways, there are lots", "tokens": [682, 300, 4688, 11, 1360, 1500, 5267, 295, 561, 4840, 294, 819, 21658, 2098, 11, 456, 366, 3195], "temperature": 0.0, "avg_logprob": -0.11579783488128145, "compression_ratio": 1.5333333333333334, "no_speech_prob": 4.6378004299185704e-06}, {"id": 1144, "seek": 555852, "start": 5573.0, "end": 5578.56, "text": " of concepts there to learn about ways in which people drive in distracted ways, even though", "tokens": [295, 10392, 456, 281, 1466, 466, 2098, 294, 597, 561, 3332, 294, 21658, 2098, 11, 754, 1673], "temperature": 0.0, "avg_logprob": -0.11579783488128145, "compression_ratio": 1.5333333333333334, "no_speech_prob": 4.6378004299185704e-06}, {"id": 1145, "seek": 555852, "start": 5578.56, "end": 5580.120000000001, "text": " they're not labeled.", "tokens": [436, 434, 406, 21335, 13], "temperature": 0.0, "avg_logprob": -0.11579783488128145, "compression_ratio": 1.5333333333333334, "no_speech_prob": 4.6378004299185704e-06}, {"id": 1146, "seek": 558012, "start": 5580.12, "end": 5588.92, "text": " So what we're doing is we're trying to learn better convolutional or dense features.", "tokens": [407, 437, 321, 434, 884, 307, 321, 434, 1382, 281, 1466, 1101, 45216, 304, 420, 18011, 4122, 13], "temperature": 0.0, "avg_logprob": -0.21840681188246783, "compression_ratio": 1.6111111111111112, "no_speech_prob": 3.3931228244910017e-06}, {"id": 1147, "seek": 558012, "start": 5588.92, "end": 5590.72, "text": " That's what I mean by learning more.", "tokens": [663, 311, 437, 286, 914, 538, 2539, 544, 13], "temperature": 0.0, "avg_logprob": -0.21840681188246783, "compression_ratio": 1.6111111111111112, "no_speech_prob": 3.3931228244910017e-06}, {"id": 1148, "seek": 558012, "start": 5590.72, "end": 5596.4, "text": " So the structure of the data here is basically what these pictures tend to look like.", "tokens": [407, 264, 3877, 295, 264, 1412, 510, 307, 1936, 437, 613, 5242, 3928, 281, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.21840681188246783, "compression_ratio": 1.6111111111111112, "no_speech_prob": 3.3931228244910017e-06}, {"id": 1149, "seek": 558012, "start": 5596.4, "end": 5599.5199999999995, "text": " More importantly, in what ways do they differ?", "tokens": [5048, 8906, 11, 294, 437, 2098, 360, 436, 743, 30], "temperature": 0.0, "avg_logprob": -0.21840681188246783, "compression_ratio": 1.6111111111111112, "no_speech_prob": 3.3931228244910017e-06}, {"id": 1150, "seek": 558012, "start": 5599.5199999999995, "end": 5606.5599999999995, "text": " Because it's the ways that they differ that therefore must be related to how they're labeled.", "tokens": [1436, 309, 311, 264, 2098, 300, 436, 743, 300, 4412, 1633, 312, 4077, 281, 577, 436, 434, 21335, 13], "temperature": 0.0, "avg_logprob": -0.21840681188246783, "compression_ratio": 1.6111111111111112, "no_speech_prob": 3.3931228244910017e-06}, {"id": 1151, "seek": 560656, "start": 5606.56, "end": 5612.4400000000005, "text": " Question- Can you use your updated model to make new labels for the validation set?", "tokens": [14464, 12, 1664, 291, 764, 428, 10588, 2316, 281, 652, 777, 16949, 337, 264, 24071, 992, 30], "temperature": 0.0, "avg_logprob": -0.21753684679667154, "compression_ratio": 1.6425339366515836, "no_speech_prob": 5.475776197272353e-05}, {"id": 1152, "seek": 560656, "start": 5612.4400000000005, "end": 5616.64, "text": " Answer- Yes, you can absolutely do pseudo-labeling on pseudo-labeling.", "tokens": [24545, 12, 1079, 11, 291, 393, 3122, 360, 35899, 12, 44990, 11031, 322, 35899, 12, 44990, 11031, 13], "temperature": 0.0, "avg_logprob": -0.21753684679667154, "compression_ratio": 1.6425339366515836, "no_speech_prob": 5.475776197272353e-05}, {"id": 1153, "seek": 560656, "start": 5616.64, "end": 5618.320000000001, "text": " And you should.", "tokens": [400, 291, 820, 13], "temperature": 0.0, "avg_logprob": -0.21753684679667154, "compression_ratio": 1.6425339366515836, "no_speech_prob": 5.475776197272353e-05}, {"id": 1154, "seek": 560656, "start": 5618.320000000001, "end": 5624.6, "text": " And if I don't get sick of running this code, I will try it next week.", "tokens": [400, 498, 286, 500, 380, 483, 4998, 295, 2614, 341, 3089, 11, 286, 486, 853, 309, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.21753684679667154, "compression_ratio": 1.6425339366515836, "no_speech_prob": 5.475776197272353e-05}, {"id": 1155, "seek": 560656, "start": 5624.6, "end": 5628.360000000001, "text": " Question- Could that introduce bias towards your validation set?", "tokens": [14464, 12, 7497, 300, 5366, 12577, 3030, 428, 24071, 992, 30], "temperature": 0.0, "avg_logprob": -0.21753684679667154, "compression_ratio": 1.6425339366515836, "no_speech_prob": 5.475776197272353e-05}, {"id": 1156, "seek": 560656, "start": 5628.360000000001, "end": 5633.160000000001, "text": " Answer- No, because we don't have any validation labels.", "tokens": [24545, 12, 883, 11, 570, 321, 500, 380, 362, 604, 24071, 16949, 13], "temperature": 0.0, "avg_logprob": -0.21753684679667154, "compression_ratio": 1.6425339366515836, "no_speech_prob": 5.475776197272353e-05}, {"id": 1157, "seek": 563316, "start": 5633.16, "end": 5641.36, "text": " One of the tricky parameters in pseudo-labeling is in each batch, how much do I make it a", "tokens": [1485, 295, 264, 12414, 9834, 294, 35899, 12, 44990, 11031, 307, 294, 1184, 15245, 11, 577, 709, 360, 286, 652, 309, 257], "temperature": 0.0, "avg_logprob": -0.1464264838250129, "compression_ratio": 1.5809523809523809, "no_speech_prob": 9.818223588808905e-06}, {"id": 1158, "seek": 563316, "start": 5641.36, "end": 5645.2, "text": " mix of training versus pseudo.", "tokens": [2890, 295, 3097, 5717, 35899, 13], "temperature": 0.0, "avg_logprob": -0.1464264838250129, "compression_ratio": 1.5809523809523809, "no_speech_prob": 9.818223588808905e-06}, {"id": 1159, "seek": 563316, "start": 5645.2, "end": 5650.5599999999995, "text": " One of the big things that stopped me from getting the test set in this week is that", "tokens": [1485, 295, 264, 955, 721, 300, 5936, 385, 490, 1242, 264, 1500, 992, 294, 341, 1243, 307, 300], "temperature": 0.0, "avg_logprob": -0.1464264838250129, "compression_ratio": 1.5809523809523809, "no_speech_prob": 9.818223588808905e-06}, {"id": 1160, "seek": 563316, "start": 5650.5599999999995, "end": 5659.0, "text": " Keras doesn't have a way of creating batches which have like 80% of this set and 20% of", "tokens": [591, 6985, 1177, 380, 362, 257, 636, 295, 4084, 15245, 279, 597, 362, 411, 4688, 4, 295, 341, 992, 293, 945, 4, 295], "temperature": 0.0, "avg_logprob": -0.1464264838250129, "compression_ratio": 1.5809523809523809, "no_speech_prob": 9.818223588808905e-06}, {"id": 1161, "seek": 563316, "start": 5659.0, "end": 5661.639999999999, "text": " that set, which is really what I want.", "tokens": [300, 992, 11, 597, 307, 534, 437, 286, 528, 13], "temperature": 0.0, "avg_logprob": -0.1464264838250129, "compression_ratio": 1.5809523809523809, "no_speech_prob": 9.818223588808905e-06}, {"id": 1162, "seek": 566164, "start": 5661.64, "end": 5669.4800000000005, "text": " If I just pseudo-labeled the whole test set and then concatenated it, then 80% of my batches", "tokens": [759, 286, 445, 35899, 12, 75, 18657, 292, 264, 1379, 1500, 992, 293, 550, 1588, 7186, 770, 309, 11, 550, 4688, 4, 295, 452, 15245, 279], "temperature": 0.0, "avg_logprob": -0.15077582576818632, "compression_ratio": 1.6220472440944882, "no_speech_prob": 1.130057262344053e-05}, {"id": 1163, "seek": 566164, "start": 5669.4800000000005, "end": 5671.12, "text": " are going to be pseudo-labels.", "tokens": [366, 516, 281, 312, 35899, 12, 44990, 1625, 13], "temperature": 0.0, "avg_logprob": -0.15077582576818632, "compression_ratio": 1.6220472440944882, "no_speech_prob": 1.130057262344053e-05}, {"id": 1164, "seek": 566164, "start": 5671.12, "end": 5675.320000000001, "text": " And generally speaking, the rule of thumb I've read is that somewhere around a quarter", "tokens": [400, 5101, 4124, 11, 264, 4978, 295, 9298, 286, 600, 1401, 307, 300, 4079, 926, 257, 6555], "temperature": 0.0, "avg_logprob": -0.15077582576818632, "compression_ratio": 1.6220472440944882, "no_speech_prob": 1.130057262344053e-05}, {"id": 1165, "seek": 566164, "start": 5675.320000000001, "end": 5679.12, "text": " to a third of your mini-batches should be pseudo-labels.", "tokens": [281, 257, 2636, 295, 428, 8382, 12, 65, 852, 279, 820, 312, 35899, 12, 44990, 1625, 13], "temperature": 0.0, "avg_logprob": -0.15077582576818632, "compression_ratio": 1.6220472440944882, "no_speech_prob": 1.130057262344053e-05}, {"id": 1166, "seek": 566164, "start": 5679.12, "end": 5685.8, "text": " So I need to write some code basically to get Keras to generate batches which are a", "tokens": [407, 286, 643, 281, 2464, 512, 3089, 1936, 281, 483, 591, 6985, 281, 8460, 15245, 279, 597, 366, 257], "temperature": 0.0, "avg_logprob": -0.15077582576818632, "compression_ratio": 1.6220472440944882, "no_speech_prob": 1.130057262344053e-05}, {"id": 1167, "seek": 566164, "start": 5685.8, "end": 5689.200000000001, "text": " mix from two different places before I can do this properly.", "tokens": [2890, 490, 732, 819, 3190, 949, 286, 393, 360, 341, 6108, 13], "temperature": 0.0, "avg_logprob": -0.15077582576818632, "compression_ratio": 1.6220472440944882, "no_speech_prob": 1.130057262344053e-05}, {"id": 1168, "seek": 568920, "start": 5689.2, "end": 5696.8, "text": " Question- Are your pseudo-labels only as good as the initial model you're beginning from?", "tokens": [14464, 12, 2014, 428, 35899, 12, 44990, 1625, 787, 382, 665, 382, 264, 5883, 2316, 291, 434, 2863, 490, 30], "temperature": 0.0, "avg_logprob": -0.2238597869873047, "compression_ratio": 1.6082474226804124, "no_speech_prob": 1.805813008104451e-05}, {"id": 1169, "seek": 568920, "start": 5696.8, "end": 5699.84, "text": " So do you need to have a particular accuracy?", "tokens": [407, 360, 291, 643, 281, 362, 257, 1729, 14170, 30], "temperature": 0.0, "avg_logprob": -0.2238597869873047, "compression_ratio": 1.6082474226804124, "no_speech_prob": 1.805813008104451e-05}, {"id": 1170, "seek": 568920, "start": 5699.84, "end": 5705.92, "text": " Answer- Yeah, your pseudo-labels are indeed as good as your model you're starting from.", "tokens": [24545, 12, 865, 11, 428, 35899, 12, 44990, 1625, 366, 6451, 382, 665, 382, 428, 2316, 291, 434, 2891, 490, 13], "temperature": 0.0, "avg_logprob": -0.2238597869873047, "compression_ratio": 1.6082474226804124, "no_speech_prob": 1.805813008104451e-05}, {"id": 1171, "seek": 568920, "start": 5705.92, "end": 5713.84, "text": " People have not studied this enough to know how sensitive it is to those initial labels.", "tokens": [3432, 362, 406, 9454, 341, 1547, 281, 458, 577, 9477, 309, 307, 281, 729, 5883, 16949, 13], "temperature": 0.0, "avg_logprob": -0.2238597869873047, "compression_ratio": 1.6082474226804124, "no_speech_prob": 1.805813008104451e-05}, {"id": 1172, "seek": 571384, "start": 5713.84, "end": 5719.56, "text": " Question- Is there a rule of thumb about what accuracy level?", "tokens": [14464, 12, 1119, 456, 257, 4978, 295, 9298, 466, 437, 14170, 1496, 30], "temperature": 0.0, "avg_logprob": -0.17045839550425707, "compression_ratio": 1.5657894736842106, "no_speech_prob": 1.2606812560989056e-05}, {"id": 1173, "seek": 571384, "start": 5719.56, "end": 5723.56, "text": " Answer- No, this is too new.", "tokens": [24545, 12, 883, 11, 341, 307, 886, 777, 13], "temperature": 0.0, "avg_logprob": -0.17045839550425707, "compression_ratio": 1.5657894736842106, "no_speech_prob": 1.2606812560989056e-05}, {"id": 1174, "seek": 571384, "start": 5723.56, "end": 5725.400000000001, "text": " Just try it.", "tokens": [1449, 853, 309, 13], "temperature": 0.0, "avg_logprob": -0.17045839550425707, "compression_ratio": 1.5657894736842106, "no_speech_prob": 1.2606812560989056e-05}, {"id": 1175, "seek": 571384, "start": 5725.400000000001, "end": 5730.76, "text": " My guess is that pseudo-labels will be useful regardless of what accuracy level you're at", "tokens": [1222, 2041, 307, 300, 35899, 12, 44990, 1625, 486, 312, 4420, 10060, 295, 437, 14170, 1496, 291, 434, 412], "temperature": 0.0, "avg_logprob": -0.17045839550425707, "compression_ratio": 1.5657894736842106, "no_speech_prob": 1.2606812560989056e-05}, {"id": 1176, "seek": 571384, "start": 5730.76, "end": 5732.32, "text": " because it will make it better.", "tokens": [570, 309, 486, 652, 309, 1101, 13], "temperature": 0.0, "avg_logprob": -0.17045839550425707, "compression_ratio": 1.5657894736842106, "no_speech_prob": 1.2606812560989056e-05}, {"id": 1177, "seek": 571384, "start": 5732.32, "end": 5736.92, "text": " As long as you are in a semi-supervised learning context, i.e. you have a lot of unlabeled", "tokens": [1018, 938, 382, 291, 366, 294, 257, 12909, 12, 48172, 24420, 2539, 4319, 11, 741, 13, 68, 13, 291, 362, 257, 688, 295, 32118, 18657, 292], "temperature": 0.0, "avg_logprob": -0.17045839550425707, "compression_ratio": 1.5657894736842106, "no_speech_prob": 1.2606812560989056e-05}, {"id": 1178, "seek": 571384, "start": 5736.92, "end": 5740.64, "text": " data that you want to take advantage of.", "tokens": [1412, 300, 291, 528, 281, 747, 5002, 295, 13], "temperature": 0.0, "avg_logprob": -0.17045839550425707, "compression_ratio": 1.5657894736842106, "no_speech_prob": 1.2606812560989056e-05}, {"id": 1179, "seek": 574064, "start": 5740.64, "end": 5747.64, "text": " I really want to move on because I told you I wanted to get us down the path to NLP this", "tokens": [286, 534, 528, 281, 1286, 322, 570, 286, 1907, 291, 286, 1415, 281, 483, 505, 760, 264, 3100, 281, 426, 45196, 341], "temperature": 0.0, "avg_logprob": -0.141999905759638, "compression_ratio": 1.6945812807881773, "no_speech_prob": 4.264652307028882e-05}, {"id": 1180, "seek": 574064, "start": 5747.64, "end": 5749.240000000001, "text": " week.", "tokens": [1243, 13], "temperature": 0.0, "avg_logprob": -0.141999905759638, "compression_ratio": 1.6945812807881773, "no_speech_prob": 4.264652307028882e-05}, {"id": 1181, "seek": 574064, "start": 5749.240000000001, "end": 5756.22, "text": " It turns out that the path to NLP, strange as it sounds, starts with collaborative filtering.", "tokens": [467, 4523, 484, 300, 264, 3100, 281, 426, 45196, 11, 5861, 382, 309, 3263, 11, 3719, 365, 16555, 30822, 13], "temperature": 0.0, "avg_logprob": -0.141999905759638, "compression_ratio": 1.6945812807881773, "no_speech_prob": 4.264652307028882e-05}, {"id": 1182, "seek": 574064, "start": 5756.22, "end": 5758.46, "text": " You will learn why next week.", "tokens": [509, 486, 1466, 983, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.141999905759638, "compression_ratio": 1.6945812807881773, "no_speech_prob": 4.264652307028882e-05}, {"id": 1183, "seek": 574064, "start": 5758.46, "end": 5762.8, "text": " This week we are going to learn about collaborative filtering.", "tokens": [639, 1243, 321, 366, 516, 281, 1466, 466, 16555, 30822, 13], "temperature": 0.0, "avg_logprob": -0.141999905759638, "compression_ratio": 1.6945812807881773, "no_speech_prob": 4.264652307028882e-05}, {"id": 1184, "seek": 574064, "start": 5762.8, "end": 5767.04, "text": " Collaborative filtering is a way of doing recommender systems.", "tokens": [44483, 1166, 30822, 307, 257, 636, 295, 884, 2748, 260, 3652, 13], "temperature": 0.0, "avg_logprob": -0.141999905759638, "compression_ratio": 1.6945812807881773, "no_speech_prob": 4.264652307028882e-05}, {"id": 1185, "seek": 576704, "start": 5767.04, "end": 5772.24, "text": " I sent you guys an email today with a link to more information about collaborative filtering", "tokens": [286, 2279, 291, 1074, 364, 3796, 965, 365, 257, 2113, 281, 544, 1589, 466, 16555, 30822], "temperature": 0.0, "avg_logprob": -0.11964172270239853, "compression_ratio": 1.5797101449275361, "no_speech_prob": 1.8342610928812064e-05}, {"id": 1186, "seek": 576704, "start": 5772.24, "end": 5778.0199999999995, "text": " and recommender systems, so please read those links if you haven't already just to get a", "tokens": [293, 2748, 260, 3652, 11, 370, 1767, 1401, 729, 6123, 498, 291, 2378, 380, 1217, 445, 281, 483, 257], "temperature": 0.0, "avg_logprob": -0.11964172270239853, "compression_ratio": 1.5797101449275361, "no_speech_prob": 1.8342610928812064e-05}, {"id": 1187, "seek": 576704, "start": 5778.0199999999995, "end": 5782.56, "text": " sense of what the problem we're solving here is.", "tokens": [2020, 295, 437, 264, 1154, 321, 434, 12606, 510, 307, 13], "temperature": 0.0, "avg_logprob": -0.11964172270239853, "compression_ratio": 1.5797101449275361, "no_speech_prob": 1.8342610928812064e-05}, {"id": 1188, "seek": 576704, "start": 5782.56, "end": 5794.8, "text": " In short, what we're trying to do is to learn to predict who is going to like what and how", "tokens": [682, 2099, 11, 437, 321, 434, 1382, 281, 360, 307, 281, 1466, 281, 6069, 567, 307, 516, 281, 411, 437, 293, 577], "temperature": 0.0, "avg_logprob": -0.11964172270239853, "compression_ratio": 1.5797101449275361, "no_speech_prob": 1.8342610928812064e-05}, {"id": 1189, "seek": 576704, "start": 5794.8, "end": 5796.28, "text": " much.", "tokens": [709, 13], "temperature": 0.0, "avg_logprob": -0.11964172270239853, "compression_ratio": 1.5797101449275361, "no_speech_prob": 1.8342610928812064e-05}, {"id": 1190, "seek": 579628, "start": 5796.28, "end": 5801.679999999999, "text": " For example, the $1 million Netflix price.", "tokens": [1171, 1365, 11, 264, 1848, 16, 2459, 12778, 3218, 13], "temperature": 0.0, "avg_logprob": -0.1429657470889208, "compression_ratio": 1.5845410628019323, "no_speech_prob": 4.09285576097318e-06}, {"id": 1191, "seek": 579628, "start": 5801.679999999999, "end": 5806.96, "text": " What rating level will this person give this movie?", "tokens": [708, 10990, 1496, 486, 341, 954, 976, 341, 3169, 30], "temperature": 0.0, "avg_logprob": -0.1429657470889208, "compression_ratio": 1.5845410628019323, "no_speech_prob": 4.09285576097318e-06}, {"id": 1192, "seek": 579628, "start": 5806.96, "end": 5810.44, "text": " If you're writing Amazon's recommender system to figure out what to show you on their home", "tokens": [759, 291, 434, 3579, 6795, 311, 2748, 260, 1185, 281, 2573, 484, 437, 281, 855, 291, 322, 641, 1280], "temperature": 0.0, "avg_logprob": -0.1429657470889208, "compression_ratio": 1.5845410628019323, "no_speech_prob": 4.09285576097318e-06}, {"id": 1193, "seek": 579628, "start": 5810.44, "end": 5818.04, "text": " page, which products is this person likely to rate highly?", "tokens": [3028, 11, 597, 3383, 307, 341, 954, 3700, 281, 3314, 5405, 30], "temperature": 0.0, "avg_logprob": -0.1429657470889208, "compression_ratio": 1.5845410628019323, "no_speech_prob": 4.09285576097318e-06}, {"id": 1194, "seek": 579628, "start": 5818.04, "end": 5822.88, "text": " If you're trying to figure out what stuff to show on a news feed, which articles is", "tokens": [759, 291, 434, 1382, 281, 2573, 484, 437, 1507, 281, 855, 322, 257, 2583, 3154, 11, 597, 11290, 307], "temperature": 0.0, "avg_logprob": -0.1429657470889208, "compression_ratio": 1.5845410628019323, "no_speech_prob": 4.09285576097318e-06}, {"id": 1195, "seek": 582288, "start": 5822.88, "end": 5826.88, "text": " this person likely to enjoy reading?", "tokens": [341, 954, 3700, 281, 2103, 3760, 30], "temperature": 0.0, "avg_logprob": -0.13555595466682502, "compression_ratio": 1.628352490421456, "no_speech_prob": 5.68233554076869e-06}, {"id": 1196, "seek": 582288, "start": 5826.88, "end": 5830.96, "text": " There's a lot of different ways of doing this, but broadly speaking, there are two main classifications", "tokens": [821, 311, 257, 688, 295, 819, 2098, 295, 884, 341, 11, 457, 19511, 4124, 11, 456, 366, 732, 2135, 1508, 7833], "temperature": 0.0, "avg_logprob": -0.13555595466682502, "compression_ratio": 1.628352490421456, "no_speech_prob": 5.68233554076869e-06}, {"id": 1197, "seek": 582288, "start": 5830.96, "end": 5833.04, "text": " of recommender systems.", "tokens": [295, 2748, 260, 3652, 13], "temperature": 0.0, "avg_logprob": -0.13555595466682502, "compression_ratio": 1.628352490421456, "no_speech_prob": 5.68233554076869e-06}, {"id": 1198, "seek": 582288, "start": 5833.04, "end": 5839.84, "text": " One is based on metadata, which is for example, this guy filled out a survey in which they", "tokens": [1485, 307, 2361, 322, 26603, 11, 597, 307, 337, 1365, 11, 341, 2146, 6412, 484, 257, 8984, 294, 597, 436], "temperature": 0.0, "avg_logprob": -0.13555595466682502, "compression_ratio": 1.628352490421456, "no_speech_prob": 5.68233554076869e-06}, {"id": 1199, "seek": 582288, "start": 5839.84, "end": 5843.6, "text": " said they liked action movies and sci-fi.", "tokens": [848, 436, 4501, 3069, 6233, 293, 2180, 12, 13325, 13], "temperature": 0.0, "avg_logprob": -0.13555595466682502, "compression_ratio": 1.628352490421456, "no_speech_prob": 5.68233554076869e-06}, {"id": 1200, "seek": 582288, "start": 5843.6, "end": 5848.32, "text": " We also have taken all of our movies and put them into genres, and here are all of our", "tokens": [492, 611, 362, 2726, 439, 295, 527, 6233, 293, 829, 552, 666, 30057, 11, 293, 510, 366, 439, 295, 527], "temperature": 0.0, "avg_logprob": -0.13555595466682502, "compression_ratio": 1.628352490421456, "no_speech_prob": 5.68233554076869e-06}, {"id": 1201, "seek": 582288, "start": 5848.32, "end": 5851.92, "text": " action sci-fi movies, so we'll use them.", "tokens": [3069, 2180, 12, 13325, 6233, 11, 370, 321, 603, 764, 552, 13], "temperature": 0.0, "avg_logprob": -0.13555595466682502, "compression_ratio": 1.628352490421456, "no_speech_prob": 5.68233554076869e-06}, {"id": 1202, "seek": 585192, "start": 5851.92, "end": 5856.08, "text": " Broadly speaking, that would be a metadata-based approach.", "tokens": [14074, 356, 4124, 11, 300, 576, 312, 257, 26603, 12, 6032, 3109, 13], "temperature": 0.0, "avg_logprob": -0.1687659037481878, "compression_ratio": 1.7456140350877194, "no_speech_prob": 6.747974566678749e-06}, {"id": 1203, "seek": 585192, "start": 5856.08, "end": 5859.08, "text": " A collaborative filtering-based approach is very different.", "tokens": [316, 16555, 30822, 12, 6032, 3109, 307, 588, 819, 13], "temperature": 0.0, "avg_logprob": -0.1687659037481878, "compression_ratio": 1.7456140350877194, "no_speech_prob": 6.747974566678749e-06}, {"id": 1204, "seek": 585192, "start": 5859.08, "end": 5866.76, "text": " It says, let's find other people like you and find out what they liked and assume that", "tokens": [467, 1619, 11, 718, 311, 915, 661, 561, 411, 291, 293, 915, 484, 437, 436, 4501, 293, 6552, 300], "temperature": 0.0, "avg_logprob": -0.1687659037481878, "compression_ratio": 1.7456140350877194, "no_speech_prob": 6.747974566678749e-06}, {"id": 1205, "seek": 585192, "start": 5866.76, "end": 5869.56, "text": " you will like the same stuff.", "tokens": [291, 486, 411, 264, 912, 1507, 13], "temperature": 0.0, "avg_logprob": -0.1687659037481878, "compression_ratio": 1.7456140350877194, "no_speech_prob": 6.747974566678749e-06}, {"id": 1206, "seek": 585192, "start": 5869.56, "end": 5874.92, "text": " And specifically when we say people like you, we mean people who rated the same movies you've", "tokens": [400, 4682, 562, 321, 584, 561, 411, 291, 11, 321, 914, 561, 567, 22103, 264, 912, 6233, 291, 600], "temperature": 0.0, "avg_logprob": -0.1687659037481878, "compression_ratio": 1.7456140350877194, "no_speech_prob": 6.747974566678749e-06}, {"id": 1207, "seek": 585192, "start": 5874.92, "end": 5878.32, "text": " watched in a similar way.", "tokens": [6337, 294, 257, 2531, 636, 13], "temperature": 0.0, "avg_logprob": -0.1687659037481878, "compression_ratio": 1.7456140350877194, "no_speech_prob": 6.747974566678749e-06}, {"id": 1208, "seek": 585192, "start": 5878.32, "end": 5880.1, "text": " And that's called collaborative filtering.", "tokens": [400, 300, 311, 1219, 16555, 30822, 13], "temperature": 0.0, "avg_logprob": -0.1687659037481878, "compression_ratio": 1.7456140350877194, "no_speech_prob": 6.747974566678749e-06}, {"id": 1209, "seek": 588010, "start": 5880.1, "end": 5885.72, "text": " It turns out that in a large enough dataset, collaborative filtering is so much better", "tokens": [467, 4523, 484, 300, 294, 257, 2416, 1547, 28872, 11, 16555, 30822, 307, 370, 709, 1101], "temperature": 0.0, "avg_logprob": -0.09858393669128418, "compression_ratio": 1.678714859437751, "no_speech_prob": 5.173856607143534e-06}, {"id": 1210, "seek": 588010, "start": 5885.72, "end": 5891.320000000001, "text": " than the metadata-based approaches that adding metadata doesn't even improve it at all.", "tokens": [813, 264, 26603, 12, 6032, 11587, 300, 5127, 26603, 1177, 380, 754, 3470, 309, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.09858393669128418, "compression_ratio": 1.678714859437751, "no_speech_prob": 5.173856607143534e-06}, {"id": 1211, "seek": 588010, "start": 5891.320000000001, "end": 5897.320000000001, "text": " So when people in the Netflix prize actually went out to IMDB and sucked in additional", "tokens": [407, 562, 561, 294, 264, 12778, 12818, 767, 1437, 484, 281, 21463, 27735, 293, 26503, 294, 4497], "temperature": 0.0, "avg_logprob": -0.09858393669128418, "compression_ratio": 1.678714859437751, "no_speech_prob": 5.173856607143534e-06}, {"id": 1212, "seek": 588010, "start": 5897.320000000001, "end": 5903.88, "text": " data and tried to use that to make it better, at a certain point it didn't help.", "tokens": [1412, 293, 3031, 281, 764, 300, 281, 652, 309, 1101, 11, 412, 257, 1629, 935, 309, 994, 380, 854, 13], "temperature": 0.0, "avg_logprob": -0.09858393669128418, "compression_ratio": 1.678714859437751, "no_speech_prob": 5.173856607143534e-06}, {"id": 1213, "seek": 588010, "start": 5903.88, "end": 5906.4800000000005, "text": " Once their collaborative filtering models were good enough, it didn't help.", "tokens": [3443, 641, 16555, 30822, 5245, 645, 665, 1547, 11, 309, 994, 380, 854, 13], "temperature": 0.0, "avg_logprob": -0.09858393669128418, "compression_ratio": 1.678714859437751, "no_speech_prob": 5.173856607143534e-06}, {"id": 1214, "seek": 590648, "start": 5906.48, "end": 5910.0, "text": " And that's because it's something I learned about 20 years ago when I used to do a lot", "tokens": [400, 300, 311, 570, 309, 311, 746, 286, 3264, 466, 945, 924, 2057, 562, 286, 1143, 281, 360, 257, 688], "temperature": 0.0, "avg_logprob": -0.1488762209492345, "compression_ratio": 1.6363636363636365, "no_speech_prob": 6.375516363732459e-07}, {"id": 1215, "seek": 590648, "start": 5910.0, "end": 5911.759999999999, "text": " of surveys and consulting.", "tokens": [295, 22711, 293, 23682, 13], "temperature": 0.0, "avg_logprob": -0.1488762209492345, "compression_ratio": 1.6363636363636365, "no_speech_prob": 6.375516363732459e-07}, {"id": 1216, "seek": 590648, "start": 5911.759999999999, "end": 5916.719999999999, "text": " It turns out that asking people about their behavior is crap compared to actually looking", "tokens": [467, 4523, 484, 300, 3365, 561, 466, 641, 5223, 307, 12426, 5347, 281, 767, 1237], "temperature": 0.0, "avg_logprob": -0.1488762209492345, "compression_ratio": 1.6363636363636365, "no_speech_prob": 6.375516363732459e-07}, {"id": 1217, "seek": 590648, "start": 5916.719999999999, "end": 5919.0, "text": " at people's behavior.", "tokens": [412, 561, 311, 5223, 13], "temperature": 0.0, "avg_logprob": -0.1488762209492345, "compression_ratio": 1.6363636363636365, "no_speech_prob": 6.375516363732459e-07}, {"id": 1218, "seek": 590648, "start": 5919.0, "end": 5922.12, "text": " So let me show you what collaborative filtering looks like.", "tokens": [407, 718, 385, 855, 291, 437, 16555, 30822, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.1488762209492345, "compression_ratio": 1.6363636363636365, "no_speech_prob": 6.375516363732459e-07}, {"id": 1219, "seek": 590648, "start": 5922.12, "end": 5925.599999999999, "text": " What we're going to do is we're going to use a dataset called MovieLens.", "tokens": [708, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 764, 257, 28872, 1219, 28766, 43, 694, 13], "temperature": 0.0, "avg_logprob": -0.1488762209492345, "compression_ratio": 1.6363636363636365, "no_speech_prob": 6.375516363732459e-07}, {"id": 1220, "seek": 590648, "start": 5925.599999999999, "end": 5930.44, "text": " So you guys hopefully will be able to play around with this this week.", "tokens": [407, 291, 1074, 4696, 486, 312, 1075, 281, 862, 926, 365, 341, 341, 1243, 13], "temperature": 0.0, "avg_logprob": -0.1488762209492345, "compression_ratio": 1.6363636363636365, "no_speech_prob": 6.375516363732459e-07}, {"id": 1221, "seek": 590648, "start": 5930.44, "end": 5936.28, "text": " Unfortunately, Rachel and I could not find any Kaggle competitions that were about recommend", "tokens": [8590, 11, 14246, 293, 286, 727, 406, 915, 604, 48751, 22631, 26185, 300, 645, 466, 2748], "temperature": 0.0, "avg_logprob": -0.1488762209492345, "compression_ratio": 1.6363636363636365, "no_speech_prob": 6.375516363732459e-07}, {"id": 1222, "seek": 593628, "start": 5936.28, "end": 5948.599999999999, "text": " agenda systems and where the competitions were still open for entries.", "tokens": [9829, 3652, 293, 689, 264, 26185, 645, 920, 1269, 337, 23041, 13], "temperature": 0.0, "avg_logprob": -0.21844556254725303, "compression_ratio": 1.5240641711229947, "no_speech_prob": 1.0952871889458038e-05}, {"id": 1223, "seek": 593628, "start": 5948.599999999999, "end": 5954.599999999999, "text": " However there is something called MovieLens, which is a widely studied dataset in academia.", "tokens": [2908, 456, 307, 746, 1219, 28766, 43, 694, 11, 597, 307, 257, 13371, 9454, 28872, 294, 28937, 13], "temperature": 0.0, "avg_logprob": -0.21844556254725303, "compression_ratio": 1.5240641711229947, "no_speech_prob": 1.0952871889458038e-05}, {"id": 1224, "seek": 593628, "start": 5954.599999999999, "end": 5961.04, "text": " Perhaps surprisingly, approaching or beating an academic state of the art is way easier", "tokens": [10517, 17600, 11, 14908, 420, 13497, 364, 7778, 1785, 295, 264, 1523, 307, 636, 3571], "temperature": 0.0, "avg_logprob": -0.21844556254725303, "compression_ratio": 1.5240641711229947, "no_speech_prob": 1.0952871889458038e-05}, {"id": 1225, "seek": 593628, "start": 5961.04, "end": 5962.759999999999, "text": " than winning a Kaggle competition.", "tokens": [813, 8224, 257, 48751, 22631, 6211, 13], "temperature": 0.0, "avg_logprob": -0.21844556254725303, "compression_ratio": 1.5240641711229947, "no_speech_prob": 1.0952871889458038e-05}, {"id": 1226, "seek": 596276, "start": 5962.76, "end": 5966.76, "text": " In Kaggle competitions, lots and lots and lots of people look at that data and they", "tokens": [682, 48751, 22631, 26185, 11, 3195, 293, 3195, 293, 3195, 295, 561, 574, 412, 300, 1412, 293, 436], "temperature": 0.0, "avg_logprob": -0.12811127415409795, "compression_ratio": 1.8571428571428572, "no_speech_prob": 5.1738520596700255e-06}, {"id": 1227, "seek": 596276, "start": 5966.76, "end": 5972.24, "text": " try lots and lots and lots of things and they use a really pragmatic approach, whereas academic", "tokens": [853, 3195, 293, 3195, 293, 3195, 295, 721, 293, 436, 764, 257, 534, 46904, 3109, 11, 9735, 7778], "temperature": 0.0, "avg_logprob": -0.12811127415409795, "compression_ratio": 1.8571428571428572, "no_speech_prob": 5.1738520596700255e-06}, {"id": 1228, "seek": 596276, "start": 5972.24, "end": 5975.4800000000005, "text": " state of the arts are done by academics.", "tokens": [1785, 295, 264, 8609, 366, 1096, 538, 25695, 13], "temperature": 0.0, "avg_logprob": -0.12811127415409795, "compression_ratio": 1.8571428571428572, "no_speech_prob": 5.1738520596700255e-06}, {"id": 1229, "seek": 596276, "start": 5975.4800000000005, "end": 5982.12, "text": " So with that said, the MovieLens benchmarks are going to be much easier to beat than any", "tokens": [407, 365, 300, 848, 11, 264, 28766, 43, 694, 43751, 366, 516, 281, 312, 709, 3571, 281, 4224, 813, 604], "temperature": 0.0, "avg_logprob": -0.12811127415409795, "compression_ratio": 1.8571428571428572, "no_speech_prob": 5.1738520596700255e-06}, {"id": 1230, "seek": 596276, "start": 5982.12, "end": 5986.6, "text": " Kaggle competition, but it's still interesting.", "tokens": [48751, 22631, 6211, 11, 457, 309, 311, 920, 1880, 13], "temperature": 0.0, "avg_logprob": -0.12811127415409795, "compression_ratio": 1.8571428571428572, "no_speech_prob": 5.1738520596700255e-06}, {"id": 1231, "seek": 596276, "start": 5986.6, "end": 5992.2, "text": " So you can download MovieLens dataset from the MovieLens dataset website, and you'll", "tokens": [407, 291, 393, 5484, 28766, 43, 694, 28872, 490, 264, 28766, 43, 694, 28872, 3144, 11, 293, 291, 603], "temperature": 0.0, "avg_logprob": -0.12811127415409795, "compression_ratio": 1.8571428571428572, "no_speech_prob": 5.1738520596700255e-06}, {"id": 1232, "seek": 599220, "start": 5992.2, "end": 5997.96, "text": " see that there's one here recommended for new research with 20 million items in.", "tokens": [536, 300, 456, 311, 472, 510, 9628, 337, 777, 2132, 365, 945, 2459, 4754, 294, 13], "temperature": 0.0, "avg_logprob": -0.18189880439827033, "compression_ratio": 1.6196581196581197, "no_speech_prob": 2.7968240829068236e-05}, {"id": 1233, "seek": 599220, "start": 5997.96, "end": 6001.54, "text": " Also conveniently, they have a small one with only 100,000 ratings.", "tokens": [2743, 44375, 11, 436, 362, 257, 1359, 472, 365, 787, 2319, 11, 1360, 24603, 13], "temperature": 0.0, "avg_logprob": -0.18189880439827033, "compression_ratio": 1.6196581196581197, "no_speech_prob": 2.7968240829068236e-05}, {"id": 1234, "seek": 599220, "start": 6001.54, "end": 6005.28, "text": " So you don't have to build a sample, they have already built a sample for you.", "tokens": [407, 291, 500, 380, 362, 281, 1322, 257, 6889, 11, 436, 362, 1217, 3094, 257, 6889, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.18189880439827033, "compression_ratio": 1.6196581196581197, "no_speech_prob": 2.7968240829068236e-05}, {"id": 1235, "seek": 599220, "start": 6005.28, "end": 6009.12, "text": " So I am of course going to use a sample.", "tokens": [407, 286, 669, 295, 1164, 516, 281, 764, 257, 6889, 13], "temperature": 0.0, "avg_logprob": -0.18189880439827033, "compression_ratio": 1.6196581196581197, "no_speech_prob": 2.7968240829068236e-05}, {"id": 1236, "seek": 599220, "start": 6009.12, "end": 6014.32, "text": " So what I do is I read in ratings.csv.", "tokens": [407, 437, 286, 360, 307, 286, 1401, 294, 24603, 13, 14368, 85, 13], "temperature": 0.0, "avg_logprob": -0.18189880439827033, "compression_ratio": 1.6196581196581197, "no_speech_prob": 2.7968240829068236e-05}, {"id": 1237, "seek": 599220, "start": 6014.32, "end": 6016.72, "text": " And as you'll see here, I've started using pandas.", "tokens": [400, 382, 291, 603, 536, 510, 11, 286, 600, 1409, 1228, 4565, 296, 13], "temperature": 0.0, "avg_logprob": -0.18189880439827033, "compression_ratio": 1.6196581196581197, "no_speech_prob": 2.7968240829068236e-05}, {"id": 1238, "seek": 599220, "start": 6016.72, "end": 6019.5199999999995, "text": " PD is PD for pandas.", "tokens": [10464, 307, 10464, 337, 4565, 296, 13], "temperature": 0.0, "avg_logprob": -0.18189880439827033, "compression_ratio": 1.6196581196581197, "no_speech_prob": 2.7968240829068236e-05}, {"id": 1239, "seek": 601952, "start": 6019.52, "end": 6022.52, "text": " How many people here have tried pandas?", "tokens": [1012, 867, 561, 510, 362, 3031, 4565, 296, 30], "temperature": 0.0, "avg_logprob": -0.17117698669433593, "compression_ratio": 1.6304347826086956, "no_speech_prob": 9.665917787060607e-06}, {"id": 1240, "seek": 601952, "start": 6022.52, "end": 6023.52, "text": " Awesome.", "tokens": [10391, 13], "temperature": 0.0, "avg_logprob": -0.17117698669433593, "compression_ratio": 1.6304347826086956, "no_speech_prob": 9.665917787060607e-06}, {"id": 1241, "seek": 601952, "start": 6023.52, "end": 6027.4800000000005, "text": " So those of you that don't, hopefully the peer-group pressure is kicking in.", "tokens": [407, 729, 295, 291, 300, 500, 380, 11, 4696, 264, 15108, 12, 17377, 3321, 307, 19137, 294, 13], "temperature": 0.0, "avg_logprob": -0.17117698669433593, "compression_ratio": 1.6304347826086956, "no_speech_prob": 9.665917787060607e-06}, {"id": 1242, "seek": 601952, "start": 6027.4800000000005, "end": 6032.080000000001, "text": " So pandas is a great way of dealing with structured data and you should use it.", "tokens": [407, 4565, 296, 307, 257, 869, 636, 295, 6260, 365, 18519, 1412, 293, 291, 820, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.17117698669433593, "compression_ratio": 1.6304347826086956, "no_speech_prob": 9.665917787060607e-06}, {"id": 1243, "seek": 601952, "start": 6032.080000000001, "end": 6037.68, "text": " Reading a CSV file is this easy, showing the first few items is this easy, finding out", "tokens": [29766, 257, 48814, 3991, 307, 341, 1858, 11, 4099, 264, 700, 1326, 4754, 307, 341, 1858, 11, 5006, 484], "temperature": 0.0, "avg_logprob": -0.17117698669433593, "compression_ratio": 1.6304347826086956, "no_speech_prob": 9.665917787060607e-06}, {"id": 1244, "seek": 601952, "start": 6037.68, "end": 6046.400000000001, "text": " how big it is, finding out how many users and movies there are, are all this easy.", "tokens": [577, 955, 309, 307, 11, 5006, 484, 577, 867, 5022, 293, 6233, 456, 366, 11, 366, 439, 341, 1858, 13], "temperature": 0.0, "avg_logprob": -0.17117698669433593, "compression_ratio": 1.6304347826086956, "no_speech_prob": 9.665917787060607e-06}, {"id": 1245, "seek": 604640, "start": 6046.4, "end": 6052.2, "text": " I wanted to play with this in Excel, because that's the only way I know how to teach.", "tokens": [286, 1415, 281, 862, 365, 341, 294, 19060, 11, 570, 300, 311, 264, 787, 636, 286, 458, 577, 281, 2924, 13], "temperature": 0.0, "avg_logprob": -0.18153520648399096, "compression_ratio": 1.609375, "no_speech_prob": 9.972840416594408e-06}, {"id": 1246, "seek": 604640, "start": 6052.2, "end": 6063.16, "text": " What I did was I grabbed the user ID by rating and grabbed the top 15 busiest movie-watching", "tokens": [708, 286, 630, 390, 286, 18607, 264, 4195, 7348, 538, 10990, 293, 18607, 264, 1192, 2119, 1255, 6495, 3169, 12, 15219, 278], "temperature": 0.0, "avg_logprob": -0.18153520648399096, "compression_ratio": 1.609375, "no_speech_prob": 9.972840416594408e-06}, {"id": 1247, "seek": 604640, "start": 6063.16, "end": 6071.759999999999, "text": " users, and then I grabbed the 15 most-watched movies, and then I created a crosstab of the", "tokens": [5022, 11, 293, 550, 286, 18607, 264, 2119, 881, 12, 15219, 292, 6233, 11, 293, 550, 286, 2942, 257, 28108, 372, 455, 295, 264], "temperature": 0.0, "avg_logprob": -0.18153520648399096, "compression_ratio": 1.609375, "no_speech_prob": 9.972840416594408e-06}, {"id": 1248, "seek": 604640, "start": 6071.759999999999, "end": 6073.32, "text": " two.", "tokens": [732, 13], "temperature": 0.0, "avg_logprob": -0.18153520648399096, "compression_ratio": 1.609375, "no_speech_prob": 9.972840416594408e-06}, {"id": 1249, "seek": 604640, "start": 6073.32, "end": 6076.36, "text": " And then I copied that into Excel.", "tokens": [400, 550, 286, 25365, 300, 666, 19060, 13], "temperature": 0.0, "avg_logprob": -0.18153520648399096, "compression_ratio": 1.609375, "no_speech_prob": 9.972840416594408e-06}, {"id": 1250, "seek": 607636, "start": 6076.36, "end": 6087.44, "text": " Here is the table I downloaded from MovieLens for the 15 busiest movie-watching users and", "tokens": [1692, 307, 264, 3199, 286, 21748, 490, 28766, 43, 694, 337, 264, 2119, 1255, 6495, 3169, 12, 15219, 278, 5022, 293], "temperature": 0.0, "avg_logprob": -0.19922892252604166, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.9832970792776905e-05}, {"id": 1251, "seek": 607636, "start": 6087.44, "end": 6090.599999999999, "text": " the 15 most widely-watched movies.", "tokens": [264, 2119, 881, 13371, 12, 15219, 292, 6233, 13], "temperature": 0.0, "avg_logprob": -0.19922892252604166, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.9832970792776905e-05}, {"id": 1252, "seek": 607636, "start": 6090.599999999999, "end": 6091.599999999999, "text": " And here are the ratings.", "tokens": [400, 510, 366, 264, 24603, 13], "temperature": 0.0, "avg_logprob": -0.19922892252604166, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.9832970792776905e-05}, {"id": 1253, "seek": 607636, "start": 6091.599999999999, "end": 6095.48, "text": " Here's the rating of user 14 for movie 27.", "tokens": [1692, 311, 264, 10990, 295, 4195, 3499, 337, 3169, 7634, 13], "temperature": 0.0, "avg_logprob": -0.19922892252604166, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.9832970792776905e-05}, {"id": 1254, "seek": 607636, "start": 6095.48, "end": 6096.48, "text": " Look at these guys.", "tokens": [2053, 412, 613, 1074, 13], "temperature": 0.0, "avg_logprob": -0.19922892252604166, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.9832970792776905e-05}, {"id": 1255, "seek": 607636, "start": 6096.48, "end": 6100.28, "text": " These 3 users have watched every single one of these movies.", "tokens": [1981, 805, 5022, 362, 6337, 633, 2167, 472, 295, 613, 6233, 13], "temperature": 0.0, "avg_logprob": -0.19922892252604166, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.9832970792776905e-05}, {"id": 1256, "seek": 607636, "start": 6100.28, "end": 6101.28, "text": " I'm probably one of them.", "tokens": [286, 478, 1391, 472, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.19922892252604166, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.9832970792776905e-05}, {"id": 1257, "seek": 607636, "start": 6101.28, "end": 6105.679999999999, "text": " I love movies.", "tokens": [286, 959, 6233, 13], "temperature": 0.0, "avg_logprob": -0.19922892252604166, "compression_ratio": 1.6153846153846154, "no_speech_prob": 1.9832970792776905e-05}, {"id": 1258, "seek": 610568, "start": 6105.68, "end": 6109.68, "text": " And these have been watched by every single one of these users.", "tokens": [400, 613, 362, 668, 6337, 538, 633, 2167, 472, 295, 613, 5022, 13], "temperature": 0.0, "avg_logprob": -0.17696757938550867, "compression_ratio": 1.5304878048780488, "no_speech_prob": 9.223370398103725e-06}, {"id": 1259, "seek": 610568, "start": 6109.68, "end": 6118.12, "text": " So user 14 kind of liked movie 27, loved movie 49, hated movie 51.", "tokens": [407, 4195, 3499, 733, 295, 4501, 3169, 7634, 11, 4333, 3169, 16513, 11, 17398, 3169, 18485, 13], "temperature": 0.0, "avg_logprob": -0.17696757938550867, "compression_ratio": 1.5304878048780488, "no_speech_prob": 9.223370398103725e-06}, {"id": 1260, "seek": 610568, "start": 6118.12, "end": 6130.84, "text": " So this guy really liked movie 49, didn't much like movie 57, so they may feel the same", "tokens": [407, 341, 2146, 534, 4501, 3169, 16513, 11, 994, 380, 709, 411, 3169, 21423, 11, 370, 436, 815, 841, 264, 912], "temperature": 0.0, "avg_logprob": -0.17696757938550867, "compression_ratio": 1.5304878048780488, "no_speech_prob": 9.223370398103725e-06}, {"id": 1261, "seek": 610568, "start": 6130.84, "end": 6133.84, "text": " way about movie 27 as that user.", "tokens": [636, 466, 3169, 7634, 382, 300, 4195, 13], "temperature": 0.0, "avg_logprob": -0.17696757938550867, "compression_ratio": 1.5304878048780488, "no_speech_prob": 9.223370398103725e-06}, {"id": 1262, "seek": 613384, "start": 6133.84, "end": 6135.84, "text": " That's the basic essence of collaborative filtering.", "tokens": [663, 311, 264, 3875, 12801, 295, 16555, 30822, 13], "temperature": 0.0, "avg_logprob": -0.20833998460036057, "compression_ratio": 1.6169154228855722, "no_speech_prob": 2.7108531867270358e-05}, {"id": 1263, "seek": 613384, "start": 6135.84, "end": 6138.6, "text": " We're going to try and automate it a little bit.", "tokens": [492, 434, 516, 281, 853, 293, 31605, 309, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.20833998460036057, "compression_ratio": 1.6169154228855722, "no_speech_prob": 2.7108531867270358e-05}, {"id": 1264, "seek": 613384, "start": 6138.6, "end": 6141.84, "text": " And the way we're going to automate it is we're going to say, let's pretend for each", "tokens": [400, 264, 636, 321, 434, 516, 281, 31605, 309, 307, 321, 434, 516, 281, 584, 11, 718, 311, 11865, 337, 1184], "temperature": 0.0, "avg_logprob": -0.20833998460036057, "compression_ratio": 1.6169154228855722, "no_speech_prob": 2.7108531867270358e-05}, {"id": 1265, "seek": 613384, "start": 6141.84, "end": 6150.0, "text": " movie we had 5 characteristics, which is like, is it sci-fi, is it action, is it dialogue", "tokens": [3169, 321, 632, 1025, 10891, 11, 597, 307, 411, 11, 307, 309, 2180, 12, 13325, 11, 307, 309, 3069, 11, 307, 309, 10221], "temperature": 0.0, "avg_logprob": -0.20833998460036057, "compression_ratio": 1.6169154228855722, "no_speech_prob": 2.7108531867270358e-05}, {"id": 1266, "seek": 613384, "start": 6150.0, "end": 6158.400000000001, "text": " heavy, is it new, and does it have Bruce Willis?", "tokens": [4676, 11, 307, 309, 777, 11, 293, 775, 309, 362, 15429, 3099, 271, 30], "temperature": 0.0, "avg_logprob": -0.20833998460036057, "compression_ratio": 1.6169154228855722, "no_speech_prob": 2.7108531867270358e-05}, {"id": 1267, "seek": 615840, "start": 6158.4, "end": 6171.4, "text": " And then we could have those 5 things for every user as well.", "tokens": [400, 550, 321, 727, 362, 729, 1025, 721, 337, 633, 4195, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.20732473191760836, "compression_ratio": 1.2113821138211383, "no_speech_prob": 3.6119636206422e-06}, {"id": 1268, "seek": 615840, "start": 6171.4, "end": 6177.04, "text": " Is this user somebody who likes sci-fi, action, dialogue, new movies, and Bruce Willis?", "tokens": [1119, 341, 4195, 2618, 567, 5902, 2180, 12, 13325, 11, 3069, 11, 10221, 11, 777, 6233, 11, 293, 15429, 3099, 271, 30], "temperature": 0.0, "avg_logprob": -0.20732473191760836, "compression_ratio": 1.2113821138211383, "no_speech_prob": 3.6119636206422e-06}, {"id": 1269, "seek": 617704, "start": 6177.04, "end": 6190.2, "text": " And so what we could then do is multiply that set of user features with that set of movie", "tokens": [400, 370, 437, 321, 727, 550, 360, 307, 12972, 300, 992, 295, 4195, 4122, 365, 300, 992, 295, 3169], "temperature": 0.0, "avg_logprob": -0.18636969445456922, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.8448137097948347e-06}, {"id": 1270, "seek": 617704, "start": 6190.2, "end": 6191.48, "text": " features.", "tokens": [4122, 13], "temperature": 0.0, "avg_logprob": -0.18636969445456922, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.8448137097948347e-06}, {"id": 1271, "seek": 617704, "start": 6191.48, "end": 6196.32, "text": " If this person likes sci-fi and it's sci-fi, and they like action and it is action, then", "tokens": [759, 341, 954, 5902, 2180, 12, 13325, 293, 309, 311, 2180, 12, 13325, 11, 293, 436, 411, 3069, 293, 309, 307, 3069, 11, 550], "temperature": 0.0, "avg_logprob": -0.18636969445456922, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.8448137097948347e-06}, {"id": 1272, "seek": 617704, "start": 6196.32, "end": 6205.7, "text": " a high number will appear in here for this matrix product of these two vectors.", "tokens": [257, 1090, 1230, 486, 4204, 294, 510, 337, 341, 8141, 1674, 295, 613, 732, 18875, 13], "temperature": 0.0, "avg_logprob": -0.18636969445456922, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.8448137097948347e-06}, {"id": 1273, "seek": 620570, "start": 6205.7, "end": 6213.2, "text": " And so this would be a cool way to build up a collaborative filtering system if only we", "tokens": [400, 370, 341, 576, 312, 257, 1627, 636, 281, 1322, 493, 257, 16555, 30822, 1185, 498, 787, 321], "temperature": 0.0, "avg_logprob": -0.11378808816274007, "compression_ratio": 1.775330396475771, "no_speech_prob": 1.9333522232045652e-06}, {"id": 1274, "seek": 620570, "start": 6213.2, "end": 6220.96, "text": " could create these 5 items for every movie and for every user.", "tokens": [727, 1884, 613, 1025, 4754, 337, 633, 3169, 293, 337, 633, 4195, 13], "temperature": 0.0, "avg_logprob": -0.11378808816274007, "compression_ratio": 1.775330396475771, "no_speech_prob": 1.9333522232045652e-06}, {"id": 1275, "seek": 620570, "start": 6220.96, "end": 6225.72, "text": " Now because we don't actually know what 5 things are most important for users and what", "tokens": [823, 570, 321, 500, 380, 767, 458, 437, 1025, 721, 366, 881, 1021, 337, 5022, 293, 437], "temperature": 0.0, "avg_logprob": -0.11378808816274007, "compression_ratio": 1.775330396475771, "no_speech_prob": 1.9333522232045652e-06}, {"id": 1276, "seek": 620570, "start": 6225.72, "end": 6230.599999999999, "text": " 5 things are most important for movies, we're instead going to learn them.", "tokens": [1025, 721, 366, 881, 1021, 337, 6233, 11, 321, 434, 2602, 516, 281, 1466, 552, 13], "temperature": 0.0, "avg_logprob": -0.11378808816274007, "compression_ratio": 1.775330396475771, "no_speech_prob": 1.9333522232045652e-06}, {"id": 1277, "seek": 620570, "start": 6230.599999999999, "end": 6235.599999999999, "text": " And the way we learn them is the way we learn everything, which is we start by randomizing", "tokens": [400, 264, 636, 321, 1466, 552, 307, 264, 636, 321, 1466, 1203, 11, 597, 307, 321, 722, 538, 4974, 3319], "temperature": 0.0, "avg_logprob": -0.11378808816274007, "compression_ratio": 1.775330396475771, "no_speech_prob": 1.9333522232045652e-06}, {"id": 1278, "seek": 623560, "start": 6235.6, "end": 6239.4400000000005, "text": " them and then we use gradient descent.", "tokens": [552, 293, 550, 321, 764, 16235, 23475, 13], "temperature": 0.0, "avg_logprob": -0.08849304016322306, "compression_ratio": 1.7668711656441718, "no_speech_prob": 1.0616004146868363e-05}, {"id": 1279, "seek": 623560, "start": 6239.4400000000005, "end": 6248.320000000001, "text": " So here are 5 random numbers for every movie, and here are 5 random numbers for every user,", "tokens": [407, 510, 366, 1025, 4974, 3547, 337, 633, 3169, 11, 293, 510, 366, 1025, 4974, 3547, 337, 633, 4195, 11], "temperature": 0.0, "avg_logprob": -0.08849304016322306, "compression_ratio": 1.7668711656441718, "no_speech_prob": 1.0616004146868363e-05}, {"id": 1280, "seek": 623560, "start": 6248.320000000001, "end": 6254.88, "text": " and in the middle is the dot product of that movie with that user.", "tokens": [293, 294, 264, 2808, 307, 264, 5893, 1674, 295, 300, 3169, 365, 300, 4195, 13], "temperature": 0.0, "avg_logprob": -0.08849304016322306, "compression_ratio": 1.7668711656441718, "no_speech_prob": 1.0616004146868363e-05}, {"id": 1281, "seek": 623560, "start": 6254.88, "end": 6262.200000000001, "text": " Once we have a good set of movie factors and user factors for each one, then each of these", "tokens": [3443, 321, 362, 257, 665, 992, 295, 3169, 6771, 293, 4195, 6771, 337, 1184, 472, 11, 550, 1184, 295, 613], "temperature": 0.0, "avg_logprob": -0.08849304016322306, "compression_ratio": 1.7668711656441718, "no_speech_prob": 1.0616004146868363e-05}, {"id": 1282, "seek": 626220, "start": 6262.2, "end": 6266.08, "text": " ratings will be similar to each of the observed ratings.", "tokens": [24603, 486, 312, 2531, 281, 1184, 295, 264, 13095, 24603, 13], "temperature": 0.0, "avg_logprob": -0.13262354906867532, "compression_ratio": 1.6009852216748768, "no_speech_prob": 4.092892140761251e-06}, {"id": 1283, "seek": 626220, "start": 6266.08, "end": 6274.2, "text": " And therefore this sum of squared errors will be low.", "tokens": [400, 4412, 341, 2408, 295, 8889, 13603, 486, 312, 2295, 13], "temperature": 0.0, "avg_logprob": -0.13262354906867532, "compression_ratio": 1.6009852216748768, "no_speech_prob": 4.092892140761251e-06}, {"id": 1284, "seek": 626220, "start": 6274.2, "end": 6276.88, "text": " Currently it is high.", "tokens": [19964, 309, 307, 1090, 13], "temperature": 0.0, "avg_logprob": -0.13262354906867532, "compression_ratio": 1.6009852216748768, "no_speech_prob": 4.092892140761251e-06}, {"id": 1285, "seek": 626220, "start": 6276.88, "end": 6283.679999999999, "text": " So we start with our random numbers, we start with a loss function of 40.", "tokens": [407, 321, 722, 365, 527, 4974, 3547, 11, 321, 722, 365, 257, 4470, 2445, 295, 3356, 13], "temperature": 0.0, "avg_logprob": -0.13262354906867532, "compression_ratio": 1.6009852216748768, "no_speech_prob": 4.092892140761251e-06}, {"id": 1286, "seek": 626220, "start": 6283.679999999999, "end": 6289.44, "text": " So we now want to use gradient descent, and it turns out that every copy of Excel has", "tokens": [407, 321, 586, 528, 281, 764, 16235, 23475, 11, 293, 309, 4523, 484, 300, 633, 5055, 295, 19060, 575], "temperature": 0.0, "avg_logprob": -0.13262354906867532, "compression_ratio": 1.6009852216748768, "no_speech_prob": 4.092892140761251e-06}, {"id": 1287, "seek": 626220, "start": 6289.44, "end": 6291.72, "text": " a gradient descent solver in it.", "tokens": [257, 16235, 23475, 1404, 331, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.13262354906867532, "compression_ratio": 1.6009852216748768, "no_speech_prob": 4.092892140761251e-06}, {"id": 1288, "seek": 629172, "start": 6291.72, "end": 6293.96, "text": " So we're going to go ahead and use it.", "tokens": [407, 321, 434, 516, 281, 352, 2286, 293, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.12817269012707622, "compression_ratio": 1.8283261802575108, "no_speech_prob": 7.527936759288423e-06}, {"id": 1289, "seek": 629172, "start": 6293.96, "end": 6295.4400000000005, "text": " It's called solver.", "tokens": [467, 311, 1219, 1404, 331, 13], "temperature": 0.0, "avg_logprob": -0.12817269012707622, "compression_ratio": 1.8283261802575108, "no_speech_prob": 7.527936759288423e-06}, {"id": 1290, "seek": 629172, "start": 6295.4400000000005, "end": 6301.780000000001, "text": " And so we have to tell it what thing to minimize, so it's saying minimize this, and which things", "tokens": [400, 370, 321, 362, 281, 980, 309, 437, 551, 281, 17522, 11, 370, 309, 311, 1566, 17522, 341, 11, 293, 597, 721], "temperature": 0.0, "avg_logprob": -0.12817269012707622, "compression_ratio": 1.8283261802575108, "no_speech_prob": 7.527936759288423e-06}, {"id": 1291, "seek": 629172, "start": 6301.780000000001, "end": 6307.4800000000005, "text": " do we want to change, which is all of our factors, and then we set it to a minimum and", "tokens": [360, 321, 528, 281, 1319, 11, 597, 307, 439, 295, 527, 6771, 11, 293, 550, 321, 992, 309, 281, 257, 7285, 293], "temperature": 0.0, "avg_logprob": -0.12817269012707622, "compression_ratio": 1.8283261802575108, "no_speech_prob": 7.527936759288423e-06}, {"id": 1292, "seek": 629172, "start": 6307.4800000000005, "end": 6309.56, "text": " we say solve.", "tokens": [321, 584, 5039, 13], "temperature": 0.0, "avg_logprob": -0.12817269012707622, "compression_ratio": 1.8283261802575108, "no_speech_prob": 7.527936759288423e-06}, {"id": 1293, "seek": 629172, "start": 6309.56, "end": 6313.84, "text": " And then we can see in the bottom left, it is trying to make this better and better and", "tokens": [400, 550, 321, 393, 536, 294, 264, 2767, 1411, 11, 309, 307, 1382, 281, 652, 341, 1101, 293, 1101, 293], "temperature": 0.0, "avg_logprob": -0.12817269012707622, "compression_ratio": 1.8283261802575108, "no_speech_prob": 7.527936759288423e-06}, {"id": 1294, "seek": 629172, "start": 6313.84, "end": 6316.88, "text": " better using gradient descent.", "tokens": [1101, 1228, 16235, 23475, 13], "temperature": 0.0, "avg_logprob": -0.12817269012707622, "compression_ratio": 1.8283261802575108, "no_speech_prob": 7.527936759288423e-06}, {"id": 1295, "seek": 629172, "start": 6316.88, "end": 6320.360000000001, "text": " Notice I'm not saying stochastic gradient descent.", "tokens": [13428, 286, 478, 406, 1566, 342, 8997, 2750, 16235, 23475, 13], "temperature": 0.0, "avg_logprob": -0.12817269012707622, "compression_ratio": 1.8283261802575108, "no_speech_prob": 7.527936759288423e-06}, {"id": 1296, "seek": 632036, "start": 6320.36, "end": 6324.679999999999, "text": " Stochastic gradient descent means it's doing a mini-batch at a mini-batch time.", "tokens": [745, 8997, 2750, 16235, 23475, 1355, 309, 311, 884, 257, 8382, 12, 65, 852, 412, 257, 8382, 12, 65, 852, 565, 13], "temperature": 0.0, "avg_logprob": -0.15407495576191724, "compression_ratio": 1.869047619047619, "no_speech_prob": 6.854270850453759e-06}, {"id": 1297, "seek": 632036, "start": 6324.679999999999, "end": 6328.12, "text": " Gradient descent means it's doing the whole dataset each time.", "tokens": [16710, 1196, 23475, 1355, 309, 311, 884, 264, 1379, 28872, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.15407495576191724, "compression_ratio": 1.869047619047619, "no_speech_prob": 6.854270850453759e-06}, {"id": 1298, "seek": 632036, "start": 6328.12, "end": 6331.5599999999995, "text": " Excel uses gradient descent, not stochastic gradient descent.", "tokens": [19060, 4960, 16235, 23475, 11, 406, 342, 8997, 2750, 16235, 23475, 13], "temperature": 0.0, "avg_logprob": -0.15407495576191724, "compression_ratio": 1.869047619047619, "no_speech_prob": 6.854270850453759e-06}, {"id": 1299, "seek": 632036, "start": 6331.5599999999995, "end": 6334.48, "text": " They give the same answer.", "tokens": [814, 976, 264, 912, 1867, 13], "temperature": 0.0, "avg_logprob": -0.15407495576191724, "compression_ratio": 1.869047619047619, "no_speech_prob": 6.854270850453759e-06}, {"id": 1300, "seek": 632036, "start": 6334.48, "end": 6337.04, "text": " You might also wonder why it's so damn slow.", "tokens": [509, 1062, 611, 2441, 983, 309, 311, 370, 8151, 2964, 13], "temperature": 0.0, "avg_logprob": -0.15407495576191724, "compression_ratio": 1.869047619047619, "no_speech_prob": 6.854270850453759e-06}, {"id": 1301, "seek": 632036, "start": 6337.04, "end": 6340.96, "text": " It's so damn slow because it doesn't know how to create analytical derivatives, so it's", "tokens": [467, 311, 370, 8151, 2964, 570, 309, 1177, 380, 458, 577, 281, 1884, 29579, 33733, 11, 370, 309, 311], "temperature": 0.0, "avg_logprob": -0.15407495576191724, "compression_ratio": 1.869047619047619, "no_speech_prob": 6.854270850453759e-06}, {"id": 1302, "seek": 632036, "start": 6340.96, "end": 6345.44, "text": " having to calculate the derivatives with finite differencing, which is slow.", "tokens": [1419, 281, 8873, 264, 33733, 365, 19362, 743, 13644, 11, 597, 307, 2964, 13], "temperature": 0.0, "avg_logprob": -0.15407495576191724, "compression_ratio": 1.869047619047619, "no_speech_prob": 6.854270850453759e-06}, {"id": 1303, "seek": 632036, "start": 6345.44, "end": 6348.92, "text": " So here we've got a solution.", "tokens": [407, 510, 321, 600, 658, 257, 3827, 13], "temperature": 0.0, "avg_logprob": -0.15407495576191724, "compression_ratio": 1.869047619047619, "no_speech_prob": 6.854270850453759e-06}, {"id": 1304, "seek": 634892, "start": 6348.92, "end": 6350.64, "text": " It got it down to 5.", "tokens": [467, 658, 309, 760, 281, 1025, 13], "temperature": 0.0, "avg_logprob": -0.13396476177459068, "compression_ratio": 1.738888888888889, "no_speech_prob": 8.801051080808975e-06}, {"id": 1305, "seek": 634892, "start": 6350.64, "end": 6351.64, "text": " That's pretty good.", "tokens": [663, 311, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.13396476177459068, "compression_ratio": 1.738888888888889, "no_speech_prob": 8.801051080808975e-06}, {"id": 1306, "seek": 634892, "start": 6351.64, "end": 6358.4800000000005, "text": " So we can see here that it predicted 5.14 and it was actually 5.", "tokens": [407, 321, 393, 536, 510, 300, 309, 19147, 1025, 13, 7271, 293, 309, 390, 767, 1025, 13], "temperature": 0.0, "avg_logprob": -0.13396476177459068, "compression_ratio": 1.738888888888889, "no_speech_prob": 8.801051080808975e-06}, {"id": 1307, "seek": 634892, "start": 6358.4800000000005, "end": 6361.64, "text": " It predicted 3.05 and it was actually 3.", "tokens": [467, 19147, 805, 13, 13328, 293, 309, 390, 767, 805, 13], "temperature": 0.0, "avg_logprob": -0.13396476177459068, "compression_ratio": 1.738888888888889, "no_speech_prob": 8.801051080808975e-06}, {"id": 1308, "seek": 634892, "start": 6361.64, "end": 6366.0, "text": " So it's done a really, really good job.", "tokens": [407, 309, 311, 1096, 257, 534, 11, 534, 665, 1691, 13], "temperature": 0.0, "avg_logprob": -0.13396476177459068, "compression_ratio": 1.738888888888889, "no_speech_prob": 8.801051080808975e-06}, {"id": 1309, "seek": 634892, "start": 6366.0, "end": 6375.4, "text": " It's a little bit too easy because there are 5 times that many user factors, movie factors", "tokens": [467, 311, 257, 707, 857, 886, 1858, 570, 456, 366, 1025, 1413, 300, 867, 4195, 6771, 11, 3169, 6771], "temperature": 0.0, "avg_logprob": -0.13396476177459068, "compression_ratio": 1.738888888888889, "no_speech_prob": 8.801051080808975e-06}, {"id": 1310, "seek": 634892, "start": 6375.4, "end": 6377.56, "text": " and 5 times that many user factors.", "tokens": [293, 1025, 1413, 300, 867, 4195, 6771, 13], "temperature": 0.0, "avg_logprob": -0.13396476177459068, "compression_ratio": 1.738888888888889, "no_speech_prob": 8.801051080808975e-06}, {"id": 1311, "seek": 637756, "start": 6377.56, "end": 6383.6, "text": " We've got nearly as many factors as we have things to calculate, so it's kind of over-specified,", "tokens": [492, 600, 658, 6217, 382, 867, 6771, 382, 321, 362, 721, 281, 8873, 11, 370, 309, 311, 733, 295, 670, 12, 7053, 66, 2587, 11], "temperature": 0.0, "avg_logprob": -0.14844386431635642, "compression_ratio": 1.6905829596412556, "no_speech_prob": 3.3405206067982363e-06}, {"id": 1312, "seek": 637756, "start": 6383.6, "end": 6386.56, "text": " but the idea is there.", "tokens": [457, 264, 1558, 307, 456, 13], "temperature": 0.0, "avg_logprob": -0.14844386431635642, "compression_ratio": 1.6905829596412556, "no_speech_prob": 3.3405206067982363e-06}, {"id": 1313, "seek": 637756, "start": 6386.56, "end": 6388.320000000001, "text": " There's one piece missing.", "tokens": [821, 311, 472, 2522, 5361, 13], "temperature": 0.0, "avg_logprob": -0.14844386431635642, "compression_ratio": 1.6905829596412556, "no_speech_prob": 3.3405206067982363e-06}, {"id": 1314, "seek": 637756, "start": 6388.320000000001, "end": 6395.4800000000005, "text": " The piece we're missing is that some users probably just like movies more than others,", "tokens": [440, 2522, 321, 434, 5361, 307, 300, 512, 5022, 1391, 445, 411, 6233, 544, 813, 2357, 11], "temperature": 0.0, "avg_logprob": -0.14844386431635642, "compression_ratio": 1.6905829596412556, "no_speech_prob": 3.3405206067982363e-06}, {"id": 1315, "seek": 637756, "start": 6395.4800000000005, "end": 6399.52, "text": " and some movies are probably just more liked than others.", "tokens": [293, 512, 6233, 366, 1391, 445, 544, 4501, 813, 2357, 13], "temperature": 0.0, "avg_logprob": -0.14844386431635642, "compression_ratio": 1.6905829596412556, "no_speech_prob": 3.3405206067982363e-06}, {"id": 1316, "seek": 637756, "start": 6399.52, "end": 6406.76, "text": " This.product does not allow us in any way to say this is an enthusiastic user or this", "tokens": [639, 2411, 33244, 775, 406, 2089, 505, 294, 604, 636, 281, 584, 341, 307, 364, 28574, 4195, 420, 341], "temperature": 0.0, "avg_logprob": -0.14844386431635642, "compression_ratio": 1.6905829596412556, "no_speech_prob": 3.3405206067982363e-06}, {"id": 1317, "seek": 640676, "start": 6406.76, "end": 6408.96, "text": " is a popular movie.", "tokens": [307, 257, 3743, 3169, 13], "temperature": 0.0, "avg_logprob": -0.11095569072625576, "compression_ratio": 1.5502645502645502, "no_speech_prob": 4.092895778740058e-06}, {"id": 1318, "seek": 640676, "start": 6408.96, "end": 6412.6, "text": " To do that, we have to add bias terms.", "tokens": [1407, 360, 300, 11, 321, 362, 281, 909, 12577, 2115, 13], "temperature": 0.0, "avg_logprob": -0.11095569072625576, "compression_ratio": 1.5502645502645502, "no_speech_prob": 4.092895778740058e-06}, {"id": 1319, "seek": 640676, "start": 6412.6, "end": 6422.320000000001, "text": " So here is exactly the same spreadsheet, but I've added one more row to the movies part", "tokens": [407, 510, 307, 2293, 264, 912, 27733, 11, 457, 286, 600, 3869, 472, 544, 5386, 281, 264, 6233, 644], "temperature": 0.0, "avg_logprob": -0.11095569072625576, "compression_ratio": 1.5502645502645502, "no_speech_prob": 4.092895778740058e-06}, {"id": 1320, "seek": 640676, "start": 6422.320000000001, "end": 6427.320000000001, "text": " and one more column to the users part for our biases.", "tokens": [293, 472, 544, 7738, 281, 264, 5022, 644, 337, 527, 32152, 13], "temperature": 0.0, "avg_logprob": -0.11095569072625576, "compression_ratio": 1.5502645502645502, "no_speech_prob": 4.092895778740058e-06}, {"id": 1321, "seek": 640676, "start": 6427.320000000001, "end": 6435.08, "text": " And I've updated the formula so that as well as the matrix multiplication, it also is adding", "tokens": [400, 286, 600, 10588, 264, 8513, 370, 300, 382, 731, 382, 264, 8141, 27290, 11, 309, 611, 307, 5127], "temperature": 0.0, "avg_logprob": -0.11095569072625576, "compression_ratio": 1.5502645502645502, "no_speech_prob": 4.092895778740058e-06}, {"id": 1322, "seek": 643508, "start": 6435.08, "end": 6438.16, "text": " the user bias and the movie bias.", "tokens": [264, 4195, 12577, 293, 264, 3169, 12577, 13], "temperature": 0.0, "avg_logprob": -0.1859142204810833, "compression_ratio": 1.5342465753424657, "no_speech_prob": 1.505696900494513e-06}, {"id": 1323, "seek": 643508, "start": 6438.16, "end": 6450.12, "text": " So this is saying this is a very popular movie and this is a very enthusiastic user, for", "tokens": [407, 341, 307, 1566, 341, 307, 257, 588, 3743, 3169, 293, 341, 307, 257, 588, 28574, 4195, 11, 337], "temperature": 0.0, "avg_logprob": -0.1859142204810833, "compression_ratio": 1.5342465753424657, "no_speech_prob": 1.505696900494513e-06}, {"id": 1324, "seek": 643508, "start": 6450.12, "end": 6451.12, "text": " example.", "tokens": [1365, 13], "temperature": 0.0, "avg_logprob": -0.1859142204810833, "compression_ratio": 1.5342465753424657, "no_speech_prob": 1.505696900494513e-06}, {"id": 1325, "seek": 643508, "start": 6451.12, "end": 6459.16, "text": " So now that we have a collaborative filtering plus bias, we can do gradient descent on that.", "tokens": [407, 586, 300, 321, 362, 257, 16555, 30822, 1804, 12577, 11, 321, 393, 360, 16235, 23475, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.1859142204810833, "compression_ratio": 1.5342465753424657, "no_speech_prob": 1.505696900494513e-06}, {"id": 1326, "seek": 645916, "start": 6459.16, "end": 6465.599999999999, "text": " So previously our gradient descent loss function was 5.6.", "tokens": [407, 8046, 527, 16235, 23475, 4470, 2445, 390, 1025, 13, 21, 13], "temperature": 0.0, "avg_logprob": -0.11743520555042085, "compression_ratio": 1.4951923076923077, "no_speech_prob": 4.092886229045689e-06}, {"id": 1327, "seek": 645916, "start": 6465.599999999999, "end": 6469.12, "text": " We would expect it to be better with bias because we can really better specify what's", "tokens": [492, 576, 2066, 309, 281, 312, 1101, 365, 12577, 570, 321, 393, 534, 1101, 16500, 437, 311], "temperature": 0.0, "avg_logprob": -0.11743520555042085, "compression_ratio": 1.4951923076923077, "no_speech_prob": 4.092886229045689e-06}, {"id": 1328, "seek": 645916, "start": 6469.12, "end": 6470.12, "text": " going on.", "tokens": [516, 322, 13], "temperature": 0.0, "avg_logprob": -0.11743520555042085, "compression_ratio": 1.4951923076923077, "no_speech_prob": 4.092886229045689e-06}, {"id": 1329, "seek": 645916, "start": 6470.12, "end": 6472.16, "text": " Let's try it.", "tokens": [961, 311, 853, 309, 13], "temperature": 0.0, "avg_logprob": -0.11743520555042085, "compression_ratio": 1.4951923076923077, "no_speech_prob": 4.092886229045689e-06}, {"id": 1330, "seek": 645916, "start": 6472.16, "end": 6479.8, "text": " So again, we run solver, solve, and we let that zip along and we see what happens.", "tokens": [407, 797, 11, 321, 1190, 1404, 331, 11, 5039, 11, 293, 321, 718, 300, 20730, 2051, 293, 321, 536, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.11743520555042085, "compression_ratio": 1.4951923076923077, "no_speech_prob": 4.092886229045689e-06}, {"id": 1331, "seek": 645916, "start": 6479.8, "end": 6486.48, "text": " So these things we're calculating are called latent factors.", "tokens": [407, 613, 721, 321, 434, 28258, 366, 1219, 48994, 6771, 13], "temperature": 0.0, "avg_logprob": -0.11743520555042085, "compression_ratio": 1.4951923076923077, "no_speech_prob": 4.092886229045689e-06}, {"id": 1332, "seek": 648648, "start": 6486.48, "end": 6492.04, "text": " A latent factor is some factor that is influencing the outcome, but we don't quite know what", "tokens": [316, 48994, 5952, 307, 512, 5952, 300, 307, 40396, 264, 9700, 11, 457, 321, 500, 380, 1596, 458, 437], "temperature": 0.0, "avg_logprob": -0.1686311827765571, "compression_ratio": 1.721774193548387, "no_speech_prob": 1.0451421985635534e-05}, {"id": 1333, "seek": 648648, "start": 6492.04, "end": 6493.04, "text": " it is.", "tokens": [309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1686311827765571, "compression_ratio": 1.721774193548387, "no_speech_prob": 1.0451421985635534e-05}, {"id": 1334, "seek": 648648, "start": 6493.04, "end": 6495.759999999999, "text": " We're just assuming it's there.", "tokens": [492, 434, 445, 11926, 309, 311, 456, 13], "temperature": 0.0, "avg_logprob": -0.1686311827765571, "compression_ratio": 1.721774193548387, "no_speech_prob": 1.0451421985635534e-05}, {"id": 1335, "seek": 648648, "start": 6495.759999999999, "end": 6500.799999999999, "text": " In fact, what happens is when people do collaborative filtering, they then go back and they draw", "tokens": [682, 1186, 11, 437, 2314, 307, 562, 561, 360, 16555, 30822, 11, 436, 550, 352, 646, 293, 436, 2642], "temperature": 0.0, "avg_logprob": -0.1686311827765571, "compression_ratio": 1.721774193548387, "no_speech_prob": 1.0451421985635534e-05}, {"id": 1336, "seek": 648648, "start": 6500.799999999999, "end": 6505.679999999999, "text": " graphs where they say, here are the movies that are scored highly on this latent factor", "tokens": [24877, 689, 436, 584, 11, 510, 366, 264, 6233, 300, 366, 18139, 5405, 322, 341, 48994, 5952], "temperature": 0.0, "avg_logprob": -0.1686311827765571, "compression_ratio": 1.721774193548387, "no_speech_prob": 1.0451421985635534e-05}, {"id": 1337, "seek": 648648, "start": 6505.679999999999, "end": 6508.5199999999995, "text": " and low on that latent factor.", "tokens": [293, 2295, 322, 300, 48994, 5952, 13], "temperature": 0.0, "avg_logprob": -0.1686311827765571, "compression_ratio": 1.721774193548387, "no_speech_prob": 1.0451421985635534e-05}, {"id": 1338, "seek": 648648, "start": 6508.5199999999995, "end": 6514.48, "text": " So they'll discover the Bruce Willis factor and the sci-fi factor and so forth.", "tokens": [407, 436, 603, 4411, 264, 15429, 3099, 271, 5952, 293, 264, 2180, 12, 13325, 5952, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1686311827765571, "compression_ratio": 1.721774193548387, "no_speech_prob": 1.0451421985635534e-05}, {"id": 1339, "seek": 651448, "start": 6514.48, "end": 6520.08, "text": " And so if you look at the Netflix Prize visualizations, you'll see these graphs people do.", "tokens": [400, 370, 498, 291, 574, 412, 264, 12778, 22604, 5056, 14455, 11, 291, 603, 536, 613, 24877, 561, 360, 13], "temperature": 0.0, "avg_logprob": -0.19367719733196756, "compression_ratio": 1.5981735159817352, "no_speech_prob": 5.507576588570373e-06}, {"id": 1340, "seek": 651448, "start": 6520.08, "end": 6522.5199999999995, "text": " The way they do them is they literally do this.", "tokens": [440, 636, 436, 360, 552, 307, 436, 3736, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.19367719733196756, "compression_ratio": 1.5981735159817352, "no_speech_prob": 5.507576588570373e-06}, {"id": 1341, "seek": 651448, "start": 6522.5199999999995, "end": 6528.5599999999995, "text": " Not in Excel, because they're not that cool, but they calculate these latent factors and", "tokens": [1726, 294, 19060, 11, 570, 436, 434, 406, 300, 1627, 11, 457, 436, 8873, 613, 48994, 6771, 293], "temperature": 0.0, "avg_logprob": -0.19367719733196756, "compression_ratio": 1.5981735159817352, "no_speech_prob": 5.507576588570373e-06}, {"id": 1342, "seek": 651448, "start": 6528.5599999999995, "end": 6532.839999999999, "text": " then they draw pictures of them and then they actually write the name of the movie on the", "tokens": [550, 436, 2642, 5242, 295, 552, 293, 550, 436, 767, 2464, 264, 1315, 295, 264, 3169, 322, 264], "temperature": 0.0, "avg_logprob": -0.19367719733196756, "compression_ratio": 1.5981735159817352, "no_speech_prob": 5.507576588570373e-06}, {"id": 1343, "seek": 651448, "start": 6532.839999999999, "end": 6533.839999999999, "text": " graph.", "tokens": [4295, 13], "temperature": 0.0, "avg_logprob": -0.19367719733196756, "compression_ratio": 1.5981735159817352, "no_speech_prob": 5.507576588570373e-06}, {"id": 1344, "seek": 651448, "start": 6533.839999999999, "end": 6539.04, "text": " Anyway, 4.6, even better.", "tokens": [5684, 11, 1017, 13, 21, 11, 754, 1101, 13], "temperature": 0.0, "avg_logprob": -0.19367719733196756, "compression_ratio": 1.5981735159817352, "no_speech_prob": 5.507576588570373e-06}, {"id": 1345, "seek": 653904, "start": 6539.04, "end": 6551.74, "text": " So you can see that, in fact I also have an error here, because any time that my rating", "tokens": [407, 291, 393, 536, 300, 11, 294, 1186, 286, 611, 362, 364, 6713, 510, 11, 570, 604, 565, 300, 452, 10990], "temperature": 0.0, "avg_logprob": -0.18743925961581143, "compression_ratio": 1.4214285714285715, "no_speech_prob": 1.9833199985441752e-05}, {"id": 1346, "seek": 653904, "start": 6551.74, "end": 6558.3, "text": " is empty, I really want to be setting this to empty as well, which means my parenthesis", "tokens": [307, 6707, 11, 286, 534, 528, 281, 312, 3287, 341, 281, 6707, 382, 731, 11, 597, 1355, 452, 23350, 9374], "temperature": 0.0, "avg_logprob": -0.18743925961581143, "compression_ratio": 1.4214285714285715, "no_speech_prob": 1.9833199985441752e-05}, {"id": 1347, "seek": 653904, "start": 6558.3, "end": 6561.34, "text": " was in the wrong place.", "tokens": [390, 294, 264, 2085, 1081, 13], "temperature": 0.0, "avg_logprob": -0.18743925961581143, "compression_ratio": 1.4214285714285715, "no_speech_prob": 1.9833199985441752e-05}, {"id": 1348, "seek": 656134, "start": 6561.34, "end": 6570.2, "text": " So I'm going to recalculate this with my error fixed up and see if we get a better answer.", "tokens": [407, 286, 478, 516, 281, 850, 304, 2444, 473, 341, 365, 452, 6713, 6806, 493, 293, 536, 498, 321, 483, 257, 1101, 1867, 13], "temperature": 0.0, "avg_logprob": -0.27365875244140625, "compression_ratio": 1.0975609756097562, "no_speech_prob": 3.426775219850242e-05}, {"id": 1349, "seek": 657020, "start": 6570.2, "end": 6595.48, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.861108144124349, "compression_ratio": 0.7714285714285715, "no_speech_prob": 5.143894304637797e-05}, {"id": 1350, "seek": 659548, "start": 6595.48, "end": 6617.08, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.8655242919921875, "compression_ratio": 0.7714285714285715, "no_speech_prob": 9.818038961384445e-06}, {"id": 1351, "seek": 661708, "start": 6617.08, "end": 6625.72, "text": " So we can look at some pictures next week, but during the week, Google for Netflix Prize", "tokens": [407, 321, 393, 574, 412, 512, 5242, 958, 1243, 11, 457, 1830, 264, 1243, 11, 3329, 337, 12778, 22604], "temperature": 0.0, "avg_logprob": -0.2042282876514253, "compression_ratio": 1.5411255411255411, "no_speech_prob": 3.7265745049808174e-06}, {"id": 1352, "seek": 661708, "start": 6625.72, "end": 6629.28, "text": " visualizations and you'll see these pictures.", "tokens": [5056, 14455, 293, 291, 603, 536, 613, 5242, 13], "temperature": 0.0, "avg_logprob": -0.2042282876514253, "compression_ratio": 1.5411255411255411, "no_speech_prob": 3.7265745049808174e-06}, {"id": 1353, "seek": 661708, "start": 6629.28, "end": 6632.88, "text": " And it really does work the way I described.", "tokens": [400, 309, 534, 775, 589, 264, 636, 286, 7619, 13], "temperature": 0.0, "avg_logprob": -0.2042282876514253, "compression_ratio": 1.5411255411255411, "no_speech_prob": 3.7265745049808174e-06}, {"id": 1354, "seek": 661708, "start": 6632.88, "end": 6640.78, "text": " It figures out what are the most interesting dimensions on which we can rate a movie.", "tokens": [467, 9624, 484, 437, 366, 264, 881, 1880, 12819, 322, 597, 321, 393, 3314, 257, 3169, 13], "temperature": 0.0, "avg_logprob": -0.2042282876514253, "compression_ratio": 1.5411255411255411, "no_speech_prob": 3.7265745049808174e-06}, {"id": 1355, "seek": 661708, "start": 6640.78, "end": 6646.72, "text": " And things like level of action and sci-fi and dialogue driven are very important features", "tokens": [400, 721, 411, 1496, 295, 3069, 293, 2180, 12, 13325, 293, 10221, 9555, 366, 588, 1021, 4122], "temperature": 0.0, "avg_logprob": -0.2042282876514253, "compression_ratio": 1.5411255411255411, "no_speech_prob": 3.7265745049808174e-06}, {"id": 1356, "seek": 664672, "start": 6646.72, "end": 6650.04, "text": " it turns out.", "tokens": [309, 4523, 484, 13], "temperature": 0.0, "avg_logprob": -0.142454993724823, "compression_ratio": 1.6192660550458715, "no_speech_prob": 2.9772452762699686e-05}, {"id": 1357, "seek": 664672, "start": 6650.04, "end": 6656.240000000001, "text": " But rather than pre-specifying those features, we have definitely learned from this class", "tokens": [583, 2831, 813, 659, 12, 7053, 66, 5489, 729, 4122, 11, 321, 362, 2138, 3264, 490, 341, 1508], "temperature": 0.0, "avg_logprob": -0.142454993724823, "compression_ratio": 1.6192660550458715, "no_speech_prob": 2.9772452762699686e-05}, {"id": 1358, "seek": 664672, "start": 6656.240000000001, "end": 6661.3, "text": " that calculating features using gradient descent is going to give us better features than trying", "tokens": [300, 28258, 4122, 1228, 16235, 23475, 307, 516, 281, 976, 505, 1101, 4122, 813, 1382], "temperature": 0.0, "avg_logprob": -0.142454993724823, "compression_ratio": 1.6192660550458715, "no_speech_prob": 2.9772452762699686e-05}, {"id": 1359, "seek": 664672, "start": 6661.3, "end": 6668.68, "text": " to engineer them by hand.", "tokens": [281, 11403, 552, 538, 1011, 13], "temperature": 0.0, "avg_logprob": -0.142454993724823, "compression_ratio": 1.6192660550458715, "no_speech_prob": 2.9772452762699686e-05}, {"id": 1360, "seek": 664672, "start": 6668.68, "end": 6671.12, "text": " Interesting that it feels crazy.", "tokens": [14711, 300, 309, 3417, 3219, 13], "temperature": 0.0, "avg_logprob": -0.142454993724823, "compression_ratio": 1.6192660550458715, "no_speech_prob": 2.9772452762699686e-05}, {"id": 1361, "seek": 664672, "start": 6671.12, "end": 6675.76, "text": " Tell me next week if you find some particularly interesting things or if it still seems crazy", "tokens": [5115, 385, 958, 1243, 498, 291, 915, 512, 4098, 1880, 721, 420, 498, 309, 920, 2544, 3219], "temperature": 0.0, "avg_logprob": -0.142454993724823, "compression_ratio": 1.6192660550458715, "no_speech_prob": 2.9772452762699686e-05}, {"id": 1362, "seek": 667576, "start": 6675.76, "end": 6680.4400000000005, "text": " and we can try to decrysify it a little bit.", "tokens": [293, 321, 393, 853, 281, 979, 627, 82, 2505, 309, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.15283789563534864, "compression_ratio": 1.4093959731543624, "no_speech_prob": 1.6963718735496514e-05}, {"id": 1363, "seek": 667576, "start": 6680.4400000000005, "end": 6684.84, "text": " So let's do this in Keras.", "tokens": [407, 718, 311, 360, 341, 294, 591, 6985, 13], "temperature": 0.0, "avg_logprob": -0.15283789563534864, "compression_ratio": 1.4093959731543624, "no_speech_prob": 1.6963718735496514e-05}, {"id": 1364, "seek": 667576, "start": 6684.84, "end": 6691.08, "text": " Now there's really only one main new concept we have to learn, which is we started out", "tokens": [823, 456, 311, 534, 787, 472, 2135, 777, 3410, 321, 362, 281, 1466, 11, 597, 307, 321, 1409, 484], "temperature": 0.0, "avg_logprob": -0.15283789563534864, "compression_ratio": 1.4093959731543624, "no_speech_prob": 1.6963718735496514e-05}, {"id": 1365, "seek": 667576, "start": 6691.08, "end": 6696.52, "text": " with data not in a crosstab form, but in this form.", "tokens": [365, 1412, 406, 294, 257, 28108, 372, 455, 1254, 11, 457, 294, 341, 1254, 13], "temperature": 0.0, "avg_logprob": -0.15283789563534864, "compression_ratio": 1.4093959731543624, "no_speech_prob": 1.6963718735496514e-05}, {"id": 1366, "seek": 669652, "start": 6696.52, "end": 6719.6, "text": " We have user ID, movie ID, rating triplets, and I crosstab them.", "tokens": [492, 362, 4195, 7348, 11, 3169, 7348, 11, 10990, 1376, 31023, 11, 293, 286, 28108, 372, 455, 552, 13], "temperature": 0.0, "avg_logprob": -0.19390554428100587, "compression_ratio": 1.4594594594594594, "no_speech_prob": 6.013797246851027e-05}, {"id": 1367, "seek": 669652, "start": 6719.6, "end": 6725.280000000001, "text": " Each of these rows is one feature of a movie, and each of these columns is one feature of", "tokens": [6947, 295, 613, 13241, 307, 472, 4111, 295, 257, 3169, 11, 293, 1184, 295, 613, 13766, 307, 472, 4111, 295], "temperature": 0.0, "avg_logprob": -0.19390554428100587, "compression_ratio": 1.4594594594594594, "no_speech_prob": 6.013797246851027e-05}, {"id": 1368, "seek": 669652, "start": 6725.280000000001, "end": 6726.400000000001, "text": " a user.", "tokens": [257, 4195, 13], "temperature": 0.0, "avg_logprob": -0.19390554428100587, "compression_ratio": 1.4594594594594594, "no_speech_prob": 6.013797246851027e-05}, {"id": 1369, "seek": 672640, "start": 6726.4, "end": 6731.759999999999, "text": " And so one of these sets of 5 is one set of features for a user.", "tokens": [400, 370, 472, 295, 613, 6352, 295, 1025, 307, 472, 992, 295, 4122, 337, 257, 4195, 13], "temperature": 0.0, "avg_logprob": -0.25078473552580804, "compression_ratio": 1.1904761904761905, "no_speech_prob": 4.908283517579548e-05}, {"id": 1370, "seek": 672640, "start": 6731.759999999999, "end": 6747.28, "text": " This is this user's latent factors.", "tokens": [639, 307, 341, 4195, 311, 48994, 6771, 13], "temperature": 0.0, "avg_logprob": -0.25078473552580804, "compression_ratio": 1.1904761904761905, "no_speech_prob": 4.908283517579548e-05}, {"id": 1371, "seek": 674728, "start": 6747.28, "end": 6756.92, "text": " This is the thing I said at the start of class, which is there's nothing mathematically complicated", "tokens": [639, 307, 264, 551, 286, 848, 412, 264, 722, 295, 1508, 11, 597, 307, 456, 311, 1825, 44003, 6179], "temperature": 0.0, "avg_logprob": -0.2660613146695224, "compression_ratio": 1.4834437086092715, "no_speech_prob": 4.069167698617093e-05}, {"id": 1372, "seek": 674728, "start": 6756.92, "end": 6757.92, "text": " about gradient descent.", "tokens": [466, 16235, 23475, 13], "temperature": 0.0, "avg_logprob": -0.2660613146695224, "compression_ratio": 1.4834437086092715, "no_speech_prob": 4.069167698617093e-05}, {"id": 1373, "seek": 674728, "start": 6757.92, "end": 6765.5599999999995, "text": " The hard part is unlearning the idea that this should be hard.", "tokens": [440, 1152, 644, 307, 25272, 2341, 264, 1558, 300, 341, 820, 312, 1152, 13], "temperature": 0.0, "avg_logprob": -0.2660613146695224, "compression_ratio": 1.4834437086092715, "no_speech_prob": 4.069167698617093e-05}, {"id": 1374, "seek": 674728, "start": 6765.5599999999995, "end": 6768.32, "text": " Gradient descent just figures it out.", "tokens": [16710, 1196, 23475, 445, 9624, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.2660613146695224, "compression_ratio": 1.4834437086092715, "no_speech_prob": 4.069167698617093e-05}, {"id": 1375, "seek": 676832, "start": 6768.32, "end": 6779.88, "text": " I just wanted to point out that this you can think of as a smaller, more concise way to", "tokens": [286, 445, 1415, 281, 935, 484, 300, 341, 291, 393, 519, 295, 382, 257, 4356, 11, 544, 44882, 636, 281], "temperature": 0.0, "avg_logprob": -0.24762719869613647, "compression_ratio": 1.5793991416309012, "no_speech_prob": 2.5465305952820927e-05}, {"id": 1376, "seek": 676832, "start": 6779.88, "end": 6783.799999999999, "text": " represent the movies and the users.", "tokens": [2906, 264, 6233, 293, 264, 5022, 13], "temperature": 0.0, "avg_logprob": -0.24762719869613647, "compression_ratio": 1.5793991416309012, "no_speech_prob": 2.5465305952820927e-05}, {"id": 1377, "seek": 676832, "start": 6783.799999999999, "end": 6787.08, "text": " In math, there's the concept of a matrix factorization.", "tokens": [682, 5221, 11, 456, 311, 264, 3410, 295, 257, 8141, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.24762719869613647, "compression_ratio": 1.5793991416309012, "no_speech_prob": 2.5465305952820927e-05}, {"id": 1378, "seek": 676832, "start": 6787.08, "end": 6793.36, "text": " An SVD for example, which is where you basically take a big matrix and turn it into a small", "tokens": [1107, 31910, 35, 337, 1365, 11, 597, 307, 689, 291, 1936, 747, 257, 955, 8141, 293, 1261, 309, 666, 257, 1359], "temperature": 0.0, "avg_logprob": -0.24762719869613647, "compression_ratio": 1.5793991416309012, "no_speech_prob": 2.5465305952820927e-05}, {"id": 1379, "seek": 676832, "start": 6793.36, "end": 6796.679999999999, "text": " narrow one and a small thin one and multiply the two together.", "tokens": [9432, 472, 293, 257, 1359, 5862, 472, 293, 12972, 264, 732, 1214, 13], "temperature": 0.0, "avg_logprob": -0.24762719869613647, "compression_ratio": 1.5793991416309012, "no_speech_prob": 2.5465305952820927e-05}, {"id": 1380, "seek": 676832, "start": 6796.679999999999, "end": 6797.679999999999, "text": " This is exactly what we're doing.", "tokens": [639, 307, 2293, 437, 321, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.24762719869613647, "compression_ratio": 1.5793991416309012, "no_speech_prob": 2.5465305952820927e-05}, {"id": 1381, "seek": 679768, "start": 6797.68, "end": 6803.360000000001, "text": " Instead of having how user 14 rated every single movie, we just have 5 numbers that", "tokens": [7156, 295, 1419, 577, 4195, 3499, 22103, 633, 2167, 3169, 11, 321, 445, 362, 1025, 3547, 300], "temperature": 0.0, "avg_logprob": -0.25919628143310547, "compression_ratio": 1.4057142857142857, "no_speech_prob": 0.00013544266403187066}, {"id": 1382, "seek": 679768, "start": 6803.360000000001, "end": 6808.84, "text": " represents it, which is pretty cool.", "tokens": [8855, 309, 11, 597, 307, 1238, 1627, 13], "temperature": 0.0, "avg_logprob": -0.25919628143310547, "compression_ratio": 1.4057142857142857, "no_speech_prob": 0.00013544266403187066}, {"id": 1383, "seek": 679768, "start": 6808.84, "end": 6815.400000000001, "text": " So earlier did you say that both the user features were random as well as the...", "tokens": [407, 3071, 630, 291, 584, 300, 1293, 264, 4195, 4122, 645, 4974, 382, 731, 382, 264, 485], "temperature": 0.0, "avg_logprob": -0.25919628143310547, "compression_ratio": 1.4057142857142857, "no_speech_prob": 0.00013544266403187066}, {"id": 1384, "seek": 679768, "start": 6815.400000000001, "end": 6816.400000000001, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.25919628143310547, "compression_ratio": 1.4057142857142857, "no_speech_prob": 0.00013544266403187066}, {"id": 1385, "seek": 679768, "start": 6816.400000000001, "end": 6819.72, "text": " I guess I'm in trouble relating to you.", "tokens": [286, 2041, 286, 478, 294, 5253, 23968, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.25919628143310547, "compression_ratio": 1.4057142857142857, "no_speech_prob": 0.00013544266403187066}, {"id": 1386, "seek": 681972, "start": 6819.72, "end": 6828.96, "text": " I thought usually we run gradient descent on something that has inputs that you know", "tokens": [286, 1194, 2673, 321, 1190, 16235, 23475, 322, 746, 300, 575, 15743, 300, 291, 458], "temperature": 0.0, "avg_logprob": -0.3500531295250202, "compression_ratio": 1.4557823129251701, "no_speech_prob": 6.921574822627008e-05}, {"id": 1387, "seek": 681972, "start": 6828.96, "end": 6832.0, "text": " and here what do you know?", "tokens": [293, 510, 437, 360, 291, 458, 30], "temperature": 0.0, "avg_logprob": -0.3500531295250202, "compression_ratio": 1.4557823129251701, "no_speech_prob": 6.921574822627008e-05}, {"id": 1388, "seek": 681972, "start": 6832.0, "end": 6835.0, "text": " What we know.", "tokens": [708, 321, 458, 13], "temperature": 0.0, "avg_logprob": -0.3500531295250202, "compression_ratio": 1.4557823129251701, "no_speech_prob": 6.921574822627008e-05}, {"id": 1389, "seek": 681972, "start": 6835.0, "end": 6837.2, "text": " That's what we know, the resulting ratings.", "tokens": [663, 311, 437, 321, 458, 11, 264, 16505, 24603, 13], "temperature": 0.0, "avg_logprob": -0.3500531295250202, "compression_ratio": 1.4557823129251701, "no_speech_prob": 6.921574822627008e-05}, {"id": 1390, "seek": 681972, "start": 6837.2, "end": 6842.2, "text": " So can you perhaps come up with the wrong...", "tokens": [407, 393, 291, 4317, 808, 493, 365, 264, 2085, 485], "temperature": 0.0, "avg_logprob": -0.3500531295250202, "compression_ratio": 1.4557823129251701, "no_speech_prob": 6.921574822627008e-05}, {"id": 1391, "seek": 684220, "start": 6842.2, "end": 6863.5199999999995, "text": " If one of the numbers was in the wrong spot, our loss function would be less good and therefore", "tokens": [759, 472, 295, 264, 3547, 390, 294, 264, 2085, 4008, 11, 527, 4470, 2445, 576, 312, 1570, 665, 293, 4412], "temperature": 0.0, "avg_logprob": -0.1659075793098001, "compression_ratio": 1.5407407407407407, "no_speech_prob": 2.7264541131444275e-06}, {"id": 1392, "seek": 684220, "start": 6863.5199999999995, "end": 6869.08, "text": " there would be a gradient from that weight to say you should make this weight a little", "tokens": [456, 576, 312, 257, 16235, 490, 300, 3364, 281, 584, 291, 820, 652, 341, 3364, 257, 707], "temperature": 0.0, "avg_logprob": -0.1659075793098001, "compression_ratio": 1.5407407407407407, "no_speech_prob": 2.7264541131444275e-06}, {"id": 1393, "seek": 684220, "start": 6869.08, "end": 6871.72, "text": " higher or a little lower.", "tokens": [2946, 420, 257, 707, 3126, 13], "temperature": 0.0, "avg_logprob": -0.1659075793098001, "compression_ratio": 1.5407407407407407, "no_speech_prob": 2.7264541131444275e-06}, {"id": 1394, "seek": 687172, "start": 6871.72, "end": 6876.4800000000005, "text": " So all the gradient descent is saying for every weight, if we make it a little higher,", "tokens": [407, 439, 264, 16235, 23475, 307, 1566, 337, 633, 3364, 11, 498, 321, 652, 309, 257, 707, 2946, 11], "temperature": 0.0, "avg_logprob": -0.1971545599203194, "compression_ratio": 1.9004329004329004, "no_speech_prob": 4.289300704840571e-06}, {"id": 1395, "seek": 687172, "start": 6876.4800000000005, "end": 6879.8, "text": " does it get better, or if we make it a little bit lower, does it get better.", "tokens": [775, 309, 483, 1101, 11, 420, 498, 321, 652, 309, 257, 707, 857, 3126, 11, 775, 309, 483, 1101, 13], "temperature": 0.0, "avg_logprob": -0.1971545599203194, "compression_ratio": 1.9004329004329004, "no_speech_prob": 4.289300704840571e-06}, {"id": 1396, "seek": 687172, "start": 6879.8, "end": 6887.6, "text": " And then we keep making them a little bit higher and lower until we can't go any better.", "tokens": [400, 550, 321, 1066, 1455, 552, 257, 707, 857, 2946, 293, 3126, 1826, 321, 393, 380, 352, 604, 1101, 13], "temperature": 0.0, "avg_logprob": -0.1971545599203194, "compression_ratio": 1.9004329004329004, "no_speech_prob": 4.289300704840571e-06}, {"id": 1397, "seek": 687172, "start": 6887.6, "end": 6891.400000000001, "text": " And we had to decide how to combine the weights.", "tokens": [400, 321, 632, 281, 4536, 577, 281, 10432, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.1971545599203194, "compression_ratio": 1.9004329004329004, "no_speech_prob": 4.289300704840571e-06}, {"id": 1398, "seek": 687172, "start": 6891.400000000001, "end": 6893.06, "text": " This was our architecture.", "tokens": [639, 390, 527, 9482, 13], "temperature": 0.0, "avg_logprob": -0.1971545599203194, "compression_ratio": 1.9004329004329004, "no_speech_prob": 4.289300704840571e-06}, {"id": 1399, "seek": 687172, "start": 6893.06, "end": 6899.16, "text": " So our architecture was, let's take a dot product of some assumed user feature and some", "tokens": [407, 527, 9482, 390, 11, 718, 311, 747, 257, 5893, 1674, 295, 512, 15895, 4195, 4111, 293, 512], "temperature": 0.0, "avg_logprob": -0.1971545599203194, "compression_ratio": 1.9004329004329004, "no_speech_prob": 4.289300704840571e-06}, {"id": 1400, "seek": 687172, "start": 6899.16, "end": 6901.320000000001, "text": " assumed movie feature.", "tokens": [15895, 3169, 4111, 13], "temperature": 0.0, "avg_logprob": -0.1971545599203194, "compression_ratio": 1.9004329004329004, "no_speech_prob": 4.289300704840571e-06}, {"id": 1401, "seek": 690132, "start": 6901.32, "end": 6906.36, "text": " And let's add in the second case some assumed bias term.", "tokens": [400, 718, 311, 909, 294, 264, 1150, 1389, 512, 15895, 12577, 1433, 13], "temperature": 0.0, "avg_logprob": -0.14507461012455455, "compression_ratio": 1.766798418972332, "no_speech_prob": 8.664575034345035e-06}, {"id": 1402, "seek": 690132, "start": 6906.36, "end": 6908.2, "text": " So we had to build an architecture.", "tokens": [407, 321, 632, 281, 1322, 364, 9482, 13], "temperature": 0.0, "avg_logprob": -0.14507461012455455, "compression_ratio": 1.766798418972332, "no_speech_prob": 8.664575034345035e-06}, {"id": 1403, "seek": 690132, "start": 6908.2, "end": 6912.599999999999, "text": " And we built the architecture using common sense, which is to say this seems like a reasonable", "tokens": [400, 321, 3094, 264, 9482, 1228, 2689, 2020, 11, 597, 307, 281, 584, 341, 2544, 411, 257, 10585], "temperature": 0.0, "avg_logprob": -0.14507461012455455, "compression_ratio": 1.766798418972332, "no_speech_prob": 8.664575034345035e-06}, {"id": 1404, "seek": 690132, "start": 6912.599999999999, "end": 6913.599999999999, "text": " way of thinking about this.", "tokens": [636, 295, 1953, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.14507461012455455, "compression_ratio": 1.766798418972332, "no_speech_prob": 8.664575034345035e-06}, {"id": 1405, "seek": 690132, "start": 6913.599999999999, "end": 6916.28, "text": " I'm going to show you a better architecture in a moment.", "tokens": [286, 478, 516, 281, 855, 291, 257, 1101, 9482, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.14507461012455455, "compression_ratio": 1.766798418972332, "no_speech_prob": 8.664575034345035e-06}, {"id": 1406, "seek": 690132, "start": 6916.28, "end": 6921.639999999999, "text": " In fact, we're running out of time, so let me jump into the better architecture.", "tokens": [682, 1186, 11, 321, 434, 2614, 484, 295, 565, 11, 370, 718, 385, 3012, 666, 264, 1101, 9482, 13], "temperature": 0.0, "avg_logprob": -0.14507461012455455, "compression_ratio": 1.766798418972332, "no_speech_prob": 8.664575034345035e-06}, {"id": 1407, "seek": 690132, "start": 6921.639999999999, "end": 6925.639999999999, "text": " So I wanted to point out that there is something new we're going to have to learn here, which", "tokens": [407, 286, 1415, 281, 935, 484, 300, 456, 307, 746, 777, 321, 434, 516, 281, 362, 281, 1466, 510, 11, 597], "temperature": 0.0, "avg_logprob": -0.14507461012455455, "compression_ratio": 1.766798418972332, "no_speech_prob": 8.664575034345035e-06}, {"id": 1408, "seek": 692564, "start": 6925.64, "end": 6933.08, "text": " is how do you start with a numeric user ID and look up to find what is their 5 element", "tokens": [307, 577, 360, 291, 722, 365, 257, 7866, 299, 4195, 7348, 293, 574, 493, 281, 915, 437, 307, 641, 1025, 4478], "temperature": 0.0, "avg_logprob": -0.18653322035266506, "compression_ratio": 1.358974358974359, "no_speech_prob": 5.338116807251936e-06}, {"id": 1409, "seek": 692564, "start": 6933.08, "end": 6936.0, "text": " latent factor matrix.", "tokens": [48994, 5952, 8141, 13], "temperature": 0.0, "avg_logprob": -0.18653322035266506, "compression_ratio": 1.358974358974359, "no_speech_prob": 5.338116807251936e-06}, {"id": 1410, "seek": 692564, "start": 6936.0, "end": 6943.12, "text": " Now remember, when we have user IDs like 1, 2 and 3, one way to specify them is using", "tokens": [823, 1604, 11, 562, 321, 362, 4195, 48212, 411, 502, 11, 568, 293, 805, 11, 472, 636, 281, 16500, 552, 307, 1228], "temperature": 0.0, "avg_logprob": -0.18653322035266506, "compression_ratio": 1.358974358974359, "no_speech_prob": 5.338116807251936e-06}, {"id": 1411, "seek": 692564, "start": 6943.12, "end": 6952.6, "text": " one-hot encoding.", "tokens": [472, 12, 12194, 43430, 13], "temperature": 0.0, "avg_logprob": -0.18653322035266506, "compression_ratio": 1.358974358974359, "no_speech_prob": 5.338116807251936e-06}, {"id": 1412, "seek": 695260, "start": 6952.6, "end": 6961.88, "text": " So one way to handle this situation would be if this was our user matrix, it was one-hot", "tokens": [407, 472, 636, 281, 4813, 341, 2590, 576, 312, 498, 341, 390, 527, 4195, 8141, 11, 309, 390, 472, 12, 12194], "temperature": 0.0, "avg_logprob": -0.210235595703125, "compression_ratio": 1.3951612903225807, "no_speech_prob": 9.080415111384355e-06}, {"id": 1413, "seek": 695260, "start": 6961.88, "end": 6976.4800000000005, "text": " encoded, and then we had a factor matrix containing a whole bunch of random numbers.", "tokens": [2058, 12340, 11, 293, 550, 321, 632, 257, 5952, 8141, 19273, 257, 1379, 3840, 295, 4974, 3547, 13], "temperature": 0.0, "avg_logprob": -0.210235595703125, "compression_ratio": 1.3951612903225807, "no_speech_prob": 9.080415111384355e-06}, {"id": 1414, "seek": 697648, "start": 6976.48, "end": 6989.24, "text": " One way to do it would be to take a dot product or a matrix product of this and this.", "tokens": [1485, 636, 281, 360, 309, 576, 312, 281, 747, 257, 5893, 1674, 420, 257, 8141, 1674, 295, 341, 293, 341, 13], "temperature": 0.0, "avg_logprob": -0.1398772821797953, "compression_ratio": 1.83125, "no_speech_prob": 4.936938239552546e-06}, {"id": 1415, "seek": 697648, "start": 6989.24, "end": 6994.5199999999995, "text": " And what that would do would be for this one here, it would basically say, let's multiply", "tokens": [400, 437, 300, 576, 360, 576, 312, 337, 341, 472, 510, 11, 309, 576, 1936, 584, 11, 718, 311, 12972], "temperature": 0.0, "avg_logprob": -0.1398772821797953, "compression_ratio": 1.83125, "no_speech_prob": 4.936938239552546e-06}, {"id": 1416, "seek": 697648, "start": 6994.5199999999995, "end": 7002.4, "text": " that by this, it would grab the first column of the matrix.", "tokens": [300, 538, 341, 11, 309, 576, 4444, 264, 700, 7738, 295, 264, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1398772821797953, "compression_ratio": 1.83125, "no_speech_prob": 4.936938239552546e-06}, {"id": 1417, "seek": 697648, "start": 7002.4, "end": 7006.08, "text": " And this here would grab the second column of the matrix.", "tokens": [400, 341, 510, 576, 4444, 264, 1150, 7738, 295, 264, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1398772821797953, "compression_ratio": 1.83125, "no_speech_prob": 4.936938239552546e-06}, {"id": 1418, "seek": 700608, "start": 7006.08, "end": 7009.14, "text": " And this here would grab the third column of the matrix.", "tokens": [400, 341, 510, 576, 4444, 264, 2636, 7738, 295, 264, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1292481111443561, "compression_ratio": 1.5627906976744186, "no_speech_prob": 1.8266247536757874e-07}, {"id": 1419, "seek": 700608, "start": 7009.14, "end": 7016.64, "text": " So one way to do this in Keras would be to represent our user IDs as one-hot encodings", "tokens": [407, 472, 636, 281, 360, 341, 294, 591, 6985, 576, 312, 281, 2906, 527, 4195, 48212, 382, 472, 12, 12194, 2058, 378, 1109], "temperature": 0.0, "avg_logprob": -0.1292481111443561, "compression_ratio": 1.5627906976744186, "no_speech_prob": 1.8266247536757874e-07}, {"id": 1420, "seek": 700608, "start": 7016.64, "end": 7024.0, "text": " and to create a user factor matrix just as a regular matrix like this, and then take", "tokens": [293, 281, 1884, 257, 4195, 5952, 8141, 445, 382, 257, 3890, 8141, 411, 341, 11, 293, 550, 747], "temperature": 0.0, "avg_logprob": -0.1292481111443561, "compression_ratio": 1.5627906976744186, "no_speech_prob": 1.8266247536757874e-07}, {"id": 1421, "seek": 700608, "start": 7024.0, "end": 7027.84, "text": " a matrix product.", "tokens": [257, 8141, 1674, 13], "temperature": 0.0, "avg_logprob": -0.1292481111443561, "compression_ratio": 1.5627906976744186, "no_speech_prob": 1.8266247536757874e-07}, {"id": 1422, "seek": 700608, "start": 7027.84, "end": 7035.84, "text": " That's horribly slow because if we have 10,000 users, then this thing is 10,000 wide, and", "tokens": [663, 311, 45028, 2964, 570, 498, 321, 362, 1266, 11, 1360, 5022, 11, 550, 341, 551, 307, 1266, 11, 1360, 4874, 11, 293], "temperature": 0.0, "avg_logprob": -0.1292481111443561, "compression_ratio": 1.5627906976744186, "no_speech_prob": 1.8266247536757874e-07}, {"id": 1423, "seek": 703584, "start": 7035.84, "end": 7038.360000000001, "text": " that's a really big matrix multiplication.", "tokens": [300, 311, 257, 534, 955, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.17964453851023027, "compression_ratio": 1.849624060150376, "no_speech_prob": 6.240862603590358e-06}, {"id": 1424, "seek": 703584, "start": 7038.360000000001, "end": 7042.64, "text": " All we're actually doing is saying for user ID number 1, take the first column.", "tokens": [1057, 321, 434, 767, 884, 307, 1566, 337, 4195, 7348, 1230, 502, 11, 747, 264, 700, 7738, 13], "temperature": 0.0, "avg_logprob": -0.17964453851023027, "compression_ratio": 1.849624060150376, "no_speech_prob": 6.240862603590358e-06}, {"id": 1425, "seek": 703584, "start": 7042.64, "end": 7044.88, "text": " For user ID number 2, take the second column.", "tokens": [1171, 4195, 7348, 1230, 568, 11, 747, 264, 1150, 7738, 13], "temperature": 0.0, "avg_logprob": -0.17964453851023027, "compression_ratio": 1.849624060150376, "no_speech_prob": 6.240862603590358e-06}, {"id": 1426, "seek": 703584, "start": 7044.88, "end": 7047.16, "text": " For user ID number 3, take the third column.", "tokens": [1171, 4195, 7348, 1230, 805, 11, 747, 264, 2636, 7738, 13], "temperature": 0.0, "avg_logprob": -0.17964453851023027, "compression_ratio": 1.849624060150376, "no_speech_prob": 6.240862603590358e-06}, {"id": 1427, "seek": 703584, "start": 7047.16, "end": 7052.32, "text": " And so Keras has something which does this for us, and it's called an embedding layer.", "tokens": [400, 370, 591, 6985, 575, 746, 597, 775, 341, 337, 505, 11, 293, 309, 311, 1219, 364, 12240, 3584, 4583, 13], "temperature": 0.0, "avg_logprob": -0.17964453851023027, "compression_ratio": 1.849624060150376, "no_speech_prob": 6.240862603590358e-06}, {"id": 1428, "seek": 703584, "start": 7052.32, "end": 7056.64, "text": " And embedding is literally something which takes an integer as an input and looks up", "tokens": [400, 12240, 3584, 307, 3736, 746, 597, 2516, 364, 24922, 382, 364, 4846, 293, 1542, 493], "temperature": 0.0, "avg_logprob": -0.17964453851023027, "compression_ratio": 1.849624060150376, "no_speech_prob": 6.240862603590358e-06}, {"id": 1429, "seek": 703584, "start": 7056.64, "end": 7059.96, "text": " and grabs the corresponding column as output.", "tokens": [293, 30028, 264, 11760, 7738, 382, 5598, 13], "temperature": 0.0, "avg_logprob": -0.17964453851023027, "compression_ratio": 1.849624060150376, "no_speech_prob": 6.240862603590358e-06}, {"id": 1430, "seek": 703584, "start": 7059.96, "end": 7063.24, "text": " So it's doing exactly what we're seeing in this spreadsheet.", "tokens": [407, 309, 311, 884, 2293, 437, 321, 434, 2577, 294, 341, 27733, 13], "temperature": 0.0, "avg_logprob": -0.17964453851023027, "compression_ratio": 1.849624060150376, "no_speech_prob": 6.240862603590358e-06}, {"id": 1431, "seek": 706324, "start": 7063.24, "end": 7066.679999999999, "text": " Question 1.", "tokens": [14464, 502, 13], "temperature": 0.0, "avg_logprob": -0.23129404971474096, "compression_ratio": 1.648780487804878, "no_speech_prob": 2.3187139959190972e-05}, {"id": 1432, "seek": 706324, "start": 7066.679999999999, "end": 7068.92, "text": " How do you deal with missing values?", "tokens": [1012, 360, 291, 2028, 365, 5361, 4190, 30], "temperature": 0.0, "avg_logprob": -0.23129404971474096, "compression_ratio": 1.648780487804878, "no_speech_prob": 2.3187139959190972e-05}, {"id": 1433, "seek": 706324, "start": 7068.92, "end": 7072.4, "text": " So if a user has not rated a particular movie?", "tokens": [407, 498, 257, 4195, 575, 406, 22103, 257, 1729, 3169, 30], "temperature": 0.0, "avg_logprob": -0.23129404971474096, "compression_ratio": 1.648780487804878, "no_speech_prob": 2.3187139959190972e-05}, {"id": 1434, "seek": 706324, "start": 7072.4, "end": 7073.4, "text": " That's no problem.", "tokens": [663, 311, 572, 1154, 13], "temperature": 0.0, "avg_logprob": -0.23129404971474096, "compression_ratio": 1.648780487804878, "no_speech_prob": 2.3187139959190972e-05}, {"id": 1435, "seek": 706324, "start": 7073.4, "end": 7074.76, "text": " So missing values are just ignored.", "tokens": [407, 5361, 4190, 366, 445, 19735, 13], "temperature": 0.0, "avg_logprob": -0.23129404971474096, "compression_ratio": 1.648780487804878, "no_speech_prob": 2.3187139959190972e-05}, {"id": 1436, "seek": 706324, "start": 7074.76, "end": 7078.84, "text": " So if it's missing, I just set the loss to 0.", "tokens": [407, 498, 309, 311, 5361, 11, 286, 445, 992, 264, 4470, 281, 1958, 13], "temperature": 0.0, "avg_logprob": -0.23129404971474096, "compression_ratio": 1.648780487804878, "no_speech_prob": 2.3187139959190972e-05}, {"id": 1437, "seek": 706324, "start": 7078.84, "end": 7081.96, "text": " And then how do you break up a training and test set?", "tokens": [400, 550, 577, 360, 291, 1821, 493, 257, 3097, 293, 1500, 992, 30], "temperature": 0.0, "avg_logprob": -0.23129404971474096, "compression_ratio": 1.648780487804878, "no_speech_prob": 2.3187139959190972e-05}, {"id": 1438, "seek": 706324, "start": 7081.96, "end": 7089.28, "text": " I broke up the training and test set randomly by grabbing random numbers and saying are", "tokens": [286, 6902, 493, 264, 3097, 293, 1500, 992, 16979, 538, 23771, 4974, 3547, 293, 1566, 366], "temperature": 0.0, "avg_logprob": -0.23129404971474096, "compression_ratio": 1.648780487804878, "no_speech_prob": 2.3187139959190972e-05}, {"id": 1439, "seek": 708928, "start": 7089.28, "end": 7106.0, "text": " they greater or less than 0.8 and then split my ratings into two groups based on that.", "tokens": [436, 5044, 420, 1570, 813, 1958, 13, 23, 293, 550, 7472, 452, 24603, 666, 732, 3935, 2361, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.28027042688107956, "compression_ratio": 1.2903225806451613, "no_speech_prob": 4.78505398859852e-06}, {"id": 1440, "seek": 708928, "start": 7106.0, "end": 7108.4, "text": " So here it is.", "tokens": [407, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.28027042688107956, "compression_ratio": 1.2903225806451613, "no_speech_prob": 4.78505398859852e-06}, {"id": 1441, "seek": 708928, "start": 7108.4, "end": 7110.5, "text": " Here's our dot product.", "tokens": [1692, 311, 527, 5893, 1674, 13], "temperature": 0.0, "avg_logprob": -0.28027042688107956, "compression_ratio": 1.2903225806451613, "no_speech_prob": 4.78505398859852e-06}, {"id": 1442, "seek": 708928, "start": 7110.5, "end": 7114.04, "text": " In Keras, there's one other thing.", "tokens": [682, 591, 6985, 11, 456, 311, 472, 661, 551, 13], "temperature": 0.0, "avg_logprob": -0.28027042688107956, "compression_ratio": 1.2903225806451613, "no_speech_prob": 4.78505398859852e-06}, {"id": 1443, "seek": 711404, "start": 7114.04, "end": 7120.1, "text": " I'm going to stop using the sequential model in Keras and start using the functional model", "tokens": [286, 478, 516, 281, 1590, 1228, 264, 42881, 2316, 294, 591, 6985, 293, 722, 1228, 264, 11745, 2316], "temperature": 0.0, "avg_logprob": -0.11675441363626275, "compression_ratio": 1.8725099601593624, "no_speech_prob": 7.296331659745192e-06}, {"id": 1444, "seek": 711404, "start": 7120.1, "end": 7121.1, "text": " in Keras.", "tokens": [294, 591, 6985, 13], "temperature": 0.0, "avg_logprob": -0.11675441363626275, "compression_ratio": 1.8725099601593624, "no_speech_prob": 7.296331659745192e-06}, {"id": 1445, "seek": 711404, "start": 7121.1, "end": 7124.12, "text": " I'll talk more about this next week, but you can read about it later in the week.", "tokens": [286, 603, 751, 544, 466, 341, 958, 1243, 11, 457, 291, 393, 1401, 466, 309, 1780, 294, 264, 1243, 13], "temperature": 0.0, "avg_logprob": -0.11675441363626275, "compression_ratio": 1.8725099601593624, "no_speech_prob": 7.296331659745192e-06}, {"id": 1446, "seek": 711404, "start": 7124.12, "end": 7128.36, "text": " There are two ways of creating models in Keras, the sequential and the functional.", "tokens": [821, 366, 732, 2098, 295, 4084, 5245, 294, 591, 6985, 11, 264, 42881, 293, 264, 11745, 13], "temperature": 0.0, "avg_logprob": -0.11675441363626275, "compression_ratio": 1.8725099601593624, "no_speech_prob": 7.296331659745192e-06}, {"id": 1447, "seek": 711404, "start": 7128.36, "end": 7133.04, "text": " They do similar things, but the functional is much more flexible and it's going to be", "tokens": [814, 360, 2531, 721, 11, 457, 264, 11745, 307, 709, 544, 11358, 293, 309, 311, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.11675441363626275, "compression_ratio": 1.8725099601593624, "no_speech_prob": 7.296331659745192e-06}, {"id": 1448, "seek": 711404, "start": 7133.04, "end": 7134.92, "text": " what we're going to need to use from now on.", "tokens": [437, 321, 434, 516, 281, 643, 281, 764, 490, 586, 322, 13], "temperature": 0.0, "avg_logprob": -0.11675441363626275, "compression_ratio": 1.8725099601593624, "no_speech_prob": 7.296331659745192e-06}, {"id": 1449, "seek": 711404, "start": 7134.92, "end": 7139.5, "text": " So this is going to look slightly unfamiliar, but the ideas are the same.", "tokens": [407, 341, 307, 516, 281, 574, 4748, 29415, 11, 457, 264, 3487, 366, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.11675441363626275, "compression_ratio": 1.8725099601593624, "no_speech_prob": 7.296331659745192e-06}, {"id": 1450, "seek": 713950, "start": 7139.5, "end": 7147.52, "text": " So we create an input layer for a user, and then we say now create an embedding layer", "tokens": [407, 321, 1884, 364, 4846, 4583, 337, 257, 4195, 11, 293, 550, 321, 584, 586, 1884, 364, 12240, 3584, 4583], "temperature": 0.0, "avg_logprob": -0.1345216609813549, "compression_ratio": 1.8012422360248448, "no_speech_prob": 2.014540041272994e-05}, {"id": 1451, "seek": 713950, "start": 7147.52, "end": 7154.52, "text": " for N users, which is 671, and we want to create how many latent factors.", "tokens": [337, 426, 5022, 11, 597, 307, 23879, 16, 11, 293, 321, 528, 281, 1884, 577, 867, 48994, 6771, 13], "temperature": 0.0, "avg_logprob": -0.1345216609813549, "compression_ratio": 1.8012422360248448, "no_speech_prob": 2.014540041272994e-05}, {"id": 1452, "seek": 713950, "start": 7154.52, "end": 7159.4, "text": " I decided not to create 5, but to create 50.", "tokens": [286, 3047, 406, 281, 1884, 1025, 11, 457, 281, 1884, 2625, 13], "temperature": 0.0, "avg_logprob": -0.1345216609813549, "compression_ratio": 1.8012422360248448, "no_speech_prob": 2.014540041272994e-05}, {"id": 1453, "seek": 713950, "start": 7159.4, "end": 7166.88, "text": " And then I create a movie input, and then I create a movie embedding with 50 factors,", "tokens": [400, 550, 286, 1884, 257, 3169, 4846, 11, 293, 550, 286, 1884, 257, 3169, 12240, 3584, 365, 2625, 6771, 11], "temperature": 0.0, "avg_logprob": -0.1345216609813549, "compression_ratio": 1.8012422360248448, "no_speech_prob": 2.014540041272994e-05}, {"id": 1454, "seek": 716688, "start": 7166.88, "end": 7174.58, "text": " and then I say take the dot product of those, and that's our model.", "tokens": [293, 550, 286, 584, 747, 264, 5893, 1674, 295, 729, 11, 293, 300, 311, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16613681316375734, "compression_ratio": 1.5086705202312138, "no_speech_prob": 2.521549731682171e-06}, {"id": 1455, "seek": 716688, "start": 7174.58, "end": 7182.16, "text": " So now please compile the model, and now train it, taking the user ID and movie ID as input,", "tokens": [407, 586, 1767, 31413, 264, 2316, 11, 293, 586, 3847, 309, 11, 1940, 264, 4195, 7348, 293, 3169, 7348, 382, 4846, 11], "temperature": 0.0, "avg_logprob": -0.16613681316375734, "compression_ratio": 1.5086705202312138, "no_speech_prob": 2.521549731682171e-06}, {"id": 1456, "seek": 716688, "start": 7182.16, "end": 7189.6, "text": " the rating as the target, and run it for 6 epochs, and I get a 1.27 loss.", "tokens": [264, 10990, 382, 264, 3779, 11, 293, 1190, 309, 337, 1386, 30992, 28346, 11, 293, 286, 483, 257, 502, 13, 10076, 4470, 13], "temperature": 0.0, "avg_logprob": -0.16613681316375734, "compression_ratio": 1.5086705202312138, "no_speech_prob": 2.521549731682171e-06}, {"id": 1457, "seek": 716688, "start": 7189.6, "end": 7193.76, "text": " This is with an RMSE loss.", "tokens": [639, 307, 365, 364, 23790, 5879, 4470, 13], "temperature": 0.0, "avg_logprob": -0.16613681316375734, "compression_ratio": 1.5086705202312138, "no_speech_prob": 2.521549731682171e-06}, {"id": 1458, "seek": 719376, "start": 7193.76, "end": 7197.04, "text": " Notice that I'm not doing anything else clever, it's just that simple dot product.", "tokens": [13428, 300, 286, 478, 406, 884, 1340, 1646, 13494, 11, 309, 311, 445, 300, 2199, 5893, 1674, 13], "temperature": 0.0, "avg_logprob": -0.20888742125860535, "compression_ratio": 1.5517241379310345, "no_speech_prob": 1.6536815792278503e-06}, {"id": 1459, "seek": 719376, "start": 7197.04, "end": 7199.88, "text": " That gives me a 1.27.", "tokens": [663, 2709, 385, 257, 502, 13, 10076, 13], "temperature": 0.0, "avg_logprob": -0.20888742125860535, "compression_ratio": 1.5517241379310345, "no_speech_prob": 1.6536815792278503e-06}, {"id": 1460, "seek": 719376, "start": 7199.88, "end": 7202.2, "text": " Here's how I add the bias.", "tokens": [1692, 311, 577, 286, 909, 264, 12577, 13], "temperature": 0.0, "avg_logprob": -0.20888742125860535, "compression_ratio": 1.5517241379310345, "no_speech_prob": 1.6536815792278503e-06}, {"id": 1461, "seek": 719376, "start": 7202.2, "end": 7207.76, "text": " I use exactly the same kind of embedding inputs as before, and I've encapsulated them in a", "tokens": [286, 764, 2293, 264, 912, 733, 295, 12240, 3584, 15743, 382, 949, 11, 293, 286, 600, 38745, 6987, 552, 294, 257], "temperature": 0.0, "avg_logprob": -0.20888742125860535, "compression_ratio": 1.5517241379310345, "no_speech_prob": 1.6536815792278503e-06}, {"id": 1462, "seek": 719376, "start": 7207.76, "end": 7208.76, "text": " function.", "tokens": [2445, 13], "temperature": 0.0, "avg_logprob": -0.20888742125860535, "compression_ratio": 1.5517241379310345, "no_speech_prob": 1.6536815792278503e-06}, {"id": 1463, "seek": 719376, "start": 7208.76, "end": 7211.16, "text": " So my user and movie embeddings are the same.", "tokens": [407, 452, 4195, 293, 3169, 12240, 29432, 366, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.20888742125860535, "compression_ratio": 1.5517241379310345, "no_speech_prob": 1.6536815792278503e-06}, {"id": 1464, "seek": 719376, "start": 7211.16, "end": 7218.4400000000005, "text": " And then I create bias by simply creating an embedding with just a single output.", "tokens": [400, 550, 286, 1884, 12577, 538, 2935, 4084, 364, 12240, 3584, 365, 445, 257, 2167, 5598, 13], "temperature": 0.0, "avg_logprob": -0.20888742125860535, "compression_ratio": 1.5517241379310345, "no_speech_prob": 1.6536815792278503e-06}, {"id": 1465, "seek": 721844, "start": 7218.44, "end": 7226.759999999999, "text": " And so then my new model is do a dot product, and then add the user bias, and add the movie", "tokens": [400, 370, 550, 452, 777, 2316, 307, 360, 257, 5893, 1674, 11, 293, 550, 909, 264, 4195, 12577, 11, 293, 909, 264, 3169], "temperature": 0.0, "avg_logprob": -0.1900392958992406, "compression_ratio": 1.5251396648044693, "no_speech_prob": 2.902291953432723e-06}, {"id": 1466, "seek": 721844, "start": 7226.759999999999, "end": 7234.24, "text": " bias, and try fitting that, and it takes me to a validation loss of 1.1.", "tokens": [12577, 11, 293, 853, 15669, 300, 11, 293, 309, 2516, 385, 281, 257, 24071, 4470, 295, 502, 13, 16, 13], "temperature": 0.0, "avg_logprob": -0.1900392958992406, "compression_ratio": 1.5251396648044693, "no_speech_prob": 2.902291953432723e-06}, {"id": 1467, "seek": 721844, "start": 7234.24, "end": 7235.24, "text": " How is that going?", "tokens": [1012, 307, 300, 516, 30], "temperature": 0.0, "avg_logprob": -0.1900392958992406, "compression_ratio": 1.5251396648044693, "no_speech_prob": 2.902291953432723e-06}, {"id": 1468, "seek": 721844, "start": 7235.24, "end": 7240.879999999999, "text": " Well, there are lots of sites on the Internet where you can find out benchmarks for movie", "tokens": [1042, 11, 456, 366, 3195, 295, 7533, 322, 264, 7703, 689, 291, 393, 915, 484, 43751, 337, 3169], "temperature": 0.0, "avg_logprob": -0.1900392958992406, "compression_ratio": 1.5251396648044693, "no_speech_prob": 2.902291953432723e-06}, {"id": 1469, "seek": 724088, "start": 7240.88, "end": 7249.56, "text": " lens, and on the 100,000 dataset, we're generally looking for RMSE of about 0.89.", "tokens": [6765, 11, 293, 322, 264, 2319, 11, 1360, 28872, 11, 321, 434, 5101, 1237, 337, 23790, 5879, 295, 466, 1958, 13, 21115, 13], "temperature": 0.0, "avg_logprob": -0.35626054693151404, "compression_ratio": 0.9642857142857143, "no_speech_prob": 2.7968804715783335e-05}, {"id": 1470, "seek": 724956, "start": 7249.56, "end": 7271.200000000001, "text": " The best one here is 0.89, and this one, RMSE, and that's on the 1 million dataset.", "tokens": [440, 1151, 472, 510, 307, 1958, 13, 21115, 11, 293, 341, 472, 11, 23790, 5879, 11, 293, 300, 311, 322, 264, 502, 2459, 28872, 13], "temperature": 0.0, "avg_logprob": -0.34462676858002284, "compression_ratio": 1.248062015503876, "no_speech_prob": 1.5689271094743162e-05}, {"id": 1471, "seek": 724956, "start": 7271.200000000001, "end": 7277.240000000001, "text": " High 0.89s, low 0.9s would be state of the art according to these benchmarks.", "tokens": [5229, 1958, 13, 21115, 82, 11, 2295, 1958, 13, 24, 82, 576, 312, 1785, 295, 264, 1523, 4650, 281, 613, 43751, 13], "temperature": 0.0, "avg_logprob": -0.34462676858002284, "compression_ratio": 1.248062015503876, "no_speech_prob": 1.5689271094743162e-05}, {"id": 1472, "seek": 727724, "start": 7277.24, "end": 7281.08, "text": " So we're on the right track, but we're not there yet.", "tokens": [407, 321, 434, 322, 264, 558, 2837, 11, 457, 321, 434, 406, 456, 1939, 13], "temperature": 0.0, "avg_logprob": -0.15034031266925715, "compression_ratio": 1.7322175732217573, "no_speech_prob": 3.1381200642499607e-06}, {"id": 1473, "seek": 727724, "start": 7281.08, "end": 7282.08, "text": " So let's try something better.", "tokens": [407, 718, 311, 853, 746, 1101, 13], "temperature": 0.0, "avg_logprob": -0.15034031266925715, "compression_ratio": 1.7322175732217573, "no_speech_prob": 3.1381200642499607e-06}, {"id": 1474, "seek": 727724, "start": 7282.08, "end": 7284.599999999999, "text": " Let's create a neural net.", "tokens": [961, 311, 1884, 257, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.15034031266925715, "compression_ratio": 1.7322175732217573, "no_speech_prob": 3.1381200642499607e-06}, {"id": 1475, "seek": 727724, "start": 7284.599999999999, "end": 7286.04, "text": " And a neural net does the same thing.", "tokens": [400, 257, 18161, 2533, 775, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.15034031266925715, "compression_ratio": 1.7322175732217573, "no_speech_prob": 3.1381200642499607e-06}, {"id": 1476, "seek": 727724, "start": 7286.04, "end": 7291.679999999999, "text": " We create a movie embedding and a user embedding, again with 50 factors, and this time we don't", "tokens": [492, 1884, 257, 3169, 12240, 3584, 293, 257, 4195, 12240, 3584, 11, 797, 365, 2625, 6771, 11, 293, 341, 565, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.15034031266925715, "compression_ratio": 1.7322175732217573, "no_speech_prob": 3.1381200642499607e-06}, {"id": 1477, "seek": 727724, "start": 7291.679999999999, "end": 7295.76, "text": " take a dot product, we just concatenate the two vectors together, stick one on the end", "tokens": [747, 257, 5893, 1674, 11, 321, 445, 1588, 7186, 473, 264, 732, 18875, 1214, 11, 2897, 472, 322, 264, 917], "temperature": 0.0, "avg_logprob": -0.15034031266925715, "compression_ratio": 1.7322175732217573, "no_speech_prob": 3.1381200642499607e-06}, {"id": 1478, "seek": 727724, "start": 7295.76, "end": 7297.9, "text": " or the other.", "tokens": [420, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.15034031266925715, "compression_ratio": 1.7322175732217573, "no_speech_prob": 3.1381200642499607e-06}, {"id": 1479, "seek": 727724, "start": 7297.9, "end": 7301.28, "text": " And because we now have one big vector, we can create a neural net.", "tokens": [400, 570, 321, 586, 362, 472, 955, 8062, 11, 321, 393, 1884, 257, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.15034031266925715, "compression_ratio": 1.7322175732217573, "no_speech_prob": 3.1381200642499607e-06}, {"id": 1480, "seek": 730128, "start": 7301.28, "end": 7311.16, "text": " Create a dense layer, add dropout, create an activation, compile it, and fit it.", "tokens": [20248, 257, 18011, 4583, 11, 909, 3270, 346, 11, 1884, 364, 24433, 11, 31413, 309, 11, 293, 3318, 309, 13], "temperature": 0.0, "avg_logprob": -0.15055274963378906, "compression_ratio": 1.5023474178403755, "no_speech_prob": 2.902297183027258e-06}, {"id": 1481, "seek": 730128, "start": 7311.16, "end": 7317.32, "text": " And after 5 epochs, we get something way better than state of the art.", "tokens": [400, 934, 1025, 30992, 28346, 11, 321, 483, 746, 636, 1101, 813, 1785, 295, 264, 1523, 13], "temperature": 0.0, "avg_logprob": -0.15055274963378906, "compression_ratio": 1.5023474178403755, "no_speech_prob": 2.902297183027258e-06}, {"id": 1482, "seek": 730128, "start": 7317.32, "end": 7320.759999999999, "text": " So we couldn't find anything better than about 0.89.", "tokens": [407, 321, 2809, 380, 915, 1340, 1101, 813, 466, 1958, 13, 21115, 13], "temperature": 0.0, "avg_logprob": -0.15055274963378906, "compression_ratio": 1.5023474178403755, "no_speech_prob": 2.902297183027258e-06}, {"id": 1483, "seek": 730128, "start": 7320.759999999999, "end": 7326.639999999999, "text": " So this whole notebook took me like half an hour to write, so I don't claim to be a collaborative", "tokens": [407, 341, 1379, 21060, 1890, 385, 411, 1922, 364, 1773, 281, 2464, 11, 370, 286, 500, 380, 3932, 281, 312, 257, 16555], "temperature": 0.0, "avg_logprob": -0.15055274963378906, "compression_ratio": 1.5023474178403755, "no_speech_prob": 2.902297183027258e-06}, {"id": 1484, "seek": 730128, "start": 7326.639999999999, "end": 7327.639999999999, "text": " filtering expert.", "tokens": [30822, 5844, 13], "temperature": 0.0, "avg_logprob": -0.15055274963378906, "compression_ratio": 1.5023474178403755, "no_speech_prob": 2.902297183027258e-06}, {"id": 1485, "seek": 732764, "start": 7327.64, "end": 7333.56, "text": " But I think it's pretty cool that these things that were written by people that write collaborative", "tokens": [583, 286, 519, 309, 311, 1238, 1627, 300, 613, 721, 300, 645, 3720, 538, 561, 300, 2464, 16555], "temperature": 0.0, "avg_logprob": -0.17546661951208628, "compression_ratio": 1.6120689655172413, "no_speech_prob": 9.314290946349502e-05}, {"id": 1486, "seek": 732764, "start": 7333.56, "end": 7338.56, "text": " filtering software for a living, that's what these websites are basically coming from,", "tokens": [30822, 4722, 337, 257, 2647, 11, 300, 311, 437, 613, 12891, 366, 1936, 1348, 490, 11], "temperature": 0.0, "avg_logprob": -0.17546661951208628, "compression_ratio": 1.6120689655172413, "no_speech_prob": 9.314290946349502e-05}, {"id": 1487, "seek": 732764, "start": 7338.56, "end": 7346.200000000001, "text": " places that use LensKit, so LensKit is a piece of software for recommender systems, we have", "tokens": [3190, 300, 764, 441, 694, 45626, 11, 370, 441, 694, 45626, 307, 257, 2522, 295, 4722, 337, 2748, 260, 3652, 11, 321, 362], "temperature": 0.0, "avg_logprob": -0.17546661951208628, "compression_ratio": 1.6120689655172413, "no_speech_prob": 9.314290946349502e-05}, {"id": 1488, "seek": 732764, "start": 7346.200000000001, "end": 7348.76, "text": " just killed their benchmark.", "tokens": [445, 4652, 641, 18927, 13], "temperature": 0.0, "avg_logprob": -0.17546661951208628, "compression_ratio": 1.6120689655172413, "no_speech_prob": 9.314290946349502e-05}, {"id": 1489, "seek": 732764, "start": 7348.76, "end": 7352.4400000000005, "text": " And it took us 10 seconds to train.", "tokens": [400, 309, 1890, 505, 1266, 3949, 281, 3847, 13], "temperature": 0.0, "avg_logprob": -0.17546661951208628, "compression_ratio": 1.6120689655172413, "no_speech_prob": 9.314290946349502e-05}, {"id": 1490, "seek": 732764, "start": 7352.4400000000005, "end": 7353.4400000000005, "text": " So I think that's pretty neat.", "tokens": [407, 286, 519, 300, 311, 1238, 10654, 13], "temperature": 0.0, "avg_logprob": -0.17546661951208628, "compression_ratio": 1.6120689655172413, "no_speech_prob": 9.314290946349502e-05}, {"id": 1491, "seek": 735344, "start": 7353.44, "end": 7373.32, "text": " We're right on time, so we're going to take one last question.", "tokens": [492, 434, 558, 322, 565, 11, 370, 321, 434, 516, 281, 747, 472, 1036, 1168, 13], "temperature": 0.0, "avg_logprob": -0.20028327306111654, "compression_ratio": 1.3973509933774835, "no_speech_prob": 5.390806109062396e-05}, {"id": 1492, "seek": 735344, "start": 7373.32, "end": 7377.48, "text": " So that was a very, very quick introduction to embeddings.", "tokens": [407, 300, 390, 257, 588, 11, 588, 1702, 9339, 281, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.20028327306111654, "compression_ratio": 1.3973509933774835, "no_speech_prob": 5.390806109062396e-05}, {"id": 1493, "seek": 735344, "start": 7377.48, "end": 7383.28, "text": " As per usual in this class, I kind of stick the new stuff in at the end and say, go study", "tokens": [1018, 680, 7713, 294, 341, 1508, 11, 286, 733, 295, 2897, 264, 777, 1507, 294, 412, 264, 917, 293, 584, 11, 352, 2979], "temperature": 0.0, "avg_logprob": -0.20028327306111654, "compression_ratio": 1.3973509933774835, "no_speech_prob": 5.390806109062396e-05}, {"id": 1494, "seek": 738328, "start": 7383.28, "end": 7384.28, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.19321395690182605, "compression_ratio": 1.5205479452054795, "no_speech_prob": 2.7967951609753072e-05}, {"id": 1495, "seek": 738328, "start": 7384.28, "end": 7389.8, "text": " So your job this week is to keep improving State Farm, hopefully win the new fisheries", "tokens": [407, 428, 1691, 341, 1243, 307, 281, 1066, 11470, 4533, 19991, 11, 4696, 1942, 264, 777, 20698, 530], "temperature": 0.0, "avg_logprob": -0.19321395690182605, "compression_ratio": 1.5205479452054795, "no_speech_prob": 2.7967951609753072e-05}, {"id": 1496, "seek": 738328, "start": 7389.8, "end": 7390.8, "text": " competition.", "tokens": [6211, 13], "temperature": 0.0, "avg_logprob": -0.19321395690182605, "compression_ratio": 1.5205479452054795, "no_speech_prob": 2.7967951609753072e-05}, {"id": 1497, "seek": 738328, "start": 7390.8, "end": 7395.92, "text": " By the way, in the last half hour, I just created this little notebook in which I basically", "tokens": [3146, 264, 636, 11, 294, 264, 1036, 1922, 1773, 11, 286, 445, 2942, 341, 707, 21060, 294, 597, 286, 1936], "temperature": 0.0, "avg_logprob": -0.19321395690182605, "compression_ratio": 1.5205479452054795, "no_speech_prob": 2.7967951609753072e-05}, {"id": 1498, "seek": 738328, "start": 7395.92, "end": 7402.96, "text": " copied the Dogs and Cats Redux competition into something which does the same thing with", "tokens": [25365, 264, 35504, 293, 40902, 4477, 2449, 6211, 666, 746, 597, 775, 264, 912, 551, 365], "temperature": 0.0, "avg_logprob": -0.19321395690182605, "compression_ratio": 1.5205479452054795, "no_speech_prob": 2.7967951609753072e-05}, {"id": 1499, "seek": 738328, "start": 7402.96, "end": 7407.9, "text": " the fish data, and I quickly submitted a result.", "tokens": [264, 3506, 1412, 11, 293, 286, 2661, 14405, 257, 1874, 13], "temperature": 0.0, "avg_logprob": -0.19321395690182605, "compression_ratio": 1.5205479452054795, "no_speech_prob": 2.7967951609753072e-05}, {"id": 1500, "seek": 740790, "start": 7407.9, "end": 7415.12, "text": " So we currently have one of us in 18th place, so hopefully you can beat that tomorrow.", "tokens": [407, 321, 4362, 362, 472, 295, 505, 294, 2443, 392, 1081, 11, 370, 4696, 291, 393, 4224, 300, 4153, 13], "temperature": 0.0, "avg_logprob": -0.2120723886004949, "compression_ratio": 1.3540372670807452, "no_speech_prob": 1.4061178262636531e-05}, {"id": 1501, "seek": 740790, "start": 7415.12, "end": 7419.44, "text": " Most importantly, download the MovieLens data and have a play with that, and we'll talk", "tokens": [4534, 8906, 11, 5484, 264, 28766, 43, 694, 1412, 293, 362, 257, 862, 365, 300, 11, 293, 321, 603, 751], "temperature": 0.0, "avg_logprob": -0.2120723886004949, "compression_ratio": 1.3540372670807452, "no_speech_prob": 1.4061178262636531e-05}, {"id": 1502, "seek": 740790, "start": 7419.44, "end": 7421.4, "text": " more about embeddings next week.", "tokens": [544, 466, 12240, 29432, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.2120723886004949, "compression_ratio": 1.3540372670807452, "no_speech_prob": 1.4061178262636531e-05}, {"id": 1503, "seek": 742140, "start": 7421.4, "end": 7438.44, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 51216], "temperature": 0.0, "avg_logprob": -0.8822329044342041, "compression_ratio": 0.5555555555555556, "no_speech_prob": 8.499134128214791e-05}], "language": "en"}