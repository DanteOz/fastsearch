{"text": " I thought what we might do today is to like Finish off where we were in this Rossman notebook looking at time series forecasting and structured data analysis And then we might do a little mini review Like everything we've learned because believe it or not This is the end like there's nothing more to know about machine learning other than everything that you're going to learn next semester and For the rest of your life But anyway, I got nothing else to teach Yes, I'll do a little review and then we'll cover like the most important part of the course which is like Thinking about like what are the what are the what are the how are ways to think about how to use this kind of technology? appropriately And you know effectively in a way that's a positive hopefully a positive impact on society So last time we got to the point where we talked a bit about This this idea that when we were looking at like building this competition months open derived variable But we actually truncated it down to be no more than 24 months And we talked about the reason why being that we actually wanted to use it as a categorical variable because categorical variables thanks to embeddings have more Flexibility and how the neural net can can use them And so that was kind of where we where we left off So let's like keep working through this because What's happening in this notebook is? Stuff which is probably going to apply to most time series data sets that you work with And as we talked about like although we use DF dot apply here This is something where it's running a piece of Python code over every row and it's that's horrifically slow right so we only do that if we can't find a Vectorized pandas or numpy function that can do it to the whole column at once But in this case I couldn't find a way to convert a year and a week number into a date without using arbitrary Python Also worth remembering this idea of a lambda function Any time you're trying to apply a function to every row of something or every element of a tensor or something like that if there isn't a vectorized version already you're going to have to call something like Data frame dot apply which will run a function you pass to every element So this is something like you know kind of This is basically a map in functional programming Since very often the function that you want to pass to it is something you're just going to use once and then throw it away It's really common to use this Lambda approach so this lambda is can creating a function Just for the purpose of telling DF dot apply what to use right so we could We could also have written this in a different way Which would have been to say define Create Chromo To sense on some value Return And then we could put that in here Okay, so that and that are the same thing Okay, so one approach is to define the function and then pass it by name or the other is to define the function in place Using lambda all right, and so if you're not Comfortable creating and using lambdas you know good thing to practice and playing around with DF dot apply is a good way to Good way to practice it Okay So let's talk about this durations section which may at first seem a Little specific, but actually it turns out not to be what we're going to do is we're going to look at three fields promo state holiday and School holiday and so basically what we have is a table of for each store For each date Does that store have a promo going on at that date? Is there a school holiday in that region of that store at that date is there a state holiday? in that region for that store at that date okay, and so this kind of thing is You know like their events and time series with events are like very very common like if you're looking at Oil and gas drilling data you're trying to say like the flow through this pipe You know here's an event representing when it set off some alarm You know or here's an event where the drill got stuck or or whatever right and so like most time series At some level will tend to represent some events so The fact that an event happened at a time is Is is interesting itself, but very often a time series will also show some something happening before and after The event so for example in this case. We're doing grocery sales prediction If there's a holiday coming up It's quite likely that sales will be higher before and after the holiday and lower during the holiday If this is a city-based store Right because you know you're going to you're going to stock up before you go away to Bring things with you and when you come back you've got a refill the fridge for instance right? So it's Although like we don't necessarily have to do this kind of feature engineering to create features Specifically about like this is before or after a holiday The the neural net you know the more we can give the neural net like the kind of information it needs The less it's going to have to learn it the less that it's going to have to learn it the more we can do with The data or we already have the more we can do with the you know the size architecture We already have so feature engineering even even with stuff like neural nets Is still important because it means that you know we'll be able to do You know get better results with whatever limited data. We have whatever limited computation we have So the basic idea here therefore is When we have events in our time series is we want to create Two new columns for each event How long is it going to be until the next time this event happens and? How long has it been since the last time that event happened so in other words how long until the next state holiday? How long since the previous state holiday? Okay, so that's not something which I'm aware of as existing as a Library or anything like that so we I wrote it here by hand right and so importantly I need to do this by store Right so I want to say like because you know for this store Or when was this stores last promo so how long has it been since the last time it had a promo? How long it will be until the next time it has a promo for instance? all right, so Here's what I'm going to do. I'm going to create a little function That's going to take a field name, and I'm going to pass it each of promo and then state holiday and then school holiday Right so let's do school holiday for example, so we'll say field equals school holiday and then we'll say Get elapsed school holiday comma After so let me show you what that's going to do so we've got a first of all sort by store and date Right so now when we loop through this we're going to be looping through within a store so store number one January the first January the second January the third and so forth and as we loop through each store we're basically going to say like is Is is this row a school holiday or not and if it is a school holiday? Then we'll keep track of this variable called last date which says this is the last date and which where we saw a school holiday Okay, and so then we're basically going to append to our result The number of days since the last school holiday. That's the kind of basic idea here, so there's a few interesting features One is the use of zip Right so I could actually write this much more simply right I could say Let's go through Well we could basically go through like for row in DF dot it a rose Right and then grab the the fields we want from each row It turns out this is 300 times slower than the version that I have and basically like iterating through a data frame And extracting specific fields out of a row Has a lot of overhead what's much faster is to iterate through a numpy array So If you take a series like DF dot store and add dot values after it that grabs a numpy array of that series Okay, so here are three numpy arrays one is the store IDs one is Whatever field is in this case. Let's say school holiday and one is the date So now what I want to do is loop through the first one of each of those lists and Then the second one of each of those lists and then the third one of each of those lists and Like this is a really really common pattern I need to do something like this in basically every notebook I write and the way to do it is with zip All right, so zip means loop through each of these lists One at a time and then this here is where we can grab That element out of the first list the second list and the third list okay, so if you haven't played around watch with zip That's a really important function to practice with like I say I use it in pretty much every notebook I write All the time you have to loop through You know a bunch of lists at the same time All right, so we're going to look through every store Every school holiday at every date Yes So is it looping through like all the possible combinations of each of those or just Yeah, exactly one one two three two two yeah Thanks for the question so in this case we basically want to say let's grab the first store The first school holiday the first date right so for store one January the first School holiday was true or false right and so if it is a school holiday I'll keep track of that fact by saying the last time I saw a school holiday was that day Okay, and then Append how long has it been since the last school holiday right and if the? Store ID is different to the last door ID I saw then I've now got to a whole new store in which case I have to basically reset Everything okay, you're part that to go What will happen to the first points that we don't have a like last Holiday yeah, so I just said I basically set this to some Arbitrary starting point it's going to end up with like the I can't remember side the light largest or the smallest possible date And you know you may need to Replace this with a missing value afterwards or some you know the zero or or whatever You know the nice thing is though thanks to Reliou's it's very easy for a neural net to kind of cut off extreme values So in this case I didn't do anything special with it. I ended up with these like negative a billion day time Stamps and it still worked fine Okay, so we can go through and so the next thing to note is There's a whole bunch of stuff that I need to do to both the training set and the test set right so in the previous Section I actually kind of added this little loop where I go for each of the Training data frame and the test data frame Do these things right so I kind of you know each cell I did for each of the data frames I've now got a whole coming up a whole Series of cells that I want to run first of all for the training set and then for the test set So in this case the way I did that was I had two different cells here One which set DF to be the training set one which set to be the test set and so the way I use this is I run just this cell Right and then I run all the cells underneath Right so it does it all to the training set and then I come back and run just this cell and then run all the cells underneath okay, so like this Notebook is not designed to be just run from top to bottom, but it's designed to be run in this Particular way and I mentioned that because like this can be a handy Trick to know like you could of course put all the stuff underneath in a function That you pass a data frame to and call it once with a test set once with a training set but I kind of like to Experiment a bit more interactively look at each step as I go so this way is an easy way to kind of run something on two different data frames without turning it into a function Okay So this is going to if I sort by store and by date Then this is keeping track of the last time something happened And so this is therefore going to end up telling me how many days was it since the last? School holiday right so now if I sort date descending And call the exact same function Then it's going to say how long until the next school holiday right, so that's a kind of a nice little trick for Adding these kind of event timers arbitrary event timers into your time series models That so if you're doing for example the Ecuadorian groceries competition right now You know maybe this kind of approach would be useful for various events in that as well Do it for state holiday do it for promo there we go, okay? The next thing that we look at here is rolling functions So rolling functions is how we rolling in pandas is how we what we call create what we call windowing functions so Let's say I had some data You know something like this right and This is like date and I don't know this is like sales or whatever What I could do is I could say like okay, let's create a window around this point of like seven days right so it'd be like okay, this is a Seven day window say right and so then I could take the average Sales in that seven day window and then I could like do the same thing like I don't know over here All right take the average sales Over that seven day window right and so if we do that for every point and join up those averages You're going to end up with a Moving average okay, so the kind of the the more generic version of the moving average Is a window function Hi something where you apply to some function to some window of data around each point Now very often the windows that I've shown here are not actually What you want if you're trying to build a predictive model you can't include the future as part of a moving average Right so quite often you actually need a window That ends here So that would be our window function right and so Pandas lets you create Window func arbitrary window functions using this rolling here this here says how many? Time steps do you do I want to apply the function to? Okay, this here says if I'm at the edge So in other words if I'm like out here Should you have should you make that a? Missing value because I don't have seven days to average over or you know How what's the minimum number of time periods to use that so here I said one okay? And then optionally you can also say do you want to set the window at the start of a period or the end of a? Period or the middle if the period okay, so and then within that you can then apply whatever function you like Okay, so here. I've got my weekly by store sums Okay, so there's a nice easy way of getting Kind of moving averages or or whatever else and you know I should mention in pandas If you go to the time series page on pandas there's literally like look at just the index here time series functionality was all of This is this like there's lots because like where's bikini who created this he was originally in hedge fund trading I believe and You know his work was all about time series And so I think like pandas originally was very focused on time series and still You know it's perhaps the strongest part of pandas So if you're playing like if you're playing around with time series computations You definitely owe it to yourself to try to learn this entire API and like it there's a lot of kind of conceptual pieces around like Timestamps and date offsets and resampling and stuff like that to kind of get your head around but it's totally worth it because Otherwise you'll be writing this stuff as loops by hand it's going to take you a lot longer than leveraging what pandas already does and And of course pandas will do it in you know highly optimized C-code for you vectorized C code where else your version is going to loop in Python so definitely worth you know if you're doing stuff in time series learning the full pandas time series API Just about as about as strong as any time series API out there Okay, so at the end of all that you can see here's those kind of starting point values I mentioned slightly on the extreme side and so you can see here the 17th of September Store one was 13 days after the last school holiday the 16th was 12 11 10 so forth okay We're currently in a promotion All right here. This is one day before the promotion here. We've got nine days after the last promotion And so forth okay So that's how we can add Kind of event counters to a time series and probably always a good idea when you're doing work with time series So now that we've done that you know we've got lots of columns in our data set And so we split them out into categorical versus continuous columns We'll talk more about that in a moment in the review section But so these are going to be all the things I'm going to create an embedding for Okay, and these are all of the things that I'm going to feed directly into the into the model so for example, we've got like Competition distance so that's distance to the nearest competitor maximum temperature and Here we've got day of week right so So here we've got maximum temperature Maybe it's like 22.1 because they use centigrade in Germany We've got distance to nearest competitor might be 321 kilometers point seven Alright, and then we've got day of week Which might be I don't know maybe Saturday is a six okay? So these numbers here are going to go straight into Our vector right the vector that we're going to be feeding into our neural net All right, twenty two one Three twenty one point seven Okay We'll see in a moment. We'll actually will normalize them, but more or less But this categorical variable we're not we need to put it through an embedding right so we'll have some embedding matrix Right of if there are seven days By I don't know maybe dimension four embedding okay, and so this will look up the sixth row to get back the Four items right and so this is going to turn into Length for vector which will then add here Okay, so that's how our continuous and categorical variables are going to work so Then all of our Categorical variables will turn them into pandas categorical variables in the same way that we've done before And Then we're going to apply the same mappings to the test set right so if Saturday is a six in the training set this apply cats makes sure that Saturday is also a six in the test set For the continuous variables make sure they're all floats because pi torch expects everything to be afloat So then this is another little trick that I use Both of these cells define something called joined sample one of them defines them as the whole training set One of them defines them as a random subset Right and so the idea is that I do all of my work on the sample Make sure it all works well play around with different hyper parameters and architectures, and then when I'm like okay I'm very happy with this I then go back and run this line of code To say okay now make that make the whole data set be the sample and then rerun it Okay, so this is a good way again similar to the what I showed you before It lets you use the same cells in your notebook to run first of all on the sample and then go back later and run It on the full data set okay? So now that we've got that joined stamp We can then pass it to proc DF as we've done before to grab the dependent variable To deal with missing values and in this case we pass one more thing which is do scale equals true do scale equals true will subtract the mean and divide by the standard deviation and So the reason for that is that if our first layer? You know it's just a matrix multiply right so here's our set of weights and our input is like I Don't know it's got something which is like point. Oh oh one And then it's got something like which is like 10 to the 6 Right and then our weight matrix has been initialized to be like random numbers between 0 and 1 Right so got like 0.6. Oh point 1 etc then basically this thing here is going to have Gradients that are nine orders of magnitude bigger than this thing here, which is not going to be good for optimization Okay, so by normalizing everything to be mean of zero standard deviation of one to start with Then that means that all of the gradients are going to be you know on the same kind of scale We didn't have to do that in random forests Right because in random forests. We only cared about the sort order. We didn't care about the values at all right, but in that with Linear models and things that are built out of layers of linear models like ie neural nets we care very much About the scale Okay, so do scale equals true Normalizes our data for us now since it normalizes our data for us it returns one extra Object which is a mapper which is an object that contains for each continuous variable What was the mean and standard deviation it was normalized with? the reason being That we're going to have to use the same Mean and standard deviation on the test set All right, because we need our test set and our training set to be scaled in the exact same way otherwise They're going to have different meanings Okay, and so these Details about making sure that your test and training set have the same categorical codings the same missing value replacement and the same Same scaling normalization are really important to get right Because if you don't get it right then your test set is you know not going to work at all Okay But if you follow these steps, you know it'll work fine We also take the log of the dependent variable and that's because in this Kaggle competition The evaluation metric was root mean squared percent error So root means great percent error means we're being penalized based on the ratio between our answer and the correct answer We don't have a loss function in pytorch called root means great percent error we could write one But easier is just to take the log of the dependent because the difference between logs is the same as the ratio Okay, so by taking the log we kind of get that for free you'll notice like the vast majority of regression competitions on Kaggle use Either root mean squared percent error or root means greater error of the log as their evaluation metric And that's because in real world problems most of the time we care more about ratios Than about raw differences So if you're designing Your own project it's quite likely that you'll want to think about using log of your dependent variable So then we create a validation set and as we've learned before Most of the time if you've got a problem involving a time component your validation set probably wants to be the most recent Time period rather than a random subset Okay, so that's what I do here When I finished modeling and I found an architecture and a set of hyper parameters and a number of epochs and all that stuff that Works really well If I want to make my model as good as possible. I'll retrain on the whole thing Right including the validation set right now currently at least fast AI assumes that you do have a validation set So my kind of hacky workaround is to set my validation set to just be one index which is the first row Okay, and that way like all the code keeps working, but there's no real validation set so obviously if you do this you need to make sure that your Final training is like the exact same hyper parameters the exact same number of epochs exactly the same as the thing that worked Because you don't actually have a proper validation set now to check against I have a question regarding Get elapsed function which we discussed before So in get elapsed function we are trying to find When is the next holiday? Like when will the next holiday come how many how many days away is it? So every year the holidays are more or less fixed like there will be holiday on 4th of July 25th of December and There is hardly any change So can't we just look from previous years and just get a list of all the holidays that are going to occur this year Maybe I mean in this case, I guess like that's not true of promo, right and Some holidays change like Easter, you know So like this this this way I get to write one piece of code that works for all of them You know and it doesn't take very long to run So yeah, so there might be ways if you're if your data set was so big that this took too long You could maybe do it on one year and then kind of somehow copy it But yeah in this case there was no need to and I gen you know I always value My time over my computer's time, so I try to keep things as simple as I can Okay So now we can create our model And so to create our model we have to create a model data object as we always do with fast AI So a columnar model data object is just a data a model data object that represents a training set a validation set and an Optional test set of standard columnar, you know structured data, okay? And we just have to tell it which of the variables should we treat as categorical? Okay, and then pass in our data frames So for each of our categorical variables Here is the number of categories it has okay So for each of our embedding matrices This tells us the number of rows in that embedding matrix and So then we define what embedding dimensionality we want If you're doing like natural language processing Then the number of dimensions you need to capture all the nuance of what a word means and how it's used Has been found empirically to be about 600 It turns out that when you do NLP models with embedding matrices that are That are smaller than 600 You don't get as good of results as you do if you do if there's the size 600 beyond 600. It doesn't seem to improve much I would say that human language is one of the most complex things that we model So I wouldn't expect you to come across many if any Categorical variables that need embedding matrices with more than 600 dimensions at the other end You know some things may have pretty simple kind of causality right so for example Let's have a look at State holiday You know Maybe if something's a holiday Then it's just a case of like okay at stores that are in the city. There's some behavior There's stores that are in the country. There's some other behavior, and that's about it. You know like maybe it's a pretty pretty simple relationship so like ideally When you decide what? embedding size to use You would kind of use your knowledge about The domain to decide like how complex is the relationship and so how big? Embedding do I need right in practice? You almost never know that Right like you would only know that because maybe somebody else has previously done that research and figured it out like in an LP So in practice you probably need to use some Rule of thumb okay, and then having tried your rule of thumb You could then maybe try a little bit higher and a little bit lower and see what helps so it's kind of experimental So here's my rule of thumb my rule of thumb is look at how how many discrete values the category has I eat a number of rows in the embedding matrix and Make the dimensionality of the embedding half of that Right so for day of week Which is the second one? eight rows and four colors So here it is there right the number of categories divided by two But then I say don't go more than 50 right so here you can see for store There's a thousand stores only have a dimensionality of 50 why 50 I don't know it seems to have worked okay so far like you may find you need something a little different Actually for the Ecuadorian groceries competition. You know I haven't really tried playing with this But I think we may need some larger embedding sizes But it's something to fiddle with Prince can you pass that left So as your variables the cardinality size becomes larger and larger You're creating more and more like Bigger and bigger wider very much serious on you therefore massively risking overfitting which is introducing so many parameters The model can never possibly capture all that Variation that's your data is absolutely huge. That's a great question And so let me remind you about my kind of like golden rule with the difference between modern machine learning and old machine learning In all machine learning we control complexity by reducing the number of parameters in modern machine learning we control complexity by regularization So the short answer is no I'm not concerned about overfitting because the way I avoid overfitting is not by reducing the number of parameters but by increasing my dropout or increasing my weight decay Okay Now having said that like there's no point using more parameters for a particular embedding than I need like because regularization Like is penalizing a model by giving it like more random data or by actually penalizing weights So we like we'd rather not use more than we have to But the kind of my general rule of thumb for designing an architecture is to you know be generous on the side of the number of Parameters, but yeah in this case if after doing some work. We kind of felt like you know what? The store doesn't actually seem to be that Important then I might manually go and like change this to make it smaller You know or if I was really finding there's not enough data here I'm either over fitting or I'm using more regularization than comfortable with again You know then you might go back, but I would always start with like being generous with parameters And yeah in this case This model turned out pretty good Okay, so now we've got a list of tuples Containing the number of rows and columns of each of our embedding matrices And so when we call get learner to create our neural net that's the first thing we pass in That is how big is each of our embeddings, okay? And then we tell it how many continuous variables we have We tell it how many activations to create for each layer, and we tell it what dropout to use for each layer Okay, and so then we can go ahead and Call fit okay So then we fit for a while and we're kind of getting something around the the point one mark all right, so I tried Running this on the test set and I submitted it to Kaggle during the week actually last week and Here it is okay private score 107 public score 103 Okay, so let's have a look and see how that would go so 107 private 103 public so let's start on public which is 103 Not there out of 3000 got to go back a long way Okay There it is 103 okay 340th, huh, that's not good So on the public leaderboard 340th, let's try the private leaderboard Which is 107 oh Fifth So like hopefully you're now thinking oh There are some Kaggle competitions finishing soon Which I entered and I spent a lot of time trying to get good results on the public leaderboard I wonder if that was a good idea and the answer is no it won't right the Kaggle public leaderboard is Not meant to be a replacement for your carefully developed validation set All right, so for example if you're doing the iceberg competition right which ones are ships which ones are icebergs Then they've actually put Something like 4,000 synthetic images into the public leaderboard and none into the private leaderboard Okay, so This is one of the really good kind of Things that tests you out on Kaggle is like are you creating a good validation set and are you trusting it? right because if you're trusting your leaderboard feedback More than your validation feedback then you may find yourself in 350th place when you thought you're in fifth, right? So in this case we actually had a pretty good validation set right because as you can see it's saying like somewhere around Point one and we actually did get somewhere around point one Okay, and so in this case the validation set that's our the public leaderboard in this competition was Entirely useless yeah Can you use the box please? so in regards to that how much does the top of the public leaderboard actually correspond to the top of the private leaderboard because in the in the churn prediction challenge, there's like four people who are just Completely above everyone else it totally depends you know like if they randomly Sampled the public and private leaderboard Then it should be extremely indicative right But it might not be right so in this case I was crushed Here it comes so in this case the person who was second on the public leaderboard did end up winning SDNT Came seventh right so in fact you can see the little green thing here right where else this guy jumped 96 places If we had entered with the neural net we just looked at we would have jumped 350 places so it yeah It just depends and so often like You can figure out Where the the public leaderboard like sometimes they'll tell you the public leaderboard was randomly sampled sometimes. They'll tell you it's not Generally you have to figure it out by looking at the correlation between your validation set results and the public leaderboard results To see how well they're correlated Sometimes if like two or three people are way ahead of everybody else they may have found some kind of leakage Or something like that Like that's often a sign that there's some trick Okay So that's Rossman And that brings us to the end of all of our material So let's come back after the break and do a quick review And then we will talk about ethics and machine learning so let's come back at in five minutes So we've learned two ways to train a model One is by building a tree and one is with SGD Okay, and so the SGD approach is a way we can train a model which is a Linear model or a stack of linear layers with nonlinearities between them Where else tree building Specifically will give us a tree right and then tree building we can combine with Bagging to create a random first or with boosting to create a GBM Or various other slight variations such as extremely randomized trees So it's worth like reminding ourselves of like what these things do So Let's let's look at some data So if we've got some data Like so actually let's look specifically let's look specifically at categorical data, right? Okay, so categorical data There's a couple of possibilities of what categorical data might look like it could be like okay So let's say we've got zip code like so we've got line for double oh three is our zip code right and then we've got like sales Right and it's like 50 and Like nine four one three one sales of 22 and so forth right so we've got some categorical variable So there's a couple of ways We could represent that categorical variable one would be just to use The number right and like maybe it wasn't a number to start You know maybe it wasn't a number at all maybe a categorical variable is like San Francisco New York Mumbai and Sydney Right, but we can turn it into a number just by like arbitrarily deciding to give them numbers right so like it ends up being a number So we could just use that kind of arbitrary number so if if it turns out that zip codes that are Numerically next to each other have somewhat similar behavior then the zip code versus sales Chart might look something like this For example right or alternatively If the zip code versus sales Like sorry if the two zip codes next to each other didn't have in any way similar Similar sales behavior you would expect to see something that looked more like this Like kind of just all over the place right Okay, so they're the kind of two possibilities So what a random forest would do if we had just encoded zip in this way is It's gonna say alright. I need to find my single Best split point okay the split point that's going to make the two sides have as Smaller standard deviation as possible or mathematically equivalently Have the lowest root means whatever so in this case it might pick Here As our first bit point because on this side There's one average and on the other side There's the other average Okay, and then for its second split point. It's going to say okay. How do I split this and? It's probably going to say I would split here Right because now we've got this average versus this average Right and then finally it's going to say okay. How do we split here, and it's going to say okay? I'll split there right so now I've got that average and that average okay So you can see that it's able to kind of hone in on the set of spits It needs even though it kind of does it greedily top down one at a time right the only reason it wouldn't be able to Do this is if like it was just such bad luck that the two halves were kind of always exactly balanced Right but even if that happens It's not going to be the end of the world it'll spit on something else some other variable and next time around You know it's very unlikely that it's still going to be exactly balanced in both parts of the tree right so in practice this works Just fine In the second case Right it can do exactly the same thing right it'll say like okay, which is my best? First split right even though there's no relationship between one zip code and its neighboring zip code numerically We can still see here if it if it's bits here Right there's the average on one side and the average on the other side is probably about here Right and then where would it spit next? Probably here Right because here's the average on one side here's the average on the other side Right so again can do the same thing right it's going to need more splits because it's going to end up Having to kind of narrow down on each individual large zip code and each individual small zip code, but it's still going to be fine Okay, so when we're dealing with Building decision trees for random forests or GBMs or whatever we tend to encode variables just as Ordinals okay on the other hand if we were doing a Neural network or like the simplest version like a Linear regression or a logistic regression the best it could do Is that? Right which is no good at all and ditto with this one. It's going to be like that okay, so an ordinal Is not going to be a useful encoding for a linear model or something that stacks? linear and nonlinear models together So instead what we do is we create a one-hot encoding right so we'll say like you know There's zero one zero zero zero. Here's zero one. Oh, oh here's oh one oh oh one Okay, and so with that encoding it can effectively create like a little histogram Right where it's going to have a different Coefficient for each level and so that way it can do exactly what it needs to do can you pass that back, please? At what point does that become like too tedious for your system, or does it not pretty much never yeah? Because remember in real life. We don't actually actually we don't actually have to create That matrix instead we can just you know have the four coefficients Right and just do an index lookup to grab the second one which is mathematically equivalent to multiply by the one-hot encoding Okay, so so that's no problem One thing to mention you know I know You guys have kind of been taught quite a bit of more like analytical solutions to things and in analytical solutions to Like a linear regression you get You can't solve something with this amount of collinearity in other words Sydney Something you know something is Sydney if it's not Mumbai or New York or San Francisco So in other words there's a hundred percent collinearity Between the fourth of these classes versus the other three and so if you try to solve a linear aggression analytically that way The whole thing falls apart now note With SGD we have no such problem Okay, like SGD. Why would it care right? We're just taking one step along the derivative It cares a little right because like in the end the main problem with Collinearity is that there's an infinite number of equally good solutions, right? so in other words we could increase all of these and decrease this or Decrease all of these and increase this and they're going to balance out right and When there's an infinitely large number of good solutions that means there's a lot of kind of Flat spots in the loss surface, and it can be harder to optimize Right so it's a really easy way to get rid of all of those flat spots Which is to add a little bit of regularization so if we added a little bit of a little bit of weight decay Like one e neg seven even then that basically says these are not all equally good anymore the one which is the best is the one Where the parameters are the smallest and the most similar to each other and so that'll again move it back to being a nice loss function yes Could you just clarify that point you made about why one hard coding wouldn't be that tedious sure? If we have a one hot encoded vector right, and we are multiplying it by a set of coefficients Right then that's exactly the same thing as simply saying let's grab the thing where the one is Right so in other words if we had stored this as a zero You know and this one has a one and this one is a two right? Then it's exactly the same as just saying hey look up that thing in the array Okay, and so we call that version and embedding right so an embedding is a Multiplicate is a weight matrix you can multiply by a one hot encoding And it's just a computational shortcut, but it's mathematically the same So there's a key difference So the first you know key differences between like solving linear type models analytically versus with SGD with SGD we don't have to worry about Collinearity and stuff or at least not nearly to the same degree And then the difference between solving a linear or Single layer or multi-layer model with SGD versus a tree a tree is going to be Versus a tree a tree is going to be like it's going to complain about less things right so in particular you can just use Ordinals as your categorical variables and as we learned just before We also don't have to worry about Normalizing continuous variables for a tree, but we do have to worry about it for these SGD trained models So then we also learned a lot about interpreting random forests in particular and If you're interested you may be interested in trying to use those same techniques to interpret Neural nets Right so if you want to know which of my features are important in a neural net You could try the same thing try shuffling each column in turn and see how much it changes your accuracy Okay, and that's going to be your feature importance for your neural net and then if you really want to have fun Recognize then that shuffling that column is Just a way of calculating how sensitive the output is to that input which in other words is the derivative of The output with respect to that input and so therefore Maybe you could just ask pipe torch to give you the derivatives with respect to the input directly and see if that gives you the same Kind of answers right? You could do the same kind of thing for a partial dependence plot you could try you know doing the exact same thing with your neural net Replace everything in a column with the same value do it for 1960 1961 1962 plot that right? I don't know of anybody who's done these things before not because it's rocket science but just because I don't know maybe no one thought of it or It's not in a library I don't know but if somebody tried it I think you should find it useful make a great blog post maybe even a paper if you wanted to take it a bit further So there's a thought that something you could do so those most of those interpretation techniques are not particularly specific to random forests Things like the tree interpreter certainly are because they're all about like what's inside the tree. Can you pass it to Karen? We are applying three interpreter for neural nets How are we going to make inference out of activations that the path follows for example? So how are we going to life in three interpreter? We are like We're looking at the path We are looking at the parts and their contributions of the features in this case It will be same with activations. I guess the contributions of each activation on their path. Yeah, maybe How are you know I haven't thought about it? How can we like making friends out of the activations? So I'd be careful say the word inference because no people normally is the word inference specifically to mean the same as like a test Attest time prediction you make like make some kind of interrogate the model. Yes. Yeah, not sure we should think about that Actually Hinton and one of his students just published a paper on how to approximate a neural net with a tree For this exact reason which I haven't read the paper yet. Could you pass that? So in linear regression and traditional statistics like one of the things that we focused on was Statistical significance of like the changes and things like that and so when thinking about a tree interpreter Or even like the waterfall chart, which I guess is just a visualization. Um, I Guess where does that fit in like because we can see like oh, yeah This looks important in the sense that it causes large changes But how do we know that it's like traditionally statistically significant or anything? Yeah So most of the time I don't care about the traditional statistical significance and the reason why is that Nowadays the main driver of statistical significance is data volume not kind of Practical importance and nowadays most of the models you build will have so much data that like every tiny thing will be statistically significant But most of them won't be practically significant. So my main focus therefore is practical significance Which is does the size of this influence? impact your business, you know Statistical significance only you know like it was much more important when we had a lot less data to work with If you do need to know statistical significance because for example You have a very small data set because it's like really expensive to label or hard to collect or whatever or it's a medical data Set for a rare disease You can always get statistical significance by bootstrapping Which is to say that you can randomly resample your data set a number of times Train your model a number of times and you can then see the actual variation in predictions Okay, so that's that's with bootstrapping you can Turn any model into something that gives you confidence intervals There's a paper by Michael Jordan which has a technique called the bag of little bootstraps which actually kind of Takes takes this a little bit further well worth reading if you're interested Actually pass it to Prince So you said we don't need one hot encoding matrix in If you're doing random forest or if you are doing any previous models What will happen if we do that and how bad can a model be if you do do one hot encoding? Yeah, we actually did do it remember We had that like maximum category size and we did create one hot encodings and the reason why we did it was that then our feature importance would tell us the importance of the individual levels and Our partial dependence plot we could include the individual levels so It doesn't necessarily make the model worse It may make it better, but it probably won't change it much at all in this case it hardly changed it This is something that we have noticed on real data also that if cardinality is higher Let's say 50 levels and if you do one hot encoding the random forest performs very badly and yeah That's right if the card now, that's why we have that in that's why in fast AI we have that like maximum Maximum categorical size, you know because at some point your one hot encoded variables become too sparse Right, so I generally like cut it off at six or seven Also because like when you get past that it's kind of becomes less useful because the feature importance There's going to be too many levels to really look at So can it not just Not look at those levels which are not important and just gives those significant features as important Yeah, yeah, I mean it's it's it's it'll be okay. You know it's just like Once the cardinality increases too high you're just you're just splitting your data up You know too much basically and so in practice your your Ordinal version is likely to be it's likely to be better Okay, so Yeah, so little there's no time to kind of review everything But I think that's the kind of key concepts and then of course remembering that you know the embedding Matrix that we could use is likely to have more than just one coefficient We'll actually have a dimensionality of a few coefficients which isn't going to be useful for most linear models But once you've got multi-layer models That's now creating a representation of your category which is kind of quite a lot richer, and you can do a lot more with it Let's now talk about the most important bit We started off Early in this course talking about how Actually a lot of machine learning is kind of misplaced People focus on predictive accuracy Like Amazon has a collaborative filtering algorithm for recommending books And they end up recommending the book which it thinks you're most likely to to write highly and So what they end up doing is probably recommending a book that you already have Or that you already know about and would have bought anyway, right? Which isn't very valuable what they should instead have done is to figure out like which book Can I recommend that would cause you to change your behavior? Right and so that way we actually Maximize our lift in sales due to recommendations And so this idea of like the difference between optimizing Influencing your actions versus just kind of improving predictive accuracy Improving predictive accuracy is a really important distinction which is like very rarely discussed In academia or industry kind of crazily enough. It's more discussed in industry It's particularly ignored in most of academia, right? So it's a really Important idea which is that in the end the idea the goal of your model presumably is to influence behavior Okay, and so and remember I actually mentioned a whole paper I have about this where I introduce this thing called the drivetrain approach Where I talk about like ways to think about how to incorporate machine learning Into like how do we actually influence behavior? So you know that's a starting point, but then the next question is like okay if we're trying to influence behavior What kind of behavior should we be influencing and how and what might it mean? when we start influencing behavior, right because like Nowadays like a lot of the companies that you're going to end up working at Are big ass companies and you'll be building stuff that can influence millions of people Right, so what does that mean? So I'm actually I'm not going to tell you what it means because like I don't know all I'm going to try and do is Make you aware of some of the issues Right and and make you believe two things about them first that you should care Right and second that they're big current issues, right? The main reason I want you to care is because I want you to want to be a good person and Show you that like not thinking about these things will make you a bad person But if you don't find that convincing I will tell you this Volkswagen Were found to be cheating on their emissions tests The person who was sent to jail for it was the programmer that implemented that piece of code They did exactly what they were told to do Right and so if you're coming in here thinking hey, I'm just a techie. You know I'll just do what I'm told Right, that's that's my job is to do what I'm told I'm telling you if you do that you can be sent to jail for doing what you're told Okay, so so a don't just do what you're told because you can be a bad person and B You can go to jail Okay second thing to realize is In the heat of the moment you're in a meeting with 20 people at work And you're all talking about how you're going to implement You know this new feature and everybody's discussing it and there's some part You know and everybody's like we could do this and here's a way of modeling it and then we can implement it and here's these constraints And there's some part of you that's thinking Am I sure we should be doing this? Right that's not the right time to be thinking about that because it's really hard So like step up then and say excuse me. I'm not sure This is a good idea you actually need to think about how you would handle that situation ahead of time Right so I want you to like think about About these issues now right and realize that by the time you're in the middle of it right You might not even realize it's happening You know like notice it'll just be a meeting like every other meeting and a bunch of people will be talking about how to solve This technical question okay, and you need to be able to recognize like oh This is actually something with ethical implications so Rachel actually wrote all of these slides I'm sorry she can't be here to present this because like she's studied this in depth and You know she's actually been in in in difficult environments herself where she's kind of seen these things happening you know and We know how hard it is right, but let me give you a sense of like what happens right so so Engineers trying to solve engineering problems is you know and causing problems is not a new thing right so in Nazi Germany IBM The group known as Hollerith, right Hollerith was the original name of IBM And it comes from the guy who actually invented the use of punch cards for tracking the US census the first Mass wide-scale use of punch cards for data collection in the world right and that turned into IBM And so at this point if this unit at least was still called Hollerith, so Hollerith sold a punch card system to Nazi Germany And so each punch card would like code you know this is a Jew 8 gypsy 12 General execution for death by gas chamber 6 and so here's one of these cards Describing the right way to kill these various people right and so a Swiss judge ruled That IBM's technical assistance facilitated the tasks of the Nazis and Commission of their crimes against humanity This led to the death of something like 20 million civilians So according to the Jewish Virtual Library where I got these pictures and quotes from Their view is that the destruction of the Jewish people became even less important because of the invigorating nature of IBM's technical Achievement only heightened by the fantastical profits to be made right So this was a long time ago, and you know hopefully you won't end up working at companies that facilitate genocide Right, but perhaps you will Right because perhaps you'll go to Facebook who are facilitating genocide right now Right and I know people at Facebook Who are doing this? And they had no idea they were doing this right so right now in Facebook the Rohingya In the middle of a genocide a Muslim population of Myanmar Babies are being grabbed out of their mother's arms and thrown into fires People are being killed hundreds of thousands of refugees When interviewed the Myanmar generals doing this say we are so grateful to Facebook For letting us know about the Rohingya fake news The words they use the Rohingya fake news that these people are actually not human that they're actually animals right now Facebook did not set out To enable the genocide of the Rohingya people in Myanmar No instead what happened is they wanted to maximize impressions and clicks right and so it turns out that for the data scientists at Facebook's their algorithms kind of learned that if you Take the kinds of stuff people are interested in and feed them slightly more extreme versions of that You're actually going to get a lot more impressions and the project managers are saying maximize these impressions and people are clicking and like it creates this this thing right and so the The potential implications are extraordinary and global Right and this is something that like is literally happening. You know this is October 2017 it's happening now okay, could you pass that back there? So I just want to clarify what was happening here, so it was the facilitation of like fake news or like inaccurate media Yeah, so what happened was let me go into it in more detail, so what happened was in mid 2016 Facebook fired its human editors right so it was humans that decided how to order things on your home page those people got fired and replaced with machine learning algorithms and So the machine learning algorithms written by data scientists like you You know they had nice clear metrics And they were trying to maximize their predictive accuracy and be like okay We think if we put this thing higher up than this thing we'll get more clicks Okay, and so it turned out that these algorithms for putting things on the Facebook newsfeed had a tendency to say like oh human nature is that we tend to click on things which like Stimulate our views and therefore like more extreme versions of things we already see okay, so So this is great for the kind of Facebook revenue model of maximizing engagement It looked good on all of their KPIs And so at the time You know there was some negative press About like you know I'm not sure that the stuff that Facebook's now putting on their trending Section is actually that accurate but from the point of view of the metrics that people are optimizing at Facebook it looked terrific right and so way back to October 2016 People started noticing some serious problems for example it is illegal to target housing to people of certain races in America that is illegal and yet a News organization discovered that Facebook was doing exactly that Right in October 2016 again not because somebody in that data science team said like let's make sure black people can't live in nice Neighborhoods right but instead you know they found that their automatic clustering and segmentation algorithm found there was a cluster of people who Didn't like African Americans and that if you targeted them with these kinds of ads then they would be more likely to Select this kind of housing or whatever right but the interesting thing is that even after being told about this three times Facebook still hasn't fixed it right and that is to say these are not just technical issues They're also economic issues right when you start saying like the thing that you get paid for that is ads you have to change The way that you structure those so that you know you either Use more people that cost money or you like a less aggressive on your algorithms to target people you know based on like Minority group status or whatever You know that can impact revenues and so the reason I mentioned this is you will likely at some point in your career find yourself in a conversation where you're thinking I'm not confident that this is like morally okay The person you're talking to is thinking in their head. This is going to make us a lot of money That and you just you don't quite ever Manage to have a successful conversation because you're talking about different things You know and so when you're talking to somebody who may be more experienced and more senior than you and they may sound like they know What they're talking about right just realize that their incentives are not necessarily Going to be focused on like How do I be a good person? You know like they're not thinking how do I be a bad person? But you know the more time you spend an industry in my experience the more desensitized you kind of get To this stuff of like okay, maybe getting promotions and making money isn't the most important thing All right, so for example I've got a lot of friends who are very good at computer vision and some of them have gone on to create startups That seem like they're almost handmade to help authoritarian governments surveil their You know their citizens and when I ask my friends like have you thought about? How this could be used in that way? You know they're generally kind of offended that I ask you know But I'm asking you To think about this like you know wherever you end up working if you end up creating a startup like Tools can be used for good or for evil right and so I'm not saying like don't create excellent object act tracking and Detection tools from computer vision because yeah, you could go on and use that to create like a much better Surgical intervention robot toolkit right I'm just saying like be aware of it think about it talk about it. You know So here's one I find like Fascinating and there's this really cool thing actually that made up comm did this is from a made up comm talk. That's online Um they they think about this they actually thought about this they actually thought you know what if We built a collaborative filtering system like we learned about in class To help people decide what meetup to go to it might notice that on the whole in San Francisco a few more men than women tend to go to techie meetups and So it might then start to decide to recommend techie meetups to more men than women as a result of which more men will go to techie meetups as A result of which when women go to techie meetups. They'll be like oh, this is all men I don't really want to go to techie meetups as a result of which the algorithm will get new data saying that men like Techie meetups better right and so it continues right and so like a little a Little bit of kind of that initial push from the algorithm can create this runaway Feedback loop and you end up with like almost all my old techie meetups for instance right and so this kind of Feedback loop is a kind of subtle issue that you really want to think about when you're thinking about like what is the behavior that? I'm changing With this algorithm that I'm building So Another example which is kind of terrifying is in this paper where the Authors describe how a lot of departments in the US are now using predictive policing algorithms right so Where can we go to find somebody who's about to commit a crime? And so you know that the algorithm Simply feeds back to you basically the data that you've given it right so if your police department Has engaged in racial profiling at all in the past then it might suggest slightly more often Maybe you should go to the black neighborhoods to check for people committing crimes All right as a result of which more of your police officers go to the black neighborhoods as a result of which they arrest more black People as a result of which the data says that the black neighborhoods are less safe as a result of which the algorithm says The policeman maybe you should go to the black neighborhoods more often and so forth right and this is not like You know Vague possibilities of something that might happen in the future. This is like documented work from top academics Who have carefully studied the data and the theory right? This is like serious scholarly work It's like no this is this is happening right now And so you know again like I'm sure the people that started creating this predictive policing algorithm Didn't think like how do we arrest more black people right? You know hopefully they were actually thinking gosh. I'd like my children to be safer on the streets How do I create you know a safer? Society right, but they didn't think about this this nasty runaway feedback loop so actually this this one about social network algorithms is actually a Article in the New York Times recently that one of my friends Renae de Resta and she did something That was kind of amazing. She set up a second Facebook account right like a fake Facebook account and She was very interested in the anti-vax movement at the time so she started following a couple of anti-vaxxers and visited a couple of anti-vaxxer links and so suddenly her newsfeed starts getting full of anti-vaxxer news along with other stuff like chemtrails and deep state conspiracy theories and all this stuff and so she's like Starts clicking on those right and the more she clicked the more Hardcore far-out conspiracy stuff Facebook recommended so now when Renee goes to that Facebook account the whole thing is just full of Angry crazy Far-out conspiracy stuff like that's all she sees and so if that was your world Right then as far as you're concerned. It's just like this continuous reminder and proof of Of all this stuff right and so again. It's like this this is to answer your question. This is the kind of Runaway feedback loop that ends up telling me and my generals you know throughout their Facebook homepage that Rohingya animals and fake news and whatever else right So you know it's it's a lot of this comes from Also from bias right and so like let's talk about bias specifically so bias in image software comes from bias in data and so most of the folks I know at Google Brain building computer vision algorithms very few of them are people of color and So when they're training the algorithms with you know photos of their families and friends They are training them with very few people of color and so when FaceApp then decided we're going to try looking at lots of Instagram photos To see which ones are like no up voted the most Without them necessarily realizing it the answer was like you know light colored faces So then they built a generative model to make you more hot and so this is the actual photo And here is the hotter version Right so the hotter version is like more white less nostrils You know more European looking right and so like This did not go down well to say the least so like the so again, you know I don't think anybody at FaceApp said like Let's create something that makes people look more white right they just trained it on a bunch of images of the people that they had around them okay, and This has kind of cut. You know serious commercial Implications as well they had to pull this feature right and they had a huge amount of negative pushback Pushback like as they should right here's another example Google photos created this photo Classifier airplanes skyscrapers cars graduation and no gorillas Right so like think about how this looks to like most people like most to most people they look at this They don't know about machine learning they say What the fuck somebody at Google wrote some code? To take black people and call them gorillas like that's what it looks like Right now we know that's not what happened right we know what happened is you know the team you know of of folks at Google computer vision experts who have none if or few people of color working in the team Built a classifier using all the photos they had available to them and so when the system came along came across You know a person with dark skin it was like oh I've only mainly seen that before amongst gorillas, so I'll put it in that category Right so again. It's the bias in the data creates a bias in the software and again the commercial implications were very significant Like Google really got a lot of bad PR from this as they should this this was a photo that some you know somebody put in their Twitter feed They said like look what look what Google photos just decided to do You can imagine what happened with the first international beauty contest judged by artificial intelligence Right basically it turns out all the beautiful people are white again, right so like you kind of see this bias in image software thanks to bias in the data thanks to By lack of diversity and the teams building it you see the same thing in in natural language processing right so here is Turkish oh is the The pronoun in Turkish which has no Gender right there is no he or versus she right okay. No no he versus she But of course in English. We don't really have a widely used Ungendered singular pronouns so Google Translate converts it to this Okay now There are plenty of people who saw this online and said like Literally, so what you know it is correctly feeding back the usual usage in English like this is you know It's it. I know how this is trained. This is like word to beck vectors I was trained on Google News Corpus Google Books Corpus. It's just telling us how things are and And like from a point of view that's entirely true right like the biased data to create this biased algorithm Is the actual data of how people have written books and newspaper articles for decades? But does that mean that this is The product that you want to create You know does this mean this is the product you have to create? Right just because the particular way you've trained the model means it ends up doing this you know Is this actually the design you want and can you think of? potential negative Implications and feedback loops this could create right and you know if any of these things bother you Then now if lucky you you have a new cool engineering problem to work on like how do I create? Unbiased NLP solutions and now there are some startups starting to do that and starting to make some money right so like opportunity You know these are opportunities for you. It's like hey. Here's some stuff where people are creating Screwed up societal outcomes because of their shitty models like okay, well you can go and build something better right so like another example of the bias in word to beck word vectors is restaurant reviews rank Mexican restaurants worse Because Mexican the Mexican words tend to be associated with criminal words in the US press and books more often Again, this is like a real problem that is happening right now So you know Rachel actually did some interesting analysis of just the plain word to beck word vectors Where she basically pulled them out and you know looked at these analogies based on some research that had been done elsewhere, and so you can see like word to beck like the The vector directions show that father is to doctor is the mother is to nurse Man is to computer programmer as woman is to homemaker And so forth right so like it's really easy to see What's in these word vectors, and you know they're kind of fundamental to Much of the NLP or probably just about all the NLP software we use today So like here's a great example So ProPublica has actually done a lot of good work in this area Judges many judges now have access to sentencing guidelines software and So sentencing guidelines software says to the judge For this individual we would recommend this kind of sentence Right and now of course a judge Doesn't understand machine learning so like they have two choices Which is either do what it says or ignore it entirely right and some people fall into each category right and So for the ones that fall into the like do what it says category. Here's what happens For those that were labeled higher risk right the subset of those that label high risk It actually turned out not to reoffend Right was about a quarter of whites and About a half of African Americans right so like nearly twice as often right People who didn't reoffend were marked as higher risk if they were African American and vice versa amongst those that are labeled lower risk but Actually did reoffend Turned out to be about half of the whites and only 28% of the African Americans like so like this is data Which I I would like to think nobody is setting out to create something that does this right, but when you start with bias data right and you know the data says that Whites and blacks smoke marijuana at about the same rate, but blacks are Jailed at I think it's something like five times more often than whites like you know the nature of the justice system in America At least at the moment is That it's not it's not equal. It's not fair and therefore the data That's fed into the machine learning model It's going to basically support that status quo and then because of the negative feedback loop It's just going to get worse and worse right I'll tell you something else interesting about this one which Research called a bong has pointed out is here are some of the questions that are being asked right so let's let's take one Was your father ever arrested Right so your answer to that question is going to decide whether you're locked up and for how long? Now as a machine learning researcher, do you think that might improve the predictive accuracy of your algorithm and get you a better? R-squared It could well, but I don't know you know maybe it does we try it out. Oh, I got a better r-squared So does that mean you should use it like well? There's another question like do you think it's reasonable to lock somebody up for longer because of who their dad was? But and yet these are actually the examples of questions that we are asking right now To offenders and then putting into a machine learning system to decide what happens to them, okay? So again like whoever designed this Presumably they were like laser focused on technical excellence getting the maximum area under the ROC curve And I found these great predictors that give me another point oh two Right and I guess didn't stop to think like well is that a reasonable way? To decide who goes to jail for longer So like putting this together you can kind of see how this can Get you know more and more scary We take a company like taser right and tasers are these Devices that kind of give you a big electric shock basically and tasers managed to do a great job of creating strong relationships with some academic researchers who seem to say whatever they Tell them to say to the extent where now if you look at the data It turns out that there's a much higher problem. You know there's a pretty high probability that if you get tased That you will die It happens you know Not unusually And yet you know the researchers who they've paid to look into this have consistently come back and said oh no It was nothing to do with the taser the fact that they died immediately afterwards was totally Unrelated it was just a random you know things things happen So this company now owns 80% of the market for body cameras And they started buying computer vision AI companies And they're going to try and now use these police body camera videos to anticipate criminal activity right and so like What does that mean right so is that like okay? I now have some augmented reality display saying like Tase this person Because they're about to do something bad you know so it's like it's kind of like a worrying direction and so You know I'm sure nobody who's a data scientist at taser or at the companies that they bought out is thinking like You know this is the world I want to help create But they could find themselves in you know or you could find yourself in the middle of this kind of discussion Where it's not explicitly about that topic, but there's part of you that says like Now wonder if this is how this could be used All right, and and you know I don't know exactly what the right thing to do in that situation is because like you can ask And of course people are going to be like no no no no So it's like you know are you gonna you know what what could you do? No, you could like ask for some kind of written promise you could decide to leave You could you know start doing some research into the legality of things to say like oh, I would at least protect my own You know legal situation. I don't know like have a think about how you would respond to that So these are some questions that Rachel created as being things to think about right So if you're looking at building a data product or you know using a model like if you're building a machine learning model It's for a reason Okay, you're trying to do something right so what bias may be in that data All right because whatever bias is in that data ends up being a bias in your predictions Potentially then biases the actions that you're influencing Potentially then biases the data that you come back and you may create a feedback loop if the team that build it isn't diverse You know what might you be missing right so for example? One senior executive at Twitter called the alarm about major major Russian bot problems at Twitter Way back well before the election That was the one Black person in the exec team at Twitter the one and Shortly afterwards they lost their job right and so like Definitely having a more diverse team that means having a more diverse set of opinions and beliefs and ideas and things to look for and so forth so Non-diverse teams seem to make more of these bad mistakes Can we order the code is it open source check for the different error rates amongst different groups? Is there like a simple rule we could use instead? That's like extremely interpretable and easy to communicate And like you know if something goes wrong do we have a good way to deal with it? right so when When we've talked to people about this and a lot of people like have come to Rachel and said like I'm I'm concerned about something my organization's doing you know what do I do? Or I'm just concerned about my toxic workplace. What do I do? and very often You know Rachel will say like well have you considered leaving and they will say oh I I don't want to lose my job All right, but actually if you can code you're in like point three percent of the population if you can code and do machine learning You're in probably like point oh one percent of the population you are massively massively in demand So like realistically you know obviously it's an organization does not want you to feel like you're somebody who could just leave and get another Job that's not in your interest in their interest, but that is absolutely true right and so one of the things I hope you leave this course with is is enough self-confidence To recognize that you have the skills you know to get to get a job and particularly Once you've got your first job your second job is an order of magnitude easier right and so you know this is important Not just so that you feel like you actually have the ability to act ethically But it's also important to realize like if you find yourself in a toxic environment right which is which is pretty damn common Unfortunately like there's a lot of shitty Tech cultures environments particularly in the Bay Area, but if you find yourself in one of those environments The best thing to do is to get the hell out right and and if you don't Have the self-confidence to think you can get another job You can get trapped right, so you know It's really important really important to know that you are leaving this program with varying demand skills and particularly after you have that first job you're now somebody with in-demand skills and Attract record of being employed in that area Okay, great so Yes This is kind of just a broad question, but what are some things that you know of that people are doing to treat bias in data? You know it's kind of like a bit of a controversial subject at the moment and There are there are like people are trying to use some people trying to use an algorithmic approach You know where they're basically trying to say How can we identify the bias and kind of like subtract it out? But like the most effective ways I know of a one sort of trying to treat it at the data level so like start with a More diverse team particularly a team involved you know which includes people from the humanities like sociologists psychologists economists people that understand feedback loops and implications for human behavior, and they tend to be equipped with You know good tools for kind of identifying and tracking these kinds of problems And so and then kind of trying to incorporate the solutions into the process itself Let's say there isn't kind of like a you know Some standard process I can point you to and say here's how to solve it You know if there is such a thing we haven't found it yet. You know it requires a Diverse team of smart people to be aware of the problems and work hard at them. It's the short answer Can you pass that back please? This is just kind of a general thing I guess for the whole class If you're interested in this stuff that I read a pretty cool book Jeremy you've probably heard of it weapons of math destruction by Cathy O'Neill Covers a lot of the same stuff. Yeah, just more more on the topic Yeah, thanks the recommendation so Cathy's great. She's also got a TED talk I Didn't manage to finish the book because it's so damn depressing. I was just like yeah no more But yeah, it's it's it's it's it's very good All right Well, that's it Thank you everybody. You know this has been This has been really intense for me You know obviously this was meant to be something that I was sharing with Rachel So I've you know ended up doing one of the hardest things in my life Which is to teach two people's worth of course on my own and also look after a sick wife and have a toddler and also Do a deep learning course and also do all this with a new library that I just wrote So I'm looking forward to getting some sleep, but it's been it's been totally worth it because you've been Amazing like I'm thrilled with how you've you know reacted to the kind of You know the opportunities I've given you and also to the feedback that I've given you So congratulations", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.0, "text": " I thought what we might do today is to like", "tokens": [286, 1194, 437, 321, 1062, 360, 965, 307, 281, 411], "temperature": 0.0, "avg_logprob": -0.3345363370833858, "compression_ratio": 1.4825581395348837, "no_speech_prob": 0.009856651537120342}, {"id": 1, "seek": 0, "start": 6.88, "end": 11.24, "text": " Finish off where we were in this Rossman notebook looking at", "tokens": [31583, 766, 689, 321, 645, 294, 341, 16140, 1601, 21060, 1237, 412], "temperature": 0.0, "avg_logprob": -0.3345363370833858, "compression_ratio": 1.4825581395348837, "no_speech_prob": 0.009856651537120342}, {"id": 2, "seek": 0, "start": 12.06, "end": 15.200000000000001, "text": " time series forecasting and structured data analysis", "tokens": [565, 2638, 44331, 293, 18519, 1412, 5215], "temperature": 0.0, "avg_logprob": -0.3345363370833858, "compression_ratio": 1.4825581395348837, "no_speech_prob": 0.009856651537120342}, {"id": 3, "seek": 0, "start": 15.76, "end": 18.14, "text": " And then we might do a little", "tokens": [400, 550, 321, 1062, 360, 257, 707], "temperature": 0.0, "avg_logprob": -0.3345363370833858, "compression_ratio": 1.4825581395348837, "no_speech_prob": 0.009856651537120342}, {"id": 4, "seek": 0, "start": 19.32, "end": 21.16, "text": " mini review", "tokens": [8382, 3131], "temperature": 0.0, "avg_logprob": -0.3345363370833858, "compression_ratio": 1.4825581395348837, "no_speech_prob": 0.009856651537120342}, {"id": 5, "seek": 0, "start": 21.16, "end": 24.12, "text": " Like everything we've learned because believe it or not", "tokens": [1743, 1203, 321, 600, 3264, 570, 1697, 309, 420, 406], "temperature": 0.0, "avg_logprob": -0.3345363370833858, "compression_ratio": 1.4825581395348837, "no_speech_prob": 0.009856651537120342}, {"id": 6, "seek": 2412, "start": 24.12, "end": 30.720000000000002, "text": " This is the end like there's nothing more to know about machine learning other than everything that you're going to learn", "tokens": [639, 307, 264, 917, 411, 456, 311, 1825, 544, 281, 458, 466, 3479, 2539, 661, 813, 1203, 300, 291, 434, 516, 281, 1466], "temperature": 0.0, "avg_logprob": -0.16014079139346168, "compression_ratio": 1.7755102040816326, "no_speech_prob": 1.0129097063327208e-05}, {"id": 7, "seek": 2412, "start": 31.400000000000002, "end": 33.4, "text": " next semester and", "tokens": [958, 11894, 293], "temperature": 0.0, "avg_logprob": -0.16014079139346168, "compression_ratio": 1.7755102040816326, "no_speech_prob": 1.0129097063327208e-05}, {"id": 8, "seek": 2412, "start": 33.96, "end": 35.96, "text": " For the rest of your life", "tokens": [1171, 264, 1472, 295, 428, 993], "temperature": 0.0, "avg_logprob": -0.16014079139346168, "compression_ratio": 1.7755102040816326, "no_speech_prob": 1.0129097063327208e-05}, {"id": 9, "seek": 2412, "start": 36.0, "end": 38.0, "text": " But anyway, I got nothing else to teach", "tokens": [583, 4033, 11, 286, 658, 1825, 1646, 281, 2924], "temperature": 0.0, "avg_logprob": -0.16014079139346168, "compression_ratio": 1.7755102040816326, "no_speech_prob": 1.0129097063327208e-05}, {"id": 10, "seek": 2412, "start": 40.2, "end": 46.040000000000006, "text": " Yes, I'll do a little review and then we'll cover like the most important part of the course which is like", "tokens": [1079, 11, 286, 603, 360, 257, 707, 3131, 293, 550, 321, 603, 2060, 411, 264, 881, 1021, 644, 295, 264, 1164, 597, 307, 411], "temperature": 0.0, "avg_logprob": -0.16014079139346168, "compression_ratio": 1.7755102040816326, "no_speech_prob": 1.0129097063327208e-05}, {"id": 11, "seek": 2412, "start": 46.64, "end": 52.120000000000005, "text": " Thinking about like what are the what are the what are the how are ways to think about how to use this kind of technology?", "tokens": [24460, 466, 411, 437, 366, 264, 437, 366, 264, 437, 366, 264, 577, 366, 2098, 281, 519, 466, 577, 281, 764, 341, 733, 295, 2899, 30], "temperature": 0.0, "avg_logprob": -0.16014079139346168, "compression_ratio": 1.7755102040816326, "no_speech_prob": 1.0129097063327208e-05}, {"id": 12, "seek": 5212, "start": 52.12, "end": 54.12, "text": " appropriately", "tokens": [23505], "temperature": 0.0, "avg_logprob": -0.21468529520155508, "compression_ratio": 1.537037037037037, "no_speech_prob": 3.120039400528185e-05}, {"id": 13, "seek": 5212, "start": 54.519999999999996, "end": 59.36, "text": " And you know effectively in a way that's a positive hopefully a positive impact on society", "tokens": [400, 291, 458, 8659, 294, 257, 636, 300, 311, 257, 3353, 4696, 257, 3353, 2712, 322, 4086], "temperature": 0.0, "avg_logprob": -0.21468529520155508, "compression_ratio": 1.537037037037037, "no_speech_prob": 3.120039400528185e-05}, {"id": 14, "seek": 5212, "start": 61.72, "end": 65.46, "text": " So last time we got to the point where we talked a bit about", "tokens": [407, 1036, 565, 321, 658, 281, 264, 935, 689, 321, 2825, 257, 857, 466], "temperature": 0.0, "avg_logprob": -0.21468529520155508, "compression_ratio": 1.537037037037037, "no_speech_prob": 3.120039400528185e-05}, {"id": 15, "seek": 5212, "start": 66.67999999999999, "end": 72.16, "text": " This this idea that when we were looking at like building this competition months open", "tokens": [639, 341, 1558, 300, 562, 321, 645, 1237, 412, 411, 2390, 341, 6211, 2493, 1269], "temperature": 0.0, "avg_logprob": -0.21468529520155508, "compression_ratio": 1.537037037037037, "no_speech_prob": 3.120039400528185e-05}, {"id": 16, "seek": 5212, "start": 72.72, "end": 74.56, "text": " derived variable", "tokens": [18949, 7006], "temperature": 0.0, "avg_logprob": -0.21468529520155508, "compression_ratio": 1.537037037037037, "no_speech_prob": 3.120039400528185e-05}, {"id": 17, "seek": 5212, "start": 74.56, "end": 77.8, "text": " But we actually truncated it down to be no more than 24 months", "tokens": [583, 321, 767, 504, 409, 66, 770, 309, 760, 281, 312, 572, 544, 813, 4022, 2493], "temperature": 0.0, "avg_logprob": -0.21468529520155508, "compression_ratio": 1.537037037037037, "no_speech_prob": 3.120039400528185e-05}, {"id": 18, "seek": 7780, "start": 77.8, "end": 85.44, "text": " And we talked about the reason why being that we actually wanted to use it as a categorical variable because categorical variables thanks to embeddings", "tokens": [400, 321, 2825, 466, 264, 1778, 983, 885, 300, 321, 767, 1415, 281, 764, 309, 382, 257, 19250, 804, 7006, 570, 19250, 804, 9102, 3231, 281, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.17482421564501385, "compression_ratio": 1.627906976744186, "no_speech_prob": 2.7264484288025415e-06}, {"id": 19, "seek": 7780, "start": 85.64, "end": 87.64, "text": " have more", "tokens": [362, 544], "temperature": 0.0, "avg_logprob": -0.17482421564501385, "compression_ratio": 1.627906976744186, "no_speech_prob": 2.7264484288025415e-06}, {"id": 20, "seek": 7780, "start": 87.92, "end": 90.67999999999999, "text": " Flexibility and how the neural net can can use them", "tokens": [29208, 2841, 293, 577, 264, 18161, 2533, 393, 393, 764, 552], "temperature": 0.0, "avg_logprob": -0.17482421564501385, "compression_ratio": 1.627906976744186, "no_speech_prob": 2.7264484288025415e-06}, {"id": 21, "seek": 7780, "start": 91.72, "end": 94.96, "text": " And so that was kind of where we where we left off", "tokens": [400, 370, 300, 390, 733, 295, 689, 321, 689, 321, 1411, 766], "temperature": 0.0, "avg_logprob": -0.17482421564501385, "compression_ratio": 1.627906976744186, "no_speech_prob": 2.7264484288025415e-06}, {"id": 22, "seek": 7780, "start": 97.32, "end": 99.32, "text": " So let's like keep working through this", "tokens": [407, 718, 311, 411, 1066, 1364, 807, 341], "temperature": 0.0, "avg_logprob": -0.17482421564501385, "compression_ratio": 1.627906976744186, "no_speech_prob": 2.7264484288025415e-06}, {"id": 23, "seek": 7780, "start": 100.47999999999999, "end": 101.72, "text": " because", "tokens": [570], "temperature": 0.0, "avg_logprob": -0.17482421564501385, "compression_ratio": 1.627906976744186, "no_speech_prob": 2.7264484288025415e-06}, {"id": 24, "seek": 7780, "start": 101.72, "end": 104.2, "text": " What's happening in this notebook is?", "tokens": [708, 311, 2737, 294, 341, 21060, 307, 30], "temperature": 0.0, "avg_logprob": -0.17482421564501385, "compression_ratio": 1.627906976744186, "no_speech_prob": 2.7264484288025415e-06}, {"id": 25, "seek": 10420, "start": 104.2, "end": 110.76, "text": " Stuff which is probably going to apply to most time series data sets that you work with", "tokens": [31347, 597, 307, 1391, 516, 281, 3079, 281, 881, 565, 2638, 1412, 6352, 300, 291, 589, 365], "temperature": 0.0, "avg_logprob": -0.18345110039961965, "compression_ratio": 1.5614754098360655, "no_speech_prob": 2.561260316724656e-06}, {"id": 26, "seek": 10420, "start": 113.32000000000001, "end": 117.56, "text": " And as we talked about like although we use DF dot apply here", "tokens": [400, 382, 321, 2825, 466, 411, 4878, 321, 764, 48336, 5893, 3079, 510], "temperature": 0.0, "avg_logprob": -0.18345110039961965, "compression_ratio": 1.5614754098360655, "no_speech_prob": 2.561260316724656e-06}, {"id": 27, "seek": 10420, "start": 117.56, "end": 121.60000000000001, "text": " This is something where it's running a piece of Python code over every row", "tokens": [639, 307, 746, 689, 309, 311, 2614, 257, 2522, 295, 15329, 3089, 670, 633, 5386], "temperature": 0.0, "avg_logprob": -0.18345110039961965, "compression_ratio": 1.5614754098360655, "no_speech_prob": 2.561260316724656e-06}, {"id": 28, "seek": 10420, "start": 122.32000000000001, "end": 128.92000000000002, "text": " and it's that's horrifically slow right so we only do that if we can't find a", "tokens": [293, 309, 311, 300, 311, 17582, 4278, 2964, 558, 370, 321, 787, 360, 300, 498, 321, 393, 380, 915, 257], "temperature": 0.0, "avg_logprob": -0.18345110039961965, "compression_ratio": 1.5614754098360655, "no_speech_prob": 2.561260316724656e-06}, {"id": 29, "seek": 12892, "start": 128.92, "end": 133.6, "text": " Vectorized pandas or numpy function that can do it to the whole column at once", "tokens": [691, 20814, 1602, 4565, 296, 420, 1031, 8200, 2445, 300, 393, 360, 309, 281, 264, 1379, 7738, 412, 1564], "temperature": 0.0, "avg_logprob": -0.28988911555363583, "compression_ratio": 1.4619883040935673, "no_speech_prob": 1.2289060578041244e-06}, {"id": 30, "seek": 12892, "start": 134.67999999999998, "end": 140.22, "text": " But in this case I couldn't find a way to convert a year and a week number", "tokens": [583, 294, 341, 1389, 286, 2809, 380, 915, 257, 636, 281, 7620, 257, 1064, 293, 257, 1243, 1230], "temperature": 0.0, "avg_logprob": -0.28988911555363583, "compression_ratio": 1.4619883040935673, "no_speech_prob": 1.2289060578041244e-06}, {"id": 31, "seek": 12892, "start": 140.92, "end": 143.16, "text": " into a date without using", "tokens": [666, 257, 4002, 1553, 1228], "temperature": 0.0, "avg_logprob": -0.28988911555363583, "compression_ratio": 1.4619883040935673, "no_speech_prob": 1.2289060578041244e-06}, {"id": 32, "seek": 12892, "start": 144.23999999999998, "end": 146.23999999999998, "text": " arbitrary Python", "tokens": [23211, 15329], "temperature": 0.0, "avg_logprob": -0.28988911555363583, "compression_ratio": 1.4619883040935673, "no_speech_prob": 1.2289060578041244e-06}, {"id": 33, "seek": 12892, "start": 148.76, "end": 152.76, "text": " Also worth remembering this idea of a lambda function", "tokens": [2743, 3163, 20719, 341, 1558, 295, 257, 13607, 2445], "temperature": 0.0, "avg_logprob": -0.28988911555363583, "compression_ratio": 1.4619883040935673, "no_speech_prob": 1.2289060578041244e-06}, {"id": 34, "seek": 15276, "start": 152.76, "end": 160.56, "text": " Any time you're trying to apply a function to every row of something or every element of a tensor or something like that if there isn't", "tokens": [2639, 565, 291, 434, 1382, 281, 3079, 257, 2445, 281, 633, 5386, 295, 746, 420, 633, 4478, 295, 257, 40863, 420, 746, 411, 300, 498, 456, 1943, 380], "temperature": 0.0, "avg_logprob": -0.21674613952636718, "compression_ratio": 1.7894736842105263, "no_speech_prob": 3.307571603272663e-07}, {"id": 35, "seek": 15276, "start": 160.56, "end": 164.7, "text": " a vectorized version already you're going to have to call something like", "tokens": [257, 8062, 1602, 3037, 1217, 291, 434, 516, 281, 362, 281, 818, 746, 411], "temperature": 0.0, "avg_logprob": -0.21674613952636718, "compression_ratio": 1.7894736842105263, "no_speech_prob": 3.307571603272663e-07}, {"id": 36, "seek": 15276, "start": 165.32, "end": 170.56, "text": " Data frame dot apply which will run a function you pass to every element", "tokens": [11888, 3920, 5893, 3079, 597, 486, 1190, 257, 2445, 291, 1320, 281, 633, 4478], "temperature": 0.0, "avg_logprob": -0.21674613952636718, "compression_ratio": 1.7894736842105263, "no_speech_prob": 3.307571603272663e-07}, {"id": 37, "seek": 15276, "start": 171.64, "end": 173.95999999999998, "text": " So this is something like you know kind of", "tokens": [407, 341, 307, 746, 411, 291, 458, 733, 295], "temperature": 0.0, "avg_logprob": -0.21674613952636718, "compression_ratio": 1.7894736842105263, "no_speech_prob": 3.307571603272663e-07}, {"id": 38, "seek": 15276, "start": 174.95999999999998, "end": 177.72, "text": " This is basically a map in functional programming", "tokens": [639, 307, 1936, 257, 4471, 294, 11745, 9410], "temperature": 0.0, "avg_logprob": -0.21674613952636718, "compression_ratio": 1.7894736842105263, "no_speech_prob": 3.307571603272663e-07}, {"id": 39, "seek": 17772, "start": 177.72, "end": 185.08, "text": " Since very often the function that you want to pass to it is something you're just going to use once and then throw it away", "tokens": [4162, 588, 2049, 264, 2445, 300, 291, 528, 281, 1320, 281, 309, 307, 746, 291, 434, 445, 516, 281, 764, 1564, 293, 550, 3507, 309, 1314], "temperature": 0.0, "avg_logprob": -0.15811059716042508, "compression_ratio": 1.6355555555555557, "no_speech_prob": 9.570800330038765e-07}, {"id": 40, "seek": 17772, "start": 185.56, "end": 187.56, "text": " It's really common to use this", "tokens": [467, 311, 534, 2689, 281, 764, 341], "temperature": 0.0, "avg_logprob": -0.15811059716042508, "compression_ratio": 1.6355555555555557, "no_speech_prob": 9.570800330038765e-07}, {"id": 41, "seek": 17772, "start": 188.56, "end": 191.88, "text": " Lambda approach so this lambda is can creating a function", "tokens": [45691, 3109, 370, 341, 13607, 307, 393, 4084, 257, 2445], "temperature": 0.0, "avg_logprob": -0.15811059716042508, "compression_ratio": 1.6355555555555557, "no_speech_prob": 9.570800330038765e-07}, {"id": 42, "seek": 17772, "start": 192.44, "end": 197.44, "text": " Just for the purpose of telling DF dot apply what to use right so we could", "tokens": [1449, 337, 264, 4334, 295, 3585, 48336, 5893, 3079, 437, 281, 764, 558, 370, 321, 727], "temperature": 0.0, "avg_logprob": -0.15811059716042508, "compression_ratio": 1.6355555555555557, "no_speech_prob": 9.570800330038765e-07}, {"id": 43, "seek": 17772, "start": 198.32, "end": 200.72, "text": " We could also have written this in a different way", "tokens": [492, 727, 611, 362, 3720, 341, 294, 257, 819, 636], "temperature": 0.0, "avg_logprob": -0.15811059716042508, "compression_ratio": 1.6355555555555557, "no_speech_prob": 9.570800330038765e-07}, {"id": 44, "seek": 20072, "start": 200.72, "end": 208.24, "text": " Which would have been to say define", "tokens": [3013, 576, 362, 668, 281, 584, 6964], "temperature": 0.0, "avg_logprob": -0.52958083152771, "compression_ratio": 1.0394736842105263, "no_speech_prob": 3.9669494071858935e-06}, {"id": 45, "seek": 20072, "start": 212.36, "end": 214.36, "text": " Create", "tokens": [20248], "temperature": 0.0, "avg_logprob": -0.52958083152771, "compression_ratio": 1.0394736842105263, "no_speech_prob": 3.9669494071858935e-06}, {"id": 46, "seek": 20072, "start": 214.72, "end": 216.48, "text": " Chromo", "tokens": [1721, 13395], "temperature": 0.0, "avg_logprob": -0.52958083152771, "compression_ratio": 1.0394736842105263, "no_speech_prob": 3.9669494071858935e-06}, {"id": 47, "seek": 20072, "start": 216.48, "end": 220.8, "text": " To sense on some value", "tokens": [1407, 2020, 322, 512, 2158], "temperature": 0.0, "avg_logprob": -0.52958083152771, "compression_ratio": 1.0394736842105263, "no_speech_prob": 3.9669494071858935e-06}, {"id": 48, "seek": 20072, "start": 222.8, "end": 224.8, "text": " Return", "tokens": [24350], "temperature": 0.0, "avg_logprob": -0.52958083152771, "compression_ratio": 1.0394736842105263, "no_speech_prob": 3.9669494071858935e-06}, {"id": 49, "seek": 22480, "start": 224.8, "end": 231.72, "text": " And then we could put that in here", "tokens": [400, 550, 321, 727, 829, 300, 294, 510], "temperature": 0.0, "avg_logprob": -0.2282304565111796, "compression_ratio": 1.1014492753623188, "no_speech_prob": 1.844803364292602e-06}, {"id": 50, "seek": 22480, "start": 245.20000000000002, "end": 248.04000000000002, "text": " Okay, so that and that are the same thing", "tokens": [1033, 11, 370, 300, 293, 300, 366, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.2282304565111796, "compression_ratio": 1.1014492753623188, "no_speech_prob": 1.844803364292602e-06}, {"id": 51, "seek": 24804, "start": 248.04, "end": 255.64, "text": " Okay, so one approach is to define the function and then pass it by name or the other is to define the function in place", "tokens": [1033, 11, 370, 472, 3109, 307, 281, 6964, 264, 2445, 293, 550, 1320, 309, 538, 1315, 420, 264, 661, 307, 281, 6964, 264, 2445, 294, 1081], "temperature": 0.0, "avg_logprob": -0.19411275675008585, "compression_ratio": 1.6727272727272726, "no_speech_prob": 2.6841817089007236e-06}, {"id": 52, "seek": 24804, "start": 256.44, "end": 260.0, "text": " Using lambda all right, and so if you're not", "tokens": [11142, 13607, 439, 558, 11, 293, 370, 498, 291, 434, 406], "temperature": 0.0, "avg_logprob": -0.19411275675008585, "compression_ratio": 1.6727272727272726, "no_speech_prob": 2.6841817089007236e-06}, {"id": 53, "seek": 24804, "start": 261.24, "end": 267.8, "text": " Comfortable creating and using lambdas you know good thing to practice and playing around with DF dot apply is a good way to", "tokens": [2432, 10124, 4084, 293, 1228, 10097, 27476, 291, 458, 665, 551, 281, 3124, 293, 2433, 926, 365, 48336, 5893, 3079, 307, 257, 665, 636, 281], "temperature": 0.0, "avg_logprob": -0.19411275675008585, "compression_ratio": 1.6727272727272726, "no_speech_prob": 2.6841817089007236e-06}, {"id": 54, "seek": 24804, "start": 267.92, "end": 269.92, "text": " Good way to practice it", "tokens": [2205, 636, 281, 3124, 309], "temperature": 0.0, "avg_logprob": -0.19411275675008585, "compression_ratio": 1.6727272727272726, "no_speech_prob": 2.6841817089007236e-06}, {"id": 55, "seek": 24804, "start": 271.64, "end": 272.8, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.19411275675008585, "compression_ratio": 1.6727272727272726, "no_speech_prob": 2.6841817089007236e-06}, {"id": 56, "seek": 27280, "start": 272.8, "end": 281.36, "text": " So let's talk about this durations section which may at first seem a", "tokens": [407, 718, 311, 751, 466, 341, 4861, 763, 3541, 597, 815, 412, 700, 1643, 257], "temperature": 0.0, "avg_logprob": -0.20130926853901632, "compression_ratio": 1.5875706214689265, "no_speech_prob": 5.014697308070026e-06}, {"id": 57, "seek": 27280, "start": 282.56, "end": 288.0, "text": " Little specific, but actually it turns out not to be what we're going to do is we're going to look at", "tokens": [8022, 2685, 11, 457, 767, 309, 4523, 484, 406, 281, 312, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 574, 412], "temperature": 0.0, "avg_logprob": -0.20130926853901632, "compression_ratio": 1.5875706214689265, "no_speech_prob": 5.014697308070026e-06}, {"id": 58, "seek": 27280, "start": 289.40000000000003, "end": 291.40000000000003, "text": " three fields promo", "tokens": [1045, 7909, 26750], "temperature": 0.0, "avg_logprob": -0.20130926853901632, "compression_ratio": 1.5875706214689265, "no_speech_prob": 5.014697308070026e-06}, {"id": 59, "seek": 27280, "start": 291.88, "end": 293.76, "text": " state holiday and", "tokens": [1785, 9960, 293], "temperature": 0.0, "avg_logprob": -0.20130926853901632, "compression_ratio": 1.5875706214689265, "no_speech_prob": 5.014697308070026e-06}, {"id": 60, "seek": 27280, "start": 293.76, "end": 298.44, "text": " School holiday and so basically what we have is a table of", "tokens": [5070, 9960, 293, 370, 1936, 437, 321, 362, 307, 257, 3199, 295], "temperature": 0.0, "avg_logprob": -0.20130926853901632, "compression_ratio": 1.5875706214689265, "no_speech_prob": 5.014697308070026e-06}, {"id": 61, "seek": 27280, "start": 299.2, "end": 301.12, "text": " for each store", "tokens": [337, 1184, 3531], "temperature": 0.0, "avg_logprob": -0.20130926853901632, "compression_ratio": 1.5875706214689265, "no_speech_prob": 5.014697308070026e-06}, {"id": 62, "seek": 30112, "start": 301.12, "end": 303.12, "text": " For each date", "tokens": [1171, 1184, 4002], "temperature": 0.0, "avg_logprob": -0.17547098073092374, "compression_ratio": 1.8105263157894738, "no_speech_prob": 1.3287718729770859e-06}, {"id": 63, "seek": 30112, "start": 303.12, "end": 305.96, "text": " Does that store have a promo going on at that date?", "tokens": [4402, 300, 3531, 362, 257, 26750, 516, 322, 412, 300, 4002, 30], "temperature": 0.0, "avg_logprob": -0.17547098073092374, "compression_ratio": 1.8105263157894738, "no_speech_prob": 1.3287718729770859e-06}, {"id": 64, "seek": 30112, "start": 306.92, "end": 313.32, "text": " Is there a school holiday in that region of that store at that date is there a state holiday?", "tokens": [1119, 456, 257, 1395, 9960, 294, 300, 4458, 295, 300, 3531, 412, 300, 4002, 307, 456, 257, 1785, 9960, 30], "temperature": 0.0, "avg_logprob": -0.17547098073092374, "compression_ratio": 1.8105263157894738, "no_speech_prob": 1.3287718729770859e-06}, {"id": 65, "seek": 30112, "start": 313.64, "end": 317.4, "text": " in that region for that store at that date okay, and so", "tokens": [294, 300, 4458, 337, 300, 3531, 412, 300, 4002, 1392, 11, 293, 370], "temperature": 0.0, "avg_logprob": -0.17547098073092374, "compression_ratio": 1.8105263157894738, "no_speech_prob": 1.3287718729770859e-06}, {"id": 66, "seek": 30112, "start": 317.96, "end": 319.96, "text": " this kind of thing is", "tokens": [341, 733, 295, 551, 307], "temperature": 0.0, "avg_logprob": -0.17547098073092374, "compression_ratio": 1.8105263157894738, "no_speech_prob": 1.3287718729770859e-06}, {"id": 67, "seek": 30112, "start": 320.68, "end": 327.6, "text": " You know like their events and time series with events are like very very common like if you're looking at", "tokens": [509, 458, 411, 641, 3931, 293, 565, 2638, 365, 3931, 366, 411, 588, 588, 2689, 411, 498, 291, 434, 1237, 412], "temperature": 0.0, "avg_logprob": -0.17547098073092374, "compression_ratio": 1.8105263157894738, "no_speech_prob": 1.3287718729770859e-06}, {"id": 68, "seek": 32760, "start": 327.6, "end": 332.8, "text": " Oil and gas drilling data you're trying to say like the flow through this pipe", "tokens": [23545, 293, 4211, 26290, 1412, 291, 434, 1382, 281, 584, 411, 264, 3095, 807, 341, 11240], "temperature": 0.0, "avg_logprob": -0.1628406188067268, "compression_ratio": 1.702970297029703, "no_speech_prob": 5.122880679664377e-07}, {"id": 69, "seek": 32760, "start": 333.12, "end": 336.68, "text": " You know here's an event representing when it set off some alarm", "tokens": [509, 458, 510, 311, 364, 2280, 13460, 562, 309, 992, 766, 512, 14183], "temperature": 0.0, "avg_logprob": -0.1628406188067268, "compression_ratio": 1.702970297029703, "no_speech_prob": 5.122880679664377e-07}, {"id": 70, "seek": 32760, "start": 336.68, "end": 344.54, "text": " You know or here's an event where the drill got stuck or or whatever right and so like most time series", "tokens": [509, 458, 420, 510, 311, 364, 2280, 689, 264, 11392, 658, 5541, 420, 420, 2035, 558, 293, 370, 411, 881, 565, 2638], "temperature": 0.0, "avg_logprob": -0.1628406188067268, "compression_ratio": 1.702970297029703, "no_speech_prob": 5.122880679664377e-07}, {"id": 71, "seek": 32760, "start": 344.92, "end": 348.08000000000004, "text": " At some level will tend to represent some events", "tokens": [1711, 512, 1496, 486, 3928, 281, 2906, 512, 3931], "temperature": 0.0, "avg_logprob": -0.1628406188067268, "compression_ratio": 1.702970297029703, "no_speech_prob": 5.122880679664377e-07}, {"id": 72, "seek": 32760, "start": 349.32000000000005, "end": 351.32000000000005, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.1628406188067268, "compression_ratio": 1.702970297029703, "no_speech_prob": 5.122880679664377e-07}, {"id": 73, "seek": 32760, "start": 351.88, "end": 355.52000000000004, "text": " The fact that an event happened at a time is", "tokens": [440, 1186, 300, 364, 2280, 2011, 412, 257, 565, 307], "temperature": 0.0, "avg_logprob": -0.1628406188067268, "compression_ratio": 1.702970297029703, "no_speech_prob": 5.122880679664377e-07}, {"id": 74, "seek": 35552, "start": 355.52, "end": 363.64, "text": " Is is interesting itself, but very often a time series will also show some something happening before and after", "tokens": [1119, 307, 1880, 2564, 11, 457, 588, 2049, 257, 565, 2638, 486, 611, 855, 512, 746, 2737, 949, 293, 934], "temperature": 0.0, "avg_logprob": -0.20378324720594618, "compression_ratio": 1.6129032258064515, "no_speech_prob": 5.714981057280966e-07}, {"id": 75, "seek": 35552, "start": 364.28, "end": 369.08, "text": " The event so for example in this case. We're doing grocery sales prediction", "tokens": [440, 2280, 370, 337, 1365, 294, 341, 1389, 13, 492, 434, 884, 14410, 5763, 17630], "temperature": 0.0, "avg_logprob": -0.20378324720594618, "compression_ratio": 1.6129032258064515, "no_speech_prob": 5.714981057280966e-07}, {"id": 76, "seek": 35552, "start": 369.76, "end": 371.88, "text": " If there's a holiday coming up", "tokens": [759, 456, 311, 257, 9960, 1348, 493], "temperature": 0.0, "avg_logprob": -0.20378324720594618, "compression_ratio": 1.6129032258064515, "no_speech_prob": 5.714981057280966e-07}, {"id": 77, "seek": 35552, "start": 372.32, "end": 378.64, "text": " It's quite likely that sales will be higher before and after the holiday and lower during the holiday", "tokens": [467, 311, 1596, 3700, 300, 5763, 486, 312, 2946, 949, 293, 934, 264, 9960, 293, 3126, 1830, 264, 9960], "temperature": 0.0, "avg_logprob": -0.20378324720594618, "compression_ratio": 1.6129032258064515, "no_speech_prob": 5.714981057280966e-07}, {"id": 78, "seek": 35552, "start": 379.52, "end": 381.71999999999997, "text": " If this is a city-based store", "tokens": [759, 341, 307, 257, 2307, 12, 6032, 3531], "temperature": 0.0, "avg_logprob": -0.20378324720594618, "compression_ratio": 1.6129032258064515, "no_speech_prob": 5.714981057280966e-07}, {"id": 79, "seek": 38172, "start": 381.72, "end": 386.20000000000005, "text": " Right because you know you're going to you're going to stock up before you go away to", "tokens": [1779, 570, 291, 458, 291, 434, 516, 281, 291, 434, 516, 281, 4127, 493, 949, 291, 352, 1314, 281], "temperature": 0.0, "avg_logprob": -0.2472077828866464, "compression_ratio": 1.619718309859155, "no_speech_prob": 2.1907615064264974e-06}, {"id": 80, "seek": 38172, "start": 387.12, "end": 391.88000000000005, "text": " Bring things with you and when you come back you've got a refill the fridge for instance right?", "tokens": [12842, 721, 365, 291, 293, 562, 291, 808, 646, 291, 600, 658, 257, 42533, 264, 13023, 337, 5197, 558, 30], "temperature": 0.0, "avg_logprob": -0.2472077828866464, "compression_ratio": 1.619718309859155, "no_speech_prob": 2.1907615064264974e-06}, {"id": 81, "seek": 38172, "start": 393.92, "end": 395.92, "text": " So it's", "tokens": [407, 309, 311], "temperature": 0.0, "avg_logprob": -0.2472077828866464, "compression_ratio": 1.619718309859155, "no_speech_prob": 2.1907615064264974e-06}, {"id": 82, "seek": 38172, "start": 396.48, "end": 401.64000000000004, "text": " Although like we don't necessarily have to do this kind of feature engineering to create features", "tokens": [5780, 411, 321, 500, 380, 4725, 362, 281, 360, 341, 733, 295, 4111, 7043, 281, 1884, 4122], "temperature": 0.0, "avg_logprob": -0.2472077828866464, "compression_ratio": 1.619718309859155, "no_speech_prob": 2.1907615064264974e-06}, {"id": 83, "seek": 38172, "start": 402.0, "end": 404.88000000000005, "text": " Specifically about like this is before or after a holiday", "tokens": [26058, 466, 411, 341, 307, 949, 420, 934, 257, 9960], "temperature": 0.0, "avg_logprob": -0.2472077828866464, "compression_ratio": 1.619718309859155, "no_speech_prob": 2.1907615064264974e-06}, {"id": 84, "seek": 40488, "start": 404.88, "end": 411.76, "text": " The the neural net you know the more we can give the neural net like the kind of information it needs", "tokens": [440, 264, 18161, 2533, 291, 458, 264, 544, 321, 393, 976, 264, 18161, 2533, 411, 264, 733, 295, 1589, 309, 2203], "temperature": 0.0, "avg_logprob": -0.1416571095304669, "compression_ratio": 2.0651162790697675, "no_speech_prob": 6.276665658333513e-07}, {"id": 85, "seek": 40488, "start": 412.12, "end": 417.64, "text": " The less it's going to have to learn it the less that it's going to have to learn it the more we can do with", "tokens": [440, 1570, 309, 311, 516, 281, 362, 281, 1466, 309, 264, 1570, 300, 309, 311, 516, 281, 362, 281, 1466, 309, 264, 544, 321, 393, 360, 365], "temperature": 0.0, "avg_logprob": -0.1416571095304669, "compression_ratio": 2.0651162790697675, "no_speech_prob": 6.276665658333513e-07}, {"id": 86, "seek": 40488, "start": 417.64, "end": 423.04, "text": " The data or we already have the more we can do with the you know the size architecture", "tokens": [440, 1412, 420, 321, 1217, 362, 264, 544, 321, 393, 360, 365, 264, 291, 458, 264, 2744, 9482], "temperature": 0.0, "avg_logprob": -0.1416571095304669, "compression_ratio": 2.0651162790697675, "no_speech_prob": 6.276665658333513e-07}, {"id": 87, "seek": 40488, "start": 423.04, "end": 427.26, "text": " We already have so feature engineering even even with stuff like neural nets", "tokens": [492, 1217, 362, 370, 4111, 7043, 754, 754, 365, 1507, 411, 18161, 36170], "temperature": 0.0, "avg_logprob": -0.1416571095304669, "compression_ratio": 2.0651162790697675, "no_speech_prob": 6.276665658333513e-07}, {"id": 88, "seek": 40488, "start": 428.36, "end": 432.36, "text": " Is still important because it means that you know we'll be able to do", "tokens": [1119, 920, 1021, 570, 309, 1355, 300, 291, 458, 321, 603, 312, 1075, 281, 360], "temperature": 0.0, "avg_logprob": -0.1416571095304669, "compression_ratio": 2.0651162790697675, "no_speech_prob": 6.276665658333513e-07}, {"id": 89, "seek": 43236, "start": 432.36, "end": 438.72, "text": " You know get better results with whatever limited data. We have whatever limited computation we have", "tokens": [509, 458, 483, 1101, 3542, 365, 2035, 5567, 1412, 13, 492, 362, 2035, 5567, 24903, 321, 362], "temperature": 0.0, "avg_logprob": -0.18299869655333845, "compression_ratio": 1.793103448275862, "no_speech_prob": 5.255259111436317e-06}, {"id": 90, "seek": 43236, "start": 440.84000000000003, "end": 443.08000000000004, "text": " So the basic idea here therefore is", "tokens": [407, 264, 3875, 1558, 510, 4412, 307], "temperature": 0.0, "avg_logprob": -0.18299869655333845, "compression_ratio": 1.793103448275862, "no_speech_prob": 5.255259111436317e-06}, {"id": 91, "seek": 43236, "start": 443.64, "end": 446.72, "text": " When we have events in our time series is we want to create", "tokens": [1133, 321, 362, 3931, 294, 527, 565, 2638, 307, 321, 528, 281, 1884], "temperature": 0.0, "avg_logprob": -0.18299869655333845, "compression_ratio": 1.793103448275862, "no_speech_prob": 5.255259111436317e-06}, {"id": 92, "seek": 43236, "start": 447.52000000000004, "end": 449.52000000000004, "text": " Two new columns for each event", "tokens": [4453, 777, 13766, 337, 1184, 2280], "temperature": 0.0, "avg_logprob": -0.18299869655333845, "compression_ratio": 1.793103448275862, "no_speech_prob": 5.255259111436317e-06}, {"id": 93, "seek": 43236, "start": 449.8, "end": 453.56, "text": " How long is it going to be until the next time this event happens and?", "tokens": [1012, 938, 307, 309, 516, 281, 312, 1826, 264, 958, 565, 341, 2280, 2314, 293, 30], "temperature": 0.0, "avg_logprob": -0.18299869655333845, "compression_ratio": 1.793103448275862, "no_speech_prob": 5.255259111436317e-06}, {"id": 94, "seek": 43236, "start": 453.84000000000003, "end": 460.04, "text": " How long has it been since the last time that event happened so in other words how long until the next state holiday?", "tokens": [1012, 938, 575, 309, 668, 1670, 264, 1036, 565, 300, 2280, 2011, 370, 294, 661, 2283, 577, 938, 1826, 264, 958, 1785, 9960, 30], "temperature": 0.0, "avg_logprob": -0.18299869655333845, "compression_ratio": 1.793103448275862, "no_speech_prob": 5.255259111436317e-06}, {"id": 95, "seek": 46004, "start": 460.04, "end": 462.44, "text": " How long since the previous state holiday?", "tokens": [1012, 938, 1670, 264, 3894, 1785, 9960, 30], "temperature": 0.0, "avg_logprob": -0.1822315015290913, "compression_ratio": 1.6244541484716157, "no_speech_prob": 1.9033751641472918e-06}, {"id": 96, "seek": 46004, "start": 463.48, "end": 468.52000000000004, "text": " Okay, so that's not something which I'm aware of as existing as a", "tokens": [1033, 11, 370, 300, 311, 406, 746, 597, 286, 478, 3650, 295, 382, 6741, 382, 257], "temperature": 0.0, "avg_logprob": -0.1822315015290913, "compression_ratio": 1.6244541484716157, "no_speech_prob": 1.9033751641472918e-06}, {"id": 97, "seek": 46004, "start": 469.56, "end": 477.64000000000004, "text": " Library or anything like that so we I wrote it here by hand right and so importantly I need to do this", "tokens": [12806, 420, 1340, 411, 300, 370, 321, 286, 4114, 309, 510, 538, 1011, 558, 293, 370, 8906, 286, 643, 281, 360, 341], "temperature": 0.0, "avg_logprob": -0.1822315015290913, "compression_ratio": 1.6244541484716157, "no_speech_prob": 1.9033751641472918e-06}, {"id": 98, "seek": 46004, "start": 478.48, "end": 480.28000000000003, "text": " by store", "tokens": [538, 3531], "temperature": 0.0, "avg_logprob": -0.1822315015290913, "compression_ratio": 1.6244541484716157, "no_speech_prob": 1.9033751641472918e-06}, {"id": 99, "seek": 46004, "start": 480.28000000000003, "end": 483.82000000000005, "text": " Right so I want to say like because you know for this store", "tokens": [1779, 370, 286, 528, 281, 584, 411, 570, 291, 458, 337, 341, 3531], "temperature": 0.0, "avg_logprob": -0.1822315015290913, "compression_ratio": 1.6244541484716157, "no_speech_prob": 1.9033751641472918e-06}, {"id": 100, "seek": 48382, "start": 483.82, "end": 489.74, "text": " Or when was this stores last promo so how long has it been since the last time it had a promo?", "tokens": [1610, 562, 390, 341, 9512, 1036, 26750, 370, 577, 938, 575, 309, 668, 1670, 264, 1036, 565, 309, 632, 257, 26750, 30], "temperature": 0.0, "avg_logprob": -0.18928311234813625, "compression_ratio": 1.8755186721991701, "no_speech_prob": 1.2878921324954717e-06}, {"id": 101, "seek": 48382, "start": 489.98, "end": 494.86, "text": " How long it will be until the next time it has a promo for instance?", "tokens": [1012, 938, 309, 486, 312, 1826, 264, 958, 565, 309, 575, 257, 26750, 337, 5197, 30], "temperature": 0.0, "avg_logprob": -0.18928311234813625, "compression_ratio": 1.8755186721991701, "no_speech_prob": 1.2878921324954717e-06}, {"id": 102, "seek": 48382, "start": 495.9, "end": 497.9, "text": " all right, so", "tokens": [439, 558, 11, 370], "temperature": 0.0, "avg_logprob": -0.18928311234813625, "compression_ratio": 1.8755186721991701, "no_speech_prob": 1.2878921324954717e-06}, {"id": 103, "seek": 48382, "start": 499.5, "end": 502.3, "text": " Here's what I'm going to do. I'm going to create a little function", "tokens": [1692, 311, 437, 286, 478, 516, 281, 360, 13, 286, 478, 516, 281, 1884, 257, 707, 2445], "temperature": 0.0, "avg_logprob": -0.18928311234813625, "compression_ratio": 1.8755186721991701, "no_speech_prob": 1.2878921324954717e-06}, {"id": 104, "seek": 48382, "start": 503.02, "end": 508.88, "text": " That's going to take a field name, and I'm going to pass it each of promo and then state holiday and then school holiday", "tokens": [663, 311, 516, 281, 747, 257, 2519, 1315, 11, 293, 286, 478, 516, 281, 1320, 309, 1184, 295, 26750, 293, 550, 1785, 9960, 293, 550, 1395, 9960], "temperature": 0.0, "avg_logprob": -0.18928311234813625, "compression_ratio": 1.8755186721991701, "no_speech_prob": 1.2878921324954717e-06}, {"id": 105, "seek": 48382, "start": 508.88, "end": 512.86, "text": " Right so let's do school holiday for example, so we'll say field equals school holiday", "tokens": [1779, 370, 718, 311, 360, 1395, 9960, 337, 1365, 11, 370, 321, 603, 584, 2519, 6915, 1395, 9960], "temperature": 0.0, "avg_logprob": -0.18928311234813625, "compression_ratio": 1.8755186721991701, "no_speech_prob": 1.2878921324954717e-06}, {"id": 106, "seek": 51286, "start": 512.86, "end": 513.94, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.24183994474865142, "compression_ratio": 1.7927927927927927, "no_speech_prob": 4.029441242892062e-06}, {"id": 107, "seek": 51286, "start": 513.94, "end": 515.7, "text": " then we'll say", "tokens": [550, 321, 603, 584], "temperature": 0.0, "avg_logprob": -0.24183994474865142, "compression_ratio": 1.7927927927927927, "no_speech_prob": 4.029441242892062e-06}, {"id": 108, "seek": 51286, "start": 515.7, "end": 518.38, "text": " Get elapsed school holiday comma", "tokens": [3240, 806, 2382, 292, 1395, 9960, 22117], "temperature": 0.0, "avg_logprob": -0.24183994474865142, "compression_ratio": 1.7927927927927927, "no_speech_prob": 4.029441242892062e-06}, {"id": 109, "seek": 51286, "start": 519.3000000000001, "end": 523.26, "text": " After so let me show you what that's going to do so we've got a first of all sort", "tokens": [2381, 370, 718, 385, 855, 291, 437, 300, 311, 516, 281, 360, 370, 321, 600, 658, 257, 700, 295, 439, 1333], "temperature": 0.0, "avg_logprob": -0.24183994474865142, "compression_ratio": 1.7927927927927927, "no_speech_prob": 4.029441242892062e-06}, {"id": 110, "seek": 51286, "start": 524.02, "end": 526.02, "text": " by store and date", "tokens": [538, 3531, 293, 4002], "temperature": 0.0, "avg_logprob": -0.24183994474865142, "compression_ratio": 1.7927927927927927, "no_speech_prob": 4.029441242892062e-06}, {"id": 111, "seek": 51286, "start": 526.22, "end": 531.62, "text": " Right so now when we loop through this we're going to be looping through within a store so store number one", "tokens": [1779, 370, 586, 562, 321, 6367, 807, 341, 321, 434, 516, 281, 312, 6367, 278, 807, 1951, 257, 3531, 370, 3531, 1230, 472], "temperature": 0.0, "avg_logprob": -0.24183994474865142, "compression_ratio": 1.7927927927927927, "no_speech_prob": 4.029441242892062e-06}, {"id": 112, "seek": 51286, "start": 531.98, "end": 535.54, "text": " January the first January the second January the third and so forth and", "tokens": [7061, 264, 700, 7061, 264, 1150, 7061, 264, 2636, 293, 370, 5220, 293], "temperature": 0.0, "avg_logprob": -0.24183994474865142, "compression_ratio": 1.7927927927927927, "no_speech_prob": 4.029441242892062e-06}, {"id": 113, "seek": 51286, "start": 536.86, "end": 541.02, "text": " as we loop through each store we're basically going to say like is", "tokens": [382, 321, 6367, 807, 1184, 3531, 321, 434, 1936, 516, 281, 584, 411, 307], "temperature": 0.0, "avg_logprob": -0.24183994474865142, "compression_ratio": 1.7927927927927927, "no_speech_prob": 4.029441242892062e-06}, {"id": 114, "seek": 54102, "start": 541.02, "end": 546.3, "text": " Is is this row a school holiday or not and if it is a school holiday?", "tokens": [1119, 307, 341, 5386, 257, 1395, 9960, 420, 406, 293, 498, 309, 307, 257, 1395, 9960, 30], "temperature": 0.0, "avg_logprob": -0.18283979354366178, "compression_ratio": 1.7788018433179724, "no_speech_prob": 1.6280470163110294e-06}, {"id": 115, "seek": 54102, "start": 546.5799999999999, "end": 553.9399999999999, "text": " Then we'll keep track of this variable called last date which says this is the last date and which where we saw a school holiday", "tokens": [1396, 321, 603, 1066, 2837, 295, 341, 7006, 1219, 1036, 4002, 597, 1619, 341, 307, 264, 1036, 4002, 293, 597, 689, 321, 1866, 257, 1395, 9960], "temperature": 0.0, "avg_logprob": -0.18283979354366178, "compression_ratio": 1.7788018433179724, "no_speech_prob": 1.6280470163110294e-06}, {"id": 116, "seek": 54102, "start": 554.6999999999999, "end": 559.26, "text": " Okay, and so then we're basically going to append to our result", "tokens": [1033, 11, 293, 370, 550, 321, 434, 1936, 516, 281, 34116, 281, 527, 1874], "temperature": 0.0, "avg_logprob": -0.18283979354366178, "compression_ratio": 1.7788018433179724, "no_speech_prob": 1.6280470163110294e-06}, {"id": 117, "seek": 54102, "start": 559.86, "end": 567.98, "text": " The number of days since the last school holiday. That's the kind of basic idea here, so there's a few interesting features", "tokens": [440, 1230, 295, 1708, 1670, 264, 1036, 1395, 9960, 13, 663, 311, 264, 733, 295, 3875, 1558, 510, 11, 370, 456, 311, 257, 1326, 1880, 4122], "temperature": 0.0, "avg_logprob": -0.18283979354366178, "compression_ratio": 1.7788018433179724, "no_speech_prob": 1.6280470163110294e-06}, {"id": 118, "seek": 56798, "start": 567.98, "end": 570.62, "text": " One is the use of zip", "tokens": [1485, 307, 264, 764, 295, 20730], "temperature": 0.0, "avg_logprob": -0.2980389815110427, "compression_ratio": 1.4596273291925466, "no_speech_prob": 1.5779590967213153e-06}, {"id": 119, "seek": 56798, "start": 571.58, "end": 577.54, "text": " Right so I could actually write this much more simply right I could say", "tokens": [1779, 370, 286, 727, 767, 2464, 341, 709, 544, 2935, 558, 286, 727, 584], "temperature": 0.0, "avg_logprob": -0.2980389815110427, "compression_ratio": 1.4596273291925466, "no_speech_prob": 1.5779590967213153e-06}, {"id": 120, "seek": 56798, "start": 579.54, "end": 581.54, "text": " Let's go through", "tokens": [961, 311, 352, 807], "temperature": 0.0, "avg_logprob": -0.2980389815110427, "compression_ratio": 1.4596273291925466, "no_speech_prob": 1.5779590967213153e-06}, {"id": 121, "seek": 56798, "start": 582.0600000000001, "end": 585.78, "text": " Well we could basically go through like for row in", "tokens": [1042, 321, 727, 1936, 352, 807, 411, 337, 5386, 294], "temperature": 0.0, "avg_logprob": -0.2980389815110427, "compression_ratio": 1.4596273291925466, "no_speech_prob": 1.5779590967213153e-06}, {"id": 122, "seek": 56798, "start": 586.58, "end": 588.58, "text": " DF dot it a rose", "tokens": [48336, 5893, 309, 257, 10895], "temperature": 0.0, "avg_logprob": -0.2980389815110427, "compression_ratio": 1.4596273291925466, "no_speech_prob": 1.5779590967213153e-06}, {"id": 123, "seek": 56798, "start": 589.78, "end": 593.6800000000001, "text": " Right and then grab the the fields we want from each row", "tokens": [1779, 293, 550, 4444, 264, 264, 7909, 321, 528, 490, 1184, 5386], "temperature": 0.0, "avg_logprob": -0.2980389815110427, "compression_ratio": 1.4596273291925466, "no_speech_prob": 1.5779590967213153e-06}, {"id": 124, "seek": 59368, "start": 593.68, "end": 600.7199999999999, "text": " It turns out this is 300 times slower than the version that I have and", "tokens": [467, 4523, 484, 341, 307, 6641, 1413, 14009, 813, 264, 3037, 300, 286, 362, 293], "temperature": 0.0, "avg_logprob": -0.27213321193572015, "compression_ratio": 1.4451219512195121, "no_speech_prob": 1.742977531193901e-07}, {"id": 125, "seek": 59368, "start": 601.76, "end": 603.16, "text": " basically like", "tokens": [1936, 411], "temperature": 0.0, "avg_logprob": -0.27213321193572015, "compression_ratio": 1.4451219512195121, "no_speech_prob": 1.742977531193901e-07}, {"id": 126, "seek": 59368, "start": 603.16, "end": 605.62, "text": " iterating through a data frame", "tokens": [17138, 990, 807, 257, 1412, 3920], "temperature": 0.0, "avg_logprob": -0.27213321193572015, "compression_ratio": 1.4451219512195121, "no_speech_prob": 1.742977531193901e-07}, {"id": 127, "seek": 59368, "start": 606.2399999999999, "end": 608.4799999999999, "text": " And extracting specific", "tokens": [400, 49844, 2685], "temperature": 0.0, "avg_logprob": -0.27213321193572015, "compression_ratio": 1.4451219512195121, "no_speech_prob": 1.742977531193901e-07}, {"id": 128, "seek": 59368, "start": 609.28, "end": 611.28, "text": " fields out of a row", "tokens": [7909, 484, 295, 257, 5386], "temperature": 0.0, "avg_logprob": -0.27213321193572015, "compression_ratio": 1.4451219512195121, "no_speech_prob": 1.742977531193901e-07}, {"id": 129, "seek": 59368, "start": 612.4, "end": 618.4399999999999, "text": " Has a lot of overhead what's much faster is to iterate through a numpy array", "tokens": [8646, 257, 688, 295, 19922, 437, 311, 709, 4663, 307, 281, 44497, 807, 257, 1031, 8200, 10225], "temperature": 0.0, "avg_logprob": -0.27213321193572015, "compression_ratio": 1.4451219512195121, "no_speech_prob": 1.742977531193901e-07}, {"id": 130, "seek": 61844, "start": 618.44, "end": 620.44, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.22547812794530114, "compression_ratio": 1.5614973262032086, "no_speech_prob": 4.5921294145045977e-07}, {"id": 131, "seek": 61844, "start": 621.84, "end": 629.7800000000001, "text": " If you take a series like DF dot store and add dot values after it that grabs a numpy array of that series", "tokens": [759, 291, 747, 257, 2638, 411, 48336, 5893, 3531, 293, 909, 5893, 4190, 934, 309, 300, 30028, 257, 1031, 8200, 10225, 295, 300, 2638], "temperature": 0.0, "avg_logprob": -0.22547812794530114, "compression_ratio": 1.5614973262032086, "no_speech_prob": 4.5921294145045977e-07}, {"id": 132, "seek": 61844, "start": 630.12, "end": 633.8800000000001, "text": " Okay, so here are three numpy arrays one is the store IDs", "tokens": [1033, 11, 370, 510, 366, 1045, 1031, 8200, 41011, 472, 307, 264, 3531, 48212], "temperature": 0.0, "avg_logprob": -0.22547812794530114, "compression_ratio": 1.5614973262032086, "no_speech_prob": 4.5921294145045977e-07}, {"id": 133, "seek": 61844, "start": 634.6800000000001, "end": 636.5600000000001, "text": " one is", "tokens": [472, 307], "temperature": 0.0, "avg_logprob": -0.22547812794530114, "compression_ratio": 1.5614973262032086, "no_speech_prob": 4.5921294145045977e-07}, {"id": 134, "seek": 61844, "start": 636.5600000000001, "end": 639.6400000000001, "text": " Whatever field is in this case. Let's say school holiday and", "tokens": [8541, 2519, 307, 294, 341, 1389, 13, 961, 311, 584, 1395, 9960, 293], "temperature": 0.0, "avg_logprob": -0.22547812794530114, "compression_ratio": 1.5614973262032086, "no_speech_prob": 4.5921294145045977e-07}, {"id": 135, "seek": 61844, "start": 640.48, "end": 641.7600000000001, "text": " one is", "tokens": [472, 307], "temperature": 0.0, "avg_logprob": -0.22547812794530114, "compression_ratio": 1.5614973262032086, "no_speech_prob": 4.5921294145045977e-07}, {"id": 136, "seek": 61844, "start": 641.7600000000001, "end": 643.1600000000001, "text": " the date", "tokens": [264, 4002], "temperature": 0.0, "avg_logprob": -0.22547812794530114, "compression_ratio": 1.5614973262032086, "no_speech_prob": 4.5921294145045977e-07}, {"id": 137, "seek": 61844, "start": 643.1600000000001, "end": 645.72, "text": " So now what I want to do is loop through", "tokens": [407, 586, 437, 286, 528, 281, 360, 307, 6367, 807], "temperature": 0.0, "avg_logprob": -0.22547812794530114, "compression_ratio": 1.5614973262032086, "no_speech_prob": 4.5921294145045977e-07}, {"id": 138, "seek": 64572, "start": 645.72, "end": 649.24, "text": " the first one of each of those lists and", "tokens": [264, 700, 472, 295, 1184, 295, 729, 14511, 293], "temperature": 0.0, "avg_logprob": -0.15620435866634402, "compression_ratio": 2.026431718061674, "no_speech_prob": 2.4439848402835196e-06}, {"id": 139, "seek": 64572, "start": 650.0400000000001, "end": 653.24, "text": " Then the second one of each of those lists and then the third one of each of those lists and", "tokens": [1396, 264, 1150, 472, 295, 1184, 295, 729, 14511, 293, 550, 264, 2636, 472, 295, 1184, 295, 729, 14511, 293], "temperature": 0.0, "avg_logprob": -0.15620435866634402, "compression_ratio": 2.026431718061674, "no_speech_prob": 2.4439848402835196e-06}, {"id": 140, "seek": 64572, "start": 653.64, "end": 655.8000000000001, "text": " Like this is a really really common pattern", "tokens": [1743, 341, 307, 257, 534, 534, 2689, 5102], "temperature": 0.0, "avg_logprob": -0.15620435866634402, "compression_ratio": 2.026431718061674, "no_speech_prob": 2.4439848402835196e-06}, {"id": 141, "seek": 64572, "start": 655.8000000000001, "end": 661.8000000000001, "text": " I need to do something like this in basically every notebook I write and the way to do it is with zip", "tokens": [286, 643, 281, 360, 746, 411, 341, 294, 1936, 633, 21060, 286, 2464, 293, 264, 636, 281, 360, 309, 307, 365, 20730], "temperature": 0.0, "avg_logprob": -0.15620435866634402, "compression_ratio": 2.026431718061674, "no_speech_prob": 2.4439848402835196e-06}, {"id": 142, "seek": 64572, "start": 661.96, "end": 666.2, "text": " All right, so zip means loop through each of these lists", "tokens": [1057, 558, 11, 370, 20730, 1355, 6367, 807, 1184, 295, 613, 14511], "temperature": 0.0, "avg_logprob": -0.15620435866634402, "compression_ratio": 2.026431718061674, "no_speech_prob": 2.4439848402835196e-06}, {"id": 143, "seek": 64572, "start": 666.72, "end": 670.5600000000001, "text": " One at a time and then this here is where we can grab", "tokens": [1485, 412, 257, 565, 293, 550, 341, 510, 307, 689, 321, 393, 4444], "temperature": 0.0, "avg_logprob": -0.15620435866634402, "compression_ratio": 2.026431718061674, "no_speech_prob": 2.4439848402835196e-06}, {"id": 144, "seek": 67056, "start": 670.56, "end": 678.4, "text": " That element out of the first list the second list and the third list okay, so if you haven't played around watch with zip", "tokens": [663, 4478, 484, 295, 264, 700, 1329, 264, 1150, 1329, 293, 264, 2636, 1329, 1392, 11, 370, 498, 291, 2378, 380, 3737, 926, 1159, 365, 20730], "temperature": 0.0, "avg_logprob": -0.1808561537000868, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.6280448562611127e-06}, {"id": 145, "seek": 67056, "start": 678.92, "end": 686.0799999999999, "text": " That's a really important function to practice with like I say I use it in pretty much every notebook I write", "tokens": [663, 311, 257, 534, 1021, 2445, 281, 3124, 365, 411, 286, 584, 286, 764, 309, 294, 1238, 709, 633, 21060, 286, 2464], "temperature": 0.0, "avg_logprob": -0.1808561537000868, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.6280448562611127e-06}, {"id": 146, "seek": 67056, "start": 687.28, "end": 689.76, "text": " All the time you have to loop through", "tokens": [1057, 264, 565, 291, 362, 281, 6367, 807], "temperature": 0.0, "avg_logprob": -0.1808561537000868, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.6280448562611127e-06}, {"id": 147, "seek": 67056, "start": 691.0799999999999, "end": 693.4399999999999, "text": " You know a bunch of lists at the same time", "tokens": [509, 458, 257, 3840, 295, 14511, 412, 264, 912, 565], "temperature": 0.0, "avg_logprob": -0.1808561537000868, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.6280448562611127e-06}, {"id": 148, "seek": 67056, "start": 695.0799999999999, "end": 697.4799999999999, "text": " All right, so we're going to look through every store", "tokens": [1057, 558, 11, 370, 321, 434, 516, 281, 574, 807, 633, 3531], "temperature": 0.0, "avg_logprob": -0.1808561537000868, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.6280448562611127e-06}, {"id": 149, "seek": 69748, "start": 697.48, "end": 700.5600000000001, "text": " Every school holiday at every date", "tokens": [2048, 1395, 9960, 412, 633, 4002], "temperature": 0.0, "avg_logprob": -0.29611978298280295, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.5779534123794292e-06}, {"id": 150, "seek": 69748, "start": 701.52, "end": 703.52, "text": " Yes", "tokens": [1079], "temperature": 0.0, "avg_logprob": -0.29611978298280295, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.5779534123794292e-06}, {"id": 151, "seek": 69748, "start": 705.16, "end": 710.36, "text": " So is it looping through like all the possible combinations of each of those or just", "tokens": [407, 307, 309, 6367, 278, 807, 411, 439, 264, 1944, 21267, 295, 1184, 295, 729, 420, 445], "temperature": 0.0, "avg_logprob": -0.29611978298280295, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.5779534123794292e-06}, {"id": 152, "seek": 69748, "start": 711.5600000000001, "end": 713.64, "text": " Yeah, exactly one one two three two two yeah", "tokens": [865, 11, 2293, 472, 472, 732, 1045, 732, 732, 1338], "temperature": 0.0, "avg_logprob": -0.29611978298280295, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.5779534123794292e-06}, {"id": 153, "seek": 69748, "start": 714.4, "end": 719.64, "text": " Thanks for the question so in this case we basically want to say let's grab the first store", "tokens": [2561, 337, 264, 1168, 370, 294, 341, 1389, 321, 1936, 528, 281, 584, 718, 311, 4444, 264, 700, 3531], "temperature": 0.0, "avg_logprob": -0.29611978298280295, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.5779534123794292e-06}, {"id": 154, "seek": 69748, "start": 720.08, "end": 726.2, "text": " The first school holiday the first date right so for store one January the first", "tokens": [440, 700, 1395, 9960, 264, 700, 4002, 558, 370, 337, 3531, 472, 7061, 264, 700], "temperature": 0.0, "avg_logprob": -0.29611978298280295, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.5779534123794292e-06}, {"id": 155, "seek": 72620, "start": 726.2, "end": 732.24, "text": " School holiday was true or false right and so if it is a school holiday", "tokens": [5070, 9960, 390, 2074, 420, 7908, 558, 293, 370, 498, 309, 307, 257, 1395, 9960], "temperature": 0.0, "avg_logprob": -0.15169379687068438, "compression_ratio": 1.7339449541284404, "no_speech_prob": 3.0894773317413637e-06}, {"id": 156, "seek": 72620, "start": 732.24, "end": 737.5200000000001, "text": " I'll keep track of that fact by saying the last time I saw a school holiday was that day", "tokens": [286, 603, 1066, 2837, 295, 300, 1186, 538, 1566, 264, 1036, 565, 286, 1866, 257, 1395, 9960, 390, 300, 786], "temperature": 0.0, "avg_logprob": -0.15169379687068438, "compression_ratio": 1.7339449541284404, "no_speech_prob": 3.0894773317413637e-06}, {"id": 157, "seek": 72620, "start": 738.12, "end": 740.12, "text": " Okay, and then", "tokens": [1033, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.15169379687068438, "compression_ratio": 1.7339449541284404, "no_speech_prob": 3.0894773317413637e-06}, {"id": 158, "seek": 72620, "start": 740.32, "end": 745.96, "text": " Append how long has it been since the last school holiday right and if the?", "tokens": [3132, 521, 577, 938, 575, 309, 668, 1670, 264, 1036, 1395, 9960, 558, 293, 498, 264, 30], "temperature": 0.0, "avg_logprob": -0.15169379687068438, "compression_ratio": 1.7339449541284404, "no_speech_prob": 3.0894773317413637e-06}, {"id": 159, "seek": 72620, "start": 746.6400000000001, "end": 750.0, "text": " Store ID is different to the last door ID", "tokens": [17242, 7348, 307, 819, 281, 264, 1036, 2853, 7348], "temperature": 0.0, "avg_logprob": -0.15169379687068438, "compression_ratio": 1.7339449541284404, "no_speech_prob": 3.0894773317413637e-06}, {"id": 160, "seek": 72620, "start": 750.0, "end": 754.9200000000001, "text": " I saw then I've now got to a whole new store in which case I have to basically reset", "tokens": [286, 1866, 550, 286, 600, 586, 658, 281, 257, 1379, 777, 3531, 294, 597, 1389, 286, 362, 281, 1936, 14322], "temperature": 0.0, "avg_logprob": -0.15169379687068438, "compression_ratio": 1.7339449541284404, "no_speech_prob": 3.0894773317413637e-06}, {"id": 161, "seek": 75492, "start": 754.92, "end": 757.36, "text": " Everything okay, you're part that to go", "tokens": [5471, 1392, 11, 291, 434, 644, 300, 281, 352], "temperature": 0.0, "avg_logprob": -0.32595380147298175, "compression_ratio": 1.5211267605633803, "no_speech_prob": 1.3630775356432423e-05}, {"id": 162, "seek": 75492, "start": 759.8, "end": 763.52, "text": " What will happen to the first points that we don't have a like last", "tokens": [708, 486, 1051, 281, 264, 700, 2793, 300, 321, 500, 380, 362, 257, 411, 1036], "temperature": 0.0, "avg_logprob": -0.32595380147298175, "compression_ratio": 1.5211267605633803, "no_speech_prob": 1.3630775356432423e-05}, {"id": 163, "seek": 75492, "start": 765.0, "end": 769.4, "text": " Holiday yeah, so I just said I basically set this to some", "tokens": [40898, 1338, 11, 370, 286, 445, 848, 286, 1936, 992, 341, 281, 512], "temperature": 0.0, "avg_logprob": -0.32595380147298175, "compression_ratio": 1.5211267605633803, "no_speech_prob": 1.3630775356432423e-05}, {"id": 164, "seek": 75492, "start": 770.8, "end": 777.4399999999999, "text": " Arbitrary starting point it's going to end up with like the I can't remember side the light largest or the smallest possible date", "tokens": [1587, 5260, 81, 822, 2891, 935, 309, 311, 516, 281, 917, 493, 365, 411, 264, 286, 393, 380, 1604, 1252, 264, 1442, 6443, 420, 264, 16998, 1944, 4002], "temperature": 0.0, "avg_logprob": -0.32595380147298175, "compression_ratio": 1.5211267605633803, "no_speech_prob": 1.3630775356432423e-05}, {"id": 165, "seek": 75492, "start": 779.68, "end": 781.68, "text": " And you know you may need to", "tokens": [400, 291, 458, 291, 815, 643, 281], "temperature": 0.0, "avg_logprob": -0.32595380147298175, "compression_ratio": 1.5211267605633803, "no_speech_prob": 1.3630775356432423e-05}, {"id": 166, "seek": 78168, "start": 781.68, "end": 790.28, "text": " Replace this with a missing value afterwards or some you know the zero or or whatever", "tokens": [1300, 6742, 341, 365, 257, 5361, 2158, 10543, 420, 512, 291, 458, 264, 4018, 420, 420, 2035], "temperature": 0.0, "avg_logprob": -0.23606148248986353, "compression_ratio": 1.526829268292683, "no_speech_prob": 6.540336471516639e-06}, {"id": 167, "seek": 78168, "start": 794.4799999999999, "end": 797.12, "text": " You know the nice thing is though thanks to", "tokens": [509, 458, 264, 1481, 551, 307, 1673, 3231, 281], "temperature": 0.0, "avg_logprob": -0.23606148248986353, "compression_ratio": 1.526829268292683, "no_speech_prob": 6.540336471516639e-06}, {"id": 168, "seek": 78168, "start": 798.3599999999999, "end": 803.8399999999999, "text": " Reliou's it's very easy for a neural net to kind of cut off extreme values", "tokens": [8738, 72, 263, 311, 309, 311, 588, 1858, 337, 257, 18161, 2533, 281, 733, 295, 1723, 766, 8084, 4190], "temperature": 0.0, "avg_logprob": -0.23606148248986353, "compression_ratio": 1.526829268292683, "no_speech_prob": 6.540336471516639e-06}, {"id": 169, "seek": 78168, "start": 804.4399999999999, "end": 809.52, "text": " So in this case I didn't do anything special with it. I ended up with these like negative a billion day time", "tokens": [407, 294, 341, 1389, 286, 994, 380, 360, 1340, 2121, 365, 309, 13, 286, 4590, 493, 365, 613, 411, 3671, 257, 5218, 786, 565], "temperature": 0.0, "avg_logprob": -0.23606148248986353, "compression_ratio": 1.526829268292683, "no_speech_prob": 6.540336471516639e-06}, {"id": 170, "seek": 80952, "start": 809.52, "end": 813.0, "text": " Stamps and it still worked fine", "tokens": [745, 23150, 293, 309, 920, 2732, 2489], "temperature": 0.0, "avg_logprob": -0.18519621803646996, "compression_ratio": 1.6, "no_speech_prob": 4.0928848648036364e-06}, {"id": 171, "seek": 80952, "start": 815.12, "end": 818.96, "text": " Okay, so we can go through and so the next thing to note is", "tokens": [1033, 11, 370, 321, 393, 352, 807, 293, 370, 264, 958, 551, 281, 3637, 307], "temperature": 0.0, "avg_logprob": -0.18519621803646996, "compression_ratio": 1.6, "no_speech_prob": 4.0928848648036364e-06}, {"id": 172, "seek": 80952, "start": 821.28, "end": 826.92, "text": " There's a whole bunch of stuff that I need to do to both the training set and the test set right so in the previous", "tokens": [821, 311, 257, 1379, 3840, 295, 1507, 300, 286, 643, 281, 360, 281, 1293, 264, 3097, 992, 293, 264, 1500, 992, 558, 370, 294, 264, 3894], "temperature": 0.0, "avg_logprob": -0.18519621803646996, "compression_ratio": 1.6, "no_speech_prob": 4.0928848648036364e-06}, {"id": 173, "seek": 80952, "start": 827.16, "end": 832.3199999999999, "text": " Section I actually kind of added this little loop where I go for each of the", "tokens": [21804, 286, 767, 733, 295, 3869, 341, 707, 6367, 689, 286, 352, 337, 1184, 295, 264], "temperature": 0.0, "avg_logprob": -0.18519621803646996, "compression_ratio": 1.6, "no_speech_prob": 4.0928848648036364e-06}, {"id": 174, "seek": 80952, "start": 832.84, "end": 835.96, "text": " Training data frame and the test data frame", "tokens": [20620, 1412, 3920, 293, 264, 1500, 1412, 3920], "temperature": 0.0, "avg_logprob": -0.18519621803646996, "compression_ratio": 1.6, "no_speech_prob": 4.0928848648036364e-06}, {"id": 175, "seek": 83596, "start": 835.96, "end": 841.86, "text": " Do these things right so I kind of you know each cell I did for each of the data frames", "tokens": [1144, 613, 721, 558, 370, 286, 733, 295, 291, 458, 1184, 2815, 286, 630, 337, 1184, 295, 264, 1412, 12083], "temperature": 0.0, "avg_logprob": -0.14576382679982228, "compression_ratio": 1.8237885462555066, "no_speech_prob": 5.422197318694089e-06}, {"id": 176, "seek": 83596, "start": 842.5600000000001, "end": 844.84, "text": " I've now got a whole coming up a whole", "tokens": [286, 600, 586, 658, 257, 1379, 1348, 493, 257, 1379], "temperature": 0.0, "avg_logprob": -0.14576382679982228, "compression_ratio": 1.8237885462555066, "no_speech_prob": 5.422197318694089e-06}, {"id": 177, "seek": 83596, "start": 846.0400000000001, "end": 852.14, "text": " Series of cells that I want to run first of all for the training set and then for the test set", "tokens": [13934, 295, 5438, 300, 286, 528, 281, 1190, 700, 295, 439, 337, 264, 3097, 992, 293, 550, 337, 264, 1500, 992], "temperature": 0.0, "avg_logprob": -0.14576382679982228, "compression_ratio": 1.8237885462555066, "no_speech_prob": 5.422197318694089e-06}, {"id": 178, "seek": 83596, "start": 852.2800000000001, "end": 855.58, "text": " So in this case the way I did that was I had two different cells here", "tokens": [407, 294, 341, 1389, 264, 636, 286, 630, 300, 390, 286, 632, 732, 819, 5438, 510], "temperature": 0.0, "avg_logprob": -0.14576382679982228, "compression_ratio": 1.8237885462555066, "no_speech_prob": 5.422197318694089e-06}, {"id": 179, "seek": 83596, "start": 855.86, "end": 862.0, "text": " One which set DF to be the training set one which set to be the test set and so the way I use this is", "tokens": [1485, 597, 992, 48336, 281, 312, 264, 3097, 992, 472, 597, 992, 281, 312, 264, 1500, 992, 293, 370, 264, 636, 286, 764, 341, 307], "temperature": 0.0, "avg_logprob": -0.14576382679982228, "compression_ratio": 1.8237885462555066, "no_speech_prob": 5.422197318694089e-06}, {"id": 180, "seek": 83596, "start": 862.0, "end": 864.0, "text": " I run just this cell", "tokens": [286, 1190, 445, 341, 2815], "temperature": 0.0, "avg_logprob": -0.14576382679982228, "compression_ratio": 1.8237885462555066, "no_speech_prob": 5.422197318694089e-06}, {"id": 181, "seek": 86400, "start": 864.0, "end": 866.96, "text": " Right and then I run all the cells underneath", "tokens": [1779, 293, 550, 286, 1190, 439, 264, 5438, 7223], "temperature": 0.0, "avg_logprob": -0.14155705769856772, "compression_ratio": 1.8917748917748918, "no_speech_prob": 7.224426781249349e-07}, {"id": 182, "seek": 86400, "start": 867.6, "end": 873.6, "text": " Right so it does it all to the training set and then I come back and run just this cell and then run all the cells", "tokens": [1779, 370, 309, 775, 309, 439, 281, 264, 3097, 992, 293, 550, 286, 808, 646, 293, 1190, 445, 341, 2815, 293, 550, 1190, 439, 264, 5438], "temperature": 0.0, "avg_logprob": -0.14155705769856772, "compression_ratio": 1.8917748917748918, "no_speech_prob": 7.224426781249349e-07}, {"id": 183, "seek": 86400, "start": 873.6, "end": 875.88, "text": " underneath okay, so like this", "tokens": [7223, 1392, 11, 370, 411, 341], "temperature": 0.0, "avg_logprob": -0.14155705769856772, "compression_ratio": 1.8917748917748918, "no_speech_prob": 7.224426781249349e-07}, {"id": 184, "seek": 86400, "start": 877.16, "end": 881.88, "text": " Notebook is not designed to be just run from top to bottom, but it's designed to be run in this", "tokens": [11633, 2939, 307, 406, 4761, 281, 312, 445, 1190, 490, 1192, 281, 2767, 11, 457, 309, 311, 4761, 281, 312, 1190, 294, 341], "temperature": 0.0, "avg_logprob": -0.14155705769856772, "compression_ratio": 1.8917748917748918, "no_speech_prob": 7.224426781249349e-07}, {"id": 185, "seek": 86400, "start": 882.56, "end": 885.56, "text": " Particular way and I mentioned that because like this can be a handy", "tokens": [4100, 14646, 636, 293, 286, 2835, 300, 570, 411, 341, 393, 312, 257, 13239], "temperature": 0.0, "avg_logprob": -0.14155705769856772, "compression_ratio": 1.8917748917748918, "no_speech_prob": 7.224426781249349e-07}, {"id": 186, "seek": 86400, "start": 886.4, "end": 891.32, "text": " Trick to know like you could of course put all the stuff underneath in a function", "tokens": [43367, 281, 458, 411, 291, 727, 295, 1164, 829, 439, 264, 1507, 7223, 294, 257, 2445], "temperature": 0.0, "avg_logprob": -0.14155705769856772, "compression_ratio": 1.8917748917748918, "no_speech_prob": 7.224426781249349e-07}, {"id": 187, "seek": 89132, "start": 891.32, "end": 896.2, "text": " That you pass a data frame to and call it once with a test set once with a training set", "tokens": [663, 291, 1320, 257, 1412, 3920, 281, 293, 818, 309, 1564, 365, 257, 1500, 992, 1564, 365, 257, 3097, 992], "temperature": 0.0, "avg_logprob": -0.15902345445421007, "compression_ratio": 1.6255924170616114, "no_speech_prob": 5.896407060390629e-07}, {"id": 188, "seek": 89132, "start": 896.9200000000001, "end": 898.9200000000001, "text": " but I kind of like to", "tokens": [457, 286, 733, 295, 411, 281], "temperature": 0.0, "avg_logprob": -0.15902345445421007, "compression_ratio": 1.6255924170616114, "no_speech_prob": 5.896407060390629e-07}, {"id": 189, "seek": 89132, "start": 899.36, "end": 906.0, "text": " Experiment a bit more interactively look at each step as I go so this way is an easy way to kind of run something on two", "tokens": [37933, 257, 857, 544, 4648, 3413, 574, 412, 1184, 1823, 382, 286, 352, 370, 341, 636, 307, 364, 1858, 636, 281, 733, 295, 1190, 746, 322, 732], "temperature": 0.0, "avg_logprob": -0.15902345445421007, "compression_ratio": 1.6255924170616114, "no_speech_prob": 5.896407060390629e-07}, {"id": 190, "seek": 89132, "start": 906.0, "end": 909.0400000000001, "text": " different data frames without turning it into a function", "tokens": [819, 1412, 12083, 1553, 6246, 309, 666, 257, 2445], "temperature": 0.0, "avg_logprob": -0.15902345445421007, "compression_ratio": 1.6255924170616114, "no_speech_prob": 5.896407060390629e-07}, {"id": 191, "seek": 89132, "start": 910.48, "end": 912.08, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.15902345445421007, "compression_ratio": 1.6255924170616114, "no_speech_prob": 5.896407060390629e-07}, {"id": 192, "seek": 89132, "start": 912.08, "end": 917.0200000000001, "text": " So this is going to if I sort by store and by date", "tokens": [407, 341, 307, 516, 281, 498, 286, 1333, 538, 3531, 293, 538, 4002], "temperature": 0.0, "avg_logprob": -0.15902345445421007, "compression_ratio": 1.6255924170616114, "no_speech_prob": 5.896407060390629e-07}, {"id": 193, "seek": 91702, "start": 917.02, "end": 920.62, "text": " Then this is keeping track of the last time something happened", "tokens": [1396, 341, 307, 5145, 2837, 295, 264, 1036, 565, 746, 2011], "temperature": 0.0, "avg_logprob": -0.2156949204005552, "compression_ratio": 1.6635071090047393, "no_speech_prob": 6.681500508420868e-07}, {"id": 194, "seek": 91702, "start": 920.62, "end": 925.3, "text": " And so this is therefore going to end up telling me how many days was it since the last?", "tokens": [400, 370, 341, 307, 4412, 516, 281, 917, 493, 3585, 385, 577, 867, 1708, 390, 309, 1670, 264, 1036, 30], "temperature": 0.0, "avg_logprob": -0.2156949204005552, "compression_ratio": 1.6635071090047393, "no_speech_prob": 6.681500508420868e-07}, {"id": 195, "seek": 91702, "start": 925.9399999999999, "end": 929.22, "text": " School holiday right so now if I sort", "tokens": [5070, 9960, 558, 370, 586, 498, 286, 1333], "temperature": 0.0, "avg_logprob": -0.2156949204005552, "compression_ratio": 1.6635071090047393, "no_speech_prob": 6.681500508420868e-07}, {"id": 196, "seek": 91702, "start": 930.14, "end": 932.14, "text": " date descending", "tokens": [4002, 40182], "temperature": 0.0, "avg_logprob": -0.2156949204005552, "compression_ratio": 1.6635071090047393, "no_speech_prob": 6.681500508420868e-07}, {"id": 197, "seek": 91702, "start": 932.9399999999999, "end": 934.9399999999999, "text": " And call the exact same function", "tokens": [400, 818, 264, 1900, 912, 2445], "temperature": 0.0, "avg_logprob": -0.2156949204005552, "compression_ratio": 1.6635071090047393, "no_speech_prob": 6.681500508420868e-07}, {"id": 198, "seek": 91702, "start": 935.86, "end": 940.34, "text": " Then it's going to say how long until the next school holiday", "tokens": [1396, 309, 311, 516, 281, 584, 577, 938, 1826, 264, 958, 1395, 9960], "temperature": 0.0, "avg_logprob": -0.2156949204005552, "compression_ratio": 1.6635071090047393, "no_speech_prob": 6.681500508420868e-07}, {"id": 199, "seek": 91702, "start": 940.6999999999999, "end": 943.74, "text": " right, so that's a kind of a nice little trick for", "tokens": [558, 11, 370, 300, 311, 257, 733, 295, 257, 1481, 707, 4282, 337], "temperature": 0.0, "avg_logprob": -0.2156949204005552, "compression_ratio": 1.6635071090047393, "no_speech_prob": 6.681500508420868e-07}, {"id": 200, "seek": 94374, "start": 943.74, "end": 949.58, "text": " Adding these kind of event timers arbitrary event timers into your time series models", "tokens": [31204, 613, 733, 295, 2280, 524, 433, 23211, 2280, 524, 433, 666, 428, 565, 2638, 5245], "temperature": 0.0, "avg_logprob": -0.18232618619317878, "compression_ratio": 1.5517241379310345, "no_speech_prob": 1.228909468409256e-06}, {"id": 201, "seek": 94374, "start": 950.02, "end": 954.38, "text": " That so if you're doing for example the Ecuadorian groceries competition right now", "tokens": [663, 370, 498, 291, 434, 884, 337, 1365, 264, 41558, 952, 31391, 6211, 558, 586], "temperature": 0.0, "avg_logprob": -0.18232618619317878, "compression_ratio": 1.5517241379310345, "no_speech_prob": 1.228909468409256e-06}, {"id": 202, "seek": 94374, "start": 954.7, "end": 960.24, "text": " You know maybe this kind of approach would be useful for various events in that as well", "tokens": [509, 458, 1310, 341, 733, 295, 3109, 576, 312, 4420, 337, 3683, 3931, 294, 300, 382, 731], "temperature": 0.0, "avg_logprob": -0.18232618619317878, "compression_ratio": 1.5517241379310345, "no_speech_prob": 1.228909468409256e-06}, {"id": 203, "seek": 94374, "start": 962.78, "end": 966.22, "text": " Do it for state holiday do it for promo there we go, okay?", "tokens": [1144, 309, 337, 1785, 9960, 360, 309, 337, 26750, 456, 321, 352, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.18232618619317878, "compression_ratio": 1.5517241379310345, "no_speech_prob": 1.228909468409256e-06}, {"id": 204, "seek": 96622, "start": 966.22, "end": 976.22, "text": " The next thing that we look at here is rolling functions", "tokens": [440, 958, 551, 300, 321, 574, 412, 510, 307, 9439, 6828], "temperature": 0.0, "avg_logprob": -0.17303791046142578, "compression_ratio": 1.6696428571428572, "no_speech_prob": 2.8572799237736035e-06}, {"id": 205, "seek": 96622, "start": 978.7, "end": 986.22, "text": " So rolling functions is how we rolling in pandas is how we what we call create what we call windowing functions", "tokens": [407, 9439, 6828, 307, 577, 321, 9439, 294, 4565, 296, 307, 577, 321, 437, 321, 818, 1884, 437, 321, 818, 4910, 278, 6828], "temperature": 0.0, "avg_logprob": -0.17303791046142578, "compression_ratio": 1.6696428571428572, "no_speech_prob": 2.8572799237736035e-06}, {"id": 206, "seek": 96622, "start": 988.0600000000001, "end": 990.0600000000001, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.17303791046142578, "compression_ratio": 1.6696428571428572, "no_speech_prob": 2.8572799237736035e-06}, {"id": 207, "seek": 96622, "start": 992.0600000000001, "end": 994.0600000000001, "text": " Let's say I had", "tokens": [961, 311, 584, 286, 632], "temperature": 0.0, "avg_logprob": -0.17303791046142578, "compression_ratio": 1.6696428571428572, "no_speech_prob": 2.8572799237736035e-06}, {"id": 208, "seek": 99406, "start": 994.06, "end": 996.06, "text": " some data", "tokens": [512, 1412], "temperature": 0.0, "avg_logprob": -0.2764139175415039, "compression_ratio": 1.4850746268656716, "no_speech_prob": 3.1381307508127065e-06}, {"id": 209, "seek": 99406, "start": 1000.9799999999999, "end": 1002.9799999999999, "text": " You know something like this", "tokens": [509, 458, 746, 411, 341], "temperature": 0.0, "avg_logprob": -0.2764139175415039, "compression_ratio": 1.4850746268656716, "no_speech_prob": 3.1381307508127065e-06}, {"id": 210, "seek": 99406, "start": 1004.6999999999999, "end": 1006.6999999999999, "text": " right and", "tokens": [558, 293], "temperature": 0.0, "avg_logprob": -0.2764139175415039, "compression_ratio": 1.4850746268656716, "no_speech_prob": 3.1381307508127065e-06}, {"id": 211, "seek": 99406, "start": 1007.3399999999999, "end": 1013.04, "text": " This is like date and I don't know this is like sales or whatever", "tokens": [639, 307, 411, 4002, 293, 286, 500, 380, 458, 341, 307, 411, 5763, 420, 2035], "temperature": 0.0, "avg_logprob": -0.2764139175415039, "compression_ratio": 1.4850746268656716, "no_speech_prob": 3.1381307508127065e-06}, {"id": 212, "seek": 99406, "start": 1015.02, "end": 1021.5799999999999, "text": " What I could do is I could say like okay, let's create a window around this point of", "tokens": [708, 286, 727, 360, 307, 286, 727, 584, 411, 1392, 11, 718, 311, 1884, 257, 4910, 926, 341, 935, 295], "temperature": 0.0, "avg_logprob": -0.2764139175415039, "compression_ratio": 1.4850746268656716, "no_speech_prob": 3.1381307508127065e-06}, {"id": 213, "seek": 102158, "start": 1021.58, "end": 1025.8600000000001, "text": " like seven days right so it'd be like okay, this is a", "tokens": [411, 3407, 1708, 558, 370, 309, 1116, 312, 411, 1392, 11, 341, 307, 257], "temperature": 0.0, "avg_logprob": -0.2089228136786099, "compression_ratio": 1.8907103825136613, "no_speech_prob": 7.934457926239702e-07}, {"id": 214, "seek": 102158, "start": 1028.7, "end": 1032.6200000000001, "text": " Seven day window say right and so then I could take the average", "tokens": [14868, 786, 4910, 584, 558, 293, 370, 550, 286, 727, 747, 264, 4274], "temperature": 0.0, "avg_logprob": -0.2089228136786099, "compression_ratio": 1.8907103825136613, "no_speech_prob": 7.934457926239702e-07}, {"id": 215, "seek": 102158, "start": 1033.42, "end": 1039.38, "text": " Sales in that seven day window and then I could like do the same thing like I don't know over here", "tokens": [23467, 294, 300, 3407, 786, 4910, 293, 550, 286, 727, 411, 360, 264, 912, 551, 411, 286, 500, 380, 458, 670, 510], "temperature": 0.0, "avg_logprob": -0.2089228136786099, "compression_ratio": 1.8907103825136613, "no_speech_prob": 7.934457926239702e-07}, {"id": 216, "seek": 102158, "start": 1040.06, "end": 1042.06, "text": " All right take the average sales", "tokens": [1057, 558, 747, 264, 4274, 5763], "temperature": 0.0, "avg_logprob": -0.2089228136786099, "compression_ratio": 1.8907103825136613, "no_speech_prob": 7.934457926239702e-07}, {"id": 217, "seek": 102158, "start": 1043.66, "end": 1049.94, "text": " Over that seven day window right and so if we do that for every point and join up those averages", "tokens": [4886, 300, 3407, 786, 4910, 558, 293, 370, 498, 321, 360, 300, 337, 633, 935, 293, 3917, 493, 729, 42257], "temperature": 0.0, "avg_logprob": -0.2089228136786099, "compression_ratio": 1.8907103825136613, "no_speech_prob": 7.934457926239702e-07}, {"id": 218, "seek": 104994, "start": 1049.94, "end": 1051.94, "text": " You're going to end up with a", "tokens": [509, 434, 516, 281, 917, 493, 365, 257], "temperature": 0.0, "avg_logprob": -0.22044364384242465, "compression_ratio": 1.6420454545454546, "no_speech_prob": 1.482350171500002e-06}, {"id": 219, "seek": 104994, "start": 1053.5800000000002, "end": 1060.1000000000001, "text": " Moving average okay, so the kind of the the more generic version of the moving average", "tokens": [14242, 4274, 1392, 11, 370, 264, 733, 295, 264, 264, 544, 19577, 3037, 295, 264, 2684, 4274], "temperature": 0.0, "avg_logprob": -0.22044364384242465, "compression_ratio": 1.6420454545454546, "no_speech_prob": 1.482350171500002e-06}, {"id": 220, "seek": 104994, "start": 1062.42, "end": 1064.42, "text": " Is a window function", "tokens": [1119, 257, 4910, 2445], "temperature": 0.0, "avg_logprob": -0.22044364384242465, "compression_ratio": 1.6420454545454546, "no_speech_prob": 1.482350171500002e-06}, {"id": 221, "seek": 104994, "start": 1064.94, "end": 1070.96, "text": " Hi something where you apply to some function to some window of data around each point", "tokens": [2421, 746, 689, 291, 3079, 281, 512, 2445, 281, 512, 4910, 295, 1412, 926, 1184, 935], "temperature": 0.0, "avg_logprob": -0.22044364384242465, "compression_ratio": 1.6420454545454546, "no_speech_prob": 1.482350171500002e-06}, {"id": 222, "seek": 104994, "start": 1071.98, "end": 1076.3, "text": " Now very often the windows that I've shown here are not actually", "tokens": [823, 588, 2049, 264, 9309, 300, 286, 600, 4898, 510, 366, 406, 767], "temperature": 0.0, "avg_logprob": -0.22044364384242465, "compression_ratio": 1.6420454545454546, "no_speech_prob": 1.482350171500002e-06}, {"id": 223, "seek": 107630, "start": 1076.3, "end": 1083.34, "text": " What you want if you're trying to build a predictive model you can't include the future as part of a moving average", "tokens": [708, 291, 528, 498, 291, 434, 1382, 281, 1322, 257, 35521, 2316, 291, 393, 380, 4090, 264, 2027, 382, 644, 295, 257, 2684, 4274], "temperature": 0.0, "avg_logprob": -0.19547414022778709, "compression_ratio": 1.4764705882352942, "no_speech_prob": 7.93447895830468e-07}, {"id": 224, "seek": 107630, "start": 1083.86, "end": 1086.94, "text": " Right so quite often you actually need a window", "tokens": [1779, 370, 1596, 2049, 291, 767, 643, 257, 4910], "temperature": 0.0, "avg_logprob": -0.19547414022778709, "compression_ratio": 1.4764705882352942, "no_speech_prob": 7.93447895830468e-07}, {"id": 225, "seek": 107630, "start": 1088.18, "end": 1090.18, "text": " That ends here", "tokens": [663, 5314, 510], "temperature": 0.0, "avg_logprob": -0.19547414022778709, "compression_ratio": 1.4764705882352942, "no_speech_prob": 7.93447895830468e-07}, {"id": 226, "seek": 107630, "start": 1091.5, "end": 1095.18, "text": " So that would be our window function right and so", "tokens": [407, 300, 576, 312, 527, 4910, 2445, 558, 293, 370], "temperature": 0.0, "avg_logprob": -0.19547414022778709, "compression_ratio": 1.4764705882352942, "no_speech_prob": 7.93447895830468e-07}, {"id": 227, "seek": 107630, "start": 1097.86, "end": 1099.86, "text": " Pandas lets you create", "tokens": [16995, 296, 6653, 291, 1884], "temperature": 0.0, "avg_logprob": -0.19547414022778709, "compression_ratio": 1.4764705882352942, "no_speech_prob": 7.93447895830468e-07}, {"id": 228, "seek": 109986, "start": 1099.86, "end": 1106.6999999999998, "text": " Window func arbitrary window functions using this rolling here this here says how many?", "tokens": [44933, 1019, 66, 23211, 4910, 6828, 1228, 341, 9439, 510, 341, 510, 1619, 577, 867, 30], "temperature": 0.0, "avg_logprob": -0.21762119250351122, "compression_ratio": 1.6534653465346534, "no_speech_prob": 1.553491074446356e-06}, {"id": 229, "seek": 109986, "start": 1107.2199999999998, "end": 1110.62, "text": " Time steps do you do I want to apply the function to?", "tokens": [6161, 4439, 360, 291, 360, 286, 528, 281, 3079, 264, 2445, 281, 30], "temperature": 0.0, "avg_logprob": -0.21762119250351122, "compression_ratio": 1.6534653465346534, "no_speech_prob": 1.553491074446356e-06}, {"id": 230, "seek": 109986, "start": 1111.3, "end": 1113.74, "text": " Okay, this here says if I'm at the edge", "tokens": [1033, 11, 341, 510, 1619, 498, 286, 478, 412, 264, 4691], "temperature": 0.0, "avg_logprob": -0.21762119250351122, "compression_ratio": 1.6534653465346534, "no_speech_prob": 1.553491074446356e-06}, {"id": 231, "seek": 109986, "start": 1114.5, "end": 1116.5, "text": " So in other words if I'm like out here", "tokens": [407, 294, 661, 2283, 498, 286, 478, 411, 484, 510], "temperature": 0.0, "avg_logprob": -0.21762119250351122, "compression_ratio": 1.6534653465346534, "no_speech_prob": 1.553491074446356e-06}, {"id": 232, "seek": 109986, "start": 1117.6999999999998, "end": 1119.6999999999998, "text": " Should you have should you make that a?", "tokens": [6454, 291, 362, 820, 291, 652, 300, 257, 30], "temperature": 0.0, "avg_logprob": -0.21762119250351122, "compression_ratio": 1.6534653465346534, "no_speech_prob": 1.553491074446356e-06}, {"id": 233, "seek": 109986, "start": 1120.54, "end": 1125.6599999999999, "text": " Missing value because I don't have seven days to average over or you know", "tokens": [5275, 278, 2158, 570, 286, 500, 380, 362, 3407, 1708, 281, 4274, 670, 420, 291, 458], "temperature": 0.0, "avg_logprob": -0.21762119250351122, "compression_ratio": 1.6534653465346534, "no_speech_prob": 1.553491074446356e-06}, {"id": 234, "seek": 112566, "start": 1125.66, "end": 1130.22, "text": " How what's the minimum number of time periods to use that so here I said one okay?", "tokens": [1012, 437, 311, 264, 7285, 1230, 295, 565, 13804, 281, 764, 300, 370, 510, 286, 848, 472, 1392, 30], "temperature": 0.0, "avg_logprob": -0.1891434457567003, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.3496970723281265e-06}, {"id": 235, "seek": 112566, "start": 1130.22, "end": 1137.42, "text": " And then optionally you can also say do you want to set the window at the start of a period or the end of a?", "tokens": [400, 550, 3614, 379, 291, 393, 611, 584, 360, 291, 528, 281, 992, 264, 4910, 412, 264, 722, 295, 257, 2896, 420, 264, 917, 295, 257, 30], "temperature": 0.0, "avg_logprob": -0.1891434457567003, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.3496970723281265e-06}, {"id": 236, "seek": 112566, "start": 1137.42, "end": 1144.64, "text": " Period or the middle if the period okay, so and then within that you can then apply whatever function you like", "tokens": [34976, 420, 264, 2808, 498, 264, 2896, 1392, 11, 370, 293, 550, 1951, 300, 291, 393, 550, 3079, 2035, 2445, 291, 411], "temperature": 0.0, "avg_logprob": -0.1891434457567003, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.3496970723281265e-06}, {"id": 237, "seek": 112566, "start": 1145.1000000000001, "end": 1147.1000000000001, "text": " Okay, so here. I've got my", "tokens": [1033, 11, 370, 510, 13, 286, 600, 658, 452], "temperature": 0.0, "avg_logprob": -0.1891434457567003, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.3496970723281265e-06}, {"id": 238, "seek": 112566, "start": 1147.18, "end": 1148.5800000000002, "text": " weekly", "tokens": [12460], "temperature": 0.0, "avg_logprob": -0.1891434457567003, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.3496970723281265e-06}, {"id": 239, "seek": 112566, "start": 1148.5800000000002, "end": 1150.5800000000002, "text": " by store", "tokens": [538, 3531], "temperature": 0.0, "avg_logprob": -0.1891434457567003, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.3496970723281265e-06}, {"id": 240, "seek": 112566, "start": 1150.78, "end": 1152.16, "text": " sums", "tokens": [34499], "temperature": 0.0, "avg_logprob": -0.1891434457567003, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.3496970723281265e-06}, {"id": 241, "seek": 112566, "start": 1152.16, "end": 1154.38, "text": " Okay, so there's a nice easy way", "tokens": [1033, 11, 370, 456, 311, 257, 1481, 1858, 636], "temperature": 0.0, "avg_logprob": -0.1891434457567003, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.3496970723281265e-06}, {"id": 242, "seek": 115438, "start": 1154.38, "end": 1156.38, "text": " of getting", "tokens": [295, 1242], "temperature": 0.0, "avg_logprob": -0.2459606071571251, "compression_ratio": 1.6428571428571428, "no_speech_prob": 5.014703219785588e-06}, {"id": 243, "seek": 115438, "start": 1157.6200000000001, "end": 1162.66, "text": " Kind of moving averages or or whatever else and you know I should mention in pandas", "tokens": [9242, 295, 2684, 42257, 420, 420, 2035, 1646, 293, 291, 458, 286, 820, 2152, 294, 4565, 296], "temperature": 0.0, "avg_logprob": -0.2459606071571251, "compression_ratio": 1.6428571428571428, "no_speech_prob": 5.014703219785588e-06}, {"id": 244, "seek": 115438, "start": 1165.7, "end": 1170.0600000000002, "text": " If you go to the time series page on pandas there's literally", "tokens": [759, 291, 352, 281, 264, 565, 2638, 3028, 322, 4565, 296, 456, 311, 3736], "temperature": 0.0, "avg_logprob": -0.2459606071571251, "compression_ratio": 1.6428571428571428, "no_speech_prob": 5.014703219785588e-06}, {"id": 245, "seek": 115438, "start": 1171.0200000000002, "end": 1176.18, "text": " like look at just the index here time series functionality was all of", "tokens": [411, 574, 412, 445, 264, 8186, 510, 565, 2638, 14980, 390, 439, 295], "temperature": 0.0, "avg_logprob": -0.2459606071571251, "compression_ratio": 1.6428571428571428, "no_speech_prob": 5.014703219785588e-06}, {"id": 246, "seek": 117618, "start": 1176.18, "end": 1184.0600000000002, "text": " This is this like there's lots because like where's bikini who created this he was originally", "tokens": [639, 307, 341, 411, 456, 311, 3195, 570, 411, 689, 311, 26730, 3812, 567, 2942, 341, 415, 390, 7993], "temperature": 0.0, "avg_logprob": -0.20524802415267282, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.0511446362215793e-06}, {"id": 247, "seek": 117618, "start": 1184.78, "end": 1187.18, "text": " in hedge fund trading I believe and", "tokens": [294, 25304, 2374, 9529, 286, 1697, 293], "temperature": 0.0, "avg_logprob": -0.20524802415267282, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.0511446362215793e-06}, {"id": 248, "seek": 117618, "start": 1187.5, "end": 1190.3400000000001, "text": " You know his work was all about time series", "tokens": [509, 458, 702, 589, 390, 439, 466, 565, 2638], "temperature": 0.0, "avg_logprob": -0.20524802415267282, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.0511446362215793e-06}, {"id": 249, "seek": 117618, "start": 1190.3400000000001, "end": 1195.1200000000001, "text": " And so I think like pandas originally was very focused on time series and still", "tokens": [400, 370, 286, 519, 411, 4565, 296, 7993, 390, 588, 5178, 322, 565, 2638, 293, 920], "temperature": 0.0, "avg_logprob": -0.20524802415267282, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.0511446362215793e-06}, {"id": 250, "seek": 117618, "start": 1195.54, "end": 1198.04, "text": " You know it's perhaps the strongest part of pandas", "tokens": [509, 458, 309, 311, 4317, 264, 16595, 644, 295, 4565, 296], "temperature": 0.0, "avg_logprob": -0.20524802415267282, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.0511446362215793e-06}, {"id": 251, "seek": 117618, "start": 1198.26, "end": 1202.22, "text": " So if you're playing like if you're playing around with time series computations", "tokens": [407, 498, 291, 434, 2433, 411, 498, 291, 434, 2433, 926, 365, 565, 2638, 2807, 763], "temperature": 0.0, "avg_logprob": -0.20524802415267282, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.0511446362215793e-06}, {"id": 252, "seek": 120222, "start": 1202.22, "end": 1206.5, "text": " You definitely owe it to yourself to try to learn this entire", "tokens": [509, 2138, 16655, 309, 281, 1803, 281, 853, 281, 1466, 341, 2302], "temperature": 0.0, "avg_logprob": -0.18194949870206872, "compression_ratio": 1.6864406779661016, "no_speech_prob": 4.5921217406430515e-07}, {"id": 253, "seek": 120222, "start": 1207.58, "end": 1213.26, "text": " API and like it there's a lot of kind of conceptual pieces around like", "tokens": [9362, 293, 411, 309, 456, 311, 257, 688, 295, 733, 295, 24106, 3755, 926, 411], "temperature": 0.0, "avg_logprob": -0.18194949870206872, "compression_ratio": 1.6864406779661016, "no_speech_prob": 4.5921217406430515e-07}, {"id": 254, "seek": 120222, "start": 1214.94, "end": 1221.38, "text": " Timestamps and date offsets and resampling and stuff like that to kind of get your head around", "tokens": [7172, 377, 23150, 293, 4002, 39457, 1385, 293, 725, 335, 11970, 293, 1507, 411, 300, 281, 733, 295, 483, 428, 1378, 926], "temperature": 0.0, "avg_logprob": -0.18194949870206872, "compression_ratio": 1.6864406779661016, "no_speech_prob": 4.5921217406430515e-07}, {"id": 255, "seek": 120222, "start": 1221.38, "end": 1223.38, "text": " but it's totally worth it because", "tokens": [457, 309, 311, 3879, 3163, 309, 570], "temperature": 0.0, "avg_logprob": -0.18194949870206872, "compression_ratio": 1.6864406779661016, "no_speech_prob": 4.5921217406430515e-07}, {"id": 256, "seek": 120222, "start": 1223.78, "end": 1227.38, "text": " Otherwise you'll be writing this stuff as loops by hand", "tokens": [10328, 291, 603, 312, 3579, 341, 1507, 382, 16121, 538, 1011], "temperature": 0.0, "avg_logprob": -0.18194949870206872, "compression_ratio": 1.6864406779661016, "no_speech_prob": 4.5921217406430515e-07}, {"id": 257, "seek": 120222, "start": 1227.38, "end": 1231.26, "text": " it's going to take you a lot longer than leveraging what pandas already does and", "tokens": [309, 311, 516, 281, 747, 291, 257, 688, 2854, 813, 32666, 437, 4565, 296, 1217, 775, 293], "temperature": 0.0, "avg_logprob": -0.18194949870206872, "compression_ratio": 1.6864406779661016, "no_speech_prob": 4.5921217406430515e-07}, {"id": 258, "seek": 123126, "start": 1231.26, "end": 1235.34, "text": " And of course pandas will do it in you know highly optimized", "tokens": [400, 295, 1164, 4565, 296, 486, 360, 309, 294, 291, 458, 5405, 26941], "temperature": 0.0, "avg_logprob": -0.2501830810155624, "compression_ratio": 1.643979057591623, "no_speech_prob": 7.071814707160229e-06}, {"id": 259, "seek": 123126, "start": 1235.98, "end": 1240.58, "text": " C-code for you vectorized C code where else your version is going to loop in Python", "tokens": [383, 12, 22332, 337, 291, 8062, 1602, 383, 3089, 689, 1646, 428, 3037, 307, 516, 281, 6367, 294, 15329], "temperature": 0.0, "avg_logprob": -0.2501830810155624, "compression_ratio": 1.643979057591623, "no_speech_prob": 7.071814707160229e-06}, {"id": 260, "seek": 123126, "start": 1240.62, "end": 1245.78, "text": " so definitely worth you know if you're doing stuff in time series learning the", "tokens": [370, 2138, 3163, 291, 458, 498, 291, 434, 884, 1507, 294, 565, 2638, 2539, 264], "temperature": 0.0, "avg_logprob": -0.2501830810155624, "compression_ratio": 1.643979057591623, "no_speech_prob": 7.071814707160229e-06}, {"id": 261, "seek": 123126, "start": 1246.42, "end": 1248.5, "text": " full pandas time series API", "tokens": [1577, 4565, 296, 565, 2638, 9362], "temperature": 0.0, "avg_logprob": -0.2501830810155624, "compression_ratio": 1.643979057591623, "no_speech_prob": 7.071814707160229e-06}, {"id": 262, "seek": 123126, "start": 1249.78, "end": 1254.08, "text": " Just about as about as strong as any time series API out there", "tokens": [1449, 466, 382, 466, 382, 2068, 382, 604, 565, 2638, 9362, 484, 456], "temperature": 0.0, "avg_logprob": -0.2501830810155624, "compression_ratio": 1.643979057591623, "no_speech_prob": 7.071814707160229e-06}, {"id": 263, "seek": 125408, "start": 1254.08, "end": 1261.9199999999998, "text": " Okay, so at the end of all that you can see here's those kind of starting point values I mentioned", "tokens": [1033, 11, 370, 412, 264, 917, 295, 439, 300, 291, 393, 536, 510, 311, 729, 733, 295, 2891, 935, 4190, 286, 2835], "temperature": 0.0, "avg_logprob": -0.18235570956499147, "compression_ratio": 1.5129533678756477, "no_speech_prob": 5.594268714048667e-06}, {"id": 264, "seek": 125408, "start": 1263.28, "end": 1265.28, "text": " slightly on the extreme side", "tokens": [4748, 322, 264, 8084, 1252], "temperature": 0.0, "avg_logprob": -0.18235570956499147, "compression_ratio": 1.5129533678756477, "no_speech_prob": 5.594268714048667e-06}, {"id": 265, "seek": 125408, "start": 1265.76, "end": 1267.76, "text": " and so you can see here the", "tokens": [293, 370, 291, 393, 536, 510, 264], "temperature": 0.0, "avg_logprob": -0.18235570956499147, "compression_ratio": 1.5129533678756477, "no_speech_prob": 5.594268714048667e-06}, {"id": 266, "seek": 125408, "start": 1268.1599999999999, "end": 1270.1599999999999, "text": " 17th of September", "tokens": [3282, 392, 295, 7216], "temperature": 0.0, "avg_logprob": -0.18235570956499147, "compression_ratio": 1.5129533678756477, "no_speech_prob": 5.594268714048667e-06}, {"id": 267, "seek": 125408, "start": 1270.4399999999998, "end": 1278.1599999999999, "text": " Store one was 13 days after the last school holiday the 16th was 12 11 10 so forth okay", "tokens": [17242, 472, 390, 3705, 1708, 934, 264, 1036, 1395, 9960, 264, 3165, 392, 390, 2272, 2975, 1266, 370, 5220, 1392], "temperature": 0.0, "avg_logprob": -0.18235570956499147, "compression_ratio": 1.5129533678756477, "no_speech_prob": 5.594268714048667e-06}, {"id": 268, "seek": 125408, "start": 1278.6399999999999, "end": 1280.6399999999999, "text": " We're currently in a promotion", "tokens": [492, 434, 4362, 294, 257, 15783], "temperature": 0.0, "avg_logprob": -0.18235570956499147, "compression_ratio": 1.5129533678756477, "no_speech_prob": 5.594268714048667e-06}, {"id": 269, "seek": 128064, "start": 1280.64, "end": 1288.46, "text": " All right here. This is one day before the promotion here. We've got nine days after the last promotion", "tokens": [1057, 558, 510, 13, 639, 307, 472, 786, 949, 264, 15783, 510, 13, 492, 600, 658, 4949, 1708, 934, 264, 1036, 15783], "temperature": 0.0, "avg_logprob": -0.16749095916748047, "compression_ratio": 1.5087719298245614, "no_speech_prob": 1.1365602858859347e-06}, {"id": 270, "seek": 128064, "start": 1289.16, "end": 1291.16, "text": " And so forth okay", "tokens": [400, 370, 5220, 1392], "temperature": 0.0, "avg_logprob": -0.16749095916748047, "compression_ratio": 1.5087719298245614, "no_speech_prob": 1.1365602858859347e-06}, {"id": 271, "seek": 128064, "start": 1292.24, "end": 1294.24, "text": " So that's how we can add", "tokens": [407, 300, 311, 577, 321, 393, 909], "temperature": 0.0, "avg_logprob": -0.16749095916748047, "compression_ratio": 1.5087719298245614, "no_speech_prob": 1.1365602858859347e-06}, {"id": 272, "seek": 128064, "start": 1295.44, "end": 1303.5600000000002, "text": " Kind of event counters to a time series and probably always a good idea when you're doing work with time series", "tokens": [9242, 295, 2280, 39338, 281, 257, 565, 2638, 293, 1391, 1009, 257, 665, 1558, 562, 291, 434, 884, 589, 365, 565, 2638], "temperature": 0.0, "avg_logprob": -0.16749095916748047, "compression_ratio": 1.5087719298245614, "no_speech_prob": 1.1365602858859347e-06}, {"id": 273, "seek": 130356, "start": 1303.56, "end": 1311.28, "text": " So now that we've done that you know we've got lots of columns in our data set", "tokens": [407, 586, 300, 321, 600, 1096, 300, 291, 458, 321, 600, 658, 3195, 295, 13766, 294, 527, 1412, 992], "temperature": 0.0, "avg_logprob": -0.14197747906049094, "compression_ratio": 1.7130044843049328, "no_speech_prob": 7.571125593131001e-07}, {"id": 274, "seek": 130356, "start": 1312.08, "end": 1316.36, "text": " And so we split them out into categorical versus continuous columns", "tokens": [400, 370, 321, 7472, 552, 484, 666, 19250, 804, 5717, 10957, 13766], "temperature": 0.0, "avg_logprob": -0.14197747906049094, "compression_ratio": 1.7130044843049328, "no_speech_prob": 7.571125593131001e-07}, {"id": 275, "seek": 130356, "start": 1317.0, "end": 1320.82, "text": " We'll talk more about that in a moment in the review section", "tokens": [492, 603, 751, 544, 466, 300, 294, 257, 1623, 294, 264, 3131, 3541], "temperature": 0.0, "avg_logprob": -0.14197747906049094, "compression_ratio": 1.7130044843049328, "no_speech_prob": 7.571125593131001e-07}, {"id": 276, "seek": 130356, "start": 1320.96, "end": 1323.9199999999998, "text": " But so these are going to be all the things I'm going to create an embedding for", "tokens": [583, 370, 613, 366, 516, 281, 312, 439, 264, 721, 286, 478, 516, 281, 1884, 364, 12240, 3584, 337], "temperature": 0.0, "avg_logprob": -0.14197747906049094, "compression_ratio": 1.7130044843049328, "no_speech_prob": 7.571125593131001e-07}, {"id": 277, "seek": 130356, "start": 1324.3999999999999, "end": 1331.12, "text": " Okay, and these are all of the things that I'm going to feed directly into the into the model", "tokens": [1033, 11, 293, 613, 366, 439, 295, 264, 721, 300, 286, 478, 516, 281, 3154, 3838, 666, 264, 666, 264, 2316], "temperature": 0.0, "avg_logprob": -0.14197747906049094, "compression_ratio": 1.7130044843049328, "no_speech_prob": 7.571125593131001e-07}, {"id": 278, "seek": 133112, "start": 1331.12, "end": 1333.6, "text": " so for example, we've got like", "tokens": [370, 337, 1365, 11, 321, 600, 658, 411], "temperature": 0.0, "avg_logprob": -0.3067763581567881, "compression_ratio": 1.564516129032258, "no_speech_prob": 2.601602773211198e-06}, {"id": 279, "seek": 133112, "start": 1334.8799999999999, "end": 1337.4399999999998, "text": " Competition distance so that's distance to the nearest competitor", "tokens": [43634, 4560, 370, 300, 311, 4560, 281, 264, 23831, 27266], "temperature": 0.0, "avg_logprob": -0.3067763581567881, "compression_ratio": 1.564516129032258, "no_speech_prob": 2.601602773211198e-06}, {"id": 280, "seek": 133112, "start": 1338.4399999999998, "end": 1340.4399999999998, "text": " maximum temperature and", "tokens": [6674, 4292, 293], "temperature": 0.0, "avg_logprob": -0.3067763581567881, "compression_ratio": 1.564516129032258, "no_speech_prob": 2.601602773211198e-06}, {"id": 281, "seek": 133112, "start": 1340.6, "end": 1343.6799999999998, "text": " Here we've got day of week right so", "tokens": [1692, 321, 600, 658, 786, 295, 1243, 558, 370], "temperature": 0.0, "avg_logprob": -0.3067763581567881, "compression_ratio": 1.564516129032258, "no_speech_prob": 2.601602773211198e-06}, {"id": 282, "seek": 133112, "start": 1350.8799999999999, "end": 1354.4399999999998, "text": " So here we've got maximum temperature", "tokens": [407, 510, 321, 600, 658, 6674, 4292], "temperature": 0.0, "avg_logprob": -0.3067763581567881, "compression_ratio": 1.564516129032258, "no_speech_prob": 2.601602773211198e-06}, {"id": 283, "seek": 135444, "start": 1354.44, "end": 1360.16, "text": " Maybe it's like 22.1 because they use centigrade in Germany", "tokens": [2704, 309, 311, 411, 5853, 13, 16, 570, 436, 764, 44731, 294, 7244], "temperature": 0.0, "avg_logprob": -0.24287316605851456, "compression_ratio": 1.43, "no_speech_prob": 2.60160118159547e-06}, {"id": 284, "seek": 135444, "start": 1360.68, "end": 1363.48, "text": " We've got distance to nearest competitor might be", "tokens": [492, 600, 658, 4560, 281, 23831, 27266, 1062, 312], "temperature": 0.0, "avg_logprob": -0.24287316605851456, "compression_ratio": 1.43, "no_speech_prob": 2.60160118159547e-06}, {"id": 285, "seek": 135444, "start": 1364.24, "end": 1366.24, "text": " 321 kilometers point seven", "tokens": [805, 4436, 13904, 935, 3407], "temperature": 0.0, "avg_logprob": -0.24287316605851456, "compression_ratio": 1.43, "no_speech_prob": 2.60160118159547e-06}, {"id": 286, "seek": 135444, "start": 1368.28, "end": 1371.76, "text": " Alright, and then we've got day of week", "tokens": [2798, 11, 293, 550, 321, 600, 658, 786, 295, 1243], "temperature": 0.0, "avg_logprob": -0.24287316605851456, "compression_ratio": 1.43, "no_speech_prob": 2.60160118159547e-06}, {"id": 287, "seek": 135444, "start": 1372.96, "end": 1377.1200000000001, "text": " Which might be I don't know maybe Saturday is a six okay?", "tokens": [3013, 1062, 312, 286, 500, 380, 458, 1310, 8803, 307, 257, 2309, 1392, 30], "temperature": 0.0, "avg_logprob": -0.24287316605851456, "compression_ratio": 1.43, "no_speech_prob": 2.60160118159547e-06}, {"id": 288, "seek": 137712, "start": 1377.12, "end": 1383.4399999999998, "text": " So these numbers here are going to go straight into", "tokens": [407, 613, 3547, 510, 366, 516, 281, 352, 2997, 666], "temperature": 0.0, "avg_logprob": -0.2893374252319336, "compression_ratio": 1.4728682170542635, "no_speech_prob": 1.760334157552279e-06}, {"id": 289, "seek": 137712, "start": 1389.6399999999999, "end": 1393.8, "text": " Our vector right the vector that we're going to be feeding into our neural net", "tokens": [2621, 8062, 558, 264, 8062, 300, 321, 434, 516, 281, 312, 12919, 666, 527, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.2893374252319336, "compression_ratio": 1.4728682170542635, "no_speech_prob": 1.760334157552279e-06}, {"id": 290, "seek": 137712, "start": 1395.36, "end": 1397.36, "text": " All right, twenty two one", "tokens": [1057, 558, 11, 7699, 732, 472], "temperature": 0.0, "avg_logprob": -0.2893374252319336, "compression_ratio": 1.4728682170542635, "no_speech_prob": 1.760334157552279e-06}, {"id": 291, "seek": 137712, "start": 1399.56, "end": 1401.56, "text": " Three twenty one point seven", "tokens": [6244, 7699, 472, 935, 3407], "temperature": 0.0, "avg_logprob": -0.2893374252319336, "compression_ratio": 1.4728682170542635, "no_speech_prob": 1.760334157552279e-06}, {"id": 292, "seek": 137712, "start": 1402.32, "end": 1404.08, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2893374252319336, "compression_ratio": 1.4728682170542635, "no_speech_prob": 1.760334157552279e-06}, {"id": 293, "seek": 140408, "start": 1404.08, "end": 1407.32, "text": " We'll see in a moment. We'll actually will normalize them, but more or less", "tokens": [492, 603, 536, 294, 257, 1623, 13, 492, 603, 767, 486, 2710, 1125, 552, 11, 457, 544, 420, 1570], "temperature": 0.0, "avg_logprob": -0.1481060070937939, "compression_ratio": 1.538812785388128, "no_speech_prob": 6.438977379730204e-06}, {"id": 294, "seek": 140408, "start": 1408.72, "end": 1415.84, "text": " But this categorical variable we're not we need to put it through an embedding right so we'll have some embedding matrix", "tokens": [583, 341, 19250, 804, 7006, 321, 434, 406, 321, 643, 281, 829, 309, 807, 364, 12240, 3584, 558, 370, 321, 603, 362, 512, 12240, 3584, 8141], "temperature": 0.0, "avg_logprob": -0.1481060070937939, "compression_ratio": 1.538812785388128, "no_speech_prob": 6.438977379730204e-06}, {"id": 295, "seek": 140408, "start": 1417.48, "end": 1420.08, "text": " Right of if there are seven days", "tokens": [1779, 295, 498, 456, 366, 3407, 1708], "temperature": 0.0, "avg_logprob": -0.1481060070937939, "compression_ratio": 1.538812785388128, "no_speech_prob": 6.438977379730204e-06}, {"id": 296, "seek": 140408, "start": 1421.0, "end": 1427.76, "text": " By I don't know maybe dimension four embedding okay, and so this will look up the sixth row", "tokens": [3146, 286, 500, 380, 458, 1310, 10139, 1451, 12240, 3584, 1392, 11, 293, 370, 341, 486, 574, 493, 264, 15102, 5386], "temperature": 0.0, "avg_logprob": -0.1481060070937939, "compression_ratio": 1.538812785388128, "no_speech_prob": 6.438977379730204e-06}, {"id": 297, "seek": 140408, "start": 1428.4399999999998, "end": 1430.4399999999998, "text": " to get back the", "tokens": [281, 483, 646, 264], "temperature": 0.0, "avg_logprob": -0.1481060070937939, "compression_ratio": 1.538812785388128, "no_speech_prob": 6.438977379730204e-06}, {"id": 298, "seek": 143044, "start": 1430.44, "end": 1434.1200000000001, "text": " Four items right and so this is going to turn into", "tokens": [7451, 4754, 558, 293, 370, 341, 307, 516, 281, 1261, 666], "temperature": 0.0, "avg_logprob": -0.20280485374982968, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.93335199583089e-06}, {"id": 299, "seek": 143044, "start": 1438.6000000000001, "end": 1444.6200000000001, "text": " Length for vector which will then add here", "tokens": [441, 4206, 337, 8062, 597, 486, 550, 909, 510], "temperature": 0.0, "avg_logprob": -0.20280485374982968, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.93335199583089e-06}, {"id": 300, "seek": 144462, "start": 1444.62, "end": 1457.5, "text": " Okay, so that's how our continuous and categorical variables are going to work so", "tokens": [1033, 11, 370, 300, 311, 577, 527, 10957, 293, 19250, 804, 9102, 366, 516, 281, 589, 370], "temperature": 0.0, "avg_logprob": -0.22019320267897385, "compression_ratio": 1.5681818181818181, "no_speech_prob": 2.2252684175327886e-06}, {"id": 301, "seek": 144462, "start": 1461.9399999999998, "end": 1463.9399999999998, "text": " Then all of our", "tokens": [1396, 439, 295, 527], "temperature": 0.0, "avg_logprob": -0.22019320267897385, "compression_ratio": 1.5681818181818181, "no_speech_prob": 2.2252684175327886e-06}, {"id": 302, "seek": 144462, "start": 1464.8999999999999, "end": 1470.58, "text": " Categorical variables will turn them into pandas categorical variables in the same way that we've done before", "tokens": [383, 2968, 284, 804, 9102, 486, 1261, 552, 666, 4565, 296, 19250, 804, 9102, 294, 264, 912, 636, 300, 321, 600, 1096, 949], "temperature": 0.0, "avg_logprob": -0.22019320267897385, "compression_ratio": 1.5681818181818181, "no_speech_prob": 2.2252684175327886e-06}, {"id": 303, "seek": 147058, "start": 1470.58, "end": 1472.58, "text": " And", "tokens": [400], "temperature": 0.0, "avg_logprob": -0.18611021444831097, "compression_ratio": 1.6534090909090908, "no_speech_prob": 2.190773102483945e-06}, {"id": 304, "seek": 147058, "start": 1473.62, "end": 1478.46, "text": " Then we're going to apply the same mappings to the test set right so if", "tokens": [1396, 321, 434, 516, 281, 3079, 264, 912, 463, 28968, 281, 264, 1500, 992, 558, 370, 498], "temperature": 0.0, "avg_logprob": -0.18611021444831097, "compression_ratio": 1.6534090909090908, "no_speech_prob": 2.190773102483945e-06}, {"id": 305, "seek": 147058, "start": 1479.26, "end": 1485.98, "text": " Saturday is a six in the training set this apply cats makes sure that Saturday is also a six in the test set", "tokens": [8803, 307, 257, 2309, 294, 264, 3097, 992, 341, 3079, 11111, 1669, 988, 300, 8803, 307, 611, 257, 2309, 294, 264, 1500, 992], "temperature": 0.0, "avg_logprob": -0.18611021444831097, "compression_ratio": 1.6534090909090908, "no_speech_prob": 2.190773102483945e-06}, {"id": 306, "seek": 147058, "start": 1487.62, "end": 1493.82, "text": " For the continuous variables make sure they're all floats because pi torch expects everything to be afloat", "tokens": [1171, 264, 10957, 9102, 652, 988, 436, 434, 439, 37878, 570, 3895, 27822, 33280, 1203, 281, 312, 3238, 752, 267], "temperature": 0.0, "avg_logprob": -0.18611021444831097, "compression_ratio": 1.6534090909090908, "no_speech_prob": 2.190773102483945e-06}, {"id": 307, "seek": 149382, "start": 1493.82, "end": 1500.74, "text": " So then this is another little trick that I use", "tokens": [407, 550, 341, 307, 1071, 707, 4282, 300, 286, 764], "temperature": 0.0, "avg_logprob": -0.20117446780204773, "compression_ratio": 1.6815286624203822, "no_speech_prob": 2.4579895807619323e-07}, {"id": 308, "seek": 149382, "start": 1502.62, "end": 1510.3799999999999, "text": " Both of these cells define something called joined sample one of them defines them as the whole training set", "tokens": [6767, 295, 613, 5438, 6964, 746, 1219, 6869, 6889, 472, 295, 552, 23122, 552, 382, 264, 1379, 3097, 992], "temperature": 0.0, "avg_logprob": -0.20117446780204773, "compression_ratio": 1.6815286624203822, "no_speech_prob": 2.4579895807619323e-07}, {"id": 309, "seek": 149382, "start": 1511.58, "end": 1514.74, "text": " One of them defines them as a random subset", "tokens": [1485, 295, 552, 23122, 552, 382, 257, 4974, 25993], "temperature": 0.0, "avg_logprob": -0.20117446780204773, "compression_ratio": 1.6815286624203822, "no_speech_prob": 2.4579895807619323e-07}, {"id": 310, "seek": 149382, "start": 1515.34, "end": 1519.1, "text": " Right and so the idea is that I do all of my work on the sample", "tokens": [1779, 293, 370, 264, 1558, 307, 300, 286, 360, 439, 295, 452, 589, 322, 264, 6889], "temperature": 0.0, "avg_logprob": -0.20117446780204773, "compression_ratio": 1.6815286624203822, "no_speech_prob": 2.4579895807619323e-07}, {"id": 311, "seek": 151910, "start": 1519.1, "end": 1525.26, "text": " Make sure it all works well play around with different hyper parameters and architectures, and then when I'm like okay", "tokens": [4387, 988, 309, 439, 1985, 731, 862, 926, 365, 819, 9848, 9834, 293, 6331, 1303, 11, 293, 550, 562, 286, 478, 411, 1392], "temperature": 0.0, "avg_logprob": -0.13871593635623194, "compression_ratio": 1.7798507462686568, "no_speech_prob": 1.4823523315499187e-06}, {"id": 312, "seek": 151910, "start": 1525.26, "end": 1529.3799999999999, "text": " I'm very happy with this I then go back and run this line of code", "tokens": [286, 478, 588, 2055, 365, 341, 286, 550, 352, 646, 293, 1190, 341, 1622, 295, 3089], "temperature": 0.0, "avg_logprob": -0.13871593635623194, "compression_ratio": 1.7798507462686568, "no_speech_prob": 1.4823523315499187e-06}, {"id": 313, "seek": 151910, "start": 1529.9399999999998, "end": 1535.08, "text": " To say okay now make that make the whole data set be the sample and then rerun it", "tokens": [1407, 584, 1392, 586, 652, 300, 652, 264, 1379, 1412, 992, 312, 264, 6889, 293, 550, 43819, 409, 309], "temperature": 0.0, "avg_logprob": -0.13871593635623194, "compression_ratio": 1.7798507462686568, "no_speech_prob": 1.4823523315499187e-06}, {"id": 314, "seek": 151910, "start": 1535.34, "end": 1538.3799999999999, "text": " Okay, so this is a good way again similar to the what I showed you before", "tokens": [1033, 11, 370, 341, 307, 257, 665, 636, 797, 2531, 281, 264, 437, 286, 4712, 291, 949], "temperature": 0.0, "avg_logprob": -0.13871593635623194, "compression_ratio": 1.7798507462686568, "no_speech_prob": 1.4823523315499187e-06}, {"id": 315, "seek": 151910, "start": 1538.6599999999999, "end": 1545.86, "text": " It lets you use the same cells in your notebook to run first of all on the sample and then go back later and run", "tokens": [467, 6653, 291, 764, 264, 912, 5438, 294, 428, 21060, 281, 1190, 700, 295, 439, 322, 264, 6889, 293, 550, 352, 646, 1780, 293, 1190], "temperature": 0.0, "avg_logprob": -0.13871593635623194, "compression_ratio": 1.7798507462686568, "no_speech_prob": 1.4823523315499187e-06}, {"id": 316, "seek": 154586, "start": 1545.86, "end": 1549.86, "text": " It on the full data set okay?", "tokens": [467, 322, 264, 1577, 1412, 992, 1392, 30], "temperature": 0.0, "avg_logprob": -0.2407447664361251, "compression_ratio": 1.5602094240837696, "no_speech_prob": 3.187554966643802e-06}, {"id": 317, "seek": 154586, "start": 1551.8999999999999, "end": 1553.8999999999999, "text": " So now that we've got that joined stamp", "tokens": [407, 586, 300, 321, 600, 658, 300, 6869, 9921], "temperature": 0.0, "avg_logprob": -0.2407447664361251, "compression_ratio": 1.5602094240837696, "no_speech_prob": 3.187554966643802e-06}, {"id": 318, "seek": 154586, "start": 1553.9799999999998, "end": 1559.9799999999998, "text": " We can then pass it to proc DF as we've done before to grab the dependent variable", "tokens": [492, 393, 550, 1320, 309, 281, 9510, 48336, 382, 321, 600, 1096, 949, 281, 4444, 264, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.2407447664361251, "compression_ratio": 1.5602094240837696, "no_speech_prob": 3.187554966643802e-06}, {"id": 319, "seek": 154586, "start": 1561.58, "end": 1568.3, "text": " To deal with missing values and in this case we pass one more thing which is do scale equals true", "tokens": [1407, 2028, 365, 5361, 4190, 293, 294, 341, 1389, 321, 1320, 472, 544, 551, 597, 307, 360, 4373, 6915, 2074], "temperature": 0.0, "avg_logprob": -0.2407447664361251, "compression_ratio": 1.5602094240837696, "no_speech_prob": 3.187554966643802e-06}, {"id": 320, "seek": 154586, "start": 1569.4599999999998, "end": 1573.9799999999998, "text": " do scale equals true will subtract the mean and", "tokens": [360, 4373, 6915, 2074, 486, 16390, 264, 914, 293], "temperature": 0.0, "avg_logprob": -0.2407447664361251, "compression_ratio": 1.5602094240837696, "no_speech_prob": 3.187554966643802e-06}, {"id": 321, "seek": 157398, "start": 1573.98, "end": 1577.42, "text": " divide by the standard deviation and", "tokens": [9845, 538, 264, 3832, 25163, 293], "temperature": 0.0, "avg_logprob": -0.2893339639686676, "compression_ratio": 1.5863874345549738, "no_speech_prob": 2.948007704617339e-06}, {"id": 322, "seek": 157398, "start": 1577.94, "end": 1581.7, "text": " So the reason for that is that if our first layer?", "tokens": [407, 264, 1778, 337, 300, 307, 300, 498, 527, 700, 4583, 30], "temperature": 0.0, "avg_logprob": -0.2893339639686676, "compression_ratio": 1.5863874345549738, "no_speech_prob": 2.948007704617339e-06}, {"id": 323, "seek": 157398, "start": 1582.66, "end": 1586.6200000000001, "text": " You know it's just a matrix multiply right so here's our set of weights", "tokens": [509, 458, 309, 311, 445, 257, 8141, 12972, 558, 370, 510, 311, 527, 992, 295, 17443], "temperature": 0.0, "avg_logprob": -0.2893339639686676, "compression_ratio": 1.5863874345549738, "no_speech_prob": 2.948007704617339e-06}, {"id": 324, "seek": 157398, "start": 1587.42, "end": 1590.42, "text": " and our input is like I", "tokens": [293, 527, 4846, 307, 411, 286], "temperature": 0.0, "avg_logprob": -0.2893339639686676, "compression_ratio": 1.5863874345549738, "no_speech_prob": 2.948007704617339e-06}, {"id": 325, "seek": 157398, "start": 1591.42, "end": 1593.66, "text": " Don't know it's got something which is like point. Oh oh one", "tokens": [1468, 380, 458, 309, 311, 658, 746, 597, 307, 411, 935, 13, 876, 1954, 472], "temperature": 0.0, "avg_logprob": -0.2893339639686676, "compression_ratio": 1.5863874345549738, "no_speech_prob": 2.948007704617339e-06}, {"id": 326, "seek": 157398, "start": 1594.46, "end": 1597.78, "text": " And then it's got something like which is like 10 to the 6", "tokens": [400, 550, 309, 311, 658, 746, 411, 597, 307, 411, 1266, 281, 264, 1386], "temperature": 0.0, "avg_logprob": -0.2893339639686676, "compression_ratio": 1.5863874345549738, "no_speech_prob": 2.948007704617339e-06}, {"id": 327, "seek": 159778, "start": 1597.78, "end": 1605.42, "text": " Right and then our weight matrix has been initialized to be like random numbers between 0 and 1", "tokens": [1779, 293, 550, 527, 3364, 8141, 575, 668, 5883, 1602, 281, 312, 411, 4974, 3547, 1296, 1958, 293, 502], "temperature": 0.0, "avg_logprob": -0.27487511830787137, "compression_ratio": 1.5357142857142858, "no_speech_prob": 5.804990337310301e-07}, {"id": 328, "seek": 159778, "start": 1606.1, "end": 1613.1, "text": " Right so got like 0.6. Oh point 1 etc then basically this thing here is going to have", "tokens": [1779, 370, 658, 411, 1958, 13, 21, 13, 876, 935, 502, 5183, 550, 1936, 341, 551, 510, 307, 516, 281, 362], "temperature": 0.0, "avg_logprob": -0.27487511830787137, "compression_ratio": 1.5357142857142858, "no_speech_prob": 5.804990337310301e-07}, {"id": 329, "seek": 159778, "start": 1614.06, "end": 1620.3, "text": " Gradients that are nine orders of magnitude bigger than this thing here, which is not going to be good for", "tokens": [16710, 2448, 300, 366, 4949, 9470, 295, 15668, 3801, 813, 341, 551, 510, 11, 597, 307, 406, 516, 281, 312, 665, 337], "temperature": 0.0, "avg_logprob": -0.27487511830787137, "compression_ratio": 1.5357142857142858, "no_speech_prob": 5.804990337310301e-07}, {"id": 330, "seek": 159778, "start": 1621.3799999999999, "end": 1622.74, "text": " optimization", "tokens": [19618], "temperature": 0.0, "avg_logprob": -0.27487511830787137, "compression_ratio": 1.5357142857142858, "no_speech_prob": 5.804990337310301e-07}, {"id": 331, "seek": 162274, "start": 1622.74, "end": 1629.3, "text": " Okay, so by normalizing everything to be mean of zero standard deviation of one to start with", "tokens": [1033, 11, 370, 538, 2710, 3319, 1203, 281, 312, 914, 295, 4018, 3832, 25163, 295, 472, 281, 722, 365], "temperature": 0.0, "avg_logprob": -0.15303598131452287, "compression_ratio": 1.699530516431925, "no_speech_prob": 2.3320621949096676e-06}, {"id": 332, "seek": 162274, "start": 1630.02, "end": 1637.1, "text": " Then that means that all of the gradients are going to be you know on the same kind of scale", "tokens": [1396, 300, 1355, 300, 439, 295, 264, 2771, 2448, 366, 516, 281, 312, 291, 458, 322, 264, 912, 733, 295, 4373], "temperature": 0.0, "avg_logprob": -0.15303598131452287, "compression_ratio": 1.699530516431925, "no_speech_prob": 2.3320621949096676e-06}, {"id": 333, "seek": 162274, "start": 1638.9, "end": 1640.9, "text": " We didn't have to do that in random forests", "tokens": [492, 994, 380, 362, 281, 360, 300, 294, 4974, 21700], "temperature": 0.0, "avg_logprob": -0.15303598131452287, "compression_ratio": 1.699530516431925, "no_speech_prob": 2.3320621949096676e-06}, {"id": 334, "seek": 162274, "start": 1641.7, "end": 1647.66, "text": " Right because in random forests. We only cared about the sort order. We didn't care about the values at all", "tokens": [1779, 570, 294, 4974, 21700, 13, 492, 787, 19779, 466, 264, 1333, 1668, 13, 492, 994, 380, 1127, 466, 264, 4190, 412, 439], "temperature": 0.0, "avg_logprob": -0.15303598131452287, "compression_ratio": 1.699530516431925, "no_speech_prob": 2.3320621949096676e-06}, {"id": 335, "seek": 162274, "start": 1648.74, "end": 1650.74, "text": " right, but in that with", "tokens": [558, 11, 457, 294, 300, 365], "temperature": 0.0, "avg_logprob": -0.15303598131452287, "compression_ratio": 1.699530516431925, "no_speech_prob": 2.3320621949096676e-06}, {"id": 336, "seek": 165074, "start": 1650.74, "end": 1658.38, "text": " Linear models and things that are built out of layers of linear models like ie neural nets we care very much", "tokens": [14670, 289, 5245, 293, 721, 300, 366, 3094, 484, 295, 7914, 295, 8213, 5245, 411, 43203, 18161, 36170, 321, 1127, 588, 709], "temperature": 0.0, "avg_logprob": -0.1883801491029801, "compression_ratio": 1.6909871244635193, "no_speech_prob": 1.4823525589235942e-06}, {"id": 337, "seek": 165074, "start": 1659.1, "end": 1661.1, "text": " About the scale", "tokens": [7769, 264, 4373], "temperature": 0.0, "avg_logprob": -0.1883801491029801, "compression_ratio": 1.6909871244635193, "no_speech_prob": 1.4823525589235942e-06}, {"id": 338, "seek": 165074, "start": 1661.1, "end": 1663.18, "text": " Okay, so do scale equals true", "tokens": [1033, 11, 370, 360, 4373, 6915, 2074], "temperature": 0.0, "avg_logprob": -0.1883801491029801, "compression_ratio": 1.6909871244635193, "no_speech_prob": 1.4823525589235942e-06}, {"id": 339, "seek": 165074, "start": 1664.02, "end": 1668.86, "text": " Normalizes our data for us now since it normalizes our data for us it returns one extra", "tokens": [21277, 5660, 527, 1412, 337, 505, 586, 1670, 309, 2710, 5660, 527, 1412, 337, 505, 309, 11247, 472, 2857], "temperature": 0.0, "avg_logprob": -0.1883801491029801, "compression_ratio": 1.6909871244635193, "no_speech_prob": 1.4823525589235942e-06}, {"id": 340, "seek": 165074, "start": 1669.5, "end": 1675.78, "text": " Object which is a mapper which is an object that contains for each continuous variable", "tokens": [24753, 597, 307, 257, 463, 3717, 597, 307, 364, 2657, 300, 8306, 337, 1184, 10957, 7006], "temperature": 0.0, "avg_logprob": -0.1883801491029801, "compression_ratio": 1.6909871244635193, "no_speech_prob": 1.4823525589235942e-06}, {"id": 341, "seek": 165074, "start": 1675.78, "end": 1678.7, "text": " What was the mean and standard deviation it was normalized with?", "tokens": [708, 390, 264, 914, 293, 3832, 25163, 309, 390, 48704, 365, 30], "temperature": 0.0, "avg_logprob": -0.1883801491029801, "compression_ratio": 1.6909871244635193, "no_speech_prob": 1.4823525589235942e-06}, {"id": 342, "seek": 167870, "start": 1678.7, "end": 1680.7, "text": " the reason being", "tokens": [264, 1778, 885], "temperature": 0.0, "avg_logprob": -0.19630449645373285, "compression_ratio": 1.7543103448275863, "no_speech_prob": 1.8162087371820235e-06}, {"id": 343, "seek": 167870, "start": 1681.82, "end": 1683.82, "text": " That we're going to have to use the same", "tokens": [663, 321, 434, 516, 281, 362, 281, 764, 264, 912], "temperature": 0.0, "avg_logprob": -0.19630449645373285, "compression_ratio": 1.7543103448275863, "no_speech_prob": 1.8162087371820235e-06}, {"id": 344, "seek": 167870, "start": 1684.98, "end": 1687.06, "text": " Mean and standard deviation on the test set", "tokens": [12302, 293, 3832, 25163, 322, 264, 1500, 992], "temperature": 0.0, "avg_logprob": -0.19630449645373285, "compression_ratio": 1.7543103448275863, "no_speech_prob": 1.8162087371820235e-06}, {"id": 345, "seek": 167870, "start": 1688.14, "end": 1692.9, "text": " All right, because we need our test set and our training set to be scaled in the exact same way otherwise", "tokens": [1057, 558, 11, 570, 321, 643, 527, 1500, 992, 293, 527, 3097, 992, 281, 312, 36039, 294, 264, 1900, 912, 636, 5911], "temperature": 0.0, "avg_logprob": -0.19630449645373285, "compression_ratio": 1.7543103448275863, "no_speech_prob": 1.8162087371820235e-06}, {"id": 346, "seek": 167870, "start": 1692.9, "end": 1694.9, "text": " They're going to have different meanings", "tokens": [814, 434, 516, 281, 362, 819, 28138], "temperature": 0.0, "avg_logprob": -0.19630449645373285, "compression_ratio": 1.7543103448275863, "no_speech_prob": 1.8162087371820235e-06}, {"id": 347, "seek": 167870, "start": 1695.06, "end": 1697.06, "text": " Okay, and so these", "tokens": [1033, 11, 293, 370, 613], "temperature": 0.0, "avg_logprob": -0.19630449645373285, "compression_ratio": 1.7543103448275863, "no_speech_prob": 1.8162087371820235e-06}, {"id": 348, "seek": 167870, "start": 1698.54, "end": 1703.78, "text": " Details about making sure that your test and training set have the same categorical codings", "tokens": [42811, 466, 1455, 988, 300, 428, 1500, 293, 3097, 992, 362, 264, 912, 19250, 804, 17656, 1109], "temperature": 0.0, "avg_logprob": -0.19630449645373285, "compression_ratio": 1.7543103448275863, "no_speech_prob": 1.8162087371820235e-06}, {"id": 349, "seek": 167870, "start": 1704.5800000000002, "end": 1707.64, "text": " the same missing value replacement and the same", "tokens": [264, 912, 5361, 2158, 14419, 293, 264, 912], "temperature": 0.0, "avg_logprob": -0.19630449645373285, "compression_ratio": 1.7543103448275863, "no_speech_prob": 1.8162087371820235e-06}, {"id": 350, "seek": 170764, "start": 1707.64, "end": 1712.2, "text": " Same scaling normalization are really important to get right", "tokens": [10635, 21589, 2710, 2144, 366, 534, 1021, 281, 483, 558], "temperature": 0.0, "avg_logprob": -0.20292182301366052, "compression_ratio": 1.5947136563876652, "no_speech_prob": 5.896407060390629e-07}, {"id": 351, "seek": 170764, "start": 1712.76, "end": 1717.8000000000002, "text": " Because if you don't get it right then your test set is you know not going to work at all", "tokens": [1436, 498, 291, 500, 380, 483, 309, 558, 550, 428, 1500, 992, 307, 291, 458, 406, 516, 281, 589, 412, 439], "temperature": 0.0, "avg_logprob": -0.20292182301366052, "compression_ratio": 1.5947136563876652, "no_speech_prob": 5.896407060390629e-07}, {"id": 352, "seek": 170764, "start": 1718.5200000000002, "end": 1720.5200000000002, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.20292182301366052, "compression_ratio": 1.5947136563876652, "no_speech_prob": 5.896407060390629e-07}, {"id": 353, "seek": 170764, "start": 1720.64, "end": 1723.64, "text": " But if you follow these steps, you know it'll work fine", "tokens": [583, 498, 291, 1524, 613, 4439, 11, 291, 458, 309, 603, 589, 2489], "temperature": 0.0, "avg_logprob": -0.20292182301366052, "compression_ratio": 1.5947136563876652, "no_speech_prob": 5.896407060390629e-07}, {"id": 354, "seek": 170764, "start": 1724.8400000000001, "end": 1730.6000000000001, "text": " We also take the log of the dependent variable and that's because in this Kaggle competition", "tokens": [492, 611, 747, 264, 3565, 295, 264, 12334, 7006, 293, 300, 311, 570, 294, 341, 48751, 22631, 6211], "temperature": 0.0, "avg_logprob": -0.20292182301366052, "compression_ratio": 1.5947136563876652, "no_speech_prob": 5.896407060390629e-07}, {"id": 355, "seek": 170764, "start": 1731.0, "end": 1734.3200000000002, "text": " The evaluation metric was root mean squared percent error", "tokens": [440, 13344, 20678, 390, 5593, 914, 8889, 3043, 6713], "temperature": 0.0, "avg_logprob": -0.20292182301366052, "compression_ratio": 1.5947136563876652, "no_speech_prob": 5.896407060390629e-07}, {"id": 356, "seek": 173432, "start": 1734.32, "end": 1738.72, "text": " So root means great percent error means we're being", "tokens": [407, 5593, 1355, 869, 3043, 6713, 1355, 321, 434, 885], "temperature": 0.0, "avg_logprob": -0.2056799938804225, "compression_ratio": 1.7253886010362693, "no_speech_prob": 1.2289151527511422e-06}, {"id": 357, "seek": 173432, "start": 1739.3999999999999, "end": 1744.76, "text": " penalized based on the ratio between our answer and the correct answer", "tokens": [13661, 1602, 2361, 322, 264, 8509, 1296, 527, 1867, 293, 264, 3006, 1867], "temperature": 0.0, "avg_logprob": -0.2056799938804225, "compression_ratio": 1.7253886010362693, "no_speech_prob": 1.2289151527511422e-06}, {"id": 358, "seek": 173432, "start": 1746.72, "end": 1752.8799999999999, "text": " We don't have a loss function in pytorch called root means great percent error we could write one", "tokens": [492, 500, 380, 362, 257, 4470, 2445, 294, 25878, 284, 339, 1219, 5593, 1355, 869, 3043, 6713, 321, 727, 2464, 472], "temperature": 0.0, "avg_logprob": -0.2056799938804225, "compression_ratio": 1.7253886010362693, "no_speech_prob": 1.2289151527511422e-06}, {"id": 359, "seek": 173432, "start": 1753.6, "end": 1759.8999999999999, "text": " But easier is just to take the log of the dependent because the difference between logs is the same as the ratio", "tokens": [583, 3571, 307, 445, 281, 747, 264, 3565, 295, 264, 12334, 570, 264, 2649, 1296, 20820, 307, 264, 912, 382, 264, 8509], "temperature": 0.0, "avg_logprob": -0.2056799938804225, "compression_ratio": 1.7253886010362693, "no_speech_prob": 1.2289151527511422e-06}, {"id": 360, "seek": 175990, "start": 1759.9, "end": 1766.6000000000001, "text": " Okay, so by taking the log we kind of get that for free you'll notice like the vast majority of regression", "tokens": [1033, 11, 370, 538, 1940, 264, 3565, 321, 733, 295, 483, 300, 337, 1737, 291, 603, 3449, 411, 264, 8369, 6286, 295, 24590], "temperature": 0.0, "avg_logprob": -0.1839913434760515, "compression_ratio": 1.5872340425531914, "no_speech_prob": 2.964903274005337e-07}, {"id": 361, "seek": 175990, "start": 1767.9, "end": 1770.3000000000002, "text": " competitions on Kaggle use", "tokens": [26185, 322, 48751, 22631, 764], "temperature": 0.0, "avg_logprob": -0.1839913434760515, "compression_ratio": 1.5872340425531914, "no_speech_prob": 2.964903274005337e-07}, {"id": 362, "seek": 175990, "start": 1771.18, "end": 1777.3200000000002, "text": " Either root mean squared percent error or root means greater error of the log as their evaluation metric", "tokens": [13746, 5593, 914, 8889, 3043, 6713, 420, 5593, 1355, 5044, 6713, 295, 264, 3565, 382, 641, 13344, 20678], "temperature": 0.0, "avg_logprob": -0.1839913434760515, "compression_ratio": 1.5872340425531914, "no_speech_prob": 2.964903274005337e-07}, {"id": 363, "seek": 175990, "start": 1777.3200000000002, "end": 1782.6200000000001, "text": " And that's because in real world problems most of the time we care more about ratios", "tokens": [400, 300, 311, 570, 294, 957, 1002, 2740, 881, 295, 264, 565, 321, 1127, 544, 466, 32435], "temperature": 0.0, "avg_logprob": -0.1839913434760515, "compression_ratio": 1.5872340425531914, "no_speech_prob": 2.964903274005337e-07}, {"id": 364, "seek": 175990, "start": 1783.22, "end": 1785.22, "text": " Than about raw differences", "tokens": [18289, 466, 8936, 7300], "temperature": 0.0, "avg_logprob": -0.1839913434760515, "compression_ratio": 1.5872340425531914, "no_speech_prob": 2.964903274005337e-07}, {"id": 365, "seek": 175990, "start": 1785.7, "end": 1787.7, "text": " So if you're designing", "tokens": [407, 498, 291, 434, 14685], "temperature": 0.0, "avg_logprob": -0.1839913434760515, "compression_ratio": 1.5872340425531914, "no_speech_prob": 2.964903274005337e-07}, {"id": 366, "seek": 178770, "start": 1787.7, "end": 1792.82, "text": " Your own project it's quite likely that you'll want to think about using", "tokens": [2260, 1065, 1716, 309, 311, 1596, 3700, 300, 291, 603, 528, 281, 519, 466, 1228], "temperature": 0.0, "avg_logprob": -0.18875718116760254, "compression_ratio": 1.6146341463414635, "no_speech_prob": 1.2878903135060682e-06}, {"id": 367, "seek": 178770, "start": 1793.66, "end": 1796.14, "text": " log of your dependent variable", "tokens": [3565, 295, 428, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.18875718116760254, "compression_ratio": 1.6146341463414635, "no_speech_prob": 1.2878903135060682e-06}, {"id": 368, "seek": 178770, "start": 1800.6200000000001, "end": 1803.92, "text": " So then we create a validation set and as we've learned before", "tokens": [407, 550, 321, 1884, 257, 24071, 992, 293, 382, 321, 600, 3264, 949], "temperature": 0.0, "avg_logprob": -0.18875718116760254, "compression_ratio": 1.6146341463414635, "no_speech_prob": 1.2878903135060682e-06}, {"id": 369, "seek": 178770, "start": 1805.3, "end": 1813.02, "text": " Most of the time if you've got a problem involving a time component your validation set probably wants to be the most recent", "tokens": [4534, 295, 264, 565, 498, 291, 600, 658, 257, 1154, 17030, 257, 565, 6542, 428, 24071, 992, 1391, 2738, 281, 312, 264, 881, 5162], "temperature": 0.0, "avg_logprob": -0.18875718116760254, "compression_ratio": 1.6146341463414635, "no_speech_prob": 1.2878903135060682e-06}, {"id": 370, "seek": 178770, "start": 1813.38, "end": 1815.94, "text": " Time period rather than a random subset", "tokens": [6161, 2896, 2831, 813, 257, 4974, 25993], "temperature": 0.0, "avg_logprob": -0.18875718116760254, "compression_ratio": 1.6146341463414635, "no_speech_prob": 1.2878903135060682e-06}, {"id": 371, "seek": 181594, "start": 1815.94, "end": 1817.94, "text": " Okay, so that's what I do here", "tokens": [1033, 11, 370, 300, 311, 437, 286, 360, 510], "temperature": 0.0, "avg_logprob": -0.13113103442721896, "compression_ratio": 1.5965665236051503, "no_speech_prob": 1.1726393722710782e-06}, {"id": 372, "seek": 181594, "start": 1819.78, "end": 1826.14, "text": " When I finished modeling and I found an architecture and a set of hyper parameters and a number of epochs and all that stuff that", "tokens": [1133, 286, 4335, 15983, 293, 286, 1352, 364, 9482, 293, 257, 992, 295, 9848, 9834, 293, 257, 1230, 295, 30992, 28346, 293, 439, 300, 1507, 300], "temperature": 0.0, "avg_logprob": -0.13113103442721896, "compression_ratio": 1.5965665236051503, "no_speech_prob": 1.1726393722710782e-06}, {"id": 373, "seek": 181594, "start": 1826.14, "end": 1827.6200000000001, "text": " Works really well", "tokens": [27914, 534, 731], "temperature": 0.0, "avg_logprob": -0.13113103442721896, "compression_ratio": 1.5965665236051503, "no_speech_prob": 1.1726393722710782e-06}, {"id": 374, "seek": 181594, "start": 1827.6200000000001, "end": 1832.22, "text": " If I want to make my model as good as possible. I'll retrain on the whole thing", "tokens": [759, 286, 528, 281, 652, 452, 2316, 382, 665, 382, 1944, 13, 286, 603, 1533, 7146, 322, 264, 1379, 551], "temperature": 0.0, "avg_logprob": -0.13113103442721896, "compression_ratio": 1.5965665236051503, "no_speech_prob": 1.1726393722710782e-06}, {"id": 375, "seek": 181594, "start": 1833.18, "end": 1840.72, "text": " Right including the validation set right now currently at least fast AI assumes that you do have a validation set", "tokens": [1779, 3009, 264, 24071, 992, 558, 586, 4362, 412, 1935, 2370, 7318, 37808, 300, 291, 360, 362, 257, 24071, 992], "temperature": 0.0, "avg_logprob": -0.13113103442721896, "compression_ratio": 1.5965665236051503, "no_speech_prob": 1.1726393722710782e-06}, {"id": 376, "seek": 184072, "start": 1840.72, "end": 1847.04, "text": " So my kind of hacky workaround is to set my validation set to just be one index which is the first row", "tokens": [407, 452, 733, 295, 10339, 88, 589, 25762, 307, 281, 992, 452, 24071, 992, 281, 445, 312, 472, 8186, 597, 307, 264, 700, 5386], "temperature": 0.0, "avg_logprob": -0.11035664876302083, "compression_ratio": 1.7366412213740459, "no_speech_prob": 1.308168180003122e-06}, {"id": 377, "seek": 184072, "start": 1847.4, "end": 1852.48, "text": " Okay, and that way like all the code keeps working, but there's no real validation set", "tokens": [1033, 11, 293, 300, 636, 411, 439, 264, 3089, 5965, 1364, 11, 457, 456, 311, 572, 957, 24071, 992], "temperature": 0.0, "avg_logprob": -0.11035664876302083, "compression_ratio": 1.7366412213740459, "no_speech_prob": 1.308168180003122e-06}, {"id": 378, "seek": 184072, "start": 1852.56, "end": 1856.1200000000001, "text": " so obviously if you do this you need to make sure that your", "tokens": [370, 2745, 498, 291, 360, 341, 291, 643, 281, 652, 988, 300, 428], "temperature": 0.0, "avg_logprob": -0.11035664876302083, "compression_ratio": 1.7366412213740459, "no_speech_prob": 1.308168180003122e-06}, {"id": 379, "seek": 184072, "start": 1857.1200000000001, "end": 1864.52, "text": " Final training is like the exact same hyper parameters the exact same number of epochs exactly the same as the thing that worked", "tokens": [13443, 3097, 307, 411, 264, 1900, 912, 9848, 9834, 264, 1900, 912, 1230, 295, 30992, 28346, 2293, 264, 912, 382, 264, 551, 300, 2732], "temperature": 0.0, "avg_logprob": -0.11035664876302083, "compression_ratio": 1.7366412213740459, "no_speech_prob": 1.308168180003122e-06}, {"id": 380, "seek": 186452, "start": 1864.52, "end": 1871.68, "text": " Because you don't actually have a proper validation set now to check against I have a question regarding", "tokens": [1436, 291, 500, 380, 767, 362, 257, 2296, 24071, 992, 586, 281, 1520, 1970, 286, 362, 257, 1168, 8595], "temperature": 0.0, "avg_logprob": -0.2209049688803183, "compression_ratio": 1.5935828877005347, "no_speech_prob": 1.4738184290763456e-05}, {"id": 381, "seek": 186452, "start": 1871.92, "end": 1874.6, "text": " Get elapsed function which we discussed before", "tokens": [3240, 806, 2382, 292, 2445, 597, 321, 7152, 949], "temperature": 0.0, "avg_logprob": -0.2209049688803183, "compression_ratio": 1.5935828877005347, "no_speech_prob": 1.4738184290763456e-05}, {"id": 382, "seek": 186452, "start": 1875.92, "end": 1879.6, "text": " So in get elapsed function we are trying to find", "tokens": [407, 294, 483, 806, 2382, 292, 2445, 321, 366, 1382, 281, 915], "temperature": 0.0, "avg_logprob": -0.2209049688803183, "compression_ratio": 1.5935828877005347, "no_speech_prob": 1.4738184290763456e-05}, {"id": 383, "seek": 186452, "start": 1880.92, "end": 1882.92, "text": " When is the next holiday?", "tokens": [1133, 307, 264, 958, 9960, 30], "temperature": 0.0, "avg_logprob": -0.2209049688803183, "compression_ratio": 1.5935828877005347, "no_speech_prob": 1.4738184290763456e-05}, {"id": 384, "seek": 186452, "start": 1883.48, "end": 1887.92, "text": " Like when will the next holiday come how many how many days away is it?", "tokens": [1743, 562, 486, 264, 958, 9960, 808, 577, 867, 577, 867, 1708, 1314, 307, 309, 30], "temperature": 0.0, "avg_logprob": -0.2209049688803183, "compression_ratio": 1.5935828877005347, "no_speech_prob": 1.4738184290763456e-05}, {"id": 385, "seek": 188792, "start": 1887.92, "end": 1895.04, "text": " So every year the holidays are more or less fixed like there will be holiday on 4th of July 25th of December and", "tokens": [407, 633, 1064, 264, 15734, 366, 544, 420, 1570, 6806, 411, 456, 486, 312, 9960, 322, 1017, 392, 295, 7370, 3552, 392, 295, 7687, 293], "temperature": 0.0, "avg_logprob": -0.15812382799513797, "compression_ratio": 1.6051502145922747, "no_speech_prob": 3.340474222568446e-06}, {"id": 386, "seek": 188792, "start": 1895.52, "end": 1897.0, "text": " There is hardly any change", "tokens": [821, 307, 13572, 604, 1319], "temperature": 0.0, "avg_logprob": -0.15812382799513797, "compression_ratio": 1.6051502145922747, "no_speech_prob": 3.340474222568446e-06}, {"id": 387, "seek": 188792, "start": 1897.0, "end": 1903.6000000000001, "text": " So can't we just look from previous years and just get a list of all the holidays that are going to occur this year", "tokens": [407, 393, 380, 321, 445, 574, 490, 3894, 924, 293, 445, 483, 257, 1329, 295, 439, 264, 15734, 300, 366, 516, 281, 5160, 341, 1064], "temperature": 0.0, "avg_logprob": -0.15812382799513797, "compression_ratio": 1.6051502145922747, "no_speech_prob": 3.340474222568446e-06}, {"id": 388, "seek": 188792, "start": 1905.96, "end": 1911.4, "text": " Maybe I mean in this case, I guess like that's not true of promo, right and", "tokens": [2704, 286, 914, 294, 341, 1389, 11, 286, 2041, 411, 300, 311, 406, 2074, 295, 26750, 11, 558, 293], "temperature": 0.0, "avg_logprob": -0.15812382799513797, "compression_ratio": 1.6051502145922747, "no_speech_prob": 3.340474222568446e-06}, {"id": 389, "seek": 188792, "start": 1912.0, "end": 1915.6200000000001, "text": " Some holidays change like Easter, you know", "tokens": [2188, 15734, 1319, 411, 9403, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.15812382799513797, "compression_ratio": 1.6051502145922747, "no_speech_prob": 3.340474222568446e-06}, {"id": 390, "seek": 191562, "start": 1915.62, "end": 1921.1399999999999, "text": " So like this this this way I get to write one piece of code that works for all of them", "tokens": [407, 411, 341, 341, 341, 636, 286, 483, 281, 2464, 472, 2522, 295, 3089, 300, 1985, 337, 439, 295, 552], "temperature": 0.0, "avg_logprob": -0.19088533609220298, "compression_ratio": 1.6517857142857142, "no_speech_prob": 5.771777068730444e-06}, {"id": 391, "seek": 191562, "start": 1923.2199999999998, "end": 1926.6, "text": " You know and it doesn't take very long to run", "tokens": [509, 458, 293, 309, 1177, 380, 747, 588, 938, 281, 1190], "temperature": 0.0, "avg_logprob": -0.19088533609220298, "compression_ratio": 1.6517857142857142, "no_speech_prob": 5.771777068730444e-06}, {"id": 392, "seek": 191562, "start": 1926.8999999999999, "end": 1932.56, "text": " So yeah, so there might be ways if you're if your data set was so big that this took too long", "tokens": [407, 1338, 11, 370, 456, 1062, 312, 2098, 498, 291, 434, 498, 428, 1412, 992, 390, 370, 955, 300, 341, 1890, 886, 938], "temperature": 0.0, "avg_logprob": -0.19088533609220298, "compression_ratio": 1.6517857142857142, "no_speech_prob": 5.771777068730444e-06}, {"id": 393, "seek": 191562, "start": 1932.56, "end": 1935.3799999999999, "text": " You could maybe do it on one year and then kind of somehow copy it", "tokens": [509, 727, 1310, 360, 309, 322, 472, 1064, 293, 550, 733, 295, 6063, 5055, 309], "temperature": 0.0, "avg_logprob": -0.19088533609220298, "compression_ratio": 1.6517857142857142, "no_speech_prob": 5.771777068730444e-06}, {"id": 394, "seek": 191562, "start": 1935.3799999999999, "end": 1939.6599999999999, "text": " But yeah in this case there was no need to and I gen you know I always", "tokens": [583, 1338, 294, 341, 1389, 456, 390, 572, 643, 281, 293, 286, 1049, 291, 458, 286, 1009], "temperature": 0.0, "avg_logprob": -0.19088533609220298, "compression_ratio": 1.6517857142857142, "no_speech_prob": 5.771777068730444e-06}, {"id": 395, "seek": 191562, "start": 1940.5, "end": 1941.7399999999998, "text": " value", "tokens": [2158], "temperature": 0.0, "avg_logprob": -0.19088533609220298, "compression_ratio": 1.6517857142857142, "no_speech_prob": 5.771777068730444e-06}, {"id": 396, "seek": 194174, "start": 1941.74, "end": 1947.28, "text": " My time over my computer's time, so I try to keep things as simple as I can", "tokens": [1222, 565, 670, 452, 3820, 311, 565, 11, 370, 286, 853, 281, 1066, 721, 382, 2199, 382, 286, 393], "temperature": 0.0, "avg_logprob": -0.17081532894986348, "compression_ratio": 1.7733333333333334, "no_speech_prob": 5.422154572443105e-06}, {"id": 397, "seek": 194174, "start": 1949.5, "end": 1950.86, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.17081532894986348, "compression_ratio": 1.7733333333333334, "no_speech_prob": 5.422154572443105e-06}, {"id": 398, "seek": 194174, "start": 1950.86, "end": 1953.5, "text": " So now we can create our model", "tokens": [407, 586, 321, 393, 1884, 527, 2316], "temperature": 0.0, "avg_logprob": -0.17081532894986348, "compression_ratio": 1.7733333333333334, "no_speech_prob": 5.422154572443105e-06}, {"id": 399, "seek": 194174, "start": 1954.34, "end": 1959.38, "text": " And so to create our model we have to create a model data object as we always do with fast AI", "tokens": [400, 370, 281, 1884, 527, 2316, 321, 362, 281, 1884, 257, 2316, 1412, 2657, 382, 321, 1009, 360, 365, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.17081532894986348, "compression_ratio": 1.7733333333333334, "no_speech_prob": 5.422154572443105e-06}, {"id": 400, "seek": 194174, "start": 1959.58, "end": 1966.38, "text": " So a columnar model data object is just a data a model data object that represents a training set a validation set and an", "tokens": [407, 257, 7738, 289, 2316, 1412, 2657, 307, 445, 257, 1412, 257, 2316, 1412, 2657, 300, 8855, 257, 3097, 992, 257, 24071, 992, 293, 364], "temperature": 0.0, "avg_logprob": -0.17081532894986348, "compression_ratio": 1.7733333333333334, "no_speech_prob": 5.422154572443105e-06}, {"id": 401, "seek": 194174, "start": 1966.38, "end": 1971.46, "text": " Optional test set of standard columnar, you know structured data, okay?", "tokens": [29284, 304, 1500, 992, 295, 3832, 7738, 289, 11, 291, 458, 18519, 1412, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.17081532894986348, "compression_ratio": 1.7733333333333334, "no_speech_prob": 5.422154572443105e-06}, {"id": 402, "seek": 197146, "start": 1971.46, "end": 1978.6000000000001, "text": " And we just have to tell it which of the variables should we treat as categorical?", "tokens": [400, 321, 445, 362, 281, 980, 309, 597, 295, 264, 9102, 820, 321, 2387, 382, 19250, 804, 30], "temperature": 0.0, "avg_logprob": -0.18769222497940063, "compression_ratio": 1.5909090909090908, "no_speech_prob": 1.8162145352107473e-06}, {"id": 403, "seek": 197146, "start": 1979.9, "end": 1982.06, "text": " Okay, and then pass in our data frames", "tokens": [1033, 11, 293, 550, 1320, 294, 527, 1412, 12083], "temperature": 0.0, "avg_logprob": -0.18769222497940063, "compression_ratio": 1.5909090909090908, "no_speech_prob": 1.8162145352107473e-06}, {"id": 404, "seek": 197146, "start": 1987.3400000000001, "end": 1989.6200000000001, "text": " So for each of our categorical variables", "tokens": [407, 337, 1184, 295, 527, 19250, 804, 9102], "temperature": 0.0, "avg_logprob": -0.18769222497940063, "compression_ratio": 1.5909090909090908, "no_speech_prob": 1.8162145352107473e-06}, {"id": 405, "seek": 197146, "start": 1991.26, "end": 1994.94, "text": " Here is the number of categories it has okay", "tokens": [1692, 307, 264, 1230, 295, 10479, 309, 575, 1392], "temperature": 0.0, "avg_logprob": -0.18769222497940063, "compression_ratio": 1.5909090909090908, "no_speech_prob": 1.8162145352107473e-06}, {"id": 406, "seek": 197146, "start": 1997.1000000000001, "end": 1999.48, "text": " So for each of our embedding matrices", "tokens": [407, 337, 1184, 295, 527, 12240, 3584, 32284], "temperature": 0.0, "avg_logprob": -0.18769222497940063, "compression_ratio": 1.5909090909090908, "no_speech_prob": 1.8162145352107473e-06}, {"id": 407, "seek": 199948, "start": 1999.48, "end": 2004.16, "text": " This tells us the number of rows in that embedding matrix", "tokens": [639, 5112, 505, 264, 1230, 295, 13241, 294, 300, 12240, 3584, 8141], "temperature": 0.0, "avg_logprob": -0.2268717134153688, "compression_ratio": 1.596938775510204, "no_speech_prob": 3.2887328416109085e-06}, {"id": 408, "seek": 199948, "start": 2005.0, "end": 2006.28, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.2268717134153688, "compression_ratio": 1.596938775510204, "no_speech_prob": 3.2887328416109085e-06}, {"id": 409, "seek": 199948, "start": 2006.28, "end": 2010.28, "text": " So then we define what embedding dimensionality we want", "tokens": [407, 550, 321, 6964, 437, 12240, 3584, 10139, 1860, 321, 528], "temperature": 0.0, "avg_logprob": -0.2268717134153688, "compression_ratio": 1.596938775510204, "no_speech_prob": 3.2887328416109085e-06}, {"id": 410, "seek": 199948, "start": 2014.24, "end": 2016.8, "text": " If you're doing like natural language processing", "tokens": [759, 291, 434, 884, 411, 3303, 2856, 9007], "temperature": 0.0, "avg_logprob": -0.2268717134153688, "compression_ratio": 1.596938775510204, "no_speech_prob": 3.2887328416109085e-06}, {"id": 411, "seek": 199948, "start": 2017.44, "end": 2022.8, "text": " Then the number of dimensions you need to capture all the nuance of what a word means and how it's used", "tokens": [1396, 264, 1230, 295, 12819, 291, 643, 281, 7983, 439, 264, 42625, 295, 437, 257, 1349, 1355, 293, 577, 309, 311, 1143], "temperature": 0.0, "avg_logprob": -0.2268717134153688, "compression_ratio": 1.596938775510204, "no_speech_prob": 3.2887328416109085e-06}, {"id": 412, "seek": 199948, "start": 2023.16, "end": 2027.22, "text": " Has been found empirically to be about 600", "tokens": [8646, 668, 1352, 25790, 984, 281, 312, 466, 11849], "temperature": 0.0, "avg_logprob": -0.2268717134153688, "compression_ratio": 1.596938775510204, "no_speech_prob": 3.2887328416109085e-06}, {"id": 413, "seek": 202722, "start": 2027.22, "end": 2030.3, "text": " It turns out that when you do", "tokens": [467, 4523, 484, 300, 562, 291, 360], "temperature": 0.0, "avg_logprob": -0.19356469754819516, "compression_ratio": 1.5228426395939085, "no_speech_prob": 1.0030103112512734e-06}, {"id": 414, "seek": 202722, "start": 2031.3, "end": 2033.1000000000001, "text": " NLP models", "tokens": [426, 45196, 5245], "temperature": 0.0, "avg_logprob": -0.19356469754819516, "compression_ratio": 1.5228426395939085, "no_speech_prob": 1.0030103112512734e-06}, {"id": 415, "seek": 202722, "start": 2033.1000000000001, "end": 2035.3, "text": " with embedding matrices that are", "tokens": [365, 12240, 3584, 32284, 300, 366], "temperature": 0.0, "avg_logprob": -0.19356469754819516, "compression_ratio": 1.5228426395939085, "no_speech_prob": 1.0030103112512734e-06}, {"id": 416, "seek": 202722, "start": 2036.3, "end": 2038.3, "text": " That are smaller than 600", "tokens": [663, 366, 4356, 813, 11849], "temperature": 0.0, "avg_logprob": -0.19356469754819516, "compression_ratio": 1.5228426395939085, "no_speech_prob": 1.0030103112512734e-06}, {"id": 417, "seek": 202722, "start": 2038.98, "end": 2045.94, "text": " You don't get as good of results as you do if you do if there's the size 600 beyond 600. It doesn't seem to improve much", "tokens": [509, 500, 380, 483, 382, 665, 295, 3542, 382, 291, 360, 498, 291, 360, 498, 456, 311, 264, 2744, 11849, 4399, 11849, 13, 467, 1177, 380, 1643, 281, 3470, 709], "temperature": 0.0, "avg_logprob": -0.19356469754819516, "compression_ratio": 1.5228426395939085, "no_speech_prob": 1.0030103112512734e-06}, {"id": 418, "seek": 202722, "start": 2046.6200000000001, "end": 2052.68, "text": " I would say that human language is one of the most complex things that we model", "tokens": [286, 576, 584, 300, 1952, 2856, 307, 472, 295, 264, 881, 3997, 721, 300, 321, 2316], "temperature": 0.0, "avg_logprob": -0.19356469754819516, "compression_ratio": 1.5228426395939085, "no_speech_prob": 1.0030103112512734e-06}, {"id": 419, "seek": 205268, "start": 2052.68, "end": 2057.0, "text": " So I wouldn't expect you to come across many if any", "tokens": [407, 286, 2759, 380, 2066, 291, 281, 808, 2108, 867, 498, 604], "temperature": 0.0, "avg_logprob": -0.16892761172670306, "compression_ratio": 1.3966480446927374, "no_speech_prob": 1.3081707948003896e-06}, {"id": 420, "seek": 205268, "start": 2058.08, "end": 2061.8599999999997, "text": " Categorical variables that need embedding matrices with more than 600", "tokens": [383, 2968, 284, 804, 9102, 300, 643, 12240, 3584, 32284, 365, 544, 813, 11849], "temperature": 0.0, "avg_logprob": -0.16892761172670306, "compression_ratio": 1.3966480446927374, "no_speech_prob": 1.3081707948003896e-06}, {"id": 421, "seek": 205268, "start": 2062.8799999999997, "end": 2064.64, "text": " dimensions", "tokens": [12819], "temperature": 0.0, "avg_logprob": -0.16892761172670306, "compression_ratio": 1.3966480446927374, "no_speech_prob": 1.3081707948003896e-06}, {"id": 422, "seek": 205268, "start": 2064.64, "end": 2066.64, "text": " at the other end", "tokens": [412, 264, 661, 917], "temperature": 0.0, "avg_logprob": -0.16892761172670306, "compression_ratio": 1.3966480446927374, "no_speech_prob": 1.3081707948003896e-06}, {"id": 423, "seek": 205268, "start": 2068.3599999999997, "end": 2074.8399999999997, "text": " You know some things may have pretty simple kind of causality right so for example", "tokens": [509, 458, 512, 721, 815, 362, 1238, 2199, 733, 295, 3302, 1860, 558, 370, 337, 1365], "temperature": 0.0, "avg_logprob": -0.16892761172670306, "compression_ratio": 1.3966480446927374, "no_speech_prob": 1.3081707948003896e-06}, {"id": 424, "seek": 205268, "start": 2077.68, "end": 2079.68, "text": " Let's have a look", "tokens": [961, 311, 362, 257, 574], "temperature": 0.0, "avg_logprob": -0.16892761172670306, "compression_ratio": 1.3966480446927374, "no_speech_prob": 1.3081707948003896e-06}, {"id": 425, "seek": 207968, "start": 2079.68, "end": 2081.68, "text": " at", "tokens": [412], "temperature": 0.0, "avg_logprob": -0.1777269382669468, "compression_ratio": 1.7198067632850242, "no_speech_prob": 8.9909087819251e-07}, {"id": 426, "seek": 207968, "start": 2081.68, "end": 2082.7999999999997, "text": " State holiday", "tokens": [4533, 9960], "temperature": 0.0, "avg_logprob": -0.1777269382669468, "compression_ratio": 1.7198067632850242, "no_speech_prob": 8.9909087819251e-07}, {"id": 427, "seek": 207968, "start": 2082.7999999999997, "end": 2084.3199999999997, "text": " You know", "tokens": [509, 458], "temperature": 0.0, "avg_logprob": -0.1777269382669468, "compression_ratio": 1.7198067632850242, "no_speech_prob": 8.9909087819251e-07}, {"id": 428, "seek": 207968, "start": 2084.3199999999997, "end": 2086.3199999999997, "text": " Maybe if something's a holiday", "tokens": [2704, 498, 746, 311, 257, 9960], "temperature": 0.0, "avg_logprob": -0.1777269382669468, "compression_ratio": 1.7198067632850242, "no_speech_prob": 8.9909087819251e-07}, {"id": 429, "seek": 207968, "start": 2086.96, "end": 2092.2799999999997, "text": " Then it's just a case of like okay at stores that are in the city. There's some behavior", "tokens": [1396, 309, 311, 445, 257, 1389, 295, 411, 1392, 412, 9512, 300, 366, 294, 264, 2307, 13, 821, 311, 512, 5223], "temperature": 0.0, "avg_logprob": -0.1777269382669468, "compression_ratio": 1.7198067632850242, "no_speech_prob": 8.9909087819251e-07}, {"id": 430, "seek": 207968, "start": 2092.2799999999997, "end": 2099.2, "text": " There's stores that are in the country. There's some other behavior, and that's about it. You know like maybe it's a pretty pretty simple", "tokens": [821, 311, 9512, 300, 366, 294, 264, 1941, 13, 821, 311, 512, 661, 5223, 11, 293, 300, 311, 466, 309, 13, 509, 458, 411, 1310, 309, 311, 257, 1238, 1238, 2199], "temperature": 0.0, "avg_logprob": -0.1777269382669468, "compression_ratio": 1.7198067632850242, "no_speech_prob": 8.9909087819251e-07}, {"id": 431, "seek": 207968, "start": 2099.8799999999997, "end": 2101.6, "text": " relationship", "tokens": [2480], "temperature": 0.0, "avg_logprob": -0.1777269382669468, "compression_ratio": 1.7198067632850242, "no_speech_prob": 8.9909087819251e-07}, {"id": 432, "seek": 207968, "start": 2101.6, "end": 2103.6, "text": " so like ideally", "tokens": [370, 411, 22915], "temperature": 0.0, "avg_logprob": -0.1777269382669468, "compression_ratio": 1.7198067632850242, "no_speech_prob": 8.9909087819251e-07}, {"id": 433, "seek": 207968, "start": 2103.6, "end": 2105.6, "text": " When you decide what?", "tokens": [1133, 291, 4536, 437, 30], "temperature": 0.0, "avg_logprob": -0.1777269382669468, "compression_ratio": 1.7198067632850242, "no_speech_prob": 8.9909087819251e-07}, {"id": 434, "seek": 207968, "start": 2106.3999999999996, "end": 2108.3999999999996, "text": " embedding size to use", "tokens": [12240, 3584, 2744, 281, 764], "temperature": 0.0, "avg_logprob": -0.1777269382669468, "compression_ratio": 1.7198067632850242, "no_speech_prob": 8.9909087819251e-07}, {"id": 435, "seek": 210840, "start": 2108.4, "end": 2111.6, "text": " You would kind of use your knowledge about", "tokens": [509, 576, 733, 295, 764, 428, 3601, 466], "temperature": 0.0, "avg_logprob": -0.19355201721191406, "compression_ratio": 1.584070796460177, "no_speech_prob": 3.520908080645313e-07}, {"id": 436, "seek": 210840, "start": 2112.2400000000002, "end": 2117.48, "text": " The domain to decide like how complex is the relationship and so how big?", "tokens": [440, 9274, 281, 4536, 411, 577, 3997, 307, 264, 2480, 293, 370, 577, 955, 30], "temperature": 0.0, "avg_logprob": -0.19355201721191406, "compression_ratio": 1.584070796460177, "no_speech_prob": 3.520908080645313e-07}, {"id": 437, "seek": 210840, "start": 2118.0, "end": 2120.8, "text": " Embedding do I need right in practice?", "tokens": [24234, 292, 3584, 360, 286, 643, 558, 294, 3124, 30], "temperature": 0.0, "avg_logprob": -0.19355201721191406, "compression_ratio": 1.584070796460177, "no_speech_prob": 3.520908080645313e-07}, {"id": 438, "seek": 210840, "start": 2121.56, "end": 2123.48, "text": " You almost never know that", "tokens": [509, 1920, 1128, 458, 300], "temperature": 0.0, "avg_logprob": -0.19355201721191406, "compression_ratio": 1.584070796460177, "no_speech_prob": 3.520908080645313e-07}, {"id": 439, "seek": 210840, "start": 2123.48, "end": 2130.34, "text": " Right like you would only know that because maybe somebody else has previously done that research and figured it out like in an LP", "tokens": [1779, 411, 291, 576, 787, 458, 300, 570, 1310, 2618, 1646, 575, 8046, 1096, 300, 2132, 293, 8932, 309, 484, 411, 294, 364, 38095], "temperature": 0.0, "avg_logprob": -0.19355201721191406, "compression_ratio": 1.584070796460177, "no_speech_prob": 3.520908080645313e-07}, {"id": 440, "seek": 210840, "start": 2131.7200000000003, "end": 2135.1, "text": " So in practice you probably need to use some", "tokens": [407, 294, 3124, 291, 1391, 643, 281, 764, 512], "temperature": 0.0, "avg_logprob": -0.19355201721191406, "compression_ratio": 1.584070796460177, "no_speech_prob": 3.520908080645313e-07}, {"id": 441, "seek": 213510, "start": 2135.1, "end": 2139.2599999999998, "text": " Rule of thumb okay, and then having tried your rule of thumb", "tokens": [27533, 295, 9298, 1392, 11, 293, 550, 1419, 3031, 428, 4978, 295, 9298], "temperature": 0.0, "avg_logprob": -0.17056590587169201, "compression_ratio": 1.7560975609756098, "no_speech_prob": 1.7061740891222144e-06}, {"id": 442, "seek": 213510, "start": 2139.74, "end": 2145.24, "text": " You could then maybe try a little bit higher and a little bit lower and see what helps so it's kind of experimental", "tokens": [509, 727, 550, 1310, 853, 257, 707, 857, 2946, 293, 257, 707, 857, 3126, 293, 536, 437, 3665, 370, 309, 311, 733, 295, 17069], "temperature": 0.0, "avg_logprob": -0.17056590587169201, "compression_ratio": 1.7560975609756098, "no_speech_prob": 1.7061740891222144e-06}, {"id": 443, "seek": 213510, "start": 2145.2599999999998, "end": 2147.8199999999997, "text": " So here's my rule of thumb my rule of thumb is", "tokens": [407, 510, 311, 452, 4978, 295, 9298, 452, 4978, 295, 9298, 307], "temperature": 0.0, "avg_logprob": -0.17056590587169201, "compression_ratio": 1.7560975609756098, "no_speech_prob": 1.7061740891222144e-06}, {"id": 444, "seek": 213510, "start": 2148.74, "end": 2150.74, "text": " look at how", "tokens": [574, 412, 577], "temperature": 0.0, "avg_logprob": -0.17056590587169201, "compression_ratio": 1.7560975609756098, "no_speech_prob": 1.7061740891222144e-06}, {"id": 445, "seek": 213510, "start": 2151.06, "end": 2156.14, "text": " how many discrete values the category has I eat a number of rows in the embedding matrix and", "tokens": [577, 867, 27706, 4190, 264, 7719, 575, 286, 1862, 257, 1230, 295, 13241, 294, 264, 12240, 3584, 8141, 293], "temperature": 0.0, "avg_logprob": -0.17056590587169201, "compression_ratio": 1.7560975609756098, "no_speech_prob": 1.7061740891222144e-06}, {"id": 446, "seek": 213510, "start": 2156.7, "end": 2159.58, "text": " Make the dimensionality of the embedding half of that", "tokens": [4387, 264, 10139, 1860, 295, 264, 12240, 3584, 1922, 295, 300], "temperature": 0.0, "avg_logprob": -0.17056590587169201, "compression_ratio": 1.7560975609756098, "no_speech_prob": 1.7061740891222144e-06}, {"id": 447, "seek": 213510, "start": 2160.18, "end": 2162.18, "text": " Right so for day of week", "tokens": [1779, 370, 337, 786, 295, 1243], "temperature": 0.0, "avg_logprob": -0.17056590587169201, "compression_ratio": 1.7560975609756098, "no_speech_prob": 1.7061740891222144e-06}, {"id": 448, "seek": 216218, "start": 2162.18, "end": 2167.22, "text": " Which is the second one? eight rows and four colors", "tokens": [3013, 307, 264, 1150, 472, 30, 3180, 13241, 293, 1451, 4577], "temperature": 0.0, "avg_logprob": -0.1659756024678548, "compression_ratio": 1.5833333333333333, "no_speech_prob": 4.785069904755801e-06}, {"id": 449, "seek": 216218, "start": 2169.74, "end": 2174.06, "text": " So here it is there right the number of categories divided by two", "tokens": [407, 510, 309, 307, 456, 558, 264, 1230, 295, 10479, 6666, 538, 732], "temperature": 0.0, "avg_logprob": -0.1659756024678548, "compression_ratio": 1.5833333333333333, "no_speech_prob": 4.785069904755801e-06}, {"id": 450, "seek": 216218, "start": 2174.54, "end": 2179.02, "text": " But then I say don't go more than 50 right so here you can see for store", "tokens": [583, 550, 286, 584, 500, 380, 352, 544, 813, 2625, 558, 370, 510, 291, 393, 536, 337, 3531], "temperature": 0.0, "avg_logprob": -0.1659756024678548, "compression_ratio": 1.5833333333333333, "no_speech_prob": 4.785069904755801e-06}, {"id": 451, "seek": 216218, "start": 2179.02, "end": 2182.58, "text": " There's a thousand stores only have a dimensionality of 50 why 50", "tokens": [821, 311, 257, 4714, 9512, 787, 362, 257, 10139, 1860, 295, 2625, 983, 2625], "temperature": 0.0, "avg_logprob": -0.1659756024678548, "compression_ratio": 1.5833333333333333, "no_speech_prob": 4.785069904755801e-06}, {"id": 452, "seek": 216218, "start": 2182.58, "end": 2188.18, "text": " I don't know it seems to have worked okay so far like you may find you need something a little different", "tokens": [286, 500, 380, 458, 309, 2544, 281, 362, 2732, 1392, 370, 1400, 411, 291, 815, 915, 291, 643, 746, 257, 707, 819], "temperature": 0.0, "avg_logprob": -0.1659756024678548, "compression_ratio": 1.5833333333333333, "no_speech_prob": 4.785069904755801e-06}, {"id": 453, "seek": 218818, "start": 2188.18, "end": 2194.2599999999998, "text": " Actually for the Ecuadorian groceries competition. You know I haven't really tried playing with this", "tokens": [5135, 337, 264, 41558, 952, 31391, 6211, 13, 509, 458, 286, 2378, 380, 534, 3031, 2433, 365, 341], "temperature": 0.0, "avg_logprob": -0.2424610392252604, "compression_ratio": 1.5336538461538463, "no_speech_prob": 9.080084964807611e-06}, {"id": 454, "seek": 218818, "start": 2194.2599999999998, "end": 2197.08, "text": " But I think we may need some larger embedding sizes", "tokens": [583, 286, 519, 321, 815, 643, 512, 4833, 12240, 3584, 11602], "temperature": 0.0, "avg_logprob": -0.2424610392252604, "compression_ratio": 1.5336538461538463, "no_speech_prob": 9.080084964807611e-06}, {"id": 455, "seek": 218818, "start": 2199.74, "end": 2202.5, "text": " But it's something to fiddle with Prince can you pass that left", "tokens": [583, 309, 311, 746, 281, 24553, 2285, 365, 9821, 393, 291, 1320, 300, 1411], "temperature": 0.0, "avg_logprob": -0.2424610392252604, "compression_ratio": 1.5336538461538463, "no_speech_prob": 9.080084964807611e-06}, {"id": 456, "seek": 218818, "start": 2204.46, "end": 2207.98, "text": " So as your variables the cardinality size becomes larger and larger", "tokens": [407, 382, 428, 9102, 264, 2920, 259, 1860, 2744, 3643, 4833, 293, 4833], "temperature": 0.0, "avg_logprob": -0.2424610392252604, "compression_ratio": 1.5336538461538463, "no_speech_prob": 9.080084964807611e-06}, {"id": 457, "seek": 218818, "start": 2209.2999999999997, "end": 2211.2999999999997, "text": " You're creating more and more like", "tokens": [509, 434, 4084, 544, 293, 544, 411], "temperature": 0.0, "avg_logprob": -0.2424610392252604, "compression_ratio": 1.5336538461538463, "no_speech_prob": 9.080084964807611e-06}, {"id": 458, "seek": 221130, "start": 2211.3, "end": 2218.6800000000003, "text": " Bigger and bigger wider very much serious on you therefore massively risking overfitting which is introducing so many parameters", "tokens": [5429, 1321, 293, 3801, 11842, 588, 709, 3156, 322, 291, 4412, 29379, 45235, 670, 69, 2414, 597, 307, 15424, 370, 867, 9834], "temperature": 0.0, "avg_logprob": -0.29459814423496283, "compression_ratio": 1.8501742160278745, "no_speech_prob": 8.397808414883912e-06}, {"id": 459, "seek": 221130, "start": 2218.6800000000003, "end": 2220.6800000000003, "text": " The model can never possibly capture all that", "tokens": [440, 2316, 393, 1128, 6264, 7983, 439, 300], "temperature": 0.0, "avg_logprob": -0.29459814423496283, "compression_ratio": 1.8501742160278745, "no_speech_prob": 8.397808414883912e-06}, {"id": 460, "seek": 221130, "start": 2221.1000000000004, "end": 2224.26, "text": " Variation that's your data is absolutely huge. That's a great question", "tokens": [32511, 399, 300, 311, 428, 1412, 307, 3122, 2603, 13, 663, 311, 257, 869, 1168], "temperature": 0.0, "avg_logprob": -0.29459814423496283, "compression_ratio": 1.8501742160278745, "no_speech_prob": 8.397808414883912e-06}, {"id": 461, "seek": 221130, "start": 2224.26, "end": 2232.1800000000003, "text": " And so let me remind you about my kind of like golden rule with the difference between modern machine learning and old machine learning", "tokens": [400, 370, 718, 385, 4160, 291, 466, 452, 733, 295, 411, 9729, 4978, 365, 264, 2649, 1296, 4363, 3479, 2539, 293, 1331, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.29459814423496283, "compression_ratio": 1.8501742160278745, "no_speech_prob": 8.397808414883912e-06}, {"id": 462, "seek": 223218, "start": 2232.18, "end": 2241.1, "text": " In all machine learning we control complexity by reducing the number of parameters in modern machine learning we control complexity by regularization", "tokens": [682, 439, 3479, 2539, 321, 1969, 14024, 538, 12245, 264, 1230, 295, 9834, 294, 4363, 3479, 2539, 321, 1969, 14024, 538, 3890, 2144], "temperature": 0.0, "avg_logprob": -0.19740257682381096, "compression_ratio": 1.9423076923076923, "no_speech_prob": 4.222802544973092e-06}, {"id": 463, "seek": 223218, "start": 2241.74, "end": 2243.74, "text": " So the short answer is no", "tokens": [407, 264, 2099, 1867, 307, 572], "temperature": 0.0, "avg_logprob": -0.19740257682381096, "compression_ratio": 1.9423076923076923, "no_speech_prob": 4.222802544973092e-06}, {"id": 464, "seek": 223218, "start": 2243.74, "end": 2250.2599999999998, "text": " I'm not concerned about overfitting because the way I avoid overfitting is not by reducing the number of parameters", "tokens": [286, 478, 406, 5922, 466, 670, 69, 2414, 570, 264, 636, 286, 5042, 670, 69, 2414, 307, 406, 538, 12245, 264, 1230, 295, 9834], "temperature": 0.0, "avg_logprob": -0.19740257682381096, "compression_ratio": 1.9423076923076923, "no_speech_prob": 4.222802544973092e-06}, {"id": 465, "seek": 223218, "start": 2250.46, "end": 2252.7799999999997, "text": " but by increasing my dropout or", "tokens": [457, 538, 5662, 452, 3270, 346, 420], "temperature": 0.0, "avg_logprob": -0.19740257682381096, "compression_ratio": 1.9423076923076923, "no_speech_prob": 4.222802544973092e-06}, {"id": 466, "seek": 223218, "start": 2253.4199999999996, "end": 2255.4199999999996, "text": " increasing my weight decay", "tokens": [5662, 452, 3364, 21039], "temperature": 0.0, "avg_logprob": -0.19740257682381096, "compression_ratio": 1.9423076923076923, "no_speech_prob": 4.222802544973092e-06}, {"id": 467, "seek": 223218, "start": 2255.5, "end": 2257.4199999999996, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.19740257682381096, "compression_ratio": 1.9423076923076923, "no_speech_prob": 4.222802544973092e-06}, {"id": 468, "seek": 223218, "start": 2257.4199999999996, "end": 2260.1, "text": " Now having said that like there's no point using", "tokens": [823, 1419, 848, 300, 411, 456, 311, 572, 935, 1228], "temperature": 0.0, "avg_logprob": -0.19740257682381096, "compression_ratio": 1.9423076923076923, "no_speech_prob": 4.222802544973092e-06}, {"id": 469, "seek": 226010, "start": 2260.1, "end": 2261.8199999999997, "text": " more", "tokens": [544], "temperature": 0.0, "avg_logprob": -0.17694313185555594, "compression_ratio": 1.6081081081081081, "no_speech_prob": 1.994712874875404e-06}, {"id": 470, "seek": 226010, "start": 2261.8199999999997, "end": 2266.94, "text": " parameters for a particular embedding than I need like because regularization", "tokens": [9834, 337, 257, 1729, 12240, 3584, 813, 286, 643, 411, 570, 3890, 2144], "temperature": 0.0, "avg_logprob": -0.17694313185555594, "compression_ratio": 1.6081081081081081, "no_speech_prob": 1.994712874875404e-06}, {"id": 471, "seek": 226010, "start": 2268.2599999999998, "end": 2273.94, "text": " Like is penalizing a model by giving it like more random data or by actually penalizing weights", "tokens": [1743, 307, 13661, 3319, 257, 2316, 538, 2902, 309, 411, 544, 4974, 1412, 420, 538, 767, 13661, 3319, 17443], "temperature": 0.0, "avg_logprob": -0.17694313185555594, "compression_ratio": 1.6081081081081081, "no_speech_prob": 1.994712874875404e-06}, {"id": 472, "seek": 226010, "start": 2274.58, "end": 2277.14, "text": " So we like we'd rather not use more than we have to", "tokens": [407, 321, 411, 321, 1116, 2831, 406, 764, 544, 813, 321, 362, 281], "temperature": 0.0, "avg_logprob": -0.17694313185555594, "compression_ratio": 1.6081081081081081, "no_speech_prob": 1.994712874875404e-06}, {"id": 473, "seek": 226010, "start": 2279.06, "end": 2287.66, "text": " But the kind of my general rule of thumb for designing an architecture is to you know be generous on the side of the number of", "tokens": [583, 264, 733, 295, 452, 2674, 4978, 295, 9298, 337, 14685, 364, 9482, 307, 281, 291, 458, 312, 14537, 322, 264, 1252, 295, 264, 1230, 295], "temperature": 0.0, "avg_logprob": -0.17694313185555594, "compression_ratio": 1.6081081081081081, "no_speech_prob": 1.994712874875404e-06}, {"id": 474, "seek": 228766, "start": 2287.66, "end": 2293.5, "text": " Parameters, but yeah in this case if after doing some work. We kind of felt like you know what?", "tokens": [34882, 6202, 11, 457, 1338, 294, 341, 1389, 498, 934, 884, 512, 589, 13, 492, 733, 295, 2762, 411, 291, 458, 437, 30], "temperature": 0.0, "avg_logprob": -0.151673885660434, "compression_ratio": 1.6559139784946237, "no_speech_prob": 2.2252709186432185e-06}, {"id": 475, "seek": 228766, "start": 2294.58, "end": 2297.66, "text": " The store doesn't actually seem to be that", "tokens": [440, 3531, 1177, 380, 767, 1643, 281, 312, 300], "temperature": 0.0, "avg_logprob": -0.151673885660434, "compression_ratio": 1.6559139784946237, "no_speech_prob": 2.2252709186432185e-06}, {"id": 476, "seek": 228766, "start": 2298.66, "end": 2302.74, "text": " Important then I might manually go and like change this to make it smaller", "tokens": [42908, 550, 286, 1062, 16945, 352, 293, 411, 1319, 341, 281, 652, 309, 4356], "temperature": 0.0, "avg_logprob": -0.151673885660434, "compression_ratio": 1.6559139784946237, "no_speech_prob": 2.2252709186432185e-06}, {"id": 477, "seek": 228766, "start": 2302.8999999999996, "end": 2306.7599999999998, "text": " You know or if I was really finding there's not enough data here", "tokens": [509, 458, 420, 498, 286, 390, 534, 5006, 456, 311, 406, 1547, 1412, 510], "temperature": 0.0, "avg_logprob": -0.151673885660434, "compression_ratio": 1.6559139784946237, "no_speech_prob": 2.2252709186432185e-06}, {"id": 478, "seek": 228766, "start": 2306.7599999999998, "end": 2311.2599999999998, "text": " I'm either over fitting or I'm using more regularization than comfortable with again", "tokens": [286, 478, 2139, 670, 15669, 420, 286, 478, 1228, 544, 3890, 2144, 813, 4619, 365, 797], "temperature": 0.0, "avg_logprob": -0.151673885660434, "compression_ratio": 1.6559139784946237, "no_speech_prob": 2.2252709186432185e-06}, {"id": 479, "seek": 228766, "start": 2311.2599999999998, "end": 2315.7799999999997, "text": " You know then you might go back, but I would always start with like being generous with parameters", "tokens": [509, 458, 550, 291, 1062, 352, 646, 11, 457, 286, 576, 1009, 722, 365, 411, 885, 14537, 365, 9834], "temperature": 0.0, "avg_logprob": -0.151673885660434, "compression_ratio": 1.6559139784946237, "no_speech_prob": 2.2252709186432185e-06}, {"id": 480, "seek": 231578, "start": 2315.78, "end": 2317.78, "text": " And yeah in this case", "tokens": [400, 1338, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.16579310748041892, "compression_ratio": 1.611353711790393, "no_speech_prob": 1.8448167793394532e-06}, {"id": 481, "seek": 231578, "start": 2318.6200000000003, "end": 2320.6200000000003, "text": " This model turned out pretty good", "tokens": [639, 2316, 3574, 484, 1238, 665], "temperature": 0.0, "avg_logprob": -0.16579310748041892, "compression_ratio": 1.611353711790393, "no_speech_prob": 1.8448167793394532e-06}, {"id": 482, "seek": 231578, "start": 2321.6000000000004, "end": 2323.94, "text": " Okay, so now we've got a list of tuples", "tokens": [1033, 11, 370, 586, 321, 600, 658, 257, 1329, 295, 2604, 2622], "temperature": 0.0, "avg_logprob": -0.16579310748041892, "compression_ratio": 1.611353711790393, "no_speech_prob": 1.8448167793394532e-06}, {"id": 483, "seek": 231578, "start": 2324.46, "end": 2327.42, "text": " Containing the number of rows and columns of each of our embedding matrices", "tokens": [4839, 3686, 264, 1230, 295, 13241, 293, 13766, 295, 1184, 295, 527, 12240, 3584, 32284], "temperature": 0.0, "avg_logprob": -0.16579310748041892, "compression_ratio": 1.611353711790393, "no_speech_prob": 1.8448167793394532e-06}, {"id": 484, "seek": 231578, "start": 2327.42, "end": 2333.0, "text": " And so when we call get learner to create our neural net that's the first thing we pass in", "tokens": [400, 370, 562, 321, 818, 483, 33347, 281, 1884, 527, 18161, 2533, 300, 311, 264, 700, 551, 321, 1320, 294], "temperature": 0.0, "avg_logprob": -0.16579310748041892, "compression_ratio": 1.611353711790393, "no_speech_prob": 1.8448167793394532e-06}, {"id": 485, "seek": 231578, "start": 2333.2200000000003, "end": 2337.28, "text": " That is how big is each of our embeddings, okay?", "tokens": [663, 307, 577, 955, 307, 1184, 295, 527, 12240, 29432, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.16579310748041892, "compression_ratio": 1.611353711790393, "no_speech_prob": 1.8448167793394532e-06}, {"id": 486, "seek": 231578, "start": 2337.9, "end": 2341.5, "text": " And then we tell it how many continuous variables we have", "tokens": [400, 550, 321, 980, 309, 577, 867, 10957, 9102, 321, 362], "temperature": 0.0, "avg_logprob": -0.16579310748041892, "compression_ratio": 1.611353711790393, "no_speech_prob": 1.8448167793394532e-06}, {"id": 487, "seek": 234150, "start": 2341.5, "end": 2348.72, "text": " We tell it how many activations to create for each layer, and we tell it what dropout to use for each layer", "tokens": [492, 980, 309, 577, 867, 2430, 763, 281, 1884, 337, 1184, 4583, 11, 293, 321, 980, 309, 437, 3270, 346, 281, 764, 337, 1184, 4583], "temperature": 0.0, "avg_logprob": -0.22502103718844327, "compression_ratio": 1.6195121951219513, "no_speech_prob": 3.500840193737531e-06}, {"id": 488, "seek": 234150, "start": 2349.38, "end": 2351.82, "text": " Okay, and so then we can go ahead and", "tokens": [1033, 11, 293, 370, 550, 321, 393, 352, 2286, 293], "temperature": 0.0, "avg_logprob": -0.22502103718844327, "compression_ratio": 1.6195121951219513, "no_speech_prob": 3.500840193737531e-06}, {"id": 489, "seek": 234150, "start": 2354.1, "end": 2356.56, "text": " Call fit okay", "tokens": [7807, 3318, 1392], "temperature": 0.0, "avg_logprob": -0.22502103718844327, "compression_ratio": 1.6195121951219513, "no_speech_prob": 3.500840193737531e-06}, {"id": 490, "seek": 234150, "start": 2358.18, "end": 2366.28, "text": " So then we fit for a while and we're kind of getting something around the the point one mark all right, so I tried", "tokens": [407, 550, 321, 3318, 337, 257, 1339, 293, 321, 434, 733, 295, 1242, 746, 926, 264, 264, 935, 472, 1491, 439, 558, 11, 370, 286, 3031], "temperature": 0.0, "avg_logprob": -0.22502103718844327, "compression_ratio": 1.6195121951219513, "no_speech_prob": 3.500840193737531e-06}, {"id": 491, "seek": 236628, "start": 2366.28, "end": 2374.0, "text": " Running this on the test set and I submitted it to Kaggle during the week actually last week and", "tokens": [28136, 341, 322, 264, 1500, 992, 293, 286, 14405, 309, 281, 48751, 22631, 1830, 264, 1243, 767, 1036, 1243, 293], "temperature": 0.0, "avg_logprob": -0.18405458960734622, "compression_ratio": 1.5348837209302326, "no_speech_prob": 2.769394768620259e-06}, {"id": 492, "seek": 236628, "start": 2377.44, "end": 2381.92, "text": " Here it is okay private score 107", "tokens": [1692, 309, 307, 1392, 4551, 6175, 1266, 22], "temperature": 0.0, "avg_logprob": -0.18405458960734622, "compression_ratio": 1.5348837209302326, "no_speech_prob": 2.769394768620259e-06}, {"id": 493, "seek": 236628, "start": 2382.88, "end": 2384.36, "text": " public score", "tokens": [1908, 6175], "temperature": 0.0, "avg_logprob": -0.18405458960734622, "compression_ratio": 1.5348837209302326, "no_speech_prob": 2.769394768620259e-06}, {"id": 494, "seek": 236628, "start": 2384.36, "end": 2385.6800000000003, "text": " 103", "tokens": [48784], "temperature": 0.0, "avg_logprob": -0.18405458960734622, "compression_ratio": 1.5348837209302326, "no_speech_prob": 2.769394768620259e-06}, {"id": 495, "seek": 238568, "start": 2385.68, "end": 2397.16, "text": " Okay, so let's have a look and see how that would go so 107 private 103 public so let's start on public which is 103", "tokens": [1033, 11, 370, 718, 311, 362, 257, 574, 293, 536, 577, 300, 576, 352, 370, 1266, 22, 4551, 48784, 1908, 370, 718, 311, 722, 322, 1908, 597, 307, 48784], "temperature": 0.0, "avg_logprob": -0.2352132797241211, "compression_ratio": 1.3015873015873016, "no_speech_prob": 9.57079805630201e-07}, {"id": 496, "seek": 238568, "start": 2400.7599999999998, "end": 2409.72, "text": " Not there out of 3000 got to go back a long way", "tokens": [1726, 456, 484, 295, 20984, 658, 281, 352, 646, 257, 938, 636], "temperature": 0.0, "avg_logprob": -0.2352132797241211, "compression_ratio": 1.3015873015873016, "no_speech_prob": 9.57079805630201e-07}, {"id": 497, "seek": 240972, "start": 2409.72, "end": 2411.72, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.3150332207773246, "compression_ratio": 1.25, "no_speech_prob": 7.934472137094417e-07}, {"id": 498, "seek": 240972, "start": 2414.0, "end": 2421.24, "text": " There it is 103 okay 340th, huh, that's not good", "tokens": [821, 309, 307, 48784, 1392, 805, 5254, 392, 11, 7020, 11, 300, 311, 406, 665], "temperature": 0.0, "avg_logprob": -0.3150332207773246, "compression_ratio": 1.25, "no_speech_prob": 7.934472137094417e-07}, {"id": 499, "seek": 240972, "start": 2423.16, "end": 2427.08, "text": " So on the public leaderboard 340th, let's try the private leaderboard", "tokens": [407, 322, 264, 1908, 5263, 3787, 805, 5254, 392, 11, 718, 311, 853, 264, 4551, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.3150332207773246, "compression_ratio": 1.25, "no_speech_prob": 7.934472137094417e-07}, {"id": 500, "seek": 240972, "start": 2429.16, "end": 2431.16, "text": " Which is 107 oh", "tokens": [3013, 307, 1266, 22, 1954], "temperature": 0.0, "avg_logprob": -0.3150332207773246, "compression_ratio": 1.25, "no_speech_prob": 7.934472137094417e-07}, {"id": 501, "seek": 240972, "start": 2433.8799999999997, "end": 2435.8799999999997, "text": " Fifth", "tokens": [33588], "temperature": 0.0, "avg_logprob": -0.3150332207773246, "compression_ratio": 1.25, "no_speech_prob": 7.934472137094417e-07}, {"id": 502, "seek": 243588, "start": 2435.88, "end": 2439.1600000000003, "text": " So like hopefully you're now thinking oh", "tokens": [407, 411, 4696, 291, 434, 586, 1953, 1954], "temperature": 0.0, "avg_logprob": -0.18826215607779367, "compression_ratio": 1.6044444444444443, "no_speech_prob": 6.375527732416231e-07}, {"id": 503, "seek": 243588, "start": 2439.96, "end": 2442.48, "text": " There are some Kaggle competitions finishing soon", "tokens": [821, 366, 512, 48751, 22631, 26185, 12693, 2321], "temperature": 0.0, "avg_logprob": -0.18826215607779367, "compression_ratio": 1.6044444444444443, "no_speech_prob": 6.375527732416231e-07}, {"id": 504, "seek": 243588, "start": 2442.48, "end": 2446.36, "text": " Which I entered and I spent a lot of time trying to get good results on the public leaderboard", "tokens": [3013, 286, 9065, 293, 286, 4418, 257, 688, 295, 565, 1382, 281, 483, 665, 3542, 322, 264, 1908, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.18826215607779367, "compression_ratio": 1.6044444444444443, "no_speech_prob": 6.375527732416231e-07}, {"id": 505, "seek": 243588, "start": 2446.36, "end": 2453.36, "text": " I wonder if that was a good idea and the answer is no it won't right the Kaggle public leaderboard is", "tokens": [286, 2441, 498, 300, 390, 257, 665, 1558, 293, 264, 1867, 307, 572, 309, 1582, 380, 558, 264, 48751, 22631, 1908, 5263, 3787, 307], "temperature": 0.0, "avg_logprob": -0.18826215607779367, "compression_ratio": 1.6044444444444443, "no_speech_prob": 6.375527732416231e-07}, {"id": 506, "seek": 243588, "start": 2454.36, "end": 2459.7200000000003, "text": " Not meant to be a replacement for your carefully developed validation set", "tokens": [1726, 4140, 281, 312, 257, 14419, 337, 428, 7500, 4743, 24071, 992], "temperature": 0.0, "avg_logprob": -0.18826215607779367, "compression_ratio": 1.6044444444444443, "no_speech_prob": 6.375527732416231e-07}, {"id": 507, "seek": 245972, "start": 2459.72, "end": 2467.2, "text": " All right, so for example if you're doing the iceberg competition right which ones are ships which ones are icebergs", "tokens": [1057, 558, 11, 370, 337, 1365, 498, 291, 434, 884, 264, 38880, 6211, 558, 597, 2306, 366, 11434, 597, 2306, 366, 38880, 82], "temperature": 0.0, "avg_logprob": -0.22071075439453125, "compression_ratio": 1.5473684210526315, "no_speech_prob": 2.918933432738413e-07}, {"id": 508, "seek": 245972, "start": 2467.56, "end": 2469.56, "text": " Then they've actually put", "tokens": [1396, 436, 600, 767, 829], "temperature": 0.0, "avg_logprob": -0.22071075439453125, "compression_ratio": 1.5473684210526315, "no_speech_prob": 2.918933432738413e-07}, {"id": 509, "seek": 245972, "start": 2470.12, "end": 2475.9599999999996, "text": " Something like 4,000 synthetic images into the public leaderboard and none into the private leaderboard", "tokens": [6595, 411, 1017, 11, 1360, 23420, 5267, 666, 264, 1908, 5263, 3787, 293, 6022, 666, 264, 4551, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.22071075439453125, "compression_ratio": 1.5473684210526315, "no_speech_prob": 2.918933432738413e-07}, {"id": 510, "seek": 245972, "start": 2477.04, "end": 2479.04, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.22071075439453125, "compression_ratio": 1.5473684210526315, "no_speech_prob": 2.918933432738413e-07}, {"id": 511, "seek": 245972, "start": 2480.4399999999996, "end": 2482.72, "text": " This is one of the really good kind of", "tokens": [639, 307, 472, 295, 264, 534, 665, 733, 295], "temperature": 0.0, "avg_logprob": -0.22071075439453125, "compression_ratio": 1.5473684210526315, "no_speech_prob": 2.918933432738413e-07}, {"id": 512, "seek": 248272, "start": 2482.72, "end": 2490.08, "text": " Things that tests you out on Kaggle is like are you creating a good validation set and are you trusting it?", "tokens": [9514, 300, 6921, 291, 484, 322, 48751, 22631, 307, 411, 366, 291, 4084, 257, 665, 24071, 992, 293, 366, 291, 28235, 309, 30], "temperature": 0.0, "avg_logprob": -0.18005976112939978, "compression_ratio": 1.7435897435897436, "no_speech_prob": 2.225260459454148e-06}, {"id": 513, "seek": 248272, "start": 2490.08, "end": 2492.08, "text": " right because if you're trusting your", "tokens": [558, 570, 498, 291, 434, 28235, 428], "temperature": 0.0, "avg_logprob": -0.18005976112939978, "compression_ratio": 1.7435897435897436, "no_speech_prob": 2.225260459454148e-06}, {"id": 514, "seek": 248272, "start": 2492.48, "end": 2493.8399999999997, "text": " leaderboard feedback", "tokens": [5263, 3787, 5824], "temperature": 0.0, "avg_logprob": -0.18005976112939978, "compression_ratio": 1.7435897435897436, "no_speech_prob": 2.225260459454148e-06}, {"id": 515, "seek": 248272, "start": 2493.8399999999997, "end": 2501.9199999999996, "text": " More than your validation feedback then you may find yourself in 350th place when you thought you're in fifth, right?", "tokens": [5048, 813, 428, 24071, 5824, 550, 291, 815, 915, 1803, 294, 18065, 392, 1081, 562, 291, 1194, 291, 434, 294, 9266, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18005976112939978, "compression_ratio": 1.7435897435897436, "no_speech_prob": 2.225260459454148e-06}, {"id": 516, "seek": 248272, "start": 2502.7999999999997, "end": 2509.3999999999996, "text": " So in this case we actually had a pretty good validation set right because as you can see it's saying like somewhere around", "tokens": [407, 294, 341, 1389, 321, 767, 632, 257, 1238, 665, 24071, 992, 558, 570, 382, 291, 393, 536, 309, 311, 1566, 411, 4079, 926], "temperature": 0.0, "avg_logprob": -0.18005976112939978, "compression_ratio": 1.7435897435897436, "no_speech_prob": 2.225260459454148e-06}, {"id": 517, "seek": 250940, "start": 2509.4, "end": 2514.04, "text": " Point one and we actually did get somewhere around point one", "tokens": [12387, 472, 293, 321, 767, 630, 483, 4079, 926, 935, 472], "temperature": 0.0, "avg_logprob": -0.21581160161913054, "compression_ratio": 1.7876106194690264, "no_speech_prob": 9.721521792016574e-07}, {"id": 518, "seek": 250940, "start": 2514.76, "end": 2520.4, "text": " Okay, and so in this case the validation set that's our the public leaderboard in this competition was", "tokens": [1033, 11, 293, 370, 294, 341, 1389, 264, 24071, 992, 300, 311, 527, 264, 1908, 5263, 3787, 294, 341, 6211, 390], "temperature": 0.0, "avg_logprob": -0.21581160161913054, "compression_ratio": 1.7876106194690264, "no_speech_prob": 9.721521792016574e-07}, {"id": 519, "seek": 250940, "start": 2521.52, "end": 2523.52, "text": " Entirely useless yeah", "tokens": [3951, 621, 356, 14115, 1338], "temperature": 0.0, "avg_logprob": -0.21581160161913054, "compression_ratio": 1.7876106194690264, "no_speech_prob": 9.721521792016574e-07}, {"id": 520, "seek": 250940, "start": 2524.28, "end": 2526.28, "text": " Can you use the box please?", "tokens": [1664, 291, 764, 264, 2424, 1767, 30], "temperature": 0.0, "avg_logprob": -0.21581160161913054, "compression_ratio": 1.7876106194690264, "no_speech_prob": 9.721521792016574e-07}, {"id": 521, "seek": 250940, "start": 2527.32, "end": 2529.32, "text": " so in regards to that how", "tokens": [370, 294, 14258, 281, 300, 577], "temperature": 0.0, "avg_logprob": -0.21581160161913054, "compression_ratio": 1.7876106194690264, "no_speech_prob": 9.721521792016574e-07}, {"id": 522, "seek": 250940, "start": 2529.76, "end": 2535.28, "text": " much does the top of the public leaderboard actually correspond to the top of the private leaderboard because in the", "tokens": [709, 775, 264, 1192, 295, 264, 1908, 5263, 3787, 767, 6805, 281, 264, 1192, 295, 264, 4551, 5263, 3787, 570, 294, 264], "temperature": 0.0, "avg_logprob": -0.21581160161913054, "compression_ratio": 1.7876106194690264, "no_speech_prob": 9.721521792016574e-07}, {"id": 523, "seek": 253528, "start": 2535.28, "end": 2540.88, "text": " in the churn prediction challenge, there's like four people who are just", "tokens": [294, 264, 417, 925, 17630, 3430, 11, 456, 311, 411, 1451, 561, 567, 366, 445], "temperature": 0.0, "avg_logprob": -0.2786688164098939, "compression_ratio": 1.4816753926701571, "no_speech_prob": 2.684186938495259e-06}, {"id": 524, "seek": 253528, "start": 2541.7200000000003, "end": 2548.1200000000003, "text": " Completely above everyone else it totally depends you know like if they randomly", "tokens": [39978, 3673, 1518, 1646, 309, 3879, 5946, 291, 458, 411, 498, 436, 16979], "temperature": 0.0, "avg_logprob": -0.2786688164098939, "compression_ratio": 1.4816753926701571, "no_speech_prob": 2.684186938495259e-06}, {"id": 525, "seek": 253528, "start": 2548.92, "end": 2550.92, "text": " Sampled the public and private leaderboard", "tokens": [4832, 15551, 264, 1908, 293, 4551, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.2786688164098939, "compression_ratio": 1.4816753926701571, "no_speech_prob": 2.684186938495259e-06}, {"id": 526, "seek": 253528, "start": 2551.36, "end": 2553.36, "text": " Then it should be extremely indicative", "tokens": [1396, 309, 820, 312, 4664, 47513], "temperature": 0.0, "avg_logprob": -0.2786688164098939, "compression_ratio": 1.4816753926701571, "no_speech_prob": 2.684186938495259e-06}, {"id": 527, "seek": 253528, "start": 2554.0, "end": 2555.96, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.2786688164098939, "compression_ratio": 1.4816753926701571, "no_speech_prob": 2.684186938495259e-06}, {"id": 528, "seek": 253528, "start": 2555.96, "end": 2558.6400000000003, "text": " But it might not be right so in this case", "tokens": [583, 309, 1062, 406, 312, 558, 370, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.2786688164098939, "compression_ratio": 1.4816753926701571, "no_speech_prob": 2.684186938495259e-06}, {"id": 529, "seek": 255864, "start": 2558.64, "end": 2563.08, "text": " I was crushed", "tokens": [286, 390, 19889], "temperature": 0.0, "avg_logprob": -0.3398205497048118, "compression_ratio": 1.4527027027027026, "no_speech_prob": 3.905447101715254e-06}, {"id": 530, "seek": 255864, "start": 2564.08, "end": 2570.48, "text": " Here it comes so in this case the person who was second on the public leaderboard did end up winning", "tokens": [1692, 309, 1487, 370, 294, 341, 1389, 264, 954, 567, 390, 1150, 322, 264, 1908, 5263, 3787, 630, 917, 493, 8224], "temperature": 0.0, "avg_logprob": -0.3398205497048118, "compression_ratio": 1.4527027027027026, "no_speech_prob": 3.905447101715254e-06}, {"id": 531, "seek": 255864, "start": 2573.56, "end": 2575.56, "text": " SDNT", "tokens": [14638, 30817], "temperature": 0.0, "avg_logprob": -0.3398205497048118, "compression_ratio": 1.4527027027027026, "no_speech_prob": 3.905447101715254e-06}, {"id": 532, "seek": 255864, "start": 2579.8399999999997, "end": 2587.56, "text": " Came seventh right so in fact you can see the little green thing here right where else this guy", "tokens": [36042, 17875, 558, 370, 294, 1186, 291, 393, 536, 264, 707, 3092, 551, 510, 558, 689, 1646, 341, 2146], "temperature": 0.0, "avg_logprob": -0.3398205497048118, "compression_ratio": 1.4527027027027026, "no_speech_prob": 3.905447101715254e-06}, {"id": 533, "seek": 258756, "start": 2587.56, "end": 2589.56, "text": " jumped 96 places", "tokens": [13864, 24124, 3190], "temperature": 0.0, "avg_logprob": -0.16494502836060757, "compression_ratio": 1.7755905511811023, "no_speech_prob": 1.9333488125994336e-06}, {"id": 534, "seek": 258756, "start": 2590.44, "end": 2596.32, "text": " If we had entered with the neural net we just looked at we would have jumped 350 places so it yeah", "tokens": [759, 321, 632, 9065, 365, 264, 18161, 2533, 321, 445, 2956, 412, 321, 576, 362, 13864, 18065, 3190, 370, 309, 1338], "temperature": 0.0, "avg_logprob": -0.16494502836060757, "compression_ratio": 1.7755905511811023, "no_speech_prob": 1.9333488125994336e-06}, {"id": 535, "seek": 258756, "start": 2596.48, "end": 2599.12, "text": " It just depends and so often like", "tokens": [467, 445, 5946, 293, 370, 2049, 411], "temperature": 0.0, "avg_logprob": -0.16494502836060757, "compression_ratio": 1.7755905511811023, "no_speech_prob": 1.9333488125994336e-06}, {"id": 536, "seek": 258756, "start": 2599.84, "end": 2601.44, "text": " You can figure out", "tokens": [509, 393, 2573, 484], "temperature": 0.0, "avg_logprob": -0.16494502836060757, "compression_ratio": 1.7755905511811023, "no_speech_prob": 1.9333488125994336e-06}, {"id": 537, "seek": 258756, "start": 2601.44, "end": 2607.96, "text": " Where the the public leaderboard like sometimes they'll tell you the public leaderboard was randomly sampled sometimes. They'll tell you it's not", "tokens": [2305, 264, 264, 1908, 5263, 3787, 411, 2171, 436, 603, 980, 291, 264, 1908, 5263, 3787, 390, 16979, 3247, 15551, 2171, 13, 814, 603, 980, 291, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.16494502836060757, "compression_ratio": 1.7755905511811023, "no_speech_prob": 1.9333488125994336e-06}, {"id": 538, "seek": 258756, "start": 2608.56, "end": 2610.68, "text": " Generally you have to figure it out by looking at", "tokens": [21082, 291, 362, 281, 2573, 309, 484, 538, 1237, 412], "temperature": 0.0, "avg_logprob": -0.16494502836060757, "compression_ratio": 1.7755905511811023, "no_speech_prob": 1.9333488125994336e-06}, {"id": 539, "seek": 258756, "start": 2611.44, "end": 2616.16, "text": " the correlation between your validation set results and the public leaderboard results", "tokens": [264, 20009, 1296, 428, 24071, 992, 3542, 293, 264, 1908, 5263, 3787, 3542], "temperature": 0.0, "avg_logprob": -0.16494502836060757, "compression_ratio": 1.7755905511811023, "no_speech_prob": 1.9333488125994336e-06}, {"id": 540, "seek": 261616, "start": 2616.16, "end": 2618.16, "text": " To see how well they're correlated", "tokens": [1407, 536, 577, 731, 436, 434, 38574], "temperature": 0.0, "avg_logprob": -0.2239370346069336, "compression_ratio": 1.5502645502645502, "no_speech_prob": 1.2679224710154813e-06}, {"id": 541, "seek": 261616, "start": 2620.3599999999997, "end": 2624.7599999999998, "text": " Sometimes if like two or three people are way ahead of everybody else they may have found some kind of leakage", "tokens": [4803, 498, 411, 732, 420, 1045, 561, 366, 636, 2286, 295, 2201, 1646, 436, 815, 362, 1352, 512, 733, 295, 47799], "temperature": 0.0, "avg_logprob": -0.2239370346069336, "compression_ratio": 1.5502645502645502, "no_speech_prob": 1.2679224710154813e-06}, {"id": 542, "seek": 261616, "start": 2625.56, "end": 2627.56, "text": " Or something like that", "tokens": [1610, 746, 411, 300], "temperature": 0.0, "avg_logprob": -0.2239370346069336, "compression_ratio": 1.5502645502645502, "no_speech_prob": 1.2679224710154813e-06}, {"id": 543, "seek": 261616, "start": 2628.24, "end": 2630.52, "text": " Like that's often a sign that there's some", "tokens": [1743, 300, 311, 2049, 257, 1465, 300, 456, 311, 512], "temperature": 0.0, "avg_logprob": -0.2239370346069336, "compression_ratio": 1.5502645502645502, "no_speech_prob": 1.2679224710154813e-06}, {"id": 544, "seek": 261616, "start": 2631.6, "end": 2633.6, "text": " trick", "tokens": [4282], "temperature": 0.0, "avg_logprob": -0.2239370346069336, "compression_ratio": 1.5502645502645502, "no_speech_prob": 1.2679224710154813e-06}, {"id": 545, "seek": 261616, "start": 2635.2799999999997, "end": 2636.6, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2239370346069336, "compression_ratio": 1.5502645502645502, "no_speech_prob": 1.2679224710154813e-06}, {"id": 546, "seek": 261616, "start": 2636.6, "end": 2638.68, "text": " So that's Rossman", "tokens": [407, 300, 311, 16140, 1601], "temperature": 0.0, "avg_logprob": -0.2239370346069336, "compression_ratio": 1.5502645502645502, "no_speech_prob": 1.2679224710154813e-06}, {"id": 547, "seek": 261616, "start": 2639.44, "end": 2641.68, "text": " And that brings us to", "tokens": [400, 300, 5607, 505, 281], "temperature": 0.0, "avg_logprob": -0.2239370346069336, "compression_ratio": 1.5502645502645502, "no_speech_prob": 1.2679224710154813e-06}, {"id": 548, "seek": 264168, "start": 2641.68, "end": 2645.2, "text": " the end of all of our material", "tokens": [264, 917, 295, 439, 295, 527, 2527], "temperature": 0.0, "avg_logprob": -0.19676010536425043, "compression_ratio": 1.5337423312883436, "no_speech_prob": 5.771823907707585e-06}, {"id": 549, "seek": 264168, "start": 2646.0, "end": 2649.9199999999996, "text": " So let's come back after the break and do a quick review", "tokens": [407, 718, 311, 808, 646, 934, 264, 1821, 293, 360, 257, 1702, 3131], "temperature": 0.0, "avg_logprob": -0.19676010536425043, "compression_ratio": 1.5337423312883436, "no_speech_prob": 5.771823907707585e-06}, {"id": 550, "seek": 264168, "start": 2650.96, "end": 2657.18, "text": " And then we will talk about ethics and machine learning so let's come back at in five minutes", "tokens": [400, 550, 321, 486, 751, 466, 19769, 293, 3479, 2539, 370, 718, 311, 808, 646, 412, 294, 1732, 2077], "temperature": 0.0, "avg_logprob": -0.19676010536425043, "compression_ratio": 1.5337423312883436, "no_speech_prob": 5.771823907707585e-06}, {"id": 551, "seek": 264168, "start": 2662.12, "end": 2666.8799999999997, "text": " So we've learned two ways to train a model", "tokens": [407, 321, 600, 3264, 732, 2098, 281, 3847, 257, 2316], "temperature": 0.0, "avg_logprob": -0.19676010536425043, "compression_ratio": 1.5337423312883436, "no_speech_prob": 5.771823907707585e-06}, {"id": 552, "seek": 266688, "start": 2666.88, "end": 2673.4, "text": " One is by building a tree and one is with SGD", "tokens": [1485, 307, 538, 2390, 257, 4230, 293, 472, 307, 365, 34520, 35], "temperature": 0.0, "avg_logprob": -0.2882212669618668, "compression_ratio": 1.5034965034965035, "no_speech_prob": 4.96525217386079e-07}, {"id": 553, "seek": 266688, "start": 2675.04, "end": 2681.38, "text": " Okay, and so the SGD approach is a way we can train a model", "tokens": [1033, 11, 293, 370, 264, 34520, 35, 3109, 307, 257, 636, 321, 393, 3847, 257, 2316], "temperature": 0.0, "avg_logprob": -0.2882212669618668, "compression_ratio": 1.5034965034965035, "no_speech_prob": 4.96525217386079e-07}, {"id": 554, "seek": 266688, "start": 2682.08, "end": 2684.08, "text": " which is a", "tokens": [597, 307, 257], "temperature": 0.0, "avg_logprob": -0.2882212669618668, "compression_ratio": 1.5034965034965035, "no_speech_prob": 4.96525217386079e-07}, {"id": 555, "seek": 266688, "start": 2684.52, "end": 2689.1600000000003, "text": " Linear model or a stack of linear layers with nonlinearities between them", "tokens": [14670, 289, 2316, 420, 257, 8630, 295, 8213, 7914, 365, 2107, 28263, 1088, 1296, 552], "temperature": 0.0, "avg_logprob": -0.2882212669618668, "compression_ratio": 1.5034965034965035, "no_speech_prob": 4.96525217386079e-07}, {"id": 556, "seek": 266688, "start": 2690.8, "end": 2692.8, "text": " Where else tree building", "tokens": [2305, 1646, 4230, 2390], "temperature": 0.0, "avg_logprob": -0.2882212669618668, "compression_ratio": 1.5034965034965035, "no_speech_prob": 4.96525217386079e-07}, {"id": 557, "seek": 269280, "start": 2692.8, "end": 2698.04, "text": " Specifically will give us a tree right and then tree building we can combine with", "tokens": [26058, 486, 976, 505, 257, 4230, 558, 293, 550, 4230, 2390, 321, 393, 10432, 365], "temperature": 0.0, "avg_logprob": -0.1848622579423208, "compression_ratio": 1.5405405405405406, "no_speech_prob": 6.083570269765914e-07}, {"id": 558, "seek": 269280, "start": 2698.96, "end": 2704.36, "text": " Bagging to create a random first or with boosting to create a GBM", "tokens": [24377, 3249, 281, 1884, 257, 4974, 700, 420, 365, 43117, 281, 1884, 257, 460, 18345], "temperature": 0.0, "avg_logprob": -0.1848622579423208, "compression_ratio": 1.5405405405405406, "no_speech_prob": 6.083570269765914e-07}, {"id": 559, "seek": 269280, "start": 2704.92, "end": 2709.44, "text": " Or various other slight variations such as extremely randomized trees", "tokens": [1610, 3683, 661, 4036, 17840, 1270, 382, 4664, 38513, 5852], "temperature": 0.0, "avg_logprob": -0.1848622579423208, "compression_ratio": 1.5405405405405406, "no_speech_prob": 6.083570269765914e-07}, {"id": 560, "seek": 269280, "start": 2711.7200000000003, "end": 2717.5800000000004, "text": " So it's worth like reminding ourselves of like what these things do", "tokens": [407, 309, 311, 3163, 411, 27639, 4175, 295, 411, 437, 613, 721, 360], "temperature": 0.0, "avg_logprob": -0.1848622579423208, "compression_ratio": 1.5405405405405406, "no_speech_prob": 6.083570269765914e-07}, {"id": 561, "seek": 271758, "start": 2717.58, "end": 2719.58, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.1981936182294573, "compression_ratio": 1.5789473684210527, "no_speech_prob": 1.328768576058792e-06}, {"id": 562, "seek": 271758, "start": 2721.9, "end": 2723.9, "text": " Let's let's look at some data", "tokens": [961, 311, 718, 311, 574, 412, 512, 1412], "temperature": 0.0, "avg_logprob": -0.1981936182294573, "compression_ratio": 1.5789473684210527, "no_speech_prob": 1.328768576058792e-06}, {"id": 563, "seek": 271758, "start": 2732.9, "end": 2734.9, "text": " So if we've got some data", "tokens": [407, 498, 321, 600, 658, 512, 1412], "temperature": 0.0, "avg_logprob": -0.1981936182294573, "compression_ratio": 1.5789473684210527, "no_speech_prob": 1.328768576058792e-06}, {"id": 564, "seek": 273490, "start": 2734.9, "end": 2744.9, "text": " Like so actually let's look specifically let's look specifically at categorical data, right?", "tokens": [1743, 370, 767, 718, 311, 574, 4682, 718, 311, 574, 4682, 412, 19250, 804, 1412, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18049682889665877, "compression_ratio": 1.858695652173913, "no_speech_prob": 5.338128630683059e-06}, {"id": 565, "seek": 273490, "start": 2747.58, "end": 2749.86, "text": " Okay, so categorical data", "tokens": [1033, 11, 370, 19250, 804, 1412], "temperature": 0.0, "avg_logprob": -0.18049682889665877, "compression_ratio": 1.858695652173913, "no_speech_prob": 5.338128630683059e-06}, {"id": 566, "seek": 273490, "start": 2750.94, "end": 2755.38, "text": " There's a couple of possibilities of what categorical data might look like it could be like okay", "tokens": [821, 311, 257, 1916, 295, 12178, 295, 437, 19250, 804, 1412, 1062, 574, 411, 309, 727, 312, 411, 1392], "temperature": 0.0, "avg_logprob": -0.18049682889665877, "compression_ratio": 1.858695652173913, "no_speech_prob": 5.338128630683059e-06}, {"id": 567, "seek": 273490, "start": 2755.38, "end": 2762.34, "text": " So let's say we've got zip code like so we've got line for double oh three is our zip code right and then we've got like sales", "tokens": [407, 718, 311, 584, 321, 600, 658, 20730, 3089, 411, 370, 321, 600, 658, 1622, 337, 3834, 1954, 1045, 307, 527, 20730, 3089, 558, 293, 550, 321, 600, 658, 411, 5763], "temperature": 0.0, "avg_logprob": -0.18049682889665877, "compression_ratio": 1.858695652173913, "no_speech_prob": 5.338128630683059e-06}, {"id": 568, "seek": 276234, "start": 2762.34, "end": 2766.7400000000002, "text": " Right and it's like 50 and", "tokens": [1779, 293, 309, 311, 411, 2625, 293], "temperature": 0.0, "avg_logprob": -0.1915003240924992, "compression_ratio": 1.5722222222222222, "no_speech_prob": 2.4824732918204973e-06}, {"id": 569, "seek": 276234, "start": 2767.86, "end": 2775.82, "text": " Like nine four one three one sales of 22 and so forth right so we've got some categorical variable", "tokens": [1743, 4949, 1451, 472, 1045, 472, 5763, 295, 5853, 293, 370, 5220, 558, 370, 321, 600, 658, 512, 19250, 804, 7006], "temperature": 0.0, "avg_logprob": -0.1915003240924992, "compression_ratio": 1.5722222222222222, "no_speech_prob": 2.4824732918204973e-06}, {"id": 570, "seek": 276234, "start": 2777.6600000000003, "end": 2779.6600000000003, "text": " So there's a couple of ways", "tokens": [407, 456, 311, 257, 1916, 295, 2098], "temperature": 0.0, "avg_logprob": -0.1915003240924992, "compression_ratio": 1.5722222222222222, "no_speech_prob": 2.4824732918204973e-06}, {"id": 571, "seek": 276234, "start": 2779.6600000000003, "end": 2784.42, "text": " We could represent that categorical variable one would be just to use", "tokens": [492, 727, 2906, 300, 19250, 804, 7006, 472, 576, 312, 445, 281, 764], "temperature": 0.0, "avg_logprob": -0.1915003240924992, "compression_ratio": 1.5722222222222222, "no_speech_prob": 2.4824732918204973e-06}, {"id": 572, "seek": 276234, "start": 2785.1400000000003, "end": 2788.94, "text": " The number right and like maybe it wasn't a number to start", "tokens": [440, 1230, 558, 293, 411, 1310, 309, 2067, 380, 257, 1230, 281, 722], "temperature": 0.0, "avg_logprob": -0.1915003240924992, "compression_ratio": 1.5722222222222222, "no_speech_prob": 2.4824732918204973e-06}, {"id": 573, "seek": 278894, "start": 2788.94, "end": 2792.98, "text": " You know maybe it wasn't a number at all maybe a categorical variable is like San Francisco", "tokens": [509, 458, 1310, 309, 2067, 380, 257, 1230, 412, 439, 1310, 257, 19250, 804, 7006, 307, 411, 5271, 12279], "temperature": 0.0, "avg_logprob": -0.19059513524635552, "compression_ratio": 1.6493506493506493, "no_speech_prob": 1.1911049568880117e-06}, {"id": 574, "seek": 278894, "start": 2793.62, "end": 2795.54, "text": " New York", "tokens": [1873, 3609], "temperature": 0.0, "avg_logprob": -0.19059513524635552, "compression_ratio": 1.6493506493506493, "no_speech_prob": 1.1911049568880117e-06}, {"id": 575, "seek": 278894, "start": 2795.54, "end": 2797.54, "text": " Mumbai and", "tokens": [34309, 293], "temperature": 0.0, "avg_logprob": -0.19059513524635552, "compression_ratio": 1.6493506493506493, "no_speech_prob": 1.1911049568880117e-06}, {"id": 576, "seek": 278894, "start": 2797.58, "end": 2799.02, "text": " Sydney", "tokens": [21065], "temperature": 0.0, "avg_logprob": -0.19059513524635552, "compression_ratio": 1.6493506493506493, "no_speech_prob": 1.1911049568880117e-06}, {"id": 577, "seek": 278894, "start": 2799.02, "end": 2807.38, "text": " Right, but we can turn it into a number just by like arbitrarily deciding to give them numbers right so like it ends up being a number", "tokens": [1779, 11, 457, 321, 393, 1261, 309, 666, 257, 1230, 445, 538, 411, 19071, 3289, 17990, 281, 976, 552, 3547, 558, 370, 411, 309, 5314, 493, 885, 257, 1230], "temperature": 0.0, "avg_logprob": -0.19059513524635552, "compression_ratio": 1.6493506493506493, "no_speech_prob": 1.1911049568880117e-06}, {"id": 578, "seek": 278894, "start": 2807.38, "end": 2813.66, "text": " So we could just use that kind of arbitrary number so if if it turns out that", "tokens": [407, 321, 727, 445, 764, 300, 733, 295, 23211, 1230, 370, 498, 498, 309, 4523, 484, 300], "temperature": 0.0, "avg_logprob": -0.19059513524635552, "compression_ratio": 1.6493506493506493, "no_speech_prob": 1.1911049568880117e-06}, {"id": 579, "seek": 278894, "start": 2814.42, "end": 2816.42, "text": " zip codes that are", "tokens": [20730, 14211, 300, 366], "temperature": 0.0, "avg_logprob": -0.19059513524635552, "compression_ratio": 1.6493506493506493, "no_speech_prob": 1.1911049568880117e-06}, {"id": 580, "seek": 281642, "start": 2816.42, "end": 2820.1800000000003, "text": " Numerically next to each other have somewhat similar behavior", "tokens": [426, 15583, 984, 958, 281, 1184, 661, 362, 8344, 2531, 5223], "temperature": 0.0, "avg_logprob": -0.2852796105777516, "compression_ratio": 1.4732824427480915, "no_speech_prob": 1.003008833322383e-06}, {"id": 581, "seek": 281642, "start": 2821.02, "end": 2822.5, "text": " then the", "tokens": [550, 264], "temperature": 0.0, "avg_logprob": -0.2852796105777516, "compression_ratio": 1.4732824427480915, "no_speech_prob": 1.003008833322383e-06}, {"id": 582, "seek": 281642, "start": 2822.5, "end": 2824.1800000000003, "text": " zip code", "tokens": [20730, 3089], "temperature": 0.0, "avg_logprob": -0.2852796105777516, "compression_ratio": 1.4732824427480915, "no_speech_prob": 1.003008833322383e-06}, {"id": 583, "seek": 281642, "start": 2824.1800000000003, "end": 2826.1800000000003, "text": " versus sales", "tokens": [5717, 5763], "temperature": 0.0, "avg_logprob": -0.2852796105777516, "compression_ratio": 1.4732824427480915, "no_speech_prob": 1.003008833322383e-06}, {"id": 584, "seek": 281642, "start": 2826.86, "end": 2828.86, "text": " Chart might look something like this", "tokens": [49762, 1062, 574, 746, 411, 341], "temperature": 0.0, "avg_logprob": -0.2852796105777516, "compression_ratio": 1.4732824427480915, "no_speech_prob": 1.003008833322383e-06}, {"id": 585, "seek": 281642, "start": 2832.9, "end": 2837.06, "text": " For example right or alternatively", "tokens": [1171, 1365, 558, 420, 8535, 356], "temperature": 0.0, "avg_logprob": -0.2852796105777516, "compression_ratio": 1.4732824427480915, "no_speech_prob": 1.003008833322383e-06}, {"id": 586, "seek": 281642, "start": 2838.7400000000002, "end": 2841.34, "text": " If the zip code versus sales", "tokens": [759, 264, 20730, 3089, 5717, 5763], "temperature": 0.0, "avg_logprob": -0.2852796105777516, "compression_ratio": 1.4732824427480915, "no_speech_prob": 1.003008833322383e-06}, {"id": 587, "seek": 284134, "start": 2841.34, "end": 2847.94, "text": " Like sorry if the two zip codes next to each other didn't have in any way similar", "tokens": [1743, 2597, 498, 264, 732, 20730, 14211, 958, 281, 1184, 661, 994, 380, 362, 294, 604, 636, 2531], "temperature": 0.0, "avg_logprob": -0.1873748685106819, "compression_ratio": 1.5420560747663552, "no_speech_prob": 8.446202173217898e-07}, {"id": 588, "seek": 284134, "start": 2848.1000000000004, "end": 2852.3, "text": " Similar sales behavior you would expect to see something that looked more like this", "tokens": [10905, 5763, 5223, 291, 576, 2066, 281, 536, 746, 300, 2956, 544, 411, 341], "temperature": 0.0, "avg_logprob": -0.1873748685106819, "compression_ratio": 1.5420560747663552, "no_speech_prob": 8.446202173217898e-07}, {"id": 589, "seek": 284134, "start": 2854.7000000000003, "end": 2856.86, "text": " Like kind of just all over the place right", "tokens": [1743, 733, 295, 445, 439, 670, 264, 1081, 558], "temperature": 0.0, "avg_logprob": -0.1873748685106819, "compression_ratio": 1.5420560747663552, "no_speech_prob": 8.446202173217898e-07}, {"id": 590, "seek": 284134, "start": 2859.9, "end": 2862.6600000000003, "text": " Okay, so they're the kind of two possibilities", "tokens": [1033, 11, 370, 436, 434, 264, 733, 295, 732, 12178], "temperature": 0.0, "avg_logprob": -0.1873748685106819, "compression_ratio": 1.5420560747663552, "no_speech_prob": 8.446202173217898e-07}, {"id": 591, "seek": 284134, "start": 2864.54, "end": 2869.42, "text": " So what a random forest would do if we had just encoded zip in this way is", "tokens": [407, 437, 257, 4974, 6719, 576, 360, 498, 321, 632, 445, 2058, 12340, 20730, 294, 341, 636, 307], "temperature": 0.0, "avg_logprob": -0.1873748685106819, "compression_ratio": 1.5420560747663552, "no_speech_prob": 8.446202173217898e-07}, {"id": 592, "seek": 286942, "start": 2869.42, "end": 2874.06, "text": " It's gonna say alright. I need to find my single", "tokens": [467, 311, 799, 584, 5845, 13, 286, 643, 281, 915, 452, 2167], "temperature": 0.0, "avg_logprob": -0.2434511937593159, "compression_ratio": 1.5467980295566504, "no_speech_prob": 1.933350631588837e-06}, {"id": 593, "seek": 286942, "start": 2874.58, "end": 2880.42, "text": " Best split point okay the split point that's going to make the two sides have as", "tokens": [9752, 7472, 935, 1392, 264, 7472, 935, 300, 311, 516, 281, 652, 264, 732, 4881, 362, 382], "temperature": 0.0, "avg_logprob": -0.2434511937593159, "compression_ratio": 1.5467980295566504, "no_speech_prob": 1.933350631588837e-06}, {"id": 594, "seek": 286942, "start": 2881.3, "end": 2884.7400000000002, "text": " Smaller standard deviation as possible or mathematically equivalently", "tokens": [15287, 260, 3832, 25163, 382, 1944, 420, 44003, 9052, 2276], "temperature": 0.0, "avg_logprob": -0.2434511937593159, "compression_ratio": 1.5467980295566504, "no_speech_prob": 1.933350631588837e-06}, {"id": 595, "seek": 286942, "start": 2885.34, "end": 2889.7000000000003, "text": " Have the lowest root means whatever so in this case it might pick", "tokens": [3560, 264, 12437, 5593, 1355, 2035, 370, 294, 341, 1389, 309, 1062, 1888], "temperature": 0.0, "avg_logprob": -0.2434511937593159, "compression_ratio": 1.5467980295566504, "no_speech_prob": 1.933350631588837e-06}, {"id": 596, "seek": 286942, "start": 2892.26, "end": 2894.26, "text": " Here", "tokens": [1692], "temperature": 0.0, "avg_logprob": -0.2434511937593159, "compression_ratio": 1.5467980295566504, "no_speech_prob": 1.933350631588837e-06}, {"id": 597, "seek": 286942, "start": 2894.26, "end": 2896.78, "text": " As our first bit point because on this side", "tokens": [1018, 527, 700, 857, 935, 570, 322, 341, 1252], "temperature": 0.0, "avg_logprob": -0.2434511937593159, "compression_ratio": 1.5467980295566504, "no_speech_prob": 1.933350631588837e-06}, {"id": 598, "seek": 289678, "start": 2896.78, "end": 2900.26, "text": " There's one average and on the other side", "tokens": [821, 311, 472, 4274, 293, 322, 264, 661, 1252], "temperature": 0.0, "avg_logprob": -0.23097847957237094, "compression_ratio": 1.9836065573770492, "no_speech_prob": 3.7266242998157395e-06}, {"id": 599, "seek": 289678, "start": 2901.1000000000004, "end": 2902.9, "text": " There's the other average", "tokens": [821, 311, 264, 661, 4274], "temperature": 0.0, "avg_logprob": -0.23097847957237094, "compression_ratio": 1.9836065573770492, "no_speech_prob": 3.7266242998157395e-06}, {"id": 600, "seek": 289678, "start": 2902.9, "end": 2908.46, "text": " Okay, and then for its second split point. It's going to say okay. How do I split this and?", "tokens": [1033, 11, 293, 550, 337, 1080, 1150, 7472, 935, 13, 467, 311, 516, 281, 584, 1392, 13, 1012, 360, 286, 7472, 341, 293, 30], "temperature": 0.0, "avg_logprob": -0.23097847957237094, "compression_ratio": 1.9836065573770492, "no_speech_prob": 3.7266242998157395e-06}, {"id": 601, "seek": 289678, "start": 2909.0600000000004, "end": 2912.02, "text": " It's probably going to say I would split here", "tokens": [467, 311, 1391, 516, 281, 584, 286, 576, 7472, 510], "temperature": 0.0, "avg_logprob": -0.23097847957237094, "compression_ratio": 1.9836065573770492, "no_speech_prob": 3.7266242998157395e-06}, {"id": 602, "seek": 289678, "start": 2912.6600000000003, "end": 2914.98, "text": " Right because now we've got this average", "tokens": [1779, 570, 586, 321, 600, 658, 341, 4274], "temperature": 0.0, "avg_logprob": -0.23097847957237094, "compression_ratio": 1.9836065573770492, "no_speech_prob": 3.7266242998157395e-06}, {"id": 603, "seek": 289678, "start": 2916.46, "end": 2917.6600000000003, "text": " versus", "tokens": [5717], "temperature": 0.0, "avg_logprob": -0.23097847957237094, "compression_ratio": 1.9836065573770492, "no_speech_prob": 3.7266242998157395e-06}, {"id": 604, "seek": 289678, "start": 2917.6600000000003, "end": 2919.26, "text": " this average", "tokens": [341, 4274], "temperature": 0.0, "avg_logprob": -0.23097847957237094, "compression_ratio": 1.9836065573770492, "no_speech_prob": 3.7266242998157395e-06}, {"id": 605, "seek": 289678, "start": 2919.26, "end": 2924.98, "text": " Right and then finally it's going to say okay. How do we split here, and it's going to say okay?", "tokens": [1779, 293, 550, 2721, 309, 311, 516, 281, 584, 1392, 13, 1012, 360, 321, 7472, 510, 11, 293, 309, 311, 516, 281, 584, 1392, 30], "temperature": 0.0, "avg_logprob": -0.23097847957237094, "compression_ratio": 1.9836065573770492, "no_speech_prob": 3.7266242998157395e-06}, {"id": 606, "seek": 292498, "start": 2924.98, "end": 2929.8, "text": " I'll split there right so now I've got that average and that average okay", "tokens": [286, 603, 7472, 456, 558, 370, 586, 286, 600, 658, 300, 4274, 293, 300, 4274, 1392], "temperature": 0.0, "avg_logprob": -0.16212625786809637, "compression_ratio": 1.6752136752136753, "no_speech_prob": 1.7603379092179239e-06}, {"id": 607, "seek": 292498, "start": 2929.8, "end": 2935.66, "text": " So you can see that it's able to kind of hone in on the set of spits", "tokens": [407, 291, 393, 536, 300, 309, 311, 1075, 281, 733, 295, 43212, 294, 322, 264, 992, 295, 637, 1208], "temperature": 0.0, "avg_logprob": -0.16212625786809637, "compression_ratio": 1.6752136752136753, "no_speech_prob": 1.7603379092179239e-06}, {"id": 608, "seek": 292498, "start": 2935.66, "end": 2941.14, "text": " It needs even though it kind of does it greedily top down one at a time right the only reason it wouldn't be able to", "tokens": [467, 2203, 754, 1673, 309, 733, 295, 775, 309, 29230, 953, 1192, 760, 472, 412, 257, 565, 558, 264, 787, 1778, 309, 2759, 380, 312, 1075, 281], "temperature": 0.0, "avg_logprob": -0.16212625786809637, "compression_ratio": 1.6752136752136753, "no_speech_prob": 1.7603379092179239e-06}, {"id": 609, "seek": 292498, "start": 2941.14, "end": 2948.2400000000002, "text": " Do this is if like it was just such bad luck that the two halves were kind of always exactly balanced", "tokens": [1144, 341, 307, 498, 411, 309, 390, 445, 1270, 1578, 3668, 300, 264, 732, 38490, 645, 733, 295, 1009, 2293, 13902], "temperature": 0.0, "avg_logprob": -0.16212625786809637, "compression_ratio": 1.6752136752136753, "no_speech_prob": 1.7603379092179239e-06}, {"id": 610, "seek": 292498, "start": 2948.58, "end": 2950.46, "text": " Right but even if that happens", "tokens": [1779, 457, 754, 498, 300, 2314], "temperature": 0.0, "avg_logprob": -0.16212625786809637, "compression_ratio": 1.6752136752136753, "no_speech_prob": 1.7603379092179239e-06}, {"id": 611, "seek": 295046, "start": 2950.46, "end": 2956.3, "text": " It's not going to be the end of the world it'll spit on something else some other variable and next time around", "tokens": [467, 311, 406, 516, 281, 312, 264, 917, 295, 264, 1002, 309, 603, 22127, 322, 746, 1646, 512, 661, 7006, 293, 958, 565, 926], "temperature": 0.0, "avg_logprob": -0.2177020853215998, "compression_ratio": 1.6920289855072463, "no_speech_prob": 1.1726392585842405e-06}, {"id": 612, "seek": 295046, "start": 2956.66, "end": 2963.2200000000003, "text": " You know it's very unlikely that it's still going to be exactly balanced in both parts of the tree right so in practice this works", "tokens": [509, 458, 309, 311, 588, 17518, 300, 309, 311, 920, 516, 281, 312, 2293, 13902, 294, 1293, 3166, 295, 264, 4230, 558, 370, 294, 3124, 341, 1985], "temperature": 0.0, "avg_logprob": -0.2177020853215998, "compression_ratio": 1.6920289855072463, "no_speech_prob": 1.1726392585842405e-06}, {"id": 613, "seek": 295046, "start": 2963.62, "end": 2965.62, "text": " Just fine", "tokens": [1449, 2489], "temperature": 0.0, "avg_logprob": -0.2177020853215998, "compression_ratio": 1.6920289855072463, "no_speech_prob": 1.1726392585842405e-06}, {"id": 614, "seek": 295046, "start": 2965.98, "end": 2967.98, "text": " In the second case", "tokens": [682, 264, 1150, 1389], "temperature": 0.0, "avg_logprob": -0.2177020853215998, "compression_ratio": 1.6920289855072463, "no_speech_prob": 1.1726392585842405e-06}, {"id": 615, "seek": 295046, "start": 2968.1, "end": 2973.1, "text": " Right it can do exactly the same thing right it'll say like okay, which is my best?", "tokens": [1779, 309, 393, 360, 2293, 264, 912, 551, 558, 309, 603, 584, 411, 1392, 11, 597, 307, 452, 1151, 30], "temperature": 0.0, "avg_logprob": -0.2177020853215998, "compression_ratio": 1.6920289855072463, "no_speech_prob": 1.1726392585842405e-06}, {"id": 616, "seek": 297310, "start": 2973.1, "end": 2980.7799999999997, "text": " First split right even though there's no relationship between one zip code and its neighboring zip code numerically", "tokens": [2386, 7472, 558, 754, 1673, 456, 311, 572, 2480, 1296, 472, 20730, 3089, 293, 1080, 31521, 20730, 3089, 7866, 984], "temperature": 0.0, "avg_logprob": -0.2150364742484144, "compression_ratio": 1.9949238578680204, "no_speech_prob": 2.9022994567640126e-06}, {"id": 617, "seek": 297310, "start": 2980.8199999999997, "end": 2983.38, "text": " We can still see here if it if it's bits here", "tokens": [492, 393, 920, 536, 510, 498, 309, 498, 309, 311, 9239, 510], "temperature": 0.0, "avg_logprob": -0.2150364742484144, "compression_ratio": 1.9949238578680204, "no_speech_prob": 2.9022994567640126e-06}, {"id": 618, "seek": 297310, "start": 2984.9, "end": 2989.5, "text": " Right there's the average on one side and the average on the other side is probably about here", "tokens": [1779, 456, 311, 264, 4274, 322, 472, 1252, 293, 264, 4274, 322, 264, 661, 1252, 307, 1391, 466, 510], "temperature": 0.0, "avg_logprob": -0.2150364742484144, "compression_ratio": 1.9949238578680204, "no_speech_prob": 2.9022994567640126e-06}, {"id": 619, "seek": 297310, "start": 2990.8199999999997, "end": 2993.62, "text": " Right and then where would it spit next?", "tokens": [1779, 293, 550, 689, 576, 309, 22127, 958, 30], "temperature": 0.0, "avg_logprob": -0.2150364742484144, "compression_ratio": 1.9949238578680204, "no_speech_prob": 2.9022994567640126e-06}, {"id": 620, "seek": 297310, "start": 2994.38, "end": 2995.74, "text": " Probably here", "tokens": [9210, 510], "temperature": 0.0, "avg_logprob": -0.2150364742484144, "compression_ratio": 1.9949238578680204, "no_speech_prob": 2.9022994567640126e-06}, {"id": 621, "seek": 297310, "start": 2995.74, "end": 2998.8399999999997, "text": " Right because here's the average on one side here's the average on the other side", "tokens": [1779, 570, 510, 311, 264, 4274, 322, 472, 1252, 510, 311, 264, 4274, 322, 264, 661, 1252], "temperature": 0.0, "avg_logprob": -0.2150364742484144, "compression_ratio": 1.9949238578680204, "no_speech_prob": 2.9022994567640126e-06}, {"id": 622, "seek": 299884, "start": 2998.84, "end": 3004.48, "text": " Right so again can do the same thing right it's going to need more splits because it's going to end up", "tokens": [1779, 370, 797, 393, 360, 264, 912, 551, 558, 309, 311, 516, 281, 643, 544, 37741, 570, 309, 311, 516, 281, 917, 493], "temperature": 0.0, "avg_logprob": -0.1955659442477756, "compression_ratio": 1.641255605381166, "no_speech_prob": 5.368737561184389e-07}, {"id": 623, "seek": 299884, "start": 3004.8, "end": 3010.7200000000003, "text": " Having to kind of narrow down on each individual large zip code and each individual small zip code, but it's still going to be fine", "tokens": [10222, 281, 733, 295, 9432, 760, 322, 1184, 2609, 2416, 20730, 3089, 293, 1184, 2609, 1359, 20730, 3089, 11, 457, 309, 311, 920, 516, 281, 312, 2489], "temperature": 0.0, "avg_logprob": -0.1955659442477756, "compression_ratio": 1.641255605381166, "no_speech_prob": 5.368737561184389e-07}, {"id": 624, "seek": 299884, "start": 3011.04, "end": 3013.7400000000002, "text": " Okay, so when we're dealing with", "tokens": [1033, 11, 370, 562, 321, 434, 6260, 365], "temperature": 0.0, "avg_logprob": -0.1955659442477756, "compression_ratio": 1.641255605381166, "no_speech_prob": 5.368737561184389e-07}, {"id": 625, "seek": 299884, "start": 3015.8, "end": 3021.7200000000003, "text": " Building decision trees for random forests or GBMs or whatever we tend to encode", "tokens": [18974, 3537, 5852, 337, 4974, 21700, 420, 460, 18345, 82, 420, 2035, 321, 3928, 281, 2058, 1429], "temperature": 0.0, "avg_logprob": -0.1955659442477756, "compression_ratio": 1.641255605381166, "no_speech_prob": 5.368737561184389e-07}, {"id": 626, "seek": 299884, "start": 3023.2400000000002, "end": 3025.2400000000002, "text": " variables just as", "tokens": [9102, 445, 382], "temperature": 0.0, "avg_logprob": -0.1955659442477756, "compression_ratio": 1.641255605381166, "no_speech_prob": 5.368737561184389e-07}, {"id": 627, "seek": 302524, "start": 3025.24, "end": 3029.7, "text": " Ordinals okay on the other hand if we were doing a", "tokens": [1610, 17557, 1124, 1392, 322, 264, 661, 1011, 498, 321, 645, 884, 257], "temperature": 0.0, "avg_logprob": -0.23768339157104493, "compression_ratio": 1.491891891891892, "no_speech_prob": 2.295911372129922e-06}, {"id": 628, "seek": 302524, "start": 3031.52, "end": 3033.9599999999996, "text": " Neural network or like the simplest version like a", "tokens": [1734, 1807, 3209, 420, 411, 264, 22811, 3037, 411, 257], "temperature": 0.0, "avg_logprob": -0.23768339157104493, "compression_ratio": 1.491891891891892, "no_speech_prob": 2.295911372129922e-06}, {"id": 629, "seek": 302524, "start": 3034.8799999999997, "end": 3038.4799999999996, "text": " Linear regression or a logistic regression the best it could do", "tokens": [14670, 289, 24590, 420, 257, 3565, 3142, 24590, 264, 1151, 309, 727, 360], "temperature": 0.0, "avg_logprob": -0.23768339157104493, "compression_ratio": 1.491891891891892, "no_speech_prob": 2.295911372129922e-06}, {"id": 630, "seek": 302524, "start": 3039.8799999999997, "end": 3041.7999999999997, "text": " Is that?", "tokens": [1119, 300, 30], "temperature": 0.0, "avg_logprob": -0.23768339157104493, "compression_ratio": 1.491891891891892, "no_speech_prob": 2.295911372129922e-06}, {"id": 631, "seek": 302524, "start": 3041.7999999999997, "end": 3048.3599999999997, "text": " Right which is no good at all and ditto with this one. It's going to be like that okay, so", "tokens": [1779, 597, 307, 572, 665, 412, 439, 293, 274, 34924, 365, 341, 472, 13, 467, 311, 516, 281, 312, 411, 300, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.23768339157104493, "compression_ratio": 1.491891891891892, "no_speech_prob": 2.295911372129922e-06}, {"id": 632, "seek": 302524, "start": 3049.2, "end": 3051.12, "text": " an ordinal", "tokens": [364, 4792, 2071], "temperature": 0.0, "avg_logprob": -0.23768339157104493, "compression_ratio": 1.491891891891892, "no_speech_prob": 2.295911372129922e-06}, {"id": 633, "seek": 305112, "start": 3051.12, "end": 3057.56, "text": " Is not going to be a useful encoding for a linear model or something that stacks?", "tokens": [1119, 406, 516, 281, 312, 257, 4420, 43430, 337, 257, 8213, 2316, 420, 746, 300, 30792, 30], "temperature": 0.0, "avg_logprob": -0.3031361662311318, "compression_ratio": 1.6342857142857143, "no_speech_prob": 4.520934453466907e-07}, {"id": 634, "seek": 305112, "start": 3058.4, "end": 3060.4, "text": " linear and nonlinear models together", "tokens": [8213, 293, 2107, 28263, 5245, 1214], "temperature": 0.0, "avg_logprob": -0.3031361662311318, "compression_ratio": 1.6342857142857143, "no_speech_prob": 4.520934453466907e-07}, {"id": 635, "seek": 305112, "start": 3061.2, "end": 3067.56, "text": " So instead what we do is we create a one-hot encoding right so we'll say like you know", "tokens": [407, 2602, 437, 321, 360, 307, 321, 1884, 257, 472, 12, 12194, 43430, 558, 370, 321, 603, 584, 411, 291, 458], "temperature": 0.0, "avg_logprob": -0.3031361662311318, "compression_ratio": 1.6342857142857143, "no_speech_prob": 4.520934453466907e-07}, {"id": 636, "seek": 305112, "start": 3068.04, "end": 3073.64, "text": " There's zero one zero zero zero. Here's zero one. Oh, oh here's oh", "tokens": [821, 311, 4018, 472, 4018, 4018, 4018, 13, 1692, 311, 4018, 472, 13, 876, 11, 1954, 510, 311, 1954], "temperature": 0.0, "avg_logprob": -0.3031361662311318, "compression_ratio": 1.6342857142857143, "no_speech_prob": 4.520934453466907e-07}, {"id": 637, "seek": 305112, "start": 3074.3199999999997, "end": 3076.3199999999997, "text": " one oh oh", "tokens": [472, 1954, 1954], "temperature": 0.0, "avg_logprob": -0.3031361662311318, "compression_ratio": 1.6342857142857143, "no_speech_prob": 4.520934453466907e-07}, {"id": 638, "seek": 305112, "start": 3076.24, "end": 3077.4, "text": " one", "tokens": [472], "temperature": 0.0, "avg_logprob": -0.3031361662311318, "compression_ratio": 1.6342857142857143, "no_speech_prob": 4.520934453466907e-07}, {"id": 639, "seek": 307740, "start": 3077.4, "end": 3083.04, "text": " Okay, and so with that encoding it can effectively create like a little histogram", "tokens": [1033, 11, 293, 370, 365, 300, 43430, 309, 393, 8659, 1884, 411, 257, 707, 49816], "temperature": 0.0, "avg_logprob": -0.15781861543655396, "compression_ratio": 1.599056603773585, "no_speech_prob": 5.285506290420017e-07}, {"id": 640, "seek": 307740, "start": 3083.92, "end": 3085.92, "text": " Right where it's going to have a different", "tokens": [1779, 689, 309, 311, 516, 281, 362, 257, 819], "temperature": 0.0, "avg_logprob": -0.15781861543655396, "compression_ratio": 1.599056603773585, "no_speech_prob": 5.285506290420017e-07}, {"id": 641, "seek": 307740, "start": 3086.6800000000003, "end": 3093.6, "text": " Coefficient for each level and so that way it can do exactly what it needs to do can you pass that back, please?", "tokens": [3066, 68, 7816, 337, 1184, 1496, 293, 370, 300, 636, 309, 393, 360, 2293, 437, 309, 2203, 281, 360, 393, 291, 1320, 300, 646, 11, 1767, 30], "temperature": 0.0, "avg_logprob": -0.15781861543655396, "compression_ratio": 1.599056603773585, "no_speech_prob": 5.285506290420017e-07}, {"id": 642, "seek": 309360, "start": 3093.6, "end": 3104.12, "text": " At what point does that become like too tedious for your system, or does it not pretty much never yeah?", "tokens": [1711, 437, 935, 775, 300, 1813, 411, 886, 38284, 337, 428, 1185, 11, 420, 775, 309, 406, 1238, 709, 1128, 1338, 30], "temperature": 0.0, "avg_logprob": -0.18606268564860026, "compression_ratio": 1.497142857142857, "no_speech_prob": 3.3596720072637254e-07}, {"id": 643, "seek": 309360, "start": 3107.56, "end": 3112.04, "text": " Because remember in real life. We don't actually actually we don't actually have to create", "tokens": [1436, 1604, 294, 957, 993, 13, 492, 500, 380, 767, 767, 321, 500, 380, 767, 362, 281, 1884], "temperature": 0.0, "avg_logprob": -0.18606268564860026, "compression_ratio": 1.497142857142857, "no_speech_prob": 3.3596720072637254e-07}, {"id": 644, "seek": 309360, "start": 3113.16, "end": 3118.2799999999997, "text": " That matrix instead we can just you know have the four coefficients", "tokens": [663, 8141, 2602, 321, 393, 445, 291, 458, 362, 264, 1451, 31994], "temperature": 0.0, "avg_logprob": -0.18606268564860026, "compression_ratio": 1.497142857142857, "no_speech_prob": 3.3596720072637254e-07}, {"id": 645, "seek": 311828, "start": 3118.28, "end": 3126.1200000000003, "text": " Right and just do an index lookup to grab the second one which is mathematically equivalent to multiply by the one-hot encoding", "tokens": [1779, 293, 445, 360, 364, 8186, 574, 1010, 281, 4444, 264, 1150, 472, 597, 307, 44003, 10344, 281, 12972, 538, 264, 472, 12, 12194, 43430], "temperature": 0.0, "avg_logprob": -0.17284038543701172, "compression_ratio": 1.575, "no_speech_prob": 6.083570838200103e-07}, {"id": 646, "seek": 311828, "start": 3126.36, "end": 3128.52, "text": " Okay, so so that's no problem", "tokens": [1033, 11, 370, 370, 300, 311, 572, 1154], "temperature": 0.0, "avg_logprob": -0.17284038543701172, "compression_ratio": 1.575, "no_speech_prob": 6.083570838200103e-07}, {"id": 647, "seek": 311828, "start": 3134.96, "end": 3136.96, "text": " One thing to mention you know I know", "tokens": [1485, 551, 281, 2152, 291, 458, 286, 458], "temperature": 0.0, "avg_logprob": -0.17284038543701172, "compression_ratio": 1.575, "no_speech_prob": 6.083570838200103e-07}, {"id": 648, "seek": 311828, "start": 3137.92, "end": 3143.0400000000004, "text": " You guys have kind of been taught quite a bit of more like analytical solutions to things", "tokens": [509, 1074, 362, 733, 295, 668, 5928, 1596, 257, 857, 295, 544, 411, 29579, 6547, 281, 721], "temperature": 0.0, "avg_logprob": -0.17284038543701172, "compression_ratio": 1.575, "no_speech_prob": 6.083570838200103e-07}, {"id": 649, "seek": 311828, "start": 3144.0400000000004, "end": 3146.6800000000003, "text": " and in analytical solutions to", "tokens": [293, 294, 29579, 6547, 281], "temperature": 0.0, "avg_logprob": -0.17284038543701172, "compression_ratio": 1.575, "no_speech_prob": 6.083570838200103e-07}, {"id": 650, "seek": 314668, "start": 3146.68, "end": 3149.6, "text": " Like a linear regression you get", "tokens": [1743, 257, 8213, 24590, 291, 483], "temperature": 0.0, "avg_logprob": -0.21559775277469936, "compression_ratio": 1.7264573991031391, "no_speech_prob": 5.014670477976324e-06}, {"id": 651, "seek": 314668, "start": 3152.7599999999998, "end": 3158.3599999999997, "text": " You can't solve something with this amount of collinearity in other words", "tokens": [509, 393, 380, 5039, 746, 365, 341, 2372, 295, 1263, 533, 17409, 294, 661, 2283], "temperature": 0.0, "avg_logprob": -0.21559775277469936, "compression_ratio": 1.7264573991031391, "no_speech_prob": 5.014670477976324e-06}, {"id": 652, "seek": 314668, "start": 3159.5, "end": 3160.96, "text": " Sydney", "tokens": [21065], "temperature": 0.0, "avg_logprob": -0.21559775277469936, "compression_ratio": 1.7264573991031391, "no_speech_prob": 5.014670477976324e-06}, {"id": 653, "seek": 314668, "start": 3160.96, "end": 3165.2799999999997, "text": " Something you know something is Sydney if it's not Mumbai or New York or San Francisco", "tokens": [6595, 291, 458, 746, 307, 21065, 498, 309, 311, 406, 34309, 420, 1873, 3609, 420, 5271, 12279], "temperature": 0.0, "avg_logprob": -0.21559775277469936, "compression_ratio": 1.7264573991031391, "no_speech_prob": 5.014670477976324e-06}, {"id": 654, "seek": 314668, "start": 3165.52, "end": 3168.12, "text": " So in other words there's a hundred percent collinearity", "tokens": [407, 294, 661, 2283, 456, 311, 257, 3262, 3043, 1263, 533, 17409], "temperature": 0.0, "avg_logprob": -0.21559775277469936, "compression_ratio": 1.7264573991031391, "no_speech_prob": 5.014670477976324e-06}, {"id": 655, "seek": 314668, "start": 3168.68, "end": 3175.08, "text": " Between the fourth of these classes versus the other three and so if you try to solve a linear aggression analytically that way", "tokens": [18967, 264, 6409, 295, 613, 5359, 5717, 264, 661, 1045, 293, 370, 498, 291, 853, 281, 5039, 257, 8213, 30268, 10783, 984, 300, 636], "temperature": 0.0, "avg_logprob": -0.21559775277469936, "compression_ratio": 1.7264573991031391, "no_speech_prob": 5.014670477976324e-06}, {"id": 656, "seek": 317508, "start": 3175.08, "end": 3177.68, "text": " The whole thing falls apart now note", "tokens": [440, 1379, 551, 8804, 4936, 586, 3637], "temperature": 0.0, "avg_logprob": -0.19317490427117598, "compression_ratio": 1.5510204081632653, "no_speech_prob": 2.090445150315645e-06}, {"id": 657, "seek": 317508, "start": 3178.24, "end": 3180.24, "text": " With SGD we have no such problem", "tokens": [2022, 34520, 35, 321, 362, 572, 1270, 1154], "temperature": 0.0, "avg_logprob": -0.19317490427117598, "compression_ratio": 1.5510204081632653, "no_speech_prob": 2.090445150315645e-06}, {"id": 658, "seek": 317508, "start": 3180.6, "end": 3185.22, "text": " Okay, like SGD. Why would it care right? We're just taking one step along the derivative", "tokens": [1033, 11, 411, 34520, 35, 13, 1545, 576, 309, 1127, 558, 30, 492, 434, 445, 1940, 472, 1823, 2051, 264, 13760], "temperature": 0.0, "avg_logprob": -0.19317490427117598, "compression_ratio": 1.5510204081632653, "no_speech_prob": 2.090445150315645e-06}, {"id": 659, "seek": 317508, "start": 3186.56, "end": 3191.18, "text": " It cares a little right because like in the end the main problem with", "tokens": [467, 12310, 257, 707, 558, 570, 411, 294, 264, 917, 264, 2135, 1154, 365], "temperature": 0.0, "avg_logprob": -0.19317490427117598, "compression_ratio": 1.5510204081632653, "no_speech_prob": 2.090445150315645e-06}, {"id": 660, "seek": 317508, "start": 3192.0, "end": 3197.2799999999997, "text": " Collinearity is that there's an infinite number of equally good solutions, right?", "tokens": [4586, 533, 17409, 307, 300, 456, 311, 364, 13785, 1230, 295, 12309, 665, 6547, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19317490427117598, "compression_ratio": 1.5510204081632653, "no_speech_prob": 2.090445150315645e-06}, {"id": 661, "seek": 317508, "start": 3197.2799999999997, "end": 3201.96, "text": " so in other words we could increase all of these and decrease this or", "tokens": [370, 294, 661, 2283, 321, 727, 3488, 439, 295, 613, 293, 11514, 341, 420], "temperature": 0.0, "avg_logprob": -0.19317490427117598, "compression_ratio": 1.5510204081632653, "no_speech_prob": 2.090445150315645e-06}, {"id": 662, "seek": 320196, "start": 3201.96, "end": 3205.16, "text": " Decrease all of these and increase this and they're going to", "tokens": [12427, 265, 651, 439, 295, 613, 293, 3488, 341, 293, 436, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.19159788455603258, "compression_ratio": 1.7489539748953975, "no_speech_prob": 3.1875379136181436e-06}, {"id": 663, "seek": 320196, "start": 3205.8, "end": 3207.8, "text": " balance out right and", "tokens": [4772, 484, 558, 293], "temperature": 0.0, "avg_logprob": -0.19159788455603258, "compression_ratio": 1.7489539748953975, "no_speech_prob": 3.1875379136181436e-06}, {"id": 664, "seek": 320196, "start": 3208.28, "end": 3213.2400000000002, "text": " When there's an infinitely large number of good solutions that means there's a lot of kind of", "tokens": [1133, 456, 311, 364, 36227, 2416, 1230, 295, 665, 6547, 300, 1355, 456, 311, 257, 688, 295, 733, 295], "temperature": 0.0, "avg_logprob": -0.19159788455603258, "compression_ratio": 1.7489539748953975, "no_speech_prob": 3.1875379136181436e-06}, {"id": 665, "seek": 320196, "start": 3213.96, "end": 3217.44, "text": " Flat spots in the loss surface, and it can be harder to optimize", "tokens": [36172, 10681, 294, 264, 4470, 3753, 11, 293, 309, 393, 312, 6081, 281, 19719], "temperature": 0.0, "avg_logprob": -0.19159788455603258, "compression_ratio": 1.7489539748953975, "no_speech_prob": 3.1875379136181436e-06}, {"id": 666, "seek": 320196, "start": 3218.12, "end": 3221.2400000000002, "text": " Right so it's a really easy way to get rid of all of those flat spots", "tokens": [1779, 370, 309, 311, 257, 534, 1858, 636, 281, 483, 3973, 295, 439, 295, 729, 4962, 10681], "temperature": 0.0, "avg_logprob": -0.19159788455603258, "compression_ratio": 1.7489539748953975, "no_speech_prob": 3.1875379136181436e-06}, {"id": 667, "seek": 320196, "start": 3221.32, "end": 3226.54, "text": " Which is to add a little bit of regularization so if we added a little bit of a little bit of weight decay", "tokens": [3013, 307, 281, 909, 257, 707, 857, 295, 3890, 2144, 370, 498, 321, 3869, 257, 707, 857, 295, 257, 707, 857, 295, 3364, 21039], "temperature": 0.0, "avg_logprob": -0.19159788455603258, "compression_ratio": 1.7489539748953975, "no_speech_prob": 3.1875379136181436e-06}, {"id": 668, "seek": 322654, "start": 3226.54, "end": 3234.04, "text": " Like one e neg seven even then that basically says these are not all equally good anymore the one which is the best is the one", "tokens": [1743, 472, 308, 2485, 3407, 754, 550, 300, 1936, 1619, 613, 366, 406, 439, 12309, 665, 3602, 264, 472, 597, 307, 264, 1151, 307, 264, 472], "temperature": 0.0, "avg_logprob": -0.22013169146598655, "compression_ratio": 1.6791666666666667, "no_speech_prob": 1.4593719015465467e-06}, {"id": 669, "seek": 322654, "start": 3234.04, "end": 3241.2, "text": " Where the parameters are the smallest and the most similar to each other and so that'll again move it back to being a nice", "tokens": [2305, 264, 9834, 366, 264, 16998, 293, 264, 881, 2531, 281, 1184, 661, 293, 370, 300, 603, 797, 1286, 309, 646, 281, 885, 257, 1481], "temperature": 0.0, "avg_logprob": -0.22013169146598655, "compression_ratio": 1.6791666666666667, "no_speech_prob": 1.4593719015465467e-06}, {"id": 670, "seek": 322654, "start": 3241.2, "end": 3243.2, "text": " loss function yes", "tokens": [4470, 2445, 2086], "temperature": 0.0, "avg_logprob": -0.22013169146598655, "compression_ratio": 1.6791666666666667, "no_speech_prob": 1.4593719015465467e-06}, {"id": 671, "seek": 322654, "start": 3243.52, "end": 3251.2, "text": " Could you just clarify that point you made about why one hard coding wouldn't be that tedious sure?", "tokens": [7497, 291, 445, 17594, 300, 935, 291, 1027, 466, 983, 472, 1152, 17720, 2759, 380, 312, 300, 38284, 988, 30], "temperature": 0.0, "avg_logprob": -0.22013169146598655, "compression_ratio": 1.6791666666666667, "no_speech_prob": 1.4593719015465467e-06}, {"id": 672, "seek": 325120, "start": 3251.2, "end": 3256.48, "text": " If we have a one hot encoded vector", "tokens": [759, 321, 362, 257, 472, 2368, 2058, 12340, 8062], "temperature": 0.0, "avg_logprob": -0.1986590020450545, "compression_ratio": 1.6284153005464481, "no_speech_prob": 2.156809614461963e-06}, {"id": 673, "seek": 325120, "start": 3257.68, "end": 3259.68, "text": " right, and we are", "tokens": [558, 11, 293, 321, 366], "temperature": 0.0, "avg_logprob": -0.1986590020450545, "compression_ratio": 1.6284153005464481, "no_speech_prob": 2.156809614461963e-06}, {"id": 674, "seek": 325120, "start": 3260.56, "end": 3263.7, "text": " multiplying it by a set of coefficients", "tokens": [30955, 309, 538, 257, 992, 295, 31994], "temperature": 0.0, "avg_logprob": -0.1986590020450545, "compression_ratio": 1.6284153005464481, "no_speech_prob": 2.156809614461963e-06}, {"id": 675, "seek": 325120, "start": 3266.3599999999997, "end": 3272.04, "text": " Right then that's exactly the same thing as simply saying let's grab the thing where the one is", "tokens": [1779, 550, 300, 311, 2293, 264, 912, 551, 382, 2935, 1566, 718, 311, 4444, 264, 551, 689, 264, 472, 307], "temperature": 0.0, "avg_logprob": -0.1986590020450545, "compression_ratio": 1.6284153005464481, "no_speech_prob": 2.156809614461963e-06}, {"id": 676, "seek": 325120, "start": 3272.7999999999997, "end": 3275.52, "text": " Right so in other words if we had stored this as a zero", "tokens": [1779, 370, 294, 661, 2283, 498, 321, 632, 12187, 341, 382, 257, 4018], "temperature": 0.0, "avg_logprob": -0.1986590020450545, "compression_ratio": 1.6284153005464481, "no_speech_prob": 2.156809614461963e-06}, {"id": 677, "seek": 327552, "start": 3275.52, "end": 3280.96, "text": " You know and this one has a one and this one is a two right?", "tokens": [509, 458, 293, 341, 472, 575, 257, 472, 293, 341, 472, 307, 257, 732, 558, 30], "temperature": 0.0, "avg_logprob": -0.22681277731190558, "compression_ratio": 1.6604651162790698, "no_speech_prob": 8.990950277620868e-07}, {"id": 678, "seek": 327552, "start": 3282.12, "end": 3285.14, "text": " Then it's exactly the same as just saying hey look up that thing in the array", "tokens": [1396, 309, 311, 2293, 264, 912, 382, 445, 1566, 4177, 574, 493, 300, 551, 294, 264, 10225], "temperature": 0.0, "avg_logprob": -0.22681277731190558, "compression_ratio": 1.6604651162790698, "no_speech_prob": 8.990950277620868e-07}, {"id": 679, "seek": 327552, "start": 3286.08, "end": 3292.28, "text": " Okay, and so we call that version and embedding right so an embedding is a", "tokens": [1033, 11, 293, 370, 321, 818, 300, 3037, 293, 12240, 3584, 558, 370, 364, 12240, 3584, 307, 257], "temperature": 0.0, "avg_logprob": -0.22681277731190558, "compression_ratio": 1.6604651162790698, "no_speech_prob": 8.990950277620868e-07}, {"id": 680, "seek": 327552, "start": 3292.6, "end": 3295.72, "text": " Multiplicate is a weight matrix you can multiply by a one hot encoding", "tokens": [29238, 4770, 473, 307, 257, 3364, 8141, 291, 393, 12972, 538, 257, 472, 2368, 43430], "temperature": 0.0, "avg_logprob": -0.22681277731190558, "compression_ratio": 1.6604651162790698, "no_speech_prob": 8.990950277620868e-07}, {"id": 681, "seek": 327552, "start": 3296.24, "end": 3301.14, "text": " And it's just a computational shortcut, but it's mathematically the same", "tokens": [400, 309, 311, 445, 257, 28270, 24822, 11, 457, 309, 311, 44003, 264, 912], "temperature": 0.0, "avg_logprob": -0.22681277731190558, "compression_ratio": 1.6604651162790698, "no_speech_prob": 8.990950277620868e-07}, {"id": 682, "seek": 330114, "start": 3301.14, "end": 3303.14, "text": " So there's a key difference", "tokens": [407, 456, 311, 257, 2141, 2649], "temperature": 0.0, "avg_logprob": -0.48895023180090863, "compression_ratio": 1.7440758293838863, "no_speech_prob": 8.579192467550456e-07}, {"id": 683, "seek": 330114, "start": 3303.46, "end": 3307.3799999999997, "text": " So the first you know key differences between like solving", "tokens": [407, 264, 700, 291, 458, 2141, 7300, 1296, 411, 12606], "temperature": 0.0, "avg_logprob": -0.48895023180090863, "compression_ratio": 1.7440758293838863, "no_speech_prob": 8.579192467550456e-07}, {"id": 684, "seek": 330114, "start": 3308.1, "end": 3313.2599999999998, "text": " linear type models analytically versus with SGD with SGD we don't have to worry about", "tokens": [8213, 2010, 5245, 10783, 984, 5717, 365, 34520, 35, 365, 34520, 35, 321, 500, 380, 362, 281, 3292, 466], "temperature": 0.0, "avg_logprob": -0.48895023180090863, "compression_ratio": 1.7440758293838863, "no_speech_prob": 8.579192467550456e-07}, {"id": 685, "seek": 330114, "start": 3314.18, "end": 3316.7799999999997, "text": " Collinearity and stuff or at least not nearly to the same degree", "tokens": [4586, 533, 17409, 293, 1507, 420, 412, 1935, 406, 6217, 281, 264, 912, 4314], "temperature": 0.0, "avg_logprob": -0.48895023180090863, "compression_ratio": 1.7440758293838863, "no_speech_prob": 8.579192467550456e-07}, {"id": 686, "seek": 330114, "start": 3318.18, "end": 3322.1, "text": " And then the difference between solving a linear or", "tokens": [400, 550, 264, 2649, 1296, 12606, 257, 8213, 420], "temperature": 0.0, "avg_logprob": -0.48895023180090863, "compression_ratio": 1.7440758293838863, "no_speech_prob": 8.579192467550456e-07}, {"id": 687, "seek": 330114, "start": 3322.98, "end": 3328.62, "text": " Single layer or multi-layer model with SGD versus a tree a tree is going to be", "tokens": [31248, 4583, 420, 4825, 12, 8376, 260, 2316, 365, 34520, 35, 5717, 257, 4230, 257, 4230, 307, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.48895023180090863, "compression_ratio": 1.7440758293838863, "no_speech_prob": 8.579192467550456e-07}, {"id": 688, "seek": 332862, "start": 3328.62, "end": 3335.98, "text": " Versus a tree a tree is going to be like it's going to complain about less things right so in particular you can just use", "tokens": [12226, 301, 257, 4230, 257, 4230, 307, 516, 281, 312, 411, 309, 311, 516, 281, 11024, 466, 1570, 721, 558, 370, 294, 1729, 291, 393, 445, 764], "temperature": 0.0, "avg_logprob": -0.15508915515656166, "compression_ratio": 1.6814159292035398, "no_speech_prob": 1.8448167793394532e-06}, {"id": 689, "seek": 332862, "start": 3336.06, "end": 3340.46, "text": " Ordinals as your categorical variables and as we learned just before", "tokens": [1610, 17557, 1124, 382, 428, 19250, 804, 9102, 293, 382, 321, 3264, 445, 949], "temperature": 0.0, "avg_logprob": -0.15508915515656166, "compression_ratio": 1.6814159292035398, "no_speech_prob": 1.8448167793394532e-06}, {"id": 690, "seek": 332862, "start": 3341.7, "end": 3343.7, "text": " We also don't have to worry about", "tokens": [492, 611, 500, 380, 362, 281, 3292, 466], "temperature": 0.0, "avg_logprob": -0.15508915515656166, "compression_ratio": 1.6814159292035398, "no_speech_prob": 1.8448167793394532e-06}, {"id": 691, "seek": 332862, "start": 3343.74, "end": 3350.46, "text": " Normalizing continuous variables for a tree, but we do have to worry about it for these SGD", "tokens": [21277, 3319, 10957, 9102, 337, 257, 4230, 11, 457, 321, 360, 362, 281, 3292, 466, 309, 337, 613, 34520, 35], "temperature": 0.0, "avg_logprob": -0.15508915515656166, "compression_ratio": 1.6814159292035398, "no_speech_prob": 1.8448167793394532e-06}, {"id": 692, "seek": 332862, "start": 3351.02, "end": 3353.02, "text": " trained models", "tokens": [8895, 5245], "temperature": 0.0, "avg_logprob": -0.15508915515656166, "compression_ratio": 1.6814159292035398, "no_speech_prob": 1.8448167793394532e-06}, {"id": 693, "seek": 332862, "start": 3353.66, "end": 3356.38, "text": " So then we also learned a lot about interpreting", "tokens": [407, 550, 321, 611, 3264, 257, 688, 466, 37395], "temperature": 0.0, "avg_logprob": -0.15508915515656166, "compression_ratio": 1.6814159292035398, "no_speech_prob": 1.8448167793394532e-06}, {"id": 694, "seek": 335638, "start": 3356.38, "end": 3359.38, "text": " random forests in particular and", "tokens": [4974, 21700, 294, 1729, 293], "temperature": 0.0, "avg_logprob": -0.1939189330391262, "compression_ratio": 1.6965811965811965, "no_speech_prob": 2.225269099653815e-06}, {"id": 695, "seek": 335638, "start": 3360.54, "end": 3367.62, "text": " If you're interested you may be interested in trying to use those same techniques to interpret", "tokens": [759, 291, 434, 3102, 291, 815, 312, 3102, 294, 1382, 281, 764, 729, 912, 7512, 281, 7302], "temperature": 0.0, "avg_logprob": -0.1939189330391262, "compression_ratio": 1.6965811965811965, "no_speech_prob": 2.225269099653815e-06}, {"id": 696, "seek": 335638, "start": 3369.6600000000003, "end": 3371.2200000000003, "text": " Neural nets", "tokens": [1734, 1807, 36170], "temperature": 0.0, "avg_logprob": -0.1939189330391262, "compression_ratio": 1.6965811965811965, "no_speech_prob": 2.225269099653815e-06}, {"id": 697, "seek": 335638, "start": 3371.2200000000003, "end": 3374.86, "text": " Right so if you want to know which of my features are important in a neural net", "tokens": [1779, 370, 498, 291, 528, 281, 458, 597, 295, 452, 4122, 366, 1021, 294, 257, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.1939189330391262, "compression_ratio": 1.6965811965811965, "no_speech_prob": 2.225269099653815e-06}, {"id": 698, "seek": 335638, "start": 3375.2200000000003, "end": 3381.9, "text": " You could try the same thing try shuffling each column in turn and see how much it changes your accuracy", "tokens": [509, 727, 853, 264, 912, 551, 853, 402, 1245, 1688, 1184, 7738, 294, 1261, 293, 536, 577, 709, 309, 2962, 428, 14170], "temperature": 0.0, "avg_logprob": -0.1939189330391262, "compression_ratio": 1.6965811965811965, "no_speech_prob": 2.225269099653815e-06}, {"id": 699, "seek": 338190, "start": 3381.9, "end": 3387.9, "text": " Okay, and that's going to be your feature importance for your neural net and then if you really want to have fun", "tokens": [1033, 11, 293, 300, 311, 516, 281, 312, 428, 4111, 7379, 337, 428, 18161, 2533, 293, 550, 498, 291, 534, 528, 281, 362, 1019], "temperature": 0.0, "avg_logprob": -0.14247901766907936, "compression_ratio": 1.8023715415019763, "no_speech_prob": 9.132501759268052e-07}, {"id": 700, "seek": 338190, "start": 3389.02, "end": 3391.9, "text": " Recognize then that shuffling that column is", "tokens": [44682, 1125, 550, 300, 402, 1245, 1688, 300, 7738, 307], "temperature": 0.0, "avg_logprob": -0.14247901766907936, "compression_ratio": 1.8023715415019763, "no_speech_prob": 9.132501759268052e-07}, {"id": 701, "seek": 338190, "start": 3392.7400000000002, "end": 3399.7000000000003, "text": " Just a way of calculating how sensitive the output is to that input which in other words is the derivative of", "tokens": [1449, 257, 636, 295, 28258, 577, 9477, 264, 5598, 307, 281, 300, 4846, 597, 294, 661, 2283, 307, 264, 13760, 295], "temperature": 0.0, "avg_logprob": -0.14247901766907936, "compression_ratio": 1.8023715415019763, "no_speech_prob": 9.132501759268052e-07}, {"id": 702, "seek": 338190, "start": 3400.7400000000002, "end": 3404.54, "text": " The output with respect to that input and so therefore", "tokens": [440, 5598, 365, 3104, 281, 300, 4846, 293, 370, 4412], "temperature": 0.0, "avg_logprob": -0.14247901766907936, "compression_ratio": 1.8023715415019763, "no_speech_prob": 9.132501759268052e-07}, {"id": 703, "seek": 340454, "start": 3404.54, "end": 3412.02, "text": " Maybe you could just ask pipe torch to give you the derivatives with respect to the input directly and see if that gives you the same", "tokens": [2704, 291, 727, 445, 1029, 11240, 27822, 281, 976, 291, 264, 33733, 365, 3104, 281, 264, 4846, 3838, 293, 536, 498, 300, 2709, 291, 264, 912], "temperature": 0.0, "avg_logprob": -0.16975950311731408, "compression_ratio": 1.7326007326007327, "no_speech_prob": 6.681493118776416e-07}, {"id": 704, "seek": 340454, "start": 3412.02, "end": 3414.02, "text": " Kind of answers right?", "tokens": [9242, 295, 6338, 558, 30], "temperature": 0.0, "avg_logprob": -0.16975950311731408, "compression_ratio": 1.7326007326007327, "no_speech_prob": 6.681493118776416e-07}, {"id": 705, "seek": 340454, "start": 3414.66, "end": 3421.2599999999998, "text": " You could do the same kind of thing for a partial dependence plot you could try you know doing the exact same thing with your neural net", "tokens": [509, 727, 360, 264, 912, 733, 295, 551, 337, 257, 14641, 31704, 7542, 291, 727, 853, 291, 458, 884, 264, 1900, 912, 551, 365, 428, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.16975950311731408, "compression_ratio": 1.7326007326007327, "no_speech_prob": 6.681493118776416e-07}, {"id": 706, "seek": 340454, "start": 3421.86, "end": 3428.22, "text": " Replace everything in a column with the same value do it for 1960 1961 1962 plot that right?", "tokens": [1300, 6742, 1203, 294, 257, 7738, 365, 264, 912, 2158, 360, 309, 337, 16157, 41720, 39498, 7542, 300, 558, 30], "temperature": 0.0, "avg_logprob": -0.16975950311731408, "compression_ratio": 1.7326007326007327, "no_speech_prob": 6.681493118776416e-07}, {"id": 707, "seek": 340454, "start": 3428.22, "end": 3433.42, "text": " I don't know of anybody who's done these things before not because it's rocket science", "tokens": [286, 500, 380, 458, 295, 4472, 567, 311, 1096, 613, 721, 949, 406, 570, 309, 311, 13012, 3497], "temperature": 0.0, "avg_logprob": -0.16975950311731408, "compression_ratio": 1.7326007326007327, "no_speech_prob": 6.681493118776416e-07}, {"id": 708, "seek": 343342, "start": 3433.42, "end": 3435.98, "text": " but just because I don't know maybe no one thought of it or", "tokens": [457, 445, 570, 286, 500, 380, 458, 1310, 572, 472, 1194, 295, 309, 420], "temperature": 0.0, "avg_logprob": -0.17477560851533533, "compression_ratio": 1.6989966555183946, "no_speech_prob": 3.1381132430396974e-06}, {"id": 709, "seek": 343342, "start": 3437.1800000000003, "end": 3438.46, "text": " It's not in a library", "tokens": [467, 311, 406, 294, 257, 6405], "temperature": 0.0, "avg_logprob": -0.17477560851533533, "compression_ratio": 1.6989966555183946, "no_speech_prob": 3.1381132430396974e-06}, {"id": 710, "seek": 343342, "start": 3438.46, "end": 3440.82, "text": " I don't know but if somebody tried it", "tokens": [286, 500, 380, 458, 457, 498, 2618, 3031, 309], "temperature": 0.0, "avg_logprob": -0.17477560851533533, "compression_ratio": 1.6989966555183946, "no_speech_prob": 3.1381132430396974e-06}, {"id": 711, "seek": 343342, "start": 3440.82, "end": 3446.3, "text": " I think you should find it useful make a great blog post maybe even a paper if you wanted to take it a bit further", "tokens": [286, 519, 291, 820, 915, 309, 4420, 652, 257, 869, 6968, 2183, 1310, 754, 257, 3035, 498, 291, 1415, 281, 747, 309, 257, 857, 3052], "temperature": 0.0, "avg_logprob": -0.17477560851533533, "compression_ratio": 1.6989966555183946, "no_speech_prob": 3.1381132430396974e-06}, {"id": 712, "seek": 343342, "start": 3447.26, "end": 3453.78, "text": " So there's a thought that something you could do so those most of those interpretation techniques are not particularly specific to random forests", "tokens": [407, 456, 311, 257, 1194, 300, 746, 291, 727, 360, 370, 729, 881, 295, 729, 14174, 7512, 366, 406, 4098, 2685, 281, 4974, 21700], "temperature": 0.0, "avg_logprob": -0.17477560851533533, "compression_ratio": 1.6989966555183946, "no_speech_prob": 3.1381132430396974e-06}, {"id": 713, "seek": 343342, "start": 3454.26, "end": 3460.42, "text": " Things like the tree interpreter certainly are because they're all about like what's inside the tree. Can you pass it to Karen?", "tokens": [9514, 411, 264, 4230, 34132, 3297, 366, 570, 436, 434, 439, 466, 411, 437, 311, 1854, 264, 4230, 13, 1664, 291, 1320, 309, 281, 14834, 30], "temperature": 0.0, "avg_logprob": -0.17477560851533533, "compression_ratio": 1.6989966555183946, "no_speech_prob": 3.1381132430396974e-06}, {"id": 714, "seek": 346042, "start": 3460.42, "end": 3466.02, "text": " We are applying three interpreter for neural nets", "tokens": [492, 366, 9275, 1045, 34132, 337, 18161, 36170], "temperature": 0.0, "avg_logprob": -0.24231607889391713, "compression_ratio": 1.8603603603603605, "no_speech_prob": 6.400779966497794e-05}, {"id": 715, "seek": 346042, "start": 3466.02, "end": 3471.46, "text": " How are we going to make inference out of activations that the path follows for example?", "tokens": [1012, 366, 321, 516, 281, 652, 38253, 484, 295, 2430, 763, 300, 264, 3100, 10002, 337, 1365, 30], "temperature": 0.0, "avg_logprob": -0.24231607889391713, "compression_ratio": 1.8603603603603605, "no_speech_prob": 6.400779966497794e-05}, {"id": 716, "seek": 346042, "start": 3471.94, "end": 3475.6800000000003, "text": " So how are we going to life in three interpreter? We are like", "tokens": [407, 577, 366, 321, 516, 281, 993, 294, 1045, 34132, 30, 492, 366, 411], "temperature": 0.0, "avg_logprob": -0.24231607889391713, "compression_ratio": 1.8603603603603605, "no_speech_prob": 6.400779966497794e-05}, {"id": 717, "seek": 346042, "start": 3476.58, "end": 3478.14, "text": " We're looking at the path", "tokens": [492, 434, 1237, 412, 264, 3100], "temperature": 0.0, "avg_logprob": -0.24231607889391713, "compression_ratio": 1.8603603603603605, "no_speech_prob": 6.400779966497794e-05}, {"id": 718, "seek": 346042, "start": 3478.14, "end": 3482.78, "text": " We are looking at the parts and their contributions of the features in this case", "tokens": [492, 366, 1237, 412, 264, 3166, 293, 641, 15725, 295, 264, 4122, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.24231607889391713, "compression_ratio": 1.8603603603603605, "no_speech_prob": 6.400779966497794e-05}, {"id": 719, "seek": 346042, "start": 3482.78, "end": 3488.1800000000003, "text": " It will be same with activations. I guess the contributions of each activation on their path. Yeah, maybe", "tokens": [467, 486, 312, 912, 365, 2430, 763, 13, 286, 2041, 264, 15725, 295, 1184, 24433, 322, 641, 3100, 13, 865, 11, 1310], "temperature": 0.0, "avg_logprob": -0.24231607889391713, "compression_ratio": 1.8603603603603605, "no_speech_prob": 6.400779966497794e-05}, {"id": 720, "seek": 348818, "start": 3488.18, "end": 3493.66, "text": " How are you know I haven't thought about it? How can we like making friends out of the activations?", "tokens": [1012, 366, 291, 458, 286, 2378, 380, 1194, 466, 309, 30, 1012, 393, 321, 411, 1455, 1855, 484, 295, 264, 2430, 763, 30], "temperature": 0.0, "avg_logprob": -0.2369134161207411, "compression_ratio": 1.6341463414634145, "no_speech_prob": 8.013404112716671e-06}, {"id": 721, "seek": 348818, "start": 3494.2599999999998, "end": 3500.2999999999997, "text": " So I'd be careful say the word inference because no people normally is the word inference specifically to mean the same as like a test", "tokens": [407, 286, 1116, 312, 5026, 584, 264, 1349, 38253, 570, 572, 561, 5646, 307, 264, 1349, 38253, 4682, 281, 914, 264, 912, 382, 411, 257, 1500], "temperature": 0.0, "avg_logprob": -0.2369134161207411, "compression_ratio": 1.6341463414634145, "no_speech_prob": 8.013404112716671e-06}, {"id": 722, "seek": 348818, "start": 3500.54, "end": 3507.2799999999997, "text": " Attest time prediction you make like make some kind of interrogate the model. Yes. Yeah, not sure we should think about that", "tokens": [7298, 377, 565, 17630, 291, 652, 411, 652, 512, 733, 295, 24871, 473, 264, 2316, 13, 1079, 13, 865, 11, 406, 988, 321, 820, 519, 466, 300], "temperature": 0.0, "avg_logprob": -0.2369134161207411, "compression_ratio": 1.6341463414634145, "no_speech_prob": 8.013404112716671e-06}, {"id": 723, "seek": 348818, "start": 3508.22, "end": 3514.3399999999997, "text": " Actually Hinton and one of his students just published a paper on how to approximate a neural net with a tree", "tokens": [5135, 389, 12442, 293, 472, 295, 702, 1731, 445, 6572, 257, 3035, 322, 577, 281, 30874, 257, 18161, 2533, 365, 257, 4230], "temperature": 0.0, "avg_logprob": -0.2369134161207411, "compression_ratio": 1.6341463414634145, "no_speech_prob": 8.013404112716671e-06}, {"id": 724, "seek": 351434, "start": 3514.34, "end": 3519.34, "text": " For this exact reason which I haven't read the paper yet. Could you pass that?", "tokens": [1171, 341, 1900, 1778, 597, 286, 2378, 380, 1401, 264, 3035, 1939, 13, 7497, 291, 1320, 300, 30], "temperature": 0.0, "avg_logprob": -0.1819721198663479, "compression_ratio": 1.5775862068965518, "no_speech_prob": 1.0615853170747869e-05}, {"id": 725, "seek": 351434, "start": 3523.6600000000003, "end": 3528.98, "text": " So in linear regression and traditional statistics like one of the things that we focused on was", "tokens": [407, 294, 8213, 24590, 293, 5164, 12523, 411, 472, 295, 264, 721, 300, 321, 5178, 322, 390], "temperature": 0.0, "avg_logprob": -0.1819721198663479, "compression_ratio": 1.5775862068965518, "no_speech_prob": 1.0615853170747869e-05}, {"id": 726, "seek": 351434, "start": 3530.06, "end": 3535.6000000000004, "text": " Statistical significance of like the changes and things like that and so when thinking about a tree interpreter", "tokens": [16249, 42686, 17687, 295, 411, 264, 2962, 293, 721, 411, 300, 293, 370, 562, 1953, 466, 257, 4230, 34132], "temperature": 0.0, "avg_logprob": -0.1819721198663479, "compression_ratio": 1.5775862068965518, "no_speech_prob": 1.0615853170747869e-05}, {"id": 727, "seek": 351434, "start": 3535.78, "end": 3540.06, "text": " Or even like the waterfall chart, which I guess is just a visualization. Um, I", "tokens": [1610, 754, 411, 264, 27848, 6927, 11, 597, 286, 2041, 307, 445, 257, 25801, 13, 3301, 11, 286], "temperature": 0.0, "avg_logprob": -0.1819721198663479, "compression_ratio": 1.5775862068965518, "no_speech_prob": 1.0615853170747869e-05}, {"id": 728, "seek": 354006, "start": 3540.06, "end": 3544.22, "text": " Guess where does that fit in like because we can see like oh, yeah", "tokens": [17795, 689, 775, 300, 3318, 294, 411, 570, 321, 393, 536, 411, 1954, 11, 1338], "temperature": 0.0, "avg_logprob": -0.17335400956400324, "compression_ratio": 1.7361702127659575, "no_speech_prob": 2.7693743049894692e-06}, {"id": 729, "seek": 354006, "start": 3544.22, "end": 3548.06, "text": " This looks important in the sense that it causes large changes", "tokens": [639, 1542, 1021, 294, 264, 2020, 300, 309, 7700, 2416, 2962], "temperature": 0.0, "avg_logprob": -0.17335400956400324, "compression_ratio": 1.7361702127659575, "no_speech_prob": 2.7693743049894692e-06}, {"id": 730, "seek": 354006, "start": 3548.06, "end": 3552.34, "text": " But how do we know that it's like traditionally statistically significant or anything? Yeah", "tokens": [583, 577, 360, 321, 458, 300, 309, 311, 411, 19067, 36478, 4776, 420, 1340, 30, 865], "temperature": 0.0, "avg_logprob": -0.17335400956400324, "compression_ratio": 1.7361702127659575, "no_speech_prob": 2.7693743049894692e-06}, {"id": 731, "seek": 354006, "start": 3554.2599999999998, "end": 3560.2999999999997, "text": " So most of the time I don't care about the traditional statistical significance and the reason why is that", "tokens": [407, 881, 295, 264, 565, 286, 500, 380, 1127, 466, 264, 5164, 22820, 17687, 293, 264, 1778, 983, 307, 300], "temperature": 0.0, "avg_logprob": -0.17335400956400324, "compression_ratio": 1.7361702127659575, "no_speech_prob": 2.7693743049894692e-06}, {"id": 732, "seek": 354006, "start": 3560.94, "end": 3564.5, "text": " Nowadays the main driver of statistical significance is data volume", "tokens": [28908, 264, 2135, 6787, 295, 22820, 17687, 307, 1412, 5523], "temperature": 0.0, "avg_logprob": -0.17335400956400324, "compression_ratio": 1.7361702127659575, "no_speech_prob": 2.7693743049894692e-06}, {"id": 733, "seek": 354006, "start": 3565.2999999999997, "end": 3567.2999999999997, "text": " not kind of", "tokens": [406, 733, 295], "temperature": 0.0, "avg_logprob": -0.17335400956400324, "compression_ratio": 1.7361702127659575, "no_speech_prob": 2.7693743049894692e-06}, {"id": 734, "seek": 356730, "start": 3567.3, "end": 3575.0600000000004, "text": " Practical importance and nowadays most of the models you build will have so much data that like every tiny thing will be", "tokens": [19170, 804, 7379, 293, 13434, 881, 295, 264, 5245, 291, 1322, 486, 362, 370, 709, 1412, 300, 411, 633, 5870, 551, 486, 312], "temperature": 0.0, "avg_logprob": -0.1988012841407289, "compression_ratio": 1.7611336032388665, "no_speech_prob": 5.594253707386088e-06}, {"id": 735, "seek": 356730, "start": 3575.1800000000003, "end": 3576.86, "text": " statistically significant", "tokens": [36478, 4776], "temperature": 0.0, "avg_logprob": -0.1988012841407289, "compression_ratio": 1.7611336032388665, "no_speech_prob": 5.594253707386088e-06}, {"id": 736, "seek": 356730, "start": 3576.86, "end": 3582.92, "text": " But most of them won't be practically significant. So my main focus therefore is practical significance", "tokens": [583, 881, 295, 552, 1582, 380, 312, 15667, 4776, 13, 407, 452, 2135, 1879, 4412, 307, 8496, 17687], "temperature": 0.0, "avg_logprob": -0.1988012841407289, "compression_ratio": 1.7611336032388665, "no_speech_prob": 5.594253707386088e-06}, {"id": 737, "seek": 356730, "start": 3582.92, "end": 3585.6600000000003, "text": " Which is does the size of this influence?", "tokens": [3013, 307, 775, 264, 2744, 295, 341, 6503, 30], "temperature": 0.0, "avg_logprob": -0.1988012841407289, "compression_ratio": 1.7611336032388665, "no_speech_prob": 5.594253707386088e-06}, {"id": 738, "seek": 356730, "start": 3586.3, "end": 3588.46, "text": " impact your business, you know", "tokens": [2712, 428, 1606, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.1988012841407289, "compression_ratio": 1.7611336032388665, "no_speech_prob": 5.594253707386088e-06}, {"id": 739, "seek": 356730, "start": 3590.3, "end": 3596.76, "text": " Statistical significance only you know like it was much more important when we had a lot less data to work with", "tokens": [16249, 42686, 17687, 787, 291, 458, 411, 309, 390, 709, 544, 1021, 562, 321, 632, 257, 688, 1570, 1412, 281, 589, 365], "temperature": 0.0, "avg_logprob": -0.1988012841407289, "compression_ratio": 1.7611336032388665, "no_speech_prob": 5.594253707386088e-06}, {"id": 740, "seek": 359676, "start": 3596.76, "end": 3600.96, "text": " If you do need to know statistical significance because for example", "tokens": [759, 291, 360, 643, 281, 458, 22820, 17687, 570, 337, 1365], "temperature": 0.0, "avg_logprob": -0.1483914898891075, "compression_ratio": 1.7896825396825398, "no_speech_prob": 1.459361556044314e-06}, {"id": 741, "seek": 359676, "start": 3600.96, "end": 3606.8, "text": " You have a very small data set because it's like really expensive to label or hard to collect or whatever or it's a medical data", "tokens": [509, 362, 257, 588, 1359, 1412, 992, 570, 309, 311, 411, 534, 5124, 281, 7645, 420, 1152, 281, 2500, 420, 2035, 420, 309, 311, 257, 4625, 1412], "temperature": 0.0, "avg_logprob": -0.1483914898891075, "compression_ratio": 1.7896825396825398, "no_speech_prob": 1.459361556044314e-06}, {"id": 742, "seek": 359676, "start": 3606.8, "end": 3608.36, "text": " Set for a rare disease", "tokens": [8928, 337, 257, 5892, 4752], "temperature": 0.0, "avg_logprob": -0.1483914898891075, "compression_ratio": 1.7896825396825398, "no_speech_prob": 1.459361556044314e-06}, {"id": 743, "seek": 359676, "start": 3608.36, "end": 3612.0, "text": " You can always get statistical significance by bootstrapping", "tokens": [509, 393, 1009, 483, 22820, 17687, 538, 11450, 19639, 3759], "temperature": 0.0, "avg_logprob": -0.1483914898891075, "compression_ratio": 1.7896825396825398, "no_speech_prob": 1.459361556044314e-06}, {"id": 744, "seek": 359676, "start": 3612.88, "end": 3618.44, "text": " Which is to say that you can randomly resample your data set a number of times", "tokens": [3013, 307, 281, 584, 300, 291, 393, 16979, 725, 335, 781, 428, 1412, 992, 257, 1230, 295, 1413], "temperature": 0.0, "avg_logprob": -0.1483914898891075, "compression_ratio": 1.7896825396825398, "no_speech_prob": 1.459361556044314e-06}, {"id": 745, "seek": 359676, "start": 3619.0800000000004, "end": 3624.8, "text": " Train your model a number of times and you can then see the actual variation in predictions", "tokens": [28029, 428, 2316, 257, 1230, 295, 1413, 293, 291, 393, 550, 536, 264, 3539, 12990, 294, 21264], "temperature": 0.0, "avg_logprob": -0.1483914898891075, "compression_ratio": 1.7896825396825398, "no_speech_prob": 1.459361556044314e-06}, {"id": 746, "seek": 362480, "start": 3624.8, "end": 3628.1600000000003, "text": " Okay, so that's that's with bootstrapping you can", "tokens": [1033, 11, 370, 300, 311, 300, 311, 365, 11450, 19639, 3759, 291, 393], "temperature": 0.0, "avg_logprob": -0.23534908923473988, "compression_ratio": 1.603305785123967, "no_speech_prob": 2.6425391297379974e-06}, {"id": 747, "seek": 362480, "start": 3628.8, "end": 3631.52, "text": " Turn any model into something that gives you confidence intervals", "tokens": [7956, 604, 2316, 666, 746, 300, 2709, 291, 6687, 26651], "temperature": 0.0, "avg_logprob": -0.23534908923473988, "compression_ratio": 1.603305785123967, "no_speech_prob": 2.6425391297379974e-06}, {"id": 748, "seek": 362480, "start": 3631.88, "end": 3638.46, "text": " There's a paper by Michael Jordan which has a technique called the bag of little bootstraps which actually kind of", "tokens": [821, 311, 257, 3035, 538, 5116, 10979, 597, 575, 257, 6532, 1219, 264, 3411, 295, 707, 11450, 19639, 1878, 597, 767, 733, 295], "temperature": 0.0, "avg_logprob": -0.23534908923473988, "compression_ratio": 1.603305785123967, "no_speech_prob": 2.6425391297379974e-06}, {"id": 749, "seek": 362480, "start": 3639.04, "end": 3642.76, "text": " Takes takes this a little bit further well worth reading if you're interested", "tokens": [44347, 2516, 341, 257, 707, 857, 3052, 731, 3163, 3760, 498, 291, 434, 3102], "temperature": 0.0, "avg_logprob": -0.23534908923473988, "compression_ratio": 1.603305785123967, "no_speech_prob": 2.6425391297379974e-06}, {"id": 750, "seek": 362480, "start": 3643.2400000000002, "end": 3645.2400000000002, "text": " Actually pass it to Prince", "tokens": [5135, 1320, 309, 281, 9821], "temperature": 0.0, "avg_logprob": -0.23534908923473988, "compression_ratio": 1.603305785123967, "no_speech_prob": 2.6425391297379974e-06}, {"id": 751, "seek": 362480, "start": 3646.48, "end": 3650.36, "text": " So you said we don't need one hot encoding matrix in", "tokens": [407, 291, 848, 321, 500, 380, 643, 472, 2368, 43430, 8141, 294], "temperature": 0.0, "avg_logprob": -0.23534908923473988, "compression_ratio": 1.603305785123967, "no_speech_prob": 2.6425391297379974e-06}, {"id": 752, "seek": 365036, "start": 3650.36, "end": 3654.6400000000003, "text": " If you're doing random forest or if you are doing any previous models", "tokens": [759, 291, 434, 884, 4974, 6719, 420, 498, 291, 366, 884, 604, 3894, 5245], "temperature": 0.0, "avg_logprob": -0.17158000268668772, "compression_ratio": 1.773076923076923, "no_speech_prob": 2.1567980184045155e-06}, {"id": 753, "seek": 365036, "start": 3655.2000000000003, "end": 3661.04, "text": " What will happen if we do that and how bad can a model be if you do do one hot encoding?", "tokens": [708, 486, 1051, 498, 321, 360, 300, 293, 577, 1578, 393, 257, 2316, 312, 498, 291, 360, 360, 472, 2368, 43430, 30], "temperature": 0.0, "avg_logprob": -0.17158000268668772, "compression_ratio": 1.773076923076923, "no_speech_prob": 2.1567980184045155e-06}, {"id": 754, "seek": 365036, "start": 3661.84, "end": 3664.1600000000003, "text": " Yeah, we actually did do it remember", "tokens": [865, 11, 321, 767, 630, 360, 309, 1604], "temperature": 0.0, "avg_logprob": -0.17158000268668772, "compression_ratio": 1.773076923076923, "no_speech_prob": 2.1567980184045155e-06}, {"id": 755, "seek": 365036, "start": 3664.1600000000003, "end": 3671.52, "text": " We had that like maximum category size and we did create one hot encodings and the reason why we did it was that then our", "tokens": [492, 632, 300, 411, 6674, 7719, 2744, 293, 321, 630, 1884, 472, 2368, 2058, 378, 1109, 293, 264, 1778, 983, 321, 630, 309, 390, 300, 550, 527], "temperature": 0.0, "avg_logprob": -0.17158000268668772, "compression_ratio": 1.773076923076923, "no_speech_prob": 2.1567980184045155e-06}, {"id": 756, "seek": 365036, "start": 3672.44, "end": 3676.28, "text": " feature importance would tell us the importance of the individual levels and", "tokens": [4111, 7379, 576, 980, 505, 264, 7379, 295, 264, 2609, 4358, 293], "temperature": 0.0, "avg_logprob": -0.17158000268668772, "compression_ratio": 1.773076923076923, "no_speech_prob": 2.1567980184045155e-06}, {"id": 757, "seek": 365036, "start": 3676.48, "end": 3679.52, "text": " Our partial dependence plot we could include the individual levels", "tokens": [2621, 14641, 31704, 7542, 321, 727, 4090, 264, 2609, 4358], "temperature": 0.0, "avg_logprob": -0.17158000268668772, "compression_ratio": 1.773076923076923, "no_speech_prob": 2.1567980184045155e-06}, {"id": 758, "seek": 367952, "start": 3679.52, "end": 3681.52, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.19323945478959517, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.225264552180306e-06}, {"id": 759, "seek": 367952, "start": 3682.16, "end": 3684.68, "text": " It doesn't necessarily make the model worse", "tokens": [467, 1177, 380, 4725, 652, 264, 2316, 5324], "temperature": 0.0, "avg_logprob": -0.19323945478959517, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.225264552180306e-06}, {"id": 760, "seek": 367952, "start": 3685.28, "end": 3690.68, "text": " It may make it better, but it probably won't change it much at all in this case it hardly changed it", "tokens": [467, 815, 652, 309, 1101, 11, 457, 309, 1391, 1582, 380, 1319, 309, 709, 412, 439, 294, 341, 1389, 309, 13572, 3105, 309], "temperature": 0.0, "avg_logprob": -0.19323945478959517, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.225264552180306e-06}, {"id": 761, "seek": 367952, "start": 3690.8, "end": 3695.88, "text": " This is something that we have noticed on real data also that if cardinality is higher", "tokens": [639, 307, 746, 300, 321, 362, 5694, 322, 957, 1412, 611, 300, 498, 2920, 259, 1860, 307, 2946], "temperature": 0.0, "avg_logprob": -0.19323945478959517, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.225264552180306e-06}, {"id": 762, "seek": 367952, "start": 3695.96, "end": 3702.8, "text": " Let's say 50 levels and if you do one hot encoding the random forest performs very badly and yeah", "tokens": [961, 311, 584, 2625, 4358, 293, 498, 291, 360, 472, 2368, 43430, 264, 4974, 6719, 26213, 588, 13425, 293, 1338], "temperature": 0.0, "avg_logprob": -0.19323945478959517, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.225264552180306e-06}, {"id": 763, "seek": 367952, "start": 3702.8, "end": 3707.0, "text": " That's right if the card now, that's why we have that in that's why in fast AI we have that like maximum", "tokens": [663, 311, 558, 498, 264, 2920, 586, 11, 300, 311, 983, 321, 362, 300, 294, 300, 311, 983, 294, 2370, 7318, 321, 362, 300, 411, 6674], "temperature": 0.0, "avg_logprob": -0.19323945478959517, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.225264552180306e-06}, {"id": 764, "seek": 370700, "start": 3707.0, "end": 3714.52, "text": " Maximum categorical size, you know because at some point your one hot encoded variables become too sparse", "tokens": [29076, 449, 19250, 804, 2744, 11, 291, 458, 570, 412, 512, 935, 428, 472, 2368, 2058, 12340, 9102, 1813, 886, 637, 11668], "temperature": 0.0, "avg_logprob": -0.20024455886289297, "compression_ratio": 1.5458715596330275, "no_speech_prob": 1.8738645621851902e-06}, {"id": 765, "seek": 370700, "start": 3714.56, "end": 3718.16, "text": " Right, so I generally like cut it off at six or seven", "tokens": [1779, 11, 370, 286, 5101, 411, 1723, 309, 766, 412, 2309, 420, 3407], "temperature": 0.0, "avg_logprob": -0.20024455886289297, "compression_ratio": 1.5458715596330275, "no_speech_prob": 1.8738645621851902e-06}, {"id": 766, "seek": 370700, "start": 3718.96, "end": 3724.2, "text": " Also because like when you get past that it's kind of becomes less useful because the feature importance", "tokens": [2743, 570, 411, 562, 291, 483, 1791, 300, 309, 311, 733, 295, 3643, 1570, 4420, 570, 264, 4111, 7379], "temperature": 0.0, "avg_logprob": -0.20024455886289297, "compression_ratio": 1.5458715596330275, "no_speech_prob": 1.8738645621851902e-06}, {"id": 767, "seek": 370700, "start": 3724.2, "end": 3726.44, "text": " There's going to be too many levels to really look at", "tokens": [821, 311, 516, 281, 312, 886, 867, 4358, 281, 534, 574, 412], "temperature": 0.0, "avg_logprob": -0.20024455886289297, "compression_ratio": 1.5458715596330275, "no_speech_prob": 1.8738645621851902e-06}, {"id": 768, "seek": 370700, "start": 3730.88, "end": 3732.88, "text": " So can it not just", "tokens": [407, 393, 309, 406, 445], "temperature": 0.0, "avg_logprob": -0.20024455886289297, "compression_ratio": 1.5458715596330275, "no_speech_prob": 1.8738645621851902e-06}, {"id": 769, "seek": 373288, "start": 3732.88, "end": 3739.76, "text": " Not look at those levels which are not important and just gives those significant features as important", "tokens": [1726, 574, 412, 729, 4358, 597, 366, 406, 1021, 293, 445, 2709, 729, 4776, 4122, 382, 1021], "temperature": 0.0, "avg_logprob": -0.15074294678708341, "compression_ratio": 1.7293577981651376, "no_speech_prob": 2.02612386601686e-06}, {"id": 770, "seek": 373288, "start": 3739.76, "end": 3744.76, "text": " Yeah, yeah, I mean it's it's it's it'll be okay. You know it's just like", "tokens": [865, 11, 1338, 11, 286, 914, 309, 311, 309, 311, 309, 311, 309, 603, 312, 1392, 13, 509, 458, 309, 311, 445, 411], "temperature": 0.0, "avg_logprob": -0.15074294678708341, "compression_ratio": 1.7293577981651376, "no_speech_prob": 2.02612386601686e-06}, {"id": 771, "seek": 373288, "start": 3745.92, "end": 3750.6400000000003, "text": " Once the cardinality increases too high you're just you're just splitting your data up", "tokens": [3443, 264, 2920, 259, 1860, 8637, 886, 1090, 291, 434, 445, 291, 434, 445, 30348, 428, 1412, 493], "temperature": 0.0, "avg_logprob": -0.15074294678708341, "compression_ratio": 1.7293577981651376, "no_speech_prob": 2.02612386601686e-06}, {"id": 772, "seek": 373288, "start": 3751.32, "end": 3755.7400000000002, "text": " You know too much basically and so in practice your your", "tokens": [509, 458, 886, 709, 1936, 293, 370, 294, 3124, 428, 428], "temperature": 0.0, "avg_logprob": -0.15074294678708341, "compression_ratio": 1.7293577981651376, "no_speech_prob": 2.02612386601686e-06}, {"id": 773, "seek": 373288, "start": 3756.1600000000003, "end": 3759.12, "text": " Ordinal version is likely to be it's likely to be better", "tokens": [29388, 2071, 3037, 307, 3700, 281, 312, 309, 311, 3700, 281, 312, 1101], "temperature": 0.0, "avg_logprob": -0.15074294678708341, "compression_ratio": 1.7293577981651376, "no_speech_prob": 2.02612386601686e-06}, {"id": 774, "seek": 375912, "start": 3759.12, "end": 3761.12, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.1713473329839018, "compression_ratio": 1.6178861788617886, "no_speech_prob": 3.5007933547603898e-06}, {"id": 775, "seek": 375912, "start": 3766.16, "end": 3768.72, "text": " Yeah, so little there's no time to kind of review everything", "tokens": [865, 11, 370, 707, 456, 311, 572, 565, 281, 733, 295, 3131, 1203], "temperature": 0.0, "avg_logprob": -0.1713473329839018, "compression_ratio": 1.6178861788617886, "no_speech_prob": 3.5007933547603898e-06}, {"id": 776, "seek": 375912, "start": 3768.72, "end": 3772.96, "text": " But I think that's the kind of key concepts and then of course remembering that you know the embedding", "tokens": [583, 286, 519, 300, 311, 264, 733, 295, 2141, 10392, 293, 550, 295, 1164, 20719, 300, 291, 458, 264, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.1713473329839018, "compression_ratio": 1.6178861788617886, "no_speech_prob": 3.5007933547603898e-06}, {"id": 777, "seek": 375912, "start": 3773.2, "end": 3776.7599999999998, "text": " Matrix that we could use is likely to have more than just one coefficient", "tokens": [36274, 300, 321, 727, 764, 307, 3700, 281, 362, 544, 813, 445, 472, 17619], "temperature": 0.0, "avg_logprob": -0.1713473329839018, "compression_ratio": 1.6178861788617886, "no_speech_prob": 3.5007933547603898e-06}, {"id": 778, "seek": 375912, "start": 3776.8399999999997, "end": 3782.8399999999997, "text": " We'll actually have a dimensionality of a few coefficients which isn't going to be useful for most linear models", "tokens": [492, 603, 767, 362, 257, 10139, 1860, 295, 257, 1326, 31994, 597, 1943, 380, 516, 281, 312, 4420, 337, 881, 8213, 5245], "temperature": 0.0, "avg_logprob": -0.1713473329839018, "compression_ratio": 1.6178861788617886, "no_speech_prob": 3.5007933547603898e-06}, {"id": 779, "seek": 375912, "start": 3783.0, "end": 3785.0, "text": " But once you've got multi-layer models", "tokens": [583, 1564, 291, 600, 658, 4825, 12, 8376, 260, 5245], "temperature": 0.0, "avg_logprob": -0.1713473329839018, "compression_ratio": 1.6178861788617886, "no_speech_prob": 3.5007933547603898e-06}, {"id": 780, "seek": 378500, "start": 3785.0, "end": 3792.08, "text": " That's now creating a representation of your category which is kind of quite a lot richer, and you can do a lot more with it", "tokens": [663, 311, 586, 4084, 257, 10290, 295, 428, 7719, 597, 307, 733, 295, 1596, 257, 688, 29021, 11, 293, 291, 393, 360, 257, 688, 544, 365, 309], "temperature": 0.0, "avg_logprob": -0.15170837354056443, "compression_ratio": 1.5392156862745099, "no_speech_prob": 8.579218047088943e-07}, {"id": 781, "seek": 378500, "start": 3793.68, "end": 3795.68, "text": " Let's now talk about the most important bit", "tokens": [961, 311, 586, 751, 466, 264, 881, 1021, 857], "temperature": 0.0, "avg_logprob": -0.15170837354056443, "compression_ratio": 1.5392156862745099, "no_speech_prob": 8.579218047088943e-07}, {"id": 782, "seek": 378500, "start": 3797.12, "end": 3799.12, "text": " We started off", "tokens": [492, 1409, 766], "temperature": 0.0, "avg_logprob": -0.15170837354056443, "compression_ratio": 1.5392156862745099, "no_speech_prob": 8.579218047088943e-07}, {"id": 783, "seek": 378500, "start": 3800.6, "end": 3802.6, "text": " Early in this course", "tokens": [18344, 294, 341, 1164], "temperature": 0.0, "avg_logprob": -0.15170837354056443, "compression_ratio": 1.5392156862745099, "no_speech_prob": 8.579218047088943e-07}, {"id": 784, "seek": 378500, "start": 3802.92, "end": 3804.92, "text": " talking about how", "tokens": [1417, 466, 577], "temperature": 0.0, "avg_logprob": -0.15170837354056443, "compression_ratio": 1.5392156862745099, "no_speech_prob": 8.579218047088943e-07}, {"id": 785, "seek": 378500, "start": 3804.96, "end": 3807.76, "text": " Actually a lot of machine learning is kind of misplaced", "tokens": [5135, 257, 688, 295, 3479, 2539, 307, 733, 295, 3346, 564, 3839], "temperature": 0.0, "avg_logprob": -0.15170837354056443, "compression_ratio": 1.5392156862745099, "no_speech_prob": 8.579218047088943e-07}, {"id": 786, "seek": 378500, "start": 3808.44, "end": 3811.08, "text": " People focus on predictive accuracy", "tokens": [3432, 1879, 322, 35521, 14170], "temperature": 0.0, "avg_logprob": -0.15170837354056443, "compression_ratio": 1.5392156862745099, "no_speech_prob": 8.579218047088943e-07}, {"id": 787, "seek": 381108, "start": 3811.08, "end": 3816.04, "text": " Like Amazon has a collaborative filtering algorithm for recommending books", "tokens": [1743, 6795, 575, 257, 16555, 30822, 9284, 337, 30559, 3642], "temperature": 0.0, "avg_logprob": -0.16808517306458717, "compression_ratio": 1.7686567164179106, "no_speech_prob": 9.570784413881483e-07}, {"id": 788, "seek": 381108, "start": 3816.04, "end": 3821.68, "text": " And they end up recommending the book which it thinks you're most likely to to write highly and", "tokens": [400, 436, 917, 493, 30559, 264, 1446, 597, 309, 7309, 291, 434, 881, 3700, 281, 281, 2464, 5405, 293], "temperature": 0.0, "avg_logprob": -0.16808517306458717, "compression_ratio": 1.7686567164179106, "no_speech_prob": 9.570784413881483e-07}, {"id": 789, "seek": 381108, "start": 3822.48, "end": 3825.84, "text": " So what they end up doing is probably recommending a book that you already have", "tokens": [407, 437, 436, 917, 493, 884, 307, 1391, 30559, 257, 1446, 300, 291, 1217, 362], "temperature": 0.0, "avg_logprob": -0.16808517306458717, "compression_ratio": 1.7686567164179106, "no_speech_prob": 9.570784413881483e-07}, {"id": 790, "seek": 381108, "start": 3826.68, "end": 3830.2, "text": " Or that you already know about and would have bought anyway, right?", "tokens": [1610, 300, 291, 1217, 458, 466, 293, 576, 362, 4243, 4033, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16808517306458717, "compression_ratio": 1.7686567164179106, "no_speech_prob": 9.570784413881483e-07}, {"id": 791, "seek": 381108, "start": 3830.2, "end": 3835.24, "text": " Which isn't very valuable what they should instead have done is to figure out like which book", "tokens": [3013, 1943, 380, 588, 8263, 437, 436, 820, 2602, 362, 1096, 307, 281, 2573, 484, 411, 597, 1446], "temperature": 0.0, "avg_logprob": -0.16808517306458717, "compression_ratio": 1.7686567164179106, "no_speech_prob": 9.570784413881483e-07}, {"id": 792, "seek": 381108, "start": 3836.08, "end": 3838.72, "text": " Can I recommend that would cause you to change your behavior?", "tokens": [1664, 286, 2748, 300, 576, 3082, 291, 281, 1319, 428, 5223, 30], "temperature": 0.0, "avg_logprob": -0.16808517306458717, "compression_ratio": 1.7686567164179106, "no_speech_prob": 9.570784413881483e-07}, {"id": 793, "seek": 383872, "start": 3838.72, "end": 3841.3999999999996, "text": " Right and so that way we actually", "tokens": [1779, 293, 370, 300, 636, 321, 767], "temperature": 0.0, "avg_logprob": -0.2674068682121508, "compression_ratio": 1.6173469387755102, "no_speech_prob": 2.726426146182348e-06}, {"id": 794, "seek": 383872, "start": 3842.2, "end": 3844.74, "text": " Maximize our lift in sales due to recommendations", "tokens": [29076, 1125, 527, 5533, 294, 5763, 3462, 281, 10434], "temperature": 0.0, "avg_logprob": -0.2674068682121508, "compression_ratio": 1.6173469387755102, "no_speech_prob": 2.726426146182348e-06}, {"id": 795, "seek": 383872, "start": 3845.24, "end": 3849.7599999999998, "text": " And so this idea of like the difference between optimizing", "tokens": [400, 370, 341, 1558, 295, 411, 264, 2649, 1296, 40425], "temperature": 0.0, "avg_logprob": -0.2674068682121508, "compression_ratio": 1.6173469387755102, "no_speech_prob": 2.726426146182348e-06}, {"id": 796, "seek": 383872, "start": 3851.68, "end": 3856.12, "text": " Influencing your actions versus just kind of improving predictive accuracy", "tokens": [11537, 2781, 13644, 428, 5909, 5717, 445, 733, 295, 11470, 35521, 14170], "temperature": 0.0, "avg_logprob": -0.2674068682121508, "compression_ratio": 1.6173469387755102, "no_speech_prob": 2.726426146182348e-06}, {"id": 797, "seek": 383872, "start": 3857.2799999999997, "end": 3863.64, "text": " Improving predictive accuracy is a really important distinction which is like very rarely discussed", "tokens": [8270, 340, 798, 35521, 14170, 307, 257, 534, 1021, 16844, 597, 307, 411, 588, 13752, 7152], "temperature": 0.0, "avg_logprob": -0.2674068682121508, "compression_ratio": 1.6173469387755102, "no_speech_prob": 2.726426146182348e-06}, {"id": 798, "seek": 386364, "start": 3863.64, "end": 3869.96, "text": " In academia or industry kind of crazily enough. It's more discussed in industry", "tokens": [682, 28937, 420, 3518, 733, 295, 46348, 953, 1547, 13, 467, 311, 544, 7152, 294, 3518], "temperature": 0.0, "avg_logprob": -0.1758507319859096, "compression_ratio": 1.6072874493927125, "no_speech_prob": 2.6015927687694784e-06}, {"id": 799, "seek": 386364, "start": 3869.96, "end": 3874.72, "text": " It's particularly ignored in most of academia, right? So it's a really", "tokens": [467, 311, 4098, 19735, 294, 881, 295, 28937, 11, 558, 30, 407, 309, 311, 257, 534], "temperature": 0.0, "avg_logprob": -0.1758507319859096, "compression_ratio": 1.6072874493927125, "no_speech_prob": 2.6015927687694784e-06}, {"id": 800, "seek": 386364, "start": 3875.6, "end": 3882.7999999999997, "text": " Important idea which is that in the end the idea the goal of your model presumably is to influence behavior", "tokens": [42908, 1558, 597, 307, 300, 294, 264, 917, 264, 1558, 264, 3387, 295, 428, 2316, 26742, 307, 281, 6503, 5223], "temperature": 0.0, "avg_logprob": -0.1758507319859096, "compression_ratio": 1.6072874493927125, "no_speech_prob": 2.6015927687694784e-06}, {"id": 801, "seek": 386364, "start": 3883.48, "end": 3887.24, "text": " Okay, and so and remember I actually mentioned a whole paper", "tokens": [1033, 11, 293, 370, 293, 1604, 286, 767, 2835, 257, 1379, 3035], "temperature": 0.0, "avg_logprob": -0.1758507319859096, "compression_ratio": 1.6072874493927125, "no_speech_prob": 2.6015927687694784e-06}, {"id": 802, "seek": 386364, "start": 3887.24, "end": 3890.4, "text": " I have about this where I introduce this thing called the drivetrain approach", "tokens": [286, 362, 466, 341, 689, 286, 5366, 341, 551, 1219, 264, 1630, 9771, 7146, 3109], "temperature": 0.0, "avg_logprob": -0.1758507319859096, "compression_ratio": 1.6072874493927125, "no_speech_prob": 2.6015927687694784e-06}, {"id": 803, "seek": 389040, "start": 3890.4, "end": 3893.7200000000003, "text": " Where I talk about like ways to think about how to incorporate", "tokens": [2305, 286, 751, 466, 411, 2098, 281, 519, 466, 577, 281, 16091], "temperature": 0.0, "avg_logprob": -0.198647188585858, "compression_ratio": 1.7136363636363636, "no_speech_prob": 2.6841785256692674e-06}, {"id": 804, "seek": 389040, "start": 3894.6800000000003, "end": 3896.2400000000002, "text": " machine learning", "tokens": [3479, 2539], "temperature": 0.0, "avg_logprob": -0.198647188585858, "compression_ratio": 1.7136363636363636, "no_speech_prob": 2.6841785256692674e-06}, {"id": 805, "seek": 389040, "start": 3896.2400000000002, "end": 3899.8, "text": " Into like how do we actually influence behavior?", "tokens": [23373, 411, 577, 360, 321, 767, 6503, 5223, 30], "temperature": 0.0, "avg_logprob": -0.198647188585858, "compression_ratio": 1.7136363636363636, "no_speech_prob": 2.6841785256692674e-06}, {"id": 806, "seek": 389040, "start": 3900.64, "end": 3906.64, "text": " So you know that's a starting point, but then the next question is like okay if we're trying to influence behavior", "tokens": [407, 291, 458, 300, 311, 257, 2891, 935, 11, 457, 550, 264, 958, 1168, 307, 411, 1392, 498, 321, 434, 1382, 281, 6503, 5223], "temperature": 0.0, "avg_logprob": -0.198647188585858, "compression_ratio": 1.7136363636363636, "no_speech_prob": 2.6841785256692674e-06}, {"id": 807, "seek": 389040, "start": 3907.6800000000003, "end": 3913.56, "text": " What kind of behavior should we be influencing and how and what might it mean?", "tokens": [708, 733, 295, 5223, 820, 321, 312, 40396, 293, 577, 293, 437, 1062, 309, 914, 30], "temperature": 0.0, "avg_logprob": -0.198647188585858, "compression_ratio": 1.7136363636363636, "no_speech_prob": 2.6841785256692674e-06}, {"id": 808, "seek": 389040, "start": 3914.12, "end": 3917.28, "text": " when we start influencing behavior, right because like", "tokens": [562, 321, 722, 40396, 5223, 11, 558, 570, 411], "temperature": 0.0, "avg_logprob": -0.198647188585858, "compression_ratio": 1.7136363636363636, "no_speech_prob": 2.6841785256692674e-06}, {"id": 809, "seek": 391728, "start": 3917.28, "end": 3922.6800000000003, "text": " Nowadays like a lot of the companies that you're going to end up working at", "tokens": [28908, 411, 257, 688, 295, 264, 3431, 300, 291, 434, 516, 281, 917, 493, 1364, 412], "temperature": 0.0, "avg_logprob": -0.19311284494924022, "compression_ratio": 1.619718309859155, "no_speech_prob": 2.3687825887463987e-06}, {"id": 810, "seek": 391728, "start": 3923.28, "end": 3928.6000000000004, "text": " Are big ass companies and you'll be building stuff that can influence millions of people", "tokens": [2014, 955, 1256, 3431, 293, 291, 603, 312, 2390, 1507, 300, 393, 6503, 6803, 295, 561], "temperature": 0.0, "avg_logprob": -0.19311284494924022, "compression_ratio": 1.619718309859155, "no_speech_prob": 2.3687825887463987e-06}, {"id": 811, "seek": 391728, "start": 3929.32, "end": 3931.4, "text": " Right, so what does that mean?", "tokens": [1779, 11, 370, 437, 775, 300, 914, 30], "temperature": 0.0, "avg_logprob": -0.19311284494924022, "compression_ratio": 1.619718309859155, "no_speech_prob": 2.3687825887463987e-06}, {"id": 812, "seek": 391728, "start": 3933.1600000000003, "end": 3934.6800000000003, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.19311284494924022, "compression_ratio": 1.619718309859155, "no_speech_prob": 2.3687825887463987e-06}, {"id": 813, "seek": 391728, "start": 3934.6800000000003, "end": 3940.1200000000003, "text": " I'm actually I'm not going to tell you what it means because like I don't know all I'm going to try and do is", "tokens": [286, 478, 767, 286, 478, 406, 516, 281, 980, 291, 437, 309, 1355, 570, 411, 286, 500, 380, 458, 439, 286, 478, 516, 281, 853, 293, 360, 307], "temperature": 0.0, "avg_logprob": -0.19311284494924022, "compression_ratio": 1.619718309859155, "no_speech_prob": 2.3687825887463987e-06}, {"id": 814, "seek": 391728, "start": 3940.52, "end": 3942.52, "text": " Make you aware of some of the issues", "tokens": [4387, 291, 3650, 295, 512, 295, 264, 2663], "temperature": 0.0, "avg_logprob": -0.19311284494924022, "compression_ratio": 1.619718309859155, "no_speech_prob": 2.3687825887463987e-06}, {"id": 815, "seek": 394252, "start": 3942.52, "end": 3947.98, "text": " Right and and make you believe two things about them first that you should care", "tokens": [1779, 293, 293, 652, 291, 1697, 732, 721, 466, 552, 700, 300, 291, 820, 1127], "temperature": 0.0, "avg_logprob": -0.18297130458957547, "compression_ratio": 1.7268518518518519, "no_speech_prob": 4.565911240206333e-06}, {"id": 816, "seek": 394252, "start": 3948.56, "end": 3953.88, "text": " Right and second that they're big current issues, right?", "tokens": [1779, 293, 1150, 300, 436, 434, 955, 2190, 2663, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18297130458957547, "compression_ratio": 1.7268518518518519, "no_speech_prob": 4.565911240206333e-06}, {"id": 817, "seek": 394252, "start": 3954.24, "end": 3960.0, "text": " The main reason I want you to care is because I want you to want to be a good person and", "tokens": [440, 2135, 1778, 286, 528, 291, 281, 1127, 307, 570, 286, 528, 291, 281, 528, 281, 312, 257, 665, 954, 293], "temperature": 0.0, "avg_logprob": -0.18297130458957547, "compression_ratio": 1.7268518518518519, "no_speech_prob": 4.565911240206333e-06}, {"id": 818, "seek": 394252, "start": 3960.2, "end": 3963.48, "text": " Show you that like not thinking about these things will make you a bad person", "tokens": [6895, 291, 300, 411, 406, 1953, 466, 613, 721, 486, 652, 291, 257, 1578, 954], "temperature": 0.0, "avg_logprob": -0.18297130458957547, "compression_ratio": 1.7268518518518519, "no_speech_prob": 4.565911240206333e-06}, {"id": 819, "seek": 394252, "start": 3964.24, "end": 3967.84, "text": " But if you don't find that convincing I will tell you this", "tokens": [583, 498, 291, 500, 380, 915, 300, 24823, 286, 486, 980, 291, 341], "temperature": 0.0, "avg_logprob": -0.18297130458957547, "compression_ratio": 1.7268518518518519, "no_speech_prob": 4.565911240206333e-06}, {"id": 820, "seek": 394252, "start": 3969.28, "end": 3970.64, "text": " Volkswagen", "tokens": [39856], "temperature": 0.0, "avg_logprob": -0.18297130458957547, "compression_ratio": 1.7268518518518519, "no_speech_prob": 4.565911240206333e-06}, {"id": 821, "seek": 397064, "start": 3970.64, "end": 3974.56, "text": " Were found to be cheating on their emissions tests", "tokens": [12448, 1352, 281, 312, 18309, 322, 641, 14607, 6921], "temperature": 0.0, "avg_logprob": -0.13525306132801793, "compression_ratio": 1.7644628099173554, "no_speech_prob": 1.2679216752076172e-06}, {"id": 822, "seek": 397064, "start": 3975.8399999999997, "end": 3981.44, "text": " The person who was sent to jail for it was the programmer that implemented that piece of code", "tokens": [440, 954, 567, 390, 2279, 281, 10511, 337, 309, 390, 264, 32116, 300, 12270, 300, 2522, 295, 3089], "temperature": 0.0, "avg_logprob": -0.13525306132801793, "compression_ratio": 1.7644628099173554, "no_speech_prob": 1.2679216752076172e-06}, {"id": 823, "seek": 397064, "start": 3982.08, "end": 3984.2799999999997, "text": " They did exactly what they were told to do", "tokens": [814, 630, 2293, 437, 436, 645, 1907, 281, 360], "temperature": 0.0, "avg_logprob": -0.13525306132801793, "compression_ratio": 1.7644628099173554, "no_speech_prob": 1.2679216752076172e-06}, {"id": 824, "seek": 397064, "start": 3984.7599999999998, "end": 3990.7999999999997, "text": " Right and so if you're coming in here thinking hey, I'm just a techie. You know I'll just do what I'm told", "tokens": [1779, 293, 370, 498, 291, 434, 1348, 294, 510, 1953, 4177, 11, 286, 478, 445, 257, 7553, 414, 13, 509, 458, 286, 603, 445, 360, 437, 286, 478, 1907], "temperature": 0.0, "avg_logprob": -0.13525306132801793, "compression_ratio": 1.7644628099173554, "no_speech_prob": 1.2679216752076172e-06}, {"id": 825, "seek": 397064, "start": 3991.3599999999997, "end": 3994.12, "text": " Right, that's that's my job is to do what I'm told", "tokens": [1779, 11, 300, 311, 300, 311, 452, 1691, 307, 281, 360, 437, 286, 478, 1907], "temperature": 0.0, "avg_logprob": -0.13525306132801793, "compression_ratio": 1.7644628099173554, "no_speech_prob": 1.2679216752076172e-06}, {"id": 826, "seek": 397064, "start": 3994.12, "end": 3998.3199999999997, "text": " I'm telling you if you do that you can be sent to jail for doing what you're told", "tokens": [286, 478, 3585, 291, 498, 291, 360, 300, 291, 393, 312, 2279, 281, 10511, 337, 884, 437, 291, 434, 1907], "temperature": 0.0, "avg_logprob": -0.13525306132801793, "compression_ratio": 1.7644628099173554, "no_speech_prob": 1.2679216752076172e-06}, {"id": 827, "seek": 399832, "start": 3998.32, "end": 4005.28, "text": " Okay, so so a don't just do what you're told because you can be a bad person and B", "tokens": [1033, 11, 370, 370, 257, 500, 380, 445, 360, 437, 291, 434, 1907, 570, 291, 393, 312, 257, 1578, 954, 293, 363], "temperature": 0.0, "avg_logprob": -0.16381804923701093, "compression_ratio": 1.803030303030303, "no_speech_prob": 4.6644342432955455e-07}, {"id": 828, "seek": 399832, "start": 4005.8, "end": 4007.6000000000004, "text": " You can go to jail", "tokens": [509, 393, 352, 281, 10511], "temperature": 0.0, "avg_logprob": -0.16381804923701093, "compression_ratio": 1.803030303030303, "no_speech_prob": 4.6644342432955455e-07}, {"id": 829, "seek": 399832, "start": 4007.6000000000004, "end": 4009.28, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.16381804923701093, "compression_ratio": 1.803030303030303, "no_speech_prob": 4.6644342432955455e-07}, {"id": 830, "seek": 399832, "start": 4009.28, "end": 4011.28, "text": " second thing to realize is", "tokens": [1150, 551, 281, 4325, 307], "temperature": 0.0, "avg_logprob": -0.16381804923701093, "compression_ratio": 1.803030303030303, "no_speech_prob": 4.6644342432955455e-07}, {"id": 831, "seek": 399832, "start": 4011.6400000000003, "end": 4015.56, "text": " In the heat of the moment you're in a meeting with 20 people at work", "tokens": [682, 264, 3738, 295, 264, 1623, 291, 434, 294, 257, 3440, 365, 945, 561, 412, 589], "temperature": 0.0, "avg_logprob": -0.16381804923701093, "compression_ratio": 1.803030303030303, "no_speech_prob": 4.6644342432955455e-07}, {"id": 832, "seek": 399832, "start": 4015.56, "end": 4017.76, "text": " And you're all talking about how you're going to implement", "tokens": [400, 291, 434, 439, 1417, 466, 577, 291, 434, 516, 281, 4445], "temperature": 0.0, "avg_logprob": -0.16381804923701093, "compression_ratio": 1.803030303030303, "no_speech_prob": 4.6644342432955455e-07}, {"id": 833, "seek": 399832, "start": 4018.04, "end": 4022.0, "text": " You know this new feature and everybody's discussing it and there's some part", "tokens": [509, 458, 341, 777, 4111, 293, 2201, 311, 10850, 309, 293, 456, 311, 512, 644], "temperature": 0.0, "avg_logprob": -0.16381804923701093, "compression_ratio": 1.803030303030303, "no_speech_prob": 4.6644342432955455e-07}, {"id": 834, "seek": 399832, "start": 4022.0, "end": 4026.32, "text": " You know and everybody's like we could do this and here's a way of modeling it and then we can implement it and here's these constraints", "tokens": [509, 458, 293, 2201, 311, 411, 321, 727, 360, 341, 293, 510, 311, 257, 636, 295, 15983, 309, 293, 550, 321, 393, 4445, 309, 293, 510, 311, 613, 18491], "temperature": 0.0, "avg_logprob": -0.16381804923701093, "compression_ratio": 1.803030303030303, "no_speech_prob": 4.6644342432955455e-07}, {"id": 835, "seek": 402632, "start": 4026.32, "end": 4028.32, "text": " And there's some part of you that's thinking", "tokens": [400, 456, 311, 512, 644, 295, 291, 300, 311, 1953], "temperature": 0.0, "avg_logprob": -0.1805596984593214, "compression_ratio": 1.7862903225806452, "no_speech_prob": 6.048880095477216e-06}, {"id": 836, "seek": 402632, "start": 4028.6800000000003, "end": 4030.6800000000003, "text": " Am I sure we should be doing this?", "tokens": [2012, 286, 988, 321, 820, 312, 884, 341, 30], "temperature": 0.0, "avg_logprob": -0.1805596984593214, "compression_ratio": 1.7862903225806452, "no_speech_prob": 6.048880095477216e-06}, {"id": 837, "seek": 402632, "start": 4031.0, "end": 4035.04, "text": " Right that's not the right time to be thinking about that because it's really hard", "tokens": [1779, 300, 311, 406, 264, 558, 565, 281, 312, 1953, 466, 300, 570, 309, 311, 534, 1152], "temperature": 0.0, "avg_logprob": -0.1805596984593214, "compression_ratio": 1.7862903225806452, "no_speech_prob": 6.048880095477216e-06}, {"id": 838, "seek": 402632, "start": 4035.8, "end": 4040.04, "text": " So like step up then and say excuse me. I'm not sure", "tokens": [407, 411, 1823, 493, 550, 293, 584, 8960, 385, 13, 286, 478, 406, 988], "temperature": 0.0, "avg_logprob": -0.1805596984593214, "compression_ratio": 1.7862903225806452, "no_speech_prob": 6.048880095477216e-06}, {"id": 839, "seek": 402632, "start": 4040.6000000000004, "end": 4045.96, "text": " This is a good idea you actually need to think about how you would handle that situation ahead of time", "tokens": [639, 307, 257, 665, 1558, 291, 767, 643, 281, 519, 466, 577, 291, 576, 4813, 300, 2590, 2286, 295, 565], "temperature": 0.0, "avg_logprob": -0.1805596984593214, "compression_ratio": 1.7862903225806452, "no_speech_prob": 6.048880095477216e-06}, {"id": 840, "seek": 402632, "start": 4046.36, "end": 4048.6400000000003, "text": " Right so I want you to like think about", "tokens": [1779, 370, 286, 528, 291, 281, 411, 519, 466], "temperature": 0.0, "avg_logprob": -0.1805596984593214, "compression_ratio": 1.7862903225806452, "no_speech_prob": 6.048880095477216e-06}, {"id": 841, "seek": 402632, "start": 4049.48, "end": 4055.4, "text": " About these issues now right and realize that by the time you're in the middle of it", "tokens": [7769, 613, 2663, 586, 558, 293, 4325, 300, 538, 264, 565, 291, 434, 294, 264, 2808, 295, 309], "temperature": 0.0, "avg_logprob": -0.1805596984593214, "compression_ratio": 1.7862903225806452, "no_speech_prob": 6.048880095477216e-06}, {"id": 842, "seek": 405540, "start": 4055.4, "end": 4057.4, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.16412164632556508, "compression_ratio": 1.6641221374045803, "no_speech_prob": 3.2887198813114082e-06}, {"id": 843, "seek": 405540, "start": 4057.44, "end": 4059.44, "text": " You might not even realize it's happening", "tokens": [509, 1062, 406, 754, 4325, 309, 311, 2737], "temperature": 0.0, "avg_logprob": -0.16412164632556508, "compression_ratio": 1.6641221374045803, "no_speech_prob": 3.2887198813114082e-06}, {"id": 844, "seek": 405540, "start": 4059.6, "end": 4064.44, "text": " You know like notice it'll just be a meeting like every other meeting and a bunch of people will be talking about how to solve", "tokens": [509, 458, 411, 3449, 309, 603, 445, 312, 257, 3440, 411, 633, 661, 3440, 293, 257, 3840, 295, 561, 486, 312, 1417, 466, 577, 281, 5039], "temperature": 0.0, "avg_logprob": -0.16412164632556508, "compression_ratio": 1.6641221374045803, "no_speech_prob": 3.2887198813114082e-06}, {"id": 845, "seek": 405540, "start": 4064.44, "end": 4069.2000000000003, "text": " This technical question okay, and you need to be able to recognize like oh", "tokens": [639, 6191, 1168, 1392, 11, 293, 291, 643, 281, 312, 1075, 281, 5521, 411, 1954], "temperature": 0.0, "avg_logprob": -0.16412164632556508, "compression_ratio": 1.6641221374045803, "no_speech_prob": 3.2887198813114082e-06}, {"id": 846, "seek": 405540, "start": 4069.76, "end": 4072.32, "text": " This is actually something with ethical implications", "tokens": [639, 307, 767, 746, 365, 18890, 16602], "temperature": 0.0, "avg_logprob": -0.16412164632556508, "compression_ratio": 1.6641221374045803, "no_speech_prob": 3.2887198813114082e-06}, {"id": 847, "seek": 405540, "start": 4073.08, "end": 4074.36, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.16412164632556508, "compression_ratio": 1.6641221374045803, "no_speech_prob": 3.2887198813114082e-06}, {"id": 848, "seek": 405540, "start": 4074.36, "end": 4076.84, "text": " Rachel actually wrote all of these slides", "tokens": [14246, 767, 4114, 439, 295, 613, 9788], "temperature": 0.0, "avg_logprob": -0.16412164632556508, "compression_ratio": 1.6641221374045803, "no_speech_prob": 3.2887198813114082e-06}, {"id": 849, "seek": 405540, "start": 4076.84, "end": 4081.48, "text": " I'm sorry she can't be here to present this because like she's studied this in depth and", "tokens": [286, 478, 2597, 750, 393, 380, 312, 510, 281, 1974, 341, 570, 411, 750, 311, 9454, 341, 294, 7161, 293], "temperature": 0.0, "avg_logprob": -0.16412164632556508, "compression_ratio": 1.6641221374045803, "no_speech_prob": 3.2887198813114082e-06}, {"id": 850, "seek": 408148, "start": 4081.48, "end": 4087.92, "text": " You know she's actually been in in in difficult environments herself where she's kind of seen these things", "tokens": [509, 458, 750, 311, 767, 668, 294, 294, 294, 2252, 12388, 7530, 689, 750, 311, 733, 295, 1612, 613, 721], "temperature": 0.0, "avg_logprob": -0.16156965891520184, "compression_ratio": 1.7354497354497354, "no_speech_prob": 3.5559421576181194e-06}, {"id": 851, "seek": 408148, "start": 4088.68, "end": 4090.68, "text": " happening you know and", "tokens": [2737, 291, 458, 293], "temperature": 0.0, "avg_logprob": -0.16156965891520184, "compression_ratio": 1.7354497354497354, "no_speech_prob": 3.5559421576181194e-06}, {"id": 852, "seek": 408148, "start": 4092.0, "end": 4098.92, "text": " We know how hard it is right, but let me give you a sense of like what happens right so so", "tokens": [492, 458, 577, 1152, 309, 307, 558, 11, 457, 718, 385, 976, 291, 257, 2020, 295, 411, 437, 2314, 558, 370, 370], "temperature": 0.0, "avg_logprob": -0.16156965891520184, "compression_ratio": 1.7354497354497354, "no_speech_prob": 3.5559421576181194e-06}, {"id": 853, "seek": 408148, "start": 4099.88, "end": 4108.08, "text": " Engineers trying to solve engineering problems is you know and causing problems is not a new thing right so", "tokens": [43950, 1382, 281, 5039, 7043, 2740, 307, 291, 458, 293, 9853, 2740, 307, 406, 257, 777, 551, 558, 370], "temperature": 0.0, "avg_logprob": -0.16156965891520184, "compression_ratio": 1.7354497354497354, "no_speech_prob": 3.5559421576181194e-06}, {"id": 854, "seek": 410808, "start": 4108.08, "end": 4110.08, "text": " in", "tokens": [294], "temperature": 0.0, "avg_logprob": -0.22566804885864258, "compression_ratio": 1.7035398230088497, "no_speech_prob": 7.338150567193225e-07}, {"id": 855, "seek": 410808, "start": 4110.6, "end": 4112.6, "text": " Nazi Germany", "tokens": [23592, 7244], "temperature": 0.0, "avg_logprob": -0.22566804885864258, "compression_ratio": 1.7035398230088497, "no_speech_prob": 7.338150567193225e-07}, {"id": 856, "seek": 410808, "start": 4112.68, "end": 4114.0, "text": " IBM", "tokens": [23487], "temperature": 0.0, "avg_logprob": -0.22566804885864258, "compression_ratio": 1.7035398230088497, "no_speech_prob": 7.338150567193225e-07}, {"id": 857, "seek": 410808, "start": 4114.0, "end": 4118.2, "text": " The group known as Hollerith, right Hollerith was the original name of IBM", "tokens": [440, 1594, 2570, 382, 17712, 260, 355, 11, 558, 17712, 260, 355, 390, 264, 3380, 1315, 295, 23487], "temperature": 0.0, "avg_logprob": -0.22566804885864258, "compression_ratio": 1.7035398230088497, "no_speech_prob": 7.338150567193225e-07}, {"id": 858, "seek": 410808, "start": 4118.2, "end": 4124.24, "text": " And it comes from the guy who actually invented the use of punch cards for tracking the US census the first", "tokens": [400, 309, 1487, 490, 264, 2146, 567, 767, 14479, 264, 764, 295, 8135, 5632, 337, 11603, 264, 2546, 23725, 264, 700], "temperature": 0.0, "avg_logprob": -0.22566804885864258, "compression_ratio": 1.7035398230088497, "no_speech_prob": 7.338150567193225e-07}, {"id": 859, "seek": 410808, "start": 4124.5199999999995, "end": 4130.72, "text": " Mass wide-scale use of punch cards for data collection in the world right and that turned into IBM", "tokens": [10482, 4874, 12, 20033, 764, 295, 8135, 5632, 337, 1412, 5765, 294, 264, 1002, 558, 293, 300, 3574, 666, 23487], "temperature": 0.0, "avg_logprob": -0.22566804885864258, "compression_ratio": 1.7035398230088497, "no_speech_prob": 7.338150567193225e-07}, {"id": 860, "seek": 410808, "start": 4130.72, "end": 4134.36, "text": " And so at this point if this unit at least was still called Hollerith, so Hollerith", "tokens": [400, 370, 412, 341, 935, 498, 341, 4985, 412, 1935, 390, 920, 1219, 17712, 260, 355, 11, 370, 17712, 260, 355], "temperature": 0.0, "avg_logprob": -0.22566804885864258, "compression_ratio": 1.7035398230088497, "no_speech_prob": 7.338150567193225e-07}, {"id": 861, "seek": 413436, "start": 4134.36, "end": 4139.36, "text": " sold a punch card system to Nazi Germany", "tokens": [3718, 257, 8135, 2920, 1185, 281, 23592, 7244], "temperature": 0.0, "avg_logprob": -0.27054665054100147, "compression_ratio": 1.4404145077720207, "no_speech_prob": 5.255281848803861e-06}, {"id": 862, "seek": 413436, "start": 4140.48, "end": 4146.599999999999, "text": " And so each punch card would like code you know this is a Jew 8 gypsy 12", "tokens": [400, 370, 1184, 8135, 2920, 576, 411, 3089, 291, 458, 341, 307, 257, 5679, 1649, 15823, 29182, 2272], "temperature": 0.0, "avg_logprob": -0.27054665054100147, "compression_ratio": 1.4404145077720207, "no_speech_prob": 5.255281848803861e-06}, {"id": 863, "seek": 413436, "start": 4147.92, "end": 4153.839999999999, "text": " General execution for death by gas chamber 6 and so here's one of these cards", "tokens": [6996, 15058, 337, 2966, 538, 4211, 13610, 1386, 293, 370, 510, 311, 472, 295, 613, 5632], "temperature": 0.0, "avg_logprob": -0.27054665054100147, "compression_ratio": 1.4404145077720207, "no_speech_prob": 5.255281848803861e-06}, {"id": 864, "seek": 413436, "start": 4154.599999999999, "end": 4159.799999999999, "text": " Describing the right way to kill these various people right and so a Swiss judge ruled", "tokens": [3885, 39541, 264, 558, 636, 281, 1961, 613, 3683, 561, 558, 293, 370, 257, 21965, 6995, 20077], "temperature": 0.0, "avg_logprob": -0.27054665054100147, "compression_ratio": 1.4404145077720207, "no_speech_prob": 5.255281848803861e-06}, {"id": 865, "seek": 415980, "start": 4159.8, "end": 4166.84, "text": " That IBM's technical assistance facilitated the tasks of the Nazis and Commission of their crimes against humanity", "tokens": [663, 23487, 311, 6191, 9683, 10217, 18266, 264, 9608, 295, 264, 29812, 293, 10766, 295, 641, 13916, 1970, 10243], "temperature": 0.0, "avg_logprob": -0.21200163364410402, "compression_ratio": 1.6048387096774193, "no_speech_prob": 6.7479181780072395e-06}, {"id": 866, "seek": 415980, "start": 4167.08, "end": 4171.24, "text": " This led to the death of something like 20 million civilians", "tokens": [639, 4684, 281, 264, 2966, 295, 746, 411, 945, 2459, 26073], "temperature": 0.0, "avg_logprob": -0.21200163364410402, "compression_ratio": 1.6048387096774193, "no_speech_prob": 6.7479181780072395e-06}, {"id": 867, "seek": 415980, "start": 4172.76, "end": 4177.16, "text": " So according to the Jewish Virtual Library where I got these pictures and quotes from", "tokens": [407, 4650, 281, 264, 9246, 23887, 12806, 689, 286, 658, 613, 5242, 293, 19963, 490], "temperature": 0.0, "avg_logprob": -0.21200163364410402, "compression_ratio": 1.6048387096774193, "no_speech_prob": 6.7479181780072395e-06}, {"id": 868, "seek": 415980, "start": 4177.84, "end": 4185.8, "text": " Their view is that the destruction of the Jewish people became even less important because of the invigorating nature of IBM's technical", "tokens": [6710, 1910, 307, 300, 264, 13563, 295, 264, 9246, 561, 3062, 754, 1570, 1021, 570, 295, 264, 1048, 47131, 990, 3687, 295, 23487, 311, 6191], "temperature": 0.0, "avg_logprob": -0.21200163364410402, "compression_ratio": 1.6048387096774193, "no_speech_prob": 6.7479181780072395e-06}, {"id": 869, "seek": 418580, "start": 4185.8, "end": 4190.72, "text": " Achievement only heightened by the fantastical profits to be made right", "tokens": [15847, 11108, 518, 787, 46154, 538, 264, 30665, 804, 17982, 281, 312, 1027, 558], "temperature": 0.0, "avg_logprob": -0.2072876200956457, "compression_ratio": 1.631336405529954, "no_speech_prob": 3.500831326164189e-06}, {"id": 870, "seek": 418580, "start": 4191.360000000001, "end": 4196.320000000001, "text": " So this was a long time ago, and you know hopefully you won't end up working at companies that facilitate", "tokens": [407, 341, 390, 257, 938, 565, 2057, 11, 293, 291, 458, 4696, 291, 1582, 380, 917, 493, 1364, 412, 3431, 300, 20207], "temperature": 0.0, "avg_logprob": -0.2072876200956457, "compression_ratio": 1.631336405529954, "no_speech_prob": 3.500831326164189e-06}, {"id": 871, "seek": 418580, "start": 4197.24, "end": 4198.52, "text": " genocide", "tokens": [31867], "temperature": 0.0, "avg_logprob": -0.2072876200956457, "compression_ratio": 1.631336405529954, "no_speech_prob": 3.500831326164189e-06}, {"id": 872, "seek": 418580, "start": 4198.52, "end": 4200.76, "text": " Right, but perhaps you will", "tokens": [1779, 11, 457, 4317, 291, 486], "temperature": 0.0, "avg_logprob": -0.2072876200956457, "compression_ratio": 1.631336405529954, "no_speech_prob": 3.500831326164189e-06}, {"id": 873, "seek": 418580, "start": 4201.4400000000005, "end": 4206.1, "text": " Right because perhaps you'll go to Facebook who are facilitating genocide right now", "tokens": [1779, 570, 4317, 291, 603, 352, 281, 4384, 567, 366, 47558, 31867, 558, 586], "temperature": 0.0, "avg_logprob": -0.2072876200956457, "compression_ratio": 1.631336405529954, "no_speech_prob": 3.500831326164189e-06}, {"id": 874, "seek": 418580, "start": 4206.64, "end": 4208.64, "text": " Right and I know people at Facebook", "tokens": [1779, 293, 286, 458, 561, 412, 4384], "temperature": 0.0, "avg_logprob": -0.2072876200956457, "compression_ratio": 1.631336405529954, "no_speech_prob": 3.500831326164189e-06}, {"id": 875, "seek": 418580, "start": 4209.28, "end": 4211.28, "text": " Who are doing this?", "tokens": [2102, 366, 884, 341, 30], "temperature": 0.0, "avg_logprob": -0.2072876200956457, "compression_ratio": 1.631336405529954, "no_speech_prob": 3.500831326164189e-06}, {"id": 876, "seek": 421128, "start": 4211.28, "end": 4217.24, "text": " And they had no idea they were doing this right so right now in Facebook the Rohingya", "tokens": [400, 436, 632, 572, 1558, 436, 645, 884, 341, 558, 370, 558, 586, 294, 4384, 264, 3101, 571, 3016], "temperature": 0.0, "avg_logprob": -0.16836090087890626, "compression_ratio": 1.6216216216216217, "no_speech_prob": 3.0415676519623958e-06}, {"id": 877, "seek": 421128, "start": 4217.599999999999, "end": 4221.2, "text": " In the middle of a genocide a Muslim population of Myanmar", "tokens": [682, 264, 2808, 295, 257, 31867, 257, 8178, 4415, 295, 42725], "temperature": 0.0, "avg_logprob": -0.16836090087890626, "compression_ratio": 1.6216216216216217, "no_speech_prob": 3.0415676519623958e-06}, {"id": 878, "seek": 421128, "start": 4223.8, "end": 4227.4, "text": " Babies are being grabbed out of their mother's arms and thrown into fires", "tokens": [15820, 530, 366, 885, 18607, 484, 295, 641, 2895, 311, 5812, 293, 11732, 666, 15044], "temperature": 0.0, "avg_logprob": -0.16836090087890626, "compression_ratio": 1.6216216216216217, "no_speech_prob": 3.0415676519623958e-06}, {"id": 879, "seek": 421128, "start": 4228.24, "end": 4231.0, "text": " People are being killed hundreds of thousands of refugees", "tokens": [3432, 366, 885, 4652, 6779, 295, 5383, 295, 18301], "temperature": 0.0, "avg_logprob": -0.16836090087890626, "compression_ratio": 1.6216216216216217, "no_speech_prob": 3.0415676519623958e-06}, {"id": 880, "seek": 421128, "start": 4232.08, "end": 4239.38, "text": " When interviewed the Myanmar generals doing this say we are so grateful to Facebook", "tokens": [1133, 19770, 264, 42725, 41346, 884, 341, 584, 321, 366, 370, 7941, 281, 4384], "temperature": 0.0, "avg_logprob": -0.16836090087890626, "compression_ratio": 1.6216216216216217, "no_speech_prob": 3.0415676519623958e-06}, {"id": 881, "seek": 423938, "start": 4239.38, "end": 4243.9800000000005, "text": " For letting us know about the Rohingya fake news", "tokens": [1171, 8295, 505, 458, 466, 264, 3101, 571, 3016, 7592, 2583], "temperature": 0.0, "avg_logprob": -0.20156851268949963, "compression_ratio": 1.7064220183486238, "no_speech_prob": 3.611918828028138e-06}, {"id": 882, "seek": 423938, "start": 4244.5, "end": 4253.2, "text": " The words they use the Rohingya fake news that these people are actually not human that they're actually animals right now Facebook did not set out", "tokens": [440, 2283, 436, 764, 264, 3101, 571, 3016, 7592, 2583, 300, 613, 561, 366, 767, 406, 1952, 300, 436, 434, 767, 4882, 558, 586, 4384, 630, 406, 992, 484], "temperature": 0.0, "avg_logprob": -0.20156851268949963, "compression_ratio": 1.7064220183486238, "no_speech_prob": 3.611918828028138e-06}, {"id": 883, "seek": 423938, "start": 4253.9800000000005, "end": 4257.58, "text": " To enable the genocide of the Rohingya people in Myanmar", "tokens": [1407, 9528, 264, 31867, 295, 264, 3101, 571, 3016, 561, 294, 42725], "temperature": 0.0, "avg_logprob": -0.20156851268949963, "compression_ratio": 1.7064220183486238, "no_speech_prob": 3.611918828028138e-06}, {"id": 884, "seek": 423938, "start": 4257.9800000000005, "end": 4265.42, "text": " No instead what happened is they wanted to maximize impressions and clicks right and so it turns out that for the data", "tokens": [883, 2602, 437, 2011, 307, 436, 1415, 281, 19874, 24245, 293, 18521, 558, 293, 370, 309, 4523, 484, 300, 337, 264, 1412], "temperature": 0.0, "avg_logprob": -0.20156851268949963, "compression_ratio": 1.7064220183486238, "no_speech_prob": 3.611918828028138e-06}, {"id": 885, "seek": 426542, "start": 4265.42, "end": 4269.54, "text": " scientists at Facebook's their algorithms kind of learned that if you", "tokens": [7708, 412, 4384, 311, 641, 14642, 733, 295, 3264, 300, 498, 291], "temperature": 0.0, "avg_logprob": -0.16527974745806526, "compression_ratio": 1.7531380753138075, "no_speech_prob": 1.4144629858492408e-06}, {"id": 886, "seek": 426542, "start": 4269.82, "end": 4275.22, "text": " Take the kinds of stuff people are interested in and feed them slightly more extreme versions of that", "tokens": [3664, 264, 3685, 295, 1507, 561, 366, 3102, 294, 293, 3154, 552, 4748, 544, 8084, 9606, 295, 300], "temperature": 0.0, "avg_logprob": -0.16527974745806526, "compression_ratio": 1.7531380753138075, "no_speech_prob": 1.4144629858492408e-06}, {"id": 887, "seek": 426542, "start": 4275.34, "end": 4283.42, "text": " You're actually going to get a lot more impressions and the project managers are saying maximize these impressions and people are clicking and like it creates this", "tokens": [509, 434, 767, 516, 281, 483, 257, 688, 544, 24245, 293, 264, 1716, 14084, 366, 1566, 19874, 613, 24245, 293, 561, 366, 9697, 293, 411, 309, 7829, 341], "temperature": 0.0, "avg_logprob": -0.16527974745806526, "compression_ratio": 1.7531380753138075, "no_speech_prob": 1.4144629858492408e-06}, {"id": 888, "seek": 426542, "start": 4284.3, "end": 4287.06, "text": " this thing right and so", "tokens": [341, 551, 558, 293, 370], "temperature": 0.0, "avg_logprob": -0.16527974745806526, "compression_ratio": 1.7531380753138075, "no_speech_prob": 1.4144629858492408e-06}, {"id": 889, "seek": 426542, "start": 4288.3, "end": 4289.9800000000005, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.16527974745806526, "compression_ratio": 1.7531380753138075, "no_speech_prob": 1.4144629858492408e-06}, {"id": 890, "seek": 426542, "start": 4289.9800000000005, "end": 4293.46, "text": " The potential implications are extraordinary and global", "tokens": [440, 3995, 16602, 366, 10581, 293, 4338], "temperature": 0.0, "avg_logprob": -0.16527974745806526, "compression_ratio": 1.7531380753138075, "no_speech_prob": 1.4144629858492408e-06}, {"id": 891, "seek": 429346, "start": 4293.46, "end": 4298.16, "text": " Right and this is something that like is literally happening. You know this is", "tokens": [1779, 293, 341, 307, 746, 300, 411, 307, 3736, 2737, 13, 509, 458, 341, 307], "temperature": 0.0, "avg_logprob": -0.17924780004164753, "compression_ratio": 1.6465116279069767, "no_speech_prob": 2.902264895965345e-06}, {"id": 892, "seek": 429346, "start": 4298.7, "end": 4302.76, "text": " October 2017 it's happening now okay, could you pass that back there?", "tokens": [7617, 6591, 309, 311, 2737, 586, 1392, 11, 727, 291, 1320, 300, 646, 456, 30], "temperature": 0.0, "avg_logprob": -0.17924780004164753, "compression_ratio": 1.6465116279069767, "no_speech_prob": 2.902264895965345e-06}, {"id": 893, "seek": 429346, "start": 4308.18, "end": 4315.12, "text": " So I just want to clarify what was happening here, so it was the facilitation of like fake news or like inaccurate media", "tokens": [407, 286, 445, 528, 281, 17594, 437, 390, 2737, 510, 11, 370, 309, 390, 264, 10217, 4614, 295, 411, 7592, 2583, 420, 411, 46443, 3021], "temperature": 0.0, "avg_logprob": -0.17924780004164753, "compression_ratio": 1.6465116279069767, "no_speech_prob": 2.902264895965345e-06}, {"id": 894, "seek": 429346, "start": 4315.7, "end": 4321.42, "text": " Yeah, so what happened was let me go into it in more detail, so what happened was in", "tokens": [865, 11, 370, 437, 2011, 390, 718, 385, 352, 666, 309, 294, 544, 2607, 11, 370, 437, 2011, 390, 294], "temperature": 0.0, "avg_logprob": -0.17924780004164753, "compression_ratio": 1.6465116279069767, "no_speech_prob": 2.902264895965345e-06}, {"id": 895, "seek": 432142, "start": 4321.42, "end": 4323.18, "text": " mid", "tokens": [2062], "temperature": 0.0, "avg_logprob": -0.21821563243865966, "compression_ratio": 1.6387665198237886, "no_speech_prob": 1.0845101314771455e-06}, {"id": 896, "seek": 432142, "start": 4323.18, "end": 4325.1, "text": " 2016", "tokens": [6549], "temperature": 0.0, "avg_logprob": -0.21821563243865966, "compression_ratio": 1.6387665198237886, "no_speech_prob": 1.0845101314771455e-06}, {"id": 897, "seek": 432142, "start": 4325.1, "end": 4332.18, "text": " Facebook fired its human editors right so it was humans that decided how to order things on your home page", "tokens": [4384, 11777, 1080, 1952, 31446, 558, 370, 309, 390, 6255, 300, 3047, 577, 281, 1668, 721, 322, 428, 1280, 3028], "temperature": 0.0, "avg_logprob": -0.21821563243865966, "compression_ratio": 1.6387665198237886, "no_speech_prob": 1.0845101314771455e-06}, {"id": 898, "seek": 432142, "start": 4332.42, "end": 4335.78, "text": " those people got fired and replaced with machine learning algorithms and", "tokens": [729, 561, 658, 11777, 293, 10772, 365, 3479, 2539, 14642, 293], "temperature": 0.0, "avg_logprob": -0.21821563243865966, "compression_ratio": 1.6387665198237886, "no_speech_prob": 1.0845101314771455e-06}, {"id": 899, "seek": 432142, "start": 4336.58, "end": 4341.38, "text": " So the machine learning algorithms written by data scientists like you", "tokens": [407, 264, 3479, 2539, 14642, 3720, 538, 1412, 7708, 411, 291], "temperature": 0.0, "avg_logprob": -0.21821563243865966, "compression_ratio": 1.6387665198237886, "no_speech_prob": 1.0845101314771455e-06}, {"id": 900, "seek": 432142, "start": 4343.42, "end": 4345.9800000000005, "text": " You know they had nice clear metrics", "tokens": [509, 458, 436, 632, 1481, 1850, 16367], "temperature": 0.0, "avg_logprob": -0.21821563243865966, "compression_ratio": 1.6387665198237886, "no_speech_prob": 1.0845101314771455e-06}, {"id": 901, "seek": 432142, "start": 4345.9800000000005, "end": 4349.62, "text": " And they were trying to maximize their predictive accuracy and be like okay", "tokens": [400, 436, 645, 1382, 281, 19874, 641, 35521, 14170, 293, 312, 411, 1392], "temperature": 0.0, "avg_logprob": -0.21821563243865966, "compression_ratio": 1.6387665198237886, "no_speech_prob": 1.0845101314771455e-06}, {"id": 902, "seek": 434962, "start": 4349.62, "end": 4353.62, "text": " We think if we put this thing higher up than this thing we'll get more clicks", "tokens": [492, 519, 498, 321, 829, 341, 551, 2946, 493, 813, 341, 551, 321, 603, 483, 544, 18521], "temperature": 0.0, "avg_logprob": -0.1606479835510254, "compression_ratio": 1.6926070038910506, "no_speech_prob": 9.570781003276352e-07}, {"id": 903, "seek": 434962, "start": 4353.98, "end": 4360.34, "text": " Okay, and so it turned out that these algorithms for putting things on the Facebook newsfeed", "tokens": [1033, 11, 293, 370, 309, 3574, 484, 300, 613, 14642, 337, 3372, 721, 322, 264, 4384, 2583, 37036], "temperature": 0.0, "avg_logprob": -0.1606479835510254, "compression_ratio": 1.6926070038910506, "no_speech_prob": 9.570781003276352e-07}, {"id": 904, "seek": 434962, "start": 4361.0199999999995, "end": 4366.5599999999995, "text": " had a tendency to say like oh human nature is that we tend to click on things which like", "tokens": [632, 257, 18187, 281, 584, 411, 1954, 1952, 3687, 307, 300, 321, 3928, 281, 2052, 322, 721, 597, 411], "temperature": 0.0, "avg_logprob": -0.1606479835510254, "compression_ratio": 1.6926070038910506, "no_speech_prob": 9.570781003276352e-07}, {"id": 905, "seek": 434962, "start": 4367.82, "end": 4373.54, "text": " Stimulate our views and therefore like more extreme versions of things we already see okay, so", "tokens": [745, 332, 5256, 527, 6809, 293, 4412, 411, 544, 8084, 9606, 295, 721, 321, 1217, 536, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.1606479835510254, "compression_ratio": 1.6926070038910506, "no_speech_prob": 9.570781003276352e-07}, {"id": 906, "seek": 434962, "start": 4374.3, "end": 4378.7, "text": " So this is great for the kind of Facebook revenue model of maximizing engagement", "tokens": [407, 341, 307, 869, 337, 264, 733, 295, 4384, 9324, 2316, 295, 5138, 3319, 8742], "temperature": 0.0, "avg_logprob": -0.1606479835510254, "compression_ratio": 1.6926070038910506, "no_speech_prob": 9.570781003276352e-07}, {"id": 907, "seek": 437870, "start": 4378.7, "end": 4380.86, "text": " It looked good on all of their KPIs", "tokens": [467, 2956, 665, 322, 439, 295, 641, 41371, 6802], "temperature": 0.0, "avg_logprob": -0.26329280553239115, "compression_ratio": 1.5676855895196506, "no_speech_prob": 5.014697308070026e-06}, {"id": 908, "seek": 437870, "start": 4381.62, "end": 4383.62, "text": " And so at the time", "tokens": [400, 370, 412, 264, 565], "temperature": 0.0, "avg_logprob": -0.26329280553239115, "compression_ratio": 1.5676855895196506, "no_speech_prob": 5.014697308070026e-06}, {"id": 909, "seek": 437870, "start": 4384.26, "end": 4386.38, "text": " You know there was some negative press", "tokens": [509, 458, 456, 390, 512, 3671, 1886], "temperature": 0.0, "avg_logprob": -0.26329280553239115, "compression_ratio": 1.5676855895196506, "no_speech_prob": 5.014697308070026e-06}, {"id": 910, "seek": 437870, "start": 4387.0199999999995, "end": 4392.3, "text": " About like you know I'm not sure that the stuff that Facebook's now putting on their trending", "tokens": [7769, 411, 291, 458, 286, 478, 406, 988, 300, 264, 1507, 300, 4384, 311, 586, 3372, 322, 641, 28692], "temperature": 0.0, "avg_logprob": -0.26329280553239115, "compression_ratio": 1.5676855895196506, "no_speech_prob": 5.014697308070026e-06}, {"id": 911, "seek": 437870, "start": 4392.86, "end": 4399.5, "text": " Section is actually that accurate but from the point of view of the metrics that people are optimizing at Facebook it looked terrific", "tokens": [21804, 307, 767, 300, 8559, 457, 490, 264, 935, 295, 1910, 295, 264, 16367, 300, 561, 366, 40425, 412, 4384, 309, 2956, 20899], "temperature": 0.0, "avg_logprob": -0.26329280553239115, "compression_ratio": 1.5676855895196506, "no_speech_prob": 5.014697308070026e-06}, {"id": 912, "seek": 437870, "start": 4400.139999999999, "end": 4402.139999999999, "text": " right and", "tokens": [558, 293], "temperature": 0.0, "avg_logprob": -0.26329280553239115, "compression_ratio": 1.5676855895196506, "no_speech_prob": 5.014697308070026e-06}, {"id": 913, "seek": 437870, "start": 4402.38, "end": 4403.38, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.26329280553239115, "compression_ratio": 1.5676855895196506, "no_speech_prob": 5.014697308070026e-06}, {"id": 914, "seek": 437870, "start": 4403.38, "end": 4406.0599999999995, "text": " way back to October 2016", "tokens": [636, 646, 281, 7617, 6549], "temperature": 0.0, "avg_logprob": -0.26329280553239115, "compression_ratio": 1.5676855895196506, "no_speech_prob": 5.014697308070026e-06}, {"id": 915, "seek": 440606, "start": 4406.06, "end": 4412.860000000001, "text": " People started noticing some serious problems for example it is illegal to target housing", "tokens": [3432, 1409, 21814, 512, 3156, 2740, 337, 1365, 309, 307, 11905, 281, 3779, 6849], "temperature": 0.0, "avg_logprob": -0.24334528711107042, "compression_ratio": 1.543859649122807, "no_speech_prob": 2.443981429678388e-06}, {"id": 916, "seek": 440606, "start": 4413.580000000001, "end": 4418.780000000001, "text": " to people of certain races in America that is illegal and yet a", "tokens": [281, 561, 295, 1629, 15484, 294, 3374, 300, 307, 11905, 293, 1939, 257], "temperature": 0.0, "avg_logprob": -0.24334528711107042, "compression_ratio": 1.543859649122807, "no_speech_prob": 2.443981429678388e-06}, {"id": 917, "seek": 440606, "start": 4419.1, "end": 4422.5, "text": " News organization discovered that Facebook was doing exactly that", "tokens": [7987, 4475, 6941, 300, 4384, 390, 884, 2293, 300], "temperature": 0.0, "avg_logprob": -0.24334528711107042, "compression_ratio": 1.543859649122807, "no_speech_prob": 2.443981429678388e-06}, {"id": 918, "seek": 440606, "start": 4423.02, "end": 4431.22, "text": " Right in October 2016 again not because somebody in that data science team said like let's make sure black people can't live in nice", "tokens": [1779, 294, 7617, 6549, 797, 406, 570, 2618, 294, 300, 1412, 3497, 1469, 848, 411, 718, 311, 652, 988, 2211, 561, 393, 380, 1621, 294, 1481], "temperature": 0.0, "avg_logprob": -0.24334528711107042, "compression_ratio": 1.543859649122807, "no_speech_prob": 2.443981429678388e-06}, {"id": 919, "seek": 443122, "start": 4431.22, "end": 4436.42, "text": " Neighborhoods right but instead you know they found that their", "tokens": [47729, 3809, 82, 558, 457, 2602, 291, 458, 436, 1352, 300, 641], "temperature": 0.0, "avg_logprob": -0.20216804597435928, "compression_ratio": 1.672340425531915, "no_speech_prob": 1.9947158307331847e-06}, {"id": 920, "seek": 443122, "start": 4437.34, "end": 4441.8, "text": " automatic clustering and segmentation algorithm found there was a cluster of people who", "tokens": [12509, 596, 48673, 293, 9469, 399, 9284, 1352, 456, 390, 257, 13630, 295, 561, 567], "temperature": 0.0, "avg_logprob": -0.20216804597435928, "compression_ratio": 1.672340425531915, "no_speech_prob": 1.9947158307331847e-06}, {"id": 921, "seek": 443122, "start": 4442.9800000000005, "end": 4449.780000000001, "text": " Didn't like African Americans and that if you targeted them with these kinds of ads then they would be more likely to", "tokens": [11151, 380, 411, 7312, 6280, 293, 300, 498, 291, 15045, 552, 365, 613, 3685, 295, 10342, 550, 436, 576, 312, 544, 3700, 281], "temperature": 0.0, "avg_logprob": -0.20216804597435928, "compression_ratio": 1.672340425531915, "no_speech_prob": 1.9947158307331847e-06}, {"id": 922, "seek": 443122, "start": 4450.3, "end": 4457.14, "text": " Select this kind of housing or whatever right but the interesting thing is that even after being told about this three times", "tokens": [13638, 341, 733, 295, 6849, 420, 2035, 558, 457, 264, 1880, 551, 307, 300, 754, 934, 885, 1907, 466, 341, 1045, 1413], "temperature": 0.0, "avg_logprob": -0.20216804597435928, "compression_ratio": 1.672340425531915, "no_speech_prob": 1.9947158307331847e-06}, {"id": 923, "seek": 445714, "start": 4457.14, "end": 4462.660000000001, "text": " Facebook still hasn't fixed it right and that is to say these are not just technical issues", "tokens": [4384, 920, 6132, 380, 6806, 309, 558, 293, 300, 307, 281, 584, 613, 366, 406, 445, 6191, 2663], "temperature": 0.0, "avg_logprob": -0.1375916302204132, "compression_ratio": 1.7211155378486056, "no_speech_prob": 2.0261163626855705e-06}, {"id": 924, "seek": 445714, "start": 4462.660000000001, "end": 4470.06, "text": " They're also economic issues right when you start saying like the thing that you get paid for that is ads you have to change", "tokens": [814, 434, 611, 4836, 2663, 558, 562, 291, 722, 1566, 411, 264, 551, 300, 291, 483, 4835, 337, 300, 307, 10342, 291, 362, 281, 1319], "temperature": 0.0, "avg_logprob": -0.1375916302204132, "compression_ratio": 1.7211155378486056, "no_speech_prob": 2.0261163626855705e-06}, {"id": 925, "seek": 445714, "start": 4470.46, "end": 4474.34, "text": " The way that you structure those so that you know you either", "tokens": [440, 636, 300, 291, 3877, 729, 370, 300, 291, 458, 291, 2139], "temperature": 0.0, "avg_logprob": -0.1375916302204132, "compression_ratio": 1.7211155378486056, "no_speech_prob": 2.0261163626855705e-06}, {"id": 926, "seek": 445714, "start": 4474.62, "end": 4482.42, "text": " Use more people that cost money or you like a less aggressive on your algorithms to target people you know based on like", "tokens": [8278, 544, 561, 300, 2063, 1460, 420, 291, 411, 257, 1570, 10762, 322, 428, 14642, 281, 3779, 561, 291, 458, 2361, 322, 411], "temperature": 0.0, "avg_logprob": -0.1375916302204132, "compression_ratio": 1.7211155378486056, "no_speech_prob": 2.0261163626855705e-06}, {"id": 927, "seek": 445714, "start": 4483.42, "end": 4485.42, "text": " Minority group status or whatever", "tokens": [36117, 507, 1594, 6558, 420, 2035], "temperature": 0.0, "avg_logprob": -0.1375916302204132, "compression_ratio": 1.7211155378486056, "no_speech_prob": 2.0261163626855705e-06}, {"id": 928, "seek": 448542, "start": 4485.42, "end": 4490.78, "text": " You know that can impact revenues and so the reason I mentioned this is you will", "tokens": [509, 458, 300, 393, 2712, 27299, 293, 370, 264, 1778, 286, 2835, 341, 307, 291, 486], "temperature": 0.0, "avg_logprob": -0.237262499214399, "compression_ratio": 1.715953307392996, "no_speech_prob": 6.681477771053324e-07}, {"id": 929, "seek": 448542, "start": 4491.46, "end": 4496.54, "text": " likely at some point in your career find yourself in a conversation where you're thinking", "tokens": [3700, 412, 512, 935, 294, 428, 3988, 915, 1803, 294, 257, 3761, 689, 291, 434, 1953], "temperature": 0.0, "avg_logprob": -0.237262499214399, "compression_ratio": 1.715953307392996, "no_speech_prob": 6.681477771053324e-07}, {"id": 930, "seek": 448542, "start": 4497.14, "end": 4500.32, "text": " I'm not confident that this is like morally okay", "tokens": [286, 478, 406, 6679, 300, 341, 307, 411, 38622, 1392], "temperature": 0.0, "avg_logprob": -0.237262499214399, "compression_ratio": 1.715953307392996, "no_speech_prob": 6.681477771053324e-07}, {"id": 931, "seek": 448542, "start": 4500.58, "end": 4504.5, "text": " The person you're talking to is thinking in their head. This is going to make us a lot of money", "tokens": [440, 954, 291, 434, 1417, 281, 307, 1953, 294, 641, 1378, 13, 639, 307, 516, 281, 652, 505, 257, 688, 295, 1460], "temperature": 0.0, "avg_logprob": -0.237262499214399, "compression_ratio": 1.715953307392996, "no_speech_prob": 6.681477771053324e-07}, {"id": 932, "seek": 448542, "start": 4505.02, "end": 4507.82, "text": " That and you just you don't quite ever", "tokens": [663, 293, 291, 445, 291, 500, 380, 1596, 1562], "temperature": 0.0, "avg_logprob": -0.237262499214399, "compression_ratio": 1.715953307392996, "no_speech_prob": 6.681477771053324e-07}, {"id": 933, "seek": 448542, "start": 4509.06, "end": 4512.34, "text": " Manage to have a successful conversation because you're talking about different things", "tokens": [2458, 609, 281, 362, 257, 4406, 3761, 570, 291, 434, 1417, 466, 819, 721], "temperature": 0.0, "avg_logprob": -0.237262499214399, "compression_ratio": 1.715953307392996, "no_speech_prob": 6.681477771053324e-07}, {"id": 934, "seek": 451234, "start": 4512.34, "end": 4519.18, "text": " You know and so when you're talking to somebody who may be more experienced and more senior than you and they may sound like they know", "tokens": [509, 458, 293, 370, 562, 291, 434, 1417, 281, 2618, 567, 815, 312, 544, 6751, 293, 544, 7965, 813, 291, 293, 436, 815, 1626, 411, 436, 458], "temperature": 0.0, "avg_logprob": -0.15976150013576043, "compression_ratio": 1.8264462809917354, "no_speech_prob": 2.026116590059246e-06}, {"id": 935, "seek": 451234, "start": 4519.18, "end": 4524.02, "text": " What they're talking about right just realize that their incentives are not necessarily", "tokens": [708, 436, 434, 1417, 466, 558, 445, 4325, 300, 641, 23374, 366, 406, 4725], "temperature": 0.0, "avg_logprob": -0.15976150013576043, "compression_ratio": 1.8264462809917354, "no_speech_prob": 2.026116590059246e-06}, {"id": 936, "seek": 451234, "start": 4524.78, "end": 4526.78, "text": " Going to be focused on like", "tokens": [10963, 281, 312, 5178, 322, 411], "temperature": 0.0, "avg_logprob": -0.15976150013576043, "compression_ratio": 1.8264462809917354, "no_speech_prob": 2.026116590059246e-06}, {"id": 937, "seek": 451234, "start": 4527.14, "end": 4528.9400000000005, "text": " How do I be a good person?", "tokens": [1012, 360, 286, 312, 257, 665, 954, 30], "temperature": 0.0, "avg_logprob": -0.15976150013576043, "compression_ratio": 1.8264462809917354, "no_speech_prob": 2.026116590059246e-06}, {"id": 938, "seek": 451234, "start": 4528.9400000000005, "end": 4531.34, "text": " You know like they're not thinking how do I be a bad person?", "tokens": [509, 458, 411, 436, 434, 406, 1953, 577, 360, 286, 312, 257, 1578, 954, 30], "temperature": 0.0, "avg_logprob": -0.15976150013576043, "compression_ratio": 1.8264462809917354, "no_speech_prob": 2.026116590059246e-06}, {"id": 939, "seek": 451234, "start": 4531.58, "end": 4537.34, "text": " But you know the more time you spend an industry in my experience the more desensitized you kind of get", "tokens": [583, 291, 458, 264, 544, 565, 291, 3496, 364, 3518, 294, 452, 1752, 264, 544, 730, 694, 270, 1602, 291, 733, 295, 483], "temperature": 0.0, "avg_logprob": -0.15976150013576043, "compression_ratio": 1.8264462809917354, "no_speech_prob": 2.026116590059246e-06}, {"id": 940, "seek": 453734, "start": 4537.34, "end": 4543.74, "text": " To this stuff of like okay, maybe getting promotions and making money isn't the most important thing", "tokens": [1407, 341, 1507, 295, 411, 1392, 11, 1310, 1242, 42127, 293, 1455, 1460, 1943, 380, 264, 881, 1021, 551], "temperature": 0.0, "avg_logprob": -0.18454092675512965, "compression_ratio": 1.584313725490196, "no_speech_prob": 1.933349039973109e-06}, {"id": 941, "seek": 453734, "start": 4544.26, "end": 4546.26, "text": " All right, so for example", "tokens": [1057, 558, 11, 370, 337, 1365], "temperature": 0.0, "avg_logprob": -0.18454092675512965, "compression_ratio": 1.584313725490196, "no_speech_prob": 1.933349039973109e-06}, {"id": 942, "seek": 453734, "start": 4546.58, "end": 4552.58, "text": " I've got a lot of friends who are very good at computer vision and some of them have gone on to create startups", "tokens": [286, 600, 658, 257, 688, 295, 1855, 567, 366, 588, 665, 412, 3820, 5201, 293, 512, 295, 552, 362, 2780, 322, 281, 1884, 28041], "temperature": 0.0, "avg_logprob": -0.18454092675512965, "compression_ratio": 1.584313725490196, "no_speech_prob": 1.933349039973109e-06}, {"id": 943, "seek": 453734, "start": 4553.38, "end": 4559.06, "text": " That seem like they're almost handmade to help authoritarian governments surveil their", "tokens": [663, 1643, 411, 436, 434, 1920, 39446, 281, 854, 37883, 11280, 11463, 388, 641], "temperature": 0.0, "avg_logprob": -0.18454092675512965, "compression_ratio": 1.584313725490196, "no_speech_prob": 1.933349039973109e-06}, {"id": 944, "seek": 453734, "start": 4560.06, "end": 4564.54, "text": " You know their citizens and when I ask my friends like have you thought about?", "tokens": [509, 458, 641, 7180, 293, 562, 286, 1029, 452, 1855, 411, 362, 291, 1194, 466, 30], "temperature": 0.0, "avg_logprob": -0.18454092675512965, "compression_ratio": 1.584313725490196, "no_speech_prob": 1.933349039973109e-06}, {"id": 945, "seek": 456454, "start": 4564.54, "end": 4567.54, "text": " How this could be used in that way?", "tokens": [1012, 341, 727, 312, 1143, 294, 300, 636, 30], "temperature": 0.0, "avg_logprob": -0.15061502229599727, "compression_ratio": 1.6519607843137254, "no_speech_prob": 8.059411698013719e-07}, {"id": 946, "seek": 456454, "start": 4568.58, "end": 4572.54, "text": " You know they're generally kind of offended that I ask you know", "tokens": [509, 458, 436, 434, 5101, 733, 295, 26776, 300, 286, 1029, 291, 458], "temperature": 0.0, "avg_logprob": -0.15061502229599727, "compression_ratio": 1.6519607843137254, "no_speech_prob": 8.059411698013719e-07}, {"id": 947, "seek": 456454, "start": 4573.06, "end": 4575.06, "text": " But I'm asking you", "tokens": [583, 286, 478, 3365, 291], "temperature": 0.0, "avg_logprob": -0.15061502229599727, "compression_ratio": 1.6519607843137254, "no_speech_prob": 8.059411698013719e-07}, {"id": 948, "seek": 456454, "start": 4575.38, "end": 4579.98, "text": " To think about this like you know wherever you end up working if you end up creating a startup like", "tokens": [1407, 519, 466, 341, 411, 291, 458, 8660, 291, 917, 493, 1364, 498, 291, 917, 493, 4084, 257, 18578, 411], "temperature": 0.0, "avg_logprob": -0.15061502229599727, "compression_ratio": 1.6519607843137254, "no_speech_prob": 8.059411698013719e-07}, {"id": 949, "seek": 456454, "start": 4582.3, "end": 4589.86, "text": " Tools can be used for good or for evil right and so I'm not saying like don't create excellent object act tracking and", "tokens": [30302, 393, 312, 1143, 337, 665, 420, 337, 6724, 558, 293, 370, 286, 478, 406, 1566, 411, 500, 380, 1884, 7103, 2657, 605, 11603, 293], "temperature": 0.0, "avg_logprob": -0.15061502229599727, "compression_ratio": 1.6519607843137254, "no_speech_prob": 8.059411698013719e-07}, {"id": 950, "seek": 458986, "start": 4589.86, "end": 4597.58, "text": " Detection tools from computer vision because yeah, you could go on and use that to create like a much better", "tokens": [4237, 10183, 3873, 490, 3820, 5201, 570, 1338, 11, 291, 727, 352, 322, 293, 764, 300, 281, 1884, 411, 257, 709, 1101], "temperature": 0.0, "avg_logprob": -0.2084633297390408, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.3287666433825507e-06}, {"id": 951, "seek": 458986, "start": 4599.179999999999, "end": 4607.42, "text": " Surgical intervention robot toolkit right I'm just saying like be aware of it think about it talk about it. You know", "tokens": [318, 46736, 13176, 7881, 40167, 558, 286, 478, 445, 1566, 411, 312, 3650, 295, 309, 519, 466, 309, 751, 466, 309, 13, 509, 458], "temperature": 0.0, "avg_logprob": -0.2084633297390408, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.3287666433825507e-06}, {"id": 952, "seek": 458986, "start": 4609.86, "end": 4611.86, "text": " So here's one I find like", "tokens": [407, 510, 311, 472, 286, 915, 411], "temperature": 0.0, "avg_logprob": -0.2084633297390408, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.3287666433825507e-06}, {"id": 953, "seek": 458986, "start": 4612.219999999999, "end": 4617.66, "text": " Fascinating and there's this really cool thing actually that made up comm did this is from a made up comm talk. That's online", "tokens": [49098, 8205, 293, 456, 311, 341, 534, 1627, 551, 767, 300, 1027, 493, 800, 630, 341, 307, 490, 257, 1027, 493, 800, 751, 13, 663, 311, 2950], "temperature": 0.0, "avg_logprob": -0.2084633297390408, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.3287666433825507e-06}, {"id": 954, "seek": 461766, "start": 4617.66, "end": 4623.18, "text": " Um they they think about this they actually thought about this they actually thought you know what if", "tokens": [3301, 436, 436, 519, 466, 341, 436, 767, 1194, 466, 341, 436, 767, 1194, 291, 458, 437, 498], "temperature": 0.0, "avg_logprob": -0.2095196667839499, "compression_ratio": 1.815, "no_speech_prob": 4.092873041372513e-06}, {"id": 955, "seek": 461766, "start": 4623.54, "end": 4627.22, "text": " We built a collaborative filtering system like we learned about in class", "tokens": [492, 3094, 257, 16555, 30822, 1185, 411, 321, 3264, 466, 294, 1508], "temperature": 0.0, "avg_logprob": -0.2095196667839499, "compression_ratio": 1.815, "no_speech_prob": 4.092873041372513e-06}, {"id": 956, "seek": 461766, "start": 4628.86, "end": 4635.98, "text": " To help people decide what meetup to go to it might notice that on the whole in San Francisco a", "tokens": [1407, 854, 561, 4536, 437, 1677, 1010, 281, 352, 281, 309, 1062, 3449, 300, 322, 264, 1379, 294, 5271, 12279, 257], "temperature": 0.0, "avg_logprob": -0.2095196667839499, "compression_ratio": 1.815, "no_speech_prob": 4.092873041372513e-06}, {"id": 957, "seek": 461766, "start": 4636.5, "end": 4640.46, "text": " few more men than women tend to go to techie meetups and", "tokens": [1326, 544, 1706, 813, 2266, 3928, 281, 352, 281, 7553, 414, 1677, 7528, 293], "temperature": 0.0, "avg_logprob": -0.2095196667839499, "compression_ratio": 1.815, "no_speech_prob": 4.092873041372513e-06}, {"id": 958, "seek": 461766, "start": 4641.26, "end": 4643.9, "text": " So it might then start to decide to", "tokens": [407, 309, 1062, 550, 722, 281, 4536, 281], "temperature": 0.0, "avg_logprob": -0.2095196667839499, "compression_ratio": 1.815, "no_speech_prob": 4.092873041372513e-06}, {"id": 959, "seek": 464390, "start": 4643.9, "end": 4651.5, "text": " recommend techie meetups to more men than women as a result of which more men will go to techie meetups as", "tokens": [2748, 7553, 414, 1677, 7528, 281, 544, 1706, 813, 2266, 382, 257, 1874, 295, 597, 544, 1706, 486, 352, 281, 7553, 414, 1677, 7528, 382], "temperature": 0.0, "avg_logprob": -0.13412452211567, "compression_ratio": 1.9897435897435898, "no_speech_prob": 2.1233624920569127e-06}, {"id": 960, "seek": 464390, "start": 4652.259999999999, "end": 4656.179999999999, "text": " A result of which when women go to techie meetups. They'll be like oh, this is all men", "tokens": [316, 1874, 295, 597, 562, 2266, 352, 281, 7553, 414, 1677, 7528, 13, 814, 603, 312, 411, 1954, 11, 341, 307, 439, 1706], "temperature": 0.0, "avg_logprob": -0.13412452211567, "compression_ratio": 1.9897435897435898, "no_speech_prob": 2.1233624920569127e-06}, {"id": 961, "seek": 464390, "start": 4656.179999999999, "end": 4661.78, "text": " I don't really want to go to techie meetups as a result of which the algorithm will get new data saying that men like", "tokens": [286, 500, 380, 534, 528, 281, 352, 281, 7553, 414, 1677, 7528, 382, 257, 1874, 295, 597, 264, 9284, 486, 483, 777, 1412, 1566, 300, 1706, 411], "temperature": 0.0, "avg_logprob": -0.13412452211567, "compression_ratio": 1.9897435897435898, "no_speech_prob": 2.1233624920569127e-06}, {"id": 962, "seek": 464390, "start": 4661.94, "end": 4667.78, "text": " Techie meetups better right and so it continues right and so like a little a", "tokens": [13795, 414, 1677, 7528, 1101, 558, 293, 370, 309, 6515, 558, 293, 370, 411, 257, 707, 257], "temperature": 0.0, "avg_logprob": -0.13412452211567, "compression_ratio": 1.9897435897435898, "no_speech_prob": 2.1233624920569127e-06}, {"id": 963, "seek": 466778, "start": 4667.78, "end": 4673.44, "text": " Little bit of kind of that initial push from the algorithm can create this runaway", "tokens": [8022, 857, 295, 733, 295, 300, 5883, 2944, 490, 264, 9284, 393, 1884, 341, 1190, 10318], "temperature": 0.0, "avg_logprob": -0.1656437234564142, "compression_ratio": 1.6844444444444444, "no_speech_prob": 1.4367432186190854e-06}, {"id": 964, "seek": 466778, "start": 4674.099999999999, "end": 4681.98, "text": " Feedback loop and you end up with like almost all my old techie meetups for instance right and so this kind of", "tokens": [33720, 3207, 6367, 293, 291, 917, 493, 365, 411, 1920, 439, 452, 1331, 7553, 414, 1677, 7528, 337, 5197, 558, 293, 370, 341, 733, 295], "temperature": 0.0, "avg_logprob": -0.1656437234564142, "compression_ratio": 1.6844444444444444, "no_speech_prob": 1.4367432186190854e-06}, {"id": 965, "seek": 466778, "start": 4682.78, "end": 4690.219999999999, "text": " Feedback loop is a kind of subtle issue that you really want to think about when you're thinking about like what is the behavior that?", "tokens": [33720, 3207, 6367, 307, 257, 733, 295, 13743, 2734, 300, 291, 534, 528, 281, 519, 466, 562, 291, 434, 1953, 466, 411, 437, 307, 264, 5223, 300, 30], "temperature": 0.0, "avg_logprob": -0.1656437234564142, "compression_ratio": 1.6844444444444444, "no_speech_prob": 1.4367432186190854e-06}, {"id": 966, "seek": 466778, "start": 4690.219999999999, "end": 4692.099999999999, "text": " I'm changing", "tokens": [286, 478, 4473], "temperature": 0.0, "avg_logprob": -0.1656437234564142, "compression_ratio": 1.6844444444444444, "no_speech_prob": 1.4367432186190854e-06}, {"id": 967, "seek": 466778, "start": 4692.099999999999, "end": 4694.099999999999, "text": " With this algorithm that I'm building", "tokens": [2022, 341, 9284, 300, 286, 478, 2390], "temperature": 0.0, "avg_logprob": -0.1656437234564142, "compression_ratio": 1.6844444444444444, "no_speech_prob": 1.4367432186190854e-06}, {"id": 968, "seek": 469410, "start": 4694.1, "end": 4696.1, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.2822254498799642, "compression_ratio": 1.4294117647058824, "no_speech_prob": 7.338163072745374e-07}, {"id": 969, "seek": 469410, "start": 4698.42, "end": 4703.9800000000005, "text": " Another example which is kind of terrifying is in this paper", "tokens": [3996, 1365, 597, 307, 733, 295, 18106, 307, 294, 341, 3035], "temperature": 0.0, "avg_logprob": -0.2822254498799642, "compression_ratio": 1.4294117647058824, "no_speech_prob": 7.338163072745374e-07}, {"id": 970, "seek": 469410, "start": 4704.9400000000005, "end": 4706.660000000001, "text": " where the", "tokens": [689, 264], "temperature": 0.0, "avg_logprob": -0.2822254498799642, "compression_ratio": 1.4294117647058824, "no_speech_prob": 7.338163072745374e-07}, {"id": 971, "seek": 469410, "start": 4706.660000000001, "end": 4714.9400000000005, "text": " Authors describe how a lot of departments in the US are now using predictive policing algorithms right so", "tokens": [40231, 830, 6786, 577, 257, 688, 295, 15326, 294, 264, 2546, 366, 586, 1228, 35521, 28799, 14642, 558, 370], "temperature": 0.0, "avg_logprob": -0.2822254498799642, "compression_ratio": 1.4294117647058824, "no_speech_prob": 7.338163072745374e-07}, {"id": 972, "seek": 469410, "start": 4715.780000000001, "end": 4718.9800000000005, "text": " Where can we go to find somebody who's about to commit a crime?", "tokens": [2305, 393, 321, 352, 281, 915, 2618, 567, 311, 466, 281, 5599, 257, 7206, 30], "temperature": 0.0, "avg_logprob": -0.2822254498799642, "compression_ratio": 1.4294117647058824, "no_speech_prob": 7.338163072745374e-07}, {"id": 973, "seek": 471898, "start": 4718.98, "end": 4723.82, "text": " And so you know that the algorithm", "tokens": [400, 370, 291, 458, 300, 264, 9284], "temperature": 0.0, "avg_logprob": -0.22496854557710536, "compression_ratio": 1.5522388059701493, "no_speech_prob": 3.4663131032175443e-07}, {"id": 974, "seek": 471898, "start": 4724.98, "end": 4731.0199999999995, "text": " Simply feeds back to you basically the data that you've given it right so if your police department", "tokens": [19596, 23712, 646, 281, 291, 1936, 264, 1412, 300, 291, 600, 2212, 309, 558, 370, 498, 428, 3804, 5882], "temperature": 0.0, "avg_logprob": -0.22496854557710536, "compression_ratio": 1.5522388059701493, "no_speech_prob": 3.4663131032175443e-07}, {"id": 975, "seek": 471898, "start": 4731.7, "end": 4739.0599999999995, "text": " Has engaged in racial profiling at all in the past then it might suggest slightly more often", "tokens": [8646, 8237, 294, 12131, 1740, 4883, 412, 439, 294, 264, 1791, 550, 309, 1062, 3402, 4748, 544, 2049], "temperature": 0.0, "avg_logprob": -0.22496854557710536, "compression_ratio": 1.5522388059701493, "no_speech_prob": 3.4663131032175443e-07}, {"id": 976, "seek": 471898, "start": 4739.459999999999, "end": 4742.62, "text": " Maybe you should go to the black neighborhoods to check for people committing crimes", "tokens": [2704, 291, 820, 352, 281, 264, 2211, 20052, 281, 1520, 337, 561, 26659, 13916], "temperature": 0.0, "avg_logprob": -0.22496854557710536, "compression_ratio": 1.5522388059701493, "no_speech_prob": 3.4663131032175443e-07}, {"id": 977, "seek": 474262, "start": 4742.62, "end": 4749.26, "text": " All right as a result of which more of your police officers go to the black neighborhoods as a result of which they arrest more black", "tokens": [1057, 558, 382, 257, 1874, 295, 597, 544, 295, 428, 3804, 9199, 352, 281, 264, 2211, 20052, 382, 257, 1874, 295, 597, 436, 7823, 544, 2211], "temperature": 0.0, "avg_logprob": -0.12458975918321724, "compression_ratio": 2.1292134831460676, "no_speech_prob": 9.87455223366851e-07}, {"id": 978, "seek": 474262, "start": 4749.26, "end": 4755.86, "text": " People as a result of which the data says that the black neighborhoods are less safe as a result of which the algorithm says", "tokens": [3432, 382, 257, 1874, 295, 597, 264, 1412, 1619, 300, 264, 2211, 20052, 366, 1570, 3273, 382, 257, 1874, 295, 597, 264, 9284, 1619], "temperature": 0.0, "avg_logprob": -0.12458975918321724, "compression_ratio": 2.1292134831460676, "no_speech_prob": 9.87455223366851e-07}, {"id": 979, "seek": 474262, "start": 4755.86, "end": 4762.46, "text": " The policeman maybe you should go to the black neighborhoods more often and so forth right and this is not like", "tokens": [440, 42658, 1310, 291, 820, 352, 281, 264, 2211, 20052, 544, 2049, 293, 370, 5220, 558, 293, 341, 307, 406, 411], "temperature": 0.0, "avg_logprob": -0.12458975918321724, "compression_ratio": 2.1292134831460676, "no_speech_prob": 9.87455223366851e-07}, {"id": 980, "seek": 474262, "start": 4764.46, "end": 4766.46, "text": " You know", "tokens": [509, 458], "temperature": 0.0, "avg_logprob": -0.12458975918321724, "compression_ratio": 2.1292134831460676, "no_speech_prob": 9.87455223366851e-07}, {"id": 981, "seek": 476646, "start": 4766.46, "end": 4773.38, "text": " Vague possibilities of something that might happen in the future. This is like documented work from top academics", "tokens": [691, 4918, 12178, 295, 746, 300, 1062, 1051, 294, 264, 2027, 13, 639, 307, 411, 23007, 589, 490, 1192, 25695], "temperature": 0.0, "avg_logprob": -0.1364658691070892, "compression_ratio": 1.6535433070866141, "no_speech_prob": 1.7061657899830607e-06}, {"id": 982, "seek": 476646, "start": 4773.38, "end": 4778.74, "text": " Who have carefully studied the data and the theory right? This is like serious scholarly work", "tokens": [2102, 362, 7500, 9454, 264, 1412, 293, 264, 5261, 558, 30, 639, 307, 411, 3156, 39589, 589], "temperature": 0.0, "avg_logprob": -0.1364658691070892, "compression_ratio": 1.6535433070866141, "no_speech_prob": 1.7061657899830607e-06}, {"id": 983, "seek": 476646, "start": 4778.74, "end": 4781.74, "text": " It's like no this is this is happening right now", "tokens": [467, 311, 411, 572, 341, 307, 341, 307, 2737, 558, 586], "temperature": 0.0, "avg_logprob": -0.1364658691070892, "compression_ratio": 1.6535433070866141, "no_speech_prob": 1.7061657899830607e-06}, {"id": 984, "seek": 476646, "start": 4781.74, "end": 4789.28, "text": " And so you know again like I'm sure the people that started creating this predictive policing algorithm", "tokens": [400, 370, 291, 458, 797, 411, 286, 478, 988, 264, 561, 300, 1409, 4084, 341, 35521, 28799, 9284], "temperature": 0.0, "avg_logprob": -0.1364658691070892, "compression_ratio": 1.6535433070866141, "no_speech_prob": 1.7061657899830607e-06}, {"id": 985, "seek": 476646, "start": 4789.38, "end": 4793.1, "text": " Didn't think like how do we arrest more black people right?", "tokens": [11151, 380, 519, 411, 577, 360, 321, 7823, 544, 2211, 561, 558, 30], "temperature": 0.0, "avg_logprob": -0.1364658691070892, "compression_ratio": 1.6535433070866141, "no_speech_prob": 1.7061657899830607e-06}, {"id": 986, "seek": 479310, "start": 4793.1, "end": 4796.9800000000005, "text": " You know hopefully they were actually thinking gosh. I'd like my children to be safer on the streets", "tokens": [509, 458, 4696, 436, 645, 767, 1953, 6502, 13, 286, 1116, 411, 452, 2227, 281, 312, 15856, 322, 264, 8481], "temperature": 0.0, "avg_logprob": -0.21003251809340256, "compression_ratio": 1.6386861313868613, "no_speech_prob": 1.7330327182207839e-06}, {"id": 987, "seek": 479310, "start": 4797.06, "end": 4800.3, "text": " How do I create you know a safer?", "tokens": [1012, 360, 286, 1884, 291, 458, 257, 15856, 30], "temperature": 0.0, "avg_logprob": -0.21003251809340256, "compression_ratio": 1.6386861313868613, "no_speech_prob": 1.7330327182207839e-06}, {"id": 988, "seek": 479310, "start": 4801.1, "end": 4807.34, "text": " Society right, but they didn't think about this this nasty runaway feedback loop", "tokens": [13742, 558, 11, 457, 436, 994, 380, 519, 466, 341, 341, 17923, 1190, 10318, 5824, 6367], "temperature": 0.0, "avg_logprob": -0.21003251809340256, "compression_ratio": 1.6386861313868613, "no_speech_prob": 1.7330327182207839e-06}, {"id": 989, "seek": 479310, "start": 4808.620000000001, "end": 4812.58, "text": " so actually this this one about social network algorithms is actually a", "tokens": [370, 767, 341, 341, 472, 466, 2093, 3209, 14642, 307, 767, 257], "temperature": 0.0, "avg_logprob": -0.21003251809340256, "compression_ratio": 1.6386861313868613, "no_speech_prob": 1.7330327182207839e-06}, {"id": 990, "seek": 479310, "start": 4813.02, "end": 4818.42, "text": " Article in the New York Times recently that one of my friends Renae de Resta and she did something", "tokens": [35230, 294, 264, 1873, 3609, 11366, 3938, 300, 472, 295, 452, 1855, 23068, 68, 368, 497, 7841, 293, 750, 630, 746], "temperature": 0.0, "avg_logprob": -0.21003251809340256, "compression_ratio": 1.6386861313868613, "no_speech_prob": 1.7330327182207839e-06}, {"id": 991, "seek": 481842, "start": 4818.42, "end": 4825.3, "text": " That was kind of amazing. She set up a second Facebook account right like a fake Facebook account and", "tokens": [663, 390, 733, 295, 2243, 13, 1240, 992, 493, 257, 1150, 4384, 2696, 558, 411, 257, 7592, 4384, 2696, 293], "temperature": 0.0, "avg_logprob": -0.26525487502415973, "compression_ratio": 1.7462686567164178, "no_speech_prob": 2.2252700091485167e-06}, {"id": 992, "seek": 481842, "start": 4826.34, "end": 4828.34, "text": " She was very interested in the anti-vax", "tokens": [1240, 390, 588, 3102, 294, 264, 6061, 12, 2757, 87], "temperature": 0.0, "avg_logprob": -0.26525487502415973, "compression_ratio": 1.7462686567164178, "no_speech_prob": 2.2252700091485167e-06}, {"id": 993, "seek": 481842, "start": 4829.3, "end": 4832.42, "text": " movement at the time so she started following a couple of", "tokens": [3963, 412, 264, 565, 370, 750, 1409, 3480, 257, 1916, 295], "temperature": 0.0, "avg_logprob": -0.26525487502415973, "compression_ratio": 1.7462686567164178, "no_speech_prob": 2.2252700091485167e-06}, {"id": 994, "seek": 481842, "start": 4833.78, "end": 4835.46, "text": " anti-vaxxers and", "tokens": [6061, 12, 2757, 30569, 433, 293], "temperature": 0.0, "avg_logprob": -0.26525487502415973, "compression_ratio": 1.7462686567164178, "no_speech_prob": 2.2252700091485167e-06}, {"id": 995, "seek": 481842, "start": 4835.46, "end": 4837.9800000000005, "text": " visited a couple of anti-vaxxer links and", "tokens": [11220, 257, 1916, 295, 6061, 12, 2757, 30569, 260, 6123, 293], "temperature": 0.0, "avg_logprob": -0.26525487502415973, "compression_ratio": 1.7462686567164178, "no_speech_prob": 2.2252700091485167e-06}, {"id": 996, "seek": 481842, "start": 4839.18, "end": 4842.26, "text": " so suddenly her newsfeed starts getting full of", "tokens": [370, 5800, 720, 2583, 37036, 3719, 1242, 1577, 295], "temperature": 0.0, "avg_logprob": -0.26525487502415973, "compression_ratio": 1.7462686567164178, "no_speech_prob": 2.2252700091485167e-06}, {"id": 997, "seek": 481842, "start": 4843.14, "end": 4845.14, "text": " anti-vaxxer news along with", "tokens": [6061, 12, 2757, 30569, 260, 2583, 2051, 365], "temperature": 0.0, "avg_logprob": -0.26525487502415973, "compression_ratio": 1.7462686567164178, "no_speech_prob": 2.2252700091485167e-06}, {"id": 998, "seek": 481842, "start": 4845.66, "end": 4847.66, "text": " other stuff like", "tokens": [661, 1507, 411], "temperature": 0.0, "avg_logprob": -0.26525487502415973, "compression_ratio": 1.7462686567164178, "no_speech_prob": 2.2252700091485167e-06}, {"id": 999, "seek": 484766, "start": 4847.66, "end": 4849.38, "text": " chemtrails and", "tokens": [4771, 17227, 4174, 293], "temperature": 0.0, "avg_logprob": -0.24505931383942905, "compression_ratio": 1.572192513368984, "no_speech_prob": 4.785051260114415e-06}, {"id": 1000, "seek": 484766, "start": 4849.38, "end": 4854.139999999999, "text": " deep state conspiracy theories and all this stuff and so she's like", "tokens": [2452, 1785, 20439, 13667, 293, 439, 341, 1507, 293, 370, 750, 311, 411], "temperature": 0.0, "avg_logprob": -0.24505931383942905, "compression_ratio": 1.572192513368984, "no_speech_prob": 4.785051260114415e-06}, {"id": 1001, "seek": 484766, "start": 4854.98, "end": 4858.139999999999, "text": " Starts clicking on those right and the more she clicked", "tokens": [6481, 82, 9697, 322, 729, 558, 293, 264, 544, 750, 23370], "temperature": 0.0, "avg_logprob": -0.24505931383942905, "compression_ratio": 1.572192513368984, "no_speech_prob": 4.785051260114415e-06}, {"id": 1002, "seek": 484766, "start": 4859.0599999999995, "end": 4860.74, "text": " the more", "tokens": [264, 544], "temperature": 0.0, "avg_logprob": -0.24505931383942905, "compression_ratio": 1.572192513368984, "no_speech_prob": 4.785051260114415e-06}, {"id": 1003, "seek": 484766, "start": 4860.74, "end": 4869.66, "text": " Hardcore far-out conspiracy stuff Facebook recommended so now when Renee goes to that Facebook account the whole thing is just full of", "tokens": [11817, 12352, 1400, 12, 346, 20439, 1507, 4384, 9628, 370, 586, 562, 47790, 1709, 281, 300, 4384, 2696, 264, 1379, 551, 307, 445, 1577, 295], "temperature": 0.0, "avg_logprob": -0.24505931383942905, "compression_ratio": 1.572192513368984, "no_speech_prob": 4.785051260114415e-06}, {"id": 1004, "seek": 484766, "start": 4871.38, "end": 4872.78, "text": " Angry", "tokens": [49860], "temperature": 0.0, "avg_logprob": -0.24505931383942905, "compression_ratio": 1.572192513368984, "no_speech_prob": 4.785051260114415e-06}, {"id": 1005, "seek": 484766, "start": 4872.78, "end": 4874.139999999999, "text": " crazy", "tokens": [3219], "temperature": 0.0, "avg_logprob": -0.24505931383942905, "compression_ratio": 1.572192513368984, "no_speech_prob": 4.785051260114415e-06}, {"id": 1006, "seek": 487414, "start": 4874.14, "end": 4879.700000000001, "text": " Far-out conspiracy stuff like that's all she sees and so if that was your world", "tokens": [9067, 12, 346, 20439, 1507, 411, 300, 311, 439, 750, 8194, 293, 370, 498, 300, 390, 428, 1002], "temperature": 0.0, "avg_logprob": -0.18276335603447372, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.3709511677006958e-06}, {"id": 1007, "seek": 487414, "start": 4880.42, "end": 4884.26, "text": " Right then as far as you're concerned. It's just like this continuous", "tokens": [1779, 550, 382, 1400, 382, 291, 434, 5922, 13, 467, 311, 445, 411, 341, 10957], "temperature": 0.0, "avg_logprob": -0.18276335603447372, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.3709511677006958e-06}, {"id": 1008, "seek": 487414, "start": 4885.54, "end": 4887.54, "text": " reminder and proof of", "tokens": [13548, 293, 8177, 295], "temperature": 0.0, "avg_logprob": -0.18276335603447372, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.3709511677006958e-06}, {"id": 1009, "seek": 487414, "start": 4888.06, "end": 4894.820000000001, "text": " Of all this stuff right and so again. It's like this this is to answer your question. This is the kind of", "tokens": [2720, 439, 341, 1507, 558, 293, 370, 797, 13, 467, 311, 411, 341, 341, 307, 281, 1867, 428, 1168, 13, 639, 307, 264, 733, 295], "temperature": 0.0, "avg_logprob": -0.18276335603447372, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.3709511677006958e-06}, {"id": 1010, "seek": 487414, "start": 4895.780000000001, "end": 4900.58, "text": " Runaway feedback loop that ends up telling me and my generals you know", "tokens": [497, 5051, 676, 5824, 6367, 300, 5314, 493, 3585, 385, 293, 452, 41346, 291, 458], "temperature": 0.0, "avg_logprob": -0.18276335603447372, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.3709511677006958e-06}, {"id": 1011, "seek": 487414, "start": 4901.38, "end": 4902.820000000001, "text": " throughout their", "tokens": [3710, 641], "temperature": 0.0, "avg_logprob": -0.18276335603447372, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.3709511677006958e-06}, {"id": 1012, "seek": 490282, "start": 4902.82, "end": 4904.98, "text": " Facebook homepage that Rohingya", "tokens": [4384, 31301, 300, 3101, 571, 3016], "temperature": 0.0, "avg_logprob": -0.24224249521891275, "compression_ratio": 1.5846994535519126, "no_speech_prob": 2.0904428765788907e-06}, {"id": 1013, "seek": 490282, "start": 4905.82, "end": 4909.0199999999995, "text": " animals and fake news and whatever else right", "tokens": [4882, 293, 7592, 2583, 293, 2035, 1646, 558], "temperature": 0.0, "avg_logprob": -0.24224249521891275, "compression_ratio": 1.5846994535519126, "no_speech_prob": 2.0904428765788907e-06}, {"id": 1014, "seek": 490282, "start": 4911.5, "end": 4915.78, "text": " So you know it's it's a lot of this comes from", "tokens": [407, 291, 458, 309, 311, 309, 311, 257, 688, 295, 341, 1487, 490], "temperature": 0.0, "avg_logprob": -0.24224249521891275, "compression_ratio": 1.5846994535519126, "no_speech_prob": 2.0904428765788907e-06}, {"id": 1015, "seek": 490282, "start": 4916.38, "end": 4923.86, "text": " Also from bias right and so like let's talk about bias specifically so bias in image software", "tokens": [2743, 490, 12577, 558, 293, 370, 411, 718, 311, 751, 466, 12577, 4682, 370, 12577, 294, 3256, 4722], "temperature": 0.0, "avg_logprob": -0.24224249521891275, "compression_ratio": 1.5846994535519126, "no_speech_prob": 2.0904428765788907e-06}, {"id": 1016, "seek": 492386, "start": 4923.86, "end": 4932.259999999999, "text": " comes from bias in data and so most of the folks I know at Google Brain", "tokens": [1487, 490, 12577, 294, 1412, 293, 370, 881, 295, 264, 4024, 286, 458, 412, 3329, 29783], "temperature": 0.0, "avg_logprob": -0.20947068313072467, "compression_ratio": 1.7194570135746607, "no_speech_prob": 2.785265849070129e-07}, {"id": 1017, "seek": 492386, "start": 4932.66, "end": 4934.86, "text": " building computer vision algorithms", "tokens": [2390, 3820, 5201, 14642], "temperature": 0.0, "avg_logprob": -0.20947068313072467, "compression_ratio": 1.7194570135746607, "no_speech_prob": 2.785265849070129e-07}, {"id": 1018, "seek": 492386, "start": 4936.38, "end": 4938.38, "text": " very few of them are people of color and", "tokens": [588, 1326, 295, 552, 366, 561, 295, 2017, 293], "temperature": 0.0, "avg_logprob": -0.20947068313072467, "compression_ratio": 1.7194570135746607, "no_speech_prob": 2.785265849070129e-07}, {"id": 1019, "seek": 492386, "start": 4939.219999999999, "end": 4943.46, "text": " So when they're training the algorithms with you know photos of their families and friends", "tokens": [407, 562, 436, 434, 3097, 264, 14642, 365, 291, 458, 5787, 295, 641, 4466, 293, 1855], "temperature": 0.0, "avg_logprob": -0.20947068313072467, "compression_ratio": 1.7194570135746607, "no_speech_prob": 2.785265849070129e-07}, {"id": 1020, "seek": 492386, "start": 4944.0199999999995, "end": 4947.82, "text": " They are training them with very few people of color and so when", "tokens": [814, 366, 3097, 552, 365, 588, 1326, 561, 295, 2017, 293, 370, 562], "temperature": 0.0, "avg_logprob": -0.20947068313072467, "compression_ratio": 1.7194570135746607, "no_speech_prob": 2.785265849070129e-07}, {"id": 1021, "seek": 492386, "start": 4948.42, "end": 4953.0599999999995, "text": " FaceApp then decided we're going to try looking at lots of Instagram photos", "tokens": [4047, 9132, 550, 3047, 321, 434, 516, 281, 853, 1237, 412, 3195, 295, 5281, 5787], "temperature": 0.0, "avg_logprob": -0.20947068313072467, "compression_ratio": 1.7194570135746607, "no_speech_prob": 2.785265849070129e-07}, {"id": 1022, "seek": 495306, "start": 4953.06, "end": 4956.860000000001, "text": " To see which ones are like no up voted the most", "tokens": [1407, 536, 597, 2306, 366, 411, 572, 493, 13415, 264, 881], "temperature": 0.0, "avg_logprob": -0.20067271319302646, "compression_ratio": 1.6788990825688073, "no_speech_prob": 1.1189378028575447e-06}, {"id": 1023, "seek": 495306, "start": 4958.1, "end": 4963.740000000001, "text": " Without them necessarily realizing it the answer was like you know light colored faces", "tokens": [9129, 552, 4725, 16734, 309, 264, 1867, 390, 411, 291, 458, 1442, 14332, 8475], "temperature": 0.0, "avg_logprob": -0.20067271319302646, "compression_ratio": 1.6788990825688073, "no_speech_prob": 1.1189378028575447e-06}, {"id": 1024, "seek": 495306, "start": 4963.740000000001, "end": 4969.740000000001, "text": " So then they built a generative model to make you more hot and so this is the actual photo", "tokens": [407, 550, 436, 3094, 257, 1337, 1166, 2316, 281, 652, 291, 544, 2368, 293, 370, 341, 307, 264, 3539, 5052], "temperature": 0.0, "avg_logprob": -0.20067271319302646, "compression_ratio": 1.6788990825688073, "no_speech_prob": 1.1189378028575447e-06}, {"id": 1025, "seek": 495306, "start": 4969.740000000001, "end": 4971.740000000001, "text": " And here is the hotter version", "tokens": [400, 510, 307, 264, 32149, 3037], "temperature": 0.0, "avg_logprob": -0.20067271319302646, "compression_ratio": 1.6788990825688073, "no_speech_prob": 1.1189378028575447e-06}, {"id": 1026, "seek": 495306, "start": 4972.18, "end": 4975.620000000001, "text": " Right so the hotter version is like more white", "tokens": [1779, 370, 264, 32149, 3037, 307, 411, 544, 2418], "temperature": 0.0, "avg_logprob": -0.20067271319302646, "compression_ratio": 1.6788990825688073, "no_speech_prob": 1.1189378028575447e-06}, {"id": 1027, "seek": 495306, "start": 4976.3, "end": 4978.1, "text": " less nostrils", "tokens": [1570, 10397, 48789], "temperature": 0.0, "avg_logprob": -0.20067271319302646, "compression_ratio": 1.6788990825688073, "no_speech_prob": 1.1189378028575447e-06}, {"id": 1028, "seek": 495306, "start": 4978.1, "end": 4981.900000000001, "text": " You know more European looking right and so like", "tokens": [509, 458, 544, 6473, 1237, 558, 293, 370, 411], "temperature": 0.0, "avg_logprob": -0.20067271319302646, "compression_ratio": 1.6788990825688073, "no_speech_prob": 1.1189378028575447e-06}, {"id": 1029, "seek": 498190, "start": 4981.9, "end": 4983.9, "text": " This did not go down well", "tokens": [639, 630, 406, 352, 760, 731], "temperature": 0.0, "avg_logprob": -0.34130179995582216, "compression_ratio": 1.6136363636363635, "no_speech_prob": 1.733039198370534e-06}, {"id": 1030, "seek": 498190, "start": 4984.339999999999, "end": 4989.9, "text": " to say the least so like the so again, you know I don't think anybody at FaceApp said like", "tokens": [281, 584, 264, 1935, 370, 411, 264, 370, 797, 11, 291, 458, 286, 500, 380, 519, 4472, 412, 4047, 9132, 848, 411], "temperature": 0.0, "avg_logprob": -0.34130179995582216, "compression_ratio": 1.6136363636363635, "no_speech_prob": 1.733039198370534e-06}, {"id": 1031, "seek": 498190, "start": 4990.62, "end": 4998.0599999999995, "text": " Let's create something that makes people look more white right they just trained it on a bunch of images of the people that they had", "tokens": [961, 311, 1884, 746, 300, 1669, 561, 574, 544, 2418, 558, 436, 445, 8895, 309, 322, 257, 3840, 295, 5267, 295, 264, 561, 300, 436, 632], "temperature": 0.0, "avg_logprob": -0.34130179995582216, "compression_ratio": 1.6136363636363635, "no_speech_prob": 1.733039198370534e-06}, {"id": 1032, "seek": 498190, "start": 4998.0599999999995, "end": 5000.0599999999995, "text": " around them okay, and", "tokens": [926, 552, 1392, 11, 293], "temperature": 0.0, "avg_logprob": -0.34130179995582216, "compression_ratio": 1.6136363636363635, "no_speech_prob": 1.733039198370534e-06}, {"id": 1033, "seek": 498190, "start": 5001.099999999999, "end": 5004.0599999999995, "text": " This has kind of cut. You know serious commercial", "tokens": [639, 575, 733, 295, 1723, 13, 509, 458, 3156, 6841], "temperature": 0.0, "avg_logprob": -0.34130179995582216, "compression_ratio": 1.6136363636363635, "no_speech_prob": 1.733039198370534e-06}, {"id": 1034, "seek": 498190, "start": 5004.86, "end": 5010.5, "text": " Implications as well they had to pull this feature right and they had a huge amount of negative pushback", "tokens": [4331, 4770, 763, 382, 731, 436, 632, 281, 2235, 341, 4111, 558, 293, 436, 632, 257, 2603, 2372, 295, 3671, 2944, 3207], "temperature": 0.0, "avg_logprob": -0.34130179995582216, "compression_ratio": 1.6136363636363635, "no_speech_prob": 1.733039198370534e-06}, {"id": 1035, "seek": 501050, "start": 5010.5, "end": 5015.82, "text": " Pushback like as they should right here's another example Google photos", "tokens": [18229, 3207, 411, 382, 436, 820, 558, 510, 311, 1071, 1365, 3329, 5787], "temperature": 0.0, "avg_logprob": -0.23895105109157333, "compression_ratio": 1.6376146788990826, "no_speech_prob": 6.048880095477216e-06}, {"id": 1036, "seek": 501050, "start": 5016.98, "end": 5018.98, "text": " created this photo", "tokens": [2942, 341, 5052], "temperature": 0.0, "avg_logprob": -0.23895105109157333, "compression_ratio": 1.6376146788990826, "no_speech_prob": 6.048880095477216e-06}, {"id": 1037, "seek": 501050, "start": 5019.86, "end": 5024.3, "text": " Classifier airplanes skyscrapers cars graduation and no gorillas", "tokens": [9471, 9902, 32947, 48227, 66, 4007, 433, 5163, 15652, 293, 572, 24012, 20243], "temperature": 0.0, "avg_logprob": -0.23895105109157333, "compression_ratio": 1.6376146788990826, "no_speech_prob": 6.048880095477216e-06}, {"id": 1038, "seek": 501050, "start": 5024.94, "end": 5032.26, "text": " Right so like think about how this looks to like most people like most to most people they look at this", "tokens": [1779, 370, 411, 519, 466, 577, 341, 1542, 281, 411, 881, 561, 411, 881, 281, 881, 561, 436, 574, 412, 341], "temperature": 0.0, "avg_logprob": -0.23895105109157333, "compression_ratio": 1.6376146788990826, "no_speech_prob": 6.048880095477216e-06}, {"id": 1039, "seek": 501050, "start": 5032.26, "end": 5034.42, "text": " They don't know about machine learning they say", "tokens": [814, 500, 380, 458, 466, 3479, 2539, 436, 584], "temperature": 0.0, "avg_logprob": -0.23895105109157333, "compression_ratio": 1.6376146788990826, "no_speech_prob": 6.048880095477216e-06}, {"id": 1040, "seek": 501050, "start": 5034.98, "end": 5038.82, "text": " What the fuck somebody at Google wrote some code?", "tokens": [708, 264, 3275, 2618, 412, 3329, 4114, 512, 3089, 30], "temperature": 0.0, "avg_logprob": -0.23895105109157333, "compression_ratio": 1.6376146788990826, "no_speech_prob": 6.048880095477216e-06}, {"id": 1041, "seek": 503882, "start": 5038.82, "end": 5043.219999999999, "text": " To take black people and call them gorillas like that's what it looks like", "tokens": [1407, 747, 2211, 561, 293, 818, 552, 24012, 20243, 411, 300, 311, 437, 309, 1542, 411], "temperature": 0.0, "avg_logprob": -0.17653820249769422, "compression_ratio": 1.7356828193832599, "no_speech_prob": 3.446539039941854e-06}, {"id": 1042, "seek": 503882, "start": 5043.66, "end": 5050.299999999999, "text": " Right now we know that's not what happened right we know what happened is you know the team you know of of", "tokens": [1779, 586, 321, 458, 300, 311, 406, 437, 2011, 558, 321, 458, 437, 2011, 307, 291, 458, 264, 1469, 291, 458, 295, 295], "temperature": 0.0, "avg_logprob": -0.17653820249769422, "compression_ratio": 1.7356828193832599, "no_speech_prob": 3.446539039941854e-06}, {"id": 1043, "seek": 503882, "start": 5051.42, "end": 5059.139999999999, "text": " folks at Google computer vision experts who have none if or few people of color working in the team", "tokens": [4024, 412, 3329, 3820, 5201, 8572, 567, 362, 6022, 498, 420, 1326, 561, 295, 2017, 1364, 294, 264, 1469], "temperature": 0.0, "avg_logprob": -0.17653820249769422, "compression_ratio": 1.7356828193832599, "no_speech_prob": 3.446539039941854e-06}, {"id": 1044, "seek": 503882, "start": 5059.94, "end": 5066.259999999999, "text": " Built a classifier using all the photos they had available to them and so when the system came along came across", "tokens": [49822, 257, 1508, 9902, 1228, 439, 264, 5787, 436, 632, 2435, 281, 552, 293, 370, 562, 264, 1185, 1361, 2051, 1361, 2108], "temperature": 0.0, "avg_logprob": -0.17653820249769422, "compression_ratio": 1.7356828193832599, "no_speech_prob": 3.446539039941854e-06}, {"id": 1045, "seek": 506626, "start": 5066.26, "end": 5071.14, "text": " You know a person with dark skin it was like oh", "tokens": [509, 458, 257, 954, 365, 2877, 3178, 309, 390, 411, 1954], "temperature": 0.0, "avg_logprob": -0.19990797837575278, "compression_ratio": 1.6031746031746033, "no_speech_prob": 2.443983021294116e-06}, {"id": 1046, "seek": 506626, "start": 5071.14, "end": 5075.54, "text": " I've only mainly seen that before amongst gorillas, so I'll put it in that category", "tokens": [286, 600, 787, 8704, 1612, 300, 949, 12918, 24012, 20243, 11, 370, 286, 603, 829, 309, 294, 300, 7719], "temperature": 0.0, "avg_logprob": -0.19990797837575278, "compression_ratio": 1.6031746031746033, "no_speech_prob": 2.443983021294116e-06}, {"id": 1047, "seek": 506626, "start": 5075.9800000000005, "end": 5084.02, "text": " Right so again. It's the bias in the data creates a bias in the software and again the commercial implications were very significant", "tokens": [1779, 370, 797, 13, 467, 311, 264, 12577, 294, 264, 1412, 7829, 257, 12577, 294, 264, 4722, 293, 797, 264, 6841, 16602, 645, 588, 4776], "temperature": 0.0, "avg_logprob": -0.19990797837575278, "compression_ratio": 1.6031746031746033, "no_speech_prob": 2.443983021294116e-06}, {"id": 1048, "seek": 506626, "start": 5084.1, "end": 5086.9800000000005, "text": " Like Google really got a lot of bad", "tokens": [1743, 3329, 534, 658, 257, 688, 295, 1578], "temperature": 0.0, "avg_logprob": -0.19990797837575278, "compression_ratio": 1.6031746031746033, "no_speech_prob": 2.443983021294116e-06}, {"id": 1049, "seek": 506626, "start": 5087.58, "end": 5092.96, "text": " PR from this as they should this this was a photo that some you know somebody put in their Twitter feed", "tokens": [11568, 490, 341, 382, 436, 820, 341, 341, 390, 257, 5052, 300, 512, 291, 458, 2618, 829, 294, 641, 5794, 3154], "temperature": 0.0, "avg_logprob": -0.19990797837575278, "compression_ratio": 1.6031746031746033, "no_speech_prob": 2.443983021294116e-06}, {"id": 1050, "seek": 509296, "start": 5092.96, "end": 5097.46, "text": " They said like look what look what Google photos just decided to do", "tokens": [814, 848, 411, 574, 437, 574, 437, 3329, 5787, 445, 3047, 281, 360], "temperature": 0.0, "avg_logprob": -0.2391947399486195, "compression_ratio": 1.656, "no_speech_prob": 1.1726372122211615e-06}, {"id": 1051, "seek": 509296, "start": 5099.1, "end": 5103.3, "text": " You can imagine what happened with the first international beauty contest judged by artificial intelligence", "tokens": [509, 393, 3811, 437, 2011, 365, 264, 700, 5058, 6643, 10287, 27485, 538, 11677, 7599], "temperature": 0.0, "avg_logprob": -0.2391947399486195, "compression_ratio": 1.656, "no_speech_prob": 1.1726372122211615e-06}, {"id": 1052, "seek": 509296, "start": 5104.02, "end": 5110.74, "text": " Right basically it turns out all the beautiful people are white again, right so like you kind of see this bias in", "tokens": [1779, 1936, 309, 4523, 484, 439, 264, 2238, 561, 366, 2418, 797, 11, 558, 370, 411, 291, 733, 295, 536, 341, 12577, 294], "temperature": 0.0, "avg_logprob": -0.2391947399486195, "compression_ratio": 1.656, "no_speech_prob": 1.1726372122211615e-06}, {"id": 1053, "seek": 509296, "start": 5111.5, "end": 5115.18, "text": " image software thanks to bias in the data thanks to", "tokens": [3256, 4722, 3231, 281, 12577, 294, 264, 1412, 3231, 281], "temperature": 0.0, "avg_logprob": -0.2391947399486195, "compression_ratio": 1.656, "no_speech_prob": 1.1726372122211615e-06}, {"id": 1054, "seek": 509296, "start": 5115.66, "end": 5120.54, "text": " By lack of diversity and the teams building it you see the same thing in", "tokens": [3146, 5011, 295, 8811, 293, 264, 5491, 2390, 309, 291, 536, 264, 912, 551, 294], "temperature": 0.0, "avg_logprob": -0.2391947399486195, "compression_ratio": 1.656, "no_speech_prob": 1.1726372122211615e-06}, {"id": 1055, "seek": 512054, "start": 5120.54, "end": 5121.74, "text": " in", "tokens": [294], "temperature": 0.0, "avg_logprob": -0.32382774353027344, "compression_ratio": 1.5885416666666667, "no_speech_prob": 3.844882485282142e-06}, {"id": 1056, "seek": 512054, "start": 5121.74, "end": 5127.7, "text": " natural language processing right so here is Turkish oh is the", "tokens": [3303, 2856, 9007, 558, 370, 510, 307, 18565, 1954, 307, 264], "temperature": 0.0, "avg_logprob": -0.32382774353027344, "compression_ratio": 1.5885416666666667, "no_speech_prob": 3.844882485282142e-06}, {"id": 1057, "seek": 512054, "start": 5129.0199999999995, "end": 5131.6, "text": " The pronoun in Turkish which has no", "tokens": [440, 14144, 294, 18565, 597, 575, 572], "temperature": 0.0, "avg_logprob": -0.32382774353027344, "compression_ratio": 1.5885416666666667, "no_speech_prob": 3.844882485282142e-06}, {"id": 1058, "seek": 512054, "start": 5132.5, "end": 5137.46, "text": " Gender right there is no he or versus she right okay. No no he versus she", "tokens": [48039, 558, 456, 307, 572, 415, 420, 5717, 750, 558, 1392, 13, 883, 572, 415, 5717, 750], "temperature": 0.0, "avg_logprob": -0.32382774353027344, "compression_ratio": 1.5885416666666667, "no_speech_prob": 3.844882485282142e-06}, {"id": 1059, "seek": 512054, "start": 5138.38, "end": 5141.78, "text": " But of course in English. We don't really have a widely used", "tokens": [583, 295, 1164, 294, 3669, 13, 492, 500, 380, 534, 362, 257, 13371, 1143], "temperature": 0.0, "avg_logprob": -0.32382774353027344, "compression_ratio": 1.5885416666666667, "no_speech_prob": 3.844882485282142e-06}, {"id": 1060, "seek": 512054, "start": 5142.46, "end": 5147.42, "text": " Ungendered singular pronouns so Google Translate converts it to this", "tokens": [43559, 3216, 292, 20010, 35883, 370, 3329, 6531, 17593, 38874, 309, 281, 341], "temperature": 0.0, "avg_logprob": -0.32382774353027344, "compression_ratio": 1.5885416666666667, "no_speech_prob": 3.844882485282142e-06}, {"id": 1061, "seek": 514742, "start": 5147.42, "end": 5149.42, "text": " Okay now", "tokens": [1033, 586], "temperature": 0.0, "avg_logprob": -0.2249519174749201, "compression_ratio": 1.5953488372093023, "no_speech_prob": 3.4465424505469855e-06}, {"id": 1062, "seek": 514742, "start": 5152.1, "end": 5156.06, "text": " There are plenty of people who saw this online and said like", "tokens": [821, 366, 7140, 295, 561, 567, 1866, 341, 2950, 293, 848, 411], "temperature": 0.0, "avg_logprob": -0.2249519174749201, "compression_ratio": 1.5953488372093023, "no_speech_prob": 3.4465424505469855e-06}, {"id": 1063, "seek": 514742, "start": 5156.9, "end": 5164.9800000000005, "text": " Literally, so what you know it is correctly feeding back the usual usage in English like this is you know", "tokens": [23768, 11, 370, 437, 291, 458, 309, 307, 8944, 12919, 646, 264, 7713, 14924, 294, 3669, 411, 341, 307, 291, 458], "temperature": 0.0, "avg_logprob": -0.2249519174749201, "compression_ratio": 1.5953488372093023, "no_speech_prob": 3.4465424505469855e-06}, {"id": 1064, "seek": 514742, "start": 5164.9800000000005, "end": 5168.76, "text": " It's it. I know how this is trained. This is like word to beck vectors", "tokens": [467, 311, 309, 13, 286, 458, 577, 341, 307, 8895, 13, 639, 307, 411, 1349, 281, 312, 547, 18875], "temperature": 0.0, "avg_logprob": -0.2249519174749201, "compression_ratio": 1.5953488372093023, "no_speech_prob": 3.4465424505469855e-06}, {"id": 1065, "seek": 514742, "start": 5168.76, "end": 5174.9800000000005, "text": " I was trained on Google News Corpus Google Books Corpus. It's just telling us how things are and", "tokens": [286, 390, 8895, 322, 3329, 7987, 3925, 31624, 3329, 33843, 3925, 31624, 13, 467, 311, 445, 3585, 505, 577, 721, 366, 293], "temperature": 0.0, "avg_logprob": -0.2249519174749201, "compression_ratio": 1.5953488372093023, "no_speech_prob": 3.4465424505469855e-06}, {"id": 1066, "seek": 517498, "start": 5174.98, "end": 5183.0199999999995, "text": " And like from a point of view that's entirely true right like the biased data to create this biased algorithm", "tokens": [400, 411, 490, 257, 935, 295, 1910, 300, 311, 7696, 2074, 558, 411, 264, 28035, 1412, 281, 1884, 341, 28035, 9284], "temperature": 0.0, "avg_logprob": -0.20346160987754922, "compression_ratio": 1.6616161616161615, "no_speech_prob": 2.156801429009647e-06}, {"id": 1067, "seek": 517498, "start": 5183.74, "end": 5189.799999999999, "text": " Is the actual data of how people have written books and newspaper articles for decades?", "tokens": [1119, 264, 3539, 1412, 295, 577, 561, 362, 3720, 3642, 293, 13669, 11290, 337, 7878, 30], "temperature": 0.0, "avg_logprob": -0.20346160987754922, "compression_ratio": 1.6616161616161615, "no_speech_prob": 2.156801429009647e-06}, {"id": 1068, "seek": 517498, "start": 5191.62, "end": 5193.82, "text": " But does that mean that this is", "tokens": [583, 775, 300, 914, 300, 341, 307], "temperature": 0.0, "avg_logprob": -0.20346160987754922, "compression_ratio": 1.6616161616161615, "no_speech_prob": 2.156801429009647e-06}, {"id": 1069, "seek": 517498, "start": 5194.66, "end": 5196.66, "text": " The product that you want to create", "tokens": [440, 1674, 300, 291, 528, 281, 1884], "temperature": 0.0, "avg_logprob": -0.20346160987754922, "compression_ratio": 1.6616161616161615, "no_speech_prob": 2.156801429009647e-06}, {"id": 1070, "seek": 517498, "start": 5196.98, "end": 5199.86, "text": " You know does this mean this is the product you have to create?", "tokens": [509, 458, 775, 341, 914, 341, 307, 264, 1674, 291, 362, 281, 1884, 30], "temperature": 0.0, "avg_logprob": -0.20346160987754922, "compression_ratio": 1.6616161616161615, "no_speech_prob": 2.156801429009647e-06}, {"id": 1071, "seek": 519986, "start": 5199.86, "end": 5206.5, "text": " Right just because the particular way you've trained the model means it ends up doing this you know", "tokens": [1779, 445, 570, 264, 1729, 636, 291, 600, 8895, 264, 2316, 1355, 309, 5314, 493, 884, 341, 291, 458], "temperature": 0.0, "avg_logprob": -0.17590247625592112, "compression_ratio": 1.6163793103448276, "no_speech_prob": 5.955067535978742e-06}, {"id": 1072, "seek": 519986, "start": 5206.82, "end": 5210.179999999999, "text": " Is this actually the design you want and can you think of?", "tokens": [1119, 341, 767, 264, 1715, 291, 528, 293, 393, 291, 519, 295, 30], "temperature": 0.0, "avg_logprob": -0.17590247625592112, "compression_ratio": 1.6163793103448276, "no_speech_prob": 5.955067535978742e-06}, {"id": 1073, "seek": 519986, "start": 5210.94, "end": 5212.5, "text": " potential negative", "tokens": [3995, 3671], "temperature": 0.0, "avg_logprob": -0.17590247625592112, "compression_ratio": 1.6163793103448276, "no_speech_prob": 5.955067535978742e-06}, {"id": 1074, "seek": 519986, "start": 5212.5, "end": 5217.139999999999, "text": " Implications and feedback loops this could create right and you know if any of these things bother you", "tokens": [4331, 4770, 763, 293, 5824, 16121, 341, 727, 1884, 558, 293, 291, 458, 498, 604, 295, 613, 721, 8677, 291], "temperature": 0.0, "avg_logprob": -0.17590247625592112, "compression_ratio": 1.6163793103448276, "no_speech_prob": 5.955067535978742e-06}, {"id": 1075, "seek": 519986, "start": 5217.58, "end": 5223.58, "text": " Then now if lucky you you have a new cool engineering problem to work on like how do I create?", "tokens": [1396, 586, 498, 6356, 291, 291, 362, 257, 777, 1627, 7043, 1154, 281, 589, 322, 411, 577, 360, 286, 1884, 30], "temperature": 0.0, "avg_logprob": -0.17590247625592112, "compression_ratio": 1.6163793103448276, "no_speech_prob": 5.955067535978742e-06}, {"id": 1076, "seek": 522358, "start": 5223.58, "end": 5231.26, "text": " Unbiased NLP solutions and now there are some startups starting to do that and starting to make some money right so like opportunity", "tokens": [1156, 5614, 1937, 426, 45196, 6547, 293, 586, 456, 366, 512, 28041, 2891, 281, 360, 300, 293, 2891, 281, 652, 512, 1460, 558, 370, 411, 2650], "temperature": 0.0, "avg_logprob": -0.19713125041886873, "compression_ratio": 1.7252747252747254, "no_speech_prob": 1.1365580121491803e-06}, {"id": 1077, "seek": 522358, "start": 5231.26, "end": 5235.34, "text": " You know these are opportunities for you. It's like hey. Here's some stuff where people are creating", "tokens": [509, 458, 613, 366, 4786, 337, 291, 13, 467, 311, 411, 4177, 13, 1692, 311, 512, 1507, 689, 561, 366, 4084], "temperature": 0.0, "avg_logprob": -0.19713125041886873, "compression_ratio": 1.7252747252747254, "no_speech_prob": 1.1365580121491803e-06}, {"id": 1078, "seek": 522358, "start": 5236.1, "end": 5241.94, "text": " Screwed up societal outcomes because of their shitty models like okay, well you can go and build something better", "tokens": [42630, 292, 493, 33472, 10070, 570, 295, 641, 30748, 5245, 411, 1392, 11, 731, 291, 393, 352, 293, 1322, 746, 1101], "temperature": 0.0, "avg_logprob": -0.19713125041886873, "compression_ratio": 1.7252747252747254, "no_speech_prob": 1.1365580121491803e-06}, {"id": 1079, "seek": 522358, "start": 5242.42, "end": 5247.38, "text": " right so like another example of the bias in word to beck word vectors is", "tokens": [558, 370, 411, 1071, 1365, 295, 264, 12577, 294, 1349, 281, 312, 547, 1349, 18875, 307], "temperature": 0.0, "avg_logprob": -0.19713125041886873, "compression_ratio": 1.7252747252747254, "no_speech_prob": 1.1365580121491803e-06}, {"id": 1080, "seek": 522358, "start": 5248.62, "end": 5251.78, "text": " restaurant reviews rank Mexican restaurants worse", "tokens": [6383, 10229, 6181, 16164, 11486, 5324], "temperature": 0.0, "avg_logprob": -0.19713125041886873, "compression_ratio": 1.7252747252747254, "no_speech_prob": 1.1365580121491803e-06}, {"id": 1081, "seek": 525178, "start": 5251.78, "end": 5253.36, "text": " Because", "tokens": [1436], "temperature": 0.0, "avg_logprob": -0.1811926853225892, "compression_ratio": 1.6075949367088607, "no_speech_prob": 1.6280374666166608e-06}, {"id": 1082, "seek": 525178, "start": 5253.36, "end": 5260.42, "text": " Mexican the Mexican words tend to be associated with criminal words in the US press and books more often", "tokens": [16164, 264, 16164, 2283, 3928, 281, 312, 6615, 365, 8628, 2283, 294, 264, 2546, 1886, 293, 3642, 544, 2049], "temperature": 0.0, "avg_logprob": -0.1811926853225892, "compression_ratio": 1.6075949367088607, "no_speech_prob": 1.6280374666166608e-06}, {"id": 1083, "seek": 525178, "start": 5261.099999999999, "end": 5264.66, "text": " Again, this is like a real problem that is happening right now", "tokens": [3764, 11, 341, 307, 411, 257, 957, 1154, 300, 307, 2737, 558, 586], "temperature": 0.0, "avg_logprob": -0.1811926853225892, "compression_ratio": 1.6075949367088607, "no_speech_prob": 1.6280374666166608e-06}, {"id": 1084, "seek": 525178, "start": 5268.259999999999, "end": 5275.0, "text": " So you know Rachel actually did some interesting analysis of just the plain word to beck word vectors", "tokens": [407, 291, 458, 14246, 767, 630, 512, 1880, 5215, 295, 445, 264, 11121, 1349, 281, 312, 547, 1349, 18875], "temperature": 0.0, "avg_logprob": -0.1811926853225892, "compression_ratio": 1.6075949367088607, "no_speech_prob": 1.6280374666166608e-06}, {"id": 1085, "seek": 527500, "start": 5275.0, "end": 5281.56, "text": " Where she basically pulled them out and you know looked at these analogies based on some research that", "tokens": [2305, 750, 1936, 7373, 552, 484, 293, 291, 458, 2956, 412, 613, 16660, 530, 2361, 322, 512, 2132, 300], "temperature": 0.0, "avg_logprob": -0.20627212524414062, "compression_ratio": 1.7007874015748032, "no_speech_prob": 9.570788961354992e-07}, {"id": 1086, "seek": 527500, "start": 5282.04, "end": 5285.96, "text": " had been done elsewhere, and so you can see like word to beck like the", "tokens": [632, 668, 1096, 14517, 11, 293, 370, 291, 393, 536, 411, 1349, 281, 312, 547, 411, 264], "temperature": 0.0, "avg_logprob": -0.20627212524414062, "compression_ratio": 1.7007874015748032, "no_speech_prob": 9.570788961354992e-07}, {"id": 1087, "seek": 527500, "start": 5286.68, "end": 5290.76, "text": " The vector directions show that father is to doctor is the mother is to nurse", "tokens": [440, 8062, 11095, 855, 300, 3086, 307, 281, 4631, 307, 264, 2895, 307, 281, 14012], "temperature": 0.0, "avg_logprob": -0.20627212524414062, "compression_ratio": 1.7007874015748032, "no_speech_prob": 9.570788961354992e-07}, {"id": 1088, "seek": 527500, "start": 5291.28, "end": 5294.24, "text": " Man is to computer programmer as woman is to homemaker", "tokens": [2458, 307, 281, 3820, 32116, 382, 3059, 307, 281, 1280, 18821], "temperature": 0.0, "avg_logprob": -0.20627212524414062, "compression_ratio": 1.7007874015748032, "no_speech_prob": 9.570788961354992e-07}, {"id": 1089, "seek": 527500, "start": 5295.24, "end": 5298.8, "text": " And so forth right so like it's really easy to see", "tokens": [400, 370, 5220, 558, 370, 411, 309, 311, 534, 1858, 281, 536], "temperature": 0.0, "avg_logprob": -0.20627212524414062, "compression_ratio": 1.7007874015748032, "no_speech_prob": 9.570788961354992e-07}, {"id": 1090, "seek": 529880, "start": 5298.8, "end": 5304.08, "text": " What's in these word vectors, and you know they're kind of fundamental to", "tokens": [708, 311, 294, 613, 1349, 18875, 11, 293, 291, 458, 436, 434, 733, 295, 8088, 281], "temperature": 0.0, "avg_logprob": -0.20836222613299335, "compression_ratio": 1.4881516587677726, "no_speech_prob": 7.453749617525318e-07}, {"id": 1091, "seek": 529880, "start": 5304.92, "end": 5308.84, "text": " Much of the NLP or probably just about all the NLP software we use today", "tokens": [12313, 295, 264, 426, 45196, 420, 1391, 445, 466, 439, 264, 426, 45196, 4722, 321, 764, 965], "temperature": 0.0, "avg_logprob": -0.20836222613299335, "compression_ratio": 1.4881516587677726, "no_speech_prob": 7.453749617525318e-07}, {"id": 1092, "seek": 529880, "start": 5310.64, "end": 5312.64, "text": " So like here's a great example", "tokens": [407, 411, 510, 311, 257, 869, 1365], "temperature": 0.0, "avg_logprob": -0.20836222613299335, "compression_ratio": 1.4881516587677726, "no_speech_prob": 7.453749617525318e-07}, {"id": 1093, "seek": 529880, "start": 5315.24, "end": 5318.24, "text": " So ProPublica has actually done a lot of good work in this area", "tokens": [407, 1705, 47, 3865, 64, 575, 767, 1096, 257, 688, 295, 665, 589, 294, 341, 1859], "temperature": 0.0, "avg_logprob": -0.20836222613299335, "compression_ratio": 1.4881516587677726, "no_speech_prob": 7.453749617525318e-07}, {"id": 1094, "seek": 529880, "start": 5321.4800000000005, "end": 5326.08, "text": " Judges many judges now have access to sentencing guidelines software and", "tokens": [7661, 2880, 867, 14449, 586, 362, 2105, 281, 2279, 13644, 12470, 4722, 293], "temperature": 0.0, "avg_logprob": -0.20836222613299335, "compression_ratio": 1.4881516587677726, "no_speech_prob": 7.453749617525318e-07}, {"id": 1095, "seek": 532608, "start": 5326.08, "end": 5329.6, "text": " So sentencing guidelines software says to the judge", "tokens": [407, 2279, 13644, 12470, 4722, 1619, 281, 264, 6995], "temperature": 0.0, "avg_logprob": -0.18813942417953955, "compression_ratio": 1.7286821705426356, "no_speech_prob": 2.948000883407076e-06}, {"id": 1096, "seek": 532608, "start": 5330.32, "end": 5334.2, "text": " For this individual we would recommend this kind of sentence", "tokens": [1171, 341, 2609, 321, 576, 2748, 341, 733, 295, 8174], "temperature": 0.0, "avg_logprob": -0.18813942417953955, "compression_ratio": 1.7286821705426356, "no_speech_prob": 2.948000883407076e-06}, {"id": 1097, "seek": 532608, "start": 5335.04, "end": 5337.04, "text": " Right and now of course a judge", "tokens": [1779, 293, 586, 295, 1164, 257, 6995], "temperature": 0.0, "avg_logprob": -0.18813942417953955, "compression_ratio": 1.7286821705426356, "no_speech_prob": 2.948000883407076e-06}, {"id": 1098, "seek": 532608, "start": 5337.84, "end": 5341.12, "text": " Doesn't understand machine learning so like they have two choices", "tokens": [12955, 380, 1223, 3479, 2539, 370, 411, 436, 362, 732, 7994], "temperature": 0.0, "avg_logprob": -0.18813942417953955, "compression_ratio": 1.7286821705426356, "no_speech_prob": 2.948000883407076e-06}, {"id": 1099, "seek": 532608, "start": 5341.2, "end": 5348.2, "text": " Which is either do what it says or ignore it entirely right and some people fall into each category right and", "tokens": [3013, 307, 2139, 360, 437, 309, 1619, 420, 11200, 309, 7696, 558, 293, 512, 561, 2100, 666, 1184, 7719, 558, 293], "temperature": 0.0, "avg_logprob": -0.18813942417953955, "compression_ratio": 1.7286821705426356, "no_speech_prob": 2.948000883407076e-06}, {"id": 1100, "seek": 532608, "start": 5348.68, "end": 5352.4, "text": " So for the ones that fall into the like do what it says category. Here's what happens", "tokens": [407, 337, 264, 2306, 300, 2100, 666, 264, 411, 360, 437, 309, 1619, 7719, 13, 1692, 311, 437, 2314], "temperature": 0.0, "avg_logprob": -0.18813942417953955, "compression_ratio": 1.7286821705426356, "no_speech_prob": 2.948000883407076e-06}, {"id": 1101, "seek": 535240, "start": 5352.4, "end": 5359.0199999999995, "text": " For those that were labeled higher risk right the subset of those that label high risk", "tokens": [1171, 729, 300, 645, 21335, 2946, 3148, 558, 264, 25993, 295, 729, 300, 7645, 1090, 3148], "temperature": 0.0, "avg_logprob": -0.25165256924099394, "compression_ratio": 1.8277511961722488, "no_speech_prob": 8.990938340502908e-07}, {"id": 1102, "seek": 535240, "start": 5359.0199999999995, "end": 5361.0199999999995, "text": " It actually turned out not to reoffend", "tokens": [467, 767, 3574, 484, 406, 281, 319, 4506, 521], "temperature": 0.0, "avg_logprob": -0.25165256924099394, "compression_ratio": 1.8277511961722488, "no_speech_prob": 8.990938340502908e-07}, {"id": 1103, "seek": 535240, "start": 5361.28, "end": 5364.0, "text": " Right was about a quarter of whites and", "tokens": [1779, 390, 466, 257, 6555, 295, 21909, 293], "temperature": 0.0, "avg_logprob": -0.25165256924099394, "compression_ratio": 1.8277511961722488, "no_speech_prob": 8.990938340502908e-07}, {"id": 1104, "seek": 535240, "start": 5364.799999999999, "end": 5369.5199999999995, "text": " About a half of African Americans right so like nearly twice as often", "tokens": [7769, 257, 1922, 295, 7312, 6280, 558, 370, 411, 6217, 6091, 382, 2049], "temperature": 0.0, "avg_logprob": -0.25165256924099394, "compression_ratio": 1.8277511961722488, "no_speech_prob": 8.990938340502908e-07}, {"id": 1105, "seek": 535240, "start": 5370.32, "end": 5372.32, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.25165256924099394, "compression_ratio": 1.8277511961722488, "no_speech_prob": 8.990938340502908e-07}, {"id": 1106, "seek": 535240, "start": 5372.48, "end": 5378.32, "text": " People who didn't reoffend were marked as higher risk if they were African American and vice versa", "tokens": [3432, 567, 994, 380, 319, 4506, 521, 645, 12658, 382, 2946, 3148, 498, 436, 645, 7312, 2665, 293, 11964, 25650], "temperature": 0.0, "avg_logprob": -0.25165256924099394, "compression_ratio": 1.8277511961722488, "no_speech_prob": 8.990938340502908e-07}, {"id": 1107, "seek": 535240, "start": 5378.839999999999, "end": 5380.839999999999, "text": " amongst those that are labeled lower risk", "tokens": [12918, 729, 300, 366, 21335, 3126, 3148], "temperature": 0.0, "avg_logprob": -0.25165256924099394, "compression_ratio": 1.8277511961722488, "no_speech_prob": 8.990938340502908e-07}, {"id": 1108, "seek": 538084, "start": 5380.84, "end": 5382.32, "text": " but", "tokens": [457], "temperature": 0.0, "avg_logprob": -0.20767033100128174, "compression_ratio": 1.5797872340425532, "no_speech_prob": 7.571118203486549e-07}, {"id": 1109, "seek": 538084, "start": 5382.32, "end": 5384.32, "text": " Actually did reoffend", "tokens": [5135, 630, 319, 4506, 521], "temperature": 0.0, "avg_logprob": -0.20767033100128174, "compression_ratio": 1.5797872340425532, "no_speech_prob": 7.571118203486549e-07}, {"id": 1110, "seek": 538084, "start": 5384.32, "end": 5390.76, "text": " Turned out to be about half of the whites and only 28% of the African Americans like so like this is data", "tokens": [7956, 292, 484, 281, 312, 466, 1922, 295, 264, 21909, 293, 787, 7562, 4, 295, 264, 7312, 6280, 411, 370, 411, 341, 307, 1412], "temperature": 0.0, "avg_logprob": -0.20767033100128174, "compression_ratio": 1.5797872340425532, "no_speech_prob": 7.571118203486549e-07}, {"id": 1111, "seek": 538084, "start": 5390.76, "end": 5397.8, "text": " Which I I would like to think nobody is setting out to create something that does this right, but when you", "tokens": [3013, 286, 286, 576, 411, 281, 519, 5079, 307, 3287, 484, 281, 1884, 746, 300, 775, 341, 558, 11, 457, 562, 291], "temperature": 0.0, "avg_logprob": -0.20767033100128174, "compression_ratio": 1.5797872340425532, "no_speech_prob": 7.571118203486549e-07}, {"id": 1112, "seek": 538084, "start": 5398.400000000001, "end": 5400.2, "text": " start with", "tokens": [722, 365], "temperature": 0.0, "avg_logprob": -0.20767033100128174, "compression_ratio": 1.5797872340425532, "no_speech_prob": 7.571118203486549e-07}, {"id": 1113, "seek": 538084, "start": 5400.2, "end": 5402.92, "text": " bias data right and you know", "tokens": [12577, 1412, 558, 293, 291, 458], "temperature": 0.0, "avg_logprob": -0.20767033100128174, "compression_ratio": 1.5797872340425532, "no_speech_prob": 7.571118203486549e-07}, {"id": 1114, "seek": 538084, "start": 5404.360000000001, "end": 5406.360000000001, "text": " the data says that", "tokens": [264, 1412, 1619, 300], "temperature": 0.0, "avg_logprob": -0.20767033100128174, "compression_ratio": 1.5797872340425532, "no_speech_prob": 7.571118203486549e-07}, {"id": 1115, "seek": 540636, "start": 5406.36, "end": 5412.0, "text": " Whites and blacks smoke marijuana at about the same rate, but blacks are", "tokens": [506, 3324, 293, 30720, 8439, 24956, 412, 466, 264, 912, 3314, 11, 457, 30720, 366], "temperature": 0.0, "avg_logprob": -0.18488477623980978, "compression_ratio": 1.7445255474452555, "no_speech_prob": 9.721505875859293e-07}, {"id": 1116, "seek": 540636, "start": 5412.719999999999, "end": 5419.92, "text": " Jailed at I think it's something like five times more often than whites like you know the nature of the justice system in America", "tokens": [508, 24731, 412, 286, 519, 309, 311, 746, 411, 1732, 1413, 544, 2049, 813, 21909, 411, 291, 458, 264, 3687, 295, 264, 6118, 1185, 294, 3374], "temperature": 0.0, "avg_logprob": -0.18488477623980978, "compression_ratio": 1.7445255474452555, "no_speech_prob": 9.721505875859293e-07}, {"id": 1117, "seek": 540636, "start": 5419.92, "end": 5421.719999999999, "text": " At least at the moment is", "tokens": [1711, 1935, 412, 264, 1623, 307], "temperature": 0.0, "avg_logprob": -0.18488477623980978, "compression_ratio": 1.7445255474452555, "no_speech_prob": 9.721505875859293e-07}, {"id": 1118, "seek": 540636, "start": 5421.719999999999, "end": 5425.599999999999, "text": " That it's not it's not equal. It's not fair and therefore the data", "tokens": [663, 309, 311, 406, 309, 311, 406, 2681, 13, 467, 311, 406, 3143, 293, 4412, 264, 1412], "temperature": 0.0, "avg_logprob": -0.18488477623980978, "compression_ratio": 1.7445255474452555, "no_speech_prob": 9.721505875859293e-07}, {"id": 1119, "seek": 540636, "start": 5426.679999999999, "end": 5428.599999999999, "text": " That's fed into the machine learning model", "tokens": [663, 311, 4636, 666, 264, 3479, 2539, 2316], "temperature": 0.0, "avg_logprob": -0.18488477623980978, "compression_ratio": 1.7445255474452555, "no_speech_prob": 9.721505875859293e-07}, {"id": 1120, "seek": 540636, "start": 5428.599999999999, "end": 5433.44, "text": " It's going to basically support that status quo and then because of the negative feedback loop", "tokens": [467, 311, 516, 281, 1936, 1406, 300, 6558, 28425, 293, 550, 570, 295, 264, 3671, 5824, 6367], "temperature": 0.0, "avg_logprob": -0.18488477623980978, "compression_ratio": 1.7445255474452555, "no_speech_prob": 9.721505875859293e-07}, {"id": 1121, "seek": 540636, "start": 5433.679999999999, "end": 5435.679999999999, "text": " It's just going to get worse and worse right", "tokens": [467, 311, 445, 516, 281, 483, 5324, 293, 5324, 558], "temperature": 0.0, "avg_logprob": -0.18488477623980978, "compression_ratio": 1.7445255474452555, "no_speech_prob": 9.721505875859293e-07}, {"id": 1122, "seek": 543568, "start": 5435.68, "end": 5438.4800000000005, "text": " I'll tell you something else interesting about this one which", "tokens": [286, 603, 980, 291, 746, 1646, 1880, 466, 341, 472, 597], "temperature": 0.0, "avg_logprob": -0.21408776233070775, "compression_ratio": 1.6237113402061856, "no_speech_prob": 1.5294085642381106e-06}, {"id": 1123, "seek": 543568, "start": 5439.04, "end": 5446.88, "text": " Research called a bong has pointed out is here are some of the questions that are being asked right so let's let's take one", "tokens": [10303, 1219, 257, 272, 556, 575, 10932, 484, 307, 510, 366, 512, 295, 264, 1651, 300, 366, 885, 2351, 558, 370, 718, 311, 718, 311, 747, 472], "temperature": 0.0, "avg_logprob": -0.21408776233070775, "compression_ratio": 1.6237113402061856, "no_speech_prob": 1.5294085642381106e-06}, {"id": 1124, "seek": 543568, "start": 5449.360000000001, "end": 5451.360000000001, "text": " Was your father ever", "tokens": [3027, 428, 3086, 1562], "temperature": 0.0, "avg_logprob": -0.21408776233070775, "compression_ratio": 1.6237113402061856, "no_speech_prob": 1.5294085642381106e-06}, {"id": 1125, "seek": 543568, "start": 5451.92, "end": 5453.04, "text": " arrested", "tokens": [12469], "temperature": 0.0, "avg_logprob": -0.21408776233070775, "compression_ratio": 1.6237113402061856, "no_speech_prob": 1.5294085642381106e-06}, {"id": 1126, "seek": 543568, "start": 5453.04, "end": 5458.6, "text": " Right so your answer to that question is going to decide whether you're locked up and for how long?", "tokens": [1779, 370, 428, 1867, 281, 300, 1168, 307, 516, 281, 4536, 1968, 291, 434, 9376, 493, 293, 337, 577, 938, 30], "temperature": 0.0, "avg_logprob": -0.21408776233070775, "compression_ratio": 1.6237113402061856, "no_speech_prob": 1.5294085642381106e-06}, {"id": 1127, "seek": 545860, "start": 5458.6, "end": 5467.200000000001, "text": " Now as a machine learning researcher, do you think that might improve the predictive accuracy of your algorithm and get you a better?", "tokens": [823, 382, 257, 3479, 2539, 21751, 11, 360, 291, 519, 300, 1062, 3470, 264, 35521, 14170, 295, 428, 9284, 293, 483, 291, 257, 1101, 30], "temperature": 0.0, "avg_logprob": -0.2172077178955078, "compression_ratio": 1.59765625, "no_speech_prob": 5.804983516100037e-07}, {"id": 1128, "seek": 545860, "start": 5467.200000000001, "end": 5468.400000000001, "text": " R-squared", "tokens": [497, 12, 33292, 1642], "temperature": 0.0, "avg_logprob": -0.2172077178955078, "compression_ratio": 1.59765625, "no_speech_prob": 5.804983516100037e-07}, {"id": 1129, "seek": 545860, "start": 5468.400000000001, "end": 5473.92, "text": " It could well, but I don't know you know maybe it does we try it out. Oh, I got a better r-squared", "tokens": [467, 727, 731, 11, 457, 286, 500, 380, 458, 291, 458, 1310, 309, 775, 321, 853, 309, 484, 13, 876, 11, 286, 658, 257, 1101, 367, 12, 33292, 1642], "temperature": 0.0, "avg_logprob": -0.2172077178955078, "compression_ratio": 1.59765625, "no_speech_prob": 5.804983516100037e-07}, {"id": 1130, "seek": 545860, "start": 5474.4800000000005, "end": 5477.52, "text": " So does that mean you should use it like well?", "tokens": [407, 775, 300, 914, 291, 820, 764, 309, 411, 731, 30], "temperature": 0.0, "avg_logprob": -0.2172077178955078, "compression_ratio": 1.59765625, "no_speech_prob": 5.804983516100037e-07}, {"id": 1131, "seek": 545860, "start": 5477.52, "end": 5483.84, "text": " There's another question like do you think it's reasonable to lock somebody up for longer because of who their dad was?", "tokens": [821, 311, 1071, 1168, 411, 360, 291, 519, 309, 311, 10585, 281, 4017, 2618, 493, 337, 2854, 570, 295, 567, 641, 3546, 390, 30], "temperature": 0.0, "avg_logprob": -0.2172077178955078, "compression_ratio": 1.59765625, "no_speech_prob": 5.804983516100037e-07}, {"id": 1132, "seek": 548384, "start": 5483.84, "end": 5490.08, "text": " But and yet these are actually the examples of questions that we are asking right now", "tokens": [583, 293, 1939, 613, 366, 767, 264, 5110, 295, 1651, 300, 321, 366, 3365, 558, 586], "temperature": 0.0, "avg_logprob": -0.1920526219510484, "compression_ratio": 1.5669291338582678, "no_speech_prob": 1.1544576636879356e-06}, {"id": 1133, "seek": 548384, "start": 5490.92, "end": 5496.9400000000005, "text": " To offenders and then putting into a machine learning system to decide what happens to them, okay?", "tokens": [1407, 49079, 293, 550, 3372, 666, 257, 3479, 2539, 1185, 281, 4536, 437, 2314, 281, 552, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.1920526219510484, "compression_ratio": 1.5669291338582678, "no_speech_prob": 1.1544576636879356e-06}, {"id": 1134, "seek": 548384, "start": 5496.9400000000005, "end": 5499.28, "text": " So again like whoever designed this", "tokens": [407, 797, 411, 11387, 4761, 341], "temperature": 0.0, "avg_logprob": -0.1920526219510484, "compression_ratio": 1.5669291338582678, "no_speech_prob": 1.1544576636879356e-06}, {"id": 1135, "seek": 548384, "start": 5500.32, "end": 5506.6, "text": " Presumably they were like laser focused on technical excellence getting the maximum area under the ROC curve", "tokens": [2718, 449, 1188, 436, 645, 411, 12530, 5178, 322, 6191, 21268, 1242, 264, 6674, 1859, 833, 264, 9025, 34, 7605], "temperature": 0.0, "avg_logprob": -0.1920526219510484, "compression_ratio": 1.5669291338582678, "no_speech_prob": 1.1544576636879356e-06}, {"id": 1136, "seek": 548384, "start": 5506.6, "end": 5510.400000000001, "text": " And I found these great predictors that give me another point oh two", "tokens": [400, 286, 1352, 613, 869, 6069, 830, 300, 976, 385, 1071, 935, 1954, 732], "temperature": 0.0, "avg_logprob": -0.1920526219510484, "compression_ratio": 1.5669291338582678, "no_speech_prob": 1.1544576636879356e-06}, {"id": 1137, "seek": 551040, "start": 5510.4, "end": 5516.5599999999995, "text": " Right and I guess didn't stop to think like well is that a reasonable way?", "tokens": [1779, 293, 286, 2041, 994, 380, 1590, 281, 519, 411, 731, 307, 300, 257, 10585, 636, 30], "temperature": 0.0, "avg_logprob": -0.19763052804129463, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.2482680631364929e-06}, {"id": 1138, "seek": 551040, "start": 5517.16, "end": 5519.679999999999, "text": " To decide who goes to jail for longer", "tokens": [1407, 4536, 567, 1709, 281, 10511, 337, 2854], "temperature": 0.0, "avg_logprob": -0.19763052804129463, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.2482680631364929e-06}, {"id": 1139, "seek": 551040, "start": 5523.4, "end": 5527.32, "text": " So like putting this together you can kind of see how this can", "tokens": [407, 411, 3372, 341, 1214, 291, 393, 733, 295, 536, 577, 341, 393], "temperature": 0.0, "avg_logprob": -0.19763052804129463, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.2482680631364929e-06}, {"id": 1140, "seek": 551040, "start": 5528.839999999999, "end": 5530.92, "text": " Get you know more and more scary", "tokens": [3240, 291, 458, 544, 293, 544, 6958], "temperature": 0.0, "avg_logprob": -0.19763052804129463, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.2482680631364929e-06}, {"id": 1141, "seek": 551040, "start": 5531.759999999999, "end": 5535.759999999999, "text": " We take a company like taser right and tasers are these", "tokens": [492, 747, 257, 2237, 411, 8023, 260, 558, 293, 8023, 433, 366, 613], "temperature": 0.0, "avg_logprob": -0.19763052804129463, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.2482680631364929e-06}, {"id": 1142, "seek": 553576, "start": 5535.76, "end": 5542.72, "text": " Devices that kind of give you a big electric shock basically and tasers managed to do a great job of creating", "tokens": [9096, 1473, 300, 733, 295, 976, 291, 257, 955, 5210, 5588, 1936, 293, 8023, 433, 6453, 281, 360, 257, 869, 1691, 295, 4084], "temperature": 0.0, "avg_logprob": -0.1756438461004519, "compression_ratio": 1.6549019607843136, "no_speech_prob": 1.90337675576302e-06}, {"id": 1143, "seek": 553576, "start": 5543.16, "end": 5548.2, "text": " strong relationships with some academic researchers who seem to say whatever they", "tokens": [2068, 6159, 365, 512, 7778, 10309, 567, 1643, 281, 584, 2035, 436], "temperature": 0.0, "avg_logprob": -0.1756438461004519, "compression_ratio": 1.6549019607843136, "no_speech_prob": 1.90337675576302e-06}, {"id": 1144, "seek": 553576, "start": 5548.52, "end": 5552.76, "text": " Tell them to say to the extent where now if you look at the data", "tokens": [5115, 552, 281, 584, 281, 264, 8396, 689, 586, 498, 291, 574, 412, 264, 1412], "temperature": 0.0, "avg_logprob": -0.1756438461004519, "compression_ratio": 1.6549019607843136, "no_speech_prob": 1.90337675576302e-06}, {"id": 1145, "seek": 553576, "start": 5553.12, "end": 5558.52, "text": " It turns out that there's a much higher problem. You know there's a pretty high probability that if you get tased", "tokens": [467, 4523, 484, 300, 456, 311, 257, 709, 2946, 1154, 13, 509, 458, 456, 311, 257, 1238, 1090, 8482, 300, 498, 291, 483, 256, 1937], "temperature": 0.0, "avg_logprob": -0.1756438461004519, "compression_ratio": 1.6549019607843136, "no_speech_prob": 1.90337675576302e-06}, {"id": 1146, "seek": 553576, "start": 5559.16, "end": 5560.8, "text": " That you will die", "tokens": [663, 291, 486, 978], "temperature": 0.0, "avg_logprob": -0.1756438461004519, "compression_ratio": 1.6549019607843136, "no_speech_prob": 1.90337675576302e-06}, {"id": 1147, "seek": 553576, "start": 5560.8, "end": 5562.68, "text": " It happens you know", "tokens": [467, 2314, 291, 458], "temperature": 0.0, "avg_logprob": -0.1756438461004519, "compression_ratio": 1.6549019607843136, "no_speech_prob": 1.90337675576302e-06}, {"id": 1148, "seek": 553576, "start": 5562.68, "end": 5564.52, "text": " Not unusually", "tokens": [1726, 10054, 671], "temperature": 0.0, "avg_logprob": -0.1756438461004519, "compression_ratio": 1.6549019607843136, "no_speech_prob": 1.90337675576302e-06}, {"id": 1149, "seek": 556452, "start": 5564.52, "end": 5571.26, "text": " And yet you know the researchers who they've paid to look into this have consistently come back and said oh no", "tokens": [400, 1939, 291, 458, 264, 10309, 567, 436, 600, 4835, 281, 574, 666, 341, 362, 14961, 808, 646, 293, 848, 1954, 572], "temperature": 0.0, "avg_logprob": -0.18818341423483456, "compression_ratio": 1.6059322033898304, "no_speech_prob": 2.4824580577842426e-06}, {"id": 1150, "seek": 556452, "start": 5571.26, "end": 5575.52, "text": " It was nothing to do with the taser the fact that they died immediately afterwards was totally", "tokens": [467, 390, 1825, 281, 360, 365, 264, 8023, 260, 264, 1186, 300, 436, 4539, 4258, 10543, 390, 3879], "temperature": 0.0, "avg_logprob": -0.18818341423483456, "compression_ratio": 1.6059322033898304, "no_speech_prob": 2.4824580577842426e-06}, {"id": 1151, "seek": 556452, "start": 5576.4800000000005, "end": 5580.400000000001, "text": " Unrelated it was just a random you know things things happen", "tokens": [1156, 12004, 309, 390, 445, 257, 4974, 291, 458, 721, 721, 1051], "temperature": 0.0, "avg_logprob": -0.18818341423483456, "compression_ratio": 1.6059322033898304, "no_speech_prob": 2.4824580577842426e-06}, {"id": 1152, "seek": 556452, "start": 5581.160000000001, "end": 5585.84, "text": " So this company now owns 80% of the market for body cameras", "tokens": [407, 341, 2237, 586, 19143, 4688, 4, 295, 264, 2142, 337, 1772, 8622], "temperature": 0.0, "avg_logprob": -0.18818341423483456, "compression_ratio": 1.6059322033898304, "no_speech_prob": 2.4824580577842426e-06}, {"id": 1153, "seek": 556452, "start": 5587.200000000001, "end": 5590.660000000001, "text": " And they started buying computer vision AI companies", "tokens": [400, 436, 1409, 6382, 3820, 5201, 7318, 3431], "temperature": 0.0, "avg_logprob": -0.18818341423483456, "compression_ratio": 1.6059322033898304, "no_speech_prob": 2.4824580577842426e-06}, {"id": 1154, "seek": 559066, "start": 5590.66, "end": 5597.18, "text": " And they're going to try and now use these police body camera videos to anticipate criminal activity", "tokens": [400, 436, 434, 516, 281, 853, 293, 586, 764, 613, 3804, 1772, 2799, 2145, 281, 21685, 8628, 5191], "temperature": 0.0, "avg_logprob": -0.2020016160122184, "compression_ratio": 1.6523809523809523, "no_speech_prob": 9.874612487692502e-07}, {"id": 1155, "seek": 559066, "start": 5598.18, "end": 5600.0199999999995, "text": " right and so like", "tokens": [558, 293, 370, 411], "temperature": 0.0, "avg_logprob": -0.2020016160122184, "compression_ratio": 1.6523809523809523, "no_speech_prob": 9.874612487692502e-07}, {"id": 1156, "seek": 559066, "start": 5600.0199999999995, "end": 5606.18, "text": " What does that mean right so is that like okay? I now have some augmented reality display saying like", "tokens": [708, 775, 300, 914, 558, 370, 307, 300, 411, 1392, 30, 286, 586, 362, 512, 36155, 4103, 4674, 1566, 411], "temperature": 0.0, "avg_logprob": -0.2020016160122184, "compression_ratio": 1.6523809523809523, "no_speech_prob": 9.874612487692502e-07}, {"id": 1157, "seek": 559066, "start": 5606.7, "end": 5608.7, "text": " Tase this person", "tokens": [314, 651, 341, 954], "temperature": 0.0, "avg_logprob": -0.2020016160122184, "compression_ratio": 1.6523809523809523, "no_speech_prob": 9.874612487692502e-07}, {"id": 1158, "seek": 559066, "start": 5609.139999999999, "end": 5611.139999999999, "text": " Because they're about to do something bad", "tokens": [1436, 436, 434, 466, 281, 360, 746, 1578], "temperature": 0.0, "avg_logprob": -0.2020016160122184, "compression_ratio": 1.6523809523809523, "no_speech_prob": 9.874612487692502e-07}, {"id": 1159, "seek": 559066, "start": 5612.0599999999995, "end": 5614.38, "text": " you know so it's like it's kind of like a", "tokens": [291, 458, 370, 309, 311, 411, 309, 311, 733, 295, 411, 257], "temperature": 0.0, "avg_logprob": -0.2020016160122184, "compression_ratio": 1.6523809523809523, "no_speech_prob": 9.874612487692502e-07}, {"id": 1160, "seek": 559066, "start": 5615.58, "end": 5617.58, "text": " worrying direction and so", "tokens": [18788, 3513, 293, 370], "temperature": 0.0, "avg_logprob": -0.2020016160122184, "compression_ratio": 1.6523809523809523, "no_speech_prob": 9.874612487692502e-07}, {"id": 1161, "seek": 561758, "start": 5617.58, "end": 5624.18, "text": " You know I'm sure nobody who's a data scientist at taser or at the companies that they bought out is thinking like", "tokens": [509, 458, 286, 478, 988, 5079, 567, 311, 257, 1412, 12662, 412, 8023, 260, 420, 412, 264, 3431, 300, 436, 4243, 484, 307, 1953, 411], "temperature": 0.0, "avg_logprob": -0.16204902806232885, "compression_ratio": 1.6962025316455696, "no_speech_prob": 2.5215592813765397e-06}, {"id": 1162, "seek": 561758, "start": 5624.82, "end": 5626.82, "text": " You know this is the world I want to help create", "tokens": [509, 458, 341, 307, 264, 1002, 286, 528, 281, 854, 1884], "temperature": 0.0, "avg_logprob": -0.16204902806232885, "compression_ratio": 1.6962025316455696, "no_speech_prob": 2.5215592813765397e-06}, {"id": 1163, "seek": 561758, "start": 5628.5, "end": 5633.74, "text": " But they could find themselves in you know or you could find yourself in the middle of this kind of discussion", "tokens": [583, 436, 727, 915, 2969, 294, 291, 458, 420, 291, 727, 915, 1803, 294, 264, 2808, 295, 341, 733, 295, 5017], "temperature": 0.0, "avg_logprob": -0.16204902806232885, "compression_ratio": 1.6962025316455696, "no_speech_prob": 2.5215592813765397e-06}, {"id": 1164, "seek": 561758, "start": 5634.14, "end": 5638.54, "text": " Where it's not explicitly about that topic, but there's part of you that says like", "tokens": [2305, 309, 311, 406, 20803, 466, 300, 4829, 11, 457, 456, 311, 644, 295, 291, 300, 1619, 411], "temperature": 0.0, "avg_logprob": -0.16204902806232885, "compression_ratio": 1.6962025316455696, "no_speech_prob": 2.5215592813765397e-06}, {"id": 1165, "seek": 561758, "start": 5639.74, "end": 5641.82, "text": " Now wonder if this is how this could be used", "tokens": [823, 2441, 498, 341, 307, 577, 341, 727, 312, 1143], "temperature": 0.0, "avg_logprob": -0.16204902806232885, "compression_ratio": 1.6962025316455696, "no_speech_prob": 2.5215592813765397e-06}, {"id": 1166, "seek": 564182, "start": 5641.82, "end": 5647.82, "text": " All right, and and you know I don't know exactly what the right thing to do in that situation is because like you can ask", "tokens": [1057, 558, 11, 293, 293, 291, 458, 286, 500, 380, 458, 2293, 437, 264, 558, 551, 281, 360, 294, 300, 2590, 307, 570, 411, 291, 393, 1029], "temperature": 0.0, "avg_logprob": -0.16211058412279403, "compression_ratio": 1.7951807228915662, "no_speech_prob": 1.5779505702084862e-06}, {"id": 1167, "seek": 564182, "start": 5647.82, "end": 5650.139999999999, "text": " And of course people are going to be like no no no no", "tokens": [400, 295, 1164, 561, 366, 516, 281, 312, 411, 572, 572, 572, 572], "temperature": 0.0, "avg_logprob": -0.16211058412279403, "compression_ratio": 1.7951807228915662, "no_speech_prob": 1.5779505702084862e-06}, {"id": 1168, "seek": 564182, "start": 5652.34, "end": 5656.66, "text": " So it's like you know are you gonna you know what what could you do?", "tokens": [407, 309, 311, 411, 291, 458, 366, 291, 799, 291, 458, 437, 437, 727, 291, 360, 30], "temperature": 0.0, "avg_logprob": -0.16211058412279403, "compression_ratio": 1.7951807228915662, "no_speech_prob": 1.5779505702084862e-06}, {"id": 1169, "seek": 564182, "start": 5656.66, "end": 5663.24, "text": " No, you could like ask for some kind of written promise you could decide to leave", "tokens": [883, 11, 291, 727, 411, 1029, 337, 512, 733, 295, 3720, 6228, 291, 727, 4536, 281, 1856], "temperature": 0.0, "avg_logprob": -0.16211058412279403, "compression_ratio": 1.7951807228915662, "no_speech_prob": 1.5779505702084862e-06}, {"id": 1170, "seek": 564182, "start": 5663.7, "end": 5670.34, "text": " You could you know start doing some research into the legality of things to say like oh, I would at least protect my own", "tokens": [509, 727, 291, 458, 722, 884, 512, 2132, 666, 264, 1676, 1860, 295, 721, 281, 584, 411, 1954, 11, 286, 576, 412, 1935, 2371, 452, 1065], "temperature": 0.0, "avg_logprob": -0.16211058412279403, "compression_ratio": 1.7951807228915662, "no_speech_prob": 1.5779505702084862e-06}, {"id": 1171, "seek": 567034, "start": 5670.34, "end": 5676.02, "text": " You know legal situation. I don't know like have a think about how you would respond to that", "tokens": [509, 458, 5089, 2590, 13, 286, 500, 380, 458, 411, 362, 257, 519, 466, 577, 291, 576, 4196, 281, 300], "temperature": 0.0, "avg_logprob": -0.17630138803035655, "compression_ratio": 1.708695652173913, "no_speech_prob": 2.0580398540914757e-06}, {"id": 1172, "seek": 567034, "start": 5679.42, "end": 5684.9400000000005, "text": " So these are some questions that Rachel created as being things to think about right", "tokens": [407, 613, 366, 512, 1651, 300, 14246, 2942, 382, 885, 721, 281, 519, 466, 558], "temperature": 0.0, "avg_logprob": -0.17630138803035655, "compression_ratio": 1.708695652173913, "no_speech_prob": 2.0580398540914757e-06}, {"id": 1173, "seek": 567034, "start": 5685.06, "end": 5691.900000000001, "text": " So if you're looking at building a data product or you know using a model like if you're building a machine learning model", "tokens": [407, 498, 291, 434, 1237, 412, 2390, 257, 1412, 1674, 420, 291, 458, 1228, 257, 2316, 411, 498, 291, 434, 2390, 257, 3479, 2539, 2316], "temperature": 0.0, "avg_logprob": -0.17630138803035655, "compression_ratio": 1.708695652173913, "no_speech_prob": 2.0580398540914757e-06}, {"id": 1174, "seek": 567034, "start": 5691.900000000001, "end": 5693.26, "text": " It's for a reason", "tokens": [467, 311, 337, 257, 1778], "temperature": 0.0, "avg_logprob": -0.17630138803035655, "compression_ratio": 1.708695652173913, "no_speech_prob": 2.0580398540914757e-06}, {"id": 1175, "seek": 567034, "start": 5693.26, "end": 5698.38, "text": " Okay, you're trying to do something right so what bias may be in that data", "tokens": [1033, 11, 291, 434, 1382, 281, 360, 746, 558, 370, 437, 12577, 815, 312, 294, 300, 1412], "temperature": 0.0, "avg_logprob": -0.17630138803035655, "compression_ratio": 1.708695652173913, "no_speech_prob": 2.0580398540914757e-06}, {"id": 1176, "seek": 569838, "start": 5698.38, "end": 5702.46, "text": " All right because whatever bias is in that data ends up being a bias in your predictions", "tokens": [1057, 558, 570, 2035, 12577, 307, 294, 300, 1412, 5314, 493, 885, 257, 12577, 294, 428, 21264], "temperature": 0.0, "avg_logprob": -0.19387731880977235, "compression_ratio": 1.7012987012987013, "no_speech_prob": 1.8448153014105628e-06}, {"id": 1177, "seek": 569838, "start": 5702.9400000000005, "end": 5705.62, "text": " Potentially then biases the actions that you're influencing", "tokens": [9145, 3137, 550, 32152, 264, 5909, 300, 291, 434, 40396], "temperature": 0.0, "avg_logprob": -0.19387731880977235, "compression_ratio": 1.7012987012987013, "no_speech_prob": 1.8448153014105628e-06}, {"id": 1178, "seek": 569838, "start": 5705.9400000000005, "end": 5712.18, "text": " Potentially then biases the data that you come back and you may create a feedback loop if the team that build it isn't diverse", "tokens": [9145, 3137, 550, 32152, 264, 1412, 300, 291, 808, 646, 293, 291, 815, 1884, 257, 5824, 6367, 498, 264, 1469, 300, 1322, 309, 1943, 380, 9521], "temperature": 0.0, "avg_logprob": -0.19387731880977235, "compression_ratio": 1.7012987012987013, "no_speech_prob": 1.8448153014105628e-06}, {"id": 1179, "seek": 569838, "start": 5712.7, "end": 5716.66, "text": " You know what might you be missing right so for example?", "tokens": [509, 458, 437, 1062, 291, 312, 5361, 558, 370, 337, 1365, 30], "temperature": 0.0, "avg_logprob": -0.19387731880977235, "compression_ratio": 1.7012987012987013, "no_speech_prob": 1.8448153014105628e-06}, {"id": 1180, "seek": 569838, "start": 5719.78, "end": 5725.66, "text": " One senior executive at Twitter called the alarm about major", "tokens": [1485, 7965, 10140, 412, 5794, 1219, 264, 14183, 466, 2563], "temperature": 0.0, "avg_logprob": -0.19387731880977235, "compression_ratio": 1.7012987012987013, "no_speech_prob": 1.8448153014105628e-06}, {"id": 1181, "seek": 572566, "start": 5725.66, "end": 5727.139999999999, "text": " major", "tokens": [2563], "temperature": 0.0, "avg_logprob": -0.21654178847127886, "compression_ratio": 1.5823529411764705, "no_speech_prob": 3.5208955750931636e-07}, {"id": 1182, "seek": 572566, "start": 5727.139999999999, "end": 5729.34, "text": " Russian bot problems at Twitter", "tokens": [7220, 10592, 2740, 412, 5794], "temperature": 0.0, "avg_logprob": -0.21654178847127886, "compression_ratio": 1.5823529411764705, "no_speech_prob": 3.5208955750931636e-07}, {"id": 1183, "seek": 572566, "start": 5730.22, "end": 5732.22, "text": " Way back well before the election", "tokens": [9558, 646, 731, 949, 264, 6618], "temperature": 0.0, "avg_logprob": -0.21654178847127886, "compression_ratio": 1.5823529411764705, "no_speech_prob": 3.5208955750931636e-07}, {"id": 1184, "seek": 572566, "start": 5733.78, "end": 5735.78, "text": " That was the one", "tokens": [663, 390, 264, 472], "temperature": 0.0, "avg_logprob": -0.21654178847127886, "compression_ratio": 1.5823529411764705, "no_speech_prob": 3.5208955750931636e-07}, {"id": 1185, "seek": 572566, "start": 5738.0199999999995, "end": 5741.94, "text": " Black person in the exec team at Twitter the one and", "tokens": [4076, 954, 294, 264, 4454, 1469, 412, 5794, 264, 472, 293], "temperature": 0.0, "avg_logprob": -0.21654178847127886, "compression_ratio": 1.5823529411764705, "no_speech_prob": 3.5208955750931636e-07}, {"id": 1186, "seek": 572566, "start": 5742.86, "end": 5744.86, "text": " Shortly afterwards they lost their job", "tokens": [40109, 10543, 436, 2731, 641, 1691], "temperature": 0.0, "avg_logprob": -0.21654178847127886, "compression_ratio": 1.5823529411764705, "no_speech_prob": 3.5208955750931636e-07}, {"id": 1187, "seek": 572566, "start": 5744.9, "end": 5746.9, "text": " right and so like", "tokens": [558, 293, 370, 411], "temperature": 0.0, "avg_logprob": -0.21654178847127886, "compression_ratio": 1.5823529411764705, "no_speech_prob": 3.5208955750931636e-07}, {"id": 1188, "seek": 572566, "start": 5747.54, "end": 5750.9, "text": " Definitely having a more diverse team that means having a more diverse", "tokens": [12151, 1419, 257, 544, 9521, 1469, 300, 1355, 1419, 257, 544, 9521], "temperature": 0.0, "avg_logprob": -0.21654178847127886, "compression_ratio": 1.5823529411764705, "no_speech_prob": 3.5208955750931636e-07}, {"id": 1189, "seek": 575090, "start": 5750.9, "end": 5757.179999999999, "text": " set of opinions and beliefs and ideas and things to look for and so forth so", "tokens": [992, 295, 11819, 293, 13585, 293, 3487, 293, 721, 281, 574, 337, 293, 370, 5220, 370], "temperature": 0.0, "avg_logprob": -0.20095033645629884, "compression_ratio": 1.6113207547169812, "no_speech_prob": 1.8448075707055978e-06}, {"id": 1190, "seek": 575090, "start": 5758.0199999999995, "end": 5761.7, "text": " Non-diverse teams seem to make more of these bad mistakes", "tokens": [8774, 12, 67, 5376, 5491, 1643, 281, 652, 544, 295, 613, 1578, 8038], "temperature": 0.0, "avg_logprob": -0.20095033645629884, "compression_ratio": 1.6113207547169812, "no_speech_prob": 1.8448075707055978e-06}, {"id": 1191, "seek": 575090, "start": 5764.219999999999, "end": 5769.339999999999, "text": " Can we order the code is it open source check for the different error rates amongst different groups?", "tokens": [1664, 321, 1668, 264, 3089, 307, 309, 1269, 4009, 1520, 337, 264, 819, 6713, 6846, 12918, 819, 3935, 30], "temperature": 0.0, "avg_logprob": -0.20095033645629884, "compression_ratio": 1.6113207547169812, "no_speech_prob": 1.8448075707055978e-06}, {"id": 1192, "seek": 575090, "start": 5770.339999999999, "end": 5775.219999999999, "text": " Is there like a simple rule we could use instead? That's like extremely interpretable and easy to communicate", "tokens": [1119, 456, 411, 257, 2199, 4978, 321, 727, 764, 2602, 30, 663, 311, 411, 4664, 7302, 712, 293, 1858, 281, 7890], "temperature": 0.0, "avg_logprob": -0.20095033645629884, "compression_ratio": 1.6113207547169812, "no_speech_prob": 1.8448075707055978e-06}, {"id": 1193, "seek": 575090, "start": 5776.259999999999, "end": 5780.379999999999, "text": " And like you know if something goes wrong do we have a good way to deal with it?", "tokens": [400, 411, 291, 458, 498, 746, 1709, 2085, 360, 321, 362, 257, 665, 636, 281, 2028, 365, 309, 30], "temperature": 0.0, "avg_logprob": -0.20095033645629884, "compression_ratio": 1.6113207547169812, "no_speech_prob": 1.8448075707055978e-06}, {"id": 1194, "seek": 578038, "start": 5780.38, "end": 5781.62, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.22685453166132388, "compression_ratio": 1.6418604651162791, "no_speech_prob": 2.2603003344556782e-06}, {"id": 1195, "seek": 578038, "start": 5781.62, "end": 5783.62, "text": " so when", "tokens": [370, 562], "temperature": 0.0, "avg_logprob": -0.22685453166132388, "compression_ratio": 1.6418604651162791, "no_speech_prob": 2.2603003344556782e-06}, {"id": 1196, "seek": 578038, "start": 5785.18, "end": 5790.3, "text": " When we've talked to people about this and a lot of people like have come to Rachel and said like I'm", "tokens": [1133, 321, 600, 2825, 281, 561, 466, 341, 293, 257, 688, 295, 561, 411, 362, 808, 281, 14246, 293, 848, 411, 286, 478], "temperature": 0.0, "avg_logprob": -0.22685453166132388, "compression_ratio": 1.6418604651162791, "no_speech_prob": 2.2603003344556782e-06}, {"id": 1197, "seek": 578038, "start": 5791.06, "end": 5794.86, "text": " I'm concerned about something my organization's doing you know what do I do?", "tokens": [286, 478, 5922, 466, 746, 452, 4475, 311, 884, 291, 458, 437, 360, 286, 360, 30], "temperature": 0.0, "avg_logprob": -0.22685453166132388, "compression_ratio": 1.6418604651162791, "no_speech_prob": 2.2603003344556782e-06}, {"id": 1198, "seek": 578038, "start": 5795.74, "end": 5800.42, "text": " Or I'm just concerned about my toxic workplace. What do I do?", "tokens": [1610, 286, 478, 445, 5922, 466, 452, 12786, 15328, 13, 708, 360, 286, 360, 30], "temperature": 0.0, "avg_logprob": -0.22685453166132388, "compression_ratio": 1.6418604651162791, "no_speech_prob": 2.2603003344556782e-06}, {"id": 1199, "seek": 578038, "start": 5801.22, "end": 5803.22, "text": " and very often", "tokens": [293, 588, 2049], "temperature": 0.0, "avg_logprob": -0.22685453166132388, "compression_ratio": 1.6418604651162791, "no_speech_prob": 2.2603003344556782e-06}, {"id": 1200, "seek": 578038, "start": 5803.58, "end": 5808.62, "text": " You know Rachel will say like well have you considered leaving and they will say oh", "tokens": [509, 458, 14246, 486, 584, 411, 731, 362, 291, 4888, 5012, 293, 436, 486, 584, 1954], "temperature": 0.0, "avg_logprob": -0.22685453166132388, "compression_ratio": 1.6418604651162791, "no_speech_prob": 2.2603003344556782e-06}, {"id": 1201, "seek": 580862, "start": 5808.62, "end": 5810.7, "text": " I I don't want to lose my job", "tokens": [286, 286, 500, 380, 528, 281, 3624, 452, 1691], "temperature": 0.0, "avg_logprob": -0.1937078865625525, "compression_ratio": 1.7629310344827587, "no_speech_prob": 4.812499696527084e-07}, {"id": 1202, "seek": 580862, "start": 5811.46, "end": 5818.98, "text": " All right, but actually if you can code you're in like point three percent of the population if you can code and do machine learning", "tokens": [1057, 558, 11, 457, 767, 498, 291, 393, 3089, 291, 434, 294, 411, 935, 1045, 3043, 295, 264, 4415, 498, 291, 393, 3089, 293, 360, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.1937078865625525, "compression_ratio": 1.7629310344827587, "no_speech_prob": 4.812499696527084e-07}, {"id": 1203, "seek": 580862, "start": 5819.94, "end": 5823.7, "text": " You're in probably like point oh one percent of the population you are massively", "tokens": [509, 434, 294, 1391, 411, 935, 1954, 472, 3043, 295, 264, 4415, 291, 366, 29379], "temperature": 0.0, "avg_logprob": -0.1937078865625525, "compression_ratio": 1.7629310344827587, "no_speech_prob": 4.812499696527084e-07}, {"id": 1204, "seek": 580862, "start": 5824.5, "end": 5826.5, "text": " massively in demand", "tokens": [29379, 294, 4733], "temperature": 0.0, "avg_logprob": -0.1937078865625525, "compression_ratio": 1.7629310344827587, "no_speech_prob": 4.812499696527084e-07}, {"id": 1205, "seek": 580862, "start": 5828.82, "end": 5836.7, "text": " So like realistically you know obviously it's an organization does not want you to feel like you're somebody who could just leave and get another", "tokens": [407, 411, 40734, 291, 458, 2745, 309, 311, 364, 4475, 775, 406, 528, 291, 281, 841, 411, 291, 434, 2618, 567, 727, 445, 1856, 293, 483, 1071], "temperature": 0.0, "avg_logprob": -0.1937078865625525, "compression_ratio": 1.7629310344827587, "no_speech_prob": 4.812499696527084e-07}, {"id": 1206, "seek": 583670, "start": 5836.7, "end": 5843.3, "text": " Job that's not in your interest in their interest, but that is absolutely true right and so one of the things", "tokens": [18602, 300, 311, 406, 294, 428, 1179, 294, 641, 1179, 11, 457, 300, 307, 3122, 2074, 558, 293, 370, 472, 295, 264, 721], "temperature": 0.0, "avg_logprob": -0.1809772195167912, "compression_ratio": 1.7237354085603114, "no_speech_prob": 7.571132414341264e-07}, {"id": 1207, "seek": 583670, "start": 5843.3, "end": 5847.139999999999, "text": " I hope you leave this course with is is enough self-confidence", "tokens": [286, 1454, 291, 1856, 341, 1164, 365, 307, 307, 1547, 2698, 12, 47273], "temperature": 0.0, "avg_logprob": -0.1809772195167912, "compression_ratio": 1.7237354085603114, "no_speech_prob": 7.571132414341264e-07}, {"id": 1208, "seek": 583670, "start": 5848.34, "end": 5854.46, "text": " To recognize that you have the skills you know to get to get a job and particularly", "tokens": [1407, 5521, 300, 291, 362, 264, 3942, 291, 458, 281, 483, 281, 483, 257, 1691, 293, 4098], "temperature": 0.0, "avg_logprob": -0.1809772195167912, "compression_ratio": 1.7237354085603114, "no_speech_prob": 7.571132414341264e-07}, {"id": 1209, "seek": 583670, "start": 5855.0199999999995, "end": 5861.62, "text": " Once you've got your first job your second job is an order of magnitude easier right and so you know this is important", "tokens": [3443, 291, 600, 658, 428, 700, 1691, 428, 1150, 1691, 307, 364, 1668, 295, 15668, 3571, 558, 293, 370, 291, 458, 341, 307, 1021], "temperature": 0.0, "avg_logprob": -0.1809772195167912, "compression_ratio": 1.7237354085603114, "no_speech_prob": 7.571132414341264e-07}, {"id": 1210, "seek": 583670, "start": 5861.62, "end": 5864.9, "text": " Not just so that you feel like you actually have the ability to act", "tokens": [1726, 445, 370, 300, 291, 841, 411, 291, 767, 362, 264, 3485, 281, 605], "temperature": 0.0, "avg_logprob": -0.1809772195167912, "compression_ratio": 1.7237354085603114, "no_speech_prob": 7.571132414341264e-07}, {"id": 1211, "seek": 586490, "start": 5864.9, "end": 5866.82, "text": " ethically", "tokens": [6468, 984], "temperature": 0.0, "avg_logprob": -0.2099976395115708, "compression_ratio": 1.7242798353909465, "no_speech_prob": 4.029433057439746e-06}, {"id": 1212, "seek": 586490, "start": 5866.82, "end": 5874.98, "text": " But it's also important to realize like if you find yourself in a toxic environment right which is which is pretty damn common", "tokens": [583, 309, 311, 611, 1021, 281, 4325, 411, 498, 291, 915, 1803, 294, 257, 12786, 2823, 558, 597, 307, 597, 307, 1238, 8151, 2689], "temperature": 0.0, "avg_logprob": -0.2099976395115708, "compression_ratio": 1.7242798353909465, "no_speech_prob": 4.029433057439746e-06}, {"id": 1213, "seek": 586490, "start": 5875.46, "end": 5877.46, "text": " Unfortunately like there's a lot of", "tokens": [8590, 411, 456, 311, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.2099976395115708, "compression_ratio": 1.7242798353909465, "no_speech_prob": 4.029433057439746e-06}, {"id": 1214, "seek": 586490, "start": 5877.54, "end": 5878.78, "text": " shitty", "tokens": [30748], "temperature": 0.0, "avg_logprob": -0.2099976395115708, "compression_ratio": 1.7242798353909465, "no_speech_prob": 4.029433057439746e-06}, {"id": 1215, "seek": 586490, "start": 5878.78, "end": 5884.379999999999, "text": " Tech cultures environments particularly in the Bay Area, but if you find yourself in one of those environments", "tokens": [13795, 12951, 12388, 4098, 294, 264, 7840, 19405, 11, 457, 498, 291, 915, 1803, 294, 472, 295, 729, 12388], "temperature": 0.0, "avg_logprob": -0.2099976395115708, "compression_ratio": 1.7242798353909465, "no_speech_prob": 4.029433057439746e-06}, {"id": 1216, "seek": 586490, "start": 5886.179999999999, "end": 5890.299999999999, "text": " The best thing to do is to get the hell out right and and if you don't", "tokens": [440, 1151, 551, 281, 360, 307, 281, 483, 264, 4921, 484, 558, 293, 293, 498, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.2099976395115708, "compression_ratio": 1.7242798353909465, "no_speech_prob": 4.029433057439746e-06}, {"id": 1217, "seek": 586490, "start": 5891.139999999999, "end": 5894.0199999999995, "text": " Have the self-confidence to think you can get another job", "tokens": [3560, 264, 2698, 12, 47273, 281, 519, 291, 393, 483, 1071, 1691], "temperature": 0.0, "avg_logprob": -0.2099976395115708, "compression_ratio": 1.7242798353909465, "no_speech_prob": 4.029433057439746e-06}, {"id": 1218, "seek": 589402, "start": 5894.02, "end": 5896.02, "text": " You can get trapped", "tokens": [509, 393, 483, 14994], "temperature": 0.0, "avg_logprob": -0.2261652090610602, "compression_ratio": 1.6084656084656084, "no_speech_prob": 2.4060846044449136e-06}, {"id": 1219, "seek": 589402, "start": 5896.780000000001, "end": 5898.780000000001, "text": " right, so you know", "tokens": [558, 11, 370, 291, 458], "temperature": 0.0, "avg_logprob": -0.2261652090610602, "compression_ratio": 1.6084656084656084, "no_speech_prob": 2.4060846044449136e-06}, {"id": 1220, "seek": 589402, "start": 5899.740000000001, "end": 5905.820000000001, "text": " It's really important really important to know that you are leaving this program with varying demand skills and", "tokens": [467, 311, 534, 1021, 534, 1021, 281, 458, 300, 291, 366, 5012, 341, 1461, 365, 22984, 4733, 3942, 293], "temperature": 0.0, "avg_logprob": -0.2261652090610602, "compression_ratio": 1.6084656084656084, "no_speech_prob": 2.4060846044449136e-06}, {"id": 1221, "seek": 589402, "start": 5906.38, "end": 5910.3, "text": " particularly after you have that first job you're now somebody with in-demand skills and", "tokens": [4098, 934, 291, 362, 300, 700, 1691, 291, 434, 586, 2618, 365, 294, 12, 10730, 474, 3942, 293], "temperature": 0.0, "avg_logprob": -0.2261652090610602, "compression_ratio": 1.6084656084656084, "no_speech_prob": 2.4060846044449136e-06}, {"id": 1222, "seek": 589402, "start": 5911.22, "end": 5913.820000000001, "text": " Attract record of being employed in that area", "tokens": [7298, 1897, 2136, 295, 885, 20115, 294, 300, 1859], "temperature": 0.0, "avg_logprob": -0.2261652090610602, "compression_ratio": 1.6084656084656084, "no_speech_prob": 2.4060846044449136e-06}, {"id": 1223, "seek": 589402, "start": 5916.580000000001, "end": 5918.580000000001, "text": " Okay, great", "tokens": [1033, 11, 869], "temperature": 0.0, "avg_logprob": -0.2261652090610602, "compression_ratio": 1.6084656084656084, "no_speech_prob": 2.4060846044449136e-06}, {"id": 1224, "seek": 589402, "start": 5918.580000000001, "end": 5919.700000000001, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.2261652090610602, "compression_ratio": 1.6084656084656084, "no_speech_prob": 2.4060846044449136e-06}, {"id": 1225, "seek": 589402, "start": 5919.700000000001, "end": 5921.540000000001, "text": " Yes", "tokens": [1079], "temperature": 0.0, "avg_logprob": -0.2261652090610602, "compression_ratio": 1.6084656084656084, "no_speech_prob": 2.4060846044449136e-06}, {"id": 1226, "seek": 592154, "start": 5921.54, "end": 5929.34, "text": " This is kind of just a broad question, but what are some things that you know of that people are doing to treat bias in data?", "tokens": [639, 307, 733, 295, 445, 257, 4152, 1168, 11, 457, 437, 366, 512, 721, 300, 291, 458, 295, 300, 561, 366, 884, 281, 2387, 12577, 294, 1412, 30], "temperature": 0.0, "avg_logprob": -0.13154346774322817, "compression_ratio": 1.7725321888412018, "no_speech_prob": 9.36846663535107e-06}, {"id": 1227, "seek": 592154, "start": 5931.62, "end": 5935.62, "text": " You know it's kind of like a bit of a controversial subject at the moment and", "tokens": [509, 458, 309, 311, 733, 295, 411, 257, 857, 295, 257, 17323, 3983, 412, 264, 1623, 293], "temperature": 0.0, "avg_logprob": -0.13154346774322817, "compression_ratio": 1.7725321888412018, "no_speech_prob": 9.36846663535107e-06}, {"id": 1228, "seek": 592154, "start": 5936.82, "end": 5941.22, "text": " There are there are like people are trying to use some people trying to use an algorithmic approach", "tokens": [821, 366, 456, 366, 411, 561, 366, 1382, 281, 764, 512, 561, 1382, 281, 764, 364, 9284, 299, 3109], "temperature": 0.0, "avg_logprob": -0.13154346774322817, "compression_ratio": 1.7725321888412018, "no_speech_prob": 9.36846663535107e-06}, {"id": 1229, "seek": 592154, "start": 5941.58, "end": 5943.58, "text": " You know where they're basically trying to say", "tokens": [509, 458, 689, 436, 434, 1936, 1382, 281, 584], "temperature": 0.0, "avg_logprob": -0.13154346774322817, "compression_ratio": 1.7725321888412018, "no_speech_prob": 9.36846663535107e-06}, {"id": 1230, "seek": 592154, "start": 5944.18, "end": 5947.18, "text": " How can we identify the bias and kind of like subtract it out?", "tokens": [1012, 393, 321, 5876, 264, 12577, 293, 733, 295, 411, 16390, 309, 484, 30], "temperature": 0.0, "avg_logprob": -0.13154346774322817, "compression_ratio": 1.7725321888412018, "no_speech_prob": 9.36846663535107e-06}, {"id": 1231, "seek": 594718, "start": 5947.18, "end": 5951.18, "text": " But like the most effective ways", "tokens": [583, 411, 264, 881, 4942, 2098], "temperature": 0.0, "avg_logprob": -0.2430839789541144, "compression_ratio": 1.6177777777777778, "no_speech_prob": 2.0904426492052153e-06}, {"id": 1232, "seek": 594718, "start": 5951.18, "end": 5956.26, "text": " I know of a one sort of trying to treat it at the data level so like start with a", "tokens": [286, 458, 295, 257, 472, 1333, 295, 1382, 281, 2387, 309, 412, 264, 1412, 1496, 370, 411, 722, 365, 257], "temperature": 0.0, "avg_logprob": -0.2430839789541144, "compression_ratio": 1.6177777777777778, "no_speech_prob": 2.0904426492052153e-06}, {"id": 1233, "seek": 594718, "start": 5956.700000000001, "end": 5962.58, "text": " More diverse team particularly a team involved you know which includes people from the humanities like sociologists", "tokens": [5048, 9521, 1469, 4098, 257, 1469, 3288, 291, 458, 597, 5974, 561, 490, 264, 36140, 411, 3075, 12256], "temperature": 0.0, "avg_logprob": -0.2430839789541144, "compression_ratio": 1.6177777777777778, "no_speech_prob": 2.0904426492052153e-06}, {"id": 1234, "seek": 594718, "start": 5963.26, "end": 5965.38, "text": " psychologists economists people that understand", "tokens": [41562, 32431, 561, 300, 1223], "temperature": 0.0, "avg_logprob": -0.2430839789541144, "compression_ratio": 1.6177777777777778, "no_speech_prob": 2.0904426492052153e-06}, {"id": 1235, "seek": 594718, "start": 5965.9800000000005, "end": 5970.54, "text": " feedback loops and implications for human behavior, and they tend to be equipped with", "tokens": [5824, 16121, 293, 16602, 337, 1952, 5223, 11, 293, 436, 3928, 281, 312, 15218, 365], "temperature": 0.0, "avg_logprob": -0.2430839789541144, "compression_ratio": 1.6177777777777778, "no_speech_prob": 2.0904426492052153e-06}, {"id": 1236, "seek": 597054, "start": 5970.54, "end": 5977.46, "text": " You know good tools for kind of identifying and tracking these kinds of problems", "tokens": [509, 458, 665, 3873, 337, 733, 295, 16696, 293, 11603, 613, 3685, 295, 2740], "temperature": 0.0, "avg_logprob": -0.13031529856252147, "compression_ratio": 1.7009345794392523, "no_speech_prob": 2.9022880880802404e-06}, {"id": 1237, "seek": 597054, "start": 5977.46, "end": 5981.58, "text": " And so and then kind of trying to incorporate the solutions into the process itself", "tokens": [400, 370, 293, 550, 733, 295, 1382, 281, 16091, 264, 6547, 666, 264, 1399, 2564], "temperature": 0.0, "avg_logprob": -0.13031529856252147, "compression_ratio": 1.7009345794392523, "no_speech_prob": 2.9022880880802404e-06}, {"id": 1238, "seek": 597054, "start": 5982.74, "end": 5984.74, "text": " Let's say there isn't kind of like a", "tokens": [961, 311, 584, 456, 1943, 380, 733, 295, 411, 257], "temperature": 0.0, "avg_logprob": -0.13031529856252147, "compression_ratio": 1.7009345794392523, "no_speech_prob": 2.9022880880802404e-06}, {"id": 1239, "seek": 597054, "start": 5985.54, "end": 5987.06, "text": " you know", "tokens": [291, 458], "temperature": 0.0, "avg_logprob": -0.13031529856252147, "compression_ratio": 1.7009345794392523, "no_speech_prob": 2.9022880880802404e-06}, {"id": 1240, "seek": 597054, "start": 5987.06, "end": 5990.86, "text": " Some standard process I can point you to and say here's how to solve it", "tokens": [2188, 3832, 1399, 286, 393, 935, 291, 281, 293, 584, 510, 311, 577, 281, 5039, 309], "temperature": 0.0, "avg_logprob": -0.13031529856252147, "compression_ratio": 1.7009345794392523, "no_speech_prob": 2.9022880880802404e-06}, {"id": 1241, "seek": 597054, "start": 5990.86, "end": 5995.98, "text": " You know if there is such a thing we haven't found it yet. You know it requires a", "tokens": [509, 458, 498, 456, 307, 1270, 257, 551, 321, 2378, 380, 1352, 309, 1939, 13, 509, 458, 309, 7029, 257], "temperature": 0.0, "avg_logprob": -0.13031529856252147, "compression_ratio": 1.7009345794392523, "no_speech_prob": 2.9022880880802404e-06}, {"id": 1242, "seek": 599598, "start": 5995.98, "end": 6001.94, "text": " Diverse team of smart people to be aware of the problems and work hard at them. It's the short answer", "tokens": [413, 5376, 1469, 295, 4069, 561, 281, 312, 3650, 295, 264, 2740, 293, 589, 1152, 412, 552, 13, 467, 311, 264, 2099, 1867], "temperature": 0.0, "avg_logprob": -0.1815793019420696, "compression_ratio": 1.5551330798479088, "no_speech_prob": 1.3006483641220257e-05}, {"id": 1243, "seek": 599598, "start": 6003.339999999999, "end": 6005.339999999999, "text": " Can you pass that back please?", "tokens": [1664, 291, 1320, 300, 646, 1767, 30], "temperature": 0.0, "avg_logprob": -0.1815793019420696, "compression_ratio": 1.5551330798479088, "no_speech_prob": 1.3006483641220257e-05}, {"id": 1244, "seek": 599598, "start": 6009.0599999999995, "end": 6011.54, "text": " This is just kind of a general thing I guess for the whole class", "tokens": [639, 307, 445, 733, 295, 257, 2674, 551, 286, 2041, 337, 264, 1379, 1508], "temperature": 0.0, "avg_logprob": -0.1815793019420696, "compression_ratio": 1.5551330798479088, "no_speech_prob": 1.3006483641220257e-05}, {"id": 1245, "seek": 599598, "start": 6012.139999999999, "end": 6018.24, "text": " If you're interested in this stuff that I read a pretty cool book Jeremy you've probably heard of it weapons of math destruction", "tokens": [759, 291, 434, 3102, 294, 341, 1507, 300, 286, 1401, 257, 1238, 1627, 1446, 17809, 291, 600, 1391, 2198, 295, 309, 7278, 295, 5221, 13563], "temperature": 0.0, "avg_logprob": -0.1815793019420696, "compression_ratio": 1.5551330798479088, "no_speech_prob": 1.3006483641220257e-05}, {"id": 1246, "seek": 599598, "start": 6018.78, "end": 6020.78, "text": " by Cathy O'Neill", "tokens": [538, 39799, 422, 6, 15496, 373], "temperature": 0.0, "avg_logprob": -0.1815793019420696, "compression_ratio": 1.5551330798479088, "no_speech_prob": 1.3006483641220257e-05}, {"id": 1247, "seek": 599598, "start": 6021.0199999999995, "end": 6024.86, "text": " Covers a lot of the same stuff. Yeah, just more more on the topic", "tokens": [3066, 840, 257, 688, 295, 264, 912, 1507, 13, 865, 11, 445, 544, 544, 322, 264, 4829], "temperature": 0.0, "avg_logprob": -0.1815793019420696, "compression_ratio": 1.5551330798479088, "no_speech_prob": 1.3006483641220257e-05}, {"id": 1248, "seek": 602486, "start": 6024.86, "end": 6029.54, "text": " Yeah, thanks the recommendation so Cathy's great. She's also got a TED talk I", "tokens": [865, 11, 3231, 264, 11879, 370, 39799, 311, 869, 13, 1240, 311, 611, 658, 257, 43036, 751, 286], "temperature": 0.0, "avg_logprob": -0.24299156024891844, "compression_ratio": 1.5235849056603774, "no_speech_prob": 1.184271604870446e-05}, {"id": 1249, "seek": 602486, "start": 6030.54, "end": 6036.139999999999, "text": " Didn't manage to finish the book because it's so damn depressing. I was just like yeah no more", "tokens": [11151, 380, 3067, 281, 2413, 264, 1446, 570, 309, 311, 370, 8151, 36355, 13, 286, 390, 445, 411, 1338, 572, 544], "temperature": 0.0, "avg_logprob": -0.24299156024891844, "compression_ratio": 1.5235849056603774, "no_speech_prob": 1.184271604870446e-05}, {"id": 1250, "seek": 602486, "start": 6037.179999999999, "end": 6040.58, "text": " But yeah, it's it's it's it's it's very good", "tokens": [583, 1338, 11, 309, 311, 309, 311, 309, 311, 309, 311, 309, 311, 588, 665], "temperature": 0.0, "avg_logprob": -0.24299156024891844, "compression_ratio": 1.5235849056603774, "no_speech_prob": 1.184271604870446e-05}, {"id": 1251, "seek": 602486, "start": 6041.54, "end": 6043.54, "text": " All right", "tokens": [1057, 558], "temperature": 0.0, "avg_logprob": -0.24299156024891844, "compression_ratio": 1.5235849056603774, "no_speech_prob": 1.184271604870446e-05}, {"id": 1252, "seek": 602486, "start": 6043.82, "end": 6045.82, "text": " Well, that's it", "tokens": [1042, 11, 300, 311, 309], "temperature": 0.0, "avg_logprob": -0.24299156024891844, "compression_ratio": 1.5235849056603774, "no_speech_prob": 1.184271604870446e-05}, {"id": 1253, "seek": 602486, "start": 6046.299999999999, "end": 6048.4, "text": " Thank you everybody. You know this has been", "tokens": [1044, 291, 2201, 13, 509, 458, 341, 575, 668], "temperature": 0.0, "avg_logprob": -0.24299156024891844, "compression_ratio": 1.5235849056603774, "no_speech_prob": 1.184271604870446e-05}, {"id": 1254, "seek": 602486, "start": 6050.219999999999, "end": 6052.46, "text": " This has been really intense for me", "tokens": [639, 575, 668, 534, 9447, 337, 385], "temperature": 0.0, "avg_logprob": -0.24299156024891844, "compression_ratio": 1.5235849056603774, "no_speech_prob": 1.184271604870446e-05}, {"id": 1255, "seek": 605246, "start": 6052.46, "end": 6057.3, "text": " You know obviously this was meant to be something that I was sharing with Rachel", "tokens": [509, 458, 2745, 341, 390, 4140, 281, 312, 746, 300, 286, 390, 5414, 365, 14246], "temperature": 0.0, "avg_logprob": -0.11772319862434456, "compression_ratio": 1.6988847583643123, "no_speech_prob": 1.723034802125767e-05}, {"id": 1256, "seek": 605246, "start": 6057.78, "end": 6061.62, "text": " So I've you know ended up doing one of the hardest things in my life", "tokens": [407, 286, 600, 291, 458, 4590, 493, 884, 472, 295, 264, 13158, 721, 294, 452, 993], "temperature": 0.0, "avg_logprob": -0.11772319862434456, "compression_ratio": 1.6988847583643123, "no_speech_prob": 1.723034802125767e-05}, {"id": 1257, "seek": 605246, "start": 6061.62, "end": 6069.22, "text": " Which is to teach two people's worth of course on my own and also look after a sick wife and have a toddler and also", "tokens": [3013, 307, 281, 2924, 732, 561, 311, 3163, 295, 1164, 322, 452, 1065, 293, 611, 574, 934, 257, 4998, 3836, 293, 362, 257, 44348, 293, 611], "temperature": 0.0, "avg_logprob": -0.11772319862434456, "compression_ratio": 1.6988847583643123, "no_speech_prob": 1.723034802125767e-05}, {"id": 1258, "seek": 605246, "start": 6069.58, "end": 6073.64, "text": " Do a deep learning course and also do all this with a new library that I just wrote", "tokens": [1144, 257, 2452, 2539, 1164, 293, 611, 360, 439, 341, 365, 257, 777, 6405, 300, 286, 445, 4114], "temperature": 0.0, "avg_logprob": -0.11772319862434456, "compression_ratio": 1.6988847583643123, "no_speech_prob": 1.723034802125767e-05}, {"id": 1259, "seek": 605246, "start": 6074.5, "end": 6081.02, "text": " So I'm looking forward to getting some sleep, but it's been it's been totally worth it because you've been", "tokens": [407, 286, 478, 1237, 2128, 281, 1242, 512, 2817, 11, 457, 309, 311, 668, 309, 311, 668, 3879, 3163, 309, 570, 291, 600, 668], "temperature": 0.0, "avg_logprob": -0.11772319862434456, "compression_ratio": 1.6988847583643123, "no_speech_prob": 1.723034802125767e-05}, {"id": 1260, "seek": 608102, "start": 6081.02, "end": 6083.860000000001, "text": " Amazing like I'm thrilled with how you've", "tokens": [14165, 411, 286, 478, 18744, 365, 577, 291, 600], "temperature": 0.0, "avg_logprob": -0.19605500647362242, "compression_ratio": 1.4094488188976377, "no_speech_prob": 7.527553862018976e-06}, {"id": 1261, "seek": 608102, "start": 6084.820000000001, "end": 6086.820000000001, "text": " you know reacted to", "tokens": [291, 458, 34037, 281], "temperature": 0.0, "avg_logprob": -0.19605500647362242, "compression_ratio": 1.4094488188976377, "no_speech_prob": 7.527553862018976e-06}, {"id": 1262, "seek": 608102, "start": 6087.9800000000005, "end": 6089.820000000001, "text": " the kind of", "tokens": [264, 733, 295], "temperature": 0.0, "avg_logprob": -0.19605500647362242, "compression_ratio": 1.4094488188976377, "no_speech_prob": 7.527553862018976e-06}, {"id": 1263, "seek": 608102, "start": 6089.820000000001, "end": 6095.46, "text": " You know the opportunities I've given you and also to the feedback that I've given you", "tokens": [509, 458, 264, 4786, 286, 600, 2212, 291, 293, 611, 281, 264, 5824, 300, 286, 600, 2212, 291], "temperature": 0.0, "avg_logprob": -0.19605500647362242, "compression_ratio": 1.4094488188976377, "no_speech_prob": 7.527553862018976e-06}, {"id": 1264, "seek": 609546, "start": 6095.46, "end": 6110.46, "text": " So congratulations", "tokens": [50364, 407, 13568, 51114], "temperature": 0.0, "avg_logprob": -0.8746345520019532, "compression_ratio": 0.6923076923076923, "no_speech_prob": 4.000681292382069e-05}], "language": "en"}