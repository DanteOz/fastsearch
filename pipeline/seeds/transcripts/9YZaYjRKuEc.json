{"text": " Hi there and welcome to lesson 16, where we are working on building our first flexible training framework, the learner. And I've got some very good news, which is that I have thought of a way of doing it a little bit more gradually and simply actually than last time. So that should, that should make things a bit easier. So we're going to take it a bit more step by step. So we're working in the 09 learner notebook today. And we've seen already this, this basic callbacks learner. And so the idea is that we've seen so far this learner, which wasn't flexible at all, but had all the basic pieces, which is we've got a fit method. We are hard coding that we can only calculate accuracy and average loss. We're hard coding, we're putting things on a default device. Hard coding a single learning rate. But the basic idea is here we go through each epoch and call one epoch to train or evaluate, depending on this flag. And then we loop through each batch in the data loader. And one batch is going to grab the x and y parts of the batch, call the model, call the loss function, and if we're training do the backward pass. And then print out, well calculate the statistics for our accuracy, and then at the end of an epoch print that out. So it wasn't very flexible. But it did do something. So that's good. So what we're going to do now is we're going to do as an intermediate step, we're going to look at a, what I'm calling a basic callbacks learner. And it actually has nearly all the functionality of the full thing. Then we're going to, after we look at this basic callbacks learner, we're then going to, after creating some callbacks and metrics, we're going to look at something called the flexible learner. So let's go step by step. So the basic callbacks learner looks very similar to the previous learner. It's got a fit function, which is going to go through each epoch, calling one epoch with training on and then training off. And then one epoch will go through each batch and call one batch, and one batch will call the model, the loss function, and if we're training it will do the backward step. So that's all pretty similar. But there's a few more things going on here. For example, if we have a look at fit, you'll see that after creating the optimizer, so we call self.optfunc. So optfunc here defaults to SGD. So we instantiate an SGD object passing in our model's parameters and the requested learning rate. And then before we start looping through one epoch at a time, now we've set epochs here, we first of all call self.callback and passing in before fit. Now what does that do? self.callback is here, and it takes a method name, so in this case it's before fit, and it calls a function called runCallbacks. It passes in a list of our callbacks and the method name, in this case before fit. So runCallbacks is something that's going to go for each callback, and it's going to sort them in order of their order attribute. And so there's a base class for our callbacks which has an order of zero. So our callbacks are all going to have the same order of zero, unless you ask otherwise. So here's an example of a callback. So before we look at how callbacks work, let's just run a callback. So we can create a ridiculously simple callback called completionCallback, which before we start fitting a new model, it will set its count attribute to zero. After each batch it will increment that, and after completing the fitting process it will print out how many batches we've done. So before we even train a model we could just run manually before fit, after batch, and after fit using this runCBs. And you can see it's ended up saying completed one batches. So what did that do? So it went through each of the CBs in this list, there's only one, so it's going to look at the one CB, and it's going to try to use getAttr to find an attribute with this name, which is before fit. So if we try that manually, so this is the kind of thing I want you to do if you find anything difficult to understand, is do it all manually. So create a callback, set it to CBs zero, just like you're doing in a loop, right, and then find out what happens if we call this, and pass in this, and you'll see it's returned a method. And then what happens to that method? It gets called. So let's try calling it. There we are. So that's what happened when we call the before fit, which doesn't do anything very interesting. But if we then call after batch, and then we call after fit, there it is, right. So yeah, make sure you don't just run code willy-nilly, but understand it by experimenting with it. And I don't always experiment with it myself in these classes, often I'm leaving that to you, but sometimes I'm trying to give you a sense of how I would experiment with code if I was learning it. So then having done that, I would then go ahead and delete those cells. But you can see I'm using this interactive notebook environment to explore, and learn, and understand. And so now we've got, and if I haven't created a simple example or something to make it really easy to understand, you should do that, right. So don't just use what I've already created, or what somebody else has already created. So we've now got something that works totally independently. We can see how it works. This is what a callback does. So a callback is something which will look at a class, a callback is a class, where you can define one or more of before after fit, before after batch, and before after epoch. So it's going to go through and run all the callbacks that have a before fit method, before we start fitting. Then it'll go through each epoch, and call one epoch with training, and one epoch with evaluation. And then when that's all done, it will call after fit callbacks. And one epoch will, before it starts on enumerating through the batches, it will call before epoch. And when it's done, it will call after epoch. The other thing you'll notice is that there's a try, except immediately before every before method, and immediately after every after method. There's a try, and there's an except. And each one has a different thing to look for. Cancel fit exception, cancel epoch exception, and cancel batch exception. So here's the bit which goes through each batch, calls before batch, processes the batch, calls after batch. And if there's an exception that's of type cancel batch exception, it gets ignored. So what's that for? So the reason we have this is that any of our callbacks could call, could raise, any one of these three exceptions. To say I don't want to do this batch please. So maybe we'll look at an example of that in a moment. So we can now train with this. So let's call, create a little get model function that creates a sequential model with just some linear layers. And then we'll call fit, and it's not telling us anything interesting because the only callback we added was the completion callback. That's fine. It's, it's training, it's doing something. And we now have a trained model. Just didn't print out any metrics or anything because we don't have any callbacks for that. That's the basic idea. So we could create a, maybe we could call it a single batch callback, which after batch, after a single batch, it raises a cancel, cancel fit exception. So that's a pretty, I mean I suppose that could be kind of useful actually if you want to just run one batch to your model to make sure it works. So we could try that. So now we're going to add to our list of callbacks the single batch callback. Let's try it. And in fact, you know, we probably want this, let's just have a think here. Oh, that's fine. Let's run it. There we go. So it ran and nothing happened. And the reason nothing happened is because this cancelled before this ran. So we could make this run second by setting its order to be higher. And we could say just order equals one because the default order is zero. And we sort in order of the order attribute. Actually let's use cancel epoch exception. There we go. That way it'll run the final fit. There we are. So it did one batch for the, it did one batch for the training and one batch for the evaluation. So that's a total of two batches. So remember callbacks are not a special magic part of like the Python language or anything. It's just a name we use to refer to these functions or classes or callables more accurately that we, that we pass into something that will then call back to that callable at particular times. And I think these are kind of interesting kinds of callbacks because these callbacks have multiple methods in them. So is each method a callback? Is each class with all those methods a callback? I don't know. I tend to think of the class with all the methods in as a single callback. I'm not sure if we have great nomenclature for this. All right. So let's actually try to get this doing something more interesting by not modifying the learner at all, but just by adding callbacks because that's the great hope of callbacks, right? So it would be very nice if it told us the accuracy and the loss. So to do that it would be great to have a class that can keep track of a metric. So I've created here a metric class and maybe before we look at it we'll see how it works. You could create, for example, an accuracy metric by defining the calculation necessary to calculate the accuracy metric, which is the mean of how often do the inputs equal the targets. The idea is you could then create an accuracy metric object. You could add a batch of inputs and targets, and add another batch of inputs and targets, and get the value, and there you would get the 0.45 accuracy. Or another way you could do it would be just to create a metric which simply takes gets the weighted average, for example, of your loss. So you could add 0.6 as the loss with a batch size of 32, 0.9 as a loss in a batch size of 2, and then that's going to give us a weighted average loss of 0.62, which is equal to this weighted average calculation. So that's like one way we could kind of make it easy to calculate metrics. So here's the class. Basically we're going to keep track of all of the actual values that we're averaging, and the number in each mini-batch. So when you add a mini-batch we call calculate, which for example for accuracy, remember this is going to override the parent class's calculate, so it does the calculation here. And then we'll add that to our list of values. We will add to our list of batch sizes the current batch size. And then when you calculate the value we will calculate the weighted sum, sorry the weighted mean, weighted average. Now notice that here value I didn't have to put parentheses after it, and that's because it's a property. I think we've seen this before, so just to remind you, property just means you don't have to put parentheses after it to get it to get the calculation to happen. All right, so just let me know if anybody's got any questions up to here of course. So we now need some way to use this metric in a callback to actually print out. The first thing I'm going to do though is I'm going to create one more, one useful metric first, a very simple one, just two lines of code called the device callback. And that is something which is going to allow us to use CUDA or the Apple GPU or whatever, without the complications we had before of, you know, how do we have multiple processes in our data loader and also use our device and not have everything fall over. So the way we could do it is we could say before fit, put the model onto the default device, and before each batch is run, put that batch onto the device. Because look what happened in the, this is really really important, in the learner, absolutely everything is put inside self dot, which means it's all modifiable. So we go for self dot iteration number comma self dot the batch itself, enumerating the data loader, and then we call one batch. But before it we call the callback. So we can modify this. Now how does the callback get access to the learner? Well what actually happens is we go through each of our callbacks and put, set an attribute called learn equal to the learner. And so that means in the callback itself we can say self dot learn dot model. And actually we could make this a bit better I think. So make it like maybe you don't want to use the default device. So this is where I would be inclined to add a constructor and set device, and we could default it to the default device of course. And then we could use that instead. And that would give us a bit more flexibility. So if you wanted to train on some different device then you could. I think that might be a slight improvement. Okay so there's a callback we can use to put things on CUDA. And we could check that it works by just quickly going back to our old learner here, remove the single batch CB, and replace it with device CB. Yep still works. So that's a good sign. Okay so now let's do our metrics. Now of course we couldn't use metrics until we built them by hand. The good news is we don't have to write every single metric now by hand, because they already exist in a fairly new project called TorchEval, which is an official PyTorch project. And so TorchEval is something that gives us actually, I came across it after I had created my own metric class, but it actually looks pretty similar to the one that I built earlier. So you can install it with pip. I'm not sure if it's on Conda yet, but it probably will be soon by the time you see the video. I think it's pure Python anyway, so it doesn't matter how you install it. And yeah it has a pretty similar approach, where you call .update and you call .compute. So they're slightly different names, but they're basically super similar to the thing that we just built. So there's a nice good list of metrics to pick from. So because we've already built our own now, that means we're allowed to use theirs. So we can import the multi-class accuracy metric and the mean metric. And just to show you they look very very similar, if we call multi-class accuracy and we can pass in a mini batch of inputs and targets and compute, and that all works nicely. Now these, in fact it's exactly the same as what I wrote, we both added this thing called reset, which basically, well, resets it. And so obviously we're going to be wanting to do that probably at the start of each epoch. And so if you reset it and then try to compute, you'll get nan, because you can't get accuracy, because accuracy is meaningless when you don't have any data yet. Okay so let's create a metrics callback so we can print out our metrics. I've got some ideas to improve this, which maybe I'll do this week, but here's a basic working version. Slightly hacky, but it's not too bad. So generally speaking, one thing I noticed actually is, I don't know if this is considered a bug, but a lot of the metrics didn't seem to work correctly in Torch eval when I had tensors that were on the GPU and had requires grad. So I created a little to CPU function, which I think is very useful, and that's just going to detach the, so detach takes the tensor and removes all the gradient history, the computation history used to calculate a gradient, and puts it on the CPU. That'll do the same for dictionaries of tensors, lists of tensors, and tuples of tensors. So our metrics callback, basically here's how we're going to use it. So let's run it. So here we're creating a metrics callback object and saying we want to create a metric called accuracy. That's what's going to print out, and this is the metrics object we're going to use to calculate accuracy, and so then we just pass that in as one of our callbacks. And so you can see what it's going to do is it's going to print out the epoch number, whether it's training or evaluating, so training set or validation set, and it'll print out our metrics and our current status. We can simplify that. We don't need to print those bits because it's all in the dictionary now. Let's do that. There we go. So let's take a look at how this works. So we are going to be creating, for the callback, we're going to be passing in the names and object, metric objects for the metrics to track and print. So here it is here, star star metrics, so we've seen star star before. And as a little shortcut I decided that it might be nice if you didn't want to write accuracy equals, you could just remove that and run it, and if you do that then it will give it a name and it'll just use the same name as the class. And so that's why you can either pass in, so star ms will be a tuple, well I mean it's going to be pulled out, so it's just passing a list of positional arguments which we turn into a tuple, or you can pass in named arguments that'll be turned into a dictionary. If you pass in positional arguments then I'm going to turn them into named arguments in the dictionary by just grabbing the name from their type. So that's where this comes from. That's all that's going on here, just a little shortcut, bit of convenience. So we'll store that away. And this is, yeah, this is a bit I think I can simplify a little bit, but I'm just adding manually an additional metric which is, I'm going to call the loss, and that's just going to be the weighted average of the losses. So before we start fitting, we're going to actually tell the learner that we are the metrics callback, and so you'll see later where we're going to actually use this. Before each epoch we will reset all of our metrics. After each epoch we will create a dictionary of the keys and values, which are the actual strings that we want to print out, and we will call log, which for now we'll just print them. And then after each batch, this is the key thing, we're going to actually grab the input and target, we're going to put them on the CPU, and then we're going to go through each of our metrics and call that update. So remember the update in the metric is the thing that actually says here's a batch of data, right? So we're passing in the batch of data, which is the predictions and the targets. And then we'll do the same thing for our special loss metric, passing in the actual loss and the size of our mini-batch. And so that's how we're able to get this, yeah, this actual running on the NVIDIA GPU and showing our metrics. And obviously there's a lot of room to improve how this is displayed, but all the information is we needed here, and it's just a case of changing that function. Okay, so that's our kind of like intermediate complexity learner. We can make it more sophisticated, but it's still exactly, it's still going to fit in a single screen of code. So this is kind of my goal here, is to keep everything in a single screen of code. This first bit is exactly the same as before, but you'll see that the one epoch and fit and batch has gone from, let's see what it was before. It's gone from quite a lot of code, all this, to much less code. And the trick to doing that is I decided to use a context manager. We're going to learn more about context managers in the next notebook, but basically, I originally last week I was saying I was going to do this as a decorator, but I realized a context manager is better. Basically what we're going to do is we're going to call our before and after callbacks in a try except block. And to say that we want to use the callbacks in the try except block, we're going to use a with statement. So in Python, a with statement says everything in that block, call our context manager before and after it. Now there's a few ways to do that, but one really easy one is using this context manager decorator. And everything up into the up to the yield statement is called before your code. Where it says yield, it then calls your code. And then everything after the yield is called after your code. So in this case, it's going to be try self.callback before name, where name is fit. And then it will call for self.epoch etc. Because that's where the yield is. And then it'll call self.callback after fit except. Okay and now we need to grab the cancel fit exception. So all of the variables that you have in Python all live inside a special dictionary called globals. So this dictionary contains all of your variables. So I can just look up in that dictionary the variable called cancel fit with a capital F exception. So this is except cancel fit exception. So this is exactly the same then as this code. Except the nice thing is now I only have to write it once, rather than at least three times. And I'm probably going to want more of them. So you know I tend to think it's worth, yeah, I tend to think it's worth refactoring a code when you have duplicate code. Particularly here we had the same code three times. So that's going to be more of a maintenance headache. We're probably going to want to add callbacks to more things later. So by putting it into a context manager just once, I think we're going to reduce our maintenance burden. Well I know we do because I've had a similar thing in fast.ai for some years now and it's been quite convenient. So that's what this context manager is about. Yeah other than that the code's exactly the same. So we create our optimizer and then with our callback context manager for fit, go through each epoch, call one epoch, set it to training or non-training mode based on the argument we pass in. Grab the training or validation set based on the argument we pass in. And then using the context manager for epoch, go through each batch in the data loader. And then for each batch in the data loader using the batch context. Now this is where something gets quite interesting. We call predict, get loss, and if we're training, backward step, and zero gret. But previously we actually called self.model etc. self.loss function etc. So we go through each batch and call before batch, do the batch. Oh sorry that's our slow version. Wait what are we doing? Oh yes we're going to be over here. Okay I'm back where we are. So previously we were calling, yeah, calling the model, calling the loss function, calling loss.backward, opt.step, opt.zero gret. But now we are calling instead self.predict, self.getloss, self.backward. And how on earth is that working? Because they're not defined here at all. And so the reason I've decided to do this is it gives us a lot of flexibility. We can now actually create our own way of doing predict, get loss, backward step, and zero gret in different situations. And we're going to see some of those situations. So what happens if we call self.predict and it doesn't exist? Well it doesn't necessarily cause an error. What actually happens is it calls a special magic method in Python called dunder getattr, as we've seen before. And what I'm doing here is I'm saying okay well if it's one of these special five things, don't raise an attribute error, which is this is the default thing it does, but instead create a callback, or actually I should say call self.callback, passing in that name. So it's actually going to call self.callback, quote, predict. And self.callback is exactly the same as before. And so what that means now is to make this work exactly the same as it did before, I need a callback which does these five things. And here it is. I'm going to call it train callback. So here are the five things. Predict, getloss, backward, step, and zero gret. So they are here. Predict, getloss, backward, step, and zero gret. Okay so they're almost exactly the same as what they looked like in our intermediate learner, except now I just need to have self.learn in front of everything, because we remember this is a callback, it's not the learner. And so for a callback, the callback can access the learner using self.learn. So self.learn.preds, there's self.learn.model, passing in self.learn.batch, and just the independent variables. Ditto for the loss, calls the loss function, backward, step, zero gret. So that's, at this point this isn't doing anything that it wasn't doing before, but the nice thing is now if you want to use HuggingFace accelerate, or you want something that works on HuggingFace data styles dictionary things, or whatever, you can actually change exactly how it behaves by just call passing, by creating a callback for training. And if you want everything except one thing to be the same, you can inherit from train CB. So this is, I've not tried this before, I haven't seen this done anywhere else, so it's a bit of an experiment. So I'm interested to hear how you go with it. And then finally, I thought it'd be nice to have a progress bar. So let's create a progress callback, and the progress bar is going to show on it our current loss, and going to put create a plot of it. So I'm going to use a project that we created called fast progress, mainly created by the wonderful Sylvain. And basically fast progress is, yeah, a very nice way to create very flexible progress bars. So let me show you what it looks like first. So let's get the model, and train, and as you can see it actually in real time updates the graph, and everything. There you go, that's pretty cool. So that's the, that's the progress bar, the metrics callback, the device callback, and the training callback all in action. So before we fit, we actually have to set self.learn.epox. Now that might look a little bit weird, but self.learn.epox is the thing that we loop through for self.epox in. So we can change that, so it's not just a normal range, but instead it is a progress bar around a range. We can then check, remember I told you that the learner is going to have the metrics attribute applied, we can then say, oh if the learner has a metrics attribute, then let's replace the underscore log method there with ours. And our one, instead we'll write to the progress bar. Now this is pretty simple, it looks very similar to before, but we could easily replace this for example with something that creates an HTML table, which is another thing fast progress does, or other stuff like that. So you can see we can modify, the nice thing is we can modify how our metrics are displayed. So that's a very powerful thing that Python lets us do, is actually replace one piece of code with another, and that's the whole purpose of why the metrics callback had this underscore log separately. So why didn't I just say print here? That's because this way classes can replace how the metrics are displayed. So we could change that to like send them over to weights and biases for example, or you know create visualizations or so forth. So before epoch we do a very similar thing, the self.learn.dl iterator, we change it to have a progress bar wrapped around it, and then after each bar we set the progress bar's comment to be the loss. It's going to print, it's going to show the loss on the progress bar as it goes, and if we've asked for a plot then we will append the losses to a list of losses, and we will update the graph with the losses and the batch numbers. So there we have it, we have a, yeah, nice working learner which is I think the most flexible learner that training loop probably that's, I hope, has ever been written, because I think the fastai2 one was the most flexible that had ever been written before, and this is more flexible. And the nice thing is you can make this your own, you know, you can, you know, fully understand this training loop, so it's kind of like you can use a framework, but it's a framework in which you're totally in control of it, and you can make it work exactly how you want to. Ideally, not by changing the learner itself, ideally by creating callbacks, but if you want to, you could certainly, like look at that, the whole learner fits on a single screen, so you could certainly change that. We haven't added inference yet, although that shouldn't be too much to add, I guess we have to do that at some point. Okay, now interestingly, I love this about Python, it's so flexible, when we said self.predict, self.getLoss, I said if they don't exist, then it's going to use getAttr, and it's going to try to find those in the callbacks. And in fact, you could have multiple callbacks that define these things, and then they would chain them together, which would be kind of interesting. But there's another way we could make these exist, which is, which is that we could subclass this. So let's not use trainCB, just to show us how this would work, and instead we're going to use a subclass. So here I'm going to subclass learner, and I'm going to override the five. Well, it's not exactly overriding, I didn't have any definition of them before, so I'm going to define the five directly in the learner subclass. So that way it's never going to end up going to getAttr, because getAttr is only called if something doesn't exist. So here it's basically all, these five are exactly the same as in our train callback, except we don't need self.learn anymore, we can just use self, because we're now in the learner. But I've changed zero grad to do something a bit crazy. I'm not sure if this has been done before, I haven't seen it, but maybe it's an old trick that I just haven't come across. But it occurred to me, zero grad, which remember is the thing that we call after we take the optimizer step, doesn't actually have to zero the gradients at all. What if instead of zeroing the gradients, we multiplied them by some number, like say 0.85? Well what would that do? Well what it would do is it would mean that your previous gradients would still be there, but they would be reduced a bit. And remember what happens in PyTorch is PyTorch always adds the gradients to the existing gradients, and that's why we normally have to call zero grad. But if instead we multiply the gradients by some number, I mean we should really make this a parameter. Let's do that, shall we? So let's create a parameter. So probably, there's a few ways we could do this. Well let's do it properly. We've got a little bit of time. So we could say, well, maybe we'll just copy and paste all those over here, and we'll add momentum, momentum equals 0.85, self.momentum equals momentum, and then super. So make sure you call the super classes passing in all the stuff. We could use delegates for this and quags, that would be possibly another way of doing it, but let's just do this for now. Okay and then so there we wouldn't make it 0.85, we would make it self.momentum. So you'll see now, still trains, but there's no train cb callback anymore in my list. I don't need one because I've defined the five methods in the subclass. Now this training at the same learning rate for the same time, the accuracy, I don't think it improves by more, let's run them all. Yeah this is a lot like gradient accumulation callback, they're kind of cooler I think. Okay so the, let's see, the loss has gone from 0.8 to 0.55, and the accuracy has gone from about 0.7 to about 0.8, so they've improved. Why is that? Well we're going to be learning a lot more about this pretty shortly, but basically what's happening here, but basically what's happening here is we have just implemented in a very interesting way, which I haven't seen done before, something called momentum. And basically what momentum does is it say like, imagine you are, you know, you're trying, you've got some kind of complex contour loss surface, right, and you know, so imagine these are hills with a marble, very similar, right, and your marble's up here. What would normally happen with gradient descent is it would go, you know, in the direction downhill, which is this way, so it'll go over here, and then over here, right. Very slow. What momentum does is it's, is the first step's the same, and then the second step says, oh I wanted to go this way, but I'm going to add together the previous direction plus the new direction, but reduce the previous direction a bit. So that would actually make me end up about here. And then the second one does the same thing. And so momentum basically makes you much more quickly go to your destination. So normally momentum is done, the reason I did it this way, partly to show you, it's just a bit of fun, a bit of interest, but it's very useful because normally momentum, you have to store a complete copy basically of all the gradients, the momentum version of the gradients, so that you can kind of keep track of that, that running exponentially weighted moving average. But using this trick, you're actually using the dot grad themselves to store the exponentially weighted moving average. So anyway, there's a little bit of fun, which hopefully, particularly those of you who are interested in accelerated optimizers and memory saving, might find a bit inspiring. All right, there's one more callback I'm going to show before the break, which is the wonderful learning rate finder. I'm assuming that anybody who's watching this already is familiar with the learning rate finder from fast.ai. If you're not, there's lots of videos and tutorials around about it. It's an idea that comes from a paper by Leslie Smith from a few years ago. And the basic idea is that we will increase the learning rate. I should have put titles on this. The x-axis here is learning rate, the y-axis here is loss. We increase the learning rate gradually over time, and we plot the loss against the learning rate, and we find how high can we bring the learning rate up before the loss starts getting worse. So we'd want roughly where about the steepest slope is. So probably here it would be about 0.1. So it'd be nice to create a learning rate finder. So here's a learning rate finder callback. So what a learning rate finder needs to do, well you have to tell it how much to multiply the learning rate by each batch. Let's say we add 30% to the learning rate each batch. So we'll store that. So before we fit, we obviously need to keep track of the learning rates, and we need to keep track of the losses, because those are the things that we put on a plot. The other thing we have to do is decide when do we stop training. So when is it clearly gone off the rails? And I decided that if the loss is three times higher than the minimum loss we've seen, then we should stop. So we're going to keep track of the minimum loss, and so let's just initially set that to infinity. That's a nice big number. Well not quite a number, but a number-ish like thing. So then after every batch, first of all let's check that we're training. Okay if we're not training, then we don't want to do anything. We don't use the learning rate finder during validation. So here's a really handy thing. Just raise cancel epoch exception, and that stops it from doing that epoch entirely. So just to see how that works, you can see here one epoch does with the callback context manager epoch, and that will say, oh it's got cancelled. So it goes straight to the except, which is going to go all the way to the end of that code, and it's going to skip it. So you can see that we're using exceptions as control structures, which is actually a really powerful programming technique that is really underutilized, in my opinion. Like a lot of things I do, it's actually somewhat controversial. Some people think it's a bad idea, but I find it actually makes my code more concise, and more maintainable, and more powerful. So I like it. So let's see, yeah, so that's, we've got our cancel epoch exception. So then we're just going to keep track of our learning rates. The learning rates, we're going to learn a lot more about optimizers shortly, so I won't worry too much about this, but basically the learning rates are stored by PyTorch inside the optimizer, and they're actually stored in things called param groups, parameter groups. So don't worry too much about the details, but we can grab the learning rate from that dictionary, and we'll learn more about that shortly. We've got to keep track of the loss, append it to our list of losses, and if it's less than the minimum we've seen, then record it as the minimum, and if it's greater than three times the minimum, then look at this, this is really cool, cancel fit exception. So this will stop everything in a very nice, clean way. No need for lots of returns and conditionals or stuff like that, just raise the cancel fit exception. And yeah, and then finally we've got to actually update our learning rate to 1.3 times the previous one, and so basically the way you do it in PyTorch is you have to go through each parameter group, and grab the learning rate in the dictionary, and multiply it by LRMult. So yeah, you've already seen it run, and we can, at the end of running, you will find that there is now a, the callback will now contain an LRs and a losses. So for this callback, I can't just add it directly to the callback list. I need to instantiate it first, and the reason I need to instantiate it first is because I need to be able to grab its learning rates and its losses. And in fact, you know, we could grab that whole thing and move it in here. So there's no reason callbacks only have to have the callback things, right? So we could do this, and now that's just going to become self. There we go. And so then we can train it again, and we could just call LRFind.plot. So callbacks can really be, you know, quite self-contained nice things, as you can see. So there's a more sophisticated callback, and I think it's doing a lot of really nice stuff here. You might have come across something in PyTorch called learning rate schedulers, and in fact we could implement this whole thing with a learning rate scheduler. It won't actually save that much time, but I just want to show you when you use stuff in PyTorch like learning rate schedulers, you're actually using things that are extremely simple. The learning rate scheduler basically does this one line of code for us. So I'm going to now create a new LRFinderCB, and this time I'm going to use the PyTorch's exponential LR scheduler, which is here. So this is, now it's interesting that actually the documentation of this is kind of actually wrong. It claims that it decays the learning rate of each parameter group by gamma, so gamma is just some number you pass in. I don't know why this has to be a Greek letter, but it sounds more fancy than multiplying by an LR multiplier. It says every epoch, but it's not actually done every epoch at all. What actually happens is in PyTorch, the schedulers have a step method, and the decay happens each time you call step. And if you set gamma, which is actually LRMult, to a number bigger than one, it's not a decay, it's an increase. So the difference now, I guess I'll copy and paste the previous version. Okay, so the previous version is on the top. So the main difference here is that before fit, we're going to create something called a self.shed equal to the scheduler. And the scheduler, because it's going to be adjusting the learning rates, it actually needs access to the optimizer. So we pass in the optimizer and the learning rate multiplier. And so then in after batch, rather than having this line of code, we replace it with this line of code, self.shed.step. So that's the only difference. And you know, I mean, we're not gaining much, as I said, by using the PyTorch exponential LR scheduler. But I mainly wanted to do it so you can see that these things like PyTorch schedulers are not doing anything magic. They're just doing that one line of code for us. And so I run it again using this new version. Oopsie-daisy. Oh, I forgot to run this line of code. There we go. And I guess I should also add the nice little plot method. So we'll just move it to the bottom. There. LR find dot plot. There we go. Put that one back to how it was. All right. Perfect timing. So we added a few very important things in here. So make sure we export. And we'll be able to use them shortly. All right. Let's have an eight-minute break. Let's just have a ten-minute break. So I'll see you back here at eight past. All right. Welcome back. So the first thing which I really like is we could rename plot to after fit. Which I really like because that means we should be able to then just call learn dot fit and delete the next one. And let's see. It didn't work. Why not? Oh no. That doesn't work, does it? Because the... hmm. You know what? I think the callback here could go into a finally block, actually. That would actually allow us to always call the callback, even if we've cancelled. I think that's reasonable. But it may have its own confusions. Anyway, we could try it for now because that would let us put this after fit in. There we go. So that automatically runs that. So that's an interesting idea. I think I quite like it. Cool. So let's now look at notebook 10. So I feel like this is the next big piece we need. So we've got a pretty good system now for training models. What I think we're really missing though is a way to identify how our models are training. And so to identify how our models are training, we need to be able to look inside them and see what's going on while they train. We don't currently have any way to do that. And therefore it's very hard for us to diagnose and fix problems. Most people have no way of looking inside their models. And so most people have no way to properly diagnose and fix models. And that's why most people, when they have a problem with training their model, randomly try things until something starts hopefully working. We're not going to do that. We're going to do it properly. So we can import the stuff that we just created in the learner. And the first thing I'm going to do, introduce now, is a set seed function. We've been using torch.manualseed before. We know all about RNGs, random number generators. We've actually got three of them. PyTorches, NumPys, and Pythons. Let's seed all of them. And also in Python PyTorch, you can use a flag to ask it to use deterministic algorithms so things should be reproducible. As we've discussed before, you shouldn't always just make things reproducible. But for lessons I think this is useful. So here's a function that lets you set a reproducible seed. All right, let's use the same data set as before, a fashion MNIST data set. We'll load it up in the same way. Let's create a model that looks very similar to our previous models. This one might be a bit bigger, mightn't it? I didn't actually check. Okay, so let's use multi-class accuracy again. Same callbacks that we used before. We'll use the trainzb version for no particular reason. And generally speaking, we want to train as fast as possible. Not just because we don't like wasting time, but actually more importantly because the higher the learning rate you train at, the more you're able to find a often more generalizable set of weights. And also, oh, training quickly also means that we can look at each batch, each item in the data less often. So we're going to have less issues with overfitting. And generally speaking, if we can train at a high learning rate, then that means that we're learning to train in a stable way. And stable training is very good. So let's try setting up a high learning rate of 0.6 and see what happens. So here's a function that's just going to create our learner with our callbacks, and fit it, and return the learner in case we want to use it. And it's training. Oh, and then it suddenly fell apart. So it's going well for a while, and then it stopped training nicely. So one nice thing about this graph is that we can immediately see when it stops training well, which is very useful. So what happened there? Why did it go badly? I mean we can guess that it might have been because of our high learning rate, but what's really going on? So let's try to look inside it. So one way to look inside it would be we could create our own sequential model, just like the sequential model we've built before. Do you remember we created one using nn.moduleList in a previous lesson? If you've forgotten, go back and check that out. And when we call that model, we go through each layer and just call the layer. And what we could do is, so we could add something in addition, which is at each layer, we could also get the mean of that layer, and the standard deviation of that layer, and append them to a couple of different lists, and activation means and activation standard deviations. This is going to contain, after we call this model, it's going to contain the means and standard deviations for each layer. And then we could define dunder iter, which makes this into an iterator, as being let's say just, oh just, when you iterate through this model, you can iterate through the layers. So we can then train this model in the usual way, and this is going to give us exactly the same outcome as before, because I'm using the same seed, so you can see it looks identical. But the difference is instead of using nn.sequential, we've now used something that's actually saved the means and standard deviations of each layer. And so therefore we can plot them. Okay, so here we've plotted the activation means, and notice that we've done it for every batch. So that's why along the x-axis here we have batch number, and on the y-axis we have the activation means, and then we have it for each layer. So rather than starting at 1, because in Python we're starting at 0, so this is the first layer is blue, second layer is orange, third layer green, fourth layer red, and fifth layer, watch for that like movie kind of color. And look what's happened. The activations have started pretty small, close to zero, and have increased at an exponentially increasing rate, and then have crashed, and then have increased again at an exponential rate, and crashed again, it increased again, crashed again, and each time they've gone up they've gone up even higher, and they've crashed, in this case even lower. And what happens? Well what's happening here when our activations are really close to zero? Well when your activations are really close to zero, that means that the inputs to each layer are numbers very close to zero. As a result of which of course the outputs are very close to zero, because we're doing just matrix multiplies. And so this is a disaster. When activations are very close to zero, they're dead units. They're not able to do anything, and you can see for ages here it's not training at all. And this is, so this is the activation means. The standard deviations tell an even stronger story. So you want, generally speaking, you want the means of the activations to be about zero, and the standard deviations to be about one. Mean of zero is fine as long as they're spread around zero, but a standard deviation of close to zero is terrible, because that means all of the activations are about the same. So here, after batch 30, all of the activations are close to zero, and all of their standard deviations are close to zero, so all the numbers are about the same, and they're about zero. So nothing's going on. And you can see the same thing's happening with the standard deviations. We start with not very much variety in the weights. It exponentially increases how much variety there is, and then it crashes again. Exponentially increases, crashes again. This is a classic shape of bad behavior, and with these two plots, you can really understand what's going on in your model. And if you train a model, and at the end of it, you kind of think, well I wonder if this is any good. If you haven't looked at this plot, you don't know, because you haven't checked to see whether it's training nicely. Maybe it could be a lot better. If you can get something, we'll see some nicer training pictures later, but generally speaking, you want something where your mean is always about zero, and your variance is always about one. Standard deviation is always about one. And if you see that, then it's a pretty good chance you're training properly. If you don't see that, you're most certainly not training properly. Okay, so what I'm going to do in the rest of this part of the lesson is explain how to do this in a more elegant way, because as I say, being able to look inside your models is such a critically important thing to building and debugging models. We don't have to do it manually. We don't have to create our own sequential model. We can actually use a PyTorch thing called hooks. So as it says here, a hook is called when a layer that it's registered to is executed during the forward pass. That's called a forward hook, or the backward pass, and that's called a backward hook. And so the key thing about hooks is we don't have to rewrite the model. We can add them to any existing model. So we can just use standard nn.sequential, passing in our layers, which were these ones here. And so we're still going to have something to keep track of the activation means and standard deviations. So just create an empty list for now, for each layer in the model. And let's create a little function that's going to be called, because a hook is going to call a function when during the forward pass, for a forward hook, or the backward pass for a backward hook. So we've got a function called appendStats. It's going to be passed the hook number, sorry, the layer number, the module, and the input and the output. So we're going to be grabbing the outputs mean and putting in activation means, and the output standard deviation, and putting it in activation standard deviations. So here's how you do it. We've got a model. You go through each layer of the model, and you call on a register forward hook. That's part of PyTorch. We don't need to write it ourselves, because we already did, right? It's just doing the same thing as this, basically. And what function is always going to be called? The function that's going to be called is the appendStats function passing in, remember partial is the equivalent of saying appendStats passing in i as the first element, the first argument. So if we now fit that model, it trains in the usual way, but after each layer, it's going to call this. And so you can see we get exactly the same thing as before. So one question we get here is what's the difference between a hook and a callback? Nothing at all. Hooks and callbacks are the same thing. It's just that PyTorch defines hooks, and they call them hooks instead of callbacks. They are less flexible than the callbacks that we used in the learner, because you don't have access to all the available states, you can't change things, but they're a particular kind of callback. It's just setting a piece of code that's going to be run for us when we, when something happens. And in this case, there's something that happens is that either a layer in the forward pass is called or a layer in the backward pass is called. I guess you could describe the function that's being called back as the callback, and the thing that's doing the callback has the hook. I'm not sure if that level of distinction is important, but maybe that's, you could do that. Okay, so anyway, this is a little bit fussy of kind of like creating globals and appending to them and stuff like that. So let's try to simplify this a little bit. So what I did here was I created a class called hook. So this class, when we create it, we're going to pass in the module that we're hooking. So we call m.registerForwardHook, and we call the function. We pass the function that we want to be given, and so here's, we pass the function. And we're also going to pass in the hook class to the function. Let's also define a remove, because this is actually the thing that removes the hook. We don't want it sitting around forever. This is called, the del is called by Python when an object is freed. And when that happens, we should also make sure that we remove this. Okay, so appendStats now, we're going to replace, it's going to instead get past the hook instead, because that's what we asked to be passed. And if there's no .stats attribute in there yet, then let's create one. And then we're going to be past the activation, so put that on the CPU, and append the mean and the standard deviation. And now the nice thing is that the stats are actually inside this object, which is convenient. So now we can do exactly the same thing as before, but we don't have to set any of that global stuff or whatever. We can just say, okay, our hooks is a hook with that layer and that function for all those models layers. And so we're just calling it, has called registerForwardHook for us. So now when we fit that, it's going to run with the hooks. There we go, it trains. I actually need to do it too. Okay, so then it trains and we get exactly the same shape as usual, and we get back the same results as usual. But as we can see, we're gradually making this more convenient, which is nice. So we can make it nicer still, because generally speaking, we're going to be adding multiple hooks and this stuff of, you know, this list comprehension, whatever, it's a bit inconvenient. So let's create a hooks class. So first of all, we'll see how the hooks class works in practice. So in the hooks class, the way we're going to use it is we're going to call with hooks, pass in the model, pass in the function to use as our hook, and then we'll fit the model, and that's it. It's going to be literally just one extra line of code to set up the whole thing. And then when we then, we can then go through each hook and plot the mean and standard deviation of each layer. So that's how, that's the hooks class is going to make things much easier. So the hooks class, as you can see, we're using a, making it a context manager. And we want to be able to loop through it. We want to be able to index into it. So it's quite a lot of behavior we want. Believe it or not, all that behavior is in this tiny little thing. And we're going to use the most flexible general way of creating context managers now. Context managers are things that we can say with. The general way of creating a context manager is to create a class and to find two special things, dunder enter and dunder exit. Dunder enter is a function that's going to be called when it hits the with statement. And if you add an as blah after it, then the contents of this variable will be whatever is returned from dunder enter. And as you can see, we just return the object itself. So the hooks object is going to be stored in hooks. Now interestingly, the hooks class inherits from list. You can do this. You can actually inherit from stuff like list in Python. So a hooks, the hooks object is a list. And therefore we need to call the super classes constructor. And we're going to pass in a, that list comprehension we saw, that list of hooks, where it's going to hook into each module in the list of modules we asked to hook into. Now we're passing in a model here, but because the model is an nn.sequential, you can actually loop through an nn.sequential and it returns each of the layers. So this is actually very, very nice and concise and convenient. So that's the constructor. Dunder enter just returns it. Dunder exit is what's called automatically at the end of the whole block. So when this whole thing's finished, it's going to remove the hooks. And removing the hooks is just going to go through each hook and remove it. The reason we can do for h itself is because remember this is a list. And then finally we've got a dunder del like before. And I've also added a dunder del item. This is the thing that lets you delete a single hook from the list, which will remove that one hook and call the list's del item. So there's our whole thing. So this is going to, this, this, this one's optional. This is the one that lets us remove a single hook rather than all of them. So let's just understand some of what's going on there. So here's a dummy context manager. As you can see here, it's got a dunder enter, which is going to return itself. It's going to print something. So you can see here I call with dummy context manager. And so therefore it prints, let's go first. The second thing it's going to do is call this code inside the context manager. So we've got as DCM. So that's itself. And so it's going to call hello, which prints hello. So here it is. And then finally, it's going to automatically call exit, dunder exit, which is all done. So here's all done. So again, if you haven't used context managers before, you want to be creating little samples like this yourself and getting them to work. So this is your key homework for this week. Is anything in the lesson where we're using a part of Python you're not 100% familiar with is for you to from scratch to create some simple like kind of dummy version that fully explores what it's doing. If you're familiar with all the Python pieces, then it's to create your own, you know, that is to explore do the same thing with the PyTorch pieces like with with hooks and so forth. And so I just wanted to show you also what it's like to inherit from list. So here I'm here inheriting from a list. And I could redefine how dunder del item works. So now I can create a dummy list. And it looks exactly the same as usual. But now if I delete an item from the list, it's going to call my overridden version. And then it will call the original version. And so the list is now got removed that item and did this at the same time. So you can see you can actually, yeah, modify how Python works, or create your own things that get all the behavior or the convenience of Python classes like this one and add stuff to them. So that's what's happening there. Okay, so that's our hooks class. So the next bit was developed, largely developed the last time I think it was that we did a part two course in San Francisco with Stefano. So many thanks to him for helping get this next bit looking great. We're going to create my favorite single image explanations of what's going on inside a model. We call them the colorful dimension, which they're histograms. We're going to take our same append stats. These are all the same as before. We're going to add an extra line of code, which is to get a histogram of the absolute values of the activations. So a histogram, to remind you, is something that takes a collection of numbers and tells you how frequent each group of numbers are. And we're going to create 50 bins for our histogram. So we will use our hooks that we just created, and we're going to use this new version of append stats. So it's going to train as before, but now we're going to, in addition, have this extra thing in stats, which is going to contain a histogram. And so with that, we're now going to create this amazing plot. Now what this plot is showing is for the first, second, third, and fourth layers, what does the training look like? And you can immediately see the basic idea is that we're seeing this same pattern. But what is this pattern showing? What exactly is going on in these pictures? So I think it might be best if we try and draw a picture of this. So let's take a normal histogram. So let's take a normal histogram where we basically have grouped all the data into bins, and then we have counts of how much is in each bin. So for example, this will be like the value of the activations, and it might be, say, from 0 to 10, and then from 10 to 20, and from 20 to 30. And these are generally equally spaced bins. Okay. And then here is the count. So that's the number of items with that range of values. So this is called a histogram. Okay. So what Stefano and I did was we actually turned that histogram, that whole histogram, into a single column of pixels. So if I take one column of pixels, that's actually one histogram. And the way we do it is we take these numbers. So let's say, let's say it's like 14, that one's like 2, 7, 9, 11, 3, 2, 4, 2. And so then what we do is we turn it into a single column. And so in this case, we've got 1, 2, 3, 4, 5, 6, 7, 8, 9 groups, right? So we would create our 9 groups. Sorry, they were meant to be evenly spaced, but they were not a very good job. Got our 9 groups. And so we take the first group, it's 14. And what we do is we color it with a gradient and a color according to how big that number is. So 14 is a real big number. So depending on, you know, what gradient we use, maybe red's really, really big. And the next one's really small, which might be like green. And then the next one's quite big in the middle, which is like blue. Next one's getting quite, quite bigger still. So maybe it's just a little bit, sorry, should go back to red, go back to more red. Next one's bigger still, so it's even more red, and so forth. So basically we're taking the histogram and taking it into a color-coded single column plot, if that makes sense. And so what that means is that at the very, so let's take layer number 2 here. Layer number 2, we can take the very first column. And so in the color scheme that actually matplotlib's picked here, yellow is the most common, and then light green is less common, and then light blue is less common, and then dark blue is zero. So you can see the vast majority is zero, and there's a few with slightly bigger numbers, which is exactly the same that we saw for index 1 layer. Here it is, right. The average is pretty close to zero, the standard deviation is pretty small. This is giving us more information, however. So as we train at this point here, the, at this point here, there is quite a few activations that are a lot larger, as you can see. And still the vast majority of them are very small. There's a few big ones, they've still got a bright yellow bar at the bottom. The other thing to notice here is what's happened is we've taken those, those stats, those histograms, we've stacked them all up into a single tensor, and then we've taken their log. Now log1p is just log of the number plus 1. That's because we've got zeros here. And so just taking the log is going to kind of let us see the full range more clearly. So that's what the log's for. So basically what we'd really ideally like to see here is that this whole thing should be a kind of, more like a rectangle, you know. The maximum should be, should be not changing very much. There shouldn't be a thick yellow bar at the bottom, but instead it should be a nice even gradient, matching a normal distribution. Each single column of pixels wants to be kind of like a normal distribution. So you know, gradually decreasing the number of activations. That's what we're aiming for. There's a, another really important and actually easier to read version of this, which is, what if we just took those first two bottom pixels, so the least common 5%, and counted up how many were in, well it's not the, sorry, least common 5%. The least common, the, not least common either, let's try again. In the bottom two pixels, we've got the smallest two equally sized groups of activations. We don't want there to be too many of them, because those are basically dead or nearly dead activations. They're much, much, much smaller than the big ones. And so taking the ratio between those bottom two groups and the total basically tells us what percentage have zero or near zero or extremely small magnitudes. And remember that these are with absolute values. So if we plot those, you can see how bad this is. And in particular, for example, at the final layer, from the, you know, nearly from the very start really, nearly all of the activations are, are entirely, just about entirely disabled. So this is, this is bad news. And if you've got a model where most of your model is close to zero, then most of your model is doing no work. And so it's, it's really, it's really not working. So it may look like at the very end things were improving, but as you can see from this chart, that's not true, right? There's still, the vast majority is still inactive. Generally speaking, I found that if early in training you see this rising crash, rising crash at all, you should stop and restart training. Because this, your model will probably never recover. Too many of the activations have gone off the rails. So we want it to look kind of like this the whole time, but with less of this very thick yellow bar, which is showing us most are inactive. Okay, so that's our activations. So we've got really now all of the kind of key pieces I think we need to be able to flexibly change how we train models, and to understand what's going on inside our models. And so from this point, we've kind of like drilled down as deep as we need to go. And we can now start to come back up again, and, and put together the pieces, building up what are all of the things that are going to help us train models reliably and quickly. And then hopefully we're going to be able to, yeah, successfully create from scratch some really high quality generative models and other models along the way. Okay, I think that's everything for this class. But next class we're going to start looking at things like initialization. It's a really important topic. If you want to do from the revision before, then just make sure that you're very comfortable with things like standard deviations and other stuff like that, because we'll be using that quite a lot for next time. And yeah, thanks for joining me. Look forward to the next lesson. See you gang!", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.84, "text": " Hi there and welcome to lesson 16, where we are working on building our first flexible", "tokens": [50364, 2421, 456, 293, 2928, 281, 6898, 3165, 11, 689, 321, 366, 1364, 322, 2390, 527, 700, 11358, 50806], "temperature": 0.0, "avg_logprob": -0.23662577072779337, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.0008690872928127646}, {"id": 1, "seek": 0, "start": 8.84, "end": 12.040000000000001, "text": " training framework, the learner.", "tokens": [50806, 3097, 8388, 11, 264, 33347, 13, 50966], "temperature": 0.0, "avg_logprob": -0.23662577072779337, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.0008690872928127646}, {"id": 2, "seek": 0, "start": 12.040000000000001, "end": 18.16, "text": " And I've got some very good news, which is that I have thought of a way of doing it a", "tokens": [50966, 400, 286, 600, 658, 512, 588, 665, 2583, 11, 597, 307, 300, 286, 362, 1194, 295, 257, 636, 295, 884, 309, 257, 51272], "temperature": 0.0, "avg_logprob": -0.23662577072779337, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.0008690872928127646}, {"id": 3, "seek": 0, "start": 18.16, "end": 23.16, "text": " little bit more gradually and simply actually than last time.", "tokens": [51272, 707, 857, 544, 13145, 293, 2935, 767, 813, 1036, 565, 13, 51522], "temperature": 0.0, "avg_logprob": -0.23662577072779337, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.0008690872928127646}, {"id": 4, "seek": 0, "start": 23.16, "end": 26.22, "text": " So that should, that should make things a bit easier.", "tokens": [51522, 407, 300, 820, 11, 300, 820, 652, 721, 257, 857, 3571, 13, 51675], "temperature": 0.0, "avg_logprob": -0.23662577072779337, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.0008690872928127646}, {"id": 5, "seek": 0, "start": 26.22, "end": 29.36, "text": " So we're going to take it a bit more step by step.", "tokens": [51675, 407, 321, 434, 516, 281, 747, 309, 257, 857, 544, 1823, 538, 1823, 13, 51832], "temperature": 0.0, "avg_logprob": -0.23662577072779337, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.0008690872928127646}, {"id": 6, "seek": 2936, "start": 29.36, "end": 37.84, "text": " So we're working in the 09 learner notebook today.", "tokens": [50364, 407, 321, 434, 1364, 294, 264, 48729, 33347, 21060, 965, 13, 50788], "temperature": 0.0, "avg_logprob": -0.2840914636288049, "compression_ratio": 1.4233576642335766, "no_speech_prob": 1.3631196452479344e-05}, {"id": 7, "seek": 2936, "start": 37.84, "end": 45.72, "text": " And we've seen already this, this basic callbacks learner.", "tokens": [50788, 400, 321, 600, 1612, 1217, 341, 11, 341, 3875, 818, 17758, 33347, 13, 51182], "temperature": 0.0, "avg_logprob": -0.2840914636288049, "compression_ratio": 1.4233576642335766, "no_speech_prob": 1.3631196452479344e-05}, {"id": 8, "seek": 2936, "start": 45.72, "end": 57.28, "text": " And so the idea is that we've seen so far this learner, which wasn't flexible at all,", "tokens": [51182, 400, 370, 264, 1558, 307, 300, 321, 600, 1612, 370, 1400, 341, 33347, 11, 597, 2067, 380, 11358, 412, 439, 11, 51760], "temperature": 0.0, "avg_logprob": -0.2840914636288049, "compression_ratio": 1.4233576642335766, "no_speech_prob": 1.3631196452479344e-05}, {"id": 9, "seek": 5728, "start": 57.28, "end": 63.14, "text": " but had all the basic pieces, which is we've got a fit method.", "tokens": [50364, 457, 632, 439, 264, 3875, 3755, 11, 597, 307, 321, 600, 658, 257, 3318, 3170, 13, 50657], "temperature": 0.0, "avg_logprob": -0.3023716055828592, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.00048029443132691085}, {"id": 10, "seek": 5728, "start": 63.14, "end": 68.3, "text": " We are hard coding that we can only calculate accuracy and average loss.", "tokens": [50657, 492, 366, 1152, 17720, 300, 321, 393, 787, 8873, 14170, 293, 4274, 4470, 13, 50915], "temperature": 0.0, "avg_logprob": -0.3023716055828592, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.00048029443132691085}, {"id": 11, "seek": 5728, "start": 68.3, "end": 74.02000000000001, "text": " We're hard coding, we're putting things on a default device.", "tokens": [50915, 492, 434, 1152, 17720, 11, 321, 434, 3372, 721, 322, 257, 7576, 4302, 13, 51201], "temperature": 0.0, "avg_logprob": -0.3023716055828592, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.00048029443132691085}, {"id": 12, "seek": 5728, "start": 74.02000000000001, "end": 75.7, "text": " Hard coding a single learning rate.", "tokens": [51201, 11817, 17720, 257, 2167, 2539, 3314, 13, 51285], "temperature": 0.0, "avg_logprob": -0.3023716055828592, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.00048029443132691085}, {"id": 13, "seek": 5728, "start": 75.7, "end": 84.86, "text": " But the basic idea is here we go through each epoch and call one epoch to train or evaluate,", "tokens": [51285, 583, 264, 3875, 1558, 307, 510, 321, 352, 807, 1184, 30992, 339, 293, 818, 472, 30992, 339, 281, 3847, 420, 13059, 11, 51743], "temperature": 0.0, "avg_logprob": -0.3023716055828592, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.00048029443132691085}, {"id": 14, "seek": 5728, "start": 84.86, "end": 86.62, "text": " depending on this flag.", "tokens": [51743, 5413, 322, 341, 7166, 13, 51831], "temperature": 0.0, "avg_logprob": -0.3023716055828592, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.00048029443132691085}, {"id": 15, "seek": 8662, "start": 86.96000000000001, "end": 90.64, "text": " And then we loop through each batch in the data loader.", "tokens": [50381, 400, 550, 321, 6367, 807, 1184, 15245, 294, 264, 1412, 3677, 260, 13, 50565], "temperature": 0.0, "avg_logprob": -0.2697589071173417, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.0006771920016035438}, {"id": 16, "seek": 8662, "start": 90.64, "end": 97.0, "text": " And one batch is going to grab the x and y parts of the batch, call the model, call the", "tokens": [50565, 400, 472, 15245, 307, 516, 281, 4444, 264, 2031, 293, 288, 3166, 295, 264, 15245, 11, 818, 264, 2316, 11, 818, 264, 50883], "temperature": 0.0, "avg_logprob": -0.2697589071173417, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.0006771920016035438}, {"id": 17, "seek": 8662, "start": 97.0, "end": 103.36000000000001, "text": " loss function, and if we're training do the backward pass.", "tokens": [50883, 4470, 2445, 11, 293, 498, 321, 434, 3097, 360, 264, 23897, 1320, 13, 51201], "temperature": 0.0, "avg_logprob": -0.2697589071173417, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.0006771920016035438}, {"id": 18, "seek": 8662, "start": 103.36000000000001, "end": 110.08000000000001, "text": " And then print out, well calculate the statistics for our accuracy, and then at the end of an", "tokens": [51201, 400, 550, 4482, 484, 11, 731, 8873, 264, 12523, 337, 527, 14170, 11, 293, 550, 412, 264, 917, 295, 364, 51537], "temperature": 0.0, "avg_logprob": -0.2697589071173417, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.0006771920016035438}, {"id": 19, "seek": 8662, "start": 110.08000000000001, "end": 111.84, "text": " epoch print that out.", "tokens": [51537, 30992, 339, 4482, 300, 484, 13, 51625], "temperature": 0.0, "avg_logprob": -0.2697589071173417, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.0006771920016035438}, {"id": 20, "seek": 8662, "start": 111.84, "end": 114.60000000000001, "text": " So it wasn't very flexible.", "tokens": [51625, 407, 309, 2067, 380, 588, 11358, 13, 51763], "temperature": 0.0, "avg_logprob": -0.2697589071173417, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.0006771920016035438}, {"id": 21, "seek": 11460, "start": 114.6, "end": 116.83999999999999, "text": " But it did do something.", "tokens": [50364, 583, 309, 630, 360, 746, 13, 50476], "temperature": 0.0, "avg_logprob": -0.23174285888671875, "compression_ratio": 2.051948051948052, "no_speech_prob": 0.05664597824215889}, {"id": 22, "seek": 11460, "start": 116.83999999999999, "end": 118.02, "text": " So that's good.", "tokens": [50476, 407, 300, 311, 665, 13, 50535], "temperature": 0.0, "avg_logprob": -0.23174285888671875, "compression_ratio": 2.051948051948052, "no_speech_prob": 0.05664597824215889}, {"id": 23, "seek": 11460, "start": 118.02, "end": 121.0, "text": " So what we're going to do now is we're going to do as an intermediate step, we're going", "tokens": [50535, 407, 437, 321, 434, 516, 281, 360, 586, 307, 321, 434, 516, 281, 360, 382, 364, 19376, 1823, 11, 321, 434, 516, 50684], "temperature": 0.0, "avg_logprob": -0.23174285888671875, "compression_ratio": 2.051948051948052, "no_speech_prob": 0.05664597824215889}, {"id": 24, "seek": 11460, "start": 121.0, "end": 125.03999999999999, "text": " to look at a, what I'm calling a basic callbacks learner.", "tokens": [50684, 281, 574, 412, 257, 11, 437, 286, 478, 5141, 257, 3875, 818, 17758, 33347, 13, 50886], "temperature": 0.0, "avg_logprob": -0.23174285888671875, "compression_ratio": 2.051948051948052, "no_speech_prob": 0.05664597824215889}, {"id": 25, "seek": 11460, "start": 125.03999999999999, "end": 128.12, "text": " And it actually has nearly all the functionality of the full thing.", "tokens": [50886, 400, 309, 767, 575, 6217, 439, 264, 14980, 295, 264, 1577, 551, 13, 51040], "temperature": 0.0, "avg_logprob": -0.23174285888671875, "compression_ratio": 2.051948051948052, "no_speech_prob": 0.05664597824215889}, {"id": 26, "seek": 11460, "start": 128.12, "end": 131.94, "text": " Then we're going to, after we look at this basic callbacks learner, we're then going", "tokens": [51040, 1396, 321, 434, 516, 281, 11, 934, 321, 574, 412, 341, 3875, 818, 17758, 33347, 11, 321, 434, 550, 516, 51231], "temperature": 0.0, "avg_logprob": -0.23174285888671875, "compression_ratio": 2.051948051948052, "no_speech_prob": 0.05664597824215889}, {"id": 27, "seek": 11460, "start": 131.94, "end": 137.28, "text": " to, after creating some callbacks and metrics, we're going to look at something called the", "tokens": [51231, 281, 11, 934, 4084, 512, 818, 17758, 293, 16367, 11, 321, 434, 516, 281, 574, 412, 746, 1219, 264, 51498], "temperature": 0.0, "avg_logprob": -0.23174285888671875, "compression_ratio": 2.051948051948052, "no_speech_prob": 0.05664597824215889}, {"id": 28, "seek": 11460, "start": 137.28, "end": 139.51999999999998, "text": " flexible learner.", "tokens": [51498, 11358, 33347, 13, 51610], "temperature": 0.0, "avg_logprob": -0.23174285888671875, "compression_ratio": 2.051948051948052, "no_speech_prob": 0.05664597824215889}, {"id": 29, "seek": 11460, "start": 139.51999999999998, "end": 142.7, "text": " So let's go step by step.", "tokens": [51610, 407, 718, 311, 352, 1823, 538, 1823, 13, 51769], "temperature": 0.0, "avg_logprob": -0.23174285888671875, "compression_ratio": 2.051948051948052, "no_speech_prob": 0.05664597824215889}, {"id": 30, "seek": 14270, "start": 142.79999999999998, "end": 149.98, "text": " So the basic callbacks learner looks very similar to the previous learner.", "tokens": [50369, 407, 264, 3875, 818, 17758, 33347, 1542, 588, 2531, 281, 264, 3894, 33347, 13, 50728], "temperature": 0.0, "avg_logprob": -0.2518337474149816, "compression_ratio": 1.4776119402985075, "no_speech_prob": 0.00016093002341222018}, {"id": 31, "seek": 14270, "start": 149.98, "end": 161.42, "text": " It's got a fit function, which is going to go through each epoch, calling one epoch with", "tokens": [50728, 467, 311, 658, 257, 3318, 2445, 11, 597, 307, 516, 281, 352, 807, 1184, 30992, 339, 11, 5141, 472, 30992, 339, 365, 51300], "temperature": 0.0, "avg_logprob": -0.2518337474149816, "compression_ratio": 1.4776119402985075, "no_speech_prob": 0.00016093002341222018}, {"id": 32, "seek": 14270, "start": 161.42, "end": 164.17999999999998, "text": " training on and then training off.", "tokens": [51300, 3097, 322, 293, 550, 3097, 766, 13, 51438], "temperature": 0.0, "avg_logprob": -0.2518337474149816, "compression_ratio": 1.4776119402985075, "no_speech_prob": 0.00016093002341222018}, {"id": 33, "seek": 16418, "start": 164.18, "end": 173.54000000000002, "text": " And then one epoch will go through each batch and call one batch, and one batch will call", "tokens": [50364, 400, 550, 472, 30992, 339, 486, 352, 807, 1184, 15245, 293, 818, 472, 15245, 11, 293, 472, 15245, 486, 818, 50832], "temperature": 0.0, "avg_logprob": -0.2303170424241286, "compression_ratio": 1.5829383886255923, "no_speech_prob": 0.0247944388538599}, {"id": 34, "seek": 16418, "start": 173.54000000000002, "end": 181.06, "text": " the model, the loss function, and if we're training it will do the backward step.", "tokens": [50832, 264, 2316, 11, 264, 4470, 2445, 11, 293, 498, 321, 434, 3097, 309, 486, 360, 264, 23897, 1823, 13, 51208], "temperature": 0.0, "avg_logprob": -0.2303170424241286, "compression_ratio": 1.5829383886255923, "no_speech_prob": 0.0247944388538599}, {"id": 35, "seek": 16418, "start": 181.06, "end": 182.3, "text": " So that's all pretty similar.", "tokens": [51208, 407, 300, 311, 439, 1238, 2531, 13, 51270], "temperature": 0.0, "avg_logprob": -0.2303170424241286, "compression_ratio": 1.5829383886255923, "no_speech_prob": 0.0247944388538599}, {"id": 36, "seek": 16418, "start": 182.3, "end": 184.42000000000002, "text": " But there's a few more things going on here.", "tokens": [51270, 583, 456, 311, 257, 1326, 544, 721, 516, 322, 510, 13, 51376], "temperature": 0.0, "avg_logprob": -0.2303170424241286, "compression_ratio": 1.5829383886255923, "no_speech_prob": 0.0247944388538599}, {"id": 37, "seek": 16418, "start": 184.42000000000002, "end": 191.58, "text": " For example, if we have a look at fit, you'll see that after creating the optimizer, so", "tokens": [51376, 1171, 1365, 11, 498, 321, 362, 257, 574, 412, 3318, 11, 291, 603, 536, 300, 934, 4084, 264, 5028, 6545, 11, 370, 51734], "temperature": 0.0, "avg_logprob": -0.2303170424241286, "compression_ratio": 1.5829383886255923, "no_speech_prob": 0.0247944388538599}, {"id": 38, "seek": 19158, "start": 191.58, "end": 193.3, "text": " we call self.optfunc.", "tokens": [50364, 321, 818, 2698, 13, 5747, 15930, 66, 13, 50450], "temperature": 0.0, "avg_logprob": -0.26808556357582847, "compression_ratio": 1.558974358974359, "no_speech_prob": 0.0016229613684117794}, {"id": 39, "seek": 19158, "start": 193.3, "end": 200.38000000000002, "text": " So optfunc here defaults to SGD.", "tokens": [50450, 407, 2427, 15930, 66, 510, 7576, 82, 281, 34520, 35, 13, 50804], "temperature": 0.0, "avg_logprob": -0.26808556357582847, "compression_ratio": 1.558974358974359, "no_speech_prob": 0.0016229613684117794}, {"id": 40, "seek": 19158, "start": 200.38000000000002, "end": 205.46, "text": " So we instantiate an SGD object passing in our model's parameters and the requested learning", "tokens": [50804, 407, 321, 9836, 13024, 364, 34520, 35, 2657, 8437, 294, 527, 2316, 311, 9834, 293, 264, 16436, 2539, 51058], "temperature": 0.0, "avg_logprob": -0.26808556357582847, "compression_ratio": 1.558974358974359, "no_speech_prob": 0.0016229613684117794}, {"id": 41, "seek": 19158, "start": 205.46, "end": 206.76000000000002, "text": " rate.", "tokens": [51058, 3314, 13, 51123], "temperature": 0.0, "avg_logprob": -0.26808556357582847, "compression_ratio": 1.558974358974359, "no_speech_prob": 0.0016229613684117794}, {"id": 42, "seek": 19158, "start": 206.76000000000002, "end": 214.78, "text": " And then before we start looping through one epoch at a time, now we've set epochs here,", "tokens": [51123, 400, 550, 949, 321, 722, 6367, 278, 807, 472, 30992, 339, 412, 257, 565, 11, 586, 321, 600, 992, 30992, 28346, 510, 11, 51524], "temperature": 0.0, "avg_logprob": -0.26808556357582847, "compression_ratio": 1.558974358974359, "no_speech_prob": 0.0016229613684117794}, {"id": 43, "seek": 19158, "start": 214.78, "end": 219.54000000000002, "text": " we first of all call self.callback and passing in before fit.", "tokens": [51524, 321, 700, 295, 439, 818, 2698, 13, 45459, 3207, 293, 8437, 294, 949, 3318, 13, 51762], "temperature": 0.0, "avg_logprob": -0.26808556357582847, "compression_ratio": 1.558974358974359, "no_speech_prob": 0.0016229613684117794}, {"id": 44, "seek": 21954, "start": 219.54, "end": 221.73999999999998, "text": " Now what does that do?", "tokens": [50364, 823, 437, 775, 300, 360, 30, 50474], "temperature": 0.0, "avg_logprob": -0.2543243532595427, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0033244285732507706}, {"id": 45, "seek": 21954, "start": 221.73999999999998, "end": 229.06, "text": " self.callback is here, and it takes a method name, so in this case it's before fit, and", "tokens": [50474, 2698, 13, 45459, 3207, 307, 510, 11, 293, 309, 2516, 257, 3170, 1315, 11, 370, 294, 341, 1389, 309, 311, 949, 3318, 11, 293, 50840], "temperature": 0.0, "avg_logprob": -0.2543243532595427, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0033244285732507706}, {"id": 46, "seek": 21954, "start": 229.06, "end": 232.42, "text": " it calls a function called runCallbacks.", "tokens": [50840, 309, 5498, 257, 2445, 1219, 1190, 46113, 17758, 13, 51008], "temperature": 0.0, "avg_logprob": -0.2543243532595427, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0033244285732507706}, {"id": 47, "seek": 21954, "start": 232.42, "end": 236.98, "text": " It passes in a list of our callbacks and the method name, in this case before fit.", "tokens": [51008, 467, 11335, 294, 257, 1329, 295, 527, 818, 17758, 293, 264, 3170, 1315, 11, 294, 341, 1389, 949, 3318, 13, 51236], "temperature": 0.0, "avg_logprob": -0.2543243532595427, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0033244285732507706}, {"id": 48, "seek": 21954, "start": 236.98, "end": 244.98, "text": " So runCallbacks is something that's going to go for each callback, and it's going to", "tokens": [51236, 407, 1190, 46113, 17758, 307, 746, 300, 311, 516, 281, 352, 337, 1184, 818, 3207, 11, 293, 309, 311, 516, 281, 51636], "temperature": 0.0, "avg_logprob": -0.2543243532595427, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0033244285732507706}, {"id": 49, "seek": 24498, "start": 244.98, "end": 250.73999999999998, "text": " sort them in order of their order attribute.", "tokens": [50364, 1333, 552, 294, 1668, 295, 641, 1668, 19667, 13, 50652], "temperature": 0.0, "avg_logprob": -0.21605799333104547, "compression_ratio": 1.8356164383561644, "no_speech_prob": 0.0792074128985405}, {"id": 50, "seek": 24498, "start": 250.73999999999998, "end": 254.17999999999998, "text": " And so there's a base class for our callbacks which has an order of zero.", "tokens": [50652, 400, 370, 456, 311, 257, 3096, 1508, 337, 527, 818, 17758, 597, 575, 364, 1668, 295, 4018, 13, 50824], "temperature": 0.0, "avg_logprob": -0.21605799333104547, "compression_ratio": 1.8356164383561644, "no_speech_prob": 0.0792074128985405}, {"id": 51, "seek": 24498, "start": 254.17999999999998, "end": 259.58, "text": " So our callbacks are all going to have the same order of zero, unless you ask otherwise.", "tokens": [50824, 407, 527, 818, 17758, 366, 439, 516, 281, 362, 264, 912, 1668, 295, 4018, 11, 5969, 291, 1029, 5911, 13, 51094], "temperature": 0.0, "avg_logprob": -0.21605799333104547, "compression_ratio": 1.8356164383561644, "no_speech_prob": 0.0792074128985405}, {"id": 52, "seek": 24498, "start": 259.58, "end": 263.68, "text": " So here's an example of a callback.", "tokens": [51094, 407, 510, 311, 364, 1365, 295, 257, 818, 3207, 13, 51299], "temperature": 0.0, "avg_logprob": -0.21605799333104547, "compression_ratio": 1.8356164383561644, "no_speech_prob": 0.0792074128985405}, {"id": 53, "seek": 24498, "start": 263.68, "end": 268.09999999999997, "text": " So before we look at how callbacks work, let's just run a callback.", "tokens": [51299, 407, 949, 321, 574, 412, 577, 818, 17758, 589, 11, 718, 311, 445, 1190, 257, 818, 3207, 13, 51520], "temperature": 0.0, "avg_logprob": -0.21605799333104547, "compression_ratio": 1.8356164383561644, "no_speech_prob": 0.0792074128985405}, {"id": 54, "seek": 24498, "start": 268.09999999999997, "end": 274.21999999999997, "text": " So we can create a ridiculously simple callback called completionCallback, which before we", "tokens": [51520, 407, 321, 393, 1884, 257, 41358, 2199, 818, 3207, 1219, 19372, 46113, 3207, 11, 597, 949, 321, 51826], "temperature": 0.0, "avg_logprob": -0.21605799333104547, "compression_ratio": 1.8356164383561644, "no_speech_prob": 0.0792074128985405}, {"id": 55, "seek": 27422, "start": 274.46000000000004, "end": 279.14000000000004, "text": " start fitting a new model, it will set its count attribute to zero.", "tokens": [50376, 722, 15669, 257, 777, 2316, 11, 309, 486, 992, 1080, 1207, 19667, 281, 4018, 13, 50610], "temperature": 0.0, "avg_logprob": -0.2567849806797357, "compression_ratio": 1.6134020618556701, "no_speech_prob": 0.004007244482636452}, {"id": 56, "seek": 27422, "start": 279.14000000000004, "end": 284.54, "text": " After each batch it will increment that, and after completing the fitting process it will", "tokens": [50610, 2381, 1184, 15245, 309, 486, 26200, 300, 11, 293, 934, 19472, 264, 15669, 1399, 309, 486, 50880], "temperature": 0.0, "avg_logprob": -0.2567849806797357, "compression_ratio": 1.6134020618556701, "no_speech_prob": 0.004007244482636452}, {"id": 57, "seek": 27422, "start": 284.54, "end": 287.26000000000005, "text": " print out how many batches we've done.", "tokens": [50880, 4482, 484, 577, 867, 15245, 279, 321, 600, 1096, 13, 51016], "temperature": 0.0, "avg_logprob": -0.2567849806797357, "compression_ratio": 1.6134020618556701, "no_speech_prob": 0.004007244482636452}, {"id": 58, "seek": 27422, "start": 287.26000000000005, "end": 294.26000000000005, "text": " So before we even train a model we could just run manually before fit, after batch, and", "tokens": [51016, 407, 949, 321, 754, 3847, 257, 2316, 321, 727, 445, 1190, 16945, 949, 3318, 11, 934, 15245, 11, 293, 51366], "temperature": 0.0, "avg_logprob": -0.2567849806797357, "compression_ratio": 1.6134020618556701, "no_speech_prob": 0.004007244482636452}, {"id": 59, "seek": 27422, "start": 294.26000000000005, "end": 300.16, "text": " after fit using this runCBs.", "tokens": [51366, 934, 3318, 1228, 341, 1190, 34, 33, 82, 13, 51661], "temperature": 0.0, "avg_logprob": -0.2567849806797357, "compression_ratio": 1.6134020618556701, "no_speech_prob": 0.004007244482636452}, {"id": 60, "seek": 30016, "start": 300.16, "end": 304.6, "text": " And you can see it's ended up saying completed one batches.", "tokens": [50364, 400, 291, 393, 536, 309, 311, 4590, 493, 1566, 7365, 472, 15245, 279, 13, 50586], "temperature": 0.0, "avg_logprob": -0.19623752908969144, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.05261540040373802}, {"id": 61, "seek": 30016, "start": 304.6, "end": 306.40000000000003, "text": " So what did that do?", "tokens": [50586, 407, 437, 630, 300, 360, 30, 50676], "temperature": 0.0, "avg_logprob": -0.19623752908969144, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.05261540040373802}, {"id": 62, "seek": 30016, "start": 306.40000000000003, "end": 312.48, "text": " So it went through each of the CBs in this list, there's only one, so it's going to look", "tokens": [50676, 407, 309, 1437, 807, 1184, 295, 264, 18745, 82, 294, 341, 1329, 11, 456, 311, 787, 472, 11, 370, 309, 311, 516, 281, 574, 50980], "temperature": 0.0, "avg_logprob": -0.19623752908969144, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.05261540040373802}, {"id": 63, "seek": 30016, "start": 312.48, "end": 321.36, "text": " at the one CB, and it's going to try to use getAttr to find an attribute with this name,", "tokens": [50980, 412, 264, 472, 18745, 11, 293, 309, 311, 516, 281, 853, 281, 764, 483, 38151, 81, 281, 915, 364, 19667, 365, 341, 1315, 11, 51424], "temperature": 0.0, "avg_logprob": -0.19623752908969144, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.05261540040373802}, {"id": 64, "seek": 30016, "start": 321.36, "end": 323.76000000000005, "text": " which is before fit.", "tokens": [51424, 597, 307, 949, 3318, 13, 51544], "temperature": 0.0, "avg_logprob": -0.19623752908969144, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.05261540040373802}, {"id": 65, "seek": 30016, "start": 323.76000000000005, "end": 327.84000000000003, "text": " So if we try that manually, so this is the kind of thing I want you to do if you find", "tokens": [51544, 407, 498, 321, 853, 300, 16945, 11, 370, 341, 307, 264, 733, 295, 551, 286, 528, 291, 281, 360, 498, 291, 915, 51748], "temperature": 0.0, "avg_logprob": -0.19623752908969144, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.05261540040373802}, {"id": 66, "seek": 32784, "start": 327.84, "end": 330.35999999999996, "text": " anything difficult to understand, is do it all manually.", "tokens": [50364, 1340, 2252, 281, 1223, 11, 307, 360, 309, 439, 16945, 13, 50490], "temperature": 0.0, "avg_logprob": -0.233792118642522, "compression_ratio": 1.5360824742268042, "no_speech_prob": 0.06465141475200653}, {"id": 67, "seek": 32784, "start": 330.35999999999996, "end": 336.76, "text": " So create a callback, set it to CBs zero, just like you're doing in a loop, right, and", "tokens": [50490, 407, 1884, 257, 818, 3207, 11, 992, 309, 281, 18745, 82, 4018, 11, 445, 411, 291, 434, 884, 294, 257, 6367, 11, 558, 11, 293, 50810], "temperature": 0.0, "avg_logprob": -0.233792118642522, "compression_ratio": 1.5360824742268042, "no_speech_prob": 0.06465141475200653}, {"id": 68, "seek": 32784, "start": 336.76, "end": 351.76, "text": " then find out what happens if we call this, and pass in this, and you'll see it's returned", "tokens": [50810, 550, 915, 484, 437, 2314, 498, 321, 818, 341, 11, 293, 1320, 294, 341, 11, 293, 291, 603, 536, 309, 311, 8752, 51560], "temperature": 0.0, "avg_logprob": -0.233792118642522, "compression_ratio": 1.5360824742268042, "no_speech_prob": 0.06465141475200653}, {"id": 69, "seek": 32784, "start": 351.76, "end": 353.2, "text": " a method.", "tokens": [51560, 257, 3170, 13, 51632], "temperature": 0.0, "avg_logprob": -0.233792118642522, "compression_ratio": 1.5360824742268042, "no_speech_prob": 0.06465141475200653}, {"id": 70, "seek": 32784, "start": 353.2, "end": 355.71999999999997, "text": " And then what happens to that method?", "tokens": [51632, 400, 550, 437, 2314, 281, 300, 3170, 30, 51758], "temperature": 0.0, "avg_logprob": -0.233792118642522, "compression_ratio": 1.5360824742268042, "no_speech_prob": 0.06465141475200653}, {"id": 71, "seek": 32784, "start": 355.71999999999997, "end": 357.26, "text": " It gets called.", "tokens": [51758, 467, 2170, 1219, 13, 51835], "temperature": 0.0, "avg_logprob": -0.233792118642522, "compression_ratio": 1.5360824742268042, "no_speech_prob": 0.06465141475200653}, {"id": 72, "seek": 35726, "start": 357.26, "end": 359.5, "text": " So let's try calling it.", "tokens": [50364, 407, 718, 311, 853, 5141, 309, 13, 50476], "temperature": 0.0, "avg_logprob": -0.2753671823545944, "compression_ratio": 1.535, "no_speech_prob": 0.0022170073352754116}, {"id": 73, "seek": 35726, "start": 359.5, "end": 362.34, "text": " There we are.", "tokens": [50476, 821, 321, 366, 13, 50618], "temperature": 0.0, "avg_logprob": -0.2753671823545944, "compression_ratio": 1.535, "no_speech_prob": 0.0022170073352754116}, {"id": 74, "seek": 35726, "start": 362.34, "end": 365.86, "text": " So that's what happened when we call the before fit, which doesn't do anything very interesting.", "tokens": [50618, 407, 300, 311, 437, 2011, 562, 321, 818, 264, 949, 3318, 11, 597, 1177, 380, 360, 1340, 588, 1880, 13, 50794], "temperature": 0.0, "avg_logprob": -0.2753671823545944, "compression_ratio": 1.535, "no_speech_prob": 0.0022170073352754116}, {"id": 75, "seek": 35726, "start": 365.86, "end": 377.53999999999996, "text": " But if we then call after batch, and then we call after fit, there it is, right.", "tokens": [50794, 583, 498, 321, 550, 818, 934, 15245, 11, 293, 550, 321, 818, 934, 3318, 11, 456, 309, 307, 11, 558, 13, 51378], "temperature": 0.0, "avg_logprob": -0.2753671823545944, "compression_ratio": 1.535, "no_speech_prob": 0.0022170073352754116}, {"id": 76, "seek": 35726, "start": 377.53999999999996, "end": 386.58, "text": " So yeah, make sure you don't just run code willy-nilly, but understand it by experimenting", "tokens": [51378, 407, 1338, 11, 652, 988, 291, 500, 380, 445, 1190, 3089, 486, 88, 12, 77, 6917, 11, 457, 1223, 309, 538, 29070, 51830], "temperature": 0.0, "avg_logprob": -0.2753671823545944, "compression_ratio": 1.535, "no_speech_prob": 0.0022170073352754116}, {"id": 77, "seek": 38658, "start": 386.9, "end": 387.9, "text": " with it.", "tokens": [50380, 365, 309, 13, 50430], "temperature": 0.0, "avg_logprob": -0.2543494561139275, "compression_ratio": 1.7755102040816326, "no_speech_prob": 0.037326592952013016}, {"id": 78, "seek": 38658, "start": 387.9, "end": 391.3, "text": " And I don't always experiment with it myself in these classes, often I'm leaving that to", "tokens": [50430, 400, 286, 500, 380, 1009, 5120, 365, 309, 2059, 294, 613, 5359, 11, 2049, 286, 478, 5012, 300, 281, 50600], "temperature": 0.0, "avg_logprob": -0.2543494561139275, "compression_ratio": 1.7755102040816326, "no_speech_prob": 0.037326592952013016}, {"id": 79, "seek": 38658, "start": 391.3, "end": 395.3, "text": " you, but sometimes I'm trying to give you a sense of how I would experiment with code", "tokens": [50600, 291, 11, 457, 2171, 286, 478, 1382, 281, 976, 291, 257, 2020, 295, 577, 286, 576, 5120, 365, 3089, 50800], "temperature": 0.0, "avg_logprob": -0.2543494561139275, "compression_ratio": 1.7755102040816326, "no_speech_prob": 0.037326592952013016}, {"id": 80, "seek": 38658, "start": 395.3, "end": 396.74, "text": " if I was learning it.", "tokens": [50800, 498, 286, 390, 2539, 309, 13, 50872], "temperature": 0.0, "avg_logprob": -0.2543494561139275, "compression_ratio": 1.7755102040816326, "no_speech_prob": 0.037326592952013016}, {"id": 81, "seek": 38658, "start": 396.74, "end": 399.3, "text": " So then having done that, I would then go ahead and delete those cells.", "tokens": [50872, 407, 550, 1419, 1096, 300, 11, 286, 576, 550, 352, 2286, 293, 12097, 729, 5438, 13, 51000], "temperature": 0.0, "avg_logprob": -0.2543494561139275, "compression_ratio": 1.7755102040816326, "no_speech_prob": 0.037326592952013016}, {"id": 82, "seek": 38658, "start": 399.3, "end": 406.02, "text": " But you can see I'm using this interactive notebook environment to explore, and learn,", "tokens": [51000, 583, 291, 393, 536, 286, 478, 1228, 341, 15141, 21060, 2823, 281, 6839, 11, 293, 1466, 11, 51336], "temperature": 0.0, "avg_logprob": -0.2543494561139275, "compression_ratio": 1.7755102040816326, "no_speech_prob": 0.037326592952013016}, {"id": 83, "seek": 38658, "start": 406.02, "end": 407.38, "text": " and understand.", "tokens": [51336, 293, 1223, 13, 51404], "temperature": 0.0, "avg_logprob": -0.2543494561139275, "compression_ratio": 1.7755102040816326, "no_speech_prob": 0.037326592952013016}, {"id": 84, "seek": 38658, "start": 407.38, "end": 413.41999999999996, "text": " And so now we've got, and if I haven't created a simple example or something to make it really", "tokens": [51404, 400, 370, 586, 321, 600, 658, 11, 293, 498, 286, 2378, 380, 2942, 257, 2199, 1365, 420, 746, 281, 652, 309, 534, 51706], "temperature": 0.0, "avg_logprob": -0.2543494561139275, "compression_ratio": 1.7755102040816326, "no_speech_prob": 0.037326592952013016}, {"id": 85, "seek": 38658, "start": 413.41999999999996, "end": 415.86, "text": " easy to understand, you should do that, right.", "tokens": [51706, 1858, 281, 1223, 11, 291, 820, 360, 300, 11, 558, 13, 51828], "temperature": 0.0, "avg_logprob": -0.2543494561139275, "compression_ratio": 1.7755102040816326, "no_speech_prob": 0.037326592952013016}, {"id": 86, "seek": 41586, "start": 416.14, "end": 419.58000000000004, "text": " So don't just use what I've already created, or what somebody else has already created.", "tokens": [50378, 407, 500, 380, 445, 764, 437, 286, 600, 1217, 2942, 11, 420, 437, 2618, 1646, 575, 1217, 2942, 13, 50550], "temperature": 0.0, "avg_logprob": -0.2334408712859201, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.035678159445524216}, {"id": 87, "seek": 41586, "start": 419.58000000000004, "end": 421.86, "text": " So we've now got something that works totally independently.", "tokens": [50550, 407, 321, 600, 586, 658, 746, 300, 1985, 3879, 21761, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2334408712859201, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.035678159445524216}, {"id": 88, "seek": 41586, "start": 421.86, "end": 422.94, "text": " We can see how it works.", "tokens": [50664, 492, 393, 536, 577, 309, 1985, 13, 50718], "temperature": 0.0, "avg_logprob": -0.2334408712859201, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.035678159445524216}, {"id": 89, "seek": 41586, "start": 422.94, "end": 424.94, "text": " This is what a callback does.", "tokens": [50718, 639, 307, 437, 257, 818, 3207, 775, 13, 50818], "temperature": 0.0, "avg_logprob": -0.2334408712859201, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.035678159445524216}, {"id": 90, "seek": 41586, "start": 424.94, "end": 430.98, "text": " So a callback is something which will look at a class, a callback is a class, where you", "tokens": [50818, 407, 257, 818, 3207, 307, 746, 597, 486, 574, 412, 257, 1508, 11, 257, 818, 3207, 307, 257, 1508, 11, 689, 291, 51120], "temperature": 0.0, "avg_logprob": -0.2334408712859201, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.035678159445524216}, {"id": 91, "seek": 41586, "start": 430.98, "end": 441.48, "text": " can define one or more of before after fit, before after batch, and before after epoch.", "tokens": [51120, 393, 6964, 472, 420, 544, 295, 949, 934, 3318, 11, 949, 934, 15245, 11, 293, 949, 934, 30992, 339, 13, 51645], "temperature": 0.0, "avg_logprob": -0.2334408712859201, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.035678159445524216}, {"id": 92, "seek": 44148, "start": 441.48, "end": 447.92, "text": " So it's going to go through and run all the callbacks that have a before fit method, before", "tokens": [50364, 407, 309, 311, 516, 281, 352, 807, 293, 1190, 439, 264, 818, 17758, 300, 362, 257, 949, 3318, 3170, 11, 949, 50686], "temperature": 0.0, "avg_logprob": -0.2539156133478338, "compression_ratio": 1.644578313253012, "no_speech_prob": 0.0024343181867152452}, {"id": 93, "seek": 44148, "start": 447.92, "end": 448.92, "text": " we start fitting.", "tokens": [50686, 321, 722, 15669, 13, 50736], "temperature": 0.0, "avg_logprob": -0.2539156133478338, "compression_ratio": 1.644578313253012, "no_speech_prob": 0.0024343181867152452}, {"id": 94, "seek": 44148, "start": 448.92, "end": 454.76, "text": " Then it'll go through each epoch, and call one epoch with training, and one epoch with", "tokens": [50736, 1396, 309, 603, 352, 807, 1184, 30992, 339, 11, 293, 818, 472, 30992, 339, 365, 3097, 11, 293, 472, 30992, 339, 365, 51028], "temperature": 0.0, "avg_logprob": -0.2539156133478338, "compression_ratio": 1.644578313253012, "no_speech_prob": 0.0024343181867152452}, {"id": 95, "seek": 44148, "start": 454.76, "end": 456.20000000000005, "text": " evaluation.", "tokens": [51028, 13344, 13, 51100], "temperature": 0.0, "avg_logprob": -0.2539156133478338, "compression_ratio": 1.644578313253012, "no_speech_prob": 0.0024343181867152452}, {"id": 96, "seek": 44148, "start": 456.20000000000005, "end": 463.32, "text": " And then when that's all done, it will call after fit callbacks.", "tokens": [51100, 400, 550, 562, 300, 311, 439, 1096, 11, 309, 486, 818, 934, 3318, 818, 17758, 13, 51456], "temperature": 0.0, "avg_logprob": -0.2539156133478338, "compression_ratio": 1.644578313253012, "no_speech_prob": 0.0024343181867152452}, {"id": 97, "seek": 46332, "start": 463.32, "end": 472.84, "text": " And one epoch will, before it starts on enumerating through the batches, it will call before epoch.", "tokens": [50364, 400, 472, 30992, 339, 486, 11, 949, 309, 3719, 322, 465, 15583, 990, 807, 264, 15245, 279, 11, 309, 486, 818, 949, 30992, 339, 13, 50840], "temperature": 0.0, "avg_logprob": -0.26506816257130017, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.051840368658304214}, {"id": 98, "seek": 46332, "start": 472.84, "end": 477.04, "text": " And when it's done, it will call after epoch.", "tokens": [50840, 400, 562, 309, 311, 1096, 11, 309, 486, 818, 934, 30992, 339, 13, 51050], "temperature": 0.0, "avg_logprob": -0.26506816257130017, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.051840368658304214}, {"id": 99, "seek": 46332, "start": 477.04, "end": 484.36, "text": " The other thing you'll notice is that there's a try, except immediately before every before", "tokens": [51050, 440, 661, 551, 291, 603, 3449, 307, 300, 456, 311, 257, 853, 11, 3993, 4258, 949, 633, 949, 51416], "temperature": 0.0, "avg_logprob": -0.26506816257130017, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.051840368658304214}, {"id": 100, "seek": 46332, "start": 484.36, "end": 487.6, "text": " method, and immediately after every after method.", "tokens": [51416, 3170, 11, 293, 4258, 934, 633, 934, 3170, 13, 51578], "temperature": 0.0, "avg_logprob": -0.26506816257130017, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.051840368658304214}, {"id": 101, "seek": 46332, "start": 487.6, "end": 490.32, "text": " There's a try, and there's an except.", "tokens": [51578, 821, 311, 257, 853, 11, 293, 456, 311, 364, 3993, 13, 51714], "temperature": 0.0, "avg_logprob": -0.26506816257130017, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.051840368658304214}, {"id": 102, "seek": 49032, "start": 490.32, "end": 492.59999999999997, "text": " And each one has a different thing to look for.", "tokens": [50364, 400, 1184, 472, 575, 257, 819, 551, 281, 574, 337, 13, 50478], "temperature": 0.0, "avg_logprob": -0.2095299424796269, "compression_ratio": 1.8054054054054054, "no_speech_prob": 0.0027149359229952097}, {"id": 103, "seek": 49032, "start": 492.59999999999997, "end": 498.52, "text": " Cancel fit exception, cancel epoch exception, and cancel batch exception.", "tokens": [50478, 1664, 4933, 3318, 11183, 11, 10373, 30992, 339, 11183, 11, 293, 10373, 15245, 11183, 13, 50774], "temperature": 0.0, "avg_logprob": -0.2095299424796269, "compression_ratio": 1.8054054054054054, "no_speech_prob": 0.0027149359229952097}, {"id": 104, "seek": 49032, "start": 498.52, "end": 503.88, "text": " So here's the bit which goes through each batch, calls before batch, processes the batch,", "tokens": [50774, 407, 510, 311, 264, 857, 597, 1709, 807, 1184, 15245, 11, 5498, 949, 15245, 11, 7555, 264, 15245, 11, 51042], "temperature": 0.0, "avg_logprob": -0.2095299424796269, "compression_ratio": 1.8054054054054054, "no_speech_prob": 0.0027149359229952097}, {"id": 105, "seek": 49032, "start": 503.88, "end": 504.88, "text": " calls after batch.", "tokens": [51042, 5498, 934, 15245, 13, 51092], "temperature": 0.0, "avg_logprob": -0.2095299424796269, "compression_ratio": 1.8054054054054054, "no_speech_prob": 0.0027149359229952097}, {"id": 106, "seek": 49032, "start": 504.88, "end": 511.28, "text": " And if there's an exception that's of type cancel batch exception, it gets ignored.", "tokens": [51092, 400, 498, 456, 311, 364, 11183, 300, 311, 295, 2010, 10373, 15245, 11183, 11, 309, 2170, 19735, 13, 51412], "temperature": 0.0, "avg_logprob": -0.2095299424796269, "compression_ratio": 1.8054054054054054, "no_speech_prob": 0.0027149359229952097}, {"id": 107, "seek": 49032, "start": 511.28, "end": 513.46, "text": " So what's that for?", "tokens": [51412, 407, 437, 311, 300, 337, 30, 51521], "temperature": 0.0, "avg_logprob": -0.2095299424796269, "compression_ratio": 1.8054054054054054, "no_speech_prob": 0.0027149359229952097}, {"id": 108, "seek": 51346, "start": 513.46, "end": 524.7800000000001, "text": " So the reason we have this is that any of our callbacks could call, could raise, any", "tokens": [50364, 407, 264, 1778, 321, 362, 341, 307, 300, 604, 295, 527, 818, 17758, 727, 818, 11, 727, 5300, 11, 604, 50930], "temperature": 0.0, "avg_logprob": -0.2867909007602268, "compression_ratio": 1.5, "no_speech_prob": 0.003172566881403327}, {"id": 109, "seek": 51346, "start": 524.7800000000001, "end": 527.1800000000001, "text": " one of these three exceptions.", "tokens": [50930, 472, 295, 613, 1045, 22847, 13, 51050], "temperature": 0.0, "avg_logprob": -0.2867909007602268, "compression_ratio": 1.5, "no_speech_prob": 0.003172566881403327}, {"id": 110, "seek": 51346, "start": 527.1800000000001, "end": 532.5, "text": " To say I don't want to do this batch please.", "tokens": [51050, 1407, 584, 286, 500, 380, 528, 281, 360, 341, 15245, 1767, 13, 51316], "temperature": 0.0, "avg_logprob": -0.2867909007602268, "compression_ratio": 1.5, "no_speech_prob": 0.003172566881403327}, {"id": 111, "seek": 51346, "start": 532.5, "end": 535.6800000000001, "text": " So maybe we'll look at an example of that in a moment.", "tokens": [51316, 407, 1310, 321, 603, 574, 412, 364, 1365, 295, 300, 294, 257, 1623, 13, 51475], "temperature": 0.0, "avg_logprob": -0.2867909007602268, "compression_ratio": 1.5, "no_speech_prob": 0.003172566881403327}, {"id": 112, "seek": 51346, "start": 535.6800000000001, "end": 539.5, "text": " So we can now train with this.", "tokens": [51475, 407, 321, 393, 586, 3847, 365, 341, 13, 51666], "temperature": 0.0, "avg_logprob": -0.2867909007602268, "compression_ratio": 1.5, "no_speech_prob": 0.003172566881403327}, {"id": 113, "seek": 53950, "start": 539.5, "end": 543.34, "text": " So let's call, create a little get model function that creates a sequential model with", "tokens": [50364, 407, 718, 311, 818, 11, 1884, 257, 707, 483, 2316, 2445, 300, 7829, 257, 42881, 2316, 365, 50556], "temperature": 0.0, "avg_logprob": -0.2318206787109375, "compression_ratio": 1.7403100775193798, "no_speech_prob": 0.007815721444785595}, {"id": 114, "seek": 53950, "start": 543.34, "end": 547.3, "text": " just some linear layers.", "tokens": [50556, 445, 512, 8213, 7914, 13, 50754], "temperature": 0.0, "avg_logprob": -0.2318206787109375, "compression_ratio": 1.7403100775193798, "no_speech_prob": 0.007815721444785595}, {"id": 115, "seek": 53950, "start": 547.3, "end": 552.66, "text": " And then we'll call fit, and it's not telling us anything interesting because the only callback", "tokens": [50754, 400, 550, 321, 603, 818, 3318, 11, 293, 309, 311, 406, 3585, 505, 1340, 1880, 570, 264, 787, 818, 3207, 51022], "temperature": 0.0, "avg_logprob": -0.2318206787109375, "compression_ratio": 1.7403100775193798, "no_speech_prob": 0.007815721444785595}, {"id": 116, "seek": 53950, "start": 552.66, "end": 557.02, "text": " we added was the completion callback.", "tokens": [51022, 321, 3869, 390, 264, 19372, 818, 3207, 13, 51240], "temperature": 0.0, "avg_logprob": -0.2318206787109375, "compression_ratio": 1.7403100775193798, "no_speech_prob": 0.007815721444785595}, {"id": 117, "seek": 53950, "start": 557.02, "end": 558.02, "text": " That's fine.", "tokens": [51240, 663, 311, 2489, 13, 51290], "temperature": 0.0, "avg_logprob": -0.2318206787109375, "compression_ratio": 1.7403100775193798, "no_speech_prob": 0.007815721444785595}, {"id": 118, "seek": 53950, "start": 558.02, "end": 560.34, "text": " It's, it's training, it's doing something.", "tokens": [51290, 467, 311, 11, 309, 311, 3097, 11, 309, 311, 884, 746, 13, 51406], "temperature": 0.0, "avg_logprob": -0.2318206787109375, "compression_ratio": 1.7403100775193798, "no_speech_prob": 0.007815721444785595}, {"id": 119, "seek": 53950, "start": 560.34, "end": 561.7, "text": " And we now have a trained model.", "tokens": [51406, 400, 321, 586, 362, 257, 8895, 2316, 13, 51474], "temperature": 0.0, "avg_logprob": -0.2318206787109375, "compression_ratio": 1.7403100775193798, "no_speech_prob": 0.007815721444785595}, {"id": 120, "seek": 53950, "start": 561.7, "end": 565.62, "text": " Just didn't print out any metrics or anything because we don't have any callbacks for that.", "tokens": [51474, 1449, 994, 380, 4482, 484, 604, 16367, 420, 1340, 570, 321, 500, 380, 362, 604, 818, 17758, 337, 300, 13, 51670], "temperature": 0.0, "avg_logprob": -0.2318206787109375, "compression_ratio": 1.7403100775193798, "no_speech_prob": 0.007815721444785595}, {"id": 121, "seek": 53950, "start": 565.62, "end": 568.1, "text": " That's the basic idea.", "tokens": [51670, 663, 311, 264, 3875, 1558, 13, 51794], "temperature": 0.0, "avg_logprob": -0.2318206787109375, "compression_ratio": 1.7403100775193798, "no_speech_prob": 0.007815721444785595}, {"id": 122, "seek": 56810, "start": 568.1, "end": 582.0600000000001, "text": " So we could create a, maybe we could call it a single batch callback, which after batch,", "tokens": [50364, 407, 321, 727, 1884, 257, 11, 1310, 321, 727, 818, 309, 257, 2167, 15245, 818, 3207, 11, 597, 934, 15245, 11, 51062], "temperature": 0.0, "avg_logprob": -0.2202133753943065, "compression_ratio": 1.551948051948052, "no_speech_prob": 0.0014550529886037111}, {"id": 123, "seek": 56810, "start": 582.0600000000001, "end": 592.7, "text": " after a single batch, it raises a cancel, cancel fit exception.", "tokens": [51062, 934, 257, 2167, 15245, 11, 309, 19658, 257, 10373, 11, 10373, 3318, 11183, 13, 51594], "temperature": 0.0, "avg_logprob": -0.2202133753943065, "compression_ratio": 1.551948051948052, "no_speech_prob": 0.0014550529886037111}, {"id": 124, "seek": 56810, "start": 592.7, "end": 596.6600000000001, "text": " So that's a pretty, I mean I suppose that could be kind of useful actually if you want", "tokens": [51594, 407, 300, 311, 257, 1238, 11, 286, 914, 286, 7297, 300, 727, 312, 733, 295, 4420, 767, 498, 291, 528, 51792], "temperature": 0.0, "avg_logprob": -0.2202133753943065, "compression_ratio": 1.551948051948052, "no_speech_prob": 0.0014550529886037111}, {"id": 125, "seek": 59666, "start": 596.66, "end": 599.9, "text": " to just run one batch to your model to make sure it works.", "tokens": [50364, 281, 445, 1190, 472, 15245, 281, 428, 2316, 281, 652, 988, 309, 1985, 13, 50526], "temperature": 0.0, "avg_logprob": -0.21680611151236076, "compression_ratio": 1.376, "no_speech_prob": 0.0685347393155098}, {"id": 126, "seek": 59666, "start": 599.9, "end": 607.6999999999999, "text": " So we could try that.", "tokens": [50526, 407, 321, 727, 853, 300, 13, 50916], "temperature": 0.0, "avg_logprob": -0.21680611151236076, "compression_ratio": 1.376, "no_speech_prob": 0.0685347393155098}, {"id": 127, "seek": 59666, "start": 607.6999999999999, "end": 615.38, "text": " So now we're going to add to our list of callbacks the single batch callback.", "tokens": [50916, 407, 586, 321, 434, 516, 281, 909, 281, 527, 1329, 295, 818, 17758, 264, 2167, 15245, 818, 3207, 13, 51300], "temperature": 0.0, "avg_logprob": -0.21680611151236076, "compression_ratio": 1.376, "no_speech_prob": 0.0685347393155098}, {"id": 128, "seek": 59666, "start": 615.38, "end": 619.06, "text": " Let's try it.", "tokens": [51300, 961, 311, 853, 309, 13, 51484], "temperature": 0.0, "avg_logprob": -0.21680611151236076, "compression_ratio": 1.376, "no_speech_prob": 0.0685347393155098}, {"id": 129, "seek": 61906, "start": 619.06, "end": 624.2199999999999, "text": " And in fact, you know, we probably want this, let's just have a think here.", "tokens": [50364, 400, 294, 1186, 11, 291, 458, 11, 321, 1391, 528, 341, 11, 718, 311, 445, 362, 257, 519, 510, 13, 50622], "temperature": 0.0, "avg_logprob": -0.33525797378185185, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.022975973784923553}, {"id": 130, "seek": 61906, "start": 624.2199999999999, "end": 626.5, "text": " Oh, that's fine.", "tokens": [50622, 876, 11, 300, 311, 2489, 13, 50736], "temperature": 0.0, "avg_logprob": -0.33525797378185185, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.022975973784923553}, {"id": 131, "seek": 61906, "start": 626.5, "end": 628.3, "text": " Let's run it.", "tokens": [50736, 961, 311, 1190, 309, 13, 50826], "temperature": 0.0, "avg_logprob": -0.33525797378185185, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.022975973784923553}, {"id": 132, "seek": 61906, "start": 628.3, "end": 630.5799999999999, "text": " There we go.", "tokens": [50826, 821, 321, 352, 13, 50940], "temperature": 0.0, "avg_logprob": -0.33525797378185185, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.022975973784923553}, {"id": 133, "seek": 61906, "start": 630.5799999999999, "end": 632.7399999999999, "text": " So it ran and nothing happened.", "tokens": [50940, 407, 309, 5872, 293, 1825, 2011, 13, 51048], "temperature": 0.0, "avg_logprob": -0.33525797378185185, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.022975973784923553}, {"id": 134, "seek": 61906, "start": 632.7399999999999, "end": 640.14, "text": " And the reason nothing happened is because this cancelled before this ran.", "tokens": [51048, 400, 264, 1778, 1825, 2011, 307, 570, 341, 25103, 949, 341, 5872, 13, 51418], "temperature": 0.0, "avg_logprob": -0.33525797378185185, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.022975973784923553}, {"id": 135, "seek": 61906, "start": 640.14, "end": 645.7199999999999, "text": " So we could make this run second by setting its order to be higher.", "tokens": [51418, 407, 321, 727, 652, 341, 1190, 1150, 538, 3287, 1080, 1668, 281, 312, 2946, 13, 51697], "temperature": 0.0, "avg_logprob": -0.33525797378185185, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.022975973784923553}, {"id": 136, "seek": 64572, "start": 645.72, "end": 652.0400000000001, "text": " And we could say just order equals one because the default order is zero.", "tokens": [50364, 400, 321, 727, 584, 445, 1668, 6915, 472, 570, 264, 7576, 1668, 307, 4018, 13, 50680], "temperature": 0.0, "avg_logprob": -0.35156373244065503, "compression_ratio": 1.48, "no_speech_prob": 0.030675288289785385}, {"id": 137, "seek": 64572, "start": 652.0400000000001, "end": 659.12, "text": " And we sort in order of the order attribute.", "tokens": [50680, 400, 321, 1333, 294, 1668, 295, 264, 1668, 19667, 13, 51034], "temperature": 0.0, "avg_logprob": -0.35156373244065503, "compression_ratio": 1.48, "no_speech_prob": 0.030675288289785385}, {"id": 138, "seek": 64572, "start": 659.12, "end": 663.48, "text": " Actually let's use cancel epoch exception.", "tokens": [51034, 5135, 718, 311, 764, 10373, 30992, 339, 11183, 13, 51252], "temperature": 0.0, "avg_logprob": -0.35156373244065503, "compression_ratio": 1.48, "no_speech_prob": 0.030675288289785385}, {"id": 139, "seek": 64572, "start": 663.48, "end": 666.0, "text": " There we go.", "tokens": [51252, 821, 321, 352, 13, 51378], "temperature": 0.0, "avg_logprob": -0.35156373244065503, "compression_ratio": 1.48, "no_speech_prob": 0.030675288289785385}, {"id": 140, "seek": 64572, "start": 666.0, "end": 668.24, "text": " That way it'll run the final fit.", "tokens": [51378, 663, 636, 309, 603, 1190, 264, 2572, 3318, 13, 51490], "temperature": 0.0, "avg_logprob": -0.35156373244065503, "compression_ratio": 1.48, "no_speech_prob": 0.030675288289785385}, {"id": 141, "seek": 64572, "start": 668.24, "end": 671.5600000000001, "text": " There we are.", "tokens": [51490, 821, 321, 366, 13, 51656], "temperature": 0.0, "avg_logprob": -0.35156373244065503, "compression_ratio": 1.48, "no_speech_prob": 0.030675288289785385}, {"id": 142, "seek": 67156, "start": 671.56, "end": 680.8, "text": " So it did one batch for the, it did one batch for the training and one batch for the evaluation.", "tokens": [50364, 407, 309, 630, 472, 15245, 337, 264, 11, 309, 630, 472, 15245, 337, 264, 3097, 293, 472, 15245, 337, 264, 13344, 13, 50826], "temperature": 0.0, "avg_logprob": -0.2293190533601785, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.0065896776504814625}, {"id": 143, "seek": 67156, "start": 680.8, "end": 683.56, "text": " So that's a total of two batches.", "tokens": [50826, 407, 300, 311, 257, 3217, 295, 732, 15245, 279, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2293190533601785, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.0065896776504814625}, {"id": 144, "seek": 67156, "start": 683.56, "end": 689.92, "text": " So remember callbacks are not a special magic part of like the Python language or anything.", "tokens": [50964, 407, 1604, 818, 17758, 366, 406, 257, 2121, 5585, 644, 295, 411, 264, 15329, 2856, 420, 1340, 13, 51282], "temperature": 0.0, "avg_logprob": -0.2293190533601785, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.0065896776504814625}, {"id": 145, "seek": 67156, "start": 689.92, "end": 697.04, "text": " It's just a name we use to refer to these functions or classes or callables more accurately", "tokens": [51282, 467, 311, 445, 257, 1315, 321, 764, 281, 2864, 281, 613, 6828, 420, 5359, 420, 818, 2965, 544, 20095, 51638], "temperature": 0.0, "avg_logprob": -0.2293190533601785, "compression_ratio": 1.6702127659574468, "no_speech_prob": 0.0065896776504814625}, {"id": 146, "seek": 69704, "start": 697.04, "end": 704.5999999999999, "text": " that we, that we pass into something that will then call back to that callable at particular", "tokens": [50364, 300, 321, 11, 300, 321, 1320, 666, 746, 300, 486, 550, 818, 646, 281, 300, 818, 712, 412, 1729, 50742], "temperature": 0.0, "avg_logprob": -0.2517530200550857, "compression_ratio": 1.9054726368159205, "no_speech_prob": 0.028870481997728348}, {"id": 147, "seek": 69704, "start": 704.5999999999999, "end": 705.5999999999999, "text": " times.", "tokens": [50742, 1413, 13, 50792], "temperature": 0.0, "avg_logprob": -0.2517530200550857, "compression_ratio": 1.9054726368159205, "no_speech_prob": 0.028870481997728348}, {"id": 148, "seek": 69704, "start": 705.5999999999999, "end": 708.64, "text": " And I think these are kind of interesting kinds of callbacks because these callbacks", "tokens": [50792, 400, 286, 519, 613, 366, 733, 295, 1880, 3685, 295, 818, 17758, 570, 613, 818, 17758, 50944], "temperature": 0.0, "avg_logprob": -0.2517530200550857, "compression_ratio": 1.9054726368159205, "no_speech_prob": 0.028870481997728348}, {"id": 149, "seek": 69704, "start": 708.64, "end": 713.1999999999999, "text": " have multiple methods in them.", "tokens": [50944, 362, 3866, 7150, 294, 552, 13, 51172], "temperature": 0.0, "avg_logprob": -0.2517530200550857, "compression_ratio": 1.9054726368159205, "no_speech_prob": 0.028870481997728348}, {"id": 150, "seek": 69704, "start": 713.1999999999999, "end": 714.7199999999999, "text": " So is each method a callback?", "tokens": [51172, 407, 307, 1184, 3170, 257, 818, 3207, 30, 51248], "temperature": 0.0, "avg_logprob": -0.2517530200550857, "compression_ratio": 1.9054726368159205, "no_speech_prob": 0.028870481997728348}, {"id": 151, "seek": 69704, "start": 714.7199999999999, "end": 716.52, "text": " Is each class with all those methods a callback?", "tokens": [51248, 1119, 1184, 1508, 365, 439, 729, 7150, 257, 818, 3207, 30, 51338], "temperature": 0.0, "avg_logprob": -0.2517530200550857, "compression_ratio": 1.9054726368159205, "no_speech_prob": 0.028870481997728348}, {"id": 152, "seek": 69704, "start": 716.52, "end": 717.52, "text": " I don't know.", "tokens": [51338, 286, 500, 380, 458, 13, 51388], "temperature": 0.0, "avg_logprob": -0.2517530200550857, "compression_ratio": 1.9054726368159205, "no_speech_prob": 0.028870481997728348}, {"id": 153, "seek": 69704, "start": 717.52, "end": 721.12, "text": " I tend to think of the class with all the methods in as a single callback.", "tokens": [51388, 286, 3928, 281, 519, 295, 264, 1508, 365, 439, 264, 7150, 294, 382, 257, 2167, 818, 3207, 13, 51568], "temperature": 0.0, "avg_logprob": -0.2517530200550857, "compression_ratio": 1.9054726368159205, "no_speech_prob": 0.028870481997728348}, {"id": 154, "seek": 72112, "start": 721.12, "end": 725.04, "text": " I'm not sure if we have great nomenclature for this.", "tokens": [50364, 286, 478, 406, 988, 498, 321, 362, 869, 297, 4726, 3474, 1503, 337, 341, 13, 50560], "temperature": 0.0, "avg_logprob": -0.24981653465414946, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.025564417243003845}, {"id": 155, "seek": 72112, "start": 725.04, "end": 727.4, "text": " All right.", "tokens": [50560, 1057, 558, 13, 50678], "temperature": 0.0, "avg_logprob": -0.24981653465414946, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.025564417243003845}, {"id": 156, "seek": 72112, "start": 727.4, "end": 730.84, "text": " So let's actually try to get this doing something more interesting by not modifying the learner", "tokens": [50678, 407, 718, 311, 767, 853, 281, 483, 341, 884, 746, 544, 1880, 538, 406, 42626, 264, 33347, 50850], "temperature": 0.0, "avg_logprob": -0.24981653465414946, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.025564417243003845}, {"id": 157, "seek": 72112, "start": 730.84, "end": 735.04, "text": " at all, but just by adding callbacks because that's the great hope of callbacks, right?", "tokens": [50850, 412, 439, 11, 457, 445, 538, 5127, 818, 17758, 570, 300, 311, 264, 869, 1454, 295, 818, 17758, 11, 558, 30, 51060], "temperature": 0.0, "avg_logprob": -0.24981653465414946, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.025564417243003845}, {"id": 158, "seek": 72112, "start": 735.04, "end": 743.88, "text": " So it would be very nice if it told us the accuracy and the loss.", "tokens": [51060, 407, 309, 576, 312, 588, 1481, 498, 309, 1907, 505, 264, 14170, 293, 264, 4470, 13, 51502], "temperature": 0.0, "avg_logprob": -0.24981653465414946, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.025564417243003845}, {"id": 159, "seek": 72112, "start": 743.88, "end": 748.92, "text": " So to do that it would be great to have a class that can keep track of a metric.", "tokens": [51502, 407, 281, 360, 300, 309, 576, 312, 869, 281, 362, 257, 1508, 300, 393, 1066, 2837, 295, 257, 20678, 13, 51754], "temperature": 0.0, "avg_logprob": -0.24981653465414946, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.025564417243003845}, {"id": 160, "seek": 74892, "start": 748.92, "end": 757.5999999999999, "text": " So I've created here a metric class and maybe before we look at it we'll see how it works.", "tokens": [50364, 407, 286, 600, 2942, 510, 257, 20678, 1508, 293, 1310, 949, 321, 574, 412, 309, 321, 603, 536, 577, 309, 1985, 13, 50798], "temperature": 0.0, "avg_logprob": -0.24884134457435136, "compression_ratio": 1.685, "no_speech_prob": 0.03021387942135334}, {"id": 161, "seek": 74892, "start": 757.5999999999999, "end": 763.24, "text": " You could create, for example, an accuracy metric by defining the calculation necessary", "tokens": [50798, 509, 727, 1884, 11, 337, 1365, 11, 364, 14170, 20678, 538, 17827, 264, 17108, 4818, 51080], "temperature": 0.0, "avg_logprob": -0.24884134457435136, "compression_ratio": 1.685, "no_speech_prob": 0.03021387942135334}, {"id": 162, "seek": 74892, "start": 763.24, "end": 770.04, "text": " to calculate the accuracy metric, which is the mean of how often do the inputs equal", "tokens": [51080, 281, 8873, 264, 14170, 20678, 11, 597, 307, 264, 914, 295, 577, 2049, 360, 264, 15743, 2681, 51420], "temperature": 0.0, "avg_logprob": -0.24884134457435136, "compression_ratio": 1.685, "no_speech_prob": 0.03021387942135334}, {"id": 163, "seek": 74892, "start": 770.04, "end": 772.16, "text": " the targets.", "tokens": [51420, 264, 12911, 13, 51526], "temperature": 0.0, "avg_logprob": -0.24884134457435136, "compression_ratio": 1.685, "no_speech_prob": 0.03021387942135334}, {"id": 164, "seek": 74892, "start": 772.16, "end": 775.7199999999999, "text": " The idea is you could then create an accuracy metric object.", "tokens": [51526, 440, 1558, 307, 291, 727, 550, 1884, 364, 14170, 20678, 2657, 13, 51704], "temperature": 0.0, "avg_logprob": -0.24884134457435136, "compression_ratio": 1.685, "no_speech_prob": 0.03021387942135334}, {"id": 165, "seek": 77572, "start": 775.72, "end": 782.12, "text": " You could add a batch of inputs and targets, and add another batch of inputs and targets,", "tokens": [50364, 509, 727, 909, 257, 15245, 295, 15743, 293, 12911, 11, 293, 909, 1071, 15245, 295, 15743, 293, 12911, 11, 50684], "temperature": 0.0, "avg_logprob": -0.23523899189476827, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.002550853416323662}, {"id": 166, "seek": 77572, "start": 782.12, "end": 790.1600000000001, "text": " and get the value, and there you would get the 0.45 accuracy.", "tokens": [50684, 293, 483, 264, 2158, 11, 293, 456, 291, 576, 483, 264, 1958, 13, 8465, 14170, 13, 51086], "temperature": 0.0, "avg_logprob": -0.23523899189476827, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.002550853416323662}, {"id": 167, "seek": 77572, "start": 790.1600000000001, "end": 794.38, "text": " Or another way you could do it would be just to create a metric which simply takes gets", "tokens": [51086, 1610, 1071, 636, 291, 727, 360, 309, 576, 312, 445, 281, 1884, 257, 20678, 597, 2935, 2516, 2170, 51297], "temperature": 0.0, "avg_logprob": -0.23523899189476827, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.002550853416323662}, {"id": 168, "seek": 77572, "start": 794.38, "end": 796.58, "text": " the weighted average, for example, of your loss.", "tokens": [51297, 264, 32807, 4274, 11, 337, 1365, 11, 295, 428, 4470, 13, 51407], "temperature": 0.0, "avg_logprob": -0.23523899189476827, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.002550853416323662}, {"id": 169, "seek": 77572, "start": 796.58, "end": 802.96, "text": " So you could add 0.6 as the loss with a batch size of 32, 0.9 as a loss in a batch size", "tokens": [51407, 407, 291, 727, 909, 1958, 13, 21, 382, 264, 4470, 365, 257, 15245, 2744, 295, 8858, 11, 1958, 13, 24, 382, 257, 4470, 294, 257, 15245, 2744, 51726], "temperature": 0.0, "avg_logprob": -0.23523899189476827, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.002550853416323662}, {"id": 170, "seek": 80296, "start": 802.96, "end": 812.08, "text": " of 2, and then that's going to give us a weighted average loss of 0.62, which is equal to this", "tokens": [50364, 295, 568, 11, 293, 550, 300, 311, 516, 281, 976, 505, 257, 32807, 4274, 4470, 295, 1958, 13, 28052, 11, 597, 307, 2681, 281, 341, 50820], "temperature": 0.0, "avg_logprob": -0.21770203367192695, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.01224104780703783}, {"id": 171, "seek": 80296, "start": 812.08, "end": 814.08, "text": " weighted average calculation.", "tokens": [50820, 32807, 4274, 17108, 13, 50920], "temperature": 0.0, "avg_logprob": -0.21770203367192695, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.01224104780703783}, {"id": 172, "seek": 80296, "start": 814.08, "end": 818.24, "text": " So that's like one way we could kind of make it easy to calculate metrics.", "tokens": [50920, 407, 300, 311, 411, 472, 636, 321, 727, 733, 295, 652, 309, 1858, 281, 8873, 16367, 13, 51128], "temperature": 0.0, "avg_logprob": -0.21770203367192695, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.01224104780703783}, {"id": 173, "seek": 80296, "start": 818.24, "end": 821.2800000000001, "text": " So here's the class.", "tokens": [51128, 407, 510, 311, 264, 1508, 13, 51280], "temperature": 0.0, "avg_logprob": -0.21770203367192695, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.01224104780703783}, {"id": 174, "seek": 80296, "start": 821.2800000000001, "end": 826.76, "text": " Basically we're going to keep track of all of the actual values that we're averaging,", "tokens": [51280, 8537, 321, 434, 516, 281, 1066, 2837, 295, 439, 295, 264, 3539, 4190, 300, 321, 434, 47308, 11, 51554], "temperature": 0.0, "avg_logprob": -0.21770203367192695, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.01224104780703783}, {"id": 175, "seek": 80296, "start": 826.76, "end": 829.08, "text": " and the number in each mini-batch.", "tokens": [51554, 293, 264, 1230, 294, 1184, 8382, 12, 65, 852, 13, 51670], "temperature": 0.0, "avg_logprob": -0.21770203367192695, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.01224104780703783}, {"id": 176, "seek": 82908, "start": 829.1600000000001, "end": 836.64, "text": " So when you add a mini-batch we call calculate, which for example for accuracy, remember this", "tokens": [50368, 407, 562, 291, 909, 257, 8382, 12, 65, 852, 321, 818, 8873, 11, 597, 337, 1365, 337, 14170, 11, 1604, 341, 50742], "temperature": 0.0, "avg_logprob": -0.28331292642129435, "compression_ratio": 1.6033519553072626, "no_speech_prob": 0.026757923886179924}, {"id": 177, "seek": 82908, "start": 836.64, "end": 843.76, "text": " is going to override the parent class's calculate, so it does the calculation here.", "tokens": [50742, 307, 516, 281, 42321, 264, 2596, 1508, 311, 8873, 11, 370, 309, 775, 264, 17108, 510, 13, 51098], "temperature": 0.0, "avg_logprob": -0.28331292642129435, "compression_ratio": 1.6033519553072626, "no_speech_prob": 0.026757923886179924}, {"id": 178, "seek": 82908, "start": 843.76, "end": 847.72, "text": " And then we'll add that to our list of values.", "tokens": [51098, 400, 550, 321, 603, 909, 300, 281, 527, 1329, 295, 4190, 13, 51296], "temperature": 0.0, "avg_logprob": -0.28331292642129435, "compression_ratio": 1.6033519553072626, "no_speech_prob": 0.026757923886179924}, {"id": 179, "seek": 82908, "start": 847.72, "end": 852.9200000000001, "text": " We will add to our list of batch sizes the current batch size.", "tokens": [51296, 492, 486, 909, 281, 527, 1329, 295, 15245, 11602, 264, 2190, 15245, 2744, 13, 51556], "temperature": 0.0, "avg_logprob": -0.28331292642129435, "compression_ratio": 1.6033519553072626, "no_speech_prob": 0.026757923886179924}, {"id": 180, "seek": 85292, "start": 852.92, "end": 861.36, "text": " And then when you calculate the value we will calculate the weighted sum, sorry the", "tokens": [50364, 400, 550, 562, 291, 8873, 264, 2158, 321, 486, 8873, 264, 32807, 2408, 11, 2597, 264, 50786], "temperature": 0.0, "avg_logprob": -0.2839357852935791, "compression_ratio": 1.8373205741626795, "no_speech_prob": 0.02886885218322277}, {"id": 181, "seek": 85292, "start": 861.36, "end": 864.3199999999999, "text": " weighted mean, weighted average.", "tokens": [50786, 32807, 914, 11, 32807, 4274, 13, 50934], "temperature": 0.0, "avg_logprob": -0.2839357852935791, "compression_ratio": 1.8373205741626795, "no_speech_prob": 0.02886885218322277}, {"id": 182, "seek": 85292, "start": 864.3199999999999, "end": 868.9599999999999, "text": " Now notice that here value I didn't have to put parentheses after it, and that's because", "tokens": [50934, 823, 3449, 300, 510, 2158, 286, 994, 380, 362, 281, 829, 34153, 934, 309, 11, 293, 300, 311, 570, 51166], "temperature": 0.0, "avg_logprob": -0.2839357852935791, "compression_ratio": 1.8373205741626795, "no_speech_prob": 0.02886885218322277}, {"id": 183, "seek": 85292, "start": 868.9599999999999, "end": 869.9599999999999, "text": " it's a property.", "tokens": [51166, 309, 311, 257, 4707, 13, 51216], "temperature": 0.0, "avg_logprob": -0.2839357852935791, "compression_ratio": 1.8373205741626795, "no_speech_prob": 0.02886885218322277}, {"id": 184, "seek": 85292, "start": 869.9599999999999, "end": 873.76, "text": " I think we've seen this before, so just to remind you, property just means you don't", "tokens": [51216, 286, 519, 321, 600, 1612, 341, 949, 11, 370, 445, 281, 4160, 291, 11, 4707, 445, 1355, 291, 500, 380, 51406], "temperature": 0.0, "avg_logprob": -0.2839357852935791, "compression_ratio": 1.8373205741626795, "no_speech_prob": 0.02886885218322277}, {"id": 185, "seek": 85292, "start": 873.76, "end": 880.16, "text": " have to put parentheses after it to get it to get the calculation to happen.", "tokens": [51406, 362, 281, 829, 34153, 934, 309, 281, 483, 309, 281, 483, 264, 17108, 281, 1051, 13, 51726], "temperature": 0.0, "avg_logprob": -0.2839357852935791, "compression_ratio": 1.8373205741626795, "no_speech_prob": 0.02886885218322277}, {"id": 186, "seek": 88016, "start": 880.16, "end": 887.76, "text": " All right, so just let me know if anybody's got any questions up to here of course.", "tokens": [50364, 1057, 558, 11, 370, 445, 718, 385, 458, 498, 4472, 311, 658, 604, 1651, 493, 281, 510, 295, 1164, 13, 50744], "temperature": 0.0, "avg_logprob": -0.2658480947667902, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.0015978271840140224}, {"id": 187, "seek": 88016, "start": 887.76, "end": 894.92, "text": " So we now need some way to use this metric in a callback to actually print out.", "tokens": [50744, 407, 321, 586, 643, 512, 636, 281, 764, 341, 20678, 294, 257, 818, 3207, 281, 767, 4482, 484, 13, 51102], "temperature": 0.0, "avg_logprob": -0.2658480947667902, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.0015978271840140224}, {"id": 188, "seek": 88016, "start": 894.92, "end": 898.4, "text": " The first thing I'm going to do though is I'm going to create one more, one useful metric", "tokens": [51102, 440, 700, 551, 286, 478, 516, 281, 360, 1673, 307, 286, 478, 516, 281, 1884, 472, 544, 11, 472, 4420, 20678, 51276], "temperature": 0.0, "avg_logprob": -0.2658480947667902, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.0015978271840140224}, {"id": 189, "seek": 88016, "start": 898.4, "end": 903.68, "text": " first, a very simple one, just two lines of code called the device callback.", "tokens": [51276, 700, 11, 257, 588, 2199, 472, 11, 445, 732, 3876, 295, 3089, 1219, 264, 4302, 818, 3207, 13, 51540], "temperature": 0.0, "avg_logprob": -0.2658480947667902, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.0015978271840140224}, {"id": 190, "seek": 90368, "start": 903.68, "end": 913.4399999999999, "text": " And that is something which is going to allow us to use CUDA or the Apple GPU or whatever,", "tokens": [50364, 400, 300, 307, 746, 597, 307, 516, 281, 2089, 505, 281, 764, 29777, 7509, 420, 264, 6373, 18407, 420, 2035, 11, 50852], "temperature": 0.0, "avg_logprob": -0.25650585398954506, "compression_ratio": 1.5934579439252337, "no_speech_prob": 0.008061905391514301}, {"id": 191, "seek": 90368, "start": 913.4399999999999, "end": 919.04, "text": " without the complications we had before of, you know, how do we have multiple processes", "tokens": [50852, 1553, 264, 26566, 321, 632, 949, 295, 11, 291, 458, 11, 577, 360, 321, 362, 3866, 7555, 51132], "temperature": 0.0, "avg_logprob": -0.25650585398954506, "compression_ratio": 1.5934579439252337, "no_speech_prob": 0.008061905391514301}, {"id": 192, "seek": 90368, "start": 919.04, "end": 923.4799999999999, "text": " in our data loader and also use our device and not have everything fall over.", "tokens": [51132, 294, 527, 1412, 3677, 260, 293, 611, 764, 527, 4302, 293, 406, 362, 1203, 2100, 670, 13, 51354], "temperature": 0.0, "avg_logprob": -0.25650585398954506, "compression_ratio": 1.5934579439252337, "no_speech_prob": 0.008061905391514301}, {"id": 193, "seek": 90368, "start": 923.4799999999999, "end": 930.3599999999999, "text": " So the way we could do it is we could say before fit, put the model onto the default", "tokens": [51354, 407, 264, 636, 321, 727, 360, 309, 307, 321, 727, 584, 949, 3318, 11, 829, 264, 2316, 3911, 264, 7576, 51698], "temperature": 0.0, "avg_logprob": -0.25650585398954506, "compression_ratio": 1.5934579439252337, "no_speech_prob": 0.008061905391514301}, {"id": 194, "seek": 93036, "start": 930.36, "end": 942.24, "text": " device, and before each batch is run, put that batch onto the device.", "tokens": [50364, 4302, 11, 293, 949, 1184, 15245, 307, 1190, 11, 829, 300, 15245, 3911, 264, 4302, 13, 50958], "temperature": 0.0, "avg_logprob": -0.24899003444573817, "compression_ratio": 1.6, "no_speech_prob": 0.016656983643770218}, {"id": 195, "seek": 93036, "start": 942.24, "end": 948.04, "text": " Because look what happened in the, this is really really important, in the learner, absolutely", "tokens": [50958, 1436, 574, 437, 2011, 294, 264, 11, 341, 307, 534, 534, 1021, 11, 294, 264, 33347, 11, 3122, 51248], "temperature": 0.0, "avg_logprob": -0.24899003444573817, "compression_ratio": 1.6, "no_speech_prob": 0.016656983643770218}, {"id": 196, "seek": 93036, "start": 948.04, "end": 953.14, "text": " everything is put inside self dot, which means it's all modifiable.", "tokens": [51248, 1203, 307, 829, 1854, 2698, 5893, 11, 597, 1355, 309, 311, 439, 1072, 30876, 13, 51503], "temperature": 0.0, "avg_logprob": -0.24899003444573817, "compression_ratio": 1.6, "no_speech_prob": 0.016656983643770218}, {"id": 197, "seek": 93036, "start": 953.14, "end": 959.0, "text": " So we go for self dot iteration number comma self dot the batch itself, enumerating the", "tokens": [51503, 407, 321, 352, 337, 2698, 5893, 24784, 1230, 22117, 2698, 5893, 264, 15245, 2564, 11, 465, 15583, 990, 264, 51796], "temperature": 0.0, "avg_logprob": -0.24899003444573817, "compression_ratio": 1.6, "no_speech_prob": 0.016656983643770218}, {"id": 198, "seek": 95900, "start": 959.0, "end": 963.52, "text": " data loader, and then we call one batch.", "tokens": [50364, 1412, 3677, 260, 11, 293, 550, 321, 818, 472, 15245, 13, 50590], "temperature": 0.0, "avg_logprob": -0.23440290957081075, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.011687202379107475}, {"id": 199, "seek": 95900, "start": 963.52, "end": 965.36, "text": " But before it we call the callback.", "tokens": [50590, 583, 949, 309, 321, 818, 264, 818, 3207, 13, 50682], "temperature": 0.0, "avg_logprob": -0.23440290957081075, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.011687202379107475}, {"id": 200, "seek": 95900, "start": 965.36, "end": 968.0, "text": " So we can modify this.", "tokens": [50682, 407, 321, 393, 16927, 341, 13, 50814], "temperature": 0.0, "avg_logprob": -0.23440290957081075, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.011687202379107475}, {"id": 201, "seek": 95900, "start": 968.0, "end": 971.04, "text": " Now how does the callback get access to the learner?", "tokens": [50814, 823, 577, 775, 264, 818, 3207, 483, 2105, 281, 264, 33347, 30, 50966], "temperature": 0.0, "avg_logprob": -0.23440290957081075, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.011687202379107475}, {"id": 202, "seek": 95900, "start": 971.04, "end": 976.92, "text": " Well what actually happens is we go through each of our callbacks and put, set an attribute", "tokens": [50966, 1042, 437, 767, 2314, 307, 321, 352, 807, 1184, 295, 527, 818, 17758, 293, 829, 11, 992, 364, 19667, 51260], "temperature": 0.0, "avg_logprob": -0.23440290957081075, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.011687202379107475}, {"id": 203, "seek": 95900, "start": 976.92, "end": 980.56, "text": " called learn equal to the learner.", "tokens": [51260, 1219, 1466, 2681, 281, 264, 33347, 13, 51442], "temperature": 0.0, "avg_logprob": -0.23440290957081075, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.011687202379107475}, {"id": 204, "seek": 95900, "start": 980.56, "end": 987.84, "text": " And so that means in the callback itself we can say self dot learn dot model.", "tokens": [51442, 400, 370, 300, 1355, 294, 264, 818, 3207, 2564, 321, 393, 584, 2698, 5893, 1466, 5893, 2316, 13, 51806], "temperature": 0.0, "avg_logprob": -0.23440290957081075, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.011687202379107475}, {"id": 205, "seek": 98784, "start": 987.84, "end": 989.76, "text": " And actually we could make this a bit better I think.", "tokens": [50364, 400, 767, 321, 727, 652, 341, 257, 857, 1101, 286, 519, 13, 50460], "temperature": 0.0, "avg_logprob": -0.2593765478024537, "compression_ratio": 1.6974358974358974, "no_speech_prob": 0.08881649374961853}, {"id": 206, "seek": 98784, "start": 989.76, "end": 992.5600000000001, "text": " So make it like maybe you don't want to use the default device.", "tokens": [50460, 407, 652, 309, 411, 1310, 291, 500, 380, 528, 281, 764, 264, 7576, 4302, 13, 50600], "temperature": 0.0, "avg_logprob": -0.2593765478024537, "compression_ratio": 1.6974358974358974, "no_speech_prob": 0.08881649374961853}, {"id": 207, "seek": 98784, "start": 992.5600000000001, "end": 998.88, "text": " So this is where I would be inclined to add a constructor and set device, and we could", "tokens": [50600, 407, 341, 307, 689, 286, 576, 312, 28173, 281, 909, 257, 47479, 293, 992, 4302, 11, 293, 321, 727, 50916], "temperature": 0.0, "avg_logprob": -0.2593765478024537, "compression_ratio": 1.6974358974358974, "no_speech_prob": 0.08881649374961853}, {"id": 208, "seek": 98784, "start": 998.88, "end": 1006.0400000000001, "text": " default it to the default device of course.", "tokens": [50916, 7576, 309, 281, 264, 7576, 4302, 295, 1164, 13, 51274], "temperature": 0.0, "avg_logprob": -0.2593765478024537, "compression_ratio": 1.6974358974358974, "no_speech_prob": 0.08881649374961853}, {"id": 209, "seek": 98784, "start": 1006.0400000000001, "end": 1009.24, "text": " And then we could use that instead.", "tokens": [51274, 400, 550, 321, 727, 764, 300, 2602, 13, 51434], "temperature": 0.0, "avg_logprob": -0.2593765478024537, "compression_ratio": 1.6974358974358974, "no_speech_prob": 0.08881649374961853}, {"id": 210, "seek": 98784, "start": 1009.24, "end": 1010.24, "text": " And that would give us a bit more flexibility.", "tokens": [51434, 400, 300, 576, 976, 505, 257, 857, 544, 12635, 13, 51484], "temperature": 0.0, "avg_logprob": -0.2593765478024537, "compression_ratio": 1.6974358974358974, "no_speech_prob": 0.08881649374961853}, {"id": 211, "seek": 101024, "start": 1010.24, "end": 1018.8, "text": " So if you wanted to train on some different device then you could.", "tokens": [50364, 407, 498, 291, 1415, 281, 3847, 322, 512, 819, 4302, 550, 291, 727, 13, 50792], "temperature": 0.0, "avg_logprob": -0.29073167883831524, "compression_ratio": 1.4064171122994653, "no_speech_prob": 0.16024640202522278}, {"id": 212, "seek": 101024, "start": 1018.8, "end": 1022.16, "text": " I think that might be a slight improvement.", "tokens": [50792, 286, 519, 300, 1062, 312, 257, 4036, 10444, 13, 50960], "temperature": 0.0, "avg_logprob": -0.29073167883831524, "compression_ratio": 1.4064171122994653, "no_speech_prob": 0.16024640202522278}, {"id": 213, "seek": 101024, "start": 1022.16, "end": 1024.6, "text": " Okay so there's a callback we can use to put things on CUDA.", "tokens": [50960, 1033, 370, 456, 311, 257, 818, 3207, 321, 393, 764, 281, 829, 721, 322, 29777, 7509, 13, 51082], "temperature": 0.0, "avg_logprob": -0.29073167883831524, "compression_ratio": 1.4064171122994653, "no_speech_prob": 0.16024640202522278}, {"id": 214, "seek": 101024, "start": 1024.6, "end": 1030.76, "text": " And we could check that it works by just quickly going back to our old learner here, remove", "tokens": [51082, 400, 321, 727, 1520, 300, 309, 1985, 538, 445, 2661, 516, 646, 281, 527, 1331, 33347, 510, 11, 4159, 51390], "temperature": 0.0, "avg_logprob": -0.29073167883831524, "compression_ratio": 1.4064171122994653, "no_speech_prob": 0.16024640202522278}, {"id": 215, "seek": 103076, "start": 1030.76, "end": 1043.16, "text": " the single batch CB, and replace it with device CB.", "tokens": [50364, 264, 2167, 15245, 18745, 11, 293, 7406, 309, 365, 4302, 18745, 13, 50984], "temperature": 0.0, "avg_logprob": -0.2939547845872782, "compression_ratio": 1.3061224489795917, "no_speech_prob": 0.05748860165476799}, {"id": 216, "seek": 103076, "start": 1043.16, "end": 1044.24, "text": " Yep still works.", "tokens": [50984, 7010, 920, 1985, 13, 51038], "temperature": 0.0, "avg_logprob": -0.2939547845872782, "compression_ratio": 1.3061224489795917, "no_speech_prob": 0.05748860165476799}, {"id": 217, "seek": 103076, "start": 1044.24, "end": 1048.8799999999999, "text": " So that's a good sign.", "tokens": [51038, 407, 300, 311, 257, 665, 1465, 13, 51270], "temperature": 0.0, "avg_logprob": -0.2939547845872782, "compression_ratio": 1.3061224489795917, "no_speech_prob": 0.05748860165476799}, {"id": 218, "seek": 103076, "start": 1048.8799999999999, "end": 1050.68, "text": " Okay so now let's do our metrics.", "tokens": [51270, 1033, 370, 586, 718, 311, 360, 527, 16367, 13, 51360], "temperature": 0.0, "avg_logprob": -0.2939547845872782, "compression_ratio": 1.3061224489795917, "no_speech_prob": 0.05748860165476799}, {"id": 219, "seek": 103076, "start": 1050.68, "end": 1057.8, "text": " Now of course we couldn't use metrics until we built them by hand.", "tokens": [51360, 823, 295, 1164, 321, 2809, 380, 764, 16367, 1826, 321, 3094, 552, 538, 1011, 13, 51716], "temperature": 0.0, "avg_logprob": -0.2939547845872782, "compression_ratio": 1.3061224489795917, "no_speech_prob": 0.05748860165476799}, {"id": 220, "seek": 105780, "start": 1057.84, "end": 1061.84, "text": " The good news is we don't have to write every single metric now by hand, because they already", "tokens": [50366, 440, 665, 2583, 307, 321, 500, 380, 362, 281, 2464, 633, 2167, 20678, 586, 538, 1011, 11, 570, 436, 1217, 50566], "temperature": 0.0, "avg_logprob": -0.21975830326909604, "compression_ratio": 1.5427350427350428, "no_speech_prob": 0.0056417821906507015}, {"id": 221, "seek": 105780, "start": 1061.84, "end": 1068.9199999999998, "text": " exist in a fairly new project called TorchEval, which is an official PyTorch project.", "tokens": [50566, 2514, 294, 257, 6457, 777, 1716, 1219, 7160, 339, 36, 3337, 11, 597, 307, 364, 4783, 9953, 51, 284, 339, 1716, 13, 50920], "temperature": 0.0, "avg_logprob": -0.21975830326909604, "compression_ratio": 1.5427350427350428, "no_speech_prob": 0.0056417821906507015}, {"id": 222, "seek": 105780, "start": 1068.9199999999998, "end": 1076.08, "text": " And so TorchEval is something that gives us actually, I came across it after I had created", "tokens": [50920, 400, 370, 7160, 339, 36, 3337, 307, 746, 300, 2709, 505, 767, 11, 286, 1361, 2108, 309, 934, 286, 632, 2942, 51278], "temperature": 0.0, "avg_logprob": -0.21975830326909604, "compression_ratio": 1.5427350427350428, "no_speech_prob": 0.0056417821906507015}, {"id": 223, "seek": 105780, "start": 1076.08, "end": 1083.1399999999999, "text": " my own metric class, but it actually looks pretty similar to the one that I built earlier.", "tokens": [51278, 452, 1065, 20678, 1508, 11, 457, 309, 767, 1542, 1238, 2531, 281, 264, 472, 300, 286, 3094, 3071, 13, 51631], "temperature": 0.0, "avg_logprob": -0.21975830326909604, "compression_ratio": 1.5427350427350428, "no_speech_prob": 0.0056417821906507015}, {"id": 224, "seek": 108314, "start": 1083.14, "end": 1086.6200000000001, "text": " So you can install it with pip.", "tokens": [50364, 407, 291, 393, 3625, 309, 365, 8489, 13, 50538], "temperature": 0.0, "avg_logprob": -0.2665049750525672, "compression_ratio": 1.5991902834008098, "no_speech_prob": 0.09946510940790176}, {"id": 225, "seek": 108314, "start": 1086.6200000000001, "end": 1089.7800000000002, "text": " I'm not sure if it's on Conda yet, but it probably will be soon by the time you see", "tokens": [50538, 286, 478, 406, 988, 498, 309, 311, 322, 383, 12233, 1939, 11, 457, 309, 1391, 486, 312, 2321, 538, 264, 565, 291, 536, 50696], "temperature": 0.0, "avg_logprob": -0.2665049750525672, "compression_ratio": 1.5991902834008098, "no_speech_prob": 0.09946510940790176}, {"id": 226, "seek": 108314, "start": 1089.7800000000002, "end": 1091.5400000000002, "text": " the video.", "tokens": [50696, 264, 960, 13, 50784], "temperature": 0.0, "avg_logprob": -0.2665049750525672, "compression_ratio": 1.5991902834008098, "no_speech_prob": 0.09946510940790176}, {"id": 227, "seek": 108314, "start": 1091.5400000000002, "end": 1096.94, "text": " I think it's pure Python anyway, so it doesn't matter how you install it.", "tokens": [50784, 286, 519, 309, 311, 6075, 15329, 4033, 11, 370, 309, 1177, 380, 1871, 577, 291, 3625, 309, 13, 51054], "temperature": 0.0, "avg_logprob": -0.2665049750525672, "compression_ratio": 1.5991902834008098, "no_speech_prob": 0.09946510940790176}, {"id": 228, "seek": 108314, "start": 1096.94, "end": 1108.3000000000002, "text": " And yeah it has a pretty similar approach, where you call .update and you call .compute.", "tokens": [51054, 400, 1338, 309, 575, 257, 1238, 2531, 3109, 11, 689, 291, 818, 2411, 1010, 17393, 293, 291, 818, 2411, 21541, 1169, 13, 51622], "temperature": 0.0, "avg_logprob": -0.2665049750525672, "compression_ratio": 1.5991902834008098, "no_speech_prob": 0.09946510940790176}, {"id": 229, "seek": 108314, "start": 1108.3000000000002, "end": 1111.38, "text": " So they're slightly different names, but they're basically super similar to the thing that", "tokens": [51622, 407, 436, 434, 4748, 819, 5288, 11, 457, 436, 434, 1936, 1687, 2531, 281, 264, 551, 300, 51776], "temperature": 0.0, "avg_logprob": -0.2665049750525672, "compression_ratio": 1.5991902834008098, "no_speech_prob": 0.09946510940790176}, {"id": 230, "seek": 108314, "start": 1111.38, "end": 1112.98, "text": " we just built.", "tokens": [51776, 321, 445, 3094, 13, 51856], "temperature": 0.0, "avg_logprob": -0.2665049750525672, "compression_ratio": 1.5991902834008098, "no_speech_prob": 0.09946510940790176}, {"id": 231, "seek": 111298, "start": 1113.82, "end": 1118.8600000000001, "text": " So there's a nice good list of metrics to pick from.", "tokens": [50406, 407, 456, 311, 257, 1481, 665, 1329, 295, 16367, 281, 1888, 490, 13, 50658], "temperature": 0.0, "avg_logprob": -0.2675648469191331, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.0037653574254363775}, {"id": 232, "seek": 111298, "start": 1118.8600000000001, "end": 1122.94, "text": " So because we've already built our own now, that means we're allowed to use theirs.", "tokens": [50658, 407, 570, 321, 600, 1217, 3094, 527, 1065, 586, 11, 300, 1355, 321, 434, 4350, 281, 764, 22760, 13, 50862], "temperature": 0.0, "avg_logprob": -0.2675648469191331, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.0037653574254363775}, {"id": 233, "seek": 111298, "start": 1122.94, "end": 1129.98, "text": " So we can import the multi-class accuracy metric and the mean metric.", "tokens": [50862, 407, 321, 393, 974, 264, 4825, 12, 11665, 14170, 20678, 293, 264, 914, 20678, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2675648469191331, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.0037653574254363775}, {"id": 234, "seek": 111298, "start": 1129.98, "end": 1135.46, "text": " And just to show you they look very very similar, if we call multi-class accuracy and we can", "tokens": [51214, 400, 445, 281, 855, 291, 436, 574, 588, 588, 2531, 11, 498, 321, 818, 4825, 12, 11665, 14170, 293, 321, 393, 51488], "temperature": 0.0, "avg_logprob": -0.2675648469191331, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.0037653574254363775}, {"id": 235, "seek": 113546, "start": 1135.46, "end": 1143.38, "text": " pass in a mini batch of inputs and targets and compute, and that all works nicely.", "tokens": [50364, 1320, 294, 257, 8382, 15245, 295, 15743, 293, 12911, 293, 14722, 11, 293, 300, 439, 1985, 9594, 13, 50760], "temperature": 0.0, "avg_logprob": -0.2892158722209039, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.1441294252872467}, {"id": 236, "seek": 113546, "start": 1143.38, "end": 1147.22, "text": " Now these, in fact it's exactly the same as what I wrote, we both added this thing called", "tokens": [50760, 823, 613, 11, 294, 1186, 309, 311, 2293, 264, 912, 382, 437, 286, 4114, 11, 321, 1293, 3869, 341, 551, 1219, 50952], "temperature": 0.0, "avg_logprob": -0.2892158722209039, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.1441294252872467}, {"id": 237, "seek": 113546, "start": 1147.22, "end": 1151.94, "text": " reset, which basically, well, resets it.", "tokens": [50952, 14322, 11, 597, 1936, 11, 731, 11, 725, 1385, 309, 13, 51188], "temperature": 0.0, "avg_logprob": -0.2892158722209039, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.1441294252872467}, {"id": 238, "seek": 113546, "start": 1151.94, "end": 1158.7, "text": " And so obviously we're going to be wanting to do that probably at the start of each epoch.", "tokens": [51188, 400, 370, 2745, 321, 434, 516, 281, 312, 7935, 281, 360, 300, 1391, 412, 264, 722, 295, 1184, 30992, 339, 13, 51526], "temperature": 0.0, "avg_logprob": -0.2892158722209039, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.1441294252872467}, {"id": 239, "seek": 113546, "start": 1158.7, "end": 1165.18, "text": " And so if you reset it and then try to compute, you'll get nan, because you can't get accuracy,", "tokens": [51526, 400, 370, 498, 291, 14322, 309, 293, 550, 853, 281, 14722, 11, 291, 603, 483, 14067, 11, 570, 291, 393, 380, 483, 14170, 11, 51850], "temperature": 0.0, "avg_logprob": -0.2892158722209039, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.1441294252872467}, {"id": 240, "seek": 116518, "start": 1165.18, "end": 1168.98, "text": " because accuracy is meaningless when you don't have any data yet.", "tokens": [50364, 570, 14170, 307, 33232, 562, 291, 500, 380, 362, 604, 1412, 1939, 13, 50554], "temperature": 0.0, "avg_logprob": -0.2788446044921875, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.0007436999585479498}, {"id": 241, "seek": 116518, "start": 1168.98, "end": 1174.8600000000001, "text": " Okay so let's create a metrics callback so we can print out our metrics.", "tokens": [50554, 1033, 370, 718, 311, 1884, 257, 16367, 818, 3207, 370, 321, 393, 4482, 484, 527, 16367, 13, 50848], "temperature": 0.0, "avg_logprob": -0.2788446044921875, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.0007436999585479498}, {"id": 242, "seek": 116518, "start": 1174.8600000000001, "end": 1178.6200000000001, "text": " I've got some ideas to improve this, which maybe I'll do this week, but here's a basic", "tokens": [50848, 286, 600, 658, 512, 3487, 281, 3470, 341, 11, 597, 1310, 286, 603, 360, 341, 1243, 11, 457, 510, 311, 257, 3875, 51036], "temperature": 0.0, "avg_logprob": -0.2788446044921875, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.0007436999585479498}, {"id": 243, "seek": 116518, "start": 1178.6200000000001, "end": 1180.7, "text": " working version.", "tokens": [51036, 1364, 3037, 13, 51140], "temperature": 0.0, "avg_logprob": -0.2788446044921875, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.0007436999585479498}, {"id": 244, "seek": 116518, "start": 1180.7, "end": 1185.0800000000002, "text": " Slightly hacky, but it's not too bad.", "tokens": [51140, 318, 44872, 10339, 88, 11, 457, 309, 311, 406, 886, 1578, 13, 51359], "temperature": 0.0, "avg_logprob": -0.2788446044921875, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.0007436999585479498}, {"id": 245, "seek": 116518, "start": 1185.0800000000002, "end": 1189.46, "text": " So generally speaking, one thing I noticed actually is, I don't know if this is considered", "tokens": [51359, 407, 5101, 4124, 11, 472, 551, 286, 5694, 767, 307, 11, 286, 500, 380, 458, 498, 341, 307, 4888, 51578], "temperature": 0.0, "avg_logprob": -0.2788446044921875, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.0007436999585479498}, {"id": 246, "seek": 118946, "start": 1189.46, "end": 1198.78, "text": " a bug, but a lot of the metrics didn't seem to work correctly in Torch eval when I had", "tokens": [50364, 257, 7426, 11, 457, 257, 688, 295, 264, 16367, 994, 380, 1643, 281, 589, 8944, 294, 7160, 339, 1073, 304, 562, 286, 632, 50830], "temperature": 0.0, "avg_logprob": -0.26707049450242376, "compression_ratio": 1.4976076555023923, "no_speech_prob": 0.17780037224292755}, {"id": 247, "seek": 118946, "start": 1198.78, "end": 1205.44, "text": " tensors that were on the GPU and had requires grad.", "tokens": [50830, 10688, 830, 300, 645, 322, 264, 18407, 293, 632, 7029, 2771, 13, 51163], "temperature": 0.0, "avg_logprob": -0.26707049450242376, "compression_ratio": 1.4976076555023923, "no_speech_prob": 0.17780037224292755}, {"id": 248, "seek": 118946, "start": 1205.44, "end": 1211.38, "text": " So I created a little to CPU function, which I think is very useful, and that's just going", "tokens": [51163, 407, 286, 2942, 257, 707, 281, 13199, 2445, 11, 597, 286, 519, 307, 588, 4420, 11, 293, 300, 311, 445, 516, 51460], "temperature": 0.0, "avg_logprob": -0.26707049450242376, "compression_ratio": 1.4976076555023923, "no_speech_prob": 0.17780037224292755}, {"id": 249, "seek": 118946, "start": 1211.38, "end": 1217.94, "text": " to detach the, so detach takes the tensor and removes all the gradient history, the", "tokens": [51460, 281, 43245, 264, 11, 370, 43245, 2516, 264, 40863, 293, 30445, 439, 264, 16235, 2503, 11, 264, 51788], "temperature": 0.0, "avg_logprob": -0.26707049450242376, "compression_ratio": 1.4976076555023923, "no_speech_prob": 0.17780037224292755}, {"id": 250, "seek": 121794, "start": 1217.94, "end": 1221.5, "text": " computation history used to calculate a gradient, and puts it on the CPU.", "tokens": [50364, 24903, 2503, 1143, 281, 8873, 257, 16235, 11, 293, 8137, 309, 322, 264, 13199, 13, 50542], "temperature": 0.0, "avg_logprob": -0.29501850088847054, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.18242090940475464}, {"id": 251, "seek": 121794, "start": 1221.5, "end": 1229.5800000000002, "text": " That'll do the same for dictionaries of tensors, lists of tensors, and tuples of tensors.", "tokens": [50542, 663, 603, 360, 264, 912, 337, 22352, 4889, 295, 10688, 830, 11, 14511, 295, 10688, 830, 11, 293, 2604, 2622, 295, 10688, 830, 13, 50946], "temperature": 0.0, "avg_logprob": -0.29501850088847054, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.18242090940475464}, {"id": 252, "seek": 121794, "start": 1229.5800000000002, "end": 1233.38, "text": " So our metrics callback, basically here's how we're going to use it.", "tokens": [50946, 407, 527, 16367, 818, 3207, 11, 1936, 510, 311, 577, 321, 434, 516, 281, 764, 309, 13, 51136], "temperature": 0.0, "avg_logprob": -0.29501850088847054, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.18242090940475464}, {"id": 253, "seek": 121794, "start": 1233.38, "end": 1236.2, "text": " So let's run it.", "tokens": [51136, 407, 718, 311, 1190, 309, 13, 51277], "temperature": 0.0, "avg_logprob": -0.29501850088847054, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.18242090940475464}, {"id": 254, "seek": 121794, "start": 1236.2, "end": 1241.7, "text": " So here we're creating a metrics callback object and saying we want to create a metric", "tokens": [51277, 407, 510, 321, 434, 4084, 257, 16367, 818, 3207, 2657, 293, 1566, 321, 528, 281, 1884, 257, 20678, 51552], "temperature": 0.0, "avg_logprob": -0.29501850088847054, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.18242090940475464}, {"id": 255, "seek": 121794, "start": 1241.7, "end": 1243.9, "text": " called accuracy.", "tokens": [51552, 1219, 14170, 13, 51662], "temperature": 0.0, "avg_logprob": -0.29501850088847054, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.18242090940475464}, {"id": 256, "seek": 124390, "start": 1243.9, "end": 1248.7, "text": " That's what's going to print out, and this is the metrics object we're going to use to", "tokens": [50364, 663, 311, 437, 311, 516, 281, 4482, 484, 11, 293, 341, 307, 264, 16367, 2657, 321, 434, 516, 281, 764, 281, 50604], "temperature": 0.0, "avg_logprob": -0.2369753074645996, "compression_ratio": 1.7914691943127963, "no_speech_prob": 0.050328563898801804}, {"id": 257, "seek": 124390, "start": 1248.7, "end": 1256.1000000000001, "text": " calculate accuracy, and so then we just pass that in as one of our callbacks.", "tokens": [50604, 8873, 14170, 11, 293, 370, 550, 321, 445, 1320, 300, 294, 382, 472, 295, 527, 818, 17758, 13, 50974], "temperature": 0.0, "avg_logprob": -0.2369753074645996, "compression_ratio": 1.7914691943127963, "no_speech_prob": 0.050328563898801804}, {"id": 258, "seek": 124390, "start": 1256.1000000000001, "end": 1261.5, "text": " And so you can see what it's going to do is it's going to print out the epoch number,", "tokens": [50974, 400, 370, 291, 393, 536, 437, 309, 311, 516, 281, 360, 307, 309, 311, 516, 281, 4482, 484, 264, 30992, 339, 1230, 11, 51244], "temperature": 0.0, "avg_logprob": -0.2369753074645996, "compression_ratio": 1.7914691943127963, "no_speech_prob": 0.050328563898801804}, {"id": 259, "seek": 124390, "start": 1261.5, "end": 1266.6200000000001, "text": " whether it's training or evaluating, so training set or validation set, and it'll print out", "tokens": [51244, 1968, 309, 311, 3097, 420, 27479, 11, 370, 3097, 992, 420, 24071, 992, 11, 293, 309, 603, 4482, 484, 51500], "temperature": 0.0, "avg_logprob": -0.2369753074645996, "compression_ratio": 1.7914691943127963, "no_speech_prob": 0.050328563898801804}, {"id": 260, "seek": 124390, "start": 1266.6200000000001, "end": 1271.46, "text": " our metrics and our current status.", "tokens": [51500, 527, 16367, 293, 527, 2190, 6558, 13, 51742], "temperature": 0.0, "avg_logprob": -0.2369753074645996, "compression_ratio": 1.7914691943127963, "no_speech_prob": 0.050328563898801804}, {"id": 261, "seek": 127146, "start": 1271.46, "end": 1276.14, "text": " We can simplify that.", "tokens": [50364, 492, 393, 20460, 300, 13, 50598], "temperature": 0.0, "avg_logprob": -0.3778245713975694, "compression_ratio": 1.314516129032258, "no_speech_prob": 0.03621895611286163}, {"id": 262, "seek": 127146, "start": 1276.14, "end": 1279.82, "text": " We don't need to print those bits because it's all in the dictionary now.", "tokens": [50598, 492, 500, 380, 643, 281, 4482, 729, 9239, 570, 309, 311, 439, 294, 264, 25890, 586, 13, 50782], "temperature": 0.0, "avg_logprob": -0.3778245713975694, "compression_ratio": 1.314516129032258, "no_speech_prob": 0.03621895611286163}, {"id": 263, "seek": 127146, "start": 1279.82, "end": 1282.82, "text": " Let's do that.", "tokens": [50782, 961, 311, 360, 300, 13, 50932], "temperature": 0.0, "avg_logprob": -0.3778245713975694, "compression_ratio": 1.314516129032258, "no_speech_prob": 0.03621895611286163}, {"id": 264, "seek": 127146, "start": 1282.82, "end": 1288.8600000000001, "text": " There we go.", "tokens": [50932, 821, 321, 352, 13, 51234], "temperature": 0.0, "avg_logprob": -0.3778245713975694, "compression_ratio": 1.314516129032258, "no_speech_prob": 0.03621895611286163}, {"id": 265, "seek": 127146, "start": 1288.8600000000001, "end": 1291.58, "text": " So let's take a look at how this works.", "tokens": [51234, 407, 718, 311, 747, 257, 574, 412, 577, 341, 1985, 13, 51370], "temperature": 0.0, "avg_logprob": -0.3778245713975694, "compression_ratio": 1.314516129032258, "no_speech_prob": 0.03621895611286163}, {"id": 266, "seek": 129158, "start": 1291.58, "end": 1301.5, "text": " So we are going to be creating, for the callback, we're going to be passing in the names and", "tokens": [50364, 407, 321, 366, 516, 281, 312, 4084, 11, 337, 264, 818, 3207, 11, 321, 434, 516, 281, 312, 8437, 294, 264, 5288, 293, 50860], "temperature": 0.0, "avg_logprob": -0.29934450475181024, "compression_ratio": 1.6243386243386244, "no_speech_prob": 0.014063198119401932}, {"id": 267, "seek": 129158, "start": 1301.5, "end": 1308.5, "text": " object, metric objects for the metrics to track and print.", "tokens": [50860, 2657, 11, 20678, 6565, 337, 264, 16367, 281, 2837, 293, 4482, 13, 51210], "temperature": 0.0, "avg_logprob": -0.29934450475181024, "compression_ratio": 1.6243386243386244, "no_speech_prob": 0.014063198119401932}, {"id": 268, "seek": 129158, "start": 1308.5, "end": 1314.26, "text": " So here it is here, star star metrics, so we've seen star star before.", "tokens": [51210, 407, 510, 309, 307, 510, 11, 3543, 3543, 16367, 11, 370, 321, 600, 1612, 3543, 3543, 949, 13, 51498], "temperature": 0.0, "avg_logprob": -0.29934450475181024, "compression_ratio": 1.6243386243386244, "no_speech_prob": 0.014063198119401932}, {"id": 269, "seek": 129158, "start": 1314.26, "end": 1318.46, "text": " And as a little shortcut I decided that it might be nice if you didn't want to write", "tokens": [51498, 400, 382, 257, 707, 24822, 286, 3047, 300, 309, 1062, 312, 1481, 498, 291, 994, 380, 528, 281, 2464, 51708], "temperature": 0.0, "avg_logprob": -0.29934450475181024, "compression_ratio": 1.6243386243386244, "no_speech_prob": 0.014063198119401932}, {"id": 270, "seek": 131846, "start": 1318.46, "end": 1325.7, "text": " accuracy equals, you could just remove that and run it, and if you do that then it will", "tokens": [50364, 14170, 6915, 11, 291, 727, 445, 4159, 300, 293, 1190, 309, 11, 293, 498, 291, 360, 300, 550, 309, 486, 50726], "temperature": 0.0, "avg_logprob": -0.24695364634195963, "compression_ratio": 1.791304347826087, "no_speech_prob": 0.12591670453548431}, {"id": 271, "seek": 131846, "start": 1325.7, "end": 1330.1200000000001, "text": " give it a name and it'll just use the same name as the class.", "tokens": [50726, 976, 309, 257, 1315, 293, 309, 603, 445, 764, 264, 912, 1315, 382, 264, 1508, 13, 50947], "temperature": 0.0, "avg_logprob": -0.24695364634195963, "compression_ratio": 1.791304347826087, "no_speech_prob": 0.12591670453548431}, {"id": 272, "seek": 131846, "start": 1330.1200000000001, "end": 1335.66, "text": " And so that's why you can either pass in, so star ms will be a tuple, well I mean it's", "tokens": [50947, 400, 370, 300, 311, 983, 291, 393, 2139, 1320, 294, 11, 370, 3543, 275, 82, 486, 312, 257, 2604, 781, 11, 731, 286, 914, 309, 311, 51224], "temperature": 0.0, "avg_logprob": -0.24695364634195963, "compression_ratio": 1.791304347826087, "no_speech_prob": 0.12591670453548431}, {"id": 273, "seek": 131846, "start": 1335.66, "end": 1340.58, "text": " going to be pulled out, so it's just passing a list of positional arguments which we turn", "tokens": [51224, 516, 281, 312, 7373, 484, 11, 370, 309, 311, 445, 8437, 257, 1329, 295, 2535, 304, 12869, 597, 321, 1261, 51470], "temperature": 0.0, "avg_logprob": -0.24695364634195963, "compression_ratio": 1.791304347826087, "no_speech_prob": 0.12591670453548431}, {"id": 274, "seek": 131846, "start": 1340.58, "end": 1345.1000000000001, "text": " into a tuple, or you can pass in named arguments that'll be turned into a dictionary.", "tokens": [51470, 666, 257, 2604, 781, 11, 420, 291, 393, 1320, 294, 4926, 12869, 300, 603, 312, 3574, 666, 257, 25890, 13, 51696], "temperature": 0.0, "avg_logprob": -0.24695364634195963, "compression_ratio": 1.791304347826087, "no_speech_prob": 0.12591670453548431}, {"id": 275, "seek": 134510, "start": 1345.1, "end": 1350.06, "text": " If you pass in positional arguments then I'm going to turn them into named arguments", "tokens": [50364, 759, 291, 1320, 294, 2535, 304, 12869, 550, 286, 478, 516, 281, 1261, 552, 666, 4926, 12869, 50612], "temperature": 0.0, "avg_logprob": -0.24226257449290792, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.03904622048139572}, {"id": 276, "seek": 134510, "start": 1350.06, "end": 1354.5, "text": " in the dictionary by just grabbing the name from their type.", "tokens": [50612, 294, 264, 25890, 538, 445, 23771, 264, 1315, 490, 641, 2010, 13, 50834], "temperature": 0.0, "avg_logprob": -0.24226257449290792, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.03904622048139572}, {"id": 277, "seek": 134510, "start": 1354.5, "end": 1356.1399999999999, "text": " So that's where this comes from.", "tokens": [50834, 407, 300, 311, 689, 341, 1487, 490, 13, 50916], "temperature": 0.0, "avg_logprob": -0.24226257449290792, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.03904622048139572}, {"id": 278, "seek": 134510, "start": 1356.1399999999999, "end": 1359.74, "text": " That's all that's going on here, just a little shortcut, bit of convenience.", "tokens": [50916, 663, 311, 439, 300, 311, 516, 322, 510, 11, 445, 257, 707, 24822, 11, 857, 295, 19283, 13, 51096], "temperature": 0.0, "avg_logprob": -0.24226257449290792, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.03904622048139572}, {"id": 279, "seek": 134510, "start": 1359.74, "end": 1363.62, "text": " So we'll store that away.", "tokens": [51096, 407, 321, 603, 3531, 300, 1314, 13, 51290], "temperature": 0.0, "avg_logprob": -0.24226257449290792, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.03904622048139572}, {"id": 280, "seek": 134510, "start": 1363.62, "end": 1366.76, "text": " And this is, yeah, this is a bit I think I can simplify a little bit, but I'm just adding", "tokens": [51290, 400, 341, 307, 11, 1338, 11, 341, 307, 257, 857, 286, 519, 286, 393, 20460, 257, 707, 857, 11, 457, 286, 478, 445, 5127, 51447], "temperature": 0.0, "avg_logprob": -0.24226257449290792, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.03904622048139572}, {"id": 281, "seek": 134510, "start": 1366.76, "end": 1372.6999999999998, "text": " manually an additional metric which is, I'm going to call the loss, and that's just going", "tokens": [51447, 16945, 364, 4497, 20678, 597, 307, 11, 286, 478, 516, 281, 818, 264, 4470, 11, 293, 300, 311, 445, 516, 51744], "temperature": 0.0, "avg_logprob": -0.24226257449290792, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.03904622048139572}, {"id": 282, "seek": 137270, "start": 1372.7, "end": 1377.5, "text": " to be the weighted average of the losses.", "tokens": [50364, 281, 312, 264, 32807, 4274, 295, 264, 15352, 13, 50604], "temperature": 0.0, "avg_logprob": -0.2411914996886521, "compression_ratio": 1.7412935323383085, "no_speech_prob": 0.03161768242716789}, {"id": 283, "seek": 137270, "start": 1377.5, "end": 1384.7, "text": " So before we start fitting, we're going to actually tell the learner that we are the", "tokens": [50604, 407, 949, 321, 722, 15669, 11, 321, 434, 516, 281, 767, 980, 264, 33347, 300, 321, 366, 264, 50964], "temperature": 0.0, "avg_logprob": -0.2411914996886521, "compression_ratio": 1.7412935323383085, "no_speech_prob": 0.03161768242716789}, {"id": 284, "seek": 137270, "start": 1384.7, "end": 1390.3400000000001, "text": " metrics callback, and so you'll see later where we're going to actually use this.", "tokens": [50964, 16367, 818, 3207, 11, 293, 370, 291, 603, 536, 1780, 689, 321, 434, 516, 281, 767, 764, 341, 13, 51246], "temperature": 0.0, "avg_logprob": -0.2411914996886521, "compression_ratio": 1.7412935323383085, "no_speech_prob": 0.03161768242716789}, {"id": 285, "seek": 137270, "start": 1390.3400000000001, "end": 1395.94, "text": " Before each epoch we will reset all of our metrics.", "tokens": [51246, 4546, 1184, 30992, 339, 321, 486, 14322, 439, 295, 527, 16367, 13, 51526], "temperature": 0.0, "avg_logprob": -0.2411914996886521, "compression_ratio": 1.7412935323383085, "no_speech_prob": 0.03161768242716789}, {"id": 286, "seek": 137270, "start": 1395.94, "end": 1401.94, "text": " After each epoch we will create a dictionary of the keys and values, which are the actual", "tokens": [51526, 2381, 1184, 30992, 339, 321, 486, 1884, 257, 25890, 295, 264, 9317, 293, 4190, 11, 597, 366, 264, 3539, 51826], "temperature": 0.0, "avg_logprob": -0.2411914996886521, "compression_ratio": 1.7412935323383085, "no_speech_prob": 0.03161768242716789}, {"id": 287, "seek": 140194, "start": 1402.18, "end": 1408.7, "text": " strings that we want to print out, and we will call log, which for now we'll just print", "tokens": [50376, 13985, 300, 321, 528, 281, 4482, 484, 11, 293, 321, 486, 818, 3565, 11, 597, 337, 586, 321, 603, 445, 4482, 50702], "temperature": 0.0, "avg_logprob": -0.25122278998879827, "compression_ratio": 1.6505376344086022, "no_speech_prob": 0.009859796613454819}, {"id": 288, "seek": 140194, "start": 1408.7, "end": 1410.3400000000001, "text": " them.", "tokens": [50702, 552, 13, 50784], "temperature": 0.0, "avg_logprob": -0.25122278998879827, "compression_ratio": 1.6505376344086022, "no_speech_prob": 0.009859796613454819}, {"id": 289, "seek": 140194, "start": 1410.3400000000001, "end": 1415.94, "text": " And then after each batch, this is the key thing, we're going to actually grab the input", "tokens": [50784, 400, 550, 934, 1184, 15245, 11, 341, 307, 264, 2141, 551, 11, 321, 434, 516, 281, 767, 4444, 264, 4846, 51064], "temperature": 0.0, "avg_logprob": -0.25122278998879827, "compression_ratio": 1.6505376344086022, "no_speech_prob": 0.009859796613454819}, {"id": 290, "seek": 140194, "start": 1415.94, "end": 1427.14, "text": " and target, we're going to put them on the CPU, and then we're going to go through each", "tokens": [51064, 293, 3779, 11, 321, 434, 516, 281, 829, 552, 322, 264, 13199, 11, 293, 550, 321, 434, 516, 281, 352, 807, 1184, 51624], "temperature": 0.0, "avg_logprob": -0.25122278998879827, "compression_ratio": 1.6505376344086022, "no_speech_prob": 0.009859796613454819}, {"id": 291, "seek": 140194, "start": 1427.14, "end": 1430.46, "text": " of our metrics and call that update.", "tokens": [51624, 295, 527, 16367, 293, 818, 300, 5623, 13, 51790], "temperature": 0.0, "avg_logprob": -0.25122278998879827, "compression_ratio": 1.6505376344086022, "no_speech_prob": 0.009859796613454819}, {"id": 292, "seek": 143046, "start": 1430.46, "end": 1437.26, "text": " So remember the update in the metric is the thing that actually says here's a batch of", "tokens": [50364, 407, 1604, 264, 5623, 294, 264, 20678, 307, 264, 551, 300, 767, 1619, 510, 311, 257, 15245, 295, 50704], "temperature": 0.0, "avg_logprob": -0.23333172739287952, "compression_ratio": 1.7102272727272727, "no_speech_prob": 0.0020507345907390118}, {"id": 293, "seek": 143046, "start": 1437.26, "end": 1438.26, "text": " data, right?", "tokens": [50704, 1412, 11, 558, 30, 50754], "temperature": 0.0, "avg_logprob": -0.23333172739287952, "compression_ratio": 1.7102272727272727, "no_speech_prob": 0.0020507345907390118}, {"id": 294, "seek": 143046, "start": 1438.26, "end": 1445.14, "text": " So we're passing in the batch of data, which is the predictions and the targets.", "tokens": [50754, 407, 321, 434, 8437, 294, 264, 15245, 295, 1412, 11, 597, 307, 264, 21264, 293, 264, 12911, 13, 51098], "temperature": 0.0, "avg_logprob": -0.23333172739287952, "compression_ratio": 1.7102272727272727, "no_speech_prob": 0.0020507345907390118}, {"id": 295, "seek": 143046, "start": 1445.14, "end": 1450.98, "text": " And then we'll do the same thing for our special loss metric, passing in the actual loss and", "tokens": [51098, 400, 550, 321, 603, 360, 264, 912, 551, 337, 527, 2121, 4470, 20678, 11, 8437, 294, 264, 3539, 4470, 293, 51390], "temperature": 0.0, "avg_logprob": -0.23333172739287952, "compression_ratio": 1.7102272727272727, "no_speech_prob": 0.0020507345907390118}, {"id": 296, "seek": 143046, "start": 1450.98, "end": 1453.94, "text": " the size of our mini-batch.", "tokens": [51390, 264, 2744, 295, 527, 8382, 12, 65, 852, 13, 51538], "temperature": 0.0, "avg_logprob": -0.23333172739287952, "compression_ratio": 1.7102272727272727, "no_speech_prob": 0.0020507345907390118}, {"id": 297, "seek": 145394, "start": 1453.94, "end": 1465.54, "text": " And so that's how we're able to get this, yeah, this actual running on the NVIDIA GPU", "tokens": [50364, 400, 370, 300, 311, 577, 321, 434, 1075, 281, 483, 341, 11, 1338, 11, 341, 3539, 2614, 322, 264, 426, 3958, 6914, 18407, 50944], "temperature": 0.0, "avg_logprob": -0.2709192145954479, "compression_ratio": 1.4845814977973568, "no_speech_prob": 0.011157738976180553}, {"id": 298, "seek": 145394, "start": 1465.54, "end": 1466.54, "text": " and showing our metrics.", "tokens": [50944, 293, 4099, 527, 16367, 13, 50994], "temperature": 0.0, "avg_logprob": -0.2709192145954479, "compression_ratio": 1.4845814977973568, "no_speech_prob": 0.011157738976180553}, {"id": 299, "seek": 145394, "start": 1466.54, "end": 1470.5800000000002, "text": " And obviously there's a lot of room to improve how this is displayed, but all the information", "tokens": [50994, 400, 2745, 456, 311, 257, 688, 295, 1808, 281, 3470, 577, 341, 307, 16372, 11, 457, 439, 264, 1589, 51196], "temperature": 0.0, "avg_logprob": -0.2709192145954479, "compression_ratio": 1.4845814977973568, "no_speech_prob": 0.011157738976180553}, {"id": 300, "seek": 145394, "start": 1470.5800000000002, "end": 1475.7, "text": " is we needed here, and it's just a case of changing that function.", "tokens": [51196, 307, 321, 2978, 510, 11, 293, 309, 311, 445, 257, 1389, 295, 4473, 300, 2445, 13, 51452], "temperature": 0.0, "avg_logprob": -0.2709192145954479, "compression_ratio": 1.4845814977973568, "no_speech_prob": 0.011157738976180553}, {"id": 301, "seek": 145394, "start": 1475.7, "end": 1481.8400000000001, "text": " Okay, so that's our kind of like intermediate complexity learner.", "tokens": [51452, 1033, 11, 370, 300, 311, 527, 733, 295, 411, 19376, 14024, 33347, 13, 51759], "temperature": 0.0, "avg_logprob": -0.2709192145954479, "compression_ratio": 1.4845814977973568, "no_speech_prob": 0.011157738976180553}, {"id": 302, "seek": 148184, "start": 1481.84, "end": 1488.84, "text": " We can make it more sophisticated, but it's still exactly, it's still going to fit in", "tokens": [50364, 492, 393, 652, 309, 544, 16950, 11, 457, 309, 311, 920, 2293, 11, 309, 311, 920, 516, 281, 3318, 294, 50714], "temperature": 0.0, "avg_logprob": -0.25522625830865675, "compression_ratio": 1.6884422110552764, "no_speech_prob": 0.020963868126273155}, {"id": 303, "seek": 148184, "start": 1488.84, "end": 1489.84, "text": " a single screen of code.", "tokens": [50714, 257, 2167, 2568, 295, 3089, 13, 50764], "temperature": 0.0, "avg_logprob": -0.25522625830865675, "compression_ratio": 1.6884422110552764, "no_speech_prob": 0.020963868126273155}, {"id": 304, "seek": 148184, "start": 1489.84, "end": 1494.8799999999999, "text": " So this is kind of my goal here, is to keep everything in a single screen of code.", "tokens": [50764, 407, 341, 307, 733, 295, 452, 3387, 510, 11, 307, 281, 1066, 1203, 294, 257, 2167, 2568, 295, 3089, 13, 51016], "temperature": 0.0, "avg_logprob": -0.25522625830865675, "compression_ratio": 1.6884422110552764, "no_speech_prob": 0.020963868126273155}, {"id": 305, "seek": 148184, "start": 1494.8799999999999, "end": 1503.9599999999998, "text": " This first bit is exactly the same as before, but you'll see that the one epoch and fit", "tokens": [51016, 639, 700, 857, 307, 2293, 264, 912, 382, 949, 11, 457, 291, 603, 536, 300, 264, 472, 30992, 339, 293, 3318, 51470], "temperature": 0.0, "avg_logprob": -0.25522625830865675, "compression_ratio": 1.6884422110552764, "no_speech_prob": 0.020963868126273155}, {"id": 306, "seek": 148184, "start": 1503.9599999999998, "end": 1511.8, "text": " and batch has gone from, let's see what it was before.", "tokens": [51470, 293, 15245, 575, 2780, 490, 11, 718, 311, 536, 437, 309, 390, 949, 13, 51862], "temperature": 0.0, "avg_logprob": -0.25522625830865675, "compression_ratio": 1.6884422110552764, "no_speech_prob": 0.020963868126273155}, {"id": 307, "seek": 151180, "start": 1512.76, "end": 1519.6399999999999, "text": " It's gone from quite a lot of code, all this, to much less code.", "tokens": [50412, 467, 311, 2780, 490, 1596, 257, 688, 295, 3089, 11, 439, 341, 11, 281, 709, 1570, 3089, 13, 50756], "temperature": 0.0, "avg_logprob": -0.22849686362526633, "compression_ratio": 1.7923728813559323, "no_speech_prob": 0.0001686527393758297}, {"id": 308, "seek": 151180, "start": 1519.6399999999999, "end": 1522.96, "text": " And the trick to doing that is I decided to use a context manager.", "tokens": [50756, 400, 264, 4282, 281, 884, 300, 307, 286, 3047, 281, 764, 257, 4319, 6598, 13, 50922], "temperature": 0.0, "avg_logprob": -0.22849686362526633, "compression_ratio": 1.7923728813559323, "no_speech_prob": 0.0001686527393758297}, {"id": 309, "seek": 151180, "start": 1522.96, "end": 1527.3999999999999, "text": " We're going to learn more about context managers in the next notebook, but basically, I originally", "tokens": [50922, 492, 434, 516, 281, 1466, 544, 466, 4319, 14084, 294, 264, 958, 21060, 11, 457, 1936, 11, 286, 7993, 51144], "temperature": 0.0, "avg_logprob": -0.22849686362526633, "compression_ratio": 1.7923728813559323, "no_speech_prob": 0.0001686527393758297}, {"id": 310, "seek": 151180, "start": 1527.3999999999999, "end": 1530.36, "text": " last week I was saying I was going to do this as a decorator, but I realized a context manager", "tokens": [51144, 1036, 1243, 286, 390, 1566, 286, 390, 516, 281, 360, 341, 382, 257, 7919, 1639, 11, 457, 286, 5334, 257, 4319, 6598, 51292], "temperature": 0.0, "avg_logprob": -0.22849686362526633, "compression_ratio": 1.7923728813559323, "no_speech_prob": 0.0001686527393758297}, {"id": 311, "seek": 151180, "start": 1530.36, "end": 1532.28, "text": " is better.", "tokens": [51292, 307, 1101, 13, 51388], "temperature": 0.0, "avg_logprob": -0.22849686362526633, "compression_ratio": 1.7923728813559323, "no_speech_prob": 0.0001686527393758297}, {"id": 312, "seek": 151180, "start": 1532.28, "end": 1538.0, "text": " Basically what we're going to do is we're going to call our before and after callbacks", "tokens": [51388, 8537, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 818, 527, 949, 293, 934, 818, 17758, 51674], "temperature": 0.0, "avg_logprob": -0.22849686362526633, "compression_ratio": 1.7923728813559323, "no_speech_prob": 0.0001686527393758297}, {"id": 313, "seek": 153800, "start": 1538.2, "end": 1542.88, "text": " in a try except block.", "tokens": [50374, 294, 257, 853, 3993, 3461, 13, 50608], "temperature": 0.0, "avg_logprob": -0.2495045610653457, "compression_ratio": 1.685, "no_speech_prob": 0.012241076678037643}, {"id": 314, "seek": 153800, "start": 1542.88, "end": 1547.16, "text": " And to say that we want to use the callbacks in the try except block, we're going to use", "tokens": [50608, 400, 281, 584, 300, 321, 528, 281, 764, 264, 818, 17758, 294, 264, 853, 3993, 3461, 11, 321, 434, 516, 281, 764, 50822], "temperature": 0.0, "avg_logprob": -0.2495045610653457, "compression_ratio": 1.685, "no_speech_prob": 0.012241076678037643}, {"id": 315, "seek": 153800, "start": 1547.16, "end": 1549.8, "text": " a with statement.", "tokens": [50822, 257, 365, 5629, 13, 50954], "temperature": 0.0, "avg_logprob": -0.2495045610653457, "compression_ratio": 1.685, "no_speech_prob": 0.012241076678037643}, {"id": 316, "seek": 153800, "start": 1549.8, "end": 1559.56, "text": " So in Python, a with statement says everything in that block, call our context manager before", "tokens": [50954, 407, 294, 15329, 11, 257, 365, 5629, 1619, 1203, 294, 300, 3461, 11, 818, 527, 4319, 6598, 949, 51442], "temperature": 0.0, "avg_logprob": -0.2495045610653457, "compression_ratio": 1.685, "no_speech_prob": 0.012241076678037643}, {"id": 317, "seek": 153800, "start": 1559.56, "end": 1560.56, "text": " and after it.", "tokens": [51442, 293, 934, 309, 13, 51492], "temperature": 0.0, "avg_logprob": -0.2495045610653457, "compression_ratio": 1.685, "no_speech_prob": 0.012241076678037643}, {"id": 318, "seek": 153800, "start": 1560.56, "end": 1565.08, "text": " Now there's a few ways to do that, but one really easy one is using this context manager", "tokens": [51492, 823, 456, 311, 257, 1326, 2098, 281, 360, 300, 11, 457, 472, 534, 1858, 472, 307, 1228, 341, 4319, 6598, 51718], "temperature": 0.0, "avg_logprob": -0.2495045610653457, "compression_ratio": 1.685, "no_speech_prob": 0.012241076678037643}, {"id": 319, "seek": 153800, "start": 1565.08, "end": 1567.16, "text": " decorator.", "tokens": [51718, 7919, 1639, 13, 51822], "temperature": 0.0, "avg_logprob": -0.2495045610653457, "compression_ratio": 1.685, "no_speech_prob": 0.012241076678037643}, {"id": 320, "seek": 156716, "start": 1567.3200000000002, "end": 1573.16, "text": " And everything up into the up to the yield statement is called before your code.", "tokens": [50372, 400, 1203, 493, 666, 264, 493, 281, 264, 11257, 5629, 307, 1219, 949, 428, 3089, 13, 50664], "temperature": 0.0, "avg_logprob": -0.29460527499516803, "compression_ratio": 1.8518518518518519, "no_speech_prob": 0.00734579237177968}, {"id": 321, "seek": 156716, "start": 1573.16, "end": 1576.76, "text": " Where it says yield, it then calls your code.", "tokens": [50664, 2305, 309, 1619, 11257, 11, 309, 550, 5498, 428, 3089, 13, 50844], "temperature": 0.0, "avg_logprob": -0.29460527499516803, "compression_ratio": 1.8518518518518519, "no_speech_prob": 0.00734579237177968}, {"id": 322, "seek": 156716, "start": 1576.76, "end": 1580.5400000000002, "text": " And then everything after the yield is called after your code.", "tokens": [50844, 400, 550, 1203, 934, 264, 11257, 307, 1219, 934, 428, 3089, 13, 51033], "temperature": 0.0, "avg_logprob": -0.29460527499516803, "compression_ratio": 1.8518518518518519, "no_speech_prob": 0.00734579237177968}, {"id": 323, "seek": 156716, "start": 1580.5400000000002, "end": 1590.3000000000002, "text": " So in this case, it's going to be try self.callback before name, where name is fit.", "tokens": [51033, 407, 294, 341, 1389, 11, 309, 311, 516, 281, 312, 853, 2698, 13, 45459, 3207, 949, 1315, 11, 689, 1315, 307, 3318, 13, 51521], "temperature": 0.0, "avg_logprob": -0.29460527499516803, "compression_ratio": 1.8518518518518519, "no_speech_prob": 0.00734579237177968}, {"id": 324, "seek": 156716, "start": 1590.3000000000002, "end": 1594.28, "text": " And then it will call for self.epoch etc.", "tokens": [51521, 400, 550, 309, 486, 818, 337, 2698, 13, 595, 8997, 5183, 13, 51720], "temperature": 0.0, "avg_logprob": -0.29460527499516803, "compression_ratio": 1.8518518518518519, "no_speech_prob": 0.00734579237177968}, {"id": 325, "seek": 156716, "start": 1594.28, "end": 1595.48, "text": " Because that's where the yield is.", "tokens": [51720, 1436, 300, 311, 689, 264, 11257, 307, 13, 51780], "temperature": 0.0, "avg_logprob": -0.29460527499516803, "compression_ratio": 1.8518518518518519, "no_speech_prob": 0.00734579237177968}, {"id": 326, "seek": 159548, "start": 1595.56, "end": 1601.92, "text": " And then it'll call self.callback after fit except.", "tokens": [50368, 400, 550, 309, 603, 818, 2698, 13, 45459, 3207, 934, 3318, 3993, 13, 50686], "temperature": 0.0, "avg_logprob": -0.31357926262749564, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.209450826048851}, {"id": 327, "seek": 159548, "start": 1601.92, "end": 1606.4, "text": " Okay and now we need to grab the cancel fit exception.", "tokens": [50686, 1033, 293, 586, 321, 643, 281, 4444, 264, 10373, 3318, 11183, 13, 50910], "temperature": 0.0, "avg_logprob": -0.31357926262749564, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.209450826048851}, {"id": 328, "seek": 159548, "start": 1606.4, "end": 1612.92, "text": " So all of the variables that you have in Python all live inside a special dictionary called", "tokens": [50910, 407, 439, 295, 264, 9102, 300, 291, 362, 294, 15329, 439, 1621, 1854, 257, 2121, 25890, 1219, 51236], "temperature": 0.0, "avg_logprob": -0.31357926262749564, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.209450826048851}, {"id": 329, "seek": 159548, "start": 1612.92, "end": 1614.3, "text": " globals.", "tokens": [51236, 16125, 1124, 13, 51305], "temperature": 0.0, "avg_logprob": -0.31357926262749564, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.209450826048851}, {"id": 330, "seek": 159548, "start": 1614.3, "end": 1616.56, "text": " So this dictionary contains all of your variables.", "tokens": [51305, 407, 341, 25890, 8306, 439, 295, 428, 9102, 13, 51418], "temperature": 0.0, "avg_logprob": -0.31357926262749564, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.209450826048851}, {"id": 331, "seek": 159548, "start": 1616.56, "end": 1623.52, "text": " So I can just look up in that dictionary the variable called cancel fit with a capital", "tokens": [51418, 407, 286, 393, 445, 574, 493, 294, 300, 25890, 264, 7006, 1219, 10373, 3318, 365, 257, 4238, 51766], "temperature": 0.0, "avg_logprob": -0.31357926262749564, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.209450826048851}, {"id": 332, "seek": 159548, "start": 1623.52, "end": 1624.8, "text": " F exception.", "tokens": [51766, 479, 11183, 13, 51830], "temperature": 0.0, "avg_logprob": -0.31357926262749564, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.209450826048851}, {"id": 333, "seek": 162480, "start": 1624.84, "end": 1627.9199999999998, "text": " So this is except cancel fit exception.", "tokens": [50366, 407, 341, 307, 3993, 10373, 3318, 11183, 13, 50520], "temperature": 0.0, "avg_logprob": -0.27109898653897374, "compression_ratio": 1.737991266375546, "no_speech_prob": 0.0004728536878246814}, {"id": 334, "seek": 162480, "start": 1627.9199999999998, "end": 1633.3999999999999, "text": " So this is exactly the same then as this code.", "tokens": [50520, 407, 341, 307, 2293, 264, 912, 550, 382, 341, 3089, 13, 50794], "temperature": 0.0, "avg_logprob": -0.27109898653897374, "compression_ratio": 1.737991266375546, "no_speech_prob": 0.0004728536878246814}, {"id": 335, "seek": 162480, "start": 1633.3999999999999, "end": 1638.48, "text": " Except the nice thing is now I only have to write it once, rather than at least three", "tokens": [50794, 16192, 264, 1481, 551, 307, 586, 286, 787, 362, 281, 2464, 309, 1564, 11, 2831, 813, 412, 1935, 1045, 51048], "temperature": 0.0, "avg_logprob": -0.27109898653897374, "compression_ratio": 1.737991266375546, "no_speech_prob": 0.0004728536878246814}, {"id": 336, "seek": 162480, "start": 1638.48, "end": 1639.48, "text": " times.", "tokens": [51048, 1413, 13, 51098], "temperature": 0.0, "avg_logprob": -0.27109898653897374, "compression_ratio": 1.737991266375546, "no_speech_prob": 0.0004728536878246814}, {"id": 337, "seek": 162480, "start": 1639.48, "end": 1640.48, "text": " And I'm probably going to want more of them.", "tokens": [51098, 400, 286, 478, 1391, 516, 281, 528, 544, 295, 552, 13, 51148], "temperature": 0.0, "avg_logprob": -0.27109898653897374, "compression_ratio": 1.737991266375546, "no_speech_prob": 0.0004728536878246814}, {"id": 338, "seek": 162480, "start": 1640.48, "end": 1648.8799999999999, "text": " So you know I tend to think it's worth, yeah, I tend to think it's worth refactoring a code", "tokens": [51148, 407, 291, 458, 286, 3928, 281, 519, 309, 311, 3163, 11, 1338, 11, 286, 3928, 281, 519, 309, 311, 3163, 1895, 578, 3662, 257, 3089, 51568], "temperature": 0.0, "avg_logprob": -0.27109898653897374, "compression_ratio": 1.737991266375546, "no_speech_prob": 0.0004728536878246814}, {"id": 339, "seek": 162480, "start": 1648.8799999999999, "end": 1651.36, "text": " when you have duplicate code.", "tokens": [51568, 562, 291, 362, 23976, 3089, 13, 51692], "temperature": 0.0, "avg_logprob": -0.27109898653897374, "compression_ratio": 1.737991266375546, "no_speech_prob": 0.0004728536878246814}, {"id": 340, "seek": 162480, "start": 1651.36, "end": 1654.36, "text": " Particularly here we had the same code three times.", "tokens": [51692, 32281, 510, 321, 632, 264, 912, 3089, 1045, 1413, 13, 51842], "temperature": 0.0, "avg_logprob": -0.27109898653897374, "compression_ratio": 1.737991266375546, "no_speech_prob": 0.0004728536878246814}, {"id": 341, "seek": 165436, "start": 1654.9199999999998, "end": 1655.9199999999998, "text": " So that's going to be more of a maintenance headache.", "tokens": [50392, 407, 300, 311, 516, 281, 312, 544, 295, 257, 11258, 23520, 13, 50442], "temperature": 0.0, "avg_logprob": -0.25834334000297215, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0012065856717526913}, {"id": 342, "seek": 165436, "start": 1655.9199999999998, "end": 1660.28, "text": " We're probably going to want to add callbacks to more things later.", "tokens": [50442, 492, 434, 1391, 516, 281, 528, 281, 909, 818, 17758, 281, 544, 721, 1780, 13, 50660], "temperature": 0.0, "avg_logprob": -0.25834334000297215, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0012065856717526913}, {"id": 343, "seek": 165436, "start": 1660.28, "end": 1666.24, "text": " So by putting it into a context manager just once, I think we're going to reduce our maintenance", "tokens": [50660, 407, 538, 3372, 309, 666, 257, 4319, 6598, 445, 1564, 11, 286, 519, 321, 434, 516, 281, 5407, 527, 11258, 50958], "temperature": 0.0, "avg_logprob": -0.25834334000297215, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0012065856717526913}, {"id": 344, "seek": 165436, "start": 1666.24, "end": 1667.24, "text": " burden.", "tokens": [50958, 12578, 13, 51008], "temperature": 0.0, "avg_logprob": -0.25834334000297215, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0012065856717526913}, {"id": 345, "seek": 165436, "start": 1667.24, "end": 1670.9599999999998, "text": " Well I know we do because I've had a similar thing in fast.ai for some years now and it's", "tokens": [51008, 1042, 286, 458, 321, 360, 570, 286, 600, 632, 257, 2531, 551, 294, 2370, 13, 1301, 337, 512, 924, 586, 293, 309, 311, 51194], "temperature": 0.0, "avg_logprob": -0.25834334000297215, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0012065856717526913}, {"id": 346, "seek": 165436, "start": 1670.9599999999998, "end": 1675.6799999999998, "text": " been quite convenient.", "tokens": [51194, 668, 1596, 10851, 13, 51430], "temperature": 0.0, "avg_logprob": -0.25834334000297215, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0012065856717526913}, {"id": 347, "seek": 165436, "start": 1675.6799999999998, "end": 1681.12, "text": " So that's what this context manager is about.", "tokens": [51430, 407, 300, 311, 437, 341, 4319, 6598, 307, 466, 13, 51702], "temperature": 0.0, "avg_logprob": -0.25834334000297215, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0012065856717526913}, {"id": 348, "seek": 165436, "start": 1681.12, "end": 1683.32, "text": " Yeah other than that the code's exactly the same.", "tokens": [51702, 865, 661, 813, 300, 264, 3089, 311, 2293, 264, 912, 13, 51812], "temperature": 0.0, "avg_logprob": -0.25834334000297215, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0012065856717526913}, {"id": 349, "seek": 168332, "start": 1683.32, "end": 1689.76, "text": " So we create our optimizer and then with our callback context manager for fit, go through", "tokens": [50364, 407, 321, 1884, 527, 5028, 6545, 293, 550, 365, 527, 818, 3207, 4319, 6598, 337, 3318, 11, 352, 807, 50686], "temperature": 0.0, "avg_logprob": -0.2746877670288086, "compression_ratio": 1.8763440860215055, "no_speech_prob": 0.0042645856738090515}, {"id": 350, "seek": 168332, "start": 1689.76, "end": 1697.08, "text": " each epoch, call one epoch, set it to training or non-training mode based on the argument", "tokens": [50686, 1184, 30992, 339, 11, 818, 472, 30992, 339, 11, 992, 309, 281, 3097, 420, 2107, 12, 17227, 1760, 4391, 2361, 322, 264, 6770, 51052], "temperature": 0.0, "avg_logprob": -0.2746877670288086, "compression_ratio": 1.8763440860215055, "no_speech_prob": 0.0042645856738090515}, {"id": 351, "seek": 168332, "start": 1697.08, "end": 1698.6399999999999, "text": " we pass in.", "tokens": [51052, 321, 1320, 294, 13, 51130], "temperature": 0.0, "avg_logprob": -0.2746877670288086, "compression_ratio": 1.8763440860215055, "no_speech_prob": 0.0042645856738090515}, {"id": 352, "seek": 168332, "start": 1698.6399999999999, "end": 1702.36, "text": " Grab the training or validation set based on the argument we pass in.", "tokens": [51130, 20357, 264, 3097, 420, 24071, 992, 2361, 322, 264, 6770, 321, 1320, 294, 13, 51316], "temperature": 0.0, "avg_logprob": -0.2746877670288086, "compression_ratio": 1.8763440860215055, "no_speech_prob": 0.0042645856738090515}, {"id": 353, "seek": 168332, "start": 1702.36, "end": 1710.08, "text": " And then using the context manager for epoch, go through each batch in the data loader.", "tokens": [51316, 400, 550, 1228, 264, 4319, 6598, 337, 30992, 339, 11, 352, 807, 1184, 15245, 294, 264, 1412, 3677, 260, 13, 51702], "temperature": 0.0, "avg_logprob": -0.2746877670288086, "compression_ratio": 1.8763440860215055, "no_speech_prob": 0.0042645856738090515}, {"id": 354, "seek": 171008, "start": 1710.08, "end": 1716.48, "text": " And then for each batch in the data loader using the batch context.", "tokens": [50364, 400, 550, 337, 1184, 15245, 294, 264, 1412, 3677, 260, 1228, 264, 15245, 4319, 13, 50684], "temperature": 0.0, "avg_logprob": -0.31946629040861785, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.003222465980798006}, {"id": 355, "seek": 171008, "start": 1716.48, "end": 1718.96, "text": " Now this is where something gets quite interesting.", "tokens": [50684, 823, 341, 307, 689, 746, 2170, 1596, 1880, 13, 50808], "temperature": 0.0, "avg_logprob": -0.31946629040861785, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.003222465980798006}, {"id": 356, "seek": 171008, "start": 1718.96, "end": 1725.4399999999998, "text": " We call predict, get loss, and if we're training, backward step, and zero gret.", "tokens": [50808, 492, 818, 6069, 11, 483, 4470, 11, 293, 498, 321, 434, 3097, 11, 23897, 1823, 11, 293, 4018, 290, 1505, 13, 51132], "temperature": 0.0, "avg_logprob": -0.31946629040861785, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.003222465980798006}, {"id": 357, "seek": 171008, "start": 1725.4399999999998, "end": 1736.3999999999999, "text": " But previously we actually called self.model etc. self.loss function etc.", "tokens": [51132, 583, 8046, 321, 767, 1219, 2698, 13, 8014, 338, 5183, 13, 2698, 13, 75, 772, 2445, 5183, 13, 51680], "temperature": 0.0, "avg_logprob": -0.31946629040861785, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.003222465980798006}, {"id": 358, "seek": 173640, "start": 1736.4, "end": 1745.4, "text": " So we go through each batch and call before batch, do the batch.", "tokens": [50364, 407, 321, 352, 807, 1184, 15245, 293, 818, 949, 15245, 11, 360, 264, 15245, 13, 50814], "temperature": 0.0, "avg_logprob": -0.513389458090572, "compression_ratio": 1.3381294964028776, "no_speech_prob": 0.23084592819213867}, {"id": 359, "seek": 173640, "start": 1745.4, "end": 1750.24, "text": " Oh sorry that's our slow version.", "tokens": [50814, 876, 2597, 300, 311, 527, 2964, 3037, 13, 51056], "temperature": 0.0, "avg_logprob": -0.513389458090572, "compression_ratio": 1.3381294964028776, "no_speech_prob": 0.23084592819213867}, {"id": 360, "seek": 173640, "start": 1750.24, "end": 1753.24, "text": " Wait what are we doing?", "tokens": [51056, 3802, 437, 366, 321, 884, 30, 51206], "temperature": 0.0, "avg_logprob": -0.513389458090572, "compression_ratio": 1.3381294964028776, "no_speech_prob": 0.23084592819213867}, {"id": 361, "seek": 173640, "start": 1753.24, "end": 1759.1200000000001, "text": " Oh yes we're going to be over here.", "tokens": [51206, 876, 2086, 321, 434, 516, 281, 312, 670, 510, 13, 51500], "temperature": 0.0, "avg_logprob": -0.513389458090572, "compression_ratio": 1.3381294964028776, "no_speech_prob": 0.23084592819213867}, {"id": 362, "seek": 173640, "start": 1759.1200000000001, "end": 1763.96, "text": " Okay I'm back where we are.", "tokens": [51500, 1033, 286, 478, 646, 689, 321, 366, 13, 51742], "temperature": 0.0, "avg_logprob": -0.513389458090572, "compression_ratio": 1.3381294964028776, "no_speech_prob": 0.23084592819213867}, {"id": 363, "seek": 176396, "start": 1763.96, "end": 1769.72, "text": " So previously we were calling, yeah, calling the model, calling the loss function, calling", "tokens": [50364, 407, 8046, 321, 645, 5141, 11, 1338, 11, 5141, 264, 2316, 11, 5141, 264, 4470, 2445, 11, 5141, 50652], "temperature": 0.0, "avg_logprob": -0.2241309574672154, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.0027575755957514048}, {"id": 364, "seek": 176396, "start": 1769.72, "end": 1773.76, "text": " loss.backward, opt.step, opt.zero gret.", "tokens": [50652, 4470, 13, 3207, 1007, 11, 2427, 13, 16792, 11, 2427, 13, 32226, 290, 1505, 13, 50854], "temperature": 0.0, "avg_logprob": -0.2241309574672154, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.0027575755957514048}, {"id": 365, "seek": 176396, "start": 1773.76, "end": 1784.72, "text": " But now we are calling instead self.predict, self.getloss, self.backward.", "tokens": [50854, 583, 586, 321, 366, 5141, 2602, 2698, 13, 79, 24945, 11, 2698, 13, 847, 75, 772, 11, 2698, 13, 3207, 1007, 13, 51402], "temperature": 0.0, "avg_logprob": -0.2241309574672154, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.0027575755957514048}, {"id": 366, "seek": 176396, "start": 1784.72, "end": 1786.08, "text": " And how on earth is that working?", "tokens": [51402, 400, 577, 322, 4120, 307, 300, 1364, 30, 51470], "temperature": 0.0, "avg_logprob": -0.2241309574672154, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.0027575755957514048}, {"id": 367, "seek": 176396, "start": 1786.08, "end": 1789.04, "text": " Because they're not defined here at all.", "tokens": [51470, 1436, 436, 434, 406, 7642, 510, 412, 439, 13, 51618], "temperature": 0.0, "avg_logprob": -0.2241309574672154, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.0027575755957514048}, {"id": 368, "seek": 176396, "start": 1789.04, "end": 1792.32, "text": " And so the reason I've decided to do this is it gives us a lot of flexibility.", "tokens": [51618, 400, 370, 264, 1778, 286, 600, 3047, 281, 360, 341, 307, 309, 2709, 505, 257, 688, 295, 12635, 13, 51782], "temperature": 0.0, "avg_logprob": -0.2241309574672154, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.0027575755957514048}, {"id": 369, "seek": 179232, "start": 1792.32, "end": 1799.12, "text": " We can now actually create our own way of doing predict, get loss, backward step, and", "tokens": [50364, 492, 393, 586, 767, 1884, 527, 1065, 636, 295, 884, 6069, 11, 483, 4470, 11, 23897, 1823, 11, 293, 50704], "temperature": 0.0, "avg_logprob": -0.28345383091976767, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.004905303940176964}, {"id": 370, "seek": 179232, "start": 1799.12, "end": 1801.6399999999999, "text": " zero gret in different situations.", "tokens": [50704, 4018, 290, 1505, 294, 819, 6851, 13, 50830], "temperature": 0.0, "avg_logprob": -0.28345383091976767, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.004905303940176964}, {"id": 371, "seek": 179232, "start": 1801.6399999999999, "end": 1808.46, "text": " And we're going to see some of those situations.", "tokens": [50830, 400, 321, 434, 516, 281, 536, 512, 295, 729, 6851, 13, 51171], "temperature": 0.0, "avg_logprob": -0.28345383091976767, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.004905303940176964}, {"id": 372, "seek": 179232, "start": 1808.46, "end": 1813.04, "text": " So what happens if we call self.predict and it doesn't exist?", "tokens": [51171, 407, 437, 2314, 498, 321, 818, 2698, 13, 79, 24945, 293, 309, 1177, 380, 2514, 30, 51400], "temperature": 0.0, "avg_logprob": -0.28345383091976767, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.004905303940176964}, {"id": 373, "seek": 179232, "start": 1813.04, "end": 1815.28, "text": " Well it doesn't necessarily cause an error.", "tokens": [51400, 1042, 309, 1177, 380, 4725, 3082, 364, 6713, 13, 51512], "temperature": 0.0, "avg_logprob": -0.28345383091976767, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.004905303940176964}, {"id": 374, "seek": 179232, "start": 1815.28, "end": 1821.48, "text": " What actually happens is it calls a special magic method in Python called dunder getattr,", "tokens": [51512, 708, 767, 2314, 307, 309, 5498, 257, 2121, 5585, 3170, 294, 15329, 1219, 274, 6617, 483, 1591, 81, 11, 51822], "temperature": 0.0, "avg_logprob": -0.28345383091976767, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.004905303940176964}, {"id": 375, "seek": 182148, "start": 1821.64, "end": 1823.3600000000001, "text": " as we've seen before.", "tokens": [50372, 382, 321, 600, 1612, 949, 13, 50458], "temperature": 0.0, "avg_logprob": -0.261569341023763, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.031143667176365852}, {"id": 376, "seek": 182148, "start": 1823.3600000000001, "end": 1829.4, "text": " And what I'm doing here is I'm saying okay well if it's one of these special five things,", "tokens": [50458, 400, 437, 286, 478, 884, 510, 307, 286, 478, 1566, 1392, 731, 498, 309, 311, 472, 295, 613, 2121, 1732, 721, 11, 50760], "temperature": 0.0, "avg_logprob": -0.261569341023763, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.031143667176365852}, {"id": 377, "seek": 182148, "start": 1829.4, "end": 1836.84, "text": " don't raise an attribute error, which is this is the default thing it does, but instead", "tokens": [50760, 500, 380, 5300, 364, 19667, 6713, 11, 597, 307, 341, 307, 264, 7576, 551, 309, 775, 11, 457, 2602, 51132], "temperature": 0.0, "avg_logprob": -0.261569341023763, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.031143667176365852}, {"id": 378, "seek": 182148, "start": 1836.84, "end": 1845.0, "text": " create a callback, or actually I should say call self.callback, passing in that name.", "tokens": [51132, 1884, 257, 818, 3207, 11, 420, 767, 286, 820, 584, 818, 2698, 13, 45459, 3207, 11, 8437, 294, 300, 1315, 13, 51540], "temperature": 0.0, "avg_logprob": -0.261569341023763, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.031143667176365852}, {"id": 379, "seek": 184500, "start": 1845.0, "end": 1851.08, "text": " So it's actually going to call self.callback, quote, predict.", "tokens": [50364, 407, 309, 311, 767, 516, 281, 818, 2698, 13, 45459, 3207, 11, 6513, 11, 6069, 13, 50668], "temperature": 0.0, "avg_logprob": -0.23549364324201616, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.059208378195762634}, {"id": 380, "seek": 184500, "start": 1851.08, "end": 1853.58, "text": " And self.callback is exactly the same as before.", "tokens": [50668, 400, 2698, 13, 45459, 3207, 307, 2293, 264, 912, 382, 949, 13, 50793], "temperature": 0.0, "avg_logprob": -0.23549364324201616, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.059208378195762634}, {"id": 381, "seek": 184500, "start": 1853.58, "end": 1858.2, "text": " And so what that means now is to make this work exactly the same as it did before, I", "tokens": [50793, 400, 370, 437, 300, 1355, 586, 307, 281, 652, 341, 589, 2293, 264, 912, 382, 309, 630, 949, 11, 286, 51024], "temperature": 0.0, "avg_logprob": -0.23549364324201616, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.059208378195762634}, {"id": 382, "seek": 184500, "start": 1858.2, "end": 1862.92, "text": " need a callback which does these five things.", "tokens": [51024, 643, 257, 818, 3207, 597, 775, 613, 1732, 721, 13, 51260], "temperature": 0.0, "avg_logprob": -0.23549364324201616, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.059208378195762634}, {"id": 383, "seek": 184500, "start": 1862.92, "end": 1863.92, "text": " And here it is.", "tokens": [51260, 400, 510, 309, 307, 13, 51310], "temperature": 0.0, "avg_logprob": -0.23549364324201616, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.059208378195762634}, {"id": 384, "seek": 184500, "start": 1863.92, "end": 1865.44, "text": " I'm going to call it train callback.", "tokens": [51310, 286, 478, 516, 281, 818, 309, 3847, 818, 3207, 13, 51386], "temperature": 0.0, "avg_logprob": -0.23549364324201616, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.059208378195762634}, {"id": 385, "seek": 184500, "start": 1865.44, "end": 1866.84, "text": " So here are the five things.", "tokens": [51386, 407, 510, 366, 264, 1732, 721, 13, 51456], "temperature": 0.0, "avg_logprob": -0.23549364324201616, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.059208378195762634}, {"id": 386, "seek": 184500, "start": 1866.84, "end": 1870.54, "text": " Predict, getloss, backward, step, and zero gret.", "tokens": [51456, 430, 24945, 11, 483, 75, 772, 11, 23897, 11, 1823, 11, 293, 4018, 290, 1505, 13, 51641], "temperature": 0.0, "avg_logprob": -0.23549364324201616, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.059208378195762634}, {"id": 387, "seek": 187054, "start": 1870.54, "end": 1871.54, "text": " So they are here.", "tokens": [50364, 407, 436, 366, 510, 13, 50414], "temperature": 0.0, "avg_logprob": -0.3070650100708008, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.024422315880656242}, {"id": 388, "seek": 187054, "start": 1871.54, "end": 1881.22, "text": " Predict, getloss, backward, step, and zero gret.", "tokens": [50414, 430, 24945, 11, 483, 75, 772, 11, 23897, 11, 1823, 11, 293, 4018, 290, 1505, 13, 50898], "temperature": 0.0, "avg_logprob": -0.3070650100708008, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.024422315880656242}, {"id": 389, "seek": 187054, "start": 1881.22, "end": 1886.82, "text": " Okay so they're almost exactly the same as what they looked like in our intermediate", "tokens": [50898, 1033, 370, 436, 434, 1920, 2293, 264, 912, 382, 437, 436, 2956, 411, 294, 527, 19376, 51178], "temperature": 0.0, "avg_logprob": -0.3070650100708008, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.024422315880656242}, {"id": 390, "seek": 187054, "start": 1886.82, "end": 1891.7, "text": " learner, except now I just need to have self.learn in front of everything, because we remember", "tokens": [51178, 33347, 11, 3993, 586, 286, 445, 643, 281, 362, 2698, 13, 306, 1083, 294, 1868, 295, 1203, 11, 570, 321, 1604, 51422], "temperature": 0.0, "avg_logprob": -0.3070650100708008, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.024422315880656242}, {"id": 391, "seek": 187054, "start": 1891.7, "end": 1893.86, "text": " this is a callback, it's not the learner.", "tokens": [51422, 341, 307, 257, 818, 3207, 11, 309, 311, 406, 264, 33347, 13, 51530], "temperature": 0.0, "avg_logprob": -0.3070650100708008, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.024422315880656242}, {"id": 392, "seek": 187054, "start": 1893.86, "end": 1897.3, "text": " And so for a callback, the callback can access the learner using self.learn.", "tokens": [51530, 400, 370, 337, 257, 818, 3207, 11, 264, 818, 3207, 393, 2105, 264, 33347, 1228, 2698, 13, 306, 1083, 13, 51702], "temperature": 0.0, "avg_logprob": -0.3070650100708008, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.024422315880656242}, {"id": 393, "seek": 189730, "start": 1897.3, "end": 1903.34, "text": " So self.learn.preds, there's self.learn.model, passing in self.learn.batch, and just the", "tokens": [50364, 407, 2698, 13, 306, 1083, 13, 79, 986, 82, 11, 456, 311, 2698, 13, 306, 1083, 13, 8014, 338, 11, 8437, 294, 2698, 13, 306, 1083, 13, 65, 852, 11, 293, 445, 264, 50666], "temperature": 0.0, "avg_logprob": -0.22991817369373566, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.04535017907619476}, {"id": 394, "seek": 189730, "start": 1903.34, "end": 1905.34, "text": " independent variables.", "tokens": [50666, 6695, 9102, 13, 50766], "temperature": 0.0, "avg_logprob": -0.22991817369373566, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.04535017907619476}, {"id": 395, "seek": 189730, "start": 1905.34, "end": 1914.6599999999999, "text": " Ditto for the loss, calls the loss function, backward, step, zero gret.", "tokens": [50766, 413, 34924, 337, 264, 4470, 11, 5498, 264, 4470, 2445, 11, 23897, 11, 1823, 11, 4018, 290, 1505, 13, 51232], "temperature": 0.0, "avg_logprob": -0.22991817369373566, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.04535017907619476}, {"id": 396, "seek": 189730, "start": 1914.6599999999999, "end": 1920.1, "text": " So that's, at this point this isn't doing anything that it wasn't doing before, but", "tokens": [51232, 407, 300, 311, 11, 412, 341, 935, 341, 1943, 380, 884, 1340, 300, 309, 2067, 380, 884, 949, 11, 457, 51504], "temperature": 0.0, "avg_logprob": -0.22991817369373566, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.04535017907619476}, {"id": 397, "seek": 189730, "start": 1920.1, "end": 1925.46, "text": " the nice thing is now if you want to use HuggingFace accelerate, or you want something that works", "tokens": [51504, 264, 1481, 551, 307, 586, 498, 291, 528, 281, 764, 46892, 3249, 37, 617, 21341, 11, 420, 291, 528, 746, 300, 1985, 51772], "temperature": 0.0, "avg_logprob": -0.22991817369373566, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.04535017907619476}, {"id": 398, "seek": 192546, "start": 1925.46, "end": 1933.94, "text": " on HuggingFace data styles dictionary things, or whatever, you can actually change exactly", "tokens": [50364, 322, 46892, 3249, 37, 617, 1412, 13273, 25890, 721, 11, 420, 2035, 11, 291, 393, 767, 1319, 2293, 50788], "temperature": 0.0, "avg_logprob": -0.2534370937862912, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.02556496672332287}, {"id": 399, "seek": 192546, "start": 1933.94, "end": 1939.94, "text": " how it behaves by just call passing, by creating a callback for training.", "tokens": [50788, 577, 309, 36896, 538, 445, 818, 8437, 11, 538, 4084, 257, 818, 3207, 337, 3097, 13, 51088], "temperature": 0.0, "avg_logprob": -0.2534370937862912, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.02556496672332287}, {"id": 400, "seek": 192546, "start": 1939.94, "end": 1942.8600000000001, "text": " And if you want everything except one thing to be the same, you can inherit from train", "tokens": [51088, 400, 498, 291, 528, 1203, 3993, 472, 551, 281, 312, 264, 912, 11, 291, 393, 21389, 490, 3847, 51234], "temperature": 0.0, "avg_logprob": -0.2534370937862912, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.02556496672332287}, {"id": 401, "seek": 192546, "start": 1942.8600000000001, "end": 1943.8600000000001, "text": " CB.", "tokens": [51234, 18745, 13, 51284], "temperature": 0.0, "avg_logprob": -0.2534370937862912, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.02556496672332287}, {"id": 402, "seek": 192546, "start": 1943.8600000000001, "end": 1949.42, "text": " So this is, I've not tried this before, I haven't seen this done anywhere else, so it's", "tokens": [51284, 407, 341, 307, 11, 286, 600, 406, 3031, 341, 949, 11, 286, 2378, 380, 1612, 341, 1096, 4992, 1646, 11, 370, 309, 311, 51562], "temperature": 0.0, "avg_logprob": -0.2534370937862912, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.02556496672332287}, {"id": 403, "seek": 192546, "start": 1949.42, "end": 1950.66, "text": " a bit of an experiment.", "tokens": [51562, 257, 857, 295, 364, 5120, 13, 51624], "temperature": 0.0, "avg_logprob": -0.2534370937862912, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.02556496672332287}, {"id": 404, "seek": 192546, "start": 1950.66, "end": 1954.44, "text": " So I'm interested to hear how you go with it.", "tokens": [51624, 407, 286, 478, 3102, 281, 1568, 577, 291, 352, 365, 309, 13, 51813], "temperature": 0.0, "avg_logprob": -0.2534370937862912, "compression_ratio": 1.582375478927203, "no_speech_prob": 0.02556496672332287}, {"id": 405, "seek": 195444, "start": 1954.44, "end": 1957.8600000000001, "text": " And then finally, I thought it'd be nice to have a progress bar.", "tokens": [50364, 400, 550, 2721, 11, 286, 1194, 309, 1116, 312, 1481, 281, 362, 257, 4205, 2159, 13, 50535], "temperature": 0.0, "avg_logprob": -0.2531350977280561, "compression_ratio": 1.712707182320442, "no_speech_prob": 0.0025909417308866978}, {"id": 406, "seek": 195444, "start": 1957.8600000000001, "end": 1963.8, "text": " So let's create a progress callback, and the progress bar is going to show on it our current", "tokens": [50535, 407, 718, 311, 1884, 257, 4205, 818, 3207, 11, 293, 264, 4205, 2159, 307, 516, 281, 855, 322, 309, 527, 2190, 50832], "temperature": 0.0, "avg_logprob": -0.2531350977280561, "compression_ratio": 1.712707182320442, "no_speech_prob": 0.0025909417308866978}, {"id": 407, "seek": 195444, "start": 1963.8, "end": 1967.56, "text": " loss, and going to put create a plot of it.", "tokens": [50832, 4470, 11, 293, 516, 281, 829, 1884, 257, 7542, 295, 309, 13, 51020], "temperature": 0.0, "avg_logprob": -0.2531350977280561, "compression_ratio": 1.712707182320442, "no_speech_prob": 0.0025909417308866978}, {"id": 408, "seek": 195444, "start": 1967.56, "end": 1975.72, "text": " So I'm going to use a project that we created called fast progress, mainly created by the", "tokens": [51020, 407, 286, 478, 516, 281, 764, 257, 1716, 300, 321, 2942, 1219, 2370, 4205, 11, 8704, 2942, 538, 264, 51428], "temperature": 0.0, "avg_logprob": -0.2531350977280561, "compression_ratio": 1.712707182320442, "no_speech_prob": 0.0025909417308866978}, {"id": 409, "seek": 195444, "start": 1975.72, "end": 1981.14, "text": " wonderful Sylvain.", "tokens": [51428, 3715, 3902, 14574, 491, 13, 51699], "temperature": 0.0, "avg_logprob": -0.2531350977280561, "compression_ratio": 1.712707182320442, "no_speech_prob": 0.0025909417308866978}, {"id": 410, "seek": 198114, "start": 1981.14, "end": 1989.3400000000001, "text": " And basically fast progress is, yeah, a very nice way to create very flexible progress", "tokens": [50364, 400, 1936, 2370, 4205, 307, 11, 1338, 11, 257, 588, 1481, 636, 281, 1884, 588, 11358, 4205, 50774], "temperature": 0.0, "avg_logprob": -0.2778276846959041, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.006589689292013645}, {"id": 411, "seek": 198114, "start": 1989.3400000000001, "end": 1990.66, "text": " bars.", "tokens": [50774, 10228, 13, 50840], "temperature": 0.0, "avg_logprob": -0.2778276846959041, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.006589689292013645}, {"id": 412, "seek": 198114, "start": 1990.66, "end": 1992.74, "text": " So let me show you what it looks like first.", "tokens": [50840, 407, 718, 385, 855, 291, 437, 309, 1542, 411, 700, 13, 50944], "temperature": 0.0, "avg_logprob": -0.2778276846959041, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.006589689292013645}, {"id": 413, "seek": 198114, "start": 1992.74, "end": 1997.92, "text": " So let's get the model, and train, and as you can see it actually in real time updates", "tokens": [50944, 407, 718, 311, 483, 264, 2316, 11, 293, 3847, 11, 293, 382, 291, 393, 536, 309, 767, 294, 957, 565, 9205, 51203], "temperature": 0.0, "avg_logprob": -0.2778276846959041, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.006589689292013645}, {"id": 414, "seek": 198114, "start": 1997.92, "end": 1999.98, "text": " the graph, and everything.", "tokens": [51203, 264, 4295, 11, 293, 1203, 13, 51306], "temperature": 0.0, "avg_logprob": -0.2778276846959041, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.006589689292013645}, {"id": 415, "seek": 198114, "start": 1999.98, "end": 2002.7800000000002, "text": " There you go, that's pretty cool.", "tokens": [51306, 821, 291, 352, 11, 300, 311, 1238, 1627, 13, 51446], "temperature": 0.0, "avg_logprob": -0.2778276846959041, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.006589689292013645}, {"id": 416, "seek": 198114, "start": 2002.7800000000002, "end": 2008.8200000000002, "text": " So that's the, that's the progress bar, the metrics callback, the device callback, and", "tokens": [51446, 407, 300, 311, 264, 11, 300, 311, 264, 4205, 2159, 11, 264, 16367, 818, 3207, 11, 264, 4302, 818, 3207, 11, 293, 51748], "temperature": 0.0, "avg_logprob": -0.2778276846959041, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.006589689292013645}, {"id": 417, "seek": 200882, "start": 2008.82, "end": 2011.34, "text": " the training callback all in action.", "tokens": [50364, 264, 3097, 818, 3207, 439, 294, 3069, 13, 50490], "temperature": 0.0, "avg_logprob": -0.19670501285129124, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.010327952913939953}, {"id": 418, "seek": 200882, "start": 2011.34, "end": 2018.8999999999999, "text": " So before we fit, we actually have to set self.learn.epox.", "tokens": [50490, 407, 949, 321, 3318, 11, 321, 767, 362, 281, 992, 2698, 13, 306, 1083, 13, 595, 5230, 13, 50868], "temperature": 0.0, "avg_logprob": -0.19670501285129124, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.010327952913939953}, {"id": 419, "seek": 200882, "start": 2018.8999999999999, "end": 2025.58, "text": " Now that might look a little bit weird, but self.learn.epox is the thing that we loop", "tokens": [50868, 823, 300, 1062, 574, 257, 707, 857, 3657, 11, 457, 2698, 13, 306, 1083, 13, 595, 5230, 307, 264, 551, 300, 321, 6367, 51202], "temperature": 0.0, "avg_logprob": -0.19670501285129124, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.010327952913939953}, {"id": 420, "seek": 200882, "start": 2025.58, "end": 2027.98, "text": " through for self.epox in.", "tokens": [51202, 807, 337, 2698, 13, 595, 5230, 294, 13, 51322], "temperature": 0.0, "avg_logprob": -0.19670501285129124, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.010327952913939953}, {"id": 421, "seek": 200882, "start": 2027.98, "end": 2035.1399999999999, "text": " So we can change that, so it's not just a normal range, but instead it is a progress", "tokens": [51322, 407, 321, 393, 1319, 300, 11, 370, 309, 311, 406, 445, 257, 2710, 3613, 11, 457, 2602, 309, 307, 257, 4205, 51680], "temperature": 0.0, "avg_logprob": -0.19670501285129124, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.010327952913939953}, {"id": 422, "seek": 203514, "start": 2035.14, "end": 2039.5, "text": " bar around a range.", "tokens": [50364, 2159, 926, 257, 3613, 13, 50582], "temperature": 0.0, "avg_logprob": -0.25862446609808476, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.05500435829162598}, {"id": 423, "seek": 203514, "start": 2039.5, "end": 2043.14, "text": " We can then check, remember I told you that the learner is going to have the metrics attribute", "tokens": [50582, 492, 393, 550, 1520, 11, 1604, 286, 1907, 291, 300, 264, 33347, 307, 516, 281, 362, 264, 16367, 19667, 50764], "temperature": 0.0, "avg_logprob": -0.25862446609808476, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.05500435829162598}, {"id": 424, "seek": 203514, "start": 2043.14, "end": 2049.42, "text": " applied, we can then say, oh if the learner has a metrics attribute, then let's replace", "tokens": [50764, 6456, 11, 321, 393, 550, 584, 11, 1954, 498, 264, 33347, 575, 257, 16367, 19667, 11, 550, 718, 311, 7406, 51078], "temperature": 0.0, "avg_logprob": -0.25862446609808476, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.05500435829162598}, {"id": 425, "seek": 203514, "start": 2049.42, "end": 2053.06, "text": " the underscore log method there with ours.", "tokens": [51078, 264, 37556, 3565, 3170, 456, 365, 11896, 13, 51260], "temperature": 0.0, "avg_logprob": -0.25862446609808476, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.05500435829162598}, {"id": 426, "seek": 203514, "start": 2053.06, "end": 2057.2200000000003, "text": " And our one, instead we'll write to the progress bar.", "tokens": [51260, 400, 527, 472, 11, 2602, 321, 603, 2464, 281, 264, 4205, 2159, 13, 51468], "temperature": 0.0, "avg_logprob": -0.25862446609808476, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.05500435829162598}, {"id": 427, "seek": 203514, "start": 2057.2200000000003, "end": 2061.06, "text": " Now this is pretty simple, it looks very similar to before, but we could easily replace this", "tokens": [51468, 823, 341, 307, 1238, 2199, 11, 309, 1542, 588, 2531, 281, 949, 11, 457, 321, 727, 3612, 7406, 341, 51660], "temperature": 0.0, "avg_logprob": -0.25862446609808476, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.05500435829162598}, {"id": 428, "seek": 206106, "start": 2061.06, "end": 2065.7799999999997, "text": " for example with something that creates an HTML table, which is another thing fast progress", "tokens": [50364, 337, 1365, 365, 746, 300, 7829, 364, 17995, 3199, 11, 597, 307, 1071, 551, 2370, 4205, 50600], "temperature": 0.0, "avg_logprob": -0.2295503807067871, "compression_ratio": 1.668, "no_speech_prob": 0.03161809965968132}, {"id": 429, "seek": 206106, "start": 2065.7799999999997, "end": 2067.62, "text": " does, or other stuff like that.", "tokens": [50600, 775, 11, 420, 661, 1507, 411, 300, 13, 50692], "temperature": 0.0, "avg_logprob": -0.2295503807067871, "compression_ratio": 1.668, "no_speech_prob": 0.03161809965968132}, {"id": 430, "seek": 206106, "start": 2067.62, "end": 2074.66, "text": " So you can see we can modify, the nice thing is we can modify how our metrics are displayed.", "tokens": [50692, 407, 291, 393, 536, 321, 393, 16927, 11, 264, 1481, 551, 307, 321, 393, 16927, 577, 527, 16367, 366, 16372, 13, 51044], "temperature": 0.0, "avg_logprob": -0.2295503807067871, "compression_ratio": 1.668, "no_speech_prob": 0.03161809965968132}, {"id": 431, "seek": 206106, "start": 2074.66, "end": 2078.5, "text": " So that's a very powerful thing that Python lets us do, is actually replace one piece", "tokens": [51044, 407, 300, 311, 257, 588, 4005, 551, 300, 15329, 6653, 505, 360, 11, 307, 767, 7406, 472, 2522, 51236], "temperature": 0.0, "avg_logprob": -0.2295503807067871, "compression_ratio": 1.668, "no_speech_prob": 0.03161809965968132}, {"id": 432, "seek": 206106, "start": 2078.5, "end": 2086.7799999999997, "text": " of code with another, and that's the whole purpose of why the metrics callback had this", "tokens": [51236, 295, 3089, 365, 1071, 11, 293, 300, 311, 264, 1379, 4334, 295, 983, 264, 16367, 818, 3207, 632, 341, 51650], "temperature": 0.0, "avg_logprob": -0.2295503807067871, "compression_ratio": 1.668, "no_speech_prob": 0.03161809965968132}, {"id": 433, "seek": 206106, "start": 2086.7799999999997, "end": 2088.54, "text": " underscore log separately.", "tokens": [51650, 37556, 3565, 14759, 13, 51738], "temperature": 0.0, "avg_logprob": -0.2295503807067871, "compression_ratio": 1.668, "no_speech_prob": 0.03161809965968132}, {"id": 434, "seek": 208854, "start": 2088.54, "end": 2091.3, "text": " So why didn't I just say print here?", "tokens": [50364, 407, 983, 994, 380, 286, 445, 584, 4482, 510, 30, 50502], "temperature": 0.0, "avg_logprob": -0.23272321465310086, "compression_ratio": 1.5115207373271888, "no_speech_prob": 0.005139551125466824}, {"id": 435, "seek": 208854, "start": 2091.3, "end": 2097.38, "text": " That's because this way classes can replace how the metrics are displayed.", "tokens": [50502, 663, 311, 570, 341, 636, 5359, 393, 7406, 577, 264, 16367, 366, 16372, 13, 50806], "temperature": 0.0, "avg_logprob": -0.23272321465310086, "compression_ratio": 1.5115207373271888, "no_speech_prob": 0.005139551125466824}, {"id": 436, "seek": 208854, "start": 2097.38, "end": 2103.3, "text": " So we could change that to like send them over to weights and biases for example, or", "tokens": [50806, 407, 321, 727, 1319, 300, 281, 411, 2845, 552, 670, 281, 17443, 293, 32152, 337, 1365, 11, 420, 51102], "temperature": 0.0, "avg_logprob": -0.23272321465310086, "compression_ratio": 1.5115207373271888, "no_speech_prob": 0.005139551125466824}, {"id": 437, "seek": 208854, "start": 2103.3, "end": 2110.42, "text": " you know create visualizations or so forth.", "tokens": [51102, 291, 458, 1884, 5056, 14455, 420, 370, 5220, 13, 51458], "temperature": 0.0, "avg_logprob": -0.23272321465310086, "compression_ratio": 1.5115207373271888, "no_speech_prob": 0.005139551125466824}, {"id": 438, "seek": 208854, "start": 2110.42, "end": 2116.22, "text": " So before epoch we do a very similar thing, the self.learn.dl iterator, we change it to", "tokens": [51458, 407, 949, 30992, 339, 321, 360, 257, 588, 2531, 551, 11, 264, 2698, 13, 306, 1083, 13, 67, 75, 17138, 1639, 11, 321, 1319, 309, 281, 51748], "temperature": 0.0, "avg_logprob": -0.23272321465310086, "compression_ratio": 1.5115207373271888, "no_speech_prob": 0.005139551125466824}, {"id": 439, "seek": 211622, "start": 2116.22, "end": 2124.66, "text": " have a progress bar wrapped around it, and then after each bar we set the progress bar's", "tokens": [50364, 362, 257, 4205, 2159, 14226, 926, 309, 11, 293, 550, 934, 1184, 2159, 321, 992, 264, 4205, 2159, 311, 50786], "temperature": 0.0, "avg_logprob": -0.3095130920410156, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.001754610799252987}, {"id": 440, "seek": 211622, "start": 2124.66, "end": 2126.8999999999996, "text": " comment to be the loss.", "tokens": [50786, 2871, 281, 312, 264, 4470, 13, 50898], "temperature": 0.0, "avg_logprob": -0.3095130920410156, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.001754610799252987}, {"id": 441, "seek": 211622, "start": 2126.8999999999996, "end": 2131.22, "text": " It's going to print, it's going to show the loss on the progress bar as it goes, and if", "tokens": [50898, 467, 311, 516, 281, 4482, 11, 309, 311, 516, 281, 855, 264, 4470, 322, 264, 4205, 2159, 382, 309, 1709, 11, 293, 498, 51114], "temperature": 0.0, "avg_logprob": -0.3095130920410156, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.001754610799252987}, {"id": 442, "seek": 211622, "start": 2131.22, "end": 2141.7799999999997, "text": " we've asked for a plot then we will append the losses to a list of losses, and we will", "tokens": [51114, 321, 600, 2351, 337, 257, 7542, 550, 321, 486, 34116, 264, 15352, 281, 257, 1329, 295, 15352, 11, 293, 321, 486, 51642], "temperature": 0.0, "avg_logprob": -0.3095130920410156, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.001754610799252987}, {"id": 443, "seek": 214178, "start": 2141.78, "end": 2151.2200000000003, "text": " update the graph with the losses and the batch numbers.", "tokens": [50364, 5623, 264, 4295, 365, 264, 15352, 293, 264, 15245, 3547, 13, 50836], "temperature": 0.0, "avg_logprob": -0.3146933317184448, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.07369401305913925}, {"id": 444, "seek": 214178, "start": 2151.2200000000003, "end": 2160.82, "text": " So there we have it, we have a, yeah, nice working learner which is I think the most", "tokens": [50836, 407, 456, 321, 362, 309, 11, 321, 362, 257, 11, 1338, 11, 1481, 1364, 33347, 597, 307, 286, 519, 264, 881, 51316], "temperature": 0.0, "avg_logprob": -0.3146933317184448, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.07369401305913925}, {"id": 445, "seek": 214178, "start": 2160.82, "end": 2166.1000000000004, "text": " flexible learner that training loop probably that's, I hope, has ever been written, because", "tokens": [51316, 11358, 33347, 300, 3097, 6367, 1391, 300, 311, 11, 286, 1454, 11, 575, 1562, 668, 3720, 11, 570, 51580], "temperature": 0.0, "avg_logprob": -0.3146933317184448, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.07369401305913925}, {"id": 446, "seek": 214178, "start": 2166.1000000000004, "end": 2169.9, "text": " I think the fastai2 one was the most flexible that had ever been written before, and this", "tokens": [51580, 286, 519, 264, 2370, 1301, 17, 472, 390, 264, 881, 11358, 300, 632, 1562, 668, 3720, 949, 11, 293, 341, 51770], "temperature": 0.0, "avg_logprob": -0.3146933317184448, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.07369401305913925}, {"id": 447, "seek": 216990, "start": 2169.9, "end": 2172.26, "text": " is more flexible.", "tokens": [50364, 307, 544, 11358, 13, 50482], "temperature": 0.0, "avg_logprob": -0.253695805867513, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.007937953807413578}, {"id": 448, "seek": 216990, "start": 2172.26, "end": 2180.0, "text": " And the nice thing is you can make this your own, you know, you can, you know, fully understand", "tokens": [50482, 400, 264, 1481, 551, 307, 291, 393, 652, 341, 428, 1065, 11, 291, 458, 11, 291, 393, 11, 291, 458, 11, 4498, 1223, 50869], "temperature": 0.0, "avg_logprob": -0.253695805867513, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.007937953807413578}, {"id": 449, "seek": 216990, "start": 2180.0, "end": 2185.5, "text": " this training loop, so it's kind of like you can use a framework, but it's a framework", "tokens": [50869, 341, 3097, 6367, 11, 370, 309, 311, 733, 295, 411, 291, 393, 764, 257, 8388, 11, 457, 309, 311, 257, 8388, 51144], "temperature": 0.0, "avg_logprob": -0.253695805867513, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.007937953807413578}, {"id": 450, "seek": 216990, "start": 2185.5, "end": 2188.6600000000003, "text": " in which you're totally in control of it, and you can make it work exactly how you want", "tokens": [51144, 294, 597, 291, 434, 3879, 294, 1969, 295, 309, 11, 293, 291, 393, 652, 309, 589, 2293, 577, 291, 528, 51302], "temperature": 0.0, "avg_logprob": -0.253695805867513, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.007937953807413578}, {"id": 451, "seek": 216990, "start": 2188.6600000000003, "end": 2189.6600000000003, "text": " to.", "tokens": [51302, 281, 13, 51352], "temperature": 0.0, "avg_logprob": -0.253695805867513, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.007937953807413578}, {"id": 452, "seek": 216990, "start": 2189.6600000000003, "end": 2195.78, "text": " Ideally, not by changing the learner itself, ideally by creating callbacks, but if you", "tokens": [51352, 40817, 11, 406, 538, 4473, 264, 33347, 2564, 11, 22915, 538, 4084, 818, 17758, 11, 457, 498, 291, 51658], "temperature": 0.0, "avg_logprob": -0.253695805867513, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.007937953807413578}, {"id": 453, "seek": 219578, "start": 2195.78, "end": 2200.6200000000003, "text": " want to, you could certainly, like look at that, the whole learner fits on a single screen,", "tokens": [50364, 528, 281, 11, 291, 727, 3297, 11, 411, 574, 412, 300, 11, 264, 1379, 33347, 9001, 322, 257, 2167, 2568, 11, 50606], "temperature": 0.0, "avg_logprob": -0.29635742436284607, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.14607803523540497}, {"id": 454, "seek": 219578, "start": 2200.6200000000003, "end": 2204.0800000000004, "text": " so you could certainly change that.", "tokens": [50606, 370, 291, 727, 3297, 1319, 300, 13, 50779], "temperature": 0.0, "avg_logprob": -0.29635742436284607, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.14607803523540497}, {"id": 455, "seek": 219578, "start": 2204.0800000000004, "end": 2207.98, "text": " We haven't added inference yet, although that shouldn't be too much to add, I guess we have", "tokens": [50779, 492, 2378, 380, 3869, 38253, 1939, 11, 4878, 300, 4659, 380, 312, 886, 709, 281, 909, 11, 286, 2041, 321, 362, 50974], "temperature": 0.0, "avg_logprob": -0.29635742436284607, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.14607803523540497}, {"id": 456, "seek": 219578, "start": 2207.98, "end": 2210.3, "text": " to do that at some point.", "tokens": [50974, 281, 360, 300, 412, 512, 935, 13, 51090], "temperature": 0.0, "avg_logprob": -0.29635742436284607, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.14607803523540497}, {"id": 457, "seek": 219578, "start": 2210.3, "end": 2222.34, "text": " Okay, now interestingly, I love this about Python, it's so flexible, when we said self.predict,", "tokens": [51090, 1033, 11, 586, 25873, 11, 286, 959, 341, 466, 15329, 11, 309, 311, 370, 11358, 11, 562, 321, 848, 2698, 13, 79, 24945, 11, 51692], "temperature": 0.0, "avg_logprob": -0.29635742436284607, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.14607803523540497}, {"id": 458, "seek": 222234, "start": 2222.34, "end": 2227.6600000000003, "text": " self.getLoss, I said if they don't exist, then it's going to use getAttr, and it's going", "tokens": [50364, 2698, 13, 847, 43, 772, 11, 286, 848, 498, 436, 500, 380, 2514, 11, 550, 309, 311, 516, 281, 764, 483, 38151, 81, 11, 293, 309, 311, 516, 50630], "temperature": 0.0, "avg_logprob": -0.2716116042364211, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.047424428164958954}, {"id": 459, "seek": 222234, "start": 2227.6600000000003, "end": 2230.6600000000003, "text": " to try to find those in the callbacks.", "tokens": [50630, 281, 853, 281, 915, 729, 294, 264, 818, 17758, 13, 50780], "temperature": 0.0, "avg_logprob": -0.2716116042364211, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.047424428164958954}, {"id": 460, "seek": 222234, "start": 2230.6600000000003, "end": 2235.42, "text": " And in fact, you could have multiple callbacks that define these things, and then they would", "tokens": [50780, 400, 294, 1186, 11, 291, 727, 362, 3866, 818, 17758, 300, 6964, 613, 721, 11, 293, 550, 436, 576, 51018], "temperature": 0.0, "avg_logprob": -0.2716116042364211, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.047424428164958954}, {"id": 461, "seek": 222234, "start": 2235.42, "end": 2239.7400000000002, "text": " chain them together, which would be kind of interesting.", "tokens": [51018, 5021, 552, 1214, 11, 597, 576, 312, 733, 295, 1880, 13, 51234], "temperature": 0.0, "avg_logprob": -0.2716116042364211, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.047424428164958954}, {"id": 462, "seek": 222234, "start": 2239.7400000000002, "end": 2245.6200000000003, "text": " But there's another way we could make these exist, which is, which is that we could subclass", "tokens": [51234, 583, 456, 311, 1071, 636, 321, 727, 652, 613, 2514, 11, 597, 307, 11, 597, 307, 300, 321, 727, 1422, 11665, 51528], "temperature": 0.0, "avg_logprob": -0.2716116042364211, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.047424428164958954}, {"id": 463, "seek": 222234, "start": 2245.6200000000003, "end": 2246.82, "text": " this.", "tokens": [51528, 341, 13, 51588], "temperature": 0.0, "avg_logprob": -0.2716116042364211, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.047424428164958954}, {"id": 464, "seek": 224682, "start": 2246.82, "end": 2253.02, "text": " So let's not use trainCB, just to show us how this would work, and instead we're going", "tokens": [50364, 407, 718, 311, 406, 764, 3847, 34, 33, 11, 445, 281, 855, 505, 577, 341, 576, 589, 11, 293, 2602, 321, 434, 516, 50674], "temperature": 0.0, "avg_logprob": -0.21920655568440756, "compression_ratio": 1.8, "no_speech_prob": 0.0174422524869442}, {"id": 465, "seek": 224682, "start": 2253.02, "end": 2257.1000000000004, "text": " to use a subclass.", "tokens": [50674, 281, 764, 257, 1422, 11665, 13, 50878], "temperature": 0.0, "avg_logprob": -0.21920655568440756, "compression_ratio": 1.8, "no_speech_prob": 0.0174422524869442}, {"id": 466, "seek": 224682, "start": 2257.1000000000004, "end": 2260.94, "text": " So here I'm going to subclass learner, and I'm going to override the five.", "tokens": [50878, 407, 510, 286, 478, 516, 281, 1422, 11665, 33347, 11, 293, 286, 478, 516, 281, 42321, 264, 1732, 13, 51070], "temperature": 0.0, "avg_logprob": -0.21920655568440756, "compression_ratio": 1.8, "no_speech_prob": 0.0174422524869442}, {"id": 467, "seek": 224682, "start": 2260.94, "end": 2265.42, "text": " Well, it's not exactly overriding, I didn't have any definition of them before, so I'm", "tokens": [51070, 1042, 11, 309, 311, 406, 2293, 670, 81, 2819, 11, 286, 994, 380, 362, 604, 7123, 295, 552, 949, 11, 370, 286, 478, 51294], "temperature": 0.0, "avg_logprob": -0.21920655568440756, "compression_ratio": 1.8, "no_speech_prob": 0.0174422524869442}, {"id": 468, "seek": 224682, "start": 2265.42, "end": 2268.98, "text": " going to define the five directly in the learner subclass.", "tokens": [51294, 516, 281, 6964, 264, 1732, 3838, 294, 264, 33347, 1422, 11665, 13, 51472], "temperature": 0.0, "avg_logprob": -0.21920655568440756, "compression_ratio": 1.8, "no_speech_prob": 0.0174422524869442}, {"id": 469, "seek": 224682, "start": 2268.98, "end": 2272.9, "text": " So that way it's never going to end up going to getAttr, because getAttr is only called", "tokens": [51472, 407, 300, 636, 309, 311, 1128, 516, 281, 917, 493, 516, 281, 483, 38151, 81, 11, 570, 483, 38151, 81, 307, 787, 1219, 51668], "temperature": 0.0, "avg_logprob": -0.21920655568440756, "compression_ratio": 1.8, "no_speech_prob": 0.0174422524869442}, {"id": 470, "seek": 227290, "start": 2272.9, "end": 2282.14, "text": " if something doesn't exist.", "tokens": [50364, 498, 746, 1177, 380, 2514, 13, 50826], "temperature": 0.0, "avg_logprob": -0.2484740836947572, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.31065860390663147}, {"id": 471, "seek": 227290, "start": 2282.14, "end": 2287.9, "text": " So here it's basically all, these five are exactly the same as in our train callback,", "tokens": [50826, 407, 510, 309, 311, 1936, 439, 11, 613, 1732, 366, 2293, 264, 912, 382, 294, 527, 3847, 818, 3207, 11, 51114], "temperature": 0.0, "avg_logprob": -0.2484740836947572, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.31065860390663147}, {"id": 472, "seek": 227290, "start": 2287.9, "end": 2291.54, "text": " except we don't need self.learn anymore, we can just use self, because we're now in the", "tokens": [51114, 3993, 321, 500, 380, 643, 2698, 13, 306, 1083, 3602, 11, 321, 393, 445, 764, 2698, 11, 570, 321, 434, 586, 294, 264, 51296], "temperature": 0.0, "avg_logprob": -0.2484740836947572, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.31065860390663147}, {"id": 473, "seek": 227290, "start": 2291.54, "end": 2292.54, "text": " learner.", "tokens": [51296, 33347, 13, 51346], "temperature": 0.0, "avg_logprob": -0.2484740836947572, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.31065860390663147}, {"id": 474, "seek": 227290, "start": 2292.54, "end": 2297.2200000000003, "text": " But I've changed zero grad to do something a bit crazy.", "tokens": [51346, 583, 286, 600, 3105, 4018, 2771, 281, 360, 746, 257, 857, 3219, 13, 51580], "temperature": 0.0, "avg_logprob": -0.2484740836947572, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.31065860390663147}, {"id": 475, "seek": 227290, "start": 2297.2200000000003, "end": 2300.82, "text": " I'm not sure if this has been done before, I haven't seen it, but maybe it's an old trick", "tokens": [51580, 286, 478, 406, 988, 498, 341, 575, 668, 1096, 949, 11, 286, 2378, 380, 1612, 309, 11, 457, 1310, 309, 311, 364, 1331, 4282, 51760], "temperature": 0.0, "avg_logprob": -0.2484740836947572, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.31065860390663147}, {"id": 476, "seek": 230082, "start": 2300.9, "end": 2302.94, "text": " that I just haven't come across.", "tokens": [50368, 300, 286, 445, 2378, 380, 808, 2108, 13, 50470], "temperature": 0.0, "avg_logprob": -0.2623152732849121, "compression_ratio": 1.4903846153846154, "no_speech_prob": 0.002981015248224139}, {"id": 477, "seek": 230082, "start": 2302.94, "end": 2309.02, "text": " But it occurred to me, zero grad, which remember is the thing that we call after we take the", "tokens": [50470, 583, 309, 11068, 281, 385, 11, 4018, 2771, 11, 597, 1604, 307, 264, 551, 300, 321, 818, 934, 321, 747, 264, 50774], "temperature": 0.0, "avg_logprob": -0.2623152732849121, "compression_ratio": 1.4903846153846154, "no_speech_prob": 0.002981015248224139}, {"id": 478, "seek": 230082, "start": 2309.02, "end": 2314.6200000000003, "text": " optimizer step, doesn't actually have to zero the gradients at all.", "tokens": [50774, 5028, 6545, 1823, 11, 1177, 380, 767, 362, 281, 4018, 264, 2771, 2448, 412, 439, 13, 51054], "temperature": 0.0, "avg_logprob": -0.2623152732849121, "compression_ratio": 1.4903846153846154, "no_speech_prob": 0.002981015248224139}, {"id": 479, "seek": 230082, "start": 2314.6200000000003, "end": 2320.5800000000004, "text": " What if instead of zeroing the gradients, we multiplied them by some number, like say", "tokens": [51054, 708, 498, 2602, 295, 4018, 278, 264, 2771, 2448, 11, 321, 17207, 552, 538, 512, 1230, 11, 411, 584, 51352], "temperature": 0.0, "avg_logprob": -0.2623152732849121, "compression_ratio": 1.4903846153846154, "no_speech_prob": 0.002981015248224139}, {"id": 480, "seek": 230082, "start": 2320.5800000000004, "end": 2326.3, "text": " 0.85?", "tokens": [51352, 1958, 13, 19287, 30, 51638], "temperature": 0.0, "avg_logprob": -0.2623152732849121, "compression_ratio": 1.4903846153846154, "no_speech_prob": 0.002981015248224139}, {"id": 481, "seek": 230082, "start": 2326.3, "end": 2328.86, "text": " Well what would that do?", "tokens": [51638, 1042, 437, 576, 300, 360, 30, 51766], "temperature": 0.0, "avg_logprob": -0.2623152732849121, "compression_ratio": 1.4903846153846154, "no_speech_prob": 0.002981015248224139}, {"id": 482, "seek": 232886, "start": 2328.9, "end": 2338.38, "text": " Well what it would do is it would mean that your previous gradients would still be there,", "tokens": [50366, 1042, 437, 309, 576, 360, 307, 309, 576, 914, 300, 428, 3894, 2771, 2448, 576, 920, 312, 456, 11, 50840], "temperature": 0.0, "avg_logprob": -0.21872128573330965, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.002323157386854291}, {"id": 483, "seek": 232886, "start": 2338.38, "end": 2340.1800000000003, "text": " but they would be reduced a bit.", "tokens": [50840, 457, 436, 576, 312, 9212, 257, 857, 13, 50930], "temperature": 0.0, "avg_logprob": -0.21872128573330965, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.002323157386854291}, {"id": 484, "seek": 232886, "start": 2340.1800000000003, "end": 2347.54, "text": " And remember what happens in PyTorch is PyTorch always adds the gradients to the existing", "tokens": [50930, 400, 1604, 437, 2314, 294, 9953, 51, 284, 339, 307, 9953, 51, 284, 339, 1009, 10860, 264, 2771, 2448, 281, 264, 6741, 51298], "temperature": 0.0, "avg_logprob": -0.21872128573330965, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.002323157386854291}, {"id": 485, "seek": 232886, "start": 2347.54, "end": 2351.2400000000002, "text": " gradients, and that's why we normally have to call zero grad.", "tokens": [51298, 2771, 2448, 11, 293, 300, 311, 983, 321, 5646, 362, 281, 818, 4018, 2771, 13, 51483], "temperature": 0.0, "avg_logprob": -0.21872128573330965, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.002323157386854291}, {"id": 486, "seek": 232886, "start": 2351.2400000000002, "end": 2355.42, "text": " But if instead we multiply the gradients by some number, I mean we should really make", "tokens": [51483, 583, 498, 2602, 321, 12972, 264, 2771, 2448, 538, 512, 1230, 11, 286, 914, 321, 820, 534, 652, 51692], "temperature": 0.0, "avg_logprob": -0.21872128573330965, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.002323157386854291}, {"id": 487, "seek": 232886, "start": 2355.42, "end": 2356.42, "text": " this a parameter.", "tokens": [51692, 341, 257, 13075, 13, 51742], "temperature": 0.0, "avg_logprob": -0.21872128573330965, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.002323157386854291}, {"id": 488, "seek": 232886, "start": 2356.42, "end": 2358.2000000000003, "text": " Let's do that, shall we?", "tokens": [51742, 961, 311, 360, 300, 11, 4393, 321, 30, 51831], "temperature": 0.0, "avg_logprob": -0.21872128573330965, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.002323157386854291}, {"id": 489, "seek": 235820, "start": 2358.54, "end": 2364.24, "text": " So let's create a parameter.", "tokens": [50381, 407, 718, 311, 1884, 257, 13075, 13, 50666], "temperature": 0.0, "avg_logprob": -0.4028840686963952, "compression_ratio": 1.2568807339449541, "no_speech_prob": 0.05665078014135361}, {"id": 490, "seek": 235820, "start": 2364.24, "end": 2368.16, "text": " So probably, there's a few ways we could do this.", "tokens": [50666, 407, 1391, 11, 456, 311, 257, 1326, 2098, 321, 727, 360, 341, 13, 50862], "temperature": 0.0, "avg_logprob": -0.4028840686963952, "compression_ratio": 1.2568807339449541, "no_speech_prob": 0.05665078014135361}, {"id": 491, "seek": 235820, "start": 2368.16, "end": 2374.3999999999996, "text": " Well let's do it properly.", "tokens": [50862, 1042, 718, 311, 360, 309, 6108, 13, 51174], "temperature": 0.0, "avg_logprob": -0.4028840686963952, "compression_ratio": 1.2568807339449541, "no_speech_prob": 0.05665078014135361}, {"id": 492, "seek": 235820, "start": 2374.3999999999996, "end": 2380.5, "text": " We've got a little bit of time.", "tokens": [51174, 492, 600, 658, 257, 707, 857, 295, 565, 13, 51479], "temperature": 0.0, "avg_logprob": -0.4028840686963952, "compression_ratio": 1.2568807339449541, "no_speech_prob": 0.05665078014135361}, {"id": 493, "seek": 238050, "start": 2380.5, "end": 2392.42, "text": " So we could say, well, maybe we'll just copy and paste all those over here, and we'll add", "tokens": [50364, 407, 321, 727, 584, 11, 731, 11, 1310, 321, 603, 445, 5055, 293, 9163, 439, 729, 670, 510, 11, 293, 321, 603, 909, 50960], "temperature": 0.0, "avg_logprob": -0.4512584474351671, "compression_ratio": 1.1125, "no_speech_prob": 0.05834127962589264}, {"id": 494, "seek": 239242, "start": 2392.42, "end": 2410.46, "text": " momentum, momentum equals 0.85, self.momentum equals momentum, and then super.", "tokens": [50364, 11244, 11, 11244, 6915, 1958, 13, 19287, 11, 2698, 13, 42544, 317, 449, 6915, 11244, 11, 293, 550, 1687, 13, 51266], "temperature": 0.0, "avg_logprob": -0.3014415502548218, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.025956343859434128}, {"id": 495, "seek": 239242, "start": 2410.46, "end": 2418.5, "text": " So make sure you call the super classes passing in all the stuff.", "tokens": [51266, 407, 652, 988, 291, 818, 264, 1687, 5359, 8437, 294, 439, 264, 1507, 13, 51668], "temperature": 0.0, "avg_logprob": -0.3014415502548218, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.025956343859434128}, {"id": 496, "seek": 241850, "start": 2418.5, "end": 2422.9, "text": " We could use delegates for this and quags, that would be possibly another way of doing", "tokens": [50364, 492, 727, 764, 45756, 337, 341, 293, 421, 12109, 11, 300, 576, 312, 6264, 1071, 636, 295, 884, 50584], "temperature": 0.0, "avg_logprob": -0.2597825004940941, "compression_ratio": 1.5493562231759657, "no_speech_prob": 0.02556428872048855}, {"id": 497, "seek": 241850, "start": 2422.9, "end": 2426.34, "text": " it, but let's just do this for now.", "tokens": [50584, 309, 11, 457, 718, 311, 445, 360, 341, 337, 586, 13, 50756], "temperature": 0.0, "avg_logprob": -0.2597825004940941, "compression_ratio": 1.5493562231759657, "no_speech_prob": 0.02556428872048855}, {"id": 498, "seek": 241850, "start": 2426.34, "end": 2433.5, "text": " Okay and then so there we wouldn't make it 0.85, we would make it self.momentum.", "tokens": [50756, 1033, 293, 550, 370, 456, 321, 2759, 380, 652, 309, 1958, 13, 19287, 11, 321, 576, 652, 309, 2698, 13, 42544, 317, 449, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2597825004940941, "compression_ratio": 1.5493562231759657, "no_speech_prob": 0.02556428872048855}, {"id": 499, "seek": 241850, "start": 2433.5, "end": 2440.86, "text": " So you'll see now, still trains, but there's no train cb callback anymore in my list.", "tokens": [51114, 407, 291, 603, 536, 586, 11, 920, 16329, 11, 457, 456, 311, 572, 3847, 269, 65, 818, 3207, 3602, 294, 452, 1329, 13, 51482], "temperature": 0.0, "avg_logprob": -0.2597825004940941, "compression_ratio": 1.5493562231759657, "no_speech_prob": 0.02556428872048855}, {"id": 500, "seek": 241850, "start": 2440.86, "end": 2446.3, "text": " I don't need one because I've defined the five methods in the subclass.", "tokens": [51482, 286, 500, 380, 643, 472, 570, 286, 600, 7642, 264, 1732, 7150, 294, 264, 1422, 11665, 13, 51754], "temperature": 0.0, "avg_logprob": -0.2597825004940941, "compression_ratio": 1.5493562231759657, "no_speech_prob": 0.02556428872048855}, {"id": 501, "seek": 244630, "start": 2446.3, "end": 2453.1400000000003, "text": " Now this training at the same learning rate for the same time, the accuracy, I don't think", "tokens": [50364, 823, 341, 3097, 412, 264, 912, 2539, 3314, 337, 264, 912, 565, 11, 264, 14170, 11, 286, 500, 380, 519, 50706], "temperature": 0.0, "avg_logprob": -0.3977957758410224, "compression_ratio": 1.4407894736842106, "no_speech_prob": 0.38488757610321045}, {"id": 502, "seek": 244630, "start": 2453.1400000000003, "end": 2459.46, "text": " it improves by more, let's run them all.", "tokens": [50706, 309, 24771, 538, 544, 11, 718, 311, 1190, 552, 439, 13, 51022], "temperature": 0.0, "avg_logprob": -0.3977957758410224, "compression_ratio": 1.4407894736842106, "no_speech_prob": 0.38488757610321045}, {"id": 503, "seek": 244630, "start": 2459.46, "end": 2469.42, "text": " Yeah this is a lot like gradient accumulation callback, they're kind of cooler I think.", "tokens": [51022, 865, 341, 307, 257, 688, 411, 16235, 35647, 818, 3207, 11, 436, 434, 733, 295, 15566, 286, 519, 13, 51520], "temperature": 0.0, "avg_logprob": -0.3977957758410224, "compression_ratio": 1.4407894736842106, "no_speech_prob": 0.38488757610321045}, {"id": 504, "seek": 246942, "start": 2469.42, "end": 2478.2200000000003, "text": " Okay so the, let's see, the loss has gone from 0.8 to 0.55, and the accuracy has gone", "tokens": [50364, 1033, 370, 264, 11, 718, 311, 536, 11, 264, 4470, 575, 2780, 490, 1958, 13, 23, 281, 1958, 13, 13622, 11, 293, 264, 14170, 575, 2780, 50804], "temperature": 0.0, "avg_logprob": -0.20682145436604818, "compression_ratio": 1.4484848484848485, "no_speech_prob": 0.07055576145648956}, {"id": 505, "seek": 246942, "start": 2478.2200000000003, "end": 2484.38, "text": " from about 0.7 to about 0.8, so they've improved.", "tokens": [50804, 490, 466, 1958, 13, 22, 281, 466, 1958, 13, 23, 11, 370, 436, 600, 9689, 13, 51112], "temperature": 0.0, "avg_logprob": -0.20682145436604818, "compression_ratio": 1.4484848484848485, "no_speech_prob": 0.07055576145648956}, {"id": 506, "seek": 246942, "start": 2484.38, "end": 2485.62, "text": " Why is that?", "tokens": [51112, 1545, 307, 300, 30, 51174], "temperature": 0.0, "avg_logprob": -0.20682145436604818, "compression_ratio": 1.4484848484848485, "no_speech_prob": 0.07055576145648956}, {"id": 507, "seek": 246942, "start": 2485.62, "end": 2493.14, "text": " Well we're going to be learning a lot more about this pretty shortly, but basically what's", "tokens": [51174, 1042, 321, 434, 516, 281, 312, 2539, 257, 688, 544, 466, 341, 1238, 13392, 11, 457, 1936, 437, 311, 51550], "temperature": 0.0, "avg_logprob": -0.20682145436604818, "compression_ratio": 1.4484848484848485, "no_speech_prob": 0.07055576145648956}, {"id": 508, "seek": 249314, "start": 2493.14, "end": 2502.3399999999997, "text": " happening here, but basically what's happening here is we have just implemented in a very", "tokens": [50364, 2737, 510, 11, 457, 1936, 437, 311, 2737, 510, 307, 321, 362, 445, 12270, 294, 257, 588, 50824], "temperature": 0.0, "avg_logprob": -0.26100416652491837, "compression_ratio": 1.5542168674698795, "no_speech_prob": 0.01472807489335537}, {"id": 509, "seek": 249314, "start": 2502.3399999999997, "end": 2506.66, "text": " interesting way, which I haven't seen done before, something called momentum.", "tokens": [50824, 1880, 636, 11, 597, 286, 2378, 380, 1612, 1096, 949, 11, 746, 1219, 11244, 13, 51040], "temperature": 0.0, "avg_logprob": -0.26100416652491837, "compression_ratio": 1.5542168674698795, "no_speech_prob": 0.01472807489335537}, {"id": 510, "seek": 249314, "start": 2506.66, "end": 2513.2599999999998, "text": " And basically what momentum does is it say like, imagine you are, you know, you're trying,", "tokens": [51040, 400, 1936, 437, 11244, 775, 307, 309, 584, 411, 11, 3811, 291, 366, 11, 291, 458, 11, 291, 434, 1382, 11, 51370], "temperature": 0.0, "avg_logprob": -0.26100416652491837, "compression_ratio": 1.5542168674698795, "no_speech_prob": 0.01472807489335537}, {"id": 511, "seek": 251326, "start": 2513.26, "end": 2524.82, "text": " you've got some kind of complex contour loss surface, right, and you know, so imagine these", "tokens": [50364, 291, 600, 658, 512, 733, 295, 3997, 21234, 4470, 3753, 11, 558, 11, 293, 291, 458, 11, 370, 3811, 613, 50942], "temperature": 0.0, "avg_logprob": -0.3228417319812994, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.006097462493926287}, {"id": 512, "seek": 251326, "start": 2524.82, "end": 2529.1400000000003, "text": " are hills with a marble, very similar, right, and your marble's up here.", "tokens": [50942, 366, 21379, 365, 257, 26844, 11, 588, 2531, 11, 558, 11, 293, 428, 26844, 311, 493, 510, 13, 51158], "temperature": 0.0, "avg_logprob": -0.3228417319812994, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.006097462493926287}, {"id": 513, "seek": 251326, "start": 2529.1400000000003, "end": 2536.0200000000004, "text": " What would normally happen with gradient descent is it would go, you know, in the direction", "tokens": [51158, 708, 576, 5646, 1051, 365, 16235, 23475, 307, 309, 576, 352, 11, 291, 458, 11, 294, 264, 3513, 51502], "temperature": 0.0, "avg_logprob": -0.3228417319812994, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.006097462493926287}, {"id": 514, "seek": 251326, "start": 2536.0200000000004, "end": 2541.6600000000003, "text": " downhill, which is this way, so it'll go over here, and then over here, right.", "tokens": [51502, 29929, 11, 597, 307, 341, 636, 11, 370, 309, 603, 352, 670, 510, 11, 293, 550, 670, 510, 11, 558, 13, 51784], "temperature": 0.0, "avg_logprob": -0.3228417319812994, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.006097462493926287}, {"id": 515, "seek": 254166, "start": 2541.66, "end": 2543.8999999999996, "text": " Very slow.", "tokens": [50364, 4372, 2964, 13, 50476], "temperature": 0.0, "avg_logprob": -0.26716573945768585, "compression_ratio": 1.691542288557214, "no_speech_prob": 0.038464490324258804}, {"id": 516, "seek": 254166, "start": 2543.8999999999996, "end": 2549.1, "text": " What momentum does is it's, is the first step's the same, and then the second step says, oh", "tokens": [50476, 708, 11244, 775, 307, 309, 311, 11, 307, 264, 700, 1823, 311, 264, 912, 11, 293, 550, 264, 1150, 1823, 1619, 11, 1954, 50736], "temperature": 0.0, "avg_logprob": -0.26716573945768585, "compression_ratio": 1.691542288557214, "no_speech_prob": 0.038464490324258804}, {"id": 517, "seek": 254166, "start": 2549.1, "end": 2555.46, "text": " I wanted to go this way, but I'm going to add together the previous direction plus the", "tokens": [50736, 286, 1415, 281, 352, 341, 636, 11, 457, 286, 478, 516, 281, 909, 1214, 264, 3894, 3513, 1804, 264, 51054], "temperature": 0.0, "avg_logprob": -0.26716573945768585, "compression_ratio": 1.691542288557214, "no_speech_prob": 0.038464490324258804}, {"id": 518, "seek": 254166, "start": 2555.46, "end": 2558.48, "text": " new direction, but reduce the previous direction a bit.", "tokens": [51054, 777, 3513, 11, 457, 5407, 264, 3894, 3513, 257, 857, 13, 51205], "temperature": 0.0, "avg_logprob": -0.26716573945768585, "compression_ratio": 1.691542288557214, "no_speech_prob": 0.038464490324258804}, {"id": 519, "seek": 254166, "start": 2558.48, "end": 2562.8999999999996, "text": " So that would actually make me end up about here.", "tokens": [51205, 407, 300, 576, 767, 652, 385, 917, 493, 466, 510, 13, 51426], "temperature": 0.0, "avg_logprob": -0.26716573945768585, "compression_ratio": 1.691542288557214, "no_speech_prob": 0.038464490324258804}, {"id": 520, "seek": 254166, "start": 2562.8999999999996, "end": 2565.58, "text": " And then the second one does the same thing.", "tokens": [51426, 400, 550, 264, 1150, 472, 775, 264, 912, 551, 13, 51560], "temperature": 0.0, "avg_logprob": -0.26716573945768585, "compression_ratio": 1.691542288557214, "no_speech_prob": 0.038464490324258804}, {"id": 521, "seek": 256558, "start": 2565.58, "end": 2572.66, "text": " And so momentum basically makes you much more quickly go to your destination.", "tokens": [50364, 400, 370, 11244, 1936, 1669, 291, 709, 544, 2661, 352, 281, 428, 12236, 13, 50718], "temperature": 0.0, "avg_logprob": -0.23216132067759102, "compression_ratio": 1.75390625, "no_speech_prob": 0.012624369002878666}, {"id": 522, "seek": 256558, "start": 2572.66, "end": 2577.22, "text": " So normally momentum is done, the reason I did it this way, partly to show you, it's", "tokens": [50718, 407, 5646, 11244, 307, 1096, 11, 264, 1778, 286, 630, 309, 341, 636, 11, 17031, 281, 855, 291, 11, 309, 311, 50946], "temperature": 0.0, "avg_logprob": -0.23216132067759102, "compression_ratio": 1.75390625, "no_speech_prob": 0.012624369002878666}, {"id": 523, "seek": 256558, "start": 2577.22, "end": 2583.02, "text": " just a bit of fun, a bit of interest, but it's very useful because normally momentum,", "tokens": [50946, 445, 257, 857, 295, 1019, 11, 257, 857, 295, 1179, 11, 457, 309, 311, 588, 4420, 570, 5646, 11244, 11, 51236], "temperature": 0.0, "avg_logprob": -0.23216132067759102, "compression_ratio": 1.75390625, "no_speech_prob": 0.012624369002878666}, {"id": 524, "seek": 256558, "start": 2583.02, "end": 2588.14, "text": " you have to store a complete copy basically of all the gradients, the momentum version", "tokens": [51236, 291, 362, 281, 3531, 257, 3566, 5055, 1936, 295, 439, 264, 2771, 2448, 11, 264, 11244, 3037, 51492], "temperature": 0.0, "avg_logprob": -0.23216132067759102, "compression_ratio": 1.75390625, "no_speech_prob": 0.012624369002878666}, {"id": 525, "seek": 256558, "start": 2588.14, "end": 2592.66, "text": " of the gradients, so that you can kind of keep track of that, that running exponentially", "tokens": [51492, 295, 264, 2771, 2448, 11, 370, 300, 291, 393, 733, 295, 1066, 2837, 295, 300, 11, 300, 2614, 37330, 51718], "temperature": 0.0, "avg_logprob": -0.23216132067759102, "compression_ratio": 1.75390625, "no_speech_prob": 0.012624369002878666}, {"id": 526, "seek": 256558, "start": 2592.66, "end": 2594.36, "text": " weighted moving average.", "tokens": [51718, 32807, 2684, 4274, 13, 51803], "temperature": 0.0, "avg_logprob": -0.23216132067759102, "compression_ratio": 1.75390625, "no_speech_prob": 0.012624369002878666}, {"id": 527, "seek": 259436, "start": 2594.6400000000003, "end": 2601.7200000000003, "text": " But using this trick, you're actually using the dot grad themselves to store the exponentially", "tokens": [50378, 583, 1228, 341, 4282, 11, 291, 434, 767, 1228, 264, 5893, 2771, 2969, 281, 3531, 264, 37330, 50732], "temperature": 0.0, "avg_logprob": -0.31658724582556524, "compression_ratio": 1.4923857868020305, "no_speech_prob": 0.011687114834785461}, {"id": 528, "seek": 259436, "start": 2601.7200000000003, "end": 2603.2000000000003, "text": " weighted moving average.", "tokens": [50732, 32807, 2684, 4274, 13, 50806], "temperature": 0.0, "avg_logprob": -0.31658724582556524, "compression_ratio": 1.4923857868020305, "no_speech_prob": 0.011687114834785461}, {"id": 529, "seek": 259436, "start": 2603.2000000000003, "end": 2607.6800000000003, "text": " So anyway, there's a little bit of fun, which hopefully, particularly those of you who are", "tokens": [50806, 407, 4033, 11, 456, 311, 257, 707, 857, 295, 1019, 11, 597, 4696, 11, 4098, 729, 295, 291, 567, 366, 51030], "temperature": 0.0, "avg_logprob": -0.31658724582556524, "compression_ratio": 1.4923857868020305, "no_speech_prob": 0.011687114834785461}, {"id": 530, "seek": 259436, "start": 2607.6800000000003, "end": 2615.92, "text": " interested in accelerated optimizers and memory saving, might find a bit inspiring.", "tokens": [51030, 3102, 294, 29763, 5028, 22525, 293, 4675, 6816, 11, 1062, 915, 257, 857, 15883, 13, 51442], "temperature": 0.0, "avg_logprob": -0.31658724582556524, "compression_ratio": 1.4923857868020305, "no_speech_prob": 0.011687114834785461}, {"id": 531, "seek": 261592, "start": 2615.92, "end": 2625.48, "text": " All right, there's one more callback I'm going to show before the break, which is the", "tokens": [50364, 1057, 558, 11, 456, 311, 472, 544, 818, 3207, 286, 478, 516, 281, 855, 949, 264, 1821, 11, 597, 307, 264, 50842], "temperature": 0.0, "avg_logprob": -0.2559534384279835, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.028868908062577248}, {"id": 532, "seek": 261592, "start": 2625.48, "end": 2627.44, "text": " wonderful learning rate finder.", "tokens": [50842, 3715, 2539, 3314, 915, 260, 13, 50940], "temperature": 0.0, "avg_logprob": -0.2559534384279835, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.028868908062577248}, {"id": 533, "seek": 261592, "start": 2627.44, "end": 2631.52, "text": " I'm assuming that anybody who's watching this already is familiar with the learning rate", "tokens": [50940, 286, 478, 11926, 300, 4472, 567, 311, 1976, 341, 1217, 307, 4963, 365, 264, 2539, 3314, 51144], "temperature": 0.0, "avg_logprob": -0.2559534384279835, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.028868908062577248}, {"id": 534, "seek": 261592, "start": 2631.52, "end": 2633.64, "text": " finder from fast.ai.", "tokens": [51144, 915, 260, 490, 2370, 13, 1301, 13, 51250], "temperature": 0.0, "avg_logprob": -0.2559534384279835, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.028868908062577248}, {"id": 535, "seek": 261592, "start": 2633.64, "end": 2639.12, "text": " If you're not, there's lots of videos and tutorials around about it.", "tokens": [51250, 759, 291, 434, 406, 11, 456, 311, 3195, 295, 2145, 293, 17616, 926, 466, 309, 13, 51524], "temperature": 0.0, "avg_logprob": -0.2559534384279835, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.028868908062577248}, {"id": 536, "seek": 261592, "start": 2639.12, "end": 2643.34, "text": " It's an idea that comes from a paper by Leslie Smith from a few years ago.", "tokens": [51524, 467, 311, 364, 1558, 300, 1487, 490, 257, 3035, 538, 28140, 8538, 490, 257, 1326, 924, 2057, 13, 51735], "temperature": 0.0, "avg_logprob": -0.2559534384279835, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.028868908062577248}, {"id": 537, "seek": 264334, "start": 2643.34, "end": 2649.26, "text": " And the basic idea is that we will increase the learning rate.", "tokens": [50364, 400, 264, 3875, 1558, 307, 300, 321, 486, 3488, 264, 2539, 3314, 13, 50660], "temperature": 0.0, "avg_logprob": -0.24277983771430123, "compression_ratio": 1.8756756756756756, "no_speech_prob": 0.0029809745028615}, {"id": 538, "seek": 264334, "start": 2649.26, "end": 2650.98, "text": " I should have put titles on this.", "tokens": [50660, 286, 820, 362, 829, 12992, 322, 341, 13, 50746], "temperature": 0.0, "avg_logprob": -0.24277983771430123, "compression_ratio": 1.8756756756756756, "no_speech_prob": 0.0029809745028615}, {"id": 539, "seek": 264334, "start": 2650.98, "end": 2656.58, "text": " The x-axis here is learning rate, the y-axis here is loss.", "tokens": [50746, 440, 2031, 12, 24633, 510, 307, 2539, 3314, 11, 264, 288, 12, 24633, 510, 307, 4470, 13, 51026], "temperature": 0.0, "avg_logprob": -0.24277983771430123, "compression_ratio": 1.8756756756756756, "no_speech_prob": 0.0029809745028615}, {"id": 540, "seek": 264334, "start": 2656.58, "end": 2662.3, "text": " We increase the learning rate gradually over time, and we plot the loss against the learning", "tokens": [51026, 492, 3488, 264, 2539, 3314, 13145, 670, 565, 11, 293, 321, 7542, 264, 4470, 1970, 264, 2539, 51312], "temperature": 0.0, "avg_logprob": -0.24277983771430123, "compression_ratio": 1.8756756756756756, "no_speech_prob": 0.0029809745028615}, {"id": 541, "seek": 264334, "start": 2662.3, "end": 2669.1800000000003, "text": " rate, and we find how high can we bring the learning rate up before the loss starts getting", "tokens": [51312, 3314, 11, 293, 321, 915, 577, 1090, 393, 321, 1565, 264, 2539, 3314, 493, 949, 264, 4470, 3719, 1242, 51656], "temperature": 0.0, "avg_logprob": -0.24277983771430123, "compression_ratio": 1.8756756756756756, "no_speech_prob": 0.0029809745028615}, {"id": 542, "seek": 264334, "start": 2669.1800000000003, "end": 2670.1800000000003, "text": " worse.", "tokens": [51656, 5324, 13, 51706], "temperature": 0.0, "avg_logprob": -0.24277983771430123, "compression_ratio": 1.8756756756756756, "no_speech_prob": 0.0029809745028615}, {"id": 543, "seek": 267018, "start": 2670.18, "end": 2673.18, "text": " So we'd want roughly where about the steepest slope is.", "tokens": [50364, 407, 321, 1116, 528, 9810, 689, 466, 264, 16841, 377, 13525, 307, 13, 50514], "temperature": 0.0, "avg_logprob": -0.2559941235710593, "compression_ratio": 1.9036144578313252, "no_speech_prob": 0.023689216002821922}, {"id": 544, "seek": 267018, "start": 2673.18, "end": 2677.54, "text": " So probably here it would be about 0.1.", "tokens": [50514, 407, 1391, 510, 309, 576, 312, 466, 1958, 13, 16, 13, 50732], "temperature": 0.0, "avg_logprob": -0.2559941235710593, "compression_ratio": 1.9036144578313252, "no_speech_prob": 0.023689216002821922}, {"id": 545, "seek": 267018, "start": 2677.54, "end": 2680.8599999999997, "text": " So it'd be nice to create a learning rate finder.", "tokens": [50732, 407, 309, 1116, 312, 1481, 281, 1884, 257, 2539, 3314, 915, 260, 13, 50898], "temperature": 0.0, "avg_logprob": -0.2559941235710593, "compression_ratio": 1.9036144578313252, "no_speech_prob": 0.023689216002821922}, {"id": 546, "seek": 267018, "start": 2680.8599999999997, "end": 2684.8999999999996, "text": " So here's a learning rate finder callback.", "tokens": [50898, 407, 510, 311, 257, 2539, 3314, 915, 260, 818, 3207, 13, 51100], "temperature": 0.0, "avg_logprob": -0.2559941235710593, "compression_ratio": 1.9036144578313252, "no_speech_prob": 0.023689216002821922}, {"id": 547, "seek": 267018, "start": 2684.8999999999996, "end": 2688.1, "text": " So what a learning rate finder needs to do, well you have to tell it how much to multiply", "tokens": [51100, 407, 437, 257, 2539, 3314, 915, 260, 2203, 281, 360, 11, 731, 291, 362, 281, 980, 309, 577, 709, 281, 12972, 51260], "temperature": 0.0, "avg_logprob": -0.2559941235710593, "compression_ratio": 1.9036144578313252, "no_speech_prob": 0.023689216002821922}, {"id": 548, "seek": 267018, "start": 2688.1, "end": 2690.1, "text": " the learning rate by each batch.", "tokens": [51260, 264, 2539, 3314, 538, 1184, 15245, 13, 51360], "temperature": 0.0, "avg_logprob": -0.2559941235710593, "compression_ratio": 1.9036144578313252, "no_speech_prob": 0.023689216002821922}, {"id": 549, "seek": 267018, "start": 2690.1, "end": 2693.8199999999997, "text": " Let's say we add 30% to the learning rate each batch.", "tokens": [51360, 961, 311, 584, 321, 909, 2217, 4, 281, 264, 2539, 3314, 1184, 15245, 13, 51546], "temperature": 0.0, "avg_logprob": -0.2559941235710593, "compression_ratio": 1.9036144578313252, "no_speech_prob": 0.023689216002821922}, {"id": 550, "seek": 267018, "start": 2693.8199999999997, "end": 2695.18, "text": " So we'll store that.", "tokens": [51546, 407, 321, 603, 3531, 300, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2559941235710593, "compression_ratio": 1.9036144578313252, "no_speech_prob": 0.023689216002821922}, {"id": 551, "seek": 267018, "start": 2695.18, "end": 2699.54, "text": " So before we fit, we obviously need to keep track of the learning rates, and we need to", "tokens": [51614, 407, 949, 321, 3318, 11, 321, 2745, 643, 281, 1066, 2837, 295, 264, 2539, 6846, 11, 293, 321, 643, 281, 51832], "temperature": 0.0, "avg_logprob": -0.2559941235710593, "compression_ratio": 1.9036144578313252, "no_speech_prob": 0.023689216002821922}, {"id": 552, "seek": 269954, "start": 2699.66, "end": 2705.7, "text": " keep track of the losses, because those are the things that we put on a plot.", "tokens": [50370, 1066, 2837, 295, 264, 15352, 11, 570, 729, 366, 264, 721, 300, 321, 829, 322, 257, 7542, 13, 50672], "temperature": 0.0, "avg_logprob": -0.2235105442550947, "compression_ratio": 1.7161572052401746, "no_speech_prob": 0.005301824305206537}, {"id": 553, "seek": 269954, "start": 2705.7, "end": 2709.14, "text": " The other thing we have to do is decide when do we stop training.", "tokens": [50672, 440, 661, 551, 321, 362, 281, 360, 307, 4536, 562, 360, 321, 1590, 3097, 13, 50844], "temperature": 0.0, "avg_logprob": -0.2235105442550947, "compression_ratio": 1.7161572052401746, "no_speech_prob": 0.005301824305206537}, {"id": 554, "seek": 269954, "start": 2709.14, "end": 2711.86, "text": " So when is it clearly gone off the rails?", "tokens": [50844, 407, 562, 307, 309, 4448, 2780, 766, 264, 27649, 30, 50980], "temperature": 0.0, "avg_logprob": -0.2235105442550947, "compression_ratio": 1.7161572052401746, "no_speech_prob": 0.005301824305206537}, {"id": 555, "seek": 269954, "start": 2711.86, "end": 2718.54, "text": " And I decided that if the loss is three times higher than the minimum loss we've seen, then", "tokens": [50980, 400, 286, 3047, 300, 498, 264, 4470, 307, 1045, 1413, 2946, 813, 264, 7285, 4470, 321, 600, 1612, 11, 550, 51314], "temperature": 0.0, "avg_logprob": -0.2235105442550947, "compression_ratio": 1.7161572052401746, "no_speech_prob": 0.005301824305206537}, {"id": 556, "seek": 269954, "start": 2718.54, "end": 2719.7799999999997, "text": " we should stop.", "tokens": [51314, 321, 820, 1590, 13, 51376], "temperature": 0.0, "avg_logprob": -0.2235105442550947, "compression_ratio": 1.7161572052401746, "no_speech_prob": 0.005301824305206537}, {"id": 557, "seek": 269954, "start": 2719.7799999999997, "end": 2723.82, "text": " So we're going to keep track of the minimum loss, and so let's just initially set that", "tokens": [51376, 407, 321, 434, 516, 281, 1066, 2837, 295, 264, 7285, 4470, 11, 293, 370, 718, 311, 445, 9105, 992, 300, 51578], "temperature": 0.0, "avg_logprob": -0.2235105442550947, "compression_ratio": 1.7161572052401746, "no_speech_prob": 0.005301824305206537}, {"id": 558, "seek": 269954, "start": 2723.82, "end": 2725.3, "text": " to infinity.", "tokens": [51578, 281, 13202, 13, 51652], "temperature": 0.0, "avg_logprob": -0.2235105442550947, "compression_ratio": 1.7161572052401746, "no_speech_prob": 0.005301824305206537}, {"id": 559, "seek": 272530, "start": 2725.3, "end": 2726.94, "text": " That's a nice big number.", "tokens": [50364, 663, 311, 257, 1481, 955, 1230, 13, 50446], "temperature": 0.0, "avg_logprob": -0.24317236299867984, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.027168849483132362}, {"id": 560, "seek": 272530, "start": 2726.94, "end": 2731.2200000000003, "text": " Well not quite a number, but a number-ish like thing.", "tokens": [50446, 1042, 406, 1596, 257, 1230, 11, 457, 257, 1230, 12, 742, 411, 551, 13, 50660], "temperature": 0.0, "avg_logprob": -0.24317236299867984, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.027168849483132362}, {"id": 561, "seek": 272530, "start": 2731.2200000000003, "end": 2734.7000000000003, "text": " So then after every batch, first of all let's check that we're training.", "tokens": [50660, 407, 550, 934, 633, 15245, 11, 700, 295, 439, 718, 311, 1520, 300, 321, 434, 3097, 13, 50834], "temperature": 0.0, "avg_logprob": -0.24317236299867984, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.027168849483132362}, {"id": 562, "seek": 272530, "start": 2734.7000000000003, "end": 2737.9, "text": " Okay if we're not training, then we don't want to do anything.", "tokens": [50834, 1033, 498, 321, 434, 406, 3097, 11, 550, 321, 500, 380, 528, 281, 360, 1340, 13, 50994], "temperature": 0.0, "avg_logprob": -0.24317236299867984, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.027168849483132362}, {"id": 563, "seek": 272530, "start": 2737.9, "end": 2740.78, "text": " We don't use the learning rate finder during validation.", "tokens": [50994, 492, 500, 380, 764, 264, 2539, 3314, 915, 260, 1830, 24071, 13, 51138], "temperature": 0.0, "avg_logprob": -0.24317236299867984, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.027168849483132362}, {"id": 564, "seek": 272530, "start": 2740.78, "end": 2742.42, "text": " So here's a really handy thing.", "tokens": [51138, 407, 510, 311, 257, 534, 13239, 551, 13, 51220], "temperature": 0.0, "avg_logprob": -0.24317236299867984, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.027168849483132362}, {"id": 565, "seek": 272530, "start": 2742.42, "end": 2748.82, "text": " Just raise cancel epoch exception, and that stops it from doing that epoch entirely.", "tokens": [51220, 1449, 5300, 10373, 30992, 339, 11183, 11, 293, 300, 10094, 309, 490, 884, 300, 30992, 339, 7696, 13, 51540], "temperature": 0.0, "avg_logprob": -0.24317236299867984, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.027168849483132362}, {"id": 566, "seek": 274882, "start": 2748.82, "end": 2758.1800000000003, "text": " So just to see how that works, you can see here one epoch does with the callback context", "tokens": [50364, 407, 445, 281, 536, 577, 300, 1985, 11, 291, 393, 536, 510, 472, 30992, 339, 775, 365, 264, 818, 3207, 4319, 50832], "temperature": 0.0, "avg_logprob": -0.22315661938159498, "compression_ratio": 1.5963855421686748, "no_speech_prob": 0.0025509006809443235}, {"id": 567, "seek": 274882, "start": 2758.1800000000003, "end": 2763.3, "text": " manager epoch, and that will say, oh it's got cancelled.", "tokens": [50832, 6598, 30992, 339, 11, 293, 300, 486, 584, 11, 1954, 309, 311, 658, 25103, 13, 51088], "temperature": 0.0, "avg_logprob": -0.22315661938159498, "compression_ratio": 1.5963855421686748, "no_speech_prob": 0.0025509006809443235}, {"id": 568, "seek": 274882, "start": 2763.3, "end": 2767.9, "text": " So it goes straight to the except, which is going to go all the way to the end of that", "tokens": [51088, 407, 309, 1709, 2997, 281, 264, 3993, 11, 597, 307, 516, 281, 352, 439, 264, 636, 281, 264, 917, 295, 300, 51318], "temperature": 0.0, "avg_logprob": -0.22315661938159498, "compression_ratio": 1.5963855421686748, "no_speech_prob": 0.0025509006809443235}, {"id": 569, "seek": 274882, "start": 2767.9, "end": 2771.1400000000003, "text": " code, and it's going to skip it.", "tokens": [51318, 3089, 11, 293, 309, 311, 516, 281, 10023, 309, 13, 51480], "temperature": 0.0, "avg_logprob": -0.22315661938159498, "compression_ratio": 1.5963855421686748, "no_speech_prob": 0.0025509006809443235}, {"id": 570, "seek": 277114, "start": 2771.14, "end": 2779.8599999999997, "text": " So you can see that we're using exceptions as control structures, which is actually a", "tokens": [50364, 407, 291, 393, 536, 300, 321, 434, 1228, 22847, 382, 1969, 9227, 11, 597, 307, 767, 257, 50800], "temperature": 0.0, "avg_logprob": -0.2292499323000853, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.0038844349328428507}, {"id": 571, "seek": 277114, "start": 2779.8599999999997, "end": 2788.02, "text": " really powerful programming technique that is really underutilized, in my opinion.", "tokens": [50800, 534, 4005, 9410, 6532, 300, 307, 534, 833, 20835, 1602, 11, 294, 452, 4800, 13, 51208], "temperature": 0.0, "avg_logprob": -0.2292499323000853, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.0038844349328428507}, {"id": 572, "seek": 277114, "start": 2788.02, "end": 2792.9, "text": " Like a lot of things I do, it's actually somewhat controversial.", "tokens": [51208, 1743, 257, 688, 295, 721, 286, 360, 11, 309, 311, 767, 8344, 17323, 13, 51452], "temperature": 0.0, "avg_logprob": -0.2292499323000853, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.0038844349328428507}, {"id": 573, "seek": 277114, "start": 2792.9, "end": 2798.18, "text": " Some people think it's a bad idea, but I find it actually makes my code more concise, and", "tokens": [51452, 2188, 561, 519, 309, 311, 257, 1578, 1558, 11, 457, 286, 915, 309, 767, 1669, 452, 3089, 544, 44882, 11, 293, 51716], "temperature": 0.0, "avg_logprob": -0.2292499323000853, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.0038844349328428507}, {"id": 574, "seek": 277114, "start": 2798.18, "end": 2800.14, "text": " more maintainable, and more powerful.", "tokens": [51716, 544, 6909, 712, 11, 293, 544, 4005, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2292499323000853, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.0038844349328428507}, {"id": 575, "seek": 280014, "start": 2800.14, "end": 2803.5, "text": " So I like it.", "tokens": [50364, 407, 286, 411, 309, 13, 50532], "temperature": 0.0, "avg_logprob": -0.24191750902118106, "compression_ratio": 1.8555555555555556, "no_speech_prob": 0.004681775346398354}, {"id": 576, "seek": 280014, "start": 2803.5, "end": 2808.94, "text": " So let's see, yeah, so that's, we've got our cancel epoch exception.", "tokens": [50532, 407, 718, 311, 536, 11, 1338, 11, 370, 300, 311, 11, 321, 600, 658, 527, 10373, 30992, 339, 11183, 13, 50804], "temperature": 0.0, "avg_logprob": -0.24191750902118106, "compression_ratio": 1.8555555555555556, "no_speech_prob": 0.004681775346398354}, {"id": 577, "seek": 280014, "start": 2808.94, "end": 2813.06, "text": " So then we're just going to keep track of our learning rates.", "tokens": [50804, 407, 550, 321, 434, 445, 516, 281, 1066, 2837, 295, 527, 2539, 6846, 13, 51010], "temperature": 0.0, "avg_logprob": -0.24191750902118106, "compression_ratio": 1.8555555555555556, "no_speech_prob": 0.004681775346398354}, {"id": 578, "seek": 280014, "start": 2813.06, "end": 2816.66, "text": " The learning rates, we're going to learn a lot more about optimizers shortly, so I won't", "tokens": [51010, 440, 2539, 6846, 11, 321, 434, 516, 281, 1466, 257, 688, 544, 466, 5028, 22525, 13392, 11, 370, 286, 1582, 380, 51190], "temperature": 0.0, "avg_logprob": -0.24191750902118106, "compression_ratio": 1.8555555555555556, "no_speech_prob": 0.004681775346398354}, {"id": 579, "seek": 280014, "start": 2816.66, "end": 2820.9, "text": " worry too much about this, but basically the learning rates are stored by PyTorch inside", "tokens": [51190, 3292, 886, 709, 466, 341, 11, 457, 1936, 264, 2539, 6846, 366, 12187, 538, 9953, 51, 284, 339, 1854, 51402], "temperature": 0.0, "avg_logprob": -0.24191750902118106, "compression_ratio": 1.8555555555555556, "no_speech_prob": 0.004681775346398354}, {"id": 580, "seek": 280014, "start": 2820.9, "end": 2825.2999999999997, "text": " the optimizer, and they're actually stored in things called param groups, parameter groups.", "tokens": [51402, 264, 5028, 6545, 11, 293, 436, 434, 767, 12187, 294, 721, 1219, 6220, 3935, 11, 13075, 3935, 13, 51622], "temperature": 0.0, "avg_logprob": -0.24191750902118106, "compression_ratio": 1.8555555555555556, "no_speech_prob": 0.004681775346398354}, {"id": 581, "seek": 280014, "start": 2825.2999999999997, "end": 2828.8599999999997, "text": " So don't worry too much about the details, but we can grab the learning rate from that", "tokens": [51622, 407, 500, 380, 3292, 886, 709, 466, 264, 4365, 11, 457, 321, 393, 4444, 264, 2539, 3314, 490, 300, 51800], "temperature": 0.0, "avg_logprob": -0.24191750902118106, "compression_ratio": 1.8555555555555556, "no_speech_prob": 0.004681775346398354}, {"id": 582, "seek": 282886, "start": 2828.86, "end": 2832.34, "text": " dictionary, and we'll learn more about that shortly.", "tokens": [50364, 25890, 11, 293, 321, 603, 1466, 544, 466, 300, 13392, 13, 50538], "temperature": 0.0, "avg_logprob": -0.23489876182711855, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.20180076360702515}, {"id": 583, "seek": 282886, "start": 2832.34, "end": 2837.1400000000003, "text": " We've got to keep track of the loss, append it to our list of losses, and if it's less", "tokens": [50538, 492, 600, 658, 281, 1066, 2837, 295, 264, 4470, 11, 34116, 309, 281, 527, 1329, 295, 15352, 11, 293, 498, 309, 311, 1570, 50778], "temperature": 0.0, "avg_logprob": -0.23489876182711855, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.20180076360702515}, {"id": 584, "seek": 282886, "start": 2837.1400000000003, "end": 2842.3, "text": " than the minimum we've seen, then record it as the minimum, and if it's greater than three", "tokens": [50778, 813, 264, 7285, 321, 600, 1612, 11, 550, 2136, 309, 382, 264, 7285, 11, 293, 498, 309, 311, 5044, 813, 1045, 51036], "temperature": 0.0, "avg_logprob": -0.23489876182711855, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.20180076360702515}, {"id": 585, "seek": 282886, "start": 2842.3, "end": 2846.26, "text": " times the minimum, then look at this, this is really cool, cancel fit exception.", "tokens": [51036, 1413, 264, 7285, 11, 550, 574, 412, 341, 11, 341, 307, 534, 1627, 11, 10373, 3318, 11183, 13, 51234], "temperature": 0.0, "avg_logprob": -0.23489876182711855, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.20180076360702515}, {"id": 586, "seek": 282886, "start": 2846.26, "end": 2854.9, "text": " So this will stop everything in a very nice, clean way.", "tokens": [51234, 407, 341, 486, 1590, 1203, 294, 257, 588, 1481, 11, 2541, 636, 13, 51666], "temperature": 0.0, "avg_logprob": -0.23489876182711855, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.20180076360702515}, {"id": 587, "seek": 285490, "start": 2854.9, "end": 2859.62, "text": " No need for lots of returns and conditionals or stuff like that, just raise the cancel", "tokens": [50364, 883, 643, 337, 3195, 295, 11247, 293, 4188, 1124, 420, 1507, 411, 300, 11, 445, 5300, 264, 10373, 50600], "temperature": 0.0, "avg_logprob": -0.27215492248535156, "compression_ratio": 1.5738396624472575, "no_speech_prob": 0.14222107827663422}, {"id": 588, "seek": 285490, "start": 2859.62, "end": 2864.62, "text": " fit exception.", "tokens": [50600, 3318, 11183, 13, 50850], "temperature": 0.0, "avg_logprob": -0.27215492248535156, "compression_ratio": 1.5738396624472575, "no_speech_prob": 0.14222107827663422}, {"id": 589, "seek": 285490, "start": 2864.62, "end": 2870.46, "text": " And yeah, and then finally we've got to actually update our learning rate to 1.3 times the", "tokens": [50850, 400, 1338, 11, 293, 550, 2721, 321, 600, 658, 281, 767, 5623, 527, 2539, 3314, 281, 502, 13, 18, 1413, 264, 51142], "temperature": 0.0, "avg_logprob": -0.27215492248535156, "compression_ratio": 1.5738396624472575, "no_speech_prob": 0.14222107827663422}, {"id": 590, "seek": 285490, "start": 2870.46, "end": 2874.94, "text": " previous one, and so basically the way you do it in PyTorch is you have to go through", "tokens": [51142, 3894, 472, 11, 293, 370, 1936, 264, 636, 291, 360, 309, 294, 9953, 51, 284, 339, 307, 291, 362, 281, 352, 807, 51366], "temperature": 0.0, "avg_logprob": -0.27215492248535156, "compression_ratio": 1.5738396624472575, "no_speech_prob": 0.14222107827663422}, {"id": 591, "seek": 285490, "start": 2874.94, "end": 2880.54, "text": " each parameter group, and grab the learning rate in the dictionary, and multiply it by", "tokens": [51366, 1184, 13075, 1594, 11, 293, 4444, 264, 2539, 3314, 294, 264, 25890, 11, 293, 12972, 309, 538, 51646], "temperature": 0.0, "avg_logprob": -0.27215492248535156, "compression_ratio": 1.5738396624472575, "no_speech_prob": 0.14222107827663422}, {"id": 592, "seek": 285490, "start": 2880.54, "end": 2882.9, "text": " LRMult.", "tokens": [51646, 441, 49, 44, 723, 13, 51764], "temperature": 0.0, "avg_logprob": -0.27215492248535156, "compression_ratio": 1.5738396624472575, "no_speech_prob": 0.14222107827663422}, {"id": 593, "seek": 288290, "start": 2882.9, "end": 2889.06, "text": " So yeah, you've already seen it run, and we can, at the end of running, you will find", "tokens": [50364, 407, 1338, 11, 291, 600, 1217, 1612, 309, 1190, 11, 293, 321, 393, 11, 412, 264, 917, 295, 2614, 11, 291, 486, 915, 50672], "temperature": 0.0, "avg_logprob": -0.21040313188419785, "compression_ratio": 1.8273092369477912, "no_speech_prob": 0.0019267298048362136}, {"id": 594, "seek": 288290, "start": 2889.06, "end": 2895.28, "text": " that there is now a, the callback will now contain an LRs and a losses.", "tokens": [50672, 300, 456, 307, 586, 257, 11, 264, 818, 3207, 486, 586, 5304, 364, 441, 49, 82, 293, 257, 15352, 13, 50983], "temperature": 0.0, "avg_logprob": -0.21040313188419785, "compression_ratio": 1.8273092369477912, "no_speech_prob": 0.0019267298048362136}, {"id": 595, "seek": 288290, "start": 2895.28, "end": 2899.98, "text": " So for this callback, I can't just add it directly to the callback list.", "tokens": [50983, 407, 337, 341, 818, 3207, 11, 286, 393, 380, 445, 909, 309, 3838, 281, 264, 818, 3207, 1329, 13, 51218], "temperature": 0.0, "avg_logprob": -0.21040313188419785, "compression_ratio": 1.8273092369477912, "no_speech_prob": 0.0019267298048362136}, {"id": 596, "seek": 288290, "start": 2899.98, "end": 2904.5, "text": " I need to instantiate it first, and the reason I need to instantiate it first is because", "tokens": [51218, 286, 643, 281, 9836, 13024, 309, 700, 11, 293, 264, 1778, 286, 643, 281, 9836, 13024, 309, 700, 307, 570, 51444], "temperature": 0.0, "avg_logprob": -0.21040313188419785, "compression_ratio": 1.8273092369477912, "no_speech_prob": 0.0019267298048362136}, {"id": 597, "seek": 288290, "start": 2904.5, "end": 2907.7400000000002, "text": " I need to be able to grab its learning rates and its losses.", "tokens": [51444, 286, 643, 281, 312, 1075, 281, 4444, 1080, 2539, 6846, 293, 1080, 15352, 13, 51606], "temperature": 0.0, "avg_logprob": -0.21040313188419785, "compression_ratio": 1.8273092369477912, "no_speech_prob": 0.0019267298048362136}, {"id": 598, "seek": 288290, "start": 2907.7400000000002, "end": 2912.06, "text": " And in fact, you know, we could grab that whole thing and move it in here.", "tokens": [51606, 400, 294, 1186, 11, 291, 458, 11, 321, 727, 4444, 300, 1379, 551, 293, 1286, 309, 294, 510, 13, 51822], "temperature": 0.0, "avg_logprob": -0.21040313188419785, "compression_ratio": 1.8273092369477912, "no_speech_prob": 0.0019267298048362136}, {"id": 599, "seek": 291206, "start": 2912.22, "end": 2916.9, "text": " So there's no reason callbacks only have to have the callback things, right?", "tokens": [50372, 407, 456, 311, 572, 1778, 818, 17758, 787, 362, 281, 362, 264, 818, 3207, 721, 11, 558, 30, 50606], "temperature": 0.0, "avg_logprob": -0.24497558048793247, "compression_ratio": 1.448051948051948, "no_speech_prob": 0.06371370702981949}, {"id": 600, "seek": 291206, "start": 2916.9, "end": 2927.94, "text": " So we could do this, and now that's just going to become self.", "tokens": [50606, 407, 321, 727, 360, 341, 11, 293, 586, 300, 311, 445, 516, 281, 1813, 2698, 13, 51158], "temperature": 0.0, "avg_logprob": -0.24497558048793247, "compression_ratio": 1.448051948051948, "no_speech_prob": 0.06371370702981949}, {"id": 601, "seek": 291206, "start": 2927.94, "end": 2931.58, "text": " There we go.", "tokens": [51158, 821, 321, 352, 13, 51340], "temperature": 0.0, "avg_logprob": -0.24497558048793247, "compression_ratio": 1.448051948051948, "no_speech_prob": 0.06371370702981949}, {"id": 602, "seek": 291206, "start": 2931.58, "end": 2938.14, "text": " And so then we can train it again, and we could just call LRFind.plot.", "tokens": [51340, 400, 370, 550, 321, 393, 3847, 309, 797, 11, 293, 321, 727, 445, 818, 441, 49, 37, 471, 13, 564, 310, 13, 51668], "temperature": 0.0, "avg_logprob": -0.24497558048793247, "compression_ratio": 1.448051948051948, "no_speech_prob": 0.06371370702981949}, {"id": 603, "seek": 293814, "start": 2938.14, "end": 2945.5, "text": " So callbacks can really be, you know, quite self-contained nice things, as you can see.", "tokens": [50364, 407, 818, 17758, 393, 534, 312, 11, 291, 458, 11, 1596, 2698, 12, 9000, 3563, 1481, 721, 11, 382, 291, 393, 536, 13, 50732], "temperature": 0.0, "avg_logprob": -0.18572466352344613, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0011694964487105608}, {"id": 604, "seek": 293814, "start": 2945.5, "end": 2949.14, "text": " So there's a more sophisticated callback, and I think it's doing a lot of really nice", "tokens": [50732, 407, 456, 311, 257, 544, 16950, 818, 3207, 11, 293, 286, 519, 309, 311, 884, 257, 688, 295, 534, 1481, 50914], "temperature": 0.0, "avg_logprob": -0.18572466352344613, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0011694964487105608}, {"id": 605, "seek": 293814, "start": 2949.14, "end": 2952.02, "text": " stuff here.", "tokens": [50914, 1507, 510, 13, 51058], "temperature": 0.0, "avg_logprob": -0.18572466352344613, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0011694964487105608}, {"id": 606, "seek": 293814, "start": 2952.02, "end": 2958.22, "text": " You might have come across something in PyTorch called learning rate schedulers, and in fact", "tokens": [51058, 509, 1062, 362, 808, 2108, 746, 294, 9953, 51, 284, 339, 1219, 2539, 3314, 12000, 433, 11, 293, 294, 1186, 51368], "temperature": 0.0, "avg_logprob": -0.18572466352344613, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0011694964487105608}, {"id": 607, "seek": 293814, "start": 2958.22, "end": 2961.98, "text": " we could implement this whole thing with a learning rate scheduler.", "tokens": [51368, 321, 727, 4445, 341, 1379, 551, 365, 257, 2539, 3314, 12000, 260, 13, 51556], "temperature": 0.0, "avg_logprob": -0.18572466352344613, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0011694964487105608}, {"id": 608, "seek": 293814, "start": 2961.98, "end": 2966.68, "text": " It won't actually save that much time, but I just want to show you when you use stuff", "tokens": [51556, 467, 1582, 380, 767, 3155, 300, 709, 565, 11, 457, 286, 445, 528, 281, 855, 291, 562, 291, 764, 1507, 51791], "temperature": 0.0, "avg_logprob": -0.18572466352344613, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0011694964487105608}, {"id": 609, "seek": 296668, "start": 2966.68, "end": 2970.2, "text": " in PyTorch like learning rate schedulers, you're actually using things that are extremely", "tokens": [50364, 294, 9953, 51, 284, 339, 411, 2539, 3314, 12000, 433, 11, 291, 434, 767, 1228, 721, 300, 366, 4664, 50540], "temperature": 0.0, "avg_logprob": -0.26405906677246094, "compression_ratio": 1.6919831223628692, "no_speech_prob": 0.010013505816459656}, {"id": 610, "seek": 296668, "start": 2970.2, "end": 2971.3599999999997, "text": " simple.", "tokens": [50540, 2199, 13, 50598], "temperature": 0.0, "avg_logprob": -0.26405906677246094, "compression_ratio": 1.6919831223628692, "no_speech_prob": 0.010013505816459656}, {"id": 611, "seek": 296668, "start": 2971.3599999999997, "end": 2974.3999999999996, "text": " The learning rate scheduler basically does this one line of code for us.", "tokens": [50598, 440, 2539, 3314, 12000, 260, 1936, 775, 341, 472, 1622, 295, 3089, 337, 505, 13, 50750], "temperature": 0.0, "avg_logprob": -0.26405906677246094, "compression_ratio": 1.6919831223628692, "no_speech_prob": 0.010013505816459656}, {"id": 612, "seek": 296668, "start": 2974.3999999999996, "end": 2981.9199999999996, "text": " So I'm going to now create a new LRFinderCB, and this time I'm going to use the PyTorch's", "tokens": [50750, 407, 286, 478, 516, 281, 586, 1884, 257, 777, 441, 49, 37, 5669, 34, 33, 11, 293, 341, 565, 286, 478, 516, 281, 764, 264, 9953, 51, 284, 339, 311, 51126], "temperature": 0.0, "avg_logprob": -0.26405906677246094, "compression_ratio": 1.6919831223628692, "no_speech_prob": 0.010013505816459656}, {"id": 613, "seek": 296668, "start": 2981.9199999999996, "end": 2988.16, "text": " exponential LR scheduler, which is here.", "tokens": [51126, 21510, 441, 49, 12000, 260, 11, 597, 307, 510, 13, 51438], "temperature": 0.0, "avg_logprob": -0.26405906677246094, "compression_ratio": 1.6919831223628692, "no_speech_prob": 0.010013505816459656}, {"id": 614, "seek": 296668, "start": 2988.16, "end": 2993.72, "text": " So this is, now it's interesting that actually the documentation of this is kind of actually", "tokens": [51438, 407, 341, 307, 11, 586, 309, 311, 1880, 300, 767, 264, 14333, 295, 341, 307, 733, 295, 767, 51716], "temperature": 0.0, "avg_logprob": -0.26405906677246094, "compression_ratio": 1.6919831223628692, "no_speech_prob": 0.010013505816459656}, {"id": 615, "seek": 296668, "start": 2993.72, "end": 2994.72, "text": " wrong.", "tokens": [51716, 2085, 13, 51766], "temperature": 0.0, "avg_logprob": -0.26405906677246094, "compression_ratio": 1.6919831223628692, "no_speech_prob": 0.010013505816459656}, {"id": 616, "seek": 299472, "start": 2994.7599999999998, "end": 2999.8799999999997, "text": " It claims that it decays the learning rate of each parameter group by gamma, so gamma", "tokens": [50366, 467, 9441, 300, 309, 979, 3772, 264, 2539, 3314, 295, 1184, 13075, 1594, 538, 15546, 11, 370, 15546, 50622], "temperature": 0.0, "avg_logprob": -0.22404339438990542, "compression_ratio": 1.6186770428015564, "no_speech_prob": 5.1442180847516283e-05}, {"id": 617, "seek": 299472, "start": 2999.8799999999997, "end": 3002.24, "text": " is just some number you pass in.", "tokens": [50622, 307, 445, 512, 1230, 291, 1320, 294, 13, 50740], "temperature": 0.0, "avg_logprob": -0.22404339438990542, "compression_ratio": 1.6186770428015564, "no_speech_prob": 5.1442180847516283e-05}, {"id": 618, "seek": 299472, "start": 3002.24, "end": 3006.7599999999998, "text": " I don't know why this has to be a Greek letter, but it sounds more fancy than multiplying", "tokens": [50740, 286, 500, 380, 458, 983, 341, 575, 281, 312, 257, 10281, 5063, 11, 457, 309, 3263, 544, 10247, 813, 30955, 50966], "temperature": 0.0, "avg_logprob": -0.22404339438990542, "compression_ratio": 1.6186770428015564, "no_speech_prob": 5.1442180847516283e-05}, {"id": 619, "seek": 299472, "start": 3006.7599999999998, "end": 3007.7599999999998, "text": " by an LR multiplier.", "tokens": [50966, 538, 364, 441, 49, 44106, 13, 51016], "temperature": 0.0, "avg_logprob": -0.22404339438990542, "compression_ratio": 1.6186770428015564, "no_speech_prob": 5.1442180847516283e-05}, {"id": 620, "seek": 299472, "start": 3007.7599999999998, "end": 3013.3999999999996, "text": " It says every epoch, but it's not actually done every epoch at all.", "tokens": [51016, 467, 1619, 633, 30992, 339, 11, 457, 309, 311, 406, 767, 1096, 633, 30992, 339, 412, 439, 13, 51298], "temperature": 0.0, "avg_logprob": -0.22404339438990542, "compression_ratio": 1.6186770428015564, "no_speech_prob": 5.1442180847516283e-05}, {"id": 621, "seek": 299472, "start": 3013.3999999999996, "end": 3022.3199999999997, "text": " What actually happens is in PyTorch, the schedulers have a step method, and the decay happens", "tokens": [51298, 708, 767, 2314, 307, 294, 9953, 51, 284, 339, 11, 264, 12000, 433, 362, 257, 1823, 3170, 11, 293, 264, 21039, 2314, 51744], "temperature": 0.0, "avg_logprob": -0.22404339438990542, "compression_ratio": 1.6186770428015564, "no_speech_prob": 5.1442180847516283e-05}, {"id": 622, "seek": 299472, "start": 3022.3199999999997, "end": 3024.4199999999996, "text": " each time you call step.", "tokens": [51744, 1184, 565, 291, 818, 1823, 13, 51849], "temperature": 0.0, "avg_logprob": -0.22404339438990542, "compression_ratio": 1.6186770428015564, "no_speech_prob": 5.1442180847516283e-05}, {"id": 623, "seek": 302442, "start": 3025.12, "end": 3030.02, "text": " And if you set gamma, which is actually LRMult, to a number bigger than one, it's not a decay,", "tokens": [50399, 400, 498, 291, 992, 15546, 11, 597, 307, 767, 441, 49, 44, 723, 11, 281, 257, 1230, 3801, 813, 472, 11, 309, 311, 406, 257, 21039, 11, 50644], "temperature": 0.0, "avg_logprob": -0.27215872660721885, "compression_ratio": 1.5758928571428572, "no_speech_prob": 0.0003514376003295183}, {"id": 624, "seek": 302442, "start": 3030.02, "end": 3031.88, "text": " it's an increase.", "tokens": [50644, 309, 311, 364, 3488, 13, 50737], "temperature": 0.0, "avg_logprob": -0.27215872660721885, "compression_ratio": 1.5758928571428572, "no_speech_prob": 0.0003514376003295183}, {"id": 625, "seek": 302442, "start": 3031.88, "end": 3036.98, "text": " So the difference now, I guess I'll copy and paste the previous version.", "tokens": [50737, 407, 264, 2649, 586, 11, 286, 2041, 286, 603, 5055, 293, 9163, 264, 3894, 3037, 13, 50992], "temperature": 0.0, "avg_logprob": -0.27215872660721885, "compression_ratio": 1.5758928571428572, "no_speech_prob": 0.0003514376003295183}, {"id": 626, "seek": 302442, "start": 3036.98, "end": 3042.46, "text": " Okay, so the previous version is on the top.", "tokens": [50992, 1033, 11, 370, 264, 3894, 3037, 307, 322, 264, 1192, 13, 51266], "temperature": 0.0, "avg_logprob": -0.27215872660721885, "compression_ratio": 1.5758928571428572, "no_speech_prob": 0.0003514376003295183}, {"id": 627, "seek": 302442, "start": 3042.46, "end": 3046.98, "text": " So the main difference here is that before fit, we're going to create something called", "tokens": [51266, 407, 264, 2135, 2649, 510, 307, 300, 949, 3318, 11, 321, 434, 516, 281, 1884, 746, 1219, 51492], "temperature": 0.0, "avg_logprob": -0.27215872660721885, "compression_ratio": 1.5758928571428572, "no_speech_prob": 0.0003514376003295183}, {"id": 628, "seek": 302442, "start": 3046.98, "end": 3052.26, "text": " a self.shed equal to the scheduler.", "tokens": [51492, 257, 2698, 13, 2716, 292, 2681, 281, 264, 12000, 260, 13, 51756], "temperature": 0.0, "avg_logprob": -0.27215872660721885, "compression_ratio": 1.5758928571428572, "no_speech_prob": 0.0003514376003295183}, {"id": 629, "seek": 305226, "start": 3052.26, "end": 3054.7400000000002, "text": " And the scheduler, because it's going to be adjusting the learning rates, it actually", "tokens": [50364, 400, 264, 12000, 260, 11, 570, 309, 311, 516, 281, 312, 23559, 264, 2539, 6846, 11, 309, 767, 50488], "temperature": 0.0, "avg_logprob": -0.2150351369482839, "compression_ratio": 1.689516129032258, "no_speech_prob": 0.007815795950591564}, {"id": 630, "seek": 305226, "start": 3054.7400000000002, "end": 3056.46, "text": " needs access to the optimizer.", "tokens": [50488, 2203, 2105, 281, 264, 5028, 6545, 13, 50574], "temperature": 0.0, "avg_logprob": -0.2150351369482839, "compression_ratio": 1.689516129032258, "no_speech_prob": 0.007815795950591564}, {"id": 631, "seek": 305226, "start": 3056.46, "end": 3063.1400000000003, "text": " So we pass in the optimizer and the learning rate multiplier.", "tokens": [50574, 407, 321, 1320, 294, 264, 5028, 6545, 293, 264, 2539, 3314, 44106, 13, 50908], "temperature": 0.0, "avg_logprob": -0.2150351369482839, "compression_ratio": 1.689516129032258, "no_speech_prob": 0.007815795950591564}, {"id": 632, "seek": 305226, "start": 3063.1400000000003, "end": 3068.34, "text": " And so then in after batch, rather than having this line of code, we replace it with this", "tokens": [50908, 400, 370, 550, 294, 934, 15245, 11, 2831, 813, 1419, 341, 1622, 295, 3089, 11, 321, 7406, 309, 365, 341, 51168], "temperature": 0.0, "avg_logprob": -0.2150351369482839, "compression_ratio": 1.689516129032258, "no_speech_prob": 0.007815795950591564}, {"id": 633, "seek": 305226, "start": 3068.34, "end": 3071.34, "text": " line of code, self.shed.step.", "tokens": [51168, 1622, 295, 3089, 11, 2698, 13, 2716, 292, 13, 16792, 13, 51318], "temperature": 0.0, "avg_logprob": -0.2150351369482839, "compression_ratio": 1.689516129032258, "no_speech_prob": 0.007815795950591564}, {"id": 634, "seek": 305226, "start": 3071.34, "end": 3073.0200000000004, "text": " So that's the only difference.", "tokens": [51318, 407, 300, 311, 264, 787, 2649, 13, 51402], "temperature": 0.0, "avg_logprob": -0.2150351369482839, "compression_ratio": 1.689516129032258, "no_speech_prob": 0.007815795950591564}, {"id": 635, "seek": 305226, "start": 3073.0200000000004, "end": 3078.6600000000003, "text": " And you know, I mean, we're not gaining much, as I said, by using the PyTorch exponential", "tokens": [51402, 400, 291, 458, 11, 286, 914, 11, 321, 434, 406, 19752, 709, 11, 382, 286, 848, 11, 538, 1228, 264, 9953, 51, 284, 339, 21510, 51684], "temperature": 0.0, "avg_logprob": -0.2150351369482839, "compression_ratio": 1.689516129032258, "no_speech_prob": 0.007815795950591564}, {"id": 636, "seek": 307866, "start": 3078.66, "end": 3079.66, "text": " LR scheduler.", "tokens": [50364, 441, 49, 12000, 260, 13, 50414], "temperature": 0.0, "avg_logprob": -0.2907830845225941, "compression_ratio": 1.5650224215246638, "no_speech_prob": 0.4339280128479004}, {"id": 637, "seek": 307866, "start": 3079.66, "end": 3084.14, "text": " But I mainly wanted to do it so you can see that these things like PyTorch schedulers", "tokens": [50414, 583, 286, 8704, 1415, 281, 360, 309, 370, 291, 393, 536, 300, 613, 721, 411, 9953, 51, 284, 339, 12000, 433, 50638], "temperature": 0.0, "avg_logprob": -0.2907830845225941, "compression_ratio": 1.5650224215246638, "no_speech_prob": 0.4339280128479004}, {"id": 638, "seek": 307866, "start": 3084.14, "end": 3086.7, "text": " are not doing anything magic.", "tokens": [50638, 366, 406, 884, 1340, 5585, 13, 50766], "temperature": 0.0, "avg_logprob": -0.2907830845225941, "compression_ratio": 1.5650224215246638, "no_speech_prob": 0.4339280128479004}, {"id": 639, "seek": 307866, "start": 3086.7, "end": 3089.14, "text": " They're just doing that one line of code for us.", "tokens": [50766, 814, 434, 445, 884, 300, 472, 1622, 295, 3089, 337, 505, 13, 50888], "temperature": 0.0, "avg_logprob": -0.2907830845225941, "compression_ratio": 1.5650224215246638, "no_speech_prob": 0.4339280128479004}, {"id": 640, "seek": 307866, "start": 3089.14, "end": 3091.8999999999996, "text": " And so I run it again using this new version.", "tokens": [50888, 400, 370, 286, 1190, 309, 797, 1228, 341, 777, 3037, 13, 51026], "temperature": 0.0, "avg_logprob": -0.2907830845225941, "compression_ratio": 1.5650224215246638, "no_speech_prob": 0.4339280128479004}, {"id": 641, "seek": 307866, "start": 3091.8999999999996, "end": 3092.8999999999996, "text": " Oopsie-daisy.", "tokens": [51026, 21726, 414, 12, 67, 1527, 88, 13, 51076], "temperature": 0.0, "avg_logprob": -0.2907830845225941, "compression_ratio": 1.5650224215246638, "no_speech_prob": 0.4339280128479004}, {"id": 642, "seek": 307866, "start": 3092.8999999999996, "end": 3098.74, "text": " Oh, I forgot to run this line of code.", "tokens": [51076, 876, 11, 286, 5298, 281, 1190, 341, 1622, 295, 3089, 13, 51368], "temperature": 0.0, "avg_logprob": -0.2907830845225941, "compression_ratio": 1.5650224215246638, "no_speech_prob": 0.4339280128479004}, {"id": 643, "seek": 307866, "start": 3098.74, "end": 3103.2999999999997, "text": " There we go.", "tokens": [51368, 821, 321, 352, 13, 51596], "temperature": 0.0, "avg_logprob": -0.2907830845225941, "compression_ratio": 1.5650224215246638, "no_speech_prob": 0.4339280128479004}, {"id": 644, "seek": 307866, "start": 3103.2999999999997, "end": 3106.14, "text": " And I guess I should also add the nice little plot method.", "tokens": [51596, 400, 286, 2041, 286, 820, 611, 909, 264, 1481, 707, 7542, 3170, 13, 51738], "temperature": 0.0, "avg_logprob": -0.2907830845225941, "compression_ratio": 1.5650224215246638, "no_speech_prob": 0.4339280128479004}, {"id": 645, "seek": 310614, "start": 3106.14, "end": 3109.8199999999997, "text": " So we'll just move it to the bottom.", "tokens": [50364, 407, 321, 603, 445, 1286, 309, 281, 264, 2767, 13, 50548], "temperature": 0.0, "avg_logprob": -0.5279201160777699, "compression_ratio": 1.316546762589928, "no_speech_prob": 0.16025567054748535}, {"id": 646, "seek": 310614, "start": 3109.8199999999997, "end": 3112.8199999999997, "text": " There.", "tokens": [50548, 821, 13, 50698], "temperature": 0.0, "avg_logprob": -0.5279201160777699, "compression_ratio": 1.316546762589928, "no_speech_prob": 0.16025567054748535}, {"id": 647, "seek": 310614, "start": 3112.8199999999997, "end": 3117.58, "text": " LR find dot plot.", "tokens": [50698, 441, 49, 915, 5893, 7542, 13, 50936], "temperature": 0.0, "avg_logprob": -0.5279201160777699, "compression_ratio": 1.316546762589928, "no_speech_prob": 0.16025567054748535}, {"id": 648, "seek": 310614, "start": 3117.58, "end": 3120.3799999999997, "text": " There we go.", "tokens": [50936, 821, 321, 352, 13, 51076], "temperature": 0.0, "avg_logprob": -0.5279201160777699, "compression_ratio": 1.316546762589928, "no_speech_prob": 0.16025567054748535}, {"id": 649, "seek": 310614, "start": 3120.3799999999997, "end": 3126.7799999999997, "text": " Put that one back to how it was.", "tokens": [51076, 4935, 300, 472, 646, 281, 577, 309, 390, 13, 51396], "temperature": 0.0, "avg_logprob": -0.5279201160777699, "compression_ratio": 1.316546762589928, "no_speech_prob": 0.16025567054748535}, {"id": 650, "seek": 310614, "start": 3126.7799999999997, "end": 3129.5, "text": " All right.", "tokens": [51396, 1057, 558, 13, 51532], "temperature": 0.0, "avg_logprob": -0.5279201160777699, "compression_ratio": 1.316546762589928, "no_speech_prob": 0.16025567054748535}, {"id": 651, "seek": 310614, "start": 3129.5, "end": 3130.5, "text": " Perfect timing.", "tokens": [51532, 10246, 10822, 13, 51582], "temperature": 0.0, "avg_logprob": -0.5279201160777699, "compression_ratio": 1.316546762589928, "no_speech_prob": 0.16025567054748535}, {"id": 652, "seek": 310614, "start": 3130.5, "end": 3133.8799999999997, "text": " So we added a few very important things in here.", "tokens": [51582, 407, 321, 3869, 257, 1326, 588, 1021, 721, 294, 510, 13, 51751], "temperature": 0.0, "avg_logprob": -0.5279201160777699, "compression_ratio": 1.316546762589928, "no_speech_prob": 0.16025567054748535}, {"id": 653, "seek": 313388, "start": 3133.88, "end": 3134.88, "text": " So make sure we export.", "tokens": [50364, 407, 652, 988, 321, 10725, 13, 50414], "temperature": 0.0, "avg_logprob": -0.3806852575850813, "compression_ratio": 1.4928571428571429, "no_speech_prob": 0.03904638811945915}, {"id": 654, "seek": 313388, "start": 3134.88, "end": 3139.6, "text": " And we'll be able to use them shortly.", "tokens": [50414, 400, 321, 603, 312, 1075, 281, 764, 552, 13392, 13, 50650], "temperature": 0.0, "avg_logprob": -0.3806852575850813, "compression_ratio": 1.4928571428571429, "no_speech_prob": 0.03904638811945915}, {"id": 655, "seek": 313388, "start": 3139.6, "end": 3140.6, "text": " All right.", "tokens": [50650, 1057, 558, 13, 50700], "temperature": 0.0, "avg_logprob": -0.3806852575850813, "compression_ratio": 1.4928571428571429, "no_speech_prob": 0.03904638811945915}, {"id": 656, "seek": 313388, "start": 3140.6, "end": 3142.1600000000003, "text": " Let's have an eight-minute break.", "tokens": [50700, 961, 311, 362, 364, 3180, 12, 18256, 1821, 13, 50778], "temperature": 0.0, "avg_logprob": -0.3806852575850813, "compression_ratio": 1.4928571428571429, "no_speech_prob": 0.03904638811945915}, {"id": 657, "seek": 313388, "start": 3142.1600000000003, "end": 3145.4, "text": " Let's just have a ten-minute break.", "tokens": [50778, 961, 311, 445, 362, 257, 2064, 12, 18256, 1821, 13, 50940], "temperature": 0.0, "avg_logprob": -0.3806852575850813, "compression_ratio": 1.4928571428571429, "no_speech_prob": 0.03904638811945915}, {"id": 658, "seek": 313388, "start": 3145.4, "end": 3153.44, "text": " So I'll see you back here at eight past.", "tokens": [50940, 407, 286, 603, 536, 291, 646, 510, 412, 3180, 1791, 13, 51342], "temperature": 0.0, "avg_logprob": -0.3806852575850813, "compression_ratio": 1.4928571428571429, "no_speech_prob": 0.03904638811945915}, {"id": 659, "seek": 313388, "start": 3153.44, "end": 3154.44, "text": " All right.", "tokens": [51342, 1057, 558, 13, 51392], "temperature": 0.0, "avg_logprob": -0.3806852575850813, "compression_ratio": 1.4928571428571429, "no_speech_prob": 0.03904638811945915}, {"id": 660, "seek": 313388, "start": 3154.44, "end": 3155.44, "text": " Welcome back.", "tokens": [51392, 4027, 646, 13, 51442], "temperature": 0.0, "avg_logprob": -0.3806852575850813, "compression_ratio": 1.4928571428571429, "no_speech_prob": 0.03904638811945915}, {"id": 661, "seek": 315544, "start": 3155.44, "end": 3165.4, "text": " So the first thing which I really like is we could rename plot to after fit.", "tokens": [50364, 407, 264, 700, 551, 597, 286, 534, 411, 307, 321, 727, 36741, 7542, 281, 934, 3318, 13, 50862], "temperature": 0.0, "avg_logprob": -0.4400663260953972, "compression_ratio": 1.5056818181818181, "no_speech_prob": 0.2750776708126068}, {"id": 662, "seek": 315544, "start": 3165.4, "end": 3169.08, "text": " Which I really like because that means we should be able to then just call learn dot", "tokens": [50862, 3013, 286, 534, 411, 570, 300, 1355, 321, 820, 312, 1075, 281, 550, 445, 818, 1466, 5893, 51046], "temperature": 0.0, "avg_logprob": -0.4400663260953972, "compression_ratio": 1.5056818181818181, "no_speech_prob": 0.2750776708126068}, {"id": 663, "seek": 315544, "start": 3169.08, "end": 3171.48, "text": " fit and delete the next one.", "tokens": [51046, 3318, 293, 12097, 264, 958, 472, 13, 51166], "temperature": 0.0, "avg_logprob": -0.4400663260953972, "compression_ratio": 1.5056818181818181, "no_speech_prob": 0.2750776708126068}, {"id": 664, "seek": 315544, "start": 3171.48, "end": 3173.48, "text": " And let's see.", "tokens": [51166, 400, 718, 311, 536, 13, 51266], "temperature": 0.0, "avg_logprob": -0.4400663260953972, "compression_ratio": 1.5056818181818181, "no_speech_prob": 0.2750776708126068}, {"id": 665, "seek": 315544, "start": 3173.48, "end": 3175.12, "text": " It didn't work.", "tokens": [51266, 467, 994, 380, 589, 13, 51348], "temperature": 0.0, "avg_logprob": -0.4400663260953972, "compression_ratio": 1.5056818181818181, "no_speech_prob": 0.2750776708126068}, {"id": 666, "seek": 315544, "start": 3175.12, "end": 3176.12, "text": " Why not?", "tokens": [51348, 1545, 406, 30, 51398], "temperature": 0.0, "avg_logprob": -0.4400663260953972, "compression_ratio": 1.5056818181818181, "no_speech_prob": 0.2750776708126068}, {"id": 667, "seek": 315544, "start": 3176.12, "end": 3177.12, "text": " Oh no.", "tokens": [51398, 876, 572, 13, 51448], "temperature": 0.0, "avg_logprob": -0.4400663260953972, "compression_ratio": 1.5056818181818181, "no_speech_prob": 0.2750776708126068}, {"id": 668, "seek": 315544, "start": 3177.12, "end": 3179.52, "text": " That doesn't work, does it?", "tokens": [51448, 663, 1177, 380, 589, 11, 775, 309, 30, 51568], "temperature": 0.0, "avg_logprob": -0.4400663260953972, "compression_ratio": 1.5056818181818181, "no_speech_prob": 0.2750776708126068}, {"id": 669, "seek": 317952, "start": 3179.52, "end": 3186.68, "text": " Because the... hmm.", "tokens": [50364, 1436, 264, 485, 16478, 13, 50722], "temperature": 0.0, "avg_logprob": -0.29601240158081055, "compression_ratio": 1.435374149659864, "no_speech_prob": 0.04672422260046005}, {"id": 670, "seek": 317952, "start": 3186.68, "end": 3187.68, "text": " You know what?", "tokens": [50722, 509, 458, 437, 30, 50772], "temperature": 0.0, "avg_logprob": -0.29601240158081055, "compression_ratio": 1.435374149659864, "no_speech_prob": 0.04672422260046005}, {"id": 671, "seek": 317952, "start": 3187.68, "end": 3199.48, "text": " I think the callback here could go into a finally block, actually.", "tokens": [50772, 286, 519, 264, 818, 3207, 510, 727, 352, 666, 257, 2721, 3461, 11, 767, 13, 51362], "temperature": 0.0, "avg_logprob": -0.29601240158081055, "compression_ratio": 1.435374149659864, "no_speech_prob": 0.04672422260046005}, {"id": 672, "seek": 317952, "start": 3199.48, "end": 3206.2, "text": " That would actually allow us to always call the callback, even if we've cancelled.", "tokens": [51362, 663, 576, 767, 2089, 505, 281, 1009, 818, 264, 818, 3207, 11, 754, 498, 321, 600, 25103, 13, 51698], "temperature": 0.0, "avg_logprob": -0.29601240158081055, "compression_ratio": 1.435374149659864, "no_speech_prob": 0.04672422260046005}, {"id": 673, "seek": 317952, "start": 3206.2, "end": 3209.08, "text": " I think that's reasonable.", "tokens": [51698, 286, 519, 300, 311, 10585, 13, 51842], "temperature": 0.0, "avg_logprob": -0.29601240158081055, "compression_ratio": 1.435374149659864, "no_speech_prob": 0.04672422260046005}, {"id": 674, "seek": 320908, "start": 3209.64, "end": 3210.64, "text": " But it may have its own confusions.", "tokens": [50392, 583, 309, 815, 362, 1080, 1065, 1497, 27255, 13, 50442], "temperature": 0.0, "avg_logprob": -0.41259215218680245, "compression_ratio": 1.391304347826087, "no_speech_prob": 0.0747649073600769}, {"id": 675, "seek": 320908, "start": 3210.64, "end": 3213.72, "text": " Anyway, we could try it for now because that would let us put this after fit in.", "tokens": [50442, 5684, 11, 321, 727, 853, 309, 337, 586, 570, 300, 576, 718, 505, 829, 341, 934, 3318, 294, 13, 50596], "temperature": 0.0, "avg_logprob": -0.41259215218680245, "compression_ratio": 1.391304347826087, "no_speech_prob": 0.0747649073600769}, {"id": 676, "seek": 320908, "start": 3213.72, "end": 3219.56, "text": " There we go.", "tokens": [50596, 821, 321, 352, 13, 50888], "temperature": 0.0, "avg_logprob": -0.41259215218680245, "compression_ratio": 1.391304347826087, "no_speech_prob": 0.0747649073600769}, {"id": 677, "seek": 320908, "start": 3219.56, "end": 3228.52, "text": " So that automatically runs that.", "tokens": [50888, 407, 300, 6772, 6676, 300, 13, 51336], "temperature": 0.0, "avg_logprob": -0.41259215218680245, "compression_ratio": 1.391304347826087, "no_speech_prob": 0.0747649073600769}, {"id": 678, "seek": 320908, "start": 3228.52, "end": 3230.52, "text": " So that's an interesting idea.", "tokens": [51336, 407, 300, 311, 364, 1880, 1558, 13, 51436], "temperature": 0.0, "avg_logprob": -0.41259215218680245, "compression_ratio": 1.391304347826087, "no_speech_prob": 0.0747649073600769}, {"id": 679, "seek": 320908, "start": 3230.52, "end": 3234.3199999999997, "text": " I think I quite like it.", "tokens": [51436, 286, 519, 286, 1596, 411, 309, 13, 51626], "temperature": 0.0, "avg_logprob": -0.41259215218680245, "compression_ratio": 1.391304347826087, "no_speech_prob": 0.0747649073600769}, {"id": 680, "seek": 320908, "start": 3234.3199999999997, "end": 3236.3199999999997, "text": " Cool.", "tokens": [51626, 8561, 13, 51726], "temperature": 0.0, "avg_logprob": -0.41259215218680245, "compression_ratio": 1.391304347826087, "no_speech_prob": 0.0747649073600769}, {"id": 681, "seek": 323632, "start": 3236.84, "end": 3243.84, "text": " So let's now look at notebook 10.", "tokens": [50390, 407, 718, 311, 586, 574, 412, 21060, 1266, 13, 50740], "temperature": 0.0, "avg_logprob": -0.23465847969055176, "compression_ratio": 1.4355828220858895, "no_speech_prob": 0.0006361826090142131}, {"id": 682, "seek": 323632, "start": 3243.84, "end": 3248.04, "text": " So I feel like this is the next big piece we need.", "tokens": [50740, 407, 286, 841, 411, 341, 307, 264, 958, 955, 2522, 321, 643, 13, 50950], "temperature": 0.0, "avg_logprob": -0.23465847969055176, "compression_ratio": 1.4355828220858895, "no_speech_prob": 0.0006361826090142131}, {"id": 683, "seek": 323632, "start": 3248.04, "end": 3253.36, "text": " So we've got a pretty good system now for training models.", "tokens": [50950, 407, 321, 600, 658, 257, 1238, 665, 1185, 586, 337, 3097, 5245, 13, 51216], "temperature": 0.0, "avg_logprob": -0.23465847969055176, "compression_ratio": 1.4355828220858895, "no_speech_prob": 0.0006361826090142131}, {"id": 684, "seek": 323632, "start": 3253.36, "end": 3262.1200000000003, "text": " What I think we're really missing though is a way to identify how our models are training.", "tokens": [51216, 708, 286, 519, 321, 434, 534, 5361, 1673, 307, 257, 636, 281, 5876, 577, 527, 5245, 366, 3097, 13, 51654], "temperature": 0.0, "avg_logprob": -0.23465847969055176, "compression_ratio": 1.4355828220858895, "no_speech_prob": 0.0006361826090142131}, {"id": 685, "seek": 326212, "start": 3262.12, "end": 3266.4, "text": " And so to identify how our models are training, we need to be able to look inside them and", "tokens": [50364, 400, 370, 281, 5876, 577, 527, 5245, 366, 3097, 11, 321, 643, 281, 312, 1075, 281, 574, 1854, 552, 293, 50578], "temperature": 0.0, "avg_logprob": -0.2105856730525655, "compression_ratio": 1.906810035842294, "no_speech_prob": 0.03209877386689186}, {"id": 686, "seek": 326212, "start": 3266.4, "end": 3268.7999999999997, "text": " see what's going on while they train.", "tokens": [50578, 536, 437, 311, 516, 322, 1339, 436, 3847, 13, 50698], "temperature": 0.0, "avg_logprob": -0.2105856730525655, "compression_ratio": 1.906810035842294, "no_speech_prob": 0.03209877386689186}, {"id": 687, "seek": 326212, "start": 3268.7999999999997, "end": 3270.7599999999998, "text": " We don't currently have any way to do that.", "tokens": [50698, 492, 500, 380, 4362, 362, 604, 636, 281, 360, 300, 13, 50796], "temperature": 0.0, "avg_logprob": -0.2105856730525655, "compression_ratio": 1.906810035842294, "no_speech_prob": 0.03209877386689186}, {"id": 688, "seek": 326212, "start": 3270.7599999999998, "end": 3276.2799999999997, "text": " And therefore it's very hard for us to diagnose and fix problems.", "tokens": [50796, 400, 4412, 309, 311, 588, 1152, 337, 505, 281, 36238, 293, 3191, 2740, 13, 51072], "temperature": 0.0, "avg_logprob": -0.2105856730525655, "compression_ratio": 1.906810035842294, "no_speech_prob": 0.03209877386689186}, {"id": 689, "seek": 326212, "start": 3276.2799999999997, "end": 3278.96, "text": " Most people have no way of looking inside their models.", "tokens": [51072, 4534, 561, 362, 572, 636, 295, 1237, 1854, 641, 5245, 13, 51206], "temperature": 0.0, "avg_logprob": -0.2105856730525655, "compression_ratio": 1.906810035842294, "no_speech_prob": 0.03209877386689186}, {"id": 690, "seek": 326212, "start": 3278.96, "end": 3282.48, "text": " And so most people have no way to properly diagnose and fix models.", "tokens": [51206, 400, 370, 881, 561, 362, 572, 636, 281, 6108, 36238, 293, 3191, 5245, 13, 51382], "temperature": 0.0, "avg_logprob": -0.2105856730525655, "compression_ratio": 1.906810035842294, "no_speech_prob": 0.03209877386689186}, {"id": 691, "seek": 326212, "start": 3282.48, "end": 3285.68, "text": " And that's why most people, when they have a problem with training their model, randomly", "tokens": [51382, 400, 300, 311, 983, 881, 561, 11, 562, 436, 362, 257, 1154, 365, 3097, 641, 2316, 11, 16979, 51542], "temperature": 0.0, "avg_logprob": -0.2105856730525655, "compression_ratio": 1.906810035842294, "no_speech_prob": 0.03209877386689186}, {"id": 692, "seek": 326212, "start": 3285.68, "end": 3289.08, "text": " try things until something starts hopefully working.", "tokens": [51542, 853, 721, 1826, 746, 3719, 4696, 1364, 13, 51712], "temperature": 0.0, "avg_logprob": -0.2105856730525655, "compression_ratio": 1.906810035842294, "no_speech_prob": 0.03209877386689186}, {"id": 693, "seek": 326212, "start": 3289.08, "end": 3290.7599999999998, "text": " We're not going to do that.", "tokens": [51712, 492, 434, 406, 516, 281, 360, 300, 13, 51796], "temperature": 0.0, "avg_logprob": -0.2105856730525655, "compression_ratio": 1.906810035842294, "no_speech_prob": 0.03209877386689186}, {"id": 694, "seek": 329076, "start": 3290.8, "end": 3293.2400000000002, "text": " We're going to do it properly.", "tokens": [50366, 492, 434, 516, 281, 360, 309, 6108, 13, 50488], "temperature": 0.0, "avg_logprob": -0.26536755902426584, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.03258807957172394}, {"id": 695, "seek": 329076, "start": 3293.2400000000002, "end": 3298.32, "text": " So we can import the stuff that we just created in the learner.", "tokens": [50488, 407, 321, 393, 974, 264, 1507, 300, 321, 445, 2942, 294, 264, 33347, 13, 50742], "temperature": 0.0, "avg_logprob": -0.26536755902426584, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.03258807957172394}, {"id": 696, "seek": 329076, "start": 3298.32, "end": 3303.48, "text": " And the first thing I'm going to do, introduce now, is a set seed function.", "tokens": [50742, 400, 264, 700, 551, 286, 478, 516, 281, 360, 11, 5366, 586, 11, 307, 257, 992, 8871, 2445, 13, 51000], "temperature": 0.0, "avg_logprob": -0.26536755902426584, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.03258807957172394}, {"id": 697, "seek": 329076, "start": 3303.48, "end": 3306.28, "text": " We've been using torch.manualseed before.", "tokens": [51000, 492, 600, 668, 1228, 27822, 13, 1601, 901, 405, 292, 949, 13, 51140], "temperature": 0.0, "avg_logprob": -0.26536755902426584, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.03258807957172394}, {"id": 698, "seek": 329076, "start": 3306.28, "end": 3310.84, "text": " We know all about RNGs, random number generators.", "tokens": [51140, 492, 458, 439, 466, 497, 30237, 82, 11, 4974, 1230, 38662, 13, 51368], "temperature": 0.0, "avg_logprob": -0.26536755902426584, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.03258807957172394}, {"id": 699, "seek": 329076, "start": 3310.84, "end": 3313.2000000000003, "text": " We've actually got three of them.", "tokens": [51368, 492, 600, 767, 658, 1045, 295, 552, 13, 51486], "temperature": 0.0, "avg_logprob": -0.26536755902426584, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.03258807957172394}, {"id": 700, "seek": 329076, "start": 3313.2000000000003, "end": 3315.88, "text": " PyTorches, NumPys, and Pythons.", "tokens": [51486, 9953, 51, 284, 3781, 11, 22592, 47, 749, 11, 293, 9953, 392, 892, 13, 51620], "temperature": 0.0, "avg_logprob": -0.26536755902426584, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.03258807957172394}, {"id": 701, "seek": 329076, "start": 3315.88, "end": 3318.0400000000004, "text": " Let's seed all of them.", "tokens": [51620, 961, 311, 8871, 439, 295, 552, 13, 51728], "temperature": 0.0, "avg_logprob": -0.26536755902426584, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.03258807957172394}, {"id": 702, "seek": 331804, "start": 3318.04, "end": 3324.68, "text": " And also in Python PyTorch, you can use a flag to ask it to use deterministic algorithms", "tokens": [50364, 400, 611, 294, 15329, 9953, 51, 284, 339, 11, 291, 393, 764, 257, 7166, 281, 1029, 309, 281, 764, 15957, 3142, 14642, 50696], "temperature": 0.0, "avg_logprob": -0.25442873084026835, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.002934926189482212}, {"id": 703, "seek": 331804, "start": 3324.68, "end": 3326.7599999999998, "text": " so things should be reproducible.", "tokens": [50696, 370, 721, 820, 312, 11408, 32128, 13, 50800], "temperature": 0.0, "avg_logprob": -0.25442873084026835, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.002934926189482212}, {"id": 704, "seek": 331804, "start": 3326.7599999999998, "end": 3330.6, "text": " As we've discussed before, you shouldn't always just make things reproducible.", "tokens": [50800, 1018, 321, 600, 7152, 949, 11, 291, 4659, 380, 1009, 445, 652, 721, 11408, 32128, 13, 50992], "temperature": 0.0, "avg_logprob": -0.25442873084026835, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.002934926189482212}, {"id": 705, "seek": 331804, "start": 3330.6, "end": 3332.36, "text": " But for lessons I think this is useful.", "tokens": [50992, 583, 337, 8820, 286, 519, 341, 307, 4420, 13, 51080], "temperature": 0.0, "avg_logprob": -0.25442873084026835, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.002934926189482212}, {"id": 706, "seek": 331804, "start": 3332.36, "end": 3336.44, "text": " So here's a function that lets you set a reproducible seed.", "tokens": [51080, 407, 510, 311, 257, 2445, 300, 6653, 291, 992, 257, 11408, 32128, 8871, 13, 51284], "temperature": 0.0, "avg_logprob": -0.25442873084026835, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.002934926189482212}, {"id": 707, "seek": 331804, "start": 3336.44, "end": 3340.44, "text": " All right, let's use the same data set as before, a fashion MNIST data set.", "tokens": [51284, 1057, 558, 11, 718, 311, 764, 264, 912, 1412, 992, 382, 949, 11, 257, 6700, 376, 45, 19756, 1412, 992, 13, 51484], "temperature": 0.0, "avg_logprob": -0.25442873084026835, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.002934926189482212}, {"id": 708, "seek": 331804, "start": 3340.44, "end": 3343.24, "text": " We'll load it up in the same way.", "tokens": [51484, 492, 603, 3677, 309, 493, 294, 264, 912, 636, 13, 51624], "temperature": 0.0, "avg_logprob": -0.25442873084026835, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.002934926189482212}, {"id": 709, "seek": 334324, "start": 3343.3999999999996, "end": 3348.64, "text": " Let's create a model that looks very similar to our previous models.", "tokens": [50372, 961, 311, 1884, 257, 2316, 300, 1542, 588, 2531, 281, 527, 3894, 5245, 13, 50634], "temperature": 0.0, "avg_logprob": -0.3472165822982788, "compression_ratio": 1.398989898989899, "no_speech_prob": 0.08151049166917801}, {"id": 710, "seek": 334324, "start": 3348.64, "end": 3350.2, "text": " This one might be a bit bigger, mightn't it?", "tokens": [50634, 639, 472, 1062, 312, 257, 857, 3801, 11, 1062, 77, 380, 309, 30, 50712], "temperature": 0.0, "avg_logprob": -0.3472165822982788, "compression_ratio": 1.398989898989899, "no_speech_prob": 0.08151049166917801}, {"id": 711, "seek": 334324, "start": 3350.2, "end": 3352.2799999999997, "text": " I didn't actually check.", "tokens": [50712, 286, 994, 380, 767, 1520, 13, 50816], "temperature": 0.0, "avg_logprob": -0.3472165822982788, "compression_ratio": 1.398989898989899, "no_speech_prob": 0.08151049166917801}, {"id": 712, "seek": 334324, "start": 3352.2799999999997, "end": 3360.04, "text": " Okay, so let's use multi-class accuracy again.", "tokens": [50816, 1033, 11, 370, 718, 311, 764, 4825, 12, 11665, 14170, 797, 13, 51204], "temperature": 0.0, "avg_logprob": -0.3472165822982788, "compression_ratio": 1.398989898989899, "no_speech_prob": 0.08151049166917801}, {"id": 713, "seek": 334324, "start": 3360.04, "end": 3362.4799999999996, "text": " Same callbacks that we used before.", "tokens": [51204, 10635, 818, 17758, 300, 321, 1143, 949, 13, 51326], "temperature": 0.0, "avg_logprob": -0.3472165822982788, "compression_ratio": 1.398989898989899, "no_speech_prob": 0.08151049166917801}, {"id": 714, "seek": 334324, "start": 3362.4799999999996, "end": 3367.08, "text": " We'll use the trainzb version for no particular reason.", "tokens": [51326, 492, 603, 764, 264, 3847, 89, 65, 3037, 337, 572, 1729, 1778, 13, 51556], "temperature": 0.0, "avg_logprob": -0.3472165822982788, "compression_ratio": 1.398989898989899, "no_speech_prob": 0.08151049166917801}, {"id": 715, "seek": 336708, "start": 3367.08, "end": 3372.56, "text": " And generally speaking, we want to train as fast as possible.", "tokens": [50364, 400, 5101, 4124, 11, 321, 528, 281, 3847, 382, 2370, 382, 1944, 13, 50638], "temperature": 0.0, "avg_logprob": -0.33735360039605033, "compression_ratio": 1.532544378698225, "no_speech_prob": 0.012241020798683167}, {"id": 716, "seek": 336708, "start": 3372.56, "end": 3377.36, "text": " Not just because we don't like wasting time, but actually more importantly because the", "tokens": [50638, 1726, 445, 570, 321, 500, 380, 411, 20457, 565, 11, 457, 767, 544, 8906, 570, 264, 50878], "temperature": 0.0, "avg_logprob": -0.33735360039605033, "compression_ratio": 1.532544378698225, "no_speech_prob": 0.012241020798683167}, {"id": 717, "seek": 336708, "start": 3377.36, "end": 3387.16, "text": " higher the learning rate you train at, the more you're able to find a often more generalizable", "tokens": [50878, 2946, 264, 2539, 3314, 291, 3847, 412, 11, 264, 544, 291, 434, 1075, 281, 915, 257, 2049, 544, 2674, 22395, 51368], "temperature": 0.0, "avg_logprob": -0.33735360039605033, "compression_ratio": 1.532544378698225, "no_speech_prob": 0.012241020798683167}, {"id": 718, "seek": 336708, "start": 3387.16, "end": 3389.52, "text": " set of weights.", "tokens": [51368, 992, 295, 17443, 13, 51486], "temperature": 0.0, "avg_logprob": -0.33735360039605033, "compression_ratio": 1.532544378698225, "no_speech_prob": 0.012241020798683167}, {"id": 719, "seek": 338952, "start": 3389.52, "end": 3400.52, "text": " And also, oh, training quickly also means that we can look at each batch, each item", "tokens": [50364, 400, 611, 11, 1954, 11, 3097, 2661, 611, 1355, 300, 321, 393, 574, 412, 1184, 15245, 11, 1184, 3174, 50914], "temperature": 0.0, "avg_logprob": -0.24767728783618445, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.016656741499900818}, {"id": 720, "seek": 338952, "start": 3400.52, "end": 3401.98, "text": " in the data less often.", "tokens": [50914, 294, 264, 1412, 1570, 2049, 13, 50987], "temperature": 0.0, "avg_logprob": -0.24767728783618445, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.016656741499900818}, {"id": 721, "seek": 338952, "start": 3401.98, "end": 3404.6, "text": " So we're going to have less issues with overfitting.", "tokens": [50987, 407, 321, 434, 516, 281, 362, 1570, 2663, 365, 670, 69, 2414, 13, 51118], "temperature": 0.0, "avg_logprob": -0.24767728783618445, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.016656741499900818}, {"id": 722, "seek": 338952, "start": 3404.6, "end": 3409.16, "text": " And generally speaking, if we can train at a high learning rate, then that means that", "tokens": [51118, 400, 5101, 4124, 11, 498, 321, 393, 3847, 412, 257, 1090, 2539, 3314, 11, 550, 300, 1355, 300, 51346], "temperature": 0.0, "avg_logprob": -0.24767728783618445, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.016656741499900818}, {"id": 723, "seek": 338952, "start": 3409.16, "end": 3411.12, "text": " we're learning to train in a stable way.", "tokens": [51346, 321, 434, 2539, 281, 3847, 294, 257, 8351, 636, 13, 51444], "temperature": 0.0, "avg_logprob": -0.24767728783618445, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.016656741499900818}, {"id": 724, "seek": 338952, "start": 3411.12, "end": 3416.52, "text": " And stable training is very good.", "tokens": [51444, 400, 8351, 3097, 307, 588, 665, 13, 51714], "temperature": 0.0, "avg_logprob": -0.24767728783618445, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.016656741499900818}, {"id": 725, "seek": 341652, "start": 3416.52, "end": 3422.36, "text": " So let's try setting up a high learning rate of 0.6 and see what happens.", "tokens": [50364, 407, 718, 311, 853, 3287, 493, 257, 1090, 2539, 3314, 295, 1958, 13, 21, 293, 536, 437, 2314, 13, 50656], "temperature": 0.0, "avg_logprob": -0.21064983565231848, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.004399372264742851}, {"id": 726, "seek": 341652, "start": 3422.36, "end": 3427.28, "text": " So here's a function that's just going to create our learner with our callbacks, and", "tokens": [50656, 407, 510, 311, 257, 2445, 300, 311, 445, 516, 281, 1884, 527, 33347, 365, 527, 818, 17758, 11, 293, 50902], "temperature": 0.0, "avg_logprob": -0.21064983565231848, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.004399372264742851}, {"id": 727, "seek": 341652, "start": 3427.28, "end": 3432.44, "text": " fit it, and return the learner in case we want to use it.", "tokens": [50902, 3318, 309, 11, 293, 2736, 264, 33347, 294, 1389, 321, 528, 281, 764, 309, 13, 51160], "temperature": 0.0, "avg_logprob": -0.21064983565231848, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.004399372264742851}, {"id": 728, "seek": 341652, "start": 3432.44, "end": 3433.44, "text": " And it's training.", "tokens": [51160, 400, 309, 311, 3097, 13, 51210], "temperature": 0.0, "avg_logprob": -0.21064983565231848, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.004399372264742851}, {"id": 729, "seek": 341652, "start": 3433.44, "end": 3435.62, "text": " Oh, and then it suddenly fell apart.", "tokens": [51210, 876, 11, 293, 550, 309, 5800, 5696, 4936, 13, 51319], "temperature": 0.0, "avg_logprob": -0.21064983565231848, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.004399372264742851}, {"id": 730, "seek": 341652, "start": 3435.62, "end": 3440.2599999999998, "text": " So it's going well for a while, and then it stopped training nicely.", "tokens": [51319, 407, 309, 311, 516, 731, 337, 257, 1339, 11, 293, 550, 309, 5936, 3097, 9594, 13, 51551], "temperature": 0.0, "avg_logprob": -0.21064983565231848, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.004399372264742851}, {"id": 731, "seek": 341652, "start": 3440.2599999999998, "end": 3443.9, "text": " So one nice thing about this graph is that we can immediately see when it stops training", "tokens": [51551, 407, 472, 1481, 551, 466, 341, 4295, 307, 300, 321, 393, 4258, 536, 562, 309, 10094, 3097, 51733], "temperature": 0.0, "avg_logprob": -0.21064983565231848, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.004399372264742851}, {"id": 732, "seek": 344390, "start": 3443.9, "end": 3449.7000000000003, "text": " well, which is very useful.", "tokens": [50364, 731, 11, 597, 307, 588, 4420, 13, 50654], "temperature": 0.0, "avg_logprob": -0.25342907508214313, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.006488212384283543}, {"id": 733, "seek": 344390, "start": 3449.7000000000003, "end": 3450.94, "text": " So what happened there?", "tokens": [50654, 407, 437, 2011, 456, 30, 50716], "temperature": 0.0, "avg_logprob": -0.25342907508214313, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.006488212384283543}, {"id": 734, "seek": 344390, "start": 3450.94, "end": 3451.94, "text": " Why did it go badly?", "tokens": [50716, 1545, 630, 309, 352, 13425, 30, 50766], "temperature": 0.0, "avg_logprob": -0.25342907508214313, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.006488212384283543}, {"id": 735, "seek": 344390, "start": 3451.94, "end": 3454.7000000000003, "text": " I mean we can guess that it might have been because of our high learning rate, but what's", "tokens": [50766, 286, 914, 321, 393, 2041, 300, 309, 1062, 362, 668, 570, 295, 527, 1090, 2539, 3314, 11, 457, 437, 311, 50904], "temperature": 0.0, "avg_logprob": -0.25342907508214313, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.006488212384283543}, {"id": 736, "seek": 344390, "start": 3454.7000000000003, "end": 3457.12, "text": " really going on?", "tokens": [50904, 534, 516, 322, 30, 51025], "temperature": 0.0, "avg_logprob": -0.25342907508214313, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.006488212384283543}, {"id": 737, "seek": 344390, "start": 3457.12, "end": 3459.08, "text": " So let's try to look inside it.", "tokens": [51025, 407, 718, 311, 853, 281, 574, 1854, 309, 13, 51123], "temperature": 0.0, "avg_logprob": -0.25342907508214313, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.006488212384283543}, {"id": 738, "seek": 344390, "start": 3459.08, "end": 3466.7400000000002, "text": " So one way to look inside it would be we could create our own sequential model, just like", "tokens": [51123, 407, 472, 636, 281, 574, 1854, 309, 576, 312, 321, 727, 1884, 527, 1065, 42881, 2316, 11, 445, 411, 51506], "temperature": 0.0, "avg_logprob": -0.25342907508214313, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.006488212384283543}, {"id": 739, "seek": 344390, "start": 3466.7400000000002, "end": 3469.1, "text": " the sequential model we've built before.", "tokens": [51506, 264, 42881, 2316, 321, 600, 3094, 949, 13, 51624], "temperature": 0.0, "avg_logprob": -0.25342907508214313, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.006488212384283543}, {"id": 740, "seek": 346910, "start": 3469.1, "end": 3473.5, "text": " Do you remember we created one using nn.moduleList in a previous lesson?", "tokens": [50364, 1144, 291, 1604, 321, 2942, 472, 1228, 297, 77, 13, 8014, 2271, 43, 468, 294, 257, 3894, 6898, 30, 50584], "temperature": 0.0, "avg_logprob": -0.2618171645373833, "compression_ratio": 1.5549738219895288, "no_speech_prob": 0.04467765986919403}, {"id": 741, "seek": 346910, "start": 3473.5, "end": 3478.22, "text": " If you've forgotten, go back and check that out.", "tokens": [50584, 759, 291, 600, 11832, 11, 352, 646, 293, 1520, 300, 484, 13, 50820], "temperature": 0.0, "avg_logprob": -0.2618171645373833, "compression_ratio": 1.5549738219895288, "no_speech_prob": 0.04467765986919403}, {"id": 742, "seek": 346910, "start": 3478.22, "end": 3485.24, "text": " And when we call that model, we go through each layer and just call the layer.", "tokens": [50820, 400, 562, 321, 818, 300, 2316, 11, 321, 352, 807, 1184, 4583, 293, 445, 818, 264, 4583, 13, 51171], "temperature": 0.0, "avg_logprob": -0.2618171645373833, "compression_ratio": 1.5549738219895288, "no_speech_prob": 0.04467765986919403}, {"id": 743, "seek": 346910, "start": 3485.24, "end": 3490.2999999999997, "text": " And what we could do is, so we could add something in addition, which is at each layer, we could", "tokens": [51171, 400, 437, 321, 727, 360, 307, 11, 370, 321, 727, 909, 746, 294, 4500, 11, 597, 307, 412, 1184, 4583, 11, 321, 727, 51424], "temperature": 0.0, "avg_logprob": -0.2618171645373833, "compression_ratio": 1.5549738219895288, "no_speech_prob": 0.04467765986919403}, {"id": 744, "seek": 349030, "start": 3490.3, "end": 3500.46, "text": " also get the mean of that layer, and the standard deviation of that layer, and append", "tokens": [50364, 611, 483, 264, 914, 295, 300, 4583, 11, 293, 264, 3832, 25163, 295, 300, 4583, 11, 293, 34116, 50872], "temperature": 0.0, "avg_logprob": -0.24065298504299587, "compression_ratio": 1.9675324675324675, "no_speech_prob": 0.04208548367023468}, {"id": 745, "seek": 349030, "start": 3500.46, "end": 3509.1400000000003, "text": " them to a couple of different lists, and activation means and activation standard deviations.", "tokens": [50872, 552, 281, 257, 1916, 295, 819, 14511, 11, 293, 24433, 1355, 293, 24433, 3832, 31219, 763, 13, 51306], "temperature": 0.0, "avg_logprob": -0.24065298504299587, "compression_ratio": 1.9675324675324675, "no_speech_prob": 0.04208548367023468}, {"id": 746, "seek": 349030, "start": 3509.1400000000003, "end": 3514.98, "text": " This is going to contain, after we call this model, it's going to contain the means and", "tokens": [51306, 639, 307, 516, 281, 5304, 11, 934, 321, 818, 341, 2316, 11, 309, 311, 516, 281, 5304, 264, 1355, 293, 51598], "temperature": 0.0, "avg_logprob": -0.24065298504299587, "compression_ratio": 1.9675324675324675, "no_speech_prob": 0.04208548367023468}, {"id": 747, "seek": 349030, "start": 3514.98, "end": 3520.1800000000003, "text": " standard deviations for each layer.", "tokens": [51598, 3832, 31219, 763, 337, 1184, 4583, 13, 51858], "temperature": 0.0, "avg_logprob": -0.24065298504299587, "compression_ratio": 1.9675324675324675, "no_speech_prob": 0.04208548367023468}, {"id": 748, "seek": 352018, "start": 3520.2599999999998, "end": 3525.06, "text": " And then we could define dunder iter, which makes this into an iterator, as being let's", "tokens": [50368, 400, 550, 321, 727, 6964, 274, 6617, 17138, 11, 597, 1669, 341, 666, 364, 17138, 1639, 11, 382, 885, 718, 311, 50608], "temperature": 0.0, "avg_logprob": -0.229428816211316, "compression_ratio": 1.7594501718213058, "no_speech_prob": 0.001622962998226285}, {"id": 749, "seek": 352018, "start": 3525.06, "end": 3531.8199999999997, "text": " say just, oh just, when you iterate through this model, you can iterate through the layers.", "tokens": [50608, 584, 445, 11, 1954, 445, 11, 562, 291, 44497, 807, 341, 2316, 11, 291, 393, 44497, 807, 264, 7914, 13, 50946], "temperature": 0.0, "avg_logprob": -0.229428816211316, "compression_ratio": 1.7594501718213058, "no_speech_prob": 0.001622962998226285}, {"id": 750, "seek": 352018, "start": 3531.8199999999997, "end": 3535.98, "text": " So we can then train this model in the usual way, and this is going to give us exactly", "tokens": [50946, 407, 321, 393, 550, 3847, 341, 2316, 294, 264, 7713, 636, 11, 293, 341, 307, 516, 281, 976, 505, 2293, 51154], "temperature": 0.0, "avg_logprob": -0.229428816211316, "compression_ratio": 1.7594501718213058, "no_speech_prob": 0.001622962998226285}, {"id": 751, "seek": 352018, "start": 3535.98, "end": 3542.02, "text": " the same outcome as before, because I'm using the same seed, so you can see it looks identical.", "tokens": [51154, 264, 912, 9700, 382, 949, 11, 570, 286, 478, 1228, 264, 912, 8871, 11, 370, 291, 393, 536, 309, 1542, 14800, 13, 51456], "temperature": 0.0, "avg_logprob": -0.229428816211316, "compression_ratio": 1.7594501718213058, "no_speech_prob": 0.001622962998226285}, {"id": 752, "seek": 352018, "start": 3542.02, "end": 3546.7, "text": " But the difference is instead of using nn.sequential, we've now used something that's actually saved", "tokens": [51456, 583, 264, 2649, 307, 2602, 295, 1228, 297, 77, 13, 11834, 2549, 11, 321, 600, 586, 1143, 746, 300, 311, 767, 6624, 51690], "temperature": 0.0, "avg_logprob": -0.229428816211316, "compression_ratio": 1.7594501718213058, "no_speech_prob": 0.001622962998226285}, {"id": 753, "seek": 352018, "start": 3546.7, "end": 3549.8999999999996, "text": " the means and standard deviations of each layer.", "tokens": [51690, 264, 1355, 293, 3832, 31219, 763, 295, 1184, 4583, 13, 51850], "temperature": 0.0, "avg_logprob": -0.229428816211316, "compression_ratio": 1.7594501718213058, "no_speech_prob": 0.001622962998226285}, {"id": 754, "seek": 354990, "start": 3549.9, "end": 3553.3, "text": " And so therefore we can plot them.", "tokens": [50364, 400, 370, 4412, 321, 393, 7542, 552, 13, 50534], "temperature": 0.0, "avg_logprob": -0.26362405909170017, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0009547294466756284}, {"id": 755, "seek": 354990, "start": 3553.3, "end": 3564.98, "text": " Okay, so here we've plotted the activation means, and notice that we've done it for every", "tokens": [50534, 1033, 11, 370, 510, 321, 600, 43288, 264, 24433, 1355, 11, 293, 3449, 300, 321, 600, 1096, 309, 337, 633, 51118], "temperature": 0.0, "avg_logprob": -0.26362405909170017, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0009547294466756284}, {"id": 756, "seek": 354990, "start": 3564.98, "end": 3567.38, "text": " batch.", "tokens": [51118, 15245, 13, 51238], "temperature": 0.0, "avg_logprob": -0.26362405909170017, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0009547294466756284}, {"id": 757, "seek": 354990, "start": 3567.38, "end": 3572.5, "text": " So that's why along the x-axis here we have batch number, and on the y-axis we have the", "tokens": [51238, 407, 300, 311, 983, 2051, 264, 2031, 12, 24633, 510, 321, 362, 15245, 1230, 11, 293, 322, 264, 288, 12, 24633, 321, 362, 264, 51494], "temperature": 0.0, "avg_logprob": -0.26362405909170017, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0009547294466756284}, {"id": 758, "seek": 354990, "start": 3572.5, "end": 3575.9, "text": " activation means, and then we have it for each layer.", "tokens": [51494, 24433, 1355, 11, 293, 550, 321, 362, 309, 337, 1184, 4583, 13, 51664], "temperature": 0.0, "avg_logprob": -0.26362405909170017, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0009547294466756284}, {"id": 759, "seek": 354990, "start": 3575.9, "end": 3579.34, "text": " So rather than starting at 1, because in Python we're starting at 0, so this is the first", "tokens": [51664, 407, 2831, 813, 2891, 412, 502, 11, 570, 294, 15329, 321, 434, 2891, 412, 1958, 11, 370, 341, 307, 264, 700, 51836], "temperature": 0.0, "avg_logprob": -0.26362405909170017, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0009547294466756284}, {"id": 760, "seek": 357934, "start": 3579.34, "end": 3587.7000000000003, "text": " layer is blue, second layer is orange, third layer green, fourth layer red, and fifth", "tokens": [50364, 4583, 307, 3344, 11, 1150, 4583, 307, 7671, 11, 2636, 4583, 3092, 11, 6409, 4583, 2182, 11, 293, 9266, 50782], "temperature": 0.0, "avg_logprob": -0.32947772382253626, "compression_ratio": 1.7461928934010151, "no_speech_prob": 0.016152722761034966}, {"id": 761, "seek": 357934, "start": 3587.7000000000003, "end": 3592.02, "text": " layer, watch for that like movie kind of color.", "tokens": [50782, 4583, 11, 1159, 337, 300, 411, 705, 12702, 733, 295, 2017, 13, 50998], "temperature": 0.0, "avg_logprob": -0.32947772382253626, "compression_ratio": 1.7461928934010151, "no_speech_prob": 0.016152722761034966}, {"id": 762, "seek": 357934, "start": 3592.02, "end": 3593.6600000000003, "text": " And look what's happened.", "tokens": [50998, 400, 574, 437, 311, 2011, 13, 51080], "temperature": 0.0, "avg_logprob": -0.32947772382253626, "compression_ratio": 1.7461928934010151, "no_speech_prob": 0.016152722761034966}, {"id": 763, "seek": 357934, "start": 3593.6600000000003, "end": 3601.1400000000003, "text": " The activations have started pretty small, close to zero, and have increased at an exponentially", "tokens": [51080, 440, 2430, 763, 362, 1409, 1238, 1359, 11, 1998, 281, 4018, 11, 293, 362, 6505, 412, 364, 37330, 51454], "temperature": 0.0, "avg_logprob": -0.32947772382253626, "compression_ratio": 1.7461928934010151, "no_speech_prob": 0.016152722761034966}, {"id": 764, "seek": 357934, "start": 3601.1400000000003, "end": 3606.94, "text": " increasing rate, and then have crashed, and then have increased again at an exponential", "tokens": [51454, 5662, 3314, 11, 293, 550, 362, 24190, 11, 293, 550, 362, 6505, 797, 412, 364, 21510, 51744], "temperature": 0.0, "avg_logprob": -0.32947772382253626, "compression_ratio": 1.7461928934010151, "no_speech_prob": 0.016152722761034966}, {"id": 765, "seek": 360694, "start": 3606.94, "end": 3612.02, "text": " rate, and crashed again, it increased again, crashed again, and each time they've gone", "tokens": [50364, 3314, 11, 293, 24190, 797, 11, 309, 6505, 797, 11, 24190, 797, 11, 293, 1184, 565, 436, 600, 2780, 50618], "temperature": 0.0, "avg_logprob": -0.2762287592483779, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.039638519287109375}, {"id": 766, "seek": 360694, "start": 3612.02, "end": 3617.82, "text": " up they've gone up even higher, and they've crashed, in this case even lower.", "tokens": [50618, 493, 436, 600, 2780, 493, 754, 2946, 11, 293, 436, 600, 24190, 11, 294, 341, 1389, 754, 3126, 13, 50908], "temperature": 0.0, "avg_logprob": -0.2762287592483779, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.039638519287109375}, {"id": 767, "seek": 360694, "start": 3617.82, "end": 3618.82, "text": " And what happens?", "tokens": [50908, 400, 437, 2314, 30, 50958], "temperature": 0.0, "avg_logprob": -0.2762287592483779, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.039638519287109375}, {"id": 768, "seek": 360694, "start": 3618.82, "end": 3623.1, "text": " Well what's happening here when our activations are really close to zero?", "tokens": [50958, 1042, 437, 311, 2737, 510, 562, 527, 2430, 763, 366, 534, 1998, 281, 4018, 30, 51172], "temperature": 0.0, "avg_logprob": -0.2762287592483779, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.039638519287109375}, {"id": 769, "seek": 360694, "start": 3623.1, "end": 3627.7000000000003, "text": " Well when your activations are really close to zero, that means that the inputs to each", "tokens": [51172, 1042, 562, 428, 2430, 763, 366, 534, 1998, 281, 4018, 11, 300, 1355, 300, 264, 15743, 281, 1184, 51402], "temperature": 0.0, "avg_logprob": -0.2762287592483779, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.039638519287109375}, {"id": 770, "seek": 360694, "start": 3627.7000000000003, "end": 3630.96, "text": " layer are numbers very close to zero.", "tokens": [51402, 4583, 366, 3547, 588, 1998, 281, 4018, 13, 51565], "temperature": 0.0, "avg_logprob": -0.2762287592483779, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.039638519287109375}, {"id": 771, "seek": 360694, "start": 3630.96, "end": 3635.54, "text": " As a result of which of course the outputs are very close to zero, because we're doing", "tokens": [51565, 1018, 257, 1874, 295, 597, 295, 1164, 264, 23930, 366, 588, 1998, 281, 4018, 11, 570, 321, 434, 884, 51794], "temperature": 0.0, "avg_logprob": -0.2762287592483779, "compression_ratio": 1.9872881355932204, "no_speech_prob": 0.039638519287109375}, {"id": 772, "seek": 363554, "start": 3635.54, "end": 3637.5, "text": " just matrix multiplies.", "tokens": [50364, 445, 8141, 12788, 530, 13, 50462], "temperature": 0.0, "avg_logprob": -0.2784952254522414, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.02716868184506893}, {"id": 773, "seek": 363554, "start": 3637.5, "end": 3640.06, "text": " And so this is a disaster.", "tokens": [50462, 400, 370, 341, 307, 257, 11293, 13, 50590], "temperature": 0.0, "avg_logprob": -0.2784952254522414, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.02716868184506893}, {"id": 774, "seek": 363554, "start": 3640.06, "end": 3646.86, "text": " When activations are very close to zero, they're dead units.", "tokens": [50590, 1133, 2430, 763, 366, 588, 1998, 281, 4018, 11, 436, 434, 3116, 6815, 13, 50930], "temperature": 0.0, "avg_logprob": -0.2784952254522414, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.02716868184506893}, {"id": 775, "seek": 363554, "start": 3646.86, "end": 3650.62, "text": " They're not able to do anything, and you can see for ages here it's not training at all.", "tokens": [50930, 814, 434, 406, 1075, 281, 360, 1340, 11, 293, 291, 393, 536, 337, 12357, 510, 309, 311, 406, 3097, 412, 439, 13, 51118], "temperature": 0.0, "avg_logprob": -0.2784952254522414, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.02716868184506893}, {"id": 776, "seek": 363554, "start": 3650.62, "end": 3653.94, "text": " And this is, so this is the activation means.", "tokens": [51118, 400, 341, 307, 11, 370, 341, 307, 264, 24433, 1355, 13, 51284], "temperature": 0.0, "avg_logprob": -0.2784952254522414, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.02716868184506893}, {"id": 777, "seek": 363554, "start": 3653.94, "end": 3656.82, "text": " The standard deviations tell an even stronger story.", "tokens": [51284, 440, 3832, 31219, 763, 980, 364, 754, 7249, 1657, 13, 51428], "temperature": 0.0, "avg_logprob": -0.2784952254522414, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.02716868184506893}, {"id": 778, "seek": 363554, "start": 3656.82, "end": 3664.86, "text": " So you want, generally speaking, you want the means of the activations to be about zero,", "tokens": [51428, 407, 291, 528, 11, 5101, 4124, 11, 291, 528, 264, 1355, 295, 264, 2430, 763, 281, 312, 466, 4018, 11, 51830], "temperature": 0.0, "avg_logprob": -0.2784952254522414, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.02716868184506893}, {"id": 779, "seek": 366486, "start": 3664.86, "end": 3668.2200000000003, "text": " and the standard deviations to be about one.", "tokens": [50364, 293, 264, 3832, 31219, 763, 281, 312, 466, 472, 13, 50532], "temperature": 0.0, "avg_logprob": -0.26773455715918726, "compression_ratio": 2.1223628691983123, "no_speech_prob": 0.00044421758502721786}, {"id": 780, "seek": 366486, "start": 3668.2200000000003, "end": 3673.5, "text": " Mean of zero is fine as long as they're spread around zero, but a standard deviation of close", "tokens": [50532, 12302, 295, 4018, 307, 2489, 382, 938, 382, 436, 434, 3974, 926, 4018, 11, 457, 257, 3832, 25163, 295, 1998, 50796], "temperature": 0.0, "avg_logprob": -0.26773455715918726, "compression_ratio": 2.1223628691983123, "no_speech_prob": 0.00044421758502721786}, {"id": 781, "seek": 366486, "start": 3673.5, "end": 3678.2000000000003, "text": " to zero is terrible, because that means all of the activations are about the same.", "tokens": [50796, 281, 4018, 307, 6237, 11, 570, 300, 1355, 439, 295, 264, 2430, 763, 366, 466, 264, 912, 13, 51031], "temperature": 0.0, "avg_logprob": -0.26773455715918726, "compression_ratio": 2.1223628691983123, "no_speech_prob": 0.00044421758502721786}, {"id": 782, "seek": 366486, "start": 3678.2000000000003, "end": 3685.54, "text": " So here, after batch 30, all of the activations are close to zero, and all of their standard", "tokens": [51031, 407, 510, 11, 934, 15245, 2217, 11, 439, 295, 264, 2430, 763, 366, 1998, 281, 4018, 11, 293, 439, 295, 641, 3832, 51398], "temperature": 0.0, "avg_logprob": -0.26773455715918726, "compression_ratio": 2.1223628691983123, "no_speech_prob": 0.00044421758502721786}, {"id": 783, "seek": 366486, "start": 3685.54, "end": 3689.6200000000003, "text": " deviations are close to zero, so all the numbers are about the same, and they're about zero.", "tokens": [51398, 31219, 763, 366, 1998, 281, 4018, 11, 370, 439, 264, 3547, 366, 466, 264, 912, 11, 293, 436, 434, 466, 4018, 13, 51602], "temperature": 0.0, "avg_logprob": -0.26773455715918726, "compression_ratio": 2.1223628691983123, "no_speech_prob": 0.00044421758502721786}, {"id": 784, "seek": 366486, "start": 3689.6200000000003, "end": 3691.02, "text": " So nothing's going on.", "tokens": [51602, 407, 1825, 311, 516, 322, 13, 51672], "temperature": 0.0, "avg_logprob": -0.26773455715918726, "compression_ratio": 2.1223628691983123, "no_speech_prob": 0.00044421758502721786}, {"id": 785, "seek": 366486, "start": 3691.02, "end": 3694.84, "text": " And you can see the same thing's happening with the standard deviations.", "tokens": [51672, 400, 291, 393, 536, 264, 912, 551, 311, 2737, 365, 264, 3832, 31219, 763, 13, 51863], "temperature": 0.0, "avg_logprob": -0.26773455715918726, "compression_ratio": 2.1223628691983123, "no_speech_prob": 0.00044421758502721786}, {"id": 786, "seek": 369484, "start": 3694.84, "end": 3698.1600000000003, "text": " We start with not very much variety in the weights.", "tokens": [50364, 492, 722, 365, 406, 588, 709, 5673, 294, 264, 17443, 13, 50530], "temperature": 0.0, "avg_logprob": -0.24877775283086867, "compression_ratio": 1.7081545064377683, "no_speech_prob": 0.0003101557958871126}, {"id": 787, "seek": 369484, "start": 3698.1600000000003, "end": 3702.44, "text": " It exponentially increases how much variety there is, and then it crashes again.", "tokens": [50530, 467, 37330, 8637, 577, 709, 5673, 456, 307, 11, 293, 550, 309, 28642, 797, 13, 50744], "temperature": 0.0, "avg_logprob": -0.24877775283086867, "compression_ratio": 1.7081545064377683, "no_speech_prob": 0.0003101557958871126}, {"id": 788, "seek": 369484, "start": 3702.44, "end": 3704.32, "text": " Exponentially increases, crashes again.", "tokens": [50744, 21391, 266, 3137, 8637, 11, 28642, 797, 13, 50838], "temperature": 0.0, "avg_logprob": -0.24877775283086867, "compression_ratio": 1.7081545064377683, "no_speech_prob": 0.0003101557958871126}, {"id": 789, "seek": 369484, "start": 3704.32, "end": 3714.46, "text": " This is a classic shape of bad behavior, and with these two plots, you can really understand", "tokens": [50838, 639, 307, 257, 7230, 3909, 295, 1578, 5223, 11, 293, 365, 613, 732, 28609, 11, 291, 393, 534, 1223, 51345], "temperature": 0.0, "avg_logprob": -0.24877775283086867, "compression_ratio": 1.7081545064377683, "no_speech_prob": 0.0003101557958871126}, {"id": 790, "seek": 369484, "start": 3714.46, "end": 3716.3, "text": " what's going on in your model.", "tokens": [51345, 437, 311, 516, 322, 294, 428, 2316, 13, 51437], "temperature": 0.0, "avg_logprob": -0.24877775283086867, "compression_ratio": 1.7081545064377683, "no_speech_prob": 0.0003101557958871126}, {"id": 791, "seek": 369484, "start": 3716.3, "end": 3720.8, "text": " And if you train a model, and at the end of it, you kind of think, well I wonder if this", "tokens": [51437, 400, 498, 291, 3847, 257, 2316, 11, 293, 412, 264, 917, 295, 309, 11, 291, 733, 295, 519, 11, 731, 286, 2441, 498, 341, 51662], "temperature": 0.0, "avg_logprob": -0.24877775283086867, "compression_ratio": 1.7081545064377683, "no_speech_prob": 0.0003101557958871126}, {"id": 792, "seek": 369484, "start": 3720.8, "end": 3722.28, "text": " is any good.", "tokens": [51662, 307, 604, 665, 13, 51736], "temperature": 0.0, "avg_logprob": -0.24877775283086867, "compression_ratio": 1.7081545064377683, "no_speech_prob": 0.0003101557958871126}, {"id": 793, "seek": 372228, "start": 3722.28, "end": 3726.0, "text": " If you haven't looked at this plot, you don't know, because you haven't checked to see whether", "tokens": [50364, 759, 291, 2378, 380, 2956, 412, 341, 7542, 11, 291, 500, 380, 458, 11, 570, 291, 2378, 380, 10033, 281, 536, 1968, 50550], "temperature": 0.0, "avg_logprob": -0.20175917395229997, "compression_ratio": 1.7751937984496124, "no_speech_prob": 0.024797437712550163}, {"id": 794, "seek": 372228, "start": 3726.0, "end": 3727.9, "text": " it's training nicely.", "tokens": [50550, 309, 311, 3097, 9594, 13, 50645], "temperature": 0.0, "avg_logprob": -0.20175917395229997, "compression_ratio": 1.7751937984496124, "no_speech_prob": 0.024797437712550163}, {"id": 795, "seek": 372228, "start": 3727.9, "end": 3729.7200000000003, "text": " Maybe it could be a lot better.", "tokens": [50645, 2704, 309, 727, 312, 257, 688, 1101, 13, 50736], "temperature": 0.0, "avg_logprob": -0.20175917395229997, "compression_ratio": 1.7751937984496124, "no_speech_prob": 0.024797437712550163}, {"id": 796, "seek": 372228, "start": 3729.7200000000003, "end": 3735.28, "text": " If you can get something, we'll see some nicer training pictures later, but generally speaking,", "tokens": [50736, 759, 291, 393, 483, 746, 11, 321, 603, 536, 512, 22842, 3097, 5242, 1780, 11, 457, 5101, 4124, 11, 51014], "temperature": 0.0, "avg_logprob": -0.20175917395229997, "compression_ratio": 1.7751937984496124, "no_speech_prob": 0.024797437712550163}, {"id": 797, "seek": 372228, "start": 3735.28, "end": 3741.4, "text": " you want something where your mean is always about zero, and your variance is always about", "tokens": [51014, 291, 528, 746, 689, 428, 914, 307, 1009, 466, 4018, 11, 293, 428, 21977, 307, 1009, 466, 51320], "temperature": 0.0, "avg_logprob": -0.20175917395229997, "compression_ratio": 1.7751937984496124, "no_speech_prob": 0.024797437712550163}, {"id": 798, "seek": 372228, "start": 3741.4, "end": 3742.4, "text": " one.", "tokens": [51320, 472, 13, 51370], "temperature": 0.0, "avg_logprob": -0.20175917395229997, "compression_ratio": 1.7751937984496124, "no_speech_prob": 0.024797437712550163}, {"id": 799, "seek": 372228, "start": 3742.4, "end": 3745.0600000000004, "text": " Standard deviation is always about one.", "tokens": [51370, 21298, 25163, 307, 1009, 466, 472, 13, 51503], "temperature": 0.0, "avg_logprob": -0.20175917395229997, "compression_ratio": 1.7751937984496124, "no_speech_prob": 0.024797437712550163}, {"id": 800, "seek": 372228, "start": 3745.0600000000004, "end": 3748.42, "text": " And if you see that, then it's a pretty good chance you're training properly.", "tokens": [51503, 400, 498, 291, 536, 300, 11, 550, 309, 311, 257, 1238, 665, 2931, 291, 434, 3097, 6108, 13, 51671], "temperature": 0.0, "avg_logprob": -0.20175917395229997, "compression_ratio": 1.7751937984496124, "no_speech_prob": 0.024797437712550163}, {"id": 801, "seek": 374842, "start": 3748.42, "end": 3751.78, "text": " If you don't see that, you're most certainly not training properly.", "tokens": [50364, 759, 291, 500, 380, 536, 300, 11, 291, 434, 881, 3297, 406, 3097, 6108, 13, 50532], "temperature": 0.0, "avg_logprob": -0.21731423523466467, "compression_ratio": 1.6984732824427482, "no_speech_prob": 0.025178024545311928}, {"id": 802, "seek": 374842, "start": 3751.78, "end": 3756.66, "text": " Okay, so what I'm going to do in the rest of this part of the lesson is explain how", "tokens": [50532, 1033, 11, 370, 437, 286, 478, 516, 281, 360, 294, 264, 1472, 295, 341, 644, 295, 264, 6898, 307, 2903, 577, 50776], "temperature": 0.0, "avg_logprob": -0.21731423523466467, "compression_ratio": 1.6984732824427482, "no_speech_prob": 0.025178024545311928}, {"id": 803, "seek": 374842, "start": 3756.66, "end": 3762.5, "text": " to do this in a more elegant way, because as I say, being able to look inside your models", "tokens": [50776, 281, 360, 341, 294, 257, 544, 21117, 636, 11, 570, 382, 286, 584, 11, 885, 1075, 281, 574, 1854, 428, 5245, 51068], "temperature": 0.0, "avg_logprob": -0.21731423523466467, "compression_ratio": 1.6984732824427482, "no_speech_prob": 0.025178024545311928}, {"id": 804, "seek": 374842, "start": 3762.5, "end": 3767.46, "text": " is such a critically important thing to building and debugging models.", "tokens": [51068, 307, 1270, 257, 22797, 1021, 551, 281, 2390, 293, 45592, 5245, 13, 51316], "temperature": 0.0, "avg_logprob": -0.21731423523466467, "compression_ratio": 1.6984732824427482, "no_speech_prob": 0.025178024545311928}, {"id": 805, "seek": 374842, "start": 3767.46, "end": 3768.78, "text": " We don't have to do it manually.", "tokens": [51316, 492, 500, 380, 362, 281, 360, 309, 16945, 13, 51382], "temperature": 0.0, "avg_logprob": -0.21731423523466467, "compression_ratio": 1.6984732824427482, "no_speech_prob": 0.025178024545311928}, {"id": 806, "seek": 374842, "start": 3768.78, "end": 3770.56, "text": " We don't have to create our own sequential model.", "tokens": [51382, 492, 500, 380, 362, 281, 1884, 527, 1065, 42881, 2316, 13, 51471], "temperature": 0.0, "avg_logprob": -0.21731423523466467, "compression_ratio": 1.6984732824427482, "no_speech_prob": 0.025178024545311928}, {"id": 807, "seek": 374842, "start": 3770.56, "end": 3774.58, "text": " We can actually use a PyTorch thing called hooks.", "tokens": [51471, 492, 393, 767, 764, 257, 9953, 51, 284, 339, 551, 1219, 26485, 13, 51672], "temperature": 0.0, "avg_logprob": -0.21731423523466467, "compression_ratio": 1.6984732824427482, "no_speech_prob": 0.025178024545311928}, {"id": 808, "seek": 377458, "start": 3774.58, "end": 3781.06, "text": " So as it says here, a hook is called when a layer that it's registered to is executed", "tokens": [50364, 407, 382, 309, 1619, 510, 11, 257, 6328, 307, 1219, 562, 257, 4583, 300, 309, 311, 13968, 281, 307, 17577, 50688], "temperature": 0.0, "avg_logprob": -0.21433115872469816, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.09400728344917297}, {"id": 809, "seek": 377458, "start": 3781.06, "end": 3782.54, "text": " during the forward pass.", "tokens": [50688, 1830, 264, 2128, 1320, 13, 50762], "temperature": 0.0, "avg_logprob": -0.21433115872469816, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.09400728344917297}, {"id": 810, "seek": 377458, "start": 3782.54, "end": 3786.7799999999997, "text": " That's called a forward hook, or the backward pass, and that's called a backward hook.", "tokens": [50762, 663, 311, 1219, 257, 2128, 6328, 11, 420, 264, 23897, 1320, 11, 293, 300, 311, 1219, 257, 23897, 6328, 13, 50974], "temperature": 0.0, "avg_logprob": -0.21433115872469816, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.09400728344917297}, {"id": 811, "seek": 377458, "start": 3786.7799999999997, "end": 3789.38, "text": " And so the key thing about hooks is we don't have to rewrite the model.", "tokens": [50974, 400, 370, 264, 2141, 551, 466, 26485, 307, 321, 500, 380, 362, 281, 28132, 264, 2316, 13, 51104], "temperature": 0.0, "avg_logprob": -0.21433115872469816, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.09400728344917297}, {"id": 812, "seek": 377458, "start": 3789.38, "end": 3792.8199999999997, "text": " We can add them to any existing model.", "tokens": [51104, 492, 393, 909, 552, 281, 604, 6741, 2316, 13, 51276], "temperature": 0.0, "avg_logprob": -0.21433115872469816, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.09400728344917297}, {"id": 813, "seek": 377458, "start": 3792.8199999999997, "end": 3799.66, "text": " So we can just use standard nn.sequential, passing in our layers, which were these ones", "tokens": [51276, 407, 321, 393, 445, 764, 3832, 297, 77, 13, 11834, 2549, 11, 8437, 294, 527, 7914, 11, 597, 645, 613, 2306, 51618], "temperature": 0.0, "avg_logprob": -0.21433115872469816, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.09400728344917297}, {"id": 814, "seek": 377458, "start": 3799.66, "end": 3803.66, "text": " here.", "tokens": [51618, 510, 13, 51818], "temperature": 0.0, "avg_logprob": -0.21433115872469816, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.09400728344917297}, {"id": 815, "seek": 380366, "start": 3803.74, "end": 3806.7, "text": " And so we're still going to have something to keep track of the activation means and", "tokens": [50368, 400, 370, 321, 434, 920, 516, 281, 362, 746, 281, 1066, 2837, 295, 264, 24433, 1355, 293, 50516], "temperature": 0.0, "avg_logprob": -0.2589703853999343, "compression_ratio": 1.8034934497816595, "no_speech_prob": 0.00025714145158417523}, {"id": 816, "seek": 380366, "start": 3806.7, "end": 3807.7, "text": " standard deviations.", "tokens": [50516, 3832, 31219, 763, 13, 50566], "temperature": 0.0, "avg_logprob": -0.2589703853999343, "compression_ratio": 1.8034934497816595, "no_speech_prob": 0.00025714145158417523}, {"id": 817, "seek": 380366, "start": 3807.7, "end": 3815.5, "text": " So just create an empty list for now, for each layer in the model.", "tokens": [50566, 407, 445, 1884, 364, 6707, 1329, 337, 586, 11, 337, 1184, 4583, 294, 264, 2316, 13, 50956], "temperature": 0.0, "avg_logprob": -0.2589703853999343, "compression_ratio": 1.8034934497816595, "no_speech_prob": 0.00025714145158417523}, {"id": 818, "seek": 380366, "start": 3815.5, "end": 3818.3799999999997, "text": " And let's create a little function that's going to be called, because a hook is going", "tokens": [50956, 400, 718, 311, 1884, 257, 707, 2445, 300, 311, 516, 281, 312, 1219, 11, 570, 257, 6328, 307, 516, 51100], "temperature": 0.0, "avg_logprob": -0.2589703853999343, "compression_ratio": 1.8034934497816595, "no_speech_prob": 0.00025714145158417523}, {"id": 819, "seek": 380366, "start": 3818.3799999999997, "end": 3823.8999999999996, "text": " to call a function when during the forward pass, for a forward hook, or the backward", "tokens": [51100, 281, 818, 257, 2445, 562, 1830, 264, 2128, 1320, 11, 337, 257, 2128, 6328, 11, 420, 264, 23897, 51376], "temperature": 0.0, "avg_logprob": -0.2589703853999343, "compression_ratio": 1.8034934497816595, "no_speech_prob": 0.00025714145158417523}, {"id": 820, "seek": 380366, "start": 3823.8999999999996, "end": 3825.2999999999997, "text": " pass for a backward hook.", "tokens": [51376, 1320, 337, 257, 23897, 6328, 13, 51446], "temperature": 0.0, "avg_logprob": -0.2589703853999343, "compression_ratio": 1.8034934497816595, "no_speech_prob": 0.00025714145158417523}, {"id": 821, "seek": 380366, "start": 3825.2999999999997, "end": 3827.12, "text": " So we've got a function called appendStats.", "tokens": [51446, 407, 321, 600, 658, 257, 2445, 1219, 34116, 4520, 1720, 13, 51537], "temperature": 0.0, "avg_logprob": -0.2589703853999343, "compression_ratio": 1.8034934497816595, "no_speech_prob": 0.00025714145158417523}, {"id": 822, "seek": 382712, "start": 3827.12, "end": 3833.2, "text": " It's going to be passed the hook number, sorry, the layer number, the module, and the", "tokens": [50364, 467, 311, 516, 281, 312, 4678, 264, 6328, 1230, 11, 2597, 11, 264, 4583, 1230, 11, 264, 10088, 11, 293, 264, 50668], "temperature": 0.0, "avg_logprob": -0.22869111751687937, "compression_ratio": 1.8464912280701755, "no_speech_prob": 0.018545644357800484}, {"id": 823, "seek": 382712, "start": 3833.2, "end": 3835.2, "text": " input and the output.", "tokens": [50668, 4846, 293, 264, 5598, 13, 50768], "temperature": 0.0, "avg_logprob": -0.22869111751687937, "compression_ratio": 1.8464912280701755, "no_speech_prob": 0.018545644357800484}, {"id": 824, "seek": 382712, "start": 3835.2, "end": 3841.48, "text": " So we're going to be grabbing the outputs mean and putting in activation means, and", "tokens": [50768, 407, 321, 434, 516, 281, 312, 23771, 264, 23930, 914, 293, 3372, 294, 24433, 1355, 11, 293, 51082], "temperature": 0.0, "avg_logprob": -0.22869111751687937, "compression_ratio": 1.8464912280701755, "no_speech_prob": 0.018545644357800484}, {"id": 825, "seek": 382712, "start": 3841.48, "end": 3846.6, "text": " the output standard deviation, and putting it in activation standard deviations.", "tokens": [51082, 264, 5598, 3832, 25163, 11, 293, 3372, 309, 294, 24433, 3832, 31219, 763, 13, 51338], "temperature": 0.0, "avg_logprob": -0.22869111751687937, "compression_ratio": 1.8464912280701755, "no_speech_prob": 0.018545644357800484}, {"id": 826, "seek": 382712, "start": 3846.6, "end": 3847.6, "text": " So here's how you do it.", "tokens": [51338, 407, 510, 311, 577, 291, 360, 309, 13, 51388], "temperature": 0.0, "avg_logprob": -0.22869111751687937, "compression_ratio": 1.8464912280701755, "no_speech_prob": 0.018545644357800484}, {"id": 827, "seek": 382712, "start": 3847.6, "end": 3849.24, "text": " We've got a model.", "tokens": [51388, 492, 600, 658, 257, 2316, 13, 51470], "temperature": 0.0, "avg_logprob": -0.22869111751687937, "compression_ratio": 1.8464912280701755, "no_speech_prob": 0.018545644357800484}, {"id": 828, "seek": 382712, "start": 3849.24, "end": 3853.96, "text": " You go through each layer of the model, and you call on a register forward hook.", "tokens": [51470, 509, 352, 807, 1184, 4583, 295, 264, 2316, 11, 293, 291, 818, 322, 257, 7280, 2128, 6328, 13, 51706], "temperature": 0.0, "avg_logprob": -0.22869111751687937, "compression_ratio": 1.8464912280701755, "no_speech_prob": 0.018545644357800484}, {"id": 829, "seek": 382712, "start": 3853.96, "end": 3856.08, "text": " That's part of PyTorch.", "tokens": [51706, 663, 311, 644, 295, 9953, 51, 284, 339, 13, 51812], "temperature": 0.0, "avg_logprob": -0.22869111751687937, "compression_ratio": 1.8464912280701755, "no_speech_prob": 0.018545644357800484}, {"id": 830, "seek": 385608, "start": 3856.16, "end": 3858.56, "text": " We don't need to write it ourselves, because we already did, right?", "tokens": [50368, 492, 500, 380, 643, 281, 2464, 309, 4175, 11, 570, 321, 1217, 630, 11, 558, 30, 50488], "temperature": 0.0, "avg_logprob": -0.2886666727590037, "compression_ratio": 1.7, "no_speech_prob": 0.001133567071519792}, {"id": 831, "seek": 385608, "start": 3858.56, "end": 3863.08, "text": " It's just doing the same thing as this, basically.", "tokens": [50488, 467, 311, 445, 884, 264, 912, 551, 382, 341, 11, 1936, 13, 50714], "temperature": 0.0, "avg_logprob": -0.2886666727590037, "compression_ratio": 1.7, "no_speech_prob": 0.001133567071519792}, {"id": 832, "seek": 385608, "start": 3863.08, "end": 3867.36, "text": " And what function is always going to be called?", "tokens": [50714, 400, 437, 2445, 307, 1009, 516, 281, 312, 1219, 30, 50928], "temperature": 0.0, "avg_logprob": -0.2886666727590037, "compression_ratio": 1.7, "no_speech_prob": 0.001133567071519792}, {"id": 833, "seek": 385608, "start": 3867.36, "end": 3872.84, "text": " The function that's going to be called is the appendStats function passing in, remember", "tokens": [50928, 440, 2445, 300, 311, 516, 281, 312, 1219, 307, 264, 34116, 4520, 1720, 2445, 8437, 294, 11, 1604, 51202], "temperature": 0.0, "avg_logprob": -0.2886666727590037, "compression_ratio": 1.7, "no_speech_prob": 0.001133567071519792}, {"id": 834, "seek": 385608, "start": 3872.84, "end": 3879.04, "text": " partial is the equivalent of saying appendStats passing in i as the first element, the first", "tokens": [51202, 14641, 307, 264, 10344, 295, 1566, 34116, 4520, 1720, 8437, 294, 741, 382, 264, 700, 4478, 11, 264, 700, 51512], "temperature": 0.0, "avg_logprob": -0.2886666727590037, "compression_ratio": 1.7, "no_speech_prob": 0.001133567071519792}, {"id": 835, "seek": 385608, "start": 3879.04, "end": 3881.12, "text": " argument.", "tokens": [51512, 6770, 13, 51616], "temperature": 0.0, "avg_logprob": -0.2886666727590037, "compression_ratio": 1.7, "no_speech_prob": 0.001133567071519792}, {"id": 836, "seek": 388112, "start": 3881.12, "end": 3888.72, "text": " So if we now fit that model, it trains in the usual way, but after each layer, it's", "tokens": [50364, 407, 498, 321, 586, 3318, 300, 2316, 11, 309, 16329, 294, 264, 7713, 636, 11, 457, 934, 1184, 4583, 11, 309, 311, 50744], "temperature": 0.0, "avg_logprob": -0.23352694511413574, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.010986803099513054}, {"id": 837, "seek": 388112, "start": 3888.72, "end": 3893.0, "text": " going to call this.", "tokens": [50744, 516, 281, 818, 341, 13, 50958], "temperature": 0.0, "avg_logprob": -0.23352694511413574, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.010986803099513054}, {"id": 838, "seek": 388112, "start": 3893.0, "end": 3896.16, "text": " And so you can see we get exactly the same thing as before.", "tokens": [50958, 400, 370, 291, 393, 536, 321, 483, 2293, 264, 912, 551, 382, 949, 13, 51116], "temperature": 0.0, "avg_logprob": -0.23352694511413574, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.010986803099513054}, {"id": 839, "seek": 388112, "start": 3896.16, "end": 3901.2799999999997, "text": " So one question we get here is what's the difference between a hook and a callback?", "tokens": [51116, 407, 472, 1168, 321, 483, 510, 307, 437, 311, 264, 2649, 1296, 257, 6328, 293, 257, 818, 3207, 30, 51372], "temperature": 0.0, "avg_logprob": -0.23352694511413574, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.010986803099513054}, {"id": 840, "seek": 388112, "start": 3901.2799999999997, "end": 3902.68, "text": " Nothing at all.", "tokens": [51372, 6693, 412, 439, 13, 51442], "temperature": 0.0, "avg_logprob": -0.23352694511413574, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.010986803099513054}, {"id": 841, "seek": 388112, "start": 3902.68, "end": 3904.7799999999997, "text": " Hooks and callbacks are the same thing.", "tokens": [51442, 33132, 82, 293, 818, 17758, 366, 264, 912, 551, 13, 51547], "temperature": 0.0, "avg_logprob": -0.23352694511413574, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.010986803099513054}, {"id": 842, "seek": 390478, "start": 3904.78, "end": 3913.6600000000003, "text": " It's just that PyTorch defines hooks, and they call them hooks instead of callbacks.", "tokens": [50364, 467, 311, 445, 300, 9953, 51, 284, 339, 23122, 26485, 11, 293, 436, 818, 552, 26485, 2602, 295, 818, 17758, 13, 50808], "temperature": 0.0, "avg_logprob": -0.2442231033787583, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.03358893841505051}, {"id": 843, "seek": 390478, "start": 3913.6600000000003, "end": 3920.42, "text": " They are less flexible than the callbacks that we used in the learner, because you don't", "tokens": [50808, 814, 366, 1570, 11358, 813, 264, 818, 17758, 300, 321, 1143, 294, 264, 33347, 11, 570, 291, 500, 380, 51146], "temperature": 0.0, "avg_logprob": -0.2442231033787583, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.03358893841505051}, {"id": 844, "seek": 390478, "start": 3920.42, "end": 3925.42, "text": " have access to all the available states, you can't change things, but they're a particular", "tokens": [51146, 362, 2105, 281, 439, 264, 2435, 4368, 11, 291, 393, 380, 1319, 721, 11, 457, 436, 434, 257, 1729, 51396], "temperature": 0.0, "avg_logprob": -0.2442231033787583, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.03358893841505051}, {"id": 845, "seek": 390478, "start": 3925.42, "end": 3926.42, "text": " kind of callback.", "tokens": [51396, 733, 295, 818, 3207, 13, 51446], "temperature": 0.0, "avg_logprob": -0.2442231033787583, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.03358893841505051}, {"id": 846, "seek": 390478, "start": 3926.42, "end": 3934.34, "text": " It's just setting a piece of code that's going to be run for us when we, when something happens.", "tokens": [51446, 467, 311, 445, 3287, 257, 2522, 295, 3089, 300, 311, 516, 281, 312, 1190, 337, 505, 562, 321, 11, 562, 746, 2314, 13, 51842], "temperature": 0.0, "avg_logprob": -0.2442231033787583, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.03358893841505051}, {"id": 847, "seek": 393434, "start": 3934.9, "end": 3938.2200000000003, "text": " And in this case, there's something that happens is that either a layer in the forward pass", "tokens": [50392, 400, 294, 341, 1389, 11, 456, 311, 746, 300, 2314, 307, 300, 2139, 257, 4583, 294, 264, 2128, 1320, 50558], "temperature": 0.0, "avg_logprob": -0.26633235836817215, "compression_ratio": 1.8267716535433072, "no_speech_prob": 0.0012843030272051692}, {"id": 848, "seek": 393434, "start": 3938.2200000000003, "end": 3943.46, "text": " is called or a layer in the backward pass is called.", "tokens": [50558, 307, 1219, 420, 257, 4583, 294, 264, 23897, 1320, 307, 1219, 13, 50820], "temperature": 0.0, "avg_logprob": -0.26633235836817215, "compression_ratio": 1.8267716535433072, "no_speech_prob": 0.0012843030272051692}, {"id": 849, "seek": 393434, "start": 3943.46, "end": 3948.7000000000003, "text": " I guess you could describe the function that's being called back as the callback, and the", "tokens": [50820, 286, 2041, 291, 727, 6786, 264, 2445, 300, 311, 885, 1219, 646, 382, 264, 818, 3207, 11, 293, 264, 51082], "temperature": 0.0, "avg_logprob": -0.26633235836817215, "compression_ratio": 1.8267716535433072, "no_speech_prob": 0.0012843030272051692}, {"id": 850, "seek": 393434, "start": 3948.7000000000003, "end": 3952.06, "text": " thing that's doing the callback has the hook.", "tokens": [51082, 551, 300, 311, 884, 264, 818, 3207, 575, 264, 6328, 13, 51250], "temperature": 0.0, "avg_logprob": -0.26633235836817215, "compression_ratio": 1.8267716535433072, "no_speech_prob": 0.0012843030272051692}, {"id": 851, "seek": 393434, "start": 3952.06, "end": 3955.78, "text": " I'm not sure if that level of distinction is important, but maybe that's, you could", "tokens": [51250, 286, 478, 406, 988, 498, 300, 1496, 295, 16844, 307, 1021, 11, 457, 1310, 300, 311, 11, 291, 727, 51436], "temperature": 0.0, "avg_logprob": -0.26633235836817215, "compression_ratio": 1.8267716535433072, "no_speech_prob": 0.0012843030272051692}, {"id": 852, "seek": 393434, "start": 3955.78, "end": 3956.78, "text": " do that.", "tokens": [51436, 360, 300, 13, 51486], "temperature": 0.0, "avg_logprob": -0.26633235836817215, "compression_ratio": 1.8267716535433072, "no_speech_prob": 0.0012843030272051692}, {"id": 853, "seek": 393434, "start": 3956.78, "end": 3962.58, "text": " Okay, so anyway, this is a little bit fussy of kind of like creating globals and appending", "tokens": [51486, 1033, 11, 370, 4033, 11, 341, 307, 257, 707, 857, 283, 26394, 295, 733, 295, 411, 4084, 16125, 1124, 293, 724, 2029, 51776], "temperature": 0.0, "avg_logprob": -0.26633235836817215, "compression_ratio": 1.8267716535433072, "no_speech_prob": 0.0012843030272051692}, {"id": 854, "seek": 396258, "start": 3962.62, "end": 3964.34, "text": " to them and stuff like that.", "tokens": [50366, 281, 552, 293, 1507, 411, 300, 13, 50452], "temperature": 0.0, "avg_logprob": -0.24169962874082762, "compression_ratio": 1.7294685990338163, "no_speech_prob": 0.004198779817670584}, {"id": 855, "seek": 396258, "start": 3964.34, "end": 3967.18, "text": " So let's try to simplify this a little bit.", "tokens": [50452, 407, 718, 311, 853, 281, 20460, 341, 257, 707, 857, 13, 50594], "temperature": 0.0, "avg_logprob": -0.24169962874082762, "compression_ratio": 1.7294685990338163, "no_speech_prob": 0.004198779817670584}, {"id": 856, "seek": 396258, "start": 3967.18, "end": 3974.5, "text": " So what I did here was I created a class called hook.", "tokens": [50594, 407, 437, 286, 630, 510, 390, 286, 2942, 257, 1508, 1219, 6328, 13, 50960], "temperature": 0.0, "avg_logprob": -0.24169962874082762, "compression_ratio": 1.7294685990338163, "no_speech_prob": 0.004198779817670584}, {"id": 857, "seek": 396258, "start": 3974.5, "end": 3981.2999999999997, "text": " So this class, when we create it, we're going to pass in the module that we're hooking.", "tokens": [50960, 407, 341, 1508, 11, 562, 321, 1884, 309, 11, 321, 434, 516, 281, 1320, 294, 264, 10088, 300, 321, 434, 1106, 5953, 13, 51300], "temperature": 0.0, "avg_logprob": -0.24169962874082762, "compression_ratio": 1.7294685990338163, "no_speech_prob": 0.004198779817670584}, {"id": 858, "seek": 396258, "start": 3981.2999999999997, "end": 3985.46, "text": " So we call m.registerForwardHook, and we call the function.", "tokens": [51300, 407, 321, 818, 275, 13, 3375, 1964, 12587, 1007, 39, 1212, 11, 293, 321, 818, 264, 2445, 13, 51508], "temperature": 0.0, "avg_logprob": -0.24169962874082762, "compression_ratio": 1.7294685990338163, "no_speech_prob": 0.004198779817670584}, {"id": 859, "seek": 396258, "start": 3985.46, "end": 3990.62, "text": " We pass the function that we want to be given, and so here's, we pass the function.", "tokens": [51508, 492, 1320, 264, 2445, 300, 321, 528, 281, 312, 2212, 11, 293, 370, 510, 311, 11, 321, 1320, 264, 2445, 13, 51766], "temperature": 0.0, "avg_logprob": -0.24169962874082762, "compression_ratio": 1.7294685990338163, "no_speech_prob": 0.004198779817670584}, {"id": 860, "seek": 399062, "start": 3990.66, "end": 3999.58, "text": " And we're also going to pass in the hook class to the function.", "tokens": [50366, 400, 321, 434, 611, 516, 281, 1320, 294, 264, 6328, 1508, 281, 264, 2445, 13, 50812], "temperature": 0.0, "avg_logprob": -0.23758240307078643, "compression_ratio": 1.5146198830409356, "no_speech_prob": 0.0004583122208714485}, {"id": 861, "seek": 399062, "start": 3999.58, "end": 4006.7799999999997, "text": " Let's also define a remove, because this is actually the thing that removes the hook.", "tokens": [50812, 961, 311, 611, 6964, 257, 4159, 11, 570, 341, 307, 767, 264, 551, 300, 30445, 264, 6328, 13, 51172], "temperature": 0.0, "avg_logprob": -0.23758240307078643, "compression_ratio": 1.5146198830409356, "no_speech_prob": 0.0004583122208714485}, {"id": 862, "seek": 399062, "start": 4006.7799999999997, "end": 4010.3399999999997, "text": " We don't want it sitting around forever.", "tokens": [51172, 492, 500, 380, 528, 309, 3798, 926, 5680, 13, 51350], "temperature": 0.0, "avg_logprob": -0.23758240307078643, "compression_ratio": 1.5146198830409356, "no_speech_prob": 0.0004583122208714485}, {"id": 863, "seek": 399062, "start": 4010.3399999999997, "end": 4015.22, "text": " This is called, the del is called by Python when an object is freed.", "tokens": [51350, 639, 307, 1219, 11, 264, 1103, 307, 1219, 538, 15329, 562, 364, 2657, 307, 21796, 13, 51594], "temperature": 0.0, "avg_logprob": -0.23758240307078643, "compression_ratio": 1.5146198830409356, "no_speech_prob": 0.0004583122208714485}, {"id": 864, "seek": 401522, "start": 4015.22, "end": 4020.5, "text": " And when that happens, we should also make sure that we remove this.", "tokens": [50364, 400, 562, 300, 2314, 11, 321, 820, 611, 652, 988, 300, 321, 4159, 341, 13, 50628], "temperature": 0.0, "avg_logprob": -0.3043179018744107, "compression_ratio": 1.4482758620689655, "no_speech_prob": 0.12251634150743484}, {"id": 865, "seek": 401522, "start": 4020.5, "end": 4030.8599999999997, "text": " Okay, so appendStats now, we're going to replace, it's going to instead get past the hook instead,", "tokens": [50628, 1033, 11, 370, 34116, 4520, 1720, 586, 11, 321, 434, 516, 281, 7406, 11, 309, 311, 516, 281, 2602, 483, 1791, 264, 6328, 2602, 11, 51146], "temperature": 0.0, "avg_logprob": -0.3043179018744107, "compression_ratio": 1.4482758620689655, "no_speech_prob": 0.12251634150743484}, {"id": 866, "seek": 401522, "start": 4030.8599999999997, "end": 4037.06, "text": " because that's what we asked to be passed.", "tokens": [51146, 570, 300, 311, 437, 321, 2351, 281, 312, 4678, 13, 51456], "temperature": 0.0, "avg_logprob": -0.3043179018744107, "compression_ratio": 1.4482758620689655, "no_speech_prob": 0.12251634150743484}, {"id": 867, "seek": 403706, "start": 4037.06, "end": 4046.54, "text": " And if there's no .stats attribute in there yet, then let's create one.", "tokens": [50364, 400, 498, 456, 311, 572, 2411, 372, 1720, 19667, 294, 456, 1939, 11, 550, 718, 311, 1884, 472, 13, 50838], "temperature": 0.0, "avg_logprob": -0.1965339588669111, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.13845059275627136}, {"id": 868, "seek": 403706, "start": 4046.54, "end": 4052.42, "text": " And then we're going to be past the activation, so put that on the CPU, and append the mean", "tokens": [50838, 400, 550, 321, 434, 516, 281, 312, 1791, 264, 24433, 11, 370, 829, 300, 322, 264, 13199, 11, 293, 34116, 264, 914, 51132], "temperature": 0.0, "avg_logprob": -0.1965339588669111, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.13845059275627136}, {"id": 869, "seek": 403706, "start": 4052.42, "end": 4054.46, "text": " and the standard deviation.", "tokens": [51132, 293, 264, 3832, 25163, 13, 51234], "temperature": 0.0, "avg_logprob": -0.1965339588669111, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.13845059275627136}, {"id": 870, "seek": 403706, "start": 4054.46, "end": 4060.34, "text": " And now the nice thing is that the stats are actually inside this object, which is convenient.", "tokens": [51234, 400, 586, 264, 1481, 551, 307, 300, 264, 18152, 366, 767, 1854, 341, 2657, 11, 597, 307, 10851, 13, 51528], "temperature": 0.0, "avg_logprob": -0.1965339588669111, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.13845059275627136}, {"id": 871, "seek": 403706, "start": 4060.34, "end": 4064.74, "text": " So now we can do exactly the same thing as before, but we don't have to set any of that", "tokens": [51528, 407, 586, 321, 393, 360, 2293, 264, 912, 551, 382, 949, 11, 457, 321, 500, 380, 362, 281, 992, 604, 295, 300, 51748], "temperature": 0.0, "avg_logprob": -0.1965339588669111, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.13845059275627136}, {"id": 872, "seek": 403706, "start": 4064.74, "end": 4066.62, "text": " global stuff or whatever.", "tokens": [51748, 4338, 1507, 420, 2035, 13, 51842], "temperature": 0.0, "avg_logprob": -0.1965339588669111, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.13845059275627136}, {"id": 873, "seek": 406662, "start": 4067.18, "end": 4074.06, "text": " We can just say, okay, our hooks is a hook with that layer and that function for all", "tokens": [50392, 492, 393, 445, 584, 11, 1392, 11, 527, 26485, 307, 257, 6328, 365, 300, 4583, 293, 300, 2445, 337, 439, 50736], "temperature": 0.0, "avg_logprob": -0.33146072836483226, "compression_ratio": 1.4654088050314464, "no_speech_prob": 0.0013885162770748138}, {"id": 874, "seek": 406662, "start": 4074.06, "end": 4077.7799999999997, "text": " those models layers.", "tokens": [50736, 729, 5245, 7914, 13, 50922], "temperature": 0.0, "avg_logprob": -0.33146072836483226, "compression_ratio": 1.4654088050314464, "no_speech_prob": 0.0013885162770748138}, {"id": 875, "seek": 406662, "start": 4077.7799999999997, "end": 4083.62, "text": " And so we're just calling it, has called registerForwardHook for us.", "tokens": [50922, 400, 370, 321, 434, 445, 5141, 309, 11, 575, 1219, 7280, 12587, 1007, 39, 1212, 337, 505, 13, 51214], "temperature": 0.0, "avg_logprob": -0.33146072836483226, "compression_ratio": 1.4654088050314464, "no_speech_prob": 0.0013885162770748138}, {"id": 876, "seek": 406662, "start": 4083.62, "end": 4090.1, "text": " So now when we fit that, it's going to run with the hooks.", "tokens": [51214, 407, 586, 562, 321, 3318, 300, 11, 309, 311, 516, 281, 1190, 365, 264, 26485, 13, 51538], "temperature": 0.0, "avg_logprob": -0.33146072836483226, "compression_ratio": 1.4654088050314464, "no_speech_prob": 0.0013885162770748138}, {"id": 877, "seek": 409010, "start": 4090.1, "end": 4098.58, "text": " There we go, it trains.", "tokens": [50364, 821, 321, 352, 11, 309, 16329, 13, 50788], "temperature": 0.0, "avg_logprob": -0.43533504710477944, "compression_ratio": 1.4513274336283186, "no_speech_prob": 0.006388141307979822}, {"id": 878, "seek": 409010, "start": 4098.58, "end": 4108.58, "text": " I actually need to do it too.", "tokens": [50788, 286, 767, 643, 281, 360, 309, 886, 13, 51288], "temperature": 0.0, "avg_logprob": -0.43533504710477944, "compression_ratio": 1.4513274336283186, "no_speech_prob": 0.006388141307979822}, {"id": 879, "seek": 409010, "start": 4108.58, "end": 4113.58, "text": " Okay, so then it trains and we get exactly the same shape as usual, and we get back the", "tokens": [51288, 1033, 11, 370, 550, 309, 16329, 293, 321, 483, 2293, 264, 912, 3909, 382, 7713, 11, 293, 321, 483, 646, 264, 51538], "temperature": 0.0, "avg_logprob": -0.43533504710477944, "compression_ratio": 1.4513274336283186, "no_speech_prob": 0.006388141307979822}, {"id": 880, "seek": 409010, "start": 4113.58, "end": 4114.58, "text": " same results as usual.", "tokens": [51538, 912, 3542, 382, 7713, 13, 51588], "temperature": 0.0, "avg_logprob": -0.43533504710477944, "compression_ratio": 1.4513274336283186, "no_speech_prob": 0.006388141307979822}, {"id": 881, "seek": 411458, "start": 4114.58, "end": 4121.66, "text": " But as we can see, we're gradually making this more convenient, which is nice.", "tokens": [50364, 583, 382, 321, 393, 536, 11, 321, 434, 13145, 1455, 341, 544, 10851, 11, 597, 307, 1481, 13, 50718], "temperature": 0.0, "avg_logprob": -0.21534935971523853, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.04813423007726669}, {"id": 882, "seek": 411458, "start": 4121.66, "end": 4126.7, "text": " So we can make it nicer still, because generally speaking, we're going to be adding multiple", "tokens": [50718, 407, 321, 393, 652, 309, 22842, 920, 11, 570, 5101, 4124, 11, 321, 434, 516, 281, 312, 5127, 3866, 50970], "temperature": 0.0, "avg_logprob": -0.21534935971523853, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.04813423007726669}, {"id": 883, "seek": 411458, "start": 4126.7, "end": 4132.82, "text": " hooks and this stuff of, you know, this list comprehension, whatever, it's a bit inconvenient.", "tokens": [50970, 26485, 293, 341, 1507, 295, 11, 291, 458, 11, 341, 1329, 44991, 11, 2035, 11, 309, 311, 257, 857, 46196, 13, 51276], "temperature": 0.0, "avg_logprob": -0.21534935971523853, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.04813423007726669}, {"id": 884, "seek": 411458, "start": 4132.82, "end": 4135.74, "text": " So let's create a hooks class.", "tokens": [51276, 407, 718, 311, 1884, 257, 26485, 1508, 13, 51422], "temperature": 0.0, "avg_logprob": -0.21534935971523853, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.04813423007726669}, {"id": 885, "seek": 411458, "start": 4135.74, "end": 4138.92, "text": " So first of all, we'll see how the hooks class works in practice.", "tokens": [51422, 407, 700, 295, 439, 11, 321, 603, 536, 577, 264, 26485, 1508, 1985, 294, 3124, 13, 51581], "temperature": 0.0, "avg_logprob": -0.21534935971523853, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.04813423007726669}, {"id": 886, "seek": 413892, "start": 4138.92, "end": 4146.32, "text": " So in the hooks class, the way we're going to use it is we're going to call with hooks,", "tokens": [50364, 407, 294, 264, 26485, 1508, 11, 264, 636, 321, 434, 516, 281, 764, 309, 307, 321, 434, 516, 281, 818, 365, 26485, 11, 50734], "temperature": 0.0, "avg_logprob": -0.24499869346618652, "compression_ratio": 1.9208333333333334, "no_speech_prob": 0.014727985486388206}, {"id": 887, "seek": 413892, "start": 4146.32, "end": 4153.36, "text": " pass in the model, pass in the function to use as our hook, and then we'll fit the model,", "tokens": [50734, 1320, 294, 264, 2316, 11, 1320, 294, 264, 2445, 281, 764, 382, 527, 6328, 11, 293, 550, 321, 603, 3318, 264, 2316, 11, 51086], "temperature": 0.0, "avg_logprob": -0.24499869346618652, "compression_ratio": 1.9208333333333334, "no_speech_prob": 0.014727985486388206}, {"id": 888, "seek": 413892, "start": 4153.36, "end": 4154.36, "text": " and that's it.", "tokens": [51086, 293, 300, 311, 309, 13, 51136], "temperature": 0.0, "avg_logprob": -0.24499869346618652, "compression_ratio": 1.9208333333333334, "no_speech_prob": 0.014727985486388206}, {"id": 889, "seek": 413892, "start": 4154.36, "end": 4157.78, "text": " It's going to be literally just one extra line of code to set up the whole thing.", "tokens": [51136, 467, 311, 516, 281, 312, 3736, 445, 472, 2857, 1622, 295, 3089, 281, 992, 493, 264, 1379, 551, 13, 51307], "temperature": 0.0, "avg_logprob": -0.24499869346618652, "compression_ratio": 1.9208333333333334, "no_speech_prob": 0.014727985486388206}, {"id": 890, "seek": 413892, "start": 4157.78, "end": 4163.88, "text": " And then when we then, we can then go through each hook and plot the mean and standard deviation", "tokens": [51307, 400, 550, 562, 321, 550, 11, 321, 393, 550, 352, 807, 1184, 6328, 293, 7542, 264, 914, 293, 3832, 25163, 51612], "temperature": 0.0, "avg_logprob": -0.24499869346618652, "compression_ratio": 1.9208333333333334, "no_speech_prob": 0.014727985486388206}, {"id": 891, "seek": 413892, "start": 4163.88, "end": 4165.18, "text": " of each layer.", "tokens": [51612, 295, 1184, 4583, 13, 51677], "temperature": 0.0, "avg_logprob": -0.24499869346618652, "compression_ratio": 1.9208333333333334, "no_speech_prob": 0.014727985486388206}, {"id": 892, "seek": 413892, "start": 4165.18, "end": 4168.86, "text": " So that's how, that's the hooks class is going to make things much easier.", "tokens": [51677, 407, 300, 311, 577, 11, 300, 311, 264, 26485, 1508, 307, 516, 281, 652, 721, 709, 3571, 13, 51861], "temperature": 0.0, "avg_logprob": -0.24499869346618652, "compression_ratio": 1.9208333333333334, "no_speech_prob": 0.014727985486388206}, {"id": 893, "seek": 416886, "start": 4169.0, "end": 4176.4, "text": " So the hooks class, as you can see, we're using a, making it a context manager.", "tokens": [50371, 407, 264, 26485, 1508, 11, 382, 291, 393, 536, 11, 321, 434, 1228, 257, 11, 1455, 309, 257, 4319, 6598, 13, 50741], "temperature": 0.0, "avg_logprob": -0.24596994263785227, "compression_ratio": 1.748917748917749, "no_speech_prob": 8.481091936118901e-05}, {"id": 894, "seek": 416886, "start": 4176.4, "end": 4181.139999999999, "text": " And we want to be able to loop through it.", "tokens": [50741, 400, 321, 528, 281, 312, 1075, 281, 6367, 807, 309, 13, 50978], "temperature": 0.0, "avg_logprob": -0.24596994263785227, "compression_ratio": 1.748917748917749, "no_speech_prob": 8.481091936118901e-05}, {"id": 895, "seek": 416886, "start": 4181.139999999999, "end": 4182.759999999999, "text": " We want to be able to index into it.", "tokens": [50978, 492, 528, 281, 312, 1075, 281, 8186, 666, 309, 13, 51059], "temperature": 0.0, "avg_logprob": -0.24596994263785227, "compression_ratio": 1.748917748917749, "no_speech_prob": 8.481091936118901e-05}, {"id": 896, "seek": 416886, "start": 4182.759999999999, "end": 4184.679999999999, "text": " So it's quite a lot of behavior we want.", "tokens": [51059, 407, 309, 311, 1596, 257, 688, 295, 5223, 321, 528, 13, 51155], "temperature": 0.0, "avg_logprob": -0.24596994263785227, "compression_ratio": 1.748917748917749, "no_speech_prob": 8.481091936118901e-05}, {"id": 897, "seek": 416886, "start": 4184.679999999999, "end": 4189.92, "text": " Believe it or not, all that behavior is in this tiny little thing.", "tokens": [51155, 21486, 309, 420, 406, 11, 439, 300, 5223, 307, 294, 341, 5870, 707, 551, 13, 51417], "temperature": 0.0, "avg_logprob": -0.24596994263785227, "compression_ratio": 1.748917748917749, "no_speech_prob": 8.481091936118901e-05}, {"id": 898, "seek": 416886, "start": 4189.92, "end": 4195.339999999999, "text": " And we're going to use the most flexible general way of creating context managers now.", "tokens": [51417, 400, 321, 434, 516, 281, 764, 264, 881, 11358, 2674, 636, 295, 4084, 4319, 14084, 586, 13, 51688], "temperature": 0.0, "avg_logprob": -0.24596994263785227, "compression_ratio": 1.748917748917749, "no_speech_prob": 8.481091936118901e-05}, {"id": 899, "seek": 416886, "start": 4195.339999999999, "end": 4198.48, "text": " Context managers are things that we can say with.", "tokens": [51688, 4839, 3828, 14084, 366, 721, 300, 321, 393, 584, 365, 13, 51845], "temperature": 0.0, "avg_logprob": -0.24596994263785227, "compression_ratio": 1.748917748917749, "no_speech_prob": 8.481091936118901e-05}, {"id": 900, "seek": 419848, "start": 4199.099999999999, "end": 4203.299999999999, "text": " The general way of creating a context manager is to create a class and to find two special", "tokens": [50395, 440, 2674, 636, 295, 4084, 257, 4319, 6598, 307, 281, 1884, 257, 1508, 293, 281, 915, 732, 2121, 50605], "temperature": 0.0, "avg_logprob": -0.23824079513549803, "compression_ratio": 1.6609442060085837, "no_speech_prob": 2.355258766328916e-05}, {"id": 901, "seek": 419848, "start": 4203.299999999999, "end": 4206.7, "text": " things, dunder enter and dunder exit.", "tokens": [50605, 721, 11, 274, 6617, 3242, 293, 274, 6617, 11043, 13, 50775], "temperature": 0.0, "avg_logprob": -0.23824079513549803, "compression_ratio": 1.6609442060085837, "no_speech_prob": 2.355258766328916e-05}, {"id": 902, "seek": 419848, "start": 4206.7, "end": 4212.82, "text": " Dunder enter is a function that's going to be called when it hits the with statement.", "tokens": [50775, 413, 6617, 3242, 307, 257, 2445, 300, 311, 516, 281, 312, 1219, 562, 309, 8664, 264, 365, 5629, 13, 51081], "temperature": 0.0, "avg_logprob": -0.23824079513549803, "compression_ratio": 1.6609442060085837, "no_speech_prob": 2.355258766328916e-05}, {"id": 903, "seek": 419848, "start": 4212.82, "end": 4219.66, "text": " And if you add an as blah after it, then the contents of this variable will be whatever", "tokens": [51081, 400, 498, 291, 909, 364, 382, 12288, 934, 309, 11, 550, 264, 15768, 295, 341, 7006, 486, 312, 2035, 51423], "temperature": 0.0, "avg_logprob": -0.23824079513549803, "compression_ratio": 1.6609442060085837, "no_speech_prob": 2.355258766328916e-05}, {"id": 904, "seek": 419848, "start": 4219.66, "end": 4222.74, "text": " is returned from dunder enter.", "tokens": [51423, 307, 8752, 490, 274, 6617, 3242, 13, 51577], "temperature": 0.0, "avg_logprob": -0.23824079513549803, "compression_ratio": 1.6609442060085837, "no_speech_prob": 2.355258766328916e-05}, {"id": 905, "seek": 419848, "start": 4222.74, "end": 4225.54, "text": " And as you can see, we just return the object itself.", "tokens": [51577, 400, 382, 291, 393, 536, 11, 321, 445, 2736, 264, 2657, 2564, 13, 51717], "temperature": 0.0, "avg_logprob": -0.23824079513549803, "compression_ratio": 1.6609442060085837, "no_speech_prob": 2.355258766328916e-05}, {"id": 906, "seek": 422554, "start": 4225.6, "end": 4233.12, "text": " So the hooks object is going to be stored in hooks.", "tokens": [50367, 407, 264, 26485, 2657, 307, 516, 281, 312, 12187, 294, 26485, 13, 50743], "temperature": 0.0, "avg_logprob": -0.2595812164910949, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.0001609303435543552}, {"id": 907, "seek": 422554, "start": 4233.12, "end": 4236.24, "text": " Now interestingly, the hooks class inherits from list.", "tokens": [50743, 823, 25873, 11, 264, 26485, 1508, 9484, 1208, 490, 1329, 13, 50899], "temperature": 0.0, "avg_logprob": -0.2595812164910949, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.0001609303435543552}, {"id": 908, "seek": 422554, "start": 4236.24, "end": 4237.24, "text": " You can do this.", "tokens": [50899, 509, 393, 360, 341, 13, 50949], "temperature": 0.0, "avg_logprob": -0.2595812164910949, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.0001609303435543552}, {"id": 909, "seek": 422554, "start": 4237.24, "end": 4240.6, "text": " You can actually inherit from stuff like list in Python.", "tokens": [50949, 509, 393, 767, 21389, 490, 1507, 411, 1329, 294, 15329, 13, 51117], "temperature": 0.0, "avg_logprob": -0.2595812164910949, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.0001609303435543552}, {"id": 910, "seek": 422554, "start": 4240.6, "end": 4243.94, "text": " So a hooks, the hooks object is a list.", "tokens": [51117, 407, 257, 26485, 11, 264, 26485, 2657, 307, 257, 1329, 13, 51284], "temperature": 0.0, "avg_logprob": -0.2595812164910949, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.0001609303435543552}, {"id": 911, "seek": 422554, "start": 4243.94, "end": 4247.84, "text": " And therefore we need to call the super classes constructor.", "tokens": [51284, 400, 4412, 321, 643, 281, 818, 264, 1687, 5359, 47479, 13, 51479], "temperature": 0.0, "avg_logprob": -0.2595812164910949, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.0001609303435543552}, {"id": 912, "seek": 422554, "start": 4247.84, "end": 4254.0, "text": " And we're going to pass in a, that list comprehension we saw, that list of hooks, where it's going", "tokens": [51479, 400, 321, 434, 516, 281, 1320, 294, 257, 11, 300, 1329, 44991, 321, 1866, 11, 300, 1329, 295, 26485, 11, 689, 309, 311, 516, 51787], "temperature": 0.0, "avg_logprob": -0.2595812164910949, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.0001609303435543552}, {"id": 913, "seek": 425400, "start": 4254.0, "end": 4261.86, "text": " to hook into each module in the list of modules we asked to hook into.", "tokens": [50364, 281, 6328, 666, 1184, 10088, 294, 264, 1329, 295, 16679, 321, 2351, 281, 6328, 666, 13, 50757], "temperature": 0.0, "avg_logprob": -0.23354609807332358, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.000656371412333101}, {"id": 914, "seek": 425400, "start": 4261.86, "end": 4266.74, "text": " Now we're passing in a model here, but because the model is an nn.sequential, you can actually", "tokens": [50757, 823, 321, 434, 8437, 294, 257, 2316, 510, 11, 457, 570, 264, 2316, 307, 364, 297, 77, 13, 11834, 2549, 11, 291, 393, 767, 51001], "temperature": 0.0, "avg_logprob": -0.23354609807332358, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.000656371412333101}, {"id": 915, "seek": 425400, "start": 4266.74, "end": 4270.82, "text": " loop through an nn.sequential and it returns each of the layers.", "tokens": [51001, 6367, 807, 364, 297, 77, 13, 11834, 2549, 293, 309, 11247, 1184, 295, 264, 7914, 13, 51205], "temperature": 0.0, "avg_logprob": -0.23354609807332358, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.000656371412333101}, {"id": 916, "seek": 425400, "start": 4270.82, "end": 4275.98, "text": " So this is actually very, very nice and concise and convenient.", "tokens": [51205, 407, 341, 307, 767, 588, 11, 588, 1481, 293, 44882, 293, 10851, 13, 51463], "temperature": 0.0, "avg_logprob": -0.23354609807332358, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.000656371412333101}, {"id": 917, "seek": 425400, "start": 4275.98, "end": 4277.54, "text": " So that's the constructor.", "tokens": [51463, 407, 300, 311, 264, 47479, 13, 51541], "temperature": 0.0, "avg_logprob": -0.23354609807332358, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.000656371412333101}, {"id": 918, "seek": 425400, "start": 4277.54, "end": 4280.62, "text": " Dunder enter just returns it.", "tokens": [51541, 413, 6617, 3242, 445, 11247, 309, 13, 51695], "temperature": 0.0, "avg_logprob": -0.23354609807332358, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.000656371412333101}, {"id": 919, "seek": 428062, "start": 4280.62, "end": 4285.78, "text": " Dunder exit is what's called automatically at the end of the whole block.", "tokens": [50364, 413, 6617, 11043, 307, 437, 311, 1219, 6772, 412, 264, 917, 295, 264, 1379, 3461, 13, 50622], "temperature": 0.0, "avg_logprob": -0.24224584242876837, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03904677554965019}, {"id": 920, "seek": 428062, "start": 4285.78, "end": 4290.5599999999995, "text": " So when this whole thing's finished, it's going to remove the hooks.", "tokens": [50622, 407, 562, 341, 1379, 551, 311, 4335, 11, 309, 311, 516, 281, 4159, 264, 26485, 13, 50861], "temperature": 0.0, "avg_logprob": -0.24224584242876837, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03904677554965019}, {"id": 921, "seek": 428062, "start": 4290.5599999999995, "end": 4294.76, "text": " And removing the hooks is just going to go through each hook and remove it.", "tokens": [50861, 400, 12720, 264, 26485, 307, 445, 516, 281, 352, 807, 1184, 6328, 293, 4159, 309, 13, 51071], "temperature": 0.0, "avg_logprob": -0.24224584242876837, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03904677554965019}, {"id": 922, "seek": 428062, "start": 4294.76, "end": 4303.44, "text": " The reason we can do for h itself is because remember this is a list.", "tokens": [51071, 440, 1778, 321, 393, 360, 337, 276, 2564, 307, 570, 1604, 341, 307, 257, 1329, 13, 51505], "temperature": 0.0, "avg_logprob": -0.24224584242876837, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03904677554965019}, {"id": 923, "seek": 428062, "start": 4303.44, "end": 4307.5599999999995, "text": " And then finally we've got a dunder del like before.", "tokens": [51505, 400, 550, 2721, 321, 600, 658, 257, 274, 6617, 1103, 411, 949, 13, 51711], "temperature": 0.0, "avg_logprob": -0.24224584242876837, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03904677554965019}, {"id": 924, "seek": 428062, "start": 4307.5599999999995, "end": 4309.5199999999995, "text": " And I've also added a dunder del item.", "tokens": [51711, 400, 286, 600, 611, 3869, 257, 274, 6617, 1103, 3174, 13, 51809], "temperature": 0.0, "avg_logprob": -0.24224584242876837, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03904677554965019}, {"id": 925, "seek": 430952, "start": 4309.52, "end": 4314.64, "text": " This is the thing that lets you delete a single hook from the list, which will remove that", "tokens": [50364, 639, 307, 264, 551, 300, 6653, 291, 12097, 257, 2167, 6328, 490, 264, 1329, 11, 597, 486, 4159, 300, 50620], "temperature": 0.0, "avg_logprob": -0.22588642665318082, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0011695083230733871}, {"id": 926, "seek": 430952, "start": 4314.64, "end": 4319.68, "text": " one hook and call the list's del item.", "tokens": [50620, 472, 6328, 293, 818, 264, 1329, 311, 1103, 3174, 13, 50872], "temperature": 0.0, "avg_logprob": -0.22588642665318082, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0011695083230733871}, {"id": 927, "seek": 430952, "start": 4319.68, "end": 4322.160000000001, "text": " So there's our whole thing.", "tokens": [50872, 407, 456, 311, 527, 1379, 551, 13, 50996], "temperature": 0.0, "avg_logprob": -0.22588642665318082, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0011695083230733871}, {"id": 928, "seek": 430952, "start": 4322.160000000001, "end": 4324.4400000000005, "text": " So this is going to, this, this, this one's optional.", "tokens": [50996, 407, 341, 307, 516, 281, 11, 341, 11, 341, 11, 341, 472, 311, 17312, 13, 51110], "temperature": 0.0, "avg_logprob": -0.22588642665318082, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0011695083230733871}, {"id": 929, "seek": 430952, "start": 4324.4400000000005, "end": 4329.860000000001, "text": " This is the one that lets us remove a single hook rather than all of them.", "tokens": [51110, 639, 307, 264, 472, 300, 6653, 505, 4159, 257, 2167, 6328, 2831, 813, 439, 295, 552, 13, 51381], "temperature": 0.0, "avg_logprob": -0.22588642665318082, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0011695083230733871}, {"id": 930, "seek": 430952, "start": 4329.860000000001, "end": 4333.34, "text": " So let's just understand some of what's going on there.", "tokens": [51381, 407, 718, 311, 445, 1223, 512, 295, 437, 311, 516, 322, 456, 13, 51555], "temperature": 0.0, "avg_logprob": -0.22588642665318082, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0011695083230733871}, {"id": 931, "seek": 430952, "start": 4333.34, "end": 4337.200000000001, "text": " So here's a dummy context manager.", "tokens": [51555, 407, 510, 311, 257, 35064, 4319, 6598, 13, 51748], "temperature": 0.0, "avg_logprob": -0.22588642665318082, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0011695083230733871}, {"id": 932, "seek": 433720, "start": 4337.2, "end": 4342.5599999999995, "text": " As you can see here, it's got a dunder enter, which is going to return itself.", "tokens": [50364, 1018, 291, 393, 536, 510, 11, 309, 311, 658, 257, 274, 6617, 3242, 11, 597, 307, 516, 281, 2736, 2564, 13, 50632], "temperature": 0.0, "avg_logprob": -0.28383777822766987, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.03021274134516716}, {"id": 933, "seek": 433720, "start": 4342.5599999999995, "end": 4344.24, "text": " It's going to print something.", "tokens": [50632, 467, 311, 516, 281, 4482, 746, 13, 50716], "temperature": 0.0, "avg_logprob": -0.28383777822766987, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.03021274134516716}, {"id": 934, "seek": 433720, "start": 4344.24, "end": 4348.22, "text": " So you can see here I call with dummy context manager.", "tokens": [50716, 407, 291, 393, 536, 510, 286, 818, 365, 35064, 4319, 6598, 13, 50915], "temperature": 0.0, "avg_logprob": -0.28383777822766987, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.03021274134516716}, {"id": 935, "seek": 433720, "start": 4348.22, "end": 4352.139999999999, "text": " And so therefore it prints, let's go first.", "tokens": [50915, 400, 370, 4412, 309, 22305, 11, 718, 311, 352, 700, 13, 51111], "temperature": 0.0, "avg_logprob": -0.28383777822766987, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.03021274134516716}, {"id": 936, "seek": 433720, "start": 4352.139999999999, "end": 4358.179999999999, "text": " The second thing it's going to do is call this code inside the context manager.", "tokens": [51111, 440, 1150, 551, 309, 311, 516, 281, 360, 307, 818, 341, 3089, 1854, 264, 4319, 6598, 13, 51413], "temperature": 0.0, "avg_logprob": -0.28383777822766987, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.03021274134516716}, {"id": 937, "seek": 433720, "start": 4358.179999999999, "end": 4359.5199999999995, "text": " So we've got as DCM.", "tokens": [51413, 407, 321, 600, 658, 382, 9114, 44, 13, 51480], "temperature": 0.0, "avg_logprob": -0.28383777822766987, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.03021274134516716}, {"id": 938, "seek": 433720, "start": 4359.5199999999995, "end": 4361.5599999999995, "text": " So that's itself.", "tokens": [51480, 407, 300, 311, 2564, 13, 51582], "temperature": 0.0, "avg_logprob": -0.28383777822766987, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.03021274134516716}, {"id": 939, "seek": 433720, "start": 4361.5599999999995, "end": 4365.099999999999, "text": " And so it's going to call hello, which prints hello.", "tokens": [51582, 400, 370, 309, 311, 516, 281, 818, 7751, 11, 597, 22305, 7751, 13, 51759], "temperature": 0.0, "avg_logprob": -0.28383777822766987, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.03021274134516716}, {"id": 940, "seek": 436510, "start": 4365.1, "end": 4367.4800000000005, "text": " So here it is.", "tokens": [50364, 407, 510, 309, 307, 13, 50483], "temperature": 0.0, "avg_logprob": -0.22393317449660527, "compression_ratio": 1.6, "no_speech_prob": 0.0056418925523757935}, {"id": 941, "seek": 436510, "start": 4367.4800000000005, "end": 4373.72, "text": " And then finally, it's going to automatically call exit, dunder exit, which is all done.", "tokens": [50483, 400, 550, 2721, 11, 309, 311, 516, 281, 6772, 818, 11043, 11, 274, 6617, 11043, 11, 597, 307, 439, 1096, 13, 50795], "temperature": 0.0, "avg_logprob": -0.22393317449660527, "compression_ratio": 1.6, "no_speech_prob": 0.0056418925523757935}, {"id": 942, "seek": 436510, "start": 4373.72, "end": 4374.780000000001, "text": " So here's all done.", "tokens": [50795, 407, 510, 311, 439, 1096, 13, 50848], "temperature": 0.0, "avg_logprob": -0.22393317449660527, "compression_ratio": 1.6, "no_speech_prob": 0.0056418925523757935}, {"id": 943, "seek": 436510, "start": 4374.780000000001, "end": 4379.54, "text": " So again, if you haven't used context managers before, you want to be creating little samples", "tokens": [50848, 407, 797, 11, 498, 291, 2378, 380, 1143, 4319, 14084, 949, 11, 291, 528, 281, 312, 4084, 707, 10938, 51086], "temperature": 0.0, "avg_logprob": -0.22393317449660527, "compression_ratio": 1.6, "no_speech_prob": 0.0056418925523757935}, {"id": 944, "seek": 436510, "start": 4379.54, "end": 4382.14, "text": " like this yourself and getting them to work.", "tokens": [51086, 411, 341, 1803, 293, 1242, 552, 281, 589, 13, 51216], "temperature": 0.0, "avg_logprob": -0.22393317449660527, "compression_ratio": 1.6, "no_speech_prob": 0.0056418925523757935}, {"id": 945, "seek": 436510, "start": 4382.14, "end": 4385.700000000001, "text": " So this is your key homework for this week.", "tokens": [51216, 407, 341, 307, 428, 2141, 14578, 337, 341, 1243, 13, 51394], "temperature": 0.0, "avg_logprob": -0.22393317449660527, "compression_ratio": 1.6, "no_speech_prob": 0.0056418925523757935}, {"id": 946, "seek": 436510, "start": 4385.700000000001, "end": 4391.64, "text": " Is anything in the lesson where we're using a part of Python you're not 100% familiar", "tokens": [51394, 1119, 1340, 294, 264, 6898, 689, 321, 434, 1228, 257, 644, 295, 15329, 291, 434, 406, 2319, 4, 4963, 51691], "temperature": 0.0, "avg_logprob": -0.22393317449660527, "compression_ratio": 1.6, "no_speech_prob": 0.0056418925523757935}, {"id": 947, "seek": 439164, "start": 4391.64, "end": 4398.240000000001, "text": " with is for you to from scratch to create some simple like kind of dummy version that", "tokens": [50364, 365, 307, 337, 291, 281, 490, 8459, 281, 1884, 512, 2199, 411, 733, 295, 35064, 3037, 300, 50694], "temperature": 0.0, "avg_logprob": -0.28429539998372394, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.28447410464286804}, {"id": 948, "seek": 439164, "start": 4398.240000000001, "end": 4401.68, "text": " fully explores what it's doing.", "tokens": [50694, 4498, 45473, 437, 309, 311, 884, 13, 50866], "temperature": 0.0, "avg_logprob": -0.28429539998372394, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.28447410464286804}, {"id": 949, "seek": 439164, "start": 4401.68, "end": 4407.8, "text": " If you're familiar with all the Python pieces, then it's to create your own, you know, that", "tokens": [50866, 759, 291, 434, 4963, 365, 439, 264, 15329, 3755, 11, 550, 309, 311, 281, 1884, 428, 1065, 11, 291, 458, 11, 300, 51172], "temperature": 0.0, "avg_logprob": -0.28429539998372394, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.28447410464286804}, {"id": 950, "seek": 439164, "start": 4407.8, "end": 4414.96, "text": " is to explore do the same thing with the PyTorch pieces like with with hooks and so forth.", "tokens": [51172, 307, 281, 6839, 360, 264, 912, 551, 365, 264, 9953, 51, 284, 339, 3755, 411, 365, 365, 26485, 293, 370, 5220, 13, 51530], "temperature": 0.0, "avg_logprob": -0.28429539998372394, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.28447410464286804}, {"id": 951, "seek": 439164, "start": 4414.96, "end": 4419.04, "text": " And so I just wanted to show you also what it's like to inherit from list.", "tokens": [51530, 400, 370, 286, 445, 1415, 281, 855, 291, 611, 437, 309, 311, 411, 281, 21389, 490, 1329, 13, 51734], "temperature": 0.0, "avg_logprob": -0.28429539998372394, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.28447410464286804}, {"id": 952, "seek": 439164, "start": 4419.04, "end": 4421.200000000001, "text": " So here I'm here inheriting from a list.", "tokens": [51734, 407, 510, 286, 478, 510, 9484, 1748, 490, 257, 1329, 13, 51842], "temperature": 0.0, "avg_logprob": -0.28429539998372394, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.28447410464286804}, {"id": 953, "seek": 442120, "start": 4421.76, "end": 4425.16, "text": " And I could redefine how dunder del item works.", "tokens": [50392, 400, 286, 727, 38818, 533, 577, 274, 6617, 1103, 3174, 1985, 13, 50562], "temperature": 0.0, "avg_logprob": -0.2432029067829091, "compression_ratio": 1.6218905472636815, "no_speech_prob": 0.004982230253517628}, {"id": 954, "seek": 442120, "start": 4425.16, "end": 4428.58, "text": " So now I can create a dummy list.", "tokens": [50562, 407, 586, 286, 393, 1884, 257, 35064, 1329, 13, 50733], "temperature": 0.0, "avg_logprob": -0.2432029067829091, "compression_ratio": 1.6218905472636815, "no_speech_prob": 0.004982230253517628}, {"id": 955, "seek": 442120, "start": 4428.58, "end": 4431.78, "text": " And it looks exactly the same as usual.", "tokens": [50733, 400, 309, 1542, 2293, 264, 912, 382, 7713, 13, 50893], "temperature": 0.0, "avg_logprob": -0.2432029067829091, "compression_ratio": 1.6218905472636815, "no_speech_prob": 0.004982230253517628}, {"id": 956, "seek": 442120, "start": 4431.78, "end": 4441.3, "text": " But now if I delete an item from the list, it's going to call my overridden version.", "tokens": [50893, 583, 586, 498, 286, 12097, 364, 3174, 490, 264, 1329, 11, 309, 311, 516, 281, 818, 452, 670, 81, 6171, 3037, 13, 51369], "temperature": 0.0, "avg_logprob": -0.2432029067829091, "compression_ratio": 1.6218905472636815, "no_speech_prob": 0.004982230253517628}, {"id": 957, "seek": 442120, "start": 4441.3, "end": 4443.76, "text": " And then it will call the original version.", "tokens": [51369, 400, 550, 309, 486, 818, 264, 3380, 3037, 13, 51492], "temperature": 0.0, "avg_logprob": -0.2432029067829091, "compression_ratio": 1.6218905472636815, "no_speech_prob": 0.004982230253517628}, {"id": 958, "seek": 442120, "start": 4443.76, "end": 4448.099999999999, "text": " And so the list is now got removed that item and did this at the same time.", "tokens": [51492, 400, 370, 264, 1329, 307, 586, 658, 7261, 300, 3174, 293, 630, 341, 412, 264, 912, 565, 13, 51709], "temperature": 0.0, "avg_logprob": -0.2432029067829091, "compression_ratio": 1.6218905472636815, "no_speech_prob": 0.004982230253517628}, {"id": 959, "seek": 444810, "start": 4448.1, "end": 4454.54, "text": " So you can see you can actually, yeah, modify how Python works, or create your own things", "tokens": [50364, 407, 291, 393, 536, 291, 393, 767, 11, 1338, 11, 16927, 577, 15329, 1985, 11, 420, 1884, 428, 1065, 721, 50686], "temperature": 0.0, "avg_logprob": -0.2556713439606048, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.02843366004526615}, {"id": 960, "seek": 444810, "start": 4454.54, "end": 4460.1, "text": " that get all the behavior or the convenience of Python classes like this one and add stuff", "tokens": [50686, 300, 483, 439, 264, 5223, 420, 264, 19283, 295, 15329, 5359, 411, 341, 472, 293, 909, 1507, 50964], "temperature": 0.0, "avg_logprob": -0.2556713439606048, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.02843366004526615}, {"id": 961, "seek": 444810, "start": 4460.1, "end": 4461.9800000000005, "text": " to them.", "tokens": [50964, 281, 552, 13, 51058], "temperature": 0.0, "avg_logprob": -0.2556713439606048, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.02843366004526615}, {"id": 962, "seek": 444810, "start": 4461.9800000000005, "end": 4464.18, "text": " So that's what's happening there.", "tokens": [51058, 407, 300, 311, 437, 311, 2737, 456, 13, 51168], "temperature": 0.0, "avg_logprob": -0.2556713439606048, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.02843366004526615}, {"id": 963, "seek": 444810, "start": 4464.18, "end": 4469.3, "text": " Okay, so that's our hooks class.", "tokens": [51168, 1033, 11, 370, 300, 311, 527, 26485, 1508, 13, 51424], "temperature": 0.0, "avg_logprob": -0.2556713439606048, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.02843366004526615}, {"id": 964, "seek": 444810, "start": 4469.3, "end": 4477.18, "text": " So the next bit was developed, largely developed the last time I think it was that we did a", "tokens": [51424, 407, 264, 958, 857, 390, 4743, 11, 11611, 4743, 264, 1036, 565, 286, 519, 309, 390, 300, 321, 630, 257, 51818], "temperature": 0.0, "avg_logprob": -0.2556713439606048, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.02843366004526615}, {"id": 965, "seek": 447718, "start": 4477.26, "end": 4481.22, "text": " part two course in San Francisco with Stefano.", "tokens": [50368, 644, 732, 1164, 294, 5271, 12279, 365, 43421, 3730, 13, 50566], "temperature": 0.0, "avg_logprob": -0.2428500002080744, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.00023413541202899069}, {"id": 966, "seek": 447718, "start": 4481.22, "end": 4485.740000000001, "text": " So many thanks to him for helping get this next bit looking great.", "tokens": [50566, 407, 867, 3231, 281, 796, 337, 4315, 483, 341, 958, 857, 1237, 869, 13, 50792], "temperature": 0.0, "avg_logprob": -0.2428500002080744, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.00023413541202899069}, {"id": 967, "seek": 447718, "start": 4485.740000000001, "end": 4496.3, "text": " We're going to create my favorite single image explanations of what's going on inside a model.", "tokens": [50792, 492, 434, 516, 281, 1884, 452, 2954, 2167, 3256, 28708, 295, 437, 311, 516, 322, 1854, 257, 2316, 13, 51320], "temperature": 0.0, "avg_logprob": -0.2428500002080744, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.00023413541202899069}, {"id": 968, "seek": 447718, "start": 4496.3, "end": 4501.1, "text": " We call them the colorful dimension, which they're histograms.", "tokens": [51320, 492, 818, 552, 264, 18506, 10139, 11, 597, 436, 434, 49816, 82, 13, 51560], "temperature": 0.0, "avg_logprob": -0.2428500002080744, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.00023413541202899069}, {"id": 969, "seek": 447718, "start": 4501.1, "end": 4503.46, "text": " We're going to take our same append stats.", "tokens": [51560, 492, 434, 516, 281, 747, 527, 912, 34116, 18152, 13, 51678], "temperature": 0.0, "avg_logprob": -0.2428500002080744, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.00023413541202899069}, {"id": 970, "seek": 447718, "start": 4503.46, "end": 4505.02, "text": " These are all the same as before.", "tokens": [51678, 1981, 366, 439, 264, 912, 382, 949, 13, 51756], "temperature": 0.0, "avg_logprob": -0.2428500002080744, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.00023413541202899069}, {"id": 971, "seek": 450502, "start": 4505.02, "end": 4510.620000000001, "text": " We're going to add an extra line of code, which is to get a histogram of the absolute", "tokens": [50364, 492, 434, 516, 281, 909, 364, 2857, 1622, 295, 3089, 11, 597, 307, 281, 483, 257, 49816, 295, 264, 8236, 50644], "temperature": 0.0, "avg_logprob": -0.21277606022822393, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.009559174068272114}, {"id": 972, "seek": 450502, "start": 4510.620000000001, "end": 4512.700000000001, "text": " values of the activations.", "tokens": [50644, 4190, 295, 264, 2430, 763, 13, 50748], "temperature": 0.0, "avg_logprob": -0.21277606022822393, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.009559174068272114}, {"id": 973, "seek": 450502, "start": 4512.700000000001, "end": 4521.360000000001, "text": " So a histogram, to remind you, is something that takes a collection of numbers and tells", "tokens": [50748, 407, 257, 49816, 11, 281, 4160, 291, 11, 307, 746, 300, 2516, 257, 5765, 295, 3547, 293, 5112, 51181], "temperature": 0.0, "avg_logprob": -0.21277606022822393, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.009559174068272114}, {"id": 974, "seek": 450502, "start": 4521.360000000001, "end": 4525.660000000001, "text": " you how frequent each group of numbers are.", "tokens": [51181, 291, 577, 18004, 1184, 1594, 295, 3547, 366, 13, 51396], "temperature": 0.0, "avg_logprob": -0.21277606022822393, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.009559174068272114}, {"id": 975, "seek": 450502, "start": 4525.660000000001, "end": 4532.3, "text": " And we're going to create 50 bins for our histogram.", "tokens": [51396, 400, 321, 434, 516, 281, 1884, 2625, 41275, 337, 527, 49816, 13, 51728], "temperature": 0.0, "avg_logprob": -0.21277606022822393, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.009559174068272114}, {"id": 976, "seek": 453230, "start": 4532.3, "end": 4540.820000000001, "text": " So we will use our hooks that we just created, and we're going to use this new version of", "tokens": [50364, 407, 321, 486, 764, 527, 26485, 300, 321, 445, 2942, 11, 293, 321, 434, 516, 281, 764, 341, 777, 3037, 295, 50790], "temperature": 0.0, "avg_logprob": -0.21275170270134422, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.01665688119828701}, {"id": 977, "seek": 453230, "start": 4540.820000000001, "end": 4542.18, "text": " append stats.", "tokens": [50790, 34116, 18152, 13, 50858], "temperature": 0.0, "avg_logprob": -0.21275170270134422, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.01665688119828701}, {"id": 978, "seek": 453230, "start": 4542.18, "end": 4547.5, "text": " So it's going to train as before, but now we're going to, in addition, have this extra", "tokens": [50858, 407, 309, 311, 516, 281, 3847, 382, 949, 11, 457, 586, 321, 434, 516, 281, 11, 294, 4500, 11, 362, 341, 2857, 51124], "temperature": 0.0, "avg_logprob": -0.21275170270134422, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.01665688119828701}, {"id": 979, "seek": 453230, "start": 4547.5, "end": 4552.4800000000005, "text": " thing in stats, which is going to contain a histogram.", "tokens": [51124, 551, 294, 18152, 11, 597, 307, 516, 281, 5304, 257, 49816, 13, 51373], "temperature": 0.0, "avg_logprob": -0.21275170270134422, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.01665688119828701}, {"id": 980, "seek": 453230, "start": 4552.4800000000005, "end": 4558.18, "text": " And so with that, we're now going to create this amazing plot.", "tokens": [51373, 400, 370, 365, 300, 11, 321, 434, 586, 516, 281, 1884, 341, 2243, 7542, 13, 51658], "temperature": 0.0, "avg_logprob": -0.21275170270134422, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.01665688119828701}, {"id": 981, "seek": 455818, "start": 4558.18, "end": 4564.58, "text": " Now what this plot is showing is for the first, second, third, and fourth layers, what", "tokens": [50364, 823, 437, 341, 7542, 307, 4099, 307, 337, 264, 700, 11, 1150, 11, 2636, 11, 293, 6409, 7914, 11, 437, 50684], "temperature": 0.0, "avg_logprob": -0.2092016643948025, "compression_ratio": 1.576036866359447, "no_speech_prob": 0.003027769038453698}, {"id": 982, "seek": 455818, "start": 4564.58, "end": 4565.780000000001, "text": " does the training look like?", "tokens": [50684, 775, 264, 3097, 574, 411, 30, 50744], "temperature": 0.0, "avg_logprob": -0.2092016643948025, "compression_ratio": 1.576036866359447, "no_speech_prob": 0.003027769038453698}, {"id": 983, "seek": 455818, "start": 4565.780000000001, "end": 4572.88, "text": " And you can immediately see the basic idea is that we're seeing this same pattern.", "tokens": [50744, 400, 291, 393, 4258, 536, 264, 3875, 1558, 307, 300, 321, 434, 2577, 341, 912, 5102, 13, 51099], "temperature": 0.0, "avg_logprob": -0.2092016643948025, "compression_ratio": 1.576036866359447, "no_speech_prob": 0.003027769038453698}, {"id": 984, "seek": 455818, "start": 4572.88, "end": 4575.22, "text": " But what is this pattern showing?", "tokens": [51099, 583, 437, 307, 341, 5102, 4099, 30, 51216], "temperature": 0.0, "avg_logprob": -0.2092016643948025, "compression_ratio": 1.576036866359447, "no_speech_prob": 0.003027769038453698}, {"id": 985, "seek": 455818, "start": 4575.22, "end": 4578.58, "text": " What exactly is going on in these pictures?", "tokens": [51216, 708, 2293, 307, 516, 322, 294, 613, 5242, 30, 51384], "temperature": 0.0, "avg_logprob": -0.2092016643948025, "compression_ratio": 1.576036866359447, "no_speech_prob": 0.003027769038453698}, {"id": 986, "seek": 455818, "start": 4578.58, "end": 4587.84, "text": " So I think it might be best if we try and draw a picture of this.", "tokens": [51384, 407, 286, 519, 309, 1062, 312, 1151, 498, 321, 853, 293, 2642, 257, 3036, 295, 341, 13, 51847], "temperature": 0.0, "avg_logprob": -0.2092016643948025, "compression_ratio": 1.576036866359447, "no_speech_prob": 0.003027769038453698}, {"id": 987, "seek": 458784, "start": 4588.5, "end": 4595.28, "text": " So let's take a normal histogram.", "tokens": [50397, 407, 718, 311, 747, 257, 2710, 49816, 13, 50736], "temperature": 0.0, "avg_logprob": -0.3457989388323845, "compression_ratio": 1.4871794871794872, "no_speech_prob": 0.00013765404582954943}, {"id": 988, "seek": 458784, "start": 4595.28, "end": 4608.0, "text": " So let's take a normal histogram where we basically have grouped all the data into bins,", "tokens": [50736, 407, 718, 311, 747, 257, 2710, 49816, 689, 321, 1936, 362, 41877, 439, 264, 1412, 666, 41275, 11, 51372], "temperature": 0.0, "avg_logprob": -0.3457989388323845, "compression_ratio": 1.4871794871794872, "no_speech_prob": 0.00013765404582954943}, {"id": 989, "seek": 458784, "start": 4608.0, "end": 4614.56, "text": " and then we have counts of how much is in each bin.", "tokens": [51372, 293, 550, 321, 362, 14893, 295, 577, 709, 307, 294, 1184, 5171, 13, 51700], "temperature": 0.0, "avg_logprob": -0.3457989388323845, "compression_ratio": 1.4871794871794872, "no_speech_prob": 0.00013765404582954943}, {"id": 990, "seek": 461456, "start": 4614.56, "end": 4624.8, "text": " So for example, this will be like the value of the activations, and it might be, say,", "tokens": [50364, 407, 337, 1365, 11, 341, 486, 312, 411, 264, 2158, 295, 264, 2430, 763, 11, 293, 309, 1062, 312, 11, 584, 11, 50876], "temperature": 0.0, "avg_logprob": -0.3747551683066548, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.0004044793313369155}, {"id": 991, "seek": 461456, "start": 4624.8, "end": 4628.92, "text": " from 0 to 10, and then from 10 to 20, and from 20 to 30.", "tokens": [50876, 490, 1958, 281, 1266, 11, 293, 550, 490, 1266, 281, 945, 11, 293, 490, 945, 281, 2217, 13, 51082], "temperature": 0.0, "avg_logprob": -0.3747551683066548, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.0004044793313369155}, {"id": 992, "seek": 461456, "start": 4628.92, "end": 4634.200000000001, "text": " And these are generally equally spaced bins.", "tokens": [51082, 400, 613, 366, 5101, 12309, 43766, 41275, 13, 51346], "temperature": 0.0, "avg_logprob": -0.3747551683066548, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.0004044793313369155}, {"id": 993, "seek": 461456, "start": 4634.200000000001, "end": 4636.68, "text": " Okay.", "tokens": [51346, 1033, 13, 51470], "temperature": 0.0, "avg_logprob": -0.3747551683066548, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.0004044793313369155}, {"id": 994, "seek": 461456, "start": 4636.68, "end": 4640.92, "text": " And then here is the count.", "tokens": [51470, 400, 550, 510, 307, 264, 1207, 13, 51682], "temperature": 0.0, "avg_logprob": -0.3747551683066548, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.0004044793313369155}, {"id": 995, "seek": 464092, "start": 4641.28, "end": 4646.32, "text": " So that's the number of items with that range of values.", "tokens": [50382, 407, 300, 311, 264, 1230, 295, 4754, 365, 300, 3613, 295, 4190, 13, 50634], "temperature": 0.0, "avg_logprob": -0.29950335874395856, "compression_ratio": 1.4324324324324325, "no_speech_prob": 0.00032503571128472686}, {"id": 996, "seek": 464092, "start": 4646.32, "end": 4649.28, "text": " So this is called a histogram.", "tokens": [50634, 407, 341, 307, 1219, 257, 49816, 13, 50782], "temperature": 0.0, "avg_logprob": -0.29950335874395856, "compression_ratio": 1.4324324324324325, "no_speech_prob": 0.00032503571128472686}, {"id": 997, "seek": 464092, "start": 4649.28, "end": 4650.84, "text": " Okay.", "tokens": [50782, 1033, 13, 50860], "temperature": 0.0, "avg_logprob": -0.29950335874395856, "compression_ratio": 1.4324324324324325, "no_speech_prob": 0.00032503571128472686}, {"id": 998, "seek": 464092, "start": 4650.84, "end": 4663.32, "text": " So what Stefano and I did was we actually turned that histogram, that whole histogram,", "tokens": [50860, 407, 437, 43421, 3730, 293, 286, 630, 390, 321, 767, 3574, 300, 49816, 11, 300, 1379, 49816, 11, 51484], "temperature": 0.0, "avg_logprob": -0.29950335874395856, "compression_ratio": 1.4324324324324325, "no_speech_prob": 0.00032503571128472686}, {"id": 999, "seek": 464092, "start": 4663.32, "end": 4666.52, "text": " into a single column of pixels.", "tokens": [51484, 666, 257, 2167, 7738, 295, 18668, 13, 51644], "temperature": 0.0, "avg_logprob": -0.29950335874395856, "compression_ratio": 1.4324324324324325, "no_speech_prob": 0.00032503571128472686}, {"id": 1000, "seek": 466652, "start": 4666.52, "end": 4672.92, "text": " So if I take one column of pixels, that's actually one histogram.", "tokens": [50364, 407, 498, 286, 747, 472, 7738, 295, 18668, 11, 300, 311, 767, 472, 49816, 13, 50684], "temperature": 0.0, "avg_logprob": -0.22025440720950856, "compression_ratio": 1.3546099290780143, "no_speech_prob": 0.02758300118148327}, {"id": 1001, "seek": 466652, "start": 4672.92, "end": 4678.040000000001, "text": " And the way we do it is we take these numbers.", "tokens": [50684, 400, 264, 636, 321, 360, 309, 307, 321, 747, 613, 3547, 13, 50940], "temperature": 0.0, "avg_logprob": -0.22025440720950856, "compression_ratio": 1.3546099290780143, "no_speech_prob": 0.02758300118148327}, {"id": 1002, "seek": 466652, "start": 4678.040000000001, "end": 4690.76, "text": " So let's say, let's say it's like 14, that one's like 2, 7, 9, 11, 3, 2, 4, 2.", "tokens": [50940, 407, 718, 311, 584, 11, 718, 311, 584, 309, 311, 411, 3499, 11, 300, 472, 311, 411, 568, 11, 1614, 11, 1722, 11, 2975, 11, 805, 11, 568, 11, 1017, 11, 568, 13, 51576], "temperature": 0.0, "avg_logprob": -0.22025440720950856, "compression_ratio": 1.3546099290780143, "no_speech_prob": 0.02758300118148327}, {"id": 1003, "seek": 469076, "start": 4690.76, "end": 4698.16, "text": " And so then what we do is we turn it into a single column.", "tokens": [50364, 400, 370, 550, 437, 321, 360, 307, 321, 1261, 309, 666, 257, 2167, 7738, 13, 50734], "temperature": 0.0, "avg_logprob": -0.2710498638367385, "compression_ratio": 1.4364640883977902, "no_speech_prob": 0.005554801318794489}, {"id": 1004, "seek": 469076, "start": 4698.16, "end": 4703.76, "text": " And so in this case, we've got 1, 2, 3, 4, 5, 6, 7, 8, 9 groups, right?", "tokens": [50734, 400, 370, 294, 341, 1389, 11, 321, 600, 658, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 1386, 11, 1614, 11, 1649, 11, 1722, 3935, 11, 558, 30, 51014], "temperature": 0.0, "avg_logprob": -0.2710498638367385, "compression_ratio": 1.4364640883977902, "no_speech_prob": 0.005554801318794489}, {"id": 1005, "seek": 469076, "start": 4703.76, "end": 4707.16, "text": " So we would create our 9 groups.", "tokens": [51014, 407, 321, 576, 1884, 527, 1722, 3935, 13, 51184], "temperature": 0.0, "avg_logprob": -0.2710498638367385, "compression_ratio": 1.4364640883977902, "no_speech_prob": 0.005554801318794489}, {"id": 1006, "seek": 469076, "start": 4707.16, "end": 4713.400000000001, "text": " Sorry, they were meant to be evenly spaced, but they were not a very good job.", "tokens": [51184, 4919, 11, 436, 645, 4140, 281, 312, 17658, 43766, 11, 457, 436, 645, 406, 257, 588, 665, 1691, 13, 51496], "temperature": 0.0, "avg_logprob": -0.2710498638367385, "compression_ratio": 1.4364640883977902, "no_speech_prob": 0.005554801318794489}, {"id": 1007, "seek": 469076, "start": 4713.400000000001, "end": 4714.6, "text": " Got our 9 groups.", "tokens": [51496, 5803, 527, 1722, 3935, 13, 51556], "temperature": 0.0, "avg_logprob": -0.2710498638367385, "compression_ratio": 1.4364640883977902, "no_speech_prob": 0.005554801318794489}, {"id": 1008, "seek": 471460, "start": 4714.6, "end": 4717.200000000001, "text": " And so we take the first group, it's 14.", "tokens": [50364, 400, 370, 321, 747, 264, 700, 1594, 11, 309, 311, 3499, 13, 50494], "temperature": 0.0, "avg_logprob": -0.2630175598396743, "compression_ratio": 1.7605042016806722, "no_speech_prob": 0.042717866599559784}, {"id": 1009, "seek": 471460, "start": 4717.200000000001, "end": 4724.4800000000005, "text": " And what we do is we color it with a gradient and a color according to how big that number", "tokens": [50494, 400, 437, 321, 360, 307, 321, 2017, 309, 365, 257, 16235, 293, 257, 2017, 4650, 281, 577, 955, 300, 1230, 50858], "temperature": 0.0, "avg_logprob": -0.2630175598396743, "compression_ratio": 1.7605042016806722, "no_speech_prob": 0.042717866599559784}, {"id": 1010, "seek": 471460, "start": 4724.4800000000005, "end": 4725.4800000000005, "text": " is.", "tokens": [50858, 307, 13, 50908], "temperature": 0.0, "avg_logprob": -0.2630175598396743, "compression_ratio": 1.7605042016806722, "no_speech_prob": 0.042717866599559784}, {"id": 1011, "seek": 471460, "start": 4725.4800000000005, "end": 4727.320000000001, "text": " So 14 is a real big number.", "tokens": [50908, 407, 3499, 307, 257, 957, 955, 1230, 13, 51000], "temperature": 0.0, "avg_logprob": -0.2630175598396743, "compression_ratio": 1.7605042016806722, "no_speech_prob": 0.042717866599559784}, {"id": 1012, "seek": 471460, "start": 4727.320000000001, "end": 4730.52, "text": " So depending on, you know, what gradient we use, maybe red's really, really big.", "tokens": [51000, 407, 5413, 322, 11, 291, 458, 11, 437, 16235, 321, 764, 11, 1310, 2182, 311, 534, 11, 534, 955, 13, 51160], "temperature": 0.0, "avg_logprob": -0.2630175598396743, "compression_ratio": 1.7605042016806722, "no_speech_prob": 0.042717866599559784}, {"id": 1013, "seek": 471460, "start": 4730.52, "end": 4734.88, "text": " And the next one's really small, which might be like green.", "tokens": [51160, 400, 264, 958, 472, 311, 534, 1359, 11, 597, 1062, 312, 411, 3092, 13, 51378], "temperature": 0.0, "avg_logprob": -0.2630175598396743, "compression_ratio": 1.7605042016806722, "no_speech_prob": 0.042717866599559784}, {"id": 1014, "seek": 471460, "start": 4734.88, "end": 4740.04, "text": " And then the next one's quite big in the middle, which is like blue.", "tokens": [51378, 400, 550, 264, 958, 472, 311, 1596, 955, 294, 264, 2808, 11, 597, 307, 411, 3344, 13, 51636], "temperature": 0.0, "avg_logprob": -0.2630175598396743, "compression_ratio": 1.7605042016806722, "no_speech_prob": 0.042717866599559784}, {"id": 1015, "seek": 471460, "start": 4740.04, "end": 4742.240000000001, "text": " Next one's getting quite, quite bigger still.", "tokens": [51636, 3087, 472, 311, 1242, 1596, 11, 1596, 3801, 920, 13, 51746], "temperature": 0.0, "avg_logprob": -0.2630175598396743, "compression_ratio": 1.7605042016806722, "no_speech_prob": 0.042717866599559784}, {"id": 1016, "seek": 474224, "start": 4742.24, "end": 4747.5599999999995, "text": " So maybe it's just a little bit, sorry, should go back to red, go back to more red.", "tokens": [50364, 407, 1310, 309, 311, 445, 257, 707, 857, 11, 2597, 11, 820, 352, 646, 281, 2182, 11, 352, 646, 281, 544, 2182, 13, 50630], "temperature": 0.0, "avg_logprob": -0.27748387130265384, "compression_ratio": 1.5868544600938967, "no_speech_prob": 0.012431472539901733}, {"id": 1017, "seek": 474224, "start": 4747.5599999999995, "end": 4751.5599999999995, "text": " Next one's bigger still, so it's even more red, and so forth.", "tokens": [50630, 3087, 472, 311, 3801, 920, 11, 370, 309, 311, 754, 544, 2182, 11, 293, 370, 5220, 13, 50830], "temperature": 0.0, "avg_logprob": -0.27748387130265384, "compression_ratio": 1.5868544600938967, "no_speech_prob": 0.012431472539901733}, {"id": 1018, "seek": 474224, "start": 4751.5599999999995, "end": 4758.679999999999, "text": " So basically we're taking the histogram and taking it into a color-coded single column", "tokens": [50830, 407, 1936, 321, 434, 1940, 264, 49816, 293, 1940, 309, 666, 257, 2017, 12, 66, 12340, 2167, 7738, 51186], "temperature": 0.0, "avg_logprob": -0.27748387130265384, "compression_ratio": 1.5868544600938967, "no_speech_prob": 0.012431472539901733}, {"id": 1019, "seek": 474224, "start": 4758.679999999999, "end": 4763.08, "text": " plot, if that makes sense.", "tokens": [51186, 7542, 11, 498, 300, 1669, 2020, 13, 51406], "temperature": 0.0, "avg_logprob": -0.27748387130265384, "compression_ratio": 1.5868544600938967, "no_speech_prob": 0.012431472539901733}, {"id": 1020, "seek": 474224, "start": 4763.08, "end": 4770.5599999999995, "text": " And so what that means is that at the very, so let's take layer number 2 here.", "tokens": [51406, 400, 370, 437, 300, 1355, 307, 300, 412, 264, 588, 11, 370, 718, 311, 747, 4583, 1230, 568, 510, 13, 51780], "temperature": 0.0, "avg_logprob": -0.27748387130265384, "compression_ratio": 1.5868544600938967, "no_speech_prob": 0.012431472539901733}, {"id": 1021, "seek": 477056, "start": 4770.56, "end": 4773.92, "text": " Layer number 2, we can take the very first column.", "tokens": [50364, 35166, 1230, 568, 11, 321, 393, 747, 264, 588, 700, 7738, 13, 50532], "temperature": 0.0, "avg_logprob": -0.2874564102717808, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.0014325326774269342}, {"id": 1022, "seek": 477056, "start": 4773.92, "end": 4779.8, "text": " And so in the color scheme that actually matplotlib's picked here, yellow is the most common, and", "tokens": [50532, 400, 370, 294, 264, 2017, 12232, 300, 767, 3803, 564, 310, 38270, 311, 6183, 510, 11, 5566, 307, 264, 881, 2689, 11, 293, 50826], "temperature": 0.0, "avg_logprob": -0.2874564102717808, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.0014325326774269342}, {"id": 1023, "seek": 477056, "start": 4779.8, "end": 4784.4800000000005, "text": " then light green is less common, and then light blue is less common, and then dark blue", "tokens": [50826, 550, 1442, 3092, 307, 1570, 2689, 11, 293, 550, 1442, 3344, 307, 1570, 2689, 11, 293, 550, 2877, 3344, 51060], "temperature": 0.0, "avg_logprob": -0.2874564102717808, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.0014325326774269342}, {"id": 1024, "seek": 477056, "start": 4784.4800000000005, "end": 4785.4800000000005, "text": " is zero.", "tokens": [51060, 307, 4018, 13, 51110], "temperature": 0.0, "avg_logprob": -0.2874564102717808, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.0014325326774269342}, {"id": 1025, "seek": 477056, "start": 4785.4800000000005, "end": 4790.4400000000005, "text": " So you can see the vast majority is zero, and there's a few with slightly bigger numbers,", "tokens": [51110, 407, 291, 393, 536, 264, 8369, 6286, 307, 4018, 11, 293, 456, 311, 257, 1326, 365, 4748, 3801, 3547, 11, 51358], "temperature": 0.0, "avg_logprob": -0.2874564102717808, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.0014325326774269342}, {"id": 1026, "seek": 477056, "start": 4790.4400000000005, "end": 4794.52, "text": " which is exactly the same that we saw for index 1 layer.", "tokens": [51358, 597, 307, 2293, 264, 912, 300, 321, 1866, 337, 8186, 502, 4583, 13, 51562], "temperature": 0.0, "avg_logprob": -0.2874564102717808, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.0014325326774269342}, {"id": 1027, "seek": 477056, "start": 4794.52, "end": 4797.0, "text": " Here it is, right.", "tokens": [51562, 1692, 309, 307, 11, 558, 13, 51686], "temperature": 0.0, "avg_logprob": -0.2874564102717808, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.0014325326774269342}, {"id": 1028, "seek": 479700, "start": 4797.0, "end": 4803.72, "text": " The average is pretty close to zero, the standard deviation is pretty small.", "tokens": [50364, 440, 4274, 307, 1238, 1998, 281, 4018, 11, 264, 3832, 25163, 307, 1238, 1359, 13, 50700], "temperature": 0.0, "avg_logprob": -0.24561682869406307, "compression_ratio": 1.5911949685534592, "no_speech_prob": 0.009267893619835377}, {"id": 1029, "seek": 479700, "start": 4803.72, "end": 4807.2, "text": " This is giving us more information, however.", "tokens": [50700, 639, 307, 2902, 505, 544, 1589, 11, 4461, 13, 50874], "temperature": 0.0, "avg_logprob": -0.24561682869406307, "compression_ratio": 1.5911949685534592, "no_speech_prob": 0.009267893619835377}, {"id": 1030, "seek": 479700, "start": 4807.2, "end": 4820.44, "text": " So as we train at this point here, the, at this point here, there is quite a few activations", "tokens": [50874, 407, 382, 321, 3847, 412, 341, 935, 510, 11, 264, 11, 412, 341, 935, 510, 11, 456, 307, 1596, 257, 1326, 2430, 763, 51536], "temperature": 0.0, "avg_logprob": -0.24561682869406307, "compression_ratio": 1.5911949685534592, "no_speech_prob": 0.009267893619835377}, {"id": 1031, "seek": 479700, "start": 4820.44, "end": 4823.44, "text": " that are a lot larger, as you can see.", "tokens": [51536, 300, 366, 257, 688, 4833, 11, 382, 291, 393, 536, 13, 51686], "temperature": 0.0, "avg_logprob": -0.24561682869406307, "compression_ratio": 1.5911949685534592, "no_speech_prob": 0.009267893619835377}, {"id": 1032, "seek": 482344, "start": 4823.44, "end": 4826.16, "text": " And still the vast majority of them are very small.", "tokens": [50364, 400, 920, 264, 8369, 6286, 295, 552, 366, 588, 1359, 13, 50500], "temperature": 0.0, "avg_logprob": -0.27143870459662545, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.07695627212524414}, {"id": 1033, "seek": 482344, "start": 4826.16, "end": 4830.679999999999, "text": " There's a few big ones, they've still got a bright yellow bar at the bottom.", "tokens": [50500, 821, 311, 257, 1326, 955, 2306, 11, 436, 600, 920, 658, 257, 4730, 5566, 2159, 412, 264, 2767, 13, 50726], "temperature": 0.0, "avg_logprob": -0.27143870459662545, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.07695627212524414}, {"id": 1034, "seek": 482344, "start": 4830.679999999999, "end": 4836.96, "text": " The other thing to notice here is what's happened is we've taken those, those stats, those histograms,", "tokens": [50726, 440, 661, 551, 281, 3449, 510, 307, 437, 311, 2011, 307, 321, 600, 2726, 729, 11, 729, 18152, 11, 729, 49816, 82, 11, 51040], "temperature": 0.0, "avg_logprob": -0.27143870459662545, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.07695627212524414}, {"id": 1035, "seek": 482344, "start": 4836.96, "end": 4841.4, "text": " we've stacked them all up into a single tensor, and then we've taken their log.", "tokens": [51040, 321, 600, 28867, 552, 439, 493, 666, 257, 2167, 40863, 11, 293, 550, 321, 600, 2726, 641, 3565, 13, 51262], "temperature": 0.0, "avg_logprob": -0.27143870459662545, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.07695627212524414}, {"id": 1036, "seek": 482344, "start": 4841.4, "end": 4845.0, "text": " Now log1p is just log of the number plus 1.", "tokens": [51262, 823, 3565, 16, 79, 307, 445, 3565, 295, 264, 1230, 1804, 502, 13, 51442], "temperature": 0.0, "avg_logprob": -0.27143870459662545, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.07695627212524414}, {"id": 1037, "seek": 482344, "start": 4845.0, "end": 4847.54, "text": " That's because we've got zeros here.", "tokens": [51442, 663, 311, 570, 321, 600, 658, 35193, 510, 13, 51569], "temperature": 0.0, "avg_logprob": -0.27143870459662545, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.07695627212524414}, {"id": 1038, "seek": 484754, "start": 4847.54, "end": 4855.82, "text": " And so just taking the log is going to kind of let us see the full range more clearly.", "tokens": [50364, 400, 370, 445, 1940, 264, 3565, 307, 516, 281, 733, 295, 718, 505, 536, 264, 1577, 3613, 544, 4448, 13, 50778], "temperature": 0.0, "avg_logprob": -0.2604404127741434, "compression_ratio": 1.6243386243386244, "no_speech_prob": 0.007460319437086582}, {"id": 1039, "seek": 484754, "start": 4855.82, "end": 4859.82, "text": " So that's what the log's for.", "tokens": [50778, 407, 300, 311, 437, 264, 3565, 311, 337, 13, 50978], "temperature": 0.0, "avg_logprob": -0.2604404127741434, "compression_ratio": 1.6243386243386244, "no_speech_prob": 0.007460319437086582}, {"id": 1040, "seek": 484754, "start": 4859.82, "end": 4866.38, "text": " So basically what we'd really ideally like to see here is that this whole thing should", "tokens": [50978, 407, 1936, 437, 321, 1116, 534, 22915, 411, 281, 536, 510, 307, 300, 341, 1379, 551, 820, 51306], "temperature": 0.0, "avg_logprob": -0.2604404127741434, "compression_ratio": 1.6243386243386244, "no_speech_prob": 0.007460319437086582}, {"id": 1041, "seek": 484754, "start": 4866.38, "end": 4870.86, "text": " be a kind of, more like a rectangle, you know.", "tokens": [51306, 312, 257, 733, 295, 11, 544, 411, 257, 21930, 11, 291, 458, 13, 51530], "temperature": 0.0, "avg_logprob": -0.2604404127741434, "compression_ratio": 1.6243386243386244, "no_speech_prob": 0.007460319437086582}, {"id": 1042, "seek": 484754, "start": 4870.86, "end": 4874.46, "text": " The maximum should be, should be not changing very much.", "tokens": [51530, 440, 6674, 820, 312, 11, 820, 312, 406, 4473, 588, 709, 13, 51710], "temperature": 0.0, "avg_logprob": -0.2604404127741434, "compression_ratio": 1.6243386243386244, "no_speech_prob": 0.007460319437086582}, {"id": 1043, "seek": 487446, "start": 4874.46, "end": 4877.74, "text": " There shouldn't be a thick yellow bar at the bottom, but instead it should be a nice", "tokens": [50364, 821, 4659, 380, 312, 257, 5060, 5566, 2159, 412, 264, 2767, 11, 457, 2602, 309, 820, 312, 257, 1481, 50528], "temperature": 0.0, "avg_logprob": -0.24474475258275083, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0009547269437462091}, {"id": 1044, "seek": 487446, "start": 4877.74, "end": 4882.26, "text": " even gradient, matching a normal distribution.", "tokens": [50528, 754, 16235, 11, 14324, 257, 2710, 7316, 13, 50754], "temperature": 0.0, "avg_logprob": -0.24474475258275083, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0009547269437462091}, {"id": 1045, "seek": 487446, "start": 4882.26, "end": 4886.14, "text": " Each single column of pixels wants to be kind of like a normal distribution.", "tokens": [50754, 6947, 2167, 7738, 295, 18668, 2738, 281, 312, 733, 295, 411, 257, 2710, 7316, 13, 50948], "temperature": 0.0, "avg_logprob": -0.24474475258275083, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0009547269437462091}, {"id": 1046, "seek": 487446, "start": 4886.14, "end": 4891.36, "text": " So you know, gradually decreasing the number of activations.", "tokens": [50948, 407, 291, 458, 11, 13145, 23223, 264, 1230, 295, 2430, 763, 13, 51209], "temperature": 0.0, "avg_logprob": -0.24474475258275083, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0009547269437462091}, {"id": 1047, "seek": 487446, "start": 4891.36, "end": 4895.86, "text": " That's what we're aiming for.", "tokens": [51209, 663, 311, 437, 321, 434, 20253, 337, 13, 51434], "temperature": 0.0, "avg_logprob": -0.24474475258275083, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0009547269437462091}, {"id": 1048, "seek": 487446, "start": 4895.86, "end": 4903.94, "text": " There's a, another really important and actually easier to read version of this, which is,", "tokens": [51434, 821, 311, 257, 11, 1071, 534, 1021, 293, 767, 3571, 281, 1401, 3037, 295, 341, 11, 597, 307, 11, 51838], "temperature": 0.0, "avg_logprob": -0.24474475258275083, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0009547269437462091}, {"id": 1049, "seek": 490394, "start": 4903.94, "end": 4910.679999999999, "text": " what if we just took those first two bottom pixels, so the least common 5%, and counted", "tokens": [50364, 437, 498, 321, 445, 1890, 729, 700, 732, 2767, 18668, 11, 370, 264, 1935, 2689, 1025, 8923, 293, 20150, 50701], "temperature": 0.0, "avg_logprob": -0.29668496294719415, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0016484742518514395}, {"id": 1050, "seek": 490394, "start": 4910.679999999999, "end": 4914.66, "text": " up how many were in, well it's not the, sorry, least common 5%.", "tokens": [50701, 493, 577, 867, 645, 294, 11, 731, 309, 311, 406, 264, 11, 2597, 11, 1935, 2689, 1025, 6856, 50900], "temperature": 0.0, "avg_logprob": -0.29668496294719415, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0016484742518514395}, {"id": 1051, "seek": 490394, "start": 4914.66, "end": 4920.099999999999, "text": " The least common, the, not least common either, let's try again.", "tokens": [50900, 440, 1935, 2689, 11, 264, 11, 406, 1935, 2689, 2139, 11, 718, 311, 853, 797, 13, 51172], "temperature": 0.0, "avg_logprob": -0.29668496294719415, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0016484742518514395}, {"id": 1052, "seek": 490394, "start": 4920.099999999999, "end": 4931.379999999999, "text": " In the bottom two pixels, we've got the smallest two equally sized groups of activations.", "tokens": [51172, 682, 264, 2767, 732, 18668, 11, 321, 600, 658, 264, 16998, 732, 12309, 20004, 3935, 295, 2430, 763, 13, 51736], "temperature": 0.0, "avg_logprob": -0.29668496294719415, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0016484742518514395}, {"id": 1053, "seek": 493138, "start": 4931.38, "end": 4935.82, "text": " We don't want there to be too many of them, because those are basically dead or nearly", "tokens": [50364, 492, 500, 380, 528, 456, 281, 312, 886, 867, 295, 552, 11, 570, 729, 366, 1936, 3116, 420, 6217, 50586], "temperature": 0.0, "avg_logprob": -0.2495210820978338, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0022872118279337883}, {"id": 1054, "seek": 493138, "start": 4935.82, "end": 4936.82, "text": " dead activations.", "tokens": [50586, 3116, 2430, 763, 13, 50636], "temperature": 0.0, "avg_logprob": -0.2495210820978338, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0022872118279337883}, {"id": 1055, "seek": 493138, "start": 4936.82, "end": 4941.1, "text": " They're much, much, much smaller than the big ones.", "tokens": [50636, 814, 434, 709, 11, 709, 11, 709, 4356, 813, 264, 955, 2306, 13, 50850], "temperature": 0.0, "avg_logprob": -0.2495210820978338, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0022872118279337883}, {"id": 1056, "seek": 493138, "start": 4941.1, "end": 4948.38, "text": " And so taking the ratio between those bottom two groups and the total basically tells us", "tokens": [50850, 400, 370, 1940, 264, 8509, 1296, 729, 2767, 732, 3935, 293, 264, 3217, 1936, 5112, 505, 51214], "temperature": 0.0, "avg_logprob": -0.2495210820978338, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0022872118279337883}, {"id": 1057, "seek": 493138, "start": 4948.38, "end": 4958.1, "text": " what percentage have zero or near zero or extremely small magnitudes.", "tokens": [51214, 437, 9668, 362, 4018, 420, 2651, 4018, 420, 4664, 1359, 4944, 16451, 13, 51700], "temperature": 0.0, "avg_logprob": -0.2495210820978338, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0022872118279337883}, {"id": 1058, "seek": 495810, "start": 4958.1, "end": 4964.54, "text": " And remember that these are with absolute values.", "tokens": [50364, 400, 1604, 300, 613, 366, 365, 8236, 4190, 13, 50686], "temperature": 0.0, "avg_logprob": -0.23994665706858914, "compression_ratio": 1.6237113402061856, "no_speech_prob": 0.0028895700816065073}, {"id": 1059, "seek": 495810, "start": 4964.54, "end": 4969.46, "text": " So if we plot those, you can see how bad this is.", "tokens": [50686, 407, 498, 321, 7542, 729, 11, 291, 393, 536, 577, 1578, 341, 307, 13, 50932], "temperature": 0.0, "avg_logprob": -0.23994665706858914, "compression_ratio": 1.6237113402061856, "no_speech_prob": 0.0028895700816065073}, {"id": 1060, "seek": 495810, "start": 4969.46, "end": 4973.54, "text": " And in particular, for example, at the final layer, from the, you know, nearly from the", "tokens": [50932, 400, 294, 1729, 11, 337, 1365, 11, 412, 264, 2572, 4583, 11, 490, 264, 11, 291, 458, 11, 6217, 490, 264, 51136], "temperature": 0.0, "avg_logprob": -0.23994665706858914, "compression_ratio": 1.6237113402061856, "no_speech_prob": 0.0028895700816065073}, {"id": 1061, "seek": 495810, "start": 4973.54, "end": 4983.700000000001, "text": " very start really, nearly all of the activations are, are entirely, just about entirely disabled.", "tokens": [51136, 588, 722, 534, 11, 6217, 439, 295, 264, 2430, 763, 366, 11, 366, 7696, 11, 445, 466, 7696, 15191, 13, 51644], "temperature": 0.0, "avg_logprob": -0.23994665706858914, "compression_ratio": 1.6237113402061856, "no_speech_prob": 0.0028895700816065073}, {"id": 1062, "seek": 495810, "start": 4983.700000000001, "end": 4986.42, "text": " So this is, this is bad news.", "tokens": [51644, 407, 341, 307, 11, 341, 307, 1578, 2583, 13, 51780], "temperature": 0.0, "avg_logprob": -0.23994665706858914, "compression_ratio": 1.6237113402061856, "no_speech_prob": 0.0028895700816065073}, {"id": 1063, "seek": 498642, "start": 4986.46, "end": 4991.58, "text": " And if you've got a model where most of your model is close to zero, then most of your", "tokens": [50366, 400, 498, 291, 600, 658, 257, 2316, 689, 881, 295, 428, 2316, 307, 1998, 281, 4018, 11, 550, 881, 295, 428, 50622], "temperature": 0.0, "avg_logprob": -0.23969603076423565, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.0011335667222738266}, {"id": 1064, "seek": 498642, "start": 4991.58, "end": 4993.74, "text": " model is doing no work.", "tokens": [50622, 2316, 307, 884, 572, 589, 13, 50730], "temperature": 0.0, "avg_logprob": -0.23969603076423565, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.0011335667222738266}, {"id": 1065, "seek": 498642, "start": 4993.74, "end": 5000.46, "text": " And so it's, it's really, it's really not working.", "tokens": [50730, 400, 370, 309, 311, 11, 309, 311, 534, 11, 309, 311, 534, 406, 1364, 13, 51066], "temperature": 0.0, "avg_logprob": -0.23969603076423565, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.0011335667222738266}, {"id": 1066, "seek": 498642, "start": 5000.46, "end": 5005.34, "text": " So it may look like at the very end things were improving, but as you can see from this", "tokens": [51066, 407, 309, 815, 574, 411, 412, 264, 588, 917, 721, 645, 11470, 11, 457, 382, 291, 393, 536, 490, 341, 51310], "temperature": 0.0, "avg_logprob": -0.23969603076423565, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.0011335667222738266}, {"id": 1067, "seek": 498642, "start": 5005.34, "end": 5008.14, "text": " chart, that's not true, right?", "tokens": [51310, 6927, 11, 300, 311, 406, 2074, 11, 558, 30, 51450], "temperature": 0.0, "avg_logprob": -0.23969603076423565, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.0011335667222738266}, {"id": 1068, "seek": 498642, "start": 5008.14, "end": 5012.02, "text": " There's still, the vast majority is still inactive.", "tokens": [51450, 821, 311, 920, 11, 264, 8369, 6286, 307, 920, 294, 12596, 13, 51644], "temperature": 0.0, "avg_logprob": -0.23969603076423565, "compression_ratio": 1.6354679802955665, "no_speech_prob": 0.0011335667222738266}, {"id": 1069, "seek": 501202, "start": 5012.02, "end": 5016.9400000000005, "text": " Generally speaking, I found that if early in training you see this rising crash, rising", "tokens": [50364, 21082, 4124, 11, 286, 1352, 300, 498, 2440, 294, 3097, 291, 536, 341, 11636, 8252, 11, 11636, 50610], "temperature": 0.0, "avg_logprob": -0.2598104132227151, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.09399784356355667}, {"id": 1070, "seek": 501202, "start": 5016.9400000000005, "end": 5022.740000000001, "text": " crash at all, you should stop and restart training.", "tokens": [50610, 8252, 412, 439, 11, 291, 820, 1590, 293, 21022, 3097, 13, 50900], "temperature": 0.0, "avg_logprob": -0.2598104132227151, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.09399784356355667}, {"id": 1071, "seek": 501202, "start": 5022.740000000001, "end": 5028.3, "text": " Because this, your model will probably never recover.", "tokens": [50900, 1436, 341, 11, 428, 2316, 486, 1391, 1128, 8114, 13, 51178], "temperature": 0.0, "avg_logprob": -0.2598104132227151, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.09399784356355667}, {"id": 1072, "seek": 501202, "start": 5028.3, "end": 5032.06, "text": " Too many of the activations have gone off the rails.", "tokens": [51178, 11395, 867, 295, 264, 2430, 763, 362, 2780, 766, 264, 27649, 13, 51366], "temperature": 0.0, "avg_logprob": -0.2598104132227151, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.09399784356355667}, {"id": 1073, "seek": 501202, "start": 5032.06, "end": 5038.34, "text": " So we want it to look kind of like this the whole time, but with less of this very thick", "tokens": [51366, 407, 321, 528, 309, 281, 574, 733, 295, 411, 341, 264, 1379, 565, 11, 457, 365, 1570, 295, 341, 588, 5060, 51680], "temperature": 0.0, "avg_logprob": -0.2598104132227151, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.09399784356355667}, {"id": 1074, "seek": 503834, "start": 5038.34, "end": 5044.58, "text": " yellow bar, which is showing us most are inactive.", "tokens": [50364, 5566, 2159, 11, 597, 307, 4099, 505, 881, 366, 294, 12596, 13, 50676], "temperature": 0.0, "avg_logprob": -0.30528684762807995, "compression_ratio": 1.3037037037037038, "no_speech_prob": 0.044677913188934326}, {"id": 1075, "seek": 503834, "start": 5044.58, "end": 5055.46, "text": " Okay, so that's our activations.", "tokens": [50676, 1033, 11, 370, 300, 311, 527, 2430, 763, 13, 51220], "temperature": 0.0, "avg_logprob": -0.30528684762807995, "compression_ratio": 1.3037037037037038, "no_speech_prob": 0.044677913188934326}, {"id": 1076, "seek": 503834, "start": 5055.46, "end": 5068.26, "text": " So we've got really now all of the kind of key pieces I think we need to be able to flexibly", "tokens": [51220, 407, 321, 600, 658, 534, 586, 439, 295, 264, 733, 295, 2141, 3755, 286, 519, 321, 643, 281, 312, 1075, 281, 5896, 3545, 51860], "temperature": 0.0, "avg_logprob": -0.30528684762807995, "compression_ratio": 1.3037037037037038, "no_speech_prob": 0.044677913188934326}, {"id": 1077, "seek": 506826, "start": 5068.38, "end": 5074.54, "text": " change how we train models, and to understand what's going on inside our models.", "tokens": [50370, 1319, 577, 321, 3847, 5245, 11, 293, 281, 1223, 437, 311, 516, 322, 1854, 527, 5245, 13, 50678], "temperature": 0.0, "avg_logprob": -0.25847641078905126, "compression_ratio": 1.6113744075829384, "no_speech_prob": 0.00019411291577853262}, {"id": 1078, "seek": 506826, "start": 5074.54, "end": 5081.5, "text": " And so from this point, we've kind of like drilled down as deep as we need to go.", "tokens": [50678, 400, 370, 490, 341, 935, 11, 321, 600, 733, 295, 411, 38210, 760, 382, 2452, 382, 321, 643, 281, 352, 13, 51026], "temperature": 0.0, "avg_logprob": -0.25847641078905126, "compression_ratio": 1.6113744075829384, "no_speech_prob": 0.00019411291577853262}, {"id": 1079, "seek": 506826, "start": 5081.5, "end": 5089.18, "text": " And we can now start to come back up again, and, and put together the pieces, building", "tokens": [51026, 400, 321, 393, 586, 722, 281, 808, 646, 493, 797, 11, 293, 11, 293, 829, 1214, 264, 3755, 11, 2390, 51410], "temperature": 0.0, "avg_logprob": -0.25847641078905126, "compression_ratio": 1.6113744075829384, "no_speech_prob": 0.00019411291577853262}, {"id": 1080, "seek": 506826, "start": 5089.18, "end": 5097.26, "text": " up what are all of the things that are going to help us train models reliably and quickly.", "tokens": [51410, 493, 437, 366, 439, 295, 264, 721, 300, 366, 516, 281, 854, 505, 3847, 5245, 49927, 293, 2661, 13, 51814], "temperature": 0.0, "avg_logprob": -0.25847641078905126, "compression_ratio": 1.6113744075829384, "no_speech_prob": 0.00019411291577853262}, {"id": 1081, "seek": 509726, "start": 5097.26, "end": 5100.74, "text": " And then hopefully we're going to be able to, yeah, successfully create from scratch", "tokens": [50364, 400, 550, 4696, 321, 434, 516, 281, 312, 1075, 281, 11, 1338, 11, 10727, 1884, 490, 8459, 50538], "temperature": 0.0, "avg_logprob": -0.2826762051926446, "compression_ratio": 1.6175298804780875, "no_speech_prob": 0.00208294834010303}, {"id": 1082, "seek": 509726, "start": 5100.74, "end": 5107.66, "text": " some really high quality generative models and other models along the way.", "tokens": [50538, 512, 534, 1090, 3125, 1337, 1166, 5245, 293, 661, 5245, 2051, 264, 636, 13, 50884], "temperature": 0.0, "avg_logprob": -0.2826762051926446, "compression_ratio": 1.6175298804780875, "no_speech_prob": 0.00208294834010303}, {"id": 1083, "seek": 509726, "start": 5107.66, "end": 5112.02, "text": " Okay, I think that's everything for this class.", "tokens": [50884, 1033, 11, 286, 519, 300, 311, 1203, 337, 341, 1508, 13, 51102], "temperature": 0.0, "avg_logprob": -0.2826762051926446, "compression_ratio": 1.6175298804780875, "no_speech_prob": 0.00208294834010303}, {"id": 1084, "seek": 509726, "start": 5112.02, "end": 5115.58, "text": " But next class we're going to start looking at things like initialization.", "tokens": [51102, 583, 958, 1508, 321, 434, 516, 281, 722, 1237, 412, 721, 411, 5883, 2144, 13, 51280], "temperature": 0.0, "avg_logprob": -0.2826762051926446, "compression_ratio": 1.6175298804780875, "no_speech_prob": 0.00208294834010303}, {"id": 1085, "seek": 509726, "start": 5115.58, "end": 5120.26, "text": " It's a really important topic.", "tokens": [51280, 467, 311, 257, 534, 1021, 4829, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2826762051926446, "compression_ratio": 1.6175298804780875, "no_speech_prob": 0.00208294834010303}, {"id": 1086, "seek": 509726, "start": 5120.26, "end": 5123.42, "text": " If you want to do from the revision before, then just make sure that you're very comfortable", "tokens": [51514, 759, 291, 528, 281, 360, 490, 264, 34218, 949, 11, 550, 445, 652, 988, 300, 291, 434, 588, 4619, 51672], "temperature": 0.0, "avg_logprob": -0.2826762051926446, "compression_ratio": 1.6175298804780875, "no_speech_prob": 0.00208294834010303}, {"id": 1087, "seek": 512342, "start": 5123.42, "end": 5128.38, "text": " with things like standard deviations and other stuff like that, because we'll be using", "tokens": [50364, 365, 721, 411, 3832, 31219, 763, 293, 661, 1507, 411, 300, 11, 570, 321, 603, 312, 1228, 50612], "temperature": 0.0, "avg_logprob": -0.3033927917480469, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.49596312642097473}, {"id": 1088, "seek": 512342, "start": 5128.38, "end": 5132.7, "text": " that quite a lot for next time.", "tokens": [50612, 300, 1596, 257, 688, 337, 958, 565, 13, 50828], "temperature": 0.0, "avg_logprob": -0.3033927917480469, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.49596312642097473}, {"id": 1089, "seek": 512342, "start": 5132.7, "end": 5134.58, "text": " And yeah, thanks for joining me.", "tokens": [50828, 400, 1338, 11, 3231, 337, 5549, 385, 13, 50922], "temperature": 0.0, "avg_logprob": -0.3033927917480469, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.49596312642097473}, {"id": 1090, "seek": 512342, "start": 5134.58, "end": 5136.42, "text": " Look forward to the next lesson.", "tokens": [50922, 2053, 2128, 281, 264, 958, 6898, 13, 51014], "temperature": 0.0, "avg_logprob": -0.3033927917480469, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.49596312642097473}, {"id": 1091, "seek": 512342, "start": 5136.42, "end": 5136.86, "text": " See you gang!", "tokens": [51014, 3008, 291, 10145, 0, 51036], "temperature": 0.0, "avg_logprob": -0.3033927917480469, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.49596312642097473}], "language": "en"}