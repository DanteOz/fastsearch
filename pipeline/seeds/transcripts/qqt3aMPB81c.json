{"text": " Welcome to Lesson 4. We are going to finish our journey through these key applications. We've already looked at a range of vision applications. We've looked at classification, localization, image regression. We've briefly touched on NLP. We're going to do a deeper dive into NLP transfer learning today. We're going to then look at tabular data and we're going to look at collaborative filtering, which are both super useful applications. And then we're going to take a complete U-turn. We're going to take that collaborative filtering example and dive deeply into it to understand exactly what's happening mathematically, exactly what's happening in the computer. And we're going to use that to gradually go back in reverse order through the applications again in order to understand exactly what's going on behind the scenes of all of those applications. Before we do, somebody on the forum was kind enough to point out that when we compared ourselves to what we think might be the state of the art or was recently the state of the art for CanVood, it wasn't a fair comparison because the paper actually used a small subset of the classes and we used all of the classes. So Jason in our study group was kind enough to rerun the experiments with the correct subset of classes from the paper and our accuracy went up to 94% compared to 91.5% in the paper. So I think that's a really cool result and a great example of how some pretty much just using the defaults nowadays can get you far beyond what was the best of a year or two ago. And certainly the best last year when we were doing this course because we started it quite intensely. So that's really exciting. So what I wanted to start with is going back over NLP a little bit to understand really what was going on there. So first of all, a quick review. So remember NLP is natural language processing, it's about taking text and doing something with it. And text classification is a particularly useful, practically useful application. It's what we're going to start off focusing on. Because classifying a text, classifying a document can be used for anything from spam prevention to identifying fake news to finding a diagnosis for medical reports, finding mentions of your product in Twitter, so on and so forth. So it's pretty interesting. And actually there was a great example during the week from one of our students who is a lawyer and he mentioned on the forum that he had really great results from classifying legal texts using this NLP approach. And I thought this was a great example. So this is the poster that they presented at an academic conference this week describing the approach. And actually this series of three steps that you see here, and I'm sure you recognize this classification matrix, this series of three steps here is what we're going to start by digging into. So we're going to start out with a movie review like this one and going to decide whether it's positive or negative sentiment about the movie. That is the problem. We have in the training set 25,000 movie reviews. So we've got 25,000 movie reviews and for each one we have like one bit of information. They liked it or they didn't like it. And as we're going to look into a lot more detail today and in the current lessons, our neural networks, remember they're just a bunch of matrix multiplies and simple nonlinearities, particularly replacing negatives with zeros. Those weight matrices start out random. And so if you start out with some random parameters and try to train those parameters to learn how to recognize positive versus negative movie reviews, you only have literally 25,000 ones and zeros to actually tell you I like this one, I don't like that one. That's clearly not enough information to learn basically how to speak English, how to speak English well enough to recognize they liked this or they didn't like this. And sometimes that can be pretty nuanced, right? The English language often particularly with like movie reviews, people, because these are like online movie reviews on IMDB, people can often like use sarcasm, it can be really quite tricky. So for a long time, in fact until very recently like this year, neural nets didn't do a good job at all of this kind of classification problem. And that was why there's not enough information available. So the trick, hopefully you can all guess, it's to use transfer learning. It's always the trick. So last year in this course, I tried something crazy, which was I thought, what if I try transfer learning to demonstrate that it can work for an LP as well? And I tried it out and it worked extraordinarily well. And so here we are a year later and transfer learning in NLP is absolutely the hit thing now. And so I'm going to describe to you what happens. The key thing is we're going to start with the same kind of thing that we used for computer vision, a pre-trained model that's been trained to do something different to what we're doing with it. And so for ImageNet, that was originally built as a model to predict which of a thousand categories each photo falls into. And people then fine-tuned that for all kinds of different things as you've seen. So we're going to start with a pre-trained model that's going to do something else, not movie review classification. We're going to start with a pre-trained model which is called a language model. A language model has a very specific meaning in NLP and it's this. A language model is a model that learns to predict the next word of a sentence. And to predict the next word of a sentence you actually have to know quite a lot about English, assuming you're doing it in English, and quite a lot of world knowledge. By world knowledge I'll give you an example. Here's your language model and it's read, I'd like to eat a hot what? Obviously dog. It was a hot what? Probably day. Now previous approaches to NLP use something called N-grams largely, which is basically saying how often do these pairs or triplets of words tend to appear next to each other. And N-grams are terrible at this kind of thing. As you can see there's not enough information here to decide what the next word probably is. But with a neural net you absolutely can. So here's the nice thing. If you train a neural net to predict the next word of a sentence, then you actually have a lot of information. Rather than having a single bit for every 2000 word movie review, liked it or didn't like it, every single word you can try and predict the next word. So in a 2000 word movie review there are 1999 opportunities to predict the next word. Better still, you don't just have to look at movie reviews. Because really the hard thing isn't so much does this person like the movie or not, but how do you speak English? So you can learn how do you speak English, roughly, from some much bigger set of documents. And so what we did was we started with Wikipedia. And Stephen Meridy and some of his colleagues built something called the WikiText 103 data set, which is simply a subset of most of the largest articles from Wikipedia with a little bit of pre-processing that's available for download. And so you're basically grabbing Wikipedia and then I built a language model on all of Wikipedia. So I've just built a neural net which would predict the next word in every significantly sized Wikipedia article. And that's a lot of information. If I remember correctly, it's something like a billion tokens. So we've got a billion separate things to predict. Every time we make a mistake on one of those predictions, we get the loss, we get gradients from that, we can update our weights and make them better and better until we can get pretty good at predicting the next word of Wikipedia. Why is that useful? Because at that point I've got a model that knows probably how to complete sentences like this and so it knows quite a lot about English and quite a lot about how the world works, what kinds of things tend to be hot in different situations, for instance. I mean ideally it would learn things like in 1996 in a speech to the United Nations, United States President, blah, said, now that would be a really good language model because it would actually have to know who was the United States President in that year. So getting really good at training language models is a great way to learn a lot about or teach a neural net a lot about what is our world, what's in our world, how do things work in our world. So it's a really fascinating topic and it's actually one that philosophers have been studying for hundreds of years now. There's actually a whole theory of philosophy which is about what can be learned from studying language alone. So it turns out empirically quite a lot. And so here's the interesting thing, you can start by training a language model on all of Wikipedia and then we can make that available to all of you. Just like a pre-trained ImageNet model for vision, we've now made available a pre-trained Wiki text model for NLP. Not because it's particularly useful of itself, predicting the next word of sentences is somewhat useful but not normally what we want to do, but it tells us it's a model that understands a lot about language and a lot about what language describes. So then we can take that and we can do transfer learning to create a new language model that's specifically good at predicting the next word of movie reviews. So if we can build a language model that's good at predicting the next word of movie reviews pre-trained with the Wiki text model, then that's going to understand a lot about my favorite actor is Tom Who. I thought the photography was fantastic but I wasn't really so happy about the director. It's going to learn a lot about specifically how movie reviews are written. It'll even learn things like what are the names of some popular movies. So that would then mean we can still use a huge corpus of lots of movie reviews even if we don't know whether they're positive or negative to learn a lot about how movie reviews are written. So for all of this pre-training and all of this language model fine-tuning, we don't need any labels at all. It's what the researcher Jan LeCun calls self-supervised learning. In other words, it's a classic supervised model. We have labels, but the labels are not things that somebody else has created. They're kind of built into the data set itself. So this is really, really neat because at this point we've now got something that's good at understanding movie reviews and we can fine-tune that with transfer learning to do the thing we want to do, which in this case is to classify movie reviews to be positive or negative. And so my hope was when I tried this last year that at that point 25,000 ones and zeros would be enough feedback to fine-tune that model. And it turned out it absolutely was. Alright, Rachel, let's go with a question. Does the language model approach work for text in forums that are informal English, misspelled words or slang or short form like S6 instead of Samsung S6? Yes, absolutely it does. Particularly if you start with your WikiText model and then fine-tune it with your, we call it the target corpus. A corpus is just a bunch of documents. It could be emails or tweets or medical reports or whatever. So you could fine-tune it so it can learn a bit about the specifics of the slang or abbreviations or whatever that didn't appear in the full corpus. And so interestingly, this is one of the big things that people were surprised about when we did this research last year. People thought that learning from something like Wikipedia wouldn't be that helpful because it's not that representative of how people tend to write. But it turns out it's extremely helpful because there's a much bigger difference between Wikipedia and random words than there is between Wikipedia and Reddit. So it kind of gets you 99% of the way there. So these language models themselves can be quite powerful. So for example there was a blog post from, what are they called, SwiftKey? The folks that do the mobile phone predictive text keyboard and they described how they kind of rewrote their underlying model to use neural nets. So this was a year or two ago, now most phone keyboards seem to do this. You'll be typing away on your mobile phone and in the predictions there'll be something telling you what word you might want next. So that's a language model in your phone. Another example was the researcher Andre Kapathy who now runs all this stuff at Tesla. Back when he was a PhD student, he created a language model of text in LaTeX documents and created these automatic generation of LaTeX documents that then became these kind of automatically generated papers. So that's pretty cute. So we're not really that interested in the output of the language model ourselves, we're just interested in it because it's helpful with this process. So we briefly looked at the process last week. So let's just have a reminder. The basic process is we're going to start with the data in some format. So for example we've prepared a little IMDB sample that you can use where it's in CSV file so you can read it in with pandas and see there's negative or positive, the text of each movie review, and a boolean of is it in the validation set or the training set. So there's an example of a movie review. And so you can just go text data bunch from CSV to grab a language model specific data bunch and then you can create a learner from that in the usual way and fit it. You can save the data bunch which means that the pre-processing that is done, you don't have to do it again, you can just load it. So what goes on behind the scenes? Well what happens behind the scenes if we now load it as a classification data bunch, that's going to allow us to see the labels as well, then as we described it basically creates a separate unit, we call it a token, for each separate part of a word. So most of them are just four words but sometimes if it's like an apostrophe s from its it'll get its own token, every bit of punctuation tends to get its own token like a comma or a full stop and so forth. And then the next thing that we do is a numericalization which is where we find what are all of the unique tokens that appear here and we create a big list of them. Here's the first 10 in order of frequency and that big list of unique possible tokens is called the vocabulary, we always call it vocab. And so what we then do is we replace the tokens with the ID of where is that token in the vocab and that's numericalization. Here's the thing though, as you'll learn every word in our vocab is going to require a separate row in a weight matrix in our neural net. And so to avoid that weight matrix getting too huge, we restrict the vocab to no more than by default 60,000 words and if a word doesn't appear more than two times, we don't put it in the vocab either. So we kind of keep the vocab to a reasonable size in that way. And so when you see these xx-unk, that's an unknown token. So when you see those unknown tokens, it just means this was something that was not a common enough word to appear in our vocab. Okay, so there is the numericalized version. We also have a couple of other special tokens like xx-field. This is a special thing where if you've got like title, summary, abstract, body, like separate parts of a document, each one will get a separate field. So they all get numbered. Also you'll find if there's something in all caps, it gets lower cased in a token called xx-cap will get added to it. Personally, I more often use the data block API because you get, you kind of, there's less to remember about exactly what data bunch to use and what parameters and so forth and it can be a bit more flexible. So another approach to doing this is to just decide what kind of list you're creating. So what's your independent variable? So in this case my independent variable is text. What is it coming from? A CSV. How do you want to split it into validation versus training? So in this case column number 2 was the is validation flag. How do you want to label it with positive or negative sentiment, for example? So column 0 had that and then turn that into a data bunch. That's going to do the same thing. Okay, so now let's grab the whole data set which has 25,000 reviews in training, 25,000 reviews in validation and then 50,000 what they call unsupervised movie reviews. So 50,000 movie reviews that haven't been scored at all. So there it is, positive, negative, unsupervised. So we're going to start as we described with the language model. Now the good news is we don't have to train the Wikitext 103 language model. Not that it's difficult, you can use exactly the same steps that you see here, just download the Wikitext 103 corpus and run the same code. But it takes 2 or 3 days on a decent GPU, so not much point you doing it. You may as well start with hours. Even if you've got a big corpus of medical documents or legal documents, you should still start with Wikitext 103. There's just no reason to start with random weights. It's always good to use transfer learning if you can. So we're going to start then at this point which is fine-tuning our IMDB language model. So we can say, okay, it's a list of text files and the full IMDB actually is not in a CSV. Each document is a separate text file, so that's why we use a different constructor for our independent variable, text files list, say where it is. And in this case we have to make sure we just don't include the train and test folders. And we randomly split it by 0.1. Now this is interesting, 10%. Why are we randomly splitting it by 10% rather than using the predefined train and test they gave us? This is one of the cool things about transfer learning. Even though our test set or validation set has to be held aside, it's actually only the labels that we have to keep aside. So we're not allowed to use the labels in the test set. So if you think about something like a Kaggle competition, you certainly can't use the labels because they don't even give them to you. But you can certainly use the independent variables. So in this case you can absolutely use the text that is in the test set to train your language model. So this is a good trick, right? Because actually when you do the language model, concatenate the training and test set together and then just split out a smaller validation set. So you've got more data to train your language model. So that's a little trick. And so if you're doing NLP stuff on Kaggle, for example, or you've just got a smaller subset of labelled data, make sure that you use all of the text you have to train your language model because there's no reason not to. How are we going to label it? Well remember a language model kind of has its own labels. So the text itself is a label. So label for language model does that for us. And create a data bunch and save it. And that takes a few minutes to tokenize and numericalize. So since that takes a few minutes, we save it. Later on you can just load it. No need to run that again. So here's what it looks like. And at this point, things are going to look very familiar. We create a learner. But instead of creating a CNN learner, we're going to create a language model learner. So behind the scenes, this is actually not going to create a CNN, a convolutional neural network. It's going to create an RNN, a recurrent neural network. So we're going to be learning exactly how they're built over the coming lessons. But in short, they're the same basic structure. The input goes into a weight matrix, a matrix multiply. Then you replace the negatives with zeros, and it goes into another matrix multiply, and so forth a bunch of times. So it's the same basic structure. So as usual, when we create a learner, you have to pass in two things. The data. So here's our language model data. And in this case, what pre-trained model we want to use. And so here, the pre-trained model is the Wikitext 103 model. That will be downloaded for you from Fast.ai if you haven't used it before, just like the same thing with things like ImageNet pre-trained models are downloaded for you. This here sets the amount of dropout. We haven't talked about that yet. We've talked briefly about this idea that there's something called regularization, and you can reduce the regularization to avoid underfitting. So for now, just know that by using a number lower than one is because when I first tried to run this, I was underfitting. And so if you reduce that number, then it will avoid underfitting. Okay, so we've got a learner. We can LR find. It looks pretty standard. And so then we can fit one cycle. And so what's happening here is we are just fine-tuning the last layers. So normally after we fine-tuned the last layers, the next thing we do is we go unfreeze and train the whole thing. And so here it is. Unfreeze and train the whole thing. And as you can see, even on a pretty beefy GPU, that takes 2 or 3 hours. And in fact, I'm still underfitting. Probably tonight I might train it overnight and try to do a little bit better. Because you can see, I guess I'm not underfitting. I'm guessing I could probably train this a bit longer because you can see the accuracy hasn't started going down again. So I wouldn't mind trying to train that a bit longer. But the accuracy, it's interesting. Point 3 means we're guessing the next word of the movie review correctly about a third of the time. That sounds like a pretty high number, the idea that you can actually guess the next word that often. So that's a good sign that my language model is doing pretty well. For more limited domain documents like medical transcripts and legal transcripts, you'll often find this accuracy gets a lot higher. So sometimes this can be even 50% or more. But point 3 or more is pretty good. So you can now run learn.predict and pass in the start of a sentence and it will try and finish off that sentence for you. Now I should mention this is not designed to be a good text generation system. This is really more designed to kind of check that it seems to be creating something that's vaguely sensible. There's a lot of tricks that you can use to generate much higher quality text, none of which we're using here. But you can kind of see that it's certainly not random words that it's generating, it sounds vaguely English-like, even though it doesn't make any sense. So at this point we have a movie review model. So now we're going to save that in order to load it into our classifier to be our pre-trained model for the classifier. But I actually don't want to save the whole thing. A lot of this, kind of the second half, as we'll learn, the second half of the language model is all about predicting the next word rather than about understanding the sentence so far. So the bit which is specifically about understanding the sentence so far is called the encoder. So I just saved that. So again we're going to learn the details of this over the coming weeks. We're just going to save the encoder so the bit that understands the sentence rather than the bit that generates the word. So now we're ready to create our classifier. So step one, as per usual, is to create a data bunch and we're going to do basically exactly the same thing, bring it in, okay, and here's our path, but we want to make sure that it uses exactly the same vocab that it used for the language model. If word number 10 was the in the language model, we need to make sure that word number 10 is the in the classifier because otherwise the pre-trained model is going to be totally meaningless. So that's why we pass in the vocab from the language model, to make sure that this data bunch is going to have exactly the same vocab. That's an important step. Split by folder, and this time label. So remember the last time we had split randomly, but this time we need to make sure that the labels of the test set are not touched, so we split by folder. And then this time we label it not for a language model, but we label these classes. And then finally create a data bunch. And remember sometimes you'll find that you run out of GPU memory. This will very often happen to you if you, so I was running this in an 11 gig machine, so you should make sure this number is a bit lower if you run out of memory. You may also want to make sure you restart the notebook and kind of start it just from here. So batch size 50 is as high as I could get on an 11 gig card. If you're using a P2 or P3 on Amazon or the K80 on Google for example, I think you'll get 16 gigs, so you might be able to make this a bit higher, get it up to 64. So you can find whatever batch size fits on your card. So here's our data bunch as we saw before and the labels. So this time rather than creating a language model learner, we're creating a text classifier learner. But again, same thing, pass in the data that we want, figure out how much regularization we need. Again, if you're overfitting, then you can increase this number. If you're underfitting, you can decrease the number. And most importantly, load in our pre-trained model. And remember specifically, it's this half of the model called the encoder, which is the bit that we want to load in. And freeze. LR find, find the learning rate and fit for a little bit. And we're already up nearly to 92% accuracy after less than 3 minutes of training. So this is a nice thing. In your particular domain, whether it be law or medicine or journalism or government or whatever, you probably only need to train your domain's language model once. And that might take overnight to train well. But once you've got it, you can now very quickly create all kinds of different classifiers and models with that. In this case, already a pretty good model after 3 minutes. So when you first start doing this, you might find it a bit annoying that your first models take 4 hours or more to create that language model. But the key thing to remember is you only have to do that once for your entire domain of stuff that you're interested in. And then you can build lots of different classifiers and other models on top of that in a few minutes. So we can save that to make sure we don't have to run it again. And then here's something interesting. I'm going to explain this more in just a few minutes. I'm not going to say unfreeze. Instead I'm going to say freeze2. And what that says is unfreeze the last 2 layers. Don't unfreeze the whole thing. And so we've just found it really helps with these text classification, not to unfreeze the whole thing, but to unfreeze one layer at a time. So unfreeze the last 2 layers, train it a little bit more, unfreeze the next layer again, train it a little bit more, unfreeze the whole thing, train it a little bit more. You'll also see I'm passing in this thing, momentum, equals.8.7. We're going to learn exactly what that means in the next week or two, probably next week. But for now, and we may even automate it, so maybe by the time you watch the video of this, this won't even be necessary anymore. Basically we've found for training recurrent neural networks, RNNs, it really helps to decrease the momentum a little bit. So that's what that is. So that gets us a 94.4 accuracy after about half an hour or less of training, actually quite a lot less of training the actual classifier. And we can actually get this quite a bit better with a few tricks. I don't know if we'll learn all the tricks this part, it might be the next part, but even this very simple kind of standard approach is pretty great. If we compare it to last year's state of the art on IMDB, this is from the Cove paper from McCann et al. at Salesforce Research. Their paper was 91.8% accurate, and the best paper they could find, they found a fairly domain specific sentiment analysis paper from 2017 that got 94.1, and here we've got 94.4. And the best models I've been able to build since have been about 95, 95.1. So if you're looking to do text classification, this really standardized transfer learning approach works super well. Any questions, Rachel? So that was NLP, and we'll be learning more about NLP later in this course. Now I wanted to switch over and look at Tabular. Now Tabular data is pretty interesting because it's the stuff that for a lot of you is actually what you use day to day at work in spreadsheets and relational databases. So where does the magic number of 2.6 to the 4th in the learning rate come from? Yeah, good question. So the learning rate is various things divided by 2.6 to the 4th. The reason it's to the 4th, you will learn about at the end of today. So let's focus on the 2.6. Why 2.6? Basically as we're going to see in more detail later today, the difference between the bottom of the slice and the top of the slice is basically what's the difference between how quickly the lowest layer of the model learns versus the highest layer of the model learns. So this is called discriminative learning rates. And so really the question is, as you go from layer to layer, how much do I decrease the learning rate by? And we found out that for NLP RNNs that the answer is 2.6. How do we find out that it's 2.6? I ran lots and lots of different models like a year ago or so using lots of different sets of hyperparameters of various types, dropout, learning rates, discriminative learning rate and so forth. And then I created something called a random forest, which is a kind of model where I attempted to predict how accurate my NLP classifier would be based on the hyperparameters. And then I used random forest interpretation methods to basically figure out what the optimal parameter settings were and I found out that the answer for this number was 2.6. So that's actually not something I've published or I don't think I've even talked about it before so there's a new piece of information. You can actually, a few months after I did this, I think Stephen Marody and somebody else did publish a paper describing a similar approach so the basic idea may be out there already. Some of that idea comes from a researcher named Frank Hutter and one of his collaborators. They did some interesting work showing how you can use random forests to actually find optimal hyperparameters so it's kind of a neat trick. A lot of people are very interested in this thing called AutoML, which is this idea of like building models to figure out how to train your model. We're not big fans of it on the whole but we do find that building models to better understand how your hyperparameters work and then finding those rules of thumb like, oh basically it can always be 2.6, quite helpful. So that's just something we've kind of been playing with. So let's talk about tabular data. So tabular data, such as you might see in a spreadsheet or a relational database or a financial report, it can contain all kinds of different things. It can contain all kinds of different things and I kind of tried to make a little list of some of the kinds of things that I've seen tabular data analysis used for. Using neural nets for analyzing tabular data is, or at least last year when I first presented this was, maybe we started this two years ago. When we first presented this, people were deeply skeptical and they thought it was a terrible idea to use neural nets to analyze tabular data because everybody knows that you should use logistic regression or random forests or gradient boosting machines, all of which have their place for certain types of things. But since that time, it's become clear that the commonly held wisdom is wrong. It's not true that neural nets are not useful for tabular data. In fact, they're extremely useful. We've shown this in quite a few of our courses but what's really kind of also helped is that some really effective organizations have started publishing papers and posts and stuff describing how they've been using neural nets for analyzing tabular data. One of the key things that comes up again and again is that although feature engineering doesn't go away, it certainly becomes simpler. So Pinterest, for example, replaced the gradient boosting machines that they were using to decide how to put stuff on their homepage with neural nets. They presented at a conference this approach and they described how it really made engineering a lot easier because a lot of the hand-created features weren't necessary anymore. You still need some, but it was just simpler. So they ended up with something that was more accurate but perhaps even more importantly, it required less maintenance. So I wouldn't say it's the only tool that you need in your toolbox for analyzing tabular data, but where else? I used to use random forests 99% of the time when I was doing machine learning with tabular data. I now use neural nets 90% of the time. It's kind of my standard first go-to approach now and it tends to be pretty reliable, pretty effective. One of the things that's made it difficult is that until now there hasn't been an easy way to kind of create and train tabular neural nets like nobody's really made it available on a library. So we've actually just created fastai.tabular and I think this is pretty much the first time that it's become really easy to use neural nets with tabular data. So let me show you how easy it is. This is actually coming directly from the examples folder in the fastai repo. I haven't changed it at all. And as per usual as well as importing fastai, you should import your application. So in this case it's tabular. We assume that your data is in a pandas data frame. A pandas data frame is kind of the standard format for tabular data in Python and it's lots of ways to get it in there but probably the most common might be pd.readcsv. But whatever your data is in, you can probably get it into a pandas data frame easily enough. Question, what are the 10% of cases where you would not default to neural nets? Good question. I guess I still tend to kind of give them a try. As you do things for a while, you start to get a sense of the areas where things don't quite work as well. I have to think about that during the week. I don't think I have a rule of thumb. But I would say you may as well try both. I would say try a random forest and try a neural net. They're both pretty quick and easy to run and see how it looks. And if they're roughly similar, I might try and dig into each and see if I can make them better and better. But if the random forest is doing way better, I'd probably just stick with that and use whatever works. So I currently have the wrong notebook in the lesson repo. So I'll update it after the class. So sorry about that. So we start with the data in a data frame. And so we've got a little thing, the adult sample. It's a classic old data set. I have to dig up the citation for it because I forgot to put it in this notebook. It's a pretty small, simple old data set that's good for experimenting with basically. And it's a CSV file, so you can read it into a data frame with pandas.read.csv. If your data is in a relational database, pandas can read from that. If it's in Spark or Hadoop, pandas can read from that. Pandas can read from most stuff that you can throw at it. So that's why we kind of use it as a default starting point. And as per usual, I think it's nice to use the data block API. And so in this case, the list that we're trying to create is a tabular list. And we're going to create it from a data frame. And so you can tell it what the data frame is and what the path that you're going to use to kind of save models and intermediate steps is. And then you need to tell it, what are your categorical variables and what are your continuous variables? So we're going to be learning a lot more about what that means to the neural net next week. But for now, the quick summary is this. Your independent variables are the things that you're using to make predictions with. So things like education and marital status and age and so forth. Some of those variables like age are basically numbers. They could be any number. You could be 13.36 years old or 19.4 years old or whatever. Whereas things like marital status are options that can be selected from a discrete group \ufffd married, single, divorced, whatever. Sometimes those options might be quite a lot more, like occupation. There's a lot of possible occupations. And sometimes they might be binary. Could be just true or false. But anything which you can select the answer from a small group of possibilities is called a categorical variable. And so we're going to need to use a different approach to the neural net to modeling categorical variables to what we use for continuous variables. For categorical variables, we're going to be using something called embeddings, which we'll be learning about later today. For continuous variables, they can just be sent into the neural net just like pixels in a neural net can, because pixels in a neural net are already numbers. These continuous things are already numbers as well. So that's easy. So that's why you have to tell the tabular list from data frame which ones are which. There are some other ways to do that by preprocessing them in pandas to make things categorical variables, but it's kind of nice to have one API for doing everything. You don't have to think too much about it. Then we've got something which is a lot like transforms in computer vision. Transforms in computer vision do things like flip a photo when it's axis or turn it a bit or brighten it or normalize it. But for tabular data, instead of having transforms, we have things called processes. And they're nearly identical, but the key difference, which is quite important, is that a processor is something that happens ahead of time. So we basically preprocess the data frame rather than doing it as we go. So transformations are really for data augmentation where you want to randomize it and do it differently each time, whereas processes are things that you want to do once ahead of time. So we have a number of processes in the Fast.ai library. The ones we're going to use this time are fill missing. So that's going to look for missing values and deal with them some way. We're going to find categorical variables and turn them into pandas categories. And we're going to do normalization ahead of time, which is to take continuous variables and subtract their mean and divide it by their standard deviation, so they're 0, 1 variables. The way we deal with missing data, we'll talk more about next week, but in short, we replace it with a median and add a new column, which is a binary column, saying whether that was missing or not. Normalization, there's an important thing here, which is in fact for all of these things, whatever you do to the training set, you need to do exactly the same thing to the validation set and the test set. So whatever you replaced your missing values with, you need to replace them with exactly the same thing in the validation set. So Fast.ai handles all these details for you. There are kinds of things that if you have to do it manually, at least if you're like me, you'll screw it up lots of times until you finally get it right. So that's what these processes are here. Then we're going to split into training versus validation sets, and in this case, we do it by providing a list of indexes, so the indexes from 800 to 1000. It's very common, I don't quite remember the details of this data set, but it's very common for wanting to keep your validation sets to be contiguous groups of things, like if they're map tiles, they should be the map tiles that are next to each other, if they're time periods, they should be days that are next to each other, if they're video frames, they should be video frames next to each other, because otherwise you're kind of cheating. So it's often a good idea to split by IDX and to grab a range that's next to each other if your data has some kind of structure like that, or find some other way to structure it in that way. So that's now given us a training and a validation set. We now need to add labels, and in this case the labels can come straight from the data frame we grabbed earlier, so we just have to tell it which column it is. And so the dependent variable is, I think it's whether they're making over $50,000 salary, that's the thing we're trying to predict in this case. We'll talk about test sets later, but in this case we can add a test set, and finally get our data bunch. So at that point we have something that looks like this. So there is our data. And then to use it, it looks very familiar. You get a learner, in this case it's a tabular learner, passing in the data, some information about your architecture and some metrics, and you then call fit. How to combine NLP tokenized data with metadata such as tabular data with Fast.ai, for instance for IMBD classification, how to use information like who the actors are, year, may, genre, etc. We're not quite up to that yet, so we need to learn a little bit more about how neural net architectures work. But conceptually it's kind of the same as the way we combine categorical variables and continuous variables. Basically in the neural network you can have two different sets of inputs merging together into some layer. It could go into an early layer or into a later layer, it kind of depends. If it's like text and an image and some metadata, you probably want the text going into an RNN, the image going into a CNN, the metadata going into some kind of tabular model like this, and then you'd have them basically all concatenated together and then go through some fully connected layers and train them end to end. We'll probably largely get into that in part two. In fact we might entirely get into part two. I'm not sure if we'll have time to cover it in part one. Conceptually it's a fairly simple extension of what we'll be learning in the next three weeks. Question 2 Do you think things like scikit-learn and XGBoost will eventually become outdated? Will everyone use deep learning tools in the future except for maybe small datasets? I have no idea. I'm not good at making predictions. I'm not a machine learning model. I mean XGBoost is a really nice piece of software. There's quite a few really nice pieces of software for gradient boosting in particular. They have some really nice features, actually Random Forest in particular has some really nice features for interpretation, which I'm sure we'll find similar versions for neural nets but they don't necessarily exist yet. So I don't know. For now they're both useful tools. Scikit-learn is a library that's often used for pre-processing and running models. It's hard to predict where things will end up. In some ways it's more focused on some older approaches to modeling. They keep on adding new things. So we'll see. I keep trying to incorporate more scikit-learn stuff into fast AI and then I keep finding ways I think I can do it better and I throw it away again. So that's why there's still no scikit-learn dependencies in fast AI. I keep finding other ways to do stuff. Okay, so we're going to learn what layers equals means either towards the end of class today or the start of class next week, but this is where we're basically defining our architecture just like when we chose ResNet34 or whatever for all conv nets. We'll look at more about metrics in a moment, but just to remind you, metrics are just the things that get printed out. They don't change our model at all. So in this case we're saying I want you to print out the accuracy to see how we're doing. Okay, so that's how to do tabular. This is going to work really well because we're going to hit our break soon and the idea was that after three and a half lessons we're going to hit the end of all of the quick overview of applications and then we're going to go down the other side. I think we're going to be to the minute we're going to hit it, because the next one is collaborative filtering. So collaborative filtering is where you have information about who bought what or who liked what. It's basically something where you have something like a user or a reviewer or whatever and information about what they've bought or what they've written about or what they've reviewed. So in the most basic version of collaborative filtering, you just have two columns, something like user ID and movie ID and that just says this user bought that movie, this user bought that movie, this user bought that movie. So for example, Amazon has a really big list of user IDs and product IDs of like what did you buy. Then you can add additional information to that table such as, oh they left a review, what review did they give it? So it's now like user ID, movie ID, number of stars. You could add a time code. So like this user bought this product at this time and gave it this review. But they're all basically the same kind of structure. So there's kind of like two ways you could draw that collaborative filtering structure. One is kind of a two column approach where you've got like user and movie. And you've got user ID, movie ID, user ID. Each pair basically describes that user watched that movie. Possibly also plus number of stars, you know, 3, 4, 1, whatever. So the other way that you could write it would be you could have like all the users down here and all the movies along here. And then you can look and find a particular cell in there to find out, you know, could be the rating of that user for that movie or there's just a 1 there if that user watched that movie or whatever. So there's like two different ways of representing the same information. Conceptually it's often easier to think of it this way, right? But most of the time you won't store it that way explicitly because most of the time you have what's called a very sparse matrix, which is to say most users haven't watched most movies or most customers haven't purchased most products. So if you store it as a matrix where every combination of customer and product is a separate cell in that matrix, it's going to be enormous. So you tend to store it like this or you can store it as a matrix using some kind of special sparse matrix format. And if that sounds interesting, you should check out Rachel's computational linear algebra course on Fast AI where we have lots and lots and lots of information about sparse matrix storage approaches. For now though, we're just going to kind of keep it in this format on the left-hand side. So for collaborative filtering, there's a really nice dataset called MovieLens created by the GroupLens group, very hopefully, and you can download various different sizes, 20 million ratings, 100,000 ratings. We've actually created an extra small version for playing around with, which is what we'll start with today and then probably next week we'll use the bigger version. You can grab the small version using urls.ml sample. And it's a CSV, so you can read it with pandas. And here it is, right? It's basically a list of user IDs. We don't actually know anything about who these users are. There's some movie IDs. There is some information about what the movies are, but we won't look at that until next week. And then there's the rating, and then there's the timestamp. We're going to ignore the timestamp for now. So that's a subset of our data, that's the head. So the head in pandas is just the first few rows. So now that we've got a data frame, the nice thing about collaborative filtering is it's incredibly simple. That's all the data that we need. So you can now go ahead and say get collaborative learner, and you can actually just pass in the data frame directly. The architecture, you have to tell it how many factors you want to use, and we're going to learn what that means after the break. And then something that can be helpful is to tell it what the range of scores are, and we're going to see how that helps after the break as well. So in this case, the minimum score is 0, the maximum score is 5. So now that you've got a learner, you can go ahead and call fit one cycle. And transfer a few epochs, and there it is. So at the end of it, you now have something where you can pick a user ID and a movie ID and guess whether or not that user will like that movie. So this is obviously a super useful application that a lot of you are probably going to try over the week in past classes. A lot of people have taken this collaborative filtering approach back to their workplaces and discovered that using it in practice is much more tricky than this because in practice you have something called the cold start problem. So the cold start problem is that the time you particularly want to be good at recommending movies is when you have a new user, and the time you particularly care about recommending a movie is when it's a new movie. But at that point you don't have any data in your collaborative filtering system and it's really hard. As I say this, we don't currently have anything built into FastAI to handle the cold start problem, and that's really because the cold start problem, the only way I know of to solve it, in fact the only way I think that conceptually you can solve it, is to have a second model which is not a collaborative filtering model, but a metadata driven model for new users or new movies. I don't know if Netflix still does this, but certainly what they used to do when I signed up to Netflix was they started showing me lots of movies and saying have you seen this, did you like it, have you seen this, did you like it. So they fixed the cold start problem through the UX. So there was no cold start problem. They found like 20 really common movies and asked me if I liked them. They used my replies to those 20 to show me 20 more that I might have seen, and by the time I had gone through 60, there was no cold start problem anymore. And for new movies, it's not really a problem because like the first 100 users who haven't seen the movie go in and say whether they liked it, and then the next 100,000, the next million, it's not a cold start problem anymore. But the other thing you can do if you for whatever reason can't go through that UX of asking people did you like those things. So for example, if you're selling products and you don't really want to show them a big selection of your products and say did you like this because you just want them to buy, you can instead try and use a metadata based tabular model, what geography did they come from, maybe you know their age and sex, you can try and make some guesses about the initial recommendations. So collaborative filtering is specifically for once you have a bit of information about your users and movies or customers and products or whatever. Yeah, okay. How does the language model trained in this manner perform on code switch data such as Hindi written in English words or text with a lot of emojis? And then do you want the second question? Yeah, that's a good question. So text with emojis, it'll be fine. There's not many emojis in Wikipedia and where they are in Wikipedia, it's more like a Wikipedia page about the emoji rather than the emoji being used in a sensible place. But you can and should do this language model fine-tuning where you take a corpus of text where people are using emojis in usual ways and so you fine-tune the Wiki text language model to your Reddit or Twitter or whatever language model and there aren't that many emojis, right? So if you think about it, there's like hundreds of thousands of possible words that people can be using but a small number of possible emojis. So it'll very quickly learn how those emojis are being used. So that's a piece of cake. So I'm not very familiar with Hindi but I'll take an example I'm very familiar with which is Mandarin. In Mandarin, you could have a model that's trained with Chinese characters. So there's kind of 5 or 6,000 Chinese characters in common use but there's also a romanization of those characters called pinion. And it's a bit tricky because although there's a nearly direct mapping from the character to the pinion, I mean there is a direct mapping, the pronunciation is not exactly direct, there isn't a direct mapping from the pinion to the character because one pinion corresponds to multiple characters. So the first thing to note is that if you're going to use this approach for Chinese, you would need to start with a Chinese language model. So actually Fast.ai has something called a model zoo where we're adding more and more language models for different languages and also increasingly for different domain areas like English medical texts or even language models for things other than NLP like genome sequences, molecular data, musical MIDI notes and so forth. So you would obviously start there. To then convert that, that'll be either simplified or traditional Chinese, to then convert that into a, if you want to do pinion, you could either kind of map the vocab directly or as you'll learn, these multi-layer models, it's only the first layer that basically converts the tokens into a set of vectors. You can actually throw that away and fine tune just the first layer of the model. So that second part is going to require a bit more, a few more weeks of learning before you exactly understand how to do that and so forth. But if it's something you're interested in doing, we can talk about it on the forum because it's a kind of a nice test of understanding. So what about time series on tabular data? Is there an RNN model involved in tabular dot models? So we're going to look at time series tabular data next week. But the short answer is, generally speaking, you don't use an RNN for time series tabular data, but instead you extract a bunch of columns for things like day of week, is it a weekend, is it a holiday, was the store open, stuff like that. It turns out that adding those extra columns, which you can do somewhat automatically, basically gives you state of the art results. There are some good uses of RNNs for time series, but not really for these kind of tabular style time series like retail store logistics databases and stuff like that. Is there a source to learn more about the cold start problem? I'm going to have to look that up. If you know a good resource, please mention it on the forums. So that is both the break in the middle of lesson 4. It's the halfway point of the course and it's the point at which we have now seen an example of all the key applications. And so the rest of this course is going to be digging deeper into how they actually work behind the scenes, more of the theory, more of how the code, the source code is written and so forth. So it's a good time to have a nice break. Come back and furthermore, it's my birthday today, so it's a really special moment. So let's have a break and come back at 5 past 8. So Microsoft Excel. This is one of my favorite ways to explore data and understand models. I'll make sure I put this in the repo. And actually this one we can probably largely do in Google Sheets. I've tried to move as much as I can over the last few weeks into Google Sheets, but I just keep finding it's such a terrible product. Please try to find a copy of Microsoft Excel because there's nothing close. I've tried everything. Anyway, spreadsheets get a bad rap from people who basically don't know how to use them, just like people who spend their lives on Excel and then they start using Python and they're like, what the hell is this stupid thing? It takes thousands of hours to get really good at spreadsheets, but a few dozen hours to get competent at them. Once you're competent at them, you can see everything in front of you. It's all laid out. It's really great. I'll give you one spreadsheet tip today, which is if you hold down the control key or command key on your keyboard and press the arrow keys, here's control right, it takes you to the end of a block of a table that you're in and it's by far the best way to move around the place. In this case, I want to skip around through this table so I can hit control down right to get to the bottom right, control left up to get to the top left, skip around and see what's going on. Here's some data and as we talked about, one way to look at collaborative filtering data is like this. What we did was we grabbed from the movie lens data the people that watched the most movies and the movies that were the most watched and just filtered the data set down to those 15. As you can see, when you do it that way, it's not sparse anymore. There's just a small number of gaps. So this is something that we can now build a model with. And so how can we build a model? What we want to do is we want to create something which can predict for user 293, will they like movie 49, for example. So we've got to come up with some way of, you know, some function that can represent that decision. And so here's a simple possible approach. And so we're going to take this idea of doing some matrix multiplications. So I've created here a random matrix. So here's one matrix of random numbers and I've created here another matrix of random numbers. More specifically, for each movie, I've created five random numbers and for each user, I've created five random numbers. And so we could say then that user 14, movie 27, did they like it or not? Well the rating, what we could do would be to multiply together this vector and that vector. So here's a dot product, and here's a dot product. And so then we can basically do that for every possible thing in here. We've got the dot product. And thanks to spreadsheets, we can just do that in one place and copy it over and it fills in the whole thing for us. Why would we do it this way? Well, this is the basic starting point of a neural net, isn't it? The basic starting point of a neural net is that you take the matrix multiplication of two matrices and that's what your first layer always is. And so we just have to come up with some way of saying, well, what are two matrices that we can multiply? And so clearly, you need a matrix for a user, or a vector for a user, a matrix for all the users, and a vector for a movie, or a matrix for all the movies, and multiply them together and you get some numbers. So they don't mean anything yet, they're just random, but we can now use gradient descent to try to make these numbers and these numbers give us results that are closer to what we wanted. So how do we do that? Well, we've set this up now as a linear model. So the next thing we need is a loss function. So we can calculate our loss function by saying, well, okay, movie 3 for user ID 14 should have been a rating of 3 with this random matrices, it's actually a rating of 0.91. So we can find the sum of squared errors would be 3 minus 0.91 squared and then we can add them up. So there's actually a sum squared in Excel already, sum x minus y squared, so we can use just sum x minus y squared function, passing in those two ranges and then divide by the count to get the mean. So here is a number that is the mean, well, it's actually the square root of the mean squared error. So sometimes you'll see people talk about MSE, so that's the mean squared error, sometimes you'll see RMSE, that's the root mean squared error. So since I've got a square root at the front, this is the square root mean squared error. So we have a loss. So now all we need to do is use gradient descent to try to modify our weight matrices to make that loss smaller. So Excel will do that for me, so it's probably worth knowing how to do that. So we have to install add-ins, solvers there, okay, so the gradient descent solver in Excel is called solver and it just does normal gradient descent. So you just go data, solver, you need to make sure that in your settings that you've enabled the solver extension, it comes with Excel, and all you need to do is say which cell represents my loss function, so there it is, v41, right, so where is your loss function stored, which cells contain your variables, right, so you can see here I've got h19 to v23, which is up here, and b25 to f39, which is over there, and then you can just say, okay, set your loss function to a minimum by changing those cells and solve. And you'll see it starts at 2.81 and you can see the numbers going down, and so all that's doing is using gradient descent exactly the same way that we did when we did it manually in the notebook the other day, okay, but it's rather than solving the mean squared error for a at b, a at x in the Python, instead it is solving the loss function here, which is the mean squared error of the dot product of each of those vectors by each of these vectors, and so there it goes. So we'll let that run for a little while and see what happens, but basically in micro here is a simple way of creating a neural network, which is really in this case it's like just a single linear layer with gradient descent to solve a collaborative filtering problem. So let's go back and see what we do over here. So over here we used getCollabLearner. So the function that was called in the notebook was getCollabLearner, and so as you dig deeper into deep learning, one of the really good ways to dig deeper into deep learning is to dig into the fastai source code and see what's going on, and so if you're going to be able to do that, you need to know how to use your editor well enough to dig through the source code, and basically there's two main things you need to know how to do. One is to jump to a particular symbol, like a particular class or function by its name, and the other is that when you're looking at a particular symbol to be able to jump to its implementation. So for example in this case I want to find getCollabLearner, so in most editors, including the one I use, vim, you can set it up so that you can hit tab or something and it jumps through all the possible completions and you can hit enter and it jumps straight to the definition for you. So here is the definition of getCollabLearner, and as you can see it's pretty small as these things tend to be, and in this case it kind of wraps a data frame and automatically creates a data bunch for you because it's so simple, but the key thing it does then is to create a model of a particular kind, which is an embedding.bias model passing in the various things you asked for. So you want to find out in your editor how you jump to the definition of that, which in vim you just hit ctrl right square bracket, and here is the definition of embedding.bias. And so now we have everything on screen at once, and as you can see there's not much going on. So the models that are being created for you by FastAI are actually PyTorch models, and a PyTorch model is called an nn.module. That's the name in PyTorch of their models. It's a little more nuanced than that, but that's a good starting point for now. And when a PyTorch nn.module is run, when you calculate the result of that layer, it always calls a method for you called forward. So it's in here that you get to find out how this thing is actually calculated. When the model is built at the start, it calls this thing called underscore underscore init underscore underscore. And as I think we briefly mentioned before, in Python people tend to call this dunder init, double underscore init. So dunder init is how we create the model, and forward is how we run the model. One thing if you're watching carefully, you might notice, is there's nothing here saying to how to calculate the gradients of the model, and that's because PyTorch does it for us. So you only have to tell it how to calculate the output of your model, and PyTorch will go ahead and calculate the gradients for you. And so in this case, the model contains a set of weights for a user, a set of weights for an item, a set of biases for a user, a set of biases for an item, and each one of those is coming from this thing called get-embedding. So let's see get-embedding. So here is the definition of get-embedding, and all it does basically is it calls this PyTorch thing called nn.embedding. So in PyTorch they have a lot of standard neural network layers set up for you. So it creates an embedding, and then this thing here is, it just randomizes it. So this is something which creates normal random numbers for the embedding. So what's an embedding? An embedding, not surprisingly, is a matrix of weights. Specifically it's a matrix of weights that looks something like this. It's a matrix of weights which you can basically look up into and grab one item out of it. So basically any kind of weight matrix, and we're going to be digging into this in a lot more detail in the coming lessons, but an embedding matrix is just a weight matrix that is designed to be something that you kind of index into it as an array and grab one vector out of it. That's what an embedding matrix is. And so in our case we have an embedding matrix for a user and an embedding matrix for a movie. And here we have been taking the dot product of them. But if you think about it, that's not quite enough because we're missing this idea that like maybe there are certain movies that everybody likes more. Maybe there are some users that just tend to like movies more. So I don't really just want to multiply these two vectors together, but I really want to add a single number of how popular is this movie and add a single number of how much does this user like movies in general. So those are called bias terms. Remember how I said there's this kind of idea of bias and the way we dealt with that in our gradient descent notebook was we added a column of ones, but what we tend to do in practice is we actually explicitly say I want to add a bias term. So we don't just want to have prediction equals dot product of these two things, we want to say it's the dot product of those two things plus a bias term for a movie plus a bias term for a user ID. So that's basically what happens. When we set up the model, we set up the embedding matrix for the users and the embedding matrix for the items and then we also set up the bias vector for the users and the bias vector for the items. And then when we calculate the model, we literally just multiply the two together just like we did, right, we just take that product, call it dot, and then we add the bias and then putting aside the min and max score for a moment, that's what we return. So you can see that our model is literally doing what we did here with the tweak that we're also adding a bias. So it's an incredibly simple linear model. And for these kinds of collaborative filtering problems, this kind of simple linear model actually tends to work pretty well. And then there's one tweak that we do at the end, which is that in our case we said that there's a min score of 0 and a max score of 5. And so here's something to point out. Here's something to point out. So if you have a range, so you do that dot product and you add on the two biases and that could give you any possible number along the number line from very negative through to very positive numbers. But we know that we always want to end up with a number between 0 and 5. Let's say that's 5. And of course this is 0. So what if we mapped that number line like so to this function? And so the shape of that function is called a sigmoid. And so it's going to asymptote to 5 and it's going to asymptote to 0. And so that way, whatever number comes out of our dot product and adding the biases, if we then stick it through this function, it's never going to be higher than 5 and never going to be smaller than 0. Now strictly speaking, that's not necessary because our parameters could learn a set of weights that gives about the right number. So why would we do this extra thing if it's not necessary? Well the reason is we want to make life as easy for our model as possible. So if we actually set it up so it's impossible for it to ever predict too much or ever predict too little, then it can spend more of its weights predicting the thing we care about, which is deciding who's going to like what movie. So this is an idea we're going to keep coming back to when it comes to making neural networks work better, is it's about all these little decisions that we make to basically make it easier for the network to learn the right thing. So that's the last tweak here, which is we take the result of this dot product plus biases, we put it through a sigmoid, and so a sigmoid is just a function, it's basically 1 over 1 plus e to the x, the definition doesn't much matter but it just has the shape that I just mentioned. And that goes between 0 and 1, and if you then multiply that by max minus min plus min, then that's going to give you something that's between min score and max score. So that means that this tiny little neural network, it's a push to call it a neural network, but it is, it's a neural network with one weight matrix and no nonlinearities, so it's kind of the world's most boring neural network, with a sigmoid at the end. That's actually, well I guess it does have a nonlinearity, the sigmoid at the end is the nonlinearity, it only has one layer of weights. That actually turns out to give close to state-of-the-art performance, like I've looked up online to find out what are the best results people have on this MovieLens 100k database, and the results I get from this little thing are better than any of the results I can find from the standard commercial products that you can download that are specialized for this. And the trick seems to be that adding this little sigmoid makes a big difference. And did you have a question? There was a question about how you set up your Vim, and I've already linked here,.vimrc, but I wanted to know if you had more to say about that. What do they want to know about Vim? They really like your setup. You like my setup? There's almost nothing in my setup. It's pretty bare, honestly. Whatever you're doing with your editor, you probably want it to look like this, which is like when you've got a class that you're not currently working on, it should be closed up so you can't see it. And so you basically want something where it's easy to close and open fold. So Vim already does all this for you. And then, as I mentioned, you also want something where you can kind of jump to the definition of things, which in Vim it's called using tags. So if you want to jump to the definition of learner. Basically Vim already does all this for you. You just have to read the instructions. My Vimrc is minimal. I basically hardly use any extensions or anything. Another great editor to use is VS Code, Visual Studio Code. It's free and it's awesome and it has all the same features that you're seeing that Vim does. Basically VS Code does all of those things as well. I quite like using Vim because I can use it on the remote machine and play around. But you can of course just clone Git onto your local computer and open it up in VS Code to play around with it. Just don't try and look through the code just on GitHub or something. That's going to drive you crazy. You need to be able to open it and close it and jump and jump back. Maybe people can create some threads on the forum for Vim tips, VS Code tips, Sublime tips, whatever. For me I would say if you're going to pick an editor, if you want to use something on your local, I would go with VS Code today. I think it's the best. If you want to use something on the terminal side, I would go with Vim or Emacs. To me they're clear winners. So what I wanted to close with today is to take this collaborative filtering example and describe how we're going to build on top of it for the next three lessons to create the more complex neural networks we've been seeing. So roughly speaking, this is the bunch of concepts that we need to learn about. Let's think about what happens when you're using a neural network to do image recognition. Basically, let's take a single pixel. You've got lots of pixels, but let's take a single pixel. So you've got a red, a green, and a blue pixel. So each one of those is some number between 0 and 255. Or we kind of normalize them so they're a floating point with the mean of 0 and the standard deviation of 1. But let's just do the 0 to 255 version. So there's like 10, 20, 30, whatever. So what do we do with these? What we do is we basically treat that as a vector and we multiply it by a matrix. So this matrix, depending on how you think of the rows and the columns, let's treat the matrix as having three rows and then how many columns? Well, you get to pick. You get to pick, just like with the collaborative filtering version, I decided to pick a vector of size 5 for each of my embedding vectors. So that would mean that that's an embedding basically of size 5. You can get to pick how big your weight matrix is. So let's make it size 5. So this is 3 by 5. So initially this weight matrix contains random numbers. Remember when we looked up get embedding weight matrix just now and there were like two lines. The first line was like create the matrix and the second was fill it with random numbers. That's what we do. It all gets hidden behind the scenes by fastai and PyTorch. That's all it's doing. It's creating a matrix of random numbers when you set it up. And the number of rows has to be 3 to match the input and the number of columns can be as big as you like. And so after you multiply the vector, the input vector by that weight matrix, you're going to end up with a vector of size 5. So people often ask like how much linear algebra do I need to know to be able to do deep learning? This is the amount you need. So and if you're not familiar with this, that's fine. You need to know about matrix products. You don't need to know a lot about them. You just need to know like computationally what are they? What do they do? And you've got to be very comfortable with like if a matrix of size blah times a matrix of size blah gives a matrix of size blah. Like how do the dimensions match up? So if you have 3 and then remember in NumPy and PyTorch we use at times 3 by 5 gives a vector of size 5. And then what happens next? It goes through an activation function such as ReLU which is just max 0, x and spits out a new vector which is of course going to be exactly the same size because no activation function changes the size. It only changes the contents. So that's still of size 5. What happens next? We multiply it by another matrix and again it can be any number of columns but the number of rows has to match nicely. So it's going to be 5 by whatever. So maybe this one has 5 say by 10. And so that's going to give some output which will be size 10 and again we put that through ReLU and again that gives us something of the same size. And then we can put that through another matrix. Just to make this a bit clearer, you'll see why in a moment. I'm going to use 8 not 10. Let's say we're doing digit recognition. So there are 10 possible digits. So my last weight matrix has to be 10 in size because then that's going to mean my final output is a vector of 10 in size. But remember if we're doing digit recognition, what happens? We take our actuals, which is 10 in size, and if the number that we're trying to predict was the number 3, that's the thing we're trying to predict, then that means that there is a 3, 0, 0, 0 in the third position. So what happens is our neural net runs along, starting with our input, weight matrix, ReLU, weight matrix, ReLU, weight matrix, final output, and then we compare these two together to see how close they are, how close they match using some loss function. We'll learn about all the loss functions that we use next week. For now the only one we've learned is mean squared error. And we compare the actual, you can think of them as probabilities for each of the 10 to the actual each of the 10 to get a loss, and then we find the gradients of every one of the weight matrices with respect to that, and we update the weight matrices. So the main thing I wanted to show right now is the terminology we use because it's really important. These things contain numbers. Specifically they initially are matrices containing random numbers. And we can refer to these yellow things as, in PyTorch they're called parameters. Terms will refer to them as weights, although weights is slightly less accurate because they can also be biases. But we kind of use the terms a little bit interchangeably, but strictly speaking we should call them parameters. And then after each of those matrix products, that calculates a vector of numbers. So here are some numbers that are calculated by a weight matrix, multiply, and then there are some other sets of numbers that are calculated as a result of a ReLU, as a result of the activation function. Either one is called activations. So activations and parameters both refer to numbers. They are numbers. The parameters are numbers that are stored, they're used to make a calculation. Activations are the result of a calculation, they're numbers that are calculated. So they're the two key things you need to remember. So use these terms and use them correctly and accurately. And if you read these terms, they mean these very specific things, so don't mix them up in your head. And remember they're nothing weird and magical, they're very simple things. An activation is the result of either a matrix multiply or an activation function. And a parameter are the numbers inside the matrices that we multiply by. And then there are some special layers. So every one of these things that does a calculation, all of these things that does a calculation, are all called layers. They're the layers of our neural net. So every layer results in a set of activations because there's a calculation that results in a set of results. There's a special layer at the start, which is called the input layer, and then at the end, you just have a set of activations. And we can refer to those special, I mean they're not special mathematically, but they're semantically special, we can call those the outputs. So the important point to realize here is the outputs of a neural net are not actually like mathematically special, they're just the activations of a layer. And so what we did in our collaborative filtering example, we did something interesting. We actually added an additional activation function right at the very end. We added an extra activation function which was sigmoid. Specifically it was a scaled sigmoid, going between 0 and 5. And that's really common. It's very common to have an activation function as your last layer, and it's almost never going to be a relu, because it's very unlikely that what you actually want is something that stops, truncates at 0. It's very often going to be a sigmoid or something similar, because it's very likely that actually what you want is something that's between two values, and kind of scaled in that way. So that's nearly it, right? So we've got inputs, weights, activations, activation functions, which we sometimes call non-linearities, output, and then the function that compares those two things together is called the loss function, which so far we've used MSE. And that's enough for today. So what we're going to do next week is we're going to add in a few more extra bits, which is we're going to learn the loss function that's used for classification, which is called cross entropy. We're going to use the activation function that's used for single label classification, which is called softmax. And we're also going to learn exactly what happens when we do fine tuning in terms of how these layers actually, what happens with unfreeze and what happens when we create transfer learning. So thanks everybody. I'm looking forward to seeing you next week.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.84, "text": " Welcome to Lesson 4.", "tokens": [4027, 281, 18649, 266, 1017, 13], "temperature": 0.0, "avg_logprob": -0.1491846916003105, "compression_ratio": 1.6203208556149733, "no_speech_prob": 0.016395729035139084}, {"id": 1, "seek": 0, "start": 4.84, "end": 11.08, "text": " We are going to finish our journey through these key applications.", "tokens": [492, 366, 516, 281, 2413, 527, 4671, 807, 613, 2141, 5821, 13], "temperature": 0.0, "avg_logprob": -0.1491846916003105, "compression_ratio": 1.6203208556149733, "no_speech_prob": 0.016395729035139084}, {"id": 2, "seek": 0, "start": 11.08, "end": 14.08, "text": " We've already looked at a range of vision applications.", "tokens": [492, 600, 1217, 2956, 412, 257, 3613, 295, 5201, 5821, 13], "temperature": 0.0, "avg_logprob": -0.1491846916003105, "compression_ratio": 1.6203208556149733, "no_speech_prob": 0.016395729035139084}, {"id": 3, "seek": 0, "start": 14.08, "end": 19.52, "text": " We've looked at classification, localization, image regression.", "tokens": [492, 600, 2956, 412, 21538, 11, 2654, 2144, 11, 3256, 24590, 13], "temperature": 0.0, "avg_logprob": -0.1491846916003105, "compression_ratio": 1.6203208556149733, "no_speech_prob": 0.016395729035139084}, {"id": 4, "seek": 0, "start": 19.52, "end": 22.32, "text": " We've briefly touched on NLP.", "tokens": [492, 600, 10515, 9828, 322, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.1491846916003105, "compression_ratio": 1.6203208556149733, "no_speech_prob": 0.016395729035139084}, {"id": 5, "seek": 0, "start": 22.32, "end": 26.92, "text": " We're going to do a deeper dive into NLP transfer learning today.", "tokens": [492, 434, 516, 281, 360, 257, 7731, 9192, 666, 426, 45196, 5003, 2539, 965, 13], "temperature": 0.0, "avg_logprob": -0.1491846916003105, "compression_ratio": 1.6203208556149733, "no_speech_prob": 0.016395729035139084}, {"id": 6, "seek": 2692, "start": 26.92, "end": 32.440000000000005, "text": " We're going to then look at tabular data and we're going to look at collaborative filtering,", "tokens": [492, 434, 516, 281, 550, 574, 412, 4421, 1040, 1412, 293, 321, 434, 516, 281, 574, 412, 16555, 30822, 11], "temperature": 0.0, "avg_logprob": -0.11486647129058838, "compression_ratio": 2.053435114503817, "no_speech_prob": 0.00021987166837789118}, {"id": 7, "seek": 2692, "start": 32.440000000000005, "end": 34.84, "text": " which are both super useful applications.", "tokens": [597, 366, 1293, 1687, 4420, 5821, 13], "temperature": 0.0, "avg_logprob": -0.11486647129058838, "compression_ratio": 2.053435114503817, "no_speech_prob": 0.00021987166837789118}, {"id": 8, "seek": 2692, "start": 34.84, "end": 37.400000000000006, "text": " And then we're going to take a complete U-turn.", "tokens": [400, 550, 321, 434, 516, 281, 747, 257, 3566, 624, 12, 33886, 13], "temperature": 0.0, "avg_logprob": -0.11486647129058838, "compression_ratio": 2.053435114503817, "no_speech_prob": 0.00021987166837789118}, {"id": 9, "seek": 2692, "start": 37.400000000000006, "end": 41.96, "text": " We're going to take that collaborative filtering example and dive deeply into it to understand", "tokens": [492, 434, 516, 281, 747, 300, 16555, 30822, 1365, 293, 9192, 8760, 666, 309, 281, 1223], "temperature": 0.0, "avg_logprob": -0.11486647129058838, "compression_ratio": 2.053435114503817, "no_speech_prob": 0.00021987166837789118}, {"id": 10, "seek": 2692, "start": 41.96, "end": 46.760000000000005, "text": " exactly what's happening mathematically, exactly what's happening in the computer.", "tokens": [2293, 437, 311, 2737, 44003, 11, 2293, 437, 311, 2737, 294, 264, 3820, 13], "temperature": 0.0, "avg_logprob": -0.11486647129058838, "compression_ratio": 2.053435114503817, "no_speech_prob": 0.00021987166837789118}, {"id": 11, "seek": 2692, "start": 46.760000000000005, "end": 50.68000000000001, "text": " And we're going to use that to gradually go back in reverse order through the applications", "tokens": [400, 321, 434, 516, 281, 764, 300, 281, 13145, 352, 646, 294, 9943, 1668, 807, 264, 5821], "temperature": 0.0, "avg_logprob": -0.11486647129058838, "compression_ratio": 2.053435114503817, "no_speech_prob": 0.00021987166837789118}, {"id": 12, "seek": 2692, "start": 50.68000000000001, "end": 55.84, "text": " again in order to understand exactly what's going on behind the scenes of all of those", "tokens": [797, 294, 1668, 281, 1223, 2293, 437, 311, 516, 322, 2261, 264, 8026, 295, 439, 295, 729], "temperature": 0.0, "avg_logprob": -0.11486647129058838, "compression_ratio": 2.053435114503817, "no_speech_prob": 0.00021987166837789118}, {"id": 13, "seek": 5584, "start": 55.84, "end": 59.720000000000006, "text": " applications.", "tokens": [5821, 13], "temperature": 0.0, "avg_logprob": -0.13674076592049947, "compression_ratio": 1.6532663316582914, "no_speech_prob": 2.1111420210218057e-05}, {"id": 14, "seek": 5584, "start": 59.720000000000006, "end": 65.08, "text": " Before we do, somebody on the forum was kind enough to point out that when we compared", "tokens": [4546, 321, 360, 11, 2618, 322, 264, 17542, 390, 733, 1547, 281, 935, 484, 300, 562, 321, 5347], "temperature": 0.0, "avg_logprob": -0.13674076592049947, "compression_ratio": 1.6532663316582914, "no_speech_prob": 2.1111420210218057e-05}, {"id": 15, "seek": 5584, "start": 65.08, "end": 70.12, "text": " ourselves to what we think might be the state of the art or was recently the state of the", "tokens": [4175, 281, 437, 321, 519, 1062, 312, 264, 1785, 295, 264, 1523, 420, 390, 3938, 264, 1785, 295, 264], "temperature": 0.0, "avg_logprob": -0.13674076592049947, "compression_ratio": 1.6532663316582914, "no_speech_prob": 2.1111420210218057e-05}, {"id": 16, "seek": 5584, "start": 70.12, "end": 76.56, "text": " art for CanVood, it wasn't a fair comparison because the paper actually used a small subset", "tokens": [1523, 337, 1664, 53, 1816, 11, 309, 2067, 380, 257, 3143, 9660, 570, 264, 3035, 767, 1143, 257, 1359, 25993], "temperature": 0.0, "avg_logprob": -0.13674076592049947, "compression_ratio": 1.6532663316582914, "no_speech_prob": 2.1111420210218057e-05}, {"id": 17, "seek": 5584, "start": 76.56, "end": 80.98, "text": " of the classes and we used all of the classes.", "tokens": [295, 264, 5359, 293, 321, 1143, 439, 295, 264, 5359, 13], "temperature": 0.0, "avg_logprob": -0.13674076592049947, "compression_ratio": 1.6532663316582914, "no_speech_prob": 2.1111420210218057e-05}, {"id": 18, "seek": 8098, "start": 80.98, "end": 86.76, "text": " So Jason in our study group was kind enough to rerun the experiments with the correct", "tokens": [407, 11181, 294, 527, 2979, 1594, 390, 733, 1547, 281, 43819, 409, 264, 12050, 365, 264, 3006], "temperature": 0.0, "avg_logprob": -0.10054899031116117, "compression_ratio": 1.5104602510460252, "no_speech_prob": 8.529980732419062e-06}, {"id": 19, "seek": 8098, "start": 86.76, "end": 94.44, "text": " subset of classes from the paper and our accuracy went up to 94% compared to 91.5% in the paper.", "tokens": [25993, 295, 5359, 490, 264, 3035, 293, 527, 14170, 1437, 493, 281, 30849, 4, 5347, 281, 31064, 13, 20, 4, 294, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.10054899031116117, "compression_ratio": 1.5104602510460252, "no_speech_prob": 8.529980732419062e-06}, {"id": 20, "seek": 8098, "start": 94.44, "end": 102.64, "text": " So I think that's a really cool result and a great example of how some pretty much just", "tokens": [407, 286, 519, 300, 311, 257, 534, 1627, 1874, 293, 257, 869, 1365, 295, 577, 512, 1238, 709, 445], "temperature": 0.0, "avg_logprob": -0.10054899031116117, "compression_ratio": 1.5104602510460252, "no_speech_prob": 8.529980732419062e-06}, {"id": 21, "seek": 8098, "start": 102.64, "end": 108.2, "text": " using the defaults nowadays can get you far beyond what was the best of a year or two", "tokens": [1228, 264, 7576, 82, 13434, 393, 483, 291, 1400, 4399, 437, 390, 264, 1151, 295, 257, 1064, 420, 732], "temperature": 0.0, "avg_logprob": -0.10054899031116117, "compression_ratio": 1.5104602510460252, "no_speech_prob": 8.529980732419062e-06}, {"id": 22, "seek": 8098, "start": 108.2, "end": 109.2, "text": " ago.", "tokens": [2057, 13], "temperature": 0.0, "avg_logprob": -0.10054899031116117, "compression_ratio": 1.5104602510460252, "no_speech_prob": 8.529980732419062e-06}, {"id": 23, "seek": 10920, "start": 109.2, "end": 114.04, "text": " And certainly the best last year when we were doing this course because we started it quite", "tokens": [400, 3297, 264, 1151, 1036, 1064, 562, 321, 645, 884, 341, 1164, 570, 321, 1409, 309, 1596], "temperature": 0.0, "avg_logprob": -0.15149842876277558, "compression_ratio": 1.4864864864864864, "no_speech_prob": 1.2805189726350363e-05}, {"id": 24, "seek": 10920, "start": 114.04, "end": 115.04, "text": " intensely.", "tokens": [43235, 13], "temperature": 0.0, "avg_logprob": -0.15149842876277558, "compression_ratio": 1.4864864864864864, "no_speech_prob": 1.2805189726350363e-05}, {"id": 25, "seek": 10920, "start": 115.04, "end": 121.24000000000001, "text": " So that's really exciting.", "tokens": [407, 300, 311, 534, 4670, 13], "temperature": 0.0, "avg_logprob": -0.15149842876277558, "compression_ratio": 1.4864864864864864, "no_speech_prob": 1.2805189726350363e-05}, {"id": 26, "seek": 10920, "start": 121.24000000000001, "end": 131.22, "text": " So what I wanted to start with is going back over NLP a little bit to understand really", "tokens": [407, 437, 286, 1415, 281, 722, 365, 307, 516, 646, 670, 426, 45196, 257, 707, 857, 281, 1223, 534], "temperature": 0.0, "avg_logprob": -0.15149842876277558, "compression_ratio": 1.4864864864864864, "no_speech_prob": 1.2805189726350363e-05}, {"id": 27, "seek": 10920, "start": 131.22, "end": 132.28, "text": " what was going on there.", "tokens": [437, 390, 516, 322, 456, 13], "temperature": 0.0, "avg_logprob": -0.15149842876277558, "compression_ratio": 1.4864864864864864, "no_speech_prob": 1.2805189726350363e-05}, {"id": 28, "seek": 10920, "start": 132.28, "end": 135.92000000000002, "text": " So first of all, a quick review.", "tokens": [407, 700, 295, 439, 11, 257, 1702, 3131, 13], "temperature": 0.0, "avg_logprob": -0.15149842876277558, "compression_ratio": 1.4864864864864864, "no_speech_prob": 1.2805189726350363e-05}, {"id": 29, "seek": 13592, "start": 135.92, "end": 143.39999999999998, "text": " So remember NLP is natural language processing, it's about taking text and doing something", "tokens": [407, 1604, 426, 45196, 307, 3303, 2856, 9007, 11, 309, 311, 466, 1940, 2487, 293, 884, 746], "temperature": 0.0, "avg_logprob": -0.207512424416738, "compression_ratio": 1.5441176470588236, "no_speech_prob": 2.0144314476056024e-05}, {"id": 30, "seek": 13592, "start": 143.39999999999998, "end": 144.39999999999998, "text": " with it.", "tokens": [365, 309, 13], "temperature": 0.0, "avg_logprob": -0.207512424416738, "compression_ratio": 1.5441176470588236, "no_speech_prob": 2.0144314476056024e-05}, {"id": 31, "seek": 13592, "start": 144.39999999999998, "end": 151.0, "text": " And text classification is a particularly useful, practically useful application.", "tokens": [400, 2487, 21538, 307, 257, 4098, 4420, 11, 15667, 4420, 3861, 13], "temperature": 0.0, "avg_logprob": -0.207512424416738, "compression_ratio": 1.5441176470588236, "no_speech_prob": 2.0144314476056024e-05}, {"id": 32, "seek": 13592, "start": 151.0, "end": 153.95999999999998, "text": " It's what we're going to start off focusing on.", "tokens": [467, 311, 437, 321, 434, 516, 281, 722, 766, 8416, 322, 13], "temperature": 0.0, "avg_logprob": -0.207512424416738, "compression_ratio": 1.5441176470588236, "no_speech_prob": 2.0144314476056024e-05}, {"id": 33, "seek": 13592, "start": 153.95999999999998, "end": 159.35999999999999, "text": " Because classifying a text, classifying a document can be used for anything from spam", "tokens": [1436, 1508, 5489, 257, 2487, 11, 1508, 5489, 257, 4166, 393, 312, 1143, 337, 1340, 490, 24028], "temperature": 0.0, "avg_logprob": -0.207512424416738, "compression_ratio": 1.5441176470588236, "no_speech_prob": 2.0144314476056024e-05}, {"id": 34, "seek": 15936, "start": 159.36, "end": 171.96, "text": " prevention to identifying fake news to finding a diagnosis for medical reports, finding mentions", "tokens": [14630, 281, 16696, 7592, 2583, 281, 5006, 257, 15217, 337, 4625, 7122, 11, 5006, 23844], "temperature": 0.0, "avg_logprob": -0.19182431893270524, "compression_ratio": 1.4662921348314606, "no_speech_prob": 1.0129627298738342e-05}, {"id": 35, "seek": 15936, "start": 171.96, "end": 176.8, "text": " of your product in Twitter, so on and so forth.", "tokens": [295, 428, 1674, 294, 5794, 11, 370, 322, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.19182431893270524, "compression_ratio": 1.4662921348314606, "no_speech_prob": 1.0129627298738342e-05}, {"id": 36, "seek": 15936, "start": 176.8, "end": 178.56, "text": " So it's pretty interesting.", "tokens": [407, 309, 311, 1238, 1880, 13], "temperature": 0.0, "avg_logprob": -0.19182431893270524, "compression_ratio": 1.4662921348314606, "no_speech_prob": 1.0129627298738342e-05}, {"id": 37, "seek": 15936, "start": 178.56, "end": 188.96, "text": " And actually there was a great example during the week from one of our students who is a", "tokens": [400, 767, 456, 390, 257, 869, 1365, 1830, 264, 1243, 490, 472, 295, 527, 1731, 567, 307, 257], "temperature": 0.0, "avg_logprob": -0.19182431893270524, "compression_ratio": 1.4662921348314606, "no_speech_prob": 1.0129627298738342e-05}, {"id": 38, "seek": 18896, "start": 188.96, "end": 197.04000000000002, "text": " lawyer and he mentioned on the forum that he had really great results from classifying", "tokens": [11613, 293, 415, 2835, 322, 264, 17542, 300, 415, 632, 534, 869, 3542, 490, 1508, 5489], "temperature": 0.0, "avg_logprob": -0.1540280296688988, "compression_ratio": 1.59375, "no_speech_prob": 4.068698763148859e-05}, {"id": 39, "seek": 18896, "start": 197.04000000000002, "end": 201.12, "text": " legal texts using this NLP approach.", "tokens": [5089, 15765, 1228, 341, 426, 45196, 3109, 13], "temperature": 0.0, "avg_logprob": -0.1540280296688988, "compression_ratio": 1.59375, "no_speech_prob": 4.068698763148859e-05}, {"id": 40, "seek": 18896, "start": 201.12, "end": 202.12, "text": " And I thought this was a great example.", "tokens": [400, 286, 1194, 341, 390, 257, 869, 1365, 13], "temperature": 0.0, "avg_logprob": -0.1540280296688988, "compression_ratio": 1.59375, "no_speech_prob": 4.068698763148859e-05}, {"id": 41, "seek": 18896, "start": 202.12, "end": 208.60000000000002, "text": " So this is the poster that they presented at an academic conference this week describing", "tokens": [407, 341, 307, 264, 17171, 300, 436, 8212, 412, 364, 7778, 7586, 341, 1243, 16141], "temperature": 0.0, "avg_logprob": -0.1540280296688988, "compression_ratio": 1.59375, "no_speech_prob": 4.068698763148859e-05}, {"id": 42, "seek": 18896, "start": 208.60000000000002, "end": 209.86, "text": " the approach.", "tokens": [264, 3109, 13], "temperature": 0.0, "avg_logprob": -0.1540280296688988, "compression_ratio": 1.59375, "no_speech_prob": 4.068698763148859e-05}, {"id": 43, "seek": 18896, "start": 209.86, "end": 215.32, "text": " And actually this series of three steps that you see here, and I'm sure you recognize this", "tokens": [400, 767, 341, 2638, 295, 1045, 4439, 300, 291, 536, 510, 11, 293, 286, 478, 988, 291, 5521, 341], "temperature": 0.0, "avg_logprob": -0.1540280296688988, "compression_ratio": 1.59375, "no_speech_prob": 4.068698763148859e-05}, {"id": 44, "seek": 21532, "start": 215.32, "end": 221.4, "text": " classification matrix, this series of three steps here is what we're going to start by", "tokens": [21538, 8141, 11, 341, 2638, 295, 1045, 4439, 510, 307, 437, 321, 434, 516, 281, 722, 538], "temperature": 0.0, "avg_logprob": -0.11734938621520996, "compression_ratio": 1.6269430051813472, "no_speech_prob": 1.777794568624813e-05}, {"id": 45, "seek": 21532, "start": 221.4, "end": 223.48, "text": " digging into.", "tokens": [17343, 666, 13], "temperature": 0.0, "avg_logprob": -0.11734938621520996, "compression_ratio": 1.6269430051813472, "no_speech_prob": 1.777794568624813e-05}, {"id": 46, "seek": 21532, "start": 223.48, "end": 228.68, "text": " So we're going to start out with a movie review like this one and going to decide whether", "tokens": [407, 321, 434, 516, 281, 722, 484, 365, 257, 3169, 3131, 411, 341, 472, 293, 516, 281, 4536, 1968], "temperature": 0.0, "avg_logprob": -0.11734938621520996, "compression_ratio": 1.6269430051813472, "no_speech_prob": 1.777794568624813e-05}, {"id": 47, "seek": 21532, "start": 228.68, "end": 234.88, "text": " it's positive or negative sentiment about the movie.", "tokens": [309, 311, 3353, 420, 3671, 16149, 466, 264, 3169, 13], "temperature": 0.0, "avg_logprob": -0.11734938621520996, "compression_ratio": 1.6269430051813472, "no_speech_prob": 1.777794568624813e-05}, {"id": 48, "seek": 21532, "start": 234.88, "end": 236.92, "text": " That is the problem.", "tokens": [663, 307, 264, 1154, 13], "temperature": 0.0, "avg_logprob": -0.11734938621520996, "compression_ratio": 1.6269430051813472, "no_speech_prob": 1.777794568624813e-05}, {"id": 49, "seek": 21532, "start": 236.92, "end": 244.84, "text": " We have in the training set 25,000 movie reviews.", "tokens": [492, 362, 294, 264, 3097, 992, 3552, 11, 1360, 3169, 10229, 13], "temperature": 0.0, "avg_logprob": -0.11734938621520996, "compression_ratio": 1.6269430051813472, "no_speech_prob": 1.777794568624813e-05}, {"id": 50, "seek": 24484, "start": 244.84, "end": 251.8, "text": " So we've got 25,000 movie reviews and for each one we have like one bit of information.", "tokens": [407, 321, 600, 658, 3552, 11, 1360, 3169, 10229, 293, 337, 1184, 472, 321, 362, 411, 472, 857, 295, 1589, 13], "temperature": 0.0, "avg_logprob": -0.12623740717307808, "compression_ratio": 1.5411764705882354, "no_speech_prob": 6.747873158019502e-06}, {"id": 51, "seek": 24484, "start": 251.8, "end": 254.32, "text": " They liked it or they didn't like it.", "tokens": [814, 4501, 309, 420, 436, 994, 380, 411, 309, 13], "temperature": 0.0, "avg_logprob": -0.12623740717307808, "compression_ratio": 1.5411764705882354, "no_speech_prob": 6.747873158019502e-06}, {"id": 52, "seek": 24484, "start": 254.32, "end": 258.72, "text": " And as we're going to look into a lot more detail today and in the current lessons, our", "tokens": [400, 382, 321, 434, 516, 281, 574, 666, 257, 688, 544, 2607, 965, 293, 294, 264, 2190, 8820, 11, 527], "temperature": 0.0, "avg_logprob": -0.12623740717307808, "compression_ratio": 1.5411764705882354, "no_speech_prob": 6.747873158019502e-06}, {"id": 53, "seek": 24484, "start": 258.72, "end": 266.08, "text": " neural networks, remember they're just a bunch of matrix multiplies and simple nonlinearities,", "tokens": [18161, 9590, 11, 1604, 436, 434, 445, 257, 3840, 295, 8141, 12788, 530, 293, 2199, 2107, 28263, 1088, 11], "temperature": 0.0, "avg_logprob": -0.12623740717307808, "compression_ratio": 1.5411764705882354, "no_speech_prob": 6.747873158019502e-06}, {"id": 54, "seek": 24484, "start": 266.08, "end": 269.0, "text": " particularly replacing negatives with zeros.", "tokens": [4098, 19139, 40019, 365, 35193, 13], "temperature": 0.0, "avg_logprob": -0.12623740717307808, "compression_ratio": 1.5411764705882354, "no_speech_prob": 6.747873158019502e-06}, {"id": 55, "seek": 24484, "start": 269.0, "end": 272.64, "text": " Those weight matrices start out random.", "tokens": [3950, 3364, 32284, 722, 484, 4974, 13], "temperature": 0.0, "avg_logprob": -0.12623740717307808, "compression_ratio": 1.5411764705882354, "no_speech_prob": 6.747873158019502e-06}, {"id": 56, "seek": 27264, "start": 272.64, "end": 280.44, "text": " And so if you start out with some random parameters and try to train those parameters to learn", "tokens": [400, 370, 498, 291, 722, 484, 365, 512, 4974, 9834, 293, 853, 281, 3847, 729, 9834, 281, 1466], "temperature": 0.0, "avg_logprob": -0.08403624701745731, "compression_ratio": 1.7489711934156378, "no_speech_prob": 5.682373739546165e-06}, {"id": 57, "seek": 27264, "start": 280.44, "end": 286.76, "text": " how to recognize positive versus negative movie reviews, you only have literally 25,000", "tokens": [577, 281, 5521, 3353, 5717, 3671, 3169, 10229, 11, 291, 787, 362, 3736, 3552, 11, 1360], "temperature": 0.0, "avg_logprob": -0.08403624701745731, "compression_ratio": 1.7489711934156378, "no_speech_prob": 5.682373739546165e-06}, {"id": 58, "seek": 27264, "start": 286.76, "end": 289.91999999999996, "text": " ones and zeros to actually tell you I like this one, I don't like that one.", "tokens": [2306, 293, 35193, 281, 767, 980, 291, 286, 411, 341, 472, 11, 286, 500, 380, 411, 300, 472, 13], "temperature": 0.0, "avg_logprob": -0.08403624701745731, "compression_ratio": 1.7489711934156378, "no_speech_prob": 5.682373739546165e-06}, {"id": 59, "seek": 27264, "start": 289.91999999999996, "end": 295.91999999999996, "text": " That's clearly not enough information to learn basically how to speak English, how to speak", "tokens": [663, 311, 4448, 406, 1547, 1589, 281, 1466, 1936, 577, 281, 1710, 3669, 11, 577, 281, 1710], "temperature": 0.0, "avg_logprob": -0.08403624701745731, "compression_ratio": 1.7489711934156378, "no_speech_prob": 5.682373739546165e-06}, {"id": 60, "seek": 27264, "start": 295.91999999999996, "end": 301.02, "text": " English well enough to recognize they liked this or they didn't like this.", "tokens": [3669, 731, 1547, 281, 5521, 436, 4501, 341, 420, 436, 994, 380, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.08403624701745731, "compression_ratio": 1.7489711934156378, "no_speech_prob": 5.682373739546165e-06}, {"id": 61, "seek": 30102, "start": 301.02, "end": 303.71999999999997, "text": " And sometimes that can be pretty nuanced, right?", "tokens": [400, 2171, 300, 393, 312, 1238, 45115, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16744067821096867, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.902288315453916e-06}, {"id": 62, "seek": 30102, "start": 303.71999999999997, "end": 308.15999999999997, "text": " The English language often particularly with like movie reviews, people, because these", "tokens": [440, 3669, 2856, 2049, 4098, 365, 411, 3169, 10229, 11, 561, 11, 570, 613], "temperature": 0.0, "avg_logprob": -0.16744067821096867, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.902288315453916e-06}, {"id": 63, "seek": 30102, "start": 308.15999999999997, "end": 312.91999999999996, "text": " are like online movie reviews on IMDB, people can often like use sarcasm, it can be really", "tokens": [366, 411, 2950, 3169, 10229, 322, 21463, 27735, 11, 561, 393, 2049, 411, 764, 36836, 14774, 11, 309, 393, 312, 534], "temperature": 0.0, "avg_logprob": -0.16744067821096867, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.902288315453916e-06}, {"id": 64, "seek": 30102, "start": 312.91999999999996, "end": 313.96, "text": " quite tricky.", "tokens": [1596, 12414, 13], "temperature": 0.0, "avg_logprob": -0.16744067821096867, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.902288315453916e-06}, {"id": 65, "seek": 30102, "start": 313.96, "end": 323.59999999999997, "text": " So for a long time, in fact until very recently like this year, neural nets didn't do a good", "tokens": [407, 337, 257, 938, 565, 11, 294, 1186, 1826, 588, 3938, 411, 341, 1064, 11, 18161, 36170, 994, 380, 360, 257, 665], "temperature": 0.0, "avg_logprob": -0.16744067821096867, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.902288315453916e-06}, {"id": 66, "seek": 30102, "start": 323.59999999999997, "end": 328.35999999999996, "text": " job at all of this kind of classification problem.", "tokens": [1691, 412, 439, 295, 341, 733, 295, 21538, 1154, 13], "temperature": 0.0, "avg_logprob": -0.16744067821096867, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.902288315453916e-06}, {"id": 67, "seek": 32836, "start": 328.36, "end": 333.58000000000004, "text": " And that was why there's not enough information available.", "tokens": [400, 300, 390, 983, 456, 311, 406, 1547, 1589, 2435, 13], "temperature": 0.0, "avg_logprob": -0.1135401522859614, "compression_ratio": 1.5862068965517242, "no_speech_prob": 7.183136403909884e-06}, {"id": 68, "seek": 32836, "start": 333.58000000000004, "end": 337.68, "text": " So the trick, hopefully you can all guess, it's to use transfer learning.", "tokens": [407, 264, 4282, 11, 4696, 291, 393, 439, 2041, 11, 309, 311, 281, 764, 5003, 2539, 13], "temperature": 0.0, "avg_logprob": -0.1135401522859614, "compression_ratio": 1.5862068965517242, "no_speech_prob": 7.183136403909884e-06}, {"id": 69, "seek": 32836, "start": 337.68, "end": 339.44, "text": " It's always the trick.", "tokens": [467, 311, 1009, 264, 4282, 13], "temperature": 0.0, "avg_logprob": -0.1135401522859614, "compression_ratio": 1.5862068965517242, "no_speech_prob": 7.183136403909884e-06}, {"id": 70, "seek": 32836, "start": 339.44, "end": 345.94, "text": " So last year in this course, I tried something crazy, which was I thought, what if I try", "tokens": [407, 1036, 1064, 294, 341, 1164, 11, 286, 3031, 746, 3219, 11, 597, 390, 286, 1194, 11, 437, 498, 286, 853], "temperature": 0.0, "avg_logprob": -0.1135401522859614, "compression_ratio": 1.5862068965517242, "no_speech_prob": 7.183136403909884e-06}, {"id": 71, "seek": 32836, "start": 345.94, "end": 351.12, "text": " transfer learning to demonstrate that it can work for an LP as well?", "tokens": [5003, 2539, 281, 11698, 300, 309, 393, 589, 337, 364, 38095, 382, 731, 30], "temperature": 0.0, "avg_logprob": -0.1135401522859614, "compression_ratio": 1.5862068965517242, "no_speech_prob": 7.183136403909884e-06}, {"id": 72, "seek": 32836, "start": 351.12, "end": 355.48, "text": " And I tried it out and it worked extraordinarily well.", "tokens": [400, 286, 3031, 309, 484, 293, 309, 2732, 34557, 731, 13], "temperature": 0.0, "avg_logprob": -0.1135401522859614, "compression_ratio": 1.5862068965517242, "no_speech_prob": 7.183136403909884e-06}, {"id": 73, "seek": 35548, "start": 355.48, "end": 361.12, "text": " And so here we are a year later and transfer learning in NLP is absolutely the hit thing", "tokens": [400, 370, 510, 321, 366, 257, 1064, 1780, 293, 5003, 2539, 294, 426, 45196, 307, 3122, 264, 2045, 551], "temperature": 0.0, "avg_logprob": -0.08797791174479894, "compression_ratio": 1.6550387596899225, "no_speech_prob": 8.664390406920575e-06}, {"id": 74, "seek": 35548, "start": 361.12, "end": 362.12, "text": " now.", "tokens": [586, 13], "temperature": 0.0, "avg_logprob": -0.08797791174479894, "compression_ratio": 1.6550387596899225, "no_speech_prob": 8.664390406920575e-06}, {"id": 75, "seek": 35548, "start": 362.12, "end": 364.84000000000003, "text": " And so I'm going to describe to you what happens.", "tokens": [400, 370, 286, 478, 516, 281, 6786, 281, 291, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.08797791174479894, "compression_ratio": 1.6550387596899225, "no_speech_prob": 8.664390406920575e-06}, {"id": 76, "seek": 35548, "start": 364.84000000000003, "end": 371.32, "text": " The key thing is we're going to start with the same kind of thing that we used for computer", "tokens": [440, 2141, 551, 307, 321, 434, 516, 281, 722, 365, 264, 912, 733, 295, 551, 300, 321, 1143, 337, 3820], "temperature": 0.0, "avg_logprob": -0.08797791174479894, "compression_ratio": 1.6550387596899225, "no_speech_prob": 8.664390406920575e-06}, {"id": 77, "seek": 35548, "start": 371.32, "end": 376.20000000000005, "text": " vision, a pre-trained model that's been trained to do something different to what we're doing", "tokens": [5201, 11, 257, 659, 12, 17227, 2001, 2316, 300, 311, 668, 8895, 281, 360, 746, 819, 281, 437, 321, 434, 884], "temperature": 0.0, "avg_logprob": -0.08797791174479894, "compression_ratio": 1.6550387596899225, "no_speech_prob": 8.664390406920575e-06}, {"id": 78, "seek": 35548, "start": 376.20000000000005, "end": 377.68, "text": " with it.", "tokens": [365, 309, 13], "temperature": 0.0, "avg_logprob": -0.08797791174479894, "compression_ratio": 1.6550387596899225, "no_speech_prob": 8.664390406920575e-06}, {"id": 79, "seek": 35548, "start": 377.68, "end": 383.28000000000003, "text": " And so for ImageNet, that was originally built as a model to predict which of a thousand", "tokens": [400, 370, 337, 29903, 31890, 11, 300, 390, 7993, 3094, 382, 257, 2316, 281, 6069, 597, 295, 257, 4714], "temperature": 0.0, "avg_logprob": -0.08797791174479894, "compression_ratio": 1.6550387596899225, "no_speech_prob": 8.664390406920575e-06}, {"id": 80, "seek": 38328, "start": 383.28, "end": 386.11999999999995, "text": " categories each photo falls into.", "tokens": [10479, 1184, 5052, 8804, 666, 13], "temperature": 0.0, "avg_logprob": -0.12007528192856733, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.3405005979147973e-06}, {"id": 81, "seek": 38328, "start": 386.11999999999995, "end": 390.47999999999996, "text": " And people then fine-tuned that for all kinds of different things as you've seen.", "tokens": [400, 561, 550, 2489, 12, 83, 43703, 300, 337, 439, 3685, 295, 819, 721, 382, 291, 600, 1612, 13], "temperature": 0.0, "avg_logprob": -0.12007528192856733, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.3405005979147973e-06}, {"id": 82, "seek": 38328, "start": 390.47999999999996, "end": 394.55999999999995, "text": " So we're going to start with a pre-trained model that's going to do something else, not", "tokens": [407, 321, 434, 516, 281, 722, 365, 257, 659, 12, 17227, 2001, 2316, 300, 311, 516, 281, 360, 746, 1646, 11, 406], "temperature": 0.0, "avg_logprob": -0.12007528192856733, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.3405005979147973e-06}, {"id": 83, "seek": 38328, "start": 394.55999999999995, "end": 396.35999999999996, "text": " movie review classification.", "tokens": [3169, 3131, 21538, 13], "temperature": 0.0, "avg_logprob": -0.12007528192856733, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.3405005979147973e-06}, {"id": 84, "seek": 38328, "start": 396.35999999999996, "end": 400.28, "text": " We're going to start with a pre-trained model which is called a language model.", "tokens": [492, 434, 516, 281, 722, 365, 257, 659, 12, 17227, 2001, 2316, 597, 307, 1219, 257, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12007528192856733, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.3405005979147973e-06}, {"id": 85, "seek": 38328, "start": 400.28, "end": 405.28, "text": " A language model has a very specific meaning in NLP and it's this.", "tokens": [316, 2856, 2316, 575, 257, 588, 2685, 3620, 294, 426, 45196, 293, 309, 311, 341, 13], "temperature": 0.0, "avg_logprob": -0.12007528192856733, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.3405005979147973e-06}, {"id": 86, "seek": 38328, "start": 405.28, "end": 411.2, "text": " A language model is a model that learns to predict the next word of a sentence.", "tokens": [316, 2856, 2316, 307, 257, 2316, 300, 27152, 281, 6069, 264, 958, 1349, 295, 257, 8174, 13], "temperature": 0.0, "avg_logprob": -0.12007528192856733, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.3405005979147973e-06}, {"id": 87, "seek": 41120, "start": 411.2, "end": 415.76, "text": " And to predict the next word of a sentence you actually have to know quite a lot about", "tokens": [400, 281, 6069, 264, 958, 1349, 295, 257, 8174, 291, 767, 362, 281, 458, 1596, 257, 688, 466], "temperature": 0.0, "avg_logprob": -0.19189051146148353, "compression_ratio": 1.5761904761904761, "no_speech_prob": 2.2252647795539815e-06}, {"id": 88, "seek": 41120, "start": 415.76, "end": 421.47999999999996, "text": " English, assuming you're doing it in English, and quite a lot of world knowledge.", "tokens": [3669, 11, 11926, 291, 434, 884, 309, 294, 3669, 11, 293, 1596, 257, 688, 295, 1002, 3601, 13], "temperature": 0.0, "avg_logprob": -0.19189051146148353, "compression_ratio": 1.5761904761904761, "no_speech_prob": 2.2252647795539815e-06}, {"id": 89, "seek": 41120, "start": 421.47999999999996, "end": 423.44, "text": " By world knowledge I'll give you an example.", "tokens": [3146, 1002, 3601, 286, 603, 976, 291, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.19189051146148353, "compression_ratio": 1.5761904761904761, "no_speech_prob": 2.2252647795539815e-06}, {"id": 90, "seek": 41120, "start": 423.44, "end": 429.36, "text": " Here's your language model and it's read, I'd like to eat a hot what?", "tokens": [1692, 311, 428, 2856, 2316, 293, 309, 311, 1401, 11, 286, 1116, 411, 281, 1862, 257, 2368, 437, 30], "temperature": 0.0, "avg_logprob": -0.19189051146148353, "compression_ratio": 1.5761904761904761, "no_speech_prob": 2.2252647795539815e-06}, {"id": 91, "seek": 41120, "start": 429.36, "end": 432.28, "text": " Obviously dog.", "tokens": [7580, 3000, 13], "temperature": 0.0, "avg_logprob": -0.19189051146148353, "compression_ratio": 1.5761904761904761, "no_speech_prob": 2.2252647795539815e-06}, {"id": 92, "seek": 41120, "start": 432.28, "end": 435.68, "text": " It was a hot what?", "tokens": [467, 390, 257, 2368, 437, 30], "temperature": 0.0, "avg_logprob": -0.19189051146148353, "compression_ratio": 1.5761904761904761, "no_speech_prob": 2.2252647795539815e-06}, {"id": 93, "seek": 41120, "start": 435.68, "end": 437.15999999999997, "text": " Probably day.", "tokens": [9210, 786, 13], "temperature": 0.0, "avg_logprob": -0.19189051146148353, "compression_ratio": 1.5761904761904761, "no_speech_prob": 2.2252647795539815e-06}, {"id": 94, "seek": 43716, "start": 437.16, "end": 442.28000000000003, "text": " Now previous approaches to NLP use something called N-grams largely, which is basically", "tokens": [823, 3894, 11587, 281, 426, 45196, 764, 746, 1219, 426, 12, 1342, 82, 11611, 11, 597, 307, 1936], "temperature": 0.0, "avg_logprob": -0.12735148703697885, "compression_ratio": 1.5802469135802468, "no_speech_prob": 1.3081577208140516e-06}, {"id": 95, "seek": 43716, "start": 442.28000000000003, "end": 447.6, "text": " saying how often do these pairs or triplets of words tend to appear next to each other.", "tokens": [1566, 577, 2049, 360, 613, 15494, 420, 1376, 31023, 295, 2283, 3928, 281, 4204, 958, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.12735148703697885, "compression_ratio": 1.5802469135802468, "no_speech_prob": 1.3081577208140516e-06}, {"id": 96, "seek": 43716, "start": 447.6, "end": 450.0, "text": " And N-grams are terrible at this kind of thing.", "tokens": [400, 426, 12, 1342, 82, 366, 6237, 412, 341, 733, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.12735148703697885, "compression_ratio": 1.5802469135802468, "no_speech_prob": 1.3081577208140516e-06}, {"id": 97, "seek": 43716, "start": 450.0, "end": 454.72, "text": " As you can see there's not enough information here to decide what the next word probably", "tokens": [1018, 291, 393, 536, 456, 311, 406, 1547, 1589, 510, 281, 4536, 437, 264, 958, 1349, 1391], "temperature": 0.0, "avg_logprob": -0.12735148703697885, "compression_ratio": 1.5802469135802468, "no_speech_prob": 1.3081577208140516e-06}, {"id": 98, "seek": 43716, "start": 454.72, "end": 455.72, "text": " is.", "tokens": [307, 13], "temperature": 0.0, "avg_logprob": -0.12735148703697885, "compression_ratio": 1.5802469135802468, "no_speech_prob": 1.3081577208140516e-06}, {"id": 99, "seek": 43716, "start": 455.72, "end": 459.66, "text": " But with a neural net you absolutely can.", "tokens": [583, 365, 257, 18161, 2533, 291, 3122, 393, 13], "temperature": 0.0, "avg_logprob": -0.12735148703697885, "compression_ratio": 1.5802469135802468, "no_speech_prob": 1.3081577208140516e-06}, {"id": 100, "seek": 43716, "start": 459.66, "end": 460.92, "text": " So here's the nice thing.", "tokens": [407, 510, 311, 264, 1481, 551, 13], "temperature": 0.0, "avg_logprob": -0.12735148703697885, "compression_ratio": 1.5802469135802468, "no_speech_prob": 1.3081577208140516e-06}, {"id": 101, "seek": 46092, "start": 460.92, "end": 468.2, "text": " If you train a neural net to predict the next word of a sentence, then you actually have", "tokens": [759, 291, 3847, 257, 18161, 2533, 281, 6069, 264, 958, 1349, 295, 257, 8174, 11, 550, 291, 767, 362], "temperature": 0.0, "avg_logprob": -0.12101930795713912, "compression_ratio": 1.7614213197969544, "no_speech_prob": 2.6841755698114866e-06}, {"id": 102, "seek": 46092, "start": 468.2, "end": 469.68, "text": " a lot of information.", "tokens": [257, 688, 295, 1589, 13], "temperature": 0.0, "avg_logprob": -0.12101930795713912, "compression_ratio": 1.7614213197969544, "no_speech_prob": 2.6841755698114866e-06}, {"id": 103, "seek": 46092, "start": 469.68, "end": 474.48, "text": " Rather than having a single bit for every 2000 word movie review, liked it or didn't", "tokens": [16571, 813, 1419, 257, 2167, 857, 337, 633, 8132, 1349, 3169, 3131, 11, 4501, 309, 420, 994, 380], "temperature": 0.0, "avg_logprob": -0.12101930795713912, "compression_ratio": 1.7614213197969544, "no_speech_prob": 2.6841755698114866e-06}, {"id": 104, "seek": 46092, "start": 474.48, "end": 479.44, "text": " like it, every single word you can try and predict the next word.", "tokens": [411, 309, 11, 633, 2167, 1349, 291, 393, 853, 293, 6069, 264, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.12101930795713912, "compression_ratio": 1.7614213197969544, "no_speech_prob": 2.6841755698114866e-06}, {"id": 105, "seek": 46092, "start": 479.44, "end": 487.92, "text": " So in a 2000 word movie review there are 1999 opportunities to predict the next word.", "tokens": [407, 294, 257, 8132, 1349, 3169, 3131, 456, 366, 19952, 4786, 281, 6069, 264, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.12101930795713912, "compression_ratio": 1.7614213197969544, "no_speech_prob": 2.6841755698114866e-06}, {"id": 106, "seek": 48792, "start": 487.92, "end": 491.88, "text": " Better still, you don't just have to look at movie reviews.", "tokens": [15753, 920, 11, 291, 500, 380, 445, 362, 281, 574, 412, 3169, 10229, 13], "temperature": 0.0, "avg_logprob": -0.17269485775787052, "compression_ratio": 1.592156862745098, "no_speech_prob": 6.747989118593978e-06}, {"id": 107, "seek": 48792, "start": 491.88, "end": 498.64000000000004, "text": " Because really the hard thing isn't so much does this person like the movie or not, but", "tokens": [1436, 534, 264, 1152, 551, 1943, 380, 370, 709, 775, 341, 954, 411, 264, 3169, 420, 406, 11, 457], "temperature": 0.0, "avg_logprob": -0.17269485775787052, "compression_ratio": 1.592156862745098, "no_speech_prob": 6.747989118593978e-06}, {"id": 108, "seek": 48792, "start": 498.64000000000004, "end": 500.88, "text": " how do you speak English?", "tokens": [577, 360, 291, 1710, 3669, 30], "temperature": 0.0, "avg_logprob": -0.17269485775787052, "compression_ratio": 1.592156862745098, "no_speech_prob": 6.747989118593978e-06}, {"id": 109, "seek": 48792, "start": 500.88, "end": 507.36, "text": " So you can learn how do you speak English, roughly, from some much bigger set of documents.", "tokens": [407, 291, 393, 1466, 577, 360, 291, 1710, 3669, 11, 9810, 11, 490, 512, 709, 3801, 992, 295, 8512, 13], "temperature": 0.0, "avg_logprob": -0.17269485775787052, "compression_ratio": 1.592156862745098, "no_speech_prob": 6.747989118593978e-06}, {"id": 110, "seek": 48792, "start": 507.36, "end": 511.64, "text": " And so what we did was we started with Wikipedia.", "tokens": [400, 370, 437, 321, 630, 390, 321, 1409, 365, 28999, 13], "temperature": 0.0, "avg_logprob": -0.17269485775787052, "compression_ratio": 1.592156862745098, "no_speech_prob": 6.747989118593978e-06}, {"id": 111, "seek": 48792, "start": 511.64, "end": 516.08, "text": " And Stephen Meridy and some of his colleagues built something called the WikiText 103 data", "tokens": [400, 13391, 6124, 38836, 293, 512, 295, 702, 7734, 3094, 746, 1219, 264, 35892, 50198, 48784, 1412], "temperature": 0.0, "avg_logprob": -0.17269485775787052, "compression_ratio": 1.592156862745098, "no_speech_prob": 6.747989118593978e-06}, {"id": 112, "seek": 51608, "start": 516.08, "end": 524.24, "text": " set, which is simply a subset of most of the largest articles from Wikipedia with a little", "tokens": [992, 11, 597, 307, 2935, 257, 25993, 295, 881, 295, 264, 6443, 11290, 490, 28999, 365, 257, 707], "temperature": 0.0, "avg_logprob": -0.145521103067601, "compression_ratio": 1.6166666666666667, "no_speech_prob": 1.8924747564597055e-05}, {"id": 113, "seek": 51608, "start": 524.24, "end": 527.24, "text": " bit of pre-processing that's available for download.", "tokens": [857, 295, 659, 12, 41075, 278, 300, 311, 2435, 337, 5484, 13], "temperature": 0.0, "avg_logprob": -0.145521103067601, "compression_ratio": 1.6166666666666667, "no_speech_prob": 1.8924747564597055e-05}, {"id": 114, "seek": 51608, "start": 527.24, "end": 531.8000000000001, "text": " And so you're basically grabbing Wikipedia and then I built a language model on all of", "tokens": [400, 370, 291, 434, 1936, 23771, 28999, 293, 550, 286, 3094, 257, 2856, 2316, 322, 439, 295], "temperature": 0.0, "avg_logprob": -0.145521103067601, "compression_ratio": 1.6166666666666667, "no_speech_prob": 1.8924747564597055e-05}, {"id": 115, "seek": 51608, "start": 531.8000000000001, "end": 532.8000000000001, "text": " Wikipedia.", "tokens": [28999, 13], "temperature": 0.0, "avg_logprob": -0.145521103067601, "compression_ratio": 1.6166666666666667, "no_speech_prob": 1.8924747564597055e-05}, {"id": 116, "seek": 51608, "start": 532.8000000000001, "end": 538.76, "text": " So I've just built a neural net which would predict the next word in every significantly", "tokens": [407, 286, 600, 445, 3094, 257, 18161, 2533, 597, 576, 6069, 264, 958, 1349, 294, 633, 10591], "temperature": 0.0, "avg_logprob": -0.145521103067601, "compression_ratio": 1.6166666666666667, "no_speech_prob": 1.8924747564597055e-05}, {"id": 117, "seek": 51608, "start": 538.76, "end": 540.6400000000001, "text": " sized Wikipedia article.", "tokens": [20004, 28999, 7222, 13], "temperature": 0.0, "avg_logprob": -0.145521103067601, "compression_ratio": 1.6166666666666667, "no_speech_prob": 1.8924747564597055e-05}, {"id": 118, "seek": 51608, "start": 540.6400000000001, "end": 542.88, "text": " And that's a lot of information.", "tokens": [400, 300, 311, 257, 688, 295, 1589, 13], "temperature": 0.0, "avg_logprob": -0.145521103067601, "compression_ratio": 1.6166666666666667, "no_speech_prob": 1.8924747564597055e-05}, {"id": 119, "seek": 54288, "start": 542.88, "end": 546.04, "text": " If I remember correctly, it's something like a billion tokens.", "tokens": [759, 286, 1604, 8944, 11, 309, 311, 746, 411, 257, 5218, 22667, 13], "temperature": 0.0, "avg_logprob": -0.12850205533139342, "compression_ratio": 1.6789667896678966, "no_speech_prob": 2.014541496464517e-05}, {"id": 120, "seek": 54288, "start": 546.04, "end": 548.88, "text": " So we've got a billion separate things to predict.", "tokens": [407, 321, 600, 658, 257, 5218, 4994, 721, 281, 6069, 13], "temperature": 0.0, "avg_logprob": -0.12850205533139342, "compression_ratio": 1.6789667896678966, "no_speech_prob": 2.014541496464517e-05}, {"id": 121, "seek": 54288, "start": 548.88, "end": 554.6, "text": " Every time we make a mistake on one of those predictions, we get the loss, we get gradients", "tokens": [2048, 565, 321, 652, 257, 6146, 322, 472, 295, 729, 21264, 11, 321, 483, 264, 4470, 11, 321, 483, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.12850205533139342, "compression_ratio": 1.6789667896678966, "no_speech_prob": 2.014541496464517e-05}, {"id": 122, "seek": 54288, "start": 554.6, "end": 558.3, "text": " from that, we can update our weights and make them better and better until we can get pretty", "tokens": [490, 300, 11, 321, 393, 5623, 527, 17443, 293, 652, 552, 1101, 293, 1101, 1826, 321, 393, 483, 1238], "temperature": 0.0, "avg_logprob": -0.12850205533139342, "compression_ratio": 1.6789667896678966, "no_speech_prob": 2.014541496464517e-05}, {"id": 123, "seek": 54288, "start": 558.3, "end": 562.0, "text": " good at predicting the next word of Wikipedia.", "tokens": [665, 412, 32884, 264, 958, 1349, 295, 28999, 13], "temperature": 0.0, "avg_logprob": -0.12850205533139342, "compression_ratio": 1.6789667896678966, "no_speech_prob": 2.014541496464517e-05}, {"id": 124, "seek": 54288, "start": 562.0, "end": 563.28, "text": " Why is that useful?", "tokens": [1545, 307, 300, 4420, 30], "temperature": 0.0, "avg_logprob": -0.12850205533139342, "compression_ratio": 1.6789667896678966, "no_speech_prob": 2.014541496464517e-05}, {"id": 125, "seek": 54288, "start": 563.28, "end": 567.08, "text": " Because at that point I've got a model that knows probably how to complete sentences like", "tokens": [1436, 412, 300, 935, 286, 600, 658, 257, 2316, 300, 3255, 1391, 577, 281, 3566, 16579, 411], "temperature": 0.0, "avg_logprob": -0.12850205533139342, "compression_ratio": 1.6789667896678966, "no_speech_prob": 2.014541496464517e-05}, {"id": 126, "seek": 56708, "start": 567.08, "end": 573.0400000000001, "text": " this and so it knows quite a lot about English and quite a lot about how the world works,", "tokens": [341, 293, 370, 309, 3255, 1596, 257, 688, 466, 3669, 293, 1596, 257, 688, 466, 577, 264, 1002, 1985, 11], "temperature": 0.0, "avg_logprob": -0.18257380013514046, "compression_ratio": 1.7016129032258065, "no_speech_prob": 1.3419466085906606e-05}, {"id": 127, "seek": 56708, "start": 573.0400000000001, "end": 577.5600000000001, "text": " what kinds of things tend to be hot in different situations, for instance.", "tokens": [437, 3685, 295, 721, 3928, 281, 312, 2368, 294, 819, 6851, 11, 337, 5197, 13], "temperature": 0.0, "avg_logprob": -0.18257380013514046, "compression_ratio": 1.7016129032258065, "no_speech_prob": 1.3419466085906606e-05}, {"id": 128, "seek": 56708, "start": 577.5600000000001, "end": 585.44, "text": " I mean ideally it would learn things like in 1996 in a speech to the United Nations,", "tokens": [286, 914, 22915, 309, 576, 1466, 721, 411, 294, 22690, 294, 257, 6218, 281, 264, 2824, 16459, 11], "temperature": 0.0, "avg_logprob": -0.18257380013514046, "compression_ratio": 1.7016129032258065, "no_speech_prob": 1.3419466085906606e-05}, {"id": 129, "seek": 56708, "start": 585.44, "end": 590.9200000000001, "text": " United States President, blah, said, now that would be a really good language model because", "tokens": [2824, 3040, 3117, 11, 12288, 11, 848, 11, 586, 300, 576, 312, 257, 534, 665, 2856, 2316, 570], "temperature": 0.0, "avg_logprob": -0.18257380013514046, "compression_ratio": 1.7016129032258065, "no_speech_prob": 1.3419466085906606e-05}, {"id": 130, "seek": 56708, "start": 590.9200000000001, "end": 595.24, "text": " it would actually have to know who was the United States President in that year.", "tokens": [309, 576, 767, 362, 281, 458, 567, 390, 264, 2824, 3040, 3117, 294, 300, 1064, 13], "temperature": 0.0, "avg_logprob": -0.18257380013514046, "compression_ratio": 1.7016129032258065, "no_speech_prob": 1.3419466085906606e-05}, {"id": 131, "seek": 59524, "start": 595.24, "end": 600.96, "text": " So getting really good at training language models is a great way to learn a lot about", "tokens": [407, 1242, 534, 665, 412, 3097, 2856, 5245, 307, 257, 869, 636, 281, 1466, 257, 688, 466], "temperature": 0.0, "avg_logprob": -0.1270029938336715, "compression_ratio": 1.8240343347639485, "no_speech_prob": 1.4285007637226954e-05}, {"id": 132, "seek": 59524, "start": 600.96, "end": 607.8, "text": " or teach a neural net a lot about what is our world, what's in our world, how do things", "tokens": [420, 2924, 257, 18161, 2533, 257, 688, 466, 437, 307, 527, 1002, 11, 437, 311, 294, 527, 1002, 11, 577, 360, 721], "temperature": 0.0, "avg_logprob": -0.1270029938336715, "compression_ratio": 1.8240343347639485, "no_speech_prob": 1.4285007637226954e-05}, {"id": 133, "seek": 59524, "start": 607.8, "end": 609.12, "text": " work in our world.", "tokens": [589, 294, 527, 1002, 13], "temperature": 0.0, "avg_logprob": -0.1270029938336715, "compression_ratio": 1.8240343347639485, "no_speech_prob": 1.4285007637226954e-05}, {"id": 134, "seek": 59524, "start": 609.12, "end": 614.16, "text": " So it's a really fascinating topic and it's actually one that philosophers have been studying", "tokens": [407, 309, 311, 257, 534, 10343, 4829, 293, 309, 311, 767, 472, 300, 36839, 362, 668, 7601], "temperature": 0.0, "avg_logprob": -0.1270029938336715, "compression_ratio": 1.8240343347639485, "no_speech_prob": 1.4285007637226954e-05}, {"id": 135, "seek": 59524, "start": 614.16, "end": 615.8, "text": " for hundreds of years now.", "tokens": [337, 6779, 295, 924, 586, 13], "temperature": 0.0, "avg_logprob": -0.1270029938336715, "compression_ratio": 1.8240343347639485, "no_speech_prob": 1.4285007637226954e-05}, {"id": 136, "seek": 59524, "start": 615.8, "end": 622.0, "text": " There's actually a whole theory of philosophy which is about what can be learned from studying", "tokens": [821, 311, 767, 257, 1379, 5261, 295, 10675, 597, 307, 466, 437, 393, 312, 3264, 490, 7601], "temperature": 0.0, "avg_logprob": -0.1270029938336715, "compression_ratio": 1.8240343347639485, "no_speech_prob": 1.4285007637226954e-05}, {"id": 137, "seek": 59524, "start": 622.0, "end": 624.52, "text": " language alone.", "tokens": [2856, 3312, 13], "temperature": 0.0, "avg_logprob": -0.1270029938336715, "compression_ratio": 1.8240343347639485, "no_speech_prob": 1.4285007637226954e-05}, {"id": 138, "seek": 62452, "start": 624.52, "end": 627.52, "text": " So it turns out empirically quite a lot.", "tokens": [407, 309, 4523, 484, 25790, 984, 1596, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.12305091857910157, "compression_ratio": 1.6745762711864407, "no_speech_prob": 9.818068065214902e-06}, {"id": 139, "seek": 62452, "start": 627.52, "end": 630.36, "text": " And so here's the interesting thing, you can start by training a language model on all", "tokens": [400, 370, 510, 311, 264, 1880, 551, 11, 291, 393, 722, 538, 3097, 257, 2856, 2316, 322, 439], "temperature": 0.0, "avg_logprob": -0.12305091857910157, "compression_ratio": 1.6745762711864407, "no_speech_prob": 9.818068065214902e-06}, {"id": 140, "seek": 62452, "start": 630.36, "end": 634.3199999999999, "text": " of Wikipedia and then we can make that available to all of you.", "tokens": [295, 28999, 293, 550, 321, 393, 652, 300, 2435, 281, 439, 295, 291, 13], "temperature": 0.0, "avg_logprob": -0.12305091857910157, "compression_ratio": 1.6745762711864407, "no_speech_prob": 9.818068065214902e-06}, {"id": 141, "seek": 62452, "start": 634.3199999999999, "end": 638.24, "text": " Just like a pre-trained ImageNet model for vision, we've now made available a pre-trained", "tokens": [1449, 411, 257, 659, 12, 17227, 2001, 29903, 31890, 2316, 337, 5201, 11, 321, 600, 586, 1027, 2435, 257, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.12305091857910157, "compression_ratio": 1.6745762711864407, "no_speech_prob": 9.818068065214902e-06}, {"id": 142, "seek": 62452, "start": 638.24, "end": 641.4, "text": " Wiki text model for NLP.", "tokens": [35892, 2487, 2316, 337, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.12305091857910157, "compression_ratio": 1.6745762711864407, "no_speech_prob": 9.818068065214902e-06}, {"id": 143, "seek": 62452, "start": 641.4, "end": 646.12, "text": " Not because it's particularly useful of itself, predicting the next word of sentences is somewhat", "tokens": [1726, 570, 309, 311, 4098, 4420, 295, 2564, 11, 32884, 264, 958, 1349, 295, 16579, 307, 8344], "temperature": 0.0, "avg_logprob": -0.12305091857910157, "compression_ratio": 1.6745762711864407, "no_speech_prob": 9.818068065214902e-06}, {"id": 144, "seek": 62452, "start": 646.12, "end": 652.66, "text": " useful but not normally what we want to do, but it tells us it's a model that understands", "tokens": [4420, 457, 406, 5646, 437, 321, 528, 281, 360, 11, 457, 309, 5112, 505, 309, 311, 257, 2316, 300, 15146], "temperature": 0.0, "avg_logprob": -0.12305091857910157, "compression_ratio": 1.6745762711864407, "no_speech_prob": 9.818068065214902e-06}, {"id": 145, "seek": 65266, "start": 652.66, "end": 656.7199999999999, "text": " a lot about language and a lot about what language describes.", "tokens": [257, 688, 466, 2856, 293, 257, 688, 466, 437, 2856, 15626, 13], "temperature": 0.0, "avg_logprob": -0.06802845001220703, "compression_ratio": 2.036082474226804, "no_speech_prob": 2.6425570922583574e-06}, {"id": 146, "seek": 65266, "start": 656.7199999999999, "end": 664.4, "text": " So then we can take that and we can do transfer learning to create a new language model that's", "tokens": [407, 550, 321, 393, 747, 300, 293, 321, 393, 360, 5003, 2539, 281, 1884, 257, 777, 2856, 2316, 300, 311], "temperature": 0.0, "avg_logprob": -0.06802845001220703, "compression_ratio": 2.036082474226804, "no_speech_prob": 2.6425570922583574e-06}, {"id": 147, "seek": 65266, "start": 664.4, "end": 670.9, "text": " specifically good at predicting the next word of movie reviews.", "tokens": [4682, 665, 412, 32884, 264, 958, 1349, 295, 3169, 10229, 13], "temperature": 0.0, "avg_logprob": -0.06802845001220703, "compression_ratio": 2.036082474226804, "no_speech_prob": 2.6425570922583574e-06}, {"id": 148, "seek": 65266, "start": 670.9, "end": 675.4, "text": " So if we can build a language model that's good at predicting the next word of movie", "tokens": [407, 498, 321, 393, 1322, 257, 2856, 2316, 300, 311, 665, 412, 32884, 264, 958, 1349, 295, 3169], "temperature": 0.0, "avg_logprob": -0.06802845001220703, "compression_ratio": 2.036082474226804, "no_speech_prob": 2.6425570922583574e-06}, {"id": 149, "seek": 65266, "start": 675.4, "end": 682.12, "text": " reviews pre-trained with the Wiki text model, then that's going to understand a lot about", "tokens": [10229, 659, 12, 17227, 2001, 365, 264, 35892, 2487, 2316, 11, 550, 300, 311, 516, 281, 1223, 257, 688, 466], "temperature": 0.0, "avg_logprob": -0.06802845001220703, "compression_ratio": 2.036082474226804, "no_speech_prob": 2.6425570922583574e-06}, {"id": 150, "seek": 68212, "start": 682.12, "end": 687.92, "text": " my favorite actor is Tom Who.", "tokens": [452, 2954, 8747, 307, 5041, 2102, 13], "temperature": 0.0, "avg_logprob": -0.16600724388571347, "compression_ratio": 1.610091743119266, "no_speech_prob": 3.7052028346806765e-05}, {"id": 151, "seek": 68212, "start": 687.92, "end": 693.96, "text": " I thought the photography was fantastic but I wasn't really so happy about the director.", "tokens": [286, 1194, 264, 13865, 390, 5456, 457, 286, 2067, 380, 534, 370, 2055, 466, 264, 5391, 13], "temperature": 0.0, "avg_logprob": -0.16600724388571347, "compression_ratio": 1.610091743119266, "no_speech_prob": 3.7052028346806765e-05}, {"id": 152, "seek": 68212, "start": 693.96, "end": 698.8, "text": " It's going to learn a lot about specifically how movie reviews are written.", "tokens": [467, 311, 516, 281, 1466, 257, 688, 466, 4682, 577, 3169, 10229, 366, 3720, 13], "temperature": 0.0, "avg_logprob": -0.16600724388571347, "compression_ratio": 1.610091743119266, "no_speech_prob": 3.7052028346806765e-05}, {"id": 153, "seek": 68212, "start": 698.8, "end": 704.34, "text": " It'll even learn things like what are the names of some popular movies.", "tokens": [467, 603, 754, 1466, 721, 411, 437, 366, 264, 5288, 295, 512, 3743, 6233, 13], "temperature": 0.0, "avg_logprob": -0.16600724388571347, "compression_ratio": 1.610091743119266, "no_speech_prob": 3.7052028346806765e-05}, {"id": 154, "seek": 68212, "start": 704.34, "end": 709.88, "text": " So that would then mean we can still use a huge corpus of lots of movie reviews even", "tokens": [407, 300, 576, 550, 914, 321, 393, 920, 764, 257, 2603, 1181, 31624, 295, 3195, 295, 3169, 10229, 754], "temperature": 0.0, "avg_logprob": -0.16600724388571347, "compression_ratio": 1.610091743119266, "no_speech_prob": 3.7052028346806765e-05}, {"id": 155, "seek": 70988, "start": 709.88, "end": 713.8, "text": " if we don't know whether they're positive or negative to learn a lot about how movie", "tokens": [498, 321, 500, 380, 458, 1968, 436, 434, 3353, 420, 3671, 281, 1466, 257, 688, 466, 577, 3169], "temperature": 0.0, "avg_logprob": -0.1357632483635749, "compression_ratio": 1.7088607594936709, "no_speech_prob": 5.014685484638903e-06}, {"id": 156, "seek": 70988, "start": 713.8, "end": 714.8, "text": " reviews are written.", "tokens": [10229, 366, 3720, 13], "temperature": 0.0, "avg_logprob": -0.1357632483635749, "compression_ratio": 1.7088607594936709, "no_speech_prob": 5.014685484638903e-06}, {"id": 157, "seek": 70988, "start": 714.8, "end": 718.0, "text": " So for all of this pre-training and all of this language model fine-tuning, we don't", "tokens": [407, 337, 439, 295, 341, 659, 12, 17227, 1760, 293, 439, 295, 341, 2856, 2316, 2489, 12, 83, 37726, 11, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.1357632483635749, "compression_ratio": 1.7088607594936709, "no_speech_prob": 5.014685484638903e-06}, {"id": 158, "seek": 70988, "start": 718.0, "end": 720.0, "text": " need any labels at all.", "tokens": [643, 604, 16949, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.1357632483635749, "compression_ratio": 1.7088607594936709, "no_speech_prob": 5.014685484638903e-06}, {"id": 159, "seek": 70988, "start": 720.0, "end": 724.84, "text": " It's what the researcher Jan LeCun calls self-supervised learning.", "tokens": [467, 311, 437, 264, 21751, 4956, 1456, 34, 409, 5498, 2698, 12, 48172, 24420, 2539, 13], "temperature": 0.0, "avg_logprob": -0.1357632483635749, "compression_ratio": 1.7088607594936709, "no_speech_prob": 5.014685484638903e-06}, {"id": 160, "seek": 70988, "start": 724.84, "end": 727.36, "text": " In other words, it's a classic supervised model.", "tokens": [682, 661, 2283, 11, 309, 311, 257, 7230, 46533, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1357632483635749, "compression_ratio": 1.7088607594936709, "no_speech_prob": 5.014685484638903e-06}, {"id": 161, "seek": 70988, "start": 727.36, "end": 731.52, "text": " We have labels, but the labels are not things that somebody else has created.", "tokens": [492, 362, 16949, 11, 457, 264, 16949, 366, 406, 721, 300, 2618, 1646, 575, 2942, 13], "temperature": 0.0, "avg_logprob": -0.1357632483635749, "compression_ratio": 1.7088607594936709, "no_speech_prob": 5.014685484638903e-06}, {"id": 162, "seek": 70988, "start": 731.52, "end": 734.4, "text": " They're kind of built into the data set itself.", "tokens": [814, 434, 733, 295, 3094, 666, 264, 1412, 992, 2564, 13], "temperature": 0.0, "avg_logprob": -0.1357632483635749, "compression_ratio": 1.7088607594936709, "no_speech_prob": 5.014685484638903e-06}, {"id": 163, "seek": 70988, "start": 734.4, "end": 737.8, "text": " So this is really, really neat because at this point we've now got something that's", "tokens": [407, 341, 307, 534, 11, 534, 10654, 570, 412, 341, 935, 321, 600, 586, 658, 746, 300, 311], "temperature": 0.0, "avg_logprob": -0.1357632483635749, "compression_ratio": 1.7088607594936709, "no_speech_prob": 5.014685484638903e-06}, {"id": 164, "seek": 73780, "start": 737.8, "end": 743.68, "text": " good at understanding movie reviews and we can fine-tune that with transfer learning", "tokens": [665, 412, 3701, 3169, 10229, 293, 321, 393, 2489, 12, 83, 2613, 300, 365, 5003, 2539], "temperature": 0.0, "avg_logprob": -0.19721412658691406, "compression_ratio": 1.6294820717131475, "no_speech_prob": 9.516058526060078e-06}, {"id": 165, "seek": 73780, "start": 743.68, "end": 748.24, "text": " to do the thing we want to do, which in this case is to classify movie reviews to be positive", "tokens": [281, 360, 264, 551, 321, 528, 281, 360, 11, 597, 294, 341, 1389, 307, 281, 33872, 3169, 10229, 281, 312, 3353], "temperature": 0.0, "avg_logprob": -0.19721412658691406, "compression_ratio": 1.6294820717131475, "no_speech_prob": 9.516058526060078e-06}, {"id": 166, "seek": 73780, "start": 748.24, "end": 749.4799999999999, "text": " or negative.", "tokens": [420, 3671, 13], "temperature": 0.0, "avg_logprob": -0.19721412658691406, "compression_ratio": 1.6294820717131475, "no_speech_prob": 9.516058526060078e-06}, {"id": 167, "seek": 73780, "start": 749.4799999999999, "end": 755.3599999999999, "text": " And so my hope was when I tried this last year that at that point 25,000 ones and zeros", "tokens": [400, 370, 452, 1454, 390, 562, 286, 3031, 341, 1036, 1064, 300, 412, 300, 935, 3552, 11, 1360, 2306, 293, 35193], "temperature": 0.0, "avg_logprob": -0.19721412658691406, "compression_ratio": 1.6294820717131475, "no_speech_prob": 9.516058526060078e-06}, {"id": 168, "seek": 73780, "start": 755.3599999999999, "end": 758.52, "text": " would be enough feedback to fine-tune that model.", "tokens": [576, 312, 1547, 5824, 281, 2489, 12, 83, 2613, 300, 2316, 13], "temperature": 0.0, "avg_logprob": -0.19721412658691406, "compression_ratio": 1.6294820717131475, "no_speech_prob": 9.516058526060078e-06}, {"id": 169, "seek": 73780, "start": 758.52, "end": 761.8399999999999, "text": " And it turned out it absolutely was.", "tokens": [400, 309, 3574, 484, 309, 3122, 390, 13], "temperature": 0.0, "avg_logprob": -0.19721412658691406, "compression_ratio": 1.6294820717131475, "no_speech_prob": 9.516058526060078e-06}, {"id": 170, "seek": 73780, "start": 761.8399999999999, "end": 767.76, "text": " Alright, Rachel, let's go with a question.", "tokens": [2798, 11, 14246, 11, 718, 311, 352, 365, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.19721412658691406, "compression_ratio": 1.6294820717131475, "no_speech_prob": 9.516058526060078e-06}, {"id": 171, "seek": 76776, "start": 767.76, "end": 772.3199999999999, "text": " Does the language model approach work for text in forums that are informal English,", "tokens": [4402, 264, 2856, 2316, 3109, 589, 337, 2487, 294, 26998, 300, 366, 24342, 3669, 11], "temperature": 0.0, "avg_logprob": -0.2549717506665862, "compression_ratio": 1.4954954954954955, "no_speech_prob": 3.5005859899683855e-06}, {"id": 172, "seek": 76776, "start": 772.3199999999999, "end": 779.0, "text": " misspelled words or slang or short form like S6 instead of Samsung S6?", "tokens": [1713, 33000, 2283, 420, 42517, 420, 2099, 1254, 411, 318, 21, 2602, 295, 13173, 318, 21, 30], "temperature": 0.0, "avg_logprob": -0.2549717506665862, "compression_ratio": 1.4954954954954955, "no_speech_prob": 3.5005859899683855e-06}, {"id": 173, "seek": 76776, "start": 779.0, "end": 783.48, "text": " Yes, absolutely it does.", "tokens": [1079, 11, 3122, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.2549717506665862, "compression_ratio": 1.4954954954954955, "no_speech_prob": 3.5005859899683855e-06}, {"id": 174, "seek": 76776, "start": 783.48, "end": 789.04, "text": " Particularly if you start with your WikiText model and then fine-tune it with your, we", "tokens": [32281, 498, 291, 722, 365, 428, 35892, 50198, 2316, 293, 550, 2489, 12, 83, 2613, 309, 365, 428, 11, 321], "temperature": 0.0, "avg_logprob": -0.2549717506665862, "compression_ratio": 1.4954954954954955, "no_speech_prob": 3.5005859899683855e-06}, {"id": 175, "seek": 76776, "start": 789.04, "end": 790.4399999999999, "text": " call it the target corpus.", "tokens": [818, 309, 264, 3779, 1181, 31624, 13], "temperature": 0.0, "avg_logprob": -0.2549717506665862, "compression_ratio": 1.4954954954954955, "no_speech_prob": 3.5005859899683855e-06}, {"id": 176, "seek": 76776, "start": 790.4399999999999, "end": 793.4399999999999, "text": " A corpus is just a bunch of documents.", "tokens": [316, 1181, 31624, 307, 445, 257, 3840, 295, 8512, 13], "temperature": 0.0, "avg_logprob": -0.2549717506665862, "compression_ratio": 1.4954954954954955, "no_speech_prob": 3.5005859899683855e-06}, {"id": 177, "seek": 79344, "start": 793.44, "end": 799.0400000000001, "text": " It could be emails or tweets or medical reports or whatever.", "tokens": [467, 727, 312, 12524, 420, 25671, 420, 4625, 7122, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.09920958316687381, "compression_ratio": 1.65234375, "no_speech_prob": 9.223156666848809e-06}, {"id": 178, "seek": 79344, "start": 799.0400000000001, "end": 806.4000000000001, "text": " So you could fine-tune it so it can learn a bit about the specifics of the slang or", "tokens": [407, 291, 727, 2489, 12, 83, 2613, 309, 370, 309, 393, 1466, 257, 857, 466, 264, 28454, 295, 264, 42517, 420], "temperature": 0.0, "avg_logprob": -0.09920958316687381, "compression_ratio": 1.65234375, "no_speech_prob": 9.223156666848809e-06}, {"id": 179, "seek": 79344, "start": 806.4000000000001, "end": 810.12, "text": " abbreviations or whatever that didn't appear in the full corpus.", "tokens": [35839, 763, 420, 2035, 300, 994, 380, 4204, 294, 264, 1577, 1181, 31624, 13], "temperature": 0.0, "avg_logprob": -0.09920958316687381, "compression_ratio": 1.65234375, "no_speech_prob": 9.223156666848809e-06}, {"id": 180, "seek": 79344, "start": 810.12, "end": 814.0400000000001, "text": " And so interestingly, this is one of the big things that people were surprised about when", "tokens": [400, 370, 25873, 11, 341, 307, 472, 295, 264, 955, 721, 300, 561, 645, 6100, 466, 562], "temperature": 0.0, "avg_logprob": -0.09920958316687381, "compression_ratio": 1.65234375, "no_speech_prob": 9.223156666848809e-06}, {"id": 181, "seek": 79344, "start": 814.0400000000001, "end": 815.96, "text": " we did this research last year.", "tokens": [321, 630, 341, 2132, 1036, 1064, 13], "temperature": 0.0, "avg_logprob": -0.09920958316687381, "compression_ratio": 1.65234375, "no_speech_prob": 9.223156666848809e-06}, {"id": 182, "seek": 79344, "start": 815.96, "end": 821.48, "text": " People thought that learning from something like Wikipedia wouldn't be that helpful because", "tokens": [3432, 1194, 300, 2539, 490, 746, 411, 28999, 2759, 380, 312, 300, 4961, 570], "temperature": 0.0, "avg_logprob": -0.09920958316687381, "compression_ratio": 1.65234375, "no_speech_prob": 9.223156666848809e-06}, {"id": 183, "seek": 82148, "start": 821.48, "end": 824.4, "text": " it's not that representative of how people tend to write.", "tokens": [309, 311, 406, 300, 12424, 295, 577, 561, 3928, 281, 2464, 13], "temperature": 0.0, "avg_logprob": -0.131372439066569, "compression_ratio": 1.558252427184466, "no_speech_prob": 7.5278740041540004e-06}, {"id": 184, "seek": 82148, "start": 824.4, "end": 829.8000000000001, "text": " But it turns out it's extremely helpful because there's a much bigger difference between Wikipedia", "tokens": [583, 309, 4523, 484, 309, 311, 4664, 4961, 570, 456, 311, 257, 709, 3801, 2649, 1296, 28999], "temperature": 0.0, "avg_logprob": -0.131372439066569, "compression_ratio": 1.558252427184466, "no_speech_prob": 7.5278740041540004e-06}, {"id": 185, "seek": 82148, "start": 829.8000000000001, "end": 835.6800000000001, "text": " and random words than there is between Wikipedia and Reddit.", "tokens": [293, 4974, 2283, 813, 456, 307, 1296, 28999, 293, 32210, 13], "temperature": 0.0, "avg_logprob": -0.131372439066569, "compression_ratio": 1.558252427184466, "no_speech_prob": 7.5278740041540004e-06}, {"id": 186, "seek": 82148, "start": 835.6800000000001, "end": 842.24, "text": " So it kind of gets you 99% of the way there.", "tokens": [407, 309, 733, 295, 2170, 291, 11803, 4, 295, 264, 636, 456, 13], "temperature": 0.0, "avg_logprob": -0.131372439066569, "compression_ratio": 1.558252427184466, "no_speech_prob": 7.5278740041540004e-06}, {"id": 187, "seek": 82148, "start": 842.24, "end": 846.2, "text": " So these language models themselves can be quite powerful.", "tokens": [407, 613, 2856, 5245, 2969, 393, 312, 1596, 4005, 13], "temperature": 0.0, "avg_logprob": -0.131372439066569, "compression_ratio": 1.558252427184466, "no_speech_prob": 7.5278740041540004e-06}, {"id": 188, "seek": 84620, "start": 846.2, "end": 855.0, "text": " So for example there was a blog post from, what are they called, SwiftKey?", "tokens": [407, 337, 1365, 456, 390, 257, 6968, 2183, 490, 11, 437, 366, 436, 1219, 11, 25539, 42, 2030, 30], "temperature": 0.0, "avg_logprob": -0.18766773768833706, "compression_ratio": 1.6472868217054264, "no_speech_prob": 6.43892417429015e-06}, {"id": 189, "seek": 84620, "start": 855.0, "end": 859.96, "text": " The folks that do the mobile phone predictive text keyboard and they described how they", "tokens": [440, 4024, 300, 360, 264, 6013, 2593, 35521, 2487, 10186, 293, 436, 7619, 577, 436], "temperature": 0.0, "avg_logprob": -0.18766773768833706, "compression_ratio": 1.6472868217054264, "no_speech_prob": 6.43892417429015e-06}, {"id": 190, "seek": 84620, "start": 859.96, "end": 866.2800000000001, "text": " kind of rewrote their underlying model to use neural nets.", "tokens": [733, 295, 319, 7449, 1370, 641, 14217, 2316, 281, 764, 18161, 36170, 13], "temperature": 0.0, "avg_logprob": -0.18766773768833706, "compression_ratio": 1.6472868217054264, "no_speech_prob": 6.43892417429015e-06}, {"id": 191, "seek": 84620, "start": 866.2800000000001, "end": 870.36, "text": " So this was a year or two ago, now most phone keyboards seem to do this.", "tokens": [407, 341, 390, 257, 1064, 420, 732, 2057, 11, 586, 881, 2593, 47808, 1643, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.18766773768833706, "compression_ratio": 1.6472868217054264, "no_speech_prob": 6.43892417429015e-06}, {"id": 192, "seek": 84620, "start": 870.36, "end": 873.76, "text": " You'll be typing away on your mobile phone and in the predictions there'll be something", "tokens": [509, 603, 312, 18444, 1314, 322, 428, 6013, 2593, 293, 294, 264, 21264, 456, 603, 312, 746], "temperature": 0.0, "avg_logprob": -0.18766773768833706, "compression_ratio": 1.6472868217054264, "no_speech_prob": 6.43892417429015e-06}, {"id": 193, "seek": 84620, "start": 873.76, "end": 875.88, "text": " telling you what word you might want next.", "tokens": [3585, 291, 437, 1349, 291, 1062, 528, 958, 13], "temperature": 0.0, "avg_logprob": -0.18766773768833706, "compression_ratio": 1.6472868217054264, "no_speech_prob": 6.43892417429015e-06}, {"id": 194, "seek": 87588, "start": 875.88, "end": 879.4, "text": " So that's a language model in your phone.", "tokens": [407, 300, 311, 257, 2856, 2316, 294, 428, 2593, 13], "temperature": 0.0, "avg_logprob": -0.21889986937073455, "compression_ratio": 1.639269406392694, "no_speech_prob": 9.818104445002973e-06}, {"id": 195, "seek": 87588, "start": 879.4, "end": 885.92, "text": " Another example was the researcher Andre Kapathy who now runs all this stuff at Tesla.", "tokens": [3996, 1365, 390, 264, 21751, 20667, 21216, 9527, 567, 586, 6676, 439, 341, 1507, 412, 13666, 13], "temperature": 0.0, "avg_logprob": -0.21889986937073455, "compression_ratio": 1.639269406392694, "no_speech_prob": 9.818104445002973e-06}, {"id": 196, "seek": 87588, "start": 885.92, "end": 893.1, "text": " Back when he was a PhD student, he created a language model of text in LaTeX documents", "tokens": [5833, 562, 415, 390, 257, 14476, 3107, 11, 415, 2942, 257, 2856, 2316, 295, 2487, 294, 2369, 14233, 55, 8512], "temperature": 0.0, "avg_logprob": -0.21889986937073455, "compression_ratio": 1.639269406392694, "no_speech_prob": 9.818104445002973e-06}, {"id": 197, "seek": 87588, "start": 893.1, "end": 898.88, "text": " and created these automatic generation of LaTeX documents that then became these kind", "tokens": [293, 2942, 613, 12509, 5125, 295, 2369, 14233, 55, 8512, 300, 550, 3062, 613, 733], "temperature": 0.0, "avg_logprob": -0.21889986937073455, "compression_ratio": 1.639269406392694, "no_speech_prob": 9.818104445002973e-06}, {"id": 198, "seek": 87588, "start": 898.88, "end": 900.36, "text": " of automatically generated papers.", "tokens": [295, 6772, 10833, 10577, 13], "temperature": 0.0, "avg_logprob": -0.21889986937073455, "compression_ratio": 1.639269406392694, "no_speech_prob": 9.818104445002973e-06}, {"id": 199, "seek": 87588, "start": 900.36, "end": 902.72, "text": " So that's pretty cute.", "tokens": [407, 300, 311, 1238, 4052, 13], "temperature": 0.0, "avg_logprob": -0.21889986937073455, "compression_ratio": 1.639269406392694, "no_speech_prob": 9.818104445002973e-06}, {"id": 200, "seek": 90272, "start": 902.72, "end": 907.08, "text": " So we're not really that interested in the output of the language model ourselves, we're", "tokens": [407, 321, 434, 406, 534, 300, 3102, 294, 264, 5598, 295, 264, 2856, 2316, 4175, 11, 321, 434], "temperature": 0.0, "avg_logprob": -0.14431485351251097, "compression_ratio": 1.6218487394957983, "no_speech_prob": 6.04883598498418e-06}, {"id": 201, "seek": 90272, "start": 907.08, "end": 913.6, "text": " just interested in it because it's helpful with this process.", "tokens": [445, 3102, 294, 309, 570, 309, 311, 4961, 365, 341, 1399, 13], "temperature": 0.0, "avg_logprob": -0.14431485351251097, "compression_ratio": 1.6218487394957983, "no_speech_prob": 6.04883598498418e-06}, {"id": 202, "seek": 90272, "start": 913.6, "end": 918.08, "text": " So we briefly looked at the process last week.", "tokens": [407, 321, 10515, 2956, 412, 264, 1399, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.14431485351251097, "compression_ratio": 1.6218487394957983, "no_speech_prob": 6.04883598498418e-06}, {"id": 203, "seek": 90272, "start": 918.08, "end": 921.0400000000001, "text": " So let's just have a reminder.", "tokens": [407, 718, 311, 445, 362, 257, 13548, 13], "temperature": 0.0, "avg_logprob": -0.14431485351251097, "compression_ratio": 1.6218487394957983, "no_speech_prob": 6.04883598498418e-06}, {"id": 204, "seek": 90272, "start": 921.0400000000001, "end": 927.12, "text": " The basic process is we're going to start with the data in some format.", "tokens": [440, 3875, 1399, 307, 321, 434, 516, 281, 722, 365, 264, 1412, 294, 512, 7877, 13], "temperature": 0.0, "avg_logprob": -0.14431485351251097, "compression_ratio": 1.6218487394957983, "no_speech_prob": 6.04883598498418e-06}, {"id": 205, "seek": 90272, "start": 927.12, "end": 931.44, "text": " So for example we've prepared a little IMDB sample that you can use where it's in CSV", "tokens": [407, 337, 1365, 321, 600, 4927, 257, 707, 21463, 27735, 6889, 300, 291, 393, 764, 689, 309, 311, 294, 48814], "temperature": 0.0, "avg_logprob": -0.14431485351251097, "compression_ratio": 1.6218487394957983, "no_speech_prob": 6.04883598498418e-06}, {"id": 206, "seek": 93144, "start": 931.44, "end": 937.12, "text": " file so you can read it in with pandas and see there's negative or positive, the text", "tokens": [3991, 370, 291, 393, 1401, 309, 294, 365, 4565, 296, 293, 536, 456, 311, 3671, 420, 3353, 11, 264, 2487], "temperature": 0.0, "avg_logprob": -0.10956660324965066, "compression_ratio": 1.7727272727272727, "no_speech_prob": 2.9771385015919805e-05}, {"id": 207, "seek": 93144, "start": 937.12, "end": 942.6800000000001, "text": " of each movie review, and a boolean of is it in the validation set or the training set.", "tokens": [295, 1184, 3169, 3131, 11, 293, 257, 748, 4812, 282, 295, 307, 309, 294, 264, 24071, 992, 420, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.10956660324965066, "compression_ratio": 1.7727272727272727, "no_speech_prob": 2.9771385015919805e-05}, {"id": 208, "seek": 93144, "start": 942.6800000000001, "end": 945.2800000000001, "text": " So there's an example of a movie review.", "tokens": [407, 456, 311, 364, 1365, 295, 257, 3169, 3131, 13], "temperature": 0.0, "avg_logprob": -0.10956660324965066, "compression_ratio": 1.7727272727272727, "no_speech_prob": 2.9771385015919805e-05}, {"id": 209, "seek": 93144, "start": 945.2800000000001, "end": 951.12, "text": " And so you can just go text data bunch from CSV to grab a language model specific data", "tokens": [400, 370, 291, 393, 445, 352, 2487, 1412, 3840, 490, 48814, 281, 4444, 257, 2856, 2316, 2685, 1412], "temperature": 0.0, "avg_logprob": -0.10956660324965066, "compression_ratio": 1.7727272727272727, "no_speech_prob": 2.9771385015919805e-05}, {"id": 210, "seek": 93144, "start": 951.12, "end": 956.32, "text": " bunch and then you can create a learner from that in the usual way and fit it.", "tokens": [3840, 293, 550, 291, 393, 1884, 257, 33347, 490, 300, 294, 264, 7713, 636, 293, 3318, 309, 13], "temperature": 0.0, "avg_logprob": -0.10956660324965066, "compression_ratio": 1.7727272727272727, "no_speech_prob": 2.9771385015919805e-05}, {"id": 211, "seek": 93144, "start": 956.32, "end": 961.2, "text": " You can save the data bunch which means that the pre-processing that is done, you don't", "tokens": [509, 393, 3155, 264, 1412, 3840, 597, 1355, 300, 264, 659, 12, 41075, 278, 300, 307, 1096, 11, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.10956660324965066, "compression_ratio": 1.7727272727272727, "no_speech_prob": 2.9771385015919805e-05}, {"id": 212, "seek": 96120, "start": 961.2, "end": 966.1600000000001, "text": " have to do it again, you can just load it.", "tokens": [362, 281, 360, 309, 797, 11, 291, 393, 445, 3677, 309, 13], "temperature": 0.0, "avg_logprob": -0.11830646068126231, "compression_ratio": 1.7016129032258065, "no_speech_prob": 7.071817890391685e-06}, {"id": 213, "seek": 96120, "start": 966.1600000000001, "end": 968.5200000000001, "text": " So what goes on behind the scenes?", "tokens": [407, 437, 1709, 322, 2261, 264, 8026, 30], "temperature": 0.0, "avg_logprob": -0.11830646068126231, "compression_ratio": 1.7016129032258065, "no_speech_prob": 7.071817890391685e-06}, {"id": 214, "seek": 96120, "start": 968.5200000000001, "end": 973.24, "text": " Well what happens behind the scenes if we now load it as a classification data bunch,", "tokens": [1042, 437, 2314, 2261, 264, 8026, 498, 321, 586, 3677, 309, 382, 257, 21538, 1412, 3840, 11], "temperature": 0.0, "avg_logprob": -0.11830646068126231, "compression_ratio": 1.7016129032258065, "no_speech_prob": 7.071817890391685e-06}, {"id": 215, "seek": 96120, "start": 973.24, "end": 979.0, "text": " that's going to allow us to see the labels as well, then as we described it basically", "tokens": [300, 311, 516, 281, 2089, 505, 281, 536, 264, 16949, 382, 731, 11, 550, 382, 321, 7619, 309, 1936], "temperature": 0.0, "avg_logprob": -0.11830646068126231, "compression_ratio": 1.7016129032258065, "no_speech_prob": 7.071817890391685e-06}, {"id": 216, "seek": 96120, "start": 979.0, "end": 985.4000000000001, "text": " creates a separate unit, we call it a token, for each separate part of a word.", "tokens": [7829, 257, 4994, 4985, 11, 321, 818, 309, 257, 14862, 11, 337, 1184, 4994, 644, 295, 257, 1349, 13], "temperature": 0.0, "avg_logprob": -0.11830646068126231, "compression_ratio": 1.7016129032258065, "no_speech_prob": 7.071817890391685e-06}, {"id": 217, "seek": 96120, "start": 985.4000000000001, "end": 990.1800000000001, "text": " So most of them are just four words but sometimes if it's like an apostrophe s from its it'll", "tokens": [407, 881, 295, 552, 366, 445, 1451, 2283, 457, 2171, 498, 309, 311, 411, 364, 19484, 27194, 262, 490, 1080, 309, 603], "temperature": 0.0, "avg_logprob": -0.11830646068126231, "compression_ratio": 1.7016129032258065, "no_speech_prob": 7.071817890391685e-06}, {"id": 218, "seek": 99018, "start": 990.18, "end": 996.88, "text": " get its own token, every bit of punctuation tends to get its own token like a comma or", "tokens": [483, 1080, 1065, 14862, 11, 633, 857, 295, 27006, 16073, 12258, 281, 483, 1080, 1065, 14862, 411, 257, 22117, 420], "temperature": 0.0, "avg_logprob": -0.1553900552832562, "compression_ratio": 1.672811059907834, "no_speech_prob": 1.723125205899123e-05}, {"id": 219, "seek": 99018, "start": 996.88, "end": 999.4399999999999, "text": " a full stop and so forth.", "tokens": [257, 1577, 1590, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1553900552832562, "compression_ratio": 1.672811059907834, "no_speech_prob": 1.723125205899123e-05}, {"id": 220, "seek": 99018, "start": 999.4399999999999, "end": 1009.8, "text": " And then the next thing that we do is a numericalization which is where we find what are all of the", "tokens": [400, 550, 264, 958, 551, 300, 321, 360, 307, 257, 29054, 2144, 597, 307, 689, 321, 915, 437, 366, 439, 295, 264], "temperature": 0.0, "avg_logprob": -0.1553900552832562, "compression_ratio": 1.672811059907834, "no_speech_prob": 1.723125205899123e-05}, {"id": 221, "seek": 99018, "start": 1009.8, "end": 1013.4, "text": " unique tokens that appear here and we create a big list of them.", "tokens": [3845, 22667, 300, 4204, 510, 293, 321, 1884, 257, 955, 1329, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.1553900552832562, "compression_ratio": 1.672811059907834, "no_speech_prob": 1.723125205899123e-05}, {"id": 222, "seek": 99018, "start": 1013.4, "end": 1018.64, "text": " Here's the first 10 in order of frequency and that big list of unique possible tokens", "tokens": [1692, 311, 264, 700, 1266, 294, 1668, 295, 7893, 293, 300, 955, 1329, 295, 3845, 1944, 22667], "temperature": 0.0, "avg_logprob": -0.1553900552832562, "compression_ratio": 1.672811059907834, "no_speech_prob": 1.723125205899123e-05}, {"id": 223, "seek": 101864, "start": 1018.64, "end": 1021.98, "text": " is called the vocabulary, we always call it vocab.", "tokens": [307, 1219, 264, 19864, 11, 321, 1009, 818, 309, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.19421043395996093, "compression_ratio": 1.5808080808080809, "no_speech_prob": 5.771887117589358e-06}, {"id": 224, "seek": 101864, "start": 1021.98, "end": 1029.96, "text": " And so what we then do is we replace the tokens with the ID of where is that token in the", "tokens": [400, 370, 437, 321, 550, 360, 307, 321, 7406, 264, 22667, 365, 264, 7348, 295, 689, 307, 300, 14862, 294, 264], "temperature": 0.0, "avg_logprob": -0.19421043395996093, "compression_ratio": 1.5808080808080809, "no_speech_prob": 5.771887117589358e-06}, {"id": 225, "seek": 101864, "start": 1029.96, "end": 1032.84, "text": " vocab and that's numericalization.", "tokens": [2329, 455, 293, 300, 311, 29054, 2144, 13], "temperature": 0.0, "avg_logprob": -0.19421043395996093, "compression_ratio": 1.5808080808080809, "no_speech_prob": 5.771887117589358e-06}, {"id": 226, "seek": 101864, "start": 1032.84, "end": 1042.32, "text": " Here's the thing though, as you'll learn every word in our vocab is going to require a separate", "tokens": [1692, 311, 264, 551, 1673, 11, 382, 291, 603, 1466, 633, 1349, 294, 527, 2329, 455, 307, 516, 281, 3651, 257, 4994], "temperature": 0.0, "avg_logprob": -0.19421043395996093, "compression_ratio": 1.5808080808080809, "no_speech_prob": 5.771887117589358e-06}, {"id": 227, "seek": 101864, "start": 1042.32, "end": 1046.04, "text": " row in a weight matrix in our neural net.", "tokens": [5386, 294, 257, 3364, 8141, 294, 527, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.19421043395996093, "compression_ratio": 1.5808080808080809, "no_speech_prob": 5.771887117589358e-06}, {"id": 228, "seek": 104604, "start": 1046.04, "end": 1052.6, "text": " And so to avoid that weight matrix getting too huge, we restrict the vocab to no more", "tokens": [400, 370, 281, 5042, 300, 3364, 8141, 1242, 886, 2603, 11, 321, 7694, 264, 2329, 455, 281, 572, 544], "temperature": 0.0, "avg_logprob": -0.1179603211423184, "compression_ratio": 1.5679611650485437, "no_speech_prob": 6.747987299604574e-06}, {"id": 229, "seek": 104604, "start": 1052.6, "end": 1058.8, "text": " than by default 60,000 words and if a word doesn't appear more than two times, we don't", "tokens": [813, 538, 7576, 4060, 11, 1360, 2283, 293, 498, 257, 1349, 1177, 380, 4204, 544, 813, 732, 1413, 11, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.1179603211423184, "compression_ratio": 1.5679611650485437, "no_speech_prob": 6.747987299604574e-06}, {"id": 230, "seek": 104604, "start": 1058.8, "end": 1060.3999999999999, "text": " put it in the vocab either.", "tokens": [829, 309, 294, 264, 2329, 455, 2139, 13], "temperature": 0.0, "avg_logprob": -0.1179603211423184, "compression_ratio": 1.5679611650485437, "no_speech_prob": 6.747987299604574e-06}, {"id": 231, "seek": 104604, "start": 1060.3999999999999, "end": 1065.96, "text": " So we kind of keep the vocab to a reasonable size in that way.", "tokens": [407, 321, 733, 295, 1066, 264, 2329, 455, 281, 257, 10585, 2744, 294, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.1179603211423184, "compression_ratio": 1.5679611650485437, "no_speech_prob": 6.747987299604574e-06}, {"id": 232, "seek": 104604, "start": 1065.96, "end": 1072.52, "text": " And so when you see these xx-unk, that's an unknown token.", "tokens": [400, 370, 562, 291, 536, 613, 2031, 87, 12, 3197, 11, 300, 311, 364, 9841, 14862, 13], "temperature": 0.0, "avg_logprob": -0.1179603211423184, "compression_ratio": 1.5679611650485437, "no_speech_prob": 6.747987299604574e-06}, {"id": 233, "seek": 107252, "start": 1072.52, "end": 1080.28, "text": " So when you see those unknown tokens, it just means this was something that was not a common", "tokens": [407, 562, 291, 536, 729, 9841, 22667, 11, 309, 445, 1355, 341, 390, 746, 300, 390, 406, 257, 2689], "temperature": 0.0, "avg_logprob": -0.15979944361318457, "compression_ratio": 1.5819672131147542, "no_speech_prob": 5.255358701106161e-06}, {"id": 234, "seek": 107252, "start": 1080.28, "end": 1083.6399999999999, "text": " enough word to appear in our vocab.", "tokens": [1547, 1349, 281, 4204, 294, 527, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.15979944361318457, "compression_ratio": 1.5819672131147542, "no_speech_prob": 5.255358701106161e-06}, {"id": 235, "seek": 107252, "start": 1083.6399999999999, "end": 1087.12, "text": " Okay, so there is the numericalized version.", "tokens": [1033, 11, 370, 456, 307, 264, 29054, 1602, 3037, 13], "temperature": 0.0, "avg_logprob": -0.15979944361318457, "compression_ratio": 1.5819672131147542, "no_speech_prob": 5.255358701106161e-06}, {"id": 236, "seek": 107252, "start": 1087.12, "end": 1092.2, "text": " We also have a couple of other special tokens like xx-field.", "tokens": [492, 611, 362, 257, 1916, 295, 661, 2121, 22667, 411, 2031, 87, 12, 7610, 13], "temperature": 0.0, "avg_logprob": -0.15979944361318457, "compression_ratio": 1.5819672131147542, "no_speech_prob": 5.255358701106161e-06}, {"id": 237, "seek": 107252, "start": 1092.2, "end": 1097.86, "text": " This is a special thing where if you've got like title, summary, abstract, body, like", "tokens": [639, 307, 257, 2121, 551, 689, 498, 291, 600, 658, 411, 4876, 11, 12691, 11, 12649, 11, 1772, 11, 411], "temperature": 0.0, "avg_logprob": -0.15979944361318457, "compression_ratio": 1.5819672131147542, "no_speech_prob": 5.255358701106161e-06}, {"id": 238, "seek": 107252, "start": 1097.86, "end": 1101.48, "text": " separate parts of a document, each one will get a separate field.", "tokens": [4994, 3166, 295, 257, 4166, 11, 1184, 472, 486, 483, 257, 4994, 2519, 13], "temperature": 0.0, "avg_logprob": -0.15979944361318457, "compression_ratio": 1.5819672131147542, "no_speech_prob": 5.255358701106161e-06}, {"id": 239, "seek": 110148, "start": 1101.48, "end": 1103.44, "text": " So they all get numbered.", "tokens": [407, 436, 439, 483, 40936, 13], "temperature": 0.0, "avg_logprob": -0.18311957655281857, "compression_ratio": 1.6404494382022472, "no_speech_prob": 9.818235412240028e-06}, {"id": 240, "seek": 110148, "start": 1103.44, "end": 1106.88, "text": " Also you'll find if there's something in all caps, it gets lower cased in a token called", "tokens": [2743, 291, 603, 915, 498, 456, 311, 746, 294, 439, 13855, 11, 309, 2170, 3126, 269, 1937, 294, 257, 14862, 1219], "temperature": 0.0, "avg_logprob": -0.18311957655281857, "compression_ratio": 1.6404494382022472, "no_speech_prob": 9.818235412240028e-06}, {"id": 241, "seek": 110148, "start": 1106.88, "end": 1110.88, "text": " xx-cap will get added to it.", "tokens": [2031, 87, 12, 9485, 486, 483, 3869, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.18311957655281857, "compression_ratio": 1.6404494382022472, "no_speech_prob": 9.818235412240028e-06}, {"id": 242, "seek": 110148, "start": 1110.88, "end": 1118.84, "text": " Personally, I more often use the data block API because you get, you kind of, there's", "tokens": [21079, 11, 286, 544, 2049, 764, 264, 1412, 3461, 9362, 570, 291, 483, 11, 291, 733, 295, 11, 456, 311], "temperature": 0.0, "avg_logprob": -0.18311957655281857, "compression_ratio": 1.6404494382022472, "no_speech_prob": 9.818235412240028e-06}, {"id": 243, "seek": 110148, "start": 1118.84, "end": 1123.04, "text": " less to remember about exactly what data bunch to use and what parameters and so forth and", "tokens": [1570, 281, 1604, 466, 2293, 437, 1412, 3840, 281, 764, 293, 437, 9834, 293, 370, 5220, 293], "temperature": 0.0, "avg_logprob": -0.18311957655281857, "compression_ratio": 1.6404494382022472, "no_speech_prob": 9.818235412240028e-06}, {"id": 244, "seek": 110148, "start": 1123.04, "end": 1125.24, "text": " it can be a bit more flexible.", "tokens": [309, 393, 312, 257, 857, 544, 11358, 13], "temperature": 0.0, "avg_logprob": -0.18311957655281857, "compression_ratio": 1.6404494382022472, "no_speech_prob": 9.818235412240028e-06}, {"id": 245, "seek": 110148, "start": 1125.24, "end": 1130.56, "text": " So another approach to doing this is to just decide what kind of list you're creating.", "tokens": [407, 1071, 3109, 281, 884, 341, 307, 281, 445, 4536, 437, 733, 295, 1329, 291, 434, 4084, 13], "temperature": 0.0, "avg_logprob": -0.18311957655281857, "compression_ratio": 1.6404494382022472, "no_speech_prob": 9.818235412240028e-06}, {"id": 246, "seek": 113056, "start": 1130.56, "end": 1132.44, "text": " So what's your independent variable?", "tokens": [407, 437, 311, 428, 6695, 7006, 30], "temperature": 0.0, "avg_logprob": -0.2124999306418679, "compression_ratio": 1.7257383966244726, "no_speech_prob": 1.260680437553674e-05}, {"id": 247, "seek": 113056, "start": 1132.44, "end": 1134.96, "text": " So in this case my independent variable is text.", "tokens": [407, 294, 341, 1389, 452, 6695, 7006, 307, 2487, 13], "temperature": 0.0, "avg_logprob": -0.2124999306418679, "compression_ratio": 1.7257383966244726, "no_speech_prob": 1.260680437553674e-05}, {"id": 248, "seek": 113056, "start": 1134.96, "end": 1135.96, "text": " What is it coming from?", "tokens": [708, 307, 309, 1348, 490, 30], "temperature": 0.0, "avg_logprob": -0.2124999306418679, "compression_ratio": 1.7257383966244726, "no_speech_prob": 1.260680437553674e-05}, {"id": 249, "seek": 113056, "start": 1135.96, "end": 1137.5, "text": " A CSV.", "tokens": [316, 48814, 13], "temperature": 0.0, "avg_logprob": -0.2124999306418679, "compression_ratio": 1.7257383966244726, "no_speech_prob": 1.260680437553674e-05}, {"id": 250, "seek": 113056, "start": 1137.5, "end": 1141.76, "text": " How do you want to split it into validation versus training?", "tokens": [1012, 360, 291, 528, 281, 7472, 309, 666, 24071, 5717, 3097, 30], "temperature": 0.0, "avg_logprob": -0.2124999306418679, "compression_ratio": 1.7257383966244726, "no_speech_prob": 1.260680437553674e-05}, {"id": 251, "seek": 113056, "start": 1141.76, "end": 1146.44, "text": " So in this case column number 2 was the is validation flag.", "tokens": [407, 294, 341, 1389, 7738, 1230, 568, 390, 264, 307, 24071, 7166, 13], "temperature": 0.0, "avg_logprob": -0.2124999306418679, "compression_ratio": 1.7257383966244726, "no_speech_prob": 1.260680437553674e-05}, {"id": 252, "seek": 113056, "start": 1146.44, "end": 1151.04, "text": " How do you want to label it with positive or negative sentiment, for example?", "tokens": [1012, 360, 291, 528, 281, 7645, 309, 365, 3353, 420, 3671, 16149, 11, 337, 1365, 30], "temperature": 0.0, "avg_logprob": -0.2124999306418679, "compression_ratio": 1.7257383966244726, "no_speech_prob": 1.260680437553674e-05}, {"id": 253, "seek": 113056, "start": 1151.04, "end": 1154.2, "text": " So column 0 had that and then turn that into a data bunch.", "tokens": [407, 7738, 1958, 632, 300, 293, 550, 1261, 300, 666, 257, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.2124999306418679, "compression_ratio": 1.7257383966244726, "no_speech_prob": 1.260680437553674e-05}, {"id": 254, "seek": 113056, "start": 1154.2, "end": 1157.04, "text": " That's going to do the same thing.", "tokens": [663, 311, 516, 281, 360, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.2124999306418679, "compression_ratio": 1.7257383966244726, "no_speech_prob": 1.260680437553674e-05}, {"id": 255, "seek": 115704, "start": 1157.04, "end": 1167.96, "text": " Okay, so now let's grab the whole data set which has 25,000 reviews in training, 25,000", "tokens": [1033, 11, 370, 586, 718, 311, 4444, 264, 1379, 1412, 992, 597, 575, 3552, 11, 1360, 10229, 294, 3097, 11, 3552, 11, 1360], "temperature": 0.0, "avg_logprob": -0.15304642219048042, "compression_ratio": 1.6272189349112427, "no_speech_prob": 2.0261352347006323e-06}, {"id": 256, "seek": 115704, "start": 1167.96, "end": 1173.32, "text": " reviews in validation and then 50,000 what they call unsupervised movie reviews.", "tokens": [10229, 294, 24071, 293, 550, 2625, 11, 1360, 437, 436, 818, 2693, 12879, 24420, 3169, 10229, 13], "temperature": 0.0, "avg_logprob": -0.15304642219048042, "compression_ratio": 1.6272189349112427, "no_speech_prob": 2.0261352347006323e-06}, {"id": 257, "seek": 115704, "start": 1173.32, "end": 1179.12, "text": " So 50,000 movie reviews that haven't been scored at all.", "tokens": [407, 2625, 11, 1360, 3169, 10229, 300, 2378, 380, 668, 18139, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.15304642219048042, "compression_ratio": 1.6272189349112427, "no_speech_prob": 2.0261352347006323e-06}, {"id": 258, "seek": 115704, "start": 1179.12, "end": 1185.3999999999999, "text": " So there it is, positive, negative, unsupervised.", "tokens": [407, 456, 309, 307, 11, 3353, 11, 3671, 11, 2693, 12879, 24420, 13], "temperature": 0.0, "avg_logprob": -0.15304642219048042, "compression_ratio": 1.6272189349112427, "no_speech_prob": 2.0261352347006323e-06}, {"id": 259, "seek": 118540, "start": 1185.4, "end": 1191.92, "text": " So we're going to start as we described with the language model.", "tokens": [407, 321, 434, 516, 281, 722, 382, 321, 7619, 365, 264, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14296982023451063, "compression_ratio": 1.6074380165289257, "no_speech_prob": 8.801015610515606e-06}, {"id": 260, "seek": 118540, "start": 1191.92, "end": 1196.2, "text": " Now the good news is we don't have to train the Wikitext 103 language model.", "tokens": [823, 264, 665, 2583, 307, 321, 500, 380, 362, 281, 3847, 264, 23377, 642, 734, 48784, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14296982023451063, "compression_ratio": 1.6074380165289257, "no_speech_prob": 8.801015610515606e-06}, {"id": 261, "seek": 118540, "start": 1196.2, "end": 1200.16, "text": " Not that it's difficult, you can use exactly the same steps that you see here, just download", "tokens": [1726, 300, 309, 311, 2252, 11, 291, 393, 764, 2293, 264, 912, 4439, 300, 291, 536, 510, 11, 445, 5484], "temperature": 0.0, "avg_logprob": -0.14296982023451063, "compression_ratio": 1.6074380165289257, "no_speech_prob": 8.801015610515606e-06}, {"id": 262, "seek": 118540, "start": 1200.16, "end": 1205.0800000000002, "text": " the Wikitext 103 corpus and run the same code.", "tokens": [264, 23377, 642, 734, 48784, 1181, 31624, 293, 1190, 264, 912, 3089, 13], "temperature": 0.0, "avg_logprob": -0.14296982023451063, "compression_ratio": 1.6074380165289257, "no_speech_prob": 8.801015610515606e-06}, {"id": 263, "seek": 118540, "start": 1205.0800000000002, "end": 1210.96, "text": " But it takes 2 or 3 days on a decent GPU, so not much point you doing it.", "tokens": [583, 309, 2516, 568, 420, 805, 1708, 322, 257, 8681, 18407, 11, 370, 406, 709, 935, 291, 884, 309, 13], "temperature": 0.0, "avg_logprob": -0.14296982023451063, "compression_ratio": 1.6074380165289257, "no_speech_prob": 8.801015610515606e-06}, {"id": 264, "seek": 118540, "start": 1210.96, "end": 1212.68, "text": " You may as well start with hours.", "tokens": [509, 815, 382, 731, 722, 365, 2496, 13], "temperature": 0.0, "avg_logprob": -0.14296982023451063, "compression_ratio": 1.6074380165289257, "no_speech_prob": 8.801015610515606e-06}, {"id": 265, "seek": 121268, "start": 1212.68, "end": 1216.88, "text": " Even if you've got a big corpus of medical documents or legal documents, you should still", "tokens": [2754, 498, 291, 600, 658, 257, 955, 1181, 31624, 295, 4625, 8512, 420, 5089, 8512, 11, 291, 820, 920], "temperature": 0.0, "avg_logprob": -0.11928176879882812, "compression_ratio": 1.5603112840466926, "no_speech_prob": 1.3709516224480467e-06}, {"id": 266, "seek": 121268, "start": 1216.88, "end": 1218.28, "text": " start with Wikitext 103.", "tokens": [722, 365, 23377, 642, 734, 48784, 13], "temperature": 0.0, "avg_logprob": -0.11928176879882812, "compression_ratio": 1.5603112840466926, "no_speech_prob": 1.3709516224480467e-06}, {"id": 267, "seek": 121268, "start": 1218.28, "end": 1221.44, "text": " There's just no reason to start with random weights.", "tokens": [821, 311, 445, 572, 1778, 281, 722, 365, 4974, 17443, 13], "temperature": 0.0, "avg_logprob": -0.11928176879882812, "compression_ratio": 1.5603112840466926, "no_speech_prob": 1.3709516224480467e-06}, {"id": 268, "seek": 121268, "start": 1221.44, "end": 1227.5600000000002, "text": " It's always good to use transfer learning if you can.", "tokens": [467, 311, 1009, 665, 281, 764, 5003, 2539, 498, 291, 393, 13], "temperature": 0.0, "avg_logprob": -0.11928176879882812, "compression_ratio": 1.5603112840466926, "no_speech_prob": 1.3709516224480467e-06}, {"id": 269, "seek": 121268, "start": 1227.5600000000002, "end": 1233.68, "text": " So we're going to start then at this point which is fine-tuning our IMDB language model.", "tokens": [407, 321, 434, 516, 281, 722, 550, 412, 341, 935, 597, 307, 2489, 12, 83, 37726, 527, 21463, 27735, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11928176879882812, "compression_ratio": 1.5603112840466926, "no_speech_prob": 1.3709516224480467e-06}, {"id": 270, "seek": 121268, "start": 1233.68, "end": 1242.0, "text": " So we can say, okay, it's a list of text files and the full IMDB actually is not in a CSV.", "tokens": [407, 321, 393, 584, 11, 1392, 11, 309, 311, 257, 1329, 295, 2487, 7098, 293, 264, 1577, 21463, 27735, 767, 307, 406, 294, 257, 48814, 13], "temperature": 0.0, "avg_logprob": -0.11928176879882812, "compression_ratio": 1.5603112840466926, "no_speech_prob": 1.3709516224480467e-06}, {"id": 271, "seek": 124200, "start": 1242.0, "end": 1245.6, "text": " Each document is a separate text file, so that's why we use a different constructor", "tokens": [6947, 4166, 307, 257, 4994, 2487, 3991, 11, 370, 300, 311, 983, 321, 764, 257, 819, 47479], "temperature": 0.0, "avg_logprob": -0.12936771617216222, "compression_ratio": 1.6462093862815885, "no_speech_prob": 1.2029446224914864e-05}, {"id": 272, "seek": 124200, "start": 1245.6, "end": 1249.6, "text": " for our independent variable, text files list, say where it is.", "tokens": [337, 527, 6695, 7006, 11, 2487, 7098, 1329, 11, 584, 689, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.12936771617216222, "compression_ratio": 1.6462093862815885, "no_speech_prob": 1.2029446224914864e-05}, {"id": 273, "seek": 124200, "start": 1249.6, "end": 1254.2, "text": " And in this case we have to make sure we just don't include the train and test folders.", "tokens": [400, 294, 341, 1389, 321, 362, 281, 652, 988, 321, 445, 500, 380, 4090, 264, 3847, 293, 1500, 31082, 13], "temperature": 0.0, "avg_logprob": -0.12936771617216222, "compression_ratio": 1.6462093862815885, "no_speech_prob": 1.2029446224914864e-05}, {"id": 274, "seek": 124200, "start": 1254.2, "end": 1258.6, "text": " And we randomly split it by 0.1.", "tokens": [400, 321, 16979, 7472, 309, 538, 1958, 13, 16, 13], "temperature": 0.0, "avg_logprob": -0.12936771617216222, "compression_ratio": 1.6462093862815885, "no_speech_prob": 1.2029446224914864e-05}, {"id": 275, "seek": 124200, "start": 1258.6, "end": 1260.16, "text": " Now this is interesting, 10%.", "tokens": [823, 341, 307, 1880, 11, 1266, 6856], "temperature": 0.0, "avg_logprob": -0.12936771617216222, "compression_ratio": 1.6462093862815885, "no_speech_prob": 1.2029446224914864e-05}, {"id": 276, "seek": 124200, "start": 1260.16, "end": 1264.84, "text": " Why are we randomly splitting it by 10% rather than using the predefined train and test they", "tokens": [1545, 366, 321, 16979, 30348, 309, 538, 1266, 4, 2831, 813, 1228, 264, 659, 37716, 3847, 293, 1500, 436], "temperature": 0.0, "avg_logprob": -0.12936771617216222, "compression_ratio": 1.6462093862815885, "no_speech_prob": 1.2029446224914864e-05}, {"id": 277, "seek": 124200, "start": 1264.84, "end": 1265.84, "text": " gave us?", "tokens": [2729, 505, 30], "temperature": 0.0, "avg_logprob": -0.12936771617216222, "compression_ratio": 1.6462093862815885, "no_speech_prob": 1.2029446224914864e-05}, {"id": 278, "seek": 124200, "start": 1265.84, "end": 1269.28, "text": " This is one of the cool things about transfer learning.", "tokens": [639, 307, 472, 295, 264, 1627, 721, 466, 5003, 2539, 13], "temperature": 0.0, "avg_logprob": -0.12936771617216222, "compression_ratio": 1.6462093862815885, "no_speech_prob": 1.2029446224914864e-05}, {"id": 279, "seek": 126928, "start": 1269.28, "end": 1274.2, "text": " Even though our test set or validation set has to be held aside, it's actually only the", "tokens": [2754, 1673, 527, 1500, 992, 420, 24071, 992, 575, 281, 312, 5167, 7359, 11, 309, 311, 767, 787, 264], "temperature": 0.0, "avg_logprob": -0.12838504497821515, "compression_ratio": 1.8284671532846715, "no_speech_prob": 5.862754278496141e-06}, {"id": 280, "seek": 126928, "start": 1274.2, "end": 1277.08, "text": " labels that we have to keep aside.", "tokens": [16949, 300, 321, 362, 281, 1066, 7359, 13], "temperature": 0.0, "avg_logprob": -0.12838504497821515, "compression_ratio": 1.8284671532846715, "no_speech_prob": 5.862754278496141e-06}, {"id": 281, "seek": 126928, "start": 1277.08, "end": 1279.84, "text": " So we're not allowed to use the labels in the test set.", "tokens": [407, 321, 434, 406, 4350, 281, 764, 264, 16949, 294, 264, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.12838504497821515, "compression_ratio": 1.8284671532846715, "no_speech_prob": 5.862754278496141e-06}, {"id": 282, "seek": 126928, "start": 1279.84, "end": 1283.12, "text": " So if you think about something like a Kaggle competition, you certainly can't use the labels", "tokens": [407, 498, 291, 519, 466, 746, 411, 257, 48751, 22631, 6211, 11, 291, 3297, 393, 380, 764, 264, 16949], "temperature": 0.0, "avg_logprob": -0.12838504497821515, "compression_ratio": 1.8284671532846715, "no_speech_prob": 5.862754278496141e-06}, {"id": 283, "seek": 126928, "start": 1283.12, "end": 1285.36, "text": " because they don't even give them to you.", "tokens": [570, 436, 500, 380, 754, 976, 552, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.12838504497821515, "compression_ratio": 1.8284671532846715, "no_speech_prob": 5.862754278496141e-06}, {"id": 284, "seek": 126928, "start": 1285.36, "end": 1287.76, "text": " But you can certainly use the independent variables.", "tokens": [583, 291, 393, 3297, 764, 264, 6695, 9102, 13], "temperature": 0.0, "avg_logprob": -0.12838504497821515, "compression_ratio": 1.8284671532846715, "no_speech_prob": 5.862754278496141e-06}, {"id": 285, "seek": 126928, "start": 1287.76, "end": 1293.52, "text": " So in this case you can absolutely use the text that is in the test set to train your", "tokens": [407, 294, 341, 1389, 291, 393, 3122, 764, 264, 2487, 300, 307, 294, 264, 1500, 992, 281, 3847, 428], "temperature": 0.0, "avg_logprob": -0.12838504497821515, "compression_ratio": 1.8284671532846715, "no_speech_prob": 5.862754278496141e-06}, {"id": 286, "seek": 126928, "start": 1293.52, "end": 1294.76, "text": " language model.", "tokens": [2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12838504497821515, "compression_ratio": 1.8284671532846715, "no_speech_prob": 5.862754278496141e-06}, {"id": 287, "seek": 126928, "start": 1294.76, "end": 1296.16, "text": " So this is a good trick, right?", "tokens": [407, 341, 307, 257, 665, 4282, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12838504497821515, "compression_ratio": 1.8284671532846715, "no_speech_prob": 5.862754278496141e-06}, {"id": 288, "seek": 129616, "start": 1296.16, "end": 1300.0, "text": " Because actually when you do the language model, concatenate the training and test set", "tokens": [1436, 767, 562, 291, 360, 264, 2856, 2316, 11, 1588, 7186, 473, 264, 3097, 293, 1500, 992], "temperature": 0.0, "avg_logprob": -0.1408721378871373, "compression_ratio": 1.7536764705882353, "no_speech_prob": 7.29630482965149e-06}, {"id": 289, "seek": 129616, "start": 1300.0, "end": 1304.3200000000002, "text": " together and then just split out a smaller validation set.", "tokens": [1214, 293, 550, 445, 7472, 484, 257, 4356, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.1408721378871373, "compression_ratio": 1.7536764705882353, "no_speech_prob": 7.29630482965149e-06}, {"id": 290, "seek": 129616, "start": 1304.3200000000002, "end": 1308.38, "text": " So you've got more data to train your language model.", "tokens": [407, 291, 600, 658, 544, 1412, 281, 3847, 428, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1408721378871373, "compression_ratio": 1.7536764705882353, "no_speech_prob": 7.29630482965149e-06}, {"id": 291, "seek": 129616, "start": 1308.38, "end": 1309.48, "text": " So that's a little trick.", "tokens": [407, 300, 311, 257, 707, 4282, 13], "temperature": 0.0, "avg_logprob": -0.1408721378871373, "compression_ratio": 1.7536764705882353, "no_speech_prob": 7.29630482965149e-06}, {"id": 292, "seek": 129616, "start": 1309.48, "end": 1315.48, "text": " And so if you're doing NLP stuff on Kaggle, for example, or you've just got a smaller", "tokens": [400, 370, 498, 291, 434, 884, 426, 45196, 1507, 322, 48751, 22631, 11, 337, 1365, 11, 420, 291, 600, 445, 658, 257, 4356], "temperature": 0.0, "avg_logprob": -0.1408721378871373, "compression_ratio": 1.7536764705882353, "no_speech_prob": 7.29630482965149e-06}, {"id": 293, "seek": 129616, "start": 1315.48, "end": 1321.16, "text": " subset of labelled data, make sure that you use all of the text you have to train your", "tokens": [25993, 295, 2715, 41307, 1412, 11, 652, 988, 300, 291, 764, 439, 295, 264, 2487, 291, 362, 281, 3847, 428], "temperature": 0.0, "avg_logprob": -0.1408721378871373, "compression_ratio": 1.7536764705882353, "no_speech_prob": 7.29630482965149e-06}, {"id": 294, "seek": 129616, "start": 1321.16, "end": 1323.3600000000001, "text": " language model because there's no reason not to.", "tokens": [2856, 2316, 570, 456, 311, 572, 1778, 406, 281, 13], "temperature": 0.0, "avg_logprob": -0.1408721378871373, "compression_ratio": 1.7536764705882353, "no_speech_prob": 7.29630482965149e-06}, {"id": 295, "seek": 129616, "start": 1323.3600000000001, "end": 1326.0, "text": " How are we going to label it?", "tokens": [1012, 366, 321, 516, 281, 7645, 309, 30], "temperature": 0.0, "avg_logprob": -0.1408721378871373, "compression_ratio": 1.7536764705882353, "no_speech_prob": 7.29630482965149e-06}, {"id": 296, "seek": 132600, "start": 1326.0, "end": 1328.84, "text": " Well remember a language model kind of has its own labels.", "tokens": [1042, 1604, 257, 2856, 2316, 733, 295, 575, 1080, 1065, 16949, 13], "temperature": 0.0, "avg_logprob": -0.18062928665515987, "compression_ratio": 1.7698412698412698, "no_speech_prob": 2.6273766707163304e-05}, {"id": 297, "seek": 132600, "start": 1328.84, "end": 1330.24, "text": " So the text itself is a label.", "tokens": [407, 264, 2487, 2564, 307, 257, 7645, 13], "temperature": 0.0, "avg_logprob": -0.18062928665515987, "compression_ratio": 1.7698412698412698, "no_speech_prob": 2.6273766707163304e-05}, {"id": 298, "seek": 132600, "start": 1330.24, "end": 1333.82, "text": " So label for language model does that for us.", "tokens": [407, 7645, 337, 2856, 2316, 775, 300, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.18062928665515987, "compression_ratio": 1.7698412698412698, "no_speech_prob": 2.6273766707163304e-05}, {"id": 299, "seek": 132600, "start": 1333.82, "end": 1336.44, "text": " And create a data bunch and save it.", "tokens": [400, 1884, 257, 1412, 3840, 293, 3155, 309, 13], "temperature": 0.0, "avg_logprob": -0.18062928665515987, "compression_ratio": 1.7698412698412698, "no_speech_prob": 2.6273766707163304e-05}, {"id": 300, "seek": 132600, "start": 1336.44, "end": 1342.6, "text": " And that takes a few minutes to tokenize and numericalize.", "tokens": [400, 300, 2516, 257, 1326, 2077, 281, 14862, 1125, 293, 29054, 1125, 13], "temperature": 0.0, "avg_logprob": -0.18062928665515987, "compression_ratio": 1.7698412698412698, "no_speech_prob": 2.6273766707163304e-05}, {"id": 301, "seek": 132600, "start": 1342.6, "end": 1344.4, "text": " So since that takes a few minutes, we save it.", "tokens": [407, 1670, 300, 2516, 257, 1326, 2077, 11, 321, 3155, 309, 13], "temperature": 0.0, "avg_logprob": -0.18062928665515987, "compression_ratio": 1.7698412698412698, "no_speech_prob": 2.6273766707163304e-05}, {"id": 302, "seek": 132600, "start": 1344.4, "end": 1345.4, "text": " Later on you can just load it.", "tokens": [11965, 322, 291, 393, 445, 3677, 309, 13], "temperature": 0.0, "avg_logprob": -0.18062928665515987, "compression_ratio": 1.7698412698412698, "no_speech_prob": 2.6273766707163304e-05}, {"id": 303, "seek": 132600, "start": 1345.4, "end": 1347.72, "text": " No need to run that again.", "tokens": [883, 643, 281, 1190, 300, 797, 13], "temperature": 0.0, "avg_logprob": -0.18062928665515987, "compression_ratio": 1.7698412698412698, "no_speech_prob": 2.6273766707163304e-05}, {"id": 304, "seek": 132600, "start": 1347.72, "end": 1350.6, "text": " So here's what it looks like.", "tokens": [407, 510, 311, 437, 309, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.18062928665515987, "compression_ratio": 1.7698412698412698, "no_speech_prob": 2.6273766707163304e-05}, {"id": 305, "seek": 132600, "start": 1350.6, "end": 1354.12, "text": " And at this point, things are going to look very familiar.", "tokens": [400, 412, 341, 935, 11, 721, 366, 516, 281, 574, 588, 4963, 13], "temperature": 0.0, "avg_logprob": -0.18062928665515987, "compression_ratio": 1.7698412698412698, "no_speech_prob": 2.6273766707163304e-05}, {"id": 306, "seek": 132600, "start": 1354.12, "end": 1355.86, "text": " We create a learner.", "tokens": [492, 1884, 257, 33347, 13], "temperature": 0.0, "avg_logprob": -0.18062928665515987, "compression_ratio": 1.7698412698412698, "no_speech_prob": 2.6273766707163304e-05}, {"id": 307, "seek": 135586, "start": 1355.86, "end": 1363.3999999999999, "text": " But instead of creating a CNN learner, we're going to create a language model learner.", "tokens": [583, 2602, 295, 4084, 257, 24859, 33347, 11, 321, 434, 516, 281, 1884, 257, 2856, 2316, 33347, 13], "temperature": 0.0, "avg_logprob": -0.12369116869839755, "compression_ratio": 1.7530864197530864, "no_speech_prob": 1.0952966476907022e-05}, {"id": 308, "seek": 135586, "start": 1363.3999999999999, "end": 1367.4399999999998, "text": " So behind the scenes, this is actually not going to create a CNN, a convolutional neural", "tokens": [407, 2261, 264, 8026, 11, 341, 307, 767, 406, 516, 281, 1884, 257, 24859, 11, 257, 45216, 304, 18161], "temperature": 0.0, "avg_logprob": -0.12369116869839755, "compression_ratio": 1.7530864197530864, "no_speech_prob": 1.0952966476907022e-05}, {"id": 309, "seek": 135586, "start": 1367.4399999999998, "end": 1368.4399999999998, "text": " network.", "tokens": [3209, 13], "temperature": 0.0, "avg_logprob": -0.12369116869839755, "compression_ratio": 1.7530864197530864, "no_speech_prob": 1.0952966476907022e-05}, {"id": 310, "seek": 135586, "start": 1368.4399999999998, "end": 1371.52, "text": " It's going to create an RNN, a recurrent neural network.", "tokens": [467, 311, 516, 281, 1884, 364, 45702, 45, 11, 257, 18680, 1753, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.12369116869839755, "compression_ratio": 1.7530864197530864, "no_speech_prob": 1.0952966476907022e-05}, {"id": 311, "seek": 135586, "start": 1371.52, "end": 1375.9599999999998, "text": " So we're going to be learning exactly how they're built over the coming lessons.", "tokens": [407, 321, 434, 516, 281, 312, 2539, 2293, 577, 436, 434, 3094, 670, 264, 1348, 8820, 13], "temperature": 0.0, "avg_logprob": -0.12369116869839755, "compression_ratio": 1.7530864197530864, "no_speech_prob": 1.0952966476907022e-05}, {"id": 312, "seek": 135586, "start": 1375.9599999999998, "end": 1379.24, "text": " But in short, they're the same basic structure.", "tokens": [583, 294, 2099, 11, 436, 434, 264, 912, 3875, 3877, 13], "temperature": 0.0, "avg_logprob": -0.12369116869839755, "compression_ratio": 1.7530864197530864, "no_speech_prob": 1.0952966476907022e-05}, {"id": 313, "seek": 135586, "start": 1379.24, "end": 1384.32, "text": " The input goes into a weight matrix, a matrix multiply.", "tokens": [440, 4846, 1709, 666, 257, 3364, 8141, 11, 257, 8141, 12972, 13], "temperature": 0.0, "avg_logprob": -0.12369116869839755, "compression_ratio": 1.7530864197530864, "no_speech_prob": 1.0952966476907022e-05}, {"id": 314, "seek": 138432, "start": 1384.32, "end": 1388.2, "text": " Then you replace the negatives with zeros, and it goes into another matrix multiply,", "tokens": [1396, 291, 7406, 264, 40019, 365, 35193, 11, 293, 309, 1709, 666, 1071, 8141, 12972, 11], "temperature": 0.0, "avg_logprob": -0.136549620501763, "compression_ratio": 1.6271186440677967, "no_speech_prob": 7.183140496636042e-06}, {"id": 315, "seek": 138432, "start": 1388.2, "end": 1391.08, "text": " and so forth a bunch of times.", "tokens": [293, 370, 5220, 257, 3840, 295, 1413, 13], "temperature": 0.0, "avg_logprob": -0.136549620501763, "compression_ratio": 1.6271186440677967, "no_speech_prob": 7.183140496636042e-06}, {"id": 316, "seek": 138432, "start": 1391.08, "end": 1394.6, "text": " So it's the same basic structure.", "tokens": [407, 309, 311, 264, 912, 3875, 3877, 13], "temperature": 0.0, "avg_logprob": -0.136549620501763, "compression_ratio": 1.6271186440677967, "no_speech_prob": 7.183140496636042e-06}, {"id": 317, "seek": 138432, "start": 1394.6, "end": 1399.2, "text": " So as usual, when we create a learner, you have to pass in two things.", "tokens": [407, 382, 7713, 11, 562, 321, 1884, 257, 33347, 11, 291, 362, 281, 1320, 294, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.136549620501763, "compression_ratio": 1.6271186440677967, "no_speech_prob": 7.183140496636042e-06}, {"id": 318, "seek": 138432, "start": 1399.2, "end": 1400.2, "text": " The data.", "tokens": [440, 1412, 13], "temperature": 0.0, "avg_logprob": -0.136549620501763, "compression_ratio": 1.6271186440677967, "no_speech_prob": 7.183140496636042e-06}, {"id": 319, "seek": 138432, "start": 1400.2, "end": 1402.84, "text": " So here's our language model data.", "tokens": [407, 510, 311, 527, 2856, 2316, 1412, 13], "temperature": 0.0, "avg_logprob": -0.136549620501763, "compression_ratio": 1.6271186440677967, "no_speech_prob": 7.183140496636042e-06}, {"id": 320, "seek": 138432, "start": 1402.84, "end": 1407.72, "text": " And in this case, what pre-trained model we want to use.", "tokens": [400, 294, 341, 1389, 11, 437, 659, 12, 17227, 2001, 2316, 321, 528, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.136549620501763, "compression_ratio": 1.6271186440677967, "no_speech_prob": 7.183140496636042e-06}, {"id": 321, "seek": 138432, "start": 1407.72, "end": 1411.8799999999999, "text": " And so here, the pre-trained model is the Wikitext 103 model.", "tokens": [400, 370, 510, 11, 264, 659, 12, 17227, 2001, 2316, 307, 264, 23377, 642, 734, 48784, 2316, 13], "temperature": 0.0, "avg_logprob": -0.136549620501763, "compression_ratio": 1.6271186440677967, "no_speech_prob": 7.183140496636042e-06}, {"id": 322, "seek": 141188, "start": 1411.88, "end": 1416.2, "text": " That will be downloaded for you from Fast.ai if you haven't used it before, just like the", "tokens": [663, 486, 312, 21748, 337, 291, 490, 15968, 13, 1301, 498, 291, 2378, 380, 1143, 309, 949, 11, 445, 411, 264], "temperature": 0.0, "avg_logprob": -0.10255234842082016, "compression_ratio": 1.745704467353952, "no_speech_prob": 1.2029426216031425e-05}, {"id": 323, "seek": 141188, "start": 1416.2, "end": 1423.16, "text": " same thing with things like ImageNet pre-trained models are downloaded for you.", "tokens": [912, 551, 365, 721, 411, 29903, 31890, 659, 12, 17227, 2001, 5245, 366, 21748, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.10255234842082016, "compression_ratio": 1.745704467353952, "no_speech_prob": 1.2029426216031425e-05}, {"id": 324, "seek": 141188, "start": 1423.16, "end": 1425.0400000000002, "text": " This here sets the amount of dropout.", "tokens": [639, 510, 6352, 264, 2372, 295, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.10255234842082016, "compression_ratio": 1.745704467353952, "no_speech_prob": 1.2029426216031425e-05}, {"id": 325, "seek": 141188, "start": 1425.0400000000002, "end": 1426.4, "text": " We haven't talked about that yet.", "tokens": [492, 2378, 380, 2825, 466, 300, 1939, 13], "temperature": 0.0, "avg_logprob": -0.10255234842082016, "compression_ratio": 1.745704467353952, "no_speech_prob": 1.2029426216031425e-05}, {"id": 326, "seek": 141188, "start": 1426.4, "end": 1429.44, "text": " We've talked briefly about this idea that there's something called regularization, and", "tokens": [492, 600, 2825, 10515, 466, 341, 1558, 300, 456, 311, 746, 1219, 3890, 2144, 11, 293], "temperature": 0.0, "avg_logprob": -0.10255234842082016, "compression_ratio": 1.745704467353952, "no_speech_prob": 1.2029426216031425e-05}, {"id": 327, "seek": 141188, "start": 1429.44, "end": 1433.16, "text": " you can reduce the regularization to avoid underfitting.", "tokens": [291, 393, 5407, 264, 3890, 2144, 281, 5042, 833, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.10255234842082016, "compression_ratio": 1.745704467353952, "no_speech_prob": 1.2029426216031425e-05}, {"id": 328, "seek": 141188, "start": 1433.16, "end": 1438.88, "text": " So for now, just know that by using a number lower than one is because when I first tried", "tokens": [407, 337, 586, 11, 445, 458, 300, 538, 1228, 257, 1230, 3126, 813, 472, 307, 570, 562, 286, 700, 3031], "temperature": 0.0, "avg_logprob": -0.10255234842082016, "compression_ratio": 1.745704467353952, "no_speech_prob": 1.2029426216031425e-05}, {"id": 329, "seek": 141188, "start": 1438.88, "end": 1441.0800000000002, "text": " to run this, I was underfitting.", "tokens": [281, 1190, 341, 11, 286, 390, 833, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.10255234842082016, "compression_ratio": 1.745704467353952, "no_speech_prob": 1.2029426216031425e-05}, {"id": 330, "seek": 144108, "start": 1441.08, "end": 1444.52, "text": " And so if you reduce that number, then it will avoid underfitting.", "tokens": [400, 370, 498, 291, 5407, 300, 1230, 11, 550, 309, 486, 5042, 833, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.1447408824291044, "compression_ratio": 1.5458715596330275, "no_speech_prob": 6.144136932562105e-06}, {"id": 331, "seek": 144108, "start": 1444.52, "end": 1447.6399999999999, "text": " Okay, so we've got a learner.", "tokens": [1033, 11, 370, 321, 600, 658, 257, 33347, 13], "temperature": 0.0, "avg_logprob": -0.1447408824291044, "compression_ratio": 1.5458715596330275, "no_speech_prob": 6.144136932562105e-06}, {"id": 332, "seek": 144108, "start": 1447.6399999999999, "end": 1448.6399999999999, "text": " We can LR find.", "tokens": [492, 393, 441, 49, 915, 13], "temperature": 0.0, "avg_logprob": -0.1447408824291044, "compression_ratio": 1.5458715596330275, "no_speech_prob": 6.144136932562105e-06}, {"id": 333, "seek": 144108, "start": 1448.6399999999999, "end": 1451.84, "text": " It looks pretty standard.", "tokens": [467, 1542, 1238, 3832, 13], "temperature": 0.0, "avg_logprob": -0.1447408824291044, "compression_ratio": 1.5458715596330275, "no_speech_prob": 6.144136932562105e-06}, {"id": 334, "seek": 144108, "start": 1451.84, "end": 1453.8799999999999, "text": " And so then we can fit one cycle.", "tokens": [400, 370, 550, 321, 393, 3318, 472, 6586, 13], "temperature": 0.0, "avg_logprob": -0.1447408824291044, "compression_ratio": 1.5458715596330275, "no_speech_prob": 6.144136932562105e-06}, {"id": 335, "seek": 144108, "start": 1453.8799999999999, "end": 1461.8799999999999, "text": " And so what's happening here is we are just fine-tuning the last layers.", "tokens": [400, 370, 437, 311, 2737, 510, 307, 321, 366, 445, 2489, 12, 83, 37726, 264, 1036, 7914, 13], "temperature": 0.0, "avg_logprob": -0.1447408824291044, "compression_ratio": 1.5458715596330275, "no_speech_prob": 6.144136932562105e-06}, {"id": 336, "seek": 144108, "start": 1461.8799999999999, "end": 1469.56, "text": " So normally after we fine-tuned the last layers, the next thing we do is we go unfreeze and", "tokens": [407, 5646, 934, 321, 2489, 12, 83, 43703, 264, 1036, 7914, 11, 264, 958, 551, 321, 360, 307, 321, 352, 3971, 701, 1381, 293], "temperature": 0.0, "avg_logprob": -0.1447408824291044, "compression_ratio": 1.5458715596330275, "no_speech_prob": 6.144136932562105e-06}, {"id": 337, "seek": 146956, "start": 1469.56, "end": 1472.52, "text": " train the whole thing.", "tokens": [3847, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.21193970574273002, "compression_ratio": 1.7410358565737052, "no_speech_prob": 1.2411384886945598e-05}, {"id": 338, "seek": 146956, "start": 1472.52, "end": 1474.12, "text": " And so here it is.", "tokens": [400, 370, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.21193970574273002, "compression_ratio": 1.7410358565737052, "no_speech_prob": 1.2411384886945598e-05}, {"id": 339, "seek": 146956, "start": 1474.12, "end": 1475.52, "text": " Unfreeze and train the whole thing.", "tokens": [8170, 701, 1381, 293, 3847, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.21193970574273002, "compression_ratio": 1.7410358565737052, "no_speech_prob": 1.2411384886945598e-05}, {"id": 340, "seek": 146956, "start": 1475.52, "end": 1481.2, "text": " And as you can see, even on a pretty beefy GPU, that takes 2 or 3 hours.", "tokens": [400, 382, 291, 393, 536, 11, 754, 322, 257, 1238, 9256, 88, 18407, 11, 300, 2516, 568, 420, 805, 2496, 13], "temperature": 0.0, "avg_logprob": -0.21193970574273002, "compression_ratio": 1.7410358565737052, "no_speech_prob": 1.2411384886945598e-05}, {"id": 341, "seek": 146956, "start": 1481.2, "end": 1483.72, "text": " And in fact, I'm still underfitting.", "tokens": [400, 294, 1186, 11, 286, 478, 920, 833, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.21193970574273002, "compression_ratio": 1.7410358565737052, "no_speech_prob": 1.2411384886945598e-05}, {"id": 342, "seek": 146956, "start": 1483.72, "end": 1489.36, "text": " Probably tonight I might train it overnight and try to do a little bit better.", "tokens": [9210, 4440, 286, 1062, 3847, 309, 13935, 293, 853, 281, 360, 257, 707, 857, 1101, 13], "temperature": 0.0, "avg_logprob": -0.21193970574273002, "compression_ratio": 1.7410358565737052, "no_speech_prob": 1.2411384886945598e-05}, {"id": 343, "seek": 146956, "start": 1489.36, "end": 1494.1599999999999, "text": " Because you can see, I guess I'm not underfitting.", "tokens": [1436, 291, 393, 536, 11, 286, 2041, 286, 478, 406, 833, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.21193970574273002, "compression_ratio": 1.7410358565737052, "no_speech_prob": 1.2411384886945598e-05}, {"id": 344, "seek": 146956, "start": 1494.1599999999999, "end": 1497.44, "text": " I'm guessing I could probably train this a bit longer because you can see the accuracy", "tokens": [286, 478, 17939, 286, 727, 1391, 3847, 341, 257, 857, 2854, 570, 291, 393, 536, 264, 14170], "temperature": 0.0, "avg_logprob": -0.21193970574273002, "compression_ratio": 1.7410358565737052, "no_speech_prob": 1.2411384886945598e-05}, {"id": 345, "seek": 146956, "start": 1497.44, "end": 1499.1599999999999, "text": " hasn't started going down again.", "tokens": [6132, 380, 1409, 516, 760, 797, 13], "temperature": 0.0, "avg_logprob": -0.21193970574273002, "compression_ratio": 1.7410358565737052, "no_speech_prob": 1.2411384886945598e-05}, {"id": 346, "seek": 149916, "start": 1499.16, "end": 1501.88, "text": " So I wouldn't mind trying to train that a bit longer.", "tokens": [407, 286, 2759, 380, 1575, 1382, 281, 3847, 300, 257, 857, 2854, 13], "temperature": 0.0, "avg_logprob": -0.1699779287297675, "compression_ratio": 1.5701754385964912, "no_speech_prob": 8.139596502587665e-06}, {"id": 347, "seek": 149916, "start": 1501.88, "end": 1504.6000000000001, "text": " But the accuracy, it's interesting.", "tokens": [583, 264, 14170, 11, 309, 311, 1880, 13], "temperature": 0.0, "avg_logprob": -0.1699779287297675, "compression_ratio": 1.5701754385964912, "no_speech_prob": 8.139596502587665e-06}, {"id": 348, "seek": 149916, "start": 1504.6000000000001, "end": 1509.8400000000001, "text": " Point 3 means we're guessing the next word of the movie review correctly about a third", "tokens": [12387, 805, 1355, 321, 434, 17939, 264, 958, 1349, 295, 264, 3169, 3131, 8944, 466, 257, 2636], "temperature": 0.0, "avg_logprob": -0.1699779287297675, "compression_ratio": 1.5701754385964912, "no_speech_prob": 8.139596502587665e-06}, {"id": 349, "seek": 149916, "start": 1509.8400000000001, "end": 1511.68, "text": " of the time.", "tokens": [295, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.1699779287297675, "compression_ratio": 1.5701754385964912, "no_speech_prob": 8.139596502587665e-06}, {"id": 350, "seek": 149916, "start": 1511.68, "end": 1515.5600000000002, "text": " That sounds like a pretty high number, the idea that you can actually guess the next", "tokens": [663, 3263, 411, 257, 1238, 1090, 1230, 11, 264, 1558, 300, 291, 393, 767, 2041, 264, 958], "temperature": 0.0, "avg_logprob": -0.1699779287297675, "compression_ratio": 1.5701754385964912, "no_speech_prob": 8.139596502587665e-06}, {"id": 351, "seek": 149916, "start": 1515.5600000000002, "end": 1517.2, "text": " word that often.", "tokens": [1349, 300, 2049, 13], "temperature": 0.0, "avg_logprob": -0.1699779287297675, "compression_ratio": 1.5701754385964912, "no_speech_prob": 8.139596502587665e-06}, {"id": 352, "seek": 149916, "start": 1517.2, "end": 1521.52, "text": " So that's a good sign that my language model is doing pretty well.", "tokens": [407, 300, 311, 257, 665, 1465, 300, 452, 2856, 2316, 307, 884, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.1699779287297675, "compression_ratio": 1.5701754385964912, "no_speech_prob": 8.139596502587665e-06}, {"id": 353, "seek": 152152, "start": 1521.52, "end": 1530.0, "text": " For more limited domain documents like medical transcripts and legal transcripts, you'll", "tokens": [1171, 544, 5567, 9274, 8512, 411, 4625, 24444, 82, 293, 5089, 24444, 82, 11, 291, 603], "temperature": 0.0, "avg_logprob": -0.13570165634155273, "compression_ratio": 1.510204081632653, "no_speech_prob": 2.190756731579313e-06}, {"id": 354, "seek": 152152, "start": 1530.0, "end": 1534.6, "text": " often find this accuracy gets a lot higher.", "tokens": [2049, 915, 341, 14170, 2170, 257, 688, 2946, 13], "temperature": 0.0, "avg_logprob": -0.13570165634155273, "compression_ratio": 1.510204081632653, "no_speech_prob": 2.190756731579313e-06}, {"id": 355, "seek": 152152, "start": 1534.6, "end": 1538.48, "text": " So sometimes this can be even 50% or more.", "tokens": [407, 2171, 341, 393, 312, 754, 2625, 4, 420, 544, 13], "temperature": 0.0, "avg_logprob": -0.13570165634155273, "compression_ratio": 1.510204081632653, "no_speech_prob": 2.190756731579313e-06}, {"id": 356, "seek": 152152, "start": 1538.48, "end": 1542.2, "text": " But point 3 or more is pretty good.", "tokens": [583, 935, 805, 420, 544, 307, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.13570165634155273, "compression_ratio": 1.510204081632653, "no_speech_prob": 2.190756731579313e-06}, {"id": 357, "seek": 152152, "start": 1542.2, "end": 1550.68, "text": " So you can now run learn.predict and pass in the start of a sentence and it will try", "tokens": [407, 291, 393, 586, 1190, 1466, 13, 79, 24945, 293, 1320, 294, 264, 722, 295, 257, 8174, 293, 309, 486, 853], "temperature": 0.0, "avg_logprob": -0.13570165634155273, "compression_ratio": 1.510204081632653, "no_speech_prob": 2.190756731579313e-06}, {"id": 358, "seek": 155068, "start": 1550.68, "end": 1553.3600000000001, "text": " and finish off that sentence for you.", "tokens": [293, 2413, 766, 300, 8174, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.12213558870203355, "compression_ratio": 1.5821596244131455, "no_speech_prob": 4.425423412612872e-06}, {"id": 359, "seek": 155068, "start": 1553.3600000000001, "end": 1559.76, "text": " Now I should mention this is not designed to be a good text generation system.", "tokens": [823, 286, 820, 2152, 341, 307, 406, 4761, 281, 312, 257, 665, 2487, 5125, 1185, 13], "temperature": 0.0, "avg_logprob": -0.12213558870203355, "compression_ratio": 1.5821596244131455, "no_speech_prob": 4.425423412612872e-06}, {"id": 360, "seek": 155068, "start": 1559.76, "end": 1563.6000000000001, "text": " This is really more designed to kind of check that it seems to be creating something that's", "tokens": [639, 307, 534, 544, 4761, 281, 733, 295, 1520, 300, 309, 2544, 281, 312, 4084, 746, 300, 311], "temperature": 0.0, "avg_logprob": -0.12213558870203355, "compression_ratio": 1.5821596244131455, "no_speech_prob": 4.425423412612872e-06}, {"id": 361, "seek": 155068, "start": 1563.6000000000001, "end": 1564.6000000000001, "text": " vaguely sensible.", "tokens": [13501, 48863, 25380, 13], "temperature": 0.0, "avg_logprob": -0.12213558870203355, "compression_ratio": 1.5821596244131455, "no_speech_prob": 4.425423412612872e-06}, {"id": 362, "seek": 155068, "start": 1564.6000000000001, "end": 1570.64, "text": " There's a lot of tricks that you can use to generate much higher quality text, none of", "tokens": [821, 311, 257, 688, 295, 11733, 300, 291, 393, 764, 281, 8460, 709, 2946, 3125, 2487, 11, 6022, 295], "temperature": 0.0, "avg_logprob": -0.12213558870203355, "compression_ratio": 1.5821596244131455, "no_speech_prob": 4.425423412612872e-06}, {"id": 363, "seek": 155068, "start": 1570.64, "end": 1573.3200000000002, "text": " which we're using here.", "tokens": [597, 321, 434, 1228, 510, 13], "temperature": 0.0, "avg_logprob": -0.12213558870203355, "compression_ratio": 1.5821596244131455, "no_speech_prob": 4.425423412612872e-06}, {"id": 364, "seek": 157332, "start": 1573.32, "end": 1581.24, "text": " But you can kind of see that it's certainly not random words that it's generating, it", "tokens": [583, 291, 393, 733, 295, 536, 300, 309, 311, 3297, 406, 4974, 2283, 300, 309, 311, 17746, 11, 309], "temperature": 0.0, "avg_logprob": -0.10656075477600098, "compression_ratio": 1.5, "no_speech_prob": 1.1189388260390842e-06}, {"id": 365, "seek": 157332, "start": 1581.24, "end": 1585.8799999999999, "text": " sounds vaguely English-like, even though it doesn't make any sense.", "tokens": [3263, 13501, 48863, 3669, 12, 4092, 11, 754, 1673, 309, 1177, 380, 652, 604, 2020, 13], "temperature": 0.0, "avg_logprob": -0.10656075477600098, "compression_ratio": 1.5, "no_speech_prob": 1.1189388260390842e-06}, {"id": 366, "seek": 157332, "start": 1585.8799999999999, "end": 1595.9399999999998, "text": " So at this point we have a movie review model.", "tokens": [407, 412, 341, 935, 321, 362, 257, 3169, 3131, 2316, 13], "temperature": 0.0, "avg_logprob": -0.10656075477600098, "compression_ratio": 1.5, "no_speech_prob": 1.1189388260390842e-06}, {"id": 367, "seek": 157332, "start": 1595.9399999999998, "end": 1602.6799999999998, "text": " So now we're going to save that in order to load it into our classifier to be our pre-trained", "tokens": [407, 586, 321, 434, 516, 281, 3155, 300, 294, 1668, 281, 3677, 309, 666, 527, 1508, 9902, 281, 312, 527, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.10656075477600098, "compression_ratio": 1.5, "no_speech_prob": 1.1189388260390842e-06}, {"id": 368, "seek": 160268, "start": 1602.68, "end": 1603.68, "text": " model for the classifier.", "tokens": [2316, 337, 264, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.15090589108674424, "compression_ratio": 1.8524590163934427, "no_speech_prob": 2.8408381695044227e-05}, {"id": 369, "seek": 160268, "start": 1603.68, "end": 1606.6000000000001, "text": " But I actually don't want to save the whole thing.", "tokens": [583, 286, 767, 500, 380, 528, 281, 3155, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.15090589108674424, "compression_ratio": 1.8524590163934427, "no_speech_prob": 2.8408381695044227e-05}, {"id": 370, "seek": 160268, "start": 1606.6000000000001, "end": 1610.52, "text": " A lot of this, kind of the second half, as we'll learn, the second half of the language", "tokens": [316, 688, 295, 341, 11, 733, 295, 264, 1150, 1922, 11, 382, 321, 603, 1466, 11, 264, 1150, 1922, 295, 264, 2856], "temperature": 0.0, "avg_logprob": -0.15090589108674424, "compression_ratio": 1.8524590163934427, "no_speech_prob": 2.8408381695044227e-05}, {"id": 371, "seek": 160268, "start": 1610.52, "end": 1616.4, "text": " model is all about predicting the next word rather than about understanding the sentence", "tokens": [2316, 307, 439, 466, 32884, 264, 958, 1349, 2831, 813, 466, 3701, 264, 8174], "temperature": 0.0, "avg_logprob": -0.15090589108674424, "compression_ratio": 1.8524590163934427, "no_speech_prob": 2.8408381695044227e-05}, {"id": 372, "seek": 160268, "start": 1616.4, "end": 1617.78, "text": " so far.", "tokens": [370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.15090589108674424, "compression_ratio": 1.8524590163934427, "no_speech_prob": 2.8408381695044227e-05}, {"id": 373, "seek": 160268, "start": 1617.78, "end": 1623.7, "text": " So the bit which is specifically about understanding the sentence so far is called the encoder.", "tokens": [407, 264, 857, 597, 307, 4682, 466, 3701, 264, 8174, 370, 1400, 307, 1219, 264, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.15090589108674424, "compression_ratio": 1.8524590163934427, "no_speech_prob": 2.8408381695044227e-05}, {"id": 374, "seek": 160268, "start": 1623.7, "end": 1625.4, "text": " So I just saved that.", "tokens": [407, 286, 445, 6624, 300, 13], "temperature": 0.0, "avg_logprob": -0.15090589108674424, "compression_ratio": 1.8524590163934427, "no_speech_prob": 2.8408381695044227e-05}, {"id": 375, "seek": 160268, "start": 1625.4, "end": 1629.6000000000001, "text": " So again we're going to learn the details of this over the coming weeks.", "tokens": [407, 797, 321, 434, 516, 281, 1466, 264, 4365, 295, 341, 670, 264, 1348, 3259, 13], "temperature": 0.0, "avg_logprob": -0.15090589108674424, "compression_ratio": 1.8524590163934427, "no_speech_prob": 2.8408381695044227e-05}, {"id": 376, "seek": 162960, "start": 1629.6, "end": 1634.4399999999998, "text": " We're just going to save the encoder so the bit that understands the sentence rather than", "tokens": [492, 434, 445, 516, 281, 3155, 264, 2058, 19866, 370, 264, 857, 300, 15146, 264, 8174, 2831, 813], "temperature": 0.0, "avg_logprob": -0.09919316551902077, "compression_ratio": 1.7617021276595746, "no_speech_prob": 8.013329534151126e-06}, {"id": 377, "seek": 162960, "start": 1634.4399999999998, "end": 1638.6799999999998, "text": " the bit that generates the word.", "tokens": [264, 857, 300, 23815, 264, 1349, 13], "temperature": 0.0, "avg_logprob": -0.09919316551902077, "compression_ratio": 1.7617021276595746, "no_speech_prob": 8.013329534151126e-06}, {"id": 378, "seek": 162960, "start": 1638.6799999999998, "end": 1641.3999999999999, "text": " So now we're ready to create our classifier.", "tokens": [407, 586, 321, 434, 1919, 281, 1884, 527, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.09919316551902077, "compression_ratio": 1.7617021276595746, "no_speech_prob": 8.013329534151126e-06}, {"id": 379, "seek": 162960, "start": 1641.3999999999999, "end": 1645.28, "text": " So step one, as per usual, is to create a data bunch and we're going to do basically", "tokens": [407, 1823, 472, 11, 382, 680, 7713, 11, 307, 281, 1884, 257, 1412, 3840, 293, 321, 434, 516, 281, 360, 1936], "temperature": 0.0, "avg_logprob": -0.09919316551902077, "compression_ratio": 1.7617021276595746, "no_speech_prob": 8.013329534151126e-06}, {"id": 380, "seek": 162960, "start": 1645.28, "end": 1651.6799999999998, "text": " exactly the same thing, bring it in, okay, and here's our path, but we want to make sure", "tokens": [2293, 264, 912, 551, 11, 1565, 309, 294, 11, 1392, 11, 293, 510, 311, 527, 3100, 11, 457, 321, 528, 281, 652, 988], "temperature": 0.0, "avg_logprob": -0.09919316551902077, "compression_ratio": 1.7617021276595746, "no_speech_prob": 8.013329534151126e-06}, {"id": 381, "seek": 162960, "start": 1651.6799999999998, "end": 1656.52, "text": " that it uses exactly the same vocab that it used for the language model.", "tokens": [300, 309, 4960, 2293, 264, 912, 2329, 455, 300, 309, 1143, 337, 264, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.09919316551902077, "compression_ratio": 1.7617021276595746, "no_speech_prob": 8.013329534151126e-06}, {"id": 382, "seek": 165652, "start": 1656.52, "end": 1661.84, "text": " If word number 10 was the in the language model, we need to make sure that word number", "tokens": [759, 1349, 1230, 1266, 390, 264, 294, 264, 2856, 2316, 11, 321, 643, 281, 652, 988, 300, 1349, 1230], "temperature": 0.0, "avg_logprob": -0.14916729010068452, "compression_ratio": 1.7762557077625571, "no_speech_prob": 3.0415783385251416e-06}, {"id": 383, "seek": 165652, "start": 1661.84, "end": 1668.56, "text": " 10 is the in the classifier because otherwise the pre-trained model is going to be totally", "tokens": [1266, 307, 264, 294, 264, 1508, 9902, 570, 5911, 264, 659, 12, 17227, 2001, 2316, 307, 516, 281, 312, 3879], "temperature": 0.0, "avg_logprob": -0.14916729010068452, "compression_ratio": 1.7762557077625571, "no_speech_prob": 3.0415783385251416e-06}, {"id": 384, "seek": 165652, "start": 1668.56, "end": 1669.56, "text": " meaningless.", "tokens": [33232, 13], "temperature": 0.0, "avg_logprob": -0.14916729010068452, "compression_ratio": 1.7762557077625571, "no_speech_prob": 3.0415783385251416e-06}, {"id": 385, "seek": 165652, "start": 1669.56, "end": 1674.8, "text": " So that's why we pass in the vocab from the language model, to make sure that this data", "tokens": [407, 300, 311, 983, 321, 1320, 294, 264, 2329, 455, 490, 264, 2856, 2316, 11, 281, 652, 988, 300, 341, 1412], "temperature": 0.0, "avg_logprob": -0.14916729010068452, "compression_ratio": 1.7762557077625571, "no_speech_prob": 3.0415783385251416e-06}, {"id": 386, "seek": 165652, "start": 1674.8, "end": 1677.16, "text": " bunch is going to have exactly the same vocab.", "tokens": [3840, 307, 516, 281, 362, 2293, 264, 912, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.14916729010068452, "compression_ratio": 1.7762557077625571, "no_speech_prob": 3.0415783385251416e-06}, {"id": 387, "seek": 165652, "start": 1677.16, "end": 1680.04, "text": " That's an important step.", "tokens": [663, 311, 364, 1021, 1823, 13], "temperature": 0.0, "avg_logprob": -0.14916729010068452, "compression_ratio": 1.7762557077625571, "no_speech_prob": 3.0415783385251416e-06}, {"id": 388, "seek": 165652, "start": 1680.04, "end": 1683.0, "text": " Split by folder, and this time label.", "tokens": [45111, 538, 10820, 11, 293, 341, 565, 7645, 13], "temperature": 0.0, "avg_logprob": -0.14916729010068452, "compression_ratio": 1.7762557077625571, "no_speech_prob": 3.0415783385251416e-06}, {"id": 389, "seek": 168300, "start": 1683.0, "end": 1688.24, "text": " So remember the last time we had split randomly, but this time we need to make sure that the", "tokens": [407, 1604, 264, 1036, 565, 321, 632, 7472, 16979, 11, 457, 341, 565, 321, 643, 281, 652, 988, 300, 264], "temperature": 0.0, "avg_logprob": -0.13474472002549606, "compression_ratio": 1.674757281553398, "no_speech_prob": 5.173886620468693e-06}, {"id": 390, "seek": 168300, "start": 1688.24, "end": 1693.2, "text": " labels of the test set are not touched, so we split by folder.", "tokens": [16949, 295, 264, 1500, 992, 366, 406, 9828, 11, 370, 321, 7472, 538, 10820, 13], "temperature": 0.0, "avg_logprob": -0.13474472002549606, "compression_ratio": 1.674757281553398, "no_speech_prob": 5.173886620468693e-06}, {"id": 391, "seek": 168300, "start": 1693.2, "end": 1700.72, "text": " And then this time we label it not for a language model, but we label these classes.", "tokens": [400, 550, 341, 565, 321, 7645, 309, 406, 337, 257, 2856, 2316, 11, 457, 321, 7645, 613, 5359, 13], "temperature": 0.0, "avg_logprob": -0.13474472002549606, "compression_ratio": 1.674757281553398, "no_speech_prob": 5.173886620468693e-06}, {"id": 392, "seek": 168300, "start": 1700.72, "end": 1703.6, "text": " And then finally create a data bunch.", "tokens": [400, 550, 2721, 1884, 257, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.13474472002549606, "compression_ratio": 1.674757281553398, "no_speech_prob": 5.173886620468693e-06}, {"id": 393, "seek": 168300, "start": 1703.6, "end": 1707.8, "text": " And remember sometimes you'll find that you run out of GPU memory.", "tokens": [400, 1604, 2171, 291, 603, 915, 300, 291, 1190, 484, 295, 18407, 4675, 13], "temperature": 0.0, "avg_logprob": -0.13474472002549606, "compression_ratio": 1.674757281553398, "no_speech_prob": 5.173886620468693e-06}, {"id": 394, "seek": 170780, "start": 1707.8, "end": 1713.34, "text": " This will very often happen to you if you, so I was running this in an 11 gig machine,", "tokens": [639, 486, 588, 2049, 1051, 281, 291, 498, 291, 11, 370, 286, 390, 2614, 341, 294, 364, 2975, 8741, 3479, 11], "temperature": 0.0, "avg_logprob": -0.13430167488429856, "compression_ratio": 1.5538461538461539, "no_speech_prob": 2.318671795364935e-05}, {"id": 395, "seek": 170780, "start": 1713.34, "end": 1716.58, "text": " so you should make sure this number is a bit lower if you run out of memory.", "tokens": [370, 291, 820, 652, 988, 341, 1230, 307, 257, 857, 3126, 498, 291, 1190, 484, 295, 4675, 13], "temperature": 0.0, "avg_logprob": -0.13430167488429856, "compression_ratio": 1.5538461538461539, "no_speech_prob": 2.318671795364935e-05}, {"id": 396, "seek": 170780, "start": 1716.58, "end": 1720.48, "text": " You may also want to make sure you restart the notebook and kind of start it just from", "tokens": [509, 815, 611, 528, 281, 652, 988, 291, 21022, 264, 21060, 293, 733, 295, 722, 309, 445, 490], "temperature": 0.0, "avg_logprob": -0.13430167488429856, "compression_ratio": 1.5538461538461539, "no_speech_prob": 2.318671795364935e-05}, {"id": 397, "seek": 170780, "start": 1720.48, "end": 1721.48, "text": " here.", "tokens": [510, 13], "temperature": 0.0, "avg_logprob": -0.13430167488429856, "compression_ratio": 1.5538461538461539, "no_speech_prob": 2.318671795364935e-05}, {"id": 398, "seek": 170780, "start": 1721.48, "end": 1725.72, "text": " So batch size 50 is as high as I could get on an 11 gig card.", "tokens": [407, 15245, 2744, 2625, 307, 382, 1090, 382, 286, 727, 483, 322, 364, 2975, 8741, 2920, 13], "temperature": 0.0, "avg_logprob": -0.13430167488429856, "compression_ratio": 1.5538461538461539, "no_speech_prob": 2.318671795364935e-05}, {"id": 399, "seek": 170780, "start": 1725.72, "end": 1734.08, "text": " If you're using a P2 or P3 on Amazon or the K80 on Google for example, I think you'll", "tokens": [759, 291, 434, 1228, 257, 430, 17, 420, 430, 18, 322, 6795, 420, 264, 591, 4702, 322, 3329, 337, 1365, 11, 286, 519, 291, 603], "temperature": 0.0, "avg_logprob": -0.13430167488429856, "compression_ratio": 1.5538461538461539, "no_speech_prob": 2.318671795364935e-05}, {"id": 400, "seek": 173408, "start": 1734.08, "end": 1738.6999999999998, "text": " get 16 gigs, so you might be able to make this a bit higher, get it up to 64.", "tokens": [483, 3165, 34586, 11, 370, 291, 1062, 312, 1075, 281, 652, 341, 257, 857, 2946, 11, 483, 309, 493, 281, 12145, 13], "temperature": 0.0, "avg_logprob": -0.122478271484375, "compression_ratio": 1.6308243727598566, "no_speech_prob": 8.139556484820787e-06}, {"id": 401, "seek": 173408, "start": 1738.6999999999998, "end": 1743.12, "text": " So you can find whatever batch size fits on your card.", "tokens": [407, 291, 393, 915, 2035, 15245, 2744, 9001, 322, 428, 2920, 13], "temperature": 0.0, "avg_logprob": -0.122478271484375, "compression_ratio": 1.6308243727598566, "no_speech_prob": 8.139556484820787e-06}, {"id": 402, "seek": 173408, "start": 1743.12, "end": 1748.1, "text": " So here's our data bunch as we saw before and the labels.", "tokens": [407, 510, 311, 527, 1412, 3840, 382, 321, 1866, 949, 293, 264, 16949, 13], "temperature": 0.0, "avg_logprob": -0.122478271484375, "compression_ratio": 1.6308243727598566, "no_speech_prob": 8.139556484820787e-06}, {"id": 403, "seek": 173408, "start": 1748.1, "end": 1753.08, "text": " So this time rather than creating a language model learner, we're creating a text classifier", "tokens": [407, 341, 565, 2831, 813, 4084, 257, 2856, 2316, 33347, 11, 321, 434, 4084, 257, 2487, 1508, 9902], "temperature": 0.0, "avg_logprob": -0.122478271484375, "compression_ratio": 1.6308243727598566, "no_speech_prob": 8.139556484820787e-06}, {"id": 404, "seek": 173408, "start": 1753.08, "end": 1754.08, "text": " learner.", "tokens": [33347, 13], "temperature": 0.0, "avg_logprob": -0.122478271484375, "compression_ratio": 1.6308243727598566, "no_speech_prob": 8.139556484820787e-06}, {"id": 405, "seek": 173408, "start": 1754.08, "end": 1758.62, "text": " But again, same thing, pass in the data that we want, figure out how much regularization", "tokens": [583, 797, 11, 912, 551, 11, 1320, 294, 264, 1412, 300, 321, 528, 11, 2573, 484, 577, 709, 3890, 2144], "temperature": 0.0, "avg_logprob": -0.122478271484375, "compression_ratio": 1.6308243727598566, "no_speech_prob": 8.139556484820787e-06}, {"id": 406, "seek": 173408, "start": 1758.62, "end": 1759.62, "text": " we need.", "tokens": [321, 643, 13], "temperature": 0.0, "avg_logprob": -0.122478271484375, "compression_ratio": 1.6308243727598566, "no_speech_prob": 8.139556484820787e-06}, {"id": 407, "seek": 173408, "start": 1759.62, "end": 1763.28, "text": " Again, if you're overfitting, then you can increase this number.", "tokens": [3764, 11, 498, 291, 434, 670, 69, 2414, 11, 550, 291, 393, 3488, 341, 1230, 13], "temperature": 0.0, "avg_logprob": -0.122478271484375, "compression_ratio": 1.6308243727598566, "no_speech_prob": 8.139556484820787e-06}, {"id": 408, "seek": 176328, "start": 1763.28, "end": 1766.12, "text": " If you're underfitting, you can decrease the number.", "tokens": [759, 291, 434, 833, 69, 2414, 11, 291, 393, 11514, 264, 1230, 13], "temperature": 0.0, "avg_logprob": -0.14527983414499382, "compression_ratio": 1.5873015873015872, "no_speech_prob": 5.771884389105253e-06}, {"id": 409, "seek": 176328, "start": 1766.12, "end": 1769.28, "text": " And most importantly, load in our pre-trained model.", "tokens": [400, 881, 8906, 11, 3677, 294, 527, 659, 12, 17227, 2001, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14527983414499382, "compression_ratio": 1.5873015873015872, "no_speech_prob": 5.771884389105253e-06}, {"id": 410, "seek": 176328, "start": 1769.28, "end": 1773.56, "text": " And remember specifically, it's this half of the model called the encoder, which is", "tokens": [400, 1604, 4682, 11, 309, 311, 341, 1922, 295, 264, 2316, 1219, 264, 2058, 19866, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.14527983414499382, "compression_ratio": 1.5873015873015872, "no_speech_prob": 5.771884389105253e-06}, {"id": 411, "seek": 176328, "start": 1773.56, "end": 1776.32, "text": " the bit that we want to load in.", "tokens": [264, 857, 300, 321, 528, 281, 3677, 294, 13], "temperature": 0.0, "avg_logprob": -0.14527983414499382, "compression_ratio": 1.5873015873015872, "no_speech_prob": 5.771884389105253e-06}, {"id": 412, "seek": 176328, "start": 1776.32, "end": 1777.32, "text": " And freeze.", "tokens": [400, 15959, 13], "temperature": 0.0, "avg_logprob": -0.14527983414499382, "compression_ratio": 1.5873015873015872, "no_speech_prob": 5.771884389105253e-06}, {"id": 413, "seek": 176328, "start": 1777.32, "end": 1783.72, "text": " LR find, find the learning rate and fit for a little bit.", "tokens": [441, 49, 915, 11, 915, 264, 2539, 3314, 293, 3318, 337, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.14527983414499382, "compression_ratio": 1.5873015873015872, "no_speech_prob": 5.771884389105253e-06}, {"id": 414, "seek": 176328, "start": 1783.72, "end": 1790.16, "text": " And we're already up nearly to 92% accuracy after less than 3 minutes of training.", "tokens": [400, 321, 434, 1217, 493, 6217, 281, 28225, 4, 14170, 934, 1570, 813, 805, 2077, 295, 3097, 13], "temperature": 0.0, "avg_logprob": -0.14527983414499382, "compression_ratio": 1.5873015873015872, "no_speech_prob": 5.771884389105253e-06}, {"id": 415, "seek": 176328, "start": 1790.16, "end": 1791.48, "text": " So this is a nice thing.", "tokens": [407, 341, 307, 257, 1481, 551, 13], "temperature": 0.0, "avg_logprob": -0.14527983414499382, "compression_ratio": 1.5873015873015872, "no_speech_prob": 5.771884389105253e-06}, {"id": 416, "seek": 179148, "start": 1791.48, "end": 1798.76, "text": " In your particular domain, whether it be law or medicine or journalism or government or", "tokens": [682, 428, 1729, 9274, 11, 1968, 309, 312, 2101, 420, 7195, 420, 23191, 420, 2463, 420], "temperature": 0.0, "avg_logprob": -0.13207962638453433, "compression_ratio": 1.5333333333333334, "no_speech_prob": 3.340511739224894e-06}, {"id": 417, "seek": 179148, "start": 1798.76, "end": 1805.8, "text": " whatever, you probably only need to train your domain's language model once.", "tokens": [2035, 11, 291, 1391, 787, 643, 281, 3847, 428, 9274, 311, 2856, 2316, 1564, 13], "temperature": 0.0, "avg_logprob": -0.13207962638453433, "compression_ratio": 1.5333333333333334, "no_speech_prob": 3.340511739224894e-06}, {"id": 418, "seek": 179148, "start": 1805.8, "end": 1810.16, "text": " And that might take overnight to train well.", "tokens": [400, 300, 1062, 747, 13935, 281, 3847, 731, 13], "temperature": 0.0, "avg_logprob": -0.13207962638453433, "compression_ratio": 1.5333333333333334, "no_speech_prob": 3.340511739224894e-06}, {"id": 419, "seek": 179148, "start": 1810.16, "end": 1815.08, "text": " But once you've got it, you can now very quickly create all kinds of different classifiers", "tokens": [583, 1564, 291, 600, 658, 309, 11, 291, 393, 586, 588, 2661, 1884, 439, 3685, 295, 819, 1508, 23463], "temperature": 0.0, "avg_logprob": -0.13207962638453433, "compression_ratio": 1.5333333333333334, "no_speech_prob": 3.340511739224894e-06}, {"id": 420, "seek": 179148, "start": 1815.08, "end": 1818.04, "text": " and models with that.", "tokens": [293, 5245, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.13207962638453433, "compression_ratio": 1.5333333333333334, "no_speech_prob": 3.340511739224894e-06}, {"id": 421, "seek": 181804, "start": 1818.04, "end": 1822.28, "text": " In this case, already a pretty good model after 3 minutes.", "tokens": [682, 341, 1389, 11, 1217, 257, 1238, 665, 2316, 934, 805, 2077, 13], "temperature": 0.0, "avg_logprob": -0.12393854232061477, "compression_ratio": 1.6811023622047243, "no_speech_prob": 2.4682440198375843e-05}, {"id": 422, "seek": 181804, "start": 1822.28, "end": 1828.2, "text": " So when you first start doing this, you might find it a bit annoying that your first models", "tokens": [407, 562, 291, 700, 722, 884, 341, 11, 291, 1062, 915, 309, 257, 857, 11304, 300, 428, 700, 5245], "temperature": 0.0, "avg_logprob": -0.12393854232061477, "compression_ratio": 1.6811023622047243, "no_speech_prob": 2.4682440198375843e-05}, {"id": 423, "seek": 181804, "start": 1828.2, "end": 1832.04, "text": " take 4 hours or more to create that language model.", "tokens": [747, 1017, 2496, 420, 544, 281, 1884, 300, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12393854232061477, "compression_ratio": 1.6811023622047243, "no_speech_prob": 2.4682440198375843e-05}, {"id": 424, "seek": 181804, "start": 1832.04, "end": 1836.56, "text": " But the key thing to remember is you only have to do that once for your entire domain", "tokens": [583, 264, 2141, 551, 281, 1604, 307, 291, 787, 362, 281, 360, 300, 1564, 337, 428, 2302, 9274], "temperature": 0.0, "avg_logprob": -0.12393854232061477, "compression_ratio": 1.6811023622047243, "no_speech_prob": 2.4682440198375843e-05}, {"id": 425, "seek": 181804, "start": 1836.56, "end": 1837.96, "text": " of stuff that you're interested in.", "tokens": [295, 1507, 300, 291, 434, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.12393854232061477, "compression_ratio": 1.6811023622047243, "no_speech_prob": 2.4682440198375843e-05}, {"id": 426, "seek": 181804, "start": 1837.96, "end": 1847.04, "text": " And then you can build lots of different classifiers and other models on top of that in a few minutes.", "tokens": [400, 550, 291, 393, 1322, 3195, 295, 819, 1508, 23463, 293, 661, 5245, 322, 1192, 295, 300, 294, 257, 1326, 2077, 13], "temperature": 0.0, "avg_logprob": -0.12393854232061477, "compression_ratio": 1.6811023622047243, "no_speech_prob": 2.4682440198375843e-05}, {"id": 427, "seek": 184704, "start": 1847.04, "end": 1849.8799999999999, "text": " So we can save that to make sure we don't have to run it again.", "tokens": [407, 321, 393, 3155, 300, 281, 652, 988, 321, 500, 380, 362, 281, 1190, 309, 797, 13], "temperature": 0.0, "avg_logprob": -0.14943465319546786, "compression_ratio": 1.8016194331983806, "no_speech_prob": 5.224867709330283e-05}, {"id": 428, "seek": 184704, "start": 1849.8799999999999, "end": 1850.8799999999999, "text": " And then here's something interesting.", "tokens": [400, 550, 510, 311, 746, 1880, 13], "temperature": 0.0, "avg_logprob": -0.14943465319546786, "compression_ratio": 1.8016194331983806, "no_speech_prob": 5.224867709330283e-05}, {"id": 429, "seek": 184704, "start": 1850.8799999999999, "end": 1853.8799999999999, "text": " I'm going to explain this more in just a few minutes.", "tokens": [286, 478, 516, 281, 2903, 341, 544, 294, 445, 257, 1326, 2077, 13], "temperature": 0.0, "avg_logprob": -0.14943465319546786, "compression_ratio": 1.8016194331983806, "no_speech_prob": 5.224867709330283e-05}, {"id": 430, "seek": 184704, "start": 1853.8799999999999, "end": 1856.48, "text": " I'm not going to say unfreeze.", "tokens": [286, 478, 406, 516, 281, 584, 3971, 701, 1381, 13], "temperature": 0.0, "avg_logprob": -0.14943465319546786, "compression_ratio": 1.8016194331983806, "no_speech_prob": 5.224867709330283e-05}, {"id": 431, "seek": 184704, "start": 1856.48, "end": 1858.32, "text": " Instead I'm going to say freeze2.", "tokens": [7156, 286, 478, 516, 281, 584, 15959, 17, 13], "temperature": 0.0, "avg_logprob": -0.14943465319546786, "compression_ratio": 1.8016194331983806, "no_speech_prob": 5.224867709330283e-05}, {"id": 432, "seek": 184704, "start": 1858.32, "end": 1862.2, "text": " And what that says is unfreeze the last 2 layers.", "tokens": [400, 437, 300, 1619, 307, 3971, 701, 1381, 264, 1036, 568, 7914, 13], "temperature": 0.0, "avg_logprob": -0.14943465319546786, "compression_ratio": 1.8016194331983806, "no_speech_prob": 5.224867709330283e-05}, {"id": 433, "seek": 184704, "start": 1862.2, "end": 1863.98, "text": " Don't unfreeze the whole thing.", "tokens": [1468, 380, 3971, 701, 1381, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.14943465319546786, "compression_ratio": 1.8016194331983806, "no_speech_prob": 5.224867709330283e-05}, {"id": 434, "seek": 184704, "start": 1863.98, "end": 1869.08, "text": " And so we've just found it really helps with these text classification, not to unfreeze", "tokens": [400, 370, 321, 600, 445, 1352, 309, 534, 3665, 365, 613, 2487, 21538, 11, 406, 281, 3971, 701, 1381], "temperature": 0.0, "avg_logprob": -0.14943465319546786, "compression_ratio": 1.8016194331983806, "no_speech_prob": 5.224867709330283e-05}, {"id": 435, "seek": 184704, "start": 1869.08, "end": 1872.94, "text": " the whole thing, but to unfreeze one layer at a time.", "tokens": [264, 1379, 551, 11, 457, 281, 3971, 701, 1381, 472, 4583, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.14943465319546786, "compression_ratio": 1.8016194331983806, "no_speech_prob": 5.224867709330283e-05}, {"id": 436, "seek": 187294, "start": 1872.94, "end": 1879.3600000000001, "text": " So unfreeze the last 2 layers, train it a little bit more, unfreeze the next layer again,", "tokens": [407, 3971, 701, 1381, 264, 1036, 568, 7914, 11, 3847, 309, 257, 707, 857, 544, 11, 3971, 701, 1381, 264, 958, 4583, 797, 11], "temperature": 0.0, "avg_logprob": -0.1679206750331781, "compression_ratio": 1.8070175438596492, "no_speech_prob": 1.1125475793960504e-05}, {"id": 437, "seek": 187294, "start": 1879.3600000000001, "end": 1884.76, "text": " train it a little bit more, unfreeze the whole thing, train it a little bit more.", "tokens": [3847, 309, 257, 707, 857, 544, 11, 3971, 701, 1381, 264, 1379, 551, 11, 3847, 309, 257, 707, 857, 544, 13], "temperature": 0.0, "avg_logprob": -0.1679206750331781, "compression_ratio": 1.8070175438596492, "no_speech_prob": 1.1125475793960504e-05}, {"id": 438, "seek": 187294, "start": 1884.76, "end": 1890.6000000000001, "text": " You'll also see I'm passing in this thing, momentum, equals.8.7.", "tokens": [509, 603, 611, 536, 286, 478, 8437, 294, 341, 551, 11, 11244, 11, 6915, 2411, 23, 13, 22, 13], "temperature": 0.0, "avg_logprob": -0.1679206750331781, "compression_ratio": 1.8070175438596492, "no_speech_prob": 1.1125475793960504e-05}, {"id": 439, "seek": 187294, "start": 1890.6000000000001, "end": 1896.16, "text": " We're going to learn exactly what that means in the next week or two, probably next week.", "tokens": [492, 434, 516, 281, 1466, 2293, 437, 300, 1355, 294, 264, 958, 1243, 420, 732, 11, 1391, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.1679206750331781, "compression_ratio": 1.8070175438596492, "no_speech_prob": 1.1125475793960504e-05}, {"id": 440, "seek": 187294, "start": 1896.16, "end": 1900.92, "text": " But for now, and we may even automate it, so maybe by the time you watch the video of", "tokens": [583, 337, 586, 11, 293, 321, 815, 754, 31605, 309, 11, 370, 1310, 538, 264, 565, 291, 1159, 264, 960, 295], "temperature": 0.0, "avg_logprob": -0.1679206750331781, "compression_ratio": 1.8070175438596492, "no_speech_prob": 1.1125475793960504e-05}, {"id": 441, "seek": 190092, "start": 1900.92, "end": 1904.0, "text": " this, this won't even be necessary anymore.", "tokens": [341, 11, 341, 1582, 380, 754, 312, 4818, 3602, 13], "temperature": 0.0, "avg_logprob": -0.15270158377560702, "compression_ratio": 1.5592417061611374, "no_speech_prob": 3.321324402350001e-05}, {"id": 442, "seek": 190092, "start": 1904.0, "end": 1910.3200000000002, "text": " Basically we've found for training recurrent neural networks, RNNs, it really helps to", "tokens": [8537, 321, 600, 1352, 337, 3097, 18680, 1753, 18161, 9590, 11, 45702, 45, 82, 11, 309, 534, 3665, 281], "temperature": 0.0, "avg_logprob": -0.15270158377560702, "compression_ratio": 1.5592417061611374, "no_speech_prob": 3.321324402350001e-05}, {"id": 443, "seek": 190092, "start": 1910.3200000000002, "end": 1912.3600000000001, "text": " decrease the momentum a little bit.", "tokens": [11514, 264, 11244, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.15270158377560702, "compression_ratio": 1.5592417061611374, "no_speech_prob": 3.321324402350001e-05}, {"id": 444, "seek": 190092, "start": 1912.3600000000001, "end": 1915.16, "text": " So that's what that is.", "tokens": [407, 300, 311, 437, 300, 307, 13], "temperature": 0.0, "avg_logprob": -0.15270158377560702, "compression_ratio": 1.5592417061611374, "no_speech_prob": 3.321324402350001e-05}, {"id": 445, "seek": 190092, "start": 1915.16, "end": 1922.2, "text": " So that gets us a 94.4 accuracy after about half an hour or less of training, actually", "tokens": [407, 300, 2170, 505, 257, 30849, 13, 19, 14170, 934, 466, 1922, 364, 1773, 420, 1570, 295, 3097, 11, 767], "temperature": 0.0, "avg_logprob": -0.15270158377560702, "compression_ratio": 1.5592417061611374, "no_speech_prob": 3.321324402350001e-05}, {"id": 446, "seek": 190092, "start": 1922.2, "end": 1926.92, "text": " quite a lot less of training the actual classifier.", "tokens": [1596, 257, 688, 1570, 295, 3097, 264, 3539, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.15270158377560702, "compression_ratio": 1.5592417061611374, "no_speech_prob": 3.321324402350001e-05}, {"id": 447, "seek": 192692, "start": 1926.92, "end": 1931.8400000000001, "text": " And we can actually get this quite a bit better with a few tricks.", "tokens": [400, 321, 393, 767, 483, 341, 1596, 257, 857, 1101, 365, 257, 1326, 11733, 13], "temperature": 0.0, "avg_logprob": -0.15467101969617478, "compression_ratio": 1.5065502183406114, "no_speech_prob": 2.58662457781611e-05}, {"id": 448, "seek": 192692, "start": 1931.8400000000001, "end": 1935.8000000000002, "text": " I don't know if we'll learn all the tricks this part, it might be the next part, but", "tokens": [286, 500, 380, 458, 498, 321, 603, 1466, 439, 264, 11733, 341, 644, 11, 309, 1062, 312, 264, 958, 644, 11, 457], "temperature": 0.0, "avg_logprob": -0.15467101969617478, "compression_ratio": 1.5065502183406114, "no_speech_prob": 2.58662457781611e-05}, {"id": 449, "seek": 192692, "start": 1935.8000000000002, "end": 1941.3400000000001, "text": " even this very simple kind of standard approach is pretty great.", "tokens": [754, 341, 588, 2199, 733, 295, 3832, 3109, 307, 1238, 869, 13], "temperature": 0.0, "avg_logprob": -0.15467101969617478, "compression_ratio": 1.5065502183406114, "no_speech_prob": 2.58662457781611e-05}, {"id": 450, "seek": 192692, "start": 1941.3400000000001, "end": 1948.72, "text": " If we compare it to last year's state of the art on IMDB, this is from the Cove paper from", "tokens": [759, 321, 6794, 309, 281, 1036, 1064, 311, 1785, 295, 264, 1523, 322, 21463, 27735, 11, 341, 307, 490, 264, 383, 1682, 3035, 490], "temperature": 0.0, "avg_logprob": -0.15467101969617478, "compression_ratio": 1.5065502183406114, "no_speech_prob": 2.58662457781611e-05}, {"id": 451, "seek": 192692, "start": 1948.72, "end": 1952.38, "text": " McCann et al. at Salesforce Research.", "tokens": [12061, 969, 1030, 419, 13, 412, 40398, 10303, 13], "temperature": 0.0, "avg_logprob": -0.15467101969617478, "compression_ratio": 1.5065502183406114, "no_speech_prob": 2.58662457781611e-05}, {"id": 452, "seek": 195238, "start": 1952.38, "end": 1959.92, "text": " Their paper was 91.8% accurate, and the best paper they could find, they found a fairly", "tokens": [6710, 3035, 390, 31064, 13, 23, 4, 8559, 11, 293, 264, 1151, 3035, 436, 727, 915, 11, 436, 1352, 257, 6457], "temperature": 0.0, "avg_logprob": -0.14009773899132097, "compression_ratio": 1.4350282485875707, "no_speech_prob": 8.39788481243886e-06}, {"id": 453, "seek": 195238, "start": 1959.92, "end": 1969.88, "text": " domain specific sentiment analysis paper from 2017 that got 94.1, and here we've got 94.4.", "tokens": [9274, 2685, 16149, 5215, 3035, 490, 6591, 300, 658, 30849, 13, 16, 11, 293, 510, 321, 600, 658, 30849, 13, 19, 13], "temperature": 0.0, "avg_logprob": -0.14009773899132097, "compression_ratio": 1.4350282485875707, "no_speech_prob": 8.39788481243886e-06}, {"id": 454, "seek": 195238, "start": 1969.88, "end": 1976.4, "text": " And the best models I've been able to build since have been about 95, 95.1.", "tokens": [400, 264, 1151, 5245, 286, 600, 668, 1075, 281, 1322, 1670, 362, 668, 466, 13420, 11, 13420, 13, 16, 13], "temperature": 0.0, "avg_logprob": -0.14009773899132097, "compression_ratio": 1.4350282485875707, "no_speech_prob": 8.39788481243886e-06}, {"id": 455, "seek": 197640, "start": 1976.4, "end": 1982.3400000000001, "text": " So if you're looking to do text classification, this really standardized transfer learning", "tokens": [407, 498, 291, 434, 1237, 281, 360, 2487, 21538, 11, 341, 534, 31677, 5003, 2539], "temperature": 0.0, "avg_logprob": -0.20472760131393652, "compression_ratio": 1.394736842105263, "no_speech_prob": 6.048812338121934e-06}, {"id": 456, "seek": 197640, "start": 1982.3400000000001, "end": 1986.4, "text": " approach works super well.", "tokens": [3109, 1985, 1687, 731, 13], "temperature": 0.0, "avg_logprob": -0.20472760131393652, "compression_ratio": 1.394736842105263, "no_speech_prob": 6.048812338121934e-06}, {"id": 457, "seek": 197640, "start": 1986.4, "end": 1991.48, "text": " Any questions, Rachel?", "tokens": [2639, 1651, 11, 14246, 30], "temperature": 0.0, "avg_logprob": -0.20472760131393652, "compression_ratio": 1.394736842105263, "no_speech_prob": 6.048812338121934e-06}, {"id": 458, "seek": 197640, "start": 1991.48, "end": 1998.16, "text": " So that was NLP, and we'll be learning more about NLP later in this course.", "tokens": [407, 300, 390, 426, 45196, 11, 293, 321, 603, 312, 2539, 544, 466, 426, 45196, 1780, 294, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.20472760131393652, "compression_ratio": 1.394736842105263, "no_speech_prob": 6.048812338121934e-06}, {"id": 459, "seek": 197640, "start": 1998.16, "end": 2002.72, "text": " Now I wanted to switch over and look at Tabular.", "tokens": [823, 286, 1415, 281, 3679, 670, 293, 574, 412, 14106, 1040, 13], "temperature": 0.0, "avg_logprob": -0.20472760131393652, "compression_ratio": 1.394736842105263, "no_speech_prob": 6.048812338121934e-06}, {"id": 460, "seek": 200272, "start": 2002.72, "end": 2008.3600000000001, "text": " Now Tabular data is pretty interesting because it's the stuff that for a lot of you is actually", "tokens": [823, 14106, 1040, 1412, 307, 1238, 1880, 570, 309, 311, 264, 1507, 300, 337, 257, 688, 295, 291, 307, 767], "temperature": 0.0, "avg_logprob": -0.19784254423329528, "compression_ratio": 1.4414893617021276, "no_speech_prob": 6.204786768648773e-05}, {"id": 461, "seek": 200272, "start": 2008.3600000000001, "end": 2018.56, "text": " what you use day to day at work in spreadsheets and relational databases.", "tokens": [437, 291, 764, 786, 281, 786, 412, 589, 294, 23651, 1385, 293, 38444, 22380, 13], "temperature": 0.0, "avg_logprob": -0.19784254423329528, "compression_ratio": 1.4414893617021276, "no_speech_prob": 6.204786768648773e-05}, {"id": 462, "seek": 200272, "start": 2018.56, "end": 2026.16, "text": " So where does the magic number of 2.6 to the 4th in the learning rate come from?", "tokens": [407, 689, 775, 264, 5585, 1230, 295, 568, 13, 21, 281, 264, 1017, 392, 294, 264, 2539, 3314, 808, 490, 30], "temperature": 0.0, "avg_logprob": -0.19784254423329528, "compression_ratio": 1.4414893617021276, "no_speech_prob": 6.204786768648773e-05}, {"id": 463, "seek": 200272, "start": 2026.16, "end": 2030.14, "text": " Yeah, good question.", "tokens": [865, 11, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.19784254423329528, "compression_ratio": 1.4414893617021276, "no_speech_prob": 6.204786768648773e-05}, {"id": 464, "seek": 203014, "start": 2030.14, "end": 2040.8400000000001, "text": " So the learning rate is various things divided by 2.6 to the 4th.", "tokens": [407, 264, 2539, 3314, 307, 3683, 721, 6666, 538, 568, 13, 21, 281, 264, 1017, 392, 13], "temperature": 0.0, "avg_logprob": -0.16569875498287012, "compression_ratio": 1.3359375, "no_speech_prob": 6.854252205812372e-06}, {"id": 465, "seek": 203014, "start": 2040.8400000000001, "end": 2047.44, "text": " The reason it's to the 4th, you will learn about at the end of today.", "tokens": [440, 1778, 309, 311, 281, 264, 1017, 392, 11, 291, 486, 1466, 466, 412, 264, 917, 295, 965, 13], "temperature": 0.0, "avg_logprob": -0.16569875498287012, "compression_ratio": 1.3359375, "no_speech_prob": 6.854252205812372e-06}, {"id": 466, "seek": 203014, "start": 2047.44, "end": 2048.88, "text": " So let's focus on the 2.6.", "tokens": [407, 718, 311, 1879, 322, 264, 568, 13, 21, 13], "temperature": 0.0, "avg_logprob": -0.16569875498287012, "compression_ratio": 1.3359375, "no_speech_prob": 6.854252205812372e-06}, {"id": 467, "seek": 203014, "start": 2048.88, "end": 2054.56, "text": " Why 2.6?", "tokens": [1545, 568, 13, 21, 30], "temperature": 0.0, "avg_logprob": -0.16569875498287012, "compression_ratio": 1.3359375, "no_speech_prob": 6.854252205812372e-06}, {"id": 468, "seek": 205456, "start": 2054.56, "end": 2061.72, "text": " Basically as we're going to see in more detail later today, the difference between the bottom", "tokens": [8537, 382, 321, 434, 516, 281, 536, 294, 544, 2607, 1780, 965, 11, 264, 2649, 1296, 264, 2767], "temperature": 0.0, "avg_logprob": -0.12391415596008301, "compression_ratio": 1.8008474576271187, "no_speech_prob": 2.368777586525539e-06}, {"id": 469, "seek": 205456, "start": 2061.72, "end": 2065.88, "text": " of the slice and the top of the slice is basically what's the difference between how quickly", "tokens": [295, 264, 13153, 293, 264, 1192, 295, 264, 13153, 307, 1936, 437, 311, 264, 2649, 1296, 577, 2661], "temperature": 0.0, "avg_logprob": -0.12391415596008301, "compression_ratio": 1.8008474576271187, "no_speech_prob": 2.368777586525539e-06}, {"id": 470, "seek": 205456, "start": 2065.88, "end": 2071.68, "text": " the lowest layer of the model learns versus the highest layer of the model learns.", "tokens": [264, 12437, 4583, 295, 264, 2316, 27152, 5717, 264, 6343, 4583, 295, 264, 2316, 27152, 13], "temperature": 0.0, "avg_logprob": -0.12391415596008301, "compression_ratio": 1.8008474576271187, "no_speech_prob": 2.368777586525539e-06}, {"id": 471, "seek": 205456, "start": 2071.68, "end": 2073.88, "text": " So this is called discriminative learning rates.", "tokens": [407, 341, 307, 1219, 20828, 1166, 2539, 6846, 13], "temperature": 0.0, "avg_logprob": -0.12391415596008301, "compression_ratio": 1.8008474576271187, "no_speech_prob": 2.368777586525539e-06}, {"id": 472, "seek": 205456, "start": 2073.88, "end": 2079.14, "text": " And so really the question is, as you go from layer to layer, how much do I decrease the", "tokens": [400, 370, 534, 264, 1168, 307, 11, 382, 291, 352, 490, 4583, 281, 4583, 11, 577, 709, 360, 286, 11514, 264], "temperature": 0.0, "avg_logprob": -0.12391415596008301, "compression_ratio": 1.8008474576271187, "no_speech_prob": 2.368777586525539e-06}, {"id": 473, "seek": 205456, "start": 2079.14, "end": 2081.44, "text": " learning rate by?", "tokens": [2539, 3314, 538, 30], "temperature": 0.0, "avg_logprob": -0.12391415596008301, "compression_ratio": 1.8008474576271187, "no_speech_prob": 2.368777586525539e-06}, {"id": 474, "seek": 208144, "start": 2081.44, "end": 2087.2400000000002, "text": " And we found out that for NLP RNNs that the answer is 2.6.", "tokens": [400, 321, 1352, 484, 300, 337, 426, 45196, 45702, 45, 82, 300, 264, 1867, 307, 568, 13, 21, 13], "temperature": 0.0, "avg_logprob": -0.13488899986698943, "compression_ratio": 1.648068669527897, "no_speech_prob": 7.889170774433296e-06}, {"id": 475, "seek": 208144, "start": 2087.2400000000002, "end": 2089.56, "text": " How do we find out that it's 2.6?", "tokens": [1012, 360, 321, 915, 484, 300, 309, 311, 568, 13, 21, 30], "temperature": 0.0, "avg_logprob": -0.13488899986698943, "compression_ratio": 1.648068669527897, "no_speech_prob": 7.889170774433296e-06}, {"id": 476, "seek": 208144, "start": 2089.56, "end": 2097.08, "text": " I ran lots and lots of different models like a year ago or so using lots of different sets", "tokens": [286, 5872, 3195, 293, 3195, 295, 819, 5245, 411, 257, 1064, 2057, 420, 370, 1228, 3195, 295, 819, 6352], "temperature": 0.0, "avg_logprob": -0.13488899986698943, "compression_ratio": 1.648068669527897, "no_speech_prob": 7.889170774433296e-06}, {"id": 477, "seek": 208144, "start": 2097.08, "end": 2102.0, "text": " of hyperparameters of various types, dropout, learning rates, discriminative learning rate", "tokens": [295, 9848, 2181, 335, 6202, 295, 3683, 3467, 11, 3270, 346, 11, 2539, 6846, 11, 20828, 1166, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.13488899986698943, "compression_ratio": 1.648068669527897, "no_speech_prob": 7.889170774433296e-06}, {"id": 478, "seek": 208144, "start": 2102.0, "end": 2103.2000000000003, "text": " and so forth.", "tokens": [293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.13488899986698943, "compression_ratio": 1.648068669527897, "no_speech_prob": 7.889170774433296e-06}, {"id": 479, "seek": 208144, "start": 2103.2000000000003, "end": 2107.68, "text": " And then I created something called a random forest, which is a kind of model where I attempted", "tokens": [400, 550, 286, 2942, 746, 1219, 257, 4974, 6719, 11, 597, 307, 257, 733, 295, 2316, 689, 286, 18997], "temperature": 0.0, "avg_logprob": -0.13488899986698943, "compression_ratio": 1.648068669527897, "no_speech_prob": 7.889170774433296e-06}, {"id": 480, "seek": 210768, "start": 2107.68, "end": 2114.48, "text": " to predict how accurate my NLP classifier would be based on the hyperparameters.", "tokens": [281, 6069, 577, 8559, 452, 426, 45196, 1508, 9902, 576, 312, 2361, 322, 264, 9848, 2181, 335, 6202, 13], "temperature": 0.0, "avg_logprob": -0.1139804150195832, "compression_ratio": 1.5555555555555556, "no_speech_prob": 6.747945462848293e-06}, {"id": 481, "seek": 210768, "start": 2114.48, "end": 2121.3999999999996, "text": " And then I used random forest interpretation methods to basically figure out what the optimal", "tokens": [400, 550, 286, 1143, 4974, 6719, 14174, 7150, 281, 1936, 2573, 484, 437, 264, 16252], "temperature": 0.0, "avg_logprob": -0.1139804150195832, "compression_ratio": 1.5555555555555556, "no_speech_prob": 6.747945462848293e-06}, {"id": 482, "seek": 210768, "start": 2121.3999999999996, "end": 2127.9199999999996, "text": " parameter settings were and I found out that the answer for this number was 2.6.", "tokens": [13075, 6257, 645, 293, 286, 1352, 484, 300, 264, 1867, 337, 341, 1230, 390, 568, 13, 21, 13], "temperature": 0.0, "avg_logprob": -0.1139804150195832, "compression_ratio": 1.5555555555555556, "no_speech_prob": 6.747945462848293e-06}, {"id": 483, "seek": 210768, "start": 2127.9199999999996, "end": 2131.6, "text": " So that's actually not something I've published or I don't think I've even talked about it", "tokens": [407, 300, 311, 767, 406, 746, 286, 600, 6572, 420, 286, 500, 380, 519, 286, 600, 754, 2825, 466, 309], "temperature": 0.0, "avg_logprob": -0.1139804150195832, "compression_ratio": 1.5555555555555556, "no_speech_prob": 6.747945462848293e-06}, {"id": 484, "seek": 210768, "start": 2131.6, "end": 2134.3199999999997, "text": " before so there's a new piece of information.", "tokens": [949, 370, 456, 311, 257, 777, 2522, 295, 1589, 13], "temperature": 0.0, "avg_logprob": -0.1139804150195832, "compression_ratio": 1.5555555555555556, "no_speech_prob": 6.747945462848293e-06}, {"id": 485, "seek": 213432, "start": 2134.32, "end": 2141.84, "text": " You can actually, a few months after I did this, I think Stephen Marody and somebody", "tokens": [509, 393, 767, 11, 257, 1326, 2493, 934, 286, 630, 341, 11, 286, 519, 13391, 2039, 843, 293, 2618], "temperature": 0.0, "avg_logprob": -0.15453972873917546, "compression_ratio": 1.5560344827586208, "no_speech_prob": 6.048672275937861e-06}, {"id": 486, "seek": 213432, "start": 2141.84, "end": 2148.6400000000003, "text": " else did publish a paper describing a similar approach so the basic idea may be out there", "tokens": [1646, 630, 11374, 257, 3035, 16141, 257, 2531, 3109, 370, 264, 3875, 1558, 815, 312, 484, 456], "temperature": 0.0, "avg_logprob": -0.15453972873917546, "compression_ratio": 1.5560344827586208, "no_speech_prob": 6.048672275937861e-06}, {"id": 487, "seek": 213432, "start": 2148.6400000000003, "end": 2149.7200000000003, "text": " already.", "tokens": [1217, 13], "temperature": 0.0, "avg_logprob": -0.15453972873917546, "compression_ratio": 1.5560344827586208, "no_speech_prob": 6.048672275937861e-06}, {"id": 488, "seek": 213432, "start": 2149.7200000000003, "end": 2155.52, "text": " Some of that idea comes from a researcher named Frank Hutter and one of his collaborators.", "tokens": [2188, 295, 300, 1558, 1487, 490, 257, 21751, 4926, 6823, 389, 9947, 293, 472, 295, 702, 39789, 13], "temperature": 0.0, "avg_logprob": -0.15453972873917546, "compression_ratio": 1.5560344827586208, "no_speech_prob": 6.048672275937861e-06}, {"id": 489, "seek": 213432, "start": 2155.52, "end": 2160.6000000000004, "text": " They did some interesting work showing how you can use random forests to actually find", "tokens": [814, 630, 512, 1880, 589, 4099, 577, 291, 393, 764, 4974, 21700, 281, 767, 915], "temperature": 0.0, "avg_logprob": -0.15453972873917546, "compression_ratio": 1.5560344827586208, "no_speech_prob": 6.048672275937861e-06}, {"id": 490, "seek": 216060, "start": 2160.6, "end": 2164.56, "text": " optimal hyperparameters so it's kind of a neat trick.", "tokens": [16252, 9848, 2181, 335, 6202, 370, 309, 311, 733, 295, 257, 10654, 4282, 13], "temperature": 0.0, "avg_logprob": -0.16670730378892687, "compression_ratio": 1.6431372549019607, "no_speech_prob": 6.438955097110011e-06}, {"id": 491, "seek": 216060, "start": 2164.56, "end": 2169.44, "text": " A lot of people are very interested in this thing called AutoML, which is this idea of", "tokens": [316, 688, 295, 561, 366, 588, 3102, 294, 341, 551, 1219, 13738, 12683, 11, 597, 307, 341, 1558, 295], "temperature": 0.0, "avg_logprob": -0.16670730378892687, "compression_ratio": 1.6431372549019607, "no_speech_prob": 6.438955097110011e-06}, {"id": 492, "seek": 216060, "start": 2169.44, "end": 2174.12, "text": " like building models to figure out how to train your model.", "tokens": [411, 2390, 5245, 281, 2573, 484, 577, 281, 3847, 428, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16670730378892687, "compression_ratio": 1.6431372549019607, "no_speech_prob": 6.438955097110011e-06}, {"id": 493, "seek": 216060, "start": 2174.12, "end": 2180.08, "text": " We're not big fans of it on the whole but we do find that building models to better", "tokens": [492, 434, 406, 955, 4499, 295, 309, 322, 264, 1379, 457, 321, 360, 915, 300, 2390, 5245, 281, 1101], "temperature": 0.0, "avg_logprob": -0.16670730378892687, "compression_ratio": 1.6431372549019607, "no_speech_prob": 6.438955097110011e-06}, {"id": 494, "seek": 216060, "start": 2180.08, "end": 2185.52, "text": " understand how your hyperparameters work and then finding those rules of thumb like, oh", "tokens": [1223, 577, 428, 9848, 2181, 335, 6202, 589, 293, 550, 5006, 729, 4474, 295, 9298, 411, 11, 1954], "temperature": 0.0, "avg_logprob": -0.16670730378892687, "compression_ratio": 1.6431372549019607, "no_speech_prob": 6.438955097110011e-06}, {"id": 495, "seek": 216060, "start": 2185.52, "end": 2190.04, "text": " basically it can always be 2.6, quite helpful.", "tokens": [1936, 309, 393, 1009, 312, 568, 13, 21, 11, 1596, 4961, 13], "temperature": 0.0, "avg_logprob": -0.16670730378892687, "compression_ratio": 1.6431372549019607, "no_speech_prob": 6.438955097110011e-06}, {"id": 496, "seek": 219004, "start": 2190.04, "end": 2201.48, "text": " So that's just something we've kind of been playing with.", "tokens": [407, 300, 311, 445, 746, 321, 600, 733, 295, 668, 2433, 365, 13], "temperature": 0.0, "avg_logprob": -0.21731907488351845, "compression_ratio": 1.7263157894736842, "no_speech_prob": 3.1201114325085655e-05}, {"id": 497, "seek": 219004, "start": 2201.48, "end": 2202.8, "text": " So let's talk about tabular data.", "tokens": [407, 718, 311, 751, 466, 4421, 1040, 1412, 13], "temperature": 0.0, "avg_logprob": -0.21731907488351845, "compression_ratio": 1.7263157894736842, "no_speech_prob": 3.1201114325085655e-05}, {"id": 498, "seek": 219004, "start": 2202.8, "end": 2208.72, "text": " So tabular data, such as you might see in a spreadsheet or a relational database or", "tokens": [407, 4421, 1040, 1412, 11, 1270, 382, 291, 1062, 536, 294, 257, 27733, 420, 257, 38444, 8149, 420], "temperature": 0.0, "avg_logprob": -0.21731907488351845, "compression_ratio": 1.7263157894736842, "no_speech_prob": 3.1201114325085655e-05}, {"id": 499, "seek": 219004, "start": 2208.72, "end": 2214.68, "text": " a financial report, it can contain all kinds of different things.", "tokens": [257, 4669, 2275, 11, 309, 393, 5304, 439, 3685, 295, 819, 721, 13], "temperature": 0.0, "avg_logprob": -0.21731907488351845, "compression_ratio": 1.7263157894736842, "no_speech_prob": 3.1201114325085655e-05}, {"id": 500, "seek": 219004, "start": 2214.68, "end": 2219.08, "text": " It can contain all kinds of different things and I kind of tried to make a little list", "tokens": [467, 393, 5304, 439, 3685, 295, 819, 721, 293, 286, 733, 295, 3031, 281, 652, 257, 707, 1329], "temperature": 0.0, "avg_logprob": -0.21731907488351845, "compression_ratio": 1.7263157894736842, "no_speech_prob": 3.1201114325085655e-05}, {"id": 501, "seek": 221908, "start": 2219.08, "end": 2226.7599999999998, "text": " of some of the kinds of things that I've seen tabular data analysis used for.", "tokens": [295, 512, 295, 264, 3685, 295, 721, 300, 286, 600, 1612, 4421, 1040, 1412, 5215, 1143, 337, 13], "temperature": 0.0, "avg_logprob": -0.15855294531518285, "compression_ratio": 1.768181818181818, "no_speech_prob": 3.120101609965786e-05}, {"id": 502, "seek": 221908, "start": 2226.7599999999998, "end": 2233.44, "text": " Using neural nets for analyzing tabular data is, or at least last year when I first presented", "tokens": [11142, 18161, 36170, 337, 23663, 4421, 1040, 1412, 307, 11, 420, 412, 1935, 1036, 1064, 562, 286, 700, 8212], "temperature": 0.0, "avg_logprob": -0.15855294531518285, "compression_ratio": 1.768181818181818, "no_speech_prob": 3.120101609965786e-05}, {"id": 503, "seek": 221908, "start": 2233.44, "end": 2237.84, "text": " this was, maybe we started this two years ago.", "tokens": [341, 390, 11, 1310, 321, 1409, 341, 732, 924, 2057, 13], "temperature": 0.0, "avg_logprob": -0.15855294531518285, "compression_ratio": 1.768181818181818, "no_speech_prob": 3.120101609965786e-05}, {"id": 504, "seek": 221908, "start": 2237.84, "end": 2241.84, "text": " When we first presented this, people were deeply skeptical and they thought it was a", "tokens": [1133, 321, 700, 8212, 341, 11, 561, 645, 8760, 28601, 293, 436, 1194, 309, 390, 257], "temperature": 0.0, "avg_logprob": -0.15855294531518285, "compression_ratio": 1.768181818181818, "no_speech_prob": 3.120101609965786e-05}, {"id": 505, "seek": 221908, "start": 2241.84, "end": 2247.12, "text": " terrible idea to use neural nets to analyze tabular data because everybody knows that", "tokens": [6237, 1558, 281, 764, 18161, 36170, 281, 12477, 4421, 1040, 1412, 570, 2201, 3255, 300], "temperature": 0.0, "avg_logprob": -0.15855294531518285, "compression_ratio": 1.768181818181818, "no_speech_prob": 3.120101609965786e-05}, {"id": 506, "seek": 224712, "start": 2247.12, "end": 2252.24, "text": " you should use logistic regression or random forests or gradient boosting machines, all", "tokens": [291, 820, 764, 3565, 3142, 24590, 420, 4974, 21700, 420, 16235, 43117, 8379, 11, 439], "temperature": 0.0, "avg_logprob": -0.14898637261721168, "compression_ratio": 1.582375478927203, "no_speech_prob": 8.530158083885908e-06}, {"id": 507, "seek": 224712, "start": 2252.24, "end": 2255.7999999999997, "text": " of which have their place for certain types of things.", "tokens": [295, 597, 362, 641, 1081, 337, 1629, 3467, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.14898637261721168, "compression_ratio": 1.582375478927203, "no_speech_prob": 8.530158083885908e-06}, {"id": 508, "seek": 224712, "start": 2255.7999999999997, "end": 2264.88, "text": " But since that time, it's become clear that the commonly held wisdom is wrong.", "tokens": [583, 1670, 300, 565, 11, 309, 311, 1813, 1850, 300, 264, 12719, 5167, 10712, 307, 2085, 13], "temperature": 0.0, "avg_logprob": -0.14898637261721168, "compression_ratio": 1.582375478927203, "no_speech_prob": 8.530158083885908e-06}, {"id": 509, "seek": 224712, "start": 2264.88, "end": 2267.6, "text": " It's not true that neural nets are not useful for tabular data.", "tokens": [467, 311, 406, 2074, 300, 18161, 36170, 366, 406, 4420, 337, 4421, 1040, 1412, 13], "temperature": 0.0, "avg_logprob": -0.14898637261721168, "compression_ratio": 1.582375478927203, "no_speech_prob": 8.530158083885908e-06}, {"id": 510, "seek": 224712, "start": 2267.6, "end": 2269.44, "text": " In fact, they're extremely useful.", "tokens": [682, 1186, 11, 436, 434, 4664, 4420, 13], "temperature": 0.0, "avg_logprob": -0.14898637261721168, "compression_ratio": 1.582375478927203, "no_speech_prob": 8.530158083885908e-06}, {"id": 511, "seek": 224712, "start": 2269.44, "end": 2276.44, "text": " We've shown this in quite a few of our courses but what's really kind of also helped is that", "tokens": [492, 600, 4898, 341, 294, 1596, 257, 1326, 295, 527, 7712, 457, 437, 311, 534, 733, 295, 611, 4254, 307, 300], "temperature": 0.0, "avg_logprob": -0.14898637261721168, "compression_ratio": 1.582375478927203, "no_speech_prob": 8.530158083885908e-06}, {"id": 512, "seek": 227644, "start": 2276.44, "end": 2284.48, "text": " some really effective organizations have started publishing papers and posts and stuff describing", "tokens": [512, 534, 4942, 6150, 362, 1409, 17832, 10577, 293, 12300, 293, 1507, 16141], "temperature": 0.0, "avg_logprob": -0.08404666781425477, "compression_ratio": 1.5991735537190082, "no_speech_prob": 1.4144636679702671e-06}, {"id": 513, "seek": 227644, "start": 2284.48, "end": 2289.2400000000002, "text": " how they've been using neural nets for analyzing tabular data.", "tokens": [577, 436, 600, 668, 1228, 18161, 36170, 337, 23663, 4421, 1040, 1412, 13], "temperature": 0.0, "avg_logprob": -0.08404666781425477, "compression_ratio": 1.5991735537190082, "no_speech_prob": 1.4144636679702671e-06}, {"id": 514, "seek": 227644, "start": 2289.2400000000002, "end": 2295.36, "text": " One of the key things that comes up again and again is that although feature engineering", "tokens": [1485, 295, 264, 2141, 721, 300, 1487, 493, 797, 293, 797, 307, 300, 4878, 4111, 7043], "temperature": 0.0, "avg_logprob": -0.08404666781425477, "compression_ratio": 1.5991735537190082, "no_speech_prob": 1.4144636679702671e-06}, {"id": 515, "seek": 227644, "start": 2295.36, "end": 2299.04, "text": " doesn't go away, it certainly becomes simpler.", "tokens": [1177, 380, 352, 1314, 11, 309, 3297, 3643, 18587, 13], "temperature": 0.0, "avg_logprob": -0.08404666781425477, "compression_ratio": 1.5991735537190082, "no_speech_prob": 1.4144636679702671e-06}, {"id": 516, "seek": 227644, "start": 2299.04, "end": 2303.04, "text": " So Pinterest, for example, replaced the gradient boosting machines that they were using to", "tokens": [407, 37986, 11, 337, 1365, 11, 10772, 264, 16235, 43117, 8379, 300, 436, 645, 1228, 281], "temperature": 0.0, "avg_logprob": -0.08404666781425477, "compression_ratio": 1.5991735537190082, "no_speech_prob": 1.4144636679702671e-06}, {"id": 517, "seek": 230304, "start": 2303.04, "end": 2308.32, "text": " decide how to put stuff on their homepage with neural nets.", "tokens": [4536, 577, 281, 829, 1507, 322, 641, 31301, 365, 18161, 36170, 13], "temperature": 0.0, "avg_logprob": -0.15793414692302327, "compression_ratio": 1.592885375494071, "no_speech_prob": 4.092877588846022e-06}, {"id": 518, "seek": 230304, "start": 2308.32, "end": 2313.04, "text": " They presented at a conference this approach and they described how it really made engineering", "tokens": [814, 8212, 412, 257, 7586, 341, 3109, 293, 436, 7619, 577, 309, 534, 1027, 7043], "temperature": 0.0, "avg_logprob": -0.15793414692302327, "compression_ratio": 1.592885375494071, "no_speech_prob": 4.092877588846022e-06}, {"id": 519, "seek": 230304, "start": 2313.04, "end": 2320.04, "text": " a lot easier because a lot of the hand-created features weren't necessary anymore.", "tokens": [257, 688, 3571, 570, 257, 688, 295, 264, 1011, 12, 66, 26559, 4122, 4999, 380, 4818, 3602, 13], "temperature": 0.0, "avg_logprob": -0.15793414692302327, "compression_ratio": 1.592885375494071, "no_speech_prob": 4.092877588846022e-06}, {"id": 520, "seek": 230304, "start": 2320.04, "end": 2322.7599999999998, "text": " You still need some, but it was just simpler.", "tokens": [509, 920, 643, 512, 11, 457, 309, 390, 445, 18587, 13], "temperature": 0.0, "avg_logprob": -0.15793414692302327, "compression_ratio": 1.592885375494071, "no_speech_prob": 4.092877588846022e-06}, {"id": 521, "seek": 230304, "start": 2322.7599999999998, "end": 2327.64, "text": " So they ended up with something that was more accurate but perhaps even more importantly,", "tokens": [407, 436, 4590, 493, 365, 746, 300, 390, 544, 8559, 457, 4317, 754, 544, 8906, 11], "temperature": 0.0, "avg_logprob": -0.15793414692302327, "compression_ratio": 1.592885375494071, "no_speech_prob": 4.092877588846022e-06}, {"id": 522, "seek": 230304, "start": 2327.64, "end": 2331.84, "text": " it required less maintenance.", "tokens": [309, 4739, 1570, 11258, 13], "temperature": 0.0, "avg_logprob": -0.15793414692302327, "compression_ratio": 1.592885375494071, "no_speech_prob": 4.092877588846022e-06}, {"id": 523, "seek": 233184, "start": 2331.84, "end": 2337.8, "text": " So I wouldn't say it's the only tool that you need in your toolbox for analyzing tabular", "tokens": [407, 286, 2759, 380, 584, 309, 311, 264, 787, 2290, 300, 291, 643, 294, 428, 44593, 337, 23663, 4421, 1040], "temperature": 0.0, "avg_logprob": -0.1157100249309929, "compression_ratio": 1.5739910313901346, "no_speech_prob": 5.255308678897563e-06}, {"id": 524, "seek": 233184, "start": 2337.8, "end": 2339.32, "text": " data, but where else?", "tokens": [1412, 11, 457, 689, 1646, 30], "temperature": 0.0, "avg_logprob": -0.1157100249309929, "compression_ratio": 1.5739910313901346, "no_speech_prob": 5.255308678897563e-06}, {"id": 525, "seek": 233184, "start": 2339.32, "end": 2345.96, "text": " I used to use random forests 99% of the time when I was doing machine learning with tabular", "tokens": [286, 1143, 281, 764, 4974, 21700, 11803, 4, 295, 264, 565, 562, 286, 390, 884, 3479, 2539, 365, 4421, 1040], "temperature": 0.0, "avg_logprob": -0.1157100249309929, "compression_ratio": 1.5739910313901346, "no_speech_prob": 5.255308678897563e-06}, {"id": 526, "seek": 233184, "start": 2345.96, "end": 2346.96, "text": " data.", "tokens": [1412, 13], "temperature": 0.0, "avg_logprob": -0.1157100249309929, "compression_ratio": 1.5739910313901346, "no_speech_prob": 5.255308678897563e-06}, {"id": 527, "seek": 233184, "start": 2346.96, "end": 2352.6400000000003, "text": " I now use neural nets 90% of the time.", "tokens": [286, 586, 764, 18161, 36170, 4289, 4, 295, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.1157100249309929, "compression_ratio": 1.5739910313901346, "no_speech_prob": 5.255308678897563e-06}, {"id": 528, "seek": 233184, "start": 2352.6400000000003, "end": 2358.56, "text": " It's kind of my standard first go-to approach now and it tends to be pretty reliable, pretty", "tokens": [467, 311, 733, 295, 452, 3832, 700, 352, 12, 1353, 3109, 586, 293, 309, 12258, 281, 312, 1238, 12924, 11, 1238], "temperature": 0.0, "avg_logprob": -0.1157100249309929, "compression_ratio": 1.5739910313901346, "no_speech_prob": 5.255308678897563e-06}, {"id": 529, "seek": 233184, "start": 2358.56, "end": 2360.88, "text": " effective.", "tokens": [4942, 13], "temperature": 0.0, "avg_logprob": -0.1157100249309929, "compression_ratio": 1.5739910313901346, "no_speech_prob": 5.255308678897563e-06}, {"id": 530, "seek": 236088, "start": 2360.88, "end": 2366.0, "text": " One of the things that's made it difficult is that until now there hasn't been an easy", "tokens": [1485, 295, 264, 721, 300, 311, 1027, 309, 2252, 307, 300, 1826, 586, 456, 6132, 380, 668, 364, 1858], "temperature": 0.0, "avg_logprob": -0.10143831041124132, "compression_ratio": 1.6325581395348838, "no_speech_prob": 9.972801308322232e-06}, {"id": 531, "seek": 236088, "start": 2366.0, "end": 2371.7200000000003, "text": " way to kind of create and train tabular neural nets like nobody's really made it available", "tokens": [636, 281, 733, 295, 1884, 293, 3847, 4421, 1040, 18161, 36170, 411, 5079, 311, 534, 1027, 309, 2435], "temperature": 0.0, "avg_logprob": -0.10143831041124132, "compression_ratio": 1.6325581395348838, "no_speech_prob": 9.972801308322232e-06}, {"id": 532, "seek": 236088, "start": 2371.7200000000003, "end": 2373.36, "text": " on a library.", "tokens": [322, 257, 6405, 13], "temperature": 0.0, "avg_logprob": -0.10143831041124132, "compression_ratio": 1.6325581395348838, "no_speech_prob": 9.972801308322232e-06}, {"id": 533, "seek": 236088, "start": 2373.36, "end": 2380.84, "text": " So we've actually just created fastai.tabular and I think this is pretty much the first", "tokens": [407, 321, 600, 767, 445, 2942, 2370, 1301, 13, 83, 455, 1040, 293, 286, 519, 341, 307, 1238, 709, 264, 700], "temperature": 0.0, "avg_logprob": -0.10143831041124132, "compression_ratio": 1.6325581395348838, "no_speech_prob": 9.972801308322232e-06}, {"id": 534, "seek": 236088, "start": 2380.84, "end": 2387.92, "text": " time that it's become really easy to use neural nets with tabular data.", "tokens": [565, 300, 309, 311, 1813, 534, 1858, 281, 764, 18161, 36170, 365, 4421, 1040, 1412, 13], "temperature": 0.0, "avg_logprob": -0.10143831041124132, "compression_ratio": 1.6325581395348838, "no_speech_prob": 9.972801308322232e-06}, {"id": 535, "seek": 238792, "start": 2387.92, "end": 2391.92, "text": " So let me show you how easy it is.", "tokens": [407, 718, 385, 855, 291, 577, 1858, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.14292144775390625, "compression_ratio": 1.5174129353233832, "no_speech_prob": 4.637837264453992e-06}, {"id": 536, "seek": 238792, "start": 2391.92, "end": 2396.84, "text": " This is actually coming directly from the examples folder in the fastai repo.", "tokens": [639, 307, 767, 1348, 3838, 490, 264, 5110, 10820, 294, 264, 2370, 1301, 49040, 13], "temperature": 0.0, "avg_logprob": -0.14292144775390625, "compression_ratio": 1.5174129353233832, "no_speech_prob": 4.637837264453992e-06}, {"id": 537, "seek": 238792, "start": 2396.84, "end": 2399.16, "text": " I haven't changed it at all.", "tokens": [286, 2378, 380, 3105, 309, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.14292144775390625, "compression_ratio": 1.5174129353233832, "no_speech_prob": 4.637837264453992e-06}, {"id": 538, "seek": 238792, "start": 2399.16, "end": 2404.16, "text": " And as per usual as well as importing fastai, you should import your application.", "tokens": [400, 382, 680, 7713, 382, 731, 382, 43866, 2370, 1301, 11, 291, 820, 974, 428, 3861, 13], "temperature": 0.0, "avg_logprob": -0.14292144775390625, "compression_ratio": 1.5174129353233832, "no_speech_prob": 4.637837264453992e-06}, {"id": 539, "seek": 238792, "start": 2404.16, "end": 2408.92, "text": " So in this case it's tabular.", "tokens": [407, 294, 341, 1389, 309, 311, 4421, 1040, 13], "temperature": 0.0, "avg_logprob": -0.14292144775390625, "compression_ratio": 1.5174129353233832, "no_speech_prob": 4.637837264453992e-06}, {"id": 540, "seek": 238792, "start": 2408.92, "end": 2413.2000000000003, "text": " We assume that your data is in a pandas data frame.", "tokens": [492, 6552, 300, 428, 1412, 307, 294, 257, 4565, 296, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.14292144775390625, "compression_ratio": 1.5174129353233832, "no_speech_prob": 4.637837264453992e-06}, {"id": 541, "seek": 241320, "start": 2413.2, "end": 2419.68, "text": " A pandas data frame is kind of the standard format for tabular data in Python and it's", "tokens": [316, 4565, 296, 1412, 3920, 307, 733, 295, 264, 3832, 7877, 337, 4421, 1040, 1412, 294, 15329, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.17885851524245572, "compression_ratio": 1.5688622754491017, "no_speech_prob": 8.397919373237528e-06}, {"id": 542, "seek": 241320, "start": 2419.68, "end": 2425.16, "text": " lots of ways to get it in there but probably the most common might be pd.readcsv.", "tokens": [3195, 295, 2098, 281, 483, 309, 294, 456, 457, 1391, 264, 881, 2689, 1062, 312, 280, 67, 13, 2538, 14368, 85, 13], "temperature": 0.0, "avg_logprob": -0.17885851524245572, "compression_ratio": 1.5688622754491017, "no_speech_prob": 8.397919373237528e-06}, {"id": 543, "seek": 241320, "start": 2425.16, "end": 2430.52, "text": " But whatever your data is in, you can probably get it into a pandas data frame easily enough.", "tokens": [583, 2035, 428, 1412, 307, 294, 11, 291, 393, 1391, 483, 309, 666, 257, 4565, 296, 1412, 3920, 3612, 1547, 13], "temperature": 0.0, "avg_logprob": -0.17885851524245572, "compression_ratio": 1.5688622754491017, "no_speech_prob": 8.397919373237528e-06}, {"id": 544, "seek": 243052, "start": 2430.52, "end": 2449.32, "text": " Question, what are the 10% of cases where you would not default to neural nets?", "tokens": [14464, 11, 437, 366, 264, 1266, 4, 295, 3331, 689, 291, 576, 406, 7576, 281, 18161, 36170, 30], "temperature": 0.0, "avg_logprob": -0.260057184431288, "compression_ratio": 1.0681818181818181, "no_speech_prob": 1.1300519872747827e-05}, {"id": 545, "seek": 243052, "start": 2449.32, "end": 2453.28, "text": " Good question.", "tokens": [2205, 1168, 13], "temperature": 0.0, "avg_logprob": -0.260057184431288, "compression_ratio": 1.0681818181818181, "no_speech_prob": 1.1300519872747827e-05}, {"id": 546, "seek": 245328, "start": 2453.28, "end": 2464.44, "text": " I guess I still tend to kind of give them a try.", "tokens": [286, 2041, 286, 920, 3928, 281, 733, 295, 976, 552, 257, 853, 13], "temperature": 0.0, "avg_logprob": -0.1449276924133301, "compression_ratio": 1.6567164179104477, "no_speech_prob": 6.605011731153354e-05}, {"id": 547, "seek": 245328, "start": 2464.44, "end": 2469.96, "text": " As you do things for a while, you start to get a sense of the areas where things don't", "tokens": [1018, 291, 360, 721, 337, 257, 1339, 11, 291, 722, 281, 483, 257, 2020, 295, 264, 3179, 689, 721, 500, 380], "temperature": 0.0, "avg_logprob": -0.1449276924133301, "compression_ratio": 1.6567164179104477, "no_speech_prob": 6.605011731153354e-05}, {"id": 548, "seek": 245328, "start": 2469.96, "end": 2470.96, "text": " quite work as well.", "tokens": [1596, 589, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1449276924133301, "compression_ratio": 1.6567164179104477, "no_speech_prob": 6.605011731153354e-05}, {"id": 549, "seek": 245328, "start": 2470.96, "end": 2472.1200000000003, "text": " I have to think about that during the week.", "tokens": [286, 362, 281, 519, 466, 300, 1830, 264, 1243, 13], "temperature": 0.0, "avg_logprob": -0.1449276924133301, "compression_ratio": 1.6567164179104477, "no_speech_prob": 6.605011731153354e-05}, {"id": 550, "seek": 245328, "start": 2472.1200000000003, "end": 2473.36, "text": " I don't think I have a rule of thumb.", "tokens": [286, 500, 380, 519, 286, 362, 257, 4978, 295, 9298, 13], "temperature": 0.0, "avg_logprob": -0.1449276924133301, "compression_ratio": 1.6567164179104477, "no_speech_prob": 6.605011731153354e-05}, {"id": 551, "seek": 245328, "start": 2473.36, "end": 2476.92, "text": " But I would say you may as well try both.", "tokens": [583, 286, 576, 584, 291, 815, 382, 731, 853, 1293, 13], "temperature": 0.0, "avg_logprob": -0.1449276924133301, "compression_ratio": 1.6567164179104477, "no_speech_prob": 6.605011731153354e-05}, {"id": 552, "seek": 245328, "start": 2476.92, "end": 2480.36, "text": " I would say try a random forest and try a neural net.", "tokens": [286, 576, 584, 853, 257, 4974, 6719, 293, 853, 257, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.1449276924133301, "compression_ratio": 1.6567164179104477, "no_speech_prob": 6.605011731153354e-05}, {"id": 553, "seek": 248036, "start": 2480.36, "end": 2484.56, "text": " They're both pretty quick and easy to run and see how it looks.", "tokens": [814, 434, 1293, 1238, 1702, 293, 1858, 281, 1190, 293, 536, 577, 309, 1542, 13], "temperature": 0.0, "avg_logprob": -0.15649511356546422, "compression_ratio": 1.5836909871244635, "no_speech_prob": 2.2827392967883497e-05}, {"id": 554, "seek": 248036, "start": 2484.56, "end": 2488.52, "text": " And if they're roughly similar, I might try and dig into each and see if I can make them", "tokens": [400, 498, 436, 434, 9810, 2531, 11, 286, 1062, 853, 293, 2528, 666, 1184, 293, 536, 498, 286, 393, 652, 552], "temperature": 0.0, "avg_logprob": -0.15649511356546422, "compression_ratio": 1.5836909871244635, "no_speech_prob": 2.2827392967883497e-05}, {"id": 555, "seek": 248036, "start": 2488.52, "end": 2489.52, "text": " better and better.", "tokens": [1101, 293, 1101, 13], "temperature": 0.0, "avg_logprob": -0.15649511356546422, "compression_ratio": 1.5836909871244635, "no_speech_prob": 2.2827392967883497e-05}, {"id": 556, "seek": 248036, "start": 2489.52, "end": 2495.28, "text": " But if the random forest is doing way better, I'd probably just stick with that and use", "tokens": [583, 498, 264, 4974, 6719, 307, 884, 636, 1101, 11, 286, 1116, 1391, 445, 2897, 365, 300, 293, 764], "temperature": 0.0, "avg_logprob": -0.15649511356546422, "compression_ratio": 1.5836909871244635, "no_speech_prob": 2.2827392967883497e-05}, {"id": 557, "seek": 248036, "start": 2495.28, "end": 2501.28, "text": " whatever works.", "tokens": [2035, 1985, 13], "temperature": 0.0, "avg_logprob": -0.15649511356546422, "compression_ratio": 1.5836909871244635, "no_speech_prob": 2.2827392967883497e-05}, {"id": 558, "seek": 248036, "start": 2501.28, "end": 2506.56, "text": " So I currently have the wrong notebook in the lesson repo.", "tokens": [407, 286, 4362, 362, 264, 2085, 21060, 294, 264, 6898, 49040, 13], "temperature": 0.0, "avg_logprob": -0.15649511356546422, "compression_ratio": 1.5836909871244635, "no_speech_prob": 2.2827392967883497e-05}, {"id": 559, "seek": 248036, "start": 2506.56, "end": 2508.76, "text": " So I'll update it after the class.", "tokens": [407, 286, 603, 5623, 309, 934, 264, 1508, 13], "temperature": 0.0, "avg_logprob": -0.15649511356546422, "compression_ratio": 1.5836909871244635, "no_speech_prob": 2.2827392967883497e-05}, {"id": 560, "seek": 250876, "start": 2508.76, "end": 2512.7200000000003, "text": " So sorry about that.", "tokens": [407, 2597, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.18255873159928757, "compression_ratio": 1.5522388059701493, "no_speech_prob": 1.9832774341921322e-05}, {"id": 561, "seek": 250876, "start": 2512.7200000000003, "end": 2517.1200000000003, "text": " So we start with the data in a data frame.", "tokens": [407, 321, 722, 365, 264, 1412, 294, 257, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.18255873159928757, "compression_ratio": 1.5522388059701493, "no_speech_prob": 1.9832774341921322e-05}, {"id": 562, "seek": 250876, "start": 2517.1200000000003, "end": 2523.32, "text": " And so we've got a little thing, the adult sample.", "tokens": [400, 370, 321, 600, 658, 257, 707, 551, 11, 264, 5075, 6889, 13], "temperature": 0.0, "avg_logprob": -0.18255873159928757, "compression_ratio": 1.5522388059701493, "no_speech_prob": 1.9832774341921322e-05}, {"id": 563, "seek": 250876, "start": 2523.32, "end": 2525.32, "text": " It's a classic old data set.", "tokens": [467, 311, 257, 7230, 1331, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.18255873159928757, "compression_ratio": 1.5522388059701493, "no_speech_prob": 1.9832774341921322e-05}, {"id": 564, "seek": 250876, "start": 2525.32, "end": 2530.28, "text": " I have to dig up the citation for it because I forgot to put it in this notebook.", "tokens": [286, 362, 281, 2528, 493, 264, 45590, 337, 309, 570, 286, 5298, 281, 829, 309, 294, 341, 21060, 13], "temperature": 0.0, "avg_logprob": -0.18255873159928757, "compression_ratio": 1.5522388059701493, "no_speech_prob": 1.9832774341921322e-05}, {"id": 565, "seek": 250876, "start": 2530.28, "end": 2536.44, "text": " It's a pretty small, simple old data set that's good for experimenting with basically.", "tokens": [467, 311, 257, 1238, 1359, 11, 2199, 1331, 1412, 992, 300, 311, 665, 337, 29070, 365, 1936, 13], "temperature": 0.0, "avg_logprob": -0.18255873159928757, "compression_ratio": 1.5522388059701493, "no_speech_prob": 1.9832774341921322e-05}, {"id": 566, "seek": 253644, "start": 2536.44, "end": 2542.98, "text": " And it's a CSV file, so you can read it into a data frame with pandas.read.csv.", "tokens": [400, 309, 311, 257, 48814, 3991, 11, 370, 291, 393, 1401, 309, 666, 257, 1412, 3920, 365, 4565, 296, 13, 2538, 13, 14368, 85, 13], "temperature": 0.0, "avg_logprob": -0.12440573567091817, "compression_ratio": 1.694736842105263, "no_speech_prob": 1.4970732081565075e-05}, {"id": 567, "seek": 253644, "start": 2542.98, "end": 2548.2000000000003, "text": " If your data is in a relational database, pandas can read from that.", "tokens": [759, 428, 1412, 307, 294, 257, 38444, 8149, 11, 4565, 296, 393, 1401, 490, 300, 13], "temperature": 0.0, "avg_logprob": -0.12440573567091817, "compression_ratio": 1.694736842105263, "no_speech_prob": 1.4970732081565075e-05}, {"id": 568, "seek": 253644, "start": 2548.2000000000003, "end": 2551.36, "text": " If it's in Spark or Hadoop, pandas can read from that.", "tokens": [759, 309, 311, 294, 23424, 420, 389, 1573, 404, 11, 4565, 296, 393, 1401, 490, 300, 13], "temperature": 0.0, "avg_logprob": -0.12440573567091817, "compression_ratio": 1.694736842105263, "no_speech_prob": 1.4970732081565075e-05}, {"id": 569, "seek": 253644, "start": 2551.36, "end": 2554.16, "text": " Pandas can read from most stuff that you can throw at it.", "tokens": [16995, 296, 393, 1401, 490, 881, 1507, 300, 291, 393, 3507, 412, 309, 13], "temperature": 0.0, "avg_logprob": -0.12440573567091817, "compression_ratio": 1.694736842105263, "no_speech_prob": 1.4970732081565075e-05}, {"id": 570, "seek": 253644, "start": 2554.16, "end": 2560.28, "text": " So that's why we kind of use it as a default starting point.", "tokens": [407, 300, 311, 983, 321, 733, 295, 764, 309, 382, 257, 7576, 2891, 935, 13], "temperature": 0.0, "avg_logprob": -0.12440573567091817, "compression_ratio": 1.694736842105263, "no_speech_prob": 1.4970732081565075e-05}, {"id": 571, "seek": 256028, "start": 2560.28, "end": 2566.92, "text": " And as per usual, I think it's nice to use the data block API.", "tokens": [400, 382, 680, 7713, 11, 286, 519, 309, 311, 1481, 281, 764, 264, 1412, 3461, 9362, 13], "temperature": 0.0, "avg_logprob": -0.10002036787505843, "compression_ratio": 1.8461538461538463, "no_speech_prob": 3.966958502132911e-06}, {"id": 572, "seek": 256028, "start": 2566.92, "end": 2572.36, "text": " And so in this case, the list that we're trying to create is a tabular list.", "tokens": [400, 370, 294, 341, 1389, 11, 264, 1329, 300, 321, 434, 1382, 281, 1884, 307, 257, 4421, 1040, 1329, 13], "temperature": 0.0, "avg_logprob": -0.10002036787505843, "compression_ratio": 1.8461538461538463, "no_speech_prob": 3.966958502132911e-06}, {"id": 573, "seek": 256028, "start": 2572.36, "end": 2574.96, "text": " And we're going to create it from a data frame.", "tokens": [400, 321, 434, 516, 281, 1884, 309, 490, 257, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.10002036787505843, "compression_ratio": 1.8461538461538463, "no_speech_prob": 3.966958502132911e-06}, {"id": 574, "seek": 256028, "start": 2574.96, "end": 2578.0, "text": " And so you can tell it what the data frame is and what the path that you're going to", "tokens": [400, 370, 291, 393, 980, 309, 437, 264, 1412, 3920, 307, 293, 437, 264, 3100, 300, 291, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.10002036787505843, "compression_ratio": 1.8461538461538463, "no_speech_prob": 3.966958502132911e-06}, {"id": 575, "seek": 256028, "start": 2578.0, "end": 2581.5600000000004, "text": " use to kind of save models and intermediate steps is.", "tokens": [764, 281, 733, 295, 3155, 5245, 293, 19376, 4439, 307, 13], "temperature": 0.0, "avg_logprob": -0.10002036787505843, "compression_ratio": 1.8461538461538463, "no_speech_prob": 3.966958502132911e-06}, {"id": 576, "seek": 256028, "start": 2581.5600000000004, "end": 2586.92, "text": " And then you need to tell it, what are your categorical variables and what are your continuous", "tokens": [400, 550, 291, 643, 281, 980, 309, 11, 437, 366, 428, 19250, 804, 9102, 293, 437, 366, 428, 10957], "temperature": 0.0, "avg_logprob": -0.10002036787505843, "compression_ratio": 1.8461538461538463, "no_speech_prob": 3.966958502132911e-06}, {"id": 577, "seek": 256028, "start": 2586.92, "end": 2587.92, "text": " variables?", "tokens": [9102, 30], "temperature": 0.0, "avg_logprob": -0.10002036787505843, "compression_ratio": 1.8461538461538463, "no_speech_prob": 3.966958502132911e-06}, {"id": 578, "seek": 258792, "start": 2587.92, "end": 2594.44, "text": " So we're going to be learning a lot more about what that means to the neural net next week.", "tokens": [407, 321, 434, 516, 281, 312, 2539, 257, 688, 544, 466, 437, 300, 1355, 281, 264, 18161, 2533, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.12489573160807292, "compression_ratio": 1.6008771929824561, "no_speech_prob": 9.665986908657942e-06}, {"id": 579, "seek": 258792, "start": 2594.44, "end": 2597.9, "text": " But for now, the quick summary is this.", "tokens": [583, 337, 586, 11, 264, 1702, 12691, 307, 341, 13], "temperature": 0.0, "avg_logprob": -0.12489573160807292, "compression_ratio": 1.6008771929824561, "no_speech_prob": 9.665986908657942e-06}, {"id": 580, "seek": 258792, "start": 2597.9, "end": 2602.6800000000003, "text": " Your independent variables are the things that you're using to make predictions with.", "tokens": [2260, 6695, 9102, 366, 264, 721, 300, 291, 434, 1228, 281, 652, 21264, 365, 13], "temperature": 0.0, "avg_logprob": -0.12489573160807292, "compression_ratio": 1.6008771929824561, "no_speech_prob": 9.665986908657942e-06}, {"id": 581, "seek": 258792, "start": 2602.6800000000003, "end": 2610.04, "text": " So things like education and marital status and age and so forth.", "tokens": [407, 721, 411, 3309, 293, 1849, 1686, 6558, 293, 3205, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.12489573160807292, "compression_ratio": 1.6008771929824561, "no_speech_prob": 9.665986908657942e-06}, {"id": 582, "seek": 258792, "start": 2610.04, "end": 2615.8, "text": " Some of those variables like age are basically numbers.", "tokens": [2188, 295, 729, 9102, 411, 3205, 366, 1936, 3547, 13], "temperature": 0.0, "avg_logprob": -0.12489573160807292, "compression_ratio": 1.6008771929824561, "no_speech_prob": 9.665986908657942e-06}, {"id": 583, "seek": 258792, "start": 2615.8, "end": 2617.32, "text": " They could be any number.", "tokens": [814, 727, 312, 604, 1230, 13], "temperature": 0.0, "avg_logprob": -0.12489573160807292, "compression_ratio": 1.6008771929824561, "no_speech_prob": 9.665986908657942e-06}, {"id": 584, "seek": 261732, "start": 2617.32, "end": 2623.2000000000003, "text": " You could be 13.36 years old or 19.4 years old or whatever.", "tokens": [509, 727, 312, 3705, 13, 11309, 924, 1331, 420, 1294, 13, 19, 924, 1331, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.15718767490792782, "compression_ratio": 1.5947136563876652, "no_speech_prob": 2.9479904242180055e-06}, {"id": 585, "seek": 261732, "start": 2623.2000000000003, "end": 2629.6800000000003, "text": " Whereas things like marital status are options that can be selected from a discrete group", "tokens": [13813, 721, 411, 1849, 1686, 6558, 366, 3956, 300, 393, 312, 8209, 490, 257, 27706, 1594], "temperature": 0.0, "avg_logprob": -0.15718767490792782, "compression_ratio": 1.5947136563876652, "no_speech_prob": 2.9479904242180055e-06}, {"id": 586, "seek": 261732, "start": 2629.6800000000003, "end": 2633.88, "text": " \ufffd married, single, divorced, whatever.", "tokens": [16867, 5259, 11, 2167, 11, 27670, 11, 2035, 13], "temperature": 0.0, "avg_logprob": -0.15718767490792782, "compression_ratio": 1.5947136563876652, "no_speech_prob": 2.9479904242180055e-06}, {"id": 587, "seek": 261732, "start": 2633.88, "end": 2637.04, "text": " Sometimes those options might be quite a lot more, like occupation.", "tokens": [4803, 729, 3956, 1062, 312, 1596, 257, 688, 544, 11, 411, 24482, 13], "temperature": 0.0, "avg_logprob": -0.15718767490792782, "compression_ratio": 1.5947136563876652, "no_speech_prob": 2.9479904242180055e-06}, {"id": 588, "seek": 261732, "start": 2637.04, "end": 2641.54, "text": " There's a lot of possible occupations.", "tokens": [821, 311, 257, 688, 295, 1944, 8073, 763, 13], "temperature": 0.0, "avg_logprob": -0.15718767490792782, "compression_ratio": 1.5947136563876652, "no_speech_prob": 2.9479904242180055e-06}, {"id": 589, "seek": 261732, "start": 2641.54, "end": 2643.52, "text": " And sometimes they might be binary.", "tokens": [400, 2171, 436, 1062, 312, 17434, 13], "temperature": 0.0, "avg_logprob": -0.15718767490792782, "compression_ratio": 1.5947136563876652, "no_speech_prob": 2.9479904242180055e-06}, {"id": 590, "seek": 261732, "start": 2643.52, "end": 2645.0800000000004, "text": " Could be just true or false.", "tokens": [7497, 312, 445, 2074, 420, 7908, 13], "temperature": 0.0, "avg_logprob": -0.15718767490792782, "compression_ratio": 1.5947136563876652, "no_speech_prob": 2.9479904242180055e-06}, {"id": 591, "seek": 264508, "start": 2645.08, "end": 2652.48, "text": " But anything which you can select the answer from a small group of possibilities is called", "tokens": [583, 1340, 597, 291, 393, 3048, 264, 1867, 490, 257, 1359, 1594, 295, 12178, 307, 1219], "temperature": 0.0, "avg_logprob": -0.1319016786379235, "compression_ratio": 1.8875502008032128, "no_speech_prob": 2.0580453110596864e-06}, {"id": 592, "seek": 264508, "start": 2652.48, "end": 2656.16, "text": " a categorical variable.", "tokens": [257, 19250, 804, 7006, 13], "temperature": 0.0, "avg_logprob": -0.1319016786379235, "compression_ratio": 1.8875502008032128, "no_speech_prob": 2.0580453110596864e-06}, {"id": 593, "seek": 264508, "start": 2656.16, "end": 2660.24, "text": " And so we're going to need to use a different approach to the neural net to modeling categorical", "tokens": [400, 370, 321, 434, 516, 281, 643, 281, 764, 257, 819, 3109, 281, 264, 18161, 2533, 281, 15983, 19250, 804], "temperature": 0.0, "avg_logprob": -0.1319016786379235, "compression_ratio": 1.8875502008032128, "no_speech_prob": 2.0580453110596864e-06}, {"id": 594, "seek": 264508, "start": 2660.24, "end": 2663.16, "text": " variables to what we use for continuous variables.", "tokens": [9102, 281, 437, 321, 764, 337, 10957, 9102, 13], "temperature": 0.0, "avg_logprob": -0.1319016786379235, "compression_ratio": 1.8875502008032128, "no_speech_prob": 2.0580453110596864e-06}, {"id": 595, "seek": 264508, "start": 2663.16, "end": 2666.24, "text": " For categorical variables, we're going to be using something called embeddings, which", "tokens": [1171, 19250, 804, 9102, 11, 321, 434, 516, 281, 312, 1228, 746, 1219, 12240, 29432, 11, 597], "temperature": 0.0, "avg_logprob": -0.1319016786379235, "compression_ratio": 1.8875502008032128, "no_speech_prob": 2.0580453110596864e-06}, {"id": 596, "seek": 264508, "start": 2666.24, "end": 2669.12, "text": " we'll be learning about later today.", "tokens": [321, 603, 312, 2539, 466, 1780, 965, 13], "temperature": 0.0, "avg_logprob": -0.1319016786379235, "compression_ratio": 1.8875502008032128, "no_speech_prob": 2.0580453110596864e-06}, {"id": 597, "seek": 264508, "start": 2669.12, "end": 2673.56, "text": " For continuous variables, they can just be sent into the neural net just like pixels", "tokens": [1171, 10957, 9102, 11, 436, 393, 445, 312, 2279, 666, 264, 18161, 2533, 445, 411, 18668], "temperature": 0.0, "avg_logprob": -0.1319016786379235, "compression_ratio": 1.8875502008032128, "no_speech_prob": 2.0580453110596864e-06}, {"id": 598, "seek": 267356, "start": 2673.56, "end": 2677.68, "text": " in a neural net can, because pixels in a neural net are already numbers.", "tokens": [294, 257, 18161, 2533, 393, 11, 570, 18668, 294, 257, 18161, 2533, 366, 1217, 3547, 13], "temperature": 0.0, "avg_logprob": -0.1299011099572275, "compression_ratio": 1.676595744680851, "no_speech_prob": 4.09287622460397e-06}, {"id": 599, "seek": 267356, "start": 2677.68, "end": 2680.56, "text": " These continuous things are already numbers as well.", "tokens": [1981, 10957, 721, 366, 1217, 3547, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1299011099572275, "compression_ratio": 1.676595744680851, "no_speech_prob": 4.09287622460397e-06}, {"id": 600, "seek": 267356, "start": 2680.56, "end": 2683.02, "text": " So that's easy.", "tokens": [407, 300, 311, 1858, 13], "temperature": 0.0, "avg_logprob": -0.1299011099572275, "compression_ratio": 1.676595744680851, "no_speech_prob": 4.09287622460397e-06}, {"id": 601, "seek": 267356, "start": 2683.02, "end": 2692.64, "text": " So that's why you have to tell the tabular list from data frame which ones are which.", "tokens": [407, 300, 311, 983, 291, 362, 281, 980, 264, 4421, 1040, 1329, 490, 1412, 3920, 597, 2306, 366, 597, 13], "temperature": 0.0, "avg_logprob": -0.1299011099572275, "compression_ratio": 1.676595744680851, "no_speech_prob": 4.09287622460397e-06}, {"id": 602, "seek": 267356, "start": 2692.64, "end": 2698.56, "text": " There are some other ways to do that by preprocessing them in pandas to make things categorical", "tokens": [821, 366, 512, 661, 2098, 281, 360, 300, 538, 2666, 340, 780, 278, 552, 294, 4565, 296, 281, 652, 721, 19250, 804], "temperature": 0.0, "avg_logprob": -0.1299011099572275, "compression_ratio": 1.676595744680851, "no_speech_prob": 4.09287622460397e-06}, {"id": 603, "seek": 267356, "start": 2698.56, "end": 2701.56, "text": " variables, but it's kind of nice to have one API for doing everything.", "tokens": [9102, 11, 457, 309, 311, 733, 295, 1481, 281, 362, 472, 9362, 337, 884, 1203, 13], "temperature": 0.0, "avg_logprob": -0.1299011099572275, "compression_ratio": 1.676595744680851, "no_speech_prob": 4.09287622460397e-06}, {"id": 604, "seek": 270156, "start": 2701.56, "end": 2705.44, "text": " You don't have to think too much about it.", "tokens": [509, 500, 380, 362, 281, 519, 886, 709, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.09617955426135695, "compression_ratio": 1.6009852216748768, "no_speech_prob": 8.939569852373097e-06}, {"id": 605, "seek": 270156, "start": 2705.44, "end": 2714.2799999999997, "text": " Then we've got something which is a lot like transforms in computer vision.", "tokens": [1396, 321, 600, 658, 746, 597, 307, 257, 688, 411, 35592, 294, 3820, 5201, 13], "temperature": 0.0, "avg_logprob": -0.09617955426135695, "compression_ratio": 1.6009852216748768, "no_speech_prob": 8.939569852373097e-06}, {"id": 606, "seek": 270156, "start": 2714.2799999999997, "end": 2718.92, "text": " Transforms in computer vision do things like flip a photo when it's axis or turn it a bit", "tokens": [27938, 82, 294, 3820, 5201, 360, 721, 411, 7929, 257, 5052, 562, 309, 311, 10298, 420, 1261, 309, 257, 857], "temperature": 0.0, "avg_logprob": -0.09617955426135695, "compression_ratio": 1.6009852216748768, "no_speech_prob": 8.939569852373097e-06}, {"id": 607, "seek": 270156, "start": 2718.92, "end": 2722.56, "text": " or brighten it or normalize it.", "tokens": [420, 49007, 309, 420, 2710, 1125, 309, 13], "temperature": 0.0, "avg_logprob": -0.09617955426135695, "compression_ratio": 1.6009852216748768, "no_speech_prob": 8.939569852373097e-06}, {"id": 608, "seek": 270156, "start": 2722.56, "end": 2729.12, "text": " But for tabular data, instead of having transforms, we have things called processes.", "tokens": [583, 337, 4421, 1040, 1412, 11, 2602, 295, 1419, 35592, 11, 321, 362, 721, 1219, 7555, 13], "temperature": 0.0, "avg_logprob": -0.09617955426135695, "compression_ratio": 1.6009852216748768, "no_speech_prob": 8.939569852373097e-06}, {"id": 609, "seek": 272912, "start": 2729.12, "end": 2732.8399999999997, "text": " And they're nearly identical, but the key difference, which is quite important, is that", "tokens": [400, 436, 434, 6217, 14800, 11, 457, 264, 2141, 2649, 11, 597, 307, 1596, 1021, 11, 307, 300], "temperature": 0.0, "avg_logprob": -0.10696413183725008, "compression_ratio": 1.7304347826086957, "no_speech_prob": 9.08037145563867e-06}, {"id": 610, "seek": 272912, "start": 2732.8399999999997, "end": 2737.24, "text": " a processor is something that happens ahead of time.", "tokens": [257, 15321, 307, 746, 300, 2314, 2286, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.10696413183725008, "compression_ratio": 1.7304347826086957, "no_speech_prob": 9.08037145563867e-06}, {"id": 611, "seek": 272912, "start": 2737.24, "end": 2744.12, "text": " So we basically preprocess the data frame rather than doing it as we go.", "tokens": [407, 321, 1936, 2666, 340, 780, 264, 1412, 3920, 2831, 813, 884, 309, 382, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.10696413183725008, "compression_ratio": 1.7304347826086957, "no_speech_prob": 9.08037145563867e-06}, {"id": 612, "seek": 272912, "start": 2744.12, "end": 2749.74, "text": " So transformations are really for data augmentation where you want to randomize it and do it differently", "tokens": [407, 34852, 366, 534, 337, 1412, 14501, 19631, 689, 291, 528, 281, 4974, 1125, 309, 293, 360, 309, 7614], "temperature": 0.0, "avg_logprob": -0.10696413183725008, "compression_ratio": 1.7304347826086957, "no_speech_prob": 9.08037145563867e-06}, {"id": 613, "seek": 272912, "start": 2749.74, "end": 2755.2799999999997, "text": " each time, whereas processes are things that you want to do once ahead of time.", "tokens": [1184, 565, 11, 9735, 7555, 366, 721, 300, 291, 528, 281, 360, 1564, 2286, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.10696413183725008, "compression_ratio": 1.7304347826086957, "no_speech_prob": 9.08037145563867e-06}, {"id": 614, "seek": 275528, "start": 2755.28, "end": 2760.36, "text": " So we have a number of processes in the Fast.ai library.", "tokens": [407, 321, 362, 257, 1230, 295, 7555, 294, 264, 15968, 13, 1301, 6405, 13], "temperature": 0.0, "avg_logprob": -0.14094968860069018, "compression_ratio": 1.6495327102803738, "no_speech_prob": 1.2218943993502762e-05}, {"id": 615, "seek": 275528, "start": 2760.36, "end": 2763.36, "text": " The ones we're going to use this time are fill missing.", "tokens": [440, 2306, 321, 434, 516, 281, 764, 341, 565, 366, 2836, 5361, 13], "temperature": 0.0, "avg_logprob": -0.14094968860069018, "compression_ratio": 1.6495327102803738, "no_speech_prob": 1.2218943993502762e-05}, {"id": 616, "seek": 275528, "start": 2763.36, "end": 2771.1600000000003, "text": " So that's going to look for missing values and deal with them some way.", "tokens": [407, 300, 311, 516, 281, 574, 337, 5361, 4190, 293, 2028, 365, 552, 512, 636, 13], "temperature": 0.0, "avg_logprob": -0.14094968860069018, "compression_ratio": 1.6495327102803738, "no_speech_prob": 1.2218943993502762e-05}, {"id": 617, "seek": 275528, "start": 2771.1600000000003, "end": 2776.8, "text": " We're going to find categorical variables and turn them into pandas categories.", "tokens": [492, 434, 516, 281, 915, 19250, 804, 9102, 293, 1261, 552, 666, 4565, 296, 10479, 13], "temperature": 0.0, "avg_logprob": -0.14094968860069018, "compression_ratio": 1.6495327102803738, "no_speech_prob": 1.2218943993502762e-05}, {"id": 618, "seek": 275528, "start": 2776.8, "end": 2780.6800000000003, "text": " And we're going to do normalization ahead of time, which is to take continuous variables", "tokens": [400, 321, 434, 516, 281, 360, 2710, 2144, 2286, 295, 565, 11, 597, 307, 281, 747, 10957, 9102], "temperature": 0.0, "avg_logprob": -0.14094968860069018, "compression_ratio": 1.6495327102803738, "no_speech_prob": 1.2218943993502762e-05}, {"id": 619, "seek": 278068, "start": 2780.68, "end": 2791.3599999999997, "text": " and subtract their mean and divide it by their standard deviation, so they're 0, 1 variables.", "tokens": [293, 16390, 641, 914, 293, 9845, 309, 538, 641, 3832, 25163, 11, 370, 436, 434, 1958, 11, 502, 9102, 13], "temperature": 0.0, "avg_logprob": -0.16275067078439812, "compression_ratio": 1.5396825396825398, "no_speech_prob": 5.014682301407447e-06}, {"id": 620, "seek": 278068, "start": 2791.3599999999997, "end": 2798.1, "text": " The way we deal with missing data, we'll talk more about next week, but in short, we replace", "tokens": [440, 636, 321, 2028, 365, 5361, 1412, 11, 321, 603, 751, 544, 466, 958, 1243, 11, 457, 294, 2099, 11, 321, 7406], "temperature": 0.0, "avg_logprob": -0.16275067078439812, "compression_ratio": 1.5396825396825398, "no_speech_prob": 5.014682301407447e-06}, {"id": 621, "seek": 278068, "start": 2798.1, "end": 2803.3999999999996, "text": " it with a median and add a new column, which is a binary column, saying whether that was", "tokens": [309, 365, 257, 26779, 293, 909, 257, 777, 7738, 11, 597, 307, 257, 17434, 7738, 11, 1566, 1968, 300, 390], "temperature": 0.0, "avg_logprob": -0.16275067078439812, "compression_ratio": 1.5396825396825398, "no_speech_prob": 5.014682301407447e-06}, {"id": 622, "seek": 278068, "start": 2803.3999999999996, "end": 2805.6, "text": " missing or not.", "tokens": [5361, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.16275067078439812, "compression_ratio": 1.5396825396825398, "no_speech_prob": 5.014682301407447e-06}, {"id": 623, "seek": 280560, "start": 2805.6, "end": 2811.44, "text": " Normalization, there's an important thing here, which is in fact for all of these things,", "tokens": [21277, 2144, 11, 456, 311, 364, 1021, 551, 510, 11, 597, 307, 294, 1186, 337, 439, 295, 613, 721, 11], "temperature": 0.0, "avg_logprob": -0.13734035656369967, "compression_ratio": 1.8821138211382114, "no_speech_prob": 2.1567866497207433e-06}, {"id": 624, "seek": 280560, "start": 2811.44, "end": 2817.52, "text": " whatever you do to the training set, you need to do exactly the same thing to the validation", "tokens": [2035, 291, 360, 281, 264, 3097, 992, 11, 291, 643, 281, 360, 2293, 264, 912, 551, 281, 264, 24071], "temperature": 0.0, "avg_logprob": -0.13734035656369967, "compression_ratio": 1.8821138211382114, "no_speech_prob": 2.1567866497207433e-06}, {"id": 625, "seek": 280560, "start": 2817.52, "end": 2819.48, "text": " set and the test set.", "tokens": [992, 293, 264, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.13734035656369967, "compression_ratio": 1.8821138211382114, "no_speech_prob": 2.1567866497207433e-06}, {"id": 626, "seek": 280560, "start": 2819.48, "end": 2823.68, "text": " So whatever you replaced your missing values with, you need to replace them with exactly", "tokens": [407, 2035, 291, 10772, 428, 5361, 4190, 365, 11, 291, 643, 281, 7406, 552, 365, 2293], "temperature": 0.0, "avg_logprob": -0.13734035656369967, "compression_ratio": 1.8821138211382114, "no_speech_prob": 2.1567866497207433e-06}, {"id": 627, "seek": 280560, "start": 2823.68, "end": 2825.8399999999997, "text": " the same thing in the validation set.", "tokens": [264, 912, 551, 294, 264, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.13734035656369967, "compression_ratio": 1.8821138211382114, "no_speech_prob": 2.1567866497207433e-06}, {"id": 628, "seek": 280560, "start": 2825.8399999999997, "end": 2828.68, "text": " So Fast.ai handles all these details for you.", "tokens": [407, 15968, 13, 1301, 18722, 439, 613, 4365, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.13734035656369967, "compression_ratio": 1.8821138211382114, "no_speech_prob": 2.1567866497207433e-06}, {"id": 629, "seek": 280560, "start": 2828.68, "end": 2831.68, "text": " There are kinds of things that if you have to do it manually, at least if you're like", "tokens": [821, 366, 3685, 295, 721, 300, 498, 291, 362, 281, 360, 309, 16945, 11, 412, 1935, 498, 291, 434, 411], "temperature": 0.0, "avg_logprob": -0.13734035656369967, "compression_ratio": 1.8821138211382114, "no_speech_prob": 2.1567866497207433e-06}, {"id": 630, "seek": 283168, "start": 2831.68, "end": 2836.96, "text": " me, you'll screw it up lots of times until you finally get it right.", "tokens": [385, 11, 291, 603, 5630, 309, 493, 3195, 295, 1413, 1826, 291, 2721, 483, 309, 558, 13], "temperature": 0.0, "avg_logprob": -0.10774102161840066, "compression_ratio": 1.5387931034482758, "no_speech_prob": 9.368612154503353e-06}, {"id": 631, "seek": 283168, "start": 2836.96, "end": 2842.04, "text": " So that's what these processes are here.", "tokens": [407, 300, 311, 437, 613, 7555, 366, 510, 13], "temperature": 0.0, "avg_logprob": -0.10774102161840066, "compression_ratio": 1.5387931034482758, "no_speech_prob": 9.368612154503353e-06}, {"id": 632, "seek": 283168, "start": 2842.04, "end": 2848.3399999999997, "text": " Then we're going to split into training versus validation sets, and in this case, we do it", "tokens": [1396, 321, 434, 516, 281, 7472, 666, 3097, 5717, 24071, 6352, 11, 293, 294, 341, 1389, 11, 321, 360, 309], "temperature": 0.0, "avg_logprob": -0.10774102161840066, "compression_ratio": 1.5387931034482758, "no_speech_prob": 9.368612154503353e-06}, {"id": 633, "seek": 283168, "start": 2848.3399999999997, "end": 2855.24, "text": " by providing a list of indexes, so the indexes from 800 to 1000.", "tokens": [538, 6530, 257, 1329, 295, 8186, 279, 11, 370, 264, 8186, 279, 490, 13083, 281, 9714, 13], "temperature": 0.0, "avg_logprob": -0.10774102161840066, "compression_ratio": 1.5387931034482758, "no_speech_prob": 9.368612154503353e-06}, {"id": 634, "seek": 283168, "start": 2855.24, "end": 2859.7599999999998, "text": " It's very common, I don't quite remember the details of this data set, but it's very common", "tokens": [467, 311, 588, 2689, 11, 286, 500, 380, 1596, 1604, 264, 4365, 295, 341, 1412, 992, 11, 457, 309, 311, 588, 2689], "temperature": 0.0, "avg_logprob": -0.10774102161840066, "compression_ratio": 1.5387931034482758, "no_speech_prob": 9.368612154503353e-06}, {"id": 635, "seek": 285976, "start": 2859.76, "end": 2865.0200000000004, "text": " for wanting to keep your validation sets to be contiguous groups of things, like if they're", "tokens": [337, 7935, 281, 1066, 428, 24071, 6352, 281, 312, 660, 30525, 3935, 295, 721, 11, 411, 498, 436, 434], "temperature": 0.0, "avg_logprob": -0.08891747016033144, "compression_ratio": 2.0669291338582676, "no_speech_prob": 1.1658440598694142e-05}, {"id": 636, "seek": 285976, "start": 2865.0200000000004, "end": 2870.0400000000004, "text": " map tiles, they should be the map tiles that are next to each other, if they're time periods,", "tokens": [4471, 21982, 11, 436, 820, 312, 264, 4471, 21982, 300, 366, 958, 281, 1184, 661, 11, 498, 436, 434, 565, 13804, 11], "temperature": 0.0, "avg_logprob": -0.08891747016033144, "compression_ratio": 2.0669291338582676, "no_speech_prob": 1.1658440598694142e-05}, {"id": 637, "seek": 285976, "start": 2870.0400000000004, "end": 2874.26, "text": " they should be days that are next to each other, if they're video frames, they should", "tokens": [436, 820, 312, 1708, 300, 366, 958, 281, 1184, 661, 11, 498, 436, 434, 960, 12083, 11, 436, 820], "temperature": 0.0, "avg_logprob": -0.08891747016033144, "compression_ratio": 2.0669291338582676, "no_speech_prob": 1.1658440598694142e-05}, {"id": 638, "seek": 285976, "start": 2874.26, "end": 2878.4, "text": " be video frames next to each other, because otherwise you're kind of cheating.", "tokens": [312, 960, 12083, 958, 281, 1184, 661, 11, 570, 5911, 291, 434, 733, 295, 18309, 13], "temperature": 0.0, "avg_logprob": -0.08891747016033144, "compression_ratio": 2.0669291338582676, "no_speech_prob": 1.1658440598694142e-05}, {"id": 639, "seek": 285976, "start": 2878.4, "end": 2884.0800000000004, "text": " So it's often a good idea to split by IDX and to grab a range that's next to each other", "tokens": [407, 309, 311, 2049, 257, 665, 1558, 281, 7472, 538, 7348, 55, 293, 281, 4444, 257, 3613, 300, 311, 958, 281, 1184, 661], "temperature": 0.0, "avg_logprob": -0.08891747016033144, "compression_ratio": 2.0669291338582676, "no_speech_prob": 1.1658440598694142e-05}, {"id": 640, "seek": 285976, "start": 2884.0800000000004, "end": 2888.2000000000003, "text": " if your data has some kind of structure like that, or find some other way to structure", "tokens": [498, 428, 1412, 575, 512, 733, 295, 3877, 411, 300, 11, 420, 915, 512, 661, 636, 281, 3877], "temperature": 0.0, "avg_logprob": -0.08891747016033144, "compression_ratio": 2.0669291338582676, "no_speech_prob": 1.1658440598694142e-05}, {"id": 641, "seek": 288820, "start": 2888.2, "end": 2891.08, "text": " it in that way.", "tokens": [309, 294, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.1455436015692283, "compression_ratio": 1.6798561151079137, "no_speech_prob": 2.1444489902933128e-05}, {"id": 642, "seek": 288820, "start": 2891.08, "end": 2894.2, "text": " So that's now given us a training and a validation set.", "tokens": [407, 300, 311, 586, 2212, 505, 257, 3097, 293, 257, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.1455436015692283, "compression_ratio": 1.6798561151079137, "no_speech_prob": 2.1444489902933128e-05}, {"id": 643, "seek": 288820, "start": 2894.2, "end": 2898.22, "text": " We now need to add labels, and in this case the labels can come straight from the data", "tokens": [492, 586, 643, 281, 909, 16949, 11, 293, 294, 341, 1389, 264, 16949, 393, 808, 2997, 490, 264, 1412], "temperature": 0.0, "avg_logprob": -0.1455436015692283, "compression_ratio": 1.6798561151079137, "no_speech_prob": 2.1444489902933128e-05}, {"id": 644, "seek": 288820, "start": 2898.22, "end": 2901.48, "text": " frame we grabbed earlier, so we just have to tell it which column it is.", "tokens": [3920, 321, 18607, 3071, 11, 370, 321, 445, 362, 281, 980, 309, 597, 7738, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1455436015692283, "compression_ratio": 1.6798561151079137, "no_speech_prob": 2.1444489902933128e-05}, {"id": 645, "seek": 288820, "start": 2901.48, "end": 2908.0, "text": " And so the dependent variable is, I think it's whether they're making over $50,000 salary,", "tokens": [400, 370, 264, 12334, 7006, 307, 11, 286, 519, 309, 311, 1968, 436, 434, 1455, 670, 1848, 2803, 11, 1360, 15360, 11], "temperature": 0.0, "avg_logprob": -0.1455436015692283, "compression_ratio": 1.6798561151079137, "no_speech_prob": 2.1444489902933128e-05}, {"id": 646, "seek": 288820, "start": 2908.0, "end": 2912.52, "text": " that's the thing we're trying to predict in this case.", "tokens": [300, 311, 264, 551, 321, 434, 1382, 281, 6069, 294, 341, 1389, 13], "temperature": 0.0, "avg_logprob": -0.1455436015692283, "compression_ratio": 1.6798561151079137, "no_speech_prob": 2.1444489902933128e-05}, {"id": 647, "seek": 288820, "start": 2912.52, "end": 2918.08, "text": " We'll talk about test sets later, but in this case we can add a test set, and finally get", "tokens": [492, 603, 751, 466, 1500, 6352, 1780, 11, 457, 294, 341, 1389, 321, 393, 909, 257, 1500, 992, 11, 293, 2721, 483], "temperature": 0.0, "avg_logprob": -0.1455436015692283, "compression_ratio": 1.6798561151079137, "no_speech_prob": 2.1444489902933128e-05}, {"id": 648, "seek": 291808, "start": 2918.08, "end": 2919.36, "text": " our data bunch.", "tokens": [527, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.12236948155645114, "compression_ratio": 1.5163398692810457, "no_speech_prob": 2.7108022550237365e-05}, {"id": 649, "seek": 291808, "start": 2919.36, "end": 2924.24, "text": " So at that point we have something that looks like this.", "tokens": [407, 412, 300, 935, 321, 362, 746, 300, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.12236948155645114, "compression_ratio": 1.5163398692810457, "no_speech_prob": 2.7108022550237365e-05}, {"id": 650, "seek": 291808, "start": 2924.24, "end": 2930.24, "text": " So there is our data.", "tokens": [407, 456, 307, 527, 1412, 13], "temperature": 0.0, "avg_logprob": -0.12236948155645114, "compression_ratio": 1.5163398692810457, "no_speech_prob": 2.7108022550237365e-05}, {"id": 651, "seek": 291808, "start": 2930.24, "end": 2935.12, "text": " And then to use it, it looks very familiar.", "tokens": [400, 550, 281, 764, 309, 11, 309, 1542, 588, 4963, 13], "temperature": 0.0, "avg_logprob": -0.12236948155645114, "compression_ratio": 1.5163398692810457, "no_speech_prob": 2.7108022550237365e-05}, {"id": 652, "seek": 291808, "start": 2935.12, "end": 2940.88, "text": " You get a learner, in this case it's a tabular learner, passing in the data, some information", "tokens": [509, 483, 257, 33347, 11, 294, 341, 1389, 309, 311, 257, 4421, 1040, 33347, 11, 8437, 294, 264, 1412, 11, 512, 1589], "temperature": 0.0, "avg_logprob": -0.12236948155645114, "compression_ratio": 1.5163398692810457, "no_speech_prob": 2.7108022550237365e-05}, {"id": 653, "seek": 294088, "start": 2940.88, "end": 2955.48, "text": " about your architecture and some metrics, and you then call fit.", "tokens": [466, 428, 9482, 293, 512, 16367, 11, 293, 291, 550, 818, 3318, 13], "temperature": 0.0, "avg_logprob": -0.2827829801119291, "compression_ratio": 1.4342857142857144, "no_speech_prob": 2.8407981517375447e-05}, {"id": 654, "seek": 294088, "start": 2955.48, "end": 2962.12, "text": " How to combine NLP tokenized data with metadata such as tabular data with Fast.ai, for instance", "tokens": [1012, 281, 10432, 426, 45196, 14862, 1602, 1412, 365, 26603, 1270, 382, 4421, 1040, 1412, 365, 15968, 13, 1301, 11, 337, 5197], "temperature": 0.0, "avg_logprob": -0.2827829801119291, "compression_ratio": 1.4342857142857144, "no_speech_prob": 2.8407981517375447e-05}, {"id": 655, "seek": 294088, "start": 2962.12, "end": 2969.12, "text": " for IMBD classification, how to use information like who the actors are, year, may, genre,", "tokens": [337, 21463, 33, 35, 21538, 11, 577, 281, 764, 1589, 411, 567, 264, 10037, 366, 11, 1064, 11, 815, 11, 11022, 11], "temperature": 0.0, "avg_logprob": -0.2827829801119291, "compression_ratio": 1.4342857142857144, "no_speech_prob": 2.8407981517375447e-05}, {"id": 656, "seek": 296912, "start": 2969.12, "end": 2971.12, "text": " etc.", "tokens": [5183, 13], "temperature": 0.0, "avg_logprob": -0.2012784140450614, "compression_ratio": 1.5586854460093897, "no_speech_prob": 1.1842793355754111e-05}, {"id": 657, "seek": 296912, "start": 2971.12, "end": 2976.6, "text": " We're not quite up to that yet, so we need to learn a little bit more about how neural", "tokens": [492, 434, 406, 1596, 493, 281, 300, 1939, 11, 370, 321, 643, 281, 1466, 257, 707, 857, 544, 466, 577, 18161], "temperature": 0.0, "avg_logprob": -0.2012784140450614, "compression_ratio": 1.5586854460093897, "no_speech_prob": 1.1842793355754111e-05}, {"id": 658, "seek": 296912, "start": 2976.6, "end": 2979.4, "text": " net architectures work.", "tokens": [2533, 6331, 1303, 589, 13], "temperature": 0.0, "avg_logprob": -0.2012784140450614, "compression_ratio": 1.5586854460093897, "no_speech_prob": 1.1842793355754111e-05}, {"id": 659, "seek": 296912, "start": 2979.4, "end": 2984.92, "text": " But conceptually it's kind of the same as the way we combine categorical variables and", "tokens": [583, 3410, 671, 309, 311, 733, 295, 264, 912, 382, 264, 636, 321, 10432, 19250, 804, 9102, 293], "temperature": 0.0, "avg_logprob": -0.2012784140450614, "compression_ratio": 1.5586854460093897, "no_speech_prob": 1.1842793355754111e-05}, {"id": 660, "seek": 296912, "start": 2984.92, "end": 2986.3599999999997, "text": " continuous variables.", "tokens": [10957, 9102, 13], "temperature": 0.0, "avg_logprob": -0.2012784140450614, "compression_ratio": 1.5586854460093897, "no_speech_prob": 1.1842793355754111e-05}, {"id": 661, "seek": 296912, "start": 2986.3599999999997, "end": 2994.4, "text": " Basically in the neural network you can have two different sets of inputs merging together", "tokens": [8537, 294, 264, 18161, 3209, 291, 393, 362, 732, 819, 6352, 295, 15743, 44559, 1214], "temperature": 0.0, "avg_logprob": -0.2012784140450614, "compression_ratio": 1.5586854460093897, "no_speech_prob": 1.1842793355754111e-05}, {"id": 662, "seek": 296912, "start": 2994.4, "end": 2995.4, "text": " into some layer.", "tokens": [666, 512, 4583, 13], "temperature": 0.0, "avg_logprob": -0.2012784140450614, "compression_ratio": 1.5586854460093897, "no_speech_prob": 1.1842793355754111e-05}, {"id": 663, "seek": 299540, "start": 2995.4, "end": 2999.32, "text": " It could go into an early layer or into a later layer, it kind of depends.", "tokens": [467, 727, 352, 666, 364, 2440, 4583, 420, 666, 257, 1780, 4583, 11, 309, 733, 295, 5946, 13], "temperature": 0.0, "avg_logprob": -0.1474847106933594, "compression_ratio": 1.8598484848484849, "no_speech_prob": 5.390376099967398e-05}, {"id": 664, "seek": 299540, "start": 2999.32, "end": 3005.76, "text": " If it's like text and an image and some metadata, you probably want the text going into an RNN,", "tokens": [759, 309, 311, 411, 2487, 293, 364, 3256, 293, 512, 26603, 11, 291, 1391, 528, 264, 2487, 516, 666, 364, 45702, 45, 11], "temperature": 0.0, "avg_logprob": -0.1474847106933594, "compression_ratio": 1.8598484848484849, "no_speech_prob": 5.390376099967398e-05}, {"id": 665, "seek": 299540, "start": 3005.76, "end": 3010.64, "text": " the image going into a CNN, the metadata going into some kind of tabular model like this,", "tokens": [264, 3256, 516, 666, 257, 24859, 11, 264, 26603, 516, 666, 512, 733, 295, 4421, 1040, 2316, 411, 341, 11], "temperature": 0.0, "avg_logprob": -0.1474847106933594, "compression_ratio": 1.8598484848484849, "no_speech_prob": 5.390376099967398e-05}, {"id": 666, "seek": 299540, "start": 3010.64, "end": 3014.32, "text": " and then you'd have them basically all concatenated together and then go through some fully connected", "tokens": [293, 550, 291, 1116, 362, 552, 1936, 439, 1588, 7186, 770, 1214, 293, 550, 352, 807, 512, 4498, 4582], "temperature": 0.0, "avg_logprob": -0.1474847106933594, "compression_ratio": 1.8598484848484849, "no_speech_prob": 5.390376099967398e-05}, {"id": 667, "seek": 299540, "start": 3014.32, "end": 3018.0, "text": " layers and train them end to end.", "tokens": [7914, 293, 3847, 552, 917, 281, 917, 13], "temperature": 0.0, "avg_logprob": -0.1474847106933594, "compression_ratio": 1.8598484848484849, "no_speech_prob": 5.390376099967398e-05}, {"id": 668, "seek": 299540, "start": 3018.0, "end": 3020.96, "text": " We'll probably largely get into that in part two.", "tokens": [492, 603, 1391, 11611, 483, 666, 300, 294, 644, 732, 13], "temperature": 0.0, "avg_logprob": -0.1474847106933594, "compression_ratio": 1.8598484848484849, "no_speech_prob": 5.390376099967398e-05}, {"id": 669, "seek": 299540, "start": 3020.96, "end": 3023.48, "text": " In fact we might entirely get into part two.", "tokens": [682, 1186, 321, 1062, 7696, 483, 666, 644, 732, 13], "temperature": 0.0, "avg_logprob": -0.1474847106933594, "compression_ratio": 1.8598484848484849, "no_speech_prob": 5.390376099967398e-05}, {"id": 670, "seek": 302348, "start": 3023.48, "end": 3028.2400000000002, "text": " I'm not sure if we'll have time to cover it in part one.", "tokens": [286, 478, 406, 988, 498, 321, 603, 362, 565, 281, 2060, 309, 294, 644, 472, 13], "temperature": 0.0, "avg_logprob": -0.24454278531281845, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.7777654647943564e-05}, {"id": 671, "seek": 302348, "start": 3028.2400000000002, "end": 3032.92, "text": " Conceptually it's a fairly simple extension of what we'll be learning in the next three", "tokens": [47482, 671, 309, 311, 257, 6457, 2199, 10320, 295, 437, 321, 603, 312, 2539, 294, 264, 958, 1045], "temperature": 0.0, "avg_logprob": -0.24454278531281845, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.7777654647943564e-05}, {"id": 672, "seek": 302348, "start": 3032.92, "end": 3033.92, "text": " weeks.", "tokens": [3259, 13], "temperature": 0.0, "avg_logprob": -0.24454278531281845, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.7777654647943564e-05}, {"id": 673, "seek": 302348, "start": 3033.92, "end": 3042.2, "text": " Question 2 Do you think things like scikit-learn and XGBoost will eventually become outdated?", "tokens": [14464, 568, 1144, 291, 519, 721, 411, 2180, 22681, 12, 306, 1083, 293, 1783, 8769, 78, 555, 486, 4728, 1813, 36313, 30], "temperature": 0.0, "avg_logprob": -0.24454278531281845, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.7777654647943564e-05}, {"id": 674, "seek": 302348, "start": 3042.2, "end": 3049.6, "text": " Will everyone use deep learning tools in the future except for maybe small datasets?", "tokens": [3099, 1518, 764, 2452, 2539, 3873, 294, 264, 2027, 3993, 337, 1310, 1359, 42856, 30], "temperature": 0.0, "avg_logprob": -0.24454278531281845, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.7777654647943564e-05}, {"id": 675, "seek": 302348, "start": 3049.6, "end": 3050.6, "text": " I have no idea.", "tokens": [286, 362, 572, 1558, 13], "temperature": 0.0, "avg_logprob": -0.24454278531281845, "compression_ratio": 1.5109170305676856, "no_speech_prob": 1.7777654647943564e-05}, {"id": 676, "seek": 305060, "start": 3050.6, "end": 3055.64, "text": " I'm not good at making predictions.", "tokens": [286, 478, 406, 665, 412, 1455, 21264, 13], "temperature": 0.0, "avg_logprob": -0.192039552625719, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.00013551200390793383}, {"id": 677, "seek": 305060, "start": 3055.64, "end": 3058.2, "text": " I'm not a machine learning model.", "tokens": [286, 478, 406, 257, 3479, 2539, 2316, 13], "temperature": 0.0, "avg_logprob": -0.192039552625719, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.00013551200390793383}, {"id": 678, "seek": 305060, "start": 3058.2, "end": 3065.08, "text": " I mean XGBoost is a really nice piece of software.", "tokens": [286, 914, 1783, 8769, 78, 555, 307, 257, 534, 1481, 2522, 295, 4722, 13], "temperature": 0.0, "avg_logprob": -0.192039552625719, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.00013551200390793383}, {"id": 679, "seek": 305060, "start": 3065.08, "end": 3069.92, "text": " There's quite a few really nice pieces of software for gradient boosting in particular.", "tokens": [821, 311, 1596, 257, 1326, 534, 1481, 3755, 295, 4722, 337, 16235, 43117, 294, 1729, 13], "temperature": 0.0, "avg_logprob": -0.192039552625719, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.00013551200390793383}, {"id": 680, "seek": 305060, "start": 3069.92, "end": 3073.12, "text": " They have some really nice features, actually Random Forest in particular has some really", "tokens": [814, 362, 512, 534, 1481, 4122, 11, 767, 37603, 18124, 294, 1729, 575, 512, 534], "temperature": 0.0, "avg_logprob": -0.192039552625719, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.00013551200390793383}, {"id": 681, "seek": 305060, "start": 3073.12, "end": 3078.64, "text": " nice features for interpretation, which I'm sure we'll find similar versions for neural", "tokens": [1481, 4122, 337, 14174, 11, 597, 286, 478, 988, 321, 603, 915, 2531, 9606, 337, 18161], "temperature": 0.0, "avg_logprob": -0.192039552625719, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.00013551200390793383}, {"id": 682, "seek": 307864, "start": 3078.64, "end": 3081.8399999999997, "text": " nets but they don't necessarily exist yet.", "tokens": [36170, 457, 436, 500, 380, 4725, 2514, 1939, 13], "temperature": 0.0, "avg_logprob": -0.2332860747380043, "compression_ratio": 1.4088050314465408, "no_speech_prob": 5.475617581396364e-05}, {"id": 683, "seek": 307864, "start": 3081.8399999999997, "end": 3082.8399999999997, "text": " So I don't know.", "tokens": [407, 286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.2332860747380043, "compression_ratio": 1.4088050314465408, "no_speech_prob": 5.475617581396364e-05}, {"id": 684, "seek": 307864, "start": 3082.8399999999997, "end": 3089.24, "text": " For now they're both useful tools.", "tokens": [1171, 586, 436, 434, 1293, 4420, 3873, 13], "temperature": 0.0, "avg_logprob": -0.2332860747380043, "compression_ratio": 1.4088050314465408, "no_speech_prob": 5.475617581396364e-05}, {"id": 685, "seek": 307864, "start": 3089.24, "end": 3100.0, "text": " Scikit-learn is a library that's often used for pre-processing and running models.", "tokens": [16942, 22681, 12, 306, 1083, 307, 257, 6405, 300, 311, 2049, 1143, 337, 659, 12, 41075, 278, 293, 2614, 5245, 13], "temperature": 0.0, "avg_logprob": -0.2332860747380043, "compression_ratio": 1.4088050314465408, "no_speech_prob": 5.475617581396364e-05}, {"id": 686, "seek": 307864, "start": 3100.0, "end": 3103.24, "text": " It's hard to predict where things will end up.", "tokens": [467, 311, 1152, 281, 6069, 689, 721, 486, 917, 493, 13], "temperature": 0.0, "avg_logprob": -0.2332860747380043, "compression_ratio": 1.4088050314465408, "no_speech_prob": 5.475617581396364e-05}, {"id": 687, "seek": 310324, "start": 3103.24, "end": 3110.3599999999997, "text": " In some ways it's more focused on some older approaches to modeling.", "tokens": [682, 512, 2098, 309, 311, 544, 5178, 322, 512, 4906, 11587, 281, 15983, 13], "temperature": 0.0, "avg_logprob": -0.250591883292565, "compression_ratio": 1.6787330316742082, "no_speech_prob": 3.0715978937223554e-05}, {"id": 688, "seek": 310324, "start": 3110.3599999999997, "end": 3112.12, "text": " They keep on adding new things.", "tokens": [814, 1066, 322, 5127, 777, 721, 13], "temperature": 0.0, "avg_logprob": -0.250591883292565, "compression_ratio": 1.6787330316742082, "no_speech_prob": 3.0715978937223554e-05}, {"id": 689, "seek": 310324, "start": 3112.12, "end": 3113.8799999999997, "text": " So we'll see.", "tokens": [407, 321, 603, 536, 13], "temperature": 0.0, "avg_logprob": -0.250591883292565, "compression_ratio": 1.6787330316742082, "no_speech_prob": 3.0715978937223554e-05}, {"id": 690, "seek": 310324, "start": 3113.8799999999997, "end": 3118.3599999999997, "text": " I keep trying to incorporate more scikit-learn stuff into fast AI and then I keep finding", "tokens": [286, 1066, 1382, 281, 16091, 544, 2180, 22681, 12, 306, 1083, 1507, 666, 2370, 7318, 293, 550, 286, 1066, 5006], "temperature": 0.0, "avg_logprob": -0.250591883292565, "compression_ratio": 1.6787330316742082, "no_speech_prob": 3.0715978937223554e-05}, {"id": 691, "seek": 310324, "start": 3118.3599999999997, "end": 3121.9599999999996, "text": " ways I think I can do it better and I throw it away again.", "tokens": [2098, 286, 519, 286, 393, 360, 309, 1101, 293, 286, 3507, 309, 1314, 797, 13], "temperature": 0.0, "avg_logprob": -0.250591883292565, "compression_ratio": 1.6787330316742082, "no_speech_prob": 3.0715978937223554e-05}, {"id": 692, "seek": 310324, "start": 3121.9599999999996, "end": 3125.8799999999997, "text": " So that's why there's still no scikit-learn dependencies in fast AI.", "tokens": [407, 300, 311, 983, 456, 311, 920, 572, 2180, 22681, 12, 306, 1083, 36606, 294, 2370, 7318, 13], "temperature": 0.0, "avg_logprob": -0.250591883292565, "compression_ratio": 1.6787330316742082, "no_speech_prob": 3.0715978937223554e-05}, {"id": 693, "seek": 310324, "start": 3125.8799999999997, "end": 3130.2799999999997, "text": " I keep finding other ways to do stuff.", "tokens": [286, 1066, 5006, 661, 2098, 281, 360, 1507, 13], "temperature": 0.0, "avg_logprob": -0.250591883292565, "compression_ratio": 1.6787330316742082, "no_speech_prob": 3.0715978937223554e-05}, {"id": 694, "seek": 313028, "start": 3130.28, "end": 3140.44, "text": " Okay, so we're going to learn what layers equals means either towards the end of class", "tokens": [1033, 11, 370, 321, 434, 516, 281, 1466, 437, 7914, 6915, 1355, 2139, 3030, 264, 917, 295, 1508], "temperature": 0.0, "avg_logprob": -0.19991119235169655, "compression_ratio": 1.575875486381323, "no_speech_prob": 2.3186161342891864e-05}, {"id": 695, "seek": 313028, "start": 3140.44, "end": 3145.0800000000004, "text": " today or the start of class next week, but this is where we're basically defining our", "tokens": [965, 420, 264, 722, 295, 1508, 958, 1243, 11, 457, 341, 307, 689, 321, 434, 1936, 17827, 527], "temperature": 0.0, "avg_logprob": -0.19991119235169655, "compression_ratio": 1.575875486381323, "no_speech_prob": 2.3186161342891864e-05}, {"id": 696, "seek": 313028, "start": 3145.0800000000004, "end": 3152.0800000000004, "text": " architecture just like when we chose ResNet34 or whatever for all conv nets.", "tokens": [9482, 445, 411, 562, 321, 5111, 5015, 31890, 12249, 420, 2035, 337, 439, 3754, 36170, 13], "temperature": 0.0, "avg_logprob": -0.19991119235169655, "compression_ratio": 1.575875486381323, "no_speech_prob": 2.3186161342891864e-05}, {"id": 697, "seek": 313028, "start": 3152.0800000000004, "end": 3155.92, "text": " We'll look at more about metrics in a moment, but just to remind you, metrics are just the", "tokens": [492, 603, 574, 412, 544, 466, 16367, 294, 257, 1623, 11, 457, 445, 281, 4160, 291, 11, 16367, 366, 445, 264], "temperature": 0.0, "avg_logprob": -0.19991119235169655, "compression_ratio": 1.575875486381323, "no_speech_prob": 2.3186161342891864e-05}, {"id": 698, "seek": 313028, "start": 3155.92, "end": 3157.6000000000004, "text": " things that get printed out.", "tokens": [721, 300, 483, 13567, 484, 13], "temperature": 0.0, "avg_logprob": -0.19991119235169655, "compression_ratio": 1.575875486381323, "no_speech_prob": 2.3186161342891864e-05}, {"id": 699, "seek": 313028, "start": 3157.6000000000004, "end": 3159.2000000000003, "text": " They don't change our model at all.", "tokens": [814, 500, 380, 1319, 527, 2316, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.19991119235169655, "compression_ratio": 1.575875486381323, "no_speech_prob": 2.3186161342891864e-05}, {"id": 700, "seek": 315920, "start": 3159.2, "end": 3165.3999999999996, "text": " So in this case we're saying I want you to print out the accuracy to see how we're doing.", "tokens": [407, 294, 341, 1389, 321, 434, 1566, 286, 528, 291, 281, 4482, 484, 264, 14170, 281, 536, 577, 321, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.1487095586715206, "compression_ratio": 1.8588235294117648, "no_speech_prob": 1.7231341189472005e-05}, {"id": 701, "seek": 315920, "start": 3165.3999999999996, "end": 3169.48, "text": " Okay, so that's how to do tabular.", "tokens": [1033, 11, 370, 300, 311, 577, 281, 360, 4421, 1040, 13], "temperature": 0.0, "avg_logprob": -0.1487095586715206, "compression_ratio": 1.8588235294117648, "no_speech_prob": 1.7231341189472005e-05}, {"id": 702, "seek": 315920, "start": 3169.48, "end": 3173.18, "text": " This is going to work really well because we're going to hit our break soon and the", "tokens": [639, 307, 516, 281, 589, 534, 731, 570, 321, 434, 516, 281, 2045, 527, 1821, 2321, 293, 264], "temperature": 0.0, "avg_logprob": -0.1487095586715206, "compression_ratio": 1.8588235294117648, "no_speech_prob": 1.7231341189472005e-05}, {"id": 703, "seek": 315920, "start": 3173.18, "end": 3177.72, "text": " idea was that after three and a half lessons we're going to hit the end of all of the quick", "tokens": [1558, 390, 300, 934, 1045, 293, 257, 1922, 8820, 321, 434, 516, 281, 2045, 264, 917, 295, 439, 295, 264, 1702], "temperature": 0.0, "avg_logprob": -0.1487095586715206, "compression_ratio": 1.8588235294117648, "no_speech_prob": 1.7231341189472005e-05}, {"id": 704, "seek": 315920, "start": 3177.72, "end": 3180.48, "text": " overview of applications and then we're going to go down the other side.", "tokens": [12492, 295, 5821, 293, 550, 321, 434, 516, 281, 352, 760, 264, 661, 1252, 13], "temperature": 0.0, "avg_logprob": -0.1487095586715206, "compression_ratio": 1.8588235294117648, "no_speech_prob": 1.7231341189472005e-05}, {"id": 705, "seek": 315920, "start": 3180.48, "end": 3187.3199999999997, "text": " I think we're going to be to the minute we're going to hit it, because the next one is collaborative", "tokens": [286, 519, 321, 434, 516, 281, 312, 281, 264, 3456, 321, 434, 516, 281, 2045, 309, 11, 570, 264, 958, 472, 307, 16555], "temperature": 0.0, "avg_logprob": -0.1487095586715206, "compression_ratio": 1.8588235294117648, "no_speech_prob": 1.7231341189472005e-05}, {"id": 706, "seek": 318732, "start": 3187.32, "end": 3189.28, "text": " filtering.", "tokens": [30822, 13], "temperature": 0.0, "avg_logprob": -0.11085249772712366, "compression_ratio": 1.872611464968153, "no_speech_prob": 3.905436187778832e-06}, {"id": 707, "seek": 318732, "start": 3189.28, "end": 3201.4, "text": " So collaborative filtering is where you have information about who bought what or who liked", "tokens": [407, 16555, 30822, 307, 689, 291, 362, 1589, 466, 567, 4243, 437, 420, 567, 4501], "temperature": 0.0, "avg_logprob": -0.11085249772712366, "compression_ratio": 1.872611464968153, "no_speech_prob": 3.905436187778832e-06}, {"id": 708, "seek": 318732, "start": 3201.4, "end": 3202.4, "text": " what.", "tokens": [437, 13], "temperature": 0.0, "avg_logprob": -0.11085249772712366, "compression_ratio": 1.872611464968153, "no_speech_prob": 3.905436187778832e-06}, {"id": 709, "seek": 318732, "start": 3202.4, "end": 3210.76, "text": " It's basically something where you have something like a user or a reviewer or whatever and", "tokens": [467, 311, 1936, 746, 689, 291, 362, 746, 411, 257, 4195, 420, 257, 3131, 260, 420, 2035, 293], "temperature": 0.0, "avg_logprob": -0.11085249772712366, "compression_ratio": 1.872611464968153, "no_speech_prob": 3.905436187778832e-06}, {"id": 710, "seek": 318732, "start": 3210.76, "end": 3216.6400000000003, "text": " information about what they've bought or what they've written about or what they've reviewed.", "tokens": [1589, 466, 437, 436, 600, 4243, 420, 437, 436, 600, 3720, 466, 420, 437, 436, 600, 18429, 13], "temperature": 0.0, "avg_logprob": -0.11085249772712366, "compression_ratio": 1.872611464968153, "no_speech_prob": 3.905436187778832e-06}, {"id": 711, "seek": 321664, "start": 3216.64, "end": 3222.08, "text": " So in the most basic version of collaborative filtering, you just have two columns, something", "tokens": [407, 294, 264, 881, 3875, 3037, 295, 16555, 30822, 11, 291, 445, 362, 732, 13766, 11, 746], "temperature": 0.0, "avg_logprob": -0.16140722572256666, "compression_ratio": 1.8181818181818181, "no_speech_prob": 3.905398443748709e-06}, {"id": 712, "seek": 321664, "start": 3222.08, "end": 3227.48, "text": " like user ID and movie ID and that just says this user bought that movie, this user bought", "tokens": [411, 4195, 7348, 293, 3169, 7348, 293, 300, 445, 1619, 341, 4195, 4243, 300, 3169, 11, 341, 4195, 4243], "temperature": 0.0, "avg_logprob": -0.16140722572256666, "compression_ratio": 1.8181818181818181, "no_speech_prob": 3.905398443748709e-06}, {"id": 713, "seek": 321664, "start": 3227.48, "end": 3228.92, "text": " that movie, this user bought that movie.", "tokens": [300, 3169, 11, 341, 4195, 4243, 300, 3169, 13], "temperature": 0.0, "avg_logprob": -0.16140722572256666, "compression_ratio": 1.8181818181818181, "no_speech_prob": 3.905398443748709e-06}, {"id": 714, "seek": 321664, "start": 3228.92, "end": 3234.8799999999997, "text": " So for example, Amazon has a really big list of user IDs and product IDs of like what did", "tokens": [407, 337, 1365, 11, 6795, 575, 257, 534, 955, 1329, 295, 4195, 48212, 293, 1674, 48212, 295, 411, 437, 630], "temperature": 0.0, "avg_logprob": -0.16140722572256666, "compression_ratio": 1.8181818181818181, "no_speech_prob": 3.905398443748709e-06}, {"id": 715, "seek": 321664, "start": 3234.8799999999997, "end": 3235.8799999999997, "text": " you buy.", "tokens": [291, 2256, 13], "temperature": 0.0, "avg_logprob": -0.16140722572256666, "compression_ratio": 1.8181818181818181, "no_speech_prob": 3.905398443748709e-06}, {"id": 716, "seek": 321664, "start": 3235.8799999999997, "end": 3242.44, "text": " Then you can add additional information to that table such as, oh they left a review,", "tokens": [1396, 291, 393, 909, 4497, 1589, 281, 300, 3199, 1270, 382, 11, 1954, 436, 1411, 257, 3131, 11], "temperature": 0.0, "avg_logprob": -0.16140722572256666, "compression_ratio": 1.8181818181818181, "no_speech_prob": 3.905398443748709e-06}, {"id": 717, "seek": 321664, "start": 3242.44, "end": 3243.72, "text": " what review did they give it?", "tokens": [437, 3131, 630, 436, 976, 309, 30], "temperature": 0.0, "avg_logprob": -0.16140722572256666, "compression_ratio": 1.8181818181818181, "no_speech_prob": 3.905398443748709e-06}, {"id": 718, "seek": 324372, "start": 3243.72, "end": 3249.56, "text": " So it's now like user ID, movie ID, number of stars.", "tokens": [407, 309, 311, 586, 411, 4195, 7348, 11, 3169, 7348, 11, 1230, 295, 6105, 13], "temperature": 0.0, "avg_logprob": -0.0889286994934082, "compression_ratio": 1.596774193548387, "no_speech_prob": 3.041553327420843e-06}, {"id": 719, "seek": 324372, "start": 3249.56, "end": 3251.24, "text": " You could add a time code.", "tokens": [509, 727, 909, 257, 565, 3089, 13], "temperature": 0.0, "avg_logprob": -0.0889286994934082, "compression_ratio": 1.596774193548387, "no_speech_prob": 3.041553327420843e-06}, {"id": 720, "seek": 324372, "start": 3251.24, "end": 3258.04, "text": " So like this user bought this product at this time and gave it this review.", "tokens": [407, 411, 341, 4195, 4243, 341, 1674, 412, 341, 565, 293, 2729, 309, 341, 3131, 13], "temperature": 0.0, "avg_logprob": -0.0889286994934082, "compression_ratio": 1.596774193548387, "no_speech_prob": 3.041553327420843e-06}, {"id": 721, "seek": 324372, "start": 3258.04, "end": 3261.68, "text": " But they're all basically the same kind of structure.", "tokens": [583, 436, 434, 439, 1936, 264, 912, 733, 295, 3877, 13], "temperature": 0.0, "avg_logprob": -0.0889286994934082, "compression_ratio": 1.596774193548387, "no_speech_prob": 3.041553327420843e-06}, {"id": 722, "seek": 324372, "start": 3261.68, "end": 3268.3999999999996, "text": " So there's kind of like two ways you could draw that collaborative filtering structure.", "tokens": [407, 456, 311, 733, 295, 411, 732, 2098, 291, 727, 2642, 300, 16555, 30822, 3877, 13], "temperature": 0.0, "avg_logprob": -0.0889286994934082, "compression_ratio": 1.596774193548387, "no_speech_prob": 3.041553327420843e-06}, {"id": 723, "seek": 326840, "start": 3268.4, "end": 3278.12, "text": " One is kind of a two column approach where you've got like user and movie.", "tokens": [1485, 307, 733, 295, 257, 732, 7738, 3109, 689, 291, 600, 658, 411, 4195, 293, 3169, 13], "temperature": 0.0, "avg_logprob": -0.2528387429057688, "compression_ratio": 1.4404761904761905, "no_speech_prob": 1.4823466472080327e-06}, {"id": 724, "seek": 326840, "start": 3278.12, "end": 3280.56, "text": " And you've got user ID, movie ID, user ID.", "tokens": [400, 291, 600, 658, 4195, 7348, 11, 3169, 7348, 11, 4195, 7348, 13], "temperature": 0.0, "avg_logprob": -0.2528387429057688, "compression_ratio": 1.4404761904761905, "no_speech_prob": 1.4823466472080327e-06}, {"id": 725, "seek": 326840, "start": 3280.56, "end": 3285.1600000000003, "text": " Each pair basically describes that user watched that movie.", "tokens": [6947, 6119, 1936, 15626, 300, 4195, 6337, 300, 3169, 13], "temperature": 0.0, "avg_logprob": -0.2528387429057688, "compression_ratio": 1.4404761904761905, "no_speech_prob": 1.4823466472080327e-06}, {"id": 726, "seek": 326840, "start": 3285.1600000000003, "end": 3293.48, "text": " Possibly also plus number of stars, you know, 3, 4, 1, whatever.", "tokens": [33112, 3545, 611, 1804, 1230, 295, 6105, 11, 291, 458, 11, 805, 11, 1017, 11, 502, 11, 2035, 13], "temperature": 0.0, "avg_logprob": -0.2528387429057688, "compression_ratio": 1.4404761904761905, "no_speech_prob": 1.4823466472080327e-06}, {"id": 727, "seek": 329348, "start": 3293.48, "end": 3301.92, "text": " So the other way that you could write it would be you could have like all the users down", "tokens": [407, 264, 661, 636, 300, 291, 727, 2464, 309, 576, 312, 291, 727, 362, 411, 439, 264, 5022, 760], "temperature": 0.0, "avg_logprob": -0.15547496622258966, "compression_ratio": 1.3626373626373627, "no_speech_prob": 4.7108610488066915e-06}, {"id": 728, "seek": 329348, "start": 3301.92, "end": 3317.52, "text": " here and all the movies along here.", "tokens": [510, 293, 439, 264, 6233, 2051, 510, 13], "temperature": 0.0, "avg_logprob": -0.15547496622258966, "compression_ratio": 1.3626373626373627, "no_speech_prob": 4.7108610488066915e-06}, {"id": 729, "seek": 331752, "start": 3317.52, "end": 3325.96, "text": " And then you can look and find a particular cell in there to find out, you know, could", "tokens": [400, 550, 291, 393, 574, 293, 915, 257, 1729, 2815, 294, 456, 281, 915, 484, 11, 291, 458, 11, 727], "temperature": 0.0, "avg_logprob": -0.15791443095487706, "compression_ratio": 1.6105769230769231, "no_speech_prob": 5.4221654863795266e-06}, {"id": 730, "seek": 331752, "start": 3325.96, "end": 3330.36, "text": " be the rating of that user for that movie or there's just a 1 there if that user watched", "tokens": [312, 264, 10990, 295, 300, 4195, 337, 300, 3169, 420, 456, 311, 445, 257, 502, 456, 498, 300, 4195, 6337], "temperature": 0.0, "avg_logprob": -0.15791443095487706, "compression_ratio": 1.6105769230769231, "no_speech_prob": 5.4221654863795266e-06}, {"id": 731, "seek": 331752, "start": 3330.36, "end": 3331.36, "text": " that movie or whatever.", "tokens": [300, 3169, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.15791443095487706, "compression_ratio": 1.6105769230769231, "no_speech_prob": 5.4221654863795266e-06}, {"id": 732, "seek": 331752, "start": 3331.36, "end": 3338.16, "text": " So there's like two different ways of representing the same information.", "tokens": [407, 456, 311, 411, 732, 819, 2098, 295, 13460, 264, 912, 1589, 13], "temperature": 0.0, "avg_logprob": -0.15791443095487706, "compression_ratio": 1.6105769230769231, "no_speech_prob": 5.4221654863795266e-06}, {"id": 733, "seek": 331752, "start": 3338.16, "end": 3344.4, "text": " Conceptually it's often easier to think of it this way, right?", "tokens": [47482, 671, 309, 311, 2049, 3571, 281, 519, 295, 309, 341, 636, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15791443095487706, "compression_ratio": 1.6105769230769231, "no_speech_prob": 5.4221654863795266e-06}, {"id": 734, "seek": 334440, "start": 3344.4, "end": 3348.7200000000003, "text": " But most of the time you won't store it that way explicitly because most of the time you", "tokens": [583, 881, 295, 264, 565, 291, 1582, 380, 3531, 309, 300, 636, 20803, 570, 881, 295, 264, 565, 291], "temperature": 0.0, "avg_logprob": -0.0884038589812897, "compression_ratio": 1.7488372093023257, "no_speech_prob": 1.1125292076030746e-05}, {"id": 735, "seek": 334440, "start": 3348.7200000000003, "end": 3355.08, "text": " have what's called a very sparse matrix, which is to say most users haven't watched most", "tokens": [362, 437, 311, 1219, 257, 588, 637, 11668, 8141, 11, 597, 307, 281, 584, 881, 5022, 2378, 380, 6337, 881], "temperature": 0.0, "avg_logprob": -0.0884038589812897, "compression_ratio": 1.7488372093023257, "no_speech_prob": 1.1125292076030746e-05}, {"id": 736, "seek": 334440, "start": 3355.08, "end": 3360.56, "text": " movies or most customers haven't purchased most products.", "tokens": [6233, 420, 881, 4581, 2378, 380, 14734, 881, 3383, 13], "temperature": 0.0, "avg_logprob": -0.0884038589812897, "compression_ratio": 1.7488372093023257, "no_speech_prob": 1.1125292076030746e-05}, {"id": 737, "seek": 334440, "start": 3360.56, "end": 3366.8, "text": " So if you store it as a matrix where every combination of customer and product is a separate", "tokens": [407, 498, 291, 3531, 309, 382, 257, 8141, 689, 633, 6562, 295, 5474, 293, 1674, 307, 257, 4994], "temperature": 0.0, "avg_logprob": -0.0884038589812897, "compression_ratio": 1.7488372093023257, "no_speech_prob": 1.1125292076030746e-05}, {"id": 738, "seek": 334440, "start": 3366.8, "end": 3369.32, "text": " cell in that matrix, it's going to be enormous.", "tokens": [2815, 294, 300, 8141, 11, 309, 311, 516, 281, 312, 11322, 13], "temperature": 0.0, "avg_logprob": -0.0884038589812897, "compression_ratio": 1.7488372093023257, "no_speech_prob": 1.1125292076030746e-05}, {"id": 739, "seek": 336932, "start": 3369.32, "end": 3375.84, "text": " So you tend to store it like this or you can store it as a matrix using some kind of special", "tokens": [407, 291, 3928, 281, 3531, 309, 411, 341, 420, 291, 393, 3531, 309, 382, 257, 8141, 1228, 512, 733, 295, 2121], "temperature": 0.0, "avg_logprob": -0.12100776518234099, "compression_ratio": 1.653061224489796, "no_speech_prob": 7.411012575175846e-06}, {"id": 740, "seek": 336932, "start": 3375.84, "end": 3378.6400000000003, "text": " sparse matrix format.", "tokens": [637, 11668, 8141, 7877, 13], "temperature": 0.0, "avg_logprob": -0.12100776518234099, "compression_ratio": 1.653061224489796, "no_speech_prob": 7.411012575175846e-06}, {"id": 741, "seek": 336932, "start": 3378.6400000000003, "end": 3383.0, "text": " And if that sounds interesting, you should check out Rachel's computational linear algebra", "tokens": [400, 498, 300, 3263, 1880, 11, 291, 820, 1520, 484, 14246, 311, 28270, 8213, 21989], "temperature": 0.0, "avg_logprob": -0.12100776518234099, "compression_ratio": 1.653061224489796, "no_speech_prob": 7.411012575175846e-06}, {"id": 742, "seek": 336932, "start": 3383.0, "end": 3388.92, "text": " course on Fast AI where we have lots and lots and lots of information about sparse matrix", "tokens": [1164, 322, 15968, 7318, 689, 321, 362, 3195, 293, 3195, 293, 3195, 295, 1589, 466, 637, 11668, 8141], "temperature": 0.0, "avg_logprob": -0.12100776518234099, "compression_ratio": 1.653061224489796, "no_speech_prob": 7.411012575175846e-06}, {"id": 743, "seek": 336932, "start": 3388.92, "end": 3389.92, "text": " storage approaches.", "tokens": [6725, 11587, 13], "temperature": 0.0, "avg_logprob": -0.12100776518234099, "compression_ratio": 1.653061224489796, "no_speech_prob": 7.411012575175846e-06}, {"id": 744, "seek": 336932, "start": 3389.92, "end": 3398.8, "text": " For now though, we're just going to kind of keep it in this format on the left-hand side.", "tokens": [1171, 586, 1673, 11, 321, 434, 445, 516, 281, 733, 295, 1066, 309, 294, 341, 7877, 322, 264, 1411, 12, 5543, 1252, 13], "temperature": 0.0, "avg_logprob": -0.12100776518234099, "compression_ratio": 1.653061224489796, "no_speech_prob": 7.411012575175846e-06}, {"id": 745, "seek": 339880, "start": 3398.8, "end": 3409.2400000000002, "text": " So for collaborative filtering, there's a really nice dataset called MovieLens created", "tokens": [407, 337, 16555, 30822, 11, 456, 311, 257, 534, 1481, 28872, 1219, 28766, 43, 694, 2942], "temperature": 0.0, "avg_logprob": -0.20195885779152453, "compression_ratio": 1.4423076923076923, "no_speech_prob": 8.530098057235591e-06}, {"id": 746, "seek": 339880, "start": 3409.2400000000002, "end": 3415.4, "text": " by the GroupLens group, very hopefully, and you can download various different sizes,", "tokens": [538, 264, 10500, 43, 694, 1594, 11, 588, 4696, 11, 293, 291, 393, 5484, 3683, 819, 11602, 11], "temperature": 0.0, "avg_logprob": -0.20195885779152453, "compression_ratio": 1.4423076923076923, "no_speech_prob": 8.530098057235591e-06}, {"id": 747, "seek": 339880, "start": 3415.4, "end": 3419.76, "text": " 20 million ratings, 100,000 ratings.", "tokens": [945, 2459, 24603, 11, 2319, 11, 1360, 24603, 13], "temperature": 0.0, "avg_logprob": -0.20195885779152453, "compression_ratio": 1.4423076923076923, "no_speech_prob": 8.530098057235591e-06}, {"id": 748, "seek": 339880, "start": 3419.76, "end": 3424.36, "text": " We've actually created an extra small version for playing around with, which is what we'll", "tokens": [492, 600, 767, 2942, 364, 2857, 1359, 3037, 337, 2433, 926, 365, 11, 597, 307, 437, 321, 603], "temperature": 0.0, "avg_logprob": -0.20195885779152453, "compression_ratio": 1.4423076923076923, "no_speech_prob": 8.530098057235591e-06}, {"id": 749, "seek": 342436, "start": 3424.36, "end": 3433.88, "text": " start with today and then probably next week we'll use the bigger version.", "tokens": [722, 365, 965, 293, 550, 1391, 958, 1243, 321, 603, 764, 264, 3801, 3037, 13], "temperature": 0.0, "avg_logprob": -0.2614346575993364, "compression_ratio": 1.490566037735849, "no_speech_prob": 2.2826929125585593e-05}, {"id": 750, "seek": 342436, "start": 3433.88, "end": 3437.84, "text": " You can grab the small version using urls.ml sample.", "tokens": [509, 393, 4444, 264, 1359, 3037, 1228, 4038, 11784, 13, 15480, 6889, 13], "temperature": 0.0, "avg_logprob": -0.2614346575993364, "compression_ratio": 1.490566037735849, "no_speech_prob": 2.2826929125585593e-05}, {"id": 751, "seek": 342436, "start": 3437.84, "end": 3443.44, "text": " And it's a CSV, so you can read it with pandas.", "tokens": [400, 309, 311, 257, 48814, 11, 370, 291, 393, 1401, 309, 365, 4565, 296, 13], "temperature": 0.0, "avg_logprob": -0.2614346575993364, "compression_ratio": 1.490566037735849, "no_speech_prob": 2.2826929125585593e-05}, {"id": 752, "seek": 342436, "start": 3443.44, "end": 3446.0, "text": " And here it is, right?", "tokens": [400, 510, 309, 307, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2614346575993364, "compression_ratio": 1.490566037735849, "no_speech_prob": 2.2826929125585593e-05}, {"id": 753, "seek": 342436, "start": 3446.0, "end": 3447.56, "text": " It's basically a list of user IDs.", "tokens": [467, 311, 1936, 257, 1329, 295, 4195, 48212, 13], "temperature": 0.0, "avg_logprob": -0.2614346575993364, "compression_ratio": 1.490566037735849, "no_speech_prob": 2.2826929125585593e-05}, {"id": 754, "seek": 342436, "start": 3447.56, "end": 3450.1600000000003, "text": " We don't actually know anything about who these users are.", "tokens": [492, 500, 380, 767, 458, 1340, 466, 567, 613, 5022, 366, 13], "temperature": 0.0, "avg_logprob": -0.2614346575993364, "compression_ratio": 1.490566037735849, "no_speech_prob": 2.2826929125585593e-05}, {"id": 755, "seek": 342436, "start": 3450.1600000000003, "end": 3452.5, "text": " There's some movie IDs.", "tokens": [821, 311, 512, 3169, 48212, 13], "temperature": 0.0, "avg_logprob": -0.2614346575993364, "compression_ratio": 1.490566037735849, "no_speech_prob": 2.2826929125585593e-05}, {"id": 756, "seek": 345250, "start": 3452.5, "end": 3455.32, "text": " There is some information about what the movies are, but we won't look at that until next", "tokens": [821, 307, 512, 1589, 466, 437, 264, 6233, 366, 11, 457, 321, 1582, 380, 574, 412, 300, 1826, 958], "temperature": 0.0, "avg_logprob": -0.13947204957928575, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.0952883712889161e-05}, {"id": 757, "seek": 345250, "start": 3455.32, "end": 3456.68, "text": " week.", "tokens": [1243, 13], "temperature": 0.0, "avg_logprob": -0.13947204957928575, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.0952883712889161e-05}, {"id": 758, "seek": 345250, "start": 3456.68, "end": 3460.8, "text": " And then there's the rating, and then there's the timestamp.", "tokens": [400, 550, 456, 311, 264, 10990, 11, 293, 550, 456, 311, 264, 49108, 1215, 13], "temperature": 0.0, "avg_logprob": -0.13947204957928575, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.0952883712889161e-05}, {"id": 759, "seek": 345250, "start": 3460.8, "end": 3466.4, "text": " We're going to ignore the timestamp for now.", "tokens": [492, 434, 516, 281, 11200, 264, 49108, 1215, 337, 586, 13], "temperature": 0.0, "avg_logprob": -0.13947204957928575, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.0952883712889161e-05}, {"id": 760, "seek": 345250, "start": 3466.4, "end": 3469.34, "text": " So that's a subset of our data, that's the head.", "tokens": [407, 300, 311, 257, 25993, 295, 527, 1412, 11, 300, 311, 264, 1378, 13], "temperature": 0.0, "avg_logprob": -0.13947204957928575, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.0952883712889161e-05}, {"id": 761, "seek": 345250, "start": 3469.34, "end": 3474.82, "text": " So the head in pandas is just the first few rows.", "tokens": [407, 264, 1378, 294, 4565, 296, 307, 445, 264, 700, 1326, 13241, 13], "temperature": 0.0, "avg_logprob": -0.13947204957928575, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.0952883712889161e-05}, {"id": 762, "seek": 345250, "start": 3474.82, "end": 3480.2, "text": " So now that we've got a data frame, the nice thing about collaborative filtering is it's", "tokens": [407, 586, 300, 321, 600, 658, 257, 1412, 3920, 11, 264, 1481, 551, 466, 16555, 30822, 307, 309, 311], "temperature": 0.0, "avg_logprob": -0.13947204957928575, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.0952883712889161e-05}, {"id": 763, "seek": 345250, "start": 3480.2, "end": 3481.2, "text": " incredibly simple.", "tokens": [6252, 2199, 13], "temperature": 0.0, "avg_logprob": -0.13947204957928575, "compression_ratio": 1.7142857142857142, "no_speech_prob": 1.0952883712889161e-05}, {"id": 764, "seek": 348120, "start": 3481.2, "end": 3483.3599999999997, "text": " That's all the data that we need.", "tokens": [663, 311, 439, 264, 1412, 300, 321, 643, 13], "temperature": 0.0, "avg_logprob": -0.14052977928748497, "compression_ratio": 1.8222222222222222, "no_speech_prob": 2.2473383069154806e-05}, {"id": 765, "seek": 348120, "start": 3483.3599999999997, "end": 3489.3599999999997, "text": " So you can now go ahead and say get collaborative learner, and you can actually just pass in", "tokens": [407, 291, 393, 586, 352, 2286, 293, 584, 483, 16555, 33347, 11, 293, 291, 393, 767, 445, 1320, 294], "temperature": 0.0, "avg_logprob": -0.14052977928748497, "compression_ratio": 1.8222222222222222, "no_speech_prob": 2.2473383069154806e-05}, {"id": 766, "seek": 348120, "start": 3489.3599999999997, "end": 3493.72, "text": " the data frame directly.", "tokens": [264, 1412, 3920, 3838, 13], "temperature": 0.0, "avg_logprob": -0.14052977928748497, "compression_ratio": 1.8222222222222222, "no_speech_prob": 2.2473383069154806e-05}, {"id": 767, "seek": 348120, "start": 3493.72, "end": 3496.8399999999997, "text": " The architecture, you have to tell it how many factors you want to use, and we're going", "tokens": [440, 9482, 11, 291, 362, 281, 980, 309, 577, 867, 6771, 291, 528, 281, 764, 11, 293, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.14052977928748497, "compression_ratio": 1.8222222222222222, "no_speech_prob": 2.2473383069154806e-05}, {"id": 768, "seek": 348120, "start": 3496.8399999999997, "end": 3500.08, "text": " to learn what that means after the break.", "tokens": [281, 1466, 437, 300, 1355, 934, 264, 1821, 13], "temperature": 0.0, "avg_logprob": -0.14052977928748497, "compression_ratio": 1.8222222222222222, "no_speech_prob": 2.2473383069154806e-05}, {"id": 769, "seek": 348120, "start": 3500.08, "end": 3504.2799999999997, "text": " And then something that can be helpful is to tell it what the range of scores are, and", "tokens": [400, 550, 746, 300, 393, 312, 4961, 307, 281, 980, 309, 437, 264, 3613, 295, 13444, 366, 11, 293], "temperature": 0.0, "avg_logprob": -0.14052977928748497, "compression_ratio": 1.8222222222222222, "no_speech_prob": 2.2473383069154806e-05}, {"id": 770, "seek": 348120, "start": 3504.2799999999997, "end": 3506.72, "text": " we're going to see how that helps after the break as well.", "tokens": [321, 434, 516, 281, 536, 577, 300, 3665, 934, 264, 1821, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.14052977928748497, "compression_ratio": 1.8222222222222222, "no_speech_prob": 2.2473383069154806e-05}, {"id": 771, "seek": 348120, "start": 3506.72, "end": 3510.56, "text": " So in this case, the minimum score is 0, the maximum score is 5.", "tokens": [407, 294, 341, 1389, 11, 264, 7285, 6175, 307, 1958, 11, 264, 6674, 6175, 307, 1025, 13], "temperature": 0.0, "avg_logprob": -0.14052977928748497, "compression_ratio": 1.8222222222222222, "no_speech_prob": 2.2473383069154806e-05}, {"id": 772, "seek": 351056, "start": 3510.56, "end": 3518.52, "text": " So now that you've got a learner, you can go ahead and call fit one cycle.", "tokens": [407, 586, 300, 291, 600, 658, 257, 33347, 11, 291, 393, 352, 2286, 293, 818, 3318, 472, 6586, 13], "temperature": 0.0, "avg_logprob": -0.1383449681599935, "compression_ratio": 1.5470588235294118, "no_speech_prob": 5.682368737325305e-06}, {"id": 773, "seek": 351056, "start": 3518.52, "end": 3521.7, "text": " And transfer a few epochs, and there it is.", "tokens": [400, 5003, 257, 1326, 30992, 28346, 11, 293, 456, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1383449681599935, "compression_ratio": 1.5470588235294118, "no_speech_prob": 5.682368737325305e-06}, {"id": 774, "seek": 351056, "start": 3521.7, "end": 3529.08, "text": " So at the end of it, you now have something where you can pick a user ID and a movie ID", "tokens": [407, 412, 264, 917, 295, 309, 11, 291, 586, 362, 746, 689, 291, 393, 1888, 257, 4195, 7348, 293, 257, 3169, 7348], "temperature": 0.0, "avg_logprob": -0.1383449681599935, "compression_ratio": 1.5470588235294118, "no_speech_prob": 5.682368737325305e-06}, {"id": 775, "seek": 351056, "start": 3529.08, "end": 3534.08, "text": " and guess whether or not that user will like that movie.", "tokens": [293, 2041, 1968, 420, 406, 300, 4195, 486, 411, 300, 3169, 13], "temperature": 0.0, "avg_logprob": -0.1383449681599935, "compression_ratio": 1.5470588235294118, "no_speech_prob": 5.682368737325305e-06}, {"id": 776, "seek": 353408, "start": 3534.08, "end": 3541.52, "text": " So this is obviously a super useful application that a lot of you are probably going to try", "tokens": [407, 341, 307, 2745, 257, 1687, 4420, 3861, 300, 257, 688, 295, 291, 366, 1391, 516, 281, 853], "temperature": 0.0, "avg_logprob": -0.13616724809010824, "compression_ratio": 1.73828125, "no_speech_prob": 2.368779178141267e-06}, {"id": 777, "seek": 353408, "start": 3541.52, "end": 3543.36, "text": " over the week in past classes.", "tokens": [670, 264, 1243, 294, 1791, 5359, 13], "temperature": 0.0, "avg_logprob": -0.13616724809010824, "compression_ratio": 1.73828125, "no_speech_prob": 2.368779178141267e-06}, {"id": 778, "seek": 353408, "start": 3543.36, "end": 3549.3199999999997, "text": " A lot of people have taken this collaborative filtering approach back to their workplaces", "tokens": [316, 688, 295, 561, 362, 2726, 341, 16555, 30822, 3109, 646, 281, 641, 589, 34840], "temperature": 0.0, "avg_logprob": -0.13616724809010824, "compression_ratio": 1.73828125, "no_speech_prob": 2.368779178141267e-06}, {"id": 779, "seek": 353408, "start": 3549.3199999999997, "end": 3554.2999999999997, "text": " and discovered that using it in practice is much more tricky than this because in practice", "tokens": [293, 6941, 300, 1228, 309, 294, 3124, 307, 709, 544, 12414, 813, 341, 570, 294, 3124], "temperature": 0.0, "avg_logprob": -0.13616724809010824, "compression_ratio": 1.73828125, "no_speech_prob": 2.368779178141267e-06}, {"id": 780, "seek": 353408, "start": 3554.2999999999997, "end": 3557.14, "text": " you have something called the cold start problem.", "tokens": [291, 362, 746, 1219, 264, 3554, 722, 1154, 13], "temperature": 0.0, "avg_logprob": -0.13616724809010824, "compression_ratio": 1.73828125, "no_speech_prob": 2.368779178141267e-06}, {"id": 781, "seek": 353408, "start": 3557.14, "end": 3562.84, "text": " So the cold start problem is that the time you particularly want to be good at recommending", "tokens": [407, 264, 3554, 722, 1154, 307, 300, 264, 565, 291, 4098, 528, 281, 312, 665, 412, 30559], "temperature": 0.0, "avg_logprob": -0.13616724809010824, "compression_ratio": 1.73828125, "no_speech_prob": 2.368779178141267e-06}, {"id": 782, "seek": 356284, "start": 3562.84, "end": 3568.36, "text": " movies is when you have a new user, and the time you particularly care about recommending", "tokens": [6233, 307, 562, 291, 362, 257, 777, 4195, 11, 293, 264, 565, 291, 4098, 1127, 466, 30559], "temperature": 0.0, "avg_logprob": -0.13445269967627338, "compression_ratio": 1.8224637681159421, "no_speech_prob": 1.1300482583465055e-05}, {"id": 783, "seek": 356284, "start": 3568.36, "end": 3571.08, "text": " a movie is when it's a new movie.", "tokens": [257, 3169, 307, 562, 309, 311, 257, 777, 3169, 13], "temperature": 0.0, "avg_logprob": -0.13445269967627338, "compression_ratio": 1.8224637681159421, "no_speech_prob": 1.1300482583465055e-05}, {"id": 784, "seek": 356284, "start": 3571.08, "end": 3574.6400000000003, "text": " But at that point you don't have any data in your collaborative filtering system and", "tokens": [583, 412, 300, 935, 291, 500, 380, 362, 604, 1412, 294, 428, 16555, 30822, 1185, 293], "temperature": 0.0, "avg_logprob": -0.13445269967627338, "compression_ratio": 1.8224637681159421, "no_speech_prob": 1.1300482583465055e-05}, {"id": 785, "seek": 356284, "start": 3574.6400000000003, "end": 3578.3, "text": " it's really hard.", "tokens": [309, 311, 534, 1152, 13], "temperature": 0.0, "avg_logprob": -0.13445269967627338, "compression_ratio": 1.8224637681159421, "no_speech_prob": 1.1300482583465055e-05}, {"id": 786, "seek": 356284, "start": 3578.3, "end": 3582.84, "text": " As I say this, we don't currently have anything built into FastAI to handle the cold start", "tokens": [1018, 286, 584, 341, 11, 321, 500, 380, 4362, 362, 1340, 3094, 666, 15968, 48698, 281, 4813, 264, 3554, 722], "temperature": 0.0, "avg_logprob": -0.13445269967627338, "compression_ratio": 1.8224637681159421, "no_speech_prob": 1.1300482583465055e-05}, {"id": 787, "seek": 356284, "start": 3582.84, "end": 3587.32, "text": " problem, and that's really because the cold start problem, the only way I know of to solve", "tokens": [1154, 11, 293, 300, 311, 534, 570, 264, 3554, 722, 1154, 11, 264, 787, 636, 286, 458, 295, 281, 5039], "temperature": 0.0, "avg_logprob": -0.13445269967627338, "compression_ratio": 1.8224637681159421, "no_speech_prob": 1.1300482583465055e-05}, {"id": 788, "seek": 356284, "start": 3587.32, "end": 3592.02, "text": " it, in fact the only way I think that conceptually you can solve it, is to have a second model", "tokens": [309, 11, 294, 1186, 264, 787, 636, 286, 519, 300, 3410, 671, 291, 393, 5039, 309, 11, 307, 281, 362, 257, 1150, 2316], "temperature": 0.0, "avg_logprob": -0.13445269967627338, "compression_ratio": 1.8224637681159421, "no_speech_prob": 1.1300482583465055e-05}, {"id": 789, "seek": 359202, "start": 3592.02, "end": 3597.66, "text": " which is not a collaborative filtering model, but a metadata driven model for new users", "tokens": [597, 307, 406, 257, 16555, 30822, 2316, 11, 457, 257, 26603, 9555, 2316, 337, 777, 5022], "temperature": 0.0, "avg_logprob": -0.14270521976329661, "compression_ratio": 1.8076923076923077, "no_speech_prob": 8.013359547476284e-06}, {"id": 790, "seek": 359202, "start": 3597.66, "end": 3601.62, "text": " or new movies.", "tokens": [420, 777, 6233, 13], "temperature": 0.0, "avg_logprob": -0.14270521976329661, "compression_ratio": 1.8076923076923077, "no_speech_prob": 8.013359547476284e-06}, {"id": 791, "seek": 359202, "start": 3601.62, "end": 3604.98, "text": " I don't know if Netflix still does this, but certainly what they used to do when I signed", "tokens": [286, 500, 380, 458, 498, 12778, 920, 775, 341, 11, 457, 3297, 437, 436, 1143, 281, 360, 562, 286, 8175], "temperature": 0.0, "avg_logprob": -0.14270521976329661, "compression_ratio": 1.8076923076923077, "no_speech_prob": 8.013359547476284e-06}, {"id": 792, "seek": 359202, "start": 3604.98, "end": 3610.16, "text": " up to Netflix was they started showing me lots of movies and saying have you seen this,", "tokens": [493, 281, 12778, 390, 436, 1409, 4099, 385, 3195, 295, 6233, 293, 1566, 362, 291, 1612, 341, 11], "temperature": 0.0, "avg_logprob": -0.14270521976329661, "compression_ratio": 1.8076923076923077, "no_speech_prob": 8.013359547476284e-06}, {"id": 793, "seek": 359202, "start": 3610.16, "end": 3613.7599999999998, "text": " did you like it, have you seen this, did you like it.", "tokens": [630, 291, 411, 309, 11, 362, 291, 1612, 341, 11, 630, 291, 411, 309, 13], "temperature": 0.0, "avg_logprob": -0.14270521976329661, "compression_ratio": 1.8076923076923077, "no_speech_prob": 8.013359547476284e-06}, {"id": 794, "seek": 359202, "start": 3613.7599999999998, "end": 3616.92, "text": " So they fixed the cold start problem through the UX.", "tokens": [407, 436, 6806, 264, 3554, 722, 1154, 807, 264, 40176, 13], "temperature": 0.0, "avg_logprob": -0.14270521976329661, "compression_ratio": 1.8076923076923077, "no_speech_prob": 8.013359547476284e-06}, {"id": 795, "seek": 359202, "start": 3616.92, "end": 3619.2, "text": " So there was no cold start problem.", "tokens": [407, 456, 390, 572, 3554, 722, 1154, 13], "temperature": 0.0, "avg_logprob": -0.14270521976329661, "compression_ratio": 1.8076923076923077, "no_speech_prob": 8.013359547476284e-06}, {"id": 796, "seek": 361920, "start": 3619.2, "end": 3624.0, "text": " They found like 20 really common movies and asked me if I liked them.", "tokens": [814, 1352, 411, 945, 534, 2689, 6233, 293, 2351, 385, 498, 286, 4501, 552, 13], "temperature": 0.0, "avg_logprob": -0.13841409198308396, "compression_ratio": 1.779527559055118, "no_speech_prob": 4.784997145179659e-06}, {"id": 797, "seek": 361920, "start": 3624.0, "end": 3628.56, "text": " They used my replies to those 20 to show me 20 more that I might have seen, and by the", "tokens": [814, 1143, 452, 42289, 281, 729, 945, 281, 855, 385, 945, 544, 300, 286, 1062, 362, 1612, 11, 293, 538, 264], "temperature": 0.0, "avg_logprob": -0.13841409198308396, "compression_ratio": 1.779527559055118, "no_speech_prob": 4.784997145179659e-06}, {"id": 798, "seek": 361920, "start": 3628.56, "end": 3634.56, "text": " time I had gone through 60, there was no cold start problem anymore.", "tokens": [565, 286, 632, 2780, 807, 4060, 11, 456, 390, 572, 3554, 722, 1154, 3602, 13], "temperature": 0.0, "avg_logprob": -0.13841409198308396, "compression_ratio": 1.779527559055118, "no_speech_prob": 4.784997145179659e-06}, {"id": 799, "seek": 361920, "start": 3634.56, "end": 3639.3199999999997, "text": " And for new movies, it's not really a problem because like the first 100 users who haven't", "tokens": [400, 337, 777, 6233, 11, 309, 311, 406, 534, 257, 1154, 570, 411, 264, 700, 2319, 5022, 567, 2378, 380], "temperature": 0.0, "avg_logprob": -0.13841409198308396, "compression_ratio": 1.779527559055118, "no_speech_prob": 4.784997145179659e-06}, {"id": 800, "seek": 361920, "start": 3639.3199999999997, "end": 3644.6, "text": " seen the movie go in and say whether they liked it, and then the next 100,000, the next", "tokens": [1612, 264, 3169, 352, 294, 293, 584, 1968, 436, 4501, 309, 11, 293, 550, 264, 958, 2319, 11, 1360, 11, 264, 958], "temperature": 0.0, "avg_logprob": -0.13841409198308396, "compression_ratio": 1.779527559055118, "no_speech_prob": 4.784997145179659e-06}, {"id": 801, "seek": 361920, "start": 3644.6, "end": 3648.52, "text": " million, it's not a cold start problem anymore.", "tokens": [2459, 11, 309, 311, 406, 257, 3554, 722, 1154, 3602, 13], "temperature": 0.0, "avg_logprob": -0.13841409198308396, "compression_ratio": 1.779527559055118, "no_speech_prob": 4.784997145179659e-06}, {"id": 802, "seek": 364852, "start": 3648.52, "end": 3654.2, "text": " But the other thing you can do if you for whatever reason can't go through that UX of", "tokens": [583, 264, 661, 551, 291, 393, 360, 498, 291, 337, 2035, 1778, 393, 380, 352, 807, 300, 40176, 295], "temperature": 0.0, "avg_logprob": -0.1829529281490105, "compression_ratio": 1.7667844522968197, "no_speech_prob": 2.468197089910973e-05}, {"id": 803, "seek": 364852, "start": 3654.2, "end": 3655.96, "text": " asking people did you like those things.", "tokens": [3365, 561, 630, 291, 411, 729, 721, 13], "temperature": 0.0, "avg_logprob": -0.1829529281490105, "compression_ratio": 1.7667844522968197, "no_speech_prob": 2.468197089910973e-05}, {"id": 804, "seek": 364852, "start": 3655.96, "end": 3660.54, "text": " So for example, if you're selling products and you don't really want to show them a big", "tokens": [407, 337, 1365, 11, 498, 291, 434, 6511, 3383, 293, 291, 500, 380, 534, 528, 281, 855, 552, 257, 955], "temperature": 0.0, "avg_logprob": -0.1829529281490105, "compression_ratio": 1.7667844522968197, "no_speech_prob": 2.468197089910973e-05}, {"id": 805, "seek": 364852, "start": 3660.54, "end": 3664.84, "text": " selection of your products and say did you like this because you just want them to buy,", "tokens": [9450, 295, 428, 3383, 293, 584, 630, 291, 411, 341, 570, 291, 445, 528, 552, 281, 2256, 11], "temperature": 0.0, "avg_logprob": -0.1829529281490105, "compression_ratio": 1.7667844522968197, "no_speech_prob": 2.468197089910973e-05}, {"id": 806, "seek": 364852, "start": 3664.84, "end": 3671.24, "text": " you can instead try and use a metadata based tabular model, what geography did they come", "tokens": [291, 393, 2602, 853, 293, 764, 257, 26603, 2361, 4421, 1040, 2316, 11, 437, 26695, 630, 436, 808], "temperature": 0.0, "avg_logprob": -0.1829529281490105, "compression_ratio": 1.7667844522968197, "no_speech_prob": 2.468197089910973e-05}, {"id": 807, "seek": 364852, "start": 3671.24, "end": 3675.88, "text": " from, maybe you know their age and sex, you can try and make some guesses about the initial", "tokens": [490, 11, 1310, 291, 458, 641, 3205, 293, 3260, 11, 291, 393, 853, 293, 652, 512, 42703, 466, 264, 5883], "temperature": 0.0, "avg_logprob": -0.1829529281490105, "compression_ratio": 1.7667844522968197, "no_speech_prob": 2.468197089910973e-05}, {"id": 808, "seek": 364852, "start": 3675.88, "end": 3677.72, "text": " recommendations.", "tokens": [10434, 13], "temperature": 0.0, "avg_logprob": -0.1829529281490105, "compression_ratio": 1.7667844522968197, "no_speech_prob": 2.468197089910973e-05}, {"id": 809, "seek": 367772, "start": 3677.72, "end": 3683.7999999999997, "text": " So collaborative filtering is specifically for once you have a bit of information about", "tokens": [407, 16555, 30822, 307, 4682, 337, 1564, 291, 362, 257, 857, 295, 1589, 466], "temperature": 0.0, "avg_logprob": -0.20491433577104048, "compression_ratio": 1.4195402298850575, "no_speech_prob": 6.961767667235108e-06}, {"id": 810, "seek": 367772, "start": 3683.7999999999997, "end": 3690.16, "text": " your users and movies or customers and products or whatever.", "tokens": [428, 5022, 293, 6233, 420, 4581, 293, 3383, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.20491433577104048, "compression_ratio": 1.4195402298850575, "no_speech_prob": 6.961767667235108e-06}, {"id": 811, "seek": 367772, "start": 3690.16, "end": 3697.7, "text": " Yeah, okay.", "tokens": [865, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.20491433577104048, "compression_ratio": 1.4195402298850575, "no_speech_prob": 6.961767667235108e-06}, {"id": 812, "seek": 367772, "start": 3697.7, "end": 3702.52, "text": " How does the language model trained in this manner perform on code switch data such as", "tokens": [1012, 775, 264, 2856, 2316, 8895, 294, 341, 9060, 2042, 322, 3089, 3679, 1412, 1270, 382], "temperature": 0.0, "avg_logprob": -0.20491433577104048, "compression_ratio": 1.4195402298850575, "no_speech_prob": 6.961767667235108e-06}, {"id": 813, "seek": 370252, "start": 3702.52, "end": 3708.12, "text": " Hindi written in English words or text with a lot of emojis?", "tokens": [36225, 3720, 294, 3669, 2283, 420, 2487, 365, 257, 688, 295, 19611, 40371, 30], "temperature": 0.0, "avg_logprob": -0.22548085530598957, "compression_ratio": 1.5406976744186047, "no_speech_prob": 1.593584966030903e-05}, {"id": 814, "seek": 370252, "start": 3708.12, "end": 3710.12, "text": " And then do you want the second question?", "tokens": [400, 550, 360, 291, 528, 264, 1150, 1168, 30], "temperature": 0.0, "avg_logprob": -0.22548085530598957, "compression_ratio": 1.5406976744186047, "no_speech_prob": 1.593584966030903e-05}, {"id": 815, "seek": 370252, "start": 3710.12, "end": 3714.24, "text": " Yeah, that's a good question.", "tokens": [865, 11, 300, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.22548085530598957, "compression_ratio": 1.5406976744186047, "no_speech_prob": 1.593584966030903e-05}, {"id": 816, "seek": 370252, "start": 3714.24, "end": 3722.48, "text": " So text with emojis, it'll be fine.", "tokens": [407, 2487, 365, 19611, 40371, 11, 309, 603, 312, 2489, 13], "temperature": 0.0, "avg_logprob": -0.22548085530598957, "compression_ratio": 1.5406976744186047, "no_speech_prob": 1.593584966030903e-05}, {"id": 817, "seek": 370252, "start": 3722.48, "end": 3730.04, "text": " There's not many emojis in Wikipedia and where they are in Wikipedia, it's more like a Wikipedia", "tokens": [821, 311, 406, 867, 19611, 40371, 294, 28999, 293, 689, 436, 366, 294, 28999, 11, 309, 311, 544, 411, 257, 28999], "temperature": 0.0, "avg_logprob": -0.22548085530598957, "compression_ratio": 1.5406976744186047, "no_speech_prob": 1.593584966030903e-05}, {"id": 818, "seek": 373004, "start": 3730.04, "end": 3735.48, "text": " page about the emoji rather than the emoji being used in a sensible place.", "tokens": [3028, 466, 264, 31595, 2831, 813, 264, 31595, 885, 1143, 294, 257, 25380, 1081, 13], "temperature": 0.0, "avg_logprob": -0.1673834618557705, "compression_ratio": 1.6650943396226414, "no_speech_prob": 1.1300455298624001e-05}, {"id": 819, "seek": 373004, "start": 3735.48, "end": 3746.12, "text": " But you can and should do this language model fine-tuning where you take a corpus of text", "tokens": [583, 291, 393, 293, 820, 360, 341, 2856, 2316, 2489, 12, 83, 37726, 689, 291, 747, 257, 1181, 31624, 295, 2487], "temperature": 0.0, "avg_logprob": -0.1673834618557705, "compression_ratio": 1.6650943396226414, "no_speech_prob": 1.1300455298624001e-05}, {"id": 820, "seek": 373004, "start": 3746.12, "end": 3751.04, "text": " where people are using emojis in usual ways and so you fine-tune the Wiki text language", "tokens": [689, 561, 366, 1228, 19611, 40371, 294, 7713, 2098, 293, 370, 291, 2489, 12, 83, 2613, 264, 35892, 2487, 2856], "temperature": 0.0, "avg_logprob": -0.1673834618557705, "compression_ratio": 1.6650943396226414, "no_speech_prob": 1.1300455298624001e-05}, {"id": 821, "seek": 373004, "start": 3751.04, "end": 3756.2799999999997, "text": " model to your Reddit or Twitter or whatever language model and there aren't that many", "tokens": [2316, 281, 428, 32210, 420, 5794, 420, 2035, 2856, 2316, 293, 456, 3212, 380, 300, 867], "temperature": 0.0, "avg_logprob": -0.1673834618557705, "compression_ratio": 1.6650943396226414, "no_speech_prob": 1.1300455298624001e-05}, {"id": 822, "seek": 373004, "start": 3756.2799999999997, "end": 3757.2799999999997, "text": " emojis, right?", "tokens": [19611, 40371, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1673834618557705, "compression_ratio": 1.6650943396226414, "no_speech_prob": 1.1300455298624001e-05}, {"id": 823, "seek": 375728, "start": 3757.28, "end": 3761.52, "text": " So if you think about it, there's like hundreds of thousands of possible words that people", "tokens": [407, 498, 291, 519, 466, 309, 11, 456, 311, 411, 6779, 295, 5383, 295, 1944, 2283, 300, 561], "temperature": 0.0, "avg_logprob": -0.12474388446447984, "compression_ratio": 1.6558704453441295, "no_speech_prob": 1.863089892140124e-05}, {"id": 824, "seek": 375728, "start": 3761.52, "end": 3764.4, "text": " can be using but a small number of possible emojis.", "tokens": [393, 312, 1228, 457, 257, 1359, 1230, 295, 1944, 19611, 40371, 13], "temperature": 0.0, "avg_logprob": -0.12474388446447984, "compression_ratio": 1.6558704453441295, "no_speech_prob": 1.863089892140124e-05}, {"id": 825, "seek": 375728, "start": 3764.4, "end": 3768.96, "text": " So it'll very quickly learn how those emojis are being used.", "tokens": [407, 309, 603, 588, 2661, 1466, 577, 729, 19611, 40371, 366, 885, 1143, 13], "temperature": 0.0, "avg_logprob": -0.12474388446447984, "compression_ratio": 1.6558704453441295, "no_speech_prob": 1.863089892140124e-05}, {"id": 826, "seek": 375728, "start": 3768.96, "end": 3775.1600000000003, "text": " So that's a piece of cake.", "tokens": [407, 300, 311, 257, 2522, 295, 5908, 13], "temperature": 0.0, "avg_logprob": -0.12474388446447984, "compression_ratio": 1.6558704453441295, "no_speech_prob": 1.863089892140124e-05}, {"id": 827, "seek": 375728, "start": 3775.1600000000003, "end": 3778.2400000000002, "text": " So I'm not very familiar with Hindi but I'll take an example I'm very familiar with which", "tokens": [407, 286, 478, 406, 588, 4963, 365, 36225, 457, 286, 603, 747, 364, 1365, 286, 478, 588, 4963, 365, 597], "temperature": 0.0, "avg_logprob": -0.12474388446447984, "compression_ratio": 1.6558704453441295, "no_speech_prob": 1.863089892140124e-05}, {"id": 828, "seek": 375728, "start": 3778.2400000000002, "end": 3779.8, "text": " is Mandarin.", "tokens": [307, 42292, 13], "temperature": 0.0, "avg_logprob": -0.12474388446447984, "compression_ratio": 1.6558704453441295, "no_speech_prob": 1.863089892140124e-05}, {"id": 829, "seek": 375728, "start": 3779.8, "end": 3785.1600000000003, "text": " In Mandarin, you could have a model that's trained with Chinese characters.", "tokens": [682, 42292, 11, 291, 727, 362, 257, 2316, 300, 311, 8895, 365, 4649, 4342, 13], "temperature": 0.0, "avg_logprob": -0.12474388446447984, "compression_ratio": 1.6558704453441295, "no_speech_prob": 1.863089892140124e-05}, {"id": 830, "seek": 378516, "start": 3785.16, "end": 3790.7599999999998, "text": " So there's kind of 5 or 6,000 Chinese characters in common use but there's also a romanization", "tokens": [407, 456, 311, 733, 295, 1025, 420, 1386, 11, 1360, 4649, 4342, 294, 2689, 764, 457, 456, 311, 611, 257, 41362, 2144], "temperature": 0.0, "avg_logprob": -0.15070745056750728, "compression_ratio": 1.9276018099547512, "no_speech_prob": 2.5861570975393988e-05}, {"id": 831, "seek": 378516, "start": 3790.7599999999998, "end": 3793.3199999999997, "text": " of those characters called pinion.", "tokens": [295, 729, 4342, 1219, 5447, 313, 13], "temperature": 0.0, "avg_logprob": -0.15070745056750728, "compression_ratio": 1.9276018099547512, "no_speech_prob": 2.5861570975393988e-05}, {"id": 832, "seek": 378516, "start": 3793.3199999999997, "end": 3800.2999999999997, "text": " And it's a bit tricky because although there's a nearly direct mapping from the character", "tokens": [400, 309, 311, 257, 857, 12414, 570, 4878, 456, 311, 257, 6217, 2047, 18350, 490, 264, 2517], "temperature": 0.0, "avg_logprob": -0.15070745056750728, "compression_ratio": 1.9276018099547512, "no_speech_prob": 2.5861570975393988e-05}, {"id": 833, "seek": 378516, "start": 3800.2999999999997, "end": 3805.52, "text": " to the pinion, I mean there is a direct mapping, the pronunciation is not exactly direct, there", "tokens": [281, 264, 5447, 313, 11, 286, 914, 456, 307, 257, 2047, 18350, 11, 264, 23338, 307, 406, 2293, 2047, 11, 456], "temperature": 0.0, "avg_logprob": -0.15070745056750728, "compression_ratio": 1.9276018099547512, "no_speech_prob": 2.5861570975393988e-05}, {"id": 834, "seek": 378516, "start": 3805.52, "end": 3812.12, "text": " isn't a direct mapping from the pinion to the character because one pinion corresponds", "tokens": [1943, 380, 257, 2047, 18350, 490, 264, 5447, 313, 281, 264, 2517, 570, 472, 5447, 313, 23249], "temperature": 0.0, "avg_logprob": -0.15070745056750728, "compression_ratio": 1.9276018099547512, "no_speech_prob": 2.5861570975393988e-05}, {"id": 835, "seek": 378516, "start": 3812.12, "end": 3814.2799999999997, "text": " to multiple characters.", "tokens": [281, 3866, 4342, 13], "temperature": 0.0, "avg_logprob": -0.15070745056750728, "compression_ratio": 1.9276018099547512, "no_speech_prob": 2.5861570975393988e-05}, {"id": 836, "seek": 381428, "start": 3814.28, "end": 3824.36, "text": " So the first thing to note is that if you're going to use this approach for Chinese, you", "tokens": [407, 264, 700, 551, 281, 3637, 307, 300, 498, 291, 434, 516, 281, 764, 341, 3109, 337, 4649, 11, 291], "temperature": 0.0, "avg_logprob": -0.11648152215140206, "compression_ratio": 1.679144385026738, "no_speech_prob": 1.7501553884358145e-05}, {"id": 837, "seek": 381428, "start": 3824.36, "end": 3827.76, "text": " would need to start with a Chinese language model.", "tokens": [576, 643, 281, 722, 365, 257, 4649, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11648152215140206, "compression_ratio": 1.679144385026738, "no_speech_prob": 1.7501553884358145e-05}, {"id": 838, "seek": 381428, "start": 3827.76, "end": 3834.48, "text": " So actually Fast.ai has something called a model zoo where we're adding more and more", "tokens": [407, 767, 15968, 13, 1301, 575, 746, 1219, 257, 2316, 25347, 689, 321, 434, 5127, 544, 293, 544], "temperature": 0.0, "avg_logprob": -0.11648152215140206, "compression_ratio": 1.679144385026738, "no_speech_prob": 1.7501553884358145e-05}, {"id": 839, "seek": 381428, "start": 3834.48, "end": 3840.36, "text": " language models for different languages and also increasingly for different domain areas", "tokens": [2856, 5245, 337, 819, 8650, 293, 611, 12980, 337, 819, 9274, 3179], "temperature": 0.0, "avg_logprob": -0.11648152215140206, "compression_ratio": 1.679144385026738, "no_speech_prob": 1.7501553884358145e-05}, {"id": 840, "seek": 384036, "start": 3840.36, "end": 3846.84, "text": " like English medical texts or even language models for things other than NLP like genome", "tokens": [411, 3669, 4625, 15765, 420, 754, 2856, 5245, 337, 721, 661, 813, 426, 45196, 411, 21953], "temperature": 0.0, "avg_logprob": -0.17313354734390501, "compression_ratio": 1.4736842105263157, "no_speech_prob": 3.2375651244365145e-06}, {"id": 841, "seek": 384036, "start": 3846.84, "end": 3853.2400000000002, "text": " sequences, molecular data, musical MIDI notes and so forth.", "tokens": [22978, 11, 19046, 1412, 11, 9165, 41474, 5570, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.17313354734390501, "compression_ratio": 1.4736842105263157, "no_speech_prob": 3.2375651244365145e-06}, {"id": 842, "seek": 384036, "start": 3853.2400000000002, "end": 3857.36, "text": " So you would obviously start there.", "tokens": [407, 291, 576, 2745, 722, 456, 13], "temperature": 0.0, "avg_logprob": -0.17313354734390501, "compression_ratio": 1.4736842105263157, "no_speech_prob": 3.2375651244365145e-06}, {"id": 843, "seek": 384036, "start": 3857.36, "end": 3863.52, "text": " To then convert that, that'll be either simplified or traditional Chinese, to then convert that", "tokens": [1407, 550, 7620, 300, 11, 300, 603, 312, 2139, 26335, 420, 5164, 4649, 11, 281, 550, 7620, 300], "temperature": 0.0, "avg_logprob": -0.17313354734390501, "compression_ratio": 1.4736842105263157, "no_speech_prob": 3.2375651244365145e-06}, {"id": 844, "seek": 386352, "start": 3863.52, "end": 3873.12, "text": " into a, if you want to do pinion, you could either kind of map the vocab directly or as", "tokens": [666, 257, 11, 498, 291, 528, 281, 360, 5447, 313, 11, 291, 727, 2139, 733, 295, 4471, 264, 2329, 455, 3838, 420, 382], "temperature": 0.0, "avg_logprob": -0.15530844223804963, "compression_ratio": 1.5923913043478262, "no_speech_prob": 1.0289068086422049e-05}, {"id": 845, "seek": 386352, "start": 3873.12, "end": 3878.68, "text": " you'll learn, these multi-layer models, it's only the first layer that basically converts", "tokens": [291, 603, 1466, 11, 613, 4825, 12, 8376, 260, 5245, 11, 309, 311, 787, 264, 700, 4583, 300, 1936, 38874], "temperature": 0.0, "avg_logprob": -0.15530844223804963, "compression_ratio": 1.5923913043478262, "no_speech_prob": 1.0289068086422049e-05}, {"id": 846, "seek": 386352, "start": 3878.68, "end": 3884.08, "text": " the tokens into a set of vectors.", "tokens": [264, 22667, 666, 257, 992, 295, 18875, 13], "temperature": 0.0, "avg_logprob": -0.15530844223804963, "compression_ratio": 1.5923913043478262, "no_speech_prob": 1.0289068086422049e-05}, {"id": 847, "seek": 386352, "start": 3884.08, "end": 3890.46, "text": " You can actually throw that away and fine tune just the first layer of the model.", "tokens": [509, 393, 767, 3507, 300, 1314, 293, 2489, 10864, 445, 264, 700, 4583, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.15530844223804963, "compression_ratio": 1.5923913043478262, "no_speech_prob": 1.0289068086422049e-05}, {"id": 848, "seek": 389046, "start": 3890.46, "end": 3895.48, "text": " So that second part is going to require a bit more, a few more weeks of learning before", "tokens": [407, 300, 1150, 644, 307, 516, 281, 3651, 257, 857, 544, 11, 257, 1326, 544, 3259, 295, 2539, 949], "temperature": 0.0, "avg_logprob": -0.13786182598191865, "compression_ratio": 1.5940170940170941, "no_speech_prob": 4.222768438921776e-06}, {"id": 849, "seek": 389046, "start": 3895.48, "end": 3898.16, "text": " you exactly understand how to do that and so forth.", "tokens": [291, 2293, 1223, 577, 281, 360, 300, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.13786182598191865, "compression_ratio": 1.5940170940170941, "no_speech_prob": 4.222768438921776e-06}, {"id": 850, "seek": 389046, "start": 3898.16, "end": 3902.28, "text": " But if it's something you're interested in doing, we can talk about it on the forum because", "tokens": [583, 498, 309, 311, 746, 291, 434, 3102, 294, 884, 11, 321, 393, 751, 466, 309, 322, 264, 17542, 570], "temperature": 0.0, "avg_logprob": -0.13786182598191865, "compression_ratio": 1.5940170940170941, "no_speech_prob": 4.222768438921776e-06}, {"id": 851, "seek": 389046, "start": 3902.28, "end": 3909.28, "text": " it's a kind of a nice test of understanding.", "tokens": [309, 311, 257, 733, 295, 257, 1481, 1500, 295, 3701, 13], "temperature": 0.0, "avg_logprob": -0.13786182598191865, "compression_ratio": 1.5940170940170941, "no_speech_prob": 4.222768438921776e-06}, {"id": 852, "seek": 389046, "start": 3909.28, "end": 3912.48, "text": " So what about time series on tabular data?", "tokens": [407, 437, 466, 565, 2638, 322, 4421, 1040, 1412, 30], "temperature": 0.0, "avg_logprob": -0.13786182598191865, "compression_ratio": 1.5940170940170941, "no_speech_prob": 4.222768438921776e-06}, {"id": 853, "seek": 389046, "start": 3912.48, "end": 3918.86, "text": " Is there an RNN model involved in tabular dot models?", "tokens": [1119, 456, 364, 45702, 45, 2316, 3288, 294, 4421, 1040, 5893, 5245, 30], "temperature": 0.0, "avg_logprob": -0.13786182598191865, "compression_ratio": 1.5940170940170941, "no_speech_prob": 4.222768438921776e-06}, {"id": 854, "seek": 391886, "start": 3918.86, "end": 3925.2000000000003, "text": " So we're going to look at time series tabular data next week.", "tokens": [407, 321, 434, 516, 281, 574, 412, 565, 2638, 4421, 1040, 1412, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.11851680569532441, "compression_ratio": 1.5789473684210527, "no_speech_prob": 4.860383342020214e-06}, {"id": 855, "seek": 391886, "start": 3925.2000000000003, "end": 3932.88, "text": " But the short answer is, generally speaking, you don't use an RNN for time series tabular", "tokens": [583, 264, 2099, 1867, 307, 11, 5101, 4124, 11, 291, 500, 380, 764, 364, 45702, 45, 337, 565, 2638, 4421, 1040], "temperature": 0.0, "avg_logprob": -0.11851680569532441, "compression_ratio": 1.5789473684210527, "no_speech_prob": 4.860383342020214e-06}, {"id": 856, "seek": 391886, "start": 3932.88, "end": 3939.92, "text": " data, but instead you extract a bunch of columns for things like day of week, is it a weekend,", "tokens": [1412, 11, 457, 2602, 291, 8947, 257, 3840, 295, 13766, 337, 721, 411, 786, 295, 1243, 11, 307, 309, 257, 6711, 11], "temperature": 0.0, "avg_logprob": -0.11851680569532441, "compression_ratio": 1.5789473684210527, "no_speech_prob": 4.860383342020214e-06}, {"id": 857, "seek": 391886, "start": 3939.92, "end": 3943.48, "text": " is it a holiday, was the store open, stuff like that.", "tokens": [307, 309, 257, 9960, 11, 390, 264, 3531, 1269, 11, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.11851680569532441, "compression_ratio": 1.5789473684210527, "no_speech_prob": 4.860383342020214e-06}, {"id": 858, "seek": 394348, "start": 3943.48, "end": 3952.2400000000002, "text": " It turns out that adding those extra columns, which you can do somewhat automatically, basically", "tokens": [467, 4523, 484, 300, 5127, 729, 2857, 13766, 11, 597, 291, 393, 360, 8344, 6772, 11, 1936], "temperature": 0.0, "avg_logprob": -0.17115691048758372, "compression_ratio": 1.5463917525773196, "no_speech_prob": 1.7603192645765375e-06}, {"id": 859, "seek": 394348, "start": 3952.2400000000002, "end": 3955.2400000000002, "text": " gives you state of the art results.", "tokens": [2709, 291, 1785, 295, 264, 1523, 3542, 13], "temperature": 0.0, "avg_logprob": -0.17115691048758372, "compression_ratio": 1.5463917525773196, "no_speech_prob": 1.7603192645765375e-06}, {"id": 860, "seek": 394348, "start": 3955.2400000000002, "end": 3963.96, "text": " There are some good uses of RNNs for time series, but not really for these kind of tabular", "tokens": [821, 366, 512, 665, 4960, 295, 45702, 45, 82, 337, 565, 2638, 11, 457, 406, 534, 337, 613, 733, 295, 4421, 1040], "temperature": 0.0, "avg_logprob": -0.17115691048758372, "compression_ratio": 1.5463917525773196, "no_speech_prob": 1.7603192645765375e-06}, {"id": 861, "seek": 394348, "start": 3963.96, "end": 3971.96, "text": " style time series like retail store logistics databases and stuff like that.", "tokens": [3758, 565, 2638, 411, 10800, 3531, 27420, 22380, 293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.17115691048758372, "compression_ratio": 1.5463917525773196, "no_speech_prob": 1.7603192645765375e-06}, {"id": 862, "seek": 397196, "start": 3971.96, "end": 3983.4, "text": " Is there a source to learn more about the cold start problem?", "tokens": [1119, 456, 257, 4009, 281, 1466, 544, 466, 264, 3554, 722, 1154, 30], "temperature": 0.0, "avg_logprob": -0.2179174264272054, "compression_ratio": 1.4066666666666667, "no_speech_prob": 1.1478266060294118e-05}, {"id": 863, "seek": 397196, "start": 3983.4, "end": 3986.48, "text": " I'm going to have to look that up.", "tokens": [286, 478, 516, 281, 362, 281, 574, 300, 493, 13], "temperature": 0.0, "avg_logprob": -0.2179174264272054, "compression_ratio": 1.4066666666666667, "no_speech_prob": 1.1478266060294118e-05}, {"id": 864, "seek": 397196, "start": 3986.48, "end": 3995.32, "text": " If you know a good resource, please mention it on the forums.", "tokens": [759, 291, 458, 257, 665, 7684, 11, 1767, 2152, 309, 322, 264, 26998, 13], "temperature": 0.0, "avg_logprob": -0.2179174264272054, "compression_ratio": 1.4066666666666667, "no_speech_prob": 1.1478266060294118e-05}, {"id": 865, "seek": 397196, "start": 3995.32, "end": 4001.0, "text": " So that is both the break in the middle of lesson 4.", "tokens": [407, 300, 307, 1293, 264, 1821, 294, 264, 2808, 295, 6898, 1017, 13], "temperature": 0.0, "avg_logprob": -0.2179174264272054, "compression_ratio": 1.4066666666666667, "no_speech_prob": 1.1478266060294118e-05}, {"id": 866, "seek": 400100, "start": 4001.0, "end": 4009.04, "text": " It's the halfway point of the course and it's the point at which we have now seen an example", "tokens": [467, 311, 264, 15461, 935, 295, 264, 1164, 293, 309, 311, 264, 935, 412, 597, 321, 362, 586, 1612, 364, 1365], "temperature": 0.0, "avg_logprob": -0.10361927434017784, "compression_ratio": 1.669811320754717, "no_speech_prob": 1.4285237739386503e-05}, {"id": 867, "seek": 400100, "start": 4009.04, "end": 4010.68, "text": " of all the key applications.", "tokens": [295, 439, 264, 2141, 5821, 13], "temperature": 0.0, "avg_logprob": -0.10361927434017784, "compression_ratio": 1.669811320754717, "no_speech_prob": 1.4285237739386503e-05}, {"id": 868, "seek": 400100, "start": 4010.68, "end": 4016.24, "text": " And so the rest of this course is going to be digging deeper into how they actually work", "tokens": [400, 370, 264, 1472, 295, 341, 1164, 307, 516, 281, 312, 17343, 7731, 666, 577, 436, 767, 589], "temperature": 0.0, "avg_logprob": -0.10361927434017784, "compression_ratio": 1.669811320754717, "no_speech_prob": 1.4285237739386503e-05}, {"id": 869, "seek": 400100, "start": 4016.24, "end": 4022.44, "text": " behind the scenes, more of the theory, more of how the code, the source code is written", "tokens": [2261, 264, 8026, 11, 544, 295, 264, 5261, 11, 544, 295, 577, 264, 3089, 11, 264, 4009, 3089, 307, 3720], "temperature": 0.0, "avg_logprob": -0.10361927434017784, "compression_ratio": 1.669811320754717, "no_speech_prob": 1.4285237739386503e-05}, {"id": 870, "seek": 400100, "start": 4022.44, "end": 4023.44, "text": " and so forth.", "tokens": [293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.10361927434017784, "compression_ratio": 1.669811320754717, "no_speech_prob": 1.4285237739386503e-05}, {"id": 871, "seek": 400100, "start": 4023.44, "end": 4028.44, "text": " So it's a good time to have a nice break.", "tokens": [407, 309, 311, 257, 665, 565, 281, 362, 257, 1481, 1821, 13], "temperature": 0.0, "avg_logprob": -0.10361927434017784, "compression_ratio": 1.669811320754717, "no_speech_prob": 1.4285237739386503e-05}, {"id": 872, "seek": 402844, "start": 4028.44, "end": 4039.52, "text": " Come back and furthermore, it's my birthday today, so it's a really special moment.", "tokens": [2492, 646, 293, 3052, 3138, 11, 309, 311, 452, 6154, 965, 11, 370, 309, 311, 257, 534, 2121, 1623, 13], "temperature": 0.0, "avg_logprob": -0.22984444576760996, "compression_ratio": 1.2459016393442623, "no_speech_prob": 5.648973092320375e-05}, {"id": 873, "seek": 402844, "start": 4039.52, "end": 4046.68, "text": " So let's have a break and come back at 5 past 8.", "tokens": [407, 718, 311, 362, 257, 1821, 293, 808, 646, 412, 1025, 1791, 1649, 13], "temperature": 0.0, "avg_logprob": -0.22984444576760996, "compression_ratio": 1.2459016393442623, "no_speech_prob": 5.648973092320375e-05}, {"id": 874, "seek": 402844, "start": 4046.68, "end": 4052.36, "text": " So Microsoft Excel.", "tokens": [407, 8116, 19060, 13], "temperature": 0.0, "avg_logprob": -0.22984444576760996, "compression_ratio": 1.2459016393442623, "no_speech_prob": 5.648973092320375e-05}, {"id": 875, "seek": 405236, "start": 4052.36, "end": 4061.0, "text": " This is one of my favorite ways to explore data and understand models.", "tokens": [639, 307, 472, 295, 452, 2954, 2098, 281, 6839, 1412, 293, 1223, 5245, 13], "temperature": 0.0, "avg_logprob": -0.17045269242252212, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.0001823335769586265}, {"id": 876, "seek": 405236, "start": 4061.0, "end": 4064.52, "text": " I'll make sure I put this in the repo.", "tokens": [286, 603, 652, 988, 286, 829, 341, 294, 264, 49040, 13], "temperature": 0.0, "avg_logprob": -0.17045269242252212, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.0001823335769586265}, {"id": 877, "seek": 405236, "start": 4064.52, "end": 4068.6400000000003, "text": " And actually this one we can probably largely do in Google Sheets.", "tokens": [400, 767, 341, 472, 321, 393, 1391, 11611, 360, 294, 3329, 1240, 1385, 13], "temperature": 0.0, "avg_logprob": -0.17045269242252212, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.0001823335769586265}, {"id": 878, "seek": 405236, "start": 4068.6400000000003, "end": 4073.48, "text": " I've tried to move as much as I can over the last few weeks into Google Sheets, but I just", "tokens": [286, 600, 3031, 281, 1286, 382, 709, 382, 286, 393, 670, 264, 1036, 1326, 3259, 666, 3329, 1240, 1385, 11, 457, 286, 445], "temperature": 0.0, "avg_logprob": -0.17045269242252212, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.0001823335769586265}, {"id": 879, "seek": 405236, "start": 4073.48, "end": 4076.96, "text": " keep finding it's such a terrible product.", "tokens": [1066, 5006, 309, 311, 1270, 257, 6237, 1674, 13], "temperature": 0.0, "avg_logprob": -0.17045269242252212, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.0001823335769586265}, {"id": 880, "seek": 407696, "start": 4076.96, "end": 4085.08, "text": " Please try to find a copy of Microsoft Excel because there's nothing close.", "tokens": [2555, 853, 281, 915, 257, 5055, 295, 8116, 19060, 570, 456, 311, 1825, 1998, 13], "temperature": 0.0, "avg_logprob": -0.24892587661743165, "compression_ratio": 1.511737089201878, "no_speech_prob": 1.8924112737295218e-05}, {"id": 881, "seek": 407696, "start": 4085.08, "end": 4088.92, "text": " I've tried everything.", "tokens": [286, 600, 3031, 1203, 13], "temperature": 0.0, "avg_logprob": -0.24892587661743165, "compression_ratio": 1.511737089201878, "no_speech_prob": 1.8924112737295218e-05}, {"id": 882, "seek": 407696, "start": 4088.92, "end": 4096.64, "text": " Anyway, spreadsheets get a bad rap from people who basically don't know how to use them,", "tokens": [5684, 11, 23651, 1385, 483, 257, 1578, 5099, 490, 561, 567, 1936, 500, 380, 458, 577, 281, 764, 552, 11], "temperature": 0.0, "avg_logprob": -0.24892587661743165, "compression_ratio": 1.511737089201878, "no_speech_prob": 1.8924112737295218e-05}, {"id": 883, "seek": 407696, "start": 4096.64, "end": 4100.24, "text": " just like people who spend their lives on Excel and then they start using Python and", "tokens": [445, 411, 561, 567, 3496, 641, 2909, 322, 19060, 293, 550, 436, 722, 1228, 15329, 293], "temperature": 0.0, "avg_logprob": -0.24892587661743165, "compression_ratio": 1.511737089201878, "no_speech_prob": 1.8924112737295218e-05}, {"id": 884, "seek": 407696, "start": 4100.24, "end": 4103.08, "text": " they're like, what the hell is this stupid thing?", "tokens": [436, 434, 411, 11, 437, 264, 4921, 307, 341, 6631, 551, 30], "temperature": 0.0, "avg_logprob": -0.24892587661743165, "compression_ratio": 1.511737089201878, "no_speech_prob": 1.8924112737295218e-05}, {"id": 885, "seek": 410308, "start": 4103.08, "end": 4108.2, "text": " It takes thousands of hours to get really good at spreadsheets, but a few dozen hours", "tokens": [467, 2516, 5383, 295, 2496, 281, 483, 534, 665, 412, 23651, 1385, 11, 457, 257, 1326, 16654, 2496], "temperature": 0.0, "avg_logprob": -0.11841102849657291, "compression_ratio": 1.7296137339055795, "no_speech_prob": 4.331743184593506e-05}, {"id": 886, "seek": 410308, "start": 4108.2, "end": 4110.36, "text": " to get competent at them.", "tokens": [281, 483, 29998, 412, 552, 13], "temperature": 0.0, "avg_logprob": -0.11841102849657291, "compression_ratio": 1.7296137339055795, "no_speech_prob": 4.331743184593506e-05}, {"id": 887, "seek": 410308, "start": 4110.36, "end": 4114.2, "text": " Once you're competent at them, you can see everything in front of you.", "tokens": [3443, 291, 434, 29998, 412, 552, 11, 291, 393, 536, 1203, 294, 1868, 295, 291, 13], "temperature": 0.0, "avg_logprob": -0.11841102849657291, "compression_ratio": 1.7296137339055795, "no_speech_prob": 4.331743184593506e-05}, {"id": 888, "seek": 410308, "start": 4114.2, "end": 4115.44, "text": " It's all laid out.", "tokens": [467, 311, 439, 9897, 484, 13], "temperature": 0.0, "avg_logprob": -0.11841102849657291, "compression_ratio": 1.7296137339055795, "no_speech_prob": 4.331743184593506e-05}, {"id": 889, "seek": 410308, "start": 4115.44, "end": 4118.5199999999995, "text": " It's really great.", "tokens": [467, 311, 534, 869, 13], "temperature": 0.0, "avg_logprob": -0.11841102849657291, "compression_ratio": 1.7296137339055795, "no_speech_prob": 4.331743184593506e-05}, {"id": 890, "seek": 410308, "start": 4118.5199999999995, "end": 4123.7, "text": " I'll give you one spreadsheet tip today, which is if you hold down the control key or command", "tokens": [286, 603, 976, 291, 472, 27733, 4125, 965, 11, 597, 307, 498, 291, 1797, 760, 264, 1969, 2141, 420, 5622], "temperature": 0.0, "avg_logprob": -0.11841102849657291, "compression_ratio": 1.7296137339055795, "no_speech_prob": 4.331743184593506e-05}, {"id": 891, "seek": 410308, "start": 4123.7, "end": 4129.16, "text": " key on your keyboard and press the arrow keys, here's control right, it takes you to the", "tokens": [2141, 322, 428, 10186, 293, 1886, 264, 11610, 9317, 11, 510, 311, 1969, 558, 11, 309, 2516, 291, 281, 264], "temperature": 0.0, "avg_logprob": -0.11841102849657291, "compression_ratio": 1.7296137339055795, "no_speech_prob": 4.331743184593506e-05}, {"id": 892, "seek": 412916, "start": 4129.16, "end": 4136.32, "text": " end of a block of a table that you're in and it's by far the best way to move around the", "tokens": [917, 295, 257, 3461, 295, 257, 3199, 300, 291, 434, 294, 293, 309, 311, 538, 1400, 264, 1151, 636, 281, 1286, 926, 264], "temperature": 0.0, "avg_logprob": -0.18624411559686427, "compression_ratio": 1.698224852071006, "no_speech_prob": 2.3922595573822036e-05}, {"id": 893, "seek": 412916, "start": 4136.32, "end": 4140.32, "text": " place.", "tokens": [1081, 13], "temperature": 0.0, "avg_logprob": -0.18624411559686427, "compression_ratio": 1.698224852071006, "no_speech_prob": 2.3922595573822036e-05}, {"id": 894, "seek": 412916, "start": 4140.32, "end": 4145.2, "text": " In this case, I want to skip around through this table so I can hit control down right", "tokens": [682, 341, 1389, 11, 286, 528, 281, 10023, 926, 807, 341, 3199, 370, 286, 393, 2045, 1969, 760, 558], "temperature": 0.0, "avg_logprob": -0.18624411559686427, "compression_ratio": 1.698224852071006, "no_speech_prob": 2.3922595573822036e-05}, {"id": 895, "seek": 412916, "start": 4145.2, "end": 4150.4, "text": " to get to the bottom right, control left up to get to the top left, skip around and see", "tokens": [281, 483, 281, 264, 2767, 558, 11, 1969, 1411, 493, 281, 483, 281, 264, 1192, 1411, 11, 10023, 926, 293, 536], "temperature": 0.0, "avg_logprob": -0.18624411559686427, "compression_ratio": 1.698224852071006, "no_speech_prob": 2.3922595573822036e-05}, {"id": 896, "seek": 412916, "start": 4150.4, "end": 4151.4, "text": " what's going on.", "tokens": [437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.18624411559686427, "compression_ratio": 1.698224852071006, "no_speech_prob": 2.3922595573822036e-05}, {"id": 897, "seek": 415140, "start": 4151.4, "end": 4161.96, "text": " Here's some data and as we talked about, one way to look at collaborative filtering data", "tokens": [1692, 311, 512, 1412, 293, 382, 321, 2825, 466, 11, 472, 636, 281, 574, 412, 16555, 30822, 1412], "temperature": 0.0, "avg_logprob": -0.1614257919956261, "compression_ratio": 1.6379310344827587, "no_speech_prob": 4.029430783702992e-06}, {"id": 898, "seek": 415140, "start": 4161.96, "end": 4163.679999999999, "text": " is like this.", "tokens": [307, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.1614257919956261, "compression_ratio": 1.6379310344827587, "no_speech_prob": 4.029430783702992e-06}, {"id": 899, "seek": 415140, "start": 4163.679999999999, "end": 4169.879999999999, "text": " What we did was we grabbed from the movie lens data the people that watched the most", "tokens": [708, 321, 630, 390, 321, 18607, 490, 264, 3169, 6765, 1412, 264, 561, 300, 6337, 264, 881], "temperature": 0.0, "avg_logprob": -0.1614257919956261, "compression_ratio": 1.6379310344827587, "no_speech_prob": 4.029430783702992e-06}, {"id": 900, "seek": 415140, "start": 4169.879999999999, "end": 4177.04, "text": " movies and the movies that were the most watched and just filtered the data set down to those", "tokens": [6233, 293, 264, 6233, 300, 645, 264, 881, 6337, 293, 445, 37111, 264, 1412, 992, 760, 281, 729], "temperature": 0.0, "avg_logprob": -0.1614257919956261, "compression_ratio": 1.6379310344827587, "no_speech_prob": 4.029430783702992e-06}, {"id": 901, "seek": 415140, "start": 4177.04, "end": 4180.98, "text": " 15.", "tokens": [2119, 13], "temperature": 0.0, "avg_logprob": -0.1614257919956261, "compression_ratio": 1.6379310344827587, "no_speech_prob": 4.029430783702992e-06}, {"id": 902, "seek": 418098, "start": 4180.98, "end": 4184.299999999999, "text": " As you can see, when you do it that way, it's not sparse anymore.", "tokens": [1018, 291, 393, 536, 11, 562, 291, 360, 309, 300, 636, 11, 309, 311, 406, 637, 11668, 3602, 13], "temperature": 0.0, "avg_logprob": -0.1641551971435547, "compression_ratio": 1.381294964028777, "no_speech_prob": 7.766899216221645e-06}, {"id": 903, "seek": 418098, "start": 4184.299999999999, "end": 4192.24, "text": " There's just a small number of gaps.", "tokens": [821, 311, 445, 257, 1359, 1230, 295, 15031, 13], "temperature": 0.0, "avg_logprob": -0.1641551971435547, "compression_ratio": 1.381294964028777, "no_speech_prob": 7.766899216221645e-06}, {"id": 904, "seek": 418098, "start": 4192.24, "end": 4201.0, "text": " So this is something that we can now build a model with.", "tokens": [407, 341, 307, 746, 300, 321, 393, 586, 1322, 257, 2316, 365, 13], "temperature": 0.0, "avg_logprob": -0.1641551971435547, "compression_ratio": 1.381294964028777, "no_speech_prob": 7.766899216221645e-06}, {"id": 905, "seek": 418098, "start": 4201.0, "end": 4204.04, "text": " And so how can we build a model?", "tokens": [400, 370, 577, 393, 321, 1322, 257, 2316, 30], "temperature": 0.0, "avg_logprob": -0.1641551971435547, "compression_ratio": 1.381294964028777, "no_speech_prob": 7.766899216221645e-06}, {"id": 906, "seek": 420404, "start": 4204.04, "end": 4212.64, "text": " What we want to do is we want to create something which can predict for user 293, will they", "tokens": [708, 321, 528, 281, 360, 307, 321, 528, 281, 1884, 746, 597, 393, 6069, 337, 4195, 9413, 18, 11, 486, 436], "temperature": 0.0, "avg_logprob": -0.13677742746141222, "compression_ratio": 1.441988950276243, "no_speech_prob": 1.084512746274413e-06}, {"id": 907, "seek": 420404, "start": 4212.64, "end": 4217.32, "text": " like movie 49, for example.", "tokens": [411, 3169, 16513, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.13677742746141222, "compression_ratio": 1.441988950276243, "no_speech_prob": 1.084512746274413e-06}, {"id": 908, "seek": 420404, "start": 4217.32, "end": 4223.32, "text": " So we've got to come up with some way of, you know, some function that can represent", "tokens": [407, 321, 600, 658, 281, 808, 493, 365, 512, 636, 295, 11, 291, 458, 11, 512, 2445, 300, 393, 2906], "temperature": 0.0, "avg_logprob": -0.13677742746141222, "compression_ratio": 1.441988950276243, "no_speech_prob": 1.084512746274413e-06}, {"id": 909, "seek": 420404, "start": 4223.32, "end": 4227.92, "text": " that decision.", "tokens": [300, 3537, 13], "temperature": 0.0, "avg_logprob": -0.13677742746141222, "compression_ratio": 1.441988950276243, "no_speech_prob": 1.084512746274413e-06}, {"id": 910, "seek": 420404, "start": 4227.92, "end": 4231.48, "text": " And so here's a simple possible approach.", "tokens": [400, 370, 510, 311, 257, 2199, 1944, 3109, 13], "temperature": 0.0, "avg_logprob": -0.13677742746141222, "compression_ratio": 1.441988950276243, "no_speech_prob": 1.084512746274413e-06}, {"id": 911, "seek": 423148, "start": 4231.48, "end": 4236.04, "text": " And so we're going to take this idea of doing some matrix multiplications.", "tokens": [400, 370, 321, 434, 516, 281, 747, 341, 1558, 295, 884, 512, 8141, 17596, 763, 13], "temperature": 0.0, "avg_logprob": -0.15729359576576635, "compression_ratio": 1.8231707317073171, "no_speech_prob": 3.0415751552936854e-06}, {"id": 912, "seek": 423148, "start": 4236.04, "end": 4242.0, "text": " So I've created here a random matrix.", "tokens": [407, 286, 600, 2942, 510, 257, 4974, 8141, 13], "temperature": 0.0, "avg_logprob": -0.15729359576576635, "compression_ratio": 1.8231707317073171, "no_speech_prob": 3.0415751552936854e-06}, {"id": 913, "seek": 423148, "start": 4242.0, "end": 4249.2, "text": " So here's one matrix of random numbers and I've created here another matrix of random", "tokens": [407, 510, 311, 472, 8141, 295, 4974, 3547, 293, 286, 600, 2942, 510, 1071, 8141, 295, 4974], "temperature": 0.0, "avg_logprob": -0.15729359576576635, "compression_ratio": 1.8231707317073171, "no_speech_prob": 3.0415751552936854e-06}, {"id": 914, "seek": 423148, "start": 4249.2, "end": 4251.16, "text": " numbers.", "tokens": [3547, 13], "temperature": 0.0, "avg_logprob": -0.15729359576576635, "compression_ratio": 1.8231707317073171, "no_speech_prob": 3.0415751552936854e-06}, {"id": 915, "seek": 423148, "start": 4251.16, "end": 4260.5199999999995, "text": " More specifically, for each movie, I've created five random numbers and for each user, I've", "tokens": [5048, 4682, 11, 337, 1184, 3169, 11, 286, 600, 2942, 1732, 4974, 3547, 293, 337, 1184, 4195, 11, 286, 600], "temperature": 0.0, "avg_logprob": -0.15729359576576635, "compression_ratio": 1.8231707317073171, "no_speech_prob": 3.0415751552936854e-06}, {"id": 916, "seek": 426052, "start": 4260.52, "end": 4264.96, "text": " created five random numbers.", "tokens": [2942, 1732, 4974, 3547, 13], "temperature": 0.0, "avg_logprob": -0.1667366204438386, "compression_ratio": 1.3541666666666667, "no_speech_prob": 1.6536832845304161e-06}, {"id": 917, "seek": 426052, "start": 4264.96, "end": 4278.84, "text": " And so we could say then that user 14, movie 27, did they like it or not?", "tokens": [400, 370, 321, 727, 584, 550, 300, 4195, 3499, 11, 3169, 7634, 11, 630, 436, 411, 309, 420, 406, 30], "temperature": 0.0, "avg_logprob": -0.1667366204438386, "compression_ratio": 1.3541666666666667, "no_speech_prob": 1.6536832845304161e-06}, {"id": 918, "seek": 426052, "start": 4278.84, "end": 4285.400000000001, "text": " Well the rating, what we could do would be to multiply together this vector and that", "tokens": [1042, 264, 10990, 11, 437, 321, 727, 360, 576, 312, 281, 12972, 1214, 341, 8062, 293, 300], "temperature": 0.0, "avg_logprob": -0.1667366204438386, "compression_ratio": 1.3541666666666667, "no_speech_prob": 1.6536832845304161e-06}, {"id": 919, "seek": 426052, "start": 4285.400000000001, "end": 4286.400000000001, "text": " vector.", "tokens": [8062, 13], "temperature": 0.0, "avg_logprob": -0.1667366204438386, "compression_ratio": 1.3541666666666667, "no_speech_prob": 1.6536832845304161e-06}, {"id": 920, "seek": 428640, "start": 4286.4, "end": 4292.24, "text": " So here's a dot product, and here's a dot product.", "tokens": [407, 510, 311, 257, 5893, 1674, 11, 293, 510, 311, 257, 5893, 1674, 13], "temperature": 0.0, "avg_logprob": -0.20071583702450707, "compression_ratio": 1.6869158878504673, "no_speech_prob": 2.753527951426804e-05}, {"id": 921, "seek": 428640, "start": 4292.24, "end": 4298.799999999999, "text": " And so then we can basically do that for every possible thing in here.", "tokens": [400, 370, 550, 321, 393, 1936, 360, 300, 337, 633, 1944, 551, 294, 510, 13], "temperature": 0.0, "avg_logprob": -0.20071583702450707, "compression_ratio": 1.6869158878504673, "no_speech_prob": 2.753527951426804e-05}, {"id": 922, "seek": 428640, "start": 4298.799999999999, "end": 4300.719999999999, "text": " We've got the dot product.", "tokens": [492, 600, 658, 264, 5893, 1674, 13], "temperature": 0.0, "avg_logprob": -0.20071583702450707, "compression_ratio": 1.6869158878504673, "no_speech_prob": 2.753527951426804e-05}, {"id": 923, "seek": 428640, "start": 4300.719999999999, "end": 4305.36, "text": " And thanks to spreadsheets, we can just do that in one place and copy it over and it", "tokens": [400, 3231, 281, 23651, 1385, 11, 321, 393, 445, 360, 300, 294, 472, 1081, 293, 5055, 309, 670, 293, 309], "temperature": 0.0, "avg_logprob": -0.20071583702450707, "compression_ratio": 1.6869158878504673, "no_speech_prob": 2.753527951426804e-05}, {"id": 924, "seek": 428640, "start": 4305.36, "end": 4308.679999999999, "text": " fills in the whole thing for us.", "tokens": [22498, 294, 264, 1379, 551, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.20071583702450707, "compression_ratio": 1.6869158878504673, "no_speech_prob": 2.753527951426804e-05}, {"id": 925, "seek": 428640, "start": 4308.679999999999, "end": 4309.96, "text": " Why would we do it this way?", "tokens": [1545, 576, 321, 360, 309, 341, 636, 30], "temperature": 0.0, "avg_logprob": -0.20071583702450707, "compression_ratio": 1.6869158878504673, "no_speech_prob": 2.753527951426804e-05}, {"id": 926, "seek": 428640, "start": 4309.96, "end": 4314.719999999999, "text": " Well, this is the basic starting point of a neural net, isn't it?", "tokens": [1042, 11, 341, 307, 264, 3875, 2891, 935, 295, 257, 18161, 2533, 11, 1943, 380, 309, 30], "temperature": 0.0, "avg_logprob": -0.20071583702450707, "compression_ratio": 1.6869158878504673, "no_speech_prob": 2.753527951426804e-05}, {"id": 927, "seek": 431472, "start": 4314.72, "end": 4320.400000000001, "text": " The basic starting point of a neural net is that you take the matrix multiplication of", "tokens": [440, 3875, 2891, 935, 295, 257, 18161, 2533, 307, 300, 291, 747, 264, 8141, 27290, 295], "temperature": 0.0, "avg_logprob": -0.1159067153930664, "compression_ratio": 1.7, "no_speech_prob": 1.3211666555434931e-05}, {"id": 928, "seek": 431472, "start": 4320.400000000001, "end": 4325.96, "text": " two matrices and that's what your first layer always is.", "tokens": [732, 32284, 293, 300, 311, 437, 428, 700, 4583, 1009, 307, 13], "temperature": 0.0, "avg_logprob": -0.1159067153930664, "compression_ratio": 1.7, "no_speech_prob": 1.3211666555434931e-05}, {"id": 929, "seek": 431472, "start": 4325.96, "end": 4329.56, "text": " And so we just have to come up with some way of saying, well, what are two matrices that", "tokens": [400, 370, 321, 445, 362, 281, 808, 493, 365, 512, 636, 295, 1566, 11, 731, 11, 437, 366, 732, 32284, 300], "temperature": 0.0, "avg_logprob": -0.1159067153930664, "compression_ratio": 1.7, "no_speech_prob": 1.3211666555434931e-05}, {"id": 930, "seek": 431472, "start": 4329.56, "end": 4333.2, "text": " we can multiply?", "tokens": [321, 393, 12972, 30], "temperature": 0.0, "avg_logprob": -0.1159067153930664, "compression_ratio": 1.7, "no_speech_prob": 1.3211666555434931e-05}, {"id": 931, "seek": 431472, "start": 4333.2, "end": 4344.68, "text": " And so clearly, you need a matrix for a user, or a vector for a user, a matrix for all the", "tokens": [400, 370, 4448, 11, 291, 643, 257, 8141, 337, 257, 4195, 11, 420, 257, 8062, 337, 257, 4195, 11, 257, 8141, 337, 439, 264], "temperature": 0.0, "avg_logprob": -0.1159067153930664, "compression_ratio": 1.7, "no_speech_prob": 1.3211666555434931e-05}, {"id": 932, "seek": 434468, "start": 4344.68, "end": 4356.08, "text": " users, and a vector for a movie, or a matrix for all the movies, and multiply them together", "tokens": [5022, 11, 293, 257, 8062, 337, 257, 3169, 11, 420, 257, 8141, 337, 439, 264, 6233, 11, 293, 12972, 552, 1214], "temperature": 0.0, "avg_logprob": -0.14645214080810548, "compression_ratio": 1.4577464788732395, "no_speech_prob": 1.3419931747193914e-05}, {"id": 933, "seek": 434468, "start": 4356.08, "end": 4359.280000000001, "text": " and you get some numbers.", "tokens": [293, 291, 483, 512, 3547, 13], "temperature": 0.0, "avg_logprob": -0.14645214080810548, "compression_ratio": 1.4577464788732395, "no_speech_prob": 1.3419931747193914e-05}, {"id": 934, "seek": 434468, "start": 4359.280000000001, "end": 4366.280000000001, "text": " So they don't mean anything yet, they're just random, but we can now use gradient descent", "tokens": [407, 436, 500, 380, 914, 1340, 1939, 11, 436, 434, 445, 4974, 11, 457, 321, 393, 586, 764, 16235, 23475], "temperature": 0.0, "avg_logprob": -0.14645214080810548, "compression_ratio": 1.4577464788732395, "no_speech_prob": 1.3419931747193914e-05}, {"id": 935, "seek": 436628, "start": 4366.28, "end": 4377.96, "text": " to try to make these numbers and these numbers give us results that are closer to what we", "tokens": [281, 853, 281, 652, 613, 3547, 293, 613, 3547, 976, 505, 3542, 300, 366, 4966, 281, 437, 321], "temperature": 0.0, "avg_logprob": -0.14529238806830513, "compression_ratio": 1.4722222222222223, "no_speech_prob": 3.6119618016527966e-06}, {"id": 936, "seek": 436628, "start": 4377.96, "end": 4378.96, "text": " wanted.", "tokens": [1415, 13], "temperature": 0.0, "avg_logprob": -0.14529238806830513, "compression_ratio": 1.4722222222222223, "no_speech_prob": 3.6119618016527966e-06}, {"id": 937, "seek": 436628, "start": 4378.96, "end": 4381.12, "text": " So how do we do that?", "tokens": [407, 577, 360, 321, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.14529238806830513, "compression_ratio": 1.4722222222222223, "no_speech_prob": 3.6119618016527966e-06}, {"id": 938, "seek": 436628, "start": 4381.12, "end": 4386.92, "text": " Well, we've set this up now as a linear model.", "tokens": [1042, 11, 321, 600, 992, 341, 493, 586, 382, 257, 8213, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14529238806830513, "compression_ratio": 1.4722222222222223, "no_speech_prob": 3.6119618016527966e-06}, {"id": 939, "seek": 436628, "start": 4386.92, "end": 4390.96, "text": " So the next thing we need is a loss function.", "tokens": [407, 264, 958, 551, 321, 643, 307, 257, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.14529238806830513, "compression_ratio": 1.4722222222222223, "no_speech_prob": 3.6119618016527966e-06}, {"id": 940, "seek": 439096, "start": 4390.96, "end": 4407.4800000000005, "text": " So we can calculate our loss function by saying, well, okay, movie 3 for user ID 14 should", "tokens": [407, 321, 393, 8873, 527, 4470, 2445, 538, 1566, 11, 731, 11, 1392, 11, 3169, 805, 337, 4195, 7348, 3499, 820], "temperature": 0.0, "avg_logprob": -0.17063679297765097, "compression_ratio": 1.2814814814814814, "no_speech_prob": 4.222802999720443e-06}, {"id": 941, "seek": 439096, "start": 4407.4800000000005, "end": 4413.72, "text": " have been a rating of 3 with this random matrices, it's actually a rating of 0.91.", "tokens": [362, 668, 257, 10990, 295, 805, 365, 341, 4974, 32284, 11, 309, 311, 767, 257, 10990, 295, 1958, 13, 29925, 13], "temperature": 0.0, "avg_logprob": -0.17063679297765097, "compression_ratio": 1.2814814814814814, "no_speech_prob": 4.222802999720443e-06}, {"id": 942, "seek": 441372, "start": 4413.72, "end": 4425.12, "text": " So we can find the sum of squared errors would be 3 minus 0.91 squared", "tokens": [407, 321, 393, 915, 264, 2408, 295, 8889, 13603, 576, 312, 805, 3175, 1958, 13, 29925, 8889], "temperature": 0.0, "avg_logprob": -0.1454142804415721, "compression_ratio": 1.4488188976377954, "no_speech_prob": 3.844922048301669e-06}, {"id": 943, "seek": 441372, "start": 4425.12, "end": 4426.64, "text": " and then we can add them up.", "tokens": [293, 550, 321, 393, 909, 552, 493, 13], "temperature": 0.0, "avg_logprob": -0.1454142804415721, "compression_ratio": 1.4488188976377954, "no_speech_prob": 3.844922048301669e-06}, {"id": 944, "seek": 441372, "start": 4426.64, "end": 4436.88, "text": " So there's actually a sum squared in Excel already, sum x minus y squared, so we can", "tokens": [407, 456, 311, 767, 257, 2408, 8889, 294, 19060, 1217, 11, 2408, 2031, 3175, 288, 8889, 11, 370, 321, 393], "temperature": 0.0, "avg_logprob": -0.1454142804415721, "compression_ratio": 1.4488188976377954, "no_speech_prob": 3.844922048301669e-06}, {"id": 945, "seek": 443688, "start": 4436.88, "end": 4444.400000000001, "text": " use just sum x minus y squared function, passing in those two ranges and then divide by the", "tokens": [764, 445, 2408, 2031, 3175, 288, 8889, 2445, 11, 8437, 294, 729, 732, 22526, 293, 550, 9845, 538, 264], "temperature": 0.0, "avg_logprob": -0.13909087230249778, "compression_ratio": 1.8316326530612246, "no_speech_prob": 1.3419919923762791e-05}, {"id": 946, "seek": 443688, "start": 4444.400000000001, "end": 4446.82, "text": " count to get the mean.", "tokens": [1207, 281, 483, 264, 914, 13], "temperature": 0.0, "avg_logprob": -0.13909087230249778, "compression_ratio": 1.8316326530612246, "no_speech_prob": 1.3419919923762791e-05}, {"id": 947, "seek": 443688, "start": 4446.82, "end": 4452.96, "text": " So here is a number that is the mean, well, it's actually the square root of the mean", "tokens": [407, 510, 307, 257, 1230, 300, 307, 264, 914, 11, 731, 11, 309, 311, 767, 264, 3732, 5593, 295, 264, 914], "temperature": 0.0, "avg_logprob": -0.13909087230249778, "compression_ratio": 1.8316326530612246, "no_speech_prob": 1.3419919923762791e-05}, {"id": 948, "seek": 443688, "start": 4452.96, "end": 4454.400000000001, "text": " squared error.", "tokens": [8889, 6713, 13], "temperature": 0.0, "avg_logprob": -0.13909087230249778, "compression_ratio": 1.8316326530612246, "no_speech_prob": 1.3419919923762791e-05}, {"id": 949, "seek": 443688, "start": 4454.400000000001, "end": 4459.84, "text": " So sometimes you'll see people talk about MSE, so that's the mean squared error, sometimes", "tokens": [407, 2171, 291, 603, 536, 561, 751, 466, 376, 5879, 11, 370, 300, 311, 264, 914, 8889, 6713, 11, 2171], "temperature": 0.0, "avg_logprob": -0.13909087230249778, "compression_ratio": 1.8316326530612246, "no_speech_prob": 1.3419919923762791e-05}, {"id": 950, "seek": 443688, "start": 4459.84, "end": 4463.08, "text": " you'll see RMSE, that's the root mean squared error.", "tokens": [291, 603, 536, 23790, 5879, 11, 300, 311, 264, 5593, 914, 8889, 6713, 13], "temperature": 0.0, "avg_logprob": -0.13909087230249778, "compression_ratio": 1.8316326530612246, "no_speech_prob": 1.3419919923762791e-05}, {"id": 951, "seek": 446308, "start": 4463.08, "end": 4471.0599999999995, "text": " So since I've got a square root at the front, this is the square root mean squared error.", "tokens": [407, 1670, 286, 600, 658, 257, 3732, 5593, 412, 264, 1868, 11, 341, 307, 264, 3732, 5593, 914, 8889, 6713, 13], "temperature": 0.0, "avg_logprob": -0.06828444121313877, "compression_ratio": 1.5034013605442176, "no_speech_prob": 4.0294403333973605e-06}, {"id": 952, "seek": 446308, "start": 4471.0599999999995, "end": 4472.92, "text": " So we have a loss.", "tokens": [407, 321, 362, 257, 4470, 13], "temperature": 0.0, "avg_logprob": -0.06828444121313877, "compression_ratio": 1.5034013605442176, "no_speech_prob": 4.0294403333973605e-06}, {"id": 953, "seek": 446308, "start": 4472.92, "end": 4481.28, "text": " So now all we need to do is use gradient descent to try to modify our weight matrices to make", "tokens": [407, 586, 439, 321, 643, 281, 360, 307, 764, 16235, 23475, 281, 853, 281, 16927, 527, 3364, 32284, 281, 652], "temperature": 0.0, "avg_logprob": -0.06828444121313877, "compression_ratio": 1.5034013605442176, "no_speech_prob": 4.0294403333973605e-06}, {"id": 954, "seek": 446308, "start": 4481.28, "end": 4485.2, "text": " that loss smaller.", "tokens": [300, 4470, 4356, 13], "temperature": 0.0, "avg_logprob": -0.06828444121313877, "compression_ratio": 1.5034013605442176, "no_speech_prob": 4.0294403333973605e-06}, {"id": 955, "seek": 448520, "start": 4485.2, "end": 4495.4, "text": " So Excel will do that for me, so it's probably worth knowing how to do that.", "tokens": [407, 19060, 486, 360, 300, 337, 385, 11, 370, 309, 311, 1391, 3163, 5276, 577, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.22000172830397083, "compression_ratio": 1.523489932885906, "no_speech_prob": 1.4738880054210313e-05}, {"id": 956, "seek": 448520, "start": 4495.4, "end": 4511.24, "text": " So we have to install add-ins, solvers there, okay, so the gradient descent solver in Excel", "tokens": [407, 321, 362, 281, 3625, 909, 12, 1292, 11, 1404, 840, 456, 11, 1392, 11, 370, 264, 16235, 23475, 1404, 331, 294, 19060], "temperature": 0.0, "avg_logprob": -0.22000172830397083, "compression_ratio": 1.523489932885906, "no_speech_prob": 1.4738880054210313e-05}, {"id": 957, "seek": 448520, "start": 4511.24, "end": 4514.24, "text": " is called solver and it just does normal gradient descent.", "tokens": [307, 1219, 1404, 331, 293, 309, 445, 775, 2710, 16235, 23475, 13], "temperature": 0.0, "avg_logprob": -0.22000172830397083, "compression_ratio": 1.523489932885906, "no_speech_prob": 1.4738880054210313e-05}, {"id": 958, "seek": 451424, "start": 4514.24, "end": 4518.96, "text": " So you just go data, solver, you need to make sure that in your settings that you've enabled", "tokens": [407, 291, 445, 352, 1412, 11, 1404, 331, 11, 291, 643, 281, 652, 988, 300, 294, 428, 6257, 300, 291, 600, 15172], "temperature": 0.0, "avg_logprob": -0.16842564669522372, "compression_ratio": 1.705607476635514, "no_speech_prob": 1.6964288079179823e-05}, {"id": 959, "seek": 451424, "start": 4518.96, "end": 4524.76, "text": " the solver extension, it comes with Excel, and all you need to do is say which cell represents", "tokens": [264, 1404, 331, 10320, 11, 309, 1487, 365, 19060, 11, 293, 439, 291, 643, 281, 360, 307, 584, 597, 2815, 8855], "temperature": 0.0, "avg_logprob": -0.16842564669522372, "compression_ratio": 1.705607476635514, "no_speech_prob": 1.6964288079179823e-05}, {"id": 960, "seek": 451424, "start": 4524.76, "end": 4532.42, "text": " my loss function, so there it is, v41, right, so where is your loss function stored, which", "tokens": [452, 4470, 2445, 11, 370, 456, 309, 307, 11, 371, 17344, 11, 558, 11, 370, 689, 307, 428, 4470, 2445, 12187, 11, 597], "temperature": 0.0, "avg_logprob": -0.16842564669522372, "compression_ratio": 1.705607476635514, "no_speech_prob": 1.6964288079179823e-05}, {"id": 961, "seek": 451424, "start": 4532.42, "end": 4542.32, "text": " cells contain your variables, right, so you can see here I've got h19 to v23, which is", "tokens": [5438, 5304, 428, 9102, 11, 558, 11, 370, 291, 393, 536, 510, 286, 600, 658, 276, 3405, 281, 371, 9356, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.16842564669522372, "compression_ratio": 1.705607476635514, "no_speech_prob": 1.6964288079179823e-05}, {"id": 962, "seek": 454232, "start": 4542.32, "end": 4552.16, "text": " up here, and b25 to f39, which is over there, and then you can just say, okay, set your", "tokens": [493, 510, 11, 293, 272, 6074, 281, 283, 12493, 11, 597, 307, 670, 456, 11, 293, 550, 291, 393, 445, 584, 11, 1392, 11, 992, 428], "temperature": 0.0, "avg_logprob": -0.1241943273651466, "compression_ratio": 1.5446009389671362, "no_speech_prob": 4.565951712720562e-06}, {"id": 963, "seek": 454232, "start": 4552.16, "end": 4559.84, "text": " loss function to a minimum by changing those cells and solve.", "tokens": [4470, 2445, 281, 257, 7285, 538, 4473, 729, 5438, 293, 5039, 13], "temperature": 0.0, "avg_logprob": -0.1241943273651466, "compression_ratio": 1.5446009389671362, "no_speech_prob": 4.565951712720562e-06}, {"id": 964, "seek": 454232, "start": 4559.84, "end": 4565.48, "text": " And you'll see it starts at 2.81 and you can see the numbers going down, and so all that's", "tokens": [400, 291, 603, 536, 309, 3719, 412, 568, 13, 32875, 293, 291, 393, 536, 264, 3547, 516, 760, 11, 293, 370, 439, 300, 311], "temperature": 0.0, "avg_logprob": -0.1241943273651466, "compression_ratio": 1.5446009389671362, "no_speech_prob": 4.565951712720562e-06}, {"id": 965, "seek": 454232, "start": 4565.48, "end": 4571.46, "text": " doing is using gradient descent exactly the same way that we did when we did it manually", "tokens": [884, 307, 1228, 16235, 23475, 2293, 264, 912, 636, 300, 321, 630, 562, 321, 630, 309, 16945], "temperature": 0.0, "avg_logprob": -0.1241943273651466, "compression_ratio": 1.5446009389671362, "no_speech_prob": 4.565951712720562e-06}, {"id": 966, "seek": 457146, "start": 4571.46, "end": 4577.24, "text": " in the notebook the other day, okay, but it's rather than solving the mean squared error", "tokens": [294, 264, 21060, 264, 661, 786, 11, 1392, 11, 457, 309, 311, 2831, 813, 12606, 264, 914, 8889, 6713], "temperature": 0.0, "avg_logprob": -0.1686446573827174, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.5206747775664553e-05}, {"id": 967, "seek": 457146, "start": 4577.24, "end": 4587.46, "text": " for a at b, a at x in the Python, instead it is solving the loss function here, which", "tokens": [337, 257, 412, 272, 11, 257, 412, 2031, 294, 264, 15329, 11, 2602, 309, 307, 12606, 264, 4470, 2445, 510, 11, 597], "temperature": 0.0, "avg_logprob": -0.1686446573827174, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.5206747775664553e-05}, {"id": 968, "seek": 457146, "start": 4587.46, "end": 4591.52, "text": " is the mean squared error of the dot product of each of those vectors by each of these", "tokens": [307, 264, 914, 8889, 6713, 295, 264, 5893, 1674, 295, 1184, 295, 729, 18875, 538, 1184, 295, 613], "temperature": 0.0, "avg_logprob": -0.1686446573827174, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.5206747775664553e-05}, {"id": 969, "seek": 457146, "start": 4591.52, "end": 4597.16, "text": " vectors, and so there it goes.", "tokens": [18875, 11, 293, 370, 456, 309, 1709, 13], "temperature": 0.0, "avg_logprob": -0.1686446573827174, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.5206747775664553e-05}, {"id": 970, "seek": 459716, "start": 4597.16, "end": 4604.86, "text": " So we'll let that run for a little while and see what happens, but basically in micro here", "tokens": [407, 321, 603, 718, 300, 1190, 337, 257, 707, 1339, 293, 536, 437, 2314, 11, 457, 1936, 294, 4532, 510], "temperature": 0.0, "avg_logprob": -0.08956118552915511, "compression_ratio": 1.4806629834254144, "no_speech_prob": 3.989713661667338e-07}, {"id": 971, "seek": 459716, "start": 4604.86, "end": 4613.12, "text": " is a simple way of creating a neural network, which is really in this case it's like just", "tokens": [307, 257, 2199, 636, 295, 4084, 257, 18161, 3209, 11, 597, 307, 534, 294, 341, 1389, 309, 311, 411, 445], "temperature": 0.0, "avg_logprob": -0.08956118552915511, "compression_ratio": 1.4806629834254144, "no_speech_prob": 3.989713661667338e-07}, {"id": 972, "seek": 459716, "start": 4613.12, "end": 4623.72, "text": " a single linear layer with gradient descent to solve a collaborative filtering problem.", "tokens": [257, 2167, 8213, 4583, 365, 16235, 23475, 281, 5039, 257, 16555, 30822, 1154, 13], "temperature": 0.0, "avg_logprob": -0.08956118552915511, "compression_ratio": 1.4806629834254144, "no_speech_prob": 3.989713661667338e-07}, {"id": 973, "seek": 462372, "start": 4623.72, "end": 4629.0, "text": " So let's go back and see what we do over here.", "tokens": [407, 718, 311, 352, 646, 293, 536, 437, 321, 360, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.11135815285347603, "compression_ratio": 1.7115384615384615, "no_speech_prob": 1.1125549463031348e-05}, {"id": 974, "seek": 462372, "start": 4629.0, "end": 4635.64, "text": " So over here we used getCollabLearner.", "tokens": [407, 670, 510, 321, 1143, 483, 35294, 455, 11020, 22916, 13], "temperature": 0.0, "avg_logprob": -0.11135815285347603, "compression_ratio": 1.7115384615384615, "no_speech_prob": 1.1125549463031348e-05}, {"id": 975, "seek": 462372, "start": 4635.64, "end": 4644.92, "text": " So the function that was called in the notebook was getCollabLearner, and so as you dig deeper", "tokens": [407, 264, 2445, 300, 390, 1219, 294, 264, 21060, 390, 483, 35294, 455, 11020, 22916, 11, 293, 370, 382, 291, 2528, 7731], "temperature": 0.0, "avg_logprob": -0.11135815285347603, "compression_ratio": 1.7115384615384615, "no_speech_prob": 1.1125549463031348e-05}, {"id": 976, "seek": 462372, "start": 4644.92, "end": 4649.52, "text": " into deep learning, one of the really good ways to dig deeper into deep learning is to", "tokens": [666, 2452, 2539, 11, 472, 295, 264, 534, 665, 2098, 281, 2528, 7731, 666, 2452, 2539, 307, 281], "temperature": 0.0, "avg_logprob": -0.11135815285347603, "compression_ratio": 1.7115384615384615, "no_speech_prob": 1.1125549463031348e-05}, {"id": 977, "seek": 464952, "start": 4649.52, "end": 4654.6, "text": " dig into the fastai source code and see what's going on, and so if you're going to be able", "tokens": [2528, 666, 264, 2370, 1301, 4009, 3089, 293, 536, 437, 311, 516, 322, 11, 293, 370, 498, 291, 434, 516, 281, 312, 1075], "temperature": 0.0, "avg_logprob": -0.1258534638778023, "compression_ratio": 1.8860759493670887, "no_speech_prob": 3.883052340825088e-05}, {"id": 978, "seek": 464952, "start": 4654.6, "end": 4659.320000000001, "text": " to do that, you need to know how to use your editor well enough to dig through the source", "tokens": [281, 360, 300, 11, 291, 643, 281, 458, 577, 281, 764, 428, 9839, 731, 1547, 281, 2528, 807, 264, 4009], "temperature": 0.0, "avg_logprob": -0.1258534638778023, "compression_ratio": 1.8860759493670887, "no_speech_prob": 3.883052340825088e-05}, {"id": 979, "seek": 464952, "start": 4659.320000000001, "end": 4663.620000000001, "text": " code, and basically there's two main things you need to know how to do.", "tokens": [3089, 11, 293, 1936, 456, 311, 732, 2135, 721, 291, 643, 281, 458, 577, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1258534638778023, "compression_ratio": 1.8860759493670887, "no_speech_prob": 3.883052340825088e-05}, {"id": 980, "seek": 464952, "start": 4663.620000000001, "end": 4669.540000000001, "text": " One is to jump to a particular symbol, like a particular class or function by its name,", "tokens": [1485, 307, 281, 3012, 281, 257, 1729, 5986, 11, 411, 257, 1729, 1508, 420, 2445, 538, 1080, 1315, 11], "temperature": 0.0, "avg_logprob": -0.1258534638778023, "compression_ratio": 1.8860759493670887, "no_speech_prob": 3.883052340825088e-05}, {"id": 981, "seek": 464952, "start": 4669.540000000001, "end": 4672.080000000001, "text": " and the other is that when you're looking at a particular symbol to be able to jump", "tokens": [293, 264, 661, 307, 300, 562, 291, 434, 1237, 412, 257, 1729, 5986, 281, 312, 1075, 281, 3012], "temperature": 0.0, "avg_logprob": -0.1258534638778023, "compression_ratio": 1.8860759493670887, "no_speech_prob": 3.883052340825088e-05}, {"id": 982, "seek": 464952, "start": 4672.080000000001, "end": 4674.200000000001, "text": " to its implementation.", "tokens": [281, 1080, 11420, 13], "temperature": 0.0, "avg_logprob": -0.1258534638778023, "compression_ratio": 1.8860759493670887, "no_speech_prob": 3.883052340825088e-05}, {"id": 983, "seek": 467420, "start": 4674.2, "end": 4684.04, "text": " So for example in this case I want to find getCollabLearner, so in most editors, including", "tokens": [407, 337, 1365, 294, 341, 1389, 286, 528, 281, 915, 483, 35294, 455, 11020, 22916, 11, 370, 294, 881, 31446, 11, 3009], "temperature": 0.0, "avg_logprob": -0.16082924681824523, "compression_ratio": 1.5628415300546448, "no_speech_prob": 1.2218813935760409e-05}, {"id": 984, "seek": 467420, "start": 4684.04, "end": 4690.72, "text": " the one I use, vim, you can set it up so that you can hit tab or something and it jumps", "tokens": [264, 472, 286, 764, 11, 371, 332, 11, 291, 393, 992, 309, 493, 370, 300, 291, 393, 2045, 4421, 420, 746, 293, 309, 16704], "temperature": 0.0, "avg_logprob": -0.16082924681824523, "compression_ratio": 1.5628415300546448, "no_speech_prob": 1.2218813935760409e-05}, {"id": 985, "seek": 467420, "start": 4690.72, "end": 4696.76, "text": " through all the possible completions and you can hit enter and it jumps straight to the", "tokens": [807, 439, 264, 1944, 1557, 626, 293, 291, 393, 2045, 3242, 293, 309, 16704, 2997, 281, 264], "temperature": 0.0, "avg_logprob": -0.16082924681824523, "compression_ratio": 1.5628415300546448, "no_speech_prob": 1.2218813935760409e-05}, {"id": 986, "seek": 467420, "start": 4696.76, "end": 4698.74, "text": " definition for you.", "tokens": [7123, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.16082924681824523, "compression_ratio": 1.5628415300546448, "no_speech_prob": 1.2218813935760409e-05}, {"id": 987, "seek": 469874, "start": 4698.74, "end": 4708.26, "text": " So here is the definition of getCollabLearner, and as you can see it's pretty small as these", "tokens": [407, 510, 307, 264, 7123, 295, 483, 35294, 455, 11020, 22916, 11, 293, 382, 291, 393, 536, 309, 311, 1238, 1359, 382, 613], "temperature": 0.0, "avg_logprob": -0.12416634559631348, "compression_ratio": 1.6422413793103448, "no_speech_prob": 1.5689272913732566e-05}, {"id": 988, "seek": 469874, "start": 4708.26, "end": 4715.16, "text": " things tend to be, and in this case it kind of wraps a data frame and automatically creates", "tokens": [721, 3928, 281, 312, 11, 293, 294, 341, 1389, 309, 733, 295, 25831, 257, 1412, 3920, 293, 6772, 7829], "temperature": 0.0, "avg_logprob": -0.12416634559631348, "compression_ratio": 1.6422413793103448, "no_speech_prob": 1.5689272913732566e-05}, {"id": 989, "seek": 469874, "start": 4715.16, "end": 4719.34, "text": " a data bunch for you because it's so simple, but the key thing it does then is to create", "tokens": [257, 1412, 3840, 337, 291, 570, 309, 311, 370, 2199, 11, 457, 264, 2141, 551, 309, 775, 550, 307, 281, 1884], "temperature": 0.0, "avg_logprob": -0.12416634559631348, "compression_ratio": 1.6422413793103448, "no_speech_prob": 1.5689272913732566e-05}, {"id": 990, "seek": 469874, "start": 4719.34, "end": 4725.96, "text": " a model of a particular kind, which is an embedding.bias model passing in the various", "tokens": [257, 2316, 295, 257, 1729, 733, 11, 597, 307, 364, 12240, 3584, 13, 65, 4609, 2316, 8437, 294, 264, 3683], "temperature": 0.0, "avg_logprob": -0.12416634559631348, "compression_ratio": 1.6422413793103448, "no_speech_prob": 1.5689272913732566e-05}, {"id": 991, "seek": 469874, "start": 4725.96, "end": 4727.179999999999, "text": " things you asked for.", "tokens": [721, 291, 2351, 337, 13], "temperature": 0.0, "avg_logprob": -0.12416634559631348, "compression_ratio": 1.6422413793103448, "no_speech_prob": 1.5689272913732566e-05}, {"id": 992, "seek": 472718, "start": 4727.18, "end": 4732.240000000001, "text": " So you want to find out in your editor how you jump to the definition of that, which", "tokens": [407, 291, 528, 281, 915, 484, 294, 428, 9839, 577, 291, 3012, 281, 264, 7123, 295, 300, 11, 597], "temperature": 0.0, "avg_logprob": -0.1364560503708689, "compression_ratio": 1.5632183908045978, "no_speech_prob": 5.862788384547457e-06}, {"id": 993, "seek": 472718, "start": 4732.240000000001, "end": 4742.8, "text": " in vim you just hit ctrl right square bracket, and here is the definition of embedding.bias.", "tokens": [294, 371, 332, 291, 445, 2045, 269, 28269, 558, 3732, 16904, 11, 293, 510, 307, 264, 7123, 295, 12240, 3584, 13, 65, 4609, 13], "temperature": 0.0, "avg_logprob": -0.1364560503708689, "compression_ratio": 1.5632183908045978, "no_speech_prob": 5.862788384547457e-06}, {"id": 994, "seek": 472718, "start": 4742.8, "end": 4749.900000000001, "text": " And so now we have everything on screen at once, and as you can see there's not much", "tokens": [400, 370, 586, 321, 362, 1203, 322, 2568, 412, 1564, 11, 293, 382, 291, 393, 536, 456, 311, 406, 709], "temperature": 0.0, "avg_logprob": -0.1364560503708689, "compression_ratio": 1.5632183908045978, "no_speech_prob": 5.862788384547457e-06}, {"id": 995, "seek": 472718, "start": 4749.900000000001, "end": 4754.240000000001, "text": " going on.", "tokens": [516, 322, 13], "temperature": 0.0, "avg_logprob": -0.1364560503708689, "compression_ratio": 1.5632183908045978, "no_speech_prob": 5.862788384547457e-06}, {"id": 996, "seek": 475424, "start": 4754.24, "end": 4761.5599999999995, "text": " So the models that are being created for you by FastAI are actually PyTorch models, and", "tokens": [407, 264, 5245, 300, 366, 885, 2942, 337, 291, 538, 15968, 48698, 366, 767, 9953, 51, 284, 339, 5245, 11, 293], "temperature": 0.0, "avg_logprob": -0.11933531079973493, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.5936429917928763e-05}, {"id": 997, "seek": 475424, "start": 4761.5599999999995, "end": 4767.4, "text": " a PyTorch model is called an nn.module.", "tokens": [257, 9953, 51, 284, 339, 2316, 307, 1219, 364, 297, 77, 13, 8014, 2271, 13], "temperature": 0.0, "avg_logprob": -0.11933531079973493, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.5936429917928763e-05}, {"id": 998, "seek": 475424, "start": 4767.4, "end": 4770.94, "text": " That's the name in PyTorch of their models.", "tokens": [663, 311, 264, 1315, 294, 9953, 51, 284, 339, 295, 641, 5245, 13], "temperature": 0.0, "avg_logprob": -0.11933531079973493, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.5936429917928763e-05}, {"id": 999, "seek": 475424, "start": 4770.94, "end": 4775.46, "text": " It's a little more nuanced than that, but that's a good starting point for now.", "tokens": [467, 311, 257, 707, 544, 45115, 813, 300, 11, 457, 300, 311, 257, 665, 2891, 935, 337, 586, 13], "temperature": 0.0, "avg_logprob": -0.11933531079973493, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.5936429917928763e-05}, {"id": 1000, "seek": 475424, "start": 4775.46, "end": 4784.2, "text": " And when a PyTorch nn.module is run, when you calculate the result of that layer, it", "tokens": [400, 562, 257, 9953, 51, 284, 339, 297, 77, 13, 8014, 2271, 307, 1190, 11, 562, 291, 8873, 264, 1874, 295, 300, 4583, 11, 309], "temperature": 0.0, "avg_logprob": -0.11933531079973493, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.5936429917928763e-05}, {"id": 1001, "seek": 478420, "start": 4784.2, "end": 4790.099999999999, "text": " always calls a method for you called forward.", "tokens": [1009, 5498, 257, 3170, 337, 291, 1219, 2128, 13], "temperature": 0.0, "avg_logprob": -0.15846107707304113, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.883049066644162e-05}, {"id": 1002, "seek": 478420, "start": 4790.099999999999, "end": 4796.0, "text": " So it's in here that you get to find out how this thing is actually calculated.", "tokens": [407, 309, 311, 294, 510, 300, 291, 483, 281, 915, 484, 577, 341, 551, 307, 767, 15598, 13], "temperature": 0.0, "avg_logprob": -0.15846107707304113, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.883049066644162e-05}, {"id": 1003, "seek": 478420, "start": 4796.0, "end": 4802.92, "text": " When the model is built at the start, it calls this thing called underscore underscore init", "tokens": [1133, 264, 2316, 307, 3094, 412, 264, 722, 11, 309, 5498, 341, 551, 1219, 37556, 37556, 3157], "temperature": 0.0, "avg_logprob": -0.15846107707304113, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.883049066644162e-05}, {"id": 1004, "seek": 478420, "start": 4802.92, "end": 4804.46, "text": " underscore underscore.", "tokens": [37556, 37556, 13], "temperature": 0.0, "avg_logprob": -0.15846107707304113, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.883049066644162e-05}, {"id": 1005, "seek": 478420, "start": 4804.46, "end": 4809.46, "text": " And as I think we briefly mentioned before, in Python people tend to call this dunder", "tokens": [400, 382, 286, 519, 321, 10515, 2835, 949, 11, 294, 15329, 561, 3928, 281, 818, 341, 274, 6617], "temperature": 0.0, "avg_logprob": -0.15846107707304113, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.883049066644162e-05}, {"id": 1006, "seek": 478420, "start": 4809.46, "end": 4811.599999999999, "text": " init, double underscore init.", "tokens": [3157, 11, 3834, 37556, 3157, 13], "temperature": 0.0, "avg_logprob": -0.15846107707304113, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.883049066644162e-05}, {"id": 1007, "seek": 481160, "start": 4811.6, "end": 4819.4800000000005, "text": " So dunder init is how we create the model, and forward is how we run the model.", "tokens": [407, 274, 6617, 3157, 307, 577, 321, 1884, 264, 2316, 11, 293, 2128, 307, 577, 321, 1190, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.0827635279241598, "compression_ratio": 1.8625592417061612, "no_speech_prob": 8.315259378832707e-07}, {"id": 1008, "seek": 481160, "start": 4819.4800000000005, "end": 4823.9400000000005, "text": " One thing if you're watching carefully, you might notice, is there's nothing here saying", "tokens": [1485, 551, 498, 291, 434, 1976, 7500, 11, 291, 1062, 3449, 11, 307, 456, 311, 1825, 510, 1566], "temperature": 0.0, "avg_logprob": -0.0827635279241598, "compression_ratio": 1.8625592417061612, "no_speech_prob": 8.315259378832707e-07}, {"id": 1009, "seek": 481160, "start": 4823.9400000000005, "end": 4830.4800000000005, "text": " to how to calculate the gradients of the model, and that's because PyTorch does it for us.", "tokens": [281, 577, 281, 8873, 264, 2771, 2448, 295, 264, 2316, 11, 293, 300, 311, 570, 9953, 51, 284, 339, 775, 309, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.0827635279241598, "compression_ratio": 1.8625592417061612, "no_speech_prob": 8.315259378832707e-07}, {"id": 1010, "seek": 481160, "start": 4830.4800000000005, "end": 4836.620000000001, "text": " So you only have to tell it how to calculate the output of your model, and PyTorch will", "tokens": [407, 291, 787, 362, 281, 980, 309, 577, 281, 8873, 264, 5598, 295, 428, 2316, 11, 293, 9953, 51, 284, 339, 486], "temperature": 0.0, "avg_logprob": -0.0827635279241598, "compression_ratio": 1.8625592417061612, "no_speech_prob": 8.315259378832707e-07}, {"id": 1011, "seek": 481160, "start": 4836.620000000001, "end": 4840.8, "text": " go ahead and calculate the gradients for you.", "tokens": [352, 2286, 293, 8873, 264, 2771, 2448, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.0827635279241598, "compression_ratio": 1.8625592417061612, "no_speech_prob": 8.315259378832707e-07}, {"id": 1012, "seek": 484080, "start": 4840.8, "end": 4851.62, "text": " And so in this case, the model contains a set of weights for a user, a set of weights", "tokens": [400, 370, 294, 341, 1389, 11, 264, 2316, 8306, 257, 992, 295, 17443, 337, 257, 4195, 11, 257, 992, 295, 17443], "temperature": 0.0, "avg_logprob": -0.09683252103400952, "compression_ratio": 1.7857142857142858, "no_speech_prob": 4.222812094667461e-06}, {"id": 1013, "seek": 484080, "start": 4851.62, "end": 4857.8, "text": " for an item, a set of biases for a user, a set of biases for an item, and each one of", "tokens": [337, 364, 3174, 11, 257, 992, 295, 32152, 337, 257, 4195, 11, 257, 992, 295, 32152, 337, 364, 3174, 11, 293, 1184, 472, 295], "temperature": 0.0, "avg_logprob": -0.09683252103400952, "compression_ratio": 1.7857142857142858, "no_speech_prob": 4.222812094667461e-06}, {"id": 1014, "seek": 484080, "start": 4857.8, "end": 4867.8, "text": " those is coming from this thing called get-embedding.", "tokens": [729, 307, 1348, 490, 341, 551, 1219, 483, 12, 443, 2883, 3584, 13], "temperature": 0.0, "avg_logprob": -0.09683252103400952, "compression_ratio": 1.7857142857142858, "no_speech_prob": 4.222812094667461e-06}, {"id": 1015, "seek": 486780, "start": 4867.8, "end": 4871.88, "text": " So let's see get-embedding.", "tokens": [407, 718, 311, 536, 483, 12, 443, 2883, 3584, 13], "temperature": 0.0, "avg_logprob": -0.10632208983103435, "compression_ratio": 1.6885245901639345, "no_speech_prob": 2.090452198899584e-06}, {"id": 1016, "seek": 486780, "start": 4871.88, "end": 4878.6, "text": " So here is the definition of get-embedding, and all it does basically is it calls this", "tokens": [407, 510, 307, 264, 7123, 295, 483, 12, 443, 2883, 3584, 11, 293, 439, 309, 775, 1936, 307, 309, 5498, 341], "temperature": 0.0, "avg_logprob": -0.10632208983103435, "compression_ratio": 1.6885245901639345, "no_speech_prob": 2.090452198899584e-06}, {"id": 1017, "seek": 486780, "start": 4878.6, "end": 4883.52, "text": " PyTorch thing called nn.embedding.", "tokens": [9953, 51, 284, 339, 551, 1219, 297, 77, 13, 443, 2883, 3584, 13], "temperature": 0.0, "avg_logprob": -0.10632208983103435, "compression_ratio": 1.6885245901639345, "no_speech_prob": 2.090452198899584e-06}, {"id": 1018, "seek": 486780, "start": 4883.52, "end": 4889.24, "text": " So in PyTorch they have a lot of standard neural network layers set up for you.", "tokens": [407, 294, 9953, 51, 284, 339, 436, 362, 257, 688, 295, 3832, 18161, 3209, 7914, 992, 493, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.10632208983103435, "compression_ratio": 1.6885245901639345, "no_speech_prob": 2.090452198899584e-06}, {"id": 1019, "seek": 486780, "start": 4889.24, "end": 4895.76, "text": " So it creates an embedding, and then this thing here is, it just randomizes it.", "tokens": [407, 309, 7829, 364, 12240, 3584, 11, 293, 550, 341, 551, 510, 307, 11, 309, 445, 4974, 5660, 309, 13], "temperature": 0.0, "avg_logprob": -0.10632208983103435, "compression_ratio": 1.6885245901639345, "no_speech_prob": 2.090452198899584e-06}, {"id": 1020, "seek": 489576, "start": 4895.76, "end": 4901.56, "text": " So this is something which creates normal random numbers for the embedding.", "tokens": [407, 341, 307, 746, 597, 7829, 2710, 4974, 3547, 337, 264, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.1893454434578879, "compression_ratio": 1.530612244897959, "no_speech_prob": 5.338135906640673e-06}, {"id": 1021, "seek": 489576, "start": 4901.56, "end": 4903.92, "text": " So what's an embedding?", "tokens": [407, 437, 311, 364, 12240, 3584, 30], "temperature": 0.0, "avg_logprob": -0.1893454434578879, "compression_ratio": 1.530612244897959, "no_speech_prob": 5.338135906640673e-06}, {"id": 1022, "seek": 489576, "start": 4903.92, "end": 4909.84, "text": " An embedding, not surprisingly, is a matrix of weights.", "tokens": [1107, 12240, 3584, 11, 406, 17600, 11, 307, 257, 8141, 295, 17443, 13], "temperature": 0.0, "avg_logprob": -0.1893454434578879, "compression_ratio": 1.530612244897959, "no_speech_prob": 5.338135906640673e-06}, {"id": 1023, "seek": 489576, "start": 4909.84, "end": 4924.26, "text": " Specifically it's a matrix of weights that looks something like this.", "tokens": [26058, 309, 311, 257, 8141, 295, 17443, 300, 1542, 746, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.1893454434578879, "compression_ratio": 1.530612244897959, "no_speech_prob": 5.338135906640673e-06}, {"id": 1024, "seek": 492426, "start": 4924.26, "end": 4933.52, "text": " It's a matrix of weights which you can basically look up into and grab one item out of it.", "tokens": [467, 311, 257, 8141, 295, 17443, 597, 291, 393, 1936, 574, 493, 666, 293, 4444, 472, 3174, 484, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.07066750526428223, "compression_ratio": 1.740566037735849, "no_speech_prob": 5.59431646252051e-06}, {"id": 1025, "seek": 492426, "start": 4933.52, "end": 4938.320000000001, "text": " So basically any kind of weight matrix, and we're going to be digging into this in a lot", "tokens": [407, 1936, 604, 733, 295, 3364, 8141, 11, 293, 321, 434, 516, 281, 312, 17343, 666, 341, 294, 257, 688], "temperature": 0.0, "avg_logprob": -0.07066750526428223, "compression_ratio": 1.740566037735849, "no_speech_prob": 5.59431646252051e-06}, {"id": 1026, "seek": 492426, "start": 4938.320000000001, "end": 4943.84, "text": " more detail in the coming lessons, but an embedding matrix is just a weight matrix that", "tokens": [544, 2607, 294, 264, 1348, 8820, 11, 457, 364, 12240, 3584, 8141, 307, 445, 257, 3364, 8141, 300], "temperature": 0.0, "avg_logprob": -0.07066750526428223, "compression_ratio": 1.740566037735849, "no_speech_prob": 5.59431646252051e-06}, {"id": 1027, "seek": 492426, "start": 4943.84, "end": 4949.56, "text": " is designed to be something that you kind of index into it as an array and grab one", "tokens": [307, 4761, 281, 312, 746, 300, 291, 733, 295, 8186, 666, 309, 382, 364, 10225, 293, 4444, 472], "temperature": 0.0, "avg_logprob": -0.07066750526428223, "compression_ratio": 1.740566037735849, "no_speech_prob": 5.59431646252051e-06}, {"id": 1028, "seek": 492426, "start": 4949.56, "end": 4952.0, "text": " vector out of it.", "tokens": [8062, 484, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.07066750526428223, "compression_ratio": 1.740566037735849, "no_speech_prob": 5.59431646252051e-06}, {"id": 1029, "seek": 495200, "start": 4952.0, "end": 4955.44, "text": " That's what an embedding matrix is.", "tokens": [663, 311, 437, 364, 12240, 3584, 8141, 307, 13], "temperature": 0.0, "avg_logprob": -0.11474294440690862, "compression_ratio": 1.7179487179487178, "no_speech_prob": 6.540399226651061e-06}, {"id": 1030, "seek": 495200, "start": 4955.44, "end": 4963.28, "text": " And so in our case we have an embedding matrix for a user and an embedding matrix for a movie.", "tokens": [400, 370, 294, 527, 1389, 321, 362, 364, 12240, 3584, 8141, 337, 257, 4195, 293, 364, 12240, 3584, 8141, 337, 257, 3169, 13], "temperature": 0.0, "avg_logprob": -0.11474294440690862, "compression_ratio": 1.7179487179487178, "no_speech_prob": 6.540399226651061e-06}, {"id": 1031, "seek": 495200, "start": 4963.28, "end": 4969.08, "text": " And here we have been taking the dot product of them.", "tokens": [400, 510, 321, 362, 668, 1940, 264, 5893, 1674, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.11474294440690862, "compression_ratio": 1.7179487179487178, "no_speech_prob": 6.540399226651061e-06}, {"id": 1032, "seek": 495200, "start": 4969.08, "end": 4974.36, "text": " But if you think about it, that's not quite enough because we're missing this idea that", "tokens": [583, 498, 291, 519, 466, 309, 11, 300, 311, 406, 1596, 1547, 570, 321, 434, 5361, 341, 1558, 300], "temperature": 0.0, "avg_logprob": -0.11474294440690862, "compression_ratio": 1.7179487179487178, "no_speech_prob": 6.540399226651061e-06}, {"id": 1033, "seek": 495200, "start": 4974.36, "end": 4980.16, "text": " like maybe there are certain movies that everybody likes more.", "tokens": [411, 1310, 456, 366, 1629, 6233, 300, 2201, 5902, 544, 13], "temperature": 0.0, "avg_logprob": -0.11474294440690862, "compression_ratio": 1.7179487179487178, "no_speech_prob": 6.540399226651061e-06}, {"id": 1034, "seek": 498016, "start": 4980.16, "end": 4984.24, "text": " Maybe there are some users that just tend to like movies more.", "tokens": [2704, 456, 366, 512, 5022, 300, 445, 3928, 281, 411, 6233, 544, 13], "temperature": 0.0, "avg_logprob": -0.12039303057121509, "compression_ratio": 1.7400881057268722, "no_speech_prob": 2.6425775558891473e-06}, {"id": 1035, "seek": 498016, "start": 4984.24, "end": 4988.68, "text": " So I don't really just want to multiply these two vectors together, but I really want to", "tokens": [407, 286, 500, 380, 534, 445, 528, 281, 12972, 613, 732, 18875, 1214, 11, 457, 286, 534, 528, 281], "temperature": 0.0, "avg_logprob": -0.12039303057121509, "compression_ratio": 1.7400881057268722, "no_speech_prob": 2.6425775558891473e-06}, {"id": 1036, "seek": 498016, "start": 4988.68, "end": 4995.92, "text": " add a single number of how popular is this movie and add a single number of how much", "tokens": [909, 257, 2167, 1230, 295, 577, 3743, 307, 341, 3169, 293, 909, 257, 2167, 1230, 295, 577, 709], "temperature": 0.0, "avg_logprob": -0.12039303057121509, "compression_ratio": 1.7400881057268722, "no_speech_prob": 2.6425775558891473e-06}, {"id": 1037, "seek": 498016, "start": 4995.92, "end": 4998.36, "text": " does this user like movies in general.", "tokens": [775, 341, 4195, 411, 6233, 294, 2674, 13], "temperature": 0.0, "avg_logprob": -0.12039303057121509, "compression_ratio": 1.7400881057268722, "no_speech_prob": 2.6425775558891473e-06}, {"id": 1038, "seek": 498016, "start": 4998.36, "end": 5000.48, "text": " So those are called bias terms.", "tokens": [407, 729, 366, 1219, 12577, 2115, 13], "temperature": 0.0, "avg_logprob": -0.12039303057121509, "compression_ratio": 1.7400881057268722, "no_speech_prob": 2.6425775558891473e-06}, {"id": 1039, "seek": 498016, "start": 5000.48, "end": 5006.0, "text": " Remember how I said there's this kind of idea of bias and the way we dealt with that in", "tokens": [5459, 577, 286, 848, 456, 311, 341, 733, 295, 1558, 295, 12577, 293, 264, 636, 321, 15991, 365, 300, 294], "temperature": 0.0, "avg_logprob": -0.12039303057121509, "compression_ratio": 1.7400881057268722, "no_speech_prob": 2.6425775558891473e-06}, {"id": 1040, "seek": 500600, "start": 5006.0, "end": 5013.08, "text": " our gradient descent notebook was we added a column of ones, but what we tend to do in", "tokens": [527, 16235, 23475, 21060, 390, 321, 3869, 257, 7738, 295, 2306, 11, 457, 437, 321, 3928, 281, 360, 294], "temperature": 0.0, "avg_logprob": -0.11699509620666504, "compression_ratio": 1.7268041237113403, "no_speech_prob": 5.2553682508005295e-06}, {"id": 1041, "seek": 500600, "start": 5013.08, "end": 5020.16, "text": " practice is we actually explicitly say I want to add a bias term.", "tokens": [3124, 307, 321, 767, 20803, 584, 286, 528, 281, 909, 257, 12577, 1433, 13], "temperature": 0.0, "avg_logprob": -0.11699509620666504, "compression_ratio": 1.7268041237113403, "no_speech_prob": 5.2553682508005295e-06}, {"id": 1042, "seek": 500600, "start": 5020.16, "end": 5027.28, "text": " So we don't just want to have prediction equals dot product of these two things, we want to", "tokens": [407, 321, 500, 380, 445, 528, 281, 362, 17630, 6915, 5893, 1674, 295, 613, 732, 721, 11, 321, 528, 281], "temperature": 0.0, "avg_logprob": -0.11699509620666504, "compression_ratio": 1.7268041237113403, "no_speech_prob": 5.2553682508005295e-06}, {"id": 1043, "seek": 500600, "start": 5027.28, "end": 5034.36, "text": " say it's the dot product of those two things plus a bias term for a movie plus a bias term", "tokens": [584, 309, 311, 264, 5893, 1674, 295, 729, 732, 721, 1804, 257, 12577, 1433, 337, 257, 3169, 1804, 257, 12577, 1433], "temperature": 0.0, "avg_logprob": -0.11699509620666504, "compression_ratio": 1.7268041237113403, "no_speech_prob": 5.2553682508005295e-06}, {"id": 1044, "seek": 503436, "start": 5034.36, "end": 5036.679999999999, "text": " for a user ID.", "tokens": [337, 257, 4195, 7348, 13], "temperature": 0.0, "avg_logprob": -0.13056358594572945, "compression_ratio": 1.9542857142857142, "no_speech_prob": 7.76690649217926e-06}, {"id": 1045, "seek": 503436, "start": 5036.679999999999, "end": 5040.0599999999995, "text": " So that's basically what happens.", "tokens": [407, 300, 311, 1936, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.13056358594572945, "compression_ratio": 1.9542857142857142, "no_speech_prob": 7.76690649217926e-06}, {"id": 1046, "seek": 503436, "start": 5040.0599999999995, "end": 5046.759999999999, "text": " When we set up the model, we set up the embedding matrix for the users and the embedding matrix", "tokens": [1133, 321, 992, 493, 264, 2316, 11, 321, 992, 493, 264, 12240, 3584, 8141, 337, 264, 5022, 293, 264, 12240, 3584, 8141], "temperature": 0.0, "avg_logprob": -0.13056358594572945, "compression_ratio": 1.9542857142857142, "no_speech_prob": 7.76690649217926e-06}, {"id": 1047, "seek": 503436, "start": 5046.759999999999, "end": 5052.24, "text": " for the items and then we also set up the bias vector for the users and the bias vector", "tokens": [337, 264, 4754, 293, 550, 321, 611, 992, 493, 264, 12577, 8062, 337, 264, 5022, 293, 264, 12577, 8062], "temperature": 0.0, "avg_logprob": -0.13056358594572945, "compression_ratio": 1.9542857142857142, "no_speech_prob": 7.76690649217926e-06}, {"id": 1048, "seek": 503436, "start": 5052.24, "end": 5054.0, "text": " for the items.", "tokens": [337, 264, 4754, 13], "temperature": 0.0, "avg_logprob": -0.13056358594572945, "compression_ratio": 1.9542857142857142, "no_speech_prob": 7.76690649217926e-06}, {"id": 1049, "seek": 503436, "start": 5054.0, "end": 5063.36, "text": " And then when we calculate the model, we literally just multiply the two together just like we", "tokens": [400, 550, 562, 321, 8873, 264, 2316, 11, 321, 3736, 445, 12972, 264, 732, 1214, 445, 411, 321], "temperature": 0.0, "avg_logprob": -0.13056358594572945, "compression_ratio": 1.9542857142857142, "no_speech_prob": 7.76690649217926e-06}, {"id": 1050, "seek": 506336, "start": 5063.36, "end": 5073.639999999999, "text": " did, right, we just take that product, call it dot, and then we add the bias and then putting", "tokens": [630, 11, 558, 11, 321, 445, 747, 300, 1674, 11, 818, 309, 5893, 11, 293, 550, 321, 909, 264, 12577, 293, 550, 3372], "temperature": 0.0, "avg_logprob": -0.15552360947067673, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.339180345094064e-06}, {"id": 1051, "seek": 506336, "start": 5073.639999999999, "end": 5078.099999999999, "text": " aside the min and max score for a moment, that's what we return.", "tokens": [7359, 264, 923, 293, 11469, 6175, 337, 257, 1623, 11, 300, 311, 437, 321, 2736, 13], "temperature": 0.0, "avg_logprob": -0.15552360947067673, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.339180345094064e-06}, {"id": 1052, "seek": 506336, "start": 5078.099999999999, "end": 5086.599999999999, "text": " So you can see that our model is literally doing what we did here with the tweak that", "tokens": [407, 291, 393, 536, 300, 527, 2316, 307, 3736, 884, 437, 321, 630, 510, 365, 264, 29879, 300], "temperature": 0.0, "avg_logprob": -0.15552360947067673, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.339180345094064e-06}, {"id": 1053, "seek": 506336, "start": 5086.599999999999, "end": 5090.0599999999995, "text": " we're also adding a bias.", "tokens": [321, 434, 611, 5127, 257, 12577, 13], "temperature": 0.0, "avg_logprob": -0.15552360947067673, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.339180345094064e-06}, {"id": 1054, "seek": 509006, "start": 5090.06, "end": 5098.8, "text": " So it's an incredibly simple linear model.", "tokens": [407, 309, 311, 364, 6252, 2199, 8213, 2316, 13], "temperature": 0.0, "avg_logprob": -0.08800041107904344, "compression_ratio": 1.5297619047619047, "no_speech_prob": 1.933350631588837e-06}, {"id": 1055, "seek": 509006, "start": 5098.8, "end": 5106.9400000000005, "text": " And for these kinds of collaborative filtering problems, this kind of simple linear model", "tokens": [400, 337, 613, 3685, 295, 16555, 30822, 2740, 11, 341, 733, 295, 2199, 8213, 2316], "temperature": 0.0, "avg_logprob": -0.08800041107904344, "compression_ratio": 1.5297619047619047, "no_speech_prob": 1.933350631588837e-06}, {"id": 1056, "seek": 509006, "start": 5106.9400000000005, "end": 5113.56, "text": " actually tends to work pretty well.", "tokens": [767, 12258, 281, 589, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.08800041107904344, "compression_ratio": 1.5297619047619047, "no_speech_prob": 1.933350631588837e-06}, {"id": 1057, "seek": 509006, "start": 5113.56, "end": 5119.280000000001, "text": " And then there's one tweak that we do at the end, which is that in our case we said that", "tokens": [400, 550, 456, 311, 472, 29879, 300, 321, 360, 412, 264, 917, 11, 597, 307, 300, 294, 527, 1389, 321, 848, 300], "temperature": 0.0, "avg_logprob": -0.08800041107904344, "compression_ratio": 1.5297619047619047, "no_speech_prob": 1.933350631588837e-06}, {"id": 1058, "seek": 511928, "start": 5119.28, "end": 5126.599999999999, "text": " there's a min score of 0 and a max score of 5.", "tokens": [456, 311, 257, 923, 6175, 295, 1958, 293, 257, 11469, 6175, 295, 1025, 13], "temperature": 0.0, "avg_logprob": -0.1866007217994103, "compression_ratio": 1.6327683615819208, "no_speech_prob": 8.397977580898441e-06}, {"id": 1059, "seek": 511928, "start": 5126.599999999999, "end": 5129.08, "text": " And so here's something to point out.", "tokens": [400, 370, 510, 311, 746, 281, 935, 484, 13], "temperature": 0.0, "avg_logprob": -0.1866007217994103, "compression_ratio": 1.6327683615819208, "no_speech_prob": 8.397977580898441e-06}, {"id": 1060, "seek": 511928, "start": 5129.08, "end": 5132.139999999999, "text": " Here's something to point out.", "tokens": [1692, 311, 746, 281, 935, 484, 13], "temperature": 0.0, "avg_logprob": -0.1866007217994103, "compression_ratio": 1.6327683615819208, "no_speech_prob": 8.397977580898441e-06}, {"id": 1061, "seek": 511928, "start": 5132.139999999999, "end": 5143.32, "text": " So if you have a range, so you do that dot product and you add on the two biases and", "tokens": [407, 498, 291, 362, 257, 3613, 11, 370, 291, 360, 300, 5893, 1674, 293, 291, 909, 322, 264, 732, 32152, 293], "temperature": 0.0, "avg_logprob": -0.1866007217994103, "compression_ratio": 1.6327683615819208, "no_speech_prob": 8.397977580898441e-06}, {"id": 1062, "seek": 511928, "start": 5143.32, "end": 5148.44, "text": " that could give you any possible number along the number line from very negative through", "tokens": [300, 727, 976, 291, 604, 1944, 1230, 2051, 264, 1230, 1622, 490, 588, 3671, 807], "temperature": 0.0, "avg_logprob": -0.1866007217994103, "compression_ratio": 1.6327683615819208, "no_speech_prob": 8.397977580898441e-06}, {"id": 1063, "seek": 514844, "start": 5148.44, "end": 5150.4, "text": " to very positive numbers.", "tokens": [281, 588, 3353, 3547, 13], "temperature": 0.0, "avg_logprob": -0.22727472819979228, "compression_ratio": 1.4081632653061225, "no_speech_prob": 3.6119695323577616e-06}, {"id": 1064, "seek": 514844, "start": 5150.4, "end": 5155.839999999999, "text": " But we know that we always want to end up with a number between 0 and 5.", "tokens": [583, 321, 458, 300, 321, 1009, 528, 281, 917, 493, 365, 257, 1230, 1296, 1958, 293, 1025, 13], "temperature": 0.0, "avg_logprob": -0.22727472819979228, "compression_ratio": 1.4081632653061225, "no_speech_prob": 3.6119695323577616e-06}, {"id": 1065, "seek": 514844, "start": 5155.839999999999, "end": 5157.879999999999, "text": " Let's say that's 5.", "tokens": [961, 311, 584, 300, 311, 1025, 13], "temperature": 0.0, "avg_logprob": -0.22727472819979228, "compression_ratio": 1.4081632653061225, "no_speech_prob": 3.6119695323577616e-06}, {"id": 1066, "seek": 514844, "start": 5157.879999999999, "end": 5161.36, "text": " And of course this is 0.", "tokens": [400, 295, 1164, 341, 307, 1958, 13], "temperature": 0.0, "avg_logprob": -0.22727472819979228, "compression_ratio": 1.4081632653061225, "no_speech_prob": 3.6119695323577616e-06}, {"id": 1067, "seek": 514844, "start": 5161.36, "end": 5173.0, "text": " So what if we mapped that number line like so to this function?", "tokens": [407, 437, 498, 321, 33318, 300, 1230, 1622, 411, 370, 281, 341, 2445, 30], "temperature": 0.0, "avg_logprob": -0.22727472819979228, "compression_ratio": 1.4081632653061225, "no_speech_prob": 3.6119695323577616e-06}, {"id": 1068, "seek": 517300, "start": 5173.0, "end": 5180.48, "text": " And so the shape of that function is called a sigmoid.", "tokens": [400, 370, 264, 3909, 295, 300, 2445, 307, 1219, 257, 4556, 3280, 327, 13], "temperature": 0.0, "avg_logprob": -0.1048608612228226, "compression_ratio": 1.7393617021276595, "no_speech_prob": 6.962218321859837e-06}, {"id": 1069, "seek": 517300, "start": 5180.48, "end": 5187.2, "text": " And so it's going to asymptote to 5 and it's going to asymptote to 0.", "tokens": [400, 370, 309, 311, 516, 281, 35114, 1370, 281, 1025, 293, 309, 311, 516, 281, 35114, 1370, 281, 1958, 13], "temperature": 0.0, "avg_logprob": -0.1048608612228226, "compression_ratio": 1.7393617021276595, "no_speech_prob": 6.962218321859837e-06}, {"id": 1070, "seek": 517300, "start": 5187.2, "end": 5195.44, "text": " And so that way, whatever number comes out of our dot product and adding the biases,", "tokens": [400, 370, 300, 636, 11, 2035, 1230, 1487, 484, 295, 527, 5893, 1674, 293, 5127, 264, 32152, 11], "temperature": 0.0, "avg_logprob": -0.1048608612228226, "compression_ratio": 1.7393617021276595, "no_speech_prob": 6.962218321859837e-06}, {"id": 1071, "seek": 517300, "start": 5195.44, "end": 5198.96, "text": " if we then stick it through this function, it's never going to be higher than 5 and never", "tokens": [498, 321, 550, 2897, 309, 807, 341, 2445, 11, 309, 311, 1128, 516, 281, 312, 2946, 813, 1025, 293, 1128], "temperature": 0.0, "avg_logprob": -0.1048608612228226, "compression_ratio": 1.7393617021276595, "no_speech_prob": 6.962218321859837e-06}, {"id": 1072, "seek": 517300, "start": 5198.96, "end": 5200.92, "text": " going to be smaller than 0.", "tokens": [516, 281, 312, 4356, 813, 1958, 13], "temperature": 0.0, "avg_logprob": -0.1048608612228226, "compression_ratio": 1.7393617021276595, "no_speech_prob": 6.962218321859837e-06}, {"id": 1073, "seek": 520092, "start": 5200.92, "end": 5210.76, "text": " Now strictly speaking, that's not necessary because our parameters could learn a set of", "tokens": [823, 20792, 4124, 11, 300, 311, 406, 4818, 570, 527, 9834, 727, 1466, 257, 992, 295], "temperature": 0.0, "avg_logprob": -0.12350110274094801, "compression_ratio": 1.4831460674157304, "no_speech_prob": 7.071872914821142e-06}, {"id": 1074, "seek": 520092, "start": 5210.76, "end": 5214.0, "text": " weights that gives about the right number.", "tokens": [17443, 300, 2709, 466, 264, 558, 1230, 13], "temperature": 0.0, "avg_logprob": -0.12350110274094801, "compression_ratio": 1.4831460674157304, "no_speech_prob": 7.071872914821142e-06}, {"id": 1075, "seek": 520092, "start": 5214.0, "end": 5217.6, "text": " So why would we do this extra thing if it's not necessary?", "tokens": [407, 983, 576, 321, 360, 341, 2857, 551, 498, 309, 311, 406, 4818, 30], "temperature": 0.0, "avg_logprob": -0.12350110274094801, "compression_ratio": 1.4831460674157304, "no_speech_prob": 7.071872914821142e-06}, {"id": 1076, "seek": 520092, "start": 5217.6, "end": 5224.16, "text": " Well the reason is we want to make life as easy for our model as possible.", "tokens": [1042, 264, 1778, 307, 321, 528, 281, 652, 993, 382, 1858, 337, 527, 2316, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.12350110274094801, "compression_ratio": 1.4831460674157304, "no_speech_prob": 7.071872914821142e-06}, {"id": 1077, "seek": 522416, "start": 5224.16, "end": 5231.72, "text": " So if we actually set it up so it's impossible for it to ever predict too much or ever predict", "tokens": [407, 498, 321, 767, 992, 309, 493, 370, 309, 311, 6243, 337, 309, 281, 1562, 6069, 886, 709, 420, 1562, 6069], "temperature": 0.0, "avg_logprob": -0.08306393623352051, "compression_ratio": 1.73109243697479, "no_speech_prob": 1.1189403039679746e-06}, {"id": 1078, "seek": 522416, "start": 5231.72, "end": 5236.68, "text": " too little, then it can spend more of its weights predicting the thing we care about,", "tokens": [886, 707, 11, 550, 309, 393, 3496, 544, 295, 1080, 17443, 32884, 264, 551, 321, 1127, 466, 11], "temperature": 0.0, "avg_logprob": -0.08306393623352051, "compression_ratio": 1.73109243697479, "no_speech_prob": 1.1189403039679746e-06}, {"id": 1079, "seek": 522416, "start": 5236.68, "end": 5239.24, "text": " which is deciding who's going to like what movie.", "tokens": [597, 307, 17990, 567, 311, 516, 281, 411, 437, 3169, 13], "temperature": 0.0, "avg_logprob": -0.08306393623352051, "compression_ratio": 1.73109243697479, "no_speech_prob": 1.1189403039679746e-06}, {"id": 1080, "seek": 522416, "start": 5239.24, "end": 5244.72, "text": " So this is an idea we're going to keep coming back to when it comes to making neural networks", "tokens": [407, 341, 307, 364, 1558, 321, 434, 516, 281, 1066, 1348, 646, 281, 562, 309, 1487, 281, 1455, 18161, 9590], "temperature": 0.0, "avg_logprob": -0.08306393623352051, "compression_ratio": 1.73109243697479, "no_speech_prob": 1.1189403039679746e-06}, {"id": 1081, "seek": 522416, "start": 5244.72, "end": 5249.94, "text": " work better, is it's about all these little decisions that we make to basically make it", "tokens": [589, 1101, 11, 307, 309, 311, 466, 439, 613, 707, 5327, 300, 321, 652, 281, 1936, 652, 309], "temperature": 0.0, "avg_logprob": -0.08306393623352051, "compression_ratio": 1.73109243697479, "no_speech_prob": 1.1189403039679746e-06}, {"id": 1082, "seek": 524994, "start": 5249.94, "end": 5254.32, "text": " easier for the network to learn the right thing.", "tokens": [3571, 337, 264, 3209, 281, 1466, 264, 558, 551, 13], "temperature": 0.0, "avg_logprob": -0.13427616202312967, "compression_ratio": 1.5837320574162679, "no_speech_prob": 6.240881248231744e-06}, {"id": 1083, "seek": 524994, "start": 5254.32, "end": 5264.759999999999, "text": " So that's the last tweak here, which is we take the result of this dot product plus biases,", "tokens": [407, 300, 311, 264, 1036, 29879, 510, 11, 597, 307, 321, 747, 264, 1874, 295, 341, 5893, 1674, 1804, 32152, 11], "temperature": 0.0, "avg_logprob": -0.13427616202312967, "compression_ratio": 1.5837320574162679, "no_speech_prob": 6.240881248231744e-06}, {"id": 1084, "seek": 524994, "start": 5264.759999999999, "end": 5269.4, "text": " we put it through a sigmoid, and so a sigmoid is just a function, it's basically 1 over", "tokens": [321, 829, 309, 807, 257, 4556, 3280, 327, 11, 293, 370, 257, 4556, 3280, 327, 307, 445, 257, 2445, 11, 309, 311, 1936, 502, 670], "temperature": 0.0, "avg_logprob": -0.13427616202312967, "compression_ratio": 1.5837320574162679, "no_speech_prob": 6.240881248231744e-06}, {"id": 1085, "seek": 524994, "start": 5269.4, "end": 5273.5199999999995, "text": " 1 plus e to the x, the definition doesn't much matter but it just has the shape that", "tokens": [502, 1804, 308, 281, 264, 2031, 11, 264, 7123, 1177, 380, 709, 1871, 457, 309, 445, 575, 264, 3909, 300], "temperature": 0.0, "avg_logprob": -0.13427616202312967, "compression_ratio": 1.5837320574162679, "no_speech_prob": 6.240881248231744e-06}, {"id": 1086, "seek": 524994, "start": 5273.5199999999995, "end": 5275.24, "text": " I just mentioned.", "tokens": [286, 445, 2835, 13], "temperature": 0.0, "avg_logprob": -0.13427616202312967, "compression_ratio": 1.5837320574162679, "no_speech_prob": 6.240881248231744e-06}, {"id": 1087, "seek": 527524, "start": 5275.24, "end": 5282.84, "text": " And that goes between 0 and 1, and if you then multiply that by max minus min plus min,", "tokens": [400, 300, 1709, 1296, 1958, 293, 502, 11, 293, 498, 291, 550, 12972, 300, 538, 11469, 3175, 923, 1804, 923, 11], "temperature": 0.0, "avg_logprob": -0.14650440216064453, "compression_ratio": 1.8111587982832618, "no_speech_prob": 1.0289393685525283e-05}, {"id": 1088, "seek": 527524, "start": 5282.84, "end": 5287.38, "text": " then that's going to give you something that's between min score and max score.", "tokens": [550, 300, 311, 516, 281, 976, 291, 746, 300, 311, 1296, 923, 6175, 293, 11469, 6175, 13], "temperature": 0.0, "avg_logprob": -0.14650440216064453, "compression_ratio": 1.8111587982832618, "no_speech_prob": 1.0289393685525283e-05}, {"id": 1089, "seek": 527524, "start": 5287.38, "end": 5293.84, "text": " So that means that this tiny little neural network, it's a push to call it a neural network,", "tokens": [407, 300, 1355, 300, 341, 5870, 707, 18161, 3209, 11, 309, 311, 257, 2944, 281, 818, 309, 257, 18161, 3209, 11], "temperature": 0.0, "avg_logprob": -0.14650440216064453, "compression_ratio": 1.8111587982832618, "no_speech_prob": 1.0289393685525283e-05}, {"id": 1090, "seek": 527524, "start": 5293.84, "end": 5300.32, "text": " but it is, it's a neural network with one weight matrix and no nonlinearities, so it's", "tokens": [457, 309, 307, 11, 309, 311, 257, 18161, 3209, 365, 472, 3364, 8141, 293, 572, 2107, 28263, 1088, 11, 370, 309, 311], "temperature": 0.0, "avg_logprob": -0.14650440216064453, "compression_ratio": 1.8111587982832618, "no_speech_prob": 1.0289393685525283e-05}, {"id": 1091, "seek": 527524, "start": 5300.32, "end": 5305.2, "text": " kind of the world's most boring neural network, with a sigmoid at the end.", "tokens": [733, 295, 264, 1002, 311, 881, 9989, 18161, 3209, 11, 365, 257, 4556, 3280, 327, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.14650440216064453, "compression_ratio": 1.8111587982832618, "no_speech_prob": 1.0289393685525283e-05}, {"id": 1092, "seek": 530520, "start": 5305.2, "end": 5309.44, "text": " That's actually, well I guess it does have a nonlinearity, the sigmoid at the end is", "tokens": [663, 311, 767, 11, 731, 286, 2041, 309, 775, 362, 257, 2107, 1889, 17409, 11, 264, 4556, 3280, 327, 412, 264, 917, 307], "temperature": 0.0, "avg_logprob": -0.11408823345779279, "compression_ratio": 1.6585365853658536, "no_speech_prob": 5.173883437237237e-06}, {"id": 1093, "seek": 530520, "start": 5309.44, "end": 5314.639999999999, "text": " the nonlinearity, it only has one layer of weights.", "tokens": [264, 2107, 1889, 17409, 11, 309, 787, 575, 472, 4583, 295, 17443, 13], "temperature": 0.0, "avg_logprob": -0.11408823345779279, "compression_ratio": 1.6585365853658536, "no_speech_prob": 5.173883437237237e-06}, {"id": 1094, "seek": 530520, "start": 5314.639999999999, "end": 5323.54, "text": " That actually turns out to give close to state-of-the-art performance, like I've looked up online to", "tokens": [663, 767, 4523, 484, 281, 976, 1998, 281, 1785, 12, 2670, 12, 3322, 12, 446, 3389, 11, 411, 286, 600, 2956, 493, 2950, 281], "temperature": 0.0, "avg_logprob": -0.11408823345779279, "compression_ratio": 1.6585365853658536, "no_speech_prob": 5.173883437237237e-06}, {"id": 1095, "seek": 530520, "start": 5323.54, "end": 5329.0, "text": " find out what are the best results people have on this MovieLens 100k database, and", "tokens": [915, 484, 437, 366, 264, 1151, 3542, 561, 362, 322, 341, 28766, 43, 694, 2319, 74, 8149, 11, 293], "temperature": 0.0, "avg_logprob": -0.11408823345779279, "compression_ratio": 1.6585365853658536, "no_speech_prob": 5.173883437237237e-06}, {"id": 1096, "seek": 530520, "start": 5329.0, "end": 5333.599999999999, "text": " the results I get from this little thing are better than any of the results I can find", "tokens": [264, 3542, 286, 483, 490, 341, 707, 551, 366, 1101, 813, 604, 295, 264, 3542, 286, 393, 915], "temperature": 0.0, "avg_logprob": -0.11408823345779279, "compression_ratio": 1.6585365853658536, "no_speech_prob": 5.173883437237237e-06}, {"id": 1097, "seek": 533360, "start": 5333.6, "end": 5337.120000000001, "text": " from the standard commercial products that you can download that are specialized for", "tokens": [490, 264, 3832, 6841, 3383, 300, 291, 393, 5484, 300, 366, 19813, 337], "temperature": 0.0, "avg_logprob": -0.20815150993914644, "compression_ratio": 1.6576923076923078, "no_speech_prob": 1.7231330275535583e-05}, {"id": 1098, "seek": 533360, "start": 5337.120000000001, "end": 5338.120000000001, "text": " this.", "tokens": [341, 13], "temperature": 0.0, "avg_logprob": -0.20815150993914644, "compression_ratio": 1.6576923076923078, "no_speech_prob": 1.7231330275535583e-05}, {"id": 1099, "seek": 533360, "start": 5338.120000000001, "end": 5344.52, "text": " And the trick seems to be that adding this little sigmoid makes a big difference.", "tokens": [400, 264, 4282, 2544, 281, 312, 300, 5127, 341, 707, 4556, 3280, 327, 1669, 257, 955, 2649, 13], "temperature": 0.0, "avg_logprob": -0.20815150993914644, "compression_ratio": 1.6576923076923078, "no_speech_prob": 1.7231330275535583e-05}, {"id": 1100, "seek": 533360, "start": 5344.52, "end": 5349.92, "text": " And did you have a question?", "tokens": [400, 630, 291, 362, 257, 1168, 30], "temperature": 0.0, "avg_logprob": -0.20815150993914644, "compression_ratio": 1.6576923076923078, "no_speech_prob": 1.7231330275535583e-05}, {"id": 1101, "seek": 533360, "start": 5349.92, "end": 5353.52, "text": " There was a question about how you set up your Vim, and I've already linked here,.vimrc,", "tokens": [821, 390, 257, 1168, 466, 577, 291, 992, 493, 428, 691, 332, 11, 293, 286, 600, 1217, 9408, 510, 11, 2411, 85, 332, 81, 66, 11], "temperature": 0.0, "avg_logprob": -0.20815150993914644, "compression_ratio": 1.6576923076923078, "no_speech_prob": 1.7231330275535583e-05}, {"id": 1102, "seek": 533360, "start": 5353.52, "end": 5356.160000000001, "text": " but I wanted to know if you had more to say about that.", "tokens": [457, 286, 1415, 281, 458, 498, 291, 632, 544, 281, 584, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.20815150993914644, "compression_ratio": 1.6576923076923078, "no_speech_prob": 1.7231330275535583e-05}, {"id": 1103, "seek": 533360, "start": 5356.160000000001, "end": 5357.68, "text": " What do they want to know about Vim?", "tokens": [708, 360, 436, 528, 281, 458, 466, 691, 332, 30], "temperature": 0.0, "avg_logprob": -0.20815150993914644, "compression_ratio": 1.6576923076923078, "no_speech_prob": 1.7231330275535583e-05}, {"id": 1104, "seek": 533360, "start": 5357.68, "end": 5359.160000000001, "text": " They really like your setup.", "tokens": [814, 534, 411, 428, 8657, 13], "temperature": 0.0, "avg_logprob": -0.20815150993914644, "compression_ratio": 1.6576923076923078, "no_speech_prob": 1.7231330275535583e-05}, {"id": 1105, "seek": 533360, "start": 5359.160000000001, "end": 5361.68, "text": " You like my setup?", "tokens": [509, 411, 452, 8657, 30], "temperature": 0.0, "avg_logprob": -0.20815150993914644, "compression_ratio": 1.6576923076923078, "no_speech_prob": 1.7231330275535583e-05}, {"id": 1106, "seek": 536168, "start": 5361.68, "end": 5364.16, "text": " There's almost nothing in my setup.", "tokens": [821, 311, 1920, 1825, 294, 452, 8657, 13], "temperature": 0.0, "avg_logprob": -0.16814756917429494, "compression_ratio": 1.5622119815668203, "no_speech_prob": 6.814650259912014e-05}, {"id": 1107, "seek": 536168, "start": 5364.16, "end": 5370.12, "text": " It's pretty bare, honestly.", "tokens": [467, 311, 1238, 6949, 11, 6095, 13], "temperature": 0.0, "avg_logprob": -0.16814756917429494, "compression_ratio": 1.5622119815668203, "no_speech_prob": 6.814650259912014e-05}, {"id": 1108, "seek": 536168, "start": 5370.12, "end": 5372.780000000001, "text": " Whatever you're doing with your editor, you probably want it to look like this, which", "tokens": [8541, 291, 434, 884, 365, 428, 9839, 11, 291, 1391, 528, 309, 281, 574, 411, 341, 11, 597], "temperature": 0.0, "avg_logprob": -0.16814756917429494, "compression_ratio": 1.5622119815668203, "no_speech_prob": 6.814650259912014e-05}, {"id": 1109, "seek": 536168, "start": 5372.780000000001, "end": 5380.52, "text": " is like when you've got a class that you're not currently working on, it should be closed", "tokens": [307, 411, 562, 291, 600, 658, 257, 1508, 300, 291, 434, 406, 4362, 1364, 322, 11, 309, 820, 312, 5395], "temperature": 0.0, "avg_logprob": -0.16814756917429494, "compression_ratio": 1.5622119815668203, "no_speech_prob": 6.814650259912014e-05}, {"id": 1110, "seek": 536168, "start": 5380.52, "end": 5383.52, "text": " up so you can't see it.", "tokens": [493, 370, 291, 393, 380, 536, 309, 13], "temperature": 0.0, "avg_logprob": -0.16814756917429494, "compression_ratio": 1.5622119815668203, "no_speech_prob": 6.814650259912014e-05}, {"id": 1111, "seek": 536168, "start": 5383.52, "end": 5388.08, "text": " And so you basically want something where it's easy to close and open fold.", "tokens": [400, 370, 291, 1936, 528, 746, 689, 309, 311, 1858, 281, 1998, 293, 1269, 4860, 13], "temperature": 0.0, "avg_logprob": -0.16814756917429494, "compression_ratio": 1.5622119815668203, "no_speech_prob": 6.814650259912014e-05}, {"id": 1112, "seek": 538808, "start": 5388.08, "end": 5392.08, "text": " So Vim already does all this for you.", "tokens": [407, 691, 332, 1217, 775, 439, 341, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.19853084843333174, "compression_ratio": 1.7857142857142858, "no_speech_prob": 2.7108248104923405e-05}, {"id": 1113, "seek": 538808, "start": 5392.08, "end": 5395.18, "text": " And then, as I mentioned, you also want something where you can kind of jump to the definition", "tokens": [400, 550, 11, 382, 286, 2835, 11, 291, 611, 528, 746, 689, 291, 393, 733, 295, 3012, 281, 264, 7123], "temperature": 0.0, "avg_logprob": -0.19853084843333174, "compression_ratio": 1.7857142857142858, "no_speech_prob": 2.7108248104923405e-05}, {"id": 1114, "seek": 538808, "start": 5395.18, "end": 5400.16, "text": " of things, which in Vim it's called using tags.", "tokens": [295, 721, 11, 597, 294, 691, 332, 309, 311, 1219, 1228, 18632, 13], "temperature": 0.0, "avg_logprob": -0.19853084843333174, "compression_ratio": 1.7857142857142858, "no_speech_prob": 2.7108248104923405e-05}, {"id": 1115, "seek": 538808, "start": 5400.16, "end": 5402.72, "text": " So if you want to jump to the definition of learner.", "tokens": [407, 498, 291, 528, 281, 3012, 281, 264, 7123, 295, 33347, 13], "temperature": 0.0, "avg_logprob": -0.19853084843333174, "compression_ratio": 1.7857142857142858, "no_speech_prob": 2.7108248104923405e-05}, {"id": 1116, "seek": 538808, "start": 5402.72, "end": 5404.12, "text": " Basically Vim already does all this for you.", "tokens": [8537, 691, 332, 1217, 775, 439, 341, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.19853084843333174, "compression_ratio": 1.7857142857142858, "no_speech_prob": 2.7108248104923405e-05}, {"id": 1117, "seek": 538808, "start": 5404.12, "end": 5406.76, "text": " You just have to read the instructions.", "tokens": [509, 445, 362, 281, 1401, 264, 9415, 13], "temperature": 0.0, "avg_logprob": -0.19853084843333174, "compression_ratio": 1.7857142857142858, "no_speech_prob": 2.7108248104923405e-05}, {"id": 1118, "seek": 538808, "start": 5406.76, "end": 5407.76, "text": " My Vimrc is minimal.", "tokens": [1222, 691, 332, 81, 66, 307, 13206, 13], "temperature": 0.0, "avg_logprob": -0.19853084843333174, "compression_ratio": 1.7857142857142858, "no_speech_prob": 2.7108248104923405e-05}, {"id": 1119, "seek": 538808, "start": 5407.76, "end": 5412.54, "text": " I basically hardly use any extensions or anything.", "tokens": [286, 1936, 13572, 764, 604, 25129, 420, 1340, 13], "temperature": 0.0, "avg_logprob": -0.19853084843333174, "compression_ratio": 1.7857142857142858, "no_speech_prob": 2.7108248104923405e-05}, {"id": 1120, "seek": 538808, "start": 5412.54, "end": 5416.66, "text": " Another great editor to use is VS Code, Visual Studio Code.", "tokens": [3996, 869, 9839, 281, 764, 307, 25091, 15549, 11, 23187, 13500, 15549, 13], "temperature": 0.0, "avg_logprob": -0.19853084843333174, "compression_ratio": 1.7857142857142858, "no_speech_prob": 2.7108248104923405e-05}, {"id": 1121, "seek": 541666, "start": 5416.66, "end": 5422.72, "text": " It's free and it's awesome and it has all the same features that you're seeing that", "tokens": [467, 311, 1737, 293, 309, 311, 3476, 293, 309, 575, 439, 264, 912, 4122, 300, 291, 434, 2577, 300], "temperature": 0.0, "avg_logprob": -0.15679621696472168, "compression_ratio": 1.6113744075829384, "no_speech_prob": 2.5070907213375904e-05}, {"id": 1122, "seek": 541666, "start": 5422.72, "end": 5423.72, "text": " Vim does.", "tokens": [691, 332, 775, 13], "temperature": 0.0, "avg_logprob": -0.15679621696472168, "compression_ratio": 1.6113744075829384, "no_speech_prob": 2.5070907213375904e-05}, {"id": 1123, "seek": 541666, "start": 5423.72, "end": 5429.639999999999, "text": " Basically VS Code does all of those things as well.", "tokens": [8537, 25091, 15549, 775, 439, 295, 729, 721, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.15679621696472168, "compression_ratio": 1.6113744075829384, "no_speech_prob": 2.5070907213375904e-05}, {"id": 1124, "seek": 541666, "start": 5429.639999999999, "end": 5434.76, "text": " I quite like using Vim because I can use it on the remote machine and play around.", "tokens": [286, 1596, 411, 1228, 691, 332, 570, 286, 393, 764, 309, 322, 264, 8607, 3479, 293, 862, 926, 13], "temperature": 0.0, "avg_logprob": -0.15679621696472168, "compression_ratio": 1.6113744075829384, "no_speech_prob": 2.5070907213375904e-05}, {"id": 1125, "seek": 541666, "start": 5434.76, "end": 5442.72, "text": " But you can of course just clone Git onto your local computer and open it up in VS Code", "tokens": [583, 291, 393, 295, 1164, 445, 26506, 16939, 3911, 428, 2654, 3820, 293, 1269, 309, 493, 294, 25091, 15549], "temperature": 0.0, "avg_logprob": -0.15679621696472168, "compression_ratio": 1.6113744075829384, "no_speech_prob": 2.5070907213375904e-05}, {"id": 1126, "seek": 541666, "start": 5442.72, "end": 5444.36, "text": " to play around with it.", "tokens": [281, 862, 926, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.15679621696472168, "compression_ratio": 1.6113744075829384, "no_speech_prob": 2.5070907213375904e-05}, {"id": 1127, "seek": 544436, "start": 5444.36, "end": 5449.16, "text": " Just don't try and look through the code just on GitHub or something.", "tokens": [1449, 500, 380, 853, 293, 574, 807, 264, 3089, 445, 322, 23331, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.135672703262203, "compression_ratio": 1.6307692307692307, "no_speech_prob": 3.169265255564824e-05}, {"id": 1128, "seek": 544436, "start": 5449.16, "end": 5450.16, "text": " That's going to drive you crazy.", "tokens": [663, 311, 516, 281, 3332, 291, 3219, 13], "temperature": 0.0, "avg_logprob": -0.135672703262203, "compression_ratio": 1.6307692307692307, "no_speech_prob": 3.169265255564824e-05}, {"id": 1129, "seek": 544436, "start": 5450.16, "end": 5455.799999999999, "text": " You need to be able to open it and close it and jump and jump back.", "tokens": [509, 643, 281, 312, 1075, 281, 1269, 309, 293, 1998, 309, 293, 3012, 293, 3012, 646, 13], "temperature": 0.0, "avg_logprob": -0.135672703262203, "compression_ratio": 1.6307692307692307, "no_speech_prob": 3.169265255564824e-05}, {"id": 1130, "seek": 544436, "start": 5455.799999999999, "end": 5461.639999999999, "text": " Maybe people can create some threads on the forum for Vim tips, VS Code tips, Sublime", "tokens": [2704, 561, 393, 1884, 512, 19314, 322, 264, 17542, 337, 691, 332, 6082, 11, 25091, 15549, 6082, 11, 8511, 40941], "temperature": 0.0, "avg_logprob": -0.135672703262203, "compression_ratio": 1.6307692307692307, "no_speech_prob": 3.169265255564824e-05}, {"id": 1131, "seek": 544436, "start": 5461.639999999999, "end": 5464.639999999999, "text": " tips, whatever.", "tokens": [6082, 11, 2035, 13], "temperature": 0.0, "avg_logprob": -0.135672703262203, "compression_ratio": 1.6307692307692307, "no_speech_prob": 3.169265255564824e-05}, {"id": 1132, "seek": 544436, "start": 5464.639999999999, "end": 5469.32, "text": " For me I would say if you're going to pick an editor, if you want to use something on", "tokens": [1171, 385, 286, 576, 584, 498, 291, 434, 516, 281, 1888, 364, 9839, 11, 498, 291, 528, 281, 764, 746, 322], "temperature": 0.0, "avg_logprob": -0.135672703262203, "compression_ratio": 1.6307692307692307, "no_speech_prob": 3.169265255564824e-05}, {"id": 1133, "seek": 544436, "start": 5469.32, "end": 5471.44, "text": " your local, I would go with VS Code today.", "tokens": [428, 2654, 11, 286, 576, 352, 365, 25091, 15549, 965, 13], "temperature": 0.0, "avg_logprob": -0.135672703262203, "compression_ratio": 1.6307692307692307, "no_speech_prob": 3.169265255564824e-05}, {"id": 1134, "seek": 544436, "start": 5471.44, "end": 5472.799999999999, "text": " I think it's the best.", "tokens": [286, 519, 309, 311, 264, 1151, 13], "temperature": 0.0, "avg_logprob": -0.135672703262203, "compression_ratio": 1.6307692307692307, "no_speech_prob": 3.169265255564824e-05}, {"id": 1135, "seek": 547280, "start": 5472.8, "end": 5477.56, "text": " If you want to use something on the terminal side, I would go with Vim or Emacs.", "tokens": [759, 291, 528, 281, 764, 746, 322, 264, 14709, 1252, 11, 286, 576, 352, 365, 691, 332, 420, 3968, 44937, 13], "temperature": 0.0, "avg_logprob": -0.1374569506872268, "compression_ratio": 1.518181818181818, "no_speech_prob": 4.289307526050834e-06}, {"id": 1136, "seek": 547280, "start": 5477.56, "end": 5486.96, "text": " To me they're clear winners.", "tokens": [1407, 385, 436, 434, 1850, 17193, 13], "temperature": 0.0, "avg_logprob": -0.1374569506872268, "compression_ratio": 1.518181818181818, "no_speech_prob": 4.289307526050834e-06}, {"id": 1137, "seek": 547280, "start": 5486.96, "end": 5492.68, "text": " So what I wanted to close with today is to take this collaborative filtering example", "tokens": [407, 437, 286, 1415, 281, 1998, 365, 965, 307, 281, 747, 341, 16555, 30822, 1365], "temperature": 0.0, "avg_logprob": -0.1374569506872268, "compression_ratio": 1.518181818181818, "no_speech_prob": 4.289307526050834e-06}, {"id": 1138, "seek": 547280, "start": 5492.68, "end": 5496.88, "text": " and describe how we're going to build on top of it for the next three lessons to create", "tokens": [293, 6786, 577, 321, 434, 516, 281, 1322, 322, 1192, 295, 309, 337, 264, 958, 1045, 8820, 281, 1884], "temperature": 0.0, "avg_logprob": -0.1374569506872268, "compression_ratio": 1.518181818181818, "no_speech_prob": 4.289307526050834e-06}, {"id": 1139, "seek": 547280, "start": 5496.88, "end": 5499.92, "text": " the more complex neural networks we've been seeing.", "tokens": [264, 544, 3997, 18161, 9590, 321, 600, 668, 2577, 13], "temperature": 0.0, "avg_logprob": -0.1374569506872268, "compression_ratio": 1.518181818181818, "no_speech_prob": 4.289307526050834e-06}, {"id": 1140, "seek": 549992, "start": 5499.92, "end": 5519.2, "text": " So roughly speaking, this is the bunch of concepts that we need to learn about.", "tokens": [407, 9810, 4124, 11, 341, 307, 264, 3840, 295, 10392, 300, 321, 643, 281, 1466, 466, 13], "temperature": 0.0, "avg_logprob": -0.20443262372698104, "compression_ratio": 1.0394736842105263, "no_speech_prob": 1.805813008104451e-05}, {"id": 1141, "seek": 551920, "start": 5519.2, "end": 5531.639999999999, "text": " Let's think about what happens when you're using a neural network to do image recognition.", "tokens": [961, 311, 519, 466, 437, 2314, 562, 291, 434, 1228, 257, 18161, 3209, 281, 360, 3256, 11150, 13], "temperature": 0.0, "avg_logprob": -0.20031708389965455, "compression_ratio": 1.5496688741721854, "no_speech_prob": 6.747976385668153e-06}, {"id": 1142, "seek": 551920, "start": 5531.639999999999, "end": 5534.96, "text": " Basically, let's take a single pixel.", "tokens": [8537, 11, 718, 311, 747, 257, 2167, 19261, 13], "temperature": 0.0, "avg_logprob": -0.20031708389965455, "compression_ratio": 1.5496688741721854, "no_speech_prob": 6.747976385668153e-06}, {"id": 1143, "seek": 551920, "start": 5534.96, "end": 5538.88, "text": " You've got lots of pixels, but let's take a single pixel.", "tokens": [509, 600, 658, 3195, 295, 18668, 11, 457, 718, 311, 747, 257, 2167, 19261, 13], "temperature": 0.0, "avg_logprob": -0.20031708389965455, "compression_ratio": 1.5496688741721854, "no_speech_prob": 6.747976385668153e-06}, {"id": 1144, "seek": 551920, "start": 5538.88, "end": 5547.8, "text": " So you've got a red, a green, and a blue pixel.", "tokens": [407, 291, 600, 658, 257, 2182, 11, 257, 3092, 11, 293, 257, 3344, 19261, 13], "temperature": 0.0, "avg_logprob": -0.20031708389965455, "compression_ratio": 1.5496688741721854, "no_speech_prob": 6.747976385668153e-06}, {"id": 1145, "seek": 554780, "start": 5547.8, "end": 5551.04, "text": " So each one of those is some number between 0 and 255.", "tokens": [407, 1184, 472, 295, 729, 307, 512, 1230, 1296, 1958, 293, 3552, 20, 13], "temperature": 0.0, "avg_logprob": -0.2966288173899931, "compression_ratio": 1.4594594594594594, "no_speech_prob": 6.108647357905284e-05}, {"id": 1146, "seek": 554780, "start": 5551.04, "end": 5557.76, "text": " Or we kind of normalize them so they're a floating point with the mean of 0 and the", "tokens": [1610, 321, 733, 295, 2710, 1125, 552, 370, 436, 434, 257, 12607, 935, 365, 264, 914, 295, 1958, 293, 264], "temperature": 0.0, "avg_logprob": -0.2966288173899931, "compression_ratio": 1.4594594594594594, "no_speech_prob": 6.108647357905284e-05}, {"id": 1147, "seek": 554780, "start": 5557.76, "end": 5558.76, "text": " standard deviation of 1.", "tokens": [3832, 25163, 295, 502, 13], "temperature": 0.0, "avg_logprob": -0.2966288173899931, "compression_ratio": 1.4594594594594594, "no_speech_prob": 6.108647357905284e-05}, {"id": 1148, "seek": 554780, "start": 5558.76, "end": 5563.4400000000005, "text": " But let's just do the 0 to 255 version.", "tokens": [583, 718, 311, 445, 360, 264, 1958, 281, 3552, 20, 3037, 13], "temperature": 0.0, "avg_logprob": -0.2966288173899931, "compression_ratio": 1.4594594594594594, "no_speech_prob": 6.108647357905284e-05}, {"id": 1149, "seek": 554780, "start": 5563.4400000000005, "end": 5573.76, "text": " So there's like 10, 20, 30, whatever.", "tokens": [407, 456, 311, 411, 1266, 11, 945, 11, 2217, 11, 2035, 13], "temperature": 0.0, "avg_logprob": -0.2966288173899931, "compression_ratio": 1.4594594594594594, "no_speech_prob": 6.108647357905284e-05}, {"id": 1150, "seek": 554780, "start": 5573.76, "end": 5575.88, "text": " So what do we do with these?", "tokens": [407, 437, 360, 321, 360, 365, 613, 30], "temperature": 0.0, "avg_logprob": -0.2966288173899931, "compression_ratio": 1.4594594594594594, "no_speech_prob": 6.108647357905284e-05}, {"id": 1151, "seek": 557588, "start": 5575.88, "end": 5586.2, "text": " What we do is we basically treat that as a vector and we multiply it by a matrix.", "tokens": [708, 321, 360, 307, 321, 1936, 2387, 300, 382, 257, 8062, 293, 321, 12972, 309, 538, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.18553736672472598, "compression_ratio": 1.54375, "no_speech_prob": 5.862782018084545e-06}, {"id": 1152, "seek": 557588, "start": 5586.2, "end": 5591.28, "text": " So this matrix, depending on how you think of the rows and the columns, let's treat the", "tokens": [407, 341, 8141, 11, 5413, 322, 577, 291, 519, 295, 264, 13241, 293, 264, 13766, 11, 718, 311, 2387, 264], "temperature": 0.0, "avg_logprob": -0.18553736672472598, "compression_ratio": 1.54375, "no_speech_prob": 5.862782018084545e-06}, {"id": 1153, "seek": 557588, "start": 5591.28, "end": 5597.04, "text": " matrix as having three rows and then how many columns?", "tokens": [8141, 382, 1419, 1045, 13241, 293, 550, 577, 867, 13766, 30], "temperature": 0.0, "avg_logprob": -0.18553736672472598, "compression_ratio": 1.54375, "no_speech_prob": 5.862782018084545e-06}, {"id": 1154, "seek": 557588, "start": 5597.04, "end": 5599.52, "text": " Well, you get to pick.", "tokens": [1042, 11, 291, 483, 281, 1888, 13], "temperature": 0.0, "avg_logprob": -0.18553736672472598, "compression_ratio": 1.54375, "no_speech_prob": 5.862782018084545e-06}, {"id": 1155, "seek": 559952, "start": 5599.52, "end": 5606.56, "text": " You get to pick, just like with the collaborative filtering version, I decided to pick a vector", "tokens": [509, 483, 281, 1888, 11, 445, 411, 365, 264, 16555, 30822, 3037, 11, 286, 3047, 281, 1888, 257, 8062], "temperature": 0.0, "avg_logprob": -0.15568414954252022, "compression_ratio": 1.5654450261780104, "no_speech_prob": 1.3287749425217044e-06}, {"id": 1156, "seek": 559952, "start": 5606.56, "end": 5610.320000000001, "text": " of size 5 for each of my embedding vectors.", "tokens": [295, 2744, 1025, 337, 1184, 295, 452, 12240, 3584, 18875, 13], "temperature": 0.0, "avg_logprob": -0.15568414954252022, "compression_ratio": 1.5654450261780104, "no_speech_prob": 1.3287749425217044e-06}, {"id": 1157, "seek": 559952, "start": 5610.320000000001, "end": 5615.64, "text": " So that would mean that that's an embedding basically of size 5.", "tokens": [407, 300, 576, 914, 300, 300, 311, 364, 12240, 3584, 1936, 295, 2744, 1025, 13], "temperature": 0.0, "avg_logprob": -0.15568414954252022, "compression_ratio": 1.5654450261780104, "no_speech_prob": 1.3287749425217044e-06}, {"id": 1158, "seek": 559952, "start": 5615.64, "end": 5617.8, "text": " You can get to pick how big your weight matrix is.", "tokens": [509, 393, 483, 281, 1888, 577, 955, 428, 3364, 8141, 307, 13], "temperature": 0.0, "avg_logprob": -0.15568414954252022, "compression_ratio": 1.5654450261780104, "no_speech_prob": 1.3287749425217044e-06}, {"id": 1159, "seek": 559952, "start": 5617.8, "end": 5621.56, "text": " So let's make it size 5.", "tokens": [407, 718, 311, 652, 309, 2744, 1025, 13], "temperature": 0.0, "avg_logprob": -0.15568414954252022, "compression_ratio": 1.5654450261780104, "no_speech_prob": 1.3287749425217044e-06}, {"id": 1160, "seek": 559952, "start": 5621.56, "end": 5626.4400000000005, "text": " So this is 3 by 5.", "tokens": [407, 341, 307, 805, 538, 1025, 13], "temperature": 0.0, "avg_logprob": -0.15568414954252022, "compression_ratio": 1.5654450261780104, "no_speech_prob": 1.3287749425217044e-06}, {"id": 1161, "seek": 562644, "start": 5626.44, "end": 5630.599999999999, "text": " So initially this weight matrix contains random numbers.", "tokens": [407, 9105, 341, 3364, 8141, 8306, 4974, 3547, 13], "temperature": 0.0, "avg_logprob": -0.17748496444137007, "compression_ratio": 1.831541218637993, "no_speech_prob": 1.280538799619535e-05}, {"id": 1162, "seek": 562644, "start": 5630.599999999999, "end": 5634.639999999999, "text": " Remember when we looked up get embedding weight matrix just now and there were like two lines.", "tokens": [5459, 562, 321, 2956, 493, 483, 12240, 3584, 3364, 8141, 445, 586, 293, 456, 645, 411, 732, 3876, 13], "temperature": 0.0, "avg_logprob": -0.17748496444137007, "compression_ratio": 1.831541218637993, "no_speech_prob": 1.280538799619535e-05}, {"id": 1163, "seek": 562644, "start": 5634.639999999999, "end": 5638.96, "text": " The first line was like create the matrix and the second was fill it with random numbers.", "tokens": [440, 700, 1622, 390, 411, 1884, 264, 8141, 293, 264, 1150, 390, 2836, 309, 365, 4974, 3547, 13], "temperature": 0.0, "avg_logprob": -0.17748496444137007, "compression_ratio": 1.831541218637993, "no_speech_prob": 1.280538799619535e-05}, {"id": 1164, "seek": 562644, "start": 5638.96, "end": 5639.96, "text": " That's what we do.", "tokens": [663, 311, 437, 321, 360, 13], "temperature": 0.0, "avg_logprob": -0.17748496444137007, "compression_ratio": 1.831541218637993, "no_speech_prob": 1.280538799619535e-05}, {"id": 1165, "seek": 562644, "start": 5639.96, "end": 5643.919999999999, "text": " It all gets hidden behind the scenes by fastai and PyTorch.", "tokens": [467, 439, 2170, 7633, 2261, 264, 8026, 538, 2370, 1301, 293, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.17748496444137007, "compression_ratio": 1.831541218637993, "no_speech_prob": 1.280538799619535e-05}, {"id": 1166, "seek": 562644, "start": 5643.919999999999, "end": 5644.919999999999, "text": " That's all it's doing.", "tokens": [663, 311, 439, 309, 311, 884, 13], "temperature": 0.0, "avg_logprob": -0.17748496444137007, "compression_ratio": 1.831541218637993, "no_speech_prob": 1.280538799619535e-05}, {"id": 1167, "seek": 562644, "start": 5644.919999999999, "end": 5650.48, "text": " It's creating a matrix of random numbers when you set it up.", "tokens": [467, 311, 4084, 257, 8141, 295, 4974, 3547, 562, 291, 992, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.17748496444137007, "compression_ratio": 1.831541218637993, "no_speech_prob": 1.280538799619535e-05}, {"id": 1168, "seek": 562644, "start": 5650.48, "end": 5654.839999999999, "text": " And the number of rows has to be 3 to match the input and the number of columns can be", "tokens": [400, 264, 1230, 295, 13241, 575, 281, 312, 805, 281, 2995, 264, 4846, 293, 264, 1230, 295, 13766, 393, 312], "temperature": 0.0, "avg_logprob": -0.17748496444137007, "compression_ratio": 1.831541218637993, "no_speech_prob": 1.280538799619535e-05}, {"id": 1169, "seek": 562644, "start": 5654.839999999999, "end": 5656.04, "text": " as big as you like.", "tokens": [382, 955, 382, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.17748496444137007, "compression_ratio": 1.831541218637993, "no_speech_prob": 1.280538799619535e-05}, {"id": 1170, "seek": 565604, "start": 5656.04, "end": 5660.2, "text": " And so after you multiply the vector, the input vector by that weight matrix, you're", "tokens": [400, 370, 934, 291, 12972, 264, 8062, 11, 264, 4846, 8062, 538, 300, 3364, 8141, 11, 291, 434], "temperature": 0.0, "avg_logprob": -0.13789227668275225, "compression_ratio": 1.563063063063063, "no_speech_prob": 4.710888788395096e-06}, {"id": 1171, "seek": 565604, "start": 5660.2, "end": 5669.28, "text": " going to end up with a vector of size 5.", "tokens": [516, 281, 917, 493, 365, 257, 8062, 295, 2744, 1025, 13], "temperature": 0.0, "avg_logprob": -0.13789227668275225, "compression_ratio": 1.563063063063063, "no_speech_prob": 4.710888788395096e-06}, {"id": 1172, "seek": 565604, "start": 5669.28, "end": 5675.74, "text": " So people often ask like how much linear algebra do I need to know to be able to do deep learning?", "tokens": [407, 561, 2049, 1029, 411, 577, 709, 8213, 21989, 360, 286, 643, 281, 458, 281, 312, 1075, 281, 360, 2452, 2539, 30], "temperature": 0.0, "avg_logprob": -0.13789227668275225, "compression_ratio": 1.563063063063063, "no_speech_prob": 4.710888788395096e-06}, {"id": 1173, "seek": 565604, "start": 5675.74, "end": 5677.56, "text": " This is the amount you need.", "tokens": [639, 307, 264, 2372, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.13789227668275225, "compression_ratio": 1.563063063063063, "no_speech_prob": 4.710888788395096e-06}, {"id": 1174, "seek": 565604, "start": 5677.56, "end": 5681.28, "text": " So and if you're not familiar with this, that's fine.", "tokens": [407, 293, 498, 291, 434, 406, 4963, 365, 341, 11, 300, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.13789227668275225, "compression_ratio": 1.563063063063063, "no_speech_prob": 4.710888788395096e-06}, {"id": 1175, "seek": 565604, "start": 5681.28, "end": 5683.72, "text": " You need to know about matrix products.", "tokens": [509, 643, 281, 458, 466, 8141, 3383, 13], "temperature": 0.0, "avg_logprob": -0.13789227668275225, "compression_ratio": 1.563063063063063, "no_speech_prob": 4.710888788395096e-06}, {"id": 1176, "seek": 568372, "start": 5683.72, "end": 5686.2, "text": " You don't need to know a lot about them.", "tokens": [509, 500, 380, 643, 281, 458, 257, 688, 466, 552, 13], "temperature": 0.0, "avg_logprob": -0.16403449584390514, "compression_ratio": 1.669683257918552, "no_speech_prob": 5.862777015863685e-06}, {"id": 1177, "seek": 568372, "start": 5686.2, "end": 5690.88, "text": " You just need to know like computationally what are they?", "tokens": [509, 445, 643, 281, 458, 411, 24903, 379, 437, 366, 436, 30], "temperature": 0.0, "avg_logprob": -0.16403449584390514, "compression_ratio": 1.669683257918552, "no_speech_prob": 5.862777015863685e-06}, {"id": 1178, "seek": 568372, "start": 5690.88, "end": 5692.2, "text": " What do they do?", "tokens": [708, 360, 436, 360, 30], "temperature": 0.0, "avg_logprob": -0.16403449584390514, "compression_ratio": 1.669683257918552, "no_speech_prob": 5.862777015863685e-06}, {"id": 1179, "seek": 568372, "start": 5692.2, "end": 5697.2, "text": " And you've got to be very comfortable with like if a matrix of size blah times a matrix", "tokens": [400, 291, 600, 658, 281, 312, 588, 4619, 365, 411, 498, 257, 8141, 295, 2744, 12288, 1413, 257, 8141], "temperature": 0.0, "avg_logprob": -0.16403449584390514, "compression_ratio": 1.669683257918552, "no_speech_prob": 5.862777015863685e-06}, {"id": 1180, "seek": 568372, "start": 5697.2, "end": 5699.72, "text": " of size blah gives a matrix of size blah.", "tokens": [295, 2744, 12288, 2709, 257, 8141, 295, 2744, 12288, 13], "temperature": 0.0, "avg_logprob": -0.16403449584390514, "compression_ratio": 1.669683257918552, "no_speech_prob": 5.862777015863685e-06}, {"id": 1181, "seek": 568372, "start": 5699.72, "end": 5702.0, "text": " Like how do the dimensions match up?", "tokens": [1743, 577, 360, 264, 12819, 2995, 493, 30], "temperature": 0.0, "avg_logprob": -0.16403449584390514, "compression_ratio": 1.669683257918552, "no_speech_prob": 5.862777015863685e-06}, {"id": 1182, "seek": 568372, "start": 5702.0, "end": 5710.240000000001, "text": " So if you have 3 and then remember in NumPy and PyTorch we use at times 3 by 5 gives a", "tokens": [407, 498, 291, 362, 805, 293, 550, 1604, 294, 22592, 47, 88, 293, 9953, 51, 284, 339, 321, 764, 412, 1413, 805, 538, 1025, 2709, 257], "temperature": 0.0, "avg_logprob": -0.16403449584390514, "compression_ratio": 1.669683257918552, "no_speech_prob": 5.862777015863685e-06}, {"id": 1183, "seek": 571024, "start": 5710.24, "end": 5713.88, "text": " vector of size 5.", "tokens": [8062, 295, 2744, 1025, 13], "temperature": 0.0, "avg_logprob": -0.13582143147786457, "compression_ratio": 1.558659217877095, "no_speech_prob": 2.4299924916704185e-05}, {"id": 1184, "seek": 571024, "start": 5713.88, "end": 5715.639999999999, "text": " And then what happens next?", "tokens": [400, 550, 437, 2314, 958, 30], "temperature": 0.0, "avg_logprob": -0.13582143147786457, "compression_ratio": 1.558659217877095, "no_speech_prob": 2.4299924916704185e-05}, {"id": 1185, "seek": 571024, "start": 5715.639999999999, "end": 5728.12, "text": " It goes through an activation function such as ReLU which is just max 0, x and spits out", "tokens": [467, 1709, 807, 364, 24433, 2445, 1270, 382, 1300, 43, 52, 597, 307, 445, 11469, 1958, 11, 2031, 293, 637, 1208, 484], "temperature": 0.0, "avg_logprob": -0.13582143147786457, "compression_ratio": 1.558659217877095, "no_speech_prob": 2.4299924916704185e-05}, {"id": 1186, "seek": 571024, "start": 5728.12, "end": 5735.5199999999995, "text": " a new vector which is of course going to be exactly the same size because no activation", "tokens": [257, 777, 8062, 597, 307, 295, 1164, 516, 281, 312, 2293, 264, 912, 2744, 570, 572, 24433], "temperature": 0.0, "avg_logprob": -0.13582143147786457, "compression_ratio": 1.558659217877095, "no_speech_prob": 2.4299924916704185e-05}, {"id": 1187, "seek": 571024, "start": 5735.5199999999995, "end": 5738.179999999999, "text": " function changes the size.", "tokens": [2445, 2962, 264, 2744, 13], "temperature": 0.0, "avg_logprob": -0.13582143147786457, "compression_ratio": 1.558659217877095, "no_speech_prob": 2.4299924916704185e-05}, {"id": 1188, "seek": 571024, "start": 5738.179999999999, "end": 5739.8, "text": " It only changes the contents.", "tokens": [467, 787, 2962, 264, 15768, 13], "temperature": 0.0, "avg_logprob": -0.13582143147786457, "compression_ratio": 1.558659217877095, "no_speech_prob": 2.4299924916704185e-05}, {"id": 1189, "seek": 573980, "start": 5739.8, "end": 5744.7, "text": " So that's still of size 5.", "tokens": [407, 300, 311, 920, 295, 2744, 1025, 13], "temperature": 0.0, "avg_logprob": -0.16824097700521978, "compression_ratio": 1.4181818181818182, "no_speech_prob": 1.5936462659738027e-05}, {"id": 1190, "seek": 573980, "start": 5744.7, "end": 5746.4800000000005, "text": " What happens next?", "tokens": [708, 2314, 958, 30], "temperature": 0.0, "avg_logprob": -0.16824097700521978, "compression_ratio": 1.4181818181818182, "no_speech_prob": 1.5936462659738027e-05}, {"id": 1191, "seek": 573980, "start": 5746.4800000000005, "end": 5752.64, "text": " We multiply it by another matrix and again it can be any number of columns but the number", "tokens": [492, 12972, 309, 538, 1071, 8141, 293, 797, 309, 393, 312, 604, 1230, 295, 13766, 457, 264, 1230], "temperature": 0.0, "avg_logprob": -0.16824097700521978, "compression_ratio": 1.4181818181818182, "no_speech_prob": 1.5936462659738027e-05}, {"id": 1192, "seek": 573980, "start": 5752.64, "end": 5754.5, "text": " of rows has to match nicely.", "tokens": [295, 13241, 575, 281, 2995, 9594, 13], "temperature": 0.0, "avg_logprob": -0.16824097700521978, "compression_ratio": 1.4181818181818182, "no_speech_prob": 1.5936462659738027e-05}, {"id": 1193, "seek": 573980, "start": 5754.5, "end": 5760.68, "text": " So it's going to be 5 by whatever.", "tokens": [407, 309, 311, 516, 281, 312, 1025, 538, 2035, 13], "temperature": 0.0, "avg_logprob": -0.16824097700521978, "compression_ratio": 1.4181818181818182, "no_speech_prob": 1.5936462659738027e-05}, {"id": 1194, "seek": 573980, "start": 5760.68, "end": 5767.58, "text": " So maybe this one has 5 say by 10.", "tokens": [407, 1310, 341, 472, 575, 1025, 584, 538, 1266, 13], "temperature": 0.0, "avg_logprob": -0.16824097700521978, "compression_ratio": 1.4181818181818182, "no_speech_prob": 1.5936462659738027e-05}, {"id": 1195, "seek": 576758, "start": 5767.58, "end": 5777.32, "text": " And so that's going to give some output which will be size 10 and again we put that through", "tokens": [400, 370, 300, 311, 516, 281, 976, 512, 5598, 597, 486, 312, 2744, 1266, 293, 797, 321, 829, 300, 807], "temperature": 0.0, "avg_logprob": -0.21723172324044362, "compression_ratio": 1.5, "no_speech_prob": 2.8573024337674724e-06}, {"id": 1196, "seek": 576758, "start": 5777.32, "end": 5784.0, "text": " ReLU and again that gives us something of the same size.", "tokens": [1300, 43, 52, 293, 797, 300, 2709, 505, 746, 295, 264, 912, 2744, 13], "temperature": 0.0, "avg_logprob": -0.21723172324044362, "compression_ratio": 1.5, "no_speech_prob": 2.8573024337674724e-06}, {"id": 1197, "seek": 576758, "start": 5784.0, "end": 5791.32, "text": " And then we can put that through another matrix.", "tokens": [400, 550, 321, 393, 829, 300, 807, 1071, 8141, 13], "temperature": 0.0, "avg_logprob": -0.21723172324044362, "compression_ratio": 1.5, "no_speech_prob": 2.8573024337674724e-06}, {"id": 1198, "seek": 576758, "start": 5791.32, "end": 5793.5199999999995, "text": " Just to make this a bit clearer, you'll see why in a moment.", "tokens": [1449, 281, 652, 341, 257, 857, 26131, 11, 291, 603, 536, 983, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.21723172324044362, "compression_ratio": 1.5, "no_speech_prob": 2.8573024337674724e-06}, {"id": 1199, "seek": 579352, "start": 5793.52, "end": 5800.6, "text": " I'm going to use 8 not 10.", "tokens": [286, 478, 516, 281, 764, 1649, 406, 1266, 13], "temperature": 0.0, "avg_logprob": -0.12778462785663028, "compression_ratio": 1.4294871794871795, "no_speech_prob": 5.4221900427364744e-06}, {"id": 1200, "seek": 579352, "start": 5800.6, "end": 5802.8, "text": " Let's say we're doing digit recognition.", "tokens": [961, 311, 584, 321, 434, 884, 14293, 11150, 13], "temperature": 0.0, "avg_logprob": -0.12778462785663028, "compression_ratio": 1.4294871794871795, "no_speech_prob": 5.4221900427364744e-06}, {"id": 1201, "seek": 579352, "start": 5802.8, "end": 5805.68, "text": " So there are 10 possible digits.", "tokens": [407, 456, 366, 1266, 1944, 27011, 13], "temperature": 0.0, "avg_logprob": -0.12778462785663028, "compression_ratio": 1.4294871794871795, "no_speech_prob": 5.4221900427364744e-06}, {"id": 1202, "seek": 579352, "start": 5805.68, "end": 5816.96, "text": " So my last weight matrix has to be 10 in size because then that's going to mean my final", "tokens": [407, 452, 1036, 3364, 8141, 575, 281, 312, 1266, 294, 2744, 570, 550, 300, 311, 516, 281, 914, 452, 2572], "temperature": 0.0, "avg_logprob": -0.12778462785663028, "compression_ratio": 1.4294871794871795, "no_speech_prob": 5.4221900427364744e-06}, {"id": 1203, "seek": 579352, "start": 5816.96, "end": 5821.080000000001, "text": " output is a vector of 10 in size.", "tokens": [5598, 307, 257, 8062, 295, 1266, 294, 2744, 13], "temperature": 0.0, "avg_logprob": -0.12778462785663028, "compression_ratio": 1.4294871794871795, "no_speech_prob": 5.4221900427364744e-06}, {"id": 1204, "seek": 582108, "start": 5821.08, "end": 5824.68, "text": " But remember if we're doing digit recognition, what happens?", "tokens": [583, 1604, 498, 321, 434, 884, 14293, 11150, 11, 437, 2314, 30], "temperature": 0.0, "avg_logprob": -0.17610836029052734, "compression_ratio": 1.5933333333333333, "no_speech_prob": 7.071847448969493e-06}, {"id": 1205, "seek": 582108, "start": 5824.68, "end": 5837.04, "text": " We take our actuals, which is 10 in size, and if the number that we're trying to predict", "tokens": [492, 747, 527, 3539, 82, 11, 597, 307, 1266, 294, 2744, 11, 293, 498, 264, 1230, 300, 321, 434, 1382, 281, 6069], "temperature": 0.0, "avg_logprob": -0.17610836029052734, "compression_ratio": 1.5933333333333333, "no_speech_prob": 7.071847448969493e-06}, {"id": 1206, "seek": 582108, "start": 5837.04, "end": 5846.4, "text": " was the number 3, that's the thing we're trying to predict, then that means that there is", "tokens": [390, 264, 1230, 805, 11, 300, 311, 264, 551, 321, 434, 1382, 281, 6069, 11, 550, 300, 1355, 300, 456, 307], "temperature": 0.0, "avg_logprob": -0.17610836029052734, "compression_ratio": 1.5933333333333333, "no_speech_prob": 7.071847448969493e-06}, {"id": 1207, "seek": 584640, "start": 5846.4, "end": 5855.36, "text": " a 3, 0, 0, 0 in the third position.", "tokens": [257, 805, 11, 1958, 11, 1958, 11, 1958, 294, 264, 2636, 2535, 13], "temperature": 0.0, "avg_logprob": -0.1747607882060702, "compression_ratio": 1.5766423357664234, "no_speech_prob": 4.029433057439746e-06}, {"id": 1208, "seek": 584640, "start": 5855.36, "end": 5865.839999999999, "text": " So what happens is our neural net runs along, starting with our input, weight matrix, ReLU,", "tokens": [407, 437, 2314, 307, 527, 18161, 2533, 6676, 2051, 11, 2891, 365, 527, 4846, 11, 3364, 8141, 11, 1300, 43, 52, 11], "temperature": 0.0, "avg_logprob": -0.1747607882060702, "compression_ratio": 1.5766423357664234, "no_speech_prob": 4.029433057439746e-06}, {"id": 1209, "seek": 584640, "start": 5865.839999999999, "end": 5875.82, "text": " weight matrix, ReLU, weight matrix, final output, and then we compare these two together", "tokens": [3364, 8141, 11, 1300, 43, 52, 11, 3364, 8141, 11, 2572, 5598, 11, 293, 550, 321, 6794, 613, 732, 1214], "temperature": 0.0, "avg_logprob": -0.1747607882060702, "compression_ratio": 1.5766423357664234, "no_speech_prob": 4.029433057439746e-06}, {"id": 1210, "seek": 587582, "start": 5875.82, "end": 5879.639999999999, "text": " to see how close they are, how close they match using some loss function.", "tokens": [281, 536, 577, 1998, 436, 366, 11, 577, 1998, 436, 2995, 1228, 512, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.17116664785199462, "compression_ratio": 1.8137651821862348, "no_speech_prob": 1.750276715029031e-05}, {"id": 1211, "seek": 587582, "start": 5879.639999999999, "end": 5882.719999999999, "text": " We'll learn about all the loss functions that we use next week.", "tokens": [492, 603, 1466, 466, 439, 264, 4470, 6828, 300, 321, 764, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.17116664785199462, "compression_ratio": 1.8137651821862348, "no_speech_prob": 1.750276715029031e-05}, {"id": 1212, "seek": 587582, "start": 5882.719999999999, "end": 5886.799999999999, "text": " For now the only one we've learned is mean squared error.", "tokens": [1171, 586, 264, 787, 472, 321, 600, 3264, 307, 914, 8889, 6713, 13], "temperature": 0.0, "avg_logprob": -0.17116664785199462, "compression_ratio": 1.8137651821862348, "no_speech_prob": 1.750276715029031e-05}, {"id": 1213, "seek": 587582, "start": 5886.799999999999, "end": 5894.4, "text": " And we compare the actual, you can think of them as probabilities for each of the 10 to", "tokens": [400, 321, 6794, 264, 3539, 11, 291, 393, 519, 295, 552, 382, 33783, 337, 1184, 295, 264, 1266, 281], "temperature": 0.0, "avg_logprob": -0.17116664785199462, "compression_ratio": 1.8137651821862348, "no_speech_prob": 1.750276715029031e-05}, {"id": 1214, "seek": 587582, "start": 5894.4, "end": 5899.48, "text": " the actual each of the 10 to get a loss, and then we find the gradients of every one of", "tokens": [264, 3539, 1184, 295, 264, 1266, 281, 483, 257, 4470, 11, 293, 550, 321, 915, 264, 2771, 2448, 295, 633, 472, 295], "temperature": 0.0, "avg_logprob": -0.17116664785199462, "compression_ratio": 1.8137651821862348, "no_speech_prob": 1.750276715029031e-05}, {"id": 1215, "seek": 587582, "start": 5899.48, "end": 5903.48, "text": " the weight matrices with respect to that, and we update the weight matrices.", "tokens": [264, 3364, 32284, 365, 3104, 281, 300, 11, 293, 321, 5623, 264, 3364, 32284, 13], "temperature": 0.0, "avg_logprob": -0.17116664785199462, "compression_ratio": 1.8137651821862348, "no_speech_prob": 1.750276715029031e-05}, {"id": 1216, "seek": 590348, "start": 5903.48, "end": 5907.759999999999, "text": " So the main thing I wanted to show right now is the terminology we use because it's really", "tokens": [407, 264, 2135, 551, 286, 1415, 281, 855, 558, 586, 307, 264, 27575, 321, 764, 570, 309, 311, 534], "temperature": 0.0, "avg_logprob": -0.16365123862650857, "compression_ratio": 1.53551912568306, "no_speech_prob": 7.411248589050956e-06}, {"id": 1217, "seek": 590348, "start": 5907.759999999999, "end": 5911.2, "text": " important.", "tokens": [1021, 13], "temperature": 0.0, "avg_logprob": -0.16365123862650857, "compression_ratio": 1.53551912568306, "no_speech_prob": 7.411248589050956e-06}, {"id": 1218, "seek": 590348, "start": 5911.2, "end": 5916.719999999999, "text": " These things contain numbers.", "tokens": [1981, 721, 5304, 3547, 13], "temperature": 0.0, "avg_logprob": -0.16365123862650857, "compression_ratio": 1.53551912568306, "no_speech_prob": 7.411248589050956e-06}, {"id": 1219, "seek": 590348, "start": 5916.719999999999, "end": 5920.639999999999, "text": " Specifically they initially are matrices containing random numbers.", "tokens": [26058, 436, 9105, 366, 32284, 19273, 4974, 3547, 13], "temperature": 0.0, "avg_logprob": -0.16365123862650857, "compression_ratio": 1.53551912568306, "no_speech_prob": 7.411248589050956e-06}, {"id": 1220, "seek": 590348, "start": 5920.639999999999, "end": 5931.919999999999, "text": " And we can refer to these yellow things as, in PyTorch they're called parameters.", "tokens": [400, 321, 393, 2864, 281, 613, 5566, 721, 382, 11, 294, 9953, 51, 284, 339, 436, 434, 1219, 9834, 13], "temperature": 0.0, "avg_logprob": -0.16365123862650857, "compression_ratio": 1.53551912568306, "no_speech_prob": 7.411248589050956e-06}, {"id": 1221, "seek": 593192, "start": 5931.92, "end": 5938.4800000000005, "text": " Terms will refer to them as weights, although weights is slightly less accurate because", "tokens": [19835, 82, 486, 2864, 281, 552, 382, 17443, 11, 4878, 17443, 307, 4748, 1570, 8559, 570], "temperature": 0.0, "avg_logprob": -0.20264020684647233, "compression_ratio": 1.5555555555555556, "no_speech_prob": 5.173891622689553e-06}, {"id": 1222, "seek": 593192, "start": 5938.4800000000005, "end": 5940.04, "text": " they can also be biases.", "tokens": [436, 393, 611, 312, 32152, 13], "temperature": 0.0, "avg_logprob": -0.20264020684647233, "compression_ratio": 1.5555555555555556, "no_speech_prob": 5.173891622689553e-06}, {"id": 1223, "seek": 593192, "start": 5940.04, "end": 5944.32, "text": " But we kind of use the terms a little bit interchangeably, but strictly speaking we", "tokens": [583, 321, 733, 295, 764, 264, 2115, 257, 707, 857, 30358, 1188, 11, 457, 20792, 4124, 321], "temperature": 0.0, "avg_logprob": -0.20264020684647233, "compression_ratio": 1.5555555555555556, "no_speech_prob": 5.173891622689553e-06}, {"id": 1224, "seek": 593192, "start": 5944.32, "end": 5946.6, "text": " should call them parameters.", "tokens": [820, 818, 552, 9834, 13], "temperature": 0.0, "avg_logprob": -0.20264020684647233, "compression_ratio": 1.5555555555555556, "no_speech_prob": 5.173891622689553e-06}, {"id": 1225, "seek": 593192, "start": 5946.6, "end": 5952.76, "text": " And then after each of those matrix products, that calculates a vector of numbers.", "tokens": [400, 550, 934, 1184, 295, 729, 8141, 3383, 11, 300, 4322, 1024, 257, 8062, 295, 3547, 13], "temperature": 0.0, "avg_logprob": -0.20264020684647233, "compression_ratio": 1.5555555555555556, "no_speech_prob": 5.173891622689553e-06}, {"id": 1226, "seek": 595276, "start": 5952.76, "end": 5966.56, "text": " So here are some numbers that are calculated by a weight matrix, multiply, and then there", "tokens": [407, 510, 366, 512, 3547, 300, 366, 15598, 538, 257, 3364, 8141, 11, 12972, 11, 293, 550, 456], "temperature": 0.0, "avg_logprob": -0.2584218061887301, "compression_ratio": 1.5984251968503937, "no_speech_prob": 6.0488937378977425e-06}, {"id": 1227, "seek": 595276, "start": 5966.56, "end": 5970.6, "text": " are some other sets of numbers that are calculated as a result of a ReLU, as a result of the", "tokens": [366, 512, 661, 6352, 295, 3547, 300, 366, 15598, 382, 257, 1874, 295, 257, 1300, 43, 52, 11, 382, 257, 1874, 295, 264], "temperature": 0.0, "avg_logprob": -0.2584218061887301, "compression_ratio": 1.5984251968503937, "no_speech_prob": 6.0488937378977425e-06}, {"id": 1228, "seek": 595276, "start": 5970.6, "end": 5975.360000000001, "text": " activation function.", "tokens": [24433, 2445, 13], "temperature": 0.0, "avg_logprob": -0.2584218061887301, "compression_ratio": 1.5984251968503937, "no_speech_prob": 6.0488937378977425e-06}, {"id": 1229, "seek": 597536, "start": 5975.36, "end": 5992.48, "text": " Either one is called activations.", "tokens": [13746, 472, 307, 1219, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.2815893107447131, "compression_ratio": 1.2093023255813953, "no_speech_prob": 5.682405117113376e-06}, {"id": 1230, "seek": 597536, "start": 5992.48, "end": 5997.16, "text": " So activations and parameters both refer to numbers.", "tokens": [407, 2430, 763, 293, 9834, 1293, 2864, 281, 3547, 13], "temperature": 0.0, "avg_logprob": -0.2815893107447131, "compression_ratio": 1.2093023255813953, "no_speech_prob": 5.682405117113376e-06}, {"id": 1231, "seek": 597536, "start": 5997.16, "end": 5998.16, "text": " They are numbers.", "tokens": [814, 366, 3547, 13], "temperature": 0.0, "avg_logprob": -0.2815893107447131, "compression_ratio": 1.2093023255813953, "no_speech_prob": 5.682405117113376e-06}, {"id": 1232, "seek": 599816, "start": 5998.16, "end": 6006.68, "text": " The parameters are numbers that are stored, they're used to make a calculation.", "tokens": [440, 9834, 366, 3547, 300, 366, 12187, 11, 436, 434, 1143, 281, 652, 257, 17108, 13], "temperature": 0.0, "avg_logprob": -0.11008749008178711, "compression_ratio": 1.7261146496815287, "no_speech_prob": 1.3081742054055212e-06}, {"id": 1233, "seek": 599816, "start": 6006.68, "end": 6011.84, "text": " Activations are the result of a calculation, they're numbers that are calculated.", "tokens": [28550, 763, 366, 264, 1874, 295, 257, 17108, 11, 436, 434, 3547, 300, 366, 15598, 13], "temperature": 0.0, "avg_logprob": -0.11008749008178711, "compression_ratio": 1.7261146496815287, "no_speech_prob": 1.3081742054055212e-06}, {"id": 1234, "seek": 599816, "start": 6011.84, "end": 6014.36, "text": " So they're the two key things you need to remember.", "tokens": [407, 436, 434, 264, 732, 2141, 721, 291, 643, 281, 1604, 13], "temperature": 0.0, "avg_logprob": -0.11008749008178711, "compression_ratio": 1.7261146496815287, "no_speech_prob": 1.3081742054055212e-06}, {"id": 1235, "seek": 599816, "start": 6014.36, "end": 6025.12, "text": " So use these terms and use them correctly and accurately.", "tokens": [407, 764, 613, 2115, 293, 764, 552, 8944, 293, 20095, 13], "temperature": 0.0, "avg_logprob": -0.11008749008178711, "compression_ratio": 1.7261146496815287, "no_speech_prob": 1.3081742054055212e-06}, {"id": 1236, "seek": 602512, "start": 6025.12, "end": 6030.32, "text": " And if you read these terms, they mean these very specific things, so don't mix them up", "tokens": [400, 498, 291, 1401, 613, 2115, 11, 436, 914, 613, 588, 2685, 721, 11, 370, 500, 380, 2890, 552, 493], "temperature": 0.0, "avg_logprob": -0.15062630971272786, "compression_ratio": 1.7511737089201878, "no_speech_prob": 1.952573256858159e-05}, {"id": 1237, "seek": 602512, "start": 6030.32, "end": 6031.32, "text": " in your head.", "tokens": [294, 428, 1378, 13], "temperature": 0.0, "avg_logprob": -0.15062630971272786, "compression_ratio": 1.7511737089201878, "no_speech_prob": 1.952573256858159e-05}, {"id": 1238, "seek": 602512, "start": 6031.32, "end": 6035.8, "text": " And remember they're nothing weird and magical, they're very simple things.", "tokens": [400, 1604, 436, 434, 1825, 3657, 293, 12066, 11, 436, 434, 588, 2199, 721, 13], "temperature": 0.0, "avg_logprob": -0.15062630971272786, "compression_ratio": 1.7511737089201878, "no_speech_prob": 1.952573256858159e-05}, {"id": 1239, "seek": 602512, "start": 6035.8, "end": 6043.68, "text": " An activation is the result of either a matrix multiply or an activation function.", "tokens": [1107, 24433, 307, 264, 1874, 295, 2139, 257, 8141, 12972, 420, 364, 24433, 2445, 13], "temperature": 0.0, "avg_logprob": -0.15062630971272786, "compression_ratio": 1.7511737089201878, "no_speech_prob": 1.952573256858159e-05}, {"id": 1240, "seek": 602512, "start": 6043.68, "end": 6050.48, "text": " And a parameter are the numbers inside the matrices that we multiply by.", "tokens": [400, 257, 13075, 366, 264, 3547, 1854, 264, 32284, 300, 321, 12972, 538, 13], "temperature": 0.0, "avg_logprob": -0.15062630971272786, "compression_ratio": 1.7511737089201878, "no_speech_prob": 1.952573256858159e-05}, {"id": 1241, "seek": 602512, "start": 6050.48, "end": 6054.76, "text": " And then there are some special layers.", "tokens": [400, 550, 456, 366, 512, 2121, 7914, 13], "temperature": 0.0, "avg_logprob": -0.15062630971272786, "compression_ratio": 1.7511737089201878, "no_speech_prob": 1.952573256858159e-05}, {"id": 1242, "seek": 605476, "start": 6054.76, "end": 6063.8, "text": " So every one of these things that does a calculation, all of these things that does a calculation,", "tokens": [407, 633, 472, 295, 613, 721, 300, 775, 257, 17108, 11, 439, 295, 613, 721, 300, 775, 257, 17108, 11], "temperature": 0.0, "avg_logprob": -0.1611851954805678, "compression_ratio": 1.9424460431654675, "no_speech_prob": 9.972885891329497e-06}, {"id": 1243, "seek": 605476, "start": 6063.8, "end": 6067.400000000001, "text": " are all called layers.", "tokens": [366, 439, 1219, 7914, 13], "temperature": 0.0, "avg_logprob": -0.1611851954805678, "compression_ratio": 1.9424460431654675, "no_speech_prob": 9.972885891329497e-06}, {"id": 1244, "seek": 605476, "start": 6067.400000000001, "end": 6069.0, "text": " They're the layers of our neural net.", "tokens": [814, 434, 264, 7914, 295, 527, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.1611851954805678, "compression_ratio": 1.9424460431654675, "no_speech_prob": 9.972885891329497e-06}, {"id": 1245, "seek": 605476, "start": 6069.0, "end": 6074.56, "text": " So every layer results in a set of activations because there's a calculation that results", "tokens": [407, 633, 4583, 3542, 294, 257, 992, 295, 2430, 763, 570, 456, 311, 257, 17108, 300, 3542], "temperature": 0.0, "avg_logprob": -0.1611851954805678, "compression_ratio": 1.9424460431654675, "no_speech_prob": 9.972885891329497e-06}, {"id": 1246, "seek": 605476, "start": 6074.56, "end": 6079.56, "text": " in a set of results.", "tokens": [294, 257, 992, 295, 3542, 13], "temperature": 0.0, "avg_logprob": -0.1611851954805678, "compression_ratio": 1.9424460431654675, "no_speech_prob": 9.972885891329497e-06}, {"id": 1247, "seek": 607956, "start": 6079.56, "end": 6085.0, "text": " There's a special layer at the start, which is called the input layer, and then at the", "tokens": [821, 311, 257, 2121, 4583, 412, 264, 722, 11, 597, 307, 1219, 264, 4846, 4583, 11, 293, 550, 412, 264], "temperature": 0.0, "avg_logprob": -0.14690332592658276, "compression_ratio": 1.8646288209606987, "no_speech_prob": 1.0953069249808323e-05}, {"id": 1248, "seek": 607956, "start": 6085.0, "end": 6088.6, "text": " end, you just have a set of activations.", "tokens": [917, 11, 291, 445, 362, 257, 992, 295, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.14690332592658276, "compression_ratio": 1.8646288209606987, "no_speech_prob": 1.0953069249808323e-05}, {"id": 1249, "seek": 607956, "start": 6088.6, "end": 6092.080000000001, "text": " And we can refer to those special, I mean they're not special mathematically, but they're", "tokens": [400, 321, 393, 2864, 281, 729, 2121, 11, 286, 914, 436, 434, 406, 2121, 44003, 11, 457, 436, 434], "temperature": 0.0, "avg_logprob": -0.14690332592658276, "compression_ratio": 1.8646288209606987, "no_speech_prob": 1.0953069249808323e-05}, {"id": 1250, "seek": 607956, "start": 6092.080000000001, "end": 6097.240000000001, "text": " semantically special, we can call those the outputs.", "tokens": [4361, 49505, 2121, 11, 321, 393, 818, 729, 264, 23930, 13], "temperature": 0.0, "avg_logprob": -0.14690332592658276, "compression_ratio": 1.8646288209606987, "no_speech_prob": 1.0953069249808323e-05}, {"id": 1251, "seek": 607956, "start": 6097.240000000001, "end": 6101.72, "text": " So the important point to realize here is the outputs of a neural net are not actually", "tokens": [407, 264, 1021, 935, 281, 4325, 510, 307, 264, 23930, 295, 257, 18161, 2533, 366, 406, 767], "temperature": 0.0, "avg_logprob": -0.14690332592658276, "compression_ratio": 1.8646288209606987, "no_speech_prob": 1.0953069249808323e-05}, {"id": 1252, "seek": 607956, "start": 6101.72, "end": 6106.6, "text": " like mathematically special, they're just the activations of a layer.", "tokens": [411, 44003, 2121, 11, 436, 434, 445, 264, 2430, 763, 295, 257, 4583, 13], "temperature": 0.0, "avg_logprob": -0.14690332592658276, "compression_ratio": 1.8646288209606987, "no_speech_prob": 1.0953069249808323e-05}, {"id": 1253, "seek": 610660, "start": 6106.6, "end": 6111.0, "text": " And so what we did in our collaborative filtering example, we did something interesting.", "tokens": [400, 370, 437, 321, 630, 294, 527, 16555, 30822, 1365, 11, 321, 630, 746, 1880, 13], "temperature": 0.0, "avg_logprob": -0.21231782758558118, "compression_ratio": 1.5824742268041236, "no_speech_prob": 9.516209502180573e-06}, {"id": 1254, "seek": 610660, "start": 6111.0, "end": 6121.200000000001, "text": " We actually added an additional activation function right at the very end.", "tokens": [492, 767, 3869, 364, 4497, 24433, 2445, 558, 412, 264, 588, 917, 13], "temperature": 0.0, "avg_logprob": -0.21231782758558118, "compression_ratio": 1.5824742268041236, "no_speech_prob": 9.516209502180573e-06}, {"id": 1255, "seek": 610660, "start": 6121.200000000001, "end": 6127.320000000001, "text": " We added an extra activation function which was sigmoid.", "tokens": [492, 3869, 364, 2857, 24433, 2445, 597, 390, 4556, 3280, 327, 13], "temperature": 0.0, "avg_logprob": -0.21231782758558118, "compression_ratio": 1.5824742268041236, "no_speech_prob": 9.516209502180573e-06}, {"id": 1256, "seek": 610660, "start": 6127.320000000001, "end": 6130.200000000001, "text": " Specifically it was a scaled sigmoid, going between 0 and 5.", "tokens": [26058, 309, 390, 257, 36039, 4556, 3280, 327, 11, 516, 1296, 1958, 293, 1025, 13], "temperature": 0.0, "avg_logprob": -0.21231782758558118, "compression_ratio": 1.5824742268041236, "no_speech_prob": 9.516209502180573e-06}, {"id": 1257, "seek": 610660, "start": 6130.200000000001, "end": 6132.64, "text": " And that's really common.", "tokens": [400, 300, 311, 534, 2689, 13], "temperature": 0.0, "avg_logprob": -0.21231782758558118, "compression_ratio": 1.5824742268041236, "no_speech_prob": 9.516209502180573e-06}, {"id": 1258, "seek": 613264, "start": 6132.64, "end": 6137.76, "text": " It's very common to have an activation function as your last layer, and it's almost never", "tokens": [467, 311, 588, 2689, 281, 362, 364, 24433, 2445, 382, 428, 1036, 4583, 11, 293, 309, 311, 1920, 1128], "temperature": 0.0, "avg_logprob": -0.12989413630854976, "compression_ratio": 1.8495575221238938, "no_speech_prob": 7.1832323556009214e-06}, {"id": 1259, "seek": 613264, "start": 6137.76, "end": 6142.280000000001, "text": " going to be a relu, because it's very unlikely that what you actually want is something that", "tokens": [516, 281, 312, 257, 1039, 84, 11, 570, 309, 311, 588, 17518, 300, 437, 291, 767, 528, 307, 746, 300], "temperature": 0.0, "avg_logprob": -0.12989413630854976, "compression_ratio": 1.8495575221238938, "no_speech_prob": 7.1832323556009214e-06}, {"id": 1260, "seek": 613264, "start": 6142.280000000001, "end": 6144.62, "text": " stops, truncates at 0.", "tokens": [10094, 11, 504, 409, 66, 1024, 412, 1958, 13], "temperature": 0.0, "avg_logprob": -0.12989413630854976, "compression_ratio": 1.8495575221238938, "no_speech_prob": 7.1832323556009214e-06}, {"id": 1261, "seek": 613264, "start": 6144.62, "end": 6148.56, "text": " It's very often going to be a sigmoid or something similar, because it's very likely that actually", "tokens": [467, 311, 588, 2049, 516, 281, 312, 257, 4556, 3280, 327, 420, 746, 2531, 11, 570, 309, 311, 588, 3700, 300, 767], "temperature": 0.0, "avg_logprob": -0.12989413630854976, "compression_ratio": 1.8495575221238938, "no_speech_prob": 7.1832323556009214e-06}, {"id": 1262, "seek": 613264, "start": 6148.56, "end": 6155.04, "text": " what you want is something that's between two values, and kind of scaled in that way.", "tokens": [437, 291, 528, 307, 746, 300, 311, 1296, 732, 4190, 11, 293, 733, 295, 36039, 294, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.12989413630854976, "compression_ratio": 1.8495575221238938, "no_speech_prob": 7.1832323556009214e-06}, {"id": 1263, "seek": 613264, "start": 6155.04, "end": 6157.26, "text": " So that's nearly it, right?", "tokens": [407, 300, 311, 6217, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12989413630854976, "compression_ratio": 1.8495575221238938, "no_speech_prob": 7.1832323556009214e-06}, {"id": 1264, "seek": 615726, "start": 6157.26, "end": 6163.4800000000005, "text": " So we've got inputs, weights, activations, activation functions, which we sometimes call", "tokens": [407, 321, 600, 658, 15743, 11, 17443, 11, 2430, 763, 11, 24433, 6828, 11, 597, 321, 2171, 818], "temperature": 0.0, "avg_logprob": -0.19167898662054716, "compression_ratio": 1.5688622754491017, "no_speech_prob": 1.86314937309362e-05}, {"id": 1265, "seek": 615726, "start": 6163.4800000000005, "end": 6170.76, "text": " non-linearities, output, and then the function that compares those two things together is", "tokens": [2107, 12, 28263, 1088, 11, 5598, 11, 293, 550, 264, 2445, 300, 38334, 729, 732, 721, 1214, 307], "temperature": 0.0, "avg_logprob": -0.19167898662054716, "compression_ratio": 1.5688622754491017, "no_speech_prob": 1.86314937309362e-05}, {"id": 1266, "seek": 615726, "start": 6170.76, "end": 6180.76, "text": " called the loss function, which so far we've used MSE.", "tokens": [1219, 264, 4470, 2445, 11, 597, 370, 1400, 321, 600, 1143, 376, 5879, 13], "temperature": 0.0, "avg_logprob": -0.19167898662054716, "compression_ratio": 1.5688622754491017, "no_speech_prob": 1.86314937309362e-05}, {"id": 1267, "seek": 615726, "start": 6180.76, "end": 6182.4800000000005, "text": " And that's enough for today.", "tokens": [400, 300, 311, 1547, 337, 965, 13], "temperature": 0.0, "avg_logprob": -0.19167898662054716, "compression_ratio": 1.5688622754491017, "no_speech_prob": 1.86314937309362e-05}, {"id": 1268, "seek": 618248, "start": 6182.48, "end": 6190.08, "text": " So what we're going to do next week is we're going to add in a few more extra bits, which", "tokens": [407, 437, 321, 434, 516, 281, 360, 958, 1243, 307, 321, 434, 516, 281, 909, 294, 257, 1326, 544, 2857, 9239, 11, 597], "temperature": 0.0, "avg_logprob": -0.12217881315845554, "compression_ratio": 2.0677966101694913, "no_speech_prob": 9.516081263427623e-06}, {"id": 1269, "seek": 618248, "start": 6190.08, "end": 6193.959999999999, "text": " is we're going to learn the loss function that's used for classification, which is called", "tokens": [307, 321, 434, 516, 281, 1466, 264, 4470, 2445, 300, 311, 1143, 337, 21538, 11, 597, 307, 1219], "temperature": 0.0, "avg_logprob": -0.12217881315845554, "compression_ratio": 2.0677966101694913, "no_speech_prob": 9.516081263427623e-06}, {"id": 1270, "seek": 618248, "start": 6193.959999999999, "end": 6195.339999999999, "text": " cross entropy.", "tokens": [3278, 30867, 13], "temperature": 0.0, "avg_logprob": -0.12217881315845554, "compression_ratio": 2.0677966101694913, "no_speech_prob": 9.516081263427623e-06}, {"id": 1271, "seek": 618248, "start": 6195.339999999999, "end": 6199.679999999999, "text": " We're going to use the activation function that's used for single label classification,", "tokens": [492, 434, 516, 281, 764, 264, 24433, 2445, 300, 311, 1143, 337, 2167, 7645, 21538, 11], "temperature": 0.0, "avg_logprob": -0.12217881315845554, "compression_ratio": 2.0677966101694913, "no_speech_prob": 9.516081263427623e-06}, {"id": 1272, "seek": 618248, "start": 6199.679999999999, "end": 6201.28, "text": " which is called softmax.", "tokens": [597, 307, 1219, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.12217881315845554, "compression_ratio": 2.0677966101694913, "no_speech_prob": 9.516081263427623e-06}, {"id": 1273, "seek": 618248, "start": 6201.28, "end": 6206.639999999999, "text": " And we're also going to learn exactly what happens when we do fine tuning in terms of", "tokens": [400, 321, 434, 611, 516, 281, 1466, 2293, 437, 2314, 562, 321, 360, 2489, 15164, 294, 2115, 295], "temperature": 0.0, "avg_logprob": -0.12217881315845554, "compression_ratio": 2.0677966101694913, "no_speech_prob": 9.516081263427623e-06}, {"id": 1274, "seek": 618248, "start": 6206.639999999999, "end": 6211.719999999999, "text": " how these layers actually, what happens with unfreeze and what happens when we create transfer", "tokens": [577, 613, 7914, 767, 11, 437, 2314, 365, 3971, 701, 1381, 293, 437, 2314, 562, 321, 1884, 5003], "temperature": 0.0, "avg_logprob": -0.12217881315845554, "compression_ratio": 2.0677966101694913, "no_speech_prob": 9.516081263427623e-06}, {"id": 1275, "seek": 621172, "start": 6211.72, "end": 6212.72, "text": " learning.", "tokens": [2539, 13], "temperature": 0.0, "avg_logprob": -0.19707920240319293, "compression_ratio": 0.9868421052631579, "no_speech_prob": 6.917935388628393e-05}, {"id": 1276, "seek": 621172, "start": 6212.72, "end": 6213.72, "text": " So thanks everybody.", "tokens": [407, 3231, 2201, 13], "temperature": 0.0, "avg_logprob": -0.19707920240319293, "compression_ratio": 0.9868421052631579, "no_speech_prob": 6.917935388628393e-05}, {"id": 1277, "seek": 621372, "start": 6213.72, "end": 6241.72, "text": " I'm looking forward to seeing you next week.", "tokens": [50364, 286, 478, 1237, 2128, 281, 2577, 291, 958, 1243, 13, 51764], "temperature": 0.0, "avg_logprob": -0.37377020028921276, "compression_ratio": 0.88, "no_speech_prob": 0.000193264422705397}], "language": "en"}