{"text": " All right, welcome back to machine learning I I'm really excited to be able to share some amazing stuff that University of San Francisco students have built during the week or written about during the week and Quite a few of the things I'm going to show you have already spread around the internet quite a bit lots of tweets and posts and all kinds of stuff happening One of the the first to be widely shared was this one by Tyler who did something really interesting He he started out by saying like what if I like create the synthetic data set where the independent variables is like the X and The Y and the dependent variable is like color right and interestingly he showed me an earlier version of this where he wasn't using color He was just like putting the actual numbers in here And this thing kind of wasn't really working at all and as soon as he started using color it started working really well And so I wanted to mention that one of the things that unfortunately we we don't teach you at USF is Theory of human perception perhaps we should Because actually when it comes to visualization, it's kind of the most important thing to know is what is the human eye? Or what is what it was the human brain good at perceiving? There's a whole area of academic study on this and one of the things that we're best at perceiving is differences in color All right, so that's why as soon as we look at this picture of the synthetic data He created you can immediately see all this kind of four areas of you know lighter red color So what he did was he said okay? What if we like tried to create a machine learning model of this synthetic data set and so specifically he created a tree? And the cool thing is that you can actually draw The tree right so after he created the tree he did this all in that plot lid that plot lid is very flexible right he actually drew the tree boundaries So that's already a pretty neat trick is to be actually able to draw the tree But then he did something even cleverer which is he said okay, so what predictions does the tree make well? It's the average of each of these areas and so to do that we can actually draw the average color right? It's actually kind of pretty Here is the predictions that the tree makes now Here's where it gets really interesting is like you can as you know randomly generate trees through resampling and So here are four trees Generated through resampling they're all like pretty similar But a little bit different and so now we can actually visualize bagging and to visualize bagging we literally take the average of The four pictures all right. That's what bagging is and There it is right and so here is like the the fuzzy decision boundaries of a random forest And I think this is kind of amazing right because it's like a I wish I had this actually when I started teaching you all random Forest because I could have skipped a couple of classes. It's just like okay. That's what we do You know we create the decision boundaries we average each area And then we we do it a few times and average all of them okay? So that's what a random forest does and I think like this is just such a great example of Making the complex easy through through pictures So congrats to Tyler for that It actually turns out That he has actually reinvented something that somebody else has already done a guy called Christian any who went on to be a One of the world's foremost machine learning research is actually included almost exactly this technique in a book He wrote about decision forests, so it's actually kind of cool that Tyler ended up reinventing something That one of the world's foremost and for authorities under fit decision forests actually it has created So I thought that was neat that's nice because when we pop when we posted this on Twitter You know got a lot of attention and finally somebody was able to say like oh, you know what this this actually already exists So Tyler's gone away, and you know started reading that book Something else which is super cool is Jason Carpenter Created a whole new library called Parfit and Parfit is a parallelized fitting of multiple models for the purpose of of selecting hyper parameters, and there's a lot I really like about this and He's shown a clear example of how to use it right and like the API looks very similar to other grid search based approaches But it uses the validation techniques that Rachel wrote about and that we learned about a couple of weeks ago of using a good validation set and You know what he's done here is in his blog post that introduces it. You know he's he's Gone right back and said like well. What are hyper parameters? Why do we have to train them and he's kind of explained every step and then the the module itself is Like it's very polished. You know he's added documentation to it. He's added a nice read me to it And it's kind of interesting when you actually look at the code you realize You know it's very simple. You know which is it's definitely not a bad thing. That's a good thing is to make things simple But by kind of Writing this little bit of code and then packaging it up so nicely He's made it really easy for other people to use this technique which is great and so One of the things I've been really thrilled to see is then Vanay went along and combined two things from our class one was to take Parfit and then the other was to take the kind of accelerated SGD approach to classification We don't learn about in the last lesson and combine the two to say like okay. Well. Let's now use Parfit to help us find the parameters of a SGD logistic aggression So I think that's really a really great idea Something else which I thought was terrific is prints actually basically went through and Summarized pretty much all the stuff we learned in the random and for random forest interpretation class And he went even further than that as he described each of the different approaches to random forest interpretation He described how it's done so here for example is feature importance through variable permutation a Little picture of each one and then super cool. Here is the code to implement it from scratch So I think this is like really Nice post you know describing something that not many people understand and showing you know exactly how it works both with pictures And with code that implements it from scratch So I think that's really really great one of the things I really like here is that for like the? Tree interpreter part he actually showed how you can take the tree interpreter Output and feed it into the new waterfall chart package that Chris our USF student built to show how you can actually visualize The contributions of the tree interpreter in a waterfall chart so again kind of a nice combination of Multiple pieces of technology we've both learned about and and built as a group I Also really thought this kernel there's been a few interesting kernels shared, and I'll share some more next week And to best wrote this really nice kernel showing there's this quite challenging Kaggle competition on detecting icebergs versus chips And it's a kind of a weird two channel satellite data, which is very hard to visualize and he actually Went through and basically described kind of the formulas for how these like radar scattering things actually work And then actually managed to come up with the code that allowed him to recreate You know the actual 3d icebergs Or ships, and I have not seen that done before like I you know It's it's quite challenging to know how to visualize this data, and then he went on to show how to build a Neural net to try to interpret this so that was pretty fantastic as well So yeah, congratulations for all of you. I know for a lot of you. You know you're Posting stuff out there to the rest of the world for the first time you know and it's kind of intimidating you're used to writing stuff that you kind of hand into a teacher, and they're the only ones who see it and You know it's kind of scary the first time you do it But then the first time somebody you know up votes your cattle Colonel or adds a clap to your median post He suddenly realized oh, I'm actually I've written something that people like that's that's pretty great So if you haven't tried yourself yet, I again invite you to Try writing something and if you're not sure you could write a summary of a lesson You could write a summary of like if there's something you found hard like maybe you found it hard to fire up a GPU based AWS instance you eventually figured it out You could write down just describe how you solve that problem or if one of your classmates Didn't understand something and you explained it to them then you could like write down something saying like oh There's this concept that some people have trouble understanding. Here's a good way. I think of explaining it There's all kinds of stuff you could you could do Okay, so let's go back to SGD And so We're going back through this notebook which Rachel put together basically taking us through Kind of SGD from scratch for the purpose of digit recognition and actually quite a lot of the stuff we look at today is going to be closely following a part of the computational linear algebra course Which you can both find the MOOCs on fast AI or at USF. It'll be an elective next year All right, so if you find some of this This stuff interesting, and I hope you do then please consider signing up for the elective or checking out the video online So We're building neural networks And We're starting with an assumption that we've downloaded the MNIST data We've normalized it by subtracting the mean and divided by the standard deviation. Okay, so the data is It's slightly unusual in that although they represent images They were they were downloaded as each image was a 784 long Rank one tensor so it's been flattened out okay, and so for the purpose of drawing pictures of it we had to resize it to 28 by 28 But the actual data we've got is not 28 by 28. It says it's it's 784 long flattened out Okay The basic steps we're going to take here is to start out with training The world's simplest neural network basically a logistic regression right so no hidden layers And we're going to train it using a library Fast AI and we're going to build the network using a library pipe torch All right, and then we're going to gradually get rid of all the libraries right so first of all we'll get rid of The NN neural net library and pipe torch and write that ourselves Then we'll get rid of the fast AI fit function and write that ourselves And then we'll get rid of the pie torch optimizer and write that ourselves and so by the end of This notebook will have written all the pieces ourselves The only thing that we'll end up relying on is the two key things that pie torch gives us which is a the ability to write Python code and have it run on the GPU and Be the ability to write Python code and have it automatically differentiated for us, okay? So they're the two things we're not going to attempt to write ourselves because it's boring and pointless But everything else will try and write ourselves on top of those two things okay, so our Starting point is like not doing anything ourselves It's basically having it all done for us and so pie torch has an NN Library which is where the neural net stuff lives you can create a multi layer Neural network by using the sequential function and then passing in a list of the layers that you want and we asked for a linear layer Followed by a softmax layer and that defines a logistic regression Okay, the input to our linear layer is 28 by 28 as we just discussed The output is 10 because we want a probability for each of the numbers not through no for each of our images, okay? Cuda sticks it on the GPU and then Fit Fits a model okay, so we start out with a random set of weights and then fit uses gradient descent to make it better We had to tell the fit function What criterion to use in other words what counts as better and we told it to use negative log likelihood? We'll learn about that in the next lesson what that is exactly We had to tell it what optimizer to use and we said please use opt-in dot Adam the details of that We won't cover in this course. We're going to use something build something simpler called SGD If you're interested in Adam, we just covered that in the deep learning course And what metrics do you want to print out we decided to print out accuracy okay, so That was that and so if we do that Okay, so after we fit it we get an accuracy of generally somewhere around 91 92 percent so what we're going to do from here is we're going to gradually We're going to repeat this exact same thing so we're going to rebuild This model you know four or five times Fitting it building it and fitting it with less and less libraries, okay? So the second thing that we did last time was to try to start to define the The module ourselves right so instead of saying the network is a sequential bunch of these layers Let's not use that library at all and try and define it ourselves from scratch, okay? So to do that we have to use oO Because that's how we build everything in pytorch, and we have to create a class which inherits from NN dot module so NN dot module is a pytorch class that takes Our class and turns it into a neural network module Which basically means will anything that you inherit from an end up module like this you can pretty much insert Into a neural network as a layer or you can treat it as a neural network It's going to get all the stuff that it needs automatically to to work as a part of or a full Neural network, and we'll talk about exactly what that means today in the next lesson right So We need to construct the object so that means we need to define the constructor thunder in it and then importantly This is a python thing is if you inherit from some other object Then you have to create the thing you inherit from first so when you say super dot Thunder in it that says construct the NN dot module piece of that first right if you don't do that then The NN dot module stuff never gets a chance to actually get constructed now, so this is just like a standard python OO subclass constructor, okay, and if any of that's unclear to you then you know this is where you definitely want to just grab a Python intro to OO because this is the standard approach right so inside our constructor We want to do the equivalent of An N dot linear all right, so what an N dot linear is doing is it's taking our? It's taking our 28 by 28 Vector so 768 long Vector and we're going to be that's going to be the input to a matrix multiplication, so we now need to create a something with 768 rows and That's 768 and 10 Collins Okay, so because the input to this is going to be a mini batch of size Actually, let's move this into a new window 768 by 10 and the input to this is going to be a mini batch of size 64 by 768 right so we're going to do this matrix product Okay, so when we say in pie torch NN dot linear It's going to construct This matrix for us right so since we're not using that we're doing things from scratch We need to make it ourselves so to make it ourselves we can say generate normal random numbers with This dimensionality which we passed in here 768 by 10 okay, so that gives us our randomly initialized matrix okay Then we want to add on to this You know we don't just want y equals ax we want y equals ax plus b Right so we need to add on what we call in neural nets a bias vector So we create here a bias vector of length 10. Hey again randomly initialized And so now here are our two randomly initialized weight tensors So that's our constructor Okay Now we need to find forward why do we need to define forward? This is a pie torch specific thing What's going to happen is this is when you create a module in pie torch? The object that you get back behaves as if it's a function you can call it with parentheses Which will do it that in a moment, and so you need to somehow define What happens when you call it as if it's a function and the answer is pie torch calls a method called? Forward okay, that's just that's the Python the pie torch kind of approach that they picked right So when it calls forward we need to do our actual Calculation of the output of this module or later okay, so here is the thing that actually gets calculated in our logistic regression So basically we take our Input X Which gets passed to forward that's basically how forward works it gets past the mini-batch And we matrix multiply it by The layer one weights which we defined up here, and then we add on The layer one bias which we defined up here, okay And actually nowadays we can define this a little bit more elegantly Using the Python 3 matrix multiplication operator, which is the at sign Okay, and when you when you use that I think you kind of end up with Something that looks closer to what the mathematical notation looked like and so I find that nicer All right, so that's that's our linear layer In our logistic regression in our zero hidden layer neural net and so then the next thing we do to that is Softmax okay, so we get the output of this Matrix multiplier okay, who wants to tell me what the dimensionality of my output of this matrix multiply is So 64 by 10. Thank you, Karen And I should mention for those of you that weren't at deep learning class yesterday We actually looked at a really cool post from Karen who described how to Do structured data analysis with neural nets which has been like super popular? And a whole bunch of people have kind of said that they've read it and found it super interesting so So that was really exciting so we get this matrix of outputs and we put this through a soft max and Why do we put it through a soft max we put it through a soft max because in the end we want probably you know for Every image we want a probability that this is zero or a one or a two or three or four right so we want a bunch Of probabilities that add up to one and where each of those probabilities is between zero and one so a soft max does Exactly that for us So for example if we weren't picking out you know numbers from naught to ten but instead we're picking out cat dog plane fish You're building the output of that matrix multiply for one particular image might look like that. These are just some random numbers And to turn that into a soft max I first go a to the power of each of those numbers I Sum up those e to the power ofs and Then I take each of those e to the power ofs and divide it by the sum and that's soft max That's the definition of soft max so because it was a to the power of it means. It's always positive Because it was divided by the sum it means that it's always between zero and one And it also means because it's divided by the sum that they always add up to one So by applying this soft max Activation function so anytime we have a Layer of outputs which we call activations And then we apply some function some nonlinear function to that that maps one one Scaler to one scalar like softmax does we call that an activation function? Okay, so the softmax activation function takes our outputs and turns it into something which behaves like a probability Right we don't strictly speaking need it we could still try and train something which where the output directly is the probabilities All right, but by creating using this function that automatically makes them always behave like probabilities It means there's less for the network to learn so it's going to learn better right so generally speaking whenever we design an architecture We try to design it in a way where it's as easy as possible For it to create something of the form that we want So that's why we use soft max Right so that's the basic steps right we have our input which is a bunch of images Right which is here gets multiplied by a weight metrics. We actually also add on a bias right to get a Output of the linear function we put it through a nonlinear activation function in this case softmax and that gives us our probabilities So there there that all is Pi torch also tends to use the log of Softmax for reasons that don't particularly bother us now. It's basically a numerical stability convenience okay, so to make this the same as our Version up here that you saw log softmax. I'm going to use log here as well, okay, so We can now instantiate This class that is create an object of this class So I have a question back for the probabilities where we were before so If we were to have a photo with a cat and a dog together would that change the way that that works or does it work? In the same basic yeah, so that's a great question so if you had a photo with a cat and a dog together And you wanted it to spit out both cat and dog This would be a very poor choice so softmax is specifically the activation function We use for categorical predictions where we only ever want to predict one of those things right and so part of the reason Why is that as you can see because we're using either that right either the slightly bigger numbers Creates much bigger numbers as a result of which we generally have just one or two things large and everything else is pretty small All right, so if I like recalculate these random numbers a few times You'll see like it tends to be a bunch of zeros and one or two high numbers right so it's really designed to Try to kind of make it easy to predict like this one thing is the thing I want If you're doing multi Label prediction so I want to find all the things in this image rather than using softmax We would instead use sigmoid that's a sigmoid recall each would cause each of these between to be between zero and one But they would no longer add to one It's a good question and like a lot of these Details about like best practices are things that we cover in the deep learning course And we won't cover heaps of them here in the machine learning course. We're more interested in the mechanics I guess But we'll try and do them with their quick All right, so now that we've got that we can instantiate an object of that class and of course We want to copy it over to the GPU so we can do computations over there Again, we need an optimizer. We'll be talking about what this is shortly, but you'll see here We've called a function on our class called parameters, but we never defined a method called parameters And the reason that is going to work is because it actually was defined for us inside n n dot module and so n n dot module actually automatically goes through the attributes we've created and Finds anything that basically we we said this is a parameter So the way you say something is a parameter is you wrap it in and end up parameter So this is just the way that you tell pi torch. This is something that I want to optimize Okay, so when we created the weight matrix. We just wrapped it with an end up parameter It's exactly the same as a regular Pi torch variable which we'll learn about shortly. It's just a little flag to say hey You should you should optimize this and so when you call net two dot parameters on our net two object We created it goes through everything that we created in the constructor Checks to see if any of them are of type parameter And if so it sets all of those being things that we want to train with the optimizer And we'll be implementing the optimizer from scratch later Okay, so having done that We can fit And we should get basically the same answer as before 91 ish So that looks good alright So What have we actually built here? Well what we've actually built as I said is something that can behave Like a regular function right so I want to show you how we can actually call this as a function So to be able to call it as a function We need to be able to pass data to it to be able to pass data to it I'm going to need to grab a mini batch of MNIST images Okay, so we used for convenience the Image classifier data from arrays method from fast AI And what that does is it creates a pie torch data loader for us a pie torch data loader is Something that grabs a few images and sticks them into a mini batch And makes them available and you can basically say give me another mini batch give me another mini batch give me another mini batch and so in Python we call these things generators generators are things where you can basically say I want another I want another I want another right There's this kind of very close connection between Iterators and generators are not going to worry about the difference between them right now, but you'll see basically to turn To actually get hold of something which we can say please give me another of of In order to grab something that we can we can use to generate many batches We have to take our data loader And so you can ask for the training data loader from our model data object You'll see there's a bunch of different data loaders you can ask for you can ask for the test data loader the train data loader the validation loader Augmented images data loader and so forth so we're going to grab the training data loader That was created for us. This is a PI standard PI torch data loader well slightly optimized by us, but same idea And you can then say this is a standard Python Thing we can say turn that into an iterator turn that into something where we can grab another one at a time from and So once you've done that We've now got something that we can iterate through you can use the standard Python Next function to grab one more thing from that generator, okay? So that's returning and the X's from our mini-batch and the wise Family mini-batch the other way that you can use Generators and iterators in Python is with a for loop I could also have said like for you know X mini-batch comma Y mini-batch in data loader And then like do something right so when you do that it's actually behind the scenes It's basically syntactic sugar for calling next lots of times okay, so this is all standard Python stuff So that returns A tensor of size 64 by 7 8 4 as we would expect right the The fast AI library we used defaults to a mini-batch size of 64. That's why it's that long These are all of the background zero pixels, but they're not actually zero in this case. Why aren't they zero? Yeah, they're normalized exactly right so we subtracted the mean divided by standard deviation right So there there it is so now what we want to do is we want to Pass that into our our logistic regression So what we might do is we'll go Variable XMV equals variable okay, I can take my x mini-batch I can move it onto the GPU because remember my Net to object is on the GPU so our data for it also has to be on the GPU And then the second thing I do is I have to wrap it in variable, so what does variable do? This is how we get for free automatic differentiation Pi torch can automatically differentiate You know pretty much anything right any tensor But to do so takes memory and time So it's not going to always keep track like to do automatic differentiation It has to keep track of exactly how something was calculated we added these things together We multiplied it by that we then took the sign blah blah blah right you have to know all of the steps Because then to do the automatic differentiation it has to Take the derivative of each step using the chain rule multiply them all together right so that's slow and memory intensive So we have to opt in to saying like okay this particular thing We're going to be taking the derivative of later, so please keep track of all of those operations for us And so the way we opt in is by wrapping a tensor in a variable right so That's how we do it and You'll see that it looks almost exactly like a tensor, but it now says variable containing This tensor right so in pi torch a variable has exactly Identical API to a tensor or actually more specifically a superset of the API of a tensor Anything we can do to a tensor we can do to a variable But it's going to keep track of exactly what we did so we can later on take the derivative okay, so we can now pass that Into our Net to object remember I said you can treat this as if it's a function right so notice. We're not calling forward We're just treating it as a function and Then remember we took the log so to undo that I'm taking the X and that will give me my probabilities okay So there's my probabilities and it's got Return something of size 64 by 10 so for each image in the mini batch We've got 10 probabilities, and you'll see most probabilities are pretty close to zero Right and a few of them are quite a bit bigger Which is exactly what we do we hope right is that it's like okay? It's not a zero is not a one is not a two it is a three is not a four is not a five and so forth So maybe this would be a bit easier to read if we just grab like the first three of them Okay, so it's like 10 to the neg 3 10 to the neg 8 2 5 5 4 ok and then suddenly here's one Which is 10 to neg 1 right? So you can kind of see what it's trying to What it's trying to do here I mean we could call like net to dot forward and it'll do exactly the same thing Right, but that's not how? All of the pie torch mechanics actually work It's actually they actually call it as if it's a function right and so this is actually a really important idea Because it means that when we define our own architectures or whatever anywhere that you would put in a function You could put in a layer anyway you put in a layer you can put in a neural net Anyway, you put in a neural net you can put in a function because as far as pie torch is concerned They're all just things that it's going to call Just like as if their functions, so they're all like interchangeable, and this is really important because that's how we create Really good neural nets is by mixing and matching lots of pieces and putting them all together right let me give an example Here is my Logistic regression which got 91 and a bit percent accuracy I'm now going to turn it Into a neural network with one hidden layer all right and the way I'm going to do that is I'm going to create one more layer I'm going to change this so it spits out a hundred rather than ten which means this one input is going to be 100 rather than ten Now this as it is can't possibly make things any better at all yet Why is this definitely not going to be better than what I had before? Yeah, can somebody pass the yeah? Because you've got a combination of two linear layers, which is just the same as one linear with different parameters exactly right So we've got two linear layers, which is just a linear layer right so to make things interesting I'm going to replace all of the negatives from the first layer with zeros Because that's a nonlinear transformation and so that nonlinear transformation is called a rectified linear unit Okay, so n n dot sequential simply is going to call each of these layers in turn for each mini-batch right so do a linear layer Replace all of the negatives with zero do another linear layer and do a softmax This is now a neural network with one hidden layer and So let's try training that instead Yeah, accuracy is now going up to 96% Okay, so the this is the idea is that the basic techniques. We're learning in this lesson Like become powerful at the point where you start stacking them together, okay? Can somebody pass the green box there and then there yes Daniel? Why did you pick a hundred no reason it was like easier to type an extra zero Like this question of like how many Activations should I have in a neural network layer is kind of part of the the scale of a deep learning practitioner? We cover it in the deep learning course not in this course When adding that additional I guess transformation Additional layer additional layer this one here is called a nonlinear layer or an activation for you Are you said activation layer activation function or activation? Does it matter that like if you would have done for example like two soft maxes or is that something you cannot do? Like when I know you can absolutely use a soft max there But it's probably not going to give you what you want and the reason why is that a soft max? Tends to push most of its activations to zero and an activation just to be clear like I've had a lot of questions in deep Learning course about like what's an activation an activation is the value that is calculated in a layer right so this is an activation Right it's not a weight a weight is not an activation It's the value that you calculate from a layer so soft max will tend to make most of its activations pretty close to zero And that's the opposite of what you want you generally want your activations to be kind of as rich and diverse and Used as possible so nothing to stop you doing it, but it probably won't work very well Basically pretty much all of your layers will be followed by Non by nonlinear activation functions that will nearly always be value except for the last layer Going two or three layers deep Do you want to switch up these activation layers? No is it okay? Just to keep them? No, that's a great question So if I wanted to go deeper I would just do That okay, that's a now to hidden layer network So I think I'd heard you said that there are a couple of different activation functions like that rectified linear unit What are some examples and? Why would you use? each yeah great question So basically like as you add like more Linear layers you kind of got your input comes in and you put it through a linear layer And then a nonlinear layer linear layer nonlinear layer Many linear layer and then the final nonlinear layer The Final nonlinear layer as we've discussed you know if it's a multi-category Classification, but you only ever pick one of them you would use softmax If it's a binary classification or a multi-label classification where you're predicting multiple things you would use sigmoid If it's a regression You would often have Nothing at all right although we learned in last night's dl course where sometimes you can use sigmoid there as well So they're basically the options main options for the final layer for the Hidden layers you pretty much always use Relu's relu All right, but there is a another Another one you can pick which is kind of interesting which is called Leaky relu, and it looks like this and Basically if it's above zero it's y equals x and if it's below zero it's like y equals 0.1 x Okay, so it's very similar to relu, but it's You know rather than being equal to zero under x. It's it's like something close to that So they're the main to relu and leaky relu There are various others, but they're kind of like things that just look very close to that so for example There's something called elu which is quite popular But like you know the details don't matter too much honestly like that that like elu is something that looks like this But it's slightly more curvy in the middle And it's kind of like it's not generally something that you so much pick based on the data set it's more like Over time we just find better activation functions, so two or three years ago everybody used relu You know a year ago pretty much everybody used leaky relu today I guess probably most people are starting to move towards elu, but honestly the choice of activation function doesn't matter terribly much actually and You know people have actually showed that you can use like a pretty arbitrary nonlinear activation functions like even a sine wave It still works Okay So although what we're going to do today is showing how to create This Network with no hidden layers To turn it into That network which is 96 percent ish accurate is it will be trivial right and in fact is something you should Probably try and do during the week right is to create that version Okay So now that we've got something where we can take our network pass in our variable and get back some predictions That's basically all that happened when we called fit, so we're going to see how how that That approach can be used to create this stochastic gradient descent one Thing to note is that the to turn the predicted probabilities? Into a predicted like which digit is it we would need to use argmax Unfortunately pi torch doesn't call it argmax Instead pi torch just calls it max and max returns two things It returns the actual max across this axis, so this is across the columns right and The second thing it returns is the index Over that maximum right so so the equivalent of argmax is to call max and then get the first Indexed thing okay, so there's our predictions right if this was in numpy. We would instead use np dot argmax All right So here are the predictions from our hand created logistic regression and in this case Looks like we got all but one correct So the next thing we're going to try and get rid of in terms of using libraries is we'll try to avoid using the matrix Multiplication operator and instead we're going to try and write that by hand So this next part we're going to learn about something which kind of seems It kind of it's going to seem like a minor little kind of programming idea But actually it's going to turn out That at least in my opinion. It's the most important Programming concept that we'll teach in this course and it's possibly the most important programming con kind of concept in all of All the things you need to build machine learning algorithms, and it's the idea of broadcasting And the idea I will show by example if we create an array of 10 6 neg 4 and an array of 2 8 7 and then add the two together It adds each of the components of those two arrays in turn we call that element wise So in other words we didn't have to write a loop right back in the old days We would have to have looped through each one and added them and then concatenated them together We don't have to do that today it happens for us automatically so in numpy We automatically get element wise operations We can do the same thing with pytorch So in fast AI we just add a little capital T to turn something into a pytorch tensor All right, and if we add those together Exactly the same thing right so element wise operations are pretty standard in these kinds of libraries It's interesting not just because we don't have to write the for loop Right, but it's actually much more interesting because of the performance things that are happening here The first is if we were doing a for loop right If we were doing a for loop that would happen in Python Right even when you use pytorch it still does the for loop in Python it has no way of like Optimizing a for loop and so a for loop in Python is something like 10,000 times lower than in C so That's your first problem. I can't remember it's like 1000 or 10,000 the second problem then is that You don't just want it to be optimized in C But you want C to take advantage of the thing that your all of your CPUs do to something called Cindy single instruction multiple data, which is it yours can your CPU is capable of taking eight things at a time Right in a vector and adding them up to another Vector with eight things in in a single CPU instruction Right so if you can take advantage of Cindy you're immediately eight times faster It depends on how big the data type is it might be four might be eight The other thing that you've got in your computer is you've got multiple processes Multiple cores So you've probably got like if this is inside happening on one side one core you've probably got about four of those Okay, so if you're using Cindy you're eight times faster if you can use multiple cores Then you're 32 times faster And then if you're doing that in C you might be something like 32 times thousand times faster Right and so the nice thing is that when we do that It's taking advantage of all of these things Okay Better still if you do it in PyTorch and your data was created with Cuda to stick it on the GPU Then your GPU can do about 10,000 things at a time All right, so that'll be another hundred times faster than C All right, so this is critical To getting good performance is you have to learn how to write loopless code By taking advantage of these element wise Operations and like it's not it's a lot more than just plus I Could also use less than right and that's going to return 011 or if we go back to numpy by False true true And so you can kind of use this to do all kinds of things without looping so for example I could now multiply that by a and here are all of the values of a As long as they're less than B Or we could take the mean This is the percentage of values in a that are less than B All right, so like there's a lot of stuff you can do with this simple idea But to take it further Right to take it further than just this element wise operation We're going to have to go the next step to something called broadcasting So let's take a five minute break come back at 217 and we'll talk about broadcasting So broadcasting This is the definition from the numpy documentation of Broadcasting and I'm going to come back to it in a moment rather than reading it now but Let's start By looking an example of broadcasting So a is a Array With one dimension also known as a rank one tensor also known as a vector We can say a greater than zero so here we have a rank one tensor right and a rank zero tensor Right a rank zero tensor is also called a scalar rank one tensor is also called a vector and We've got an operation between the two All right now you've probably done it a thousand times without even noticing that's kind of weird right that you've got these things of different Ranks and different sizes, so what is it actually doing right? But what it's actually doing is it's taking that scalar and copying it here here here Right and then it's actually going element wise Ten is greater than zero Six is greater than zero minus four is greater than zero even giving us back the three answers Right and that's called broadcasting broadcasting means Copying one or more axes of my tensor To allow it to be the same shape as the other tensor It doesn't really copy it though What it actually does is it stores this kind of internal? Indicator that says pretend that this is a vector of three zeros But it actually just like with rather than kind of going to the next row or going to the next scalar it goes back To where it came from if you're interested in learning about this specifically it's they set the stride on that axis To be zero that's a minor advanced concept for those who are curious So we could do a Plus one right it's going to broadcast the scalar one To be one one one and then do element wise addition We could do the same with a matrix right here's our matrix two times the matrix is going to broadcast two to be two two two two two two two two two two two and Then do element wise multiplication right so that's our kind of most simple version of Broadcasting So here's a slightly more complex version of broadcasting Here's an array called C right so this is a rank one tensor and Here's our matrix M from before Our rank two tensor we can add M plus C Right so what's going on here? one two three four five six seven eight nine That's M right and then C ten Twenty thirty You can see that what it's done is to add that to each row Right eleven twenty two thirty three fourteen 2536 and so we can kind of figure it seems to have done the same kind of idea as broadcasting a scalar It's like made copies of it And then it treats those As if it's a rank two matrix and now we can do element wise addition Does that make sense now that's yes, can can you pass that Devon over there? Thank you So as it's like by looking at this example it like copies it down Making new rows So how would we want to do it if we wanted to get new columns? I'm so glad you asked So Instead we would do this Ten twenty thirty All right, and then copy that ten twenty thirty ten twenty thirty and Now treat that as our matrix So to get numpy to do that we need to not pass in a vector but to pass in a Matrix with one column a rank two tensor So basically it turns out that NumPy is going to think of a Rank one tensor for these purposes as if it was a rank two tensor which represents a row Right so in other words that it is one by three Right so we want to create a tensor which is three by one There's a couple of ways to do that One is to use NP dot expandims and if you then pass in this argument It says please insert a length one axis here, please so in our case we want to turn it into a Three by one so if we said expand in C comma one Okay, so if we say expand in C comma one it changes the shape to three comma one So if we look at what that looks like That looks like a column okay, so if we now go that Plus M You can see it's doing exactly what we hoped it would do All right, which is to add ten twenty thirty to the column Ten twenty thirty to the column ten twenty thirty to the column Okay now because the location of A unit axis turns out to be so important It's really helpful to kind of experiment with creating these extra unit axes and know how to do it easily and NP dot expandims isn't in my opinion the easiest way to do this the easiest way The easiest way is to index into the tensor with a special index None and what none does is it creates a new axis in that location of length one right so this is Going to add a new axis at the start of length one This is going to add a new axis at the end of length one or Why not do both? Right so if you think about it like a tensor Which has like three? Things in it could be of any rank you like right you can just add unit axes all over the place and so that way we can kind of Decide how we want our broadcasting to work So there's a pretty convenient thing In numpy called broadcast to and what that does is it takes our vector and Broadcasts it to that shape and shows us what that would look like right so if you're ever like unsure Of what's going on in some broadcasting operation you can say broadcast to and so for example here We could say like rather than three comma three we could say m dot shape Right and see exactly what's happening going to happen and so that's what's going to happen before we add it to m Right so if we said Turn it into a column That's what that looks like Make sense So that's kind of like the intuitive Definition of broadcasting and so now hopefully we can go back to that numpy documentation and understand What it means right? Broadcasting describes how numpy is going to treat arrays of different shapes when we do some operation Right the smaller array is broadcast across the larger array by smaller array they mean lower rank tensor basically broadcast across the light the higher rank tensor so they have compatible shapes it Vectorizes array operations so vectorizing generally means like using SIMD and stuff like that so that multiple things happen at the same time All the looping occurs in C But it doesn't actually make needless copies of data. It kind of just acts as if it had So there's our definition now in deep learning you very often deal with tensors of rank 4 or more and you very often combine them with tensors of rank 1 or 2 and Trying to just rely on intuition to do that correctly is nearly impossible, so you really need to know the rules So here are the rules Okay, here's m dot shape here's C dot shape so the rule are that we're going to compare The shapes of our two tensors element wise we're going to look at one at a time And we're going to start at the end right so look at the trailing dimensions and then go Towards the front okay, and so two dimensions are going to be compatible When one of these two things is true, right so let's check right we've got our M and C compatible M is 3 3 C is 3 right so we're going to start at the end trailing dimensions first and check are they compatible? They're compatible if the dimensions are equal okay, so these ones are equal so they're compatible right? All right, let's go to the next one. Oh, we're missing Right C is missing something so what happens if something is missing as we insert a one? Okay, that's the rule right and so let's now check are these compatible one of them is one yes, they're compatible Okay, so now you can see why it is that numpy treats the one dimensional array as If it is a rank 2 tensor Which is representing a row it's because we're basically inserting a one at the front Okay, so that's the rule so for example This is something that you very commonly have to do which is you start with like an image they're like 256 pixels by 256 pixels by three channels and You want to subtract? The mean of each channel right so you've got 256 by 256 by 3 and you want to subtract something of length 3 Right so yeah, you can do that Absolutely because 3 and 3 are compatible because they're the same All right 256 and empty is compatible. It's going to insert a 1 256 and empty is compatible. It's going to insert a 1 right, so you're going to end up with This is going to be broadcast over all of this axis And then that whole thing will be broadcast over this axis and so we'll end up with a 256 by 256 by 3 effective tensor here right so interestingly like very few people in The data science or machine learning communities understand broadcasting and the vast majority of the time for example when I see people doing Pre-processing for computer vision like subtracting the mean they always write loops over the channels right and I kind of think like It's it's like so handy to not have to do that and it's often so much faster to not have to do that So if you get good at broadcasting You'll have this like super useful skill that very very few people have and and like it's it's it's an ancient skill You know it goes it goes all the way back to the days of APL So APL was from the late 50s stands for our programming language and Kenneth Iverson wrote this paper called notation as a tool for thought in which he proposed a new math notation and He proposed that if we use this new math notation it gives us new tools for thought and allows us to think things we couldn't before and one of his ideas was broadcasting not as a computer programming tool, but as a piece of math notation and And so he ended up implementing this notation as a tool for thought as a programming language called APL and His son has gone on to further develop that Into a piece of software called J Which is basically what you get when you put 60 years of very smart people working on this idea And with this programming language you can express very complex mathematical ideas often just with a line of code or two And so I mean it's great that we have J But it's even greater that these ideas have found their ways into the languages We all use like in Python the numpy and pytorch libraries right these are not just little Kind of niche ideas these like fundamental ways to think about math and to do programming Like let me give an example of like this kind of notation as a tool for thought Let's Let's look here. We've got C right here. We've got C None right notice this is no up to square brackets right so this is kind of like a one row rank to tensor Here it is a little column So what is Do we just round ones? Okay, what's that going to do I Would think about it Anybody want to have a go even talk through your thinking okay? Can we pass the check just over there? Thank you Kind of outer product yes absolutely so take us through your thinking how's that going to work? so The diagonal elements can be directly visualized from the squares Mm-hmm across 10 20 cross 20 and 30 cross 30 mm-hmm and If you multiply the first row with this column you can get the first row of the matrix So finally you'll get a 3 cross 3 matrix yeah, and so to think of this in terms of like those broadcasting rules We're basically taking This column right which is of rank Three comma one right and this kind of row Sorry, I've mentioned three comma one and this row which is of dimension one comma three right and so to make these Compatible with our broadcasting rules right this one here has to be duplicated Duplicated three times because it needs to match this okay? And now this one's going to have to be duplicated three times to match this Okay, and so now I've got two Matrices to do an element-wise product of and so as you say There is our outer product right now the interesting thing here is That suddenly now that this is not a special mathematical case But just a specific version of the general idea of broadcasting we can do like an outer plus Or we can do an order greater than Right or or whatever right so it's suddenly we've kind of got this this this concept that we can use to build New ideas, and then we can start to experiment with those new ideas, and so you know interestingly numpy actually Uses this sometimes For example if you want to create a grid This is how numpy does it right actually this is kind of the sorry let me show you this way If you want to create a grid, this is how numpy does it it actually returns zero one two three four and Zero one two three four One is a column one is a row so we could say like okay. That's x grid comma y grid And now you could do something like Well I mean we could obviously go Like that right and so suddenly we've expanded that out into A grid right and so Yeah, it's kind of interesting how like some of these like simple little concepts Kind of get built on and built on and built on so if you lose something like APL or J. It's this whole environment Of layers and layers and layers of this we don't have such a deep environment in numpy But you know you can certainly see these ideas of like broadcasting Coming through in simple things like how do we create a grid in in numpy? So yeah, so that's that's broadcasting and so what we can do with this now is Use this to implement matrix multiplication ourselves Okay Now why would we want to do that well obviously we don't write matrix multiplication has already been handled perfectly nicely for us by our libraries but very often you'll find in All kinds of areas in in machine learning and particularly in deep learning that there'll be particular types of linear Function that you want to do that aren't quite Done for you right so for example. There's like whole areas called like tensor regression and tensor decomposition Which are really being developed a lot at the moment, and they're kind of talking about like how do we take like higher rank tensors and kind of turn them into combinations of rows Columns and faces and it turns out that when you can kind of do this you can basically like Deal with really high dimensional data structures with not much memory and not with not much computation time for example There's a really terrific library Called tensely Which does a whole lot of this kind of stuff? for you So it's a really really important area it covers like all of deep learning lots of modern machine learning in general and So even though you're not going to like define matrix multiplication. You're very likely to want to define some other Slightly different tensor product you know So it's really useful to kind of understand how to do that So let's go back and look at our matrix and our our 2d array and 1d array rank 2 tensor rank 1 tensor and Remember we can do a matrix multiplication Using the at sign or the old way and pay dot matmul Okay, and so what that's actually doing when we do that is we're basically saying Okay 1 times 10 plus 2 times 20 plus 3 times 30 is 140 right and so we do that for each row and We can go through and do the same thing for the next one and for the next one to get our result right? You could do that in torch as well We could make this a little shorter Okay, same thing Okay, but that is not matrix multiplication. What's that? Okay element wise specifically we've got a matrix and a vector so Broadcasting okay good, so we've got this is element wise with broadcasting but notice The numbers it's created 10 40 90 are the exact three numbers that I needed to Calculate when I did that first Piece of my matrix multiplication So in other words if we sum this up Over the columns which is axis equals one We get our matrix sector product Okay, so we can kind of Do this stuff without special help from our library So now Let's expand this out to an matrix matrix product so a matrix matrix product Looks like this. This is this great site called matrix multiplication dot xyz and It shows us this is what happens when we multiply two matrices Okay, so this is the matrix matrix product Okay, that's what matrix multiplication is Operationally speaking so in other words what we just did there Was we first of all took the first column with the first row to get this one and Then we took the second column with the first row to get that one alright, so we're basically doing The thing we just did the matrix vector product. We're just doing it twice right once With this column and once with this column, and then we concatenate the two together Okay, so we can now go ahead and do that Like so M times the first column dot sum M times the second top column dot sum and so there are the two columns of our matrix multiplication Okay So I didn't want to like make our code too messy So I'm not going to actually like use that but like we have it there now if we want to we don't need to use Torch or numpy matrix multiplication anymore. We've got we've got our own that we can use using nothing but element wise operations broadcasting and some Okay So this is our Logistic regression from scratch class again. I just copied it here Here is where we instantiate the object copy it to the GPU We create an optimizer which we'll learn about in a moment, and we call fit okay So the goal is to now repeat this without needing to call fit So to do that We're going to need a loop Which grabs a mini batch of data at a time and with each mini batch of data? We need to pass it to the optimizer and Say please try to come up with a slightly better set of predictions for this mini batch So as we learned in order to grab a mini batch of the training set at a time We have to ask the model data object for the training data loader We have to wrap it in either either to create an iterator or a generator And so that gives us our data loader okay, so pi torch calls this a data loader We actually wrote our own fast AI data loader, but it's it's all it's basically the same idea and So the next thing we do is we grab the X and the Y tensor the next one from our data loader Okay Wrap it in a variable to say I need to be able to take the derivative of the calculations using this because if I can't Take the derivative then I can't get the gradients, and I can't update the weights right, and I need to put it on the GPU because my module is on the GPU and So we can now take that variable and pass it to To the object that we instantiated our logistic regression Remember our module we can use it as if it's a function because that's how pi torch works And that gives us a set of predictions as we saw seen before Okay So now we can check the loss and the loss we defined as being a Negative log likelihood loss object and we're going to learn about how that's calculated in the next lesson For now think of it. Just like root mean squared error, but for classification problems So we can call that also just like a function so you can kind of see this is very general idea in pi torch that you know Kind of treat everything ideally like it's a function so in this case we have a loss Negative log likelihood loss object we could treat it like a function we pass in our predictions and We pass in our actuals right again the actuals need to be turned into a variable and put on the GPU Because the loss is specifically the thing that we actually want to take the derivative of right so that gives us our loss And there it is that's our loss two point four three Okay, so it's a variable and because it's a variable it knows how it was calculated All right, it knows it was calculated with this loss function. It knows that the predictions were calculated with this Network it knows that this network consisted of these operations and so we can get the gradient Automatically, right? So to get the gradient We call L dot backward remember L is the thing that contains our loss All right, so L dot backward is is something which is added to anything That's a variable you can call dot backward and that says please calculate the gradients Okay, and so that calculates the gradients and stores them inside that that the basically for each of the Weights that was used it used each of the parameters that was used to calculate that it's now stored A dot grad we'll see it later. It's basically stored the gradient right so we can then call Optimizer dot step and we're going to do this step manually shortly And that's the bit that says please make the weights a little bit better right and so what optimizer dot step is doing Is it saying like okay if you had like a really simple function? Like this All right, then what the optimizer does is it says okay? Let's pick a random starting point Right and let's calculate the value of the loss right so here's our parameter Here's our loss right let's take the derivative All right the derivative tells us which way is down so it tells us we need to go that direction Okay, and we take a small step and Then we take the derivative again, and we take a small step derivative again take a small step Give it again take a small step Until eventually we're taking such small steps that we stop okay, so that's what? gradient descent does Okay How big a step is a small step well we basically take the derivative here, so let's say derivative there is like eight All right, and we multiply it by a small number like say oh point oh one and that tells us What step size to take? This small number here is called the learning rate And it's the most important hyper parameter to set right if you pick two smaller learning rate Then your steps down are going to be like tiny and it's going to take you forever All right to a bigger learning rate, and you'll jump too far Right and then you'll jump too far and your diverge rather than converge, okay? We're not going to talk about how to pick a learning rate in this class, but in the deep learning class We actually show you a specific technique that very reliably picks a very good learning rate So that's basically what's happening right so we calculate the derivatives And we call the optimizer that does a step in other words update the weights based on the gradients and the learning rate We should hopefully find that after doing that we have a better loss than we did before So I just reran this and got a loss here of four point one six and After one step it's now four point oh three okay So it worked the way we hoped it would based on this mini batch it updated all of the weights in our Network to be a little better than they were as a result of which our loss went down, okay? So let's turn that into a training loop All right, we're going to go through a hundred steps Grab one more mini batch of data from the data loader Calculate our predictions from our network Calculate our loss from the predictions and the actuals Every ten goes will print out the accuracy just take the mean of the whether they're equal or not One pie torch specific thing you have to zero the gradients basically you can have networks where like You've got lots of different loss functions that you might want to add all of the gradients together Right so you have to tell pie torch like when to set the gradients back to zero right so this just says set all the gradients to zero Calculate the gradients that's caught backward and then take one step of the optimizer so update the weights using the gradients and the learning rate and so once we run it you can see the loss goes down and The accuracy goes up Okay so That's the basic approach and so next lesson. We'll see What that does all right? We're looking in detail when we're looking at the data loader All right, we're looking in detail. We're not going to look inside here as I say we're going to basically take the calculation of the derivatives as As a given right but basically What's happening there? And any kind of deep network you have kind of like a function That's like you know a linear function And then you pass the output of that into another function That might be like a relu and you pass the output of that into another function That might be another linear net linear layer you pass that into another function That might be another relu and so forth right so these these deep networks are just Functions of functions of functions, so you could write them mathematically like that right and so All backprop does is it says let's just simplify this down to the two version is We can say okay you equals f of x Right and so therefore the derivative of g of f of x is we can calculate with the chain rule as being g dash u F dash x Right and so you can see we can do the same thing for the functions of the functions of the functions and so when you apply a Function to a function of a function you can take the derivative just by taking the product of the derivatives of each of those layers okay, and in neural networks we call this back propagation Okay, so when you hear back propagation it just means use the chain rule to calculate the derivatives And so when you see a neural network defined Like here right Like if it's defined sequentially literally all this means is Apply this function to the input Apply this function to that apply this function to that apply this function to that right so this is just defining a composition of a function to a function to a function to a function Okay, and so Yeah, so although. We're not going to bother with calculating the gradients ourselves. You can now see why it can do it right as long As it has internally You know a it knows like what's the what's the derivative of to the power of what's the derivative of sine? What's the derivative of plus and so forth then our Python code in? In here. It's just combining those things together So it just needs to know how to compose them together with the chain rule and away it goes okay? Okay, so I think we can leave it there for now and yeah and in the next class We'll go and we'll see how to write our own Optimizer and then we'll have solved M list from scratch ourselves. See you then", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 2.58, "text": " All right, welcome back to machine learning I", "tokens": [1057, 558, 11, 2928, 646, 281, 3479, 2539, 286], "temperature": 0.0, "avg_logprob": -0.25087979196131915, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.0014322389615699649}, {"id": 1, "seek": 0, "start": 4.64, "end": 9.8, "text": " I'm really excited to be able to share some amazing stuff that University of San Francisco", "tokens": [286, 478, 534, 2919, 281, 312, 1075, 281, 2073, 512, 2243, 1507, 300, 3535, 295, 5271, 12279], "temperature": 0.0, "avg_logprob": -0.25087979196131915, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.0014322389615699649}, {"id": 2, "seek": 0, "start": 10.24, "end": 14.280000000000001, "text": " students have built during the week or written about during the week and", "tokens": [1731, 362, 3094, 1830, 264, 1243, 420, 3720, 466, 1830, 264, 1243, 293], "temperature": 0.0, "avg_logprob": -0.25087979196131915, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.0014322389615699649}, {"id": 3, "seek": 0, "start": 15.280000000000001, "end": 17.44, "text": " Quite a few of the things I'm going to show you have already", "tokens": [20464, 257, 1326, 295, 264, 721, 286, 478, 516, 281, 855, 291, 362, 1217], "temperature": 0.0, "avg_logprob": -0.25087979196131915, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.0014322389615699649}, {"id": 4, "seek": 0, "start": 18.28, "end": 20.64, "text": " spread around the internet quite a bit", "tokens": [3974, 926, 264, 4705, 1596, 257, 857], "temperature": 0.0, "avg_logprob": -0.25087979196131915, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.0014322389615699649}, {"id": 5, "seek": 0, "start": 21.64, "end": 23.2, "text": " lots of", "tokens": [3195, 295], "temperature": 0.0, "avg_logprob": -0.25087979196131915, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.0014322389615699649}, {"id": 6, "seek": 0, "start": 23.2, "end": 26.900000000000002, "text": " tweets and posts and all kinds of stuff happening", "tokens": [25671, 293, 12300, 293, 439, 3685, 295, 1507, 2737], "temperature": 0.0, "avg_logprob": -0.25087979196131915, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.0014322389615699649}, {"id": 7, "seek": 2690, "start": 26.9, "end": 34.48, "text": " One of the the first to be widely shared was this one by Tyler who did something really interesting", "tokens": [1485, 295, 264, 264, 700, 281, 312, 13371, 5507, 390, 341, 472, 538, 16869, 567, 630, 746, 534, 1880], "temperature": 0.0, "avg_logprob": -0.18275328742133246, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.5907254641642794e-05}, {"id": 8, "seek": 2690, "start": 36.46, "end": 43.7, "text": " He he started out by saying like what if I like create the synthetic data set where the independent variables is like the X and", "tokens": [634, 415, 1409, 484, 538, 1566, 411, 437, 498, 286, 411, 1884, 264, 23420, 1412, 992, 689, 264, 6695, 9102, 307, 411, 264, 1783, 293], "temperature": 0.0, "avg_logprob": -0.18275328742133246, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.5907254641642794e-05}, {"id": 9, "seek": 2690, "start": 43.7, "end": 51.099999999999994, "text": " The Y and the dependent variable is like color right and interestingly he showed me an earlier version of this where he wasn't using color", "tokens": [440, 398, 293, 264, 12334, 7006, 307, 411, 2017, 558, 293, 25873, 415, 4712, 385, 364, 3071, 3037, 295, 341, 689, 415, 2067, 380, 1228, 2017], "temperature": 0.0, "avg_logprob": -0.18275328742133246, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.5907254641642794e-05}, {"id": 10, "seek": 2690, "start": 51.22, "end": 54.099999999999994, "text": " He was just like putting the actual numbers in here", "tokens": [634, 390, 445, 411, 3372, 264, 3539, 3547, 294, 510], "temperature": 0.0, "avg_logprob": -0.18275328742133246, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.5907254641642794e-05}, {"id": 11, "seek": 5410, "start": 54.1, "end": 60.42, "text": " And this thing kind of wasn't really working at all and as soon as he started using color it started working really well", "tokens": [400, 341, 551, 733, 295, 2067, 380, 534, 1364, 412, 439, 293, 382, 2321, 382, 415, 1409, 1228, 2017, 309, 1409, 1364, 534, 731], "temperature": 0.0, "avg_logprob": -0.18108471308913188, "compression_ratio": 1.722007722007722, "no_speech_prob": 8.53013352752896e-06}, {"id": 12, "seek": 5410, "start": 60.42, "end": 65.58, "text": " And so I wanted to mention that one of the things that unfortunately we we don't teach you", "tokens": [400, 370, 286, 1415, 281, 2152, 300, 472, 295, 264, 721, 300, 7015, 321, 321, 500, 380, 2924, 291], "temperature": 0.0, "avg_logprob": -0.18108471308913188, "compression_ratio": 1.722007722007722, "no_speech_prob": 8.53013352752896e-06}, {"id": 13, "seek": 5410, "start": 66.14, "end": 68.14, "text": " at USF is", "tokens": [412, 2546, 37, 307], "temperature": 0.0, "avg_logprob": -0.18108471308913188, "compression_ratio": 1.722007722007722, "no_speech_prob": 8.53013352752896e-06}, {"id": 14, "seek": 5410, "start": 68.54, "end": 70.86, "text": " Theory of human perception perhaps we should", "tokens": [29009, 295, 1952, 12860, 4317, 321, 820], "temperature": 0.0, "avg_logprob": -0.18108471308913188, "compression_ratio": 1.722007722007722, "no_speech_prob": 8.53013352752896e-06}, {"id": 15, "seek": 5410, "start": 71.42, "end": 77.02000000000001, "text": " Because actually when it comes to visualization, it's kind of the most important thing to know is what is the human eye?", "tokens": [1436, 767, 562, 309, 1487, 281, 25801, 11, 309, 311, 733, 295, 264, 881, 1021, 551, 281, 458, 307, 437, 307, 264, 1952, 3313, 30], "temperature": 0.0, "avg_logprob": -0.18108471308913188, "compression_ratio": 1.722007722007722, "no_speech_prob": 8.53013352752896e-06}, {"id": 16, "seek": 5410, "start": 77.9, "end": 80.98, "text": " Or what is what it was the human brain good at perceiving?", "tokens": [1610, 437, 307, 437, 309, 390, 264, 1952, 3567, 665, 412, 9016, 2123, 30], "temperature": 0.0, "avg_logprob": -0.18108471308913188, "compression_ratio": 1.722007722007722, "no_speech_prob": 8.53013352752896e-06}, {"id": 17, "seek": 8098, "start": 80.98, "end": 88.02000000000001, "text": " There's a whole area of academic study on this and one of the things that we're best at perceiving is differences in color", "tokens": [821, 311, 257, 1379, 1859, 295, 7778, 2979, 322, 341, 293, 472, 295, 264, 721, 300, 321, 434, 1151, 412, 9016, 2123, 307, 7300, 294, 2017], "temperature": 0.0, "avg_logprob": -0.13470231162177193, "compression_ratio": 1.6777777777777778, "no_speech_prob": 1.7061735206880257e-06}, {"id": 18, "seek": 8098, "start": 88.46000000000001, "end": 92.26, "text": " All right, so that's why as soon as we look at this picture of the synthetic data", "tokens": [1057, 558, 11, 370, 300, 311, 983, 382, 2321, 382, 321, 574, 412, 341, 3036, 295, 264, 23420, 1412], "temperature": 0.0, "avg_logprob": -0.13470231162177193, "compression_ratio": 1.6777777777777778, "no_speech_prob": 1.7061735206880257e-06}, {"id": 19, "seek": 8098, "start": 92.26, "end": 98.10000000000001, "text": " He created you can immediately see all this kind of four areas of you know lighter red color", "tokens": [634, 2942, 291, 393, 4258, 536, 439, 341, 733, 295, 1451, 3179, 295, 291, 458, 11546, 2182, 2017], "temperature": 0.0, "avg_logprob": -0.13470231162177193, "compression_ratio": 1.6777777777777778, "no_speech_prob": 1.7061735206880257e-06}, {"id": 20, "seek": 8098, "start": 98.7, "end": 100.82000000000001, "text": " So what he did was he said okay?", "tokens": [407, 437, 415, 630, 390, 415, 848, 1392, 30], "temperature": 0.0, "avg_logprob": -0.13470231162177193, "compression_ratio": 1.6777777777777778, "no_speech_prob": 1.7061735206880257e-06}, {"id": 21, "seek": 8098, "start": 100.82000000000001, "end": 109.26, "text": " What if we like tried to create a machine learning model of this synthetic data set and so specifically he created a tree?", "tokens": [708, 498, 321, 411, 3031, 281, 1884, 257, 3479, 2539, 2316, 295, 341, 23420, 1412, 992, 293, 370, 4682, 415, 2942, 257, 4230, 30], "temperature": 0.0, "avg_logprob": -0.13470231162177193, "compression_ratio": 1.6777777777777778, "no_speech_prob": 1.7061735206880257e-06}, {"id": 22, "seek": 10926, "start": 109.26, "end": 112.26, "text": " And the cool thing is that you can actually draw", "tokens": [400, 264, 1627, 551, 307, 300, 291, 393, 767, 2642], "temperature": 0.0, "avg_logprob": -0.18762257224635073, "compression_ratio": 1.8828125, "no_speech_prob": 1.4823519904894056e-06}, {"id": 23, "seek": 10926, "start": 112.9, "end": 119.46000000000001, "text": " The tree right so after he created the tree he did this all in that plot lid that plot lid is very flexible right he actually", "tokens": [440, 4230, 558, 370, 934, 415, 2942, 264, 4230, 415, 630, 341, 439, 294, 300, 7542, 10252, 300, 7542, 10252, 307, 588, 11358, 558, 415, 767], "temperature": 0.0, "avg_logprob": -0.18762257224635073, "compression_ratio": 1.8828125, "no_speech_prob": 1.4823519904894056e-06}, {"id": 24, "seek": 10926, "start": 119.58000000000001, "end": 121.84, "text": " drew the tree boundaries", "tokens": [12804, 264, 4230, 13180], "temperature": 0.0, "avg_logprob": -0.18762257224635073, "compression_ratio": 1.8828125, "no_speech_prob": 1.4823519904894056e-06}, {"id": 25, "seek": 10926, "start": 123.36, "end": 127.80000000000001, "text": " So that's already a pretty neat trick is to be actually able to draw the tree", "tokens": [407, 300, 311, 1217, 257, 1238, 10654, 4282, 307, 281, 312, 767, 1075, 281, 2642, 264, 4230], "temperature": 0.0, "avg_logprob": -0.18762257224635073, "compression_ratio": 1.8828125, "no_speech_prob": 1.4823519904894056e-06}, {"id": 26, "seek": 10926, "start": 127.86000000000001, "end": 133.42000000000002, "text": " But then he did something even cleverer which is he said okay, so what predictions does the tree make well?", "tokens": [583, 550, 415, 630, 746, 754, 13494, 260, 597, 307, 415, 848, 1392, 11, 370, 437, 21264, 775, 264, 4230, 652, 731, 30], "temperature": 0.0, "avg_logprob": -0.18762257224635073, "compression_ratio": 1.8828125, "no_speech_prob": 1.4823519904894056e-06}, {"id": 27, "seek": 13342, "start": 133.42, "end": 139.38, "text": " It's the average of each of these areas and so to do that we can actually draw the average color right?", "tokens": [467, 311, 264, 4274, 295, 1184, 295, 613, 3179, 293, 370, 281, 360, 300, 321, 393, 767, 2642, 264, 4274, 2017, 558, 30], "temperature": 0.0, "avg_logprob": -0.188921469229239, "compression_ratio": 1.6391752577319587, "no_speech_prob": 5.122892048348149e-07}, {"id": 28, "seek": 13342, "start": 139.89999999999998, "end": 141.89999999999998, "text": " It's actually kind of pretty", "tokens": [467, 311, 767, 733, 295, 1238], "temperature": 0.0, "avg_logprob": -0.188921469229239, "compression_ratio": 1.6391752577319587, "no_speech_prob": 5.122892048348149e-07}, {"id": 29, "seek": 13342, "start": 142.77999999999997, "end": 145.6, "text": " Here is the predictions that the tree makes", "tokens": [1692, 307, 264, 21264, 300, 264, 4230, 1669], "temperature": 0.0, "avg_logprob": -0.188921469229239, "compression_ratio": 1.6391752577319587, "no_speech_prob": 5.122892048348149e-07}, {"id": 30, "seek": 13342, "start": 146.54, "end": 148.17999999999998, "text": " now", "tokens": [586], "temperature": 0.0, "avg_logprob": -0.188921469229239, "compression_ratio": 1.6391752577319587, "no_speech_prob": 5.122892048348149e-07}, {"id": 31, "seek": 13342, "start": 148.17999999999998, "end": 152.66, "text": " Here's where it gets really interesting is like you can as you know randomly", "tokens": [1692, 311, 689, 309, 2170, 534, 1880, 307, 411, 291, 393, 382, 291, 458, 16979], "temperature": 0.0, "avg_logprob": -0.188921469229239, "compression_ratio": 1.6391752577319587, "no_speech_prob": 5.122892048348149e-07}, {"id": 32, "seek": 13342, "start": 153.98, "end": 156.73999999999998, "text": " generate trees through resampling and", "tokens": [8460, 5852, 807, 725, 335, 11970, 293], "temperature": 0.0, "avg_logprob": -0.188921469229239, "compression_ratio": 1.6391752577319587, "no_speech_prob": 5.122892048348149e-07}, {"id": 33, "seek": 13342, "start": 157.66, "end": 159.66, "text": " So here are four trees", "tokens": [407, 510, 366, 1451, 5852], "temperature": 0.0, "avg_logprob": -0.188921469229239, "compression_ratio": 1.6391752577319587, "no_speech_prob": 5.122892048348149e-07}, {"id": 34, "seek": 15966, "start": 159.66, "end": 162.9, "text": " Generated through resampling they're all like pretty similar", "tokens": [15409, 770, 807, 725, 335, 11970, 436, 434, 439, 411, 1238, 2531], "temperature": 0.0, "avg_logprob": -0.18192337606554834, "compression_ratio": 1.7350746268656716, "no_speech_prob": 2.6425782380101737e-06}, {"id": 35, "seek": 15966, "start": 162.9, "end": 170.22, "text": " But a little bit different and so now we can actually visualize bagging and to visualize bagging we literally take the average of", "tokens": [583, 257, 707, 857, 819, 293, 370, 586, 321, 393, 767, 23273, 3411, 3249, 293, 281, 23273, 3411, 3249, 321, 3736, 747, 264, 4274, 295], "temperature": 0.0, "avg_logprob": -0.18192337606554834, "compression_ratio": 1.7350746268656716, "no_speech_prob": 2.6425782380101737e-06}, {"id": 36, "seek": 15966, "start": 170.62, "end": 173.9, "text": " The four pictures all right. That's what bagging is and", "tokens": [440, 1451, 5242, 439, 558, 13, 663, 311, 437, 3411, 3249, 307, 293], "temperature": 0.0, "avg_logprob": -0.18192337606554834, "compression_ratio": 1.7350746268656716, "no_speech_prob": 2.6425782380101737e-06}, {"id": 37, "seek": 15966, "start": 174.9, "end": 181.51999999999998, "text": " There it is right and so here is like the the fuzzy decision boundaries of a random forest", "tokens": [821, 309, 307, 558, 293, 370, 510, 307, 411, 264, 264, 34710, 3537, 13180, 295, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.18192337606554834, "compression_ratio": 1.7350746268656716, "no_speech_prob": 2.6425782380101737e-06}, {"id": 38, "seek": 15966, "start": 182.22, "end": 189.42, "text": " And I think this is kind of amazing right because it's like a I wish I had this actually when I started teaching you all random", "tokens": [400, 286, 519, 341, 307, 733, 295, 2243, 558, 570, 309, 311, 411, 257, 286, 3172, 286, 632, 341, 767, 562, 286, 1409, 4571, 291, 439, 4974], "temperature": 0.0, "avg_logprob": -0.18192337606554834, "compression_ratio": 1.7350746268656716, "no_speech_prob": 2.6425782380101737e-06}, {"id": 39, "seek": 18942, "start": 189.42, "end": 193.94, "text": " Forest because I could have skipped a couple of classes. It's just like okay. That's what we do", "tokens": [18124, 570, 286, 727, 362, 30193, 257, 1916, 295, 5359, 13, 467, 311, 445, 411, 1392, 13, 663, 311, 437, 321, 360], "temperature": 0.0, "avg_logprob": -0.17557953399361917, "compression_ratio": 1.6324110671936758, "no_speech_prob": 2.948010205727769e-06}, {"id": 40, "seek": 18942, "start": 194.1, "end": 198.42, "text": " You know we create the decision boundaries we average each area", "tokens": [509, 458, 321, 1884, 264, 3537, 13180, 321, 4274, 1184, 1859], "temperature": 0.0, "avg_logprob": -0.17557953399361917, "compression_ratio": 1.6324110671936758, "no_speech_prob": 2.948010205727769e-06}, {"id": 41, "seek": 18942, "start": 198.42, "end": 202.45999999999998, "text": " And then we we do it a few times and average all of them okay?", "tokens": [400, 550, 321, 321, 360, 309, 257, 1326, 1413, 293, 4274, 439, 295, 552, 1392, 30], "temperature": 0.0, "avg_logprob": -0.17557953399361917, "compression_ratio": 1.6324110671936758, "no_speech_prob": 2.948010205727769e-06}, {"id": 42, "seek": 18942, "start": 202.45999999999998, "end": 206.1, "text": " So that's what a random forest does and I think like this is just such a great example of", "tokens": [407, 300, 311, 437, 257, 4974, 6719, 775, 293, 286, 519, 411, 341, 307, 445, 1270, 257, 869, 1365, 295], "temperature": 0.0, "avg_logprob": -0.17557953399361917, "compression_ratio": 1.6324110671936758, "no_speech_prob": 2.948010205727769e-06}, {"id": 43, "seek": 18942, "start": 208.57999999999998, "end": 212.16, "text": " Making the complex easy through through pictures", "tokens": [14595, 264, 3997, 1858, 807, 807, 5242], "temperature": 0.0, "avg_logprob": -0.17557953399361917, "compression_ratio": 1.6324110671936758, "no_speech_prob": 2.948010205727769e-06}, {"id": 44, "seek": 18942, "start": 212.82, "end": 214.82, "text": " So congrats to Tyler for that", "tokens": [407, 8882, 1720, 281, 16869, 337, 300], "temperature": 0.0, "avg_logprob": -0.17557953399361917, "compression_ratio": 1.6324110671936758, "no_speech_prob": 2.948010205727769e-06}, {"id": 45, "seek": 18942, "start": 215.33999999999997, "end": 217.33999999999997, "text": " It actually turns out", "tokens": [467, 767, 4523, 484], "temperature": 0.0, "avg_logprob": -0.17557953399361917, "compression_ratio": 1.6324110671936758, "no_speech_prob": 2.948010205727769e-06}, {"id": 46, "seek": 21734, "start": 217.34, "end": 224.02, "text": " That he has actually reinvented something that somebody else has already done a guy called Christian any who went on to be a", "tokens": [663, 415, 575, 767, 33477, 292, 746, 300, 2618, 1646, 575, 1217, 1096, 257, 2146, 1219, 5778, 604, 567, 1437, 322, 281, 312, 257], "temperature": 0.0, "avg_logprob": -0.18992747579302108, "compression_ratio": 1.8299319727891157, "no_speech_prob": 4.495104803936556e-06}, {"id": 47, "seek": 21734, "start": 225.14000000000001, "end": 231.02, "text": " One of the world's foremost machine learning research is actually included almost exactly this technique in a book", "tokens": [1485, 295, 264, 1002, 311, 18864, 3479, 2539, 2132, 307, 767, 5556, 1920, 2293, 341, 6532, 294, 257, 1446], "temperature": 0.0, "avg_logprob": -0.18992747579302108, "compression_ratio": 1.8299319727891157, "no_speech_prob": 4.495104803936556e-06}, {"id": 48, "seek": 21734, "start": 231.02, "end": 236.2, "text": " He wrote about decision forests, so it's actually kind of cool that Tyler ended up reinventing something", "tokens": [634, 4114, 466, 3537, 21700, 11, 370, 309, 311, 767, 733, 295, 1627, 300, 16869, 4590, 493, 33477, 278, 746], "temperature": 0.0, "avg_logprob": -0.18992747579302108, "compression_ratio": 1.8299319727891157, "no_speech_prob": 4.495104803936556e-06}, {"id": 49, "seek": 21734, "start": 236.46, "end": 241.72, "text": " That one of the world's foremost and for authorities under fit decision forests actually it has created", "tokens": [663, 472, 295, 264, 1002, 311, 18864, 293, 337, 12076, 833, 3318, 3537, 21700, 767, 309, 575, 2942], "temperature": 0.0, "avg_logprob": -0.18992747579302108, "compression_ratio": 1.8299319727891157, "no_speech_prob": 4.495104803936556e-06}, {"id": 50, "seek": 21734, "start": 241.72, "end": 245.96, "text": " So I thought that was neat that's nice because when we pop when we posted this on Twitter", "tokens": [407, 286, 1194, 300, 390, 10654, 300, 311, 1481, 570, 562, 321, 1665, 562, 321, 9437, 341, 322, 5794], "temperature": 0.0, "avg_logprob": -0.18992747579302108, "compression_ratio": 1.8299319727891157, "no_speech_prob": 4.495104803936556e-06}, {"id": 51, "seek": 24596, "start": 245.96, "end": 251.52, "text": " You know got a lot of attention and finally somebody was able to say like oh, you know what this this actually already exists", "tokens": [509, 458, 658, 257, 688, 295, 3202, 293, 2721, 2618, 390, 1075, 281, 584, 411, 1954, 11, 291, 458, 437, 341, 341, 767, 1217, 8198], "temperature": 0.0, "avg_logprob": -0.22830404084304284, "compression_ratio": 1.5521739130434782, "no_speech_prob": 1.760335408107494e-06}, {"id": 52, "seek": 24596, "start": 251.96, "end": 254.56, "text": " So Tyler's gone away, and you know started reading that book", "tokens": [407, 16869, 311, 2780, 1314, 11, 293, 291, 458, 1409, 3760, 300, 1446], "temperature": 0.0, "avg_logprob": -0.22830404084304284, "compression_ratio": 1.5521739130434782, "no_speech_prob": 1.760335408107494e-06}, {"id": 53, "seek": 24596, "start": 257.16, "end": 260.32, "text": " Something else which is super cool is Jason Carpenter", "tokens": [6595, 1646, 597, 307, 1687, 1627, 307, 11181, 2741, 79, 14278], "temperature": 0.0, "avg_logprob": -0.22830404084304284, "compression_ratio": 1.5521739130434782, "no_speech_prob": 1.760335408107494e-06}, {"id": 54, "seek": 24596, "start": 262.16, "end": 264.88, "text": " Created a whole new library called Parfit", "tokens": [11972, 292, 257, 1379, 777, 6405, 1219, 3457, 6845], "temperature": 0.0, "avg_logprob": -0.22830404084304284, "compression_ratio": 1.5521739130434782, "no_speech_prob": 1.760335408107494e-06}, {"id": 55, "seek": 24596, "start": 265.64, "end": 267.64, "text": " and Parfit is a", "tokens": [293, 3457, 6845, 307, 257], "temperature": 0.0, "avg_logprob": -0.22830404084304284, "compression_ratio": 1.5521739130434782, "no_speech_prob": 1.760335408107494e-06}, {"id": 56, "seek": 24596, "start": 268.16, "end": 271.36, "text": " parallelized fitting of multiple models for the purpose of", "tokens": [8952, 1602, 15669, 295, 3866, 5245, 337, 264, 4334, 295], "temperature": 0.0, "avg_logprob": -0.22830404084304284, "compression_ratio": 1.5521739130434782, "no_speech_prob": 1.760335408107494e-06}, {"id": 57, "seek": 27136, "start": 271.36, "end": 277.56, "text": " of selecting hyper parameters, and there's a lot I really like about this and", "tokens": [295, 18182, 9848, 9834, 11, 293, 456, 311, 257, 688, 286, 534, 411, 466, 341, 293], "temperature": 0.0, "avg_logprob": -0.20940770159710895, "compression_ratio": 1.610878661087866, "no_speech_prob": 5.507525656867074e-06}, {"id": 58, "seek": 27136, "start": 279.40000000000003, "end": 286.8, "text": " He's shown a clear example of how to use it right and like the API looks very similar to other grid search based approaches", "tokens": [634, 311, 4898, 257, 1850, 1365, 295, 577, 281, 764, 309, 558, 293, 411, 264, 9362, 1542, 588, 2531, 281, 661, 10748, 3164, 2361, 11587], "temperature": 0.0, "avg_logprob": -0.20940770159710895, "compression_ratio": 1.610878661087866, "no_speech_prob": 5.507525656867074e-06}, {"id": 59, "seek": 27136, "start": 287.0, "end": 290.16, "text": " But it uses the validation techniques that", "tokens": [583, 309, 4960, 264, 24071, 7512, 300], "temperature": 0.0, "avg_logprob": -0.20940770159710895, "compression_ratio": 1.610878661087866, "no_speech_prob": 5.507525656867074e-06}, {"id": 60, "seek": 27136, "start": 290.84000000000003, "end": 295.6, "text": " Rachel wrote about and that we learned about a couple of weeks ago of using a good validation set", "tokens": [14246, 4114, 466, 293, 300, 321, 3264, 466, 257, 1916, 295, 3259, 2057, 295, 1228, 257, 665, 24071, 992], "temperature": 0.0, "avg_logprob": -0.20940770159710895, "compression_ratio": 1.610878661087866, "no_speech_prob": 5.507525656867074e-06}, {"id": 61, "seek": 27136, "start": 296.48, "end": 297.88, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.20940770159710895, "compression_ratio": 1.610878661087866, "no_speech_prob": 5.507525656867074e-06}, {"id": 62, "seek": 29788, "start": 297.88, "end": 304.24, "text": " You know what he's done here is in his blog post that introduces it. You know he's he's", "tokens": [509, 458, 437, 415, 311, 1096, 510, 307, 294, 702, 6968, 2183, 300, 31472, 309, 13, 509, 458, 415, 311, 415, 311], "temperature": 0.0, "avg_logprob": -0.13701151035450124, "compression_ratio": 1.66015625, "no_speech_prob": 5.594285084953299e-06}, {"id": 63, "seek": 29788, "start": 305.2, "end": 307.96, "text": " Gone right back and said like well. What are hyper parameters?", "tokens": [39068, 558, 646, 293, 848, 411, 731, 13, 708, 366, 9848, 9834, 30], "temperature": 0.0, "avg_logprob": -0.13701151035450124, "compression_ratio": 1.66015625, "no_speech_prob": 5.594285084953299e-06}, {"id": 64, "seek": 29788, "start": 307.96, "end": 313.92, "text": " Why do we have to train them and he's kind of explained every step and then the the module itself is", "tokens": [1545, 360, 321, 362, 281, 3847, 552, 293, 415, 311, 733, 295, 8825, 633, 1823, 293, 550, 264, 264, 10088, 2564, 307], "temperature": 0.0, "avg_logprob": -0.13701151035450124, "compression_ratio": 1.66015625, "no_speech_prob": 5.594285084953299e-06}, {"id": 65, "seek": 29788, "start": 314.32, "end": 319.62, "text": " Like it's very polished. You know he's added documentation to it. He's added a nice read me to it", "tokens": [1743, 309, 311, 588, 29079, 13, 509, 458, 415, 311, 3869, 14333, 281, 309, 13, 634, 311, 3869, 257, 1481, 1401, 385, 281, 309], "temperature": 0.0, "avg_logprob": -0.13701151035450124, "compression_ratio": 1.66015625, "no_speech_prob": 5.594285084953299e-06}, {"id": 66, "seek": 29788, "start": 320.15999999999997, "end": 322.96, "text": " And it's kind of interesting when you actually look at the code you realize", "tokens": [400, 309, 311, 733, 295, 1880, 562, 291, 767, 574, 412, 264, 3089, 291, 4325], "temperature": 0.0, "avg_logprob": -0.13701151035450124, "compression_ratio": 1.66015625, "no_speech_prob": 5.594285084953299e-06}, {"id": 67, "seek": 32296, "start": 322.96, "end": 329.52, "text": " You know it's very simple. You know which is it's definitely not a bad thing. That's a good thing is to make things simple", "tokens": [509, 458, 309, 311, 588, 2199, 13, 509, 458, 597, 307, 309, 311, 2138, 406, 257, 1578, 551, 13, 663, 311, 257, 665, 551, 307, 281, 652, 721, 2199], "temperature": 0.0, "avg_logprob": -0.19573571547022406, "compression_ratio": 1.6706349206349207, "no_speech_prob": 2.9479908789653564e-06}, {"id": 68, "seek": 32296, "start": 331.23999999999995, "end": 333.08, "text": " But by kind of", "tokens": [583, 538, 733, 295], "temperature": 0.0, "avg_logprob": -0.19573571547022406, "compression_ratio": 1.6706349206349207, "no_speech_prob": 2.9479908789653564e-06}, {"id": 69, "seek": 32296, "start": 333.08, "end": 335.76, "text": " Writing this little bit of code and then packaging it up so nicely", "tokens": [32774, 341, 707, 857, 295, 3089, 293, 550, 16836, 309, 493, 370, 9594], "temperature": 0.0, "avg_logprob": -0.19573571547022406, "compression_ratio": 1.6706349206349207, "no_speech_prob": 2.9479908789653564e-06}, {"id": 70, "seek": 32296, "start": 335.76, "end": 341.47999999999996, "text": " He's made it really easy for other people to use this technique which is great and so", "tokens": [634, 311, 1027, 309, 534, 1858, 337, 661, 561, 281, 764, 341, 6532, 597, 307, 869, 293, 370], "temperature": 0.0, "avg_logprob": -0.19573571547022406, "compression_ratio": 1.6706349206349207, "no_speech_prob": 2.9479908789653564e-06}, {"id": 71, "seek": 32296, "start": 342.44, "end": 344.67999999999995, "text": " One of the things I've been really thrilled to see is then", "tokens": [1485, 295, 264, 721, 286, 600, 668, 534, 18744, 281, 536, 307, 550], "temperature": 0.0, "avg_logprob": -0.19573571547022406, "compression_ratio": 1.6706349206349207, "no_speech_prob": 2.9479908789653564e-06}, {"id": 72, "seek": 32296, "start": 345.12, "end": 350.15999999999997, "text": " Vanay went along and combined two things from our class one was to take", "tokens": [8979, 320, 1437, 2051, 293, 9354, 732, 721, 490, 527, 1508, 472, 390, 281, 747], "temperature": 0.0, "avg_logprob": -0.19573571547022406, "compression_ratio": 1.6706349206349207, "no_speech_prob": 2.9479908789653564e-06}, {"id": 73, "seek": 35016, "start": 350.16, "end": 356.32000000000005, "text": " Parfit and then the other was to take the kind of accelerated SGD approach to classification", "tokens": [3457, 6845, 293, 550, 264, 661, 390, 281, 747, 264, 733, 295, 29763, 34520, 35, 3109, 281, 21538], "temperature": 0.0, "avg_logprob": -0.2540216138285975, "compression_ratio": 1.5732758620689655, "no_speech_prob": 1.0676974397938466e-06}, {"id": 74, "seek": 35016, "start": 356.40000000000003, "end": 361.76000000000005, "text": " We don't learn about in the last lesson and combine the two to say like okay. Well. Let's now use", "tokens": [492, 500, 380, 1466, 466, 294, 264, 1036, 6898, 293, 10432, 264, 732, 281, 584, 411, 1392, 13, 1042, 13, 961, 311, 586, 764], "temperature": 0.0, "avg_logprob": -0.2540216138285975, "compression_ratio": 1.5732758620689655, "no_speech_prob": 1.0676974397938466e-06}, {"id": 75, "seek": 35016, "start": 362.76000000000005, "end": 365.76000000000005, "text": " Parfit to help us find the parameters of a", "tokens": [3457, 6845, 281, 854, 505, 915, 264, 9834, 295, 257], "temperature": 0.0, "avg_logprob": -0.2540216138285975, "compression_ratio": 1.5732758620689655, "no_speech_prob": 1.0676974397938466e-06}, {"id": 76, "seek": 35016, "start": 366.56, "end": 368.56, "text": " SGD logistic aggression", "tokens": [34520, 35, 3565, 3142, 30268], "temperature": 0.0, "avg_logprob": -0.2540216138285975, "compression_ratio": 1.5732758620689655, "no_speech_prob": 1.0676974397938466e-06}, {"id": 77, "seek": 35016, "start": 369.12, "end": 371.68, "text": " So I think that's really a really great idea", "tokens": [407, 286, 519, 300, 311, 534, 257, 534, 869, 1558], "temperature": 0.0, "avg_logprob": -0.2540216138285975, "compression_ratio": 1.5732758620689655, "no_speech_prob": 1.0676974397938466e-06}, {"id": 78, "seek": 35016, "start": 374.20000000000005, "end": 376.20000000000005, "text": " Something else which I thought was terrific is", "tokens": [6595, 1646, 597, 286, 1194, 390, 20899, 307], "temperature": 0.0, "avg_logprob": -0.2540216138285975, "compression_ratio": 1.5732758620689655, "no_speech_prob": 1.0676974397938466e-06}, {"id": 79, "seek": 35016, "start": 376.96000000000004, "end": 378.96000000000004, "text": " prints actually", "tokens": [22305, 767], "temperature": 0.0, "avg_logprob": -0.2540216138285975, "compression_ratio": 1.5732758620689655, "no_speech_prob": 1.0676974397938466e-06}, {"id": 80, "seek": 37896, "start": 378.96, "end": 380.96, "text": " basically went through and", "tokens": [1936, 1437, 807, 293], "temperature": 0.0, "avg_logprob": -0.18646437781197683, "compression_ratio": 1.7178217821782178, "no_speech_prob": 3.668834779091412e-06}, {"id": 81, "seek": 37896, "start": 382.32, "end": 388.0, "text": " Summarized pretty much all the stuff we learned in the random and for random forest interpretation class", "tokens": [8626, 6209, 1602, 1238, 709, 439, 264, 1507, 321, 3264, 294, 264, 4974, 293, 337, 4974, 6719, 14174, 1508], "temperature": 0.0, "avg_logprob": -0.18646437781197683, "compression_ratio": 1.7178217821782178, "no_speech_prob": 3.668834779091412e-06}, {"id": 82, "seek": 37896, "start": 388.84, "end": 395.14, "text": " And he went even further than that as he described each of the different approaches to random forest interpretation", "tokens": [400, 415, 1437, 754, 3052, 813, 300, 382, 415, 7619, 1184, 295, 264, 819, 11587, 281, 4974, 6719, 14174], "temperature": 0.0, "avg_logprob": -0.18646437781197683, "compression_ratio": 1.7178217821782178, "no_speech_prob": 3.668834779091412e-06}, {"id": 83, "seek": 37896, "start": 397.03999999999996, "end": 402.91999999999996, "text": " He described how it's done so here for example is feature importance through variable permutation a", "tokens": [634, 7619, 577, 309, 311, 1096, 370, 510, 337, 1365, 307, 4111, 7379, 807, 7006, 4784, 11380, 257], "temperature": 0.0, "avg_logprob": -0.18646437781197683, "compression_ratio": 1.7178217821782178, "no_speech_prob": 3.668834779091412e-06}, {"id": 84, "seek": 40292, "start": 402.92, "end": 409.44, "text": " Little picture of each one and then super cool. Here is the code to implement it from scratch", "tokens": [8022, 3036, 295, 1184, 472, 293, 550, 1687, 1627, 13, 1692, 307, 264, 3089, 281, 4445, 309, 490, 8459], "temperature": 0.0, "avg_logprob": -0.15986192095410692, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.1726381217158632e-06}, {"id": 85, "seek": 40292, "start": 410.64000000000004, "end": 412.64000000000004, "text": " So I think this is like really", "tokens": [407, 286, 519, 341, 307, 411, 534], "temperature": 0.0, "avg_logprob": -0.15986192095410692, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.1726381217158632e-06}, {"id": 86, "seek": 40292, "start": 413.36, "end": 420.78000000000003, "text": " Nice post you know describing something that not many people understand and showing you know exactly how it works both with pictures", "tokens": [5490, 2183, 291, 458, 16141, 746, 300, 406, 867, 561, 1223, 293, 4099, 291, 458, 2293, 577, 309, 1985, 1293, 365, 5242], "temperature": 0.0, "avg_logprob": -0.15986192095410692, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.1726381217158632e-06}, {"id": 87, "seek": 40292, "start": 421.32, "end": 423.64, "text": " And with code that implements it from scratch", "tokens": [400, 365, 3089, 300, 704, 17988, 309, 490, 8459], "temperature": 0.0, "avg_logprob": -0.15986192095410692, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.1726381217158632e-06}, {"id": 88, "seek": 40292, "start": 424.40000000000003, "end": 429.16, "text": " So I think that's really really great one of the things I really like here is that for like the?", "tokens": [407, 286, 519, 300, 311, 534, 534, 869, 472, 295, 264, 721, 286, 534, 411, 510, 307, 300, 337, 411, 264, 30], "temperature": 0.0, "avg_logprob": -0.15986192095410692, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.1726381217158632e-06}, {"id": 89, "seek": 42916, "start": 429.16, "end": 434.36, "text": " Tree interpreter part he actually showed how you can take the tree interpreter", "tokens": [22291, 34132, 644, 415, 767, 4712, 577, 291, 393, 747, 264, 4230, 34132], "temperature": 0.0, "avg_logprob": -0.19632123765491305, "compression_ratio": 1.7256637168141593, "no_speech_prob": 1.061598231899552e-05}, {"id": 90, "seek": 42916, "start": 434.84000000000003, "end": 443.0, "text": " Output and feed it into the new waterfall chart package that Chris our USF student built to show how you can actually visualize", "tokens": [5925, 2582, 293, 3154, 309, 666, 264, 777, 27848, 6927, 7372, 300, 6688, 527, 2546, 37, 3107, 3094, 281, 855, 577, 291, 393, 767, 23273], "temperature": 0.0, "avg_logprob": -0.19632123765491305, "compression_ratio": 1.7256637168141593, "no_speech_prob": 1.061598231899552e-05}, {"id": 91, "seek": 42916, "start": 444.36, "end": 450.18, "text": " The contributions of the tree interpreter in a waterfall chart so again kind of a nice combination of", "tokens": [440, 15725, 295, 264, 4230, 34132, 294, 257, 27848, 6927, 370, 797, 733, 295, 257, 1481, 6562, 295], "temperature": 0.0, "avg_logprob": -0.19632123765491305, "compression_ratio": 1.7256637168141593, "no_speech_prob": 1.061598231899552e-05}, {"id": 92, "seek": 42916, "start": 450.8, "end": 455.88, "text": " Multiple pieces of technology we've both learned about and and built as a group", "tokens": [40056, 3755, 295, 2899, 321, 600, 1293, 3264, 466, 293, 293, 3094, 382, 257, 1594], "temperature": 0.0, "avg_logprob": -0.19632123765491305, "compression_ratio": 1.7256637168141593, "no_speech_prob": 1.061598231899552e-05}, {"id": 93, "seek": 42916, "start": 457.0, "end": 458.12, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.19632123765491305, "compression_ratio": 1.7256637168141593, "no_speech_prob": 1.061598231899552e-05}, {"id": 94, "seek": 45812, "start": 458.12, "end": 463.2, "text": " Also really thought this kernel there's been a few interesting kernels shared, and I'll share some more next week", "tokens": [2743, 534, 1194, 341, 28256, 456, 311, 668, 257, 1326, 1880, 23434, 1625, 5507, 11, 293, 286, 603, 2073, 512, 544, 958, 1243], "temperature": 0.0, "avg_logprob": -0.2774757238534781, "compression_ratio": 1.625, "no_speech_prob": 9.818175385589711e-06}, {"id": 95, "seek": 45812, "start": 463.2, "end": 471.44, "text": " And to best wrote this really nice kernel showing there's this quite challenging Kaggle competition on detecting icebergs", "tokens": [400, 281, 1151, 4114, 341, 534, 1481, 28256, 4099, 456, 311, 341, 1596, 7595, 48751, 22631, 6211, 322, 40237, 38880, 82], "temperature": 0.0, "avg_logprob": -0.2774757238534781, "compression_ratio": 1.625, "no_speech_prob": 9.818175385589711e-06}, {"id": 96, "seek": 45812, "start": 472.48, "end": 473.96, "text": " versus chips", "tokens": [5717, 11583], "temperature": 0.0, "avg_logprob": -0.2774757238534781, "compression_ratio": 1.625, "no_speech_prob": 9.818175385589711e-06}, {"id": 97, "seek": 45812, "start": 473.96, "end": 481.92, "text": " And it's a kind of a weird two channel satellite data, which is very hard to visualize and he actually", "tokens": [400, 309, 311, 257, 733, 295, 257, 3657, 732, 2269, 16016, 1412, 11, 597, 307, 588, 1152, 281, 23273, 293, 415, 767], "temperature": 0.0, "avg_logprob": -0.2774757238534781, "compression_ratio": 1.625, "no_speech_prob": 9.818175385589711e-06}, {"id": 98, "seek": 48192, "start": 481.92, "end": 490.44, "text": " Went through and basically described kind of the formulas for how these like radar scattering things actually work", "tokens": [31809, 807, 293, 1936, 7619, 733, 295, 264, 30546, 337, 577, 613, 411, 16544, 42314, 721, 767, 589], "temperature": 0.0, "avg_logprob": -0.19696722311132095, "compression_ratio": 1.513089005235602, "no_speech_prob": 3.726612703758292e-06}, {"id": 99, "seek": 48192, "start": 491.44, "end": 497.0, "text": " And then actually managed to come up with the code that allowed him to recreate", "tokens": [400, 550, 767, 6453, 281, 808, 493, 365, 264, 3089, 300, 4350, 796, 281, 25833], "temperature": 0.0, "avg_logprob": -0.19696722311132095, "compression_ratio": 1.513089005235602, "no_speech_prob": 3.726612703758292e-06}, {"id": 100, "seek": 48192, "start": 497.8, "end": 499.8, "text": " You know the actual 3d", "tokens": [509, 458, 264, 3539, 805, 67], "temperature": 0.0, "avg_logprob": -0.19696722311132095, "compression_ratio": 1.513089005235602, "no_speech_prob": 3.726612703758292e-06}, {"id": 101, "seek": 48192, "start": 501.32, "end": 503.16, "text": " icebergs", "tokens": [38880, 82], "temperature": 0.0, "avg_logprob": -0.19696722311132095, "compression_ratio": 1.513089005235602, "no_speech_prob": 3.726612703758292e-06}, {"id": 102, "seek": 48192, "start": 503.16, "end": 508.1, "text": " Or ships, and I have not seen that done before like I you know", "tokens": [1610, 11434, 11, 293, 286, 362, 406, 1612, 300, 1096, 949, 411, 286, 291, 458], "temperature": 0.0, "avg_logprob": -0.19696722311132095, "compression_ratio": 1.513089005235602, "no_speech_prob": 3.726612703758292e-06}, {"id": 103, "seek": 50810, "start": 508.1, "end": 513.44, "text": " It's it's quite challenging to know how to visualize this data, and then he went on to show how to build a", "tokens": [467, 311, 309, 311, 1596, 7595, 281, 458, 577, 281, 23273, 341, 1412, 11, 293, 550, 415, 1437, 322, 281, 855, 577, 281, 1322, 257], "temperature": 0.0, "avg_logprob": -0.13982944646157508, "compression_ratio": 1.75, "no_speech_prob": 6.339030733215623e-06}, {"id": 104, "seek": 50810, "start": 514.0400000000001, "end": 518.84, "text": " Neural net to try to interpret this so that was pretty fantastic as well", "tokens": [1734, 1807, 2533, 281, 853, 281, 7302, 341, 370, 300, 390, 1238, 5456, 382, 731], "temperature": 0.0, "avg_logprob": -0.13982944646157508, "compression_ratio": 1.75, "no_speech_prob": 6.339030733215623e-06}, {"id": 105, "seek": 50810, "start": 520.12, "end": 524.2, "text": " So yeah, congratulations for all of you. I know for a lot of you. You know you're", "tokens": [407, 1338, 11, 13568, 337, 439, 295, 291, 13, 286, 458, 337, 257, 688, 295, 291, 13, 509, 458, 291, 434], "temperature": 0.0, "avg_logprob": -0.13982944646157508, "compression_ratio": 1.75, "no_speech_prob": 6.339030733215623e-06}, {"id": 106, "seek": 50810, "start": 525.64, "end": 531.34, "text": " Posting stuff out there to the rest of the world for the first time you know and it's kind of intimidating", "tokens": [10223, 278, 1507, 484, 456, 281, 264, 1472, 295, 264, 1002, 337, 264, 700, 565, 291, 458, 293, 309, 311, 733, 295, 29714], "temperature": 0.0, "avg_logprob": -0.13982944646157508, "compression_ratio": 1.75, "no_speech_prob": 6.339030733215623e-06}, {"id": 107, "seek": 50810, "start": 531.5600000000001, "end": 536.4, "text": " you're used to writing stuff that you kind of hand into a teacher, and they're the only ones who see it and", "tokens": [291, 434, 1143, 281, 3579, 1507, 300, 291, 733, 295, 1011, 666, 257, 5027, 11, 293, 436, 434, 264, 787, 2306, 567, 536, 309, 293], "temperature": 0.0, "avg_logprob": -0.13982944646157508, "compression_ratio": 1.75, "no_speech_prob": 6.339030733215623e-06}, {"id": 108, "seek": 53640, "start": 536.4, "end": 540.0799999999999, "text": " You know it's kind of scary the first time you do it", "tokens": [509, 458, 309, 311, 733, 295, 6958, 264, 700, 565, 291, 360, 309], "temperature": 0.0, "avg_logprob": -0.18976417852907765, "compression_ratio": 1.6370967741935485, "no_speech_prob": 6.048853265383514e-06}, {"id": 109, "seek": 53640, "start": 540.0799999999999, "end": 545.52, "text": " But then the first time somebody you know up votes your cattle Colonel or adds a clap to your median post", "tokens": [583, 550, 264, 700, 565, 2618, 291, 458, 493, 12068, 428, 19992, 28478, 420, 10860, 257, 20760, 281, 428, 26779, 2183], "temperature": 0.0, "avg_logprob": -0.18976417852907765, "compression_ratio": 1.6370967741935485, "no_speech_prob": 6.048853265383514e-06}, {"id": 110, "seek": 53640, "start": 545.56, "end": 551.4399999999999, "text": " He suddenly realized oh, I'm actually I've written something that people like that's that's pretty great", "tokens": [634, 5800, 5334, 1954, 11, 286, 478, 767, 286, 600, 3720, 746, 300, 561, 411, 300, 311, 300, 311, 1238, 869], "temperature": 0.0, "avg_logprob": -0.18976417852907765, "compression_ratio": 1.6370967741935485, "no_speech_prob": 6.048853265383514e-06}, {"id": 111, "seek": 53640, "start": 552.04, "end": 557.1999999999999, "text": " So if you haven't tried yourself yet, I again invite you to", "tokens": [407, 498, 291, 2378, 380, 3031, 1803, 1939, 11, 286, 797, 7980, 291, 281], "temperature": 0.0, "avg_logprob": -0.18976417852907765, "compression_ratio": 1.6370967741935485, "no_speech_prob": 6.048853265383514e-06}, {"id": 112, "seek": 53640, "start": 558.04, "end": 562.52, "text": " Try writing something and if you're not sure you could write a summary of a lesson", "tokens": [6526, 3579, 746, 293, 498, 291, 434, 406, 988, 291, 727, 2464, 257, 12691, 295, 257, 6898], "temperature": 0.0, "avg_logprob": -0.18976417852907765, "compression_ratio": 1.6370967741935485, "no_speech_prob": 6.048853265383514e-06}, {"id": 113, "seek": 56252, "start": 562.52, "end": 569.6999999999999, "text": " You could write a summary of like if there's something you found hard like maybe you found it hard to fire up a GPU based", "tokens": [509, 727, 2464, 257, 12691, 295, 411, 498, 456, 311, 746, 291, 1352, 1152, 411, 1310, 291, 1352, 309, 1152, 281, 2610, 493, 257, 18407, 2361], "temperature": 0.0, "avg_logprob": -0.17406124538845485, "compression_ratio": 1.8206896551724139, "no_speech_prob": 3.966929853049805e-06}, {"id": 114, "seek": 56252, "start": 569.88, "end": 571.76, "text": " AWS instance you eventually figured it out", "tokens": [17650, 5197, 291, 4728, 8932, 309, 484], "temperature": 0.0, "avg_logprob": -0.17406124538845485, "compression_ratio": 1.8206896551724139, "no_speech_prob": 3.966929853049805e-06}, {"id": 115, "seek": 56252, "start": 571.76, "end": 576.3199999999999, "text": " You could write down just describe how you solve that problem or if one of your classmates", "tokens": [509, 727, 2464, 760, 445, 6786, 577, 291, 5039, 300, 1154, 420, 498, 472, 295, 428, 24964], "temperature": 0.0, "avg_logprob": -0.17406124538845485, "compression_ratio": 1.8206896551724139, "no_speech_prob": 3.966929853049805e-06}, {"id": 116, "seek": 56252, "start": 577.0799999999999, "end": 582.4399999999999, "text": " Didn't understand something and you explained it to them then you could like write down something saying like oh", "tokens": [11151, 380, 1223, 746, 293, 291, 8825, 309, 281, 552, 550, 291, 727, 411, 2464, 760, 746, 1566, 411, 1954], "temperature": 0.0, "avg_logprob": -0.17406124538845485, "compression_ratio": 1.8206896551724139, "no_speech_prob": 3.966929853049805e-06}, {"id": 117, "seek": 56252, "start": 582.4399999999999, "end": 586.84, "text": " There's this concept that some people have trouble understanding. Here's a good way. I think of explaining it", "tokens": [821, 311, 341, 3410, 300, 512, 561, 362, 5253, 3701, 13, 1692, 311, 257, 665, 636, 13, 286, 519, 295, 13468, 309], "temperature": 0.0, "avg_logprob": -0.17406124538845485, "compression_ratio": 1.8206896551724139, "no_speech_prob": 3.966929853049805e-06}, {"id": 118, "seek": 56252, "start": 587.28, "end": 589.88, "text": " There's all kinds of stuff you could you could do", "tokens": [821, 311, 439, 3685, 295, 1507, 291, 727, 291, 727, 360], "temperature": 0.0, "avg_logprob": -0.17406124538845485, "compression_ratio": 1.8206896551724139, "no_speech_prob": 3.966929853049805e-06}, {"id": 119, "seek": 58988, "start": 589.88, "end": 598.0, "text": " Okay, so let's go back to SGD", "tokens": [1033, 11, 370, 718, 311, 352, 646, 281, 34520, 35], "temperature": 0.0, "avg_logprob": -0.3186979480818206, "compression_ratio": 1.3172413793103448, "no_speech_prob": 6.962167844903888e-06}, {"id": 120, "seek": 58988, "start": 599.64, "end": 601.64, "text": " And so", "tokens": [400, 370], "temperature": 0.0, "avg_logprob": -0.3186979480818206, "compression_ratio": 1.3172413793103448, "no_speech_prob": 6.962167844903888e-06}, {"id": 121, "seek": 58988, "start": 603.52, "end": 607.88, "text": " We're going back through this notebook which", "tokens": [492, 434, 516, 646, 807, 341, 21060, 597], "temperature": 0.0, "avg_logprob": -0.3186979480818206, "compression_ratio": 1.3172413793103448, "no_speech_prob": 6.962167844903888e-06}, {"id": 122, "seek": 58988, "start": 609.96, "end": 613.0, "text": " Rachel put together basically taking us through", "tokens": [14246, 829, 1214, 1936, 1940, 505, 807], "temperature": 0.0, "avg_logprob": -0.3186979480818206, "compression_ratio": 1.3172413793103448, "no_speech_prob": 6.962167844903888e-06}, {"id": 123, "seek": 58988, "start": 613.68, "end": 618.4, "text": " Kind of SGD from scratch for the purpose of digit recognition", "tokens": [9242, 295, 34520, 35, 490, 8459, 337, 264, 4334, 295, 14293, 11150], "temperature": 0.0, "avg_logprob": -0.3186979480818206, "compression_ratio": 1.3172413793103448, "no_speech_prob": 6.962167844903888e-06}, {"id": 124, "seek": 61840, "start": 618.4, "end": 621.64, "text": " and actually quite a lot of the stuff we look at today is", "tokens": [293, 767, 1596, 257, 688, 295, 264, 1507, 321, 574, 412, 965, 307], "temperature": 0.0, "avg_logprob": -0.1934117686991789, "compression_ratio": 1.5418326693227091, "no_speech_prob": 5.255295491224388e-06}, {"id": 125, "seek": 61840, "start": 622.36, "end": 624.36, "text": " going to be", "tokens": [516, 281, 312], "temperature": 0.0, "avg_logprob": -0.1934117686991789, "compression_ratio": 1.5418326693227091, "no_speech_prob": 5.255295491224388e-06}, {"id": 126, "seek": 61840, "start": 624.4399999999999, "end": 628.8, "text": " closely following a part of the computational linear algebra course", "tokens": [8185, 3480, 257, 644, 295, 264, 28270, 8213, 21989, 1164], "temperature": 0.0, "avg_logprob": -0.1934117686991789, "compression_ratio": 1.5418326693227091, "no_speech_prob": 5.255295491224388e-06}, {"id": 127, "seek": 61840, "start": 629.4399999999999, "end": 635.6, "text": " Which you can both find the MOOCs on fast AI or at USF. It'll be an elective next year", "tokens": [3013, 291, 393, 1293, 915, 264, 49197, 33290, 322, 2370, 7318, 420, 412, 2546, 37, 13, 467, 603, 312, 364, 2185, 488, 958, 1064], "temperature": 0.0, "avg_logprob": -0.1934117686991789, "compression_ratio": 1.5418326693227091, "no_speech_prob": 5.255295491224388e-06}, {"id": 128, "seek": 61840, "start": 635.6, "end": 637.6, "text": " All right, so if you find some of this", "tokens": [1057, 558, 11, 370, 498, 291, 915, 512, 295, 341], "temperature": 0.0, "avg_logprob": -0.1934117686991789, "compression_ratio": 1.5418326693227091, "no_speech_prob": 5.255295491224388e-06}, {"id": 129, "seek": 61840, "start": 638.6, "end": 646.0, "text": " This stuff interesting, and I hope you do then please consider signing up for the elective or checking out the video online", "tokens": [639, 1507, 1880, 11, 293, 286, 1454, 291, 360, 550, 1767, 1949, 13393, 493, 337, 264, 2185, 488, 420, 8568, 484, 264, 960, 2950], "temperature": 0.0, "avg_logprob": -0.1934117686991789, "compression_ratio": 1.5418326693227091, "no_speech_prob": 5.255295491224388e-06}, {"id": 130, "seek": 64600, "start": 646.0, "end": 648.0, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.24249034769394817, "compression_ratio": 1.4293193717277486, "no_speech_prob": 2.812980483213323e-06}, {"id": 131, "seek": 64600, "start": 650.24, "end": 653.72, "text": " We're building neural networks", "tokens": [492, 434, 2390, 18161, 9590], "temperature": 0.0, "avg_logprob": -0.24249034769394817, "compression_ratio": 1.4293193717277486, "no_speech_prob": 2.812980483213323e-06}, {"id": 132, "seek": 64600, "start": 656.36, "end": 657.64, "text": " And", "tokens": [400], "temperature": 0.0, "avg_logprob": -0.24249034769394817, "compression_ratio": 1.4293193717277486, "no_speech_prob": 2.812980483213323e-06}, {"id": 133, "seek": 64600, "start": 657.64, "end": 661.54, "text": " We're starting with an assumption that we've downloaded the MNIST data", "tokens": [492, 434, 2891, 365, 364, 15302, 300, 321, 600, 21748, 264, 376, 45, 19756, 1412], "temperature": 0.0, "avg_logprob": -0.24249034769394817, "compression_ratio": 1.4293193717277486, "no_speech_prob": 2.812980483213323e-06}, {"id": 134, "seek": 64600, "start": 661.84, "end": 667.76, "text": " We've normalized it by subtracting the mean and divided by the standard deviation. Okay, so the data is", "tokens": [492, 600, 48704, 309, 538, 16390, 278, 264, 914, 293, 6666, 538, 264, 3832, 25163, 13, 1033, 11, 370, 264, 1412, 307], "temperature": 0.0, "avg_logprob": -0.24249034769394817, "compression_ratio": 1.4293193717277486, "no_speech_prob": 2.812980483213323e-06}, {"id": 135, "seek": 64600, "start": 669.04, "end": 672.8, "text": " It's slightly unusual in that although they represent images", "tokens": [467, 311, 4748, 10901, 294, 300, 4878, 436, 2906, 5267], "temperature": 0.0, "avg_logprob": -0.24249034769394817, "compression_ratio": 1.4293193717277486, "no_speech_prob": 2.812980483213323e-06}, {"id": 136, "seek": 67280, "start": 672.8, "end": 676.04, "text": " They were they were downloaded as each image was a", "tokens": [814, 645, 436, 645, 21748, 382, 1184, 3256, 390, 257], "temperature": 0.0, "avg_logprob": -0.20764907009630318, "compression_ratio": 1.5303867403314917, "no_speech_prob": 1.6028009213187033e-06}, {"id": 137, "seek": 67280, "start": 676.7199999999999, "end": 678.7199999999999, "text": " 784 long", "tokens": [1614, 25494, 938], "temperature": 0.0, "avg_logprob": -0.20764907009630318, "compression_ratio": 1.5303867403314917, "no_speech_prob": 1.6028009213187033e-06}, {"id": 138, "seek": 67280, "start": 678.8399999999999, "end": 686.52, "text": " Rank one tensor so it's been flattened out okay, and so for the purpose of drawing pictures of it we had to", "tokens": [35921, 472, 40863, 370, 309, 311, 668, 24183, 292, 484, 1392, 11, 293, 370, 337, 264, 4334, 295, 6316, 5242, 295, 309, 321, 632, 281], "temperature": 0.0, "avg_logprob": -0.20764907009630318, "compression_ratio": 1.5303867403314917, "no_speech_prob": 1.6028009213187033e-06}, {"id": 139, "seek": 67280, "start": 687.3599999999999, "end": 688.76, "text": " resize it", "tokens": [50069, 309], "temperature": 0.0, "avg_logprob": -0.20764907009630318, "compression_ratio": 1.5303867403314917, "no_speech_prob": 1.6028009213187033e-06}, {"id": 140, "seek": 67280, "start": 688.76, "end": 690.76, "text": " to 28 by 28", "tokens": [281, 7562, 538, 7562], "temperature": 0.0, "avg_logprob": -0.20764907009630318, "compression_ratio": 1.5303867403314917, "no_speech_prob": 1.6028009213187033e-06}, {"id": 141, "seek": 67280, "start": 691.3199999999999, "end": 698.16, "text": " But the actual data we've got is not 28 by 28. It says it's it's 784 long flattened out", "tokens": [583, 264, 3539, 1412, 321, 600, 658, 307, 406, 7562, 538, 7562, 13, 467, 1619, 309, 311, 309, 311, 1614, 25494, 938, 24183, 292, 484], "temperature": 0.0, "avg_logprob": -0.20764907009630318, "compression_ratio": 1.5303867403314917, "no_speech_prob": 1.6028009213187033e-06}, {"id": 142, "seek": 69816, "start": 698.16, "end": 700.16, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.23955593938412872, "compression_ratio": 1.6569767441860466, "no_speech_prob": 1.6028026266212692e-06}, {"id": 143, "seek": 69816, "start": 703.4, "end": 708.52, "text": " The basic steps we're going to take here is to start out with training", "tokens": [440, 3875, 4439, 321, 434, 516, 281, 747, 510, 307, 281, 722, 484, 365, 3097], "temperature": 0.0, "avg_logprob": -0.23955593938412872, "compression_ratio": 1.6569767441860466, "no_speech_prob": 1.6028026266212692e-06}, {"id": 144, "seek": 69816, "start": 709.36, "end": 715.1999999999999, "text": " The world's simplest neural network basically a logistic regression right so no hidden layers", "tokens": [440, 1002, 311, 22811, 18161, 3209, 1936, 257, 3565, 3142, 24590, 558, 370, 572, 7633, 7914], "temperature": 0.0, "avg_logprob": -0.23955593938412872, "compression_ratio": 1.6569767441860466, "no_speech_prob": 1.6028026266212692e-06}, {"id": 145, "seek": 69816, "start": 715.1999999999999, "end": 718.1999999999999, "text": " And we're going to train it using a library", "tokens": [400, 321, 434, 516, 281, 3847, 309, 1228, 257, 6405], "temperature": 0.0, "avg_logprob": -0.23955593938412872, "compression_ratio": 1.6569767441860466, "no_speech_prob": 1.6028026266212692e-06}, {"id": 146, "seek": 69816, "start": 718.8, "end": 723.56, "text": " Fast AI and we're going to build the network using a library pipe torch", "tokens": [15968, 7318, 293, 321, 434, 516, 281, 1322, 264, 3209, 1228, 257, 6405, 11240, 27822], "temperature": 0.0, "avg_logprob": -0.23955593938412872, "compression_ratio": 1.6569767441860466, "no_speech_prob": 1.6028026266212692e-06}, {"id": 147, "seek": 72356, "start": 723.56, "end": 729.64, "text": " All right, and then we're going to gradually get rid of all the libraries right so first of all we'll get rid of", "tokens": [1057, 558, 11, 293, 550, 321, 434, 516, 281, 13145, 483, 3973, 295, 439, 264, 15148, 558, 370, 700, 295, 439, 321, 603, 483, 3973, 295], "temperature": 0.0, "avg_logprob": -0.17861378070005438, "compression_ratio": 1.9754901960784315, "no_speech_prob": 8.851532697917719e-07}, {"id": 148, "seek": 72356, "start": 730.3599999999999, "end": 734.1199999999999, "text": " The NN neural net library and pipe torch and write that ourselves", "tokens": [440, 426, 45, 18161, 2533, 6405, 293, 11240, 27822, 293, 2464, 300, 4175], "temperature": 0.0, "avg_logprob": -0.17861378070005438, "compression_ratio": 1.9754901960784315, "no_speech_prob": 8.851532697917719e-07}, {"id": 149, "seek": 72356, "start": 735.9599999999999, "end": 740.1199999999999, "text": " Then we'll get rid of the fast AI fit function and write that ourselves", "tokens": [1396, 321, 603, 483, 3973, 295, 264, 2370, 7318, 3318, 2445, 293, 2464, 300, 4175], "temperature": 0.0, "avg_logprob": -0.17861378070005438, "compression_ratio": 1.9754901960784315, "no_speech_prob": 8.851532697917719e-07}, {"id": 150, "seek": 72356, "start": 740.1199999999999, "end": 746.16, "text": " And then we'll get rid of the pie torch optimizer and write that ourselves and so by the end of", "tokens": [400, 550, 321, 603, 483, 3973, 295, 264, 1730, 27822, 5028, 6545, 293, 2464, 300, 4175, 293, 370, 538, 264, 917, 295], "temperature": 0.0, "avg_logprob": -0.17861378070005438, "compression_ratio": 1.9754901960784315, "no_speech_prob": 8.851532697917719e-07}, {"id": 151, "seek": 72356, "start": 747.92, "end": 750.56, "text": " This notebook will have written all the pieces ourselves", "tokens": [639, 21060, 486, 362, 3720, 439, 264, 3755, 4175], "temperature": 0.0, "avg_logprob": -0.17861378070005438, "compression_ratio": 1.9754901960784315, "no_speech_prob": 8.851532697917719e-07}, {"id": 152, "seek": 75056, "start": 750.56, "end": 756.2399999999999, "text": " The only thing that we'll end up relying on is the two key things that pie torch gives us", "tokens": [440, 787, 551, 300, 321, 603, 917, 493, 24140, 322, 307, 264, 732, 2141, 721, 300, 1730, 27822, 2709, 505], "temperature": 0.0, "avg_logprob": -0.14509735107421876, "compression_ratio": 1.8353909465020577, "no_speech_prob": 4.5921234459456173e-07}, {"id": 153, "seek": 75056, "start": 756.2399999999999, "end": 760.3599999999999, "text": " which is a the ability to write Python code and have it run on the GPU and", "tokens": [597, 307, 257, 264, 3485, 281, 2464, 15329, 3089, 293, 362, 309, 1190, 322, 264, 18407, 293], "temperature": 0.0, "avg_logprob": -0.14509735107421876, "compression_ratio": 1.8353909465020577, "no_speech_prob": 4.5921234459456173e-07}, {"id": 154, "seek": 75056, "start": 761.04, "end": 767.76, "text": " Be the ability to write Python code and have it automatically differentiated for us, okay?", "tokens": [879, 264, 3485, 281, 2464, 15329, 3089, 293, 362, 309, 6772, 27372, 770, 337, 505, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.14509735107421876, "compression_ratio": 1.8353909465020577, "no_speech_prob": 4.5921234459456173e-07}, {"id": 155, "seek": 75056, "start": 767.76, "end": 772.28, "text": " So they're the two things we're not going to attempt to write ourselves because it's boring and pointless", "tokens": [407, 436, 434, 264, 732, 721, 321, 434, 406, 516, 281, 5217, 281, 2464, 4175, 570, 309, 311, 9989, 293, 32824], "temperature": 0.0, "avg_logprob": -0.14509735107421876, "compression_ratio": 1.8353909465020577, "no_speech_prob": 4.5921234459456173e-07}, {"id": 156, "seek": 75056, "start": 772.28, "end": 777.64, "text": " But everything else will try and write ourselves on top of those two things okay, so", "tokens": [583, 1203, 1646, 486, 853, 293, 2464, 4175, 322, 1192, 295, 729, 732, 721, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.14509735107421876, "compression_ratio": 1.8353909465020577, "no_speech_prob": 4.5921234459456173e-07}, {"id": 157, "seek": 77764, "start": 777.64, "end": 779.3199999999999, "text": " our", "tokens": [527], "temperature": 0.0, "avg_logprob": -0.19660691420237222, "compression_ratio": 1.6356275303643724, "no_speech_prob": 4.381834060041001e-07}, {"id": 158, "seek": 77764, "start": 779.3199999999999, "end": 783.68, "text": " Starting point is like not doing anything ourselves", "tokens": [16217, 935, 307, 411, 406, 884, 1340, 4175], "temperature": 0.0, "avg_logprob": -0.19660691420237222, "compression_ratio": 1.6356275303643724, "no_speech_prob": 4.381834060041001e-07}, {"id": 159, "seek": 77764, "start": 783.68, "end": 787.16, "text": " It's basically having it all done for us and so pie torch has an NN", "tokens": [467, 311, 1936, 1419, 309, 439, 1096, 337, 505, 293, 370, 1730, 27822, 575, 364, 426, 45], "temperature": 0.0, "avg_logprob": -0.19660691420237222, "compression_ratio": 1.6356275303643724, "no_speech_prob": 4.381834060041001e-07}, {"id": 160, "seek": 77764, "start": 787.56, "end": 792.8, "text": " Library which is where the neural net stuff lives you can create a multi layer", "tokens": [12806, 597, 307, 689, 264, 18161, 2533, 1507, 2909, 291, 393, 1884, 257, 4825, 4583], "temperature": 0.0, "avg_logprob": -0.19660691420237222, "compression_ratio": 1.6356275303643724, "no_speech_prob": 4.381834060041001e-07}, {"id": 161, "seek": 77764, "start": 793.04, "end": 800.26, "text": " Neural network by using the sequential function and then passing in a list of the layers that you want and we asked for a linear layer", "tokens": [1734, 1807, 3209, 538, 1228, 264, 42881, 2445, 293, 550, 8437, 294, 257, 1329, 295, 264, 7914, 300, 291, 528, 293, 321, 2351, 337, 257, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.19660691420237222, "compression_ratio": 1.6356275303643724, "no_speech_prob": 4.381834060041001e-07}, {"id": 162, "seek": 77764, "start": 800.92, "end": 805.68, "text": " Followed by a softmax layer and that defines a logistic regression", "tokens": [9876, 292, 538, 257, 2787, 41167, 4583, 293, 300, 23122, 257, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.19660691420237222, "compression_ratio": 1.6356275303643724, "no_speech_prob": 4.381834060041001e-07}, {"id": 163, "seek": 80568, "start": 805.68, "end": 811.2399999999999, "text": " Okay, the input to our linear layer is 28 by 28 as we just discussed", "tokens": [1033, 11, 264, 4846, 281, 527, 8213, 4583, 307, 7562, 538, 7562, 382, 321, 445, 7152], "temperature": 0.0, "avg_logprob": -0.22976159267738216, "compression_ratio": 1.376543209876543, "no_speech_prob": 1.628047584745218e-06}, {"id": 164, "seek": 80568, "start": 811.4799999999999, "end": 819.18, "text": " The output is 10 because we want a probability for each of the numbers not through no for each of our images, okay?", "tokens": [440, 5598, 307, 1266, 570, 321, 528, 257, 8482, 337, 1184, 295, 264, 3547, 406, 807, 572, 337, 1184, 295, 527, 5267, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.22976159267738216, "compression_ratio": 1.376543209876543, "no_speech_prob": 1.628047584745218e-06}, {"id": 165, "seek": 80568, "start": 822.0, "end": 826.4399999999999, "text": " Cuda sticks it on the GPU and then", "tokens": [383, 11152, 12518, 309, 322, 264, 18407, 293, 550], "temperature": 0.0, "avg_logprob": -0.22976159267738216, "compression_ratio": 1.376543209876543, "no_speech_prob": 1.628047584745218e-06}, {"id": 166, "seek": 80568, "start": 828.1999999999999, "end": 830.1999999999999, "text": " Fit", "tokens": [29263], "temperature": 0.0, "avg_logprob": -0.22976159267738216, "compression_ratio": 1.376543209876543, "no_speech_prob": 1.628047584745218e-06}, {"id": 167, "seek": 83020, "start": 830.2, "end": 837.36, "text": " Fits a model okay, so we start out with a random set of weights and then fit uses gradient descent to make it better", "tokens": [479, 1208, 257, 2316, 1392, 11, 370, 321, 722, 484, 365, 257, 4974, 992, 295, 17443, 293, 550, 3318, 4960, 16235, 23475, 281, 652, 309, 1101], "temperature": 0.0, "avg_logprob": -0.18974504193055977, "compression_ratio": 1.6979591836734693, "no_speech_prob": 2.4439832486677915e-06}, {"id": 168, "seek": 83020, "start": 838.84, "end": 841.0, "text": " We had to tell the fit function", "tokens": [492, 632, 281, 980, 264, 3318, 2445], "temperature": 0.0, "avg_logprob": -0.18974504193055977, "compression_ratio": 1.6979591836734693, "no_speech_prob": 2.4439832486677915e-06}, {"id": 169, "seek": 83020, "start": 841.6800000000001, "end": 847.7800000000001, "text": " What criterion to use in other words what counts as better and we told it to use negative log likelihood?", "tokens": [708, 46691, 281, 764, 294, 661, 2283, 437, 14893, 382, 1101, 293, 321, 1907, 309, 281, 764, 3671, 3565, 22119, 30], "temperature": 0.0, "avg_logprob": -0.18974504193055977, "compression_ratio": 1.6979591836734693, "no_speech_prob": 2.4439832486677915e-06}, {"id": 170, "seek": 83020, "start": 847.7800000000001, "end": 850.6800000000001, "text": " We'll learn about that in the next lesson what that is exactly", "tokens": [492, 603, 1466, 466, 300, 294, 264, 958, 6898, 437, 300, 307, 2293], "temperature": 0.0, "avg_logprob": -0.18974504193055977, "compression_ratio": 1.6979591836734693, "no_speech_prob": 2.4439832486677915e-06}, {"id": 171, "seek": 83020, "start": 852.0400000000001, "end": 858.0400000000001, "text": " We had to tell it what optimizer to use and we said please use opt-in dot Adam the details of that", "tokens": [492, 632, 281, 980, 309, 437, 5028, 6545, 281, 764, 293, 321, 848, 1767, 764, 2427, 12, 259, 5893, 7938, 264, 4365, 295, 300], "temperature": 0.0, "avg_logprob": -0.18974504193055977, "compression_ratio": 1.6979591836734693, "no_speech_prob": 2.4439832486677915e-06}, {"id": 172, "seek": 85804, "start": 858.04, "end": 863.2199999999999, "text": " We won't cover in this course. We're going to use something build something simpler called SGD", "tokens": [492, 1582, 380, 2060, 294, 341, 1164, 13, 492, 434, 516, 281, 764, 746, 1322, 746, 18587, 1219, 34520, 35], "temperature": 0.0, "avg_logprob": -0.19968605041503906, "compression_ratio": 1.6170212765957446, "no_speech_prob": 2.6425693704368314e-06}, {"id": 173, "seek": 85804, "start": 863.7199999999999, "end": 866.5999999999999, "text": " If you're interested in Adam, we just covered that in the deep learning course", "tokens": [759, 291, 434, 3102, 294, 7938, 11, 321, 445, 5343, 300, 294, 264, 2452, 2539, 1164], "temperature": 0.0, "avg_logprob": -0.19968605041503906, "compression_ratio": 1.6170212765957446, "no_speech_prob": 2.6425693704368314e-06}, {"id": 174, "seek": 85804, "start": 867.8, "end": 872.76, "text": " And what metrics do you want to print out we decided to print out accuracy okay, so", "tokens": [400, 437, 16367, 360, 291, 528, 281, 4482, 484, 321, 3047, 281, 4482, 484, 14170, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.19968605041503906, "compression_ratio": 1.6170212765957446, "no_speech_prob": 2.6425693704368314e-06}, {"id": 175, "seek": 85804, "start": 873.52, "end": 875.9599999999999, "text": " That was that and so if we do that", "tokens": [663, 390, 300, 293, 370, 498, 321, 360, 300], "temperature": 0.0, "avg_logprob": -0.19968605041503906, "compression_ratio": 1.6170212765957446, "no_speech_prob": 2.6425693704368314e-06}, {"id": 176, "seek": 87596, "start": 875.96, "end": 886.1600000000001, "text": " Okay, so after we fit it we get an accuracy of generally somewhere around 91", "tokens": [1033, 11, 370, 934, 321, 3318, 309, 321, 483, 364, 14170, 295, 5101, 4079, 926, 31064], "temperature": 0.0, "avg_logprob": -0.2194516999380929, "compression_ratio": 1.6534653465346534, "no_speech_prob": 1.067696871359658e-06}, {"id": 177, "seek": 87596, "start": 886.6800000000001, "end": 891.0, "text": " 92 percent so what we're going to do from here is we're going to gradually", "tokens": [28225, 3043, 370, 437, 321, 434, 516, 281, 360, 490, 510, 307, 321, 434, 516, 281, 13145], "temperature": 0.0, "avg_logprob": -0.2194516999380929, "compression_ratio": 1.6534653465346534, "no_speech_prob": 1.067696871359658e-06}, {"id": 178, "seek": 87596, "start": 891.6, "end": 895.2, "text": " We're going to repeat this exact same thing so we're going to rebuild", "tokens": [492, 434, 516, 281, 7149, 341, 1900, 912, 551, 370, 321, 434, 516, 281, 16877], "temperature": 0.0, "avg_logprob": -0.2194516999380929, "compression_ratio": 1.6534653465346534, "no_speech_prob": 1.067696871359658e-06}, {"id": 179, "seek": 87596, "start": 896.36, "end": 899.6, "text": " This model you know four or five times", "tokens": [639, 2316, 291, 458, 1451, 420, 1732, 1413], "temperature": 0.0, "avg_logprob": -0.2194516999380929, "compression_ratio": 1.6534653465346534, "no_speech_prob": 1.067696871359658e-06}, {"id": 180, "seek": 87596, "start": 900.2800000000001, "end": 904.0400000000001, "text": " Fitting it building it and fitting it with less and less libraries, okay?", "tokens": [479, 2414, 309, 2390, 309, 293, 15669, 309, 365, 1570, 293, 1570, 15148, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.2194516999380929, "compression_ratio": 1.6534653465346534, "no_speech_prob": 1.067696871359658e-06}, {"id": 181, "seek": 90404, "start": 904.04, "end": 906.3199999999999, "text": " So the second thing that we did", "tokens": [407, 264, 1150, 551, 300, 321, 630], "temperature": 0.0, "avg_logprob": -0.2282245249687871, "compression_ratio": 1.5396825396825398, "no_speech_prob": 2.5612614535930334e-06}, {"id": 182, "seek": 90404, "start": 907.04, "end": 909.04, "text": " last time", "tokens": [1036, 565], "temperature": 0.0, "avg_logprob": -0.2282245249687871, "compression_ratio": 1.5396825396825398, "no_speech_prob": 2.5612614535930334e-06}, {"id": 183, "seek": 90404, "start": 909.24, "end": 911.8, "text": " was to try to start to define the", "tokens": [390, 281, 853, 281, 722, 281, 6964, 264], "temperature": 0.0, "avg_logprob": -0.2282245249687871, "compression_ratio": 1.5396825396825398, "no_speech_prob": 2.5612614535930334e-06}, {"id": 184, "seek": 90404, "start": 913.5999999999999, "end": 921.04, "text": " The module ourselves right so instead of saying the network is a sequential bunch of these layers", "tokens": [440, 10088, 4175, 558, 370, 2602, 295, 1566, 264, 3209, 307, 257, 42881, 3840, 295, 613, 7914], "temperature": 0.0, "avg_logprob": -0.2282245249687871, "compression_ratio": 1.5396825396825398, "no_speech_prob": 2.5612614535930334e-06}, {"id": 185, "seek": 90404, "start": 921.76, "end": 927.4, "text": " Let's not use that library at all and try and define it ourselves from scratch, okay?", "tokens": [961, 311, 406, 764, 300, 6405, 412, 439, 293, 853, 293, 6964, 309, 4175, 490, 8459, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.2282245249687871, "compression_ratio": 1.5396825396825398, "no_speech_prob": 2.5612614535930334e-06}, {"id": 186, "seek": 90404, "start": 928.1999999999999, "end": 930.1999999999999, "text": " So to do that we have to use", "tokens": [407, 281, 360, 300, 321, 362, 281, 764], "temperature": 0.0, "avg_logprob": -0.2282245249687871, "compression_ratio": 1.5396825396825398, "no_speech_prob": 2.5612614535930334e-06}, {"id": 187, "seek": 90404, "start": 930.92, "end": 932.16, "text": " oO", "tokens": [277, 46], "temperature": 0.0, "avg_logprob": -0.2282245249687871, "compression_ratio": 1.5396825396825398, "no_speech_prob": 2.5612614535930334e-06}, {"id": 188, "seek": 93216, "start": 932.16, "end": 937.04, "text": " Because that's how we build everything in pytorch, and we have to create a", "tokens": [1436, 300, 311, 577, 321, 1322, 1203, 294, 25878, 284, 339, 11, 293, 321, 362, 281, 1884, 257], "temperature": 0.0, "avg_logprob": -0.23151111602783203, "compression_ratio": 1.6414141414141414, "no_speech_prob": 1.653682488722552e-06}, {"id": 189, "seek": 93216, "start": 937.92, "end": 939.04, "text": " class", "tokens": [1508], "temperature": 0.0, "avg_logprob": -0.23151111602783203, "compression_ratio": 1.6414141414141414, "no_speech_prob": 1.653682488722552e-06}, {"id": 190, "seek": 93216, "start": 939.04, "end": 941.04, "text": " which inherits from", "tokens": [597, 9484, 1208, 490], "temperature": 0.0, "avg_logprob": -0.23151111602783203, "compression_ratio": 1.6414141414141414, "no_speech_prob": 1.653682488722552e-06}, {"id": 191, "seek": 93216, "start": 941.4399999999999, "end": 946.48, "text": " NN dot module so NN dot module is a pytorch class that takes", "tokens": [426, 45, 5893, 10088, 370, 426, 45, 5893, 10088, 307, 257, 25878, 284, 339, 1508, 300, 2516], "temperature": 0.0, "avg_logprob": -0.23151111602783203, "compression_ratio": 1.6414141414141414, "no_speech_prob": 1.653682488722552e-06}, {"id": 192, "seek": 93216, "start": 946.9599999999999, "end": 950.7199999999999, "text": " Our class and turns it into a neural network module", "tokens": [2621, 1508, 293, 4523, 309, 666, 257, 18161, 3209, 10088], "temperature": 0.0, "avg_logprob": -0.23151111602783203, "compression_ratio": 1.6414141414141414, "no_speech_prob": 1.653682488722552e-06}, {"id": 193, "seek": 93216, "start": 951.7199999999999, "end": 957.36, "text": " Which basically means will anything that you inherit from an end up module like this you can pretty much insert", "tokens": [3013, 1936, 1355, 486, 1340, 300, 291, 21389, 490, 364, 917, 493, 10088, 411, 341, 291, 393, 1238, 709, 8969], "temperature": 0.0, "avg_logprob": -0.23151111602783203, "compression_ratio": 1.6414141414141414, "no_speech_prob": 1.653682488722552e-06}, {"id": 194, "seek": 95736, "start": 957.36, "end": 962.04, "text": " Into a neural network as a layer or you can treat it as a neural network", "tokens": [23373, 257, 18161, 3209, 382, 257, 4583, 420, 291, 393, 2387, 309, 382, 257, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.20309496449900197, "compression_ratio": 1.7327188940092166, "no_speech_prob": 8.579218047088943e-07}, {"id": 195, "seek": 95736, "start": 962.4, "end": 968.38, "text": " It's going to get all the stuff that it needs automatically to to work as a part of or a full", "tokens": [467, 311, 516, 281, 483, 439, 264, 1507, 300, 309, 2203, 6772, 281, 281, 589, 382, 257, 644, 295, 420, 257, 1577], "temperature": 0.0, "avg_logprob": -0.20309496449900197, "compression_ratio": 1.7327188940092166, "no_speech_prob": 8.579218047088943e-07}, {"id": 196, "seek": 95736, "start": 968.76, "end": 973.82, "text": " Neural network, and we'll talk about exactly what that means today in the next lesson right", "tokens": [1734, 1807, 3209, 11, 293, 321, 603, 751, 466, 2293, 437, 300, 1355, 965, 294, 264, 958, 6898, 558], "temperature": 0.0, "avg_logprob": -0.20309496449900197, "compression_ratio": 1.7327188940092166, "no_speech_prob": 8.579218047088943e-07}, {"id": 197, "seek": 95736, "start": 975.96, "end": 977.16, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.20309496449900197, "compression_ratio": 1.7327188940092166, "no_speech_prob": 8.579218047088943e-07}, {"id": 198, "seek": 95736, "start": 977.16, "end": 984.6, "text": " We need to construct the object so that means we need to define the constructor thunder in it and then importantly", "tokens": [492, 643, 281, 7690, 264, 2657, 370, 300, 1355, 321, 643, 281, 6964, 264, 47479, 19898, 294, 309, 293, 550, 8906], "temperature": 0.0, "avg_logprob": -0.20309496449900197, "compression_ratio": 1.7327188940092166, "no_speech_prob": 8.579218047088943e-07}, {"id": 199, "seek": 98460, "start": 984.6, "end": 989.76, "text": " This is a python thing is if you inherit from some other object", "tokens": [639, 307, 257, 38797, 551, 307, 498, 291, 21389, 490, 512, 661, 2657], "temperature": 0.0, "avg_logprob": -0.2518304678109976, "compression_ratio": 1.7596153846153846, "no_speech_prob": 5.20355911248771e-07}, {"id": 200, "seek": 98460, "start": 989.88, "end": 995.36, "text": " Then you have to create the thing you inherit from first so when you say super dot", "tokens": [1396, 291, 362, 281, 1884, 264, 551, 291, 21389, 490, 700, 370, 562, 291, 584, 1687, 5893], "temperature": 0.0, "avg_logprob": -0.2518304678109976, "compression_ratio": 1.7596153846153846, "no_speech_prob": 5.20355911248771e-07}, {"id": 201, "seek": 98460, "start": 995.36, "end": 998.02, "text": " Thunder in it that says construct the", "tokens": [21023, 294, 309, 300, 1619, 7690, 264], "temperature": 0.0, "avg_logprob": -0.2518304678109976, "compression_ratio": 1.7596153846153846, "no_speech_prob": 5.20355911248771e-07}, {"id": 202, "seek": 98460, "start": 998.52, "end": 1003.76, "text": " NN dot module piece of that first right if you don't do that then", "tokens": [426, 45, 5893, 10088, 2522, 295, 300, 700, 558, 498, 291, 500, 380, 360, 300, 550], "temperature": 0.0, "avg_logprob": -0.2518304678109976, "compression_ratio": 1.7596153846153846, "no_speech_prob": 5.20355911248771e-07}, {"id": 203, "seek": 98460, "start": 1004.2, "end": 1010.88, "text": " The NN dot module stuff never gets a chance to actually get constructed now, so this is just like a standard", "tokens": [440, 426, 45, 5893, 10088, 1507, 1128, 2170, 257, 2931, 281, 767, 483, 17083, 586, 11, 370, 341, 307, 445, 411, 257, 3832], "temperature": 0.0, "avg_logprob": -0.2518304678109976, "compression_ratio": 1.7596153846153846, "no_speech_prob": 5.20355911248771e-07}, {"id": 204, "seek": 98460, "start": 1011.5600000000001, "end": 1012.9200000000001, "text": " python", "tokens": [38797], "temperature": 0.0, "avg_logprob": -0.2518304678109976, "compression_ratio": 1.7596153846153846, "no_speech_prob": 5.20355911248771e-07}, {"id": 205, "seek": 101292, "start": 1012.92, "end": 1018.4399999999999, "text": " OO subclass constructor, okay, and if any of that's unclear to you", "tokens": [422, 46, 1422, 11665, 47479, 11, 1392, 11, 293, 498, 604, 295, 300, 311, 25636, 281, 291], "temperature": 0.0, "avg_logprob": -0.22107226508004324, "compression_ratio": 1.6169154228855722, "no_speech_prob": 3.0894811970938463e-06}, {"id": 206, "seek": 101292, "start": 1018.4399999999999, "end": 1021.3199999999999, "text": " then you know this is where you definitely want to just grab a", "tokens": [550, 291, 458, 341, 307, 689, 291, 2138, 528, 281, 445, 4444, 257], "temperature": 0.0, "avg_logprob": -0.22107226508004324, "compression_ratio": 1.6169154228855722, "no_speech_prob": 3.0894811970938463e-06}, {"id": 207, "seek": 101292, "start": 1021.9599999999999, "end": 1028.78, "text": " Python intro to OO because this is the standard approach right so inside our constructor", "tokens": [15329, 12897, 281, 422, 46, 570, 341, 307, 264, 3832, 3109, 558, 370, 1854, 527, 47479], "temperature": 0.0, "avg_logprob": -0.22107226508004324, "compression_ratio": 1.6169154228855722, "no_speech_prob": 3.0894811970938463e-06}, {"id": 208, "seek": 101292, "start": 1029.6, "end": 1031.6, "text": " We want to do the equivalent of", "tokens": [492, 528, 281, 360, 264, 10344, 295], "temperature": 0.0, "avg_logprob": -0.22107226508004324, "compression_ratio": 1.6169154228855722, "no_speech_prob": 3.0894811970938463e-06}, {"id": 209, "seek": 103160, "start": 1031.6, "end": 1042.1599999999999, "text": " An N dot linear all right, so what an N dot linear is doing is it's taking our?", "tokens": [1107, 426, 5893, 8213, 439, 558, 11, 370, 437, 364, 426, 5893, 8213, 307, 884, 307, 309, 311, 1940, 527, 30], "temperature": 0.0, "avg_logprob": -0.23470301023671325, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.1189387123522465e-06}, {"id": 210, "seek": 103160, "start": 1042.7199999999998, "end": 1045.36, "text": " It's taking our 28 by 28", "tokens": [467, 311, 1940, 527, 7562, 538, 7562], "temperature": 0.0, "avg_logprob": -0.23470301023671325, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.1189387123522465e-06}, {"id": 211, "seek": 103160, "start": 1049.3999999999999, "end": 1051.3999999999999, "text": " Vector so 768 long", "tokens": [691, 20814, 370, 24733, 23, 938], "temperature": 0.0, "avg_logprob": -0.23470301023671325, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.1189387123522465e-06}, {"id": 212, "seek": 103160, "start": 1052.0, "end": 1058.7199999999998, "text": " Vector and we're going to be that's going to be the input to a matrix multiplication, so we now need to create a", "tokens": [691, 20814, 293, 321, 434, 516, 281, 312, 300, 311, 516, 281, 312, 264, 4846, 281, 257, 8141, 27290, 11, 370, 321, 586, 643, 281, 1884, 257], "temperature": 0.0, "avg_logprob": -0.23470301023671325, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.1189387123522465e-06}, {"id": 213, "seek": 105872, "start": 1058.72, "end": 1060.72, "text": " something with", "tokens": [746, 365], "temperature": 0.0, "avg_logprob": -0.29790782928466797, "compression_ratio": 1.263157894736842, "no_speech_prob": 1.9637966488517122e-06}, {"id": 214, "seek": 105872, "start": 1063.4, "end": 1065.88, "text": " 768 rows and", "tokens": [24733, 23, 13241, 293], "temperature": 0.0, "avg_logprob": -0.29790782928466797, "compression_ratio": 1.263157894736842, "no_speech_prob": 1.9637966488517122e-06}, {"id": 215, "seek": 105872, "start": 1067.52, "end": 1069.68, "text": " That's 768 and 10 Collins", "tokens": [663, 311, 24733, 23, 293, 1266, 27973], "temperature": 0.0, "avg_logprob": -0.29790782928466797, "compression_ratio": 1.263157894736842, "no_speech_prob": 1.9637966488517122e-06}, {"id": 216, "seek": 105872, "start": 1070.8, "end": 1076.56, "text": " Okay, so because the input to this is going to be a mini batch", "tokens": [1033, 11, 370, 570, 264, 4846, 281, 341, 307, 516, 281, 312, 257, 8382, 15245], "temperature": 0.0, "avg_logprob": -0.29790782928466797, "compression_ratio": 1.263157894736842, "no_speech_prob": 1.9637966488517122e-06}, {"id": 217, "seek": 105872, "start": 1077.3600000000001, "end": 1079.3600000000001, "text": " of size", "tokens": [295, 2744], "temperature": 0.0, "avg_logprob": -0.29790782928466797, "compression_ratio": 1.263157894736842, "no_speech_prob": 1.9637966488517122e-06}, {"id": 218, "seek": 105872, "start": 1080.08, "end": 1082.08, "text": " Actually, let's move this into a new window", "tokens": [5135, 11, 718, 311, 1286, 341, 666, 257, 777, 4910], "temperature": 0.0, "avg_logprob": -0.29790782928466797, "compression_ratio": 1.263157894736842, "no_speech_prob": 1.9637966488517122e-06}, {"id": 219, "seek": 108208, "start": 1082.08, "end": 1093.26, "text": " 768 by 10 and the input to this is going to be a mini batch of size", "tokens": [24733, 23, 538, 1266, 293, 264, 4846, 281, 341, 307, 516, 281, 312, 257, 8382, 15245, 295, 2744], "temperature": 0.0, "avg_logprob": -0.29796791076660156, "compression_ratio": 1.3285714285714285, "no_speech_prob": 8.446191941402503e-07}, {"id": 220, "seek": 108208, "start": 1094.6799999999998, "end": 1096.6799999999998, "text": " 64 by", "tokens": [12145, 538], "temperature": 0.0, "avg_logprob": -0.29796791076660156, "compression_ratio": 1.3285714285714285, "no_speech_prob": 8.446191941402503e-07}, {"id": 221, "seek": 108208, "start": 1099.08, "end": 1102.8, "text": " 768 right so we're going to do this matrix product", "tokens": [24733, 23, 558, 370, 321, 434, 516, 281, 360, 341, 8141, 1674], "temperature": 0.0, "avg_logprob": -0.29796791076660156, "compression_ratio": 1.3285714285714285, "no_speech_prob": 8.446191941402503e-07}, {"id": 222, "seek": 108208, "start": 1103.4399999999998, "end": 1108.28, "text": " Okay, so when we say in pie torch NN dot linear", "tokens": [1033, 11, 370, 562, 321, 584, 294, 1730, 27822, 426, 45, 5893, 8213], "temperature": 0.0, "avg_logprob": -0.29796791076660156, "compression_ratio": 1.3285714285714285, "no_speech_prob": 8.446191941402503e-07}, {"id": 223, "seek": 108208, "start": 1109.6799999999998, "end": 1111.6799999999998, "text": " It's going to", "tokens": [467, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.29796791076660156, "compression_ratio": 1.3285714285714285, "no_speech_prob": 8.446191941402503e-07}, {"id": 224, "seek": 111168, "start": 1111.68, "end": 1113.28, "text": " construct", "tokens": [7690], "temperature": 0.0, "avg_logprob": -0.2062955769625577, "compression_ratio": 1.5933014354066986, "no_speech_prob": 5.014712314732606e-06}, {"id": 225, "seek": 111168, "start": 1113.28, "end": 1117.98, "text": " This matrix for us right so since we're not using that we're doing things from scratch", "tokens": [639, 8141, 337, 505, 558, 370, 1670, 321, 434, 406, 1228, 300, 321, 434, 884, 721, 490, 8459], "temperature": 0.0, "avg_logprob": -0.2062955769625577, "compression_ratio": 1.5933014354066986, "no_speech_prob": 5.014712314732606e-06}, {"id": 226, "seek": 111168, "start": 1117.98, "end": 1121.02, "text": " We need to make it ourselves so to make it ourselves we can say", "tokens": [492, 643, 281, 652, 309, 4175, 370, 281, 652, 309, 4175, 321, 393, 584], "temperature": 0.0, "avg_logprob": -0.2062955769625577, "compression_ratio": 1.5933014354066986, "no_speech_prob": 5.014712314732606e-06}, {"id": 227, "seek": 111168, "start": 1122.16, "end": 1124.16, "text": " generate normal random numbers", "tokens": [8460, 2710, 4974, 3547], "temperature": 0.0, "avg_logprob": -0.2062955769625577, "compression_ratio": 1.5933014354066986, "no_speech_prob": 5.014712314732606e-06}, {"id": 228, "seek": 111168, "start": 1125.04, "end": 1126.2, "text": " with", "tokens": [365], "temperature": 0.0, "avg_logprob": -0.2062955769625577, "compression_ratio": 1.5933014354066986, "no_speech_prob": 5.014712314732606e-06}, {"id": 229, "seek": 111168, "start": 1126.2, "end": 1132.0, "text": " This dimensionality which we passed in here 768 by 10 okay, so that gives us our", "tokens": [639, 10139, 1860, 597, 321, 4678, 294, 510, 24733, 23, 538, 1266, 1392, 11, 370, 300, 2709, 505, 527], "temperature": 0.0, "avg_logprob": -0.2062955769625577, "compression_ratio": 1.5933014354066986, "no_speech_prob": 5.014712314732606e-06}, {"id": 230, "seek": 111168, "start": 1133.3600000000001, "end": 1135.3600000000001, "text": " randomly initialized", "tokens": [16979, 5883, 1602], "temperature": 0.0, "avg_logprob": -0.2062955769625577, "compression_ratio": 1.5933014354066986, "no_speech_prob": 5.014712314732606e-06}, {"id": 231, "seek": 111168, "start": 1135.3600000000001, "end": 1137.3600000000001, "text": " matrix okay", "tokens": [8141, 1392], "temperature": 0.0, "avg_logprob": -0.2062955769625577, "compression_ratio": 1.5933014354066986, "no_speech_prob": 5.014712314732606e-06}, {"id": 232, "seek": 111168, "start": 1138.0, "end": 1140.0, "text": " Then we want to add on", "tokens": [1396, 321, 528, 281, 909, 322], "temperature": 0.0, "avg_logprob": -0.2062955769625577, "compression_ratio": 1.5933014354066986, "no_speech_prob": 5.014712314732606e-06}, {"id": 233, "seek": 114000, "start": 1140.0, "end": 1142.0, "text": " to this", "tokens": [281, 341], "temperature": 0.0, "avg_logprob": -0.20748086817124312, "compression_ratio": 1.6402116402116402, "no_speech_prob": 1.1189406450284878e-06}, {"id": 234, "seek": 114000, "start": 1143.6, "end": 1148.2, "text": " You know we don't just want y equals ax we want y equals ax plus b", "tokens": [509, 458, 321, 500, 380, 445, 528, 288, 6915, 6360, 321, 528, 288, 6915, 6360, 1804, 272], "temperature": 0.0, "avg_logprob": -0.20748086817124312, "compression_ratio": 1.6402116402116402, "no_speech_prob": 1.1189406450284878e-06}, {"id": 235, "seek": 114000, "start": 1148.6, "end": 1153.52, "text": " Right so we need to add on what we call in neural nets a bias vector", "tokens": [1779, 370, 321, 643, 281, 909, 322, 437, 321, 818, 294, 18161, 36170, 257, 12577, 8062], "temperature": 0.0, "avg_logprob": -0.20748086817124312, "compression_ratio": 1.6402116402116402, "no_speech_prob": 1.1189406450284878e-06}, {"id": 236, "seek": 114000, "start": 1153.84, "end": 1160.28, "text": " So we create here a bias vector of length 10. Hey again randomly initialized", "tokens": [407, 321, 1884, 510, 257, 12577, 8062, 295, 4641, 1266, 13, 1911, 797, 16979, 5883, 1602], "temperature": 0.0, "avg_logprob": -0.20748086817124312, "compression_ratio": 1.6402116402116402, "no_speech_prob": 1.1189406450284878e-06}, {"id": 237, "seek": 114000, "start": 1160.8, "end": 1164.68, "text": " And so now here are our two randomly initialized", "tokens": [400, 370, 586, 510, 366, 527, 732, 16979, 5883, 1602], "temperature": 0.0, "avg_logprob": -0.20748086817124312, "compression_ratio": 1.6402116402116402, "no_speech_prob": 1.1189406450284878e-06}, {"id": 238, "seek": 114000, "start": 1165.44, "end": 1167.44, "text": " weight tensors", "tokens": [3364, 10688, 830], "temperature": 0.0, "avg_logprob": -0.20748086817124312, "compression_ratio": 1.6402116402116402, "no_speech_prob": 1.1189406450284878e-06}, {"id": 239, "seek": 114000, "start": 1167.48, "end": 1169.48, "text": " So that's our constructor", "tokens": [407, 300, 311, 527, 47479], "temperature": 0.0, "avg_logprob": -0.20748086817124312, "compression_ratio": 1.6402116402116402, "no_speech_prob": 1.1189406450284878e-06}, {"id": 240, "seek": 116948, "start": 1169.48, "end": 1171.0, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.1872259406156318, "compression_ratio": 1.6262135922330097, "no_speech_prob": 1.459373379475437e-06}, {"id": 241, "seek": 116948, "start": 1171.0, "end": 1176.94, "text": " Now we need to find forward why do we need to define forward? This is a pie torch specific thing", "tokens": [823, 321, 643, 281, 915, 2128, 983, 360, 321, 643, 281, 6964, 2128, 30, 639, 307, 257, 1730, 27822, 2685, 551], "temperature": 0.0, "avg_logprob": -0.1872259406156318, "compression_ratio": 1.6262135922330097, "no_speech_prob": 1.459373379475437e-06}, {"id": 242, "seek": 116948, "start": 1177.3600000000001, "end": 1183.76, "text": " What's going to happen is this is when you create a module in pie torch?", "tokens": [708, 311, 516, 281, 1051, 307, 341, 307, 562, 291, 1884, 257, 10088, 294, 1730, 27822, 30], "temperature": 0.0, "avg_logprob": -0.1872259406156318, "compression_ratio": 1.6262135922330097, "no_speech_prob": 1.459373379475437e-06}, {"id": 243, "seek": 116948, "start": 1184.06, "end": 1189.5, "text": " The object that you get back behaves as if it's a function you can call it with parentheses", "tokens": [440, 2657, 300, 291, 483, 646, 36896, 382, 498, 309, 311, 257, 2445, 291, 393, 818, 309, 365, 34153], "temperature": 0.0, "avg_logprob": -0.1872259406156318, "compression_ratio": 1.6262135922330097, "no_speech_prob": 1.459373379475437e-06}, {"id": 244, "seek": 116948, "start": 1189.5, "end": 1193.1200000000001, "text": " Which will do it that in a moment, and so you need to somehow define", "tokens": [3013, 486, 360, 309, 300, 294, 257, 1623, 11, 293, 370, 291, 643, 281, 6063, 6964], "temperature": 0.0, "avg_logprob": -0.1872259406156318, "compression_ratio": 1.6262135922330097, "no_speech_prob": 1.459373379475437e-06}, {"id": 245, "seek": 119312, "start": 1193.12, "end": 1200.4799999999998, "text": " What happens when you call it as if it's a function and the answer is pie torch calls a method called?", "tokens": [708, 2314, 562, 291, 818, 309, 382, 498, 309, 311, 257, 2445, 293, 264, 1867, 307, 1730, 27822, 5498, 257, 3170, 1219, 30], "temperature": 0.0, "avg_logprob": -0.1872584942689876, "compression_ratio": 1.6900826446280992, "no_speech_prob": 1.0188081205342314e-06}, {"id": 246, "seek": 119312, "start": 1201.0, "end": 1207.8, "text": " Forward okay, that's just that's the Python the pie torch kind of approach that they picked right", "tokens": [35524, 1392, 11, 300, 311, 445, 300, 311, 264, 15329, 264, 1730, 27822, 733, 295, 3109, 300, 436, 6183, 558], "temperature": 0.0, "avg_logprob": -0.1872584942689876, "compression_ratio": 1.6900826446280992, "no_speech_prob": 1.0188081205342314e-06}, {"id": 247, "seek": 119312, "start": 1208.1999999999998, "end": 1211.6, "text": " So when it calls forward we need to do our actual", "tokens": [407, 562, 309, 5498, 2128, 321, 643, 281, 360, 527, 3539], "temperature": 0.0, "avg_logprob": -0.1872584942689876, "compression_ratio": 1.6900826446280992, "no_speech_prob": 1.0188081205342314e-06}, {"id": 248, "seek": 119312, "start": 1212.28, "end": 1219.6, "text": " Calculation of the output of this module or later okay, so here is the thing that actually gets calculated in our logistic regression", "tokens": [3511, 2444, 399, 295, 264, 5598, 295, 341, 10088, 420, 1780, 1392, 11, 370, 510, 307, 264, 551, 300, 767, 2170, 15598, 294, 527, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.1872584942689876, "compression_ratio": 1.6900826446280992, "no_speech_prob": 1.0188081205342314e-06}, {"id": 249, "seek": 121960, "start": 1219.6, "end": 1222.08, "text": " So basically we take our", "tokens": [407, 1936, 321, 747, 527], "temperature": 0.0, "avg_logprob": -0.26437764625026755, "compression_ratio": 1.646341463414634, "no_speech_prob": 7.224433602459612e-07}, {"id": 250, "seek": 121960, "start": 1224.0, "end": 1226.0, "text": " Input X", "tokens": [682, 2582, 1783], "temperature": 0.0, "avg_logprob": -0.26437764625026755, "compression_ratio": 1.646341463414634, "no_speech_prob": 7.224433602459612e-07}, {"id": 251, "seek": 121960, "start": 1226.56, "end": 1232.34, "text": " Which gets passed to forward that's basically how forward works it gets past the mini-batch", "tokens": [3013, 2170, 4678, 281, 2128, 300, 311, 1936, 577, 2128, 1985, 309, 2170, 1791, 264, 8382, 12, 65, 852], "temperature": 0.0, "avg_logprob": -0.26437764625026755, "compression_ratio": 1.646341463414634, "no_speech_prob": 7.224433602459612e-07}, {"id": 252, "seek": 121960, "start": 1233.4399999999998, "end": 1235.62, "text": " And we matrix multiply it by", "tokens": [400, 321, 8141, 12972, 309, 538], "temperature": 0.0, "avg_logprob": -0.26437764625026755, "compression_ratio": 1.646341463414634, "no_speech_prob": 7.224433602459612e-07}, {"id": 253, "seek": 121960, "start": 1236.8799999999999, "end": 1241.7199999999998, "text": " The layer one weights which we defined up here, and then we add on", "tokens": [440, 4583, 472, 17443, 597, 321, 7642, 493, 510, 11, 293, 550, 321, 909, 322], "temperature": 0.0, "avg_logprob": -0.26437764625026755, "compression_ratio": 1.646341463414634, "no_speech_prob": 7.224433602459612e-07}, {"id": 254, "seek": 121960, "start": 1242.8799999999999, "end": 1246.52, "text": " The layer one bias which we defined up here, okay", "tokens": [440, 4583, 472, 12577, 597, 321, 7642, 493, 510, 11, 1392], "temperature": 0.0, "avg_logprob": -0.26437764625026755, "compression_ratio": 1.646341463414634, "no_speech_prob": 7.224433602459612e-07}, {"id": 255, "seek": 124652, "start": 1246.52, "end": 1250.2, "text": " And actually nowadays we can define this a little bit more elegantly", "tokens": [400, 767, 13434, 321, 393, 6964, 341, 257, 707, 857, 544, 14459, 3627], "temperature": 0.0, "avg_logprob": -0.23078334762389402, "compression_ratio": 1.5550660792951543, "no_speech_prob": 8.186351010408544e-07}, {"id": 256, "seek": 124652, "start": 1252.72, "end": 1257.7, "text": " Using the Python 3 matrix multiplication operator, which is the at sign", "tokens": [11142, 264, 15329, 805, 8141, 27290, 12973, 11, 597, 307, 264, 412, 1465], "temperature": 0.0, "avg_logprob": -0.23078334762389402, "compression_ratio": 1.5550660792951543, "no_speech_prob": 8.186351010408544e-07}, {"id": 257, "seek": 124652, "start": 1258.16, "end": 1261.3799999999999, "text": " Okay, and when you when you use that I think you kind of end up with", "tokens": [1033, 11, 293, 562, 291, 562, 291, 764, 300, 286, 519, 291, 733, 295, 917, 493, 365], "temperature": 0.0, "avg_logprob": -0.23078334762389402, "compression_ratio": 1.5550660792951543, "no_speech_prob": 8.186351010408544e-07}, {"id": 258, "seek": 124652, "start": 1262.0, "end": 1267.04, "text": " Something that looks closer to what the mathematical notation looked like and so I find that nicer", "tokens": [6595, 300, 1542, 4966, 281, 437, 264, 18894, 24657, 2956, 411, 293, 370, 286, 915, 300, 22842], "temperature": 0.0, "avg_logprob": -0.23078334762389402, "compression_ratio": 1.5550660792951543, "no_speech_prob": 8.186351010408544e-07}, {"id": 259, "seek": 124652, "start": 1269.6399999999999, "end": 1272.98, "text": " All right, so that's that's our linear layer", "tokens": [1057, 558, 11, 370, 300, 311, 300, 311, 527, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.23078334762389402, "compression_ratio": 1.5550660792951543, "no_speech_prob": 8.186351010408544e-07}, {"id": 260, "seek": 127298, "start": 1272.98, "end": 1279.78, "text": " In our logistic regression in our zero hidden layer neural net and so then the next thing we do to that is", "tokens": [682, 527, 3565, 3142, 24590, 294, 527, 4018, 7633, 4583, 18161, 2533, 293, 370, 550, 264, 958, 551, 321, 360, 281, 300, 307], "temperature": 0.0, "avg_logprob": -0.2696215311686198, "compression_ratio": 1.6211180124223603, "no_speech_prob": 2.0261384179320885e-06}, {"id": 261, "seek": 127298, "start": 1281.74, "end": 1286.8600000000001, "text": " Softmax okay, so we get the output of this", "tokens": [16985, 41167, 1392, 11, 370, 321, 483, 264, 5598, 295, 341], "temperature": 0.0, "avg_logprob": -0.2696215311686198, "compression_ratio": 1.6211180124223603, "no_speech_prob": 2.0261384179320885e-06}, {"id": 262, "seek": 127298, "start": 1289.46, "end": 1297.46, "text": " Matrix multiplier okay, who wants to tell me what the dimensionality of my output of this matrix multiply is", "tokens": [36274, 44106, 1392, 11, 567, 2738, 281, 980, 385, 437, 264, 10139, 1860, 295, 452, 5598, 295, 341, 8141, 12972, 307], "temperature": 0.0, "avg_logprob": -0.2696215311686198, "compression_ratio": 1.6211180124223603, "no_speech_prob": 2.0261384179320885e-06}, {"id": 263, "seek": 127298, "start": 1300.34, "end": 1302.04, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.2696215311686198, "compression_ratio": 1.6211180124223603, "no_speech_prob": 2.0261384179320885e-06}, {"id": 264, "seek": 130204, "start": 1302.04, "end": 1304.04, "text": " 64 by 10. Thank you, Karen", "tokens": [12145, 538, 1266, 13, 1044, 291, 11, 14834], "temperature": 0.0, "avg_logprob": -0.2079554848048998, "compression_ratio": 1.5372549019607844, "no_speech_prob": 2.948000883407076e-06}, {"id": 265, "seek": 130204, "start": 1306.62, "end": 1310.62, "text": " And I should mention for those of you that weren't at deep learning class yesterday", "tokens": [400, 286, 820, 2152, 337, 729, 295, 291, 300, 4999, 380, 412, 2452, 2539, 1508, 5186], "temperature": 0.0, "avg_logprob": -0.2079554848048998, "compression_ratio": 1.5372549019607844, "no_speech_prob": 2.948000883407076e-06}, {"id": 266, "seek": 130204, "start": 1310.6599999999999, "end": 1315.0, "text": " We actually looked at a really cool post from Karen who described how to", "tokens": [492, 767, 2956, 412, 257, 534, 1627, 2183, 490, 14834, 567, 7619, 577, 281], "temperature": 0.0, "avg_logprob": -0.2079554848048998, "compression_ratio": 1.5372549019607844, "no_speech_prob": 2.948000883407076e-06}, {"id": 267, "seek": 130204, "start": 1315.6599999999999, "end": 1320.42, "text": " Do structured data analysis with neural nets which has been like super popular?", "tokens": [1144, 18519, 1412, 5215, 365, 18161, 36170, 597, 575, 668, 411, 1687, 3743, 30], "temperature": 0.0, "avg_logprob": -0.2079554848048998, "compression_ratio": 1.5372549019607844, "no_speech_prob": 2.948000883407076e-06}, {"id": 268, "seek": 130204, "start": 1320.86, "end": 1325.6399999999999, "text": " And a whole bunch of people have kind of said that they've read it and found it super interesting so", "tokens": [400, 257, 1379, 3840, 295, 561, 362, 733, 295, 848, 300, 436, 600, 1401, 309, 293, 1352, 309, 1687, 1880, 370], "temperature": 0.0, "avg_logprob": -0.2079554848048998, "compression_ratio": 1.5372549019607844, "no_speech_prob": 2.948000883407076e-06}, {"id": 269, "seek": 130204, "start": 1327.02, "end": 1329.02, "text": " So that was really exciting", "tokens": [407, 300, 390, 534, 4670], "temperature": 0.0, "avg_logprob": -0.2079554848048998, "compression_ratio": 1.5372549019607844, "no_speech_prob": 2.948000883407076e-06}, {"id": 270, "seek": 132902, "start": 1329.02, "end": 1332.04, "text": " so we get this matrix of", "tokens": [370, 321, 483, 341, 8141, 295], "temperature": 0.0, "avg_logprob": -0.1944940208208443, "compression_ratio": 1.9758454106280192, "no_speech_prob": 1.118938143918058e-06}, {"id": 271, "seek": 132902, "start": 1332.8799999999999, "end": 1336.16, "text": " outputs and we put this through a soft max and", "tokens": [23930, 293, 321, 829, 341, 807, 257, 2787, 11469, 293], "temperature": 0.0, "avg_logprob": -0.1944940208208443, "compression_ratio": 1.9758454106280192, "no_speech_prob": 1.118938143918058e-06}, {"id": 272, "seek": 132902, "start": 1338.0, "end": 1344.0, "text": " Why do we put it through a soft max we put it through a soft max because in the end we want probably you know for", "tokens": [1545, 360, 321, 829, 309, 807, 257, 2787, 11469, 321, 829, 309, 807, 257, 2787, 11469, 570, 294, 264, 917, 321, 528, 1391, 291, 458, 337], "temperature": 0.0, "avg_logprob": -0.1944940208208443, "compression_ratio": 1.9758454106280192, "no_speech_prob": 1.118938143918058e-06}, {"id": 273, "seek": 132902, "start": 1344.0, "end": 1349.48, "text": " Every image we want a probability that this is zero or a one or a two or three or four right so we want a bunch", "tokens": [2048, 3256, 321, 528, 257, 8482, 300, 341, 307, 4018, 420, 257, 472, 420, 257, 732, 420, 1045, 420, 1451, 558, 370, 321, 528, 257, 3840], "temperature": 0.0, "avg_logprob": -0.1944940208208443, "compression_ratio": 1.9758454106280192, "no_speech_prob": 1.118938143918058e-06}, {"id": 274, "seek": 134948, "start": 1349.48, "end": 1359.52, "text": " Of probabilities that add up to one and where each of those probabilities is between zero and one so a soft max does", "tokens": [2720, 33783, 300, 909, 493, 281, 472, 293, 689, 1184, 295, 729, 33783, 307, 1296, 4018, 293, 472, 370, 257, 2787, 11469, 775], "temperature": 0.0, "avg_logprob": -0.1799116463496767, "compression_ratio": 1.6122448979591837, "no_speech_prob": 2.5215626919816714e-06}, {"id": 275, "seek": 134948, "start": 1359.52, "end": 1361.52, "text": " Exactly that for us", "tokens": [7587, 300, 337, 505], "temperature": 0.0, "avg_logprob": -0.1799116463496767, "compression_ratio": 1.6122448979591837, "no_speech_prob": 2.5215626919816714e-06}, {"id": 276, "seek": 134948, "start": 1362.08, "end": 1368.08, "text": " So for example if we weren't picking out you know numbers from naught to ten but instead we're picking out cat dog plane fish", "tokens": [407, 337, 1365, 498, 321, 4999, 380, 8867, 484, 291, 458, 3547, 490, 13138, 281, 2064, 457, 2602, 321, 434, 8867, 484, 3857, 3000, 5720, 3506], "temperature": 0.0, "avg_logprob": -0.1799116463496767, "compression_ratio": 1.6122448979591837, "no_speech_prob": 2.5215626919816714e-06}, {"id": 277, "seek": 134948, "start": 1368.08, "end": 1374.72, "text": " You're building the output of that matrix multiply for one particular image might look like that. These are just some random numbers", "tokens": [509, 434, 2390, 264, 5598, 295, 300, 8141, 12972, 337, 472, 1729, 3256, 1062, 574, 411, 300, 13, 1981, 366, 445, 512, 4974, 3547], "temperature": 0.0, "avg_logprob": -0.1799116463496767, "compression_ratio": 1.6122448979591837, "no_speech_prob": 2.5215626919816714e-06}, {"id": 278, "seek": 137472, "start": 1374.72, "end": 1382.48, "text": " And to turn that into a soft max I first go a to the power of each of those numbers I", "tokens": [400, 281, 1261, 300, 666, 257, 2787, 11469, 286, 700, 352, 257, 281, 264, 1347, 295, 1184, 295, 729, 3547, 286], "temperature": 0.0, "avg_logprob": -0.22110519409179688, "compression_ratio": 1.9949238578680204, "no_speech_prob": 1.136561763814825e-06}, {"id": 279, "seek": 137472, "start": 1384.28, "end": 1387.6000000000001, "text": " Sum up those e to the power ofs and", "tokens": [8626, 493, 729, 308, 281, 264, 1347, 295, 82, 293], "temperature": 0.0, "avg_logprob": -0.22110519409179688, "compression_ratio": 1.9949238578680204, "no_speech_prob": 1.136561763814825e-06}, {"id": 280, "seek": 137472, "start": 1389.08, "end": 1394.2, "text": " Then I take each of those e to the power ofs and divide it by the sum and that's soft max", "tokens": [1396, 286, 747, 1184, 295, 729, 308, 281, 264, 1347, 295, 82, 293, 9845, 309, 538, 264, 2408, 293, 300, 311, 2787, 11469], "temperature": 0.0, "avg_logprob": -0.22110519409179688, "compression_ratio": 1.9949238578680204, "no_speech_prob": 1.136561763814825e-06}, {"id": 281, "seek": 137472, "start": 1394.2, "end": 1399.56, "text": " That's the definition of soft max so because it was a to the power of it means. It's always positive", "tokens": [663, 311, 264, 7123, 295, 2787, 11469, 370, 570, 309, 390, 257, 281, 264, 1347, 295, 309, 1355, 13, 467, 311, 1009, 3353], "temperature": 0.0, "avg_logprob": -0.22110519409179688, "compression_ratio": 1.9949238578680204, "no_speech_prob": 1.136561763814825e-06}, {"id": 282, "seek": 139956, "start": 1399.56, "end": 1404.6, "text": " Because it was divided by the sum it means that it's always between zero and one", "tokens": [1436, 309, 390, 6666, 538, 264, 2408, 309, 1355, 300, 309, 311, 1009, 1296, 4018, 293, 472], "temperature": 0.0, "avg_logprob": -0.15162318251853765, "compression_ratio": 1.7425742574257426, "no_speech_prob": 4.247027334258746e-07}, {"id": 283, "seek": 139956, "start": 1404.6, "end": 1409.8, "text": " And it also means because it's divided by the sum that they always add up to one", "tokens": [400, 309, 611, 1355, 570, 309, 311, 6666, 538, 264, 2408, 300, 436, 1009, 909, 493, 281, 472], "temperature": 0.0, "avg_logprob": -0.15162318251853765, "compression_ratio": 1.7425742574257426, "no_speech_prob": 4.247027334258746e-07}, {"id": 284, "seek": 139956, "start": 1410.44, "end": 1412.9199999999998, "text": " So by applying this soft max", "tokens": [407, 538, 9275, 341, 2787, 11469], "temperature": 0.0, "avg_logprob": -0.15162318251853765, "compression_ratio": 1.7425742574257426, "no_speech_prob": 4.247027334258746e-07}, {"id": 285, "seek": 139956, "start": 1414.48, "end": 1416.9199999999998, "text": " Activation function so anytime we have a", "tokens": [28550, 399, 2445, 370, 13038, 321, 362, 257], "temperature": 0.0, "avg_logprob": -0.15162318251853765, "compression_ratio": 1.7425742574257426, "no_speech_prob": 4.247027334258746e-07}, {"id": 286, "seek": 139956, "start": 1417.6399999999999, "end": 1420.12, "text": " Layer of outputs which we call activations", "tokens": [35166, 295, 23930, 597, 321, 818, 2430, 763], "temperature": 0.0, "avg_logprob": -0.15162318251853765, "compression_ratio": 1.7425742574257426, "no_speech_prob": 4.247027334258746e-07}, {"id": 287, "seek": 139956, "start": 1420.56, "end": 1426.0, "text": " And then we apply some function some nonlinear function to that that maps one", "tokens": [400, 550, 321, 3079, 512, 2445, 512, 2107, 28263, 2445, 281, 300, 300, 11317, 472], "temperature": 0.0, "avg_logprob": -0.15162318251853765, "compression_ratio": 1.7425742574257426, "no_speech_prob": 4.247027334258746e-07}, {"id": 288, "seek": 142600, "start": 1426.0, "end": 1428.0, "text": " one", "tokens": [472], "temperature": 0.0, "avg_logprob": -0.22513283623589408, "compression_ratio": 1.782258064516129, "no_speech_prob": 1.1365608543201233e-06}, {"id": 289, "seek": 142600, "start": 1428.0, "end": 1432.02, "text": " Scaler to one scalar like softmax does we call that an activation function?", "tokens": [2747, 17148, 281, 472, 39684, 411, 2787, 41167, 775, 321, 818, 300, 364, 24433, 2445, 30], "temperature": 0.0, "avg_logprob": -0.22513283623589408, "compression_ratio": 1.782258064516129, "no_speech_prob": 1.1365608543201233e-06}, {"id": 290, "seek": 142600, "start": 1432.02, "end": 1439.28, "text": " Okay, so the softmax activation function takes our outputs and turns it into something which behaves like a probability", "tokens": [1033, 11, 370, 264, 2787, 41167, 24433, 2445, 2516, 527, 23930, 293, 4523, 309, 666, 746, 597, 36896, 411, 257, 8482], "temperature": 0.0, "avg_logprob": -0.22513283623589408, "compression_ratio": 1.782258064516129, "no_speech_prob": 1.1365608543201233e-06}, {"id": 291, "seek": 142600, "start": 1439.56, "end": 1448.0, "text": " Right we don't strictly speaking need it we could still try and train something which where the output directly is the probabilities", "tokens": [1779, 321, 500, 380, 20792, 4124, 643, 309, 321, 727, 920, 853, 293, 3847, 746, 597, 689, 264, 5598, 3838, 307, 264, 33783], "temperature": 0.0, "avg_logprob": -0.22513283623589408, "compression_ratio": 1.782258064516129, "no_speech_prob": 1.1365608543201233e-06}, {"id": 292, "seek": 142600, "start": 1448.44, "end": 1455.64, "text": " All right, but by creating using this function that automatically makes them always behave like probabilities", "tokens": [1057, 558, 11, 457, 538, 4084, 1228, 341, 2445, 300, 6772, 1669, 552, 1009, 15158, 411, 33783], "temperature": 0.0, "avg_logprob": -0.22513283623589408, "compression_ratio": 1.782258064516129, "no_speech_prob": 1.1365608543201233e-06}, {"id": 293, "seek": 145564, "start": 1455.64, "end": 1461.96, "text": " It means there's less for the network to learn so it's going to learn better right so generally speaking whenever we design", "tokens": [467, 1355, 456, 311, 1570, 337, 264, 3209, 281, 1466, 370, 309, 311, 516, 281, 1466, 1101, 558, 370, 5101, 4124, 5699, 321, 1715], "temperature": 0.0, "avg_logprob": -0.1805554653735871, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.5056984921102412e-06}, {"id": 294, "seek": 145564, "start": 1462.72, "end": 1464.64, "text": " an architecture", "tokens": [364, 9482], "temperature": 0.0, "avg_logprob": -0.1805554653735871, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.5056984921102412e-06}, {"id": 295, "seek": 145564, "start": 1464.64, "end": 1468.5600000000002, "text": " We try to design it in a way where it's as easy as possible", "tokens": [492, 853, 281, 1715, 309, 294, 257, 636, 689, 309, 311, 382, 1858, 382, 1944], "temperature": 0.0, "avg_logprob": -0.1805554653735871, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.5056984921102412e-06}, {"id": 296, "seek": 145564, "start": 1469.2800000000002, "end": 1472.5, "text": " For it to create something of the form that we want", "tokens": [1171, 309, 281, 1884, 746, 295, 264, 1254, 300, 321, 528], "temperature": 0.0, "avg_logprob": -0.1805554653735871, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.5056984921102412e-06}, {"id": 297, "seek": 145564, "start": 1473.4, "end": 1475.4, "text": " So that's why we use", "tokens": [407, 300, 311, 983, 321, 764], "temperature": 0.0, "avg_logprob": -0.1805554653735871, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.5056984921102412e-06}, {"id": 298, "seek": 145564, "start": 1475.5200000000002, "end": 1477.5200000000002, "text": " soft max", "tokens": [2787, 11469], "temperature": 0.0, "avg_logprob": -0.1805554653735871, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.5056984921102412e-06}, {"id": 299, "seek": 145564, "start": 1479.76, "end": 1484.16, "text": " Right so that's the basic steps right we have our input which is a bunch of images", "tokens": [1779, 370, 300, 311, 264, 3875, 4439, 558, 321, 362, 527, 4846, 597, 307, 257, 3840, 295, 5267], "temperature": 0.0, "avg_logprob": -0.1805554653735871, "compression_ratio": 1.6322869955156951, "no_speech_prob": 1.5056984921102412e-06}, {"id": 300, "seek": 148416, "start": 1484.16, "end": 1491.68, "text": " Right which is here gets multiplied by a weight metrics. We actually also add on a bias", "tokens": [1779, 597, 307, 510, 2170, 17207, 538, 257, 3364, 16367, 13, 492, 767, 611, 909, 322, 257, 12577], "temperature": 0.0, "avg_logprob": -0.19946530371001273, "compression_ratio": 1.528735632183908, "no_speech_prob": 2.4060948362603085e-06}, {"id": 301, "seek": 148416, "start": 1492.76, "end": 1494.76, "text": " right to get a", "tokens": [558, 281, 483, 257], "temperature": 0.0, "avg_logprob": -0.19946530371001273, "compression_ratio": 1.528735632183908, "no_speech_prob": 2.4060948362603085e-06}, {"id": 302, "seek": 148416, "start": 1495.0800000000002, "end": 1502.8000000000002, "text": " Output of the linear function we put it through a nonlinear activation function in this case softmax and that gives us our", "tokens": [5925, 2582, 295, 264, 8213, 2445, 321, 829, 309, 807, 257, 2107, 28263, 24433, 2445, 294, 341, 1389, 2787, 41167, 293, 300, 2709, 505, 527], "temperature": 0.0, "avg_logprob": -0.19946530371001273, "compression_ratio": 1.528735632183908, "no_speech_prob": 2.4060948362603085e-06}, {"id": 303, "seek": 148416, "start": 1503.64, "end": 1505.64, "text": " probabilities", "tokens": [33783], "temperature": 0.0, "avg_logprob": -0.19946530371001273, "compression_ratio": 1.528735632183908, "no_speech_prob": 2.4060948362603085e-06}, {"id": 304, "seek": 148416, "start": 1507.0800000000002, "end": 1509.0800000000002, "text": " So there there that all is", "tokens": [407, 456, 456, 300, 439, 307], "temperature": 0.0, "avg_logprob": -0.19946530371001273, "compression_ratio": 1.528735632183908, "no_speech_prob": 2.4060948362603085e-06}, {"id": 305, "seek": 150908, "start": 1509.08, "end": 1515.1599999999999, "text": " Pi torch also tends to use the log of", "tokens": [17741, 27822, 611, 12258, 281, 764, 264, 3565, 295], "temperature": 0.0, "avg_logprob": -0.2377476325401893, "compression_ratio": 1.5208333333333333, "no_speech_prob": 1.3709504855796695e-06}, {"id": 306, "seek": 150908, "start": 1516.24, "end": 1520.78, "text": " Softmax for reasons that don't particularly bother us now. It's basically a", "tokens": [16985, 41167, 337, 4112, 300, 500, 380, 4098, 8677, 505, 586, 13, 467, 311, 1936, 257], "temperature": 0.0, "avg_logprob": -0.2377476325401893, "compression_ratio": 1.5208333333333333, "no_speech_prob": 1.3709504855796695e-06}, {"id": 307, "seek": 150908, "start": 1521.48, "end": 1526.04, "text": " numerical stability convenience okay, so to make this the same as our", "tokens": [29054, 11826, 19283, 1392, 11, 370, 281, 652, 341, 264, 912, 382, 527], "temperature": 0.0, "avg_logprob": -0.2377476325401893, "compression_ratio": 1.5208333333333333, "no_speech_prob": 1.3709504855796695e-06}, {"id": 308, "seek": 150908, "start": 1527.4399999999998, "end": 1534.4399999999998, "text": " Version up here that you saw log softmax. I'm going to use log here as well, okay, so", "tokens": [35965, 493, 510, 300, 291, 1866, 3565, 2787, 41167, 13, 286, 478, 516, 281, 764, 3565, 510, 382, 731, 11, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.2377476325401893, "compression_ratio": 1.5208333333333333, "no_speech_prob": 1.3709504855796695e-06}, {"id": 309, "seek": 150908, "start": 1536.0, "end": 1538.0, "text": " We can now instantiate", "tokens": [492, 393, 586, 9836, 13024], "temperature": 0.0, "avg_logprob": -0.2377476325401893, "compression_ratio": 1.5208333333333333, "no_speech_prob": 1.3709504855796695e-06}, {"id": 310, "seek": 153800, "start": 1538.0, "end": 1541.08, "text": " This class that is create an object of this class", "tokens": [639, 1508, 300, 307, 1884, 364, 2657, 295, 341, 1508], "temperature": 0.0, "avg_logprob": -0.22689678693058515, "compression_ratio": 1.7916666666666667, "no_speech_prob": 1.328773009845463e-06}, {"id": 311, "seek": 153800, "start": 1543.04, "end": 1549.4, "text": " So I have a question back for the probabilities where we were before so", "tokens": [407, 286, 362, 257, 1168, 646, 337, 264, 33783, 689, 321, 645, 949, 370], "temperature": 0.0, "avg_logprob": -0.22689678693058515, "compression_ratio": 1.7916666666666667, "no_speech_prob": 1.328773009845463e-06}, {"id": 312, "seek": 153800, "start": 1551.0, "end": 1557.5, "text": " If we were to have a photo with a cat and a dog together would that change the way that that works or does it work?", "tokens": [759, 321, 645, 281, 362, 257, 5052, 365, 257, 3857, 293, 257, 3000, 1214, 576, 300, 1319, 264, 636, 300, 300, 1985, 420, 775, 309, 589, 30], "temperature": 0.0, "avg_logprob": -0.22689678693058515, "compression_ratio": 1.7916666666666667, "no_speech_prob": 1.328773009845463e-06}, {"id": 313, "seek": 153800, "start": 1557.5, "end": 1563.2, "text": " In the same basic yeah, so that's a great question so if you had a photo with a cat and a dog together", "tokens": [682, 264, 912, 3875, 1338, 11, 370, 300, 311, 257, 869, 1168, 370, 498, 291, 632, 257, 5052, 365, 257, 3857, 293, 257, 3000, 1214], "temperature": 0.0, "avg_logprob": -0.22689678693058515, "compression_ratio": 1.7916666666666667, "no_speech_prob": 1.328773009845463e-06}, {"id": 314, "seek": 153800, "start": 1563.84, "end": 1567.12, "text": " And you wanted it to spit out both cat and dog", "tokens": [400, 291, 1415, 309, 281, 22127, 484, 1293, 3857, 293, 3000], "temperature": 0.0, "avg_logprob": -0.22689678693058515, "compression_ratio": 1.7916666666666667, "no_speech_prob": 1.328773009845463e-06}, {"id": 315, "seek": 156712, "start": 1567.12, "end": 1573.12, "text": " This would be a very poor choice so softmax is specifically the activation function", "tokens": [639, 576, 312, 257, 588, 4716, 3922, 370, 2787, 41167, 307, 4682, 264, 24433, 2445], "temperature": 0.0, "avg_logprob": -0.1336377974479429, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.356860245025018e-06}, {"id": 316, "seek": 156712, "start": 1573.12, "end": 1581.04, "text": " We use for categorical predictions where we only ever want to predict one of those things right and so part of the reason", "tokens": [492, 764, 337, 19250, 804, 21264, 689, 321, 787, 1562, 528, 281, 6069, 472, 295, 729, 721, 558, 293, 370, 644, 295, 264, 1778], "temperature": 0.0, "avg_logprob": -0.1336377974479429, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.356860245025018e-06}, {"id": 317, "seek": 156712, "start": 1581.04, "end": 1587.1599999999999, "text": " Why is that as you can see because we're using either that right either the slightly bigger numbers", "tokens": [1545, 307, 300, 382, 291, 393, 536, 570, 321, 434, 1228, 2139, 300, 558, 2139, 264, 4748, 3801, 3547], "temperature": 0.0, "avg_logprob": -0.1336377974479429, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.356860245025018e-06}, {"id": 318, "seek": 156712, "start": 1587.3999999999999, "end": 1594.3999999999999, "text": " Creates much bigger numbers as a result of which we generally have just one or two things large and everything else is pretty small", "tokens": [11972, 279, 709, 3801, 3547, 382, 257, 1874, 295, 597, 321, 5101, 362, 445, 472, 420, 732, 721, 2416, 293, 1203, 1646, 307, 1238, 1359], "temperature": 0.0, "avg_logprob": -0.1336377974479429, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.356860245025018e-06}, {"id": 319, "seek": 159440, "start": 1594.4, "end": 1597.64, "text": " All right, so if I like recalculate these random numbers a few times", "tokens": [1057, 558, 11, 370, 498, 286, 411, 850, 304, 2444, 473, 613, 4974, 3547, 257, 1326, 1413], "temperature": 0.0, "avg_logprob": -0.15152522921562195, "compression_ratio": 1.6355555555555557, "no_speech_prob": 1.2482669262681156e-06}, {"id": 320, "seek": 159440, "start": 1597.96, "end": 1604.4, "text": " You'll see like it tends to be a bunch of zeros and one or two high numbers right so it's really designed to", "tokens": [509, 603, 536, 411, 309, 12258, 281, 312, 257, 3840, 295, 35193, 293, 472, 420, 732, 1090, 3547, 558, 370, 309, 311, 534, 4761, 281], "temperature": 0.0, "avg_logprob": -0.15152522921562195, "compression_ratio": 1.6355555555555557, "no_speech_prob": 1.2482669262681156e-06}, {"id": 321, "seek": 159440, "start": 1605.3600000000001, "end": 1611.5, "text": " Try to kind of make it easy to predict like this one thing is the thing I want", "tokens": [6526, 281, 733, 295, 652, 309, 1858, 281, 6069, 411, 341, 472, 551, 307, 264, 551, 286, 528], "temperature": 0.0, "avg_logprob": -0.15152522921562195, "compression_ratio": 1.6355555555555557, "no_speech_prob": 1.2482669262681156e-06}, {"id": 322, "seek": 159440, "start": 1612.0, "end": 1614.0, "text": " If you're doing multi", "tokens": [759, 291, 434, 884, 4825], "temperature": 0.0, "avg_logprob": -0.15152522921562195, "compression_ratio": 1.6355555555555557, "no_speech_prob": 1.2482669262681156e-06}, {"id": 323, "seek": 159440, "start": 1614.5600000000002, "end": 1619.38, "text": " Label prediction so I want to find all the things in this image rather than using softmax", "tokens": [10137, 338, 17630, 370, 286, 528, 281, 915, 439, 264, 721, 294, 341, 3256, 2831, 813, 1228, 2787, 41167], "temperature": 0.0, "avg_logprob": -0.15152522921562195, "compression_ratio": 1.6355555555555557, "no_speech_prob": 1.2482669262681156e-06}, {"id": 324, "seek": 161938, "start": 1619.38, "end": 1625.64, "text": " We would instead use sigmoid that's a sigmoid recall each would cause each of these between to be between zero and one", "tokens": [492, 576, 2602, 764, 4556, 3280, 327, 300, 311, 257, 4556, 3280, 327, 9901, 1184, 576, 3082, 1184, 295, 613, 1296, 281, 312, 1296, 4018, 293, 472], "temperature": 0.0, "avg_logprob": -0.15038445618775514, "compression_ratio": 1.7698412698412698, "no_speech_prob": 2.902291498685372e-06}, {"id": 325, "seek": 161938, "start": 1625.8400000000001, "end": 1627.8400000000001, "text": " But they would no longer add to one", "tokens": [583, 436, 576, 572, 2854, 909, 281, 472], "temperature": 0.0, "avg_logprob": -0.15038445618775514, "compression_ratio": 1.7698412698412698, "no_speech_prob": 2.902291498685372e-06}, {"id": 326, "seek": 161938, "start": 1628.5600000000002, "end": 1631.5, "text": " It's a good question and like a lot of these", "tokens": [467, 311, 257, 665, 1168, 293, 411, 257, 688, 295, 613], "temperature": 0.0, "avg_logprob": -0.15038445618775514, "compression_ratio": 1.7698412698412698, "no_speech_prob": 2.902291498685372e-06}, {"id": 327, "seek": 161938, "start": 1633.2800000000002, "end": 1638.1200000000001, "text": " Details about like best practices are things that we cover in the deep learning course", "tokens": [42811, 466, 411, 1151, 7525, 366, 721, 300, 321, 2060, 294, 264, 2452, 2539, 1164], "temperature": 0.0, "avg_logprob": -0.15038445618775514, "compression_ratio": 1.7698412698412698, "no_speech_prob": 2.902291498685372e-06}, {"id": 328, "seek": 161938, "start": 1638.1200000000001, "end": 1642.14, "text": " And we won't cover heaps of them here in the machine learning course. We're more interested in the", "tokens": [400, 321, 1582, 380, 2060, 415, 2382, 295, 552, 510, 294, 264, 3479, 2539, 1164, 13, 492, 434, 544, 3102, 294, 264], "temperature": 0.0, "avg_logprob": -0.15038445618775514, "compression_ratio": 1.7698412698412698, "no_speech_prob": 2.902291498685372e-06}, {"id": 329, "seek": 161938, "start": 1642.7800000000002, "end": 1644.7800000000002, "text": " mechanics I guess", "tokens": [12939, 286, 2041], "temperature": 0.0, "avg_logprob": -0.15038445618775514, "compression_ratio": 1.7698412698412698, "no_speech_prob": 2.902291498685372e-06}, {"id": 330, "seek": 164478, "start": 1644.78, "end": 1647.84, "text": " But we'll try and do them with their quick", "tokens": [583, 321, 603, 853, 293, 360, 552, 365, 641, 1702], "temperature": 0.0, "avg_logprob": -0.1387080868471016, "compression_ratio": 1.6887966804979253, "no_speech_prob": 1.4367442418006249e-06}, {"id": 331, "seek": 164478, "start": 1649.94, "end": 1655.42, "text": " All right, so now that we've got that we can instantiate an object of that class and of course", "tokens": [1057, 558, 11, 370, 586, 300, 321, 600, 658, 300, 321, 393, 9836, 13024, 364, 2657, 295, 300, 1508, 293, 295, 1164], "temperature": 0.0, "avg_logprob": -0.1387080868471016, "compression_ratio": 1.6887966804979253, "no_speech_prob": 1.4367442418006249e-06}, {"id": 332, "seek": 164478, "start": 1655.42, "end": 1658.94, "text": " We want to copy it over to the GPU so we can do computations over there", "tokens": [492, 528, 281, 5055, 309, 670, 281, 264, 18407, 370, 321, 393, 360, 2807, 763, 670, 456], "temperature": 0.0, "avg_logprob": -0.1387080868471016, "compression_ratio": 1.6887966804979253, "no_speech_prob": 1.4367442418006249e-06}, {"id": 333, "seek": 164478, "start": 1659.86, "end": 1664.58, "text": " Again, we need an optimizer. We'll be talking about what this is shortly, but you'll see here", "tokens": [3764, 11, 321, 643, 364, 5028, 6545, 13, 492, 603, 312, 1417, 466, 437, 341, 307, 13392, 11, 457, 291, 603, 536, 510], "temperature": 0.0, "avg_logprob": -0.1387080868471016, "compression_ratio": 1.6887966804979253, "no_speech_prob": 1.4367442418006249e-06}, {"id": 334, "seek": 164478, "start": 1664.58, "end": 1671.34, "text": " We've called a function on our class called parameters, but we never defined a method called parameters", "tokens": [492, 600, 1219, 257, 2445, 322, 527, 1508, 1219, 9834, 11, 457, 321, 1128, 7642, 257, 3170, 1219, 9834], "temperature": 0.0, "avg_logprob": -0.1387080868471016, "compression_ratio": 1.6887966804979253, "no_speech_prob": 1.4367442418006249e-06}, {"id": 335, "seek": 167134, "start": 1671.34, "end": 1677.74, "text": " And the reason that is going to work is because it actually was defined for us inside n n dot module and so n n dot module", "tokens": [400, 264, 1778, 300, 307, 516, 281, 589, 307, 570, 309, 767, 390, 7642, 337, 505, 1854, 297, 297, 5893, 10088, 293, 370, 297, 297, 5893, 10088], "temperature": 0.0, "avg_logprob": -0.22122650917130288, "compression_ratio": 1.7932489451476794, "no_speech_prob": 7.411239494103938e-06}, {"id": 336, "seek": 167134, "start": 1678.22, "end": 1682.32, "text": " actually automatically goes through the attributes we've created and", "tokens": [767, 6772, 1709, 807, 264, 17212, 321, 600, 2942, 293], "temperature": 0.0, "avg_logprob": -0.22122650917130288, "compression_ratio": 1.7932489451476794, "no_speech_prob": 7.411239494103938e-06}, {"id": 337, "seek": 167134, "start": 1683.02, "end": 1687.86, "text": " Finds anything that basically we we said this is a parameter", "tokens": [11809, 82, 1340, 300, 1936, 321, 321, 848, 341, 307, 257, 13075], "temperature": 0.0, "avg_logprob": -0.22122650917130288, "compression_ratio": 1.7932489451476794, "no_speech_prob": 7.411239494103938e-06}, {"id": 338, "seek": 167134, "start": 1687.86, "end": 1691.26, "text": " So the way you say something is a parameter is you wrap it in and end up parameter", "tokens": [407, 264, 636, 291, 584, 746, 307, 257, 13075, 307, 291, 7019, 309, 294, 293, 917, 493, 13075], "temperature": 0.0, "avg_logprob": -0.22122650917130288, "compression_ratio": 1.7932489451476794, "no_speech_prob": 7.411239494103938e-06}, {"id": 339, "seek": 167134, "start": 1691.26, "end": 1696.1399999999999, "text": " So this is just the way that you tell pi torch. This is something that I want to optimize", "tokens": [407, 341, 307, 445, 264, 636, 300, 291, 980, 3895, 27822, 13, 639, 307, 746, 300, 286, 528, 281, 19719], "temperature": 0.0, "avg_logprob": -0.22122650917130288, "compression_ratio": 1.7932489451476794, "no_speech_prob": 7.411239494103938e-06}, {"id": 340, "seek": 169614, "start": 1696.14, "end": 1701.42, "text": " Okay, so when we created the weight matrix. We just wrapped it with an end up parameter", "tokens": [1033, 11, 370, 562, 321, 2942, 264, 3364, 8141, 13, 492, 445, 14226, 309, 365, 364, 917, 493, 13075], "temperature": 0.0, "avg_logprob": -0.1756602434011606, "compression_ratio": 1.6628352490421456, "no_speech_prob": 2.813005494317622e-06}, {"id": 341, "seek": 169614, "start": 1701.42, "end": 1703.8600000000001, "text": " It's exactly the same as a regular", "tokens": [467, 311, 2293, 264, 912, 382, 257, 3890], "temperature": 0.0, "avg_logprob": -0.1756602434011606, "compression_ratio": 1.6628352490421456, "no_speech_prob": 2.813005494317622e-06}, {"id": 342, "seek": 169614, "start": 1704.38, "end": 1709.5, "text": " Pi torch variable which we'll learn about shortly. It's just a little flag to say hey", "tokens": [17741, 27822, 7006, 597, 321, 603, 1466, 466, 13392, 13, 467, 311, 445, 257, 707, 7166, 281, 584, 4177], "temperature": 0.0, "avg_logprob": -0.1756602434011606, "compression_ratio": 1.6628352490421456, "no_speech_prob": 2.813005494317622e-06}, {"id": 343, "seek": 169614, "start": 1709.5, "end": 1715.72, "text": " You should you should optimize this and so when you call net two dot parameters on our net two object", "tokens": [509, 820, 291, 820, 19719, 341, 293, 370, 562, 291, 818, 2533, 732, 5893, 9834, 322, 527, 2533, 732, 2657], "temperature": 0.0, "avg_logprob": -0.1756602434011606, "compression_ratio": 1.6628352490421456, "no_speech_prob": 2.813005494317622e-06}, {"id": 344, "seek": 169614, "start": 1715.72, "end": 1718.9, "text": " We created it goes through everything that we created in the constructor", "tokens": [492, 2942, 309, 1709, 807, 1203, 300, 321, 2942, 294, 264, 47479], "temperature": 0.0, "avg_logprob": -0.1756602434011606, "compression_ratio": 1.6628352490421456, "no_speech_prob": 2.813005494317622e-06}, {"id": 345, "seek": 169614, "start": 1719.38, "end": 1721.8600000000001, "text": " Checks to see if any of them are of type parameter", "tokens": [3351, 2761, 281, 536, 498, 604, 295, 552, 366, 295, 2010, 13075], "temperature": 0.0, "avg_logprob": -0.1756602434011606, "compression_ratio": 1.6628352490421456, "no_speech_prob": 2.813005494317622e-06}, {"id": 346, "seek": 172186, "start": 1721.86, "end": 1726.62, "text": " And if so it sets all of those being things that we want to train with the optimizer", "tokens": [400, 498, 370, 309, 6352, 439, 295, 729, 885, 721, 300, 321, 528, 281, 3847, 365, 264, 5028, 6545], "temperature": 0.0, "avg_logprob": -0.20586381639753068, "compression_ratio": 1.5024630541871922, "no_speech_prob": 1.191106889564253e-06}, {"id": 347, "seek": 172186, "start": 1726.62, "end": 1730.04, "text": " And we'll be implementing the optimizer from scratch later", "tokens": [400, 321, 603, 312, 18114, 264, 5028, 6545, 490, 8459, 1780], "temperature": 0.0, "avg_logprob": -0.20586381639753068, "compression_ratio": 1.5024630541871922, "no_speech_prob": 1.191106889564253e-06}, {"id": 348, "seek": 172186, "start": 1731.02, "end": 1733.02, "text": " Okay, so having done that", "tokens": [1033, 11, 370, 1419, 1096, 300], "temperature": 0.0, "avg_logprob": -0.20586381639753068, "compression_ratio": 1.5024630541871922, "no_speech_prob": 1.191106889564253e-06}, {"id": 349, "seek": 172186, "start": 1736.8999999999999, "end": 1738.8999999999999, "text": " We can fit", "tokens": [492, 393, 3318], "temperature": 0.0, "avg_logprob": -0.20586381639753068, "compression_ratio": 1.5024630541871922, "no_speech_prob": 1.191106889564253e-06}, {"id": 350, "seek": 172186, "start": 1738.8999999999999, "end": 1742.62, "text": " And we should get basically the same answer as before 91 ish", "tokens": [400, 321, 820, 483, 1936, 264, 912, 1867, 382, 949, 31064, 307, 71], "temperature": 0.0, "avg_logprob": -0.20586381639753068, "compression_ratio": 1.5024630541871922, "no_speech_prob": 1.191106889564253e-06}, {"id": 351, "seek": 172186, "start": 1743.74, "end": 1745.74, "text": " So that looks good", "tokens": [407, 300, 1542, 665], "temperature": 0.0, "avg_logprob": -0.20586381639753068, "compression_ratio": 1.5024630541871922, "no_speech_prob": 1.191106889564253e-06}, {"id": 352, "seek": 172186, "start": 1745.78, "end": 1747.78, "text": " alright", "tokens": [5845], "temperature": 0.0, "avg_logprob": -0.20586381639753068, "compression_ratio": 1.5024630541871922, "no_speech_prob": 1.191106889564253e-06}, {"id": 353, "seek": 172186, "start": 1748.02, "end": 1749.5, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.20586381639753068, "compression_ratio": 1.5024630541871922, "no_speech_prob": 1.191106889564253e-06}, {"id": 354, "seek": 174950, "start": 1749.5, "end": 1755.1, "text": " What have we actually built here? Well what we've actually built as I said is something that can behave", "tokens": [708, 362, 321, 767, 3094, 510, 30, 1042, 437, 321, 600, 767, 3094, 382, 286, 848, 307, 746, 300, 393, 15158], "temperature": 0.0, "avg_logprob": -0.14267562936853478, "compression_ratio": 1.7927927927927927, "no_speech_prob": 3.7266197523422306e-06}, {"id": 355, "seek": 174950, "start": 1755.7, "end": 1761.66, "text": " Like a regular function right so I want to show you how we can actually call this as a function", "tokens": [1743, 257, 3890, 2445, 558, 370, 286, 528, 281, 855, 291, 577, 321, 393, 767, 818, 341, 382, 257, 2445], "temperature": 0.0, "avg_logprob": -0.14267562936853478, "compression_ratio": 1.7927927927927927, "no_speech_prob": 3.7266197523422306e-06}, {"id": 356, "seek": 174950, "start": 1761.66, "end": 1763.66, "text": " So to be able to call it as a function", "tokens": [407, 281, 312, 1075, 281, 818, 309, 382, 257, 2445], "temperature": 0.0, "avg_logprob": -0.14267562936853478, "compression_ratio": 1.7927927927927927, "no_speech_prob": 3.7266197523422306e-06}, {"id": 357, "seek": 174950, "start": 1763.82, "end": 1768.18, "text": " We need to be able to pass data to it to be able to pass data to it", "tokens": [492, 643, 281, 312, 1075, 281, 1320, 1412, 281, 309, 281, 312, 1075, 281, 1320, 1412, 281, 309], "temperature": 0.0, "avg_logprob": -0.14267562936853478, "compression_ratio": 1.7927927927927927, "no_speech_prob": 3.7266197523422306e-06}, {"id": 358, "seek": 174950, "start": 1768.18, "end": 1771.74, "text": " I'm going to need to grab a mini batch of MNIST images", "tokens": [286, 478, 516, 281, 643, 281, 4444, 257, 8382, 15245, 295, 376, 45, 19756, 5267], "temperature": 0.0, "avg_logprob": -0.14267562936853478, "compression_ratio": 1.7927927927927927, "no_speech_prob": 3.7266197523422306e-06}, {"id": 359, "seek": 174950, "start": 1772.82, "end": 1774.82, "text": " Okay, so we used", "tokens": [1033, 11, 370, 321, 1143], "temperature": 0.0, "avg_logprob": -0.14267562936853478, "compression_ratio": 1.7927927927927927, "no_speech_prob": 3.7266197523422306e-06}, {"id": 360, "seek": 174950, "start": 1775.26, "end": 1777.26, "text": " for convenience the", "tokens": [337, 19283, 264], "temperature": 0.0, "avg_logprob": -0.14267562936853478, "compression_ratio": 1.7927927927927927, "no_speech_prob": 3.7266197523422306e-06}, {"id": 361, "seek": 177726, "start": 1777.26, "end": 1780.3799999999999, "text": " Image classifier data from arrays method from fast AI", "tokens": [29903, 1508, 9902, 1412, 490, 41011, 3170, 490, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.23929221131080805, "compression_ratio": 1.9344262295081966, "no_speech_prob": 2.9022892249486176e-06}, {"id": 362, "seek": 177726, "start": 1780.9, "end": 1787.1, "text": " And what that does is it creates a pie torch data loader for us a pie torch data loader is", "tokens": [400, 437, 300, 775, 307, 309, 7829, 257, 1730, 27822, 1412, 3677, 260, 337, 505, 257, 1730, 27822, 1412, 3677, 260, 307], "temperature": 0.0, "avg_logprob": -0.23929221131080805, "compression_ratio": 1.9344262295081966, "no_speech_prob": 2.9022892249486176e-06}, {"id": 363, "seek": 177726, "start": 1787.42, "end": 1791.26, "text": " Something that grabs a few images and sticks them into a mini batch", "tokens": [6595, 300, 30028, 257, 1326, 5267, 293, 12518, 552, 666, 257, 8382, 15245], "temperature": 0.0, "avg_logprob": -0.23929221131080805, "compression_ratio": 1.9344262295081966, "no_speech_prob": 2.9022892249486176e-06}, {"id": 364, "seek": 177726, "start": 1791.42, "end": 1797.3, "text": " And makes them available and you can basically say give me another mini batch give me another mini batch give me another mini batch", "tokens": [400, 1669, 552, 2435, 293, 291, 393, 1936, 584, 976, 385, 1071, 8382, 15245, 976, 385, 1071, 8382, 15245, 976, 385, 1071, 8382, 15245], "temperature": 0.0, "avg_logprob": -0.23929221131080805, "compression_ratio": 1.9344262295081966, "no_speech_prob": 2.9022892249486176e-06}, {"id": 365, "seek": 177726, "start": 1797.3, "end": 1799.3, "text": " and so", "tokens": [293, 370], "temperature": 0.0, "avg_logprob": -0.23929221131080805, "compression_ratio": 1.9344262295081966, "no_speech_prob": 2.9022892249486176e-06}, {"id": 366, "seek": 177726, "start": 1800.66, "end": 1802.42, "text": " in", "tokens": [294], "temperature": 0.0, "avg_logprob": -0.23929221131080805, "compression_ratio": 1.9344262295081966, "no_speech_prob": 2.9022892249486176e-06}, {"id": 367, "seek": 180242, "start": 1802.42, "end": 1810.02, "text": " Python we call these things generators generators are things where you can basically say I want another I want another I want another right", "tokens": [15329, 321, 818, 613, 721, 38662, 38662, 366, 721, 689, 291, 393, 1936, 584, 286, 528, 1071, 286, 528, 1071, 286, 528, 1071, 558], "temperature": 0.0, "avg_logprob": -0.1556918132736022, "compression_ratio": 1.8364485981308412, "no_speech_prob": 3.3405090107407887e-06}, {"id": 368, "seek": 180242, "start": 1813.78, "end": 1815.94, "text": " There's this kind of very close connection between", "tokens": [821, 311, 341, 733, 295, 588, 1998, 4984, 1296], "temperature": 0.0, "avg_logprob": -0.1556918132736022, "compression_ratio": 1.8364485981308412, "no_speech_prob": 3.3405090107407887e-06}, {"id": 369, "seek": 180242, "start": 1816.5, "end": 1823.14, "text": " Iterators and generators are not going to worry about the difference between them right now, but you'll see basically to turn", "tokens": [286, 391, 3391, 293, 38662, 366, 406, 516, 281, 3292, 466, 264, 2649, 1296, 552, 558, 586, 11, 457, 291, 603, 536, 1936, 281, 1261], "temperature": 0.0, "avg_logprob": -0.1556918132736022, "compression_ratio": 1.8364485981308412, "no_speech_prob": 3.3405090107407887e-06}, {"id": 370, "seek": 180242, "start": 1825.22, "end": 1829.5, "text": " To actually get hold of something which we can say please give me another of", "tokens": [1407, 767, 483, 1797, 295, 746, 597, 321, 393, 584, 1767, 976, 385, 1071, 295], "temperature": 0.0, "avg_logprob": -0.1556918132736022, "compression_ratio": 1.8364485981308412, "no_speech_prob": 3.3405090107407887e-06}, {"id": 371, "seek": 182950, "start": 1829.5, "end": 1831.5, "text": " of", "tokens": [295], "temperature": 0.0, "avg_logprob": -0.14897813882913674, "compression_ratio": 2.0483091787439616, "no_speech_prob": 1.733039994178398e-06}, {"id": 372, "seek": 182950, "start": 1831.98, "end": 1836.34, "text": " In order to grab something that we can we can use to generate many batches", "tokens": [682, 1668, 281, 4444, 746, 300, 321, 393, 321, 393, 764, 281, 8460, 867, 15245, 279], "temperature": 0.0, "avg_logprob": -0.14897813882913674, "compression_ratio": 2.0483091787439616, "no_speech_prob": 1.733039994178398e-06}, {"id": 373, "seek": 182950, "start": 1836.58, "end": 1838.46, "text": " We have to take our data loader", "tokens": [492, 362, 281, 747, 527, 1412, 3677, 260], "temperature": 0.0, "avg_logprob": -0.14897813882913674, "compression_ratio": 2.0483091787439616, "no_speech_prob": 1.733039994178398e-06}, {"id": 374, "seek": 182950, "start": 1838.46, "end": 1843.18, "text": " And so you can ask for the training data loader from our model data object", "tokens": [400, 370, 291, 393, 1029, 337, 264, 3097, 1412, 3677, 260, 490, 527, 2316, 1412, 2657], "temperature": 0.0, "avg_logprob": -0.14897813882913674, "compression_ratio": 2.0483091787439616, "no_speech_prob": 1.733039994178398e-06}, {"id": 375, "seek": 182950, "start": 1843.18, "end": 1849.46, "text": " You'll see there's a bunch of different data loaders you can ask for you can ask for the test data loader the train data loader", "tokens": [509, 603, 536, 456, 311, 257, 3840, 295, 819, 1412, 3677, 433, 291, 393, 1029, 337, 291, 393, 1029, 337, 264, 1500, 1412, 3677, 260, 264, 3847, 1412, 3677, 260], "temperature": 0.0, "avg_logprob": -0.14897813882913674, "compression_ratio": 2.0483091787439616, "no_speech_prob": 1.733039994178398e-06}, {"id": 376, "seek": 182950, "start": 1849.46, "end": 1851.46, "text": " the validation loader", "tokens": [264, 24071, 3677, 260], "temperature": 0.0, "avg_logprob": -0.14897813882913674, "compression_ratio": 2.0483091787439616, "no_speech_prob": 1.733039994178398e-06}, {"id": 377, "seek": 182950, "start": 1851.98, "end": 1856.06, "text": " Augmented images data loader and so forth so we're going to grab the training data loader", "tokens": [6088, 14684, 5267, 1412, 3677, 260, 293, 370, 5220, 370, 321, 434, 516, 281, 4444, 264, 3097, 1412, 3677, 260], "temperature": 0.0, "avg_logprob": -0.14897813882913674, "compression_ratio": 2.0483091787439616, "no_speech_prob": 1.733039994178398e-06}, {"id": 378, "seek": 185606, "start": 1856.06, "end": 1863.3, "text": " That was created for us. This is a PI standard PI torch data loader well slightly optimized by us, but same idea", "tokens": [663, 390, 2942, 337, 505, 13, 639, 307, 257, 27176, 3832, 27176, 27822, 1412, 3677, 260, 731, 4748, 26941, 538, 505, 11, 457, 912, 1558], "temperature": 0.0, "avg_logprob": -0.17080091922841173, "compression_ratio": 1.7706422018348624, "no_speech_prob": 2.4060880150500452e-06}, {"id": 379, "seek": 185606, "start": 1863.8999999999999, "end": 1867.02, "text": " And you can then say this is a standard Python", "tokens": [400, 291, 393, 550, 584, 341, 307, 257, 3832, 15329], "temperature": 0.0, "avg_logprob": -0.17080091922841173, "compression_ratio": 1.7706422018348624, "no_speech_prob": 2.4060880150500452e-06}, {"id": 380, "seek": 185606, "start": 1867.46, "end": 1874.1399999999999, "text": " Thing we can say turn that into an iterator turn that into something where we can grab another one at a time from and", "tokens": [30902, 321, 393, 584, 1261, 300, 666, 364, 17138, 1639, 1261, 300, 666, 746, 689, 321, 393, 4444, 1071, 472, 412, 257, 565, 490, 293], "temperature": 0.0, "avg_logprob": -0.17080091922841173, "compression_ratio": 1.7706422018348624, "no_speech_prob": 2.4060880150500452e-06}, {"id": 381, "seek": 185606, "start": 1874.34, "end": 1876.34, "text": " So once you've done that", "tokens": [407, 1564, 291, 600, 1096, 300], "temperature": 0.0, "avg_logprob": -0.17080091922841173, "compression_ratio": 1.7706422018348624, "no_speech_prob": 2.4060880150500452e-06}, {"id": 382, "seek": 185606, "start": 1876.54, "end": 1880.86, "text": " We've now got something that we can iterate through you can use the standard Python", "tokens": [492, 600, 586, 658, 746, 300, 321, 393, 44497, 807, 291, 393, 764, 264, 3832, 15329], "temperature": 0.0, "avg_logprob": -0.17080091922841173, "compression_ratio": 1.7706422018348624, "no_speech_prob": 2.4060880150500452e-06}, {"id": 383, "seek": 188086, "start": 1880.86, "end": 1886.82, "text": " Next function to grab one more thing from that generator, okay?", "tokens": [3087, 2445, 281, 4444, 472, 544, 551, 490, 300, 19265, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.28683560827504034, "compression_ratio": 1.6111111111111112, "no_speech_prob": 2.6425734631629894e-06}, {"id": 384, "seek": 188086, "start": 1889.26, "end": 1893.1399999999999, "text": " So that's returning and the X's from our mini-batch and the wise", "tokens": [407, 300, 311, 12678, 293, 264, 1783, 311, 490, 527, 8382, 12, 65, 852, 293, 264, 10829], "temperature": 0.0, "avg_logprob": -0.28683560827504034, "compression_ratio": 1.6111111111111112, "no_speech_prob": 2.6425734631629894e-06}, {"id": 385, "seek": 188086, "start": 1893.74, "end": 1896.5, "text": " Family mini-batch the other way that you can use", "tokens": [11661, 8382, 12, 65, 852, 264, 661, 636, 300, 291, 393, 764], "temperature": 0.0, "avg_logprob": -0.28683560827504034, "compression_ratio": 1.6111111111111112, "no_speech_prob": 2.6425734631629894e-06}, {"id": 386, "seek": 188086, "start": 1897.4199999999998, "end": 1902.34, "text": " Generators and iterators in Python is with a for loop I could also have said like for you know", "tokens": [15409, 3391, 293, 17138, 3391, 294, 15329, 307, 365, 257, 337, 6367, 286, 727, 611, 362, 848, 411, 337, 291, 458], "temperature": 0.0, "avg_logprob": -0.28683560827504034, "compression_ratio": 1.6111111111111112, "no_speech_prob": 2.6425734631629894e-06}, {"id": 387, "seek": 188086, "start": 1902.5, "end": 1905.2199999999998, "text": " X mini-batch comma Y mini-batch in", "tokens": [1783, 8382, 12, 65, 852, 22117, 398, 8382, 12, 65, 852, 294], "temperature": 0.0, "avg_logprob": -0.28683560827504034, "compression_ratio": 1.6111111111111112, "no_speech_prob": 2.6425734631629894e-06}, {"id": 388, "seek": 188086, "start": 1905.86, "end": 1907.4599999999998, "text": " data loader", "tokens": [1412, 3677, 260], "temperature": 0.0, "avg_logprob": -0.28683560827504034, "compression_ratio": 1.6111111111111112, "no_speech_prob": 2.6425734631629894e-06}, {"id": 389, "seek": 190746, "start": 1907.46, "end": 1911.94, "text": " And then like do something right so when you do that it's actually behind the scenes", "tokens": [400, 550, 411, 360, 746, 558, 370, 562, 291, 360, 300, 309, 311, 767, 2261, 264, 8026], "temperature": 0.0, "avg_logprob": -0.2645846503121512, "compression_ratio": 1.4293478260869565, "no_speech_prob": 4.8604238145344425e-06}, {"id": 390, "seek": 190746, "start": 1911.94, "end": 1918.02, "text": " It's basically syntactic sugar for calling next lots of times okay, so this is all standard", "tokens": [467, 311, 1936, 23980, 19892, 5076, 337, 5141, 958, 3195, 295, 1413, 1392, 11, 370, 341, 307, 439, 3832], "temperature": 0.0, "avg_logprob": -0.2645846503121512, "compression_ratio": 1.4293478260869565, "no_speech_prob": 4.8604238145344425e-06}, {"id": 391, "seek": 190746, "start": 1918.74, "end": 1920.74, "text": " Python stuff", "tokens": [15329, 1507], "temperature": 0.0, "avg_logprob": -0.2645846503121512, "compression_ratio": 1.4293478260869565, "no_speech_prob": 4.8604238145344425e-06}, {"id": 392, "seek": 190746, "start": 1921.14, "end": 1923.14, "text": " So that returns", "tokens": [407, 300, 11247], "temperature": 0.0, "avg_logprob": -0.2645846503121512, "compression_ratio": 1.4293478260869565, "no_speech_prob": 4.8604238145344425e-06}, {"id": 393, "seek": 190746, "start": 1926.82, "end": 1930.26, "text": " A tensor of size 64", "tokens": [316, 40863, 295, 2744, 12145], "temperature": 0.0, "avg_logprob": -0.2645846503121512, "compression_ratio": 1.4293478260869565, "no_speech_prob": 4.8604238145344425e-06}, {"id": 394, "seek": 190746, "start": 1930.74, "end": 1934.3400000000001, "text": " by 7 8 4 as we would expect right the", "tokens": [538, 1614, 1649, 1017, 382, 321, 576, 2066, 558, 264], "temperature": 0.0, "avg_logprob": -0.2645846503121512, "compression_ratio": 1.4293478260869565, "no_speech_prob": 4.8604238145344425e-06}, {"id": 395, "seek": 193434, "start": 1934.34, "end": 1940.36, "text": " The fast AI library we used defaults to a mini-batch size of 64. That's why it's that long", "tokens": [440, 2370, 7318, 6405, 321, 1143, 7576, 82, 281, 257, 8382, 12, 65, 852, 2744, 295, 12145, 13, 663, 311, 983, 309, 311, 300, 938], "temperature": 0.0, "avg_logprob": -0.13101203390892516, "compression_ratio": 1.5316455696202531, "no_speech_prob": 2.642574827405042e-06}, {"id": 396, "seek": 193434, "start": 1941.3, "end": 1947.1599999999999, "text": " These are all of the background zero pixels, but they're not actually zero in this case. Why aren't they zero?", "tokens": [1981, 366, 439, 295, 264, 3678, 4018, 18668, 11, 457, 436, 434, 406, 767, 4018, 294, 341, 1389, 13, 1545, 3212, 380, 436, 4018, 30], "temperature": 0.0, "avg_logprob": -0.13101203390892516, "compression_ratio": 1.5316455696202531, "no_speech_prob": 2.642574827405042e-06}, {"id": 397, "seek": 193434, "start": 1948.82, "end": 1953.4199999999998, "text": " Yeah, they're normalized exactly right so we subtracted the mean divided by standard deviation right", "tokens": [865, 11, 436, 434, 48704, 2293, 558, 370, 321, 16390, 292, 264, 914, 6666, 538, 3832, 25163, 558], "temperature": 0.0, "avg_logprob": -0.13101203390892516, "compression_ratio": 1.5316455696202531, "no_speech_prob": 2.642574827405042e-06}, {"id": 398, "seek": 193434, "start": 1956.1399999999999, "end": 1960.62, "text": " So there there it is so now what we want to do is we want to", "tokens": [407, 456, 456, 309, 307, 370, 586, 437, 321, 528, 281, 360, 307, 321, 528, 281], "temperature": 0.0, "avg_logprob": -0.13101203390892516, "compression_ratio": 1.5316455696202531, "no_speech_prob": 2.642574827405042e-06}, {"id": 399, "seek": 196062, "start": 1960.62, "end": 1964.1799999999998, "text": " Pass that into our", "tokens": [10319, 300, 666, 527], "temperature": 0.0, "avg_logprob": -0.2248361015319824, "compression_ratio": 1.5944700460829493, "no_speech_prob": 3.0415860692301067e-06}, {"id": 400, "seek": 196062, "start": 1965.06, "end": 1966.9799999999998, "text": " our logistic regression", "tokens": [527, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.2248361015319824, "compression_ratio": 1.5944700460829493, "no_speech_prob": 3.0415860692301067e-06}, {"id": 401, "seek": 196062, "start": 1966.9799999999998, "end": 1968.9799999999998, "text": " So what we might do is we'll go", "tokens": [407, 437, 321, 1062, 360, 307, 321, 603, 352], "temperature": 0.0, "avg_logprob": -0.2248361015319824, "compression_ratio": 1.5944700460829493, "no_speech_prob": 3.0415860692301067e-06}, {"id": 402, "seek": 196062, "start": 1970.5, "end": 1975.62, "text": " Variable XMV equals variable okay, I can take my x mini-batch I", "tokens": [32511, 712, 1783, 44, 53, 6915, 7006, 1392, 11, 286, 393, 747, 452, 2031, 8382, 12, 65, 852, 286], "temperature": 0.0, "avg_logprob": -0.2248361015319824, "compression_ratio": 1.5944700460829493, "no_speech_prob": 3.0415860692301067e-06}, {"id": 403, "seek": 196062, "start": 1976.2199999999998, "end": 1979.1799999999998, "text": " can move it onto the GPU because remember my", "tokens": [393, 1286, 309, 3911, 264, 18407, 570, 1604, 452], "temperature": 0.0, "avg_logprob": -0.2248361015319824, "compression_ratio": 1.5944700460829493, "no_speech_prob": 3.0415860692301067e-06}, {"id": 404, "seek": 196062, "start": 1979.8999999999999, "end": 1985.02, "text": " Net to object is on the GPU so our data for it also has to be on the GPU", "tokens": [6188, 281, 2657, 307, 322, 264, 18407, 370, 527, 1412, 337, 309, 611, 575, 281, 312, 322, 264, 18407], "temperature": 0.0, "avg_logprob": -0.2248361015319824, "compression_ratio": 1.5944700460829493, "no_speech_prob": 3.0415860692301067e-06}, {"id": 405, "seek": 198502, "start": 1985.02, "end": 1990.42, "text": " And then the second thing I do is I have to wrap it in variable, so what does variable do?", "tokens": [400, 550, 264, 1150, 551, 286, 360, 307, 286, 362, 281, 7019, 309, 294, 7006, 11, 370, 437, 775, 7006, 360, 30], "temperature": 0.0, "avg_logprob": -0.22982306480407716, "compression_ratio": 1.6411483253588517, "no_speech_prob": 5.285511406327714e-07}, {"id": 406, "seek": 198502, "start": 1991.22, "end": 1995.06, "text": " This is how we get for free automatic differentiation", "tokens": [639, 307, 577, 321, 483, 337, 1737, 12509, 38902], "temperature": 0.0, "avg_logprob": -0.22982306480407716, "compression_ratio": 1.6411483253588517, "no_speech_prob": 5.285511406327714e-07}, {"id": 407, "seek": 198502, "start": 1996.26, "end": 1999.1, "text": " Pi torch can automatically differentiate", "tokens": [17741, 27822, 393, 6772, 23203], "temperature": 0.0, "avg_logprob": -0.22982306480407716, "compression_ratio": 1.6411483253588517, "no_speech_prob": 5.285511406327714e-07}, {"id": 408, "seek": 198502, "start": 1999.94, "end": 2002.54, "text": " You know pretty much anything right any tensor", "tokens": [509, 458, 1238, 709, 1340, 558, 604, 40863], "temperature": 0.0, "avg_logprob": -0.22982306480407716, "compression_ratio": 1.6411483253588517, "no_speech_prob": 5.285511406327714e-07}, {"id": 409, "seek": 198502, "start": 2003.18, "end": 2005.42, "text": " But to do so takes memory and time", "tokens": [583, 281, 360, 370, 2516, 4675, 293, 565], "temperature": 0.0, "avg_logprob": -0.22982306480407716, "compression_ratio": 1.6411483253588517, "no_speech_prob": 5.285511406327714e-07}, {"id": 410, "seek": 198502, "start": 2006.02, "end": 2010.5, "text": " So it's not going to always keep track like to do automatic differentiation", "tokens": [407, 309, 311, 406, 516, 281, 1009, 1066, 2837, 411, 281, 360, 12509, 38902], "temperature": 0.0, "avg_logprob": -0.22982306480407716, "compression_ratio": 1.6411483253588517, "no_speech_prob": 5.285511406327714e-07}, {"id": 411, "seek": 201050, "start": 2010.5, "end": 2015.38, "text": " It has to keep track of exactly how something was calculated we added these things together", "tokens": [467, 575, 281, 1066, 2837, 295, 2293, 577, 746, 390, 15598, 321, 3869, 613, 721, 1214], "temperature": 0.0, "avg_logprob": -0.13597932912535587, "compression_ratio": 1.8282828282828283, "no_speech_prob": 9.57080260377552e-07}, {"id": 412, "seek": 201050, "start": 2015.62, "end": 2020.86, "text": " We multiplied it by that we then took the sign blah blah blah right you have to know all of the steps", "tokens": [492, 17207, 309, 538, 300, 321, 550, 1890, 264, 1465, 12288, 12288, 12288, 558, 291, 362, 281, 458, 439, 295, 264, 4439], "temperature": 0.0, "avg_logprob": -0.13597932912535587, "compression_ratio": 1.8282828282828283, "no_speech_prob": 9.57080260377552e-07}, {"id": 413, "seek": 201050, "start": 2021.22, "end": 2025.1, "text": " Because then to do the automatic differentiation it has to", "tokens": [1436, 550, 281, 360, 264, 12509, 38902, 309, 575, 281], "temperature": 0.0, "avg_logprob": -0.13597932912535587, "compression_ratio": 1.8282828282828283, "no_speech_prob": 9.57080260377552e-07}, {"id": 414, "seek": 201050, "start": 2025.74, "end": 2032.2, "text": " Take the derivative of each step using the chain rule multiply them all together right so that's slow and memory intensive", "tokens": [3664, 264, 13760, 295, 1184, 1823, 1228, 264, 5021, 4978, 12972, 552, 439, 1214, 558, 370, 300, 311, 2964, 293, 4675, 18957], "temperature": 0.0, "avg_logprob": -0.13597932912535587, "compression_ratio": 1.8282828282828283, "no_speech_prob": 9.57080260377552e-07}, {"id": 415, "seek": 201050, "start": 2032.2, "end": 2036.08, "text": " So we have to opt in to saying like okay this particular thing", "tokens": [407, 321, 362, 281, 2427, 294, 281, 1566, 411, 1392, 341, 1729, 551], "temperature": 0.0, "avg_logprob": -0.13597932912535587, "compression_ratio": 1.8282828282828283, "no_speech_prob": 9.57080260377552e-07}, {"id": 416, "seek": 203608, "start": 2036.08, "end": 2040.36, "text": " We're going to be taking the derivative of later, so please keep track of all of those operations for us", "tokens": [492, 434, 516, 281, 312, 1940, 264, 13760, 295, 1780, 11, 370, 1767, 1066, 2837, 295, 439, 295, 729, 7705, 337, 505], "temperature": 0.0, "avg_logprob": -0.1665501868587801, "compression_ratio": 1.6291079812206573, "no_speech_prob": 1.5294083368644351e-06}, {"id": 417, "seek": 203608, "start": 2040.36, "end": 2046.0, "text": " And so the way we opt in is by wrapping a tensor in a variable right so", "tokens": [400, 370, 264, 636, 321, 2427, 294, 307, 538, 21993, 257, 40863, 294, 257, 7006, 558, 370], "temperature": 0.0, "avg_logprob": -0.1665501868587801, "compression_ratio": 1.6291079812206573, "no_speech_prob": 1.5294083368644351e-06}, {"id": 418, "seek": 203608, "start": 2048.14, "end": 2050.14, "text": " That's how we do it and", "tokens": [663, 311, 577, 321, 360, 309, 293], "temperature": 0.0, "avg_logprob": -0.1665501868587801, "compression_ratio": 1.6291079812206573, "no_speech_prob": 1.5294083368644351e-06}, {"id": 419, "seek": 203608, "start": 2050.22, "end": 2056.5, "text": " You'll see that it looks almost exactly like a tensor, but it now says variable containing", "tokens": [509, 603, 536, 300, 309, 1542, 1920, 2293, 411, 257, 40863, 11, 457, 309, 586, 1619, 7006, 19273], "temperature": 0.0, "avg_logprob": -0.1665501868587801, "compression_ratio": 1.6291079812206573, "no_speech_prob": 1.5294083368644351e-06}, {"id": 420, "seek": 203608, "start": 2057.02, "end": 2061.86, "text": " This tensor right so in pi torch a variable has exactly", "tokens": [639, 40863, 558, 370, 294, 3895, 27822, 257, 7006, 575, 2293], "temperature": 0.0, "avg_logprob": -0.1665501868587801, "compression_ratio": 1.6291079812206573, "no_speech_prob": 1.5294083368644351e-06}, {"id": 421, "seek": 206186, "start": 2061.86, "end": 2067.86, "text": " Identical API to a tensor or actually more specifically a superset of the API of a tensor", "tokens": [25905, 804, 9362, 281, 257, 40863, 420, 767, 544, 4682, 257, 37906, 302, 295, 264, 9362, 295, 257, 40863], "temperature": 0.0, "avg_logprob": -0.17741546911351821, "compression_ratio": 1.5868263473053892, "no_speech_prob": 9.422420248483832e-07}, {"id": 422, "seek": 206186, "start": 2068.6200000000003, "end": 2070.6200000000003, "text": " Anything we can do to a tensor we can do to a variable", "tokens": [11998, 321, 393, 360, 281, 257, 40863, 321, 393, 360, 281, 257, 7006], "temperature": 0.0, "avg_logprob": -0.17741546911351821, "compression_ratio": 1.5868263473053892, "no_speech_prob": 9.422420248483832e-07}, {"id": 423, "seek": 206186, "start": 2071.98, "end": 2079.98, "text": " But it's going to keep track of exactly what we did so we can later on take the derivative okay, so we can now pass that", "tokens": [583, 309, 311, 516, 281, 1066, 2837, 295, 2293, 437, 321, 630, 370, 321, 393, 1780, 322, 747, 264, 13760, 1392, 11, 370, 321, 393, 586, 1320, 300], "temperature": 0.0, "avg_logprob": -0.17741546911351821, "compression_ratio": 1.5868263473053892, "no_speech_prob": 9.422420248483832e-07}, {"id": 424, "seek": 207998, "start": 2079.98, "end": 2085.98, "text": " Into our", "tokens": [23373, 527], "temperature": 0.0, "avg_logprob": -0.2111795121344967, "compression_ratio": 1.5142857142857142, "no_speech_prob": 9.721526339490083e-07}, {"id": 425, "seek": 207998, "start": 2087.5, "end": 2094.7, "text": " Net to object remember I said you can treat this as if it's a function right so notice. We're not calling", "tokens": [6188, 281, 2657, 1604, 286, 848, 291, 393, 2387, 341, 382, 498, 309, 311, 257, 2445, 558, 370, 3449, 13, 492, 434, 406, 5141], "temperature": 0.0, "avg_logprob": -0.2111795121344967, "compression_ratio": 1.5142857142857142, "no_speech_prob": 9.721526339490083e-07}, {"id": 426, "seek": 207998, "start": 2095.62, "end": 2097.38, "text": " forward", "tokens": [2128], "temperature": 0.0, "avg_logprob": -0.2111795121344967, "compression_ratio": 1.5142857142857142, "no_speech_prob": 9.721526339490083e-07}, {"id": 427, "seek": 207998, "start": 2097.38, "end": 2099.38, "text": " We're just treating it as a function and", "tokens": [492, 434, 445, 15083, 309, 382, 257, 2445, 293], "temperature": 0.0, "avg_logprob": -0.2111795121344967, "compression_ratio": 1.5142857142857142, "no_speech_prob": 9.721526339490083e-07}, {"id": 428, "seek": 209938, "start": 2099.38, "end": 2108.94, "text": " Then remember we took the log so to undo that I'm taking the X and that will give me my probabilities okay", "tokens": [1396, 1604, 321, 1890, 264, 3565, 370, 281, 23779, 300, 286, 478, 1940, 264, 1783, 293, 300, 486, 976, 385, 452, 33783, 1392], "temperature": 0.0, "avg_logprob": -0.23420583551580257, "compression_ratio": 1.4183006535947713, "no_speech_prob": 1.1365606269464479e-06}, {"id": 429, "seek": 209938, "start": 2111.1, "end": 2114.06, "text": " So there's my probabilities and it's got", "tokens": [407, 456, 311, 452, 33783, 293, 309, 311, 658], "temperature": 0.0, "avg_logprob": -0.23420583551580257, "compression_ratio": 1.4183006535947713, "no_speech_prob": 1.1365606269464479e-06}, {"id": 430, "seek": 209938, "start": 2119.1, "end": 2123.06, "text": " Return something of size 64 by 10 so for each image in the mini batch", "tokens": [24350, 746, 295, 2744, 12145, 538, 1266, 370, 337, 1184, 3256, 294, 264, 8382, 15245], "temperature": 0.0, "avg_logprob": -0.23420583551580257, "compression_ratio": 1.4183006535947713, "no_speech_prob": 1.1365606269464479e-06}, {"id": 431, "seek": 212306, "start": 2123.06, "end": 2129.62, "text": " We've got 10 probabilities, and you'll see most probabilities are pretty close to zero", "tokens": [492, 600, 658, 1266, 33783, 11, 293, 291, 603, 536, 881, 33783, 366, 1238, 1998, 281, 4018], "temperature": 0.0, "avg_logprob": -0.1723487010368934, "compression_ratio": 1.6812227074235808, "no_speech_prob": 3.905464836861938e-06}, {"id": 432, "seek": 212306, "start": 2130.54, "end": 2133.02, "text": " Right and a few of them are quite a bit bigger", "tokens": [1779, 293, 257, 1326, 295, 552, 366, 1596, 257, 857, 3801], "temperature": 0.0, "avg_logprob": -0.1723487010368934, "compression_ratio": 1.6812227074235808, "no_speech_prob": 3.905464836861938e-06}, {"id": 433, "seek": 212306, "start": 2133.46, "end": 2138.14, "text": " Which is exactly what we do we hope right is that it's like okay?", "tokens": [3013, 307, 2293, 437, 321, 360, 321, 1454, 558, 307, 300, 309, 311, 411, 1392, 30], "temperature": 0.0, "avg_logprob": -0.1723487010368934, "compression_ratio": 1.6812227074235808, "no_speech_prob": 3.905464836861938e-06}, {"id": 434, "seek": 212306, "start": 2138.14, "end": 2142.7799999999997, "text": " It's not a zero is not a one is not a two it is a three is not a four is not a five and so forth", "tokens": [467, 311, 406, 257, 4018, 307, 406, 257, 472, 307, 406, 257, 732, 309, 307, 257, 1045, 307, 406, 257, 1451, 307, 406, 257, 1732, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.1723487010368934, "compression_ratio": 1.6812227074235808, "no_speech_prob": 3.905464836861938e-06}, {"id": 435, "seek": 212306, "start": 2143.7, "end": 2147.18, "text": " So maybe this would be a bit easier to read if we just grab like the first three of them", "tokens": [407, 1310, 341, 576, 312, 257, 857, 3571, 281, 1401, 498, 321, 445, 4444, 411, 264, 700, 1045, 295, 552], "temperature": 0.0, "avg_logprob": -0.1723487010368934, "compression_ratio": 1.6812227074235808, "no_speech_prob": 3.905464836861938e-06}, {"id": 436, "seek": 214718, "start": 2147.18, "end": 2156.12, "text": " Okay, so it's like 10 to the neg 3 10 to the neg 8 2 5 5 4 ok and then suddenly here's one", "tokens": [1033, 11, 370, 309, 311, 411, 1266, 281, 264, 2485, 805, 1266, 281, 264, 2485, 1649, 568, 1025, 1025, 1017, 3133, 293, 550, 5800, 510, 311, 472], "temperature": 0.0, "avg_logprob": -0.23187132804624497, "compression_ratio": 1.5549738219895288, "no_speech_prob": 8.139642886817455e-06}, {"id": 437, "seek": 214718, "start": 2156.12, "end": 2158.12, "text": " Which is 10 to neg 1 right?", "tokens": [3013, 307, 1266, 281, 2485, 502, 558, 30], "temperature": 0.0, "avg_logprob": -0.23187132804624497, "compression_ratio": 1.5549738219895288, "no_speech_prob": 8.139642886817455e-06}, {"id": 438, "seek": 214718, "start": 2158.66, "end": 2160.66, "text": " So you can kind of see what it's trying to", "tokens": [407, 291, 393, 733, 295, 536, 437, 309, 311, 1382, 281], "temperature": 0.0, "avg_logprob": -0.23187132804624497, "compression_ratio": 1.5549738219895288, "no_speech_prob": 8.139642886817455e-06}, {"id": 439, "seek": 214718, "start": 2161.18, "end": 2163.18, "text": " What it's trying to do here", "tokens": [708, 309, 311, 1382, 281, 360, 510], "temperature": 0.0, "avg_logprob": -0.23187132804624497, "compression_ratio": 1.5549738219895288, "no_speech_prob": 8.139642886817455e-06}, {"id": 440, "seek": 214718, "start": 2163.2599999999998, "end": 2170.4199999999996, "text": " I mean we could call like net to dot forward and it'll do exactly the same thing", "tokens": [286, 914, 321, 727, 818, 411, 2533, 281, 5893, 2128, 293, 309, 603, 360, 2293, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.23187132804624497, "compression_ratio": 1.5549738219895288, "no_speech_prob": 8.139642886817455e-06}, {"id": 441, "seek": 214718, "start": 2171.1, "end": 2173.1, "text": " Right, but that's not how?", "tokens": [1779, 11, 457, 300, 311, 406, 577, 30], "temperature": 0.0, "avg_logprob": -0.23187132804624497, "compression_ratio": 1.5549738219895288, "no_speech_prob": 8.139642886817455e-06}, {"id": 442, "seek": 217310, "start": 2173.1, "end": 2176.66, "text": " All of the pie torch mechanics actually work", "tokens": [1057, 295, 264, 1730, 27822, 12939, 767, 589], "temperature": 0.0, "avg_logprob": -0.19848091319455938, "compression_ratio": 1.9721115537848606, "no_speech_prob": 3.905458015651675e-06}, {"id": 443, "seek": 217310, "start": 2176.66, "end": 2182.66, "text": " It's actually they actually call it as if it's a function right and so this is actually a really important idea", "tokens": [467, 311, 767, 436, 767, 818, 309, 382, 498, 309, 311, 257, 2445, 558, 293, 370, 341, 307, 767, 257, 534, 1021, 1558], "temperature": 0.0, "avg_logprob": -0.19848091319455938, "compression_ratio": 1.9721115537848606, "no_speech_prob": 3.905458015651675e-06}, {"id": 444, "seek": 217310, "start": 2183.8199999999997, "end": 2190.52, "text": " Because it means that when we define our own architectures or whatever anywhere that you would put in a function", "tokens": [1436, 309, 1355, 300, 562, 321, 6964, 527, 1065, 6331, 1303, 420, 2035, 4992, 300, 291, 576, 829, 294, 257, 2445], "temperature": 0.0, "avg_logprob": -0.19848091319455938, "compression_ratio": 1.9721115537848606, "no_speech_prob": 3.905458015651675e-06}, {"id": 445, "seek": 217310, "start": 2190.64, "end": 2194.56, "text": " You could put in a layer anyway you put in a layer you can put in a neural net", "tokens": [509, 727, 829, 294, 257, 4583, 4033, 291, 829, 294, 257, 4583, 291, 393, 829, 294, 257, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.19848091319455938, "compression_ratio": 1.9721115537848606, "no_speech_prob": 3.905458015651675e-06}, {"id": 446, "seek": 217310, "start": 2194.56, "end": 2199.08, "text": " Anyway, you put in a neural net you can put in a function because as far as pie torch is concerned", "tokens": [5684, 11, 291, 829, 294, 257, 18161, 2533, 291, 393, 829, 294, 257, 2445, 570, 382, 1400, 382, 1730, 27822, 307, 5922], "temperature": 0.0, "avg_logprob": -0.19848091319455938, "compression_ratio": 1.9721115537848606, "no_speech_prob": 3.905458015651675e-06}, {"id": 447, "seek": 217310, "start": 2199.1, "end": 2201.1, "text": " They're all just things that it's going to call", "tokens": [814, 434, 439, 445, 721, 300, 309, 311, 516, 281, 818], "temperature": 0.0, "avg_logprob": -0.19848091319455938, "compression_ratio": 1.9721115537848606, "no_speech_prob": 3.905458015651675e-06}, {"id": 448, "seek": 220110, "start": 2201.1, "end": 2207.86, "text": " Just like as if their functions, so they're all like interchangeable, and this is really important because that's how we create", "tokens": [1449, 411, 382, 498, 641, 6828, 11, 370, 436, 434, 439, 411, 30358, 712, 11, 293, 341, 307, 534, 1021, 570, 300, 311, 577, 321, 1884], "temperature": 0.0, "avg_logprob": -0.21505041264775973, "compression_ratio": 1.489795918367347, "no_speech_prob": 7.934477253002115e-07}, {"id": 449, "seek": 220110, "start": 2208.86, "end": 2215.3399999999997, "text": " Really good neural nets is by mixing and matching lots of pieces and putting them all together right let me give an example", "tokens": [4083, 665, 18161, 36170, 307, 538, 11983, 293, 14324, 3195, 295, 3755, 293, 3372, 552, 439, 1214, 558, 718, 385, 976, 364, 1365], "temperature": 0.0, "avg_logprob": -0.21505041264775973, "compression_ratio": 1.489795918367347, "no_speech_prob": 7.934477253002115e-07}, {"id": 450, "seek": 220110, "start": 2218.22, "end": 2220.22, "text": " Here is my", "tokens": [1692, 307, 452], "temperature": 0.0, "avg_logprob": -0.21505041264775973, "compression_ratio": 1.489795918367347, "no_speech_prob": 7.934477253002115e-07}, {"id": 451, "seek": 220110, "start": 2222.18, "end": 2224.58, "text": " Logistic regression which got", "tokens": [10824, 3142, 24590, 597, 658], "temperature": 0.0, "avg_logprob": -0.21505041264775973, "compression_ratio": 1.489795918367347, "no_speech_prob": 7.934477253002115e-07}, {"id": 452, "seek": 222458, "start": 2224.58, "end": 2230.9, "text": " 91 and a bit percent accuracy I'm now going to turn it", "tokens": [31064, 293, 257, 857, 3043, 14170, 286, 478, 586, 516, 281, 1261, 309], "temperature": 0.0, "avg_logprob": -0.177754706943158, "compression_ratio": 1.733644859813084, "no_speech_prob": 1.1189388260390842e-06}, {"id": 453, "seek": 222458, "start": 2231.9, "end": 2237.14, "text": " Into a neural network with one hidden layer all right and the way I'm going to do that is I'm going to create", "tokens": [23373, 257, 18161, 3209, 365, 472, 7633, 4583, 439, 558, 293, 264, 636, 286, 478, 516, 281, 360, 300, 307, 286, 478, 516, 281, 1884], "temperature": 0.0, "avg_logprob": -0.177754706943158, "compression_ratio": 1.733644859813084, "no_speech_prob": 1.1189388260390842e-06}, {"id": 454, "seek": 222458, "start": 2237.9, "end": 2239.9, "text": " one more layer", "tokens": [472, 544, 4583], "temperature": 0.0, "avg_logprob": -0.177754706943158, "compression_ratio": 1.733644859813084, "no_speech_prob": 1.1189388260390842e-06}, {"id": 455, "seek": 222458, "start": 2240.38, "end": 2247.14, "text": " I'm going to change this so it spits out a hundred rather than ten which means this one input is going to be", "tokens": [286, 478, 516, 281, 1319, 341, 370, 309, 637, 1208, 484, 257, 3262, 2831, 813, 2064, 597, 1355, 341, 472, 4846, 307, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.177754706943158, "compression_ratio": 1.733644859813084, "no_speech_prob": 1.1189388260390842e-06}, {"id": 456, "seek": 222458, "start": 2247.66, "end": 2249.66, "text": " 100 rather than ten", "tokens": [2319, 2831, 813, 2064], "temperature": 0.0, "avg_logprob": -0.177754706943158, "compression_ratio": 1.733644859813084, "no_speech_prob": 1.1189388260390842e-06}, {"id": 457, "seek": 224966, "start": 2249.66, "end": 2255.3799999999997, "text": " Now this as it is can't possibly make things any better at all yet", "tokens": [823, 341, 382, 309, 307, 393, 380, 6264, 652, 721, 604, 1101, 412, 439, 1939], "temperature": 0.0, "avg_logprob": -0.24589526399653008, "compression_ratio": 1.74235807860262, "no_speech_prob": 3.6119627111474983e-06}, {"id": 458, "seek": 224966, "start": 2256.14, "end": 2259.1, "text": " Why is this definitely not going to be better than what I had before?", "tokens": [1545, 307, 341, 2138, 406, 516, 281, 312, 1101, 813, 437, 286, 632, 949, 30], "temperature": 0.0, "avg_logprob": -0.24589526399653008, "compression_ratio": 1.74235807860262, "no_speech_prob": 3.6119627111474983e-06}, {"id": 459, "seek": 224966, "start": 2260.18, "end": 2262.54, "text": " Yeah, can somebody pass the yeah?", "tokens": [865, 11, 393, 2618, 1320, 264, 1338, 30], "temperature": 0.0, "avg_logprob": -0.24589526399653008, "compression_ratio": 1.74235807860262, "no_speech_prob": 3.6119627111474983e-06}, {"id": 460, "seek": 224966, "start": 2264.3399999999997, "end": 2270.2, "text": " Because you've got a combination of two linear layers, which is just the same as one linear with different parameters exactly right", "tokens": [1436, 291, 600, 658, 257, 6562, 295, 732, 8213, 7914, 11, 597, 307, 445, 264, 912, 382, 472, 8213, 365, 819, 9834, 2293, 558], "temperature": 0.0, "avg_logprob": -0.24589526399653008, "compression_ratio": 1.74235807860262, "no_speech_prob": 3.6119627111474983e-06}, {"id": 461, "seek": 224966, "start": 2270.2, "end": 2274.94, "text": " So we've got two linear layers, which is just a linear layer right so to make things interesting", "tokens": [407, 321, 600, 658, 732, 8213, 7914, 11, 597, 307, 445, 257, 8213, 4583, 558, 370, 281, 652, 721, 1880], "temperature": 0.0, "avg_logprob": -0.24589526399653008, "compression_ratio": 1.74235807860262, "no_speech_prob": 3.6119627111474983e-06}, {"id": 462, "seek": 227494, "start": 2274.94, "end": 2280.94, "text": " I'm going to replace all of the negatives from the first layer with zeros", "tokens": [286, 478, 516, 281, 7406, 439, 295, 264, 40019, 490, 264, 700, 4583, 365, 35193], "temperature": 0.0, "avg_logprob": -0.1559997952502707, "compression_ratio": 1.841121495327103, "no_speech_prob": 2.026135007326957e-06}, {"id": 463, "seek": 227494, "start": 2281.66, "end": 2287.86, "text": " Because that's a nonlinear transformation and so that nonlinear transformation is called a rectified linear unit", "tokens": [1436, 300, 311, 257, 2107, 28263, 9887, 293, 370, 300, 2107, 28263, 9887, 307, 1219, 257, 11048, 2587, 8213, 4985], "temperature": 0.0, "avg_logprob": -0.1559997952502707, "compression_ratio": 1.841121495327103, "no_speech_prob": 2.026135007326957e-06}, {"id": 464, "seek": 227494, "start": 2290.42, "end": 2298.38, "text": " Okay, so n n dot sequential simply is going to call each of these layers in turn for each mini-batch right so do a linear layer", "tokens": [1033, 11, 370, 297, 297, 5893, 42881, 2935, 307, 516, 281, 818, 1184, 295, 613, 7914, 294, 1261, 337, 1184, 8382, 12, 65, 852, 558, 370, 360, 257, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.1559997952502707, "compression_ratio": 1.841121495327103, "no_speech_prob": 2.026135007326957e-06}, {"id": 465, "seek": 227494, "start": 2299.02, "end": 2303.18, "text": " Replace all of the negatives with zero do another linear layer and do a softmax", "tokens": [1300, 6742, 439, 295, 264, 40019, 365, 4018, 360, 1071, 8213, 4583, 293, 360, 257, 2787, 41167], "temperature": 0.0, "avg_logprob": -0.1559997952502707, "compression_ratio": 1.841121495327103, "no_speech_prob": 2.026135007326957e-06}, {"id": 466, "seek": 230318, "start": 2303.18, "end": 2305.18, "text": " This is now a neural network", "tokens": [639, 307, 586, 257, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.22037109771332183, "compression_ratio": 1.4630541871921183, "no_speech_prob": 1.018807438413205e-06}, {"id": 467, "seek": 230318, "start": 2306.06, "end": 2308.06, "text": " with one hidden layer and", "tokens": [365, 472, 7633, 4583, 293], "temperature": 0.0, "avg_logprob": -0.22037109771332183, "compression_ratio": 1.4630541871921183, "no_speech_prob": 1.018807438413205e-06}, {"id": 468, "seek": 230318, "start": 2308.5, "end": 2310.5, "text": " So let's try training that instead", "tokens": [407, 718, 311, 853, 3097, 300, 2602], "temperature": 0.0, "avg_logprob": -0.22037109771332183, "compression_ratio": 1.4630541871921183, "no_speech_prob": 1.018807438413205e-06}, {"id": 469, "seek": 230318, "start": 2315.14, "end": 2317.14, "text": " Yeah, accuracy is now going up to 96%", "tokens": [865, 11, 14170, 307, 586, 516, 493, 281, 24124, 4], "temperature": 0.0, "avg_logprob": -0.22037109771332183, "compression_ratio": 1.4630541871921183, "no_speech_prob": 1.018807438413205e-06}, {"id": 470, "seek": 230318, "start": 2318.06, "end": 2323.46, "text": " Okay, so the this is the idea is that the basic techniques. We're learning in this lesson", "tokens": [1033, 11, 370, 264, 341, 307, 264, 1558, 307, 300, 264, 3875, 7512, 13, 492, 434, 2539, 294, 341, 6898], "temperature": 0.0, "avg_logprob": -0.22037109771332183, "compression_ratio": 1.4630541871921183, "no_speech_prob": 1.018807438413205e-06}, {"id": 471, "seek": 230318, "start": 2324.06, "end": 2329.58, "text": " Like become powerful at the point where you start stacking them together, okay?", "tokens": [1743, 1813, 4005, 412, 264, 935, 689, 291, 722, 41376, 552, 1214, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.22037109771332183, "compression_ratio": 1.4630541871921183, "no_speech_prob": 1.018807438413205e-06}, {"id": 472, "seek": 232958, "start": 2329.58, "end": 2333.68, "text": " Can somebody pass the green box there and then there yes Daniel?", "tokens": [1664, 2618, 1320, 264, 3092, 2424, 456, 293, 550, 456, 2086, 8033, 30], "temperature": 0.0, "avg_logprob": -0.21533451285413516, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.1907655991526553e-06}, {"id": 473, "seek": 232958, "start": 2335.46, "end": 2339.98, "text": " Why did you pick a hundred no reason it was like easier to type an extra zero", "tokens": [1545, 630, 291, 1888, 257, 3262, 572, 1778, 309, 390, 411, 3571, 281, 2010, 364, 2857, 4018], "temperature": 0.0, "avg_logprob": -0.21533451285413516, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.1907655991526553e-06}, {"id": 474, "seek": 232958, "start": 2342.2599999999998, "end": 2344.2599999999998, "text": " Like this question of like how many", "tokens": [1743, 341, 1168, 295, 411, 577, 867], "temperature": 0.0, "avg_logprob": -0.21533451285413516, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.1907655991526553e-06}, {"id": 475, "seek": 232958, "start": 2344.58, "end": 2349.7999999999997, "text": " Activations should I have in a neural network layer is kind of part of the the scale of a deep learning practitioner?", "tokens": [28550, 763, 820, 286, 362, 294, 257, 18161, 3209, 4583, 307, 733, 295, 644, 295, 264, 264, 4373, 295, 257, 2452, 2539, 32125, 30], "temperature": 0.0, "avg_logprob": -0.21533451285413516, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.1907655991526553e-06}, {"id": 476, "seek": 232958, "start": 2350.0, "end": 2353.06, "text": " We cover it in the deep learning course not in this course", "tokens": [492, 2060, 309, 294, 264, 2452, 2539, 1164, 406, 294, 341, 1164], "temperature": 0.0, "avg_logprob": -0.21533451285413516, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.1907655991526553e-06}, {"id": 477, "seek": 232958, "start": 2355.46, "end": 2358.14, "text": " When adding that additional I guess", "tokens": [1133, 5127, 300, 4497, 286, 2041], "temperature": 0.0, "avg_logprob": -0.21533451285413516, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.1907655991526553e-06}, {"id": 478, "seek": 235814, "start": 2358.14, "end": 2360.14, "text": " transformation", "tokens": [9887], "temperature": 0.0, "avg_logprob": -0.2872816595700708, "compression_ratio": 1.8204081632653062, "no_speech_prob": 5.955104370514164e-06}, {"id": 479, "seek": 235814, "start": 2360.66, "end": 2366.02, "text": " Additional layer additional layer this one here is called a nonlinear layer or an activation for you", "tokens": [44272, 4583, 4497, 4583, 341, 472, 510, 307, 1219, 257, 2107, 28263, 4583, 420, 364, 24433, 337, 291], "temperature": 0.0, "avg_logprob": -0.2872816595700708, "compression_ratio": 1.8204081632653062, "no_speech_prob": 5.955104370514164e-06}, {"id": 480, "seek": 235814, "start": 2366.02, "end": 2369.46, "text": " Are you said activation layer activation function or activation?", "tokens": [2014, 291, 848, 24433, 4583, 24433, 2445, 420, 24433, 30], "temperature": 0.0, "avg_logprob": -0.2872816595700708, "compression_ratio": 1.8204081632653062, "no_speech_prob": 5.955104370514164e-06}, {"id": 481, "seek": 235814, "start": 2372.62, "end": 2379.06, "text": " Does it matter that like if you would have done for example like two soft maxes or is that something you cannot do?", "tokens": [4402, 309, 1871, 300, 411, 498, 291, 576, 362, 1096, 337, 1365, 411, 732, 2787, 11469, 279, 420, 307, 300, 746, 291, 2644, 360, 30], "temperature": 0.0, "avg_logprob": -0.2872816595700708, "compression_ratio": 1.8204081632653062, "no_speech_prob": 5.955104370514164e-06}, {"id": 482, "seek": 235814, "start": 2379.2599999999998, "end": 2382.14, "text": " Like when I know you can absolutely use a soft max there", "tokens": [1743, 562, 286, 458, 291, 393, 3122, 764, 257, 2787, 11469, 456], "temperature": 0.0, "avg_logprob": -0.2872816595700708, "compression_ratio": 1.8204081632653062, "no_speech_prob": 5.955104370514164e-06}, {"id": 483, "seek": 238214, "start": 2382.14, "end": 2387.8599999999997, "text": " But it's probably not going to give you what you want and the reason why is that a soft max?", "tokens": [583, 309, 311, 1391, 406, 516, 281, 976, 291, 437, 291, 528, 293, 264, 1778, 983, 307, 300, 257, 2787, 11469, 30], "temperature": 0.0, "avg_logprob": -0.159209615176486, "compression_ratio": 1.8035714285714286, "no_speech_prob": 2.6425798296259018e-06}, {"id": 484, "seek": 238214, "start": 2388.22, "end": 2395.46, "text": " Tends to push most of its activations to zero and an activation just to be clear like I've had a lot of questions in deep", "tokens": [314, 2581, 281, 2944, 881, 295, 1080, 2430, 763, 281, 4018, 293, 364, 24433, 445, 281, 312, 1850, 411, 286, 600, 632, 257, 688, 295, 1651, 294, 2452], "temperature": 0.0, "avg_logprob": -0.159209615176486, "compression_ratio": 1.8035714285714286, "no_speech_prob": 2.6425798296259018e-06}, {"id": 485, "seek": 238214, "start": 2395.46, "end": 2404.14, "text": " Learning course about like what's an activation an activation is the value that is calculated in a layer right so this is an activation", "tokens": [15205, 1164, 466, 411, 437, 311, 364, 24433, 364, 24433, 307, 264, 2158, 300, 307, 15598, 294, 257, 4583, 558, 370, 341, 307, 364, 24433], "temperature": 0.0, "avg_logprob": -0.159209615176486, "compression_ratio": 1.8035714285714286, "no_speech_prob": 2.6425798296259018e-06}, {"id": 486, "seek": 238214, "start": 2404.94, "end": 2408.7, "text": " Right it's not a weight a weight is not an activation", "tokens": [1779, 309, 311, 406, 257, 3364, 257, 3364, 307, 406, 364, 24433], "temperature": 0.0, "avg_logprob": -0.159209615176486, "compression_ratio": 1.8035714285714286, "no_speech_prob": 2.6425798296259018e-06}, {"id": 487, "seek": 240870, "start": 2408.7, "end": 2415.7, "text": " It's the value that you calculate from a layer so soft max will tend to make most of its activations pretty close to zero", "tokens": [467, 311, 264, 2158, 300, 291, 8873, 490, 257, 4583, 370, 2787, 11469, 486, 3928, 281, 652, 881, 295, 1080, 2430, 763, 1238, 1998, 281, 4018], "temperature": 0.0, "avg_logprob": -0.1672547900158426, "compression_ratio": 1.6340425531914893, "no_speech_prob": 1.5534938029304612e-06}, {"id": 488, "seek": 240870, "start": 2415.7, "end": 2422.58, "text": " And that's the opposite of what you want you generally want your activations to be kind of as rich and diverse and", "tokens": [400, 300, 311, 264, 6182, 295, 437, 291, 528, 291, 5101, 528, 428, 2430, 763, 281, 312, 733, 295, 382, 4593, 293, 9521, 293], "temperature": 0.0, "avg_logprob": -0.1672547900158426, "compression_ratio": 1.6340425531914893, "no_speech_prob": 1.5534938029304612e-06}, {"id": 489, "seek": 240870, "start": 2423.02, "end": 2427.3399999999997, "text": " Used as possible so nothing to stop you doing it, but it probably won't work very well", "tokens": [43237, 382, 1944, 370, 1825, 281, 1590, 291, 884, 309, 11, 457, 309, 1391, 1582, 380, 589, 588, 731], "temperature": 0.0, "avg_logprob": -0.1672547900158426, "compression_ratio": 1.6340425531914893, "no_speech_prob": 1.5534938029304612e-06}, {"id": 490, "seek": 240870, "start": 2429.54, "end": 2431.02, "text": " Basically", "tokens": [8537], "temperature": 0.0, "avg_logprob": -0.1672547900158426, "compression_ratio": 1.6340425531914893, "no_speech_prob": 1.5534938029304612e-06}, {"id": 491, "seek": 240870, "start": 2431.02, "end": 2434.64, "text": " pretty much all of your layers will be followed by", "tokens": [1238, 709, 439, 295, 428, 7914, 486, 312, 6263, 538], "temperature": 0.0, "avg_logprob": -0.1672547900158426, "compression_ratio": 1.6340425531914893, "no_speech_prob": 1.5534938029304612e-06}, {"id": 492, "seek": 243464, "start": 2434.64, "end": 2439.8399999999997, "text": " Non by nonlinear activation functions that will nearly always be value", "tokens": [8774, 538, 2107, 28263, 24433, 6828, 300, 486, 6217, 1009, 312, 2158], "temperature": 0.0, "avg_logprob": -0.27972412109375, "compression_ratio": 1.521505376344086, "no_speech_prob": 6.962121460674098e-06}, {"id": 493, "seek": 243464, "start": 2440.56, "end": 2442.56, "text": " except for the last layer", "tokens": [3993, 337, 264, 1036, 4583], "temperature": 0.0, "avg_logprob": -0.27972412109375, "compression_ratio": 1.521505376344086, "no_speech_prob": 6.962121460674098e-06}, {"id": 494, "seek": 243464, "start": 2449.7599999999998, "end": 2451.7599999999998, "text": " Going two or three layers deep", "tokens": [10963, 732, 420, 1045, 7914, 2452], "temperature": 0.0, "avg_logprob": -0.27972412109375, "compression_ratio": 1.521505376344086, "no_speech_prob": 6.962121460674098e-06}, {"id": 495, "seek": 243464, "start": 2452.8399999999997, "end": 2457.52, "text": " Do you want to switch up these activation layers? No is it okay? Just to keep them? No, that's a great question", "tokens": [1144, 291, 528, 281, 3679, 493, 613, 24433, 7914, 30, 883, 307, 309, 1392, 30, 1449, 281, 1066, 552, 30, 883, 11, 300, 311, 257, 869, 1168], "temperature": 0.0, "avg_logprob": -0.27972412109375, "compression_ratio": 1.521505376344086, "no_speech_prob": 6.962121460674098e-06}, {"id": 496, "seek": 243464, "start": 2457.52, "end": 2459.52, "text": " So if I wanted to go deeper I", "tokens": [407, 498, 286, 1415, 281, 352, 7731, 286], "temperature": 0.0, "avg_logprob": -0.27972412109375, "compression_ratio": 1.521505376344086, "no_speech_prob": 6.962121460674098e-06}, {"id": 497, "seek": 243464, "start": 2459.96, "end": 2461.96, "text": " would just do", "tokens": [576, 445, 360], "temperature": 0.0, "avg_logprob": -0.27972412109375, "compression_ratio": 1.521505376344086, "no_speech_prob": 6.962121460674098e-06}, {"id": 498, "seek": 246196, "start": 2461.96, "end": 2465.9, "text": " That okay, that's a now to hidden layer network", "tokens": [663, 1392, 11, 300, 311, 257, 586, 281, 7633, 4583, 3209], "temperature": 0.0, "avg_logprob": -0.21444143567766463, "compression_ratio": 1.5108695652173914, "no_speech_prob": 2.6016000447270926e-06}, {"id": 499, "seek": 246196, "start": 2469.28, "end": 2476.7200000000003, "text": " So I think I'd heard you said that there are a couple of different activation functions like that rectified linear unit", "tokens": [407, 286, 519, 286, 1116, 2198, 291, 848, 300, 456, 366, 257, 1916, 295, 819, 24433, 6828, 411, 300, 11048, 2587, 8213, 4985], "temperature": 0.0, "avg_logprob": -0.21444143567766463, "compression_ratio": 1.5108695652173914, "no_speech_prob": 2.6016000447270926e-06}, {"id": 500, "seek": 246196, "start": 2477.36, "end": 2479.36, "text": " What are some examples and?", "tokens": [708, 366, 512, 5110, 293, 30], "temperature": 0.0, "avg_logprob": -0.21444143567766463, "compression_ratio": 1.5108695652173914, "no_speech_prob": 2.6016000447270926e-06}, {"id": 501, "seek": 246196, "start": 2480.08, "end": 2482.08, "text": " Why would you use?", "tokens": [1545, 576, 291, 764, 30], "temperature": 0.0, "avg_logprob": -0.21444143567766463, "compression_ratio": 1.5108695652173914, "no_speech_prob": 2.6016000447270926e-06}, {"id": 502, "seek": 246196, "start": 2482.2400000000002, "end": 2484.2400000000002, "text": " each yeah great question", "tokens": [1184, 1338, 869, 1168], "temperature": 0.0, "avg_logprob": -0.21444143567766463, "compression_ratio": 1.5108695652173914, "no_speech_prob": 2.6016000447270926e-06}, {"id": 503, "seek": 248424, "start": 2484.24, "end": 2491.12, "text": " So basically like as you add like more", "tokens": [407, 1936, 411, 382, 291, 909, 411, 544], "temperature": 0.0, "avg_logprob": -0.2165011373059503, "compression_ratio": 1.88, "no_speech_prob": 1.3496966175807756e-06}, {"id": 504, "seek": 248424, "start": 2492.0, "end": 2497.56, "text": " Linear layers you kind of got your input comes in and you put it through a linear layer", "tokens": [14670, 289, 7914, 291, 733, 295, 658, 428, 4846, 1487, 294, 293, 291, 829, 309, 807, 257, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.2165011373059503, "compression_ratio": 1.88, "no_speech_prob": 1.3496966175807756e-06}, {"id": 505, "seek": 248424, "start": 2497.56, "end": 2501.2, "text": " And then a nonlinear layer linear layer nonlinear layer", "tokens": [400, 550, 257, 2107, 28263, 4583, 8213, 4583, 2107, 28263, 4583], "temperature": 0.0, "avg_logprob": -0.2165011373059503, "compression_ratio": 1.88, "no_speech_prob": 1.3496966175807756e-06}, {"id": 506, "seek": 248424, "start": 2505.3599999999997, "end": 2510.9199999999996, "text": " Many linear layer and then the final nonlinear layer", "tokens": [5126, 8213, 4583, 293, 550, 264, 2572, 2107, 28263, 4583], "temperature": 0.0, "avg_logprob": -0.2165011373059503, "compression_ratio": 1.88, "no_speech_prob": 1.3496966175807756e-06}, {"id": 507, "seek": 251092, "start": 2510.92, "end": 2512.92, "text": " The", "tokens": [440], "temperature": 0.0, "avg_logprob": -0.22235190447639017, "compression_ratio": 1.6789473684210525, "no_speech_prob": 1.505690420344763e-06}, {"id": 508, "seek": 251092, "start": 2513.08, "end": 2516.2000000000003, "text": " Final nonlinear layer as we've discussed you know if it's a", "tokens": [13443, 2107, 28263, 4583, 382, 321, 600, 7152, 291, 458, 498, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.22235190447639017, "compression_ratio": 1.6789473684210525, "no_speech_prob": 1.505690420344763e-06}, {"id": 509, "seek": 251092, "start": 2517.64, "end": 2518.88, "text": " multi-category", "tokens": [4825, 12, 66, 48701], "temperature": 0.0, "avg_logprob": -0.22235190447639017, "compression_ratio": 1.6789473684210525, "no_speech_prob": 1.505690420344763e-06}, {"id": 510, "seek": 251092, "start": 2518.88, "end": 2523.56, "text": " Classification, but you only ever pick one of them you would use softmax", "tokens": [9471, 3774, 11, 457, 291, 787, 1562, 1888, 472, 295, 552, 291, 576, 764, 2787, 41167], "temperature": 0.0, "avg_logprob": -0.22235190447639017, "compression_ratio": 1.6789473684210525, "no_speech_prob": 1.505690420344763e-06}, {"id": 511, "seek": 251092, "start": 2524.76, "end": 2532.08, "text": " If it's a binary classification or a multi-label classification where you're predicting multiple things you would use sigmoid", "tokens": [759, 309, 311, 257, 17434, 21538, 420, 257, 4825, 12, 75, 18657, 21538, 689, 291, 434, 32884, 3866, 721, 291, 576, 764, 4556, 3280, 327], "temperature": 0.0, "avg_logprob": -0.22235190447639017, "compression_ratio": 1.6789473684210525, "no_speech_prob": 1.505690420344763e-06}, {"id": 512, "seek": 251092, "start": 2533.52, "end": 2535.52, "text": " If it's a regression", "tokens": [759, 309, 311, 257, 24590], "temperature": 0.0, "avg_logprob": -0.22235190447639017, "compression_ratio": 1.6789473684210525, "no_speech_prob": 1.505690420344763e-06}, {"id": 513, "seek": 251092, "start": 2536.2000000000003, "end": 2537.96, "text": " You would often have", "tokens": [509, 576, 2049, 362], "temperature": 0.0, "avg_logprob": -0.22235190447639017, "compression_ratio": 1.6789473684210525, "no_speech_prob": 1.505690420344763e-06}, {"id": 514, "seek": 253796, "start": 2537.96, "end": 2543.32, "text": " Nothing at all right although we learned in last night's dl course where sometimes you can use sigmoid there as well", "tokens": [6693, 412, 439, 558, 4878, 321, 3264, 294, 1036, 1818, 311, 37873, 1164, 689, 2171, 291, 393, 764, 4556, 3280, 327, 456, 382, 731], "temperature": 0.0, "avg_logprob": -0.21579933166503906, "compression_ratio": 1.5, "no_speech_prob": 1.154456640506396e-06}, {"id": 515, "seek": 253796, "start": 2543.8, "end": 2548.52, "text": " So they're basically the options main options for the final layer", "tokens": [407, 436, 434, 1936, 264, 3956, 2135, 3956, 337, 264, 2572, 4583], "temperature": 0.0, "avg_logprob": -0.21579933166503906, "compression_ratio": 1.5, "no_speech_prob": 1.154456640506396e-06}, {"id": 516, "seek": 253796, "start": 2549.96, "end": 2551.96, "text": " for the", "tokens": [337, 264], "temperature": 0.0, "avg_logprob": -0.21579933166503906, "compression_ratio": 1.5, "no_speech_prob": 1.154456640506396e-06}, {"id": 517, "seek": 253796, "start": 2552.36, "end": 2555.4, "text": " Hidden layers you pretty much always use", "tokens": [41156, 7914, 291, 1238, 709, 1009, 764], "temperature": 0.0, "avg_logprob": -0.21579933166503906, "compression_ratio": 1.5, "no_speech_prob": 1.154456640506396e-06}, {"id": 518, "seek": 255540, "start": 2555.4, "end": 2563.4, "text": " Relu's relu", "tokens": [8738, 84, 311, 1039, 84], "temperature": 0.4, "avg_logprob": -0.4759020524866441, "compression_ratio": 1.2234042553191489, "no_speech_prob": 3.3404926398361567e-06}, {"id": 519, "seek": 255540, "start": 2567.8, "end": 2570.42, "text": " All right, but there is a another", "tokens": [1057, 558, 11, 457, 456, 307, 257, 1071], "temperature": 0.4, "avg_logprob": -0.4759020524866441, "compression_ratio": 1.2234042553191489, "no_speech_prob": 3.3404926398361567e-06}, {"id": 520, "seek": 255540, "start": 2572.8, "end": 2576.7200000000003, "text": " Another one you can pick which is kind of interesting which is called", "tokens": [3996, 472, 291, 393, 1888, 597, 307, 733, 295, 1880, 597, 307, 1219], "temperature": 0.4, "avg_logprob": -0.4759020524866441, "compression_ratio": 1.2234042553191489, "no_speech_prob": 3.3404926398361567e-06}, {"id": 521, "seek": 257672, "start": 2576.72, "end": 2584.6, "text": " Leaky relu, and it looks like this and", "tokens": [1456, 15681, 1039, 84, 11, 293, 309, 1542, 411, 341, 293], "temperature": 0.0, "avg_logprob": -0.2777486651131276, "compression_ratio": 1.5737704918032787, "no_speech_prob": 4.289282969693886e-06}, {"id": 522, "seek": 257672, "start": 2587.4399999999996, "end": 2593.6, "text": " Basically if it's above zero it's y equals x and if it's below zero it's like y equals 0.1 x", "tokens": [8537, 498, 309, 311, 3673, 4018, 309, 311, 288, 6915, 2031, 293, 498, 309, 311, 2507, 4018, 309, 311, 411, 288, 6915, 1958, 13, 16, 2031], "temperature": 0.0, "avg_logprob": -0.2777486651131276, "compression_ratio": 1.5737704918032787, "no_speech_prob": 4.289282969693886e-06}, {"id": 523, "seek": 257672, "start": 2594.2, "end": 2596.68, "text": " Okay, so it's very similar to relu, but it's", "tokens": [1033, 11, 370, 309, 311, 588, 2531, 281, 1039, 84, 11, 457, 309, 311], "temperature": 0.0, "avg_logprob": -0.2777486651131276, "compression_ratio": 1.5737704918032787, "no_speech_prob": 4.289282969693886e-06}, {"id": 524, "seek": 257672, "start": 2597.48, "end": 2602.12, "text": " You know rather than being equal to zero under x. It's it's like something close to that", "tokens": [509, 458, 2831, 813, 885, 2681, 281, 4018, 833, 2031, 13, 467, 311, 309, 311, 411, 746, 1998, 281, 300], "temperature": 0.0, "avg_logprob": -0.2777486651131276, "compression_ratio": 1.5737704918032787, "no_speech_prob": 4.289282969693886e-06}, {"id": 525, "seek": 257672, "start": 2603.2799999999997, "end": 2605.2799999999997, "text": " So they're the main to", "tokens": [407, 436, 434, 264, 2135, 281], "temperature": 0.0, "avg_logprob": -0.2777486651131276, "compression_ratio": 1.5737704918032787, "no_speech_prob": 4.289282969693886e-06}, {"id": 526, "seek": 260528, "start": 2605.28, "end": 2607.0400000000004, "text": " relu and", "tokens": [1039, 84, 293], "temperature": 0.0, "avg_logprob": -0.17277553163725753, "compression_ratio": 1.619047619047619, "no_speech_prob": 2.3320635591517203e-06}, {"id": 527, "seek": 260528, "start": 2607.0400000000004, "end": 2609.0400000000004, "text": " leaky relu", "tokens": [476, 15681, 1039, 84], "temperature": 0.0, "avg_logprob": -0.17277553163725753, "compression_ratio": 1.619047619047619, "no_speech_prob": 2.3320635591517203e-06}, {"id": 528, "seek": 260528, "start": 2613.28, "end": 2619.0800000000004, "text": " There are various others, but they're kind of like things that just look very close to that so for example", "tokens": [821, 366, 3683, 2357, 11, 457, 436, 434, 733, 295, 411, 721, 300, 445, 574, 588, 1998, 281, 300, 370, 337, 1365], "temperature": 0.0, "avg_logprob": -0.17277553163725753, "compression_ratio": 1.619047619047619, "no_speech_prob": 2.3320635591517203e-06}, {"id": 529, "seek": 260528, "start": 2619.0800000000004, "end": 2621.44, "text": " There's something called elu which is quite popular", "tokens": [821, 311, 746, 1219, 806, 84, 597, 307, 1596, 3743], "temperature": 0.0, "avg_logprob": -0.17277553163725753, "compression_ratio": 1.619047619047619, "no_speech_prob": 2.3320635591517203e-06}, {"id": 530, "seek": 260528, "start": 2622.0, "end": 2627.7200000000003, "text": " But like you know the details don't matter too much honestly like that that like elu is something that looks like this", "tokens": [583, 411, 291, 458, 264, 4365, 500, 380, 1871, 886, 709, 6095, 411, 300, 300, 411, 806, 84, 307, 746, 300, 1542, 411, 341], "temperature": 0.0, "avg_logprob": -0.17277553163725753, "compression_ratio": 1.619047619047619, "no_speech_prob": 2.3320635591517203e-06}, {"id": 531, "seek": 260528, "start": 2627.7200000000003, "end": 2629.7200000000003, "text": " But it's slightly more curvy in the middle", "tokens": [583, 309, 311, 4748, 544, 1262, 11869, 294, 264, 2808], "temperature": 0.0, "avg_logprob": -0.17277553163725753, "compression_ratio": 1.619047619047619, "no_speech_prob": 2.3320635591517203e-06}, {"id": 532, "seek": 262972, "start": 2629.72, "end": 2638.2, "text": " And it's kind of like it's not generally something that you so much pick based on the data set it's more like", "tokens": [400, 309, 311, 733, 295, 411, 309, 311, 406, 5101, 746, 300, 291, 370, 709, 1888, 2361, 322, 264, 1412, 992, 309, 311, 544, 411], "temperature": 0.0, "avg_logprob": -0.1463340960050884, "compression_ratio": 1.689516129032258, "no_speech_prob": 1.3496900237441878e-06}, {"id": 533, "seek": 262972, "start": 2639.3999999999996, "end": 2645.3399999999997, "text": " Over time we just find better activation functions, so two or three years ago everybody used relu", "tokens": [4886, 565, 321, 445, 915, 1101, 24433, 6828, 11, 370, 732, 420, 1045, 924, 2057, 2201, 1143, 1039, 84], "temperature": 0.0, "avg_logprob": -0.1463340960050884, "compression_ratio": 1.689516129032258, "no_speech_prob": 1.3496900237441878e-06}, {"id": 534, "seek": 262972, "start": 2645.56, "end": 2649.3999999999996, "text": " You know a year ago pretty much everybody used leaky relu today", "tokens": [509, 458, 257, 1064, 2057, 1238, 709, 2201, 1143, 476, 15681, 1039, 84, 965], "temperature": 0.0, "avg_logprob": -0.1463340960050884, "compression_ratio": 1.689516129032258, "no_speech_prob": 1.3496900237441878e-06}, {"id": 535, "seek": 262972, "start": 2649.3999999999996, "end": 2655.4599999999996, "text": " I guess probably most people are starting to move towards elu, but honestly the choice of activation function doesn't matter", "tokens": [286, 2041, 1391, 881, 561, 366, 2891, 281, 1286, 3030, 806, 84, 11, 457, 6095, 264, 3922, 295, 24433, 2445, 1177, 380, 1871], "temperature": 0.0, "avg_logprob": -0.1463340960050884, "compression_ratio": 1.689516129032258, "no_speech_prob": 1.3496900237441878e-06}, {"id": 536, "seek": 265546, "start": 2655.46, "end": 2659.46, "text": " terribly much actually and", "tokens": [22903, 709, 767, 293], "temperature": 0.0, "avg_logprob": -0.2044886538856908, "compression_ratio": 1.463855421686747, "no_speech_prob": 2.3320626496570185e-06}, {"id": 537, "seek": 265546, "start": 2659.98, "end": 2666.2, "text": " You know people have actually showed that you can use like a pretty arbitrary nonlinear activation functions like even a sine wave", "tokens": [509, 458, 561, 362, 767, 4712, 300, 291, 393, 764, 411, 257, 1238, 23211, 2107, 28263, 24433, 6828, 411, 754, 257, 18609, 5772], "temperature": 0.0, "avg_logprob": -0.2044886538856908, "compression_ratio": 1.463855421686747, "no_speech_prob": 2.3320626496570185e-06}, {"id": 538, "seek": 265546, "start": 2666.86, "end": 2668.86, "text": " It still works", "tokens": [467, 920, 1985], "temperature": 0.0, "avg_logprob": -0.2044886538856908, "compression_ratio": 1.463855421686747, "no_speech_prob": 2.3320626496570185e-06}, {"id": 539, "seek": 265546, "start": 2670.98, "end": 2672.98, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2044886538856908, "compression_ratio": 1.463855421686747, "no_speech_prob": 2.3320626496570185e-06}, {"id": 540, "seek": 265546, "start": 2676.54, "end": 2680.5, "text": " So although what we're going to do today is showing how to create", "tokens": [407, 4878, 437, 321, 434, 516, 281, 360, 965, 307, 4099, 577, 281, 1884], "temperature": 0.0, "avg_logprob": -0.2044886538856908, "compression_ratio": 1.463855421686747, "no_speech_prob": 2.3320626496570185e-06}, {"id": 541, "seek": 268050, "start": 2680.5, "end": 2682.5, "text": " This", "tokens": [639], "temperature": 0.0, "avg_logprob": -0.2473501071595309, "compression_ratio": 1.4522292993630572, "no_speech_prob": 9.66598599916324e-06}, {"id": 542, "seek": 268050, "start": 2684.38, "end": 2686.38, "text": " Network with no hidden layers", "tokens": [12640, 365, 572, 7633, 7914], "temperature": 0.0, "avg_logprob": -0.2473501071595309, "compression_ratio": 1.4522292993630572, "no_speech_prob": 9.66598599916324e-06}, {"id": 543, "seek": 268050, "start": 2687.86, "end": 2689.86, "text": " To turn it into", "tokens": [1407, 1261, 309, 666], "temperature": 0.0, "avg_logprob": -0.2473501071595309, "compression_ratio": 1.4522292993630572, "no_speech_prob": 9.66598599916324e-06}, {"id": 544, "seek": 268050, "start": 2690.06, "end": 2697.22, "text": " That network which is 96 percent ish accurate is it will be trivial right and in fact is something you should", "tokens": [663, 3209, 597, 307, 24124, 3043, 307, 71, 8559, 307, 309, 486, 312, 26703, 558, 293, 294, 1186, 307, 746, 291, 820], "temperature": 0.0, "avg_logprob": -0.2473501071595309, "compression_ratio": 1.4522292993630572, "no_speech_prob": 9.66598599916324e-06}, {"id": 545, "seek": 268050, "start": 2697.86, "end": 2702.06, "text": " Probably try and do during the week right is to create that version", "tokens": [9210, 853, 293, 360, 1830, 264, 1243, 558, 307, 281, 1884, 300, 3037], "temperature": 0.0, "avg_logprob": -0.2473501071595309, "compression_ratio": 1.4522292993630572, "no_speech_prob": 9.66598599916324e-06}, {"id": 546, "seek": 270206, "start": 2702.06, "end": 2704.06, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.18415634772356818, "compression_ratio": 1.5164835164835164, "no_speech_prob": 2.857279696399928e-06}, {"id": 547, "seek": 270206, "start": 2710.58, "end": 2718.7, "text": " So now that we've got something where we can take our network pass in our variable and get back some", "tokens": [407, 586, 300, 321, 600, 658, 746, 689, 321, 393, 747, 527, 3209, 1320, 294, 527, 7006, 293, 483, 646, 512], "temperature": 0.0, "avg_logprob": -0.18415634772356818, "compression_ratio": 1.5164835164835164, "no_speech_prob": 2.857279696399928e-06}, {"id": 548, "seek": 270206, "start": 2719.54, "end": 2721.54, "text": " predictions", "tokens": [21264], "temperature": 0.0, "avg_logprob": -0.18415634772356818, "compression_ratio": 1.5164835164835164, "no_speech_prob": 2.857279696399928e-06}, {"id": 549, "seek": 270206, "start": 2722.58, "end": 2726.58, "text": " That's basically all that happened when we called fit, so we're going to see how how that", "tokens": [663, 311, 1936, 439, 300, 2011, 562, 321, 1219, 3318, 11, 370, 321, 434, 516, 281, 536, 577, 577, 300], "temperature": 0.0, "avg_logprob": -0.18415634772356818, "compression_ratio": 1.5164835164835164, "no_speech_prob": 2.857279696399928e-06}, {"id": 550, "seek": 270206, "start": 2727.38, "end": 2731.1, "text": " That approach can be used to create this stochastic gradient descent", "tokens": [663, 3109, 393, 312, 1143, 281, 1884, 341, 342, 8997, 2750, 16235, 23475], "temperature": 0.0, "avg_logprob": -0.18415634772356818, "compression_ratio": 1.5164835164835164, "no_speech_prob": 2.857279696399928e-06}, {"id": 551, "seek": 273110, "start": 2731.1, "end": 2732.58, "text": " one", "tokens": [472], "temperature": 0.0, "avg_logprob": -0.2788499801877945, "compression_ratio": 1.5512820512820513, "no_speech_prob": 4.289298885851167e-06}, {"id": 552, "seek": 273110, "start": 2732.58, "end": 2737.62, "text": " Thing to note is that the to turn the predicted probabilities?", "tokens": [30902, 281, 3637, 307, 300, 264, 281, 1261, 264, 19147, 33783, 30], "temperature": 0.0, "avg_logprob": -0.2788499801877945, "compression_ratio": 1.5512820512820513, "no_speech_prob": 4.289298885851167e-06}, {"id": 553, "seek": 273110, "start": 2738.58, "end": 2743.54, "text": " Into a predicted like which digit is it we would need to use argmax", "tokens": [23373, 257, 19147, 411, 597, 14293, 307, 309, 321, 576, 643, 281, 764, 3882, 41167], "temperature": 0.0, "avg_logprob": -0.2788499801877945, "compression_ratio": 1.5512820512820513, "no_speech_prob": 4.289298885851167e-06}, {"id": 554, "seek": 273110, "start": 2745.38, "end": 2748.2599999999998, "text": " Unfortunately pi torch doesn't call it argmax", "tokens": [8590, 3895, 27822, 1177, 380, 818, 309, 3882, 41167], "temperature": 0.0, "avg_logprob": -0.2788499801877945, "compression_ratio": 1.5512820512820513, "no_speech_prob": 4.289298885851167e-06}, {"id": 555, "seek": 273110, "start": 2749.22, "end": 2753.54, "text": " Instead pi torch just calls it max and max returns", "tokens": [7156, 3895, 27822, 445, 5498, 309, 11469, 293, 11469, 11247], "temperature": 0.0, "avg_logprob": -0.2788499801877945, "compression_ratio": 1.5512820512820513, "no_speech_prob": 4.289298885851167e-06}, {"id": 556, "seek": 273110, "start": 2754.2999999999997, "end": 2755.98, "text": " two things", "tokens": [732, 721], "temperature": 0.0, "avg_logprob": -0.2788499801877945, "compression_ratio": 1.5512820512820513, "no_speech_prob": 4.289298885851167e-06}, {"id": 557, "seek": 275598, "start": 2755.98, "end": 2762.3, "text": " It returns the actual max across this axis, so this is across the columns right and", "tokens": [467, 11247, 264, 3539, 11469, 2108, 341, 10298, 11, 370, 341, 307, 2108, 264, 13766, 558, 293], "temperature": 0.0, "avg_logprob": -0.255474292530733, "compression_ratio": 1.6600985221674878, "no_speech_prob": 1.9637993773358176e-06}, {"id": 558, "seek": 275598, "start": 2762.9, "end": 2765.06, "text": " The second thing it returns is the index", "tokens": [440, 1150, 551, 309, 11247, 307, 264, 8186], "temperature": 0.0, "avg_logprob": -0.255474292530733, "compression_ratio": 1.6600985221674878, "no_speech_prob": 1.9637993773358176e-06}, {"id": 559, "seek": 275598, "start": 2766.02, "end": 2772.18, "text": " Over that maximum right so so the equivalent of argmax is to call max and then get the first", "tokens": [4886, 300, 6674, 558, 370, 370, 264, 10344, 295, 3882, 41167, 307, 281, 818, 11469, 293, 550, 483, 264, 700], "temperature": 0.0, "avg_logprob": -0.255474292530733, "compression_ratio": 1.6600985221674878, "no_speech_prob": 1.9637993773358176e-06}, {"id": 560, "seek": 275598, "start": 2772.98, "end": 2779.48, "text": " Indexed thing okay, so there's our predictions right if this was in numpy. We would instead use np dot argmax", "tokens": [33552, 292, 551, 1392, 11, 370, 456, 311, 527, 21264, 558, 498, 341, 390, 294, 1031, 8200, 13, 492, 576, 2602, 764, 33808, 5893, 3882, 41167], "temperature": 0.0, "avg_logprob": -0.255474292530733, "compression_ratio": 1.6600985221674878, "no_speech_prob": 1.9637993773358176e-06}, {"id": 561, "seek": 275598, "start": 2783.7, "end": 2785.7, "text": " All right", "tokens": [1057, 558], "temperature": 0.0, "avg_logprob": -0.255474292530733, "compression_ratio": 1.6600985221674878, "no_speech_prob": 1.9637993773358176e-06}, {"id": 562, "seek": 278570, "start": 2785.7, "end": 2791.6, "text": " So here are the predictions from our hand created logistic regression and in this case", "tokens": [407, 510, 366, 264, 21264, 490, 527, 1011, 2942, 3565, 3142, 24590, 293, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.1539505406429893, "compression_ratio": 1.6546391752577319, "no_speech_prob": 4.637842266674852e-06}, {"id": 563, "seek": 278570, "start": 2792.7, "end": 2794.8599999999997, "text": " Looks like we got all but one correct", "tokens": [10027, 411, 321, 658, 439, 457, 472, 3006], "temperature": 0.0, "avg_logprob": -0.1539505406429893, "compression_ratio": 1.6546391752577319, "no_speech_prob": 4.637842266674852e-06}, {"id": 564, "seek": 278570, "start": 2797.3399999999997, "end": 2803.8199999999997, "text": " So the next thing we're going to try and get rid of in terms of using libraries is we'll try to avoid using the matrix", "tokens": [407, 264, 958, 551, 321, 434, 516, 281, 853, 293, 483, 3973, 295, 294, 2115, 295, 1228, 15148, 307, 321, 603, 853, 281, 5042, 1228, 264, 8141], "temperature": 0.0, "avg_logprob": -0.1539505406429893, "compression_ratio": 1.6546391752577319, "no_speech_prob": 4.637842266674852e-06}, {"id": 565, "seek": 280382, "start": 2803.82, "end": 2815.9, "text": " Multiplication operator and instead we're going to try and write that by hand", "tokens": [29238, 4770, 399, 12973, 293, 2602, 321, 434, 516, 281, 853, 293, 2464, 300, 538, 1011], "temperature": 0.0, "avg_logprob": -0.2672546489818676, "compression_ratio": 1.3693693693693694, "no_speech_prob": 7.338205136875331e-07}, {"id": 566, "seek": 280382, "start": 2819.42, "end": 2823.86, "text": " So this next part we're going to learn about something which kind of seems", "tokens": [407, 341, 958, 644, 321, 434, 516, 281, 1466, 466, 746, 597, 733, 295, 2544], "temperature": 0.0, "avg_logprob": -0.2672546489818676, "compression_ratio": 1.3693693693693694, "no_speech_prob": 7.338205136875331e-07}, {"id": 567, "seek": 282386, "start": 2823.86, "end": 2832.46, "text": " It kind of it's going to seem like a minor little kind of programming idea", "tokens": [467, 733, 295, 309, 311, 516, 281, 1643, 411, 257, 6696, 707, 733, 295, 9410, 1558], "temperature": 0.0, "avg_logprob": -0.16063622368706598, "compression_ratio": 1.7534883720930232, "no_speech_prob": 1.9333467662363546e-06}, {"id": 568, "seek": 282386, "start": 2832.6200000000003, "end": 2834.6200000000003, "text": " But actually it's going to turn out", "tokens": [583, 767, 309, 311, 516, 281, 1261, 484], "temperature": 0.0, "avg_logprob": -0.16063622368706598, "compression_ratio": 1.7534883720930232, "no_speech_prob": 1.9333467662363546e-06}, {"id": 569, "seek": 282386, "start": 2835.6200000000003, "end": 2838.54, "text": " That at least in my opinion. It's the most important", "tokens": [663, 412, 1935, 294, 452, 4800, 13, 467, 311, 264, 881, 1021], "temperature": 0.0, "avg_logprob": -0.16063622368706598, "compression_ratio": 1.7534883720930232, "no_speech_prob": 1.9333467662363546e-06}, {"id": 570, "seek": 282386, "start": 2839.46, "end": 2846.7400000000002, "text": " Programming concept that we'll teach in this course and it's possibly the most important programming con kind of concept in all of", "tokens": [8338, 2810, 3410, 300, 321, 603, 2924, 294, 341, 1164, 293, 309, 311, 6264, 264, 881, 1021, 9410, 416, 733, 295, 3410, 294, 439, 295], "temperature": 0.0, "avg_logprob": -0.16063622368706598, "compression_ratio": 1.7534883720930232, "no_speech_prob": 1.9333467662363546e-06}, {"id": 571, "seek": 282386, "start": 2847.94, "end": 2852.02, "text": " All the things you need to build machine learning algorithms, and it's the idea of", "tokens": [1057, 264, 721, 291, 643, 281, 1322, 3479, 2539, 14642, 11, 293, 309, 311, 264, 1558, 295], "temperature": 0.0, "avg_logprob": -0.16063622368706598, "compression_ratio": 1.7534883720930232, "no_speech_prob": 1.9333467662363546e-06}, {"id": 572, "seek": 285202, "start": 2852.02, "end": 2854.02, "text": " broadcasting", "tokens": [30024], "temperature": 0.0, "avg_logprob": -0.2222123087188344, "compression_ratio": 1.546875, "no_speech_prob": 2.295907279403764e-06}, {"id": 573, "seek": 285202, "start": 2854.38, "end": 2856.94, "text": " And the idea I will show by example", "tokens": [400, 264, 1558, 286, 486, 855, 538, 1365], "temperature": 0.0, "avg_logprob": -0.2222123087188344, "compression_ratio": 1.546875, "no_speech_prob": 2.295907279403764e-06}, {"id": 574, "seek": 285202, "start": 2857.82, "end": 2859.98, "text": " if we create an array of", "tokens": [498, 321, 1884, 364, 10225, 295], "temperature": 0.0, "avg_logprob": -0.2222123087188344, "compression_ratio": 1.546875, "no_speech_prob": 2.295907279403764e-06}, {"id": 575, "seek": 285202, "start": 2860.34, "end": 2865.14, "text": " 10 6 neg 4 and an array of 2 8 7 and then add the two together", "tokens": [1266, 1386, 2485, 1017, 293, 364, 10225, 295, 568, 1649, 1614, 293, 550, 909, 264, 732, 1214], "temperature": 0.0, "avg_logprob": -0.2222123087188344, "compression_ratio": 1.546875, "no_speech_prob": 2.295907279403764e-06}, {"id": 576, "seek": 285202, "start": 2867.86, "end": 2874.1, "text": " It adds each of the components of those two arrays in turn we call that element wise", "tokens": [467, 10860, 1184, 295, 264, 6677, 295, 729, 732, 41011, 294, 1261, 321, 818, 300, 4478, 10829], "temperature": 0.0, "avg_logprob": -0.2222123087188344, "compression_ratio": 1.546875, "no_speech_prob": 2.295907279403764e-06}, {"id": 577, "seek": 285202, "start": 2874.9, "end": 2878.7599999999998, "text": " So in other words we didn't have to write a loop right back in the old days", "tokens": [407, 294, 661, 2283, 321, 994, 380, 362, 281, 2464, 257, 6367, 558, 646, 294, 264, 1331, 1708], "temperature": 0.0, "avg_logprob": -0.2222123087188344, "compression_ratio": 1.546875, "no_speech_prob": 2.295907279403764e-06}, {"id": 578, "seek": 287876, "start": 2878.76, "end": 2882.5400000000004, "text": " We would have to have looped through each one and added them and then concatenated them together", "tokens": [492, 576, 362, 281, 362, 6367, 292, 807, 1184, 472, 293, 3869, 552, 293, 550, 1588, 7186, 770, 552, 1214], "temperature": 0.0, "avg_logprob": -0.1800244275261374, "compression_ratio": 1.6536585365853658, "no_speech_prob": 6.1793701888746e-07}, {"id": 579, "seek": 287876, "start": 2882.78, "end": 2888.0200000000004, "text": " We don't have to do that today it happens for us automatically so in numpy", "tokens": [492, 500, 380, 362, 281, 360, 300, 965, 309, 2314, 337, 505, 6772, 370, 294, 1031, 8200], "temperature": 0.0, "avg_logprob": -0.1800244275261374, "compression_ratio": 1.6536585365853658, "no_speech_prob": 6.1793701888746e-07}, {"id": 580, "seek": 287876, "start": 2888.5, "end": 2891.6600000000003, "text": " We automatically get element wise operations", "tokens": [492, 6772, 483, 4478, 10829, 7705], "temperature": 0.0, "avg_logprob": -0.1800244275261374, "compression_ratio": 1.6536585365853658, "no_speech_prob": 6.1793701888746e-07}, {"id": 581, "seek": 287876, "start": 2897.1800000000003, "end": 2900.44, "text": " We can do the same thing with pytorch", "tokens": [492, 393, 360, 264, 912, 551, 365, 25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.1800244275261374, "compression_ratio": 1.6536585365853658, "no_speech_prob": 6.1793701888746e-07}, {"id": 582, "seek": 287876, "start": 2903.1000000000004, "end": 2907.6600000000003, "text": " So in fast AI we just add a little capital T to turn something into a pytorch tensor", "tokens": [407, 294, 2370, 7318, 321, 445, 909, 257, 707, 4238, 314, 281, 1261, 746, 666, 257, 25878, 284, 339, 40863], "temperature": 0.0, "avg_logprob": -0.1800244275261374, "compression_ratio": 1.6536585365853658, "no_speech_prob": 6.1793701888746e-07}, {"id": 583, "seek": 290766, "start": 2907.66, "end": 2909.8199999999997, "text": " All right, and if we add those together", "tokens": [1057, 558, 11, 293, 498, 321, 909, 729, 1214], "temperature": 0.0, "avg_logprob": -0.17151101813258895, "compression_ratio": 1.6222222222222222, "no_speech_prob": 8.990930950858456e-07}, {"id": 584, "seek": 290766, "start": 2911.42, "end": 2917.74, "text": " Exactly the same thing right so element wise operations are pretty standard in these kinds of libraries", "tokens": [7587, 264, 912, 551, 558, 370, 4478, 10829, 7705, 366, 1238, 3832, 294, 613, 3685, 295, 15148], "temperature": 0.0, "avg_logprob": -0.17151101813258895, "compression_ratio": 1.6222222222222222, "no_speech_prob": 8.990930950858456e-07}, {"id": 585, "seek": 290766, "start": 2919.66, "end": 2924.12, "text": " It's interesting not just because we don't have to write the for loop", "tokens": [467, 311, 1880, 406, 445, 570, 321, 500, 380, 362, 281, 2464, 264, 337, 6367], "temperature": 0.0, "avg_logprob": -0.17151101813258895, "compression_ratio": 1.6222222222222222, "no_speech_prob": 8.990930950858456e-07}, {"id": 586, "seek": 290766, "start": 2924.66, "end": 2929.3999999999996, "text": " Right, but it's actually much more interesting because of the performance things that are happening here", "tokens": [1779, 11, 457, 309, 311, 767, 709, 544, 1880, 570, 295, 264, 3389, 721, 300, 366, 2737, 510], "temperature": 0.0, "avg_logprob": -0.17151101813258895, "compression_ratio": 1.6222222222222222, "no_speech_prob": 8.990930950858456e-07}, {"id": 587, "seek": 290766, "start": 2929.7, "end": 2932.06, "text": " The first is if we were doing a for loop", "tokens": [440, 700, 307, 498, 321, 645, 884, 257, 337, 6367], "temperature": 0.0, "avg_logprob": -0.17151101813258895, "compression_ratio": 1.6222222222222222, "no_speech_prob": 8.990930950858456e-07}, {"id": 588, "seek": 293206, "start": 2932.06, "end": 2938.06, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.21403872626168388, "compression_ratio": 1.5759493670886076, "no_speech_prob": 8.714323485037312e-07}, {"id": 589, "seek": 293206, "start": 2939.06, "end": 2942.66, "text": " If we were doing a for loop that would happen in Python", "tokens": [759, 321, 645, 884, 257, 337, 6367, 300, 576, 1051, 294, 15329], "temperature": 0.0, "avg_logprob": -0.21403872626168388, "compression_ratio": 1.5759493670886076, "no_speech_prob": 8.714323485037312e-07}, {"id": 590, "seek": 293206, "start": 2943.9, "end": 2950.18, "text": " Right even when you use pytorch it still does the for loop in Python it has no way of like", "tokens": [1779, 754, 562, 291, 764, 25878, 284, 339, 309, 920, 775, 264, 337, 6367, 294, 15329, 309, 575, 572, 636, 295, 411], "temperature": 0.0, "avg_logprob": -0.21403872626168388, "compression_ratio": 1.5759493670886076, "no_speech_prob": 8.714323485037312e-07}, {"id": 591, "seek": 293206, "start": 2950.82, "end": 2955.66, "text": " Optimizing a for loop and so a for loop in Python is something like", "tokens": [35013, 3319, 257, 337, 6367, 293, 370, 257, 337, 6367, 294, 15329, 307, 746, 411], "temperature": 0.0, "avg_logprob": -0.21403872626168388, "compression_ratio": 1.5759493670886076, "no_speech_prob": 8.714323485037312e-07}, {"id": 592, "seek": 293206, "start": 2956.58, "end": 2958.7799999999997, "text": " 10,000 times lower than in C", "tokens": [1266, 11, 1360, 1413, 3126, 813, 294, 383], "temperature": 0.0, "avg_logprob": -0.21403872626168388, "compression_ratio": 1.5759493670886076, "no_speech_prob": 8.714323485037312e-07}, {"id": 593, "seek": 295878, "start": 2958.78, "end": 2960.78, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.25654846994500413, "compression_ratio": 1.5633187772925765, "no_speech_prob": 2.6425734631629894e-06}, {"id": 594, "seek": 295878, "start": 2960.78, "end": 2963.82, "text": " That's your first problem. I can't remember it's like 1000 or 10,000", "tokens": [663, 311, 428, 700, 1154, 13, 286, 393, 380, 1604, 309, 311, 411, 9714, 420, 1266, 11, 1360], "temperature": 0.0, "avg_logprob": -0.25654846994500413, "compression_ratio": 1.5633187772925765, "no_speech_prob": 2.6425734631629894e-06}, {"id": 595, "seek": 295878, "start": 2964.6200000000003, "end": 2967.6200000000003, "text": " the second problem then is that", "tokens": [264, 1150, 1154, 550, 307, 300], "temperature": 0.0, "avg_logprob": -0.25654846994500413, "compression_ratio": 1.5633187772925765, "no_speech_prob": 2.6425734631629894e-06}, {"id": 596, "seek": 295878, "start": 2969.3, "end": 2971.5400000000004, "text": " You don't just want it to be optimized in C", "tokens": [509, 500, 380, 445, 528, 309, 281, 312, 26941, 294, 383], "temperature": 0.0, "avg_logprob": -0.25654846994500413, "compression_ratio": 1.5633187772925765, "no_speech_prob": 2.6425734631629894e-06}, {"id": 597, "seek": 295878, "start": 2971.7000000000003, "end": 2977.7400000000002, "text": " But you want C to take advantage of the thing that your all of your CPUs do to something called Cindy", "tokens": [583, 291, 528, 383, 281, 747, 5002, 295, 264, 551, 300, 428, 439, 295, 428, 13199, 82, 360, 281, 746, 1219, 32185], "temperature": 0.0, "avg_logprob": -0.25654846994500413, "compression_ratio": 1.5633187772925765, "no_speech_prob": 2.6425734631629894e-06}, {"id": 598, "seek": 295878, "start": 2977.86, "end": 2983.5400000000004, "text": " single instruction multiple data, which is it yours can your CPU is capable of taking", "tokens": [2167, 10951, 3866, 1412, 11, 597, 307, 309, 6342, 393, 428, 13199, 307, 8189, 295, 1940], "temperature": 0.0, "avg_logprob": -0.25654846994500413, "compression_ratio": 1.5633187772925765, "no_speech_prob": 2.6425734631629894e-06}, {"id": 599, "seek": 295878, "start": 2984.3, "end": 2986.3, "text": " eight things at a time", "tokens": [3180, 721, 412, 257, 565], "temperature": 0.0, "avg_logprob": -0.25654846994500413, "compression_ratio": 1.5633187772925765, "no_speech_prob": 2.6425734631629894e-06}, {"id": 600, "seek": 298630, "start": 2986.3, "end": 2989.86, "text": " Right in a vector and adding them up to another", "tokens": [1779, 294, 257, 8062, 293, 5127, 552, 493, 281, 1071], "temperature": 0.0, "avg_logprob": -0.18405531701587496, "compression_ratio": 1.6418604651162791, "no_speech_prob": 5.989265332573268e-07}, {"id": 601, "seek": 298630, "start": 2990.6200000000003, "end": 2994.42, "text": " Vector with eight things in in a single CPU instruction", "tokens": [691, 20814, 365, 3180, 721, 294, 294, 257, 2167, 13199, 10951], "temperature": 0.0, "avg_logprob": -0.18405531701587496, "compression_ratio": 1.6418604651162791, "no_speech_prob": 5.989265332573268e-07}, {"id": 602, "seek": 298630, "start": 2995.1000000000004, "end": 2999.3, "text": " Right so if you can take advantage of Cindy you're immediately eight times faster", "tokens": [1779, 370, 498, 291, 393, 747, 5002, 295, 32185, 291, 434, 4258, 3180, 1413, 4663], "temperature": 0.0, "avg_logprob": -0.18405531701587496, "compression_ratio": 1.6418604651162791, "no_speech_prob": 5.989265332573268e-07}, {"id": 603, "seek": 298630, "start": 2999.3, "end": 3002.34, "text": " It depends on how big the data type is it might be four might be eight", "tokens": [467, 5946, 322, 577, 955, 264, 1412, 2010, 307, 309, 1062, 312, 1451, 1062, 312, 3180], "temperature": 0.0, "avg_logprob": -0.18405531701587496, "compression_ratio": 1.6418604651162791, "no_speech_prob": 5.989265332573268e-07}, {"id": 604, "seek": 298630, "start": 3002.86, "end": 3007.3, "text": " The other thing that you've got in your computer is you've got multiple processes", "tokens": [440, 661, 551, 300, 291, 600, 658, 294, 428, 3820, 307, 291, 600, 658, 3866, 7555], "temperature": 0.0, "avg_logprob": -0.18405531701587496, "compression_ratio": 1.6418604651162791, "no_speech_prob": 5.989265332573268e-07}, {"id": 605, "seek": 298630, "start": 3009.34, "end": 3011.42, "text": " Multiple cores", "tokens": [40056, 24826], "temperature": 0.0, "avg_logprob": -0.18405531701587496, "compression_ratio": 1.6418604651162791, "no_speech_prob": 5.989265332573268e-07}, {"id": 606, "seek": 301142, "start": 3011.42, "end": 3018.1, "text": " So you've probably got like if this is inside happening on one side one core you've probably got about four of those", "tokens": [407, 291, 600, 1391, 658, 411, 498, 341, 307, 1854, 2737, 322, 472, 1252, 472, 4965, 291, 600, 1391, 658, 466, 1451, 295, 729], "temperature": 0.0, "avg_logprob": -0.16658214353165537, "compression_ratio": 1.8093220338983051, "no_speech_prob": 1.1726381217158632e-06}, {"id": 607, "seek": 301142, "start": 3019.34, "end": 3023.8, "text": " Okay, so if you're using Cindy you're eight times faster if you can use multiple cores", "tokens": [1033, 11, 370, 498, 291, 434, 1228, 32185, 291, 434, 3180, 1413, 4663, 498, 291, 393, 764, 3866, 24826], "temperature": 0.0, "avg_logprob": -0.16658214353165537, "compression_ratio": 1.8093220338983051, "no_speech_prob": 1.1726381217158632e-06}, {"id": 608, "seek": 301142, "start": 3023.8, "end": 3025.8, "text": " Then you're 32 times faster", "tokens": [1396, 291, 434, 8858, 1413, 4663], "temperature": 0.0, "avg_logprob": -0.16658214353165537, "compression_ratio": 1.8093220338983051, "no_speech_prob": 1.1726381217158632e-06}, {"id": 609, "seek": 301142, "start": 3025.8, "end": 3031.46, "text": " And then if you're doing that in C you might be something like 32 times thousand times faster", "tokens": [400, 550, 498, 291, 434, 884, 300, 294, 383, 291, 1062, 312, 746, 411, 8858, 1413, 4714, 1413, 4663], "temperature": 0.0, "avg_logprob": -0.16658214353165537, "compression_ratio": 1.8093220338983051, "no_speech_prob": 1.1726381217158632e-06}, {"id": 610, "seek": 301142, "start": 3031.58, "end": 3034.9, "text": " Right and so the nice thing is that when we do that", "tokens": [1779, 293, 370, 264, 1481, 551, 307, 300, 562, 321, 360, 300], "temperature": 0.0, "avg_logprob": -0.16658214353165537, "compression_ratio": 1.8093220338983051, "no_speech_prob": 1.1726381217158632e-06}, {"id": 611, "seek": 301142, "start": 3035.66, "end": 3038.38, "text": " It's taking advantage of all of these things", "tokens": [467, 311, 1940, 5002, 295, 439, 295, 613, 721], "temperature": 0.0, "avg_logprob": -0.16658214353165537, "compression_ratio": 1.8093220338983051, "no_speech_prob": 1.1726381217158632e-06}, {"id": 612, "seek": 301142, "start": 3039.34, "end": 3040.34, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.16658214353165537, "compression_ratio": 1.8093220338983051, "no_speech_prob": 1.1726381217158632e-06}, {"id": 613, "seek": 304034, "start": 3040.34, "end": 3043.2400000000002, "text": " Better still if you do it in", "tokens": [15753, 920, 498, 291, 360, 309, 294], "temperature": 0.0, "avg_logprob": -0.27842089864942765, "compression_ratio": 1.451219512195122, "no_speech_prob": 7.002162192293326e-07}, {"id": 614, "seek": 304034, "start": 3044.7400000000002, "end": 3048.32, "text": " PyTorch and your data was created with", "tokens": [9953, 51, 284, 339, 293, 428, 1412, 390, 2942, 365], "temperature": 0.0, "avg_logprob": -0.27842089864942765, "compression_ratio": 1.451219512195122, "no_speech_prob": 7.002162192293326e-07}, {"id": 615, "seek": 304034, "start": 3050.3, "end": 3052.3, "text": " Cuda to stick it on the GPU", "tokens": [383, 11152, 281, 2897, 309, 322, 264, 18407], "temperature": 0.0, "avg_logprob": -0.27842089864942765, "compression_ratio": 1.451219512195122, "no_speech_prob": 7.002162192293326e-07}, {"id": 616, "seek": 304034, "start": 3054.38, "end": 3057.58, "text": " Then your GPU can do about 10,000 things at a time", "tokens": [1396, 428, 18407, 393, 360, 466, 1266, 11, 1360, 721, 412, 257, 565], "temperature": 0.0, "avg_logprob": -0.27842089864942765, "compression_ratio": 1.451219512195122, "no_speech_prob": 7.002162192293326e-07}, {"id": 617, "seek": 304034, "start": 3058.1000000000004, "end": 3061.5, "text": " All right, so that'll be another hundred times faster than C", "tokens": [1057, 558, 11, 370, 300, 603, 312, 1071, 3262, 1413, 4663, 813, 383], "temperature": 0.0, "avg_logprob": -0.27842089864942765, "compression_ratio": 1.451219512195122, "no_speech_prob": 7.002162192293326e-07}, {"id": 618, "seek": 304034, "start": 3062.02, "end": 3064.5, "text": " All right, so this is critical", "tokens": [1057, 558, 11, 370, 341, 307, 4924], "temperature": 0.0, "avg_logprob": -0.27842089864942765, "compression_ratio": 1.451219512195122, "no_speech_prob": 7.002162192293326e-07}, {"id": 619, "seek": 306450, "start": 3064.5, "end": 3070.06, "text": " To getting good performance is you have to learn how to write", "tokens": [1407, 1242, 665, 3389, 307, 291, 362, 281, 1466, 577, 281, 2464], "temperature": 0.0, "avg_logprob": -0.2106525081477753, "compression_ratio": 1.451086956521739, "no_speech_prob": 5.368744382394652e-07}, {"id": 620, "seek": 306450, "start": 3070.86, "end": 3072.54, "text": " loopless code", "tokens": [6367, 1832, 3089], "temperature": 0.0, "avg_logprob": -0.2106525081477753, "compression_ratio": 1.451086956521739, "no_speech_prob": 5.368744382394652e-07}, {"id": 621, "seek": 306450, "start": 3072.54, "end": 3074.86, "text": " By taking advantage of these element wise", "tokens": [3146, 1940, 5002, 295, 613, 4478, 10829], "temperature": 0.0, "avg_logprob": -0.2106525081477753, "compression_ratio": 1.451086956521739, "no_speech_prob": 5.368744382394652e-07}, {"id": 622, "seek": 306450, "start": 3075.94, "end": 3079.06, "text": " Operations and like it's not it's a lot more than just plus I", "tokens": [36381, 293, 411, 309, 311, 406, 309, 311, 257, 688, 544, 813, 445, 1804, 286], "temperature": 0.0, "avg_logprob": -0.2106525081477753, "compression_ratio": 1.451086956521739, "no_speech_prob": 5.368744382394652e-07}, {"id": 623, "seek": 306450, "start": 3080.86, "end": 3085.16, "text": " Could also use less than right and that's going to return", "tokens": [7497, 611, 764, 1570, 813, 558, 293, 300, 311, 516, 281, 2736], "temperature": 0.0, "avg_logprob": -0.2106525081477753, "compression_ratio": 1.451086956521739, "no_speech_prob": 5.368744382394652e-07}, {"id": 624, "seek": 306450, "start": 3086.54, "end": 3088.86, "text": " 011 or if we go back to numpy", "tokens": [1958, 5348, 420, 498, 321, 352, 646, 281, 1031, 8200], "temperature": 0.0, "avg_logprob": -0.2106525081477753, "compression_ratio": 1.451086956521739, "no_speech_prob": 5.368744382394652e-07}, {"id": 625, "seek": 308886, "start": 3088.86, "end": 3090.86, "text": " by", "tokens": [538], "temperature": 0.0, "avg_logprob": -0.23941673951990464, "compression_ratio": 1.4807692307692308, "no_speech_prob": 5.80498635827098e-07}, {"id": 626, "seek": 308886, "start": 3093.6600000000003, "end": 3095.6600000000003, "text": " False true true", "tokens": [50040, 2074, 2074], "temperature": 0.0, "avg_logprob": -0.23941673951990464, "compression_ratio": 1.4807692307692308, "no_speech_prob": 5.80498635827098e-07}, {"id": 627, "seek": 308886, "start": 3096.54, "end": 3102.08, "text": " And so you can kind of use this to do all kinds of things without looping so for example", "tokens": [400, 370, 291, 393, 733, 295, 764, 341, 281, 360, 439, 3685, 295, 721, 1553, 6367, 278, 370, 337, 1365], "temperature": 0.0, "avg_logprob": -0.23941673951990464, "compression_ratio": 1.4807692307692308, "no_speech_prob": 5.80498635827098e-07}, {"id": 628, "seek": 308886, "start": 3102.08, "end": 3107.46, "text": " I could now multiply that by a and here are all of the values of a", "tokens": [286, 727, 586, 12972, 300, 538, 257, 293, 510, 366, 439, 295, 264, 4190, 295, 257], "temperature": 0.0, "avg_logprob": -0.23941673951990464, "compression_ratio": 1.4807692307692308, "no_speech_prob": 5.80498635827098e-07}, {"id": 629, "seek": 308886, "start": 3108.78, "end": 3110.78, "text": " As long as they're less than B", "tokens": [1018, 938, 382, 436, 434, 1570, 813, 363], "temperature": 0.0, "avg_logprob": -0.23941673951990464, "compression_ratio": 1.4807692307692308, "no_speech_prob": 5.80498635827098e-07}, {"id": 630, "seek": 308886, "start": 3111.54, "end": 3113.54, "text": " Or we could take the mean", "tokens": [1610, 321, 727, 747, 264, 914], "temperature": 0.0, "avg_logprob": -0.23941673951990464, "compression_ratio": 1.4807692307692308, "no_speech_prob": 5.80498635827098e-07}, {"id": 631, "seek": 311354, "start": 3113.54, "end": 3119.5, "text": " This is the percentage of values in a that are less than B", "tokens": [639, 307, 264, 9668, 295, 4190, 294, 257, 300, 366, 1570, 813, 363], "temperature": 0.0, "avg_logprob": -0.17812899107574134, "compression_ratio": 1.590717299578059, "no_speech_prob": 2.260314658997231e-06}, {"id": 632, "seek": 311354, "start": 3120.2599999999998, "end": 3123.7, "text": " All right, so like there's a lot of stuff you can do with this simple idea", "tokens": [1057, 558, 11, 370, 411, 456, 311, 257, 688, 295, 1507, 291, 393, 360, 365, 341, 2199, 1558], "temperature": 0.0, "avg_logprob": -0.17812899107574134, "compression_ratio": 1.590717299578059, "no_speech_prob": 2.260314658997231e-06}, {"id": 633, "seek": 311354, "start": 3124.3, "end": 3126.3, "text": " But to take it further", "tokens": [583, 281, 747, 309, 3052], "temperature": 0.0, "avg_logprob": -0.17812899107574134, "compression_ratio": 1.590717299578059, "no_speech_prob": 2.260314658997231e-06}, {"id": 634, "seek": 311354, "start": 3126.3, "end": 3129.72, "text": " Right to take it further than just this element wise operation", "tokens": [1779, 281, 747, 309, 3052, 813, 445, 341, 4478, 10829, 6916], "temperature": 0.0, "avg_logprob": -0.17812899107574134, "compression_ratio": 1.590717299578059, "no_speech_prob": 2.260314658997231e-06}, {"id": 635, "seek": 311354, "start": 3130.06, "end": 3133.2599999999998, "text": " We're going to have to go the next step to something called broadcasting", "tokens": [492, 434, 516, 281, 362, 281, 352, 264, 958, 1823, 281, 746, 1219, 30024], "temperature": 0.0, "avg_logprob": -0.17812899107574134, "compression_ratio": 1.590717299578059, "no_speech_prob": 2.260314658997231e-06}, {"id": 636, "seek": 313326, "start": 3133.26, "end": 3143.5, "text": " So let's take a five minute break come back at 217 and we'll talk about broadcasting", "tokens": [407, 718, 311, 747, 257, 1732, 3456, 1821, 808, 646, 412, 5080, 22, 293, 321, 603, 751, 466, 30024], "temperature": 0.0, "avg_logprob": -0.2476579189300537, "compression_ratio": 1.5286624203821657, "no_speech_prob": 2.0261331883375533e-06}, {"id": 637, "seek": 313326, "start": 3144.9, "end": 3148.5, "text": " So broadcasting", "tokens": [407, 30024], "temperature": 0.0, "avg_logprob": -0.2476579189300537, "compression_ratio": 1.5286624203821657, "no_speech_prob": 2.0261331883375533e-06}, {"id": 638, "seek": 313326, "start": 3153.34, "end": 3156.86, "text": " This is the definition from the numpy documentation of", "tokens": [639, 307, 264, 7123, 490, 264, 1031, 8200, 14333, 295], "temperature": 0.0, "avg_logprob": -0.2476579189300537, "compression_ratio": 1.5286624203821657, "no_speech_prob": 2.0261331883375533e-06}, {"id": 639, "seek": 313326, "start": 3157.98, "end": 3161.78, "text": " Broadcasting and I'm going to come back to it in a moment rather than reading it now", "tokens": [14074, 48860, 293, 286, 478, 516, 281, 808, 646, 281, 309, 294, 257, 1623, 2831, 813, 3760, 309, 586], "temperature": 0.0, "avg_logprob": -0.2476579189300537, "compression_ratio": 1.5286624203821657, "no_speech_prob": 2.0261331883375533e-06}, {"id": 640, "seek": 316178, "start": 3161.78, "end": 3163.46, "text": " but", "tokens": [457], "temperature": 0.0, "avg_logprob": -0.3100363511305589, "compression_ratio": 1.5182481751824817, "no_speech_prob": 1.2482674947023042e-06}, {"id": 641, "seek": 316178, "start": 3163.46, "end": 3165.26, "text": " Let's start", "tokens": [961, 311, 722], "temperature": 0.0, "avg_logprob": -0.3100363511305589, "compression_ratio": 1.5182481751824817, "no_speech_prob": 1.2482674947023042e-06}, {"id": 642, "seek": 316178, "start": 3165.26, "end": 3167.26, "text": " By looking an example of broadcasting", "tokens": [3146, 1237, 364, 1365, 295, 30024], "temperature": 0.0, "avg_logprob": -0.3100363511305589, "compression_ratio": 1.5182481751824817, "no_speech_prob": 1.2482674947023042e-06}, {"id": 643, "seek": 316178, "start": 3168.86, "end": 3170.86, "text": " So a is a", "tokens": [407, 257, 307, 257], "temperature": 0.0, "avg_logprob": -0.3100363511305589, "compression_ratio": 1.5182481751824817, "no_speech_prob": 1.2482674947023042e-06}, {"id": 644, "seek": 316178, "start": 3172.1400000000003, "end": 3173.82, "text": " Array", "tokens": [1587, 3458], "temperature": 0.0, "avg_logprob": -0.3100363511305589, "compression_ratio": 1.5182481751824817, "no_speech_prob": 1.2482674947023042e-06}, {"id": 645, "seek": 316178, "start": 3173.82, "end": 3179.02, "text": " With one dimension also known as a rank one tensor also known as a vector", "tokens": [2022, 472, 10139, 611, 2570, 382, 257, 6181, 472, 40863, 611, 2570, 382, 257, 8062], "temperature": 0.0, "avg_logprob": -0.3100363511305589, "compression_ratio": 1.5182481751824817, "no_speech_prob": 1.2482674947023042e-06}, {"id": 646, "seek": 316178, "start": 3180.86, "end": 3183.86, "text": " We can say a greater than zero", "tokens": [492, 393, 584, 257, 5044, 813, 4018], "temperature": 0.0, "avg_logprob": -0.3100363511305589, "compression_ratio": 1.5182481751824817, "no_speech_prob": 1.2482674947023042e-06}, {"id": 647, "seek": 316178, "start": 3185.1800000000003, "end": 3187.6600000000003, "text": " so here we have a", "tokens": [370, 510, 321, 362, 257], "temperature": 0.0, "avg_logprob": -0.3100363511305589, "compression_ratio": 1.5182481751824817, "no_speech_prob": 1.2482674947023042e-06}, {"id": 648, "seek": 318766, "start": 3187.66, "end": 3195.14, "text": " rank one tensor right and a rank zero tensor", "tokens": [6181, 472, 40863, 558, 293, 257, 6181, 4018, 40863], "temperature": 0.0, "avg_logprob": -0.20639249440786, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.570794645696878e-07}, {"id": 649, "seek": 318766, "start": 3196.5, "end": 3203.94, "text": " Right a rank zero tensor is also called a scalar rank one tensor is also called a vector and", "tokens": [1779, 257, 6181, 4018, 40863, 307, 611, 1219, 257, 39684, 6181, 472, 40863, 307, 611, 1219, 257, 8062, 293], "temperature": 0.0, "avg_logprob": -0.20639249440786, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.570794645696878e-07}, {"id": 650, "seek": 318766, "start": 3205.74, "end": 3207.8999999999996, "text": " We've got an operation between the two", "tokens": [492, 600, 658, 364, 6916, 1296, 264, 732], "temperature": 0.0, "avg_logprob": -0.20639249440786, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.570794645696878e-07}, {"id": 651, "seek": 318766, "start": 3208.66, "end": 3215.46, "text": " All right now you've probably done it a thousand times without even noticing that's kind of weird right that you've got these things of different", "tokens": [1057, 558, 586, 291, 600, 1391, 1096, 309, 257, 4714, 1413, 1553, 754, 21814, 300, 311, 733, 295, 3657, 558, 300, 291, 600, 658, 613, 721, 295, 819], "temperature": 0.0, "avg_logprob": -0.20639249440786, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.570794645696878e-07}, {"id": 652, "seek": 321546, "start": 3215.46, "end": 3219.82, "text": " Ranks and different sizes, so what is it actually doing right?", "tokens": [497, 14592, 293, 819, 11602, 11, 370, 437, 307, 309, 767, 884, 558, 30], "temperature": 0.0, "avg_logprob": -0.192777224949428, "compression_ratio": 1.8481675392670156, "no_speech_prob": 1.3287739193401649e-06}, {"id": 653, "seek": 321546, "start": 3219.82, "end": 3225.7400000000002, "text": " But what it's actually doing is it's taking that scalar and copying it here here here", "tokens": [583, 437, 309, 311, 767, 884, 307, 309, 311, 1940, 300, 39684, 293, 27976, 309, 510, 510, 510], "temperature": 0.0, "avg_logprob": -0.192777224949428, "compression_ratio": 1.8481675392670156, "no_speech_prob": 1.3287739193401649e-06}, {"id": 654, "seek": 321546, "start": 3227.46, "end": 3230.1, "text": " Right and then it's actually going element wise", "tokens": [1779, 293, 550, 309, 311, 767, 516, 4478, 10829], "temperature": 0.0, "avg_logprob": -0.192777224949428, "compression_ratio": 1.8481675392670156, "no_speech_prob": 1.3287739193401649e-06}, {"id": 655, "seek": 321546, "start": 3231.78, "end": 3233.78, "text": " Ten is greater than zero", "tokens": [9380, 307, 5044, 813, 4018], "temperature": 0.0, "avg_logprob": -0.192777224949428, "compression_ratio": 1.8481675392670156, "no_speech_prob": 1.3287739193401649e-06}, {"id": 656, "seek": 321546, "start": 3235.2200000000003, "end": 3240.86, "text": " Six is greater than zero minus four is greater than zero even giving us back the three answers", "tokens": [11678, 307, 5044, 813, 4018, 3175, 1451, 307, 5044, 813, 4018, 754, 2902, 505, 646, 264, 1045, 6338], "temperature": 0.0, "avg_logprob": -0.192777224949428, "compression_ratio": 1.8481675392670156, "no_speech_prob": 1.3287739193401649e-06}, {"id": 657, "seek": 321546, "start": 3241.3, "end": 3243.3, "text": " Right and that's called broadcasting", "tokens": [1779, 293, 300, 311, 1219, 30024], "temperature": 0.0, "avg_logprob": -0.192777224949428, "compression_ratio": 1.8481675392670156, "no_speech_prob": 1.3287739193401649e-06}, {"id": 658, "seek": 324330, "start": 3243.3, "end": 3245.3, "text": " broadcasting means", "tokens": [30024, 1355], "temperature": 0.0, "avg_logprob": -0.19285948617117746, "compression_ratio": 1.5229885057471264, "no_speech_prob": 1.4367456060426775e-06}, {"id": 659, "seek": 324330, "start": 3246.7400000000002, "end": 3251.0600000000004, "text": " Copying one or more axes of my tensor", "tokens": [11579, 1840, 472, 420, 544, 35387, 295, 452, 40863], "temperature": 0.0, "avg_logprob": -0.19285948617117746, "compression_ratio": 1.5229885057471264, "no_speech_prob": 1.4367456060426775e-06}, {"id": 660, "seek": 324330, "start": 3251.86, "end": 3256.6600000000003, "text": " To allow it to be the same shape as the other tensor", "tokens": [1407, 2089, 309, 281, 312, 264, 912, 3909, 382, 264, 661, 40863], "temperature": 0.0, "avg_logprob": -0.19285948617117746, "compression_ratio": 1.5229885057471264, "no_speech_prob": 1.4367456060426775e-06}, {"id": 661, "seek": 324330, "start": 3258.5800000000004, "end": 3260.5800000000004, "text": " It doesn't really copy it though", "tokens": [467, 1177, 380, 534, 5055, 309, 1673], "temperature": 0.0, "avg_logprob": -0.19285948617117746, "compression_ratio": 1.5229885057471264, "no_speech_prob": 1.4367456060426775e-06}, {"id": 662, "seek": 324330, "start": 3261.7400000000002, "end": 3266.26, "text": " What it actually does is it stores this kind of internal?", "tokens": [708, 309, 767, 775, 307, 309, 9512, 341, 733, 295, 6920, 30], "temperature": 0.0, "avg_logprob": -0.19285948617117746, "compression_ratio": 1.5229885057471264, "no_speech_prob": 1.4367456060426775e-06}, {"id": 663, "seek": 324330, "start": 3266.7400000000002, "end": 3271.94, "text": " Indicator that says pretend that this is a vector of three zeros", "tokens": [2333, 299, 1639, 300, 1619, 11865, 300, 341, 307, 257, 8062, 295, 1045, 35193], "temperature": 0.0, "avg_logprob": -0.19285948617117746, "compression_ratio": 1.5229885057471264, "no_speech_prob": 1.4367456060426775e-06}, {"id": 664, "seek": 327194, "start": 3271.94, "end": 3278.58, "text": " But it actually just like with rather than kind of going to the next row or going to the next scalar it goes back", "tokens": [583, 309, 767, 445, 411, 365, 2831, 813, 733, 295, 516, 281, 264, 958, 5386, 420, 516, 281, 264, 958, 39684, 309, 1709, 646], "temperature": 0.0, "avg_logprob": -0.2164900885687934, "compression_ratio": 1.6228070175438596, "no_speech_prob": 1.2878937241111998e-06}, {"id": 665, "seek": 327194, "start": 3278.86, "end": 3285.46, "text": " To where it came from if you're interested in learning about this specifically it's they set the stride on that axis", "tokens": [1407, 689, 309, 1361, 490, 498, 291, 434, 3102, 294, 2539, 466, 341, 4682, 309, 311, 436, 992, 264, 1056, 482, 322, 300, 10298], "temperature": 0.0, "avg_logprob": -0.2164900885687934, "compression_ratio": 1.6228070175438596, "no_speech_prob": 1.2878937241111998e-06}, {"id": 666, "seek": 327194, "start": 3285.94, "end": 3290.34, "text": " To be zero that's a minor advanced concept for those who are curious", "tokens": [1407, 312, 4018, 300, 311, 257, 6696, 7339, 3410, 337, 729, 567, 366, 6369], "temperature": 0.0, "avg_logprob": -0.2164900885687934, "compression_ratio": 1.6228070175438596, "no_speech_prob": 1.2878937241111998e-06}, {"id": 667, "seek": 327194, "start": 3291.82, "end": 3294.06, "text": " So we could do a", "tokens": [407, 321, 727, 360, 257], "temperature": 0.0, "avg_logprob": -0.2164900885687934, "compression_ratio": 1.6228070175438596, "no_speech_prob": 1.2878937241111998e-06}, {"id": 668, "seek": 327194, "start": 3295.46, "end": 3299.2400000000002, "text": " Plus one right it's going to broadcast the scalar one", "tokens": [7721, 472, 558, 309, 311, 516, 281, 9975, 264, 39684, 472], "temperature": 0.0, "avg_logprob": -0.2164900885687934, "compression_ratio": 1.6228070175438596, "no_speech_prob": 1.2878937241111998e-06}, {"id": 669, "seek": 329924, "start": 3299.24, "end": 3303.04, "text": " To be one one one and then do element wise addition", "tokens": [1407, 312, 472, 472, 472, 293, 550, 360, 4478, 10829, 4500], "temperature": 0.0, "avg_logprob": -0.2360454269602329, "compression_ratio": 1.896341463414634, "no_speech_prob": 1.3709554878005292e-06}, {"id": 670, "seek": 329924, "start": 3304.0, "end": 3307.1, "text": " We could do the same with a matrix right here's our matrix", "tokens": [492, 727, 360, 264, 912, 365, 257, 8141, 558, 510, 311, 527, 8141], "temperature": 0.0, "avg_logprob": -0.2360454269602329, "compression_ratio": 1.896341463414634, "no_speech_prob": 1.3709554878005292e-06}, {"id": 671, "seek": 329924, "start": 3307.4799999999996, "end": 3314.0, "text": " two times the matrix is going to broadcast two to be two two two two two two two two two two two and", "tokens": [732, 1413, 264, 8141, 307, 516, 281, 9975, 732, 281, 312, 732, 732, 732, 732, 732, 732, 732, 732, 732, 732, 732, 293], "temperature": 0.0, "avg_logprob": -0.2360454269602329, "compression_ratio": 1.896341463414634, "no_speech_prob": 1.3709554878005292e-06}, {"id": 672, "seek": 329924, "start": 3314.52, "end": 3316.52, "text": " Then do element wise", "tokens": [1396, 360, 4478, 10829], "temperature": 0.0, "avg_logprob": -0.2360454269602329, "compression_ratio": 1.896341463414634, "no_speech_prob": 1.3709554878005292e-06}, {"id": 673, "seek": 329924, "start": 3317.4399999999996, "end": 3321.9599999999996, "text": " multiplication right so that's our kind of most simple version of", "tokens": [27290, 558, 370, 300, 311, 527, 733, 295, 881, 2199, 3037, 295], "temperature": 0.0, "avg_logprob": -0.2360454269602329, "compression_ratio": 1.896341463414634, "no_speech_prob": 1.3709554878005292e-06}, {"id": 674, "seek": 329924, "start": 3324.12, "end": 3326.12, "text": " Broadcasting", "tokens": [14074, 48860], "temperature": 0.0, "avg_logprob": -0.2360454269602329, "compression_ratio": 1.896341463414634, "no_speech_prob": 1.3709554878005292e-06}, {"id": 675, "seek": 332612, "start": 3326.12, "end": 3330.52, "text": " So here's a slightly more complex version of broadcasting", "tokens": [407, 510, 311, 257, 4748, 544, 3997, 3037, 295, 30024], "temperature": 0.0, "avg_logprob": -0.2404419051276313, "compression_ratio": 1.5197740112994351, "no_speech_prob": 1.8738672906692955e-06}, {"id": 676, "seek": 332612, "start": 3331.2799999999997, "end": 3336.24, "text": " Here's an array called C right so this is a rank one tensor and", "tokens": [1692, 311, 364, 10225, 1219, 383, 558, 370, 341, 307, 257, 6181, 472, 40863, 293], "temperature": 0.0, "avg_logprob": -0.2404419051276313, "compression_ratio": 1.5197740112994351, "no_speech_prob": 1.8738672906692955e-06}, {"id": 677, "seek": 332612, "start": 3337.0, "end": 3339.0, "text": " Here's our matrix M from before", "tokens": [1692, 311, 527, 8141, 376, 490, 949], "temperature": 0.0, "avg_logprob": -0.2404419051276313, "compression_ratio": 1.5197740112994351, "no_speech_prob": 1.8738672906692955e-06}, {"id": 678, "seek": 332612, "start": 3339.92, "end": 3343.64, "text": " Our rank two tensor we can add M plus C", "tokens": [2621, 6181, 732, 40863, 321, 393, 909, 376, 1804, 383], "temperature": 0.0, "avg_logprob": -0.2404419051276313, "compression_ratio": 1.5197740112994351, "no_speech_prob": 1.8738672906692955e-06}, {"id": 679, "seek": 332612, "start": 3344.16, "end": 3346.16, "text": " Right so what's going on here?", "tokens": [1779, 370, 437, 311, 516, 322, 510, 30], "temperature": 0.0, "avg_logprob": -0.2404419051276313, "compression_ratio": 1.5197740112994351, "no_speech_prob": 1.8738672906692955e-06}, {"id": 680, "seek": 334616, "start": 3346.16, "end": 3355.08, "text": " one two three four five six seven eight nine", "tokens": [472, 732, 1045, 1451, 1732, 2309, 3407, 3180, 4949], "temperature": 0.0, "avg_logprob": -0.33578706670690467, "compression_ratio": 1.4375, "no_speech_prob": 1.87386706329562e-06}, {"id": 681, "seek": 334616, "start": 3356.8399999999997, "end": 3360.48, "text": " That's M right and then C", "tokens": [663, 311, 376, 558, 293, 550, 383], "temperature": 0.0, "avg_logprob": -0.33578706670690467, "compression_ratio": 1.4375, "no_speech_prob": 1.87386706329562e-06}, {"id": 682, "seek": 334616, "start": 3362.0, "end": 3364.0, "text": " ten", "tokens": [2064], "temperature": 0.0, "avg_logprob": -0.33578706670690467, "compression_ratio": 1.4375, "no_speech_prob": 1.87386706329562e-06}, {"id": 683, "seek": 334616, "start": 3364.96, "end": 3366.96, "text": " Twenty thirty", "tokens": [28789, 11790], "temperature": 0.0, "avg_logprob": -0.33578706670690467, "compression_ratio": 1.4375, "no_speech_prob": 1.87386706329562e-06}, {"id": 684, "seek": 334616, "start": 3367.08, "end": 3371.06, "text": " You can see that what it's done is to add that to each row", "tokens": [509, 393, 536, 300, 437, 309, 311, 1096, 307, 281, 909, 300, 281, 1184, 5386], "temperature": 0.0, "avg_logprob": -0.33578706670690467, "compression_ratio": 1.4375, "no_speech_prob": 1.87386706329562e-06}, {"id": 685, "seek": 334616, "start": 3371.92, "end": 3374.12, "text": " Right eleven twenty two thirty three", "tokens": [1779, 21090, 7699, 732, 11790, 1045], "temperature": 0.0, "avg_logprob": -0.33578706670690467, "compression_ratio": 1.4375, "no_speech_prob": 1.87386706329562e-06}, {"id": 686, "seek": 337412, "start": 3374.12, "end": 3376.12, "text": " fourteen", "tokens": [32253], "temperature": 0.0, "avg_logprob": -0.32088610529899597, "compression_ratio": 1.4746835443037976, "no_speech_prob": 2.295906824656413e-06}, {"id": 687, "seek": 337412, "start": 3376.64, "end": 3382.52, "text": " 2536 and so we can kind of figure it seems to have done the same kind of idea as broadcasting a scalar", "tokens": [3552, 11309, 293, 370, 321, 393, 733, 295, 2573, 309, 2544, 281, 362, 1096, 264, 912, 733, 295, 1558, 382, 30024, 257, 39684], "temperature": 0.0, "avg_logprob": -0.32088610529899597, "compression_ratio": 1.4746835443037976, "no_speech_prob": 2.295906824656413e-06}, {"id": 688, "seek": 337412, "start": 3382.72, "end": 3384.72, "text": " It's like made copies of it", "tokens": [467, 311, 411, 1027, 14341, 295, 309], "temperature": 0.0, "avg_logprob": -0.32088610529899597, "compression_ratio": 1.4746835443037976, "no_speech_prob": 2.295906824656413e-06}, {"id": 689, "seek": 337412, "start": 3388.96, "end": 3391.64, "text": " And then it treats those", "tokens": [400, 550, 309, 19566, 729], "temperature": 0.0, "avg_logprob": -0.32088610529899597, "compression_ratio": 1.4746835443037976, "no_speech_prob": 2.295906824656413e-06}, {"id": 690, "seek": 337412, "start": 3397.2799999999997, "end": 3401.04, "text": " As if it's a rank two matrix and now we can do element wise addition", "tokens": [1018, 498, 309, 311, 257, 6181, 732, 8141, 293, 586, 321, 393, 360, 4478, 10829, 4500], "temperature": 0.0, "avg_logprob": -0.32088610529899597, "compression_ratio": 1.4746835443037976, "no_speech_prob": 2.295906824656413e-06}, {"id": 691, "seek": 340104, "start": 3401.04, "end": 3408.18, "text": " Does that make sense now that's yes, can can you pass that Devon over there? Thank you", "tokens": [4402, 300, 652, 2020, 586, 300, 311, 2086, 11, 393, 393, 291, 1320, 300, 9096, 266, 670, 456, 30, 1044, 291], "temperature": 0.0, "avg_logprob": -0.2704688665029165, "compression_ratio": 1.4970760233918128, "no_speech_prob": 3.041506261070026e-06}, {"id": 692, "seek": 340104, "start": 3410.0, "end": 3415.7, "text": " So as it's like by looking at this example it like copies it down", "tokens": [407, 382, 309, 311, 411, 538, 1237, 412, 341, 1365, 309, 411, 14341, 309, 760], "temperature": 0.0, "avg_logprob": -0.2704688665029165, "compression_ratio": 1.4970760233918128, "no_speech_prob": 3.041506261070026e-06}, {"id": 693, "seek": 340104, "start": 3416.48, "end": 3418.4, "text": " Making new rows", "tokens": [14595, 777, 13241], "temperature": 0.0, "avg_logprob": -0.2704688665029165, "compression_ratio": 1.4970760233918128, "no_speech_prob": 3.041506261070026e-06}, {"id": 694, "seek": 340104, "start": 3418.4, "end": 3422.7599999999998, "text": " So how would we want to do it if we wanted to get new columns? I'm so glad you asked", "tokens": [407, 577, 576, 321, 528, 281, 360, 309, 498, 321, 1415, 281, 483, 777, 13766, 30, 286, 478, 370, 5404, 291, 2351], "temperature": 0.0, "avg_logprob": -0.2704688665029165, "compression_ratio": 1.4970760233918128, "no_speech_prob": 3.041506261070026e-06}, {"id": 695, "seek": 340104, "start": 3425.2799999999997, "end": 3427.2799999999997, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.2704688665029165, "compression_ratio": 1.4970760233918128, "no_speech_prob": 3.041506261070026e-06}, {"id": 696, "seek": 342728, "start": 3427.28, "end": 3434.6800000000003, "text": " Instead we would do this", "tokens": [7156, 321, 576, 360, 341], "temperature": 0.0, "avg_logprob": -0.2714775432239879, "compression_ratio": 1.528, "no_speech_prob": 5.0146763896918856e-06}, {"id": 697, "seek": 342728, "start": 3438.44, "end": 3440.44, "text": " Ten twenty thirty", "tokens": [9380, 7699, 11790], "temperature": 0.0, "avg_logprob": -0.2714775432239879, "compression_ratio": 1.528, "no_speech_prob": 5.0146763896918856e-06}, {"id": 698, "seek": 342728, "start": 3441.32, "end": 3446.32, "text": " All right, and then copy that ten twenty thirty ten", "tokens": [1057, 558, 11, 293, 550, 5055, 300, 2064, 7699, 11790, 2064], "temperature": 0.0, "avg_logprob": -0.2714775432239879, "compression_ratio": 1.528, "no_speech_prob": 5.0146763896918856e-06}, {"id": 699, "seek": 342728, "start": 3447.48, "end": 3449.4, "text": " twenty thirty and", "tokens": [7699, 11790, 293], "temperature": 0.0, "avg_logprob": -0.2714775432239879, "compression_ratio": 1.528, "no_speech_prob": 5.0146763896918856e-06}, {"id": 700, "seek": 342728, "start": 3449.4, "end": 3451.4, "text": " Now treat that as our matrix", "tokens": [823, 2387, 300, 382, 527, 8141], "temperature": 0.0, "avg_logprob": -0.2714775432239879, "compression_ratio": 1.528, "no_speech_prob": 5.0146763896918856e-06}, {"id": 701, "seek": 345140, "start": 3451.4, "end": 3457.6, "text": " So to get numpy to do that we need to not pass in a vector", "tokens": [407, 281, 483, 1031, 8200, 281, 360, 300, 321, 643, 281, 406, 1320, 294, 257, 8062], "temperature": 0.0, "avg_logprob": -0.25993541308811735, "compression_ratio": 1.408, "no_speech_prob": 8.446195352007635e-07}, {"id": 702, "seek": 345140, "start": 3458.7200000000003, "end": 3460.7200000000003, "text": " but to pass in a", "tokens": [457, 281, 1320, 294, 257], "temperature": 0.0, "avg_logprob": -0.25993541308811735, "compression_ratio": 1.408, "no_speech_prob": 8.446195352007635e-07}, {"id": 703, "seek": 345140, "start": 3462.28, "end": 3466.46, "text": " Matrix with one column a rank two tensor", "tokens": [36274, 365, 472, 7738, 257, 6181, 732, 40863], "temperature": 0.0, "avg_logprob": -0.25993541308811735, "compression_ratio": 1.408, "no_speech_prob": 8.446195352007635e-07}, {"id": 704, "seek": 345140, "start": 3468.1600000000003, "end": 3470.84, "text": " So basically it turns out that", "tokens": [407, 1936, 309, 4523, 484, 300], "temperature": 0.0, "avg_logprob": -0.25993541308811735, "compression_ratio": 1.408, "no_speech_prob": 8.446195352007635e-07}, {"id": 705, "seek": 345140, "start": 3472.2000000000003, "end": 3474.36, "text": " NumPy is going to think of a", "tokens": [22592, 47, 88, 307, 516, 281, 519, 295, 257], "temperature": 0.0, "avg_logprob": -0.25993541308811735, "compression_ratio": 1.408, "no_speech_prob": 8.446195352007635e-07}, {"id": 706, "seek": 347436, "start": 3474.36, "end": 3481.4, "text": " Rank one tensor for these purposes as if it was a rank two tensor which represents a row", "tokens": [35921, 472, 40863, 337, 613, 9932, 382, 498, 309, 390, 257, 6181, 732, 40863, 597, 8855, 257, 5386], "temperature": 0.0, "avg_logprob": -0.2316679411296603, "compression_ratio": 1.6162162162162161, "no_speech_prob": 9.721512697069556e-07}, {"id": 707, "seek": 347436, "start": 3482.1200000000003, "end": 3484.92, "text": " Right so in other words that it is one by three", "tokens": [1779, 370, 294, 661, 2283, 300, 309, 307, 472, 538, 1045], "temperature": 0.0, "avg_logprob": -0.2316679411296603, "compression_ratio": 1.6162162162162161, "no_speech_prob": 9.721512697069556e-07}, {"id": 708, "seek": 347436, "start": 3485.6, "end": 3490.1600000000003, "text": " Right so we want to create a tensor which is three by one", "tokens": [1779, 370, 321, 528, 281, 1884, 257, 40863, 597, 307, 1045, 538, 472], "temperature": 0.0, "avg_logprob": -0.2316679411296603, "compression_ratio": 1.6162162162162161, "no_speech_prob": 9.721512697069556e-07}, {"id": 709, "seek": 347436, "start": 3492.0, "end": 3494.0, "text": " There's a couple of ways to do that", "tokens": [821, 311, 257, 1916, 295, 2098, 281, 360, 300], "temperature": 0.0, "avg_logprob": -0.2316679411296603, "compression_ratio": 1.6162162162162161, "no_speech_prob": 9.721512697069556e-07}, {"id": 710, "seek": 347436, "start": 3494.48, "end": 3500.7200000000003, "text": " One is to use NP dot expandims and if you then pass in this argument", "tokens": [1485, 307, 281, 764, 38611, 5893, 5268, 18857, 293, 498, 291, 550, 1320, 294, 341, 6770], "temperature": 0.0, "avg_logprob": -0.2316679411296603, "compression_ratio": 1.6162162162162161, "no_speech_prob": 9.721512697069556e-07}, {"id": 711, "seek": 350072, "start": 3500.72, "end": 3508.2, "text": " It says please insert a length one axis here, please so in our case we want to turn it into a", "tokens": [467, 1619, 1767, 8969, 257, 4641, 472, 10298, 510, 11, 1767, 370, 294, 527, 1389, 321, 528, 281, 1261, 309, 666, 257], "temperature": 0.0, "avg_logprob": -0.1842600665514982, "compression_ratio": 1.7218934911242603, "no_speech_prob": 3.089486199314706e-06}, {"id": 712, "seek": 350072, "start": 3509.08, "end": 3513.0, "text": " Three by one so if we said expand in C comma one", "tokens": [6244, 538, 472, 370, 498, 321, 848, 5268, 294, 383, 22117, 472], "temperature": 0.0, "avg_logprob": -0.1842600665514982, "compression_ratio": 1.7218934911242603, "no_speech_prob": 3.089486199314706e-06}, {"id": 713, "seek": 350072, "start": 3519.4399999999996, "end": 3524.54, "text": " Okay, so if we say expand in C comma one it changes the shape to three comma one", "tokens": [1033, 11, 370, 498, 321, 584, 5268, 294, 383, 22117, 472, 309, 2962, 264, 3909, 281, 1045, 22117, 472], "temperature": 0.0, "avg_logprob": -0.1842600665514982, "compression_ratio": 1.7218934911242603, "no_speech_prob": 3.089486199314706e-06}, {"id": 714, "seek": 350072, "start": 3525.04, "end": 3527.04, "text": " So if we look at what that looks like", "tokens": [407, 498, 321, 574, 412, 437, 300, 1542, 411], "temperature": 0.0, "avg_logprob": -0.1842600665514982, "compression_ratio": 1.7218934911242603, "no_speech_prob": 3.089486199314706e-06}, {"id": 715, "seek": 352704, "start": 3527.04, "end": 3532.4, "text": " That looks like a column okay, so if we now go", "tokens": [663, 1542, 411, 257, 7738, 1392, 11, 370, 498, 321, 586, 352], "temperature": 0.0, "avg_logprob": -0.22786616040514662, "compression_ratio": 1.7151898734177216, "no_speech_prob": 5.896415586903458e-07}, {"id": 716, "seek": 352704, "start": 3533.04, "end": 3534.32, "text": " that", "tokens": [300], "temperature": 0.0, "avg_logprob": -0.22786616040514662, "compression_ratio": 1.7151898734177216, "no_speech_prob": 5.896415586903458e-07}, {"id": 717, "seek": 352704, "start": 3534.32, "end": 3535.8, "text": " Plus M", "tokens": [7721, 376], "temperature": 0.0, "avg_logprob": -0.22786616040514662, "compression_ratio": 1.7151898734177216, "no_speech_prob": 5.896415586903458e-07}, {"id": 718, "seek": 352704, "start": 3535.8, "end": 3539.0, "text": " You can see it's doing exactly what we hoped it would do", "tokens": [509, 393, 536, 309, 311, 884, 2293, 437, 321, 19737, 309, 576, 360], "temperature": 0.0, "avg_logprob": -0.22786616040514662, "compression_ratio": 1.7151898734177216, "no_speech_prob": 5.896415586903458e-07}, {"id": 719, "seek": 352704, "start": 3540.6, "end": 3543.6, "text": " All right, which is to add ten twenty thirty to the column", "tokens": [1057, 558, 11, 597, 307, 281, 909, 2064, 7699, 11790, 281, 264, 7738], "temperature": 0.0, "avg_logprob": -0.22786616040514662, "compression_ratio": 1.7151898734177216, "no_speech_prob": 5.896415586903458e-07}, {"id": 720, "seek": 352704, "start": 3544.32, "end": 3548.2799999999997, "text": " Ten twenty thirty to the column ten twenty thirty to the column", "tokens": [9380, 7699, 11790, 281, 264, 7738, 2064, 7699, 11790, 281, 264, 7738], "temperature": 0.0, "avg_logprob": -0.22786616040514662, "compression_ratio": 1.7151898734177216, "no_speech_prob": 5.896415586903458e-07}, {"id": 721, "seek": 352704, "start": 3548.92, "end": 3550.2, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.22786616040514662, "compression_ratio": 1.7151898734177216, "no_speech_prob": 5.896415586903458e-07}, {"id": 722, "seek": 352704, "start": 3550.2, "end": 3553.04, "text": " now because the location of", "tokens": [586, 570, 264, 4914, 295], "temperature": 0.0, "avg_logprob": -0.22786616040514662, "compression_ratio": 1.7151898734177216, "no_speech_prob": 5.896415586903458e-07}, {"id": 723, "seek": 355304, "start": 3553.04, "end": 3557.08, "text": " A unit axis turns out to be so important", "tokens": [316, 4985, 10298, 4523, 484, 281, 312, 370, 1021], "temperature": 0.0, "avg_logprob": -0.14905200206058125, "compression_ratio": 1.6292134831460674, "no_speech_prob": 6.375537964231626e-07}, {"id": 724, "seek": 355304, "start": 3560.6, "end": 3567.84, "text": " It's really helpful to kind of experiment with creating these extra unit axes and know how to do it easily and", "tokens": [467, 311, 534, 4961, 281, 733, 295, 5120, 365, 4084, 613, 2857, 4985, 35387, 293, 458, 577, 281, 360, 309, 3612, 293], "temperature": 0.0, "avg_logprob": -0.14905200206058125, "compression_ratio": 1.6292134831460674, "no_speech_prob": 6.375537964231626e-07}, {"id": 725, "seek": 355304, "start": 3568.08, "end": 3573.42, "text": " NP dot expandims isn't in my opinion the easiest way to do this the easiest way", "tokens": [38611, 5893, 5268, 18857, 1943, 380, 294, 452, 4800, 264, 12889, 636, 281, 360, 341, 264, 12889, 636], "temperature": 0.0, "avg_logprob": -0.14905200206058125, "compression_ratio": 1.6292134831460674, "no_speech_prob": 6.375537964231626e-07}, {"id": 726, "seek": 355304, "start": 3575.68, "end": 3580.08, "text": " The easiest way is to index into the tensor with a special", "tokens": [440, 12889, 636, 307, 281, 8186, 666, 264, 40863, 365, 257, 2121], "temperature": 0.0, "avg_logprob": -0.14905200206058125, "compression_ratio": 1.6292134831460674, "no_speech_prob": 6.375537964231626e-07}, {"id": 727, "seek": 358008, "start": 3580.08, "end": 3582.08, "text": " index", "tokens": [8186], "temperature": 0.0, "avg_logprob": -0.2969100201716188, "compression_ratio": 1.8205128205128205, "no_speech_prob": 9.874620445771143e-07}, {"id": 728, "seek": 358008, "start": 3582.12, "end": 3589.22, "text": " None and what none does is it creates a new axis in that location of", "tokens": [14492, 293, 437, 6022, 775, 307, 309, 7829, 257, 777, 10298, 294, 300, 4914, 295], "temperature": 0.0, "avg_logprob": -0.2969100201716188, "compression_ratio": 1.8205128205128205, "no_speech_prob": 9.874620445771143e-07}, {"id": 729, "seek": 358008, "start": 3590.0, "end": 3593.64, "text": " length one right so this is", "tokens": [4641, 472, 558, 370, 341, 307], "temperature": 0.0, "avg_logprob": -0.2969100201716188, "compression_ratio": 1.8205128205128205, "no_speech_prob": 9.874620445771143e-07}, {"id": 730, "seek": 358008, "start": 3594.64, "end": 3598.48, "text": " Going to add a new axis at the start of length one", "tokens": [10963, 281, 909, 257, 777, 10298, 412, 264, 722, 295, 4641, 472], "temperature": 0.0, "avg_logprob": -0.2969100201716188, "compression_ratio": 1.8205128205128205, "no_speech_prob": 9.874620445771143e-07}, {"id": 731, "seek": 358008, "start": 3602.88, "end": 3608.0, "text": " This is going to add a new axis at the end of length one or", "tokens": [639, 307, 516, 281, 909, 257, 777, 10298, 412, 264, 917, 295, 4641, 472, 420], "temperature": 0.0, "avg_logprob": -0.2969100201716188, "compression_ratio": 1.8205128205128205, "no_speech_prob": 9.874620445771143e-07}, {"id": 732, "seek": 360800, "start": 3608.0, "end": 3610.0, "text": " Why not do both?", "tokens": [1545, 406, 360, 1293, 30], "temperature": 0.0, "avg_logprob": -0.18897917300839967, "compression_ratio": 1.486910994764398, "no_speech_prob": 7.811472073626646e-07}, {"id": 733, "seek": 360800, "start": 3612.16, "end": 3615.24, "text": " Right so if you think about it like a tensor", "tokens": [1779, 370, 498, 291, 519, 466, 309, 411, 257, 40863], "temperature": 0.0, "avg_logprob": -0.18897917300839967, "compression_ratio": 1.486910994764398, "no_speech_prob": 7.811472073626646e-07}, {"id": 734, "seek": 360800, "start": 3616.0, "end": 3618.0, "text": " Which has like three?", "tokens": [3013, 575, 411, 1045, 30], "temperature": 0.0, "avg_logprob": -0.18897917300839967, "compression_ratio": 1.486910994764398, "no_speech_prob": 7.811472073626646e-07}, {"id": 735, "seek": 360800, "start": 3618.36, "end": 3622.88, "text": " Things in it could be of any rank you like right you can just add", "tokens": [9514, 294, 309, 727, 312, 295, 604, 6181, 291, 411, 558, 291, 393, 445, 909], "temperature": 0.0, "avg_logprob": -0.18897917300839967, "compression_ratio": 1.486910994764398, "no_speech_prob": 7.811472073626646e-07}, {"id": 736, "seek": 360800, "start": 3623.4, "end": 3627.56, "text": " unit axes all over the place and so that way we can kind of", "tokens": [4985, 35387, 439, 670, 264, 1081, 293, 370, 300, 636, 321, 393, 733, 295], "temperature": 0.0, "avg_logprob": -0.18897917300839967, "compression_ratio": 1.486910994764398, "no_speech_prob": 7.811472073626646e-07}, {"id": 737, "seek": 360800, "start": 3628.68, "end": 3632.24, "text": " Decide how we want our broadcasting to work", "tokens": [12427, 482, 577, 321, 528, 527, 30024, 281, 589], "temperature": 0.0, "avg_logprob": -0.18897917300839967, "compression_ratio": 1.486910994764398, "no_speech_prob": 7.811472073626646e-07}, {"id": 738, "seek": 363224, "start": 3632.24, "end": 3637.56, "text": " So there's a pretty convenient thing", "tokens": [407, 456, 311, 257, 1238, 10851, 551], "temperature": 0.0, "avg_logprob": -0.254124505179269, "compression_ratio": 1.746606334841629, "no_speech_prob": 1.3287710771692218e-06}, {"id": 739, "seek": 363224, "start": 3638.2, "end": 3643.68, "text": " In numpy called broadcast to and what that does is it takes our vector and", "tokens": [682, 1031, 8200, 1219, 9975, 281, 293, 437, 300, 775, 307, 309, 2516, 527, 8062, 293], "temperature": 0.0, "avg_logprob": -0.254124505179269, "compression_ratio": 1.746606334841629, "no_speech_prob": 1.3287710771692218e-06}, {"id": 740, "seek": 363224, "start": 3645.16, "end": 3651.3999999999996, "text": " Broadcasts it to that shape and shows us what that would look like right so if you're ever like unsure", "tokens": [14074, 3734, 82, 309, 281, 300, 3909, 293, 3110, 505, 437, 300, 576, 574, 411, 558, 370, 498, 291, 434, 1562, 411, 32486], "temperature": 0.0, "avg_logprob": -0.254124505179269, "compression_ratio": 1.746606334841629, "no_speech_prob": 1.3287710771692218e-06}, {"id": 741, "seek": 363224, "start": 3652.16, "end": 3658.64, "text": " Of what's going on in some broadcasting operation you can say broadcast to and so for example here", "tokens": [2720, 437, 311, 516, 322, 294, 512, 30024, 6916, 291, 393, 584, 9975, 281, 293, 370, 337, 1365, 510], "temperature": 0.0, "avg_logprob": -0.254124505179269, "compression_ratio": 1.746606334841629, "no_speech_prob": 1.3287710771692218e-06}, {"id": 742, "seek": 365864, "start": 3658.64, "end": 3662.02, "text": " We could say like rather than three comma three we could say m dot shape", "tokens": [492, 727, 584, 411, 2831, 813, 1045, 22117, 1045, 321, 727, 584, 275, 5893, 3909], "temperature": 0.0, "avg_logprob": -0.19474618027849896, "compression_ratio": 1.7471264367816093, "no_speech_prob": 1.1015914651579806e-06}, {"id": 743, "seek": 365864, "start": 3663.12, "end": 3669.64, "text": " Right and see exactly what's happening going to happen and so that's what's going to happen before we add it to m", "tokens": [1779, 293, 536, 2293, 437, 311, 2737, 516, 281, 1051, 293, 370, 300, 311, 437, 311, 516, 281, 1051, 949, 321, 909, 309, 281, 275], "temperature": 0.0, "avg_logprob": -0.19474618027849896, "compression_ratio": 1.7471264367816093, "no_speech_prob": 1.1015914651579806e-06}, {"id": 744, "seek": 365864, "start": 3670.04, "end": 3672.04, "text": " Right so if we said", "tokens": [1779, 370, 498, 321, 848], "temperature": 0.0, "avg_logprob": -0.19474618027849896, "compression_ratio": 1.7471264367816093, "no_speech_prob": 1.1015914651579806e-06}, {"id": 745, "seek": 365864, "start": 3675.04, "end": 3677.04, "text": " Turn it into a column", "tokens": [7956, 309, 666, 257, 7738], "temperature": 0.0, "avg_logprob": -0.19474618027849896, "compression_ratio": 1.7471264367816093, "no_speech_prob": 1.1015914651579806e-06}, {"id": 746, "seek": 365864, "start": 3679.52, "end": 3681.52, "text": " That's what that looks like", "tokens": [663, 311, 437, 300, 1542, 411], "temperature": 0.0, "avg_logprob": -0.19474618027849896, "compression_ratio": 1.7471264367816093, "no_speech_prob": 1.1015914651579806e-06}, {"id": 747, "seek": 365864, "start": 3682.04, "end": 3683.56, "text": " Make sense", "tokens": [4387, 2020], "temperature": 0.0, "avg_logprob": -0.19474618027849896, "compression_ratio": 1.7471264367816093, "no_speech_prob": 1.1015914651579806e-06}, {"id": 748, "seek": 365864, "start": 3683.56, "end": 3686.52, "text": " So that's kind of like the intuitive", "tokens": [407, 300, 311, 733, 295, 411, 264, 21769], "temperature": 0.0, "avg_logprob": -0.19474618027849896, "compression_ratio": 1.7471264367816093, "no_speech_prob": 1.1015914651579806e-06}, {"id": 749, "seek": 368652, "start": 3686.52, "end": 3691.96, "text": " Definition of broadcasting and so now hopefully we can go back to that", "tokens": [46245, 849, 295, 30024, 293, 370, 586, 4696, 321, 393, 352, 646, 281, 300], "temperature": 0.0, "avg_logprob": -0.1999201907051934, "compression_ratio": 1.6616161616161615, "no_speech_prob": 3.9897113879305834e-07}, {"id": 750, "seek": 368652, "start": 3692.96, "end": 3694.96, "text": " numpy documentation and understand", "tokens": [1031, 8200, 14333, 293, 1223], "temperature": 0.0, "avg_logprob": -0.1999201907051934, "compression_ratio": 1.6616161616161615, "no_speech_prob": 3.9897113879305834e-07}, {"id": 751, "seek": 368652, "start": 3695.6, "end": 3697.6, "text": " What it means right?", "tokens": [708, 309, 1355, 558, 30], "temperature": 0.0, "avg_logprob": -0.1999201907051934, "compression_ratio": 1.6616161616161615, "no_speech_prob": 3.9897113879305834e-07}, {"id": 752, "seek": 368652, "start": 3698.24, "end": 3702.7599999999998, "text": " Broadcasting describes how numpy is going to treat arrays of different shapes when we do some operation", "tokens": [14074, 48860, 15626, 577, 1031, 8200, 307, 516, 281, 2387, 41011, 295, 819, 10854, 562, 321, 360, 512, 6916], "temperature": 0.0, "avg_logprob": -0.1999201907051934, "compression_ratio": 1.6616161616161615, "no_speech_prob": 3.9897113879305834e-07}, {"id": 753, "seek": 368652, "start": 3702.96, "end": 3710.24, "text": " Right the smaller array is broadcast across the larger array by smaller array they mean lower rank", "tokens": [1779, 264, 4356, 10225, 307, 9975, 2108, 264, 4833, 10225, 538, 4356, 10225, 436, 914, 3126, 6181], "temperature": 0.0, "avg_logprob": -0.1999201907051934, "compression_ratio": 1.6616161616161615, "no_speech_prob": 3.9897113879305834e-07}, {"id": 754, "seek": 371024, "start": 3710.24, "end": 3717.72, "text": " tensor basically broadcast across the light the higher rank tensor so they have compatible shapes it", "tokens": [40863, 1936, 9975, 2108, 264, 1442, 264, 2946, 6181, 40863, 370, 436, 362, 18218, 10854, 309], "temperature": 0.0, "avg_logprob": -0.18817344578829678, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.1365606269464479e-06}, {"id": 755, "seek": 371024, "start": 3718.24, "end": 3725.52, "text": " Vectorizes array operations so vectorizing generally means like using SIMD and stuff like that so that multiple things happen at the same time", "tokens": [691, 20814, 5660, 10225, 7705, 370, 8062, 3319, 5101, 1355, 411, 1228, 24738, 35, 293, 1507, 411, 300, 370, 300, 3866, 721, 1051, 412, 264, 912, 565], "temperature": 0.0, "avg_logprob": -0.18817344578829678, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.1365606269464479e-06}, {"id": 756, "seek": 371024, "start": 3726.7999999999997, "end": 3728.7999999999997, "text": " All the looping occurs in C", "tokens": [1057, 264, 6367, 278, 11843, 294, 383], "temperature": 0.0, "avg_logprob": -0.18817344578829678, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.1365606269464479e-06}, {"id": 757, "seek": 371024, "start": 3729.2, "end": 3735.12, "text": " But it doesn't actually make needless copies of data. It kind of just acts as if it had", "tokens": [583, 309, 1177, 380, 767, 652, 643, 1832, 14341, 295, 1412, 13, 467, 733, 295, 445, 10672, 382, 498, 309, 632], "temperature": 0.0, "avg_logprob": -0.18817344578829678, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.1365606269464479e-06}, {"id": 758, "seek": 371024, "start": 3736.04, "end": 3738.04, "text": " So there's our definition", "tokens": [407, 456, 311, 527, 7123], "temperature": 0.0, "avg_logprob": -0.18817344578829678, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.1365606269464479e-06}, {"id": 759, "seek": 373804, "start": 3738.04, "end": 3744.6, "text": " now in deep learning you very often deal with tensors of rank 4 or more and", "tokens": [586, 294, 2452, 2539, 291, 588, 2049, 2028, 365, 10688, 830, 295, 6181, 1017, 420, 544, 293], "temperature": 0.0, "avg_logprob": -0.17594805885763729, "compression_ratio": 1.646341463414634, "no_speech_prob": 1.1544548215169925e-06}, {"id": 760, "seek": 373804, "start": 3744.96, "end": 3749.08, "text": " you very often combine them with tensors of rank 1 or 2 and", "tokens": [291, 588, 2049, 10432, 552, 365, 10688, 830, 295, 6181, 502, 420, 568, 293], "temperature": 0.0, "avg_logprob": -0.17594805885763729, "compression_ratio": 1.646341463414634, "no_speech_prob": 1.1544548215169925e-06}, {"id": 761, "seek": 373804, "start": 3749.7599999999998, "end": 3756.44, "text": " Trying to just rely on intuition to do that correctly is nearly impossible, so you really need to know the rules", "tokens": [20180, 281, 445, 10687, 322, 24002, 281, 360, 300, 8944, 307, 6217, 6243, 11, 370, 291, 534, 643, 281, 458, 264, 4474], "temperature": 0.0, "avg_logprob": -0.17594805885763729, "compression_ratio": 1.646341463414634, "no_speech_prob": 1.1544548215169925e-06}, {"id": 762, "seek": 373804, "start": 3760.2799999999997, "end": 3762.2799999999997, "text": " So here are the rules", "tokens": [407, 510, 366, 264, 4474], "temperature": 0.0, "avg_logprob": -0.17594805885763729, "compression_ratio": 1.646341463414634, "no_speech_prob": 1.1544548215169925e-06}, {"id": 763, "seek": 376228, "start": 3762.28, "end": 3770.1600000000003, "text": " Okay, here's m dot shape here's C dot shape so the rule are that we're going to compare", "tokens": [1033, 11, 510, 311, 275, 5893, 3909, 510, 311, 383, 5893, 3909, 370, 264, 4978, 366, 300, 321, 434, 516, 281, 6794], "temperature": 0.0, "avg_logprob": -0.24179963085138909, "compression_ratio": 1.7920353982300885, "no_speech_prob": 5.122889206177206e-07}, {"id": 764, "seek": 376228, "start": 3770.52, "end": 3774.7200000000003, "text": " The shapes of our two tensors element wise we're going to look at one at a time", "tokens": [440, 10854, 295, 527, 732, 10688, 830, 4478, 10829, 321, 434, 516, 281, 574, 412, 472, 412, 257, 565], "temperature": 0.0, "avg_logprob": -0.24179963085138909, "compression_ratio": 1.7920353982300885, "no_speech_prob": 5.122889206177206e-07}, {"id": 765, "seek": 376228, "start": 3775.44, "end": 3779.1600000000003, "text": " And we're going to start at the end right so look at the trailing dimensions and", "tokens": [400, 321, 434, 516, 281, 722, 412, 264, 917, 558, 370, 574, 412, 264, 944, 4883, 12819, 293], "temperature": 0.0, "avg_logprob": -0.24179963085138909, "compression_ratio": 1.7920353982300885, "no_speech_prob": 5.122889206177206e-07}, {"id": 766, "seek": 376228, "start": 3779.96, "end": 3781.44, "text": " then go", "tokens": [550, 352], "temperature": 0.0, "avg_logprob": -0.24179963085138909, "compression_ratio": 1.7920353982300885, "no_speech_prob": 5.122889206177206e-07}, {"id": 767, "seek": 376228, "start": 3781.44, "end": 3786.2000000000003, "text": " Towards the front okay, and so two dimensions are going to be compatible", "tokens": [48938, 264, 1868, 1392, 11, 293, 370, 732, 12819, 366, 516, 281, 312, 18218], "temperature": 0.0, "avg_logprob": -0.24179963085138909, "compression_ratio": 1.7920353982300885, "no_speech_prob": 5.122889206177206e-07}, {"id": 768, "seek": 378620, "start": 3786.2, "end": 3792.08, "text": " When one of these two things is true, right so let's check right we've got our", "tokens": [1133, 472, 295, 613, 732, 721, 307, 2074, 11, 558, 370, 718, 311, 1520, 558, 321, 600, 658, 527], "temperature": 0.0, "avg_logprob": -0.26377290667909564, "compression_ratio": 1.7630331753554502, "no_speech_prob": 1.0030122439275146e-06}, {"id": 769, "seek": 378620, "start": 3792.6, "end": 3796.08, "text": " M and C compatible M is 3 3", "tokens": [376, 293, 383, 18218, 376, 307, 805, 805], "temperature": 0.0, "avg_logprob": -0.26377290667909564, "compression_ratio": 1.7630331753554502, "no_speech_prob": 1.0030122439275146e-06}, {"id": 770, "seek": 378620, "start": 3797.16, "end": 3804.08, "text": " C is 3 right so we're going to start at the end trailing dimensions first and check are they compatible?", "tokens": [383, 307, 805, 558, 370, 321, 434, 516, 281, 722, 412, 264, 917, 944, 4883, 12819, 700, 293, 1520, 366, 436, 18218, 30], "temperature": 0.0, "avg_logprob": -0.26377290667909564, "compression_ratio": 1.7630331753554502, "no_speech_prob": 1.0030122439275146e-06}, {"id": 771, "seek": 378620, "start": 3804.2799999999997, "end": 3810.3999999999996, "text": " They're compatible if the dimensions are equal okay, so these ones are equal so they're compatible right?", "tokens": [814, 434, 18218, 498, 264, 12819, 366, 2681, 1392, 11, 370, 613, 2306, 366, 2681, 370, 436, 434, 18218, 558, 30], "temperature": 0.0, "avg_logprob": -0.26377290667909564, "compression_ratio": 1.7630331753554502, "no_speech_prob": 1.0030122439275146e-06}, {"id": 772, "seek": 378620, "start": 3811.16, "end": 3814.2, "text": " All right, let's go to the next one. Oh, we're missing", "tokens": [1057, 558, 11, 718, 311, 352, 281, 264, 958, 472, 13, 876, 11, 321, 434, 5361], "temperature": 0.0, "avg_logprob": -0.26377290667909564, "compression_ratio": 1.7630331753554502, "no_speech_prob": 1.0030122439275146e-06}, {"id": 773, "seek": 381420, "start": 3814.2, "end": 3821.12, "text": " Right C is missing something so what happens if something is missing as we insert a one?", "tokens": [1779, 383, 307, 5361, 746, 370, 437, 2314, 498, 746, 307, 5361, 382, 321, 8969, 257, 472, 30], "temperature": 0.0, "avg_logprob": -0.20840045383998326, "compression_ratio": 1.631578947368421, "no_speech_prob": 2.2252750113693764e-06}, {"id": 774, "seek": 381420, "start": 3822.7999999999997, "end": 3829.3599999999997, "text": " Okay, that's the rule right and so let's now check are these compatible one of them is one yes, they're compatible", "tokens": [1033, 11, 300, 311, 264, 4978, 558, 293, 370, 718, 311, 586, 1520, 366, 613, 18218, 472, 295, 552, 307, 472, 2086, 11, 436, 434, 18218], "temperature": 0.0, "avg_logprob": -0.20840045383998326, "compression_ratio": 1.631578947368421, "no_speech_prob": 2.2252750113693764e-06}, {"id": 775, "seek": 381420, "start": 3830.9199999999996, "end": 3835.3199999999997, "text": " Okay, so now you can see why it is that numpy treats", "tokens": [1033, 11, 370, 586, 291, 393, 536, 983, 309, 307, 300, 1031, 8200, 19566], "temperature": 0.0, "avg_logprob": -0.20840045383998326, "compression_ratio": 1.631578947368421, "no_speech_prob": 2.2252750113693764e-06}, {"id": 776, "seek": 381420, "start": 3836.0, "end": 3837.48, "text": " the one", "tokens": [264, 472], "temperature": 0.0, "avg_logprob": -0.20840045383998326, "compression_ratio": 1.631578947368421, "no_speech_prob": 2.2252750113693764e-06}, {"id": 777, "seek": 381420, "start": 3837.48, "end": 3839.48, "text": " dimensional array as", "tokens": [18795, 10225, 382], "temperature": 0.0, "avg_logprob": -0.20840045383998326, "compression_ratio": 1.631578947368421, "no_speech_prob": 2.2252750113693764e-06}, {"id": 778, "seek": 381420, "start": 3839.52, "end": 3842.1, "text": " If it is a rank 2 tensor", "tokens": [759, 309, 307, 257, 6181, 568, 40863], "temperature": 0.0, "avg_logprob": -0.20840045383998326, "compression_ratio": 1.631578947368421, "no_speech_prob": 2.2252750113693764e-06}, {"id": 779, "seek": 384210, "start": 3842.1, "end": 3848.58, "text": " Which is representing a row it's because we're basically inserting a one at the front", "tokens": [3013, 307, 13460, 257, 5386, 309, 311, 570, 321, 434, 1936, 46567, 257, 472, 412, 264, 1868], "temperature": 0.0, "avg_logprob": -0.18950458765029907, "compression_ratio": 1.5333333333333334, "no_speech_prob": 6.786726203245053e-07}, {"id": 780, "seek": 384210, "start": 3849.42, "end": 3852.14, "text": " Okay, so that's the rule so for example", "tokens": [1033, 11, 370, 300, 311, 264, 4978, 370, 337, 1365], "temperature": 0.0, "avg_logprob": -0.18950458765029907, "compression_ratio": 1.5333333333333334, "no_speech_prob": 6.786726203245053e-07}, {"id": 781, "seek": 384210, "start": 3855.9, "end": 3860.7799999999997, "text": " This is something that you very commonly have to do which is you start with like an", "tokens": [639, 307, 746, 300, 291, 588, 12719, 362, 281, 360, 597, 307, 291, 722, 365, 411, 364], "temperature": 0.0, "avg_logprob": -0.18950458765029907, "compression_ratio": 1.5333333333333334, "no_speech_prob": 6.786726203245053e-07}, {"id": 782, "seek": 384210, "start": 3861.9, "end": 3867.02, "text": " image they're like 256 pixels by 256 pixels by three channels and", "tokens": [3256, 436, 434, 411, 38882, 18668, 538, 38882, 18668, 538, 1045, 9235, 293], "temperature": 0.0, "avg_logprob": -0.18950458765029907, "compression_ratio": 1.5333333333333334, "no_speech_prob": 6.786726203245053e-07}, {"id": 783, "seek": 384210, "start": 3867.8199999999997, "end": 3869.8199999999997, "text": " You want to subtract?", "tokens": [509, 528, 281, 16390, 30], "temperature": 0.0, "avg_logprob": -0.18950458765029907, "compression_ratio": 1.5333333333333334, "no_speech_prob": 6.786726203245053e-07}, {"id": 784, "seek": 386982, "start": 3869.82, "end": 3877.2200000000003, "text": " The mean of each channel right so you've got 256 by 256 by 3 and you want to subtract something of length 3", "tokens": [440, 914, 295, 1184, 2269, 558, 370, 291, 600, 658, 38882, 538, 38882, 538, 805, 293, 291, 528, 281, 16390, 746, 295, 4641, 805], "temperature": 0.0, "avg_logprob": -0.2149266329678622, "compression_ratio": 1.8772727272727272, "no_speech_prob": 2.1568130250670947e-06}, {"id": 785, "seek": 386982, "start": 3877.5, "end": 3879.5, "text": " Right so yeah, you can do that", "tokens": [1779, 370, 1338, 11, 291, 393, 360, 300], "temperature": 0.0, "avg_logprob": -0.2149266329678622, "compression_ratio": 1.8772727272727272, "no_speech_prob": 2.1568130250670947e-06}, {"id": 786, "seek": 386982, "start": 3880.42, "end": 3884.06, "text": " Absolutely because 3 and 3 are compatible because they're the same", "tokens": [7021, 570, 805, 293, 805, 366, 18218, 570, 436, 434, 264, 912], "temperature": 0.0, "avg_logprob": -0.2149266329678622, "compression_ratio": 1.8772727272727272, "no_speech_prob": 2.1568130250670947e-06}, {"id": 787, "seek": 386982, "start": 3884.5800000000004, "end": 3885.78, "text": " All right", "tokens": [1057, 558], "temperature": 0.0, "avg_logprob": -0.2149266329678622, "compression_ratio": 1.8772727272727272, "no_speech_prob": 2.1568130250670947e-06}, {"id": 788, "seek": 386982, "start": 3885.78, "end": 3888.42, "text": " 256 and empty is compatible. It's going to insert a 1", "tokens": [38882, 293, 6707, 307, 18218, 13, 467, 311, 516, 281, 8969, 257, 502], "temperature": 0.0, "avg_logprob": -0.2149266329678622, "compression_ratio": 1.8772727272727272, "no_speech_prob": 2.1568130250670947e-06}, {"id": 789, "seek": 386982, "start": 3889.2200000000003, "end": 3891.7400000000002, "text": " 256 and empty is compatible. It's going to insert a 1", "tokens": [38882, 293, 6707, 307, 18218, 13, 467, 311, 516, 281, 8969, 257, 502], "temperature": 0.0, "avg_logprob": -0.2149266329678622, "compression_ratio": 1.8772727272727272, "no_speech_prob": 2.1568130250670947e-06}, {"id": 790, "seek": 386982, "start": 3892.6200000000003, "end": 3894.6200000000003, "text": " right, so you're going to end up with", "tokens": [558, 11, 370, 291, 434, 516, 281, 917, 493, 365], "temperature": 0.0, "avg_logprob": -0.2149266329678622, "compression_ratio": 1.8772727272727272, "no_speech_prob": 2.1568130250670947e-06}, {"id": 791, "seek": 389462, "start": 3894.62, "end": 3899.7, "text": " This is going to be broadcast over all of this axis", "tokens": [639, 307, 516, 281, 312, 9975, 670, 439, 295, 341, 10298], "temperature": 0.0, "avg_logprob": -0.22242332994937897, "compression_ratio": 1.510204081632653, "no_speech_prob": 9.57080260377552e-07}, {"id": 792, "seek": 389462, "start": 3899.7, "end": 3905.22, "text": " And then that whole thing will be broadcast over this axis and so we'll end up with a", "tokens": [400, 550, 300, 1379, 551, 486, 312, 9975, 670, 341, 10298, 293, 370, 321, 603, 917, 493, 365, 257], "temperature": 0.0, "avg_logprob": -0.22242332994937897, "compression_ratio": 1.510204081632653, "no_speech_prob": 9.57080260377552e-07}, {"id": 793, "seek": 389462, "start": 3906.42, "end": 3908.5, "text": " 256 by 256 by 3", "tokens": [38882, 538, 38882, 538, 805], "temperature": 0.0, "avg_logprob": -0.22242332994937897, "compression_ratio": 1.510204081632653, "no_speech_prob": 9.57080260377552e-07}, {"id": 794, "seek": 389462, "start": 3910.22, "end": 3912.22, "text": " effective", "tokens": [4942], "temperature": 0.0, "avg_logprob": -0.22242332994937897, "compression_ratio": 1.510204081632653, "no_speech_prob": 9.57080260377552e-07}, {"id": 795, "seek": 389462, "start": 3912.2599999999998, "end": 3913.7, "text": " tensor here", "tokens": [40863, 510], "temperature": 0.0, "avg_logprob": -0.22242332994937897, "compression_ratio": 1.510204081632653, "no_speech_prob": 9.57080260377552e-07}, {"id": 796, "seek": 389462, "start": 3913.7, "end": 3915.2999999999997, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.22242332994937897, "compression_ratio": 1.510204081632653, "no_speech_prob": 9.57080260377552e-07}, {"id": 797, "seek": 389462, "start": 3915.2999999999997, "end": 3917.2999999999997, "text": " so interestingly like", "tokens": [370, 25873, 411], "temperature": 0.0, "avg_logprob": -0.22242332994937897, "compression_ratio": 1.510204081632653, "no_speech_prob": 9.57080260377552e-07}, {"id": 798, "seek": 389462, "start": 3917.74, "end": 3919.74, "text": " very few people in", "tokens": [588, 1326, 561, 294], "temperature": 0.0, "avg_logprob": -0.22242332994937897, "compression_ratio": 1.510204081632653, "no_speech_prob": 9.57080260377552e-07}, {"id": 799, "seek": 391974, "start": 3919.74, "end": 3926.9399999999996, "text": " The data science or machine learning communities understand broadcasting and the vast majority of the time for example when I see people doing", "tokens": [440, 1412, 3497, 420, 3479, 2539, 4456, 1223, 30024, 293, 264, 8369, 6286, 295, 264, 565, 337, 1365, 562, 286, 536, 561, 884], "temperature": 0.0, "avg_logprob": -0.15179018264121197, "compression_ratio": 1.6983471074380165, "no_speech_prob": 3.7266190702212043e-06}, {"id": 800, "seek": 391974, "start": 3927.54, "end": 3932.74, "text": " Pre-processing for computer vision like subtracting the mean they always write loops", "tokens": [6001, 12, 41075, 278, 337, 3820, 5201, 411, 16390, 278, 264, 914, 436, 1009, 2464, 16121], "temperature": 0.0, "avg_logprob": -0.15179018264121197, "compression_ratio": 1.6983471074380165, "no_speech_prob": 3.7266190702212043e-06}, {"id": 801, "seek": 391974, "start": 3933.4599999999996, "end": 3936.7799999999997, "text": " over the channels right and I kind of think like", "tokens": [670, 264, 9235, 558, 293, 286, 733, 295, 519, 411], "temperature": 0.0, "avg_logprob": -0.15179018264121197, "compression_ratio": 1.6983471074380165, "no_speech_prob": 3.7266190702212043e-06}, {"id": 802, "seek": 391974, "start": 3937.5, "end": 3944.18, "text": " It's it's like so handy to not have to do that and it's often so much faster to not have to do that", "tokens": [467, 311, 309, 311, 411, 370, 13239, 281, 406, 362, 281, 360, 300, 293, 309, 311, 2049, 370, 709, 4663, 281, 406, 362, 281, 360, 300], "temperature": 0.0, "avg_logprob": -0.15179018264121197, "compression_ratio": 1.6983471074380165, "no_speech_prob": 3.7266190702212043e-06}, {"id": 803, "seek": 391974, "start": 3944.18, "end": 3946.18, "text": " So if you get good at broadcasting", "tokens": [407, 498, 291, 483, 665, 412, 30024], "temperature": 0.0, "avg_logprob": -0.15179018264121197, "compression_ratio": 1.6983471074380165, "no_speech_prob": 3.7266190702212043e-06}, {"id": 804, "seek": 394618, "start": 3946.18, "end": 3955.2599999999998, "text": " You'll have this like super useful skill that very very few people have and and like it's it's it's an ancient skill", "tokens": [509, 603, 362, 341, 411, 1687, 4420, 5389, 300, 588, 588, 1326, 561, 362, 293, 293, 411, 309, 311, 309, 311, 309, 311, 364, 7832, 5389], "temperature": 0.0, "avg_logprob": -0.17810189723968506, "compression_ratio": 1.553921568627451, "no_speech_prob": 9.132508580478316e-07}, {"id": 805, "seek": 394618, "start": 3955.2599999999998, "end": 3957.94, "text": " You know it goes it goes all the way back to", "tokens": [509, 458, 309, 1709, 309, 1709, 439, 264, 636, 646, 281], "temperature": 0.0, "avg_logprob": -0.17810189723968506, "compression_ratio": 1.553921568627451, "no_speech_prob": 9.132508580478316e-07}, {"id": 806, "seek": 394618, "start": 3958.98, "end": 3960.98, "text": " the days of APL", "tokens": [264, 1708, 295, 5372, 43], "temperature": 0.0, "avg_logprob": -0.17810189723968506, "compression_ratio": 1.553921568627451, "no_speech_prob": 9.132508580478316e-07}, {"id": 807, "seek": 394618, "start": 3961.4199999999996, "end": 3964.66, "text": " So APL was from the late 50s", "tokens": [407, 5372, 43, 390, 490, 264, 3469, 2625, 82], "temperature": 0.0, "avg_logprob": -0.17810189723968506, "compression_ratio": 1.553921568627451, "no_speech_prob": 9.132508580478316e-07}, {"id": 808, "seek": 394618, "start": 3965.74, "end": 3967.74, "text": " stands for our programming language and", "tokens": [7382, 337, 527, 9410, 2856, 293], "temperature": 0.0, "avg_logprob": -0.17810189723968506, "compression_ratio": 1.553921568627451, "no_speech_prob": 9.132508580478316e-07}, {"id": 809, "seek": 394618, "start": 3969.18, "end": 3971.1, "text": " Kenneth Iverson", "tokens": [33735, 286, 840, 266], "temperature": 0.0, "avg_logprob": -0.17810189723968506, "compression_ratio": 1.553921568627451, "no_speech_prob": 9.132508580478316e-07}, {"id": 810, "seek": 394618, "start": 3971.1, "end": 3973.1, "text": " wrote this paper called", "tokens": [4114, 341, 3035, 1219], "temperature": 0.0, "avg_logprob": -0.17810189723968506, "compression_ratio": 1.553921568627451, "no_speech_prob": 9.132508580478316e-07}, {"id": 811, "seek": 397310, "start": 3973.1, "end": 3975.5, "text": " notation as a tool for thought", "tokens": [24657, 382, 257, 2290, 337, 1194], "temperature": 0.0, "avg_logprob": -0.19640392065048218, "compression_ratio": 1.7966101694915255, "no_speech_prob": 1.7330319224129198e-06}, {"id": 812, "seek": 397310, "start": 3976.62, "end": 3980.1, "text": " in which he proposed a new math notation and", "tokens": [294, 597, 415, 10348, 257, 777, 5221, 24657, 293], "temperature": 0.0, "avg_logprob": -0.19640392065048218, "compression_ratio": 1.7966101694915255, "no_speech_prob": 1.7330319224129198e-06}, {"id": 813, "seek": 397310, "start": 3981.14, "end": 3984.7, "text": " He proposed that if we use this new math notation", "tokens": [634, 10348, 300, 498, 321, 764, 341, 777, 5221, 24657], "temperature": 0.0, "avg_logprob": -0.19640392065048218, "compression_ratio": 1.7966101694915255, "no_speech_prob": 1.7330319224129198e-06}, {"id": 814, "seek": 397310, "start": 3984.7, "end": 3990.18, "text": " it gives us new tools for thought and allows us to think things we couldn't before and", "tokens": [309, 2709, 505, 777, 3873, 337, 1194, 293, 4045, 505, 281, 519, 721, 321, 2809, 380, 949, 293], "temperature": 0.0, "avg_logprob": -0.19640392065048218, "compression_ratio": 1.7966101694915255, "no_speech_prob": 1.7330319224129198e-06}, {"id": 815, "seek": 397310, "start": 3990.66, "end": 3992.66, "text": " one of his ideas was", "tokens": [472, 295, 702, 3487, 390], "temperature": 0.0, "avg_logprob": -0.19640392065048218, "compression_ratio": 1.7966101694915255, "no_speech_prob": 1.7330319224129198e-06}, {"id": 816, "seek": 397310, "start": 3993.66, "end": 3995.66, "text": " broadcasting not as a", "tokens": [30024, 406, 382, 257], "temperature": 0.0, "avg_logprob": -0.19640392065048218, "compression_ratio": 1.7966101694915255, "no_speech_prob": 1.7330319224129198e-06}, {"id": 817, "seek": 397310, "start": 3996.46, "end": 4000.2599999999998, "text": " computer programming tool, but as a piece of math notation and", "tokens": [3820, 9410, 2290, 11, 457, 382, 257, 2522, 295, 5221, 24657, 293], "temperature": 0.0, "avg_logprob": -0.19640392065048218, "compression_ratio": 1.7966101694915255, "no_speech_prob": 1.7330319224129198e-06}, {"id": 818, "seek": 400026, "start": 4000.26, "end": 4002.6200000000003, "text": " And so he ended up implementing", "tokens": [400, 370, 415, 4590, 493, 18114], "temperature": 0.0, "avg_logprob": -0.1970956142132099, "compression_ratio": 1.5781990521327014, "no_speech_prob": 9.132515970122768e-07}, {"id": 819, "seek": 400026, "start": 4003.6600000000003, "end": 4008.42, "text": " this notation as a tool for thought as a programming language called APL and", "tokens": [341, 24657, 382, 257, 2290, 337, 1194, 382, 257, 9410, 2856, 1219, 5372, 43, 293], "temperature": 0.0, "avg_logprob": -0.1970956142132099, "compression_ratio": 1.5781990521327014, "no_speech_prob": 9.132515970122768e-07}, {"id": 820, "seek": 400026, "start": 4009.26, "end": 4014.1200000000003, "text": " His son has gone on to further develop that", "tokens": [2812, 1872, 575, 2780, 322, 281, 3052, 1499, 300], "temperature": 0.0, "avg_logprob": -0.1970956142132099, "compression_ratio": 1.5781990521327014, "no_speech_prob": 9.132515970122768e-07}, {"id": 821, "seek": 400026, "start": 4015.2200000000003, "end": 4017.2200000000003, "text": " Into a piece of software called J", "tokens": [23373, 257, 2522, 295, 4722, 1219, 508], "temperature": 0.0, "avg_logprob": -0.1970956142132099, "compression_ratio": 1.5781990521327014, "no_speech_prob": 9.132515970122768e-07}, {"id": 822, "seek": 400026, "start": 4017.5400000000004, "end": 4024.1800000000003, "text": " Which is basically what you get when you put 60 years of very smart people working on this idea", "tokens": [3013, 307, 1936, 437, 291, 483, 562, 291, 829, 4060, 924, 295, 588, 4069, 561, 1364, 322, 341, 1558], "temperature": 0.0, "avg_logprob": -0.1970956142132099, "compression_ratio": 1.5781990521327014, "no_speech_prob": 9.132515970122768e-07}, {"id": 823, "seek": 400026, "start": 4024.46, "end": 4027.82, "text": " And with this programming language you can express", "tokens": [400, 365, 341, 9410, 2856, 291, 393, 5109], "temperature": 0.0, "avg_logprob": -0.1970956142132099, "compression_ratio": 1.5781990521327014, "no_speech_prob": 9.132515970122768e-07}, {"id": 824, "seek": 402782, "start": 4027.82, "end": 4033.36, "text": " very complex mathematical ideas often just with a line of code or two", "tokens": [588, 3997, 18894, 3487, 2049, 445, 365, 257, 1622, 295, 3089, 420, 732], "temperature": 0.0, "avg_logprob": -0.2123357400126841, "compression_ratio": 1.618421052631579, "no_speech_prob": 9.276325272367103e-07}, {"id": 825, "seek": 402782, "start": 4034.46, "end": 4036.94, "text": " And so I mean it's great that we have J", "tokens": [400, 370, 286, 914, 309, 311, 869, 300, 321, 362, 508], "temperature": 0.0, "avg_logprob": -0.2123357400126841, "compression_ratio": 1.618421052631579, "no_speech_prob": 9.276325272367103e-07}, {"id": 826, "seek": 402782, "start": 4036.94, "end": 4041.02, "text": " But it's even greater that these ideas have found their ways into the languages", "tokens": [583, 309, 311, 754, 5044, 300, 613, 3487, 362, 1352, 641, 2098, 666, 264, 8650], "temperature": 0.0, "avg_logprob": -0.2123357400126841, "compression_ratio": 1.618421052631579, "no_speech_prob": 9.276325272367103e-07}, {"id": 827, "seek": 402782, "start": 4041.02, "end": 4046.78, "text": " We all use like in Python the numpy and pytorch libraries right these are not just little", "tokens": [492, 439, 764, 411, 294, 15329, 264, 1031, 8200, 293, 25878, 284, 339, 15148, 558, 613, 366, 406, 445, 707], "temperature": 0.0, "avg_logprob": -0.2123357400126841, "compression_ratio": 1.618421052631579, "no_speech_prob": 9.276325272367103e-07}, {"id": 828, "seek": 402782, "start": 4047.6600000000003, "end": 4053.1400000000003, "text": " Kind of niche ideas these like fundamental ways to think about math and to do programming", "tokens": [9242, 295, 19956, 3487, 613, 411, 8088, 2098, 281, 519, 466, 5221, 293, 281, 360, 9410], "temperature": 0.0, "avg_logprob": -0.2123357400126841, "compression_ratio": 1.618421052631579, "no_speech_prob": 9.276325272367103e-07}, {"id": 829, "seek": 405314, "start": 4053.14, "end": 4058.2599999999998, "text": " Like let me give an example of like this kind of notation as a tool for thought", "tokens": [1743, 718, 385, 976, 364, 1365, 295, 411, 341, 733, 295, 24657, 382, 257, 2290, 337, 1194], "temperature": 0.0, "avg_logprob": -0.23529551369803292, "compression_ratio": 1.6158940397350994, "no_speech_prob": 7.571131845907075e-07}, {"id": 830, "seek": 405314, "start": 4059.22, "end": 4061.22, "text": " Let's", "tokens": [961, 311], "temperature": 0.0, "avg_logprob": -0.23529551369803292, "compression_ratio": 1.6158940397350994, "no_speech_prob": 7.571131845907075e-07}, {"id": 831, "seek": 405314, "start": 4063.42, "end": 4067.7, "text": " Let's look here. We've got C right here. We've got C", "tokens": [961, 311, 574, 510, 13, 492, 600, 658, 383, 558, 510, 13, 492, 600, 658, 383], "temperature": 0.0, "avg_logprob": -0.23529551369803292, "compression_ratio": 1.6158940397350994, "no_speech_prob": 7.571131845907075e-07}, {"id": 832, "seek": 405314, "start": 4069.74, "end": 4075.98, "text": " None right notice this is no up to square brackets right so this is kind of like a one row", "tokens": [14492, 558, 3449, 341, 307, 572, 493, 281, 3732, 26179, 558, 370, 341, 307, 733, 295, 411, 257, 472, 5386], "temperature": 0.0, "avg_logprob": -0.23529551369803292, "compression_ratio": 1.6158940397350994, "no_speech_prob": 7.571131845907075e-07}, {"id": 833, "seek": 405314, "start": 4077.1, "end": 4079.1, "text": " rank to tensor", "tokens": [6181, 281, 40863], "temperature": 0.0, "avg_logprob": -0.23529551369803292, "compression_ratio": 1.6158940397350994, "no_speech_prob": 7.571131845907075e-07}, {"id": 834, "seek": 407910, "start": 4079.1, "end": 4083.46, "text": " Here it is a little column", "tokens": [1692, 309, 307, 257, 707, 7738], "temperature": 0.0, "avg_logprob": -0.44894986274914866, "compression_ratio": 1.118811881188119, "no_speech_prob": 7.071824711601948e-06}, {"id": 835, "seek": 407910, "start": 4086.66, "end": 4088.66, "text": " So what is", "tokens": [407, 437, 307], "temperature": 0.0, "avg_logprob": -0.44894986274914866, "compression_ratio": 1.118811881188119, "no_speech_prob": 7.071824711601948e-06}, {"id": 836, "seek": 407910, "start": 4097.34, "end": 4099.34, "text": " Do we just round ones?", "tokens": [1144, 321, 445, 3098, 2306, 30], "temperature": 0.0, "avg_logprob": -0.44894986274914866, "compression_ratio": 1.118811881188119, "no_speech_prob": 7.071824711601948e-06}, {"id": 837, "seek": 407910, "start": 4102.0199999999995, "end": 4104.18, "text": " Okay, what's that going to do I", "tokens": [1033, 11, 437, 311, 300, 516, 281, 360, 286], "temperature": 0.0, "avg_logprob": -0.44894986274914866, "compression_ratio": 1.118811881188119, "no_speech_prob": 7.071824711601948e-06}, {"id": 838, "seek": 410418, "start": 4104.18, "end": 4107.860000000001, "text": " Would think about it", "tokens": [6068, 519, 466, 309], "temperature": 0.0, "avg_logprob": -0.2849914945405105, "compression_ratio": 1.4871794871794872, "no_speech_prob": 6.540215508721303e-06}, {"id": 839, "seek": 410418, "start": 4114.62, "end": 4120.62, "text": " Anybody want to have a go even talk through your thinking okay? Can we pass the check just over there? Thank you", "tokens": [19082, 528, 281, 362, 257, 352, 754, 751, 807, 428, 1953, 1392, 30, 1664, 321, 1320, 264, 1520, 445, 670, 456, 30, 1044, 291], "temperature": 0.0, "avg_logprob": -0.2849914945405105, "compression_ratio": 1.4871794871794872, "no_speech_prob": 6.540215508721303e-06}, {"id": 840, "seek": 410418, "start": 4122.62, "end": 4127.820000000001, "text": " Kind of outer product yes absolutely so take us through your thinking how's that going to work?", "tokens": [9242, 295, 10847, 1674, 2086, 3122, 370, 747, 505, 807, 428, 1953, 577, 311, 300, 516, 281, 589, 30], "temperature": 0.0, "avg_logprob": -0.2849914945405105, "compression_ratio": 1.4871794871794872, "no_speech_prob": 6.540215508721303e-06}, {"id": 841, "seek": 410418, "start": 4129.38, "end": 4130.9400000000005, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.2849914945405105, "compression_ratio": 1.4871794871794872, "no_speech_prob": 6.540215508721303e-06}, {"id": 842, "seek": 413094, "start": 4130.94, "end": 4134.66, "text": " The diagonal elements can be directly visualized from the squares", "tokens": [440, 21539, 4959, 393, 312, 3838, 5056, 1602, 490, 264, 19368], "temperature": 0.0, "avg_logprob": -0.2675365625425827, "compression_ratio": 1.6, "no_speech_prob": 3.647650373750366e-05}, {"id": 843, "seek": 413094, "start": 4136.46, "end": 4140.46, "text": " Mm-hmm across 10 20 cross 20 and 30 cross 30 mm-hmm and", "tokens": [8266, 12, 10250, 2108, 1266, 945, 3278, 945, 293, 2217, 3278, 2217, 11169, 12, 10250, 293], "temperature": 0.0, "avg_logprob": -0.2675365625425827, "compression_ratio": 1.6, "no_speech_prob": 3.647650373750366e-05}, {"id": 844, "seek": 413094, "start": 4141.5, "end": 4147.94, "text": " If you multiply the first row with this column you can get the first row of the matrix", "tokens": [759, 291, 12972, 264, 700, 5386, 365, 341, 7738, 291, 393, 483, 264, 700, 5386, 295, 264, 8141], "temperature": 0.0, "avg_logprob": -0.2675365625425827, "compression_ratio": 1.6, "no_speech_prob": 3.647650373750366e-05}, {"id": 845, "seek": 413094, "start": 4148.9, "end": 4156.24, "text": " So finally you'll get a 3 cross 3 matrix yeah, and so to think of this in terms of like those broadcasting rules", "tokens": [407, 2721, 291, 603, 483, 257, 805, 3278, 805, 8141, 1338, 11, 293, 370, 281, 519, 295, 341, 294, 2115, 295, 411, 729, 30024, 4474], "temperature": 0.0, "avg_logprob": -0.2675365625425827, "compression_ratio": 1.6, "no_speech_prob": 3.647650373750366e-05}, {"id": 846, "seek": 413094, "start": 4156.66, "end": 4158.66, "text": " We're basically taking", "tokens": [492, 434, 1936, 1940], "temperature": 0.0, "avg_logprob": -0.2675365625425827, "compression_ratio": 1.6, "no_speech_prob": 3.647650373750366e-05}, {"id": 847, "seek": 415866, "start": 4158.66, "end": 4161.74, "text": " This column right which is of rank", "tokens": [639, 7738, 558, 597, 307, 295, 6181], "temperature": 0.0, "avg_logprob": -0.2314047562448602, "compression_ratio": 1.6898395721925135, "no_speech_prob": 2.601608457553084e-06}, {"id": 848, "seek": 415866, "start": 4164.0199999999995, "end": 4167.3, "text": " Three comma one right and this kind of row", "tokens": [6244, 22117, 472, 558, 293, 341, 733, 295, 5386], "temperature": 0.0, "avg_logprob": -0.2314047562448602, "compression_ratio": 1.6898395721925135, "no_speech_prob": 2.601608457553084e-06}, {"id": 849, "seek": 415866, "start": 4168.82, "end": 4176.62, "text": " Sorry, I've mentioned three comma one and this row which is of dimension one comma three right and so to make these", "tokens": [4919, 11, 286, 600, 2835, 1045, 22117, 472, 293, 341, 5386, 597, 307, 295, 10139, 472, 22117, 1045, 558, 293, 370, 281, 652, 613], "temperature": 0.0, "avg_logprob": -0.2314047562448602, "compression_ratio": 1.6898395721925135, "no_speech_prob": 2.601608457553084e-06}, {"id": 850, "seek": 415866, "start": 4177.34, "end": 4182.139999999999, "text": " Compatible with our broadcasting rules right this one here has to be duplicated", "tokens": [6620, 267, 964, 365, 527, 30024, 4474, 558, 341, 472, 510, 575, 281, 312, 1581, 564, 3587], "temperature": 0.0, "avg_logprob": -0.2314047562448602, "compression_ratio": 1.6898395721925135, "no_speech_prob": 2.601608457553084e-06}, {"id": 851, "seek": 418214, "start": 4182.14, "end": 4188.34, "text": " Duplicated three times because it needs to match this okay?", "tokens": [5153, 564, 3587, 1045, 1413, 570, 309, 2203, 281, 2995, 341, 1392, 30], "temperature": 0.0, "avg_logprob": -0.20936195055643717, "compression_ratio": 1.5070422535211268, "no_speech_prob": 2.4439884782623267e-06}, {"id": 852, "seek": 418214, "start": 4194.14, "end": 4197.740000000001, "text": " And now this one's going to have to be duplicated three times to match this", "tokens": [400, 586, 341, 472, 311, 516, 281, 362, 281, 312, 1581, 564, 3587, 1045, 1413, 281, 2995, 341], "temperature": 0.0, "avg_logprob": -0.20936195055643717, "compression_ratio": 1.5070422535211268, "no_speech_prob": 2.4439884782623267e-06}, {"id": 853, "seek": 418214, "start": 4203.02, "end": 4205.14, "text": " Okay, and so now I've got two", "tokens": [1033, 11, 293, 370, 586, 286, 600, 658, 732], "temperature": 0.0, "avg_logprob": -0.20936195055643717, "compression_ratio": 1.5070422535211268, "no_speech_prob": 2.4439884782623267e-06}, {"id": 854, "seek": 420514, "start": 4205.14, "end": 4212.860000000001, "text": " Matrices to do an element-wise product of and so as you say", "tokens": [6789, 24373, 281, 360, 364, 4478, 12, 3711, 1674, 295, 293, 370, 382, 291, 584], "temperature": 0.0, "avg_logprob": -0.20585483493226947, "compression_ratio": 1.5842696629213484, "no_speech_prob": 3.7266172512318008e-06}, {"id": 855, "seek": 420514, "start": 4213.820000000001, "end": 4217.9800000000005, "text": " There is our outer product right now the interesting thing here is", "tokens": [821, 307, 527, 10847, 1674, 558, 586, 264, 1880, 551, 510, 307], "temperature": 0.0, "avg_logprob": -0.20585483493226947, "compression_ratio": 1.5842696629213484, "no_speech_prob": 3.7266172512318008e-06}, {"id": 856, "seek": 420514, "start": 4218.62, "end": 4222.740000000001, "text": " That suddenly now that this is not a special mathematical case", "tokens": [663, 5800, 586, 300, 341, 307, 406, 257, 2121, 18894, 1389], "temperature": 0.0, "avg_logprob": -0.20585483493226947, "compression_ratio": 1.5842696629213484, "no_speech_prob": 3.7266172512318008e-06}, {"id": 857, "seek": 420514, "start": 4223.26, "end": 4231.06, "text": " But just a specific version of the general idea of broadcasting we can do like an outer plus", "tokens": [583, 445, 257, 2685, 3037, 295, 264, 2674, 1558, 295, 30024, 321, 393, 360, 411, 364, 10847, 1804], "temperature": 0.0, "avg_logprob": -0.20585483493226947, "compression_ratio": 1.5842696629213484, "no_speech_prob": 3.7266172512318008e-06}, {"id": 858, "seek": 423106, "start": 4231.06, "end": 4235.1, "text": " Or we can do an order greater than", "tokens": [1610, 321, 393, 360, 364, 1668, 5044, 813], "temperature": 0.0, "avg_logprob": -0.24135829977793236, "compression_ratio": 1.5706214689265536, "no_speech_prob": 1.0845141105164657e-06}, {"id": 859, "seek": 423106, "start": 4236.34, "end": 4241.3, "text": " Right or or whatever right so it's suddenly we've kind of got this this this", "tokens": [1779, 420, 420, 2035, 558, 370, 309, 311, 5800, 321, 600, 733, 295, 658, 341, 341, 341], "temperature": 0.0, "avg_logprob": -0.24135829977793236, "compression_ratio": 1.5706214689265536, "no_speech_prob": 1.0845141105164657e-06}, {"id": 860, "seek": 423106, "start": 4241.860000000001, "end": 4244.9800000000005, "text": " concept that we can use to build", "tokens": [3410, 300, 321, 393, 764, 281, 1322], "temperature": 0.0, "avg_logprob": -0.24135829977793236, "compression_ratio": 1.5706214689265536, "no_speech_prob": 1.0845141105164657e-06}, {"id": 861, "seek": 423106, "start": 4245.660000000001, "end": 4251.3, "text": " New ideas, and then we can start to experiment with those new ideas, and so you know interestingly", "tokens": [1873, 3487, 11, 293, 550, 321, 393, 722, 281, 5120, 365, 729, 777, 3487, 11, 293, 370, 291, 458, 25873], "temperature": 0.0, "avg_logprob": -0.24135829977793236, "compression_ratio": 1.5706214689265536, "no_speech_prob": 1.0845141105164657e-06}, {"id": 862, "seek": 423106, "start": 4252.14, "end": 4254.14, "text": " numpy actually", "tokens": [1031, 8200, 767], "temperature": 0.0, "avg_logprob": -0.24135829977793236, "compression_ratio": 1.5706214689265536, "no_speech_prob": 1.0845141105164657e-06}, {"id": 863, "seek": 423106, "start": 4254.660000000001, "end": 4256.660000000001, "text": " Uses this sometimes", "tokens": [4958, 279, 341, 2171], "temperature": 0.0, "avg_logprob": -0.24135829977793236, "compression_ratio": 1.5706214689265536, "no_speech_prob": 1.0845141105164657e-06}, {"id": 864, "seek": 425666, "start": 4256.66, "end": 4262.139999999999, "text": " For example if you want to create a grid", "tokens": [1171, 1365, 498, 291, 528, 281, 1884, 257, 10748], "temperature": 0.0, "avg_logprob": -0.18426961898803712, "compression_ratio": 1.80625, "no_speech_prob": 8.186351578842732e-07}, {"id": 865, "seek": 425666, "start": 4265.62, "end": 4271.7, "text": " This is how numpy does it right actually this is kind of the sorry let me show you this way", "tokens": [639, 307, 577, 1031, 8200, 775, 309, 558, 767, 341, 307, 733, 295, 264, 2597, 718, 385, 855, 291, 341, 636], "temperature": 0.0, "avg_logprob": -0.18426961898803712, "compression_ratio": 1.80625, "no_speech_prob": 8.186351578842732e-07}, {"id": 866, "seek": 425666, "start": 4273.18, "end": 4276.82, "text": " If you want to create a grid, this is how numpy does it it actually returns", "tokens": [759, 291, 528, 281, 1884, 257, 10748, 11, 341, 307, 577, 1031, 8200, 775, 309, 309, 767, 11247], "temperature": 0.0, "avg_logprob": -0.18426961898803712, "compression_ratio": 1.80625, "no_speech_prob": 8.186351578842732e-07}, {"id": 867, "seek": 425666, "start": 4278.0199999999995, "end": 4280.0199999999995, "text": " zero one two three four and", "tokens": [4018, 472, 732, 1045, 1451, 293], "temperature": 0.0, "avg_logprob": -0.18426961898803712, "compression_ratio": 1.80625, "no_speech_prob": 8.186351578842732e-07}, {"id": 868, "seek": 425666, "start": 4281.62, "end": 4283.62, "text": " Zero one two three four", "tokens": [17182, 472, 732, 1045, 1451], "temperature": 0.0, "avg_logprob": -0.18426961898803712, "compression_ratio": 1.80625, "no_speech_prob": 8.186351578842732e-07}, {"id": 869, "seek": 428362, "start": 4283.62, "end": 4290.38, "text": " One is a column one is a row so we could say like okay. That's x grid comma y grid", "tokens": [1485, 307, 257, 7738, 472, 307, 257, 5386, 370, 321, 727, 584, 411, 1392, 13, 663, 311, 2031, 10748, 22117, 288, 10748], "temperature": 0.0, "avg_logprob": -0.2510972704206194, "compression_ratio": 1.4246575342465753, "no_speech_prob": 6.375540806402569e-07}, {"id": 870, "seek": 428362, "start": 4293.18, "end": 4296.22, "text": " And now you could do something like", "tokens": [400, 586, 291, 727, 360, 746, 411], "temperature": 0.0, "avg_logprob": -0.2510972704206194, "compression_ratio": 1.4246575342465753, "no_speech_prob": 6.375540806402569e-07}, {"id": 871, "seek": 428362, "start": 4300.58, "end": 4302.58, "text": " Well I mean we could obviously go", "tokens": [1042, 286, 914, 321, 727, 2745, 352], "temperature": 0.0, "avg_logprob": -0.2510972704206194, "compression_ratio": 1.4246575342465753, "no_speech_prob": 6.375540806402569e-07}, {"id": 872, "seek": 430258, "start": 4302.58, "end": 4310.58, "text": " Like that right and so suddenly we've expanded that out into", "tokens": [1743, 300, 558, 293, 370, 5800, 321, 600, 14342, 300, 484, 666], "temperature": 0.0, "avg_logprob": -0.2112636157444545, "compression_ratio": 1.5963855421686748, "no_speech_prob": 8.186347031369223e-07}, {"id": 873, "seek": 430258, "start": 4315.42, "end": 4317.42, "text": " A grid right", "tokens": [316, 10748, 558], "temperature": 0.0, "avg_logprob": -0.2112636157444545, "compression_ratio": 1.5963855421686748, "no_speech_prob": 8.186347031369223e-07}, {"id": 874, "seek": 430258, "start": 4317.82, "end": 4320.0599999999995, "text": " and so", "tokens": [293, 370], "temperature": 0.0, "avg_logprob": -0.2112636157444545, "compression_ratio": 1.5963855421686748, "no_speech_prob": 8.186347031369223e-07}, {"id": 875, "seek": 430258, "start": 4320.0599999999995, "end": 4325.62, "text": " Yeah, it's kind of interesting how like some of these like simple little concepts", "tokens": [865, 11, 309, 311, 733, 295, 1880, 577, 411, 512, 295, 613, 411, 2199, 707, 10392], "temperature": 0.0, "avg_logprob": -0.2112636157444545, "compression_ratio": 1.5963855421686748, "no_speech_prob": 8.186347031369223e-07}, {"id": 876, "seek": 432562, "start": 4325.62, "end": 4332.42, "text": " Kind of get built on and built on and built on so if you lose something like APL or J. It's this whole environment", "tokens": [9242, 295, 483, 3094, 322, 293, 3094, 322, 293, 3094, 322, 370, 498, 291, 3624, 746, 411, 5372, 43, 420, 508, 13, 467, 311, 341, 1379, 2823], "temperature": 0.0, "avg_logprob": -0.1529693603515625, "compression_ratio": 1.7489539748953975, "no_speech_prob": 2.2603137495025294e-06}, {"id": 877, "seek": 432562, "start": 4333.5, "end": 4338.3, "text": " Of layers and layers and layers of this we don't have such a deep environment in numpy", "tokens": [2720, 7914, 293, 7914, 293, 7914, 295, 341, 321, 500, 380, 362, 1270, 257, 2452, 2823, 294, 1031, 8200], "temperature": 0.0, "avg_logprob": -0.1529693603515625, "compression_ratio": 1.7489539748953975, "no_speech_prob": 2.2603137495025294e-06}, {"id": 878, "seek": 432562, "start": 4338.3, "end": 4341.74, "text": " But you know you can certainly see these ideas of like broadcasting", "tokens": [583, 291, 458, 291, 393, 3297, 536, 613, 3487, 295, 411, 30024], "temperature": 0.0, "avg_logprob": -0.1529693603515625, "compression_ratio": 1.7489539748953975, "no_speech_prob": 2.2603137495025294e-06}, {"id": 879, "seek": 432562, "start": 4342.18, "end": 4347.3, "text": " Coming through in simple things like how do we create a grid in in numpy?", "tokens": [12473, 807, 294, 2199, 721, 411, 577, 360, 321, 1884, 257, 10748, 294, 294, 1031, 8200, 30], "temperature": 0.0, "avg_logprob": -0.1529693603515625, "compression_ratio": 1.7489539748953975, "no_speech_prob": 2.2603137495025294e-06}, {"id": 880, "seek": 434730, "start": 4347.3, "end": 4354.900000000001, "text": " So yeah, so that's that's broadcasting and so what we can do with this now is", "tokens": [407, 1338, 11, 370, 300, 311, 300, 311, 30024, 293, 370, 437, 321, 393, 360, 365, 341, 586, 307], "temperature": 0.0, "avg_logprob": -0.1795081225308505, "compression_ratio": 1.5683060109289617, "no_speech_prob": 1.3287744877743535e-06}, {"id": 881, "seek": 434730, "start": 4357.22, "end": 4360.860000000001, "text": " Use this to implement matrix multiplication ourselves", "tokens": [8278, 341, 281, 4445, 8141, 27290, 4175], "temperature": 0.0, "avg_logprob": -0.1795081225308505, "compression_ratio": 1.5683060109289617, "no_speech_prob": 1.3287744877743535e-06}, {"id": 882, "seek": 434730, "start": 4361.9800000000005, "end": 4363.9800000000005, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.1795081225308505, "compression_ratio": 1.5683060109289617, "no_speech_prob": 1.3287744877743535e-06}, {"id": 883, "seek": 434730, "start": 4364.02, "end": 4370.66, "text": " Now why would we want to do that well obviously we don't write matrix multiplication has already been handled", "tokens": [823, 983, 576, 321, 528, 281, 360, 300, 731, 2745, 321, 500, 380, 2464, 8141, 27290, 575, 1217, 668, 18033], "temperature": 0.0, "avg_logprob": -0.1795081225308505, "compression_ratio": 1.5683060109289617, "no_speech_prob": 1.3287744877743535e-06}, {"id": 884, "seek": 434730, "start": 4371.3, "end": 4374.24, "text": " perfectly nicely for us by our libraries", "tokens": [6239, 9594, 337, 505, 538, 527, 15148], "temperature": 0.0, "avg_logprob": -0.1795081225308505, "compression_ratio": 1.5683060109289617, "no_speech_prob": 1.3287744877743535e-06}, {"id": 885, "seek": 437424, "start": 4374.24, "end": 4377.639999999999, "text": " but very often you'll find in", "tokens": [457, 588, 2049, 291, 603, 915, 294], "temperature": 0.0, "avg_logprob": -0.2198453553965394, "compression_ratio": 1.545945945945946, "no_speech_prob": 1.3709517361348844e-06}, {"id": 886, "seek": 437424, "start": 4379.28, "end": 4385.04, "text": " All kinds of areas in in machine learning and particularly in deep learning that there'll be", "tokens": [1057, 3685, 295, 3179, 294, 294, 3479, 2539, 293, 4098, 294, 2452, 2539, 300, 456, 603, 312], "temperature": 0.0, "avg_logprob": -0.2198453553965394, "compression_ratio": 1.545945945945946, "no_speech_prob": 1.3709517361348844e-06}, {"id": 887, "seek": 437424, "start": 4386.5199999999995, "end": 4388.5199999999995, "text": " particular types of linear", "tokens": [1729, 3467, 295, 8213], "temperature": 0.0, "avg_logprob": -0.2198453553965394, "compression_ratio": 1.545945945945946, "no_speech_prob": 1.3709517361348844e-06}, {"id": 888, "seek": 437424, "start": 4390.08, "end": 4392.639999999999, "text": " Function that you want to do that aren't quite", "tokens": [11166, 882, 300, 291, 528, 281, 360, 300, 3212, 380, 1596], "temperature": 0.0, "avg_logprob": -0.2198453553965394, "compression_ratio": 1.545945945945946, "no_speech_prob": 1.3709517361348844e-06}, {"id": 889, "seek": 437424, "start": 4393.36, "end": 4397.74, "text": " Done for you right so for example. There's like whole areas", "tokens": [18658, 337, 291, 558, 370, 337, 1365, 13, 821, 311, 411, 1379, 3179], "temperature": 0.0, "avg_logprob": -0.2198453553965394, "compression_ratio": 1.545945945945946, "no_speech_prob": 1.3709517361348844e-06}, {"id": 890, "seek": 437424, "start": 4398.84, "end": 4400.679999999999, "text": " called like", "tokens": [1219, 411], "temperature": 0.0, "avg_logprob": -0.2198453553965394, "compression_ratio": 1.545945945945946, "no_speech_prob": 1.3709517361348844e-06}, {"id": 891, "seek": 440068, "start": 4400.68, "end": 4404.68, "text": " tensor regression and", "tokens": [40863, 24590, 293], "temperature": 0.0, "avg_logprob": -0.15667125341054555, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.860394255956635e-06}, {"id": 892, "seek": 440068, "start": 4405.08, "end": 4407.08, "text": " tensor decomposition", "tokens": [40863, 48356], "temperature": 0.0, "avg_logprob": -0.15667125341054555, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.860394255956635e-06}, {"id": 893, "seek": 440068, "start": 4412.320000000001, "end": 4417.92, "text": " Which are really being developed a lot at the moment, and they're kind of talking about like how do we take like", "tokens": [3013, 366, 534, 885, 4743, 257, 688, 412, 264, 1623, 11, 293, 436, 434, 733, 295, 1417, 466, 411, 577, 360, 321, 747, 411], "temperature": 0.0, "avg_logprob": -0.15667125341054555, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.860394255956635e-06}, {"id": 894, "seek": 440068, "start": 4418.4400000000005, "end": 4423.320000000001, "text": " higher rank tensors and kind of turn them into combinations of rows", "tokens": [2946, 6181, 10688, 830, 293, 733, 295, 1261, 552, 666, 21267, 295, 13241], "temperature": 0.0, "avg_logprob": -0.15667125341054555, "compression_ratio": 1.6666666666666667, "no_speech_prob": 4.860394255956635e-06}, {"id": 895, "seek": 442332, "start": 4423.32, "end": 4430.28, "text": " Columns and faces and it turns out that when you can kind of do this you can basically like", "tokens": [4004, 449, 3695, 293, 8475, 293, 309, 4523, 484, 300, 562, 291, 393, 733, 295, 360, 341, 291, 393, 1936, 411], "temperature": 0.0, "avg_logprob": -0.15753635705686084, "compression_ratio": 1.7380952380952381, "no_speech_prob": 2.1907692371314624e-06}, {"id": 896, "seek": 442332, "start": 4430.48, "end": 4436.54, "text": " Deal with really high dimensional data structures with not much memory and not with not much computation time for example", "tokens": [27227, 365, 534, 1090, 18795, 1412, 9227, 365, 406, 709, 4675, 293, 406, 365, 406, 709, 24903, 565, 337, 1365], "temperature": 0.0, "avg_logprob": -0.15753635705686084, "compression_ratio": 1.7380952380952381, "no_speech_prob": 2.1907692371314624e-06}, {"id": 897, "seek": 442332, "start": 4436.54, "end": 4438.54, "text": " There's a really terrific library", "tokens": [821, 311, 257, 534, 20899, 6405], "temperature": 0.0, "avg_logprob": -0.15753635705686084, "compression_ratio": 1.7380952380952381, "no_speech_prob": 2.1907692371314624e-06}, {"id": 898, "seek": 442332, "start": 4438.759999999999, "end": 4440.5199999999995, "text": " Called tensely", "tokens": [45001, 10688, 736], "temperature": 0.0, "avg_logprob": -0.15753635705686084, "compression_ratio": 1.7380952380952381, "no_speech_prob": 2.1907692371314624e-06}, {"id": 899, "seek": 442332, "start": 4440.5199999999995, "end": 4442.5199999999995, "text": " Which does a whole lot of this kind of stuff?", "tokens": [3013, 775, 257, 1379, 688, 295, 341, 733, 295, 1507, 30], "temperature": 0.0, "avg_logprob": -0.15753635705686084, "compression_ratio": 1.7380952380952381, "no_speech_prob": 2.1907692371314624e-06}, {"id": 900, "seek": 442332, "start": 4443.639999999999, "end": 4445.639999999999, "text": " for you", "tokens": [337, 291], "temperature": 0.0, "avg_logprob": -0.15753635705686084, "compression_ratio": 1.7380952380952381, "no_speech_prob": 2.1907692371314624e-06}, {"id": 901, "seek": 442332, "start": 4445.679999999999, "end": 4452.719999999999, "text": " So it's a really really important area it covers like all of deep learning lots of modern machine learning in general and", "tokens": [407, 309, 311, 257, 534, 534, 1021, 1859, 309, 10538, 411, 439, 295, 2452, 2539, 3195, 295, 4363, 3479, 2539, 294, 2674, 293], "temperature": 0.0, "avg_logprob": -0.15753635705686084, "compression_ratio": 1.7380952380952381, "no_speech_prob": 2.1907692371314624e-06}, {"id": 902, "seek": 445272, "start": 4452.72, "end": 4459.68, "text": " So even though you're not going to like define matrix multiplication. You're very likely to want to define some other", "tokens": [407, 754, 1673, 291, 434, 406, 516, 281, 411, 6964, 8141, 27290, 13, 509, 434, 588, 3700, 281, 528, 281, 6964, 512, 661], "temperature": 0.0, "avg_logprob": -0.21161711843390213, "compression_ratio": 1.695852534562212, "no_speech_prob": 2.726457523749559e-06}, {"id": 903, "seek": 445272, "start": 4460.76, "end": 4462.84, "text": " Slightly different tensor product you know", "tokens": [318, 44872, 819, 40863, 1674, 291, 458], "temperature": 0.0, "avg_logprob": -0.21161711843390213, "compression_ratio": 1.695852534562212, "no_speech_prob": 2.726457523749559e-06}, {"id": 904, "seek": 445272, "start": 4464.240000000001, "end": 4466.72, "text": " So it's really useful to kind of understand how to do that", "tokens": [407, 309, 311, 534, 4420, 281, 733, 295, 1223, 577, 281, 360, 300], "temperature": 0.0, "avg_logprob": -0.21161711843390213, "compression_ratio": 1.695852534562212, "no_speech_prob": 2.726457523749559e-06}, {"id": 905, "seek": 445272, "start": 4467.72, "end": 4469.72, "text": " So let's go back and look at our", "tokens": [407, 718, 311, 352, 646, 293, 574, 412, 527], "temperature": 0.0, "avg_logprob": -0.21161711843390213, "compression_ratio": 1.695852534562212, "no_speech_prob": 2.726457523749559e-06}, {"id": 906, "seek": 445272, "start": 4470.76, "end": 4472.76, "text": " matrix and our", "tokens": [8141, 293, 527], "temperature": 0.0, "avg_logprob": -0.21161711843390213, "compression_ratio": 1.695852534562212, "no_speech_prob": 2.726457523749559e-06}, {"id": 907, "seek": 445272, "start": 4472.76, "end": 4474.280000000001, "text": " our", "tokens": [527], "temperature": 0.0, "avg_logprob": -0.21161711843390213, "compression_ratio": 1.695852534562212, "no_speech_prob": 2.726457523749559e-06}, {"id": 908, "seek": 445272, "start": 4474.280000000001, "end": 4478.04, "text": " 2d array and 1d array rank 2 tensor rank 1 tensor and", "tokens": [568, 67, 10225, 293, 502, 67, 10225, 6181, 568, 40863, 6181, 502, 40863, 293], "temperature": 0.0, "avg_logprob": -0.21161711843390213, "compression_ratio": 1.695852534562212, "no_speech_prob": 2.726457523749559e-06}, {"id": 909, "seek": 445272, "start": 4478.88, "end": 4480.88, "text": " Remember we can do a matrix multiplication", "tokens": [5459, 321, 393, 360, 257, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.21161711843390213, "compression_ratio": 1.695852534562212, "no_speech_prob": 2.726457523749559e-06}, {"id": 910, "seek": 448088, "start": 4480.88, "end": 4485.92, "text": " Using the at sign or the old way and pay dot matmul", "tokens": [11142, 264, 412, 1465, 420, 264, 1331, 636, 293, 1689, 5893, 3803, 76, 425], "temperature": 0.0, "avg_logprob": -0.2632026944841657, "compression_ratio": 1.5165562913907285, "no_speech_prob": 1.5534943713646499e-06}, {"id": 911, "seek": 448088, "start": 4486.04, "end": 4491.6, "text": " Okay, and so what that's actually doing when we do that is we're basically saying", "tokens": [1033, 11, 293, 370, 437, 300, 311, 767, 884, 562, 321, 360, 300, 307, 321, 434, 1936, 1566], "temperature": 0.0, "avg_logprob": -0.2632026944841657, "compression_ratio": 1.5165562913907285, "no_speech_prob": 1.5534943713646499e-06}, {"id": 912, "seek": 448088, "start": 4492.92, "end": 4494.0, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2632026944841657, "compression_ratio": 1.5165562913907285, "no_speech_prob": 1.5534943713646499e-06}, {"id": 913, "seek": 448088, "start": 4494.0, "end": 4496.4800000000005, "text": " 1 times 10 plus", "tokens": [502, 1413, 1266, 1804], "temperature": 0.0, "avg_logprob": -0.2632026944841657, "compression_ratio": 1.5165562913907285, "no_speech_prob": 1.5534943713646499e-06}, {"id": 914, "seek": 448088, "start": 4498.32, "end": 4501.96, "text": " 2 times 20 plus 3 times 30 is", "tokens": [568, 1413, 945, 1804, 805, 1413, 2217, 307], "temperature": 0.0, "avg_logprob": -0.2632026944841657, "compression_ratio": 1.5165562913907285, "no_speech_prob": 1.5534943713646499e-06}, {"id": 915, "seek": 448088, "start": 4502.8, "end": 4506.88, "text": " 140 right and so we do that for each row and", "tokens": [21548, 558, 293, 370, 321, 360, 300, 337, 1184, 5386, 293], "temperature": 0.0, "avg_logprob": -0.2632026944841657, "compression_ratio": 1.5165562913907285, "no_speech_prob": 1.5534943713646499e-06}, {"id": 916, "seek": 450688, "start": 4506.88, "end": 4512.72, "text": " We can go through and do the same thing for the next one and for the next one to get our result right?", "tokens": [492, 393, 352, 807, 293, 360, 264, 912, 551, 337, 264, 958, 472, 293, 337, 264, 958, 472, 281, 483, 527, 1874, 558, 30], "temperature": 0.0, "avg_logprob": -0.12866946762683346, "compression_ratio": 1.5241935483870968, "no_speech_prob": 3.96696304960642e-06}, {"id": 917, "seek": 450688, "start": 4514.4800000000005, "end": 4517.04, "text": " You could do that in torch as well", "tokens": [509, 727, 360, 300, 294, 27822, 382, 731], "temperature": 0.0, "avg_logprob": -0.12866946762683346, "compression_ratio": 1.5241935483870968, "no_speech_prob": 3.96696304960642e-06}, {"id": 918, "seek": 450688, "start": 4519.36, "end": 4521.36, "text": " We could make this a little shorter", "tokens": [492, 727, 652, 341, 257, 707, 11639], "temperature": 0.0, "avg_logprob": -0.12866946762683346, "compression_ratio": 1.5241935483870968, "no_speech_prob": 3.96696304960642e-06}, {"id": 919, "seek": 452136, "start": 4521.36, "end": 4533.24, "text": " Okay, same thing", "tokens": [1033, 11, 912, 551], "temperature": 0.0, "avg_logprob": -0.28995300375896954, "compression_ratio": 1.0136986301369864, "no_speech_prob": 5.338078608474461e-06}, {"id": 920, "seek": 452136, "start": 4539.719999999999, "end": 4545.2, "text": " Okay, but that is not matrix multiplication. What's that?", "tokens": [1033, 11, 457, 300, 307, 406, 8141, 27290, 13, 708, 311, 300, 30], "temperature": 0.0, "avg_logprob": -0.28995300375896954, "compression_ratio": 1.0136986301369864, "no_speech_prob": 5.338078608474461e-06}, {"id": 921, "seek": 454520, "start": 4545.2, "end": 4549.84, "text": " Okay element wise specifically we've got a matrix and a vector so", "tokens": [1033, 4478, 10829, 4682, 321, 600, 658, 257, 8141, 293, 257, 8062, 370], "temperature": 0.0, "avg_logprob": -0.47335029510130366, "compression_ratio": 1.5345622119815667, "no_speech_prob": 2.3687896373303374e-06}, {"id": 922, "seek": 454520, "start": 4551.08, "end": 4556.2, "text": " Broadcasting okay good, so we've got this is element wise with broadcasting but notice", "tokens": [14074, 48860, 1392, 665, 11, 370, 321, 600, 658, 341, 307, 4478, 10829, 365, 30024, 457, 3449], "temperature": 0.0, "avg_logprob": -0.47335029510130366, "compression_ratio": 1.5345622119815667, "no_speech_prob": 2.3687896373303374e-06}, {"id": 923, "seek": 454520, "start": 4556.96, "end": 4563.12, "text": " The numbers it's created 10 40 90 are the exact three numbers that I needed to", "tokens": [440, 3547, 309, 311, 2942, 1266, 3356, 4289, 366, 264, 1900, 1045, 3547, 300, 286, 2978, 281], "temperature": 0.0, "avg_logprob": -0.47335029510130366, "compression_ratio": 1.5345622119815667, "no_speech_prob": 2.3687896373303374e-06}, {"id": 924, "seek": 454520, "start": 4563.88, "end": 4565.88, "text": " Calculate when I did that first", "tokens": [3511, 2444, 473, 562, 286, 630, 300, 700], "temperature": 0.0, "avg_logprob": -0.47335029510130366, "compression_ratio": 1.5345622119815667, "no_speech_prob": 2.3687896373303374e-06}, {"id": 925, "seek": 454520, "start": 4566.2, "end": 4568.2, "text": " Piece of my matrix multiplication", "tokens": [42868, 295, 452, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.47335029510130366, "compression_ratio": 1.5345622119815667, "no_speech_prob": 2.3687896373303374e-06}, {"id": 926, "seek": 454520, "start": 4568.44, "end": 4571.16, "text": " So in other words if we sum this up", "tokens": [407, 294, 661, 2283, 498, 321, 2408, 341, 493], "temperature": 0.0, "avg_logprob": -0.47335029510130366, "compression_ratio": 1.5345622119815667, "no_speech_prob": 2.3687896373303374e-06}, {"id": 927, "seek": 457116, "start": 4571.16, "end": 4575.28, "text": " Over the columns which is axis equals one", "tokens": [4886, 264, 13766, 597, 307, 10298, 6915, 472], "temperature": 0.0, "avg_logprob": -0.7226579054346625, "compression_ratio": 1.3028169014084507, "no_speech_prob": 2.2252766029851045e-06}, {"id": 928, "seek": 457116, "start": 4577.28, "end": 4579.28, "text": " We get our matrix sector product", "tokens": [492, 483, 527, 8141, 6977, 1674], "temperature": 0.0, "avg_logprob": -0.7226579054346625, "compression_ratio": 1.3028169014084507, "no_speech_prob": 2.2252766029851045e-06}, {"id": 929, "seek": 457116, "start": 4580.5199999999995, "end": 4583.5199999999995, "text": " Okay, so we can kind of", "tokens": [1033, 11, 370, 321, 393, 733, 295], "temperature": 0.0, "avg_logprob": -0.7226579054346625, "compression_ratio": 1.3028169014084507, "no_speech_prob": 2.2252766029851045e-06}, {"id": 930, "seek": 457116, "start": 4585.5199999999995, "end": 4588.96, "text": " Do this stuff without special help from our library", "tokens": [1144, 341, 1507, 1553, 2121, 854, 490, 527, 6405], "temperature": 0.0, "avg_logprob": -0.7226579054346625, "compression_ratio": 1.3028169014084507, "no_speech_prob": 2.2252766029851045e-06}, {"id": 931, "seek": 457116, "start": 4590.28, "end": 4592.28, "text": " So now", "tokens": [407, 586], "temperature": 0.0, "avg_logprob": -0.7226579054346625, "compression_ratio": 1.3028169014084507, "no_speech_prob": 2.2252766029851045e-06}, {"id": 932, "seek": 457116, "start": 4592.599999999999, "end": 4594.84, "text": " Let's expand this out to an", "tokens": [961, 311, 5268, 341, 484, 281, 364], "temperature": 0.0, "avg_logprob": -0.7226579054346625, "compression_ratio": 1.3028169014084507, "no_speech_prob": 2.2252766029851045e-06}, {"id": 933, "seek": 459484, "start": 4594.84, "end": 4600.84, "text": " matrix matrix product so a matrix matrix product", "tokens": [8141, 8141, 1674, 370, 257, 8141, 8141, 1674], "temperature": 0.0, "avg_logprob": -0.7134449351917613, "compression_ratio": 1.673758865248227, "no_speech_prob": 5.285507995722583e-07}, {"id": 934, "seek": 459484, "start": 4602.12, "end": 4608.12, "text": " Looks like this. This is this great site called matrix multiplication dot xyz and", "tokens": [10027, 411, 341, 13, 639, 307, 341, 869, 3621, 1219, 8141, 27290, 5893, 2031, 37433, 293], "temperature": 0.0, "avg_logprob": -0.7134449351917613, "compression_ratio": 1.673758865248227, "no_speech_prob": 5.285507995722583e-07}, {"id": 935, "seek": 459484, "start": 4609.12, "end": 4612.12, "text": " It shows us this is what happens when we multiply two matrices", "tokens": [467, 3110, 505, 341, 307, 437, 2314, 562, 321, 12972, 732, 32284], "temperature": 0.0, "avg_logprob": -0.7134449351917613, "compression_ratio": 1.673758865248227, "no_speech_prob": 5.285507995722583e-07}, {"id": 936, "seek": 459484, "start": 4618.72, "end": 4622.72, "text": " Okay, so this is the matrix matrix product", "tokens": [1033, 11, 370, 341, 307, 264, 8141, 8141, 1674], "temperature": 0.0, "avg_logprob": -0.7134449351917613, "compression_ratio": 1.673758865248227, "no_speech_prob": 5.285507995722583e-07}, {"id": 937, "seek": 462272, "start": 4622.72, "end": 4626.4400000000005, "text": " Okay, that's what matrix multiplication is", "tokens": [1033, 11, 300, 311, 437, 8141, 27290, 307], "temperature": 0.0, "avg_logprob": -0.19469526539678159, "compression_ratio": 1.698224852071006, "no_speech_prob": 3.089485062446329e-06}, {"id": 938, "seek": 462272, "start": 4627.92, "end": 4631.4400000000005, "text": " Operationally speaking so in other words what we just did there", "tokens": [27946, 379, 4124, 370, 294, 661, 2283, 437, 321, 445, 630, 456], "temperature": 0.0, "avg_logprob": -0.19469526539678159, "compression_ratio": 1.698224852071006, "no_speech_prob": 3.089485062446329e-06}, {"id": 939, "seek": 462272, "start": 4633.64, "end": 4640.68, "text": " Was we first of all took the first column with the first row to get this one and", "tokens": [3027, 321, 700, 295, 439, 1890, 264, 700, 7738, 365, 264, 700, 5386, 281, 483, 341, 472, 293], "temperature": 0.0, "avg_logprob": -0.19469526539678159, "compression_ratio": 1.698224852071006, "no_speech_prob": 3.089485062446329e-06}, {"id": 940, "seek": 462272, "start": 4641.88, "end": 4649.08, "text": " Then we took the second column with the first row to get that one alright, so we're basically doing", "tokens": [1396, 321, 1890, 264, 1150, 7738, 365, 264, 700, 5386, 281, 483, 300, 472, 5845, 11, 370, 321, 434, 1936, 884], "temperature": 0.0, "avg_logprob": -0.19469526539678159, "compression_ratio": 1.698224852071006, "no_speech_prob": 3.089485062446329e-06}, {"id": 941, "seek": 464908, "start": 4649.08, "end": 4656.12, "text": " The thing we just did the matrix vector product. We're just doing it twice right once", "tokens": [440, 551, 321, 445, 630, 264, 8141, 8062, 1674, 13, 492, 434, 445, 884, 309, 6091, 558, 1564], "temperature": 0.0, "avg_logprob": -0.1765503663283128, "compression_ratio": 1.5365853658536586, "no_speech_prob": 1.1911058663827134e-06}, {"id": 942, "seek": 464908, "start": 4659.44, "end": 4664.5599999999995, "text": " With this column and once with this column, and then we concatenate the two together", "tokens": [2022, 341, 7738, 293, 1564, 365, 341, 7738, 11, 293, 550, 321, 1588, 7186, 473, 264, 732, 1214], "temperature": 0.0, "avg_logprob": -0.1765503663283128, "compression_ratio": 1.5365853658536586, "no_speech_prob": 1.1911058663827134e-06}, {"id": 943, "seek": 464908, "start": 4665.96, "end": 4669.74, "text": " Okay, so we can now go ahead and do that", "tokens": [1033, 11, 370, 321, 393, 586, 352, 2286, 293, 360, 300], "temperature": 0.0, "avg_logprob": -0.1765503663283128, "compression_ratio": 1.5365853658536586, "no_speech_prob": 1.1911058663827134e-06}, {"id": 944, "seek": 466974, "start": 4669.74, "end": 4677.7, "text": " Like so M times the first column dot sum", "tokens": [1743, 370, 376, 1413, 264, 700, 7738, 5893, 2408], "temperature": 0.0, "avg_logprob": -0.20104685916176326, "compression_ratio": 1.650273224043716, "no_speech_prob": 1.287892814616498e-06}, {"id": 945, "seek": 466974, "start": 4679.179999999999, "end": 4686.5, "text": " M times the second top column dot sum and so there are the two columns of our matrix multiplication", "tokens": [376, 1413, 264, 1150, 1192, 7738, 5893, 2408, 293, 370, 456, 366, 264, 732, 13766, 295, 527, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.20104685916176326, "compression_ratio": 1.650273224043716, "no_speech_prob": 1.287892814616498e-06}, {"id": 946, "seek": 466974, "start": 4687.66, "end": 4689.26, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.20104685916176326, "compression_ratio": 1.650273224043716, "no_speech_prob": 1.287892814616498e-06}, {"id": 947, "seek": 466974, "start": 4689.26, "end": 4692.82, "text": " So I didn't want to like make our code too messy", "tokens": [407, 286, 994, 380, 528, 281, 411, 652, 527, 3089, 886, 16191], "temperature": 0.0, "avg_logprob": -0.20104685916176326, "compression_ratio": 1.650273224043716, "no_speech_prob": 1.287892814616498e-06}, {"id": 948, "seek": 469282, "start": 4692.82, "end": 4699.58, "text": " So I'm not going to actually like use that but like we have it there now if we want to we don't need to use", "tokens": [407, 286, 478, 406, 516, 281, 767, 411, 764, 300, 457, 411, 321, 362, 309, 456, 586, 498, 321, 528, 281, 321, 500, 380, 643, 281, 764], "temperature": 0.0, "avg_logprob": -0.16348321814286082, "compression_ratio": 1.4789473684210526, "no_speech_prob": 4.495154371397803e-06}, {"id": 949, "seek": 469282, "start": 4700.299999999999, "end": 4706.58, "text": " Torch or numpy matrix multiplication anymore. We've got we've got our own that we can use using nothing but", "tokens": [7160, 339, 420, 1031, 8200, 8141, 27290, 3602, 13, 492, 600, 658, 321, 600, 658, 527, 1065, 300, 321, 393, 764, 1228, 1825, 457], "temperature": 0.0, "avg_logprob": -0.16348321814286082, "compression_ratio": 1.4789473684210526, "no_speech_prob": 4.495154371397803e-06}, {"id": 950, "seek": 469282, "start": 4707.099999999999, "end": 4709.66, "text": " element wise operations broadcasting and", "tokens": [4478, 10829, 7705, 30024, 293], "temperature": 0.0, "avg_logprob": -0.16348321814286082, "compression_ratio": 1.4789473684210526, "no_speech_prob": 4.495154371397803e-06}, {"id": 951, "seek": 469282, "start": 4710.7, "end": 4712.7, "text": " some", "tokens": [512], "temperature": 0.0, "avg_logprob": -0.16348321814286082, "compression_ratio": 1.4789473684210526, "no_speech_prob": 4.495154371397803e-06}, {"id": 952, "seek": 469282, "start": 4714.0199999999995, "end": 4716.0199999999995, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.16348321814286082, "compression_ratio": 1.4789473684210526, "no_speech_prob": 4.495154371397803e-06}, {"id": 953, "seek": 469282, "start": 4717.94, "end": 4719.94, "text": " So this is our", "tokens": [407, 341, 307, 527], "temperature": 0.0, "avg_logprob": -0.16348321814286082, "compression_ratio": 1.4789473684210526, "no_speech_prob": 4.495154371397803e-06}, {"id": 954, "seek": 471994, "start": 4719.94, "end": 4725.299999999999, "text": " Logistic regression from scratch class again. I just copied it here", "tokens": [10824, 3142, 24590, 490, 8459, 1508, 797, 13, 286, 445, 25365, 309, 510], "temperature": 0.0, "avg_logprob": -0.2028975603057117, "compression_ratio": 1.5121951219512195, "no_speech_prob": 2.295905233040685e-06}, {"id": 955, "seek": 471994, "start": 4726.0199999999995, "end": 4728.54, "text": " Here is where we instantiate the object copy it to the GPU", "tokens": [1692, 307, 689, 321, 9836, 13024, 264, 2657, 5055, 309, 281, 264, 18407], "temperature": 0.0, "avg_logprob": -0.2028975603057117, "compression_ratio": 1.5121951219512195, "no_speech_prob": 2.295905233040685e-06}, {"id": 956, "seek": 471994, "start": 4728.82, "end": 4733.24, "text": " We create an optimizer which we'll learn about in a moment, and we call fit okay", "tokens": [492, 1884, 364, 5028, 6545, 597, 321, 603, 1466, 466, 294, 257, 1623, 11, 293, 321, 818, 3318, 1392], "temperature": 0.0, "avg_logprob": -0.2028975603057117, "compression_ratio": 1.5121951219512195, "no_speech_prob": 2.295905233040685e-06}, {"id": 957, "seek": 471994, "start": 4733.259999999999, "end": 4738.62, "text": " So the goal is to now repeat this without needing to call fit", "tokens": [407, 264, 3387, 307, 281, 586, 7149, 341, 1553, 18006, 281, 818, 3318], "temperature": 0.0, "avg_logprob": -0.2028975603057117, "compression_ratio": 1.5121951219512195, "no_speech_prob": 2.295905233040685e-06}, {"id": 958, "seek": 471994, "start": 4741.78, "end": 4743.78, "text": " So to do that", "tokens": [407, 281, 360, 300], "temperature": 0.0, "avg_logprob": -0.2028975603057117, "compression_ratio": 1.5121951219512195, "no_speech_prob": 2.295905233040685e-06}, {"id": 959, "seek": 474378, "start": 4743.78, "end": 4749.34, "text": " We're going to need a loop", "tokens": [492, 434, 516, 281, 643, 257, 6367], "temperature": 0.0, "avg_logprob": -0.13181149229711417, "compression_ratio": 1.7725118483412323, "no_speech_prob": 1.5779542081872933e-06}, {"id": 960, "seek": 474378, "start": 4750.179999999999, "end": 4754.74, "text": " Which grabs a mini batch of data at a time and with each mini batch of data?", "tokens": [3013, 30028, 257, 8382, 15245, 295, 1412, 412, 257, 565, 293, 365, 1184, 8382, 15245, 295, 1412, 30], "temperature": 0.0, "avg_logprob": -0.13181149229711417, "compression_ratio": 1.7725118483412323, "no_speech_prob": 1.5779542081872933e-06}, {"id": 961, "seek": 474378, "start": 4755.7, "end": 4758.0199999999995, "text": " We need to pass it to the optimizer and", "tokens": [492, 643, 281, 1320, 309, 281, 264, 5028, 6545, 293], "temperature": 0.0, "avg_logprob": -0.13181149229711417, "compression_ratio": 1.7725118483412323, "no_speech_prob": 1.5779542081872933e-06}, {"id": 962, "seek": 474378, "start": 4758.66, "end": 4763.34, "text": " Say please try to come up with a slightly better set of predictions for this mini batch", "tokens": [6463, 1767, 853, 281, 808, 493, 365, 257, 4748, 1101, 992, 295, 21264, 337, 341, 8382, 15245], "temperature": 0.0, "avg_logprob": -0.13181149229711417, "compression_ratio": 1.7725118483412323, "no_speech_prob": 1.5779542081872933e-06}, {"id": 963, "seek": 474378, "start": 4764.34, "end": 4768.58, "text": " So as we learned in order to grab a mini batch of the training set at a time", "tokens": [407, 382, 321, 3264, 294, 1668, 281, 4444, 257, 8382, 15245, 295, 264, 3097, 992, 412, 257, 565], "temperature": 0.0, "avg_logprob": -0.13181149229711417, "compression_ratio": 1.7725118483412323, "no_speech_prob": 1.5779542081872933e-06}, {"id": 964, "seek": 474378, "start": 4768.58, "end": 4771.86, "text": " We have to ask the model data object for the training data loader", "tokens": [492, 362, 281, 1029, 264, 2316, 1412, 2657, 337, 264, 3097, 1412, 3677, 260], "temperature": 0.0, "avg_logprob": -0.13181149229711417, "compression_ratio": 1.7725118483412323, "no_speech_prob": 1.5779542081872933e-06}, {"id": 965, "seek": 477186, "start": 4771.86, "end": 4776.98, "text": " We have to wrap it in either either to create an iterator or a generator", "tokens": [492, 362, 281, 7019, 309, 294, 2139, 2139, 281, 1884, 364, 17138, 1639, 420, 257, 19265], "temperature": 0.0, "avg_logprob": -0.265233049687651, "compression_ratio": 1.6847290640394088, "no_speech_prob": 9.570796919433633e-07}, {"id": 966, "seek": 477186, "start": 4777.98, "end": 4783.94, "text": " And so that gives us our data loader okay, so pi torch calls this a data loader", "tokens": [400, 370, 300, 2709, 505, 527, 1412, 3677, 260, 1392, 11, 370, 3895, 27822, 5498, 341, 257, 1412, 3677, 260], "temperature": 0.0, "avg_logprob": -0.265233049687651, "compression_ratio": 1.6847290640394088, "no_speech_prob": 9.570796919433633e-07}, {"id": 967, "seek": 477186, "start": 4784.099999999999, "end": 4788.139999999999, "text": " We actually wrote our own fast AI data loader, but it's it's all it's basically the same idea", "tokens": [492, 767, 4114, 527, 1065, 2370, 7318, 1412, 3677, 260, 11, 457, 309, 311, 309, 311, 439, 309, 311, 1936, 264, 912, 1558], "temperature": 0.0, "avg_logprob": -0.265233049687651, "compression_ratio": 1.6847290640394088, "no_speech_prob": 9.570796919433633e-07}, {"id": 968, "seek": 477186, "start": 4789.0199999999995, "end": 4790.299999999999, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.265233049687651, "compression_ratio": 1.6847290640394088, "no_speech_prob": 9.570796919433633e-07}, {"id": 969, "seek": 477186, "start": 4790.299999999999, "end": 4795.5199999999995, "text": " So the next thing we do is we grab the X and the Y", "tokens": [407, 264, 958, 551, 321, 360, 307, 321, 4444, 264, 1783, 293, 264, 398], "temperature": 0.0, "avg_logprob": -0.265233049687651, "compression_ratio": 1.6847290640394088, "no_speech_prob": 9.570796919433633e-07}, {"id": 970, "seek": 477186, "start": 4796.0599999999995, "end": 4798.74, "text": " tensor the next one from our data loader", "tokens": [40863, 264, 958, 472, 490, 527, 1412, 3677, 260], "temperature": 0.0, "avg_logprob": -0.265233049687651, "compression_ratio": 1.6847290640394088, "no_speech_prob": 9.570796919433633e-07}, {"id": 971, "seek": 479874, "start": 4798.74, "end": 4800.58, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.17179701063368055, "compression_ratio": 1.7553191489361701, "no_speech_prob": 1.7330470427623368e-06}, {"id": 972, "seek": 479874, "start": 4800.58, "end": 4808.0199999999995, "text": " Wrap it in a variable to say I need to be able to take the derivative of the calculations using this because if I can't", "tokens": [41291, 309, 294, 257, 7006, 281, 584, 286, 643, 281, 312, 1075, 281, 747, 264, 13760, 295, 264, 20448, 1228, 341, 570, 498, 286, 393, 380], "temperature": 0.0, "avg_logprob": -0.17179701063368055, "compression_ratio": 1.7553191489361701, "no_speech_prob": 1.7330470427623368e-06}, {"id": 973, "seek": 479874, "start": 4808.0199999999995, "end": 4811.98, "text": " Take the derivative then I can't get the gradients, and I can't update the weights", "tokens": [3664, 264, 13760, 550, 286, 393, 380, 483, 264, 2771, 2448, 11, 293, 286, 393, 380, 5623, 264, 17443], "temperature": 0.0, "avg_logprob": -0.17179701063368055, "compression_ratio": 1.7553191489361701, "no_speech_prob": 1.7330470427623368e-06}, {"id": 974, "seek": 479874, "start": 4812.46, "end": 4815.9, "text": " right, and I need to put it on the GPU because my", "tokens": [558, 11, 293, 286, 643, 281, 829, 309, 322, 264, 18407, 570, 452], "temperature": 0.0, "avg_logprob": -0.17179701063368055, "compression_ratio": 1.7553191489361701, "no_speech_prob": 1.7330470427623368e-06}, {"id": 975, "seek": 479874, "start": 4816.78, "end": 4818.78, "text": " module is on the GPU and", "tokens": [10088, 307, 322, 264, 18407, 293], "temperature": 0.0, "avg_logprob": -0.17179701063368055, "compression_ratio": 1.7553191489361701, "no_speech_prob": 1.7330470427623368e-06}, {"id": 976, "seek": 479874, "start": 4820.0599999999995, "end": 4823.42, "text": " So we can now take that variable and pass it to", "tokens": [407, 321, 393, 586, 747, 300, 7006, 293, 1320, 309, 281], "temperature": 0.0, "avg_logprob": -0.17179701063368055, "compression_ratio": 1.7553191489361701, "no_speech_prob": 1.7330470427623368e-06}, {"id": 977, "seek": 482342, "start": 4823.42, "end": 4828.4800000000005, "text": " To the object that we instantiated our logistic regression", "tokens": [1407, 264, 2657, 300, 321, 9836, 72, 770, 527, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.23289014215338721, "compression_ratio": 1.5191256830601092, "no_speech_prob": 3.2377463412558427e-06}, {"id": 978, "seek": 482342, "start": 4828.9, "end": 4832.86, "text": " Remember our module we can use it as if it's a function because that's how pi torch works", "tokens": [5459, 527, 10088, 321, 393, 764, 309, 382, 498, 309, 311, 257, 2445, 570, 300, 311, 577, 3895, 27822, 1985], "temperature": 0.0, "avg_logprob": -0.23289014215338721, "compression_ratio": 1.5191256830601092, "no_speech_prob": 3.2377463412558427e-06}, {"id": 979, "seek": 482342, "start": 4833.46, "end": 4837.9400000000005, "text": " And that gives us a set of predictions as we saw seen before", "tokens": [400, 300, 2709, 505, 257, 992, 295, 21264, 382, 321, 1866, 1612, 949], "temperature": 0.0, "avg_logprob": -0.23289014215338721, "compression_ratio": 1.5191256830601092, "no_speech_prob": 3.2377463412558427e-06}, {"id": 980, "seek": 482342, "start": 4839.38, "end": 4841.38, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.23289014215338721, "compression_ratio": 1.5191256830601092, "no_speech_prob": 3.2377463412558427e-06}, {"id": 981, "seek": 482342, "start": 4841.82, "end": 4846.5, "text": " So now we can check the loss and the loss we defined as being a", "tokens": [407, 586, 321, 393, 1520, 264, 4470, 293, 264, 4470, 321, 7642, 382, 885, 257], "temperature": 0.0, "avg_logprob": -0.23289014215338721, "compression_ratio": 1.5191256830601092, "no_speech_prob": 3.2377463412558427e-06}, {"id": 982, "seek": 484650, "start": 4846.5, "end": 4853.74, "text": " Negative log likelihood loss object and we're going to learn about how that's calculated in the next lesson", "tokens": [43230, 3565, 22119, 4470, 2657, 293, 321, 434, 516, 281, 1466, 466, 577, 300, 311, 15598, 294, 264, 958, 6898], "temperature": 0.0, "avg_logprob": -0.2077058739618424, "compression_ratio": 1.8154981549815499, "no_speech_prob": 9.570800330038765e-07}, {"id": 983, "seek": 484650, "start": 4854.1, "end": 4858.38, "text": " For now think of it. Just like root mean squared error, but for classification problems", "tokens": [1171, 586, 519, 295, 309, 13, 1449, 411, 5593, 914, 8889, 6713, 11, 457, 337, 21538, 2740], "temperature": 0.0, "avg_logprob": -0.2077058739618424, "compression_ratio": 1.8154981549815499, "no_speech_prob": 9.570800330038765e-07}, {"id": 984, "seek": 484650, "start": 4859.5, "end": 4866.14, "text": " So we can call that also just like a function so you can kind of see this is very general idea in pi torch that you know", "tokens": [407, 321, 393, 818, 300, 611, 445, 411, 257, 2445, 370, 291, 393, 733, 295, 536, 341, 307, 588, 2674, 1558, 294, 3895, 27822, 300, 291, 458], "temperature": 0.0, "avg_logprob": -0.2077058739618424, "compression_ratio": 1.8154981549815499, "no_speech_prob": 9.570800330038765e-07}, {"id": 985, "seek": 484650, "start": 4866.14, "end": 4870.62, "text": " Kind of treat everything ideally like it's a function so in this case we have a loss", "tokens": [9242, 295, 2387, 1203, 22915, 411, 309, 311, 257, 2445, 370, 294, 341, 1389, 321, 362, 257, 4470], "temperature": 0.0, "avg_logprob": -0.2077058739618424, "compression_ratio": 1.8154981549815499, "no_speech_prob": 9.570800330038765e-07}, {"id": 986, "seek": 487062, "start": 4870.62, "end": 4876.58, "text": " Negative log likelihood loss object we could treat it like a function we pass in our predictions and", "tokens": [43230, 3565, 22119, 4470, 2657, 321, 727, 2387, 309, 411, 257, 2445, 321, 1320, 294, 527, 21264, 293], "temperature": 0.0, "avg_logprob": -0.17761725048686183, "compression_ratio": 1.709090909090909, "no_speech_prob": 1.5534945987383253e-06}, {"id": 987, "seek": 487062, "start": 4877.26, "end": 4883.42, "text": " We pass in our actuals right again the actuals need to be turned into a variable and put on the GPU", "tokens": [492, 1320, 294, 527, 3539, 82, 558, 797, 264, 3539, 82, 643, 281, 312, 3574, 666, 257, 7006, 293, 829, 322, 264, 18407], "temperature": 0.0, "avg_logprob": -0.17761725048686183, "compression_ratio": 1.709090909090909, "no_speech_prob": 1.5534945987383253e-06}, {"id": 988, "seek": 487062, "start": 4883.94, "end": 4890.98, "text": " Because the loss is specifically the thing that we actually want to take the derivative of right so that gives us our loss", "tokens": [1436, 264, 4470, 307, 4682, 264, 551, 300, 321, 767, 528, 281, 747, 264, 13760, 295, 558, 370, 300, 2709, 505, 527, 4470], "temperature": 0.0, "avg_logprob": -0.17761725048686183, "compression_ratio": 1.709090909090909, "no_speech_prob": 1.5534945987383253e-06}, {"id": 989, "seek": 487062, "start": 4892.54, "end": 4895.0, "text": " And there it is that's our loss two point four three", "tokens": [400, 456, 309, 307, 300, 311, 527, 4470, 732, 935, 1451, 1045], "temperature": 0.0, "avg_logprob": -0.17761725048686183, "compression_ratio": 1.709090909090909, "no_speech_prob": 1.5534945987383253e-06}, {"id": 990, "seek": 489500, "start": 4895.0, "end": 4901.28, "text": " Okay, so it's a variable and because it's a variable it knows how it was calculated", "tokens": [1033, 11, 370, 309, 311, 257, 7006, 293, 570, 309, 311, 257, 7006, 309, 3255, 577, 309, 390, 15598], "temperature": 0.0, "avg_logprob": -0.21367576122283935, "compression_ratio": 1.8440860215053763, "no_speech_prob": 9.874627266981406e-07}, {"id": 991, "seek": 489500, "start": 4902.0, "end": 4907.98, "text": " All right, it knows it was calculated with this loss function. It knows that the predictions were calculated with this", "tokens": [1057, 558, 11, 309, 3255, 309, 390, 15598, 365, 341, 4470, 2445, 13, 467, 3255, 300, 264, 21264, 645, 15598, 365, 341], "temperature": 0.0, "avg_logprob": -0.21367576122283935, "compression_ratio": 1.8440860215053763, "no_speech_prob": 9.874627266981406e-07}, {"id": 992, "seek": 489500, "start": 4908.92, "end": 4915.88, "text": " Network it knows that this network consisted of these operations and so we can get the gradient", "tokens": [12640, 309, 3255, 300, 341, 3209, 38227, 295, 613, 7705, 293, 370, 321, 393, 483, 264, 16235], "temperature": 0.0, "avg_logprob": -0.21367576122283935, "compression_ratio": 1.8440860215053763, "no_speech_prob": 9.874627266981406e-07}, {"id": 993, "seek": 489500, "start": 4916.8, "end": 4918.8, "text": " Automatically, right?", "tokens": [24619, 5030, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21367576122283935, "compression_ratio": 1.8440860215053763, "no_speech_prob": 9.874627266981406e-07}, {"id": 994, "seek": 489500, "start": 4918.92, "end": 4920.92, "text": " So to get the gradient", "tokens": [407, 281, 483, 264, 16235], "temperature": 0.0, "avg_logprob": -0.21367576122283935, "compression_ratio": 1.8440860215053763, "no_speech_prob": 9.874627266981406e-07}, {"id": 995, "seek": 492092, "start": 4920.92, "end": 4926.56, "text": " We call L dot backward remember L is the thing that contains our loss", "tokens": [492, 818, 441, 5893, 23897, 1604, 441, 307, 264, 551, 300, 8306, 527, 4470], "temperature": 0.0, "avg_logprob": -0.1587851881980896, "compression_ratio": 1.731958762886598, "no_speech_prob": 2.123369768014527e-06}, {"id": 996, "seek": 492092, "start": 4927.6, "end": 4932.36, "text": " All right, so L dot backward is is something which is added to anything", "tokens": [1057, 558, 11, 370, 441, 5893, 23897, 307, 307, 746, 597, 307, 3869, 281, 1340], "temperature": 0.0, "avg_logprob": -0.1587851881980896, "compression_ratio": 1.731958762886598, "no_speech_prob": 2.123369768014527e-06}, {"id": 997, "seek": 492092, "start": 4932.36, "end": 4936.4400000000005, "text": " That's a variable you can call dot backward and that says please calculate the gradients", "tokens": [663, 311, 257, 7006, 291, 393, 818, 5893, 23897, 293, 300, 1619, 1767, 8873, 264, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.1587851881980896, "compression_ratio": 1.731958762886598, "no_speech_prob": 2.123369768014527e-06}, {"id": 998, "seek": 492092, "start": 4937.56, "end": 4944.12, "text": " Okay, and so that calculates the gradients and stores them inside that that", "tokens": [1033, 11, 293, 370, 300, 4322, 1024, 264, 2771, 2448, 293, 9512, 552, 1854, 300, 300], "temperature": 0.0, "avg_logprob": -0.1587851881980896, "compression_ratio": 1.731958762886598, "no_speech_prob": 2.123369768014527e-06}, {"id": 999, "seek": 492092, "start": 4945.4, "end": 4947.4, "text": " the basically for each of the", "tokens": [264, 1936, 337, 1184, 295, 264], "temperature": 0.0, "avg_logprob": -0.1587851881980896, "compression_ratio": 1.731958762886598, "no_speech_prob": 2.123369768014527e-06}, {"id": 1000, "seek": 494740, "start": 4947.4, "end": 4953.5599999999995, "text": " Weights that was used it used each of the parameters that was used to calculate that it's now stored", "tokens": [492, 5761, 300, 390, 1143, 309, 1143, 1184, 295, 264, 9834, 300, 390, 1143, 281, 8873, 300, 309, 311, 586, 12187], "temperature": 0.0, "avg_logprob": -0.19800424121675037, "compression_ratio": 1.8081632653061224, "no_speech_prob": 1.1544598237378523e-06}, {"id": 1001, "seek": 494740, "start": 4954.36, "end": 4960.32, "text": " A dot grad we'll see it later. It's basically stored the gradient right so we can then call", "tokens": [316, 5893, 2771, 321, 603, 536, 309, 1780, 13, 467, 311, 1936, 12187, 264, 16235, 558, 370, 321, 393, 550, 818], "temperature": 0.0, "avg_logprob": -0.19800424121675037, "compression_ratio": 1.8081632653061224, "no_speech_prob": 1.1544598237378523e-06}, {"id": 1002, "seek": 494740, "start": 4961.08, "end": 4964.5199999999995, "text": " Optimizer dot step and we're going to do this step manually shortly", "tokens": [35013, 6545, 5893, 1823, 293, 321, 434, 516, 281, 360, 341, 1823, 16945, 13392], "temperature": 0.0, "avg_logprob": -0.19800424121675037, "compression_ratio": 1.8081632653061224, "no_speech_prob": 1.1544598237378523e-06}, {"id": 1003, "seek": 494740, "start": 4964.5199999999995, "end": 4971.92, "text": " And that's the bit that says please make the weights a little bit better right and so what optimizer dot step is doing", "tokens": [400, 300, 311, 264, 857, 300, 1619, 1767, 652, 264, 17443, 257, 707, 857, 1101, 558, 293, 370, 437, 5028, 6545, 5893, 1823, 307, 884], "temperature": 0.0, "avg_logprob": -0.19800424121675037, "compression_ratio": 1.8081632653061224, "no_speech_prob": 1.1544598237378523e-06}, {"id": 1004, "seek": 497192, "start": 4971.92, "end": 4977.52, "text": " Is it saying like okay if you had like a really simple function?", "tokens": [1119, 309, 1566, 411, 1392, 498, 291, 632, 411, 257, 534, 2199, 2445, 30], "temperature": 0.0, "avg_logprob": -0.18598916079546954, "compression_ratio": 1.5698924731182795, "no_speech_prob": 1.505698151049728e-06}, {"id": 1005, "seek": 497192, "start": 4982.6, "end": 4984.6, "text": " Like this", "tokens": [1743, 341], "temperature": 0.0, "avg_logprob": -0.18598916079546954, "compression_ratio": 1.5698924731182795, "no_speech_prob": 1.505698151049728e-06}, {"id": 1006, "seek": 497192, "start": 4985.32, "end": 4991.58, "text": " All right, then what the optimizer does is it says okay? Let's pick a random starting point", "tokens": [1057, 558, 11, 550, 437, 264, 5028, 6545, 775, 307, 309, 1619, 1392, 30, 961, 311, 1888, 257, 4974, 2891, 935], "temperature": 0.0, "avg_logprob": -0.18598916079546954, "compression_ratio": 1.5698924731182795, "no_speech_prob": 1.505698151049728e-06}, {"id": 1007, "seek": 497192, "start": 4992.72, "end": 4997.4400000000005, "text": " Right and let's calculate the value of the loss right so here's our parameter", "tokens": [1779, 293, 718, 311, 8873, 264, 2158, 295, 264, 4470, 558, 370, 510, 311, 527, 13075], "temperature": 0.0, "avg_logprob": -0.18598916079546954, "compression_ratio": 1.5698924731182795, "no_speech_prob": 1.505698151049728e-06}, {"id": 1008, "seek": 499744, "start": 4997.44, "end": 5001.32, "text": " Here's our loss right let's take the derivative", "tokens": [1692, 311, 527, 4470, 558, 718, 311, 747, 264, 13760], "temperature": 0.0, "avg_logprob": -0.24092070589360504, "compression_ratio": 2.026178010471204, "no_speech_prob": 1.4367490166478092e-06}, {"id": 1009, "seek": 499744, "start": 5003.32, "end": 5008.48, "text": " All right the derivative tells us which way is down so it tells us we need to go that direction", "tokens": [1057, 558, 264, 13760, 5112, 505, 597, 636, 307, 760, 370, 309, 5112, 505, 321, 643, 281, 352, 300, 3513], "temperature": 0.0, "avg_logprob": -0.24092070589360504, "compression_ratio": 2.026178010471204, "no_speech_prob": 1.4367490166478092e-06}, {"id": 1010, "seek": 499744, "start": 5009.5599999999995, "end": 5011.919999999999, "text": " Okay, and we take a small step and", "tokens": [1033, 11, 293, 321, 747, 257, 1359, 1823, 293], "temperature": 0.0, "avg_logprob": -0.24092070589360504, "compression_ratio": 2.026178010471204, "no_speech_prob": 1.4367490166478092e-06}, {"id": 1011, "seek": 499744, "start": 5013.44, "end": 5018.32, "text": " Then we take the derivative again, and we take a small step derivative again take a small step", "tokens": [1396, 321, 747, 264, 13760, 797, 11, 293, 321, 747, 257, 1359, 1823, 13760, 797, 747, 257, 1359, 1823], "temperature": 0.0, "avg_logprob": -0.24092070589360504, "compression_ratio": 2.026178010471204, "no_speech_prob": 1.4367490166478092e-06}, {"id": 1012, "seek": 499744, "start": 5018.48, "end": 5020.48, "text": " Give it again take a small step", "tokens": [5303, 309, 797, 747, 257, 1359, 1823], "temperature": 0.0, "avg_logprob": -0.24092070589360504, "compression_ratio": 2.026178010471204, "no_speech_prob": 1.4367490166478092e-06}, {"id": 1013, "seek": 499744, "start": 5021.12, "end": 5025.5599999999995, "text": " Until eventually we're taking such small steps that we stop okay, so that's what?", "tokens": [9088, 4728, 321, 434, 1940, 1270, 1359, 4439, 300, 321, 1590, 1392, 11, 370, 300, 311, 437, 30], "temperature": 0.0, "avg_logprob": -0.24092070589360504, "compression_ratio": 2.026178010471204, "no_speech_prob": 1.4367490166478092e-06}, {"id": 1014, "seek": 502556, "start": 5025.56, "end": 5027.56, "text": " gradient descent does", "tokens": [16235, 23475, 775], "temperature": 0.0, "avg_logprob": -0.24106870723676077, "compression_ratio": 1.61139896373057, "no_speech_prob": 1.2482689726311946e-06}, {"id": 1015, "seek": 502556, "start": 5028.360000000001, "end": 5030.080000000001, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.24106870723676077, "compression_ratio": 1.61139896373057, "no_speech_prob": 1.2482689726311946e-06}, {"id": 1016, "seek": 502556, "start": 5030.080000000001, "end": 5037.320000000001, "text": " How big a step is a small step well we basically take the derivative here, so let's say derivative there is like eight", "tokens": [1012, 955, 257, 1823, 307, 257, 1359, 1823, 731, 321, 1936, 747, 264, 13760, 510, 11, 370, 718, 311, 584, 13760, 456, 307, 411, 3180], "temperature": 0.0, "avg_logprob": -0.24106870723676077, "compression_ratio": 1.61139896373057, "no_speech_prob": 1.2482689726311946e-06}, {"id": 1017, "seek": 502556, "start": 5037.76, "end": 5044.080000000001, "text": " All right, and we multiply it by a small number like say oh point oh one and that tells us", "tokens": [1057, 558, 11, 293, 321, 12972, 309, 538, 257, 1359, 1230, 411, 584, 1954, 935, 1954, 472, 293, 300, 5112, 505], "temperature": 0.0, "avg_logprob": -0.24106870723676077, "compression_ratio": 1.61139896373057, "no_speech_prob": 1.2482689726311946e-06}, {"id": 1018, "seek": 502556, "start": 5044.080000000001, "end": 5046.080000000001, "text": " What step size to take?", "tokens": [708, 1823, 2744, 281, 747, 30], "temperature": 0.0, "avg_logprob": -0.24106870723676077, "compression_ratio": 1.61139896373057, "no_speech_prob": 1.2482689726311946e-06}, {"id": 1019, "seek": 502556, "start": 5046.64, "end": 5049.780000000001, "text": " This small number here is called the learning rate", "tokens": [639, 1359, 1230, 510, 307, 1219, 264, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.24106870723676077, "compression_ratio": 1.61139896373057, "no_speech_prob": 1.2482689726311946e-06}, {"id": 1020, "seek": 504978, "start": 5049.78, "end": 5057.219999999999, "text": " And it's the most important hyper parameter to set right if you pick two smaller learning rate", "tokens": [400, 309, 311, 264, 881, 1021, 9848, 13075, 281, 992, 558, 498, 291, 1888, 732, 4356, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.19855856284117088, "compression_ratio": 1.6772486772486772, "no_speech_prob": 7.93448748481751e-07}, {"id": 1021, "seek": 504978, "start": 5057.78, "end": 5063.219999999999, "text": " Then your steps down are going to be like tiny and it's going to take you forever", "tokens": [1396, 428, 4439, 760, 366, 516, 281, 312, 411, 5870, 293, 309, 311, 516, 281, 747, 291, 5680], "temperature": 0.0, "avg_logprob": -0.19855856284117088, "compression_ratio": 1.6772486772486772, "no_speech_prob": 7.93448748481751e-07}, {"id": 1022, "seek": 504978, "start": 5064.259999999999, "end": 5067.98, "text": " All right to a bigger learning rate, and you'll jump too far", "tokens": [1057, 558, 281, 257, 3801, 2539, 3314, 11, 293, 291, 603, 3012, 886, 1400], "temperature": 0.0, "avg_logprob": -0.19855856284117088, "compression_ratio": 1.6772486772486772, "no_speech_prob": 7.93448748481751e-07}, {"id": 1023, "seek": 504978, "start": 5070.179999999999, "end": 5075.719999999999, "text": " Right and then you'll jump too far and your diverge rather than converge, okay?", "tokens": [1779, 293, 550, 291, 603, 3012, 886, 1400, 293, 428, 18558, 432, 2831, 813, 41881, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.19855856284117088, "compression_ratio": 1.6772486772486772, "no_speech_prob": 7.93448748481751e-07}, {"id": 1024, "seek": 507572, "start": 5075.72, "end": 5080.8, "text": " We're not going to talk about how to pick a learning rate in this class, but in the deep learning class", "tokens": [492, 434, 406, 516, 281, 751, 466, 577, 281, 1888, 257, 2539, 3314, 294, 341, 1508, 11, 457, 294, 264, 2452, 2539, 1508], "temperature": 0.0, "avg_logprob": -0.09761820168330751, "compression_ratio": 1.7544642857142858, "no_speech_prob": 2.6425766463944456e-06}, {"id": 1025, "seek": 507572, "start": 5080.8, "end": 5086.240000000001, "text": " We actually show you a specific technique that very reliably picks a very good learning rate", "tokens": [492, 767, 855, 291, 257, 2685, 6532, 300, 588, 49927, 16137, 257, 588, 665, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.09761820168330751, "compression_ratio": 1.7544642857142858, "no_speech_prob": 2.6425766463944456e-06}, {"id": 1026, "seek": 507572, "start": 5089.88, "end": 5093.1, "text": " So that's basically what's happening right so we calculate the derivatives", "tokens": [407, 300, 311, 1936, 437, 311, 2737, 558, 370, 321, 8873, 264, 33733], "temperature": 0.0, "avg_logprob": -0.09761820168330751, "compression_ratio": 1.7544642857142858, "no_speech_prob": 2.6425766463944456e-06}, {"id": 1027, "seek": 507572, "start": 5093.1, "end": 5101.04, "text": " And we call the optimizer that does a step in other words update the weights based on the gradients and the learning rate", "tokens": [400, 321, 818, 264, 5028, 6545, 300, 775, 257, 1823, 294, 661, 2283, 5623, 264, 17443, 2361, 322, 264, 2771, 2448, 293, 264, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.09761820168330751, "compression_ratio": 1.7544642857142858, "no_speech_prob": 2.6425766463944456e-06}, {"id": 1028, "seek": 510104, "start": 5101.04, "end": 5106.36, "text": " We should hopefully find that after doing that we have a better loss than we did before", "tokens": [492, 820, 4696, 915, 300, 934, 884, 300, 321, 362, 257, 1101, 4470, 813, 321, 630, 949], "temperature": 0.0, "avg_logprob": -0.3319777679443359, "compression_ratio": 1.6851063829787234, "no_speech_prob": 1.0348500154577778e-06}, {"id": 1029, "seek": 510104, "start": 5106.84, "end": 5111.08, "text": " So I just reran this and got a loss here of four point one six and", "tokens": [407, 286, 445, 43819, 282, 341, 293, 658, 257, 4470, 510, 295, 1451, 935, 472, 2309, 293], "temperature": 0.0, "avg_logprob": -0.3319777679443359, "compression_ratio": 1.6851063829787234, "no_speech_prob": 1.0348500154577778e-06}, {"id": 1030, "seek": 510104, "start": 5111.64, "end": 5115.28, "text": " After one step it's now four point oh three okay", "tokens": [2381, 472, 1823, 309, 311, 586, 1451, 935, 1954, 1045, 1392], "temperature": 0.0, "avg_logprob": -0.3319777679443359, "compression_ratio": 1.6851063829787234, "no_speech_prob": 1.0348500154577778e-06}, {"id": 1031, "seek": 510104, "start": 5115.28, "end": 5121.16, "text": " So it worked the way we hoped it would based on this mini batch it updated all of the weights in our", "tokens": [407, 309, 2732, 264, 636, 321, 19737, 309, 576, 2361, 322, 341, 8382, 15245, 309, 10588, 439, 295, 264, 17443, 294, 527], "temperature": 0.0, "avg_logprob": -0.3319777679443359, "compression_ratio": 1.6851063829787234, "no_speech_prob": 1.0348500154577778e-06}, {"id": 1032, "seek": 510104, "start": 5121.5199999999995, "end": 5126.24, "text": " Network to be a little better than they were as a result of which our loss went down, okay?", "tokens": [12640, 281, 312, 257, 707, 1101, 813, 436, 645, 382, 257, 1874, 295, 597, 527, 4470, 1437, 760, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.3319777679443359, "compression_ratio": 1.6851063829787234, "no_speech_prob": 1.0348500154577778e-06}, {"id": 1033, "seek": 512624, "start": 5126.24, "end": 5130.04, "text": " So let's turn that into a training loop", "tokens": [407, 718, 311, 1261, 300, 666, 257, 3097, 6367], "temperature": 0.0, "avg_logprob": -0.5223429968597693, "compression_ratio": 1.7685950413223142, "no_speech_prob": 1.7330449963992578e-06}, {"id": 1034, "seek": 512624, "start": 5130.04, "end": 5133.0, "text": " All right, we're going to go through a hundred steps", "tokens": [1057, 558, 11, 321, 434, 516, 281, 352, 807, 257, 3262, 4439], "temperature": 0.0, "avg_logprob": -0.5223429968597693, "compression_ratio": 1.7685950413223142, "no_speech_prob": 1.7330449963992578e-06}, {"id": 1035, "seek": 512624, "start": 5133.719999999999, "end": 5136.16, "text": " Grab one more mini batch of data from the data loader", "tokens": [20357, 472, 544, 8382, 15245, 295, 1412, 490, 264, 1412, 3677, 260], "temperature": 0.0, "avg_logprob": -0.5223429968597693, "compression_ratio": 1.7685950413223142, "no_speech_prob": 1.7330449963992578e-06}, {"id": 1036, "seek": 512624, "start": 5137.16, "end": 5139.16, "text": " Calculate our predictions from our network", "tokens": [3511, 2444, 473, 527, 21264, 490, 527, 3209], "temperature": 0.0, "avg_logprob": -0.5223429968597693, "compression_ratio": 1.7685950413223142, "no_speech_prob": 1.7330449963992578e-06}, {"id": 1037, "seek": 512624, "start": 5140.08, "end": 5143.0, "text": " Calculate our loss from the predictions and the actuals", "tokens": [3511, 2444, 473, 527, 4470, 490, 264, 21264, 293, 264, 3539, 82], "temperature": 0.0, "avg_logprob": -0.5223429968597693, "compression_ratio": 1.7685950413223142, "no_speech_prob": 1.7330449963992578e-06}, {"id": 1038, "seek": 512624, "start": 5144.28, "end": 5149.32, "text": " Every ten goes will print out the accuracy just take the mean of the whether they're equal or not", "tokens": [2048, 2064, 1709, 486, 4482, 484, 264, 14170, 445, 747, 264, 914, 295, 264, 1968, 436, 434, 2681, 420, 406], "temperature": 0.0, "avg_logprob": -0.5223429968597693, "compression_ratio": 1.7685950413223142, "no_speech_prob": 1.7330449963992578e-06}, {"id": 1039, "seek": 514932, "start": 5149.32, "end": 5154.92, "text": " One pie torch specific thing you have to zero the gradients basically you can have networks where like", "tokens": [1485, 1730, 27822, 2685, 551, 291, 362, 281, 4018, 264, 2771, 2448, 1936, 291, 393, 362, 9590, 689, 411], "temperature": 0.0, "avg_logprob": -0.30025329956641567, "compression_ratio": 1.8818565400843883, "no_speech_prob": 8.990906508188345e-07}, {"id": 1040, "seek": 514932, "start": 5154.92, "end": 5159.36, "text": " You've got lots of different loss functions that you might want to add all of the gradients together", "tokens": [509, 600, 658, 3195, 295, 819, 4470, 6828, 300, 291, 1062, 528, 281, 909, 439, 295, 264, 2771, 2448, 1214], "temperature": 0.0, "avg_logprob": -0.30025329956641567, "compression_ratio": 1.8818565400843883, "no_speech_prob": 8.990906508188345e-07}, {"id": 1041, "seek": 514932, "start": 5159.679999999999, "end": 5166.759999999999, "text": " Right so you have to tell pie torch like when to set the gradients back to zero right so this just says set all the gradients to", "tokens": [1779, 370, 291, 362, 281, 980, 1730, 27822, 411, 562, 281, 992, 264, 2771, 2448, 646, 281, 4018, 558, 370, 341, 445, 1619, 992, 439, 264, 2771, 2448, 281], "temperature": 0.0, "avg_logprob": -0.30025329956641567, "compression_ratio": 1.8818565400843883, "no_speech_prob": 8.990906508188345e-07}, {"id": 1042, "seek": 514932, "start": 5166.759999999999, "end": 5168.28, "text": " zero", "tokens": [4018], "temperature": 0.0, "avg_logprob": -0.30025329956641567, "compression_ratio": 1.8818565400843883, "no_speech_prob": 8.990906508188345e-07}, {"id": 1043, "seek": 514932, "start": 5168.28, "end": 5174.92, "text": " Calculate the gradients that's caught backward and then take one step of the optimizer so update the weights", "tokens": [3511, 2444, 473, 264, 2771, 2448, 300, 311, 5415, 23897, 293, 550, 747, 472, 1823, 295, 264, 5028, 6545, 370, 5623, 264, 17443], "temperature": 0.0, "avg_logprob": -0.30025329956641567, "compression_ratio": 1.8818565400843883, "no_speech_prob": 8.990906508188345e-07}, {"id": 1044, "seek": 517492, "start": 5174.92, "end": 5180.64, "text": " using the gradients and the learning rate and so once we run it you can see the loss goes down and", "tokens": [1228, 264, 2771, 2448, 293, 264, 2539, 3314, 293, 370, 1564, 321, 1190, 309, 291, 393, 536, 264, 4470, 1709, 760, 293], "temperature": 0.0, "avg_logprob": -0.6308711163409344, "compression_ratio": 1.5664739884393064, "no_speech_prob": 1.6797259831946576e-06}, {"id": 1045, "seek": 517492, "start": 5181.64, "end": 5183.64, "text": " The accuracy goes up", "tokens": [440, 14170, 1709, 493], "temperature": 0.0, "avg_logprob": -0.6308711163409344, "compression_ratio": 1.5664739884393064, "no_speech_prob": 1.6797259831946576e-06}, {"id": 1046, "seek": 517492, "start": 5185.08, "end": 5187.08, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.6308711163409344, "compression_ratio": 1.5664739884393064, "no_speech_prob": 1.6797259831946576e-06}, {"id": 1047, "seek": 517492, "start": 5187.08, "end": 5189.08, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.6308711163409344, "compression_ratio": 1.5664739884393064, "no_speech_prob": 1.6797259831946576e-06}, {"id": 1048, "seek": 517492, "start": 5189.08, "end": 5194.2, "text": " That's the basic approach and so next lesson. We'll see", "tokens": [663, 311, 264, 3875, 3109, 293, 370, 958, 6898, 13, 492, 603, 536], "temperature": 0.0, "avg_logprob": -0.6308711163409344, "compression_ratio": 1.5664739884393064, "no_speech_prob": 1.6797259831946576e-06}, {"id": 1049, "seek": 517492, "start": 5195.56, "end": 5197.56, "text": " What that does all right?", "tokens": [708, 300, 775, 439, 558, 30], "temperature": 0.0, "avg_logprob": -0.6308711163409344, "compression_ratio": 1.5664739884393064, "no_speech_prob": 1.6797259831946576e-06}, {"id": 1050, "seek": 517492, "start": 5198.12, "end": 5201.4, "text": " We're looking in detail when we're looking at the data loader", "tokens": [492, 434, 1237, 294, 2607, 562, 321, 434, 1237, 412, 264, 1412, 3677, 260], "temperature": 0.0, "avg_logprob": -0.6308711163409344, "compression_ratio": 1.5664739884393064, "no_speech_prob": 1.6797259831946576e-06}, {"id": 1051, "seek": 520140, "start": 5201.4, "end": 5208.36, "text": " All right, we're looking in detail. We're not going to look inside here as I say we're going to basically take the", "tokens": [1057, 558, 11, 321, 434, 1237, 294, 2607, 13, 492, 434, 406, 516, 281, 574, 1854, 510, 382, 286, 584, 321, 434, 516, 281, 1936, 747, 264], "temperature": 0.0, "avg_logprob": -0.23041479928152903, "compression_ratio": 1.6805555555555556, "no_speech_prob": 2.9480031571438303e-06}, {"id": 1052, "seek": 520140, "start": 5208.719999999999, "end": 5210.719999999999, "text": " calculation of the derivatives as", "tokens": [17108, 295, 264, 33733, 382], "temperature": 0.0, "avg_logprob": -0.23041479928152903, "compression_ratio": 1.6805555555555556, "no_speech_prob": 2.9480031571438303e-06}, {"id": 1053, "seek": 520140, "start": 5211.28, "end": 5213.28, "text": " As a given right but basically", "tokens": [1018, 257, 2212, 558, 457, 1936], "temperature": 0.0, "avg_logprob": -0.23041479928152903, "compression_ratio": 1.6805555555555556, "no_speech_prob": 2.9480031571438303e-06}, {"id": 1054, "seek": 520140, "start": 5214.28, "end": 5216.28, "text": " What's happening there?", "tokens": [708, 311, 2737, 456, 30], "temperature": 0.0, "avg_logprob": -0.23041479928152903, "compression_ratio": 1.6805555555555556, "no_speech_prob": 2.9480031571438303e-06}, {"id": 1055, "seek": 520140, "start": 5217.96, "end": 5221.12, "text": " And any kind of deep network you have kind of like a function", "tokens": [400, 604, 733, 295, 2452, 3209, 291, 362, 733, 295, 411, 257, 2445], "temperature": 0.0, "avg_logprob": -0.23041479928152903, "compression_ratio": 1.6805555555555556, "no_speech_prob": 2.9480031571438303e-06}, {"id": 1056, "seek": 520140, "start": 5221.28, "end": 5223.48, "text": " That's like you know a linear function", "tokens": [663, 311, 411, 291, 458, 257, 8213, 2445], "temperature": 0.0, "avg_logprob": -0.23041479928152903, "compression_ratio": 1.6805555555555556, "no_speech_prob": 2.9480031571438303e-06}, {"id": 1057, "seek": 520140, "start": 5223.48, "end": 5227.36, "text": " And then you pass the output of that into another function", "tokens": [400, 550, 291, 1320, 264, 5598, 295, 300, 666, 1071, 2445], "temperature": 0.0, "avg_logprob": -0.23041479928152903, "compression_ratio": 1.6805555555555556, "no_speech_prob": 2.9480031571438303e-06}, {"id": 1058, "seek": 522736, "start": 5227.36, "end": 5231.839999999999, "text": " That might be like a relu and you pass the output of that into another function", "tokens": [663, 1062, 312, 411, 257, 1039, 84, 293, 291, 1320, 264, 5598, 295, 300, 666, 1071, 2445], "temperature": 0.0, "avg_logprob": -0.18223214400442023, "compression_ratio": 1.9627906976744185, "no_speech_prob": 4.936959612678038e-06}, {"id": 1059, "seek": 522736, "start": 5232.04, "end": 5236.28, "text": " That might be another linear net linear layer you pass that into another function", "tokens": [663, 1062, 312, 1071, 8213, 2533, 8213, 4583, 291, 1320, 300, 666, 1071, 2445], "temperature": 0.0, "avg_logprob": -0.18223214400442023, "compression_ratio": 1.9627906976744185, "no_speech_prob": 4.936959612678038e-06}, {"id": 1060, "seek": 522736, "start": 5236.679999999999, "end": 5242.32, "text": " That might be another relu and so forth right so these these deep networks are just", "tokens": [663, 1062, 312, 1071, 1039, 84, 293, 370, 5220, 558, 370, 613, 613, 2452, 9590, 366, 445], "temperature": 0.0, "avg_logprob": -0.18223214400442023, "compression_ratio": 1.9627906976744185, "no_speech_prob": 4.936959612678038e-06}, {"id": 1061, "seek": 522736, "start": 5242.88, "end": 5247.96, "text": " Functions of functions of functions, so you could write them mathematically like that right and so", "tokens": [11166, 3916, 295, 6828, 295, 6828, 11, 370, 291, 727, 2464, 552, 44003, 411, 300, 558, 293, 370], "temperature": 0.0, "avg_logprob": -0.18223214400442023, "compression_ratio": 1.9627906976744185, "no_speech_prob": 4.936959612678038e-06}, {"id": 1062, "seek": 524796, "start": 5247.96, "end": 5254.8, "text": " All backprop does is it says let's just simplify this down to the two version is", "tokens": [1057, 646, 79, 1513, 775, 307, 309, 1619, 718, 311, 445, 20460, 341, 760, 281, 264, 732, 3037, 307], "temperature": 0.0, "avg_logprob": -0.27774754692526427, "compression_ratio": 1.4625, "no_speech_prob": 1.2098623756173765e-06}, {"id": 1063, "seek": 524796, "start": 5258.4, "end": 5260.92, "text": " We can say okay you equals f of x", "tokens": [492, 393, 584, 1392, 291, 6915, 283, 295, 2031], "temperature": 0.0, "avg_logprob": -0.27774754692526427, "compression_ratio": 1.4625, "no_speech_prob": 1.2098623756173765e-06}, {"id": 1064, "seek": 524796, "start": 5262.8, "end": 5270.2, "text": " Right and so therefore the derivative of g of f of x is we can calculate with the chain rule as being", "tokens": [1779, 293, 370, 4412, 264, 13760, 295, 290, 295, 283, 295, 2031, 307, 321, 393, 8873, 365, 264, 5021, 4978, 382, 885], "temperature": 0.0, "avg_logprob": -0.27774754692526427, "compression_ratio": 1.4625, "no_speech_prob": 1.2098623756173765e-06}, {"id": 1065, "seek": 524796, "start": 5271.44, "end": 5273.44, "text": " g dash u", "tokens": [290, 8240, 344], "temperature": 0.0, "avg_logprob": -0.27774754692526427, "compression_ratio": 1.4625, "no_speech_prob": 1.2098623756173765e-06}, {"id": 1066, "seek": 524796, "start": 5274.16, "end": 5276.16, "text": " F dash x", "tokens": [479, 8240, 2031], "temperature": 0.0, "avg_logprob": -0.27774754692526427, "compression_ratio": 1.4625, "no_speech_prob": 1.2098623756173765e-06}, {"id": 1067, "seek": 527616, "start": 5276.16, "end": 5282.92, "text": " Right and so you can see we can do the same thing for the functions of the functions of the functions and so when you apply a", "tokens": [1779, 293, 370, 291, 393, 536, 321, 393, 360, 264, 912, 551, 337, 264, 6828, 295, 264, 6828, 295, 264, 6828, 293, 370, 562, 291, 3079, 257], "temperature": 0.0, "avg_logprob": -0.20475156098893546, "compression_ratio": 2.127853881278539, "no_speech_prob": 2.3320651507674484e-06}, {"id": 1068, "seek": 527616, "start": 5283.88, "end": 5289.92, "text": " Function to a function of a function you can take the derivative just by taking the product of the derivatives of each of those", "tokens": [11166, 882, 281, 257, 2445, 295, 257, 2445, 291, 393, 747, 264, 13760, 445, 538, 1940, 264, 1674, 295, 264, 33733, 295, 1184, 295, 729], "temperature": 0.0, "avg_logprob": -0.20475156098893546, "compression_ratio": 2.127853881278539, "no_speech_prob": 2.3320651507674484e-06}, {"id": 1069, "seek": 527616, "start": 5290.16, "end": 5295.08, "text": " layers okay, and in neural networks we call this back propagation", "tokens": [7914, 1392, 11, 293, 294, 18161, 9590, 321, 818, 341, 646, 38377], "temperature": 0.0, "avg_logprob": -0.20475156098893546, "compression_ratio": 2.127853881278539, "no_speech_prob": 2.3320651507674484e-06}, {"id": 1070, "seek": 527616, "start": 5295.72, "end": 5300.38, "text": " Okay, so when you hear back propagation it just means use the chain rule to calculate the derivatives", "tokens": [1033, 11, 370, 562, 291, 1568, 646, 38377, 309, 445, 1355, 764, 264, 5021, 4978, 281, 8873, 264, 33733], "temperature": 0.0, "avg_logprob": -0.20475156098893546, "compression_ratio": 2.127853881278539, "no_speech_prob": 2.3320651507674484e-06}, {"id": 1071, "seek": 530038, "start": 5300.38, "end": 5305.5, "text": " And so when you see a neural network defined", "tokens": [400, 370, 562, 291, 536, 257, 18161, 3209, 7642], "temperature": 0.0, "avg_logprob": -0.17604723572731018, "compression_ratio": 2.0, "no_speech_prob": 8.059433866947074e-07}, {"id": 1072, "seek": 530038, "start": 5309.58, "end": 5311.58, "text": " Like here right", "tokens": [1743, 510, 558], "temperature": 0.0, "avg_logprob": -0.17604723572731018, "compression_ratio": 2.0, "no_speech_prob": 8.059433866947074e-07}, {"id": 1073, "seek": 530038, "start": 5313.46, "end": 5317.5, "text": " Like if it's defined sequentially literally all this means is", "tokens": [1743, 498, 309, 311, 7642, 5123, 3137, 3736, 439, 341, 1355, 307], "temperature": 0.0, "avg_logprob": -0.17604723572731018, "compression_ratio": 2.0, "no_speech_prob": 8.059433866947074e-07}, {"id": 1074, "seek": 530038, "start": 5318.86, "end": 5320.86, "text": " Apply this function to the input", "tokens": [25264, 341, 2445, 281, 264, 4846], "temperature": 0.0, "avg_logprob": -0.17604723572731018, "compression_ratio": 2.0, "no_speech_prob": 8.059433866947074e-07}, {"id": 1075, "seek": 530038, "start": 5322.02, "end": 5329.02, "text": " Apply this function to that apply this function to that apply this function to that right so this is just defining a", "tokens": [25264, 341, 2445, 281, 300, 3079, 341, 2445, 281, 300, 3079, 341, 2445, 281, 300, 558, 370, 341, 307, 445, 17827, 257], "temperature": 0.0, "avg_logprob": -0.17604723572731018, "compression_ratio": 2.0, "no_speech_prob": 8.059433866947074e-07}, {"id": 1076, "seek": 532902, "start": 5329.02, "end": 5332.740000000001, "text": " composition of a function to a function to a function to a function", "tokens": [12686, 295, 257, 2445, 281, 257, 2445, 281, 257, 2445, 281, 257, 2445], "temperature": 0.0, "avg_logprob": -0.1536829281697231, "compression_ratio": 1.8612244897959183, "no_speech_prob": 1.0188061878579902e-06}, {"id": 1077, "seek": 532902, "start": 5334.02, "end": 5336.02, "text": " Okay, and so", "tokens": [1033, 11, 293, 370], "temperature": 0.0, "avg_logprob": -0.1536829281697231, "compression_ratio": 1.8612244897959183, "no_speech_prob": 1.0188061878579902e-06}, {"id": 1078, "seek": 532902, "start": 5336.660000000001, "end": 5342.34, "text": " Yeah, so although. We're not going to bother with calculating the gradients ourselves. You can now see why it can do it right as long", "tokens": [865, 11, 370, 4878, 13, 492, 434, 406, 516, 281, 8677, 365, 28258, 264, 2771, 2448, 4175, 13, 509, 393, 586, 536, 983, 309, 393, 360, 309, 558, 382, 938], "temperature": 0.0, "avg_logprob": -0.1536829281697231, "compression_ratio": 1.8612244897959183, "no_speech_prob": 1.0188061878579902e-06}, {"id": 1079, "seek": 532902, "start": 5342.34, "end": 5344.18, "text": " As it has internally", "tokens": [1018, 309, 575, 19501], "temperature": 0.0, "avg_logprob": -0.1536829281697231, "compression_ratio": 1.8612244897959183, "no_speech_prob": 1.0188061878579902e-06}, {"id": 1080, "seek": 532902, "start": 5344.18, "end": 5350.46, "text": " You know a it knows like what's the what's the derivative of to the power of what's the derivative of sine?", "tokens": [509, 458, 257, 309, 3255, 411, 437, 311, 264, 437, 311, 264, 13760, 295, 281, 264, 1347, 295, 437, 311, 264, 13760, 295, 18609, 30], "temperature": 0.0, "avg_logprob": -0.1536829281697231, "compression_ratio": 1.8612244897959183, "no_speech_prob": 1.0188061878579902e-06}, {"id": 1081, "seek": 532902, "start": 5350.46, "end": 5354.38, "text": " What's the derivative of plus and so forth then our Python code in?", "tokens": [708, 311, 264, 13760, 295, 1804, 293, 370, 5220, 550, 527, 15329, 3089, 294, 30], "temperature": 0.0, "avg_logprob": -0.1536829281697231, "compression_ratio": 1.8612244897959183, "no_speech_prob": 1.0188061878579902e-06}, {"id": 1082, "seek": 535438, "start": 5354.38, "end": 5357.88, "text": " In here. It's just combining those things together", "tokens": [682, 510, 13, 467, 311, 445, 21928, 729, 721, 1214], "temperature": 0.0, "avg_logprob": -0.20865614349777634, "compression_ratio": 1.5284090909090908, "no_speech_prob": 1.8162155583922868e-06}, {"id": 1083, "seek": 535438, "start": 5358.9800000000005, "end": 5366.18, "text": " So it just needs to know how to compose them together with the chain rule and away it goes okay?", "tokens": [407, 309, 445, 2203, 281, 458, 577, 281, 35925, 552, 1214, 365, 264, 5021, 4978, 293, 1314, 309, 1709, 1392, 30], "temperature": 0.0, "avg_logprob": -0.20865614349777634, "compression_ratio": 1.5284090909090908, "no_speech_prob": 1.8162155583922868e-06}, {"id": 1084, "seek": 535438, "start": 5373.3, "end": 5378.28, "text": " Okay, so I think we can leave it there for now and yeah and in the next class", "tokens": [1033, 11, 370, 286, 519, 321, 393, 1856, 309, 456, 337, 586, 293, 1338, 293, 294, 264, 958, 1508], "temperature": 0.0, "avg_logprob": -0.20865614349777634, "compression_ratio": 1.5284090909090908, "no_speech_prob": 1.8162155583922868e-06}, {"id": 1085, "seek": 535438, "start": 5378.3, "end": 5380.3, "text": " We'll go and we'll see how to", "tokens": [492, 603, 352, 293, 321, 603, 536, 577, 281], "temperature": 0.0, "avg_logprob": -0.20865614349777634, "compression_ratio": 1.5284090909090908, "no_speech_prob": 1.8162155583922868e-06}, {"id": 1086, "seek": 535438, "start": 5380.78, "end": 5382.26, "text": " write our own", "tokens": [2464, 527, 1065], "temperature": 0.0, "avg_logprob": -0.20865614349777634, "compression_ratio": 1.5284090909090908, "no_speech_prob": 1.8162155583922868e-06}, {"id": 1087, "seek": 538226, "start": 5382.26, "end": 5387.74, "text": " Optimizer and then we'll have solved M list from scratch ourselves. See you then", "tokens": [50364, 35013, 6545, 293, 550, 321, 603, 362, 13041, 376, 1329, 490, 8459, 4175, 13, 3008, 291, 550, 50638], "temperature": 0.0, "avg_logprob": -0.2528790712356567, "compression_ratio": 1.0, "no_speech_prob": 1.8053962776320986e-05}], "language": "en"}