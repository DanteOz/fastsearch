{"text": " Okay, there we go. Hi, everybody. Can you see me and hear me okay? Great. Hi. Hope you're ready for tabular. Oh, and we have Andrew Schor here today as well. Andrew just joined Whamory Medical Research Institute as research director. And he is the person you may be familiar with from Music Autobot, which if you haven't checked it out, you should because it's super-sigical. But now he's moving from the Jackson 5 to medical AI research. So tabular is a cool notebook, I think. It's a bit of fun. And the basic idea, well, let's start at the end to see how it's going to work. So this is we're going to look at the adult data set, which is the one that we use in most of the docs in version one and we used in some of the lessons, I think. It's a pretty simple small data set. It's got 32,000 rows in it and 15 columns in it. And here's what it looks like when we grab it straight from data. So basically, in order to create models from this, we need to take the categorical variables and convert them into ints, possibly with some missing category. The continuous variables, if there's anything with a missing value in a continuous variable, we need to replace it with something. So normally we replace it with something like the median. And then normally we add an additional column for each thing that has a missing value, for each column that has a missing value, and that column will be binary, which is is it missing or not. So we need to know which things are going to be the categorical variables, which column names are going to be the continuous variables. So we know how to process each one. We're going to need to know how to split our validation and training set. And we need to know what's our dependent variable. So we've created a class called tabular. Basically tabular contains a data frame and it also contains a list of which things are categorical, which things are continuous, and what's your dependent variable. And also some processes, which we'll look at in a moment, but they do things like turning strings into ints for categories and filling the missing data and doing normalization of continuous. So that creates a tabular object. And from a tabular object, you can get a data source if you pass in a list of split indexes. So feel free to ask also Andrew if you have any questions as we go. Oh, what's that David? You want a jagged competition? What's a jagged competition? Tell us more. Oh, Kaggle. Which one? So now that we've got a data source, we created a data loader from it. And so we have a little wrapper for that to make it easier for tabular. And so then we can go show batch. Oh, which is now broken somehow. Nice one Jeremy. Damn it. This was all working a moment ago. And then Andrew and I were just changing things at the last minute. So what did we break? Okay, I am going to ignore that. All right. So we just broke something apparently before we started recording. But anyway, show batch would then show the data. And then you can take a test set that's not processed. And basically what you want to be able to do with a test set is say I have the same categorical names and continuous names and the same dependent variable and the same processes. And you should then be able to apply the same pre-processing to the test set. And so to.new creates a new tabular object with all that metadata. And then process will do the same processing. And so then you can see here is the normalized age, normalized education. All these things have been turned into ints. Oh, so far. Excuse me. So that's basically what this looks like. You'll see I've used a subclass of tabular called tabular pandas. I don't know if it's always going to stay this way or not. But currently, well, we want to be able to support multiple different backends. So currently we have a pandas backend for tabular. And we're also working on a rapids backend. For those of you that haven't seen it, rapids is a really great project coming out of Nvidia, which allows you to do so they've got something called a CUDIF, which basically gives you which gives you GPU accelerated data frames. So that's why we have different subclasses. I don't know if this will always be a subclass of tabular. We may end up instead of having subclasses of data source. But for now, we've got subclasses of tabular. So that's where we're heading. All right. Do you have any advice to speed up inference on the tabular amount of predictions? Well, hopefully there'll be fast, particularly if you use rapids. I mean, yeah, basically if you use rapids. In fact, if you look at rapids, Nvidia, fast AI, hopefully. Yeah, here we go. So even a lot of people remember from the forums, he's at Nvidia now, which is pretty awesome. And he he's working on the rapids team. And he recently posted how he got a 15x acceleration and got in the top 20 of a competition, a very popular competition on Kaggle by combining rapids by torching fast AI. So that would be a good place to start. But hopefully by the time fast AI version two comes out, this will be super simple. And we're working with even on making that happen. So thank you, even and Nvidia for that help. All right. So let's start at tabular. So basically the idea of tabular is that we. The URL. Oh, yes. I just Googled for Nvidia AI rapids fast AI. And here it is, accelerating deep learning recommendation systems. I'm sure we can somebody will hopefully add it to the notes as well. OK, so the basic idea with tabular was I kind of wanted to I kind of like to have a class which kind of has all the information it needs to do what we want it to do. So in this case, you know, a data frame doesn't have enough information to to actually build models because until you know what the categorical variables are, the continuous variables are and what pre-processing to do and what the dependent variable is, you know, you can't really do much. So that's the basic kind of idea of this design was to have something with that information. So categorical names, continuous names, Y names. Normally there's just one dependent variable, so one Y name, but you could have more and processes. So those are basically the four things that we want to put in our tabular. So there we go. All right. So those are that's why we're passing in those four things. And then the other thing we need to know about Y is is why categorical or continuous? So you just pass that in the symbolium. So a man wants to know why might we need more than one Y name? So few examples, you could be doing a regression problem where you're trying to predict the X and Y coordinates of the destination of a taxi ride or perhaps you're just doing a multi-label classification where you can have multiple things being true would be a couple of examples. And maybe it's already one-hot encoded or something in that case. Okay, so that's the theory behind the design of tabular. So when we initialize it, we just pass in that stuff. The processes that we pass in are just going to be transforms. So we can dump in a pipeline. And so this stuff kind of you'll see that we just keep reusing the same foundational concepts throughout fast.ai version two, which is a good sign that there's strong foundations. So the processes are things that we want to run a bunch of them. We want to depend on what type something is as to whether we run it or not, stuff like that. So it's got all that kind of behavior we want to the pipeline. Unlike tiffmds, tiffmdl, tiffmlist, all of those things apply transformations lazily. On tabular data, we don't generally do that. A number of reasons why. The first is that unlike kind of opening an image or something, it doesn't take a long time to grab a row of data. So like it's fine to read the whole lot of rows normally, except in some cases of really, really big data sets. The second reason is that most kind of tabular stuff is designed to work quickly on lots of rows at a time. So it's going to be much, much faster if you do it ahead of time. The third is that most pre-processing is going to be not data augmentation kind of stuff, but more just once off, cleaning up labels and things like that. So for all these kinds of reasons, our processing in tabular is generally done ahead of time rather than lazily. But it's still a pipeline of transforms. Okay. So then we're going to store something called cat y, which will be our y variable if it's a categorical, otherwise none, and vice versa for convoy. So that's our initializer. And let's take a look at it in use. So basically we create a data frame containing two columns. And from that we can create a tabular object, passing in that data frame and saying cat names, con names, y names, whatever. So in this case we'll just have cat names. One thing that's always a good idea is to make sure that things pickle okay, because the inference and stuff for this metadata has to be pickable. So dumping something to a string and then loading that string up again is always a good idea to make sure it works and make sure it's the same as what you started with. So we're inheriting from call base and call base is just a small little thing in core, which defines the basic things you would expect to have in a collection and it implements them by composition. So you pass in some list or whatever. And so the length of your call base will be the length of that list. So like you can often just inherit from something to do this, but composition is often more interesting, can give you some more flexibility. So this basically gives you the very quick, get item is defined by calling the self.items get item and length is defined by calling the self.items length and so forth. So by inheriting from call base, we can then in the inner simply say super.inert df. And so that means we're going to now going to have something called self.items, which is going to be that data frame. And so here we check that, you know, the pick old version of items is the same as the tabular objects version of items. Okay. So the, let me just press this. Where are we? So the next thing to notice is that there are various useful little attributes like allColumns, allCats for allCategoricalColumns, allConts for allContinuousColumns. And then there's also the same with names at the end, allContNames, allCatNames, allColumnNames. So you can see allContNames is just the continuous names plus the continuous y if there is one. This would not work except for the fact that we use the capital L plus. So if you add a none to a capital L plus, it doesn't change it, which is exactly what you want most of the time. So some of the things in capital L are a bit more convenient than this. I think that's right. Anyway, maybe I should double check before I say things that might not be true. Yeah. I'll do that where else you can do that and get exactly the expected behavior. And Ls, by the way, always show you how big they are before they print out their contents. You'll see that the allCols does not actually appear anywhere here. And that's because we just created a little thing called addProperty that just adds cats or cats, consts or consts or cols. And so for each one, it creates a read version of the property, which is to just grab whatever, cat names or cat names, cont names, et cetera, which are all defined in here, and then indexes into our data frame with that list of columns. And then it creates a setter, which simply sets that list of names to whatever value you provide. So that's just a quick way to create the setter and get a versions of all of those. So that's where allCols come from. So in this case, because the tabular object only has this one column mentioned as being part of what we're modeling, even though the data frame had an A and a B, tabular object at allCols only has the A column in it. Because by allCols, it means all of the columns we're using in modeling, so continuous and categorical and dependent variables. And so one of the nice things is that because everything is super consistent in the API, we can now just say dot show, just like everything else we can say dot show. And in this case, we see the allCols data frame. Okay. So then processes, tabular processes are just transforms. Now, specifically they're in-place transforms, but don't let that bother you because in-place transform is simply a transform where when we recall it and then we return the original thing. So like all the transforms we've seen so far return something different to what you passed in, like that's the whole point of them. But processes, the whole point of them is that they change the actual stored data, so that's why they just return whatever you started with. So that's all in-place transform means. And so a tabular processor is just a transform that returns itself when you recall it. And when you set it up, it just does the normal setup, but it also calls done to call, in other words, self round-accords. Why does it do that? Well, let's take a look at an example, categorify. So categorify is a tabular proc where the setup is going to create a category map. So this is just a mapping from int numbered bits of vocab to string vocab. That's what a category map is. And so it's going to go through all of the categorical columns and it's going to go.iloc into the data frame for each of those columns. And it's going to create a category map for that column. So this is just creating, so self.classes then is going to be a dictionary that goes from the column names to the vocab for that categorical column. So that's what setup does. So setup kind of sets up the metadata, it's the vocab. And codes, on the other hand, is the thing that actually takes a categorical column and converts it into ints using the vocab that we created earlier. And so we need them to be two separate things because if you think about inference, at inference time you don't want to run setup. At inference time you just want to run encodes. But at training time you want to do both. Anytime you don't do setup, you're definitely also going to want to process. So that's why setup, after it sets up, immediately does the encoding because that's in practice always what you want. So that's why we override setup in tabular procs. That's all tabular proc is. It's just a transform that when you set it up it also calls it straight away. So that, so categorify is pretty similar to the categorized transform we've seen for dependent variables for image classification, but it's a little bit different because it's for tabular objects. So you can see an example. Here's a data frame, R1, R2, R2, and just a single column. So again we can create a tabular object, passing in the data frame, passing in any processes we want to run, and passing in, so the first thing that we pass in after that will be the category names, the categorical names, so we're just going to have one. So once you have created your tabular object, the next thing you want to do is to call setup. And remember that setup is going to do two things. It's going to call your setups and it's going to call your encodes. Are there any object detectors? We haven't done any models yet, David. We're only really working through the transformation pipeline, but we have certainly looked at a version two object detectors in previous lessons for like, well, not in detail. We've touched on the places where the object detection data is defined and yeah, hopefully it's clear enough that you can figure that out. So patch property does not call add prop. I mean, you can check the code easily enough yourself, right? So patch property. Let's see. Okay. So no, it doesn't. It calls patch two. No, it doesn't. So there's no way to create a setup using that decorator yet. If you can think of a good syntax for that that you think would work well, feel free to suggest it or even implement it in a PR. Okay. So all right, so we've created our tabular object. As we told it to categorify, we said this is our only categorical variable. Then we call setup, which is going to go ahead and let's have a look. Setup is going to in our processor create classes and then it's going to change our columns to call apply cats, which will map the data in that column using this dictionary. So map is a interesting pandas method. You can pass it a function, which is going to be super slow. So don't do that, but you can also pass it a dictionary and that will just map from keys to values in the dictionary. So if we have a look at this case, we can see that two dot a starts out as 01202 and ends up as 12313. And the reason for that is that the vocab that it created is hash NA 012. So whenever we create a categorified column for the vocab, we always put a NA at the start, which is similar to what we've done in version one. So that way, if you in the future get a value outside of your vocab, then that's what we're going to set it to NA. And so therefore one, if we index that into the vocab, that maps to zero. So that's why this is zero became one example. One of the things that I recently added to pipeline, because remember that two dot prox is a pipeline. And one of the things I added to pipeline is a getAtra, which will try to, so this is not an attribute in pipeline, not surprisingly. So if it finds an attribute it doesn't understand, it will try and find that attribute in any transforms inside that pipeline, which is exactly what we want. So in this case, it's going to look for a transform with a type categorified and it converts it to snake case. This is very similar if you've watched the part two, the most recent part two videos, we did the same thing for callbacks. Callbacks got automatically inserted. And I think version one does this too, get automatically added as attributes. So pipeline does something very similar, but it doesn't actually add them as attributes. It uses getAtra to do the same thing. So in this case, we added a categorify transform. So we haven't instantiated it. We just passed in the type. So it's going to instantiate it for us. And it will always instantiate your types for you if you don't instantiate them. And so later on we want to say, okay, let's find out what the vocab was, which means we need to grab the processes out of our transform object and ask for the categorify transform. So now that we've got that categorified transform, categorify defines getItem and it will return the vocab for that column. So here is the vocab for that column. So we can have a look at a, as you can see, there it is. And that is actually type category map. So as well as having the items that we just saw, it also has the reverse mapping. So this is, as you can see, goes the opposite direction. So to answer Amman's question, this, yes, this should take care of the mapping in the test set. Because if it comes across, so it's going to look at O2I when it tries to call applyCats. It's going to try to find in your categories, it's going to try and find that column. It's going to grab O2I, which is this dictionary. That's then going to use that to map everything in the column. But because it's a default dict, if there's anything it doesn't recognize, it will become zero, which is the NA category. So yeah, that should all work nicely. You have to figure out how to model it, of course, but the data processing will handle it for you. Okay. So what else? So now, imagine it's inference time. And so we come along with some new data frame that we want to run inference on. Oh, it's a test set or whatever. So here's our data frame. So we now have to say, okay, I want to create a new tabular object with the same metadata that we had before. So the same processes, same vocab, the same categorical continuous variables. The way to do that is to start with an existing tabular object, which has that metadata and call new and pass in a new data frame that you have. And that's going to give you a new tabular object with the same metadata and processes and stuff that we had before, but with this different data in it. Now, of course, we don't want to call setup on that because setup would replace the vocab. And the whole point is that we want to actually use the same vocab for inference on this new data set. So instead you call process. And so all process does in tabular is it just calls the processes, which is a pipeline. So you can just treat it as a function. So in this case, our vocab was 0, 1, 2, and an a, right? And here there's a couple of things that are not in that list of 0, 1, 2, specifically this one is 3 and this one is minus 1. So 1, 0, and 2 will get replaced by their 1, 0, and 2. So 2, 1, 3, 2, 1, 3. So there's 2, there's our new a column. And then as we just discussed, the two things that don't map are going to be 0. So then if you call decode on our processor, then as you would expect, you end up with the same data you started with. But of course, this is now going to be an a because we said we don't know what those are. So like decoding in general in fast AI doesn't mean you always get back exactly what you started with, right? It's kind of trying to display the kind of transformed version of the data. In some cases like normalization, it should pretty much be exactly what you started with. Some things like categorify the missing values, it won't be exactly what you started with. You don't have to pass in just a type name. You can instantiate a processor yourself and then pass that in. So then that means you don't have to dig it out again like this. So sometimes that's more convenient. So this is just another way of doing the same thing. But in this case, we're also going to split the training set and the validation set. And this is particularly important for things like categorify because if our training set is the first three elements and our validation set is the last two, then this thing here three is not in the training set. And so therefore it should not be part of the vocab. So let's make sure that that's the case. So here we are, categorical variable. Yep, it doesn't have three in it. The vocab doesn't have three in it. So the way we pass in these split indexes is by calling tabular object dot data source and that converts the tabular object to a data source. The only thing you pass it is the list of splits. And so that gives you a standard data source object, just like the one that we saw in our last walkthrough. So that's what you get. And so that data source, let's take it out, right? So that data source object will have a train, for example, and a valid. And those are just the ways of saying subset 0 and subset 1. Should test set be 3, 2? No, these are the indexes of what's the indexes of things in the training set. These are the indexes in the validation set. So these are indexes 3 and 4 in the validation set. Yes, they are ID indexes. So data source, so in terms of looking at the code of tabular, it's super tiny, which is nice, in terms of the things that are more than one line in it, because there's a bunch of things to store, and data source. And the only reason data source is more than a couple of lines is because in Rappers, on the GPU, trying to index into a data frame with arbitrary indexes is really, really, really, really slow. So you have to pass in a contiguous list of indexes to make Rappers fast. So what we do is when you pass in splits, we actually concatenate all of those splits together into a single list, and we index into the data frame with that list. So that's going to shuffle the list so that all the stuff that's in the same validation or training set is all together. And so that way, now when we create our data source, rather than passing in the actual splits, we just pass in a range of all the numbers from 0 to the length of the first split, and then all of the numbers from the length of the first split to the length of the whole thing. And so our data source is then able to always use contiguous indexes. So that's why that bit of code is there. Other than that, one thing I don't like about Python is that anytime you want to create a property, you have to put it on another row like this. Like in a lot of programming languages, you can kind of do that kind of thing on the same line, but not in Python for some reason. And so I find it kind of takes up a lot of room just for the processor saying these are properties. I added an alternative syntax, which is to create a list of all the things that are properties. So that's all that is. Like most of these things, it's super tiny. So it's just one line of code. It just goes through and calls property on them to make them properties. Oh, okay. So then another thing about this is I kind of tried to make tabular look a lot like data frame. And one way that happens is we've inherited from getAtra, which means that any unknown attributes it's going to pass down to whatever is the default property, which is self.items, which is a data frame. So in other words, it behaves a lot like a data frame because anything unknown, it will actually pass it along to the data frame. But one thing I did want to change is in data frames, it's not convenient to index into a row by row number and a column by name. You can use iloc to get row by number, column by number. You can do loc to say row by name or index and column by name or index. But most of the time I want to use row numbers and column names. So we redefine iloc to use this tabular iloc indexer, which is here. And as you can see, if you have a row and a column, then the columns I actually replace with the integer index of the column, so that way we can use column names and row numbers. And also it will wrap it back up in a tabular object as well. So we end up with if you index into a tabular object with iloc, you'll get back a tabular object. So then the way Categorify is implemented, as you see in encodes, is it calls transform, passing in a bunch of column names and a function. The function applyCats is the thing we saw before, which is the thing that calls map, unless you have a pandas categorical column, in which case you actually already have the pandas has done the coding for you, so you just return it, just cat.codes plus one. And so how does this function get applied to each of these columns? That's because we have a thing called tabulaObject.transform, and that's the thing that at the moment is defined explicitly for pandas. And as you can see, for pandas it just is this column equals the transformed version of this column, because pandas has a.transform method for a series. So that's all we needed to do there. Okay. So Juvian, your question there about numerical versus continuous. You should watch the introduction to machine learning for coders course where we talk about that in a lot of detail. That's something you probably have time to cover here. All right. So that's categorify. So here's the other way to use categorify, as I was kind of beginning to mention, is you can actually create a categorical column in pandas. And one of the reasons to do that is so that you can say, these are the categories I want to use and they have an order. And so that way high, medium, low will now be ordered correctly, which is super useful. Also pandas is just nice and efficient at dealing with categories. So now if we go categorify just like before, it's going to give us exactly the same kind of results, except that when we look at the categorical processor, these will be in the right order. So we're going to end up with things that have been mapped in that way. And it also be done potentially more efficiently because it's using the internal pandas catcode stuff. Thank you, David. That is very kind. I'm so thrilled. I would love to know where you are working as a computer vision data scientist. It's a very cool job. Andrew was starting his job as the in-house research director and data scientist here in 10 days time. And also, two years ago. And working very, very, very, very hard on being amazing, which also helps. Lots of people do the course and don't end up with great jobs because they don't work as hard. Although I think everybody listening to this by definition is going to an extra level of effort. So I'm sure you will all do great. Normalize is just something where we're going to subtract the means and divide by the standard deviations. And the decodes, we're going to do the opposite. And you'll see what we generally do in these setup things. We do this in lots of places. As we say, get atra, data source, comma, train, comma, data source. What this means is it's the same as if we'd written this. If dsrc.train if has atra dsrc, train, else dsrc. It's the same as writing that. And so the reason we're doing that is because we want you to be able to either pass in an actual data source object, which has a train and a valid, or not. If you aren't doing separate things, a train and valid, then that should be fine as well. And so as long as the thing you pass in, if it does have a train, then it should give you back some kind of object that has the right methods that you need. So in the case of data source and tabular, it will. Data source has a train attribute that will return a tabular object. Or if you just pass in a tabular object directly, then it won't have a train. So dsrc will just be a tabular object, which has a continuous variables, const attribute. So now we have this data frame containing just the continuous variables and optionally just for our training set, if we have one. So then we can set up our means and standard deviations. That's the metadata we need. So here we can create our normalize, create a data frame to test it out on, pass in that processor. This time we're just going to say these are continuous variables. This is one which is a, make sure we call setup. And so now we should find here is the same data that we had before, but let's make it into an array for testing and calculate the standard deviation. So we should find if we go norm dot means a. So self dot means was df dot mean. This is quite nice, right? So in pandas, if you call dot mean on a data frame, you will get back a, I think it's a series object, which you can index into with column names. So this is quite neat, right? That we were able to get all the means and standard deviations all at once for all the columns and even apply them to all the columns at once. This is kind of the magic of Python indexes. So I think that's, that's actually pretty nice. So yeah, make sure that the mean is m, standard deviation should be around s and the values after processing should be around x minus m over s. One thing to notice is that this here, setup, we didn't call here. Why didn't we call setup? And the reason why is that if you look at data source, it calls setup. Why? Because we now definitely have all the information we need to set it up, right? We know the data because that was a data frame you passed in and now we know what the training set is because that was passed in. So there's no reason, there's no reason to ask you to manually call setup. So you've got two ways to set up your processes. One is to call setup on your tabular object or the other is just to create a data source. And it's kind of, it's something you kind of have to be aware of, right? Because calling data source is not just returning a data source. It is also modifying your tabular object data to process it. And so it's kind of like, it's a very non-pure, non-functional kind of approach coming on here. We're not changing things and returning them. We're like changing things in place. The reason for that is that with tabular data, you don't want to be creating lots of copies of it. You really want to be doing stuff in place. It's got a lot of important performance issues. So we try to do things just once and do it where it is. So normalize in this case, we're calling setup. And so again, for inference, here's some new data set we want to call inference on. We go tabular object.new on the new data frame. We process it. We don't call setup because we don't want to create new mean standard deviation. We need to use the same standard deviation and mean that we used for our training. And then here's the version where we use instead data source. So you'll find that the mean and standard deviation now, the mean standard deviation is 0, 1, 2, because that's the only stuff in the training set. And again, normalization and stuff should only be done with the training set. So all this stuff of kind of using this stuff makes it much harder to screw things up in terms of modeling and accidentally do things on the whole data set, get leakage and stuff like that because we try to automatically do the right thing for you. So then fill missing is going to go through each continuous column and it will see if there are any missing values in the column. And if there are missing values, then it's going to create a NA dictionary. So if there's any NA's or any missing or any nulls or all the same idea in pandas, then that column will appear as a column name in this dictionary and the value of it will be dependent on what fill strategy you ask for. So fill strategy, this is a kind of fun little trick, fill strategy is a class that contains three different methods and you can say which of those methods you want to use. Do you want to fill things with the median or with some constant or with the mode? And so we assume by default that it's the median. So this here is actually going to call fill strategy.median, passing in the column and that's going to return the median. So that's the dictionary we create. So then later on when you're calling codes, we actually need to go through and do two things. The first thing is use the pandas fill NA to fill missing values with whatever value we put into the dictionary for that column and again we do it in place. Then the second thing is if you're asked to add an extra column to say which ones we filled missing in, which by default is true, then we're going to add a column with the same name that underscore NA at the end, which is going to be billions of true if that was originally missing and false otherwise. So here you can see we're creating three different processes which are just processes, a fill missing process with each of the possible strategies and so then we create a data frame with a missing value and then we just go through and create three tabular objects with those three different processes and make sure that the NA dict for our A column has the appropriate median or constant or load as requested. And then remember set up also processes so then we can go through and make sure that they have been replaced correctly and also make sure that the tabular object now has a categorical name which is in this case ANA. So it's not enough just to add it to the data frame, it also has to be added to cat names in the tabular object because this is something a categorical column we want to use for modeling. So Madavan asks shouldn't setups be called in the constructor and no it shouldn't. Setups is what transforms call when you call setup using the type dispatch stuff we talked about in the transforms walkthrough and then setup is something which should be called automatically only when we have enough information to know what to set up with and that information is only available once you've told us what your training set is. So that's why it's called by data source not called by the constructor. But if you're not going to use a data source then you can call it yourself. So this section is mainly kind of a few more examples of putting it all together. So here's a bunch of processes, normalize, categorify, fill missing, do nothing at all, obviously you don't need this one it's just to show you. And here's a data frame with a couple of columns, A is the categorical, B is the continuous because remember that was the order that we use. It would probably be better if we actually wrote those here at least the first time so you didn't have to remember. So we call setup because we're not using a data source on this one and so the processes you'll have noticed explicitly only work on the columns of the right type so these work just on the continuous columns for normalize. The categorify goes through the categorical columns. You might have noticed that was all cat names and that's because you also want to categorize categorical dependent variables but normalize we don't normalize continuous dependent variables. Normally for that you'll do like a sigmoid in the model or something like that. So you can throw them all in there and it will do the right thing for the right columns automatically so it just goes through and make sure that all works fine. So these are really just a bunch of tests and examples. So last section which is okay so now we have a tabular object which has got some cats and some consts and dependent variable wise. If we want to use this for modeling we need tensors. We actually need three tensors. One tensor for the continuous, one for the categorical and one for the dependent and the reason for that is that the continuous and the categorical have different data types. So we can't put them all on the same tensor because tensors have to be all of the same data type. So if you look at the version one tabular stuff it's the same thing. We have those three different tensors. So now we create some normal transform. So a lazy transform that's applied as we're getting our batches and all we do is we say okay this is the tabular object which we're going to be transforming and we just grab the encodes. We don't actually need that state at all. The encodes we're just going to grab all of the categorical variables, turn them into a tensor and make it a long and then we'll grab all the continuous, turn it into a tensor, make it a float. And so then the first thing, a tuple, is itself a tuple with those two things. So that's our independent variables and then our dependent variable is the target turned into a long. This is actually a mistake. It shouldn't always turn it into a long. It should only turn it into a long if it's continuous, sorry categorical y, otherwise it should be a continuous I think. No, let's wait until we get to modeling. I can't quite remember. If it's categorical we're then going to one-hot encode it. No, we're going to use it as, yeah that's right, so it's a long if it's categorical but for continuous it has to be float. That's right. So up to do use float for continuous target. Okay, so that's a little mistake. We haven't done any tabular regression yet in version one, version two. So that's all encodes is going to do. So then we'll come back to decodes later. So in our example here we grabbed our path to the adult sample, we read the CSV, we split it into a test set and the main bit. Made a list of our categorical and continuous, a list of the processes we wanted to use, the indexes of the splits that we wanted. So then we can create that tabular as we discussed. We can turn it into a data source with the splits. Now you'll see here it never mentioned read tab batch and the reason for that is that we don't want to force you to do things that we can do for you. So if you just say give me a tabular data loader rather than a normal data loader, the tabular data loader is a transformed data loader where we know that any after batch that you asked for we have to also add in read tab batch. So that's how that's automatically added to the transforms for you. The other thing about tabular data loader is we want to do everything a batch at a time. So particularly for the rapids on GPU stuff we don't want to pull out individual rows and then collect them later. Everything's done by grabbing a whole batch at a time. So we replace do item which is the thing that normally grabs a single item for collection. We replace it with do nothing, replace it with no of. And then we replace create batch which is the thing that normally collects things to say don't collect things but instead actually grab all of the samples directly from the tabular object using my lock. So this is if you look at that blog post I mentioned from even at Nvidia about how they got the 16x speed up by using rapids, a key piece of that was that they wrote their own version of this kind of stuff to kind of do everything batch at a time. And this is one of the key reasons we replaced the PyTorch data loader is to make this kind of thing super easy. So as you can see creating a kind of a batch at a time data loader is seven lines of code super nice and easy. So yeah I was pretty excited when this came out so quick. Okay so that's what happens when we create the tabular data loader. We could of course also create a data bunch which would probably add this to the example. And yeah that's basically it. So then at inference time as we discussed you can now do the same dot new trick we saw before in the dot process and then you can grab whatever is orc holes which is going to give us a data frame with all the modeling columns and since this is not so show batch will be the decoded version but this is not the decoded version this is the encoded version that you can pass to your modeling. All right any questions Andrew? Okay cool. All right thanks all that is it and it's Friday right? Yeah so I think we're on for Monday I'll double check and I'll let you all know. Either way I will see you later. Bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 23.88, "text": " Okay, there we go.", "tokens": [1033, 11, 456, 321, 352, 13], "temperature": 0.2, "avg_logprob": -0.8287678956985474, "compression_ratio": 0.8048780487804879, "no_speech_prob": 0.22131769359111786}, {"id": 1, "seek": 0, "start": 23.88, "end": 24.88, "text": " Hi, everybody.", "tokens": [2421, 11, 2201, 13], "temperature": 0.2, "avg_logprob": -0.8287678956985474, "compression_ratio": 0.8048780487804879, "no_speech_prob": 0.22131769359111786}, {"id": 2, "seek": 2488, "start": 24.88, "end": 37.76, "text": " Can you see me and hear me okay?", "tokens": [1664, 291, 536, 385, 293, 1568, 385, 1392, 30], "temperature": 0.0, "avg_logprob": -0.5433328877324644, "compression_ratio": 1.1388888888888888, "no_speech_prob": 6.917162681929767e-05}, {"id": 3, "seek": 2488, "start": 37.76, "end": 40.32, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.5433328877324644, "compression_ratio": 1.1388888888888888, "no_speech_prob": 6.917162681929767e-05}, {"id": 4, "seek": 2488, "start": 40.32, "end": 42.92, "text": " Hi.", "tokens": [2421, 13], "temperature": 0.0, "avg_logprob": -0.5433328877324644, "compression_ratio": 1.1388888888888888, "no_speech_prob": 6.917162681929767e-05}, {"id": 5, "seek": 2488, "start": 42.92, "end": 49.76, "text": " Hope you're ready for tabular.", "tokens": [6483, 291, 434, 1919, 337, 4421, 1040, 13], "temperature": 0.0, "avg_logprob": -0.5433328877324644, "compression_ratio": 1.1388888888888888, "no_speech_prob": 6.917162681929767e-05}, {"id": 6, "seek": 2488, "start": 49.76, "end": 54.2, "text": " Oh, and we have Andrew Schor here today as well.", "tokens": [876, 11, 293, 321, 362, 10110, 2065, 284, 510, 965, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.5433328877324644, "compression_ratio": 1.1388888888888888, "no_speech_prob": 6.917162681929767e-05}, {"id": 7, "seek": 5420, "start": 54.2, "end": 60.32, "text": " Andrew just joined Whamory Medical Research Institute as research director.", "tokens": [10110, 445, 6869, 506, 335, 827, 15896, 10303, 9446, 382, 2132, 5391, 13], "temperature": 0.0, "avg_logprob": -0.3468494415283203, "compression_ratio": 1.3395061728395061, "no_speech_prob": 0.00012140601029386744}, {"id": 8, "seek": 5420, "start": 60.32, "end": 70.12, "text": " And he is the person you may be familiar with from Music Autobot, which if you haven't checked", "tokens": [400, 415, 307, 264, 954, 291, 815, 312, 4963, 365, 490, 7609, 49909, 310, 11, 597, 498, 291, 2378, 380, 10033], "temperature": 0.0, "avg_logprob": -0.3468494415283203, "compression_ratio": 1.3395061728395061, "no_speech_prob": 0.00012140601029386744}, {"id": 9, "seek": 5420, "start": 70.12, "end": 73.16, "text": " it out, you should because it's super-sigical.", "tokens": [309, 484, 11, 291, 820, 570, 309, 311, 1687, 12, 82, 328, 804, 13], "temperature": 0.0, "avg_logprob": -0.3468494415283203, "compression_ratio": 1.3395061728395061, "no_speech_prob": 0.00012140601029386744}, {"id": 10, "seek": 7316, "start": 73.16, "end": 85.36, "text": " But now he's moving from the Jackson 5 to medical AI research.", "tokens": [583, 586, 415, 311, 2684, 490, 264, 10647, 1025, 281, 4625, 7318, 2132, 13], "temperature": 0.0, "avg_logprob": -0.4143134593963623, "compression_ratio": 1.1308411214953271, "no_speech_prob": 1.0129100701306015e-05}, {"id": 11, "seek": 7316, "start": 85.36, "end": 91.56, "text": " So tabular is a cool notebook, I think.", "tokens": [407, 4421, 1040, 307, 257, 1627, 21060, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.4143134593963623, "compression_ratio": 1.1308411214953271, "no_speech_prob": 1.0129100701306015e-05}, {"id": 12, "seek": 7316, "start": 91.56, "end": 96.12, "text": " It's a bit of fun.", "tokens": [467, 311, 257, 857, 295, 1019, 13], "temperature": 0.0, "avg_logprob": -0.4143134593963623, "compression_ratio": 1.1308411214953271, "no_speech_prob": 1.0129100701306015e-05}, {"id": 13, "seek": 9612, "start": 96.12, "end": 112.0, "text": " And the basic idea, well, let's start at the end to see how it's going to work.", "tokens": [400, 264, 3875, 1558, 11, 731, 11, 718, 311, 722, 412, 264, 917, 281, 536, 577, 309, 311, 516, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.20798450792339487, "compression_ratio": 1.5512820512820513, "no_speech_prob": 5.389680518419482e-05}, {"id": 14, "seek": 9612, "start": 112.0, "end": 117.52000000000001, "text": " So this is we're going to look at the adult data set, which is the one that we use in", "tokens": [407, 341, 307, 321, 434, 516, 281, 574, 412, 264, 5075, 1412, 992, 11, 597, 307, 264, 472, 300, 321, 764, 294], "temperature": 0.0, "avg_logprob": -0.20798450792339487, "compression_ratio": 1.5512820512820513, "no_speech_prob": 5.389680518419482e-05}, {"id": 15, "seek": 9612, "start": 117.52000000000001, "end": 122.24000000000001, "text": " most of the docs in version one and we used in some of the lessons, I think.", "tokens": [881, 295, 264, 45623, 294, 3037, 472, 293, 321, 1143, 294, 512, 295, 264, 8820, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.20798450792339487, "compression_ratio": 1.5512820512820513, "no_speech_prob": 5.389680518419482e-05}, {"id": 16, "seek": 12224, "start": 122.24, "end": 133.28, "text": " It's a pretty simple small data set.", "tokens": [467, 311, 257, 1238, 2199, 1359, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.30335176908052885, "compression_ratio": 0.8181818181818182, "no_speech_prob": 6.438569016609108e-06}, {"id": 17, "seek": 13328, "start": 133.28, "end": 155.08, "text": " It's got 32,000 rows in it and 15 columns in it.", "tokens": [467, 311, 658, 8858, 11, 1360, 13241, 294, 309, 293, 2119, 13766, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.1334430906507704, "compression_ratio": 1.1176470588235294, "no_speech_prob": 2.1233479401416844e-06}, {"id": 18, "seek": 13328, "start": 155.08, "end": 163.16, "text": " And here's what it looks like when we grab it straight from data.", "tokens": [400, 510, 311, 437, 309, 1542, 411, 562, 321, 4444, 309, 2997, 490, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1334430906507704, "compression_ratio": 1.1176470588235294, "no_speech_prob": 2.1233479401416844e-06}, {"id": 19, "seek": 16316, "start": 163.16, "end": 174.04, "text": " So basically, in order to create models from this, we need to take the categorical variables", "tokens": [407, 1936, 11, 294, 1668, 281, 1884, 5245, 490, 341, 11, 321, 643, 281, 747, 264, 19250, 804, 9102], "temperature": 0.0, "avg_logprob": -0.12311913674337822, "compression_ratio": 1.5822784810126582, "no_speech_prob": 8.529940714652184e-06}, {"id": 20, "seek": 16316, "start": 174.04, "end": 183.4, "text": " and convert them into ints, possibly with some missing category.", "tokens": [293, 7620, 552, 666, 560, 82, 11, 6264, 365, 512, 5361, 7719, 13], "temperature": 0.0, "avg_logprob": -0.12311913674337822, "compression_ratio": 1.5822784810126582, "no_speech_prob": 8.529940714652184e-06}, {"id": 21, "seek": 16316, "start": 183.4, "end": 192.16, "text": " The continuous variables, if there's anything with a missing value in a continuous variable,", "tokens": [440, 10957, 9102, 11, 498, 456, 311, 1340, 365, 257, 5361, 2158, 294, 257, 10957, 7006, 11], "temperature": 0.0, "avg_logprob": -0.12311913674337822, "compression_ratio": 1.5822784810126582, "no_speech_prob": 8.529940714652184e-06}, {"id": 22, "seek": 19216, "start": 192.16, "end": 193.64, "text": " we need to replace it with something.", "tokens": [321, 643, 281, 7406, 309, 365, 746, 13], "temperature": 0.0, "avg_logprob": -0.1508980299297132, "compression_ratio": 2.095890410958904, "no_speech_prob": 9.223241249856073e-06}, {"id": 23, "seek": 19216, "start": 193.64, "end": 197.32, "text": " So normally we replace it with something like the median.", "tokens": [407, 5646, 321, 7406, 309, 365, 746, 411, 264, 26779, 13], "temperature": 0.0, "avg_logprob": -0.1508980299297132, "compression_ratio": 2.095890410958904, "no_speech_prob": 9.223241249856073e-06}, {"id": 24, "seek": 19216, "start": 197.32, "end": 201.8, "text": " And then normally we add an additional column for each thing that has a missing value, for", "tokens": [400, 550, 5646, 321, 909, 364, 4497, 7738, 337, 1184, 551, 300, 575, 257, 5361, 2158, 11, 337], "temperature": 0.0, "avg_logprob": -0.1508980299297132, "compression_ratio": 2.095890410958904, "no_speech_prob": 9.223241249856073e-06}, {"id": 25, "seek": 19216, "start": 201.8, "end": 205.88, "text": " each column that has a missing value, and that column will be binary, which is is it", "tokens": [1184, 7738, 300, 575, 257, 5361, 2158, 11, 293, 300, 7738, 486, 312, 17434, 11, 597, 307, 307, 309], "temperature": 0.0, "avg_logprob": -0.1508980299297132, "compression_ratio": 2.095890410958904, "no_speech_prob": 9.223241249856073e-06}, {"id": 26, "seek": 19216, "start": 205.88, "end": 208.16, "text": " missing or not.", "tokens": [5361, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.1508980299297132, "compression_ratio": 2.095890410958904, "no_speech_prob": 9.223241249856073e-06}, {"id": 27, "seek": 19216, "start": 208.16, "end": 212.84, "text": " So we need to know which things are going to be the categorical variables, which column", "tokens": [407, 321, 643, 281, 458, 597, 721, 366, 516, 281, 312, 264, 19250, 804, 9102, 11, 597, 7738], "temperature": 0.0, "avg_logprob": -0.1508980299297132, "compression_ratio": 2.095890410958904, "no_speech_prob": 9.223241249856073e-06}, {"id": 28, "seek": 19216, "start": 212.84, "end": 215.7, "text": " names are going to be the continuous variables.", "tokens": [5288, 366, 516, 281, 312, 264, 10957, 9102, 13], "temperature": 0.0, "avg_logprob": -0.1508980299297132, "compression_ratio": 2.095890410958904, "no_speech_prob": 9.223241249856073e-06}, {"id": 29, "seek": 19216, "start": 215.7, "end": 221.44, "text": " So we know how to process each one.", "tokens": [407, 321, 458, 577, 281, 1399, 1184, 472, 13], "temperature": 0.0, "avg_logprob": -0.1508980299297132, "compression_ratio": 2.095890410958904, "no_speech_prob": 9.223241249856073e-06}, {"id": 30, "seek": 22144, "start": 221.44, "end": 227.88, "text": " We're going to need to know how to split our validation and training set.", "tokens": [492, 434, 516, 281, 643, 281, 458, 577, 281, 7472, 527, 24071, 293, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.16804932057857513, "compression_ratio": 1.5240963855421688, "no_speech_prob": 7.183069556049304e-06}, {"id": 31, "seek": 22144, "start": 227.88, "end": 230.32, "text": " And we need to know what's our dependent variable.", "tokens": [400, 321, 643, 281, 458, 437, 311, 527, 12334, 7006, 13], "temperature": 0.0, "avg_logprob": -0.16804932057857513, "compression_ratio": 1.5240963855421688, "no_speech_prob": 7.183069556049304e-06}, {"id": 32, "seek": 22144, "start": 230.32, "end": 239.16, "text": " So we've created a class called tabular.", "tokens": [407, 321, 600, 2942, 257, 1508, 1219, 4421, 1040, 13], "temperature": 0.0, "avg_logprob": -0.16804932057857513, "compression_ratio": 1.5240963855421688, "no_speech_prob": 7.183069556049304e-06}, {"id": 33, "seek": 22144, "start": 239.16, "end": 245.76, "text": " Basically tabular contains a data frame and it also contains a list of which things are", "tokens": [8537, 4421, 1040, 8306, 257, 1412, 3920, 293, 309, 611, 8306, 257, 1329, 295, 597, 721, 366], "temperature": 0.0, "avg_logprob": -0.16804932057857513, "compression_ratio": 1.5240963855421688, "no_speech_prob": 7.183069556049304e-06}, {"id": 34, "seek": 24576, "start": 245.76, "end": 253.04, "text": " categorical, which things are continuous, and what's your dependent variable.", "tokens": [19250, 804, 11, 597, 721, 366, 10957, 11, 293, 437, 311, 428, 12334, 7006, 13], "temperature": 0.0, "avg_logprob": -0.19444175615702589, "compression_ratio": 1.5408163265306123, "no_speech_prob": 1.7330369246337796e-06}, {"id": 35, "seek": 24576, "start": 253.04, "end": 260.84, "text": " And also some processes, which we'll look at in a moment, but they do things like turning", "tokens": [400, 611, 512, 7555, 11, 597, 321, 603, 574, 412, 294, 257, 1623, 11, 457, 436, 360, 721, 411, 6246], "temperature": 0.0, "avg_logprob": -0.19444175615702589, "compression_ratio": 1.5408163265306123, "no_speech_prob": 1.7330369246337796e-06}, {"id": 36, "seek": 24576, "start": 260.84, "end": 265.64, "text": " strings into ints for categories and filling the missing data and doing normalization of", "tokens": [13985, 666, 560, 82, 337, 10479, 293, 10623, 264, 5361, 1412, 293, 884, 2710, 2144, 295], "temperature": 0.0, "avg_logprob": -0.19444175615702589, "compression_ratio": 1.5408163265306123, "no_speech_prob": 1.7330369246337796e-06}, {"id": 37, "seek": 24576, "start": 265.64, "end": 269.44, "text": " continuous.", "tokens": [10957, 13], "temperature": 0.0, "avg_logprob": -0.19444175615702589, "compression_ratio": 1.5408163265306123, "no_speech_prob": 1.7330369246337796e-06}, {"id": 38, "seek": 24576, "start": 269.44, "end": 272.0, "text": " So that creates a tabular object.", "tokens": [407, 300, 7829, 257, 4421, 1040, 2657, 13], "temperature": 0.0, "avg_logprob": -0.19444175615702589, "compression_ratio": 1.5408163265306123, "no_speech_prob": 1.7330369246337796e-06}, {"id": 39, "seek": 27200, "start": 272.0, "end": 277.32, "text": " And from a tabular object, you can get a data source if you pass in a list of split indexes.", "tokens": [400, 490, 257, 4421, 1040, 2657, 11, 291, 393, 483, 257, 1412, 4009, 498, 291, 1320, 294, 257, 1329, 295, 7472, 8186, 279, 13], "temperature": 0.0, "avg_logprob": -0.31781559200077264, "compression_ratio": 1.4583333333333333, "no_speech_prob": 4.092847575520864e-06}, {"id": 40, "seek": 27200, "start": 277.32, "end": 283.8, "text": " So feel free to ask also Andrew if you have any questions as we go.", "tokens": [407, 841, 1737, 281, 1029, 611, 10110, 498, 291, 362, 604, 1651, 382, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.31781559200077264, "compression_ratio": 1.4583333333333333, "no_speech_prob": 4.092847575520864e-06}, {"id": 41, "seek": 27200, "start": 283.8, "end": 287.68, "text": " Oh, what's that David?", "tokens": [876, 11, 437, 311, 300, 4389, 30], "temperature": 0.0, "avg_logprob": -0.31781559200077264, "compression_ratio": 1.4583333333333333, "no_speech_prob": 4.092847575520864e-06}, {"id": 42, "seek": 27200, "start": 287.68, "end": 289.24, "text": " You want a jagged competition?", "tokens": [509, 528, 257, 6368, 3004, 6211, 30], "temperature": 0.0, "avg_logprob": -0.31781559200077264, "compression_ratio": 1.4583333333333333, "no_speech_prob": 4.092847575520864e-06}, {"id": 43, "seek": 27200, "start": 289.24, "end": 290.24, "text": " What's a jagged competition?", "tokens": [708, 311, 257, 6368, 3004, 6211, 30], "temperature": 0.0, "avg_logprob": -0.31781559200077264, "compression_ratio": 1.4583333333333333, "no_speech_prob": 4.092847575520864e-06}, {"id": 44, "seek": 27200, "start": 290.24, "end": 293.24, "text": " Tell us more.", "tokens": [5115, 505, 544, 13], "temperature": 0.0, "avg_logprob": -0.31781559200077264, "compression_ratio": 1.4583333333333333, "no_speech_prob": 4.092847575520864e-06}, {"id": 45, "seek": 27200, "start": 293.24, "end": 295.64, "text": " Oh, Kaggle.", "tokens": [876, 11, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.31781559200077264, "compression_ratio": 1.4583333333333333, "no_speech_prob": 4.092847575520864e-06}, {"id": 46, "seek": 27200, "start": 295.64, "end": 299.36, "text": " Which one?", "tokens": [3013, 472, 30], "temperature": 0.0, "avg_logprob": -0.31781559200077264, "compression_ratio": 1.4583333333333333, "no_speech_prob": 4.092847575520864e-06}, {"id": 47, "seek": 29936, "start": 299.36, "end": 303.04, "text": " So now that we've got a data source, we created a data loader from it.", "tokens": [407, 586, 300, 321, 600, 658, 257, 1412, 4009, 11, 321, 2942, 257, 1412, 3677, 260, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.279702262878418, "compression_ratio": 1.5504587155963303, "no_speech_prob": 3.089389338128967e-06}, {"id": 48, "seek": 29936, "start": 303.04, "end": 310.72, "text": " And so we have a little wrapper for that to make it easier for tabular.", "tokens": [400, 370, 321, 362, 257, 707, 46906, 337, 300, 281, 652, 309, 3571, 337, 4421, 1040, 13], "temperature": 0.0, "avg_logprob": -0.279702262878418, "compression_ratio": 1.5504587155963303, "no_speech_prob": 3.089389338128967e-06}, {"id": 49, "seek": 29936, "start": 310.72, "end": 312.24, "text": " And so then we can go show batch.", "tokens": [400, 370, 550, 321, 393, 352, 855, 15245, 13], "temperature": 0.0, "avg_logprob": -0.279702262878418, "compression_ratio": 1.5504587155963303, "no_speech_prob": 3.089389338128967e-06}, {"id": 50, "seek": 29936, "start": 312.24, "end": 314.76, "text": " Oh, which is now broken somehow.", "tokens": [876, 11, 597, 307, 586, 5463, 6063, 13], "temperature": 0.0, "avg_logprob": -0.279702262878418, "compression_ratio": 1.5504587155963303, "no_speech_prob": 3.089389338128967e-06}, {"id": 51, "seek": 29936, "start": 314.76, "end": 317.04, "text": " Nice one Jeremy.", "tokens": [5490, 472, 17809, 13], "temperature": 0.0, "avg_logprob": -0.279702262878418, "compression_ratio": 1.5504587155963303, "no_speech_prob": 3.089389338128967e-06}, {"id": 52, "seek": 29936, "start": 317.04, "end": 319.16, "text": " Damn it.", "tokens": [11907, 309, 13], "temperature": 0.0, "avg_logprob": -0.279702262878418, "compression_ratio": 1.5504587155963303, "no_speech_prob": 3.089389338128967e-06}, {"id": 53, "seek": 29936, "start": 319.16, "end": 324.28000000000003, "text": " This was all working a moment ago.", "tokens": [639, 390, 439, 1364, 257, 1623, 2057, 13], "temperature": 0.0, "avg_logprob": -0.279702262878418, "compression_ratio": 1.5504587155963303, "no_speech_prob": 3.089389338128967e-06}, {"id": 54, "seek": 29936, "start": 324.28000000000003, "end": 327.28000000000003, "text": " And then Andrew and I were just changing things at the last minute.", "tokens": [400, 550, 10110, 293, 286, 645, 445, 4473, 721, 412, 264, 1036, 3456, 13], "temperature": 0.0, "avg_logprob": -0.279702262878418, "compression_ratio": 1.5504587155963303, "no_speech_prob": 3.089389338128967e-06}, {"id": 55, "seek": 32728, "start": 327.28, "end": 330.28, "text": " So what did we break?", "tokens": [407, 437, 630, 321, 1821, 30], "temperature": 0.0, "avg_logprob": -0.35270122381357044, "compression_ratio": 1.3021582733812949, "no_speech_prob": 1.172604697785573e-06}, {"id": 56, "seek": 32728, "start": 330.28, "end": 344.59999999999997, "text": " Okay, I am going to ignore that.", "tokens": [1033, 11, 286, 669, 516, 281, 11200, 300, 13], "temperature": 0.0, "avg_logprob": -0.35270122381357044, "compression_ratio": 1.3021582733812949, "no_speech_prob": 1.172604697785573e-06}, {"id": 57, "seek": 32728, "start": 344.59999999999997, "end": 347.96, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.35270122381357044, "compression_ratio": 1.3021582733812949, "no_speech_prob": 1.172604697785573e-06}, {"id": 58, "seek": 32728, "start": 347.96, "end": 350.67999999999995, "text": " So we just broke something apparently before we started recording.", "tokens": [407, 321, 445, 6902, 746, 7970, 949, 321, 1409, 6613, 13], "temperature": 0.0, "avg_logprob": -0.35270122381357044, "compression_ratio": 1.3021582733812949, "no_speech_prob": 1.172604697785573e-06}, {"id": 59, "seek": 32728, "start": 350.67999999999995, "end": 355.59999999999997, "text": " But anyway, show batch would then show the data.", "tokens": [583, 4033, 11, 855, 15245, 576, 550, 855, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.35270122381357044, "compression_ratio": 1.3021582733812949, "no_speech_prob": 1.172604697785573e-06}, {"id": 60, "seek": 35560, "start": 355.6, "end": 360.24, "text": " And then you can take a test set that's not processed.", "tokens": [400, 550, 291, 393, 747, 257, 1500, 992, 300, 311, 406, 18846, 13], "temperature": 0.0, "avg_logprob": -0.11979851767281506, "compression_ratio": 1.90990990990991, "no_speech_prob": 8.938848623074591e-06}, {"id": 61, "seek": 35560, "start": 360.24, "end": 363.64000000000004, "text": " And basically what you want to be able to do with a test set is say I have the same", "tokens": [400, 1936, 437, 291, 528, 281, 312, 1075, 281, 360, 365, 257, 1500, 992, 307, 584, 286, 362, 264, 912], "temperature": 0.0, "avg_logprob": -0.11979851767281506, "compression_ratio": 1.90990990990991, "no_speech_prob": 8.938848623074591e-06}, {"id": 62, "seek": 35560, "start": 363.64000000000004, "end": 368.72, "text": " categorical names and continuous names and the same dependent variable and the same processes.", "tokens": [19250, 804, 5288, 293, 10957, 5288, 293, 264, 912, 12334, 7006, 293, 264, 912, 7555, 13], "temperature": 0.0, "avg_logprob": -0.11979851767281506, "compression_ratio": 1.90990990990991, "no_speech_prob": 8.938848623074591e-06}, {"id": 63, "seek": 35560, "start": 368.72, "end": 373.40000000000003, "text": " And you should then be able to apply the same pre-processing to the test set.", "tokens": [400, 291, 820, 550, 312, 1075, 281, 3079, 264, 912, 659, 12, 41075, 278, 281, 264, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.11979851767281506, "compression_ratio": 1.90990990990991, "no_speech_prob": 8.938848623074591e-06}, {"id": 64, "seek": 35560, "start": 373.40000000000003, "end": 378.48, "text": " And so to.new creates a new tabular object with all that metadata.", "tokens": [400, 370, 281, 13, 7686, 7829, 257, 777, 4421, 1040, 2657, 365, 439, 300, 26603, 13], "temperature": 0.0, "avg_logprob": -0.11979851767281506, "compression_ratio": 1.90990990990991, "no_speech_prob": 8.938848623074591e-06}, {"id": 65, "seek": 35560, "start": 378.48, "end": 381.84000000000003, "text": " And then process will do the same processing.", "tokens": [400, 550, 1399, 486, 360, 264, 912, 9007, 13], "temperature": 0.0, "avg_logprob": -0.11979851767281506, "compression_ratio": 1.90990990990991, "no_speech_prob": 8.938848623074591e-06}, {"id": 66, "seek": 38184, "start": 381.84, "end": 387.88, "text": " And so then you can see here is the normalized age, normalized education.", "tokens": [400, 370, 550, 291, 393, 536, 510, 307, 264, 48704, 3205, 11, 48704, 3309, 13], "temperature": 0.0, "avg_logprob": -0.3112833810889203, "compression_ratio": 1.4879227053140096, "no_speech_prob": 2.468120692356024e-05}, {"id": 67, "seek": 38184, "start": 387.88, "end": 390.59999999999997, "text": " All these things have been turned into ints.", "tokens": [1057, 613, 721, 362, 668, 3574, 666, 560, 82, 13], "temperature": 0.0, "avg_logprob": -0.3112833810889203, "compression_ratio": 1.4879227053140096, "no_speech_prob": 2.468120692356024e-05}, {"id": 68, "seek": 38184, "start": 390.59999999999997, "end": 392.64, "text": " Oh, so far.", "tokens": [876, 11, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.3112833810889203, "compression_ratio": 1.4879227053140096, "no_speech_prob": 2.468120692356024e-05}, {"id": 69, "seek": 38184, "start": 392.64, "end": 396.12, "text": " Excuse me.", "tokens": [11359, 385, 13], "temperature": 0.0, "avg_logprob": -0.3112833810889203, "compression_ratio": 1.4879227053140096, "no_speech_prob": 2.468120692356024e-05}, {"id": 70, "seek": 38184, "start": 396.12, "end": 401.55999999999995, "text": " So that's basically what this looks like.", "tokens": [407, 300, 311, 1936, 437, 341, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.3112833810889203, "compression_ratio": 1.4879227053140096, "no_speech_prob": 2.468120692356024e-05}, {"id": 71, "seek": 38184, "start": 401.55999999999995, "end": 405.96, "text": " You'll see I've used a subclass of tabular called tabular pandas.", "tokens": [509, 603, 536, 286, 600, 1143, 257, 1422, 11665, 295, 4421, 1040, 1219, 4421, 1040, 4565, 296, 13], "temperature": 0.0, "avg_logprob": -0.3112833810889203, "compression_ratio": 1.4879227053140096, "no_speech_prob": 2.468120692356024e-05}, {"id": 72, "seek": 38184, "start": 405.96, "end": 408.32, "text": " I don't know if it's always going to stay this way or not.", "tokens": [286, 500, 380, 458, 498, 309, 311, 1009, 516, 281, 1754, 341, 636, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.3112833810889203, "compression_ratio": 1.4879227053140096, "no_speech_prob": 2.468120692356024e-05}, {"id": 73, "seek": 40832, "start": 408.32, "end": 412.8, "text": " But currently, well, we want to be able to support multiple different backends.", "tokens": [583, 4362, 11, 731, 11, 321, 528, 281, 312, 1075, 281, 1406, 3866, 819, 646, 2581, 13], "temperature": 0.0, "avg_logprob": -0.24283284407395583, "compression_ratio": 1.5992063492063493, "no_speech_prob": 8.397832061746158e-06}, {"id": 74, "seek": 40832, "start": 412.8, "end": 416.88, "text": " So currently we have a pandas backend for tabular.", "tokens": [407, 4362, 321, 362, 257, 4565, 296, 38087, 337, 4421, 1040, 13], "temperature": 0.0, "avg_logprob": -0.24283284407395583, "compression_ratio": 1.5992063492063493, "no_speech_prob": 8.397832061746158e-06}, {"id": 75, "seek": 40832, "start": 416.88, "end": 419.52, "text": " And we're also working on a rapids backend.", "tokens": [400, 321, 434, 611, 1364, 322, 257, 5099, 3742, 38087, 13], "temperature": 0.0, "avg_logprob": -0.24283284407395583, "compression_ratio": 1.5992063492063493, "no_speech_prob": 8.397832061746158e-06}, {"id": 76, "seek": 40832, "start": 419.52, "end": 425.92, "text": " For those of you that haven't seen it, rapids is a really great project coming out of Nvidia,", "tokens": [1171, 729, 295, 291, 300, 2378, 380, 1612, 309, 11, 5099, 3742, 307, 257, 534, 869, 1716, 1348, 484, 295, 46284, 11], "temperature": 0.0, "avg_logprob": -0.24283284407395583, "compression_ratio": 1.5992063492063493, "no_speech_prob": 8.397832061746158e-06}, {"id": 77, "seek": 40832, "start": 425.92, "end": 433.68, "text": " which allows you to do so they've got something called a CUDIF, which basically gives you", "tokens": [597, 4045, 291, 281, 360, 370, 436, 600, 658, 746, 1219, 257, 29777, 3085, 37, 11, 597, 1936, 2709, 291], "temperature": 0.0, "avg_logprob": -0.24283284407395583, "compression_ratio": 1.5992063492063493, "no_speech_prob": 8.397832061746158e-06}, {"id": 78, "seek": 40832, "start": 433.68, "end": 437.92, "text": " which gives you GPU accelerated data frames.", "tokens": [597, 2709, 291, 18407, 29763, 1412, 12083, 13], "temperature": 0.0, "avg_logprob": -0.24283284407395583, "compression_ratio": 1.5992063492063493, "no_speech_prob": 8.397832061746158e-06}, {"id": 79, "seek": 43792, "start": 437.92, "end": 440.48, "text": " So that's why we have different subclasses.", "tokens": [407, 300, 311, 983, 321, 362, 819, 1422, 11665, 279, 13], "temperature": 0.0, "avg_logprob": -0.2194122314453125, "compression_ratio": 1.6961538461538461, "no_speech_prob": 9.080075869860593e-06}, {"id": 80, "seek": 43792, "start": 440.48, "end": 442.8, "text": " I don't know if this will always be a subclass of tabular.", "tokens": [286, 500, 380, 458, 498, 341, 486, 1009, 312, 257, 1422, 11665, 295, 4421, 1040, 13], "temperature": 0.0, "avg_logprob": -0.2194122314453125, "compression_ratio": 1.6961538461538461, "no_speech_prob": 9.080075869860593e-06}, {"id": 81, "seek": 43792, "start": 442.8, "end": 445.84000000000003, "text": " We may end up instead of having subclasses of data source.", "tokens": [492, 815, 917, 493, 2602, 295, 1419, 1422, 11665, 279, 295, 1412, 4009, 13], "temperature": 0.0, "avg_logprob": -0.2194122314453125, "compression_ratio": 1.6961538461538461, "no_speech_prob": 9.080075869860593e-06}, {"id": 82, "seek": 43792, "start": 445.84000000000003, "end": 448.96000000000004, "text": " But for now, we've got subclasses of tabular.", "tokens": [583, 337, 586, 11, 321, 600, 658, 1422, 11665, 279, 295, 4421, 1040, 13], "temperature": 0.0, "avg_logprob": -0.2194122314453125, "compression_ratio": 1.6961538461538461, "no_speech_prob": 9.080075869860593e-06}, {"id": 83, "seek": 43792, "start": 448.96000000000004, "end": 451.96000000000004, "text": " So that's where we're heading.", "tokens": [407, 300, 311, 689, 321, 434, 9864, 13], "temperature": 0.0, "avg_logprob": -0.2194122314453125, "compression_ratio": 1.6961538461538461, "no_speech_prob": 9.080075869860593e-06}, {"id": 84, "seek": 43792, "start": 451.96000000000004, "end": 454.96000000000004, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.2194122314453125, "compression_ratio": 1.6961538461538461, "no_speech_prob": 9.080075869860593e-06}, {"id": 85, "seek": 43792, "start": 454.96000000000004, "end": 461.20000000000005, "text": " Do you have any advice to speed up inference on the tabular amount of predictions?", "tokens": [1144, 291, 362, 604, 5192, 281, 3073, 493, 38253, 322, 264, 4421, 1040, 2372, 295, 21264, 30], "temperature": 0.0, "avg_logprob": -0.2194122314453125, "compression_ratio": 1.6961538461538461, "no_speech_prob": 9.080075869860593e-06}, {"id": 86, "seek": 43792, "start": 461.20000000000005, "end": 464.92, "text": " Well, hopefully there'll be fast, particularly if you use rapids.", "tokens": [1042, 11, 4696, 456, 603, 312, 2370, 11, 4098, 498, 291, 764, 5099, 3742, 13], "temperature": 0.0, "avg_logprob": -0.2194122314453125, "compression_ratio": 1.6961538461538461, "no_speech_prob": 9.080075869860593e-06}, {"id": 87, "seek": 43792, "start": 464.92, "end": 467.72, "text": " I mean, yeah, basically if you use rapids.", "tokens": [286, 914, 11, 1338, 11, 1936, 498, 291, 764, 5099, 3742, 13], "temperature": 0.0, "avg_logprob": -0.2194122314453125, "compression_ratio": 1.6961538461538461, "no_speech_prob": 9.080075869860593e-06}, {"id": 88, "seek": 46772, "start": 467.72, "end": 475.32000000000005, "text": " In fact, if you look at rapids, Nvidia, fast AI, hopefully.", "tokens": [682, 1186, 11, 498, 291, 574, 412, 5099, 3742, 11, 46284, 11, 2370, 7318, 11, 4696, 13], "temperature": 0.0, "avg_logprob": -0.28729766348133917, "compression_ratio": 1.4688995215311005, "no_speech_prob": 6.540220965689514e-06}, {"id": 89, "seek": 46772, "start": 475.32000000000005, "end": 478.24, "text": " Yeah, here we go.", "tokens": [865, 11, 510, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.28729766348133917, "compression_ratio": 1.4688995215311005, "no_speech_prob": 6.540220965689514e-06}, {"id": 90, "seek": 46772, "start": 478.24, "end": 486.0, "text": " So even a lot of people remember from the forums, he's at Nvidia now, which is pretty", "tokens": [407, 754, 257, 688, 295, 561, 1604, 490, 264, 26998, 11, 415, 311, 412, 46284, 586, 11, 597, 307, 1238], "temperature": 0.0, "avg_logprob": -0.28729766348133917, "compression_ratio": 1.4688995215311005, "no_speech_prob": 6.540220965689514e-06}, {"id": 91, "seek": 46772, "start": 486.0, "end": 487.0, "text": " awesome.", "tokens": [3476, 13], "temperature": 0.0, "avg_logprob": -0.28729766348133917, "compression_ratio": 1.4688995215311005, "no_speech_prob": 6.540220965689514e-06}, {"id": 92, "seek": 46772, "start": 487.0, "end": 489.28000000000003, "text": " And he he's working on the rapids team.", "tokens": [400, 415, 415, 311, 1364, 322, 264, 5099, 3742, 1469, 13], "temperature": 0.0, "avg_logprob": -0.28729766348133917, "compression_ratio": 1.4688995215311005, "no_speech_prob": 6.540220965689514e-06}, {"id": 93, "seek": 46772, "start": 489.28000000000003, "end": 496.92, "text": " And he recently posted how he got a 15x acceleration and got in the top 20 of a competition, a", "tokens": [400, 415, 3938, 9437, 577, 415, 658, 257, 2119, 87, 17162, 293, 658, 294, 264, 1192, 945, 295, 257, 6211, 11, 257], "temperature": 0.0, "avg_logprob": -0.28729766348133917, "compression_ratio": 1.4688995215311005, "no_speech_prob": 6.540220965689514e-06}, {"id": 94, "seek": 49692, "start": 496.92, "end": 501.36, "text": " very popular competition on Kaggle by combining rapids by torching fast AI.", "tokens": [588, 3743, 6211, 322, 48751, 22631, 538, 21928, 5099, 3742, 538, 3930, 17354, 2370, 7318, 13], "temperature": 0.0, "avg_logprob": -0.2215546638734879, "compression_ratio": 1.5275229357798166, "no_speech_prob": 6.14392365605454e-06}, {"id": 95, "seek": 49692, "start": 501.36, "end": 504.16, "text": " So that would be a good place to start.", "tokens": [407, 300, 576, 312, 257, 665, 1081, 281, 722, 13], "temperature": 0.0, "avg_logprob": -0.2215546638734879, "compression_ratio": 1.5275229357798166, "no_speech_prob": 6.14392365605454e-06}, {"id": 96, "seek": 49692, "start": 504.16, "end": 508.0, "text": " But hopefully by the time fast AI version two comes out, this will be super simple.", "tokens": [583, 4696, 538, 264, 565, 2370, 7318, 3037, 732, 1487, 484, 11, 341, 486, 312, 1687, 2199, 13], "temperature": 0.0, "avg_logprob": -0.2215546638734879, "compression_ratio": 1.5275229357798166, "no_speech_prob": 6.14392365605454e-06}, {"id": 97, "seek": 49692, "start": 508.0, "end": 511.6, "text": " And we're working with even on making that happen.", "tokens": [400, 321, 434, 1364, 365, 754, 322, 1455, 300, 1051, 13], "temperature": 0.0, "avg_logprob": -0.2215546638734879, "compression_ratio": 1.5275229357798166, "no_speech_prob": 6.14392365605454e-06}, {"id": 98, "seek": 49692, "start": 511.6, "end": 517.64, "text": " So thank you, even and Nvidia for that help.", "tokens": [407, 1309, 291, 11, 754, 293, 46284, 337, 300, 854, 13], "temperature": 0.0, "avg_logprob": -0.2215546638734879, "compression_ratio": 1.5275229357798166, "no_speech_prob": 6.14392365605454e-06}, {"id": 99, "seek": 49692, "start": 517.64, "end": 519.28, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.2215546638734879, "compression_ratio": 1.5275229357798166, "no_speech_prob": 6.14392365605454e-06}, {"id": 100, "seek": 49692, "start": 519.28, "end": 520.82, "text": " So let's start at tabular.", "tokens": [407, 718, 311, 722, 412, 4421, 1040, 13], "temperature": 0.0, "avg_logprob": -0.2215546638734879, "compression_ratio": 1.5275229357798166, "no_speech_prob": 6.14392365605454e-06}, {"id": 101, "seek": 52082, "start": 520.82, "end": 527.4000000000001, "text": " So basically the idea of tabular is that we.", "tokens": [407, 1936, 264, 1558, 295, 4421, 1040, 307, 300, 321, 13], "temperature": 0.0, "avg_logprob": -0.5139184474945069, "compression_ratio": 1.0388349514563107, "no_speech_prob": 6.5401141000620555e-06}, {"id": 102, "seek": 52082, "start": 527.4000000000001, "end": 532.4000000000001, "text": " The URL.", "tokens": [440, 12905, 13], "temperature": 0.0, "avg_logprob": -0.5139184474945069, "compression_ratio": 1.0388349514563107, "no_speech_prob": 6.5401141000620555e-06}, {"id": 103, "seek": 52082, "start": 532.4000000000001, "end": 542.2, "text": " Oh, yes.", "tokens": [876, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.5139184474945069, "compression_ratio": 1.0388349514563107, "no_speech_prob": 6.5401141000620555e-06}, {"id": 104, "seek": 52082, "start": 542.2, "end": 547.6800000000001, "text": " I just Googled for Nvidia AI rapids fast AI.", "tokens": [286, 445, 45005, 1493, 337, 46284, 7318, 5099, 3742, 2370, 7318, 13], "temperature": 0.0, "avg_logprob": -0.5139184474945069, "compression_ratio": 1.0388349514563107, "no_speech_prob": 6.5401141000620555e-06}, {"id": 105, "seek": 54768, "start": 547.68, "end": 550.92, "text": " And here it is, accelerating deep learning recommendation systems.", "tokens": [400, 510, 309, 307, 11, 34391, 2452, 2539, 11879, 3652, 13], "temperature": 0.0, "avg_logprob": -0.21658782469920623, "compression_ratio": 1.5279187817258884, "no_speech_prob": 6.747741281287745e-06}, {"id": 106, "seek": 54768, "start": 550.92, "end": 558.4, "text": " I'm sure we can somebody will hopefully add it to the notes as well.", "tokens": [286, 478, 988, 321, 393, 2618, 486, 4696, 909, 309, 281, 264, 5570, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.21658782469920623, "compression_ratio": 1.5279187817258884, "no_speech_prob": 6.747741281287745e-06}, {"id": 107, "seek": 54768, "start": 558.4, "end": 569.8, "text": " OK, so the basic idea with tabular was I kind of wanted to I kind of like to have a class", "tokens": [2264, 11, 370, 264, 3875, 1558, 365, 4421, 1040, 390, 286, 733, 295, 1415, 281, 286, 733, 295, 411, 281, 362, 257, 1508], "temperature": 0.0, "avg_logprob": -0.21658782469920623, "compression_ratio": 1.5279187817258884, "no_speech_prob": 6.747741281287745e-06}, {"id": 108, "seek": 54768, "start": 569.8, "end": 575.24, "text": " which kind of has all the information it needs to do what we want it to do.", "tokens": [597, 733, 295, 575, 439, 264, 1589, 309, 2203, 281, 360, 437, 321, 528, 309, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.21658782469920623, "compression_ratio": 1.5279187817258884, "no_speech_prob": 6.747741281287745e-06}, {"id": 109, "seek": 57524, "start": 575.24, "end": 584.8, "text": " So in this case, you know, a data frame doesn't have enough information to to actually build", "tokens": [407, 294, 341, 1389, 11, 291, 458, 11, 257, 1412, 3920, 1177, 380, 362, 1547, 1589, 281, 281, 767, 1322], "temperature": 0.0, "avg_logprob": -0.16228809564009958, "compression_ratio": 1.7207207207207207, "no_speech_prob": 4.936803634336684e-06}, {"id": 110, "seek": 57524, "start": 584.8, "end": 589.6800000000001, "text": " models because until you know what the categorical variables are, the continuous variables are", "tokens": [5245, 570, 1826, 291, 458, 437, 264, 19250, 804, 9102, 366, 11, 264, 10957, 9102, 366], "temperature": 0.0, "avg_logprob": -0.16228809564009958, "compression_ratio": 1.7207207207207207, "no_speech_prob": 4.936803634336684e-06}, {"id": 111, "seek": 57524, "start": 589.6800000000001, "end": 593.8, "text": " and what pre-processing to do and what the dependent variable is, you know, you can't", "tokens": [293, 437, 659, 12, 41075, 278, 281, 360, 293, 437, 264, 12334, 7006, 307, 11, 291, 458, 11, 291, 393, 380], "temperature": 0.0, "avg_logprob": -0.16228809564009958, "compression_ratio": 1.7207207207207207, "no_speech_prob": 4.936803634336684e-06}, {"id": 112, "seek": 57524, "start": 593.8, "end": 595.6, "text": " really do much.", "tokens": [534, 360, 709, 13], "temperature": 0.0, "avg_logprob": -0.16228809564009958, "compression_ratio": 1.7207207207207207, "no_speech_prob": 4.936803634336684e-06}, {"id": 113, "seek": 57524, "start": 595.6, "end": 602.92, "text": " So that's the basic kind of idea of this design was to have something with that information.", "tokens": [407, 300, 311, 264, 3875, 733, 295, 1558, 295, 341, 1715, 390, 281, 362, 746, 365, 300, 1589, 13], "temperature": 0.0, "avg_logprob": -0.16228809564009958, "compression_ratio": 1.7207207207207207, "no_speech_prob": 4.936803634336684e-06}, {"id": 114, "seek": 60292, "start": 602.92, "end": 608.0799999999999, "text": " So categorical names, continuous names, Y names.", "tokens": [407, 19250, 804, 5288, 11, 10957, 5288, 11, 398, 5288, 13], "temperature": 0.0, "avg_logprob": -0.231363328836732, "compression_ratio": 1.4387096774193548, "no_speech_prob": 3.34048399963649e-06}, {"id": 115, "seek": 60292, "start": 608.0799999999999, "end": 613.9599999999999, "text": " Normally there's just one dependent variable, so one Y name, but you could have more and", "tokens": [17424, 456, 311, 445, 472, 12334, 7006, 11, 370, 472, 398, 1315, 11, 457, 291, 727, 362, 544, 293], "temperature": 0.0, "avg_logprob": -0.231363328836732, "compression_ratio": 1.4387096774193548, "no_speech_prob": 3.34048399963649e-06}, {"id": 116, "seek": 60292, "start": 613.9599999999999, "end": 614.9599999999999, "text": " processes.", "tokens": [7555, 13], "temperature": 0.0, "avg_logprob": -0.231363328836732, "compression_ratio": 1.4387096774193548, "no_speech_prob": 3.34048399963649e-06}, {"id": 117, "seek": 60292, "start": 614.9599999999999, "end": 624.68, "text": " So those are basically the four things that we want to put in our tabular.", "tokens": [407, 729, 366, 1936, 264, 1451, 721, 300, 321, 528, 281, 829, 294, 527, 4421, 1040, 13], "temperature": 0.0, "avg_logprob": -0.231363328836732, "compression_ratio": 1.4387096774193548, "no_speech_prob": 3.34048399963649e-06}, {"id": 118, "seek": 62468, "start": 624.68, "end": 645.1999999999999, "text": " So there we go.", "tokens": [407, 456, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.4227633476257324, "compression_ratio": 1.0886075949367089, "no_speech_prob": 6.643157576036174e-06}, {"id": 119, "seek": 62468, "start": 645.1999999999999, "end": 648.88, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.4227633476257324, "compression_ratio": 1.0886075949367089, "no_speech_prob": 6.643157576036174e-06}, {"id": 120, "seek": 62468, "start": 648.88, "end": 654.4, "text": " So those are that's why we're passing in those four things.", "tokens": [407, 729, 366, 300, 311, 983, 321, 434, 8437, 294, 729, 1451, 721, 13], "temperature": 0.0, "avg_logprob": -0.4227633476257324, "compression_ratio": 1.0886075949367089, "no_speech_prob": 6.643157576036174e-06}, {"id": 121, "seek": 65440, "start": 654.4, "end": 662.12, "text": " And then the other thing we need to know about Y is is why categorical or continuous?", "tokens": [400, 550, 264, 661, 551, 321, 643, 281, 458, 466, 398, 307, 307, 983, 19250, 804, 420, 10957, 30], "temperature": 0.0, "avg_logprob": -0.26527798694113025, "compression_ratio": 1.5921052631578947, "no_speech_prob": 1.0952587217616383e-05}, {"id": 122, "seek": 65440, "start": 662.12, "end": 665.1999999999999, "text": " So you just pass that in the symbolium.", "tokens": [407, 291, 445, 1320, 300, 294, 264, 5986, 2197, 13], "temperature": 0.0, "avg_logprob": -0.26527798694113025, "compression_ratio": 1.5921052631578947, "no_speech_prob": 1.0952587217616383e-05}, {"id": 123, "seek": 65440, "start": 665.1999999999999, "end": 669.9599999999999, "text": " So a man wants to know why might we need more than one Y name?", "tokens": [407, 257, 587, 2738, 281, 458, 983, 1062, 321, 643, 544, 813, 472, 398, 1315, 30], "temperature": 0.0, "avg_logprob": -0.26527798694113025, "compression_ratio": 1.5921052631578947, "no_speech_prob": 1.0952587217616383e-05}, {"id": 124, "seek": 65440, "start": 669.9599999999999, "end": 674.4399999999999, "text": " So few examples, you could be doing a regression problem where you're trying to predict the", "tokens": [407, 1326, 5110, 11, 291, 727, 312, 884, 257, 24590, 1154, 689, 291, 434, 1382, 281, 6069, 264], "temperature": 0.0, "avg_logprob": -0.26527798694113025, "compression_ratio": 1.5921052631578947, "no_speech_prob": 1.0952587217616383e-05}, {"id": 125, "seek": 65440, "start": 674.4399999999999, "end": 680.24, "text": " X and Y coordinates of the destination of a taxi ride or perhaps you're just doing", "tokens": [1783, 293, 398, 21056, 295, 264, 12236, 295, 257, 18984, 5077, 420, 4317, 291, 434, 445, 884], "temperature": 0.0, "avg_logprob": -0.26527798694113025, "compression_ratio": 1.5921052631578947, "no_speech_prob": 1.0952587217616383e-05}, {"id": 126, "seek": 68024, "start": 680.24, "end": 685.84, "text": " a multi-label classification where you can have multiple things being true would be a", "tokens": [257, 4825, 12, 75, 18657, 21538, 689, 291, 393, 362, 3866, 721, 885, 2074, 576, 312, 257], "temperature": 0.0, "avg_logprob": -0.21055064481847427, "compression_ratio": 1.5889830508474576, "no_speech_prob": 3.3405249268980697e-06}, {"id": 127, "seek": 68024, "start": 685.84, "end": 686.84, "text": " couple of examples.", "tokens": [1916, 295, 5110, 13], "temperature": 0.0, "avg_logprob": -0.21055064481847427, "compression_ratio": 1.5889830508474576, "no_speech_prob": 3.3405249268980697e-06}, {"id": 128, "seek": 68024, "start": 686.84, "end": 691.52, "text": " And maybe it's already one-hot encoded or something in that case.", "tokens": [400, 1310, 309, 311, 1217, 472, 12, 12194, 2058, 12340, 420, 746, 294, 300, 1389, 13], "temperature": 0.0, "avg_logprob": -0.21055064481847427, "compression_ratio": 1.5889830508474576, "no_speech_prob": 3.3405249268980697e-06}, {"id": 129, "seek": 68024, "start": 691.52, "end": 697.36, "text": " Okay, so that's the theory behind the design of tabular.", "tokens": [1033, 11, 370, 300, 311, 264, 5261, 2261, 264, 1715, 295, 4421, 1040, 13], "temperature": 0.0, "avg_logprob": -0.21055064481847427, "compression_ratio": 1.5889830508474576, "no_speech_prob": 3.3405249268980697e-06}, {"id": 130, "seek": 68024, "start": 697.36, "end": 701.8, "text": " So when we initialize it, we just pass in that stuff.", "tokens": [407, 562, 321, 5883, 1125, 309, 11, 321, 445, 1320, 294, 300, 1507, 13], "temperature": 0.0, "avg_logprob": -0.21055064481847427, "compression_ratio": 1.5889830508474576, "no_speech_prob": 3.3405249268980697e-06}, {"id": 131, "seek": 68024, "start": 701.8, "end": 707.6, "text": " The processes that we pass in are just going to be transforms.", "tokens": [440, 7555, 300, 321, 1320, 294, 366, 445, 516, 281, 312, 35592, 13], "temperature": 0.0, "avg_logprob": -0.21055064481847427, "compression_ratio": 1.5889830508474576, "no_speech_prob": 3.3405249268980697e-06}, {"id": 132, "seek": 68024, "start": 707.6, "end": 708.96, "text": " So we can dump in a pipeline.", "tokens": [407, 321, 393, 11430, 294, 257, 15517, 13], "temperature": 0.0, "avg_logprob": -0.21055064481847427, "compression_ratio": 1.5889830508474576, "no_speech_prob": 3.3405249268980697e-06}, {"id": 133, "seek": 70896, "start": 708.96, "end": 712.8000000000001, "text": " And so this stuff kind of you'll see that we just keep reusing the same foundational", "tokens": [400, 370, 341, 1507, 733, 295, 291, 603, 536, 300, 321, 445, 1066, 319, 7981, 264, 912, 32195], "temperature": 0.0, "avg_logprob": -0.19956835033824144, "compression_ratio": 1.6877637130801688, "no_speech_prob": 1.4367386711455765e-06}, {"id": 134, "seek": 70896, "start": 712.8000000000001, "end": 717.84, "text": " concepts throughout fast.ai version two, which is a good sign that there's strong foundations.", "tokens": [10392, 3710, 2370, 13, 1301, 3037, 732, 11, 597, 307, 257, 665, 1465, 300, 456, 311, 2068, 22467, 13], "temperature": 0.0, "avg_logprob": -0.19956835033824144, "compression_ratio": 1.6877637130801688, "no_speech_prob": 1.4367386711455765e-06}, {"id": 135, "seek": 70896, "start": 717.84, "end": 723.6, "text": " So the processes are things that we want to run a bunch of them.", "tokens": [407, 264, 7555, 366, 721, 300, 321, 528, 281, 1190, 257, 3840, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.19956835033824144, "compression_ratio": 1.6877637130801688, "no_speech_prob": 1.4367386711455765e-06}, {"id": 136, "seek": 70896, "start": 723.6, "end": 727.5600000000001, "text": " We want to depend on what type something is as to whether we run it or not, stuff like", "tokens": [492, 528, 281, 5672, 322, 437, 2010, 746, 307, 382, 281, 1968, 321, 1190, 309, 420, 406, 11, 1507, 411], "temperature": 0.0, "avg_logprob": -0.19956835033824144, "compression_ratio": 1.6877637130801688, "no_speech_prob": 1.4367386711455765e-06}, {"id": 137, "seek": 70896, "start": 727.5600000000001, "end": 729.4200000000001, "text": " that.", "tokens": [300, 13], "temperature": 0.0, "avg_logprob": -0.19956835033824144, "compression_ratio": 1.6877637130801688, "no_speech_prob": 1.4367386711455765e-06}, {"id": 138, "seek": 70896, "start": 729.4200000000001, "end": 733.2800000000001, "text": " So it's got all that kind of behavior we want to the pipeline.", "tokens": [407, 309, 311, 658, 439, 300, 733, 295, 5223, 321, 528, 281, 264, 15517, 13], "temperature": 0.0, "avg_logprob": -0.19956835033824144, "compression_ratio": 1.6877637130801688, "no_speech_prob": 1.4367386711455765e-06}, {"id": 139, "seek": 73328, "start": 733.28, "end": 742.76, "text": " Unlike tiffmds, tiffmdl, tiffmlist, all of those things apply transformations lazily.", "tokens": [17657, 256, 3661, 76, 16063, 11, 256, 3661, 76, 67, 75, 11, 256, 3661, 76, 8264, 11, 439, 295, 729, 721, 3079, 34852, 19320, 953, 13], "temperature": 0.0, "avg_logprob": -0.17762409066254237, "compression_ratio": 1.5859030837004404, "no_speech_prob": 1.6280322370221256e-06}, {"id": 140, "seek": 73328, "start": 742.76, "end": 746.3199999999999, "text": " On tabular data, we don't generally do that.", "tokens": [1282, 4421, 1040, 1412, 11, 321, 500, 380, 5101, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.17762409066254237, "compression_ratio": 1.5859030837004404, "no_speech_prob": 1.6280322370221256e-06}, {"id": 141, "seek": 73328, "start": 746.3199999999999, "end": 748.12, "text": " A number of reasons why.", "tokens": [316, 1230, 295, 4112, 983, 13], "temperature": 0.0, "avg_logprob": -0.17762409066254237, "compression_ratio": 1.5859030837004404, "no_speech_prob": 1.6280322370221256e-06}, {"id": 142, "seek": 73328, "start": 748.12, "end": 752.92, "text": " The first is that unlike kind of opening an image or something, it doesn't take a long", "tokens": [440, 700, 307, 300, 8343, 733, 295, 5193, 364, 3256, 420, 746, 11, 309, 1177, 380, 747, 257, 938], "temperature": 0.0, "avg_logprob": -0.17762409066254237, "compression_ratio": 1.5859030837004404, "no_speech_prob": 1.6280322370221256e-06}, {"id": 143, "seek": 73328, "start": 752.92, "end": 756.88, "text": " time to grab a row of data.", "tokens": [565, 281, 4444, 257, 5386, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.17762409066254237, "compression_ratio": 1.5859030837004404, "no_speech_prob": 1.6280322370221256e-06}, {"id": 144, "seek": 73328, "start": 756.88, "end": 762.92, "text": " So like it's fine to read the whole lot of rows normally, except in some cases of really,", "tokens": [407, 411, 309, 311, 2489, 281, 1401, 264, 1379, 688, 295, 13241, 5646, 11, 3993, 294, 512, 3331, 295, 534, 11], "temperature": 0.0, "avg_logprob": -0.17762409066254237, "compression_ratio": 1.5859030837004404, "no_speech_prob": 1.6280322370221256e-06}, {"id": 145, "seek": 76292, "start": 762.92, "end": 765.1999999999999, "text": " really big data sets.", "tokens": [534, 955, 1412, 6352, 13], "temperature": 0.0, "avg_logprob": -0.11589646339416504, "compression_ratio": 1.7739463601532568, "no_speech_prob": 3.6687883948616218e-06}, {"id": 146, "seek": 76292, "start": 765.1999999999999, "end": 770.64, "text": " The second reason is that most kind of tabular stuff is designed to work quickly on lots", "tokens": [440, 1150, 1778, 307, 300, 881, 733, 295, 4421, 1040, 1507, 307, 4761, 281, 589, 2661, 322, 3195], "temperature": 0.0, "avg_logprob": -0.11589646339416504, "compression_ratio": 1.7739463601532568, "no_speech_prob": 3.6687883948616218e-06}, {"id": 147, "seek": 76292, "start": 770.64, "end": 771.64, "text": " of rows at a time.", "tokens": [295, 13241, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.11589646339416504, "compression_ratio": 1.7739463601532568, "no_speech_prob": 3.6687883948616218e-06}, {"id": 148, "seek": 76292, "start": 771.64, "end": 775.12, "text": " So it's going to be much, much faster if you do it ahead of time.", "tokens": [407, 309, 311, 516, 281, 312, 709, 11, 709, 4663, 498, 291, 360, 309, 2286, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.11589646339416504, "compression_ratio": 1.7739463601532568, "no_speech_prob": 3.6687883948616218e-06}, {"id": 149, "seek": 76292, "start": 775.12, "end": 780.4799999999999, "text": " The third is that most pre-processing is going to be not data augmentation kind of stuff,", "tokens": [440, 2636, 307, 300, 881, 659, 12, 41075, 278, 307, 516, 281, 312, 406, 1412, 14501, 19631, 733, 295, 1507, 11], "temperature": 0.0, "avg_logprob": -0.11589646339416504, "compression_ratio": 1.7739463601532568, "no_speech_prob": 3.6687883948616218e-06}, {"id": 150, "seek": 76292, "start": 780.4799999999999, "end": 785.64, "text": " but more just once off, cleaning up labels and things like that.", "tokens": [457, 544, 445, 1564, 766, 11, 8924, 493, 16949, 293, 721, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.11589646339416504, "compression_ratio": 1.7739463601532568, "no_speech_prob": 3.6687883948616218e-06}, {"id": 151, "seek": 76292, "start": 785.64, "end": 789.76, "text": " So for all these kinds of reasons, our processing in tabular is generally done ahead of time", "tokens": [407, 337, 439, 613, 3685, 295, 4112, 11, 527, 9007, 294, 4421, 1040, 307, 5101, 1096, 2286, 295, 565], "temperature": 0.0, "avg_logprob": -0.11589646339416504, "compression_ratio": 1.7739463601532568, "no_speech_prob": 3.6687883948616218e-06}, {"id": 152, "seek": 76292, "start": 789.76, "end": 790.76, "text": " rather than lazily.", "tokens": [2831, 813, 19320, 953, 13], "temperature": 0.0, "avg_logprob": -0.11589646339416504, "compression_ratio": 1.7739463601532568, "no_speech_prob": 3.6687883948616218e-06}, {"id": 153, "seek": 79076, "start": 790.76, "end": 794.48, "text": " But it's still a pipeline of transforms.", "tokens": [583, 309, 311, 920, 257, 15517, 295, 35592, 13], "temperature": 0.0, "avg_logprob": -0.1901812710604825, "compression_ratio": 1.5023696682464456, "no_speech_prob": 3.905468474840745e-06}, {"id": 154, "seek": 79076, "start": 794.48, "end": 795.92, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1901812710604825, "compression_ratio": 1.5023696682464456, "no_speech_prob": 3.905468474840745e-06}, {"id": 155, "seek": 79076, "start": 795.92, "end": 800.96, "text": " So then we're going to store something called cat y, which will be our y variable if it's", "tokens": [407, 550, 321, 434, 516, 281, 3531, 746, 1219, 3857, 288, 11, 597, 486, 312, 527, 288, 7006, 498, 309, 311], "temperature": 0.0, "avg_logprob": -0.1901812710604825, "compression_ratio": 1.5023696682464456, "no_speech_prob": 3.905468474840745e-06}, {"id": 156, "seek": 79076, "start": 800.96, "end": 807.72, "text": " a categorical, otherwise none, and vice versa for convoy.", "tokens": [257, 19250, 804, 11, 5911, 6022, 11, 293, 11964, 25650, 337, 3754, 939, 13], "temperature": 0.0, "avg_logprob": -0.1901812710604825, "compression_ratio": 1.5023696682464456, "no_speech_prob": 3.905468474840745e-06}, {"id": 157, "seek": 79076, "start": 807.72, "end": 810.3199999999999, "text": " So that's our initializer.", "tokens": [407, 300, 311, 527, 5883, 6545, 13], "temperature": 0.0, "avg_logprob": -0.1901812710604825, "compression_ratio": 1.5023696682464456, "no_speech_prob": 3.905468474840745e-06}, {"id": 158, "seek": 79076, "start": 810.3199999999999, "end": 812.28, "text": " And let's take a look at it in use.", "tokens": [400, 718, 311, 747, 257, 574, 412, 309, 294, 764, 13], "temperature": 0.0, "avg_logprob": -0.1901812710604825, "compression_ratio": 1.5023696682464456, "no_speech_prob": 3.905468474840745e-06}, {"id": 159, "seek": 79076, "start": 812.28, "end": 817.4, "text": " So basically we create a data frame containing two columns.", "tokens": [407, 1936, 321, 1884, 257, 1412, 3920, 19273, 732, 13766, 13], "temperature": 0.0, "avg_logprob": -0.1901812710604825, "compression_ratio": 1.5023696682464456, "no_speech_prob": 3.905468474840745e-06}, {"id": 160, "seek": 81740, "start": 817.4, "end": 824.16, "text": " And from that we can create a tabular object, passing in that data frame and saying cat", "tokens": [400, 490, 300, 321, 393, 1884, 257, 4421, 1040, 2657, 11, 8437, 294, 300, 1412, 3920, 293, 1566, 3857], "temperature": 0.0, "avg_logprob": -0.15558431982024898, "compression_ratio": 1.8157894736842106, "no_speech_prob": 4.4254661588638555e-06}, {"id": 161, "seek": 81740, "start": 824.16, "end": 826.04, "text": " names, con names, y names, whatever.", "tokens": [5288, 11, 416, 5288, 11, 288, 5288, 11, 2035, 13], "temperature": 0.0, "avg_logprob": -0.15558431982024898, "compression_ratio": 1.8157894736842106, "no_speech_prob": 4.4254661588638555e-06}, {"id": 162, "seek": 81740, "start": 826.04, "end": 829.16, "text": " So in this case we'll just have cat names.", "tokens": [407, 294, 341, 1389, 321, 603, 445, 362, 3857, 5288, 13], "temperature": 0.0, "avg_logprob": -0.15558431982024898, "compression_ratio": 1.8157894736842106, "no_speech_prob": 4.4254661588638555e-06}, {"id": 163, "seek": 81740, "start": 829.16, "end": 832.76, "text": " One thing that's always a good idea is to make sure that things pickle okay, because", "tokens": [1485, 551, 300, 311, 1009, 257, 665, 1558, 307, 281, 652, 988, 300, 721, 31433, 1392, 11, 570], "temperature": 0.0, "avg_logprob": -0.15558431982024898, "compression_ratio": 1.8157894736842106, "no_speech_prob": 4.4254661588638555e-06}, {"id": 164, "seek": 81740, "start": 832.76, "end": 835.68, "text": " the inference and stuff for this metadata has to be pickable.", "tokens": [264, 38253, 293, 1507, 337, 341, 26603, 575, 281, 312, 1888, 712, 13], "temperature": 0.0, "avg_logprob": -0.15558431982024898, "compression_ratio": 1.8157894736842106, "no_speech_prob": 4.4254661588638555e-06}, {"id": 165, "seek": 81740, "start": 835.68, "end": 839.9599999999999, "text": " So dumping something to a string and then loading that string up again is always a good", "tokens": [407, 42224, 746, 281, 257, 6798, 293, 550, 15114, 300, 6798, 493, 797, 307, 1009, 257, 665], "temperature": 0.0, "avg_logprob": -0.15558431982024898, "compression_ratio": 1.8157894736842106, "no_speech_prob": 4.4254661588638555e-06}, {"id": 166, "seek": 81740, "start": 839.9599999999999, "end": 846.0799999999999, "text": " idea to make sure it works and make sure it's the same as what you started with.", "tokens": [1558, 281, 652, 988, 309, 1985, 293, 652, 988, 309, 311, 264, 912, 382, 437, 291, 1409, 365, 13], "temperature": 0.0, "avg_logprob": -0.15558431982024898, "compression_ratio": 1.8157894736842106, "no_speech_prob": 4.4254661588638555e-06}, {"id": 167, "seek": 84608, "start": 846.08, "end": 854.6, "text": " So we're inheriting from call base and call base is just a small little thing in core,", "tokens": [407, 321, 434, 9484, 1748, 490, 818, 3096, 293, 818, 3096, 307, 445, 257, 1359, 707, 551, 294, 4965, 11], "temperature": 0.0, "avg_logprob": -0.21159720907405932, "compression_ratio": 1.752212389380531, "no_speech_prob": 4.936925961374072e-06}, {"id": 168, "seek": 84608, "start": 854.6, "end": 860.2, "text": " which defines the basic things you would expect to have in a collection and it implements", "tokens": [597, 23122, 264, 3875, 721, 291, 576, 2066, 281, 362, 294, 257, 5765, 293, 309, 704, 17988], "temperature": 0.0, "avg_logprob": -0.21159720907405932, "compression_ratio": 1.752212389380531, "no_speech_prob": 4.936925961374072e-06}, {"id": 169, "seek": 84608, "start": 860.2, "end": 861.2, "text": " them by composition.", "tokens": [552, 538, 12686, 13], "temperature": 0.0, "avg_logprob": -0.21159720907405932, "compression_ratio": 1.752212389380531, "no_speech_prob": 4.936925961374072e-06}, {"id": 170, "seek": 84608, "start": 861.2, "end": 866.12, "text": " So you pass in some list or whatever.", "tokens": [407, 291, 1320, 294, 512, 1329, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.21159720907405932, "compression_ratio": 1.752212389380531, "no_speech_prob": 4.936925961374072e-06}, {"id": 171, "seek": 84608, "start": 866.12, "end": 869.12, "text": " And so the length of your call base will be the length of that list.", "tokens": [400, 370, 264, 4641, 295, 428, 818, 3096, 486, 312, 264, 4641, 295, 300, 1329, 13], "temperature": 0.0, "avg_logprob": -0.21159720907405932, "compression_ratio": 1.752212389380531, "no_speech_prob": 4.936925961374072e-06}, {"id": 172, "seek": 84608, "start": 869.12, "end": 876.0400000000001, "text": " So like you can often just inherit from something to do this, but composition is often more", "tokens": [407, 411, 291, 393, 2049, 445, 21389, 490, 746, 281, 360, 341, 11, 457, 12686, 307, 2049, 544], "temperature": 0.0, "avg_logprob": -0.21159720907405932, "compression_ratio": 1.752212389380531, "no_speech_prob": 4.936925961374072e-06}, {"id": 173, "seek": 87604, "start": 876.04, "end": 879.56, "text": " interesting, can give you some more flexibility.", "tokens": [1880, 11, 393, 976, 291, 512, 544, 12635, 13], "temperature": 0.0, "avg_logprob": -0.18604454339719287, "compression_ratio": 1.8, "no_speech_prob": 4.710852408607025e-06}, {"id": 174, "seek": 87604, "start": 879.56, "end": 884.7199999999999, "text": " So this basically gives you the very quick, get item is defined by calling the self.items", "tokens": [407, 341, 1936, 2709, 291, 264, 588, 1702, 11, 483, 3174, 307, 7642, 538, 5141, 264, 2698, 13, 270, 9097], "temperature": 0.0, "avg_logprob": -0.18604454339719287, "compression_ratio": 1.8, "no_speech_prob": 4.710852408607025e-06}, {"id": 175, "seek": 87604, "start": 884.7199999999999, "end": 890.12, "text": " get item and length is defined by calling the self.items length and so forth.", "tokens": [483, 3174, 293, 4641, 307, 7642, 538, 5141, 264, 2698, 13, 270, 9097, 4641, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.18604454339719287, "compression_ratio": 1.8, "no_speech_prob": 4.710852408607025e-06}, {"id": 176, "seek": 87604, "start": 890.12, "end": 898.16, "text": " So by inheriting from call base, we can then in the inner simply say super.inert df.", "tokens": [407, 538, 9484, 1748, 490, 818, 3096, 11, 321, 393, 550, 294, 264, 7284, 2935, 584, 1687, 13, 259, 911, 274, 69, 13], "temperature": 0.0, "avg_logprob": -0.18604454339719287, "compression_ratio": 1.8, "no_speech_prob": 4.710852408607025e-06}, {"id": 177, "seek": 87604, "start": 898.16, "end": 902.0799999999999, "text": " And so that means we're going to now going to have something called self.items, which", "tokens": [400, 370, 300, 1355, 321, 434, 516, 281, 586, 516, 281, 362, 746, 1219, 2698, 13, 270, 9097, 11, 597], "temperature": 0.0, "avg_logprob": -0.18604454339719287, "compression_ratio": 1.8, "no_speech_prob": 4.710852408607025e-06}, {"id": 178, "seek": 90208, "start": 902.08, "end": 906.2800000000001, "text": " is going to be that data frame.", "tokens": [307, 516, 281, 312, 300, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.3600874755342128, "compression_ratio": 1.4104477611940298, "no_speech_prob": 5.014597263652831e-06}, {"id": 179, "seek": 90208, "start": 906.2800000000001, "end": 911.76, "text": " And so here we check that, you know, the pick old version of items is the same as the tabular", "tokens": [400, 370, 510, 321, 1520, 300, 11, 291, 458, 11, 264, 1888, 1331, 3037, 295, 4754, 307, 264, 912, 382, 264, 4421, 1040], "temperature": 0.0, "avg_logprob": -0.3600874755342128, "compression_ratio": 1.4104477611940298, "no_speech_prob": 5.014597263652831e-06}, {"id": 180, "seek": 90208, "start": 911.76, "end": 914.84, "text": " objects version of items.", "tokens": [6565, 3037, 295, 4754, 13], "temperature": 0.0, "avg_logprob": -0.3600874755342128, "compression_ratio": 1.4104477611940298, "no_speech_prob": 5.014597263652831e-06}, {"id": 181, "seek": 90208, "start": 914.84, "end": 916.98, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3600874755342128, "compression_ratio": 1.4104477611940298, "no_speech_prob": 5.014597263652831e-06}, {"id": 182, "seek": 90208, "start": 916.98, "end": 924.24, "text": " So the, let me just press this.", "tokens": [407, 264, 11, 718, 385, 445, 1886, 341, 13], "temperature": 0.0, "avg_logprob": -0.3600874755342128, "compression_ratio": 1.4104477611940298, "no_speech_prob": 5.014597263652831e-06}, {"id": 183, "seek": 92424, "start": 924.24, "end": 941.5600000000001, "text": " Where are we?", "tokens": [2305, 366, 321, 30], "temperature": 0.0, "avg_logprob": -0.22339412689208984, "compression_ratio": 1.1686746987951808, "no_speech_prob": 8.530203558620997e-06}, {"id": 184, "seek": 92424, "start": 941.5600000000001, "end": 948.36, "text": " So the next thing to notice is that there are various useful little attributes like", "tokens": [407, 264, 958, 551, 281, 3449, 307, 300, 456, 366, 3683, 4420, 707, 17212, 411], "temperature": 0.0, "avg_logprob": -0.22339412689208984, "compression_ratio": 1.1686746987951808, "no_speech_prob": 8.530203558620997e-06}, {"id": 185, "seek": 94836, "start": 948.36, "end": 960.46, "text": " allColumns, allCats for allCategoricalColumns, allConts for allContinuousColumns.", "tokens": [439, 34, 401, 449, 3695, 11, 439, 34, 1720, 337, 439, 34, 2968, 284, 804, 34, 401, 449, 3695, 11, 439, 34, 896, 82, 337, 439, 34, 23039, 12549, 34, 401, 449, 3695, 13], "temperature": 0.0, "avg_logprob": -0.19205001027960528, "compression_ratio": 1.9027777777777777, "no_speech_prob": 2.684188530110987e-06}, {"id": 186, "seek": 94836, "start": 960.46, "end": 964.36, "text": " And then there's also the same with names at the end, allContNames, allCatNames, allColumnNames.", "tokens": [400, 550, 456, 311, 611, 264, 912, 365, 5288, 412, 264, 917, 11, 439, 29821, 45, 1632, 11, 439, 34, 267, 45, 1632, 11, 439, 34, 401, 16449, 45, 1632, 13], "temperature": 0.0, "avg_logprob": -0.19205001027960528, "compression_ratio": 1.9027777777777777, "no_speech_prob": 2.684188530110987e-06}, {"id": 187, "seek": 94836, "start": 964.36, "end": 973.54, "text": " So you can see allContNames is just the continuous names plus the continuous y if there is one.", "tokens": [407, 291, 393, 536, 439, 29821, 45, 1632, 307, 445, 264, 10957, 5288, 1804, 264, 10957, 288, 498, 456, 307, 472, 13], "temperature": 0.0, "avg_logprob": -0.19205001027960528, "compression_ratio": 1.9027777777777777, "no_speech_prob": 2.684188530110987e-06}, {"id": 188, "seek": 97354, "start": 973.54, "end": 978.5, "text": " This would not work except for the fact that we use the capital L plus.", "tokens": [639, 576, 406, 589, 3993, 337, 264, 1186, 300, 321, 764, 264, 4238, 441, 1804, 13], "temperature": 0.0, "avg_logprob": -0.22567442351696537, "compression_ratio": 1.6177777777777778, "no_speech_prob": 4.425466613611206e-06}, {"id": 189, "seek": 97354, "start": 978.5, "end": 984.16, "text": " So if you add a none to a capital L plus, it doesn't change it, which is exactly what", "tokens": [407, 498, 291, 909, 257, 6022, 281, 257, 4238, 441, 1804, 11, 309, 1177, 380, 1319, 309, 11, 597, 307, 2293, 437], "temperature": 0.0, "avg_logprob": -0.22567442351696537, "compression_ratio": 1.6177777777777778, "no_speech_prob": 4.425466613611206e-06}, {"id": 190, "seek": 97354, "start": 984.16, "end": 985.28, "text": " you want most of the time.", "tokens": [291, 528, 881, 295, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.22567442351696537, "compression_ratio": 1.6177777777777778, "no_speech_prob": 4.425466613611206e-06}, {"id": 191, "seek": 97354, "start": 985.28, "end": 989.76, "text": " So some of the things in capital L are a bit more convenient than this.", "tokens": [407, 512, 295, 264, 721, 294, 4238, 441, 366, 257, 857, 544, 10851, 813, 341, 13], "temperature": 0.0, "avg_logprob": -0.22567442351696537, "compression_ratio": 1.6177777777777778, "no_speech_prob": 4.425466613611206e-06}, {"id": 192, "seek": 97354, "start": 989.76, "end": 992.48, "text": " I think that's right.", "tokens": [286, 519, 300, 311, 558, 13], "temperature": 0.0, "avg_logprob": -0.22567442351696537, "compression_ratio": 1.6177777777777778, "no_speech_prob": 4.425466613611206e-06}, {"id": 193, "seek": 97354, "start": 992.48, "end": 998.7199999999999, "text": " Anyway, maybe I should double check before I say things that might not be true.", "tokens": [5684, 11, 1310, 286, 820, 3834, 1520, 949, 286, 584, 721, 300, 1062, 406, 312, 2074, 13], "temperature": 0.0, "avg_logprob": -0.22567442351696537, "compression_ratio": 1.6177777777777778, "no_speech_prob": 4.425466613611206e-06}, {"id": 194, "seek": 97354, "start": 998.7199999999999, "end": 999.7199999999999, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.22567442351696537, "compression_ratio": 1.6177777777777778, "no_speech_prob": 4.425466613611206e-06}, {"id": 195, "seek": 99972, "start": 999.72, "end": 1007.88, "text": " I'll do that where else you can do that and get exactly the expected behavior.", "tokens": [286, 603, 360, 300, 689, 1646, 291, 393, 360, 300, 293, 483, 2293, 264, 5176, 5223, 13], "temperature": 0.0, "avg_logprob": -0.20241318979570944, "compression_ratio": 1.4451219512195121, "no_speech_prob": 4.157343482802389e-06}, {"id": 196, "seek": 99972, "start": 1007.88, "end": 1015.96, "text": " And Ls, by the way, always show you how big they are before they print out their contents.", "tokens": [400, 441, 82, 11, 538, 264, 636, 11, 1009, 855, 291, 577, 955, 436, 366, 949, 436, 4482, 484, 641, 15768, 13], "temperature": 0.0, "avg_logprob": -0.20241318979570944, "compression_ratio": 1.4451219512195121, "no_speech_prob": 4.157343482802389e-06}, {"id": 197, "seek": 99972, "start": 1015.96, "end": 1025.3600000000001, "text": " You'll see that the allCols does not actually appear anywhere here.", "tokens": [509, 603, 536, 300, 264, 439, 34, 19385, 775, 406, 767, 4204, 4992, 510, 13], "temperature": 0.0, "avg_logprob": -0.20241318979570944, "compression_ratio": 1.4451219512195121, "no_speech_prob": 4.157343482802389e-06}, {"id": 198, "seek": 102536, "start": 1025.36, "end": 1029.8, "text": " And that's because we just created a little thing called addProperty that just adds cats", "tokens": [400, 300, 311, 570, 321, 445, 2942, 257, 707, 551, 1219, 909, 12681, 610, 874, 300, 445, 10860, 11111], "temperature": 0.0, "avg_logprob": -0.15086952520876515, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.1189371207365184e-06}, {"id": 199, "seek": 102536, "start": 1029.8, "end": 1033.04, "text": " or cats, consts or consts or cols.", "tokens": [420, 11111, 11, 1817, 82, 420, 1817, 82, 420, 1173, 82, 13], "temperature": 0.0, "avg_logprob": -0.15086952520876515, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.1189371207365184e-06}, {"id": 200, "seek": 102536, "start": 1033.04, "end": 1039.6399999999999, "text": " And so for each one, it creates a read version of the property, which is to just grab whatever,", "tokens": [400, 370, 337, 1184, 472, 11, 309, 7829, 257, 1401, 3037, 295, 264, 4707, 11, 597, 307, 281, 445, 4444, 2035, 11], "temperature": 0.0, "avg_logprob": -0.15086952520876515, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.1189371207365184e-06}, {"id": 201, "seek": 102536, "start": 1039.6399999999999, "end": 1046.1599999999999, "text": " cat names or cat names, cont names, et cetera, which are all defined in here, and then indexes", "tokens": [3857, 5288, 420, 3857, 5288, 11, 660, 5288, 11, 1030, 11458, 11, 597, 366, 439, 7642, 294, 510, 11, 293, 550, 8186, 279], "temperature": 0.0, "avg_logprob": -0.15086952520876515, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.1189371207365184e-06}, {"id": 202, "seek": 102536, "start": 1046.1599999999999, "end": 1051.3999999999999, "text": " into our data frame with that list of columns.", "tokens": [666, 527, 1412, 3920, 365, 300, 1329, 295, 13766, 13], "temperature": 0.0, "avg_logprob": -0.15086952520876515, "compression_ratio": 1.7028301886792452, "no_speech_prob": 1.1189371207365184e-06}, {"id": 203, "seek": 105140, "start": 1051.4, "end": 1061.0, "text": " And then it creates a setter, which simply sets that list of names to whatever value", "tokens": [400, 550, 309, 7829, 257, 992, 391, 11, 597, 2935, 6352, 300, 1329, 295, 5288, 281, 2035, 2158], "temperature": 0.0, "avg_logprob": -0.10056065350044065, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.0845142242033035e-06}, {"id": 204, "seek": 105140, "start": 1061.0, "end": 1062.0, "text": " you provide.", "tokens": [291, 2893, 13], "temperature": 0.0, "avg_logprob": -0.10056065350044065, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.0845142242033035e-06}, {"id": 205, "seek": 105140, "start": 1062.0, "end": 1067.72, "text": " So that's just a quick way to create the setter and get a versions of all of those.", "tokens": [407, 300, 311, 445, 257, 1702, 636, 281, 1884, 264, 992, 391, 293, 483, 257, 9606, 295, 439, 295, 729, 13], "temperature": 0.0, "avg_logprob": -0.10056065350044065, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.0845142242033035e-06}, {"id": 206, "seek": 105140, "start": 1067.72, "end": 1070.3200000000002, "text": " So that's where allCols come from.", "tokens": [407, 300, 311, 689, 439, 34, 19385, 808, 490, 13], "temperature": 0.0, "avg_logprob": -0.10056065350044065, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.0845142242033035e-06}, {"id": 207, "seek": 105140, "start": 1070.3200000000002, "end": 1078.1200000000001, "text": " So in this case, because the tabular object only has this one column mentioned as being", "tokens": [407, 294, 341, 1389, 11, 570, 264, 4421, 1040, 2657, 787, 575, 341, 472, 7738, 2835, 382, 885], "temperature": 0.0, "avg_logprob": -0.10056065350044065, "compression_ratio": 1.5510204081632653, "no_speech_prob": 1.0845142242033035e-06}, {"id": 208, "seek": 107812, "start": 1078.12, "end": 1084.9199999999998, "text": " part of what we're modeling, even though the data frame had an A and a B, tabular object", "tokens": [644, 295, 437, 321, 434, 15983, 11, 754, 1673, 264, 1412, 3920, 632, 364, 316, 293, 257, 363, 11, 4421, 1040, 2657], "temperature": 0.0, "avg_logprob": -0.1191106418986897, "compression_ratio": 1.5898617511520738, "no_speech_prob": 3.237738837924553e-06}, {"id": 209, "seek": 107812, "start": 1084.9199999999998, "end": 1090.2399999999998, "text": " at allCols only has the A column in it.", "tokens": [412, 439, 34, 19385, 787, 575, 264, 316, 7738, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.1191106418986897, "compression_ratio": 1.5898617511520738, "no_speech_prob": 3.237738837924553e-06}, {"id": 210, "seek": 107812, "start": 1090.2399999999998, "end": 1094.36, "text": " Because by allCols, it means all of the columns we're using in modeling, so continuous and", "tokens": [1436, 538, 439, 34, 19385, 11, 309, 1355, 439, 295, 264, 13766, 321, 434, 1228, 294, 15983, 11, 370, 10957, 293], "temperature": 0.0, "avg_logprob": -0.1191106418986897, "compression_ratio": 1.5898617511520738, "no_speech_prob": 3.237738837924553e-06}, {"id": 211, "seek": 107812, "start": 1094.36, "end": 1098.8799999999999, "text": " categorical and dependent variables.", "tokens": [19250, 804, 293, 12334, 9102, 13], "temperature": 0.0, "avg_logprob": -0.1191106418986897, "compression_ratio": 1.5898617511520738, "no_speech_prob": 3.237738837924553e-06}, {"id": 212, "seek": 107812, "start": 1098.8799999999999, "end": 1105.9199999999998, "text": " And so one of the nice things is that because everything is super consistent in the API,", "tokens": [400, 370, 472, 295, 264, 1481, 721, 307, 300, 570, 1203, 307, 1687, 8398, 294, 264, 9362, 11], "temperature": 0.0, "avg_logprob": -0.1191106418986897, "compression_ratio": 1.5898617511520738, "no_speech_prob": 3.237738837924553e-06}, {"id": 213, "seek": 110592, "start": 1105.92, "end": 1111.0, "text": " we can now just say dot show, just like everything else we can say dot show.", "tokens": [321, 393, 586, 445, 584, 5893, 855, 11, 445, 411, 1203, 1646, 321, 393, 584, 5893, 855, 13], "temperature": 0.0, "avg_logprob": -0.24742251309481533, "compression_ratio": 1.4210526315789473, "no_speech_prob": 1.8738636526904884e-06}, {"id": 214, "seek": 110592, "start": 1111.0, "end": 1118.96, "text": " And in this case, we see the allCols data frame.", "tokens": [400, 294, 341, 1389, 11, 321, 536, 264, 439, 34, 19385, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.24742251309481533, "compression_ratio": 1.4210526315789473, "no_speech_prob": 1.8738636526904884e-06}, {"id": 215, "seek": 110592, "start": 1118.96, "end": 1121.0800000000002, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.24742251309481533, "compression_ratio": 1.4210526315789473, "no_speech_prob": 1.8738636526904884e-06}, {"id": 216, "seek": 110592, "start": 1121.0800000000002, "end": 1129.92, "text": " So then processes, tabular processes are just transforms.", "tokens": [407, 550, 7555, 11, 4421, 1040, 7555, 366, 445, 35592, 13], "temperature": 0.0, "avg_logprob": -0.24742251309481533, "compression_ratio": 1.4210526315789473, "no_speech_prob": 1.8738636526904884e-06}, {"id": 217, "seek": 112992, "start": 1129.92, "end": 1136.48, "text": " Now, specifically they're in-place transforms, but don't let that bother you because in-place", "tokens": [823, 11, 4682, 436, 434, 294, 12, 6742, 35592, 11, 457, 500, 380, 718, 300, 8677, 291, 570, 294, 12, 6742], "temperature": 0.0, "avg_logprob": -0.18962327739860438, "compression_ratio": 1.6494845360824741, "no_speech_prob": 6.962224233575398e-06}, {"id": 218, "seek": 112992, "start": 1136.48, "end": 1144.48, "text": " transform is simply a transform where when we recall it and then we return the original", "tokens": [4088, 307, 2935, 257, 4088, 689, 562, 321, 9901, 309, 293, 550, 321, 2736, 264, 3380], "temperature": 0.0, "avg_logprob": -0.18962327739860438, "compression_ratio": 1.6494845360824741, "no_speech_prob": 6.962224233575398e-06}, {"id": 219, "seek": 112992, "start": 1144.48, "end": 1145.48, "text": " thing.", "tokens": [551, 13], "temperature": 0.0, "avg_logprob": -0.18962327739860438, "compression_ratio": 1.6494845360824741, "no_speech_prob": 6.962224233575398e-06}, {"id": 220, "seek": 112992, "start": 1145.48, "end": 1150.88, "text": " So like all the transforms we've seen so far return something different to what you passed", "tokens": [407, 411, 439, 264, 35592, 321, 600, 1612, 370, 1400, 2736, 746, 819, 281, 437, 291, 4678], "temperature": 0.0, "avg_logprob": -0.18962327739860438, "compression_ratio": 1.6494845360824741, "no_speech_prob": 6.962224233575398e-06}, {"id": 221, "seek": 112992, "start": 1150.88, "end": 1153.0, "text": " in, like that's the whole point of them.", "tokens": [294, 11, 411, 300, 311, 264, 1379, 935, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.18962327739860438, "compression_ratio": 1.6494845360824741, "no_speech_prob": 6.962224233575398e-06}, {"id": 222, "seek": 115300, "start": 1153.0, "end": 1160.24, "text": " But processes, the whole point of them is that they change the actual stored data, so", "tokens": [583, 7555, 11, 264, 1379, 935, 295, 552, 307, 300, 436, 1319, 264, 3539, 12187, 1412, 11, 370], "temperature": 0.0, "avg_logprob": -0.1435054665181174, "compression_ratio": 1.5614035087719298, "no_speech_prob": 1.136562218562176e-06}, {"id": 223, "seek": 115300, "start": 1160.24, "end": 1165.56, "text": " that's why they just return whatever you started with.", "tokens": [300, 311, 983, 436, 445, 2736, 2035, 291, 1409, 365, 13], "temperature": 0.0, "avg_logprob": -0.1435054665181174, "compression_ratio": 1.5614035087719298, "no_speech_prob": 1.136562218562176e-06}, {"id": 224, "seek": 115300, "start": 1165.56, "end": 1168.4, "text": " So that's all in-place transform means.", "tokens": [407, 300, 311, 439, 294, 12, 6742, 4088, 1355, 13], "temperature": 0.0, "avg_logprob": -0.1435054665181174, "compression_ratio": 1.5614035087719298, "no_speech_prob": 1.136562218562176e-06}, {"id": 225, "seek": 115300, "start": 1168.4, "end": 1176.28, "text": " And so a tabular processor is just a transform that returns itself when you recall it.", "tokens": [400, 370, 257, 4421, 1040, 15321, 307, 445, 257, 4088, 300, 11247, 2564, 562, 291, 9901, 309, 13], "temperature": 0.0, "avg_logprob": -0.1435054665181174, "compression_ratio": 1.5614035087719298, "no_speech_prob": 1.136562218562176e-06}, {"id": 226, "seek": 117628, "start": 1176.28, "end": 1186.0, "text": " And when you set it up, it just does the normal setup, but it also calls done to call, in", "tokens": [400, 562, 291, 992, 309, 493, 11, 309, 445, 775, 264, 2710, 8657, 11, 457, 309, 611, 5498, 1096, 281, 818, 11, 294], "temperature": 0.0, "avg_logprob": -0.26607961539762565, "compression_ratio": 1.5303867403314917, "no_speech_prob": 1.6797247326394427e-06}, {"id": 227, "seek": 117628, "start": 1186.0, "end": 1187.0, "text": " other words, self round-accords.", "tokens": [661, 2283, 11, 2698, 3098, 12, 8476, 5703, 13], "temperature": 0.0, "avg_logprob": -0.26607961539762565, "compression_ratio": 1.5303867403314917, "no_speech_prob": 1.6797247326394427e-06}, {"id": 228, "seek": 117628, "start": 1187.0, "end": 1190.0, "text": " Why does it do that?", "tokens": [1545, 775, 309, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.26607961539762565, "compression_ratio": 1.5303867403314917, "no_speech_prob": 1.6797247326394427e-06}, {"id": 229, "seek": 117628, "start": 1190.0, "end": 1194.36, "text": " Well, let's take a look at an example, categorify.", "tokens": [1042, 11, 718, 311, 747, 257, 574, 412, 364, 1365, 11, 19250, 2505, 13], "temperature": 0.0, "avg_logprob": -0.26607961539762565, "compression_ratio": 1.5303867403314917, "no_speech_prob": 1.6797247326394427e-06}, {"id": 230, "seek": 117628, "start": 1194.36, "end": 1204.04, "text": " So categorify is a tabular proc where the setup is going to create a category map.", "tokens": [407, 19250, 2505, 307, 257, 4421, 1040, 9510, 689, 264, 8657, 307, 516, 281, 1884, 257, 7719, 4471, 13], "temperature": 0.0, "avg_logprob": -0.26607961539762565, "compression_ratio": 1.5303867403314917, "no_speech_prob": 1.6797247326394427e-06}, {"id": 231, "seek": 120404, "start": 1204.04, "end": 1210.68, "text": " So this is just a mapping from int numbered bits of vocab to string vocab.", "tokens": [407, 341, 307, 445, 257, 18350, 490, 560, 40936, 9239, 295, 2329, 455, 281, 6798, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.1757626874106271, "compression_ratio": 1.7325581395348837, "no_speech_prob": 2.368787818340934e-06}, {"id": 232, "seek": 120404, "start": 1210.68, "end": 1214.08, "text": " That's what a category map is.", "tokens": [663, 311, 437, 257, 7719, 4471, 307, 13], "temperature": 0.0, "avg_logprob": -0.1757626874106271, "compression_ratio": 1.7325581395348837, "no_speech_prob": 2.368787818340934e-06}, {"id": 233, "seek": 120404, "start": 1214.08, "end": 1224.08, "text": " And so it's going to go through all of the categorical columns and it's going to go.iloc", "tokens": [400, 370, 309, 311, 516, 281, 352, 807, 439, 295, 264, 19250, 804, 13766, 293, 309, 311, 516, 281, 352, 2411, 388, 905], "temperature": 0.0, "avg_logprob": -0.1757626874106271, "compression_ratio": 1.7325581395348837, "no_speech_prob": 2.368787818340934e-06}, {"id": 234, "seek": 120404, "start": 1224.08, "end": 1228.72, "text": " into the data frame for each of those columns.", "tokens": [666, 264, 1412, 3920, 337, 1184, 295, 729, 13766, 13], "temperature": 0.0, "avg_logprob": -0.1757626874106271, "compression_ratio": 1.7325581395348837, "no_speech_prob": 2.368787818340934e-06}, {"id": 235, "seek": 120404, "start": 1228.72, "end": 1232.3799999999999, "text": " And it's going to create a category map for that column.", "tokens": [400, 309, 311, 516, 281, 1884, 257, 7719, 4471, 337, 300, 7738, 13], "temperature": 0.0, "avg_logprob": -0.1757626874106271, "compression_ratio": 1.7325581395348837, "no_speech_prob": 2.368787818340934e-06}, {"id": 236, "seek": 123238, "start": 1232.38, "end": 1237.6200000000001, "text": " So this is just creating, so self.classes then is going to be a dictionary that goes", "tokens": [407, 341, 307, 445, 4084, 11, 370, 2698, 13, 11665, 279, 550, 307, 516, 281, 312, 257, 25890, 300, 1709], "temperature": 0.0, "avg_logprob": -0.13670466412072893, "compression_ratio": 1.6476683937823835, "no_speech_prob": 9.874611350824125e-07}, {"id": 237, "seek": 123238, "start": 1237.6200000000001, "end": 1242.6000000000001, "text": " from the column names to the vocab for that categorical column.", "tokens": [490, 264, 7738, 5288, 281, 264, 2329, 455, 337, 300, 19250, 804, 7738, 13], "temperature": 0.0, "avg_logprob": -0.13670466412072893, "compression_ratio": 1.6476683937823835, "no_speech_prob": 9.874611350824125e-07}, {"id": 238, "seek": 123238, "start": 1242.6000000000001, "end": 1244.3600000000001, "text": " So that's what setup does.", "tokens": [407, 300, 311, 437, 8657, 775, 13], "temperature": 0.0, "avg_logprob": -0.13670466412072893, "compression_ratio": 1.6476683937823835, "no_speech_prob": 9.874611350824125e-07}, {"id": 239, "seek": 123238, "start": 1244.3600000000001, "end": 1249.64, "text": " So setup kind of sets up the metadata, it's the vocab.", "tokens": [407, 8657, 733, 295, 6352, 493, 264, 26603, 11, 309, 311, 264, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.13670466412072893, "compression_ratio": 1.6476683937823835, "no_speech_prob": 9.874611350824125e-07}, {"id": 240, "seek": 123238, "start": 1249.64, "end": 1256.7600000000002, "text": " And codes, on the other hand, is the thing that actually takes a categorical column and", "tokens": [400, 14211, 11, 322, 264, 661, 1011, 11, 307, 264, 551, 300, 767, 2516, 257, 19250, 804, 7738, 293], "temperature": 0.0, "avg_logprob": -0.13670466412072893, "compression_ratio": 1.6476683937823835, "no_speech_prob": 9.874611350824125e-07}, {"id": 241, "seek": 125676, "start": 1256.76, "end": 1262.96, "text": " converts it into ints using the vocab that we created earlier.", "tokens": [38874, 309, 666, 560, 82, 1228, 264, 2329, 455, 300, 321, 2942, 3071, 13], "temperature": 0.0, "avg_logprob": -0.1591259765625, "compression_ratio": 1.7048192771084338, "no_speech_prob": 9.276340051656007e-07}, {"id": 242, "seek": 125676, "start": 1262.96, "end": 1268.6, "text": " And so we need them to be two separate things because if you think about inference, at inference", "tokens": [400, 370, 321, 643, 552, 281, 312, 732, 4994, 721, 570, 498, 291, 519, 466, 38253, 11, 412, 38253], "temperature": 0.0, "avg_logprob": -0.1591259765625, "compression_ratio": 1.7048192771084338, "no_speech_prob": 9.276340051656007e-07}, {"id": 243, "seek": 125676, "start": 1268.6, "end": 1272.64, "text": " time you don't want to run setup.", "tokens": [565, 291, 500, 380, 528, 281, 1190, 8657, 13], "temperature": 0.0, "avg_logprob": -0.1591259765625, "compression_ratio": 1.7048192771084338, "no_speech_prob": 9.276340051656007e-07}, {"id": 244, "seek": 125676, "start": 1272.64, "end": 1276.96, "text": " At inference time you just want to run encodes.", "tokens": [1711, 38253, 565, 291, 445, 528, 281, 1190, 2058, 4789, 13], "temperature": 0.0, "avg_logprob": -0.1591259765625, "compression_ratio": 1.7048192771084338, "no_speech_prob": 9.276340051656007e-07}, {"id": 245, "seek": 125676, "start": 1276.96, "end": 1281.92, "text": " But at training time you want to do both.", "tokens": [583, 412, 3097, 565, 291, 528, 281, 360, 1293, 13], "temperature": 0.0, "avg_logprob": -0.1591259765625, "compression_ratio": 1.7048192771084338, "no_speech_prob": 9.276340051656007e-07}, {"id": 246, "seek": 128192, "start": 1281.92, "end": 1286.74, "text": " Anytime you don't do setup, you're definitely also going to want to process.", "tokens": [39401, 291, 500, 380, 360, 8657, 11, 291, 434, 2138, 611, 516, 281, 528, 281, 1399, 13], "temperature": 0.0, "avg_logprob": -0.15821976912649055, "compression_ratio": 1.6826923076923077, "no_speech_prob": 1.994724243559176e-06}, {"id": 247, "seek": 128192, "start": 1286.74, "end": 1294.74, "text": " So that's why setup, after it sets up, immediately does the encoding because that's in practice", "tokens": [407, 300, 311, 983, 8657, 11, 934, 309, 6352, 493, 11, 4258, 775, 264, 43430, 570, 300, 311, 294, 3124], "temperature": 0.0, "avg_logprob": -0.15821976912649055, "compression_ratio": 1.6826923076923077, "no_speech_prob": 1.994724243559176e-06}, {"id": 248, "seek": 128192, "start": 1294.74, "end": 1297.52, "text": " always what you want.", "tokens": [1009, 437, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.15821976912649055, "compression_ratio": 1.6826923076923077, "no_speech_prob": 1.994724243559176e-06}, {"id": 249, "seek": 128192, "start": 1297.52, "end": 1300.92, "text": " So that's why we override setup in tabular procs.", "tokens": [407, 300, 311, 983, 321, 42321, 8657, 294, 4421, 1040, 9510, 82, 13], "temperature": 0.0, "avg_logprob": -0.15821976912649055, "compression_ratio": 1.6826923076923077, "no_speech_prob": 1.994724243559176e-06}, {"id": 250, "seek": 128192, "start": 1300.92, "end": 1301.92, "text": " That's all tabular proc is.", "tokens": [663, 311, 439, 4421, 1040, 9510, 307, 13], "temperature": 0.0, "avg_logprob": -0.15821976912649055, "compression_ratio": 1.6826923076923077, "no_speech_prob": 1.994724243559176e-06}, {"id": 251, "seek": 128192, "start": 1301.92, "end": 1310.0, "text": " It's just a transform that when you set it up it also calls it straight away.", "tokens": [467, 311, 445, 257, 4088, 300, 562, 291, 992, 309, 493, 309, 611, 5498, 309, 2997, 1314, 13], "temperature": 0.0, "avg_logprob": -0.15821976912649055, "compression_ratio": 1.6826923076923077, "no_speech_prob": 1.994724243559176e-06}, {"id": 252, "seek": 131000, "start": 1310.0, "end": 1321.84, "text": " So that, so categorify is pretty similar to the categorized transform we've seen for dependent", "tokens": [407, 300, 11, 370, 19250, 2505, 307, 1238, 2531, 281, 264, 19250, 1602, 4088, 321, 600, 1612, 337, 12334], "temperature": 0.0, "avg_logprob": -0.23817098593410058, "compression_ratio": 1.461139896373057, "no_speech_prob": 8.315267336911347e-07}, {"id": 253, "seek": 131000, "start": 1321.84, "end": 1328.28, "text": " variables for image classification, but it's a little bit different because it's for tabular", "tokens": [9102, 337, 3256, 21538, 11, 457, 309, 311, 257, 707, 857, 819, 570, 309, 311, 337, 4421, 1040], "temperature": 0.0, "avg_logprob": -0.23817098593410058, "compression_ratio": 1.461139896373057, "no_speech_prob": 8.315267336911347e-07}, {"id": 254, "seek": 131000, "start": 1328.28, "end": 1329.9, "text": " objects.", "tokens": [6565, 13], "temperature": 0.0, "avg_logprob": -0.23817098593410058, "compression_ratio": 1.461139896373057, "no_speech_prob": 8.315267336911347e-07}, {"id": 255, "seek": 131000, "start": 1329.9, "end": 1332.84, "text": " So you can see an example.", "tokens": [407, 291, 393, 536, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.23817098593410058, "compression_ratio": 1.461139896373057, "no_speech_prob": 8.315267336911347e-07}, {"id": 256, "seek": 131000, "start": 1332.84, "end": 1337.8, "text": " Here's a data frame, R1, R2, R2, and just a single column.", "tokens": [1692, 311, 257, 1412, 3920, 11, 497, 16, 11, 497, 17, 11, 497, 17, 11, 293, 445, 257, 2167, 7738, 13], "temperature": 0.0, "avg_logprob": -0.23817098593410058, "compression_ratio": 1.461139896373057, "no_speech_prob": 8.315267336911347e-07}, {"id": 257, "seek": 133780, "start": 1337.8, "end": 1343.32, "text": " So again we can create a tabular object, passing in the data frame, passing in any processes", "tokens": [407, 797, 321, 393, 1884, 257, 4421, 1040, 2657, 11, 8437, 294, 264, 1412, 3920, 11, 8437, 294, 604, 7555], "temperature": 0.0, "avg_logprob": -0.12774073160611665, "compression_ratio": 1.8558139534883722, "no_speech_prob": 3.6119524793321034e-06}, {"id": 258, "seek": 133780, "start": 1343.32, "end": 1349.68, "text": " we want to run, and passing in, so the first thing that we pass in after that will be the", "tokens": [321, 528, 281, 1190, 11, 293, 8437, 294, 11, 370, 264, 700, 551, 300, 321, 1320, 294, 934, 300, 486, 312, 264], "temperature": 0.0, "avg_logprob": -0.12774073160611665, "compression_ratio": 1.8558139534883722, "no_speech_prob": 3.6119524793321034e-06}, {"id": 259, "seek": 133780, "start": 1349.68, "end": 1355.32, "text": " category names, the categorical names, so we're just going to have one.", "tokens": [7719, 5288, 11, 264, 19250, 804, 5288, 11, 370, 321, 434, 445, 516, 281, 362, 472, 13], "temperature": 0.0, "avg_logprob": -0.12774073160611665, "compression_ratio": 1.8558139534883722, "no_speech_prob": 3.6119524793321034e-06}, {"id": 260, "seek": 133780, "start": 1355.32, "end": 1363.12, "text": " So once you have created your tabular object, the next thing you want to do is to call setup.", "tokens": [407, 1564, 291, 362, 2942, 428, 4421, 1040, 2657, 11, 264, 958, 551, 291, 528, 281, 360, 307, 281, 818, 8657, 13], "temperature": 0.0, "avg_logprob": -0.12774073160611665, "compression_ratio": 1.8558139534883722, "no_speech_prob": 3.6119524793321034e-06}, {"id": 261, "seek": 133780, "start": 1363.12, "end": 1364.9199999999998, "text": " And remember that setup is going to do two things.", "tokens": [400, 1604, 300, 8657, 307, 516, 281, 360, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.12774073160611665, "compression_ratio": 1.8558139534883722, "no_speech_prob": 3.6119524793321034e-06}, {"id": 262, "seek": 136492, "start": 1364.92, "end": 1375.68, "text": " It's going to call your setups and it's going to call your encodes.", "tokens": [467, 311, 516, 281, 818, 428, 46832, 293, 309, 311, 516, 281, 818, 428, 2058, 4789, 13], "temperature": 0.0, "avg_logprob": -0.20843833532088843, "compression_ratio": 1.5909090909090908, "no_speech_prob": 2.4824560114211636e-06}, {"id": 263, "seek": 136492, "start": 1375.68, "end": 1377.8000000000002, "text": " Are there any object detectors?", "tokens": [2014, 456, 604, 2657, 46866, 30], "temperature": 0.0, "avg_logprob": -0.20843833532088843, "compression_ratio": 1.5909090909090908, "no_speech_prob": 2.4824560114211636e-06}, {"id": 264, "seek": 136492, "start": 1377.8000000000002, "end": 1379.72, "text": " We haven't done any models yet, David.", "tokens": [492, 2378, 380, 1096, 604, 5245, 1939, 11, 4389, 13], "temperature": 0.0, "avg_logprob": -0.20843833532088843, "compression_ratio": 1.5909090909090908, "no_speech_prob": 2.4824560114211636e-06}, {"id": 265, "seek": 136492, "start": 1379.72, "end": 1383.68, "text": " We're only really working through the transformation pipeline, but we have certainly looked at a", "tokens": [492, 434, 787, 534, 1364, 807, 264, 9887, 15517, 11, 457, 321, 362, 3297, 2956, 412, 257], "temperature": 0.0, "avg_logprob": -0.20843833532088843, "compression_ratio": 1.5909090909090908, "no_speech_prob": 2.4824560114211636e-06}, {"id": 266, "seek": 136492, "start": 1383.68, "end": 1390.76, "text": " version two object detectors in previous lessons for like, well, not in detail.", "tokens": [3037, 732, 2657, 46866, 294, 3894, 8820, 337, 411, 11, 731, 11, 406, 294, 2607, 13], "temperature": 0.0, "avg_logprob": -0.20843833532088843, "compression_ratio": 1.5909090909090908, "no_speech_prob": 2.4824560114211636e-06}, {"id": 267, "seek": 139076, "start": 1390.76, "end": 1399.72, "text": " We've touched on the places where the object detection data is defined and yeah, hopefully", "tokens": [492, 600, 9828, 322, 264, 3190, 689, 264, 2657, 17784, 1412, 307, 7642, 293, 1338, 11, 4696], "temperature": 0.0, "avg_logprob": -0.3470558315128475, "compression_ratio": 1.5027027027027027, "no_speech_prob": 4.28922339779092e-06}, {"id": 268, "seek": 139076, "start": 1399.72, "end": 1407.56, "text": " it's clear enough that you can figure that out.", "tokens": [309, 311, 1850, 1547, 300, 291, 393, 2573, 300, 484, 13], "temperature": 0.0, "avg_logprob": -0.3470558315128475, "compression_ratio": 1.5027027027027027, "no_speech_prob": 4.28922339779092e-06}, {"id": 269, "seek": 139076, "start": 1407.56, "end": 1412.24, "text": " So patch property does not call add prop.", "tokens": [407, 9972, 4707, 775, 406, 818, 909, 2365, 13], "temperature": 0.0, "avg_logprob": -0.3470558315128475, "compression_ratio": 1.5027027027027027, "no_speech_prob": 4.28922339779092e-06}, {"id": 270, "seek": 139076, "start": 1412.24, "end": 1414.64, "text": " I mean, you can check the code easily enough yourself, right?", "tokens": [286, 914, 11, 291, 393, 1520, 264, 3089, 3612, 1547, 1803, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.3470558315128475, "compression_ratio": 1.5027027027027027, "no_speech_prob": 4.28922339779092e-06}, {"id": 271, "seek": 139076, "start": 1414.64, "end": 1416.64, "text": " So patch property.", "tokens": [407, 9972, 4707, 13], "temperature": 0.0, "avg_logprob": -0.3470558315128475, "compression_ratio": 1.5027027027027027, "no_speech_prob": 4.28922339779092e-06}, {"id": 272, "seek": 139076, "start": 1416.64, "end": 1418.44, "text": " Let's see.", "tokens": [961, 311, 536, 13], "temperature": 0.0, "avg_logprob": -0.3470558315128475, "compression_ratio": 1.5027027027027027, "no_speech_prob": 4.28922339779092e-06}, {"id": 273, "seek": 139076, "start": 1418.44, "end": 1419.44, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3470558315128475, "compression_ratio": 1.5027027027027027, "no_speech_prob": 4.28922339779092e-06}, {"id": 274, "seek": 141944, "start": 1419.44, "end": 1421.44, "text": " So no, it doesn't.", "tokens": [407, 572, 11, 309, 1177, 380, 13], "temperature": 0.0, "avg_logprob": -0.2603185912709177, "compression_ratio": 1.4593023255813953, "no_speech_prob": 3.1380780001200037e-06}, {"id": 275, "seek": 141944, "start": 1421.44, "end": 1423.56, "text": " It calls patch two.", "tokens": [467, 5498, 9972, 732, 13], "temperature": 0.0, "avg_logprob": -0.2603185912709177, "compression_ratio": 1.4593023255813953, "no_speech_prob": 3.1380780001200037e-06}, {"id": 276, "seek": 141944, "start": 1423.56, "end": 1427.44, "text": " No, it doesn't.", "tokens": [883, 11, 309, 1177, 380, 13], "temperature": 0.0, "avg_logprob": -0.2603185912709177, "compression_ratio": 1.4593023255813953, "no_speech_prob": 3.1380780001200037e-06}, {"id": 277, "seek": 141944, "start": 1427.44, "end": 1437.16, "text": " So there's no way to create a setup using that decorator yet.", "tokens": [407, 456, 311, 572, 636, 281, 1884, 257, 8657, 1228, 300, 7919, 1639, 1939, 13], "temperature": 0.0, "avg_logprob": -0.2603185912709177, "compression_ratio": 1.4593023255813953, "no_speech_prob": 3.1380780001200037e-06}, {"id": 278, "seek": 141944, "start": 1437.16, "end": 1440.2, "text": " If you can think of a good syntax for that that you think would work well, feel free", "tokens": [759, 291, 393, 519, 295, 257, 665, 28431, 337, 300, 300, 291, 519, 576, 589, 731, 11, 841, 1737], "temperature": 0.0, "avg_logprob": -0.2603185912709177, "compression_ratio": 1.4593023255813953, "no_speech_prob": 3.1380780001200037e-06}, {"id": 279, "seek": 141944, "start": 1440.2, "end": 1445.56, "text": " to suggest it or even implement it in a PR.", "tokens": [281, 3402, 309, 420, 754, 4445, 309, 294, 257, 11568, 13], "temperature": 0.0, "avg_logprob": -0.2603185912709177, "compression_ratio": 1.4593023255813953, "no_speech_prob": 3.1380780001200037e-06}, {"id": 280, "seek": 141944, "start": 1445.56, "end": 1446.88, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2603185912709177, "compression_ratio": 1.4593023255813953, "no_speech_prob": 3.1380780001200037e-06}, {"id": 281, "seek": 144688, "start": 1446.88, "end": 1452.8400000000001, "text": " So all right, so we've created our tabular object.", "tokens": [407, 439, 558, 11, 370, 321, 600, 2942, 527, 4421, 1040, 2657, 13], "temperature": 0.0, "avg_logprob": -0.24386383651138901, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.2878867892140988e-06}, {"id": 282, "seek": 144688, "start": 1452.8400000000001, "end": 1457.7600000000002, "text": " As we told it to categorify, we said this is our only categorical variable.", "tokens": [1018, 321, 1907, 309, 281, 19250, 2505, 11, 321, 848, 341, 307, 527, 787, 19250, 804, 7006, 13], "temperature": 0.0, "avg_logprob": -0.24386383651138901, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.2878867892140988e-06}, {"id": 283, "seek": 144688, "start": 1457.7600000000002, "end": 1465.92, "text": " Then we call setup, which is going to go ahead and let's have a look.", "tokens": [1396, 321, 818, 8657, 11, 597, 307, 516, 281, 352, 2286, 293, 718, 311, 362, 257, 574, 13], "temperature": 0.0, "avg_logprob": -0.24386383651138901, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.2878867892140988e-06}, {"id": 284, "seek": 144688, "start": 1465.92, "end": 1476.0400000000002, "text": " Setup is going to in our processor create classes and then it's going to change our", "tokens": [8928, 1010, 307, 516, 281, 294, 527, 15321, 1884, 5359, 293, 550, 309, 311, 516, 281, 1319, 527], "temperature": 0.0, "avg_logprob": -0.24386383651138901, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.2878867892140988e-06}, {"id": 285, "seek": 147604, "start": 1476.04, "end": 1491.62, "text": " columns to call apply cats, which will map the data in that column using this dictionary.", "tokens": [13766, 281, 818, 3079, 11111, 11, 597, 486, 4471, 264, 1412, 294, 300, 7738, 1228, 341, 25890, 13], "temperature": 0.0, "avg_logprob": -0.1037880492536989, "compression_ratio": 1.5977011494252873, "no_speech_prob": 3.6119597552897176e-06}, {"id": 286, "seek": 147604, "start": 1491.62, "end": 1496.08, "text": " So map is a interesting pandas method.", "tokens": [407, 4471, 307, 257, 1880, 4565, 296, 3170, 13], "temperature": 0.0, "avg_logprob": -0.1037880492536989, "compression_ratio": 1.5977011494252873, "no_speech_prob": 3.6119597552897176e-06}, {"id": 287, "seek": 147604, "start": 1496.08, "end": 1499.56, "text": " You can pass it a function, which is going to be super slow.", "tokens": [509, 393, 1320, 309, 257, 2445, 11, 597, 307, 516, 281, 312, 1687, 2964, 13], "temperature": 0.0, "avg_logprob": -0.1037880492536989, "compression_ratio": 1.5977011494252873, "no_speech_prob": 3.6119597552897176e-06}, {"id": 288, "seek": 147604, "start": 1499.56, "end": 1504.6, "text": " So don't do that, but you can also pass it a dictionary and that will just map from keys", "tokens": [407, 500, 380, 360, 300, 11, 457, 291, 393, 611, 1320, 309, 257, 25890, 293, 300, 486, 445, 4471, 490, 9317], "temperature": 0.0, "avg_logprob": -0.1037880492536989, "compression_ratio": 1.5977011494252873, "no_speech_prob": 3.6119597552897176e-06}, {"id": 289, "seek": 150460, "start": 1504.6, "end": 1507.76, "text": " to values in the dictionary.", "tokens": [281, 4190, 294, 264, 25890, 13], "temperature": 0.0, "avg_logprob": -0.19184891627385067, "compression_ratio": 1.4137931034482758, "no_speech_prob": 3.1875454169494333e-06}, {"id": 290, "seek": 150460, "start": 1507.76, "end": 1521.32, "text": " So if we have a look at this case, we can see that two dot a starts out as 01202 and", "tokens": [407, 498, 321, 362, 257, 574, 412, 341, 1389, 11, 321, 393, 536, 300, 732, 5893, 257, 3719, 484, 382, 1958, 4762, 12756, 293], "temperature": 0.0, "avg_logprob": -0.19184891627385067, "compression_ratio": 1.4137931034482758, "no_speech_prob": 3.1875454169494333e-06}, {"id": 291, "seek": 150460, "start": 1521.32, "end": 1524.6399999999999, "text": " ends up as 12313.", "tokens": [5314, 493, 382, 34466, 7668, 13], "temperature": 0.0, "avg_logprob": -0.19184891627385067, "compression_ratio": 1.4137931034482758, "no_speech_prob": 3.1875454169494333e-06}, {"id": 292, "seek": 150460, "start": 1524.6399999999999, "end": 1533.32, "text": " And the reason for that is that the vocab that it created is hash NA 012.", "tokens": [400, 264, 1778, 337, 300, 307, 300, 264, 2329, 455, 300, 309, 2942, 307, 22019, 16585, 1958, 4762, 13], "temperature": 0.0, "avg_logprob": -0.19184891627385067, "compression_ratio": 1.4137931034482758, "no_speech_prob": 3.1875454169494333e-06}, {"id": 293, "seek": 153332, "start": 1533.32, "end": 1541.6799999999998, "text": " So whenever we create a categorified column for the vocab, we always put a NA at the start,", "tokens": [407, 5699, 321, 1884, 257, 19250, 2587, 7738, 337, 264, 2329, 455, 11, 321, 1009, 829, 257, 16585, 412, 264, 722, 11], "temperature": 0.0, "avg_logprob": -0.14379703005154928, "compression_ratio": 1.603864734299517, "no_speech_prob": 1.8448020000505494e-06}, {"id": 294, "seek": 153332, "start": 1541.6799999999998, "end": 1544.8, "text": " which is similar to what we've done in version one.", "tokens": [597, 307, 2531, 281, 437, 321, 600, 1096, 294, 3037, 472, 13], "temperature": 0.0, "avg_logprob": -0.14379703005154928, "compression_ratio": 1.603864734299517, "no_speech_prob": 1.8448020000505494e-06}, {"id": 295, "seek": 153332, "start": 1544.8, "end": 1552.72, "text": " So that way, if you in the future get a value outside of your vocab, then that's what we're", "tokens": [407, 300, 636, 11, 498, 291, 294, 264, 2027, 483, 257, 2158, 2380, 295, 428, 2329, 455, 11, 550, 300, 311, 437, 321, 434], "temperature": 0.0, "avg_logprob": -0.14379703005154928, "compression_ratio": 1.603864734299517, "no_speech_prob": 1.8448020000505494e-06}, {"id": 296, "seek": 153332, "start": 1552.72, "end": 1555.12, "text": " going to set it to NA.", "tokens": [516, 281, 992, 309, 281, 16585, 13], "temperature": 0.0, "avg_logprob": -0.14379703005154928, "compression_ratio": 1.603864734299517, "no_speech_prob": 1.8448020000505494e-06}, {"id": 297, "seek": 153332, "start": 1555.12, "end": 1561.36, "text": " And so therefore one, if we index that into the vocab, that maps to zero.", "tokens": [400, 370, 4412, 472, 11, 498, 321, 8186, 300, 666, 264, 2329, 455, 11, 300, 11317, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.14379703005154928, "compression_ratio": 1.603864734299517, "no_speech_prob": 1.8448020000505494e-06}, {"id": 298, "seek": 156136, "start": 1561.36, "end": 1568.6799999999998, "text": " So that's why this is zero became one example.", "tokens": [407, 300, 311, 983, 341, 307, 4018, 3062, 472, 1365, 13], "temperature": 0.0, "avg_logprob": -0.24184661923032819, "compression_ratio": 1.5460526315789473, "no_speech_prob": 1.7603360902285203e-06}, {"id": 299, "seek": 156136, "start": 1568.6799999999998, "end": 1577.56, "text": " One of the things that I recently added to pipeline, because remember that two dot prox", "tokens": [1485, 295, 264, 721, 300, 286, 3938, 3869, 281, 15517, 11, 570, 1604, 300, 732, 5893, 447, 87], "temperature": 0.0, "avg_logprob": -0.24184661923032819, "compression_ratio": 1.5460526315789473, "no_speech_prob": 1.7603360902285203e-06}, {"id": 300, "seek": 156136, "start": 1577.56, "end": 1579.4199999999998, "text": " is a pipeline.", "tokens": [307, 257, 15517, 13], "temperature": 0.0, "avg_logprob": -0.24184661923032819, "compression_ratio": 1.5460526315789473, "no_speech_prob": 1.7603360902285203e-06}, {"id": 301, "seek": 156136, "start": 1579.4199999999998, "end": 1587.78, "text": " And one of the things I added to pipeline is a getAtra, which will try to, so this is", "tokens": [400, 472, 295, 264, 721, 286, 3869, 281, 15517, 307, 257, 483, 18684, 424, 11, 597, 486, 853, 281, 11, 370, 341, 307], "temperature": 0.0, "avg_logprob": -0.24184661923032819, "compression_ratio": 1.5460526315789473, "no_speech_prob": 1.7603360902285203e-06}, {"id": 302, "seek": 158778, "start": 1587.78, "end": 1592.0, "text": " not an attribute in pipeline, not surprisingly.", "tokens": [406, 364, 19667, 294, 15517, 11, 406, 17600, 13], "temperature": 0.0, "avg_logprob": -0.155896029894865, "compression_ratio": 1.6898395721925135, "no_speech_prob": 1.0511461141504697e-06}, {"id": 303, "seek": 158778, "start": 1592.0, "end": 1597.3999999999999, "text": " So if it finds an attribute it doesn't understand, it will try and find that attribute in any", "tokens": [407, 498, 309, 10704, 364, 19667, 309, 1177, 380, 1223, 11, 309, 486, 853, 293, 915, 300, 19667, 294, 604], "temperature": 0.0, "avg_logprob": -0.155896029894865, "compression_ratio": 1.6898395721925135, "no_speech_prob": 1.0511461141504697e-06}, {"id": 304, "seek": 158778, "start": 1597.3999999999999, "end": 1606.84, "text": " transforms inside that pipeline, which is exactly what we want.", "tokens": [35592, 1854, 300, 15517, 11, 597, 307, 2293, 437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.155896029894865, "compression_ratio": 1.6898395721925135, "no_speech_prob": 1.0511461141504697e-06}, {"id": 305, "seek": 158778, "start": 1606.84, "end": 1615.04, "text": " So in this case, it's going to look for a transform with a type categorified and it", "tokens": [407, 294, 341, 1389, 11, 309, 311, 516, 281, 574, 337, 257, 4088, 365, 257, 2010, 19250, 2587, 293, 309], "temperature": 0.0, "avg_logprob": -0.155896029894865, "compression_ratio": 1.6898395721925135, "no_speech_prob": 1.0511461141504697e-06}, {"id": 306, "seek": 158778, "start": 1615.04, "end": 1616.72, "text": " converts it to snake case.", "tokens": [38874, 309, 281, 12650, 1389, 13], "temperature": 0.0, "avg_logprob": -0.155896029894865, "compression_ratio": 1.6898395721925135, "no_speech_prob": 1.0511461141504697e-06}, {"id": 307, "seek": 161672, "start": 1616.72, "end": 1620.88, "text": " This is very similar if you've watched the part two, the most recent part two videos,", "tokens": [639, 307, 588, 2531, 498, 291, 600, 6337, 264, 644, 732, 11, 264, 881, 5162, 644, 732, 2145, 11], "temperature": 0.0, "avg_logprob": -0.1784101292706918, "compression_ratio": 1.8357142857142856, "no_speech_prob": 5.42218276677886e-06}, {"id": 308, "seek": 161672, "start": 1620.88, "end": 1624.28, "text": " we did the same thing for callbacks.", "tokens": [321, 630, 264, 912, 551, 337, 818, 17758, 13], "temperature": 0.0, "avg_logprob": -0.1784101292706918, "compression_ratio": 1.8357142857142856, "no_speech_prob": 5.42218276677886e-06}, {"id": 309, "seek": 161672, "start": 1624.28, "end": 1625.28, "text": " Callbacks got automatically inserted.", "tokens": [7807, 17758, 658, 6772, 27992, 13], "temperature": 0.0, "avg_logprob": -0.1784101292706918, "compression_ratio": 1.8357142857142856, "no_speech_prob": 5.42218276677886e-06}, {"id": 310, "seek": 161672, "start": 1625.28, "end": 1630.92, "text": " And I think version one does this too, get automatically added as attributes.", "tokens": [400, 286, 519, 3037, 472, 775, 341, 886, 11, 483, 6772, 3869, 382, 17212, 13], "temperature": 0.0, "avg_logprob": -0.1784101292706918, "compression_ratio": 1.8357142857142856, "no_speech_prob": 5.42218276677886e-06}, {"id": 311, "seek": 161672, "start": 1630.92, "end": 1634.3600000000001, "text": " So pipeline does something very similar, but it doesn't actually add them as attributes.", "tokens": [407, 15517, 775, 746, 588, 2531, 11, 457, 309, 1177, 380, 767, 909, 552, 382, 17212, 13], "temperature": 0.0, "avg_logprob": -0.1784101292706918, "compression_ratio": 1.8357142857142856, "no_speech_prob": 5.42218276677886e-06}, {"id": 312, "seek": 161672, "start": 1634.3600000000001, "end": 1637.92, "text": " It uses getAtra to do the same thing.", "tokens": [467, 4960, 483, 18684, 424, 281, 360, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.1784101292706918, "compression_ratio": 1.8357142857142856, "no_speech_prob": 5.42218276677886e-06}, {"id": 313, "seek": 161672, "start": 1637.92, "end": 1641.52, "text": " So in this case, we added a categorify transform.", "tokens": [407, 294, 341, 1389, 11, 321, 3869, 257, 19250, 2505, 4088, 13], "temperature": 0.0, "avg_logprob": -0.1784101292706918, "compression_ratio": 1.8357142857142856, "no_speech_prob": 5.42218276677886e-06}, {"id": 314, "seek": 161672, "start": 1641.52, "end": 1643.32, "text": " So we haven't instantiated it.", "tokens": [407, 321, 2378, 380, 9836, 72, 770, 309, 13], "temperature": 0.0, "avg_logprob": -0.1784101292706918, "compression_ratio": 1.8357142857142856, "no_speech_prob": 5.42218276677886e-06}, {"id": 315, "seek": 161672, "start": 1643.32, "end": 1644.32, "text": " We just passed in the type.", "tokens": [492, 445, 4678, 294, 264, 2010, 13], "temperature": 0.0, "avg_logprob": -0.1784101292706918, "compression_ratio": 1.8357142857142856, "no_speech_prob": 5.42218276677886e-06}, {"id": 316, "seek": 161672, "start": 1644.32, "end": 1646.3600000000001, "text": " So it's going to instantiate it for us.", "tokens": [407, 309, 311, 516, 281, 9836, 13024, 309, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.1784101292706918, "compression_ratio": 1.8357142857142856, "no_speech_prob": 5.42218276677886e-06}, {"id": 317, "seek": 164636, "start": 1646.36, "end": 1650.3999999999999, "text": " And it will always instantiate your types for you if you don't instantiate them.", "tokens": [400, 309, 486, 1009, 9836, 13024, 428, 3467, 337, 291, 498, 291, 500, 380, 9836, 13024, 552, 13], "temperature": 0.0, "avg_logprob": -0.1202201424064217, "compression_ratio": 1.6807511737089202, "no_speech_prob": 4.222774350637337e-06}, {"id": 318, "seek": 164636, "start": 1650.3999999999999, "end": 1655.1999999999998, "text": " And so later on we want to say, okay, let's find out what the vocab was, which means we", "tokens": [400, 370, 1780, 322, 321, 528, 281, 584, 11, 1392, 11, 718, 311, 915, 484, 437, 264, 2329, 455, 390, 11, 597, 1355, 321], "temperature": 0.0, "avg_logprob": -0.1202201424064217, "compression_ratio": 1.6807511737089202, "no_speech_prob": 4.222774350637337e-06}, {"id": 319, "seek": 164636, "start": 1655.1999999999998, "end": 1662.3999999999999, "text": " need to grab the processes out of our transform object and ask for the categorify transform.", "tokens": [643, 281, 4444, 264, 7555, 484, 295, 527, 4088, 2657, 293, 1029, 337, 264, 19250, 2505, 4088, 13], "temperature": 0.0, "avg_logprob": -0.1202201424064217, "compression_ratio": 1.6807511737089202, "no_speech_prob": 4.222774350637337e-06}, {"id": 320, "seek": 164636, "start": 1662.3999999999999, "end": 1672.8799999999999, "text": " So now that we've got that categorified transform, categorify defines getItem and it will return", "tokens": [407, 586, 300, 321, 600, 658, 300, 19250, 2587, 4088, 11, 19250, 2505, 23122, 483, 3522, 443, 293, 309, 486, 2736], "temperature": 0.0, "avg_logprob": -0.1202201424064217, "compression_ratio": 1.6807511737089202, "no_speech_prob": 4.222774350637337e-06}, {"id": 321, "seek": 167288, "start": 1672.88, "end": 1676.92, "text": " the vocab for that column.", "tokens": [264, 2329, 455, 337, 300, 7738, 13], "temperature": 0.0, "avg_logprob": -0.2572006369536778, "compression_ratio": 1.4636363636363636, "no_speech_prob": 1.8924911273643374e-05}, {"id": 322, "seek": 167288, "start": 1676.92, "end": 1686.6000000000001, "text": " So here is the vocab for that column.", "tokens": [407, 510, 307, 264, 2329, 455, 337, 300, 7738, 13], "temperature": 0.0, "avg_logprob": -0.2572006369536778, "compression_ratio": 1.4636363636363636, "no_speech_prob": 1.8924911273643374e-05}, {"id": 323, "seek": 167288, "start": 1686.6000000000001, "end": 1694.48, "text": " So we can have a look at a, as you can see, there it is.", "tokens": [407, 321, 393, 362, 257, 574, 412, 257, 11, 382, 291, 393, 536, 11, 456, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.2572006369536778, "compression_ratio": 1.4636363636363636, "no_speech_prob": 1.8924911273643374e-05}, {"id": 324, "seek": 167288, "start": 1694.48, "end": 1700.44, "text": " And that is actually type category map.", "tokens": [400, 300, 307, 767, 2010, 7719, 4471, 13], "temperature": 0.0, "avg_logprob": -0.2572006369536778, "compression_ratio": 1.4636363636363636, "no_speech_prob": 1.8924911273643374e-05}, {"id": 325, "seek": 170044, "start": 1700.44, "end": 1709.28, "text": " So as well as having the items that we just saw, it also has the reverse mapping.", "tokens": [407, 382, 731, 382, 1419, 264, 4754, 300, 321, 445, 1866, 11, 309, 611, 575, 264, 9943, 18350, 13], "temperature": 0.0, "avg_logprob": -0.1605802596883571, "compression_ratio": 1.528301886792453, "no_speech_prob": 2.857272420442314e-06}, {"id": 326, "seek": 170044, "start": 1709.28, "end": 1712.4, "text": " So this is, as you can see, goes the opposite direction.", "tokens": [407, 341, 307, 11, 382, 291, 393, 536, 11, 1709, 264, 6182, 3513, 13], "temperature": 0.0, "avg_logprob": -0.1605802596883571, "compression_ratio": 1.528301886792453, "no_speech_prob": 2.857272420442314e-06}, {"id": 327, "seek": 170044, "start": 1712.4, "end": 1718.48, "text": " So to answer Amman's question, this, yes, this should take care of the mapping in the", "tokens": [407, 281, 1867, 2012, 1601, 311, 1168, 11, 341, 11, 2086, 11, 341, 820, 747, 1127, 295, 264, 18350, 294, 264], "temperature": 0.0, "avg_logprob": -0.1605802596883571, "compression_ratio": 1.528301886792453, "no_speech_prob": 2.857272420442314e-06}, {"id": 328, "seek": 170044, "start": 1718.48, "end": 1721.76, "text": " test set.", "tokens": [1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.1605802596883571, "compression_ratio": 1.528301886792453, "no_speech_prob": 2.857272420442314e-06}, {"id": 329, "seek": 170044, "start": 1721.76, "end": 1729.44, "text": " Because if it comes across, so it's going to look at O2I when it tries to call applyCats.", "tokens": [1436, 498, 309, 1487, 2108, 11, 370, 309, 311, 516, 281, 574, 412, 422, 17, 40, 562, 309, 9898, 281, 818, 3079, 34, 1720, 13], "temperature": 0.0, "avg_logprob": -0.1605802596883571, "compression_ratio": 1.528301886792453, "no_speech_prob": 2.857272420442314e-06}, {"id": 330, "seek": 172944, "start": 1729.44, "end": 1737.04, "text": " It's going to try to find in your categories, it's going to try and find that column.", "tokens": [467, 311, 516, 281, 853, 281, 915, 294, 428, 10479, 11, 309, 311, 516, 281, 853, 293, 915, 300, 7738, 13], "temperature": 0.0, "avg_logprob": -0.19372631072998048, "compression_ratio": 1.6574074074074074, "no_speech_prob": 4.425464794621803e-06}, {"id": 331, "seek": 172944, "start": 1737.04, "end": 1741.28, "text": " It's going to grab O2I, which is this dictionary.", "tokens": [467, 311, 516, 281, 4444, 422, 17, 40, 11, 597, 307, 341, 25890, 13], "temperature": 0.0, "avg_logprob": -0.19372631072998048, "compression_ratio": 1.6574074074074074, "no_speech_prob": 4.425464794621803e-06}, {"id": 332, "seek": 172944, "start": 1741.28, "end": 1744.16, "text": " That's then going to use that to map everything in the column.", "tokens": [663, 311, 550, 516, 281, 764, 300, 281, 4471, 1203, 294, 264, 7738, 13], "temperature": 0.0, "avg_logprob": -0.19372631072998048, "compression_ratio": 1.6574074074074074, "no_speech_prob": 4.425464794621803e-06}, {"id": 333, "seek": 172944, "start": 1744.16, "end": 1748.68, "text": " But because it's a default dict, if there's anything it doesn't recognize, it will become", "tokens": [583, 570, 309, 311, 257, 7576, 12569, 11, 498, 456, 311, 1340, 309, 1177, 380, 5521, 11, 309, 486, 1813], "temperature": 0.0, "avg_logprob": -0.19372631072998048, "compression_ratio": 1.6574074074074074, "no_speech_prob": 4.425464794621803e-06}, {"id": 334, "seek": 172944, "start": 1748.68, "end": 1752.8, "text": " zero, which is the NA category.", "tokens": [4018, 11, 597, 307, 264, 16585, 7719, 13], "temperature": 0.0, "avg_logprob": -0.19372631072998048, "compression_ratio": 1.6574074074074074, "no_speech_prob": 4.425464794621803e-06}, {"id": 335, "seek": 172944, "start": 1752.8, "end": 1757.3200000000002, "text": " So yeah, that should all work nicely.", "tokens": [407, 1338, 11, 300, 820, 439, 589, 9594, 13], "temperature": 0.0, "avg_logprob": -0.19372631072998048, "compression_ratio": 1.6574074074074074, "no_speech_prob": 4.425464794621803e-06}, {"id": 336, "seek": 175732, "start": 1757.32, "end": 1760.4399999999998, "text": " You have to figure out how to model it, of course, but the data processing will handle", "tokens": [509, 362, 281, 2573, 484, 577, 281, 2316, 309, 11, 295, 1164, 11, 457, 264, 1412, 9007, 486, 4813], "temperature": 0.0, "avg_logprob": -0.2395073481968471, "compression_ratio": 1.4417177914110428, "no_speech_prob": 2.6841858016268816e-06}, {"id": 337, "seek": 175732, "start": 1760.4399999999998, "end": 1764.4399999999998, "text": " it for you.", "tokens": [309, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.2395073481968471, "compression_ratio": 1.4417177914110428, "no_speech_prob": 2.6841858016268816e-06}, {"id": 338, "seek": 175732, "start": 1764.4399999999998, "end": 1767.36, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2395073481968471, "compression_ratio": 1.4417177914110428, "no_speech_prob": 2.6841858016268816e-06}, {"id": 339, "seek": 175732, "start": 1767.36, "end": 1774.32, "text": " So what else?", "tokens": [407, 437, 1646, 30], "temperature": 0.0, "avg_logprob": -0.2395073481968471, "compression_ratio": 1.4417177914110428, "no_speech_prob": 2.6841858016268816e-06}, {"id": 340, "seek": 175732, "start": 1774.32, "end": 1780.9199999999998, "text": " So now, imagine it's inference time.", "tokens": [407, 586, 11, 3811, 309, 311, 38253, 565, 13], "temperature": 0.0, "avg_logprob": -0.2395073481968471, "compression_ratio": 1.4417177914110428, "no_speech_prob": 2.6841858016268816e-06}, {"id": 341, "seek": 175732, "start": 1780.9199999999998, "end": 1785.28, "text": " And so we come along with some new data frame that we want to run inference on.", "tokens": [400, 370, 321, 808, 2051, 365, 512, 777, 1412, 3920, 300, 321, 528, 281, 1190, 38253, 322, 13], "temperature": 0.0, "avg_logprob": -0.2395073481968471, "compression_ratio": 1.4417177914110428, "no_speech_prob": 2.6841858016268816e-06}, {"id": 342, "seek": 178528, "start": 1785.28, "end": 1788.12, "text": " Oh, it's a test set or whatever.", "tokens": [876, 11, 309, 311, 257, 1500, 992, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.1369596525680187, "compression_ratio": 1.8893280632411067, "no_speech_prob": 6.747980933141662e-06}, {"id": 343, "seek": 178528, "start": 1788.12, "end": 1789.6, "text": " So here's our data frame.", "tokens": [407, 510, 311, 527, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.1369596525680187, "compression_ratio": 1.8893280632411067, "no_speech_prob": 6.747980933141662e-06}, {"id": 344, "seek": 178528, "start": 1789.6, "end": 1795.6, "text": " So we now have to say, okay, I want to create a new tabular object with the same metadata", "tokens": [407, 321, 586, 362, 281, 584, 11, 1392, 11, 286, 528, 281, 1884, 257, 777, 4421, 1040, 2657, 365, 264, 912, 26603], "temperature": 0.0, "avg_logprob": -0.1369596525680187, "compression_ratio": 1.8893280632411067, "no_speech_prob": 6.747980933141662e-06}, {"id": 345, "seek": 178528, "start": 1795.6, "end": 1796.6, "text": " that we had before.", "tokens": [300, 321, 632, 949, 13], "temperature": 0.0, "avg_logprob": -0.1369596525680187, "compression_ratio": 1.8893280632411067, "no_speech_prob": 6.747980933141662e-06}, {"id": 346, "seek": 178528, "start": 1796.6, "end": 1802.3999999999999, "text": " So the same processes, same vocab, the same categorical continuous variables.", "tokens": [407, 264, 912, 7555, 11, 912, 2329, 455, 11, 264, 912, 19250, 804, 10957, 9102, 13], "temperature": 0.0, "avg_logprob": -0.1369596525680187, "compression_ratio": 1.8893280632411067, "no_speech_prob": 6.747980933141662e-06}, {"id": 347, "seek": 178528, "start": 1802.3999999999999, "end": 1806.48, "text": " The way to do that is to start with an existing tabular object, which has that metadata and", "tokens": [440, 636, 281, 360, 300, 307, 281, 722, 365, 364, 6741, 4421, 1040, 2657, 11, 597, 575, 300, 26603, 293], "temperature": 0.0, "avg_logprob": -0.1369596525680187, "compression_ratio": 1.8893280632411067, "no_speech_prob": 6.747980933141662e-06}, {"id": 348, "seek": 178528, "start": 1806.48, "end": 1809.8799999999999, "text": " call new and pass in a new data frame that you have.", "tokens": [818, 777, 293, 1320, 294, 257, 777, 1412, 3920, 300, 291, 362, 13], "temperature": 0.0, "avg_logprob": -0.1369596525680187, "compression_ratio": 1.8893280632411067, "no_speech_prob": 6.747980933141662e-06}, {"id": 349, "seek": 178528, "start": 1809.8799999999999, "end": 1813.92, "text": " And that's going to give you a new tabular object with the same metadata and processes", "tokens": [400, 300, 311, 516, 281, 976, 291, 257, 777, 4421, 1040, 2657, 365, 264, 912, 26603, 293, 7555], "temperature": 0.0, "avg_logprob": -0.1369596525680187, "compression_ratio": 1.8893280632411067, "no_speech_prob": 6.747980933141662e-06}, {"id": 350, "seek": 181392, "start": 1813.92, "end": 1818.8000000000002, "text": " and stuff that we had before, but with this different data in it.", "tokens": [293, 1507, 300, 321, 632, 949, 11, 457, 365, 341, 819, 1412, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.15583057892628205, "compression_ratio": 1.565217391304348, "no_speech_prob": 3.288728976258426e-06}, {"id": 351, "seek": 181392, "start": 1818.8000000000002, "end": 1824.76, "text": " Now, of course, we don't want to call setup on that because setup would replace the vocab.", "tokens": [823, 11, 295, 1164, 11, 321, 500, 380, 528, 281, 818, 8657, 322, 300, 570, 8657, 576, 7406, 264, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.15583057892628205, "compression_ratio": 1.565217391304348, "no_speech_prob": 3.288728976258426e-06}, {"id": 352, "seek": 181392, "start": 1824.76, "end": 1831.2, "text": " And the whole point is that we want to actually use the same vocab for inference on this new", "tokens": [400, 264, 1379, 935, 307, 300, 321, 528, 281, 767, 764, 264, 912, 2329, 455, 337, 38253, 322, 341, 777], "temperature": 0.0, "avg_logprob": -0.15583057892628205, "compression_ratio": 1.565217391304348, "no_speech_prob": 3.288728976258426e-06}, {"id": 353, "seek": 181392, "start": 1831.2, "end": 1833.64, "text": " data set.", "tokens": [1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.15583057892628205, "compression_ratio": 1.565217391304348, "no_speech_prob": 3.288728976258426e-06}, {"id": 354, "seek": 181392, "start": 1833.64, "end": 1836.1200000000001, "text": " So instead you call process.", "tokens": [407, 2602, 291, 818, 1399, 13], "temperature": 0.0, "avg_logprob": -0.15583057892628205, "compression_ratio": 1.565217391304348, "no_speech_prob": 3.288728976258426e-06}, {"id": 355, "seek": 183612, "start": 1836.12, "end": 1844.12, "text": " And so all process does in tabular is it just calls the processes, which is a pipeline.", "tokens": [400, 370, 439, 1399, 775, 294, 4421, 1040, 307, 309, 445, 5498, 264, 7555, 11, 597, 307, 257, 15517, 13], "temperature": 0.0, "avg_logprob": -0.1526364286740621, "compression_ratio": 1.5816326530612246, "no_speech_prob": 6.681508466499508e-07}, {"id": 356, "seek": 183612, "start": 1844.12, "end": 1850.7199999999998, "text": " So you can just treat it as a function.", "tokens": [407, 291, 393, 445, 2387, 309, 382, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1526364286740621, "compression_ratio": 1.5816326530612246, "no_speech_prob": 6.681508466499508e-07}, {"id": 357, "seek": 183612, "start": 1850.7199999999998, "end": 1857.9199999999998, "text": " So in this case, our vocab was 0, 1, 2, and an a, right?", "tokens": [407, 294, 341, 1389, 11, 527, 2329, 455, 390, 1958, 11, 502, 11, 568, 11, 293, 364, 257, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1526364286740621, "compression_ratio": 1.5816326530612246, "no_speech_prob": 6.681508466499508e-07}, {"id": 358, "seek": 183612, "start": 1857.9199999999998, "end": 1862.2399999999998, "text": " And here there's a couple of things that are not in that list of 0, 1, 2, specifically", "tokens": [400, 510, 456, 311, 257, 1916, 295, 721, 300, 366, 406, 294, 300, 1329, 295, 1958, 11, 502, 11, 568, 11, 4682], "temperature": 0.0, "avg_logprob": -0.1526364286740621, "compression_ratio": 1.5816326530612246, "no_speech_prob": 6.681508466499508e-07}, {"id": 359, "seek": 183612, "start": 1862.2399999999998, "end": 1865.9599999999998, "text": " this one is 3 and this one is minus 1.", "tokens": [341, 472, 307, 805, 293, 341, 472, 307, 3175, 502, 13], "temperature": 0.0, "avg_logprob": -0.1526364286740621, "compression_ratio": 1.5816326530612246, "no_speech_prob": 6.681508466499508e-07}, {"id": 360, "seek": 186596, "start": 1865.96, "end": 1875.72, "text": " So 1, 0, and 2 will get replaced by their 1, 0, and 2.", "tokens": [407, 502, 11, 1958, 11, 293, 568, 486, 483, 10772, 538, 641, 502, 11, 1958, 11, 293, 568, 13], "temperature": 0.0, "avg_logprob": -0.21188886745555982, "compression_ratio": 1.4233576642335766, "no_speech_prob": 3.555967168722418e-06}, {"id": 361, "seek": 186596, "start": 1875.72, "end": 1879.32, "text": " So 2, 1, 3, 2, 1, 3.", "tokens": [407, 568, 11, 502, 11, 805, 11, 568, 11, 502, 11, 805, 13], "temperature": 0.0, "avg_logprob": -0.21188886745555982, "compression_ratio": 1.4233576642335766, "no_speech_prob": 3.555967168722418e-06}, {"id": 362, "seek": 186596, "start": 1879.32, "end": 1883.3600000000001, "text": " So there's 2, there's our new a column.", "tokens": [407, 456, 311, 568, 11, 456, 311, 527, 777, 257, 7738, 13], "temperature": 0.0, "avg_logprob": -0.21188886745555982, "compression_ratio": 1.4233576642335766, "no_speech_prob": 3.555967168722418e-06}, {"id": 363, "seek": 186596, "start": 1883.3600000000001, "end": 1889.04, "text": " And then as we just discussed, the two things that don't map are going to be 0.", "tokens": [400, 550, 382, 321, 445, 7152, 11, 264, 732, 721, 300, 500, 380, 4471, 366, 516, 281, 312, 1958, 13], "temperature": 0.0, "avg_logprob": -0.21188886745555982, "compression_ratio": 1.4233576642335766, "no_speech_prob": 3.555967168722418e-06}, {"id": 364, "seek": 188904, "start": 1889.04, "end": 1900.24, "text": " So then if you call decode on our processor, then as you would expect, you end up with", "tokens": [407, 550, 498, 291, 818, 979, 1429, 322, 527, 15321, 11, 550, 382, 291, 576, 2066, 11, 291, 917, 493, 365], "temperature": 0.0, "avg_logprob": -0.16953323664290182, "compression_ratio": 1.5742574257425743, "no_speech_prob": 5.896376933378633e-07}, {"id": 365, "seek": 188904, "start": 1900.24, "end": 1902.0, "text": " the same data you started with.", "tokens": [264, 912, 1412, 291, 1409, 365, 13], "temperature": 0.0, "avg_logprob": -0.16953323664290182, "compression_ratio": 1.5742574257425743, "no_speech_prob": 5.896376933378633e-07}, {"id": 366, "seek": 188904, "start": 1902.0, "end": 1910.12, "text": " But of course, this is now going to be an a because we said we don't know what those", "tokens": [583, 295, 1164, 11, 341, 307, 586, 516, 281, 312, 364, 257, 570, 321, 848, 321, 500, 380, 458, 437, 729], "temperature": 0.0, "avg_logprob": -0.16953323664290182, "compression_ratio": 1.5742574257425743, "no_speech_prob": 5.896376933378633e-07}, {"id": 367, "seek": 188904, "start": 1910.12, "end": 1911.12, "text": " are.", "tokens": [366, 13], "temperature": 0.0, "avg_logprob": -0.16953323664290182, "compression_ratio": 1.5742574257425743, "no_speech_prob": 5.896376933378633e-07}, {"id": 368, "seek": 188904, "start": 1911.12, "end": 1916.44, "text": " So like decoding in general in fast AI doesn't mean you always get back exactly what you", "tokens": [407, 411, 979, 8616, 294, 2674, 294, 2370, 7318, 1177, 380, 914, 291, 1009, 483, 646, 2293, 437, 291], "temperature": 0.0, "avg_logprob": -0.16953323664290182, "compression_ratio": 1.5742574257425743, "no_speech_prob": 5.896376933378633e-07}, {"id": 369, "seek": 188904, "start": 1916.44, "end": 1918.08, "text": " started with, right?", "tokens": [1409, 365, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16953323664290182, "compression_ratio": 1.5742574257425743, "no_speech_prob": 5.896376933378633e-07}, {"id": 370, "seek": 191808, "start": 1918.08, "end": 1928.28, "text": " It's kind of trying to display the kind of transformed version of the data.", "tokens": [467, 311, 733, 295, 1382, 281, 4674, 264, 733, 295, 16894, 3037, 295, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.12933778127034506, "compression_ratio": 1.6304347826086956, "no_speech_prob": 5.4221454774960876e-06}, {"id": 371, "seek": 191808, "start": 1928.28, "end": 1934.1599999999999, "text": " In some cases like normalization, it should pretty much be exactly what you started with.", "tokens": [682, 512, 3331, 411, 2710, 2144, 11, 309, 820, 1238, 709, 312, 2293, 437, 291, 1409, 365, 13], "temperature": 0.0, "avg_logprob": -0.12933778127034506, "compression_ratio": 1.6304347826086956, "no_speech_prob": 5.4221454774960876e-06}, {"id": 372, "seek": 191808, "start": 1934.1599999999999, "end": 1942.36, "text": " Some things like categorify the missing values, it won't be exactly what you started with.", "tokens": [2188, 721, 411, 19250, 2505, 264, 5361, 4190, 11, 309, 1582, 380, 312, 2293, 437, 291, 1409, 365, 13], "temperature": 0.0, "avg_logprob": -0.12933778127034506, "compression_ratio": 1.6304347826086956, "no_speech_prob": 5.4221454774960876e-06}, {"id": 373, "seek": 191808, "start": 1942.36, "end": 1944.6399999999999, "text": " You don't have to pass in just a type name.", "tokens": [509, 500, 380, 362, 281, 1320, 294, 445, 257, 2010, 1315, 13], "temperature": 0.0, "avg_logprob": -0.12933778127034506, "compression_ratio": 1.6304347826086956, "no_speech_prob": 5.4221454774960876e-06}, {"id": 374, "seek": 194464, "start": 1944.64, "end": 1951.3600000000001, "text": " You can instantiate a processor yourself and then pass that in.", "tokens": [509, 393, 9836, 13024, 257, 15321, 1803, 293, 550, 1320, 300, 294, 13], "temperature": 0.0, "avg_logprob": -0.13183886806170145, "compression_ratio": 1.6512605042016806, "no_speech_prob": 5.682378741767025e-06}, {"id": 375, "seek": 194464, "start": 1951.3600000000001, "end": 1956.96, "text": " So then that means you don't have to dig it out again like this.", "tokens": [407, 550, 300, 1355, 291, 500, 380, 362, 281, 2528, 309, 484, 797, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.13183886806170145, "compression_ratio": 1.6512605042016806, "no_speech_prob": 5.682378741767025e-06}, {"id": 376, "seek": 194464, "start": 1956.96, "end": 1957.96, "text": " So sometimes that's more convenient.", "tokens": [407, 2171, 300, 311, 544, 10851, 13], "temperature": 0.0, "avg_logprob": -0.13183886806170145, "compression_ratio": 1.6512605042016806, "no_speech_prob": 5.682378741767025e-06}, {"id": 377, "seek": 194464, "start": 1957.96, "end": 1961.0800000000002, "text": " So this is just another way of doing the same thing.", "tokens": [407, 341, 307, 445, 1071, 636, 295, 884, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.13183886806170145, "compression_ratio": 1.6512605042016806, "no_speech_prob": 5.682378741767025e-06}, {"id": 378, "seek": 194464, "start": 1961.0800000000002, "end": 1965.64, "text": " But in this case, we're also going to split the training set and the validation set.", "tokens": [583, 294, 341, 1389, 11, 321, 434, 611, 516, 281, 7472, 264, 3097, 992, 293, 264, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.13183886806170145, "compression_ratio": 1.6512605042016806, "no_speech_prob": 5.682378741767025e-06}, {"id": 379, "seek": 194464, "start": 1965.64, "end": 1970.0800000000002, "text": " And this is particularly important for things like categorify because if our training set", "tokens": [400, 341, 307, 4098, 1021, 337, 721, 411, 19250, 2505, 570, 498, 527, 3097, 992], "temperature": 0.0, "avg_logprob": -0.13183886806170145, "compression_ratio": 1.6512605042016806, "no_speech_prob": 5.682378741767025e-06}, {"id": 380, "seek": 197008, "start": 1970.08, "end": 1976.48, "text": " is the first three elements and our validation set is the last two, then this thing here", "tokens": [307, 264, 700, 1045, 4959, 293, 527, 24071, 992, 307, 264, 1036, 732, 11, 550, 341, 551, 510], "temperature": 0.0, "avg_logprob": -0.17395688570462742, "compression_ratio": 1.8132295719844358, "no_speech_prob": 7.411204933305271e-06}, {"id": 381, "seek": 197008, "start": 1976.48, "end": 1978.6399999999999, "text": " three is not in the training set.", "tokens": [1045, 307, 406, 294, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.17395688570462742, "compression_ratio": 1.8132295719844358, "no_speech_prob": 7.411204933305271e-06}, {"id": 382, "seek": 197008, "start": 1978.6399999999999, "end": 1982.04, "text": " And so therefore it should not be part of the vocab.", "tokens": [400, 370, 4412, 309, 820, 406, 312, 644, 295, 264, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.17395688570462742, "compression_ratio": 1.8132295719844358, "no_speech_prob": 7.411204933305271e-06}, {"id": 383, "seek": 197008, "start": 1982.04, "end": 1983.84, "text": " So let's make sure that that's the case.", "tokens": [407, 718, 311, 652, 988, 300, 300, 311, 264, 1389, 13], "temperature": 0.0, "avg_logprob": -0.17395688570462742, "compression_ratio": 1.8132295719844358, "no_speech_prob": 7.411204933305271e-06}, {"id": 384, "seek": 197008, "start": 1983.84, "end": 1986.3999999999999, "text": " So here we are, categorical variable.", "tokens": [407, 510, 321, 366, 11, 19250, 804, 7006, 13], "temperature": 0.0, "avg_logprob": -0.17395688570462742, "compression_ratio": 1.8132295719844358, "no_speech_prob": 7.411204933305271e-06}, {"id": 385, "seek": 197008, "start": 1986.3999999999999, "end": 1988.36, "text": " Yep, it doesn't have three in it.", "tokens": [7010, 11, 309, 1177, 380, 362, 1045, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.17395688570462742, "compression_ratio": 1.8132295719844358, "no_speech_prob": 7.411204933305271e-06}, {"id": 386, "seek": 197008, "start": 1988.36, "end": 1990.28, "text": " The vocab doesn't have three in it.", "tokens": [440, 2329, 455, 1177, 380, 362, 1045, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.17395688570462742, "compression_ratio": 1.8132295719844358, "no_speech_prob": 7.411204933305271e-06}, {"id": 387, "seek": 197008, "start": 1990.28, "end": 1996.6799999999998, "text": " So the way we pass in these split indexes is by calling tabular object dot data source", "tokens": [407, 264, 636, 321, 1320, 294, 613, 7472, 8186, 279, 307, 538, 5141, 4421, 1040, 2657, 5893, 1412, 4009], "temperature": 0.0, "avg_logprob": -0.17395688570462742, "compression_ratio": 1.8132295719844358, "no_speech_prob": 7.411204933305271e-06}, {"id": 388, "seek": 197008, "start": 1996.6799999999998, "end": 1999.52, "text": " and that converts the tabular object to a data source.", "tokens": [293, 300, 38874, 264, 4421, 1040, 2657, 281, 257, 1412, 4009, 13], "temperature": 0.0, "avg_logprob": -0.17395688570462742, "compression_ratio": 1.8132295719844358, "no_speech_prob": 7.411204933305271e-06}, {"id": 389, "seek": 199952, "start": 1999.52, "end": 2005.68, "text": " The only thing you pass it is the list of splits.", "tokens": [440, 787, 551, 291, 1320, 309, 307, 264, 1329, 295, 37741, 13], "temperature": 0.0, "avg_logprob": -0.16206540844657205, "compression_ratio": 1.6, "no_speech_prob": 2.1568082502199104e-06}, {"id": 390, "seek": 199952, "start": 2005.68, "end": 2012.8, "text": " And so that gives you a standard data source object, just like the one that we saw in our", "tokens": [400, 370, 300, 2709, 291, 257, 3832, 1412, 4009, 2657, 11, 445, 411, 264, 472, 300, 321, 1866, 294, 527], "temperature": 0.0, "avg_logprob": -0.16206540844657205, "compression_ratio": 1.6, "no_speech_prob": 2.1568082502199104e-06}, {"id": 391, "seek": 199952, "start": 2012.8, "end": 2015.96, "text": " last walkthrough.", "tokens": [1036, 1792, 11529, 13], "temperature": 0.0, "avg_logprob": -0.16206540844657205, "compression_ratio": 1.6, "no_speech_prob": 2.1568082502199104e-06}, {"id": 392, "seek": 199952, "start": 2015.96, "end": 2017.8, "text": " So that's what you get.", "tokens": [407, 300, 311, 437, 291, 483, 13], "temperature": 0.0, "avg_logprob": -0.16206540844657205, "compression_ratio": 1.6, "no_speech_prob": 2.1568082502199104e-06}, {"id": 393, "seek": 199952, "start": 2017.8, "end": 2020.12, "text": " And so that data source, let's take it out, right?", "tokens": [400, 370, 300, 1412, 4009, 11, 718, 311, 747, 309, 484, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16206540844657205, "compression_ratio": 1.6, "no_speech_prob": 2.1568082502199104e-06}, {"id": 394, "seek": 199952, "start": 2020.12, "end": 2027.6, "text": " So that data source object will have a train, for example, and a valid.", "tokens": [407, 300, 1412, 4009, 2657, 486, 362, 257, 3847, 11, 337, 1365, 11, 293, 257, 7363, 13], "temperature": 0.0, "avg_logprob": -0.16206540844657205, "compression_ratio": 1.6, "no_speech_prob": 2.1568082502199104e-06}, {"id": 395, "seek": 202760, "start": 2027.6, "end": 2041.08, "text": " And those are just the ways of saying subset 0 and subset 1.", "tokens": [400, 729, 366, 445, 264, 2098, 295, 1566, 25993, 1958, 293, 25993, 502, 13], "temperature": 0.0, "avg_logprob": -0.378793512071882, "compression_ratio": 1.0493827160493827, "no_speech_prob": 5.255313226371072e-06}, {"id": 396, "seek": 202760, "start": 2041.08, "end": 2046.08, "text": " Should test set be 3, 2?", "tokens": [6454, 1500, 992, 312, 805, 11, 568, 30], "temperature": 0.0, "avg_logprob": -0.378793512071882, "compression_ratio": 1.0493827160493827, "no_speech_prob": 5.255313226371072e-06}, {"id": 397, "seek": 204608, "start": 2046.08, "end": 2059.2, "text": " No, these are the indexes of what's the indexes of things in the training set.", "tokens": [883, 11, 613, 366, 264, 8186, 279, 295, 437, 311, 264, 8186, 279, 295, 721, 294, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.22507478370041142, "compression_ratio": 1.8108108108108107, "no_speech_prob": 4.785009423358133e-06}, {"id": 398, "seek": 204608, "start": 2059.2, "end": 2061.04, "text": " These are the indexes in the validation set.", "tokens": [1981, 366, 264, 8186, 279, 294, 264, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.22507478370041142, "compression_ratio": 1.8108108108108107, "no_speech_prob": 4.785009423358133e-06}, {"id": 399, "seek": 204608, "start": 2061.04, "end": 2065.3199999999997, "text": " So these are indexes 3 and 4 in the validation set.", "tokens": [407, 613, 366, 8186, 279, 805, 293, 1017, 294, 264, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.22507478370041142, "compression_ratio": 1.8108108108108107, "no_speech_prob": 4.785009423358133e-06}, {"id": 400, "seek": 204608, "start": 2065.3199999999997, "end": 2070.68, "text": " Yes, they are ID indexes.", "tokens": [1079, 11, 436, 366, 7348, 8186, 279, 13], "temperature": 0.0, "avg_logprob": -0.22507478370041142, "compression_ratio": 1.8108108108108107, "no_speech_prob": 4.785009423358133e-06}, {"id": 401, "seek": 207068, "start": 2070.68, "end": 2081.24, "text": " So data source, so in terms of looking at the code of tabular, it's super tiny, which", "tokens": [407, 1412, 4009, 11, 370, 294, 2115, 295, 1237, 412, 264, 3089, 295, 4421, 1040, 11, 309, 311, 1687, 5870, 11, 597], "temperature": 0.0, "avg_logprob": -0.22754383087158203, "compression_ratio": 1.6685082872928176, "no_speech_prob": 1.209861807183188e-06}, {"id": 402, "seek": 207068, "start": 2081.24, "end": 2087.9199999999996, "text": " is nice, in terms of the things that are more than one line in it, because there's a bunch", "tokens": [307, 1481, 11, 294, 2115, 295, 264, 721, 300, 366, 544, 813, 472, 1622, 294, 309, 11, 570, 456, 311, 257, 3840], "temperature": 0.0, "avg_logprob": -0.22754383087158203, "compression_ratio": 1.6685082872928176, "no_speech_prob": 1.209861807183188e-06}, {"id": 403, "seek": 207068, "start": 2087.9199999999996, "end": 2090.8399999999997, "text": " of things to store, and data source.", "tokens": [295, 721, 281, 3531, 11, 293, 1412, 4009, 13], "temperature": 0.0, "avg_logprob": -0.22754383087158203, "compression_ratio": 1.6685082872928176, "no_speech_prob": 1.209861807183188e-06}, {"id": 404, "seek": 207068, "start": 2090.8399999999997, "end": 2099.68, "text": " And the only reason data source is more than a couple of lines is because in Rappers, on", "tokens": [400, 264, 787, 1778, 1412, 4009, 307, 544, 813, 257, 1916, 295, 3876, 307, 570, 294, 497, 1746, 433, 11, 322], "temperature": 0.0, "avg_logprob": -0.22754383087158203, "compression_ratio": 1.6685082872928176, "no_speech_prob": 1.209861807183188e-06}, {"id": 405, "seek": 209968, "start": 2099.68, "end": 2106.24, "text": " the GPU, trying to index into a data frame with arbitrary indexes is really, really,", "tokens": [264, 18407, 11, 1382, 281, 8186, 666, 257, 1412, 3920, 365, 23211, 8186, 279, 307, 534, 11, 534, 11], "temperature": 0.0, "avg_logprob": -0.1044583964992214, "compression_ratio": 1.5868263473053892, "no_speech_prob": 4.710852408607025e-06}, {"id": 406, "seek": 209968, "start": 2106.24, "end": 2111.3999999999996, "text": " really, really slow.", "tokens": [534, 11, 534, 2964, 13], "temperature": 0.0, "avg_logprob": -0.1044583964992214, "compression_ratio": 1.5868263473053892, "no_speech_prob": 4.710852408607025e-06}, {"id": 407, "seek": 209968, "start": 2111.3999999999996, "end": 2115.8799999999997, "text": " So you have to pass in a contiguous list of indexes to make Rappers fast.", "tokens": [407, 291, 362, 281, 1320, 294, 257, 660, 30525, 1329, 295, 8186, 279, 281, 652, 497, 1746, 433, 2370, 13], "temperature": 0.0, "avg_logprob": -0.1044583964992214, "compression_ratio": 1.5868263473053892, "no_speech_prob": 4.710852408607025e-06}, {"id": 408, "seek": 209968, "start": 2115.8799999999997, "end": 2125.52, "text": " So what we do is when you pass in splits, we actually concatenate all of those splits", "tokens": [407, 437, 321, 360, 307, 562, 291, 1320, 294, 37741, 11, 321, 767, 1588, 7186, 473, 439, 295, 729, 37741], "temperature": 0.0, "avg_logprob": -0.1044583964992214, "compression_ratio": 1.5868263473053892, "no_speech_prob": 4.710852408607025e-06}, {"id": 409, "seek": 212552, "start": 2125.52, "end": 2132.84, "text": " together into a single list, and we index into the data frame with that list.", "tokens": [1214, 666, 257, 2167, 1329, 11, 293, 321, 8186, 666, 264, 1412, 3920, 365, 300, 1329, 13], "temperature": 0.0, "avg_logprob": -0.13279793110299618, "compression_ratio": 1.705069124423963, "no_speech_prob": 7.527935849793721e-06}, {"id": 410, "seek": 212552, "start": 2132.84, "end": 2137.96, "text": " So that's going to shuffle the list so that all the stuff that's in the same validation", "tokens": [407, 300, 311, 516, 281, 39426, 264, 1329, 370, 300, 439, 264, 1507, 300, 311, 294, 264, 912, 24071], "temperature": 0.0, "avg_logprob": -0.13279793110299618, "compression_ratio": 1.705069124423963, "no_speech_prob": 7.527935849793721e-06}, {"id": 411, "seek": 212552, "start": 2137.96, "end": 2141.04, "text": " or training set is all together.", "tokens": [420, 3097, 992, 307, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.13279793110299618, "compression_ratio": 1.705069124423963, "no_speech_prob": 7.527935849793721e-06}, {"id": 412, "seek": 212552, "start": 2141.04, "end": 2146.7599999999998, "text": " And so that way, now when we create our data source, rather than passing in the actual", "tokens": [400, 370, 300, 636, 11, 586, 562, 321, 1884, 527, 1412, 4009, 11, 2831, 813, 8437, 294, 264, 3539], "temperature": 0.0, "avg_logprob": -0.13279793110299618, "compression_ratio": 1.705069124423963, "no_speech_prob": 7.527935849793721e-06}, {"id": 413, "seek": 212552, "start": 2146.7599999999998, "end": 2152.52, "text": " splits, we just pass in a range of all the numbers from 0 to the length of the first", "tokens": [37741, 11, 321, 445, 1320, 294, 257, 3613, 295, 439, 264, 3547, 490, 1958, 281, 264, 4641, 295, 264, 700], "temperature": 0.0, "avg_logprob": -0.13279793110299618, "compression_ratio": 1.705069124423963, "no_speech_prob": 7.527935849793721e-06}, {"id": 414, "seek": 215252, "start": 2152.52, "end": 2157.24, "text": " split, and then all of the numbers from the length of the first split to the length of", "tokens": [7472, 11, 293, 550, 439, 295, 264, 3547, 490, 264, 4641, 295, 264, 700, 7472, 281, 264, 4641, 295], "temperature": 0.0, "avg_logprob": -0.15305771827697753, "compression_ratio": 1.5285714285714285, "no_speech_prob": 2.726456386881182e-06}, {"id": 415, "seek": 215252, "start": 2157.24, "end": 2160.52, "text": " the whole thing.", "tokens": [264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.15305771827697753, "compression_ratio": 1.5285714285714285, "no_speech_prob": 2.726456386881182e-06}, {"id": 416, "seek": 215252, "start": 2160.52, "end": 2169.46, "text": " And so our data source is then able to always use contiguous indexes.", "tokens": [400, 370, 527, 1412, 4009, 307, 550, 1075, 281, 1009, 764, 660, 30525, 8186, 279, 13], "temperature": 0.0, "avg_logprob": -0.15305771827697753, "compression_ratio": 1.5285714285714285, "no_speech_prob": 2.726456386881182e-06}, {"id": 417, "seek": 215252, "start": 2169.46, "end": 2172.72, "text": " So that's why that bit of code is there.", "tokens": [407, 300, 311, 983, 300, 857, 295, 3089, 307, 456, 13], "temperature": 0.0, "avg_logprob": -0.15305771827697753, "compression_ratio": 1.5285714285714285, "no_speech_prob": 2.726456386881182e-06}, {"id": 418, "seek": 217272, "start": 2172.72, "end": 2183.16, "text": " Other than that, one thing I don't like about Python is that anytime you want to create", "tokens": [5358, 813, 300, 11, 472, 551, 286, 500, 380, 411, 466, 15329, 307, 300, 13038, 291, 528, 281, 1884], "temperature": 0.0, "avg_logprob": -0.15879063654427578, "compression_ratio": 1.6592920353982301, "no_speech_prob": 3.7853058074688306e-06}, {"id": 419, "seek": 217272, "start": 2183.16, "end": 2188.3199999999997, "text": " a property, you have to put it on another row like this.", "tokens": [257, 4707, 11, 291, 362, 281, 829, 309, 322, 1071, 5386, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.15879063654427578, "compression_ratio": 1.6592920353982301, "no_speech_prob": 3.7853058074688306e-06}, {"id": 420, "seek": 217272, "start": 2188.3199999999997, "end": 2191.52, "text": " Like in a lot of programming languages, you can kind of do that kind of thing on the same", "tokens": [1743, 294, 257, 688, 295, 9410, 8650, 11, 291, 393, 733, 295, 360, 300, 733, 295, 551, 322, 264, 912], "temperature": 0.0, "avg_logprob": -0.15879063654427578, "compression_ratio": 1.6592920353982301, "no_speech_prob": 3.7853058074688306e-06}, {"id": 421, "seek": 217272, "start": 2191.52, "end": 2194.68, "text": " line, but not in Python for some reason.", "tokens": [1622, 11, 457, 406, 294, 15329, 337, 512, 1778, 13], "temperature": 0.0, "avg_logprob": -0.15879063654427578, "compression_ratio": 1.6592920353982301, "no_speech_prob": 3.7853058074688306e-06}, {"id": 422, "seek": 217272, "start": 2194.68, "end": 2198.48, "text": " And so I find it kind of takes up a lot of room just for the processor saying these are", "tokens": [400, 370, 286, 915, 309, 733, 295, 2516, 493, 257, 688, 295, 1808, 445, 337, 264, 15321, 1566, 613, 366], "temperature": 0.0, "avg_logprob": -0.15879063654427578, "compression_ratio": 1.6592920353982301, "no_speech_prob": 3.7853058074688306e-06}, {"id": 423, "seek": 217272, "start": 2198.48, "end": 2199.48, "text": " properties.", "tokens": [7221, 13], "temperature": 0.0, "avg_logprob": -0.15879063654427578, "compression_ratio": 1.6592920353982301, "no_speech_prob": 3.7853058074688306e-06}, {"id": 424, "seek": 219948, "start": 2199.48, "end": 2205.36, "text": " I added an alternative syntax, which is to create a list of all the things that are properties.", "tokens": [286, 3869, 364, 8535, 28431, 11, 597, 307, 281, 1884, 257, 1329, 295, 439, 264, 721, 300, 366, 7221, 13], "temperature": 0.0, "avg_logprob": -0.2612785387642776, "compression_ratio": 1.5771428571428572, "no_speech_prob": 1.4970880329201464e-05}, {"id": 425, "seek": 219948, "start": 2205.36, "end": 2207.96, "text": " So that's all that is.", "tokens": [407, 300, 311, 439, 300, 307, 13], "temperature": 0.0, "avg_logprob": -0.2612785387642776, "compression_ratio": 1.5771428571428572, "no_speech_prob": 1.4970880329201464e-05}, {"id": 426, "seek": 219948, "start": 2207.96, "end": 2211.8, "text": " Like most of these things, it's super tiny.", "tokens": [1743, 881, 295, 613, 721, 11, 309, 311, 1687, 5870, 13], "temperature": 0.0, "avg_logprob": -0.2612785387642776, "compression_ratio": 1.5771428571428572, "no_speech_prob": 1.4970880329201464e-05}, {"id": 427, "seek": 219948, "start": 2211.8, "end": 2213.08, "text": " So it's just one line of code.", "tokens": [407, 309, 311, 445, 472, 1622, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.2612785387642776, "compression_ratio": 1.5771428571428572, "no_speech_prob": 1.4970880329201464e-05}, {"id": 428, "seek": 219948, "start": 2213.08, "end": 2220.6, "text": " It just goes through and calls property on them to make them properties.", "tokens": [467, 445, 1709, 807, 293, 5498, 4707, 322, 552, 281, 652, 552, 7221, 13], "temperature": 0.0, "avg_logprob": -0.2612785387642776, "compression_ratio": 1.5771428571428572, "no_speech_prob": 1.4970880329201464e-05}, {"id": 429, "seek": 219948, "start": 2220.6, "end": 2224.64, "text": " Oh, okay.", "tokens": [876, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.2612785387642776, "compression_ratio": 1.5771428571428572, "no_speech_prob": 1.4970880329201464e-05}, {"id": 430, "seek": 222464, "start": 2224.64, "end": 2231.96, "text": " So then another thing about this is I kind of tried to make tabular look a lot like data", "tokens": [407, 550, 1071, 551, 466, 341, 307, 286, 733, 295, 3031, 281, 652, 4421, 1040, 574, 257, 688, 411, 1412], "temperature": 0.0, "avg_logprob": -0.1503080141426313, "compression_ratio": 1.6782608695652175, "no_speech_prob": 2.260307837786968e-06}, {"id": 431, "seek": 222464, "start": 2231.96, "end": 2233.04, "text": " frame.", "tokens": [3920, 13], "temperature": 0.0, "avg_logprob": -0.1503080141426313, "compression_ratio": 1.6782608695652175, "no_speech_prob": 2.260307837786968e-06}, {"id": 432, "seek": 222464, "start": 2233.04, "end": 2239.66, "text": " And one way that happens is we've inherited from getAtra, which means that any unknown", "tokens": [400, 472, 636, 300, 2314, 307, 321, 600, 27091, 490, 483, 18684, 424, 11, 597, 1355, 300, 604, 9841], "temperature": 0.0, "avg_logprob": -0.1503080141426313, "compression_ratio": 1.6782608695652175, "no_speech_prob": 2.260307837786968e-06}, {"id": 433, "seek": 222464, "start": 2239.66, "end": 2247.2, "text": " attributes it's going to pass down to whatever is the default property, which is self.items,", "tokens": [17212, 309, 311, 516, 281, 1320, 760, 281, 2035, 307, 264, 7576, 4707, 11, 597, 307, 2698, 13, 270, 9097, 11], "temperature": 0.0, "avg_logprob": -0.1503080141426313, "compression_ratio": 1.6782608695652175, "no_speech_prob": 2.260307837786968e-06}, {"id": 434, "seek": 222464, "start": 2247.2, "end": 2248.8799999999997, "text": " which is a data frame.", "tokens": [597, 307, 257, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.1503080141426313, "compression_ratio": 1.6782608695652175, "no_speech_prob": 2.260307837786968e-06}, {"id": 435, "seek": 222464, "start": 2248.8799999999997, "end": 2252.8399999999997, "text": " So in other words, it behaves a lot like a data frame because anything unknown, it will", "tokens": [407, 294, 661, 2283, 11, 309, 36896, 257, 688, 411, 257, 1412, 3920, 570, 1340, 9841, 11, 309, 486], "temperature": 0.0, "avg_logprob": -0.1503080141426313, "compression_ratio": 1.6782608695652175, "no_speech_prob": 2.260307837786968e-06}, {"id": 436, "seek": 225284, "start": 2252.84, "end": 2256.96, "text": " actually pass it along to the data frame.", "tokens": [767, 1320, 309, 2051, 281, 264, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.1353994930491728, "compression_ratio": 1.7751479289940828, "no_speech_prob": 4.6644456119793176e-07}, {"id": 437, "seek": 225284, "start": 2256.96, "end": 2265.7200000000003, "text": " But one thing I did want to change is in data frames, it's not convenient to index into", "tokens": [583, 472, 551, 286, 630, 528, 281, 1319, 307, 294, 1412, 12083, 11, 309, 311, 406, 10851, 281, 8186, 666], "temperature": 0.0, "avg_logprob": -0.1353994930491728, "compression_ratio": 1.7751479289940828, "no_speech_prob": 4.6644456119793176e-07}, {"id": 438, "seek": 225284, "start": 2265.7200000000003, "end": 2269.78, "text": " a row by row number and a column by name.", "tokens": [257, 5386, 538, 5386, 1230, 293, 257, 7738, 538, 1315, 13], "temperature": 0.0, "avg_logprob": -0.1353994930491728, "compression_ratio": 1.7751479289940828, "no_speech_prob": 4.6644456119793176e-07}, {"id": 439, "seek": 225284, "start": 2269.78, "end": 2274.2000000000003, "text": " You can use iloc to get row by number, column by number.", "tokens": [509, 393, 764, 1930, 905, 281, 483, 5386, 538, 1230, 11, 7738, 538, 1230, 13], "temperature": 0.0, "avg_logprob": -0.1353994930491728, "compression_ratio": 1.7751479289940828, "no_speech_prob": 4.6644456119793176e-07}, {"id": 440, "seek": 225284, "start": 2274.2000000000003, "end": 2280.88, "text": " You can do loc to say row by name or index and column by name or index.", "tokens": [509, 393, 360, 1628, 281, 584, 5386, 538, 1315, 420, 8186, 293, 7738, 538, 1315, 420, 8186, 13], "temperature": 0.0, "avg_logprob": -0.1353994930491728, "compression_ratio": 1.7751479289940828, "no_speech_prob": 4.6644456119793176e-07}, {"id": 441, "seek": 228088, "start": 2280.88, "end": 2285.84, "text": " But most of the time I want to use row numbers and column names.", "tokens": [583, 881, 295, 264, 565, 286, 528, 281, 764, 5386, 3547, 293, 7738, 5288, 13], "temperature": 0.0, "avg_logprob": -0.115789495408535, "compression_ratio": 1.4539473684210527, "no_speech_prob": 4.052540703014529e-07}, {"id": 442, "seek": 228088, "start": 2285.84, "end": 2295.06, "text": " So we redefine iloc to use this tabular iloc indexer, which is here.", "tokens": [407, 321, 38818, 533, 1930, 905, 281, 764, 341, 4421, 1040, 1930, 905, 8186, 260, 11, 597, 307, 510, 13], "temperature": 0.0, "avg_logprob": -0.115789495408535, "compression_ratio": 1.4539473684210527, "no_speech_prob": 4.052540703014529e-07}, {"id": 443, "seek": 228088, "start": 2295.06, "end": 2303.84, "text": " And as you can see, if you have a row and a column, then the columns I actually replace", "tokens": [400, 382, 291, 393, 536, 11, 498, 291, 362, 257, 5386, 293, 257, 7738, 11, 550, 264, 13766, 286, 767, 7406], "temperature": 0.0, "avg_logprob": -0.115789495408535, "compression_ratio": 1.4539473684210527, "no_speech_prob": 4.052540703014529e-07}, {"id": 444, "seek": 230384, "start": 2303.84, "end": 2316.88, "text": " with the integer index of the column, so that way we can use column names and row numbers.", "tokens": [365, 264, 24922, 8186, 295, 264, 7738, 11, 370, 300, 636, 321, 393, 764, 7738, 5288, 293, 5386, 3547, 13], "temperature": 0.0, "avg_logprob": -0.1380211247338189, "compression_ratio": 1.6233766233766234, "no_speech_prob": 7.338184673244541e-07}, {"id": 445, "seek": 230384, "start": 2316.88, "end": 2322.2000000000003, "text": " And also it will wrap it back up in a tabular object as well.", "tokens": [400, 611, 309, 486, 7019, 309, 646, 493, 294, 257, 4421, 1040, 2657, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1380211247338189, "compression_ratio": 1.6233766233766234, "no_speech_prob": 7.338184673244541e-07}, {"id": 446, "seek": 230384, "start": 2322.2000000000003, "end": 2329.52, "text": " So we end up with if you index into a tabular object with iloc, you'll get back a tabular", "tokens": [407, 321, 917, 493, 365, 498, 291, 8186, 666, 257, 4421, 1040, 2657, 365, 1930, 905, 11, 291, 603, 483, 646, 257, 4421, 1040], "temperature": 0.0, "avg_logprob": -0.1380211247338189, "compression_ratio": 1.6233766233766234, "no_speech_prob": 7.338184673244541e-07}, {"id": 447, "seek": 230384, "start": 2329.52, "end": 2333.56, "text": " object.", "tokens": [2657, 13], "temperature": 0.0, "avg_logprob": -0.1380211247338189, "compression_ratio": 1.6233766233766234, "no_speech_prob": 7.338184673244541e-07}, {"id": 448, "seek": 233356, "start": 2333.56, "end": 2344.52, "text": " So then the way Categorify is implemented, as you see in encodes, is it calls transform,", "tokens": [407, 550, 264, 636, 383, 2968, 284, 2505, 307, 12270, 11, 382, 291, 536, 294, 2058, 4789, 11, 307, 309, 5498, 4088, 11], "temperature": 0.0, "avg_logprob": -0.20400559029928067, "compression_ratio": 1.6185567010309279, "no_speech_prob": 2.64256345872127e-06}, {"id": 449, "seek": 233356, "start": 2344.52, "end": 2348.08, "text": " passing in a bunch of column names and a function.", "tokens": [8437, 294, 257, 3840, 295, 7738, 5288, 293, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.20400559029928067, "compression_ratio": 1.6185567010309279, "no_speech_prob": 2.64256345872127e-06}, {"id": 450, "seek": 233356, "start": 2348.08, "end": 2354.84, "text": " The function applyCats is the thing we saw before, which is the thing that calls map,", "tokens": [440, 2445, 3079, 34, 1720, 307, 264, 551, 321, 1866, 949, 11, 597, 307, 264, 551, 300, 5498, 4471, 11], "temperature": 0.0, "avg_logprob": -0.20400559029928067, "compression_ratio": 1.6185567010309279, "no_speech_prob": 2.64256345872127e-06}, {"id": 451, "seek": 233356, "start": 2354.84, "end": 2360.48, "text": " unless you have a pandas categorical column, in which case you actually already have the", "tokens": [5969, 291, 362, 257, 4565, 296, 19250, 804, 7738, 11, 294, 597, 1389, 291, 767, 1217, 362, 264], "temperature": 0.0, "avg_logprob": -0.20400559029928067, "compression_ratio": 1.6185567010309279, "no_speech_prob": 2.64256345872127e-06}, {"id": 452, "seek": 236048, "start": 2360.48, "end": 2367.44, "text": " pandas has done the coding for you, so you just return it, just cat.codes plus one.", "tokens": [4565, 296, 575, 1096, 264, 17720, 337, 291, 11, 370, 291, 445, 2736, 309, 11, 445, 3857, 13, 66, 4789, 1804, 472, 13], "temperature": 0.0, "avg_logprob": -0.15344361413883256, "compression_ratio": 1.471794871794872, "no_speech_prob": 2.857305844372604e-06}, {"id": 453, "seek": 236048, "start": 2367.44, "end": 2376.8, "text": " And so how does this function get applied to each of these columns?", "tokens": [400, 370, 577, 775, 341, 2445, 483, 6456, 281, 1184, 295, 613, 13766, 30], "temperature": 0.0, "avg_logprob": -0.15344361413883256, "compression_ratio": 1.471794871794872, "no_speech_prob": 2.857305844372604e-06}, {"id": 454, "seek": 236048, "start": 2376.8, "end": 2382.84, "text": " That's because we have a thing called tabulaObject.transform, and that's the thing that at the moment is", "tokens": [663, 311, 570, 321, 362, 257, 551, 1219, 4421, 3780, 45483, 1020, 13, 24999, 837, 11, 293, 300, 311, 264, 551, 300, 412, 264, 1623, 307], "temperature": 0.0, "avg_logprob": -0.15344361413883256, "compression_ratio": 1.471794871794872, "no_speech_prob": 2.857305844372604e-06}, {"id": 455, "seek": 236048, "start": 2382.84, "end": 2387.56, "text": " defined explicitly for pandas.", "tokens": [7642, 20803, 337, 4565, 296, 13], "temperature": 0.0, "avg_logprob": -0.15344361413883256, "compression_ratio": 1.471794871794872, "no_speech_prob": 2.857305844372604e-06}, {"id": 456, "seek": 238756, "start": 2387.56, "end": 2392.68, "text": " And as you can see, for pandas it just is this column equals the transformed version", "tokens": [400, 382, 291, 393, 536, 11, 337, 4565, 296, 309, 445, 307, 341, 7738, 6915, 264, 16894, 3037], "temperature": 0.0, "avg_logprob": -0.28943271107143825, "compression_ratio": 1.5263157894736843, "no_speech_prob": 3.3405042358936043e-06}, {"id": 457, "seek": 238756, "start": 2392.68, "end": 2400.24, "text": " of this column, because pandas has a.transform method for a series.", "tokens": [295, 341, 7738, 11, 570, 4565, 296, 575, 257, 2411, 24999, 837, 3170, 337, 257, 2638, 13], "temperature": 0.0, "avg_logprob": -0.28943271107143825, "compression_ratio": 1.5263157894736843, "no_speech_prob": 3.3405042358936043e-06}, {"id": 458, "seek": 238756, "start": 2400.24, "end": 2407.52, "text": " So that's all we needed to do there.", "tokens": [407, 300, 311, 439, 321, 2978, 281, 360, 456, 13], "temperature": 0.0, "avg_logprob": -0.28943271107143825, "compression_ratio": 1.5263157894736843, "no_speech_prob": 3.3405042358936043e-06}, {"id": 459, "seek": 238756, "start": 2407.52, "end": 2412.6, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.28943271107143825, "compression_ratio": 1.5263157894736843, "no_speech_prob": 3.3405042358936043e-06}, {"id": 460, "seek": 238756, "start": 2412.6, "end": 2416.2, "text": " So Juvian, your question there about numerical versus continuous.", "tokens": [407, 13582, 85, 952, 11, 428, 1168, 456, 466, 29054, 5717, 10957, 13], "temperature": 0.0, "avg_logprob": -0.28943271107143825, "compression_ratio": 1.5263157894736843, "no_speech_prob": 3.3405042358936043e-06}, {"id": 461, "seek": 241620, "start": 2416.2, "end": 2421.9199999999996, "text": " You should watch the introduction to machine learning for coders course where we talk about", "tokens": [509, 820, 1159, 264, 9339, 281, 3479, 2539, 337, 17656, 433, 1164, 689, 321, 751, 466], "temperature": 0.0, "avg_logprob": -0.22273901234502377, "compression_ratio": 1.554054054054054, "no_speech_prob": 3.6119524793321034e-06}, {"id": 462, "seek": 241620, "start": 2421.9199999999996, "end": 2423.8799999999997, "text": " that in a lot of detail.", "tokens": [300, 294, 257, 688, 295, 2607, 13], "temperature": 0.0, "avg_logprob": -0.22273901234502377, "compression_ratio": 1.554054054054054, "no_speech_prob": 3.6119524793321034e-06}, {"id": 463, "seek": 241620, "start": 2423.8799999999997, "end": 2428.64, "text": " That's something you probably have time to cover here.", "tokens": [663, 311, 746, 291, 1391, 362, 565, 281, 2060, 510, 13], "temperature": 0.0, "avg_logprob": -0.22273901234502377, "compression_ratio": 1.554054054054054, "no_speech_prob": 3.6119524793321034e-06}, {"id": 464, "seek": 241620, "start": 2428.64, "end": 2434.08, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.22273901234502377, "compression_ratio": 1.554054054054054, "no_speech_prob": 3.6119524793321034e-06}, {"id": 465, "seek": 241620, "start": 2434.08, "end": 2435.8399999999997, "text": " So that's categorify.", "tokens": [407, 300, 311, 19250, 2505, 13], "temperature": 0.0, "avg_logprob": -0.22273901234502377, "compression_ratio": 1.554054054054054, "no_speech_prob": 3.6119524793321034e-06}, {"id": 466, "seek": 241620, "start": 2435.8399999999997, "end": 2442.0, "text": " So here's the other way to use categorify, as I was kind of beginning to mention, is", "tokens": [407, 510, 311, 264, 661, 636, 281, 764, 19250, 2505, 11, 382, 286, 390, 733, 295, 2863, 281, 2152, 11, 307], "temperature": 0.0, "avg_logprob": -0.22273901234502377, "compression_ratio": 1.554054054054054, "no_speech_prob": 3.6119524793321034e-06}, {"id": 467, "seek": 241620, "start": 2442.0, "end": 2445.56, "text": " you can actually create a categorical column in pandas.", "tokens": [291, 393, 767, 1884, 257, 19250, 804, 7738, 294, 4565, 296, 13], "temperature": 0.0, "avg_logprob": -0.22273901234502377, "compression_ratio": 1.554054054054054, "no_speech_prob": 3.6119524793321034e-06}, {"id": 468, "seek": 244556, "start": 2445.56, "end": 2449.12, "text": " And one of the reasons to do that is so that you can say, these are the categories I want", "tokens": [400, 472, 295, 264, 4112, 281, 360, 300, 307, 370, 300, 291, 393, 584, 11, 613, 366, 264, 10479, 286, 528], "temperature": 0.0, "avg_logprob": -0.08668733897962068, "compression_ratio": 1.6177777777777778, "no_speech_prob": 3.187540187354898e-06}, {"id": 469, "seek": 244556, "start": 2449.12, "end": 2451.7999999999997, "text": " to use and they have an order.", "tokens": [281, 764, 293, 436, 362, 364, 1668, 13], "temperature": 0.0, "avg_logprob": -0.08668733897962068, "compression_ratio": 1.6177777777777778, "no_speech_prob": 3.187540187354898e-06}, {"id": 470, "seek": 244556, "start": 2451.7999999999997, "end": 2458.32, "text": " And so that way high, medium, low will now be ordered correctly, which is super useful.", "tokens": [400, 370, 300, 636, 1090, 11, 6399, 11, 2295, 486, 586, 312, 8866, 8944, 11, 597, 307, 1687, 4420, 13], "temperature": 0.0, "avg_logprob": -0.08668733897962068, "compression_ratio": 1.6177777777777778, "no_speech_prob": 3.187540187354898e-06}, {"id": 471, "seek": 244556, "start": 2458.32, "end": 2464.2, "text": " Also pandas is just nice and efficient at dealing with categories.", "tokens": [2743, 4565, 296, 307, 445, 1481, 293, 7148, 412, 6260, 365, 10479, 13], "temperature": 0.0, "avg_logprob": -0.08668733897962068, "compression_ratio": 1.6177777777777778, "no_speech_prob": 3.187540187354898e-06}, {"id": 472, "seek": 244556, "start": 2464.2, "end": 2471.08, "text": " So now if we go categorify just like before, it's going to give us exactly the same kind", "tokens": [407, 586, 498, 321, 352, 19250, 2505, 445, 411, 949, 11, 309, 311, 516, 281, 976, 505, 2293, 264, 912, 733], "temperature": 0.0, "avg_logprob": -0.08668733897962068, "compression_ratio": 1.6177777777777778, "no_speech_prob": 3.187540187354898e-06}, {"id": 473, "seek": 247108, "start": 2471.08, "end": 2479.08, "text": " of results, except that when we look at the categorical processor, these will be in the", "tokens": [295, 3542, 11, 3993, 300, 562, 321, 574, 412, 264, 19250, 804, 15321, 11, 613, 486, 312, 294, 264], "temperature": 0.0, "avg_logprob": -0.26602709811666736, "compression_ratio": 1.5091743119266054, "no_speech_prob": 1.349689568996837e-06}, {"id": 474, "seek": 247108, "start": 2479.08, "end": 2480.52, "text": " right order.", "tokens": [558, 1668, 13], "temperature": 0.0, "avg_logprob": -0.26602709811666736, "compression_ratio": 1.5091743119266054, "no_speech_prob": 1.349689568996837e-06}, {"id": 475, "seek": 247108, "start": 2480.52, "end": 2484.84, "text": " So we're going to end up with things that have been mapped in that way.", "tokens": [407, 321, 434, 516, 281, 917, 493, 365, 721, 300, 362, 668, 33318, 294, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.26602709811666736, "compression_ratio": 1.5091743119266054, "no_speech_prob": 1.349689568996837e-06}, {"id": 476, "seek": 247108, "start": 2484.84, "end": 2492.84, "text": " And it also be done potentially more efficiently because it's using the internal pandas catcode", "tokens": [400, 309, 611, 312, 1096, 7263, 544, 19621, 570, 309, 311, 1228, 264, 6920, 4565, 296, 3857, 22332], "temperature": 0.0, "avg_logprob": -0.26602709811666736, "compression_ratio": 1.5091743119266054, "no_speech_prob": 1.349689568996837e-06}, {"id": 477, "seek": 247108, "start": 2492.84, "end": 2493.84, "text": " stuff.", "tokens": [1507, 13], "temperature": 0.0, "avg_logprob": -0.26602709811666736, "compression_ratio": 1.5091743119266054, "no_speech_prob": 1.349689568996837e-06}, {"id": 478, "seek": 247108, "start": 2493.84, "end": 2495.44, "text": " Thank you, David.", "tokens": [1044, 291, 11, 4389, 13], "temperature": 0.0, "avg_logprob": -0.26602709811666736, "compression_ratio": 1.5091743119266054, "no_speech_prob": 1.349689568996837e-06}, {"id": 479, "seek": 247108, "start": 2495.44, "end": 2497.64, "text": " That is very kind.", "tokens": [663, 307, 588, 733, 13], "temperature": 0.0, "avg_logprob": -0.26602709811666736, "compression_ratio": 1.5091743119266054, "no_speech_prob": 1.349689568996837e-06}, {"id": 480, "seek": 247108, "start": 2497.64, "end": 2498.64, "text": " I'm so thrilled.", "tokens": [286, 478, 370, 18744, 13], "temperature": 0.0, "avg_logprob": -0.26602709811666736, "compression_ratio": 1.5091743119266054, "no_speech_prob": 1.349689568996837e-06}, {"id": 481, "seek": 249864, "start": 2498.64, "end": 2503.72, "text": " I would love to know where you are working as a computer vision data scientist.", "tokens": [286, 576, 959, 281, 458, 689, 291, 366, 1364, 382, 257, 3820, 5201, 1412, 12662, 13], "temperature": 0.0, "avg_logprob": -0.3381295261612858, "compression_ratio": 1.5721649484536082, "no_speech_prob": 1.5444964446942322e-05}, {"id": 482, "seek": 249864, "start": 2503.72, "end": 2510.16, "text": " It's a very cool job.", "tokens": [467, 311, 257, 588, 1627, 1691, 13], "temperature": 0.0, "avg_logprob": -0.3381295261612858, "compression_ratio": 1.5721649484536082, "no_speech_prob": 1.5444964446942322e-05}, {"id": 483, "seek": 249864, "start": 2510.16, "end": 2516.0, "text": " Andrew was starting his job as the in-house research director and data scientist here", "tokens": [10110, 390, 2891, 702, 1691, 382, 264, 294, 12, 6410, 2132, 5391, 293, 1412, 12662, 510], "temperature": 0.0, "avg_logprob": -0.3381295261612858, "compression_ratio": 1.5721649484536082, "no_speech_prob": 1.5444964446942322e-05}, {"id": 484, "seek": 249864, "start": 2516.0, "end": 2518.04, "text": " in 10 days time.", "tokens": [294, 1266, 1708, 565, 13], "temperature": 0.0, "avg_logprob": -0.3381295261612858, "compression_ratio": 1.5721649484536082, "no_speech_prob": 1.5444964446942322e-05}, {"id": 485, "seek": 249864, "start": 2518.04, "end": 2521.52, "text": " And also, two years ago.", "tokens": [400, 611, 11, 732, 924, 2057, 13], "temperature": 0.0, "avg_logprob": -0.3381295261612858, "compression_ratio": 1.5721649484536082, "no_speech_prob": 1.5444964446942322e-05}, {"id": 486, "seek": 249864, "start": 2521.52, "end": 2528.2, "text": " And working very, very, very, very hard on being amazing, which also helps.", "tokens": [400, 1364, 588, 11, 588, 11, 588, 11, 588, 1152, 322, 885, 2243, 11, 597, 611, 3665, 13], "temperature": 0.0, "avg_logprob": -0.3381295261612858, "compression_ratio": 1.5721649484536082, "no_speech_prob": 1.5444964446942322e-05}, {"id": 487, "seek": 252820, "start": 2528.2, "end": 2534.68, "text": " Lots of people do the course and don't end up with great jobs because they don't work", "tokens": [15908, 295, 561, 360, 264, 1164, 293, 500, 380, 917, 493, 365, 869, 4782, 570, 436, 500, 380, 589], "temperature": 0.0, "avg_logprob": -0.23001295944740033, "compression_ratio": 1.5, "no_speech_prob": 7.41099938750267e-06}, {"id": 488, "seek": 252820, "start": 2534.68, "end": 2535.68, "text": " as hard.", "tokens": [382, 1152, 13], "temperature": 0.0, "avg_logprob": -0.23001295944740033, "compression_ratio": 1.5, "no_speech_prob": 7.41099938750267e-06}, {"id": 489, "seek": 252820, "start": 2535.68, "end": 2541.68, "text": " Although I think everybody listening to this by definition is going to an extra level of", "tokens": [5780, 286, 519, 2201, 4764, 281, 341, 538, 7123, 307, 516, 281, 364, 2857, 1496, 295], "temperature": 0.0, "avg_logprob": -0.23001295944740033, "compression_ratio": 1.5, "no_speech_prob": 7.41099938750267e-06}, {"id": 490, "seek": 252820, "start": 2541.68, "end": 2542.68, "text": " effort.", "tokens": [4630, 13], "temperature": 0.0, "avg_logprob": -0.23001295944740033, "compression_ratio": 1.5, "no_speech_prob": 7.41099938750267e-06}, {"id": 491, "seek": 252820, "start": 2542.68, "end": 2547.3199999999997, "text": " So I'm sure you will all do great.", "tokens": [407, 286, 478, 988, 291, 486, 439, 360, 869, 13], "temperature": 0.0, "avg_logprob": -0.23001295944740033, "compression_ratio": 1.5, "no_speech_prob": 7.41099938750267e-06}, {"id": 492, "seek": 252820, "start": 2547.3199999999997, "end": 2553.6, "text": " Normalize is just something where we're going to subtract the means and divide by the standard", "tokens": [21277, 1125, 307, 445, 746, 689, 321, 434, 516, 281, 16390, 264, 1355, 293, 9845, 538, 264, 3832], "temperature": 0.0, "avg_logprob": -0.23001295944740033, "compression_ratio": 1.5, "no_speech_prob": 7.41099938750267e-06}, {"id": 493, "seek": 252820, "start": 2553.6, "end": 2554.6, "text": " deviations.", "tokens": [31219, 763, 13], "temperature": 0.0, "avg_logprob": -0.23001295944740033, "compression_ratio": 1.5, "no_speech_prob": 7.41099938750267e-06}, {"id": 494, "seek": 255460, "start": 2554.6, "end": 2561.08, "text": " And the decodes, we're going to do the opposite.", "tokens": [400, 264, 979, 4789, 11, 321, 434, 516, 281, 360, 264, 6182, 13], "temperature": 0.0, "avg_logprob": -0.27655439023618344, "compression_ratio": 1.5562130177514792, "no_speech_prob": 9.665215657150839e-06}, {"id": 495, "seek": 255460, "start": 2561.08, "end": 2563.56, "text": " And you'll see what we generally do in these setup things.", "tokens": [400, 291, 603, 536, 437, 321, 5101, 360, 294, 613, 8657, 721, 13], "temperature": 0.0, "avg_logprob": -0.27655439023618344, "compression_ratio": 1.5562130177514792, "no_speech_prob": 9.665215657150839e-06}, {"id": 496, "seek": 255460, "start": 2563.56, "end": 2565.6, "text": " We do this in lots of places.", "tokens": [492, 360, 341, 294, 3195, 295, 3190, 13], "temperature": 0.0, "avg_logprob": -0.27655439023618344, "compression_ratio": 1.5562130177514792, "no_speech_prob": 9.665215657150839e-06}, {"id": 497, "seek": 255460, "start": 2565.6, "end": 2573.2799999999997, "text": " As we say, get atra, data source, comma, train, comma, data source.", "tokens": [1018, 321, 584, 11, 483, 412, 424, 11, 1412, 4009, 11, 22117, 11, 3847, 11, 22117, 11, 1412, 4009, 13], "temperature": 0.0, "avg_logprob": -0.27655439023618344, "compression_ratio": 1.5562130177514792, "no_speech_prob": 9.665215657150839e-06}, {"id": 498, "seek": 255460, "start": 2573.2799999999997, "end": 2578.04, "text": " What this means is it's the same as if we'd written this.", "tokens": [708, 341, 1355, 307, 309, 311, 264, 912, 382, 498, 321, 1116, 3720, 341, 13], "temperature": 0.0, "avg_logprob": -0.27655439023618344, "compression_ratio": 1.5562130177514792, "no_speech_prob": 9.665215657150839e-06}, {"id": 499, "seek": 257804, "start": 2578.04, "end": 2595.16, "text": " If dsrc.train if has atra dsrc, train, else dsrc.", "tokens": [759, 274, 82, 81, 66, 13, 83, 7146, 498, 575, 412, 424, 274, 82, 81, 66, 11, 3847, 11, 1646, 274, 82, 81, 66, 13], "temperature": 0.0, "avg_logprob": -0.2593033083023564, "compression_ratio": 1.3821138211382114, "no_speech_prob": 2.7264393338555237e-06}, {"id": 500, "seek": 257804, "start": 2595.16, "end": 2597.48, "text": " It's the same as writing that.", "tokens": [467, 311, 264, 912, 382, 3579, 300, 13], "temperature": 0.0, "avg_logprob": -0.2593033083023564, "compression_ratio": 1.3821138211382114, "no_speech_prob": 2.7264393338555237e-06}, {"id": 501, "seek": 257804, "start": 2597.48, "end": 2606.68, "text": " And so the reason we're doing that is because we want you to be able to either pass in an", "tokens": [400, 370, 264, 1778, 321, 434, 884, 300, 307, 570, 321, 528, 291, 281, 312, 1075, 281, 2139, 1320, 294, 364], "temperature": 0.0, "avg_logprob": -0.2593033083023564, "compression_ratio": 1.3821138211382114, "no_speech_prob": 2.7264393338555237e-06}, {"id": 502, "seek": 260668, "start": 2606.68, "end": 2612.3999999999996, "text": " actual data source object, which has a train and a valid, or not.", "tokens": [3539, 1412, 4009, 2657, 11, 597, 575, 257, 3847, 293, 257, 7363, 11, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.10221123695373535, "compression_ratio": 1.6421052631578947, "no_speech_prob": 2.52155177804525e-06}, {"id": 503, "seek": 260668, "start": 2612.3999999999996, "end": 2618.2799999999997, "text": " If you aren't doing separate things, a train and valid, then that should be fine as well.", "tokens": [759, 291, 3212, 380, 884, 4994, 721, 11, 257, 3847, 293, 7363, 11, 550, 300, 820, 312, 2489, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.10221123695373535, "compression_ratio": 1.6421052631578947, "no_speech_prob": 2.52155177804525e-06}, {"id": 504, "seek": 260668, "start": 2618.2799999999997, "end": 2625.12, "text": " And so as long as the thing you pass in, if it does have a train, then it should give", "tokens": [400, 370, 382, 938, 382, 264, 551, 291, 1320, 294, 11, 498, 309, 775, 362, 257, 3847, 11, 550, 309, 820, 976], "temperature": 0.0, "avg_logprob": -0.10221123695373535, "compression_ratio": 1.6421052631578947, "no_speech_prob": 2.52155177804525e-06}, {"id": 505, "seek": 260668, "start": 2625.12, "end": 2630.7999999999997, "text": " you back some kind of object that has the right methods that you need.", "tokens": [291, 646, 512, 733, 295, 2657, 300, 575, 264, 558, 7150, 300, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.10221123695373535, "compression_ratio": 1.6421052631578947, "no_speech_prob": 2.52155177804525e-06}, {"id": 506, "seek": 263080, "start": 2630.8, "end": 2636.5600000000004, "text": " So in the case of data source and tabular, it will.", "tokens": [407, 294, 264, 1389, 295, 1412, 4009, 293, 4421, 1040, 11, 309, 486, 13], "temperature": 0.0, "avg_logprob": -0.12615103721618653, "compression_ratio": 1.6941176470588235, "no_speech_prob": 1.4144706028673681e-06}, {"id": 507, "seek": 263080, "start": 2636.5600000000004, "end": 2642.96, "text": " Data source has a train attribute that will return a tabular object.", "tokens": [11888, 4009, 575, 257, 3847, 19667, 300, 486, 2736, 257, 4421, 1040, 2657, 13], "temperature": 0.0, "avg_logprob": -0.12615103721618653, "compression_ratio": 1.6941176470588235, "no_speech_prob": 1.4144706028673681e-06}, {"id": 508, "seek": 263080, "start": 2642.96, "end": 2647.44, "text": " Or if you just pass in a tabular object directly, then it won't have a train.", "tokens": [1610, 498, 291, 445, 1320, 294, 257, 4421, 1040, 2657, 3838, 11, 550, 309, 1582, 380, 362, 257, 3847, 13], "temperature": 0.0, "avg_logprob": -0.12615103721618653, "compression_ratio": 1.6941176470588235, "no_speech_prob": 1.4144706028673681e-06}, {"id": 509, "seek": 263080, "start": 2647.44, "end": 2659.44, "text": " So dsrc will just be a tabular object, which has a continuous variables, const attribute.", "tokens": [407, 274, 82, 81, 66, 486, 445, 312, 257, 4421, 1040, 2657, 11, 597, 575, 257, 10957, 9102, 11, 1817, 19667, 13], "temperature": 0.0, "avg_logprob": -0.12615103721618653, "compression_ratio": 1.6941176470588235, "no_speech_prob": 1.4144706028673681e-06}, {"id": 510, "seek": 265944, "start": 2659.44, "end": 2665.8, "text": " So now we have this data frame containing just the continuous variables and optionally", "tokens": [407, 586, 321, 362, 341, 1412, 3920, 19273, 445, 264, 10957, 9102, 293, 3614, 379], "temperature": 0.0, "avg_logprob": -0.1609888847428139, "compression_ratio": 1.7477064220183487, "no_speech_prob": 2.8129761631134897e-06}, {"id": 511, "seek": 265944, "start": 2665.8, "end": 2668.68, "text": " just for our training set, if we have one.", "tokens": [445, 337, 527, 3097, 992, 11, 498, 321, 362, 472, 13], "temperature": 0.0, "avg_logprob": -0.1609888847428139, "compression_ratio": 1.7477064220183487, "no_speech_prob": 2.8129761631134897e-06}, {"id": 512, "seek": 265944, "start": 2668.68, "end": 2672.56, "text": " So then we can set up our means and standard deviations.", "tokens": [407, 550, 321, 393, 992, 493, 527, 1355, 293, 3832, 31219, 763, 13], "temperature": 0.0, "avg_logprob": -0.1609888847428139, "compression_ratio": 1.7477064220183487, "no_speech_prob": 2.8129761631134897e-06}, {"id": 513, "seek": 265944, "start": 2672.56, "end": 2675.6, "text": " That's the metadata we need.", "tokens": [663, 311, 264, 26603, 321, 643, 13], "temperature": 0.0, "avg_logprob": -0.1609888847428139, "compression_ratio": 1.7477064220183487, "no_speech_prob": 2.8129761631134897e-06}, {"id": 514, "seek": 265944, "start": 2675.6, "end": 2681.96, "text": " So here we can create our normalize, create a data frame to test it out on, pass in that", "tokens": [407, 510, 321, 393, 1884, 527, 2710, 1125, 11, 1884, 257, 1412, 3920, 281, 1500, 309, 484, 322, 11, 1320, 294, 300], "temperature": 0.0, "avg_logprob": -0.1609888847428139, "compression_ratio": 1.7477064220183487, "no_speech_prob": 2.8129761631134897e-06}, {"id": 515, "seek": 265944, "start": 2681.96, "end": 2682.96, "text": " processor.", "tokens": [15321, 13], "temperature": 0.0, "avg_logprob": -0.1609888847428139, "compression_ratio": 1.7477064220183487, "no_speech_prob": 2.8129761631134897e-06}, {"id": 516, "seek": 265944, "start": 2682.96, "end": 2685.8, "text": " This time we're just going to say these are continuous variables.", "tokens": [639, 565, 321, 434, 445, 516, 281, 584, 613, 366, 10957, 9102, 13], "temperature": 0.0, "avg_logprob": -0.1609888847428139, "compression_ratio": 1.7477064220183487, "no_speech_prob": 2.8129761631134897e-06}, {"id": 517, "seek": 268580, "start": 2685.8, "end": 2690.5600000000004, "text": " This is one which is a, make sure we call setup.", "tokens": [639, 307, 472, 597, 307, 257, 11, 652, 988, 321, 818, 8657, 13], "temperature": 0.0, "avg_logprob": -0.20023491165854715, "compression_ratio": 1.5968586387434556, "no_speech_prob": 1.577955117681995e-06}, {"id": 518, "seek": 268580, "start": 2690.5600000000004, "end": 2697.88, "text": " And so now we should find here is the same data that we had before, but let's make it", "tokens": [400, 370, 586, 321, 820, 915, 510, 307, 264, 912, 1412, 300, 321, 632, 949, 11, 457, 718, 311, 652, 309], "temperature": 0.0, "avg_logprob": -0.20023491165854715, "compression_ratio": 1.5968586387434556, "no_speech_prob": 1.577955117681995e-06}, {"id": 519, "seek": 268580, "start": 2697.88, "end": 2702.1600000000003, "text": " into an array for testing and calculate the standard deviation.", "tokens": [666, 364, 10225, 337, 4997, 293, 8873, 264, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.20023491165854715, "compression_ratio": 1.5968586387434556, "no_speech_prob": 1.577955117681995e-06}, {"id": 520, "seek": 268580, "start": 2702.1600000000003, "end": 2707.9, "text": " So we should find if we go norm dot means a.", "tokens": [407, 321, 820, 915, 498, 321, 352, 2026, 5893, 1355, 257, 13], "temperature": 0.0, "avg_logprob": -0.20023491165854715, "compression_ratio": 1.5968586387434556, "no_speech_prob": 1.577955117681995e-06}, {"id": 521, "seek": 268580, "start": 2707.9, "end": 2710.7200000000003, "text": " So self dot means was df dot mean.", "tokens": [407, 2698, 5893, 1355, 390, 274, 69, 5893, 914, 13], "temperature": 0.0, "avg_logprob": -0.20023491165854715, "compression_ratio": 1.5968586387434556, "no_speech_prob": 1.577955117681995e-06}, {"id": 522, "seek": 268580, "start": 2710.7200000000003, "end": 2711.7200000000003, "text": " This is quite nice, right?", "tokens": [639, 307, 1596, 1481, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20023491165854715, "compression_ratio": 1.5968586387434556, "no_speech_prob": 1.577955117681995e-06}, {"id": 523, "seek": 271172, "start": 2711.72, "end": 2717.7599999999998, "text": " So in pandas, if you call dot mean on a data frame, you will get back a, I think it's a", "tokens": [407, 294, 4565, 296, 11, 498, 291, 818, 5893, 914, 322, 257, 1412, 3920, 11, 291, 486, 483, 646, 257, 11, 286, 519, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.16811115988369646, "compression_ratio": 1.6680161943319838, "no_speech_prob": 1.7880597624753136e-06}, {"id": 524, "seek": 271172, "start": 2717.7599999999998, "end": 2722.0, "text": " series object, which you can index into with column names.", "tokens": [2638, 2657, 11, 597, 291, 393, 8186, 666, 365, 7738, 5288, 13], "temperature": 0.0, "avg_logprob": -0.16811115988369646, "compression_ratio": 1.6680161943319838, "no_speech_prob": 1.7880597624753136e-06}, {"id": 525, "seek": 271172, "start": 2722.0, "end": 2724.24, "text": " So this is quite neat, right?", "tokens": [407, 341, 307, 1596, 10654, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16811115988369646, "compression_ratio": 1.6680161943319838, "no_speech_prob": 1.7880597624753136e-06}, {"id": 526, "seek": 271172, "start": 2724.24, "end": 2727.2799999999997, "text": " That we were able to get all the means and standard deviations all at once for all the", "tokens": [663, 321, 645, 1075, 281, 483, 439, 264, 1355, 293, 3832, 31219, 763, 439, 412, 1564, 337, 439, 264], "temperature": 0.0, "avg_logprob": -0.16811115988369646, "compression_ratio": 1.6680161943319838, "no_speech_prob": 1.7880597624753136e-06}, {"id": 527, "seek": 271172, "start": 2727.2799999999997, "end": 2730.8399999999997, "text": " columns and even apply them to all the columns at once.", "tokens": [13766, 293, 754, 3079, 552, 281, 439, 264, 13766, 412, 1564, 13], "temperature": 0.0, "avg_logprob": -0.16811115988369646, "compression_ratio": 1.6680161943319838, "no_speech_prob": 1.7880597624753136e-06}, {"id": 528, "seek": 271172, "start": 2730.8399999999997, "end": 2734.8399999999997, "text": " This is kind of the magic of Python indexes.", "tokens": [639, 307, 733, 295, 264, 5585, 295, 15329, 8186, 279, 13], "temperature": 0.0, "avg_logprob": -0.16811115988369646, "compression_ratio": 1.6680161943319838, "no_speech_prob": 1.7880597624753136e-06}, {"id": 529, "seek": 271172, "start": 2734.8399999999997, "end": 2738.54, "text": " So I think that's, that's actually pretty nice.", "tokens": [407, 286, 519, 300, 311, 11, 300, 311, 767, 1238, 1481, 13], "temperature": 0.0, "avg_logprob": -0.16811115988369646, "compression_ratio": 1.6680161943319838, "no_speech_prob": 1.7880597624753136e-06}, {"id": 530, "seek": 273854, "start": 2738.54, "end": 2745.2799999999997, "text": " So yeah, make sure that the mean is m, standard deviation should be around s and the values", "tokens": [407, 1338, 11, 652, 988, 300, 264, 914, 307, 275, 11, 3832, 25163, 820, 312, 926, 262, 293, 264, 4190], "temperature": 0.0, "avg_logprob": -0.13809876075157754, "compression_ratio": 1.5031847133757963, "no_speech_prob": 6.2408726080320776e-06}, {"id": 531, "seek": 273854, "start": 2745.2799999999997, "end": 2751.06, "text": " after processing should be around x minus m over s.", "tokens": [934, 9007, 820, 312, 926, 2031, 3175, 275, 670, 262, 13], "temperature": 0.0, "avg_logprob": -0.13809876075157754, "compression_ratio": 1.5031847133757963, "no_speech_prob": 6.2408726080320776e-06}, {"id": 532, "seek": 273854, "start": 2751.06, "end": 2763.8, "text": " One thing to notice is that this here, setup, we didn't call here.", "tokens": [1485, 551, 281, 3449, 307, 300, 341, 510, 11, 8657, 11, 321, 994, 380, 818, 510, 13], "temperature": 0.0, "avg_logprob": -0.13809876075157754, "compression_ratio": 1.5031847133757963, "no_speech_prob": 6.2408726080320776e-06}, {"id": 533, "seek": 273854, "start": 2763.8, "end": 2765.7599999999998, "text": " Why didn't we call setup?", "tokens": [1545, 994, 380, 321, 818, 8657, 30], "temperature": 0.0, "avg_logprob": -0.13809876075157754, "compression_ratio": 1.5031847133757963, "no_speech_prob": 6.2408726080320776e-06}, {"id": 534, "seek": 276576, "start": 2765.76, "end": 2774.28, "text": " And the reason why is that if you look at data source, it calls setup.", "tokens": [400, 264, 1778, 983, 307, 300, 498, 291, 574, 412, 1412, 4009, 11, 309, 5498, 8657, 13], "temperature": 0.0, "avg_logprob": -0.1490420081398704, "compression_ratio": 1.7553648068669527, "no_speech_prob": 4.71086013931199e-06}, {"id": 535, "seek": 276576, "start": 2774.28, "end": 2776.4, "text": " Why?", "tokens": [1545, 30], "temperature": 0.0, "avg_logprob": -0.1490420081398704, "compression_ratio": 1.7553648068669527, "no_speech_prob": 4.71086013931199e-06}, {"id": 536, "seek": 276576, "start": 2776.4, "end": 2779.96, "text": " Because we now definitely have all the information we need to set it up, right?", "tokens": [1436, 321, 586, 2138, 362, 439, 264, 1589, 321, 643, 281, 992, 309, 493, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1490420081398704, "compression_ratio": 1.7553648068669527, "no_speech_prob": 4.71086013931199e-06}, {"id": 537, "seek": 276576, "start": 2779.96, "end": 2784.1200000000003, "text": " We know the data because that was a data frame you passed in and now we know what the training", "tokens": [492, 458, 264, 1412, 570, 300, 390, 257, 1412, 3920, 291, 4678, 294, 293, 586, 321, 458, 437, 264, 3097], "temperature": 0.0, "avg_logprob": -0.1490420081398704, "compression_ratio": 1.7553648068669527, "no_speech_prob": 4.71086013931199e-06}, {"id": 538, "seek": 276576, "start": 2784.1200000000003, "end": 2786.4, "text": " set is because that was passed in.", "tokens": [992, 307, 570, 300, 390, 4678, 294, 13], "temperature": 0.0, "avg_logprob": -0.1490420081398704, "compression_ratio": 1.7553648068669527, "no_speech_prob": 4.71086013931199e-06}, {"id": 539, "seek": 276576, "start": 2786.4, "end": 2790.32, "text": " So there's no reason, there's no reason to ask you to manually call setup.", "tokens": [407, 456, 311, 572, 1778, 11, 456, 311, 572, 1778, 281, 1029, 291, 281, 16945, 818, 8657, 13], "temperature": 0.0, "avg_logprob": -0.1490420081398704, "compression_ratio": 1.7553648068669527, "no_speech_prob": 4.71086013931199e-06}, {"id": 540, "seek": 276576, "start": 2790.32, "end": 2794.4, "text": " So you've got two ways to set up your processes.", "tokens": [407, 291, 600, 658, 732, 2098, 281, 992, 493, 428, 7555, 13], "temperature": 0.0, "avg_logprob": -0.1490420081398704, "compression_ratio": 1.7553648068669527, "no_speech_prob": 4.71086013931199e-06}, {"id": 541, "seek": 279440, "start": 2794.4, "end": 2805.28, "text": " One is to call setup on your tabular object or the other is just to create a data source.", "tokens": [1485, 307, 281, 818, 8657, 322, 428, 4421, 1040, 2657, 420, 264, 661, 307, 445, 281, 1884, 257, 1412, 4009, 13], "temperature": 0.0, "avg_logprob": -0.12855316463269686, "compression_ratio": 1.6551724137931034, "no_speech_prob": 7.57113411964383e-07}, {"id": 542, "seek": 279440, "start": 2805.28, "end": 2810.76, "text": " And it's kind of, it's something you kind of have to be aware of, right?", "tokens": [400, 309, 311, 733, 295, 11, 309, 311, 746, 291, 733, 295, 362, 281, 312, 3650, 295, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12855316463269686, "compression_ratio": 1.6551724137931034, "no_speech_prob": 7.57113411964383e-07}, {"id": 543, "seek": 279440, "start": 2810.76, "end": 2813.88, "text": " Because calling data source is not just returning a data source.", "tokens": [1436, 5141, 1412, 4009, 307, 406, 445, 12678, 257, 1412, 4009, 13], "temperature": 0.0, "avg_logprob": -0.12855316463269686, "compression_ratio": 1.6551724137931034, "no_speech_prob": 7.57113411964383e-07}, {"id": 544, "seek": 279440, "start": 2813.88, "end": 2819.2400000000002, "text": " It is also modifying your tabular object data to process it.", "tokens": [467, 307, 611, 42626, 428, 4421, 1040, 2657, 1412, 281, 1399, 309, 13], "temperature": 0.0, "avg_logprob": -0.12855316463269686, "compression_ratio": 1.6551724137931034, "no_speech_prob": 7.57113411964383e-07}, {"id": 545, "seek": 281924, "start": 2819.24, "end": 2826.4399999999996, "text": " And so it's kind of like, it's a very non-pure, non-functional kind of approach coming on", "tokens": [400, 370, 309, 311, 733, 295, 411, 11, 309, 311, 257, 588, 2107, 12, 79, 540, 11, 2107, 12, 22845, 304, 733, 295, 3109, 1348, 322], "temperature": 0.0, "avg_logprob": -0.1301773440453314, "compression_ratio": 1.6932270916334662, "no_speech_prob": 1.6536741895833984e-06}, {"id": 546, "seek": 281924, "start": 2826.4399999999996, "end": 2827.4399999999996, "text": " here.", "tokens": [510, 13], "temperature": 0.0, "avg_logprob": -0.1301773440453314, "compression_ratio": 1.6932270916334662, "no_speech_prob": 1.6536741895833984e-06}, {"id": 547, "seek": 281924, "start": 2827.4399999999996, "end": 2828.7999999999997, "text": " We're not changing things and returning them.", "tokens": [492, 434, 406, 4473, 721, 293, 12678, 552, 13], "temperature": 0.0, "avg_logprob": -0.1301773440453314, "compression_ratio": 1.6932270916334662, "no_speech_prob": 1.6536741895833984e-06}, {"id": 548, "seek": 281924, "start": 2828.7999999999997, "end": 2830.9599999999996, "text": " We're like changing things in place.", "tokens": [492, 434, 411, 4473, 721, 294, 1081, 13], "temperature": 0.0, "avg_logprob": -0.1301773440453314, "compression_ratio": 1.6932270916334662, "no_speech_prob": 1.6536741895833984e-06}, {"id": 549, "seek": 281924, "start": 2830.9599999999996, "end": 2837.9599999999996, "text": " The reason for that is that with tabular data, you don't want to be creating lots of copies", "tokens": [440, 1778, 337, 300, 307, 300, 365, 4421, 1040, 1412, 11, 291, 500, 380, 528, 281, 312, 4084, 3195, 295, 14341], "temperature": 0.0, "avg_logprob": -0.1301773440453314, "compression_ratio": 1.6932270916334662, "no_speech_prob": 1.6536741895833984e-06}, {"id": 550, "seek": 281924, "start": 2837.9599999999996, "end": 2838.9599999999996, "text": " of it.", "tokens": [295, 309, 13], "temperature": 0.0, "avg_logprob": -0.1301773440453314, "compression_ratio": 1.6932270916334662, "no_speech_prob": 1.6536741895833984e-06}, {"id": 551, "seek": 281924, "start": 2838.9599999999996, "end": 2839.9599999999996, "text": " You really want to be doing stuff in place.", "tokens": [509, 534, 528, 281, 312, 884, 1507, 294, 1081, 13], "temperature": 0.0, "avg_logprob": -0.1301773440453314, "compression_ratio": 1.6932270916334662, "no_speech_prob": 1.6536741895833984e-06}, {"id": 552, "seek": 281924, "start": 2839.9599999999996, "end": 2844.7599999999998, "text": " It's got a lot of important performance issues.", "tokens": [467, 311, 658, 257, 688, 295, 1021, 3389, 2663, 13], "temperature": 0.0, "avg_logprob": -0.1301773440453314, "compression_ratio": 1.6932270916334662, "no_speech_prob": 1.6536741895833984e-06}, {"id": 553, "seek": 281924, "start": 2844.7599999999998, "end": 2849.2, "text": " So we try to do things just once and do it where it is.", "tokens": [407, 321, 853, 281, 360, 721, 445, 1564, 293, 360, 309, 689, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1301773440453314, "compression_ratio": 1.6932270916334662, "no_speech_prob": 1.6536741895833984e-06}, {"id": 554, "seek": 284920, "start": 2849.2, "end": 2853.22, "text": " So normalize in this case, we're calling setup.", "tokens": [407, 2710, 1125, 294, 341, 1389, 11, 321, 434, 5141, 8657, 13], "temperature": 0.0, "avg_logprob": -0.20168609225872866, "compression_ratio": 1.7211538461538463, "no_speech_prob": 6.144101917016087e-06}, {"id": 555, "seek": 284920, "start": 2853.22, "end": 2859.3199999999997, "text": " And so again, for inference, here's some new data set we want to call inference on.", "tokens": [400, 370, 797, 11, 337, 38253, 11, 510, 311, 512, 777, 1412, 992, 321, 528, 281, 818, 38253, 322, 13], "temperature": 0.0, "avg_logprob": -0.20168609225872866, "compression_ratio": 1.7211538461538463, "no_speech_prob": 6.144101917016087e-06}, {"id": 556, "seek": 284920, "start": 2859.3199999999997, "end": 2863.2799999999997, "text": " We go tabular object.new on the new data frame.", "tokens": [492, 352, 4421, 1040, 2657, 13, 7686, 322, 264, 777, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.20168609225872866, "compression_ratio": 1.7211538461538463, "no_speech_prob": 6.144101917016087e-06}, {"id": 557, "seek": 284920, "start": 2863.2799999999997, "end": 2864.68, "text": " We process it.", "tokens": [492, 1399, 309, 13], "temperature": 0.0, "avg_logprob": -0.20168609225872866, "compression_ratio": 1.7211538461538463, "no_speech_prob": 6.144101917016087e-06}, {"id": 558, "seek": 284920, "start": 2864.68, "end": 2867.8799999999997, "text": " We don't call setup because we don't want to create new mean standard deviation.", "tokens": [492, 500, 380, 818, 8657, 570, 321, 500, 380, 528, 281, 1884, 777, 914, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.20168609225872866, "compression_ratio": 1.7211538461538463, "no_speech_prob": 6.144101917016087e-06}, {"id": 559, "seek": 284920, "start": 2867.8799999999997, "end": 2876.56, "text": " We need to use the same standard deviation and mean that we used for our training.", "tokens": [492, 643, 281, 764, 264, 912, 3832, 25163, 293, 914, 300, 321, 1143, 337, 527, 3097, 13], "temperature": 0.0, "avg_logprob": -0.20168609225872866, "compression_ratio": 1.7211538461538463, "no_speech_prob": 6.144101917016087e-06}, {"id": 560, "seek": 287656, "start": 2876.56, "end": 2880.94, "text": " And then here's the version where we use instead data source.", "tokens": [400, 550, 510, 311, 264, 3037, 689, 321, 764, 2602, 1412, 4009, 13], "temperature": 0.0, "avg_logprob": -0.14502336623820852, "compression_ratio": 1.7327188940092166, "no_speech_prob": 2.9480006560334004e-06}, {"id": 561, "seek": 287656, "start": 2880.94, "end": 2887.68, "text": " So you'll find that the mean and standard deviation now, the mean standard deviation", "tokens": [407, 291, 603, 915, 300, 264, 914, 293, 3832, 25163, 586, 11, 264, 914, 3832, 25163], "temperature": 0.0, "avg_logprob": -0.14502336623820852, "compression_ratio": 1.7327188940092166, "no_speech_prob": 2.9480006560334004e-06}, {"id": 562, "seek": 287656, "start": 2887.68, "end": 2891.04, "text": " is 0, 1, 2, because that's the only stuff in the training set.", "tokens": [307, 1958, 11, 502, 11, 568, 11, 570, 300, 311, 264, 787, 1507, 294, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.14502336623820852, "compression_ratio": 1.7327188940092166, "no_speech_prob": 2.9480006560334004e-06}, {"id": 563, "seek": 287656, "start": 2891.04, "end": 2895.36, "text": " And again, normalization and stuff should only be done with the training set.", "tokens": [400, 797, 11, 2710, 2144, 293, 1507, 820, 787, 312, 1096, 365, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.14502336623820852, "compression_ratio": 1.7327188940092166, "no_speech_prob": 2.9480006560334004e-06}, {"id": 564, "seek": 287656, "start": 2895.36, "end": 2903.2, "text": " So all this stuff of kind of using this stuff makes it much harder to screw things up in", "tokens": [407, 439, 341, 1507, 295, 733, 295, 1228, 341, 1507, 1669, 309, 709, 6081, 281, 5630, 721, 493, 294], "temperature": 0.0, "avg_logprob": -0.14502336623820852, "compression_ratio": 1.7327188940092166, "no_speech_prob": 2.9480006560334004e-06}, {"id": 565, "seek": 290320, "start": 2903.2, "end": 2909.04, "text": " terms of modeling and accidentally do things on the whole data set, get leakage and stuff", "tokens": [2115, 295, 15983, 293, 15715, 360, 721, 322, 264, 1379, 1412, 992, 11, 483, 47799, 293, 1507], "temperature": 0.0, "avg_logprob": -0.18982505798339844, "compression_ratio": 1.53125, "no_speech_prob": 9.87458065537794e-07}, {"id": 566, "seek": 290320, "start": 2909.04, "end": 2917.4399999999996, "text": " like that because we try to automatically do the right thing for you.", "tokens": [411, 300, 570, 321, 853, 281, 6772, 360, 264, 558, 551, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.18982505798339844, "compression_ratio": 1.53125, "no_speech_prob": 9.87458065537794e-07}, {"id": 567, "seek": 290320, "start": 2917.4399999999996, "end": 2929.8399999999997, "text": " So then fill missing is going to go through each continuous column and it will see if", "tokens": [407, 550, 2836, 5361, 307, 516, 281, 352, 807, 1184, 10957, 7738, 293, 309, 486, 536, 498], "temperature": 0.0, "avg_logprob": -0.18982505798339844, "compression_ratio": 1.53125, "no_speech_prob": 9.87458065537794e-07}, {"id": 568, "seek": 292984, "start": 2929.84, "end": 2939.76, "text": " there are any missing values in the column.", "tokens": [456, 366, 604, 5361, 4190, 294, 264, 7738, 13], "temperature": 0.0, "avg_logprob": -0.23488477180744038, "compression_ratio": 1.5681818181818181, "no_speech_prob": 2.332051053599571e-06}, {"id": 569, "seek": 292984, "start": 2939.76, "end": 2951.2000000000003, "text": " And if there are missing values, then it's going to create a NA dictionary.", "tokens": [400, 498, 456, 366, 5361, 4190, 11, 550, 309, 311, 516, 281, 1884, 257, 16585, 25890, 13], "temperature": 0.0, "avg_logprob": -0.23488477180744038, "compression_ratio": 1.5681818181818181, "no_speech_prob": 2.332051053599571e-06}, {"id": 570, "seek": 292984, "start": 2951.2000000000003, "end": 2956.04, "text": " So if there's any NA's or any missing or any nulls or all the same idea in pandas, then", "tokens": [407, 498, 456, 311, 604, 16585, 311, 420, 604, 5361, 420, 604, 18184, 82, 420, 439, 264, 912, 1558, 294, 4565, 296, 11, 550], "temperature": 0.0, "avg_logprob": -0.23488477180744038, "compression_ratio": 1.5681818181818181, "no_speech_prob": 2.332051053599571e-06}, {"id": 571, "seek": 295604, "start": 2956.04, "end": 2966.56, "text": " that column will appear as a column name in this dictionary and the value of it will be", "tokens": [300, 7738, 486, 4204, 382, 257, 7738, 1315, 294, 341, 25890, 293, 264, 2158, 295, 309, 486, 312], "temperature": 0.0, "avg_logprob": -0.1461045351895419, "compression_ratio": 1.5845070422535212, "no_speech_prob": 2.0904503799101803e-06}, {"id": 572, "seek": 295604, "start": 2966.56, "end": 2968.92, "text": " dependent on what fill strategy you ask for.", "tokens": [12334, 322, 437, 2836, 5206, 291, 1029, 337, 13], "temperature": 0.0, "avg_logprob": -0.1461045351895419, "compression_ratio": 1.5845070422535212, "no_speech_prob": 2.0904503799101803e-06}, {"id": 573, "seek": 295604, "start": 2968.92, "end": 2977.68, "text": " So fill strategy, this is a kind of fun little trick, fill strategy is a class that contains", "tokens": [407, 2836, 5206, 11, 341, 307, 257, 733, 295, 1019, 707, 4282, 11, 2836, 5206, 307, 257, 1508, 300, 8306], "temperature": 0.0, "avg_logprob": -0.1461045351895419, "compression_ratio": 1.5845070422535212, "no_speech_prob": 2.0904503799101803e-06}, {"id": 574, "seek": 297768, "start": 2977.68, "end": 2987.04, "text": " three different methods and you can say which of those methods you want to use.", "tokens": [1045, 819, 7150, 293, 291, 393, 584, 597, 295, 729, 7150, 291, 528, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.18468905131022134, "compression_ratio": 1.6162162162162161, "no_speech_prob": 4.785063083545538e-06}, {"id": 575, "seek": 297768, "start": 2987.04, "end": 2994.68, "text": " Do you want to fill things with the median or with some constant or with the mode?", "tokens": [1144, 291, 528, 281, 2836, 721, 365, 264, 26779, 420, 365, 512, 5754, 420, 365, 264, 4391, 30], "temperature": 0.0, "avg_logprob": -0.18468905131022134, "compression_ratio": 1.6162162162162161, "no_speech_prob": 4.785063083545538e-06}, {"id": 576, "seek": 297768, "start": 2994.68, "end": 2997.94, "text": " And so we assume by default that it's the median.", "tokens": [400, 370, 321, 6552, 538, 7576, 300, 309, 311, 264, 26779, 13], "temperature": 0.0, "avg_logprob": -0.18468905131022134, "compression_ratio": 1.6162162162162161, "no_speech_prob": 4.785063083545538e-06}, {"id": 577, "seek": 297768, "start": 2997.94, "end": 3005.7999999999997, "text": " So this here is actually going to call fill strategy.median, passing in the column and", "tokens": [407, 341, 510, 307, 767, 516, 281, 818, 2836, 5206, 13, 1912, 952, 11, 8437, 294, 264, 7738, 293], "temperature": 0.0, "avg_logprob": -0.18468905131022134, "compression_ratio": 1.6162162162162161, "no_speech_prob": 4.785063083545538e-06}, {"id": 578, "seek": 300580, "start": 3005.8, "end": 3011.36, "text": " that's going to return the median.", "tokens": [300, 311, 516, 281, 2736, 264, 26779, 13], "temperature": 0.0, "avg_logprob": -0.14905273553096887, "compression_ratio": 1.5, "no_speech_prob": 7.112424782462767e-07}, {"id": 579, "seek": 300580, "start": 3011.36, "end": 3014.36, "text": " So that's the dictionary we create.", "tokens": [407, 300, 311, 264, 25890, 321, 1884, 13], "temperature": 0.0, "avg_logprob": -0.14905273553096887, "compression_ratio": 1.5, "no_speech_prob": 7.112424782462767e-07}, {"id": 580, "seek": 300580, "start": 3014.36, "end": 3028.36, "text": " So then later on when you're calling codes, we actually need to go through and do two", "tokens": [407, 550, 1780, 322, 562, 291, 434, 5141, 14211, 11, 321, 767, 643, 281, 352, 807, 293, 360, 732], "temperature": 0.0, "avg_logprob": -0.14905273553096887, "compression_ratio": 1.5, "no_speech_prob": 7.112424782462767e-07}, {"id": 581, "seek": 300580, "start": 3028.36, "end": 3029.36, "text": " things.", "tokens": [721, 13], "temperature": 0.0, "avg_logprob": -0.14905273553096887, "compression_ratio": 1.5, "no_speech_prob": 7.112424782462767e-07}, {"id": 582, "seek": 300580, "start": 3029.36, "end": 3035.6000000000004, "text": " The first thing is use the pandas fill NA to fill missing values with whatever value", "tokens": [440, 700, 551, 307, 764, 264, 4565, 296, 2836, 16585, 281, 2836, 5361, 4190, 365, 2035, 2158], "temperature": 0.0, "avg_logprob": -0.14905273553096887, "compression_ratio": 1.5, "no_speech_prob": 7.112424782462767e-07}, {"id": 583, "seek": 303560, "start": 3035.6, "end": 3042.7999999999997, "text": " we put into the dictionary for that column and again we do it in place.", "tokens": [321, 829, 666, 264, 25890, 337, 300, 7738, 293, 797, 321, 360, 309, 294, 1081, 13], "temperature": 0.0, "avg_logprob": -0.12541253706988167, "compression_ratio": 1.645320197044335, "no_speech_prob": 6.339170795399696e-06}, {"id": 584, "seek": 303560, "start": 3042.7999999999997, "end": 3049.2999999999997, "text": " Then the second thing is if you're asked to add an extra column to say which ones we filled", "tokens": [1396, 264, 1150, 551, 307, 498, 291, 434, 2351, 281, 909, 364, 2857, 7738, 281, 584, 597, 2306, 321, 6412], "temperature": 0.0, "avg_logprob": -0.12541253706988167, "compression_ratio": 1.645320197044335, "no_speech_prob": 6.339170795399696e-06}, {"id": 585, "seek": 303560, "start": 3049.2999999999997, "end": 3054.2, "text": " missing in, which by default is true, then we're going to add a column with the same", "tokens": [5361, 294, 11, 597, 538, 7576, 307, 2074, 11, 550, 321, 434, 516, 281, 909, 257, 7738, 365, 264, 912], "temperature": 0.0, "avg_logprob": -0.12541253706988167, "compression_ratio": 1.645320197044335, "no_speech_prob": 6.339170795399696e-06}, {"id": 586, "seek": 303560, "start": 3054.2, "end": 3062.12, "text": " name that underscore NA at the end, which is going to be billions of true if that was", "tokens": [1315, 300, 37556, 16585, 412, 264, 917, 11, 597, 307, 516, 281, 312, 17375, 295, 2074, 498, 300, 390], "temperature": 0.0, "avg_logprob": -0.12541253706988167, "compression_ratio": 1.645320197044335, "no_speech_prob": 6.339170795399696e-06}, {"id": 587, "seek": 306212, "start": 3062.12, "end": 3066.72, "text": " originally missing and false otherwise.", "tokens": [7993, 5361, 293, 7908, 5911, 13], "temperature": 0.0, "avg_logprob": -0.17177754289963665, "compression_ratio": 1.6881720430107527, "no_speech_prob": 1.370945369671972e-06}, {"id": 588, "seek": 306212, "start": 3066.72, "end": 3073.4, "text": " So here you can see we're creating three different processes which are just processes, a fill", "tokens": [407, 510, 291, 393, 536, 321, 434, 4084, 1045, 819, 7555, 597, 366, 445, 7555, 11, 257, 2836], "temperature": 0.0, "avg_logprob": -0.17177754289963665, "compression_ratio": 1.6881720430107527, "no_speech_prob": 1.370945369671972e-06}, {"id": 589, "seek": 306212, "start": 3073.4, "end": 3079.3599999999997, "text": " missing process with each of the possible strategies and so then we create a data frame", "tokens": [5361, 1399, 365, 1184, 295, 264, 1944, 9029, 293, 370, 550, 321, 1884, 257, 1412, 3920], "temperature": 0.0, "avg_logprob": -0.17177754289963665, "compression_ratio": 1.6881720430107527, "no_speech_prob": 1.370945369671972e-06}, {"id": 590, "seek": 306212, "start": 3079.3599999999997, "end": 3087.16, "text": " with a missing value and then we just go through and create three tabular objects with those", "tokens": [365, 257, 5361, 2158, 293, 550, 321, 445, 352, 807, 293, 1884, 1045, 4421, 1040, 6565, 365, 729], "temperature": 0.0, "avg_logprob": -0.17177754289963665, "compression_ratio": 1.6881720430107527, "no_speech_prob": 1.370945369671972e-06}, {"id": 591, "seek": 308716, "start": 3087.16, "end": 3095.56, "text": " three different processes and make sure that the NA dict for our A column has the appropriate", "tokens": [1045, 819, 7555, 293, 652, 988, 300, 264, 16585, 12569, 337, 527, 316, 7738, 575, 264, 6854], "temperature": 0.0, "avg_logprob": -0.16510427474975586, "compression_ratio": 1.5422535211267605, "no_speech_prob": 3.0415828859986505e-06}, {"id": 592, "seek": 308716, "start": 3095.56, "end": 3102.6, "text": " median or constant or load as requested.", "tokens": [26779, 420, 5754, 420, 3677, 382, 16436, 13], "temperature": 0.0, "avg_logprob": -0.16510427474975586, "compression_ratio": 1.5422535211267605, "no_speech_prob": 3.0415828859986505e-06}, {"id": 593, "seek": 308716, "start": 3102.6, "end": 3111.7999999999997, "text": " And then remember set up also processes so then we can go through and make sure that", "tokens": [400, 550, 1604, 992, 493, 611, 7555, 370, 550, 321, 393, 352, 807, 293, 652, 988, 300], "temperature": 0.0, "avg_logprob": -0.16510427474975586, "compression_ratio": 1.5422535211267605, "no_speech_prob": 3.0415828859986505e-06}, {"id": 594, "seek": 311180, "start": 3111.8, "end": 3120.0, "text": " they have been replaced correctly and also make sure that the tabular object now has", "tokens": [436, 362, 668, 10772, 8944, 293, 611, 652, 988, 300, 264, 4421, 1040, 2657, 586, 575], "temperature": 0.0, "avg_logprob": -0.1406070214730722, "compression_ratio": 1.614213197969543, "no_speech_prob": 2.0580437194439583e-06}, {"id": 595, "seek": 311180, "start": 3120.0, "end": 3125.94, "text": " a categorical name which is in this case ANA.", "tokens": [257, 19250, 804, 1315, 597, 307, 294, 341, 1389, 5252, 32, 13], "temperature": 0.0, "avg_logprob": -0.1406070214730722, "compression_ratio": 1.614213197969543, "no_speech_prob": 2.0580437194439583e-06}, {"id": 596, "seek": 311180, "start": 3125.94, "end": 3132.32, "text": " So it's not enough just to add it to the data frame, it also has to be added to cat names", "tokens": [407, 309, 311, 406, 1547, 445, 281, 909, 309, 281, 264, 1412, 3920, 11, 309, 611, 575, 281, 312, 3869, 281, 3857, 5288], "temperature": 0.0, "avg_logprob": -0.1406070214730722, "compression_ratio": 1.614213197969543, "no_speech_prob": 2.0580437194439583e-06}, {"id": 597, "seek": 311180, "start": 3132.32, "end": 3138.92, "text": " in the tabular object because this is something a categorical column we want to use for modeling.", "tokens": [294, 264, 4421, 1040, 2657, 570, 341, 307, 746, 257, 19250, 804, 7738, 321, 528, 281, 764, 337, 15983, 13], "temperature": 0.0, "avg_logprob": -0.1406070214730722, "compression_ratio": 1.614213197969543, "no_speech_prob": 2.0580437194439583e-06}, {"id": 598, "seek": 313892, "start": 3138.92, "end": 3144.2400000000002, "text": " So Madavan asks shouldn't setups be called in the constructor and no it shouldn't.", "tokens": [407, 5326, 21071, 8962, 4659, 380, 46832, 312, 1219, 294, 264, 47479, 293, 572, 309, 4659, 380, 13], "temperature": 0.0, "avg_logprob": -0.14389751803490422, "compression_ratio": 1.8008474576271187, "no_speech_prob": 9.81825178314466e-06}, {"id": 599, "seek": 313892, "start": 3144.2400000000002, "end": 3151.7200000000003, "text": " Setups is what transforms call when you call setup using the type dispatch stuff we talked", "tokens": [8928, 7528, 307, 437, 35592, 818, 562, 291, 818, 8657, 1228, 264, 2010, 36729, 1507, 321, 2825], "temperature": 0.0, "avg_logprob": -0.14389751803490422, "compression_ratio": 1.8008474576271187, "no_speech_prob": 9.81825178314466e-06}, {"id": 600, "seek": 313892, "start": 3151.7200000000003, "end": 3159.0, "text": " about in the transforms walkthrough and then setup is something which should be called", "tokens": [466, 294, 264, 35592, 1792, 11529, 293, 550, 8657, 307, 746, 597, 820, 312, 1219], "temperature": 0.0, "avg_logprob": -0.14389751803490422, "compression_ratio": 1.8008474576271187, "no_speech_prob": 9.81825178314466e-06}, {"id": 601, "seek": 313892, "start": 3159.0, "end": 3163.88, "text": " automatically only when we have enough information to know what to set up with and that information", "tokens": [6772, 787, 562, 321, 362, 1547, 1589, 281, 458, 437, 281, 992, 493, 365, 293, 300, 1589], "temperature": 0.0, "avg_logprob": -0.14389751803490422, "compression_ratio": 1.8008474576271187, "no_speech_prob": 9.81825178314466e-06}, {"id": 602, "seek": 313892, "start": 3163.88, "end": 3167.88, "text": " is only available once you've told us what your training set is.", "tokens": [307, 787, 2435, 1564, 291, 600, 1907, 505, 437, 428, 3097, 992, 307, 13], "temperature": 0.0, "avg_logprob": -0.14389751803490422, "compression_ratio": 1.8008474576271187, "no_speech_prob": 9.81825178314466e-06}, {"id": 603, "seek": 316788, "start": 3167.88, "end": 3172.52, "text": " So that's why it's called by data source not called by the constructor.", "tokens": [407, 300, 311, 983, 309, 311, 1219, 538, 1412, 4009, 406, 1219, 538, 264, 47479, 13], "temperature": 0.0, "avg_logprob": -0.2214050531387329, "compression_ratio": 1.3738317757009346, "no_speech_prob": 4.565953986457316e-06}, {"id": 604, "seek": 316788, "start": 3172.52, "end": 3192.0, "text": " But if you're not going to use a data source then you can call it yourself.", "tokens": [583, 498, 291, 434, 406, 516, 281, 764, 257, 1412, 4009, 550, 291, 393, 818, 309, 1803, 13], "temperature": 0.0, "avg_logprob": -0.2214050531387329, "compression_ratio": 1.3738317757009346, "no_speech_prob": 4.565953986457316e-06}, {"id": 605, "seek": 319200, "start": 3192.0, "end": 3197.88, "text": " So this section is mainly kind of a few more examples of putting it all together.", "tokens": [407, 341, 3541, 307, 8704, 733, 295, 257, 1326, 544, 5110, 295, 3372, 309, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.23646815069790544, "compression_ratio": 1.6330935251798562, "no_speech_prob": 9.972838597605005e-06}, {"id": 606, "seek": 319200, "start": 3197.88, "end": 3202.64, "text": " So here's a bunch of processes, normalize, categorify, fill missing, do nothing at all,", "tokens": [407, 510, 311, 257, 3840, 295, 7555, 11, 2710, 1125, 11, 19250, 2505, 11, 2836, 5361, 11, 360, 1825, 412, 439, 11], "temperature": 0.0, "avg_logprob": -0.23646815069790544, "compression_ratio": 1.6330935251798562, "no_speech_prob": 9.972838597605005e-06}, {"id": 607, "seek": 319200, "start": 3202.64, "end": 3206.44, "text": " obviously you don't need this one it's just to show you.", "tokens": [2745, 291, 500, 380, 643, 341, 472, 309, 311, 445, 281, 855, 291, 13], "temperature": 0.0, "avg_logprob": -0.23646815069790544, "compression_ratio": 1.6330935251798562, "no_speech_prob": 9.972838597605005e-06}, {"id": 608, "seek": 319200, "start": 3206.44, "end": 3210.76, "text": " And here's a data frame with a couple of columns, A is the categorical, B is the continuous", "tokens": [400, 510, 311, 257, 1412, 3920, 365, 257, 1916, 295, 13766, 11, 316, 307, 264, 19250, 804, 11, 363, 307, 264, 10957], "temperature": 0.0, "avg_logprob": -0.23646815069790544, "compression_ratio": 1.6330935251798562, "no_speech_prob": 9.972838597605005e-06}, {"id": 609, "seek": 319200, "start": 3210.76, "end": 3215.36, "text": " because remember that was the order that we use.", "tokens": [570, 1604, 300, 390, 264, 1668, 300, 321, 764, 13], "temperature": 0.0, "avg_logprob": -0.23646815069790544, "compression_ratio": 1.6330935251798562, "no_speech_prob": 9.972838597605005e-06}, {"id": 610, "seek": 319200, "start": 3215.36, "end": 3219.72, "text": " It would probably be better if we actually wrote those here at least the first time so", "tokens": [467, 576, 1391, 312, 1101, 498, 321, 767, 4114, 729, 510, 412, 1935, 264, 700, 565, 370], "temperature": 0.0, "avg_logprob": -0.23646815069790544, "compression_ratio": 1.6330935251798562, "no_speech_prob": 9.972838597605005e-06}, {"id": 611, "seek": 321972, "start": 3219.72, "end": 3222.64, "text": " you didn't have to remember.", "tokens": [291, 994, 380, 362, 281, 1604, 13], "temperature": 0.0, "avg_logprob": -0.2711059816422001, "compression_ratio": 1.4970059880239521, "no_speech_prob": 1.392538933941978e-06}, {"id": 612, "seek": 321972, "start": 3222.64, "end": 3235.4399999999996, "text": " So we call setup because we're not using a data source on this one and so the processes", "tokens": [407, 321, 818, 8657, 570, 321, 434, 406, 1228, 257, 1412, 4009, 322, 341, 472, 293, 370, 264, 7555], "temperature": 0.0, "avg_logprob": -0.2711059816422001, "compression_ratio": 1.4970059880239521, "no_speech_prob": 1.392538933941978e-06}, {"id": 613, "seek": 321972, "start": 3235.4399999999996, "end": 3240.52, "text": " you'll have noticed explicitly only work on the columns of the right type so these work", "tokens": [291, 603, 362, 5694, 20803, 787, 589, 322, 264, 13766, 295, 264, 558, 2010, 370, 613, 589], "temperature": 0.0, "avg_logprob": -0.2711059816422001, "compression_ratio": 1.4970059880239521, "no_speech_prob": 1.392538933941978e-06}, {"id": 614, "seek": 321972, "start": 3240.52, "end": 3244.24, "text": " just on the continuous columns for normalize.", "tokens": [445, 322, 264, 10957, 13766, 337, 2710, 1125, 13], "temperature": 0.0, "avg_logprob": -0.2711059816422001, "compression_ratio": 1.4970059880239521, "no_speech_prob": 1.392538933941978e-06}, {"id": 615, "seek": 324424, "start": 3244.24, "end": 3250.08, "text": " The categorify goes through the categorical columns.", "tokens": [440, 19250, 2505, 1709, 807, 264, 19250, 804, 13766, 13], "temperature": 0.0, "avg_logprob": -0.2646757881596403, "compression_ratio": 1.635135135135135, "no_speech_prob": 1.3496970723281265e-06}, {"id": 616, "seek": 324424, "start": 3250.08, "end": 3260.2, "text": " You might have noticed that was all cat names and that's because you also want to categorize", "tokens": [509, 1062, 362, 5694, 300, 390, 439, 3857, 5288, 293, 300, 311, 570, 291, 611, 528, 281, 19250, 1125], "temperature": 0.0, "avg_logprob": -0.2646757881596403, "compression_ratio": 1.635135135135135, "no_speech_prob": 1.3496970723281265e-06}, {"id": 617, "seek": 324424, "start": 3260.2, "end": 3266.0, "text": " categorical dependent variables but normalize we don't normalize continuous dependent variables.", "tokens": [19250, 804, 12334, 9102, 457, 2710, 1125, 321, 500, 380, 2710, 1125, 10957, 12334, 9102, 13], "temperature": 0.0, "avg_logprob": -0.2646757881596403, "compression_ratio": 1.635135135135135, "no_speech_prob": 1.3496970723281265e-06}, {"id": 618, "seek": 326600, "start": 3266.0, "end": 3275.24, "text": " Normally for that you'll do like a sigmoid in the model or something like that.", "tokens": [17424, 337, 300, 291, 603, 360, 411, 257, 4556, 3280, 327, 294, 264, 2316, 420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.19667923128282702, "compression_ratio": 1.6536312849162011, "no_speech_prob": 4.785066266776994e-06}, {"id": 619, "seek": 326600, "start": 3275.24, "end": 3278.4, "text": " So you can throw them all in there and it will do the right thing for the right columns", "tokens": [407, 291, 393, 3507, 552, 439, 294, 456, 293, 309, 486, 360, 264, 558, 551, 337, 264, 558, 13766], "temperature": 0.0, "avg_logprob": -0.19667923128282702, "compression_ratio": 1.6536312849162011, "no_speech_prob": 4.785066266776994e-06}, {"id": 620, "seek": 326600, "start": 3278.4, "end": 3284.16, "text": " automatically so it just goes through and make sure that all works fine.", "tokens": [6772, 370, 309, 445, 1709, 807, 293, 652, 988, 300, 439, 1985, 2489, 13], "temperature": 0.0, "avg_logprob": -0.19667923128282702, "compression_ratio": 1.6536312849162011, "no_speech_prob": 4.785066266776994e-06}, {"id": 621, "seek": 326600, "start": 3284.16, "end": 3289.0, "text": " So these are really just a bunch of tests and examples.", "tokens": [407, 613, 366, 534, 445, 257, 3840, 295, 6921, 293, 5110, 13], "temperature": 0.0, "avg_logprob": -0.19667923128282702, "compression_ratio": 1.6536312849162011, "no_speech_prob": 4.785066266776994e-06}, {"id": 622, "seek": 328900, "start": 3289.0, "end": 3296.64, "text": " So last section which is okay so now we have a tabular object which has got some cats and", "tokens": [407, 1036, 3541, 597, 307, 1392, 370, 586, 321, 362, 257, 4421, 1040, 2657, 597, 575, 658, 512, 11111, 293], "temperature": 0.0, "avg_logprob": -0.17384048302968344, "compression_ratio": 1.813953488372093, "no_speech_prob": 5.3381381803774275e-06}, {"id": 623, "seek": 328900, "start": 3296.64, "end": 3302.2, "text": " some consts and dependent variable wise.", "tokens": [512, 1817, 82, 293, 12334, 7006, 10829, 13], "temperature": 0.0, "avg_logprob": -0.17384048302968344, "compression_ratio": 1.813953488372093, "no_speech_prob": 5.3381381803774275e-06}, {"id": 624, "seek": 328900, "start": 3302.2, "end": 3306.96, "text": " If we want to use this for modeling we need tensors.", "tokens": [759, 321, 528, 281, 764, 341, 337, 15983, 321, 643, 10688, 830, 13], "temperature": 0.0, "avg_logprob": -0.17384048302968344, "compression_ratio": 1.813953488372093, "no_speech_prob": 5.3381381803774275e-06}, {"id": 625, "seek": 328900, "start": 3306.96, "end": 3309.4, "text": " We actually need three tensors.", "tokens": [492, 767, 643, 1045, 10688, 830, 13], "temperature": 0.0, "avg_logprob": -0.17384048302968344, "compression_ratio": 1.813953488372093, "no_speech_prob": 5.3381381803774275e-06}, {"id": 626, "seek": 328900, "start": 3309.4, "end": 3313.8, "text": " One tensor for the continuous, one for the categorical and one for the dependent and", "tokens": [1485, 40863, 337, 264, 10957, 11, 472, 337, 264, 19250, 804, 293, 472, 337, 264, 12334, 293], "temperature": 0.0, "avg_logprob": -0.17384048302968344, "compression_ratio": 1.813953488372093, "no_speech_prob": 5.3381381803774275e-06}, {"id": 627, "seek": 328900, "start": 3313.8, "end": 3318.84, "text": " the reason for that is that the continuous and the categorical have different data types.", "tokens": [264, 1778, 337, 300, 307, 300, 264, 10957, 293, 264, 19250, 804, 362, 819, 1412, 3467, 13], "temperature": 0.0, "avg_logprob": -0.17384048302968344, "compression_ratio": 1.813953488372093, "no_speech_prob": 5.3381381803774275e-06}, {"id": 628, "seek": 331884, "start": 3318.84, "end": 3321.88, "text": " So we can't put them all on the same tensor because tensors have to be all of the same", "tokens": [407, 321, 393, 380, 829, 552, 439, 322, 264, 912, 40863, 570, 10688, 830, 362, 281, 312, 439, 295, 264, 912], "temperature": 0.0, "avg_logprob": -0.1807834998421047, "compression_ratio": 1.665, "no_speech_prob": 3.340435341669945e-06}, {"id": 629, "seek": 331884, "start": 3321.88, "end": 3324.44, "text": " data type.", "tokens": [1412, 2010, 13], "temperature": 0.0, "avg_logprob": -0.1807834998421047, "compression_ratio": 1.665, "no_speech_prob": 3.340435341669945e-06}, {"id": 630, "seek": 331884, "start": 3324.44, "end": 3328.48, "text": " So if you look at the version one tabular stuff it's the same thing.", "tokens": [407, 498, 291, 574, 412, 264, 3037, 472, 4421, 1040, 1507, 309, 311, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.1807834998421047, "compression_ratio": 1.665, "no_speech_prob": 3.340435341669945e-06}, {"id": 631, "seek": 331884, "start": 3328.48, "end": 3331.28, "text": " We have those three different tensors.", "tokens": [492, 362, 729, 1045, 819, 10688, 830, 13], "temperature": 0.0, "avg_logprob": -0.1807834998421047, "compression_ratio": 1.665, "no_speech_prob": 3.340435341669945e-06}, {"id": 632, "seek": 331884, "start": 3331.28, "end": 3336.44, "text": " So now we create some normal transform.", "tokens": [407, 586, 321, 1884, 512, 2710, 4088, 13], "temperature": 0.0, "avg_logprob": -0.1807834998421047, "compression_ratio": 1.665, "no_speech_prob": 3.340435341669945e-06}, {"id": 633, "seek": 331884, "start": 3336.44, "end": 3345.0, "text": " So a lazy transform that's applied as we're getting our batches and all we do is we say", "tokens": [407, 257, 14847, 4088, 300, 311, 6456, 382, 321, 434, 1242, 527, 15245, 279, 293, 439, 321, 360, 307, 321, 584], "temperature": 0.0, "avg_logprob": -0.1807834998421047, "compression_ratio": 1.665, "no_speech_prob": 3.340435341669945e-06}, {"id": 634, "seek": 334500, "start": 3345.0, "end": 3352.16, "text": " okay this is the tabular object which we're going to be transforming and we just grab", "tokens": [1392, 341, 307, 264, 4421, 1040, 2657, 597, 321, 434, 516, 281, 312, 27210, 293, 321, 445, 4444], "temperature": 0.0, "avg_logprob": -0.22451382097990616, "compression_ratio": 1.7216494845360826, "no_speech_prob": 4.222813913656864e-06}, {"id": 635, "seek": 334500, "start": 3352.16, "end": 3357.84, "text": " the encodes.", "tokens": [264, 2058, 4789, 13], "temperature": 0.0, "avg_logprob": -0.22451382097990616, "compression_ratio": 1.7216494845360826, "no_speech_prob": 4.222813913656864e-06}, {"id": 636, "seek": 334500, "start": 3357.84, "end": 3360.2, "text": " We don't actually need that state at all.", "tokens": [492, 500, 380, 767, 643, 300, 1785, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.22451382097990616, "compression_ratio": 1.7216494845360826, "no_speech_prob": 4.222813913656864e-06}, {"id": 637, "seek": 334500, "start": 3360.2, "end": 3364.52, "text": " The encodes we're just going to grab all of the categorical variables, turn them into", "tokens": [440, 2058, 4789, 321, 434, 445, 516, 281, 4444, 439, 295, 264, 19250, 804, 9102, 11, 1261, 552, 666], "temperature": 0.0, "avg_logprob": -0.22451382097990616, "compression_ratio": 1.7216494845360826, "no_speech_prob": 4.222813913656864e-06}, {"id": 638, "seek": 334500, "start": 3364.52, "end": 3370.56, "text": " a tensor and make it a long and then we'll grab all the continuous, turn it into a tensor,", "tokens": [257, 40863, 293, 652, 309, 257, 938, 293, 550, 321, 603, 4444, 439, 264, 10957, 11, 1261, 309, 666, 257, 40863, 11], "temperature": 0.0, "avg_logprob": -0.22451382097990616, "compression_ratio": 1.7216494845360826, "no_speech_prob": 4.222813913656864e-06}, {"id": 639, "seek": 334500, "start": 3370.56, "end": 3371.56, "text": " make it a float.", "tokens": [652, 309, 257, 15706, 13], "temperature": 0.0, "avg_logprob": -0.22451382097990616, "compression_ratio": 1.7216494845360826, "no_speech_prob": 4.222813913656864e-06}, {"id": 640, "seek": 337156, "start": 3371.56, "end": 3376.12, "text": " And so then the first thing, a tuple, is itself a tuple with those two things.", "tokens": [400, 370, 550, 264, 700, 551, 11, 257, 2604, 781, 11, 307, 2564, 257, 2604, 781, 365, 729, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.1982637463193951, "compression_ratio": 1.7370892018779343, "no_speech_prob": 5.7718862080946565e-06}, {"id": 641, "seek": 337156, "start": 3376.12, "end": 3381.72, "text": " So that's our independent variables and then our dependent variable is the target turned", "tokens": [407, 300, 311, 527, 6695, 9102, 293, 550, 527, 12334, 7006, 307, 264, 3779, 3574], "temperature": 0.0, "avg_logprob": -0.1982637463193951, "compression_ratio": 1.7370892018779343, "no_speech_prob": 5.7718862080946565e-06}, {"id": 642, "seek": 337156, "start": 3381.72, "end": 3383.68, "text": " into a long.", "tokens": [666, 257, 938, 13], "temperature": 0.0, "avg_logprob": -0.1982637463193951, "compression_ratio": 1.7370892018779343, "no_speech_prob": 5.7718862080946565e-06}, {"id": 643, "seek": 337156, "start": 3383.68, "end": 3385.36, "text": " This is actually a mistake.", "tokens": [639, 307, 767, 257, 6146, 13], "temperature": 0.0, "avg_logprob": -0.1982637463193951, "compression_ratio": 1.7370892018779343, "no_speech_prob": 5.7718862080946565e-06}, {"id": 644, "seek": 337156, "start": 3385.36, "end": 3387.64, "text": " It shouldn't always turn it into a long.", "tokens": [467, 4659, 380, 1009, 1261, 309, 666, 257, 938, 13], "temperature": 0.0, "avg_logprob": -0.1982637463193951, "compression_ratio": 1.7370892018779343, "no_speech_prob": 5.7718862080946565e-06}, {"id": 645, "seek": 337156, "start": 3387.64, "end": 3392.08, "text": " It should only turn it into a long if it's continuous, sorry categorical y, otherwise", "tokens": [467, 820, 787, 1261, 309, 666, 257, 938, 498, 309, 311, 10957, 11, 2597, 19250, 804, 288, 11, 5911], "temperature": 0.0, "avg_logprob": -0.1982637463193951, "compression_ratio": 1.7370892018779343, "no_speech_prob": 5.7718862080946565e-06}, {"id": 646, "seek": 337156, "start": 3392.08, "end": 3396.12, "text": " it should be a continuous I think.", "tokens": [309, 820, 312, 257, 10957, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.1982637463193951, "compression_ratio": 1.7370892018779343, "no_speech_prob": 5.7718862080946565e-06}, {"id": 647, "seek": 339612, "start": 3396.12, "end": 3401.92, "text": " No, let's wait until we get to modeling.", "tokens": [883, 11, 718, 311, 1699, 1826, 321, 483, 281, 15983, 13], "temperature": 0.0, "avg_logprob": -0.3330497352444396, "compression_ratio": 1.6758241758241759, "no_speech_prob": 1.4970933989388868e-05}, {"id": 648, "seek": 339612, "start": 3401.92, "end": 3402.92, "text": " I can't quite remember.", "tokens": [286, 393, 380, 1596, 1604, 13], "temperature": 0.0, "avg_logprob": -0.3330497352444396, "compression_ratio": 1.6758241758241759, "no_speech_prob": 1.4970933989388868e-05}, {"id": 649, "seek": 339612, "start": 3402.92, "end": 3407.48, "text": " If it's categorical we're then going to one-hot encode it.", "tokens": [759, 309, 311, 19250, 804, 321, 434, 550, 516, 281, 472, 12, 12194, 2058, 1429, 309, 13], "temperature": 0.0, "avg_logprob": -0.3330497352444396, "compression_ratio": 1.6758241758241759, "no_speech_prob": 1.4970933989388868e-05}, {"id": 650, "seek": 339612, "start": 3407.48, "end": 3414.12, "text": " No, we're going to use it as, yeah that's right, so it's a long if it's categorical", "tokens": [883, 11, 321, 434, 516, 281, 764, 309, 382, 11, 1338, 300, 311, 558, 11, 370, 309, 311, 257, 938, 498, 309, 311, 19250, 804], "temperature": 0.0, "avg_logprob": -0.3330497352444396, "compression_ratio": 1.6758241758241759, "no_speech_prob": 1.4970933989388868e-05}, {"id": 651, "seek": 339612, "start": 3414.12, "end": 3416.08, "text": " but for continuous it has to be float.", "tokens": [457, 337, 10957, 309, 575, 281, 312, 15706, 13], "temperature": 0.0, "avg_logprob": -0.3330497352444396, "compression_ratio": 1.6758241758241759, "no_speech_prob": 1.4970933989388868e-05}, {"id": 652, "seek": 339612, "start": 3416.08, "end": 3417.08, "text": " That's right.", "tokens": [663, 311, 558, 13], "temperature": 0.0, "avg_logprob": -0.3330497352444396, "compression_ratio": 1.6758241758241759, "no_speech_prob": 1.4970933989388868e-05}, {"id": 653, "seek": 339612, "start": 3417.08, "end": 3422.56, "text": " So up to do use float for continuous target.", "tokens": [407, 493, 281, 360, 764, 15706, 337, 10957, 3779, 13], "temperature": 0.0, "avg_logprob": -0.3330497352444396, "compression_ratio": 1.6758241758241759, "no_speech_prob": 1.4970933989388868e-05}, {"id": 654, "seek": 342256, "start": 3422.56, "end": 3427.56, "text": " Okay, so that's a little mistake.", "tokens": [1033, 11, 370, 300, 311, 257, 707, 6146, 13], "temperature": 0.0, "avg_logprob": -0.22155361590178116, "compression_ratio": 1.5678391959798994, "no_speech_prob": 2.1907703739998396e-06}, {"id": 655, "seek": 342256, "start": 3427.56, "end": 3434.04, "text": " We haven't done any tabular regression yet in version one, version two.", "tokens": [492, 2378, 380, 1096, 604, 4421, 1040, 24590, 1939, 294, 3037, 472, 11, 3037, 732, 13], "temperature": 0.0, "avg_logprob": -0.22155361590178116, "compression_ratio": 1.5678391959798994, "no_speech_prob": 2.1907703739998396e-06}, {"id": 656, "seek": 342256, "start": 3434.04, "end": 3436.36, "text": " So that's all encodes is going to do.", "tokens": [407, 300, 311, 439, 2058, 4789, 307, 516, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.22155361590178116, "compression_ratio": 1.5678391959798994, "no_speech_prob": 2.1907703739998396e-06}, {"id": 657, "seek": 342256, "start": 3436.36, "end": 3440.08, "text": " So then we'll come back to decodes later.", "tokens": [407, 550, 321, 603, 808, 646, 281, 979, 4789, 1780, 13], "temperature": 0.0, "avg_logprob": -0.22155361590178116, "compression_ratio": 1.5678391959798994, "no_speech_prob": 2.1907703739998396e-06}, {"id": 658, "seek": 342256, "start": 3440.08, "end": 3447.32, "text": " So in our example here we grabbed our path to the adult sample, we read the CSV, we split", "tokens": [407, 294, 527, 1365, 510, 321, 18607, 527, 3100, 281, 264, 5075, 6889, 11, 321, 1401, 264, 48814, 11, 321, 7472], "temperature": 0.0, "avg_logprob": -0.22155361590178116, "compression_ratio": 1.5678391959798994, "no_speech_prob": 2.1907703739998396e-06}, {"id": 659, "seek": 342256, "start": 3447.32, "end": 3450.56, "text": " it into a test set and the main bit.", "tokens": [309, 666, 257, 1500, 992, 293, 264, 2135, 857, 13], "temperature": 0.0, "avg_logprob": -0.22155361590178116, "compression_ratio": 1.5678391959798994, "no_speech_prob": 2.1907703739998396e-06}, {"id": 660, "seek": 345056, "start": 3450.56, "end": 3459.88, "text": " Made a list of our categorical and continuous, a list of the processes we wanted to use,", "tokens": [18330, 257, 1329, 295, 527, 19250, 804, 293, 10957, 11, 257, 1329, 295, 264, 7555, 321, 1415, 281, 764, 11], "temperature": 0.0, "avg_logprob": -0.16664960980415344, "compression_ratio": 1.5850340136054422, "no_speech_prob": 4.092894869245356e-06}, {"id": 661, "seek": 345056, "start": 3459.88, "end": 3463.2799999999997, "text": " the indexes of the splits that we wanted.", "tokens": [264, 8186, 279, 295, 264, 37741, 300, 321, 1415, 13], "temperature": 0.0, "avg_logprob": -0.16664960980415344, "compression_ratio": 1.5850340136054422, "no_speech_prob": 4.092894869245356e-06}, {"id": 662, "seek": 345056, "start": 3463.2799999999997, "end": 3468.24, "text": " So then we can create that tabular as we discussed.", "tokens": [407, 550, 321, 393, 1884, 300, 4421, 1040, 382, 321, 7152, 13], "temperature": 0.0, "avg_logprob": -0.16664960980415344, "compression_ratio": 1.5850340136054422, "no_speech_prob": 4.092894869245356e-06}, {"id": 663, "seek": 345056, "start": 3468.24, "end": 3474.32, "text": " We can turn it into a data source with the splits.", "tokens": [492, 393, 1261, 309, 666, 257, 1412, 4009, 365, 264, 37741, 13], "temperature": 0.0, "avg_logprob": -0.16664960980415344, "compression_ratio": 1.5850340136054422, "no_speech_prob": 4.092894869245356e-06}, {"id": 664, "seek": 347432, "start": 3474.32, "end": 3481.76, "text": " Now you'll see here it never mentioned read tab batch and the reason for that is that", "tokens": [823, 291, 603, 536, 510, 309, 1128, 2835, 1401, 4421, 15245, 293, 264, 1778, 337, 300, 307, 300], "temperature": 0.0, "avg_logprob": -0.12394649203460996, "compression_ratio": 1.7914691943127963, "no_speech_prob": 3.187543825333705e-06}, {"id": 665, "seek": 347432, "start": 3481.76, "end": 3484.94, "text": " we don't want to force you to do things that we can do for you.", "tokens": [321, 500, 380, 528, 281, 3464, 291, 281, 360, 721, 300, 321, 393, 360, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.12394649203460996, "compression_ratio": 1.7914691943127963, "no_speech_prob": 3.187543825333705e-06}, {"id": 666, "seek": 347432, "start": 3484.94, "end": 3489.7200000000003, "text": " So if you just say give me a tabular data loader rather than a normal data loader, the", "tokens": [407, 498, 291, 445, 584, 976, 385, 257, 4421, 1040, 1412, 3677, 260, 2831, 813, 257, 2710, 1412, 3677, 260, 11, 264], "temperature": 0.0, "avg_logprob": -0.12394649203460996, "compression_ratio": 1.7914691943127963, "no_speech_prob": 3.187543825333705e-06}, {"id": 667, "seek": 347432, "start": 3489.7200000000003, "end": 3497.52, "text": " tabular data loader is a transformed data loader where we know that any after batch", "tokens": [4421, 1040, 1412, 3677, 260, 307, 257, 16894, 1412, 3677, 260, 689, 321, 458, 300, 604, 934, 15245], "temperature": 0.0, "avg_logprob": -0.12394649203460996, "compression_ratio": 1.7914691943127963, "no_speech_prob": 3.187543825333705e-06}, {"id": 668, "seek": 347432, "start": 3497.52, "end": 3502.76, "text": " that you asked for we have to also add in read tab batch.", "tokens": [300, 291, 2351, 337, 321, 362, 281, 611, 909, 294, 1401, 4421, 15245, 13], "temperature": 0.0, "avg_logprob": -0.12394649203460996, "compression_ratio": 1.7914691943127963, "no_speech_prob": 3.187543825333705e-06}, {"id": 669, "seek": 350276, "start": 3502.76, "end": 3513.36, "text": " So that's how that's automatically added to the transforms for you.", "tokens": [407, 300, 311, 577, 300, 311, 6772, 3869, 281, 264, 35592, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.08223671510995152, "compression_ratio": 1.5980392156862746, "no_speech_prob": 1.0030076964540058e-06}, {"id": 670, "seek": 350276, "start": 3513.36, "end": 3521.0, "text": " The other thing about tabular data loader is we want to do everything a batch at a time.", "tokens": [440, 661, 551, 466, 4421, 1040, 1412, 3677, 260, 307, 321, 528, 281, 360, 1203, 257, 15245, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.08223671510995152, "compression_ratio": 1.5980392156862746, "no_speech_prob": 1.0030076964540058e-06}, {"id": 671, "seek": 350276, "start": 3521.0, "end": 3527.5200000000004, "text": " So particularly for the rapids on GPU stuff we don't want to pull out individual rows", "tokens": [407, 4098, 337, 264, 5099, 3742, 322, 18407, 1507, 321, 500, 380, 528, 281, 2235, 484, 2609, 13241], "temperature": 0.0, "avg_logprob": -0.08223671510995152, "compression_ratio": 1.5980392156862746, "no_speech_prob": 1.0030076964540058e-06}, {"id": 672, "seek": 350276, "start": 3527.5200000000004, "end": 3529.0, "text": " and then collect them later.", "tokens": [293, 550, 2500, 552, 1780, 13], "temperature": 0.0, "avg_logprob": -0.08223671510995152, "compression_ratio": 1.5980392156862746, "no_speech_prob": 1.0030076964540058e-06}, {"id": 673, "seek": 350276, "start": 3529.0, "end": 3532.0200000000004, "text": " Everything's done by grabbing a whole batch at a time.", "tokens": [5471, 311, 1096, 538, 23771, 257, 1379, 15245, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.08223671510995152, "compression_ratio": 1.5980392156862746, "no_speech_prob": 1.0030076964540058e-06}, {"id": 674, "seek": 353202, "start": 3532.02, "end": 3537.48, "text": " So we replace do item which is the thing that normally grabs a single item for collection.", "tokens": [407, 321, 7406, 360, 3174, 597, 307, 264, 551, 300, 5646, 30028, 257, 2167, 3174, 337, 5765, 13], "temperature": 0.0, "avg_logprob": -0.2171859034785518, "compression_ratio": 1.8072916666666667, "no_speech_prob": 1.8448150740368874e-06}, {"id": 675, "seek": 353202, "start": 3537.48, "end": 3541.44, "text": " We replace it with do nothing, replace it with no of.", "tokens": [492, 7406, 309, 365, 360, 1825, 11, 7406, 309, 365, 572, 295, 13], "temperature": 0.0, "avg_logprob": -0.2171859034785518, "compression_ratio": 1.8072916666666667, "no_speech_prob": 1.8448150740368874e-06}, {"id": 676, "seek": 353202, "start": 3541.44, "end": 3546.44, "text": " And then we replace create batch which is the thing that normally collects things to", "tokens": [400, 550, 321, 7406, 1884, 15245, 597, 307, 264, 551, 300, 5646, 39897, 721, 281], "temperature": 0.0, "avg_logprob": -0.2171859034785518, "compression_ratio": 1.8072916666666667, "no_speech_prob": 1.8448150740368874e-06}, {"id": 677, "seek": 353202, "start": 3546.44, "end": 3553.78, "text": " say don't collect things but instead actually grab all of the samples directly from the", "tokens": [584, 500, 380, 2500, 721, 457, 2602, 767, 4444, 439, 295, 264, 10938, 3838, 490, 264], "temperature": 0.0, "avg_logprob": -0.2171859034785518, "compression_ratio": 1.8072916666666667, "no_speech_prob": 1.8448150740368874e-06}, {"id": 678, "seek": 353202, "start": 3553.78, "end": 3557.36, "text": " tabular object using my lock.", "tokens": [4421, 1040, 2657, 1228, 452, 4017, 13], "temperature": 0.0, "avg_logprob": -0.2171859034785518, "compression_ratio": 1.8072916666666667, "no_speech_prob": 1.8448150740368874e-06}, {"id": 679, "seek": 355736, "start": 3557.36, "end": 3563.08, "text": " So this is if you look at that blog post I mentioned from even at Nvidia about how they", "tokens": [407, 341, 307, 498, 291, 574, 412, 300, 6968, 2183, 286, 2835, 490, 754, 412, 46284, 466, 577, 436], "temperature": 0.0, "avg_logprob": -0.16313396800648083, "compression_ratio": 1.7578125, "no_speech_prob": 2.090445150315645e-06}, {"id": 680, "seek": 355736, "start": 3563.08, "end": 3570.08, "text": " got the 16x speed up by using rapids, a key piece of that was that they wrote their own", "tokens": [658, 264, 3165, 87, 3073, 493, 538, 1228, 5099, 3742, 11, 257, 2141, 2522, 295, 300, 390, 300, 436, 4114, 641, 1065], "temperature": 0.0, "avg_logprob": -0.16313396800648083, "compression_ratio": 1.7578125, "no_speech_prob": 2.090445150315645e-06}, {"id": 681, "seek": 355736, "start": 3570.08, "end": 3575.08, "text": " version of this kind of stuff to kind of do everything batch at a time.", "tokens": [3037, 295, 341, 733, 295, 1507, 281, 733, 295, 360, 1203, 15245, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.16313396800648083, "compression_ratio": 1.7578125, "no_speech_prob": 2.090445150315645e-06}, {"id": 682, "seek": 355736, "start": 3575.08, "end": 3580.76, "text": " And this is one of the key reasons we replaced the PyTorch data loader is to make this kind", "tokens": [400, 341, 307, 472, 295, 264, 2141, 4112, 321, 10772, 264, 9953, 51, 284, 339, 1412, 3677, 260, 307, 281, 652, 341, 733], "temperature": 0.0, "avg_logprob": -0.16313396800648083, "compression_ratio": 1.7578125, "no_speech_prob": 2.090445150315645e-06}, {"id": 683, "seek": 355736, "start": 3580.76, "end": 3582.0, "text": " of thing super easy.", "tokens": [295, 551, 1687, 1858, 13], "temperature": 0.0, "avg_logprob": -0.16313396800648083, "compression_ratio": 1.7578125, "no_speech_prob": 2.090445150315645e-06}, {"id": 684, "seek": 355736, "start": 3582.0, "end": 3587.2000000000003, "text": " So as you can see creating a kind of a batch at a time data loader is seven lines of code", "tokens": [407, 382, 291, 393, 536, 4084, 257, 733, 295, 257, 15245, 412, 257, 565, 1412, 3677, 260, 307, 3407, 3876, 295, 3089], "temperature": 0.0, "avg_logprob": -0.16313396800648083, "compression_ratio": 1.7578125, "no_speech_prob": 2.090445150315645e-06}, {"id": 685, "seek": 358720, "start": 3587.2, "end": 3592.0, "text": " super nice and easy.", "tokens": [1687, 1481, 293, 1858, 13], "temperature": 0.0, "avg_logprob": -0.15093433289300828, "compression_ratio": 1.2586206896551724, "no_speech_prob": 3.041538093384588e-06}, {"id": 686, "seek": 358720, "start": 3592.0, "end": 3601.8399999999997, "text": " So yeah I was pretty excited when this came out so quick.", "tokens": [407, 1338, 286, 390, 1238, 2919, 562, 341, 1361, 484, 370, 1702, 13], "temperature": 0.0, "avg_logprob": -0.15093433289300828, "compression_ratio": 1.2586206896551724, "no_speech_prob": 3.041538093384588e-06}, {"id": 687, "seek": 358720, "start": 3601.8399999999997, "end": 3614.68, "text": " Okay so that's what happens when we create the tabular data loader.", "tokens": [1033, 370, 300, 311, 437, 2314, 562, 321, 1884, 264, 4421, 1040, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.15093433289300828, "compression_ratio": 1.2586206896551724, "no_speech_prob": 3.041538093384588e-06}, {"id": 688, "seek": 361468, "start": 3614.68, "end": 3631.7999999999997, "text": " We could of course also create a data bunch which would probably add this to the example.", "tokens": [492, 727, 295, 1164, 611, 1884, 257, 1412, 3840, 597, 576, 1391, 909, 341, 281, 264, 1365, 13], "temperature": 0.0, "avg_logprob": -0.21534963448842367, "compression_ratio": 1.5315789473684212, "no_speech_prob": 2.9943848858238198e-06}, {"id": 689, "seek": 361468, "start": 3631.7999999999997, "end": 3633.96, "text": " And yeah that's basically it.", "tokens": [400, 1338, 300, 311, 1936, 309, 13], "temperature": 0.0, "avg_logprob": -0.21534963448842367, "compression_ratio": 1.5315789473684212, "no_speech_prob": 2.9943848858238198e-06}, {"id": 690, "seek": 361468, "start": 3633.96, "end": 3637.6, "text": " So then at inference time as we discussed you can now do the same dot new trick we saw", "tokens": [407, 550, 412, 38253, 565, 382, 321, 7152, 291, 393, 586, 360, 264, 912, 5893, 777, 4282, 321, 1866], "temperature": 0.0, "avg_logprob": -0.21534963448842367, "compression_ratio": 1.5315789473684212, "no_speech_prob": 2.9943848858238198e-06}, {"id": 691, "seek": 361468, "start": 3637.6, "end": 3643.04, "text": " before in the dot process and then you can grab whatever is orc holes which is going", "tokens": [949, 294, 264, 5893, 1399, 293, 550, 291, 393, 4444, 2035, 307, 420, 66, 8118, 597, 307, 516], "temperature": 0.0, "avg_logprob": -0.21534963448842367, "compression_ratio": 1.5315789473684212, "no_speech_prob": 2.9943848858238198e-06}, {"id": 692, "seek": 364304, "start": 3643.04, "end": 3648.36, "text": " to give us a data frame with all the modeling columns and since this is not so show batch", "tokens": [281, 976, 505, 257, 1412, 3920, 365, 439, 264, 15983, 13766, 293, 1670, 341, 307, 406, 370, 855, 15245], "temperature": 0.0, "avg_logprob": -0.24347091582884273, "compression_ratio": 1.7119565217391304, "no_speech_prob": 6.540255981235532e-06}, {"id": 693, "seek": 364304, "start": 3648.36, "end": 3653.2799999999997, "text": " will be the decoded version but this is not the decoded version this is the encoded version", "tokens": [486, 312, 264, 979, 12340, 3037, 457, 341, 307, 406, 264, 979, 12340, 3037, 341, 307, 264, 2058, 12340, 3037], "temperature": 0.0, "avg_logprob": -0.24347091582884273, "compression_ratio": 1.7119565217391304, "no_speech_prob": 6.540255981235532e-06}, {"id": 694, "seek": 364304, "start": 3653.2799999999997, "end": 3658.12, "text": " that you can pass to your modeling.", "tokens": [300, 291, 393, 1320, 281, 428, 15983, 13], "temperature": 0.0, "avg_logprob": -0.24347091582884273, "compression_ratio": 1.7119565217391304, "no_speech_prob": 6.540255981235532e-06}, {"id": 695, "seek": 364304, "start": 3658.12, "end": 3662.08, "text": " All right any questions Andrew?", "tokens": [1057, 558, 604, 1651, 10110, 30], "temperature": 0.0, "avg_logprob": -0.24347091582884273, "compression_ratio": 1.7119565217391304, "no_speech_prob": 6.540255981235532e-06}, {"id": 696, "seek": 364304, "start": 3662.08, "end": 3663.08, "text": " Okay cool.", "tokens": [1033, 1627, 13], "temperature": 0.0, "avg_logprob": -0.24347091582884273, "compression_ratio": 1.7119565217391304, "no_speech_prob": 6.540255981235532e-06}, {"id": 697, "seek": 364304, "start": 3663.08, "end": 3668.48, "text": " All right thanks all that is it and it's Friday right?", "tokens": [1057, 558, 3231, 439, 300, 307, 309, 293, 309, 311, 6984, 558, 30], "temperature": 0.0, "avg_logprob": -0.24347091582884273, "compression_ratio": 1.7119565217391304, "no_speech_prob": 6.540255981235532e-06}, {"id": 698, "seek": 366848, "start": 3668.48, "end": 3674.08, "text": " Yeah so I think we're on for Monday I'll double check and I'll let you all know.", "tokens": [865, 370, 286, 519, 321, 434, 322, 337, 8138, 286, 603, 3834, 1520, 293, 286, 603, 718, 291, 439, 458, 13], "temperature": 0.0, "avg_logprob": -0.30810870622333725, "compression_ratio": 1.145631067961165, "no_speech_prob": 2.7958627470070496e-05}, {"id": 699, "seek": 366848, "start": 3674.08, "end": 3676.28, "text": " Either way I will see you later.", "tokens": [13746, 636, 286, 486, 536, 291, 1780, 13], "temperature": 0.0, "avg_logprob": -0.30810870622333725, "compression_ratio": 1.145631067961165, "no_speech_prob": 2.7958627470070496e-05}, {"id": 700, "seek": 367628, "start": 3676.28, "end": 3699.0, "text": " Bye.", "tokens": [50364, 4621, 13, 51500], "temperature": 0.0, "avg_logprob": -0.8092408180236816, "compression_ratio": 0.3333333333333333, "no_speech_prob": 0.00013110326835885644}], "language": "en"}