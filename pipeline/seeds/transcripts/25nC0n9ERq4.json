{"text": " So I'm Rachel Thomas and I'm sorry I wasn't able to be here earlier because I was teaching at University of San Francisco but it sounds like it was a really exciting morning and earlier part of the day and I wanted to get a feel just for people's backgrounds. Raise your hand if you're an engineer. Okay how about data scientists? Okay, PM, designers, cool it's a great variety and then are there any that I missed? Feel free to shout them out and I definitely, please stop and ask questions and if you have them because I really want this to be helpful for you. So I wanted to start just briefly share a little bit of my backgrounds. I studied math and computer science in undergrad and then did a PhD in mathematics. I worked as a quant for two years in energy trading which involves a lot of programming and working with data and that's what interested me in becoming a data scientist. I was a data scientist and backend engineer at uber in 2013 and 2014 and then I taught full stack software development at hackbrite which is the all women coding academy and I know there are several folks from there here today. Yay, I love teaching and I love working with women. One year ago I started fast AI with the goal of making deep learning more accessible and easier to use and we also partner with the University of San Francisco so I work there part-time. I'm on Twitter at math Rachel. I blog about diversity and inclusion on medium at Rachel THO and then I blog about data science at fast.ai. So first I just wanted to kind of define what is deep learning. I know it often seems like there are a lot of buzzwords in the data science space but deep learning refers to a very specific class of algorithms, multi-layered neural networks and it's a subclass of machine learning. Is there is the feedback annoying on the microphone or it's okay? Tolerable. Let me see if it's like better if it's a little bit further. Can you still hear me? Okay cool that sounds better. Yeah so deep learning is a subset of machine learning just the specific class of algorithms which is a subset of AI and AI is a very broad field. A lot of a lot of the kind of specific advances being made in AI right now are all coming from deep learning though. Sorry I wanted to go to full screen mode to get rid of the sidebar. Okay there we go. So a kind of really common example of deep learning that you've probably seen is Google Photos will automatically group your photos for you. So here it's kind of group them into skylines, temples, food and deep learning is behind that kind of automatic classification. And then another great example is Skype Translator. So Skype Translator can translate in real time between eight different languages for voice and between 50 languages for text and written. So deep learning is particularly powerful for working with images and for working with language which are two huge areas. This is an article from Fortune last fall about why you need to learn about deep learning. A lot of people have been comparing deep learning to the internet in terms of the impact that it's going to have. I'm kind of saying you know we're at just like the early 90s in terms of when the internet was still young and there's just so much potential for what can be done. Andrew Ng is a CS professor from Stanford. He's the co-founder of Coursera and he was chief scientist at Baidu for four years which is you know one of the top tech companies out there. And he said that comparing deep learning to the internet doesn't do it justice and he thinks it's more like electricity in terms of the impact it'll have in transforming industry after industry. I wanted to, so I kind of showed you some very mainstream examples with Google Photos and Skype of where deep learning is being used and I wanted to share some less common ones and these all come from students in a course that I helped create with Fast AI. And so one of them is Xinxin Li and she is working on wearable devices to monitor Parkinson's disease which is a neurodegenerative disorder. And so typically doctors will kind of watch a patient walk to see how the disease is progressing and just do this like visual assessment. But she's a kind of consultant with Akashi which is working with Exceed which makes the wearables and they're developing algorithms that this could be tracking the patient's movement, gathering a lot more data you know than just observing someone while they're in the doctor's office and so this could be a lot more accurate. Another one of our students is Sarah Hooker. She's founder of Delta Analytics which partners teams of data scientists to volunteer with nonprofits and help them with their data science problems. And she's with in this picture a team of other data scientists that are working together with Rainforest Connection and Rainforest Connection puts recycled cell phones in endangered rainforest and then streams audio and uses deep learning to identify chainsaw noises if people are illegally cutting down the forest. And then they have responders that they can send out in the field and so this is a deep learning problem of identifying what's a chainsaw and what's not. She said that mosquitoes actually have a very similar frequency to chainsaws. So there are a lot of kind of interesting challenges and what they're what they're facing. These are pictures sent to me by Sahil Singla from India who works with Farm Guide and in India thousands of farmers commit suicide every year because well largely because they are taking these really predatory loans from loan sharks that then kind of threaten them with violence and threaten their families. And part of the issue is that they can't prove how much land they own or what kind of crops they're growing. And so Sahil and his team are scraping data from Google Earth and then using deep learning to identify the plots of land and what's being grown to help the farmers qualify for better fairer loans. And then Samar Hayder is another one of our students and he's a natural language researcher in Pakistan. Pakistan has over 70 spoken languages and none of them not even Urdu which is the most common have that many resources in terms of being able to translate other materials into and out of the languages. And he was inspired by our course to kind of build something similar to Word2Vec but for Urdu. And so this will be a really useful tool for people working with the language. And then this is a little bit more light-hearted example created by one of our students. This is a picture of Kanye West drawn with Captain Picard's face. So you'll see their Captain Picard's faces in miniature making Kanye West's face. So deep learning is being used to create art as well. And then kind of a more more serious art style another one of our students Vincent Moran came up with a new style transfer technique. Let me open this up and this is actually a video. I'll just show you part of it. But it takes a photo such as of this owl and then it takes a picture with a particular style like this psychedelic cat and it gradually changes the owl to more and more have the style of the psychedelic cat. And so this is I think really can make some really pretty and interesting stuff from this. And you can you can apply this to other other styles not just like a delicate but combining photos with Van Gogh or with other artists with distinctive styles. Let me just do one more and then we'll stop this. And I think it's neat to kind of see the transformation of you know gradually going from you know this photo of the earth to this artistic rendering. So I just kind of share these to show that there's a really wide variety of the type of problems that can be or even you know type of things that can be created with deep learning. So I'll be mentioning this some during this. So fast AI is me and Jeremy Howard and we partnered with the University of San Francisco's Data Institute and taught a course this fall this past fall deep learning part one and it was distinctive in that there were no math prerequisites and a lot of materials for deep learning kind of assume that you have a graduate level math background and we wanted to see if we could create something that anyone with a year or two of coding experience could do. And we actually weren't even sure if it would work it was a bit of an experiment and we released it for free online so all the materials are available for free at course.fast.ai and it was a huge success. We had one of our students was accepted to the Google Brain residency program which is very prestigious and she just learned to code two years ago. We had several other students get new job offers. One student got a patent for an algorithm he developed that he said is actually pretty similar to one of our lessons and that got him a bonus at work. We had one of our students worked for the TV show Silicon Valley and so he created the not hot dog app if you're watching the current season and he also contributed code back to core tensorflow as part of that. So a lot of interesting stories coming out of it and so I'll occasionally mention the course but just wanted to let you know that it's there and it's completely free. And then we also have forums going along with it at forums.fast.ai and this is a place where people can ask questions or share interesting links and I think it's a pretty helpful community and so if you're even if you're not taking the course if you have deep learning related questions or things that you find interesting I wanted to let you know that this is out there. Okay so move on to and again stop me at any point if you have questions. So I'll stop now. Are there any questions? Okay so moving on to today's topic which is word embeddings. First I wanted to show you where word embeddings are used just a few examples. So this is an article from last year but Baidu's deep learning system is better at English and Mandarin speech recognition than most people which is really impressive and that's using word embeddings. This is called image captioning and these are algorithms that you give a picture such as picture on the left here and then the algorithm comes up with a caption. So in this case man in black shirt is playing guitar or for the picture on the right you give it the picture and the algorithm comes up with construction worker an orange safety vest is working on the road. And so this is that a few years ago something that was not not being done. And then this is a Google's smart reply for your email where it makes a few suggestions of how you how you can reply to an email. This is somebody asking about vacation plans and Google's suggesting that maybe you want to say no plans yet or I just sent them to you. This was actually an April Fool's Day joke from Google in 2009 because at that point it was so outlandish that you know email could write itself and then they a few years later released it as a product. But all of these are using word embeddings. And then this is one from part two of our course we taught part two this spring and here you can give it words and so we're saying tench plus net. Tench is the type of fish and then we're seeing back what image we get. And this is a tension and net. This is kind of the reverse. You give it some words you get back an image. This is a blog post from another one of our students and I like that it's how QUID uses deep learning with small data. And QUID is a company that has a lot of a lot of data sets. There's a lot of data science and they said that one issue for them is they have a database of company descriptions and having kind of low quality descriptions that are very generic and vague. They want to identify those because those are bad for their use case. And so for example having something that says our patent pending support system is engineered and designed to bring comfort and style that doesn't actually convey much information and so that would be a bad description that they're trying to identify. Whereas an example of a good description would be it has developed lens lensless imaging optic, the natural eye optic, the NEO. The NEO replicates the human vision system. That's a description that actually has information details and they wanted to be able to distinguish between these. And they use deep learning including word embeddings to do that. So computers and machine learning treat pictures and words as numbers. So to be able to kind of process pictures and words they need to read them in as numbers. This is an example and we're not really talking about images today but I just wanted to show you this because it shows up so much. But this is a handwritten digit 8 and showing how it can be represented as a 20 by 20 matrix of numbers. And in this case each white pixel is a 0, each black pixel is 255, and then you can have any value in between for gray. And so this is a way to you know take this white and black picture and transfer it to a 20 by 20 matrix. And that's how the computer is going to work with it. And then we'll be going into much more detail about word embeddings because that's how computers can deal with language. But I just want to let you know that there are vectors and they can capture relationships. Like here we see there's a particular relationship between walking and walked. That's very similar to the relationship between swimming and swam. So kind of to move from one vector to the other you're going the same same distance and direction to get between these different verb tenses. So before I go further into this, I actually wanted to check so who in here loves math? Cool, me too. And then who in here has some sort of anxiety or any negative feelings about math? Okay so yeah, few people, not as many as I often hear, but I wanted to explain this because I get this a lot from people who really don't feel good about their own math ability or kind of have anxiety around it. So there's a beautiful essay called A Mathematician's Lament by Paul Lockhart. And Paul Lockhart was a math professor at Brown and he quit to go teach KQ12 education because he thought math education was such a disaster in the United States. And so he has this essay where he writes about a world where children are not allowed to sing songs or play instruments until graduate school because it's really important that they have over a decade of studying music notation and transcribing notes by hand before we can let them sing. And so it's a, you know, sounds horrifying, like that doesn't make sense to make people wait till they're, you know, 20 to sing. But that's what we do with math education is what he argues. And I agree that we often focus on kind of dry disconnected notation and that the really more beautiful and creative parts of it don't come until later after many people have dropped out of the field. This is an essay called The Myth of Innate Ability in Tech by Omuju Miller. And Omuju has her PhD in CS education from Berkeley. And this, I think this myth shows up a lot in mathematics as well. But it's the idea of like, oh, and this idea is completely false and the evidence is against it, but the idea that, oh, some people, you know, innately, you know, have a brain for math and some people don't, that becomes a self-fulfilling prophecy. And I think it's particularly prevalent in the United States, this idea that some people can, yeah, their brain works that way, other people's don't. And that's not the case, but it's a very harmful myth. Then it gets worse if you're a woman in that there was a study, so 90% of elementary school teachers are women in the United States. And I apologize, what I'm saying is very US-centric and that's what I'm kind of most familiar with the research for, but this is something that really varies a lot from culture to culture. And you will see like there's some countries where, you know, there are a ton of girls on the Math Olympiad teams at all levels. But yeah, this is an area where the US is not strong. But so most elementary school teachers are women and in college the major with the highest level of math anxiety is elementary education. And then this often gets transmitted to kids kind of subconsciously. And there's a study that found, and this was published in the Proceedings of the National Academy of Sciences, which is very prestigious kind of top journal, that girl children pick up on that anxiety and it impacts how they see the gender ability around math, whereas boys students don't pick up on it from women teachers. So it's kind of harmful to girls but not to boys. And so this is something that may be impacting people, or definitely is impacting people. And then another issue is in the US there are a lot of stereotypes and myths about the ability of people of color. And so particularly if you're black or Latina or indigenous you may have encountered kind of negative stereotypes or perceptions about your math ability. And I just wanted to share all these of kind of just all the forces that might be kind of conspiring against you loving math as you could and that have nothing to do with your actual ability or aptitude. So there's this guy on Twitter that said you to do machine learning you have to love math as a teenager, which I think is false. And so I responded many kids are turned off to math because of poor teaching methods, lack of resources, lack of support, doesn't mean you can't learn machine learning later. And then all these people responded with really inspiring stories and so I just wanted to share a few of them. So one is data science Renee who is a data scientist who has a large Twitter following and a data science podcast. She says that she struggled with certain teachers and with ADHD and that she didn't start she didn't take advanced math until she was over 30 years old and she became a data scientist at 34. Stephanie said that she didn't love math until age 36 when she returned to school for a biology degree and now she's becoming a data scientist. And then my favorite was Kimberly said I'm learning machine learning as a 50 year old with straight A's and a PhD program in biomedical informatics and health data science age is just a number. So I thought that was really inspiring and just yeah for any of you that did feel a little anxious about math I hope this is inspiring. Oh and then one more this came out just a week or two ago at Cathy O'Neill. Weapons of math destruction is very good yeah if you're I'll mention it a few times if you're interested in bias and algorithms and Cathy has a book called weapons of math destruction she also blogs at mathbabe.org and she she has her PhD in math from Harvard but she wrote this article in Bloomberg that a mathematician's secret we're not all geniuses and she says you don't have to be a genius to become a mathematician if you find the statement at all surprising you're an example of what's wrong with the way our society identifies encourages and rewards talent. So I definitely agree with that and then she also says to even imagine becoming a mathematician I had to ignore the question of whether I was a genius or whether I would need to be a genius had I been poked and prodded and measured to see how exceptional I was I probably would have lost the nerve. But yeah so just some more confirmation and I just wanted to kind of put all that out there before I get into get into numbers. So going back to our topic of word embeddings we need a way to represent words as numbers and so one approach would be to go through the dictionary and number words so art of art is one, apple's two, art is three and so on. Does anyone see what a shortcoming of this approach might be? So a big one is this doesn't really give us any notion of what it means for words to be similar so you see cat and catastrophe are right next to each other but there's not you know some deep deep link between cat and catastrophe and that doesn't capture anything of like how is cat related to kitten if we just have this numbered list. So a better approach is to treat words as vectors so vectors just a list of numbers and here I've just made these up that we're gonna have three three numbers in each vector in reality we're gonna have a lot more but we could say you know puppy's gonna be represented as 0.9 comma 1.0 comma 0.0 and dog is 1.0 comma 0.2 comma 0.0 and so looking at this what might you say about the first first number in each vector? See I heard someone shout out canine so this is something that puppies and dogs have a lot of and that kittens and cats don't have. I'm gonna capture there's something about dogs. How about for the the second number? Age, youth, yeah so we see that kittens and puppies are both very young whereas dogs and cats are not so much and then the third one is something cat related or feline and so this is neat because it's given us a way to capture some meaning and to say you know cat and kitten are similar in this way and then there's also you know the similarity between kittens and puppies and so these are these are word embeddings and so I I made up these numbers but in reality it would be virtually impossible for a human to do this well just because you know there's so many dimensions of meaning and to try to capture that would be really tough and so this is something that we want to train computers to come up with for us so I wouldn't actually make up the word embedding we're gonna get the computer to do that. So what is Word2Vec? Word2Vec is a library of word embeddings released by Google. It's not deep learning although it can and often is used in deep learning including in the examples I showed before or mentioned before of speech translation, image captioning, and the email generation so those are all using kind of deep learning and word embeddings. Word2Vec is not an algorithm although it used specific algorithms to train it and come up with the embeddings and then GloVe is a very similar set of embeddings that was created at Stanford and I'm gonna be using GloVe today but you can think of them as kind of two versions of the same thing they're both both sets of word embeddings. So why it's useful? So training Word2Vec takes a lot of data and that data may not even all be publicly available so in the case of Google they have a lot of data particularly with their books that isn't publicly available that they used in training Word2Vec. It can be really time-consuming and slow and it can take a lot of computational power and so Google has a lot more servers than I do and so it's really handy for them to do this and then share the results with the rest of us and it's yeah it's much easier to download this already trained version and when I say trained that's this process of coming up with what the the word embeddings should be. So learning meaning from context and so this is famous quote by the linguist J.R. Firth you shall know a word by the company it keeps. So I just went to the New York Times and I googled for banks and then took a few quotes with words before and after them and the idea of learning meaning from context is that the words that appear near bank repeatedly are an indicator of what it means because they represent how it's used. So you'll see we've got words like Wall Street, hyped rates, soft inflation readings, economy, gross domestic product, tightening monetary policy and you're gonna do this for a huge corpus of tax and kind of see what's kind of repeatedly showing up near banks and get an indication of like okay those things are related. Yeah so words frequently used together with bank represent its meaning. Are there questions about this? And the idea is that even though some words show up like you know presence which doesn't particularly have anything to do with banks that's gonna show up next to a bunch of other words and so it's not disproportionately showing up near banks so that wouldn't be picked up in the final embedding. Yes? Yes, yes exactly. Yeah and I actually have to repeat the question since I'm recording this but the question was you know Watson is reading all of Wikipedia do we have to translate all of that into numbers and it does this very quickly and yes it is yeah yeah in three seconds is what I'm hearing yeah so that's um and that comes back to they have I'm sure they have a ton of GPUs and really powerful computers and so that's something that you and I would not be able to do without access to kind of like these really expensive resources and actually I think that's a good a good lead-in to I just so we're gonna get to Jupiter notebook in a little bit but I have an example and I should say this example oh there's another question yes so that's something that in a general version is not going to be able to tell different meanings I'll actually see that in a little bit in the notebook intelligence is going to be linked to things like CIA as well as you know being smart even though those are kind of two different meanings and there are there are linguists that build more complicated systems kind of on top of this are using these tools but the kind of basic approach yeah it's not going to pick up on this has two different meanings yes and so the question was do they also define it by words that would never be associated associated with bank and that's correct yes and what they actually do is they randomly change some of the words so they'll kind of like put the wrong word in in place and then say that's not what you're supposed to get now these are good questions anymore okay so this is an Excel and I did this just um excels are really can be a very useful visual tool just to see what things look like laid out I was starting to say these this as well as the notebook are I've been modified from part five or less than five of our deep learning course but here this is the text of Sam I am and it's a good size would you need it larger a little bit larger okay so let me go over here so this column is the original text I am Daniel I am Sam Sam I am that Sam I am I do not like that Sam I am and so if we wanted to be able to represent this into a computer what we need is a dictionary of all the the words that show up and fortunately this is only 14 words so this is a pretty simple example we're giving them each a word ID so this was kind of our initial numbering the dictionary and that is a step but that's not where we're gonna stay and then we're using a five-dimensional embedding so here these five numbers represent am so this vector of length five is am and wherever am shows up in the text we're gonna put that vector so it's 0.98 0.95 scroll over a little bit and so you'll see the same vector for am shows up many times since am shows up many times oh whoops that's I okay ignore where I accidentally highlighted I but all the every time am is in the text you're putting the vector for am and it's the exact same so even though we're kind of getting this matrix now that represents the text of green eggs and ham it's just these 14 vectors being repeated where needed to represent the words are there questions about this yes so the question was how do you determine how many vectors you need and I assume you mean the kind of the dimension of the vector and that's an interesting question and actually with gloves they have a bunch of different versions there's like a 50 dimensional version and a hundred and two hundred so I think that I don't know that there's a clear answer I mean I think some of it would be like experimentation to see for the problem you're working on what gets you better results yeah it's a good am you asking if more more dimensions means that you can capture more meaning and I think quite possibly yeah like definitely at a large scale yeah like if you only if you were trying to capture 40,000 words in five dimensions you're not going to be able to capture much meaning so for that I just meet these up but typically you would be looking those up like in glove or in word2vec and so that's where word2vec or glove comes in handy it's like that'll tell you like okay use this vector yeah but there's not not actually a five-dimensional version although this is something that you could potentially train yourself because this is a small small data set although you also I guess wouldn't capture much meaning because you have a very small input text there's that question over here so there's um there's a yeah there's several and so basically it's you can download these but word2vec is available for download glove is available and I've got instructions for downloading glove because we have a site hosted at files.fast.ai but yeah you can get them and you've got yeah this is long list of words and we'll actually get into this in a moment I'll show you what it looks like and then this yeah this dictionary can like look up a word and get get an embedding and no so computers did this so there's an algorithm that was used to train this that kind of took in a ton of yeah a ton of text from the lab and then came up with embeddings yes the three-dimensional ones yeah yeah so the question is what's some intuition about how do you get to these like really large embeddings where you have a ton of words and the idea behind it is kind of going back to this thinking about what words appear together and so word2vec is kind of training taking in just a bunch of text from Wikipedia or from Google Books and training based on what words are showing up next to each other and then if someone asked earlier it also kind of has these counter examples where it's like randomly put words together that wouldn't necessarily go together and is training to find an embedding that kind of puts things that are near each other and text often closer together in the embedding oh yeah so it's yeah so it's very likely yeah that Sam and Daniel could have a something in common that indicates they're both first names something that's a little bit subtle about this is that since you know the computer is not saying like this is a first name so I'm gonna make it similar here it's really just about how it's used it's not always obvious to us looking at a vector of like what is this capturing or what is this close to but you're right like and I think yeah like men's names are going to be kind of like clumped together ah no it's a good question yes so the the question is about grammar and is this acknowledging you know whether a word could be a pronoun or verb tense and this is not explicitly acknowledging that but that shows up like the example with you know swim is to swimming is walk is to walking and that's just coming from how they're used that kind of the way they're going to be used in a sentence can capture it like what words are near it but it's not not anything explicit and this is a very statistical approach to language of kind of you just have like all this data and you're seeing how it's used whereas there are linguists that kind of study things in a more like a way where they're talking about the structure of language or what the rules we know about language are so the question was is where to back just from empirical observations of how words are used together and yes yeah it's just coming from how they're used together and this is something that linguists have very and natural language researchers have very differing opinions on kind of how to approach this so that's Carrie who's going to be speaking later today I know that her company pat.ai is taking a very different approach and that they're not using statistics and I think do have more of a kind of rule based thing that they know whereas word to back though is just yeah like just take a ton of ton of text and assume that that's going to kind of capture information you need any more questions all right this is great let me go back to to the slides and yeah so being able to learn meaning from context you know put it in vectors we are able to capture similarity so this is from glove which will be using today and the question is what words are closest to the word frog and what they've done is see what vectors are closest to the vector representing frog and what words do they represent so this is just coming from comparing how far apart different vectors are in this list of numbers and the first closest thing to frog is frogs plural so that's what we would expect and then second is toad that's Latoria which I didn't know but that's a looks like some sort of tree frog next is leptodactylde which is another type of frog number six is lizard and so this this seems reasonable what we would expect that it is that is capturing things near frog and these different types of frogs are all close together and in terms of what their vectors look like okay so I'm going to switch over to the Jupiter notebook demo now and for that if you haven't already seen it it's at github.com you can go to slash fast AI slash word- embeddings-workshop to get the the materials needed although I've tried to create this that I think it'll be interesting to watch even if you don't have it running on your computer but everything is available there so that you can run this yourself and then actually let me get a show of hands who who's used Jupiter notebooks before okay it looks like most people who's new to Jupiter notebooks oh okay a lot of people and so let me talk a little bit about Jupiter notebooks as well so let me go to the github page so Jupiter notebooks are super useful and very widely used particularly by data scientists and what they what they allow you to do is I'll go back to the notebook in a web browser to mix text and code and diagrams and math equations and pictures and so they're also a really good teaching tool and there are a lot of kind of online textbooks now appearing as Jupiter notebooks and to create something that you can share with others that has you know has text and links and then these interactive blocks where you can write code but just to get set up with these so here this would be from the console you can use W get to get the the glove embeddings and so here I've zipped up 50 dimensional version as well as a hundred dimensional version of glove and then you can kind of tar them to open them and I'll be using Python so you might want to create a virtual environment unless you already have anaconda or kind of the scientific Python stack installed on your computer and what you do to start a Jupiter notebook is type Jupiter notebook into the console and then your web browser should kind of pop up with it with it running and feel feel free to stop me if you have questions so kind of setting out on this we've got some import statements just to import the libraries we need here I've put the same information that's on github of where to go to get the data and you'll have to update this kind of based on you know where in your computer you know put it in the same directory is where you're running Jupiter notebook from so that you can access it I'm using numpy which is kind of a core numerical library in Python that's yeah very fast and optimized and very useful and I've state saved the glove vectors as numpy arrays and so we're using numpy dot load to open these files and I think it's help it or most helpful kind of whenever you have different objects that you're using in Python or any language it's really just nice to see what is your data look like like what are your objects so here we're going to look at words words is a list it is 400,000 long so we've got a list of 400,000 words and doing words square brackets colon 10 that slices off the first 10 elements of the list and so we can see words starts the comma period of two and in a double quotes so that's those are words or and here it's defined a bit loosely you know that we're including period and comma I thought it'd be more interesting to look later in the list since we just kind of had these short ones at the beginning so I just randomly decided let's look at what said place 600 to 610 in our word list and I get these words together Congress index Australia results and so on actually is good or should I go larger on this larger okay yeah and feel free to shout out stuff like that if one more yeah if you're having trouble seeing something or something's unclear and so this is words it's just a 400,000 long list of words their questions so far and so these I actually don't remember exactly what text love was trained on and I would guess that these words might be ordered by frequency since the ones in the the first 10 are all very common words but this would be something like Wikipedia or a bunch of a bunch of internet sites or you know in the case of Google they've used Google Books and it is possible you know 400,000 words is a lot but it's not all words so we might you know want to see a word later that's not in there so then another item that we've loaded is word IDX and actually I should be oh and so some keyboard commands for Jupiter notebook you can type B to open up so you can switch between when you hit escape you're kind of a see there's a cursor on the cell but you're not in it and then you hit enter to kind of start typing and then you can go escape to get back out so you can check the type of word IDX and we have tab completion which is nice and then shift enter will run the cell so it's telling us word IDX is a dictionary we can look up to see is intelligence in there it is this is saying it's at spot 1226 or sorry word IDX dictionary it's in there key intelligence has value 1226 and I was getting ahead of myself and in that case what this means is that index 1226 of our words list is intelligence and so this is giving us a way to look specific words up in our list and see if they're included questions so the question was is a list of words a word vector no no so the the word vector the word vector would be we'll get to that in the moment the word vector is like the embedding yeah so the embedding for a particular word where's this list of words is yeah just letting us know which which words we have word vectors for good question yeah it's really um yes it's good to feel comfortable with what the objects will be using are any other questions all right does anyone want to suggest a word and I can look it up see if it's in here feminist good one we're at 118 53 and then just to confirm that that's accurate type that into here and it is so we've got this relationship between word index lets us go from word to indices and words lets us go from indices to words so they're kind of a inverses of each other and so far what I've given you is just equivalent to numbering words in a dictionary so we're still not able to say what words are similar to each other yes yes that's a great question so we're gonna get to that in a moment yeah and yeah now I am really curious I used intelligence but we're gonna try it on feminist yeah so that now we've got that we've got this other object backs and actually let me check the type of her backs and I'm hitting shift enter I don't know why it keeps asking me this and is a numpy nd array and that's that's just a matrix so numpy is kind of a primary way to store matrices so this is this is what intelligence looks like so yeah it's actually oh yeah you can see most of it so it's a hundred numbers and this is not not very human readable to have this array of a hundred numbers but that's how the computer is representing intelligence and then let me do it for feminist so feminist was 118 53 so this is what feminist looks like just this list of numbers which yeah on its own is not that helpful to us but it's gonna let us do calculations so to see how far apart things are we're gonna want to have a concept of distance and for vectors in high dimension you want to use something called cosine distance but this is yeah just giving us exactly like it sounds a measure of how far apart two things are so now I've checked how far apart our puppy and dog and let me say what I'm doing here so word IDX puppy that's going to return the index for puppy which is a number and then we look that number up in facts to get the hundred dimensional array that represents puppy yeah so yeah for puppy look up its index use its index to look up what is the hundred dimensional vector and that's what's happening at this the statement and then we have to do the same thing with dog find out what index is dog and then look up the hundred dimensional vector for dog and then say what are the distance between those two things and it's point two seven which is a lot smaller than these things that are not connected so like kitten and airplane I was just trying to think of words that don't seem to have any connection those are point eight seven apart those are much further apart so even though the hundred dimensional vector you know isn't really human readable to us we can look at how it compares to other hundred dimensional vectors and we get you get the Queen and Princess are just point two zero apart the words like celebrity and dusty are point nine eight since there's no connection there avalanche and antique are point nine six so these are these are words that are not close Melissa okay speak a little bit louder yes mm-hmm this is a yeah this is a great question so Melissa was saying that thinking about dimensions higher than three makes her head hurt and actually all mathematicians feel that way and so they typically will draw things as though they're in three dimensions no matter what dimension they're in let me try actually my stylist drawing and so Melissa was asking kind of like what does it look like you know do we have like baby animals to say that they're clumped together so imagining that this is just three dimensions so I'm just making a three-dimensional axes yeah the way you could think about it is that like maybe this this vector and so you can just think of vectors as as line like a line to a specific point kind of going from the origin to the specific point so we've got kitten and puppy pretty close together and what we're actually doing I was glossing over this is we're looking at the cosine distance which is the angle between them they were just saying you know we've got two lines and there's a small angle between them and then this avalanche was the other word I had avalanche might be over here change colors and the angle between kitten and avalanche is really large and so I think kind of yeah going to the baby animal approach you might find that yeah that they're kind of clustered like this if this is duckling or yeah and it's um yeah fine to think of this as just being three dimensions even though it's a hundred because it's easier to visualize is that helpful any other questions about this yes in the back oh this is a good question so the question is to get more insight into meaning could we say give me all the words that have in the first location of one you know and try to pick out then is there something that those words have in common I think it's so an issue is that it may be linear combinations of things that have significance and so it's quite possible that maybe youth is captured as being near 0.5 in the first spot and 0.1 in the second spot and 0.8 in the fifth spot and so I think that it I mean it's definitely worth looking at but I would imagine that a lot of the things that it's capturing aren't necessarily kind of just in the in a single spot but rather like a linear combination of spots that's a good question anymore okay let me go back to the notebook oh it's not the notebook and so then I want to introduce the idea of bias so here I've checked how far away is man and genius and I get point five and then I checked up check for a woman in genius and it's point six nine so those are a lot farther away so that's an area for potential concern that will be we'll be talking more about and I should say that just looking at random pairs like this is not very scientific and this their researchers that have approached this in a much more systematic way but I wanted to show you kind of like an interactive way that you can test test different pairs to see but we're kind of seeing the beginnings of you know like it was great that we could capture that kitten and cat were near each other but now this is not so great that we're getting man is a lot closer to genius than woman is and I did want to let you know like I tried some some pairs that weren't what I expected so for instance I found man was closer to emotional than woman was and some of that is it's also hard to know what distances are significant like you'll notice this is a much smaller distance than the difference we saw between genius so there are you know this isn't precise in terms of you know how how close does something need to be to be significant but I just wanted to kind of show you that's a good let me try that one actually so the only copy these so um Jupiter Salish you can copy just with a C and then V to paste and so the the comment just to repeat it was that that maybe women are assumed to be emotional and so text would be less likely to need to explicitly say that and that's kind of getting at the the fact that there's you know there's a lot of assumptions that go into into written text and oh wait I forgot to run that oh weird okay so the same dist is not defined I must have not run the cell where I import it like that and again shift enter runs a cell and you'll see the number updates here kind of to let you know the order that you're running things in okay so we do get that woman is closer to hysterical than man is so man and hysterical or point seven eight apart woman and hysterical or point six nine apart and so then I'm gonna come back in a moment and talk about kind of how how researchers who study this approach this problem in a more scientific way but I think this is a good illustration of kind of how the problem is manifesting itself so then getting back to a question oh is that a hand that's a great question so there are people that have come up with techniques for that yeah and I'll mention and link to those yeah later and then there's also people that kind of dispute about like what's the best way to handle that because it's not something that has you know like a clear cut this is the only solution so getting back to this question of can we find what words are closest to a given word I think this must be like a spotty spotty internet connection so we're gonna use an algorithm called nearest neighbors it's coming from scikit-learn and scikit-learn is kind of the main machine learning library in Python and is really well supported and nearest neighbors is exactly what it sounds like it can take something or you give it you know a group of things and then for a given item it finds which items are closest to it and so we're gonna construct the nearest neighbors classifier and specify that we're gonna want to be able to get the 10 nearest neighbors to each thing we're telling it what metric we want to use and then we run a dot fit which they should run these again to create create it and so then we can put in one of our hundred dimensional vectors so here I'm getting the one for intelligence and it's gonna give me back a list of distances and a list of indices and the indices will correspond to which words were closest and the distances will say how far away each of those are and it's important to look at the distances too because it would be worth noting if even the closest thing to a word was pretty far away so I've done this for intelligence and this came up earlier CIA was the closest word which I wasn't expecting but it makes sense and you also get information security FBI military secret counterterrorism Pentagon defense and then let's let's do this for her feminist oh actually should have left facts so we're gonna use word IDX to look up what is the index for feminist and then we'll use that index to look up in facts the vector enter that and we get feminism feminist lesbian is number six humanist modernist left-wing postmodern so these are words that have appeared near near feminist questions about this so this idea of nearest neighbors and then also I wanted to check I was planning to take a short break halfway through do you all feel yes I see thumbs up so let's take a five minute break so yeah hold on to any questions you have we'll come back to this it's 353 right now so let's meet back at 358 to continue thanks all right let's go ahead and start back up and so just during the break Twain told me about this projector dot tensor flow dot org which I wasn't familiar with but it's really neat so this is using word to back and you can choose this is I guess word to back 10k being used here you can enter a word I looked up feminist and then it lists the nearest points in the original space so here it's feminism feminist activist anarchist journalists socialist literary romantic and then you can also kind of rotate this around and there's a lot going on in here but kind of see where the different different words are oops oh and I guess this is back out to kind of all words but so this is a neat neat tool for kind of visualizing some of these well let me go back to back to our notebook actually let me try using I'm gonna stick with the notebook I was using and so we left off let's see I know I was in this one and looking at the nearest neighbors and so finding what are the ten words nearest to a given word by looking at which vectors are closest other questions about where we left off okay so to keep going um since these are vectors you know which are just these kind of listed numbers we can do things like add two words together and see what the result is so that's you know a hundred dimensional vector plus a hundred dimensional vector is gonna give us another hundred dimensional vector and so here I added artificial and intelligence and I want to note this is different than the compound compound phrase artificial intelligence this is literally adding you know this hundred dimensional vector to this other hundred dimensional vector and then I saw what oh and someone asked about this during the break and it's giving warnings if you run the code that I have if you put square brackets around new back the warning goes away and so that's just putting this hundred dimensional array inside an additional list oh one thing that I actually I meant to say this at the beginning and forgot I ran this from I was working on one computer and then this morning I tried running it from a different computer to make sure that it would work and I got an error with with open words dot text and the solution to that up here was to add it is with open words that tax comma encoding equals UTF 8 so if anyone had trouble reading in words dot text sometimes you have to tell it what the encoding is so I just wanted to highlight that although so this is being run on my local computer and I have a copy that I'm running from a server elsewhere and I didn't get the air on one of them yeah just a minor thing if you saw that saw that air so yeah down oh sure this I mean I feel like you do get a reasonable number of like encoding errors particularly if you're someone that's like switching between Python 2 and Python 3 and I actually can show so if I take this out so it was a unicode decoder when I didn't have it but putting it in resolve that and it's able to read in the file okay sorry I meant to mention that kind of back when we covered that but so yeah going back to our adding two words together we've added together artificial and intelligence and now what we're getting so let's go back to what we were getting before and I think I overwrote it and when we just had intelligence we were getting a lot of CIA FBI military connotations however in this case we're getting words like so artificial intelligence are the two closest which kind of makes sense because those are the words we we combine to create this we're getting information knowledge secret human biological using scientific communication and I just thought that was interesting because it's kind of a very different connotations than what we got for just intelligence and again this is kind of creating this new vector by adding two vectors together and then seeing what are the 10 10 vectors closest to that does anyone have suggestions for words they want to try adding together okay we'll go on so what you can do so we're gonna next we're gonna look at King and the words and again to get out of this air you could our warning you could just put an extra extra set of square brackets around it words closest to King our King Prince Queen son brother monarch throne kingdom father Emperor which is all very reasonable now we can look at King minus he plus she so this is the idea that we're kind of taking away this key vector which is perhaps representing something masculine and adding a she vector to see what we get and and you'll notice we were getting Queen the first time as a word that's that's related to King but now we are getting Queen princess daughter Elizabeth mother sister so we are definitely getting more feminine more feminine words and that that seems reasonable on its own if we look at things like programmer so I'm checking that programmers in there the words closest to programmer our programmer animator software computer technician engineer user translator linguist and now we can try doing programmer minus he plus she so what is what is that going to give us and this is kind of interesting we get some words that aren't don't seem that related to programmer to me stylist we still have animator programmer choreographer technician designer prodigy screenwriter and so this is trying to capture kind of what's this you know according to this word embedding a more feminine version of programmer if we do the reverse minus she plus he we get words programmer engineer compiler software animator computer mechanic setup developer that seem a lot closer to programmer than the words that we got for the more feminine version and so this is kind of an example of we'll see in a moment people talk about doing these analogies with word to that and this is similar what to what they're doing but this idea of kind of looking at how you know kind of what is the difference between he and she and what does it mean to apply apply that to these different professions and this is an example of where you find a lot of bias and word to back or glove in both of them questions about this idea okay and I did I did a few more examples I did doctor so this is just doctor kind of the word by itself we got doctor physician nurse doctor like dr. period doctors patient medical surgeon hospital psychiatrist a more feminine version is nurse mother woman pregnant girl patient she child herself so a lot less kind of doctor connotations the more masculine version is doctor physician medical he doctor doctors plural dr. period surgeon him hospital himself so again kind of seeing seeing this bias in how the masculine version versus the feminine version what words are closest to it all right and so I think I'm gonna switch back to my slides now I have some more more slides about this so first I just wanted to kind of highlight that word analogies are useful like in general being able to say the fact that we're to that can glove let us talk about these relationships like walking is to walk to swimming is to swim Spain is to Madrid as Italy is to Rome and a lot of people have kind of looked at these analogies and are like where to back has really captured something that we all know about language and that's that's a good thing for the most part so so I mentioned that kind of I you know I had done these like pairwise comparisons of just looking you know is me I'm closer to genius or as women that's not super scientific and so there's a really great paper and this is that the title and authors these are researchers at Princeton and the University of Bath and what they did for their methodology is they looked at what they called baskets of words but taking a collection of you know like 20 words and comparing how those baskets of words how far apart they were from each other and that's this is a lot less noisy because now you've got a lot of words that you're comparing and they used and this is coming from concept and linguistics and they didn't even they were not the ones to define these baskets they kind of used existing existing concepts so they had one of flowers and that included clover poppy marigold iris orchid rose lilac tulip it was actually a lot larger than that but this is some of them you get the idea and then they had insects like locust spider bedbug maggot fly bee cockroach mosquito and then they had pleasant words like health love peace cheer friend and unpleasant words like abuse filth murder death grief hatred pollute ugly and they checked our flowers our flowers are insects kind of which is closer to the pleasant words and which is closer to the unpleasant words and not surprisingly they found that flowers are much closer to pleasant words than insects are and the insects are a lot closer to unpleasant words and this kind of fits with them I think our natural biases most humans like flowers and don't like locusts or bed bugs or maggots and so this is this is an example of bias that is not a bad thing this is something that is yeah it's natural however they next looked at they took a group of European American names and a group of African American names and they found that the European American names were much closer to the pleasant words and the African American names were closer to the unpleasant words and so that's something that is a huge problem that the embeddings have kind of captured this association and that's something that's been seen in a lot of studies on bias that you know taking a resume and changing the name to an African American name changes how people respond to the resume and they did a lot of different pairings they also took like men's names and women's names and they saw that the men's names were a lot closer to all these like scientific and mathematical terms than the women's names were so they I commend checking out the paper because it's really interesting and they kind of go through a bunch of the different groups that they checked and they checked some that are you know neutral things like musical instruments are more pleasant than weapons but then they also kind of checked these you know dangerous human biases around race or gender so so that's one paper another one is quantifying reducing stereotypes and word embeddings and this was written about an MIT Tech Review and I recommend this article I think it's really accessible how vector space mathematics reveals the hidden sexism and language and so this was the article where they found the analogies that father is to doctor his mother is to nurse or father or man is to computer programmer as woman is to homemaker and so these are kind of these biased analogies and they also they propose a solution in here too a way and this is something that people are asking about but they propose a method for de-biasing the embeddings any questions yes yes so yeah so the question was AI is reflecting reality since reality is discriminatory isn't AI going to be discriminatory and I see Twain has her hand up although I have something to say about this as well but yeah go ahead Twain well I'll go and I'm going to get into this in the next slide but there's actually so this is kind of disagreed upon but the authors of the callus calluskin Islam Bryson and neranyan for the for this paper they recommend they actually think that the correction of bias should happen at the time of action not perception so the idea that yeah like the word is biased and we perceive that and computers should be able to perceive that but that we need to be careful with you know when these algorithms are used to make decisions of not wanting to produce biased decisions and so they say that they think that de-biasing vectors kind of at the vector level that your bias is going to seep in other places which I think is what you might be suggesting although this paper and then there's a blog post that's really well done and I'm going to recommend do you have these kind of strategies for de-biasing vectors and I would I would say no matter what you do I don't think you can ever say like oh I'm done with bias you know like I've devised my vectors I don't have to worry and I think that when working with algorithms or AI you always want to be on the lookout for bias Twain? I'm going to put a slide on that which I'm happy to share as well So one of the underlying assumptions with most of the algorithms is probabilistic bias and you may find that probabilistic bias is actually about frequency of event distribution of events and events like this which goes actually in measure of perceptual biases So it could be that some magnifications will have to get reasonably in measure of the actual ratio of human biases as ever involved probabilistic bias in some events Okay and we're just going to rephrase restate that for the recording but Twain was recommending that everyone Google the definitions of probabilistic bias of legal bias and of economic bias to look at those And I'm going to talk more in a moment about how to deal with bias first I wanted to talk about kind of the impact of it So this is an example from Google Translate and it's going from Turkish which has a gender neutral singular pronoun So it would be like saying singular they are a doctor they are a nurse and Google Translate translates that to he is a doctor she is a nurse So even though there was no gender assigned in Turkish Google Translate is assuming the doctor must be masculine and the woman must be feminine I was also kind of surprised because this example has been pointed out a lot but I like went and checked and Google Translate still does this So that's an example and now there's a great blog post and I'll put the Twain sent to me on concept net and I have the title of it in the next slide But the author talks about a system for restaurant reviews that used word embeddings and it ranked Mexican restaurants lower because the word embeddings had learned that Mexican was close to the word illegal and so that's a huge problem and was affecting these ratings and so this is a way that yeah the bias can kind of seep into these systems And the author Rob Spear of this post he went through I think and did correlations for he kind of listed many different ethnicities and then many different adjectives To kind of see how some are much closer to different adjectives than others and so that's something that's really dangerous but present with these word embeddings And then something else is that it's been shown that word embeddings can improve web search results and so and to my knowledge I don't know that any search engines are using this yet But what if searching for something like grad student neural networks was more likely to return male names if that you know if men's names are seen as closer to math and science Which it was shown in the calif um califskine Islam paper so these are kind of just showing that this can definitely have a real world impact on systems to have have these biases So this is the blog post I mentioned by Rob Spear it's called concept net number batch better less stereotyped word vectors and so what he's done is develop or released a set of debiased word vectors that are available to be downloaded Um yeah so I kind of mentioned this earlier but there are I think two main schools of thoughts on potential ways to address this one is to debias your embeddings and so this would probably be the quickest would be to download what Rob has done There's also in the paper second paper I mentioned by Balak bossy they have a they outline a technique that you can use to debias word embeddings And then the caliskan Islam paper says awareness is better than blindness debiasing should happen at time of action not at perception Kind of with the fear that if you debias the embeddings bias is going to seep into your system in other ways and I thought that was a really interesting argument and I think that their point that even even if you use the biased word embeddings continue to look out for bias and don't think that you're home free for it Any questions about this Yes. And so debiasing would basically just be changing the vectors so it's Yeah, so you're just having Yeah, you need a way to kind of like alter all the numbers. Yeah, so that now doctor is not closer to man than to woman. Yeah, I mean they're kind of a marking like basically they in the Balak bossy paper kind of specifically looks at professions as part of it, but they kind of come up with this mathematical theorem but they've grouped words of kind of like okay these are professions Yeah, that should not be closer to to one than the other. And actually I don't know that much about Rob Spears methodology because I know that he does, or is attempting to take race and ethnicity into account with the debiasing as well. Yes. It's a mix. Yeah, so the Balak bossy paper is based on, I believe, check, but I believe the Balak bossy is based on word to that and the caliskan Islam paper is using love. Oh, sorry, I'm trying to repeat the questions I forgot but the question was does does this get updated. These are still, I would say like relatively new enough that I don't know how much they've been updated or need to be updated. And something also that Twain pointed out during the break is that love. She said was trained on kind of like the largest corpus possible of everything going back to Gutenberg. And so to keep in mind that a lot of you know a lot of that text is not changing kind of this really older historical text. Yeah, there are questions about this. Okay. Transition. Was it right so I wanted to talk some about bias and image software. And so this example is from face app. And this was recently in the news just like in April, but it's an app that uses neural networks and applies filters, and it released a filter called hotness kind of to make you feel hotter. And what it did was make people's bleach people's skin and gave them more European style noses. And so this received a lot of negative publicity and it was pulled up, pulled off the app store. But this, this is actually I think even more noteworthy. We noticed that this has actually been happening for years. So this is an icon camera asking a woman if she is blinking she's not blinking an Asian woman. This is Google Photos in 2015, when they released their classification label to black people as gorillas. So really, really offensive it's also just, I think like shocking that Google released this to the public. This is I highly recommend Joy's Ted talk. It's really good she's am doing her PhD at MIT Media Lab and as a AI researcher, and she talks about she uses a lot of facial recognition software that won't recognize her own face and so kind of as a computer science researcher she has to wear a white mask to get some of these facial recognition programs to work for her. And she's done a really great Ted talk on that. This beauty AI was going to be the first international beauty contest judged by AI. This was in 2016, and it judged that white contestants were more beautiful than people of other races. So this is an example of a man being told that he can't upload his photo as a profile photo because his eyes are closed and his eyes are not closed. And so, and I was actually originally just going to show like one or two of these examples, but then I thought it was significant that there's so many, and that so many of these were very. Publicity is mistakes, and that they continue happening, I think is really notable that people kind of keep making this mistake. I know that this is, this is around image processing, but this is yeah an example of bias that's not not being addressed well, and then I even I tweeted about this and people sent me additional examples that aren't here so there are even more. I wanted to bring this up. And yeah, the sheer number of them. I think it's really telling. Oh yes. Oh no no I just it just seems like such an oversight to. Yeah, I mean they clearly yeah they clearly were just testing on white people I would guess that was mostly white people that built this. I just was so like when I tweeted this some people are like oh but it's hard to you know like get different image sets or something where it's like their Google like they have access to anything. But yeah, no I don't think it was intentional but it's just like a really huge, huge air to make. But yeah and so, even more so than the rest of tech. People working in AI are kind of overwhelmingly white and overwhelmingly male. It's not a not a diverse crowd building these. And then even more tragic example. So there is software used in US courtrooms to predict recidivism rates which then can be a factor in judges decisions about parole. So the US Department of Justice did an investigation in 2016 and found that the air rates were very different for black and white defendants. And so that it was more likely to incorrectly predict that a black defendant was at high risk of reoffending even when they weren't. And vice versa for a white defendant and this is something that was being used and I mentioned not sure on the status of. I think it may still be used in courtrooms. But I think it's a really good example of how to really say this is having like a very material impact on people's lives. And then Abe Gong is a data scientist who gave a nice talk on ethical algorithms and he kind of does a case study on this and he really dug into something that's really difficult with these cases is that you know it's a private company that makes the software and so they kind of can't reveal their technology. So as a you know as the public we don't have much transparency into what they're doing. But Abe shared some of the survey questions they use and he was able to dig up and it includes things like you know if your parents separated how old were you. And that's just horrifying that that would be a factor in how you know whether someone gets parole or not to kind of explicitly make these things that someone has no control over. Yes. Yeah. Oh wow. OK. Yeah. Yeah. Yes. Yeah. Just to repeat that for the recording the these questions would mostly be illegal for a judge to ask as part of someone sentencing but they're kind of yes sneaking in through being part of this computer software that's used. And that yeah this kind of raises a lot of legal issues and that there's often a gulf between kind of like the legal profession and the AI knowledge needed. Uh huh. Yeah. Yeah. So the question is about kind of who who's making these product decisions on the government side to purchase the software which is proprietary and not definitely not transparent but not even being audited or kind of having oversight. And this is definitely an example of kind of yes. Yeah. Yeah. I mean I think this is. Yeah I want to not get too off on a tangent but yeah I mean this is definitely like an area for a huge concern. And it gets even more concerning. Taser bought two AI companies earlier this year and they've actually rebranded themselves as acts on AI. So Taser you know that make the electric stun guns. And Taser also makes 80 percent or they have 80 percent of the police body cam market and they have very close relationships with many police departments. And so they are I just kind of wanted to flag this as something that I think is really scary. They have no transparency because again they're protecting their software and also their databases of the body cam footage as proprietary. And so they're not subject to the same state public record laws that that we think of or that you know that the government is since they're a private company. And I just I don't see how there you know there's no way they could have a data set that's not racially biased and then combining that with kind of what I mentioned earlier with this like repeated failure of computer vision on people of color. That yeah I don't have a solution of this but this is a scary and something I think more more pressure needs to be applied about addressing this. Yes. Yeah that's interesting. So the question is could we use some of this kind of whitewashing to try to remove indicators of race from this footage and would that help to bias it. I mean that's a really interesting question. I would imagine that it's very hard to completely remove race from like using it but yeah that is an interesting thought and it does kind of raise. I guess I'll get to this later but this kind of question like I definitely think there's in general the possibility with AI of us having an opportunity to try to address some of our biases and to create systems that are less biased than we are as humans and so that there's promising potential there. Other questions. OK so these are I mean these were some really dark examples and I do sometimes kind of get questions from people like almost like how can we stop AI. So I just wanted to remind you of kind of the examples I talked about earlier with some fast AI students who are working on Parkinson's disease and addressing illegal deforestation or helping helping farmers qualify for better loans creating more more resources for under resourced languages and also to bring up the medical example. So this example this slide is from my partner Jeremy Howard but there's a huge shortage of doctors in much of the world. This slide is from Nigeria. Nigeria would need over 700000 additional doctors by 2030 and it would take 300 years to train that many people. And this is true in many countries kind of similar numbers. So this is an area I think it's really important when thinking about deep learning or AI to try to balance that there's kind of all these really positive opportunities as well as all these really scary threats and I feel like I hear from a lot of people that are kind of either at one extreme of being these techno utopians. We don't have to worry tech will solve everything or people that even admit like I'm a Luddite I want to stop AI and so to really try to hold on to kind of both that we have like a lot of a lot of opportunity as well as a lot of scary risk. And so this is this is from Jeremy started startup called in Liddick a few years ago that was the first startup to apply deep learning to medicine and they specifically were working with radiology data and they this data is from 2015 so they have improved in the two years since then. They were this is for identifying lung cancer and they were more accurate than human radiologist so they had. They had fewer false negatives and fewer false positives. And this is like a panel of highly qualified radiologists and so that's also something that can be concerning to hear about even in places where there are lots of doctors the error rates are often higher than you would be comfortable knowing about. And so this is an area where deep learning can really help. Yeah and then here I wanted to emphasize and I think we've kind of already talked about this and I'm running out of time that humans are really biased as well and so when we think about you know this is why we create bias technology, but the alternative to not using tech at all is also that you know like we have human. Human biases and so it would be awesome if we could create tech that was that removed bias and so this is about rate race and medicine just some really sad statistics about kind of the worst worst quality of care, particularly that black and Latino patients receive. This is from an article in Scientific American. So the Kaliskin Islam paper that I've been referencing had a quote saying one advantage of AI at least where the algorithms and outcomes are open to inspection is that it can at least make such errors explicit and therefore subject to monitoring and correction. And so there's you know this big if you know where they're open and that you know some of these algorithms we've been talking about are not open, but there is this possibility that like when you do have open algorithms. We can correct biases in a way that human doctors have not been able to kind of correct their their biases. And so I put together some questions that I think are really helpful to ask when evaluating AI to try to get at this issue of bias. One is what bias may be in the data and this is a great quote from Angela Bassa who's the director of data science for iRobot and she said it's not that data can be biased and data is biased. If you want to use data you need to understand how it was generated. How diverse is the team that built it. I think that that is a huge problem like I think that teams need to be representative of the people that will be impacted by their technology and right now that's not the case. Can the code and data be audited or the open source and I think this and Cathy O'Neill argues for this in her book weapons of mass destruction and she gives a lot of examples of algorithms or software that are being used in hiring or firing decisions. Yeah kind of these these big decisions also yeah with the prison sentences and when those are not not open to being audited at least we don't have kind of like public oversight to what's happening. What are error rates for different subgroups. So again with the recidivism software. If you just looked at the overall error rate that wouldn't capture how different it was for white and black defendants. What is the accuracy of a simple rule based alternative. That's really helpful to just kind of get a baseline of what a reasonable accuracy is to know even you know is your is your software kind of performing better than what can be done with something much simpler. And then what processes are in place to handle appeals or mistakes and I think this is really important for things that are impacting people's lives. So Cathy O'Neill gives an example of software that was used to fire teachers and no reason would be given and so she talks with teachers that were the students love them the parents love them the principal love them and they get fired and then they're not given a reason it's just like the algorithm said so. So these are good questions to ask about AI. I should also I should have been distinguishing as I went. I tried to point out when I was giving examples of things that use deep learning some of the examples I gave other algorithms and are not necessarily deep learning. For instance the recidivism software I don't think is deep learning but I wanted to talk about them because they raise these important ethical issues and increasingly deep learning is being used for more and more things and so I think deep learning will be applied to these problems in the future. Even in cases where other algorithms are being used now. Any questions about these or additional questions you think would be good to ask. Yes. Oh yes. Okay so the question was about how referring back to it so it was Sarah Hooker with Delta Analytics and Rainforest Connection that are capturing sound from the jungle. How is that sound data represented and so that is kind of not not represented with word embeddings that's with the frequencies and I actually think that for a while they were treating those as pictures. So you can you know represent sound frequencies as numbers or you can just kind of have the picture and I believe that that's what they were using and so then kind of applying these convolutional neural networks which are what are used for a lot of image problems to basically the images of the sound waves. So I mean so it's it's relation. The question was is it's still relational. So deep learning and I won't have time to really get into this today but deep learning is learning patterns for the most part so you kind of have all these parameters that are learning patterns and so it is recognizing patterns and you can use that to get similarity and like this is something I would like to see more of but that would be possible to say like what. What other sounds you know sound you've heard in the past are most similar to the sound and so you can get that back. Although that's not not necessarily going to be explicit there you know like that's a kind of question you have to ask it or you know program it to do but it's possible. Good question. Any other questions. Okay. So this is a quote from a Linda Gates that I really agree with. If we don't get women and people of color at the table real technologists doing the real work we will buy a systems trying to reverse that a decade or two from now will be so much more difficult if not close to impossible. And I think we're at a really in a lot of ways a very exciting point in history but so much technology is being built right now and will be you know for the next next five years that it's really really important I think to to see more diversity in the field of AI as soon as possible because it will be so much harder. If we do encode kind of all our existing societal biases into this technology that's going to be much harder to undo in the future than to try to address it now as we're building the technology. So yeah I just wanted to encourage you that if you find if you find this interesting that you consider. Yeah consider learning more and getting more into the field of deep learning because the field really needs more women more people of color. It needs more people who care about ethics and bias and inclusion and social good and are asking these questions and just people with a wide variety of backgrounds. Yes. Yeah. Yes. But yeah so the question was about the compass software which is the recidivism software saying that a lot of writing about this has been like oh the algorithms bad and that the large part of the issue is these questions that were being asked have so much bias and someone else pointed out earlier things that aren't even legal for a judge to ask. Yeah and I think the bias plays a huge role like I would say that I think to have an algorithm that was making unbiased decisions I think would probably be an improvement over we know that judges are really biased and the decisions they make and so if you had something that could make decisions that were better than the decisions human judges make that that would be a good thing. But yeah the problem is that this is not. It's not a question. And then I wanted to share a quote from Francois Chalet who's someone I really admire. He created the library Keras which is a library for neural networks. I think it's the most accessible library out there. So if you're interested in deep learning I recommend you start with Keras spelled K E R A S. He's a researcher at Google Brain and he tweeted recently that he used to think that machine learning people didn't know enough math that that was what was limiting the field. Now I think the field needs more humanities. I thought that was really encouraging. He's kind of getting at this issue of people that are thinking about these ethical questions and knowing kind of what are what are the right problems to be even solving and working on. And so I just wanted to encourage you that no matter what your background that I think you could add a lot to the field of A.I. Sorry I saw someone was taking a photo. Got it. And then I'm over time. So let me check with Twain. I can keep going. OK. Yeah. So cut me off if I start. Yeah. I'm getting into the next speaker spot or anything. I wasn't I was not sure how long this would take at all. So I guess I've prepared too much material. But I wanted I wanted to give you some tips for getting started with deep learning. So so far in the first demo we were just seeing the word embeddings which are a really useful tool and are used in a lot of algorithms. So to kind of re re state this I talked about it a little bit at the beginning. So Jeremy Howard and I are fast day I our slogan is making neural nets and uncool again. And the idea is that being cool is about being exclusive and we wanted to build something that was more inclusive that you didn't have to be kind of have a Stanford Ph.D. or this elite background. But the outsiders should be joining A.I. And we partnered with the University of San Francisco's Data Institute offered this course in the evenings was open to the community. Most people taking the course were software engineers and a lot of the deep learning materials out there kind of assume a graduate level math background. We wanted something that had no math background and just we required one or two years of coding experience and it's available for free at course stop fast. I. So these are just kind of a few I want to share a little bit about our approach and lessons we learned that I think will be helpful. So some problems we've seen with other technical teaching one is that it's often math centric versus code centric and I'm someone that loves math. I have a math Ph.D. when I first got interested in deep learning I found it frustrating because I wanted to be able to code and even though I can understand equations I was like this isn't showing me how to implement it. Or the kind of the practical information. I went to a meetup. This is in 2013 where kind of a star in the field was presenting and I asked a practical question and he was like oh that's part of a dirty bag of tricks nobody publishes. You know it's kind of like we're only sharing the theory but not what you need to do this. And so we kind of we wanted something that was very code first and code focused. Then there's this term element Titus and David Perkins came up with this Perkins is a professor at Harvard. He actually has his Ph.D. in A.I. and became an education professor. And he uses an analogy with baseball and says you know we don't require children to learn kind of all the formal rules of baseball before they're allowed to play. You know they're able to play a simplified version of the game and get a sense of it and have fun. And then as they get older they may learn more and more rules and play you know more innings and get closer to kind of the real game. But then you should be able to get into a field and kind of have fun and get a sense for the big picture even without knowing all the details. And element Titus is the opposite of that and that's kind of this idea of like you have to learn each individual component. You know and then eventually years from now you can like build those components together to do something interesting. And so with that with this course we kind of start with things that are black boxes just to kind of get people going. And then over time we dig in and kind of peel back the layers and it's always motivated by wanting to improve our performance or needing to solve a particular bug. So in the end you are learning about the low level details. It's just in the opposite order because we want you kind of able to solve problems right away and then we'll get into the details as time goes on. And then of the more practical or code based resources out there many of them kind of just settle with for good enough results or kind of are on these toy problems. And we still want it to be getting to state of the art results and getting stuff that you can actually use at your company or in production as opposed to just kind of these simplified toy examples which can get frustrating. So this was this was our approach to fast AI. Well the ones on the bottom kind of addressing addressing these problems. My recommendation is to start with Kairos which is a high level neural network API in Python. I read a blog post in January. So Google has spent a lot of money advertising and promoting TensorFlow which is somewhat unfortunate because I don't think it's a good place to start. And so I wrote TensorFlow makes me feel like I'm not smart enough to use TensorFlow whereas Kairos makes me feel like neural networks are easier than expected. This actually got a lot of attention. People on the TensorFlow team read it and they invited me to the TensorFlow Dev Summit which was very nice. And here someone had responded to my post saying TensorFlow is like being handed an injection molding machine and being told to make a toy. Kairos is like being handed Legos and I think that captures it really well. Kairos is very kind of very modular and gives you these pieces you can use quickly. And I should say Kairos is being incorporated into TensorFlow. So there may be less of a division but definitely if you're looking for tutorials I would recommend looking for Kairos stuff. So kind of some tips. I think it's really good to have a practical coding project that leads the way if you have a problem that you're interested in solving. And also to start on that right away and not feel like you have to spend a year studying theory and then you can do the thing you're interested in. Having a project that you care about is going to motivate you and it's going to drive your learning. And so it's a kind of let that drive like learn what you need to do the project you care about. Get something working as fast as possible. It's okay if you don't understand all the details because I think you can get really bogged down in trying to learn all these low level details and then you're not actually getting this working. And there's time to learn the details later. That doesn't mean that you're not going to learn them. And then I really recommend helping others answering questions and writing about what you're learning. And I'm recommending that because I think it's a crucial part of your own learning. So this is even though I think altruism is important. That's not why I'm saying this. It's because the test of whether you know something is whether you can build with it and whether you can explain it to somebody else. And so it's really important to do that. Also with the writing about what you're learning. I really encourage people to blog and it's something where you don't you don't have to be an expert to blog. The idea is that your audience is the person that's one or two steps behind you and your best position to help that person because often experts have forgotten what was hard about getting into the field. Whereas you know it may still be fresh in your memory. You know what would have been helpful to you to read a month ago if only you had known then. So that should kind of be your target audience. And then the things to avoid. Don't just read or watch tutorials without taking time to code. You kind of really learn by doing. Similarly don't kind of just execute other people's code. Shift enter is what executes a cell in a Jupyter notebook. It's really valuable to be typing and to be modifying the inputs and seeing how that affects the outputs and to try little variations and see how that impacts the code that you learn. By doing. And then yeah don't feel like you have to understand all the components first because there's time to learn them later and I think particularly something like this where there are just so many components. It's really easy to get bogged down and not not get anywhere. Any questions about this. Yes. So this course will be offered again starting in late October I believe. Yes. So keep an eye out for it. We also we offer diversity fellowships and I will tweet about that and we'll post it on our website fastai. And we will be offered and it's one evening a week. I see a hand over here. So the question was about productionizing these models and having concerns that your company that this is slow to train or hard to production eyes. And this is something I haven't developed and that Jeremy and I want to develop a kind of tips on productionizing. I want to say about the point about requiring a lot of data to train or being slow to train that that's I think a very common misperception about deep learning. And there's something called transfer learning which is covered in the court is kind of a main feature of the course. But it's the idea that you can use models that were trained on other data sets and apply them to your data set. And that allows people that don't have that much data and don't have the computational power to kind of benefit from what companies like Google have done when they create models that are shared. And they often get I think surprisingly good results. So you would have something maybe that was originally trained on image net which is this classic problem of identifying all these different categories. I don't know. That's a coffee table. That's a snowmobile. And you can apply that to another computer vision problem. And even if it's something fairly different than you just like retrain like the last layer the last few layers which is quicker and you don't have to have as much data and you can get good performance. So that's a possibility. But yeah productionizing is something that I expect us to do more on in the future. Other questions. Yes. Oh really. Oh it's not really. Oh crazy. OK. But so has she publicly revealed that it was a joke because this gets like cited all over the place. OK. Interesting. I would just for me to repeat for the recording someone is saying that her friend was the woman in the Nikon camera picture and that photo was faked. I do want to point out that there was another photo of an Asian man with a different software being told that his eyes were closed. So I think this issue has arisen elsewhere. Yeah that's fascinating. I'll search for that. Well yeah and then I've kind of already covered this about yeah the test of whether you understand something is if you can code and build it if you can teach it to somebody else. These were blog posts written by two students in our course. You know what is a beginner's guide to commonly use linear algebra operations and so again remembering you can you can write blog post with people a step or two behind you as the target audience. Yes. Yeah. So the question was what sort of roles can be played by people that are not not technical in a particularly someone that has a background as a lawyer and working in policy or that you said low income. Yeah I think. Yeah I mean actually when I was preparing this talk I was like I hope their lawyers at this talk. And then I was like oh there probably won't be. But just like some of the legal issues that were raised I feel like are really really important. I would I mean I would encourage you to to learn some coding even if I think even if you're still kind of working in the capacity as a lawyer like a lawyer who codes and know some about a I just feel like it's going to have a lot of potential to do things. But yeah I am excited to have a lawyer here because I think that this does raise a lot of legal issues. But yeah I would yeah I would encourage you to. Yeah. Yeah. I mean if you're if you're brand do you code or. Yeah. Okay. You might want to spend some time on Python first. So if you're kind of brand new to coding I would spend some time learning Python and then move on to move on to this. Yeah I'm excited that you're interested. Okay. And I was going to share a quote from one of our students. I personally fell into the habit of watching the lectures too much and Googling definitions concepts too much without running the code. At first I thought that I should read the code quickly and then spend time researching the theory behind it and retrospect I should have spent the majority of my time on the actual code in the notebooks running it and seeing what goes into it and what comes out of it. And I think this is really good advice learned from this person's mistake and definitely kind of focus on the code and running it. And then. So I said another entire demo plan I'm going to stop. Yeah I'm getting the stop signal I see people packing up. I'm sorry. I'm sorry that this ran long. Thank you for your patience. The demo is available in the Jupyter notebook and it has lots of text. So I encourage you to give it a try on your own even though we didn't have time for it here. It's kind of just the second part down down here and doing some movie review analysis. But yeah thank you everyone for your patience and your attention. And also feel free to email or tweet at me if you have additional questions or concerns that come up. Thanks.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.76, "text": " So I'm Rachel Thomas and I'm sorry I wasn't able to be here earlier because", "tokens": [407, 286, 478, 14246, 8500, 293, 286, 478, 2597, 286, 2067, 380, 1075, 281, 312, 510, 3071, 570], "temperature": 0.0, "avg_logprob": -0.15579807130913986, "compression_ratio": 1.4902912621359223, "no_speech_prob": 0.007811767514795065}, {"id": 1, "seek": 0, "start": 5.76, "end": 8.6, "text": " I was teaching at University of San Francisco but it sounds like it was a", "tokens": [286, 390, 4571, 412, 3535, 295, 5271, 12279, 457, 309, 3263, 411, 309, 390, 257], "temperature": 0.0, "avg_logprob": -0.15579807130913986, "compression_ratio": 1.4902912621359223, "no_speech_prob": 0.007811767514795065}, {"id": 2, "seek": 0, "start": 8.6, "end": 14.84, "text": " really exciting morning and earlier part of the day and I wanted to get a feel", "tokens": [534, 4670, 2446, 293, 3071, 644, 295, 264, 786, 293, 286, 1415, 281, 483, 257, 841], "temperature": 0.0, "avg_logprob": -0.15579807130913986, "compression_ratio": 1.4902912621359223, "no_speech_prob": 0.007811767514795065}, {"id": 3, "seek": 0, "start": 14.84, "end": 21.6, "text": " just for people's backgrounds. Raise your hand if you're an engineer. Okay how", "tokens": [445, 337, 561, 311, 17336, 13, 30062, 428, 1011, 498, 291, 434, 364, 11403, 13, 1033, 577], "temperature": 0.0, "avg_logprob": -0.15579807130913986, "compression_ratio": 1.4902912621359223, "no_speech_prob": 0.007811767514795065}, {"id": 4, "seek": 2160, "start": 21.6, "end": 33.56, "text": " about data scientists? Okay, PM, designers, cool it's a great variety and then are", "tokens": [466, 1412, 7708, 30, 1033, 11, 12499, 11, 16196, 11, 1627, 309, 311, 257, 869, 5673, 293, 550, 366], "temperature": 0.0, "avg_logprob": -0.17392199552511867, "compression_ratio": 1.4694835680751173, "no_speech_prob": 1.280268952541519e-05}, {"id": 5, "seek": 2160, "start": 33.56, "end": 39.400000000000006, "text": " there any that I missed? Feel free to shout them out and I definitely, please", "tokens": [456, 604, 300, 286, 6721, 30, 14113, 1737, 281, 8043, 552, 484, 293, 286, 2138, 11, 1767], "temperature": 0.0, "avg_logprob": -0.17392199552511867, "compression_ratio": 1.4694835680751173, "no_speech_prob": 1.280268952541519e-05}, {"id": 6, "seek": 2160, "start": 39.400000000000006, "end": 43.0, "text": " stop and ask questions and if you have them because I really want this to be", "tokens": [1590, 293, 1029, 1651, 293, 498, 291, 362, 552, 570, 286, 534, 528, 341, 281, 312], "temperature": 0.0, "avg_logprob": -0.17392199552511867, "compression_ratio": 1.4694835680751173, "no_speech_prob": 1.280268952541519e-05}, {"id": 7, "seek": 2160, "start": 43.0, "end": 50.120000000000005, "text": " helpful for you. So I wanted to start just briefly share a little bit of my", "tokens": [4961, 337, 291, 13, 407, 286, 1415, 281, 722, 445, 10515, 2073, 257, 707, 857, 295, 452], "temperature": 0.0, "avg_logprob": -0.17392199552511867, "compression_ratio": 1.4694835680751173, "no_speech_prob": 1.280268952541519e-05}, {"id": 8, "seek": 5012, "start": 50.12, "end": 55.64, "text": " backgrounds. I studied math and computer science in undergrad and then did a PhD", "tokens": [17336, 13, 286, 9454, 5221, 293, 3820, 3497, 294, 14295, 293, 550, 630, 257, 14476], "temperature": 0.0, "avg_logprob": -0.15392724594267287, "compression_ratio": 1.6806083650190113, "no_speech_prob": 6.602599023608491e-05}, {"id": 9, "seek": 5012, "start": 55.64, "end": 61.72, "text": " in mathematics. I worked as a quant for two years in energy trading which", "tokens": [294, 18666, 13, 286, 2732, 382, 257, 4426, 337, 732, 924, 294, 2281, 9529, 597], "temperature": 0.0, "avg_logprob": -0.15392724594267287, "compression_ratio": 1.6806083650190113, "no_speech_prob": 6.602599023608491e-05}, {"id": 10, "seek": 5012, "start": 61.72, "end": 64.52, "text": " involves a lot of programming and working with data and that's what", "tokens": [11626, 257, 688, 295, 9410, 293, 1364, 365, 1412, 293, 300, 311, 437], "temperature": 0.0, "avg_logprob": -0.15392724594267287, "compression_ratio": 1.6806083650190113, "no_speech_prob": 6.602599023608491e-05}, {"id": 11, "seek": 5012, "start": 64.52, "end": 69.56, "text": " interested me in becoming a data scientist. I was a data scientist and", "tokens": [3102, 385, 294, 5617, 257, 1412, 12662, 13, 286, 390, 257, 1412, 12662, 293], "temperature": 0.0, "avg_logprob": -0.15392724594267287, "compression_ratio": 1.6806083650190113, "no_speech_prob": 6.602599023608491e-05}, {"id": 12, "seek": 5012, "start": 69.56, "end": 74.68, "text": " backend engineer at uber in 2013 and 2014 and then I taught full stack", "tokens": [38087, 11403, 412, 344, 607, 294, 9012, 293, 8227, 293, 550, 286, 5928, 1577, 8630], "temperature": 0.0, "avg_logprob": -0.15392724594267287, "compression_ratio": 1.6806083650190113, "no_speech_prob": 6.602599023608491e-05}, {"id": 13, "seek": 5012, "start": 74.68, "end": 79.28, "text": " software development at hackbrite which is the all women coding academy and I", "tokens": [4722, 3250, 412, 10339, 1443, 642, 597, 307, 264, 439, 2266, 17720, 25525, 293, 286], "temperature": 0.0, "avg_logprob": -0.15392724594267287, "compression_ratio": 1.6806083650190113, "no_speech_prob": 6.602599023608491e-05}, {"id": 14, "seek": 7928, "start": 79.28, "end": 86.12, "text": " know there are several folks from there here today. Yay, I love teaching and I", "tokens": [458, 456, 366, 2940, 4024, 490, 456, 510, 965, 13, 13268, 11, 286, 959, 4571, 293, 286], "temperature": 0.0, "avg_logprob": -0.16129138905514953, "compression_ratio": 1.5950413223140496, "no_speech_prob": 9.664840035839006e-06}, {"id": 15, "seek": 7928, "start": 86.12, "end": 91.68, "text": " love working with women. One year ago I started fast AI with the goal of making", "tokens": [959, 1364, 365, 2266, 13, 1485, 1064, 2057, 286, 1409, 2370, 7318, 365, 264, 3387, 295, 1455], "temperature": 0.0, "avg_logprob": -0.16129138905514953, "compression_ratio": 1.5950413223140496, "no_speech_prob": 9.664840035839006e-06}, {"id": 16, "seek": 7928, "start": 91.68, "end": 97.56, "text": " deep learning more accessible and easier to use and we also partner with the", "tokens": [2452, 2539, 544, 9515, 293, 3571, 281, 764, 293, 321, 611, 4975, 365, 264], "temperature": 0.0, "avg_logprob": -0.16129138905514953, "compression_ratio": 1.5950413223140496, "no_speech_prob": 9.664840035839006e-06}, {"id": 17, "seek": 7928, "start": 97.56, "end": 102.12, "text": " University of San Francisco so I work there part-time. I'm on Twitter at math", "tokens": [3535, 295, 5271, 12279, 370, 286, 589, 456, 644, 12, 3766, 13, 286, 478, 322, 5794, 412, 5221], "temperature": 0.0, "avg_logprob": -0.16129138905514953, "compression_ratio": 1.5950413223140496, "no_speech_prob": 9.664840035839006e-06}, {"id": 18, "seek": 7928, "start": 102.12, "end": 108.68, "text": " Rachel. I blog about diversity and inclusion on medium at Rachel THO and", "tokens": [14246, 13, 286, 6968, 466, 8811, 293, 15874, 322, 6399, 412, 14246, 3578, 46, 293], "temperature": 0.0, "avg_logprob": -0.16129138905514953, "compression_ratio": 1.5950413223140496, "no_speech_prob": 9.664840035839006e-06}, {"id": 19, "seek": 10868, "start": 108.68, "end": 115.24000000000001, "text": " then I blog about data science at fast.ai. So first I just wanted to kind of", "tokens": [550, 286, 6968, 466, 1412, 3497, 412, 2370, 13, 1301, 13, 407, 700, 286, 445, 1415, 281, 733, 295], "temperature": 0.0, "avg_logprob": -0.18581665668290914, "compression_ratio": 1.652542372881356, "no_speech_prob": 8.397113560931757e-06}, {"id": 20, "seek": 10868, "start": 115.24000000000001, "end": 119.2, "text": " define what is deep learning. I know it often seems like there are a lot of", "tokens": [6964, 437, 307, 2452, 2539, 13, 286, 458, 309, 2049, 2544, 411, 456, 366, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.18581665668290914, "compression_ratio": 1.652542372881356, "no_speech_prob": 8.397113560931757e-06}, {"id": 21, "seek": 10868, "start": 119.2, "end": 125.64000000000001, "text": " buzzwords in the data science space but deep learning refers to a very specific", "tokens": [13036, 13832, 294, 264, 1412, 3497, 1901, 457, 2452, 2539, 14942, 281, 257, 588, 2685], "temperature": 0.0, "avg_logprob": -0.18581665668290914, "compression_ratio": 1.652542372881356, "no_speech_prob": 8.397113560931757e-06}, {"id": 22, "seek": 10868, "start": 125.64000000000001, "end": 132.20000000000002, "text": " class of algorithms, multi-layered neural networks and it's a subclass of machine", "tokens": [1508, 295, 14642, 11, 4825, 12, 8376, 4073, 18161, 9590, 293, 309, 311, 257, 1422, 11665, 295, 3479], "temperature": 0.0, "avg_logprob": -0.18581665668290914, "compression_ratio": 1.652542372881356, "no_speech_prob": 8.397113560931757e-06}, {"id": 23, "seek": 10868, "start": 132.20000000000002, "end": 137.84, "text": " learning. Is there is the feedback annoying on the microphone or it's okay?", "tokens": [2539, 13, 1119, 456, 307, 264, 5824, 11304, 322, 264, 10952, 420, 309, 311, 1392, 30], "temperature": 0.0, "avg_logprob": -0.18581665668290914, "compression_ratio": 1.652542372881356, "no_speech_prob": 8.397113560931757e-06}, {"id": 24, "seek": 13784, "start": 137.84, "end": 142.92000000000002, "text": " Tolerable. Let me see if it's like better if it's a little bit further. Can you", "tokens": [21402, 260, 712, 13, 961, 385, 536, 498, 309, 311, 411, 1101, 498, 309, 311, 257, 707, 857, 3052, 13, 1664, 291], "temperature": 0.0, "avg_logprob": -0.15032850966161612, "compression_ratio": 1.646808510638298, "no_speech_prob": 3.480337909422815e-05}, {"id": 25, "seek": 13784, "start": 142.92000000000002, "end": 149.44, "text": " still hear me? Okay cool that sounds better. Yeah so deep learning is a subset", "tokens": [920, 1568, 385, 30, 1033, 1627, 300, 3263, 1101, 13, 865, 370, 2452, 2539, 307, 257, 25993], "temperature": 0.0, "avg_logprob": -0.15032850966161612, "compression_ratio": 1.646808510638298, "no_speech_prob": 3.480337909422815e-05}, {"id": 26, "seek": 13784, "start": 149.44, "end": 154.04, "text": " of machine learning just the specific class of algorithms which is a subset of", "tokens": [295, 3479, 2539, 445, 264, 2685, 1508, 295, 14642, 597, 307, 257, 25993, 295], "temperature": 0.0, "avg_logprob": -0.15032850966161612, "compression_ratio": 1.646808510638298, "no_speech_prob": 3.480337909422815e-05}, {"id": 27, "seek": 13784, "start": 154.04, "end": 159.68, "text": " AI and AI is a very broad field. A lot of a lot of the kind of specific advances", "tokens": [7318, 293, 7318, 307, 257, 588, 4152, 2519, 13, 316, 688, 295, 257, 688, 295, 264, 733, 295, 2685, 25297], "temperature": 0.0, "avg_logprob": -0.15032850966161612, "compression_ratio": 1.646808510638298, "no_speech_prob": 3.480337909422815e-05}, {"id": 28, "seek": 15968, "start": 159.68, "end": 177.32, "text": " being made in AI right now are all coming from deep learning though. Sorry I", "tokens": [885, 1027, 294, 7318, 558, 586, 366, 439, 1348, 490, 2452, 2539, 1673, 13, 4919, 286], "temperature": 0.0, "avg_logprob": -0.15497206506274996, "compression_ratio": 1.2727272727272727, "no_speech_prob": 2.902260121118161e-06}, {"id": 29, "seek": 15968, "start": 177.32, "end": 187.04000000000002, "text": " wanted to go to full screen mode to get rid of the sidebar. Okay there we go.", "tokens": [1415, 281, 352, 281, 1577, 2568, 4391, 281, 483, 3973, 295, 264, 1252, 5356, 13, 1033, 456, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.15497206506274996, "compression_ratio": 1.2727272727272727, "no_speech_prob": 2.902260121118161e-06}, {"id": 30, "seek": 18704, "start": 187.04, "end": 191.23999999999998, "text": " So a kind of really common example of deep learning that you've probably seen", "tokens": [407, 257, 733, 295, 534, 2689, 1365, 295, 2452, 2539, 300, 291, 600, 1391, 1612], "temperature": 0.0, "avg_logprob": -0.10549297116019508, "compression_ratio": 1.6858407079646018, "no_speech_prob": 1.7228898286703043e-05}, {"id": 31, "seek": 18704, "start": 191.23999999999998, "end": 195.92, "text": " is Google Photos will automatically group your photos for you. So here it's", "tokens": [307, 3329, 13919, 329, 486, 6772, 1594, 428, 5787, 337, 291, 13, 407, 510, 309, 311], "temperature": 0.0, "avg_logprob": -0.10549297116019508, "compression_ratio": 1.6858407079646018, "no_speech_prob": 1.7228898286703043e-05}, {"id": 32, "seek": 18704, "start": 195.92, "end": 201.44, "text": " kind of group them into skylines, temples, food and deep learning is behind that", "tokens": [733, 295, 1594, 552, 666, 5443, 11045, 11, 27431, 11, 1755, 293, 2452, 2539, 307, 2261, 300], "temperature": 0.0, "avg_logprob": -0.10549297116019508, "compression_ratio": 1.6858407079646018, "no_speech_prob": 1.7228898286703043e-05}, {"id": 33, "seek": 18704, "start": 201.44, "end": 206.92, "text": " kind of automatic classification. And then another great example is Skype", "tokens": [733, 295, 12509, 21538, 13, 400, 550, 1071, 869, 1365, 307, 31743], "temperature": 0.0, "avg_logprob": -0.10549297116019508, "compression_ratio": 1.6858407079646018, "no_speech_prob": 1.7228898286703043e-05}, {"id": 34, "seek": 18704, "start": 206.92, "end": 212.0, "text": " Translator. So Skype Translator can translate in real time between eight", "tokens": [6531, 75, 1639, 13, 407, 31743, 6531, 75, 1639, 393, 13799, 294, 957, 565, 1296, 3180], "temperature": 0.0, "avg_logprob": -0.10549297116019508, "compression_ratio": 1.6858407079646018, "no_speech_prob": 1.7228898286703043e-05}, {"id": 35, "seek": 21200, "start": 212.0, "end": 218.04, "text": " different languages for voice and between 50 languages for text and written.", "tokens": [819, 8650, 337, 3177, 293, 1296, 2625, 8650, 337, 2487, 293, 3720, 13], "temperature": 0.0, "avg_logprob": -0.0897419200224035, "compression_ratio": 1.7130434782608697, "no_speech_prob": 1.1299847756163217e-05}, {"id": 36, "seek": 21200, "start": 218.04, "end": 222.84, "text": " So deep learning is particularly powerful for working with images and for working", "tokens": [407, 2452, 2539, 307, 4098, 4005, 337, 1364, 365, 5267, 293, 337, 1364], "temperature": 0.0, "avg_logprob": -0.0897419200224035, "compression_ratio": 1.7130434782608697, "no_speech_prob": 1.1299847756163217e-05}, {"id": 37, "seek": 21200, "start": 222.84, "end": 230.8, "text": " with language which are two huge areas. This is an article from Fortune last", "tokens": [365, 2856, 597, 366, 732, 2603, 3179, 13, 639, 307, 364, 7222, 490, 38508, 1036], "temperature": 0.0, "avg_logprob": -0.0897419200224035, "compression_ratio": 1.7130434782608697, "no_speech_prob": 1.1299847756163217e-05}, {"id": 38, "seek": 21200, "start": 230.8, "end": 235.28, "text": " fall about why you need to learn about deep learning. A lot of people have been", "tokens": [2100, 466, 983, 291, 643, 281, 1466, 466, 2452, 2539, 13, 316, 688, 295, 561, 362, 668], "temperature": 0.0, "avg_logprob": -0.0897419200224035, "compression_ratio": 1.7130434782608697, "no_speech_prob": 1.1299847756163217e-05}, {"id": 39, "seek": 21200, "start": 235.28, "end": 239.04, "text": " comparing deep learning to the internet in terms of the impact that it's going", "tokens": [15763, 2452, 2539, 281, 264, 4705, 294, 2115, 295, 264, 2712, 300, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.0897419200224035, "compression_ratio": 1.7130434782608697, "no_speech_prob": 1.1299847756163217e-05}, {"id": 40, "seek": 23904, "start": 239.04, "end": 244.56, "text": " to have. I'm kind of saying you know we're at just like the early 90s in terms", "tokens": [281, 362, 13, 286, 478, 733, 295, 1566, 291, 458, 321, 434, 412, 445, 411, 264, 2440, 4289, 82, 294, 2115], "temperature": 0.0, "avg_logprob": -0.10008655894886363, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.722868728393223e-05}, {"id": 41, "seek": 23904, "start": 244.56, "end": 248.84, "text": " of when the internet was still young and there's just so much potential for what", "tokens": [295, 562, 264, 4705, 390, 920, 2037, 293, 456, 311, 445, 370, 709, 3995, 337, 437], "temperature": 0.0, "avg_logprob": -0.10008655894886363, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.722868728393223e-05}, {"id": 42, "seek": 23904, "start": 248.84, "end": 254.72, "text": " can be done. Andrew Ng is a CS professor from Stanford. He's the co-founder of", "tokens": [393, 312, 1096, 13, 10110, 21198, 307, 257, 9460, 8304, 490, 20374, 13, 634, 311, 264, 598, 12, 33348, 295], "temperature": 0.0, "avg_logprob": -0.10008655894886363, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.722868728393223e-05}, {"id": 43, "seek": 23904, "start": 254.72, "end": 260.0, "text": " Coursera and he was chief scientist at Baidu for four years which is you know", "tokens": [383, 5067, 1663, 293, 415, 390, 9588, 12662, 412, 6777, 327, 84, 337, 1451, 924, 597, 307, 291, 458], "temperature": 0.0, "avg_logprob": -0.10008655894886363, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.722868728393223e-05}, {"id": 44, "seek": 23904, "start": 260.0, "end": 264.88, "text": " one of the top tech companies out there. And he said that comparing deep learning", "tokens": [472, 295, 264, 1192, 7553, 3431, 484, 456, 13, 400, 415, 848, 300, 15763, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.10008655894886363, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.722868728393223e-05}, {"id": 45, "seek": 23904, "start": 264.88, "end": 268.32, "text": " to the internet doesn't do it justice and he thinks it's more like", "tokens": [281, 264, 4705, 1177, 380, 360, 309, 6118, 293, 415, 7309, 309, 311, 544, 411], "temperature": 0.0, "avg_logprob": -0.10008655894886363, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.722868728393223e-05}, {"id": 46, "seek": 26832, "start": 268.32, "end": 273.32, "text": " electricity in terms of the impact it'll have in transforming industry after", "tokens": [10356, 294, 2115, 295, 264, 2712, 309, 603, 362, 294, 27210, 3518, 934], "temperature": 0.0, "avg_logprob": -0.1684203619485373, "compression_ratio": 1.559670781893004, "no_speech_prob": 2.1441332137328573e-05}, {"id": 47, "seek": 26832, "start": 273.32, "end": 281.32, "text": " industry. I wanted to, so I kind of showed you some very mainstream examples with", "tokens": [3518, 13, 286, 1415, 281, 11, 370, 286, 733, 295, 4712, 291, 512, 588, 15960, 5110, 365], "temperature": 0.0, "avg_logprob": -0.1684203619485373, "compression_ratio": 1.559670781893004, "no_speech_prob": 2.1441332137328573e-05}, {"id": 48, "seek": 26832, "start": 281.32, "end": 284.84, "text": " Google Photos and Skype of where deep learning is being used and I wanted to", "tokens": [3329, 13919, 329, 293, 31743, 295, 689, 2452, 2539, 307, 885, 1143, 293, 286, 1415, 281], "temperature": 0.0, "avg_logprob": -0.1684203619485373, "compression_ratio": 1.559670781893004, "no_speech_prob": 2.1441332137328573e-05}, {"id": 49, "seek": 26832, "start": 284.84, "end": 289.08, "text": " share some less common ones and these all come from students in a", "tokens": [2073, 512, 1570, 2689, 2306, 293, 613, 439, 808, 490, 1731, 294, 257], "temperature": 0.0, "avg_logprob": -0.1684203619485373, "compression_ratio": 1.559670781893004, "no_speech_prob": 2.1441332137328573e-05}, {"id": 50, "seek": 26832, "start": 289.08, "end": 294.76, "text": " course that I helped create with Fast AI. And so one of them is Xinxin Li and", "tokens": [1164, 300, 286, 4254, 1884, 365, 15968, 7318, 13, 400, 370, 472, 295, 552, 307, 24368, 87, 259, 8349, 293], "temperature": 0.0, "avg_logprob": -0.1684203619485373, "compression_ratio": 1.559670781893004, "no_speech_prob": 2.1441332137328573e-05}, {"id": 51, "seek": 29476, "start": 294.76, "end": 300.76, "text": " she is working on wearable devices to monitor Parkinson's disease which is a", "tokens": [750, 307, 1364, 322, 3728, 712, 5759, 281, 6002, 35823, 311, 4752, 597, 307, 257], "temperature": 0.0, "avg_logprob": -0.1371039686531856, "compression_ratio": 1.6336206896551724, "no_speech_prob": 1.112389327317942e-05}, {"id": 52, "seek": 29476, "start": 300.76, "end": 306.28, "text": " neurodegenerative disorder. And so typically doctors will kind of watch a", "tokens": [16499, 67, 1146, 7971, 1166, 13399, 13, 400, 370, 5850, 8778, 486, 733, 295, 1159, 257], "temperature": 0.0, "avg_logprob": -0.1371039686531856, "compression_ratio": 1.6336206896551724, "no_speech_prob": 1.112389327317942e-05}, {"id": 53, "seek": 29476, "start": 306.28, "end": 311.4, "text": " patient walk to see how the disease is progressing and just do this like visual", "tokens": [4537, 1792, 281, 536, 577, 264, 4752, 307, 36305, 293, 445, 360, 341, 411, 5056], "temperature": 0.0, "avg_logprob": -0.1371039686531856, "compression_ratio": 1.6336206896551724, "no_speech_prob": 1.112389327317942e-05}, {"id": 54, "seek": 29476, "start": 311.4, "end": 316.28, "text": " assessment. But she's a kind of consultant with Akashi which is working", "tokens": [9687, 13, 583, 750, 311, 257, 733, 295, 24676, 365, 9629, 15612, 597, 307, 1364], "temperature": 0.0, "avg_logprob": -0.1371039686531856, "compression_ratio": 1.6336206896551724, "no_speech_prob": 1.112389327317942e-05}, {"id": 55, "seek": 29476, "start": 316.28, "end": 320.59999999999997, "text": " with Exceed which makes the wearables and they're developing algorithms that", "tokens": [365, 2111, 4357, 597, 1669, 264, 3728, 2965, 293, 436, 434, 6416, 14642, 300], "temperature": 0.0, "avg_logprob": -0.1371039686531856, "compression_ratio": 1.6336206896551724, "no_speech_prob": 1.112389327317942e-05}, {"id": 56, "seek": 32060, "start": 320.6, "end": 325.72, "text": " this could be tracking the patient's movement, gathering a lot more data you", "tokens": [341, 727, 312, 11603, 264, 4537, 311, 3963, 11, 13519, 257, 688, 544, 1412, 291], "temperature": 0.0, "avg_logprob": -0.10168223705106569, "compression_ratio": 1.7148148148148148, "no_speech_prob": 7.071104846545495e-06}, {"id": 57, "seek": 32060, "start": 325.72, "end": 328.48, "text": " know than just observing someone while they're in the doctor's office and so", "tokens": [458, 813, 445, 22107, 1580, 1339, 436, 434, 294, 264, 4631, 311, 3398, 293, 370], "temperature": 0.0, "avg_logprob": -0.10168223705106569, "compression_ratio": 1.7148148148148148, "no_speech_prob": 7.071104846545495e-06}, {"id": 58, "seek": 32060, "start": 328.48, "end": 333.40000000000003, "text": " this could be a lot more accurate. Another one of our students is Sarah", "tokens": [341, 727, 312, 257, 688, 544, 8559, 13, 3996, 472, 295, 527, 1731, 307, 9519], "temperature": 0.0, "avg_logprob": -0.10168223705106569, "compression_ratio": 1.7148148148148148, "no_speech_prob": 7.071104846545495e-06}, {"id": 59, "seek": 32060, "start": 333.40000000000003, "end": 340.16, "text": " Hooker. She's founder of Delta Analytics which partners teams of data scientists", "tokens": [33132, 260, 13, 1240, 311, 14917, 295, 18183, 25944, 597, 4462, 5491, 295, 1412, 7708], "temperature": 0.0, "avg_logprob": -0.10168223705106569, "compression_ratio": 1.7148148148148148, "no_speech_prob": 7.071104846545495e-06}, {"id": 60, "seek": 32060, "start": 340.16, "end": 343.72, "text": " to volunteer with nonprofits and help them with their data science problems.", "tokens": [281, 13835, 365, 42851, 293, 854, 552, 365, 641, 1412, 3497, 2740, 13], "temperature": 0.0, "avg_logprob": -0.10168223705106569, "compression_ratio": 1.7148148148148148, "no_speech_prob": 7.071104846545495e-06}, {"id": 61, "seek": 32060, "start": 343.72, "end": 348.84000000000003, "text": " And she's with in this picture a team of other data scientists that are working", "tokens": [400, 750, 311, 365, 294, 341, 3036, 257, 1469, 295, 661, 1412, 7708, 300, 366, 1364], "temperature": 0.0, "avg_logprob": -0.10168223705106569, "compression_ratio": 1.7148148148148148, "no_speech_prob": 7.071104846545495e-06}, {"id": 62, "seek": 34884, "start": 348.84, "end": 353.64, "text": " together with Rainforest Connection and Rainforest Connection puts recycled", "tokens": [1214, 365, 14487, 36629, 11653, 313, 293, 14487, 36629, 11653, 313, 8137, 30674], "temperature": 0.0, "avg_logprob": -0.10247645086171675, "compression_ratio": 1.78125, "no_speech_prob": 1.4144221722744987e-06}, {"id": 63, "seek": 34884, "start": 353.64, "end": 359.59999999999997, "text": " cell phones in endangered rainforest and then streams audio and uses deep", "tokens": [2815, 10216, 294, 37539, 48531, 293, 550, 15842, 6278, 293, 4960, 2452], "temperature": 0.0, "avg_logprob": -0.10247645086171675, "compression_ratio": 1.78125, "no_speech_prob": 1.4144221722744987e-06}, {"id": 64, "seek": 34884, "start": 359.59999999999997, "end": 363.88, "text": " learning to identify chainsaw noises if people are illegally cutting down the", "tokens": [2539, 281, 5876, 12626, 1607, 14620, 498, 561, 366, 39585, 6492, 760, 264], "temperature": 0.0, "avg_logprob": -0.10247645086171675, "compression_ratio": 1.78125, "no_speech_prob": 1.4144221722744987e-06}, {"id": 65, "seek": 34884, "start": 363.88, "end": 368.76, "text": " forest. And then they have responders that they can send out in the field and", "tokens": [6719, 13, 400, 550, 436, 362, 37542, 300, 436, 393, 2845, 484, 294, 264, 2519, 293], "temperature": 0.0, "avg_logprob": -0.10247645086171675, "compression_ratio": 1.78125, "no_speech_prob": 1.4144221722744987e-06}, {"id": 66, "seek": 34884, "start": 368.76, "end": 372.71999999999997, "text": " so this is a deep learning problem of identifying what's a chainsaw and what's", "tokens": [370, 341, 307, 257, 2452, 2539, 1154, 295, 16696, 437, 311, 257, 12626, 1607, 293, 437, 311], "temperature": 0.0, "avg_logprob": -0.10247645086171675, "compression_ratio": 1.78125, "no_speech_prob": 1.4144221722744987e-06}, {"id": 67, "seek": 34884, "start": 372.71999999999997, "end": 376.35999999999996, "text": " not. She said that mosquitoes actually have a very similar frequency to", "tokens": [406, 13, 1240, 848, 300, 39394, 767, 362, 257, 588, 2531, 7893, 281], "temperature": 0.0, "avg_logprob": -0.10247645086171675, "compression_ratio": 1.78125, "no_speech_prob": 1.4144221722744987e-06}, {"id": 68, "seek": 37636, "start": 376.36, "end": 380.04, "text": " chainsaws. So there are a lot of kind of interesting challenges and what they're", "tokens": [12626, 12282, 13, 407, 456, 366, 257, 688, 295, 733, 295, 1880, 4759, 293, 437, 436, 434], "temperature": 0.0, "avg_logprob": -0.14635105790763064, "compression_ratio": 1.676595744680851, "no_speech_prob": 7.766483577142935e-06}, {"id": 69, "seek": 37636, "start": 380.04, "end": 387.36, "text": " what they're facing. These are pictures sent to me by Sahil Singla from India", "tokens": [437, 436, 434, 7170, 13, 1981, 366, 5242, 2279, 281, 385, 538, 18280, 388, 7474, 875, 490, 5282], "temperature": 0.0, "avg_logprob": -0.14635105790763064, "compression_ratio": 1.676595744680851, "no_speech_prob": 7.766483577142935e-06}, {"id": 70, "seek": 37636, "start": 387.36, "end": 392.40000000000003, "text": " who works with Farm Guide and in India thousands of farmers commit suicide", "tokens": [567, 1985, 365, 19991, 18727, 293, 294, 5282, 5383, 295, 11339, 5599, 12308], "temperature": 0.0, "avg_logprob": -0.14635105790763064, "compression_ratio": 1.676595744680851, "no_speech_prob": 7.766483577142935e-06}, {"id": 71, "seek": 37636, "start": 392.40000000000003, "end": 398.44, "text": " every year because well largely because they are taking these really predatory", "tokens": [633, 1064, 570, 731, 11611, 570, 436, 366, 1940, 613, 534, 3852, 4745], "temperature": 0.0, "avg_logprob": -0.14635105790763064, "compression_ratio": 1.676595744680851, "no_speech_prob": 7.766483577142935e-06}, {"id": 72, "seek": 37636, "start": 398.44, "end": 402.56, "text": " loans from loan sharks that then kind of threaten them with violence and threaten", "tokens": [15443, 490, 10529, 26312, 300, 550, 733, 295, 29864, 552, 365, 6270, 293, 29864], "temperature": 0.0, "avg_logprob": -0.14635105790763064, "compression_ratio": 1.676595744680851, "no_speech_prob": 7.766483577142935e-06}, {"id": 73, "seek": 40256, "start": 402.56, "end": 407.52, "text": " their families. And part of the issue is that they can't prove how much land they", "tokens": [641, 4466, 13, 400, 644, 295, 264, 2734, 307, 300, 436, 393, 380, 7081, 577, 709, 2117, 436], "temperature": 0.0, "avg_logprob": -0.11043140292167664, "compression_ratio": 1.6382978723404256, "no_speech_prob": 5.337667971616611e-06}, {"id": 74, "seek": 40256, "start": 407.52, "end": 412.6, "text": " own or what kind of crops they're growing. And so Sahil and his team are", "tokens": [1065, 420, 437, 733, 295, 16829, 436, 434, 4194, 13, 400, 370, 18280, 388, 293, 702, 1469, 366], "temperature": 0.0, "avg_logprob": -0.11043140292167664, "compression_ratio": 1.6382978723404256, "no_speech_prob": 5.337667971616611e-06}, {"id": 75, "seek": 40256, "start": 412.6, "end": 417.08, "text": " scraping data from Google Earth and then using deep learning to identify the", "tokens": [43738, 1412, 490, 3329, 4755, 293, 550, 1228, 2452, 2539, 281, 5876, 264], "temperature": 0.0, "avg_logprob": -0.11043140292167664, "compression_ratio": 1.6382978723404256, "no_speech_prob": 5.337667971616611e-06}, {"id": 76, "seek": 40256, "start": 417.08, "end": 422.2, "text": " plots of land and what's being grown to help the farmers qualify for better", "tokens": [28609, 295, 2117, 293, 437, 311, 885, 7709, 281, 854, 264, 11339, 20276, 337, 1101], "temperature": 0.0, "avg_logprob": -0.11043140292167664, "compression_ratio": 1.6382978723404256, "no_speech_prob": 5.337667971616611e-06}, {"id": 77, "seek": 40256, "start": 422.2, "end": 428.56, "text": " fairer loans. And then Samar Hayder is another one of our students and he's a", "tokens": [3143, 260, 15443, 13, 400, 550, 4832, 289, 8721, 1068, 307, 1071, 472, 295, 527, 1731, 293, 415, 311, 257], "temperature": 0.0, "avg_logprob": -0.11043140292167664, "compression_ratio": 1.6382978723404256, "no_speech_prob": 5.337667971616611e-06}, {"id": 78, "seek": 42856, "start": 428.56, "end": 433.92, "text": " natural language researcher in Pakistan. Pakistan has over 70 spoken languages", "tokens": [3303, 2856, 21751, 294, 15985, 13, 15985, 575, 670, 5285, 10759, 8650], "temperature": 0.0, "avg_logprob": -0.1326528495212771, "compression_ratio": 1.6765799256505576, "no_speech_prob": 3.2184772862819955e-05}, {"id": 79, "seek": 42856, "start": 433.92, "end": 438.56, "text": " and none of them not even Urdu which is the most common have that many resources", "tokens": [293, 6022, 295, 552, 406, 754, 9533, 769, 597, 307, 264, 881, 2689, 362, 300, 867, 3593], "temperature": 0.0, "avg_logprob": -0.1326528495212771, "compression_ratio": 1.6765799256505576, "no_speech_prob": 3.2184772862819955e-05}, {"id": 80, "seek": 42856, "start": 438.56, "end": 441.56, "text": " in terms of being able to translate other materials into and out of the", "tokens": [294, 2115, 295, 885, 1075, 281, 13799, 661, 5319, 666, 293, 484, 295, 264], "temperature": 0.0, "avg_logprob": -0.1326528495212771, "compression_ratio": 1.6765799256505576, "no_speech_prob": 3.2184772862819955e-05}, {"id": 81, "seek": 42856, "start": 441.56, "end": 445.28, "text": " languages. And he was inspired by our course to kind of build something", "tokens": [8650, 13, 400, 415, 390, 7547, 538, 527, 1164, 281, 733, 295, 1322, 746], "temperature": 0.0, "avg_logprob": -0.1326528495212771, "compression_ratio": 1.6765799256505576, "no_speech_prob": 3.2184772862819955e-05}, {"id": 82, "seek": 42856, "start": 445.28, "end": 450.12, "text": " similar to Word2Vec but for Urdu. And so this will be a really useful tool for", "tokens": [2531, 281, 8725, 17, 53, 3045, 457, 337, 9533, 769, 13, 400, 370, 341, 486, 312, 257, 534, 4420, 2290, 337], "temperature": 0.0, "avg_logprob": -0.1326528495212771, "compression_ratio": 1.6765799256505576, "no_speech_prob": 3.2184772862819955e-05}, {"id": 83, "seek": 42856, "start": 450.12, "end": 457.28, "text": " people working with the language. And then this is a little bit more", "tokens": [561, 1364, 365, 264, 2856, 13, 400, 550, 341, 307, 257, 707, 857, 544], "temperature": 0.0, "avg_logprob": -0.1326528495212771, "compression_ratio": 1.6765799256505576, "no_speech_prob": 3.2184772862819955e-05}, {"id": 84, "seek": 45728, "start": 457.28, "end": 463.03999999999996, "text": " light-hearted example created by one of our students. This is a picture of Kanye", "tokens": [1442, 12, 25471, 1365, 2942, 538, 472, 295, 527, 1731, 13, 639, 307, 257, 3036, 295, 37654], "temperature": 0.0, "avg_logprob": -0.1613056885568719, "compression_ratio": 1.683982683982684, "no_speech_prob": 1.7225846022483893e-05}, {"id": 85, "seek": 45728, "start": 463.03999999999996, "end": 468.4, "text": " West drawn with Captain Picard's face. So you'll see their Captain Picard's", "tokens": [4055, 10117, 365, 10873, 25895, 515, 311, 1851, 13, 407, 291, 603, 536, 641, 10873, 25895, 515, 311], "temperature": 0.0, "avg_logprob": -0.1613056885568719, "compression_ratio": 1.683982683982684, "no_speech_prob": 1.7225846022483893e-05}, {"id": 86, "seek": 45728, "start": 468.4, "end": 474.4, "text": " faces in miniature making Kanye West's face. So deep learning is being used to", "tokens": [8475, 294, 34674, 1455, 37654, 4055, 311, 1851, 13, 407, 2452, 2539, 307, 885, 1143, 281], "temperature": 0.0, "avg_logprob": -0.1613056885568719, "compression_ratio": 1.683982683982684, "no_speech_prob": 1.7225846022483893e-05}, {"id": 87, "seek": 45728, "start": 474.4, "end": 482.15999999999997, "text": " create art as well. And then kind of a more more serious art style another one", "tokens": [1884, 1523, 382, 731, 13, 400, 550, 733, 295, 257, 544, 544, 3156, 1523, 3758, 1071, 472], "temperature": 0.0, "avg_logprob": -0.1613056885568719, "compression_ratio": 1.683982683982684, "no_speech_prob": 1.7225846022483893e-05}, {"id": 88, "seek": 45728, "start": 482.15999999999997, "end": 487.0, "text": " of our students Vincent Moran came up with a new style transfer technique.", "tokens": [295, 527, 1731, 28003, 5146, 282, 1361, 493, 365, 257, 777, 3758, 5003, 6532, 13], "temperature": 0.0, "avg_logprob": -0.1613056885568719, "compression_ratio": 1.683982683982684, "no_speech_prob": 1.7225846022483893e-05}, {"id": 89, "seek": 48700, "start": 487.0, "end": 495.92, "text": " Let me open this up and this is actually a video. I'll just show you part of it.", "tokens": [961, 385, 1269, 341, 493, 293, 341, 307, 767, 257, 960, 13, 286, 603, 445, 855, 291, 644, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.12430433579433112, "compression_ratio": 1.6720430107526882, "no_speech_prob": 9.911914094118401e-05}, {"id": 90, "seek": 48700, "start": 495.92, "end": 502.6, "text": " But it takes a photo such as of this owl and then it takes a picture with a", "tokens": [583, 309, 2516, 257, 5052, 1270, 382, 295, 341, 34488, 293, 550, 309, 2516, 257, 3036, 365, 257], "temperature": 0.0, "avg_logprob": -0.12430433579433112, "compression_ratio": 1.6720430107526882, "no_speech_prob": 9.911914094118401e-05}, {"id": 91, "seek": 48700, "start": 502.6, "end": 509.08, "text": " particular style like this psychedelic cat and it gradually changes the owl to", "tokens": [1729, 3758, 411, 341, 47732, 299, 3857, 293, 309, 13145, 2962, 264, 34488, 281], "temperature": 0.0, "avg_logprob": -0.12430433579433112, "compression_ratio": 1.6720430107526882, "no_speech_prob": 9.911914094118401e-05}, {"id": 92, "seek": 48700, "start": 509.08, "end": 514.96, "text": " more and more have the style of the psychedelic cat. And so this is I think", "tokens": [544, 293, 544, 362, 264, 3758, 295, 264, 47732, 299, 3857, 13, 400, 370, 341, 307, 286, 519], "temperature": 0.0, "avg_logprob": -0.12430433579433112, "compression_ratio": 1.6720430107526882, "no_speech_prob": 9.911914094118401e-05}, {"id": 93, "seek": 51496, "start": 514.96, "end": 518.5600000000001, "text": " really can make some really pretty and interesting stuff from this. And you can", "tokens": [534, 393, 652, 512, 534, 1238, 293, 1880, 1507, 490, 341, 13, 400, 291, 393], "temperature": 0.0, "avg_logprob": -0.16152493158976236, "compression_ratio": 1.6136363636363635, "no_speech_prob": 5.063406933913939e-05}, {"id": 94, "seek": 51496, "start": 518.5600000000001, "end": 524.4000000000001, "text": " you can apply this to other other styles not just like a delicate but combining", "tokens": [291, 393, 3079, 341, 281, 661, 661, 13273, 406, 445, 411, 257, 21417, 457, 21928], "temperature": 0.0, "avg_logprob": -0.16152493158976236, "compression_ratio": 1.6136363636363635, "no_speech_prob": 5.063406933913939e-05}, {"id": 95, "seek": 51496, "start": 524.4000000000001, "end": 529.9200000000001, "text": " photos with Van Gogh or with other artists with distinctive styles. Let me", "tokens": [5787, 365, 8979, 39690, 71, 420, 365, 661, 6910, 365, 27766, 13273, 13, 961, 385], "temperature": 0.0, "avg_logprob": -0.16152493158976236, "compression_ratio": 1.6136363636363635, "no_speech_prob": 5.063406933913939e-05}, {"id": 96, "seek": 51496, "start": 529.9200000000001, "end": 534.24, "text": " just do one more and then we'll stop this.", "tokens": [445, 360, 472, 544, 293, 550, 321, 603, 1590, 341, 13], "temperature": 0.0, "avg_logprob": -0.16152493158976236, "compression_ratio": 1.6136363636363635, "no_speech_prob": 5.063406933913939e-05}, {"id": 97, "seek": 51496, "start": 536.8000000000001, "end": 540.72, "text": " And I think it's neat to kind of see the transformation of you know gradually", "tokens": [400, 286, 519, 309, 311, 10654, 281, 733, 295, 536, 264, 9887, 295, 291, 458, 13145], "temperature": 0.0, "avg_logprob": -0.16152493158976236, "compression_ratio": 1.6136363636363635, "no_speech_prob": 5.063406933913939e-05}, {"id": 98, "seek": 54072, "start": 540.72, "end": 547.0400000000001, "text": " going from you know this photo of the earth to this artistic rendering.", "tokens": [516, 490, 291, 458, 341, 5052, 295, 264, 4120, 281, 341, 17090, 22407, 13], "temperature": 0.0, "avg_logprob": -0.17876673380533853, "compression_ratio": 1.6758241758241759, "no_speech_prob": 1.5935800547595136e-05}, {"id": 99, "seek": 54072, "start": 550.72, "end": 555.44, "text": " So I just kind of share these to show that there's a really wide variety of", "tokens": [407, 286, 445, 733, 295, 2073, 613, 281, 855, 300, 456, 311, 257, 534, 4874, 5673, 295], "temperature": 0.0, "avg_logprob": -0.17876673380533853, "compression_ratio": 1.6758241758241759, "no_speech_prob": 1.5935800547595136e-05}, {"id": 100, "seek": 54072, "start": 555.44, "end": 561.0, "text": " the type of problems that can be or even you know type of things that can be", "tokens": [264, 2010, 295, 2740, 300, 393, 312, 420, 754, 291, 458, 2010, 295, 721, 300, 393, 312], "temperature": 0.0, "avg_logprob": -0.17876673380533853, "compression_ratio": 1.6758241758241759, "no_speech_prob": 1.5935800547595136e-05}, {"id": 101, "seek": 54072, "start": 561.0, "end": 568.72, "text": " created with deep learning. So I'll be mentioning this some during this. So fast", "tokens": [2942, 365, 2452, 2539, 13, 407, 286, 603, 312, 18315, 341, 512, 1830, 341, 13, 407, 2370], "temperature": 0.0, "avg_logprob": -0.17876673380533853, "compression_ratio": 1.6758241758241759, "no_speech_prob": 1.5935800547595136e-05}, {"id": 102, "seek": 56872, "start": 568.72, "end": 573.72, "text": " AI is me and Jeremy Howard and we partnered with the University of San", "tokens": [7318, 307, 385, 293, 17809, 17626, 293, 321, 29865, 365, 264, 3535, 295, 5271], "temperature": 0.0, "avg_logprob": -0.12219448459958568, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.5202543181658257e-05}, {"id": 103, "seek": 56872, "start": 573.72, "end": 579.5600000000001, "text": " Francisco's Data Institute and taught a course this fall this past fall deep", "tokens": [12279, 311, 11888, 9446, 293, 5928, 257, 1164, 341, 2100, 341, 1791, 2100, 2452], "temperature": 0.0, "avg_logprob": -0.12219448459958568, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.5202543181658257e-05}, {"id": 104, "seek": 56872, "start": 579.5600000000001, "end": 584.32, "text": " learning part one and it was distinctive in that there were no math prerequisites", "tokens": [2539, 644, 472, 293, 309, 390, 27766, 294, 300, 456, 645, 572, 5221, 38333, 15398, 3324], "temperature": 0.0, "avg_logprob": -0.12219448459958568, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.5202543181658257e-05}, {"id": 105, "seek": 56872, "start": 584.32, "end": 589.24, "text": " and a lot of materials for deep learning kind of assume that you have a graduate", "tokens": [293, 257, 688, 295, 5319, 337, 2452, 2539, 733, 295, 6552, 300, 291, 362, 257, 8080], "temperature": 0.0, "avg_logprob": -0.12219448459958568, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.5202543181658257e-05}, {"id": 106, "seek": 56872, "start": 589.24, "end": 592.44, "text": " level math background and we wanted to see if we could create something that", "tokens": [1496, 5221, 3678, 293, 321, 1415, 281, 536, 498, 321, 727, 1884, 746, 300], "temperature": 0.0, "avg_logprob": -0.12219448459958568, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.5202543181658257e-05}, {"id": 107, "seek": 56872, "start": 592.44, "end": 596.4, "text": " anyone with a year or two of coding experience could do. And we actually", "tokens": [2878, 365, 257, 1064, 420, 732, 295, 17720, 1752, 727, 360, 13, 400, 321, 767], "temperature": 0.0, "avg_logprob": -0.12219448459958568, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.5202543181658257e-05}, {"id": 108, "seek": 59640, "start": 596.4, "end": 600.88, "text": " weren't even sure if it would work it was a bit of an experiment and we", "tokens": [4999, 380, 754, 988, 498, 309, 576, 589, 309, 390, 257, 857, 295, 364, 5120, 293, 321], "temperature": 0.0, "avg_logprob": -0.11505224969651964, "compression_ratio": 1.6691176470588236, "no_speech_prob": 4.784240445587784e-06}, {"id": 109, "seek": 59640, "start": 600.88, "end": 604.8, "text": " released it for free online so all the materials are available for free at", "tokens": [4736, 309, 337, 1737, 2950, 370, 439, 264, 5319, 366, 2435, 337, 1737, 412], "temperature": 0.0, "avg_logprob": -0.11505224969651964, "compression_ratio": 1.6691176470588236, "no_speech_prob": 4.784240445587784e-06}, {"id": 110, "seek": 59640, "start": 604.8, "end": 609.8, "text": " course.fast.ai and it was a huge success. We had one of our students was", "tokens": [1164, 13, 7011, 13, 1301, 293, 309, 390, 257, 2603, 2245, 13, 492, 632, 472, 295, 527, 1731, 390], "temperature": 0.0, "avg_logprob": -0.11505224969651964, "compression_ratio": 1.6691176470588236, "no_speech_prob": 4.784240445587784e-06}, {"id": 111, "seek": 59640, "start": 609.8, "end": 614.0799999999999, "text": " accepted to the Google Brain residency program which is very prestigious and", "tokens": [9035, 281, 264, 3329, 29783, 34014, 1461, 597, 307, 588, 33510, 293], "temperature": 0.0, "avg_logprob": -0.11505224969651964, "compression_ratio": 1.6691176470588236, "no_speech_prob": 4.784240445587784e-06}, {"id": 112, "seek": 59640, "start": 614.0799999999999, "end": 619.36, "text": " she just learned to code two years ago. We had several other students get new", "tokens": [750, 445, 3264, 281, 3089, 732, 924, 2057, 13, 492, 632, 2940, 661, 1731, 483, 777], "temperature": 0.0, "avg_logprob": -0.11505224969651964, "compression_ratio": 1.6691176470588236, "no_speech_prob": 4.784240445587784e-06}, {"id": 113, "seek": 59640, "start": 619.36, "end": 624.3199999999999, "text": " job offers. One student got a patent for an algorithm he developed that he said", "tokens": [1691, 7736, 13, 1485, 3107, 658, 257, 20495, 337, 364, 9284, 415, 4743, 300, 415, 848], "temperature": 0.0, "avg_logprob": -0.11505224969651964, "compression_ratio": 1.6691176470588236, "no_speech_prob": 4.784240445587784e-06}, {"id": 114, "seek": 62432, "start": 624.32, "end": 628.2, "text": " is actually pretty similar to one of our lessons and that got him a bonus at work.", "tokens": [307, 767, 1238, 2531, 281, 472, 295, 527, 8820, 293, 300, 658, 796, 257, 10882, 412, 589, 13], "temperature": 0.0, "avg_logprob": -0.1692045916308154, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.1124610864499118e-05}, {"id": 115, "seek": 62432, "start": 628.2, "end": 633.32, "text": " We had one of our students worked for the TV show Silicon Valley and so he", "tokens": [492, 632, 472, 295, 527, 1731, 2732, 337, 264, 3558, 855, 25351, 10666, 293, 370, 415], "temperature": 0.0, "avg_logprob": -0.1692045916308154, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.1124610864499118e-05}, {"id": 116, "seek": 62432, "start": 633.32, "end": 638.5600000000001, "text": " created the not hot dog app if you're watching the current season and he also", "tokens": [2942, 264, 406, 2368, 3000, 724, 498, 291, 434, 1976, 264, 2190, 3196, 293, 415, 611], "temperature": 0.0, "avg_logprob": -0.1692045916308154, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.1124610864499118e-05}, {"id": 117, "seek": 62432, "start": 638.5600000000001, "end": 644.2800000000001, "text": " contributed code back to core tensorflow as part of that. So a lot of", "tokens": [18434, 3089, 646, 281, 4965, 40863, 10565, 382, 644, 295, 300, 13, 407, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.1692045916308154, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.1124610864499118e-05}, {"id": 118, "seek": 62432, "start": 644.2800000000001, "end": 647.6400000000001, "text": " interesting stories coming out of it and so I'll occasionally mention the course", "tokens": [1880, 3676, 1348, 484, 295, 309, 293, 370, 286, 603, 16895, 2152, 264, 1164], "temperature": 0.0, "avg_logprob": -0.1692045916308154, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.1124610864499118e-05}, {"id": 119, "seek": 62432, "start": 647.6400000000001, "end": 652.96, "text": " but just wanted to let you know that it's there and it's completely free.", "tokens": [457, 445, 1415, 281, 718, 291, 458, 300, 309, 311, 456, 293, 309, 311, 2584, 1737, 13], "temperature": 0.0, "avg_logprob": -0.1692045916308154, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.1124610864499118e-05}, {"id": 120, "seek": 65296, "start": 652.96, "end": 658.2800000000001, "text": " And then we also have forums going along with it at forums.fast.ai and this", "tokens": [400, 550, 321, 611, 362, 26998, 516, 2051, 365, 309, 412, 26998, 13, 7011, 13, 1301, 293, 341], "temperature": 0.0, "avg_logprob": -0.1278011906254399, "compression_ratio": 1.751937984496124, "no_speech_prob": 4.3988249672111124e-05}, {"id": 121, "seek": 65296, "start": 658.2800000000001, "end": 662.52, "text": " is a place where people can ask questions or share interesting links and", "tokens": [307, 257, 1081, 689, 561, 393, 1029, 1651, 420, 2073, 1880, 6123, 293], "temperature": 0.0, "avg_logprob": -0.1278011906254399, "compression_ratio": 1.751937984496124, "no_speech_prob": 4.3988249672111124e-05}, {"id": 122, "seek": 65296, "start": 662.52, "end": 666.6, "text": " I think it's a pretty helpful community and so if you're even if you're not", "tokens": [286, 519, 309, 311, 257, 1238, 4961, 1768, 293, 370, 498, 291, 434, 754, 498, 291, 434, 406], "temperature": 0.0, "avg_logprob": -0.1278011906254399, "compression_ratio": 1.751937984496124, "no_speech_prob": 4.3988249672111124e-05}, {"id": 123, "seek": 65296, "start": 666.6, "end": 670.0400000000001, "text": " taking the course if you have deep learning related questions or things", "tokens": [1940, 264, 1164, 498, 291, 362, 2452, 2539, 4077, 1651, 420, 721], "temperature": 0.0, "avg_logprob": -0.1278011906254399, "compression_ratio": 1.751937984496124, "no_speech_prob": 4.3988249672111124e-05}, {"id": 124, "seek": 65296, "start": 670.0400000000001, "end": 675.36, "text": " that you find interesting I wanted to let you know that this is out there.", "tokens": [300, 291, 915, 1880, 286, 1415, 281, 718, 291, 458, 300, 341, 307, 484, 456, 13], "temperature": 0.0, "avg_logprob": -0.1278011906254399, "compression_ratio": 1.751937984496124, "no_speech_prob": 4.3988249672111124e-05}, {"id": 125, "seek": 65296, "start": 676.12, "end": 682.12, "text": " Okay so move on to and again stop me at any point if you have questions. So I'll", "tokens": [1033, 370, 1286, 322, 281, 293, 797, 1590, 385, 412, 604, 935, 498, 291, 362, 1651, 13, 407, 286, 603], "temperature": 0.0, "avg_logprob": -0.1278011906254399, "compression_ratio": 1.751937984496124, "no_speech_prob": 4.3988249672111124e-05}, {"id": 126, "seek": 68212, "start": 682.12, "end": 690.8, "text": " stop now. Are there any questions? Okay so moving on to today's topic which is", "tokens": [1590, 586, 13, 2014, 456, 604, 1651, 30, 1033, 370, 2684, 322, 281, 965, 311, 4829, 597, 307], "temperature": 0.0, "avg_logprob": -0.14257650265748473, "compression_ratio": 1.5252100840336134, "no_speech_prob": 1.7229462173418142e-05}, {"id": 127, "seek": 68212, "start": 690.8, "end": 696.72, "text": " word embeddings. First I wanted to show you where word embeddings are used just", "tokens": [1349, 12240, 29432, 13, 2386, 286, 1415, 281, 855, 291, 689, 1349, 12240, 29432, 366, 1143, 445], "temperature": 0.0, "avg_logprob": -0.14257650265748473, "compression_ratio": 1.5252100840336134, "no_speech_prob": 1.7229462173418142e-05}, {"id": 128, "seek": 68212, "start": 696.72, "end": 701.64, "text": " a few examples. So this is an article from last year but", "tokens": [257, 1326, 5110, 13, 407, 341, 307, 364, 7222, 490, 1036, 1064, 457], "temperature": 0.0, "avg_logprob": -0.14257650265748473, "compression_ratio": 1.5252100840336134, "no_speech_prob": 1.7229462173418142e-05}, {"id": 129, "seek": 68212, "start": 701.64, "end": 705.36, "text": " Baidu's deep learning system is better at English and Mandarin speech", "tokens": [6777, 327, 84, 311, 2452, 2539, 1185, 307, 1101, 412, 3669, 293, 42292, 6218], "temperature": 0.0, "avg_logprob": -0.14257650265748473, "compression_ratio": 1.5252100840336134, "no_speech_prob": 1.7229462173418142e-05}, {"id": 130, "seek": 68212, "start": 705.36, "end": 710.96, "text": " recognition than most people which is really impressive and that's using word", "tokens": [11150, 813, 881, 561, 597, 307, 534, 8992, 293, 300, 311, 1228, 1349], "temperature": 0.0, "avg_logprob": -0.14257650265748473, "compression_ratio": 1.5252100840336134, "no_speech_prob": 1.7229462173418142e-05}, {"id": 131, "seek": 71096, "start": 710.96, "end": 716.76, "text": " embeddings. This is called image captioning and these are algorithms that", "tokens": [12240, 29432, 13, 639, 307, 1219, 3256, 31974, 278, 293, 613, 366, 14642, 300], "temperature": 0.0, "avg_logprob": -0.15864388147989908, "compression_ratio": 1.8428571428571427, "no_speech_prob": 3.3727497793734074e-05}, {"id": 132, "seek": 71096, "start": 716.76, "end": 722.8000000000001, "text": " you give a picture such as picture on the left here and then the algorithm", "tokens": [291, 976, 257, 3036, 1270, 382, 3036, 322, 264, 1411, 510, 293, 550, 264, 9284], "temperature": 0.0, "avg_logprob": -0.15864388147989908, "compression_ratio": 1.8428571428571427, "no_speech_prob": 3.3727497793734074e-05}, {"id": 133, "seek": 71096, "start": 722.8000000000001, "end": 727.5600000000001, "text": " comes up with a caption. So in this case man in black shirt is playing guitar or", "tokens": [1487, 493, 365, 257, 31974, 13, 407, 294, 341, 1389, 587, 294, 2211, 8336, 307, 2433, 7531, 420], "temperature": 0.0, "avg_logprob": -0.15864388147989908, "compression_ratio": 1.8428571428571427, "no_speech_prob": 3.3727497793734074e-05}, {"id": 134, "seek": 71096, "start": 727.5600000000001, "end": 732.52, "text": " for the picture on the right you give it the picture and the algorithm comes", "tokens": [337, 264, 3036, 322, 264, 558, 291, 976, 309, 264, 3036, 293, 264, 9284, 1487], "temperature": 0.0, "avg_logprob": -0.15864388147989908, "compression_ratio": 1.8428571428571427, "no_speech_prob": 3.3727497793734074e-05}, {"id": 135, "seek": 71096, "start": 732.52, "end": 736.8000000000001, "text": " up with construction worker an orange safety vest is working on the road. And so", "tokens": [493, 365, 6435, 11346, 364, 7671, 4514, 15814, 307, 1364, 322, 264, 3060, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.15864388147989908, "compression_ratio": 1.8428571428571427, "no_speech_prob": 3.3727497793734074e-05}, {"id": 136, "seek": 73680, "start": 736.8, "end": 742.88, "text": " this is that a few years ago something that was not not being done. And then", "tokens": [341, 307, 300, 257, 1326, 924, 2057, 746, 300, 390, 406, 406, 885, 1096, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.12037374618205618, "compression_ratio": 1.668122270742358, "no_speech_prob": 3.821885911747813e-05}, {"id": 137, "seek": 73680, "start": 742.88, "end": 749.04, "text": " this is a Google's smart reply for your email where it makes a few suggestions", "tokens": [341, 307, 257, 3329, 311, 4069, 16972, 337, 428, 3796, 689, 309, 1669, 257, 1326, 13396], "temperature": 0.0, "avg_logprob": -0.12037374618205618, "compression_ratio": 1.668122270742358, "no_speech_prob": 3.821885911747813e-05}, {"id": 138, "seek": 73680, "start": 749.04, "end": 753.68, "text": " of how you how you can reply to an email. This is somebody asking about", "tokens": [295, 577, 291, 577, 291, 393, 16972, 281, 364, 3796, 13, 639, 307, 2618, 3365, 466], "temperature": 0.0, "avg_logprob": -0.12037374618205618, "compression_ratio": 1.668122270742358, "no_speech_prob": 3.821885911747813e-05}, {"id": 139, "seek": 73680, "start": 753.68, "end": 758.52, "text": " vacation plans and Google's suggesting that maybe you want to say no plans yet", "tokens": [12830, 5482, 293, 3329, 311, 18094, 300, 1310, 291, 528, 281, 584, 572, 5482, 1939], "temperature": 0.0, "avg_logprob": -0.12037374618205618, "compression_ratio": 1.668122270742358, "no_speech_prob": 3.821885911747813e-05}, {"id": 140, "seek": 73680, "start": 758.52, "end": 763.0799999999999, "text": " or I just sent them to you. This was actually an April Fool's Day joke from", "tokens": [420, 286, 445, 2279, 552, 281, 291, 13, 639, 390, 767, 364, 6929, 41583, 311, 5226, 7647, 490], "temperature": 0.0, "avg_logprob": -0.12037374618205618, "compression_ratio": 1.668122270742358, "no_speech_prob": 3.821885911747813e-05}, {"id": 141, "seek": 76308, "start": 763.08, "end": 767.64, "text": " Google in 2009 because at that point it was so outlandish that you know email", "tokens": [3329, 294, 11453, 570, 412, 300, 935, 309, 390, 370, 484, 1661, 742, 300, 291, 458, 3796], "temperature": 0.0, "avg_logprob": -0.16256548433887716, "compression_ratio": 1.596638655462185, "no_speech_prob": 1.5205687304842286e-05}, {"id": 142, "seek": 76308, "start": 767.64, "end": 772.12, "text": " could write itself and then they a few years later released it as a product.", "tokens": [727, 2464, 2564, 293, 550, 436, 257, 1326, 924, 1780, 4736, 309, 382, 257, 1674, 13], "temperature": 0.0, "avg_logprob": -0.16256548433887716, "compression_ratio": 1.596638655462185, "no_speech_prob": 1.5205687304842286e-05}, {"id": 143, "seek": 76308, "start": 772.12, "end": 778.32, "text": " But all of these are using word embeddings. And then this is one from", "tokens": [583, 439, 295, 613, 366, 1228, 1349, 12240, 29432, 13, 400, 550, 341, 307, 472, 490], "temperature": 0.0, "avg_logprob": -0.16256548433887716, "compression_ratio": 1.596638655462185, "no_speech_prob": 1.5205687304842286e-05}, {"id": 144, "seek": 76308, "start": 778.32, "end": 784.5200000000001, "text": " part two of our course we taught part two this spring and here you can give it", "tokens": [644, 732, 295, 527, 1164, 321, 5928, 644, 732, 341, 5587, 293, 510, 291, 393, 976, 309], "temperature": 0.0, "avg_logprob": -0.16256548433887716, "compression_ratio": 1.596638655462185, "no_speech_prob": 1.5205687304842286e-05}, {"id": 145, "seek": 76308, "start": 784.5200000000001, "end": 790.34, "text": " words and so we're saying tench plus net. Tench is the type of fish and then", "tokens": [2283, 293, 370, 321, 434, 1566, 2064, 339, 1804, 2533, 13, 314, 40765, 307, 264, 2010, 295, 3506, 293, 550], "temperature": 0.0, "avg_logprob": -0.16256548433887716, "compression_ratio": 1.596638655462185, "no_speech_prob": 1.5205687304842286e-05}, {"id": 146, "seek": 79034, "start": 790.34, "end": 795.84, "text": " we're seeing back what image we get. And this is a tension and net. This is kind", "tokens": [321, 434, 2577, 646, 437, 3256, 321, 483, 13, 400, 341, 307, 257, 8980, 293, 2533, 13, 639, 307, 733], "temperature": 0.0, "avg_logprob": -0.21083387927474262, "compression_ratio": 1.6680851063829787, "no_speech_prob": 8.800201612757519e-06}, {"id": 147, "seek": 79034, "start": 795.84, "end": 803.24, "text": " of the reverse. You give it some words you get back an image. This is a blog", "tokens": [295, 264, 9943, 13, 509, 976, 309, 512, 2283, 291, 483, 646, 364, 3256, 13, 639, 307, 257, 6968], "temperature": 0.0, "avg_logprob": -0.21083387927474262, "compression_ratio": 1.6680851063829787, "no_speech_prob": 8.800201612757519e-06}, {"id": 148, "seek": 79034, "start": 803.24, "end": 808.0400000000001, "text": " post from another one of our students and I like that it's how QUID uses deep", "tokens": [2183, 490, 1071, 472, 295, 527, 1731, 293, 286, 411, 300, 309, 311, 577, 7246, 2777, 4960, 2452], "temperature": 0.0, "avg_logprob": -0.21083387927474262, "compression_ratio": 1.6680851063829787, "no_speech_prob": 8.800201612757519e-06}, {"id": 149, "seek": 79034, "start": 808.0400000000001, "end": 813.1600000000001, "text": " learning with small data. And QUID is a company that has a lot of a lot of", "tokens": [2539, 365, 1359, 1412, 13, 400, 7246, 2777, 307, 257, 2237, 300, 575, 257, 688, 295, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.21083387927474262, "compression_ratio": 1.6680851063829787, "no_speech_prob": 8.800201612757519e-06}, {"id": 150, "seek": 79034, "start": 813.1600000000001, "end": 818.0400000000001, "text": " data sets. There's a lot of data science and they said that one issue for them is", "tokens": [1412, 6352, 13, 821, 311, 257, 688, 295, 1412, 3497, 293, 436, 848, 300, 472, 2734, 337, 552, 307], "temperature": 0.0, "avg_logprob": -0.21083387927474262, "compression_ratio": 1.6680851063829787, "no_speech_prob": 8.800201612757519e-06}, {"id": 151, "seek": 81804, "start": 818.04, "end": 823.24, "text": " they have a database of company descriptions and having kind of low", "tokens": [436, 362, 257, 8149, 295, 2237, 24406, 293, 1419, 733, 295, 2295], "temperature": 0.0, "avg_logprob": -0.16002775548578618, "compression_ratio": 1.6980392156862745, "no_speech_prob": 5.421246441983385e-06}, {"id": 152, "seek": 81804, "start": 823.24, "end": 827.16, "text": " quality descriptions that are very generic and vague. They want to identify", "tokens": [3125, 24406, 300, 366, 588, 19577, 293, 24247, 13, 814, 528, 281, 5876], "temperature": 0.0, "avg_logprob": -0.16002775548578618, "compression_ratio": 1.6980392156862745, "no_speech_prob": 5.421246441983385e-06}, {"id": 153, "seek": 81804, "start": 827.16, "end": 833.28, "text": " those because those are bad for their use case. And so for example having", "tokens": [729, 570, 729, 366, 1578, 337, 641, 764, 1389, 13, 400, 370, 337, 1365, 1419], "temperature": 0.0, "avg_logprob": -0.16002775548578618, "compression_ratio": 1.6980392156862745, "no_speech_prob": 5.421246441983385e-06}, {"id": 154, "seek": 81804, "start": 833.28, "end": 837.16, "text": " something that says our patent pending support system is engineered and", "tokens": [746, 300, 1619, 527, 20495, 32110, 1406, 1185, 307, 38648, 293], "temperature": 0.0, "avg_logprob": -0.16002775548578618, "compression_ratio": 1.6980392156862745, "no_speech_prob": 5.421246441983385e-06}, {"id": 155, "seek": 81804, "start": 837.16, "end": 841.0799999999999, "text": " designed to bring comfort and style that doesn't actually convey much", "tokens": [4761, 281, 1565, 3400, 293, 3758, 300, 1177, 380, 767, 16965, 709], "temperature": 0.0, "avg_logprob": -0.16002775548578618, "compression_ratio": 1.6980392156862745, "no_speech_prob": 5.421246441983385e-06}, {"id": 156, "seek": 81804, "start": 841.0799999999999, "end": 845.48, "text": " information and so that would be a bad description that they're trying to", "tokens": [1589, 293, 370, 300, 576, 312, 257, 1578, 3855, 300, 436, 434, 1382, 281], "temperature": 0.0, "avg_logprob": -0.16002775548578618, "compression_ratio": 1.6980392156862745, "no_speech_prob": 5.421246441983385e-06}, {"id": 157, "seek": 84548, "start": 845.48, "end": 849.44, "text": " identify. Whereas an example of a good description would be it has developed", "tokens": [5876, 13, 13813, 364, 1365, 295, 257, 665, 3855, 576, 312, 309, 575, 4743], "temperature": 0.0, "avg_logprob": -0.17650533563950482, "compression_ratio": 1.5938864628820961, "no_speech_prob": 7.88805937190773e-06}, {"id": 158, "seek": 84548, "start": 849.44, "end": 854.9200000000001, "text": " lens lensless imaging optic, the natural eye optic, the NEO. The NEO replicates", "tokens": [6765, 6765, 1832, 25036, 48269, 11, 264, 3303, 3313, 48269, 11, 264, 426, 6004, 13, 440, 426, 6004, 3248, 299, 1024], "temperature": 0.0, "avg_logprob": -0.17650533563950482, "compression_ratio": 1.5938864628820961, "no_speech_prob": 7.88805937190773e-06}, {"id": 159, "seek": 84548, "start": 854.9200000000001, "end": 858.16, "text": " the human vision system. That's a description that actually has", "tokens": [264, 1952, 5201, 1185, 13, 663, 311, 257, 3855, 300, 767, 575], "temperature": 0.0, "avg_logprob": -0.17650533563950482, "compression_ratio": 1.5938864628820961, "no_speech_prob": 7.88805937190773e-06}, {"id": 160, "seek": 84548, "start": 858.16, "end": 862.24, "text": " information details and they wanted to be able to distinguish between these.", "tokens": [1589, 4365, 293, 436, 1415, 281, 312, 1075, 281, 20206, 1296, 613, 13], "temperature": 0.0, "avg_logprob": -0.17650533563950482, "compression_ratio": 1.5938864628820961, "no_speech_prob": 7.88805937190773e-06}, {"id": 161, "seek": 86224, "start": 862.24, "end": 876.28, "text": " And they use deep learning including word embeddings to do that. So computers", "tokens": [400, 436, 764, 2452, 2539, 3009, 1349, 12240, 29432, 281, 360, 300, 13, 407, 10807], "temperature": 0.0, "avg_logprob": -0.12189710956730254, "compression_ratio": 1.656084656084656, "no_speech_prob": 5.0933049351442605e-06}, {"id": 162, "seek": 86224, "start": 876.28, "end": 880.96, "text": " and machine learning treat pictures and words as numbers. So to be able to kind", "tokens": [293, 3479, 2539, 2387, 5242, 293, 2283, 382, 3547, 13, 407, 281, 312, 1075, 281, 733], "temperature": 0.0, "avg_logprob": -0.12189710956730254, "compression_ratio": 1.656084656084656, "no_speech_prob": 5.0933049351442605e-06}, {"id": 163, "seek": 86224, "start": 880.96, "end": 886.76, "text": " of process pictures and words they need to read them in as numbers. This is an", "tokens": [295, 1399, 5242, 293, 2283, 436, 643, 281, 1401, 552, 294, 382, 3547, 13, 639, 307, 364], "temperature": 0.0, "avg_logprob": -0.12189710956730254, "compression_ratio": 1.656084656084656, "no_speech_prob": 5.0933049351442605e-06}, {"id": 164, "seek": 86224, "start": 886.76, "end": 889.6, "text": " example and we're not really talking about images today but I just wanted to", "tokens": [1365, 293, 321, 434, 406, 534, 1417, 466, 5267, 965, 457, 286, 445, 1415, 281], "temperature": 0.0, "avg_logprob": -0.12189710956730254, "compression_ratio": 1.656084656084656, "no_speech_prob": 5.0933049351442605e-06}, {"id": 165, "seek": 88960, "start": 889.6, "end": 893.96, "text": " show you this because it shows up so much. But this is a handwritten digit 8", "tokens": [855, 291, 341, 570, 309, 3110, 493, 370, 709, 13, 583, 341, 307, 257, 1011, 26859, 14293, 1649], "temperature": 0.0, "avg_logprob": -0.14073162608676487, "compression_ratio": 1.7148936170212765, "no_speech_prob": 7.25272111594677e-05}, {"id": 166, "seek": 88960, "start": 893.96, "end": 900.48, "text": " and showing how it can be represented as a 20 by 20 matrix of numbers. And in this", "tokens": [293, 4099, 577, 309, 393, 312, 10379, 382, 257, 945, 538, 945, 8141, 295, 3547, 13, 400, 294, 341], "temperature": 0.0, "avg_logprob": -0.14073162608676487, "compression_ratio": 1.7148936170212765, "no_speech_prob": 7.25272111594677e-05}, {"id": 167, "seek": 88960, "start": 900.48, "end": 907.6, "text": " case each white pixel is a 0, each black pixel is 255, and then you can have any", "tokens": [1389, 1184, 2418, 19261, 307, 257, 1958, 11, 1184, 2211, 19261, 307, 3552, 20, 11, 293, 550, 291, 393, 362, 604], "temperature": 0.0, "avg_logprob": -0.14073162608676487, "compression_ratio": 1.7148936170212765, "no_speech_prob": 7.25272111594677e-05}, {"id": 168, "seek": 88960, "start": 907.6, "end": 912.2, "text": " value in between for gray. And so this is a way to you know take this white and", "tokens": [2158, 294, 1296, 337, 10855, 13, 400, 370, 341, 307, 257, 636, 281, 291, 458, 747, 341, 2418, 293], "temperature": 0.0, "avg_logprob": -0.14073162608676487, "compression_ratio": 1.7148936170212765, "no_speech_prob": 7.25272111594677e-05}, {"id": 169, "seek": 88960, "start": 912.2, "end": 917.5600000000001, "text": " black picture and transfer it to a 20 by 20 matrix. And that's how the computer is", "tokens": [2211, 3036, 293, 5003, 309, 281, 257, 945, 538, 945, 8141, 13, 400, 300, 311, 577, 264, 3820, 307], "temperature": 0.0, "avg_logprob": -0.14073162608676487, "compression_ratio": 1.7148936170212765, "no_speech_prob": 7.25272111594677e-05}, {"id": 170, "seek": 91756, "start": 917.56, "end": 923.9599999999999, "text": " going to work with it. And then we'll be going into much more detail about word", "tokens": [516, 281, 589, 365, 309, 13, 400, 550, 321, 603, 312, 516, 666, 709, 544, 2607, 466, 1349], "temperature": 0.0, "avg_logprob": -0.10976495566191496, "compression_ratio": 1.7397769516728625, "no_speech_prob": 1.7501695765531622e-05}, {"id": 171, "seek": 91756, "start": 923.9599999999999, "end": 929.4799999999999, "text": " embeddings because that's how computers can deal with language. But I just want", "tokens": [12240, 29432, 570, 300, 311, 577, 10807, 393, 2028, 365, 2856, 13, 583, 286, 445, 528], "temperature": 0.0, "avg_logprob": -0.10976495566191496, "compression_ratio": 1.7397769516728625, "no_speech_prob": 1.7501695765531622e-05}, {"id": 172, "seek": 91756, "start": 929.4799999999999, "end": 933.3199999999999, "text": " to let you know that there are vectors and they can capture relationships. Like", "tokens": [281, 718, 291, 458, 300, 456, 366, 18875, 293, 436, 393, 7983, 6159, 13, 1743], "temperature": 0.0, "avg_logprob": -0.10976495566191496, "compression_ratio": 1.7397769516728625, "no_speech_prob": 1.7501695765531622e-05}, {"id": 173, "seek": 91756, "start": 933.3199999999999, "end": 936.5999999999999, "text": " here we see there's a particular relationship between walking and walked.", "tokens": [510, 321, 536, 456, 311, 257, 1729, 2480, 1296, 4494, 293, 7628, 13], "temperature": 0.0, "avg_logprob": -0.10976495566191496, "compression_ratio": 1.7397769516728625, "no_speech_prob": 1.7501695765531622e-05}, {"id": 174, "seek": 91756, "start": 936.5999999999999, "end": 942.18, "text": " That's very similar to the relationship between swimming and swam. So kind of to", "tokens": [663, 311, 588, 2531, 281, 264, 2480, 1296, 11989, 293, 1693, 335, 13, 407, 733, 295, 281], "temperature": 0.0, "avg_logprob": -0.10976495566191496, "compression_ratio": 1.7397769516728625, "no_speech_prob": 1.7501695765531622e-05}, {"id": 175, "seek": 91756, "start": 942.18, "end": 946.2399999999999, "text": " move from one vector to the other you're going the same same distance and", "tokens": [1286, 490, 472, 8062, 281, 264, 661, 291, 434, 516, 264, 912, 912, 4560, 293], "temperature": 0.0, "avg_logprob": -0.10976495566191496, "compression_ratio": 1.7397769516728625, "no_speech_prob": 1.7501695765531622e-05}, {"id": 176, "seek": 94624, "start": 946.24, "end": 953.8, "text": " direction to get between these different verb tenses. So before I go", "tokens": [3513, 281, 483, 1296, 613, 819, 9595, 256, 9085, 13, 407, 949, 286, 352], "temperature": 0.0, "avg_logprob": -0.3068663201680997, "compression_ratio": 1.5095238095238095, "no_speech_prob": 8.01322948973393e-06}, {"id": 177, "seek": 94624, "start": 953.8, "end": 959.44, "text": " further into this, I actually wanted to check so who in here loves math?", "tokens": [3052, 666, 341, 11, 286, 767, 1415, 281, 1520, 370, 567, 294, 510, 6752, 5221, 30], "temperature": 0.0, "avg_logprob": -0.3068663201680997, "compression_ratio": 1.5095238095238095, "no_speech_prob": 8.01322948973393e-06}, {"id": 178, "seek": 94624, "start": 959.44, "end": 967.88, "text": " Cool, me too. And then who in here has some sort of anxiety or any negative feelings about math?", "tokens": [8561, 11, 385, 886, 13, 400, 550, 567, 294, 510, 575, 512, 1333, 295, 9119, 420, 604, 3671, 6640, 466, 5221, 30], "temperature": 0.0, "avg_logprob": -0.3068663201680997, "compression_ratio": 1.5095238095238095, "no_speech_prob": 8.01322948973393e-06}, {"id": 179, "seek": 94624, "start": 967.88, "end": 972.5600000000001, "text": " Okay so yeah, few people, not as many as I often hear, but I wanted to explain", "tokens": [1033, 370, 1338, 11, 1326, 561, 11, 406, 382, 867, 382, 286, 2049, 1568, 11, 457, 286, 1415, 281, 2903], "temperature": 0.0, "avg_logprob": -0.3068663201680997, "compression_ratio": 1.5095238095238095, "no_speech_prob": 8.01322948973393e-06}, {"id": 180, "seek": 97256, "start": 972.56, "end": 976.52, "text": " this because I get this a lot from people who really don't feel good about", "tokens": [341, 570, 286, 483, 341, 257, 688, 490, 561, 567, 534, 500, 380, 841, 665, 466], "temperature": 0.0, "avg_logprob": -0.12454265356063843, "compression_ratio": 1.5673469387755101, "no_speech_prob": 1.2217858966323547e-05}, {"id": 181, "seek": 97256, "start": 976.52, "end": 980.88, "text": " their own math ability or kind of have anxiety around it. So there's a beautiful", "tokens": [641, 1065, 5221, 3485, 420, 733, 295, 362, 9119, 926, 309, 13, 407, 456, 311, 257, 2238], "temperature": 0.0, "avg_logprob": -0.12454265356063843, "compression_ratio": 1.5673469387755101, "no_speech_prob": 1.2217858966323547e-05}, {"id": 182, "seek": 97256, "start": 980.88, "end": 989.68, "text": " essay called A Mathematician's Lament by Paul Lockhart. And Paul Lockhart was a", "tokens": [16238, 1219, 316, 15776, 14911, 952, 311, 441, 2466, 538, 4552, 16736, 42535, 13, 400, 4552, 16736, 42535, 390, 257], "temperature": 0.0, "avg_logprob": -0.12454265356063843, "compression_ratio": 1.5673469387755101, "no_speech_prob": 1.2217858966323547e-05}, {"id": 183, "seek": 97256, "start": 989.68, "end": 994.68, "text": " math professor at Brown and he quit to go teach KQ12 education because he", "tokens": [5221, 8304, 412, 8030, 293, 415, 10366, 281, 352, 2924, 591, 48, 4762, 3309, 570, 415], "temperature": 0.0, "avg_logprob": -0.12454265356063843, "compression_ratio": 1.5673469387755101, "no_speech_prob": 1.2217858966323547e-05}, {"id": 184, "seek": 97256, "start": 994.68, "end": 999.5, "text": " thought math education was such a disaster in the United States. And so he", "tokens": [1194, 5221, 3309, 390, 1270, 257, 11293, 294, 264, 2824, 3040, 13, 400, 370, 415], "temperature": 0.0, "avg_logprob": -0.12454265356063843, "compression_ratio": 1.5673469387755101, "no_speech_prob": 1.2217858966323547e-05}, {"id": 185, "seek": 99950, "start": 999.5, "end": 1003.24, "text": " has this essay where he writes about a world where children are not allowed to", "tokens": [575, 341, 16238, 689, 415, 13657, 466, 257, 1002, 689, 2227, 366, 406, 4350, 281], "temperature": 0.0, "avg_logprob": -0.16295909881591797, "compression_ratio": 1.6501766784452296, "no_speech_prob": 1.7228452634299174e-05}, {"id": 186, "seek": 99950, "start": 1003.24, "end": 1008.28, "text": " sing songs or play instruments until graduate school because it's really", "tokens": [1522, 5781, 420, 862, 12190, 1826, 8080, 1395, 570, 309, 311, 534], "temperature": 0.0, "avg_logprob": -0.16295909881591797, "compression_ratio": 1.6501766784452296, "no_speech_prob": 1.7228452634299174e-05}, {"id": 187, "seek": 99950, "start": 1008.28, "end": 1011.68, "text": " important that they have over a decade of studying music notation and", "tokens": [1021, 300, 436, 362, 670, 257, 10378, 295, 7601, 1318, 24657, 293], "temperature": 0.0, "avg_logprob": -0.16295909881591797, "compression_ratio": 1.6501766784452296, "no_speech_prob": 1.7228452634299174e-05}, {"id": 188, "seek": 99950, "start": 1011.68, "end": 1017.68, "text": " transcribing notes by hand before we can let them sing. And so it's a, you know,", "tokens": [1145, 39541, 5570, 538, 1011, 949, 321, 393, 718, 552, 1522, 13, 400, 370, 309, 311, 257, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.16295909881591797, "compression_ratio": 1.6501766784452296, "no_speech_prob": 1.7228452634299174e-05}, {"id": 189, "seek": 99950, "start": 1017.68, "end": 1020.68, "text": " sounds horrifying, like that doesn't make sense to make people wait till they're,", "tokens": [3263, 40227, 11, 411, 300, 1177, 380, 652, 2020, 281, 652, 561, 1699, 4288, 436, 434, 11], "temperature": 0.0, "avg_logprob": -0.16295909881591797, "compression_ratio": 1.6501766784452296, "no_speech_prob": 1.7228452634299174e-05}, {"id": 190, "seek": 99950, "start": 1020.68, "end": 1026.56, "text": " you know, 20 to sing. But that's what we do with math education is what he argues.", "tokens": [291, 458, 11, 945, 281, 1522, 13, 583, 300, 311, 437, 321, 360, 365, 5221, 3309, 307, 437, 415, 38218, 13], "temperature": 0.0, "avg_logprob": -0.16295909881591797, "compression_ratio": 1.6501766784452296, "no_speech_prob": 1.7228452634299174e-05}, {"id": 191, "seek": 102656, "start": 1026.56, "end": 1033.08, "text": " And I agree that we often focus on kind of dry disconnected notation and that", "tokens": [400, 286, 3986, 300, 321, 2049, 1879, 322, 733, 295, 4016, 29426, 24657, 293, 300], "temperature": 0.0, "avg_logprob": -0.1486949920654297, "compression_ratio": 1.5182186234817814, "no_speech_prob": 1.2605780284502544e-05}, {"id": 192, "seek": 102656, "start": 1033.08, "end": 1037.0, "text": " the really more beautiful and creative parts of it don't come until later", "tokens": [264, 534, 544, 2238, 293, 5880, 3166, 295, 309, 500, 380, 808, 1826, 1780], "temperature": 0.0, "avg_logprob": -0.1486949920654297, "compression_ratio": 1.5182186234817814, "no_speech_prob": 1.2605780284502544e-05}, {"id": 193, "seek": 102656, "start": 1037.0, "end": 1043.72, "text": " after many people have dropped out of the field. This is an essay called The", "tokens": [934, 867, 561, 362, 8119, 484, 295, 264, 2519, 13, 639, 307, 364, 16238, 1219, 440], "temperature": 0.0, "avg_logprob": -0.1486949920654297, "compression_ratio": 1.5182186234817814, "no_speech_prob": 1.2605780284502544e-05}, {"id": 194, "seek": 102656, "start": 1043.72, "end": 1049.32, "text": " Myth of Innate Ability in Tech by Omuju Miller. And Omuju has her PhD in CS", "tokens": [26371, 295, 34066, 473, 2847, 1140, 294, 13795, 538, 9757, 45652, 16932, 13, 400, 9757, 45652, 575, 720, 14476, 294, 9460], "temperature": 0.0, "avg_logprob": -0.1486949920654297, "compression_ratio": 1.5182186234817814, "no_speech_prob": 1.2605780284502544e-05}, {"id": 195, "seek": 102656, "start": 1049.32, "end": 1053.6, "text": " education from Berkeley. And this, I think this myth shows up a lot in", "tokens": [3309, 490, 23684, 13, 400, 341, 11, 286, 519, 341, 9474, 3110, 493, 257, 688, 294], "temperature": 0.0, "avg_logprob": -0.1486949920654297, "compression_ratio": 1.5182186234817814, "no_speech_prob": 1.2605780284502544e-05}, {"id": 196, "seek": 105360, "start": 1053.6, "end": 1058.36, "text": " mathematics as well. But it's the idea of like, oh, and this idea is completely", "tokens": [18666, 382, 731, 13, 583, 309, 311, 264, 1558, 295, 411, 11, 1954, 11, 293, 341, 1558, 307, 2584], "temperature": 0.0, "avg_logprob": -0.14367327094078064, "compression_ratio": 1.762081784386617, "no_speech_prob": 1.9830009478027932e-05}, {"id": 197, "seek": 105360, "start": 1058.36, "end": 1063.12, "text": " false and the evidence is against it, but the idea that, oh, some people, you know,", "tokens": [7908, 293, 264, 4467, 307, 1970, 309, 11, 457, 264, 1558, 300, 11, 1954, 11, 512, 561, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.14367327094078064, "compression_ratio": 1.762081784386617, "no_speech_prob": 1.9830009478027932e-05}, {"id": 198, "seek": 105360, "start": 1063.12, "end": 1067.9599999999998, "text": " innately, you know, have a brain for math and some people don't, that becomes a", "tokens": [7714, 1592, 11, 291, 458, 11, 362, 257, 3567, 337, 5221, 293, 512, 561, 500, 380, 11, 300, 3643, 257], "temperature": 0.0, "avg_logprob": -0.14367327094078064, "compression_ratio": 1.762081784386617, "no_speech_prob": 1.9830009478027932e-05}, {"id": 199, "seek": 105360, "start": 1067.9599999999998, "end": 1072.48, "text": " self-fulfilling prophecy. And I think it's particularly prevalent in the", "tokens": [2698, 12, 906, 69, 7345, 23945, 13, 400, 286, 519, 309, 311, 4098, 30652, 294, 264], "temperature": 0.0, "avg_logprob": -0.14367327094078064, "compression_ratio": 1.762081784386617, "no_speech_prob": 1.9830009478027932e-05}, {"id": 200, "seek": 105360, "start": 1072.48, "end": 1077.04, "text": " United States, this idea that some people can, yeah, their brain works that way,", "tokens": [2824, 3040, 11, 341, 1558, 300, 512, 561, 393, 11, 1338, 11, 641, 3567, 1985, 300, 636, 11], "temperature": 0.0, "avg_logprob": -0.14367327094078064, "compression_ratio": 1.762081784386617, "no_speech_prob": 1.9830009478027932e-05}, {"id": 201, "seek": 107704, "start": 1077.04, "end": 1084.36, "text": " other people's don't. And that's not the case, but it's a very harmful myth. Then", "tokens": [661, 561, 311, 500, 380, 13, 400, 300, 311, 406, 264, 1389, 11, 457, 309, 311, 257, 588, 19727, 9474, 13, 1396], "temperature": 0.0, "avg_logprob": -0.1446913746954168, "compression_ratio": 1.549800796812749, "no_speech_prob": 5.771370979346102e-06}, {"id": 202, "seek": 107704, "start": 1084.36, "end": 1090.24, "text": " it gets worse if you're a woman in that there was a study, so 90% of", "tokens": [309, 2170, 5324, 498, 291, 434, 257, 3059, 294, 300, 456, 390, 257, 2979, 11, 370, 4289, 4, 295], "temperature": 0.0, "avg_logprob": -0.1446913746954168, "compression_ratio": 1.549800796812749, "no_speech_prob": 5.771370979346102e-06}, {"id": 203, "seek": 107704, "start": 1090.24, "end": 1094.92, "text": " elementary school teachers are women in the United States. And I apologize, what", "tokens": [16429, 1395, 6023, 366, 2266, 294, 264, 2824, 3040, 13, 400, 286, 12328, 11, 437], "temperature": 0.0, "avg_logprob": -0.1446913746954168, "compression_ratio": 1.549800796812749, "no_speech_prob": 5.771370979346102e-06}, {"id": 204, "seek": 107704, "start": 1094.92, "end": 1099.8799999999999, "text": " I'm saying is very US-centric and that's what I'm kind of most familiar with the", "tokens": [286, 478, 1566, 307, 588, 2546, 12, 45300, 293, 300, 311, 437, 286, 478, 733, 295, 881, 4963, 365, 264], "temperature": 0.0, "avg_logprob": -0.1446913746954168, "compression_ratio": 1.549800796812749, "no_speech_prob": 5.771370979346102e-06}, {"id": 205, "seek": 107704, "start": 1099.8799999999999, "end": 1103.52, "text": " research for, but this is something that really varies a lot from culture to", "tokens": [2132, 337, 11, 457, 341, 307, 746, 300, 534, 21716, 257, 688, 490, 3713, 281], "temperature": 0.0, "avg_logprob": -0.1446913746954168, "compression_ratio": 1.549800796812749, "no_speech_prob": 5.771370979346102e-06}, {"id": 206, "seek": 110352, "start": 1103.52, "end": 1107.04, "text": " culture. And you will see like there's some countries where, you know, there are", "tokens": [3713, 13, 400, 291, 486, 536, 411, 456, 311, 512, 3517, 689, 11, 291, 458, 11, 456, 366], "temperature": 0.0, "avg_logprob": -0.1635280588398809, "compression_ratio": 1.61864406779661, "no_speech_prob": 1.0781483979371842e-05}, {"id": 207, "seek": 110352, "start": 1107.04, "end": 1114.36, "text": " a ton of girls on the Math Olympiad teams at all levels. But yeah, this is an area", "tokens": [257, 2952, 295, 4519, 322, 264, 15776, 10395, 38069, 5491, 412, 439, 4358, 13, 583, 1338, 11, 341, 307, 364, 1859], "temperature": 0.0, "avg_logprob": -0.1635280588398809, "compression_ratio": 1.61864406779661, "no_speech_prob": 1.0781483979371842e-05}, {"id": 208, "seek": 110352, "start": 1114.36, "end": 1119.6, "text": " where the US is not strong. But so most elementary school teachers are women", "tokens": [689, 264, 2546, 307, 406, 2068, 13, 583, 370, 881, 16429, 1395, 6023, 366, 2266], "temperature": 0.0, "avg_logprob": -0.1635280588398809, "compression_ratio": 1.61864406779661, "no_speech_prob": 1.0781483979371842e-05}, {"id": 209, "seek": 110352, "start": 1119.6, "end": 1126.8799999999999, "text": " and in college the major with the highest level of math anxiety is", "tokens": [293, 294, 3859, 264, 2563, 365, 264, 6343, 1496, 295, 5221, 9119, 307], "temperature": 0.0, "avg_logprob": -0.1635280588398809, "compression_ratio": 1.61864406779661, "no_speech_prob": 1.0781483979371842e-05}, {"id": 210, "seek": 110352, "start": 1126.8799999999999, "end": 1131.16, "text": " elementary education. And then this often gets transmitted to kids kind of", "tokens": [16429, 3309, 13, 400, 550, 341, 2049, 2170, 25355, 281, 2301, 733, 295], "temperature": 0.0, "avg_logprob": -0.1635280588398809, "compression_ratio": 1.61864406779661, "no_speech_prob": 1.0781483979371842e-05}, {"id": 211, "seek": 113116, "start": 1131.16, "end": 1136.48, "text": " subconsciously. And there's a study that found, and this was published in the", "tokens": [27389, 356, 13, 400, 456, 311, 257, 2979, 300, 1352, 11, 293, 341, 390, 6572, 294, 264], "temperature": 0.0, "avg_logprob": -0.1470935127951882, "compression_ratio": 1.6347517730496455, "no_speech_prob": 1.7229966033482924e-05}, {"id": 212, "seek": 113116, "start": 1136.48, "end": 1139.96, "text": " Proceedings of the National Academy of Sciences, which is very prestigious", "tokens": [1705, 4357, 1109, 295, 264, 4862, 11735, 295, 21108, 11, 597, 307, 588, 33510], "temperature": 0.0, "avg_logprob": -0.1470935127951882, "compression_ratio": 1.6347517730496455, "no_speech_prob": 1.7229966033482924e-05}, {"id": 213, "seek": 113116, "start": 1139.96, "end": 1146.28, "text": " kind of top journal, that girl children pick up on that anxiety and it impacts", "tokens": [733, 295, 1192, 6708, 11, 300, 2013, 2227, 1888, 493, 322, 300, 9119, 293, 309, 11606], "temperature": 0.0, "avg_logprob": -0.1470935127951882, "compression_ratio": 1.6347517730496455, "no_speech_prob": 1.7229966033482924e-05}, {"id": 214, "seek": 113116, "start": 1146.28, "end": 1152.44, "text": " how they see the gender ability around math, whereas boys students", "tokens": [577, 436, 536, 264, 7898, 3485, 926, 5221, 11, 9735, 6347, 1731], "temperature": 0.0, "avg_logprob": -0.1470935127951882, "compression_ratio": 1.6347517730496455, "no_speech_prob": 1.7229966033482924e-05}, {"id": 215, "seek": 113116, "start": 1152.44, "end": 1156.3200000000002, "text": " don't pick up on it from women teachers. So it's kind of harmful to girls but not", "tokens": [500, 380, 1888, 493, 322, 309, 490, 2266, 6023, 13, 407, 309, 311, 733, 295, 19727, 281, 4519, 457, 406], "temperature": 0.0, "avg_logprob": -0.1470935127951882, "compression_ratio": 1.6347517730496455, "no_speech_prob": 1.7229966033482924e-05}, {"id": 216, "seek": 113116, "start": 1156.3200000000002, "end": 1160.8000000000002, "text": " to boys. And so this is something that may be impacting people, or definitely is", "tokens": [281, 6347, 13, 400, 370, 341, 307, 746, 300, 815, 312, 29963, 561, 11, 420, 2138, 307], "temperature": 0.0, "avg_logprob": -0.1470935127951882, "compression_ratio": 1.6347517730496455, "no_speech_prob": 1.7229966033482924e-05}, {"id": 217, "seek": 116080, "start": 1160.8, "end": 1169.0, "text": " impacting people. And then another issue is in the US there are a lot of", "tokens": [29963, 561, 13, 400, 550, 1071, 2734, 307, 294, 264, 2546, 456, 366, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.16371608310275607, "compression_ratio": 1.6952789699570816, "no_speech_prob": 2.4679382477188483e-05}, {"id": 218, "seek": 116080, "start": 1169.0, "end": 1172.8, "text": " stereotypes and myths about the ability of people of color. And so particularly", "tokens": [30853, 293, 28205, 466, 264, 3485, 295, 561, 295, 2017, 13, 400, 370, 4098], "temperature": 0.0, "avg_logprob": -0.16371608310275607, "compression_ratio": 1.6952789699570816, "no_speech_prob": 2.4679382477188483e-05}, {"id": 219, "seek": 116080, "start": 1172.8, "end": 1178.24, "text": " if you're black or Latina or indigenous you may have encountered kind of negative", "tokens": [498, 291, 434, 2211, 420, 7354, 1426, 420, 15511, 291, 815, 362, 20381, 733, 295, 3671], "temperature": 0.0, "avg_logprob": -0.16371608310275607, "compression_ratio": 1.6952789699570816, "no_speech_prob": 2.4679382477188483e-05}, {"id": 220, "seek": 116080, "start": 1178.24, "end": 1183.0, "text": " stereotypes or perceptions about your math ability. And I just wanted to share", "tokens": [30853, 420, 35258, 466, 428, 5221, 3485, 13, 400, 286, 445, 1415, 281, 2073], "temperature": 0.0, "avg_logprob": -0.16371608310275607, "compression_ratio": 1.6952789699570816, "no_speech_prob": 2.4679382477188483e-05}, {"id": 221, "seek": 116080, "start": 1183.0, "end": 1187.6399999999999, "text": " all these of kind of just all the forces that might be kind of conspiring against", "tokens": [439, 613, 295, 733, 295, 445, 439, 264, 5874, 300, 1062, 312, 733, 295, 1014, 79, 5057, 1970], "temperature": 0.0, "avg_logprob": -0.16371608310275607, "compression_ratio": 1.6952789699570816, "no_speech_prob": 2.4679382477188483e-05}, {"id": 222, "seek": 118764, "start": 1187.64, "end": 1192.2, "text": " you loving math as you could and that have nothing to do with your actual", "tokens": [291, 9344, 5221, 382, 291, 727, 293, 300, 362, 1825, 281, 360, 365, 428, 3539], "temperature": 0.0, "avg_logprob": -0.1619606121726658, "compression_ratio": 1.6, "no_speech_prob": 1.3630035937239882e-05}, {"id": 223, "seek": 118764, "start": 1192.2, "end": 1201.8000000000002, "text": " ability or aptitude. So there's this guy on Twitter that said", "tokens": [3485, 420, 29427, 4377, 13, 407, 456, 311, 341, 2146, 322, 5794, 300, 848], "temperature": 0.0, "avg_logprob": -0.1619606121726658, "compression_ratio": 1.6, "no_speech_prob": 1.3630035937239882e-05}, {"id": 224, "seek": 118764, "start": 1201.8000000000002, "end": 1207.2, "text": " you to do machine learning you have to love math as a teenager, which I think is", "tokens": [291, 281, 360, 3479, 2539, 291, 362, 281, 959, 5221, 382, 257, 21440, 11, 597, 286, 519, 307], "temperature": 0.0, "avg_logprob": -0.1619606121726658, "compression_ratio": 1.6, "no_speech_prob": 1.3630035937239882e-05}, {"id": 225, "seek": 118764, "start": 1207.2, "end": 1211.0, "text": " false. And so I responded many kids are turned off to math because of poor", "tokens": [7908, 13, 400, 370, 286, 15806, 867, 2301, 366, 3574, 766, 281, 5221, 570, 295, 4716], "temperature": 0.0, "avg_logprob": -0.1619606121726658, "compression_ratio": 1.6, "no_speech_prob": 1.3630035937239882e-05}, {"id": 226, "seek": 118764, "start": 1211.0, "end": 1215.48, "text": " teaching methods, lack of resources, lack of support, doesn't mean you can't", "tokens": [4571, 7150, 11, 5011, 295, 3593, 11, 5011, 295, 1406, 11, 1177, 380, 914, 291, 393, 380], "temperature": 0.0, "avg_logprob": -0.1619606121726658, "compression_ratio": 1.6, "no_speech_prob": 1.3630035937239882e-05}, {"id": 227, "seek": 121548, "start": 1215.48, "end": 1219.24, "text": " learn machine learning later. And then all these people responded with really", "tokens": [1466, 3479, 2539, 1780, 13, 400, 550, 439, 613, 561, 15806, 365, 534], "temperature": 0.0, "avg_logprob": -0.1263541640522324, "compression_ratio": 1.7343173431734318, "no_speech_prob": 3.822264989139512e-05}, {"id": 228, "seek": 121548, "start": 1219.24, "end": 1224.1200000000001, "text": " inspiring stories and so I just wanted to share a few of them. So one is data", "tokens": [15883, 3676, 293, 370, 286, 445, 1415, 281, 2073, 257, 1326, 295, 552, 13, 407, 472, 307, 1412], "temperature": 0.0, "avg_logprob": -0.1263541640522324, "compression_ratio": 1.7343173431734318, "no_speech_prob": 3.822264989139512e-05}, {"id": 229, "seek": 121548, "start": 1224.1200000000001, "end": 1228.48, "text": " science Renee who is a data scientist who has a large Twitter following and a", "tokens": [3497, 47790, 567, 307, 257, 1412, 12662, 567, 575, 257, 2416, 5794, 3480, 293, 257], "temperature": 0.0, "avg_logprob": -0.1263541640522324, "compression_ratio": 1.7343173431734318, "no_speech_prob": 3.822264989139512e-05}, {"id": 230, "seek": 121548, "start": 1228.48, "end": 1233.52, "text": " data science podcast. She says that she struggled with certain teachers and with", "tokens": [1412, 3497, 7367, 13, 1240, 1619, 300, 750, 19023, 365, 1629, 6023, 293, 365], "temperature": 0.0, "avg_logprob": -0.1263541640522324, "compression_ratio": 1.7343173431734318, "no_speech_prob": 3.822264989139512e-05}, {"id": 231, "seek": 121548, "start": 1233.52, "end": 1239.16, "text": " ADHD and that she didn't start she didn't take advanced math until she was", "tokens": [38680, 293, 300, 750, 994, 380, 722, 750, 994, 380, 747, 7339, 5221, 1826, 750, 390], "temperature": 0.0, "avg_logprob": -0.1263541640522324, "compression_ratio": 1.7343173431734318, "no_speech_prob": 3.822264989139512e-05}, {"id": 232, "seek": 121548, "start": 1239.16, "end": 1245.2, "text": " over 30 years old and she became a data scientist at 34. Stephanie said that she", "tokens": [670, 2217, 924, 1331, 293, 750, 3062, 257, 1412, 12662, 412, 12790, 13, 18634, 848, 300, 750], "temperature": 0.0, "avg_logprob": -0.1263541640522324, "compression_ratio": 1.7343173431734318, "no_speech_prob": 3.822264989139512e-05}, {"id": 233, "seek": 124520, "start": 1245.2, "end": 1249.16, "text": " didn't love math until age 36 when she returned to school for a biology degree", "tokens": [994, 380, 959, 5221, 1826, 3205, 8652, 562, 750, 8752, 281, 1395, 337, 257, 14956, 4314], "temperature": 0.0, "avg_logprob": -0.12777154080502623, "compression_ratio": 1.6040955631399318, "no_speech_prob": 1.1299380275886506e-05}, {"id": 234, "seek": 124520, "start": 1249.16, "end": 1254.32, "text": " and now she's becoming a data scientist. And then my favorite was Kimberly said", "tokens": [293, 586, 750, 311, 5617, 257, 1412, 12662, 13, 400, 550, 452, 2954, 390, 39804, 848], "temperature": 0.0, "avg_logprob": -0.12777154080502623, "compression_ratio": 1.6040955631399318, "no_speech_prob": 1.1299380275886506e-05}, {"id": 235, "seek": 124520, "start": 1254.32, "end": 1257.88, "text": " I'm learning machine learning as a 50 year old with straight A's and a PhD", "tokens": [286, 478, 2539, 3479, 2539, 382, 257, 2625, 1064, 1331, 365, 2997, 316, 311, 293, 257, 14476], "temperature": 0.0, "avg_logprob": -0.12777154080502623, "compression_ratio": 1.6040955631399318, "no_speech_prob": 1.1299380275886506e-05}, {"id": 236, "seek": 124520, "start": 1257.88, "end": 1262.96, "text": " program in biomedical informatics and health data science age is just a number.", "tokens": [1461, 294, 49775, 1356, 30292, 293, 1585, 1412, 3497, 3205, 307, 445, 257, 1230, 13], "temperature": 0.0, "avg_logprob": -0.12777154080502623, "compression_ratio": 1.6040955631399318, "no_speech_prob": 1.1299380275886506e-05}, {"id": 237, "seek": 124520, "start": 1262.96, "end": 1267.64, "text": " So I thought that was really inspiring and just yeah for any of you that did", "tokens": [407, 286, 1194, 300, 390, 534, 15883, 293, 445, 1338, 337, 604, 295, 291, 300, 630], "temperature": 0.0, "avg_logprob": -0.12777154080502623, "compression_ratio": 1.6040955631399318, "no_speech_prob": 1.1299380275886506e-05}, {"id": 238, "seek": 124520, "start": 1267.64, "end": 1274.8, "text": " feel a little anxious about math I hope this is inspiring. Oh and then one more", "tokens": [841, 257, 707, 15166, 466, 5221, 286, 1454, 341, 307, 15883, 13, 876, 293, 550, 472, 544], "temperature": 0.0, "avg_logprob": -0.12777154080502623, "compression_ratio": 1.6040955631399318, "no_speech_prob": 1.1299380275886506e-05}, {"id": 239, "seek": 127480, "start": 1274.8, "end": 1281.52, "text": " this came out just a week or two ago at Cathy O'Neill. Weapons of math", "tokens": [341, 1361, 484, 445, 257, 1243, 420, 732, 2057, 412, 39799, 422, 6, 15496, 373, 13, 492, 48071, 295, 5221], "temperature": 0.0, "avg_logprob": -0.19733063547234786, "compression_ratio": 1.6391304347826088, "no_speech_prob": 2.6683301257435232e-05}, {"id": 240, "seek": 127480, "start": 1281.52, "end": 1285.24, "text": " destruction is very good yeah if you're I'll mention it a few times if you're", "tokens": [13563, 307, 588, 665, 1338, 498, 291, 434, 286, 603, 2152, 309, 257, 1326, 1413, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.19733063547234786, "compression_ratio": 1.6391304347826088, "no_speech_prob": 2.6683301257435232e-05}, {"id": 241, "seek": 127480, "start": 1285.24, "end": 1289.8799999999999, "text": " interested in bias and algorithms and Cathy has a book called weapons of math", "tokens": [3102, 294, 12577, 293, 14642, 293, 39799, 575, 257, 1446, 1219, 7278, 295, 5221], "temperature": 0.0, "avg_logprob": -0.19733063547234786, "compression_ratio": 1.6391304347826088, "no_speech_prob": 2.6683301257435232e-05}, {"id": 242, "seek": 127480, "start": 1289.8799999999999, "end": 1297.32, "text": " destruction she also blogs at mathbabe.org and she she has her PhD in math", "tokens": [13563, 750, 611, 31038, 412, 5221, 65, 4488, 13, 4646, 293, 750, 750, 575, 720, 14476, 294, 5221], "temperature": 0.0, "avg_logprob": -0.19733063547234786, "compression_ratio": 1.6391304347826088, "no_speech_prob": 2.6683301257435232e-05}, {"id": 243, "seek": 127480, "start": 1297.32, "end": 1302.76, "text": " from Harvard but she wrote this article in Bloomberg that a mathematician's", "tokens": [490, 13378, 457, 750, 4114, 341, 7222, 294, 40363, 300, 257, 48281, 311], "temperature": 0.0, "avg_logprob": -0.19733063547234786, "compression_ratio": 1.6391304347826088, "no_speech_prob": 2.6683301257435232e-05}, {"id": 244, "seek": 130276, "start": 1302.76, "end": 1307.8799999999999, "text": " secret we're not all geniuses and she says you don't have to be a genius to", "tokens": [4054, 321, 434, 406, 439, 14017, 279, 293, 750, 1619, 291, 500, 380, 362, 281, 312, 257, 14017, 281], "temperature": 0.0, "avg_logprob": -0.08861116062511097, "compression_ratio": 1.806201550387597, "no_speech_prob": 2.668573142727837e-05}, {"id": 245, "seek": 130276, "start": 1307.8799999999999, "end": 1312.04, "text": " become a mathematician if you find the statement at all surprising you're an", "tokens": [1813, 257, 48281, 498, 291, 915, 264, 5629, 412, 439, 8830, 291, 434, 364], "temperature": 0.0, "avg_logprob": -0.08861116062511097, "compression_ratio": 1.806201550387597, "no_speech_prob": 2.668573142727837e-05}, {"id": 246, "seek": 130276, "start": 1312.04, "end": 1315.96, "text": " example of what's wrong with the way our society identifies encourages and", "tokens": [1365, 295, 437, 311, 2085, 365, 264, 636, 527, 4086, 34597, 28071, 293], "temperature": 0.0, "avg_logprob": -0.08861116062511097, "compression_ratio": 1.806201550387597, "no_speech_prob": 2.668573142727837e-05}, {"id": 247, "seek": 130276, "start": 1315.96, "end": 1322.28, "text": " rewards talent. So I definitely agree with that and then she also says to even", "tokens": [17203, 8301, 13, 407, 286, 2138, 3986, 365, 300, 293, 550, 750, 611, 1619, 281, 754], "temperature": 0.0, "avg_logprob": -0.08861116062511097, "compression_ratio": 1.806201550387597, "no_speech_prob": 2.668573142727837e-05}, {"id": 248, "seek": 130276, "start": 1322.28, "end": 1326.36, "text": " imagine becoming a mathematician I had to ignore the question of whether I was", "tokens": [3811, 5617, 257, 48281, 286, 632, 281, 11200, 264, 1168, 295, 1968, 286, 390], "temperature": 0.0, "avg_logprob": -0.08861116062511097, "compression_ratio": 1.806201550387597, "no_speech_prob": 2.668573142727837e-05}, {"id": 249, "seek": 130276, "start": 1326.36, "end": 1330.4, "text": " a genius or whether I would need to be a genius had I been poked and prodded and", "tokens": [257, 14017, 420, 1968, 286, 576, 643, 281, 312, 257, 14017, 632, 286, 668, 280, 9511, 293, 15792, 9207, 293], "temperature": 0.0, "avg_logprob": -0.08861116062511097, "compression_ratio": 1.806201550387597, "no_speech_prob": 2.668573142727837e-05}, {"id": 250, "seek": 133040, "start": 1330.4, "end": 1337.64, "text": " measured to see how exceptional I was I probably would have lost the nerve. But", "tokens": [12690, 281, 536, 577, 19279, 286, 390, 286, 1391, 576, 362, 2731, 264, 16355, 13, 583], "temperature": 0.0, "avg_logprob": -0.11129058373940957, "compression_ratio": 1.5431472081218274, "no_speech_prob": 1.3419122296909336e-05}, {"id": 251, "seek": 133040, "start": 1337.64, "end": 1340.8400000000001, "text": " yeah so just some more confirmation and I just wanted to kind of put all that", "tokens": [1338, 370, 445, 512, 544, 21871, 293, 286, 445, 1415, 281, 733, 295, 829, 439, 300], "temperature": 0.0, "avg_logprob": -0.11129058373940957, "compression_ratio": 1.5431472081218274, "no_speech_prob": 1.3419122296909336e-05}, {"id": 252, "seek": 133040, "start": 1340.8400000000001, "end": 1349.3200000000002, "text": " out there before I get into get into numbers. So going back to our topic of", "tokens": [484, 456, 949, 286, 483, 666, 483, 666, 3547, 13, 407, 516, 646, 281, 527, 4829, 295], "temperature": 0.0, "avg_logprob": -0.11129058373940957, "compression_ratio": 1.5431472081218274, "no_speech_prob": 1.3419122296909336e-05}, {"id": 253, "seek": 133040, "start": 1349.3200000000002, "end": 1355.44, "text": " word embeddings we need a way to represent words as numbers and so one", "tokens": [1349, 12240, 29432, 321, 643, 257, 636, 281, 2906, 2283, 382, 3547, 293, 370, 472], "temperature": 0.0, "avg_logprob": -0.11129058373940957, "compression_ratio": 1.5431472081218274, "no_speech_prob": 1.3419122296909336e-05}, {"id": 254, "seek": 135544, "start": 1355.44, "end": 1360.68, "text": " approach would be to go through the dictionary and number words so art of", "tokens": [3109, 576, 312, 281, 352, 807, 264, 25890, 293, 1230, 2283, 370, 1523, 295], "temperature": 0.0, "avg_logprob": -0.1610499198990639, "compression_ratio": 1.6307053941908713, "no_speech_prob": 1.406305545970099e-05}, {"id": 255, "seek": 135544, "start": 1360.68, "end": 1368.04, "text": " art is one, apple's two, art is three and so on. Does anyone see what a shortcoming", "tokens": [1523, 307, 472, 11, 10606, 311, 732, 11, 1523, 307, 1045, 293, 370, 322, 13, 4402, 2878, 536, 437, 257, 2099, 6590], "temperature": 0.0, "avg_logprob": -0.1610499198990639, "compression_ratio": 1.6307053941908713, "no_speech_prob": 1.406305545970099e-05}, {"id": 256, "seek": 135544, "start": 1368.04, "end": 1376.64, "text": " of this approach might be? So a big one is this doesn't really give us any", "tokens": [295, 341, 3109, 1062, 312, 30, 407, 257, 955, 472, 307, 341, 1177, 380, 534, 976, 505, 604], "temperature": 0.0, "avg_logprob": -0.1610499198990639, "compression_ratio": 1.6307053941908713, "no_speech_prob": 1.406305545970099e-05}, {"id": 257, "seek": 135544, "start": 1376.64, "end": 1380.48, "text": " notion of what it means for words to be similar so you see cat and catastrophe", "tokens": [10710, 295, 437, 309, 1355, 337, 2283, 281, 312, 2531, 370, 291, 536, 3857, 293, 36043], "temperature": 0.0, "avg_logprob": -0.1610499198990639, "compression_ratio": 1.6307053941908713, "no_speech_prob": 1.406305545970099e-05}, {"id": 258, "seek": 135544, "start": 1380.48, "end": 1384.3600000000001, "text": " are right next to each other but there's not you know some deep deep link between", "tokens": [366, 558, 958, 281, 1184, 661, 457, 456, 311, 406, 291, 458, 512, 2452, 2452, 2113, 1296], "temperature": 0.0, "avg_logprob": -0.1610499198990639, "compression_ratio": 1.6307053941908713, "no_speech_prob": 1.406305545970099e-05}, {"id": 259, "seek": 138436, "start": 1384.36, "end": 1388.6399999999999, "text": " cat and catastrophe and that doesn't capture anything of like how is cat", "tokens": [3857, 293, 36043, 293, 300, 1177, 380, 7983, 1340, 295, 411, 577, 307, 3857], "temperature": 0.0, "avg_logprob": -0.13834288845891538, "compression_ratio": 1.7363636363636363, "no_speech_prob": 9.222992048307788e-06}, {"id": 260, "seek": 138436, "start": 1388.6399999999999, "end": 1396.1799999999998, "text": " related to kitten if we just have this numbered list. So a better approach is to", "tokens": [4077, 281, 39696, 498, 321, 445, 362, 341, 40936, 1329, 13, 407, 257, 1101, 3109, 307, 281], "temperature": 0.0, "avg_logprob": -0.13834288845891538, "compression_ratio": 1.7363636363636363, "no_speech_prob": 9.222992048307788e-06}, {"id": 261, "seek": 138436, "start": 1396.1799999999998, "end": 1401.8, "text": " treat words as vectors so vectors just a list of numbers and here I've just made", "tokens": [2387, 2283, 382, 18875, 370, 18875, 445, 257, 1329, 295, 3547, 293, 510, 286, 600, 445, 1027], "temperature": 0.0, "avg_logprob": -0.13834288845891538, "compression_ratio": 1.7363636363636363, "no_speech_prob": 9.222992048307788e-06}, {"id": 262, "seek": 138436, "start": 1401.8, "end": 1409.1399999999999, "text": " these up that we're gonna have three three numbers in each vector in reality", "tokens": [613, 493, 300, 321, 434, 799, 362, 1045, 1045, 3547, 294, 1184, 8062, 294, 4103], "temperature": 0.0, "avg_logprob": -0.13834288845891538, "compression_ratio": 1.7363636363636363, "no_speech_prob": 9.222992048307788e-06}, {"id": 263, "seek": 138436, "start": 1409.1399999999999, "end": 1412.8799999999999, "text": " we're gonna have a lot more but we could say you know puppy's gonna be", "tokens": [321, 434, 799, 362, 257, 688, 544, 457, 321, 727, 584, 291, 458, 18196, 311, 799, 312], "temperature": 0.0, "avg_logprob": -0.13834288845891538, "compression_ratio": 1.7363636363636363, "no_speech_prob": 9.222992048307788e-06}, {"id": 264, "seek": 141288, "start": 1412.88, "end": 1423.44, "text": " represented as 0.9 comma 1.0 comma 0.0 and dog is 1.0 comma 0.2 comma 0.0 and", "tokens": [10379, 382, 1958, 13, 24, 22117, 502, 13, 15, 22117, 1958, 13, 15, 293, 3000, 307, 502, 13, 15, 22117, 1958, 13, 17, 22117, 1958, 13, 15, 293], "temperature": 0.0, "avg_logprob": -0.14601316235282205, "compression_ratio": 1.6145833333333333, "no_speech_prob": 9.368514838570263e-06}, {"id": 265, "seek": 141288, "start": 1423.44, "end": 1429.5600000000002, "text": " so looking at this what might you say about the first first number in each", "tokens": [370, 1237, 412, 341, 437, 1062, 291, 584, 466, 264, 700, 700, 1230, 294, 1184], "temperature": 0.0, "avg_logprob": -0.14601316235282205, "compression_ratio": 1.6145833333333333, "no_speech_prob": 9.368514838570263e-06}, {"id": 266, "seek": 141288, "start": 1429.5600000000002, "end": 1435.96, "text": " vector? See I heard someone shout out canine so this is something that puppies", "tokens": [8062, 30, 3008, 286, 2198, 1580, 8043, 484, 393, 533, 370, 341, 307, 746, 300, 33734], "temperature": 0.0, "avg_logprob": -0.14601316235282205, "compression_ratio": 1.6145833333333333, "no_speech_prob": 9.368514838570263e-06}, {"id": 267, "seek": 141288, "start": 1435.96, "end": 1440.4, "text": " and dogs have a lot of and that kittens and cats don't have. I'm gonna capture", "tokens": [293, 7197, 362, 257, 688, 295, 293, 300, 47363, 293, 11111, 500, 380, 362, 13, 286, 478, 799, 7983], "temperature": 0.0, "avg_logprob": -0.14601316235282205, "compression_ratio": 1.6145833333333333, "no_speech_prob": 9.368514838570263e-06}, {"id": 268, "seek": 144040, "start": 1440.4, "end": 1447.3600000000001, "text": " there's something about dogs. How about for the the second number? Age, youth, yeah", "tokens": [456, 311, 746, 466, 7197, 13, 1012, 466, 337, 264, 264, 1150, 1230, 30, 16280, 11, 7503, 11, 1338], "temperature": 0.0, "avg_logprob": -0.10993693799388651, "compression_ratio": 1.7217391304347827, "no_speech_prob": 4.029321189591428e-06}, {"id": 269, "seek": 144040, "start": 1447.3600000000001, "end": 1452.3200000000002, "text": " so we see that kittens and puppies are both very young whereas dogs and cats", "tokens": [370, 321, 536, 300, 47363, 293, 33734, 366, 1293, 588, 2037, 9735, 7197, 293, 11111], "temperature": 0.0, "avg_logprob": -0.10993693799388651, "compression_ratio": 1.7217391304347827, "no_speech_prob": 4.029321189591428e-06}, {"id": 270, "seek": 144040, "start": 1452.3200000000002, "end": 1458.4, "text": " are not so much and then the third one is something cat related or feline and", "tokens": [366, 406, 370, 709, 293, 550, 264, 2636, 472, 307, 746, 3857, 4077, 420, 283, 5440, 293], "temperature": 0.0, "avg_logprob": -0.10993693799388651, "compression_ratio": 1.7217391304347827, "no_speech_prob": 4.029321189591428e-06}, {"id": 271, "seek": 144040, "start": 1458.4, "end": 1461.64, "text": " so this is neat because it's given us a way to capture some meaning and to say", "tokens": [370, 341, 307, 10654, 570, 309, 311, 2212, 505, 257, 636, 281, 7983, 512, 3620, 293, 281, 584], "temperature": 0.0, "avg_logprob": -0.10993693799388651, "compression_ratio": 1.7217391304347827, "no_speech_prob": 4.029321189591428e-06}, {"id": 272, "seek": 144040, "start": 1461.64, "end": 1465.76, "text": " you know cat and kitten are similar in this way and then there's also you know", "tokens": [291, 458, 3857, 293, 39696, 366, 2531, 294, 341, 636, 293, 550, 456, 311, 611, 291, 458], "temperature": 0.0, "avg_logprob": -0.10993693799388651, "compression_ratio": 1.7217391304347827, "no_speech_prob": 4.029321189591428e-06}, {"id": 273, "seek": 146576, "start": 1465.76, "end": 1471.4, "text": " the similarity between kittens and puppies and so these are these are word", "tokens": [264, 32194, 1296, 47363, 293, 33734, 293, 370, 613, 366, 613, 366, 1349], "temperature": 0.0, "avg_logprob": -0.08992389429395443, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.138017518722336e-06}, {"id": 274, "seek": 146576, "start": 1471.4, "end": 1478.8, "text": " embeddings and so I I made up these numbers but in reality it would be", "tokens": [12240, 29432, 293, 370, 286, 286, 1027, 493, 613, 3547, 457, 294, 4103, 309, 576, 312], "temperature": 0.0, "avg_logprob": -0.08992389429395443, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.138017518722336e-06}, {"id": 275, "seek": 146576, "start": 1478.8, "end": 1483.28, "text": " virtually impossible for a human to do this well just because you know there's", "tokens": [14103, 6243, 337, 257, 1952, 281, 360, 341, 731, 445, 570, 291, 458, 456, 311], "temperature": 0.0, "avg_logprob": -0.08992389429395443, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.138017518722336e-06}, {"id": 276, "seek": 146576, "start": 1483.28, "end": 1487.6, "text": " so many dimensions of meaning and to try to capture that would be really tough", "tokens": [370, 867, 12819, 295, 3620, 293, 281, 853, 281, 7983, 300, 576, 312, 534, 4930], "temperature": 0.0, "avg_logprob": -0.08992389429395443, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.138017518722336e-06}, {"id": 277, "seek": 146576, "start": 1487.6, "end": 1492.0, "text": " and so this is something that we want to train computers to come up with for us", "tokens": [293, 370, 341, 307, 746, 300, 321, 528, 281, 3847, 10807, 281, 808, 493, 365, 337, 505], "temperature": 0.0, "avg_logprob": -0.08992389429395443, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.138017518722336e-06}, {"id": 278, "seek": 146576, "start": 1492.0, "end": 1495.74, "text": " so I wouldn't actually make up the word embedding we're gonna get the computer", "tokens": [370, 286, 2759, 380, 767, 652, 493, 264, 1349, 12240, 3584, 321, 434, 799, 483, 264, 3820], "temperature": 0.0, "avg_logprob": -0.08992389429395443, "compression_ratio": 1.8333333333333333, "no_speech_prob": 3.138017518722336e-06}, {"id": 279, "seek": 149574, "start": 1495.74, "end": 1504.08, "text": " to do that. So what is Word2Vec? Word2Vec is a library of word embeddings", "tokens": [281, 360, 300, 13, 407, 437, 307, 8725, 17, 53, 3045, 30, 8725, 17, 53, 3045, 307, 257, 6405, 295, 1349, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.11379146575927734, "compression_ratio": 1.5520833333333333, "no_speech_prob": 5.014371708966792e-06}, {"id": 280, "seek": 149574, "start": 1504.08, "end": 1509.52, "text": " released by Google. It's not deep learning although it can and often is", "tokens": [4736, 538, 3329, 13, 467, 311, 406, 2452, 2539, 4878, 309, 393, 293, 2049, 307], "temperature": 0.0, "avg_logprob": -0.11379146575927734, "compression_ratio": 1.5520833333333333, "no_speech_prob": 5.014371708966792e-06}, {"id": 281, "seek": 149574, "start": 1509.52, "end": 1514.08, "text": " used in deep learning including in the examples I showed before or mentioned", "tokens": [1143, 294, 2452, 2539, 3009, 294, 264, 5110, 286, 4712, 949, 420, 2835], "temperature": 0.0, "avg_logprob": -0.11379146575927734, "compression_ratio": 1.5520833333333333, "no_speech_prob": 5.014371708966792e-06}, {"id": 282, "seek": 149574, "start": 1514.08, "end": 1522.24, "text": " before of speech translation, image captioning, and the email generation so", "tokens": [949, 295, 6218, 12853, 11, 3256, 31974, 278, 11, 293, 264, 3796, 5125, 370], "temperature": 0.0, "avg_logprob": -0.11379146575927734, "compression_ratio": 1.5520833333333333, "no_speech_prob": 5.014371708966792e-06}, {"id": 283, "seek": 152224, "start": 1522.24, "end": 1529.0, "text": " those are all using kind of deep learning and word embeddings. Word2Vec is", "tokens": [729, 366, 439, 1228, 733, 295, 2452, 2539, 293, 1349, 12240, 29432, 13, 8725, 17, 53, 3045, 307], "temperature": 0.0, "avg_logprob": -0.11107531372381717, "compression_ratio": 1.6637931034482758, "no_speech_prob": 6.9619222813344095e-06}, {"id": 284, "seek": 152224, "start": 1529.0, "end": 1532.56, "text": " not an algorithm although it used specific algorithms to train it and come", "tokens": [406, 364, 9284, 4878, 309, 1143, 2685, 14642, 281, 3847, 309, 293, 808], "temperature": 0.0, "avg_logprob": -0.11107531372381717, "compression_ratio": 1.6637931034482758, "no_speech_prob": 6.9619222813344095e-06}, {"id": 285, "seek": 152224, "start": 1532.56, "end": 1539.24, "text": " up with the embeddings and then GloVe is a very similar set of embeddings that", "tokens": [493, 365, 264, 12240, 29432, 293, 550, 10786, 53, 68, 307, 257, 588, 2531, 992, 295, 12240, 29432, 300], "temperature": 0.0, "avg_logprob": -0.11107531372381717, "compression_ratio": 1.6637931034482758, "no_speech_prob": 6.9619222813344095e-06}, {"id": 286, "seek": 152224, "start": 1539.24, "end": 1544.1200000000001, "text": " was created at Stanford and I'm gonna be using GloVe today but you can think of", "tokens": [390, 2942, 412, 20374, 293, 286, 478, 799, 312, 1228, 10786, 53, 68, 965, 457, 291, 393, 519, 295], "temperature": 0.0, "avg_logprob": -0.11107531372381717, "compression_ratio": 1.6637931034482758, "no_speech_prob": 6.9619222813344095e-06}, {"id": 287, "seek": 152224, "start": 1544.1200000000001, "end": 1549.0, "text": " them as kind of two versions of the same thing they're both both sets of word", "tokens": [552, 382, 733, 295, 732, 9606, 295, 264, 912, 551, 436, 434, 1293, 1293, 6352, 295, 1349], "temperature": 0.0, "avg_logprob": -0.11107531372381717, "compression_ratio": 1.6637931034482758, "no_speech_prob": 6.9619222813344095e-06}, {"id": 288, "seek": 154900, "start": 1549.0, "end": 1562.4, "text": " embeddings. So why it's useful? So training Word2Vec takes a lot of data", "tokens": [12240, 29432, 13, 407, 983, 309, 311, 4420, 30, 407, 3097, 8725, 17, 53, 3045, 2516, 257, 688, 295, 1412], "temperature": 0.0, "avg_logprob": -0.15080342610677083, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.5007603855774505e-06}, {"id": 289, "seek": 154900, "start": 1562.4, "end": 1566.92, "text": " and that data may not even all be publicly available so in the case of", "tokens": [293, 300, 1412, 815, 406, 754, 439, 312, 14843, 2435, 370, 294, 264, 1389, 295], "temperature": 0.0, "avg_logprob": -0.15080342610677083, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.5007603855774505e-06}, {"id": 290, "seek": 154900, "start": 1566.92, "end": 1571.84, "text": " Google they have a lot of data particularly with their books that isn't", "tokens": [3329, 436, 362, 257, 688, 295, 1412, 4098, 365, 641, 3642, 300, 1943, 380], "temperature": 0.0, "avg_logprob": -0.15080342610677083, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.5007603855774505e-06}, {"id": 291, "seek": 154900, "start": 1571.84, "end": 1575.76, "text": " publicly available that they used in training Word2Vec. It can be really", "tokens": [14843, 2435, 300, 436, 1143, 294, 3097, 8725, 17, 53, 3045, 13, 467, 393, 312, 534], "temperature": 0.0, "avg_logprob": -0.15080342610677083, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.5007603855774505e-06}, {"id": 292, "seek": 157576, "start": 1575.76, "end": 1580.8, "text": " time-consuming and slow and it can take a lot of computational power and so", "tokens": [565, 12, 21190, 24919, 293, 2964, 293, 309, 393, 747, 257, 688, 295, 28270, 1347, 293, 370], "temperature": 0.0, "avg_logprob": -0.10393420686113074, "compression_ratio": 1.7117117117117118, "no_speech_prob": 1.0952157936117146e-05}, {"id": 293, "seek": 157576, "start": 1580.8, "end": 1585.16, "text": " Google has a lot more servers than I do and so it's really handy for them to do", "tokens": [3329, 575, 257, 688, 544, 15909, 813, 286, 360, 293, 370, 309, 311, 534, 13239, 337, 552, 281, 360], "temperature": 0.0, "avg_logprob": -0.10393420686113074, "compression_ratio": 1.7117117117117118, "no_speech_prob": 1.0952157936117146e-05}, {"id": 294, "seek": 157576, "start": 1585.16, "end": 1590.1, "text": " this and then share the results with the rest of us and it's yeah it's much", "tokens": [341, 293, 550, 2073, 264, 3542, 365, 264, 1472, 295, 505, 293, 309, 311, 1338, 309, 311, 709], "temperature": 0.0, "avg_logprob": -0.10393420686113074, "compression_ratio": 1.7117117117117118, "no_speech_prob": 1.0952157936117146e-05}, {"id": 295, "seek": 157576, "start": 1590.1, "end": 1595.56, "text": " easier to download this already trained version and when I say trained that's", "tokens": [3571, 281, 5484, 341, 1217, 8895, 3037, 293, 562, 286, 584, 8895, 300, 311], "temperature": 0.0, "avg_logprob": -0.10393420686113074, "compression_ratio": 1.7117117117117118, "no_speech_prob": 1.0952157936117146e-05}, {"id": 296, "seek": 159556, "start": 1595.56, "end": 1607.36, "text": " this process of coming up with what the the word embeddings should be. So learning", "tokens": [341, 1399, 295, 1348, 493, 365, 437, 264, 264, 1349, 12240, 29432, 820, 312, 13, 407, 2539], "temperature": 0.0, "avg_logprob": -0.12228146994986185, "compression_ratio": 1.5217391304347827, "no_speech_prob": 2.406047997283167e-06}, {"id": 297, "seek": 159556, "start": 1607.36, "end": 1612.28, "text": " meaning from context and so this is famous quote by the linguist J.R. Firth", "tokens": [3620, 490, 4319, 293, 370, 341, 307, 4618, 6513, 538, 264, 21766, 468, 508, 13, 49, 13, 28164, 392], "temperature": 0.0, "avg_logprob": -0.12228146994986185, "compression_ratio": 1.5217391304347827, "no_speech_prob": 2.406047997283167e-06}, {"id": 298, "seek": 159556, "start": 1612.28, "end": 1618.08, "text": " you shall know a word by the company it keeps. So I just went to the New York", "tokens": [291, 4393, 458, 257, 1349, 538, 264, 2237, 309, 5965, 13, 407, 286, 445, 1437, 281, 264, 1873, 3609], "temperature": 0.0, "avg_logprob": -0.12228146994986185, "compression_ratio": 1.5217391304347827, "no_speech_prob": 2.406047997283167e-06}, {"id": 299, "seek": 159556, "start": 1618.08, "end": 1622.84, "text": " Times and I googled for banks and then took a few quotes with words before and", "tokens": [11366, 293, 286, 50061, 1493, 337, 10237, 293, 550, 1890, 257, 1326, 19963, 365, 2283, 949, 293], "temperature": 0.0, "avg_logprob": -0.12228146994986185, "compression_ratio": 1.5217391304347827, "no_speech_prob": 2.406047997283167e-06}, {"id": 300, "seek": 162284, "start": 1622.84, "end": 1627.72, "text": " after them and the idea of learning meaning from context is that the words", "tokens": [934, 552, 293, 264, 1558, 295, 2539, 3620, 490, 4319, 307, 300, 264, 2283], "temperature": 0.0, "avg_logprob": -0.10836014066423688, "compression_ratio": 1.522167487684729, "no_speech_prob": 8.529443221050315e-06}, {"id": 301, "seek": 162284, "start": 1627.72, "end": 1633.52, "text": " that appear near bank repeatedly are an indicator of what it means because they", "tokens": [300, 4204, 2651, 3765, 18227, 366, 364, 16961, 295, 437, 309, 1355, 570, 436], "temperature": 0.0, "avg_logprob": -0.10836014066423688, "compression_ratio": 1.522167487684729, "no_speech_prob": 8.529443221050315e-06}, {"id": 302, "seek": 162284, "start": 1633.52, "end": 1640.8, "text": " represent how it's used. So you'll see we've got words like Wall Street, hyped", "tokens": [2906, 577, 309, 311, 1143, 13, 407, 291, 603, 536, 321, 600, 658, 2283, 411, 9551, 7638, 11, 43172], "temperature": 0.0, "avg_logprob": -0.10836014066423688, "compression_ratio": 1.522167487684729, "no_speech_prob": 8.529443221050315e-06}, {"id": 303, "seek": 162284, "start": 1640.8, "end": 1649.1599999999999, "text": " rates, soft inflation readings, economy, gross domestic product, tightening", "tokens": [6846, 11, 2787, 15860, 27319, 11, 5010, 11, 11367, 10939, 1674, 11, 42217], "temperature": 0.0, "avg_logprob": -0.10836014066423688, "compression_ratio": 1.522167487684729, "no_speech_prob": 8.529443221050315e-06}, {"id": 304, "seek": 164916, "start": 1649.16, "end": 1655.0800000000002, "text": " monetary policy and you're gonna do this for a huge corpus of tax and kind of see", "tokens": [26388, 3897, 293, 291, 434, 799, 360, 341, 337, 257, 2603, 1181, 31624, 295, 3366, 293, 733, 295, 536], "temperature": 0.0, "avg_logprob": -0.11531620675867255, "compression_ratio": 1.5958333333333334, "no_speech_prob": 2.368684818065958e-06}, {"id": 305, "seek": 164916, "start": 1655.0800000000002, "end": 1659.16, "text": " what's kind of repeatedly showing up near banks and get an indication of like", "tokens": [437, 311, 733, 295, 18227, 4099, 493, 2651, 10237, 293, 483, 364, 18877, 295, 411], "temperature": 0.0, "avg_logprob": -0.11531620675867255, "compression_ratio": 1.5958333333333334, "no_speech_prob": 2.368684818065958e-06}, {"id": 306, "seek": 164916, "start": 1659.16, "end": 1665.92, "text": " okay those things are related. Yeah so words frequently used together with", "tokens": [1392, 729, 721, 366, 4077, 13, 865, 370, 2283, 10374, 1143, 1214, 365], "temperature": 0.0, "avg_logprob": -0.11531620675867255, "compression_ratio": 1.5958333333333334, "no_speech_prob": 2.368684818065958e-06}, {"id": 307, "seek": 164916, "start": 1665.92, "end": 1672.96, "text": " bank represent its meaning. Are there questions about this? And the idea is", "tokens": [3765, 2906, 1080, 3620, 13, 2014, 456, 1651, 466, 341, 30, 400, 264, 1558, 307], "temperature": 0.0, "avg_logprob": -0.11531620675867255, "compression_ratio": 1.5958333333333334, "no_speech_prob": 2.368684818065958e-06}, {"id": 308, "seek": 164916, "start": 1672.96, "end": 1677.1200000000001, "text": " that even though some words show up like you know presence which doesn't", "tokens": [300, 754, 1673, 512, 2283, 855, 493, 411, 291, 458, 6814, 597, 1177, 380], "temperature": 0.0, "avg_logprob": -0.11531620675867255, "compression_ratio": 1.5958333333333334, "no_speech_prob": 2.368684818065958e-06}, {"id": 309, "seek": 167712, "start": 1677.12, "end": 1680.6799999999998, "text": " particularly have anything to do with banks that's gonna show up next to a", "tokens": [4098, 362, 1340, 281, 360, 365, 10237, 300, 311, 799, 855, 493, 958, 281, 257], "temperature": 0.0, "avg_logprob": -0.1905643756573017, "compression_ratio": 1.4133333333333333, "no_speech_prob": 1.750173032633029e-05}, {"id": 310, "seek": 167712, "start": 1680.6799999999998, "end": 1684.6799999999998, "text": " bunch of other words and so it's not disproportionately showing up near", "tokens": [3840, 295, 661, 2283, 293, 370, 309, 311, 406, 43397, 4099, 493, 2651], "temperature": 0.0, "avg_logprob": -0.1905643756573017, "compression_ratio": 1.4133333333333333, "no_speech_prob": 1.750173032633029e-05}, {"id": 311, "seek": 168468, "start": 1684.68, "end": 1709.64, "text": " banks so that wouldn't be picked up in the final embedding. Yes? Yes, yes exactly.", "tokens": [10237, 370, 300, 2759, 380, 312, 6183, 493, 294, 264, 2572, 12240, 3584, 13, 1079, 30, 1079, 11, 2086, 2293, 13], "temperature": 0.0, "avg_logprob": -0.2375085467383975, "compression_ratio": 1.2519685039370079, "no_speech_prob": 1.618653732293751e-05}, {"id": 312, "seek": 168468, "start": 1709.64, "end": 1713.6000000000001, "text": " Yeah and I actually have to repeat the question since I'm recording this but", "tokens": [865, 293, 286, 767, 362, 281, 7149, 264, 1168, 1670, 286, 478, 6613, 341, 457], "temperature": 0.0, "avg_logprob": -0.2375085467383975, "compression_ratio": 1.2519685039370079, "no_speech_prob": 1.618653732293751e-05}, {"id": 313, "seek": 171360, "start": 1713.6, "end": 1717.24, "text": " the question was you know Watson is reading all of Wikipedia do we have to", "tokens": [264, 1168, 390, 291, 458, 25640, 307, 3760, 439, 295, 28999, 360, 321, 362, 281], "temperature": 0.0, "avg_logprob": -0.12343912936271505, "compression_ratio": 1.668141592920354, "no_speech_prob": 2.0143012079643086e-05}, {"id": 314, "seek": 171360, "start": 1717.24, "end": 1721.1999999999998, "text": " translate all of that into numbers and it does this very quickly and yes it is", "tokens": [13799, 439, 295, 300, 666, 3547, 293, 309, 775, 341, 588, 2661, 293, 2086, 309, 307], "temperature": 0.0, "avg_logprob": -0.12343912936271505, "compression_ratio": 1.668141592920354, "no_speech_prob": 2.0143012079643086e-05}, {"id": 315, "seek": 171360, "start": 1721.1999999999998, "end": 1726.3999999999999, "text": " yeah yeah in three seconds is what I'm hearing yeah so that's um and that comes", "tokens": [1338, 1338, 294, 1045, 3949, 307, 437, 286, 478, 4763, 1338, 370, 300, 311, 1105, 293, 300, 1487], "temperature": 0.0, "avg_logprob": -0.12343912936271505, "compression_ratio": 1.668141592920354, "no_speech_prob": 2.0143012079643086e-05}, {"id": 316, "seek": 171360, "start": 1726.3999999999999, "end": 1731.7199999999998, "text": " back to they have I'm sure they have a ton of GPUs and really powerful", "tokens": [646, 281, 436, 362, 286, 478, 988, 436, 362, 257, 2952, 295, 18407, 82, 293, 534, 4005], "temperature": 0.0, "avg_logprob": -0.12343912936271505, "compression_ratio": 1.668141592920354, "no_speech_prob": 2.0143012079643086e-05}, {"id": 317, "seek": 171360, "start": 1731.7199999999998, "end": 1735.8, "text": " computers and so that's something that you and I would not be able to do", "tokens": [10807, 293, 370, 300, 311, 746, 300, 291, 293, 286, 576, 406, 312, 1075, 281, 360], "temperature": 0.0, "avg_logprob": -0.12343912936271505, "compression_ratio": 1.668141592920354, "no_speech_prob": 2.0143012079643086e-05}, {"id": 318, "seek": 173580, "start": 1735.8, "end": 1744.84, "text": " without access to kind of like these really expensive resources and actually", "tokens": [1553, 2105, 281, 733, 295, 411, 613, 534, 5124, 3593, 293, 767], "temperature": 0.0, "avg_logprob": -0.14526400072821255, "compression_ratio": 1.4746835443037976, "no_speech_prob": 1.0952522643492557e-05}, {"id": 319, "seek": 173580, "start": 1744.84, "end": 1749.76, "text": " I think that's a good a good lead-in to I just so we're gonna get to Jupiter", "tokens": [286, 519, 300, 311, 257, 665, 257, 665, 1477, 12, 259, 281, 286, 445, 370, 321, 434, 799, 483, 281, 24567], "temperature": 0.0, "avg_logprob": -0.14526400072821255, "compression_ratio": 1.4746835443037976, "no_speech_prob": 1.0952522643492557e-05}, {"id": 320, "seek": 173580, "start": 1749.76, "end": 1754.32, "text": " notebook in a little bit but I have an example and I should say this example oh", "tokens": [21060, 294, 257, 707, 857, 457, 286, 362, 364, 1365, 293, 286, 820, 584, 341, 1365, 1954], "temperature": 0.0, "avg_logprob": -0.14526400072821255, "compression_ratio": 1.4746835443037976, "no_speech_prob": 1.0952522643492557e-05}, {"id": 321, "seek": 175432, "start": 1754.32, "end": 1769.32, "text": " there's another question yes so that's something that in a general version is", "tokens": [456, 311, 1071, 1168, 2086, 370, 300, 311, 746, 300, 294, 257, 2674, 3037, 307], "temperature": 0.0, "avg_logprob": -0.1595031420389811, "compression_ratio": 1.5810810810810811, "no_speech_prob": 1.341900679108221e-05}, {"id": 322, "seek": 175432, "start": 1769.32, "end": 1772.8, "text": " not going to be able to tell different meanings I'll actually see that in a", "tokens": [406, 516, 281, 312, 1075, 281, 980, 819, 28138, 286, 603, 767, 536, 300, 294, 257], "temperature": 0.0, "avg_logprob": -0.1595031420389811, "compression_ratio": 1.5810810810810811, "no_speech_prob": 1.341900679108221e-05}, {"id": 323, "seek": 175432, "start": 1772.8, "end": 1779.32, "text": " little bit in the notebook intelligence is going to be linked to things like CIA", "tokens": [707, 857, 294, 264, 21060, 7599, 307, 516, 281, 312, 9408, 281, 721, 411, 25143], "temperature": 0.0, "avg_logprob": -0.1595031420389811, "compression_ratio": 1.5810810810810811, "no_speech_prob": 1.341900679108221e-05}, {"id": 324, "seek": 177932, "start": 1779.32, "end": 1784.3999999999999, "text": " as well as you know being smart even though those are kind of two different", "tokens": [382, 731, 382, 291, 458, 885, 4069, 754, 1673, 729, 366, 733, 295, 732, 819], "temperature": 0.0, "avg_logprob": -0.18971125284830728, "compression_ratio": 1.7341040462427746, "no_speech_prob": 5.173645604372723e-06}, {"id": 325, "seek": 177932, "start": 1784.3999999999999, "end": 1789.76, "text": " meanings and there are there are linguists that build more complicated", "tokens": [28138, 293, 456, 366, 456, 366, 21766, 1751, 300, 1322, 544, 6179], "temperature": 0.0, "avg_logprob": -0.18971125284830728, "compression_ratio": 1.7341040462427746, "no_speech_prob": 5.173645604372723e-06}, {"id": 326, "seek": 177932, "start": 1789.76, "end": 1795.24, "text": " systems kind of on top of this are using these tools but the kind of basic", "tokens": [3652, 733, 295, 322, 1192, 295, 341, 366, 1228, 613, 3873, 457, 264, 733, 295, 3875], "temperature": 0.0, "avg_logprob": -0.18971125284830728, "compression_ratio": 1.7341040462427746, "no_speech_prob": 5.173645604372723e-06}, {"id": 327, "seek": 177932, "start": 1795.24, "end": 1803.4399999999998, "text": " approach yeah it's not going to pick up on this has two different meanings yes", "tokens": [3109, 1338, 309, 311, 406, 516, 281, 1888, 493, 322, 341, 575, 732, 819, 28138, 2086], "temperature": 0.0, "avg_logprob": -0.18971125284830728, "compression_ratio": 1.7341040462427746, "no_speech_prob": 5.173645604372723e-06}, {"id": 328, "seek": 180344, "start": 1803.44, "end": 1812.0, "text": " and so the question was do they also define it by words that would never be", "tokens": [293, 370, 264, 1168, 390, 360, 436, 611, 6964, 309, 538, 2283, 300, 576, 1128, 312], "temperature": 0.0, "avg_logprob": -0.09261058128043397, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.092947958473815e-06}, {"id": 329, "seek": 180344, "start": 1812.0, "end": 1817.28, "text": " associated associated with bank and that's correct yes and what they", "tokens": [6615, 6615, 365, 3765, 293, 300, 311, 3006, 2086, 293, 437, 436], "temperature": 0.0, "avg_logprob": -0.09261058128043397, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.092947958473815e-06}, {"id": 330, "seek": 180344, "start": 1817.28, "end": 1821.8, "text": " actually do is they randomly change some of the words so they'll kind of like put", "tokens": [767, 360, 307, 436, 16979, 1319, 512, 295, 264, 2283, 370, 436, 603, 733, 295, 411, 829], "temperature": 0.0, "avg_logprob": -0.09261058128043397, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.092947958473815e-06}, {"id": 331, "seek": 180344, "start": 1821.8, "end": 1828.0800000000002, "text": " the wrong word in in place and then say that's not what you're supposed to get", "tokens": [264, 2085, 1349, 294, 294, 1081, 293, 550, 584, 300, 311, 406, 437, 291, 434, 3442, 281, 483], "temperature": 0.0, "avg_logprob": -0.09261058128043397, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.092947958473815e-06}, {"id": 332, "seek": 182808, "start": 1828.08, "end": 1839.0, "text": " now these are good questions anymore okay so this is an Excel and I did this", "tokens": [586, 613, 366, 665, 1651, 3602, 1392, 370, 341, 307, 364, 19060, 293, 286, 630, 341], "temperature": 0.0, "avg_logprob": -0.21271713256835936, "compression_ratio": 1.5947368421052632, "no_speech_prob": 6.401076825568452e-05}, {"id": 333, "seek": 182808, "start": 1839.0, "end": 1843.96, "text": " just um excels are really can be a very useful visual tool just to see what", "tokens": [445, 1105, 1624, 1625, 366, 534, 393, 312, 257, 588, 4420, 5056, 2290, 445, 281, 536, 437], "temperature": 0.0, "avg_logprob": -0.21271713256835936, "compression_ratio": 1.5947368421052632, "no_speech_prob": 6.401076825568452e-05}, {"id": 334, "seek": 182808, "start": 1843.96, "end": 1849.32, "text": " things look like laid out I was starting to say these this as well as the", "tokens": [721, 574, 411, 9897, 484, 286, 390, 2891, 281, 584, 613, 341, 382, 731, 382, 264], "temperature": 0.0, "avg_logprob": -0.21271713256835936, "compression_ratio": 1.5947368421052632, "no_speech_prob": 6.401076825568452e-05}, {"id": 335, "seek": 182808, "start": 1849.32, "end": 1854.84, "text": " notebook are I've been modified from part five or less than five of our deep", "tokens": [21060, 366, 286, 600, 668, 15873, 490, 644, 1732, 420, 1570, 813, 1732, 295, 527, 2452], "temperature": 0.0, "avg_logprob": -0.21271713256835936, "compression_ratio": 1.5947368421052632, "no_speech_prob": 6.401076825568452e-05}, {"id": 336, "seek": 185484, "start": 1854.84, "end": 1862.76, "text": " learning course but here this is the text of Sam I am and it's a good size", "tokens": [2539, 1164, 457, 510, 341, 307, 264, 2487, 295, 4832, 286, 669, 293, 309, 311, 257, 665, 2744], "temperature": 0.0, "avg_logprob": -0.1586081289475964, "compression_ratio": 1.5655172413793104, "no_speech_prob": 6.643211690970929e-06}, {"id": 337, "seek": 185484, "start": 1862.76, "end": 1872.6399999999999, "text": " would you need it larger a little bit larger okay so let me go over here so", "tokens": [576, 291, 643, 309, 4833, 257, 707, 857, 4833, 1392, 370, 718, 385, 352, 670, 510, 370], "temperature": 0.0, "avg_logprob": -0.1586081289475964, "compression_ratio": 1.5655172413793104, "no_speech_prob": 6.643211690970929e-06}, {"id": 338, "seek": 185484, "start": 1872.6399999999999, "end": 1879.6399999999999, "text": " this column is the original text I am Daniel I am Sam Sam I am that Sam I am", "tokens": [341, 7738, 307, 264, 3380, 2487, 286, 669, 8033, 286, 669, 4832, 4832, 286, 669, 300, 4832, 286, 669], "temperature": 0.0, "avg_logprob": -0.1586081289475964, "compression_ratio": 1.5655172413793104, "no_speech_prob": 6.643211690970929e-06}, {"id": 339, "seek": 187964, "start": 1879.64, "end": 1885.44, "text": " I do not like that Sam I am and so if we wanted to be able to represent this into", "tokens": [286, 360, 406, 411, 300, 4832, 286, 669, 293, 370, 498, 321, 1415, 281, 312, 1075, 281, 2906, 341, 666], "temperature": 0.0, "avg_logprob": -0.09202937489932346, "compression_ratio": 1.6991150442477876, "no_speech_prob": 6.643233064096421e-06}, {"id": 340, "seek": 187964, "start": 1885.44, "end": 1891.24, "text": " a computer what we need is a dictionary of all the the words that show up and", "tokens": [257, 3820, 437, 321, 643, 307, 257, 25890, 295, 439, 264, 264, 2283, 300, 855, 493, 293], "temperature": 0.0, "avg_logprob": -0.09202937489932346, "compression_ratio": 1.6991150442477876, "no_speech_prob": 6.643233064096421e-06}, {"id": 341, "seek": 187964, "start": 1891.24, "end": 1896.0200000000002, "text": " fortunately this is only 14 words so this is a pretty simple example we're", "tokens": [25511, 341, 307, 787, 3499, 2283, 370, 341, 307, 257, 1238, 2199, 1365, 321, 434], "temperature": 0.0, "avg_logprob": -0.09202937489932346, "compression_ratio": 1.6991150442477876, "no_speech_prob": 6.643233064096421e-06}, {"id": 342, "seek": 187964, "start": 1896.0200000000002, "end": 1900.0800000000002, "text": " giving them each a word ID so this was kind of our initial numbering the", "tokens": [2902, 552, 1184, 257, 1349, 7348, 370, 341, 390, 733, 295, 527, 5883, 1230, 278, 264], "temperature": 0.0, "avg_logprob": -0.09202937489932346, "compression_ratio": 1.6991150442477876, "no_speech_prob": 6.643233064096421e-06}, {"id": 343, "seek": 187964, "start": 1900.0800000000002, "end": 1905.44, "text": " dictionary and that is a step but that's not where we're gonna stay and then", "tokens": [25890, 293, 300, 307, 257, 1823, 457, 300, 311, 406, 689, 321, 434, 799, 1754, 293, 550], "temperature": 0.0, "avg_logprob": -0.09202937489932346, "compression_ratio": 1.6991150442477876, "no_speech_prob": 6.643233064096421e-06}, {"id": 344, "seek": 190544, "start": 1905.44, "end": 1912.1200000000001, "text": " we're using a five-dimensional embedding so here these five numbers", "tokens": [321, 434, 1228, 257, 1732, 12, 18759, 12240, 3584, 370, 510, 613, 1732, 3547], "temperature": 0.0, "avg_logprob": -0.20430529912312825, "compression_ratio": 1.4640522875816993, "no_speech_prob": 5.862703801540192e-06}, {"id": 345, "seek": 190544, "start": 1912.1200000000001, "end": 1919.04, "text": " represent am so this vector of length five is am and wherever am shows up in", "tokens": [2906, 669, 370, 341, 8062, 295, 4641, 1732, 307, 669, 293, 8660, 669, 3110, 493, 294], "temperature": 0.0, "avg_logprob": -0.20430529912312825, "compression_ratio": 1.4640522875816993, "no_speech_prob": 5.862703801540192e-06}, {"id": 346, "seek": 190544, "start": 1919.04, "end": 1926.3200000000002, "text": " the text we're gonna put that vector so it's 0.98 0.95 scroll over a little bit", "tokens": [264, 2487, 321, 434, 799, 829, 300, 8062, 370, 309, 311, 1958, 13, 22516, 1958, 13, 15718, 11369, 670, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.20430529912312825, "compression_ratio": 1.4640522875816993, "no_speech_prob": 5.862703801540192e-06}, {"id": 347, "seek": 192632, "start": 1926.32, "end": 1936.24, "text": " and so you'll see the same vector for am shows up many times since am shows up", "tokens": [293, 370, 291, 603, 536, 264, 912, 8062, 337, 669, 3110, 493, 867, 1413, 1670, 669, 3110, 493], "temperature": 0.0, "avg_logprob": -0.11377173203688401, "compression_ratio": 1.7087912087912087, "no_speech_prob": 3.187532911397284e-06}, {"id": 348, "seek": 192632, "start": 1936.24, "end": 1943.12, "text": " many times oh whoops that's I okay ignore where I accidentally highlighted I", "tokens": [867, 1413, 1954, 567, 3370, 300, 311, 286, 1392, 11200, 689, 286, 15715, 17173, 286], "temperature": 0.0, "avg_logprob": -0.11377173203688401, "compression_ratio": 1.7087912087912087, "no_speech_prob": 3.187532911397284e-06}, {"id": 349, "seek": 192632, "start": 1943.12, "end": 1949.3999999999999, "text": " but all the every time am is in the text you're putting the vector for am and", "tokens": [457, 439, 264, 633, 565, 669, 307, 294, 264, 2487, 291, 434, 3372, 264, 8062, 337, 669, 293], "temperature": 0.0, "avg_logprob": -0.11377173203688401, "compression_ratio": 1.7087912087912087, "no_speech_prob": 3.187532911397284e-06}, {"id": 350, "seek": 192632, "start": 1949.3999999999999, "end": 1953.76, "text": " it's the exact same so even though we're kind of getting this matrix now that", "tokens": [309, 311, 264, 1900, 912, 370, 754, 1673, 321, 434, 733, 295, 1242, 341, 8141, 586, 300], "temperature": 0.0, "avg_logprob": -0.11377173203688401, "compression_ratio": 1.7087912087912087, "no_speech_prob": 3.187532911397284e-06}, {"id": 351, "seek": 195376, "start": 1953.76, "end": 1960.12, "text": " represents the text of green eggs and ham it's just these 14 vectors being", "tokens": [8855, 264, 2487, 295, 3092, 6466, 293, 7852, 309, 311, 445, 613, 3499, 18875, 885], "temperature": 0.0, "avg_logprob": -0.1793387316275334, "compression_ratio": 1.6885245901639345, "no_speech_prob": 9.817730642680544e-06}, {"id": 352, "seek": 195376, "start": 1960.12, "end": 1970.2, "text": " repeated where needed to represent the words are there questions about this yes", "tokens": [10477, 689, 2978, 281, 2906, 264, 2283, 366, 456, 1651, 466, 341, 2086], "temperature": 0.0, "avg_logprob": -0.1793387316275334, "compression_ratio": 1.6885245901639345, "no_speech_prob": 9.817730642680544e-06}, {"id": 353, "seek": 195376, "start": 1971.32, "end": 1976.28, "text": " so the question was how do you determine how many vectors you need and I assume", "tokens": [370, 264, 1168, 390, 577, 360, 291, 6997, 577, 867, 18875, 291, 643, 293, 286, 6552], "temperature": 0.0, "avg_logprob": -0.1793387316275334, "compression_ratio": 1.6885245901639345, "no_speech_prob": 9.817730642680544e-06}, {"id": 354, "seek": 195376, "start": 1976.28, "end": 1980.24, "text": " you mean the kind of the dimension of the vector and that's an interesting", "tokens": [291, 914, 264, 733, 295, 264, 10139, 295, 264, 8062, 293, 300, 311, 364, 1880], "temperature": 0.0, "avg_logprob": -0.1793387316275334, "compression_ratio": 1.6885245901639345, "no_speech_prob": 9.817730642680544e-06}, {"id": 355, "seek": 198024, "start": 1980.24, "end": 1984.68, "text": " question and actually with gloves they have a bunch of different versions", "tokens": [1168, 293, 767, 365, 14976, 436, 362, 257, 3840, 295, 819, 9606], "temperature": 0.0, "avg_logprob": -0.18324639625156047, "compression_ratio": 1.7754237288135593, "no_speech_prob": 1.0450787158333696e-05}, {"id": 356, "seek": 198024, "start": 1984.68, "end": 1989.44, "text": " there's like a 50 dimensional version and a hundred and two hundred so I think", "tokens": [456, 311, 411, 257, 2625, 18795, 3037, 293, 257, 3262, 293, 732, 3262, 370, 286, 519], "temperature": 0.0, "avg_logprob": -0.18324639625156047, "compression_ratio": 1.7754237288135593, "no_speech_prob": 1.0450787158333696e-05}, {"id": 357, "seek": 198024, "start": 1989.44, "end": 1992.44, "text": " that I don't know that there's a clear answer I mean I think some of it would", "tokens": [300, 286, 500, 380, 458, 300, 456, 311, 257, 1850, 1867, 286, 914, 286, 519, 512, 295, 309, 576], "temperature": 0.0, "avg_logprob": -0.18324639625156047, "compression_ratio": 1.7754237288135593, "no_speech_prob": 1.0450787158333696e-05}, {"id": 358, "seek": 198024, "start": 1992.44, "end": 1996.4, "text": " be like experimentation to see for the problem you're working on what gets you", "tokens": [312, 411, 37142, 281, 536, 337, 264, 1154, 291, 434, 1364, 322, 437, 2170, 291], "temperature": 0.0, "avg_logprob": -0.18324639625156047, "compression_ratio": 1.7754237288135593, "no_speech_prob": 1.0450787158333696e-05}, {"id": 359, "seek": 198024, "start": 1996.4, "end": 2001.64, "text": " better results yeah it's a good", "tokens": [1101, 3542, 1338, 309, 311, 257, 665], "temperature": 0.0, "avg_logprob": -0.18324639625156047, "compression_ratio": 1.7754237288135593, "no_speech_prob": 1.0450787158333696e-05}, {"id": 360, "seek": 198024, "start": 2003.64, "end": 2008.76, "text": " am you asking if more more dimensions means that you can capture more meaning", "tokens": [669, 291, 3365, 498, 544, 544, 12819, 1355, 300, 291, 393, 7983, 544, 3620], "temperature": 0.0, "avg_logprob": -0.18324639625156047, "compression_ratio": 1.7754237288135593, "no_speech_prob": 1.0450787158333696e-05}, {"id": 361, "seek": 200876, "start": 2008.76, "end": 2016.2, "text": " and I think quite possibly yeah like definitely at a large scale yeah like if", "tokens": [293, 286, 519, 1596, 6264, 1338, 411, 2138, 412, 257, 2416, 4373, 1338, 411, 498], "temperature": 0.0, "avg_logprob": -0.15811544389867072, "compression_ratio": 1.538888888888889, "no_speech_prob": 1.0129483598575462e-05}, {"id": 362, "seek": 200876, "start": 2016.2, "end": 2021.8799999999999, "text": " you only if you were trying to capture 40,000 words in five dimensions you're", "tokens": [291, 787, 498, 291, 645, 1382, 281, 7983, 3356, 11, 1360, 2283, 294, 1732, 12819, 291, 434], "temperature": 0.0, "avg_logprob": -0.15811544389867072, "compression_ratio": 1.538888888888889, "no_speech_prob": 1.0129483598575462e-05}, {"id": 363, "seek": 200876, "start": 2021.8799999999999, "end": 2025.4, "text": " not going to be able to capture much meaning", "tokens": [406, 516, 281, 312, 1075, 281, 7983, 709, 3620], "temperature": 0.0, "avg_logprob": -0.15811544389867072, "compression_ratio": 1.538888888888889, "no_speech_prob": 1.0129483598575462e-05}, {"id": 364, "seek": 200876, "start": 2029.08, "end": 2035.64, "text": " so for that I just meet these up but typically you would be looking those up", "tokens": [370, 337, 300, 286, 445, 1677, 613, 493, 457, 5850, 291, 576, 312, 1237, 729, 493], "temperature": 0.0, "avg_logprob": -0.15811544389867072, "compression_ratio": 1.538888888888889, "no_speech_prob": 1.0129483598575462e-05}, {"id": 365, "seek": 203564, "start": 2035.64, "end": 2040.0800000000002, "text": " like in glove or in word2vec and so that's where word2vec or glove comes in", "tokens": [411, 294, 26928, 420, 294, 1349, 17, 303, 66, 293, 370, 300, 311, 689, 1349, 17, 303, 66, 420, 26928, 1487, 294], "temperature": 0.0, "avg_logprob": -0.10875410562033182, "compression_ratio": 1.7104072398190044, "no_speech_prob": 1.0451027264934964e-05}, {"id": 366, "seek": 203564, "start": 2040.0800000000002, "end": 2045.48, "text": " handy it's like that'll tell you like okay use this vector yeah but there's", "tokens": [13239, 309, 311, 411, 300, 603, 980, 291, 411, 1392, 764, 341, 8062, 1338, 457, 456, 311], "temperature": 0.0, "avg_logprob": -0.10875410562033182, "compression_ratio": 1.7104072398190044, "no_speech_prob": 1.0451027264934964e-05}, {"id": 367, "seek": 203564, "start": 2045.48, "end": 2051.8, "text": " not not actually a five-dimensional version although this is something that", "tokens": [406, 406, 767, 257, 1732, 12, 18759, 3037, 4878, 341, 307, 746, 300], "temperature": 0.0, "avg_logprob": -0.10875410562033182, "compression_ratio": 1.7104072398190044, "no_speech_prob": 1.0451027264934964e-05}, {"id": 368, "seek": 203564, "start": 2051.8, "end": 2056.84, "text": " you could potentially train yourself because this is a small small data set", "tokens": [291, 727, 7263, 3847, 1803, 570, 341, 307, 257, 1359, 1359, 1412, 992], "temperature": 0.0, "avg_logprob": -0.10875410562033182, "compression_ratio": 1.7104072398190044, "no_speech_prob": 1.0451027264934964e-05}, {"id": 369, "seek": 203564, "start": 2056.84, "end": 2060.84, "text": " although you also I guess wouldn't capture much meaning because you have a", "tokens": [4878, 291, 611, 286, 2041, 2759, 380, 7983, 709, 3620, 570, 291, 362, 257], "temperature": 0.0, "avg_logprob": -0.10875410562033182, "compression_ratio": 1.7104072398190044, "no_speech_prob": 1.0451027264934964e-05}, {"id": 370, "seek": 206084, "start": 2060.84, "end": 2070.76, "text": " very small input text there's that question over here so there's um there's", "tokens": [588, 1359, 4846, 2487, 456, 311, 300, 1168, 670, 510, 370, 456, 311, 1105, 456, 311], "temperature": 0.0, "avg_logprob": -0.18576933208264804, "compression_ratio": 1.7798165137614679, "no_speech_prob": 2.506765304133296e-05}, {"id": 371, "seek": 206084, "start": 2070.76, "end": 2074.96, "text": " a yeah there's several and so basically it's you can download these but word2vec", "tokens": [257, 1338, 456, 311, 2940, 293, 370, 1936, 309, 311, 291, 393, 5484, 613, 457, 1349, 17, 303, 66], "temperature": 0.0, "avg_logprob": -0.18576933208264804, "compression_ratio": 1.7798165137614679, "no_speech_prob": 2.506765304133296e-05}, {"id": 372, "seek": 206084, "start": 2074.96, "end": 2079.44, "text": " is available for download glove is available and I've got instructions for", "tokens": [307, 2435, 337, 5484, 26928, 307, 2435, 293, 286, 600, 658, 9415, 337], "temperature": 0.0, "avg_logprob": -0.18576933208264804, "compression_ratio": 1.7798165137614679, "no_speech_prob": 2.506765304133296e-05}, {"id": 373, "seek": 206084, "start": 2079.44, "end": 2085.92, "text": " downloading glove because we have a site hosted at files.fast.ai but yeah you", "tokens": [32529, 26928, 570, 321, 362, 257, 3621, 19204, 412, 7098, 13, 7011, 13, 1301, 457, 1338, 291], "temperature": 0.0, "avg_logprob": -0.18576933208264804, "compression_ratio": 1.7798165137614679, "no_speech_prob": 2.506765304133296e-05}, {"id": 374, "seek": 206084, "start": 2085.92, "end": 2090.28, "text": " can get them and you've got yeah this is long list of words and we'll actually", "tokens": [393, 483, 552, 293, 291, 600, 658, 1338, 341, 307, 938, 1329, 295, 2283, 293, 321, 603, 767], "temperature": 0.0, "avg_logprob": -0.18576933208264804, "compression_ratio": 1.7798165137614679, "no_speech_prob": 2.506765304133296e-05}, {"id": 375, "seek": 209028, "start": 2090.28, "end": 2093.48, "text": " get into this in a moment I'll show you what it looks like and then this yeah", "tokens": [483, 666, 341, 294, 257, 1623, 286, 603, 855, 291, 437, 309, 1542, 411, 293, 550, 341, 1338], "temperature": 0.0, "avg_logprob": -0.20300734320352243, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.0450269655848388e-05}, {"id": 376, "seek": 209028, "start": 2093.48, "end": 2100.2000000000003, "text": " this dictionary can like look up a word and get get an embedding and no so", "tokens": [341, 25890, 393, 411, 574, 493, 257, 1349, 293, 483, 483, 364, 12240, 3584, 293, 572, 370], "temperature": 0.0, "avg_logprob": -0.20300734320352243, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.0450269655848388e-05}, {"id": 377, "seek": 209028, "start": 2100.2000000000003, "end": 2105.6800000000003, "text": " computers did this so there's an algorithm that was used to train this", "tokens": [10807, 630, 341, 370, 456, 311, 364, 9284, 300, 390, 1143, 281, 3847, 341], "temperature": 0.0, "avg_logprob": -0.20300734320352243, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.0450269655848388e-05}, {"id": 378, "seek": 209028, "start": 2105.6800000000003, "end": 2109.96, "text": " that kind of took in a ton of yeah a ton of text from the lab and then came up", "tokens": [300, 733, 295, 1890, 294, 257, 2952, 295, 1338, 257, 2952, 295, 2487, 490, 264, 2715, 293, 550, 1361, 493], "temperature": 0.0, "avg_logprob": -0.20300734320352243, "compression_ratio": 1.6615384615384616, "no_speech_prob": 1.0450269655848388e-05}, {"id": 379, "seek": 210996, "start": 2109.96, "end": 2131.28, "text": " with embeddings yes the three-dimensional ones", "tokens": [50364, 365, 12240, 29432, 2086, 264, 1045, 12, 18759, 2306, 51430], "temperature": 0.0, "avg_logprob": -0.5296269257863363, "compression_ratio": 0.8846153846153846, "no_speech_prob": 1.8160632180297398e-06}, {"id": 380, "seek": 213996, "start": 2139.96, "end": 2163.7200000000003, "text": " yeah yeah so the question is what's some intuition about how do you get to these", "tokens": [1338, 1338, 370, 264, 1168, 307, 437, 311, 512, 24002, 466, 577, 360, 291, 483, 281, 613], "temperature": 0.0, "avg_logprob": -0.22905672164190383, "compression_ratio": 1.1267605633802817, "no_speech_prob": 0.009406965225934982}, {"id": 381, "seek": 216372, "start": 2163.72, "end": 2169.9199999999996, "text": " like really large embeddings where you have a ton of words and the idea behind", "tokens": [411, 534, 2416, 12240, 29432, 689, 291, 362, 257, 2952, 295, 2283, 293, 264, 1558, 2261], "temperature": 0.0, "avg_logprob": -0.10706964406100186, "compression_ratio": 1.6801801801801801, "no_speech_prob": 2.58625514106825e-05}, {"id": 382, "seek": 216372, "start": 2169.9199999999996, "end": 2173.3199999999997, "text": " it is kind of going back to this thinking about what words appear", "tokens": [309, 307, 733, 295, 516, 646, 281, 341, 1953, 466, 437, 2283, 4204], "temperature": 0.0, "avg_logprob": -0.10706964406100186, "compression_ratio": 1.6801801801801801, "no_speech_prob": 2.58625514106825e-05}, {"id": 383, "seek": 216372, "start": 2173.3199999999997, "end": 2180.68, "text": " together and so word2vec is kind of training taking in just a bunch of text", "tokens": [1214, 293, 370, 1349, 17, 303, 66, 307, 733, 295, 3097, 1940, 294, 445, 257, 3840, 295, 2487], "temperature": 0.0, "avg_logprob": -0.10706964406100186, "compression_ratio": 1.6801801801801801, "no_speech_prob": 2.58625514106825e-05}, {"id": 384, "seek": 216372, "start": 2180.68, "end": 2186.4399999999996, "text": " from Wikipedia or from Google Books and training based on what words are showing", "tokens": [490, 28999, 420, 490, 3329, 33843, 293, 3097, 2361, 322, 437, 2283, 366, 4099], "temperature": 0.0, "avg_logprob": -0.10706964406100186, "compression_ratio": 1.6801801801801801, "no_speech_prob": 2.58625514106825e-05}, {"id": 385, "seek": 216372, "start": 2186.4399999999996, "end": 2190.12, "text": " up next to each other and then if someone asked earlier it also kind of", "tokens": [493, 958, 281, 1184, 661, 293, 550, 498, 1580, 2351, 3071, 309, 611, 733, 295], "temperature": 0.0, "avg_logprob": -0.10706964406100186, "compression_ratio": 1.6801801801801801, "no_speech_prob": 2.58625514106825e-05}, {"id": 386, "seek": 219012, "start": 2190.12, "end": 2195.08, "text": " has these counter examples where it's like randomly put words together that", "tokens": [575, 613, 5682, 5110, 689, 309, 311, 411, 16979, 829, 2283, 1214, 300], "temperature": 0.0, "avg_logprob": -0.1864015752618963, "compression_ratio": 1.6418918918918919, "no_speech_prob": 3.4264583518961444e-05}, {"id": 387, "seek": 219012, "start": 2195.08, "end": 2201.68, "text": " wouldn't necessarily go together and is training to find an embedding that kind", "tokens": [2759, 380, 4725, 352, 1214, 293, 307, 3097, 281, 915, 364, 12240, 3584, 300, 733], "temperature": 0.0, "avg_logprob": -0.1864015752618963, "compression_ratio": 1.6418918918918919, "no_speech_prob": 3.4264583518961444e-05}, {"id": 388, "seek": 219012, "start": 2201.68, "end": 2206.7999999999997, "text": " of puts things that are near each other and text often closer together in the", "tokens": [295, 8137, 721, 300, 366, 2651, 1184, 661, 293, 2487, 2049, 4966, 1214, 294, 264], "temperature": 0.0, "avg_logprob": -0.1864015752618963, "compression_ratio": 1.6418918918918919, "no_speech_prob": 3.4264583518961444e-05}, {"id": 389, "seek": 219012, "start": 2206.7999999999997, "end": 2209.16, "text": " embedding", "tokens": [12240, 3584], "temperature": 0.0, "avg_logprob": -0.1864015752618963, "compression_ratio": 1.6418918918918919, "no_speech_prob": 3.4264583518961444e-05}, {"id": 390, "seek": 220916, "start": 2209.16, "end": 2223.7999999999997, "text": " oh yeah so it's yeah so it's very likely yeah that Sam and Daniel could have a", "tokens": [1954, 1338, 370, 309, 311, 1338, 370, 309, 311, 588, 3700, 1338, 300, 4832, 293, 8033, 727, 362, 257], "temperature": 0.0, "avg_logprob": -0.10032611846923828, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.7501179172541015e-05}, {"id": 391, "seek": 220916, "start": 2223.7999999999997, "end": 2228.48, "text": " something in common that indicates they're both first names something", "tokens": [746, 294, 2689, 300, 16203, 436, 434, 1293, 700, 5288, 746], "temperature": 0.0, "avg_logprob": -0.10032611846923828, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.7501179172541015e-05}, {"id": 392, "seek": 220916, "start": 2228.48, "end": 2234.6, "text": " that's a little bit subtle about this is that since you know the computer is not", "tokens": [300, 311, 257, 707, 857, 13743, 466, 341, 307, 300, 1670, 291, 458, 264, 3820, 307, 406], "temperature": 0.0, "avg_logprob": -0.10032611846923828, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.7501179172541015e-05}, {"id": 393, "seek": 220916, "start": 2234.6, "end": 2238.3199999999997, "text": " saying like this is a first name so I'm gonna make it similar here it's really", "tokens": [1566, 411, 341, 307, 257, 700, 1315, 370, 286, 478, 799, 652, 309, 2531, 510, 309, 311, 534], "temperature": 0.0, "avg_logprob": -0.10032611846923828, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.7501179172541015e-05}, {"id": 394, "seek": 223832, "start": 2238.32, "end": 2244.2000000000003, "text": " just about how it's used it's not always obvious to us looking at a vector of", "tokens": [445, 466, 577, 309, 311, 1143, 309, 311, 406, 1009, 6322, 281, 505, 1237, 412, 257, 8062, 295], "temperature": 0.0, "avg_logprob": -0.17321130095935258, "compression_ratio": 1.5466666666666666, "no_speech_prob": 1.7501926777185872e-05}, {"id": 395, "seek": 223832, "start": 2244.2000000000003, "end": 2249.44, "text": " like what is this capturing or what is this close to but you're right like and", "tokens": [411, 437, 307, 341, 23384, 420, 437, 307, 341, 1998, 281, 457, 291, 434, 558, 411, 293], "temperature": 0.0, "avg_logprob": -0.17321130095935258, "compression_ratio": 1.5466666666666666, "no_speech_prob": 1.7501926777185872e-05}, {"id": 396, "seek": 223832, "start": 2249.44, "end": 2254.1600000000003, "text": " I think yeah like men's names are going to be kind of like clumped together", "tokens": [286, 519, 1338, 411, 1706, 311, 5288, 366, 516, 281, 312, 733, 295, 411, 596, 1420, 292, 1214], "temperature": 0.0, "avg_logprob": -0.17321130095935258, "compression_ratio": 1.5466666666666666, "no_speech_prob": 1.7501926777185872e-05}, {"id": 397, "seek": 225416, "start": 2254.16, "end": 2269.56, "text": " ah no it's a good question yes so the the question is about grammar and is", "tokens": [3716, 572, 309, 311, 257, 665, 1168, 2086, 370, 264, 264, 1168, 307, 466, 22317, 293, 307], "temperature": 0.0, "avg_logprob": -0.227311872086435, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.2804751349904109e-05}, {"id": 398, "seek": 225416, "start": 2269.56, "end": 2274.64, "text": " this acknowledging you know whether a word could be a pronoun or verb tense", "tokens": [341, 30904, 291, 458, 1968, 257, 1349, 727, 312, 257, 14144, 420, 9595, 18760], "temperature": 0.0, "avg_logprob": -0.227311872086435, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.2804751349904109e-05}, {"id": 399, "seek": 225416, "start": 2274.64, "end": 2280.72, "text": " and this is not explicitly acknowledging that but that shows up like the example", "tokens": [293, 341, 307, 406, 20803, 30904, 300, 457, 300, 3110, 493, 411, 264, 1365], "temperature": 0.0, "avg_logprob": -0.227311872086435, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.2804751349904109e-05}, {"id": 400, "seek": 228072, "start": 2280.72, "end": 2284.3999999999996, "text": " with you know swim is to swimming is walk is to walking and that's just", "tokens": [365, 291, 458, 7110, 307, 281, 11989, 307, 1792, 307, 281, 4494, 293, 300, 311, 445], "temperature": 0.0, "avg_logprob": -0.09739505295204905, "compression_ratio": 1.8669354838709677, "no_speech_prob": 7.965650002006441e-05}, {"id": 401, "seek": 228072, "start": 2284.3999999999996, "end": 2287.52, "text": " coming from how they're used that kind of the way they're going to be used in a", "tokens": [1348, 490, 577, 436, 434, 1143, 300, 733, 295, 264, 636, 436, 434, 516, 281, 312, 1143, 294, 257], "temperature": 0.0, "avg_logprob": -0.09739505295204905, "compression_ratio": 1.8669354838709677, "no_speech_prob": 7.965650002006441e-05}, {"id": 402, "seek": 228072, "start": 2287.52, "end": 2293.2799999999997, "text": " sentence can capture it like what words are near it but it's not not anything", "tokens": [8174, 393, 7983, 309, 411, 437, 2283, 366, 2651, 309, 457, 309, 311, 406, 406, 1340], "temperature": 0.0, "avg_logprob": -0.09739505295204905, "compression_ratio": 1.8669354838709677, "no_speech_prob": 7.965650002006441e-05}, {"id": 403, "seek": 228072, "start": 2293.2799999999997, "end": 2300.08, "text": " explicit and this is a very statistical approach to language of kind of you just", "tokens": [13691, 293, 341, 307, 257, 588, 22820, 3109, 281, 2856, 295, 733, 295, 291, 445], "temperature": 0.0, "avg_logprob": -0.09739505295204905, "compression_ratio": 1.8669354838709677, "no_speech_prob": 7.965650002006441e-05}, {"id": 404, "seek": 228072, "start": 2300.08, "end": 2304.2999999999997, "text": " have like all this data and you're seeing how it's used whereas there are", "tokens": [362, 411, 439, 341, 1412, 293, 291, 434, 2577, 577, 309, 311, 1143, 9735, 456, 366], "temperature": 0.0, "avg_logprob": -0.09739505295204905, "compression_ratio": 1.8669354838709677, "no_speech_prob": 7.965650002006441e-05}, {"id": 405, "seek": 228072, "start": 2304.2999999999997, "end": 2310.0, "text": " linguists that kind of study things in a more like a way where they're talking", "tokens": [21766, 1751, 300, 733, 295, 2979, 721, 294, 257, 544, 411, 257, 636, 689, 436, 434, 1417], "temperature": 0.0, "avg_logprob": -0.09739505295204905, "compression_ratio": 1.8669354838709677, "no_speech_prob": 7.965650002006441e-05}, {"id": 406, "seek": 231000, "start": 2310.0, "end": 2315.36, "text": " about the structure of language or what the rules we know about language are", "tokens": [466, 264, 3877, 295, 2856, 420, 437, 264, 4474, 321, 458, 466, 2856, 366], "temperature": 0.0, "avg_logprob": -0.19536700177548535, "compression_ratio": 1.7094972067039107, "no_speech_prob": 5.954864718660247e-06}, {"id": 407, "seek": 231000, "start": 2324.12, "end": 2330.48, "text": " so the question was is where to back just from empirical observations of how", "tokens": [370, 264, 1168, 390, 307, 689, 281, 646, 445, 490, 31886, 18163, 295, 577], "temperature": 0.0, "avg_logprob": -0.19536700177548535, "compression_ratio": 1.7094972067039107, "no_speech_prob": 5.954864718660247e-06}, {"id": 408, "seek": 231000, "start": 2330.48, "end": 2335.0, "text": " words are used together and yes yeah it's just coming from how they're used", "tokens": [2283, 366, 1143, 1214, 293, 2086, 1338, 309, 311, 445, 1348, 490, 577, 436, 434, 1143], "temperature": 0.0, "avg_logprob": -0.19536700177548535, "compression_ratio": 1.7094972067039107, "no_speech_prob": 5.954864718660247e-06}, {"id": 409, "seek": 231000, "start": 2335.0, "end": 2339.2, "text": " together and this is something that linguists have very and natural language", "tokens": [1214, 293, 341, 307, 746, 300, 21766, 1751, 362, 588, 293, 3303, 2856], "temperature": 0.0, "avg_logprob": -0.19536700177548535, "compression_ratio": 1.7094972067039107, "no_speech_prob": 5.954864718660247e-06}, {"id": 410, "seek": 233920, "start": 2339.2, "end": 2345.6, "text": " researchers have very differing opinions on kind of how to approach this so that's", "tokens": [10309, 362, 588, 743, 278, 11819, 322, 733, 295, 577, 281, 3109, 341, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.1593857221705939, "compression_ratio": 1.7074235807860263, "no_speech_prob": 0.00021977792493999004}, {"id": 411, "seek": 233920, "start": 2345.6, "end": 2350.96, "text": " Carrie who's going to be speaking later today I know that her company pat.ai is", "tokens": [34654, 567, 311, 516, 281, 312, 4124, 1780, 965, 286, 458, 300, 720, 2237, 1947, 13, 1301, 307], "temperature": 0.0, "avg_logprob": -0.1593857221705939, "compression_ratio": 1.7074235807860263, "no_speech_prob": 0.00021977792493999004}, {"id": 412, "seek": 233920, "start": 2350.96, "end": 2354.3599999999997, "text": " taking a very different approach and that they're not using statistics and I", "tokens": [1940, 257, 588, 819, 3109, 293, 300, 436, 434, 406, 1228, 12523, 293, 286], "temperature": 0.0, "avg_logprob": -0.1593857221705939, "compression_ratio": 1.7074235807860263, "no_speech_prob": 0.00021977792493999004}, {"id": 413, "seek": 233920, "start": 2354.3599999999997, "end": 2358.72, "text": " think do have more of a kind of rule based thing that they know whereas", "tokens": [519, 360, 362, 544, 295, 257, 733, 295, 4978, 2361, 551, 300, 436, 458, 9735], "temperature": 0.0, "avg_logprob": -0.1593857221705939, "compression_ratio": 1.7074235807860263, "no_speech_prob": 0.00021977792493999004}, {"id": 414, "seek": 233920, "start": 2358.72, "end": 2365.0, "text": " word to back though is just yeah like just take a ton of ton of text and assume", "tokens": [1349, 281, 646, 1673, 307, 445, 1338, 411, 445, 747, 257, 2952, 295, 2952, 295, 2487, 293, 6552], "temperature": 0.0, "avg_logprob": -0.1593857221705939, "compression_ratio": 1.7074235807860263, "no_speech_prob": 0.00021977792493999004}, {"id": 415, "seek": 236500, "start": 2365.0, "end": 2373.52, "text": " that that's going to kind of capture information you need any more questions", "tokens": [300, 300, 311, 516, 281, 733, 295, 7983, 1589, 291, 643, 604, 544, 1651], "temperature": 0.0, "avg_logprob": -0.19334357125418528, "compression_ratio": 1.5359477124183007, "no_speech_prob": 8.529762453690637e-06}, {"id": 416, "seek": 236500, "start": 2376.12, "end": 2386.32, "text": " all right this is great let me go back to to the slides and yeah so being able", "tokens": [439, 558, 341, 307, 869, 718, 385, 352, 646, 281, 281, 264, 9788, 293, 1338, 370, 885, 1075], "temperature": 0.0, "avg_logprob": -0.19334357125418528, "compression_ratio": 1.5359477124183007, "no_speech_prob": 8.529762453690637e-06}, {"id": 417, "seek": 236500, "start": 2386.32, "end": 2393.36, "text": " to learn meaning from context you know put it in vectors we are able to capture", "tokens": [281, 1466, 3620, 490, 4319, 291, 458, 829, 309, 294, 18875, 321, 366, 1075, 281, 7983], "temperature": 0.0, "avg_logprob": -0.19334357125418528, "compression_ratio": 1.5359477124183007, "no_speech_prob": 8.529762453690637e-06}, {"id": 418, "seek": 239336, "start": 2393.36, "end": 2398.4, "text": " similarity so this is from glove which will be using today and the question is", "tokens": [32194, 370, 341, 307, 490, 26928, 597, 486, 312, 1228, 965, 293, 264, 1168, 307], "temperature": 0.0, "avg_logprob": -0.07874347405000166, "compression_ratio": 1.9271844660194175, "no_speech_prob": 6.500512972706929e-05}, {"id": 419, "seek": 239336, "start": 2398.4, "end": 2403.92, "text": " what words are closest to the word frog and what they've done is see what vectors", "tokens": [437, 2283, 366, 13699, 281, 264, 1349, 17259, 293, 437, 436, 600, 1096, 307, 536, 437, 18875], "temperature": 0.0, "avg_logprob": -0.07874347405000166, "compression_ratio": 1.9271844660194175, "no_speech_prob": 6.500512972706929e-05}, {"id": 420, "seek": 239336, "start": 2403.92, "end": 2409.08, "text": " are closest to the vector representing frog and what words do they represent so", "tokens": [366, 13699, 281, 264, 8062, 13460, 17259, 293, 437, 2283, 360, 436, 2906, 370], "temperature": 0.0, "avg_logprob": -0.07874347405000166, "compression_ratio": 1.9271844660194175, "no_speech_prob": 6.500512972706929e-05}, {"id": 421, "seek": 239336, "start": 2409.08, "end": 2413.08, "text": " this is just coming from comparing how far apart different vectors are in this", "tokens": [341, 307, 445, 1348, 490, 15763, 577, 1400, 4936, 819, 18875, 366, 294, 341], "temperature": 0.0, "avg_logprob": -0.07874347405000166, "compression_ratio": 1.9271844660194175, "no_speech_prob": 6.500512972706929e-05}, {"id": 422, "seek": 239336, "start": 2413.08, "end": 2419.52, "text": " list of numbers and the first closest thing to frog is frogs plural so that's", "tokens": [1329, 295, 3547, 293, 264, 700, 13699, 551, 281, 17259, 307, 37107, 25377, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.07874347405000166, "compression_ratio": 1.9271844660194175, "no_speech_prob": 6.500512972706929e-05}, {"id": 423, "seek": 241952, "start": 2419.52, "end": 2425.24, "text": " what we would expect and then second is toad that's Latoria which I didn't know", "tokens": [437, 321, 576, 2066, 293, 550, 1150, 307, 281, 345, 300, 311, 7354, 8172, 597, 286, 994, 380, 458], "temperature": 0.0, "avg_logprob": -0.19797255595525107, "compression_ratio": 1.7342342342342343, "no_speech_prob": 1.2028898709104396e-05}, {"id": 424, "seek": 241952, "start": 2425.24, "end": 2431.48, "text": " but that's a looks like some sort of tree frog next is leptodactylde which", "tokens": [457, 300, 311, 257, 1542, 411, 512, 1333, 295, 4230, 17259, 958, 307, 476, 662, 378, 578, 88, 348, 68, 597], "temperature": 0.0, "avg_logprob": -0.19797255595525107, "compression_ratio": 1.7342342342342343, "no_speech_prob": 1.2028898709104396e-05}, {"id": 425, "seek": 241952, "start": 2431.48, "end": 2438.0, "text": " is another type of frog number six is lizard and so this this seems reasonable", "tokens": [307, 1071, 2010, 295, 17259, 1230, 2309, 307, 39215, 293, 370, 341, 341, 2544, 10585], "temperature": 0.0, "avg_logprob": -0.19797255595525107, "compression_ratio": 1.7342342342342343, "no_speech_prob": 1.2028898709104396e-05}, {"id": 426, "seek": 241952, "start": 2438.0, "end": 2442.52, "text": " what we would expect that it is that is capturing things near frog and these", "tokens": [437, 321, 576, 2066, 300, 309, 307, 300, 307, 23384, 721, 2651, 17259, 293, 613], "temperature": 0.0, "avg_logprob": -0.19797255595525107, "compression_ratio": 1.7342342342342343, "no_speech_prob": 1.2028898709104396e-05}, {"id": 427, "seek": 241952, "start": 2442.52, "end": 2446.06, "text": " different types of frogs are all close together and in terms of what their", "tokens": [819, 3467, 295, 37107, 366, 439, 1998, 1214, 293, 294, 2115, 295, 437, 641], "temperature": 0.0, "avg_logprob": -0.19797255595525107, "compression_ratio": 1.7342342342342343, "no_speech_prob": 1.2028898709104396e-05}, {"id": 428, "seek": 244606, "start": 2446.06, "end": 2455.88, "text": " vectors look like okay so I'm going to switch over to the Jupiter notebook demo", "tokens": [18875, 574, 411, 1392, 370, 286, 478, 516, 281, 3679, 670, 281, 264, 24567, 21060, 10723], "temperature": 0.0, "avg_logprob": -0.15616005186050658, "compression_ratio": 1.4259259259259258, "no_speech_prob": 8.138504199450836e-06}, {"id": 429, "seek": 244606, "start": 2455.88, "end": 2464.36, "text": " now and for that if you haven't already seen it it's at github.com you can go to", "tokens": [586, 293, 337, 300, 498, 291, 2378, 380, 1217, 1612, 309, 309, 311, 412, 290, 355, 836, 13, 1112, 291, 393, 352, 281], "temperature": 0.0, "avg_logprob": -0.15616005186050658, "compression_ratio": 1.4259259259259258, "no_speech_prob": 8.138504199450836e-06}, {"id": 430, "seek": 244606, "start": 2464.36, "end": 2473.7999999999997, "text": " slash fast AI slash word- embeddings-workshop to get the the materials", "tokens": [17330, 2370, 7318, 17330, 1349, 12, 12240, 29432, 12, 1902, 29431, 281, 483, 264, 264, 5319], "temperature": 0.0, "avg_logprob": -0.15616005186050658, "compression_ratio": 1.4259259259259258, "no_speech_prob": 8.138504199450836e-06}, {"id": 431, "seek": 247380, "start": 2473.8, "end": 2479.4, "text": " needed although I've tried to create this that I think it'll be interesting to", "tokens": [2978, 4878, 286, 600, 3031, 281, 1884, 341, 300, 286, 519, 309, 603, 312, 1880, 281], "temperature": 0.0, "avg_logprob": -0.16818437170475087, "compression_ratio": 1.4710144927536233, "no_speech_prob": 8.089909533737227e-05}, {"id": 432, "seek": 247380, "start": 2479.4, "end": 2484.1200000000003, "text": " watch even if you don't have it running on your computer but everything is", "tokens": [1159, 754, 498, 291, 500, 380, 362, 309, 2614, 322, 428, 3820, 457, 1203, 307], "temperature": 0.0, "avg_logprob": -0.16818437170475087, "compression_ratio": 1.4710144927536233, "no_speech_prob": 8.089909533737227e-05}, {"id": 433, "seek": 248412, "start": 2484.12, "end": 2505.24, "text": " available there so that you can run this yourself and then actually let me get a", "tokens": [2435, 456, 370, 300, 291, 393, 1190, 341, 1803, 293, 550, 767, 718, 385, 483, 257], "temperature": 0.0, "avg_logprob": -0.1987497226611988, "compression_ratio": 1.3389830508474576, "no_speech_prob": 5.421631158242235e-06}, {"id": 434, "seek": 248412, "start": 2505.24, "end": 2511.2799999999997, "text": " show of hands who who's used Jupiter notebooks before okay it looks like most", "tokens": [855, 295, 2377, 567, 567, 311, 1143, 24567, 43782, 949, 1392, 309, 1542, 411, 881], "temperature": 0.0, "avg_logprob": -0.1987497226611988, "compression_ratio": 1.3389830508474576, "no_speech_prob": 5.421631158242235e-06}, {"id": 435, "seek": 251128, "start": 2511.28, "end": 2516.0, "text": " people who's new to Jupiter notebooks oh okay a lot of people and so let me talk", "tokens": [561, 567, 311, 777, 281, 24567, 43782, 1954, 1392, 257, 688, 295, 561, 293, 370, 718, 385, 751], "temperature": 0.0, "avg_logprob": -0.1443976226605867, "compression_ratio": 1.747191011235955, "no_speech_prob": 4.637563051801408e-06}, {"id": 436, "seek": 251128, "start": 2516.0, "end": 2523.6400000000003, "text": " a little bit about Jupiter notebooks as well so let me go to the github page so", "tokens": [257, 707, 857, 466, 24567, 43782, 382, 731, 370, 718, 385, 352, 281, 264, 290, 355, 836, 3028, 370], "temperature": 0.0, "avg_logprob": -0.1443976226605867, "compression_ratio": 1.747191011235955, "no_speech_prob": 4.637563051801408e-06}, {"id": 437, "seek": 251128, "start": 2523.6400000000003, "end": 2530.1600000000003, "text": " Jupiter notebooks are super useful and very widely used particularly by data", "tokens": [24567, 43782, 366, 1687, 4420, 293, 588, 13371, 1143, 4098, 538, 1412], "temperature": 0.0, "avg_logprob": -0.1443976226605867, "compression_ratio": 1.747191011235955, "no_speech_prob": 4.637563051801408e-06}, {"id": 438, "seek": 251128, "start": 2530.1600000000003, "end": 2535.6000000000004, "text": " scientists and what they what they allow you to do is I'll go back to the", "tokens": [7708, 293, 437, 436, 437, 436, 2089, 291, 281, 360, 307, 286, 603, 352, 646, 281, 264], "temperature": 0.0, "avg_logprob": -0.1443976226605867, "compression_ratio": 1.747191011235955, "no_speech_prob": 4.637563051801408e-06}, {"id": 439, "seek": 253560, "start": 2535.6, "end": 2542.68, "text": " notebook in a web browser to mix text and code and diagrams and math equations", "tokens": [21060, 294, 257, 3670, 11185, 281, 2890, 2487, 293, 3089, 293, 36709, 293, 5221, 11787], "temperature": 0.0, "avg_logprob": -0.08699480322904365, "compression_ratio": 1.7366071428571428, "no_speech_prob": 1.4508410458802246e-05}, {"id": 440, "seek": 253560, "start": 2542.68, "end": 2546.36, "text": " and pictures and so they're also a really good teaching tool and there are", "tokens": [293, 5242, 293, 370, 436, 434, 611, 257, 534, 665, 4571, 2290, 293, 456, 366], "temperature": 0.0, "avg_logprob": -0.08699480322904365, "compression_ratio": 1.7366071428571428, "no_speech_prob": 1.4508410458802246e-05}, {"id": 441, "seek": 253560, "start": 2546.36, "end": 2551.4, "text": " a lot of kind of online textbooks now appearing as Jupiter notebooks and to", "tokens": [257, 688, 295, 733, 295, 2950, 33587, 586, 19870, 382, 24567, 43782, 293, 281], "temperature": 0.0, "avg_logprob": -0.08699480322904365, "compression_ratio": 1.7366071428571428, "no_speech_prob": 1.4508410458802246e-05}, {"id": 442, "seek": 253560, "start": 2551.4, "end": 2555.24, "text": " create something that you can share with others that has you know has text and", "tokens": [1884, 746, 300, 291, 393, 2073, 365, 2357, 300, 575, 291, 458, 575, 2487, 293], "temperature": 0.0, "avg_logprob": -0.08699480322904365, "compression_ratio": 1.7366071428571428, "no_speech_prob": 1.4508410458802246e-05}, {"id": 443, "seek": 253560, "start": 2555.24, "end": 2560.92, "text": " links and then these interactive blocks where you can write code but just to get", "tokens": [6123, 293, 550, 613, 15141, 8474, 689, 291, 393, 2464, 3089, 457, 445, 281, 483], "temperature": 0.0, "avg_logprob": -0.08699480322904365, "compression_ratio": 1.7366071428571428, "no_speech_prob": 1.4508410458802246e-05}, {"id": 444, "seek": 256092, "start": 2560.92, "end": 2567.7200000000003, "text": " set up with these so here this would be from the console you can use W get to", "tokens": [992, 493, 365, 613, 370, 510, 341, 576, 312, 490, 264, 11076, 291, 393, 764, 343, 483, 281], "temperature": 0.0, "avg_logprob": -0.11955307103410552, "compression_ratio": 1.6122448979591837, "no_speech_prob": 1.3005880646232981e-05}, {"id": 445, "seek": 256092, "start": 2567.7200000000003, "end": 2576.44, "text": " get the the glove embeddings and so here I've zipped up 50 dimensional version as", "tokens": [483, 264, 264, 26928, 12240, 29432, 293, 370, 510, 286, 600, 710, 5529, 493, 2625, 18795, 3037, 382], "temperature": 0.0, "avg_logprob": -0.11955307103410552, "compression_ratio": 1.6122448979591837, "no_speech_prob": 1.3005880646232981e-05}, {"id": 446, "seek": 256092, "start": 2576.44, "end": 2584.52, "text": " well as a hundred dimensional version of glove and then you can kind of tar them", "tokens": [731, 382, 257, 3262, 18795, 3037, 295, 26928, 293, 550, 291, 393, 733, 295, 3112, 552], "temperature": 0.0, "avg_logprob": -0.11955307103410552, "compression_ratio": 1.6122448979591837, "no_speech_prob": 1.3005880646232981e-05}, {"id": 447, "seek": 256092, "start": 2584.52, "end": 2588.7200000000003, "text": " to open them and I'll be using Python so you might want to create a virtual", "tokens": [281, 1269, 552, 293, 286, 603, 312, 1228, 15329, 370, 291, 1062, 528, 281, 1884, 257, 6374], "temperature": 0.0, "avg_logprob": -0.11955307103410552, "compression_ratio": 1.6122448979591837, "no_speech_prob": 1.3005880646232981e-05}, {"id": 448, "seek": 258872, "start": 2588.72, "end": 2593.3199999999997, "text": " environment unless you already have anaconda or kind of the scientific", "tokens": [2823, 5969, 291, 1217, 362, 364, 326, 12233, 420, 733, 295, 264, 8134], "temperature": 0.0, "avg_logprob": -0.1486751444199506, "compression_ratio": 1.7188940092165899, "no_speech_prob": 1.384543429594487e-05}, {"id": 449, "seek": 258872, "start": 2593.3199999999997, "end": 2598.3599999999997, "text": " Python stack installed on your computer and what you do to start a Jupiter", "tokens": [15329, 8630, 8899, 322, 428, 3820, 293, 437, 291, 360, 281, 722, 257, 24567], "temperature": 0.0, "avg_logprob": -0.1486751444199506, "compression_ratio": 1.7188940092165899, "no_speech_prob": 1.384543429594487e-05}, {"id": 450, "seek": 258872, "start": 2598.3599999999997, "end": 2603.9199999999996, "text": " notebook is type Jupiter notebook into the console and then your web browser", "tokens": [21060, 307, 2010, 24567, 21060, 666, 264, 11076, 293, 550, 428, 3670, 11185], "temperature": 0.0, "avg_logprob": -0.1486751444199506, "compression_ratio": 1.7188940092165899, "no_speech_prob": 1.384543429594487e-05}, {"id": 451, "seek": 258872, "start": 2603.9199999999996, "end": 2609.9199999999996, "text": " should kind of pop up with it with it running and feel feel free to stop me if", "tokens": [820, 733, 295, 1665, 493, 365, 309, 365, 309, 2614, 293, 841, 841, 1737, 281, 1590, 385, 498], "temperature": 0.0, "avg_logprob": -0.1486751444199506, "compression_ratio": 1.7188940092165899, "no_speech_prob": 1.384543429594487e-05}, {"id": 452, "seek": 258872, "start": 2609.9199999999996, "end": 2618.0, "text": " you have questions so kind of setting out on this we've got some import", "tokens": [291, 362, 1651, 370, 733, 295, 3287, 484, 322, 341, 321, 600, 658, 512, 974], "temperature": 0.0, "avg_logprob": -0.1486751444199506, "compression_ratio": 1.7188940092165899, "no_speech_prob": 1.384543429594487e-05}, {"id": 453, "seek": 261800, "start": 2618.0, "end": 2623.12, "text": " statements just to import the libraries we need here I've put the same", "tokens": [12363, 445, 281, 974, 264, 15148, 321, 643, 510, 286, 600, 829, 264, 912], "temperature": 0.0, "avg_logprob": -0.1126263902542439, "compression_ratio": 1.7008928571428572, "no_speech_prob": 2.7105617846245877e-05}, {"id": 454, "seek": 261800, "start": 2623.12, "end": 2629.96, "text": " information that's on github of where to go to get the data and you'll have to", "tokens": [1589, 300, 311, 322, 290, 355, 836, 295, 689, 281, 352, 281, 483, 264, 1412, 293, 291, 603, 362, 281], "temperature": 0.0, "avg_logprob": -0.1126263902542439, "compression_ratio": 1.7008928571428572, "no_speech_prob": 2.7105617846245877e-05}, {"id": 455, "seek": 261800, "start": 2629.96, "end": 2634.24, "text": " update this kind of based on you know where in your computer you know put it", "tokens": [5623, 341, 733, 295, 2361, 322, 291, 458, 689, 294, 428, 3820, 291, 458, 829, 309], "temperature": 0.0, "avg_logprob": -0.1126263902542439, "compression_ratio": 1.7008928571428572, "no_speech_prob": 2.7105617846245877e-05}, {"id": 456, "seek": 261800, "start": 2634.24, "end": 2637.72, "text": " in the same directory is where you're running Jupiter notebook from so that", "tokens": [294, 264, 912, 21120, 307, 689, 291, 434, 2614, 24567, 21060, 490, 370, 300], "temperature": 0.0, "avg_logprob": -0.1126263902542439, "compression_ratio": 1.7008928571428572, "no_speech_prob": 2.7105617846245877e-05}, {"id": 457, "seek": 261800, "start": 2637.72, "end": 2644.12, "text": " you can access it I'm using numpy which is kind of a core numerical library in", "tokens": [291, 393, 2105, 309, 286, 478, 1228, 1031, 8200, 597, 307, 733, 295, 257, 4965, 29054, 6405, 294], "temperature": 0.0, "avg_logprob": -0.1126263902542439, "compression_ratio": 1.7008928571428572, "no_speech_prob": 2.7105617846245877e-05}, {"id": 458, "seek": 264412, "start": 2644.12, "end": 2650.0, "text": " Python that's yeah very fast and optimized and very useful and I've", "tokens": [15329, 300, 311, 1338, 588, 2370, 293, 26941, 293, 588, 4420, 293, 286, 600], "temperature": 0.0, "avg_logprob": -0.16558346541031546, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3006252629566006e-05}, {"id": 459, "seek": 264412, "start": 2650.0, "end": 2656.72, "text": " state saved the glove vectors as numpy arrays and so we're using numpy dot load", "tokens": [1785, 6624, 264, 26928, 18875, 382, 1031, 8200, 41011, 293, 370, 321, 434, 1228, 1031, 8200, 5893, 3677], "temperature": 0.0, "avg_logprob": -0.16558346541031546, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3006252629566006e-05}, {"id": 460, "seek": 264412, "start": 2656.72, "end": 2664.48, "text": " to open these files and I think it's help it or most helpful kind of whenever", "tokens": [281, 1269, 613, 7098, 293, 286, 519, 309, 311, 854, 309, 420, 881, 4961, 733, 295, 5699], "temperature": 0.0, "avg_logprob": -0.16558346541031546, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3006252629566006e-05}, {"id": 461, "seek": 264412, "start": 2664.48, "end": 2669.56, "text": " you have different objects that you're using in Python or any language it's", "tokens": [291, 362, 819, 6565, 300, 291, 434, 1228, 294, 15329, 420, 604, 2856, 309, 311], "temperature": 0.0, "avg_logprob": -0.16558346541031546, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3006252629566006e-05}, {"id": 462, "seek": 264412, "start": 2669.56, "end": 2673.2799999999997, "text": " really just nice to see what is your data look like like what are your objects", "tokens": [534, 445, 1481, 281, 536, 437, 307, 428, 1412, 574, 411, 411, 437, 366, 428, 6565], "temperature": 0.0, "avg_logprob": -0.16558346541031546, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3006252629566006e-05}, {"id": 463, "seek": 267328, "start": 2673.28, "end": 2681.44, "text": " so here we're going to look at words words is a list it is 400,000 long so", "tokens": [370, 510, 321, 434, 516, 281, 574, 412, 2283, 2283, 307, 257, 1329, 309, 307, 8423, 11, 1360, 938, 370], "temperature": 0.0, "avg_logprob": -0.10228982279377599, "compression_ratio": 1.6083916083916083, "no_speech_prob": 5.2246559789637104e-05}, {"id": 464, "seek": 267328, "start": 2681.44, "end": 2688.6400000000003, "text": " we've got a list of 400,000 words and doing words square brackets colon 10", "tokens": [321, 600, 658, 257, 1329, 295, 8423, 11, 1360, 2283, 293, 884, 2283, 3732, 26179, 8255, 1266], "temperature": 0.0, "avg_logprob": -0.10228982279377599, "compression_ratio": 1.6083916083916083, "no_speech_prob": 5.2246559789637104e-05}, {"id": 465, "seek": 267328, "start": 2688.6400000000003, "end": 2694.0400000000004, "text": " that slices off the first 10 elements of the list and so we can see words starts", "tokens": [300, 19793, 766, 264, 700, 1266, 4959, 295, 264, 1329, 293, 370, 321, 393, 536, 2283, 3719], "temperature": 0.0, "avg_logprob": -0.10228982279377599, "compression_ratio": 1.6083916083916083, "no_speech_prob": 5.2246559789637104e-05}, {"id": 466, "seek": 269404, "start": 2694.04, "end": 2704.48, "text": " the comma period of two and in a double quotes so that's those are words or and", "tokens": [264, 22117, 2896, 295, 732, 293, 294, 257, 3834, 19963, 370, 300, 311, 729, 366, 2283, 420, 293], "temperature": 0.0, "avg_logprob": -0.13496479545671916, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.64609740144806e-06}, {"id": 467, "seek": 269404, "start": 2704.48, "end": 2709.08, "text": " here it's defined a bit loosely you know that we're including period and comma I", "tokens": [510, 309, 311, 7642, 257, 857, 37966, 291, 458, 300, 321, 434, 3009, 2896, 293, 22117, 286], "temperature": 0.0, "avg_logprob": -0.13496479545671916, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.64609740144806e-06}, {"id": 468, "seek": 269404, "start": 2709.08, "end": 2712.68, "text": " thought it'd be more interesting to look later in the list since we just kind of", "tokens": [1194, 309, 1116, 312, 544, 1880, 281, 574, 1780, 294, 264, 1329, 1670, 321, 445, 733, 295], "temperature": 0.0, "avg_logprob": -0.13496479545671916, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.64609740144806e-06}, {"id": 469, "seek": 269404, "start": 2712.68, "end": 2716.7599999999998, "text": " had these short ones at the beginning so I just randomly decided let's look at", "tokens": [632, 613, 2099, 2306, 412, 264, 2863, 370, 286, 445, 16979, 3047, 718, 311, 574, 412], "temperature": 0.0, "avg_logprob": -0.13496479545671916, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.64609740144806e-06}, {"id": 470, "seek": 269404, "start": 2716.7599999999998, "end": 2722.24, "text": " what said place 600 to 610 in our word list and I get these words together", "tokens": [437, 848, 1081, 11849, 281, 1386, 3279, 294, 527, 1349, 1329, 293, 286, 483, 613, 2283, 1214], "temperature": 0.0, "avg_logprob": -0.13496479545671916, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.64609740144806e-06}, {"id": 471, "seek": 272224, "start": 2722.24, "end": 2726.3599999999997, "text": " Congress index Australia results and so on", "tokens": [6426, 8186, 7060, 3542, 293, 370, 322], "temperature": 0.0, "avg_logprob": -0.21936344032856955, "compression_ratio": 1.5136612021857923, "no_speech_prob": 1.750169030856341e-05}, {"id": 472, "seek": 272224, "start": 2726.3599999999997, "end": 2732.7999999999997, "text": " actually is good or should I go larger on this larger okay yeah and feel free to", "tokens": [767, 307, 665, 420, 820, 286, 352, 4833, 322, 341, 4833, 1392, 1338, 293, 841, 1737, 281], "temperature": 0.0, "avg_logprob": -0.21936344032856955, "compression_ratio": 1.5136612021857923, "no_speech_prob": 1.750169030856341e-05}, {"id": 473, "seek": 272224, "start": 2732.7999999999997, "end": 2739.9599999999996, "text": " shout out stuff like that if one more yeah if you're having trouble seeing", "tokens": [8043, 484, 1507, 411, 300, 498, 472, 544, 1338, 498, 291, 434, 1419, 5253, 2577], "temperature": 0.0, "avg_logprob": -0.21936344032856955, "compression_ratio": 1.5136612021857923, "no_speech_prob": 1.750169030856341e-05}, {"id": 474, "seek": 272224, "start": 2739.9599999999996, "end": 2746.9599999999996, "text": " something or something's unclear and so this is words it's just a 400,000 long", "tokens": [746, 420, 746, 311, 25636, 293, 370, 341, 307, 2283, 309, 311, 445, 257, 8423, 11, 1360, 938], "temperature": 0.0, "avg_logprob": -0.21936344032856955, "compression_ratio": 1.5136612021857923, "no_speech_prob": 1.750169030856341e-05}, {"id": 475, "seek": 274696, "start": 2746.96, "end": 2759.88, "text": " list of words their questions so far and so these I actually don't remember", "tokens": [1329, 295, 2283, 641, 1651, 370, 1400, 293, 370, 613, 286, 767, 500, 380, 1604], "temperature": 0.0, "avg_logprob": -0.15403749324657298, "compression_ratio": 1.4654088050314464, "no_speech_prob": 7.64625110605266e-06}, {"id": 476, "seek": 274696, "start": 2759.88, "end": 2765.2, "text": " exactly what text love was trained on and I would guess that these words might", "tokens": [2293, 437, 2487, 959, 390, 8895, 322, 293, 286, 576, 2041, 300, 613, 2283, 1062], "temperature": 0.0, "avg_logprob": -0.15403749324657298, "compression_ratio": 1.4654088050314464, "no_speech_prob": 7.64625110605266e-06}, {"id": 477, "seek": 274696, "start": 2765.2, "end": 2770.4, "text": " be ordered by frequency since the ones in the the first 10 are all very common", "tokens": [312, 8866, 538, 7893, 1670, 264, 2306, 294, 264, 264, 700, 1266, 366, 439, 588, 2689], "temperature": 0.0, "avg_logprob": -0.15403749324657298, "compression_ratio": 1.4654088050314464, "no_speech_prob": 7.64625110605266e-06}, {"id": 478, "seek": 277040, "start": 2770.4, "end": 2782.64, "text": " words but this would be something like Wikipedia or a bunch of a bunch of", "tokens": [2283, 457, 341, 576, 312, 746, 411, 28999, 420, 257, 3840, 295, 257, 3840, 295], "temperature": 0.0, "avg_logprob": -0.1286643425623576, "compression_ratio": 1.471698113207547, "no_speech_prob": 4.6376162572414614e-06}, {"id": 479, "seek": 277040, "start": 2782.64, "end": 2793.52, "text": " internet sites or you know in the case of Google they've used Google Books and", "tokens": [4705, 7533, 420, 291, 458, 294, 264, 1389, 295, 3329, 436, 600, 1143, 3329, 33843, 293], "temperature": 0.0, "avg_logprob": -0.1286643425623576, "compression_ratio": 1.471698113207547, "no_speech_prob": 4.6376162572414614e-06}, {"id": 480, "seek": 277040, "start": 2793.52, "end": 2798.4, "text": " it is possible you know 400,000 words is a lot but it's not all words so we might", "tokens": [309, 307, 1944, 291, 458, 8423, 11, 1360, 2283, 307, 257, 688, 457, 309, 311, 406, 439, 2283, 370, 321, 1062], "temperature": 0.0, "avg_logprob": -0.1286643425623576, "compression_ratio": 1.471698113207547, "no_speech_prob": 4.6376162572414614e-06}, {"id": 481, "seek": 279840, "start": 2798.4, "end": 2804.12, "text": " you know want to see a word later that's not in there so then another item that", "tokens": [291, 458, 528, 281, 536, 257, 1349, 1780, 300, 311, 406, 294, 456, 370, 550, 1071, 3174, 300], "temperature": 0.0, "avg_logprob": -0.12781723178162865, "compression_ratio": 1.6828193832599119, "no_speech_prob": 4.0689221350476146e-05}, {"id": 482, "seek": 279840, "start": 2804.12, "end": 2811.56, "text": " we've loaded is word IDX and actually I should be oh and so some keyboard", "tokens": [321, 600, 13210, 307, 1349, 7348, 55, 293, 767, 286, 820, 312, 1954, 293, 370, 512, 10186], "temperature": 0.0, "avg_logprob": -0.12781723178162865, "compression_ratio": 1.6828193832599119, "no_speech_prob": 4.0689221350476146e-05}, {"id": 483, "seek": 279840, "start": 2811.56, "end": 2817.2400000000002, "text": " commands for Jupiter notebook you can type B to open up so you can switch", "tokens": [16901, 337, 24567, 21060, 291, 393, 2010, 363, 281, 1269, 493, 370, 291, 393, 3679], "temperature": 0.0, "avg_logprob": -0.12781723178162865, "compression_ratio": 1.6828193832599119, "no_speech_prob": 4.0689221350476146e-05}, {"id": 484, "seek": 279840, "start": 2817.2400000000002, "end": 2822.84, "text": " between when you hit escape you're kind of a see there's a cursor on the cell", "tokens": [1296, 562, 291, 2045, 7615, 291, 434, 733, 295, 257, 536, 456, 311, 257, 28169, 322, 264, 2815], "temperature": 0.0, "avg_logprob": -0.12781723178162865, "compression_ratio": 1.6828193832599119, "no_speech_prob": 4.0689221350476146e-05}, {"id": 485, "seek": 279840, "start": 2822.84, "end": 2827.1600000000003, "text": " but you're not in it and then you hit enter to kind of start typing and then", "tokens": [457, 291, 434, 406, 294, 309, 293, 550, 291, 2045, 3242, 281, 733, 295, 722, 18444, 293, 550], "temperature": 0.0, "avg_logprob": -0.12781723178162865, "compression_ratio": 1.6828193832599119, "no_speech_prob": 4.0689221350476146e-05}, {"id": 486, "seek": 282716, "start": 2827.16, "end": 2835.64, "text": " you can go escape to get back out so you can check the type of word IDX and we", "tokens": [291, 393, 352, 7615, 281, 483, 646, 484, 370, 291, 393, 1520, 264, 2010, 295, 1349, 7348, 55, 293, 321], "temperature": 0.0, "avg_logprob": -0.11872284321845332, "compression_ratio": 1.544502617801047, "no_speech_prob": 1.363075807603309e-05}, {"id": 487, "seek": 282716, "start": 2835.64, "end": 2840.68, "text": " have tab completion which is nice and then shift enter will run the cell so", "tokens": [362, 4421, 19372, 597, 307, 1481, 293, 550, 5513, 3242, 486, 1190, 264, 2815, 370], "temperature": 0.0, "avg_logprob": -0.11872284321845332, "compression_ratio": 1.544502617801047, "no_speech_prob": 1.363075807603309e-05}, {"id": 488, "seek": 282716, "start": 2840.68, "end": 2847.16, "text": " it's telling us word IDX is a dictionary we can look up to see is", "tokens": [309, 311, 3585, 505, 1349, 7348, 55, 307, 257, 25890, 321, 393, 574, 493, 281, 536, 307], "temperature": 0.0, "avg_logprob": -0.11872284321845332, "compression_ratio": 1.544502617801047, "no_speech_prob": 1.363075807603309e-05}, {"id": 489, "seek": 282716, "start": 2847.16, "end": 2855.12, "text": " intelligence in there it is this is saying it's at spot 1226 or sorry word", "tokens": [7599, 294, 456, 309, 307, 341, 307, 1566, 309, 311, 412, 4008, 2272, 10880, 420, 2597, 1349], "temperature": 0.0, "avg_logprob": -0.11872284321845332, "compression_ratio": 1.544502617801047, "no_speech_prob": 1.363075807603309e-05}, {"id": 490, "seek": 285512, "start": 2855.12, "end": 2862.04, "text": " IDX dictionary it's in there key intelligence has value 1226 and I was", "tokens": [7348, 55, 25890, 309, 311, 294, 456, 2141, 7599, 575, 2158, 2272, 10880, 293, 286, 390], "temperature": 0.0, "avg_logprob": -0.1086867984972502, "compression_ratio": 1.4522292993630572, "no_speech_prob": 9.972561201720964e-06}, {"id": 491, "seek": 285512, "start": 2862.04, "end": 2868.96, "text": " getting ahead of myself and in that case what this means is that index 1226 of", "tokens": [1242, 2286, 295, 2059, 293, 294, 300, 1389, 437, 341, 1355, 307, 300, 8186, 2272, 10880, 295], "temperature": 0.0, "avg_logprob": -0.1086867984972502, "compression_ratio": 1.4522292993630572, "no_speech_prob": 9.972561201720964e-06}, {"id": 492, "seek": 285512, "start": 2868.96, "end": 2874.3199999999997, "text": " our words list is intelligence and so this is giving us a way to look specific", "tokens": [527, 2283, 1329, 307, 7599, 293, 370, 341, 307, 2902, 505, 257, 636, 281, 574, 2685], "temperature": 0.0, "avg_logprob": -0.1086867984972502, "compression_ratio": 1.4522292993630572, "no_speech_prob": 9.972561201720964e-06}, {"id": 493, "seek": 287432, "start": 2874.32, "end": 2890.6800000000003, "text": " words up in our list and see if they're included questions so the question was", "tokens": [2283, 493, 294, 527, 1329, 293, 536, 498, 436, 434, 5556, 1651, 370, 264, 1168, 390], "temperature": 0.0, "avg_logprob": -0.13729116916656495, "compression_ratio": 1.625, "no_speech_prob": 4.222753887006547e-06}, {"id": 494, "seek": 287432, "start": 2890.6800000000003, "end": 2903.8, "text": " is a list of words a word vector no no so the the word vector the word vector", "tokens": [307, 257, 1329, 295, 2283, 257, 1349, 8062, 572, 572, 370, 264, 264, 1349, 8062, 264, 1349, 8062], "temperature": 0.0, "avg_logprob": -0.13729116916656495, "compression_ratio": 1.625, "no_speech_prob": 4.222753887006547e-06}, {"id": 495, "seek": 290380, "start": 2903.8, "end": 2911.8, "text": " would be we'll get to that in the moment the word vector is like the embedding", "tokens": [576, 312, 321, 603, 483, 281, 300, 294, 264, 1623, 264, 1349, 8062, 307, 411, 264, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.13508366902669272, "compression_ratio": 1.718232044198895, "no_speech_prob": 1.544581573398318e-05}, {"id": 496, "seek": 290380, "start": 2911.8, "end": 2919.28, "text": " yeah so the embedding for a particular word where's this list of words is yeah", "tokens": [1338, 370, 264, 12240, 3584, 337, 257, 1729, 1349, 689, 311, 341, 1329, 295, 2283, 307, 1338], "temperature": 0.0, "avg_logprob": -0.13508366902669272, "compression_ratio": 1.718232044198895, "no_speech_prob": 1.544581573398318e-05}, {"id": 497, "seek": 290380, "start": 2919.28, "end": 2925.4, "text": " just letting us know which which words we have word vectors for good question", "tokens": [445, 8295, 505, 458, 597, 597, 2283, 321, 362, 1349, 18875, 337, 665, 1168], "temperature": 0.0, "avg_logprob": -0.13508366902669272, "compression_ratio": 1.718232044198895, "no_speech_prob": 1.544581573398318e-05}, {"id": 498, "seek": 290380, "start": 2925.4, "end": 2931.04, "text": " yeah it's really um yes it's good to feel comfortable with what the objects", "tokens": [1338, 309, 311, 534, 1105, 2086, 309, 311, 665, 281, 841, 4619, 365, 437, 264, 6565], "temperature": 0.0, "avg_logprob": -0.13508366902669272, "compression_ratio": 1.718232044198895, "no_speech_prob": 1.544581573398318e-05}, {"id": 499, "seek": 293104, "start": 2931.04, "end": 2937.24, "text": " will be using are any other questions", "tokens": [486, 312, 1228, 366, 604, 661, 1651], "temperature": 0.0, "avg_logprob": -0.23132640910598468, "compression_ratio": 1.375886524822695, "no_speech_prob": 9.97254392132163e-06}, {"id": 500, "seek": 293104, "start": 2940.2, "end": 2947.6, "text": " all right does anyone want to suggest a word and I can look it up see if it's in", "tokens": [439, 558, 775, 2878, 528, 281, 3402, 257, 1349, 293, 286, 393, 574, 309, 493, 536, 498, 309, 311, 294], "temperature": 0.0, "avg_logprob": -0.23132640910598468, "compression_ratio": 1.375886524822695, "no_speech_prob": 9.97254392132163e-06}, {"id": 501, "seek": 293104, "start": 2947.6, "end": 2958.04, "text": " here feminist good one we're at 118 53 and then just to confirm that that's", "tokens": [510, 26229, 665, 472, 321, 434, 412, 2975, 23, 21860, 293, 550, 445, 281, 9064, 300, 300, 311], "temperature": 0.0, "avg_logprob": -0.23132640910598468, "compression_ratio": 1.375886524822695, "no_speech_prob": 9.97254392132163e-06}, {"id": 502, "seek": 295804, "start": 2958.04, "end": 2966.36, "text": " accurate type that into here and it is so we've got this relationship between", "tokens": [8559, 2010, 300, 666, 510, 293, 309, 307, 370, 321, 600, 658, 341, 2480, 1296], "temperature": 0.0, "avg_logprob": -0.11583749135335286, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.260651242773747e-05}, {"id": 503, "seek": 295804, "start": 2966.36, "end": 2971.04, "text": " word index lets us go from word to indices and words lets us go from", "tokens": [1349, 8186, 6653, 505, 352, 490, 1349, 281, 43840, 293, 2283, 6653, 505, 352, 490], "temperature": 0.0, "avg_logprob": -0.11583749135335286, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.260651242773747e-05}, {"id": 504, "seek": 295804, "start": 2971.04, "end": 2977.64, "text": " indices to words so they're kind of a inverses of each other and so far what", "tokens": [43840, 281, 2283, 370, 436, 434, 733, 295, 257, 21378, 279, 295, 1184, 661, 293, 370, 1400, 437], "temperature": 0.0, "avg_logprob": -0.11583749135335286, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.260651242773747e-05}, {"id": 505, "seek": 295804, "start": 2977.64, "end": 2983.4, "text": " I've given you is just equivalent to numbering words in a dictionary so we're", "tokens": [286, 600, 2212, 291, 307, 445, 10344, 281, 1230, 278, 2283, 294, 257, 25890, 370, 321, 434], "temperature": 0.0, "avg_logprob": -0.11583749135335286, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.260651242773747e-05}, {"id": 506, "seek": 298340, "start": 2983.4, "end": 2995.88, "text": " still not able to say what words are similar to each other yes yes that's a", "tokens": [920, 406, 1075, 281, 584, 437, 2283, 366, 2531, 281, 1184, 661, 2086, 2086, 300, 311, 257], "temperature": 0.0, "avg_logprob": -0.13099225774987952, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.2029178833472542e-05}, {"id": 507, "seek": 298340, "start": 2995.88, "end": 3000.08, "text": " great question so we're gonna get to that in a moment yeah and yeah now I am", "tokens": [869, 1168, 370, 321, 434, 799, 483, 281, 300, 294, 257, 1623, 1338, 293, 1338, 586, 286, 669], "temperature": 0.0, "avg_logprob": -0.13099225774987952, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.2029178833472542e-05}, {"id": 508, "seek": 298340, "start": 3000.08, "end": 3006.08, "text": " really curious I used intelligence but we're gonna try it on feminist yeah so", "tokens": [534, 6369, 286, 1143, 7599, 457, 321, 434, 799, 853, 309, 322, 26229, 1338, 370], "temperature": 0.0, "avg_logprob": -0.13099225774987952, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.2029178833472542e-05}, {"id": 509, "seek": 298340, "start": 3006.08, "end": 3012.04, "text": " that now we've got that we've got this other object backs and actually let me", "tokens": [300, 586, 321, 600, 658, 300, 321, 600, 658, 341, 661, 2657, 19513, 293, 767, 718, 385], "temperature": 0.0, "avg_logprob": -0.13099225774987952, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.2029178833472542e-05}, {"id": 510, "seek": 301204, "start": 3012.04, "end": 3020.04, "text": " check the type of her backs and I'm hitting shift enter I don't know why it", "tokens": [1520, 264, 2010, 295, 720, 19513, 293, 286, 478, 8850, 5513, 3242, 286, 500, 380, 458, 983, 309], "temperature": 0.0, "avg_logprob": -0.27155832143930286, "compression_ratio": 1.3983739837398375, "no_speech_prob": 3.3732274459907785e-05}, {"id": 511, "seek": 301204, "start": 3020.04, "end": 3026.7599999999998, "text": " keeps asking me this", "tokens": [5965, 3365, 385, 341], "temperature": 0.0, "avg_logprob": -0.27155832143930286, "compression_ratio": 1.3983739837398375, "no_speech_prob": 3.3732274459907785e-05}, {"id": 512, "seek": 301204, "start": 3031.16, "end": 3041.2, "text": " and is a numpy nd array and that's that's just a matrix so numpy is kind of", "tokens": [293, 307, 257, 1031, 8200, 220, 273, 10225, 293, 300, 311, 300, 311, 445, 257, 8141, 370, 1031, 8200, 307, 733, 295], "temperature": 0.0, "avg_logprob": -0.27155832143930286, "compression_ratio": 1.3983739837398375, "no_speech_prob": 3.3732274459907785e-05}, {"id": 513, "seek": 304120, "start": 3041.2, "end": 3050.56, "text": " a primary way to store matrices so this is this is what intelligence looks like", "tokens": [257, 6194, 636, 281, 3531, 32284, 370, 341, 307, 341, 307, 437, 7599, 1542, 411], "temperature": 0.0, "avg_logprob": -0.10702812009387547, "compression_ratio": 1.711111111111111, "no_speech_prob": 5.3380022109195124e-06}, {"id": 514, "seek": 304120, "start": 3050.56, "end": 3056.56, "text": " so yeah it's actually oh yeah you can see most of it so it's a hundred numbers", "tokens": [370, 1338, 309, 311, 767, 1954, 1338, 291, 393, 536, 881, 295, 309, 370, 309, 311, 257, 3262, 3547], "temperature": 0.0, "avg_logprob": -0.10702812009387547, "compression_ratio": 1.711111111111111, "no_speech_prob": 5.3380022109195124e-06}, {"id": 515, "seek": 304120, "start": 3056.56, "end": 3062.2799999999997, "text": " and this is not not very human readable to have this array of a hundred numbers", "tokens": [293, 341, 307, 406, 406, 588, 1952, 49857, 281, 362, 341, 10225, 295, 257, 3262, 3547], "temperature": 0.0, "avg_logprob": -0.10702812009387547, "compression_ratio": 1.711111111111111, "no_speech_prob": 5.3380022109195124e-06}, {"id": 516, "seek": 304120, "start": 3062.2799999999997, "end": 3065.7999999999997, "text": " but that's how the computer is representing intelligence and then let", "tokens": [457, 300, 311, 577, 264, 3820, 307, 13460, 7599, 293, 550, 718], "temperature": 0.0, "avg_logprob": -0.10702812009387547, "compression_ratio": 1.711111111111111, "no_speech_prob": 5.3380022109195124e-06}, {"id": 517, "seek": 306580, "start": 3065.8, "end": 3074.1600000000003, "text": " me do it for feminist so feminist was 118 53", "tokens": [385, 360, 309, 337, 26229, 370, 26229, 390, 2975, 23, 21860], "temperature": 0.0, "avg_logprob": -0.18571998522831842, "compression_ratio": 1.4666666666666666, "no_speech_prob": 6.43885732642957e-06}, {"id": 518, "seek": 306580, "start": 3081.2400000000002, "end": 3089.32, "text": " so this is what feminist looks like just this list of numbers which yeah on its", "tokens": [370, 341, 307, 437, 26229, 1542, 411, 445, 341, 1329, 295, 3547, 597, 1338, 322, 1080], "temperature": 0.0, "avg_logprob": -0.18571998522831842, "compression_ratio": 1.4666666666666666, "no_speech_prob": 6.43885732642957e-06}, {"id": 519, "seek": 306580, "start": 3089.32, "end": 3095.6800000000003, "text": " own is not that helpful to us but it's gonna let us do calculations so to", "tokens": [1065, 307, 406, 300, 4961, 281, 505, 457, 309, 311, 799, 718, 505, 360, 20448, 370, 281], "temperature": 0.0, "avg_logprob": -0.18571998522831842, "compression_ratio": 1.4666666666666666, "no_speech_prob": 6.43885732642957e-06}, {"id": 520, "seek": 309568, "start": 3095.68, "end": 3100.52, "text": " see how far apart things are we're gonna want to have a concept of distance and", "tokens": [536, 577, 1400, 4936, 721, 366, 321, 434, 799, 528, 281, 362, 257, 3410, 295, 4560, 293], "temperature": 0.0, "avg_logprob": -0.09514848709106445, "compression_ratio": 1.6825396825396826, "no_speech_prob": 4.7107369027798995e-06}, {"id": 521, "seek": 309568, "start": 3100.52, "end": 3106.12, "text": " for vectors in high dimension you want to use something called cosine distance", "tokens": [337, 18875, 294, 1090, 10139, 291, 528, 281, 764, 746, 1219, 23565, 4560], "temperature": 0.0, "avg_logprob": -0.09514848709106445, "compression_ratio": 1.6825396825396826, "no_speech_prob": 4.7107369027798995e-06}, {"id": 522, "seek": 309568, "start": 3106.12, "end": 3111.3599999999997, "text": " but this is yeah just giving us exactly like it sounds a measure of how far apart", "tokens": [457, 341, 307, 1338, 445, 2902, 505, 2293, 411, 309, 3263, 257, 3481, 295, 577, 1400, 4936], "temperature": 0.0, "avg_logprob": -0.09514848709106445, "compression_ratio": 1.6825396825396826, "no_speech_prob": 4.7107369027798995e-06}, {"id": 523, "seek": 309568, "start": 3111.3599999999997, "end": 3121.48, "text": " two things are so now I've checked how far apart our puppy and dog and let me", "tokens": [732, 721, 366, 370, 586, 286, 600, 10033, 577, 1400, 4936, 527, 18196, 293, 3000, 293, 718, 385], "temperature": 0.0, "avg_logprob": -0.09514848709106445, "compression_ratio": 1.6825396825396826, "no_speech_prob": 4.7107369027798995e-06}, {"id": 524, "seek": 312148, "start": 3121.48, "end": 3128.68, "text": " say what I'm doing here so word IDX puppy that's going to return the index", "tokens": [584, 437, 286, 478, 884, 510, 370, 1349, 7348, 55, 18196, 300, 311, 516, 281, 2736, 264, 8186], "temperature": 0.0, "avg_logprob": -0.18594309748435506, "compression_ratio": 1.4428571428571428, "no_speech_prob": 3.119574466836639e-05}, {"id": 525, "seek": 312148, "start": 3128.68, "end": 3134.08, "text": " for puppy which is a number and then we look that number up in facts to get the", "tokens": [337, 18196, 597, 307, 257, 1230, 293, 550, 321, 574, 300, 1230, 493, 294, 9130, 281, 483, 264], "temperature": 0.0, "avg_logprob": -0.18594309748435506, "compression_ratio": 1.4428571428571428, "no_speech_prob": 3.119574466836639e-05}, {"id": 526, "seek": 313408, "start": 3134.08, "end": 3156.16, "text": " hundred dimensional array that represents puppy yeah so yeah for puppy", "tokens": [3262, 18795, 10225, 300, 8855, 18196, 1338, 370, 1338, 337, 18196], "temperature": 0.0, "avg_logprob": -0.13879723846912384, "compression_ratio": 1.5425531914893618, "no_speech_prob": 5.682196388079319e-06}, {"id": 527, "seek": 313408, "start": 3156.16, "end": 3160.4, "text": " look up its index use its index to look up what is the hundred dimensional", "tokens": [574, 493, 1080, 8186, 764, 1080, 8186, 281, 574, 493, 437, 307, 264, 3262, 18795], "temperature": 0.0, "avg_logprob": -0.13879723846912384, "compression_ratio": 1.5425531914893618, "no_speech_prob": 5.682196388079319e-06}, {"id": 528, "seek": 316040, "start": 3160.4, "end": 3166.08, "text": " vector and that's what's happening at this the statement and then we have to", "tokens": [8062, 293, 300, 311, 437, 311, 2737, 412, 341, 264, 5629, 293, 550, 321, 362, 281], "temperature": 0.0, "avg_logprob": -0.11063981506059754, "compression_ratio": 1.8473895582329318, "no_speech_prob": 1.1842496860481333e-05}, {"id": 529, "seek": 316040, "start": 3166.08, "end": 3170.76, "text": " do the same thing with dog find out what index is dog and then look up the", "tokens": [360, 264, 912, 551, 365, 3000, 915, 484, 437, 8186, 307, 3000, 293, 550, 574, 493, 264], "temperature": 0.0, "avg_logprob": -0.11063981506059754, "compression_ratio": 1.8473895582329318, "no_speech_prob": 1.1842496860481333e-05}, {"id": 530, "seek": 316040, "start": 3170.76, "end": 3173.92, "text": " hundred dimensional vector for dog and then say what are the distance between", "tokens": [3262, 18795, 8062, 337, 3000, 293, 550, 584, 437, 366, 264, 4560, 1296], "temperature": 0.0, "avg_logprob": -0.11063981506059754, "compression_ratio": 1.8473895582329318, "no_speech_prob": 1.1842496860481333e-05}, {"id": 531, "seek": 316040, "start": 3173.92, "end": 3180.64, "text": " those two things and it's point two seven which is a lot smaller than these", "tokens": [729, 732, 721, 293, 309, 311, 935, 732, 3407, 597, 307, 257, 688, 4356, 813, 613], "temperature": 0.0, "avg_logprob": -0.11063981506059754, "compression_ratio": 1.8473895582329318, "no_speech_prob": 1.1842496860481333e-05}, {"id": 532, "seek": 316040, "start": 3180.64, "end": 3185.36, "text": " things that are not connected so like kitten and airplane I was just trying", "tokens": [721, 300, 366, 406, 4582, 370, 411, 39696, 293, 17130, 286, 390, 445, 1382], "temperature": 0.0, "avg_logprob": -0.11063981506059754, "compression_ratio": 1.8473895582329318, "no_speech_prob": 1.1842496860481333e-05}, {"id": 533, "seek": 316040, "start": 3185.36, "end": 3188.96, "text": " to think of words that don't seem to have any connection those are point eight", "tokens": [281, 519, 295, 2283, 300, 500, 380, 1643, 281, 362, 604, 4984, 729, 366, 935, 3180], "temperature": 0.0, "avg_logprob": -0.11063981506059754, "compression_ratio": 1.8473895582329318, "no_speech_prob": 1.1842496860481333e-05}, {"id": 534, "seek": 318896, "start": 3188.96, "end": 3194.4, "text": " seven apart those are much further apart so even though the hundred dimensional", "tokens": [3407, 4936, 729, 366, 709, 3052, 4936, 370, 754, 1673, 264, 3262, 18795], "temperature": 0.0, "avg_logprob": -0.1491537460914025, "compression_ratio": 1.7342995169082125, "no_speech_prob": 1.4970412848924752e-05}, {"id": 535, "seek": 318896, "start": 3194.4, "end": 3199.28, "text": " vector you know isn't really human readable to us we can look at how it", "tokens": [8062, 291, 458, 1943, 380, 534, 1952, 49857, 281, 505, 321, 393, 574, 412, 577, 309], "temperature": 0.0, "avg_logprob": -0.1491537460914025, "compression_ratio": 1.7342995169082125, "no_speech_prob": 1.4970412848924752e-05}, {"id": 536, "seek": 318896, "start": 3199.28, "end": 3203.84, "text": " compares to other hundred dimensional vectors and we get you get the Queen and", "tokens": [38334, 281, 661, 3262, 18795, 18875, 293, 321, 483, 291, 483, 264, 10077, 293], "temperature": 0.0, "avg_logprob": -0.1491537460914025, "compression_ratio": 1.7342995169082125, "no_speech_prob": 1.4970412848924752e-05}, {"id": 537, "seek": 318896, "start": 3203.84, "end": 3209.4, "text": " Princess are just point two zero apart the words like celebrity and dusty are", "tokens": [13903, 366, 445, 935, 732, 4018, 4936, 264, 2283, 411, 18597, 293, 41973, 366], "temperature": 0.0, "avg_logprob": -0.1491537460914025, "compression_ratio": 1.7342995169082125, "no_speech_prob": 1.4970412848924752e-05}, {"id": 538, "seek": 318896, "start": 3209.4, "end": 3213.7200000000003, "text": " point nine eight since there's no connection there", "tokens": [935, 4949, 3180, 1670, 456, 311, 572, 4984, 456], "temperature": 0.0, "avg_logprob": -0.1491537460914025, "compression_ratio": 1.7342995169082125, "no_speech_prob": 1.4970412848924752e-05}, {"id": 539, "seek": 321372, "start": 3213.72, "end": 3218.8799999999997, "text": " avalanche and antique are point nine six so these are these are words that are not", "tokens": [1305, 14163, 1876, 293, 41220, 366, 935, 4949, 2309, 370, 613, 366, 613, 366, 2283, 300, 366, 406], "temperature": 0.0, "avg_logprob": -0.21771784389720245, "compression_ratio": 1.3505154639175259, "no_speech_prob": 2.0144170775893144e-05}, {"id": 540, "seek": 321372, "start": 3218.8799999999997, "end": 3229.2799999999997, "text": " close Melissa okay speak a little bit louder", "tokens": [1998, 22844, 1392, 1710, 257, 707, 857, 22717], "temperature": 0.0, "avg_logprob": -0.21771784389720245, "compression_ratio": 1.3505154639175259, "no_speech_prob": 2.0144170775893144e-05}, {"id": 541, "seek": 322928, "start": 3229.28, "end": 3238.1200000000003, "text": " yes", "tokens": [2086], "temperature": 0.0, "avg_logprob": -0.9797524452209473, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.00017387664411216974}, {"id": 542, "seek": 323812, "start": 3238.12, "end": 3241.3199999999997, "text": " mm-hmm", "tokens": [11169, 12, 10250], "temperature": 0.0, "avg_logprob": -0.21104521500436882, "compression_ratio": 1.3873873873873874, "no_speech_prob": 1.3844668501405977e-05}, {"id": 543, "seek": 323812, "start": 3258.2, "end": 3262.88, "text": " this is a yeah this is a great question so Melissa was saying that thinking", "tokens": [341, 307, 257, 1338, 341, 307, 257, 869, 1168, 370, 22844, 390, 1566, 300, 1953], "temperature": 0.0, "avg_logprob": -0.21104521500436882, "compression_ratio": 1.3873873873873874, "no_speech_prob": 1.3844668501405977e-05}, {"id": 544, "seek": 323812, "start": 3262.88, "end": 3266.56, "text": " about dimensions higher than three makes her head hurt and actually all", "tokens": [466, 12819, 2946, 813, 1045, 1669, 720, 1378, 4607, 293, 767, 439], "temperature": 0.0, "avg_logprob": -0.21104521500436882, "compression_ratio": 1.3873873873873874, "no_speech_prob": 1.3844668501405977e-05}, {"id": 545, "seek": 326656, "start": 3266.56, "end": 3270.36, "text": " mathematicians feel that way and so they typically will draw things as though", "tokens": [32811, 2567, 841, 300, 636, 293, 370, 436, 5850, 486, 2642, 721, 382, 1673], "temperature": 0.0, "avg_logprob": -0.11613670091950491, "compression_ratio": 1.7625570776255708, "no_speech_prob": 1.1658888070087414e-05}, {"id": 546, "seek": 326656, "start": 3270.36, "end": 3274.72, "text": " they're in three dimensions no matter what dimension they're in let me try", "tokens": [436, 434, 294, 1045, 12819, 572, 1871, 437, 10139, 436, 434, 294, 718, 385, 853], "temperature": 0.0, "avg_logprob": -0.11613670091950491, "compression_ratio": 1.7625570776255708, "no_speech_prob": 1.1658888070087414e-05}, {"id": 547, "seek": 326656, "start": 3274.72, "end": 3281.2, "text": " actually my stylist drawing and so Melissa was asking kind of like what does", "tokens": [767, 452, 48544, 6316, 293, 370, 22844, 390, 3365, 733, 295, 411, 437, 775], "temperature": 0.0, "avg_logprob": -0.11613670091950491, "compression_ratio": 1.7625570776255708, "no_speech_prob": 1.1658888070087414e-05}, {"id": 548, "seek": 326656, "start": 3281.2, "end": 3285.7999999999997, "text": " it look like you know do we have like baby animals to say that they're clumped", "tokens": [309, 574, 411, 291, 458, 360, 321, 362, 411, 3186, 4882, 281, 584, 300, 436, 434, 596, 1420, 292], "temperature": 0.0, "avg_logprob": -0.11613670091950491, "compression_ratio": 1.7625570776255708, "no_speech_prob": 1.1658888070087414e-05}, {"id": 549, "seek": 326656, "start": 3285.7999999999997, "end": 3293.16, "text": " together so imagining that this is just three dimensions so I'm just making a", "tokens": [1214, 370, 27798, 300, 341, 307, 445, 1045, 12819, 370, 286, 478, 445, 1455, 257], "temperature": 0.0, "avg_logprob": -0.11613670091950491, "compression_ratio": 1.7625570776255708, "no_speech_prob": 1.1658888070087414e-05}, {"id": 550, "seek": 329316, "start": 3293.16, "end": 3304.24, "text": " three-dimensional axes yeah the way you could think about it is that like maybe", "tokens": [1045, 12, 18759, 35387, 1338, 264, 636, 291, 727, 519, 466, 309, 307, 300, 411, 1310], "temperature": 0.0, "avg_logprob": -0.11447001355034965, "compression_ratio": 1.6267605633802817, "no_speech_prob": 1.3845613466401119e-05}, {"id": 551, "seek": 329316, "start": 3304.24, "end": 3311.68, "text": " this this vector and so you can just think of vectors as as line like a line", "tokens": [341, 341, 8062, 293, 370, 291, 393, 445, 519, 295, 18875, 382, 382, 1622, 411, 257, 1622], "temperature": 0.0, "avg_logprob": -0.11447001355034965, "compression_ratio": 1.6267605633802817, "no_speech_prob": 1.3845613466401119e-05}, {"id": 552, "seek": 329316, "start": 3311.68, "end": 3315.64, "text": " to a specific point kind of going from the origin to the specific point so", "tokens": [281, 257, 2685, 935, 733, 295, 516, 490, 264, 4957, 281, 264, 2685, 935, 370], "temperature": 0.0, "avg_logprob": -0.11447001355034965, "compression_ratio": 1.6267605633802817, "no_speech_prob": 1.3845613466401119e-05}, {"id": 553, "seek": 331564, "start": 3315.64, "end": 3324.72, "text": " we've got kitten and puppy pretty close together and what we're actually doing I", "tokens": [321, 600, 658, 39696, 293, 18196, 1238, 1998, 1214, 293, 437, 321, 434, 767, 884, 286], "temperature": 0.0, "avg_logprob": -0.10159436025117573, "compression_ratio": 1.6702127659574468, "no_speech_prob": 6.643238521064632e-06}, {"id": 554, "seek": 331564, "start": 3324.72, "end": 3328.0, "text": " was glossing over this is we're looking at the cosine distance which is the", "tokens": [390, 19574, 278, 670, 341, 307, 321, 434, 1237, 412, 264, 23565, 4560, 597, 307, 264], "temperature": 0.0, "avg_logprob": -0.10159436025117573, "compression_ratio": 1.6702127659574468, "no_speech_prob": 6.643238521064632e-06}, {"id": 555, "seek": 331564, "start": 3328.0, "end": 3332.2799999999997, "text": " angle between them they were just saying you know we've got two lines and there's", "tokens": [5802, 1296, 552, 436, 645, 445, 1566, 291, 458, 321, 600, 658, 732, 3876, 293, 456, 311], "temperature": 0.0, "avg_logprob": -0.10159436025117573, "compression_ratio": 1.6702127659574468, "no_speech_prob": 6.643238521064632e-06}, {"id": 556, "seek": 331564, "start": 3332.2799999999997, "end": 3338.56, "text": " a small angle between them and then this avalanche was the other word I had", "tokens": [257, 1359, 5802, 1296, 552, 293, 550, 341, 1305, 14163, 1876, 390, 264, 661, 1349, 286, 632], "temperature": 0.0, "avg_logprob": -0.10159436025117573, "compression_ratio": 1.6702127659574468, "no_speech_prob": 6.643238521064632e-06}, {"id": 557, "seek": 333856, "start": 3338.56, "end": 3346.84, "text": " avalanche might be over here change colors", "tokens": [1305, 14163, 1876, 1062, 312, 670, 510, 1319, 4577], "temperature": 0.0, "avg_logprob": -0.197333296140035, "compression_ratio": 1.5078125, "no_speech_prob": 9.08027413970558e-06}, {"id": 558, "seek": 333856, "start": 3350.84, "end": 3357.68, "text": " and the angle between kitten and avalanche is really large and so I think", "tokens": [293, 264, 5802, 1296, 39696, 293, 1305, 14163, 1876, 307, 534, 2416, 293, 370, 286, 519], "temperature": 0.0, "avg_logprob": -0.197333296140035, "compression_ratio": 1.5078125, "no_speech_prob": 9.08027413970558e-06}, {"id": 559, "seek": 333856, "start": 3357.68, "end": 3365.84, "text": " kind of yeah going to the baby animal approach you might find that yeah that", "tokens": [733, 295, 1338, 516, 281, 264, 3186, 5496, 3109, 291, 1062, 915, 300, 1338, 300], "temperature": 0.0, "avg_logprob": -0.197333296140035, "compression_ratio": 1.5078125, "no_speech_prob": 9.08027413970558e-06}, {"id": 560, "seek": 336584, "start": 3365.84, "end": 3375.76, "text": " they're kind of clustered like this if this is duckling or yeah and it's um yeah", "tokens": [436, 434, 733, 295, 596, 38624, 411, 341, 498, 341, 307, 12482, 1688, 420, 1338, 293, 309, 311, 1105, 1338], "temperature": 0.0, "avg_logprob": -0.19503583174485425, "compression_ratio": 1.5609756097560976, "no_speech_prob": 6.240717084438074e-06}, {"id": 561, "seek": 336584, "start": 3375.76, "end": 3378.6000000000004, "text": " fine to think of this as just being three dimensions even though it's a", "tokens": [2489, 281, 519, 295, 341, 382, 445, 885, 1045, 12819, 754, 1673, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.19503583174485425, "compression_ratio": 1.5609756097560976, "no_speech_prob": 6.240717084438074e-06}, {"id": 562, "seek": 336584, "start": 3378.6000000000004, "end": 3385.0, "text": " hundred because it's easier to visualize is that helpful any other questions about", "tokens": [3262, 570, 309, 311, 3571, 281, 23273, 307, 300, 4961, 604, 661, 1651, 466], "temperature": 0.0, "avg_logprob": -0.19503583174485425, "compression_ratio": 1.5609756097560976, "no_speech_prob": 6.240717084438074e-06}, {"id": 563, "seek": 336584, "start": 3385.0, "end": 3387.48, "text": " this", "tokens": [341], "temperature": 0.0, "avg_logprob": -0.19503583174485425, "compression_ratio": 1.5609756097560976, "no_speech_prob": 6.240717084438074e-06}, {"id": 564, "seek": 338748, "start": 3387.48, "end": 3411.2, "text": " yes in the back oh this is a good question so the question is to get more", "tokens": [2086, 294, 264, 646, 1954, 341, 307, 257, 665, 1168, 370, 264, 1168, 307, 281, 483, 544], "temperature": 0.0, "avg_logprob": -0.10417111714680989, "compression_ratio": 1.140625, "no_speech_prob": 9.81763605523156e-06}, {"id": 565, "seek": 341120, "start": 3411.2, "end": 3418.56, "text": " insight into meaning could we say give me all the words that have in the first", "tokens": [11269, 666, 3620, 727, 321, 584, 976, 385, 439, 264, 2283, 300, 362, 294, 264, 700], "temperature": 0.0, "avg_logprob": -0.10589980407499931, "compression_ratio": 1.641304347826087, "no_speech_prob": 8.26711857371265e-06}, {"id": 566, "seek": 341120, "start": 3418.56, "end": 3422.9199999999996, "text": " location of one you know and try to pick out then is there something that those", "tokens": [4914, 295, 472, 291, 458, 293, 853, 281, 1888, 484, 550, 307, 456, 746, 300, 729], "temperature": 0.0, "avg_logprob": -0.10589980407499931, "compression_ratio": 1.641304347826087, "no_speech_prob": 8.26711857371265e-06}, {"id": 567, "seek": 341120, "start": 3422.9199999999996, "end": 3432.48, "text": " words have in common I think it's so an issue is that it may be linear", "tokens": [2283, 362, 294, 2689, 286, 519, 309, 311, 370, 364, 2734, 307, 300, 309, 815, 312, 8213], "temperature": 0.0, "avg_logprob": -0.10589980407499931, "compression_ratio": 1.641304347826087, "no_speech_prob": 8.26711857371265e-06}, {"id": 568, "seek": 341120, "start": 3432.48, "end": 3438.3999999999996, "text": " combinations of things that have significance and so it's quite possible", "tokens": [21267, 295, 721, 300, 362, 17687, 293, 370, 309, 311, 1596, 1944], "temperature": 0.0, "avg_logprob": -0.10589980407499931, "compression_ratio": 1.641304347826087, "no_speech_prob": 8.26711857371265e-06}, {"id": 569, "seek": 343840, "start": 3438.4, "end": 3445.48, "text": " that maybe youth is captured as being near 0.5 in the first spot and 0.1 in", "tokens": [300, 1310, 7503, 307, 11828, 382, 885, 2651, 1958, 13, 20, 294, 264, 700, 4008, 293, 1958, 13, 16, 294], "temperature": 0.0, "avg_logprob": -0.11274776226136743, "compression_ratio": 1.6397849462365592, "no_speech_prob": 2.930756272689905e-05}, {"id": 570, "seek": 343840, "start": 3445.48, "end": 3452.48, "text": " the second spot and 0.8 in the fifth spot and so I think that it I mean it's", "tokens": [264, 1150, 4008, 293, 1958, 13, 23, 294, 264, 9266, 4008, 293, 370, 286, 519, 300, 309, 286, 914, 309, 311], "temperature": 0.0, "avg_logprob": -0.11274776226136743, "compression_ratio": 1.6397849462365592, "no_speech_prob": 2.930756272689905e-05}, {"id": 571, "seek": 343840, "start": 3452.48, "end": 3456.36, "text": " definitely worth looking at but I would imagine that a lot of the things that", "tokens": [2138, 3163, 1237, 412, 457, 286, 576, 3811, 300, 257, 688, 295, 264, 721, 300], "temperature": 0.0, "avg_logprob": -0.11274776226136743, "compression_ratio": 1.6397849462365592, "no_speech_prob": 2.930756272689905e-05}, {"id": 572, "seek": 343840, "start": 3456.36, "end": 3461.6800000000003, "text": " it's capturing aren't necessarily kind of just in the in a single spot but", "tokens": [309, 311, 23384, 3212, 380, 4725, 733, 295, 445, 294, 264, 294, 257, 2167, 4008, 457], "temperature": 0.0, "avg_logprob": -0.11274776226136743, "compression_ratio": 1.6397849462365592, "no_speech_prob": 2.930756272689905e-05}, {"id": 573, "seek": 346168, "start": 3461.68, "end": 3470.08, "text": " rather like a linear combination of spots that's a good question anymore", "tokens": [2831, 411, 257, 8213, 6562, 295, 10681, 300, 311, 257, 665, 1168, 3602], "temperature": 0.0, "avg_logprob": -0.3385597864786784, "compression_ratio": 1.2413793103448276, "no_speech_prob": 2.9942975743324496e-06}, {"id": 574, "seek": 346168, "start": 3472.68, "end": 3477.0, "text": " okay let me go back to the notebook", "tokens": [1392, 718, 385, 352, 646, 281, 264, 21060], "temperature": 0.0, "avg_logprob": -0.3385597864786784, "compression_ratio": 1.2413793103448276, "no_speech_prob": 2.9942975743324496e-06}, {"id": 575, "seek": 347700, "start": 3477.0, "end": 3498.8, "text": " oh it's not the notebook and so then I want to introduce the idea of bias so", "tokens": [1954, 309, 311, 406, 264, 21060, 293, 370, 550, 286, 528, 281, 5366, 264, 1558, 295, 12577, 370], "temperature": 0.0, "avg_logprob": -0.15182413056839345, "compression_ratio": 1.3893805309734513, "no_speech_prob": 1.241072459379211e-05}, {"id": 576, "seek": 347700, "start": 3498.8, "end": 3505.84, "text": " here I've checked how far away is man and genius and I get point five and then I", "tokens": [510, 286, 600, 10033, 577, 1400, 1314, 307, 587, 293, 14017, 293, 286, 483, 935, 1732, 293, 550, 286], "temperature": 0.0, "avg_logprob": -0.15182413056839345, "compression_ratio": 1.3893805309734513, "no_speech_prob": 1.241072459379211e-05}, {"id": 577, "seek": 350584, "start": 3505.84, "end": 3510.32, "text": " checked up check for a woman in genius and it's point six nine so those are a", "tokens": [10033, 493, 1520, 337, 257, 3059, 294, 14017, 293, 309, 311, 935, 2309, 4949, 370, 729, 366, 257], "temperature": 0.0, "avg_logprob": -0.11056773217169794, "compression_ratio": 1.6808510638297873, "no_speech_prob": 5.014286671212176e-06}, {"id": 578, "seek": 350584, "start": 3510.32, "end": 3517.2000000000003, "text": " lot farther away so that's an area for potential concern that will be we'll be", "tokens": [688, 20344, 1314, 370, 300, 311, 364, 1859, 337, 3995, 3136, 300, 486, 312, 321, 603, 312], "temperature": 0.0, "avg_logprob": -0.11056773217169794, "compression_ratio": 1.6808510638297873, "no_speech_prob": 5.014286671212176e-06}, {"id": 579, "seek": 350584, "start": 3517.2000000000003, "end": 3523.92, "text": " talking more about and I should say that just looking at random pairs like this", "tokens": [1417, 544, 466, 293, 286, 820, 584, 300, 445, 1237, 412, 4974, 15494, 411, 341], "temperature": 0.0, "avg_logprob": -0.11056773217169794, "compression_ratio": 1.6808510638297873, "no_speech_prob": 5.014286671212176e-06}, {"id": 580, "seek": 350584, "start": 3523.92, "end": 3528.52, "text": " is not very scientific and this their researchers that have approached this in", "tokens": [307, 406, 588, 8134, 293, 341, 641, 10309, 300, 362, 17247, 341, 294], "temperature": 0.0, "avg_logprob": -0.11056773217169794, "compression_ratio": 1.6808510638297873, "no_speech_prob": 5.014286671212176e-06}, {"id": 581, "seek": 350584, "start": 3528.52, "end": 3532.32, "text": " a much more systematic way but I wanted to show you kind of like an interactive", "tokens": [257, 709, 544, 27249, 636, 457, 286, 1415, 281, 855, 291, 733, 295, 411, 364, 15141], "temperature": 0.0, "avg_logprob": -0.11056773217169794, "compression_ratio": 1.6808510638297873, "no_speech_prob": 5.014286671212176e-06}, {"id": 582, "seek": 353232, "start": 3532.32, "end": 3536.92, "text": " way that you can test test different pairs to see but we're kind of seeing", "tokens": [636, 300, 291, 393, 1500, 1500, 819, 15494, 281, 536, 457, 321, 434, 733, 295, 2577], "temperature": 0.0, "avg_logprob": -0.09170351425806682, "compression_ratio": 1.7612612612612613, "no_speech_prob": 7.765938789816573e-06}, {"id": 583, "seek": 353232, "start": 3536.92, "end": 3540.96, "text": " the beginnings of you know like it was great that we could capture that kitten", "tokens": [264, 37281, 295, 291, 458, 411, 309, 390, 869, 300, 321, 727, 7983, 300, 39696], "temperature": 0.0, "avg_logprob": -0.09170351425806682, "compression_ratio": 1.7612612612612613, "no_speech_prob": 7.765938789816573e-06}, {"id": 584, "seek": 353232, "start": 3540.96, "end": 3545.0, "text": " and cat were near each other but now this is not so great that we're getting", "tokens": [293, 3857, 645, 2651, 1184, 661, 457, 586, 341, 307, 406, 370, 869, 300, 321, 434, 1242], "temperature": 0.0, "avg_logprob": -0.09170351425806682, "compression_ratio": 1.7612612612612613, "no_speech_prob": 7.765938789816573e-06}, {"id": 585, "seek": 353232, "start": 3545.0, "end": 3551.04, "text": " man is a lot closer to genius than woman is and I did want to let you know like I", "tokens": [587, 307, 257, 688, 4966, 281, 14017, 813, 3059, 307, 293, 286, 630, 528, 281, 718, 291, 458, 411, 286], "temperature": 0.0, "avg_logprob": -0.09170351425806682, "compression_ratio": 1.7612612612612613, "no_speech_prob": 7.765938789816573e-06}, {"id": 586, "seek": 353232, "start": 3551.04, "end": 3555.44, "text": " tried some some pairs that weren't what I expected so for instance I found man", "tokens": [3031, 512, 512, 15494, 300, 4999, 380, 437, 286, 5176, 370, 337, 5197, 286, 1352, 587], "temperature": 0.0, "avg_logprob": -0.09170351425806682, "compression_ratio": 1.7612612612612613, "no_speech_prob": 7.765938789816573e-06}, {"id": 587, "seek": 355544, "start": 3555.44, "end": 3562.32, "text": " was closer to emotional than woman was and some of that is it's also hard to", "tokens": [390, 4966, 281, 6863, 813, 3059, 390, 293, 512, 295, 300, 307, 309, 311, 611, 1152, 281], "temperature": 0.0, "avg_logprob": -0.10757978757222493, "compression_ratio": 1.6756756756756757, "no_speech_prob": 1.3005725122638978e-05}, {"id": 588, "seek": 355544, "start": 3562.32, "end": 3566.36, "text": " know what distances are significant like you'll notice this is a much smaller", "tokens": [458, 437, 22182, 366, 4776, 411, 291, 603, 3449, 341, 307, 257, 709, 4356], "temperature": 0.0, "avg_logprob": -0.10757978757222493, "compression_ratio": 1.6756756756756757, "no_speech_prob": 1.3005725122638978e-05}, {"id": 589, "seek": 355544, "start": 3566.36, "end": 3571.7200000000003, "text": " distance than the difference we saw between genius so there are you know this", "tokens": [4560, 813, 264, 2649, 321, 1866, 1296, 14017, 370, 456, 366, 291, 458, 341], "temperature": 0.0, "avg_logprob": -0.10757978757222493, "compression_ratio": 1.6756756756756757, "no_speech_prob": 1.3005725122638978e-05}, {"id": 590, "seek": 355544, "start": 3571.7200000000003, "end": 3577.76, "text": " isn't precise in terms of you know how how close does something need to be to", "tokens": [1943, 380, 13600, 294, 2115, 295, 291, 458, 577, 577, 1998, 775, 746, 643, 281, 312, 281], "temperature": 0.0, "avg_logprob": -0.10757978757222493, "compression_ratio": 1.6756756756756757, "no_speech_prob": 1.3005725122638978e-05}, {"id": 591, "seek": 357776, "start": 3577.76, "end": 3598.6800000000003, "text": " be significant but I just wanted to kind of show you that's a good let me try", "tokens": [312, 4776, 457, 286, 445, 1415, 281, 733, 295, 855, 291, 300, 311, 257, 665, 718, 385, 853], "temperature": 0.0, "avg_logprob": -0.22863015895936548, "compression_ratio": 1.319327731092437, "no_speech_prob": 0.0001354969572275877}, {"id": 592, "seek": 357776, "start": 3598.6800000000003, "end": 3607.36, "text": " that one actually so the only copy these so um Jupiter Salish you can copy just", "tokens": [300, 472, 767, 370, 264, 787, 5055, 613, 370, 1105, 24567, 5996, 742, 291, 393, 5055, 445], "temperature": 0.0, "avg_logprob": -0.22863015895936548, "compression_ratio": 1.319327731092437, "no_speech_prob": 0.0001354969572275877}, {"id": 593, "seek": 360736, "start": 3607.36, "end": 3613.84, "text": " with a C and then V to paste and so the the comment just to repeat it was that", "tokens": [365, 257, 383, 293, 550, 691, 281, 9163, 293, 370, 264, 264, 2871, 445, 281, 7149, 309, 390, 300], "temperature": 0.0, "avg_logprob": -0.0939652731544093, "compression_ratio": 1.6868131868131868, "no_speech_prob": 9.222600056091323e-06}, {"id": 594, "seek": 360736, "start": 3613.84, "end": 3619.76, "text": " that maybe women are assumed to be emotional and so text would be less", "tokens": [300, 1310, 2266, 366, 15895, 281, 312, 6863, 293, 370, 2487, 576, 312, 1570], "temperature": 0.0, "avg_logprob": -0.0939652731544093, "compression_ratio": 1.6868131868131868, "no_speech_prob": 9.222600056091323e-06}, {"id": 595, "seek": 360736, "start": 3619.76, "end": 3626.7200000000003, "text": " likely to need to explicitly say that and that's kind of getting at the the", "tokens": [3700, 281, 643, 281, 20803, 584, 300, 293, 300, 311, 733, 295, 1242, 412, 264, 264], "temperature": 0.0, "avg_logprob": -0.0939652731544093, "compression_ratio": 1.6868131868131868, "no_speech_prob": 9.222600056091323e-06}, {"id": 596, "seek": 360736, "start": 3626.7200000000003, "end": 3631.56, "text": " fact that there's you know there's a lot of assumptions that go into into written", "tokens": [1186, 300, 456, 311, 291, 458, 456, 311, 257, 688, 295, 17695, 300, 352, 666, 666, 3720], "temperature": 0.0, "avg_logprob": -0.0939652731544093, "compression_ratio": 1.6868131868131868, "no_speech_prob": 9.222600056091323e-06}, {"id": 597, "seek": 363156, "start": 3631.56, "end": 3651.72, "text": " text and oh wait I forgot to run that oh weird okay so the same dist is not", "tokens": [2487, 293, 1954, 1699, 286, 5298, 281, 1190, 300, 1954, 3657, 1392, 370, 264, 912, 1483, 307, 406], "temperature": 0.0, "avg_logprob": -0.19532164335250854, "compression_ratio": 1.3888888888888888, "no_speech_prob": 1.0451098205521703e-05}, {"id": 598, "seek": 363156, "start": 3651.72, "end": 3660.24, "text": " defined I must have not run the cell where I import it like that and again", "tokens": [7642, 286, 1633, 362, 406, 1190, 264, 2815, 689, 286, 974, 309, 411, 300, 293, 797], "temperature": 0.0, "avg_logprob": -0.19532164335250854, "compression_ratio": 1.3888888888888888, "no_speech_prob": 1.0451098205521703e-05}, {"id": 599, "seek": 366024, "start": 3660.24, "end": 3664.72, "text": " shift enter runs a cell and you'll see the number updates here kind of to let", "tokens": [5513, 3242, 6676, 257, 2815, 293, 291, 603, 536, 264, 1230, 9205, 510, 733, 295, 281, 718], "temperature": 0.0, "avg_logprob": -0.1655133320735051, "compression_ratio": 1.723756906077348, "no_speech_prob": 1.4509107131743804e-05}, {"id": 600, "seek": 366024, "start": 3664.72, "end": 3675.04, "text": " you know the order that you're running things in okay so we do get that woman", "tokens": [291, 458, 264, 1668, 300, 291, 434, 2614, 721, 294, 1392, 370, 321, 360, 483, 300, 3059], "temperature": 0.0, "avg_logprob": -0.1655133320735051, "compression_ratio": 1.723756906077348, "no_speech_prob": 1.4509107131743804e-05}, {"id": 601, "seek": 366024, "start": 3675.04, "end": 3680.24, "text": " is closer to hysterical than man is so man and hysterical or point seven eight", "tokens": [307, 4966, 281, 35915, 804, 813, 587, 307, 370, 587, 293, 35915, 804, 420, 935, 3407, 3180], "temperature": 0.0, "avg_logprob": -0.1655133320735051, "compression_ratio": 1.723756906077348, "no_speech_prob": 1.4509107131743804e-05}, {"id": 602, "seek": 366024, "start": 3680.24, "end": 3690.2, "text": " apart woman and hysterical or point six nine apart and so then I'm gonna come", "tokens": [4936, 3059, 293, 35915, 804, 420, 935, 2309, 4949, 4936, 293, 370, 550, 286, 478, 799, 808], "temperature": 0.0, "avg_logprob": -0.1655133320735051, "compression_ratio": 1.723756906077348, "no_speech_prob": 1.4509107131743804e-05}, {"id": 603, "seek": 369020, "start": 3690.2, "end": 3696.2, "text": " back in a moment and talk about kind of how how researchers who study this", "tokens": [646, 294, 257, 1623, 293, 751, 466, 733, 295, 577, 577, 10309, 567, 2979, 341], "temperature": 0.0, "avg_logprob": -0.08459200415500374, "compression_ratio": 1.7251184834123223, "no_speech_prob": 1.3418926755548455e-05}, {"id": 604, "seek": 369020, "start": 3696.2, "end": 3700.3199999999997, "text": " approach this problem in a more scientific way but I think this is a", "tokens": [3109, 341, 1154, 294, 257, 544, 8134, 636, 457, 286, 519, 341, 307, 257], "temperature": 0.0, "avg_logprob": -0.08459200415500374, "compression_ratio": 1.7251184834123223, "no_speech_prob": 1.3418926755548455e-05}, {"id": 605, "seek": 369020, "start": 3700.3199999999997, "end": 3707.7999999999997, "text": " good illustration of kind of how the problem is manifesting itself so then", "tokens": [665, 22645, 295, 733, 295, 577, 264, 1154, 307, 8173, 8714, 2564, 370, 550], "temperature": 0.0, "avg_logprob": -0.08459200415500374, "compression_ratio": 1.7251184834123223, "no_speech_prob": 1.3418926755548455e-05}, {"id": 606, "seek": 369020, "start": 3707.7999999999997, "end": 3716.06, "text": " getting back to a question oh is that a hand that's a great question so there", "tokens": [1242, 646, 281, 257, 1168, 1954, 307, 300, 257, 1011, 300, 311, 257, 869, 1168, 370, 456], "temperature": 0.0, "avg_logprob": -0.08459200415500374, "compression_ratio": 1.7251184834123223, "no_speech_prob": 1.3418926755548455e-05}, {"id": 607, "seek": 369020, "start": 3716.06, "end": 3718.2799999999997, "text": " are people that have come up with techniques for that yeah and I'll", "tokens": [366, 561, 300, 362, 808, 493, 365, 7512, 337, 300, 1338, 293, 286, 603], "temperature": 0.0, "avg_logprob": -0.08459200415500374, "compression_ratio": 1.7251184834123223, "no_speech_prob": 1.3418926755548455e-05}, {"id": 608, "seek": 371828, "start": 3718.28, "end": 3724.2400000000002, "text": " mention and link to those yeah later and then there's also people that kind of", "tokens": [2152, 293, 2113, 281, 729, 1338, 1780, 293, 550, 456, 311, 611, 561, 300, 733, 295], "temperature": 0.0, "avg_logprob": -0.06814371884524167, "compression_ratio": 1.7181818181818183, "no_speech_prob": 6.240642505872529e-06}, {"id": 609, "seek": 371828, "start": 3724.2400000000002, "end": 3727.5600000000004, "text": " dispute about like what's the best way to handle that because it's not", "tokens": [25379, 466, 411, 437, 311, 264, 1151, 636, 281, 4813, 300, 570, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.06814371884524167, "compression_ratio": 1.7181818181818183, "no_speech_prob": 6.240642505872529e-06}, {"id": 610, "seek": 371828, "start": 3727.5600000000004, "end": 3734.1200000000003, "text": " something that has you know like a clear cut this is the only solution so", "tokens": [746, 300, 575, 291, 458, 411, 257, 1850, 1723, 341, 307, 264, 787, 3827, 370], "temperature": 0.0, "avg_logprob": -0.06814371884524167, "compression_ratio": 1.7181818181818183, "no_speech_prob": 6.240642505872529e-06}, {"id": 611, "seek": 371828, "start": 3734.1200000000003, "end": 3737.96, "text": " getting back to this question of can we find what words are closest to a given", "tokens": [1242, 646, 281, 341, 1168, 295, 393, 321, 915, 437, 2283, 366, 13699, 281, 257, 2212], "temperature": 0.0, "avg_logprob": -0.06814371884524167, "compression_ratio": 1.7181818181818183, "no_speech_prob": 6.240642505872529e-06}, {"id": 612, "seek": 371828, "start": 3737.96, "end": 3747.8, "text": " word I think this must be like a spotty spotty internet connection so we're", "tokens": [1349, 286, 519, 341, 1633, 312, 411, 257, 4008, 874, 4008, 874, 4705, 4984, 370, 321, 434], "temperature": 0.0, "avg_logprob": -0.06814371884524167, "compression_ratio": 1.7181818181818183, "no_speech_prob": 6.240642505872529e-06}, {"id": 613, "seek": 374780, "start": 3747.8, "end": 3752.2400000000002, "text": " gonna use an algorithm called nearest neighbors it's coming from scikit-learn", "tokens": [799, 764, 364, 9284, 1219, 23831, 12512, 309, 311, 1348, 490, 2180, 22681, 12, 306, 1083], "temperature": 0.0, "avg_logprob": -0.12022515181656722, "compression_ratio": 1.6995515695067265, "no_speech_prob": 1.593539309396874e-05}, {"id": 614, "seek": 374780, "start": 3752.2400000000002, "end": 3760.36, "text": " and scikit-learn is kind of the main machine learning library in Python and", "tokens": [293, 2180, 22681, 12, 306, 1083, 307, 733, 295, 264, 2135, 3479, 2539, 6405, 294, 15329, 293], "temperature": 0.0, "avg_logprob": -0.12022515181656722, "compression_ratio": 1.6995515695067265, "no_speech_prob": 1.593539309396874e-05}, {"id": 615, "seek": 374780, "start": 3760.36, "end": 3764.44, "text": " is really well supported and nearest neighbors is exactly what it sounds like", "tokens": [307, 534, 731, 8104, 293, 23831, 12512, 307, 2293, 437, 309, 3263, 411], "temperature": 0.0, "avg_logprob": -0.12022515181656722, "compression_ratio": 1.6995515695067265, "no_speech_prob": 1.593539309396874e-05}, {"id": 616, "seek": 374780, "start": 3764.44, "end": 3769.48, "text": " it can take something or you give it you know a group of things and then for a", "tokens": [309, 393, 747, 746, 420, 291, 976, 309, 291, 458, 257, 1594, 295, 721, 293, 550, 337, 257], "temperature": 0.0, "avg_logprob": -0.12022515181656722, "compression_ratio": 1.6995515695067265, "no_speech_prob": 1.593539309396874e-05}, {"id": 617, "seek": 374780, "start": 3769.48, "end": 3773.7200000000003, "text": " given item it finds which items are closest to it and so we're gonna", "tokens": [2212, 3174, 309, 10704, 597, 4754, 366, 13699, 281, 309, 293, 370, 321, 434, 799], "temperature": 0.0, "avg_logprob": -0.12022515181656722, "compression_ratio": 1.6995515695067265, "no_speech_prob": 1.593539309396874e-05}, {"id": 618, "seek": 377372, "start": 3773.72, "end": 3778.04, "text": " construct the nearest neighbors classifier and specify that we're gonna", "tokens": [7690, 264, 23831, 12512, 1508, 9902, 293, 16500, 300, 321, 434, 799], "temperature": 0.0, "avg_logprob": -0.13528897011116758, "compression_ratio": 1.745664739884393, "no_speech_prob": 2.54650876740925e-05}, {"id": 619, "seek": 377372, "start": 3778.04, "end": 3782.7599999999998, "text": " want to be able to get the 10 nearest neighbors to each thing we're telling it", "tokens": [528, 281, 312, 1075, 281, 483, 264, 1266, 23831, 12512, 281, 1184, 551, 321, 434, 3585, 309], "temperature": 0.0, "avg_logprob": -0.13528897011116758, "compression_ratio": 1.745664739884393, "no_speech_prob": 2.54650876740925e-05}, {"id": 620, "seek": 377372, "start": 3782.7599999999998, "end": 3788.2799999999997, "text": " what metric we want to use and then we run a dot fit which they should run", "tokens": [437, 20678, 321, 528, 281, 764, 293, 550, 321, 1190, 257, 5893, 3318, 597, 436, 820, 1190], "temperature": 0.0, "avg_logprob": -0.13528897011116758, "compression_ratio": 1.745664739884393, "no_speech_prob": 2.54650876740925e-05}, {"id": 621, "seek": 377372, "start": 3788.2799999999997, "end": 3801.0, "text": " these again to create create it and so then we can put in one of our hundred", "tokens": [613, 797, 281, 1884, 1884, 309, 293, 370, 550, 321, 393, 829, 294, 472, 295, 527, 3262], "temperature": 0.0, "avg_logprob": -0.13528897011116758, "compression_ratio": 1.745664739884393, "no_speech_prob": 2.54650876740925e-05}, {"id": 622, "seek": 380100, "start": 3801.0, "end": 3804.8, "text": " dimensional vectors so here I'm getting the one for intelligence and it's gonna", "tokens": [18795, 18875, 370, 510, 286, 478, 1242, 264, 472, 337, 7599, 293, 309, 311, 799], "temperature": 0.0, "avg_logprob": -0.08772930773821744, "compression_ratio": 1.794392523364486, "no_speech_prob": 1.4284915778262075e-05}, {"id": 623, "seek": 380100, "start": 3804.8, "end": 3809.72, "text": " give me back a list of distances and a list of indices and the indices will", "tokens": [976, 385, 646, 257, 1329, 295, 22182, 293, 257, 1329, 295, 43840, 293, 264, 43840, 486], "temperature": 0.0, "avg_logprob": -0.08772930773821744, "compression_ratio": 1.794392523364486, "no_speech_prob": 1.4284915778262075e-05}, {"id": 624, "seek": 380100, "start": 3809.72, "end": 3814.6, "text": " correspond to which words were closest and the distances will say how far away", "tokens": [6805, 281, 597, 2283, 645, 13699, 293, 264, 22182, 486, 584, 577, 1400, 1314], "temperature": 0.0, "avg_logprob": -0.08772930773821744, "compression_ratio": 1.794392523364486, "no_speech_prob": 1.4284915778262075e-05}, {"id": 625, "seek": 380100, "start": 3814.6, "end": 3818.32, "text": " each of those are and it's important to look at the distances too because it", "tokens": [1184, 295, 729, 366, 293, 309, 311, 1021, 281, 574, 412, 264, 22182, 886, 570, 309], "temperature": 0.0, "avg_logprob": -0.08772930773821744, "compression_ratio": 1.794392523364486, "no_speech_prob": 1.4284915778262075e-05}, {"id": 626, "seek": 380100, "start": 3818.32, "end": 3823.08, "text": " would be worth noting if even the closest thing to a word was pretty far", "tokens": [576, 312, 3163, 26801, 498, 754, 264, 13699, 551, 281, 257, 1349, 390, 1238, 1400], "temperature": 0.0, "avg_logprob": -0.08772930773821744, "compression_ratio": 1.794392523364486, "no_speech_prob": 1.4284915778262075e-05}, {"id": 627, "seek": 382308, "start": 3823.08, "end": 3832.68, "text": " away so I've done this for intelligence and this came up earlier CIA was the", "tokens": [1314, 370, 286, 600, 1096, 341, 337, 7599, 293, 341, 1361, 493, 3071, 25143, 390, 264], "temperature": 0.0, "avg_logprob": -0.1545716094970703, "compression_ratio": 1.396341463414634, "no_speech_prob": 9.08022320800228e-06}, {"id": 628, "seek": 382308, "start": 3832.68, "end": 3837.16, "text": " closest word which I wasn't expecting but it makes sense and you also get", "tokens": [13699, 1349, 597, 286, 2067, 380, 9650, 457, 309, 1669, 2020, 293, 291, 611, 483], "temperature": 0.0, "avg_logprob": -0.1545716094970703, "compression_ratio": 1.396341463414634, "no_speech_prob": 9.08022320800228e-06}, {"id": 629, "seek": 382308, "start": 3837.16, "end": 3844.84, "text": " information security FBI military secret counterterrorism Pentagon defense and", "tokens": [1589, 3825, 17441, 4632, 4054, 5682, 48024, 1434, 36371, 7654, 293], "temperature": 0.0, "avg_logprob": -0.1545716094970703, "compression_ratio": 1.396341463414634, "no_speech_prob": 9.08022320800228e-06}, {"id": 630, "seek": 384484, "start": 3844.84, "end": 3853.1600000000003, "text": " then let's let's do this for her feminist oh actually should have left", "tokens": [550, 718, 311, 718, 311, 360, 341, 337, 720, 26229, 1954, 767, 820, 362, 1411], "temperature": 0.0, "avg_logprob": -0.33009240793627365, "compression_ratio": 1.3247863247863247, "no_speech_prob": 5.507513833435951e-06}, {"id": 631, "seek": 384484, "start": 3853.1600000000003, "end": 3855.48, "text": " facts", "tokens": [9130], "temperature": 0.0, "avg_logprob": -0.33009240793627365, "compression_ratio": 1.3247863247863247, "no_speech_prob": 5.507513833435951e-06}, {"id": 632, "seek": 384484, "start": 3863.1600000000003, "end": 3874.82, "text": " so we're gonna use word IDX to look up what is the index for feminist and then", "tokens": [370, 321, 434, 799, 764, 1349, 7348, 55, 281, 574, 493, 437, 307, 264, 8186, 337, 26229, 293, 550], "temperature": 0.0, "avg_logprob": -0.33009240793627365, "compression_ratio": 1.3247863247863247, "no_speech_prob": 5.507513833435951e-06}, {"id": 633, "seek": 387482, "start": 3874.82, "end": 3883.1600000000003, "text": " we'll use that index to look up in facts the vector enter that and we get", "tokens": [321, 603, 764, 300, 8186, 281, 574, 493, 294, 9130, 264, 8062, 3242, 300, 293, 321, 483], "temperature": 0.0, "avg_logprob": -0.19091617144071138, "compression_ratio": 1.5586206896551724, "no_speech_prob": 3.726360773725901e-06}, {"id": 634, "seek": 387482, "start": 3883.1600000000003, "end": 3892.36, "text": " feminism feminist lesbian is number six humanist modernist left-wing postmodern", "tokens": [37187, 26229, 30253, 307, 1230, 2309, 1952, 468, 4363, 468, 1411, 12, 7904, 2183, 42359], "temperature": 0.0, "avg_logprob": -0.19091617144071138, "compression_ratio": 1.5586206896551724, "no_speech_prob": 3.726360773725901e-06}, {"id": 635, "seek": 387482, "start": 3892.36, "end": 3900.56, "text": " so these are words that have appeared near near feminist questions about", "tokens": [370, 613, 366, 2283, 300, 362, 8516, 2651, 2651, 26229, 1651, 466], "temperature": 0.0, "avg_logprob": -0.19091617144071138, "compression_ratio": 1.5586206896551724, "no_speech_prob": 3.726360773725901e-06}, {"id": 636, "seek": 390056, "start": 3900.56, "end": 3912.64, "text": " this so this idea of nearest neighbors and then also I wanted to check I was", "tokens": [341, 370, 341, 1558, 295, 23831, 12512, 293, 550, 611, 286, 1415, 281, 1520, 286, 390], "temperature": 0.0, "avg_logprob": -0.11954674897370515, "compression_ratio": 1.566326530612245, "no_speech_prob": 8.528959369868971e-06}, {"id": 637, "seek": 390056, "start": 3912.64, "end": 3916.32, "text": " planning to take a short break halfway through do you all feel yes I see", "tokens": [5038, 281, 747, 257, 2099, 1821, 15461, 807, 360, 291, 439, 841, 2086, 286, 536], "temperature": 0.0, "avg_logprob": -0.11954674897370515, "compression_ratio": 1.566326530612245, "no_speech_prob": 8.528959369868971e-06}, {"id": 638, "seek": 390056, "start": 3916.32, "end": 3921.36, "text": " thumbs up so let's take a five minute break so yeah hold on to any questions", "tokens": [8838, 493, 370, 718, 311, 747, 257, 1732, 3456, 1821, 370, 1338, 1797, 322, 281, 604, 1651], "temperature": 0.0, "avg_logprob": -0.11954674897370515, "compression_ratio": 1.566326530612245, "no_speech_prob": 8.528959369868971e-06}, {"id": 639, "seek": 390056, "start": 3921.36, "end": 3927.2799999999997, "text": " you have we'll come back to this it's 353 right now so let's meet back at 358 to", "tokens": [291, 362, 321, 603, 808, 646, 281, 341, 309, 311, 6976, 18, 558, 586, 370, 718, 311, 1677, 646, 412, 6976, 23, 281], "temperature": 0.0, "avg_logprob": -0.11954674897370515, "compression_ratio": 1.566326530612245, "no_speech_prob": 8.528959369868971e-06}, {"id": 640, "seek": 392728, "start": 3927.28, "end": 3931.0800000000004, "text": " continue thanks", "tokens": [2354, 3231], "temperature": 0.0, "avg_logprob": -0.20851864925650662, "compression_ratio": 1.3492063492063493, "no_speech_prob": 1.3005804248678032e-05}, {"id": 641, "seek": 392728, "start": 3938.96, "end": 3946.4, "text": " all right let's go ahead and start back up and so just during the break Twain", "tokens": [439, 558, 718, 311, 352, 2286, 293, 722, 646, 493, 293, 370, 445, 1830, 264, 1821, 2574, 491], "temperature": 0.0, "avg_logprob": -0.20851864925650662, "compression_ratio": 1.3492063492063493, "no_speech_prob": 1.3005804248678032e-05}, {"id": 642, "seek": 392728, "start": 3946.4, "end": 3952.1600000000003, "text": " told me about this projector dot tensor flow dot org which I wasn't familiar", "tokens": [1907, 385, 466, 341, 39792, 5893, 40863, 3095, 5893, 14045, 597, 286, 2067, 380, 4963], "temperature": 0.0, "avg_logprob": -0.20851864925650662, "compression_ratio": 1.3492063492063493, "no_speech_prob": 1.3005804248678032e-05}, {"id": 643, "seek": 395216, "start": 3952.16, "end": 3959.48, "text": " with but it's really neat so this is using word to back and you can choose", "tokens": [365, 457, 309, 311, 534, 10654, 370, 341, 307, 1228, 1349, 281, 646, 293, 291, 393, 2826], "temperature": 0.0, "avg_logprob": -0.12398548126220703, "compression_ratio": 1.6914285714285715, "no_speech_prob": 3.3730684663169086e-05}, {"id": 644, "seek": 395216, "start": 3959.48, "end": 3965.72, "text": " this is I guess word to back 10k being used here you can enter a word I looked", "tokens": [341, 307, 286, 2041, 1349, 281, 646, 1266, 74, 885, 1143, 510, 291, 393, 3242, 257, 1349, 286, 2956], "temperature": 0.0, "avg_logprob": -0.12398548126220703, "compression_ratio": 1.6914285714285715, "no_speech_prob": 3.3730684663169086e-05}, {"id": 645, "seek": 395216, "start": 3965.72, "end": 3971.7599999999998, "text": " up feminist and then it lists the nearest points in the original space so", "tokens": [493, 26229, 293, 550, 309, 14511, 264, 23831, 2793, 294, 264, 3380, 1901, 370], "temperature": 0.0, "avg_logprob": -0.12398548126220703, "compression_ratio": 1.6914285714285715, "no_speech_prob": 3.3730684663169086e-05}, {"id": 646, "seek": 395216, "start": 3971.7599999999998, "end": 3976.72, "text": " here it's feminism feminist activist anarchist journalists socialist", "tokens": [510, 309, 311, 37187, 26229, 24836, 41957, 468, 19535, 33981], "temperature": 0.0, "avg_logprob": -0.12398548126220703, "compression_ratio": 1.6914285714285715, "no_speech_prob": 3.3730684663169086e-05}, {"id": 647, "seek": 397672, "start": 3976.72, "end": 3983.9599999999996, "text": " literary romantic and then you can also kind of rotate this around and there's a", "tokens": [24194, 13590, 293, 550, 291, 393, 611, 733, 295, 13121, 341, 926, 293, 456, 311, 257], "temperature": 0.0, "avg_logprob": -0.16482231980663234, "compression_ratio": 1.6690140845070423, "no_speech_prob": 2.212342224083841e-05}, {"id": 648, "seek": 397672, "start": 3983.9599999999996, "end": 3987.9599999999996, "text": " lot going on in here but kind of see where the different different words are", "tokens": [688, 516, 322, 294, 510, 457, 733, 295, 536, 689, 264, 819, 819, 2283, 366], "temperature": 0.0, "avg_logprob": -0.16482231980663234, "compression_ratio": 1.6690140845070423, "no_speech_prob": 2.212342224083841e-05}, {"id": 649, "seek": 397672, "start": 3987.9599999999996, "end": 3996.3199999999997, "text": " oops oh and I guess this is back out to kind of all words but so this is a neat", "tokens": [34166, 1954, 293, 286, 2041, 341, 307, 646, 484, 281, 733, 295, 439, 2283, 457, 370, 341, 307, 257, 10654], "temperature": 0.0, "avg_logprob": -0.16482231980663234, "compression_ratio": 1.6690140845070423, "no_speech_prob": 2.212342224083841e-05}, {"id": 650, "seek": 399632, "start": 3996.32, "end": 4009.04, "text": " neat tool for kind of visualizing some of these well let me go back to back to", "tokens": [10654, 2290, 337, 733, 295, 5056, 3319, 512, 295, 613, 731, 718, 385, 352, 646, 281, 646, 281], "temperature": 0.0, "avg_logprob": -0.1863825519879659, "compression_ratio": 1.4645669291338583, "no_speech_prob": 6.853881131974049e-06}, {"id": 651, "seek": 399632, "start": 4009.04, "end": 4015.88, "text": " our notebook actually let me try using I'm gonna stick with the notebook I was", "tokens": [527, 21060, 767, 718, 385, 853, 1228, 286, 478, 799, 2897, 365, 264, 21060, 286, 390], "temperature": 0.0, "avg_logprob": -0.1863825519879659, "compression_ratio": 1.4645669291338583, "no_speech_prob": 6.853881131974049e-06}, {"id": 652, "seek": 401588, "start": 4015.88, "end": 4034.7200000000003, "text": " using and so we left off let's see I know I was in this one and looking at", "tokens": [1228, 293, 370, 321, 1411, 766, 718, 311, 536, 286, 458, 286, 390, 294, 341, 472, 293, 1237, 412], "temperature": 0.0, "avg_logprob": -0.16138889171459056, "compression_ratio": 1.6028368794326242, "no_speech_prob": 2.726397497099242e-06}, {"id": 653, "seek": 401588, "start": 4034.7200000000003, "end": 4039.2400000000002, "text": " the nearest neighbors and so finding what are the ten words nearest to a", "tokens": [264, 23831, 12512, 293, 370, 5006, 437, 366, 264, 2064, 2283, 23831, 281, 257], "temperature": 0.0, "avg_logprob": -0.16138889171459056, "compression_ratio": 1.6028368794326242, "no_speech_prob": 2.726397497099242e-06}, {"id": 654, "seek": 401588, "start": 4039.2400000000002, "end": 4045.76, "text": " given word by looking at which vectors are closest other questions about where", "tokens": [2212, 1349, 538, 1237, 412, 597, 18875, 366, 13699, 661, 1651, 466, 689], "temperature": 0.0, "avg_logprob": -0.16138889171459056, "compression_ratio": 1.6028368794326242, "no_speech_prob": 2.726397497099242e-06}, {"id": 655, "seek": 404576, "start": 4045.76, "end": 4056.36, "text": " we left off okay so to keep going um since these are vectors you know which", "tokens": [321, 1411, 766, 1392, 370, 281, 1066, 516, 1105, 1670, 613, 366, 18875, 291, 458, 597], "temperature": 0.0, "avg_logprob": -0.1320723797901567, "compression_ratio": 1.8529411764705883, "no_speech_prob": 6.961920462345006e-06}, {"id": 656, "seek": 404576, "start": 4056.36, "end": 4060.76, "text": " are just these kind of listed numbers we can do things like add two words", "tokens": [366, 445, 613, 733, 295, 10052, 3547, 321, 393, 360, 721, 411, 909, 732, 2283], "temperature": 0.0, "avg_logprob": -0.1320723797901567, "compression_ratio": 1.8529411764705883, "no_speech_prob": 6.961920462345006e-06}, {"id": 657, "seek": 404576, "start": 4060.76, "end": 4065.88, "text": " together and see what the result is so that's you know a hundred dimensional", "tokens": [1214, 293, 536, 437, 264, 1874, 307, 370, 300, 311, 291, 458, 257, 3262, 18795], "temperature": 0.0, "avg_logprob": -0.1320723797901567, "compression_ratio": 1.8529411764705883, "no_speech_prob": 6.961920462345006e-06}, {"id": 658, "seek": 404576, "start": 4065.88, "end": 4068.7200000000003, "text": " vector plus a hundred dimensional vector is gonna give us another hundred", "tokens": [8062, 1804, 257, 3262, 18795, 8062, 307, 799, 976, 505, 1071, 3262], "temperature": 0.0, "avg_logprob": -0.1320723797901567, "compression_ratio": 1.8529411764705883, "no_speech_prob": 6.961920462345006e-06}, {"id": 659, "seek": 404576, "start": 4068.7200000000003, "end": 4073.6400000000003, "text": " dimensional vector and so here I added artificial and intelligence and I want", "tokens": [18795, 8062, 293, 370, 510, 286, 3869, 11677, 293, 7599, 293, 286, 528], "temperature": 0.0, "avg_logprob": -0.1320723797901567, "compression_ratio": 1.8529411764705883, "no_speech_prob": 6.961920462345006e-06}, {"id": 660, "seek": 407364, "start": 4073.64, "end": 4077.72, "text": " to note this is different than the compound compound phrase artificial", "tokens": [281, 3637, 341, 307, 819, 813, 264, 14154, 14154, 9535, 11677], "temperature": 0.0, "avg_logprob": -0.09484856805683654, "compression_ratio": 1.7962962962962963, "no_speech_prob": 9.223097549693193e-06}, {"id": 661, "seek": 407364, "start": 4077.72, "end": 4082.4, "text": " intelligence this is literally adding you know this hundred dimensional vector", "tokens": [7599, 341, 307, 3736, 5127, 291, 458, 341, 3262, 18795, 8062], "temperature": 0.0, "avg_logprob": -0.09484856805683654, "compression_ratio": 1.7962962962962963, "no_speech_prob": 9.223097549693193e-06}, {"id": 662, "seek": 407364, "start": 4082.4, "end": 4089.48, "text": " to this other hundred dimensional vector and then I saw what oh and someone asked", "tokens": [281, 341, 661, 3262, 18795, 8062, 293, 550, 286, 1866, 437, 1954, 293, 1580, 2351], "temperature": 0.0, "avg_logprob": -0.09484856805683654, "compression_ratio": 1.7962962962962963, "no_speech_prob": 9.223097549693193e-06}, {"id": 663, "seek": 407364, "start": 4089.48, "end": 4096.08, "text": " about this during the break and it's giving warnings if you run the code that", "tokens": [466, 341, 1830, 264, 1821, 293, 309, 311, 2902, 30009, 498, 291, 1190, 264, 3089, 300], "temperature": 0.0, "avg_logprob": -0.09484856805683654, "compression_ratio": 1.7962962962962963, "no_speech_prob": 9.223097549693193e-06}, {"id": 664, "seek": 407364, "start": 4096.08, "end": 4102.08, "text": " I have if you put square brackets around new back the warning goes away and so", "tokens": [286, 362, 498, 291, 829, 3732, 26179, 926, 777, 646, 264, 9164, 1709, 1314, 293, 370], "temperature": 0.0, "avg_logprob": -0.09484856805683654, "compression_ratio": 1.7962962962962963, "no_speech_prob": 9.223097549693193e-06}, {"id": 665, "seek": 410208, "start": 4102.08, "end": 4106.68, "text": " that's just putting this hundred dimensional array inside an additional", "tokens": [300, 311, 445, 3372, 341, 3262, 18795, 10225, 1854, 364, 4497], "temperature": 0.0, "avg_logprob": -0.10207627540410952, "compression_ratio": 1.728110599078341, "no_speech_prob": 6.338957064144779e-06}, {"id": 666, "seek": 410208, "start": 4106.68, "end": 4110.76, "text": " list oh one thing that I actually I meant to say this at the beginning and", "tokens": [1329, 1954, 472, 551, 300, 286, 767, 286, 4140, 281, 584, 341, 412, 264, 2863, 293], "temperature": 0.0, "avg_logprob": -0.10207627540410952, "compression_ratio": 1.728110599078341, "no_speech_prob": 6.338957064144779e-06}, {"id": 667, "seek": 410208, "start": 4110.76, "end": 4118.36, "text": " forgot I ran this from I was working on one computer and then this morning I", "tokens": [5298, 286, 5872, 341, 490, 286, 390, 1364, 322, 472, 3820, 293, 550, 341, 2446, 286], "temperature": 0.0, "avg_logprob": -0.10207627540410952, "compression_ratio": 1.728110599078341, "no_speech_prob": 6.338957064144779e-06}, {"id": 668, "seek": 410208, "start": 4118.36, "end": 4120.68, "text": " tried running it from a different computer to make sure that it would work", "tokens": [3031, 2614, 309, 490, 257, 819, 3820, 281, 652, 988, 300, 309, 576, 589], "temperature": 0.0, "avg_logprob": -0.10207627540410952, "compression_ratio": 1.728110599078341, "no_speech_prob": 6.338957064144779e-06}, {"id": 669, "seek": 410208, "start": 4120.68, "end": 4128.32, "text": " and I got an error with with open words dot text and the solution to that up", "tokens": [293, 286, 658, 364, 6713, 365, 365, 1269, 2283, 5893, 2487, 293, 264, 3827, 281, 300, 493], "temperature": 0.0, "avg_logprob": -0.10207627540410952, "compression_ratio": 1.728110599078341, "no_speech_prob": 6.338957064144779e-06}, {"id": 670, "seek": 412832, "start": 4128.32, "end": 4143.88, "text": " here was to add it is with open words that tax comma encoding equals UTF 8 so", "tokens": [510, 390, 281, 909, 309, 307, 365, 1269, 2283, 300, 3366, 22117, 43430, 6915, 624, 20527, 1649, 370], "temperature": 0.0, "avg_logprob": -0.15428379125762404, "compression_ratio": 1.490566037735849, "no_speech_prob": 3.089392293986748e-06}, {"id": 671, "seek": 412832, "start": 4143.88, "end": 4149.679999999999, "text": " if anyone had trouble reading in words dot text sometimes you have to tell it", "tokens": [498, 2878, 632, 5253, 3760, 294, 2283, 5893, 2487, 2171, 291, 362, 281, 980, 309], "temperature": 0.0, "avg_logprob": -0.15428379125762404, "compression_ratio": 1.490566037735849, "no_speech_prob": 3.089392293986748e-06}, {"id": 672, "seek": 412832, "start": 4149.679999999999, "end": 4155.04, "text": " what the encoding is so I just wanted to highlight that although so this is being", "tokens": [437, 264, 43430, 307, 370, 286, 445, 1415, 281, 5078, 300, 4878, 370, 341, 307, 885], "temperature": 0.0, "avg_logprob": -0.15428379125762404, "compression_ratio": 1.490566037735849, "no_speech_prob": 3.089392293986748e-06}, {"id": 673, "seek": 415504, "start": 4155.04, "end": 4158.5199999999995, "text": " run on my local computer and I have a copy that I'm running from a server", "tokens": [1190, 322, 452, 2654, 3820, 293, 286, 362, 257, 5055, 300, 286, 478, 2614, 490, 257, 7154], "temperature": 0.0, "avg_logprob": -0.1283547038763342, "compression_ratio": 1.5730994152046784, "no_speech_prob": 5.771569249191089e-06}, {"id": 674, "seek": 415504, "start": 4158.5199999999995, "end": 4164.88, "text": " elsewhere and I didn't get the air on one of them yeah just a minor thing if", "tokens": [14517, 293, 286, 994, 380, 483, 264, 1988, 322, 472, 295, 552, 1338, 445, 257, 6696, 551, 498], "temperature": 0.0, "avg_logprob": -0.1283547038763342, "compression_ratio": 1.5730994152046784, "no_speech_prob": 5.771569249191089e-06}, {"id": 675, "seek": 415504, "start": 4164.88, "end": 4173.2, "text": " you saw that saw that air so yeah down oh sure", "tokens": [291, 1866, 300, 1866, 300, 1988, 370, 1338, 760, 1954, 988], "temperature": 0.0, "avg_logprob": -0.1283547038763342, "compression_ratio": 1.5730994152046784, "no_speech_prob": 5.771569249191089e-06}, {"id": 676, "seek": 415504, "start": 4176.36, "end": 4183.8, "text": " this I mean I feel like you do get a reasonable number of like encoding", "tokens": [341, 286, 914, 286, 841, 411, 291, 360, 483, 257, 10585, 1230, 295, 411, 43430], "temperature": 0.0, "avg_logprob": -0.1283547038763342, "compression_ratio": 1.5730994152046784, "no_speech_prob": 5.771569249191089e-06}, {"id": 677, "seek": 418380, "start": 4183.8, "end": 4187.0, "text": " errors particularly if you're someone that's like switching between Python 2", "tokens": [13603, 4098, 498, 291, 434, 1580, 300, 311, 411, 16493, 1296, 15329, 568], "temperature": 0.0, "avg_logprob": -0.20586344047828956, "compression_ratio": 1.4, "no_speech_prob": 2.9309336241567507e-05}, {"id": 678, "seek": 418380, "start": 4187.0, "end": 4195.360000000001, "text": " and Python 3 and I actually can show so if I take this out so it was a unicode", "tokens": [293, 15329, 805, 293, 286, 767, 393, 855, 370, 498, 286, 747, 341, 484, 370, 309, 390, 257, 517, 299, 1429], "temperature": 0.0, "avg_logprob": -0.20586344047828956, "compression_ratio": 1.4, "no_speech_prob": 2.9309336241567507e-05}, {"id": 679, "seek": 418380, "start": 4195.360000000001, "end": 4201.76, "text": " decoder when I didn't have it but putting it in", "tokens": [979, 19866, 562, 286, 994, 380, 362, 309, 457, 3372, 309, 294], "temperature": 0.0, "avg_logprob": -0.20586344047828956, "compression_ratio": 1.4, "no_speech_prob": 2.9309336241567507e-05}, {"id": 680, "seek": 420176, "start": 4201.76, "end": 4219.96, "text": " resolve that and it's able to read in the file okay sorry I meant to mention", "tokens": [14151, 300, 293, 309, 311, 1075, 281, 1401, 294, 264, 3991, 1392, 2597, 286, 4140, 281, 2152], "temperature": 0.0, "avg_logprob": -0.13558998107910156, "compression_ratio": 1.3928571428571428, "no_speech_prob": 6.438345280912472e-06}, {"id": 681, "seek": 420176, "start": 4219.96, "end": 4226.68, "text": " that kind of back when we covered that but so yeah going back to our adding two", "tokens": [300, 733, 295, 646, 562, 321, 5343, 300, 457, 370, 1338, 516, 646, 281, 527, 5127, 732], "temperature": 0.0, "avg_logprob": -0.13558998107910156, "compression_ratio": 1.3928571428571428, "no_speech_prob": 6.438345280912472e-06}, {"id": 682, "seek": 422668, "start": 4226.68, "end": 4233.8, "text": " words together we've added together artificial and intelligence and now", "tokens": [2283, 1214, 321, 600, 3869, 1214, 11677, 293, 7599, 293, 586], "temperature": 0.0, "avg_logprob": -0.12648448237666376, "compression_ratio": 1.6842105263157894, "no_speech_prob": 4.222629286232404e-06}, {"id": 683, "seek": 422668, "start": 4233.8, "end": 4238.4400000000005, "text": " what we're getting so let's go back to what we were getting before and I think", "tokens": [437, 321, 434, 1242, 370, 718, 311, 352, 646, 281, 437, 321, 645, 1242, 949, 293, 286, 519], "temperature": 0.0, "avg_logprob": -0.12648448237666376, "compression_ratio": 1.6842105263157894, "no_speech_prob": 4.222629286232404e-06}, {"id": 684, "seek": 422668, "start": 4238.4400000000005, "end": 4248.72, "text": " I overwrote it and when we just had intelligence we were getting a lot of", "tokens": [286, 670, 7449, 1370, 309, 293, 562, 321, 445, 632, 7599, 321, 645, 1242, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.12648448237666376, "compression_ratio": 1.6842105263157894, "no_speech_prob": 4.222629286232404e-06}, {"id": 685, "seek": 424872, "start": 4248.72, "end": 4259.84, "text": " CIA FBI military connotations however in this case we're getting words like so", "tokens": [25143, 17441, 4632, 46371, 763, 4461, 294, 341, 1389, 321, 434, 1242, 2283, 411, 370], "temperature": 0.0, "avg_logprob": -0.1519494883219401, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.0450221452629194e-05}, {"id": 686, "seek": 424872, "start": 4259.84, "end": 4263.2, "text": " artificial intelligence are the two closest which kind of makes sense because", "tokens": [11677, 7599, 366, 264, 732, 13699, 597, 733, 295, 1669, 2020, 570], "temperature": 0.0, "avg_logprob": -0.1519494883219401, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.0450221452629194e-05}, {"id": 687, "seek": 424872, "start": 4263.2, "end": 4267.400000000001, "text": " those are the words we we combine to create this we're getting information", "tokens": [729, 366, 264, 2283, 321, 321, 10432, 281, 1884, 341, 321, 434, 1242, 1589], "temperature": 0.0, "avg_logprob": -0.1519494883219401, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.0450221452629194e-05}, {"id": 688, "seek": 424872, "start": 4267.400000000001, "end": 4272.72, "text": " knowledge secret human biological using scientific communication and I just", "tokens": [3601, 4054, 1952, 13910, 1228, 8134, 6101, 293, 286, 445], "temperature": 0.0, "avg_logprob": -0.1519494883219401, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.0450221452629194e-05}, {"id": 689, "seek": 424872, "start": 4272.72, "end": 4274.76, "text": " thought that was interesting because it's kind of a very different", "tokens": [1194, 300, 390, 1880, 570, 309, 311, 733, 295, 257, 588, 819], "temperature": 0.0, "avg_logprob": -0.1519494883219401, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.0450221452629194e-05}, {"id": 690, "seek": 427476, "start": 4274.76, "end": 4279.8, "text": " connotations than what we got for just intelligence and again this is kind of", "tokens": [46371, 763, 813, 437, 321, 658, 337, 445, 7599, 293, 797, 341, 307, 733, 295], "temperature": 0.0, "avg_logprob": -0.07440955873946069, "compression_ratio": 1.6702702702702703, "no_speech_prob": 7.071508662193082e-06}, {"id": 691, "seek": 427476, "start": 4279.8, "end": 4284.4800000000005, "text": " creating this new vector by adding two vectors together and then seeing what", "tokens": [4084, 341, 777, 8062, 538, 5127, 732, 18875, 1214, 293, 550, 2577, 437], "temperature": 0.0, "avg_logprob": -0.07440955873946069, "compression_ratio": 1.6702702702702703, "no_speech_prob": 7.071508662193082e-06}, {"id": 692, "seek": 427476, "start": 4284.4800000000005, "end": 4291.280000000001, "text": " are the 10 10 vectors closest to that does anyone have suggestions for words", "tokens": [366, 264, 1266, 1266, 18875, 13699, 281, 300, 775, 2878, 362, 13396, 337, 2283], "temperature": 0.0, "avg_logprob": -0.07440955873946069, "compression_ratio": 1.6702702702702703, "no_speech_prob": 7.071508662193082e-06}, {"id": 693, "seek": 427476, "start": 4291.280000000001, "end": 4304.320000000001, "text": " they want to try adding together okay we'll go on so what you can do so we're", "tokens": [436, 528, 281, 853, 5127, 1214, 1392, 321, 603, 352, 322, 370, 437, 291, 393, 360, 370, 321, 434], "temperature": 0.0, "avg_logprob": -0.07440955873946069, "compression_ratio": 1.6702702702702703, "no_speech_prob": 7.071508662193082e-06}, {"id": 694, "seek": 430432, "start": 4304.32, "end": 4308.36, "text": " gonna next we're gonna look at King and the words and again to get out of this", "tokens": [799, 958, 321, 434, 799, 574, 412, 3819, 293, 264, 2283, 293, 797, 281, 483, 484, 295, 341], "temperature": 0.0, "avg_logprob": -0.16660979624544636, "compression_ratio": 1.6828193832599119, "no_speech_prob": 7.646072845091112e-06}, {"id": 695, "seek": 430432, "start": 4308.36, "end": 4312.5599999999995, "text": " air you could our warning you could just put an extra extra set of square", "tokens": [1988, 291, 727, 527, 9164, 291, 727, 445, 829, 364, 2857, 2857, 992, 295, 3732], "temperature": 0.0, "avg_logprob": -0.16660979624544636, "compression_ratio": 1.6828193832599119, "no_speech_prob": 7.646072845091112e-06}, {"id": 696, "seek": 430432, "start": 4312.5599999999995, "end": 4318.92, "text": " brackets around it words closest to King our King Prince Queen son brother", "tokens": [26179, 926, 309, 2283, 13699, 281, 3819, 527, 3819, 9821, 10077, 1872, 3708], "temperature": 0.0, "avg_logprob": -0.16660979624544636, "compression_ratio": 1.6828193832599119, "no_speech_prob": 7.646072845091112e-06}, {"id": 697, "seek": 430432, "start": 4318.92, "end": 4325.679999999999, "text": " monarch throne kingdom father Emperor which is all very reasonable now we can", "tokens": [33658, 17678, 10231, 3086, 17913, 597, 307, 439, 588, 10585, 586, 321, 393], "temperature": 0.0, "avg_logprob": -0.16660979624544636, "compression_ratio": 1.6828193832599119, "no_speech_prob": 7.646072845091112e-06}, {"id": 698, "seek": 430432, "start": 4325.679999999999, "end": 4332.0, "text": " look at King minus he plus she so this is the idea that we're kind of taking", "tokens": [574, 412, 3819, 3175, 415, 1804, 750, 370, 341, 307, 264, 1558, 300, 321, 434, 733, 295, 1940], "temperature": 0.0, "avg_logprob": -0.16660979624544636, "compression_ratio": 1.6828193832599119, "no_speech_prob": 7.646072845091112e-06}, {"id": 699, "seek": 433200, "start": 4332.0, "end": 4338.24, "text": " away this key vector which is perhaps representing something masculine and", "tokens": [1314, 341, 2141, 8062, 597, 307, 4317, 13460, 746, 28992, 293], "temperature": 0.0, "avg_logprob": -0.1249780051315887, "compression_ratio": 1.7183098591549295, "no_speech_prob": 8.800477189652156e-06}, {"id": 700, "seek": 433200, "start": 4338.24, "end": 4343.6, "text": " adding a she vector to see what we get and and you'll notice we were getting", "tokens": [5127, 257, 750, 8062, 281, 536, 437, 321, 483, 293, 293, 291, 603, 3449, 321, 645, 1242], "temperature": 0.0, "avg_logprob": -0.1249780051315887, "compression_ratio": 1.7183098591549295, "no_speech_prob": 8.800477189652156e-06}, {"id": 701, "seek": 433200, "start": 4343.6, "end": 4348.16, "text": " Queen the first time as a word that's that's related to King but now we are", "tokens": [10077, 264, 700, 565, 382, 257, 1349, 300, 311, 300, 311, 4077, 281, 3819, 457, 586, 321, 366], "temperature": 0.0, "avg_logprob": -0.1249780051315887, "compression_ratio": 1.7183098591549295, "no_speech_prob": 8.800477189652156e-06}, {"id": 702, "seek": 433200, "start": 4348.16, "end": 4353.28, "text": " getting Queen princess daughter Elizabeth mother sister so we are", "tokens": [1242, 10077, 14742, 4653, 12978, 2895, 4892, 370, 321, 366], "temperature": 0.0, "avg_logprob": -0.1249780051315887, "compression_ratio": 1.7183098591549295, "no_speech_prob": 8.800477189652156e-06}, {"id": 703, "seek": 433200, "start": 4353.28, "end": 4361.2, "text": " definitely getting more feminine more feminine words and that that seems", "tokens": [2138, 1242, 544, 24648, 544, 24648, 2283, 293, 300, 300, 2544], "temperature": 0.0, "avg_logprob": -0.1249780051315887, "compression_ratio": 1.7183098591549295, "no_speech_prob": 8.800477189652156e-06}, {"id": 704, "seek": 436120, "start": 4361.2, "end": 4367.16, "text": " reasonable on its own if we look at things like programmer so I'm checking", "tokens": [10585, 322, 1080, 1065, 498, 321, 574, 412, 721, 411, 32116, 370, 286, 478, 8568], "temperature": 0.0, "avg_logprob": -0.1163683608174324, "compression_ratio": 1.7118644067796611, "no_speech_prob": 1.3845025023329072e-05}, {"id": 705, "seek": 436120, "start": 4367.16, "end": 4373.72, "text": " that programmers in there the words closest to programmer our programmer", "tokens": [300, 41504, 294, 456, 264, 2283, 13699, 281, 32116, 527, 32116], "temperature": 0.0, "avg_logprob": -0.1163683608174324, "compression_ratio": 1.7118644067796611, "no_speech_prob": 1.3845025023329072e-05}, {"id": 706, "seek": 436120, "start": 4373.72, "end": 4381.96, "text": " animator software computer technician engineer user translator linguist and", "tokens": [2383, 1639, 4722, 3820, 38247, 11403, 4195, 35223, 21766, 468, 293], "temperature": 0.0, "avg_logprob": -0.1163683608174324, "compression_ratio": 1.7118644067796611, "no_speech_prob": 1.3845025023329072e-05}, {"id": 707, "seek": 436120, "start": 4381.96, "end": 4388.32, "text": " now we can try doing programmer minus he plus she so what is what is that going", "tokens": [586, 321, 393, 853, 884, 32116, 3175, 415, 1804, 750, 370, 437, 307, 437, 307, 300, 516], "temperature": 0.0, "avg_logprob": -0.1163683608174324, "compression_ratio": 1.7118644067796611, "no_speech_prob": 1.3845025023329072e-05}, {"id": 708, "seek": 438832, "start": 4388.32, "end": 4393.719999999999, "text": " to give us and this is kind of interesting we get some words that aren't", "tokens": [281, 976, 505, 293, 341, 307, 733, 295, 1880, 321, 483, 512, 2283, 300, 3212, 380], "temperature": 0.0, "avg_logprob": -0.1105317845064051, "compression_ratio": 1.7429906542056075, "no_speech_prob": 8.397461897402536e-06}, {"id": 709, "seek": 438832, "start": 4393.719999999999, "end": 4398.24, "text": " don't seem that related to programmer to me stylist we still have animator", "tokens": [500, 380, 1643, 300, 4077, 281, 32116, 281, 385, 48544, 321, 920, 362, 2383, 1639], "temperature": 0.0, "avg_logprob": -0.1105317845064051, "compression_ratio": 1.7429906542056075, "no_speech_prob": 8.397461897402536e-06}, {"id": 710, "seek": 438832, "start": 4398.24, "end": 4406.16, "text": " programmer choreographer technician designer prodigy screenwriter and so", "tokens": [32116, 14625, 13624, 38247, 11795, 15792, 328, 88, 2568, 23681, 293, 370], "temperature": 0.0, "avg_logprob": -0.1105317845064051, "compression_ratio": 1.7429906542056075, "no_speech_prob": 8.397461897402536e-06}, {"id": 711, "seek": 438832, "start": 4406.16, "end": 4411.08, "text": " this is trying to capture kind of what's this you know according to this word", "tokens": [341, 307, 1382, 281, 7983, 733, 295, 437, 311, 341, 291, 458, 4650, 281, 341, 1349], "temperature": 0.0, "avg_logprob": -0.1105317845064051, "compression_ratio": 1.7429906542056075, "no_speech_prob": 8.397461897402536e-06}, {"id": 712, "seek": 438832, "start": 4411.08, "end": 4416.5199999999995, "text": " embedding a more feminine version of programmer if we do the reverse minus", "tokens": [12240, 3584, 257, 544, 24648, 3037, 295, 32116, 498, 321, 360, 264, 9943, 3175], "temperature": 0.0, "avg_logprob": -0.1105317845064051, "compression_ratio": 1.7429906542056075, "no_speech_prob": 8.397461897402536e-06}, {"id": 713, "seek": 441652, "start": 4416.52, "end": 4424.64, "text": " she plus he we get words programmer engineer compiler software animator", "tokens": [750, 1804, 415, 321, 483, 2283, 32116, 11403, 31958, 4722, 2383, 1639], "temperature": 0.0, "avg_logprob": -0.10219679207637392, "compression_ratio": 1.7342342342342343, "no_speech_prob": 7.646064659638796e-06}, {"id": 714, "seek": 441652, "start": 4424.64, "end": 4430.080000000001, "text": " computer mechanic setup developer that seem a lot closer to programmer than the", "tokens": [3820, 23860, 8657, 10754, 300, 1643, 257, 688, 4966, 281, 32116, 813, 264], "temperature": 0.0, "avg_logprob": -0.10219679207637392, "compression_ratio": 1.7342342342342343, "no_speech_prob": 7.646064659638796e-06}, {"id": 715, "seek": 441652, "start": 4430.080000000001, "end": 4435.240000000001, "text": " words that we got for the more feminine version and so this is kind of an", "tokens": [2283, 300, 321, 658, 337, 264, 544, 24648, 3037, 293, 370, 341, 307, 733, 295, 364], "temperature": 0.0, "avg_logprob": -0.10219679207637392, "compression_ratio": 1.7342342342342343, "no_speech_prob": 7.646064659638796e-06}, {"id": 716, "seek": 441652, "start": 4435.240000000001, "end": 4439.76, "text": " example of we'll see in a moment people talk about doing these analogies with", "tokens": [1365, 295, 321, 603, 536, 294, 257, 1623, 561, 751, 466, 884, 613, 16660, 530, 365], "temperature": 0.0, "avg_logprob": -0.10219679207637392, "compression_ratio": 1.7342342342342343, "no_speech_prob": 7.646064659638796e-06}, {"id": 717, "seek": 441652, "start": 4439.76, "end": 4444.52, "text": " word to that and this is similar what to what they're doing but this idea of kind", "tokens": [1349, 281, 300, 293, 341, 307, 2531, 437, 281, 437, 436, 434, 884, 457, 341, 1558, 295, 733], "temperature": 0.0, "avg_logprob": -0.10219679207637392, "compression_ratio": 1.7342342342342343, "no_speech_prob": 7.646064659638796e-06}, {"id": 718, "seek": 444452, "start": 4444.52, "end": 4450.280000000001, "text": " of looking at how you know kind of what is the difference between he and she and", "tokens": [295, 1237, 412, 577, 291, 458, 733, 295, 437, 307, 264, 2649, 1296, 415, 293, 750, 293], "temperature": 0.0, "avg_logprob": -0.14477654659386838, "compression_ratio": 1.6790123456790123, "no_speech_prob": 3.4462802886991994e-06}, {"id": 719, "seek": 444452, "start": 4450.280000000001, "end": 4454.4800000000005, "text": " what does it mean to apply apply that to these different professions and this is", "tokens": [437, 775, 309, 914, 281, 3079, 3079, 300, 281, 613, 819, 38129, 293, 341, 307], "temperature": 0.0, "avg_logprob": -0.14477654659386838, "compression_ratio": 1.6790123456790123, "no_speech_prob": 3.4462802886991994e-06}, {"id": 720, "seek": 444452, "start": 4454.4800000000005, "end": 4460.4800000000005, "text": " an example of where you find a lot of bias and word to back or glove in both", "tokens": [364, 1365, 295, 689, 291, 915, 257, 688, 295, 12577, 293, 1349, 281, 646, 420, 26928, 294, 1293], "temperature": 0.0, "avg_logprob": -0.14477654659386838, "compression_ratio": 1.6790123456790123, "no_speech_prob": 3.4462802886991994e-06}, {"id": 721, "seek": 444452, "start": 4460.4800000000005, "end": 4465.0, "text": " of them questions about this idea", "tokens": [295, 552, 1651, 466, 341, 1558], "temperature": 0.0, "avg_logprob": -0.14477654659386838, "compression_ratio": 1.6790123456790123, "no_speech_prob": 3.4462802886991994e-06}, {"id": 722, "seek": 446500, "start": 4465.0, "end": 4478.36, "text": " okay and I did I did a few more examples I did doctor so this is just doctor kind", "tokens": [1392, 293, 286, 630, 286, 630, 257, 1326, 544, 5110, 286, 630, 4631, 370, 341, 307, 445, 4631, 733], "temperature": 0.0, "avg_logprob": -0.13800167587568174, "compression_ratio": 1.5695364238410596, "no_speech_prob": 1.6796170712041203e-06}, {"id": 723, "seek": 446500, "start": 4478.36, "end": 4483.84, "text": " of the word by itself we got doctor physician nurse doctor like dr. period", "tokens": [295, 264, 1349, 538, 2564, 321, 658, 4631, 16456, 14012, 4631, 411, 1224, 13, 2896], "temperature": 0.0, "avg_logprob": -0.13800167587568174, "compression_ratio": 1.5695364238410596, "no_speech_prob": 1.6796170712041203e-06}, {"id": 724, "seek": 446500, "start": 4483.84, "end": 4490.76, "text": " doctors patient medical surgeon hospital psychiatrist a more feminine version is", "tokens": [8778, 4537, 4625, 22913, 4530, 41287, 257, 544, 24648, 3037, 307], "temperature": 0.0, "avg_logprob": -0.13800167587568174, "compression_ratio": 1.5695364238410596, "no_speech_prob": 1.6796170712041203e-06}, {"id": 725, "seek": 449076, "start": 4490.76, "end": 4499.04, "text": " nurse mother woman pregnant girl patient she child herself so a lot less kind of", "tokens": [14012, 2895, 3059, 10435, 2013, 4537, 750, 1440, 7530, 370, 257, 688, 1570, 733, 295], "temperature": 0.0, "avg_logprob": -0.10207345750596789, "compression_ratio": 1.7514124293785311, "no_speech_prob": 2.9021912268945016e-06}, {"id": 726, "seek": 449076, "start": 4499.04, "end": 4505.64, "text": " doctor connotations the more masculine version is doctor physician medical he", "tokens": [4631, 46371, 763, 264, 544, 28992, 3037, 307, 4631, 16456, 4625, 415], "temperature": 0.0, "avg_logprob": -0.10207345750596789, "compression_ratio": 1.7514124293785311, "no_speech_prob": 2.9021912268945016e-06}, {"id": 727, "seek": 449076, "start": 4505.64, "end": 4513.8, "text": " doctor doctors plural dr. period surgeon him hospital himself so again kind of", "tokens": [4631, 8778, 25377, 1224, 13, 2896, 22913, 796, 4530, 3647, 370, 797, 733, 295], "temperature": 0.0, "avg_logprob": -0.10207345750596789, "compression_ratio": 1.7514124293785311, "no_speech_prob": 2.9021912268945016e-06}, {"id": 728, "seek": 449076, "start": 4513.8, "end": 4520.64, "text": " seeing seeing this bias in how the masculine version versus the feminine", "tokens": [2577, 2577, 341, 12577, 294, 577, 264, 28992, 3037, 5717, 264, 24648], "temperature": 0.0, "avg_logprob": -0.10207345750596789, "compression_ratio": 1.7514124293785311, "no_speech_prob": 2.9021912268945016e-06}, {"id": 729, "seek": 452064, "start": 4520.64, "end": 4524.240000000001, "text": " version what words are closest to it", "tokens": [3037, 437, 2283, 366, 13699, 281, 309], "temperature": 0.0, "avg_logprob": -0.17570324885992356, "compression_ratio": 1.6407766990291262, "no_speech_prob": 9.665675861469936e-06}, {"id": 730, "seek": 452064, "start": 4526.52, "end": 4530.72, "text": " all right and so I think I'm gonna switch back to my slides now I have some", "tokens": [439, 558, 293, 370, 286, 519, 286, 478, 799, 3679, 646, 281, 452, 9788, 586, 286, 362, 512], "temperature": 0.0, "avg_logprob": -0.17570324885992356, "compression_ratio": 1.6407766990291262, "no_speech_prob": 9.665675861469936e-06}, {"id": 731, "seek": 452064, "start": 4530.72, "end": 4540.400000000001, "text": " more more slides about this so first I just wanted to kind of highlight that", "tokens": [544, 544, 9788, 466, 341, 370, 700, 286, 445, 1415, 281, 733, 295, 5078, 300], "temperature": 0.0, "avg_logprob": -0.17570324885992356, "compression_ratio": 1.6407766990291262, "no_speech_prob": 9.665675861469936e-06}, {"id": 732, "seek": 452064, "start": 4540.400000000001, "end": 4545.160000000001, "text": " word analogies are useful like in general being able to say the fact that", "tokens": [1349, 16660, 530, 366, 4420, 411, 294, 2674, 885, 1075, 281, 584, 264, 1186, 300], "temperature": 0.0, "avg_logprob": -0.17570324885992356, "compression_ratio": 1.6407766990291262, "no_speech_prob": 9.665675861469936e-06}, {"id": 733, "seek": 452064, "start": 4545.160000000001, "end": 4549.360000000001, "text": " we're to that can glove let us talk about these relationships like walking", "tokens": [321, 434, 281, 300, 393, 26928, 718, 505, 751, 466, 613, 6159, 411, 4494], "temperature": 0.0, "avg_logprob": -0.17570324885992356, "compression_ratio": 1.6407766990291262, "no_speech_prob": 9.665675861469936e-06}, {"id": 734, "seek": 454936, "start": 4549.36, "end": 4556.12, "text": " is to walk to swimming is to swim Spain is to Madrid as Italy is to Rome and a", "tokens": [307, 281, 1792, 281, 11989, 307, 281, 7110, 12838, 307, 281, 22091, 382, 10705, 307, 281, 12043, 293, 257], "temperature": 0.0, "avg_logprob": -0.14834117889404297, "compression_ratio": 1.7537313432835822, "no_speech_prob": 1.3417823538475204e-05}, {"id": 735, "seek": 454936, "start": 4556.12, "end": 4558.44, "text": " lot of people have kind of looked at these analogies and are like where to", "tokens": [688, 295, 561, 362, 733, 295, 2956, 412, 613, 16660, 530, 293, 366, 411, 689, 281], "temperature": 0.0, "avg_logprob": -0.14834117889404297, "compression_ratio": 1.7537313432835822, "no_speech_prob": 1.3417823538475204e-05}, {"id": 736, "seek": 454936, "start": 4558.44, "end": 4561.04, "text": " back has really captured something that we all know about language and that's", "tokens": [646, 575, 534, 11828, 746, 300, 321, 439, 458, 466, 2856, 293, 300, 311], "temperature": 0.0, "avg_logprob": -0.14834117889404297, "compression_ratio": 1.7537313432835822, "no_speech_prob": 1.3417823538475204e-05}, {"id": 737, "seek": 454936, "start": 4561.04, "end": 4568.28, "text": " that's a good thing for the most part so so I mentioned that kind of I you know I", "tokens": [300, 311, 257, 665, 551, 337, 264, 881, 644, 370, 370, 286, 2835, 300, 733, 295, 286, 291, 458, 286], "temperature": 0.0, "avg_logprob": -0.14834117889404297, "compression_ratio": 1.7537313432835822, "no_speech_prob": 1.3417823538475204e-05}, {"id": 738, "seek": 454936, "start": 4568.28, "end": 4571.599999999999, "text": " had done these like pairwise comparisons of just looking you know is me I'm", "tokens": [632, 1096, 613, 411, 6119, 3711, 33157, 295, 445, 1237, 291, 458, 307, 385, 286, 478], "temperature": 0.0, "avg_logprob": -0.14834117889404297, "compression_ratio": 1.7537313432835822, "no_speech_prob": 1.3417823538475204e-05}, {"id": 739, "seek": 454936, "start": 4571.599999999999, "end": 4576.88, "text": " closer to genius or as women that's not super scientific and so there's a really", "tokens": [4966, 281, 14017, 420, 382, 2266, 300, 311, 406, 1687, 8134, 293, 370, 456, 311, 257, 534], "temperature": 0.0, "avg_logprob": -0.14834117889404297, "compression_ratio": 1.7537313432835822, "no_speech_prob": 1.3417823538475204e-05}, {"id": 740, "seek": 457688, "start": 4576.88, "end": 4583.4800000000005, "text": " great paper and this is that the title and authors these are researchers at", "tokens": [869, 3035, 293, 341, 307, 300, 264, 4876, 293, 16552, 613, 366, 10309, 412], "temperature": 0.0, "avg_logprob": -0.08260704920842098, "compression_ratio": 1.8063241106719368, "no_speech_prob": 1.3001998013351113e-05}, {"id": 741, "seek": 457688, "start": 4583.4800000000005, "end": 4587.96, "text": " Princeton and the University of Bath and what they did for their methodology is", "tokens": [36592, 293, 264, 3535, 295, 36167, 293, 437, 436, 630, 337, 641, 24850, 307], "temperature": 0.0, "avg_logprob": -0.08260704920842098, "compression_ratio": 1.8063241106719368, "no_speech_prob": 1.3001998013351113e-05}, {"id": 742, "seek": 457688, "start": 4587.96, "end": 4592.86, "text": " they looked at what they called baskets of words but taking a collection of you", "tokens": [436, 2956, 412, 437, 436, 1219, 42853, 295, 2283, 457, 1940, 257, 5765, 295, 291], "temperature": 0.0, "avg_logprob": -0.08260704920842098, "compression_ratio": 1.8063241106719368, "no_speech_prob": 1.3001998013351113e-05}, {"id": 743, "seek": 457688, "start": 4592.86, "end": 4597.4800000000005, "text": " know like 20 words and comparing how those baskets of words how far apart", "tokens": [458, 411, 945, 2283, 293, 15763, 577, 729, 42853, 295, 2283, 577, 1400, 4936], "temperature": 0.0, "avg_logprob": -0.08260704920842098, "compression_ratio": 1.8063241106719368, "no_speech_prob": 1.3001998013351113e-05}, {"id": 744, "seek": 457688, "start": 4597.4800000000005, "end": 4602.76, "text": " they were from each other and that's this is a lot less noisy because now", "tokens": [436, 645, 490, 1184, 661, 293, 300, 311, 341, 307, 257, 688, 1570, 24518, 570, 586], "temperature": 0.0, "avg_logprob": -0.08260704920842098, "compression_ratio": 1.8063241106719368, "no_speech_prob": 1.3001998013351113e-05}, {"id": 745, "seek": 457688, "start": 4602.76, "end": 4606.56, "text": " you've got a lot of words that you're comparing and they used and this is", "tokens": [291, 600, 658, 257, 688, 295, 2283, 300, 291, 434, 15763, 293, 436, 1143, 293, 341, 307], "temperature": 0.0, "avg_logprob": -0.08260704920842098, "compression_ratio": 1.8063241106719368, "no_speech_prob": 1.3001998013351113e-05}, {"id": 746, "seek": 460656, "start": 4606.56, "end": 4610.0, "text": " coming from concept and linguistics and they didn't even they were not the ones", "tokens": [1348, 490, 3410, 293, 21766, 6006, 293, 436, 994, 380, 754, 436, 645, 406, 264, 2306], "temperature": 0.0, "avg_logprob": -0.10886037964181802, "compression_ratio": 1.719298245614035, "no_speech_prob": 2.9940665626781993e-06}, {"id": 747, "seek": 460656, "start": 4610.0, "end": 4615.0, "text": " to define these baskets they kind of used existing existing concepts so they", "tokens": [281, 6964, 613, 42853, 436, 733, 295, 1143, 6741, 6741, 10392, 370, 436], "temperature": 0.0, "avg_logprob": -0.10886037964181802, "compression_ratio": 1.719298245614035, "no_speech_prob": 2.9940665626781993e-06}, {"id": 748, "seek": 460656, "start": 4615.0, "end": 4620.56, "text": " had one of flowers and that included clover poppy marigold iris orchid rose", "tokens": [632, 472, 295, 8085, 293, 300, 5556, 596, 3570, 1665, 8200, 1849, 328, 2641, 3418, 271, 34850, 327, 10895], "temperature": 0.0, "avg_logprob": -0.10886037964181802, "compression_ratio": 1.719298245614035, "no_speech_prob": 2.9940665626781993e-06}, {"id": 749, "seek": 460656, "start": 4620.56, "end": 4625.200000000001, "text": " lilac tulip it was actually a lot larger than that but this is some of them you", "tokens": [36532, 326, 30210, 647, 309, 390, 767, 257, 688, 4833, 813, 300, 457, 341, 307, 512, 295, 552, 291], "temperature": 0.0, "avg_logprob": -0.10886037964181802, "compression_ratio": 1.719298245614035, "no_speech_prob": 2.9940665626781993e-06}, {"id": 750, "seek": 460656, "start": 4625.200000000001, "end": 4631.6, "text": " get the idea and then they had insects like locust spider bedbug maggot fly bee", "tokens": [483, 264, 1558, 293, 550, 436, 632, 20201, 411, 1628, 381, 17614, 2901, 44455, 44639, 310, 3603, 17479], "temperature": 0.0, "avg_logprob": -0.10886037964181802, "compression_ratio": 1.719298245614035, "no_speech_prob": 2.9940665626781993e-06}, {"id": 751, "seek": 463160, "start": 4631.6, "end": 4636.96, "text": " cockroach mosquito and then they had pleasant words like health love peace", "tokens": [45927, 608, 23970, 293, 550, 436, 632, 16232, 2283, 411, 1585, 959, 4336], "temperature": 0.0, "avg_logprob": -0.11151310160190245, "compression_ratio": 1.9895287958115184, "no_speech_prob": 2.6425202577229356e-06}, {"id": 752, "seek": 463160, "start": 4636.96, "end": 4643.4400000000005, "text": " cheer friend and unpleasant words like abuse filth murder death grief hatred", "tokens": [12581, 1277, 293, 29128, 2283, 411, 9852, 1387, 392, 6568, 2966, 18998, 21890], "temperature": 0.0, "avg_logprob": -0.11151310160190245, "compression_ratio": 1.9895287958115184, "no_speech_prob": 2.6425202577229356e-06}, {"id": 753, "seek": 463160, "start": 4643.4400000000005, "end": 4650.56, "text": " pollute ugly and they checked our flowers our flowers are insects kind of", "tokens": [6418, 1169, 12246, 293, 436, 10033, 527, 8085, 527, 8085, 366, 20201, 733, 295], "temperature": 0.0, "avg_logprob": -0.11151310160190245, "compression_ratio": 1.9895287958115184, "no_speech_prob": 2.6425202577229356e-06}, {"id": 754, "seek": 463160, "start": 4650.56, "end": 4653.88, "text": " which is closer to the pleasant words and which is closer to the unpleasant", "tokens": [597, 307, 4966, 281, 264, 16232, 2283, 293, 597, 307, 4966, 281, 264, 29128], "temperature": 0.0, "avg_logprob": -0.11151310160190245, "compression_ratio": 1.9895287958115184, "no_speech_prob": 2.6425202577229356e-06}, {"id": 755, "seek": 463160, "start": 4653.88, "end": 4658.46, "text": " words and not surprisingly they found that flowers are much closer to pleasant", "tokens": [2283, 293, 406, 17600, 436, 1352, 300, 8085, 366, 709, 4966, 281, 16232], "temperature": 0.0, "avg_logprob": -0.11151310160190245, "compression_ratio": 1.9895287958115184, "no_speech_prob": 2.6425202577229356e-06}, {"id": 756, "seek": 465846, "start": 4658.46, "end": 4662.8, "text": " words than insects are and the insects are a lot closer to unpleasant words", "tokens": [2283, 813, 20201, 366, 293, 264, 20201, 366, 257, 688, 4966, 281, 29128, 2283], "temperature": 0.0, "avg_logprob": -0.1289380308869597, "compression_ratio": 1.7443181818181819, "no_speech_prob": 2.857199660866172e-06}, {"id": 757, "seek": 465846, "start": 4662.8, "end": 4669.16, "text": " and this kind of fits with them I think our natural biases most humans like", "tokens": [293, 341, 733, 295, 9001, 365, 552, 286, 519, 527, 3303, 32152, 881, 6255, 411], "temperature": 0.0, "avg_logprob": -0.1289380308869597, "compression_ratio": 1.7443181818181819, "no_speech_prob": 2.857199660866172e-06}, {"id": 758, "seek": 465846, "start": 4669.16, "end": 4675.16, "text": " flowers and don't like locusts or bed bugs or maggots and so this is this is", "tokens": [8085, 293, 500, 380, 411, 1628, 381, 82, 420, 2901, 15120, 420, 44639, 1971, 293, 370, 341, 307, 341, 307], "temperature": 0.0, "avg_logprob": -0.1289380308869597, "compression_ratio": 1.7443181818181819, "no_speech_prob": 2.857199660866172e-06}, {"id": 759, "seek": 465846, "start": 4675.16, "end": 4682.6, "text": " an example of bias that is not a bad thing this is something that is yeah it's", "tokens": [364, 1365, 295, 12577, 300, 307, 406, 257, 1578, 551, 341, 307, 746, 300, 307, 1338, 309, 311], "temperature": 0.0, "avg_logprob": -0.1289380308869597, "compression_ratio": 1.7443181818181819, "no_speech_prob": 2.857199660866172e-06}, {"id": 760, "seek": 468260, "start": 4682.6, "end": 4690.160000000001, "text": " natural however they next looked at they took a group of European American names", "tokens": [3303, 4461, 436, 958, 2956, 412, 436, 1890, 257, 1594, 295, 6473, 2665, 5288], "temperature": 0.0, "avg_logprob": -0.07937992943657769, "compression_ratio": 2.1009174311926606, "no_speech_prob": 1.2409916053002235e-05}, {"id": 761, "seek": 468260, "start": 4690.160000000001, "end": 4695.400000000001, "text": " and a group of African American names and they found that the European", "tokens": [293, 257, 1594, 295, 7312, 2665, 5288, 293, 436, 1352, 300, 264, 6473], "temperature": 0.0, "avg_logprob": -0.07937992943657769, "compression_ratio": 2.1009174311926606, "no_speech_prob": 1.2409916053002235e-05}, {"id": 762, "seek": 468260, "start": 4695.400000000001, "end": 4699.72, "text": " American names were much closer to the pleasant words and the African American", "tokens": [2665, 5288, 645, 709, 4966, 281, 264, 16232, 2283, 293, 264, 7312, 2665], "temperature": 0.0, "avg_logprob": -0.07937992943657769, "compression_ratio": 2.1009174311926606, "no_speech_prob": 1.2409916053002235e-05}, {"id": 763, "seek": 468260, "start": 4699.72, "end": 4703.120000000001, "text": " names were closer to the unpleasant words and so that's something that is a", "tokens": [5288, 645, 4966, 281, 264, 29128, 2283, 293, 370, 300, 311, 746, 300, 307, 257], "temperature": 0.0, "avg_logprob": -0.07937992943657769, "compression_ratio": 2.1009174311926606, "no_speech_prob": 1.2409916053002235e-05}, {"id": 764, "seek": 468260, "start": 4703.120000000001, "end": 4708.84, "text": " huge problem that the embeddings have kind of captured this association and", "tokens": [2603, 1154, 300, 264, 12240, 29432, 362, 733, 295, 11828, 341, 14598, 293], "temperature": 0.0, "avg_logprob": -0.07937992943657769, "compression_ratio": 2.1009174311926606, "no_speech_prob": 1.2409916053002235e-05}, {"id": 765, "seek": 468260, "start": 4708.84, "end": 4712.200000000001, "text": " that's something that's been seen in a lot of studies on bias that you know", "tokens": [300, 311, 746, 300, 311, 668, 1612, 294, 257, 688, 295, 5313, 322, 12577, 300, 291, 458], "temperature": 0.0, "avg_logprob": -0.07937992943657769, "compression_ratio": 2.1009174311926606, "no_speech_prob": 1.2409916053002235e-05}, {"id": 766, "seek": 471220, "start": 4712.2, "end": 4716.72, "text": " taking a resume and changing the name to an African American name changes how", "tokens": [1940, 257, 15358, 293, 4473, 264, 1315, 281, 364, 7312, 2665, 1315, 2962, 577], "temperature": 0.0, "avg_logprob": -0.07645219105940598, "compression_ratio": 1.9012345679012346, "no_speech_prob": 4.829942554351874e-05}, {"id": 767, "seek": 471220, "start": 4716.72, "end": 4721.36, "text": " people respond to the resume and they did a lot of different pairings they", "tokens": [561, 4196, 281, 264, 15358, 293, 436, 630, 257, 688, 295, 819, 6119, 1109, 436], "temperature": 0.0, "avg_logprob": -0.07645219105940598, "compression_ratio": 1.9012345679012346, "no_speech_prob": 4.829942554351874e-05}, {"id": 768, "seek": 471220, "start": 4721.36, "end": 4726.28, "text": " also took like men's names and women's names and they saw that the men's names", "tokens": [611, 1890, 411, 1706, 311, 5288, 293, 2266, 311, 5288, 293, 436, 1866, 300, 264, 1706, 311, 5288], "temperature": 0.0, "avg_logprob": -0.07645219105940598, "compression_ratio": 1.9012345679012346, "no_speech_prob": 4.829942554351874e-05}, {"id": 769, "seek": 471220, "start": 4726.28, "end": 4730.44, "text": " were a lot closer to all these like scientific and mathematical terms than", "tokens": [645, 257, 688, 4966, 281, 439, 613, 411, 8134, 293, 18894, 2115, 813], "temperature": 0.0, "avg_logprob": -0.07645219105940598, "compression_ratio": 1.9012345679012346, "no_speech_prob": 4.829942554351874e-05}, {"id": 770, "seek": 471220, "start": 4730.44, "end": 4735.42, "text": " the women's names were so they I commend checking out the paper because it's", "tokens": [264, 2266, 311, 5288, 645, 370, 436, 286, 35987, 8568, 484, 264, 3035, 570, 309, 311], "temperature": 0.0, "avg_logprob": -0.07645219105940598, "compression_ratio": 1.9012345679012346, "no_speech_prob": 4.829942554351874e-05}, {"id": 771, "seek": 471220, "start": 4735.42, "end": 4738.44, "text": " really interesting and they kind of go through a bunch of the different groups", "tokens": [534, 1880, 293, 436, 733, 295, 352, 807, 257, 3840, 295, 264, 819, 3935], "temperature": 0.0, "avg_logprob": -0.07645219105940598, "compression_ratio": 1.9012345679012346, "no_speech_prob": 4.829942554351874e-05}, {"id": 772, "seek": 473844, "start": 4738.44, "end": 4743.36, "text": " that they checked and they checked some that are you know neutral things like", "tokens": [300, 436, 10033, 293, 436, 10033, 512, 300, 366, 291, 458, 10598, 721, 411], "temperature": 0.0, "avg_logprob": -0.10304495096206664, "compression_ratio": 1.6493506493506493, "no_speech_prob": 1.0780951924971305e-05}, {"id": 773, "seek": 473844, "start": 4743.36, "end": 4748.16, "text": " musical instruments are more pleasant than weapons but then they also kind of", "tokens": [9165, 12190, 366, 544, 16232, 813, 7278, 457, 550, 436, 611, 733, 295], "temperature": 0.0, "avg_logprob": -0.10304495096206664, "compression_ratio": 1.6493506493506493, "no_speech_prob": 1.0780951924971305e-05}, {"id": 774, "seek": 473844, "start": 4748.16, "end": 4756.58, "text": " checked these you know dangerous human biases around race or gender so so", "tokens": [10033, 613, 291, 458, 5795, 1952, 32152, 926, 4569, 420, 7898, 370, 370], "temperature": 0.0, "avg_logprob": -0.10304495096206664, "compression_ratio": 1.6493506493506493, "no_speech_prob": 1.0780951924971305e-05}, {"id": 775, "seek": 473844, "start": 4756.58, "end": 4761.5199999999995, "text": " that's one paper another one is quantifying reducing stereotypes and word", "tokens": [300, 311, 472, 3035, 1071, 472, 307, 4426, 5489, 12245, 30853, 293, 1349], "temperature": 0.0, "avg_logprob": -0.10304495096206664, "compression_ratio": 1.6493506493506493, "no_speech_prob": 1.0780951924971305e-05}, {"id": 776, "seek": 473844, "start": 4761.5199999999995, "end": 4767.0, "text": " embeddings and this was written about an MIT Tech Review and I recommend this", "tokens": [12240, 29432, 293, 341, 390, 3720, 466, 364, 13100, 13795, 19954, 293, 286, 2748, 341], "temperature": 0.0, "avg_logprob": -0.10304495096206664, "compression_ratio": 1.6493506493506493, "no_speech_prob": 1.0780951924971305e-05}, {"id": 777, "seek": 476700, "start": 4767.0, "end": 4771.2, "text": " article I think it's really accessible how vector space mathematics reveals", "tokens": [7222, 286, 519, 309, 311, 534, 9515, 577, 8062, 1901, 18666, 20893], "temperature": 0.0, "avg_logprob": -0.11461320084132505, "compression_ratio": 1.8093023255813954, "no_speech_prob": 8.529763363185339e-06}, {"id": 778, "seek": 476700, "start": 4771.2, "end": 4775.84, "text": " the hidden sexism and language and so this was the article where they found", "tokens": [264, 7633, 3260, 1434, 293, 2856, 293, 370, 341, 390, 264, 7222, 689, 436, 1352], "temperature": 0.0, "avg_logprob": -0.11461320084132505, "compression_ratio": 1.8093023255813954, "no_speech_prob": 8.529763363185339e-06}, {"id": 779, "seek": 476700, "start": 4775.84, "end": 4781.88, "text": " the analogies that father is to doctor his mother is to nurse or father or man", "tokens": [264, 16660, 530, 300, 3086, 307, 281, 4631, 702, 2895, 307, 281, 14012, 420, 3086, 420, 587], "temperature": 0.0, "avg_logprob": -0.11461320084132505, "compression_ratio": 1.8093023255813954, "no_speech_prob": 8.529763363185339e-06}, {"id": 780, "seek": 476700, "start": 4781.88, "end": 4786.76, "text": " is to computer programmer as woman is to homemaker and so these are kind of these", "tokens": [307, 281, 3820, 32116, 382, 3059, 307, 281, 1280, 18821, 293, 370, 613, 366, 733, 295, 613], "temperature": 0.0, "avg_logprob": -0.11461320084132505, "compression_ratio": 1.8093023255813954, "no_speech_prob": 8.529763363185339e-06}, {"id": 781, "seek": 476700, "start": 4786.76, "end": 4795.48, "text": " biased analogies and they also they propose a solution in here too a way and", "tokens": [28035, 16660, 530, 293, 436, 611, 436, 17421, 257, 3827, 294, 510, 886, 257, 636, 293], "temperature": 0.0, "avg_logprob": -0.11461320084132505, "compression_ratio": 1.8093023255813954, "no_speech_prob": 8.529763363185339e-06}, {"id": 782, "seek": 479548, "start": 4795.48, "end": 4799.0, "text": " this is something that people are asking about but they propose a method for", "tokens": [341, 307, 746, 300, 561, 366, 3365, 466, 457, 436, 17421, 257, 3170, 337], "temperature": 0.0, "avg_logprob": -0.3472063382466634, "compression_ratio": 1.263157894736842, "no_speech_prob": 3.53394789271988e-05}, {"id": 783, "seek": 479548, "start": 4799.0, "end": 4811.16, "text": " de-biasing the embeddings any questions yes", "tokens": [368, 12, 5614, 3349, 264, 12240, 29432, 604, 1651, 2086], "temperature": 0.0, "avg_logprob": -0.3472063382466634, "compression_ratio": 1.263157894736842, "no_speech_prob": 3.53394789271988e-05}, {"id": 784, "seek": 481116, "start": 4811.16, "end": 4829.88, "text": " yes so yeah so the question was AI is reflecting reality since reality is", "tokens": [2086, 370, 1338, 370, 264, 1168, 390, 7318, 307, 23543, 4103, 1670, 4103, 307], "temperature": 0.0, "avg_logprob": -0.18988279862837357, "compression_ratio": 1.516778523489933, "no_speech_prob": 5.9173071349505335e-05}, {"id": 785, "seek": 481116, "start": 4829.88, "end": 4835.88, "text": " discriminatory isn't AI going to be discriminatory and I see Twain has her", "tokens": [20828, 4745, 1943, 380, 7318, 516, 281, 312, 20828, 4745, 293, 286, 536, 2574, 491, 575, 720], "temperature": 0.0, "avg_logprob": -0.18988279862837357, "compression_ratio": 1.516778523489933, "no_speech_prob": 5.9173071349505335e-05}, {"id": 786, "seek": 481116, "start": 4835.88, "end": 4839.32, "text": " hand up although I have something to say about this as well but yeah go ahead", "tokens": [1011, 493, 4878, 286, 362, 746, 281, 584, 466, 341, 382, 731, 457, 1338, 352, 2286], "temperature": 0.0, "avg_logprob": -0.18988279862837357, "compression_ratio": 1.516778523489933, "no_speech_prob": 5.9173071349505335e-05}, {"id": 787, "seek": 483932, "start": 4839.32, "end": 4844.96, "text": " Twain well I'll go and I'm going to get into this in the next slide but there's", "tokens": [2574, 491, 731, 286, 603, 352, 293, 286, 478, 516, 281, 483, 666, 341, 294, 264, 958, 4137, 457, 456, 311], "temperature": 0.0, "avg_logprob": -0.2343277750135977, "compression_ratio": 1.648936170212766, "no_speech_prob": 7.028254185570404e-05}, {"id": 788, "seek": 483932, "start": 4844.96, "end": 4850.639999999999, "text": " actually so this is kind of disagreed upon but the authors of the callus", "tokens": [767, 370, 341, 307, 733, 295, 23926, 292, 3564, 457, 264, 16552, 295, 264, 818, 301], "temperature": 0.0, "avg_logprob": -0.2343277750135977, "compression_ratio": 1.648936170212766, "no_speech_prob": 7.028254185570404e-05}, {"id": 789, "seek": 483932, "start": 4850.639999999999, "end": 4858.5199999999995, "text": " calluskin Islam Bryson and neranyan for the for this paper they recommend they", "tokens": [818, 301, 5843, 8571, 12812, 3015, 293, 18219, 1325, 282, 337, 264, 337, 341, 3035, 436, 2748, 436], "temperature": 0.0, "avg_logprob": -0.2343277750135977, "compression_ratio": 1.648936170212766, "no_speech_prob": 7.028254185570404e-05}, {"id": 790, "seek": 483932, "start": 4858.5199999999995, "end": 4864.679999999999, "text": " actually think that the correction of bias should happen at the time of action", "tokens": [767, 519, 300, 264, 19984, 295, 12577, 820, 1051, 412, 264, 565, 295, 3069], "temperature": 0.0, "avg_logprob": -0.2343277750135977, "compression_ratio": 1.648936170212766, "no_speech_prob": 7.028254185570404e-05}, {"id": 791, "seek": 486468, "start": 4864.68, "end": 4869.72, "text": " not perception so the idea that yeah like the word is biased and we perceive", "tokens": [406, 12860, 370, 264, 1558, 300, 1338, 411, 264, 1349, 307, 28035, 293, 321, 20281], "temperature": 0.0, "avg_logprob": -0.07599075862339565, "compression_ratio": 1.840637450199203, "no_speech_prob": 0.0003049520018976182}, {"id": 792, "seek": 486468, "start": 4869.72, "end": 4872.8, "text": " that and computers should be able to perceive that but that we need to be", "tokens": [300, 293, 10807, 820, 312, 1075, 281, 20281, 300, 457, 300, 321, 643, 281, 312], "temperature": 0.0, "avg_logprob": -0.07599075862339565, "compression_ratio": 1.840637450199203, "no_speech_prob": 0.0003049520018976182}, {"id": 793, "seek": 486468, "start": 4872.8, "end": 4876.280000000001, "text": " careful with you know when these algorithms are used to make decisions of", "tokens": [5026, 365, 291, 458, 562, 613, 14642, 366, 1143, 281, 652, 5327, 295], "temperature": 0.0, "avg_logprob": -0.07599075862339565, "compression_ratio": 1.840637450199203, "no_speech_prob": 0.0003049520018976182}, {"id": 794, "seek": 486468, "start": 4876.280000000001, "end": 4883.0, "text": " not wanting to produce biased decisions and so they say that they think that", "tokens": [406, 7935, 281, 5258, 28035, 5327, 293, 370, 436, 584, 300, 436, 519, 300], "temperature": 0.0, "avg_logprob": -0.07599075862339565, "compression_ratio": 1.840637450199203, "no_speech_prob": 0.0003049520018976182}, {"id": 795, "seek": 486468, "start": 4883.0, "end": 4887.4400000000005, "text": " de-biasing vectors kind of at the vector level that your bias is going to seep in", "tokens": [368, 12, 5614, 3349, 18875, 733, 295, 412, 264, 8062, 1496, 300, 428, 12577, 307, 516, 281, 536, 79, 294], "temperature": 0.0, "avg_logprob": -0.07599075862339565, "compression_ratio": 1.840637450199203, "no_speech_prob": 0.0003049520018976182}, {"id": 796, "seek": 486468, "start": 4887.4400000000005, "end": 4892.360000000001, "text": " other places which I think is what you might be suggesting although this paper", "tokens": [661, 3190, 597, 286, 519, 307, 437, 291, 1062, 312, 18094, 4878, 341, 3035], "temperature": 0.0, "avg_logprob": -0.07599075862339565, "compression_ratio": 1.840637450199203, "no_speech_prob": 0.0003049520018976182}, {"id": 797, "seek": 489236, "start": 4892.36, "end": 4895.36, "text": " and then there's a blog post that's really well done and I'm going to", "tokens": [293, 550, 456, 311, 257, 6968, 2183, 300, 311, 534, 731, 1096, 293, 286, 478, 516, 281], "temperature": 0.0, "avg_logprob": -0.13033656354220408, "compression_ratio": 1.7136752136752136, "no_speech_prob": 3.319982351968065e-05}, {"id": 798, "seek": 489236, "start": 4895.36, "end": 4900.679999999999, "text": " recommend do you have these kind of strategies for de-biasing vectors and I", "tokens": [2748, 360, 291, 362, 613, 733, 295, 9029, 337, 368, 12, 5614, 3349, 18875, 293, 286], "temperature": 0.0, "avg_logprob": -0.13033656354220408, "compression_ratio": 1.7136752136752136, "no_speech_prob": 3.319982351968065e-05}, {"id": 799, "seek": 489236, "start": 4900.679999999999, "end": 4905.599999999999, "text": " would I would say no matter what you do I don't think you can ever say like oh", "tokens": [576, 286, 576, 584, 572, 1871, 437, 291, 360, 286, 500, 380, 519, 291, 393, 1562, 584, 411, 1954], "temperature": 0.0, "avg_logprob": -0.13033656354220408, "compression_ratio": 1.7136752136752136, "no_speech_prob": 3.319982351968065e-05}, {"id": 800, "seek": 489236, "start": 4905.599999999999, "end": 4908.679999999999, "text": " I'm done with bias you know like I've devised my vectors I don't have to", "tokens": [286, 478, 1096, 365, 12577, 291, 458, 411, 286, 600, 1905, 2640, 452, 18875, 286, 500, 380, 362, 281], "temperature": 0.0, "avg_logprob": -0.13033656354220408, "compression_ratio": 1.7136752136752136, "no_speech_prob": 3.319982351968065e-05}, {"id": 801, "seek": 489236, "start": 4908.679999999999, "end": 4913.24, "text": " worry and I think that when working with algorithms or AI you always want to be", "tokens": [3292, 293, 286, 519, 300, 562, 1364, 365, 14642, 420, 7318, 291, 1009, 528, 281, 312], "temperature": 0.0, "avg_logprob": -0.13033656354220408, "compression_ratio": 1.7136752136752136, "no_speech_prob": 3.319982351968065e-05}, {"id": 802, "seek": 489236, "start": 4913.24, "end": 4916.2, "text": " on the lookout for bias", "tokens": [322, 264, 41025, 337, 12577], "temperature": 0.0, "avg_logprob": -0.13033656354220408, "compression_ratio": 1.7136752136752136, "no_speech_prob": 3.319982351968065e-05}, {"id": 803, "seek": 491620, "start": 4916.2, "end": 4923.2, "text": " Twain?", "tokens": [50364, 2574, 491, 30, 50714], "temperature": 0.0, "avg_logprob": -0.8300193150838217, "compression_ratio": 0.42857142857142855, "no_speech_prob": 0.0051618232391774654}, {"id": 804, "seek": 494620, "start": 4946.2, "end": 4953.2, "text": " I'm going to put a slide on that which I'm happy to share as well", "tokens": [286, 478, 516, 281, 829, 257, 4137, 322, 300, 597, 286, 478, 2055, 281, 2073, 382, 731], "temperature": 0.0, "avg_logprob": -0.571208245413644, "compression_ratio": 1.2788461538461537, "no_speech_prob": 0.7445354461669922}, {"id": 805, "seek": 494620, "start": 4953.2, "end": 4961.2, "text": " So one of the underlying assumptions with most of the algorithms is", "tokens": [407, 472, 295, 264, 14217, 17695, 365, 881, 295, 264, 14642, 307], "temperature": 0.0, "avg_logprob": -0.571208245413644, "compression_ratio": 1.2788461538461537, "no_speech_prob": 0.7445354461669922}, {"id": 806, "seek": 496120, "start": 4961.2, "end": 4977.2, "text": " probabilistic bias and you may find that probabilistic bias is actually about frequency of event distribution of events and events like this", "tokens": [31959, 3142, 12577, 293, 291, 815, 915, 300, 31959, 3142, 12577, 307, 767, 466, 7893, 295, 2280, 7316, 295, 3931, 293, 3931, 411, 341], "temperature": 0.0, "avg_logprob": -0.37644327603853667, "compression_ratio": 1.6134453781512605, "no_speech_prob": 0.0007770600495859981}, {"id": 807, "seek": 496120, "start": 4977.2, "end": 4983.2, "text": " which goes actually in measure of perceptual biases", "tokens": [597, 1709, 767, 294, 3481, 295, 43276, 901, 32152], "temperature": 0.0, "avg_logprob": -0.37644327603853667, "compression_ratio": 1.6134453781512605, "no_speech_prob": 0.0007770600495859981}, {"id": 808, "seek": 498320, "start": 4983.2, "end": 5000.2, "text": " So it could be that some magnifications will have to get reasonably in measure of the actual ratio of human biases as ever involved probabilistic bias in some events", "tokens": [407, 309, 727, 312, 300, 512, 4944, 7833, 486, 362, 281, 483, 23551, 294, 3481, 295, 264, 3539, 8509, 295, 1952, 32152, 382, 1562, 3288, 31959, 3142, 12577, 294, 512, 3931], "temperature": 0.0, "avg_logprob": -0.46221918378557475, "compression_ratio": 1.599009900990099, "no_speech_prob": 5.214846532908268e-05}, {"id": 809, "seek": 498320, "start": 5000.2, "end": 5004.2, "text": " Okay and we're just going to rephrase restate that for the recording but Twain was", "tokens": [1033, 293, 321, 434, 445, 516, 281, 319, 44598, 651, 1472, 473, 300, 337, 264, 6613, 457, 2574, 491, 390], "temperature": 0.0, "avg_logprob": -0.46221918378557475, "compression_ratio": 1.599009900990099, "no_speech_prob": 5.214846532908268e-05}, {"id": 810, "seek": 498320, "start": 5004.2, "end": 5008.72, "text": " recommending that everyone Google the definitions of probabilistic bias of", "tokens": [30559, 300, 1518, 3329, 264, 21988, 295, 31959, 3142, 12577, 295], "temperature": 0.0, "avg_logprob": -0.46221918378557475, "compression_ratio": 1.599009900990099, "no_speech_prob": 5.214846532908268e-05}, {"id": 811, "seek": 500872, "start": 5008.72, "end": 5016.2, "text": " legal bias and of economic bias to look at those", "tokens": [5089, 12577, 293, 295, 4836, 12577, 281, 574, 412, 729], "temperature": 0.0, "avg_logprob": -0.20470717374016256, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.951805825228803e-05}, {"id": 812, "seek": 500872, "start": 5019.2, "end": 5026.2, "text": " And I'm going to talk more in a moment about how to deal with bias first I wanted to talk about kind of the impact of it", "tokens": [400, 286, 478, 516, 281, 751, 544, 294, 257, 1623, 466, 577, 281, 2028, 365, 12577, 700, 286, 1415, 281, 751, 466, 733, 295, 264, 2712, 295, 309], "temperature": 0.0, "avg_logprob": -0.20470717374016256, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.951805825228803e-05}, {"id": 813, "seek": 500872, "start": 5026.2, "end": 5035.2, "text": " So this is an example from Google Translate and it's going from Turkish which has a gender neutral singular pronoun", "tokens": [407, 341, 307, 364, 1365, 490, 3329, 6531, 17593, 293, 309, 311, 516, 490, 18565, 597, 575, 257, 7898, 10598, 20010, 14144], "temperature": 0.0, "avg_logprob": -0.20470717374016256, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.951805825228803e-05}, {"id": 814, "seek": 503520, "start": 5035.2, "end": 5045.2, "text": " So it would be like saying singular they are a doctor they are a nurse and Google Translate translates that to he is a doctor she is a nurse", "tokens": [407, 309, 576, 312, 411, 1566, 20010, 436, 366, 257, 4631, 436, 366, 257, 14012, 293, 3329, 6531, 17593, 28468, 300, 281, 415, 307, 257, 4631, 750, 307, 257, 14012], "temperature": 0.0, "avg_logprob": -0.15918305612379505, "compression_ratio": 1.7530864197530864, "no_speech_prob": 2.044024040515069e-05}, {"id": 815, "seek": 503520, "start": 5045.2, "end": 5056.2, "text": " So even though there was no gender assigned in Turkish Google Translate is assuming the doctor must be masculine and the woman must be feminine", "tokens": [407, 754, 1673, 456, 390, 572, 7898, 13279, 294, 18565, 3329, 6531, 17593, 307, 11926, 264, 4631, 1633, 312, 28992, 293, 264, 3059, 1633, 312, 24648], "temperature": 0.0, "avg_logprob": -0.15918305612379505, "compression_ratio": 1.7530864197530864, "no_speech_prob": 2.044024040515069e-05}, {"id": 816, "seek": 505620, "start": 5056.2, "end": 5065.2, "text": " I was also kind of surprised because this example has been pointed out a lot but I like went and checked and Google Translate still does this", "tokens": [286, 390, 611, 733, 295, 6100, 570, 341, 1365, 575, 668, 10932, 484, 257, 688, 457, 286, 411, 1437, 293, 10033, 293, 3329, 6531, 17593, 920, 775, 341], "temperature": 0.0, "avg_logprob": -0.1404299668862786, "compression_ratio": 1.5591397849462365, "no_speech_prob": 2.245637915621046e-05}, {"id": 817, "seek": 505620, "start": 5065.2, "end": 5077.2, "text": " So that's an example and now there's a great blog post and I'll put the Twain sent to me on concept net and I have the title of it in the next slide", "tokens": [407, 300, 311, 364, 1365, 293, 586, 456, 311, 257, 869, 6968, 2183, 293, 286, 603, 829, 264, 2574, 491, 2279, 281, 385, 322, 3410, 2533, 293, 286, 362, 264, 4876, 295, 309, 294, 264, 958, 4137], "temperature": 0.0, "avg_logprob": -0.1404299668862786, "compression_ratio": 1.5591397849462365, "no_speech_prob": 2.245637915621046e-05}, {"id": 818, "seek": 507720, "start": 5077.2, "end": 5088.2, "text": " But the author talks about a system for restaurant reviews that used word embeddings and it ranked Mexican restaurants lower because the word embeddings had learned that", "tokens": [583, 264, 3793, 6686, 466, 257, 1185, 337, 6383, 10229, 300, 1143, 1349, 12240, 29432, 293, 309, 20197, 16164, 11486, 3126, 570, 264, 1349, 12240, 29432, 632, 3264, 300], "temperature": 0.0, "avg_logprob": -0.10688554578357273, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.0001270066568395123}, {"id": 819, "seek": 507720, "start": 5088.2, "end": 5101.2, "text": " Mexican was close to the word illegal and so that's a huge problem and was affecting these ratings and so this is a way that yeah the bias can kind of seep into these systems", "tokens": [16164, 390, 1998, 281, 264, 1349, 11905, 293, 370, 300, 311, 257, 2603, 1154, 293, 390, 17476, 613, 24603, 293, 370, 341, 307, 257, 636, 300, 1338, 264, 12577, 393, 733, 295, 536, 79, 666, 613, 3652], "temperature": 0.0, "avg_logprob": -0.10688554578357273, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.0001270066568395123}, {"id": 820, "seek": 510120, "start": 5101.2, "end": 5114.2, "text": " And the author Rob Spear of this post he went through I think and did correlations for he kind of listed many different ethnicities and then many different adjectives", "tokens": [400, 264, 3793, 5424, 3550, 289, 295, 341, 2183, 415, 1437, 807, 286, 519, 293, 630, 13983, 763, 337, 415, 733, 295, 10052, 867, 819, 14363, 1088, 293, 550, 867, 819, 29378, 1539], "temperature": 0.0, "avg_logprob": -0.10433953148978097, "compression_ratio": 1.6700507614213198, "no_speech_prob": 0.00013536370533984154}, {"id": 821, "seek": 510120, "start": 5114.2, "end": 5126.2, "text": " To kind of see how some are much closer to different adjectives than others and so that's something that's really dangerous but present with these word embeddings", "tokens": [1407, 733, 295, 536, 577, 512, 366, 709, 4966, 281, 819, 29378, 1539, 813, 2357, 293, 370, 300, 311, 746, 300, 311, 534, 5795, 457, 1974, 365, 613, 1349, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.10433953148978097, "compression_ratio": 1.6700507614213198, "no_speech_prob": 0.00013536370533984154}, {"id": 822, "seek": 512620, "start": 5126.2, "end": 5138.2, "text": " And then something else is that it's been shown that word embeddings can improve web search results and so and to my knowledge I don't know that any search engines are using this yet", "tokens": [400, 550, 746, 1646, 307, 300, 309, 311, 668, 4898, 300, 1349, 12240, 29432, 393, 3470, 3670, 3164, 3542, 293, 370, 293, 281, 452, 3601, 286, 500, 380, 458, 300, 604, 3164, 12982, 366, 1228, 341, 1939], "temperature": 0.0, "avg_logprob": -0.06697833692872679, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.367236998514272e-06}, {"id": 823, "seek": 512620, "start": 5138.2, "end": 5148.2, "text": " But what if searching for something like grad student neural networks was more likely to return male names if that you know if men's names are seen as closer to math and science", "tokens": [583, 437, 498, 10808, 337, 746, 411, 2771, 3107, 18161, 9590, 390, 544, 3700, 281, 2736, 7133, 5288, 498, 300, 291, 458, 498, 1706, 311, 5288, 366, 1612, 382, 4966, 281, 5221, 293, 3497], "temperature": 0.0, "avg_logprob": -0.06697833692872679, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.367236998514272e-06}, {"id": 824, "seek": 514820, "start": 5148.2, "end": 5162.2, "text": " Which it was shown in the calif um califskine Islam paper so these are kind of just showing that this can definitely have a real world impact on systems to have have these biases", "tokens": [3013, 309, 390, 4898, 294, 264, 2104, 351, 1105, 2104, 351, 5161, 533, 8571, 3035, 370, 613, 366, 733, 295, 445, 4099, 300, 341, 393, 2138, 362, 257, 957, 1002, 2712, 322, 3652, 281, 362, 362, 613, 32152], "temperature": 0.0, "avg_logprob": -0.2942417689732143, "compression_ratio": 1.424, "no_speech_prob": 1.3844707609678153e-05}, {"id": 825, "seek": 516220, "start": 5162.2, "end": 5179.2, "text": " So this is the blog post I mentioned by Rob Spear it's called concept net number batch better less stereotyped word vectors and so what he's done is develop or released a set of debiased word vectors that are available to be downloaded", "tokens": [407, 341, 307, 264, 6968, 2183, 286, 2835, 538, 5424, 3550, 289, 309, 311, 1219, 3410, 2533, 1230, 15245, 1101, 1570, 41182, 3452, 1349, 18875, 293, 370, 437, 415, 311, 1096, 307, 1499, 420, 4736, 257, 992, 295, 3001, 4609, 292, 1349, 18875, 300, 366, 2435, 281, 312, 21748], "temperature": 0.0, "avg_logprob": -0.13972522627632572, "compression_ratio": 1.4873417721518987, "no_speech_prob": 1.0951173862849828e-05}, {"id": 826, "seek": 517920, "start": 5179.2, "end": 5196.2, "text": " Um yeah so I kind of mentioned this earlier but there are I think two main schools of thoughts on potential ways to address this one is to debias your embeddings and so this would probably be the quickest would be to download what Rob has done", "tokens": [3301, 1338, 370, 286, 733, 295, 2835, 341, 3071, 457, 456, 366, 286, 519, 732, 2135, 4656, 295, 4598, 322, 3995, 2098, 281, 2985, 341, 472, 307, 281, 3001, 4609, 428, 12240, 29432, 293, 370, 341, 576, 1391, 312, 264, 49403, 576, 312, 281, 5484, 437, 5424, 575, 1096], "temperature": 0.0, "avg_logprob": -0.14852900774973743, "compression_ratio": 1.4817073170731707, "no_speech_prob": 1.4967706192692276e-05}, {"id": 827, "seek": 519620, "start": 5196.2, "end": 5209.2, "text": " There's also in the paper second paper I mentioned by Balak bossy they have a they outline a technique that you can use to debias word embeddings", "tokens": [821, 311, 611, 294, 264, 3035, 1150, 3035, 286, 2835, 538, 13140, 514, 5741, 88, 436, 362, 257, 436, 16387, 257, 6532, 300, 291, 393, 764, 281, 3001, 4609, 1349, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.18002985417842865, "compression_ratio": 1.532967032967033, "no_speech_prob": 9.078207767743152e-06}, {"id": 828, "seek": 519620, "start": 5209.2, "end": 5218.2, "text": " And then the caliskan Islam paper says awareness is better than blindness debiasing should happen at time of action not at perception", "tokens": [400, 550, 264, 2104, 271, 5225, 8571, 3035, 1619, 8888, 307, 1101, 813, 46101, 3001, 4609, 278, 820, 1051, 412, 565, 295, 3069, 406, 412, 12860], "temperature": 0.0, "avg_logprob": -0.18002985417842865, "compression_ratio": 1.532967032967033, "no_speech_prob": 9.078207767743152e-06}, {"id": 829, "seek": 521820, "start": 5218.2, "end": 5242.2, "text": " Kind of with the fear that if you debias the embeddings bias is going to seep into your system in other ways and I thought that was a really interesting argument and I think that their point that even even if you use the biased word embeddings continue to look out for bias and don't think that you're home free for it", "tokens": [9242, 295, 365, 264, 4240, 300, 498, 291, 3001, 4609, 264, 12240, 29432, 12577, 307, 516, 281, 536, 79, 666, 428, 1185, 294, 661, 2098, 293, 286, 1194, 300, 390, 257, 534, 1880, 6770, 293, 286, 519, 300, 641, 935, 300, 754, 754, 498, 291, 764, 264, 28035, 1349, 12240, 29432, 2354, 281, 574, 484, 337, 12577, 293, 500, 380, 519, 300, 291, 434, 1280, 1737, 337, 309], "temperature": 0.0, "avg_logprob": -0.11028994046724759, "compression_ratio": 1.698019801980198, "no_speech_prob": 4.565175458992599e-06}, {"id": 830, "seek": 521820, "start": 5242.2, "end": 5246.2, "text": " Any questions about this", "tokens": [2639, 1651, 466, 341], "temperature": 0.0, "avg_logprob": -0.11028994046724759, "compression_ratio": 1.698019801980198, "no_speech_prob": 4.565175458992599e-06}, {"id": 831, "seek": 524620, "start": 5246.2, "end": 5249.2, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.17872215086413967, "compression_ratio": 1.457516339869281, "no_speech_prob": 2.4676355678820983e-05}, {"id": 832, "seek": 524620, "start": 5249.2, "end": 5256.2, "text": " And so debiasing would basically just be changing the vectors so it's", "tokens": [400, 370, 3001, 4609, 278, 576, 1936, 445, 312, 4473, 264, 18875, 370, 309, 311], "temperature": 0.0, "avg_logprob": -0.17872215086413967, "compression_ratio": 1.457516339869281, "no_speech_prob": 2.4676355678820983e-05}, {"id": 833, "seek": 524620, "start": 5256.2, "end": 5268.2, "text": " Yeah, so you're just having Yeah, you need a way to kind of like alter all the numbers. Yeah, so that now doctor is not closer to man than to woman.", "tokens": [865, 11, 370, 291, 434, 445, 1419, 865, 11, 291, 643, 257, 636, 281, 733, 295, 411, 11337, 439, 264, 3547, 13, 865, 11, 370, 300, 586, 4631, 307, 406, 4966, 281, 587, 813, 281, 3059, 13], "temperature": 0.0, "avg_logprob": -0.17872215086413967, "compression_ratio": 1.457516339869281, "no_speech_prob": 2.4676355678820983e-05}, {"id": 834, "seek": 526820, "start": 5268.2, "end": 5283.2, "text": " Yeah, I mean they're kind of a marking like basically they in the Balak bossy paper kind of specifically looks at professions as part of it, but they kind of come up with this mathematical theorem but they've grouped words of kind of like okay these are professions", "tokens": [865, 11, 286, 914, 436, 434, 733, 295, 257, 25482, 411, 1936, 436, 294, 264, 13140, 514, 5741, 88, 3035, 733, 295, 4682, 1542, 412, 38129, 382, 644, 295, 309, 11, 457, 436, 733, 295, 808, 493, 365, 341, 18894, 20904, 457, 436, 600, 41877, 2283, 295, 733, 295, 411, 1392, 613, 366, 38129], "temperature": 0.0, "avg_logprob": -0.12368882935622642, "compression_ratio": 1.606060606060606, "no_speech_prob": 2.3181268261396326e-05}, {"id": 835, "seek": 528320, "start": 5283.2, "end": 5299.2, "text": " Yeah, that should not be closer to to one than the other. And actually I don't know that much about Rob Spears methodology because I know that he does, or is attempting to take race and ethnicity into account with the debiasing as well.", "tokens": [865, 11, 300, 820, 406, 312, 4966, 281, 281, 472, 813, 264, 661, 13, 400, 767, 286, 500, 380, 458, 300, 709, 466, 5424, 3550, 685, 24850, 570, 286, 458, 300, 415, 775, 11, 420, 307, 22001, 281, 747, 4569, 293, 33774, 666, 2696, 365, 264, 3001, 4609, 278, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.0801650603612264, "compression_ratio": 1.4260355029585798, "no_speech_prob": 2.443855692035868e-06}, {"id": 836, "seek": 528320, "start": 5299.2, "end": 5307.2, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.0801650603612264, "compression_ratio": 1.4260355029585798, "no_speech_prob": 2.443855692035868e-06}, {"id": 837, "seek": 530720, "start": 5307.2, "end": 5325.2, "text": " It's a mix. Yeah, so the Balak bossy paper is based on, I believe, check, but I believe the Balak bossy is based on word to that and the caliskan Islam paper is using love.", "tokens": [467, 311, 257, 2890, 13, 865, 11, 370, 264, 13140, 514, 5741, 88, 3035, 307, 2361, 322, 11, 286, 1697, 11, 1520, 11, 457, 286, 1697, 264, 13140, 514, 5741, 88, 307, 2361, 322, 1349, 281, 300, 293, 264, 2104, 271, 5225, 8571, 3035, 307, 1228, 959, 13], "temperature": 0.0, "avg_logprob": -0.21443966718820426, "compression_ratio": 1.4576271186440677, "no_speech_prob": 5.014353064325405e-06}, {"id": 838, "seek": 532520, "start": 5325.2, "end": 5339.2, "text": " Oh, sorry, I'm trying to repeat the questions I forgot but the question was does does this get updated. These are still, I would say like relatively new enough that I don't know how much they've been updated or need to be updated.", "tokens": [876, 11, 2597, 11, 286, 478, 1382, 281, 7149, 264, 1651, 286, 5298, 457, 264, 1168, 390, 775, 775, 341, 483, 10588, 13, 1981, 366, 920, 11, 286, 576, 584, 411, 7226, 777, 1547, 300, 286, 500, 380, 458, 577, 709, 436, 600, 668, 10588, 420, 643, 281, 312, 10588, 13], "temperature": 0.0, "avg_logprob": -0.1525090304287997, "compression_ratio": 1.4556962025316456, "no_speech_prob": 1.5444620657945052e-05}, {"id": 839, "seek": 533920, "start": 5339.2, "end": 5358.2, "text": " And something also that Twain pointed out during the break is that love. She said was trained on kind of like the largest corpus possible of everything going back to Gutenberg. And so to keep in mind that a lot of you know a lot of that text is not changing kind of this really older historical text.", "tokens": [400, 746, 611, 300, 2574, 491, 10932, 484, 1830, 264, 1821, 307, 300, 959, 13, 1240, 848, 390, 8895, 322, 733, 295, 411, 264, 6443, 1181, 31624, 1944, 295, 1203, 516, 646, 281, 42833, 6873, 13, 400, 370, 281, 1066, 294, 1575, 300, 257, 688, 295, 291, 458, 257, 688, 295, 300, 2487, 307, 406, 4473, 733, 295, 341, 534, 4906, 8584, 2487, 13], "temperature": 0.0, "avg_logprob": -0.10821094268407577, "compression_ratio": 1.5648148148148149, "no_speech_prob": 4.860070021095453e-06}, {"id": 840, "seek": 533920, "start": 5358.2, "end": 5364.2, "text": " Yeah, there are questions about this.", "tokens": [865, 11, 456, 366, 1651, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.10821094268407577, "compression_ratio": 1.5648148148148149, "no_speech_prob": 4.860070021095453e-06}, {"id": 841, "seek": 536420, "start": 5364.2, "end": 5369.2, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.20661854417356726, "compression_ratio": 1.49009900990099, "no_speech_prob": 5.770024472440127e-06}, {"id": 842, "seek": 536420, "start": 5369.2, "end": 5386.2, "text": " Transition. Was it right so I wanted to talk some about bias and image software. And so this example is from face app. And this was recently in the news just like in April, but it's an app that uses neural networks and applies filters, and it released a filter called hotness kind of to make you", "tokens": [6531, 849, 13, 3027, 309, 558, 370, 286, 1415, 281, 751, 512, 466, 12577, 293, 3256, 4722, 13, 400, 370, 341, 1365, 307, 490, 1851, 724, 13, 400, 341, 390, 3938, 294, 264, 2583, 445, 411, 294, 6929, 11, 457, 309, 311, 364, 724, 300, 4960, 18161, 9590, 293, 13165, 15995, 11, 293, 309, 4736, 257, 6608, 1219, 2368, 1287, 733, 295, 281, 652, 291], "temperature": 0.0, "avg_logprob": -0.20661854417356726, "compression_ratio": 1.49009900990099, "no_speech_prob": 5.770024472440127e-06}, {"id": 843, "seek": 538620, "start": 5386.2, "end": 5402.2, "text": " feel hotter. And what it did was make people's bleach people's skin and gave them more European style noses. And so this received a lot of negative publicity and it was pulled up, pulled off the app store.", "tokens": [841, 32149, 13, 400, 437, 309, 630, 390, 652, 561, 311, 39631, 561, 311, 3178, 293, 2729, 552, 544, 6473, 3758, 3269, 279, 13, 400, 370, 341, 4613, 257, 688, 295, 3671, 37264, 293, 309, 390, 7373, 493, 11, 7373, 766, 264, 724, 3531, 13], "temperature": 0.0, "avg_logprob": -0.17719017729467276, "compression_ratio": 1.4137931034482758, "no_speech_prob": 1.6959073036559857e-05}, {"id": 844, "seek": 540220, "start": 5402.2, "end": 5417.2, "text": " But this, this is actually I think even more noteworthy. We noticed that this has actually been happening for years. So this is an icon camera asking a woman if she is blinking she's not blinking an Asian woman.", "tokens": [583, 341, 11, 341, 307, 767, 286, 519, 754, 544, 406, 1023, 2652, 88, 13, 492, 5694, 300, 341, 575, 767, 668, 2737, 337, 924, 13, 407, 341, 307, 364, 6528, 2799, 3365, 257, 3059, 498, 750, 307, 45879, 750, 311, 406, 45879, 364, 10645, 3059, 13], "temperature": 0.0, "avg_logprob": -0.08504758161656997, "compression_ratio": 1.49645390070922, "no_speech_prob": 8.663322660140693e-06}, {"id": 845, "seek": 541720, "start": 5417.2, "end": 5435.2, "text": " This is Google Photos in 2015, when they released their classification label to black people as gorillas. So really, really offensive it's also just, I think like shocking that Google released this to the public.", "tokens": [639, 307, 3329, 13919, 329, 294, 7546, 11, 562, 436, 4736, 641, 21538, 7645, 281, 2211, 561, 382, 24012, 20243, 13, 407, 534, 11, 534, 15710, 309, 311, 611, 445, 11, 286, 519, 411, 18776, 300, 3329, 4736, 341, 281, 264, 1908, 13], "temperature": 0.0, "avg_logprob": -0.10966037182097739, "compression_ratio": 1.358974358974359, "no_speech_prob": 3.1869406029727543e-06}, {"id": 846, "seek": 543520, "start": 5435.2, "end": 5440.2, "text": " This is I highly recommend Joy's Ted talk.", "tokens": [639, 307, 286, 5405, 2748, 15571, 311, 14985, 751, 13], "temperature": 0.0, "avg_logprob": -0.17922456874403841, "compression_ratio": 1.605263157894737, "no_speech_prob": 9.971118743123952e-06}, {"id": 847, "seek": 543520, "start": 5440.2, "end": 5458.2, "text": " It's really good she's am doing her PhD at MIT Media Lab and as a AI researcher, and she talks about she uses a lot of facial recognition software that won't recognize her own face and so kind of as a computer science researcher she has to wear a white mask to get some of", "tokens": [467, 311, 534, 665, 750, 311, 669, 884, 720, 14476, 412, 13100, 14741, 10137, 293, 382, 257, 7318, 21751, 11, 293, 750, 6686, 466, 750, 4960, 257, 688, 295, 15642, 11150, 4722, 300, 1582, 380, 5521, 720, 1065, 1851, 293, 370, 733, 295, 382, 257, 3820, 3497, 21751, 750, 575, 281, 3728, 257, 2418, 6094, 281, 483, 512, 295], "temperature": 0.0, "avg_logprob": -0.17922456874403841, "compression_ratio": 1.605263157894737, "no_speech_prob": 9.971118743123952e-06}, {"id": 848, "seek": 543520, "start": 5458.2, "end": 5462.2, "text": " these facial recognition programs to work for her.", "tokens": [613, 15642, 11150, 4268, 281, 589, 337, 720, 13], "temperature": 0.0, "avg_logprob": -0.17922456874403841, "compression_ratio": 1.605263157894737, "no_speech_prob": 9.971118743123952e-06}, {"id": 849, "seek": 546220, "start": 5462.2, "end": 5465.2, "text": " And she's done a really great Ted talk on that.", "tokens": [400, 750, 311, 1096, 257, 534, 869, 14985, 751, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.17824102331090858, "compression_ratio": 1.453416149068323, "no_speech_prob": 2.0141251297900453e-05}, {"id": 850, "seek": 546220, "start": 5465.2, "end": 5481.2, "text": " This beauty AI was going to be the first international beauty contest judged by AI. This was in 2016, and it judged that white contestants were more beautiful than people of other races.", "tokens": [639, 6643, 7318, 390, 516, 281, 312, 264, 700, 5058, 6643, 10287, 27485, 538, 7318, 13, 639, 390, 294, 6549, 11, 293, 309, 27485, 300, 2418, 39676, 645, 544, 2238, 813, 561, 295, 661, 15484, 13], "temperature": 0.0, "avg_logprob": -0.17824102331090858, "compression_ratio": 1.453416149068323, "no_speech_prob": 2.0141251297900453e-05}, {"id": 851, "seek": 548120, "start": 5481.2, "end": 5492.2, "text": " So this is an example of a man being told that he can't upload his photo as a profile photo because his eyes are closed and his eyes are not closed.", "tokens": [407, 341, 307, 364, 1365, 295, 257, 587, 885, 1907, 300, 415, 393, 380, 6580, 702, 5052, 382, 257, 7964, 5052, 570, 702, 2575, 366, 5395, 293, 702, 2575, 366, 406, 5395, 13], "temperature": 0.0, "avg_logprob": -0.08373122156402212, "compression_ratio": 1.6519607843137254, "no_speech_prob": 2.0459701772779226e-05}, {"id": 852, "seek": 548120, "start": 5492.2, "end": 5503.2, "text": " And so, and I was actually originally just going to show like one or two of these examples, but then I thought it was significant that there's so many, and that so many of these were very.", "tokens": [400, 370, 11, 293, 286, 390, 767, 7993, 445, 516, 281, 855, 411, 472, 420, 732, 295, 613, 5110, 11, 457, 550, 286, 1194, 309, 390, 4776, 300, 456, 311, 370, 867, 11, 293, 300, 370, 867, 295, 613, 645, 588, 13], "temperature": 0.0, "avg_logprob": -0.08373122156402212, "compression_ratio": 1.6519607843137254, "no_speech_prob": 2.0459701772779226e-05}, {"id": 853, "seek": 550320, "start": 5503.2, "end": 5514.2, "text": " Publicity is mistakes, and that they continue happening, I think is really notable that people kind of keep making this mistake.", "tokens": [9489, 507, 307, 8038, 11, 293, 300, 436, 2354, 2737, 11, 286, 519, 307, 534, 22556, 300, 561, 733, 295, 1066, 1455, 341, 6146, 13], "temperature": 0.0, "avg_logprob": -0.10355339280094009, "compression_ratio": 1.7302325581395348, "no_speech_prob": 8.939079634728841e-06}, {"id": 854, "seek": 550320, "start": 5514.2, "end": 5530.2, "text": " I know that this is, this is around image processing, but this is yeah an example of bias that's not not being addressed well, and then I even I tweeted about this and people sent me additional examples that aren't here so there are even more.", "tokens": [286, 458, 300, 341, 307, 11, 341, 307, 926, 3256, 9007, 11, 457, 341, 307, 1338, 364, 1365, 295, 12577, 300, 311, 406, 406, 885, 13847, 731, 11, 293, 550, 286, 754, 286, 25646, 466, 341, 293, 561, 2279, 385, 4497, 5110, 300, 3212, 380, 510, 370, 456, 366, 754, 544, 13], "temperature": 0.0, "avg_logprob": -0.10355339280094009, "compression_ratio": 1.7302325581395348, "no_speech_prob": 8.939079634728841e-06}, {"id": 855, "seek": 553020, "start": 5530.2, "end": 5534.2, "text": " I wanted to bring this up.", "tokens": [286, 1415, 281, 1565, 341, 493, 13], "temperature": 0.0, "avg_logprob": -0.2110585065988394, "compression_ratio": 1.2580645161290323, "no_speech_prob": 2.546294308558572e-05}, {"id": 856, "seek": 553020, "start": 5534.2, "end": 5537.2, "text": " And yeah, the sheer number of them.", "tokens": [400, 1338, 11, 264, 23061, 1230, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.2110585065988394, "compression_ratio": 1.2580645161290323, "no_speech_prob": 2.546294308558572e-05}, {"id": 857, "seek": 553020, "start": 5537.2, "end": 5540.2, "text": " I think it's really telling.", "tokens": [286, 519, 309, 311, 534, 3585, 13], "temperature": 0.0, "avg_logprob": -0.2110585065988394, "compression_ratio": 1.2580645161290323, "no_speech_prob": 2.546294308558572e-05}, {"id": 858, "seek": 553020, "start": 5540.2, "end": 5545.2, "text": " Oh yes.", "tokens": [876, 2086, 13], "temperature": 0.0, "avg_logprob": -0.2110585065988394, "compression_ratio": 1.2580645161290323, "no_speech_prob": 2.546294308558572e-05}, {"id": 859, "seek": 553020, "start": 5545.2, "end": 5551.2, "text": " Oh no no I just it just seems like such an oversight to.", "tokens": [876, 572, 572, 286, 445, 309, 445, 2544, 411, 1270, 364, 29146, 281, 13], "temperature": 0.0, "avg_logprob": -0.2110585065988394, "compression_ratio": 1.2580645161290323, "no_speech_prob": 2.546294308558572e-05}, {"id": 860, "seek": 555120, "start": 5551.2, "end": 5560.2, "text": " Yeah, I mean they clearly yeah they clearly were just testing on white people I would guess that was mostly white people that built this.", "tokens": [865, 11, 286, 914, 436, 4448, 1338, 436, 4448, 645, 445, 4997, 322, 2418, 561, 286, 576, 2041, 300, 390, 5240, 2418, 561, 300, 3094, 341, 13], "temperature": 0.0, "avg_logprob": -0.12573935253785387, "compression_ratio": 1.7520325203252032, "no_speech_prob": 6.14270311416476e-06}, {"id": 861, "seek": 555120, "start": 5560.2, "end": 5570.2, "text": " I just was so like when I tweeted this some people are like oh but it's hard to you know like get different image sets or something where it's like their Google like they have access to anything.", "tokens": [286, 445, 390, 370, 411, 562, 286, 25646, 341, 512, 561, 366, 411, 1954, 457, 309, 311, 1152, 281, 291, 458, 411, 483, 819, 3256, 6352, 420, 746, 689, 309, 311, 411, 641, 3329, 411, 436, 362, 2105, 281, 1340, 13], "temperature": 0.0, "avg_logprob": -0.12573935253785387, "compression_ratio": 1.7520325203252032, "no_speech_prob": 6.14270311416476e-06}, {"id": 862, "seek": 555120, "start": 5570.2, "end": 5579.2, "text": " But yeah, no I don't think it was intentional but it's just like a really huge, huge air to make.", "tokens": [583, 1338, 11, 572, 286, 500, 380, 519, 309, 390, 21935, 457, 309, 311, 445, 411, 257, 534, 2603, 11, 2603, 1988, 281, 652, 13], "temperature": 0.0, "avg_logprob": -0.12573935253785387, "compression_ratio": 1.7520325203252032, "no_speech_prob": 6.14270311416476e-06}, {"id": 863, "seek": 557920, "start": 5579.2, "end": 5584.2, "text": " But yeah and so,", "tokens": [583, 1338, 293, 370, 11], "temperature": 0.0, "avg_logprob": -0.16314280551412833, "compression_ratio": 1.3692307692307693, "no_speech_prob": 1.4284023563959636e-05}, {"id": 864, "seek": 557920, "start": 5584.2, "end": 5587.2, "text": " even more so than the rest of tech.", "tokens": [754, 544, 370, 813, 264, 1472, 295, 7553, 13], "temperature": 0.0, "avg_logprob": -0.16314280551412833, "compression_ratio": 1.3692307692307693, "no_speech_prob": 1.4284023563959636e-05}, {"id": 865, "seek": 557920, "start": 5587.2, "end": 5596.2, "text": " People working in AI are kind of overwhelmingly white and overwhelmingly male. It's not a not a diverse crowd building these.", "tokens": [3432, 1364, 294, 7318, 366, 733, 295, 42926, 2418, 293, 42926, 7133, 13, 467, 311, 406, 257, 406, 257, 9521, 6919, 2390, 613, 13], "temperature": 0.0, "avg_logprob": -0.16314280551412833, "compression_ratio": 1.3692307692307693, "no_speech_prob": 1.4284023563959636e-05}, {"id": 866, "seek": 559620, "start": 5596.2, "end": 5610.2, "text": " And then even more tragic example. So there is software used in US courtrooms to predict recidivism rates which then can be a factor in judges decisions about parole.", "tokens": [400, 550, 754, 544, 20385, 1365, 13, 407, 456, 307, 4722, 1143, 294, 2546, 4753, 32346, 281, 6069, 850, 327, 592, 1434, 6846, 597, 550, 393, 312, 257, 5952, 294, 14449, 5327, 466, 26783, 13], "temperature": 0.0, "avg_logprob": -0.1411566978845841, "compression_ratio": 1.3174603174603174, "no_speech_prob": 3.070993261644617e-05}, {"id": 867, "seek": 561020, "start": 5610.2, "end": 5628.2, "text": " So the US Department of Justice did an investigation in 2016 and found that the air rates were very different for black and white defendants. And so that it was more likely to incorrectly predict that a black defendant was at high risk of reoffending even when they weren't.", "tokens": [407, 264, 2546, 5982, 295, 10422, 630, 364, 9627, 294, 6549, 293, 1352, 300, 264, 1988, 6846, 645, 588, 819, 337, 2211, 293, 2418, 8602, 1719, 13, 400, 370, 300, 309, 390, 544, 3700, 281, 42892, 6069, 300, 257, 2211, 34053, 390, 412, 1090, 3148, 295, 319, 4506, 2029, 754, 562, 436, 4999, 380, 13], "temperature": 0.0, "avg_logprob": -0.20932075657795385, "compression_ratio": 1.6603773584905661, "no_speech_prob": 8.798422641120851e-06}, {"id": 868, "seek": 561020, "start": 5628.2, "end": 5638.2, "text": " And vice versa for a white defendant and this is something that was being used and I mentioned not sure on the status of. I think it may still be used in courtrooms.", "tokens": [400, 11964, 25650, 337, 257, 2418, 34053, 293, 341, 307, 746, 300, 390, 885, 1143, 293, 286, 2835, 406, 988, 322, 264, 6558, 295, 13, 286, 519, 309, 815, 920, 312, 1143, 294, 4753, 32346, 13], "temperature": 0.0, "avg_logprob": -0.20932075657795385, "compression_ratio": 1.6603773584905661, "no_speech_prob": 8.798422641120851e-06}, {"id": 869, "seek": 563820, "start": 5638.2, "end": 5647.2, "text": " But I think it's a really good example of how to really say this is having like a very material impact on people's lives.", "tokens": [583, 286, 519, 309, 311, 257, 534, 665, 1365, 295, 577, 281, 534, 584, 341, 307, 1419, 411, 257, 588, 2527, 2712, 322, 561, 311, 2909, 13], "temperature": 0.0, "avg_logprob": -0.277890247768826, "compression_ratio": 1.6502057613168724, "no_speech_prob": 4.33046261605341e-05}, {"id": 870, "seek": 563820, "start": 5647.2, "end": 5665.2, "text": " And then Abe Gong is a data scientist who gave a nice talk on ethical algorithms and he kind of does a case study on this and he really dug into something that's really difficult with these cases is that you know it's a private company that makes the software and so they kind of", "tokens": [400, 550, 38472, 33231, 307, 257, 1412, 12662, 567, 2729, 257, 1481, 751, 322, 18890, 14642, 293, 415, 733, 295, 775, 257, 1389, 2979, 322, 341, 293, 415, 534, 22954, 666, 746, 300, 311, 534, 2252, 365, 613, 3331, 307, 300, 291, 458, 309, 311, 257, 4551, 2237, 300, 1669, 264, 4722, 293, 370, 436, 733, 295], "temperature": 0.0, "avg_logprob": -0.277890247768826, "compression_ratio": 1.6502057613168724, "no_speech_prob": 4.33046261605341e-05}, {"id": 871, "seek": 566520, "start": 5665.2, "end": 5684.2, "text": " can't reveal their technology. So as a you know as the public we don't have much transparency into what they're doing. But Abe shared some of the survey questions they use and he was able to dig up and it includes things like you know if your parents separated how old were you.", "tokens": [393, 380, 10658, 641, 2899, 13, 407, 382, 257, 291, 458, 382, 264, 1908, 321, 500, 380, 362, 709, 17131, 666, 437, 436, 434, 884, 13, 583, 38472, 5507, 512, 295, 264, 8984, 1651, 436, 764, 293, 415, 390, 1075, 281, 2528, 493, 293, 309, 5974, 721, 411, 291, 458, 498, 428, 3152, 12005, 577, 1331, 645, 291, 13], "temperature": 0.0, "avg_logprob": -0.131709961664109, "compression_ratio": 1.5027027027027027, "no_speech_prob": 1.1657677532639354e-05}, {"id": 872, "seek": 568420, "start": 5684.2, "end": 5695.2, "text": " And that's just horrifying that that would be a factor in how you know whether someone gets parole or not to kind of explicitly make these things that someone has no control over. Yes.", "tokens": [400, 300, 311, 445, 40227, 300, 300, 576, 312, 257, 5952, 294, 577, 291, 458, 1968, 1580, 2170, 26783, 420, 406, 281, 733, 295, 20803, 652, 613, 721, 300, 1580, 575, 572, 1969, 670, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.1539850932795827, "compression_ratio": 1.362962962962963, "no_speech_prob": 9.81701032287674e-06}, {"id": 873, "seek": 569520, "start": 5695.2, "end": 5718.2, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.8156909942626953, "compression_ratio": 0.38461538461538464, "no_speech_prob": 3.1678831874160096e-05}, {"id": 874, "seek": 571820, "start": 5718.2, "end": 5728.2, "text": " Oh wow. OK. Yeah.", "tokens": [876, 6076, 13, 2264, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.39347508748372395, "compression_ratio": 0.92, "no_speech_prob": 7.180323791544652e-06}, {"id": 875, "seek": 571820, "start": 5728.2, "end": 5741.2, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.39347508748372395, "compression_ratio": 0.92, "no_speech_prob": 7.180323791544652e-06}, {"id": 876, "seek": 574120, "start": 5741.2, "end": 5765.2, "text": " Yes. Yeah. Just to repeat that for the recording the these questions would mostly be illegal for a judge to ask as part of someone sentencing but they're kind of yes sneaking in through being part of this computer software that's used.", "tokens": [1079, 13, 865, 13, 1449, 281, 7149, 300, 337, 264, 6613, 264, 613, 1651, 576, 5240, 312, 11905, 337, 257, 6995, 281, 1029, 382, 644, 295, 1580, 2279, 13644, 457, 436, 434, 733, 295, 2086, 48525, 294, 807, 885, 644, 295, 341, 3820, 4722, 300, 311, 1143, 13], "temperature": 0.0, "avg_logprob": -0.1320319909315843, "compression_ratio": 1.46875, "no_speech_prob": 1.0028649057858274e-06}, {"id": 877, "seek": 576520, "start": 5765.2, "end": 5776.2, "text": " And that yeah this kind of raises a lot of legal issues and that there's often a gulf between kind of like the legal profession and the AI knowledge needed.", "tokens": [400, 300, 1338, 341, 733, 295, 19658, 257, 688, 295, 5089, 2663, 293, 300, 456, 311, 2049, 257, 290, 5757, 1296, 733, 295, 411, 264, 5089, 7032, 293, 264, 7318, 3601, 2978, 13], "temperature": 0.0, "avg_logprob": -0.1321240115810085, "compression_ratio": 1.4054054054054055, "no_speech_prob": 1.3843173292116262e-05}, {"id": 878, "seek": 577620, "start": 5776.2, "end": 5803.2, "text": " Uh huh.", "tokens": [4019, 7020, 13], "temperature": 0.0, "avg_logprob": -0.5172250952039447, "compression_ratio": 0.4666666666666667, "no_speech_prob": 3.469016883173026e-05}, {"id": 879, "seek": 580320, "start": 5803.2, "end": 5824.2, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.6827872594197592, "compression_ratio": 0.38461538461538464, "no_speech_prob": 8.736518066143617e-05}, {"id": 880, "seek": 582420, "start": 5824.2, "end": 5845.2, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.43300620714823407, "compression_ratio": 0.38461538461538464, "no_speech_prob": 2.8563176783791278e-06}, {"id": 881, "seek": 584520, "start": 5845.2, "end": 5862.2, "text": " So the question is about kind of who who's making these product decisions on the government side to purchase the software which is proprietary and not definitely not transparent but not even being audited or kind of having oversight.", "tokens": [407, 264, 1168, 307, 466, 733, 295, 567, 567, 311, 1455, 613, 1674, 5327, 322, 264, 2463, 1252, 281, 8110, 264, 4722, 597, 307, 38992, 293, 406, 2138, 406, 12737, 457, 406, 754, 885, 2379, 1226, 420, 733, 295, 1419, 29146, 13], "temperature": 0.0, "avg_logprob": -0.1092547022778055, "compression_ratio": 1.4935897435897436, "no_speech_prob": 1.50527591813443e-06}, {"id": 882, "seek": 586220, "start": 5862.2, "end": 5885.2, "text": " And this is definitely an example of kind of yes.", "tokens": [400, 341, 307, 2138, 364, 1365, 295, 733, 295, 2086, 13], "temperature": 0.0, "avg_logprob": -0.12361488342285157, "compression_ratio": 0.9245283018867925, "no_speech_prob": 3.5000493880943395e-06}, {"id": 883, "seek": 588520, "start": 5885.2, "end": 5893.2, "text": " Yeah. Yeah. I mean I think this is.", "tokens": [865, 13, 865, 13, 286, 914, 286, 519, 341, 307, 13], "temperature": 0.0, "avg_logprob": -0.19000111069790152, "compression_ratio": 1.3738317757009346, "no_speech_prob": 3.0890028028807137e-06}, {"id": 884, "seek": 588520, "start": 5893.2, "end": 5901.2, "text": " Yeah I want to not get too off on a tangent but yeah I mean this is definitely like an area for a huge concern.", "tokens": [865, 286, 528, 281, 406, 483, 886, 766, 322, 257, 27747, 457, 1338, 286, 914, 341, 307, 2138, 411, 364, 1859, 337, 257, 2603, 3136, 13], "temperature": 0.0, "avg_logprob": -0.19000111069790152, "compression_ratio": 1.3738317757009346, "no_speech_prob": 3.0890028028807137e-06}, {"id": 885, "seek": 590120, "start": 5901.2, "end": 5916.2, "text": " And it gets even more concerning. Taser bought two AI companies earlier this year and they've actually rebranded themselves as acts on AI. So Taser you know that make the electric stun guns.", "tokens": [400, 309, 2170, 754, 544, 18087, 13, 314, 17756, 4243, 732, 7318, 3431, 3071, 341, 1064, 293, 436, 600, 767, 12970, 3699, 292, 2969, 382, 10672, 322, 7318, 13, 407, 314, 17756, 291, 458, 300, 652, 264, 5210, 11885, 10153, 13], "temperature": 0.0, "avg_logprob": -0.266609615749783, "compression_ratio": 1.3571428571428572, "no_speech_prob": 8.010995770746376e-06}, {"id": 886, "seek": 591620, "start": 5916.2, "end": 5935.2, "text": " And Taser also makes 80 percent or they have 80 percent of the police body cam market and they have very close relationships with many police departments. And so they are I just kind of wanted to flag this as something that I think is really scary.", "tokens": [400, 314, 17756, 611, 1669, 4688, 3043, 420, 436, 362, 4688, 3043, 295, 264, 3804, 1772, 1945, 2142, 293, 436, 362, 588, 1998, 6159, 365, 867, 3804, 15326, 13, 400, 370, 436, 366, 286, 445, 733, 295, 1415, 281, 7166, 341, 382, 746, 300, 286, 519, 307, 534, 6958, 13], "temperature": 0.0, "avg_logprob": -0.07504953278435601, "compression_ratio": 1.5121951219512195, "no_speech_prob": 8.528191756340675e-06}, {"id": 887, "seek": 593520, "start": 5935.2, "end": 5955.2, "text": " They have no transparency because again they're protecting their software and also their databases of the body cam footage as proprietary. And so they're not subject to the same state public record laws that that we think of or that you know that the government is since they're a private company.", "tokens": [814, 362, 572, 17131, 570, 797, 436, 434, 12316, 641, 4722, 293, 611, 641, 22380, 295, 264, 1772, 1945, 9556, 382, 38992, 13, 400, 370, 436, 434, 406, 3983, 281, 264, 912, 1785, 1908, 2136, 6064, 300, 300, 321, 519, 295, 420, 300, 291, 458, 300, 264, 2463, 307, 1670, 436, 434, 257, 4551, 2237, 13], "temperature": 0.0, "avg_logprob": -0.10766366322835287, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.041344598386786e-06}, {"id": 888, "seek": 595520, "start": 5955.2, "end": 5970.2, "text": " And I just I don't see how there you know there's no way they could have a data set that's not racially biased and then combining that with kind of what I mentioned earlier with this like repeated failure of computer vision on people of color.", "tokens": [400, 286, 445, 286, 500, 380, 536, 577, 456, 291, 458, 456, 311, 572, 636, 436, 727, 362, 257, 1412, 992, 300, 311, 406, 4129, 2270, 28035, 293, 550, 21928, 300, 365, 733, 295, 437, 286, 2835, 3071, 365, 341, 411, 10477, 7763, 295, 3820, 5201, 322, 561, 295, 2017, 13], "temperature": 0.0, "avg_logprob": -0.08221237009221857, "compression_ratio": 1.49079754601227, "no_speech_prob": 4.1570337998564355e-06}, {"id": 889, "seek": 597020, "start": 5970.2, "end": 5992.2, "text": " That yeah I don't have a solution of this but this is a scary and something I think more more pressure needs to be applied about addressing this.", "tokens": [663, 1338, 286, 500, 380, 362, 257, 3827, 295, 341, 457, 341, 307, 257, 6958, 293, 746, 286, 519, 544, 544, 3321, 2203, 281, 312, 6456, 466, 14329, 341, 13], "temperature": 0.0, "avg_logprob": -0.16249248560737162, "compression_ratio": 1.3063063063063063, "no_speech_prob": 1.544305996503681e-05}, {"id": 890, "seek": 599220, "start": 5992.2, "end": 6013.2, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.3024991949399312, "compression_ratio": 0.3333333333333333, "no_speech_prob": 1.0951671356451698e-05}, {"id": 891, "seek": 601320, "start": 6013.2, "end": 6025.2, "text": " Yeah that's interesting. So the question is could we use some of this kind of whitewashing to try to remove indicators of race from this footage and would that help to bias it.", "tokens": [865, 300, 311, 1880, 13, 407, 264, 1168, 307, 727, 321, 764, 512, 295, 341, 733, 295, 2418, 6569, 571, 281, 853, 281, 4159, 22176, 295, 4569, 490, 341, 9556, 293, 576, 300, 854, 281, 12577, 309, 13], "temperature": 0.0, "avg_logprob": -0.11130200113568987, "compression_ratio": 1.3643410852713178, "no_speech_prob": 4.0920958781498484e-06}, {"id": 892, "seek": 602520, "start": 6025.2, "end": 6043.2, "text": " I mean that's a really interesting question. I would imagine that it's very hard to completely remove race from like using it but yeah that is an interesting thought and it does kind of raise.", "tokens": [286, 914, 300, 311, 257, 534, 1880, 1168, 13, 286, 576, 3811, 300, 309, 311, 588, 1152, 281, 2584, 4159, 4569, 490, 411, 1228, 309, 457, 1338, 300, 307, 364, 1880, 1194, 293, 309, 775, 733, 295, 5300, 13], "temperature": 0.0, "avg_logprob": -0.08452180928962175, "compression_ratio": 1.4222222222222223, "no_speech_prob": 2.025985395448515e-06}, {"id": 893, "seek": 604320, "start": 6043.2, "end": 6066.2, "text": " I guess I'll get to this later but this kind of question like I definitely think there's in general the possibility with AI of us having an opportunity to try to address some of our biases and to create systems that are less biased than we are as humans and so that there's promising potential there.", "tokens": [286, 2041, 286, 603, 483, 281, 341, 1780, 457, 341, 733, 295, 1168, 411, 286, 2138, 519, 456, 311, 294, 2674, 264, 7959, 365, 7318, 295, 505, 1419, 364, 2650, 281, 853, 281, 2985, 512, 295, 527, 32152, 293, 281, 1884, 3652, 300, 366, 1570, 28035, 813, 321, 366, 382, 6255, 293, 370, 300, 456, 311, 20257, 3995, 456, 13], "temperature": 0.0, "avg_logprob": -0.11985689401626587, "compression_ratio": 1.5789473684210527, "no_speech_prob": 7.527474735979922e-06}, {"id": 894, "seek": 606620, "start": 6066.2, "end": 6078.2, "text": " Other questions. OK so these are I mean these were some really dark examples and I do sometimes kind of get questions from people like almost like how can we stop AI.", "tokens": [5358, 1651, 13, 2264, 370, 613, 366, 286, 914, 613, 645, 512, 534, 2877, 5110, 293, 286, 360, 2171, 733, 295, 483, 1651, 490, 561, 411, 1920, 411, 577, 393, 321, 1590, 7318, 13], "temperature": 0.0, "avg_logprob": -0.16674341653522692, "compression_ratio": 1.3495934959349594, "no_speech_prob": 8.529146725777537e-06}, {"id": 895, "seek": 607820, "start": 6078.2, "end": 6097.2, "text": " So I just wanted to remind you of kind of the examples I talked about earlier with some fast AI students who are working on Parkinson's disease and addressing illegal deforestation or helping helping farmers qualify for better loans creating more more resources for under", "tokens": [407, 286, 445, 1415, 281, 4160, 291, 295, 733, 295, 264, 5110, 286, 2825, 466, 3071, 365, 512, 2370, 7318, 1731, 567, 366, 1364, 322, 35823, 311, 4752, 293, 14329, 11905, 368, 845, 19159, 420, 4315, 4315, 11339, 20276, 337, 1101, 15443, 4084, 544, 544, 3593, 337, 833], "temperature": 0.0, "avg_logprob": -0.0808572585766132, "compression_ratio": 1.489010989010989, "no_speech_prob": 6.374952477017359e-07}, {"id": 896, "seek": 609720, "start": 6097.2, "end": 6110.2, "text": " resourced languages and also to bring up the medical example. So this example this slide is from my partner Jeremy Howard but there's a huge shortage of doctors in much of the world.", "tokens": [725, 396, 1232, 8650, 293, 611, 281, 1565, 493, 264, 4625, 1365, 13, 407, 341, 1365, 341, 4137, 307, 490, 452, 4975, 17809, 17626, 457, 456, 311, 257, 2603, 24708, 295, 8778, 294, 709, 295, 264, 1002, 13], "temperature": 0.0, "avg_logprob": -0.1343112900143578, "compression_ratio": 1.348148148148148, "no_speech_prob": 1.0442981874803081e-05}, {"id": 897, "seek": 611020, "start": 6110.2, "end": 6127.2, "text": " This slide is from Nigeria. Nigeria would need over 700000 additional doctors by 2030 and it would take 300 years to train that many people. And this is true in many countries kind of similar numbers.", "tokens": [639, 4137, 307, 490, 28828, 13, 28828, 576, 643, 670, 15204, 1360, 4497, 8778, 538, 28638, 293, 309, 576, 747, 6641, 924, 281, 3847, 300, 867, 561, 13, 400, 341, 307, 2074, 294, 867, 3517, 733, 295, 2531, 3547, 13], "temperature": 0.0, "avg_logprob": -0.08206528967077081, "compression_ratio": 1.3513513513513513, "no_speech_prob": 1.2799150681530591e-05}, {"id": 898, "seek": 612720, "start": 6127.2, "end": 6145.2, "text": " So this is an area I think it's really important when thinking about deep learning or AI to try to balance that there's kind of all these really positive opportunities as well as all these really scary threats and I feel like I hear from a lot of people that are kind of either at one extreme of being these techno utopians.", "tokens": [407, 341, 307, 364, 1859, 286, 519, 309, 311, 534, 1021, 562, 1953, 466, 2452, 2539, 420, 7318, 281, 853, 281, 4772, 300, 456, 311, 733, 295, 439, 613, 534, 3353, 4786, 382, 731, 382, 439, 613, 534, 6958, 14909, 293, 286, 841, 411, 286, 1568, 490, 257, 688, 295, 561, 300, 366, 733, 295, 2139, 412, 472, 8084, 295, 885, 613, 36728, 2839, 404, 2567, 13], "temperature": 0.0, "avg_logprob": -0.1314546826859595, "compression_ratio": 1.653061224489796, "no_speech_prob": 3.76098723791074e-05}, {"id": 899, "seek": 614520, "start": 6145.2, "end": 6162.2, "text": " We don't have to worry tech will solve everything or people that even admit like I'm a Luddite I want to stop AI and so to really try to hold on to kind of both that we have like a lot of a lot of opportunity as well as a lot of scary risk.", "tokens": [492, 500, 380, 362, 281, 3292, 7553, 486, 5039, 1203, 420, 561, 300, 754, 9796, 411, 286, 478, 257, 441, 26656, 642, 286, 528, 281, 1590, 7318, 293, 370, 281, 534, 853, 281, 1797, 322, 281, 733, 295, 1293, 300, 321, 362, 411, 257, 688, 295, 257, 688, 295, 2650, 382, 731, 382, 257, 688, 295, 6958, 3148, 13], "temperature": 0.0, "avg_logprob": -0.10440877127268958, "compression_ratio": 1.5, "no_speech_prob": 1.0948614544759039e-05}, {"id": 900, "seek": 616220, "start": 6162.2, "end": 6181.2, "text": " And so this is this is from Jeremy started startup called in Liddick a few years ago that was the first startup to apply deep learning to medicine and they specifically were working with radiology data and they this data is from 2015 so they have improved in the two years since then.", "tokens": [400, 370, 341, 307, 341, 307, 490, 17809, 1409, 18578, 1219, 294, 441, 14273, 618, 257, 1326, 924, 2057, 300, 390, 264, 700, 18578, 281, 3079, 2452, 2539, 281, 7195, 293, 436, 4682, 645, 1364, 365, 16335, 1793, 1412, 293, 436, 341, 1412, 307, 490, 7546, 370, 436, 362, 9689, 294, 264, 732, 924, 1670, 550, 13], "temperature": 0.0, "avg_logprob": -0.17922735995933659, "compression_ratio": 1.595505617977528, "no_speech_prob": 2.1768733859062195e-05}, {"id": 901, "seek": 618120, "start": 6181.2, "end": 6192.2, "text": " They were this is for identifying lung cancer and they were more accurate than human radiologist so they had.", "tokens": [814, 645, 341, 307, 337, 16696, 16730, 5592, 293, 436, 645, 544, 8559, 813, 1952, 16335, 9201, 370, 436, 632, 13], "temperature": 0.0, "avg_logprob": -0.18789552997898412, "compression_ratio": 1.4521739130434783, "no_speech_prob": 3.784839691434172e-06}, {"id": 902, "seek": 618120, "start": 6192.2, "end": 6199.2, "text": " They had fewer false negatives and fewer false positives.", "tokens": [814, 632, 13366, 7908, 40019, 293, 13366, 7908, 35127, 13], "temperature": 0.0, "avg_logprob": -0.18789552997898412, "compression_ratio": 1.4521739130434783, "no_speech_prob": 3.784839691434172e-06}, {"id": 903, "seek": 619920, "start": 6199.2, "end": 6213.2, "text": " And this is like a panel of highly qualified radiologists and so that's also something that can be concerning to hear about even in places where there are lots of doctors the error rates are often higher than you would be comfortable knowing about.", "tokens": [400, 341, 307, 411, 257, 4831, 295, 5405, 15904, 16335, 12256, 293, 370, 300, 311, 611, 746, 300, 393, 312, 18087, 281, 1568, 466, 754, 294, 3190, 689, 456, 366, 3195, 295, 8778, 264, 6713, 6846, 366, 2049, 2946, 813, 291, 576, 312, 4619, 5276, 466, 13], "temperature": 0.0, "avg_logprob": -0.07518864400459058, "compression_ratio": 1.5876288659793814, "no_speech_prob": 8.663421795063186e-06}, {"id": 904, "seek": 619920, "start": 6213.2, "end": 6220.2, "text": " And so this is an area where deep learning can really help.", "tokens": [400, 370, 341, 307, 364, 1859, 689, 2452, 2539, 393, 534, 854, 13], "temperature": 0.0, "avg_logprob": -0.07518864400459058, "compression_ratio": 1.5876288659793814, "no_speech_prob": 8.663421795063186e-06}, {"id": 905, "seek": 622020, "start": 6220.2, "end": 6240.2, "text": " Yeah and then here I wanted to emphasize and I think we've kind of already talked about this and I'm running out of time that humans are really biased as well and so when we think about you know this is why we create bias technology, but the alternative to not using tech at all is also that you know like we have human.", "tokens": [865, 293, 550, 510, 286, 1415, 281, 16078, 293, 286, 519, 321, 600, 733, 295, 1217, 2825, 466, 341, 293, 286, 478, 2614, 484, 295, 565, 300, 6255, 366, 534, 28035, 382, 731, 293, 370, 562, 321, 519, 466, 291, 458, 341, 307, 983, 321, 1884, 12577, 2899, 11, 457, 264, 8535, 281, 406, 1228, 7553, 412, 439, 307, 611, 300, 291, 458, 411, 321, 362, 1952, 13], "temperature": 0.0, "avg_logprob": -0.17723355028364393, "compression_ratio": 1.6080402010050252, "no_speech_prob": 1.6535046825083555e-06}, {"id": 906, "seek": 624020, "start": 6240.2, "end": 6259.2, "text": " Human biases and so it would be awesome if we could create tech that was that removed bias and so this is about rate race and medicine just some really sad statistics about kind of the worst worst quality of care, particularly that black and Latino patients receive.", "tokens": [10294, 32152, 293, 370, 309, 576, 312, 3476, 498, 321, 727, 1884, 7553, 300, 390, 300, 7261, 12577, 293, 370, 341, 307, 466, 3314, 4569, 293, 7195, 445, 512, 534, 4227, 12523, 466, 733, 295, 264, 5855, 5855, 3125, 295, 1127, 11, 4098, 300, 2211, 293, 25422, 4209, 4774, 13], "temperature": 0.0, "avg_logprob": -0.1353335160475511, "compression_ratio": 1.57, "no_speech_prob": 4.860049557464663e-06}, {"id": 907, "seek": 624020, "start": 6259.2, "end": 6265.2, "text": " This is from an article in Scientific American.", "tokens": [639, 307, 490, 364, 7222, 294, 47437, 2665, 13], "temperature": 0.0, "avg_logprob": -0.1353335160475511, "compression_ratio": 1.57, "no_speech_prob": 4.860049557464663e-06}, {"id": 908, "seek": 626520, "start": 6265.2, "end": 6278.2, "text": " So the Kaliskin Islam paper that I've been referencing had a quote saying one advantage of AI at least where the algorithms and outcomes are open to inspection is that it can at least make such errors explicit and therefore subject to monitoring and correction.", "tokens": [407, 264, 12655, 271, 5843, 8571, 3035, 300, 286, 600, 668, 40582, 632, 257, 6513, 1566, 472, 5002, 295, 7318, 412, 1935, 689, 264, 14642, 293, 10070, 366, 1269, 281, 22085, 307, 300, 309, 393, 412, 1935, 652, 1270, 13603, 13691, 293, 4412, 3983, 281, 11028, 293, 19984, 13], "temperature": 0.0, "avg_logprob": -0.10871484964200766, "compression_ratio": 1.7683823529411764, "no_speech_prob": 3.0234337828005664e-05}, {"id": 909, "seek": 626520, "start": 6278.2, "end": 6290.2, "text": " And so there's you know this big if you know where they're open and that you know some of these algorithms we've been talking about are not open, but there is this possibility that like when you do have open algorithms.", "tokens": [400, 370, 456, 311, 291, 458, 341, 955, 498, 291, 458, 689, 436, 434, 1269, 293, 300, 291, 458, 512, 295, 613, 14642, 321, 600, 668, 1417, 466, 366, 406, 1269, 11, 457, 456, 307, 341, 7959, 300, 411, 562, 291, 360, 362, 1269, 14642, 13], "temperature": 0.0, "avg_logprob": -0.10871484964200766, "compression_ratio": 1.7683823529411764, "no_speech_prob": 3.0234337828005664e-05}, {"id": 910, "seek": 629020, "start": 6290.2, "end": 6301.2, "text": " We can correct biases in a way that human doctors have not been able to kind of correct their their biases.", "tokens": [492, 393, 3006, 32152, 294, 257, 636, 300, 1952, 8778, 362, 406, 668, 1075, 281, 733, 295, 3006, 641, 641, 32152, 13], "temperature": 0.0, "avg_logprob": -0.06140631437301636, "compression_ratio": 1.4753086419753085, "no_speech_prob": 1.0128090252692346e-05}, {"id": 911, "seek": 629020, "start": 6301.2, "end": 6309.2, "text": " And so I put together some questions that I think are really helpful to ask when evaluating AI to try to get at this issue of bias.", "tokens": [400, 370, 286, 829, 1214, 512, 1651, 300, 286, 519, 366, 534, 4961, 281, 1029, 562, 27479, 7318, 281, 853, 281, 483, 412, 341, 2734, 295, 12577, 13], "temperature": 0.0, "avg_logprob": -0.06140631437301636, "compression_ratio": 1.4753086419753085, "no_speech_prob": 1.0128090252692346e-05}, {"id": 912, "seek": 630920, "start": 6309.2, "end": 6322.2, "text": " One is what bias may be in the data and this is a great quote from Angela Bassa who's the director of data science for iRobot and she said it's not that data can be biased and data is biased.", "tokens": [1485, 307, 437, 12577, 815, 312, 294, 264, 1412, 293, 341, 307, 257, 869, 6513, 490, 20848, 363, 18948, 567, 311, 264, 5391, 295, 1412, 3497, 337, 741, 26332, 310, 293, 750, 848, 309, 311, 406, 300, 1412, 393, 312, 28035, 293, 1412, 307, 28035, 13], "temperature": 0.0, "avg_logprob": -0.12542015758912956, "compression_ratio": 1.5568862275449102, "no_speech_prob": 2.8844764528912492e-05}, {"id": 913, "seek": 630920, "start": 6322.2, "end": 6328.2, "text": " If you want to use data you need to understand how it was generated.", "tokens": [759, 291, 528, 281, 764, 1412, 291, 643, 281, 1223, 577, 309, 390, 10833, 13], "temperature": 0.0, "avg_logprob": -0.12542015758912956, "compression_ratio": 1.5568862275449102, "no_speech_prob": 2.8844764528912492e-05}, {"id": 914, "seek": 632820, "start": 6328.2, "end": 6342.2, "text": " How diverse is the team that built it. I think that that is a huge problem like I think that teams need to be representative of the people that will be impacted by their technology and right now that's not the case.", "tokens": [1012, 9521, 307, 264, 1469, 300, 3094, 309, 13, 286, 519, 300, 300, 307, 257, 2603, 1154, 411, 286, 519, 300, 5491, 643, 281, 312, 12424, 295, 264, 561, 300, 486, 312, 15653, 538, 641, 2899, 293, 558, 586, 300, 311, 406, 264, 1389, 13], "temperature": 0.0, "avg_logprob": -0.08216035609342615, "compression_ratio": 1.4930555555555556, "no_speech_prob": 8.394353244511876e-06}, {"id": 915, "seek": 634220, "start": 6342.2, "end": 6361.2, "text": " Can the code and data be audited or the open source and I think this and Cathy O'Neill argues for this in her book weapons of mass destruction and she gives a lot of examples of algorithms or software that are being used in hiring or firing decisions.", "tokens": [1664, 264, 3089, 293, 1412, 312, 2379, 1226, 420, 264, 1269, 4009, 293, 286, 519, 341, 293, 39799, 422, 6, 15496, 373, 38218, 337, 341, 294, 720, 1446, 7278, 295, 2758, 13563, 293, 750, 2709, 257, 688, 295, 5110, 295, 14642, 420, 4722, 300, 366, 885, 1143, 294, 15335, 420, 16045, 5327, 13], "temperature": 0.0, "avg_logprob": -0.09696906909608004, "compression_ratio": 1.494047619047619, "no_speech_prob": 6.745616701664403e-06}, {"id": 916, "seek": 636120, "start": 6361.2, "end": 6375.2, "text": " Yeah kind of these these big decisions also yeah with the prison sentences and when those are not not open to being audited at least we don't have kind of like public oversight to what's happening.", "tokens": [865, 733, 295, 613, 613, 955, 5327, 611, 1338, 365, 264, 6168, 16579, 293, 562, 729, 366, 406, 406, 1269, 281, 885, 2379, 1226, 412, 1935, 321, 500, 380, 362, 733, 295, 411, 1908, 29146, 281, 437, 311, 2737, 13], "temperature": 0.0, "avg_logprob": -0.06620868576897515, "compression_ratio": 1.6341463414634145, "no_speech_prob": 6.641992513323203e-06}, {"id": 917, "seek": 636120, "start": 6375.2, "end": 6387.2, "text": " What are error rates for different subgroups. So again with the recidivism software. If you just looked at the overall error rate that wouldn't capture how different it was for white and black defendants.", "tokens": [708, 366, 6713, 6846, 337, 819, 1422, 17377, 82, 13, 407, 797, 365, 264, 850, 327, 592, 1434, 4722, 13, 759, 291, 445, 2956, 412, 264, 4787, 6713, 3314, 300, 2759, 380, 7983, 577, 819, 309, 390, 337, 2418, 293, 2211, 8602, 1719, 13], "temperature": 0.0, "avg_logprob": -0.06620868576897515, "compression_ratio": 1.6341463414634145, "no_speech_prob": 6.641992513323203e-06}, {"id": 918, "seek": 638720, "start": 6387.2, "end": 6404.2, "text": " What is the accuracy of a simple rule based alternative. That's really helpful to just kind of get a baseline of what a reasonable accuracy is to know even you know is your is your software kind of performing better than what can be done with something much simpler.", "tokens": [708, 307, 264, 14170, 295, 257, 2199, 4978, 2361, 8535, 13, 663, 311, 534, 4961, 281, 445, 733, 295, 483, 257, 20518, 295, 437, 257, 10585, 14170, 307, 281, 458, 754, 291, 458, 307, 428, 307, 428, 4722, 733, 295, 10205, 1101, 813, 437, 393, 312, 1096, 365, 746, 709, 18587, 13], "temperature": 0.0, "avg_logprob": -0.07749570267541069, "compression_ratio": 1.5647058823529412, "no_speech_prob": 1.8340668248129077e-05}, {"id": 919, "seek": 640420, "start": 6404.2, "end": 6428.2, "text": " And then what processes are in place to handle appeals or mistakes and I think this is really important for things that are impacting people's lives. So Cathy O'Neill gives an example of software that was used to fire teachers and no reason would be given and so she talks with teachers that were the students love them the parents love them the principal love them and they get fired and then they're not given a reason it's just like the algorithm said so.", "tokens": [400, 550, 437, 7555, 366, 294, 1081, 281, 4813, 32603, 420, 8038, 293, 286, 519, 341, 307, 534, 1021, 337, 721, 300, 366, 29963, 561, 311, 2909, 13, 407, 39799, 422, 6, 15496, 373, 2709, 364, 1365, 295, 4722, 300, 390, 1143, 281, 2610, 6023, 293, 572, 1778, 576, 312, 2212, 293, 370, 750, 6686, 365, 6023, 300, 645, 264, 1731, 959, 552, 264, 3152, 959, 552, 264, 9716, 959, 552, 293, 436, 483, 11777, 293, 550, 436, 434, 406, 2212, 257, 1778, 309, 311, 445, 411, 264, 9284, 848, 370, 13], "temperature": 0.0, "avg_logprob": -0.10286079843839009, "compression_ratio": 1.7547892720306513, "no_speech_prob": 4.860060471401084e-06}, {"id": 920, "seek": 642820, "start": 6428.2, "end": 6443.2, "text": " So these are good questions to ask about AI. I should also I should have been distinguishing as I went. I tried to point out when I was giving examples of things that use deep learning some of the examples I gave other algorithms and are not necessarily deep learning.", "tokens": [407, 613, 366, 665, 1651, 281, 1029, 466, 7318, 13, 286, 820, 611, 286, 820, 362, 668, 11365, 3807, 382, 286, 1437, 13, 286, 3031, 281, 935, 484, 562, 286, 390, 2902, 5110, 295, 721, 300, 764, 2452, 2539, 512, 295, 264, 5110, 286, 2729, 661, 14642, 293, 366, 406, 4725, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.09791372576330462, "compression_ratio": 1.8496732026143792, "no_speech_prob": 3.3207219530595466e-05}, {"id": 921, "seek": 642820, "start": 6443.2, "end": 6457.2, "text": " For instance the recidivism software I don't think is deep learning but I wanted to talk about them because they raise these important ethical issues and increasingly deep learning is being used for more and more things and so I think deep learning will be applied to these problems in the future.", "tokens": [1171, 5197, 264, 850, 327, 592, 1434, 4722, 286, 500, 380, 519, 307, 2452, 2539, 457, 286, 1415, 281, 751, 466, 552, 570, 436, 5300, 613, 1021, 18890, 2663, 293, 12980, 2452, 2539, 307, 885, 1143, 337, 544, 293, 544, 721, 293, 370, 286, 519, 2452, 2539, 486, 312, 6456, 281, 613, 2740, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.09791372576330462, "compression_ratio": 1.8496732026143792, "no_speech_prob": 3.3207219530595466e-05}, {"id": 922, "seek": 645720, "start": 6457.2, "end": 6461.2, "text": " Even in cases where other algorithms are being used now.", "tokens": [2754, 294, 3331, 689, 661, 14642, 366, 885, 1143, 586, 13], "temperature": 0.0, "avg_logprob": -0.12369361188676622, "compression_ratio": 1.2767857142857142, "no_speech_prob": 4.830066245631315e-05}, {"id": 923, "seek": 645720, "start": 6461.2, "end": 6467.2, "text": " Any questions about these or additional questions you think would be good to ask.", "tokens": [2639, 1651, 466, 613, 420, 4497, 1651, 291, 519, 576, 312, 665, 281, 1029, 13], "temperature": 0.0, "avg_logprob": -0.12369361188676622, "compression_ratio": 1.2767857142857142, "no_speech_prob": 4.830066245631315e-05}, {"id": 924, "seek": 645720, "start": 6467.2, "end": 6475.2, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.12369361188676622, "compression_ratio": 1.2767857142857142, "no_speech_prob": 4.830066245631315e-05}, {"id": 925, "seek": 647520, "start": 6475.2, "end": 6488.2, "text": " Oh yes.", "tokens": [876, 2086, 13], "temperature": 0.0, "avg_logprob": -0.24282705783843994, "compression_ratio": 0.4666666666666667, "no_speech_prob": 1.5677329429308884e-05}, {"id": 926, "seek": 648820, "start": 6488.2, "end": 6512.2, "text": " Okay so the question was about how referring back to it so it was Sarah Hooker with Delta Analytics and Rainforest Connection that are capturing sound from the jungle. How is that sound data represented and so that is kind of not not represented with word embeddings that's with the frequencies and I actually think that for a while they were treating those as pictures.", "tokens": [1033, 370, 264, 1168, 390, 466, 577, 13761, 646, 281, 309, 370, 309, 390, 9519, 33132, 260, 365, 18183, 25944, 293, 14487, 36629, 11653, 313, 300, 366, 23384, 1626, 490, 264, 18228, 13, 1012, 307, 300, 1626, 1412, 10379, 293, 370, 300, 307, 733, 295, 406, 406, 10379, 365, 1349, 12240, 29432, 300, 311, 365, 264, 20250, 293, 286, 767, 519, 300, 337, 257, 1339, 436, 645, 15083, 729, 382, 5242, 13], "temperature": 0.0, "avg_logprob": -0.16133234375401548, "compression_ratio": 1.5948275862068966, "no_speech_prob": 2.974943163280841e-05}, {"id": 927, "seek": 651220, "start": 6512.2, "end": 6539.2, "text": " So you can you know represent sound frequencies as numbers or you can just kind of have the picture and I believe that that's what they were using and so then kind of applying these convolutional neural networks which are what are used for a lot of image problems to basically the images of the sound waves.", "tokens": [407, 291, 393, 291, 458, 2906, 1626, 20250, 382, 3547, 420, 291, 393, 445, 733, 295, 362, 264, 3036, 293, 286, 1697, 300, 300, 311, 437, 436, 645, 1228, 293, 370, 550, 733, 295, 9275, 613, 45216, 304, 18161, 9590, 597, 366, 437, 366, 1143, 337, 257, 688, 295, 3256, 2740, 281, 1936, 264, 5267, 295, 264, 1626, 9417, 13], "temperature": 0.0, "avg_logprob": -0.06363394111394882, "compression_ratio": 1.6157894736842104, "no_speech_prob": 8.39613039715914e-06}, {"id": 928, "seek": 653920, "start": 6539.2, "end": 6568.2, "text": " So I mean so it's it's relation. The question was is it's still relational. So deep learning and I won't have time to really get into this today but deep learning is learning patterns for the most part so you kind of have all these parameters that are learning patterns and so it is recognizing patterns and you can use that to get similarity and like this is something I would like to see more of but that would be possible to say like what.", "tokens": [407, 286, 914, 370, 309, 311, 309, 311, 9721, 13, 440, 1168, 390, 307, 309, 311, 920, 38444, 13, 407, 2452, 2539, 293, 286, 1582, 380, 362, 565, 281, 534, 483, 666, 341, 965, 457, 2452, 2539, 307, 2539, 8294, 337, 264, 881, 644, 370, 291, 733, 295, 362, 439, 613, 9834, 300, 366, 2539, 8294, 293, 370, 309, 307, 18538, 8294, 293, 291, 393, 764, 300, 281, 483, 32194, 293, 411, 341, 307, 746, 286, 576, 411, 281, 536, 544, 295, 457, 300, 576, 312, 1944, 281, 584, 411, 437, 13], "temperature": 0.0, "avg_logprob": -0.1657802164554596, "compression_ratio": 1.8493723849372385, "no_speech_prob": 4.7565048589603975e-05}, {"id": 929, "seek": 656820, "start": 6568.2, "end": 6576.2, "text": " What other sounds you know sound you've heard in the past are most similar to the sound and so you can get that back.", "tokens": [708, 661, 3263, 291, 458, 1626, 291, 600, 2198, 294, 264, 1791, 366, 881, 2531, 281, 264, 1626, 293, 370, 291, 393, 483, 300, 646, 13], "temperature": 0.0, "avg_logprob": -0.09626905734722431, "compression_ratio": 1.6479591836734695, "no_speech_prob": 2.467987906129565e-05}, {"id": 930, "seek": 656820, "start": 6576.2, "end": 6586.2, "text": " Although that's not not necessarily going to be explicit there you know like that's a kind of question you have to ask it or you know program it to do but it's possible. Good question.", "tokens": [5780, 300, 311, 406, 406, 4725, 516, 281, 312, 13691, 456, 291, 458, 411, 300, 311, 257, 733, 295, 1168, 291, 362, 281, 1029, 309, 420, 291, 458, 1461, 309, 281, 360, 457, 309, 311, 1944, 13, 2205, 1168, 13], "temperature": 0.0, "avg_logprob": -0.09626905734722431, "compression_ratio": 1.6479591836734695, "no_speech_prob": 2.467987906129565e-05}, {"id": 931, "seek": 656820, "start": 6586.2, "end": 6591.2, "text": " Any other questions.", "tokens": [2639, 661, 1651, 13], "temperature": 0.0, "avg_logprob": -0.09626905734722431, "compression_ratio": 1.6479591836734695, "no_speech_prob": 2.467987906129565e-05}, {"id": 932, "seek": 659120, "start": 6591.2, "end": 6607.2, "text": " Okay. So this is a quote from a Linda Gates that I really agree with. If we don't get women and people of color at the table real technologists doing the real work we will buy a systems trying to reverse that a decade or two from now will be so much more difficult if not close to impossible.", "tokens": [1033, 13, 407, 341, 307, 257, 6513, 490, 257, 20324, 26494, 300, 286, 534, 3986, 365, 13, 759, 321, 500, 380, 483, 2266, 293, 561, 295, 2017, 412, 264, 3199, 957, 1537, 12256, 884, 264, 957, 589, 321, 486, 2256, 257, 3652, 1382, 281, 9943, 300, 257, 10378, 420, 732, 490, 586, 486, 312, 370, 709, 544, 2252, 498, 406, 1998, 281, 6243, 13], "temperature": 0.0, "avg_logprob": -0.11336669501136332, "compression_ratio": 1.482233502538071, "no_speech_prob": 1.862455246737227e-05}, {"id": 933, "seek": 660720, "start": 6607.2, "end": 6628.2, "text": " And I think we're at a really in a lot of ways a very exciting point in history but so much technology is being built right now and will be you know for the next next five years that it's really really important I think to to see more diversity in the field of AI as soon as possible because it will be so much harder.", "tokens": [400, 286, 519, 321, 434, 412, 257, 534, 294, 257, 688, 295, 2098, 257, 588, 4670, 935, 294, 2503, 457, 370, 709, 2899, 307, 885, 3094, 558, 586, 293, 486, 312, 291, 458, 337, 264, 958, 958, 1732, 924, 300, 309, 311, 534, 534, 1021, 286, 519, 281, 281, 536, 544, 8811, 294, 264, 2519, 295, 7318, 382, 2321, 382, 1944, 570, 309, 486, 312, 370, 709, 6081, 13], "temperature": 0.0, "avg_logprob": -0.14469916199984617, "compression_ratio": 1.59, "no_speech_prob": 7.71869090385735e-05}, {"id": 934, "seek": 662820, "start": 6628.2, "end": 6641.2, "text": " If we do encode kind of all our existing societal biases into this technology that's going to be much harder to undo in the future than to try to address it now as we're building the technology.", "tokens": [759, 321, 360, 2058, 1429, 733, 295, 439, 527, 6741, 33472, 32152, 666, 341, 2899, 300, 311, 516, 281, 312, 709, 6081, 281, 23779, 294, 264, 2027, 813, 281, 853, 281, 2985, 309, 586, 382, 321, 434, 2390, 264, 2899, 13], "temperature": 0.0, "avg_logprob": -0.09073076965988323, "compression_ratio": 1.7701612903225807, "no_speech_prob": 5.593919922830537e-06}, {"id": 935, "seek": 662820, "start": 6641.2, "end": 6654.2, "text": " So yeah I just wanted to encourage you that if you find if you find this interesting that you consider. Yeah consider learning more and getting more into the field of deep learning because the field really needs more women more people of color.", "tokens": [407, 1338, 286, 445, 1415, 281, 5373, 291, 300, 498, 291, 915, 498, 291, 915, 341, 1880, 300, 291, 1949, 13, 865, 1949, 2539, 544, 293, 1242, 544, 666, 264, 2519, 295, 2452, 2539, 570, 264, 2519, 534, 2203, 544, 2266, 544, 561, 295, 2017, 13], "temperature": 0.0, "avg_logprob": -0.09073076965988323, "compression_ratio": 1.7701612903225807, "no_speech_prob": 5.593919922830537e-06}, {"id": 936, "seek": 665420, "start": 6654.2, "end": 6664.2, "text": " It needs more people who care about ethics and bias and inclusion and social good and are asking these questions and just people with a wide variety of backgrounds.", "tokens": [467, 2203, 544, 561, 567, 1127, 466, 19769, 293, 12577, 293, 15874, 293, 2093, 665, 293, 366, 3365, 613, 1651, 293, 445, 561, 365, 257, 4874, 5673, 295, 17336, 13], "temperature": 0.0, "avg_logprob": -0.10513170560201009, "compression_ratio": 1.4112903225806452, "no_speech_prob": 2.668301203812007e-05}, {"id": 937, "seek": 665420, "start": 6664.2, "end": 6668.2, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.10513170560201009, "compression_ratio": 1.4112903225806452, "no_speech_prob": 2.668301203812007e-05}, {"id": 938, "seek": 665420, "start": 6668.2, "end": 6683.2, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.10513170560201009, "compression_ratio": 1.4112903225806452, "no_speech_prob": 2.668301203812007e-05}, {"id": 939, "seek": 668320, "start": 6683.2, "end": 6691.2, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.15407894819210738, "compression_ratio": 1.6682464454976302, "no_speech_prob": 4.329005241743289e-05}, {"id": 940, "seek": 668320, "start": 6691.2, "end": 6712.2, "text": " But yeah so the question was about the compass software which is the recidivism software saying that a lot of writing about this has been like oh the algorithms bad and that the large part of the issue is these questions that were being asked have so much bias and someone else pointed out earlier things that aren't even legal for a judge to ask.", "tokens": [583, 1338, 370, 264, 1168, 390, 466, 264, 10707, 4722, 597, 307, 264, 850, 327, 592, 1434, 4722, 1566, 300, 257, 688, 295, 3579, 466, 341, 575, 668, 411, 1954, 264, 14642, 1578, 293, 300, 264, 2416, 644, 295, 264, 2734, 307, 613, 1651, 300, 645, 885, 2351, 362, 370, 709, 12577, 293, 1580, 1646, 10932, 484, 3071, 721, 300, 3212, 380, 754, 5089, 337, 257, 6995, 281, 1029, 13], "temperature": 0.0, "avg_logprob": -0.15407894819210738, "compression_ratio": 1.6682464454976302, "no_speech_prob": 4.329005241743289e-05}, {"id": 941, "seek": 671220, "start": 6712.2, "end": 6733.2, "text": " Yeah and I think the bias plays a huge role like I would say that I think to have an algorithm that was making unbiased decisions I think would probably be an improvement over we know that judges are really biased and the decisions they make and so if you had something that could make decisions that were better than the decisions human judges make that that would be a good thing.", "tokens": [865, 293, 286, 519, 264, 12577, 5749, 257, 2603, 3090, 411, 286, 576, 584, 300, 286, 519, 281, 362, 364, 9284, 300, 390, 1455, 517, 5614, 1937, 5327, 286, 519, 576, 1391, 312, 364, 10444, 670, 321, 458, 300, 14449, 366, 534, 28035, 293, 264, 5327, 436, 652, 293, 370, 498, 291, 632, 746, 300, 727, 652, 5327, 300, 645, 1101, 813, 264, 5327, 1952, 14449, 652, 300, 300, 576, 312, 257, 665, 551, 13], "temperature": 0.0, "avg_logprob": -0.14458609151316212, "compression_ratio": 1.8275862068965518, "no_speech_prob": 9.366970516566653e-06}, {"id": 942, "seek": 671220, "start": 6733.2, "end": 6738.2, "text": " But yeah the problem is that this is not.", "tokens": [583, 1338, 264, 1154, 307, 300, 341, 307, 406, 13], "temperature": 0.0, "avg_logprob": -0.14458609151316212, "compression_ratio": 1.8275862068965518, "no_speech_prob": 9.366970516566653e-06}, {"id": 943, "seek": 673820, "start": 6738.2, "end": 6751.2, "text": " It's not a question. And then I wanted to share a quote from Francois Chalet who's someone I really admire. He created the library Keras which is a library for neural networks.", "tokens": [467, 311, 406, 257, 1168, 13, 400, 550, 286, 1415, 281, 2073, 257, 6513, 490, 34695, 271, 761, 49744, 567, 311, 1580, 286, 534, 21951, 13, 634, 2942, 264, 6405, 591, 6985, 597, 307, 257, 6405, 337, 18161, 9590, 13], "temperature": 0.0, "avg_logprob": -0.1961563931235784, "compression_ratio": 1.4930232558139536, "no_speech_prob": 2.2469930627266876e-05}, {"id": 944, "seek": 673820, "start": 6751.2, "end": 6759.2, "text": " I think it's the most accessible library out there. So if you're interested in deep learning I recommend you start with Keras spelled K E R A S.", "tokens": [286, 519, 309, 311, 264, 881, 9515, 6405, 484, 456, 13, 407, 498, 291, 434, 3102, 294, 2452, 2539, 286, 2748, 291, 722, 365, 591, 6985, 34388, 591, 462, 497, 316, 318, 13], "temperature": 0.0, "avg_logprob": -0.1961563931235784, "compression_ratio": 1.4930232558139536, "no_speech_prob": 2.2469930627266876e-05}, {"id": 945, "seek": 675920, "start": 6759.2, "end": 6770.2, "text": " He's a researcher at Google Brain and he tweeted recently that he used to think that machine learning people didn't know enough math that that was what was limiting the field. Now I think the field needs more humanities.", "tokens": [634, 311, 257, 21751, 412, 3329, 29783, 293, 415, 25646, 3938, 300, 415, 1143, 281, 519, 300, 3479, 2539, 561, 994, 380, 458, 1547, 5221, 300, 300, 390, 437, 390, 22083, 264, 2519, 13, 823, 286, 519, 264, 2519, 2203, 544, 36140, 13], "temperature": 0.0, "avg_logprob": -0.09007708922676418, "compression_ratio": 1.7549407114624507, "no_speech_prob": 3.3200114557985216e-05}, {"id": 946, "seek": 675920, "start": 6770.2, "end": 6781.2, "text": " I thought that was really encouraging. He's kind of getting at this issue of people that are thinking about these ethical questions and knowing kind of what are what are the right problems to be even solving and working on.", "tokens": [286, 1194, 300, 390, 534, 14580, 13, 634, 311, 733, 295, 1242, 412, 341, 2734, 295, 561, 300, 366, 1953, 466, 613, 18890, 1651, 293, 5276, 733, 295, 437, 366, 437, 366, 264, 558, 2740, 281, 312, 754, 12606, 293, 1364, 322, 13], "temperature": 0.0, "avg_logprob": -0.09007708922676418, "compression_ratio": 1.7549407114624507, "no_speech_prob": 3.3200114557985216e-05}, {"id": 947, "seek": 678120, "start": 6781.2, "end": 6792.2, "text": " And so I just wanted to encourage you that no matter what your background that I think you could add a lot to the field of A.I.", "tokens": [400, 370, 286, 445, 1415, 281, 5373, 291, 300, 572, 1871, 437, 428, 3678, 300, 286, 519, 291, 727, 909, 257, 688, 281, 264, 2519, 295, 316, 13, 40, 13], "temperature": 0.0, "avg_logprob": -0.15925362586975098, "compression_ratio": 1.3157894736842106, "no_speech_prob": 7.2957614065671805e-06}, {"id": 948, "seek": 678120, "start": 6792.2, "end": 6797.2, "text": " Sorry I saw someone was taking a photo.", "tokens": [4919, 286, 1866, 1580, 390, 1940, 257, 5052, 13], "temperature": 0.0, "avg_logprob": -0.15925362586975098, "compression_ratio": 1.3157894736842106, "no_speech_prob": 7.2957614065671805e-06}, {"id": 949, "seek": 678120, "start": 6797.2, "end": 6799.2, "text": " Got it.", "tokens": [5803, 309, 13], "temperature": 0.0, "avg_logprob": -0.15925362586975098, "compression_ratio": 1.3157894736842106, "no_speech_prob": 7.2957614065671805e-06}, {"id": 950, "seek": 679920, "start": 6799.2, "end": 6814.2, "text": " And then I'm over time. So let me check with Twain. I can keep going. OK. Yeah. So cut me off if I start. Yeah. I'm getting into the next speaker spot or anything.", "tokens": [400, 550, 286, 478, 670, 565, 13, 407, 718, 385, 1520, 365, 2574, 491, 13, 286, 393, 1066, 516, 13, 2264, 13, 865, 13, 407, 1723, 385, 766, 498, 286, 722, 13, 865, 13, 286, 478, 1242, 666, 264, 958, 8145, 4008, 420, 1340, 13], "temperature": 0.0, "avg_logprob": -0.11475187732327369, "compression_ratio": 1.5818181818181818, "no_speech_prob": 1.593296656210441e-05}, {"id": 951, "seek": 679920, "start": 6814.2, "end": 6822.2, "text": " I wasn't I was not sure how long this would take at all. So I guess I've prepared too much material. But I wanted I wanted to give you some tips for getting started with deep learning.", "tokens": [286, 2067, 380, 286, 390, 406, 988, 577, 938, 341, 576, 747, 412, 439, 13, 407, 286, 2041, 286, 600, 4927, 886, 709, 2527, 13, 583, 286, 1415, 286, 1415, 281, 976, 291, 512, 6082, 337, 1242, 1409, 365, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.11475187732327369, "compression_ratio": 1.5818181818181818, "no_speech_prob": 1.593296656210441e-05}, {"id": 952, "seek": 682220, "start": 6822.2, "end": 6832.2, "text": " So so far in the first demo we were just seeing the word embeddings which are a really useful tool and are used in a lot of algorithms.", "tokens": [407, 370, 1400, 294, 264, 700, 10723, 321, 645, 445, 2577, 264, 1349, 12240, 29432, 597, 366, 257, 534, 4420, 2290, 293, 366, 1143, 294, 257, 688, 295, 14642, 13], "temperature": 0.0, "avg_logprob": -0.17991622924804687, "compression_ratio": 1.558974358974359, "no_speech_prob": 1.384432653139811e-05}, {"id": 953, "seek": 682220, "start": 6832.2, "end": 6844.2, "text": " So to kind of re re state this I talked about it a little bit at the beginning. So Jeremy Howard and I are fast day I our slogan is making neural nets and uncool again.", "tokens": [407, 281, 733, 295, 319, 319, 1785, 341, 286, 2825, 466, 309, 257, 707, 857, 412, 264, 2863, 13, 407, 17809, 17626, 293, 286, 366, 2370, 786, 286, 527, 33052, 307, 1455, 18161, 36170, 293, 6219, 1092, 797, 13], "temperature": 0.0, "avg_logprob": -0.17991622924804687, "compression_ratio": 1.558974358974359, "no_speech_prob": 1.384432653139811e-05}, {"id": 954, "seek": 684420, "start": 6844.2, "end": 6853.2, "text": " And the idea is that being cool is about being exclusive and we wanted to build something that was more inclusive that you didn't have to be kind of have a Stanford Ph.D.", "tokens": [400, 264, 1558, 307, 300, 885, 1627, 307, 466, 885, 13005, 293, 321, 1415, 281, 1322, 746, 300, 390, 544, 13429, 300, 291, 994, 380, 362, 281, 312, 733, 295, 362, 257, 20374, 2623, 13, 35, 13], "temperature": 0.0, "avg_logprob": -0.0888118062700544, "compression_ratio": 1.5654008438818565, "no_speech_prob": 1.7220248992089182e-05}, {"id": 955, "seek": 684420, "start": 6853.2, "end": 6859.2, "text": " or this elite background. But the outsiders should be joining A.I.", "tokens": [420, 341, 17801, 3678, 13, 583, 264, 49825, 820, 312, 5549, 316, 13, 40, 13], "temperature": 0.0, "avg_logprob": -0.0888118062700544, "compression_ratio": 1.5654008438818565, "no_speech_prob": 1.7220248992089182e-05}, {"id": 956, "seek": 684420, "start": 6859.2, "end": 6867.2, "text": " And we partnered with the University of San Francisco's Data Institute offered this course in the evenings was open to the community.", "tokens": [400, 321, 29865, 365, 264, 3535, 295, 5271, 12279, 311, 11888, 9446, 8059, 341, 1164, 294, 264, 42835, 390, 1269, 281, 264, 1768, 13], "temperature": 0.0, "avg_logprob": -0.0888118062700544, "compression_ratio": 1.5654008438818565, "no_speech_prob": 1.7220248992089182e-05}, {"id": 957, "seek": 686720, "start": 6867.2, "end": 6876.2, "text": " Most people taking the course were software engineers and a lot of the deep learning materials out there kind of assume a graduate level math background.", "tokens": [4534, 561, 1940, 264, 1164, 645, 4722, 11955, 293, 257, 688, 295, 264, 2452, 2539, 5319, 484, 456, 733, 295, 6552, 257, 8080, 1496, 5221, 3678, 13], "temperature": 0.0, "avg_logprob": -0.12869267662366232, "compression_ratio": 1.6729323308270676, "no_speech_prob": 1.862809767771978e-05}, {"id": 958, "seek": 686720, "start": 6876.2, "end": 6886.2, "text": " We wanted something that had no math background and just we required one or two years of coding experience and it's available for free at course stop fast.", "tokens": [492, 1415, 746, 300, 632, 572, 5221, 3678, 293, 445, 321, 4739, 472, 420, 732, 924, 295, 17720, 1752, 293, 309, 311, 2435, 337, 1737, 412, 1164, 1590, 2370, 13], "temperature": 0.0, "avg_logprob": -0.12869267662366232, "compression_ratio": 1.6729323308270676, "no_speech_prob": 1.862809767771978e-05}, {"id": 959, "seek": 686720, "start": 6886.2, "end": 6896.2, "text": " I. So these are just kind of a few I want to share a little bit about our approach and lessons we learned that I think will be helpful.", "tokens": [286, 13, 407, 613, 366, 445, 733, 295, 257, 1326, 286, 528, 281, 2073, 257, 707, 857, 466, 527, 3109, 293, 8820, 321, 3264, 300, 286, 519, 486, 312, 4961, 13], "temperature": 0.0, "avg_logprob": -0.12869267662366232, "compression_ratio": 1.6729323308270676, "no_speech_prob": 1.862809767771978e-05}, {"id": 960, "seek": 689620, "start": 6896.2, "end": 6906.2, "text": " So some problems we've seen with other technical teaching one is that it's often math centric versus code centric and I'm someone that loves math.", "tokens": [407, 512, 2740, 321, 600, 1612, 365, 661, 6191, 4571, 472, 307, 300, 309, 311, 2049, 5221, 1489, 1341, 5717, 3089, 1489, 1341, 293, 286, 478, 1580, 300, 6752, 5221, 13], "temperature": 0.0, "avg_logprob": -0.09359543744255515, "compression_ratio": 1.5956521739130434, "no_speech_prob": 5.42147972737439e-06}, {"id": 961, "seek": 689620, "start": 6906.2, "end": 6918.2, "text": " I have a math Ph.D. when I first got interested in deep learning I found it frustrating because I wanted to be able to code and even though I can understand equations I was like this isn't showing me how to implement it.", "tokens": [286, 362, 257, 5221, 2623, 13, 35, 13, 562, 286, 700, 658, 3102, 294, 2452, 2539, 286, 1352, 309, 16522, 570, 286, 1415, 281, 312, 1075, 281, 3089, 293, 754, 1673, 286, 393, 1223, 11787, 286, 390, 411, 341, 1943, 380, 4099, 385, 577, 281, 4445, 309, 13], "temperature": 0.0, "avg_logprob": -0.09359543744255515, "compression_ratio": 1.5956521739130434, "no_speech_prob": 5.42147972737439e-06}, {"id": 962, "seek": 691820, "start": 6918.2, "end": 6931.2, "text": " Or the kind of the practical information. I went to a meetup. This is in 2013 where kind of a star in the field was presenting and I asked a practical question and he was like oh that's part of a dirty bag of tricks nobody publishes.", "tokens": [1610, 264, 733, 295, 264, 8496, 1589, 13, 286, 1437, 281, 257, 1677, 1010, 13, 639, 307, 294, 9012, 689, 733, 295, 257, 3543, 294, 264, 2519, 390, 15578, 293, 286, 2351, 257, 8496, 1168, 293, 415, 390, 411, 1954, 300, 311, 644, 295, 257, 9360, 3411, 295, 11733, 5079, 11374, 279, 13], "temperature": 0.0, "avg_logprob": -0.12520740470107722, "compression_ratio": 1.62, "no_speech_prob": 4.004799120593816e-05}, {"id": 963, "seek": 691820, "start": 6931.2, "end": 6940.2, "text": " You know it's kind of like we're only sharing the theory but not what you need to do this. And so we kind of we wanted something that was very code first and code focused.", "tokens": [509, 458, 309, 311, 733, 295, 411, 321, 434, 787, 5414, 264, 5261, 457, 406, 437, 291, 643, 281, 360, 341, 13, 400, 370, 321, 733, 295, 321, 1415, 746, 300, 390, 588, 3089, 700, 293, 3089, 5178, 13], "temperature": 0.0, "avg_logprob": -0.12520740470107722, "compression_ratio": 1.62, "no_speech_prob": 4.004799120593816e-05}, {"id": 964, "seek": 694020, "start": 6940.2, "end": 6953.2, "text": " Then there's this term element Titus and David Perkins came up with this Perkins is a professor at Harvard. He actually has his Ph.D. in A.I. and became an education professor.", "tokens": [1396, 456, 311, 341, 1433, 4478, 14489, 301, 293, 4389, 3026, 10277, 1361, 493, 365, 341, 3026, 10277, 307, 257, 8304, 412, 13378, 13, 634, 767, 575, 702, 2623, 13, 35, 13, 294, 316, 13, 40, 13, 293, 3062, 364, 3309, 8304, 13], "temperature": 0.0, "avg_logprob": -0.11524485378730588, "compression_ratio": 1.540909090909091, "no_speech_prob": 2.2819845980848186e-05}, {"id": 965, "seek": 694020, "start": 6953.2, "end": 6963.2, "text": " And he uses an analogy with baseball and says you know we don't require children to learn kind of all the formal rules of baseball before they're allowed to play.", "tokens": [400, 415, 4960, 364, 21663, 365, 14323, 293, 1619, 291, 458, 321, 500, 380, 3651, 2227, 281, 1466, 733, 295, 439, 264, 9860, 4474, 295, 14323, 949, 436, 434, 4350, 281, 862, 13], "temperature": 0.0, "avg_logprob": -0.11524485378730588, "compression_ratio": 1.540909090909091, "no_speech_prob": 2.2819845980848186e-05}, {"id": 966, "seek": 696320, "start": 6963.2, "end": 6974.2, "text": " You know they're able to play a simplified version of the game and get a sense of it and have fun. And then as they get older they may learn more and more rules and play you know more innings and get closer to kind of the real game.", "tokens": [509, 458, 436, 434, 1075, 281, 862, 257, 26335, 3037, 295, 264, 1216, 293, 483, 257, 2020, 295, 309, 293, 362, 1019, 13, 400, 550, 382, 436, 483, 4906, 436, 815, 1466, 544, 293, 544, 4474, 293, 862, 291, 458, 544, 294, 24451, 293, 483, 4966, 281, 733, 295, 264, 957, 1216, 13], "temperature": 0.0, "avg_logprob": -0.0614906771708343, "compression_ratio": 1.8759398496240602, "no_speech_prob": 5.647712168865837e-05}, {"id": 967, "seek": 696320, "start": 6974.2, "end": 6981.2, "text": " But then you should be able to get into a field and kind of have fun and get a sense for the big picture even without knowing all the details.", "tokens": [583, 550, 291, 820, 312, 1075, 281, 483, 666, 257, 2519, 293, 733, 295, 362, 1019, 293, 483, 257, 2020, 337, 264, 955, 3036, 754, 1553, 5276, 439, 264, 4365, 13], "temperature": 0.0, "avg_logprob": -0.0614906771708343, "compression_ratio": 1.8759398496240602, "no_speech_prob": 5.647712168865837e-05}, {"id": 968, "seek": 696320, "start": 6981.2, "end": 6987.2, "text": " And element Titus is the opposite of that and that's kind of this idea of like you have to learn each individual component.", "tokens": [400, 4478, 14489, 301, 307, 264, 6182, 295, 300, 293, 300, 311, 733, 295, 341, 1558, 295, 411, 291, 362, 281, 1466, 1184, 2609, 6542, 13], "temperature": 0.0, "avg_logprob": -0.0614906771708343, "compression_ratio": 1.8759398496240602, "no_speech_prob": 5.647712168865837e-05}, {"id": 969, "seek": 698720, "start": 6987.2, "end": 6998.2, "text": " You know and then eventually years from now you can like build those components together to do something interesting. And so with that with this course we kind of start with things that are black boxes just to kind of get people going.", "tokens": [509, 458, 293, 550, 4728, 924, 490, 586, 291, 393, 411, 1322, 729, 6677, 1214, 281, 360, 746, 1880, 13, 400, 370, 365, 300, 365, 341, 1164, 321, 733, 295, 722, 365, 721, 300, 366, 2211, 9002, 445, 281, 733, 295, 483, 561, 516, 13], "temperature": 0.0, "avg_logprob": -0.05751880009969076, "compression_ratio": 1.6906779661016949, "no_speech_prob": 3.5908797144657e-05}, {"id": 970, "seek": 698720, "start": 6998.2, "end": 7008.2, "text": " And then over time we dig in and kind of peel back the layers and it's always motivated by wanting to improve our performance or needing to solve a particular bug.", "tokens": [400, 550, 670, 565, 321, 2528, 294, 293, 733, 295, 13889, 646, 264, 7914, 293, 309, 311, 1009, 14515, 538, 7935, 281, 3470, 527, 3389, 420, 18006, 281, 5039, 257, 1729, 7426, 13], "temperature": 0.0, "avg_logprob": -0.05751880009969076, "compression_ratio": 1.6906779661016949, "no_speech_prob": 3.5908797144657e-05}, {"id": 971, "seek": 700820, "start": 7008.2, "end": 7021.2, "text": " So in the end you are learning about the low level details. It's just in the opposite order because we want you kind of able to solve problems right away and then we'll get into the details as time goes on.", "tokens": [407, 294, 264, 917, 291, 366, 2539, 466, 264, 2295, 1496, 4365, 13, 467, 311, 445, 294, 264, 6182, 1668, 570, 321, 528, 291, 733, 295, 1075, 281, 5039, 2740, 558, 1314, 293, 550, 321, 603, 483, 666, 264, 4365, 382, 565, 1709, 322, 13], "temperature": 0.0, "avg_logprob": -0.07156785329182942, "compression_ratio": 1.663677130044843, "no_speech_prob": 7.182660283433506e-06}, {"id": 972, "seek": 700820, "start": 7021.2, "end": 7032.2, "text": " And then of the more practical or code based resources out there many of them kind of just settle with for good enough results or kind of are on these toy problems.", "tokens": [400, 550, 295, 264, 544, 8496, 420, 3089, 2361, 3593, 484, 456, 867, 295, 552, 733, 295, 445, 11852, 365, 337, 665, 1547, 3542, 420, 733, 295, 366, 322, 613, 12058, 2740, 13], "temperature": 0.0, "avg_logprob": -0.07156785329182942, "compression_ratio": 1.663677130044843, "no_speech_prob": 7.182660283433506e-06}, {"id": 973, "seek": 703220, "start": 7032.2, "end": 7046.2, "text": " And we still want it to be getting to state of the art results and getting stuff that you can actually use at your company or in production as opposed to just kind of these simplified toy examples which can get frustrating.", "tokens": [400, 321, 920, 528, 309, 281, 312, 1242, 281, 1785, 295, 264, 1523, 3542, 293, 1242, 1507, 300, 291, 393, 767, 764, 412, 428, 2237, 420, 294, 4265, 382, 8851, 281, 445, 733, 295, 613, 26335, 12058, 5110, 597, 393, 483, 16522, 13], "temperature": 0.0, "avg_logprob": -0.13919488044634257, "compression_ratio": 1.6255924170616114, "no_speech_prob": 1.8923563402495347e-05}, {"id": 974, "seek": 703220, "start": 7046.2, "end": 7055.2, "text": " So this was this was our approach to fast AI. Well the ones on the bottom kind of addressing addressing these problems.", "tokens": [407, 341, 390, 341, 390, 527, 3109, 281, 2370, 7318, 13, 1042, 264, 2306, 322, 264, 2767, 733, 295, 14329, 14329, 613, 2740, 13], "temperature": 0.0, "avg_logprob": -0.13919488044634257, "compression_ratio": 1.6255924170616114, "no_speech_prob": 1.8923563402495347e-05}, {"id": 975, "seek": 705520, "start": 7055.2, "end": 7064.2, "text": " My recommendation is to start with Kairos which is a high level neural network API in Python. I read a blog post in January.", "tokens": [1222, 11879, 307, 281, 722, 365, 591, 1246, 329, 597, 307, 257, 1090, 1496, 18161, 3209, 9362, 294, 15329, 13, 286, 1401, 257, 6968, 2183, 294, 7061, 13], "temperature": 0.0, "avg_logprob": -0.13886055113777282, "compression_ratio": 1.422680412371134, "no_speech_prob": 4.4681633880827576e-05}, {"id": 976, "seek": 705520, "start": 7064.2, "end": 7074.2, "text": " So Google has spent a lot of money advertising and promoting TensorFlow which is somewhat unfortunate because I don't think it's a good place to start.", "tokens": [407, 3329, 575, 4418, 257, 688, 295, 1460, 13097, 293, 16383, 37624, 597, 307, 8344, 17843, 570, 286, 500, 380, 519, 309, 311, 257, 665, 1081, 281, 722, 13], "temperature": 0.0, "avg_logprob": -0.13886055113777282, "compression_ratio": 1.422680412371134, "no_speech_prob": 4.4681633880827576e-05}, {"id": 977, "seek": 707420, "start": 7074.2, "end": 7085.2, "text": " And so I wrote TensorFlow makes me feel like I'm not smart enough to use TensorFlow whereas Kairos makes me feel like neural networks are easier than expected. This actually got a lot of attention.", "tokens": [400, 370, 286, 4114, 37624, 1669, 385, 841, 411, 286, 478, 406, 4069, 1547, 281, 764, 37624, 9735, 591, 1246, 329, 1669, 385, 841, 411, 18161, 9590, 366, 3571, 813, 5176, 13, 639, 767, 658, 257, 688, 295, 3202, 13], "temperature": 0.0, "avg_logprob": -0.05948494374752045, "compression_ratio": 1.6984732824427482, "no_speech_prob": 1.2604413313965779e-05}, {"id": 978, "seek": 707420, "start": 7085.2, "end": 7090.2, "text": " People on the TensorFlow team read it and they invited me to the TensorFlow Dev Summit which was very nice.", "tokens": [3432, 322, 264, 37624, 1469, 1401, 309, 293, 436, 9185, 385, 281, 264, 37624, 9096, 28726, 597, 390, 588, 1481, 13], "temperature": 0.0, "avg_logprob": -0.05948494374752045, "compression_ratio": 1.6984732824427482, "no_speech_prob": 1.2604413313965779e-05}, {"id": 979, "seek": 707420, "start": 7090.2, "end": 7099.2, "text": " And here someone had responded to my post saying TensorFlow is like being handed an injection molding machine and being told to make a toy.", "tokens": [400, 510, 1580, 632, 15806, 281, 452, 2183, 1566, 37624, 307, 411, 885, 16013, 364, 22873, 11102, 278, 3479, 293, 885, 1907, 281, 652, 257, 12058, 13], "temperature": 0.0, "avg_logprob": -0.05948494374752045, "compression_ratio": 1.6984732824427482, "no_speech_prob": 1.2604413313965779e-05}, {"id": 980, "seek": 709920, "start": 7099.2, "end": 7108.2, "text": " Kairos is like being handed Legos and I think that captures it really well. Kairos is very kind of very modular and gives you these pieces you can use quickly.", "tokens": [591, 1246, 329, 307, 411, 885, 16013, 7470, 329, 293, 286, 519, 300, 27986, 309, 534, 731, 13, 591, 1246, 329, 307, 588, 733, 295, 588, 31111, 293, 2709, 291, 613, 3755, 291, 393, 764, 2661, 13], "temperature": 0.0, "avg_logprob": -0.09129352104373095, "compression_ratio": 1.5675675675675675, "no_speech_prob": 3.071532046305947e-05}, {"id": 981, "seek": 709920, "start": 7108.2, "end": 7122.2, "text": " And I should say Kairos is being incorporated into TensorFlow. So there may be less of a division but definitely if you're looking for tutorials I would recommend looking for Kairos stuff.", "tokens": [400, 286, 820, 584, 591, 1246, 329, 307, 885, 21654, 666, 37624, 13, 407, 456, 815, 312, 1570, 295, 257, 10044, 457, 2138, 498, 291, 434, 1237, 337, 17616, 286, 576, 2748, 1237, 337, 591, 1246, 329, 1507, 13], "temperature": 0.0, "avg_logprob": -0.09129352104373095, "compression_ratio": 1.5675675675675675, "no_speech_prob": 3.071532046305947e-05}, {"id": 982, "seek": 712220, "start": 7122.2, "end": 7132.2, "text": " So kind of some tips. I think it's really good to have a practical coding project that leads the way if you have a problem that you're interested in solving.", "tokens": [407, 733, 295, 512, 6082, 13, 286, 519, 309, 311, 534, 665, 281, 362, 257, 8496, 17720, 1716, 300, 6689, 264, 636, 498, 291, 362, 257, 1154, 300, 291, 434, 3102, 294, 12606, 13], "temperature": 0.0, "avg_logprob": -0.07366463008679841, "compression_ratio": 1.780701754385965, "no_speech_prob": 2.7528791179065593e-05}, {"id": 983, "seek": 712220, "start": 7132.2, "end": 7140.2, "text": " And also to start on that right away and not feel like you have to spend a year studying theory and then you can do the thing you're interested in.", "tokens": [400, 611, 281, 722, 322, 300, 558, 1314, 293, 406, 841, 411, 291, 362, 281, 3496, 257, 1064, 7601, 5261, 293, 550, 291, 393, 360, 264, 551, 291, 434, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.07366463008679841, "compression_ratio": 1.780701754385965, "no_speech_prob": 2.7528791179065593e-05}, {"id": 984, "seek": 712220, "start": 7140.2, "end": 7146.2, "text": " Having a project that you care about is going to motivate you and it's going to drive your learning.", "tokens": [10222, 257, 1716, 300, 291, 1127, 466, 307, 516, 281, 28497, 291, 293, 309, 311, 516, 281, 3332, 428, 2539, 13], "temperature": 0.0, "avg_logprob": -0.07366463008679841, "compression_ratio": 1.780701754385965, "no_speech_prob": 2.7528791179065593e-05}, {"id": 985, "seek": 714620, "start": 7146.2, "end": 7152.2, "text": " And so it's a kind of let that drive like learn what you need to do the project you care about.", "tokens": [400, 370, 309, 311, 257, 733, 295, 718, 300, 3332, 411, 1466, 437, 291, 643, 281, 360, 264, 1716, 291, 1127, 466, 13], "temperature": 0.0, "avg_logprob": -0.0849157127679563, "compression_ratio": 1.7530364372469636, "no_speech_prob": 1.805517968023196e-05}, {"id": 986, "seek": 714620, "start": 7152.2, "end": 7167.2, "text": " Get something working as fast as possible. It's okay if you don't understand all the details because I think you can get really bogged down in trying to learn all these low level details and then you're not actually getting this working.", "tokens": [3240, 746, 1364, 382, 2370, 382, 1944, 13, 467, 311, 1392, 498, 291, 500, 380, 1223, 439, 264, 4365, 570, 286, 519, 291, 393, 483, 534, 26132, 3004, 760, 294, 1382, 281, 1466, 439, 613, 2295, 1496, 4365, 293, 550, 291, 434, 406, 767, 1242, 341, 1364, 13], "temperature": 0.0, "avg_logprob": -0.0849157127679563, "compression_ratio": 1.7530364372469636, "no_speech_prob": 1.805517968023196e-05}, {"id": 987, "seek": 714620, "start": 7167.2, "end": 7172.2, "text": " And there's time to learn the details later. That doesn't mean that you're not going to learn them.", "tokens": [400, 456, 311, 565, 281, 1466, 264, 4365, 1780, 13, 663, 1177, 380, 914, 300, 291, 434, 406, 516, 281, 1466, 552, 13], "temperature": 0.0, "avg_logprob": -0.0849157127679563, "compression_ratio": 1.7530364372469636, "no_speech_prob": 1.805517968023196e-05}, {"id": 988, "seek": 717220, "start": 7172.2, "end": 7178.2, "text": " And then I really recommend helping others answering questions and writing about what you're learning.", "tokens": [400, 550, 286, 534, 2748, 4315, 2357, 13430, 1651, 293, 3579, 466, 437, 291, 434, 2539, 13], "temperature": 0.0, "avg_logprob": -0.04692134767208459, "compression_ratio": 1.766798418972332, "no_speech_prob": 1.593269917066209e-05}, {"id": 989, "seek": 717220, "start": 7178.2, "end": 7183.2, "text": " And I'm recommending that because I think it's a crucial part of your own learning.", "tokens": [400, 286, 478, 30559, 300, 570, 286, 519, 309, 311, 257, 11462, 644, 295, 428, 1065, 2539, 13], "temperature": 0.0, "avg_logprob": -0.04692134767208459, "compression_ratio": 1.766798418972332, "no_speech_prob": 1.593269917066209e-05}, {"id": 990, "seek": 717220, "start": 7183.2, "end": 7187.2, "text": " So this is even though I think altruism is important. That's not why I'm saying this.", "tokens": [407, 341, 307, 754, 1673, 286, 519, 4955, 894, 1434, 307, 1021, 13, 663, 311, 406, 983, 286, 478, 1566, 341, 13], "temperature": 0.0, "avg_logprob": -0.04692134767208459, "compression_ratio": 1.766798418972332, "no_speech_prob": 1.593269917066209e-05}, {"id": 991, "seek": 717220, "start": 7187.2, "end": 7195.2, "text": " It's because the test of whether you know something is whether you can build with it and whether you can explain it to somebody else.", "tokens": [467, 311, 570, 264, 1500, 295, 1968, 291, 458, 746, 307, 1968, 291, 393, 1322, 365, 309, 293, 1968, 291, 393, 2903, 309, 281, 2618, 1646, 13], "temperature": 0.0, "avg_logprob": -0.04692134767208459, "compression_ratio": 1.766798418972332, "no_speech_prob": 1.593269917066209e-05}, {"id": 992, "seek": 717220, "start": 7195.2, "end": 7197.2, "text": " And so it's really important to do that.", "tokens": [400, 370, 309, 311, 534, 1021, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.04692134767208459, "compression_ratio": 1.766798418972332, "no_speech_prob": 1.593269917066209e-05}, {"id": 993, "seek": 719720, "start": 7197.2, "end": 7206.2, "text": " Also with the writing about what you're learning. I really encourage people to blog and it's something where you don't you don't have to be an expert to blog.", "tokens": [2743, 365, 264, 3579, 466, 437, 291, 434, 2539, 13, 286, 534, 5373, 561, 281, 6968, 293, 309, 311, 746, 689, 291, 500, 380, 291, 500, 380, 362, 281, 312, 364, 5844, 281, 6968, 13], "temperature": 0.0, "avg_logprob": -0.06439332365989685, "compression_ratio": 1.6837209302325582, "no_speech_prob": 1.4061402907827869e-05}, {"id": 994, "seek": 719720, "start": 7206.2, "end": 7218.2, "text": " The idea is that your audience is the person that's one or two steps behind you and your best position to help that person because often experts have forgotten what was hard about getting into the field.", "tokens": [440, 1558, 307, 300, 428, 4034, 307, 264, 954, 300, 311, 472, 420, 732, 4439, 2261, 291, 293, 428, 1151, 2535, 281, 854, 300, 954, 570, 2049, 8572, 362, 11832, 437, 390, 1152, 466, 1242, 666, 264, 2519, 13], "temperature": 0.0, "avg_logprob": -0.06439332365989685, "compression_ratio": 1.6837209302325582, "no_speech_prob": 1.4061402907827869e-05}, {"id": 995, "seek": 721820, "start": 7218.2, "end": 7229.2, "text": " Whereas you know it may still be fresh in your memory. You know what would have been helpful to you to read a month ago if only you had known then. So that should kind of be your target audience.", "tokens": [13813, 291, 458, 309, 815, 920, 312, 4451, 294, 428, 4675, 13, 509, 458, 437, 576, 362, 668, 4961, 281, 291, 281, 1401, 257, 1618, 2057, 498, 787, 291, 632, 2570, 550, 13, 407, 300, 820, 733, 295, 312, 428, 3779, 4034, 13], "temperature": 0.0, "avg_logprob": -0.1350567557594993, "compression_ratio": 1.542857142857143, "no_speech_prob": 8.800025170785375e-06}, {"id": 996, "seek": 721820, "start": 7229.2, "end": 7240.2, "text": " And then the things to avoid. Don't just read or watch tutorials without taking time to code. You kind of really learn by doing.", "tokens": [400, 550, 264, 721, 281, 5042, 13, 1468, 380, 445, 1401, 420, 1159, 17616, 1553, 1940, 565, 281, 3089, 13, 509, 733, 295, 534, 1466, 538, 884, 13], "temperature": 0.0, "avg_logprob": -0.1350567557594993, "compression_ratio": 1.542857142857143, "no_speech_prob": 8.800025170785375e-06}, {"id": 997, "seek": 724020, "start": 7240.2, "end": 7249.2, "text": " Similarly don't kind of just execute other people's code. Shift enter is what executes a cell in a Jupyter notebook.", "tokens": [13157, 500, 380, 733, 295, 445, 14483, 661, 561, 311, 3089, 13, 28304, 3242, 307, 437, 4454, 1819, 257, 2815, 294, 257, 22125, 88, 391, 21060, 13], "temperature": 0.0, "avg_logprob": -0.11915848220604053, "compression_ratio": 1.5873015873015872, "no_speech_prob": 2.282517743878998e-05}, {"id": 998, "seek": 724020, "start": 7249.2, "end": 7261.2, "text": " It's really valuable to be typing and to be modifying the inputs and seeing how that affects the outputs and to try little variations and see how that impacts the code that you learn.", "tokens": [467, 311, 534, 8263, 281, 312, 18444, 293, 281, 312, 42626, 264, 15743, 293, 2577, 577, 300, 11807, 264, 23930, 293, 281, 853, 707, 17840, 293, 536, 577, 300, 11606, 264, 3089, 300, 291, 1466, 13], "temperature": 0.0, "avg_logprob": -0.11915848220604053, "compression_ratio": 1.5873015873015872, "no_speech_prob": 2.282517743878998e-05}, {"id": 999, "seek": 726120, "start": 7261.2, "end": 7273.2, "text": " By doing. And then yeah don't feel like you have to understand all the components first because there's time to learn them later and I think particularly something like this where there are just so many components.", "tokens": [3146, 884, 13, 400, 550, 1338, 500, 380, 841, 411, 291, 362, 281, 1223, 439, 264, 6677, 700, 570, 456, 311, 565, 281, 1466, 552, 1780, 293, 286, 519, 4098, 746, 411, 341, 689, 456, 366, 445, 370, 867, 6677, 13], "temperature": 0.0, "avg_logprob": -0.15246631788170856, "compression_ratio": 1.5583756345177664, "no_speech_prob": 2.2123324015410617e-05}, {"id": 1000, "seek": 726120, "start": 7273.2, "end": 7285.2, "text": " It's really easy to get bogged down and not not get anywhere. Any questions about this. Yes.", "tokens": [467, 311, 534, 1858, 281, 483, 26132, 3004, 760, 293, 406, 406, 483, 4992, 13, 2639, 1651, 466, 341, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.15246631788170856, "compression_ratio": 1.5583756345177664, "no_speech_prob": 2.2123324015410617e-05}, {"id": 1001, "seek": 728520, "start": 7285.2, "end": 7300.2, "text": " So this course will be offered again starting in late October I believe. Yes. So keep an eye out for it. We also we offer diversity fellowships and I will tweet about that and we'll post it on our website fastai.", "tokens": [407, 341, 1164, 486, 312, 8059, 797, 2891, 294, 3469, 7617, 286, 1697, 13, 1079, 13, 407, 1066, 364, 3313, 484, 337, 309, 13, 492, 611, 321, 2626, 8811, 24989, 82, 293, 286, 486, 15258, 466, 300, 293, 321, 603, 2183, 309, 322, 527, 3144, 2370, 1301, 13], "temperature": 0.0, "avg_logprob": -0.21291250448960525, "compression_ratio": 1.3856209150326797, "no_speech_prob": 1.921662624226883e-05}, {"id": 1002, "seek": 730020, "start": 7300.2, "end": 7325.2, "text": " And we will be offered and it's one evening a week. I see a hand over here.", "tokens": [400, 321, 486, 312, 8059, 293, 309, 311, 472, 5634, 257, 1243, 13, 286, 536, 257, 1011, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.19724905490875244, "compression_ratio": 1.0273972602739727, "no_speech_prob": 7.409623322018888e-06}, {"id": 1003, "seek": 732520, "start": 7325.2, "end": 7335.2, "text": " So the question was about productionizing these models and having concerns that your company that this is slow to train or hard to production eyes.", "tokens": [407, 264, 1168, 390, 466, 4265, 3319, 613, 5245, 293, 1419, 7389, 300, 428, 2237, 300, 341, 307, 2964, 281, 3847, 420, 1152, 281, 4265, 2575, 13], "temperature": 0.0, "avg_logprob": -0.1014479677727882, "compression_ratio": 1.8076923076923077, "no_speech_prob": 1.5441260984516703e-05}, {"id": 1004, "seek": 732520, "start": 7335.2, "end": 7341.2, "text": " And this is something I haven't developed and that Jeremy and I want to develop a kind of tips on productionizing.", "tokens": [400, 341, 307, 746, 286, 2378, 380, 4743, 293, 300, 17809, 293, 286, 528, 281, 1499, 257, 733, 295, 6082, 322, 4265, 3319, 13], "temperature": 0.0, "avg_logprob": -0.1014479677727882, "compression_ratio": 1.8076923076923077, "no_speech_prob": 1.5441260984516703e-05}, {"id": 1005, "seek": 732520, "start": 7341.2, "end": 7352.2, "text": " I want to say about the point about requiring a lot of data to train or being slow to train that that's I think a very common misperception about deep learning.", "tokens": [286, 528, 281, 584, 466, 264, 935, 466, 24165, 257, 688, 295, 1412, 281, 3847, 420, 885, 2964, 281, 3847, 300, 300, 311, 286, 519, 257, 588, 2689, 3346, 610, 7311, 466, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.1014479677727882, "compression_ratio": 1.8076923076923077, "no_speech_prob": 1.5441260984516703e-05}, {"id": 1006, "seek": 735220, "start": 7352.2, "end": 7358.2, "text": " And there's something called transfer learning which is covered in the court is kind of a main feature of the course.", "tokens": [400, 456, 311, 746, 1219, 5003, 2539, 597, 307, 5343, 294, 264, 4753, 307, 733, 295, 257, 2135, 4111, 295, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.05956391878025506, "compression_ratio": 1.7448559670781894, "no_speech_prob": 2.428701554890722e-05}, {"id": 1007, "seek": 735220, "start": 7358.2, "end": 7365.2, "text": " But it's the idea that you can use models that were trained on other data sets and apply them to your data set.", "tokens": [583, 309, 311, 264, 1558, 300, 291, 393, 764, 5245, 300, 645, 8895, 322, 661, 1412, 6352, 293, 3079, 552, 281, 428, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.05956391878025506, "compression_ratio": 1.7448559670781894, "no_speech_prob": 2.428701554890722e-05}, {"id": 1008, "seek": 735220, "start": 7365.2, "end": 7377.2, "text": " And that allows people that don't have that much data and don't have the computational power to kind of benefit from what companies like Google have done when they create models that are shared.", "tokens": [400, 300, 4045, 561, 300, 500, 380, 362, 300, 709, 1412, 293, 500, 380, 362, 264, 28270, 1347, 281, 733, 295, 5121, 490, 437, 3431, 411, 3329, 362, 1096, 562, 436, 1884, 5245, 300, 366, 5507, 13], "temperature": 0.0, "avg_logprob": -0.05956391878025506, "compression_ratio": 1.7448559670781894, "no_speech_prob": 2.428701554890722e-05}, {"id": 1009, "seek": 737720, "start": 7377.2, "end": 7391.2, "text": " And they often get I think surprisingly good results. So you would have something maybe that was originally trained on image net which is this classic problem of identifying all these different categories.", "tokens": [400, 436, 2049, 483, 286, 519, 17600, 665, 3542, 13, 407, 291, 576, 362, 746, 1310, 300, 390, 7993, 8895, 322, 3256, 2533, 597, 307, 341, 7230, 1154, 295, 16696, 439, 613, 819, 10479, 13], "temperature": 0.0, "avg_logprob": -0.10651926994323731, "compression_ratio": 1.5046728971962617, "no_speech_prob": 4.9354512157151476e-06}, {"id": 1010, "seek": 737720, "start": 7391.2, "end": 7397.2, "text": " I don't know. That's a coffee table. That's a snowmobile. And you can apply that to another computer vision problem.", "tokens": [286, 500, 380, 458, 13, 663, 311, 257, 4982, 3199, 13, 663, 311, 257, 5756, 76, 13632, 13, 400, 291, 393, 3079, 300, 281, 1071, 3820, 5201, 1154, 13], "temperature": 0.0, "avg_logprob": -0.10651926994323731, "compression_ratio": 1.5046728971962617, "no_speech_prob": 4.9354512157151476e-06}, {"id": 1011, "seek": 739720, "start": 7397.2, "end": 7410.2, "text": " And even if it's something fairly different than you just like retrain like the last layer the last few layers which is quicker and you don't have to have as much data and you can get good performance.", "tokens": [400, 754, 498, 309, 311, 746, 6457, 819, 813, 291, 445, 411, 1533, 7146, 411, 264, 1036, 4583, 264, 1036, 1326, 7914, 597, 307, 16255, 293, 291, 500, 380, 362, 281, 362, 382, 709, 1412, 293, 291, 393, 483, 665, 3389, 13], "temperature": 0.0, "avg_logprob": -0.07502178351084392, "compression_ratio": 1.55, "no_speech_prob": 4.2224987737427e-06}, {"id": 1012, "seek": 739720, "start": 7410.2, "end": 7418.2, "text": " So that's a possibility. But yeah productionizing is something that I expect us to do more on in the future.", "tokens": [407, 300, 311, 257, 7959, 13, 583, 1338, 4265, 3319, 307, 746, 300, 286, 2066, 505, 281, 360, 544, 322, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.07502178351084392, "compression_ratio": 1.55, "no_speech_prob": 4.2224987737427e-06}, {"id": 1013, "seek": 741820, "start": 7418.2, "end": 7429.2, "text": " Other questions. Yes.", "tokens": [5358, 1651, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.1972559860774449, "compression_ratio": 0.8421052631578947, "no_speech_prob": 1.2605058145709336e-05}, {"id": 1014, "seek": 741820, "start": 7429.2, "end": 7441.2, "text": " Oh really.", "tokens": [876, 534, 13], "temperature": 0.0, "avg_logprob": -0.1972559860774449, "compression_ratio": 0.8421052631578947, "no_speech_prob": 1.2605058145709336e-05}, {"id": 1015, "seek": 744120, "start": 7441.2, "end": 7449.2, "text": " Oh it's not really.", "tokens": [876, 309, 311, 406, 534, 13], "temperature": 0.0, "avg_logprob": -0.19137123051811666, "compression_ratio": 0.868421052631579, "no_speech_prob": 3.2885336622712202e-06}, {"id": 1016, "seek": 744120, "start": 7449.2, "end": 7455.2, "text": " Oh crazy. OK.", "tokens": [876, 3219, 13, 2264, 13], "temperature": 0.0, "avg_logprob": -0.19137123051811666, "compression_ratio": 0.868421052631579, "no_speech_prob": 3.2885336622712202e-06}, {"id": 1017, "seek": 745520, "start": 7455.2, "end": 7471.2, "text": " But so has she publicly revealed that it was a joke because this gets like cited all over the place.", "tokens": [583, 370, 575, 750, 14843, 9599, 300, 309, 390, 257, 7647, 570, 341, 2170, 411, 30134, 439, 670, 264, 1081, 13], "temperature": 0.0, "avg_logprob": -0.17254838943481446, "compression_ratio": 1.1764705882352942, "no_speech_prob": 2.331759333173977e-06}, {"id": 1018, "seek": 747120, "start": 7471.2, "end": 7489.2, "text": " OK. Interesting. I would just for me to repeat for the recording someone is saying that her friend was the woman in the Nikon camera picture and that photo was faked. I do want to point out that there was another photo of an Asian man with a different software being told that his eyes were closed.", "tokens": [2264, 13, 14711, 13, 286, 576, 445, 337, 385, 281, 7149, 337, 264, 6613, 1580, 307, 1566, 300, 720, 1277, 390, 264, 3059, 294, 264, 13969, 266, 2799, 3036, 293, 300, 5052, 390, 283, 7301, 13, 286, 360, 528, 281, 935, 484, 300, 456, 390, 1071, 5052, 295, 364, 10645, 587, 365, 257, 819, 4722, 885, 1907, 300, 702, 2575, 645, 5395, 13], "temperature": 0.0, "avg_logprob": -0.12240475548638238, "compression_ratio": 1.6483050847457628, "no_speech_prob": 5.013179816160118e-06}, {"id": 1019, "seek": 747120, "start": 7489.2, "end": 7498.2, "text": " So I think this issue has arisen elsewhere. Yeah that's fascinating. I'll search for that.", "tokens": [407, 286, 519, 341, 2734, 575, 594, 11106, 14517, 13, 865, 300, 311, 10343, 13, 286, 603, 3164, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.12240475548638238, "compression_ratio": 1.6483050847457628, "no_speech_prob": 5.013179816160118e-06}, {"id": 1020, "seek": 749820, "start": 7498.2, "end": 7511.2, "text": " Well yeah and then I've kind of already covered this about yeah the test of whether you understand something is if you can code and build it if you can teach it to somebody else. These were blog posts written by two students in our course.", "tokens": [1042, 1338, 293, 550, 286, 600, 733, 295, 1217, 5343, 341, 466, 1338, 264, 1500, 295, 1968, 291, 1223, 746, 307, 498, 291, 393, 3089, 293, 1322, 309, 498, 291, 393, 2924, 309, 281, 2618, 1646, 13, 1981, 645, 6968, 12300, 3720, 538, 732, 1731, 294, 527, 1164, 13], "temperature": 0.0, "avg_logprob": -0.11873679007253339, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.269115768489428e-05}, {"id": 1021, "seek": 749820, "start": 7511.2, "end": 7525.2, "text": " You know what is a beginner's guide to commonly use linear algebra operations and so again remembering you can you can write blog post with people a step or two behind you as the target audience.", "tokens": [509, 458, 437, 307, 257, 22080, 311, 5934, 281, 12719, 764, 8213, 21989, 7705, 293, 370, 797, 20719, 291, 393, 291, 393, 2464, 6968, 2183, 365, 561, 257, 1823, 420, 732, 2261, 291, 382, 264, 3779, 4034, 13], "temperature": 0.0, "avg_logprob": -0.11873679007253339, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.269115768489428e-05}, {"id": 1022, "seek": 752520, "start": 7525.2, "end": 7542.2, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.16356819868087769, "compression_ratio": 0.3333333333333333, "no_speech_prob": 3.373021900188178e-05}, {"id": 1023, "seek": 754220, "start": 7542.2, "end": 7556.2, "text": " Yeah. So the question was what sort of roles can be played by people that are not not technical in a particularly someone that has a background as a lawyer and working in policy or that you said low income.", "tokens": [865, 13, 407, 264, 1168, 390, 437, 1333, 295, 9604, 393, 312, 3737, 538, 561, 300, 366, 406, 406, 6191, 294, 257, 4098, 1580, 300, 575, 257, 3678, 382, 257, 11613, 293, 1364, 294, 3897, 420, 300, 291, 848, 2295, 5742, 13], "temperature": 0.0, "avg_logprob": -0.186453280729406, "compression_ratio": 1.6339285714285714, "no_speech_prob": 1.4965931768529117e-05}, {"id": 1024, "seek": 754220, "start": 7556.2, "end": 7563.2, "text": " Yeah I think. Yeah I mean actually when I was preparing this talk I was like I hope their lawyers at this talk. And then I was like oh there probably won't be.", "tokens": [865, 286, 519, 13, 865, 286, 914, 767, 562, 286, 390, 10075, 341, 751, 286, 390, 411, 286, 1454, 641, 16219, 412, 341, 751, 13, 400, 550, 286, 390, 411, 1954, 456, 1391, 1582, 380, 312, 13], "temperature": 0.0, "avg_logprob": -0.186453280729406, "compression_ratio": 1.6339285714285714, "no_speech_prob": 1.4965931768529117e-05}, {"id": 1025, "seek": 756320, "start": 7563.2, "end": 7588.2, "text": " But just like some of the legal issues that were raised I feel like are really really important. I would I mean I would encourage you to to learn some coding even if I think even if you're still kind of working in the capacity as a lawyer like a lawyer who codes and know some about a I just feel like it's going to have a lot of potential to do things.", "tokens": [583, 445, 411, 512, 295, 264, 5089, 2663, 300, 645, 6005, 286, 841, 411, 366, 534, 534, 1021, 13, 286, 576, 286, 914, 286, 576, 5373, 291, 281, 281, 1466, 512, 17720, 754, 498, 286, 519, 754, 498, 291, 434, 920, 733, 295, 1364, 294, 264, 6042, 382, 257, 11613, 411, 257, 11613, 567, 14211, 293, 458, 512, 466, 257, 286, 445, 841, 411, 309, 311, 516, 281, 362, 257, 688, 295, 3995, 281, 360, 721, 13], "temperature": 0.0, "avg_logprob": -0.13367862465940875, "compression_ratio": 1.6650943396226414, "no_speech_prob": 1.3004791071580257e-05}, {"id": 1026, "seek": 758820, "start": 7588.2, "end": 7594.2, "text": " But yeah I am excited to have a lawyer here because I think that this does raise a lot of legal issues.", "tokens": [583, 1338, 286, 669, 2919, 281, 362, 257, 11613, 510, 570, 286, 519, 300, 341, 775, 5300, 257, 688, 295, 5089, 2663, 13], "temperature": 0.0, "avg_logprob": -0.2473826841874556, "compression_ratio": 1.5448275862068965, "no_speech_prob": 9.079526535060722e-06}, {"id": 1027, "seek": 758820, "start": 7594.2, "end": 7597.2, "text": " But yeah I would yeah I would encourage you to.", "tokens": [583, 1338, 286, 576, 1338, 286, 576, 5373, 291, 281, 13], "temperature": 0.0, "avg_logprob": -0.2473826841874556, "compression_ratio": 1.5448275862068965, "no_speech_prob": 9.079526535060722e-06}, {"id": 1028, "seek": 758820, "start": 7597.2, "end": 7604.2, "text": " Yeah. Yeah. I mean if you're if you're brand do you code or.", "tokens": [865, 13, 865, 13, 286, 914, 498, 291, 434, 498, 291, 434, 3360, 360, 291, 3089, 420, 13], "temperature": 0.0, "avg_logprob": -0.2473826841874556, "compression_ratio": 1.5448275862068965, "no_speech_prob": 9.079526535060722e-06}, {"id": 1029, "seek": 758820, "start": 7604.2, "end": 7607.2, "text": " Yeah. Okay.", "tokens": [865, 13, 1033, 13], "temperature": 0.0, "avg_logprob": -0.2473826841874556, "compression_ratio": 1.5448275862068965, "no_speech_prob": 9.079526535060722e-06}, {"id": 1030, "seek": 760720, "start": 7607.2, "end": 7618.2, "text": " You might want to spend some time on Python first. So if you're kind of brand new to coding I would spend some time learning Python and then move on to move on to this.", "tokens": [509, 1062, 528, 281, 3496, 512, 565, 322, 15329, 700, 13, 407, 498, 291, 434, 733, 295, 3360, 777, 281, 17720, 286, 576, 3496, 512, 565, 2539, 15329, 293, 550, 1286, 322, 281, 1286, 322, 281, 341, 13], "temperature": 0.0, "avg_logprob": -0.09667559389798146, "compression_ratio": 1.471830985915493, "no_speech_prob": 6.438383934437297e-06}, {"id": 1031, "seek": 760720, "start": 7618.2, "end": 7630.2, "text": " Yeah I'm excited that you're interested.", "tokens": [865, 286, 478, 2919, 300, 291, 434, 3102, 13], "temperature": 0.0, "avg_logprob": -0.09667559389798146, "compression_ratio": 1.471830985915493, "no_speech_prob": 6.438383934437297e-06}, {"id": 1032, "seek": 763020, "start": 7630.2, "end": 7642.2, "text": " Okay. And I was going to share a quote from one of our students. I personally fell into the habit of watching the lectures too much and Googling definitions concepts too much without running the code.", "tokens": [1033, 13, 400, 286, 390, 516, 281, 2073, 257, 6513, 490, 472, 295, 527, 1731, 13, 286, 5665, 5696, 666, 264, 7164, 295, 1976, 264, 16564, 886, 709, 293, 45005, 1688, 21988, 10392, 886, 709, 1553, 2614, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.09036698967519433, "compression_ratio": 1.763157894736842, "no_speech_prob": 8.662129403091967e-06}, {"id": 1033, "seek": 763020, "start": 7642.2, "end": 7654.2, "text": " At first I thought that I should read the code quickly and then spend time researching the theory behind it and retrospect I should have spent the majority of my time on the actual code in the notebooks running it and seeing what goes into it and what comes out of it.", "tokens": [1711, 700, 286, 1194, 300, 286, 820, 1401, 264, 3089, 2661, 293, 550, 3496, 565, 24176, 264, 5261, 2261, 309, 293, 34997, 286, 820, 362, 4418, 264, 6286, 295, 452, 565, 322, 264, 3539, 3089, 294, 264, 43782, 2614, 309, 293, 2577, 437, 1709, 666, 309, 293, 437, 1487, 484, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.09036698967519433, "compression_ratio": 1.763157894736842, "no_speech_prob": 8.662129403091967e-06}, {"id": 1034, "seek": 765420, "start": 7654.2, "end": 7666.2, "text": " And I think this is really good advice learned from this person's mistake and definitely kind of focus on the code and running it.", "tokens": [400, 286, 519, 341, 307, 534, 665, 5192, 3264, 490, 341, 954, 311, 6146, 293, 2138, 733, 295, 1879, 322, 264, 3089, 293, 2614, 309, 13], "temperature": 0.0, "avg_logprob": -0.14464633847460334, "compression_ratio": 1.5631067961165048, "no_speech_prob": 6.7474252318788785e-06}, {"id": 1035, "seek": 765420, "start": 7666.2, "end": 7675.2, "text": " And then. So I said another entire demo plan I'm going to stop. Yeah I'm getting the stop signal I see people packing up.", "tokens": [400, 550, 13, 407, 286, 848, 1071, 2302, 10723, 1393, 286, 478, 516, 281, 1590, 13, 865, 286, 478, 1242, 264, 1590, 6358, 286, 536, 561, 20815, 493, 13], "temperature": 0.0, "avg_logprob": -0.14464633847460334, "compression_ratio": 1.5631067961165048, "no_speech_prob": 6.7474252318788785e-06}, {"id": 1036, "seek": 765420, "start": 7675.2, "end": 7680.2, "text": " I'm sorry. I'm sorry that this ran long. Thank you for your patience.", "tokens": [286, 478, 2597, 13, 286, 478, 2597, 300, 341, 5872, 938, 13, 1044, 291, 337, 428, 14826, 13], "temperature": 0.0, "avg_logprob": -0.14464633847460334, "compression_ratio": 1.5631067961165048, "no_speech_prob": 6.7474252318788785e-06}, {"id": 1037, "seek": 768020, "start": 7680.2, "end": 7689.2, "text": " The demo is available in the Jupyter notebook and it has lots of text. So I encourage you to give it a try on your own even though we didn't have time for it here.", "tokens": [440, 10723, 307, 2435, 294, 264, 22125, 88, 391, 21060, 293, 309, 575, 3195, 295, 2487, 13, 407, 286, 5373, 291, 281, 976, 309, 257, 853, 322, 428, 1065, 754, 1673, 321, 994, 380, 362, 565, 337, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.1002358260907625, "compression_ratio": 1.5490196078431373, "no_speech_prob": 1.777158831828274e-05}, {"id": 1038, "seek": 768020, "start": 7689.2, "end": 7704.2, "text": " It's kind of just the second part down down here and doing some movie review analysis. But yeah thank you everyone for your patience and your attention.", "tokens": [467, 311, 733, 295, 445, 264, 1150, 644, 760, 760, 510, 293, 884, 512, 3169, 3131, 5215, 13, 583, 1338, 1309, 291, 1518, 337, 428, 14826, 293, 428, 3202, 13], "temperature": 0.0, "avg_logprob": -0.1002358260907625, "compression_ratio": 1.5490196078431373, "no_speech_prob": 1.777158831828274e-05}, {"id": 1039, "seek": 770420, "start": 7704.2, "end": 7716.2, "text": " And also feel free to email or tweet at me if you have additional questions or concerns that come up. Thanks.", "tokens": [50364, 400, 611, 841, 1737, 281, 3796, 420, 15258, 412, 385, 498, 291, 362, 4497, 1651, 420, 7389, 300, 808, 493, 13, 2561, 13, 50964], "temperature": 0.0, "avg_logprob": -0.0904986308171199, "compression_ratio": 1.1720430107526882, "no_speech_prob": 0.00019044200598727912}], "language": "en"}