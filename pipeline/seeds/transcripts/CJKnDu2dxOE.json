{"text": " Welcome everybody to lesson five. And so we have officially peaked and everything is downhill from here. As of halfway through the last lesson. We started with computer vision because it's the most mature, kind of out of the box, ready to use deep learning application. It's something which if you're not using deep learning, you won't be getting good results. So the difference, you know, hopefully between not doing lesson one versus doing lesson one, you've gained a new capability you didn't have before. And you kind of get to see a lot of the kind of tradecraft of training and effective neural net. And so then we moved into NLP because text is kind of another one, which you really kind of can't do really well without deep learning, generally speaking. And it's just got to the point where it's pretty, you know, works pretty well now. In fact, the New York Times just featured an article about the latest advances in deep learning for text yesterday. And talked quite a lot about the work that we've done in that area along with OpenAI and Google and the Allen Institute of Artificial Intelligence. And then we've kind of finished our application journey with tabular and collaborative filtering, partly because tabular and collaborative filtering are things that you can still do pretty well without deep learning. So it's not such a big step. It's not a kind of whole new thing that you could do that you couldn't used to do. And also because the, you know, we're going to try to get to a point where we understand pretty much every line of code and the implementations of these things and the implementations of those things. It's much less intricate than vision and NLP. So as we come down this other side of the journey, which is like all the stuff we've just done, how does it actually work? By starting where we just ended, which is starting with collaborative filtering and then tabular data, we're going to be able to see what all those lines of code do by the end of today's lesson. That's our goal. So particularly this lesson, you should not expect to come away knowing how to solve, you know, how to do applications you couldn't do before. But instead, you should have a better understanding of how we've actually been solving the applications we've seen so far. Particularly, we're going to understand a lot more about regularization, which is how we go about managing over versus under fitting. And so hopefully you can use some of the tools from this lesson to go back to your previous projects and get a little bit more performance or handle models where previously maybe you felt like your data was not enough or maybe you were under fitting and so forth. And it's also going to lay the groundwork for understanding convolutional neural networks and recurrent neural networks that we'll do deep dives into in the next two lessons. And as we do that, we're also going to look at some new applications, some new vision and NLP applications. Let's start where we left off last week. Do you remember this picture? So this picture, we were looking at kind of what does a deep neural net look like? And we had various layers. And the first thing we pointed out is that there are only and exactly two types of layer. There are layers that contain parameters and there are layers that contain activations. Parameters are the things that your model learns. They're the things that you use gradient descent to go parameters minus equals learning rate times parameters dot grad. That's our basic, that's what we do. And those parameters are used by multiplying them by input activations doing a matrix product. So the yellow things are our weight matrices or weight tensors more generally, but that's close enough. So we take some input activations or some layer activations and we multiply it by a weight matrix to get a bunch of activations. So activations are numbers, but these are numbers that are calculated. So I find in our study group, I keep getting questions about where does that number come from? And I always answer it in the same way. You tell me, is it a parameter or is it an activation? Because it's one of those two things. That's where numbers come from. I guess inputs are kind of a special activation. So they're not calculated. They're just there. So maybe that's a special case. So maybe it's an input or a parameter or an activation. Activations don't only come out of matrix multiplications. They also come out of activation functions. And the most important thing to remember about an activation function is that it's an element wise function. So it's a function that is applied to each element of the input activations in turn and creates one activation for each input element. So if it starts with a 20 long vector, it creates a 20 long vector by looking at each one of those in turn, doing one thing to it and spitting out the answer. So an element wise function. Relu is the main one we've looked at. And honestly, it doesn't too much matter which you pick. So we don't spend much time talking about activation functions because if you just use Relu, you'll get a pretty good answer pretty much all the time. And so then we learned that this combination of matrix multiplications followed by Relu's stacked together has this amazing mathematical property called the universal approximation theorem, which is if you have big enough weight matrices and enough of them, it can solve any arbitrarily complex mathematical function to any arbitrarily high level of accuracy. Assuming that you can train the parameters both in terms of time and data availability and so forth. So that's the bit which I find particularly more advanced computer scientists get really confused about is they're always asking like, where's the next bit? What's the trick? How does it work? But that's it. You know, you just do those things and you pass back the gradients and you update the weights with the learning rate. And that's it. So that piece where we take the loss function between the actual targets and the output of the final layer, so the final activations, we calculate the gradients with respect to all of these yellow things. And then we update those yellow things by subtracting learning rate times the gradient. That process of calculating those gradients and then subtracting like that is called back propagation. So when you hear the term back propagation, it's one of these terms that neural networking folks love to use. Sounds very impressive, OK, but you can replace it with your head with weights minus equals weights grad times learning rate or parameters, I should say, rather than weights, a bit more general. OK, so that's what we covered last week. And then I mentioned last week that we're going to cover a couple more things. I'm going to come back to these ones, Cross-Entropy and Softmax later today. Let's talk about fine tuning now. So what happens when we take a ResNet 34 and we do transfer learning? What's actually going on? So the first thing to notice is the ResNet 34 that we grab from ImageNet has a very specific weight matrix at the end. It's a weight matrix that has 1000 columns. Why is that? Because ImageNet, the problem they ask you to solve in the ImageNet competition, is please figure out which one of these 1000 image categories this picture is. So that's why they need a thousand things here, because in ImageNet, this target vector is length 1000. So you've got to pick the probability that it's which one of those thousand things. So there's a couple of reasons this weight matrix is no good to you when you're doing transfer learning. The first is that you probably don't have a thousand categories. You know, I was trying to do teddy bears, black bears or brown bears. So I don't want a thousand categories. And the second is, even if I did have exactly a thousand categories, I would still have the same thousand categories that are in ImageNet. So basically, this whole weight matrix is a waste of time for me. So what do we do? We throw it away. So when you go create CNN in Fast.AI, it deletes that. So what does it do instead? Instead, it puts in two new weight matrices in there for you with a ReLU in between. And so there are some defaults as to what size this first one is. But the second one, the size there is as big as you need it to be. So in your data bunch, which you pass to your learner, from that, we know how many activations you need. If you're doing classification, it's whether many classes you have. If you're doing regression, it's however many numbers you're trying to predict in the regression problem. And so remember that if your data bunch is called data, that'll be called data.c. So we'll add for you this weight matrix of size data.c, by however much was in the previous layer. OK, so now we need to train those, because initially these weight matrices are full of random numbers. Because new weight matrices are always full of random numbers if they're new. And these ones are new. We've just grabbed them and thrown them in there. So we need to train them. But the other layers are not new. The other layers are good at something. And what are they good at? Well, let's remember that Zeiler and Fergus paper. Here are examples of some visualization of some filters, some weight matrices in the first layer, and some examples of some things that they found. So the first layer had one part of the weight matrix was good at finding diagonal edges in this direction. And then in layer two, one of the filters was good at finding corners in the top left. And then in layer three, one of the filters was good at finding repeating patterns. Another one was good at finding round orange things. Another one was good at finding kind of like fairy or floral textures. So as we go up, they're becoming more sophisticated, but also more specific. So like layer four, I think was finding eyeballs, for instance. Now, if you're wanting to transfer and learn to something for histopathology slides, there's probably going to be no eyeballs in that. So the later layers are no good for you. But there'll certainly be some repeating patterns and there'll certainly be some diagonal edges. So the earlier you go in the model, the more likely it is that you want those weights to stay as they are. Well, to start with, we definitely need to train these new weights because they're random. So let's not bother training any of the other weights at all to start with. So what we do is we basically say, let's freeze. Let's freeze all of those other layers. So what does that mean? All that means is that we're asking Fast.ai and PyTorch that when we train, however many epochs we do, when we call fit, don't back propagate the weights, don't back propagate the gradients back into those layers. In other words, when you go parameters equals parameters minus learning rate times gradient, only do it for the new layers, don't bother doing it for the other layers. That's what freezing means. Just means don't update those parameters. So it'll be a little bit faster as well because there's a few less calculations to do. It'll take up a little bit less memory because there's a few less gradients that we have to store. But most importantly, it's not going to change weights that are already better than nothing. They're better than random at the very least. So that's what happens when you call freeze. It doesn't freeze the whole thing. It freezes everything except the randomly generated added layers that we put on for you. So then what happens next? OK, after a while, we say, OK, this is looking pretty good. We probably should train the rest of the network now. So we unfreeze. And so now we're going to train the whole thing. But we still have a pretty good sense that these new layers we added to the end probably need more training. And these ones right at the start that might just be like diagonal edges probably don't need much training at all. So we split our model into a few sections and we say, let's give different parts of the model different learning rates. So this part of the model, we might give a learning rate of one e neg five. And this part of the model, we might give a learning rate of one e neg three. And so what's going to happen now is that we can keep training the entire network. But because the learning rate for the early layers is smaller, it's going to move them around less because we think they're already pretty good. And also, like if it's already pretty good to the optimal value, if you used a higher learning rate, it could kick it out. It could actually make it worse, which we really don't want to happen. So this this process is called using discriminative learning rates. You won't find much online about it because I think we were kind of the first to use it for this purpose or at least talk about it extensively. Maybe other probably other people used it without writing it down. So most of the stuff you'll find about this will be faster students, but it's starting to get more well known slowly now. But it's a really, really important concept for transfer learning without using this. You just can't get nearly as good results. So how do we do discriminative learning rates in fast? When you when you anywhere you can put a learning rate in fast such as with the fit function. The first thing you put in is the number of epochs. And then the second thing you put in is learning rate. Same if you use fit one cycle, the learning rate, you can put a number of things that you can put a single number like one, a neg three. You can write a slice so you can write slice, for example, one, a neg three with a single number or you can write slice. With two numbers. What are each of those mean in the first case? Just using a single number means every layer gets the same learning rate. So you're not using discriminative learning rates. If you pass a single number to slice, it means the final layers get a learning rate of whatever you wrote down. One, a neg three. And then all the other layers get the same learning rate, which is that divided by three. So all of the other layers will be one, a neg three divided by three. The last layers will be one, a neg three. And the last case, the final layers, these randomly hidden added layers will still be again one, a neg three. The first layers will get one, a neg five. And the other layers will get learning rates that are equally spread between those two. So it's multiplicatively equal. So if there were three layers, there would be one, a neg five, one, a neg four, one, a neg three. So equal multiples each time. One slight tweak to make things a little bit simpler to manage. We don't actually give a different learning rate to every layer. We give a different learning rate to every layer group, which is just we decide to put the groups together for you. And so specifically what we do is the randomly added extra layers. We call those one layer group. This is by default. You can modify it. And then all the rest we split in half into two layer groups. So by default, at least with a CNN, you'll get three layer groups. And so if you say slice one, a neg five, one, a neg three, you will get one, a neg five learning rate for the first layer group, one, a neg four for the second, one, a neg three for the third. So now if you go back and look at the way that we're training, hopefully you'll see that this makes a lot of sense. This divided by three thing is a little weird. And we won't talk about why that is until part two of the course. It's a specific work around batch normalization. So we can discuss that in the advanced topic if anybody's interested. All right. So that is. That is fine tuning. So hopefully that makes that a little bit less mysterious. So we were looking at. Collaborative filtering last week and in the collaborative filtering example, we called fit one cycle and we passed in just a single number. And that makes sense because in collaborative filtering, we only have one layer. There's a few different pieces in it, but there isn't, you know, a matrix multiply followed by an activation function followed by another matrix model play. I know introduce a another piece of jargon here. They're not always exactly matrix multiplications. There's something very similar there. They're linear functions that we add together. But the more general term for these for these things that are more general than matrix multiplications is affine functions. OK. So if you hear me say the word affine function, you can replace it in your head with matrix multiplication. But as we'll see when we do convolutions, convolutions are matrix multiplications where some of the weights are tied. And so it would be slightly more accurate to call them affine functions. I'd like to introduce a little bit more jargon each lesson so that when you read books or papers or watch other courses or read documentation, there will be more of the words you'll recognize. So when you say affine function, it just means a linear function. And it means something very, very close to matrix multiplication. Matrix multiplication is the most common kind of affine function, at least in deep learning. So specifically for collaborative filtering, the model we were using was this one. It was where we had a bunch of numbers here and a bunch of numbers here. And we took the dot product of them. And given that one here is a row and one is a column, we can actually that's the same as a matrix product. So M molt in Excel multiplies matrices. So here is the matrix product of those two. And so I started this training last week by using Solver in Excel. And we never actually went back to see how it went. So let's go and have a look now. So the average sum of squared error got down to 0.39. So we're trying to predict something on a scale of 0.5 to 5. So on average, we're being wrong by about 0.4. That's pretty good. And you can kind of see it's pretty good if you look at like 3.51 is what it meant to be. 3.25, 5.1, 0.98. That's pretty close. Right. So you get the general idea. And then I started to talk about this idea of embedding matrices. And so in order to understand that, let's put this worksheet aside. I look at another worksheet. So here's another worksheet. And what I've done here is I have copied over those two weight matrices from the previous worksheet. Right. Here's the one for users and here's the one for movies. And the movies one, I've transposed it. So it's now got exactly the same dimensions as the users one. So here are two weight matrices. Initially, they were random. We can train them with gradient descent. In the original data, the user IDs and movie IDs were numbers like these. To make life more convenient, I've converted them to numbers from 1 to 15. OK. So in these columns, I've got for every rating, I've got user ID movie ID rating using these mapped numbers so that they're contiguous starting at 1. OK. Now I'm going to replace user ID number one with this vector. The vector contains a 1 followed by 14 zeros. And then user number two, I'm going to replace with a vector of 0 and then 1 and then 13 zeros and so forth. So movie ID 14, all these are movie ID 14, I've also replaced with another vector, which is 13 zeros and then a 1 and then a 0. OK. So these are called one-hot encodings, by the way. So this is not part of a neural net. This is just like some input pre-processing where I'm literally making this my new inputs. This is my new inputs for my movies. This is my new inputs for my users. So these are my inputs to a neural net. So what I'm going to do now is I'm going to take this input matrix and I'm going to do a matrix multiply by this weight matrix. And that will work because this has 15 rows and this has 15 columns. So I can multiply those two matrices together because they match. And you can do matrix multiplication in Excel using the ammult function. Just be careful if you're using Excel because this is a function that returns multiple numbers. You can't just hit enter when you finish with it. You have to hit control shift enter. Control shift enter means this is a array function, something that returns multiple values. So here is the matrix product of this input matrix of inputs and this parameter matrix or weight matrix. So that's just a normal neural network layer. It's just a regular matrix multiply. And so we can do the same thing for movies. And so here's the matrix multiply for movies. Well, here's the thing. This input is we claim is this kind of one hot encoded version of user ID number one. And these activations are the activations for user ID number one. Why is that? Because if you think about it, a matrix multiplication between a one hot encoded vector and some matrix is actually going to find the nth row of that matrix when the one is in position n. Does that make sense? So what we've done here is we've actually got a matrix multiply that is creating these output activations, right? But it's doing it in a very interesting way, which is it's effectively finding a particular row in the input matrix. So having done that, we can then multiply those two sets together, just a dot product. And we can then find the loss squared. And then we can find the average loss. And lo and behold, that number, point three nine, is the same as this number, because they're doing the same thing. So this one was kind of finding this particular user's embedding vector. This one is just doing a matrix multiply. And therefore, we know they are mathematically identical. So let's lay that out again. So here's our final version. This is the same weight matrices again, exactly the same. I've copied them over. And here's those user IDs and movie IDs again. But this time I've laid them out just in a normal kind of tabular form, just like you would expect to see in the input to your model. And this time I've got exactly the same set of activations here that I had here. But in this case, I've calculated these activations using Excel's offset function, which is an array lookup, right? It says, find the first row of this. So this is doing it as an array lookup. So this version is identical to this version. But obviously, it's much less memory intensive and much faster, because I don't actually create the one hot encoded matrix and I don't actually do a matrix multiply, because that matrix multiply is nearly all multiplying by zero, which is a total waste of time. So in other words, multiplying by a one hot encoded matrix is identical to doing an array lookup. Therefore, we should always do the array lookup version. And therefore, we have a specific way of doing we have a specific way of saying, I want to do a matrix multiplication by a one hot encoded matrix without ever actually creating it. I'm just instead going to pass in a bunch of ints and pretend they're one hot encoded. And that is called an embedding. So you might have heard this word embedding all over the place, as if it's some magic advanced mathy thing. But embedding means look something up in an array. But it's interesting to know that looking something up in an array is mathematically identical to doing a matrix product by a one hot encoded matrix. And therefore, an embedding fits very nicely in our standard model of how neural networks work. So now suddenly, it's as if we have another whole kind of layer. It's a kind of layer where we get to look things up in an array. But we actually didn't do anything special. Right. We just added this computational shortcut, this thing called an embedding, which is simply a fast and memory efficient way of multiplying by a one hot encoded matrix. OK. So this is really important because when you hear people say embedding, you need to replace it in your head with an array lookup, which we know is mathematically identical to a matrix multiplied by a one hot encoded matrix. Here's the thing, though. It has kind of interesting semantics. Right. Because when you do multiply something by a one hot encoded matrix, you get this nice feature where the rows of your weight matrix, the values only appear for row number one, for example, where you get user ID number one in your inputs. Right. So in other words, you kind of end up with this weight matrix where certain rows of weights correspond to certain values of your input. And that's pretty interesting. It's particularly interesting here, because going back to a kind of most convenient way to look at this, because the only way that we can calculate an output activation is by doing a dot product of these two input vectors. That means that they kind of have to correspond with each other. Right. Like there has to be some way of saying if this number for a user is high and this number for a movie is high, then the user will like the movie. So the only way that can possibly make sense is if these numbers represent features of personal taste and corresponding features of movies. For example, the movie has John Travolta in it and user ID likes John Travolta, then you'll like this movie. OK. So like we're not actually deciding the rows mean anything. We're not doing anything to make the rows mean anything. But the only way that this gradient descent could possibly come up with a good answer is if it figures out what the aspects of movie taste are and the corresponding features of movies are. So those underlying kind of features that appear are called latent factors or latent features. They're these hidden things that were there all along, and once we train this neural net, they suddenly appear. Now here's the problem. No one's going to like Battlefield Earth. It's not a good movie even though it has John Travolta in it. So how are we going to deal with that? Because there's this feature called I like John Travolta movies and this feature called this movie has John Travolta. And so this is now like you're going to like the movie. But we need to have some way to say unless it's Battlefield Earth or you're a Scientologist, either one. So how do we do that? We need to add in bias. So here is the same thing again. Same weight matrix, sorry, not the same weight matrix. He's the same construct, same shape of everything. But this time we've got an extra row. So now this is not just the matrix product of that and that, but I'm also adding on this number and this number, which means now each movie can have an overall, this is a great movie versus this isn't a great movie, and every user can have an overall, this user rates movies highly or this user doesn't rate movies highly. So that's called the bias. So this is hopefully going to look very familiar, right? This is the same usual linear model concept or linear layer concept from a neural net that you have a matrix product and a bias. And remember from lesson two, the lesson two SGD notebook, you never actually need a bias. You could always just add a column of ones to your input data, and then that gives you bias for free. But that's pretty inefficient, right? So in practice, all neural networks library explicitly have a concept of bias. We don't actually add the column of ones. So what does that do? Well, just before I came in today, I ran tools solver or data solver on this as well, and we can check the RMSE. And so the root means grid here is 0.32 versus the version without bias was 0.39. Okay, so you can see that this slightly better model gives us a better result. And it's better because it's giving both more flexibility, right? And it's also just makes sense semantically that you need to be able to say, whether I like the movie is not just about the combination of what actors it has and whether it's dialogue driven and how much action is in it, but just is it a good movie? Okay, or am I somebody who rates movies highly? Okay, so there's all the pieces of this collaborative filtering model. How are we going, Sanchezco? Any questions? We have three questions. Okay. Okay, so our first question then is, when we load a pre-trained model, can we explore the activation grids to see what they might be good at recognizing? Yes, you can. And we will learn how to, should be in the next lesson. Can we have an explanation of what the first argument in fit one cycle actually represents? Is it equivalent to an epoch? Yes. The first argument to fit one cycle or fit is number of epochs. In other words, an epoch is looking at every input once. So if you do 10 epochs, you're looking at every input 10 times. And so there's a chance you might start overfitting if you've got lots and lots of parameters and a high learning rate. If you only do one epoch, it's impossible to overfit. So that's why it's kind of useful to remember how many epochs you're doing. Can we have an explanation? What is an affine function? An affine function is a linear function. I don't know if we need much more detail than that. If you're multiplying things together and adding them up, it's an affine function. I'm not going to bother with the exact mathematical definition, partly because I'm a terrible mathematician and partly because it doesn't matter. But if you just remember that you're multiplying things together and then adding them up, that's the most important thing. It's linear. And therefore, if you put an affine function on top of an affine function, that's just another affine function. You haven't won anything at all. That's a total waste of time. So you need to sandwich it with any kind of non-linearity pretty much works, including replacing the negatives with zeros, which we call ReLU. So if you draw affine ReLU, affine ReLU, affine ReLU, you have a deep neural network. OK, so let's go back to the collaborative filtering notebook and this time we're going to grab the whole MovieLens 100K data set. There's also a 20 million data set, by the way. So really great project made by this group called GroupLens. They actually update the MovieLens data sets on a regular basis, but they helpfully provide the original one. And we're going to use the original one because that means that we can compare to baselines because everybody basically they say, hey, if you're going to compare the baselines, make sure you all use the same data set. Here's the one you should use. Unfortunately, it means that we're going to be restricted to movies that are before 1998. So maybe you won't have seen them all. But that's the price we pay. You can replace this with ML Latest when you download it and use it if you want to play around with movies that are up to date. OK, the original MovieLens data set, the more recent ones are in a CSV file. It's super convenient to use. The original one is a slightly messy. First of all, they don't use commas for delimiters. They use tabs. So in pandas, you can just say what's the delimiter when you load it in. The second is they don't add a header row so that you know what color is what. So you have to tell pandas there's no header row. And then since there's no header row, you have to tell pandas what are the names of the columns. Other than that, that's all we need. OK, so we can then have a look at head, which remembers the first few rows. And there is our user ratings, user movie rating. And let's make it more fun. Let's see what the movies actually are. I'll just point something out here, which is this thing called encoding equals. I've got to get rid of it. And I get this error. Unicode. I just want to point this out because you'll all see this at some point in your lives. Codec can't decode, blah, blah, blah. What this means is that this is not a Unicode file. This will be quite common when you're using data sets that are a little bit older. Back before us folks in the West really realized that there are people that use languages other than English. Nowadays, we're much better at handling different languages. We use this standard called Unicode for it. And Python very helpfully uses Unicode by default. So if you try to load an old file that's not Unicode, you actually, believe it or not, have to guess how it was coded. But since it's really likely that it was created by some Western European or American person, they almost certainly used Latin one. So if you just pop in encoding equals Latin one, if you use file open in Python or pandas open or whatever, that will generally get around your problem. Again, they didn't have the names, so we had to list the names. This is kind of interesting. They had a separate column for every one of however many genres they had, 19 genres. And you'll see this looks one-hot encoded, but it's actually not. It's actually n-hot encoded. In other words, a movie can be in multiple genres. We're not going to look at genres today, but it's just interesting to point out that this is a way that sometimes people will represent something like genre. In the more recent version, they actually list the genres directly, which is much more convenient. OK. So I find life is... So we've got 100,000 ratings. I find life is easier when you're modeling, when you actually denormalize the data. So I actually want the movie title directly in my ratings. So pandas has a merge function to let us do that. So here's the ratings table with actual titles. So as per usual, we can create a data bunch for our applications or a colab data bunch for the colab application from what? From a data frame. There's our data frame. Set aside some validation data. Really, we should use the validation sets and cross validation approach that they used if you're going to properly compare with a benchmark. So take these comparisons with the Graham assault. By default, Colab data bunch assumes that your first column is user, second column of item, third column is rating. But now we're actually going to use the title column as item. So we have to tell it what the item column name is. And then all of our data bunches support show batch. So you can just check what's in there. And there it is. OK. So I'm going to try and get as good a result as I can. So I'm going to try and use whatever tricks I can come up with to get a good a good answer. Now, one of the tricks is to use the Y range. And remember, the Y range was the thing that made the final activation function a sigmoid. And specifically, last week, we said, let's have a sigmoid that goes from naught to five. And that way, it's going to ensure that it's going to help the neural network predict things that are in the right range. I actually didn't do that in my Excel version. And so you can see I've actually got some negatives. And there's also some things bigger than five. So if you want to beat me in Excel, you could you could add the sigmoid to Excel and train this and you'll get a slightly better answer. Now, the problem is that a sigmoid actually asymptotes at, say, whatever the maximums. We said five, which means you can never actually predict five. But plenty of movies have a rating of five. So that's a problem. So actually, it's slightly better to make your Y range go from a little bit less than the minimum to a little bit more than the maximum. And the minimum of this data is point five and the maximum is five. So this range is just a little bit further. So that's a that's one little trick to get a little bit more accuracy. The other trick I used is to add something called weight decay. And we're going to look at that next. OK, after this section, we're going to learn about weight decay. So then how many how many factors do you want? Well, what are factors? The number of factors is the width of the embedding matrix. So why don't we say embedding size? Maybe we should. But in the world of collaborative filtering, they don't use that word. They use the word factors because of this idea of latent factors and because the standard way of doing collaborative filtering has been with something called matrix factorization. And in fact, what we just saw happens to actually be a way of doing matrix factorization. So we've we've actually accidentally learned how to do matrix factorization today. So so this is a term that's kind of specific to this domain. But you can just remember it as the width of the embedding matrix. And so why 40? Well, this is one of these architectural decisions you have to play around with and see what works. So I tried 10, 20, 40 and 80, and I found 40 seemed to work pretty well. It's changed really quickly, so like you can chuck it in a little for loop to try a few things and see what looks best. And then for learning rates, so use the learning rate finder as usual. So five e neg three seemed to work pretty well. Remember, this is just a rule of thumb. Five e neg three is a bit lower than both Sylvain's rule and my rule. So Sylvain's role is find the bottom and go back by 10. So his role would be more like two e neg two, I reckon. My rule is kind of find about the steepest section, which is about here, which again, like often it agrees with Sylvain. So that would be about two e neg two. I tried that, but I always like to try like 10x less and 10x more just to check. And actually, I found a bit less was helpful. So the answer to the question, like, should I do blah is always try blah and see. That's how you actually become a good practitioner. So that gave me point eight one three. And as usual, you can save the result to save you another 33 seconds from having to do it again later. And so there's a library called Libreq and they publish some benchmarks for movie lens 100K. And there's a root means grid error section. And about point nine one is about as good as they seem to have been able to get. Point nine one is the root means grid error. We use the mean grid error, not the root. So we have to go point nine one squared, which is point eight three. And we're getting point eight one. So that's cool. With this very simple model, we're doing a little bit better, quite a lot better, actually. Although, as I said, take it with a grain of salt because we're not doing the same splits and the same cross validation. So we're at least highly competitive with their approaches. OK, so we're going to look at the Python code that does this in a moment. We're going to look at the Python code that does this in a moment. But for now, just take my word for it that we're going to see something that's just doing this, right? Looking things up in an array and then model plug them together, adding them up, and doing the mean squared error loss function. So given that and given that we noticed that the only way that that can do anything interesting is by trying to kind of find these latent factors. It makes sense to look and see what they found, right? Particularly since as well as finding latent factors, we also now have a specific bias number for every user and every movie, right? Now, you could just say, what's the average rating for each movie? But there's a few issues with that. In particular, this is something you see a lot with like anime. People who like anime just love anime, right? And so they watch lots of anime and then they just rate all the anime highly. And so very often on kind of charts of movies, you'll see a lot of anime at the top. Particularly if it's like, you know, a hundred long series of anime, you'll find, you know, every single item of that series in the top thousand movie list or something. So how do we deal with that? Well, the nice thing is that instead if we look at the movie bias, right? The movie bias says kind of once we've included the user bias, right? Which for an anime lover might be a very high number because they're just rating a lot of movies highly. And once we account for the specifics of this kind of movie, which again might be people love anime, right? What's left over is something specific to that movie itself. So it's kind of interesting to look at movie bias numbers as a way of saying, what are the best movies or what do people really like as movies? Even if those people don't rate movies very highly or even if that movie doesn't have the kind of features that people tend to have rate highly. So it's kind of nice. It's funny to say this. And by using the bias, we get an unbiased kind of movie score. So how do we do that? Well, to make it interesting, particularly because this data set only goes to 1998, let's only look at movies that plenty of people watch. So we'll use pandas to grab our rating movie table, group it by title, and then count the number of ratings. I'm not measuring how high their rating, just how many ratings do they have? And so the top thousand are the movies that have been rated the most. And so they're hopefully movies that we might have seen. That's the only reason I'm doing this. And so I've called this top movies, by which I mean not good movies, just movies we're likely to have seen. But not surprisingly, Star Wars is the one that at that point most the most people had put a rating to. Independence Day. There you go. So we can then take our learner that we trained and ask it for the bias of the items listed here. So is item equals true? You would pass true to say I want the items or false to say I want the users. So this is kind of like a pretty common piece of momentum for collaborative filtering. These IDs tend to be called users. These IDs tend to be called items. Even if your problem has got nothing to do with users and items at all, you know, we just use these names for convenience. OK, so they're just they're just words. So in our case, we want the items. This is the list of items we want. We want the bias. So this is specific to collaborative filtering. And so that's going to give us back a thousand numbers because we asked for this has a thousand movies in it. So we can now take and just for comparison, let's also group the titles by the mean rating. So then we can zip through. So going through together each of the movies along with the bias and grab their rating and the bias and the movie. And then we can sort them all by the zero index thing, which is the bias. So here are the lowest numbers. So I can say, you know, Mortal Kombat Annihilation, not a great movie. Mon Mo Man 2, not a great movie. I haven't seen Children of the Corn, but we did have a long discussion at SF study group today and people who have seen it agree. Not a great movie. And you can kind of see like some of them actually have pretty decent ratings, even though like relative to. Right. So this one's actually got a much higher rating than the next one. Right. But, you know, that's kind of saying, well, the kind of actors that were in this and the kind of movie that this was and the kind of people who do like it, who watch it, you would expect it to be higher. And then here's the sort by reverse. OK, Shinzo's List, Titanic, Shawshank Redemption seems reasonable. And again, you can kind of look for ones where like the rating, you know, isn't that high, but it's still very high here. So that's kind of like, you know, at least in 1998, people weren't that into Leonardo DiCaprio or, you know, people aren't that into dialogue driven movies or people aren't that into romances or whatever. But still people liked it more than you would expect. So it's interesting to kind of like interpret our models in this way. We can go a bit further and grab not just the biases, but the weights. So that is these things. And again, we're going to grab the weights for the items for our top movies. And that is a thousand by forty because we asked for forty factors. So rather than having a width of five, we have a width of forty. Often, really, there's there isn't really conceptually forty latent factors involved in taste. And so trying to look at the forty can be, you know, not that intuitive. So what we want to do is we want to squish those forty down to just three. And there's something that we're not going to look into called PCA stands for principal components analysis. So this is a movie. W is a torch tensor and fast. I adds the PCA method to torch tensors. And what PCA does principle components analysis is it's a simple linear transformation that takes an input matrix and tries to find a smaller number of columns that kind of cover a lot of the space of that original matrix. If that sounds interesting, which it totally is, you should check out our course computational linear algebra, which Rachel teaches, where we will show you how to calculate PCA from scratch and why you'd want to do it and lots of stuff like that. It is absolutely not a prerequisite for anything in this course. But it's definitely worth knowing that taking layers of neural nets and chucking them through PCA is very often a good idea because very often you have like way more activations than you want in a layer. And there's all kinds of reasons you might want to play with it. For example, Francisco, who's sitting next to me today, is has been working on something to do image similarity. Right. And for image similarity, a nice way to do that is to compare activations from a model. But often those activations will be huge and therefore your thing could be really slow and unwieldy. So people often for something like image similarity will chuck it through a PCA first. And that's kind of cool. In our case, we're just going to do it so that we take our 40 components down to three components. So hopefully they'll be easier for us to interpret. So we can grab each of those three factors. We'll call them factor not one and two. And let's grab that movie components and then sort. And now the thing is we have no idea what this is going to mean, but we're pretty sure it's going to be some aspect of taste and movie feature. So if we print it out, the top and the bottom, we can see that the highest ranked things on this feature, you would kind of describe them as, you know, connoisseurs movies, I guess, you know, like Chinatown, you know, really classic Jack Nicholson movie. Everybody knows Casablanca and even like Wrong Trousers is like this kind of classic claymation movie and so forth. Right. So, yeah, this this is definitely measuring like things that are very high on the kind of connoisseur level. Where else? Maybe Home Alone 3, not such a favorite with connoisseurs, perhaps. It's just not to say that there aren't people who don't like it. Right. But probably not the same kind of people that would appreciate secrets and lies. So you can kind of see this idea that this is found some feature of movies and a corresponding feature of the kind of things people like. So let's look at another feature. So here's factor number one. So this seems to have found like, OK, these are just big hits that you could watch with the family. You know, these are definitely not that, you know, Trainspotting, very gritty kind of, you know, thing. So, again, it's kind of found this interesting feature of taste. And we could even like. Draw them on a graph, right. I just kind of them randomly to make them easier to see. And you can kind of see like this is just the top 50 most popular movies by rating by how many times they've been rated. And so kind of on this one factor, you've got that of the Terminators really high up here and the kind of English patient and Stindle's List at the other end. And then kind of is your godfather and Monty Python over here and Independence Day and Liar Liar over there. So you get the idea. So that's kind of fun. It would be interesting to see if you can come up with some stuff at work or other kind of data sets where you could try to pull out some some features and play with them. So how does that work? Any questions? One. OK. The question is, why am I sometimes getting negative loss? When training. You shouldn't be. So you're doing something wrong. So ask on show us your your particularly since people are uploading this, I guess other people seen it too. So put it on the forum. I mean, they said they're doing negative log likelihood. Yeah. So we're going to be learning about cross entropy and negative log likelihood after the break today. They lost functions that have very specific expectations about what your input looks like. And if your input doesn't look like that, then they're going to give very weird answers. So probably you press the wrong buttons. So don't do that. OK. OK. So we said. Colab learner. And so here is the Colab learner function. The Colab learner function, as per usual, takes a. A data bunch and normally learners also take something where you ask for a particular architectural details. In this case, there's only one thing which does that, which is basically do you want to use a multi layer neural net or do you want to use a classic collaborative filtering? And we're only going to look at the classic collaborative filtering today. Or maybe you'll briefly look at the other one too. We'll see. And so what actually happens here? Well, basically we're going to create we create a an embedding bias model and then we pass back a learner which has our data and that model. So obviously all the interesting stuff is happening here and embedding bias. So let's take a look at that. I clearly press the wrong button. Beding bias. There we go. OK. So. Here's our embedding bias model. It is a. NN dot module. So in in PyTorch to remind you all PyTorch layers and models are NN dot modules. They are things that once you create them look exactly like a function. You call them with parentheses and you pass them arguments. But they're not functions. They don't even have normally in Python to make something look like a function. You have to give it a method called Dunder call. Remember that means underscore underscore call underscore underscore which doesn't exist here. And the reason is that PyTorch actually expects you to have something called forward. And that's what PyTorch will call for you when you call it like a function. So when this model is being trained to get the predictions it's actually going to call forward for us. So this is where we. Do the calculations right. To calculate our predictions. So this is where you can see we grab our. Why is this users rather than user. That's because everything's done a mini batch at a time. Right. So it is kind of when I read the forward in in a PyTorch module I tend to ignore in my head the fact that there's a mini batch and I pretend there's just one. Because PyTorch automatically handles all of the stuff about doing it to everything in the mini batch for you. Right. So let's pretend there's just one user. Right. So grab that user and what is this self dot you underscore weight self dot you underscore weight is. An embedding we create an embedding for each of. Users by factors items by factors users by one items by one. That makes sense right. So users by one. Is. Here that's the users bias. Right. And then users by factor is here. So. Users by factors is the first couple so that's going to go in you underscore weight. And users comma one is the third so that's going to go in you underscore bias. So remember when PyTorch creates our and end up module it calls Dunder in it. And so this is where we have to create our weight matrices. And we don't normally create the actual weight matrix tensors we normally use PyTorch convenience functions to do that for us. And we're going to see some of that after the break. So for now just recognize that this function is going to create an embedding matrix for us. It's going to be a PyTorch and end up module as well. So therefore to actually pass stuff into that embedding matrix and get activations out you treat it as if it was a function. Stick it in parentheses. So if you want to look in the Python PyTorch source code and find an end on embedding you will find there's something called forward in there which will do this array look up for us. So here's where we grab the users. Here's where we grab the items. And so we've now got the embeddings for each. Right. And so at this point we're kind of like here and we found that and that. So we multiply them together and sum them up and then we add on the user bias and the item bias. And then if we've got a wide range then we do our sigmoid trick. And so the nice thing is you know you now understand the entirety of this model. And this is not just any model. This is a model that we just found is at the very least highly competitive with and perhaps slightly better than some published table of pretty good numbers from a software group that does nothing but this. So you're doing well. This is nice. So that's probably a good place to have a break. And so after the break we're going to come back and we're going to talk about the one piece of this puzzle we haven't learned yet which is what the hell does this do. OK. So let's come back at seven fifty. OK. So this idea of interpreting embeddings is really interesting. And as we'll see later in this lesson the things that we create for categorical variables more generally in tabular data sets are also embedding matrices. And again that's just a normal matrix multiply by a one hot encoded input where we skip the computational computational and memory burden of it by doing it in a more efficient way. And it happens to end up with these interesting semantics kind of accidentally. And there was this really interesting paper by these folks who came second in a capital competition for something called a Rossman. We'll probably look in more detail at the Rossman competition in part two. I think we're going to run out of time in part one. But it's basically this pretty standard tabular stuff. The main interesting stuff is in the pre-processing. And it was interesting because they came second despite the fact that the person who came first and pretty much everybody else was at the top of the leaderboard did a massive amount of highly specific feature engineering whereas these folks did way less feature engineering than anybody else. But instead they used a neural net. And this was at a time in 2016 when just no one did that. No one was doing neural nets with tabular data. So they have the kind of stuff that we've been talking about kind of arose there or at least was kind of popularized there. And when I say popularized I mean only popularized a tiny bit. Still most people aren't aware of this idea. But it's pretty cool because in their paper they show that the mean average percentage error for various techniques K nearest neighbors random forest and gradient boosted trees. Well first you know neural nets just worked worked a lot better. But then with entity embeddings which is what they call this using entity matrices in tabular data. You can actually they actually added the entity embeddings to all of these different tasks after training them and they all got way better. Right. So neural nets with entity embeddings are still the best. But a random forest with empty embeddings was not at all far behind. And you know that's often kind of that's kind of nice track because you could train these entity matrices for products or stores or genome motifs or whatever. And then use them in lots of different models possibly using faster things like random forests by getting a lot of the benefits. But here was something interesting. They took a two dimensional projection of their of their embedding matrix for state for example German state because this was a German supermarket chain. I think using the same kind of approach we did I don't remember if they use PCA or something else like different. And then here's the interesting thing. I've circled here you know a few things in this embedding space and I've circled it with the same color over here. And here I've circled some same color over here. And it's like oh my god the embedding projection. Has actually discovered geography. They didn't do that. But it's it's it's found things that are nearby each other in grocery purchasing patterns because this was about predicting how many sales there will be. You know it's there is some geographic element of that. In fact here is a graph of the distance between two embedding vectors so you can just take an embedding vector and say what's the sum of squared you know compared to some other embedding vector. That's the Euclidean distance. What's the distance in embedding space and then plot it against the distance in real life between shops. And you get this very strong positive correlation. Here is an embedding space for the days of the week. And as you can see there's a very clear path through them. Here's the embedding space for the month of the year. And again there's a very clear path through them. So like. Embeddings are amazing and I don't feel like anybody's even close to. Exploring. The kind of interpretation that you could get right. So if you've got. Genome motifs or plant species or. Products that your shop sells or whatever like it would be really interesting to train a few models and try and kind of fine tune some embeddings and then like. Start looking at them in these ways in terms of similarity to other ones and clustering them and projecting them into these spaces and whatever I think is really interesting. So we were trying to make sure we understood what every line of code did in this some pretty good. Colab learner model we built and so the one piece missing is this WD piece and WD starts stands for weight decay. So what is weight decay weight decay is a type of regularization. What is regularization. Well let's start by going back to this nice little chart that Andrew did in his terrific machine learning course where he plot plotted some data and then showed a few different lines through it. This one here because Andrew's at Stanford he has to use Greek letters. OK so we can say this is a plus BX but you know if you want to go there theta naught plus theta 1 X. Is a line right. It's a line even if it's a Greek letters it's still a line. So here's a second degree polynomial A plus BX plus CX squared bit of curve. And here's a high degree polynomial which is curvy as anything. So. Models with more parameters tend to look more like this. And so in traditional statistics we say hey let's use less parameters because we don't want it to look like this because if it looks like this then the predictions over here and over here they're going to be all wrong. It's not going to generalize well. We're overfitting. So we avoid overfitting by using less parameters. And so if any of you are unlucky enough to have been brainwashed by a background in statistics or psychology or econometrics or any of these kinds of courses you'll have you know you're going to have to unlearn the idea that you need less parameters because what you instead need to realize this is. You will fit this lie that you need less parameters because it's a convenient fiction for the real truth which is you don't want your function to be too complex and having less parameters is one way of making it less complex. But what if you had a thousand parameters and 999 of those parameters were one E neg nine. Well what if there was zero if there's zero then they're not really there or if they want to make nine they're hardly there. Right. So like why can't I have lots of parameters if like lots of them are really small. And the answer is you can. OK. You know so this this thing of like counting the number of parameters is how we limit complexity is actually extremely limiting. It's a fiction that really has a lot of problems right. And so if in your head complexity is scored by how many parameters you have you're doing it or wrong. Right. Score it properly. Right. So why do we care. Why would I want to use more parameters because more parameters means more nonlinearities more interactions more curvy bits. Right. And real life is full of curvy bits. Right. Real life does not look like this. But we don't want them to be more curvy than necessary or more interacting than necessary. So therefore let's use lots of parameters and then penalize complexity. OK. So one way to penalize complexity is as I kind of suggested before is let's sum up the value of your parameters. Now that doesn't quite work because some parameters are positive and some are negative. Right. So what if we sum up the square of the parameters. Right. And that's actually a really good idea. Let's actually create a model and in the loss function we're going to add the sum of the square of the parameters. Now here's the problem with that though. Maybe that number is way too big and it's so big that the best loss is to set all of the parameters to zero. That would be no good. Right. So actually we want to make sure that doesn't happen. So therefore let's not just add the sum of the square to the parameters to the model but let's multiply that by some number that we choose. And that number that we choose in fast AI is called WD. OK. So that's what we're going to do. We're going to take our loss function and we're going to add to it the sum of the square of the parameters multiplied by some number WD. What should that number be. Well generally it should be zero point one. People with fancy machine learning PhDs are extremely skeptical and dismissive of any claims that a learning rate can be three e next three most of the time or a weight decay can be point one most of the time. But here's the thing. We've done a lot of experiments on a lot of data sets and we've had a lot of trouble finding anywhere a weight decay of point one isn't great. However we don't make that the default. We actually make the default point oh one. Why. Because in those rare occasions where you have too much weight decay no matter how much you train it just never quite fits well enough. Where else if you have too little weight decay you can still train well you'll just start to overfit. So you just have to stop a little bit early. So we've been a little bit conservative with our defaults but my suggestion to you is this now that you know that every learner has a WD argument and I should mention you won't always see it in this list. Right. Because this is concept of KW args in Python which is basically parameters that are going to get passed up the chain to the next thing that we call. And so basically all of the learners will call eventually this constructor and this constructor has a WD. Right. So this is just one of those things that you can either look in the docs or you now know it. Anytime you're constructing a learner from pretty much any kind of function in faster you can pass WD. And so passing point one instead of the default point 0 1 will often help. So give it a go. So what's really going on here. It would be helpful I think to go back to lesson 2 SGD because everything we're doing for the rest of today really is based on this. And this is where we created some data and then we try and then we added a loss function MSE and then we created a function called update which calculated our predictions. That's our weight matrix multiply. This is just a one layer. So there's no value. We calculated our loss using that means grid error. We calculated the gradients using lost up backward. We then subtracted in place the learning rate times the gradients and that is gradient descent. So if you haven't reviewed lesson 2 SGD please do because this is where we're this is our starting point. So if you don't get this then none of this is going to make sense. If you're watching the video maybe pause now go back rewatch this part of listen to make sure you get it. Remember a dot sub underscore is basically the same as a minus equals because a dot sub is subtract and everything in pie torch. If you add an underscore to it means do it in place. So this is updating our parameters which started out as minus point one one. We just arbitrarily picked those numbers and it gradually makes them better. So let's write that down. So we are trying to calculate the parameters. I'm going to call them weights because this is just more common. In kind of epoch T or time T and they're going to be equal to whatever the weights were in the previous epoch minus our learning rate multiplied by. It's the derivative of our loss function with respect to our weights at time T minus one. So that's that's what this is doing. OK. And we don't have to calculate the derivative because it's boring and because it computers do it for us fast and then they store it here for us. So we're good. So make sure you're exceptionally comfortable with either that equation or that line of code because they have the same thing. Where do we go from here. All right. So. What's that. What's our loss. Our loss. Is some function. Of. Our independent variable variables X and. Our weights. Right. And in our case we're using. Mean squared error for example and it's between our predictions and our actuals. Right. So where does X and W come in. Well our predictions come from running some model. Call it M. On those predictions and that model contains some weights. So that's that's what our loss function might be. And this might be all kinds of other loss functions. We'll see some more today. And so that's what ends up. Creating. A dot grad over here. So we're going to do something else. We're going to add. Weight decay some number which in our case is 0.1 times. Times the sum. Of. Weights. Squared. OK. So let's do that. And let's make it interesting. By not using synthetic data but let's say some real data. And we're going to use MNIST the hand drawn digits. Right. But we're going to do this as a standard fully connected net not as a convolutional net because we haven't learned. The details of how to really create one of those from scratch. So in this case is actually deep learning dot net provides MNIST as a Python pickle file. In other words it's a file that pickle that Python can just open up and it'll give you numpy arrays straight away and they're flat numpy arrays. We don't have to do anything to them. So go grab that. And it's a GZIP file so you can actually just GZIP dot open it directly. And then you can pick all dot load it directly. And again encoding equals Latin one because yeah you know. And then we can just put that's that'll give us the training the validation and the test set. I don't care about the test set. So generally in Python if there's like something you don't care about you tend to use this special variable called underscore. There's no reason you have to. It's just kind of people know you mean I don't care about this. So there's a training training X and Y and a valid X and Y. Now this actually comes in as a as you can see if I sprint the shape 50,000 rows by 784 columns but the 784 columns are actually 28 by 28 pixel pictures. So if I reshape one of them into a 28 by 28 pixel picture and plot it right then you can see it's the number five. OK. So that's our data. We've seen MNIST before and it's kind of pre reshaped version. Here it is in its flattened version. So I'm going to be using it in its flattened version. And currently they are NumPy arrays. I need them to be tensors so I can just map torch dot tensor across all of them. And so now the tensors. OK. I may as well create a variable with the number of things I have which we normally call N. And remember we normally have a thing called you know we tend to use C to mean the number of activations we need. We're actually so this is not going to be activation. Sorry this is going to be a number of columns. That's not a great name for it. Sorry. OK. So there we are. And then the the Y not surprisingly the minimum value is zero and the maximum value is nine because that's the extra number we're going to predict. So in lesson two SGD we like we created data where we actually added a column of ones on so that we didn't have to worry about bias. We're not going to do that. We're going to have pie torch do that kind of implicitly for us. We had to write our own MSC function. We're not going to do that. We had to write our own little matrix multiplication thing. We're not going to do that. We're going to have pie torch do all this stuff for us. OK. And what's more and really important we got we're going to do mini batches. This is a big enough data set. We probably don't want to do it all at once. So if you want to do mini batches so we're not going to use too much fast stuff here. Pie torch has something called tensor data set that basically grabs a any kind of tensor or two tensors and creates a data set. Remember a data set is something where if you index into it you get back an X value and a Y value just one of them. So it kind of looks like it looks a lot like a list of X Y couples. Once you have a data set then you can use a little bit of convenience by calling data bunch dot create. And what's that going to do is it's going to create data loaders for you. A data loader is something which you don't say I want the first thing or the fifth thing. You just say I want the next thing and it will give you a batch a mini batch of whatever size you asked for. And specifically it'll give you the X and the Y of a mini batch. So if I just grab the next of the iterator this is just standard Python. If you haven't used iterators in Python before here's my training data loader that data bunch dot create creates for you. And you can check that as you would expect the X is 64 by 784 because there's 784 pixels flattened out 64 in a mini batch. The Y is just 64 numbers. There are things we're trying to predict. So and you know if you look at the source code for data batch dot create you'll see there's not much there. But feel free to do so. We just make sure that like your training set gets shuffled randomly shuffled for you. We make sure that the data is put on the GPU for you. Just a couple of little convenience things like that. But don't let it be magic. If it feels magic check out the source code to make sure you see what's going on. OK. So rather than do this Y hat equals X hat a thing we're going to create an end up module. All right. If you want to create an end up module that does something different to what's already out there you have to subclass it. Right. So subclassing is very very very normal in PyTorch. So if you're not comfortable with subclassing stuff in Python go read a couple of tutorials to make sure you are. The main thing is you have to override the constructor done to in it and make sure that you call the super classes constructor because end up module super classes constructor is going to like set it all up to be a proper end up module for you. So if you try to using if you're trying to create your own PyTorch subclass and things don't work it's almost certainly because you forgot this line of code. All right. So the only thing we want to add is we want to create an attribute in our class which contains a linear layer and an end up linear module. What is an end up linear module. It's something which does that but actually it doesn't only do that it actually is X at a plus B. So in other words we don't have to add the column of ones. That's all it does. OK so if you want to play around why don't you try and create your own and end up linear class. You could create something called my linear and it'll take you you know depending on your PyTorch background an hour or two and then you'll feel like OK this is we don't want any of this to be magic and you know all of the things necessary to create this now. So you know these are the kind of things that you should be doing for your assignments this week is not so much new applications but try to start writing more of these things from scratch and get them to work learn how to debug them check what's going in and out and so forth. OK. But we can just use an end up linear and that's just going to do so it's going to have a depth forward in it that goes a at X plus B. Right. And so then an hour forward. How do we calculate the result of this. Well remember every end up module looks like a function. So we pass our X mini batch. So I tend to use X B to mean a batch of X to self dot Lin and that's going to give us back the result of the A at X plus B on this mini batch. So this is a logistic regression model a logistic regression model is also known as a neural net with no hidden layers. So it's a one layer neural net no nonlinearities because we're doing stuff ourselves a little bit. We have to put the weight matrices the parameters onto the GPU manually. So just type dot CUDA to do that. So here's our model. And as you can see the end up module machinery has automatically given us a representation of it. It's automatically stored the dot Lin thing and it's telling us what's inside it. So there's a lot of little conveniences that pie torch does for us. So if you look at now at model dot Lin you can see not surprisingly here it is perhaps the most interesting thing to point out is that our model automatically gets a bunch of methods and properties and perhaps the most interesting one is the one called parameters which contains all of the yellow squares from our picture that contains our parameters it contains our weight matrices and bias matrices in as much as they're different. So if we have a look at P dot shape for P and model dot parameters there's something of 10 by 784 and there's something of 10. So what are they or 10 by 784. OK so that's the thing that's going to take in 784 dimensional input and spit out a 10 dimensional output because that's handy because our input is 784 dimensional and we need something that's going to give us the probability of 10 numbers. After that happens we've got 10 activations which we then want to add the bias to. So there we go. Here's a vector of length 10. So you can see why this this model we've created has exactly the stuff that we need to do our AX plus B. So let's grab a learning right. We're going to come back to this loss function in a moment but we can't use MS. Well we can't really use MSC for this right because we're not trying to say how close are you. Did you predict three and actually it was four. Gosh you were really close. It's like no three is just as far away from four as zero is away from four when you're trying to predict what number did somebody draw. So we're not going to use MSC. We're going to use cross entropy loss which we'll look at in a moment. And here's our update function. I copied it from lesson 2 SGD. But now we're calling our model rather than going AX we're calling our model as if it was a function to get Y hat. And we're calling our loss func rather than calling MSC to get our loss. And then this is all the same as before except rather than going through each parameter and going parameter dot sub underscore learning rate times gradient. We loop through the parameters. OK. Because very nicely for us pipe torch will automatically create this list of the parameters of anything that we created in our Dundee in it. And look I've added something else. I've got this thing called W2. I go through each PN model dot parameters and I add to W2 the sum of squares. So W2 now contains my sum of squares weights and then I multiply it by some number which I set to one A neg five. So now I just implemented weight decay. OK. So when people talk about weight decay it's not an amazing magic complex thing containing thousands of lines of CUDA C++ code. It's. Those two lines of Python that's where to get this is not a simplified version that's just enough for now. This is where to get that's it. OK. And so here's the thing. There's a really interesting kind of dual way of thinking about weight decay. One is that we're adding the sum of squared weights and that seems like a very sound thing to do. And it is. And let's go ahead and run this. So here I've just got a list comprehension that's going through my data loader. So the data loader gives you back one mini batch for the whole thing giving you X Y each time. I'm going to call update for each. Each one returns loss. Now PyTorch tensors. Since I did it all on the GPU that's sitting in the GPU and it's like got all this stuff attached to it to calculate gradients. It's going to use up a lot of memory. So if you if you call dot item on a scalar tensor it turns it into an actual normal Python number. So this is just means I'm returning back normal Python numbers and then I can plot them. And yeah there you go. My loss function is going down. And you know it's really nice to try this stuff to see it behaves as you expect. Like we thought this is what would happen as we get closer and closer to the answer. It bounces around more and more. Right. Because we're kind of close to where we should be. It's kind of getting flat probably flatter in weight space. So we kind of jumping further. And so you can see why we would probably want to be reducing our learning rate as we go. Learning rate annealing. Now here's the thing that is only interesting for training a neural net because. It appears here. Because we take the gradient of it. That's the thing that actually updates the weights. Right. So they actually the only thing interesting about WD times sum of W squared is its gradient. So we don't do a lot of math here but I think we can handle that. The gradient. Of this whole thing. If you remember back to your high school math is equal to the gradient of each part taken separately and then add them together. So let's just take the gradient of that. Right. Because we already know the gradient of this is just whatever we had before. Right. So what's the gradient of WD times the sum of W squared. Right. Let's remove the sum and pretend that just one parameter. It doesn't change the generality of it. So the gradient of WD times W squared. What's the gradient of that. With respect to W. It's just two WD times W. Right. And so remember this is our constant which in our case was like well in that little loop it was one E neg five. OK. And that's our weights. And like we could replace WD with like two WD without loss of generality. So let's throw away the two. So in other words all weight decay does is it subtracts some constant times the weights every time we do a batch. So that's why it's called weight decay. When it's in this form where we add the square to the loss function that's called L2 regularization. When it's in this form where we subtract WD times weights from the gradients that's called weight decay. And they are kind of mathematically identical for everything we've seen so far. In fact they are mathematically identical. And we'll see in a moment a place where they're not where things get interesting. OK. So this is just a really important tool you now have in your toolbox. You can make giant neural networks and still avoid overfitting by adding more weight decay. Or you could use really small data sets with moderately large sized models and avoid overfitting with weight decay. It's not magic. Right. Like you might still find you don't have enough data in which case like you get to the point where you're not overfitting by adding lots of weight decay. And it's just not training very well. That can happen. But at least this is something that you can now play around with. Just to kind of go on here. Now that we've got this update function we could replace this MNIST logistic with MNIST neural network and build a neural network from scratch. But now we just need two linear layers. Right. And the first one we could use a weight matrix of size 50. And so we did need to make sure that the second linear layer has an input of size 50 so it matches. The final layer has to have an output of size 10 because that's the number of classes we're predicting. And so now our forward just goes to a linear layer. Calculate value to a second linear layer. And now we've actually created a neural network from scratch. I mean we didn't write it in linear but you can write it yourself or you could like do the matrices directly you know how to. So again you know if we go a model. Cuda and then we can calculate losses with the exact same update function. There it goes. Right. So this is why this kind of idea of neural nets is so easy. Right. Once you have something that can do gradient descent. Right. Then you can try different models. And then you can start to add more pie torch stuff. So like rather than add doing all this stuff yourself. Why not just go. Opt equals opt in dot something. So the something we've done so far is SGD. And so now you're saying to pie torch. I want you to take these parameters and optimize them using SGD. And so this now rather than saying for P and parameters P minus equals LR times P dot grad you just say up dot step. It's the same thing. It's just less code. Right. But and it does the same thing. But the reason it's kind of particularly interesting is that now you can replace SGD with Adam for example. And you can even add things like weight decay. Right. Because like there's more stuff that's kind of in these things for you. Right. So that's why we tend to use you know opt in dot blast. So behind the scenes this is actually what we do in fast. So if I go up to him dot SGD. OK. So there's that. Right. And so that's that's just that picture. But if we change to a different optimizer. So look what happened. It diverged. We've seen a great picture of that from one of our students who showed what divergence looks like. This is what it looks like when you try to train something. So let's use reducing a different optimizer. So we need a different learning rate. And you can't just continue training because by the time it's diverged the group the weights are like really really big and really really small. They're not going to come back. So start again. OK. There's a better learning rate. But look at this. We're down underneath point five by about a POC 200. Where else before. I'm not even sure we ever got to quite that level. So what's going on. What's what's Adam. Let me show you. And we're going to do gradient descent in Excel because why wouldn't you. OK. So here is some randomly generated data case of X's and some Y's. Well they're actually they're randomly generated X's and the Y's are all calculated by doing X plus B where A is two and B is 30. OK. So this is some data that we're going to try and match. And here is SGD. And so we're going to do it with SGD. Now in our lesson two SGD notebook we did the whole data set at once as a batch in the notebook we just looked at we did many batches in this spreadsheet. We're going to do online gradient descent which means every single row of data is a batch. It's got a batch size of one. So as per usual we're going to start by picking an intercept and slope kind of arbitrarily. So I'm just going to pick them at one. Doesn't really matter. So here I've copied over the data. This is my X and Y. And so my intercept and slope as I said is one. I'm just literally referring back to this cell here. So my prediction for this particular intercept and slope would be 14 times one plus one which is 15. And so there's my error means this my sum of squareds. Well not even a sum at this point. It's the squared error. OK. So now I need to calculate the gradient so that I can update. There's two ways you can calculate the gradient. One is analytically. And so I you know you can just look them up on Wolfram Alpha or whatever. So there's the gradients if you write it out by hand or look it up or you can do something called finite differencing. Because remember gradients just how far you move in. So how far you how far the outcome moves divided by how far your change was to really small changes. So let's just make a really small change. So here we've taken our intercept and added point 0 1 to it. Right. And then calculated our loss. And you can see that our loss went down a little bit. Right. And we added point 0 1 here. So our derivative is that difference divided by that point 0 1. And that's called finite differencing. You can always do derivatives of finite differencing. It's slow. We don't do it in practice but it's nice for just checking stuff out. So we can do the same thing for a term at point 0 1 to that. Take the difference and divide by point 0 1. Or as I say we can calculate it directly using the actual derivative analytical. And you can see that you know that and that are as you'd expect very similar. And that and that are very similar. So gradient descent then just says let's take our current value of that weight and subtract the learning rate times the derivative. There it is. And so now we can copy that intercept and that slope to the next row and do it again. And do it lots of times. And at the end we've done one epoch. So at the end of that epoch we could say oh great. So this is our slope. So let's copy that over to where it says slope. And this is our intercept. So a copy it where it says intercept. And now it's done another epoch. OK. So that's kind of boring. I'm copying and pasting. So I created a very sophisticated macro which copies and pastes for you. And so I just recorded it basically. And so and then I created a very sophisticated for loop that goes through and does it five times. And I attached that to the run button. So if I press run it'll go ahead and do it five times and just keep track of the error each time. OK. So that is SGD. And as you can see it is just infuriatingly slow. Like particularly the intercept is meant to be 30. And we're still only up to 1.57 and like just it's just going so slowly. So let's speed it up. So the first thing we can do to speed it up is to use something called momentum. So here's the exact same spreadsheet as the last worksheet. I've removed the finite differencing version of the derivatives because they're not that useful. Just the analytical ones here. And here's the thing where I take the the derivative and I'm going to update by the derivative. But what I do it's kind of more interesting to look at this one is I take the derivative and I multiply it by point one. And what I do is I look at the previous update and I multiply that by point nine and I add the two together. So in other words the update that I do is not just based on the derivative but a tenth of it is the derivative. And 90 percent of it is just the same direction I went last time. And this is called momentum. Right. What it means is remember how we kind of thought about what might happen if you're trying to find the minimum of this. And you were here and your learning rate was too small. Right. And you just keep doing the same steps or if you keep doing the same steps then if you also add in the step you took last time then your steps are going to get bigger and bigger aren't they. OK. Until eventually they go too far. But now of course your gradient is pointing the other direction to where your momentum is pointing. So you might just take a little step over here and then you start going small steps bigger steps bigger steps all steps bigger steps like that. Right. So that's kind of what momentum does. Or if you're if you're kind of going too far. Like this which is also slow. Right. Then the average of your last few steps is actually somewhere. Kind of between the two isn't it. Right. So this is a really common idea. Right. It's like when you have something that says kind of my what is in this case it's like my step my step at time T equals some number. People often use alpha because like I say you've got to love these Greek letters some number times the actual thing I want to do. Right. So in this case it's like the gradient right plus one minus alpha times whatever you had last time as T minus one. This thing here is called an exponentially weighted moving average. And the reason why is that if you think about it these one minus alphas are going to multiply. So s t minus two is in here with a kind of a one minus alpha squared and s t minus three is in there with a one minus alpha cubed. So in other words this ends up being the actual thing I want plus a weighted average of the last few time periods where the most recent ones are exponentially higher weighted. OK. And this is going to keep popping up again and again. Right. So that's what momentum is. It says I want to go based on the current gradient plus the exponentially weighted moving average of my last few steps. So that's useful. That's called s g d with momentum. And we can do it by changing this here to saying s g d momentum and momentum point nine is really common. It's like it's so common it's always point nine just about for basic stuff. So that's how you do s g d with momentum. And again it's not I didn't show you some simplified version. I showed you the version that is that is s g d. OK. That's that's again you can write your own try it out. That would be a great assignment would be to take less than two s g d and add momentum to it or even the new notebook we've got for MNIST. Get rid of the opt in dot and write your own update function with momentum. Then there's a cool thing called RMS prop one of the really cool things about RMS prop is that Jeffrey Hinton created it. Famous neural net guy. Everybody uses it. It's like really popular. It's really common. The correct citation for RMS prop is the Coursera online free MOOC. That that's where he first mentioned RMS prop. So I love this thing that like you know cool new things appear in MOOC said not a paper. So RMS prop is very similar to momentum. But this time we have an exponentially weighted moving average not of the gradient updates but of F8 squared. That's the gradient squared. So what the gradient squared times point one plus the previous value times point nine. So it's an exponentially this is an exponentially weighted moving average of the gradient squared. So what's this number going to mean. Well if my gradients really small and consistently really small this will be a small number. If my gradient is highly volatile it's going to be a big number or if it's just really big all the time it'll be a big number. And why is that interesting. Because when we do our update this time we say weight minus learning rate times gradient divided by the square root of this. In other words if our gradients consistently very small and not volatile let's take bigger jumps. And that's kind of what we want right. When we watched how the intercept moves so damn slowly. But it's like obviously you need to just try to go faster. So if I now run this. After just five epochs this is already up to three. Right. Where else with the basic version after five epochs. It's still at one point two seven. And remember we have to get to 30. So the obvious thing to do and by obvious I mean only a couple of years ago did anybody actually figure this out is do both. Right. So that's called Adam. So Adam is simply keep track of the exponentially weighted moving average of the gradient squared. And also keep track of the exponentially weighted moving average of my steps. Right. And both divide by the exponentially weighted moving average of the squared terms and you know take point nine of a step in the same direction as last time. So it's it's momentum and I miss prop. That's called Adam. And look at this. OK. Five steps for 25. OK. So you know these these are these optimizers people call them dynamic learning rates. A lot of people have the misunderstanding that you don't have to set a learning rate. Of course you do. Right. It's just like trying to identify parameters that need to move faster you know or consistently go in the same direction. It doesn't mean you don't need learning rates. We still have a learning rate. OK. And in fact you know if I run this again currently my my error. Now just do it again. So we're trying to get to 30 comma two. So if I run it again. It's getting better. But eventually. Now it's just moving around the same place. Right. So you can see what's happened is the learning rates too high. So we could just go in here and drop it down. And run it some more. Getting pretty close now. Right. So you can see how you still need learning rate and dealing even with Adam. OK. So that spreadsheets fun to play around with. I do have a Google Sheets version of basic S.G.D. That actually works and the macros work and everything. Google Sheets is so awful and I went so insane making that work I gave up on making the other ones work. So I'll share a link to the Google Sheets version. Oh my God. They do have a macro language but it's just ridiculous. So anyway if somebody feels like fighting it to actually get all the other ones to work they will work. It's just it's just annoying. So maybe somebody can get this working on Google Sheets too. OK. So that's weight decay and Adam and Adam is amazingly fast. And we let's go back to this one but we don't tend to use opt in dot whatever and create the optimizer ourselves and all that stuff because instead we tend to use learner. But learn is just doing those things for you. But again there's no magic. So if you create a learner you say here's my data bunch. Here's my pie torch and end up module instance. Here's my loss function. And here are my metrics. Remember the metrics are just stuff to print out. That's it. Right. Then you just get a few nice things like learned out LR finds starts working and it starts recording this and you can say fit one cycle instead of just fit. But like these things really help a lot. Like by using the floating rate finder I found a good learning rate. And then like look at this. My loss here point one three here I wasn't getting much beneath point five. So these these tweaks make huge differences not tiny differences. And this is still just one one epoch. Now what is fit one cycle do. What does it really do. This is what it really does. Right. And we've seen this chart on the left before. Just to remind you this is plotting the learning rate per batch. I remember Adam has a learning rate and we use Adam by default or minor variation which we might try to talk about. So the learning rate starts really low and it increases about half the time and then it decreases about half the time because at the very start. We don't know where we are. Right. So we're in some part of function space that's just bumpy as all hell. So if you start jumping around those bumps have big gradients and it will throw you into crazy parts of the space. Right. So start slow and then you'll gradually move into parts of the weight space that you know they're kind of sensible. And as you get to the point where they're sensible you can increase the learning rate. You know because the the gradients are actually in the direction you want to go. And then as we've discussed a few times as you get close to the final answer you need to anneal your learning rate to hone in on it. But here's the interesting thing on the left is the momentum plot. And actually every time our learning rate is small our momentum is high. Why is that because if you do have a learning small learning rate but you keep going in the same direction you may as well go faster. Right. But if you're jumping really far don't like jump jump really far because it's going to throw you off. Right. And then as you get to the end again you're fine tuning in. But actually if you keep going in the same direction again and again go faster. So this combination is called one cycle and it's just this amazing like it's a simple thing but it's astonishing. Like this can help you get what's called super convergence that can let you train 10 times faster. Now this is just last year's paper and some of you may have seen the interview with Leslie Smith that I did last week. Amazing guy incredibly humble and also I should say somebody who is doing groundbreaking research well into his 60s and all of these things are inspiring. I'll show you something else interesting when you plot the losses with fast AI it doesn't look like that. It looks like that. Why is that because fast AI calculates the exponentially weighted moving average of the losses for you. All right. So this this concept of exponentially weighted stuff it's just really handy and I use it all the time. And one of the things that is to make it easier to read these charts. OK. It does mean that these charts from fast AI might be kind of an epoch or two sorry a batch or two behind where they should be. You know there's that slight downside when you use an exponentially weighted moving average is you've got a little bit of history in there as well. But it can make it much easier to see what's going on. So we're now at a point coming to the end of this co-label tabular section where we're going to try to understand all of the code in our tabular model. So remember the tabular model uses data set called adult which is trying to predict who's going to make more money. It's a classification problem. And we've got a number of categorical variables and a number of continuous variables. So the first thing we realize is we actually don't know how to predict a categorical variable yet because so far we did some hand waving around the fact that our loss function was an end up cross entropy loss. What is that. Let's find out. And of course we're going to find out by looking at Microsoft Excel. So cross entropy loss is just another loss function. We already know one loss function which is mean squared error y hat minus y squared. OK. So that's not a good loss function for us because in our case we have like for MNIST 10 possible digits and we have 10 activations each with a probability of that digit. So we need something where predicting the right thing correctly and confidently should have very little loss. Predicting the wrong thing confidently should have a lot of loss. So that's what we want. OK. So here's an example. Here is a cat versus dog one hot encoded. OK. And here are my two activations for each one from some model that I built. Probability cat probability dog. This one's not very confident of anything. This one's very confident of it being a cat and it's right. This one's very confident of being a cat and it's wrong. So we want a loss that for this one should be a moderate loss because not predicting anything confidently is not really what we want. So here's a point three. This thing's predicting the correct thing very confidently. So point one. This thing's predicting the wrong thing very confidently. So what. So how do we do that. This is the cross entropy loss and it is equal to. Whether it's a cat. Multiplied by log of the probability of cat. This is actually an activation. So I should say so it's multiplied by the log of the cat activation. Negative that minus is it a dog times the log of the dog activation. And that's it. OK. So in other words it's the sum of all of your one hot encoded variables times all of your activations. So interestingly these ones here are exactly the same numbers as these ones here but I've written it differently. I've written it with an F function because it's exactly this quiz because the zeros don't actually add anything. Right. So actually it's exactly the same as saying if it's a cat then take the log of cat in us. And if it's a dog. So otherwise take the log of one minus cat in us. In other words the log of dog in us. So the sum of the one hot encoded times the activations is the same as an F function. Which if you think about it is actually because this is just a matrix multiply. This is we now know from our from our embedding discussion that's the same as an index lookup. So you can also to do cross entropy you can also just look up the log of the activation for the correct answer. Now that's only going to work if these rows add up to one. And this is one reason that you can get screwy cross entropy numbers is that's why I said you press the wrong button. If they don't add up to one you've got a trouble. So how do you make sure that they add up to one. You make sure they add up to one by using the correct activation function in your last layer. And the correct activation function to use for this is softmax. Softmax is an activation function where all of the activations add up to one. All of the activations are greater than zero and all of the activations are less than one. So that's what we want. Right. That's what we need. How do you do that. Well let's say we were predicting one of five things cat dog plane fish building. And these were the numbers that came out of our neural net for one set of predictions. Well what if I did e to the power of that. So that's one step in the right direction because e to the power of something is always bigger than zero. So there's a bunch of numbers that are always bigger than zero. Here's the sum of those numbers. Here is e to the number divided by the sum of e to the number. Now this number is always less than one. Right. Because all of the things were positive so you can't possibly have one of the pieces be bigger than 100 percent of its sum. OK. And all of those things must add up to one. Right. Because each one of them was just that percentage of the total. So that's it. So this thing softmax is equal to e to the activation divided by the sum of e to the activations. That's called softmax. And so when we're doing single label multi class classification you generally want softmax as your activation function and you generally want cross entropy as your loss. Because these things go together in such friendly ways. PyTorch will do them both for you. Right. So you might have noticed that in this MNIST example I never added a softmax here. And that's because if you ask for cross entropy loss it actually does the softmax in inside the loss function. So it's not really just cross entropy loss. It's actually softmax then cross entropy loss. So you probably noticed this but sometimes your predictions from your models will come out looking more like this. Pretty big numbers with negatives in rather than this. Numbers between 0 and 1 that add up to 1. The reason would be that PyTorch it's a PyTorch model that doesn't have a softmax in because we're using cross entropy loss. And so you might have to do the softmax for it. Fast AI is getting increasingly good at knowing when this is happening. Generally if you're using a loss function that we recognize when you get the predictions we will try to add the softmax in there for you. But particularly if you're using a custom loss function that you know might call an n.cross entropy loss behind the scenes or something like that. You might find yourself with this situation. We only have three minutes left but I'm going to point something out to you which is that next week when we finish off tabular which we'll do in like the first 10 minutes. This is forward in tabular and it basically goes through a bunch of embeddings. It's going to call each one of those embeddings E and you can use it like a function of course. It's going to pass in each categorical variable to each embedding. It's going to concatenate them together into a single matrix. It's going to then call a bunch of layers which are basically a bunch of linear layers. And then it's going to do our sigmoid trick. And then there's only two new things we need to learn. One is dropout and the other is the n.cont batch norm. And these are two additional regularization strategies. There are basically batch norm does more than just regularization but amongst other things it does regularization. And the basic ways you regularize your model are weight decay, batch norm and dropout. And then you can also avoid overfitting using something called data augmentation. So batch norm and dropout we're going to touch on at the start of next week. And we're also going to look at data augmentation and then we're also going to look at what convolutions are. And we're going to learn some new computer vision architectures and some new computer vision applications. But basically we're very nearly there. You already know how the entirety of collab.py, fastai.collab works. You know why it's there and what it does. And you're very close to knowing what the entirety of tabular model does. And this tabular model is actually the one that if you run it on Rossman you'll get the same answer that I showed you in that paper. You'll get that second place result. Even a little bit better. I'll show you next week if I remember how I actually ran some additional experiments where I figured out some minor tweaks that can do even slightly better than that. So yeah, we'll see you next week. Thanks very much and enjoy the smoke outside.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.0, "text": " Welcome everybody to lesson five.", "tokens": [4027, 2201, 281, 6898, 1732, 13], "temperature": 0.0, "avg_logprob": -0.2840674591064453, "compression_ratio": 1.3666666666666667, "no_speech_prob": 0.010159318335354328}, {"id": 1, "seek": 0, "start": 4.0, "end": 12.0, "text": " And so we have officially peaked and everything is downhill from here.", "tokens": [400, 370, 321, 362, 12053, 520, 7301, 293, 1203, 307, 29929, 490, 510, 13], "temperature": 0.0, "avg_logprob": -0.2840674591064453, "compression_ratio": 1.3666666666666667, "no_speech_prob": 0.010159318335354328}, {"id": 2, "seek": 0, "start": 12.0, "end": 16.0, "text": " As of halfway through the last lesson.", "tokens": [1018, 295, 15461, 807, 264, 1036, 6898, 13], "temperature": 0.0, "avg_logprob": -0.2840674591064453, "compression_ratio": 1.3666666666666667, "no_speech_prob": 0.010159318335354328}, {"id": 3, "seek": 0, "start": 16.0, "end": 25.0, "text": " We started with computer vision because it's the most mature,", "tokens": [492, 1409, 365, 3820, 5201, 570, 309, 311, 264, 881, 14442, 11], "temperature": 0.0, "avg_logprob": -0.2840674591064453, "compression_ratio": 1.3666666666666667, "no_speech_prob": 0.010159318335354328}, {"id": 4, "seek": 2500, "start": 25.0, "end": 30.0, "text": " kind of out of the box, ready to use deep learning application.", "tokens": [733, 295, 484, 295, 264, 2424, 11, 1919, 281, 764, 2452, 2539, 3861, 13], "temperature": 0.0, "avg_logprob": -0.1111593246459961, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.00039681370253674686}, {"id": 5, "seek": 2500, "start": 30.0, "end": 35.0, "text": " It's something which if you're not using deep learning, you won't be getting good results.", "tokens": [467, 311, 746, 597, 498, 291, 434, 406, 1228, 2452, 2539, 11, 291, 1582, 380, 312, 1242, 665, 3542, 13], "temperature": 0.0, "avg_logprob": -0.1111593246459961, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.00039681370253674686}, {"id": 6, "seek": 2500, "start": 35.0, "end": 39.0, "text": " So the difference, you know, hopefully between not doing lesson one versus doing lesson one,", "tokens": [407, 264, 2649, 11, 291, 458, 11, 4696, 1296, 406, 884, 6898, 472, 5717, 884, 6898, 472, 11], "temperature": 0.0, "avg_logprob": -0.1111593246459961, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.00039681370253674686}, {"id": 7, "seek": 2500, "start": 39.0, "end": 43.0, "text": " you've gained a new capability you didn't have before.", "tokens": [291, 600, 12634, 257, 777, 13759, 291, 994, 380, 362, 949, 13], "temperature": 0.0, "avg_logprob": -0.1111593246459961, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.00039681370253674686}, {"id": 8, "seek": 2500, "start": 43.0, "end": 52.0, "text": " And you kind of get to see a lot of the kind of tradecraft of training and effective neural net.", "tokens": [400, 291, 733, 295, 483, 281, 536, 257, 688, 295, 264, 733, 295, 4923, 5611, 295, 3097, 293, 4942, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.1111593246459961, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.00039681370253674686}, {"id": 9, "seek": 5200, "start": 52.0, "end": 59.0, "text": " And so then we moved into NLP because text is kind of another one,", "tokens": [400, 370, 550, 321, 4259, 666, 426, 45196, 570, 2487, 307, 733, 295, 1071, 472, 11], "temperature": 0.0, "avg_logprob": -0.0829298219015432, "compression_ratio": 1.5874439461883407, "no_speech_prob": 9.156014129985124e-05}, {"id": 10, "seek": 5200, "start": 59.0, "end": 65.0, "text": " which you really kind of can't do really well without deep learning, generally speaking.", "tokens": [597, 291, 534, 733, 295, 393, 380, 360, 534, 731, 1553, 2452, 2539, 11, 5101, 4124, 13], "temperature": 0.0, "avg_logprob": -0.0829298219015432, "compression_ratio": 1.5874439461883407, "no_speech_prob": 9.156014129985124e-05}, {"id": 11, "seek": 5200, "start": 65.0, "end": 70.0, "text": " And it's just got to the point where it's pretty, you know, works pretty well now.", "tokens": [400, 309, 311, 445, 658, 281, 264, 935, 689, 309, 311, 1238, 11, 291, 458, 11, 1985, 1238, 731, 586, 13], "temperature": 0.0, "avg_logprob": -0.0829298219015432, "compression_ratio": 1.5874439461883407, "no_speech_prob": 9.156014129985124e-05}, {"id": 12, "seek": 5200, "start": 70.0, "end": 77.0, "text": " In fact, the New York Times just featured an article about the latest advances in deep learning for text yesterday.", "tokens": [682, 1186, 11, 264, 1873, 3609, 11366, 445, 13822, 364, 7222, 466, 264, 6792, 25297, 294, 2452, 2539, 337, 2487, 5186, 13], "temperature": 0.0, "avg_logprob": -0.0829298219015432, "compression_ratio": 1.5874439461883407, "no_speech_prob": 9.156014129985124e-05}, {"id": 13, "seek": 7700, "start": 77.0, "end": 85.0, "text": " And talked quite a lot about the work that we've done in that area along with OpenAI and Google", "tokens": [400, 2825, 1596, 257, 688, 466, 264, 589, 300, 321, 600, 1096, 294, 300, 1859, 2051, 365, 7238, 48698, 293, 3329], "temperature": 0.0, "avg_logprob": -0.09096038991754705, "compression_ratio": 1.467065868263473, "no_speech_prob": 1.4963283319957554e-05}, {"id": 14, "seek": 7700, "start": 85.0, "end": 89.0, "text": " and the Allen Institute of Artificial Intelligence.", "tokens": [293, 264, 17160, 9446, 295, 5735, 10371, 27274, 13], "temperature": 0.0, "avg_logprob": -0.09096038991754705, "compression_ratio": 1.467065868263473, "no_speech_prob": 1.4963283319957554e-05}, {"id": 15, "seek": 7700, "start": 89.0, "end": 100.0, "text": " And then we've kind of finished our application journey with tabular and collaborative filtering,", "tokens": [400, 550, 321, 600, 733, 295, 4335, 527, 3861, 4671, 365, 4421, 1040, 293, 16555, 30822, 11], "temperature": 0.0, "avg_logprob": -0.09096038991754705, "compression_ratio": 1.467065868263473, "no_speech_prob": 1.4963283319957554e-05}, {"id": 16, "seek": 10000, "start": 100.0, "end": 107.0, "text": " partly because tabular and collaborative filtering are things that you can still do pretty well without deep learning.", "tokens": [17031, 570, 4421, 1040, 293, 16555, 30822, 366, 721, 300, 291, 393, 920, 360, 1238, 731, 1553, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.05379032153709262, "compression_ratio": 1.7883817427385893, "no_speech_prob": 1.012776920106262e-05}, {"id": 17, "seek": 10000, "start": 107.0, "end": 112.0, "text": " So it's not such a big step. It's not a kind of whole new thing that you could do that you couldn't used to do.", "tokens": [407, 309, 311, 406, 1270, 257, 955, 1823, 13, 467, 311, 406, 257, 733, 295, 1379, 777, 551, 300, 291, 727, 360, 300, 291, 2809, 380, 1143, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.05379032153709262, "compression_ratio": 1.7883817427385893, "no_speech_prob": 1.012776920106262e-05}, {"id": 18, "seek": 10000, "start": 112.0, "end": 120.0, "text": " And also because the, you know, we're going to try to get to a point where we understand", "tokens": [400, 611, 570, 264, 11, 291, 458, 11, 321, 434, 516, 281, 853, 281, 483, 281, 257, 935, 689, 321, 1223], "temperature": 0.0, "avg_logprob": -0.05379032153709262, "compression_ratio": 1.7883817427385893, "no_speech_prob": 1.012776920106262e-05}, {"id": 19, "seek": 10000, "start": 120.0, "end": 125.0, "text": " pretty much every line of code and the implementations of these things and the implementations of those things.", "tokens": [1238, 709, 633, 1622, 295, 3089, 293, 264, 4445, 763, 295, 613, 721, 293, 264, 4445, 763, 295, 729, 721, 13], "temperature": 0.0, "avg_logprob": -0.05379032153709262, "compression_ratio": 1.7883817427385893, "no_speech_prob": 1.012776920106262e-05}, {"id": 20, "seek": 12500, "start": 125.0, "end": 130.0, "text": " It's much less intricate than vision and NLP.", "tokens": [467, 311, 709, 1570, 38015, 813, 5201, 293, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.06396264259261314, "compression_ratio": 1.5573770491803278, "no_speech_prob": 7.029604603303596e-05}, {"id": 21, "seek": 12500, "start": 130.0, "end": 137.0, "text": " So as we come down this other side of the journey, which is like all the stuff we've just done, how does it actually work?", "tokens": [407, 382, 321, 808, 760, 341, 661, 1252, 295, 264, 4671, 11, 597, 307, 411, 439, 264, 1507, 321, 600, 445, 1096, 11, 577, 775, 309, 767, 589, 30], "temperature": 0.0, "avg_logprob": -0.06396264259261314, "compression_ratio": 1.5573770491803278, "no_speech_prob": 7.029604603303596e-05}, {"id": 22, "seek": 12500, "start": 137.0, "end": 143.0, "text": " By starting where we just ended, which is starting with collaborative filtering and then tabular data,", "tokens": [3146, 2891, 689, 321, 445, 4590, 11, 597, 307, 2891, 365, 16555, 30822, 293, 550, 4421, 1040, 1412, 11], "temperature": 0.0, "avg_logprob": -0.06396264259261314, "compression_ratio": 1.5573770491803278, "no_speech_prob": 7.029604603303596e-05}, {"id": 23, "seek": 12500, "start": 143.0, "end": 149.0, "text": " we're going to be able to see what all those lines of code do by the end of today's lesson.", "tokens": [321, 434, 516, 281, 312, 1075, 281, 536, 437, 439, 729, 3876, 295, 3089, 360, 538, 264, 917, 295, 965, 311, 6898, 13], "temperature": 0.0, "avg_logprob": -0.06396264259261314, "compression_ratio": 1.5573770491803278, "no_speech_prob": 7.029604603303596e-05}, {"id": 24, "seek": 12500, "start": 149.0, "end": 151.0, "text": " That's our goal.", "tokens": [663, 311, 527, 3387, 13], "temperature": 0.0, "avg_logprob": -0.06396264259261314, "compression_ratio": 1.5573770491803278, "no_speech_prob": 7.029604603303596e-05}, {"id": 25, "seek": 15100, "start": 151.0, "end": 160.0, "text": " So particularly this lesson, you should not expect to come away knowing how to solve, you know,", "tokens": [407, 4098, 341, 6898, 11, 291, 820, 406, 2066, 281, 808, 1314, 5276, 577, 281, 5039, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.05820903778076172, "compression_ratio": 1.7347826086956522, "no_speech_prob": 3.822136204689741e-05}, {"id": 26, "seek": 15100, "start": 160.0, "end": 162.0, "text": " how to do applications you couldn't do before.", "tokens": [577, 281, 360, 5821, 291, 2809, 380, 360, 949, 13], "temperature": 0.0, "avg_logprob": -0.05820903778076172, "compression_ratio": 1.7347826086956522, "no_speech_prob": 3.822136204689741e-05}, {"id": 27, "seek": 15100, "start": 162.0, "end": 169.0, "text": " But instead, you should have a better understanding of how we've actually been solving the applications we've seen so far.", "tokens": [583, 2602, 11, 291, 820, 362, 257, 1101, 3701, 295, 577, 321, 600, 767, 668, 12606, 264, 5821, 321, 600, 1612, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.05820903778076172, "compression_ratio": 1.7347826086956522, "no_speech_prob": 3.822136204689741e-05}, {"id": 28, "seek": 15100, "start": 169.0, "end": 177.0, "text": " Particularly, we're going to understand a lot more about regularization, which is how we go about managing over versus under fitting.", "tokens": [32281, 11, 321, 434, 516, 281, 1223, 257, 688, 544, 466, 3890, 2144, 11, 597, 307, 577, 321, 352, 466, 11642, 670, 5717, 833, 15669, 13], "temperature": 0.0, "avg_logprob": -0.05820903778076172, "compression_ratio": 1.7347826086956522, "no_speech_prob": 3.822136204689741e-05}, {"id": 29, "seek": 17700, "start": 177.0, "end": 182.0, "text": " And so hopefully you can use some of the tools from this lesson to go back to your previous projects", "tokens": [400, 370, 4696, 291, 393, 764, 512, 295, 264, 3873, 490, 341, 6898, 281, 352, 646, 281, 428, 3894, 4455], "temperature": 0.0, "avg_logprob": -0.04763105450844278, "compression_ratio": 1.7176470588235293, "no_speech_prob": 3.119625762337819e-05}, {"id": 30, "seek": 17700, "start": 182.0, "end": 189.0, "text": " and get a little bit more performance or handle models where previously maybe you felt like your data was not enough", "tokens": [293, 483, 257, 707, 857, 544, 3389, 420, 4813, 5245, 689, 8046, 1310, 291, 2762, 411, 428, 1412, 390, 406, 1547], "temperature": 0.0, "avg_logprob": -0.04763105450844278, "compression_ratio": 1.7176470588235293, "no_speech_prob": 3.119625762337819e-05}, {"id": 31, "seek": 17700, "start": 189.0, "end": 193.0, "text": " or maybe you were under fitting and so forth.", "tokens": [420, 1310, 291, 645, 833, 15669, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.04763105450844278, "compression_ratio": 1.7176470588235293, "no_speech_prob": 3.119625762337819e-05}, {"id": 32, "seek": 17700, "start": 193.0, "end": 198.0, "text": " And it's also going to lay the groundwork for understanding convolutional neural networks", "tokens": [400, 309, 311, 611, 516, 281, 2360, 264, 2727, 1902, 337, 3701, 45216, 304, 18161, 9590], "temperature": 0.0, "avg_logprob": -0.04763105450844278, "compression_ratio": 1.7176470588235293, "no_speech_prob": 3.119625762337819e-05}, {"id": 33, "seek": 17700, "start": 198.0, "end": 202.0, "text": " and recurrent neural networks that we'll do deep dives into in the next two lessons.", "tokens": [293, 18680, 1753, 18161, 9590, 300, 321, 603, 360, 2452, 274, 1539, 666, 294, 264, 958, 732, 8820, 13], "temperature": 0.0, "avg_logprob": -0.04763105450844278, "compression_ratio": 1.7176470588235293, "no_speech_prob": 3.119625762337819e-05}, {"id": 34, "seek": 20200, "start": 202.0, "end": 213.0, "text": " And as we do that, we're also going to look at some new applications, some new vision and NLP applications.", "tokens": [400, 382, 321, 360, 300, 11, 321, 434, 611, 516, 281, 574, 412, 512, 777, 5821, 11, 512, 777, 5201, 293, 426, 45196, 5821, 13], "temperature": 0.0, "avg_logprob": -0.05975100458884726, "compression_ratio": 1.3383458646616542, "no_speech_prob": 2.1441919670905918e-05}, {"id": 35, "seek": 20200, "start": 213.0, "end": 220.0, "text": " Let's start where we left off last week.", "tokens": [961, 311, 722, 689, 321, 1411, 766, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.05975100458884726, "compression_ratio": 1.3383458646616542, "no_speech_prob": 2.1441919670905918e-05}, {"id": 36, "seek": 20200, "start": 220.0, "end": 225.0, "text": " Do you remember this picture?", "tokens": [1144, 291, 1604, 341, 3036, 30], "temperature": 0.0, "avg_logprob": -0.05975100458884726, "compression_ratio": 1.3383458646616542, "no_speech_prob": 2.1441919670905918e-05}, {"id": 37, "seek": 22500, "start": 225.0, "end": 234.0, "text": " So this picture, we were looking at kind of what does a deep neural net look like?", "tokens": [407, 341, 3036, 11, 321, 645, 1237, 412, 733, 295, 437, 775, 257, 2452, 18161, 2533, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.07723626723656288, "compression_ratio": 1.4014084507042253, "no_speech_prob": 3.3722117223078385e-05}, {"id": 38, "seek": 22500, "start": 234.0, "end": 239.0, "text": " And we had various layers.", "tokens": [400, 321, 632, 3683, 7914, 13], "temperature": 0.0, "avg_logprob": -0.07723626723656288, "compression_ratio": 1.4014084507042253, "no_speech_prob": 3.3722117223078385e-05}, {"id": 39, "seek": 22500, "start": 239.0, "end": 248.0, "text": " And the first thing we pointed out is that there are only and exactly two types of layer.", "tokens": [400, 264, 700, 551, 321, 10932, 484, 307, 300, 456, 366, 787, 293, 2293, 732, 3467, 295, 4583, 13], "temperature": 0.0, "avg_logprob": -0.07723626723656288, "compression_ratio": 1.4014084507042253, "no_speech_prob": 3.3722117223078385e-05}, {"id": 40, "seek": 24800, "start": 248.0, "end": 256.0, "text": " There are layers that contain parameters and there are layers that contain activations.", "tokens": [821, 366, 7914, 300, 5304, 9834, 293, 456, 366, 7914, 300, 5304, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.06012365994630037, "compression_ratio": 1.8489208633093526, "no_speech_prob": 8.663866537972353e-06}, {"id": 41, "seek": 24800, "start": 256.0, "end": 261.0, "text": " Parameters are the things that your model learns.", "tokens": [34882, 6202, 366, 264, 721, 300, 428, 2316, 27152, 13], "temperature": 0.0, "avg_logprob": -0.06012365994630037, "compression_ratio": 1.8489208633093526, "no_speech_prob": 8.663866537972353e-06}, {"id": 42, "seek": 24800, "start": 261.0, "end": 273.0, "text": " They're the things that you use gradient descent to go parameters minus equals learning rate times parameters dot grad.", "tokens": [814, 434, 264, 721, 300, 291, 764, 16235, 23475, 281, 352, 9834, 3175, 6915, 2539, 3314, 1413, 9834, 5893, 2771, 13], "temperature": 0.0, "avg_logprob": -0.06012365994630037, "compression_ratio": 1.8489208633093526, "no_speech_prob": 8.663866537972353e-06}, {"id": 43, "seek": 27300, "start": 273.0, "end": 278.0, "text": " That's our basic, that's what we do.", "tokens": [663, 311, 527, 3875, 11, 300, 311, 437, 321, 360, 13], "temperature": 0.0, "avg_logprob": -0.08926667665180407, "compression_ratio": 1.4506172839506173, "no_speech_prob": 9.972365660360083e-06}, {"id": 44, "seek": 27300, "start": 278.0, "end": 288.0, "text": " And those parameters are used by multiplying them by input activations doing a matrix product.", "tokens": [400, 729, 9834, 366, 1143, 538, 30955, 552, 538, 4846, 2430, 763, 884, 257, 8141, 1674, 13], "temperature": 0.0, "avg_logprob": -0.08926667665180407, "compression_ratio": 1.4506172839506173, "no_speech_prob": 9.972365660360083e-06}, {"id": 45, "seek": 27300, "start": 288.0, "end": 296.0, "text": " So the yellow things are our weight matrices or weight tensors more generally, but that's close enough.", "tokens": [407, 264, 5566, 721, 366, 527, 3364, 32284, 420, 3364, 10688, 830, 544, 5101, 11, 457, 300, 311, 1998, 1547, 13], "temperature": 0.0, "avg_logprob": -0.08926667665180407, "compression_ratio": 1.4506172839506173, "no_speech_prob": 9.972365660360083e-06}, {"id": 46, "seek": 29600, "start": 296.0, "end": 305.0, "text": " So we take some input activations or some layer activations and we multiply it by a weight matrix to get a bunch of activations.", "tokens": [407, 321, 747, 512, 4846, 2430, 763, 420, 512, 4583, 2430, 763, 293, 321, 12972, 309, 538, 257, 3364, 8141, 281, 483, 257, 3840, 295, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.08364547762954444, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.3630333342007361e-05}, {"id": 47, "seek": 29600, "start": 305.0, "end": 311.0, "text": " So activations are numbers, but these are numbers that are calculated.", "tokens": [407, 2430, 763, 366, 3547, 11, 457, 613, 366, 3547, 300, 366, 15598, 13], "temperature": 0.0, "avg_logprob": -0.08364547762954444, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.3630333342007361e-05}, {"id": 48, "seek": 29600, "start": 311.0, "end": 317.0, "text": " So I find in our study group, I keep getting questions about where does that number come from?", "tokens": [407, 286, 915, 294, 527, 2979, 1594, 11, 286, 1066, 1242, 1651, 466, 689, 775, 300, 1230, 808, 490, 30], "temperature": 0.0, "avg_logprob": -0.08364547762954444, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.3630333342007361e-05}, {"id": 49, "seek": 29600, "start": 317.0, "end": 322.0, "text": " And I always answer it in the same way. You tell me, is it a parameter or is it an activation?", "tokens": [400, 286, 1009, 1867, 309, 294, 264, 912, 636, 13, 509, 980, 385, 11, 307, 309, 257, 13075, 420, 307, 309, 364, 24433, 30], "temperature": 0.0, "avg_logprob": -0.08364547762954444, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.3630333342007361e-05}, {"id": 50, "seek": 29600, "start": 322.0, "end": 325.0, "text": " Because it's one of those two things. That's where numbers come from.", "tokens": [1436, 309, 311, 472, 295, 729, 732, 721, 13, 663, 311, 689, 3547, 808, 490, 13], "temperature": 0.0, "avg_logprob": -0.08364547762954444, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.3630333342007361e-05}, {"id": 51, "seek": 32500, "start": 325.0, "end": 332.0, "text": " I guess inputs are kind of a special activation. So they're not calculated. They're just there.", "tokens": [286, 2041, 15743, 366, 733, 295, 257, 2121, 24433, 13, 407, 436, 434, 406, 15598, 13, 814, 434, 445, 456, 13], "temperature": 0.0, "avg_logprob": -0.06821442686993143, "compression_ratio": 1.872037914691943, "no_speech_prob": 2.710613262024708e-05}, {"id": 52, "seek": 32500, "start": 332.0, "end": 339.0, "text": " So maybe that's a special case. So maybe it's an input or a parameter or an activation.", "tokens": [407, 1310, 300, 311, 257, 2121, 1389, 13, 407, 1310, 309, 311, 364, 4846, 420, 257, 13075, 420, 364, 24433, 13], "temperature": 0.0, "avg_logprob": -0.06821442686993143, "compression_ratio": 1.872037914691943, "no_speech_prob": 2.710613262024708e-05}, {"id": 53, "seek": 32500, "start": 339.0, "end": 345.0, "text": " Activations don't only come out of matrix multiplications. They also come out of activation functions.", "tokens": [28550, 763, 500, 380, 787, 808, 484, 295, 8141, 17596, 763, 13, 814, 611, 808, 484, 295, 24433, 6828, 13], "temperature": 0.0, "avg_logprob": -0.06821442686993143, "compression_ratio": 1.872037914691943, "no_speech_prob": 2.710613262024708e-05}, {"id": 54, "seek": 32500, "start": 345.0, "end": 351.0, "text": " And the most important thing to remember about an activation function is that it's an element wise function.", "tokens": [400, 264, 881, 1021, 551, 281, 1604, 466, 364, 24433, 2445, 307, 300, 309, 311, 364, 4478, 10829, 2445, 13], "temperature": 0.0, "avg_logprob": -0.06821442686993143, "compression_ratio": 1.872037914691943, "no_speech_prob": 2.710613262024708e-05}, {"id": 55, "seek": 35100, "start": 351.0, "end": 359.0, "text": " So it's a function that is applied to each element of the input activations in turn and creates one activation for each input element.", "tokens": [407, 309, 311, 257, 2445, 300, 307, 6456, 281, 1184, 4478, 295, 264, 4846, 2430, 763, 294, 1261, 293, 7829, 472, 24433, 337, 1184, 4846, 4478, 13], "temperature": 0.0, "avg_logprob": -0.07756672965155707, "compression_ratio": 1.7391304347826086, "no_speech_prob": 1.6963129382929765e-05}, {"id": 56, "seek": 35100, "start": 359.0, "end": 369.0, "text": " So if it starts with a 20 long vector, it creates a 20 long vector by looking at each one of those in turn, doing one thing to it and spitting out the answer.", "tokens": [407, 498, 309, 3719, 365, 257, 945, 938, 8062, 11, 309, 7829, 257, 945, 938, 8062, 538, 1237, 412, 1184, 472, 295, 729, 294, 1261, 11, 884, 472, 551, 281, 309, 293, 637, 2414, 484, 264, 1867, 13], "temperature": 0.0, "avg_logprob": -0.07756672965155707, "compression_ratio": 1.7391304347826086, "no_speech_prob": 1.6963129382929765e-05}, {"id": 57, "seek": 35100, "start": 369.0, "end": 375.0, "text": " So an element wise function. Relu is the main one we've looked at.", "tokens": [407, 364, 4478, 10829, 2445, 13, 8738, 84, 307, 264, 2135, 472, 321, 600, 2956, 412, 13], "temperature": 0.0, "avg_logprob": -0.07756672965155707, "compression_ratio": 1.7391304347826086, "no_speech_prob": 1.6963129382929765e-05}, {"id": 58, "seek": 37500, "start": 375.0, "end": 387.0, "text": " And honestly, it doesn't too much matter which you pick. So we don't spend much time talking about activation functions because if you just use Relu, you'll get a pretty good answer pretty much all the time.", "tokens": [400, 6095, 11, 309, 1177, 380, 886, 709, 1871, 597, 291, 1888, 13, 407, 321, 500, 380, 3496, 709, 565, 1417, 466, 24433, 6828, 570, 498, 291, 445, 764, 8738, 84, 11, 291, 603, 483, 257, 1238, 665, 1867, 1238, 709, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.09004755373354312, "compression_ratio": 1.6008064516129032, "no_speech_prob": 1.669927041803021e-05}, {"id": 59, "seek": 37500, "start": 387.0, "end": 399.0, "text": " And so then we learned that this combination of matrix multiplications followed by Relu's stacked together has this amazing mathematical property called the universal approximation theorem,", "tokens": [400, 370, 550, 321, 3264, 300, 341, 6562, 295, 8141, 17596, 763, 6263, 538, 8738, 84, 311, 28867, 1214, 575, 341, 2243, 18894, 4707, 1219, 264, 11455, 28023, 20904, 11], "temperature": 0.0, "avg_logprob": -0.09004755373354312, "compression_ratio": 1.6008064516129032, "no_speech_prob": 1.669927041803021e-05}, {"id": 60, "seek": 39900, "start": 399.0, "end": 413.0, "text": " which is if you have big enough weight matrices and enough of them, it can solve any arbitrarily complex mathematical function to any arbitrarily high level of accuracy.", "tokens": [597, 307, 498, 291, 362, 955, 1547, 3364, 32284, 293, 1547, 295, 552, 11, 309, 393, 5039, 604, 19071, 3289, 3997, 18894, 2445, 281, 604, 19071, 3289, 1090, 1496, 295, 14170, 13], "temperature": 0.0, "avg_logprob": -0.03884933323695742, "compression_ratio": 1.5517241379310345, "no_speech_prob": 8.012979378690943e-06}, {"id": 61, "seek": 39900, "start": 413.0, "end": 423.0, "text": " Assuming that you can train the parameters both in terms of time and data availability and so forth.", "tokens": [6281, 24919, 300, 291, 393, 3847, 264, 9834, 1293, 294, 2115, 295, 565, 293, 1412, 17945, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.03884933323695742, "compression_ratio": 1.5517241379310345, "no_speech_prob": 8.012979378690943e-06}, {"id": 62, "seek": 42300, "start": 423.0, "end": 437.0, "text": " So that's the bit which I find particularly more advanced computer scientists get really confused about is they're always asking like, where's the next bit? What's the trick? How does it work?", "tokens": [407, 300, 311, 264, 857, 597, 286, 915, 4098, 544, 7339, 3820, 7708, 483, 534, 9019, 466, 307, 436, 434, 1009, 3365, 411, 11, 689, 311, 264, 958, 857, 30, 708, 311, 264, 4282, 30, 1012, 775, 309, 589, 30], "temperature": 0.0, "avg_logprob": -0.09308037004972759, "compression_ratio": 1.5116279069767442, "no_speech_prob": 3.426246621529572e-05}, {"id": 63, "seek": 42300, "start": 437.0, "end": 446.0, "text": " But that's it. You know, you just do those things and you pass back the gradients and you update the weights with the learning rate.", "tokens": [583, 300, 311, 309, 13, 509, 458, 11, 291, 445, 360, 729, 721, 293, 291, 1320, 646, 264, 2771, 2448, 293, 291, 5623, 264, 17443, 365, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.09308037004972759, "compression_ratio": 1.5116279069767442, "no_speech_prob": 3.426246621529572e-05}, {"id": 64, "seek": 44600, "start": 446.0, "end": 462.0, "text": " And that's it. So that piece where we take the loss function between the actual targets and the output of the final layer, so the final activations,", "tokens": [400, 300, 311, 309, 13, 407, 300, 2522, 689, 321, 747, 264, 4470, 2445, 1296, 264, 3539, 12911, 293, 264, 5598, 295, 264, 2572, 4583, 11, 370, 264, 2572, 2430, 763, 11], "temperature": 0.0, "avg_logprob": -0.08208165449254654, "compression_ratio": 1.6594594594594594, "no_speech_prob": 1.9523287846823223e-05}, {"id": 65, "seek": 44600, "start": 462.0, "end": 472.0, "text": " we calculate the gradients with respect to all of these yellow things. And then we update those yellow things by subtracting learning rate times the gradient.", "tokens": [321, 8873, 264, 2771, 2448, 365, 3104, 281, 439, 295, 613, 5566, 721, 13, 400, 550, 321, 5623, 729, 5566, 721, 538, 16390, 278, 2539, 3314, 1413, 264, 16235, 13], "temperature": 0.0, "avg_logprob": -0.08208165449254654, "compression_ratio": 1.6594594594594594, "no_speech_prob": 1.9523287846823223e-05}, {"id": 66, "seek": 47200, "start": 472.0, "end": 478.0, "text": " That process of calculating those gradients and then subtracting like that is called back propagation.", "tokens": [663, 1399, 295, 28258, 729, 2771, 2448, 293, 550, 16390, 278, 411, 300, 307, 1219, 646, 38377, 13], "temperature": 0.0, "avg_logprob": -0.09144540543251849, "compression_ratio": 1.4825174825174825, "no_speech_prob": 2.4681707145646214e-05}, {"id": 67, "seek": 47200, "start": 478.0, "end": 497.0, "text": " So when you hear the term back propagation, it's one of these terms that neural networking folks love to use.", "tokens": [407, 562, 291, 1568, 264, 1433, 646, 38377, 11, 309, 311, 472, 295, 613, 2115, 300, 18161, 17985, 4024, 959, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.09144540543251849, "compression_ratio": 1.4825174825174825, "no_speech_prob": 2.4681707145646214e-05}, {"id": 68, "seek": 49700, "start": 497.0, "end": 512.0, "text": " Sounds very impressive, OK, but you can replace it with your head with weights minus equals weights grad times learning rate or parameters, I should say, rather than weights, a bit more general.", "tokens": [14576, 588, 8992, 11, 2264, 11, 457, 291, 393, 7406, 309, 365, 428, 1378, 365, 17443, 3175, 6915, 17443, 2771, 1413, 2539, 3314, 420, 9834, 11, 286, 820, 584, 11, 2831, 813, 17443, 11, 257, 857, 544, 2674, 13], "temperature": 0.0, "avg_logprob": -0.15183558529370453, "compression_ratio": 1.5467980295566504, "no_speech_prob": 2.9307653676369227e-05}, {"id": 69, "seek": 49700, "start": 512.0, "end": 521.0, "text": " OK, so that's what we covered last week. And then I mentioned last week that we're going to cover a couple more things.", "tokens": [2264, 11, 370, 300, 311, 437, 321, 5343, 1036, 1243, 13, 400, 550, 286, 2835, 1036, 1243, 300, 321, 434, 516, 281, 2060, 257, 1916, 544, 721, 13], "temperature": 0.0, "avg_logprob": -0.15183558529370453, "compression_ratio": 1.5467980295566504, "no_speech_prob": 2.9307653676369227e-05}, {"id": 70, "seek": 52100, "start": 521.0, "end": 527.0, "text": " I'm going to come back to these ones, Cross-Entropy and Softmax later today. Let's talk about fine tuning now.", "tokens": [286, 478, 516, 281, 808, 646, 281, 613, 2306, 11, 11623, 12, 42837, 27514, 293, 16985, 41167, 1780, 965, 13, 961, 311, 751, 466, 2489, 15164, 586, 13], "temperature": 0.0, "avg_logprob": -0.10587890849393956, "compression_ratio": 1.4639639639639639, "no_speech_prob": 3.882489545503631e-05}, {"id": 71, "seek": 52100, "start": 527.0, "end": 536.0, "text": " So what happens when we take a ResNet 34 and we do transfer learning? What's actually going on?", "tokens": [407, 437, 2314, 562, 321, 747, 257, 5015, 31890, 12790, 293, 321, 360, 5003, 2539, 30, 708, 311, 767, 516, 322, 30], "temperature": 0.0, "avg_logprob": -0.10587890849393956, "compression_ratio": 1.4639639639639639, "no_speech_prob": 3.882489545503631e-05}, {"id": 72, "seek": 52100, "start": 536.0, "end": 546.0, "text": " So the first thing to notice is the ResNet 34 that we grab from ImageNet has a very specific weight matrix at the end.", "tokens": [407, 264, 700, 551, 281, 3449, 307, 264, 5015, 31890, 12790, 300, 321, 4444, 490, 29903, 31890, 575, 257, 588, 2685, 3364, 8141, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.10587890849393956, "compression_ratio": 1.4639639639639639, "no_speech_prob": 3.882489545503631e-05}, {"id": 73, "seek": 54600, "start": 546.0, "end": 555.0, "text": " It's a weight matrix that has 1000 columns. Why is that? Because ImageNet, the problem they ask you to solve in the ImageNet competition,", "tokens": [467, 311, 257, 3364, 8141, 300, 575, 9714, 13766, 13, 1545, 307, 300, 30, 1436, 29903, 31890, 11, 264, 1154, 436, 1029, 291, 281, 5039, 294, 264, 29903, 31890, 6211, 11], "temperature": 0.0, "avg_logprob": -0.08571665627615792, "compression_ratio": 1.5165876777251184, "no_speech_prob": 3.1198986107483506e-05}, {"id": 74, "seek": 54600, "start": 555.0, "end": 561.0, "text": " is please figure out which one of these 1000 image categories this picture is.", "tokens": [307, 1767, 2573, 484, 597, 472, 295, 613, 9714, 3256, 10479, 341, 3036, 307, 13], "temperature": 0.0, "avg_logprob": -0.08571665627615792, "compression_ratio": 1.5165876777251184, "no_speech_prob": 3.1198986107483506e-05}, {"id": 75, "seek": 54600, "start": 561.0, "end": 569.0, "text": " So that's why they need a thousand things here, because in ImageNet, this target vector is length 1000.", "tokens": [407, 300, 311, 983, 436, 643, 257, 4714, 721, 510, 11, 570, 294, 29903, 31890, 11, 341, 3779, 8062, 307, 4641, 9714, 13], "temperature": 0.0, "avg_logprob": -0.08571665627615792, "compression_ratio": 1.5165876777251184, "no_speech_prob": 3.1198986107483506e-05}, {"id": 76, "seek": 56900, "start": 569.0, "end": 576.0, "text": " So you've got to pick the probability that it's which one of those thousand things.", "tokens": [407, 291, 600, 658, 281, 1888, 264, 8482, 300, 309, 311, 597, 472, 295, 729, 4714, 721, 13], "temperature": 0.0, "avg_logprob": -0.07771006914285514, "compression_ratio": 1.7379032258064515, "no_speech_prob": 1.0615497558319476e-05}, {"id": 77, "seek": 56900, "start": 576.0, "end": 582.0, "text": " So there's a couple of reasons this weight matrix is no good to you when you're doing transfer learning.", "tokens": [407, 456, 311, 257, 1916, 295, 4112, 341, 3364, 8141, 307, 572, 665, 281, 291, 562, 291, 434, 884, 5003, 2539, 13], "temperature": 0.0, "avg_logprob": -0.07771006914285514, "compression_ratio": 1.7379032258064515, "no_speech_prob": 1.0615497558319476e-05}, {"id": 78, "seek": 56900, "start": 582.0, "end": 588.0, "text": " The first is that you probably don't have a thousand categories. You know, I was trying to do teddy bears, black bears or brown bears.", "tokens": [440, 700, 307, 300, 291, 1391, 500, 380, 362, 257, 4714, 10479, 13, 509, 458, 11, 286, 390, 1382, 281, 360, 45116, 17276, 11, 2211, 17276, 420, 6292, 17276, 13], "temperature": 0.0, "avg_logprob": -0.07771006914285514, "compression_ratio": 1.7379032258064515, "no_speech_prob": 1.0615497558319476e-05}, {"id": 79, "seek": 56900, "start": 588.0, "end": 593.0, "text": " So I don't want a thousand categories. And the second is, even if I did have exactly a thousand categories,", "tokens": [407, 286, 500, 380, 528, 257, 4714, 10479, 13, 400, 264, 1150, 307, 11, 754, 498, 286, 630, 362, 2293, 257, 4714, 10479, 11], "temperature": 0.0, "avg_logprob": -0.07771006914285514, "compression_ratio": 1.7379032258064515, "no_speech_prob": 1.0615497558319476e-05}, {"id": 80, "seek": 59300, "start": 593.0, "end": 602.0, "text": " I would still have the same thousand categories that are in ImageNet. So basically, this whole weight matrix is a waste of time for me.", "tokens": [286, 576, 920, 362, 264, 912, 4714, 10479, 300, 366, 294, 29903, 31890, 13, 407, 1936, 11, 341, 1379, 3364, 8141, 307, 257, 5964, 295, 565, 337, 385, 13], "temperature": 0.0, "avg_logprob": -0.1282296411452755, "compression_ratio": 1.3780487804878048, "no_speech_prob": 9.368192877445836e-06}, {"id": 81, "seek": 59300, "start": 602.0, "end": 609.0, "text": " So what do we do? We throw it away. So when you go create CNN in Fast.AI, it deletes that.", "tokens": [407, 437, 360, 321, 360, 30, 492, 3507, 309, 1314, 13, 407, 562, 291, 352, 1884, 24859, 294, 15968, 13, 48698, 11, 309, 1103, 37996, 300, 13], "temperature": 0.0, "avg_logprob": -0.1282296411452755, "compression_ratio": 1.3780487804878048, "no_speech_prob": 9.368192877445836e-06}, {"id": 82, "seek": 60900, "start": 609.0, "end": 623.0, "text": " So what does it do instead? Instead, it puts in two new weight matrices in there for you with a ReLU in between.", "tokens": [407, 437, 775, 309, 360, 2602, 30, 7156, 11, 309, 8137, 294, 732, 777, 3364, 32284, 294, 456, 337, 291, 365, 257, 1300, 43, 52, 294, 1296, 13], "temperature": 0.0, "avg_logprob": -0.15499608993530273, "compression_ratio": 1.3484848484848484, "no_speech_prob": 1.4061204637982883e-05}, {"id": 83, "seek": 60900, "start": 623.0, "end": 631.0, "text": " And so there are some defaults as to what size this first one is.", "tokens": [400, 370, 456, 366, 512, 7576, 82, 382, 281, 437, 2744, 341, 700, 472, 307, 13], "temperature": 0.0, "avg_logprob": -0.15499608993530273, "compression_ratio": 1.3484848484848484, "no_speech_prob": 1.4061204637982883e-05}, {"id": 84, "seek": 63100, "start": 631.0, "end": 640.0, "text": " But the second one, the size there is as big as you need it to be. So in your data bunch, which you pass to your learner,", "tokens": [583, 264, 1150, 472, 11, 264, 2744, 456, 307, 382, 955, 382, 291, 643, 309, 281, 312, 13, 407, 294, 428, 1412, 3840, 11, 597, 291, 1320, 281, 428, 33347, 11], "temperature": 0.0, "avg_logprob": -0.0838052561116773, "compression_ratio": 1.6829268292682926, "no_speech_prob": 3.822200233116746e-05}, {"id": 85, "seek": 63100, "start": 640.0, "end": 648.0, "text": " from that, we know how many activations you need. If you're doing classification, it's whether many classes you have.", "tokens": [490, 300, 11, 321, 458, 577, 867, 2430, 763, 291, 643, 13, 759, 291, 434, 884, 21538, 11, 309, 311, 1968, 867, 5359, 291, 362, 13], "temperature": 0.0, "avg_logprob": -0.0838052561116773, "compression_ratio": 1.6829268292682926, "no_speech_prob": 3.822200233116746e-05}, {"id": 86, "seek": 63100, "start": 648.0, "end": 653.0, "text": " If you're doing regression, it's however many numbers you're trying to predict in the regression problem.", "tokens": [759, 291, 434, 884, 24590, 11, 309, 311, 4461, 867, 3547, 291, 434, 1382, 281, 6069, 294, 264, 24590, 1154, 13], "temperature": 0.0, "avg_logprob": -0.0838052561116773, "compression_ratio": 1.6829268292682926, "no_speech_prob": 3.822200233116746e-05}, {"id": 87, "seek": 65300, "start": 653.0, "end": 663.0, "text": " And so remember that if your data bunch is called data, that'll be called data.c. So we'll add for you this weight matrix of size data.c,", "tokens": [400, 370, 1604, 300, 498, 428, 1412, 3840, 307, 1219, 1412, 11, 300, 603, 312, 1219, 1412, 13, 66, 13, 407, 321, 603, 909, 337, 291, 341, 3364, 8141, 295, 2744, 1412, 13, 66, 11], "temperature": 0.0, "avg_logprob": -0.11914600738107342, "compression_ratio": 1.5053191489361701, "no_speech_prob": 2.2471162083093077e-05}, {"id": 88, "seek": 65300, "start": 663.0, "end": 669.0, "text": " by however much was in the previous layer.", "tokens": [538, 4461, 709, 390, 294, 264, 3894, 4583, 13], "temperature": 0.0, "avg_logprob": -0.11914600738107342, "compression_ratio": 1.5053191489361701, "no_speech_prob": 2.2471162083093077e-05}, {"id": 89, "seek": 65300, "start": 669.0, "end": 677.0, "text": " OK, so now we need to train those, because initially these weight matrices are full of random numbers.", "tokens": [2264, 11, 370, 586, 321, 643, 281, 3847, 729, 11, 570, 9105, 613, 3364, 32284, 366, 1577, 295, 4974, 3547, 13], "temperature": 0.0, "avg_logprob": -0.11914600738107342, "compression_ratio": 1.5053191489361701, "no_speech_prob": 2.2471162083093077e-05}, {"id": 90, "seek": 67700, "start": 677.0, "end": 685.0, "text": " Because new weight matrices are always full of random numbers if they're new. And these ones are new. We've just grabbed them and thrown them in there.", "tokens": [1436, 777, 3364, 32284, 366, 1009, 1577, 295, 4974, 3547, 498, 436, 434, 777, 13, 400, 613, 2306, 366, 777, 13, 492, 600, 445, 18607, 552, 293, 11732, 552, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.10155403323289824, "compression_ratio": 1.6127450980392157, "no_speech_prob": 1.544502447359264e-05}, {"id": 91, "seek": 67700, "start": 685.0, "end": 694.0, "text": " So we need to train them. But the other layers are not new. The other layers are good at something.", "tokens": [407, 321, 643, 281, 3847, 552, 13, 583, 264, 661, 7914, 366, 406, 777, 13, 440, 661, 7914, 366, 665, 412, 746, 13], "temperature": 0.0, "avg_logprob": -0.10155403323289824, "compression_ratio": 1.6127450980392157, "no_speech_prob": 1.544502447359264e-05}, {"id": 92, "seek": 67700, "start": 694.0, "end": 702.0, "text": " And what are they good at? Well, let's remember that Zeiler and Fergus paper.", "tokens": [400, 437, 366, 436, 665, 412, 30, 1042, 11, 718, 311, 1604, 300, 4853, 5441, 293, 36790, 3035, 13], "temperature": 0.0, "avg_logprob": -0.10155403323289824, "compression_ratio": 1.6127450980392157, "no_speech_prob": 1.544502447359264e-05}, {"id": 93, "seek": 70200, "start": 702.0, "end": 712.0, "text": " Here are examples of some visualization of some filters, some weight matrices in the first layer, and some examples of some things that they found.", "tokens": [1692, 366, 5110, 295, 512, 25801, 295, 512, 15995, 11, 512, 3364, 32284, 294, 264, 700, 4583, 11, 293, 512, 5110, 295, 512, 721, 300, 436, 1352, 13], "temperature": 0.0, "avg_logprob": -0.047828569040670024, "compression_ratio": 1.8235294117647058, "no_speech_prob": 9.66563220572425e-06}, {"id": 94, "seek": 70200, "start": 712.0, "end": 719.0, "text": " So the first layer had one part of the weight matrix was good at finding diagonal edges in this direction.", "tokens": [407, 264, 700, 4583, 632, 472, 644, 295, 264, 3364, 8141, 390, 665, 412, 5006, 21539, 8819, 294, 341, 3513, 13], "temperature": 0.0, "avg_logprob": -0.047828569040670024, "compression_ratio": 1.8235294117647058, "no_speech_prob": 9.66563220572425e-06}, {"id": 95, "seek": 70200, "start": 719.0, "end": 726.0, "text": " And then in layer two, one of the filters was good at finding corners in the top left.", "tokens": [400, 550, 294, 4583, 732, 11, 472, 295, 264, 15995, 390, 665, 412, 5006, 12413, 294, 264, 1192, 1411, 13], "temperature": 0.0, "avg_logprob": -0.047828569040670024, "compression_ratio": 1.8235294117647058, "no_speech_prob": 9.66563220572425e-06}, {"id": 96, "seek": 72600, "start": 726.0, "end": 735.0, "text": " And then in layer three, one of the filters was good at finding repeating patterns. Another one was good at finding round orange things.", "tokens": [400, 550, 294, 4583, 1045, 11, 472, 295, 264, 15995, 390, 665, 412, 5006, 18617, 8294, 13, 3996, 472, 390, 665, 412, 5006, 3098, 7671, 721, 13], "temperature": 0.0, "avg_logprob": -0.06744847153172348, "compression_ratio": 1.7108433734939759, "no_speech_prob": 6.540166850754758e-06}, {"id": 97, "seek": 72600, "start": 735.0, "end": 740.0, "text": " Another one was good at finding kind of like fairy or floral textures.", "tokens": [3996, 472, 390, 665, 412, 5006, 733, 295, 411, 19104, 420, 38900, 24501, 13], "temperature": 0.0, "avg_logprob": -0.06744847153172348, "compression_ratio": 1.7108433734939759, "no_speech_prob": 6.540166850754758e-06}, {"id": 98, "seek": 72600, "start": 740.0, "end": 749.0, "text": " So as we go up, they're becoming more sophisticated, but also more specific.", "tokens": [407, 382, 321, 352, 493, 11, 436, 434, 5617, 544, 16950, 11, 457, 611, 544, 2685, 13], "temperature": 0.0, "avg_logprob": -0.06744847153172348, "compression_ratio": 1.7108433734939759, "no_speech_prob": 6.540166850754758e-06}, {"id": 99, "seek": 74900, "start": 749.0, "end": 760.0, "text": " So like layer four, I think was finding eyeballs, for instance. Now, if you're wanting to transfer and learn to something for histopathology slides,", "tokens": [407, 411, 4583, 1451, 11, 286, 519, 390, 5006, 43758, 11, 337, 5197, 13, 823, 11, 498, 291, 434, 7935, 281, 5003, 293, 1466, 281, 746, 337, 1758, 27212, 1793, 9788, 11], "temperature": 0.0, "avg_logprob": -0.08884211948939733, "compression_ratio": 1.7509881422924902, "no_speech_prob": 1.0288501471222844e-05}, {"id": 100, "seek": 74900, "start": 760.0, "end": 766.0, "text": " there's probably going to be no eyeballs in that. So the later layers are no good for you.", "tokens": [456, 311, 1391, 516, 281, 312, 572, 43758, 294, 300, 13, 407, 264, 1780, 7914, 366, 572, 665, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.08884211948939733, "compression_ratio": 1.7509881422924902, "no_speech_prob": 1.0288501471222844e-05}, {"id": 101, "seek": 74900, "start": 766.0, "end": 770.0, "text": " But there'll certainly be some repeating patterns and there'll certainly be some diagonal edges.", "tokens": [583, 456, 603, 3297, 312, 512, 18617, 8294, 293, 456, 603, 3297, 312, 512, 21539, 8819, 13], "temperature": 0.0, "avg_logprob": -0.08884211948939733, "compression_ratio": 1.7509881422924902, "no_speech_prob": 1.0288501471222844e-05}, {"id": 102, "seek": 74900, "start": 770.0, "end": 778.0, "text": " So the earlier you go in the model, the more likely it is that you want those weights to stay as they are.", "tokens": [407, 264, 3071, 291, 352, 294, 264, 2316, 11, 264, 544, 3700, 309, 307, 300, 291, 528, 729, 17443, 281, 1754, 382, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.08884211948939733, "compression_ratio": 1.7509881422924902, "no_speech_prob": 1.0288501471222844e-05}, {"id": 103, "seek": 77800, "start": 778.0, "end": 785.0, "text": " Well, to start with, we definitely need to train these new weights because they're random.", "tokens": [1042, 11, 281, 722, 365, 11, 321, 2138, 643, 281, 3847, 613, 777, 17443, 570, 436, 434, 4974, 13], "temperature": 0.0, "avg_logprob": -0.0687846408949958, "compression_ratio": 1.6411764705882352, "no_speech_prob": 2.7106345442007296e-05}, {"id": 104, "seek": 77800, "start": 785.0, "end": 793.0, "text": " So let's not bother training any of the other weights at all to start with. So what we do is we basically say, let's freeze.", "tokens": [407, 718, 311, 406, 8677, 3097, 604, 295, 264, 661, 17443, 412, 439, 281, 722, 365, 13, 407, 437, 321, 360, 307, 321, 1936, 584, 11, 718, 311, 15959, 13], "temperature": 0.0, "avg_logprob": -0.0687846408949958, "compression_ratio": 1.6411764705882352, "no_speech_prob": 2.7106345442007296e-05}, {"id": 105, "seek": 77800, "start": 793.0, "end": 801.0, "text": " Let's freeze all of those other layers. So what does that mean?", "tokens": [961, 311, 15959, 439, 295, 729, 661, 7914, 13, 407, 437, 775, 300, 914, 30], "temperature": 0.0, "avg_logprob": -0.0687846408949958, "compression_ratio": 1.6411764705882352, "no_speech_prob": 2.7106345442007296e-05}, {"id": 106, "seek": 80100, "start": 801.0, "end": 811.0, "text": " All that means is that we're asking Fast.ai and PyTorch that when we train, however many epochs we do, when we call fit,", "tokens": [1057, 300, 1355, 307, 300, 321, 434, 3365, 15968, 13, 1301, 293, 9953, 51, 284, 339, 300, 562, 321, 3847, 11, 4461, 867, 30992, 28346, 321, 360, 11, 562, 321, 818, 3318, 11], "temperature": 0.0, "avg_logprob": -0.1159658808457224, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.3921389583847485e-05}, {"id": 107, "seek": 80100, "start": 811.0, "end": 819.0, "text": " don't back propagate the weights, don't back propagate the gradients back into those layers.", "tokens": [500, 380, 646, 48256, 264, 17443, 11, 500, 380, 646, 48256, 264, 2771, 2448, 646, 666, 729, 7914, 13], "temperature": 0.0, "avg_logprob": -0.1159658808457224, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.3921389583847485e-05}, {"id": 108, "seek": 80100, "start": 819.0, "end": 826.0, "text": " In other words, when you go parameters equals parameters minus learning rate times gradient,", "tokens": [682, 661, 2283, 11, 562, 291, 352, 9834, 6915, 9834, 3175, 2539, 3314, 1413, 16235, 11], "temperature": 0.0, "avg_logprob": -0.1159658808457224, "compression_ratio": 1.6451612903225807, "no_speech_prob": 2.3921389583847485e-05}, {"id": 109, "seek": 82600, "start": 826.0, "end": 831.0, "text": " only do it for the new layers, don't bother doing it for the other layers. That's what freezing means.", "tokens": [787, 360, 309, 337, 264, 777, 7914, 11, 500, 380, 8677, 884, 309, 337, 264, 661, 7914, 13, 663, 311, 437, 20200, 1355, 13], "temperature": 0.0, "avg_logprob": -0.0644544536627612, "compression_ratio": 1.7625, "no_speech_prob": 6.301676330622286e-05}, {"id": 110, "seek": 82600, "start": 831.0, "end": 842.0, "text": " Just means don't update those parameters. So it'll be a little bit faster as well because there's a few less calculations to do.", "tokens": [1449, 1355, 500, 380, 5623, 729, 9834, 13, 407, 309, 603, 312, 257, 707, 857, 4663, 382, 731, 570, 456, 311, 257, 1326, 1570, 20448, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.0644544536627612, "compression_ratio": 1.7625, "no_speech_prob": 6.301676330622286e-05}, {"id": 111, "seek": 82600, "start": 842.0, "end": 847.0, "text": " It'll take up a little bit less memory because there's a few less gradients that we have to store.", "tokens": [467, 603, 747, 493, 257, 707, 857, 1570, 4675, 570, 456, 311, 257, 1326, 1570, 2771, 2448, 300, 321, 362, 281, 3531, 13], "temperature": 0.0, "avg_logprob": -0.0644544536627612, "compression_ratio": 1.7625, "no_speech_prob": 6.301676330622286e-05}, {"id": 112, "seek": 82600, "start": 847.0, "end": 854.0, "text": " But most importantly, it's not going to change weights that are already better than nothing.", "tokens": [583, 881, 8906, 11, 309, 311, 406, 516, 281, 1319, 17443, 300, 366, 1217, 1101, 813, 1825, 13], "temperature": 0.0, "avg_logprob": -0.0644544536627612, "compression_ratio": 1.7625, "no_speech_prob": 6.301676330622286e-05}, {"id": 113, "seek": 85400, "start": 854.0, "end": 858.0, "text": " They're better than random at the very least. So that's what happens when you call freeze.", "tokens": [814, 434, 1101, 813, 4974, 412, 264, 588, 1935, 13, 407, 300, 311, 437, 2314, 562, 291, 818, 15959, 13], "temperature": 0.0, "avg_logprob": -0.051285824022795024, "compression_ratio": 1.6200873362445414, "no_speech_prob": 2.7107331334264018e-05}, {"id": 114, "seek": 85400, "start": 858.0, "end": 865.0, "text": " It doesn't freeze the whole thing. It freezes everything except the randomly generated added layers that we put on for you.", "tokens": [467, 1177, 380, 15959, 264, 1379, 551, 13, 467, 1737, 12214, 1203, 3993, 264, 16979, 10833, 3869, 7914, 300, 321, 829, 322, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.051285824022795024, "compression_ratio": 1.6200873362445414, "no_speech_prob": 2.7107331334264018e-05}, {"id": 115, "seek": 85400, "start": 865.0, "end": 871.0, "text": " So then what happens next? OK, after a while, we say, OK, this is looking pretty good.", "tokens": [407, 550, 437, 2314, 958, 30, 2264, 11, 934, 257, 1339, 11, 321, 584, 11, 2264, 11, 341, 307, 1237, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.051285824022795024, "compression_ratio": 1.6200873362445414, "no_speech_prob": 2.7107331334264018e-05}, {"id": 116, "seek": 85400, "start": 871.0, "end": 879.0, "text": " We probably should train the rest of the network now. So we unfreeze.", "tokens": [492, 1391, 820, 3847, 264, 1472, 295, 264, 3209, 586, 13, 407, 321, 3971, 701, 1381, 13], "temperature": 0.0, "avg_logprob": -0.051285824022795024, "compression_ratio": 1.6200873362445414, "no_speech_prob": 2.7107331334264018e-05}, {"id": 117, "seek": 87900, "start": 879.0, "end": 887.0, "text": " And so now we're going to train the whole thing. But we still have a pretty good sense that these new layers we added to the end", "tokens": [400, 370, 586, 321, 434, 516, 281, 3847, 264, 1379, 551, 13, 583, 321, 920, 362, 257, 1238, 665, 2020, 300, 613, 777, 7914, 321, 3869, 281, 264, 917], "temperature": 0.0, "avg_logprob": -0.06808498927525111, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.1299975994916167e-05}, {"id": 118, "seek": 87900, "start": 887.0, "end": 897.0, "text": " probably need more training. And these ones right at the start that might just be like diagonal edges probably don't need much training at all.", "tokens": [1391, 643, 544, 3097, 13, 400, 613, 2306, 558, 412, 264, 722, 300, 1062, 445, 312, 411, 21539, 8819, 1391, 500, 380, 643, 709, 3097, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.06808498927525111, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.1299975994916167e-05}, {"id": 119, "seek": 89700, "start": 897.0, "end": 914.0, "text": " So we split our model into a few sections and we say, let's give different parts of the model different learning rates.", "tokens": [407, 321, 7472, 527, 2316, 666, 257, 1326, 10863, 293, 321, 584, 11, 718, 311, 976, 819, 3166, 295, 264, 2316, 819, 2539, 6846, 13], "temperature": 0.0, "avg_logprob": -0.12743756294250488, "compression_ratio": 1.5853658536585367, "no_speech_prob": 1.750121737131849e-05}, {"id": 120, "seek": 89700, "start": 914.0, "end": 922.0, "text": " So this part of the model, we might give a learning rate of one e neg five.", "tokens": [407, 341, 644, 295, 264, 2316, 11, 321, 1062, 976, 257, 2539, 3314, 295, 472, 308, 2485, 1732, 13], "temperature": 0.0, "avg_logprob": -0.12743756294250488, "compression_ratio": 1.5853658536585367, "no_speech_prob": 1.750121737131849e-05}, {"id": 121, "seek": 92200, "start": 922.0, "end": 930.0, "text": " And this part of the model, we might give a learning rate of one e neg three.", "tokens": [400, 341, 644, 295, 264, 2316, 11, 321, 1062, 976, 257, 2539, 3314, 295, 472, 308, 2485, 1045, 13], "temperature": 0.0, "avg_logprob": -0.04805742899576823, "compression_ratio": 1.7833333333333334, "no_speech_prob": 1.5205856470856816e-05}, {"id": 122, "seek": 92200, "start": 930.0, "end": 935.0, "text": " And so what's going to happen now is that we can keep training the entire network.", "tokens": [400, 370, 437, 311, 516, 281, 1051, 586, 307, 300, 321, 393, 1066, 3097, 264, 2302, 3209, 13], "temperature": 0.0, "avg_logprob": -0.04805742899576823, "compression_ratio": 1.7833333333333334, "no_speech_prob": 1.5205856470856816e-05}, {"id": 123, "seek": 92200, "start": 935.0, "end": 943.0, "text": " But because the learning rate for the early layers is smaller, it's going to move them around less because we think they're already pretty good.", "tokens": [583, 570, 264, 2539, 3314, 337, 264, 2440, 7914, 307, 4356, 11, 309, 311, 516, 281, 1286, 552, 926, 1570, 570, 321, 519, 436, 434, 1217, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.04805742899576823, "compression_ratio": 1.7833333333333334, "no_speech_prob": 1.5205856470856816e-05}, {"id": 124, "seek": 92200, "start": 943.0, "end": 949.0, "text": " And also, like if it's already pretty good to the optimal value, if you used a higher learning rate, it could kick it out.", "tokens": [400, 611, 11, 411, 498, 309, 311, 1217, 1238, 665, 281, 264, 16252, 2158, 11, 498, 291, 1143, 257, 2946, 2539, 3314, 11, 309, 727, 4437, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.04805742899576823, "compression_ratio": 1.7833333333333334, "no_speech_prob": 1.5205856470856816e-05}, {"id": 125, "seek": 94900, "start": 949.0, "end": 953.0, "text": " It could actually make it worse, which we really don't want to happen.", "tokens": [467, 727, 767, 652, 309, 5324, 11, 597, 321, 534, 500, 380, 528, 281, 1051, 13], "temperature": 0.0, "avg_logprob": -0.07020273326355735, "compression_ratio": 1.579185520361991, "no_speech_prob": 5.7381643273402005e-05}, {"id": 126, "seek": 94900, "start": 953.0, "end": 959.0, "text": " So this this process is called using discriminative learning rates.", "tokens": [407, 341, 341, 1399, 307, 1219, 1228, 20828, 1166, 2539, 6846, 13], "temperature": 0.0, "avg_logprob": -0.07020273326355735, "compression_ratio": 1.579185520361991, "no_speech_prob": 5.7381643273402005e-05}, {"id": 127, "seek": 94900, "start": 959.0, "end": 968.0, "text": " You won't find much online about it because I think we were kind of the first to use it for this purpose or at least talk about it extensively.", "tokens": [509, 1582, 380, 915, 709, 2950, 466, 309, 570, 286, 519, 321, 645, 733, 295, 264, 700, 281, 764, 309, 337, 341, 4334, 420, 412, 1935, 751, 466, 309, 32636, 13], "temperature": 0.0, "avg_logprob": -0.07020273326355735, "compression_ratio": 1.579185520361991, "no_speech_prob": 5.7381643273402005e-05}, {"id": 128, "seek": 94900, "start": 968.0, "end": 971.0, "text": " Maybe other probably other people used it without writing it down.", "tokens": [2704, 661, 1391, 661, 561, 1143, 309, 1553, 3579, 309, 760, 13], "temperature": 0.0, "avg_logprob": -0.07020273326355735, "compression_ratio": 1.579185520361991, "no_speech_prob": 5.7381643273402005e-05}, {"id": 129, "seek": 97100, "start": 971.0, "end": 980.0, "text": " So most of the stuff you'll find about this will be faster students, but it's starting to get more well known slowly now.", "tokens": [407, 881, 295, 264, 1507, 291, 603, 915, 466, 341, 486, 312, 4663, 1731, 11, 457, 309, 311, 2891, 281, 483, 544, 731, 2570, 5692, 586, 13], "temperature": 0.0, "avg_logprob": -0.10427480697631836, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.618534588487819e-05}, {"id": 130, "seek": 97100, "start": 980.0, "end": 985.0, "text": " But it's a really, really important concept for transfer learning without using this.", "tokens": [583, 309, 311, 257, 534, 11, 534, 1021, 3410, 337, 5003, 2539, 1553, 1228, 341, 13], "temperature": 0.0, "avg_logprob": -0.10427480697631836, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.618534588487819e-05}, {"id": 131, "seek": 97100, "start": 985.0, "end": 987.0, "text": " You just can't get nearly as good results.", "tokens": [509, 445, 393, 380, 483, 6217, 382, 665, 3542, 13], "temperature": 0.0, "avg_logprob": -0.10427480697631836, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.618534588487819e-05}, {"id": 132, "seek": 97100, "start": 987.0, "end": 993.0, "text": " So how do we do discriminative learning rates in fast?", "tokens": [407, 577, 360, 321, 360, 20828, 1166, 2539, 6846, 294, 2370, 30], "temperature": 0.0, "avg_logprob": -0.10427480697631836, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.618534588487819e-05}, {"id": 133, "seek": 99300, "start": 993.0, "end": 1003.0, "text": " When you when you anywhere you can put a learning rate in fast such as with the fit function.", "tokens": [1133, 291, 562, 291, 4992, 291, 393, 829, 257, 2539, 3314, 294, 2370, 1270, 382, 365, 264, 3318, 2445, 13], "temperature": 0.0, "avg_logprob": -0.17567448182539505, "compression_ratio": 1.9147727272727273, "no_speech_prob": 2.668626984814182e-05}, {"id": 134, "seek": 99300, "start": 1003.0, "end": 1005.0, "text": " The first thing you put in is the number of epochs.", "tokens": [440, 700, 551, 291, 829, 294, 307, 264, 1230, 295, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.17567448182539505, "compression_ratio": 1.9147727272727273, "no_speech_prob": 2.668626984814182e-05}, {"id": 135, "seek": 99300, "start": 1005.0, "end": 1008.0, "text": " And then the second thing you put in is learning rate.", "tokens": [400, 550, 264, 1150, 551, 291, 829, 294, 307, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.17567448182539505, "compression_ratio": 1.9147727272727273, "no_speech_prob": 2.668626984814182e-05}, {"id": 136, "seek": 99300, "start": 1008.0, "end": 1016.0, "text": " Same if you use fit one cycle, the learning rate, you can put a number of things that you can put a single number like one, a neg three.", "tokens": [10635, 498, 291, 764, 3318, 472, 6586, 11, 264, 2539, 3314, 11, 291, 393, 829, 257, 1230, 295, 721, 300, 291, 393, 829, 257, 2167, 1230, 411, 472, 11, 257, 2485, 1045, 13], "temperature": 0.0, "avg_logprob": -0.17567448182539505, "compression_ratio": 1.9147727272727273, "no_speech_prob": 2.668626984814182e-05}, {"id": 137, "seek": 101600, "start": 1016.0, "end": 1027.0, "text": " You can write a slice so you can write slice, for example, one, a neg three with a single number or you can write slice.", "tokens": [509, 393, 2464, 257, 13153, 370, 291, 393, 2464, 13153, 11, 337, 1365, 11, 472, 11, 257, 2485, 1045, 365, 257, 2167, 1230, 420, 291, 393, 2464, 13153, 13], "temperature": 0.0, "avg_logprob": -0.20813604501577523, "compression_ratio": 1.5163934426229508, "no_speech_prob": 1.1124761840619612e-05}, {"id": 138, "seek": 101600, "start": 1027.0, "end": 1034.0, "text": " With two numbers.", "tokens": [2022, 732, 3547, 13], "temperature": 0.0, "avg_logprob": -0.20813604501577523, "compression_ratio": 1.5163934426229508, "no_speech_prob": 1.1124761840619612e-05}, {"id": 139, "seek": 101600, "start": 1034.0, "end": 1039.0, "text": " What are each of those mean in the first case?", "tokens": [708, 366, 1184, 295, 729, 914, 294, 264, 700, 1389, 30], "temperature": 0.0, "avg_logprob": -0.20813604501577523, "compression_ratio": 1.5163934426229508, "no_speech_prob": 1.1124761840619612e-05}, {"id": 140, "seek": 103900, "start": 1039.0, "end": 1047.0, "text": " Just using a single number means every layer gets the same learning rate. So you're not using discriminative learning rates.", "tokens": [1449, 1228, 257, 2167, 1230, 1355, 633, 4583, 2170, 264, 912, 2539, 3314, 13, 407, 291, 434, 406, 1228, 20828, 1166, 2539, 6846, 13], "temperature": 0.0, "avg_logprob": -0.06278552832426848, "compression_ratio": 1.6275862068965516, "no_speech_prob": 1.0450813533680048e-05}, {"id": 141, "seek": 103900, "start": 1047.0, "end": 1058.0, "text": " If you pass a single number to slice, it means the final layers get a learning rate of whatever you wrote down.", "tokens": [759, 291, 1320, 257, 2167, 1230, 281, 13153, 11, 309, 1355, 264, 2572, 7914, 483, 257, 2539, 3314, 295, 2035, 291, 4114, 760, 13], "temperature": 0.0, "avg_logprob": -0.06278552832426848, "compression_ratio": 1.6275862068965516, "no_speech_prob": 1.0450813533680048e-05}, {"id": 142, "seek": 105800, "start": 1058.0, "end": 1069.0, "text": " One, a neg three. And then all the other layers get the same learning rate, which is that divided by three.", "tokens": [1485, 11, 257, 2485, 1045, 13, 400, 550, 439, 264, 661, 7914, 483, 264, 912, 2539, 3314, 11, 597, 307, 300, 6666, 538, 1045, 13], "temperature": 0.0, "avg_logprob": -0.14008301702038994, "compression_ratio": 2.069182389937107, "no_speech_prob": 7.889025255281013e-06}, {"id": 143, "seek": 105800, "start": 1069.0, "end": 1072.0, "text": " So all of the other layers will be one, a neg three divided by three.", "tokens": [407, 439, 295, 264, 661, 7914, 486, 312, 472, 11, 257, 2485, 1045, 6666, 538, 1045, 13], "temperature": 0.0, "avg_logprob": -0.14008301702038994, "compression_ratio": 2.069182389937107, "no_speech_prob": 7.889025255281013e-06}, {"id": 144, "seek": 105800, "start": 1072.0, "end": 1075.0, "text": " The last layers will be one, a neg three.", "tokens": [440, 1036, 7914, 486, 312, 472, 11, 257, 2485, 1045, 13], "temperature": 0.0, "avg_logprob": -0.14008301702038994, "compression_ratio": 2.069182389937107, "no_speech_prob": 7.889025255281013e-06}, {"id": 145, "seek": 105800, "start": 1075.0, "end": 1083.0, "text": " And the last case, the final layers, these randomly hidden added layers will still be again one, a neg three.", "tokens": [400, 264, 1036, 1389, 11, 264, 2572, 7914, 11, 613, 16979, 7633, 3869, 7914, 486, 920, 312, 797, 472, 11, 257, 2485, 1045, 13], "temperature": 0.0, "avg_logprob": -0.14008301702038994, "compression_ratio": 2.069182389937107, "no_speech_prob": 7.889025255281013e-06}, {"id": 146, "seek": 108300, "start": 1083.0, "end": 1094.0, "text": " The first layers will get one, a neg five. And the other layers will get learning rates that are equally spread between those two.", "tokens": [440, 700, 7914, 486, 483, 472, 11, 257, 2485, 1732, 13, 400, 264, 661, 7914, 486, 483, 2539, 6846, 300, 366, 12309, 3974, 1296, 729, 732, 13], "temperature": 0.0, "avg_logprob": -0.10608055652716221, "compression_ratio": 1.8012422360248448, "no_speech_prob": 2.5865821953630075e-05}, {"id": 147, "seek": 108300, "start": 1094.0, "end": 1099.0, "text": " So it's multiplicatively equal.", "tokens": [407, 309, 311, 17596, 19020, 2681, 13], "temperature": 0.0, "avg_logprob": -0.10608055652716221, "compression_ratio": 1.8012422360248448, "no_speech_prob": 2.5865821953630075e-05}, {"id": 148, "seek": 108300, "start": 1099.0, "end": 1104.0, "text": " So if there were three layers, there would be one, a neg five, one, a neg four, one, a neg three.", "tokens": [407, 498, 456, 645, 1045, 7914, 11, 456, 576, 312, 472, 11, 257, 2485, 1732, 11, 472, 11, 257, 2485, 1451, 11, 472, 11, 257, 2485, 1045, 13], "temperature": 0.0, "avg_logprob": -0.10608055652716221, "compression_ratio": 1.8012422360248448, "no_speech_prob": 2.5865821953630075e-05}, {"id": 149, "seek": 108300, "start": 1104.0, "end": 1109.0, "text": " So equal multiples each time.", "tokens": [407, 2681, 46099, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.10608055652716221, "compression_ratio": 1.8012422360248448, "no_speech_prob": 2.5865821953630075e-05}, {"id": 150, "seek": 110900, "start": 1109.0, "end": 1114.0, "text": " One slight tweak to make things a little bit simpler to manage.", "tokens": [1485, 4036, 29879, 281, 652, 721, 257, 707, 857, 18587, 281, 3067, 13], "temperature": 0.0, "avg_logprob": -0.06396886595973263, "compression_ratio": 1.772, "no_speech_prob": 4.831957630813122e-05}, {"id": 151, "seek": 110900, "start": 1114.0, "end": 1118.0, "text": " We don't actually give a different learning rate to every layer.", "tokens": [492, 500, 380, 767, 976, 257, 819, 2539, 3314, 281, 633, 4583, 13], "temperature": 0.0, "avg_logprob": -0.06396886595973263, "compression_ratio": 1.772, "no_speech_prob": 4.831957630813122e-05}, {"id": 152, "seek": 110900, "start": 1118.0, "end": 1125.0, "text": " We give a different learning rate to every layer group, which is just we decide to put the groups together for you.", "tokens": [492, 976, 257, 819, 2539, 3314, 281, 633, 4583, 1594, 11, 597, 307, 445, 321, 4536, 281, 829, 264, 3935, 1214, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.06396886595973263, "compression_ratio": 1.772, "no_speech_prob": 4.831957630813122e-05}, {"id": 153, "seek": 110900, "start": 1125.0, "end": 1130.0, "text": " And so specifically what we do is the randomly added extra layers.", "tokens": [400, 370, 4682, 437, 321, 360, 307, 264, 16979, 3869, 2857, 7914, 13], "temperature": 0.0, "avg_logprob": -0.06396886595973263, "compression_ratio": 1.772, "no_speech_prob": 4.831957630813122e-05}, {"id": 154, "seek": 110900, "start": 1130.0, "end": 1134.0, "text": " We call those one layer group. This is by default. You can modify it.", "tokens": [492, 818, 729, 472, 4583, 1594, 13, 639, 307, 538, 7576, 13, 509, 393, 16927, 309, 13], "temperature": 0.0, "avg_logprob": -0.06396886595973263, "compression_ratio": 1.772, "no_speech_prob": 4.831957630813122e-05}, {"id": 155, "seek": 110900, "start": 1134.0, "end": 1138.0, "text": " And then all the rest we split in half into two layer groups.", "tokens": [400, 550, 439, 264, 1472, 321, 7472, 294, 1922, 666, 732, 4583, 3935, 13], "temperature": 0.0, "avg_logprob": -0.06396886595973263, "compression_ratio": 1.772, "no_speech_prob": 4.831957630813122e-05}, {"id": 156, "seek": 113800, "start": 1138.0, "end": 1141.0, "text": " So by default, at least with a CNN, you'll get three layer groups.", "tokens": [407, 538, 7576, 11, 412, 1935, 365, 257, 24859, 11, 291, 603, 483, 1045, 4583, 3935, 13], "temperature": 0.0, "avg_logprob": -0.06800229107892072, "compression_ratio": 1.8074074074074074, "no_speech_prob": 3.3734024327713996e-05}, {"id": 157, "seek": 113800, "start": 1141.0, "end": 1152.0, "text": " And so if you say slice one, a neg five, one, a neg three, you will get one, a neg five learning rate for the first layer group, one, a neg four for the second, one, a neg three for the third.", "tokens": [400, 370, 498, 291, 584, 13153, 472, 11, 257, 2485, 1732, 11, 472, 11, 257, 2485, 1045, 11, 291, 486, 483, 472, 11, 257, 2485, 1732, 2539, 3314, 337, 264, 700, 4583, 1594, 11, 472, 11, 257, 2485, 1451, 337, 264, 1150, 11, 472, 11, 257, 2485, 1045, 337, 264, 2636, 13], "temperature": 0.0, "avg_logprob": -0.06800229107892072, "compression_ratio": 1.8074074074074074, "no_speech_prob": 3.3734024327713996e-05}, {"id": 158, "seek": 113800, "start": 1152.0, "end": 1158.0, "text": " So now if you go back and look at the way that we're training, hopefully you'll see that this makes a lot of sense.", "tokens": [407, 586, 498, 291, 352, 646, 293, 574, 412, 264, 636, 300, 321, 434, 3097, 11, 4696, 291, 603, 536, 300, 341, 1669, 257, 688, 295, 2020, 13], "temperature": 0.0, "avg_logprob": -0.06800229107892072, "compression_ratio": 1.8074074074074074, "no_speech_prob": 3.3734024327713996e-05}, {"id": 159, "seek": 113800, "start": 1158.0, "end": 1162.0, "text": " This divided by three thing is a little weird.", "tokens": [639, 6666, 538, 1045, 551, 307, 257, 707, 3657, 13], "temperature": 0.0, "avg_logprob": -0.06800229107892072, "compression_ratio": 1.8074074074074074, "no_speech_prob": 3.3734024327713996e-05}, {"id": 160, "seek": 113800, "start": 1162.0, "end": 1167.0, "text": " And we won't talk about why that is until part two of the course.", "tokens": [400, 321, 1582, 380, 751, 466, 983, 300, 307, 1826, 644, 732, 295, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.06800229107892072, "compression_ratio": 1.8074074074074074, "no_speech_prob": 3.3734024327713996e-05}, {"id": 161, "seek": 116700, "start": 1167.0, "end": 1172.0, "text": " It's a specific work around batch normalization.", "tokens": [467, 311, 257, 2685, 589, 926, 15245, 2710, 2144, 13], "temperature": 0.0, "avg_logprob": -0.12716852991204514, "compression_ratio": 1.4076433121019107, "no_speech_prob": 1.5205674571916461e-05}, {"id": 162, "seek": 116700, "start": 1172.0, "end": 1177.0, "text": " So we can discuss that in the advanced topic if anybody's interested.", "tokens": [407, 321, 393, 2248, 300, 294, 264, 7339, 4829, 498, 4472, 311, 3102, 13], "temperature": 0.0, "avg_logprob": -0.12716852991204514, "compression_ratio": 1.4076433121019107, "no_speech_prob": 1.5205674571916461e-05}, {"id": 163, "seek": 116700, "start": 1177.0, "end": 1182.0, "text": " All right. So that is.", "tokens": [1057, 558, 13, 407, 300, 307, 13], "temperature": 0.0, "avg_logprob": -0.12716852991204514, "compression_ratio": 1.4076433121019107, "no_speech_prob": 1.5205674571916461e-05}, {"id": 164, "seek": 116700, "start": 1182.0, "end": 1190.0, "text": " That is fine tuning. So hopefully that makes that a little bit less mysterious.", "tokens": [663, 307, 2489, 15164, 13, 407, 4696, 300, 1669, 300, 257, 707, 857, 1570, 13831, 13], "temperature": 0.0, "avg_logprob": -0.12716852991204514, "compression_ratio": 1.4076433121019107, "no_speech_prob": 1.5205674571916461e-05}, {"id": 165, "seek": 119000, "start": 1190.0, "end": 1206.0, "text": " So we were looking at. Collaborative filtering last week and in the collaborative filtering example, we called fit one cycle and we passed in just a single number.", "tokens": [407, 321, 645, 1237, 412, 13, 44483, 1166, 30822, 1036, 1243, 293, 294, 264, 16555, 30822, 1365, 11, 321, 1219, 3318, 472, 6586, 293, 321, 4678, 294, 445, 257, 2167, 1230, 13], "temperature": 0.0, "avg_logprob": -0.11158764137412017, "compression_ratio": 1.6486486486486487, "no_speech_prob": 5.0142962209065445e-06}, {"id": 166, "seek": 119000, "start": 1206.0, "end": 1213.0, "text": " And that makes sense because in collaborative filtering, we only have one layer.", "tokens": [400, 300, 1669, 2020, 570, 294, 16555, 30822, 11, 321, 787, 362, 472, 4583, 13], "temperature": 0.0, "avg_logprob": -0.11158764137412017, "compression_ratio": 1.6486486486486487, "no_speech_prob": 5.0142962209065445e-06}, {"id": 167, "seek": 121300, "start": 1213.0, "end": 1225.0, "text": " There's a few different pieces in it, but there isn't, you know, a matrix multiply followed by an activation function followed by another matrix model play.", "tokens": [821, 311, 257, 1326, 819, 3755, 294, 309, 11, 457, 456, 1943, 380, 11, 291, 458, 11, 257, 8141, 12972, 6263, 538, 364, 24433, 2445, 6263, 538, 1071, 8141, 2316, 862, 13], "temperature": 0.0, "avg_logprob": -0.15627490557157075, "compression_ratio": 1.6715686274509804, "no_speech_prob": 1.669976882112678e-05}, {"id": 168, "seek": 121300, "start": 1225.0, "end": 1230.0, "text": " I know introduce a another piece of jargon here.", "tokens": [286, 458, 5366, 257, 1071, 2522, 295, 15181, 10660, 510, 13], "temperature": 0.0, "avg_logprob": -0.15627490557157075, "compression_ratio": 1.6715686274509804, "no_speech_prob": 1.669976882112678e-05}, {"id": 169, "seek": 121300, "start": 1230.0, "end": 1234.0, "text": " They're not always exactly matrix multiplications.", "tokens": [814, 434, 406, 1009, 2293, 8141, 17596, 763, 13], "temperature": 0.0, "avg_logprob": -0.15627490557157075, "compression_ratio": 1.6715686274509804, "no_speech_prob": 1.669976882112678e-05}, {"id": 170, "seek": 121300, "start": 1234.0, "end": 1239.0, "text": " There's something very similar there. They're linear functions that we add together.", "tokens": [821, 311, 746, 588, 2531, 456, 13, 814, 434, 8213, 6828, 300, 321, 909, 1214, 13], "temperature": 0.0, "avg_logprob": -0.15627490557157075, "compression_ratio": 1.6715686274509804, "no_speech_prob": 1.669976882112678e-05}, {"id": 171, "seek": 123900, "start": 1239.0, "end": 1246.0, "text": " But the more general term for these for these things that are more general than matrix multiplications is affine functions.", "tokens": [583, 264, 544, 2674, 1433, 337, 613, 337, 613, 721, 300, 366, 544, 2674, 813, 8141, 17596, 763, 307, 2096, 533, 6828, 13], "temperature": 0.0, "avg_logprob": -0.07803344239993971, "compression_ratio": 1.9017857142857142, "no_speech_prob": 2.2471875126939267e-05}, {"id": 172, "seek": 123900, "start": 1246.0, "end": 1255.0, "text": " OK. So if you hear me say the word affine function, you can replace it in your head with matrix multiplication.", "tokens": [2264, 13, 407, 498, 291, 1568, 385, 584, 264, 1349, 2096, 533, 2445, 11, 291, 393, 7406, 309, 294, 428, 1378, 365, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.07803344239993971, "compression_ratio": 1.9017857142857142, "no_speech_prob": 2.2471875126939267e-05}, {"id": 173, "seek": 123900, "start": 1255.0, "end": 1262.0, "text": " But as we'll see when we do convolutions, convolutions are matrix multiplications where some of the weights are tied.", "tokens": [583, 382, 321, 603, 536, 562, 321, 360, 3754, 15892, 11, 3754, 15892, 366, 8141, 17596, 763, 689, 512, 295, 264, 17443, 366, 9601, 13], "temperature": 0.0, "avg_logprob": -0.07803344239993971, "compression_ratio": 1.9017857142857142, "no_speech_prob": 2.2471875126939267e-05}, {"id": 174, "seek": 123900, "start": 1262.0, "end": 1266.0, "text": " And so it would be slightly more accurate to call them affine functions.", "tokens": [400, 370, 309, 576, 312, 4748, 544, 8559, 281, 818, 552, 2096, 533, 6828, 13], "temperature": 0.0, "avg_logprob": -0.07803344239993971, "compression_ratio": 1.9017857142857142, "no_speech_prob": 2.2471875126939267e-05}, {"id": 175, "seek": 126600, "start": 1266.0, "end": 1275.0, "text": " I'd like to introduce a little bit more jargon each lesson so that when you read books or papers or watch other courses or read documentation,", "tokens": [286, 1116, 411, 281, 5366, 257, 707, 857, 544, 15181, 10660, 1184, 6898, 370, 300, 562, 291, 1401, 3642, 420, 10577, 420, 1159, 661, 7712, 420, 1401, 14333, 11], "temperature": 0.0, "avg_logprob": -0.09722988326828201, "compression_ratio": 1.5576923076923077, "no_speech_prob": 5.6820108511601575e-06}, {"id": 176, "seek": 126600, "start": 1275.0, "end": 1278.0, "text": " there will be more of the words you'll recognize.", "tokens": [456, 486, 312, 544, 295, 264, 2283, 291, 603, 5521, 13], "temperature": 0.0, "avg_logprob": -0.09722988326828201, "compression_ratio": 1.5576923076923077, "no_speech_prob": 5.6820108511601575e-06}, {"id": 177, "seek": 126600, "start": 1278.0, "end": 1283.0, "text": " So when you say affine function, it just means a linear function.", "tokens": [407, 562, 291, 584, 2096, 533, 2445, 11, 309, 445, 1355, 257, 8213, 2445, 13], "temperature": 0.0, "avg_logprob": -0.09722988326828201, "compression_ratio": 1.5576923076923077, "no_speech_prob": 5.6820108511601575e-06}, {"id": 178, "seek": 126600, "start": 1283.0, "end": 1288.0, "text": " And it means something very, very close to matrix multiplication.", "tokens": [400, 309, 1355, 746, 588, 11, 588, 1998, 281, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.09722988326828201, "compression_ratio": 1.5576923076923077, "no_speech_prob": 5.6820108511601575e-06}, {"id": 179, "seek": 128800, "start": 1288.0, "end": 1296.0, "text": " Matrix multiplication is the most common kind of affine function, at least in deep learning.", "tokens": [36274, 27290, 307, 264, 881, 2689, 733, 295, 2096, 533, 2445, 11, 412, 1935, 294, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.07947449026436641, "compression_ratio": 1.5403726708074534, "no_speech_prob": 6.4388318605779205e-06}, {"id": 180, "seek": 128800, "start": 1296.0, "end": 1306.0, "text": " So specifically for collaborative filtering, the model we were using was this one.", "tokens": [407, 4682, 337, 16555, 30822, 11, 264, 2316, 321, 645, 1228, 390, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.07947449026436641, "compression_ratio": 1.5403726708074534, "no_speech_prob": 6.4388318605779205e-06}, {"id": 181, "seek": 128800, "start": 1306.0, "end": 1314.0, "text": " It was where we had a bunch of numbers here and a bunch of numbers here.", "tokens": [467, 390, 689, 321, 632, 257, 3840, 295, 3547, 510, 293, 257, 3840, 295, 3547, 510, 13], "temperature": 0.0, "avg_logprob": -0.07947449026436641, "compression_ratio": 1.5403726708074534, "no_speech_prob": 6.4388318605779205e-06}, {"id": 182, "seek": 131400, "start": 1314.0, "end": 1322.0, "text": " And we took the dot product of them. And given that one here is a row and one is a column, we can actually that's the same as a matrix product.", "tokens": [400, 321, 1890, 264, 5893, 1674, 295, 552, 13, 400, 2212, 300, 472, 510, 307, 257, 5386, 293, 472, 307, 257, 7738, 11, 321, 393, 767, 300, 311, 264, 912, 382, 257, 8141, 1674, 13], "temperature": 0.0, "avg_logprob": -0.07985098999325592, "compression_ratio": 1.676991150442478, "no_speech_prob": 2.5068206014111638e-05}, {"id": 183, "seek": 131400, "start": 1322.0, "end": 1329.0, "text": " So M molt in Excel multiplies matrices. So here is the matrix product of those two.", "tokens": [407, 376, 10739, 294, 19060, 12788, 530, 32284, 13, 407, 510, 307, 264, 8141, 1674, 295, 729, 732, 13], "temperature": 0.0, "avg_logprob": -0.07985098999325592, "compression_ratio": 1.676991150442478, "no_speech_prob": 2.5068206014111638e-05}, {"id": 184, "seek": 131400, "start": 1329.0, "end": 1335.0, "text": " And so I started this training last week by using Solver in Excel.", "tokens": [400, 370, 286, 1409, 341, 3097, 1036, 1243, 538, 1228, 7026, 331, 294, 19060, 13], "temperature": 0.0, "avg_logprob": -0.07985098999325592, "compression_ratio": 1.676991150442478, "no_speech_prob": 2.5068206014111638e-05}, {"id": 185, "seek": 131400, "start": 1335.0, "end": 1341.0, "text": " And we never actually went back to see how it went. So let's go and have a look now.", "tokens": [400, 321, 1128, 767, 1437, 646, 281, 536, 577, 309, 1437, 13, 407, 718, 311, 352, 293, 362, 257, 574, 586, 13], "temperature": 0.0, "avg_logprob": -0.07985098999325592, "compression_ratio": 1.676991150442478, "no_speech_prob": 2.5068206014111638e-05}, {"id": 186, "seek": 134100, "start": 1341.0, "end": 1346.0, "text": " So the average sum of squared error got down to 0.39.", "tokens": [407, 264, 4274, 2408, 295, 8889, 6713, 658, 760, 281, 1958, 13, 12493, 13], "temperature": 0.0, "avg_logprob": -0.1255085919354413, "compression_ratio": 1.5610859728506787, "no_speech_prob": 2.4680486603756435e-05}, {"id": 187, "seek": 134100, "start": 1346.0, "end": 1353.0, "text": " So we're trying to predict something on a scale of 0.5 to 5. So on average, we're being wrong by about 0.4.", "tokens": [407, 321, 434, 1382, 281, 6069, 746, 322, 257, 4373, 295, 1958, 13, 20, 281, 1025, 13, 407, 322, 4274, 11, 321, 434, 885, 2085, 538, 466, 1958, 13, 19, 13], "temperature": 0.0, "avg_logprob": -0.1255085919354413, "compression_ratio": 1.5610859728506787, "no_speech_prob": 2.4680486603756435e-05}, {"id": 188, "seek": 134100, "start": 1353.0, "end": 1361.0, "text": " That's pretty good. And you can kind of see it's pretty good if you look at like 3.51 is what it meant to be.", "tokens": [663, 311, 1238, 665, 13, 400, 291, 393, 733, 295, 536, 309, 311, 1238, 665, 498, 291, 574, 412, 411, 805, 13, 18682, 307, 437, 309, 4140, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.1255085919354413, "compression_ratio": 1.5610859728506787, "no_speech_prob": 2.4680486603756435e-05}, {"id": 189, "seek": 134100, "start": 1361.0, "end": 1370.0, "text": " 3.25, 5.1, 0.98. That's pretty close. Right. So you get the general idea.", "tokens": [805, 13, 6074, 11, 1025, 13, 16, 11, 1958, 13, 22516, 13, 663, 311, 1238, 1998, 13, 1779, 13, 407, 291, 483, 264, 2674, 1558, 13], "temperature": 0.0, "avg_logprob": -0.1255085919354413, "compression_ratio": 1.5610859728506787, "no_speech_prob": 2.4680486603756435e-05}, {"id": 190, "seek": 137000, "start": 1370.0, "end": 1381.0, "text": " And then I started to talk about this idea of embedding matrices. And so in order to understand that, let's put this worksheet aside.", "tokens": [400, 550, 286, 1409, 281, 751, 466, 341, 1558, 295, 12240, 3584, 32284, 13, 400, 370, 294, 1668, 281, 1223, 300, 11, 718, 311, 829, 341, 49890, 7359, 13], "temperature": 0.0, "avg_logprob": -0.06683355137921762, "compression_ratio": 1.6222222222222222, "no_speech_prob": 7.071638265188085e-06}, {"id": 191, "seek": 137000, "start": 1381.0, "end": 1386.0, "text": " I look at another worksheet. So here's another worksheet.", "tokens": [286, 574, 412, 1071, 49890, 13, 407, 510, 311, 1071, 49890, 13], "temperature": 0.0, "avg_logprob": -0.06683355137921762, "compression_ratio": 1.6222222222222222, "no_speech_prob": 7.071638265188085e-06}, {"id": 192, "seek": 137000, "start": 1386.0, "end": 1394.0, "text": " And what I've done here is I have copied over those two weight matrices from the previous worksheet.", "tokens": [400, 437, 286, 600, 1096, 510, 307, 286, 362, 25365, 670, 729, 732, 3364, 32284, 490, 264, 3894, 49890, 13], "temperature": 0.0, "avg_logprob": -0.06683355137921762, "compression_ratio": 1.6222222222222222, "no_speech_prob": 7.071638265188085e-06}, {"id": 193, "seek": 139400, "start": 1394.0, "end": 1401.0, "text": " Right. Here's the one for users and here's the one for movies. And the movies one, I've transposed it.", "tokens": [1779, 13, 1692, 311, 264, 472, 337, 5022, 293, 510, 311, 264, 472, 337, 6233, 13, 400, 264, 6233, 472, 11, 286, 600, 7132, 1744, 309, 13], "temperature": 0.0, "avg_logprob": -0.09711572256955234, "compression_ratio": 1.6161137440758293, "no_speech_prob": 1.0952586308121681e-05}, {"id": 194, "seek": 139400, "start": 1401.0, "end": 1405.0, "text": " So it's now got exactly the same dimensions as the users one.", "tokens": [407, 309, 311, 586, 658, 2293, 264, 912, 12819, 382, 264, 5022, 472, 13], "temperature": 0.0, "avg_logprob": -0.09711572256955234, "compression_ratio": 1.6161137440758293, "no_speech_prob": 1.0952586308121681e-05}, {"id": 195, "seek": 139400, "start": 1405.0, "end": 1415.0, "text": " So here are two weight matrices. Initially, they were random. We can train them with gradient descent.", "tokens": [407, 510, 366, 732, 3364, 32284, 13, 29446, 11, 436, 645, 4974, 13, 492, 393, 3847, 552, 365, 16235, 23475, 13], "temperature": 0.0, "avg_logprob": -0.09711572256955234, "compression_ratio": 1.6161137440758293, "no_speech_prob": 1.0952586308121681e-05}, {"id": 196, "seek": 139400, "start": 1415.0, "end": 1422.0, "text": " In the original data, the user IDs and movie IDs were numbers like these.", "tokens": [682, 264, 3380, 1412, 11, 264, 4195, 48212, 293, 3169, 48212, 645, 3547, 411, 613, 13], "temperature": 0.0, "avg_logprob": -0.09711572256955234, "compression_ratio": 1.6161137440758293, "no_speech_prob": 1.0952586308121681e-05}, {"id": 197, "seek": 142200, "start": 1422.0, "end": 1430.0, "text": " To make life more convenient, I've converted them to numbers from 1 to 15.", "tokens": [1407, 652, 993, 544, 10851, 11, 286, 600, 16424, 552, 281, 3547, 490, 502, 281, 2119, 13], "temperature": 0.0, "avg_logprob": -0.13105954752339946, "compression_ratio": 1.572192513368984, "no_speech_prob": 1.5445752069354057e-05}, {"id": 198, "seek": 142200, "start": 1430.0, "end": 1440.0, "text": " OK. So in these columns, I've got for every rating, I've got user ID movie ID rating using these mapped numbers so that they're contiguous starting at 1.", "tokens": [2264, 13, 407, 294, 613, 13766, 11, 286, 600, 658, 337, 633, 10990, 11, 286, 600, 658, 4195, 7348, 3169, 7348, 10990, 1228, 613, 33318, 3547, 370, 300, 436, 434, 660, 30525, 2891, 412, 502, 13], "temperature": 0.0, "avg_logprob": -0.13105954752339946, "compression_ratio": 1.572192513368984, "no_speech_prob": 1.5445752069354057e-05}, {"id": 199, "seek": 142200, "start": 1440.0, "end": 1448.0, "text": " OK. Now I'm going to replace user ID number one with this vector.", "tokens": [2264, 13, 823, 286, 478, 516, 281, 7406, 4195, 7348, 1230, 472, 365, 341, 8062, 13], "temperature": 0.0, "avg_logprob": -0.13105954752339946, "compression_ratio": 1.572192513368984, "no_speech_prob": 1.5445752069354057e-05}, {"id": 200, "seek": 144800, "start": 1448.0, "end": 1453.0, "text": " The vector contains a 1 followed by 14 zeros.", "tokens": [440, 8062, 8306, 257, 502, 6263, 538, 3499, 35193, 13], "temperature": 0.0, "avg_logprob": -0.10408290227254231, "compression_ratio": 1.6395348837209303, "no_speech_prob": 1.4509660104522482e-05}, {"id": 201, "seek": 144800, "start": 1453.0, "end": 1462.0, "text": " And then user number two, I'm going to replace with a vector of 0 and then 1 and then 13 zeros and so forth.", "tokens": [400, 550, 4195, 1230, 732, 11, 286, 478, 516, 281, 7406, 365, 257, 8062, 295, 1958, 293, 550, 502, 293, 550, 3705, 35193, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.10408290227254231, "compression_ratio": 1.6395348837209303, "no_speech_prob": 1.4509660104522482e-05}, {"id": 202, "seek": 144800, "start": 1462.0, "end": 1473.0, "text": " So movie ID 14, all these are movie ID 14, I've also replaced with another vector, which is 13 zeros and then a 1 and then a 0.", "tokens": [407, 3169, 7348, 3499, 11, 439, 613, 366, 3169, 7348, 3499, 11, 286, 600, 611, 10772, 365, 1071, 8062, 11, 597, 307, 3705, 35193, 293, 550, 257, 502, 293, 550, 257, 1958, 13], "temperature": 0.0, "avg_logprob": -0.10408290227254231, "compression_ratio": 1.6395348837209303, "no_speech_prob": 1.4509660104522482e-05}, {"id": 203, "seek": 147300, "start": 1473.0, "end": 1481.0, "text": " OK. So these are called one-hot encodings, by the way.", "tokens": [2264, 13, 407, 613, 366, 1219, 472, 12, 12194, 2058, 378, 1109, 11, 538, 264, 636, 13], "temperature": 0.0, "avg_logprob": -0.08839256062227137, "compression_ratio": 1.7878787878787878, "no_speech_prob": 9.817474165174644e-06}, {"id": 204, "seek": 147300, "start": 1481.0, "end": 1489.0, "text": " So this is not part of a neural net. This is just like some input pre-processing where I'm literally making this my new inputs.", "tokens": [407, 341, 307, 406, 644, 295, 257, 18161, 2533, 13, 639, 307, 445, 411, 512, 4846, 659, 12, 41075, 278, 689, 286, 478, 3736, 1455, 341, 452, 777, 15743, 13], "temperature": 0.0, "avg_logprob": -0.08839256062227137, "compression_ratio": 1.7878787878787878, "no_speech_prob": 9.817474165174644e-06}, {"id": 205, "seek": 147300, "start": 1489.0, "end": 1495.0, "text": " This is my new inputs for my movies. This is my new inputs for my users.", "tokens": [639, 307, 452, 777, 15743, 337, 452, 6233, 13, 639, 307, 452, 777, 15743, 337, 452, 5022, 13], "temperature": 0.0, "avg_logprob": -0.08839256062227137, "compression_ratio": 1.7878787878787878, "no_speech_prob": 9.817474165174644e-06}, {"id": 206, "seek": 147300, "start": 1495.0, "end": 1498.0, "text": " So these are my inputs to a neural net.", "tokens": [407, 613, 366, 452, 15743, 281, 257, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.08839256062227137, "compression_ratio": 1.7878787878787878, "no_speech_prob": 9.817474165174644e-06}, {"id": 207, "seek": 149800, "start": 1498.0, "end": 1511.0, "text": " So what I'm going to do now is I'm going to take this input matrix and I'm going to do a matrix multiply by this weight matrix.", "tokens": [407, 437, 286, 478, 516, 281, 360, 586, 307, 286, 478, 516, 281, 747, 341, 4846, 8141, 293, 286, 478, 516, 281, 360, 257, 8141, 12972, 538, 341, 3364, 8141, 13], "temperature": 0.0, "avg_logprob": -0.05574984622724129, "compression_ratio": 1.7012987012987013, "no_speech_prob": 1.6962931113084778e-05}, {"id": 208, "seek": 149800, "start": 1511.0, "end": 1519.0, "text": " And that will work because this has 15 rows and this has 15 columns.", "tokens": [400, 300, 486, 589, 570, 341, 575, 2119, 13241, 293, 341, 575, 2119, 13766, 13], "temperature": 0.0, "avg_logprob": -0.05574984622724129, "compression_ratio": 1.7012987012987013, "no_speech_prob": 1.6962931113084778e-05}, {"id": 209, "seek": 149800, "start": 1519.0, "end": 1522.0, "text": " So I can multiply those two matrices together because they match.", "tokens": [407, 286, 393, 12972, 729, 732, 32284, 1214, 570, 436, 2995, 13], "temperature": 0.0, "avg_logprob": -0.05574984622724129, "compression_ratio": 1.7012987012987013, "no_speech_prob": 1.6962931113084778e-05}, {"id": 210, "seek": 152200, "start": 1522.0, "end": 1528.0, "text": " And you can do matrix multiplication in Excel using the ammult function.", "tokens": [400, 291, 393, 360, 8141, 27290, 294, 19060, 1228, 264, 669, 76, 723, 2445, 13], "temperature": 0.0, "avg_logprob": -0.10397161411333687, "compression_ratio": 1.7263681592039801, "no_speech_prob": 2.976783252961468e-05}, {"id": 211, "seek": 152200, "start": 1528.0, "end": 1535.0, "text": " Just be careful if you're using Excel because this is a function that returns multiple numbers.", "tokens": [1449, 312, 5026, 498, 291, 434, 1228, 19060, 570, 341, 307, 257, 2445, 300, 11247, 3866, 3547, 13], "temperature": 0.0, "avg_logprob": -0.10397161411333687, "compression_ratio": 1.7263681592039801, "no_speech_prob": 2.976783252961468e-05}, {"id": 212, "seek": 152200, "start": 1535.0, "end": 1538.0, "text": " You can't just hit enter when you finish with it. You have to hit control shift enter.", "tokens": [509, 393, 380, 445, 2045, 3242, 562, 291, 2413, 365, 309, 13, 509, 362, 281, 2045, 1969, 5513, 3242, 13], "temperature": 0.0, "avg_logprob": -0.10397161411333687, "compression_ratio": 1.7263681592039801, "no_speech_prob": 2.976783252961468e-05}, {"id": 213, "seek": 152200, "start": 1538.0, "end": 1543.0, "text": " Control shift enter means this is a array function, something that returns multiple values.", "tokens": [12912, 5513, 3242, 1355, 341, 307, 257, 10225, 2445, 11, 746, 300, 11247, 3866, 4190, 13], "temperature": 0.0, "avg_logprob": -0.10397161411333687, "compression_ratio": 1.7263681592039801, "no_speech_prob": 2.976783252961468e-05}, {"id": 214, "seek": 154300, "start": 1543.0, "end": 1560.0, "text": " So here is the matrix product of this input matrix of inputs and this parameter matrix or weight matrix.", "tokens": [407, 510, 307, 264, 8141, 1674, 295, 341, 4846, 8141, 295, 15743, 293, 341, 13075, 8141, 420, 3364, 8141, 13], "temperature": 0.0, "avg_logprob": -0.09004536000165073, "compression_ratio": 1.5080645161290323, "no_speech_prob": 1.4509750144497957e-05}, {"id": 215, "seek": 154300, "start": 1560.0, "end": 1568.0, "text": " So that's just a normal neural network layer. It's just a regular matrix multiply.", "tokens": [407, 300, 311, 445, 257, 2710, 18161, 3209, 4583, 13, 467, 311, 445, 257, 3890, 8141, 12972, 13], "temperature": 0.0, "avg_logprob": -0.09004536000165073, "compression_ratio": 1.5080645161290323, "no_speech_prob": 1.4509750144497957e-05}, {"id": 216, "seek": 156800, "start": 1568.0, "end": 1576.0, "text": " And so we can do the same thing for movies. And so here's the matrix multiply for movies.", "tokens": [400, 370, 321, 393, 360, 264, 912, 551, 337, 6233, 13, 400, 370, 510, 311, 264, 8141, 12972, 337, 6233, 13], "temperature": 0.0, "avg_logprob": -0.07475107245975071, "compression_ratio": 1.65625, "no_speech_prob": 1.0451052730786614e-05}, {"id": 217, "seek": 156800, "start": 1576.0, "end": 1581.0, "text": " Well, here's the thing.", "tokens": [1042, 11, 510, 311, 264, 551, 13], "temperature": 0.0, "avg_logprob": -0.07475107245975071, "compression_ratio": 1.65625, "no_speech_prob": 1.0451052730786614e-05}, {"id": 218, "seek": 156800, "start": 1581.0, "end": 1587.0, "text": " This input is we claim is this kind of one hot encoded version of user ID number one.", "tokens": [639, 4846, 307, 321, 3932, 307, 341, 733, 295, 472, 2368, 2058, 12340, 3037, 295, 4195, 7348, 1230, 472, 13], "temperature": 0.0, "avg_logprob": -0.07475107245975071, "compression_ratio": 1.65625, "no_speech_prob": 1.0451052730786614e-05}, {"id": 219, "seek": 156800, "start": 1587.0, "end": 1593.0, "text": " And these activations are the activations for user ID number one.", "tokens": [400, 613, 2430, 763, 366, 264, 2430, 763, 337, 4195, 7348, 1230, 472, 13], "temperature": 0.0, "avg_logprob": -0.07475107245975071, "compression_ratio": 1.65625, "no_speech_prob": 1.0451052730786614e-05}, {"id": 220, "seek": 159300, "start": 1593.0, "end": 1599.0, "text": " Why is that? Because if you think about it, a matrix multiplication between a one hot encoded vector", "tokens": [1545, 307, 300, 30, 1436, 498, 291, 519, 466, 309, 11, 257, 8141, 27290, 1296, 257, 472, 2368, 2058, 12340, 8062], "temperature": 0.0, "avg_logprob": -0.059418208162549516, "compression_ratio": 1.580110497237569, "no_speech_prob": 1.9525221432559192e-05}, {"id": 221, "seek": 159300, "start": 1599.0, "end": 1609.0, "text": " and some matrix is actually going to find the nth row of that matrix when the one is in position n.", "tokens": [293, 512, 8141, 307, 767, 516, 281, 915, 264, 297, 392, 5386, 295, 300, 8141, 562, 264, 472, 307, 294, 2535, 297, 13], "temperature": 0.0, "avg_logprob": -0.059418208162549516, "compression_ratio": 1.580110497237569, "no_speech_prob": 1.9525221432559192e-05}, {"id": 222, "seek": 159300, "start": 1609.0, "end": 1618.0, "text": " Does that make sense? So what we've done here is we've actually got a matrix multiply", "tokens": [4402, 300, 652, 2020, 30, 407, 437, 321, 600, 1096, 510, 307, 321, 600, 767, 658, 257, 8141, 12972], "temperature": 0.0, "avg_logprob": -0.059418208162549516, "compression_ratio": 1.580110497237569, "no_speech_prob": 1.9525221432559192e-05}, {"id": 223, "seek": 161800, "start": 1618.0, "end": 1623.0, "text": " that is creating these output activations, right? But it's doing it in a very interesting way,", "tokens": [300, 307, 4084, 613, 5598, 2430, 763, 11, 558, 30, 583, 309, 311, 884, 309, 294, 257, 588, 1880, 636, 11], "temperature": 0.0, "avg_logprob": -0.10606849484327363, "compression_ratio": 1.6067961165048543, "no_speech_prob": 1.147840612247819e-05}, {"id": 224, "seek": 161800, "start": 1623.0, "end": 1627.0, "text": " which is it's effectively finding a particular row in the input matrix.", "tokens": [597, 307, 309, 311, 8659, 5006, 257, 1729, 5386, 294, 264, 4846, 8141, 13], "temperature": 0.0, "avg_logprob": -0.10606849484327363, "compression_ratio": 1.6067961165048543, "no_speech_prob": 1.147840612247819e-05}, {"id": 225, "seek": 161800, "start": 1627.0, "end": 1636.0, "text": " So having done that, we can then multiply those two sets together, just a dot product.", "tokens": [407, 1419, 1096, 300, 11, 321, 393, 550, 12972, 729, 732, 6352, 1214, 11, 445, 257, 5893, 1674, 13], "temperature": 0.0, "avg_logprob": -0.10606849484327363, "compression_ratio": 1.6067961165048543, "no_speech_prob": 1.147840612247819e-05}, {"id": 226, "seek": 161800, "start": 1636.0, "end": 1644.0, "text": " And we can then find the loss squared. And then we can find the average loss.", "tokens": [400, 321, 393, 550, 915, 264, 4470, 8889, 13, 400, 550, 321, 393, 915, 264, 4274, 4470, 13], "temperature": 0.0, "avg_logprob": -0.10606849484327363, "compression_ratio": 1.6067961165048543, "no_speech_prob": 1.147840612247819e-05}, {"id": 227, "seek": 164400, "start": 1644.0, "end": 1654.0, "text": " And lo and behold, that number, point three nine, is the same as this number,", "tokens": [400, 450, 293, 27234, 11, 300, 1230, 11, 935, 1045, 4949, 11, 307, 264, 912, 382, 341, 1230, 11], "temperature": 0.0, "avg_logprob": -0.14388790726661682, "compression_ratio": 1.4242424242424243, "no_speech_prob": 5.95503570366418e-06}, {"id": 228, "seek": 164400, "start": 1654.0, "end": 1671.0, "text": " because they're doing the same thing. So this one was kind of finding this particular user's embedding vector.", "tokens": [570, 436, 434, 884, 264, 912, 551, 13, 407, 341, 472, 390, 733, 295, 5006, 341, 1729, 4195, 311, 12240, 3584, 8062, 13], "temperature": 0.0, "avg_logprob": -0.14388790726661682, "compression_ratio": 1.4242424242424243, "no_speech_prob": 5.95503570366418e-06}, {"id": 229, "seek": 167100, "start": 1671.0, "end": 1677.0, "text": " This one is just doing a matrix multiply. And therefore, we know they are mathematically identical.", "tokens": [639, 472, 307, 445, 884, 257, 8141, 12972, 13, 400, 4412, 11, 321, 458, 436, 366, 44003, 14800, 13], "temperature": 0.0, "avg_logprob": -0.07710682448520455, "compression_ratio": 1.579646017699115, "no_speech_prob": 6.1440723584382795e-06}, {"id": 230, "seek": 167100, "start": 1677.0, "end": 1682.0, "text": " So let's lay that out again. So here's our final version.", "tokens": [407, 718, 311, 2360, 300, 484, 797, 13, 407, 510, 311, 527, 2572, 3037, 13], "temperature": 0.0, "avg_logprob": -0.07710682448520455, "compression_ratio": 1.579646017699115, "no_speech_prob": 6.1440723584382795e-06}, {"id": 231, "seek": 167100, "start": 1682.0, "end": 1688.0, "text": " This is the same weight matrices again, exactly the same. I've copied them over.", "tokens": [639, 307, 264, 912, 3364, 32284, 797, 11, 2293, 264, 912, 13, 286, 600, 25365, 552, 670, 13], "temperature": 0.0, "avg_logprob": -0.07710682448520455, "compression_ratio": 1.579646017699115, "no_speech_prob": 6.1440723584382795e-06}, {"id": 232, "seek": 167100, "start": 1688.0, "end": 1692.0, "text": " And here's those user IDs and movie IDs again.", "tokens": [400, 510, 311, 729, 4195, 48212, 293, 3169, 48212, 797, 13], "temperature": 0.0, "avg_logprob": -0.07710682448520455, "compression_ratio": 1.579646017699115, "no_speech_prob": 6.1440723584382795e-06}, {"id": 233, "seek": 167100, "start": 1692.0, "end": 1697.0, "text": " But this time I've laid them out just in a normal kind of tabular form,", "tokens": [583, 341, 565, 286, 600, 9897, 552, 484, 445, 294, 257, 2710, 733, 295, 4421, 1040, 1254, 11], "temperature": 0.0, "avg_logprob": -0.07710682448520455, "compression_ratio": 1.579646017699115, "no_speech_prob": 6.1440723584382795e-06}, {"id": 234, "seek": 169700, "start": 1697.0, "end": 1701.0, "text": " just like you would expect to see in the input to your model.", "tokens": [445, 411, 291, 576, 2066, 281, 536, 294, 264, 4846, 281, 428, 2316, 13], "temperature": 0.0, "avg_logprob": -0.07601572592047197, "compression_ratio": 1.4898989898989898, "no_speech_prob": 2.931042035925202e-05}, {"id": 235, "seek": 169700, "start": 1701.0, "end": 1712.0, "text": " And this time I've got exactly the same set of activations here that I had here.", "tokens": [400, 341, 565, 286, 600, 658, 2293, 264, 912, 992, 295, 2430, 763, 510, 300, 286, 632, 510, 13], "temperature": 0.0, "avg_logprob": -0.07601572592047197, "compression_ratio": 1.4898989898989898, "no_speech_prob": 2.931042035925202e-05}, {"id": 236, "seek": 169700, "start": 1712.0, "end": 1719.0, "text": " But in this case, I've calculated these activations using Excel's offset function, which is an array lookup, right?", "tokens": [583, 294, 341, 1389, 11, 286, 600, 15598, 613, 2430, 763, 1228, 19060, 311, 18687, 2445, 11, 597, 307, 364, 10225, 574, 1010, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.07601572592047197, "compression_ratio": 1.4898989898989898, "no_speech_prob": 2.931042035925202e-05}, {"id": 237, "seek": 169700, "start": 1719.0, "end": 1725.0, "text": " It says, find the first row of this.", "tokens": [467, 1619, 11, 915, 264, 700, 5386, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.07601572592047197, "compression_ratio": 1.4898989898989898, "no_speech_prob": 2.931042035925202e-05}, {"id": 238, "seek": 172500, "start": 1725.0, "end": 1732.0, "text": " So this is doing it as an array lookup. So this version is identical to this version.", "tokens": [407, 341, 307, 884, 309, 382, 364, 10225, 574, 1010, 13, 407, 341, 3037, 307, 14800, 281, 341, 3037, 13], "temperature": 0.0, "avg_logprob": -0.061583933376130606, "compression_ratio": 1.6763285024154588, "no_speech_prob": 3.218956044293009e-05}, {"id": 239, "seek": 172500, "start": 1732.0, "end": 1736.0, "text": " But obviously, it's much less memory intensive and much faster,", "tokens": [583, 2745, 11, 309, 311, 709, 1570, 4675, 18957, 293, 709, 4663, 11], "temperature": 0.0, "avg_logprob": -0.061583933376130606, "compression_ratio": 1.6763285024154588, "no_speech_prob": 3.218956044293009e-05}, {"id": 240, "seek": 172500, "start": 1736.0, "end": 1741.0, "text": " because I don't actually create the one hot encoded matrix and I don't actually do a matrix multiply,", "tokens": [570, 286, 500, 380, 767, 1884, 264, 472, 2368, 2058, 12340, 8141, 293, 286, 500, 380, 767, 360, 257, 8141, 12972, 11], "temperature": 0.0, "avg_logprob": -0.061583933376130606, "compression_ratio": 1.6763285024154588, "no_speech_prob": 3.218956044293009e-05}, {"id": 241, "seek": 172500, "start": 1741.0, "end": 1747.0, "text": " because that matrix multiply is nearly all multiplying by zero, which is a total waste of time.", "tokens": [570, 300, 8141, 12972, 307, 6217, 439, 30955, 538, 4018, 11, 597, 307, 257, 3217, 5964, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.061583933376130606, "compression_ratio": 1.6763285024154588, "no_speech_prob": 3.218956044293009e-05}, {"id": 242, "seek": 174700, "start": 1747.0, "end": 1756.0, "text": " So in other words, multiplying by a one hot encoded matrix is identical to doing an array lookup.", "tokens": [407, 294, 661, 2283, 11, 30955, 538, 257, 472, 2368, 2058, 12340, 8141, 307, 14800, 281, 884, 364, 10225, 574, 1010, 13], "temperature": 0.0, "avg_logprob": -0.06251195581947885, "compression_ratio": 1.7914438502673797, "no_speech_prob": 1.0129448128282093e-05}, {"id": 243, "seek": 174700, "start": 1756.0, "end": 1760.0, "text": " Therefore, we should always do the array lookup version.", "tokens": [7504, 11, 321, 820, 1009, 360, 264, 10225, 574, 1010, 3037, 13], "temperature": 0.0, "avg_logprob": -0.06251195581947885, "compression_ratio": 1.7914438502673797, "no_speech_prob": 1.0129448128282093e-05}, {"id": 244, "seek": 174700, "start": 1760.0, "end": 1765.0, "text": " And therefore, we have a specific way of doing we have a specific way of saying,", "tokens": [400, 4412, 11, 321, 362, 257, 2685, 636, 295, 884, 321, 362, 257, 2685, 636, 295, 1566, 11], "temperature": 0.0, "avg_logprob": -0.06251195581947885, "compression_ratio": 1.7914438502673797, "no_speech_prob": 1.0129448128282093e-05}, {"id": 245, "seek": 174700, "start": 1765.0, "end": 1771.0, "text": " I want to do a matrix multiplication by a one hot encoded matrix without ever actually creating it.", "tokens": [286, 528, 281, 360, 257, 8141, 27290, 538, 257, 472, 2368, 2058, 12340, 8141, 1553, 1562, 767, 4084, 309, 13], "temperature": 0.0, "avg_logprob": -0.06251195581947885, "compression_ratio": 1.7914438502673797, "no_speech_prob": 1.0129448128282093e-05}, {"id": 246, "seek": 177100, "start": 1771.0, "end": 1777.0, "text": " I'm just instead going to pass in a bunch of ints and pretend they're one hot encoded.", "tokens": [286, 478, 445, 2602, 516, 281, 1320, 294, 257, 3840, 295, 560, 82, 293, 11865, 436, 434, 472, 2368, 2058, 12340, 13], "temperature": 0.0, "avg_logprob": -0.09433260716890034, "compression_ratio": 1.5108695652173914, "no_speech_prob": 7.5278658187016845e-06}, {"id": 247, "seek": 177100, "start": 1777.0, "end": 1781.0, "text": " And that is called an embedding.", "tokens": [400, 300, 307, 1219, 364, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.09433260716890034, "compression_ratio": 1.5108695652173914, "no_speech_prob": 7.5278658187016845e-06}, {"id": 248, "seek": 177100, "start": 1781.0, "end": 1790.0, "text": " So you might have heard this word embedding all over the place, as if it's some magic advanced mathy thing.", "tokens": [407, 291, 1062, 362, 2198, 341, 1349, 12240, 3584, 439, 670, 264, 1081, 11, 382, 498, 309, 311, 512, 5585, 7339, 5221, 88, 551, 13], "temperature": 0.0, "avg_logprob": -0.09433260716890034, "compression_ratio": 1.5108695652173914, "no_speech_prob": 7.5278658187016845e-06}, {"id": 249, "seek": 177100, "start": 1790.0, "end": 1796.0, "text": " But embedding means look something up in an array.", "tokens": [583, 12240, 3584, 1355, 574, 746, 493, 294, 364, 10225, 13], "temperature": 0.0, "avg_logprob": -0.09433260716890034, "compression_ratio": 1.5108695652173914, "no_speech_prob": 7.5278658187016845e-06}, {"id": 250, "seek": 179600, "start": 1796.0, "end": 1806.0, "text": " But it's interesting to know that looking something up in an array is mathematically identical to doing a matrix product by a one hot encoded matrix.", "tokens": [583, 309, 311, 1880, 281, 458, 300, 1237, 746, 493, 294, 364, 10225, 307, 44003, 14800, 281, 884, 257, 8141, 1674, 538, 257, 472, 2368, 2058, 12340, 8141, 13], "temperature": 0.0, "avg_logprob": -0.06873261694814645, "compression_ratio": 1.6561264822134387, "no_speech_prob": 5.173810222913744e-06}, {"id": 251, "seek": 179600, "start": 1806.0, "end": 1814.0, "text": " And therefore, an embedding fits very nicely in our standard model of how neural networks work.", "tokens": [400, 4412, 11, 364, 12240, 3584, 9001, 588, 9594, 294, 527, 3832, 2316, 295, 577, 18161, 9590, 589, 13], "temperature": 0.0, "avg_logprob": -0.06873261694814645, "compression_ratio": 1.6561264822134387, "no_speech_prob": 5.173810222913744e-06}, {"id": 252, "seek": 179600, "start": 1814.0, "end": 1818.0, "text": " So now suddenly, it's as if we have another whole kind of layer.", "tokens": [407, 586, 5800, 11, 309, 311, 382, 498, 321, 362, 1071, 1379, 733, 295, 4583, 13], "temperature": 0.0, "avg_logprob": -0.06873261694814645, "compression_ratio": 1.6561264822134387, "no_speech_prob": 5.173810222913744e-06}, {"id": 253, "seek": 179600, "start": 1818.0, "end": 1821.0, "text": " It's a kind of layer where we get to look things up in an array.", "tokens": [467, 311, 257, 733, 295, 4583, 689, 321, 483, 281, 574, 721, 493, 294, 364, 10225, 13], "temperature": 0.0, "avg_logprob": -0.06873261694814645, "compression_ratio": 1.6561264822134387, "no_speech_prob": 5.173810222913744e-06}, {"id": 254, "seek": 179600, "start": 1821.0, "end": 1824.0, "text": " But we actually didn't do anything special.", "tokens": [583, 321, 767, 994, 380, 360, 1340, 2121, 13], "temperature": 0.0, "avg_logprob": -0.06873261694814645, "compression_ratio": 1.6561264822134387, "no_speech_prob": 5.173810222913744e-06}, {"id": 255, "seek": 182400, "start": 1824.0, "end": 1829.0, "text": " Right. We just added this computational shortcut, this thing called an embedding,", "tokens": [1779, 13, 492, 445, 3869, 341, 28270, 24822, 11, 341, 551, 1219, 364, 12240, 3584, 11], "temperature": 0.0, "avg_logprob": -0.08818763933683696, "compression_ratio": 1.6991525423728813, "no_speech_prob": 1.2411116586008575e-05}, {"id": 256, "seek": 182400, "start": 1829.0, "end": 1834.0, "text": " which is simply a fast and memory efficient way of multiplying by a one hot encoded matrix.", "tokens": [597, 307, 2935, 257, 2370, 293, 4675, 7148, 636, 295, 30955, 538, 257, 472, 2368, 2058, 12340, 8141, 13], "temperature": 0.0, "avg_logprob": -0.08818763933683696, "compression_ratio": 1.6991525423728813, "no_speech_prob": 1.2411116586008575e-05}, {"id": 257, "seek": 182400, "start": 1834.0, "end": 1841.0, "text": " OK. So this is really important because when you hear people say embedding,", "tokens": [2264, 13, 407, 341, 307, 534, 1021, 570, 562, 291, 1568, 561, 584, 12240, 3584, 11], "temperature": 0.0, "avg_logprob": -0.08818763933683696, "compression_ratio": 1.6991525423728813, "no_speech_prob": 1.2411116586008575e-05}, {"id": 258, "seek": 182400, "start": 1841.0, "end": 1846.0, "text": " you need to replace it in your head with an array lookup,", "tokens": [291, 643, 281, 7406, 309, 294, 428, 1378, 365, 364, 10225, 574, 1010, 11], "temperature": 0.0, "avg_logprob": -0.08818763933683696, "compression_ratio": 1.6991525423728813, "no_speech_prob": 1.2411116586008575e-05}, {"id": 259, "seek": 182400, "start": 1846.0, "end": 1853.0, "text": " which we know is mathematically identical to a matrix multiplied by a one hot encoded matrix.", "tokens": [597, 321, 458, 307, 44003, 14800, 281, 257, 8141, 17207, 538, 257, 472, 2368, 2058, 12340, 8141, 13], "temperature": 0.0, "avg_logprob": -0.08818763933683696, "compression_ratio": 1.6991525423728813, "no_speech_prob": 1.2411116586008575e-05}, {"id": 260, "seek": 185300, "start": 1853.0, "end": 1859.0, "text": " Here's the thing, though. It has kind of interesting semantics.", "tokens": [1692, 311, 264, 551, 11, 1673, 13, 467, 575, 733, 295, 1880, 4361, 45298, 13], "temperature": 0.0, "avg_logprob": -0.08896083955640917, "compression_ratio": 1.535, "no_speech_prob": 1.078285276889801e-05}, {"id": 261, "seek": 185300, "start": 1859.0, "end": 1863.0, "text": " Right. Because when you do multiply something by a one hot encoded matrix,", "tokens": [1779, 13, 1436, 562, 291, 360, 12972, 746, 538, 257, 472, 2368, 2058, 12340, 8141, 11], "temperature": 0.0, "avg_logprob": -0.08896083955640917, "compression_ratio": 1.535, "no_speech_prob": 1.078285276889801e-05}, {"id": 262, "seek": 185300, "start": 1863.0, "end": 1870.0, "text": " you get this nice feature where the rows of your weight matrix,", "tokens": [291, 483, 341, 1481, 4111, 689, 264, 13241, 295, 428, 3364, 8141, 11], "temperature": 0.0, "avg_logprob": -0.08896083955640917, "compression_ratio": 1.535, "no_speech_prob": 1.078285276889801e-05}, {"id": 263, "seek": 185300, "start": 1870.0, "end": 1879.0, "text": " the values only appear for row number one, for example, where you get user ID number one in your inputs.", "tokens": [264, 4190, 787, 4204, 337, 5386, 1230, 472, 11, 337, 1365, 11, 689, 291, 483, 4195, 7348, 1230, 472, 294, 428, 15743, 13], "temperature": 0.0, "avg_logprob": -0.08896083955640917, "compression_ratio": 1.535, "no_speech_prob": 1.078285276889801e-05}, {"id": 264, "seek": 187900, "start": 1879.0, "end": 1884.0, "text": " Right. So in other words, you kind of end up with this weight matrix", "tokens": [1779, 13, 407, 294, 661, 2283, 11, 291, 733, 295, 917, 493, 365, 341, 3364, 8141], "temperature": 0.0, "avg_logprob": -0.06421419336825987, "compression_ratio": 1.6208530805687205, "no_speech_prob": 6.240727998374496e-06}, {"id": 265, "seek": 187900, "start": 1884.0, "end": 1891.0, "text": " where certain rows of weights correspond to certain values of your input.", "tokens": [689, 1629, 13241, 295, 17443, 6805, 281, 1629, 4190, 295, 428, 4846, 13], "temperature": 0.0, "avg_logprob": -0.06421419336825987, "compression_ratio": 1.6208530805687205, "no_speech_prob": 6.240727998374496e-06}, {"id": 266, "seek": 187900, "start": 1891.0, "end": 1895.0, "text": " And that's pretty interesting. It's particularly interesting here,", "tokens": [400, 300, 311, 1238, 1880, 13, 467, 311, 4098, 1880, 510, 11], "temperature": 0.0, "avg_logprob": -0.06421419336825987, "compression_ratio": 1.6208530805687205, "no_speech_prob": 6.240727998374496e-06}, {"id": 267, "seek": 187900, "start": 1895.0, "end": 1901.0, "text": " because going back to a kind of most convenient way to look at this,", "tokens": [570, 516, 646, 281, 257, 733, 295, 881, 10851, 636, 281, 574, 412, 341, 11], "temperature": 0.0, "avg_logprob": -0.06421419336825987, "compression_ratio": 1.6208530805687205, "no_speech_prob": 6.240727998374496e-06}, {"id": 268, "seek": 187900, "start": 1901.0, "end": 1905.0, "text": " because the only way that we can calculate an output activation", "tokens": [570, 264, 787, 636, 300, 321, 393, 8873, 364, 5598, 24433], "temperature": 0.0, "avg_logprob": -0.06421419336825987, "compression_ratio": 1.6208530805687205, "no_speech_prob": 6.240727998374496e-06}, {"id": 269, "seek": 190500, "start": 1905.0, "end": 1909.0, "text": " is by doing a dot product of these two input vectors.", "tokens": [307, 538, 884, 257, 5893, 1674, 295, 613, 732, 4846, 18875, 13], "temperature": 0.0, "avg_logprob": -0.06145071161204371, "compression_ratio": 1.6555023923444976, "no_speech_prob": 4.539555447991006e-05}, {"id": 270, "seek": 190500, "start": 1909.0, "end": 1916.0, "text": " That means that they kind of have to correspond with each other.", "tokens": [663, 1355, 300, 436, 733, 295, 362, 281, 6805, 365, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.06145071161204371, "compression_ratio": 1.6555023923444976, "no_speech_prob": 4.539555447991006e-05}, {"id": 271, "seek": 190500, "start": 1916.0, "end": 1923.0, "text": " Right. Like there has to be some way of saying if this number for a user is high", "tokens": [1779, 13, 1743, 456, 575, 281, 312, 512, 636, 295, 1566, 498, 341, 1230, 337, 257, 4195, 307, 1090], "temperature": 0.0, "avg_logprob": -0.06145071161204371, "compression_ratio": 1.6555023923444976, "no_speech_prob": 4.539555447991006e-05}, {"id": 272, "seek": 190500, "start": 1923.0, "end": 1928.0, "text": " and this number for a movie is high, then the user will like the movie.", "tokens": [293, 341, 1230, 337, 257, 3169, 307, 1090, 11, 550, 264, 4195, 486, 411, 264, 3169, 13], "temperature": 0.0, "avg_logprob": -0.06145071161204371, "compression_ratio": 1.6555023923444976, "no_speech_prob": 4.539555447991006e-05}, {"id": 273, "seek": 190500, "start": 1928.0, "end": 1933.0, "text": " So the only way that can possibly make sense is if these numbers represent", "tokens": [407, 264, 787, 636, 300, 393, 6264, 652, 2020, 307, 498, 613, 3547, 2906], "temperature": 0.0, "avg_logprob": -0.06145071161204371, "compression_ratio": 1.6555023923444976, "no_speech_prob": 4.539555447991006e-05}, {"id": 274, "seek": 193300, "start": 1933.0, "end": 1939.0, "text": " features of personal taste and corresponding features of movies.", "tokens": [4122, 295, 2973, 3939, 293, 11760, 4122, 295, 6233, 13], "temperature": 0.0, "avg_logprob": -0.08348168553532781, "compression_ratio": 1.6704545454545454, "no_speech_prob": 1.644182157178875e-05}, {"id": 275, "seek": 193300, "start": 1939.0, "end": 1950.0, "text": " For example, the movie has John Travolta in it and user ID likes John Travolta,", "tokens": [1171, 1365, 11, 264, 3169, 575, 2619, 5403, 9646, 1328, 294, 309, 293, 4195, 7348, 5902, 2619, 5403, 9646, 1328, 11], "temperature": 0.0, "avg_logprob": -0.08348168553532781, "compression_ratio": 1.6704545454545454, "no_speech_prob": 1.644182157178875e-05}, {"id": 276, "seek": 193300, "start": 1950.0, "end": 1959.0, "text": " then you'll like this movie. OK. So like we're not actually deciding the rows mean anything.", "tokens": [550, 291, 603, 411, 341, 3169, 13, 2264, 13, 407, 411, 321, 434, 406, 767, 17990, 264, 13241, 914, 1340, 13], "temperature": 0.0, "avg_logprob": -0.08348168553532781, "compression_ratio": 1.6704545454545454, "no_speech_prob": 1.644182157178875e-05}, {"id": 277, "seek": 193300, "start": 1959.0, "end": 1961.0, "text": " We're not doing anything to make the rows mean anything.", "tokens": [492, 434, 406, 884, 1340, 281, 652, 264, 13241, 914, 1340, 13], "temperature": 0.0, "avg_logprob": -0.08348168553532781, "compression_ratio": 1.6704545454545454, "no_speech_prob": 1.644182157178875e-05}, {"id": 278, "seek": 196100, "start": 1961.0, "end": 1965.0, "text": " But the only way that this gradient descent could possibly come up with a good answer", "tokens": [583, 264, 787, 636, 300, 341, 16235, 23475, 727, 6264, 808, 493, 365, 257, 665, 1867], "temperature": 0.0, "avg_logprob": -0.044860440693544536, "compression_ratio": 1.6808510638297873, "no_speech_prob": 1.568882362334989e-05}, {"id": 279, "seek": 196100, "start": 1965.0, "end": 1971.0, "text": " is if it figures out what the aspects of movie taste are", "tokens": [307, 498, 309, 9624, 484, 437, 264, 7270, 295, 3169, 3939, 366], "temperature": 0.0, "avg_logprob": -0.044860440693544536, "compression_ratio": 1.6808510638297873, "no_speech_prob": 1.568882362334989e-05}, {"id": 280, "seek": 196100, "start": 1971.0, "end": 1974.0, "text": " and the corresponding features of movies are.", "tokens": [293, 264, 11760, 4122, 295, 6233, 366, 13], "temperature": 0.0, "avg_logprob": -0.044860440693544536, "compression_ratio": 1.6808510638297873, "no_speech_prob": 1.568882362334989e-05}, {"id": 281, "seek": 196100, "start": 1974.0, "end": 1982.0, "text": " So those underlying kind of features that appear are called latent factors or latent features.", "tokens": [407, 729, 14217, 733, 295, 4122, 300, 4204, 366, 1219, 48994, 6771, 420, 48994, 4122, 13], "temperature": 0.0, "avg_logprob": -0.044860440693544536, "compression_ratio": 1.6808510638297873, "no_speech_prob": 1.568882362334989e-05}, {"id": 282, "seek": 196100, "start": 1982.0, "end": 1985.0, "text": " They're these hidden things that were there all along,", "tokens": [814, 434, 613, 7633, 721, 300, 645, 456, 439, 2051, 11], "temperature": 0.0, "avg_logprob": -0.044860440693544536, "compression_ratio": 1.6808510638297873, "no_speech_prob": 1.568882362334989e-05}, {"id": 283, "seek": 196100, "start": 1985.0, "end": 1989.0, "text": " and once we train this neural net, they suddenly appear.", "tokens": [293, 1564, 321, 3847, 341, 18161, 2533, 11, 436, 5800, 4204, 13], "temperature": 0.0, "avg_logprob": -0.044860440693544536, "compression_ratio": 1.6808510638297873, "no_speech_prob": 1.568882362334989e-05}, {"id": 284, "seek": 198900, "start": 1989.0, "end": 1994.0, "text": " Now here's the problem. No one's going to like Battlefield Earth.", "tokens": [823, 510, 311, 264, 1154, 13, 883, 472, 311, 516, 281, 411, 41091, 4755, 13], "temperature": 0.0, "avg_logprob": -0.08714594492098181, "compression_ratio": 1.8237704918032787, "no_speech_prob": 1.2804909601982217e-05}, {"id": 285, "seek": 198900, "start": 1994.0, "end": 1999.0, "text": " It's not a good movie even though it has John Travolta in it.", "tokens": [467, 311, 406, 257, 665, 3169, 754, 1673, 309, 575, 2619, 5403, 9646, 1328, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.08714594492098181, "compression_ratio": 1.8237704918032787, "no_speech_prob": 1.2804909601982217e-05}, {"id": 286, "seek": 198900, "start": 1999.0, "end": 2001.0, "text": " So how are we going to deal with that?", "tokens": [407, 577, 366, 321, 516, 281, 2028, 365, 300, 30], "temperature": 0.0, "avg_logprob": -0.08714594492098181, "compression_ratio": 1.8237704918032787, "no_speech_prob": 1.2804909601982217e-05}, {"id": 287, "seek": 198900, "start": 2001.0, "end": 2004.0, "text": " Because there's this feature called I like John Travolta movies", "tokens": [1436, 456, 311, 341, 4111, 1219, 286, 411, 2619, 5403, 9646, 1328, 6233], "temperature": 0.0, "avg_logprob": -0.08714594492098181, "compression_ratio": 1.8237704918032787, "no_speech_prob": 1.2804909601982217e-05}, {"id": 288, "seek": 198900, "start": 2004.0, "end": 2007.0, "text": " and this feature called this movie has John Travolta.", "tokens": [293, 341, 4111, 1219, 341, 3169, 575, 2619, 5403, 9646, 1328, 13], "temperature": 0.0, "avg_logprob": -0.08714594492098181, "compression_ratio": 1.8237704918032787, "no_speech_prob": 1.2804909601982217e-05}, {"id": 289, "seek": 198900, "start": 2007.0, "end": 2010.0, "text": " And so this is now like you're going to like the movie.", "tokens": [400, 370, 341, 307, 586, 411, 291, 434, 516, 281, 411, 264, 3169, 13], "temperature": 0.0, "avg_logprob": -0.08714594492098181, "compression_ratio": 1.8237704918032787, "no_speech_prob": 1.2804909601982217e-05}, {"id": 290, "seek": 198900, "start": 2010.0, "end": 2014.0, "text": " But we need to have some way to say unless it's Battlefield Earth", "tokens": [583, 321, 643, 281, 362, 512, 636, 281, 584, 5969, 309, 311, 41091, 4755], "temperature": 0.0, "avg_logprob": -0.08714594492098181, "compression_ratio": 1.8237704918032787, "no_speech_prob": 1.2804909601982217e-05}, {"id": 291, "seek": 198900, "start": 2014.0, "end": 2017.0, "text": " or you're a Scientologist, either one.", "tokens": [420, 291, 434, 257, 18944, 9201, 11, 2139, 472, 13], "temperature": 0.0, "avg_logprob": -0.08714594492098181, "compression_ratio": 1.8237704918032787, "no_speech_prob": 1.2804909601982217e-05}, {"id": 292, "seek": 201700, "start": 2017.0, "end": 2022.0, "text": " So how do we do that? We need to add in bias.", "tokens": [407, 577, 360, 321, 360, 300, 30, 492, 643, 281, 909, 294, 12577, 13], "temperature": 0.0, "avg_logprob": -0.1281111613813653, "compression_ratio": 1.5898876404494382, "no_speech_prob": 3.5354041756363586e-05}, {"id": 293, "seek": 201700, "start": 2022.0, "end": 2027.0, "text": " So here is the same thing again.", "tokens": [407, 510, 307, 264, 912, 551, 797, 13], "temperature": 0.0, "avg_logprob": -0.1281111613813653, "compression_ratio": 1.5898876404494382, "no_speech_prob": 3.5354041756363586e-05}, {"id": 294, "seek": 201700, "start": 2027.0, "end": 2030.0, "text": " Same weight matrix, sorry, not the same weight matrix.", "tokens": [10635, 3364, 8141, 11, 2597, 11, 406, 264, 912, 3364, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1281111613813653, "compression_ratio": 1.5898876404494382, "no_speech_prob": 3.5354041756363586e-05}, {"id": 295, "seek": 201700, "start": 2030.0, "end": 2036.0, "text": " He's the same construct, same shape of everything.", "tokens": [634, 311, 264, 912, 7690, 11, 912, 3909, 295, 1203, 13], "temperature": 0.0, "avg_logprob": -0.1281111613813653, "compression_ratio": 1.5898876404494382, "no_speech_prob": 3.5354041756363586e-05}, {"id": 296, "seek": 201700, "start": 2036.0, "end": 2039.0, "text": " But this time we've got an extra row.", "tokens": [583, 341, 565, 321, 600, 658, 364, 2857, 5386, 13], "temperature": 0.0, "avg_logprob": -0.1281111613813653, "compression_ratio": 1.5898876404494382, "no_speech_prob": 3.5354041756363586e-05}, {"id": 297, "seek": 201700, "start": 2039.0, "end": 2046.0, "text": " So now this is not just the matrix product of that and that,", "tokens": [407, 586, 341, 307, 406, 445, 264, 8141, 1674, 295, 300, 293, 300, 11], "temperature": 0.0, "avg_logprob": -0.1281111613813653, "compression_ratio": 1.5898876404494382, "no_speech_prob": 3.5354041756363586e-05}, {"id": 298, "seek": 204600, "start": 2046.0, "end": 2051.0, "text": " but I'm also adding on this number and this number,", "tokens": [457, 286, 478, 611, 5127, 322, 341, 1230, 293, 341, 1230, 11], "temperature": 0.0, "avg_logprob": -0.07546207484077005, "compression_ratio": 1.8523809523809525, "no_speech_prob": 2.3922415493871085e-05}, {"id": 299, "seek": 204600, "start": 2051.0, "end": 2056.0, "text": " which means now each movie can have an overall,", "tokens": [597, 1355, 586, 1184, 3169, 393, 362, 364, 4787, 11], "temperature": 0.0, "avg_logprob": -0.07546207484077005, "compression_ratio": 1.8523809523809525, "no_speech_prob": 2.3922415493871085e-05}, {"id": 300, "seek": 204600, "start": 2056.0, "end": 2059.0, "text": " this is a great movie versus this isn't a great movie,", "tokens": [341, 307, 257, 869, 3169, 5717, 341, 1943, 380, 257, 869, 3169, 11], "temperature": 0.0, "avg_logprob": -0.07546207484077005, "compression_ratio": 1.8523809523809525, "no_speech_prob": 2.3922415493871085e-05}, {"id": 301, "seek": 204600, "start": 2059.0, "end": 2061.0, "text": " and every user can have an overall,", "tokens": [293, 633, 4195, 393, 362, 364, 4787, 11], "temperature": 0.0, "avg_logprob": -0.07546207484077005, "compression_ratio": 1.8523809523809525, "no_speech_prob": 2.3922415493871085e-05}, {"id": 302, "seek": 204600, "start": 2061.0, "end": 2066.0, "text": " this user rates movies highly or this user doesn't rate movies highly.", "tokens": [341, 4195, 6846, 6233, 5405, 420, 341, 4195, 1177, 380, 3314, 6233, 5405, 13], "temperature": 0.0, "avg_logprob": -0.07546207484077005, "compression_ratio": 1.8523809523809525, "no_speech_prob": 2.3922415493871085e-05}, {"id": 303, "seek": 204600, "start": 2066.0, "end": 2068.0, "text": " So that's called the bias.", "tokens": [407, 300, 311, 1219, 264, 12577, 13], "temperature": 0.0, "avg_logprob": -0.07546207484077005, "compression_ratio": 1.8523809523809525, "no_speech_prob": 2.3922415493871085e-05}, {"id": 304, "seek": 204600, "start": 2068.0, "end": 2071.0, "text": " So this is hopefully going to look very familiar, right?", "tokens": [407, 341, 307, 4696, 516, 281, 574, 588, 4963, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.07546207484077005, "compression_ratio": 1.8523809523809525, "no_speech_prob": 2.3922415493871085e-05}, {"id": 305, "seek": 204600, "start": 2071.0, "end": 2074.0, "text": " This is the same usual linear model concept", "tokens": [639, 307, 264, 912, 7713, 8213, 2316, 3410], "temperature": 0.0, "avg_logprob": -0.07546207484077005, "compression_ratio": 1.8523809523809525, "no_speech_prob": 2.3922415493871085e-05}, {"id": 306, "seek": 207400, "start": 2074.0, "end": 2076.0, "text": " or linear layer concept from a neural net", "tokens": [420, 8213, 4583, 3410, 490, 257, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.07648345880341112, "compression_ratio": 1.6704980842911878, "no_speech_prob": 5.920502735534683e-05}, {"id": 307, "seek": 207400, "start": 2076.0, "end": 2079.0, "text": " that you have a matrix product and a bias.", "tokens": [300, 291, 362, 257, 8141, 1674, 293, 257, 12577, 13], "temperature": 0.0, "avg_logprob": -0.07648345880341112, "compression_ratio": 1.6704980842911878, "no_speech_prob": 5.920502735534683e-05}, {"id": 308, "seek": 207400, "start": 2079.0, "end": 2083.0, "text": " And remember from lesson two, the lesson two SGD notebook,", "tokens": [400, 1604, 490, 6898, 732, 11, 264, 6898, 732, 34520, 35, 21060, 11], "temperature": 0.0, "avg_logprob": -0.07648345880341112, "compression_ratio": 1.6704980842911878, "no_speech_prob": 5.920502735534683e-05}, {"id": 309, "seek": 207400, "start": 2083.0, "end": 2086.0, "text": " you never actually need a bias.", "tokens": [291, 1128, 767, 643, 257, 12577, 13], "temperature": 0.0, "avg_logprob": -0.07648345880341112, "compression_ratio": 1.6704980842911878, "no_speech_prob": 5.920502735534683e-05}, {"id": 310, "seek": 207400, "start": 2086.0, "end": 2089.0, "text": " You could always just add a column of ones to your input data,", "tokens": [509, 727, 1009, 445, 909, 257, 7738, 295, 2306, 281, 428, 4846, 1412, 11], "temperature": 0.0, "avg_logprob": -0.07648345880341112, "compression_ratio": 1.6704980842911878, "no_speech_prob": 5.920502735534683e-05}, {"id": 311, "seek": 207400, "start": 2089.0, "end": 2092.0, "text": " and then that gives you bias for free.", "tokens": [293, 550, 300, 2709, 291, 12577, 337, 1737, 13], "temperature": 0.0, "avg_logprob": -0.07648345880341112, "compression_ratio": 1.6704980842911878, "no_speech_prob": 5.920502735534683e-05}, {"id": 312, "seek": 207400, "start": 2092.0, "end": 2094.0, "text": " But that's pretty inefficient, right?", "tokens": [583, 300, 311, 1238, 43495, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.07648345880341112, "compression_ratio": 1.6704980842911878, "no_speech_prob": 5.920502735534683e-05}, {"id": 313, "seek": 207400, "start": 2094.0, "end": 2100.0, "text": " So in practice, all neural networks library explicitly have a concept of bias.", "tokens": [407, 294, 3124, 11, 439, 18161, 9590, 6405, 20803, 362, 257, 3410, 295, 12577, 13], "temperature": 0.0, "avg_logprob": -0.07648345880341112, "compression_ratio": 1.6704980842911878, "no_speech_prob": 5.920502735534683e-05}, {"id": 314, "seek": 207400, "start": 2100.0, "end": 2102.0, "text": " We don't actually add the column of ones.", "tokens": [492, 500, 380, 767, 909, 264, 7738, 295, 2306, 13], "temperature": 0.0, "avg_logprob": -0.07648345880341112, "compression_ratio": 1.6704980842911878, "no_speech_prob": 5.920502735534683e-05}, {"id": 315, "seek": 210200, "start": 2102.0, "end": 2104.0, "text": " So what does that do?", "tokens": [407, 437, 775, 300, 360, 30], "temperature": 0.0, "avg_logprob": -0.10688893513012958, "compression_ratio": 1.475, "no_speech_prob": 2.2826679924037308e-05}, {"id": 316, "seek": 210200, "start": 2104.0, "end": 2106.0, "text": " Well, just before I came in today,", "tokens": [1042, 11, 445, 949, 286, 1361, 294, 965, 11], "temperature": 0.0, "avg_logprob": -0.10688893513012958, "compression_ratio": 1.475, "no_speech_prob": 2.2826679924037308e-05}, {"id": 317, "seek": 210200, "start": 2106.0, "end": 2112.0, "text": " I ran tools solver or data solver on this as well,", "tokens": [286, 5872, 3873, 1404, 331, 420, 1412, 1404, 331, 322, 341, 382, 731, 11], "temperature": 0.0, "avg_logprob": -0.10688893513012958, "compression_ratio": 1.475, "no_speech_prob": 2.2826679924037308e-05}, {"id": 318, "seek": 210200, "start": 2112.0, "end": 2114.0, "text": " and we can check the RMSE.", "tokens": [293, 321, 393, 1520, 264, 23790, 5879, 13], "temperature": 0.0, "avg_logprob": -0.10688893513012958, "compression_ratio": 1.475, "no_speech_prob": 2.2826679924037308e-05}, {"id": 319, "seek": 210200, "start": 2114.0, "end": 2118.0, "text": " And so the root means grid here is 0.32", "tokens": [400, 370, 264, 5593, 1355, 10748, 510, 307, 1958, 13, 11440], "temperature": 0.0, "avg_logprob": -0.10688893513012958, "compression_ratio": 1.475, "no_speech_prob": 2.2826679924037308e-05}, {"id": 320, "seek": 210200, "start": 2118.0, "end": 2124.0, "text": " versus the version without bias was 0.39.", "tokens": [5717, 264, 3037, 1553, 12577, 390, 1958, 13, 12493, 13], "temperature": 0.0, "avg_logprob": -0.10688893513012958, "compression_ratio": 1.475, "no_speech_prob": 2.2826679924037308e-05}, {"id": 321, "seek": 210200, "start": 2124.0, "end": 2129.0, "text": " Okay, so you can see that this slightly better model", "tokens": [1033, 11, 370, 291, 393, 536, 300, 341, 4748, 1101, 2316], "temperature": 0.0, "avg_logprob": -0.10688893513012958, "compression_ratio": 1.475, "no_speech_prob": 2.2826679924037308e-05}, {"id": 322, "seek": 210200, "start": 2129.0, "end": 2130.0, "text": " gives us a better result.", "tokens": [2709, 505, 257, 1101, 1874, 13], "temperature": 0.0, "avg_logprob": -0.10688893513012958, "compression_ratio": 1.475, "no_speech_prob": 2.2826679924037308e-05}, {"id": 323, "seek": 213000, "start": 2130.0, "end": 2135.0, "text": " And it's better because it's giving both more flexibility, right?", "tokens": [400, 309, 311, 1101, 570, 309, 311, 2902, 1293, 544, 12635, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.10886007485930453, "compression_ratio": 1.5851528384279476, "no_speech_prob": 4.610930409398861e-05}, {"id": 324, "seek": 213000, "start": 2135.0, "end": 2138.0, "text": " And it's also just makes sense semantically", "tokens": [400, 309, 311, 611, 445, 1669, 2020, 4361, 49505], "temperature": 0.0, "avg_logprob": -0.10886007485930453, "compression_ratio": 1.5851528384279476, "no_speech_prob": 4.610930409398861e-05}, {"id": 325, "seek": 213000, "start": 2138.0, "end": 2141.0, "text": " that you need to be able to say,", "tokens": [300, 291, 643, 281, 312, 1075, 281, 584, 11], "temperature": 0.0, "avg_logprob": -0.10886007485930453, "compression_ratio": 1.5851528384279476, "no_speech_prob": 4.610930409398861e-05}, {"id": 326, "seek": 213000, "start": 2141.0, "end": 2146.0, "text": " whether I like the movie is not just about the combination of what actors it has", "tokens": [1968, 286, 411, 264, 3169, 307, 406, 445, 466, 264, 6562, 295, 437, 10037, 309, 575], "temperature": 0.0, "avg_logprob": -0.10886007485930453, "compression_ratio": 1.5851528384279476, "no_speech_prob": 4.610930409398861e-05}, {"id": 327, "seek": 213000, "start": 2146.0, "end": 2148.0, "text": " and whether it's dialogue driven and how much action is in it,", "tokens": [293, 1968, 309, 311, 10221, 9555, 293, 577, 709, 3069, 307, 294, 309, 11], "temperature": 0.0, "avg_logprob": -0.10886007485930453, "compression_ratio": 1.5851528384279476, "no_speech_prob": 4.610930409398861e-05}, {"id": 328, "seek": 213000, "start": 2148.0, "end": 2150.0, "text": " but just is it a good movie?", "tokens": [457, 445, 307, 309, 257, 665, 3169, 30], "temperature": 0.0, "avg_logprob": -0.10886007485930453, "compression_ratio": 1.5851528384279476, "no_speech_prob": 4.610930409398861e-05}, {"id": 329, "seek": 213000, "start": 2150.0, "end": 2155.0, "text": " Okay, or am I somebody who rates movies highly?", "tokens": [1033, 11, 420, 669, 286, 2618, 567, 6846, 6233, 5405, 30], "temperature": 0.0, "avg_logprob": -0.10886007485930453, "compression_ratio": 1.5851528384279476, "no_speech_prob": 4.610930409398861e-05}, {"id": 330, "seek": 215500, "start": 2155.0, "end": 2164.0, "text": " Okay, so there's all the pieces of this collaborative filtering model.", "tokens": [1033, 11, 370, 456, 311, 439, 264, 3755, 295, 341, 16555, 30822, 2316, 13], "temperature": 0.0, "avg_logprob": -0.18339836597442627, "compression_ratio": 1.3846153846153846, "no_speech_prob": 4.538439679890871e-05}, {"id": 331, "seek": 215500, "start": 2164.0, "end": 2167.0, "text": " How are we going, Sanchezco? Any questions?", "tokens": [1012, 366, 321, 516, 11, 5271, 30196, 1291, 30, 2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.18339836597442627, "compression_ratio": 1.3846153846153846, "no_speech_prob": 4.538439679890871e-05}, {"id": 332, "seek": 215500, "start": 2167.0, "end": 2171.0, "text": " We have three questions. Okay.", "tokens": [492, 362, 1045, 1651, 13, 1033, 13], "temperature": 0.0, "avg_logprob": -0.18339836597442627, "compression_ratio": 1.3846153846153846, "no_speech_prob": 4.538439679890871e-05}, {"id": 333, "seek": 215500, "start": 2171.0, "end": 2175.0, "text": " Okay, so our first question then is,", "tokens": [1033, 11, 370, 527, 700, 1168, 550, 307, 11], "temperature": 0.0, "avg_logprob": -0.18339836597442627, "compression_ratio": 1.3846153846153846, "no_speech_prob": 4.538439679890871e-05}, {"id": 334, "seek": 215500, "start": 2175.0, "end": 2179.0, "text": " when we load a pre-trained model,", "tokens": [562, 321, 3677, 257, 659, 12, 17227, 2001, 2316, 11], "temperature": 0.0, "avg_logprob": -0.18339836597442627, "compression_ratio": 1.3846153846153846, "no_speech_prob": 4.538439679890871e-05}, {"id": 335, "seek": 217900, "start": 2179.0, "end": 2185.0, "text": " can we explore the activation grids to see what they might be good at recognizing?", "tokens": [393, 321, 6839, 264, 24433, 677, 3742, 281, 536, 437, 436, 1062, 312, 665, 412, 18538, 30], "temperature": 0.0, "avg_logprob": -0.07729291915893555, "compression_ratio": 1.6103286384976525, "no_speech_prob": 0.00010883273353101686}, {"id": 336, "seek": 217900, "start": 2185.0, "end": 2192.0, "text": " Yes, you can. And we will learn how to, should be in the next lesson.", "tokens": [1079, 11, 291, 393, 13, 400, 321, 486, 1466, 577, 281, 11, 820, 312, 294, 264, 958, 6898, 13], "temperature": 0.0, "avg_logprob": -0.07729291915893555, "compression_ratio": 1.6103286384976525, "no_speech_prob": 0.00010883273353101686}, {"id": 337, "seek": 217900, "start": 2192.0, "end": 2198.0, "text": " Can we have an explanation of what the first argument in fit one cycle actually represents?", "tokens": [1664, 321, 362, 364, 10835, 295, 437, 264, 700, 6770, 294, 3318, 472, 6586, 767, 8855, 30], "temperature": 0.0, "avg_logprob": -0.07729291915893555, "compression_ratio": 1.6103286384976525, "no_speech_prob": 0.00010883273353101686}, {"id": 338, "seek": 217900, "start": 2198.0, "end": 2200.0, "text": " Is it equivalent to an epoch? Yes.", "tokens": [1119, 309, 10344, 281, 364, 30992, 339, 30, 1079, 13], "temperature": 0.0, "avg_logprob": -0.07729291915893555, "compression_ratio": 1.6103286384976525, "no_speech_prob": 0.00010883273353101686}, {"id": 339, "seek": 217900, "start": 2200.0, "end": 2207.0, "text": " The first argument to fit one cycle or fit is number of epochs.", "tokens": [440, 700, 6770, 281, 3318, 472, 6586, 420, 3318, 307, 1230, 295, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.07729291915893555, "compression_ratio": 1.6103286384976525, "no_speech_prob": 0.00010883273353101686}, {"id": 340, "seek": 220700, "start": 2207.0, "end": 2215.0, "text": " In other words, an epoch is looking at every input once.", "tokens": [682, 661, 2283, 11, 364, 30992, 339, 307, 1237, 412, 633, 4846, 1564, 13], "temperature": 0.0, "avg_logprob": -0.07658916496368776, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.140040543163195e-05}, {"id": 341, "seek": 220700, "start": 2215.0, "end": 2221.0, "text": " So if you do 10 epochs, you're looking at every input 10 times.", "tokens": [407, 498, 291, 360, 1266, 30992, 28346, 11, 291, 434, 1237, 412, 633, 4846, 1266, 1413, 13], "temperature": 0.0, "avg_logprob": -0.07658916496368776, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.140040543163195e-05}, {"id": 342, "seek": 220700, "start": 2221.0, "end": 2227.0, "text": " And so there's a chance you might start overfitting if you've got lots and lots of parameters and a high learning rate.", "tokens": [400, 370, 456, 311, 257, 2931, 291, 1062, 722, 670, 69, 2414, 498, 291, 600, 658, 3195, 293, 3195, 295, 9834, 293, 257, 1090, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.07658916496368776, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.140040543163195e-05}, {"id": 343, "seek": 220700, "start": 2227.0, "end": 2232.0, "text": " If you only do one epoch, it's impossible to overfit.", "tokens": [759, 291, 787, 360, 472, 30992, 339, 11, 309, 311, 6243, 281, 670, 6845, 13], "temperature": 0.0, "avg_logprob": -0.07658916496368776, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.140040543163195e-05}, {"id": 344, "seek": 223200, "start": 2232.0, "end": 2239.0, "text": " So that's why it's kind of useful to remember how many epochs you're doing.", "tokens": [407, 300, 311, 983, 309, 311, 733, 295, 4420, 281, 1604, 577, 867, 30992, 28346, 291, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.0631767851965768, "compression_ratio": 1.5989583333333333, "no_speech_prob": 5.0144844863098115e-06}, {"id": 345, "seek": 223200, "start": 2239.0, "end": 2245.0, "text": " Can we have an explanation? What is an affine function?", "tokens": [1664, 321, 362, 364, 10835, 30, 708, 307, 364, 2096, 533, 2445, 30], "temperature": 0.0, "avg_logprob": -0.0631767851965768, "compression_ratio": 1.5989583333333333, "no_speech_prob": 5.0144844863098115e-06}, {"id": 346, "seek": 223200, "start": 2245.0, "end": 2250.0, "text": " An affine function is a linear function.", "tokens": [1107, 2096, 533, 2445, 307, 257, 8213, 2445, 13], "temperature": 0.0, "avg_logprob": -0.0631767851965768, "compression_ratio": 1.5989583333333333, "no_speech_prob": 5.0144844863098115e-06}, {"id": 347, "seek": 223200, "start": 2250.0, "end": 2253.0, "text": " I don't know if we need much more detail than that.", "tokens": [286, 500, 380, 458, 498, 321, 643, 709, 544, 2607, 813, 300, 13], "temperature": 0.0, "avg_logprob": -0.0631767851965768, "compression_ratio": 1.5989583333333333, "no_speech_prob": 5.0144844863098115e-06}, {"id": 348, "seek": 223200, "start": 2253.0, "end": 2260.0, "text": " If you're multiplying things together and adding them up, it's an affine function.", "tokens": [759, 291, 434, 30955, 721, 1214, 293, 5127, 552, 493, 11, 309, 311, 364, 2096, 533, 2445, 13], "temperature": 0.0, "avg_logprob": -0.0631767851965768, "compression_ratio": 1.5989583333333333, "no_speech_prob": 5.0144844863098115e-06}, {"id": 349, "seek": 226000, "start": 2260.0, "end": 2267.0, "text": " I'm not going to bother with the exact mathematical definition, partly because I'm a terrible mathematician and partly because it doesn't matter.", "tokens": [286, 478, 406, 516, 281, 8677, 365, 264, 1900, 18894, 7123, 11, 17031, 570, 286, 478, 257, 6237, 48281, 293, 17031, 570, 309, 1177, 380, 1871, 13], "temperature": 0.0, "avg_logprob": -0.044767809343767596, "compression_ratio": 1.78125, "no_speech_prob": 6.708542059641331e-05}, {"id": 350, "seek": 226000, "start": 2267.0, "end": 2272.0, "text": " But if you just remember that you're multiplying things together and then adding them up, that's the most important thing.", "tokens": [583, 498, 291, 445, 1604, 300, 291, 434, 30955, 721, 1214, 293, 550, 5127, 552, 493, 11, 300, 311, 264, 881, 1021, 551, 13], "temperature": 0.0, "avg_logprob": -0.044767809343767596, "compression_ratio": 1.78125, "no_speech_prob": 6.708542059641331e-05}, {"id": 351, "seek": 226000, "start": 2272.0, "end": 2273.0, "text": " It's linear.", "tokens": [467, 311, 8213, 13], "temperature": 0.0, "avg_logprob": -0.044767809343767596, "compression_ratio": 1.78125, "no_speech_prob": 6.708542059641331e-05}, {"id": 352, "seek": 226000, "start": 2273.0, "end": 2279.0, "text": " And therefore, if you put an affine function on top of an affine function, that's just another affine function.", "tokens": [400, 4412, 11, 498, 291, 829, 364, 2096, 533, 2445, 322, 1192, 295, 364, 2096, 533, 2445, 11, 300, 311, 445, 1071, 2096, 533, 2445, 13], "temperature": 0.0, "avg_logprob": -0.044767809343767596, "compression_ratio": 1.78125, "no_speech_prob": 6.708542059641331e-05}, {"id": 353, "seek": 226000, "start": 2279.0, "end": 2281.0, "text": " You haven't won anything at all.", "tokens": [509, 2378, 380, 1582, 1340, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.044767809343767596, "compression_ratio": 1.78125, "no_speech_prob": 6.708542059641331e-05}, {"id": 354, "seek": 226000, "start": 2281.0, "end": 2283.0, "text": " That's a total waste of time.", "tokens": [663, 311, 257, 3217, 5964, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.044767809343767596, "compression_ratio": 1.78125, "no_speech_prob": 6.708542059641331e-05}, {"id": 355, "seek": 228300, "start": 2283.0, "end": 2293.0, "text": " So you need to sandwich it with any kind of non-linearity pretty much works, including replacing the negatives with zeros, which we call ReLU.", "tokens": [407, 291, 643, 281, 11141, 309, 365, 604, 733, 295, 2107, 12, 1889, 17409, 1238, 709, 1985, 11, 3009, 19139, 264, 40019, 365, 35193, 11, 597, 321, 818, 1300, 43, 52, 13], "temperature": 0.0, "avg_logprob": -0.11448442402170665, "compression_ratio": 1.5, "no_speech_prob": 1.3419255083135795e-05}, {"id": 356, "seek": 228300, "start": 2293.0, "end": 2301.0, "text": " So if you draw affine ReLU, affine ReLU, affine ReLU, you have a deep neural network.", "tokens": [407, 498, 291, 2642, 2096, 533, 1300, 43, 52, 11, 2096, 533, 1300, 43, 52, 11, 2096, 533, 1300, 43, 52, 11, 291, 362, 257, 2452, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.11448442402170665, "compression_ratio": 1.5, "no_speech_prob": 1.3419255083135795e-05}, {"id": 357, "seek": 230100, "start": 2301.0, "end": 2315.0, "text": " OK, so let's go back to the collaborative filtering notebook and this time we're going to grab the whole MovieLens 100K data set.", "tokens": [2264, 11, 370, 718, 311, 352, 646, 281, 264, 16555, 30822, 21060, 293, 341, 565, 321, 434, 516, 281, 4444, 264, 1379, 28766, 43, 694, 2319, 42, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.13872569799423218, "compression_ratio": 1.391812865497076, "no_speech_prob": 1.9523395167198032e-05}, {"id": 358, "seek": 230100, "start": 2315.0, "end": 2320.0, "text": " There's also a 20 million data set, by the way.", "tokens": [821, 311, 611, 257, 945, 2459, 1412, 992, 11, 538, 264, 636, 13], "temperature": 0.0, "avg_logprob": -0.13872569799423218, "compression_ratio": 1.391812865497076, "no_speech_prob": 1.9523395167198032e-05}, {"id": 359, "seek": 230100, "start": 2320.0, "end": 2326.0, "text": " So really great project made by this group called GroupLens.", "tokens": [407, 534, 869, 1716, 1027, 538, 341, 1594, 1219, 10500, 43, 694, 13], "temperature": 0.0, "avg_logprob": -0.13872569799423218, "compression_ratio": 1.391812865497076, "no_speech_prob": 1.9523395167198032e-05}, {"id": 360, "seek": 232600, "start": 2326.0, "end": 2332.0, "text": " They actually update the MovieLens data sets on a regular basis, but they helpfully provide the original one.", "tokens": [814, 767, 5623, 264, 28766, 43, 694, 1412, 6352, 322, 257, 3890, 5143, 11, 457, 436, 854, 2277, 2893, 264, 3380, 472, 13], "temperature": 0.0, "avg_logprob": -0.08014655858278275, "compression_ratio": 1.8041958041958042, "no_speech_prob": 4.83159747091122e-05}, {"id": 361, "seek": 232600, "start": 2332.0, "end": 2343.0, "text": " And we're going to use the original one because that means that we can compare to baselines because everybody basically they say, hey, if you're going to compare the baselines, make sure you all use the same data set.", "tokens": [400, 321, 434, 516, 281, 764, 264, 3380, 472, 570, 300, 1355, 300, 321, 393, 6794, 281, 987, 9173, 570, 2201, 1936, 436, 584, 11, 4177, 11, 498, 291, 434, 516, 281, 6794, 264, 987, 9173, 11, 652, 988, 291, 439, 764, 264, 912, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.08014655858278275, "compression_ratio": 1.8041958041958042, "no_speech_prob": 4.83159747091122e-05}, {"id": 362, "seek": 232600, "start": 2343.0, "end": 2344.0, "text": " Here's the one you should use.", "tokens": [1692, 311, 264, 472, 291, 820, 764, 13], "temperature": 0.0, "avg_logprob": -0.08014655858278275, "compression_ratio": 1.8041958041958042, "no_speech_prob": 4.83159747091122e-05}, {"id": 363, "seek": 232600, "start": 2344.0, "end": 2349.0, "text": " Unfortunately, it means that we're going to be restricted to movies that are before 1998.", "tokens": [8590, 11, 309, 1355, 300, 321, 434, 516, 281, 312, 20608, 281, 6233, 300, 366, 949, 21404, 13], "temperature": 0.0, "avg_logprob": -0.08014655858278275, "compression_ratio": 1.8041958041958042, "no_speech_prob": 4.83159747091122e-05}, {"id": 364, "seek": 232600, "start": 2349.0, "end": 2352.0, "text": " So maybe you won't have seen them all.", "tokens": [407, 1310, 291, 1582, 380, 362, 1612, 552, 439, 13], "temperature": 0.0, "avg_logprob": -0.08014655858278275, "compression_ratio": 1.8041958041958042, "no_speech_prob": 4.83159747091122e-05}, {"id": 365, "seek": 232600, "start": 2352.0, "end": 2354.0, "text": " But that's the price we pay.", "tokens": [583, 300, 311, 264, 3218, 321, 1689, 13], "temperature": 0.0, "avg_logprob": -0.08014655858278275, "compression_ratio": 1.8041958041958042, "no_speech_prob": 4.83159747091122e-05}, {"id": 366, "seek": 235400, "start": 2354.0, "end": 2363.0, "text": " You can replace this with ML Latest when you download it and use it if you want to play around with movies that are up to date.", "tokens": [509, 393, 7406, 341, 365, 21601, 7354, 377, 562, 291, 5484, 309, 293, 764, 309, 498, 291, 528, 281, 862, 926, 365, 6233, 300, 366, 493, 281, 4002, 13], "temperature": 0.0, "avg_logprob": -0.09152401380302493, "compression_ratio": 1.6031128404669261, "no_speech_prob": 2.9769986213068478e-05}, {"id": 367, "seek": 235400, "start": 2363.0, "end": 2370.0, "text": " OK, the original MovieLens data set, the more recent ones are in a CSV file.", "tokens": [2264, 11, 264, 3380, 28766, 43, 694, 1412, 992, 11, 264, 544, 5162, 2306, 366, 294, 257, 48814, 3991, 13], "temperature": 0.0, "avg_logprob": -0.09152401380302493, "compression_ratio": 1.6031128404669261, "no_speech_prob": 2.9769986213068478e-05}, {"id": 368, "seek": 235400, "start": 2370.0, "end": 2372.0, "text": " It's super convenient to use.", "tokens": [467, 311, 1687, 10851, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.09152401380302493, "compression_ratio": 1.6031128404669261, "no_speech_prob": 2.9769986213068478e-05}, {"id": 369, "seek": 235400, "start": 2372.0, "end": 2375.0, "text": " The original one is a slightly messy.", "tokens": [440, 3380, 472, 307, 257, 4748, 16191, 13], "temperature": 0.0, "avg_logprob": -0.09152401380302493, "compression_ratio": 1.6031128404669261, "no_speech_prob": 2.9769986213068478e-05}, {"id": 370, "seek": 235400, "start": 2375.0, "end": 2377.0, "text": " First of all, they don't use commas for delimiters.", "tokens": [2386, 295, 439, 11, 436, 500, 380, 764, 800, 296, 337, 1103, 332, 270, 433, 13], "temperature": 0.0, "avg_logprob": -0.09152401380302493, "compression_ratio": 1.6031128404669261, "no_speech_prob": 2.9769986213068478e-05}, {"id": 371, "seek": 235400, "start": 2377.0, "end": 2378.0, "text": " They use tabs.", "tokens": [814, 764, 20743, 13], "temperature": 0.0, "avg_logprob": -0.09152401380302493, "compression_ratio": 1.6031128404669261, "no_speech_prob": 2.9769986213068478e-05}, {"id": 372, "seek": 235400, "start": 2378.0, "end": 2382.0, "text": " So in pandas, you can just say what's the delimiter when you load it in.", "tokens": [407, 294, 4565, 296, 11, 291, 393, 445, 584, 437, 311, 264, 1103, 332, 1681, 562, 291, 3677, 309, 294, 13], "temperature": 0.0, "avg_logprob": -0.09152401380302493, "compression_ratio": 1.6031128404669261, "no_speech_prob": 2.9769986213068478e-05}, {"id": 373, "seek": 238200, "start": 2382.0, "end": 2385.0, "text": " The second is they don't add a header row so that you know what color is what.", "tokens": [440, 1150, 307, 436, 500, 380, 909, 257, 23117, 5386, 370, 300, 291, 458, 437, 2017, 307, 437, 13], "temperature": 0.0, "avg_logprob": -0.0672651584331806, "compression_ratio": 1.853658536585366, "no_speech_prob": 2.6685143893701024e-05}, {"id": 374, "seek": 238200, "start": 2385.0, "end": 2388.0, "text": " So you have to tell pandas there's no header row.", "tokens": [407, 291, 362, 281, 980, 4565, 296, 456, 311, 572, 23117, 5386, 13], "temperature": 0.0, "avg_logprob": -0.0672651584331806, "compression_ratio": 1.853658536585366, "no_speech_prob": 2.6685143893701024e-05}, {"id": 375, "seek": 238200, "start": 2388.0, "end": 2393.0, "text": " And then since there's no header row, you have to tell pandas what are the names of the columns.", "tokens": [400, 550, 1670, 456, 311, 572, 23117, 5386, 11, 291, 362, 281, 980, 4565, 296, 437, 366, 264, 5288, 295, 264, 13766, 13], "temperature": 0.0, "avg_logprob": -0.0672651584331806, "compression_ratio": 1.853658536585366, "no_speech_prob": 2.6685143893701024e-05}, {"id": 376, "seek": 238200, "start": 2393.0, "end": 2396.0, "text": " Other than that, that's all we need.", "tokens": [5358, 813, 300, 11, 300, 311, 439, 321, 643, 13], "temperature": 0.0, "avg_logprob": -0.0672651584331806, "compression_ratio": 1.853658536585366, "no_speech_prob": 2.6685143893701024e-05}, {"id": 377, "seek": 238200, "start": 2396.0, "end": 2400.0, "text": " OK, so we can then have a look at head, which remembers the first few rows.", "tokens": [2264, 11, 370, 321, 393, 550, 362, 257, 574, 412, 1378, 11, 597, 26228, 264, 700, 1326, 13241, 13], "temperature": 0.0, "avg_logprob": -0.0672651584331806, "compression_ratio": 1.853658536585366, "no_speech_prob": 2.6685143893701024e-05}, {"id": 378, "seek": 238200, "start": 2400.0, "end": 2405.0, "text": " And there is our user ratings, user movie rating.", "tokens": [400, 456, 307, 527, 4195, 24603, 11, 4195, 3169, 10990, 13], "temperature": 0.0, "avg_logprob": -0.0672651584331806, "compression_ratio": 1.853658536585366, "no_speech_prob": 2.6685143893701024e-05}, {"id": 379, "seek": 238200, "start": 2405.0, "end": 2407.0, "text": " And let's make it more fun.", "tokens": [400, 718, 311, 652, 309, 544, 1019, 13], "temperature": 0.0, "avg_logprob": -0.0672651584331806, "compression_ratio": 1.853658536585366, "no_speech_prob": 2.6685143893701024e-05}, {"id": 380, "seek": 238200, "start": 2407.0, "end": 2410.0, "text": " Let's see what the movies actually are.", "tokens": [961, 311, 536, 437, 264, 6233, 767, 366, 13], "temperature": 0.0, "avg_logprob": -0.0672651584331806, "compression_ratio": 1.853658536585366, "no_speech_prob": 2.6685143893701024e-05}, {"id": 381, "seek": 241000, "start": 2410.0, "end": 2416.0, "text": " I'll just point something out here, which is this thing called encoding equals.", "tokens": [286, 603, 445, 935, 746, 484, 510, 11, 597, 307, 341, 551, 1219, 43430, 6915, 13], "temperature": 0.0, "avg_logprob": -0.08983940189167605, "compression_ratio": 1.6751054852320675, "no_speech_prob": 5.828410212416202e-05}, {"id": 382, "seek": 241000, "start": 2416.0, "end": 2418.0, "text": " I've got to get rid of it.", "tokens": [286, 600, 658, 281, 483, 3973, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.08983940189167605, "compression_ratio": 1.6751054852320675, "no_speech_prob": 5.828410212416202e-05}, {"id": 383, "seek": 241000, "start": 2418.0, "end": 2420.0, "text": " And I get this error.", "tokens": [400, 286, 483, 341, 6713, 13], "temperature": 0.0, "avg_logprob": -0.08983940189167605, "compression_ratio": 1.6751054852320675, "no_speech_prob": 5.828410212416202e-05}, {"id": 384, "seek": 241000, "start": 2420.0, "end": 2421.0, "text": " Unicode.", "tokens": [1156, 299, 1429, 13], "temperature": 0.0, "avg_logprob": -0.08983940189167605, "compression_ratio": 1.6751054852320675, "no_speech_prob": 5.828410212416202e-05}, {"id": 385, "seek": 241000, "start": 2421.0, "end": 2425.0, "text": " I just want to point this out because you'll all see this at some point in your lives.", "tokens": [286, 445, 528, 281, 935, 341, 484, 570, 291, 603, 439, 536, 341, 412, 512, 935, 294, 428, 2909, 13], "temperature": 0.0, "avg_logprob": -0.08983940189167605, "compression_ratio": 1.6751054852320675, "no_speech_prob": 5.828410212416202e-05}, {"id": 386, "seek": 241000, "start": 2425.0, "end": 2428.0, "text": " Codec can't decode, blah, blah, blah.", "tokens": [15549, 66, 393, 380, 979, 1429, 11, 12288, 11, 12288, 11, 12288, 13], "temperature": 0.0, "avg_logprob": -0.08983940189167605, "compression_ratio": 1.6751054852320675, "no_speech_prob": 5.828410212416202e-05}, {"id": 387, "seek": 241000, "start": 2428.0, "end": 2431.0, "text": " What this means is that this is not a Unicode file.", "tokens": [708, 341, 1355, 307, 300, 341, 307, 406, 257, 1156, 299, 1429, 3991, 13], "temperature": 0.0, "avg_logprob": -0.08983940189167605, "compression_ratio": 1.6751054852320675, "no_speech_prob": 5.828410212416202e-05}, {"id": 388, "seek": 241000, "start": 2431.0, "end": 2437.0, "text": " This will be quite common when you're using data sets that are a little bit older.", "tokens": [639, 486, 312, 1596, 2689, 562, 291, 434, 1228, 1412, 6352, 300, 366, 257, 707, 857, 4906, 13], "temperature": 0.0, "avg_logprob": -0.08983940189167605, "compression_ratio": 1.6751054852320675, "no_speech_prob": 5.828410212416202e-05}, {"id": 389, "seek": 243700, "start": 2437.0, "end": 2447.0, "text": " Back before us folks in the West really realized that there are people that use languages other than English.", "tokens": [5833, 949, 505, 4024, 294, 264, 4055, 534, 5334, 300, 456, 366, 561, 300, 764, 8650, 661, 813, 3669, 13], "temperature": 0.0, "avg_logprob": -0.07612749021880481, "compression_ratio": 1.5609756097560976, "no_speech_prob": 4.9850306822918355e-05}, {"id": 390, "seek": 243700, "start": 2447.0, "end": 2451.0, "text": " Nowadays, we're much better at handling different languages.", "tokens": [28908, 11, 321, 434, 709, 1101, 412, 13175, 819, 8650, 13], "temperature": 0.0, "avg_logprob": -0.07612749021880481, "compression_ratio": 1.5609756097560976, "no_speech_prob": 4.9850306822918355e-05}, {"id": 391, "seek": 243700, "start": 2451.0, "end": 2455.0, "text": " We use this standard called Unicode for it.", "tokens": [492, 764, 341, 3832, 1219, 1156, 299, 1429, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.07612749021880481, "compression_ratio": 1.5609756097560976, "no_speech_prob": 4.9850306822918355e-05}, {"id": 392, "seek": 243700, "start": 2455.0, "end": 2459.0, "text": " And Python very helpfully uses Unicode by default.", "tokens": [400, 15329, 588, 854, 2277, 4960, 1156, 299, 1429, 538, 7576, 13], "temperature": 0.0, "avg_logprob": -0.07612749021880481, "compression_ratio": 1.5609756097560976, "no_speech_prob": 4.9850306822918355e-05}, {"id": 393, "seek": 243700, "start": 2459.0, "end": 2466.0, "text": " So if you try to load an old file that's not Unicode, you actually, believe it or not, have to guess how it was coded.", "tokens": [407, 498, 291, 853, 281, 3677, 364, 1331, 3991, 300, 311, 406, 1156, 299, 1429, 11, 291, 767, 11, 1697, 309, 420, 406, 11, 362, 281, 2041, 577, 309, 390, 34874, 13], "temperature": 0.0, "avg_logprob": -0.07612749021880481, "compression_ratio": 1.5609756097560976, "no_speech_prob": 4.9850306822918355e-05}, {"id": 394, "seek": 246600, "start": 2466.0, "end": 2476.0, "text": " But since it's really likely that it was created by some Western European or American person,", "tokens": [583, 1670, 309, 311, 534, 3700, 300, 309, 390, 2942, 538, 512, 8724, 6473, 420, 2665, 954, 11], "temperature": 0.0, "avg_logprob": -0.07489915455088896, "compression_ratio": 1.4489795918367347, "no_speech_prob": 2.5859870220301673e-05}, {"id": 395, "seek": 246600, "start": 2476.0, "end": 2480.0, "text": " they almost certainly used Latin one.", "tokens": [436, 1920, 3297, 1143, 10803, 472, 13], "temperature": 0.0, "avg_logprob": -0.07489915455088896, "compression_ratio": 1.4489795918367347, "no_speech_prob": 2.5859870220301673e-05}, {"id": 396, "seek": 246600, "start": 2480.0, "end": 2488.0, "text": " So if you just pop in encoding equals Latin one, if you use file open in Python or pandas open or whatever,", "tokens": [407, 498, 291, 445, 1665, 294, 43430, 6915, 10803, 472, 11, 498, 291, 764, 3991, 1269, 294, 15329, 420, 4565, 296, 1269, 420, 2035, 11], "temperature": 0.0, "avg_logprob": -0.07489915455088896, "compression_ratio": 1.4489795918367347, "no_speech_prob": 2.5859870220301673e-05}, {"id": 397, "seek": 246600, "start": 2488.0, "end": 2493.0, "text": " that will generally get around your problem.", "tokens": [300, 486, 5101, 483, 926, 428, 1154, 13], "temperature": 0.0, "avg_logprob": -0.07489915455088896, "compression_ratio": 1.4489795918367347, "no_speech_prob": 2.5859870220301673e-05}, {"id": 398, "seek": 249300, "start": 2493.0, "end": 2497.0, "text": " Again, they didn't have the names, so we had to list the names.", "tokens": [3764, 11, 436, 994, 380, 362, 264, 5288, 11, 370, 321, 632, 281, 1329, 264, 5288, 13], "temperature": 0.0, "avg_logprob": -0.06387399137020111, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.00014195266703609377}, {"id": 399, "seek": 249300, "start": 2497.0, "end": 2499.0, "text": " This is kind of interesting.", "tokens": [639, 307, 733, 295, 1880, 13], "temperature": 0.0, "avg_logprob": -0.06387399137020111, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.00014195266703609377}, {"id": 400, "seek": 249300, "start": 2499.0, "end": 2504.0, "text": " They had a separate column for every one of however many genres they had, 19 genres.", "tokens": [814, 632, 257, 4994, 7738, 337, 633, 472, 295, 4461, 867, 30057, 436, 632, 11, 1294, 30057, 13], "temperature": 0.0, "avg_logprob": -0.06387399137020111, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.00014195266703609377}, {"id": 401, "seek": 249300, "start": 2504.0, "end": 2507.0, "text": " And you'll see this looks one-hot encoded, but it's actually not.", "tokens": [400, 291, 603, 536, 341, 1542, 472, 12, 12194, 2058, 12340, 11, 457, 309, 311, 767, 406, 13], "temperature": 0.0, "avg_logprob": -0.06387399137020111, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.00014195266703609377}, {"id": 402, "seek": 249300, "start": 2507.0, "end": 2509.0, "text": " It's actually n-hot encoded.", "tokens": [467, 311, 767, 297, 12, 12194, 2058, 12340, 13], "temperature": 0.0, "avg_logprob": -0.06387399137020111, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.00014195266703609377}, {"id": 403, "seek": 249300, "start": 2509.0, "end": 2512.0, "text": " In other words, a movie can be in multiple genres.", "tokens": [682, 661, 2283, 11, 257, 3169, 393, 312, 294, 3866, 30057, 13], "temperature": 0.0, "avg_logprob": -0.06387399137020111, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.00014195266703609377}, {"id": 404, "seek": 249300, "start": 2512.0, "end": 2520.0, "text": " We're not going to look at genres today, but it's just interesting to point out that this is a way that sometimes people will represent something like genre.", "tokens": [492, 434, 406, 516, 281, 574, 412, 30057, 965, 11, 457, 309, 311, 445, 1880, 281, 935, 484, 300, 341, 307, 257, 636, 300, 2171, 561, 486, 2906, 746, 411, 11022, 13], "temperature": 0.0, "avg_logprob": -0.06387399137020111, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.00014195266703609377}, {"id": 405, "seek": 252000, "start": 2520.0, "end": 2526.0, "text": " In the more recent version, they actually list the genres directly, which is much more convenient.", "tokens": [682, 264, 544, 5162, 3037, 11, 436, 767, 1329, 264, 30057, 3838, 11, 597, 307, 709, 544, 10851, 13], "temperature": 0.0, "avg_logprob": -0.09293791880974403, "compression_ratio": 1.6982758620689655, "no_speech_prob": 3.94355010939762e-05}, {"id": 406, "seek": 252000, "start": 2526.0, "end": 2531.0, "text": " OK. So I find life is... So we've got 100,000 ratings.", "tokens": [2264, 13, 407, 286, 915, 993, 307, 485, 407, 321, 600, 658, 2319, 11, 1360, 24603, 13], "temperature": 0.0, "avg_logprob": -0.09293791880974403, "compression_ratio": 1.6982758620689655, "no_speech_prob": 3.94355010939762e-05}, {"id": 407, "seek": 252000, "start": 2531.0, "end": 2535.0, "text": " I find life is easier when you're modeling, when you actually denormalize the data.", "tokens": [286, 915, 993, 307, 3571, 562, 291, 434, 15983, 11, 562, 291, 767, 1441, 24440, 1125, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.09293791880974403, "compression_ratio": 1.6982758620689655, "no_speech_prob": 3.94355010939762e-05}, {"id": 408, "seek": 252000, "start": 2535.0, "end": 2538.0, "text": " So I actually want the movie title directly in my ratings.", "tokens": [407, 286, 767, 528, 264, 3169, 4876, 3838, 294, 452, 24603, 13], "temperature": 0.0, "avg_logprob": -0.09293791880974403, "compression_ratio": 1.6982758620689655, "no_speech_prob": 3.94355010939762e-05}, {"id": 409, "seek": 252000, "start": 2538.0, "end": 2541.0, "text": " So pandas has a merge function to let us do that.", "tokens": [407, 4565, 296, 575, 257, 22183, 2445, 281, 718, 505, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.09293791880974403, "compression_ratio": 1.6982758620689655, "no_speech_prob": 3.94355010939762e-05}, {"id": 410, "seek": 252000, "start": 2541.0, "end": 2546.0, "text": " So here's the ratings table with actual titles.", "tokens": [407, 510, 311, 264, 24603, 3199, 365, 3539, 12992, 13], "temperature": 0.0, "avg_logprob": -0.09293791880974403, "compression_ratio": 1.6982758620689655, "no_speech_prob": 3.94355010939762e-05}, {"id": 411, "seek": 254600, "start": 2546.0, "end": 2553.0, "text": " So as per usual, we can create a data bunch for our applications or a colab data bunch for the colab application from what?", "tokens": [407, 382, 680, 7713, 11, 321, 393, 1884, 257, 1412, 3840, 337, 527, 5821, 420, 257, 1173, 455, 1412, 3840, 337, 264, 1173, 455, 3861, 490, 437, 30], "temperature": 0.0, "avg_logprob": -0.15532995792145424, "compression_ratio": 1.7256637168141593, "no_speech_prob": 4.7563728003297e-05}, {"id": 412, "seek": 254600, "start": 2553.0, "end": 2555.0, "text": " From a data frame.", "tokens": [3358, 257, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.15532995792145424, "compression_ratio": 1.7256637168141593, "no_speech_prob": 4.7563728003297e-05}, {"id": 413, "seek": 254600, "start": 2555.0, "end": 2557.0, "text": " There's our data frame.", "tokens": [821, 311, 527, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.15532995792145424, "compression_ratio": 1.7256637168141593, "no_speech_prob": 4.7563728003297e-05}, {"id": 414, "seek": 254600, "start": 2557.0, "end": 2560.0, "text": " Set aside some validation data.", "tokens": [8928, 7359, 512, 24071, 1412, 13], "temperature": 0.0, "avg_logprob": -0.15532995792145424, "compression_ratio": 1.7256637168141593, "no_speech_prob": 4.7563728003297e-05}, {"id": 415, "seek": 254600, "start": 2560.0, "end": 2567.0, "text": " Really, we should use the validation sets and cross validation approach that they used if you're going to properly compare with a benchmark.", "tokens": [4083, 11, 321, 820, 764, 264, 24071, 6352, 293, 3278, 24071, 3109, 300, 436, 1143, 498, 291, 434, 516, 281, 6108, 6794, 365, 257, 18927, 13], "temperature": 0.0, "avg_logprob": -0.15532995792145424, "compression_ratio": 1.7256637168141593, "no_speech_prob": 4.7563728003297e-05}, {"id": 416, "seek": 254600, "start": 2567.0, "end": 2570.0, "text": " So take these comparisons with the Graham assault.", "tokens": [407, 747, 613, 33157, 365, 264, 22691, 12458, 13], "temperature": 0.0, "avg_logprob": -0.15532995792145424, "compression_ratio": 1.7256637168141593, "no_speech_prob": 4.7563728003297e-05}, {"id": 417, "seek": 257000, "start": 2570.0, "end": 2581.0, "text": " By default, Colab data bunch assumes that your first column is user, second column of item, third column is rating.", "tokens": [3146, 7576, 11, 4004, 455, 1412, 3840, 37808, 300, 428, 700, 7738, 307, 4195, 11, 1150, 7738, 295, 3174, 11, 2636, 7738, 307, 10990, 13], "temperature": 0.0, "avg_logprob": -0.1101956220017266, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.406247065460775e-05}, {"id": 418, "seek": 257000, "start": 2581.0, "end": 2585.0, "text": " But now we're actually going to use the title column as item.", "tokens": [583, 586, 321, 434, 767, 516, 281, 764, 264, 4876, 7738, 382, 3174, 13], "temperature": 0.0, "avg_logprob": -0.1101956220017266, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.406247065460775e-05}, {"id": 419, "seek": 257000, "start": 2585.0, "end": 2590.0, "text": " So we have to tell it what the item column name is.", "tokens": [407, 321, 362, 281, 980, 309, 437, 264, 3174, 7738, 1315, 307, 13], "temperature": 0.0, "avg_logprob": -0.1101956220017266, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.406247065460775e-05}, {"id": 420, "seek": 257000, "start": 2590.0, "end": 2592.0, "text": " And then all of our data bunches support show batch.", "tokens": [400, 550, 439, 295, 527, 1412, 3840, 279, 1406, 855, 15245, 13], "temperature": 0.0, "avg_logprob": -0.1101956220017266, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.406247065460775e-05}, {"id": 421, "seek": 257000, "start": 2592.0, "end": 2594.0, "text": " So you can just check what's in there.", "tokens": [407, 291, 393, 445, 1520, 437, 311, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.1101956220017266, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.406247065460775e-05}, {"id": 422, "seek": 257000, "start": 2594.0, "end": 2595.0, "text": " And there it is.", "tokens": [400, 456, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1101956220017266, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.406247065460775e-05}, {"id": 423, "seek": 257000, "start": 2595.0, "end": 2599.0, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.1101956220017266, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.406247065460775e-05}, {"id": 424, "seek": 259900, "start": 2599.0, "end": 2604.0, "text": " So I'm going to try and get as good a result as I can.", "tokens": [407, 286, 478, 516, 281, 853, 293, 483, 382, 665, 257, 1874, 382, 286, 393, 13], "temperature": 0.0, "avg_logprob": -0.08974613653165157, "compression_ratio": 1.7110091743119267, "no_speech_prob": 1.4284284588939045e-05}, {"id": 425, "seek": 259900, "start": 2604.0, "end": 2608.0, "text": " So I'm going to try and use whatever tricks I can come up with to get a good a good answer.", "tokens": [407, 286, 478, 516, 281, 853, 293, 764, 2035, 11733, 286, 393, 808, 493, 365, 281, 483, 257, 665, 257, 665, 1867, 13], "temperature": 0.0, "avg_logprob": -0.08974613653165157, "compression_ratio": 1.7110091743119267, "no_speech_prob": 1.4284284588939045e-05}, {"id": 426, "seek": 259900, "start": 2608.0, "end": 2612.0, "text": " Now, one of the tricks is to use the Y range.", "tokens": [823, 11, 472, 295, 264, 11733, 307, 281, 764, 264, 398, 3613, 13], "temperature": 0.0, "avg_logprob": -0.08974613653165157, "compression_ratio": 1.7110091743119267, "no_speech_prob": 1.4284284588939045e-05}, {"id": 427, "seek": 259900, "start": 2612.0, "end": 2620.0, "text": " And remember, the Y range was the thing that made the final activation function a sigmoid.", "tokens": [400, 1604, 11, 264, 398, 3613, 390, 264, 551, 300, 1027, 264, 2572, 24433, 2445, 257, 4556, 3280, 327, 13], "temperature": 0.0, "avg_logprob": -0.08974613653165157, "compression_ratio": 1.7110091743119267, "no_speech_prob": 1.4284284588939045e-05}, {"id": 428, "seek": 259900, "start": 2620.0, "end": 2627.0, "text": " And specifically, last week, we said, let's have a sigmoid that goes from naught to five.", "tokens": [400, 4682, 11, 1036, 1243, 11, 321, 848, 11, 718, 311, 362, 257, 4556, 3280, 327, 300, 1709, 490, 13138, 281, 1732, 13], "temperature": 0.0, "avg_logprob": -0.08974613653165157, "compression_ratio": 1.7110091743119267, "no_speech_prob": 1.4284284588939045e-05}, {"id": 429, "seek": 262700, "start": 2627.0, "end": 2633.0, "text": " And that way, it's going to ensure that it's going to help the neural network predict things that are in the right range.", "tokens": [400, 300, 636, 11, 309, 311, 516, 281, 5586, 300, 309, 311, 516, 281, 854, 264, 18161, 3209, 6069, 721, 300, 366, 294, 264, 558, 3613, 13], "temperature": 0.0, "avg_logprob": -0.08770044417608353, "compression_ratio": 1.6875, "no_speech_prob": 1.5935374904074706e-05}, {"id": 430, "seek": 262700, "start": 2633.0, "end": 2638.0, "text": " I actually didn't do that in my Excel version.", "tokens": [286, 767, 994, 380, 360, 300, 294, 452, 19060, 3037, 13], "temperature": 0.0, "avg_logprob": -0.08770044417608353, "compression_ratio": 1.6875, "no_speech_prob": 1.5935374904074706e-05}, {"id": 431, "seek": 262700, "start": 2638.0, "end": 2641.0, "text": " And so you can see I've actually got some negatives.", "tokens": [400, 370, 291, 393, 536, 286, 600, 767, 658, 512, 40019, 13], "temperature": 0.0, "avg_logprob": -0.08770044417608353, "compression_ratio": 1.6875, "no_speech_prob": 1.5935374904074706e-05}, {"id": 432, "seek": 262700, "start": 2641.0, "end": 2643.0, "text": " And there's also some things bigger than five.", "tokens": [400, 456, 311, 611, 512, 721, 3801, 813, 1732, 13], "temperature": 0.0, "avg_logprob": -0.08770044417608353, "compression_ratio": 1.6875, "no_speech_prob": 1.5935374904074706e-05}, {"id": 433, "seek": 262700, "start": 2643.0, "end": 2655.0, "text": " So if you want to beat me in Excel, you could you could add the sigmoid to Excel and train this and you'll get a slightly better answer.", "tokens": [407, 498, 291, 528, 281, 4224, 385, 294, 19060, 11, 291, 727, 291, 727, 909, 264, 4556, 3280, 327, 281, 19060, 293, 3847, 341, 293, 291, 603, 483, 257, 4748, 1101, 1867, 13], "temperature": 0.0, "avg_logprob": -0.08770044417608353, "compression_ratio": 1.6875, "no_speech_prob": 1.5935374904074706e-05}, {"id": 434, "seek": 265500, "start": 2655.0, "end": 2661.0, "text": " Now, the problem is that a sigmoid actually asymptotes at, say, whatever the maximums.", "tokens": [823, 11, 264, 1154, 307, 300, 257, 4556, 3280, 327, 767, 35114, 17251, 412, 11, 584, 11, 2035, 264, 6674, 82, 13], "temperature": 0.0, "avg_logprob": -0.06400587128811196, "compression_ratio": 1.8228346456692914, "no_speech_prob": 3.7852191780984867e-06}, {"id": 435, "seek": 265500, "start": 2661.0, "end": 2665.0, "text": " We said five, which means you can never actually predict five.", "tokens": [492, 848, 1732, 11, 597, 1355, 291, 393, 1128, 767, 6069, 1732, 13], "temperature": 0.0, "avg_logprob": -0.06400587128811196, "compression_ratio": 1.8228346456692914, "no_speech_prob": 3.7852191780984867e-06}, {"id": 436, "seek": 265500, "start": 2665.0, "end": 2667.0, "text": " But plenty of movies have a rating of five.", "tokens": [583, 7140, 295, 6233, 362, 257, 10990, 295, 1732, 13], "temperature": 0.0, "avg_logprob": -0.06400587128811196, "compression_ratio": 1.8228346456692914, "no_speech_prob": 3.7852191780984867e-06}, {"id": 437, "seek": 265500, "start": 2667.0, "end": 2668.0, "text": " So that's a problem.", "tokens": [407, 300, 311, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.06400587128811196, "compression_ratio": 1.8228346456692914, "no_speech_prob": 3.7852191780984867e-06}, {"id": 438, "seek": 265500, "start": 2668.0, "end": 2675.0, "text": " So actually, it's slightly better to make your Y range go from a little bit less than the minimum to a little bit more than the maximum.", "tokens": [407, 767, 11, 309, 311, 4748, 1101, 281, 652, 428, 398, 3613, 352, 490, 257, 707, 857, 1570, 813, 264, 7285, 281, 257, 707, 857, 544, 813, 264, 6674, 13], "temperature": 0.0, "avg_logprob": -0.06400587128811196, "compression_ratio": 1.8228346456692914, "no_speech_prob": 3.7852191780984867e-06}, {"id": 439, "seek": 265500, "start": 2675.0, "end": 2679.0, "text": " And the minimum of this data is point five and the maximum is five.", "tokens": [400, 264, 7285, 295, 341, 1412, 307, 935, 1732, 293, 264, 6674, 307, 1732, 13], "temperature": 0.0, "avg_logprob": -0.06400587128811196, "compression_ratio": 1.8228346456692914, "no_speech_prob": 3.7852191780984867e-06}, {"id": 440, "seek": 265500, "start": 2679.0, "end": 2682.0, "text": " So this range is just a little bit further.", "tokens": [407, 341, 3613, 307, 445, 257, 707, 857, 3052, 13], "temperature": 0.0, "avg_logprob": -0.06400587128811196, "compression_ratio": 1.8228346456692914, "no_speech_prob": 3.7852191780984867e-06}, {"id": 441, "seek": 268200, "start": 2682.0, "end": 2689.0, "text": " So that's a that's one little trick to get a little bit more accuracy.", "tokens": [407, 300, 311, 257, 300, 311, 472, 707, 4282, 281, 483, 257, 707, 857, 544, 14170, 13], "temperature": 0.0, "avg_logprob": -0.07643396685821842, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.1300571713945828e-05}, {"id": 442, "seek": 268200, "start": 2689.0, "end": 2693.0, "text": " The other trick I used is to add something called weight decay.", "tokens": [440, 661, 4282, 286, 1143, 307, 281, 909, 746, 1219, 3364, 21039, 13], "temperature": 0.0, "avg_logprob": -0.07643396685821842, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.1300571713945828e-05}, {"id": 443, "seek": 268200, "start": 2693.0, "end": 2697.0, "text": " And we're going to look at that next. OK, after this section, we're going to learn about weight decay.", "tokens": [400, 321, 434, 516, 281, 574, 412, 300, 958, 13, 2264, 11, 934, 341, 3541, 11, 321, 434, 516, 281, 1466, 466, 3364, 21039, 13], "temperature": 0.0, "avg_logprob": -0.07643396685821842, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.1300571713945828e-05}, {"id": 444, "seek": 268200, "start": 2697.0, "end": 2704.0, "text": " So then how many how many factors do you want?", "tokens": [407, 550, 577, 867, 577, 867, 6771, 360, 291, 528, 30], "temperature": 0.0, "avg_logprob": -0.07643396685821842, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.1300571713945828e-05}, {"id": 445, "seek": 268200, "start": 2704.0, "end": 2706.0, "text": " Well, what are factors?", "tokens": [1042, 11, 437, 366, 6771, 30], "temperature": 0.0, "avg_logprob": -0.07643396685821842, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.1300571713945828e-05}, {"id": 446, "seek": 268200, "start": 2706.0, "end": 2710.0, "text": " The number of factors is the width of the embedding matrix.", "tokens": [440, 1230, 295, 6771, 307, 264, 11402, 295, 264, 12240, 3584, 8141, 13], "temperature": 0.0, "avg_logprob": -0.07643396685821842, "compression_ratio": 1.6727272727272726, "no_speech_prob": 1.1300571713945828e-05}, {"id": 447, "seek": 271000, "start": 2710.0, "end": 2713.0, "text": " So why don't we say embedding size?", "tokens": [407, 983, 500, 380, 321, 584, 12240, 3584, 2744, 30], "temperature": 0.0, "avg_logprob": -0.05485011195088481, "compression_ratio": 1.7702702702702702, "no_speech_prob": 1.6700385458534583e-05}, {"id": 448, "seek": 271000, "start": 2713.0, "end": 2714.0, "text": " Maybe we should.", "tokens": [2704, 321, 820, 13], "temperature": 0.0, "avg_logprob": -0.05485011195088481, "compression_ratio": 1.7702702702702702, "no_speech_prob": 1.6700385458534583e-05}, {"id": 449, "seek": 271000, "start": 2714.0, "end": 2719.0, "text": " But in the world of collaborative filtering, they don't use that word.", "tokens": [583, 294, 264, 1002, 295, 16555, 30822, 11, 436, 500, 380, 764, 300, 1349, 13], "temperature": 0.0, "avg_logprob": -0.05485011195088481, "compression_ratio": 1.7702702702702702, "no_speech_prob": 1.6700385458534583e-05}, {"id": 450, "seek": 271000, "start": 2719.0, "end": 2729.0, "text": " They use the word factors because of this idea of latent factors and because the standard way of doing collaborative filtering has been with something called matrix factorization.", "tokens": [814, 764, 264, 1349, 6771, 570, 295, 341, 1558, 295, 48994, 6771, 293, 570, 264, 3832, 636, 295, 884, 16555, 30822, 575, 668, 365, 746, 1219, 8141, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.05485011195088481, "compression_ratio": 1.7702702702702702, "no_speech_prob": 1.6700385458534583e-05}, {"id": 451, "seek": 271000, "start": 2729.0, "end": 2736.0, "text": " And in fact, what we just saw happens to actually be a way of doing matrix factorization.", "tokens": [400, 294, 1186, 11, 437, 321, 445, 1866, 2314, 281, 767, 312, 257, 636, 295, 884, 8141, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.05485011195088481, "compression_ratio": 1.7702702702702702, "no_speech_prob": 1.6700385458534583e-05}, {"id": 452, "seek": 273600, "start": 2736.0, "end": 2742.0, "text": " So we've we've actually accidentally learned how to do matrix factorization today.", "tokens": [407, 321, 600, 321, 600, 767, 15715, 3264, 577, 281, 360, 8141, 5952, 2144, 965, 13], "temperature": 0.0, "avg_logprob": -0.07233944479024636, "compression_ratio": 1.5426356589147288, "no_speech_prob": 1.0782719982671551e-05}, {"id": 453, "seek": 273600, "start": 2742.0, "end": 2746.0, "text": " So so this is a term that's kind of specific to this domain.", "tokens": [407, 370, 341, 307, 257, 1433, 300, 311, 733, 295, 2685, 281, 341, 9274, 13], "temperature": 0.0, "avg_logprob": -0.07233944479024636, "compression_ratio": 1.5426356589147288, "no_speech_prob": 1.0782719982671551e-05}, {"id": 454, "seek": 273600, "start": 2746.0, "end": 2749.0, "text": " But you can just remember it as the width of the embedding matrix.", "tokens": [583, 291, 393, 445, 1604, 309, 382, 264, 11402, 295, 264, 12240, 3584, 8141, 13], "temperature": 0.0, "avg_logprob": -0.07233944479024636, "compression_ratio": 1.5426356589147288, "no_speech_prob": 1.0782719982671551e-05}, {"id": 455, "seek": 273600, "start": 2749.0, "end": 2751.0, "text": " And so why 40?", "tokens": [400, 370, 983, 3356, 30], "temperature": 0.0, "avg_logprob": -0.07233944479024636, "compression_ratio": 1.5426356589147288, "no_speech_prob": 1.0782719982671551e-05}, {"id": 456, "seek": 273600, "start": 2751.0, "end": 2756.0, "text": " Well, this is one of these architectural decisions you have to play around with and see what works.", "tokens": [1042, 11, 341, 307, 472, 295, 613, 26621, 5327, 291, 362, 281, 862, 926, 365, 293, 536, 437, 1985, 13], "temperature": 0.0, "avg_logprob": -0.07233944479024636, "compression_ratio": 1.5426356589147288, "no_speech_prob": 1.0782719982671551e-05}, {"id": 457, "seek": 273600, "start": 2756.0, "end": 2762.0, "text": " So I tried 10, 20, 40 and 80, and I found 40 seemed to work pretty well.", "tokens": [407, 286, 3031, 1266, 11, 945, 11, 3356, 293, 4688, 11, 293, 286, 1352, 3356, 6576, 281, 589, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.07233944479024636, "compression_ratio": 1.5426356589147288, "no_speech_prob": 1.0782719982671551e-05}, {"id": 458, "seek": 276200, "start": 2762.0, "end": 2770.0, "text": " It's changed really quickly, so like you can chuck it in a little for loop to try a few things and see what looks best.", "tokens": [467, 311, 3105, 534, 2661, 11, 370, 411, 291, 393, 20870, 309, 294, 257, 707, 337, 6367, 281, 853, 257, 1326, 721, 293, 536, 437, 1542, 1151, 13], "temperature": 0.0, "avg_logprob": -0.1586900175663463, "compression_ratio": 1.6290322580645162, "no_speech_prob": 2.5069675757549703e-05}, {"id": 459, "seek": 276200, "start": 2770.0, "end": 2775.0, "text": " And then for learning rates, so use the learning rate finder as usual.", "tokens": [400, 550, 337, 2539, 6846, 11, 370, 764, 264, 2539, 3314, 915, 260, 382, 7713, 13], "temperature": 0.0, "avg_logprob": -0.1586900175663463, "compression_ratio": 1.6290322580645162, "no_speech_prob": 2.5069675757549703e-05}, {"id": 460, "seek": 276200, "start": 2775.0, "end": 2778.0, "text": " So five e neg three seemed to work pretty well.", "tokens": [407, 1732, 308, 2485, 1045, 6576, 281, 589, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.1586900175663463, "compression_ratio": 1.6290322580645162, "no_speech_prob": 2.5069675757549703e-05}, {"id": 461, "seek": 276200, "start": 2778.0, "end": 2780.0, "text": " Remember, this is just a rule of thumb.", "tokens": [5459, 11, 341, 307, 445, 257, 4978, 295, 9298, 13], "temperature": 0.0, "avg_logprob": -0.1586900175663463, "compression_ratio": 1.6290322580645162, "no_speech_prob": 2.5069675757549703e-05}, {"id": 462, "seek": 276200, "start": 2780.0, "end": 2784.0, "text": " Five e neg three is a bit lower than both Sylvain's rule and my rule.", "tokens": [9436, 308, 2485, 1045, 307, 257, 857, 3126, 813, 1293, 3902, 14574, 491, 311, 4978, 293, 452, 4978, 13], "temperature": 0.0, "avg_logprob": -0.1586900175663463, "compression_ratio": 1.6290322580645162, "no_speech_prob": 2.5069675757549703e-05}, {"id": 463, "seek": 276200, "start": 2784.0, "end": 2788.0, "text": " So Sylvain's role is find the bottom and go back by 10.", "tokens": [407, 3902, 14574, 491, 311, 3090, 307, 915, 264, 2767, 293, 352, 646, 538, 1266, 13], "temperature": 0.0, "avg_logprob": -0.1586900175663463, "compression_ratio": 1.6290322580645162, "no_speech_prob": 2.5069675757549703e-05}, {"id": 464, "seek": 278800, "start": 2788.0, "end": 2792.0, "text": " So his role would be more like two e neg two, I reckon.", "tokens": [407, 702, 3090, 576, 312, 544, 411, 732, 308, 2485, 732, 11, 286, 29548, 13], "temperature": 0.0, "avg_logprob": -0.12287645103517643, "compression_ratio": 1.6852589641434264, "no_speech_prob": 1.4509952961816452e-05}, {"id": 465, "seek": 278800, "start": 2792.0, "end": 2798.0, "text": " My rule is kind of find about the steepest section, which is about here, which again, like often it agrees with Sylvain.", "tokens": [1222, 4978, 307, 733, 295, 915, 466, 264, 16841, 377, 3541, 11, 597, 307, 466, 510, 11, 597, 797, 11, 411, 2049, 309, 26383, 365, 3902, 14574, 491, 13], "temperature": 0.0, "avg_logprob": -0.12287645103517643, "compression_ratio": 1.6852589641434264, "no_speech_prob": 1.4509952961816452e-05}, {"id": 466, "seek": 278800, "start": 2798.0, "end": 2800.0, "text": " So that would be about two e neg two.", "tokens": [407, 300, 576, 312, 466, 732, 308, 2485, 732, 13], "temperature": 0.0, "avg_logprob": -0.12287645103517643, "compression_ratio": 1.6852589641434264, "no_speech_prob": 1.4509952961816452e-05}, {"id": 467, "seek": 278800, "start": 2800.0, "end": 2806.0, "text": " I tried that, but I always like to try like 10x less and 10x more just to check.", "tokens": [286, 3031, 300, 11, 457, 286, 1009, 411, 281, 853, 411, 1266, 87, 1570, 293, 1266, 87, 544, 445, 281, 1520, 13], "temperature": 0.0, "avg_logprob": -0.12287645103517643, "compression_ratio": 1.6852589641434264, "no_speech_prob": 1.4509952961816452e-05}, {"id": 468, "seek": 278800, "start": 2806.0, "end": 2808.0, "text": " And actually, I found a bit less was helpful.", "tokens": [400, 767, 11, 286, 1352, 257, 857, 1570, 390, 4961, 13], "temperature": 0.0, "avg_logprob": -0.12287645103517643, "compression_ratio": 1.6852589641434264, "no_speech_prob": 1.4509952961816452e-05}, {"id": 469, "seek": 278800, "start": 2808.0, "end": 2816.0, "text": " So the answer to the question, like, should I do blah is always try blah and see.", "tokens": [407, 264, 1867, 281, 264, 1168, 11, 411, 11, 820, 286, 360, 12288, 307, 1009, 853, 12288, 293, 536, 13], "temperature": 0.0, "avg_logprob": -0.12287645103517643, "compression_ratio": 1.6852589641434264, "no_speech_prob": 1.4509952961816452e-05}, {"id": 470, "seek": 281600, "start": 2816.0, "end": 2821.0, "text": " That's how you actually become a good practitioner.", "tokens": [663, 311, 577, 291, 767, 1813, 257, 665, 32125, 13], "temperature": 0.0, "avg_logprob": -0.14155373292810777, "compression_ratio": 1.5253456221198156, "no_speech_prob": 2.9308663215488195e-05}, {"id": 471, "seek": 281600, "start": 2821.0, "end": 2824.0, "text": " So that gave me point eight one three.", "tokens": [407, 300, 2729, 385, 935, 3180, 472, 1045, 13], "temperature": 0.0, "avg_logprob": -0.14155373292810777, "compression_ratio": 1.5253456221198156, "no_speech_prob": 2.9308663215488195e-05}, {"id": 472, "seek": 281600, "start": 2824.0, "end": 2829.0, "text": " And as usual, you can save the result to save you another 33 seconds from having to do it again later.", "tokens": [400, 382, 7713, 11, 291, 393, 3155, 264, 1874, 281, 3155, 291, 1071, 11816, 3949, 490, 1419, 281, 360, 309, 797, 1780, 13], "temperature": 0.0, "avg_logprob": -0.14155373292810777, "compression_ratio": 1.5253456221198156, "no_speech_prob": 2.9308663215488195e-05}, {"id": 473, "seek": 281600, "start": 2829.0, "end": 2840.0, "text": " And so there's a library called Libreq and they publish some benchmarks for movie lens 100K.", "tokens": [400, 370, 456, 311, 257, 6405, 1219, 15834, 265, 80, 293, 436, 11374, 512, 43751, 337, 3169, 6765, 2319, 42, 13], "temperature": 0.0, "avg_logprob": -0.14155373292810777, "compression_ratio": 1.5253456221198156, "no_speech_prob": 2.9308663215488195e-05}, {"id": 474, "seek": 281600, "start": 2840.0, "end": 2843.0, "text": " And there's a root means grid error section.", "tokens": [400, 456, 311, 257, 5593, 1355, 10748, 6713, 3541, 13], "temperature": 0.0, "avg_logprob": -0.14155373292810777, "compression_ratio": 1.5253456221198156, "no_speech_prob": 2.9308663215488195e-05}, {"id": 475, "seek": 284300, "start": 2843.0, "end": 2847.0, "text": " And about point nine one is about as good as they seem to have been able to get.", "tokens": [400, 466, 935, 4949, 472, 307, 466, 382, 665, 382, 436, 1643, 281, 362, 668, 1075, 281, 483, 13], "temperature": 0.0, "avg_logprob": -0.07824375093445297, "compression_ratio": 1.7900763358778626, "no_speech_prob": 5.063667049398646e-05}, {"id": 476, "seek": 284300, "start": 2847.0, "end": 2850.0, "text": " Point nine one is the root means grid error.", "tokens": [12387, 4949, 472, 307, 264, 5593, 1355, 10748, 6713, 13], "temperature": 0.0, "avg_logprob": -0.07824375093445297, "compression_ratio": 1.7900763358778626, "no_speech_prob": 5.063667049398646e-05}, {"id": 477, "seek": 284300, "start": 2850.0, "end": 2852.0, "text": " We use the mean grid error, not the root.", "tokens": [492, 764, 264, 914, 10748, 6713, 11, 406, 264, 5593, 13], "temperature": 0.0, "avg_logprob": -0.07824375093445297, "compression_ratio": 1.7900763358778626, "no_speech_prob": 5.063667049398646e-05}, {"id": 478, "seek": 284300, "start": 2852.0, "end": 2855.0, "text": " So we have to go point nine one squared, which is point eight three.", "tokens": [407, 321, 362, 281, 352, 935, 4949, 472, 8889, 11, 597, 307, 935, 3180, 1045, 13], "temperature": 0.0, "avg_logprob": -0.07824375093445297, "compression_ratio": 1.7900763358778626, "no_speech_prob": 5.063667049398646e-05}, {"id": 479, "seek": 284300, "start": 2855.0, "end": 2857.0, "text": " And we're getting point eight one.", "tokens": [400, 321, 434, 1242, 935, 3180, 472, 13], "temperature": 0.0, "avg_logprob": -0.07824375093445297, "compression_ratio": 1.7900763358778626, "no_speech_prob": 5.063667049398646e-05}, {"id": 480, "seek": 284300, "start": 2857.0, "end": 2859.0, "text": " So that's cool.", "tokens": [407, 300, 311, 1627, 13], "temperature": 0.0, "avg_logprob": -0.07824375093445297, "compression_ratio": 1.7900763358778626, "no_speech_prob": 5.063667049398646e-05}, {"id": 481, "seek": 284300, "start": 2859.0, "end": 2868.0, "text": " With this very simple model, we're doing a little bit better, quite a lot better, actually.", "tokens": [2022, 341, 588, 2199, 2316, 11, 321, 434, 884, 257, 707, 857, 1101, 11, 1596, 257, 688, 1101, 11, 767, 13], "temperature": 0.0, "avg_logprob": -0.07824375093445297, "compression_ratio": 1.7900763358778626, "no_speech_prob": 5.063667049398646e-05}, {"id": 482, "seek": 284300, "start": 2868.0, "end": 2872.0, "text": " Although, as I said, take it with a grain of salt because we're not doing the same splits", "tokens": [5780, 11, 382, 286, 848, 11, 747, 309, 365, 257, 12837, 295, 5139, 570, 321, 434, 406, 884, 264, 912, 37741], "temperature": 0.0, "avg_logprob": -0.07824375093445297, "compression_ratio": 1.7900763358778626, "no_speech_prob": 5.063667049398646e-05}, {"id": 483, "seek": 287200, "start": 2872.0, "end": 2874.0, "text": " and the same cross validation.", "tokens": [293, 264, 912, 3278, 24071, 13], "temperature": 0.0, "avg_logprob": -0.06388787473185678, "compression_ratio": 1.7956989247311828, "no_speech_prob": 3.2696090784156695e-05}, {"id": 484, "seek": 287200, "start": 2874.0, "end": 2880.0, "text": " So we're at least highly competitive with their approaches.", "tokens": [407, 321, 434, 412, 1935, 5405, 10043, 365, 641, 11587, 13], "temperature": 0.0, "avg_logprob": -0.06388787473185678, "compression_ratio": 1.7956989247311828, "no_speech_prob": 3.2696090784156695e-05}, {"id": 485, "seek": 287200, "start": 2880.0, "end": 2886.0, "text": " OK, so we're going to look at the Python code that does this in a moment.", "tokens": [2264, 11, 370, 321, 434, 516, 281, 574, 412, 264, 15329, 3089, 300, 775, 341, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.06388787473185678, "compression_ratio": 1.7956989247311828, "no_speech_prob": 3.2696090784156695e-05}, {"id": 486, "seek": 287200, "start": 2886.0, "end": 2889.0, "text": " We're going to look at the Python code that does this in a moment.", "tokens": [492, 434, 516, 281, 574, 412, 264, 15329, 3089, 300, 775, 341, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.06388787473185678, "compression_ratio": 1.7956989247311828, "no_speech_prob": 3.2696090784156695e-05}, {"id": 487, "seek": 287200, "start": 2889.0, "end": 2898.0, "text": " But for now, just take my word for it that we're going to see something that's just doing this, right?", "tokens": [583, 337, 586, 11, 445, 747, 452, 1349, 337, 309, 300, 321, 434, 516, 281, 536, 746, 300, 311, 445, 884, 341, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.06388787473185678, "compression_ratio": 1.7956989247311828, "no_speech_prob": 3.2696090784156695e-05}, {"id": 488, "seek": 289800, "start": 2898.0, "end": 2904.0, "text": " Looking things up in an array and then model plug them together, adding them up,", "tokens": [11053, 721, 493, 294, 364, 10225, 293, 550, 2316, 5452, 552, 1214, 11, 5127, 552, 493, 11], "temperature": 0.0, "avg_logprob": -0.12152182400881589, "compression_ratio": 1.6695652173913043, "no_speech_prob": 1.4969818948884495e-05}, {"id": 489, "seek": 289800, "start": 2904.0, "end": 2907.0, "text": " and doing the mean squared error loss function.", "tokens": [293, 884, 264, 914, 8889, 6713, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.12152182400881589, "compression_ratio": 1.6695652173913043, "no_speech_prob": 1.4969818948884495e-05}, {"id": 490, "seek": 289800, "start": 2907.0, "end": 2912.0, "text": " So given that and given that we noticed that the only way that that can do anything interesting", "tokens": [407, 2212, 300, 293, 2212, 300, 321, 5694, 300, 264, 787, 636, 300, 300, 393, 360, 1340, 1880], "temperature": 0.0, "avg_logprob": -0.12152182400881589, "compression_ratio": 1.6695652173913043, "no_speech_prob": 1.4969818948884495e-05}, {"id": 491, "seek": 289800, "start": 2912.0, "end": 2918.0, "text": " is by trying to kind of find these latent factors.", "tokens": [307, 538, 1382, 281, 733, 295, 915, 613, 48994, 6771, 13], "temperature": 0.0, "avg_logprob": -0.12152182400881589, "compression_ratio": 1.6695652173913043, "no_speech_prob": 1.4969818948884495e-05}, {"id": 492, "seek": 289800, "start": 2918.0, "end": 2921.0, "text": " It makes sense to look and see what they found, right?", "tokens": [467, 1669, 2020, 281, 574, 293, 536, 437, 436, 1352, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12152182400881589, "compression_ratio": 1.6695652173913043, "no_speech_prob": 1.4969818948884495e-05}, {"id": 493, "seek": 289800, "start": 2921.0, "end": 2925.0, "text": " Particularly since as well as finding latent factors,", "tokens": [32281, 1670, 382, 731, 382, 5006, 48994, 6771, 11], "temperature": 0.0, "avg_logprob": -0.12152182400881589, "compression_ratio": 1.6695652173913043, "no_speech_prob": 1.4969818948884495e-05}, {"id": 494, "seek": 292500, "start": 2925.0, "end": 2931.0, "text": " we also now have a specific bias number for every user and every movie, right?", "tokens": [321, 611, 586, 362, 257, 2685, 12577, 1230, 337, 633, 4195, 293, 633, 3169, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.06433910192902555, "compression_ratio": 1.6255506607929515, "no_speech_prob": 1.722994420561008e-05}, {"id": 495, "seek": 292500, "start": 2931.0, "end": 2937.0, "text": " Now, you could just say, what's the average rating for each movie?", "tokens": [823, 11, 291, 727, 445, 584, 11, 437, 311, 264, 4274, 10990, 337, 1184, 3169, 30], "temperature": 0.0, "avg_logprob": -0.06433910192902555, "compression_ratio": 1.6255506607929515, "no_speech_prob": 1.722994420561008e-05}, {"id": 496, "seek": 292500, "start": 2937.0, "end": 2939.0, "text": " But there's a few issues with that.", "tokens": [583, 456, 311, 257, 1326, 2663, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.06433910192902555, "compression_ratio": 1.6255506607929515, "no_speech_prob": 1.722994420561008e-05}, {"id": 497, "seek": 292500, "start": 2939.0, "end": 2943.0, "text": " In particular, this is something you see a lot with like anime.", "tokens": [682, 1729, 11, 341, 307, 746, 291, 536, 257, 688, 365, 411, 12435, 13], "temperature": 0.0, "avg_logprob": -0.06433910192902555, "compression_ratio": 1.6255506607929515, "no_speech_prob": 1.722994420561008e-05}, {"id": 498, "seek": 292500, "start": 2943.0, "end": 2947.0, "text": " People who like anime just love anime, right?", "tokens": [3432, 567, 411, 12435, 445, 959, 12435, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.06433910192902555, "compression_ratio": 1.6255506607929515, "no_speech_prob": 1.722994420561008e-05}, {"id": 499, "seek": 292500, "start": 2947.0, "end": 2951.0, "text": " And so they watch lots of anime and then they just rate all the anime highly.", "tokens": [400, 370, 436, 1159, 3195, 295, 12435, 293, 550, 436, 445, 3314, 439, 264, 12435, 5405, 13], "temperature": 0.0, "avg_logprob": -0.06433910192902555, "compression_ratio": 1.6255506607929515, "no_speech_prob": 1.722994420561008e-05}, {"id": 500, "seek": 295100, "start": 2951.0, "end": 2957.0, "text": " And so very often on kind of charts of movies, you'll see a lot of anime at the top.", "tokens": [400, 370, 588, 2049, 322, 733, 295, 17767, 295, 6233, 11, 291, 603, 536, 257, 688, 295, 12435, 412, 264, 1192, 13], "temperature": 0.0, "avg_logprob": -0.05744475247908612, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.976806354126893e-05}, {"id": 501, "seek": 295100, "start": 2957.0, "end": 2962.0, "text": " Particularly if it's like, you know, a hundred long series of anime,", "tokens": [32281, 498, 309, 311, 411, 11, 291, 458, 11, 257, 3262, 938, 2638, 295, 12435, 11], "temperature": 0.0, "avg_logprob": -0.05744475247908612, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.976806354126893e-05}, {"id": 502, "seek": 295100, "start": 2962.0, "end": 2969.0, "text": " you'll find, you know, every single item of that series in the top thousand movie list or something.", "tokens": [291, 603, 915, 11, 291, 458, 11, 633, 2167, 3174, 295, 300, 2638, 294, 264, 1192, 4714, 3169, 1329, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.05744475247908612, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.976806354126893e-05}, {"id": 503, "seek": 295100, "start": 2969.0, "end": 2971.0, "text": " So how do we deal with that?", "tokens": [407, 577, 360, 321, 2028, 365, 300, 30], "temperature": 0.0, "avg_logprob": -0.05744475247908612, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.976806354126893e-05}, {"id": 504, "seek": 295100, "start": 2971.0, "end": 2976.0, "text": " Well, the nice thing is that instead if we look at the movie bias, right?", "tokens": [1042, 11, 264, 1481, 551, 307, 300, 2602, 498, 321, 574, 412, 264, 3169, 12577, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.05744475247908612, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.976806354126893e-05}, {"id": 505, "seek": 297600, "start": 2976.0, "end": 2982.0, "text": " The movie bias says kind of once we've included the user bias, right?", "tokens": [440, 3169, 12577, 1619, 733, 295, 1564, 321, 600, 5556, 264, 4195, 12577, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.06119182734813505, "compression_ratio": 1.7254098360655739, "no_speech_prob": 1.0951666808978189e-05}, {"id": 506, "seek": 297600, "start": 2982.0, "end": 2987.0, "text": " Which for an anime lover might be a very high number because they're just rating a lot of movies highly.", "tokens": [3013, 337, 364, 12435, 18009, 1062, 312, 257, 588, 1090, 1230, 570, 436, 434, 445, 10990, 257, 688, 295, 6233, 5405, 13], "temperature": 0.0, "avg_logprob": -0.06119182734813505, "compression_ratio": 1.7254098360655739, "no_speech_prob": 1.0951666808978189e-05}, {"id": 507, "seek": 297600, "start": 2987.0, "end": 2994.0, "text": " And once we account for the specifics of this kind of movie, which again might be people love anime, right?", "tokens": [400, 1564, 321, 2696, 337, 264, 28454, 295, 341, 733, 295, 3169, 11, 597, 797, 1062, 312, 561, 959, 12435, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.06119182734813505, "compression_ratio": 1.7254098360655739, "no_speech_prob": 1.0951666808978189e-05}, {"id": 508, "seek": 297600, "start": 2994.0, "end": 2998.0, "text": " What's left over is something specific to that movie itself.", "tokens": [708, 311, 1411, 670, 307, 746, 2685, 281, 300, 3169, 2564, 13], "temperature": 0.0, "avg_logprob": -0.06119182734813505, "compression_ratio": 1.7254098360655739, "no_speech_prob": 1.0951666808978189e-05}, {"id": 509, "seek": 297600, "start": 2998.0, "end": 3004.0, "text": " So it's kind of interesting to look at movie bias numbers as a way of saying,", "tokens": [407, 309, 311, 733, 295, 1880, 281, 574, 412, 3169, 12577, 3547, 382, 257, 636, 295, 1566, 11], "temperature": 0.0, "avg_logprob": -0.06119182734813505, "compression_ratio": 1.7254098360655739, "no_speech_prob": 1.0951666808978189e-05}, {"id": 510, "seek": 300400, "start": 3004.0, "end": 3008.0, "text": " what are the best movies or what do people really like as movies?", "tokens": [437, 366, 264, 1151, 6233, 420, 437, 360, 561, 534, 411, 382, 6233, 30], "temperature": 0.0, "avg_logprob": -0.12767046055895218, "compression_ratio": 1.688118811881188, "no_speech_prob": 2.9303768315003254e-05}, {"id": 511, "seek": 300400, "start": 3008.0, "end": 3017.0, "text": " Even if those people don't rate movies very highly or even if that movie doesn't have the kind of features that people tend to have rate highly.", "tokens": [2754, 498, 729, 561, 500, 380, 3314, 6233, 588, 5405, 420, 754, 498, 300, 3169, 1177, 380, 362, 264, 733, 295, 4122, 300, 561, 3928, 281, 362, 3314, 5405, 13], "temperature": 0.0, "avg_logprob": -0.12767046055895218, "compression_ratio": 1.688118811881188, "no_speech_prob": 2.9303768315003254e-05}, {"id": 512, "seek": 300400, "start": 3017.0, "end": 3022.0, "text": " So it's kind of nice. It's funny to say this.", "tokens": [407, 309, 311, 733, 295, 1481, 13, 467, 311, 4074, 281, 584, 341, 13], "temperature": 0.0, "avg_logprob": -0.12767046055895218, "compression_ratio": 1.688118811881188, "no_speech_prob": 2.9303768315003254e-05}, {"id": 513, "seek": 300400, "start": 3022.0, "end": 3027.0, "text": " And by using the bias, we get an unbiased kind of movie score.", "tokens": [400, 538, 1228, 264, 12577, 11, 321, 483, 364, 517, 5614, 1937, 733, 295, 3169, 6175, 13], "temperature": 0.0, "avg_logprob": -0.12767046055895218, "compression_ratio": 1.688118811881188, "no_speech_prob": 2.9303768315003254e-05}, {"id": 514, "seek": 300400, "start": 3027.0, "end": 3029.0, "text": " So how do we do that?", "tokens": [407, 577, 360, 321, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.12767046055895218, "compression_ratio": 1.688118811881188, "no_speech_prob": 2.9303768315003254e-05}, {"id": 515, "seek": 302900, "start": 3029.0, "end": 3039.0, "text": " Well, to make it interesting, particularly because this data set only goes to 1998,", "tokens": [1042, 11, 281, 652, 309, 1880, 11, 4098, 570, 341, 1412, 992, 787, 1709, 281, 21404, 11], "temperature": 0.0, "avg_logprob": -0.11536471436663372, "compression_ratio": 1.5355450236966826, "no_speech_prob": 1.643773248360958e-05}, {"id": 516, "seek": 302900, "start": 3039.0, "end": 3043.0, "text": " let's only look at movies that plenty of people watch.", "tokens": [718, 311, 787, 574, 412, 6233, 300, 7140, 295, 561, 1159, 13], "temperature": 0.0, "avg_logprob": -0.11536471436663372, "compression_ratio": 1.5355450236966826, "no_speech_prob": 1.643773248360958e-05}, {"id": 517, "seek": 302900, "start": 3043.0, "end": 3052.0, "text": " So we'll use pandas to grab our rating movie table, group it by title, and then count the number of ratings.", "tokens": [407, 321, 603, 764, 4565, 296, 281, 4444, 527, 10990, 3169, 3199, 11, 1594, 309, 538, 4876, 11, 293, 550, 1207, 264, 1230, 295, 24603, 13], "temperature": 0.0, "avg_logprob": -0.11536471436663372, "compression_ratio": 1.5355450236966826, "no_speech_prob": 1.643773248360958e-05}, {"id": 518, "seek": 302900, "start": 3052.0, "end": 3057.0, "text": " I'm not measuring how high their rating, just how many ratings do they have?", "tokens": [286, 478, 406, 13389, 577, 1090, 641, 10990, 11, 445, 577, 867, 24603, 360, 436, 362, 30], "temperature": 0.0, "avg_logprob": -0.11536471436663372, "compression_ratio": 1.5355450236966826, "no_speech_prob": 1.643773248360958e-05}, {"id": 519, "seek": 305700, "start": 3057.0, "end": 3065.0, "text": " And so the top thousand are the movies that have been rated the most.", "tokens": [400, 370, 264, 1192, 4714, 366, 264, 6233, 300, 362, 668, 22103, 264, 881, 13], "temperature": 0.0, "avg_logprob": -0.09206828936724595, "compression_ratio": 1.644578313253012, "no_speech_prob": 2.840076012944337e-05}, {"id": 520, "seek": 305700, "start": 3065.0, "end": 3068.0, "text": " And so they're hopefully movies that we might have seen.", "tokens": [400, 370, 436, 434, 4696, 6233, 300, 321, 1062, 362, 1612, 13], "temperature": 0.0, "avg_logprob": -0.09206828936724595, "compression_ratio": 1.644578313253012, "no_speech_prob": 2.840076012944337e-05}, {"id": 521, "seek": 305700, "start": 3068.0, "end": 3076.0, "text": " That's the only reason I'm doing this. And so I've called this top movies, by which I mean not good movies, just movies we're likely to have seen.", "tokens": [663, 311, 264, 787, 1778, 286, 478, 884, 341, 13, 400, 370, 286, 600, 1219, 341, 1192, 6233, 11, 538, 597, 286, 914, 406, 665, 6233, 11, 445, 6233, 321, 434, 3700, 281, 362, 1612, 13], "temperature": 0.0, "avg_logprob": -0.09206828936724595, "compression_ratio": 1.644578313253012, "no_speech_prob": 2.840076012944337e-05}, {"id": 522, "seek": 307600, "start": 3076.0, "end": 3088.0, "text": " But not surprisingly, Star Wars is the one that at that point most the most people had put a rating to.", "tokens": [583, 406, 17600, 11, 5705, 9818, 307, 264, 472, 300, 412, 300, 935, 881, 264, 881, 561, 632, 829, 257, 10990, 281, 13], "temperature": 0.0, "avg_logprob": -0.11665872732798259, "compression_ratio": 1.4746835443037976, "no_speech_prob": 4.8601996240904555e-06}, {"id": 523, "seek": 307600, "start": 3088.0, "end": 3090.0, "text": " Independence Day. There you go.", "tokens": [33631, 5226, 13, 821, 291, 352, 13], "temperature": 0.0, "avg_logprob": -0.11665872732798259, "compression_ratio": 1.4746835443037976, "no_speech_prob": 4.8601996240904555e-06}, {"id": 524, "seek": 307600, "start": 3090.0, "end": 3105.0, "text": " So we can then take our learner that we trained and ask it for the bias of the items listed here.", "tokens": [407, 321, 393, 550, 747, 527, 33347, 300, 321, 8895, 293, 1029, 309, 337, 264, 12577, 295, 264, 4754, 10052, 510, 13], "temperature": 0.0, "avg_logprob": -0.11665872732798259, "compression_ratio": 1.4746835443037976, "no_speech_prob": 4.8601996240904555e-06}, {"id": 525, "seek": 310500, "start": 3105.0, "end": 3107.0, "text": " So is item equals true?", "tokens": [407, 307, 3174, 6915, 2074, 30], "temperature": 0.0, "avg_logprob": -0.1296622717558448, "compression_ratio": 1.6125, "no_speech_prob": 2.6685096599976532e-05}, {"id": 526, "seek": 310500, "start": 3107.0, "end": 3112.0, "text": " You would pass true to say I want the items or false to say I want the users.", "tokens": [509, 576, 1320, 2074, 281, 584, 286, 528, 264, 4754, 420, 7908, 281, 584, 286, 528, 264, 5022, 13], "temperature": 0.0, "avg_logprob": -0.1296622717558448, "compression_ratio": 1.6125, "no_speech_prob": 2.6685096599976532e-05}, {"id": 527, "seek": 310500, "start": 3112.0, "end": 3119.0, "text": " So this is kind of like a pretty common piece of momentum for collaborative filtering.", "tokens": [407, 341, 307, 733, 295, 411, 257, 1238, 2689, 2522, 295, 11244, 337, 16555, 30822, 13], "temperature": 0.0, "avg_logprob": -0.1296622717558448, "compression_ratio": 1.6125, "no_speech_prob": 2.6685096599976532e-05}, {"id": 528, "seek": 310500, "start": 3119.0, "end": 3127.0, "text": " These IDs tend to be called users. These IDs tend to be called items.", "tokens": [1981, 48212, 3928, 281, 312, 1219, 5022, 13, 1981, 48212, 3928, 281, 312, 1219, 4754, 13], "temperature": 0.0, "avg_logprob": -0.1296622717558448, "compression_ratio": 1.6125, "no_speech_prob": 2.6685096599976532e-05}, {"id": 529, "seek": 312700, "start": 3127.0, "end": 3135.0, "text": " Even if your problem has got nothing to do with users and items at all, you know, we just use these names for convenience.", "tokens": [2754, 498, 428, 1154, 575, 658, 1825, 281, 360, 365, 5022, 293, 4754, 412, 439, 11, 291, 458, 11, 321, 445, 764, 613, 5288, 337, 19283, 13], "temperature": 0.0, "avg_logprob": -0.08250710597405067, "compression_ratio": 1.6585365853658536, "no_speech_prob": 1.9221633920096792e-05}, {"id": 530, "seek": 312700, "start": 3135.0, "end": 3140.0, "text": " OK, so they're just they're just words. So in our case, we want the items.", "tokens": [2264, 11, 370, 436, 434, 445, 436, 434, 445, 2283, 13, 407, 294, 527, 1389, 11, 321, 528, 264, 4754, 13], "temperature": 0.0, "avg_logprob": -0.08250710597405067, "compression_ratio": 1.6585365853658536, "no_speech_prob": 1.9221633920096792e-05}, {"id": 531, "seek": 312700, "start": 3140.0, "end": 3143.0, "text": " This is the list of items we want. We want the bias.", "tokens": [639, 307, 264, 1329, 295, 4754, 321, 528, 13, 492, 528, 264, 12577, 13], "temperature": 0.0, "avg_logprob": -0.08250710597405067, "compression_ratio": 1.6585365853658536, "no_speech_prob": 1.9221633920096792e-05}, {"id": 532, "seek": 312700, "start": 3143.0, "end": 3152.0, "text": " So this is specific to collaborative filtering. And so that's going to give us back a thousand numbers because we asked for this has a thousand movies in it.", "tokens": [407, 341, 307, 2685, 281, 16555, 30822, 13, 400, 370, 300, 311, 516, 281, 976, 505, 646, 257, 4714, 3547, 570, 321, 2351, 337, 341, 575, 257, 4714, 6233, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.08250710597405067, "compression_ratio": 1.6585365853658536, "no_speech_prob": 1.9221633920096792e-05}, {"id": 533, "seek": 315200, "start": 3152.0, "end": 3161.0, "text": " So we can now take and just for comparison, let's also group the titles by the mean rating.", "tokens": [407, 321, 393, 586, 747, 293, 445, 337, 9660, 11, 718, 311, 611, 1594, 264, 12992, 538, 264, 914, 10990, 13], "temperature": 0.0, "avg_logprob": -0.06704648335774739, "compression_ratio": 1.6137931034482758, "no_speech_prob": 2.0781842977157794e-05}, {"id": 534, "seek": 315200, "start": 3161.0, "end": 3172.0, "text": " So then we can zip through. So going through together each of the movies along with the bias and grab their rating and the bias and the movie.", "tokens": [407, 550, 321, 393, 20730, 807, 13, 407, 516, 807, 1214, 1184, 295, 264, 6233, 2051, 365, 264, 12577, 293, 4444, 641, 10990, 293, 264, 12577, 293, 264, 3169, 13], "temperature": 0.0, "avg_logprob": -0.06704648335774739, "compression_ratio": 1.6137931034482758, "no_speech_prob": 2.0781842977157794e-05}, {"id": 535, "seek": 317200, "start": 3172.0, "end": 3183.0, "text": " And then we can sort them all by the zero index thing, which is the bias.", "tokens": [400, 550, 321, 393, 1333, 552, 439, 538, 264, 4018, 8186, 551, 11, 597, 307, 264, 12577, 13], "temperature": 0.0, "avg_logprob": -0.10133864832859413, "compression_ratio": 1.3134328358208955, "no_speech_prob": 7.295411705854349e-06}, {"id": 536, "seek": 317200, "start": 3183.0, "end": 3195.0, "text": " So here are the lowest numbers. So I can say, you know, Mortal Kombat Annihilation, not a great movie.", "tokens": [407, 510, 366, 264, 12437, 3547, 13, 407, 286, 393, 584, 11, 291, 458, 11, 45797, 49131, 1107, 3722, 71, 16067, 11, 406, 257, 869, 3169, 13], "temperature": 0.0, "avg_logprob": -0.10133864832859413, "compression_ratio": 1.3134328358208955, "no_speech_prob": 7.295411705854349e-06}, {"id": 537, "seek": 319500, "start": 3195.0, "end": 3202.0, "text": " Mon Mo Man 2, not a great movie. I haven't seen Children of the Corn, but we did have a long discussion at SF study group today and people who have seen it agree.", "tokens": [4713, 3335, 2458, 568, 11, 406, 257, 869, 3169, 13, 286, 2378, 380, 1612, 13354, 295, 264, 21590, 11, 457, 321, 630, 362, 257, 938, 5017, 412, 31095, 2979, 1594, 965, 293, 561, 567, 362, 1612, 309, 3986, 13], "temperature": 0.0, "avg_logprob": -0.17033427102225168, "compression_ratio": 1.5689655172413792, "no_speech_prob": 6.540135927934898e-06}, {"id": 538, "seek": 319500, "start": 3202.0, "end": 3214.0, "text": " Not a great movie. And you can kind of see like some of them actually have pretty decent ratings, even though like relative to.", "tokens": [1726, 257, 869, 3169, 13, 400, 291, 393, 733, 295, 536, 411, 512, 295, 552, 767, 362, 1238, 8681, 24603, 11, 754, 1673, 411, 4972, 281, 13], "temperature": 0.0, "avg_logprob": -0.17033427102225168, "compression_ratio": 1.5689655172413792, "no_speech_prob": 6.540135927934898e-06}, {"id": 539, "seek": 319500, "start": 3214.0, "end": 3219.0, "text": " Right. So this one's actually got a much higher rating than the next one.", "tokens": [1779, 13, 407, 341, 472, 311, 767, 658, 257, 709, 2946, 10990, 813, 264, 958, 472, 13], "temperature": 0.0, "avg_logprob": -0.17033427102225168, "compression_ratio": 1.5689655172413792, "no_speech_prob": 6.540135927934898e-06}, {"id": 540, "seek": 321900, "start": 3219.0, "end": 3231.0, "text": " Right. But, you know, that's kind of saying, well, the kind of actors that were in this and the kind of movie that this was and the kind of people who do like it, who watch it, you would expect it to be higher.", "tokens": [1779, 13, 583, 11, 291, 458, 11, 300, 311, 733, 295, 1566, 11, 731, 11, 264, 733, 295, 10037, 300, 645, 294, 341, 293, 264, 733, 295, 3169, 300, 341, 390, 293, 264, 733, 295, 561, 567, 360, 411, 309, 11, 567, 1159, 309, 11, 291, 576, 2066, 309, 281, 312, 2946, 13], "temperature": 0.0, "avg_logprob": -0.13294536784543828, "compression_ratio": 1.703125, "no_speech_prob": 3.119893517578021e-05}, {"id": 541, "seek": 321900, "start": 3231.0, "end": 3238.0, "text": " And then here's the sort by reverse. OK, Shinzo's List, Titanic, Shawshank Redemption seems reasonable.", "tokens": [400, 550, 510, 311, 264, 1333, 538, 9943, 13, 2264, 11, 17347, 4765, 311, 17668, 11, 42183, 11, 27132, 2716, 657, 4477, 26033, 2544, 10585, 13], "temperature": 0.0, "avg_logprob": -0.13294536784543828, "compression_ratio": 1.703125, "no_speech_prob": 3.119893517578021e-05}, {"id": 542, "seek": 321900, "start": 3238.0, "end": 3246.0, "text": " And again, you can kind of look for ones where like the rating, you know, isn't that high, but it's still very high here.", "tokens": [400, 797, 11, 291, 393, 733, 295, 574, 337, 2306, 689, 411, 264, 10990, 11, 291, 458, 11, 1943, 380, 300, 1090, 11, 457, 309, 311, 920, 588, 1090, 510, 13], "temperature": 0.0, "avg_logprob": -0.13294536784543828, "compression_ratio": 1.703125, "no_speech_prob": 3.119893517578021e-05}, {"id": 543, "seek": 324600, "start": 3246.0, "end": 3258.0, "text": " So that's kind of like, you know, at least in 1998, people weren't that into Leonardo DiCaprio or, you know, people aren't that into dialogue driven movies or people aren't that into romances or whatever.", "tokens": [407, 300, 311, 733, 295, 411, 11, 291, 458, 11, 412, 1935, 294, 21404, 11, 561, 4999, 380, 300, 666, 36523, 8789, 46671, 6584, 420, 11, 291, 458, 11, 561, 3212, 380, 300, 666, 10221, 9555, 6233, 420, 561, 3212, 380, 300, 666, 7438, 2676, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.07711957681058633, "compression_ratio": 1.6906779661016949, "no_speech_prob": 1.8920956790680066e-05}, {"id": 544, "seek": 324600, "start": 3258.0, "end": 3267.0, "text": " But still people liked it more than you would expect. So it's interesting to kind of like interpret our models in this way.", "tokens": [583, 920, 561, 4501, 309, 544, 813, 291, 576, 2066, 13, 407, 309, 311, 1880, 281, 733, 295, 411, 7302, 527, 5245, 294, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.07711957681058633, "compression_ratio": 1.6906779661016949, "no_speech_prob": 1.8920956790680066e-05}, {"id": 545, "seek": 324600, "start": 3267.0, "end": 3273.0, "text": " We can go a bit further and grab not just the biases, but the weights.", "tokens": [492, 393, 352, 257, 857, 3052, 293, 4444, 406, 445, 264, 32152, 11, 457, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.07711957681058633, "compression_ratio": 1.6906779661016949, "no_speech_prob": 1.8920956790680066e-05}, {"id": 546, "seek": 327300, "start": 3273.0, "end": 3279.0, "text": " So that is these things.", "tokens": [407, 300, 307, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.06509183299156927, "compression_ratio": 1.5496688741721854, "no_speech_prob": 2.586493792477995e-05}, {"id": 547, "seek": 327300, "start": 3279.0, "end": 3284.0, "text": " And again, we're going to grab the weights for the items for our top movies.", "tokens": [400, 797, 11, 321, 434, 516, 281, 4444, 264, 17443, 337, 264, 4754, 337, 527, 1192, 6233, 13], "temperature": 0.0, "avg_logprob": -0.06509183299156927, "compression_ratio": 1.5496688741721854, "no_speech_prob": 2.586493792477995e-05}, {"id": 548, "seek": 327300, "start": 3284.0, "end": 3296.0, "text": " And that is a thousand by forty because we asked for forty factors. So rather than having a width of five, we have a width of forty.", "tokens": [400, 300, 307, 257, 4714, 538, 15815, 570, 321, 2351, 337, 15815, 6771, 13, 407, 2831, 813, 1419, 257, 11402, 295, 1732, 11, 321, 362, 257, 11402, 295, 15815, 13], "temperature": 0.0, "avg_logprob": -0.06509183299156927, "compression_ratio": 1.5496688741721854, "no_speech_prob": 2.586493792477995e-05}, {"id": 549, "seek": 329600, "start": 3296.0, "end": 3304.0, "text": " Often, really, there's there isn't really conceptually forty latent factors involved in taste.", "tokens": [20043, 11, 534, 11, 456, 311, 456, 1943, 380, 534, 3410, 671, 15815, 48994, 6771, 3288, 294, 3939, 13], "temperature": 0.0, "avg_logprob": -0.07561632089836653, "compression_ratio": 1.610091743119266, "no_speech_prob": 1.0782626304717269e-05}, {"id": 550, "seek": 329600, "start": 3304.0, "end": 3310.0, "text": " And so trying to look at the forty can be, you know, not that intuitive.", "tokens": [400, 370, 1382, 281, 574, 412, 264, 15815, 393, 312, 11, 291, 458, 11, 406, 300, 21769, 13], "temperature": 0.0, "avg_logprob": -0.07561632089836653, "compression_ratio": 1.610091743119266, "no_speech_prob": 1.0782626304717269e-05}, {"id": 551, "seek": 329600, "start": 3310.0, "end": 3317.0, "text": " So what we want to do is we want to squish those forty down to just three.", "tokens": [407, 437, 321, 528, 281, 360, 307, 321, 528, 281, 31379, 729, 15815, 760, 281, 445, 1045, 13], "temperature": 0.0, "avg_logprob": -0.07561632089836653, "compression_ratio": 1.610091743119266, "no_speech_prob": 1.0782626304717269e-05}, {"id": 552, "seek": 329600, "start": 3317.0, "end": 3321.0, "text": " And there's something that we're not going to look into called PCA stands for principal components analysis.", "tokens": [400, 456, 311, 746, 300, 321, 434, 406, 516, 281, 574, 666, 1219, 6465, 32, 7382, 337, 9716, 6677, 5215, 13], "temperature": 0.0, "avg_logprob": -0.07561632089836653, "compression_ratio": 1.610091743119266, "no_speech_prob": 1.0782626304717269e-05}, {"id": 553, "seek": 332100, "start": 3321.0, "end": 3327.0, "text": " So this is a movie. W is a torch tensor and fast.", "tokens": [407, 341, 307, 257, 3169, 13, 343, 307, 257, 27822, 40863, 293, 2370, 13], "temperature": 0.0, "avg_logprob": -0.1607860389508699, "compression_ratio": 1.5757575757575757, "no_speech_prob": 2.045975998044014e-05}, {"id": 554, "seek": 332100, "start": 3327.0, "end": 3331.0, "text": " I adds the PCA method to torch tensors.", "tokens": [286, 10860, 264, 6465, 32, 3170, 281, 27822, 10688, 830, 13], "temperature": 0.0, "avg_logprob": -0.1607860389508699, "compression_ratio": 1.5757575757575757, "no_speech_prob": 2.045975998044014e-05}, {"id": 555, "seek": 332100, "start": 3331.0, "end": 3348.0, "text": " And what PCA does principle components analysis is it's a simple linear transformation that takes an input matrix and tries to find a smaller number of columns that kind of cover a lot of the space of that original matrix.", "tokens": [400, 437, 6465, 32, 775, 8665, 6677, 5215, 307, 309, 311, 257, 2199, 8213, 9887, 300, 2516, 364, 4846, 8141, 293, 9898, 281, 915, 257, 4356, 1230, 295, 13766, 300, 733, 295, 2060, 257, 688, 295, 264, 1901, 295, 300, 3380, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1607860389508699, "compression_ratio": 1.5757575757575757, "no_speech_prob": 2.045975998044014e-05}, {"id": 556, "seek": 334800, "start": 3348.0, "end": 3366.0, "text": " If that sounds interesting, which it totally is, you should check out our course computational linear algebra, which Rachel teaches, where we will show you how to calculate PCA from scratch and why you'd want to do it and lots of stuff like that.", "tokens": [759, 300, 3263, 1880, 11, 597, 309, 3879, 307, 11, 291, 820, 1520, 484, 527, 1164, 28270, 8213, 21989, 11, 597, 14246, 16876, 11, 689, 321, 486, 855, 291, 577, 281, 8873, 6465, 32, 490, 8459, 293, 983, 291, 1116, 528, 281, 360, 309, 293, 3195, 295, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.0961533682686942, "compression_ratio": 1.502415458937198, "no_speech_prob": 2.144324935215991e-05}, {"id": 557, "seek": 334800, "start": 3366.0, "end": 3371.0, "text": " It is absolutely not a prerequisite for anything in this course.", "tokens": [467, 307, 3122, 406, 257, 38333, 34152, 337, 1340, 294, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.0961533682686942, "compression_ratio": 1.502415458937198, "no_speech_prob": 2.144324935215991e-05}, {"id": 558, "seek": 337100, "start": 3371.0, "end": 3383.0, "text": " But it's definitely worth knowing that taking layers of neural nets and chucking them through PCA is very often a good idea because very often you have like way more activations than you want in a layer.", "tokens": [583, 309, 311, 2138, 3163, 5276, 300, 1940, 7914, 295, 18161, 36170, 293, 20870, 278, 552, 807, 6465, 32, 307, 588, 2049, 257, 665, 1558, 570, 588, 2049, 291, 362, 411, 636, 544, 2430, 763, 813, 291, 528, 294, 257, 4583, 13], "temperature": 0.0, "avg_logprob": -0.07987052675277467, "compression_ratio": 1.5141242937853108, "no_speech_prob": 1.0288686098647304e-05}, {"id": 559, "seek": 337100, "start": 3383.0, "end": 3386.0, "text": " And there's all kinds of reasons you might want to play with it.", "tokens": [400, 456, 311, 439, 3685, 295, 4112, 291, 1062, 528, 281, 862, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.07987052675277467, "compression_ratio": 1.5141242937853108, "no_speech_prob": 1.0288686098647304e-05}, {"id": 560, "seek": 338600, "start": 3386.0, "end": 3403.0, "text": " For example, Francisco, who's sitting next to me today, is has been working on something to do image similarity. Right. And for image similarity, a nice way to do that is to compare activations from a model.", "tokens": [1171, 1365, 11, 12279, 11, 567, 311, 3798, 958, 281, 385, 965, 11, 307, 575, 668, 1364, 322, 746, 281, 360, 3256, 32194, 13, 1779, 13, 400, 337, 3256, 32194, 11, 257, 1481, 636, 281, 360, 300, 307, 281, 6794, 2430, 763, 490, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11180319851391936, "compression_ratio": 1.54, "no_speech_prob": 1.0781695891637355e-05}, {"id": 561, "seek": 338600, "start": 3403.0, "end": 3408.0, "text": " But often those activations will be huge and therefore your thing could be really slow and unwieldy.", "tokens": [583, 2049, 729, 2430, 763, 486, 312, 2603, 293, 4412, 428, 551, 727, 312, 534, 2964, 293, 14853, 1789, 88, 13], "temperature": 0.0, "avg_logprob": -0.11180319851391936, "compression_ratio": 1.54, "no_speech_prob": 1.0781695891637355e-05}, {"id": 562, "seek": 340800, "start": 3408.0, "end": 3416.0, "text": " So people often for something like image similarity will chuck it through a PCA first. And that's kind of cool.", "tokens": [407, 561, 2049, 337, 746, 411, 3256, 32194, 486, 20870, 309, 807, 257, 6465, 32, 700, 13, 400, 300, 311, 733, 295, 1627, 13], "temperature": 0.0, "avg_logprob": -0.10407908396287398, "compression_ratio": 1.5109170305676856, "no_speech_prob": 9.515334568277467e-06}, {"id": 563, "seek": 340800, "start": 3416.0, "end": 3421.0, "text": " In our case, we're just going to do it so that we take our 40 components down to three components.", "tokens": [682, 527, 1389, 11, 321, 434, 445, 516, 281, 360, 309, 370, 300, 321, 747, 527, 3356, 6677, 760, 281, 1045, 6677, 13], "temperature": 0.0, "avg_logprob": -0.10407908396287398, "compression_ratio": 1.5109170305676856, "no_speech_prob": 9.515334568277467e-06}, {"id": 564, "seek": 340800, "start": 3421.0, "end": 3425.0, "text": " So hopefully they'll be easier for us to interpret.", "tokens": [407, 4696, 436, 603, 312, 3571, 337, 505, 281, 7302, 13], "temperature": 0.0, "avg_logprob": -0.10407908396287398, "compression_ratio": 1.5109170305676856, "no_speech_prob": 9.515334568277467e-06}, {"id": 565, "seek": 340800, "start": 3425.0, "end": 3431.0, "text": " So we can grab each of those three factors. We'll call them factor not one and two.", "tokens": [407, 321, 393, 4444, 1184, 295, 729, 1045, 6771, 13, 492, 603, 818, 552, 5952, 406, 472, 293, 732, 13], "temperature": 0.0, "avg_logprob": -0.10407908396287398, "compression_ratio": 1.5109170305676856, "no_speech_prob": 9.515334568277467e-06}, {"id": 566, "seek": 343100, "start": 3431.0, "end": 3448.0, "text": " And let's grab that movie components and then sort. And now the thing is we have no idea what this is going to mean, but we're pretty sure it's going to be some aspect of taste and movie feature.", "tokens": [400, 718, 311, 4444, 300, 3169, 6677, 293, 550, 1333, 13, 400, 586, 264, 551, 307, 321, 362, 572, 1558, 437, 341, 307, 516, 281, 914, 11, 457, 321, 434, 1238, 988, 309, 311, 516, 281, 312, 512, 4171, 295, 3939, 293, 3169, 4111, 13], "temperature": 0.0, "avg_logprob": -0.0547939125372439, "compression_ratio": 1.4233576642335766, "no_speech_prob": 1.2218386473250575e-05}, {"id": 567, "seek": 344800, "start": 3448.0, "end": 3468.0, "text": " So if we print it out, the top and the bottom, we can see that the highest ranked things on this feature, you would kind of describe them as, you know, connoisseurs movies, I guess, you know, like Chinatown, you know, really classic Jack Nicholson movie.", "tokens": [407, 498, 321, 4482, 309, 484, 11, 264, 1192, 293, 264, 2767, 11, 321, 393, 536, 300, 264, 6343, 20197, 721, 322, 341, 4111, 11, 291, 576, 733, 295, 6786, 552, 382, 11, 291, 458, 11, 416, 1771, 7746, 2156, 6233, 11, 286, 2041, 11, 291, 458, 11, 411, 4430, 267, 648, 11, 291, 458, 11, 534, 7230, 4718, 17102, 401, 3015, 3169, 13], "temperature": 0.0, "avg_logprob": -0.11404254219748756, "compression_ratio": 1.625, "no_speech_prob": 7.646044650755357e-06}, {"id": 568, "seek": 344800, "start": 3468.0, "end": 3477.0, "text": " Everybody knows Casablanca and even like Wrong Trousers is like this kind of classic claymation movie and so forth. Right.", "tokens": [7646, 3255, 16100, 455, 8658, 496, 293, 754, 411, 28150, 1765, 563, 433, 307, 411, 341, 733, 295, 7230, 13517, 76, 399, 3169, 293, 370, 5220, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.11404254219748756, "compression_ratio": 1.625, "no_speech_prob": 7.646044650755357e-06}, {"id": 569, "seek": 347700, "start": 3477.0, "end": 3484.0, "text": " So, yeah, this this is definitely measuring like things that are very high on the kind of connoisseur level.", "tokens": [407, 11, 1338, 11, 341, 341, 307, 2138, 13389, 411, 721, 300, 366, 588, 1090, 322, 264, 733, 295, 416, 1771, 7746, 374, 1496, 13], "temperature": 0.0, "avg_logprob": -0.13354287202330842, "compression_ratio": 1.508849557522124, "no_speech_prob": 1.618670103198383e-05}, {"id": 570, "seek": 347700, "start": 3484.0, "end": 3490.0, "text": " Where else? Maybe Home Alone 3, not such a favorite with connoisseurs, perhaps.", "tokens": [2305, 1646, 30, 2704, 8719, 42056, 805, 11, 406, 1270, 257, 2954, 365, 416, 1771, 7746, 2156, 11, 4317, 13], "temperature": 0.0, "avg_logprob": -0.13354287202330842, "compression_ratio": 1.508849557522124, "no_speech_prob": 1.618670103198383e-05}, {"id": 571, "seek": 347700, "start": 3490.0, "end": 3498.0, "text": " It's just not to say that there aren't people who don't like it. Right. But probably not the same kind of people that would appreciate secrets and lies.", "tokens": [467, 311, 445, 406, 281, 584, 300, 456, 3212, 380, 561, 567, 500, 380, 411, 309, 13, 1779, 13, 583, 1391, 406, 264, 912, 733, 295, 561, 300, 576, 4449, 14093, 293, 9134, 13], "temperature": 0.0, "avg_logprob": -0.13354287202330842, "compression_ratio": 1.508849557522124, "no_speech_prob": 1.618670103198383e-05}, {"id": 572, "seek": 349800, "start": 3498.0, "end": 3507.0, "text": " So you can kind of see this idea that this is found some feature of movies and a corresponding feature of the kind of things people like. So let's look at another feature.", "tokens": [407, 291, 393, 733, 295, 536, 341, 1558, 300, 341, 307, 1352, 512, 4111, 295, 6233, 293, 257, 11760, 4111, 295, 264, 733, 295, 721, 561, 411, 13, 407, 718, 311, 574, 412, 1071, 4111, 13], "temperature": 0.0, "avg_logprob": -0.10265081458621556, "compression_ratio": 1.5789473684210527, "no_speech_prob": 4.565858944260981e-06}, {"id": 573, "seek": 349800, "start": 3507.0, "end": 3518.0, "text": " So here's factor number one. So this seems to have found like, OK, these are just big hits that you could watch with the family.", "tokens": [407, 510, 311, 5952, 1230, 472, 13, 407, 341, 2544, 281, 362, 1352, 411, 11, 2264, 11, 613, 366, 445, 955, 8664, 300, 291, 727, 1159, 365, 264, 1605, 13], "temperature": 0.0, "avg_logprob": -0.10265081458621556, "compression_ratio": 1.5789473684210527, "no_speech_prob": 4.565858944260981e-06}, {"id": 574, "seek": 351800, "start": 3518.0, "end": 3531.0, "text": " You know, these are definitely not that, you know, Trainspotting, very gritty kind of, you know, thing. So, again, it's kind of found this interesting feature of taste.", "tokens": [509, 458, 11, 613, 366, 2138, 406, 300, 11, 291, 458, 11, 5403, 1292, 17698, 783, 11, 588, 677, 10016, 733, 295, 11, 291, 458, 11, 551, 13, 407, 11, 797, 11, 309, 311, 733, 295, 1352, 341, 1880, 4111, 295, 3939, 13], "temperature": 0.0, "avg_logprob": -0.13301687491567513, "compression_ratio": 1.5108695652173914, "no_speech_prob": 1.8630640624905936e-05}, {"id": 575, "seek": 351800, "start": 3531.0, "end": 3538.0, "text": " And we could even like. Draw them on a graph, right. I just kind of them randomly to make them easier to see.", "tokens": [400, 321, 727, 754, 411, 13, 20386, 552, 322, 257, 4295, 11, 558, 13, 286, 445, 733, 295, 552, 16979, 281, 652, 552, 3571, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.13301687491567513, "compression_ratio": 1.5108695652173914, "no_speech_prob": 1.8630640624905936e-05}, {"id": 576, "seek": 353800, "start": 3538.0, "end": 3548.0, "text": " And you can kind of see like this is just the top 50 most popular movies by rating by how many times they've been rated.", "tokens": [400, 291, 393, 733, 295, 536, 411, 341, 307, 445, 264, 1192, 2625, 881, 3743, 6233, 538, 10990, 538, 577, 867, 1413, 436, 600, 668, 22103, 13], "temperature": 0.0, "avg_logprob": -0.17799025036039806, "compression_ratio": 1.59765625, "no_speech_prob": 5.7375276810489595e-05}, {"id": 577, "seek": 353800, "start": 3548.0, "end": 3555.0, "text": " And so kind of on this one factor, you've got that of the Terminators really high up here and the kind of English patient and Stindle's List at the other end.", "tokens": [400, 370, 733, 295, 322, 341, 472, 5952, 11, 291, 600, 658, 300, 295, 264, 19835, 259, 3391, 534, 1090, 493, 510, 293, 264, 733, 295, 3669, 4537, 293, 745, 471, 306, 311, 17668, 412, 264, 661, 917, 13], "temperature": 0.0, "avg_logprob": -0.17799025036039806, "compression_ratio": 1.59765625, "no_speech_prob": 5.7375276810489595e-05}, {"id": 578, "seek": 353800, "start": 3555.0, "end": 3563.0, "text": " And then kind of is your godfather and Monty Python over here and Independence Day and Liar Liar over there. So you get the idea.", "tokens": [400, 550, 733, 295, 307, 428, 3044, 11541, 293, 4713, 874, 15329, 670, 510, 293, 33631, 5226, 293, 8349, 289, 8349, 289, 670, 456, 13, 407, 291, 483, 264, 1558, 13], "temperature": 0.0, "avg_logprob": -0.17799025036039806, "compression_ratio": 1.59765625, "no_speech_prob": 5.7375276810489595e-05}, {"id": 579, "seek": 356300, "start": 3563.0, "end": 3581.0, "text": " So that's kind of fun. It would be interesting to see if you can come up with some stuff at work or other kind of data sets where you could try to pull out some some features and play with them.", "tokens": [407, 300, 311, 733, 295, 1019, 13, 467, 576, 312, 1880, 281, 536, 498, 291, 393, 808, 493, 365, 512, 1507, 412, 589, 420, 661, 733, 295, 1412, 6352, 689, 291, 727, 853, 281, 2235, 484, 512, 512, 4122, 293, 862, 365, 552, 13], "temperature": 0.0, "avg_logprob": -0.15400455181415265, "compression_ratio": 1.4518072289156627, "no_speech_prob": 2.4297651179949753e-05}, {"id": 580, "seek": 356300, "start": 3581.0, "end": 3586.0, "text": " So how does that work? Any questions?", "tokens": [407, 577, 775, 300, 589, 30, 2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.15400455181415265, "compression_ratio": 1.4518072289156627, "no_speech_prob": 2.4297651179949753e-05}, {"id": 581, "seek": 356300, "start": 3586.0, "end": 3590.0, "text": " One. OK.", "tokens": [1485, 13, 2264, 13], "temperature": 0.0, "avg_logprob": -0.15400455181415265, "compression_ratio": 1.4518072289156627, "no_speech_prob": 2.4297651179949753e-05}, {"id": 582, "seek": 359000, "start": 3590.0, "end": 3595.0, "text": " The question is, why am I sometimes getting negative loss?", "tokens": [440, 1168, 307, 11, 983, 669, 286, 2171, 1242, 3671, 4470, 30], "temperature": 0.0, "avg_logprob": -0.18998613749464896, "compression_ratio": 1.427027027027027, "no_speech_prob": 3.071367245865986e-05}, {"id": 583, "seek": 359000, "start": 3595.0, "end": 3598.0, "text": " When training.", "tokens": [1133, 3097, 13], "temperature": 0.0, "avg_logprob": -0.18998613749464896, "compression_ratio": 1.427027027027027, "no_speech_prob": 3.071367245865986e-05}, {"id": 584, "seek": 359000, "start": 3598.0, "end": 3600.0, "text": " You shouldn't be.", "tokens": [509, 4659, 380, 312, 13], "temperature": 0.0, "avg_logprob": -0.18998613749464896, "compression_ratio": 1.427027027027027, "no_speech_prob": 3.071367245865986e-05}, {"id": 585, "seek": 359000, "start": 3600.0, "end": 3605.0, "text": " So you're doing something wrong.", "tokens": [407, 291, 434, 884, 746, 2085, 13], "temperature": 0.0, "avg_logprob": -0.18998613749464896, "compression_ratio": 1.427027027027027, "no_speech_prob": 3.071367245865986e-05}, {"id": 586, "seek": 359000, "start": 3605.0, "end": 3611.0, "text": " So ask on show us your your particularly since people are uploading this, I guess other people seen it too.", "tokens": [407, 1029, 322, 855, 505, 428, 428, 4098, 1670, 561, 366, 27301, 341, 11, 286, 2041, 661, 561, 1612, 309, 886, 13], "temperature": 0.0, "avg_logprob": -0.18998613749464896, "compression_ratio": 1.427027027027027, "no_speech_prob": 3.071367245865986e-05}, {"id": 587, "seek": 359000, "start": 3611.0, "end": 3616.0, "text": " So put it on the forum. I mean,", "tokens": [407, 829, 309, 322, 264, 17542, 13, 286, 914, 11], "temperature": 0.0, "avg_logprob": -0.18998613749464896, "compression_ratio": 1.427027027027027, "no_speech_prob": 3.071367245865986e-05}, {"id": 588, "seek": 361600, "start": 3616.0, "end": 3625.0, "text": " they said they're doing negative log likelihood. Yeah. So we're going to be learning about cross entropy and negative log likelihood after the break today.", "tokens": [436, 848, 436, 434, 884, 3671, 3565, 22119, 13, 865, 13, 407, 321, 434, 516, 281, 312, 2539, 466, 3278, 30867, 293, 3671, 3565, 22119, 934, 264, 1821, 965, 13], "temperature": 0.0, "avg_logprob": -0.09408763214782044, "compression_ratio": 1.7433628318584071, "no_speech_prob": 1.7229416698683053e-05}, {"id": 589, "seek": 361600, "start": 3625.0, "end": 3630.0, "text": " They lost functions that have very specific expectations about what your input looks like.", "tokens": [814, 2731, 6828, 300, 362, 588, 2685, 9843, 466, 437, 428, 4846, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.09408763214782044, "compression_ratio": 1.7433628318584071, "no_speech_prob": 1.7229416698683053e-05}, {"id": 590, "seek": 361600, "start": 3630.0, "end": 3634.0, "text": " And if your input doesn't look like that, then they're going to give very weird answers.", "tokens": [400, 498, 428, 4846, 1177, 380, 574, 411, 300, 11, 550, 436, 434, 516, 281, 976, 588, 3657, 6338, 13], "temperature": 0.0, "avg_logprob": -0.09408763214782044, "compression_ratio": 1.7433628318584071, "no_speech_prob": 1.7229416698683053e-05}, {"id": 591, "seek": 361600, "start": 3634.0, "end": 3638.0, "text": " So probably you press the wrong buttons.", "tokens": [407, 1391, 291, 1886, 264, 2085, 9905, 13], "temperature": 0.0, "avg_logprob": -0.09408763214782044, "compression_ratio": 1.7433628318584071, "no_speech_prob": 1.7229416698683053e-05}, {"id": 592, "seek": 361600, "start": 3638.0, "end": 3641.0, "text": " So don't do that.", "tokens": [407, 500, 380, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.09408763214782044, "compression_ratio": 1.7433628318584071, "no_speech_prob": 1.7229416698683053e-05}, {"id": 593, "seek": 364100, "start": 3641.0, "end": 3650.0, "text": " OK. OK. So we said.", "tokens": [2264, 13, 2264, 13, 407, 321, 848, 13], "temperature": 0.0, "avg_logprob": -0.1768240186903212, "compression_ratio": 1.391304347826087, "no_speech_prob": 8.012651960598305e-06}, {"id": 594, "seek": 364100, "start": 3650.0, "end": 3654.0, "text": " Colab learner.", "tokens": [4004, 455, 33347, 13], "temperature": 0.0, "avg_logprob": -0.1768240186903212, "compression_ratio": 1.391304347826087, "no_speech_prob": 8.012651960598305e-06}, {"id": 595, "seek": 364100, "start": 3654.0, "end": 3661.0, "text": " And so here is the Colab learner function.", "tokens": [400, 370, 510, 307, 264, 4004, 455, 33347, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1768240186903212, "compression_ratio": 1.391304347826087, "no_speech_prob": 8.012651960598305e-06}, {"id": 596, "seek": 364100, "start": 3661.0, "end": 3668.0, "text": " The Colab learner function, as per usual, takes a.", "tokens": [440, 4004, 455, 33347, 2445, 11, 382, 680, 7713, 11, 2516, 257, 13], "temperature": 0.0, "avg_logprob": -0.1768240186903212, "compression_ratio": 1.391304347826087, "no_speech_prob": 8.012651960598305e-06}, {"id": 597, "seek": 366800, "start": 3668.0, "end": 3676.0, "text": " A data bunch and normally learners also take something where you ask for a particular architectural details.", "tokens": [316, 1412, 3840, 293, 5646, 23655, 611, 747, 746, 689, 291, 1029, 337, 257, 1729, 26621, 4365, 13], "temperature": 0.0, "avg_logprob": -0.14384707775744762, "compression_ratio": 1.7435897435897436, "no_speech_prob": 3.119665780104697e-05}, {"id": 598, "seek": 366800, "start": 3676.0, "end": 3683.0, "text": " In this case, there's only one thing which does that, which is basically do you want to use a multi layer neural net or do you want to use a classic collaborative filtering?", "tokens": [682, 341, 1389, 11, 456, 311, 787, 472, 551, 597, 775, 300, 11, 597, 307, 1936, 360, 291, 528, 281, 764, 257, 4825, 4583, 18161, 2533, 420, 360, 291, 528, 281, 764, 257, 7230, 16555, 30822, 30], "temperature": 0.0, "avg_logprob": -0.14384707775744762, "compression_ratio": 1.7435897435897436, "no_speech_prob": 3.119665780104697e-05}, {"id": 599, "seek": 366800, "start": 3683.0, "end": 3687.0, "text": " And we're only going to look at the classic collaborative filtering today.", "tokens": [400, 321, 434, 787, 516, 281, 574, 412, 264, 7230, 16555, 30822, 965, 13], "temperature": 0.0, "avg_logprob": -0.14384707775744762, "compression_ratio": 1.7435897435897436, "no_speech_prob": 3.119665780104697e-05}, {"id": 600, "seek": 366800, "start": 3687.0, "end": 3689.0, "text": " Or maybe you'll briefly look at the other one too.", "tokens": [1610, 1310, 291, 603, 10515, 574, 412, 264, 661, 472, 886, 13], "temperature": 0.0, "avg_logprob": -0.14384707775744762, "compression_ratio": 1.7435897435897436, "no_speech_prob": 3.119665780104697e-05}, {"id": 601, "seek": 368900, "start": 3689.0, "end": 3703.0, "text": " We'll see. And so what actually happens here? Well, basically we're going to create we create a an embedding bias model and then we pass back a learner which has our data and that model.", "tokens": [492, 603, 536, 13, 400, 370, 437, 767, 2314, 510, 30, 1042, 11, 1936, 321, 434, 516, 281, 1884, 321, 1884, 257, 364, 12240, 3584, 12577, 2316, 293, 550, 321, 1320, 646, 257, 33347, 597, 575, 527, 1412, 293, 300, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14905106149068692, "compression_ratio": 1.5951219512195123, "no_speech_prob": 5.1734673434111755e-06}, {"id": 602, "seek": 368900, "start": 3703.0, "end": 3708.0, "text": " So obviously all the interesting stuff is happening here and embedding bias.", "tokens": [407, 2745, 439, 264, 1880, 1507, 307, 2737, 510, 293, 12240, 3584, 12577, 13], "temperature": 0.0, "avg_logprob": -0.14905106149068692, "compression_ratio": 1.5951219512195123, "no_speech_prob": 5.1734673434111755e-06}, {"id": 603, "seek": 368900, "start": 3708.0, "end": 3710.0, "text": " So let's take a look at that.", "tokens": [407, 718, 311, 747, 257, 574, 412, 300, 13], "temperature": 0.0, "avg_logprob": -0.14905106149068692, "compression_ratio": 1.5951219512195123, "no_speech_prob": 5.1734673434111755e-06}, {"id": 604, "seek": 368900, "start": 3710.0, "end": 3712.0, "text": " I clearly press the wrong button.", "tokens": [286, 4448, 1886, 264, 2085, 2960, 13], "temperature": 0.0, "avg_logprob": -0.14905106149068692, "compression_ratio": 1.5951219512195123, "no_speech_prob": 5.1734673434111755e-06}, {"id": 605, "seek": 371200, "start": 3712.0, "end": 3719.0, "text": " Beding bias. There we go. OK. So.", "tokens": [363, 9794, 12577, 13, 821, 321, 352, 13, 2264, 13, 407, 13], "temperature": 0.0, "avg_logprob": -0.22971698216029576, "compression_ratio": 1.3571428571428572, "no_speech_prob": 2.15670547731861e-06}, {"id": 606, "seek": 371200, "start": 3719.0, "end": 3724.0, "text": " Here's our embedding bias model.", "tokens": [1692, 311, 527, 12240, 3584, 12577, 2316, 13], "temperature": 0.0, "avg_logprob": -0.22971698216029576, "compression_ratio": 1.3571428571428572, "no_speech_prob": 2.15670547731861e-06}, {"id": 607, "seek": 371200, "start": 3724.0, "end": 3728.0, "text": " It is a. NN dot module.", "tokens": [467, 307, 257, 13, 426, 45, 5893, 10088, 13], "temperature": 0.0, "avg_logprob": -0.22971698216029576, "compression_ratio": 1.3571428571428572, "no_speech_prob": 2.15670547731861e-06}, {"id": 608, "seek": 371200, "start": 3728.0, "end": 3737.0, "text": " So in in PyTorch to remind you all PyTorch layers and models are NN dot modules.", "tokens": [407, 294, 294, 9953, 51, 284, 339, 281, 4160, 291, 439, 9953, 51, 284, 339, 7914, 293, 5245, 366, 426, 45, 5893, 16679, 13], "temperature": 0.0, "avg_logprob": -0.22971698216029576, "compression_ratio": 1.3571428571428572, "no_speech_prob": 2.15670547731861e-06}, {"id": 609, "seek": 373700, "start": 3737.0, "end": 3742.0, "text": " They are things that once you create them look exactly like a function.", "tokens": [814, 366, 721, 300, 1564, 291, 1884, 552, 574, 2293, 411, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1070867516528601, "compression_ratio": 1.76036866359447, "no_speech_prob": 2.3182832592283376e-05}, {"id": 610, "seek": 373700, "start": 3742.0, "end": 3746.0, "text": " You call them with parentheses and you pass them arguments.", "tokens": [509, 818, 552, 365, 34153, 293, 291, 1320, 552, 12869, 13], "temperature": 0.0, "avg_logprob": -0.1070867516528601, "compression_ratio": 1.76036866359447, "no_speech_prob": 2.3182832592283376e-05}, {"id": 611, "seek": 373700, "start": 3746.0, "end": 3748.0, "text": " But they're not functions.", "tokens": [583, 436, 434, 406, 6828, 13], "temperature": 0.0, "avg_logprob": -0.1070867516528601, "compression_ratio": 1.76036866359447, "no_speech_prob": 2.3182832592283376e-05}, {"id": 612, "seek": 373700, "start": 3748.0, "end": 3753.0, "text": " They don't even have normally in Python to make something look like a function.", "tokens": [814, 500, 380, 754, 362, 5646, 294, 15329, 281, 652, 746, 574, 411, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1070867516528601, "compression_ratio": 1.76036866359447, "no_speech_prob": 2.3182832592283376e-05}, {"id": 613, "seek": 373700, "start": 3753.0, "end": 3756.0, "text": " You have to give it a method called Dunder call.", "tokens": [509, 362, 281, 976, 309, 257, 3170, 1219, 413, 6617, 818, 13], "temperature": 0.0, "avg_logprob": -0.1070867516528601, "compression_ratio": 1.76036866359447, "no_speech_prob": 2.3182832592283376e-05}, {"id": 614, "seek": 373700, "start": 3756.0, "end": 3760.0, "text": " Remember that means underscore underscore call underscore underscore which doesn't exist here.", "tokens": [5459, 300, 1355, 37556, 37556, 818, 37556, 37556, 597, 1177, 380, 2514, 510, 13], "temperature": 0.0, "avg_logprob": -0.1070867516528601, "compression_ratio": 1.76036866359447, "no_speech_prob": 2.3182832592283376e-05}, {"id": 615, "seek": 376000, "start": 3760.0, "end": 3767.0, "text": " And the reason is that PyTorch actually expects you to have something called forward.", "tokens": [400, 264, 1778, 307, 300, 9953, 51, 284, 339, 767, 33280, 291, 281, 362, 746, 1219, 2128, 13], "temperature": 0.0, "avg_logprob": -0.06337175718167933, "compression_ratio": 1.6335078534031413, "no_speech_prob": 4.3567551983869635e-06}, {"id": 616, "seek": 376000, "start": 3767.0, "end": 3771.0, "text": " And that's what PyTorch will call for you when you call it like a function.", "tokens": [400, 300, 311, 437, 9953, 51, 284, 339, 486, 818, 337, 291, 562, 291, 818, 309, 411, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.06337175718167933, "compression_ratio": 1.6335078534031413, "no_speech_prob": 4.3567551983869635e-06}, {"id": 617, "seek": 376000, "start": 3771.0, "end": 3778.0, "text": " So when this model is being trained to get the predictions it's actually going to call forward for us.", "tokens": [407, 562, 341, 2316, 307, 885, 8895, 281, 483, 264, 21264, 309, 311, 767, 516, 281, 818, 2128, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.06337175718167933, "compression_ratio": 1.6335078534031413, "no_speech_prob": 4.3567551983869635e-06}, {"id": 618, "seek": 376000, "start": 3778.0, "end": 3782.0, "text": " So this is where we.", "tokens": [407, 341, 307, 689, 321, 13], "temperature": 0.0, "avg_logprob": -0.06337175718167933, "compression_ratio": 1.6335078534031413, "no_speech_prob": 4.3567551983869635e-06}, {"id": 619, "seek": 376000, "start": 3782.0, "end": 3786.0, "text": " Do the calculations right.", "tokens": [1144, 264, 20448, 558, 13], "temperature": 0.0, "avg_logprob": -0.06337175718167933, "compression_ratio": 1.6335078534031413, "no_speech_prob": 4.3567551983869635e-06}, {"id": 620, "seek": 378600, "start": 3786.0, "end": 3793.0, "text": " To calculate our predictions. So this is where you can see we grab our.", "tokens": [1407, 8873, 527, 21264, 13, 407, 341, 307, 689, 291, 393, 536, 321, 4444, 527, 13], "temperature": 0.0, "avg_logprob": -0.11155833138359918, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.0129736438102555e-05}, {"id": 621, "seek": 378600, "start": 3793.0, "end": 3795.0, "text": " Why is this users rather than user.", "tokens": [1545, 307, 341, 5022, 2831, 813, 4195, 13], "temperature": 0.0, "avg_logprob": -0.11155833138359918, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.0129736438102555e-05}, {"id": 622, "seek": 378600, "start": 3795.0, "end": 3798.0, "text": " That's because everything's done a mini batch at a time.", "tokens": [663, 311, 570, 1203, 311, 1096, 257, 8382, 15245, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.11155833138359918, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.0129736438102555e-05}, {"id": 623, "seek": 378600, "start": 3798.0, "end": 3812.0, "text": " Right. So it is kind of when I read the forward in in a PyTorch module I tend to ignore in my head the fact that there's a mini batch and I pretend there's just one.", "tokens": [1779, 13, 407, 309, 307, 733, 295, 562, 286, 1401, 264, 2128, 294, 294, 257, 9953, 51, 284, 339, 10088, 286, 3928, 281, 11200, 294, 452, 1378, 264, 1186, 300, 456, 311, 257, 8382, 15245, 293, 286, 11865, 456, 311, 445, 472, 13], "temperature": 0.0, "avg_logprob": -0.11155833138359918, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.0129736438102555e-05}, {"id": 624, "seek": 381200, "start": 3812.0, "end": 3819.0, "text": " Because PyTorch automatically handles all of the stuff about doing it to everything in the mini batch for you.", "tokens": [1436, 9953, 51, 284, 339, 6772, 18722, 439, 295, 264, 1507, 466, 884, 309, 281, 1203, 294, 264, 8382, 15245, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.11075465286834331, "compression_ratio": 1.6702127659574468, "no_speech_prob": 1.98327124962816e-05}, {"id": 625, "seek": 381200, "start": 3819.0, "end": 3822.0, "text": " Right. So let's pretend there's just one user.", "tokens": [1779, 13, 407, 718, 311, 11865, 456, 311, 445, 472, 4195, 13], "temperature": 0.0, "avg_logprob": -0.11075465286834331, "compression_ratio": 1.6702127659574468, "no_speech_prob": 1.98327124962816e-05}, {"id": 626, "seek": 381200, "start": 3822.0, "end": 3830.0, "text": " Right. So grab that user and what is this self dot you underscore weight self dot you underscore weight is.", "tokens": [1779, 13, 407, 4444, 300, 4195, 293, 437, 307, 341, 2698, 5893, 291, 37556, 3364, 2698, 5893, 291, 37556, 3364, 307, 13], "temperature": 0.0, "avg_logprob": -0.11075465286834331, "compression_ratio": 1.6702127659574468, "no_speech_prob": 1.98327124962816e-05}, {"id": 627, "seek": 381200, "start": 3830.0, "end": 3835.0, "text": " An embedding we create an embedding for each of.", "tokens": [1107, 12240, 3584, 321, 1884, 364, 12240, 3584, 337, 1184, 295, 13], "temperature": 0.0, "avg_logprob": -0.11075465286834331, "compression_ratio": 1.6702127659574468, "no_speech_prob": 1.98327124962816e-05}, {"id": 628, "seek": 383500, "start": 3835.0, "end": 3842.0, "text": " Users by factors items by factors users by one items by one.", "tokens": [47092, 538, 6771, 4754, 538, 6771, 5022, 538, 472, 4754, 538, 472, 13], "temperature": 0.0, "avg_logprob": -0.2030179582793137, "compression_ratio": 1.575221238938053, "no_speech_prob": 5.475740181282163e-05}, {"id": 629, "seek": 383500, "start": 3842.0, "end": 3847.0, "text": " That makes sense right. So users by one.", "tokens": [663, 1669, 2020, 558, 13, 407, 5022, 538, 472, 13], "temperature": 0.0, "avg_logprob": -0.2030179582793137, "compression_ratio": 1.575221238938053, "no_speech_prob": 5.475740181282163e-05}, {"id": 630, "seek": 383500, "start": 3847.0, "end": 3849.0, "text": " Is.", "tokens": [1119, 13], "temperature": 0.0, "avg_logprob": -0.2030179582793137, "compression_ratio": 1.575221238938053, "no_speech_prob": 5.475740181282163e-05}, {"id": 631, "seek": 383500, "start": 3849.0, "end": 3852.0, "text": " Here that's the users bias.", "tokens": [1692, 300, 311, 264, 5022, 12577, 13], "temperature": 0.0, "avg_logprob": -0.2030179582793137, "compression_ratio": 1.575221238938053, "no_speech_prob": 5.475740181282163e-05}, {"id": 632, "seek": 383500, "start": 3852.0, "end": 3858.0, "text": " Right. And then users by factor is here.", "tokens": [1779, 13, 400, 550, 5022, 538, 5952, 307, 510, 13], "temperature": 0.0, "avg_logprob": -0.2030179582793137, "compression_ratio": 1.575221238938053, "no_speech_prob": 5.475740181282163e-05}, {"id": 633, "seek": 383500, "start": 3858.0, "end": 3860.0, "text": " So.", "tokens": [407, 13], "temperature": 0.0, "avg_logprob": -0.2030179582793137, "compression_ratio": 1.575221238938053, "no_speech_prob": 5.475740181282163e-05}, {"id": 634, "seek": 386000, "start": 3860.0, "end": 3865.0, "text": " Users by factors is the first couple so that's going to go in you underscore weight.", "tokens": [47092, 538, 6771, 307, 264, 700, 1916, 370, 300, 311, 516, 281, 352, 294, 291, 37556, 3364, 13], "temperature": 0.0, "avg_logprob": -0.19169609456122677, "compression_ratio": 1.6910112359550562, "no_speech_prob": 1.4738140635017771e-05}, {"id": 635, "seek": 386000, "start": 3865.0, "end": 3870.0, "text": " And users comma one is the third so that's going to go in you underscore bias.", "tokens": [400, 5022, 22117, 472, 307, 264, 2636, 370, 300, 311, 516, 281, 352, 294, 291, 37556, 12577, 13], "temperature": 0.0, "avg_logprob": -0.19169609456122677, "compression_ratio": 1.6910112359550562, "no_speech_prob": 1.4738140635017771e-05}, {"id": 636, "seek": 386000, "start": 3870.0, "end": 3878.0, "text": " So remember when PyTorch creates our and end up module it calls Dunder in it.", "tokens": [407, 1604, 562, 9953, 51, 284, 339, 7829, 527, 293, 917, 493, 10088, 309, 5498, 413, 6617, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.19169609456122677, "compression_ratio": 1.6910112359550562, "no_speech_prob": 1.4738140635017771e-05}, {"id": 637, "seek": 386000, "start": 3878.0, "end": 3882.0, "text": " And so this is where we have to create our weight matrices.", "tokens": [400, 370, 341, 307, 689, 321, 362, 281, 1884, 527, 3364, 32284, 13], "temperature": 0.0, "avg_logprob": -0.19169609456122677, "compression_ratio": 1.6910112359550562, "no_speech_prob": 1.4738140635017771e-05}, {"id": 638, "seek": 388200, "start": 3882.0, "end": 3890.0, "text": " And we don't normally create the actual weight matrix tensors we normally use PyTorch convenience functions to do that for us.", "tokens": [400, 321, 500, 380, 5646, 1884, 264, 3539, 3364, 8141, 10688, 830, 321, 5646, 764, 9953, 51, 284, 339, 19283, 6828, 281, 360, 300, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.09544186259424964, "compression_ratio": 1.6169154228855722, "no_speech_prob": 1.4284723874880001e-05}, {"id": 639, "seek": 388200, "start": 3890.0, "end": 3892.0, "text": " And we're going to see some of that after the break.", "tokens": [400, 321, 434, 516, 281, 536, 512, 295, 300, 934, 264, 1821, 13], "temperature": 0.0, "avg_logprob": -0.09544186259424964, "compression_ratio": 1.6169154228855722, "no_speech_prob": 1.4284723874880001e-05}, {"id": 640, "seek": 388200, "start": 3892.0, "end": 3900.0, "text": " So for now just recognize that this function is going to create an embedding matrix for us.", "tokens": [407, 337, 586, 445, 5521, 300, 341, 2445, 307, 516, 281, 1884, 364, 12240, 3584, 8141, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.09544186259424964, "compression_ratio": 1.6169154228855722, "no_speech_prob": 1.4284723874880001e-05}, {"id": 641, "seek": 388200, "start": 3900.0, "end": 3905.0, "text": " It's going to be a PyTorch and end up module as well.", "tokens": [467, 311, 516, 281, 312, 257, 9953, 51, 284, 339, 293, 917, 493, 10088, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.09544186259424964, "compression_ratio": 1.6169154228855722, "no_speech_prob": 1.4284723874880001e-05}, {"id": 642, "seek": 390500, "start": 3905.0, "end": 3916.0, "text": " So therefore to actually pass stuff into that embedding matrix and get activations out you treat it as if it was a function.", "tokens": [407, 4412, 281, 767, 1320, 1507, 666, 300, 12240, 3584, 8141, 293, 483, 2430, 763, 484, 291, 2387, 309, 382, 498, 309, 390, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.10253416697184245, "compression_ratio": 1.660633484162896, "no_speech_prob": 3.7851862089155475e-06}, {"id": 643, "seek": 390500, "start": 3916.0, "end": 3930.0, "text": " Stick it in parentheses. So if you want to look in the Python PyTorch source code and find an end on embedding you will find there's something called forward in there which will do this array look up for us.", "tokens": [22744, 309, 294, 34153, 13, 407, 498, 291, 528, 281, 574, 294, 264, 15329, 9953, 51, 284, 339, 4009, 3089, 293, 915, 364, 917, 322, 12240, 3584, 291, 486, 915, 456, 311, 746, 1219, 2128, 294, 456, 597, 486, 360, 341, 10225, 574, 493, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.10253416697184245, "compression_ratio": 1.660633484162896, "no_speech_prob": 3.7851862089155475e-06}, {"id": 644, "seek": 390500, "start": 3930.0, "end": 3934.0, "text": " So here's where we grab the users.", "tokens": [407, 510, 311, 689, 321, 4444, 264, 5022, 13], "temperature": 0.0, "avg_logprob": -0.10253416697184245, "compression_ratio": 1.660633484162896, "no_speech_prob": 3.7851862089155475e-06}, {"id": 645, "seek": 393400, "start": 3934.0, "end": 3937.0, "text": " Here's where we grab the items.", "tokens": [1692, 311, 689, 321, 4444, 264, 4754, 13], "temperature": 0.0, "avg_logprob": -0.08540767431259155, "compression_ratio": 1.5679012345679013, "no_speech_prob": 9.817855243454687e-06}, {"id": 646, "seek": 393400, "start": 3937.0, "end": 3940.0, "text": " And so we've now got the embeddings for each.", "tokens": [400, 370, 321, 600, 586, 658, 264, 12240, 29432, 337, 1184, 13], "temperature": 0.0, "avg_logprob": -0.08540767431259155, "compression_ratio": 1.5679012345679013, "no_speech_prob": 9.817855243454687e-06}, {"id": 647, "seek": 393400, "start": 3940.0, "end": 3950.0, "text": " Right. And so at this point we're kind of like here and we found that and that.", "tokens": [1779, 13, 400, 370, 412, 341, 935, 321, 434, 733, 295, 411, 510, 293, 321, 1352, 300, 293, 300, 13], "temperature": 0.0, "avg_logprob": -0.08540767431259155, "compression_ratio": 1.5679012345679013, "no_speech_prob": 9.817855243454687e-06}, {"id": 648, "seek": 393400, "start": 3950.0, "end": 3959.0, "text": " So we multiply them together and sum them up and then we add on the user bias and the item bias.", "tokens": [407, 321, 12972, 552, 1214, 293, 2408, 552, 493, 293, 550, 321, 909, 322, 264, 4195, 12577, 293, 264, 3174, 12577, 13], "temperature": 0.0, "avg_logprob": -0.08540767431259155, "compression_ratio": 1.5679012345679013, "no_speech_prob": 9.817855243454687e-06}, {"id": 649, "seek": 395900, "start": 3959.0, "end": 3964.0, "text": " And then if we've got a wide range then we do our sigmoid trick.", "tokens": [400, 550, 498, 321, 600, 658, 257, 4874, 3613, 550, 321, 360, 527, 4556, 3280, 327, 4282, 13], "temperature": 0.0, "avg_logprob": -0.07047336241778206, "compression_ratio": 1.6036036036036037, "no_speech_prob": 5.594149115495384e-06}, {"id": 650, "seek": 395900, "start": 3964.0, "end": 3972.0, "text": " And so the nice thing is you know you now understand the entirety of this model.", "tokens": [400, 370, 264, 1481, 551, 307, 291, 458, 291, 586, 1223, 264, 31557, 295, 341, 2316, 13], "temperature": 0.0, "avg_logprob": -0.07047336241778206, "compression_ratio": 1.6036036036036037, "no_speech_prob": 5.594149115495384e-06}, {"id": 651, "seek": 395900, "start": 3972.0, "end": 3974.0, "text": " And this is not just any model.", "tokens": [400, 341, 307, 406, 445, 604, 2316, 13], "temperature": 0.0, "avg_logprob": -0.07047336241778206, "compression_ratio": 1.6036036036036037, "no_speech_prob": 5.594149115495384e-06}, {"id": 652, "seek": 395900, "start": 3974.0, "end": 3988.0, "text": " This is a model that we just found is at the very least highly competitive with and perhaps slightly better than some published table of pretty good numbers from a software group", "tokens": [639, 307, 257, 2316, 300, 321, 445, 1352, 307, 412, 264, 588, 1935, 5405, 10043, 365, 293, 4317, 4748, 1101, 813, 512, 6572, 3199, 295, 1238, 665, 3547, 490, 257, 4722, 1594], "temperature": 0.0, "avg_logprob": -0.07047336241778206, "compression_ratio": 1.6036036036036037, "no_speech_prob": 5.594149115495384e-06}, {"id": 653, "seek": 398800, "start": 3988.0, "end": 3990.0, "text": " that does nothing but this.", "tokens": [300, 775, 1825, 457, 341, 13], "temperature": 0.0, "avg_logprob": -0.06876524900778747, "compression_ratio": 1.5674157303370786, "no_speech_prob": 2.977104850288015e-05}, {"id": 654, "seek": 398800, "start": 3990.0, "end": 3992.0, "text": " So you're doing well.", "tokens": [407, 291, 434, 884, 731, 13], "temperature": 0.0, "avg_logprob": -0.06876524900778747, "compression_ratio": 1.5674157303370786, "no_speech_prob": 2.977104850288015e-05}, {"id": 655, "seek": 398800, "start": 3992.0, "end": 3995.0, "text": " This is nice.", "tokens": [639, 307, 1481, 13], "temperature": 0.0, "avg_logprob": -0.06876524900778747, "compression_ratio": 1.5674157303370786, "no_speech_prob": 2.977104850288015e-05}, {"id": 656, "seek": 398800, "start": 3995.0, "end": 4001.0, "text": " So that's probably a good place to have a break.", "tokens": [407, 300, 311, 1391, 257, 665, 1081, 281, 362, 257, 1821, 13], "temperature": 0.0, "avg_logprob": -0.06876524900778747, "compression_ratio": 1.5674157303370786, "no_speech_prob": 2.977104850288015e-05}, {"id": 657, "seek": 398800, "start": 4001.0, "end": 4011.0, "text": " And so after the break we're going to come back and we're going to talk about the one piece of this puzzle we haven't learned yet which is what the hell does this do.", "tokens": [400, 370, 934, 264, 1821, 321, 434, 516, 281, 808, 646, 293, 321, 434, 516, 281, 751, 466, 264, 472, 2522, 295, 341, 12805, 321, 2378, 380, 3264, 1939, 597, 307, 437, 264, 4921, 775, 341, 360, 13], "temperature": 0.0, "avg_logprob": -0.06876524900778747, "compression_ratio": 1.5674157303370786, "no_speech_prob": 2.977104850288015e-05}, {"id": 658, "seek": 401100, "start": 4011.0, "end": 4021.0, "text": " OK. So let's come back at seven fifty.", "tokens": [2264, 13, 407, 718, 311, 808, 646, 412, 3407, 13442, 13], "temperature": 0.0, "avg_logprob": -0.0874992325192406, "compression_ratio": 1.5116279069767442, "no_speech_prob": 1.300541862292448e-05}, {"id": 659, "seek": 401100, "start": 4021.0, "end": 4028.0, "text": " OK. So this idea of interpreting embeddings is really interesting.", "tokens": [2264, 13, 407, 341, 1558, 295, 37395, 12240, 29432, 307, 534, 1880, 13], "temperature": 0.0, "avg_logprob": -0.0874992325192406, "compression_ratio": 1.5116279069767442, "no_speech_prob": 1.300541862292448e-05}, {"id": 660, "seek": 401100, "start": 4028.0, "end": 4039.0, "text": " And as we'll see later in this lesson the things that we create for categorical variables more generally in tabular data sets are also embedding matrices.", "tokens": [400, 382, 321, 603, 536, 1780, 294, 341, 6898, 264, 721, 300, 321, 1884, 337, 19250, 804, 9102, 544, 5101, 294, 4421, 1040, 1412, 6352, 366, 611, 12240, 3584, 32284, 13], "temperature": 0.0, "avg_logprob": -0.0874992325192406, "compression_ratio": 1.5116279069767442, "no_speech_prob": 1.300541862292448e-05}, {"id": 661, "seek": 403900, "start": 4039.0, "end": 4052.0, "text": " And again that's just a normal matrix multiply by a one hot encoded input where we skip the computational computational and memory burden of it by doing it in a more efficient way.", "tokens": [400, 797, 300, 311, 445, 257, 2710, 8141, 12972, 538, 257, 472, 2368, 2058, 12340, 4846, 689, 321, 10023, 264, 28270, 28270, 293, 4675, 12578, 295, 309, 538, 884, 309, 294, 257, 544, 7148, 636, 13], "temperature": 0.0, "avg_logprob": -0.06469480196634929, "compression_ratio": 1.5204678362573099, "no_speech_prob": 2.467757076374255e-05}, {"id": 662, "seek": 403900, "start": 4052.0, "end": 4057.0, "text": " And it happens to end up with these interesting semantics kind of accidentally.", "tokens": [400, 309, 2314, 281, 917, 493, 365, 613, 1880, 4361, 45298, 733, 295, 15715, 13], "temperature": 0.0, "avg_logprob": -0.06469480196634929, "compression_ratio": 1.5204678362573099, "no_speech_prob": 2.467757076374255e-05}, {"id": 663, "seek": 405700, "start": 4057.0, "end": 4069.0, "text": " And there was this really interesting paper by these folks who came second in a capital competition for something called a Rossman.", "tokens": [400, 456, 390, 341, 534, 1880, 3035, 538, 613, 4024, 567, 1361, 1150, 294, 257, 4238, 6211, 337, 746, 1219, 257, 16140, 1601, 13], "temperature": 0.0, "avg_logprob": -0.13426159205061666, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.5856224965536967e-05}, {"id": 664, "seek": 405700, "start": 4069.0, "end": 4073.0, "text": " We'll probably look in more detail at the Rossman competition in part two.", "tokens": [492, 603, 1391, 574, 294, 544, 2607, 412, 264, 16140, 1601, 6211, 294, 644, 732, 13], "temperature": 0.0, "avg_logprob": -0.13426159205061666, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.5856224965536967e-05}, {"id": 665, "seek": 405700, "start": 4073.0, "end": 4076.0, "text": " I think we're going to run out of time in part one.", "tokens": [286, 519, 321, 434, 516, 281, 1190, 484, 295, 565, 294, 644, 472, 13], "temperature": 0.0, "avg_logprob": -0.13426159205061666, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.5856224965536967e-05}, {"id": 666, "seek": 405700, "start": 4076.0, "end": 4080.0, "text": " But it's basically this pretty standard tabular stuff.", "tokens": [583, 309, 311, 1936, 341, 1238, 3832, 4421, 1040, 1507, 13], "temperature": 0.0, "avg_logprob": -0.13426159205061666, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.5856224965536967e-05}, {"id": 667, "seek": 405700, "start": 4080.0, "end": 4085.0, "text": " The main interesting stuff is in the pre-processing.", "tokens": [440, 2135, 1880, 1507, 307, 294, 264, 659, 12, 41075, 278, 13], "temperature": 0.0, "avg_logprob": -0.13426159205061666, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.5856224965536967e-05}, {"id": 668, "seek": 408500, "start": 4085.0, "end": 4096.0, "text": " And it was interesting because they came second despite the fact that the person who came first and pretty much everybody else was at the top of the leaderboard did a massive amount of highly specific feature engineering", "tokens": [400, 309, 390, 1880, 570, 436, 1361, 1150, 7228, 264, 1186, 300, 264, 954, 567, 1361, 700, 293, 1238, 709, 2201, 1646, 390, 412, 264, 1192, 295, 264, 5263, 3787, 630, 257, 5994, 2372, 295, 5405, 2685, 4111, 7043], "temperature": 0.0, "avg_logprob": -0.09105283837569388, "compression_ratio": 1.7019607843137254, "no_speech_prob": 1.750052069837693e-05}, {"id": 669, "seek": 408500, "start": 4096.0, "end": 4100.0, "text": " whereas these folks did way less feature engineering than anybody else.", "tokens": [9735, 613, 4024, 630, 636, 1570, 4111, 7043, 813, 4472, 1646, 13], "temperature": 0.0, "avg_logprob": -0.09105283837569388, "compression_ratio": 1.7019607843137254, "no_speech_prob": 1.750052069837693e-05}, {"id": 670, "seek": 408500, "start": 4100.0, "end": 4106.0, "text": " But instead they used a neural net. And this was at a time in 2016 when just no one did that.", "tokens": [583, 2602, 436, 1143, 257, 18161, 2533, 13, 400, 341, 390, 412, 257, 565, 294, 6549, 562, 445, 572, 472, 630, 300, 13], "temperature": 0.0, "avg_logprob": -0.09105283837569388, "compression_ratio": 1.7019607843137254, "no_speech_prob": 1.750052069837693e-05}, {"id": 671, "seek": 408500, "start": 4106.0, "end": 4109.0, "text": " No one was doing neural nets with tabular data.", "tokens": [883, 472, 390, 884, 18161, 36170, 365, 4421, 1040, 1412, 13], "temperature": 0.0, "avg_logprob": -0.09105283837569388, "compression_ratio": 1.7019607843137254, "no_speech_prob": 1.750052069837693e-05}, {"id": 672, "seek": 410900, "start": 4109.0, "end": 4118.0, "text": " So they have the kind of stuff that we've been talking about kind of arose there or at least was kind of popularized there.", "tokens": [407, 436, 362, 264, 733, 295, 1507, 300, 321, 600, 668, 1417, 466, 733, 295, 37192, 456, 420, 412, 1935, 390, 733, 295, 3743, 1602, 456, 13], "temperature": 0.0, "avg_logprob": -0.10497946380287089, "compression_ratio": 1.642570281124498, "no_speech_prob": 1.544400947750546e-05}, {"id": 673, "seek": 410900, "start": 4118.0, "end": 4121.0, "text": " And when I say popularized I mean only popularized a tiny bit.", "tokens": [400, 562, 286, 584, 3743, 1602, 286, 914, 787, 3743, 1602, 257, 5870, 857, 13], "temperature": 0.0, "avg_logprob": -0.10497946380287089, "compression_ratio": 1.642570281124498, "no_speech_prob": 1.544400947750546e-05}, {"id": 674, "seek": 410900, "start": 4121.0, "end": 4124.0, "text": " Still most people aren't aware of this idea.", "tokens": [8291, 881, 561, 3212, 380, 3650, 295, 341, 1558, 13], "temperature": 0.0, "avg_logprob": -0.10497946380287089, "compression_ratio": 1.642570281124498, "no_speech_prob": 1.544400947750546e-05}, {"id": 675, "seek": 410900, "start": 4124.0, "end": 4135.0, "text": " But it's pretty cool because in their paper they show that the mean average percentage error for various techniques K nearest neighbors random forest and gradient boosted trees.", "tokens": [583, 309, 311, 1238, 1627, 570, 294, 641, 3035, 436, 855, 300, 264, 914, 4274, 9668, 6713, 337, 3683, 7512, 591, 23831, 12512, 4974, 6719, 293, 16235, 9194, 292, 5852, 13], "temperature": 0.0, "avg_logprob": -0.10497946380287089, "compression_ratio": 1.642570281124498, "no_speech_prob": 1.544400947750546e-05}, {"id": 676, "seek": 413500, "start": 4135.0, "end": 4146.0, "text": " Well first you know neural nets just worked worked a lot better. But then with entity embeddings which is what they call this using entity matrices in tabular data.", "tokens": [1042, 700, 291, 458, 18161, 36170, 445, 2732, 2732, 257, 688, 1101, 13, 583, 550, 365, 13977, 12240, 29432, 597, 307, 437, 436, 818, 341, 1228, 13977, 32284, 294, 4421, 1040, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1215044421914183, "compression_ratio": 1.7740384615384615, "no_speech_prob": 1.6963187590590678e-05}, {"id": 677, "seek": 413500, "start": 4146.0, "end": 4154.0, "text": " You can actually they actually added the entity embeddings to all of these different tasks after training them and they all got way better.", "tokens": [509, 393, 767, 436, 767, 3869, 264, 13977, 12240, 29432, 281, 439, 295, 613, 819, 9608, 934, 3097, 552, 293, 436, 439, 658, 636, 1101, 13], "temperature": 0.0, "avg_logprob": -0.1215044421914183, "compression_ratio": 1.7740384615384615, "no_speech_prob": 1.6963187590590678e-05}, {"id": 678, "seek": 413500, "start": 4154.0, "end": 4157.0, "text": " Right. So neural nets with entity embeddings are still the best.", "tokens": [1779, 13, 407, 18161, 36170, 365, 13977, 12240, 29432, 366, 920, 264, 1151, 13], "temperature": 0.0, "avg_logprob": -0.1215044421914183, "compression_ratio": 1.7740384615384615, "no_speech_prob": 1.6963187590590678e-05}, {"id": 679, "seek": 415700, "start": 4157.0, "end": 4173.0, "text": " But a random forest with empty embeddings was not at all far behind. And you know that's often kind of that's kind of nice track because you could train these entity matrices for products or stores or genome motifs or whatever.", "tokens": [583, 257, 4974, 6719, 365, 6707, 12240, 29432, 390, 406, 412, 439, 1400, 2261, 13, 400, 291, 458, 300, 311, 2049, 733, 295, 300, 311, 733, 295, 1481, 2837, 570, 291, 727, 3847, 613, 13977, 32284, 337, 3383, 420, 9512, 420, 21953, 2184, 18290, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.0929692503693816, "compression_ratio": 1.640552995391705, "no_speech_prob": 1.0450396985106636e-05}, {"id": 680, "seek": 415700, "start": 4173.0, "end": 4183.0, "text": " And then use them in lots of different models possibly using faster things like random forests by getting a lot of the benefits.", "tokens": [400, 550, 764, 552, 294, 3195, 295, 819, 5245, 6264, 1228, 4663, 721, 411, 4974, 21700, 538, 1242, 257, 688, 295, 264, 5311, 13], "temperature": 0.0, "avg_logprob": -0.0929692503693816, "compression_ratio": 1.640552995391705, "no_speech_prob": 1.0450396985106636e-05}, {"id": 681, "seek": 418300, "start": 4183.0, "end": 4197.0, "text": " But here was something interesting. They took a two dimensional projection of their of their embedding matrix for state for example German state because this was a German supermarket chain.", "tokens": [583, 510, 390, 746, 1880, 13, 814, 1890, 257, 732, 18795, 22743, 295, 641, 295, 641, 12240, 3584, 8141, 337, 1785, 337, 1365, 6521, 1785, 570, 341, 390, 257, 6521, 25180, 5021, 13], "temperature": 0.0, "avg_logprob": -0.14575797319412231, "compression_ratio": 1.5303030303030303, "no_speech_prob": 2.3920891180750914e-05}, {"id": 682, "seek": 418300, "start": 4197.0, "end": 4204.0, "text": " I think using the same kind of approach we did I don't remember if they use PCA or something else like different.", "tokens": [286, 519, 1228, 264, 912, 733, 295, 3109, 321, 630, 286, 500, 380, 1604, 498, 436, 764, 6465, 32, 420, 746, 1646, 411, 819, 13], "temperature": 0.0, "avg_logprob": -0.14575797319412231, "compression_ratio": 1.5303030303030303, "no_speech_prob": 2.3920891180750914e-05}, {"id": 683, "seek": 420400, "start": 4204.0, "end": 4216.0, "text": " And then here's the interesting thing. I've circled here you know a few things in this embedding space and I've circled it with the same color over here.", "tokens": [400, 550, 510, 311, 264, 1880, 551, 13, 286, 600, 3510, 1493, 510, 291, 458, 257, 1326, 721, 294, 341, 12240, 3584, 1901, 293, 286, 600, 3510, 1493, 309, 365, 264, 912, 2017, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.11065369436185654, "compression_ratio": 1.718562874251497, "no_speech_prob": 1.3845021385350265e-05}, {"id": 684, "seek": 420400, "start": 4216.0, "end": 4225.0, "text": " And here I've circled some same color over here. And it's like oh my god the embedding projection.", "tokens": [400, 510, 286, 600, 3510, 1493, 512, 912, 2017, 670, 510, 13, 400, 309, 311, 411, 1954, 452, 3044, 264, 12240, 3584, 22743, 13], "temperature": 0.0, "avg_logprob": -0.11065369436185654, "compression_ratio": 1.718562874251497, "no_speech_prob": 1.3845021385350265e-05}, {"id": 685, "seek": 420400, "start": 4225.0, "end": 4228.0, "text": " Has actually discovered geography.", "tokens": [8646, 767, 6941, 26695, 13], "temperature": 0.0, "avg_logprob": -0.11065369436185654, "compression_ratio": 1.718562874251497, "no_speech_prob": 1.3845021385350265e-05}, {"id": 686, "seek": 422800, "start": 4228.0, "end": 4241.0, "text": " They didn't do that. But it's it's it's found things that are nearby each other in grocery purchasing patterns because this was about predicting how many sales there will be.", "tokens": [814, 994, 380, 360, 300, 13, 583, 309, 311, 309, 311, 309, 311, 1352, 721, 300, 366, 11184, 1184, 661, 294, 14410, 20906, 8294, 570, 341, 390, 466, 32884, 577, 867, 5763, 456, 486, 312, 13], "temperature": 0.0, "avg_logprob": -0.13879575552763762, "compression_ratio": 1.4375, "no_speech_prob": 1.4969923540775198e-05}, {"id": 687, "seek": 422800, "start": 4241.0, "end": 4246.0, "text": " You know it's there is some geographic element of that.", "tokens": [509, 458, 309, 311, 456, 307, 512, 32318, 4478, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.13879575552763762, "compression_ratio": 1.4375, "no_speech_prob": 1.4969923540775198e-05}, {"id": 688, "seek": 424600, "start": 4246.0, "end": 4259.0, "text": " In fact here is a graph of the distance between two embedding vectors so you can just take an embedding vector and say what's the sum of squared you know compared to some other embedding vector.", "tokens": [682, 1186, 510, 307, 257, 4295, 295, 264, 4560, 1296, 732, 12240, 3584, 18875, 370, 291, 393, 445, 747, 364, 12240, 3584, 8062, 293, 584, 437, 311, 264, 2408, 295, 8889, 291, 458, 5347, 281, 512, 661, 12240, 3584, 8062, 13], "temperature": 0.0, "avg_logprob": -0.09040283073078502, "compression_ratio": 1.8056872037914693, "no_speech_prob": 2.6681056624511257e-05}, {"id": 689, "seek": 424600, "start": 4259.0, "end": 4266.0, "text": " That's the Euclidean distance. What's the distance in embedding space and then plot it against the distance in real life between shops.", "tokens": [663, 311, 264, 462, 1311, 31264, 282, 4560, 13, 708, 311, 264, 4560, 294, 12240, 3584, 1901, 293, 550, 7542, 309, 1970, 264, 4560, 294, 957, 993, 1296, 14457, 13], "temperature": 0.0, "avg_logprob": -0.09040283073078502, "compression_ratio": 1.8056872037914693, "no_speech_prob": 2.6681056624511257e-05}, {"id": 690, "seek": 424600, "start": 4266.0, "end": 4270.0, "text": " And you get this very strong positive correlation.", "tokens": [400, 291, 483, 341, 588, 2068, 3353, 20009, 13], "temperature": 0.0, "avg_logprob": -0.09040283073078502, "compression_ratio": 1.8056872037914693, "no_speech_prob": 2.6681056624511257e-05}, {"id": 691, "seek": 427000, "start": 4270.0, "end": 4280.0, "text": " Here is an embedding space for the days of the week. And as you can see there's a very clear path through them. Here's the embedding space for the month of the year.", "tokens": [1692, 307, 364, 12240, 3584, 1901, 337, 264, 1708, 295, 264, 1243, 13, 400, 382, 291, 393, 536, 456, 311, 257, 588, 1850, 3100, 807, 552, 13, 1692, 311, 264, 12240, 3584, 1901, 337, 264, 1618, 295, 264, 1064, 13], "temperature": 0.0, "avg_logprob": -0.08011828193181678, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.863044781202916e-05}, {"id": 692, "seek": 427000, "start": 4280.0, "end": 4285.0, "text": " And again there's a very clear path through them. So like.", "tokens": [400, 797, 456, 311, 257, 588, 1850, 3100, 807, 552, 13, 407, 411, 13], "temperature": 0.0, "avg_logprob": -0.08011828193181678, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.863044781202916e-05}, {"id": 693, "seek": 427000, "start": 4285.0, "end": 4292.0, "text": " Embeddings are amazing and I don't feel like anybody's even close to.", "tokens": [24234, 292, 29432, 366, 2243, 293, 286, 500, 380, 841, 411, 4472, 311, 754, 1998, 281, 13], "temperature": 0.0, "avg_logprob": -0.08011828193181678, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.863044781202916e-05}, {"id": 694, "seek": 429200, "start": 4292.0, "end": 4300.0, "text": " Exploring. The kind of interpretation that you could get right. So if you've got.", "tokens": [12514, 3662, 13, 440, 733, 295, 14174, 300, 291, 727, 483, 558, 13, 407, 498, 291, 600, 658, 13], "temperature": 0.0, "avg_logprob": -0.11219056914834415, "compression_ratio": 1.5054347826086956, "no_speech_prob": 3.5353066778043285e-05}, {"id": 695, "seek": 429200, "start": 4300.0, "end": 4305.0, "text": " Genome motifs or plant species or.", "tokens": [3632, 423, 2184, 18290, 420, 3709, 6172, 420, 13], "temperature": 0.0, "avg_logprob": -0.11219056914834415, "compression_ratio": 1.5054347826086956, "no_speech_prob": 3.5353066778043285e-05}, {"id": 696, "seek": 429200, "start": 4305.0, "end": 4316.0, "text": " Products that your shop sells or whatever like it would be really interesting to train a few models and try and kind of fine tune some embeddings and then like.", "tokens": [47699, 300, 428, 3945, 20897, 420, 2035, 411, 309, 576, 312, 534, 1880, 281, 3847, 257, 1326, 5245, 293, 853, 293, 733, 295, 2489, 10864, 512, 12240, 29432, 293, 550, 411, 13], "temperature": 0.0, "avg_logprob": -0.11219056914834415, "compression_ratio": 1.5054347826086956, "no_speech_prob": 3.5353066778043285e-05}, {"id": 697, "seek": 431600, "start": 4316.0, "end": 4329.0, "text": " Start looking at them in these ways in terms of similarity to other ones and clustering them and projecting them into these spaces and whatever I think is really interesting.", "tokens": [6481, 1237, 412, 552, 294, 613, 2098, 294, 2115, 295, 32194, 281, 661, 2306, 293, 596, 48673, 552, 293, 43001, 552, 666, 613, 7673, 293, 2035, 286, 519, 307, 534, 1880, 13], "temperature": 0.0, "avg_logprob": -0.19261002814632722, "compression_ratio": 1.668103448275862, "no_speech_prob": 2.0143375877523795e-05}, {"id": 698, "seek": 431600, "start": 4329.0, "end": 4335.0, "text": " So we were trying to make sure we understood what every line of code did in this some pretty good.", "tokens": [407, 321, 645, 1382, 281, 652, 988, 321, 7320, 437, 633, 1622, 295, 3089, 630, 294, 341, 512, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.19261002814632722, "compression_ratio": 1.668103448275862, "no_speech_prob": 2.0143375877523795e-05}, {"id": 699, "seek": 431600, "start": 4335.0, "end": 4343.0, "text": " Colab learner model we built and so the one piece missing is this WD piece and WD starts stands for weight decay.", "tokens": [4004, 455, 33347, 2316, 321, 3094, 293, 370, 264, 472, 2522, 5361, 307, 341, 343, 35, 2522, 293, 343, 35, 3719, 7382, 337, 3364, 21039, 13], "temperature": 0.0, "avg_logprob": -0.19261002814632722, "compression_ratio": 1.668103448275862, "no_speech_prob": 2.0143375877523795e-05}, {"id": 700, "seek": 434300, "start": 4343.0, "end": 4351.0, "text": " So what is weight decay weight decay is a type of regularization. What is regularization.", "tokens": [407, 437, 307, 3364, 21039, 3364, 21039, 307, 257, 2010, 295, 3890, 2144, 13, 708, 307, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.12183936180606965, "compression_ratio": 1.5604395604395604, "no_speech_prob": 2.2471655029221438e-05}, {"id": 701, "seek": 434300, "start": 4351.0, "end": 4363.0, "text": " Well let's start by going back to this nice little chart that Andrew did in his terrific machine learning course where he plot plotted some data and then showed a few different lines through it.", "tokens": [1042, 718, 311, 722, 538, 516, 646, 281, 341, 1481, 707, 6927, 300, 10110, 630, 294, 702, 20899, 3479, 2539, 1164, 689, 415, 7542, 43288, 512, 1412, 293, 550, 4712, 257, 1326, 819, 3876, 807, 309, 13], "temperature": 0.0, "avg_logprob": -0.12183936180606965, "compression_ratio": 1.5604395604395604, "no_speech_prob": 2.2471655029221438e-05}, {"id": 702, "seek": 436300, "start": 4363.0, "end": 4377.0, "text": " This one here because Andrew's at Stanford he has to use Greek letters. OK so we can say this is a plus BX but you know if you want to go there theta naught plus theta 1 X.", "tokens": [639, 472, 510, 570, 10110, 311, 412, 20374, 415, 575, 281, 764, 10281, 7825, 13, 2264, 370, 321, 393, 584, 341, 307, 257, 1804, 363, 55, 457, 291, 458, 498, 291, 528, 281, 352, 456, 9725, 13138, 1804, 9725, 502, 1783, 13], "temperature": 0.0, "avg_logprob": -0.18899645124162948, "compression_ratio": 1.5, "no_speech_prob": 2.4680917704245076e-05}, {"id": 703, "seek": 436300, "start": 4377.0, "end": 4382.0, "text": " Is a line right. It's a line even if it's a Greek letters it's still a line.", "tokens": [1119, 257, 1622, 558, 13, 467, 311, 257, 1622, 754, 498, 309, 311, 257, 10281, 7825, 309, 311, 920, 257, 1622, 13], "temperature": 0.0, "avg_logprob": -0.18899645124162948, "compression_ratio": 1.5, "no_speech_prob": 2.4680917704245076e-05}, {"id": 704, "seek": 438200, "start": 4382.0, "end": 4397.0, "text": " So here's a second degree polynomial A plus BX plus CX squared bit of curve. And here's a high degree polynomial which is curvy as anything.", "tokens": [407, 510, 311, 257, 1150, 4314, 26110, 316, 1804, 363, 55, 1804, 383, 55, 8889, 857, 295, 7605, 13, 400, 510, 311, 257, 1090, 4314, 26110, 597, 307, 1262, 11869, 382, 1340, 13], "temperature": 0.0, "avg_logprob": -0.15187112340387307, "compression_ratio": 1.425531914893617, "no_speech_prob": 8.013232218218036e-06}, {"id": 705, "seek": 438200, "start": 4397.0, "end": 4404.0, "text": " So. Models with more parameters tend to look more like this.", "tokens": [407, 13, 6583, 1625, 365, 544, 9834, 3928, 281, 574, 544, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.15187112340387307, "compression_ratio": 1.425531914893617, "no_speech_prob": 8.013232218218036e-06}, {"id": 706, "seek": 440400, "start": 4404.0, "end": 4420.0, "text": " And so in traditional statistics we say hey let's use less parameters because we don't want it to look like this because if it looks like this then the predictions over here and over here they're going to be all wrong.", "tokens": [400, 370, 294, 5164, 12523, 321, 584, 4177, 718, 311, 764, 1570, 9834, 570, 321, 500, 380, 528, 309, 281, 574, 411, 341, 570, 498, 309, 1542, 411, 341, 550, 264, 21264, 670, 510, 293, 670, 510, 436, 434, 516, 281, 312, 439, 2085, 13], "temperature": 0.0, "avg_logprob": -0.0846930489395604, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.54457738972269e-05}, {"id": 707, "seek": 440400, "start": 4420.0, "end": 4424.0, "text": " It's not going to generalize well. We're overfitting.", "tokens": [467, 311, 406, 516, 281, 2674, 1125, 731, 13, 492, 434, 670, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.0846930489395604, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.54457738972269e-05}, {"id": 708, "seek": 442400, "start": 4424.0, "end": 4445.0, "text": " So we avoid overfitting by using less parameters. And so if any of you are unlucky enough to have been brainwashed by a background in statistics or psychology or econometrics or any of these kinds of courses you'll have you know you're going to have to unlearn the idea that you need less parameters because what you instead need to realize this is.", "tokens": [407, 321, 5042, 670, 69, 2414, 538, 1228, 1570, 9834, 13, 400, 370, 498, 604, 295, 291, 366, 38838, 1547, 281, 362, 668, 3567, 86, 12219, 538, 257, 3678, 294, 12523, 420, 15105, 420, 23692, 649, 10716, 420, 604, 295, 613, 3685, 295, 7712, 291, 603, 362, 291, 458, 291, 434, 516, 281, 362, 281, 25272, 1083, 264, 1558, 300, 291, 643, 1570, 9834, 570, 437, 291, 2602, 643, 281, 4325, 341, 307, 13], "temperature": 0.0, "avg_logprob": -0.07891227648808406, "compression_ratio": 1.6384976525821595, "no_speech_prob": 4.936828190693632e-06}, {"id": 709, "seek": 444500, "start": 4445.0, "end": 4462.0, "text": " You will fit this lie that you need less parameters because it's a convenient fiction for the real truth which is you don't want your function to be too complex and having less parameters is one way of making it less complex.", "tokens": [509, 486, 3318, 341, 4544, 300, 291, 643, 1570, 9834, 570, 309, 311, 257, 10851, 13266, 337, 264, 957, 3494, 597, 307, 291, 500, 380, 528, 428, 2445, 281, 312, 886, 3997, 293, 1419, 1570, 9834, 307, 472, 636, 295, 1455, 309, 1570, 3997, 13], "temperature": 0.0, "avg_logprob": -0.09517730121881189, "compression_ratio": 1.6122448979591837, "no_speech_prob": 4.469222403713502e-05}, {"id": 710, "seek": 444500, "start": 4462.0, "end": 4471.0, "text": " But what if you had a thousand parameters and 999 of those parameters were one E neg nine.", "tokens": [583, 437, 498, 291, 632, 257, 4714, 9834, 293, 1722, 8494, 295, 729, 9834, 645, 472, 462, 2485, 4949, 13], "temperature": 0.0, "avg_logprob": -0.09517730121881189, "compression_ratio": 1.6122448979591837, "no_speech_prob": 4.469222403713502e-05}, {"id": 711, "seek": 447100, "start": 4471.0, "end": 4482.0, "text": " Well what if there was zero if there's zero then they're not really there or if they want to make nine they're hardly there. Right. So like why can't I have lots of parameters if like lots of them are really small.", "tokens": [1042, 437, 498, 456, 390, 4018, 498, 456, 311, 4018, 550, 436, 434, 406, 534, 456, 420, 498, 436, 528, 281, 652, 4949, 436, 434, 13572, 456, 13, 1779, 13, 407, 411, 983, 393, 380, 286, 362, 3195, 295, 9834, 498, 411, 3195, 295, 552, 366, 534, 1359, 13], "temperature": 0.0, "avg_logprob": -0.18423295295101472, "compression_ratio": 1.7, "no_speech_prob": 2.8856009521405213e-05}, {"id": 712, "seek": 447100, "start": 4482.0, "end": 4496.0, "text": " And the answer is you can. OK. You know so this this thing of like counting the number of parameters is how we limit complexity is actually extremely limiting.", "tokens": [400, 264, 1867, 307, 291, 393, 13, 2264, 13, 509, 458, 370, 341, 341, 551, 295, 411, 13251, 264, 1230, 295, 9834, 307, 577, 321, 4948, 14024, 307, 767, 4664, 22083, 13], "temperature": 0.0, "avg_logprob": -0.18423295295101472, "compression_ratio": 1.7, "no_speech_prob": 2.8856009521405213e-05}, {"id": 713, "seek": 449600, "start": 4496.0, "end": 4506.0, "text": " It's a fiction that really has a lot of problems right. And so if in your head complexity is scored by how many parameters you have you're doing it or wrong.", "tokens": [467, 311, 257, 13266, 300, 534, 575, 257, 688, 295, 2740, 558, 13, 400, 370, 498, 294, 428, 1378, 14024, 307, 18139, 538, 577, 867, 9834, 291, 362, 291, 434, 884, 309, 420, 2085, 13], "temperature": 0.0, "avg_logprob": -0.09916112973139836, "compression_ratio": 1.6292682926829267, "no_speech_prob": 1.3006857443542685e-05}, {"id": 714, "seek": 449600, "start": 4506.0, "end": 4522.0, "text": " Right. Score it properly. Right. So why do we care. Why would I want to use more parameters because more parameters means more nonlinearities more interactions more curvy bits.", "tokens": [1779, 13, 47901, 309, 6108, 13, 1779, 13, 407, 983, 360, 321, 1127, 13, 1545, 576, 286, 528, 281, 764, 544, 9834, 570, 544, 9834, 1355, 544, 2107, 28263, 1088, 544, 13280, 544, 1262, 11869, 9239, 13], "temperature": 0.0, "avg_logprob": -0.09916112973139836, "compression_ratio": 1.6292682926829267, "no_speech_prob": 1.3006857443542685e-05}, {"id": 715, "seek": 452200, "start": 4522.0, "end": 4532.0, "text": " Right. And real life is full of curvy bits. Right. Real life does not look like this.", "tokens": [1779, 13, 400, 957, 993, 307, 1577, 295, 1262, 11869, 9239, 13, 1779, 13, 8467, 993, 775, 406, 574, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.0874556582024757, "compression_ratio": 1.4308943089430894, "no_speech_prob": 8.01341411715839e-06}, {"id": 716, "seek": 452200, "start": 4532.0, "end": 4538.0, "text": " But we don't want them to be more curvy than necessary or more interacting than necessary.", "tokens": [583, 321, 500, 380, 528, 552, 281, 312, 544, 1262, 11869, 813, 4818, 420, 544, 18017, 813, 4818, 13], "temperature": 0.0, "avg_logprob": -0.0874556582024757, "compression_ratio": 1.4308943089430894, "no_speech_prob": 8.01341411715839e-06}, {"id": 717, "seek": 453800, "start": 4538.0, "end": 4557.0, "text": " So therefore let's use lots of parameters and then penalize complexity. OK. So one way to penalize complexity is as I kind of suggested before is let's sum up the value of your parameters.", "tokens": [407, 4412, 718, 311, 764, 3195, 295, 9834, 293, 550, 13661, 1125, 14024, 13, 2264, 13, 407, 472, 636, 281, 13661, 1125, 14024, 307, 382, 286, 733, 295, 10945, 949, 307, 718, 311, 2408, 493, 264, 2158, 295, 428, 9834, 13], "temperature": 0.0, "avg_logprob": -0.06256880230373807, "compression_ratio": 1.4461538461538461, "no_speech_prob": 9.972809493774548e-06}, {"id": 718, "seek": 455700, "start": 4557.0, "end": 4568.0, "text": " Now that doesn't quite work because some parameters are positive and some are negative. Right. So what if we sum up the square of the parameters. Right.", "tokens": [823, 300, 1177, 380, 1596, 589, 570, 512, 9834, 366, 3353, 293, 512, 366, 3671, 13, 1779, 13, 407, 437, 498, 321, 2408, 493, 264, 3732, 295, 264, 9834, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.054257611433664955, "compression_ratio": 1.7055555555555555, "no_speech_prob": 4.092885319550987e-06}, {"id": 719, "seek": 455700, "start": 4568.0, "end": 4580.0, "text": " And that's actually a really good idea. Let's actually create a model and in the loss function we're going to add the sum of the square of the parameters.", "tokens": [400, 300, 311, 767, 257, 534, 665, 1558, 13, 961, 311, 767, 1884, 257, 2316, 293, 294, 264, 4470, 2445, 321, 434, 516, 281, 909, 264, 2408, 295, 264, 3732, 295, 264, 9834, 13], "temperature": 0.0, "avg_logprob": -0.054257611433664955, "compression_ratio": 1.7055555555555555, "no_speech_prob": 4.092885319550987e-06}, {"id": 720, "seek": 458000, "start": 4580.0, "end": 4593.0, "text": " Now here's the problem with that though. Maybe that number is way too big and it's so big that the best loss is to set all of the parameters to zero.", "tokens": [823, 510, 311, 264, 1154, 365, 300, 1673, 13, 2704, 300, 1230, 307, 636, 886, 955, 293, 309, 311, 370, 955, 300, 264, 1151, 4470, 307, 281, 992, 439, 295, 264, 9834, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.05138976086852371, "compression_ratio": 1.6801801801801801, "no_speech_prob": 9.81814901024336e-06}, {"id": 721, "seek": 458000, "start": 4593.0, "end": 4599.0, "text": " That would be no good. Right. So actually we want to make sure that doesn't happen.", "tokens": [663, 576, 312, 572, 665, 13, 1779, 13, 407, 767, 321, 528, 281, 652, 988, 300, 1177, 380, 1051, 13], "temperature": 0.0, "avg_logprob": -0.05138976086852371, "compression_ratio": 1.6801801801801801, "no_speech_prob": 9.81814901024336e-06}, {"id": 722, "seek": 458000, "start": 4599.0, "end": 4608.0, "text": " So therefore let's not just add the sum of the square to the parameters to the model but let's multiply that by some number that we choose.", "tokens": [407, 4412, 718, 311, 406, 445, 909, 264, 2408, 295, 264, 3732, 281, 264, 9834, 281, 264, 2316, 457, 718, 311, 12972, 300, 538, 512, 1230, 300, 321, 2826, 13], "temperature": 0.0, "avg_logprob": -0.05138976086852371, "compression_ratio": 1.6801801801801801, "no_speech_prob": 9.81814901024336e-06}, {"id": 723, "seek": 460800, "start": 4608.0, "end": 4619.0, "text": " And that number that we choose in fast AI is called WD. OK. So that's what we're going to do.", "tokens": [400, 300, 1230, 300, 321, 2826, 294, 2370, 7318, 307, 1219, 343, 35, 13, 2264, 13, 407, 300, 311, 437, 321, 434, 516, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.10211100653996544, "compression_ratio": 1.4935064935064934, "no_speech_prob": 2.212513209087774e-05}, {"id": 724, "seek": 460800, "start": 4619.0, "end": 4631.0, "text": " We're going to take our loss function and we're going to add to it the sum of the square of the parameters multiplied by some number WD.", "tokens": [492, 434, 516, 281, 747, 527, 4470, 2445, 293, 321, 434, 516, 281, 909, 281, 309, 264, 2408, 295, 264, 3732, 295, 264, 9834, 17207, 538, 512, 1230, 343, 35, 13], "temperature": 0.0, "avg_logprob": -0.10211100653996544, "compression_ratio": 1.4935064935064934, "no_speech_prob": 2.212513209087774e-05}, {"id": 725, "seek": 463100, "start": 4631.0, "end": 4640.0, "text": " What should that number be. Well generally it should be zero point one.", "tokens": [708, 820, 300, 1230, 312, 13, 1042, 5101, 309, 820, 312, 4018, 935, 472, 13], "temperature": 0.0, "avg_logprob": -0.1942838728427887, "compression_ratio": 1.5965909090909092, "no_speech_prob": 1.428431824024301e-05}, {"id": 726, "seek": 463100, "start": 4640.0, "end": 4656.0, "text": " People with fancy machine learning PhDs are extremely skeptical and dismissive of any claims that a learning rate can be three e next three most of the time or a weight decay can be point one most of the time.", "tokens": [3432, 365, 10247, 3479, 2539, 14476, 82, 366, 4664, 28601, 293, 16974, 488, 295, 604, 9441, 300, 257, 2539, 3314, 393, 312, 1045, 308, 958, 1045, 881, 295, 264, 565, 420, 257, 3364, 21039, 393, 312, 935, 472, 881, 295, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.1942838728427887, "compression_ratio": 1.5965909090909092, "no_speech_prob": 1.428431824024301e-05}, {"id": 727, "seek": 465600, "start": 4656.0, "end": 4666.0, "text": " But here's the thing. We've done a lot of experiments on a lot of data sets and we've had a lot of trouble finding anywhere a weight decay of point one isn't great.", "tokens": [583, 510, 311, 264, 551, 13, 492, 600, 1096, 257, 688, 295, 12050, 322, 257, 688, 295, 1412, 6352, 293, 321, 600, 632, 257, 688, 295, 5253, 5006, 4992, 257, 3364, 21039, 295, 935, 472, 1943, 380, 869, 13], "temperature": 0.0, "avg_logprob": -0.09601673439367493, "compression_ratio": 1.565217391304348, "no_speech_prob": 9.818019861995708e-06}, {"id": 728, "seek": 465600, "start": 4666.0, "end": 4674.0, "text": " However we don't make that the default. We actually make the default point oh one.", "tokens": [2908, 321, 500, 380, 652, 300, 264, 7576, 13, 492, 767, 652, 264, 7576, 935, 1954, 472, 13], "temperature": 0.0, "avg_logprob": -0.09601673439367493, "compression_ratio": 1.565217391304348, "no_speech_prob": 9.818019861995708e-06}, {"id": 729, "seek": 465600, "start": 4674.0, "end": 4676.0, "text": " Why.", "tokens": [1545, 13], "temperature": 0.0, "avg_logprob": -0.09601673439367493, "compression_ratio": 1.565217391304348, "no_speech_prob": 9.818019861995708e-06}, {"id": 730, "seek": 467600, "start": 4676.0, "end": 4686.0, "text": " Because in those rare occasions where you have too much weight decay no matter how much you train it just never quite fits well enough.", "tokens": [1436, 294, 729, 5892, 20641, 689, 291, 362, 886, 709, 3364, 21039, 572, 1871, 577, 709, 291, 3847, 309, 445, 1128, 1596, 9001, 731, 1547, 13], "temperature": 0.0, "avg_logprob": -0.09165811538696289, "compression_ratio": 1.6588235294117648, "no_speech_prob": 6.540079084516037e-06}, {"id": 731, "seek": 467600, "start": 4686.0, "end": 4695.0, "text": " Where else if you have too little weight decay you can still train well you'll just start to overfit. So you just have to stop a little bit early.", "tokens": [2305, 1646, 498, 291, 362, 886, 707, 3364, 21039, 291, 393, 920, 3847, 731, 291, 603, 445, 722, 281, 670, 6845, 13, 407, 291, 445, 362, 281, 1590, 257, 707, 857, 2440, 13], "temperature": 0.0, "avg_logprob": -0.09165811538696289, "compression_ratio": 1.6588235294117648, "no_speech_prob": 6.540079084516037e-06}, {"id": 732, "seek": 469500, "start": 4695.0, "end": 4709.0, "text": " So we've been a little bit conservative with our defaults but my suggestion to you is this now that you know that every learner has a WD argument and I should mention you won't always see it in this list.", "tokens": [407, 321, 600, 668, 257, 707, 857, 13780, 365, 527, 7576, 82, 457, 452, 16541, 281, 291, 307, 341, 586, 300, 291, 458, 300, 633, 33347, 575, 257, 343, 35, 6770, 293, 286, 820, 2152, 291, 1582, 380, 1009, 536, 309, 294, 341, 1329, 13], "temperature": 0.0, "avg_logprob": -0.1230382246129653, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.710722350864671e-06}, {"id": 733, "seek": 469500, "start": 4709.0, "end": 4719.0, "text": " Right. Because this is concept of KW args in Python which is basically parameters that are going to get passed up the chain to the next thing that we call.", "tokens": [1779, 13, 1436, 341, 307, 3410, 295, 591, 54, 3882, 82, 294, 15329, 597, 307, 1936, 9834, 300, 366, 516, 281, 483, 4678, 493, 264, 5021, 281, 264, 958, 551, 300, 321, 818, 13], "temperature": 0.0, "avg_logprob": -0.1230382246129653, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.710722350864671e-06}, {"id": 734, "seek": 471900, "start": 4719.0, "end": 4731.0, "text": " And so basically all of the learners will call eventually this constructor and this constructor has a WD.", "tokens": [400, 370, 1936, 439, 295, 264, 23655, 486, 818, 4728, 341, 47479, 293, 341, 47479, 575, 257, 343, 35, 13], "temperature": 0.0, "avg_logprob": -0.14549314159236543, "compression_ratio": 1.5958549222797926, "no_speech_prob": 7.410952548525529e-06}, {"id": 735, "seek": 471900, "start": 4731.0, "end": 4737.0, "text": " Right. So this is just one of those things that you can either look in the docs or you now know it.", "tokens": [1779, 13, 407, 341, 307, 445, 472, 295, 729, 721, 300, 291, 393, 2139, 574, 294, 264, 45623, 420, 291, 586, 458, 309, 13], "temperature": 0.0, "avg_logprob": -0.14549314159236543, "compression_ratio": 1.5958549222797926, "no_speech_prob": 7.410952548525529e-06}, {"id": 736, "seek": 471900, "start": 4737.0, "end": 4744.0, "text": " Anytime you're constructing a learner from pretty much any kind of function in faster you can pass WD.", "tokens": [39401, 291, 434, 39969, 257, 33347, 490, 1238, 709, 604, 733, 295, 2445, 294, 4663, 291, 393, 1320, 343, 35, 13], "temperature": 0.0, "avg_logprob": -0.14549314159236543, "compression_ratio": 1.5958549222797926, "no_speech_prob": 7.410952548525529e-06}, {"id": 737, "seek": 474400, "start": 4744.0, "end": 4756.0, "text": " And so passing point one instead of the default point 0 1 will often help. So give it a go.", "tokens": [400, 370, 8437, 935, 472, 2602, 295, 264, 7576, 935, 1958, 502, 486, 2049, 854, 13, 407, 976, 309, 257, 352, 13], "temperature": 0.0, "avg_logprob": -0.17158361275990805, "compression_ratio": 1.2058823529411764, "no_speech_prob": 9.665676770964637e-06}, {"id": 738, "seek": 474400, "start": 4756.0, "end": 4761.0, "text": " So what's really going on here.", "tokens": [407, 437, 311, 534, 516, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.17158361275990805, "compression_ratio": 1.2058823529411764, "no_speech_prob": 9.665676770964637e-06}, {"id": 739, "seek": 476100, "start": 4761.0, "end": 4774.0, "text": " It would be helpful I think to go back to lesson 2 SGD because everything we're doing for the rest of today really is based on this.", "tokens": [467, 576, 312, 4961, 286, 519, 281, 352, 646, 281, 6898, 568, 34520, 35, 570, 1203, 321, 434, 884, 337, 264, 1472, 295, 965, 534, 307, 2361, 322, 341, 13], "temperature": 0.0, "avg_logprob": -0.12624423163277762, "compression_ratio": 1.6084656084656084, "no_speech_prob": 2.7531679734238423e-05}, {"id": 740, "seek": 476100, "start": 4774.0, "end": 4788.0, "text": " And this is where we created some data and then we try and then we added a loss function MSE and then we created a function called update which calculated our predictions.", "tokens": [400, 341, 307, 689, 321, 2942, 512, 1412, 293, 550, 321, 853, 293, 550, 321, 3869, 257, 4470, 2445, 376, 5879, 293, 550, 321, 2942, 257, 2445, 1219, 5623, 597, 15598, 527, 21264, 13], "temperature": 0.0, "avg_logprob": -0.12624423163277762, "compression_ratio": 1.6084656084656084, "no_speech_prob": 2.7531679734238423e-05}, {"id": 741, "seek": 478800, "start": 4788.0, "end": 4794.0, "text": " That's our weight matrix multiply. This is just a one layer. So there's no value.", "tokens": [663, 311, 527, 3364, 8141, 12972, 13, 639, 307, 445, 257, 472, 4583, 13, 407, 456, 311, 572, 2158, 13], "temperature": 0.0, "avg_logprob": -0.16703044727284422, "compression_ratio": 1.646808510638298, "no_speech_prob": 2.977032090711873e-05}, {"id": 742, "seek": 478800, "start": 4794.0, "end": 4801.0, "text": " We calculated our loss using that means grid error. We calculated the gradients using lost up backward.", "tokens": [492, 15598, 527, 4470, 1228, 300, 1355, 10748, 6713, 13, 492, 15598, 264, 2771, 2448, 1228, 2731, 493, 23897, 13], "temperature": 0.0, "avg_logprob": -0.16703044727284422, "compression_ratio": 1.646808510638298, "no_speech_prob": 2.977032090711873e-05}, {"id": 743, "seek": 478800, "start": 4801.0, "end": 4807.0, "text": " We then subtracted in place the learning rate times the gradients and that is gradient descent.", "tokens": [492, 550, 16390, 292, 294, 1081, 264, 2539, 3314, 1413, 264, 2771, 2448, 293, 300, 307, 16235, 23475, 13], "temperature": 0.0, "avg_logprob": -0.16703044727284422, "compression_ratio": 1.646808510638298, "no_speech_prob": 2.977032090711873e-05}, {"id": 744, "seek": 478800, "start": 4807.0, "end": 4816.0, "text": " So if you haven't reviewed lesson 2 SGD please do because this is where we're this is our starting point.", "tokens": [407, 498, 291, 2378, 380, 18429, 6898, 568, 34520, 35, 1767, 360, 570, 341, 307, 689, 321, 434, 341, 307, 527, 2891, 935, 13], "temperature": 0.0, "avg_logprob": -0.16703044727284422, "compression_ratio": 1.646808510638298, "no_speech_prob": 2.977032090711873e-05}, {"id": 745, "seek": 481600, "start": 4816.0, "end": 4819.0, "text": " So if you don't get this then none of this is going to make sense.", "tokens": [407, 498, 291, 500, 380, 483, 341, 550, 6022, 295, 341, 307, 516, 281, 652, 2020, 13], "temperature": 0.0, "avg_logprob": -0.12503975790900154, "compression_ratio": 1.5595854922279793, "no_speech_prob": 7.601912511745468e-05}, {"id": 746, "seek": 481600, "start": 4819.0, "end": 4826.0, "text": " If you're watching the video maybe pause now go back rewatch this part of listen to make sure you get it.", "tokens": [759, 291, 434, 1976, 264, 960, 1310, 10465, 586, 352, 646, 319, 15219, 341, 644, 295, 2140, 281, 652, 988, 291, 483, 309, 13], "temperature": 0.0, "avg_logprob": -0.12503975790900154, "compression_ratio": 1.5595854922279793, "no_speech_prob": 7.601912511745468e-05}, {"id": 747, "seek": 481600, "start": 4826.0, "end": 4839.0, "text": " Remember a dot sub underscore is basically the same as a minus equals because a dot sub is subtract and everything in pie torch.", "tokens": [5459, 257, 5893, 1422, 37556, 307, 1936, 264, 912, 382, 257, 3175, 6915, 570, 257, 5893, 1422, 307, 16390, 293, 1203, 294, 1730, 27822, 13], "temperature": 0.0, "avg_logprob": -0.12503975790900154, "compression_ratio": 1.5595854922279793, "no_speech_prob": 7.601912511745468e-05}, {"id": 748, "seek": 483900, "start": 4839.0, "end": 4849.0, "text": " If you add an underscore to it means do it in place. So this is updating our parameters which started out as minus point one one.", "tokens": [759, 291, 909, 364, 37556, 281, 309, 1355, 360, 309, 294, 1081, 13, 407, 341, 307, 25113, 527, 9834, 597, 1409, 484, 382, 3175, 935, 472, 472, 13], "temperature": 0.0, "avg_logprob": -0.0940782084609523, "compression_ratio": 1.4946236559139785, "no_speech_prob": 5.4221054597292095e-06}, {"id": 749, "seek": 483900, "start": 4849.0, "end": 4853.0, "text": " We just arbitrarily picked those numbers and it gradually makes them better.", "tokens": [492, 445, 19071, 3289, 6183, 729, 3547, 293, 309, 13145, 1669, 552, 1101, 13], "temperature": 0.0, "avg_logprob": -0.0940782084609523, "compression_ratio": 1.4946236559139785, "no_speech_prob": 5.4221054597292095e-06}, {"id": 750, "seek": 483900, "start": 4853.0, "end": 4866.0, "text": " So let's write that down. So we are trying to calculate the parameters.", "tokens": [407, 718, 311, 2464, 300, 760, 13, 407, 321, 366, 1382, 281, 8873, 264, 9834, 13], "temperature": 0.0, "avg_logprob": -0.0940782084609523, "compression_ratio": 1.4946236559139785, "no_speech_prob": 5.4221054597292095e-06}, {"id": 751, "seek": 486600, "start": 4866.0, "end": 4875.0, "text": " I'm going to call them weights because this is just more common.", "tokens": [286, 478, 516, 281, 818, 552, 17443, 570, 341, 307, 445, 544, 2689, 13], "temperature": 0.0, "avg_logprob": -0.14025823125299416, "compression_ratio": 1.4391891891891893, "no_speech_prob": 1.520632031315472e-05}, {"id": 752, "seek": 486600, "start": 4875.0, "end": 4891.0, "text": " In kind of epoch T or time T and they're going to be equal to whatever the weights were in the previous epoch minus our learning rate multiplied by.", "tokens": [682, 733, 295, 30992, 339, 314, 420, 565, 314, 293, 436, 434, 516, 281, 312, 2681, 281, 2035, 264, 17443, 645, 294, 264, 3894, 30992, 339, 3175, 527, 2539, 3314, 17207, 538, 13], "temperature": 0.0, "avg_logprob": -0.14025823125299416, "compression_ratio": 1.4391891891891893, "no_speech_prob": 1.520632031315472e-05}, {"id": 753, "seek": 489100, "start": 4891.0, "end": 4903.0, "text": " It's the derivative of our loss function with respect to our weights at time T minus one.", "tokens": [467, 311, 264, 13760, 295, 527, 4470, 2445, 365, 3104, 281, 527, 17443, 412, 565, 314, 3175, 472, 13], "temperature": 0.0, "avg_logprob": -0.124160521371024, "compression_ratio": 1.2233009708737863, "no_speech_prob": 2.0461977328523062e-05}, {"id": 754, "seek": 489100, "start": 4903.0, "end": 4910.0, "text": " So that's that's what this is doing.", "tokens": [407, 300, 311, 300, 311, 437, 341, 307, 884, 13], "temperature": 0.0, "avg_logprob": -0.124160521371024, "compression_ratio": 1.2233009708737863, "no_speech_prob": 2.0461977328523062e-05}, {"id": 755, "seek": 491000, "start": 4910.0, "end": 4921.0, "text": " OK. And we don't have to calculate the derivative because it's boring and because it computers do it for us fast and then they store it here for us.", "tokens": [2264, 13, 400, 321, 500, 380, 362, 281, 8873, 264, 13760, 570, 309, 311, 9989, 293, 570, 309, 10807, 360, 309, 337, 505, 2370, 293, 550, 436, 3531, 309, 510, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.11466730886430883, "compression_ratio": 1.576086956521739, "no_speech_prob": 4.22276025346946e-06}, {"id": 756, "seek": 491000, "start": 4921.0, "end": 4939.0, "text": " So we're good. So make sure you're exceptionally comfortable with either that equation or that line of code because they have the same thing.", "tokens": [407, 321, 434, 665, 13, 407, 652, 988, 291, 434, 37807, 4619, 365, 2139, 300, 5367, 420, 300, 1622, 295, 3089, 570, 436, 362, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.11466730886430883, "compression_ratio": 1.576086956521739, "no_speech_prob": 4.22276025346946e-06}, {"id": 757, "seek": 493900, "start": 4939.0, "end": 4946.0, "text": " Where do we go from here. All right. So.", "tokens": [2305, 360, 321, 352, 490, 510, 13, 1057, 558, 13, 407, 13], "temperature": 0.0, "avg_logprob": -0.2368239383308255, "compression_ratio": 1.3333333333333333, "no_speech_prob": 2.3551072445116006e-05}, {"id": 758, "seek": 493900, "start": 4946.0, "end": 4949.0, "text": " What's that. What's our loss.", "tokens": [708, 311, 300, 13, 708, 311, 527, 4470, 13], "temperature": 0.0, "avg_logprob": -0.2368239383308255, "compression_ratio": 1.3333333333333333, "no_speech_prob": 2.3551072445116006e-05}, {"id": 759, "seek": 493900, "start": 4949.0, "end": 4954.0, "text": " Our loss. Is some function.", "tokens": [2621, 4470, 13, 1119, 512, 2445, 13], "temperature": 0.0, "avg_logprob": -0.2368239383308255, "compression_ratio": 1.3333333333333333, "no_speech_prob": 2.3551072445116006e-05}, {"id": 760, "seek": 493900, "start": 4954.0, "end": 4956.0, "text": " Of.", "tokens": [2720, 13], "temperature": 0.0, "avg_logprob": -0.2368239383308255, "compression_ratio": 1.3333333333333333, "no_speech_prob": 2.3551072445116006e-05}, {"id": 761, "seek": 493900, "start": 4956.0, "end": 4963.0, "text": " Our independent variable variables X and.", "tokens": [2621, 6695, 7006, 9102, 1783, 293, 13], "temperature": 0.0, "avg_logprob": -0.2368239383308255, "compression_ratio": 1.3333333333333333, "no_speech_prob": 2.3551072445116006e-05}, {"id": 762, "seek": 496300, "start": 4963.0, "end": 4969.0, "text": " Our weights. Right. And in our case we're using.", "tokens": [2621, 17443, 13, 1779, 13, 400, 294, 527, 1389, 321, 434, 1228, 13], "temperature": 0.0, "avg_logprob": -0.1730049869470429, "compression_ratio": 1.4630872483221478, "no_speech_prob": 2.014435813180171e-05}, {"id": 763, "seek": 496300, "start": 4969.0, "end": 4978.0, "text": " Mean squared error for example and it's between our predictions and our actuals.", "tokens": [12302, 8889, 6713, 337, 1365, 293, 309, 311, 1296, 527, 21264, 293, 527, 3539, 82, 13], "temperature": 0.0, "avg_logprob": -0.1730049869470429, "compression_ratio": 1.4630872483221478, "no_speech_prob": 2.014435813180171e-05}, {"id": 764, "seek": 496300, "start": 4978.0, "end": 4986.0, "text": " Right. So where does X and W come in. Well our predictions come from running some model.", "tokens": [1779, 13, 407, 689, 775, 1783, 293, 343, 808, 294, 13, 1042, 527, 21264, 808, 490, 2614, 512, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1730049869470429, "compression_ratio": 1.4630872483221478, "no_speech_prob": 2.014435813180171e-05}, {"id": 765, "seek": 498600, "start": 4986.0, "end": 4993.0, "text": " Call it M. On those predictions and that model contains some weights.", "tokens": [7807, 309, 376, 13, 1282, 729, 21264, 293, 300, 2316, 8306, 512, 17443, 13], "temperature": 0.0, "avg_logprob": -0.12424817350175646, "compression_ratio": 1.5889570552147239, "no_speech_prob": 8.267460543720517e-06}, {"id": 766, "seek": 498600, "start": 4993.0, "end": 4998.0, "text": " So that's that's what our loss function might be. And this might be all kinds of other loss functions.", "tokens": [407, 300, 311, 300, 311, 437, 527, 4470, 2445, 1062, 312, 13, 400, 341, 1062, 312, 439, 3685, 295, 661, 4470, 6828, 13], "temperature": 0.0, "avg_logprob": -0.12424817350175646, "compression_ratio": 1.5889570552147239, "no_speech_prob": 8.267460543720517e-06}, {"id": 767, "seek": 498600, "start": 4998.0, "end": 5004.0, "text": " We'll see some more today. And so that's what ends up.", "tokens": [492, 603, 536, 512, 544, 965, 13, 400, 370, 300, 311, 437, 5314, 493, 13], "temperature": 0.0, "avg_logprob": -0.12424817350175646, "compression_ratio": 1.5889570552147239, "no_speech_prob": 8.267460543720517e-06}, {"id": 768, "seek": 498600, "start": 5004.0, "end": 5007.0, "text": " Creating.", "tokens": [40002, 13], "temperature": 0.0, "avg_logprob": -0.12424817350175646, "compression_ratio": 1.5889570552147239, "no_speech_prob": 8.267460543720517e-06}, {"id": 769, "seek": 498600, "start": 5007.0, "end": 5013.0, "text": " A dot grad over here.", "tokens": [316, 5893, 2771, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.12424817350175646, "compression_ratio": 1.5889570552147239, "no_speech_prob": 8.267460543720517e-06}, {"id": 770, "seek": 501300, "start": 5013.0, "end": 5017.0, "text": " So we're going to do something else. We're going to add.", "tokens": [407, 321, 434, 516, 281, 360, 746, 1646, 13, 492, 434, 516, 281, 909, 13], "temperature": 0.0, "avg_logprob": -0.14499886512756346, "compression_ratio": 1.2935779816513762, "no_speech_prob": 1.321117906627478e-05}, {"id": 771, "seek": 501300, "start": 5017.0, "end": 5028.0, "text": " Weight decay some number which in our case is 0.1 times.", "tokens": [44464, 21039, 512, 1230, 597, 294, 527, 1389, 307, 1958, 13, 16, 1413, 13], "temperature": 0.0, "avg_logprob": -0.14499886512756346, "compression_ratio": 1.2935779816513762, "no_speech_prob": 1.321117906627478e-05}, {"id": 772, "seek": 501300, "start": 5028.0, "end": 5032.0, "text": " Times the sum.", "tokens": [11366, 264, 2408, 13], "temperature": 0.0, "avg_logprob": -0.14499886512756346, "compression_ratio": 1.2935779816513762, "no_speech_prob": 1.321117906627478e-05}, {"id": 773, "seek": 501300, "start": 5032.0, "end": 5034.0, "text": " Of.", "tokens": [2720, 13], "temperature": 0.0, "avg_logprob": -0.14499886512756346, "compression_ratio": 1.2935779816513762, "no_speech_prob": 1.321117906627478e-05}, {"id": 774, "seek": 501300, "start": 5034.0, "end": 5036.0, "text": " Weights.", "tokens": [492, 5761, 13], "temperature": 0.0, "avg_logprob": -0.14499886512756346, "compression_ratio": 1.2935779816513762, "no_speech_prob": 1.321117906627478e-05}, {"id": 775, "seek": 503600, "start": 5036.0, "end": 5044.0, "text": " Squared. OK. So let's do that.", "tokens": [8683, 1642, 13, 2264, 13, 407, 718, 311, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.12848401651149843, "compression_ratio": 1.5, "no_speech_prob": 6.962099632801255e-06}, {"id": 776, "seek": 503600, "start": 5044.0, "end": 5047.0, "text": " And let's make it interesting.", "tokens": [400, 718, 311, 652, 309, 1880, 13], "temperature": 0.0, "avg_logprob": -0.12848401651149843, "compression_ratio": 1.5, "no_speech_prob": 6.962099632801255e-06}, {"id": 777, "seek": 503600, "start": 5047.0, "end": 5051.0, "text": " By not using synthetic data but let's say some real data.", "tokens": [3146, 406, 1228, 23420, 1412, 457, 718, 311, 584, 512, 957, 1412, 13], "temperature": 0.0, "avg_logprob": -0.12848401651149843, "compression_ratio": 1.5, "no_speech_prob": 6.962099632801255e-06}, {"id": 778, "seek": 503600, "start": 5051.0, "end": 5063.0, "text": " And we're going to use MNIST the hand drawn digits. Right. But we're going to do this as a standard fully connected net not as a convolutional net because we haven't learned.", "tokens": [400, 321, 434, 516, 281, 764, 376, 45, 19756, 264, 1011, 10117, 27011, 13, 1779, 13, 583, 321, 434, 516, 281, 360, 341, 382, 257, 3832, 4498, 4582, 2533, 406, 382, 257, 45216, 304, 2533, 570, 321, 2378, 380, 3264, 13], "temperature": 0.0, "avg_logprob": -0.12848401651149843, "compression_ratio": 1.5, "no_speech_prob": 6.962099632801255e-06}, {"id": 779, "seek": 506300, "start": 5063.0, "end": 5067.0, "text": " The details of how to really create one of those from scratch.", "tokens": [440, 4365, 295, 577, 281, 534, 1884, 472, 295, 729, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.11218850691239912, "compression_ratio": 1.5644444444444445, "no_speech_prob": 1.3630175999423955e-05}, {"id": 780, "seek": 506300, "start": 5067.0, "end": 5074.0, "text": " So in this case is actually deep learning dot net provides MNIST as a Python pickle file.", "tokens": [407, 294, 341, 1389, 307, 767, 2452, 2539, 5893, 2533, 6417, 376, 45, 19756, 382, 257, 15329, 31433, 3991, 13], "temperature": 0.0, "avg_logprob": -0.11218850691239912, "compression_ratio": 1.5644444444444445, "no_speech_prob": 1.3630175999423955e-05}, {"id": 781, "seek": 506300, "start": 5074.0, "end": 5082.0, "text": " In other words it's a file that pickle that Python can just open up and it'll give you numpy arrays straight away and they're flat numpy arrays.", "tokens": [682, 661, 2283, 309, 311, 257, 3991, 300, 31433, 300, 15329, 393, 445, 1269, 493, 293, 309, 603, 976, 291, 1031, 8200, 41011, 2997, 1314, 293, 436, 434, 4962, 1031, 8200, 41011, 13], "temperature": 0.0, "avg_logprob": -0.11218850691239912, "compression_ratio": 1.5644444444444445, "no_speech_prob": 1.3630175999423955e-05}, {"id": 782, "seek": 506300, "start": 5082.0, "end": 5087.0, "text": " We don't have to do anything to them. So go grab that.", "tokens": [492, 500, 380, 362, 281, 360, 1340, 281, 552, 13, 407, 352, 4444, 300, 13], "temperature": 0.0, "avg_logprob": -0.11218850691239912, "compression_ratio": 1.5644444444444445, "no_speech_prob": 1.3630175999423955e-05}, {"id": 783, "seek": 508700, "start": 5087.0, "end": 5093.0, "text": " And it's a GZIP file so you can actually just GZIP dot open it directly.", "tokens": [400, 309, 311, 257, 460, 57, 9139, 3991, 370, 291, 393, 767, 445, 460, 57, 9139, 5893, 1269, 309, 3838, 13], "temperature": 0.0, "avg_logprob": -0.17897642226446242, "compression_ratio": 1.732, "no_speech_prob": 1.544547194498591e-05}, {"id": 784, "seek": 508700, "start": 5093.0, "end": 5101.0, "text": " And then you can pick all dot load it directly. And again encoding equals Latin one because yeah you know.", "tokens": [400, 550, 291, 393, 1888, 439, 5893, 3677, 309, 3838, 13, 400, 797, 43430, 6915, 10803, 472, 570, 1338, 291, 458, 13], "temperature": 0.0, "avg_logprob": -0.17897642226446242, "compression_ratio": 1.732, "no_speech_prob": 1.544547194498591e-05}, {"id": 785, "seek": 508700, "start": 5101.0, "end": 5114.0, "text": " And then we can just put that's that'll give us the training the validation and the test set. I don't care about the test set. So generally in Python if there's like something you don't care about you tend to use this special variable called underscore.", "tokens": [400, 550, 321, 393, 445, 829, 300, 311, 300, 603, 976, 505, 264, 3097, 264, 24071, 293, 264, 1500, 992, 13, 286, 500, 380, 1127, 466, 264, 1500, 992, 13, 407, 5101, 294, 15329, 498, 456, 311, 411, 746, 291, 500, 380, 1127, 466, 291, 3928, 281, 764, 341, 2121, 7006, 1219, 37556, 13], "temperature": 0.0, "avg_logprob": -0.17897642226446242, "compression_ratio": 1.732, "no_speech_prob": 1.544547194498591e-05}, {"id": 786, "seek": 511400, "start": 5114.0, "end": 5119.0, "text": " There's no reason you have to. It's just kind of people know you mean I don't care about this.", "tokens": [821, 311, 572, 1778, 291, 362, 281, 13, 467, 311, 445, 733, 295, 561, 458, 291, 914, 286, 500, 380, 1127, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.13095627953024472, "compression_ratio": 1.5172413793103448, "no_speech_prob": 8.800928299024235e-06}, {"id": 787, "seek": 511400, "start": 5119.0, "end": 5124.0, "text": " So there's a training training X and Y and a valid X and Y.", "tokens": [407, 456, 311, 257, 3097, 3097, 1783, 293, 398, 293, 257, 7363, 1783, 293, 398, 13], "temperature": 0.0, "avg_logprob": -0.13095627953024472, "compression_ratio": 1.5172413793103448, "no_speech_prob": 8.800928299024235e-06}, {"id": 788, "seek": 511400, "start": 5124.0, "end": 5135.0, "text": " Now this actually comes in as a as you can see if I sprint the shape 50,000 rows by 784 columns but the 784 columns are actually 28 by 28 pixel pictures.", "tokens": [823, 341, 767, 1487, 294, 382, 257, 382, 291, 393, 536, 498, 286, 25075, 264, 3909, 2625, 11, 1360, 13241, 538, 1614, 25494, 13766, 457, 264, 1614, 25494, 13766, 366, 767, 7562, 538, 7562, 19261, 5242, 13], "temperature": 0.0, "avg_logprob": -0.13095627953024472, "compression_ratio": 1.5172413793103448, "no_speech_prob": 8.800928299024235e-06}, {"id": 789, "seek": 513500, "start": 5135.0, "end": 5145.0, "text": " So if I reshape one of them into a 28 by 28 pixel picture and plot it right then you can see it's the number five. OK. So that's our data.", "tokens": [407, 498, 286, 725, 42406, 472, 295, 552, 666, 257, 7562, 538, 7562, 19261, 3036, 293, 7542, 309, 558, 550, 291, 393, 536, 309, 311, 264, 1230, 1732, 13, 2264, 13, 407, 300, 311, 527, 1412, 13], "temperature": 0.0, "avg_logprob": -0.11876713661920457, "compression_ratio": 1.553191489361702, "no_speech_prob": 9.665546713222284e-06}, {"id": 790, "seek": 513500, "start": 5145.0, "end": 5155.0, "text": " We've seen MNIST before and it's kind of pre reshaped version. Here it is in its flattened version. So I'm going to be using it in its flattened version.", "tokens": [492, 600, 1612, 376, 45, 19756, 949, 293, 309, 311, 733, 295, 659, 725, 71, 18653, 3037, 13, 1692, 309, 307, 294, 1080, 24183, 292, 3037, 13, 407, 286, 478, 516, 281, 312, 1228, 309, 294, 1080, 24183, 292, 3037, 13], "temperature": 0.0, "avg_logprob": -0.11876713661920457, "compression_ratio": 1.553191489361702, "no_speech_prob": 9.665546713222284e-06}, {"id": 791, "seek": 515500, "start": 5155.0, "end": 5165.0, "text": " And currently they are NumPy arrays. I need them to be tensors so I can just map torch dot tensor across all of them.", "tokens": [400, 4362, 436, 366, 22592, 47, 88, 41011, 13, 286, 643, 552, 281, 312, 10688, 830, 370, 286, 393, 445, 4471, 27822, 5893, 40863, 2108, 439, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.10833659538855919, "compression_ratio": 1.674641148325359, "no_speech_prob": 2.3549660909338854e-05}, {"id": 792, "seek": 515500, "start": 5165.0, "end": 5174.0, "text": " And so now the tensors. OK. I may as well create a variable with the number of things I have which we normally call N.", "tokens": [400, 370, 586, 264, 10688, 830, 13, 2264, 13, 286, 815, 382, 731, 1884, 257, 7006, 365, 264, 1230, 295, 721, 286, 362, 597, 321, 5646, 818, 426, 13], "temperature": 0.0, "avg_logprob": -0.10833659538855919, "compression_ratio": 1.674641148325359, "no_speech_prob": 2.3549660909338854e-05}, {"id": 793, "seek": 515500, "start": 5174.0, "end": 5180.0, "text": " And remember we normally have a thing called you know we tend to use C to mean the number of activations we need.", "tokens": [400, 1604, 321, 5646, 362, 257, 551, 1219, 291, 458, 321, 3928, 281, 764, 383, 281, 914, 264, 1230, 295, 2430, 763, 321, 643, 13], "temperature": 0.0, "avg_logprob": -0.10833659538855919, "compression_ratio": 1.674641148325359, "no_speech_prob": 2.3549660909338854e-05}, {"id": 794, "seek": 518000, "start": 5180.0, "end": 5188.0, "text": " We're actually so this is not going to be activation. Sorry this is going to be a number of columns. That's not a great name for it. Sorry.", "tokens": [492, 434, 767, 370, 341, 307, 406, 516, 281, 312, 24433, 13, 4919, 341, 307, 516, 281, 312, 257, 1230, 295, 13766, 13, 663, 311, 406, 257, 869, 1315, 337, 309, 13, 4919, 13], "temperature": 0.0, "avg_logprob": -0.16271775728696353, "compression_ratio": 1.6296296296296295, "no_speech_prob": 1.2804623111151159e-05}, {"id": 795, "seek": 518000, "start": 5188.0, "end": 5198.0, "text": " OK. So there we are. And then the the Y not surprisingly the minimum value is zero and the maximum value is nine because that's the extra number we're going to predict.", "tokens": [2264, 13, 407, 456, 321, 366, 13, 400, 550, 264, 264, 398, 406, 17600, 264, 7285, 2158, 307, 4018, 293, 264, 6674, 2158, 307, 4949, 570, 300, 311, 264, 2857, 1230, 321, 434, 516, 281, 6069, 13], "temperature": 0.0, "avg_logprob": -0.16271775728696353, "compression_ratio": 1.6296296296296295, "no_speech_prob": 1.2804623111151159e-05}, {"id": 796, "seek": 519800, "start": 5198.0, "end": 5210.0, "text": " So in lesson two SGD we like we created data where we actually added a column of ones on so that we didn't have to worry about bias. We're not going to do that.", "tokens": [407, 294, 6898, 732, 34520, 35, 321, 411, 321, 2942, 1412, 689, 321, 767, 3869, 257, 7738, 295, 2306, 322, 370, 300, 321, 994, 380, 362, 281, 3292, 466, 12577, 13, 492, 434, 406, 516, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.1063353136966103, "compression_ratio": 2.032710280373832, "no_speech_prob": 9.368351129523944e-06}, {"id": 797, "seek": 519800, "start": 5210.0, "end": 5217.0, "text": " We're going to have pie torch do that kind of implicitly for us. We had to write our own MSC function. We're not going to do that.", "tokens": [492, 434, 516, 281, 362, 1730, 27822, 360, 300, 733, 295, 26947, 356, 337, 505, 13, 492, 632, 281, 2464, 527, 1065, 7395, 34, 2445, 13, 492, 434, 406, 516, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.1063353136966103, "compression_ratio": 2.032710280373832, "no_speech_prob": 9.368351129523944e-06}, {"id": 798, "seek": 519800, "start": 5217.0, "end": 5223.0, "text": " We had to write our own little matrix multiplication thing. We're not going to do that. We're going to have pie torch do all this stuff for us.", "tokens": [492, 632, 281, 2464, 527, 1065, 707, 8141, 27290, 551, 13, 492, 434, 406, 516, 281, 360, 300, 13, 492, 434, 516, 281, 362, 1730, 27822, 360, 439, 341, 1507, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.1063353136966103, "compression_ratio": 2.032710280373832, "no_speech_prob": 9.368351129523944e-06}, {"id": 799, "seek": 522300, "start": 5223.0, "end": 5234.0, "text": " OK. And what's more and really important we got we're going to do mini batches. This is a big enough data set. We probably don't want to do it all at once.", "tokens": [2264, 13, 400, 437, 311, 544, 293, 534, 1021, 321, 658, 321, 434, 516, 281, 360, 8382, 15245, 279, 13, 639, 307, 257, 955, 1547, 1412, 992, 13, 492, 1391, 500, 380, 528, 281, 360, 309, 439, 412, 1564, 13], "temperature": 0.0, "avg_logprob": -0.20989339491900275, "compression_ratio": 1.50625, "no_speech_prob": 8.529940714652184e-06}, {"id": 800, "seek": 522300, "start": 5234.0, "end": 5242.0, "text": " So if you want to do mini batches so we're not going to use too much fast stuff here.", "tokens": [407, 498, 291, 528, 281, 360, 8382, 15245, 279, 370, 321, 434, 406, 516, 281, 764, 886, 709, 2370, 1507, 510, 13], "temperature": 0.0, "avg_logprob": -0.20989339491900275, "compression_ratio": 1.50625, "no_speech_prob": 8.529940714652184e-06}, {"id": 801, "seek": 524200, "start": 5242.0, "end": 5254.0, "text": " Pie torch has something called tensor data set that basically grabs a any kind of tensor or two tensors and creates a data set.", "tokens": [22914, 27822, 575, 746, 1219, 40863, 1412, 992, 300, 1936, 30028, 257, 604, 733, 295, 40863, 420, 732, 10688, 830, 293, 7829, 257, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.13624710372731655, "compression_ratio": 1.690217391304348, "no_speech_prob": 1.3211310943006538e-05}, {"id": 802, "seek": 524200, "start": 5254.0, "end": 5263.0, "text": " Remember a data set is something where if you index into it you get back an X value and a Y value just one of them.", "tokens": [5459, 257, 1412, 992, 307, 746, 689, 498, 291, 8186, 666, 309, 291, 483, 646, 364, 1783, 2158, 293, 257, 398, 2158, 445, 472, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.13624710372731655, "compression_ratio": 1.690217391304348, "no_speech_prob": 1.3211310943006538e-05}, {"id": 803, "seek": 524200, "start": 5263.0, "end": 5269.0, "text": " So it kind of looks like it looks a lot like a list of X Y couples.", "tokens": [407, 309, 733, 295, 1542, 411, 309, 1542, 257, 688, 411, 257, 1329, 295, 1783, 398, 20368, 13], "temperature": 0.0, "avg_logprob": -0.13624710372731655, "compression_ratio": 1.690217391304348, "no_speech_prob": 1.3211310943006538e-05}, {"id": 804, "seek": 526900, "start": 5269.0, "end": 5276.0, "text": " Once you have a data set then you can use a little bit of convenience by calling data bunch dot create.", "tokens": [3443, 291, 362, 257, 1412, 992, 550, 291, 393, 764, 257, 707, 857, 295, 19283, 538, 5141, 1412, 3840, 5893, 1884, 13], "temperature": 0.0, "avg_logprob": -0.05828144675806949, "compression_ratio": 1.7407407407407407, "no_speech_prob": 1.80577662831638e-05}, {"id": 805, "seek": 526900, "start": 5276.0, "end": 5286.0, "text": " And what's that going to do is it's going to create data loaders for you. A data loader is something which you don't say I want the first thing or the fifth thing.", "tokens": [400, 437, 311, 300, 516, 281, 360, 307, 309, 311, 516, 281, 1884, 1412, 3677, 433, 337, 291, 13, 316, 1412, 3677, 260, 307, 746, 597, 291, 500, 380, 584, 286, 528, 264, 700, 551, 420, 264, 9266, 551, 13], "temperature": 0.0, "avg_logprob": -0.05828144675806949, "compression_ratio": 1.7407407407407407, "no_speech_prob": 1.80577662831638e-05}, {"id": 806, "seek": 526900, "start": 5286.0, "end": 5293.0, "text": " You just say I want the next thing and it will give you a batch a mini batch of whatever size you asked for.", "tokens": [509, 445, 584, 286, 528, 264, 958, 551, 293, 309, 486, 976, 291, 257, 15245, 257, 8382, 15245, 295, 2035, 2744, 291, 2351, 337, 13], "temperature": 0.0, "avg_logprob": -0.05828144675806949, "compression_ratio": 1.7407407407407407, "no_speech_prob": 1.80577662831638e-05}, {"id": 807, "seek": 529300, "start": 5293.0, "end": 5302.0, "text": " And specifically it'll give you the X and the Y of a mini batch. So if I just grab the next of the iterator this is just standard Python.", "tokens": [400, 4682, 309, 603, 976, 291, 264, 1783, 293, 264, 398, 295, 257, 8382, 15245, 13, 407, 498, 286, 445, 4444, 264, 958, 295, 264, 17138, 1639, 341, 307, 445, 3832, 15329, 13], "temperature": 0.0, "avg_logprob": -0.12792656373004524, "compression_ratio": 1.598326359832636, "no_speech_prob": 1.0782785466290079e-05}, {"id": 808, "seek": 529300, "start": 5302.0, "end": 5310.0, "text": " If you haven't used iterators in Python before here's my training data loader that data bunch dot create creates for you.", "tokens": [759, 291, 2378, 380, 1143, 17138, 3391, 294, 15329, 949, 510, 311, 452, 3097, 1412, 3677, 260, 300, 1412, 3840, 5893, 1884, 7829, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.12792656373004524, "compression_ratio": 1.598326359832636, "no_speech_prob": 1.0782785466290079e-05}, {"id": 809, "seek": 529300, "start": 5310.0, "end": 5320.0, "text": " And you can check that as you would expect the X is 64 by 784 because there's 784 pixels flattened out 64 in a mini batch.", "tokens": [400, 291, 393, 1520, 300, 382, 291, 576, 2066, 264, 1783, 307, 12145, 538, 1614, 25494, 570, 456, 311, 1614, 25494, 18668, 24183, 292, 484, 12145, 294, 257, 8382, 15245, 13], "temperature": 0.0, "avg_logprob": -0.12792656373004524, "compression_ratio": 1.598326359832636, "no_speech_prob": 1.0782785466290079e-05}, {"id": 810, "seek": 532000, "start": 5320.0, "end": 5324.0, "text": " The Y is just 64 numbers. There are things we're trying to predict.", "tokens": [440, 398, 307, 445, 12145, 3547, 13, 821, 366, 721, 321, 434, 1382, 281, 6069, 13], "temperature": 0.0, "avg_logprob": -0.09032151222229004, "compression_ratio": 1.6147540983606556, "no_speech_prob": 7.0717042035539635e-06}, {"id": 811, "seek": 532000, "start": 5324.0, "end": 5331.0, "text": " So and you know if you look at the source code for data batch dot create you'll see there's not much there.", "tokens": [407, 293, 291, 458, 498, 291, 574, 412, 264, 4009, 3089, 337, 1412, 15245, 5893, 1884, 291, 603, 536, 456, 311, 406, 709, 456, 13], "temperature": 0.0, "avg_logprob": -0.09032151222229004, "compression_ratio": 1.6147540983606556, "no_speech_prob": 7.0717042035539635e-06}, {"id": 812, "seek": 532000, "start": 5331.0, "end": 5337.0, "text": " But feel free to do so. We just make sure that like your training set gets shuffled randomly shuffled for you.", "tokens": [583, 841, 1737, 281, 360, 370, 13, 492, 445, 652, 988, 300, 411, 428, 3097, 992, 2170, 402, 33974, 16979, 402, 33974, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.09032151222229004, "compression_ratio": 1.6147540983606556, "no_speech_prob": 7.0717042035539635e-06}, {"id": 813, "seek": 532000, "start": 5337.0, "end": 5346.0, "text": " We make sure that the data is put on the GPU for you. Just a couple of little convenience things like that.", "tokens": [492, 652, 988, 300, 264, 1412, 307, 829, 322, 264, 18407, 337, 291, 13, 1449, 257, 1916, 295, 707, 19283, 721, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.09032151222229004, "compression_ratio": 1.6147540983606556, "no_speech_prob": 7.0717042035539635e-06}, {"id": 814, "seek": 534600, "start": 5346.0, "end": 5352.0, "text": " But don't let it be magic. If it feels magic check out the source code to make sure you see what's going on. OK.", "tokens": [583, 500, 380, 718, 309, 312, 5585, 13, 759, 309, 3417, 5585, 1520, 484, 264, 4009, 3089, 281, 652, 988, 291, 536, 437, 311, 516, 322, 13, 2264, 13], "temperature": 0.0, "avg_logprob": -0.12961553443561902, "compression_ratio": 1.6262135922330097, "no_speech_prob": 1.0129627298738342e-05}, {"id": 815, "seek": 534600, "start": 5352.0, "end": 5359.0, "text": " So rather than do this Y hat equals X hat a thing we're going to create an end up module.", "tokens": [407, 2831, 813, 360, 341, 398, 2385, 6915, 1783, 2385, 257, 551, 321, 434, 516, 281, 1884, 364, 917, 493, 10088, 13], "temperature": 0.0, "avg_logprob": -0.12961553443561902, "compression_ratio": 1.6262135922330097, "no_speech_prob": 1.0129627298738342e-05}, {"id": 816, "seek": 534600, "start": 5359.0, "end": 5366.0, "text": " All right. If you want to create an end up module that does something different to what's already out there you have to subclass it.", "tokens": [1057, 558, 13, 759, 291, 528, 281, 1884, 364, 917, 493, 10088, 300, 775, 746, 819, 281, 437, 311, 1217, 484, 456, 291, 362, 281, 1422, 11665, 309, 13], "temperature": 0.0, "avg_logprob": -0.12961553443561902, "compression_ratio": 1.6262135922330097, "no_speech_prob": 1.0129627298738342e-05}, {"id": 817, "seek": 536600, "start": 5366.0, "end": 5379.0, "text": " Right. So subclassing is very very very normal in PyTorch. So if you're not comfortable with subclassing stuff in Python go read a couple of tutorials to make sure you are.", "tokens": [1779, 13, 407, 1422, 11665, 278, 307, 588, 588, 588, 2710, 294, 9953, 51, 284, 339, 13, 407, 498, 291, 434, 406, 4619, 365, 1422, 11665, 278, 1507, 294, 15329, 352, 1401, 257, 1916, 295, 17616, 281, 652, 988, 291, 366, 13], "temperature": 0.0, "avg_logprob": -0.14033014749743275, "compression_ratio": 1.8488888888888888, "no_speech_prob": 9.368211976834573e-06}, {"id": 818, "seek": 536600, "start": 5379.0, "end": 5394.0, "text": " The main thing is you have to override the constructor done to in it and make sure that you call the super classes constructor because end up module super classes constructor is going to like set it all up to be a proper end up module for you.", "tokens": [440, 2135, 551, 307, 291, 362, 281, 42321, 264, 47479, 1096, 281, 294, 309, 293, 652, 988, 300, 291, 818, 264, 1687, 5359, 47479, 570, 917, 493, 10088, 1687, 5359, 47479, 307, 516, 281, 411, 992, 309, 439, 493, 281, 312, 257, 2296, 917, 493, 10088, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.14033014749743275, "compression_ratio": 1.8488888888888888, "no_speech_prob": 9.368211976834573e-06}, {"id": 819, "seek": 539400, "start": 5394.0, "end": 5404.0, "text": " So if you try to using if you're trying to create your own PyTorch subclass and things don't work it's almost certainly because you forgot this line of code.", "tokens": [407, 498, 291, 853, 281, 1228, 498, 291, 434, 1382, 281, 1884, 428, 1065, 9953, 51, 284, 339, 1422, 11665, 293, 721, 500, 380, 589, 309, 311, 1920, 3297, 570, 291, 5298, 341, 1622, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.08559796684666683, "compression_ratio": 1.5824742268041236, "no_speech_prob": 7.071630079735769e-06}, {"id": 820, "seek": 539400, "start": 5404.0, "end": 5415.0, "text": " All right. So the only thing we want to add is we want to create an attribute in our class which contains a linear layer and an end up linear module.", "tokens": [1057, 558, 13, 407, 264, 787, 551, 321, 528, 281, 909, 307, 321, 528, 281, 1884, 364, 19667, 294, 527, 1508, 597, 8306, 257, 8213, 4583, 293, 364, 917, 493, 8213, 10088, 13], "temperature": 0.0, "avg_logprob": -0.08559796684666683, "compression_ratio": 1.5824742268041236, "no_speech_prob": 7.071630079735769e-06}, {"id": 821, "seek": 541500, "start": 5415.0, "end": 5426.0, "text": " What is an end up linear module. It's something which does that but actually it doesn't only do that it actually is X at a plus B.", "tokens": [708, 307, 364, 917, 493, 8213, 10088, 13, 467, 311, 746, 597, 775, 300, 457, 767, 309, 1177, 380, 787, 360, 300, 309, 767, 307, 1783, 412, 257, 1804, 363, 13], "temperature": 0.0, "avg_logprob": -0.10513900888377223, "compression_ratio": 1.4217687074829932, "no_speech_prob": 1.8924331016023643e-05}, {"id": 822, "seek": 541500, "start": 5426.0, "end": 5430.0, "text": " So in other words we don't have to add the column of ones. That's all it does.", "tokens": [407, 294, 661, 2283, 321, 500, 380, 362, 281, 909, 264, 7738, 295, 2306, 13, 663, 311, 439, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.10513900888377223, "compression_ratio": 1.4217687074829932, "no_speech_prob": 1.8924331016023643e-05}, {"id": 823, "seek": 543000, "start": 5430.0, "end": 5455.0, "text": " OK so if you want to play around why don't you try and create your own and end up linear class. You could create something called my linear and it'll take you you know depending on your PyTorch background an hour or two and then you'll feel like OK this is we don't want any of this to be magic and you know all of the things necessary to create this now.", "tokens": [2264, 370, 498, 291, 528, 281, 862, 926, 983, 500, 380, 291, 853, 293, 1884, 428, 1065, 293, 917, 493, 8213, 1508, 13, 509, 727, 1884, 746, 1219, 452, 8213, 293, 309, 603, 747, 291, 291, 458, 5413, 322, 428, 9953, 51, 284, 339, 3678, 364, 1773, 420, 732, 293, 550, 291, 603, 841, 411, 2264, 341, 307, 321, 500, 380, 528, 604, 295, 341, 281, 312, 5585, 293, 291, 458, 439, 295, 264, 721, 4818, 281, 1884, 341, 586, 13], "temperature": 0.0, "avg_logprob": -0.14688610750086167, "compression_ratio": 1.6824644549763033, "no_speech_prob": 1.1842120329674799e-05}, {"id": 824, "seek": 545500, "start": 5455.0, "end": 5469.0, "text": " So you know these are the kind of things that you should be doing for your assignments this week is not so much new applications but try to start writing more of these things from scratch and get them to work learn how to debug them check what's going in and out and so forth.", "tokens": [407, 291, 458, 613, 366, 264, 733, 295, 721, 300, 291, 820, 312, 884, 337, 428, 22546, 341, 1243, 307, 406, 370, 709, 777, 5821, 457, 853, 281, 722, 3579, 544, 295, 613, 721, 490, 8459, 293, 483, 552, 281, 589, 1466, 577, 281, 24083, 552, 1520, 437, 311, 516, 294, 293, 484, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.13036745071411132, "compression_ratio": 1.656, "no_speech_prob": 3.9438720705220476e-05}, {"id": 825, "seek": 545500, "start": 5469.0, "end": 5478.0, "text": " OK. But we can just use an end up linear and that's just going to do so it's going to have a depth forward in it that goes a at X plus B.", "tokens": [2264, 13, 583, 321, 393, 445, 764, 364, 917, 493, 8213, 293, 300, 311, 445, 516, 281, 360, 370, 309, 311, 516, 281, 362, 257, 7161, 2128, 294, 309, 300, 1709, 257, 412, 1783, 1804, 363, 13], "temperature": 0.0, "avg_logprob": -0.13036745071411132, "compression_ratio": 1.656, "no_speech_prob": 3.9438720705220476e-05}, {"id": 826, "seek": 547800, "start": 5478.0, "end": 5502.0, "text": " Right. And so then an hour forward. How do we calculate the result of this. Well remember every end up module looks like a function. So we pass our X mini batch. So I tend to use X B to mean a batch of X to self dot Lin and that's going to give us back the result of the A at X plus B on this mini batch.", "tokens": [1779, 13, 400, 370, 550, 364, 1773, 2128, 13, 1012, 360, 321, 8873, 264, 1874, 295, 341, 13, 1042, 1604, 633, 917, 493, 10088, 1542, 411, 257, 2445, 13, 407, 321, 1320, 527, 1783, 8382, 15245, 13, 407, 286, 3928, 281, 764, 1783, 363, 281, 914, 257, 15245, 295, 1783, 281, 2698, 5893, 9355, 293, 300, 311, 516, 281, 976, 505, 646, 264, 1874, 295, 264, 316, 412, 1783, 1804, 363, 322, 341, 8382, 15245, 13], "temperature": 0.0, "avg_logprob": -0.18479136228561402, "compression_ratio": 1.52, "no_speech_prob": 3.4804746974259615e-05}, {"id": 827, "seek": 550200, "start": 5502.0, "end": 5517.0, "text": " So this is a logistic regression model a logistic regression model is also known as a neural net with no hidden layers. So it's a one layer neural net no nonlinearities because we're doing stuff ourselves a little bit.", "tokens": [407, 341, 307, 257, 3565, 3142, 24590, 2316, 257, 3565, 3142, 24590, 2316, 307, 611, 2570, 382, 257, 18161, 2533, 365, 572, 7633, 7914, 13, 407, 309, 311, 257, 472, 4583, 18161, 2533, 572, 2107, 28263, 1088, 570, 321, 434, 884, 1507, 4175, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.11952543258666992, "compression_ratio": 1.5912408759124088, "no_speech_prob": 1.8057080524158664e-05}, {"id": 828, "seek": 551700, "start": 5517.0, "end": 5534.0, "text": " We have to put the weight matrices the parameters onto the GPU manually. So just type dot CUDA to do that. So here's our model. And as you can see the end up module machinery has automatically given us a representation of it.", "tokens": [492, 362, 281, 829, 264, 3364, 32284, 264, 9834, 3911, 264, 18407, 16945, 13, 407, 445, 2010, 5893, 29777, 7509, 281, 360, 300, 13, 407, 510, 311, 527, 2316, 13, 400, 382, 291, 393, 536, 264, 917, 493, 10088, 27302, 575, 6772, 2212, 505, 257, 10290, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.1494635035482685, "compression_ratio": 1.5822784810126582, "no_speech_prob": 3.943277260987088e-05}, {"id": 829, "seek": 551700, "start": 5534.0, "end": 5542.0, "text": " It's automatically stored the dot Lin thing and it's telling us what's inside it. So there's a lot of little conveniences that pie torch does for us.", "tokens": [467, 311, 6772, 12187, 264, 5893, 9355, 551, 293, 309, 311, 3585, 505, 437, 311, 1854, 309, 13, 407, 456, 311, 257, 688, 295, 707, 7158, 14004, 300, 1730, 27822, 775, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.1494635035482685, "compression_ratio": 1.5822784810126582, "no_speech_prob": 3.943277260987088e-05}, {"id": 830, "seek": 554200, "start": 5542.0, "end": 5562.0, "text": " So if you look at now at model dot Lin you can see not surprisingly here it is perhaps the most interesting thing to point out is that our model automatically gets a bunch of methods and properties and perhaps the most interesting one is the one called parameters", "tokens": [407, 498, 291, 574, 412, 586, 412, 2316, 5893, 9355, 291, 393, 536, 406, 17600, 510, 309, 307, 4317, 264, 881, 1880, 551, 281, 935, 484, 307, 300, 527, 2316, 6772, 2170, 257, 3840, 295, 7150, 293, 7221, 293, 4317, 264, 881, 1880, 472, 307, 264, 472, 1219, 9834], "temperature": 0.0, "avg_logprob": -0.08025511255804098, "compression_ratio": 1.730263157894737, "no_speech_prob": 1.6700359992682934e-05}, {"id": 831, "seek": 556200, "start": 5562.0, "end": 5575.0, "text": " which contains all of the yellow squares from our picture that contains our parameters it contains our weight matrices and bias matrices in as much as they're different.", "tokens": [597, 8306, 439, 295, 264, 5566, 19368, 490, 527, 3036, 300, 8306, 527, 9834, 309, 8306, 527, 3364, 32284, 293, 12577, 32284, 294, 382, 709, 382, 436, 434, 819, 13], "temperature": 0.0, "avg_logprob": -0.1299161768671292, "compression_ratio": 1.6629213483146068, "no_speech_prob": 1.5445564713445492e-05}, {"id": 832, "seek": 556200, "start": 5575.0, "end": 5584.0, "text": " So if we have a look at P dot shape for P and model dot parameters there's something of 10 by 784 and there's something of 10.", "tokens": [407, 498, 321, 362, 257, 574, 412, 430, 5893, 3909, 337, 430, 293, 2316, 5893, 9834, 456, 311, 746, 295, 1266, 538, 1614, 25494, 293, 456, 311, 746, 295, 1266, 13], "temperature": 0.0, "avg_logprob": -0.1299161768671292, "compression_ratio": 1.6629213483146068, "no_speech_prob": 1.5445564713445492e-05}, {"id": 833, "seek": 558400, "start": 5584.0, "end": 5600.0, "text": " So what are they or 10 by 784. OK so that's the thing that's going to take in 784 dimensional input and spit out a 10 dimensional output because that's handy because our input is 784 dimensional and we need something that's going to give us the probability of 10 numbers.", "tokens": [407, 437, 366, 436, 420, 1266, 538, 1614, 25494, 13, 2264, 370, 300, 311, 264, 551, 300, 311, 516, 281, 747, 294, 1614, 25494, 18795, 4846, 293, 22127, 484, 257, 1266, 18795, 5598, 570, 300, 311, 13239, 570, 527, 4846, 307, 1614, 25494, 18795, 293, 321, 643, 746, 300, 311, 516, 281, 976, 505, 264, 8482, 295, 1266, 3547, 13], "temperature": 0.0, "avg_logprob": -0.11477933079004288, "compression_ratio": 1.6832298136645962, "no_speech_prob": 1.0288546945957933e-05}, {"id": 834, "seek": 560000, "start": 5600.0, "end": 5621.0, "text": " After that happens we've got 10 activations which we then want to add the bias to. So there we go. Here's a vector of length 10. So you can see why this this model we've created has exactly the stuff that we need to do our AX plus B.", "tokens": [2381, 300, 2314, 321, 600, 658, 1266, 2430, 763, 597, 321, 550, 528, 281, 909, 264, 12577, 281, 13, 407, 456, 321, 352, 13, 1692, 311, 257, 8062, 295, 4641, 1266, 13, 407, 291, 393, 536, 983, 341, 341, 2316, 321, 600, 2942, 575, 2293, 264, 1507, 300, 321, 643, 281, 360, 527, 316, 55, 1804, 363, 13], "temperature": 0.0, "avg_logprob": -0.06924288503585323, "compression_ratio": 1.420731707317073, "no_speech_prob": 1.1478286978672259e-05}, {"id": 835, "seek": 562100, "start": 5621.0, "end": 5632.0, "text": " So let's grab a learning right. We're going to come back to this loss function in a moment but we can't use MS. Well we can't really use MSC for this right because we're not trying to say how close are you.", "tokens": [407, 718, 311, 4444, 257, 2539, 558, 13, 492, 434, 516, 281, 808, 646, 281, 341, 4470, 2445, 294, 257, 1623, 457, 321, 393, 380, 764, 7395, 13, 1042, 321, 393, 380, 534, 764, 7395, 34, 337, 341, 558, 570, 321, 434, 406, 1382, 281, 584, 577, 1998, 366, 291, 13], "temperature": 0.0, "avg_logprob": -0.12018804642760637, "compression_ratio": 1.6923076923076923, "no_speech_prob": 3.5909215512219816e-05}, {"id": 836, "seek": 562100, "start": 5632.0, "end": 5644.0, "text": " Did you predict three and actually it was four. Gosh you were really close. It's like no three is just as far away from four as zero is away from four when you're trying to predict what number did somebody draw.", "tokens": [2589, 291, 6069, 1045, 293, 767, 309, 390, 1451, 13, 19185, 291, 645, 534, 1998, 13, 467, 311, 411, 572, 1045, 307, 445, 382, 1400, 1314, 490, 1451, 382, 4018, 307, 1314, 490, 1451, 562, 291, 434, 1382, 281, 6069, 437, 1230, 630, 2618, 2642, 13], "temperature": 0.0, "avg_logprob": -0.12018804642760637, "compression_ratio": 1.6923076923076923, "no_speech_prob": 3.5909215512219816e-05}, {"id": 837, "seek": 564400, "start": 5644.0, "end": 5655.0, "text": " So we're not going to use MSC. We're going to use cross entropy loss which we'll look at in a moment. And here's our update function. I copied it from lesson 2 SGD.", "tokens": [407, 321, 434, 406, 516, 281, 764, 7395, 34, 13, 492, 434, 516, 281, 764, 3278, 30867, 4470, 597, 321, 603, 574, 412, 294, 257, 1623, 13, 400, 510, 311, 527, 5623, 2445, 13, 286, 25365, 309, 490, 6898, 568, 34520, 35, 13], "temperature": 0.0, "avg_logprob": -0.11290504054019326, "compression_ratio": 1.76, "no_speech_prob": 1.1300122423563153e-05}, {"id": 838, "seek": 564400, "start": 5655.0, "end": 5668.0, "text": " But now we're calling our model rather than going AX we're calling our model as if it was a function to get Y hat. And we're calling our loss func rather than calling MSC to get our loss.", "tokens": [583, 586, 321, 434, 5141, 527, 2316, 2831, 813, 516, 316, 55, 321, 434, 5141, 527, 2316, 382, 498, 309, 390, 257, 2445, 281, 483, 398, 2385, 13, 400, 321, 434, 5141, 527, 4470, 1019, 66, 2831, 813, 5141, 7395, 34, 281, 483, 527, 4470, 13], "temperature": 0.0, "avg_logprob": -0.11290504054019326, "compression_ratio": 1.76, "no_speech_prob": 1.1300122423563153e-05}, {"id": 839, "seek": 566800, "start": 5668.0, "end": 5678.0, "text": " And then this is all the same as before except rather than going through each parameter and going parameter dot sub underscore learning rate times gradient.", "tokens": [400, 550, 341, 307, 439, 264, 912, 382, 949, 3993, 2831, 813, 516, 807, 1184, 13075, 293, 516, 13075, 5893, 1422, 37556, 2539, 3314, 1413, 16235, 13], "temperature": 0.0, "avg_logprob": -0.13877209373142407, "compression_ratio": 1.6323529411764706, "no_speech_prob": 3.373474100953899e-05}, {"id": 840, "seek": 566800, "start": 5678.0, "end": 5691.0, "text": " We loop through the parameters. OK. Because very nicely for us pipe torch will automatically create this list of the parameters of anything that we created in our Dundee in it.", "tokens": [492, 6367, 807, 264, 9834, 13, 2264, 13, 1436, 588, 9594, 337, 505, 11240, 27822, 486, 6772, 1884, 341, 1329, 295, 264, 9834, 295, 1340, 300, 321, 2942, 294, 527, 413, 997, 1653, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.13877209373142407, "compression_ratio": 1.6323529411764706, "no_speech_prob": 3.373474100953899e-05}, {"id": 841, "seek": 569100, "start": 5691.0, "end": 5704.0, "text": " And look I've added something else. I've got this thing called W2. I go through each PN model dot parameters and I add to W2 the sum of squares.", "tokens": [400, 574, 286, 600, 3869, 746, 1646, 13, 286, 600, 658, 341, 551, 1219, 343, 17, 13, 286, 352, 807, 1184, 430, 45, 2316, 5893, 9834, 293, 286, 909, 281, 343, 17, 264, 2408, 295, 19368, 13], "temperature": 0.0, "avg_logprob": -0.14005654897445288, "compression_ratio": 1.5126903553299493, "no_speech_prob": 2.4297494746861048e-05}, {"id": 842, "seek": 569100, "start": 5704.0, "end": 5717.0, "text": " So W2 now contains my sum of squares weights and then I multiply it by some number which I set to one A neg five. So now I just implemented weight decay.", "tokens": [407, 343, 17, 586, 8306, 452, 2408, 295, 19368, 17443, 293, 550, 286, 12972, 309, 538, 512, 1230, 597, 286, 992, 281, 472, 316, 2485, 1732, 13, 407, 586, 286, 445, 12270, 3364, 21039, 13], "temperature": 0.0, "avg_logprob": -0.14005654897445288, "compression_ratio": 1.5126903553299493, "no_speech_prob": 2.4297494746861048e-05}, {"id": 843, "seek": 571700, "start": 5717.0, "end": 5727.0, "text": " OK. So when people talk about weight decay it's not an amazing magic complex thing containing thousands of lines of CUDA C++ code.", "tokens": [2264, 13, 407, 562, 561, 751, 466, 3364, 21039, 309, 311, 406, 364, 2243, 5585, 3997, 551, 19273, 5383, 295, 3876, 295, 29777, 7509, 383, 25472, 3089, 13], "temperature": 0.0, "avg_logprob": -0.19303934124932773, "compression_ratio": 1.4864864864864864, "no_speech_prob": 1.0952184311463498e-05}, {"id": 844, "seek": 571700, "start": 5727.0, "end": 5737.0, "text": " It's. Those two lines of Python that's where to get this is not a simplified version that's just enough for now. This is where to get that's it.", "tokens": [467, 311, 13, 3950, 732, 3876, 295, 15329, 300, 311, 689, 281, 483, 341, 307, 406, 257, 26335, 3037, 300, 311, 445, 1547, 337, 586, 13, 639, 307, 689, 281, 483, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.19303934124932773, "compression_ratio": 1.4864864864864864, "no_speech_prob": 1.0952184311463498e-05}, {"id": 845, "seek": 573700, "start": 5737.0, "end": 5747.0, "text": " OK. And so here's the thing. There's a really interesting kind of dual way of thinking about weight decay.", "tokens": [2264, 13, 400, 370, 510, 311, 264, 551, 13, 821, 311, 257, 534, 1880, 733, 295, 11848, 636, 295, 1953, 466, 3364, 21039, 13], "temperature": 0.0, "avg_logprob": -0.10943980715168056, "compression_ratio": 1.5121951219512195, "no_speech_prob": 3.55583733835374e-06}, {"id": 846, "seek": 573700, "start": 5747.0, "end": 5753.0, "text": " One is that we're adding the sum of squared weights and that seems like a very sound thing to do.", "tokens": [1485, 307, 300, 321, 434, 5127, 264, 2408, 295, 8889, 17443, 293, 300, 2544, 411, 257, 588, 1626, 551, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.10943980715168056, "compression_ratio": 1.5121951219512195, "no_speech_prob": 3.55583733835374e-06}, {"id": 847, "seek": 573700, "start": 5753.0, "end": 5762.0, "text": " And it is. And let's go ahead and run this.", "tokens": [400, 309, 307, 13, 400, 718, 311, 352, 2286, 293, 1190, 341, 13], "temperature": 0.0, "avg_logprob": -0.10943980715168056, "compression_ratio": 1.5121951219512195, "no_speech_prob": 3.55583733835374e-06}, {"id": 848, "seek": 576200, "start": 5762.0, "end": 5768.0, "text": " So here I've just got a list comprehension that's going through my data loader.", "tokens": [407, 510, 286, 600, 445, 658, 257, 1329, 44991, 300, 311, 516, 807, 452, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.12668473112816905, "compression_ratio": 1.6008403361344539, "no_speech_prob": 2.2825703126727603e-05}, {"id": 849, "seek": 576200, "start": 5768.0, "end": 5774.0, "text": " So the data loader gives you back one mini batch for the whole thing giving you X Y each time.", "tokens": [407, 264, 1412, 3677, 260, 2709, 291, 646, 472, 8382, 15245, 337, 264, 1379, 551, 2902, 291, 1783, 398, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.12668473112816905, "compression_ratio": 1.6008403361344539, "no_speech_prob": 2.2825703126727603e-05}, {"id": 850, "seek": 576200, "start": 5774.0, "end": 5779.0, "text": " I'm going to call update for each. Each one returns loss.", "tokens": [286, 478, 516, 281, 818, 5623, 337, 1184, 13, 6947, 472, 11247, 4470, 13], "temperature": 0.0, "avg_logprob": -0.12668473112816905, "compression_ratio": 1.6008403361344539, "no_speech_prob": 2.2825703126727603e-05}, {"id": 851, "seek": 576200, "start": 5779.0, "end": 5790.0, "text": " Now PyTorch tensors. Since I did it all on the GPU that's sitting in the GPU and it's like got all this stuff attached to it to calculate gradients.", "tokens": [823, 9953, 51, 284, 339, 10688, 830, 13, 4162, 286, 630, 309, 439, 322, 264, 18407, 300, 311, 3798, 294, 264, 18407, 293, 309, 311, 411, 658, 439, 341, 1507, 8570, 281, 309, 281, 8873, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.12668473112816905, "compression_ratio": 1.6008403361344539, "no_speech_prob": 2.2825703126727603e-05}, {"id": 852, "seek": 579000, "start": 5790.0, "end": 5799.0, "text": " It's going to use up a lot of memory. So if you if you call dot item on a scalar tensor it turns it into an actual normal Python number.", "tokens": [467, 311, 516, 281, 764, 493, 257, 688, 295, 4675, 13, 407, 498, 291, 498, 291, 818, 5893, 3174, 322, 257, 39684, 40863, 309, 4523, 309, 666, 364, 3539, 2710, 15329, 1230, 13], "temperature": 0.0, "avg_logprob": -0.08153522268254706, "compression_ratio": 1.6116071428571428, "no_speech_prob": 1.3211506484367419e-05}, {"id": 853, "seek": 579000, "start": 5799.0, "end": 5805.0, "text": " So this is just means I'm returning back normal Python numbers and then I can plot them.", "tokens": [407, 341, 307, 445, 1355, 286, 478, 12678, 646, 2710, 15329, 3547, 293, 550, 286, 393, 7542, 552, 13], "temperature": 0.0, "avg_logprob": -0.08153522268254706, "compression_ratio": 1.6116071428571428, "no_speech_prob": 1.3211506484367419e-05}, {"id": 854, "seek": 579000, "start": 5805.0, "end": 5809.0, "text": " And yeah there you go. My loss function is going down.", "tokens": [400, 1338, 456, 291, 352, 13, 1222, 4470, 2445, 307, 516, 760, 13], "temperature": 0.0, "avg_logprob": -0.08153522268254706, "compression_ratio": 1.6116071428571428, "no_speech_prob": 1.3211506484367419e-05}, {"id": 855, "seek": 579000, "start": 5809.0, "end": 5813.0, "text": " And you know it's really nice to try this stuff to see it behaves as you expect.", "tokens": [400, 291, 458, 309, 311, 534, 1481, 281, 853, 341, 1507, 281, 536, 309, 36896, 382, 291, 2066, 13], "temperature": 0.0, "avg_logprob": -0.08153522268254706, "compression_ratio": 1.6116071428571428, "no_speech_prob": 1.3211506484367419e-05}, {"id": 856, "seek": 581300, "start": 5813.0, "end": 5820.0, "text": " Like we thought this is what would happen as we get closer and closer to the answer. It bounces around more and more.", "tokens": [1743, 321, 1194, 341, 307, 437, 576, 1051, 382, 321, 483, 4966, 293, 4966, 281, 264, 1867, 13, 467, 46901, 926, 544, 293, 544, 13], "temperature": 0.0, "avg_logprob": -0.09034531090849189, "compression_ratio": 1.6740088105726871, "no_speech_prob": 6.339128503896063e-06}, {"id": 857, "seek": 581300, "start": 5820.0, "end": 5825.0, "text": " Right. Because we're kind of close to where we should be. It's kind of getting flat probably flatter in weight space.", "tokens": [1779, 13, 1436, 321, 434, 733, 295, 1998, 281, 689, 321, 820, 312, 13, 467, 311, 733, 295, 1242, 4962, 1391, 41247, 294, 3364, 1901, 13], "temperature": 0.0, "avg_logprob": -0.09034531090849189, "compression_ratio": 1.6740088105726871, "no_speech_prob": 6.339128503896063e-06}, {"id": 858, "seek": 581300, "start": 5825.0, "end": 5830.0, "text": " So we kind of jumping further. And so you can see why we would probably want to be reducing our learning rate as we go.", "tokens": [407, 321, 733, 295, 11233, 3052, 13, 400, 370, 291, 393, 536, 983, 321, 576, 1391, 528, 281, 312, 12245, 527, 2539, 3314, 382, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.09034531090849189, "compression_ratio": 1.6740088105726871, "no_speech_prob": 6.339128503896063e-06}, {"id": 859, "seek": 581300, "start": 5830.0, "end": 5833.0, "text": " Learning rate annealing.", "tokens": [15205, 3314, 22256, 4270, 13], "temperature": 0.0, "avg_logprob": -0.09034531090849189, "compression_ratio": 1.6740088105726871, "no_speech_prob": 6.339128503896063e-06}, {"id": 860, "seek": 583300, "start": 5833.0, "end": 5847.0, "text": " Now here's the thing that is only interesting for training a neural net because.", "tokens": [823, 510, 311, 264, 551, 300, 307, 787, 1880, 337, 3097, 257, 18161, 2533, 570, 13], "temperature": 0.0, "avg_logprob": -0.14145731925964355, "compression_ratio": 1.4230769230769231, "no_speech_prob": 1.0288929843227379e-05}, {"id": 861, "seek": 583300, "start": 5847.0, "end": 5853.0, "text": " It appears here. Because we take the gradient of it.", "tokens": [467, 7038, 510, 13, 1436, 321, 747, 264, 16235, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.14145731925964355, "compression_ratio": 1.4230769230769231, "no_speech_prob": 1.0288929843227379e-05}, {"id": 862, "seek": 583300, "start": 5853.0, "end": 5856.0, "text": " That's the thing that actually updates the weights.", "tokens": [663, 311, 264, 551, 300, 767, 9205, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.14145731925964355, "compression_ratio": 1.4230769230769231, "no_speech_prob": 1.0288929843227379e-05}, {"id": 863, "seek": 585600, "start": 5856.0, "end": 5864.0, "text": " Right. So they actually the only thing interesting about WD times sum of W squared is its gradient.", "tokens": [1779, 13, 407, 436, 767, 264, 787, 551, 1880, 466, 343, 35, 1413, 2408, 295, 343, 8889, 307, 1080, 16235, 13], "temperature": 0.0, "avg_logprob": -0.10111973925334651, "compression_ratio": 1.5666666666666667, "no_speech_prob": 9.22317758522695e-06}, {"id": 864, "seek": 585600, "start": 5864.0, "end": 5869.0, "text": " So we don't do a lot of math here but I think we can handle that.", "tokens": [407, 321, 500, 380, 360, 257, 688, 295, 5221, 510, 457, 286, 519, 321, 393, 4813, 300, 13], "temperature": 0.0, "avg_logprob": -0.10111973925334651, "compression_ratio": 1.5666666666666667, "no_speech_prob": 9.22317758522695e-06}, {"id": 865, "seek": 585600, "start": 5869.0, "end": 5873.0, "text": " The gradient. Of this whole thing.", "tokens": [440, 16235, 13, 2720, 341, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.10111973925334651, "compression_ratio": 1.5666666666666667, "no_speech_prob": 9.22317758522695e-06}, {"id": 866, "seek": 585600, "start": 5873.0, "end": 5881.0, "text": " If you remember back to your high school math is equal to the gradient of each part taken separately and then add them together.", "tokens": [759, 291, 1604, 646, 281, 428, 1090, 1395, 5221, 307, 2681, 281, 264, 16235, 295, 1184, 644, 2726, 14759, 293, 550, 909, 552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.10111973925334651, "compression_ratio": 1.5666666666666667, "no_speech_prob": 9.22317758522695e-06}, {"id": 867, "seek": 588100, "start": 5881.0, "end": 5886.0, "text": " So let's just take the gradient of that. Right. Because we already know the gradient of this is just whatever we had before.", "tokens": [407, 718, 311, 445, 747, 264, 16235, 295, 300, 13, 1779, 13, 1436, 321, 1217, 458, 264, 16235, 295, 341, 307, 445, 2035, 321, 632, 949, 13], "temperature": 0.0, "avg_logprob": -0.09929787195645846, "compression_ratio": 1.890547263681592, "no_speech_prob": 1.7230848243343644e-05}, {"id": 868, "seek": 588100, "start": 5886.0, "end": 5890.0, "text": " Right. So what's the gradient of WD times the sum of W squared.", "tokens": [1779, 13, 407, 437, 311, 264, 16235, 295, 343, 35, 1413, 264, 2408, 295, 343, 8889, 13], "temperature": 0.0, "avg_logprob": -0.09929787195645846, "compression_ratio": 1.890547263681592, "no_speech_prob": 1.7230848243343644e-05}, {"id": 869, "seek": 588100, "start": 5890.0, "end": 5894.0, "text": " Right. Let's remove the sum and pretend that just one parameter.", "tokens": [1779, 13, 961, 311, 4159, 264, 2408, 293, 11865, 300, 445, 472, 13075, 13], "temperature": 0.0, "avg_logprob": -0.09929787195645846, "compression_ratio": 1.890547263681592, "no_speech_prob": 1.7230848243343644e-05}, {"id": 870, "seek": 588100, "start": 5894.0, "end": 5903.0, "text": " It doesn't change the generality of it. So the gradient of WD times W squared.", "tokens": [467, 1177, 380, 1319, 264, 1337, 1860, 295, 309, 13, 407, 264, 16235, 295, 343, 35, 1413, 343, 8889, 13], "temperature": 0.0, "avg_logprob": -0.09929787195645846, "compression_ratio": 1.890547263681592, "no_speech_prob": 1.7230848243343644e-05}, {"id": 871, "seek": 588100, "start": 5903.0, "end": 5906.0, "text": " What's the gradient of that.", "tokens": [708, 311, 264, 16235, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.09929787195645846, "compression_ratio": 1.890547263681592, "no_speech_prob": 1.7230848243343644e-05}, {"id": 872, "seek": 588100, "start": 5906.0, "end": 5908.0, "text": " With respect to W.", "tokens": [2022, 3104, 281, 343, 13], "temperature": 0.0, "avg_logprob": -0.09929787195645846, "compression_ratio": 1.890547263681592, "no_speech_prob": 1.7230848243343644e-05}, {"id": 873, "seek": 590800, "start": 5908.0, "end": 5916.0, "text": " It's just two WD times W.", "tokens": [467, 311, 445, 732, 343, 35, 1413, 343, 13], "temperature": 0.0, "avg_logprob": -0.14931776465439214, "compression_ratio": 1.4864864864864864, "no_speech_prob": 2.6687446734285913e-05}, {"id": 874, "seek": 590800, "start": 5916.0, "end": 5925.0, "text": " Right. And so remember this is our constant which in our case was like well in that little loop it was one E neg five.", "tokens": [1779, 13, 400, 370, 1604, 341, 307, 527, 5754, 597, 294, 527, 1389, 390, 411, 731, 294, 300, 707, 6367, 309, 390, 472, 462, 2485, 1732, 13], "temperature": 0.0, "avg_logprob": -0.14931776465439214, "compression_ratio": 1.4864864864864864, "no_speech_prob": 2.6687446734285913e-05}, {"id": 875, "seek": 590800, "start": 5925.0, "end": 5927.0, "text": " OK. And that's our weights.", "tokens": [2264, 13, 400, 300, 311, 527, 17443, 13], "temperature": 0.0, "avg_logprob": -0.14931776465439214, "compression_ratio": 1.4864864864864864, "no_speech_prob": 2.6687446734285913e-05}, {"id": 876, "seek": 590800, "start": 5927.0, "end": 5933.0, "text": " And like we could replace WD with like two WD without loss of generality.", "tokens": [400, 411, 321, 727, 7406, 343, 35, 365, 411, 732, 343, 35, 1553, 4470, 295, 1337, 1860, 13], "temperature": 0.0, "avg_logprob": -0.14931776465439214, "compression_ratio": 1.4864864864864864, "no_speech_prob": 2.6687446734285913e-05}, {"id": 877, "seek": 590800, "start": 5933.0, "end": 5935.0, "text": " So let's throw away the two.", "tokens": [407, 718, 311, 3507, 1314, 264, 732, 13], "temperature": 0.0, "avg_logprob": -0.14931776465439214, "compression_ratio": 1.4864864864864864, "no_speech_prob": 2.6687446734285913e-05}, {"id": 878, "seek": 593500, "start": 5935.0, "end": 5947.0, "text": " So in other words all weight decay does is it subtracts some constant times the weights every time we do a batch.", "tokens": [407, 294, 661, 2283, 439, 3364, 21039, 775, 307, 309, 16390, 82, 512, 5754, 1413, 264, 17443, 633, 565, 321, 360, 257, 15245, 13], "temperature": 0.0, "avg_logprob": -0.08694408856905424, "compression_ratio": 1.5301204819277108, "no_speech_prob": 1.696372783044353e-05}, {"id": 879, "seek": 593500, "start": 5947.0, "end": 5951.0, "text": " So that's why it's called weight decay.", "tokens": [407, 300, 311, 983, 309, 311, 1219, 3364, 21039, 13], "temperature": 0.0, "avg_logprob": -0.08694408856905424, "compression_ratio": 1.5301204819277108, "no_speech_prob": 1.696372783044353e-05}, {"id": 880, "seek": 593500, "start": 5951.0, "end": 5964.0, "text": " When it's in this form where we add the square to the loss function that's called L2 regularization.", "tokens": [1133, 309, 311, 294, 341, 1254, 689, 321, 909, 264, 3732, 281, 264, 4470, 2445, 300, 311, 1219, 441, 17, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.08694408856905424, "compression_ratio": 1.5301204819277108, "no_speech_prob": 1.696372783044353e-05}, {"id": 881, "seek": 596400, "start": 5964.0, "end": 5977.0, "text": " When it's in this form where we subtract WD times weights from the gradients that's called weight decay.", "tokens": [1133, 309, 311, 294, 341, 1254, 689, 321, 16390, 343, 35, 1413, 17443, 490, 264, 2771, 2448, 300, 311, 1219, 3364, 21039, 13], "temperature": 0.0, "avg_logprob": -0.0800599429918372, "compression_ratio": 1.6652173913043478, "no_speech_prob": 7.411008937197039e-06}, {"id": 882, "seek": 596400, "start": 5977.0, "end": 5981.0, "text": " And they are kind of mathematically identical for everything we've seen so far.", "tokens": [400, 436, 366, 733, 295, 44003, 14800, 337, 1203, 321, 600, 1612, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.0800599429918372, "compression_ratio": 1.6652173913043478, "no_speech_prob": 7.411008937197039e-06}, {"id": 883, "seek": 596400, "start": 5981.0, "end": 5983.0, "text": " In fact they are mathematically identical.", "tokens": [682, 1186, 436, 366, 44003, 14800, 13], "temperature": 0.0, "avg_logprob": -0.0800599429918372, "compression_ratio": 1.6652173913043478, "no_speech_prob": 7.411008937197039e-06}, {"id": 884, "seek": 596400, "start": 5983.0, "end": 5988.0, "text": " And we'll see in a moment a place where they're not where things get interesting.", "tokens": [400, 321, 603, 536, 294, 257, 1623, 257, 1081, 689, 436, 434, 406, 689, 721, 483, 1880, 13], "temperature": 0.0, "avg_logprob": -0.0800599429918372, "compression_ratio": 1.6652173913043478, "no_speech_prob": 7.411008937197039e-06}, {"id": 885, "seek": 596400, "start": 5988.0, "end": 5993.0, "text": " OK. So this is just a really important tool you now have in your toolbox.", "tokens": [2264, 13, 407, 341, 307, 445, 257, 534, 1021, 2290, 291, 586, 362, 294, 428, 44593, 13], "temperature": 0.0, "avg_logprob": -0.0800599429918372, "compression_ratio": 1.6652173913043478, "no_speech_prob": 7.411008937197039e-06}, {"id": 886, "seek": 599300, "start": 5993.0, "end": 6002.0, "text": " You can make giant neural networks and still avoid overfitting by adding more weight decay.", "tokens": [509, 393, 652, 7410, 18161, 9590, 293, 920, 5042, 670, 69, 2414, 538, 5127, 544, 3364, 21039, 13], "temperature": 0.0, "avg_logprob": -0.07844461978060528, "compression_ratio": 1.7510373443983402, "no_speech_prob": 1.2218442861922085e-05}, {"id": 887, "seek": 599300, "start": 6002.0, "end": 6010.0, "text": " Or you could use really small data sets with moderately large sized models and avoid overfitting with weight decay.", "tokens": [1610, 291, 727, 764, 534, 1359, 1412, 6352, 365, 10494, 1592, 2416, 20004, 5245, 293, 5042, 670, 69, 2414, 365, 3364, 21039, 13], "temperature": 0.0, "avg_logprob": -0.07844461978060528, "compression_ratio": 1.7510373443983402, "no_speech_prob": 1.2218442861922085e-05}, {"id": 888, "seek": 599300, "start": 6010.0, "end": 6012.0, "text": " It's not magic. Right.", "tokens": [467, 311, 406, 5585, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.07844461978060528, "compression_ratio": 1.7510373443983402, "no_speech_prob": 1.2218442861922085e-05}, {"id": 889, "seek": 599300, "start": 6012.0, "end": 6019.0, "text": " Like you might still find you don't have enough data in which case like you get to the point where you're not overfitting by adding lots of weight decay.", "tokens": [1743, 291, 1062, 920, 915, 291, 500, 380, 362, 1547, 1412, 294, 597, 1389, 411, 291, 483, 281, 264, 935, 689, 291, 434, 406, 670, 69, 2414, 538, 5127, 3195, 295, 3364, 21039, 13], "temperature": 0.0, "avg_logprob": -0.07844461978060528, "compression_ratio": 1.7510373443983402, "no_speech_prob": 1.2218442861922085e-05}, {"id": 890, "seek": 599300, "start": 6019.0, "end": 6021.0, "text": " And it's just not training very well.", "tokens": [400, 309, 311, 445, 406, 3097, 588, 731, 13], "temperature": 0.0, "avg_logprob": -0.07844461978060528, "compression_ratio": 1.7510373443983402, "no_speech_prob": 1.2218442861922085e-05}, {"id": 891, "seek": 602100, "start": 6021.0, "end": 6023.0, "text": " That can happen.", "tokens": [663, 393, 1051, 13], "temperature": 0.0, "avg_logprob": -0.08384330456073467, "compression_ratio": 1.5233160621761659, "no_speech_prob": 1.5935298506519757e-05}, {"id": 892, "seek": 602100, "start": 6023.0, "end": 6030.0, "text": " But at least this is something that you can now play around with.", "tokens": [583, 412, 1935, 341, 307, 746, 300, 291, 393, 586, 862, 926, 365, 13], "temperature": 0.0, "avg_logprob": -0.08384330456073467, "compression_ratio": 1.5233160621761659, "no_speech_prob": 1.5935298506519757e-05}, {"id": 893, "seek": 602100, "start": 6030.0, "end": 6034.0, "text": " Just to kind of go on here.", "tokens": [1449, 281, 733, 295, 352, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.08384330456073467, "compression_ratio": 1.5233160621761659, "no_speech_prob": 1.5935298506519757e-05}, {"id": 894, "seek": 602100, "start": 6034.0, "end": 6043.0, "text": " Now that we've got this update function we could replace this MNIST logistic with MNIST neural network and build a neural network from scratch.", "tokens": [823, 300, 321, 600, 658, 341, 5623, 2445, 321, 727, 7406, 341, 376, 45, 19756, 3565, 3142, 365, 376, 45, 19756, 18161, 3209, 293, 1322, 257, 18161, 3209, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.08384330456073467, "compression_ratio": 1.5233160621761659, "no_speech_prob": 1.5935298506519757e-05}, {"id": 895, "seek": 602100, "start": 6043.0, "end": 6045.0, "text": " But now we just need two linear layers.", "tokens": [583, 586, 321, 445, 643, 732, 8213, 7914, 13], "temperature": 0.0, "avg_logprob": -0.08384330456073467, "compression_ratio": 1.5233160621761659, "no_speech_prob": 1.5935298506519757e-05}, {"id": 896, "seek": 604500, "start": 6045.0, "end": 6054.0, "text": " Right. And the first one we could use a weight matrix of size 50. And so we did need to make sure that the second linear layer has an input of size 50 so it matches.", "tokens": [1779, 13, 400, 264, 700, 472, 321, 727, 764, 257, 3364, 8141, 295, 2744, 2625, 13, 400, 370, 321, 630, 643, 281, 652, 988, 300, 264, 1150, 8213, 4583, 575, 364, 4846, 295, 2744, 2625, 370, 309, 10676, 13], "temperature": 0.0, "avg_logprob": -0.09288133639041508, "compression_ratio": 1.7276422764227641, "no_speech_prob": 2.796660555759445e-05}, {"id": 897, "seek": 604500, "start": 6054.0, "end": 6059.0, "text": " The final layer has to have an output of size 10 because that's the number of classes we're predicting.", "tokens": [440, 2572, 4583, 575, 281, 362, 364, 5598, 295, 2744, 1266, 570, 300, 311, 264, 1230, 295, 5359, 321, 434, 32884, 13], "temperature": 0.0, "avg_logprob": -0.09288133639041508, "compression_ratio": 1.7276422764227641, "no_speech_prob": 2.796660555759445e-05}, {"id": 898, "seek": 604500, "start": 6059.0, "end": 6063.0, "text": " And so now our forward just goes to a linear layer.", "tokens": [400, 370, 586, 527, 2128, 445, 1709, 281, 257, 8213, 4583, 13], "temperature": 0.0, "avg_logprob": -0.09288133639041508, "compression_ratio": 1.7276422764227641, "no_speech_prob": 2.796660555759445e-05}, {"id": 899, "seek": 604500, "start": 6063.0, "end": 6067.0, "text": " Calculate value to a second linear layer.", "tokens": [3511, 2444, 473, 2158, 281, 257, 1150, 8213, 4583, 13], "temperature": 0.0, "avg_logprob": -0.09288133639041508, "compression_ratio": 1.7276422764227641, "no_speech_prob": 2.796660555759445e-05}, {"id": 900, "seek": 604500, "start": 6067.0, "end": 6070.0, "text": " And now we've actually created a neural network from scratch.", "tokens": [400, 586, 321, 600, 767, 2942, 257, 18161, 3209, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.09288133639041508, "compression_ratio": 1.7276422764227641, "no_speech_prob": 2.796660555759445e-05}, {"id": 901, "seek": 607000, "start": 6070.0, "end": 6079.0, "text": " I mean we didn't write it in linear but you can write it yourself or you could like do the matrices directly you know how to.", "tokens": [286, 914, 321, 994, 380, 2464, 309, 294, 8213, 457, 291, 393, 2464, 309, 1803, 420, 291, 727, 411, 360, 264, 32284, 3838, 291, 458, 577, 281, 13], "temperature": 0.0, "avg_logprob": -0.12627781867980958, "compression_ratio": 1.6266094420600858, "no_speech_prob": 1.2218592928547878e-05}, {"id": 902, "seek": 607000, "start": 6079.0, "end": 6081.0, "text": " So again you know if we go a model.", "tokens": [407, 797, 291, 458, 498, 321, 352, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12627781867980958, "compression_ratio": 1.6266094420600858, "no_speech_prob": 1.2218592928547878e-05}, {"id": 903, "seek": 607000, "start": 6081.0, "end": 6086.0, "text": " Cuda and then we can calculate losses with the exact same update function.", "tokens": [383, 11152, 293, 550, 321, 393, 8873, 15352, 365, 264, 1900, 912, 5623, 2445, 13], "temperature": 0.0, "avg_logprob": -0.12627781867980958, "compression_ratio": 1.6266094420600858, "no_speech_prob": 1.2218592928547878e-05}, {"id": 904, "seek": 607000, "start": 6086.0, "end": 6088.0, "text": " There it goes.", "tokens": [821, 309, 1709, 13], "temperature": 0.0, "avg_logprob": -0.12627781867980958, "compression_ratio": 1.6266094420600858, "no_speech_prob": 1.2218592928547878e-05}, {"id": 905, "seek": 607000, "start": 6088.0, "end": 6091.0, "text": " Right. So this is why this kind of idea of neural nets is so easy.", "tokens": [1779, 13, 407, 341, 307, 983, 341, 733, 295, 1558, 295, 18161, 36170, 307, 370, 1858, 13], "temperature": 0.0, "avg_logprob": -0.12627781867980958, "compression_ratio": 1.6266094420600858, "no_speech_prob": 1.2218592928547878e-05}, {"id": 906, "seek": 607000, "start": 6091.0, "end": 6095.0, "text": " Right. Once you have something that can do gradient descent.", "tokens": [1779, 13, 3443, 291, 362, 746, 300, 393, 360, 16235, 23475, 13], "temperature": 0.0, "avg_logprob": -0.12627781867980958, "compression_ratio": 1.6266094420600858, "no_speech_prob": 1.2218592928547878e-05}, {"id": 907, "seek": 609500, "start": 6095.0, "end": 6100.0, "text": " Right. Then you can try different models.", "tokens": [1779, 13, 1396, 291, 393, 853, 819, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1724381563140125, "compression_ratio": 1.5053763440860215, "no_speech_prob": 5.422067260951735e-06}, {"id": 908, "seek": 609500, "start": 6100.0, "end": 6102.0, "text": " And then you can start to add more pie torch stuff.", "tokens": [400, 550, 291, 393, 722, 281, 909, 544, 1730, 27822, 1507, 13], "temperature": 0.0, "avg_logprob": -0.1724381563140125, "compression_ratio": 1.5053763440860215, "no_speech_prob": 5.422067260951735e-06}, {"id": 909, "seek": 609500, "start": 6102.0, "end": 6106.0, "text": " So like rather than add doing all this stuff yourself.", "tokens": [407, 411, 2831, 813, 909, 884, 439, 341, 1507, 1803, 13], "temperature": 0.0, "avg_logprob": -0.1724381563140125, "compression_ratio": 1.5053763440860215, "no_speech_prob": 5.422067260951735e-06}, {"id": 910, "seek": 609500, "start": 6106.0, "end": 6109.0, "text": " Why not just go.", "tokens": [1545, 406, 445, 352, 13], "temperature": 0.0, "avg_logprob": -0.1724381563140125, "compression_ratio": 1.5053763440860215, "no_speech_prob": 5.422067260951735e-06}, {"id": 911, "seek": 609500, "start": 6109.0, "end": 6113.0, "text": " Opt equals opt in dot something.", "tokens": [21455, 6915, 2427, 294, 5893, 746, 13], "temperature": 0.0, "avg_logprob": -0.1724381563140125, "compression_ratio": 1.5053763440860215, "no_speech_prob": 5.422067260951735e-06}, {"id": 912, "seek": 609500, "start": 6113.0, "end": 6117.0, "text": " So the something we've done so far is SGD.", "tokens": [407, 264, 746, 321, 600, 1096, 370, 1400, 307, 34520, 35, 13], "temperature": 0.0, "avg_logprob": -0.1724381563140125, "compression_ratio": 1.5053763440860215, "no_speech_prob": 5.422067260951735e-06}, {"id": 913, "seek": 609500, "start": 6117.0, "end": 6119.0, "text": " And so now you're saying to pie torch.", "tokens": [400, 370, 586, 291, 434, 1566, 281, 1730, 27822, 13], "temperature": 0.0, "avg_logprob": -0.1724381563140125, "compression_ratio": 1.5053763440860215, "no_speech_prob": 5.422067260951735e-06}, {"id": 914, "seek": 611900, "start": 6119.0, "end": 6125.0, "text": " I want you to take these parameters and optimize them using SGD.", "tokens": [286, 528, 291, 281, 747, 613, 9834, 293, 19719, 552, 1228, 34520, 35, 13], "temperature": 0.0, "avg_logprob": -0.15975808478021003, "compression_ratio": 1.5321637426900585, "no_speech_prob": 1.1658564289973583e-05}, {"id": 915, "seek": 611900, "start": 6125.0, "end": 6135.0, "text": " And so this now rather than saying for P and parameters P minus equals LR times P dot grad you just say up dot step.", "tokens": [400, 370, 341, 586, 2831, 813, 1566, 337, 430, 293, 9834, 430, 3175, 6915, 441, 49, 1413, 430, 5893, 2771, 291, 445, 584, 493, 5893, 1823, 13], "temperature": 0.0, "avg_logprob": -0.15975808478021003, "compression_ratio": 1.5321637426900585, "no_speech_prob": 1.1658564289973583e-05}, {"id": 916, "seek": 611900, "start": 6135.0, "end": 6137.0, "text": " It's the same thing.", "tokens": [467, 311, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.15975808478021003, "compression_ratio": 1.5321637426900585, "no_speech_prob": 1.1658564289973583e-05}, {"id": 917, "seek": 611900, "start": 6137.0, "end": 6139.0, "text": " It's just less code.", "tokens": [467, 311, 445, 1570, 3089, 13], "temperature": 0.0, "avg_logprob": -0.15975808478021003, "compression_ratio": 1.5321637426900585, "no_speech_prob": 1.1658564289973583e-05}, {"id": 918, "seek": 611900, "start": 6139.0, "end": 6140.0, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.15975808478021003, "compression_ratio": 1.5321637426900585, "no_speech_prob": 1.1658564289973583e-05}, {"id": 919, "seek": 611900, "start": 6140.0, "end": 6141.0, "text": " But and it does the same thing.", "tokens": [583, 293, 309, 775, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.15975808478021003, "compression_ratio": 1.5321637426900585, "no_speech_prob": 1.1658564289973583e-05}, {"id": 920, "seek": 614100, "start": 6141.0, "end": 6150.0, "text": " But the reason it's kind of particularly interesting is that now you can replace SGD with Adam for example.", "tokens": [583, 264, 1778, 309, 311, 733, 295, 4098, 1880, 307, 300, 586, 291, 393, 7406, 34520, 35, 365, 7938, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.10425180121313167, "compression_ratio": 1.549738219895288, "no_speech_prob": 1.1658977200568188e-05}, {"id": 921, "seek": 614100, "start": 6150.0, "end": 6156.0, "text": " And you can even add things like weight decay.", "tokens": [400, 291, 393, 754, 909, 721, 411, 3364, 21039, 13], "temperature": 0.0, "avg_logprob": -0.10425180121313167, "compression_ratio": 1.549738219895288, "no_speech_prob": 1.1658977200568188e-05}, {"id": 922, "seek": 614100, "start": 6156.0, "end": 6157.0, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.10425180121313167, "compression_ratio": 1.549738219895288, "no_speech_prob": 1.1658977200568188e-05}, {"id": 923, "seek": 614100, "start": 6157.0, "end": 6161.0, "text": " Because like there's more stuff that's kind of in these things for you.", "tokens": [1436, 411, 456, 311, 544, 1507, 300, 311, 733, 295, 294, 613, 721, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.10425180121313167, "compression_ratio": 1.549738219895288, "no_speech_prob": 1.1658977200568188e-05}, {"id": 924, "seek": 614100, "start": 6161.0, "end": 6166.0, "text": " Right. So that's why we tend to use you know opt in dot blast.", "tokens": [1779, 13, 407, 300, 311, 983, 321, 3928, 281, 764, 291, 458, 2427, 294, 5893, 12035, 13], "temperature": 0.0, "avg_logprob": -0.10425180121313167, "compression_ratio": 1.549738219895288, "no_speech_prob": 1.1658977200568188e-05}, {"id": 925, "seek": 616600, "start": 6166.0, "end": 6172.0, "text": " So behind the scenes this is actually what we do in fast.", "tokens": [407, 2261, 264, 8026, 341, 307, 767, 437, 321, 360, 294, 2370, 13], "temperature": 0.0, "avg_logprob": -0.1762149069044325, "compression_ratio": 1.3726708074534162, "no_speech_prob": 7.410468242596835e-06}, {"id": 926, "seek": 616600, "start": 6172.0, "end": 6180.0, "text": " So if I go up to him dot SGD.", "tokens": [407, 498, 286, 352, 493, 281, 796, 5893, 34520, 35, 13], "temperature": 0.0, "avg_logprob": -0.1762149069044325, "compression_ratio": 1.3726708074534162, "no_speech_prob": 7.410468242596835e-06}, {"id": 927, "seek": 616600, "start": 6180.0, "end": 6181.0, "text": " OK. So there's that.", "tokens": [2264, 13, 407, 456, 311, 300, 13], "temperature": 0.0, "avg_logprob": -0.1762149069044325, "compression_ratio": 1.3726708074534162, "no_speech_prob": 7.410468242596835e-06}, {"id": 928, "seek": 616600, "start": 6181.0, "end": 6184.0, "text": " Right. And so that's that's just that picture.", "tokens": [1779, 13, 400, 370, 300, 311, 300, 311, 445, 300, 3036, 13], "temperature": 0.0, "avg_logprob": -0.1762149069044325, "compression_ratio": 1.3726708074534162, "no_speech_prob": 7.410468242596835e-06}, {"id": 929, "seek": 616600, "start": 6184.0, "end": 6192.0, "text": " But if we change to a different optimizer.", "tokens": [583, 498, 321, 1319, 281, 257, 819, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.1762149069044325, "compression_ratio": 1.3726708074534162, "no_speech_prob": 7.410468242596835e-06}, {"id": 930, "seek": 616600, "start": 6192.0, "end": 6194.0, "text": " So look what happened.", "tokens": [407, 574, 437, 2011, 13], "temperature": 0.0, "avg_logprob": -0.1762149069044325, "compression_ratio": 1.3726708074534162, "no_speech_prob": 7.410468242596835e-06}, {"id": 931, "seek": 619400, "start": 6194.0, "end": 6196.0, "text": " It diverged.", "tokens": [467, 18558, 3004, 13], "temperature": 0.0, "avg_logprob": -0.07097995758056641, "compression_ratio": 1.7311827956989247, "no_speech_prob": 1.0952515367534943e-05}, {"id": 932, "seek": 619400, "start": 6196.0, "end": 6201.0, "text": " We've seen a great picture of that from one of our students who showed what divergence looks like.", "tokens": [492, 600, 1612, 257, 869, 3036, 295, 300, 490, 472, 295, 527, 1731, 567, 4712, 437, 47387, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.07097995758056641, "compression_ratio": 1.7311827956989247, "no_speech_prob": 1.0952515367534943e-05}, {"id": 933, "seek": 619400, "start": 6201.0, "end": 6203.0, "text": " This is what it looks like when you try to train something.", "tokens": [639, 307, 437, 309, 1542, 411, 562, 291, 853, 281, 3847, 746, 13], "temperature": 0.0, "avg_logprob": -0.07097995758056641, "compression_ratio": 1.7311827956989247, "no_speech_prob": 1.0952515367534943e-05}, {"id": 934, "seek": 619400, "start": 6203.0, "end": 6206.0, "text": " So let's use reducing a different optimizer.", "tokens": [407, 718, 311, 764, 12245, 257, 819, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.07097995758056641, "compression_ratio": 1.7311827956989247, "no_speech_prob": 1.0952515367534943e-05}, {"id": 935, "seek": 619400, "start": 6206.0, "end": 6209.0, "text": " So we need a different learning rate.", "tokens": [407, 321, 643, 257, 819, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.07097995758056641, "compression_ratio": 1.7311827956989247, "no_speech_prob": 1.0952515367534943e-05}, {"id": 936, "seek": 619400, "start": 6209.0, "end": 6216.0, "text": " And you can't just continue training because by the time it's diverged the group the weights are like really really big and really really small.", "tokens": [400, 291, 393, 380, 445, 2354, 3097, 570, 538, 264, 565, 309, 311, 18558, 3004, 264, 1594, 264, 17443, 366, 411, 534, 534, 955, 293, 534, 534, 1359, 13], "temperature": 0.0, "avg_logprob": -0.07097995758056641, "compression_ratio": 1.7311827956989247, "no_speech_prob": 1.0952515367534943e-05}, {"id": 937, "seek": 619400, "start": 6216.0, "end": 6217.0, "text": " They're not going to come back.", "tokens": [814, 434, 406, 516, 281, 808, 646, 13], "temperature": 0.0, "avg_logprob": -0.07097995758056641, "compression_ratio": 1.7311827956989247, "no_speech_prob": 1.0952515367534943e-05}, {"id": 938, "seek": 619400, "start": 6217.0, "end": 6221.0, "text": " So start again.", "tokens": [407, 722, 797, 13], "temperature": 0.0, "avg_logprob": -0.07097995758056641, "compression_ratio": 1.7311827956989247, "no_speech_prob": 1.0952515367534943e-05}, {"id": 939, "seek": 619400, "start": 6221.0, "end": 6222.0, "text": " OK. There's a better learning rate.", "tokens": [2264, 13, 821, 311, 257, 1101, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.07097995758056641, "compression_ratio": 1.7311827956989247, "no_speech_prob": 1.0952515367534943e-05}, {"id": 940, "seek": 622200, "start": 6222.0, "end": 6227.0, "text": " But look at this. We're down underneath point five by about a POC 200.", "tokens": [583, 574, 412, 341, 13, 492, 434, 760, 7223, 935, 1732, 538, 466, 257, 22299, 34, 2331, 13], "temperature": 0.0, "avg_logprob": -0.156020450592041, "compression_ratio": 1.3969072164948453, "no_speech_prob": 9.515765668766107e-06}, {"id": 941, "seek": 622200, "start": 6227.0, "end": 6233.0, "text": " Where else before. I'm not even sure we ever got to quite that level.", "tokens": [2305, 1646, 949, 13, 286, 478, 406, 754, 988, 321, 1562, 658, 281, 1596, 300, 1496, 13], "temperature": 0.0, "avg_logprob": -0.156020450592041, "compression_ratio": 1.3969072164948453, "no_speech_prob": 9.515765668766107e-06}, {"id": 942, "seek": 622200, "start": 6233.0, "end": 6237.0, "text": " So what's going on. What's what's Adam.", "tokens": [407, 437, 311, 516, 322, 13, 708, 311, 437, 311, 7938, 13], "temperature": 0.0, "avg_logprob": -0.156020450592041, "compression_ratio": 1.3969072164948453, "no_speech_prob": 9.515765668766107e-06}, {"id": 943, "seek": 622200, "start": 6237.0, "end": 6240.0, "text": " Let me show you.", "tokens": [961, 385, 855, 291, 13], "temperature": 0.0, "avg_logprob": -0.156020450592041, "compression_ratio": 1.3969072164948453, "no_speech_prob": 9.515765668766107e-06}, {"id": 944, "seek": 622200, "start": 6240.0, "end": 6246.0, "text": " And we're going to do gradient descent in Excel because why wouldn't you.", "tokens": [400, 321, 434, 516, 281, 360, 16235, 23475, 294, 19060, 570, 983, 2759, 380, 291, 13], "temperature": 0.0, "avg_logprob": -0.156020450592041, "compression_ratio": 1.3969072164948453, "no_speech_prob": 9.515765668766107e-06}, {"id": 945, "seek": 624600, "start": 6246.0, "end": 6252.0, "text": " OK. So here is some randomly generated data case of X's and some Y's.", "tokens": [2264, 13, 407, 510, 307, 512, 16979, 10833, 1412, 1389, 295, 1783, 311, 293, 512, 398, 311, 13], "temperature": 0.0, "avg_logprob": -0.16880057485480057, "compression_ratio": 1.6702127659574468, "no_speech_prob": 9.665659490565304e-06}, {"id": 946, "seek": 624600, "start": 6252.0, "end": 6262.0, "text": " Well they're actually they're randomly generated X's and the Y's are all calculated by doing X plus B where A is two and B is 30.", "tokens": [1042, 436, 434, 767, 436, 434, 16979, 10833, 1783, 311, 293, 264, 398, 311, 366, 439, 15598, 538, 884, 1783, 1804, 363, 689, 316, 307, 732, 293, 363, 307, 2217, 13], "temperature": 0.0, "avg_logprob": -0.16880057485480057, "compression_ratio": 1.6702127659574468, "no_speech_prob": 9.665659490565304e-06}, {"id": 947, "seek": 624600, "start": 6262.0, "end": 6266.0, "text": " OK. So this is some data that we're going to try and match.", "tokens": [2264, 13, 407, 341, 307, 512, 1412, 300, 321, 434, 516, 281, 853, 293, 2995, 13], "temperature": 0.0, "avg_logprob": -0.16880057485480057, "compression_ratio": 1.6702127659574468, "no_speech_prob": 9.665659490565304e-06}, {"id": 948, "seek": 624600, "start": 6266.0, "end": 6270.0, "text": " And here is SGD.", "tokens": [400, 510, 307, 34520, 35, 13], "temperature": 0.0, "avg_logprob": -0.16880057485480057, "compression_ratio": 1.6702127659574468, "no_speech_prob": 9.665659490565304e-06}, {"id": 949, "seek": 624600, "start": 6270.0, "end": 6272.0, "text": " And so we're going to do it with SGD.", "tokens": [400, 370, 321, 434, 516, 281, 360, 309, 365, 34520, 35, 13], "temperature": 0.0, "avg_logprob": -0.16880057485480057, "compression_ratio": 1.6702127659574468, "no_speech_prob": 9.665659490565304e-06}, {"id": 950, "seek": 627200, "start": 6272.0, "end": 6284.0, "text": " Now in our lesson two SGD notebook we did the whole data set at once as a batch in the notebook we just looked at we did many batches in this spreadsheet.", "tokens": [823, 294, 527, 6898, 732, 34520, 35, 21060, 321, 630, 264, 1379, 1412, 992, 412, 1564, 382, 257, 15245, 294, 264, 21060, 321, 445, 2956, 412, 321, 630, 867, 15245, 279, 294, 341, 27733, 13], "temperature": 0.0, "avg_logprob": -0.1272070272913519, "compression_ratio": 1.6846473029045643, "no_speech_prob": 1.1842337698908523e-05}, {"id": 951, "seek": 627200, "start": 6284.0, "end": 6289.0, "text": " We're going to do online gradient descent which means every single row of data is a batch.", "tokens": [492, 434, 516, 281, 360, 2950, 16235, 23475, 597, 1355, 633, 2167, 5386, 295, 1412, 307, 257, 15245, 13], "temperature": 0.0, "avg_logprob": -0.1272070272913519, "compression_ratio": 1.6846473029045643, "no_speech_prob": 1.1842337698908523e-05}, {"id": 952, "seek": 627200, "start": 6289.0, "end": 6291.0, "text": " It's got a batch size of one.", "tokens": [467, 311, 658, 257, 15245, 2744, 295, 472, 13], "temperature": 0.0, "avg_logprob": -0.1272070272913519, "compression_ratio": 1.6846473029045643, "no_speech_prob": 1.1842337698908523e-05}, {"id": 953, "seek": 627200, "start": 6291.0, "end": 6296.0, "text": " So as per usual we're going to start by picking an intercept and slope kind of arbitrarily.", "tokens": [407, 382, 680, 7713, 321, 434, 516, 281, 722, 538, 8867, 364, 24700, 293, 13525, 733, 295, 19071, 3289, 13], "temperature": 0.0, "avg_logprob": -0.1272070272913519, "compression_ratio": 1.6846473029045643, "no_speech_prob": 1.1842337698908523e-05}, {"id": 954, "seek": 627200, "start": 6296.0, "end": 6299.0, "text": " So I'm just going to pick them at one.", "tokens": [407, 286, 478, 445, 516, 281, 1888, 552, 412, 472, 13], "temperature": 0.0, "avg_logprob": -0.1272070272913519, "compression_ratio": 1.6846473029045643, "no_speech_prob": 1.1842337698908523e-05}, {"id": 955, "seek": 629900, "start": 6299.0, "end": 6303.0, "text": " Doesn't really matter. So here I've copied over the data.", "tokens": [12955, 380, 534, 1871, 13, 407, 510, 286, 600, 25365, 670, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.12636228365318797, "compression_ratio": 1.617283950617284, "no_speech_prob": 4.90820748382248e-05}, {"id": 956, "seek": 629900, "start": 6303.0, "end": 6307.0, "text": " This is my X and Y. And so my intercept and slope as I said is one.", "tokens": [639, 307, 452, 1783, 293, 398, 13, 400, 370, 452, 24700, 293, 13525, 382, 286, 848, 307, 472, 13], "temperature": 0.0, "avg_logprob": -0.12636228365318797, "compression_ratio": 1.617283950617284, "no_speech_prob": 4.90820748382248e-05}, {"id": 957, "seek": 629900, "start": 6307.0, "end": 6311.0, "text": " I'm just literally referring back to this cell here.", "tokens": [286, 478, 445, 3736, 13761, 646, 281, 341, 2815, 510, 13], "temperature": 0.0, "avg_logprob": -0.12636228365318797, "compression_ratio": 1.617283950617284, "no_speech_prob": 4.90820748382248e-05}, {"id": 958, "seek": 629900, "start": 6311.0, "end": 6318.0, "text": " So my prediction for this particular intercept and slope would be 14 times one plus one which is 15.", "tokens": [407, 452, 17630, 337, 341, 1729, 24700, 293, 13525, 576, 312, 3499, 1413, 472, 1804, 472, 597, 307, 2119, 13], "temperature": 0.0, "avg_logprob": -0.12636228365318797, "compression_ratio": 1.617283950617284, "no_speech_prob": 4.90820748382248e-05}, {"id": 959, "seek": 629900, "start": 6318.0, "end": 6322.0, "text": " And so there's my error means this my sum of squareds.", "tokens": [400, 370, 456, 311, 452, 6713, 1355, 341, 452, 2408, 295, 8889, 82, 13], "temperature": 0.0, "avg_logprob": -0.12636228365318797, "compression_ratio": 1.617283950617284, "no_speech_prob": 4.90820748382248e-05}, {"id": 960, "seek": 629900, "start": 6322.0, "end": 6325.0, "text": " Well not even a sum at this point. It's the squared error.", "tokens": [1042, 406, 754, 257, 2408, 412, 341, 935, 13, 467, 311, 264, 8889, 6713, 13], "temperature": 0.0, "avg_logprob": -0.12636228365318797, "compression_ratio": 1.617283950617284, "no_speech_prob": 4.90820748382248e-05}, {"id": 961, "seek": 632500, "start": 6325.0, "end": 6331.0, "text": " OK. So now I need to calculate the gradient so that I can update.", "tokens": [2264, 13, 407, 586, 286, 643, 281, 8873, 264, 16235, 370, 300, 286, 393, 5623, 13], "temperature": 0.0, "avg_logprob": -0.10225914825092662, "compression_ratio": 1.6157635467980296, "no_speech_prob": 4.757252099807374e-05}, {"id": 962, "seek": 632500, "start": 6331.0, "end": 6333.0, "text": " There's two ways you can calculate the gradient.", "tokens": [821, 311, 732, 2098, 291, 393, 8873, 264, 16235, 13], "temperature": 0.0, "avg_logprob": -0.10225914825092662, "compression_ratio": 1.6157635467980296, "no_speech_prob": 4.757252099807374e-05}, {"id": 963, "seek": 632500, "start": 6333.0, "end": 6336.0, "text": " One is analytically.", "tokens": [1485, 307, 10783, 984, 13], "temperature": 0.0, "avg_logprob": -0.10225914825092662, "compression_ratio": 1.6157635467980296, "no_speech_prob": 4.757252099807374e-05}, {"id": 964, "seek": 632500, "start": 6336.0, "end": 6340.0, "text": " And so I you know you can just look them up on Wolfram Alpha or whatever.", "tokens": [400, 370, 286, 291, 458, 291, 393, 445, 574, 552, 493, 322, 16634, 2356, 20588, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.10225914825092662, "compression_ratio": 1.6157635467980296, "no_speech_prob": 4.757252099807374e-05}, {"id": 965, "seek": 632500, "start": 6340.0, "end": 6347.0, "text": " So there's the gradients if you write it out by hand or look it up or you can do something called finite differencing.", "tokens": [407, 456, 311, 264, 2771, 2448, 498, 291, 2464, 309, 484, 538, 1011, 420, 574, 309, 493, 420, 291, 393, 360, 746, 1219, 19362, 743, 13644, 13], "temperature": 0.0, "avg_logprob": -0.10225914825092662, "compression_ratio": 1.6157635467980296, "no_speech_prob": 4.757252099807374e-05}, {"id": 966, "seek": 634700, "start": 6347.0, "end": 6360.0, "text": " Because remember gradients just how far you move in. So how far you how far the outcome moves divided by how far your change was to really small changes.", "tokens": [1436, 1604, 2771, 2448, 445, 577, 1400, 291, 1286, 294, 13, 407, 577, 1400, 291, 577, 1400, 264, 9700, 6067, 6666, 538, 577, 1400, 428, 1319, 390, 281, 534, 1359, 2962, 13], "temperature": 0.0, "avg_logprob": -0.15694383474496695, "compression_ratio": 1.6305732484076434, "no_speech_prob": 1.9524195522535592e-05}, {"id": 967, "seek": 634700, "start": 6360.0, "end": 6366.0, "text": " So let's just make a really small change.", "tokens": [407, 718, 311, 445, 652, 257, 534, 1359, 1319, 13], "temperature": 0.0, "avg_logprob": -0.15694383474496695, "compression_ratio": 1.6305732484076434, "no_speech_prob": 1.9524195522535592e-05}, {"id": 968, "seek": 634700, "start": 6366.0, "end": 6372.0, "text": " So here we've taken our intercept and added point 0 1 to it.", "tokens": [407, 510, 321, 600, 2726, 527, 24700, 293, 3869, 935, 1958, 502, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.15694383474496695, "compression_ratio": 1.6305732484076434, "no_speech_prob": 1.9524195522535592e-05}, {"id": 969, "seek": 637200, "start": 6372.0, "end": 6377.0, "text": " Right. And then calculated our loss.", "tokens": [1779, 13, 400, 550, 15598, 527, 4470, 13], "temperature": 0.0, "avg_logprob": -0.10073197013453433, "compression_ratio": 1.7169811320754718, "no_speech_prob": 1.4509883840219118e-05}, {"id": 970, "seek": 637200, "start": 6377.0, "end": 6382.0, "text": " And you can see that our loss went down a little bit.", "tokens": [400, 291, 393, 536, 300, 527, 4470, 1437, 760, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.10073197013453433, "compression_ratio": 1.7169811320754718, "no_speech_prob": 1.4509883840219118e-05}, {"id": 971, "seek": 637200, "start": 6382.0, "end": 6389.0, "text": " Right. And we added point 0 1 here. So our derivative is that difference divided by that point 0 1.", "tokens": [1779, 13, 400, 321, 3869, 935, 1958, 502, 510, 13, 407, 527, 13760, 307, 300, 2649, 6666, 538, 300, 935, 1958, 502, 13], "temperature": 0.0, "avg_logprob": -0.10073197013453433, "compression_ratio": 1.7169811320754718, "no_speech_prob": 1.4509883840219118e-05}, {"id": 972, "seek": 637200, "start": 6389.0, "end": 6393.0, "text": " And that's called finite differencing. You can always do derivatives of finite differencing.", "tokens": [400, 300, 311, 1219, 19362, 743, 13644, 13, 509, 393, 1009, 360, 33733, 295, 19362, 743, 13644, 13], "temperature": 0.0, "avg_logprob": -0.10073197013453433, "compression_ratio": 1.7169811320754718, "no_speech_prob": 1.4509883840219118e-05}, {"id": 973, "seek": 637200, "start": 6393.0, "end": 6397.0, "text": " It's slow. We don't do it in practice but it's nice for just checking stuff out.", "tokens": [467, 311, 2964, 13, 492, 500, 380, 360, 309, 294, 3124, 457, 309, 311, 1481, 337, 445, 8568, 1507, 484, 13], "temperature": 0.0, "avg_logprob": -0.10073197013453433, "compression_ratio": 1.7169811320754718, "no_speech_prob": 1.4509883840219118e-05}, {"id": 974, "seek": 639700, "start": 6397.0, "end": 6402.0, "text": " So we can do the same thing for a term at point 0 1 to that.", "tokens": [407, 321, 393, 360, 264, 912, 551, 337, 257, 1433, 412, 935, 1958, 502, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.14057545661926268, "compression_ratio": 1.639344262295082, "no_speech_prob": 7.766201633785386e-06}, {"id": 975, "seek": 639700, "start": 6402.0, "end": 6404.0, "text": " Take the difference and divide by point 0 1.", "tokens": [3664, 264, 2649, 293, 9845, 538, 935, 1958, 502, 13], "temperature": 0.0, "avg_logprob": -0.14057545661926268, "compression_ratio": 1.639344262295082, "no_speech_prob": 7.766201633785386e-06}, {"id": 976, "seek": 639700, "start": 6404.0, "end": 6409.0, "text": " Or as I say we can calculate it directly using the actual derivative analytical.", "tokens": [1610, 382, 286, 584, 321, 393, 8873, 309, 3838, 1228, 264, 3539, 13760, 29579, 13], "temperature": 0.0, "avg_logprob": -0.14057545661926268, "compression_ratio": 1.639344262295082, "no_speech_prob": 7.766201633785386e-06}, {"id": 977, "seek": 639700, "start": 6409.0, "end": 6414.0, "text": " And you can see that you know that and that are as you'd expect very similar.", "tokens": [400, 291, 393, 536, 300, 291, 458, 300, 293, 300, 366, 382, 291, 1116, 2066, 588, 2531, 13], "temperature": 0.0, "avg_logprob": -0.14057545661926268, "compression_ratio": 1.639344262295082, "no_speech_prob": 7.766201633785386e-06}, {"id": 978, "seek": 639700, "start": 6414.0, "end": 6418.0, "text": " And that and that are very similar.", "tokens": [400, 300, 293, 300, 366, 588, 2531, 13], "temperature": 0.0, "avg_logprob": -0.14057545661926268, "compression_ratio": 1.639344262295082, "no_speech_prob": 7.766201633785386e-06}, {"id": 979, "seek": 641800, "start": 6418.0, "end": 6429.0, "text": " So gradient descent then just says let's take our current value of that weight and subtract the learning rate times the derivative.", "tokens": [407, 16235, 23475, 550, 445, 1619, 718, 311, 747, 527, 2190, 2158, 295, 300, 3364, 293, 16390, 264, 2539, 3314, 1413, 264, 13760, 13], "temperature": 0.0, "avg_logprob": -0.06977288143054859, "compression_ratio": 1.5783783783783785, "no_speech_prob": 1.2410533599904738e-05}, {"id": 980, "seek": 641800, "start": 6429.0, "end": 6441.0, "text": " There it is. And so now we can copy that intercept and that slope to the next row and do it again.", "tokens": [821, 309, 307, 13, 400, 370, 586, 321, 393, 5055, 300, 24700, 293, 300, 13525, 281, 264, 958, 5386, 293, 360, 309, 797, 13], "temperature": 0.0, "avg_logprob": -0.06977288143054859, "compression_ratio": 1.5783783783783785, "no_speech_prob": 1.2410533599904738e-05}, {"id": 981, "seek": 641800, "start": 6441.0, "end": 6445.0, "text": " And do it lots of times. And at the end we've done one epoch.", "tokens": [400, 360, 309, 3195, 295, 1413, 13, 400, 412, 264, 917, 321, 600, 1096, 472, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.06977288143054859, "compression_ratio": 1.5783783783783785, "no_speech_prob": 1.2410533599904738e-05}, {"id": 982, "seek": 644500, "start": 6445.0, "end": 6449.0, "text": " So at the end of that epoch we could say oh great.", "tokens": [407, 412, 264, 917, 295, 300, 30992, 339, 321, 727, 584, 1954, 869, 13], "temperature": 0.0, "avg_logprob": -0.12118210111345563, "compression_ratio": 1.644578313253012, "no_speech_prob": 1.6963402231340297e-05}, {"id": 983, "seek": 644500, "start": 6449.0, "end": 6457.0, "text": " So this is our slope. So let's copy that over to where it says slope.", "tokens": [407, 341, 307, 527, 13525, 13, 407, 718, 311, 5055, 300, 670, 281, 689, 309, 1619, 13525, 13], "temperature": 0.0, "avg_logprob": -0.12118210111345563, "compression_ratio": 1.644578313253012, "no_speech_prob": 1.6963402231340297e-05}, {"id": 984, "seek": 644500, "start": 6457.0, "end": 6465.0, "text": " And this is our intercept. So a copy it where it says intercept.", "tokens": [400, 341, 307, 527, 24700, 13, 407, 257, 5055, 309, 689, 309, 1619, 24700, 13], "temperature": 0.0, "avg_logprob": -0.12118210111345563, "compression_ratio": 1.644578313253012, "no_speech_prob": 1.6963402231340297e-05}, {"id": 985, "seek": 644500, "start": 6465.0, "end": 6469.0, "text": " And now it's done another epoch. OK.", "tokens": [400, 586, 309, 311, 1096, 1071, 30992, 339, 13, 2264, 13], "temperature": 0.0, "avg_logprob": -0.12118210111345563, "compression_ratio": 1.644578313253012, "no_speech_prob": 1.6963402231340297e-05}, {"id": 986, "seek": 644500, "start": 6469.0, "end": 6473.0, "text": " So that's kind of boring. I'm copying and pasting.", "tokens": [407, 300, 311, 733, 295, 9989, 13, 286, 478, 27976, 293, 1791, 278, 13], "temperature": 0.0, "avg_logprob": -0.12118210111345563, "compression_ratio": 1.644578313253012, "no_speech_prob": 1.6963402231340297e-05}, {"id": 987, "seek": 647300, "start": 6473.0, "end": 6482.0, "text": " So I created a very sophisticated macro which copies and pastes for you.", "tokens": [407, 286, 2942, 257, 588, 16950, 18887, 597, 14341, 293, 1791, 279, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.06798358850700911, "compression_ratio": 1.7461928934010151, "no_speech_prob": 2.7967238565906882e-05}, {"id": 988, "seek": 647300, "start": 6482.0, "end": 6485.0, "text": " And so I just recorded it basically.", "tokens": [400, 370, 286, 445, 8287, 309, 1936, 13], "temperature": 0.0, "avg_logprob": -0.06798358850700911, "compression_ratio": 1.7461928934010151, "no_speech_prob": 2.7967238565906882e-05}, {"id": 989, "seek": 647300, "start": 6485.0, "end": 6490.0, "text": " And so and then I created a very sophisticated for loop that goes through and does it five times.", "tokens": [400, 370, 293, 550, 286, 2942, 257, 588, 16950, 337, 6367, 300, 1709, 807, 293, 775, 309, 1732, 1413, 13], "temperature": 0.0, "avg_logprob": -0.06798358850700911, "compression_ratio": 1.7461928934010151, "no_speech_prob": 2.7967238565906882e-05}, {"id": 990, "seek": 647300, "start": 6490.0, "end": 6498.0, "text": " And I attached that to the run button. So if I press run it'll go ahead and do it five times and just keep track of the error each time.", "tokens": [400, 286, 8570, 300, 281, 264, 1190, 2960, 13, 407, 498, 286, 1886, 1190, 309, 603, 352, 2286, 293, 360, 309, 1732, 1413, 293, 445, 1066, 2837, 295, 264, 6713, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.06798358850700911, "compression_ratio": 1.7461928934010151, "no_speech_prob": 2.7967238565906882e-05}, {"id": 991, "seek": 649800, "start": 6498.0, "end": 6507.0, "text": " OK. So that is SGD. And as you can see it is just infuriatingly slow.", "tokens": [2264, 13, 407, 300, 307, 34520, 35, 13, 400, 382, 291, 393, 536, 309, 307, 445, 1536, 9744, 990, 356, 2964, 13], "temperature": 0.0, "avg_logprob": -0.11055374145507812, "compression_ratio": 1.494949494949495, "no_speech_prob": 8.139418241626117e-06}, {"id": 992, "seek": 649800, "start": 6507.0, "end": 6514.0, "text": " Like particularly the intercept is meant to be 30.", "tokens": [1743, 4098, 264, 24700, 307, 4140, 281, 312, 2217, 13], "temperature": 0.0, "avg_logprob": -0.11055374145507812, "compression_ratio": 1.494949494949495, "no_speech_prob": 8.139418241626117e-06}, {"id": 993, "seek": 649800, "start": 6514.0, "end": 6521.0, "text": " And we're still only up to 1.57 and like just it's just going so slowly.", "tokens": [400, 321, 434, 920, 787, 493, 281, 502, 13, 19004, 293, 411, 445, 309, 311, 445, 516, 370, 5692, 13], "temperature": 0.0, "avg_logprob": -0.11055374145507812, "compression_ratio": 1.494949494949495, "no_speech_prob": 8.139418241626117e-06}, {"id": 994, "seek": 649800, "start": 6521.0, "end": 6526.0, "text": " So let's speed it up. So the first thing we can do to speed it up is to use something called momentum.", "tokens": [407, 718, 311, 3073, 309, 493, 13, 407, 264, 700, 551, 321, 393, 360, 281, 3073, 309, 493, 307, 281, 764, 746, 1219, 11244, 13], "temperature": 0.0, "avg_logprob": -0.11055374145507812, "compression_ratio": 1.494949494949495, "no_speech_prob": 8.139418241626117e-06}, {"id": 995, "seek": 652600, "start": 6526.0, "end": 6531.0, "text": " So here's the exact same spreadsheet as the last worksheet.", "tokens": [407, 510, 311, 264, 1900, 912, 27733, 382, 264, 1036, 49890, 13], "temperature": 0.0, "avg_logprob": -0.06223074106069711, "compression_ratio": 1.6079545454545454, "no_speech_prob": 9.079840310732834e-06}, {"id": 996, "seek": 652600, "start": 6531.0, "end": 6536.0, "text": " I've removed the finite differencing version of the derivatives because they're not that useful.", "tokens": [286, 600, 7261, 264, 19362, 743, 13644, 3037, 295, 264, 33733, 570, 436, 434, 406, 300, 4420, 13], "temperature": 0.0, "avg_logprob": -0.06223074106069711, "compression_ratio": 1.6079545454545454, "no_speech_prob": 9.079840310732834e-06}, {"id": 997, "seek": 652600, "start": 6536.0, "end": 6549.0, "text": " Just the analytical ones here. And here's the thing where I take the the derivative and I'm going to update by the derivative.", "tokens": [1449, 264, 29579, 2306, 510, 13, 400, 510, 311, 264, 551, 689, 286, 747, 264, 264, 13760, 293, 286, 478, 516, 281, 5623, 538, 264, 13760, 13], "temperature": 0.0, "avg_logprob": -0.06223074106069711, "compression_ratio": 1.6079545454545454, "no_speech_prob": 9.079840310732834e-06}, {"id": 998, "seek": 654900, "start": 6549.0, "end": 6560.0, "text": " But what I do it's kind of more interesting to look at this one is I take the derivative and I multiply it by point one.", "tokens": [583, 437, 286, 360, 309, 311, 733, 295, 544, 1880, 281, 574, 412, 341, 472, 307, 286, 747, 264, 13760, 293, 286, 12972, 309, 538, 935, 472, 13], "temperature": 0.0, "avg_logprob": -0.0568086213843767, "compression_ratio": 1.7947368421052632, "no_speech_prob": 3.966836175095523e-06}, {"id": 999, "seek": 654900, "start": 6560.0, "end": 6567.0, "text": " And what I do is I look at the previous update and I multiply that by point nine and I add the two together.", "tokens": [400, 437, 286, 360, 307, 286, 574, 412, 264, 3894, 5623, 293, 286, 12972, 300, 538, 935, 4949, 293, 286, 909, 264, 732, 1214, 13], "temperature": 0.0, "avg_logprob": -0.0568086213843767, "compression_ratio": 1.7947368421052632, "no_speech_prob": 3.966836175095523e-06}, {"id": 1000, "seek": 654900, "start": 6567.0, "end": 6577.0, "text": " So in other words the update that I do is not just based on the derivative but a tenth of it is the derivative.", "tokens": [407, 294, 661, 2283, 264, 5623, 300, 286, 360, 307, 406, 445, 2361, 322, 264, 13760, 457, 257, 27269, 295, 309, 307, 264, 13760, 13], "temperature": 0.0, "avg_logprob": -0.0568086213843767, "compression_ratio": 1.7947368421052632, "no_speech_prob": 3.966836175095523e-06}, {"id": 1001, "seek": 657700, "start": 6577.0, "end": 6583.0, "text": " And 90 percent of it is just the same direction I went last time.", "tokens": [400, 4289, 3043, 295, 309, 307, 445, 264, 912, 3513, 286, 1437, 1036, 565, 13], "temperature": 0.0, "avg_logprob": -0.09858780588422503, "compression_ratio": 1.5053191489361701, "no_speech_prob": 1.6700843843864277e-05}, {"id": 1002, "seek": 657700, "start": 6583.0, "end": 6598.0, "text": " And this is called momentum. Right. What it means is remember how we kind of thought about what might happen if you're trying to find the minimum of this.", "tokens": [400, 341, 307, 1219, 11244, 13, 1779, 13, 708, 309, 1355, 307, 1604, 577, 321, 733, 295, 1194, 466, 437, 1062, 1051, 498, 291, 434, 1382, 281, 915, 264, 7285, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.09858780588422503, "compression_ratio": 1.5053191489361701, "no_speech_prob": 1.6700843843864277e-05}, {"id": 1003, "seek": 657700, "start": 6598.0, "end": 6602.0, "text": " And you were here and your learning rate was too small. Right.", "tokens": [400, 291, 645, 510, 293, 428, 2539, 3314, 390, 886, 1359, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.09858780588422503, "compression_ratio": 1.5053191489361701, "no_speech_prob": 1.6700843843864277e-05}, {"id": 1004, "seek": 660200, "start": 6602.0, "end": 6613.0, "text": " And you just keep doing the same steps or if you keep doing the same steps then if you also add in the step you took last time", "tokens": [400, 291, 445, 1066, 884, 264, 912, 4439, 420, 498, 291, 1066, 884, 264, 912, 4439, 550, 498, 291, 611, 909, 294, 264, 1823, 291, 1890, 1036, 565], "temperature": 0.0, "avg_logprob": -0.09754415611168007, "compression_ratio": 1.7354497354497354, "no_speech_prob": 1.3006246263103094e-05}, {"id": 1005, "seek": 660200, "start": 6613.0, "end": 6617.0, "text": " then your steps are going to get bigger and bigger aren't they. OK.", "tokens": [550, 428, 4439, 366, 516, 281, 483, 3801, 293, 3801, 3212, 380, 436, 13, 2264, 13], "temperature": 0.0, "avg_logprob": -0.09754415611168007, "compression_ratio": 1.7354497354497354, "no_speech_prob": 1.3006246263103094e-05}, {"id": 1006, "seek": 660200, "start": 6617.0, "end": 6626.0, "text": " Until eventually they go too far. But now of course your gradient is pointing the other direction to where your momentum is pointing.", "tokens": [9088, 4728, 436, 352, 886, 1400, 13, 583, 586, 295, 1164, 428, 16235, 307, 12166, 264, 661, 3513, 281, 689, 428, 11244, 307, 12166, 13], "temperature": 0.0, "avg_logprob": -0.09754415611168007, "compression_ratio": 1.7354497354497354, "no_speech_prob": 1.3006246263103094e-05}, {"id": 1007, "seek": 662600, "start": 6626.0, "end": 6635.0, "text": " So you might just take a little step over here and then you start going small steps bigger steps bigger steps all steps bigger steps like that.", "tokens": [407, 291, 1062, 445, 747, 257, 707, 1823, 670, 510, 293, 550, 291, 722, 516, 1359, 4439, 3801, 4439, 3801, 4439, 439, 4439, 3801, 4439, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.1247416118095661, "compression_ratio": 1.6714285714285715, "no_speech_prob": 1.3845242392562795e-05}, {"id": 1008, "seek": 662600, "start": 6635.0, "end": 6648.0, "text": " Right. So that's kind of what momentum does. Or if you're if you're kind of going too far.", "tokens": [1779, 13, 407, 300, 311, 733, 295, 437, 11244, 775, 13, 1610, 498, 291, 434, 498, 291, 434, 733, 295, 516, 886, 1400, 13], "temperature": 0.0, "avg_logprob": -0.1247416118095661, "compression_ratio": 1.6714285714285715, "no_speech_prob": 1.3845242392562795e-05}, {"id": 1009, "seek": 664800, "start": 6648.0, "end": 6660.0, "text": " Like this which is also slow. Right. Then the average of your last few steps is actually somewhere.", "tokens": [1743, 341, 597, 307, 611, 2964, 13, 1779, 13, 1396, 264, 4274, 295, 428, 1036, 1326, 4439, 307, 767, 4079, 13], "temperature": 0.0, "avg_logprob": -0.10600072145462036, "compression_ratio": 1.3953488372093024, "no_speech_prob": 8.530108971172012e-06}, {"id": 1010, "seek": 664800, "start": 6660.0, "end": 6667.0, "text": " Kind of between the two isn't it. Right. So this is a really common idea. Right.", "tokens": [9242, 295, 1296, 264, 732, 1943, 380, 309, 13, 1779, 13, 407, 341, 307, 257, 534, 2689, 1558, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.10600072145462036, "compression_ratio": 1.3953488372093024, "no_speech_prob": 8.530108971172012e-06}, {"id": 1011, "seek": 666700, "start": 6667.0, "end": 6682.0, "text": " It's like when you have something that says kind of my what is in this case it's like my step my step at time T equals some number.", "tokens": [467, 311, 411, 562, 291, 362, 746, 300, 1619, 733, 295, 452, 437, 307, 294, 341, 1389, 309, 311, 411, 452, 1823, 452, 1823, 412, 565, 314, 6915, 512, 1230, 13], "temperature": 0.0, "avg_logprob": -0.13068923354148865, "compression_ratio": 1.5535714285714286, "no_speech_prob": 1.0129242582479492e-05}, {"id": 1012, "seek": 666700, "start": 6682.0, "end": 6693.0, "text": " People often use alpha because like I say you've got to love these Greek letters some number times the actual thing I want to do.", "tokens": [3432, 2049, 764, 8961, 570, 411, 286, 584, 291, 600, 658, 281, 959, 613, 10281, 7825, 512, 1230, 1413, 264, 3539, 551, 286, 528, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.13068923354148865, "compression_ratio": 1.5535714285714286, "no_speech_prob": 1.0129242582479492e-05}, {"id": 1013, "seek": 669300, "start": 6693.0, "end": 6710.0, "text": " Right. So in this case it's like the gradient right plus one minus alpha times whatever you had last time as T minus one.", "tokens": [1779, 13, 407, 294, 341, 1389, 309, 311, 411, 264, 16235, 558, 1804, 472, 3175, 8961, 1413, 2035, 291, 632, 1036, 565, 382, 314, 3175, 472, 13], "temperature": 0.0, "avg_logprob": -0.102229815810474, "compression_ratio": 1.5604395604395604, "no_speech_prob": 1.0952732736768667e-05}, {"id": 1014, "seek": 669300, "start": 6710.0, "end": 6716.0, "text": " This thing here is called an exponentially weighted moving average.", "tokens": [639, 551, 510, 307, 1219, 364, 37330, 32807, 2684, 4274, 13], "temperature": 0.0, "avg_logprob": -0.102229815810474, "compression_ratio": 1.5604395604395604, "no_speech_prob": 1.0952732736768667e-05}, {"id": 1015, "seek": 669300, "start": 6716.0, "end": 6721.0, "text": " And the reason why is that if you think about it these one minus alphas are going to multiply.", "tokens": [400, 264, 1778, 983, 307, 300, 498, 291, 519, 466, 309, 613, 472, 3175, 419, 7485, 366, 516, 281, 12972, 13], "temperature": 0.0, "avg_logprob": -0.102229815810474, "compression_ratio": 1.5604395604395604, "no_speech_prob": 1.0952732736768667e-05}, {"id": 1016, "seek": 672100, "start": 6721.0, "end": 6730.0, "text": " So s t minus two is in here with a kind of a one minus alpha squared and s t minus three is in there with a one minus alpha cubed.", "tokens": [407, 262, 256, 3175, 732, 307, 294, 510, 365, 257, 733, 295, 257, 472, 3175, 8961, 8889, 293, 262, 256, 3175, 1045, 307, 294, 456, 365, 257, 472, 3175, 8961, 36510, 13], "temperature": 0.0, "avg_logprob": -0.09885720803704061, "compression_ratio": 1.7094972067039107, "no_speech_prob": 4.425401129992679e-06}, {"id": 1017, "seek": 672100, "start": 6730.0, "end": 6745.0, "text": " So in other words this ends up being the actual thing I want plus a weighted average of the last few time periods where the most recent ones are exponentially higher weighted.", "tokens": [407, 294, 661, 2283, 341, 5314, 493, 885, 264, 3539, 551, 286, 528, 1804, 257, 32807, 4274, 295, 264, 1036, 1326, 565, 13804, 689, 264, 881, 5162, 2306, 366, 37330, 2946, 32807, 13], "temperature": 0.0, "avg_logprob": -0.09885720803704061, "compression_ratio": 1.7094972067039107, "no_speech_prob": 4.425401129992679e-06}, {"id": 1018, "seek": 674500, "start": 6745.0, "end": 6751.0, "text": " OK. And this is going to keep popping up again and again. Right. So that's what momentum is.", "tokens": [2264, 13, 400, 341, 307, 516, 281, 1066, 18374, 493, 797, 293, 797, 13, 1779, 13, 407, 300, 311, 437, 11244, 307, 13], "temperature": 0.0, "avg_logprob": -0.08964131860172048, "compression_ratio": 1.4640883977900552, "no_speech_prob": 9.97270308289444e-06}, {"id": 1019, "seek": 674500, "start": 6751.0, "end": 6761.0, "text": " It says I want to go based on the current gradient plus the exponentially weighted moving average of my last few steps.", "tokens": [467, 1619, 286, 528, 281, 352, 2361, 322, 264, 2190, 16235, 1804, 264, 37330, 32807, 2684, 4274, 295, 452, 1036, 1326, 4439, 13], "temperature": 0.0, "avg_logprob": -0.08964131860172048, "compression_ratio": 1.4640883977900552, "no_speech_prob": 9.97270308289444e-06}, {"id": 1020, "seek": 674500, "start": 6761.0, "end": 6765.0, "text": " So that's useful. That's called s g d with momentum.", "tokens": [407, 300, 311, 4420, 13, 663, 311, 1219, 262, 290, 274, 365, 11244, 13], "temperature": 0.0, "avg_logprob": -0.08964131860172048, "compression_ratio": 1.4640883977900552, "no_speech_prob": 9.97270308289444e-06}, {"id": 1021, "seek": 676500, "start": 6765.0, "end": 6775.0, "text": " And we can do it by changing this here to saying s g d momentum and momentum point nine is really common.", "tokens": [400, 321, 393, 360, 309, 538, 4473, 341, 510, 281, 1566, 262, 290, 274, 11244, 293, 11244, 935, 4949, 307, 534, 2689, 13], "temperature": 0.0, "avg_logprob": -0.1704163614908854, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.9832432371913455e-05}, {"id": 1022, "seek": 676500, "start": 6775.0, "end": 6781.0, "text": " It's like it's so common it's always point nine just about for basic stuff.", "tokens": [467, 311, 411, 309, 311, 370, 2689, 309, 311, 1009, 935, 4949, 445, 466, 337, 3875, 1507, 13], "temperature": 0.0, "avg_logprob": -0.1704163614908854, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.9832432371913455e-05}, {"id": 1023, "seek": 676500, "start": 6781.0, "end": 6788.0, "text": " So that's how you do s g d with momentum. And again it's not I didn't show you some simplified version.", "tokens": [407, 300, 311, 577, 291, 360, 262, 290, 274, 365, 11244, 13, 400, 797, 309, 311, 406, 286, 994, 380, 855, 291, 512, 26335, 3037, 13], "temperature": 0.0, "avg_logprob": -0.1704163614908854, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.9832432371913455e-05}, {"id": 1024, "seek": 678800, "start": 6788.0, "end": 6795.0, "text": " I showed you the version that is that is s g d. OK. That's that's again you can write your own try it out.", "tokens": [286, 4712, 291, 264, 3037, 300, 307, 300, 307, 262, 290, 274, 13, 2264, 13, 663, 311, 300, 311, 797, 291, 393, 2464, 428, 1065, 853, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.15056684647483387, "compression_ratio": 1.621212121212121, "no_speech_prob": 3.555908051566803e-06}, {"id": 1025, "seek": 678800, "start": 6795.0, "end": 6805.0, "text": " That would be a great assignment would be to take less than two s g d and add momentum to it or even the new notebook we've got for MNIST.", "tokens": [663, 576, 312, 257, 869, 15187, 576, 312, 281, 747, 1570, 813, 732, 262, 290, 274, 293, 909, 11244, 281, 309, 420, 754, 264, 777, 21060, 321, 600, 658, 337, 376, 45, 19756, 13], "temperature": 0.0, "avg_logprob": -0.15056684647483387, "compression_ratio": 1.621212121212121, "no_speech_prob": 3.555908051566803e-06}, {"id": 1026, "seek": 678800, "start": 6805.0, "end": 6810.0, "text": " Get rid of the opt in dot and write your own update function with momentum.", "tokens": [3240, 3973, 295, 264, 2427, 294, 5893, 293, 2464, 428, 1065, 5623, 2445, 365, 11244, 13], "temperature": 0.0, "avg_logprob": -0.15056684647483387, "compression_ratio": 1.621212121212121, "no_speech_prob": 3.555908051566803e-06}, {"id": 1027, "seek": 681000, "start": 6810.0, "end": 6819.0, "text": " Then there's a cool thing called RMS prop one of the really cool things about RMS prop is that Jeffrey Hinton created it.", "tokens": [1396, 456, 311, 257, 1627, 551, 1219, 497, 10288, 2365, 472, 295, 264, 534, 1627, 721, 466, 497, 10288, 2365, 307, 300, 28721, 389, 12442, 2942, 309, 13], "temperature": 0.0, "avg_logprob": -0.1348515845633842, "compression_ratio": 1.538888888888889, "no_speech_prob": 1.4738151548954193e-05}, {"id": 1028, "seek": 681000, "start": 6819.0, "end": 6824.0, "text": " Famous neural net guy. Everybody uses it. It's like really popular.", "tokens": [7342, 563, 18161, 2533, 2146, 13, 7646, 4960, 309, 13, 467, 311, 411, 534, 3743, 13], "temperature": 0.0, "avg_logprob": -0.1348515845633842, "compression_ratio": 1.538888888888889, "no_speech_prob": 1.4738151548954193e-05}, {"id": 1029, "seek": 681000, "start": 6824.0, "end": 6832.0, "text": " It's really common. The correct citation for RMS prop is the Coursera online free MOOC.", "tokens": [467, 311, 534, 2689, 13, 440, 3006, 45590, 337, 497, 10288, 2365, 307, 264, 383, 5067, 1663, 2950, 1737, 49197, 34, 13], "temperature": 0.0, "avg_logprob": -0.1348515845633842, "compression_ratio": 1.538888888888889, "no_speech_prob": 1.4738151548954193e-05}, {"id": 1030, "seek": 683200, "start": 6832.0, "end": 6843.0, "text": " That that's where he first mentioned RMS prop. So I love this thing that like you know cool new things appear in MOOC said not a paper.", "tokens": [663, 300, 311, 689, 415, 700, 2835, 497, 10288, 2365, 13, 407, 286, 959, 341, 551, 300, 411, 291, 458, 1627, 777, 721, 4204, 294, 49197, 34, 848, 406, 257, 3035, 13], "temperature": 0.0, "avg_logprob": -0.1499751961749533, "compression_ratio": 1.4517766497461928, "no_speech_prob": 5.255227279121755e-06}, {"id": 1031, "seek": 683200, "start": 6843.0, "end": 6857.0, "text": " So RMS prop is very similar to momentum. But this time we have an exponentially weighted moving average not of the gradient updates but of F8 squared.", "tokens": [407, 497, 10288, 2365, 307, 588, 2531, 281, 11244, 13, 583, 341, 565, 321, 362, 364, 37330, 32807, 2684, 4274, 406, 295, 264, 16235, 9205, 457, 295, 479, 23, 8889, 13], "temperature": 0.0, "avg_logprob": -0.1499751961749533, "compression_ratio": 1.4517766497461928, "no_speech_prob": 5.255227279121755e-06}, {"id": 1032, "seek": 685700, "start": 6857.0, "end": 6868.0, "text": " That's the gradient squared. So what the gradient squared times point one plus the previous value times point nine.", "tokens": [663, 311, 264, 16235, 8889, 13, 407, 437, 264, 16235, 8889, 1413, 935, 472, 1804, 264, 3894, 2158, 1413, 935, 4949, 13], "temperature": 0.0, "avg_logprob": -0.07644439387965847, "compression_ratio": 1.9217877094972067, "no_speech_prob": 8.397929377679247e-06}, {"id": 1033, "seek": 685700, "start": 6868.0, "end": 6872.0, "text": " So it's an exponentially this is an exponentially weighted moving average of the gradient squared.", "tokens": [407, 309, 311, 364, 37330, 341, 307, 364, 37330, 32807, 2684, 4274, 295, 264, 16235, 8889, 13], "temperature": 0.0, "avg_logprob": -0.07644439387965847, "compression_ratio": 1.9217877094972067, "no_speech_prob": 8.397929377679247e-06}, {"id": 1034, "seek": 685700, "start": 6872.0, "end": 6882.0, "text": " So what's this number going to mean. Well if my gradients really small and consistently really small this will be a small number.", "tokens": [407, 437, 311, 341, 1230, 516, 281, 914, 13, 1042, 498, 452, 2771, 2448, 534, 1359, 293, 14961, 534, 1359, 341, 486, 312, 257, 1359, 1230, 13], "temperature": 0.0, "avg_logprob": -0.07644439387965847, "compression_ratio": 1.9217877094972067, "no_speech_prob": 8.397929377679247e-06}, {"id": 1035, "seek": 688200, "start": 6882.0, "end": 6890.0, "text": " If my gradient is highly volatile it's going to be a big number or if it's just really big all the time it'll be a big number.", "tokens": [759, 452, 16235, 307, 5405, 34377, 309, 311, 516, 281, 312, 257, 955, 1230, 420, 498, 309, 311, 445, 534, 955, 439, 264, 565, 309, 603, 312, 257, 955, 1230, 13], "temperature": 0.0, "avg_logprob": -0.08693785453910258, "compression_ratio": 1.5555555555555556, "no_speech_prob": 4.936874120176071e-06}, {"id": 1036, "seek": 688200, "start": 6890.0, "end": 6910.0, "text": " And why is that interesting. Because when we do our update this time we say weight minus learning rate times gradient divided by the square root of this.", "tokens": [400, 983, 307, 300, 1880, 13, 1436, 562, 321, 360, 527, 5623, 341, 565, 321, 584, 3364, 3175, 2539, 3314, 1413, 16235, 6666, 538, 264, 3732, 5593, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.08693785453910258, "compression_ratio": 1.5555555555555556, "no_speech_prob": 4.936874120176071e-06}, {"id": 1037, "seek": 691000, "start": 6910.0, "end": 6917.0, "text": " In other words if our gradients consistently very small and not volatile let's take bigger jumps.", "tokens": [682, 661, 2283, 498, 527, 2771, 2448, 14961, 588, 1359, 293, 406, 34377, 718, 311, 747, 3801, 16704, 13], "temperature": 0.0, "avg_logprob": -0.13751121189283289, "compression_ratio": 1.436842105263158, "no_speech_prob": 1.1842488675029017e-05}, {"id": 1038, "seek": 691000, "start": 6917.0, "end": 6923.0, "text": " And that's kind of what we want right. When we watched how the intercept moves so damn slowly.", "tokens": [400, 300, 311, 733, 295, 437, 321, 528, 558, 13, 1133, 321, 6337, 577, 264, 24700, 6067, 370, 8151, 5692, 13], "temperature": 0.0, "avg_logprob": -0.13751121189283289, "compression_ratio": 1.436842105263158, "no_speech_prob": 1.1842488675029017e-05}, {"id": 1039, "seek": 691000, "start": 6923.0, "end": 6931.0, "text": " But it's like obviously you need to just try to go faster. So if I now run this.", "tokens": [583, 309, 311, 411, 2745, 291, 643, 281, 445, 853, 281, 352, 4663, 13, 407, 498, 286, 586, 1190, 341, 13], "temperature": 0.0, "avg_logprob": -0.13751121189283289, "compression_ratio": 1.436842105263158, "no_speech_prob": 1.1842488675029017e-05}, {"id": 1040, "seek": 693100, "start": 6931.0, "end": 6941.0, "text": " After just five epochs this is already up to three. Right. Where else with the basic version after five epochs.", "tokens": [2381, 445, 1732, 30992, 28346, 341, 307, 1217, 493, 281, 1045, 13, 1779, 13, 2305, 1646, 365, 264, 3875, 3037, 934, 1732, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.10982401747452586, "compression_ratio": 1.4782608695652173, "no_speech_prob": 9.08009496924933e-06}, {"id": 1041, "seek": 693100, "start": 6941.0, "end": 6954.0, "text": " It's still at one point two seven. And remember we have to get to 30. So the obvious thing to do and by obvious I mean only a couple of years ago did anybody actually figure this out is do both.", "tokens": [467, 311, 920, 412, 472, 935, 732, 3407, 13, 400, 1604, 321, 362, 281, 483, 281, 2217, 13, 407, 264, 6322, 551, 281, 360, 293, 538, 6322, 286, 914, 787, 257, 1916, 295, 924, 2057, 630, 4472, 767, 2573, 341, 484, 307, 360, 1293, 13], "temperature": 0.0, "avg_logprob": -0.10982401747452586, "compression_ratio": 1.4782608695652173, "no_speech_prob": 9.08009496924933e-06}, {"id": 1042, "seek": 695400, "start": 6954.0, "end": 6964.0, "text": " Right. So that's called Adam. So Adam is simply keep track of the exponentially weighted moving average of the gradient squared.", "tokens": [1779, 13, 407, 300, 311, 1219, 7938, 13, 407, 7938, 307, 2935, 1066, 2837, 295, 264, 37330, 32807, 2684, 4274, 295, 264, 16235, 8889, 13], "temperature": 0.0, "avg_logprob": -0.0837930969045132, "compression_ratio": 2.010869565217391, "no_speech_prob": 2.2124548195279203e-05}, {"id": 1043, "seek": 695400, "start": 6964.0, "end": 6970.0, "text": " And also keep track of the exponentially weighted moving average of my steps. Right.", "tokens": [400, 611, 1066, 2837, 295, 264, 37330, 32807, 2684, 4274, 295, 452, 4439, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.0837930969045132, "compression_ratio": 2.010869565217391, "no_speech_prob": 2.2124548195279203e-05}, {"id": 1044, "seek": 695400, "start": 6970.0, "end": 6983.0, "text": " And both divide by the exponentially weighted moving average of the squared terms and you know take point nine of a step in the same direction as last time.", "tokens": [400, 1293, 9845, 538, 264, 37330, 32807, 2684, 4274, 295, 264, 8889, 2115, 293, 291, 458, 747, 935, 4949, 295, 257, 1823, 294, 264, 912, 3513, 382, 1036, 565, 13], "temperature": 0.0, "avg_logprob": -0.0837930969045132, "compression_ratio": 2.010869565217391, "no_speech_prob": 2.2124548195279203e-05}, {"id": 1045, "seek": 698300, "start": 6983.0, "end": 6994.0, "text": " So it's it's momentum and I miss prop. That's called Adam. And look at this.", "tokens": [407, 309, 311, 309, 311, 11244, 293, 286, 1713, 2365, 13, 663, 311, 1219, 7938, 13, 400, 574, 412, 341, 13], "temperature": 0.0, "avg_logprob": -0.17263906213301647, "compression_ratio": 1.5151515151515151, "no_speech_prob": 3.785256240007584e-06}, {"id": 1046, "seek": 698300, "start": 6994.0, "end": 7003.0, "text": " OK. Five steps for 25. OK. So you know these these are these optimizers people call them dynamic learning rates.", "tokens": [2264, 13, 9436, 4439, 337, 3552, 13, 2264, 13, 407, 291, 458, 613, 613, 366, 613, 5028, 22525, 561, 818, 552, 8546, 2539, 6846, 13], "temperature": 0.0, "avg_logprob": -0.17263906213301647, "compression_ratio": 1.5151515151515151, "no_speech_prob": 3.785256240007584e-06}, {"id": 1047, "seek": 698300, "start": 7003.0, "end": 7011.0, "text": " A lot of people have the misunderstanding that you don't have to set a learning rate. Of course you do. Right.", "tokens": [316, 688, 295, 561, 362, 264, 29227, 300, 291, 500, 380, 362, 281, 992, 257, 2539, 3314, 13, 2720, 1164, 291, 360, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.17263906213301647, "compression_ratio": 1.5151515151515151, "no_speech_prob": 3.785256240007584e-06}, {"id": 1048, "seek": 701100, "start": 7011.0, "end": 7020.0, "text": " It's just like trying to identify parameters that need to move faster you know or consistently go in the same direction.", "tokens": [467, 311, 445, 411, 1382, 281, 5876, 9834, 300, 643, 281, 1286, 4663, 291, 458, 420, 14961, 352, 294, 264, 912, 3513, 13], "temperature": 0.0, "avg_logprob": -0.08038307250814235, "compression_ratio": 1.4452554744525548, "no_speech_prob": 4.565762992569944e-06}, {"id": 1049, "seek": 701100, "start": 7020.0, "end": 7025.0, "text": " It doesn't mean you don't need learning rates. We still have a learning rate.", "tokens": [467, 1177, 380, 914, 291, 500, 380, 643, 2539, 6846, 13, 492, 920, 362, 257, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.08038307250814235, "compression_ratio": 1.4452554744525548, "no_speech_prob": 4.565762992569944e-06}, {"id": 1050, "seek": 702500, "start": 7025.0, "end": 7041.0, "text": " OK. And in fact you know if I run this again currently my my error. Now just do it again. So we're trying to get to 30 comma two.", "tokens": [2264, 13, 400, 294, 1186, 291, 458, 498, 286, 1190, 341, 797, 4362, 452, 452, 6713, 13, 823, 445, 360, 309, 797, 13, 407, 321, 434, 1382, 281, 483, 281, 2217, 22117, 732, 13], "temperature": 0.0, "avg_logprob": -0.2665962169044896, "compression_ratio": 1.162162162162162, "no_speech_prob": 6.2407889345195144e-06}, {"id": 1051, "seek": 704100, "start": 7041.0, "end": 7056.0, "text": " So if I run it again. It's getting better. But eventually. Now it's just moving around the same place. Right.", "tokens": [407, 498, 286, 1190, 309, 797, 13, 467, 311, 1242, 1101, 13, 583, 4728, 13, 823, 309, 311, 445, 2684, 926, 264, 912, 1081, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.09707338626568134, "compression_ratio": 1.411764705882353, "no_speech_prob": 9.818023499974515e-06}, {"id": 1052, "seek": 704100, "start": 7056.0, "end": 7066.0, "text": " So you can see what's happened is the learning rates too high. So we could just go in here and drop it down. And run it some more.", "tokens": [407, 291, 393, 536, 437, 311, 2011, 307, 264, 2539, 6846, 886, 1090, 13, 407, 321, 727, 445, 352, 294, 510, 293, 3270, 309, 760, 13, 400, 1190, 309, 512, 544, 13], "temperature": 0.0, "avg_logprob": -0.09707338626568134, "compression_ratio": 1.411764705882353, "no_speech_prob": 9.818023499974515e-06}, {"id": 1053, "seek": 706600, "start": 7066.0, "end": 7077.0, "text": " Getting pretty close now. Right. So you can see how you still need learning rate and dealing even with Adam.", "tokens": [13674, 1238, 1998, 586, 13, 1779, 13, 407, 291, 393, 536, 577, 291, 920, 643, 2539, 3314, 293, 6260, 754, 365, 7938, 13], "temperature": 0.0, "avg_logprob": -0.13573176400703296, "compression_ratio": 1.3506493506493507, "no_speech_prob": 1.696390609140508e-05}, {"id": 1054, "seek": 706600, "start": 7077.0, "end": 7087.0, "text": " OK. So that spreadsheets fun to play around with. I do have a Google Sheets version of basic S.G.D.", "tokens": [2264, 13, 407, 300, 23651, 1385, 1019, 281, 862, 926, 365, 13, 286, 360, 362, 257, 3329, 1240, 1385, 3037, 295, 3875, 318, 13, 38, 13, 35, 13], "temperature": 0.0, "avg_logprob": -0.13573176400703296, "compression_ratio": 1.3506493506493507, "no_speech_prob": 1.696390609140508e-05}, {"id": 1055, "seek": 708700, "start": 7087.0, "end": 7096.0, "text": " That actually works and the macros work and everything. Google Sheets is so awful and I went so insane making that work I gave up on making the other ones work.", "tokens": [663, 767, 1985, 293, 264, 7912, 2635, 589, 293, 1203, 13, 3329, 1240, 1385, 307, 370, 11232, 293, 286, 1437, 370, 10838, 1455, 300, 589, 286, 2729, 493, 322, 1455, 264, 661, 2306, 589, 13], "temperature": 0.0, "avg_logprob": -0.07504380575501092, "compression_ratio": 1.7166666666666666, "no_speech_prob": 3.703907714225352e-05}, {"id": 1056, "seek": 708700, "start": 7096.0, "end": 7107.0, "text": " So I'll share a link to the Google Sheets version. Oh my God. They do have a macro language but it's just ridiculous.", "tokens": [407, 286, 603, 2073, 257, 2113, 281, 264, 3329, 1240, 1385, 3037, 13, 876, 452, 1265, 13, 814, 360, 362, 257, 18887, 2856, 457, 309, 311, 445, 11083, 13], "temperature": 0.0, "avg_logprob": -0.07504380575501092, "compression_ratio": 1.7166666666666666, "no_speech_prob": 3.703907714225352e-05}, {"id": 1057, "seek": 708700, "start": 7107.0, "end": 7114.0, "text": " So anyway if somebody feels like fighting it to actually get all the other ones to work they will work. It's just it's just annoying.", "tokens": [407, 4033, 498, 2618, 3417, 411, 5237, 309, 281, 767, 483, 439, 264, 661, 2306, 281, 589, 436, 486, 589, 13, 467, 311, 445, 309, 311, 445, 11304, 13], "temperature": 0.0, "avg_logprob": -0.07504380575501092, "compression_ratio": 1.7166666666666666, "no_speech_prob": 3.703907714225352e-05}, {"id": 1058, "seek": 711400, "start": 7114.0, "end": 7132.0, "text": " So maybe somebody can get this working on Google Sheets too. OK. So that's weight decay and Adam and Adam is amazingly fast.", "tokens": [407, 1310, 2618, 393, 483, 341, 1364, 322, 3329, 1240, 1385, 886, 13, 2264, 13, 407, 300, 311, 3364, 21039, 293, 7938, 293, 7938, 307, 31762, 2370, 13], "temperature": 0.0, "avg_logprob": -0.12271726876497269, "compression_ratio": 1.158878504672897, "no_speech_prob": 2.0143248548265547e-05}, {"id": 1059, "seek": 713200, "start": 7132.0, "end": 7147.0, "text": " And we let's go back to this one but we don't tend to use opt in dot whatever and create the optimizer ourselves and all that stuff because instead we tend to use learner.", "tokens": [400, 321, 718, 311, 352, 646, 281, 341, 472, 457, 321, 500, 380, 3928, 281, 764, 2427, 294, 5893, 2035, 293, 1884, 264, 5028, 6545, 4175, 293, 439, 300, 1507, 570, 2602, 321, 3928, 281, 764, 33347, 13], "temperature": 0.0, "avg_logprob": -0.16431287859306962, "compression_ratio": 1.5217391304347827, "no_speech_prob": 2.1442892830236815e-05}, {"id": 1060, "seek": 713200, "start": 7147.0, "end": 7152.0, "text": " But learn is just doing those things for you. But again there's no magic.", "tokens": [583, 1466, 307, 445, 884, 729, 721, 337, 291, 13, 583, 797, 456, 311, 572, 5585, 13], "temperature": 0.0, "avg_logprob": -0.16431287859306962, "compression_ratio": 1.5217391304347827, "no_speech_prob": 2.1442892830236815e-05}, {"id": 1061, "seek": 715200, "start": 7152.0, "end": 7163.0, "text": " So if you create a learner you say here's my data bunch. Here's my pie torch and end up module instance. Here's my loss function.", "tokens": [407, 498, 291, 1884, 257, 33347, 291, 584, 510, 311, 452, 1412, 3840, 13, 1692, 311, 452, 1730, 27822, 293, 917, 493, 10088, 5197, 13, 1692, 311, 452, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.19322271549955328, "compression_ratio": 1.6550218340611353, "no_speech_prob": 6.438689524657093e-06}, {"id": 1062, "seek": 715200, "start": 7163.0, "end": 7170.0, "text": " And here are my metrics. Remember the metrics are just stuff to print out. That's it. Right.", "tokens": [400, 510, 366, 452, 16367, 13, 5459, 264, 16367, 366, 445, 1507, 281, 4482, 484, 13, 663, 311, 309, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.19322271549955328, "compression_ratio": 1.6550218340611353, "no_speech_prob": 6.438689524657093e-06}, {"id": 1063, "seek": 715200, "start": 7170.0, "end": 7179.0, "text": " Then you just get a few nice things like learned out LR finds starts working and it starts recording this and you can say fit one cycle instead of just fit.", "tokens": [1396, 291, 445, 483, 257, 1326, 1481, 721, 411, 3264, 484, 441, 49, 10704, 3719, 1364, 293, 309, 3719, 6613, 341, 293, 291, 393, 584, 3318, 472, 6586, 2602, 295, 445, 3318, 13], "temperature": 0.0, "avg_logprob": -0.19322271549955328, "compression_ratio": 1.6550218340611353, "no_speech_prob": 6.438689524657093e-06}, {"id": 1064, "seek": 717900, "start": 7179.0, "end": 7185.0, "text": " But like these things really help a lot. Like by using the floating rate finder I found a good learning rate.", "tokens": [583, 411, 613, 721, 534, 854, 257, 688, 13, 1743, 538, 1228, 264, 12607, 3314, 915, 260, 286, 1352, 257, 665, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.13382795582646909, "compression_ratio": 1.6343612334801763, "no_speech_prob": 9.515957572148181e-06}, {"id": 1065, "seek": 717900, "start": 7185.0, "end": 7192.0, "text": " And then like look at this. My loss here point one three here I wasn't getting much beneath point five.", "tokens": [400, 550, 411, 574, 412, 341, 13, 1222, 4470, 510, 935, 472, 1045, 510, 286, 2067, 380, 1242, 709, 17149, 935, 1732, 13], "temperature": 0.0, "avg_logprob": -0.13382795582646909, "compression_ratio": 1.6343612334801763, "no_speech_prob": 9.515957572148181e-06}, {"id": 1066, "seek": 717900, "start": 7192.0, "end": 7202.0, "text": " So these these tweaks make huge differences not tiny differences. And this is still just one one epoch.", "tokens": [407, 613, 613, 46664, 652, 2603, 7300, 406, 5870, 7300, 13, 400, 341, 307, 920, 445, 472, 472, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.13382795582646909, "compression_ratio": 1.6343612334801763, "no_speech_prob": 9.515957572148181e-06}, {"id": 1067, "seek": 717900, "start": 7202.0, "end": 7208.0, "text": " Now what is fit one cycle do. What does it really do.", "tokens": [823, 437, 307, 3318, 472, 6586, 360, 13, 708, 775, 309, 534, 360, 13], "temperature": 0.0, "avg_logprob": -0.13382795582646909, "compression_ratio": 1.6343612334801763, "no_speech_prob": 9.515957572148181e-06}, {"id": 1068, "seek": 720800, "start": 7208.0, "end": 7212.0, "text": " This is what it really does. Right. And we've seen this chart on the left before.", "tokens": [639, 307, 437, 309, 534, 775, 13, 1779, 13, 400, 321, 600, 1612, 341, 6927, 322, 264, 1411, 949, 13], "temperature": 0.0, "avg_logprob": -0.07850462260999178, "compression_ratio": 1.7850877192982457, "no_speech_prob": 1.2218521078466438e-05}, {"id": 1069, "seek": 720800, "start": 7212.0, "end": 7218.0, "text": " Just to remind you this is plotting the learning rate per batch.", "tokens": [1449, 281, 4160, 291, 341, 307, 41178, 264, 2539, 3314, 680, 15245, 13], "temperature": 0.0, "avg_logprob": -0.07850462260999178, "compression_ratio": 1.7850877192982457, "no_speech_prob": 1.2218521078466438e-05}, {"id": 1070, "seek": 720800, "start": 7218.0, "end": 7227.0, "text": " I remember Adam has a learning rate and we use Adam by default or minor variation which we might try to talk about.", "tokens": [286, 1604, 7938, 575, 257, 2539, 3314, 293, 321, 764, 7938, 538, 7576, 420, 6696, 12990, 597, 321, 1062, 853, 281, 751, 466, 13], "temperature": 0.0, "avg_logprob": -0.07850462260999178, "compression_ratio": 1.7850877192982457, "no_speech_prob": 1.2218521078466438e-05}, {"id": 1071, "seek": 720800, "start": 7227.0, "end": 7237.0, "text": " So the learning rate starts really low and it increases about half the time and then it decreases about half the time because at the very start.", "tokens": [407, 264, 2539, 3314, 3719, 534, 2295, 293, 309, 8637, 466, 1922, 264, 565, 293, 550, 309, 24108, 466, 1922, 264, 565, 570, 412, 264, 588, 722, 13], "temperature": 0.0, "avg_logprob": -0.07850462260999178, "compression_ratio": 1.7850877192982457, "no_speech_prob": 1.2218521078466438e-05}, {"id": 1072, "seek": 723700, "start": 7237.0, "end": 7243.0, "text": " We don't know where we are. Right. So we're in some part of function space that's just bumpy as all hell.", "tokens": [492, 500, 380, 458, 689, 321, 366, 13, 1779, 13, 407, 321, 434, 294, 512, 644, 295, 2445, 1901, 300, 311, 445, 49400, 382, 439, 4921, 13], "temperature": 0.0, "avg_logprob": -0.1159455072312128, "compression_ratio": 1.7338709677419355, "no_speech_prob": 1.7230839148396626e-05}, {"id": 1073, "seek": 723700, "start": 7243.0, "end": 7250.0, "text": " So if you start jumping around those bumps have big gradients and it will throw you into crazy parts of the space.", "tokens": [407, 498, 291, 722, 11233, 926, 729, 27719, 362, 955, 2771, 2448, 293, 309, 486, 3507, 291, 666, 3219, 3166, 295, 264, 1901, 13], "temperature": 0.0, "avg_logprob": -0.1159455072312128, "compression_ratio": 1.7338709677419355, "no_speech_prob": 1.7230839148396626e-05}, {"id": 1074, "seek": 723700, "start": 7250.0, "end": 7258.0, "text": " Right. So start slow and then you'll gradually move into parts of the weight space that you know they're kind of sensible.", "tokens": [1779, 13, 407, 722, 2964, 293, 550, 291, 603, 13145, 1286, 666, 3166, 295, 264, 3364, 1901, 300, 291, 458, 436, 434, 733, 295, 25380, 13], "temperature": 0.0, "avg_logprob": -0.1159455072312128, "compression_ratio": 1.7338709677419355, "no_speech_prob": 1.7230839148396626e-05}, {"id": 1075, "seek": 723700, "start": 7258.0, "end": 7262.0, "text": " And as you get to the point where they're sensible you can increase the learning rate.", "tokens": [400, 382, 291, 483, 281, 264, 935, 689, 436, 434, 25380, 291, 393, 3488, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.1159455072312128, "compression_ratio": 1.7338709677419355, "no_speech_prob": 1.7230839148396626e-05}, {"id": 1076, "seek": 726200, "start": 7262.0, "end": 7268.0, "text": " You know because the the gradients are actually in the direction you want to go.", "tokens": [509, 458, 570, 264, 264, 2771, 2448, 366, 767, 294, 264, 3513, 291, 528, 281, 352, 13], "temperature": 0.0, "avg_logprob": -0.08911139466041743, "compression_ratio": 1.7135922330097086, "no_speech_prob": 1.86311353900237e-05}, {"id": 1077, "seek": 726200, "start": 7268.0, "end": 7276.0, "text": " And then as we've discussed a few times as you get close to the final answer you need to anneal your learning rate to hone in on it.", "tokens": [400, 550, 382, 321, 600, 7152, 257, 1326, 1413, 382, 291, 483, 1998, 281, 264, 2572, 1867, 291, 643, 281, 22256, 304, 428, 2539, 3314, 281, 43212, 294, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.08911139466041743, "compression_ratio": 1.7135922330097086, "no_speech_prob": 1.86311353900237e-05}, {"id": 1078, "seek": 726200, "start": 7276.0, "end": 7281.0, "text": " But here's the interesting thing on the left is the momentum plot.", "tokens": [583, 510, 311, 264, 1880, 551, 322, 264, 1411, 307, 264, 11244, 7542, 13], "temperature": 0.0, "avg_logprob": -0.08911139466041743, "compression_ratio": 1.7135922330097086, "no_speech_prob": 1.86311353900237e-05}, {"id": 1079, "seek": 726200, "start": 7281.0, "end": 7287.0, "text": " And actually every time our learning rate is small our momentum is high.", "tokens": [400, 767, 633, 565, 527, 2539, 3314, 307, 1359, 527, 11244, 307, 1090, 13], "temperature": 0.0, "avg_logprob": -0.08911139466041743, "compression_ratio": 1.7135922330097086, "no_speech_prob": 1.86311353900237e-05}, {"id": 1080, "seek": 728700, "start": 7287.0, "end": 7296.0, "text": " Why is that because if you do have a learning small learning rate but you keep going in the same direction you may as well go faster.", "tokens": [1545, 307, 300, 570, 498, 291, 360, 362, 257, 2539, 1359, 2539, 3314, 457, 291, 1066, 516, 294, 264, 912, 3513, 291, 815, 382, 731, 352, 4663, 13], "temperature": 0.0, "avg_logprob": -0.06450491150220235, "compression_ratio": 1.916256157635468, "no_speech_prob": 1.165921912615886e-05}, {"id": 1081, "seek": 728700, "start": 7296.0, "end": 7304.0, "text": " Right. But if you're jumping really far don't like jump jump really far because it's going to throw you off.", "tokens": [1779, 13, 583, 498, 291, 434, 11233, 534, 1400, 500, 380, 411, 3012, 3012, 534, 1400, 570, 309, 311, 516, 281, 3507, 291, 766, 13], "temperature": 0.0, "avg_logprob": -0.06450491150220235, "compression_ratio": 1.916256157635468, "no_speech_prob": 1.165921912615886e-05}, {"id": 1082, "seek": 728700, "start": 7304.0, "end": 7308.0, "text": " Right. And then as you get to the end again you're fine tuning in.", "tokens": [1779, 13, 400, 550, 382, 291, 483, 281, 264, 917, 797, 291, 434, 2489, 15164, 294, 13], "temperature": 0.0, "avg_logprob": -0.06450491150220235, "compression_ratio": 1.916256157635468, "no_speech_prob": 1.165921912615886e-05}, {"id": 1083, "seek": 728700, "start": 7308.0, "end": 7313.0, "text": " But actually if you keep going in the same direction again and again go faster.", "tokens": [583, 767, 498, 291, 1066, 516, 294, 264, 912, 3513, 797, 293, 797, 352, 4663, 13], "temperature": 0.0, "avg_logprob": -0.06450491150220235, "compression_ratio": 1.916256157635468, "no_speech_prob": 1.165921912615886e-05}, {"id": 1084, "seek": 731300, "start": 7313.0, "end": 7321.0, "text": " So this combination is called one cycle and it's just this amazing like it's a simple thing but it's astonishing.", "tokens": [407, 341, 6562, 307, 1219, 472, 6586, 293, 309, 311, 445, 341, 2243, 411, 309, 311, 257, 2199, 551, 457, 309, 311, 35264, 13], "temperature": 0.0, "avg_logprob": -0.0745691030453413, "compression_ratio": 1.6146341463414635, "no_speech_prob": 1.520488876849413e-05}, {"id": 1085, "seek": 731300, "start": 7321.0, "end": 7328.0, "text": " Like this can help you get what's called super convergence that can let you train 10 times faster.", "tokens": [1743, 341, 393, 854, 291, 483, 437, 311, 1219, 1687, 32181, 300, 393, 718, 291, 3847, 1266, 1413, 4663, 13], "temperature": 0.0, "avg_logprob": -0.0745691030453413, "compression_ratio": 1.6146341463414635, "no_speech_prob": 1.520488876849413e-05}, {"id": 1086, "seek": 731300, "start": 7328.0, "end": 7333.0, "text": " Now this is just last year's paper and some of you may have seen the interview with Leslie Smith that I did last week.", "tokens": [823, 341, 307, 445, 1036, 1064, 311, 3035, 293, 512, 295, 291, 815, 362, 1612, 264, 4049, 365, 28140, 8538, 300, 286, 630, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.0745691030453413, "compression_ratio": 1.6146341463414635, "no_speech_prob": 1.520488876849413e-05}, {"id": 1087, "seek": 733300, "start": 7333.0, "end": 7346.0, "text": " Amazing guy incredibly humble and also I should say somebody who is doing groundbreaking research well into his 60s and all of these things are inspiring.", "tokens": [14165, 2146, 6252, 16735, 293, 611, 286, 820, 584, 2618, 567, 307, 884, 42491, 2132, 731, 666, 702, 4060, 82, 293, 439, 295, 613, 721, 366, 15883, 13], "temperature": 0.0, "avg_logprob": -0.08478707220496201, "compression_ratio": 1.606694560669456, "no_speech_prob": 2.406030944257509e-06}, {"id": 1088, "seek": 733300, "start": 7346.0, "end": 7352.0, "text": " I'll show you something else interesting when you plot the losses with fast AI it doesn't look like that.", "tokens": [286, 603, 855, 291, 746, 1646, 1880, 562, 291, 7542, 264, 15352, 365, 2370, 7318, 309, 1177, 380, 574, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.08478707220496201, "compression_ratio": 1.606694560669456, "no_speech_prob": 2.406030944257509e-06}, {"id": 1089, "seek": 733300, "start": 7352.0, "end": 7361.0, "text": " It looks like that. Why is that because fast AI calculates the exponentially weighted moving average of the losses for you.", "tokens": [467, 1542, 411, 300, 13, 1545, 307, 300, 570, 2370, 7318, 4322, 1024, 264, 37330, 32807, 2684, 4274, 295, 264, 15352, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.08478707220496201, "compression_ratio": 1.606694560669456, "no_speech_prob": 2.406030944257509e-06}, {"id": 1090, "seek": 736100, "start": 7361.0, "end": 7367.0, "text": " All right. So this this concept of exponentially weighted stuff it's just really handy and I use it all the time.", "tokens": [1057, 558, 13, 407, 341, 341, 3410, 295, 37330, 32807, 1507, 309, 311, 445, 534, 13239, 293, 286, 764, 309, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.08910239826549184, "compression_ratio": 1.7047970479704797, "no_speech_prob": 1.9831139070447534e-05}, {"id": 1091, "seek": 736100, "start": 7367.0, "end": 7370.0, "text": " And one of the things that is to make it easier to read these charts.", "tokens": [400, 472, 295, 264, 721, 300, 307, 281, 652, 309, 3571, 281, 1401, 613, 17767, 13], "temperature": 0.0, "avg_logprob": -0.08910239826549184, "compression_ratio": 1.7047970479704797, "no_speech_prob": 1.9831139070447534e-05}, {"id": 1092, "seek": 736100, "start": 7370.0, "end": 7381.0, "text": " OK. It does mean that these charts from fast AI might be kind of an epoch or two sorry a batch or two behind where they should be.", "tokens": [2264, 13, 467, 775, 914, 300, 613, 17767, 490, 2370, 7318, 1062, 312, 733, 295, 364, 30992, 339, 420, 732, 2597, 257, 15245, 420, 732, 2261, 689, 436, 820, 312, 13], "temperature": 0.0, "avg_logprob": -0.08910239826549184, "compression_ratio": 1.7047970479704797, "no_speech_prob": 1.9831139070447534e-05}, {"id": 1093, "seek": 736100, "start": 7381.0, "end": 7387.0, "text": " You know there's that slight downside when you use an exponentially weighted moving average is you've got a little bit of history in there as well.", "tokens": [509, 458, 456, 311, 300, 4036, 25060, 562, 291, 764, 364, 37330, 32807, 2684, 4274, 307, 291, 600, 658, 257, 707, 857, 295, 2503, 294, 456, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.08910239826549184, "compression_ratio": 1.7047970479704797, "no_speech_prob": 1.9831139070447534e-05}, {"id": 1094, "seek": 738700, "start": 7387.0, "end": 7396.0, "text": " But it can make it much easier to see what's going on.", "tokens": [583, 309, 393, 652, 309, 709, 3571, 281, 536, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.13929110559923896, "compression_ratio": 1.4507042253521127, "no_speech_prob": 1.952449201780837e-05}, {"id": 1095, "seek": 738700, "start": 7396.0, "end": 7407.0, "text": " So we're now at a point coming to the end of this co-label tabular section where we're going to try to understand all of the code in our tabular model.", "tokens": [407, 321, 434, 586, 412, 257, 935, 1348, 281, 264, 917, 295, 341, 598, 12, 75, 18657, 4421, 1040, 3541, 689, 321, 434, 516, 281, 853, 281, 1223, 439, 295, 264, 3089, 294, 527, 4421, 1040, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13929110559923896, "compression_ratio": 1.4507042253521127, "no_speech_prob": 1.952449201780837e-05}, {"id": 1096, "seek": 740700, "start": 7407.0, "end": 7418.0, "text": " So remember the tabular model uses data set called adult which is trying to predict who's going to make more money. It's a classification problem.", "tokens": [407, 1604, 264, 4421, 1040, 2316, 4960, 1412, 992, 1219, 5075, 597, 307, 1382, 281, 6069, 567, 311, 516, 281, 652, 544, 1460, 13, 467, 311, 257, 21538, 1154, 13], "temperature": 0.0, "avg_logprob": -0.08185685410791514, "compression_ratio": 1.7, "no_speech_prob": 7.182283752626972e-06}, {"id": 1097, "seek": 740700, "start": 7418.0, "end": 7423.0, "text": " And we've got a number of categorical variables and a number of continuous variables.", "tokens": [400, 321, 600, 658, 257, 1230, 295, 19250, 804, 9102, 293, 257, 1230, 295, 10957, 9102, 13], "temperature": 0.0, "avg_logprob": -0.08185685410791514, "compression_ratio": 1.7, "no_speech_prob": 7.182283752626972e-06}, {"id": 1098, "seek": 740700, "start": 7423.0, "end": 7435.0, "text": " So the first thing we realize is we actually don't know how to predict a categorical variable yet because so far we did some hand waving around the fact that our loss function was an end up cross entropy loss.", "tokens": [407, 264, 700, 551, 321, 4325, 307, 321, 767, 500, 380, 458, 577, 281, 6069, 257, 19250, 804, 7006, 1939, 570, 370, 1400, 321, 630, 512, 1011, 35347, 926, 264, 1186, 300, 527, 4470, 2445, 390, 364, 917, 493, 3278, 30867, 4470, 13], "temperature": 0.0, "avg_logprob": -0.08185685410791514, "compression_ratio": 1.7, "no_speech_prob": 7.182283752626972e-06}, {"id": 1099, "seek": 743500, "start": 7435.0, "end": 7445.0, "text": " What is that. Let's find out. And of course we're going to find out by looking at Microsoft Excel.", "tokens": [708, 307, 300, 13, 961, 311, 915, 484, 13, 400, 295, 1164, 321, 434, 516, 281, 915, 484, 538, 1237, 412, 8116, 19060, 13], "temperature": 0.0, "avg_logprob": -0.12981183068794117, "compression_ratio": 1.4658385093167703, "no_speech_prob": 2.4297076379298232e-05}, {"id": 1100, "seek": 743500, "start": 7445.0, "end": 7455.0, "text": " So cross entropy loss is just another loss function. We already know one loss function which is mean squared error y hat minus y squared.", "tokens": [407, 3278, 30867, 4470, 307, 445, 1071, 4470, 2445, 13, 492, 1217, 458, 472, 4470, 2445, 597, 307, 914, 8889, 6713, 288, 2385, 3175, 288, 8889, 13], "temperature": 0.0, "avg_logprob": -0.12981183068794117, "compression_ratio": 1.4658385093167703, "no_speech_prob": 2.4297076379298232e-05}, {"id": 1101, "seek": 745500, "start": 7455.0, "end": 7468.0, "text": " OK. So that's not a good loss function for us because in our case we have like for MNIST 10 possible digits and we have 10 activations each with a probability of that digit.", "tokens": [2264, 13, 407, 300, 311, 406, 257, 665, 4470, 2445, 337, 505, 570, 294, 527, 1389, 321, 362, 411, 337, 376, 45, 19756, 1266, 1944, 27011, 293, 321, 362, 1266, 2430, 763, 1184, 365, 257, 8482, 295, 300, 14293, 13], "temperature": 0.0, "avg_logprob": -0.09906596690416336, "compression_ratio": 1.4663212435233162, "no_speech_prob": 1.0615593964757863e-05}, {"id": 1102, "seek": 745500, "start": 7468.0, "end": 7479.0, "text": " So we need something where predicting the right thing correctly and confidently should have very little loss.", "tokens": [407, 321, 643, 746, 689, 32884, 264, 558, 551, 8944, 293, 41956, 820, 362, 588, 707, 4470, 13], "temperature": 0.0, "avg_logprob": -0.09906596690416336, "compression_ratio": 1.4663212435233162, "no_speech_prob": 1.0615593964757863e-05}, {"id": 1103, "seek": 747900, "start": 7479.0, "end": 7488.0, "text": " Predicting the wrong thing confidently should have a lot of loss. So that's what we want. OK. So here's an example.", "tokens": [32969, 21490, 264, 2085, 551, 41956, 820, 362, 257, 688, 295, 4470, 13, 407, 300, 311, 437, 321, 528, 13, 2264, 13, 407, 510, 311, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.09636188436437536, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.2218759366078302e-05}, {"id": 1104, "seek": 747900, "start": 7488.0, "end": 7499.0, "text": " Here is a cat versus dog one hot encoded. OK. And here are my two activations for each one from some model that I built.", "tokens": [1692, 307, 257, 3857, 5717, 3000, 472, 2368, 2058, 12340, 13, 2264, 13, 400, 510, 366, 452, 732, 2430, 763, 337, 1184, 472, 490, 512, 2316, 300, 286, 3094, 13], "temperature": 0.0, "avg_logprob": -0.09636188436437536, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.2218759366078302e-05}, {"id": 1105, "seek": 747900, "start": 7499.0, "end": 7504.0, "text": " Probability cat probability dog. This one's not very confident of anything.", "tokens": [8736, 2310, 3857, 8482, 3000, 13, 639, 472, 311, 406, 588, 6679, 295, 1340, 13], "temperature": 0.0, "avg_logprob": -0.09636188436437536, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.2218759366078302e-05}, {"id": 1106, "seek": 750400, "start": 7504.0, "end": 7510.0, "text": " This one's very confident of it being a cat and it's right. This one's very confident of being a cat and it's wrong.", "tokens": [639, 472, 311, 588, 6679, 295, 309, 885, 257, 3857, 293, 309, 311, 558, 13, 639, 472, 311, 588, 6679, 295, 885, 257, 3857, 293, 309, 311, 2085, 13], "temperature": 0.0, "avg_logprob": -0.09955821847015957, "compression_ratio": 2.1683168316831685, "no_speech_prob": 1.0451288289914373e-05}, {"id": 1107, "seek": 750400, "start": 7510.0, "end": 7519.0, "text": " So we want a loss that for this one should be a moderate loss because not predicting anything confidently is not really what we want.", "tokens": [407, 321, 528, 257, 4470, 300, 337, 341, 472, 820, 312, 257, 18174, 4470, 570, 406, 32884, 1340, 41956, 307, 406, 534, 437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.09955821847015957, "compression_ratio": 2.1683168316831685, "no_speech_prob": 1.0451288289914373e-05}, {"id": 1108, "seek": 750400, "start": 7519.0, "end": 7524.0, "text": " So here's a point three. This thing's predicting the correct thing very confidently.", "tokens": [407, 510, 311, 257, 935, 1045, 13, 639, 551, 311, 32884, 264, 3006, 551, 588, 41956, 13], "temperature": 0.0, "avg_logprob": -0.09955821847015957, "compression_ratio": 2.1683168316831685, "no_speech_prob": 1.0451288289914373e-05}, {"id": 1109, "seek": 750400, "start": 7524.0, "end": 7532.0, "text": " So point one. This thing's predicting the wrong thing very confidently. So what. So how do we do that.", "tokens": [407, 935, 472, 13, 639, 551, 311, 32884, 264, 2085, 551, 588, 41956, 13, 407, 437, 13, 407, 577, 360, 321, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.09955821847015957, "compression_ratio": 2.1683168316831685, "no_speech_prob": 1.0451288289914373e-05}, {"id": 1110, "seek": 753200, "start": 7532.0, "end": 7539.0, "text": " This is the cross entropy loss and it is equal to.", "tokens": [639, 307, 264, 3278, 30867, 4470, 293, 309, 307, 2681, 281, 13], "temperature": 0.0, "avg_logprob": -0.11712017997366483, "compression_ratio": 1.542857142857143, "no_speech_prob": 2.247331394755747e-05}, {"id": 1111, "seek": 753200, "start": 7539.0, "end": 7542.0, "text": " Whether it's a cat.", "tokens": [8503, 309, 311, 257, 3857, 13], "temperature": 0.0, "avg_logprob": -0.11712017997366483, "compression_ratio": 1.542857142857143, "no_speech_prob": 2.247331394755747e-05}, {"id": 1112, "seek": 753200, "start": 7542.0, "end": 7547.0, "text": " Multiplied by log of the probability of cat. This is actually an activation.", "tokens": [29238, 564, 1091, 538, 3565, 295, 264, 8482, 295, 3857, 13, 639, 307, 767, 364, 24433, 13], "temperature": 0.0, "avg_logprob": -0.11712017997366483, "compression_ratio": 1.542857142857143, "no_speech_prob": 2.247331394755747e-05}, {"id": 1113, "seek": 753200, "start": 7547.0, "end": 7553.0, "text": " So I should say so it's multiplied by the log of the cat activation.", "tokens": [407, 286, 820, 584, 370, 309, 311, 17207, 538, 264, 3565, 295, 264, 3857, 24433, 13], "temperature": 0.0, "avg_logprob": -0.11712017997366483, "compression_ratio": 1.542857142857143, "no_speech_prob": 2.247331394755747e-05}, {"id": 1114, "seek": 755300, "start": 7553.0, "end": 7562.0, "text": " Negative that minus is it a dog times the log of the dog activation.", "tokens": [43230, 300, 3175, 307, 309, 257, 3000, 1413, 264, 3565, 295, 264, 3000, 24433, 13], "temperature": 0.0, "avg_logprob": -0.08625421524047852, "compression_ratio": 1.625668449197861, "no_speech_prob": 3.966923941334244e-06}, {"id": 1115, "seek": 755300, "start": 7562.0, "end": 7574.0, "text": " And that's it. OK. So in other words it's the sum of all of your one hot encoded variables times all of your activations.", "tokens": [400, 300, 311, 309, 13, 2264, 13, 407, 294, 661, 2283, 309, 311, 264, 2408, 295, 439, 295, 428, 472, 2368, 2058, 12340, 9102, 1413, 439, 295, 428, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.08625421524047852, "compression_ratio": 1.625668449197861, "no_speech_prob": 3.966923941334244e-06}, {"id": 1116, "seek": 755300, "start": 7574.0, "end": 7582.0, "text": " So interestingly these ones here are exactly the same numbers as these ones here but I've written it differently.", "tokens": [407, 25873, 613, 2306, 510, 366, 2293, 264, 912, 3547, 382, 613, 2306, 510, 457, 286, 600, 3720, 309, 7614, 13], "temperature": 0.0, "avg_logprob": -0.08625421524047852, "compression_ratio": 1.625668449197861, "no_speech_prob": 3.966923941334244e-06}, {"id": 1117, "seek": 758200, "start": 7582.0, "end": 7589.0, "text": " I've written it with an F function because it's exactly this quiz because the zeros don't actually add anything.", "tokens": [286, 600, 3720, 309, 365, 364, 479, 2445, 570, 309, 311, 2293, 341, 15450, 570, 264, 35193, 500, 380, 767, 909, 1340, 13], "temperature": 0.0, "avg_logprob": -0.1444932601031135, "compression_ratio": 1.7696629213483146, "no_speech_prob": 9.972494808607735e-06}, {"id": 1118, "seek": 758200, "start": 7589.0, "end": 7598.0, "text": " Right. So actually it's exactly the same as saying if it's a cat then take the log of cat in us.", "tokens": [1779, 13, 407, 767, 309, 311, 2293, 264, 912, 382, 1566, 498, 309, 311, 257, 3857, 550, 747, 264, 3565, 295, 3857, 294, 505, 13], "temperature": 0.0, "avg_logprob": -0.1444932601031135, "compression_ratio": 1.7696629213483146, "no_speech_prob": 9.972494808607735e-06}, {"id": 1119, "seek": 758200, "start": 7598.0, "end": 7607.0, "text": " And if it's a dog. So otherwise take the log of one minus cat in us. In other words the log of dog in us.", "tokens": [400, 498, 309, 311, 257, 3000, 13, 407, 5911, 747, 264, 3565, 295, 472, 3175, 3857, 294, 505, 13, 682, 661, 2283, 264, 3565, 295, 3000, 294, 505, 13], "temperature": 0.0, "avg_logprob": -0.1444932601031135, "compression_ratio": 1.7696629213483146, "no_speech_prob": 9.972494808607735e-06}, {"id": 1120, "seek": 760700, "start": 7607.0, "end": 7616.0, "text": " So the sum of the one hot encoded times the activations is the same as an F function.", "tokens": [407, 264, 2408, 295, 264, 472, 2368, 2058, 12340, 1413, 264, 2430, 763, 307, 264, 912, 382, 364, 479, 2445, 13], "temperature": 0.0, "avg_logprob": -0.08321360687711346, "compression_ratio": 1.5204678362573099, "no_speech_prob": 6.540131380461389e-06}, {"id": 1121, "seek": 760700, "start": 7616.0, "end": 7623.0, "text": " Which if you think about it is actually because this is just a matrix multiply.", "tokens": [3013, 498, 291, 519, 466, 309, 307, 767, 570, 341, 307, 445, 257, 8141, 12972, 13], "temperature": 0.0, "avg_logprob": -0.08321360687711346, "compression_ratio": 1.5204678362573099, "no_speech_prob": 6.540131380461389e-06}, {"id": 1122, "seek": 760700, "start": 7623.0, "end": 7629.0, "text": " This is we now know from our from our embedding discussion that's the same as an index lookup.", "tokens": [639, 307, 321, 586, 458, 490, 527, 490, 527, 12240, 3584, 5017, 300, 311, 264, 912, 382, 364, 8186, 574, 1010, 13], "temperature": 0.0, "avg_logprob": -0.08321360687711346, "compression_ratio": 1.5204678362573099, "no_speech_prob": 6.540131380461389e-06}, {"id": 1123, "seek": 762900, "start": 7629.0, "end": 7640.0, "text": " So you can also to do cross entropy you can also just look up the log of the activation for the correct answer.", "tokens": [407, 291, 393, 611, 281, 360, 3278, 30867, 291, 393, 611, 445, 574, 493, 264, 3565, 295, 264, 24433, 337, 264, 3006, 1867, 13], "temperature": 0.0, "avg_logprob": -0.04638818666046741, "compression_ratio": 1.751131221719457, "no_speech_prob": 1.4738474419573322e-05}, {"id": 1124, "seek": 762900, "start": 7640.0, "end": 7645.0, "text": " Now that's only going to work if these rows add up to one.", "tokens": [823, 300, 311, 787, 516, 281, 589, 498, 613, 13241, 909, 493, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.04638818666046741, "compression_ratio": 1.751131221719457, "no_speech_prob": 1.4738474419573322e-05}, {"id": 1125, "seek": 762900, "start": 7645.0, "end": 7652.0, "text": " And this is one reason that you can get screwy cross entropy numbers is that's why I said you press the wrong button.", "tokens": [400, 341, 307, 472, 1778, 300, 291, 393, 483, 5630, 88, 3278, 30867, 3547, 307, 300, 311, 983, 286, 848, 291, 1886, 264, 2085, 2960, 13], "temperature": 0.0, "avg_logprob": -0.04638818666046741, "compression_ratio": 1.751131221719457, "no_speech_prob": 1.4738474419573322e-05}, {"id": 1126, "seek": 762900, "start": 7652.0, "end": 7657.0, "text": " If they don't add up to one you've got a trouble. So how do you make sure that they add up to one.", "tokens": [759, 436, 500, 380, 909, 493, 281, 472, 291, 600, 658, 257, 5253, 13, 407, 577, 360, 291, 652, 988, 300, 436, 909, 493, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.04638818666046741, "compression_ratio": 1.751131221719457, "no_speech_prob": 1.4738474419573322e-05}, {"id": 1127, "seek": 765700, "start": 7657.0, "end": 7663.0, "text": " You make sure they add up to one by using the correct activation function in your last layer.", "tokens": [509, 652, 988, 436, 909, 493, 281, 472, 538, 1228, 264, 3006, 24433, 2445, 294, 428, 1036, 4583, 13], "temperature": 0.0, "avg_logprob": -0.05251109366323434, "compression_ratio": 2.01010101010101, "no_speech_prob": 3.219127393094823e-05}, {"id": 1128, "seek": 765700, "start": 7663.0, "end": 7667.0, "text": " And the correct activation function to use for this is softmax.", "tokens": [400, 264, 3006, 24433, 2445, 281, 764, 337, 341, 307, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.05251109366323434, "compression_ratio": 2.01010101010101, "no_speech_prob": 3.219127393094823e-05}, {"id": 1129, "seek": 765700, "start": 7667.0, "end": 7674.0, "text": " Softmax is an activation function where all of the activations add up to one.", "tokens": [16985, 41167, 307, 364, 24433, 2445, 689, 439, 295, 264, 2430, 763, 909, 493, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.05251109366323434, "compression_ratio": 2.01010101010101, "no_speech_prob": 3.219127393094823e-05}, {"id": 1130, "seek": 765700, "start": 7674.0, "end": 7680.0, "text": " All of the activations are greater than zero and all of the activations are less than one.", "tokens": [1057, 295, 264, 2430, 763, 366, 5044, 813, 4018, 293, 439, 295, 264, 2430, 763, 366, 1570, 813, 472, 13], "temperature": 0.0, "avg_logprob": -0.05251109366323434, "compression_ratio": 2.01010101010101, "no_speech_prob": 3.219127393094823e-05}, {"id": 1131, "seek": 765700, "start": 7680.0, "end": 7686.0, "text": " So that's what we want. Right. That's what we need. How do you do that.", "tokens": [407, 300, 311, 437, 321, 528, 13, 1779, 13, 663, 311, 437, 321, 643, 13, 1012, 360, 291, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.05251109366323434, "compression_ratio": 2.01010101010101, "no_speech_prob": 3.219127393094823e-05}, {"id": 1132, "seek": 768600, "start": 7686.0, "end": 7691.0, "text": " Well let's say we were predicting one of five things cat dog plane fish building.", "tokens": [1042, 718, 311, 584, 321, 645, 32884, 472, 295, 1732, 721, 3857, 3000, 5720, 3506, 2390, 13], "temperature": 0.0, "avg_logprob": -0.07055702517109533, "compression_ratio": 1.7557603686635945, "no_speech_prob": 1.2804995094484184e-05}, {"id": 1133, "seek": 768600, "start": 7691.0, "end": 7698.0, "text": " And these were the numbers that came out of our neural net for one set of predictions.", "tokens": [400, 613, 645, 264, 3547, 300, 1361, 484, 295, 527, 18161, 2533, 337, 472, 992, 295, 21264, 13], "temperature": 0.0, "avg_logprob": -0.07055702517109533, "compression_ratio": 1.7557603686635945, "no_speech_prob": 1.2804995094484184e-05}, {"id": 1134, "seek": 768600, "start": 7698.0, "end": 7706.0, "text": " Well what if I did e to the power of that. So that's one step in the right direction because e to the power of something is always bigger than zero.", "tokens": [1042, 437, 498, 286, 630, 308, 281, 264, 1347, 295, 300, 13, 407, 300, 311, 472, 1823, 294, 264, 558, 3513, 570, 308, 281, 264, 1347, 295, 746, 307, 1009, 3801, 813, 4018, 13], "temperature": 0.0, "avg_logprob": -0.07055702517109533, "compression_ratio": 1.7557603686635945, "no_speech_prob": 1.2804995094484184e-05}, {"id": 1135, "seek": 768600, "start": 7706.0, "end": 7711.0, "text": " So there's a bunch of numbers that are always bigger than zero.", "tokens": [407, 456, 311, 257, 3840, 295, 3547, 300, 366, 1009, 3801, 813, 4018, 13], "temperature": 0.0, "avg_logprob": -0.07055702517109533, "compression_ratio": 1.7557603686635945, "no_speech_prob": 1.2804995094484184e-05}, {"id": 1136, "seek": 771100, "start": 7711.0, "end": 7716.0, "text": " Here's the sum of those numbers.", "tokens": [1692, 311, 264, 2408, 295, 729, 3547, 13], "temperature": 0.0, "avg_logprob": -0.08067544077483702, "compression_ratio": 1.5614035087719298, "no_speech_prob": 1.3419579772744328e-05}, {"id": 1137, "seek": 771100, "start": 7716.0, "end": 7721.0, "text": " Here is e to the number divided by the sum of e to the number.", "tokens": [1692, 307, 308, 281, 264, 1230, 6666, 538, 264, 2408, 295, 308, 281, 264, 1230, 13], "temperature": 0.0, "avg_logprob": -0.08067544077483702, "compression_ratio": 1.5614035087719298, "no_speech_prob": 1.3419579772744328e-05}, {"id": 1138, "seek": 771100, "start": 7721.0, "end": 7727.0, "text": " Now this number is always less than one. Right.", "tokens": [823, 341, 1230, 307, 1009, 1570, 813, 472, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.08067544077483702, "compression_ratio": 1.5614035087719298, "no_speech_prob": 1.3419579772744328e-05}, {"id": 1139, "seek": 771100, "start": 7727.0, "end": 7734.0, "text": " Because all of the things were positive so you can't possibly have one of the pieces be bigger than 100 percent of its sum.", "tokens": [1436, 439, 295, 264, 721, 645, 3353, 370, 291, 393, 380, 6264, 362, 472, 295, 264, 3755, 312, 3801, 813, 2319, 3043, 295, 1080, 2408, 13], "temperature": 0.0, "avg_logprob": -0.08067544077483702, "compression_ratio": 1.5614035087719298, "no_speech_prob": 1.3419579772744328e-05}, {"id": 1140, "seek": 773400, "start": 7734.0, "end": 7745.0, "text": " OK. And all of those things must add up to one. Right. Because each one of them was just that percentage of the total.", "tokens": [2264, 13, 400, 439, 295, 729, 721, 1633, 909, 493, 281, 472, 13, 1779, 13, 1436, 1184, 472, 295, 552, 390, 445, 300, 9668, 295, 264, 3217, 13], "temperature": 0.0, "avg_logprob": -0.09649852343967982, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.0451210982864723e-05}, {"id": 1141, "seek": 773400, "start": 7745.0, "end": 7756.0, "text": " So that's it. So this thing softmax is equal to e to the activation divided by the sum of e to the activations.", "tokens": [407, 300, 311, 309, 13, 407, 341, 551, 2787, 41167, 307, 2681, 281, 308, 281, 264, 24433, 6666, 538, 264, 2408, 295, 308, 281, 264, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.09649852343967982, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.0451210982864723e-05}, {"id": 1142, "seek": 773400, "start": 7756.0, "end": 7759.0, "text": " That's called softmax.", "tokens": [663, 311, 1219, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.09649852343967982, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.0451210982864723e-05}, {"id": 1143, "seek": 775900, "start": 7759.0, "end": 7773.0, "text": " And so when we're doing single label multi class classification you generally want softmax as your activation function and you generally want cross entropy as your loss.", "tokens": [400, 370, 562, 321, 434, 884, 2167, 7645, 4825, 1508, 21538, 291, 5101, 528, 2787, 41167, 382, 428, 24433, 2445, 293, 291, 5101, 528, 3278, 30867, 382, 428, 4470, 13], "temperature": 0.0, "avg_logprob": -0.12808969746465268, "compression_ratio": 1.530612244897959, "no_speech_prob": 8.664441338623874e-06}, {"id": 1144, "seek": 775900, "start": 7773.0, "end": 7778.0, "text": " Because these things go together in such friendly ways.", "tokens": [1436, 613, 721, 352, 1214, 294, 1270, 9208, 2098, 13], "temperature": 0.0, "avg_logprob": -0.12808969746465268, "compression_ratio": 1.530612244897959, "no_speech_prob": 8.664441338623874e-06}, {"id": 1145, "seek": 777800, "start": 7778.0, "end": 7790.0, "text": " PyTorch will do them both for you. Right. So you might have noticed that in this MNIST example I never added a softmax here.", "tokens": [9953, 51, 284, 339, 486, 360, 552, 1293, 337, 291, 13, 1779, 13, 407, 291, 1062, 362, 5694, 300, 294, 341, 376, 45, 19756, 1365, 286, 1128, 3869, 257, 2787, 41167, 510, 13], "temperature": 0.0, "avg_logprob": -0.07000018869127546, "compression_ratio": 1.663265306122449, "no_speech_prob": 6.240813490876462e-06}, {"id": 1146, "seek": 777800, "start": 7790.0, "end": 7797.0, "text": " And that's because if you ask for cross entropy loss it actually does the softmax in inside the loss function.", "tokens": [400, 300, 311, 570, 498, 291, 1029, 337, 3278, 30867, 4470, 309, 767, 775, 264, 2787, 41167, 294, 1854, 264, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.07000018869127546, "compression_ratio": 1.663265306122449, "no_speech_prob": 6.240813490876462e-06}, {"id": 1147, "seek": 777800, "start": 7797.0, "end": 7803.0, "text": " So it's not really just cross entropy loss. It's actually softmax then cross entropy loss.", "tokens": [407, 309, 311, 406, 534, 445, 3278, 30867, 4470, 13, 467, 311, 767, 2787, 41167, 550, 3278, 30867, 4470, 13], "temperature": 0.0, "avg_logprob": -0.07000018869127546, "compression_ratio": 1.663265306122449, "no_speech_prob": 6.240813490876462e-06}, {"id": 1148, "seek": 780300, "start": 7803.0, "end": 7813.0, "text": " So you probably noticed this but sometimes your predictions from your models will come out looking more like this.", "tokens": [407, 291, 1391, 5694, 341, 457, 2171, 428, 21264, 490, 428, 5245, 486, 808, 484, 1237, 544, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.08667751679937523, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.6441017578472383e-05}, {"id": 1149, "seek": 780300, "start": 7813.0, "end": 7819.0, "text": " Pretty big numbers with negatives in rather than this. Numbers between 0 and 1 that add up to 1.", "tokens": [10693, 955, 3547, 365, 40019, 294, 2831, 813, 341, 13, 22592, 1616, 1296, 1958, 293, 502, 300, 909, 493, 281, 502, 13], "temperature": 0.0, "avg_logprob": -0.08667751679937523, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.6441017578472383e-05}, {"id": 1150, "seek": 780300, "start": 7819.0, "end": 7828.0, "text": " The reason would be that PyTorch it's a PyTorch model that doesn't have a softmax in because we're using cross entropy loss.", "tokens": [440, 1778, 576, 312, 300, 9953, 51, 284, 339, 309, 311, 257, 9953, 51, 284, 339, 2316, 300, 1177, 380, 362, 257, 2787, 41167, 294, 570, 321, 434, 1228, 3278, 30867, 4470, 13], "temperature": 0.0, "avg_logprob": -0.08667751679937523, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.6441017578472383e-05}, {"id": 1151, "seek": 782800, "start": 7828.0, "end": 7834.0, "text": " And so you might have to do the softmax for it.", "tokens": [400, 370, 291, 1062, 362, 281, 360, 264, 2787, 41167, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.09213434381687895, "compression_ratio": 1.7118644067796611, "no_speech_prob": 2.5865905627142638e-05}, {"id": 1152, "seek": 782800, "start": 7834.0, "end": 7838.0, "text": " Fast AI is getting increasingly good at knowing when this is happening.", "tokens": [15968, 7318, 307, 1242, 12980, 665, 412, 5276, 562, 341, 307, 2737, 13], "temperature": 0.0, "avg_logprob": -0.09213434381687895, "compression_ratio": 1.7118644067796611, "no_speech_prob": 2.5865905627142638e-05}, {"id": 1153, "seek": 782800, "start": 7838.0, "end": 7847.0, "text": " Generally if you're using a loss function that we recognize when you get the predictions we will try to add the softmax in there for you.", "tokens": [21082, 498, 291, 434, 1228, 257, 4470, 2445, 300, 321, 5521, 562, 291, 483, 264, 21264, 321, 486, 853, 281, 909, 264, 2787, 41167, 294, 456, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.09213434381687895, "compression_ratio": 1.7118644067796611, "no_speech_prob": 2.5865905627142638e-05}, {"id": 1154, "seek": 782800, "start": 7847.0, "end": 7855.0, "text": " But particularly if you're using a custom loss function that you know might call an n.cross entropy loss behind the scenes or something like that.", "tokens": [583, 4098, 498, 291, 434, 1228, 257, 2375, 4470, 2445, 300, 291, 458, 1062, 818, 364, 297, 13, 35418, 30867, 4470, 2261, 264, 8026, 420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.09213434381687895, "compression_ratio": 1.7118644067796611, "no_speech_prob": 2.5865905627142638e-05}, {"id": 1155, "seek": 785500, "start": 7855.0, "end": 7860.0, "text": " You might find yourself with this situation.", "tokens": [509, 1062, 915, 1803, 365, 341, 2590, 13], "temperature": 0.0, "avg_logprob": -0.13062941860145247, "compression_ratio": 1.4874371859296482, "no_speech_prob": 8.664353117637802e-06}, {"id": 1156, "seek": 785500, "start": 7860.0, "end": 7872.0, "text": " We only have three minutes left but I'm going to point something out to you which is that next week when we finish off tabular which we'll do in like the first 10 minutes.", "tokens": [492, 787, 362, 1045, 2077, 1411, 457, 286, 478, 516, 281, 935, 746, 484, 281, 291, 597, 307, 300, 958, 1243, 562, 321, 2413, 766, 4421, 1040, 597, 321, 603, 360, 294, 411, 264, 700, 1266, 2077, 13], "temperature": 0.0, "avg_logprob": -0.13062941860145247, "compression_ratio": 1.4874371859296482, "no_speech_prob": 8.664353117637802e-06}, {"id": 1157, "seek": 785500, "start": 7872.0, "end": 7882.0, "text": " This is forward in tabular and it basically goes through a bunch of embeddings.", "tokens": [639, 307, 2128, 294, 4421, 1040, 293, 309, 1936, 1709, 807, 257, 3840, 295, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.13062941860145247, "compression_ratio": 1.4874371859296482, "no_speech_prob": 8.664353117637802e-06}, {"id": 1158, "seek": 788200, "start": 7882.0, "end": 7887.0, "text": " It's going to call each one of those embeddings E and you can use it like a function of course.", "tokens": [467, 311, 516, 281, 818, 1184, 472, 295, 729, 12240, 29432, 462, 293, 291, 393, 764, 309, 411, 257, 2445, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.06942037633947425, "compression_ratio": 1.8392857142857142, "no_speech_prob": 4.610640826285817e-05}, {"id": 1159, "seek": 788200, "start": 7887.0, "end": 7890.0, "text": " It's going to pass in each categorical variable to each embedding.", "tokens": [467, 311, 516, 281, 1320, 294, 1184, 19250, 804, 7006, 281, 1184, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.06942037633947425, "compression_ratio": 1.8392857142857142, "no_speech_prob": 4.610640826285817e-05}, {"id": 1160, "seek": 788200, "start": 7890.0, "end": 7896.0, "text": " It's going to concatenate them together into a single matrix.", "tokens": [467, 311, 516, 281, 1588, 7186, 473, 552, 1214, 666, 257, 2167, 8141, 13], "temperature": 0.0, "avg_logprob": -0.06942037633947425, "compression_ratio": 1.8392857142857142, "no_speech_prob": 4.610640826285817e-05}, {"id": 1161, "seek": 788200, "start": 7896.0, "end": 7903.0, "text": " It's going to then call a bunch of layers which are basically a bunch of linear layers.", "tokens": [467, 311, 516, 281, 550, 818, 257, 3840, 295, 7914, 597, 366, 1936, 257, 3840, 295, 8213, 7914, 13], "temperature": 0.0, "avg_logprob": -0.06942037633947425, "compression_ratio": 1.8392857142857142, "no_speech_prob": 4.610640826285817e-05}, {"id": 1162, "seek": 788200, "start": 7903.0, "end": 7907.0, "text": " And then it's going to do our sigmoid trick.", "tokens": [400, 550, 309, 311, 516, 281, 360, 527, 4556, 3280, 327, 4282, 13], "temperature": 0.0, "avg_logprob": -0.06942037633947425, "compression_ratio": 1.8392857142857142, "no_speech_prob": 4.610640826285817e-05}, {"id": 1163, "seek": 788200, "start": 7907.0, "end": 7911.0, "text": " And then there's only two new things we need to learn.", "tokens": [400, 550, 456, 311, 787, 732, 777, 721, 321, 643, 281, 1466, 13], "temperature": 0.0, "avg_logprob": -0.06942037633947425, "compression_ratio": 1.8392857142857142, "no_speech_prob": 4.610640826285817e-05}, {"id": 1164, "seek": 791100, "start": 7911.0, "end": 7921.0, "text": " One is dropout and the other is the n.cont batch norm.", "tokens": [1485, 307, 3270, 346, 293, 264, 661, 307, 264, 297, 13, 9000, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.1549019748217439, "compression_ratio": 1.7727272727272727, "no_speech_prob": 1.9831415556836873e-05}, {"id": 1165, "seek": 791100, "start": 7921.0, "end": 7924.0, "text": " And these are two additional regularization strategies.", "tokens": [400, 613, 366, 732, 4497, 3890, 2144, 9029, 13], "temperature": 0.0, "avg_logprob": -0.1549019748217439, "compression_ratio": 1.7727272727272727, "no_speech_prob": 1.9831415556836873e-05}, {"id": 1166, "seek": 791100, "start": 7924.0, "end": 7930.0, "text": " There are basically batch norm does more than just regularization but amongst other things it does regularization.", "tokens": [821, 366, 1936, 15245, 2026, 775, 544, 813, 445, 3890, 2144, 457, 12918, 661, 721, 309, 775, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.1549019748217439, "compression_ratio": 1.7727272727272727, "no_speech_prob": 1.9831415556836873e-05}, {"id": 1167, "seek": 791100, "start": 7930.0, "end": 7939.0, "text": " And the basic ways you regularize your model are weight decay, batch norm and dropout.", "tokens": [400, 264, 3875, 2098, 291, 3890, 1125, 428, 2316, 366, 3364, 21039, 11, 15245, 2026, 293, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.1549019748217439, "compression_ratio": 1.7727272727272727, "no_speech_prob": 1.9831415556836873e-05}, {"id": 1168, "seek": 793900, "start": 7939.0, "end": 7944.0, "text": " And then you can also avoid overfitting using something called data augmentation.", "tokens": [400, 550, 291, 393, 611, 5042, 670, 69, 2414, 1228, 746, 1219, 1412, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.05023907170151219, "compression_ratio": 1.8904109589041096, "no_speech_prob": 2.144381323887501e-05}, {"id": 1169, "seek": 793900, "start": 7944.0, "end": 7949.0, "text": " So batch norm and dropout we're going to touch on at the start of next week.", "tokens": [407, 15245, 2026, 293, 3270, 346, 321, 434, 516, 281, 2557, 322, 412, 264, 722, 295, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.05023907170151219, "compression_ratio": 1.8904109589041096, "no_speech_prob": 2.144381323887501e-05}, {"id": 1170, "seek": 793900, "start": 7949.0, "end": 7954.0, "text": " And we're also going to look at data augmentation and then we're also going to look at what convolutions are.", "tokens": [400, 321, 434, 611, 516, 281, 574, 412, 1412, 14501, 19631, 293, 550, 321, 434, 611, 516, 281, 574, 412, 437, 3754, 15892, 366, 13], "temperature": 0.0, "avg_logprob": -0.05023907170151219, "compression_ratio": 1.8904109589041096, "no_speech_prob": 2.144381323887501e-05}, {"id": 1171, "seek": 793900, "start": 7954.0, "end": 7962.0, "text": " And we're going to learn some new computer vision architectures and some new computer vision applications.", "tokens": [400, 321, 434, 516, 281, 1466, 512, 777, 3820, 5201, 6331, 1303, 293, 512, 777, 3820, 5201, 5821, 13], "temperature": 0.0, "avg_logprob": -0.05023907170151219, "compression_ratio": 1.8904109589041096, "no_speech_prob": 2.144381323887501e-05}, {"id": 1172, "seek": 793900, "start": 7962.0, "end": 7965.0, "text": " But basically we're very nearly there.", "tokens": [583, 1936, 321, 434, 588, 6217, 456, 13], "temperature": 0.0, "avg_logprob": -0.05023907170151219, "compression_ratio": 1.8904109589041096, "no_speech_prob": 2.144381323887501e-05}, {"id": 1173, "seek": 796500, "start": 7965.0, "end": 7972.0, "text": " You already know how the entirety of collab.py, fastai.collab works.", "tokens": [509, 1217, 458, 577, 264, 31557, 295, 44228, 13, 8200, 11, 2370, 1301, 13, 33891, 455, 1985, 13], "temperature": 0.0, "avg_logprob": -0.09561222791671753, "compression_ratio": 1.7184466019417475, "no_speech_prob": 4.399020690470934e-05}, {"id": 1174, "seek": 796500, "start": 7972.0, "end": 7974.0, "text": " You know why it's there and what it does.", "tokens": [509, 458, 983, 309, 311, 456, 293, 437, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.09561222791671753, "compression_ratio": 1.7184466019417475, "no_speech_prob": 4.399020690470934e-05}, {"id": 1175, "seek": 796500, "start": 7974.0, "end": 7982.0, "text": " And you're very close to knowing what the entirety of tabular model does.", "tokens": [400, 291, 434, 588, 1998, 281, 5276, 437, 264, 31557, 295, 4421, 1040, 2316, 775, 13], "temperature": 0.0, "avg_logprob": -0.09561222791671753, "compression_ratio": 1.7184466019417475, "no_speech_prob": 4.399020690470934e-05}, {"id": 1176, "seek": 796500, "start": 7982.0, "end": 7990.0, "text": " And this tabular model is actually the one that if you run it on Rossman you'll get the same answer that I showed you in that paper.", "tokens": [400, 341, 4421, 1040, 2316, 307, 767, 264, 472, 300, 498, 291, 1190, 309, 322, 16140, 1601, 291, 603, 483, 264, 912, 1867, 300, 286, 4712, 291, 294, 300, 3035, 13], "temperature": 0.0, "avg_logprob": -0.09561222791671753, "compression_ratio": 1.7184466019417475, "no_speech_prob": 4.399020690470934e-05}, {"id": 1177, "seek": 796500, "start": 7990.0, "end": 7992.0, "text": " You'll get that second place result.", "tokens": [509, 603, 483, 300, 1150, 1081, 1874, 13], "temperature": 0.0, "avg_logprob": -0.09561222791671753, "compression_ratio": 1.7184466019417475, "no_speech_prob": 4.399020690470934e-05}, {"id": 1178, "seek": 799200, "start": 7992.0, "end": 7995.0, "text": " Even a little bit better.", "tokens": [2754, 257, 707, 857, 1101, 13], "temperature": 0.0, "avg_logprob": -0.09599603306163441, "compression_ratio": 1.4945054945054945, "no_speech_prob": 6.0072106862207875e-05}, {"id": 1179, "seek": 799200, "start": 7995.0, "end": 8003.0, "text": " I'll show you next week if I remember how I actually ran some additional experiments where I figured out some minor tweaks that can do even slightly better than that.", "tokens": [286, 603, 855, 291, 958, 1243, 498, 286, 1604, 577, 286, 767, 5872, 512, 4497, 12050, 689, 286, 8932, 484, 512, 6696, 46664, 300, 393, 360, 754, 4748, 1101, 813, 300, 13], "temperature": 0.0, "avg_logprob": -0.09599603306163441, "compression_ratio": 1.4945054945054945, "no_speech_prob": 6.0072106862207875e-05}, {"id": 1180, "seek": 799200, "start": 8003.0, "end": 8005.0, "text": " So yeah, we'll see you next week.", "tokens": [407, 1338, 11, 321, 603, 536, 291, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.09599603306163441, "compression_ratio": 1.4945054945054945, "no_speech_prob": 6.0072106862207875e-05}, {"id": 1181, "seek": 800500, "start": 8005.0, "end": 8023.0, "text": " Thanks very much and enjoy the smoke outside.", "tokens": [50364, 2561, 588, 709, 293, 2103, 264, 8439, 2380, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1848952372868856, "compression_ratio": 0.8490566037735849, "no_speech_prob": 3.0969258659752086e-05}], "language": "en"}