{"text": " Welcome to Lesson 14, the final lesson for now. We'll talk at the end about what's next. As you can see from what's increasingly been happening, what's next is very much about you with us rather than us leading you or telling you. We're a community now and we can figure these stuff out together. And obviously USF is a wonderful ally to have. So for now, this is the last of these lessons. One of the things that was great to see this week was this terrific article in Forbes that talked about deep learning education and it was written by one of our terrific students, Maria, and focuses on the great work of some of the students that have come through this course. So I wanted to say thank you very much and congratulations on this great article. I hope everybody will check it out. It's really beautifully written as well and terrific stories. I found it quite inspiring. So today we are going to be talking about a couple of things. We're going to start with time series and structured data. Time series, I wanted to start very briefly by talking about something which I think you basically know how to do. This is a fantastic paper because it is not by deep mind, nobody's heard of it. It actually comes from the Children's Hospital of Los Angeles. Believe it or not, perhaps the epicenter of practical applied AI in medicine today is in Southern California. Specifically, Southern California Pediatrics, the Children's Hospital of Orange County, CHOC, and the Children's Hospital of Los Angeles, CHLA. CHLA, which this paper comes from, actually has this thing they call VPICU, the Virtual Pediatric Intensive Care Unit, where for many many years they've been tracking every electronic signal about how every patient, every kid in the hospital is treated and what all their ongoing sensor readings are. One of the extraordinary things they do is when the doctors there do rounds, data scientists come with them. I don't know anywhere else in the world that this happens. And so a couple of months ago they released a draft of this amazing paper where they talked about how they pulled out all this data from the EMR and from the sensors and attempted to predict patient mortality. The reason this is interesting is that when a kid goes into the ICU, if a model starts saying this kid's looking like they might die, then that's the thing that sets the alarms going and everybody rushes over and starts looking after them. They found that they built a model that was more accurate than any existing model. Those existing models were built on many years of deep clinical input and they used an RNN. Now this kind of time series data is what I'm going to refer to as signal type time series data. So let's say you've got a series of blood pressure readings. So they might be, they come in and their blood pressure is kind of low and it's kind of all over the place and then suddenly it shoots up. In addition to that, maybe there's other readings such as at which points they receive some kind of medical intervention. There was one here and one here and then there was like 6 here and so forth. So these kinds of things, generally speaking, the state of health at time t is probably best predicted by all of the various sensor readings at t-1, t-2, and t-3. So in statistical terms we would refer to that as autocorrelation. Autocorrelation means correlation with previous time periods. And for this kind of signal, I think it's very likely that an RNN is the way to go. Obviously you could probably get a better result using a bidirectional RNN, but that's not going to be any help in the ICU because you don't have the future time period sensors. So be careful of this data leakage issue. And indeed this is what this team at the VPICU, Children's Hospital of Los Angeles, did. They used an RNN to get this state-of-the-art result. I'm not really going to teach you more about this because basically you already know how to do it. You can check out the paper, you'll see there's almost nothing special. The only thing which was quite clever was that their sensor readings were not necessarily equally spaced. For example, did they receive some particular medical intervention? Clearly they're very widely spaced and they're not equally spaced. So rather than having the RNN have basically a sequence of interventions that gets fed to the RNN, instead they actually have two things. One is the signal and the other is the time since the last signal was read. So each point at the RNN, if the RNN is basically just some function f, it's receiving two things. It's receiving the signal at time t and the value of t itself, the difference in time, how long was it since the last one. But that doesn't require any different deep learning, that's just concatenating one extra thing on the other. They actually show mathematically that this makes a certain amount of sense as a way to deal with this, and then they find empirically that it does actually seem to work pretty well. I can't tell you whether this is state of the art for anything because I just haven't seen deep comparative papers or any competitions or anything that really have this kind of data, which is weird because a lot of the world runs on this kind of data. This kind of data, effectively things with it are super valuable. If you're an oil and gas company, what's the drill head telling you, or what's the signals coming out of the pipe telling you, or so on and so forth. But there we go, it's not the kind of cool thing that the Google kids work on. So I'm not going to talk more about that, that's how you can do time series with this kind of signal data. You can also incorporate all of the other stuff we're about to talk about, which is the other kind of time series data. For example, there was a Kaggle competition which was looking at forecasting sales using for each store at this big company in Europe called Rossmann, based on the date and what promotions are going on and what the competitors are doing and so forth. This kind of data is likely to look like this. It's likely to be something like that, or maybe it'll have some kind of trend to it. So these kinds of seasonal time series are very widely analyzed by econometricians. They're everywhere, particularly in business, if you're trying to predict how many widgets you have to buy next month, or whether to increase or decrease your prices. All kinds of operational type things tend to look like this, how full your planes are going to be, whether you should add promotions, so on and so forth. So it turns out that the state of the art for this kind of approach is not necessarily to use an RNN. I'm actually going to look at the 3rd place result from this competition, because the 3rd place result was nearly as good as places 1 and 2, but way, way, way, way simpler. And also it turns out that there's stuff that we can build on top of for almost every model of this kind. And basically, surprise, surprise, it turns out that the answer is to use a neural network. So I need to warn you again, what I'm going to teach you here is very, very uncool. You'll never read about it from DeepMind or OpenAI. It doesn't involve any robot arms, it doesn't involve thousands of GPUs. It's the kind of boring stuff that normal companies use to make more money or spend less money or satisfy their customers. So I apologize deeply for that oversight. Having said that, in the 25 years or more I've been doing machine learning work applied in the real world, 98% of it has been this kind of data. Whether it be when I was working in agriculture, I've worked in wool, macadamia nuts, and rice, and we were figuring out how full our barrels were going to be, whether we needed more, we were figuring out how to set futures markets prices for agricultural goods, whatever. I worked in mining and brewing, which required analyzing all kinds of engineering data and sales data. I've worked in banking that required looking at transaction account pricing and risk and fraud. All of these areas basically involve this kind of data. So I think although no one publishes stuff about this, because anybody who comes out of a Stanford PhD and goes to Google doesn't know about any of those things, I guess, it's probably the most useful thing for the vast majority of people. And excitingly, it turns out that you don't need to learn any new techniques at all. In fact, the model that they got this third place result with, a very very simple model, is basically one where each different categorical variable was one-hot encoded and chucked into an embedding layer. The embedding layers were concatenated and chucked through a dense layer, and then a second dense layer, and then went through a sigmoid function into an output layer. So very simple. The continuous variables, they haven't drawn here. All these pictures are going to come straight from this paper, which the folks that came third kindly actually wrote a paper about this. The continuous variables basically get fed directly into the dense layer. So that's the structure of the model. How well does it work? So the short answer is, compared to k-nearest neighbors, random forests and GBMs, just a simple neural network beats all of those approaches, just with standard one-hot encoding. But then the EE is entity embeddings. So adding in this idea of using embeddings, interestingly you can take the embeddings trained by a neural network and feed them into a KNN or a random forest or a GBM. In fact, using embeddings with every one of those things is way better than all of those things are anything other than neural networks. So that's pretty interesting. And then if you use the embeddings with a neural network, you get the best result still. So this actually is kind of fascinating because training this neural network took me some hours on a Titan X, whereas training the GBM took I think less than a second. It was so fast I thought I had screwed something up. And then I tried running it, it was like, holy shit, it's giving accurate predictions. So GBMs and random forests are so fast. So in your organization, you could try taking everything that you could think of as a categorical variable and once a month train a neural net with embeddings and then store those embeddings in a database table and tell all of your business users, hey, anytime you want to create a model that incorporates day of week or store ID or customer ID, you can go grab the embeddings. So they're basically like word vectors, but they're customer vectors and store vectors and product vectors. So I've never seen anybody write about this other than this paper. And even in this paper, they don't really get to this hugely important idea of what you could do with these embeddings. What's the difference between A and B and C? Is it different data types flowing in? We're going to get to that in a moment. Basically the different things are like A might be the store ID, B might be the product ID, and C might be the day of week. One of the really nice things that they did in this paper was to then draw some projections of some of these embeddings. They just used t-sneak, it doesn't really matter what the projection method is. But here's some interesting ideas. They took each state of Germany, this is all based in Germany, and they did a projection of the embeddings from the state field. And here are those projections. And I've drawn around them different colored circles, and you might notice the different colored circles exactly correspond to the different colored circles on a map of Germany. Now these were just random embeddings trained with SGD trying to predict sales in stores at Rossmann. And yet somehow they've drawn a map of Germany. So obviously the reason why is because things close to each other in Germany have similar behaviors around how they respond to events, who buys what kinds of products, and so on and so forth. So that's like crazy fascinating. Here's the kind of bigger picture. Every one of these dots is the distance between two stores. And this shows the correlation between the distance in embedding space versus the actual distance between the stores. So you can basically see that there's a strong correlation between things being close to each other in real life and close to each other in these SGD trained embeddings. Here's a couple more pictures. All the lines are drawn on top of mine, but everything else is just from the paper. On the left is days of the week embedding. And you can see the days of the week that are near each other have ended up embedded close together. On the right is months of the year embedding. Again, same thing. And you can see that the weekend is clearly separate. So that's where we're going to get to. And I'm actually going to take you through the end-to-end process. I rebuilt the end-to-end process from scratch and tried to make it in as few lines as code as possible because we just haven't really looked at any of these structured data type problems before. So it's kind of a very different process, even a different set of techniques. So we import the usual stuff. When you try to do this stuff yourself, you'll find 3 or 4 libraries we haven't used before. So when you hit something that says module not found, you can just pip install all these things. They're pure Python. We'll talk about them as we get to them. So the data that comes from Kaggle comes down as a bunch of CSV files. I wrote a quick thing to combine some of those CSVs together. This was one of those competitions where people were allowed to use additional external data as long as they were shared on the forum. So the data I'll share with you, I'm going to combine it all into one place for you. So I've commented these out because the stuff I'll give you will have already run this concatenation process. So the basic tables that you're going to get access to is the training set itself, a list of stores, a list of which state each store is in, a list of the abbreviation and the name of each state in Germany, a list of data from Google Trends. So if you've used Google Trends, you can basically see how particular keywords change over time. I don't actually know which keywords they used, but somebody found that there were some Google Trends keywords that correlated well, so we've got access to those. Some information about the weather and then a test set. So I'm not sure that we've really used Pandas much if at all yet, so let's talk a bit about Pandas. Pandas lets us take this kind of structured data and manipulate it in similar ways to the way you would manipulate it in a database. So Pandas, just like NumPy, tends to become NP, Pandas tends to become PD. So pd.read.csv is going to return a data frame. So a data frame is like a database table. If you've used R, it's called the same thing. So this read.csv is going to return a data frame containing the information from this CSV file. We're going to go through each one of those table names and read the CSV. So this list comprehension is going to return a list of data frames. So I can now go ahead and display the head, so the first 5 rows from each table. That's a good way to get a sense of what these tables are. So here's the first one, the trading set. So for some store, on some date, they had some level of sales to some number of customers. They were either open or closed, they either had a promotion on or they didn't, it either was a holiday or it wasn't, first date and school and then some additional information about the date. So that's the basic information we have. And then everything else we join onto that. So for example, for each store, we can join up some information, some kind of categorical variable about what kind of store it is. I have no idea what this is, it might be a different brand or something. What kinds of products do they carry? Again, it's just a letter, I don't know what it means, but maybe it's like some are electronics, maybe some are supermarkets, maybe some are full-spectrum. How far away is the nearest competitor? And what year and month did the competitor open for business? Notice that sometimes the competitor opened for business quite late in the game, like later than some of the data we're actually looking at. So that's going to be a little bit confusing. And then this thing called promo2, which as far as I understand it is basically is this a store which has some kind of standard promotion timing going on. So you can see here that this store has standard promotions in January, April, July and October. So that's the stores. We also know for each store what state they're in based on the abbreviation. And then we can find out for each state what is the name of that state. And then for each state, this is slightly weird, this is the state abbreviation, the last 2 letters. In this state during this week, this was the Google Trend data for some keyword, I'm not sure what keyword it was. For this state name on this date, here's the temperature, view point, so forth. And then finally here's the test set. It's identical to the training set, but we don't have the number of customers and we don't have the number of sales. So this is a pretty standard kind of industry data set. We've got a central table, various tables related to that, and some things representing time periods or time points. One of the nice things you can do in Pandas is to use this Pandas summary module and call data frame summary for table.summary and that will return a whole bunch of information about every field. So I'm not going to go through all of it in detail, but you can see for example for the sales, on average 5800 sales, standard deviation of 3800, sometimes the sales goes all the way down to 0, sometimes all the way up to 41000. There's no missing to sales, that's good to know. So this is the kind of thing that's good to scroll through and identify. Competition open since month is missing about a third of the time, that's good to know. There's 12 unique states, that might be worth checking because there's actually 16 things in our state table for some reason. Google Trend data is never missing, that's good. The year goes from 2012 through 2015. The weather data is never missing. And then here's our test set. This is the kind of thing that might screw up a model, it's like actually sometimes the test set is missing the information about whether that store was open or not. So we can take that list of tables and just destructure it out into a whole bunch of different table names, find out how big the training set is, find out how big the test set is. And then with this kind of problem, there's going to be a whole bunch of data cleaning, and a whole bunch of feature engineering. And so neural nets don't make any of that go away, particularly because we're using this style of neural net where we're basically feeding in a whole bunch of separate continuous and categorical variables. So simplify things a bit, turn state holidays into Booleans. And then I'm going to join all of these tables together. I always use a default join type of an outer join. So you can see here, this is how we join in pandas. We say table.merge table2, and then to make a left outer join, how equals left. And then you say, what's the name of the fields that you're going to join on the left-hand side, what are the fields you're going to join on the right-hand side. And then if both tables have some fields with the same name, what are you going to suffix those fields with. So on the left-hand side we're not going to add any suffix, on the right-hand side we'll put in underscore y. So again, I try to refactor things as much as I can, so we're going to join lots of things. Let's create one function to do the joining, and then we can call it lots of times. Was there any fields referring to the same file but named differently? Not that I saw, no. And it wouldn't matter too much if there were because when we run the model, no problem. Question from the forum. Would you liken the use of embeddings from a neural network to extraction of implicit features, or can we think of it more like what a PCA would do, like dimensionality reduction? Let's talk about it more when we get there. Basically, when you deal with categorical variables in any kind of model, you have to decide what to do with them. One of my favorite data scientists, or a pair of them actually, who are very nearly neighbors of Rachel and mine have this fantastic R package called Vtreat which has a bunch of state-of-the-art approaches to dealing with stuff like categorical variable encoding. And so the obvious way to do categorical variable encoding is to just do a one-hot encoding, and that's the way nearly everybody puts it into their gradient boosting machines or random forests or whatever. But one of the things that Vtreat does is it has some much more interesting techniques. So for example, you could look at the univariate mean of sales for each day of week, and you could encode day of week using a continuous variable which represents the mean of sales. But then you have to think about, would I take that mean from the training set or the test set or the validation set? How do I avoid overfitting? There's all kinds of complex statistical subtleties to think about that Vtreat handles all this stuff automatically. There's a lot of great techniques, but they're kind of complicated and in the end they tend to make a whole bunch of assumptions about linearity or univariate correlations or whatever. Whereas with embeddings, we're using SGD to learn how to deal with it. Just like we do when we build an NLP model or a collaborative filtering model, we provide some initially random embeddings and the system learns how the movies vary and compare to each other, or users vary or words vary or whatever. So this is to me the ultimate pure technique. And of course the other nice thing about embeddings is we get to pick the dimensionality of the embedding so we can decide how much complexity and how much learning are we going to put into each of the categorical variables. We'll see how to do that in a moment. So one complexity was that the weather uses the name of the state rather than the abbreviation of the state, so we can just go ahead and join weather to states to get the abbreviation. The Google Trend information about the week from A to B, we can split that apart. You can see here one of the things that happens in the Google Trend data is that one of the states is called ni, whereas the rest of the data is called hb, ni. So this is a good opportunity to learn about pandas indexing. So pandas indexing, most of the time you want to use this.ix method. And the.ix method is your general indexing method. It's going to take two things, a list of rows to select and a list of columns to select. And you can use it in pretty standard intuitive ways. This is a lot like NumPy. This here is going to return a list of Booleans, which things are in this state. And if you pass the list of Booleans to the pandas row selector, it will just return the rows where that Boolean is true. So therefore this is just going to return the rows from Google Trend where Google Trend.state is ni. And then the second thing we pass in is a list of columns. In this case we just got one column. And one very important thing to remember, again just like NumPy, you can put this kind of thing on the left-hand side of an equal sign. In computer science we call this an L-value. So we can take this state field, four things which are equal to ni, and change their value to this. So this is like a very nice, simple technique that you'll use all the time in pandas, both for looking at things and for changing things. Question- In this particular example, do you think the granularity of the data matter, as in per day or per week? Is one better than the other? Answer. I would want to have the lower granularity so that I can capture that. Ideally you'd want time as well. It kind of depends how the organization is going to use it. What are they going to do with this information? It's probably for purchasing and stuff, so maybe they don't care about an hourly level. But clearly the difference between Sunday sales and Wednesday sales would be quite significant. So this is mainly a business context or domain understanding question. Question- Do you know if there's any work that compares for structured data supervised embeddings like these to embeddings that come from an unsupervised paradigm such as an auto-encoder? It seems like you'd get more useful for prediction embeddings with the former case, but if you wanted general purpose embeddings you might prefer the latter. Answer. I think you guys are aware of my feelings about auto-encoders. It's like giving up on life. You can always come up with a loss function that's more interesting than an auto-encoder loss function. I would be very surprised if embeddings that came from a sales model were not more useful for just about everything than something that came from an unsupervised model. These things are easily tested, and if you do find a model that they don't work as well with, then you can come up with a different set of supervised embeddings for that model. Question- There's also just a note that.ix is deprecated and we should use.loc instead. Answer. I was going to mention Pandas is changing a lot. Because I've been running this course, I have not been keeping track of the recent versions of Pandas. So that should be google trend.log. In Pandas there's a whole page called Advanced Indexing Methods. I don't find the Pandas documentation terribly clear, to be honest, but there is a fantastic book by the author of Pandas called Python for Data Analysis. There is a new edition out, and it covers Pandas, NumPy, Matplotlib, whatever. That's the best way by far to actually understand Pandas, because the documentation is a bit of a nightmare and it keeps changing, so the new version has all the new stuff in it. With these kind of indexing methods, Pandas tries really hard to be intuitive, which means that quite often you'll read the documentation for these methods and it'll say, if you pass it a boolean, it'll behave in this way. If you pass it a float, it'll behave this way. If it's an index, it's this way, unless this other thing happens. I don't find it intuitive at all because in the end I need to know how something works in order to use it correctly. You end up having to remember this huge list of things. I think Pandas is great, but this is one thing to be very careful of, is to really make sure you understand how all these indexing methods actually work. I know Rachel's laughing because she's been there and probably laughing in disgust at what we all have to go through. Question. When you use embeddings from a supervised model in another model, do you have to worry about data leakage? Yes, you always have to worry about data leakage. I think that's a great point. I don't think I've got anything to add to that. You can figure out easily enough if there's data leakage. That's a great question and definitely something to think about. So there's this kind of standard set of steps that I take for every single structured machine learning model I do. One of those is every time I see a date, I always do this. I always create 4 more fields. The year, the month of year, the week of year, and the day of week. This is something which should be automatically built into every data loader, I feel. It's so important because these are the kinds of structures that you see. Once every single date has got this added to it, you're doing great. So you can see that I add that into all of my tables that have a date field. So we'll have that from now on. So now I go ahead and do all of these outer joins. You'll see that the first thing I do after every outer join is check whether the thing I just joined with has any nulls. So even if you're sure that these things match perfectly, I would still never ever do an inner join. Do the outer join and then check for nulls. That way if anything changes ever or if you ever make a mistake, one of these things will not be 0. If this was happening in a production process, this would be an assert. This would be emailing Henry at 2am to say something you're relying on is not working the way it was meant to look out. So that's why I always do it this way. So you can see I'm just basically joining my training to everything else until it's all in there together in one big thing. So that table, everything joined together, is called joined. And then I do a whole bunch more thinking about the people that won this competition, and then I replicated their results from scratch. Think about what are all the other things you might want to do with these dates. So competition open, we noticed before, a third of the time they're empty. So we just fill in the empties with some kind of sentinel value. Because a lot of machine learning systems don't like missing values. Fill in the missing months with some sentinel value. Again, keep on filling in missing data. So fill in A is a really important thing to be aware of. Question. Is the filling in the month with 1 a real value, isn't that a problem? I guess the answer is yes, it is a problem. But in this case I happen to know that every time year is empty, month is also empty, and we only ever use both of them together. So any model, whether it be tree-based or neural net-based or whatever, is going to take advantage of that fact. Probably would have been safer to pick something else. So we don't really care actually when the competition store was opened. What we really care about is how long is it between when they were opened and the particular row that we're looking at. The sales on the 2nd of February 2014, how long was it between 2nd of February 2014 and when the competition opened. So you can see here we use this very important.apply function which just runs a Python function on every row of a data frame. In this case the function is to create a new date from the open since year and the open since month. We're just going to assume that it's the middle of the month. That's our competition open since. Then we can get our days open by just doing a subtract. In Pandas, every date field has this special magical dt property, which is what all the days, month, year, all that stuff sits inside this little dt property. Now sometimes, as I mentioned, the competition actually opened later than the particular observation we're looking at. So that would give us a negative, so if we replace our negatives with 0. We're going to use an embedding for this, so that's why we replace days open with months open, so we have less values. I didn't actually try replacing this with a continuous variable. I suspect it wouldn't make too much difference, but this is what they did. In order to make the embedding again not too big, they replaced anything that was bigger than 2 years with 2 years. So there's our unique values. Every time you do something, print something out to make sure that the thing you thought you did is what you actually did. It's much easier if we were using Excel because you see straight away what you're doing. In Python, this is the kind of stuff that you have to really be rigorous about checking your work at every step. When I build stuff like this, I generally make at least one error in every cell, so check carefully. Do the same thing for the promo days, turn those into weeks. So that's some basic preprocessing. You get the idea of how Pandas works, hopefully. So the next thing that they did in the paper was a very common kind of time series feature manipulation, one to be aware of. They basically wanted to say, every time there's a promotion, every time there's a holiday, I want to create some additional fields for every one of our training set roads, which is on a particular date. On that date, how long is it until the next holiday? How long is it until the previous holiday? How long is it until the next promotion? How long is it since the previous promotion? So if we basically create those fields, this is the kind of thing which is super difficult for any GBM or random forest or neural net to figure out how to calculate itself. There's no obvious kind of mathematical function that it's going to build on its own. So this is the kind of feature engineering that we have to do in order to allow us to use these kinds of techniques effectively on time series data. So a lot of people who work with time series data, particularly in academia, outside of industry, they're just not aware of the fact that the state-of-the-art approaches really involve all these heuristics. Separating out your dates into their components, turning everything you can into durations, both forwards and backwards, and also doing a bunch of, you'll see in a moment, running averages. So when I used to do a lot of this kind of work, I had a bunch of library functions that I would run on every file that came in and would automatically do these things for every combination of dates. So this thing of how long until the next promotion, how long since the previous promotion, is not easy to do in any database system pretty much, or indeed in pandas. Because generally speaking, these kind of systems are looking for relationships between tables, but we're trying to look at relationships between rows. So I had to create this tiny little simple little class to do this. So basically what happens is, let's say I'm looking at school holiday. So I sort my data frame by store and then by date, and I call this little function called add elapsed, school holiday, after. What does add elapsed do? Add elapsed is going to create an instance of this class called elapsed, and in this case it's going to be called with school holiday. So what this class is going to do, we're going to be calling this apply function again, it's going to run on every single row, and it's going to call my elapsed class.det for every row. So I'm going to go through every row in order of store, in order of date, and I'm going to try to find how long has it been since the last school holiday. So when I create this object, I just have to keep track of what field is it. School holiday, initialize. When was the last time we saw a school holiday? The answer is we haven't, so initialize it to not a number. And we also have to know each time we cross over to a new store. When we cross over to a new store, we have to reinitialize. So the previous store was 0. So every time we call get, we basically check, have we crossed over to a new store, and if so, just initialize both of those things back again. And then we just say, is this a school holiday? If so, then the last time you saw a school holiday is today. And then finally return, how long is it between today and the last time you saw a school holiday. So basically this class is a way of keeping track of some memory about when did I last see this observation. So then by just calling df.apply, it's going to keep track of this for every single row, and so then I can call that for school holiday after and before, the only difference being that for before I just sort my dates in descending order. State holiday and promo. So that's going to add in the end 6 fields, how long until and how long since the last school holiday, state holiday, and promotion. And then there's 2 questions. One asking, is this similar to a windowing function? Not quite, we're about to do a windowing function. And then is there a reason to think that the current approach would be problematic with sparse data? I don't see why, but I'm not sure I quite follow. So we don't care about absolute dates, we care about time deltas between events. We care about 2 things. We do care about the dates, but we care about what year is it, what day of the week it is. And we also care about the elapsed time between the date I'm predicting sales for and the previous and next of various events. And then windowing functions. For the features that are time until an event, how do you deal with that given that you might not know when the last event is in the data? Well all I do is I've sorted descending, and then we initialize last with not a number. So basically when we then go subtract, here we are, subtract, and it tries to subtract not a number, we'll end up with a null. So basically anything that's at an unknown time because it's at one end or the other is going to end up null, which is why we're going to replace those nulls with zeros. Pandas has this slightly strange way of thinking about indexes, but once you get used to it it's fine. At any point you can call dataFrame.setIndex and pass in a field. You then have to just remember what field you have as the index because quite a few methods in pandas use the currently active index by default, and of course things will run faster when you do stuff with the currently active index. And you can pass multiple fields, in which case you end up with a multiple key index. So the next thing we do is these windowing functions. So a windowing function in pandas, we can use this rolling. So this is like a rolling mean, rolling min, rolling max, whatever you like. So this basically says, alright, let's take our data frame with the columns we're interested in, school, holidays, holiday, and promo, and we're going to keep track of how many holidays are there in the next week and the previous week. How many promos are there in the next week and the previous week. To do that we can sort by date, group by store, and then rolling will be applied to each group. So within each group, create a rolling 7-day sum. So it's kind of like, it's the kind of notation I'm never likely to remember, but you can just look it up. So this is how you do group by type stuff. Pandas actually has quite a lot of time series functions. This rolling function is one of the most useful ones. Wes McKinney had a background as a quant with memory serves correctly, so quants love their time series functions, so I think that was a lot of the history of pandas. So if you're interested in time series stuff, you'll find a lot of time series stuff in pandas. One helpful parameter that sits inside a lot of methods is inplace equals true. That means that rather than returning a new data frame with this change made, it changes the data frame you already have. And when your data frames are quite big, this is going to save a lot of time and memory. That's a good little trick to know about. So now we merge all these together and we can now see that we've got all these after-school holiday, before-school holiday, and our backward and forward running means. And then we join that up to our original data frame, and here we have our final result. So there it is. We started out with a pretty small set of fields in the training set, but we've done this feature engineering. This feature engineering is not arbitrary. Although I didn't create this solution, I was just re-implementing the solution that came from the competition 3rd place getters. This is nearly exactly the set of feature engineering steps I would have done. It's just a really standard way of thinking about a time series. So you can definitely borrow these ideas pretty closely. So now that we've got that, we've got this table, we've done our feature engineering, we now want to feed it into a neural network. So to feed it into a neural network, we have to do a few things. The categorical variables have to be turned into one-hot encoded variables, or at least into contiguous integers. And the continuous variables we probably want to normalize to a zero-mean one-statter deviation. There's a very little-known package called sklearnpandas, and actually I contributed some new stuff to it for this course to make this even easier to use. If you use this data frame mapper from sklearnpandas, as you'll see, it makes life very easy. Without it, life is very hard. And because very few people know about it, the vast majority of code you'll find on the Internet makes life look very hard. So use this code, not the other code. Actually I was talking to some of the students the other day and they were saying for their project they were stealing lots of code from part 1 of the course because they just couldn't find anywhere else people writing any of the kinds of code that we've used. The stuff that we've learned throughout this course is on the whole not code that lives elsewhere very much at all. So feel free to use a lot of these functions in your own work, because I've really tried to make them the best version of that function. So one way to do the embeddings and the way that they did it in the paper is to basically say for each categorical variable they just manually decided what embedding dimensionality to use. They don't say in the paper how they picked these dimensionalities. But generally speaking things with a larger number of separate levels tend to have more dimensions. So I think there's like 1000 stores, so that has a big embedding dimensionality. Where else obviously things like promo, forward and backward, or day of week or whatever have much smaller ones. So this is this dictionary I created that basically goes from the name of the field to the embedding dimensionality. Again this is all code that you guys can use in your models. So then all I do is I say, my categorical variables is go through my dictionary, sort it in reverse order of the value, and then get the first thing from that. So that's going to give me the keys from this in reverse order of dimensionality. Continuous variables, it's just a list. Just make sure that there's no nulls. So continuous variables replace nulls with zeros. Categorical variables replace nulls with empties. And then here's where we use the data frame mapper. A data frame mapper takes a list of tuples, a list of tuples with just 2 items in. The first item is the name of the variable. So in this case I'm looping through each categorical variable name. The second thing in the tuple is a class, or actually an instance of a class, which is going to do your preprocessing. And there's really just 2 that you're going to use almost all the time. For categorical variables, sklearn comes with something called labelencoder. It's really badly documented, in fact really misleadingly documented. But this is exactly what you want. It's something that takes a column, figures out what are all the unique values that appear in that column, and replaces them with a set of contiguous integers. So if you've got the days of the week, Monday through Sunday, it'll replace them with zeros through sevens. And then very importantly, this is critically important, you need to make sure that the training set and the test set have the same codes. There's no point in having Sunday be zero in the training set and one in the test set. So because we're actually instantiating this class here, this object is going to actually keep track of which codes it's using. And then ditto for the continuous, we want to normalize them to a 0, 1 variable. But again we need to remember what was the mean that we subtracted, what was the standard deviation we divided by, so that we can do exactly the same thing to the test set. Otherwise again our models are going to be nearly totally useless. So the way the data frame mapper works is that it's using this instantiated object, it's going to keep track of this information. So this is basically code you can copy and paste in every one of your models. Once we've got those mappings, you just pass those to a data frame mapper, and then you call.fit passing in your data set. So this thing now is a special object which has a.features property that's going to contain all of the features, all of the pre-processed features that you want. So categorical columns contains the result of doing this mapping, basically doing this labeling coding. So in some ways the details of how this works doesn't matter too much because you can just use exactly this code in every one of your models. Same for continuous, it's exactly the same code. But of course for continuous, it's going to be using standard scalar, which is the Skykit learn thing that turns it into a 0 mean, 1 standard deviation variable. So we've now got continuous columns that have all been standardized. Here's an example of the first 5 rows from the 0th column for a categorical and then ditto for a continuous. So you can see these have been turned into integers and these have been turned into numbers which are going to average to 0 and have a standard deviation of 1. One of the nice things about this data frame mapper is that you can now take that object and actually store it, pickle it, and so now you can use those categorical encodings and scaling parameters elsewhere. By just unpickling it, you've immediately got those same parameters. For my categorical variables, you can see here the number of unique classes in every one. So here's my 1100 stores and 31 days of the month and 7 days of the week and so forth. So that's the kind of key preprocessing that has to be done. So here is their big mistake. And I think if they didn't do this big mistake, they probably would have won. Their big mistake is that they went join.sales not equal to 0. So they've removed all of the rows with no sales. Those are all of the rows where the store was closed. Why was this a big mistake? Because if you go to the Rosman Store Sales Competition website and click on Kernels and look at the kernel that got the highest rating, Exploratory Data Analysis Rosman, I'll show you a couple of pictures. Here is an example of a store, store 708, and these are all from this kernel. Here is a period of time where it was closed to refurbishment. This happens a lot in Rosman stores. You get these periods of time when you get zeros for sales, lots in a row. Look what happens immediately before and after. So in the data set that we're looking at, our unfortunate 3rd place winners deleted all of these. So they had no ability to build a feature that could find this. So this store 708. Look, here's another one where it was closed. Same thing. So this turns out to be super common. And the 2nd place winner actually built a feature. It's going to be exactly the same feature we've seen before. How many days since the closing and how many days until the next closing. If they had just done that, I'm pretty sure they would have won. So that was their big mistake. This kernel has a number of interesting analyses in it. Here's another one which I think our neural net can capture, although it might have been better to be explicit. Some stores opened on Sundays. Most didn't, but some did. And for those stores that opened on Sundays, their sales on Sundays were far higher than on any other day. I guess that's because in Germany not many shops open on Sundays. So something else that they didn't explicitly do was create a like, is store open on Sunday field. Having said that, I think the neural net may have been able to put that in the embedding. So if you're interested during the week, you could try adding this field and see if it actually improves it or not. It'd certainly be interesting to hear if you try adding this field. Do you find that you actually would win the competition? This Sunday thing, these are all from the same Kaggle kernel. Here's the day of week, and here's the sales as a box plot. You can see normally on a Sunday, it's not that the sales are much higher. So it's really explicitly just for these particular stores. So that's the kind of visualization stuff which is really helpful to do as you work through these kinds of problems. Just draw lots of pictures. And those pictures were drawn in R. R is actually pretty good for this kind of structured data. So for categorical fields, they're converted by the numbers, not with mean 0. They were just Monday 0, Tuesday 1, whatever. As ease, they will send to neural network. We're going to get there. We're going to use embeddings. Just like we did with word embeddings, remember we turned every word into a word index. So our sentences rather than being like the dog ate the beans, it would be 3, 6, 12, 2. We're going to do the same basic thing. We've done the same basic thing. So now that we've done our terrible mistake, we've still got 844,000 rows left. As per usual, I made it really easy for me to create a random sample and did most of my analysis with a random sample, but can just as easily not do the random sample. So now I've got a separate sample version of it. Split it into training and test. Now notice here, the way I split it into training and test is not randomly. And the reason it's not randomly is because in the Kaggle competition, they set it up the smart way. The smart way to set up a test set in a time series is to make your test set the most recent period of time. If you choose random points, you've got two problems. The first is you're predicting tomorrow's sales where you always have the previous day's sales, which is very rarely the way things really work. And then secondly, you're ignoring the fact that in the real world, you're always trying to model a few days or a few weeks or a few months in the future that haven't happened yet. So the way you want to set up, if you were building, if you were setting up the data for such a model yourself, you would need to be deciding how often am I going to be rerunning this model, how long is it going to take for those model results to get into the field to be used and however they're being used. In this case, I guess they decide, I can't remember, I think it's like a month or two. So in that case I should make sure there's a month or two gap or a month or two test set, which is the last bit. So you can see here I've taken the last 10% of my validation set and it's literally just here's the first bit and here's the last bit. And since it was already sorted by date, this ensures that I have it done the way I want. This is how you take that data frame map or object we created earlier, we called.fit, in order to learn the transformation parameters, you then call transform to actually do it. So take my training set and transform it to grab the categorical variables, and then the continuous preprocessing is the same thing for my continuous map. So preprocess my training set and grab my continuous variables. So that's nearly done. The only final piece is in their solution, they modified their target, their sales value. And the way they modified it was that they found what is the highest amount of sales, and they took the log of that, and then they modified all of their Y values to take the log of sales divided by the maximum log of sales. So what this means is that the Y values are going to be no higher than 1. And furthermore, remember how they had a long tail, the average was 5000, but the maximum was like 40-something thousand. This is really common. Most financial data, sales data, so forth, generally has a nicer shape when it's logged than it does not. So taking a log is a really good idea. The reason that as well as taking the log, they also did this division, is it means that what we can now do is we can use an activation function in our neural net of a sigmoid, which goes between 0 and 1, and then just multiply by the maximum log. So that's basically going to ensure that the data is in the right scaling area. I actually tried taking this out, and this technique doesn't really seem to help. It actually reminds me of the style transfer paper where they mentioned they originally had a hyperbolic tan layer at the end for exactly the same reason, to make sure everything was between 0 and 255. And it turns out if you just use a linear activation, it worked just as well. So interestingly, this idea of using sigmoids at the end in order to get the right range doesn't seem to be that helpful. My guess is the reason why is because for a sigmoid it's really difficult to get the maximum. And I think actually what they should have done is they probably should have, instead of using maximum, they should have used maximum times 1.25, so that they never have to predict 1, because it's impossible to predict 1 because it's a sigmoid. Someone asked, is there any issue in fitting the preprocessors on the full training and validation data, shouldn't they be fit only to the training set? No, it's fine. In fact, for the categorical variables, if you don't include the test set, then you're going to have some codes that aren't there at all. Or else this way they're just going to be random, which is better than failing. As for deciding what to divide and subtract in order to get a 0 or 1 random variable, it doesn't really matter. There's no leakage involved, that's what you're worried about. Crit mean squared percent error is what the Kaggle competition used as the official loss function. So this is just calculating that. So before we take a break, we'll just finally take a look at the definition of the model. I'll kind of work backwards. Here's the basic model. Get our embeddings, combine the embeddings with the continuous variables, a tiny bit of dropout, 1 dense layer, 2 dense layers, more dropout, and then the final sigmoid activation function. You'll see that I've got commented out stuff all over the place. This is because I had a lot of questions about some of the details. Why did they do things certain ways? Some of the things they did were so weird, I just thought they couldn't possibly be right. So I did some experimenting. We'll learn more about that in a moment. So the embeddings, as per usual, I create a little function to create an embedding which first of all creates my regular Keras input layer, and then it creates my embedding layer, and then how many embedding dimensions am I going to use? Sometimes I look them up in that dictionary I had earlier, and sometimes I calculated them using this simple approach of saying I will use however many levels there are in the categorical variable, divide it by 2 with a maximum of 50. These were 2 different techniques I was playing with. Normally with word embeddings, you have a whole sentence. So you've got to feed it to an RNN, and so you have time steps. So normally you have an input length equal to the length of your sentence. This is the time steps for an RNN. We don't have an RNN. We don't have any time steps. We just have one element in one column. So therefore I have to pass flatten after this because it's going to have this redundant unit 1 time axis that I don't want. So this is just because people don't normally do this kind of stuff with embeddings, so they're assuming that you're going to want it in a format ready to go to an RNN, so this is just turning it back into a normal format. So we grab each embedding, we end up with a whole list of those. We then combine all of those embeddings with all of our continuous variables into a single list of variables. And so then our model is going to have all of those embedding inputs and all of our continuous inputs, and then we can compile it and train it. So let's take a break and see you back here at 5 past 8. So we've got our neural net set up. We train it in the usual way, go.fit, and away we go. So that's basically that. It trains reasonably quickly, 6 minutes in this case. Question from the audience. One of them is, for the normalization, is it possible to use another function other than log such as sigmoid? I don't think you'd want to use sigmoid. That kind of financial data and sales data and stuff tends to be of a shape where log will make it more linear, which is generally what you want. And then when we log transform our target variable, we're also transforming the squared error. Is this a problem or is it helping the model to find a better minimum error in the untransformed space? So you've got to be careful about what loss function you want. In this case, the Calc competition is trying to minimize root and mean squared percent error. So I actually then said, I want you to do mean absolute error. Because in log space, that's basically doing the same thing. The percent is a ratio, so this is the absolute error between two logs which is basically the same as a ratio. So you need to make sure your loss function is appropriate in that space. I think this is one of the things they didn't do in the original competition. As you can see I tried changing it and I think it helped. By the way, XGBoost is fantastic. Here is the same series of steps to run this model with XGBoost. As you can see, I just concatenate my categorical and continuous for training and for my validation set. Here is a set of parameters which tends to work pretty well. XGBoost has a data type called D matrix, a data matrix, which is basically a normal matrix but it keeps track of the names of the features so it prints out better information. Then you go.train and this takes less than a second to run and it's not massively worse than our previous result. So this is a good way to get started. The reason that XGBoost and Random Forest is particularly helpful is because it does something called variable importance. So this is how you get the variable importance for an XGBoost model. So it takes a second and suddenly here is the information you need. So when I was having trouble replicating the original results from the 3rd place winners, one of the things that helped me a lot was to look at this feature importance plot and say, OK, competition distance, holy cow, that's really really important, let's make sure that my competition distance pre-processing results really is exactly the same. On the other hand, events doesn't really matter at all, so I'm not going to worry at all about checking my events. This feature importance or variable importance plot, also as it's known, you can also create with a random forest. These are amazing. Because you're using a tree ensemble, it doesn't matter the shape of anything, it doesn't matter if you have or don't have interactions, this is all totally assumption-free. In real life, this is the first thing I do. The first thing I do is try to get a feature importance plot printed. Because that tells me, often it turns out there's only 3 or 4 variables that matter. Like if you've got 10,000 variables, so I worked on a big credit scoring problem a couple of years ago, I had 9,500 variables, it turned out that only 9 of them mattered. So the company I was working for, literally it spent something like $5 million on this big management consulting project, and this big management consulting project had told them all these ways in which they can capture all this information in this really clean way for their credit scoring models, and of course none of those things were in these 9 that mattered. So they could have saved $5 million, but they didn't. Because management consulting companies don't use random forests. So I can't overstate the importance of this plot, but this is a deep learning course, so we're not really going to spend time talking about it. I mentioned that I had a whole bunch of really, really, really weird things in the way that the competition, the place-getters, did things. For one, they didn't normalize their continuous variables. Who does that? But then when people do well in a competition, something's working. The ways in which they initialized their embeddings were really, really weird. But all these things were really, really weird. So what I did was I wrote a little script, Rosman Experiments. What I did was basically I copied and pasted all the important code out of my notebook. Remember I've already pickled the parameters for the labeling coder and the scalar, so I didn't have to worry about doing those again. Once I copied and pasted all that code in, so this is exactly all the code you just saw, I then had this bunch of for loops, pretty inelegant. But these are all the things that I wanted to basically find out. Does it matter whether or not you use 1-0 scaling? Does it matter whether you use their weird approach to initializing embeddings? Does it matter whether you use their particular dictionary of embedding dimensions or use my simple little formula? Something else I tried is they basically took all their continuous variables and put them through a separate little dense layer each. I was like, why don't we put them all together. I also tried some other things like using batch normalization. So I ran this and got back every possible combination of these. This is where you want to be using the script. I'm not going to tell you that I jumped straight to this. First of all, I spent days screwing around with experiments in a notebook by hand, continually forgetting what I had just done until eventually it took me like an hour to write this. And then of course I pasted it into Excel. And here it is. Chucked it into a pivot table, used conditional formatting, and here's my results. So you can see all my different combinations, with and without normalization, with my special function versus their dictionary, using a single dense matrix versus putting everything together, using my init versus their lack of init. And here is this dark blue here, is what they did. It's full of weird to me. But as you can see, it's actually the darkest blue. It actually is the best. But then when you kind of zoom out, you realize there's a whole corner over here that's got a couple of 8.6s, it's nearly as good, but seems much more consistent. And also more consistent with sanity. Yes, do normalize your data, and yes, do use an appropriate initialization function. And if you do those two things, it doesn't really matter what else you do, it's all going to work fine. So what I then did was I created a little spark line in Excel for the actual training graphs. And so here's their winning one, again,.085. But here's the variance of getting there. And as you can see, their approach was pretty bumpy, up and down, up and down, up and down. The second best, on the other hand,.086 rather than.085, is going down very smoothly. And so that made me think, given that it's in this very stable part of the world, and given that it's training much better, I actually think this is just random chance. It just happened to be low in this point. I actually thought this is a better approach. It's more sensible and it's more consistent. So this kind of approach to running experiments, I thought I'd just show you. When you run experiments, try and do it in a rigorous way and track both the stability of the approach as well as the actual result of the approach. So this one here makes so much sense. It's like, use my simple function rather than their weird dictionary, use normalization, use a single dense matrix and use a thoughtful initialization. And you do all of those things, you end up with something that basically is good and much more stable. That's all I wanted to say about Rosman. I'm going to very briefly mention another competition, which is the Kaggle taxi destination competition. You were saying that you did a couple of experiments. One, you figured out the embeddings and then put the embeddings into random forest, and then put embeddings again into neural network. I didn't do that, that was from the paper. So what they did was, for this one here, this 115, they trained the neural network I just showed you. They then threw away the neural network and trained a GBM model. But for the categorical variables, rather than using one-hot encodings, they used the embeddings. That's all. The taxi competition was won by the team with this Unicode name, which is pretty cool. It's actually turned out to be a team run by Yoshua Bengio, who's one of the people that stuck it out through the AI winter and is now one of the leading lights in deep learning. Interestingly, the thing I just showed you, the Rossman competition, this paper they wrote in the Rossman competition, they claimed to have invented this idea of categorical embeddings. But actually, Yoshua Bengio's team won this competition a year earlier with his same technique. But again, it's so uncool, nobody noticed, even though it was Yoshua Bengio. So I want to quickly show you what they did. This is the paper they wrote. Their approach to picking an embedding size was very simple. Use 10. So the data was, which customer is taking this taxi, which taxi are they in, which taxi stand did they get the taxi from, and then quarter hour of the day, day of the week, week of the year. And they didn't add all kinds of other stuff, this is basically it. And so then they said, we're going to learn embeddings inspired by NLP. So actually to my knowledge, this is the first time this appears in the literature. Having said that, I'm sure 1000 people have done it before, it's just not obvious to actually make it into a paper. As a quick sanity check, if you have day of the week with 7 one-hot variable potentials, and embedding size of 10, that doesn't make any sense, right? I used to think that, but actually it does. I've in the last few months quite often ended up with bigger embeddings than my original patternality. Often it does give better results. And I think it's just like, when you realize it's just a dense layer on top of a one-hot encoding, it's like, why shouldn't the dense layer have more information. I found it weird too, I still find it a little weird, but it definitely seems to be something that's quite useful. I've absolutely found plenty of times now where I need a bigger embedding matrix dimensionality than my cardinality of my categorical variable. Now in this competition, again it's a time series competition really because the main thing you're given, other than all this metadata, is a series of GPS points, which is every GPS point along a route. And at some point for the test set, the route is cut off and you have to figure out what the final GPS point would have been, where are they going. Here's the model that they won with. It turns out, again, to be very simple. You take all the metadata we just saw and chuck it through the embeddings. You then take the first 5 GPS points and the last 5 GPS points and concatenate them together with the embeddings, chuck them through a hidden layer, then through a softmax. Now this is quite interesting. What they then do is they take the result of this softmax and they combine it with clusters. Now what are these clusters? They used mean shift clustering. And they used mean shift clustering to figure out where are the places people tend to go. So with taxis, people tend to go to the airport, or they tend to go to the hospital, or they tend to go to the shopping strip. So using mean shift clustering, they came up with I think it was about 3000 clusters, x, y coordinates of places that people tended to go. However, people don't always go to those 3000 places. So this is a really cool thing. By using a softmax and then they took the softmax and they multiplied it and took a weighted average using the softmax as the weights and the cluster centers as the thing that you're taking the weighted average of. So in other words, if they're going to the airport, for sure, the softmax will end up giving a p of very close to 1 for the airport cluster. On the other hand, if it's not really that clear whether they're going to this shopping strip or this movie, then those 2 cluster centers could both have a softmax of about 0.5, so it's going to end up predicting somewhere halfway between the 2. So this is really interesting. They've built a different kind of architecture to anything we've seen before where the softmax is not the last thing we do. It's being used to average a bunch of clusters. So this is really smart because the softmax forces it to be easier for it to pick a specific destination that's very common, but also makes it possible for it to predict any destination anywhere by combining the average of a number of clusters together. I think this is really elegant architecture engineering. So last 4 dots, isn't that what we're trying to predict? Last 5 GPS points that we're given. So to create the training set, what they did was they took all of the routes and they truncated them randomly basically. So every time they sampled another route, think of the data generator, basically the data generator would randomly slice it off somewhere. So this was the last 5 points which we have access to and the first 5 points. The reason it's not all the points is because they're using a standard multilayer perceptron here. So it's a variable length, A, and also you don't want it to be too big. Question. So the prefix is not fed into an RNN, it's just fed into a dense layer? Correct. So we just get 10 points concatenated together into a dense layer. So surprisingly simple. How good was it? Well, look at the results. 214, 214, 213, 213, 213, 213, 211, 214, 212, everybody's clustered together. One person's a bit better at 208 and they're way better at 203. And they mention, by the way, in the paper, they didn't actually have time to finish training. So when they actually finished training, it was actually 1.87. So they won so easily, it's not funny. And interestingly in the paper they actually mentioned the test set was so small that they knew that the only way they could be sure to win was to make sure they won easily. Now because the test set was so small, the leaderboard's actually not statistically that great. So they created a custom test set and tried to see if they could find something that's even better still on the custom test set. And it turns out that actually an RNN is better still. It still would have won the competition, but there's not enough data in the Kaggle test set that this is a statistically significant result. In this case it is statistically significant. A regular RNN wasn't better. But what they did instead was they said, let's take an RNN where we pass in 5 points at a time into the RNN. I think what probably would have been even better would be to have had a convolutional layer first and then pass that into an RNN. They didn't try it as far as I can see from the paper. And importantly a bidirectional RNN which ensures that the initial points and the last points tend to have more weight because we know that their state generally reflects things they've seen more recently. So there's the result of this model. So our poor long-suffering intern Brad has been trying to replicate this result. He has had at least 2 all-nighters in the last 2 weeks, but hasn't quite managed to yet. So I'm not going to show you the code, but hopefully once Brad starts sleeping again, he'll be able to finish it off and we can show you the notebook during the week on the forum that actually re-implements this thing. It was an interesting process to watch Brad try to replicate this. The vast majority of the time in my experience when people say they've tried a model and the model didn't work out and they've given up on the model, it turns out that it's actually because they screwed something up, not because of the problem with the model. And if you weren't comparing to Yoshua Bengio's team's result, knowing that you haven't replicated it yet, at which point do you give up and say, oh my model's not working, versus saying, no I've still got bugs. It's very difficult to debug machine learning models. What Brad's actually had to end up doing is literally take the original Bengio team code, run it line by line, and then try to replicate it in Keras line by line, in like literally np.org clause every time. Because to build a model like this, it doesn't look that complex, but there's just so many places that you can make little mistakes. No normal person will make like zero mistakes. In fact, normal people like me will make dozens of mistakes. So when you build a model like this, you need to find a way to test every single line of code. Any line of code you don't test, I guarantee you'll end up with a bug, and you won't know you have a bug, and there's no way to ever find out you had a bug. Question. We have several questions. One is, note that PI times CI is very similar to what happens in the memory network paper. In that case the output embeddings are weighted by the attention probability. Answer. Yeah, or it's a lot like a regular attentional language model. Question. Can you talk more about the idea you have about first having the convolutional layer and passing that to an RNN? What do you mean by that? So here is a fantastic paper. We looked at these kind of sub-word encodings last week for language models. I don't know if any of you thought about this and wondered what if we just had individual characters. There's a really fascinating paper called Fully Character Level Machine Translation with No Explicit Segmentation. It's from November of last year. They actually get fantastic results on just character level, beating pretty much everything including the BPE approach we saw last time. So they looked at lots of different approaches and comparing BPE to individual character, and most of the time they got the best results. Their model looks like this. They start out with every individual character. It goes through a character embedding, just like we've used character embeddings lots of times. Then you take those character embeddings and you pass it through a one-dimensional convolution. I don't know if you guys remember, but in part 1 of this course, Ben actually had a blog post about showing how you can do multiple size convolutions and concatenate them all together. So you could use that approach, or you could just pick a single size. So you end up basically scrolling your convolutional window across your sets of characters. So you end up with the same number of convolution outputs as you started out with letters, but they're now representing the information in a window around that letter. In this case, they then did max-pulling. So they basically said, which window, assuming that maybe we had a different size, size 4, size 3, or size 5, which bits seem to have got the highest activations around here. And then they took those max-pulled things and they put them through a second set of segment embeddings. They then put that through something called a highway network, which the details don't matter too much. It's kind of something like a dense net like we learned about last week. This is a slightly older approach than the dense net. And then finally after doing all that, stick that through an RNN. So the idea here in this model was they basically did as much learned preprocessing as possible and then finally put that into an RNN. And because we've got these max-pulling layers, this RNN ends up with a lot less time points, which is really important to minimize the amount of processing in the RNN. So I'm not going to go into detail on this, but check out this paper because it's really interesting. Question. So for the destinations, we would have more air for the peripheral points. Are we taking a centroid of clusters? Answer. I don't understand that, sorry. You understand that? All we're doing is we're taking the softmax-p multiplied by the cluster C, multiply them and add them up. Question. I thought the first part was asking that with destinations that are more peripheral, they would have higher air because they would be harder to predict this way. Answer. Yeah, probably, which is fine because by definition they're not close to a cluster center so they're not common. Question. Going back, there was a question on the Rossmann example. What does MAPE with neural network mean? I would have expected that result to be the same. Why is it lower? Answer. This is just using a one-hot encoding without an embedding layer. We kind of ran out of time a bit quickly, but I really want to show you this. So quite a few of the students and I have been trying to get a new approach to segmentation working. I finally got it working in the last day or two and I really wanted to show it to you. We talked last week about DenseNet and I mentioned that DenseNet is like ass-kickingly good at doing image classification with a small number of data points. Like crazily good. But I also mentioned that it's the basis of this thing called the 100 layers tiramisu, which is an approach to segmentation. So segmentation refers to taking a picture, an image, and figuring out where's the tree, where's the dog, where's the bicycle, and so forth. So it seems like we're Yoshua Bengio fans today because this is one of his Crips papers as well. Let me set the scene. So Brendan, one of our students, who many of you have seen a lot of his blog posts, he has successfully got a PyTorch of this working. So I've shared that on our files.fast.ai and I got the Keras version of it working. So I'll show you the Keras version because I actually understand it. And if anybody's interested in asking questions about the PyTorch version, hopefully Brendan will be happy to answer them during the week. So the data looks like this. There's an image, and then there's labels. So that's basically what it looks like. So you can see here you've got traffic lights, you've got poles, you've got trees, buildings, roads. Interestingly, the dataset we're using is something called Canvid and the dataset is actually frames from a video. So a lot of the frames look very similar to each other. And there's only like 600 frames in total, so there's very, very little data in this Canvid dataset. Furthermore, we're not going to do any pre-training. So we're going to try and build a state-of-the-art classification system on video, which is already much lower information content because most of the frames are pretty similar, using just 600 frames without pre-training. Now if you had asked me a month ago, I would have told you it's not possible. This just seems like an incredibly difficult thing to do. But just watch. I'm going to skip to the answer first. Here's an example of a particular frame we're trying to match. Here is the ground truth for that frame. You can see there's a tiny car here and a little car here. There's a tree. Trees are really difficult. They're incredibly fine, funny things. And here is my trained model. And as you can see, it's done really, really well. It's interesting to look at the mistakes it made. This little thing here is a person. You can see that the person, their head looks a lot like traffic light and their jacket looks a lot like mailbox. Whereas these tiny little people here, it's done perfectly. Whereas this person got a little bit confused. Another example of where it's gone wrong is this should be a road, whereas it wasn't quite sure what was road and what was footpath. Which makes sense, the colors do look very similar. But had we have pre-trained something, a pre-trained network would have understood that crossroads tend to go straight across, they don't tend to look like that. So you can kind of see where the minor mistakes it made, it also would have learned, had it looked at more than a couple of hundred examples of people, that people generally are a particular shape. So there's just not enough data for it to have learned some of these things. But nonetheless, it is extraordinarily effective. Look at this traffic light, it's kind of surrounded by a sign. So the ground truth actually has the traffic light and then a tiny little edge of sign, and it's even got that right. So it's an incredibly accurate model. So how does it work? And in particular, how does it do these amazing trees? So the answer is in this picture. Basically, until this is inspired by a model called UNET, until the UNET model came along, everybody was doing these kinds of segmentation models using an approach just like what we did for style transfer, which is basically you have a number of convolutional layers with max pooling or with a stride of 2, which gradually make the image smaller and smaller, bigger receptive field, and then you go back up the other side using up-sampling or decompolutions until you get back to the original size, and then your final layer is the same size as your starting layer and has a bunch of different classes that you're trying to use in the softmax. The problem with that is that you end up with, in fact I'll show you an example, there's a really nice paper called ENET. ENET is not only an incredibly accurate model for segmentation, but it's also incredibly fast. It actually can run in real-time. You can actually run it on a video. But the mistakes it makes, look at this chair. This chair has a big gap here and here and here, but ENET gets it totally wrong. And the reason why is because they use a very traditional down-sampling, up-sampling approach. And by the time they get to the bottom, they've just lost track of the fine detail. So the trick are these connections here. What we do is we start with our input, we do a standard initial convolution, just like we did with style transfer. We then have a dense net block, which we learned about last week. And then that block, we keep going down, we do a max-pooling type thing, another dense net block, max-pooling type thing, keep going down. And then as we go up the other side, so we do a deconvolution, dense block, deconvolution, dense block, we take the output from the dense block on the way down and we actually copy it over to here and concatenate the two together. So actually, Brendan a few days ago actually drew this on our whiteboard when we were explaining it to Melissa, and so he's shown us every stage here. We start out with a 224x224 input, goes through the convolutions with 48 filters, goes through our dense block, adds another 80 filters, and then goes throughout, they call it a transition down, so basically a max-pooling, so it's now size 112. We keep doing that. Dense block, transition down, so it's now 56x56, 28x28, 14x14, 7x7, and then on the way up again, we go transition up, it's now 14x14. We copy across the results of the 14x14 from the transition down and concatenate together. Then we do a dense block, transition up, it's now 28x28, so we copy across our 28x28 from the transition down and so forth. So by the time we get all the way back up here, we're actually copying across something that was originally of size 224x224. It hadn't had much done to it. It had only been through one convolutional layer and one dense block, so it hadn't really got much rich computation being done. But the thing is, by the time it gets back up all the way up here, the model knows pretty much this is a tree, this is a person, and this is a house, and it just needs to get the fine little details. Where exactly does this leaf finish? Where exactly does the person's hat finish? So it's basically copying across something which is very high resolution but doesn't have that much rich information, but that's fine because it really only needs to fill in the details. So these things here, they're called skip connections. They were really inspired by this paper called UNET, which has won many Kaggle competitions. But it's using dense blocks rather than normal fully connected blocks. So let me show you. We're not going to have time to go into this in detail, but I've done all this coding Keras from scratch. This is actually a fantastic fit for Keras. I didn't have to create any custom layers, I didn't have to do anything weird at all, except for one thing, data augmentation. So the data augmentation was we start with 480x360 images, we randomly crop some 224x224 part, and also randomly we may flip it horizontally. That's all perfectly fine. Well, Keras doesn't really have the random crops, unfortunately. But more importantly, whatever we do to the input image, we also have to do to the target image. We need to get the same 224x224 crop and we need to do the same horizontal flip. So I had to write a data generator, which you guys may actually find useful anyway. So this is my data generator. Basically I call it a segment generator. It's just a standard generator, so it's got a next function. Each time you call next, it grabs some random bunch of indexes, it goes through each one of those indexes and grabs the necessary item, grabbing a random slice, sometimes randomly flipping it horizontally, and then it's doing this to both the Xs and the Ys, returning them back. Along with this segment generator, in order to randomly grab a batch of random indexes each time, I created this little class called batch indexes, which can basically do that. It can have either shuffle true or shuffle false. So this pair of classes, you guys might find really helpful for creating your own data generators. This batch indexes class in particular, now that I've written it, you can see how it works. If I say batch indexes from a data set of size 10, I want to grab 3 indexes at a time, so then let's grab 5 batches. Now in this case I've got by default shuffle equals false, so it just returns 012, 345, 678, 9, and finished. On the other hand, if I say shuffle equals true, it returns them in random order, but it still makes sure it captures all of them. And then when we're done, it starts a new random order. So this makes it really easy to create random generators. So that was the only thing I had to add to Keras to get this all to work. Other than that, we wrote the tiramisu. The tiramisu looks very very similar to the DenseNet that we saw last week. We've got all our pieces, the relu, the dropout, the batch norm, the relu on top of batch norm, a concat layer, so this is something I had to add, a convolution 2D followed by dropout, and then finally my batch norm followed by relu followed by convolution 2D. So this is just the dense block that we saw last week. So a dense block is something where we just keep grabbing 12 or 16 filters at a time, concatenating them to the last set and doing that a few times. That's what a dense block is. So here's something interesting. The original paper for its down sampling, they call it transition down, did a 1x1 convolution followed by a max-pauling. I actually discovered that doing a stride-2 convolution gives better results. So you'll see I actually have not followed the paper. The one that's commented out here is what the paper did, but actually this works better. So that was interesting. Interestingly though, on the transition up side, do you remember that checkerboard artifacts blog post we saw that showed that upsampling 2D followed by a convolutional layer works better? It does not work better for this. In fact, a deconvolution works better for this. So that's why you can see I've got this deconvolution layer. So I thought that was interesting. So basically you can see when I go down sampling a bunch of times, it's basically do a dense block and then I have to keep track of my skip connections. So basically keep a list of all of those skip connections. So I've got to hang on to all of these. So every one of these skip connections, I just stick in this little array, appending them after every dense block. So then I keep them all and then I pass them to my upward path. So I basically do my transition up and then I concatenate that with that skip connection. So that's the basic approach. So then the actual tiramisu model itself with those pieces is less than a screen of code. It's basically just do a 3x3 conv, do my down path, do my up path using those skip connections, then a 1x1 conv at the end, and a softmax. So these dense nets, and indeed this fully convolutional dense net or this tiramisu model, they actually take quite a long time to train. They don't have very many parameters, which is why I think they work so well with these tiny datasets, but they do still take a long time to train. Each epoch took a couple of minutes, and in the end I had to do many hundreds of epochs. I was also doing a bunch of learning rate annealing. So in the end this really had to train overnight, even though I had only about 500-600 frames. But in the end, I got a really good result. I was a bit nervous at first, I was getting this like 87.6% accuracy, but in the paper they were getting 90%+. It turned out to be 3% of the pixels are marked as void. I don't know why they're marked as void, but in the paper they actually remove them. So you'll see when you get to my results section, I've got this bit where I remove those void ones, and I ended up with 89.5%. None of us in class managed to replicate the paper. The paper got 91.5% or 91.2%. We tried the lasagna code they provided, we tried Brendan's PyTorch, we tried Mike Keras. Even though we couldn't replicate their result, this is still better than any other result I've found. So this is still super accurate. A couple of quick notes about this. First is, they tried training also on something called the GATECH dataset, which is another video dataset. The degree to which this is an amazing model is really clear here. This 76% is from a model which is specifically built for video, so it actually includes the time component, which is absolutely critical, and it uses a pre-trained network, so it's used like a million images to kind of pre-train, and it's still not as good as this model. So that is an extraordinary comparison. This is the CAMTEC comparison. Here's the model we were just looking at. And again, I actually looked into this. I thought 91.5%, whereas this one here, 88%, wow, it actually looks like it's not that much better. I'm really surprised. Even Tree, I really thought it should win easily on Tree, but it doesn't win by very much. So I actually went back and looked at this paper, and it turns out that the authors of the DenseNet paper, this is the paper by the way, Modiscale, that they're comparing to. It turned out that they actually trained on crops of 852x852, so they actually used a way higher resolution image to start with. So you've got to be really careful when you read these comparisons. Sometimes people actually shoot themselves in the foot. So these guys were comparing their result to another model that was using twice as big a picture. So again, this is actually way better than they actually made it look like. Another one, this one here, this 88%, also looks impressive. But then I looked across here and I noticed that the Dilution8 model is way better than this model on every single category, way better. And yet somehow the average is only.3 better, and then I realized this actually has to be an error. So this model is actually a lot better than this table gives the impression. I briefly mentioned that there's a model which doesn't have any skip connections called Enet, which is actually better than the Tiramisu on everything except for tree. But on the tree, it's terrible. It's 77.8 vs. 77.3. I'm sure it was less good than this model, but now I can't find that data. The reason I wanted to mention this is that Eugenio is about to release a new model which combines these approaches with skip connections. It's called LinkNet. So keep an eye on the forum because I'll be looking into that quite shortly. A lot of you have come up to me and been like, We're finishing, what do we do now? And the answer is, we have now created a community of all these people who have spent well over 100 hours working on deep learning for many, many months, and have built their own boxes, and written blog posts, and done all kinds of stuff, set up social impact talks, written articles in Forbes. This community is happening. So it doesn't make any sense in my opinion for Rachel and I to now be saying, Here's what happens next. So just like Elena has decided, I want a book club. So she talked to Mindy and we now have a book club. So what's next? Well, the forums will continue forever. We all know each other. Let's do good shit. Most importantly, write code. Please write code. Build apps, take your work projects and try doing them with deep learning. Build libraries to make things easier. Maybe go back to stuff from part 1 of the course and look back and think, Why didn't we do it this other way? Maybe I could make it simpler. Write papers. So I showed you that amazing result of the new style transfer approach from Vincent last week. Hopefully that will get my tape into a paper. Write blog posts. In a few weeks' time, all the MOOC guys are going to be coming through and doing part 2 of the course. So help them out on the forum. Teaching is the best way to learn yourself. I really want to hear the success stories. People don't believe that what you've done is possible. I know that because as recently as yesterday, there was the highest ranked Hacker News comment on a story about deep learning. It's pointless trying to do deep learning unless you have years of mathematical background and you know C++ and you're an expert in machine learning techniques across the board. Otherwise there's no way you're going to be able to do anything useful in the real world project. That today is what everybody believes. We now know that's not true. So Rachel and I are going to start up a podcast where we're going to try to help deep learning learners. But one of the key things we want to do is tell your stories. So if you've done something interesting at work, or you've got an interesting new result, or you're just in the middle of a project that's kind of fun, please tell us. Either on the forum or private message or whatever. Please tell us, because we really want to share your story. And if it's not a story yet, tell us enough that we can help you, and that the community can help you. Get together. The book club, if you're watching this on the MOOC, organize other people in your geography to get together and meet up at your workplace. In this group here, I know we've got people from Apple and Uber and Airbnb who started doing this in lunchtime MOOC chats, and now they're here at this course. Yes Rachel? I also wanted to recommend it would be great to start meetups to help lead other people through part 1 of the course, assist them going through it. So Rachel and I really just want to spend the next 6-12 months focused on supporting your projects. So I'm very interested in working on this lung cancer stuff, but I'm also interested in every project that you guys are working on, I want to help with that. I also want to help people who want to teach this. So Yannett is going to go from being a student to teacher hopefully soon, and will be teaching USF students about deep learning, and hopefully the next batch of people about deep learning. Anybody who's interested in teaching, let us know. This is the best high-leverage activity, is to teach the teachers. So I don't know where this is going to end up, but my hope is really that, basically I would say the experiment has worked. You guys are all here, you're reading papers, you're writing code, you're understanding the most cutting-edge research-level deep learning that exists today. We've gone beyond some of the cutting-edge research in many situations, some of you have gone beyond the cutting-edge research. So let's build from here as a community, and anything that Rachel and I can do to help, please tell us, because we just want you to be successful, we want the community to be successful. So will you be still active in the forums? My job is to make you guys successful. So thank you all so much for coming, and congratulations to all of you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 17.96, "text": " Welcome to Lesson 14, the final lesson for now. We'll talk at the end about what's next.", "tokens": [4027, 281, 18649, 266, 3499, 11, 264, 2572, 6898, 337, 586, 13, 492, 603, 751, 412, 264, 917, 466, 437, 311, 958, 13], "temperature": 0.0, "avg_logprob": -0.1867271382757958, "compression_ratio": 1.3461538461538463, "no_speech_prob": 0.018538611009716988}, {"id": 1, "seek": 0, "start": 17.96, "end": 22.44, "text": " As you can see from what's increasingly been happening, what's next is very much about", "tokens": [1018, 291, 393, 536, 490, 437, 311, 12980, 668, 2737, 11, 437, 311, 958, 307, 588, 709, 466], "temperature": 0.0, "avg_logprob": -0.1867271382757958, "compression_ratio": 1.3461538461538463, "no_speech_prob": 0.018538611009716988}, {"id": 2, "seek": 2244, "start": 22.44, "end": 30.400000000000002, "text": " you with us rather than us leading you or telling you. We're a community now and we", "tokens": [291, 365, 505, 2831, 813, 505, 5775, 291, 420, 3585, 291, 13, 492, 434, 257, 1768, 586, 293, 321], "temperature": 0.0, "avg_logprob": -0.1794717703292619, "compression_ratio": 1.48, "no_speech_prob": 0.0001177171288873069}, {"id": 3, "seek": 2244, "start": 30.400000000000002, "end": 39.480000000000004, "text": " can figure these stuff out together. And obviously USF is a wonderful ally to have.", "tokens": [393, 2573, 613, 1507, 484, 1214, 13, 400, 2745, 2546, 37, 307, 257, 3715, 23356, 281, 362, 13], "temperature": 0.0, "avg_logprob": -0.1794717703292619, "compression_ratio": 1.48, "no_speech_prob": 0.0001177171288873069}, {"id": 4, "seek": 2244, "start": 39.480000000000004, "end": 48.36, "text": " So for now, this is the last of these lessons. One of the things that was great to see this", "tokens": [407, 337, 586, 11, 341, 307, 264, 1036, 295, 613, 8820, 13, 1485, 295, 264, 721, 300, 390, 869, 281, 536, 341], "temperature": 0.0, "avg_logprob": -0.1794717703292619, "compression_ratio": 1.48, "no_speech_prob": 0.0001177171288873069}, {"id": 5, "seek": 4836, "start": 48.36, "end": 55.32, "text": " week was this terrific article in Forbes that talked about deep learning education and it", "tokens": [1243, 390, 341, 20899, 7222, 294, 45950, 300, 2825, 466, 2452, 2539, 3309, 293, 309], "temperature": 0.0, "avg_logprob": -0.15185542165497204, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.00042356460471637547}, {"id": 6, "seek": 4836, "start": 55.32, "end": 61.68, "text": " was written by one of our terrific students, Maria, and focuses on the great work of some", "tokens": [390, 3720, 538, 472, 295, 527, 20899, 1731, 11, 12734, 11, 293, 16109, 322, 264, 869, 589, 295, 512], "temperature": 0.0, "avg_logprob": -0.15185542165497204, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.00042356460471637547}, {"id": 7, "seek": 4836, "start": 61.68, "end": 68.24, "text": " of the students that have come through this course. So I wanted to say thank you very", "tokens": [295, 264, 1731, 300, 362, 808, 807, 341, 1164, 13, 407, 286, 1415, 281, 584, 1309, 291, 588], "temperature": 0.0, "avg_logprob": -0.15185542165497204, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.00042356460471637547}, {"id": 8, "seek": 4836, "start": 68.24, "end": 75.0, "text": " much and congratulations on this great article. I hope everybody will check it out. It's really", "tokens": [709, 293, 13568, 322, 341, 869, 7222, 13, 286, 1454, 2201, 486, 1520, 309, 484, 13, 467, 311, 534], "temperature": 0.0, "avg_logprob": -0.15185542165497204, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.00042356460471637547}, {"id": 9, "seek": 7500, "start": 75.0, "end": 85.92, "text": " beautifully written as well and terrific stories. I found it quite inspiring.", "tokens": [16525, 3720, 382, 731, 293, 20899, 3676, 13, 286, 1352, 309, 1596, 15883, 13], "temperature": 0.0, "avg_logprob": -0.13962103956836766, "compression_ratio": 1.5149700598802396, "no_speech_prob": 3.426594048505649e-05}, {"id": 10, "seek": 7500, "start": 85.92, "end": 93.92, "text": " So today we are going to be talking about a couple of things. We're going to start with", "tokens": [407, 965, 321, 366, 516, 281, 312, 1417, 466, 257, 1916, 295, 721, 13, 492, 434, 516, 281, 722, 365], "temperature": 0.0, "avg_logprob": -0.13962103956836766, "compression_ratio": 1.5149700598802396, "no_speech_prob": 3.426594048505649e-05}, {"id": 11, "seek": 7500, "start": 93.92, "end": 104.08, "text": " time series and structured data. Time series, I wanted to start very briefly by talking", "tokens": [565, 2638, 293, 18519, 1412, 13, 6161, 2638, 11, 286, 1415, 281, 722, 588, 10515, 538, 1417], "temperature": 0.0, "avg_logprob": -0.13962103956836766, "compression_ratio": 1.5149700598802396, "no_speech_prob": 3.426594048505649e-05}, {"id": 12, "seek": 10408, "start": 104.08, "end": 109.6, "text": " about something which I think you basically know how to do. This is a fantastic paper", "tokens": [466, 746, 597, 286, 519, 291, 1936, 458, 577, 281, 360, 13, 639, 307, 257, 5456, 3035], "temperature": 0.0, "avg_logprob": -0.1779823652127894, "compression_ratio": 1.504201680672269, "no_speech_prob": 0.00013549871800933033}, {"id": 13, "seek": 10408, "start": 109.6, "end": 116.12, "text": " because it is not by deep mind, nobody's heard of it. It actually comes from the Children's", "tokens": [570, 309, 307, 406, 538, 2452, 1575, 11, 5079, 311, 2198, 295, 309, 13, 467, 767, 1487, 490, 264, 13354, 311], "temperature": 0.0, "avg_logprob": -0.1779823652127894, "compression_ratio": 1.504201680672269, "no_speech_prob": 0.00013549871800933033}, {"id": 14, "seek": 10408, "start": 116.12, "end": 124.08, "text": " Hospital of Los Angeles. Believe it or not, perhaps the epicenter of practical applied", "tokens": [15645, 295, 7632, 12292, 13, 21486, 309, 420, 406, 11, 4317, 264, 13581, 14278, 295, 8496, 6456], "temperature": 0.0, "avg_logprob": -0.1779823652127894, "compression_ratio": 1.504201680672269, "no_speech_prob": 0.00013549871800933033}, {"id": 15, "seek": 10408, "start": 124.08, "end": 129.04, "text": " AI in medicine today is in Southern California. Specifically, Southern California Pediatrics,", "tokens": [7318, 294, 7195, 965, 307, 294, 13724, 5384, 13, 26058, 11, 13724, 5384, 16689, 7676, 10716, 11], "temperature": 0.0, "avg_logprob": -0.1779823652127894, "compression_ratio": 1.504201680672269, "no_speech_prob": 0.00013549871800933033}, {"id": 16, "seek": 12904, "start": 129.04, "end": 134.79999999999998, "text": " the Children's Hospital of Orange County, CHOC, and the Children's Hospital of Los Angeles,", "tokens": [264, 13354, 311, 15645, 295, 17106, 6658, 11, 5995, 30087, 11, 293, 264, 13354, 311, 15645, 295, 7632, 12292, 11], "temperature": 0.0, "avg_logprob": -0.18718629472711112, "compression_ratio": 1.6043478260869566, "no_speech_prob": 9.027230407809839e-05}, {"id": 17, "seek": 12904, "start": 134.79999999999998, "end": 142.12, "text": " CHLA. CHLA, which this paper comes from, actually has this thing they call VPICU, the Virtual", "tokens": [5995, 11435, 13, 5995, 11435, 11, 597, 341, 3035, 1487, 490, 11, 767, 575, 341, 551, 436, 818, 35812, 2532, 52, 11, 264, 23887], "temperature": 0.0, "avg_logprob": -0.18718629472711112, "compression_ratio": 1.6043478260869566, "no_speech_prob": 9.027230407809839e-05}, {"id": 18, "seek": 12904, "start": 142.12, "end": 148.35999999999999, "text": " Pediatric Intensive Care Unit, where for many many years they've been tracking every electronic", "tokens": [16689, 17252, 5681, 2953, 9532, 27894, 11, 689, 337, 867, 867, 924, 436, 600, 668, 11603, 633, 10092], "temperature": 0.0, "avg_logprob": -0.18718629472711112, "compression_ratio": 1.6043478260869566, "no_speech_prob": 9.027230407809839e-05}, {"id": 19, "seek": 12904, "start": 148.35999999999999, "end": 154.79999999999998, "text": " signal about how every patient, every kid in the hospital is treated and what all their", "tokens": [6358, 466, 577, 633, 4537, 11, 633, 1636, 294, 264, 4530, 307, 8668, 293, 437, 439, 641], "temperature": 0.0, "avg_logprob": -0.18718629472711112, "compression_ratio": 1.6043478260869566, "no_speech_prob": 9.027230407809839e-05}, {"id": 20, "seek": 15480, "start": 154.8, "end": 163.32000000000002, "text": " ongoing sensor readings are. One of the extraordinary things they do is when the doctors there do", "tokens": [10452, 10200, 27319, 366, 13, 1485, 295, 264, 10581, 721, 436, 360, 307, 562, 264, 8778, 456, 360], "temperature": 0.0, "avg_logprob": -0.16941647031413976, "compression_ratio": 1.5319148936170213, "no_speech_prob": 1.3006761037104297e-05}, {"id": 21, "seek": 15480, "start": 163.32000000000002, "end": 169.56, "text": " rounds, data scientists come with them. I don't know anywhere else in the world that", "tokens": [13757, 11, 1412, 7708, 808, 365, 552, 13, 286, 500, 380, 458, 4992, 1646, 294, 264, 1002, 300], "temperature": 0.0, "avg_logprob": -0.16941647031413976, "compression_ratio": 1.5319148936170213, "no_speech_prob": 1.3006761037104297e-05}, {"id": 22, "seek": 15480, "start": 169.56, "end": 172.16000000000003, "text": " this happens.", "tokens": [341, 2314, 13], "temperature": 0.0, "avg_logprob": -0.16941647031413976, "compression_ratio": 1.5319148936170213, "no_speech_prob": 1.3006761037104297e-05}, {"id": 23, "seek": 15480, "start": 172.16000000000003, "end": 179.92000000000002, "text": " And so a couple of months ago they released a draft of this amazing paper where they talked", "tokens": [400, 370, 257, 1916, 295, 2493, 2057, 436, 4736, 257, 11206, 295, 341, 2243, 3035, 689, 436, 2825], "temperature": 0.0, "avg_logprob": -0.16941647031413976, "compression_ratio": 1.5319148936170213, "no_speech_prob": 1.3006761037104297e-05}, {"id": 24, "seek": 17992, "start": 179.92, "end": 186.51999999999998, "text": " about how they pulled out all this data from the EMR and from the sensors and attempted", "tokens": [466, 577, 436, 7373, 484, 439, 341, 1412, 490, 264, 16237, 49, 293, 490, 264, 14840, 293, 18997], "temperature": 0.0, "avg_logprob": -0.10694153168622185, "compression_ratio": 1.722007722007722, "no_speech_prob": 1.1300452570139896e-05}, {"id": 25, "seek": 17992, "start": 186.51999999999998, "end": 191.88, "text": " to predict patient mortality. The reason this is interesting is that when a kid goes into", "tokens": [281, 6069, 4537, 23330, 13, 440, 1778, 341, 307, 1880, 307, 300, 562, 257, 1636, 1709, 666], "temperature": 0.0, "avg_logprob": -0.10694153168622185, "compression_ratio": 1.722007722007722, "no_speech_prob": 1.1300452570139896e-05}, {"id": 26, "seek": 17992, "start": 191.88, "end": 200.07999999999998, "text": " the ICU, if a model starts saying this kid's looking like they might die, then that's the", "tokens": [264, 38123, 11, 498, 257, 2316, 3719, 1566, 341, 1636, 311, 1237, 411, 436, 1062, 978, 11, 550, 300, 311, 264], "temperature": 0.0, "avg_logprob": -0.10694153168622185, "compression_ratio": 1.722007722007722, "no_speech_prob": 1.1300452570139896e-05}, {"id": 27, "seek": 17992, "start": 200.07999999999998, "end": 205.16, "text": " thing that sets the alarms going and everybody rushes over and starts looking after them.", "tokens": [551, 300, 6352, 264, 45039, 516, 293, 2201, 9300, 279, 670, 293, 3719, 1237, 934, 552, 13], "temperature": 0.0, "avg_logprob": -0.10694153168622185, "compression_ratio": 1.722007722007722, "no_speech_prob": 1.1300452570139896e-05}, {"id": 28, "seek": 17992, "start": 205.16, "end": 208.92, "text": " They found that they built a model that was more accurate than any existing model. Those", "tokens": [814, 1352, 300, 436, 3094, 257, 2316, 300, 390, 544, 8559, 813, 604, 6741, 2316, 13, 3950], "temperature": 0.0, "avg_logprob": -0.10694153168622185, "compression_ratio": 1.722007722007722, "no_speech_prob": 1.1300452570139896e-05}, {"id": 29, "seek": 20892, "start": 208.92, "end": 215.76, "text": " existing models were built on many years of deep clinical input and they used an RNN.", "tokens": [6741, 5245, 645, 3094, 322, 867, 924, 295, 2452, 9115, 4846, 293, 436, 1143, 364, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.22810827602039685, "compression_ratio": 1.6699029126213591, "no_speech_prob": 8.530212653568015e-06}, {"id": 30, "seek": 20892, "start": 215.76, "end": 223.79999999999998, "text": " Now this kind of time series data is what I'm going to refer to as signal type time", "tokens": [823, 341, 733, 295, 565, 2638, 1412, 307, 437, 286, 478, 516, 281, 2864, 281, 382, 6358, 2010, 565], "temperature": 0.0, "avg_logprob": -0.22810827602039685, "compression_ratio": 1.6699029126213591, "no_speech_prob": 8.530212653568015e-06}, {"id": 31, "seek": 20892, "start": 223.79999999999998, "end": 232.56, "text": " series data. So let's say you've got a series of blood pressure readings. So they might", "tokens": [2638, 1412, 13, 407, 718, 311, 584, 291, 600, 658, 257, 2638, 295, 3390, 3321, 27319, 13, 407, 436, 1062], "temperature": 0.0, "avg_logprob": -0.22810827602039685, "compression_ratio": 1.6699029126213591, "no_speech_prob": 8.530212653568015e-06}, {"id": 32, "seek": 20892, "start": 232.56, "end": 237.11999999999998, "text": " be, they come in and their blood pressure is kind of low and it's kind of all over the", "tokens": [312, 11, 436, 808, 294, 293, 641, 3390, 3321, 307, 733, 295, 2295, 293, 309, 311, 733, 295, 439, 670, 264], "temperature": 0.0, "avg_logprob": -0.22810827602039685, "compression_ratio": 1.6699029126213591, "no_speech_prob": 8.530212653568015e-06}, {"id": 33, "seek": 23712, "start": 237.12, "end": 246.32, "text": " place and then suddenly it shoots up. In addition to that, maybe there's other readings such", "tokens": [1081, 293, 550, 5800, 309, 20704, 493, 13, 682, 4500, 281, 300, 11, 1310, 456, 311, 661, 27319, 1270], "temperature": 0.0, "avg_logprob": -0.20826594034830728, "compression_ratio": 1.5128205128205128, "no_speech_prob": 5.25532232131809e-06}, {"id": 34, "seek": 23712, "start": 246.32, "end": 256.44, "text": " as at which points they receive some kind of medical intervention. There was one here", "tokens": [382, 412, 597, 2793, 436, 4774, 512, 733, 295, 4625, 13176, 13, 821, 390, 472, 510], "temperature": 0.0, "avg_logprob": -0.20826594034830728, "compression_ratio": 1.5128205128205128, "no_speech_prob": 5.25532232131809e-06}, {"id": 35, "seek": 23712, "start": 256.44, "end": 261.96, "text": " and one here and then there was like 6 here and so forth.", "tokens": [293, 472, 510, 293, 550, 456, 390, 411, 1386, 510, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.20826594034830728, "compression_ratio": 1.5128205128205128, "no_speech_prob": 5.25532232131809e-06}, {"id": 36, "seek": 26196, "start": 261.96, "end": 268.68, "text": " So these kinds of things, generally speaking, the state of health at time t is probably", "tokens": [407, 613, 3685, 295, 721, 11, 5101, 4124, 11, 264, 1785, 295, 1585, 412, 565, 256, 307, 1391], "temperature": 0.0, "avg_logprob": -0.15891431782343615, "compression_ratio": 1.521978021978022, "no_speech_prob": 2.726435923250392e-06}, {"id": 37, "seek": 26196, "start": 268.68, "end": 276.59999999999997, "text": " best predicted by all of the various sensor readings at t-1, t-2, and t-3. So in statistical", "tokens": [1151, 19147, 538, 439, 295, 264, 3683, 10200, 27319, 412, 256, 12, 16, 11, 256, 12, 17, 11, 293, 256, 12, 18, 13, 407, 294, 22820], "temperature": 0.0, "avg_logprob": -0.15891431782343615, "compression_ratio": 1.521978021978022, "no_speech_prob": 2.726435923250392e-06}, {"id": 38, "seek": 26196, "start": 276.59999999999997, "end": 283.12, "text": " terms we would refer to that as autocorrelation. Autocorrelation means correlation with previous", "tokens": [2115, 321, 576, 2864, 281, 300, 382, 45833, 284, 4419, 399, 13, 6049, 905, 284, 4419, 399, 1355, 20009, 365, 3894], "temperature": 0.0, "avg_logprob": -0.15891431782343615, "compression_ratio": 1.521978021978022, "no_speech_prob": 2.726435923250392e-06}, {"id": 39, "seek": 28312, "start": 283.12, "end": 293.28000000000003, "text": " time periods. And for this kind of signal, I think it's very likely that an RNN is the", "tokens": [565, 13804, 13, 400, 337, 341, 733, 295, 6358, 11, 286, 519, 309, 311, 588, 3700, 300, 364, 45702, 45, 307, 264], "temperature": 0.0, "avg_logprob": -0.182873174684857, "compression_ratio": 1.5451127819548873, "no_speech_prob": 3.4464869713701773e-06}, {"id": 40, "seek": 28312, "start": 293.28000000000003, "end": 298.56, "text": " way to go. Obviously you could probably get a better result using a bidirectional RNN,", "tokens": [636, 281, 352, 13, 7580, 291, 727, 1391, 483, 257, 1101, 1874, 1228, 257, 12957, 621, 41048, 45702, 45, 11], "temperature": 0.0, "avg_logprob": -0.182873174684857, "compression_ratio": 1.5451127819548873, "no_speech_prob": 3.4464869713701773e-06}, {"id": 41, "seek": 28312, "start": 298.56, "end": 301.88, "text": " but that's not going to be any help in the ICU because you don't have the future time", "tokens": [457, 300, 311, 406, 516, 281, 312, 604, 854, 294, 264, 38123, 570, 291, 500, 380, 362, 264, 2027, 565], "temperature": 0.0, "avg_logprob": -0.182873174684857, "compression_ratio": 1.5451127819548873, "no_speech_prob": 3.4464869713701773e-06}, {"id": 42, "seek": 28312, "start": 301.88, "end": 307.12, "text": " period sensors. So be careful of this data leakage issue.", "tokens": [2896, 14840, 13, 407, 312, 5026, 295, 341, 1412, 47799, 2734, 13], "temperature": 0.0, "avg_logprob": -0.182873174684857, "compression_ratio": 1.5451127819548873, "no_speech_prob": 3.4464869713701773e-06}, {"id": 43, "seek": 28312, "start": 307.12, "end": 312.8, "text": " And indeed this is what this team at the VPICU, Children's Hospital of Los Angeles, did. They", "tokens": [400, 6451, 341, 307, 437, 341, 1469, 412, 264, 35812, 2532, 52, 11, 13354, 311, 15645, 295, 7632, 12292, 11, 630, 13, 814], "temperature": 0.0, "avg_logprob": -0.182873174684857, "compression_ratio": 1.5451127819548873, "no_speech_prob": 3.4464869713701773e-06}, {"id": 44, "seek": 31280, "start": 312.8, "end": 318.84000000000003, "text": " used an RNN to get this state-of-the-art result. I'm not really going to teach you more about", "tokens": [1143, 364, 45702, 45, 281, 483, 341, 1785, 12, 2670, 12, 3322, 12, 446, 1874, 13, 286, 478, 406, 534, 516, 281, 2924, 291, 544, 466], "temperature": 0.0, "avg_logprob": -0.12915865941481156, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.422088634077227e-06}, {"id": 45, "seek": 31280, "start": 318.84000000000003, "end": 323.64, "text": " this because basically you already know how to do it. You can check out the paper, you'll", "tokens": [341, 570, 1936, 291, 1217, 458, 577, 281, 360, 309, 13, 509, 393, 1520, 484, 264, 3035, 11, 291, 603], "temperature": 0.0, "avg_logprob": -0.12915865941481156, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.422088634077227e-06}, {"id": 46, "seek": 31280, "start": 323.64, "end": 332.36, "text": " see there's almost nothing special. The only thing which was quite clever was that their", "tokens": [536, 456, 311, 1920, 1825, 2121, 13, 440, 787, 551, 597, 390, 1596, 13494, 390, 300, 641], "temperature": 0.0, "avg_logprob": -0.12915865941481156, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.422088634077227e-06}, {"id": 47, "seek": 31280, "start": 332.36, "end": 340.98, "text": " sensor readings were not necessarily equally spaced. For example, did they receive some", "tokens": [10200, 27319, 645, 406, 4725, 12309, 43766, 13, 1171, 1365, 11, 630, 436, 4774, 512], "temperature": 0.0, "avg_logprob": -0.12915865941481156, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.422088634077227e-06}, {"id": 48, "seek": 34098, "start": 340.98, "end": 345.64000000000004, "text": " particular medical intervention? Clearly they're very widely spaced and they're not equally", "tokens": [1729, 4625, 13176, 30, 24120, 436, 434, 588, 13371, 43766, 293, 436, 434, 406, 12309], "temperature": 0.0, "avg_logprob": -0.1429414597768632, "compression_ratio": 1.5813953488372092, "no_speech_prob": 6.438768195948796e-06}, {"id": 49, "seek": 34098, "start": 345.64000000000004, "end": 346.92, "text": " spaced.", "tokens": [43766, 13], "temperature": 0.0, "avg_logprob": -0.1429414597768632, "compression_ratio": 1.5813953488372092, "no_speech_prob": 6.438768195948796e-06}, {"id": 50, "seek": 34098, "start": 346.92, "end": 357.48, "text": " So rather than having the RNN have basically a sequence of interventions that gets fed", "tokens": [407, 2831, 813, 1419, 264, 45702, 45, 362, 1936, 257, 8310, 295, 20924, 300, 2170, 4636], "temperature": 0.0, "avg_logprob": -0.1429414597768632, "compression_ratio": 1.5813953488372092, "no_speech_prob": 6.438768195948796e-06}, {"id": 51, "seek": 34098, "start": 357.48, "end": 365.76, "text": " to the RNN, instead they actually have two things. One is the signal and the other is", "tokens": [281, 264, 45702, 45, 11, 2602, 436, 767, 362, 732, 721, 13, 1485, 307, 264, 6358, 293, 264, 661, 307], "temperature": 0.0, "avg_logprob": -0.1429414597768632, "compression_ratio": 1.5813953488372092, "no_speech_prob": 6.438768195948796e-06}, {"id": 52, "seek": 36576, "start": 365.76, "end": 371.88, "text": " the time since the last signal was read. So each point at the RNN, if the RNN is basically", "tokens": [264, 565, 1670, 264, 1036, 6358, 390, 1401, 13, 407, 1184, 935, 412, 264, 45702, 45, 11, 498, 264, 45702, 45, 307, 1936], "temperature": 0.0, "avg_logprob": -0.1733202830604885, "compression_ratio": 1.6807511737089202, "no_speech_prob": 5.3380058488983195e-06}, {"id": 53, "seek": 36576, "start": 371.88, "end": 379.76, "text": " just some function f, it's receiving two things. It's receiving the signal at time t and the", "tokens": [445, 512, 2445, 283, 11, 309, 311, 10040, 732, 721, 13, 467, 311, 10040, 264, 6358, 412, 565, 256, 293, 264], "temperature": 0.0, "avg_logprob": -0.1733202830604885, "compression_ratio": 1.6807511737089202, "no_speech_prob": 5.3380058488983195e-06}, {"id": 54, "seek": 36576, "start": 379.76, "end": 388.4, "text": " value of t itself, the difference in time, how long was it since the last one. But that", "tokens": [2158, 295, 256, 2564, 11, 264, 2649, 294, 565, 11, 577, 938, 390, 309, 1670, 264, 1036, 472, 13, 583, 300], "temperature": 0.0, "avg_logprob": -0.1733202830604885, "compression_ratio": 1.6807511737089202, "no_speech_prob": 5.3380058488983195e-06}, {"id": 55, "seek": 36576, "start": 388.4, "end": 393.52, "text": " doesn't require any different deep learning, that's just concatenating one extra thing", "tokens": [1177, 380, 3651, 604, 819, 2452, 2539, 11, 300, 311, 445, 1588, 7186, 990, 472, 2857, 551], "temperature": 0.0, "avg_logprob": -0.1733202830604885, "compression_ratio": 1.6807511737089202, "no_speech_prob": 5.3380058488983195e-06}, {"id": 56, "seek": 39352, "start": 393.52, "end": 400.12, "text": " on the other. They actually show mathematically that this makes a certain amount of sense", "tokens": [322, 264, 661, 13, 814, 767, 855, 44003, 300, 341, 1669, 257, 1629, 2372, 295, 2020], "temperature": 0.0, "avg_logprob": -0.2492245962453443, "compression_ratio": 1.6801801801801801, "no_speech_prob": 8.139596502587665e-06}, {"id": 57, "seek": 39352, "start": 400.12, "end": 403.91999999999996, "text": " as a way to deal with this, and then they find empirically that it does actually seem", "tokens": [382, 257, 636, 281, 2028, 365, 341, 11, 293, 550, 436, 915, 25790, 984, 300, 309, 775, 767, 1643], "temperature": 0.0, "avg_logprob": -0.2492245962453443, "compression_ratio": 1.6801801801801801, "no_speech_prob": 8.139596502587665e-06}, {"id": 58, "seek": 39352, "start": 403.91999999999996, "end": 408.32, "text": " to work pretty well.", "tokens": [281, 589, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.2492245962453443, "compression_ratio": 1.6801801801801801, "no_speech_prob": 8.139596502587665e-06}, {"id": 59, "seek": 39352, "start": 408.32, "end": 411.79999999999995, "text": " I can't tell you whether this is state of the art for anything because I just haven't", "tokens": [286, 393, 380, 980, 291, 1968, 341, 307, 1785, 295, 264, 1523, 337, 1340, 570, 286, 445, 2378, 380], "temperature": 0.0, "avg_logprob": -0.2492245962453443, "compression_ratio": 1.6801801801801801, "no_speech_prob": 8.139596502587665e-06}, {"id": 60, "seek": 39352, "start": 411.79999999999995, "end": 419.24, "text": " seen deep comparative papers or any competitions or anything that really have this kind of", "tokens": [1612, 2452, 39292, 10577, 420, 604, 26185, 420, 1340, 300, 534, 362, 341, 733, 295], "temperature": 0.0, "avg_logprob": -0.2492245962453443, "compression_ratio": 1.6801801801801801, "no_speech_prob": 8.139596502587665e-06}, {"id": 61, "seek": 41924, "start": 419.24, "end": 424.2, "text": " data, which is weird because a lot of the world runs on this kind of data. This kind", "tokens": [1412, 11, 597, 307, 3657, 570, 257, 688, 295, 264, 1002, 6676, 322, 341, 733, 295, 1412, 13, 639, 733], "temperature": 0.0, "avg_logprob": -0.18730693817138672, "compression_ratio": 1.7004608294930876, "no_speech_prob": 2.1782021576655097e-05}, {"id": 62, "seek": 41924, "start": 424.2, "end": 430.48, "text": " of data, effectively things with it are super valuable. If you're an oil and gas company,", "tokens": [295, 1412, 11, 8659, 721, 365, 309, 366, 1687, 8263, 13, 759, 291, 434, 364, 3184, 293, 4211, 2237, 11], "temperature": 0.0, "avg_logprob": -0.18730693817138672, "compression_ratio": 1.7004608294930876, "no_speech_prob": 2.1782021576655097e-05}, {"id": 63, "seek": 41924, "start": 430.48, "end": 434.68, "text": " what's the drill head telling you, or what's the signals coming out of the pipe telling", "tokens": [437, 311, 264, 11392, 1378, 3585, 291, 11, 420, 437, 311, 264, 12354, 1348, 484, 295, 264, 11240, 3585], "temperature": 0.0, "avg_logprob": -0.18730693817138672, "compression_ratio": 1.7004608294930876, "no_speech_prob": 2.1782021576655097e-05}, {"id": 64, "seek": 41924, "start": 434.68, "end": 440.92, "text": " you, or so on and so forth. But there we go, it's not the kind of cool thing that the Google", "tokens": [291, 11, 420, 370, 322, 293, 370, 5220, 13, 583, 456, 321, 352, 11, 309, 311, 406, 264, 733, 295, 1627, 551, 300, 264, 3329], "temperature": 0.0, "avg_logprob": -0.18730693817138672, "compression_ratio": 1.7004608294930876, "no_speech_prob": 2.1782021576655097e-05}, {"id": 65, "seek": 41924, "start": 440.92, "end": 445.8, "text": " kids work on.", "tokens": [2301, 589, 322, 13], "temperature": 0.0, "avg_logprob": -0.18730693817138672, "compression_ratio": 1.7004608294930876, "no_speech_prob": 2.1782021576655097e-05}, {"id": 66, "seek": 44580, "start": 445.8, "end": 450.52000000000004, "text": " So I'm not going to talk more about that, that's how you can do time series with this", "tokens": [407, 286, 478, 406, 516, 281, 751, 544, 466, 300, 11, 300, 311, 577, 291, 393, 360, 565, 2638, 365, 341], "temperature": 0.0, "avg_logprob": -0.10055381664331409, "compression_ratio": 1.560693641618497, "no_speech_prob": 1.1842971616715658e-05}, {"id": 67, "seek": 44580, "start": 450.52000000000004, "end": 456.24, "text": " kind of signal data. You can also incorporate all of the other stuff we're about to talk", "tokens": [733, 295, 6358, 1412, 13, 509, 393, 611, 16091, 439, 295, 264, 661, 1507, 321, 434, 466, 281, 751], "temperature": 0.0, "avg_logprob": -0.10055381664331409, "compression_ratio": 1.560693641618497, "no_speech_prob": 1.1842971616715658e-05}, {"id": 68, "seek": 44580, "start": 456.24, "end": 464.8, "text": " about, which is the other kind of time series data. For example, there was a Kaggle competition", "tokens": [466, 11, 597, 307, 264, 661, 733, 295, 565, 2638, 1412, 13, 1171, 1365, 11, 456, 390, 257, 48751, 22631, 6211], "temperature": 0.0, "avg_logprob": -0.10055381664331409, "compression_ratio": 1.560693641618497, "no_speech_prob": 1.1842971616715658e-05}, {"id": 69, "seek": 46480, "start": 464.8, "end": 482.68, "text": " which was looking at forecasting sales using for each store at this big company in Europe", "tokens": [597, 390, 1237, 412, 44331, 5763, 1228, 337, 1184, 3531, 412, 341, 955, 2237, 294, 3315], "temperature": 0.0, "avg_logprob": -0.2657433748245239, "compression_ratio": 1.4108527131782946, "no_speech_prob": 8.013349543034565e-06}, {"id": 70, "seek": 46480, "start": 482.68, "end": 489.24, "text": " called Rossmann, based on the date and what promotions are going on and what the competitors", "tokens": [1219, 16140, 14912, 11, 2361, 322, 264, 4002, 293, 437, 42127, 366, 516, 322, 293, 437, 264, 18333], "temperature": 0.0, "avg_logprob": -0.2657433748245239, "compression_ratio": 1.4108527131782946, "no_speech_prob": 8.013349543034565e-06}, {"id": 71, "seek": 48924, "start": 489.24, "end": 499.28000000000003, "text": " are doing and so forth. This kind of data is likely to look like this. It's likely to", "tokens": [366, 884, 293, 370, 5220, 13, 639, 733, 295, 1412, 307, 3700, 281, 574, 411, 341, 13, 467, 311, 3700, 281], "temperature": 0.0, "avg_logprob": -0.23134199142456055, "compression_ratio": 1.1486486486486487, "no_speech_prob": 1.9033541320823133e-06}, {"id": 72, "seek": 49928, "start": 499.28, "end": 520.76, "text": " be something like that, or maybe it'll have some kind of trend to it.", "tokens": [312, 746, 411, 300, 11, 420, 1310, 309, 603, 362, 512, 733, 295, 6028, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.12020981879461379, "compression_ratio": 1.0, "no_speech_prob": 1.8448090486344881e-06}, {"id": 73, "seek": 52076, "start": 520.76, "end": 533.88, "text": " So these kinds of seasonal time series are very widely analyzed by econometricians. They're", "tokens": [407, 613, 3685, 295, 27421, 565, 2638, 366, 588, 13371, 28181, 538, 23692, 29470, 2567, 13, 814, 434], "temperature": 0.0, "avg_logprob": -0.15393662061847624, "compression_ratio": 1.4705882352941178, "no_speech_prob": 3.785157787206117e-06}, {"id": 74, "seek": 52076, "start": 533.88, "end": 540.04, "text": " everywhere, particularly in business, if you're trying to predict how many widgets you have", "tokens": [5315, 11, 4098, 294, 1606, 11, 498, 291, 434, 1382, 281, 6069, 577, 867, 43355, 291, 362], "temperature": 0.0, "avg_logprob": -0.15393662061847624, "compression_ratio": 1.4705882352941178, "no_speech_prob": 3.785157787206117e-06}, {"id": 75, "seek": 52076, "start": 540.04, "end": 548.84, "text": " to buy next month, or whether to increase or decrease your prices. All kinds of operational", "tokens": [281, 2256, 958, 1618, 11, 420, 1968, 281, 3488, 420, 11514, 428, 7901, 13, 1057, 3685, 295, 16607], "temperature": 0.0, "avg_logprob": -0.15393662061847624, "compression_ratio": 1.4705882352941178, "no_speech_prob": 3.785157787206117e-06}, {"id": 76, "seek": 54884, "start": 548.84, "end": 556.8000000000001, "text": " type things tend to look like this, how full your planes are going to be, whether you should", "tokens": [2010, 721, 3928, 281, 574, 411, 341, 11, 577, 1577, 428, 14952, 366, 516, 281, 312, 11, 1968, 291, 820], "temperature": 0.0, "avg_logprob": -0.1619025966789149, "compression_ratio": 1.5276381909547738, "no_speech_prob": 1.3211637451604474e-05}, {"id": 77, "seek": 54884, "start": 556.8000000000001, "end": 560.24, "text": " add promotions, so on and so forth.", "tokens": [909, 42127, 11, 370, 322, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1619025966789149, "compression_ratio": 1.5276381909547738, "no_speech_prob": 1.3211637451604474e-05}, {"id": 78, "seek": 54884, "start": 560.24, "end": 566.4, "text": " So it turns out that the state of the art for this kind of approach is not necessarily", "tokens": [407, 309, 4523, 484, 300, 264, 1785, 295, 264, 1523, 337, 341, 733, 295, 3109, 307, 406, 4725], "temperature": 0.0, "avg_logprob": -0.1619025966789149, "compression_ratio": 1.5276381909547738, "no_speech_prob": 1.3211637451604474e-05}, {"id": 79, "seek": 54884, "start": 566.4, "end": 574.8000000000001, "text": " to use an RNN. I'm actually going to look at the 3rd place result from this competition,", "tokens": [281, 764, 364, 45702, 45, 13, 286, 478, 767, 516, 281, 574, 412, 264, 805, 7800, 1081, 1874, 490, 341, 6211, 11], "temperature": 0.0, "avg_logprob": -0.1619025966789149, "compression_ratio": 1.5276381909547738, "no_speech_prob": 1.3211637451604474e-05}, {"id": 80, "seek": 57480, "start": 574.8, "end": 581.1999999999999, "text": " because the 3rd place result was nearly as good as places 1 and 2, but way, way, way,", "tokens": [570, 264, 805, 7800, 1081, 1874, 390, 6217, 382, 665, 382, 3190, 502, 293, 568, 11, 457, 636, 11, 636, 11, 636, 11], "temperature": 0.0, "avg_logprob": -0.1421234965324402, "compression_ratio": 1.6263736263736264, "no_speech_prob": 1.3845755347574595e-05}, {"id": 81, "seek": 57480, "start": 581.1999999999999, "end": 587.52, "text": " way simpler. And also it turns out that there's stuff that we can build on top of for almost", "tokens": [636, 18587, 13, 400, 611, 309, 4523, 484, 300, 456, 311, 1507, 300, 321, 393, 1322, 322, 1192, 295, 337, 1920], "temperature": 0.0, "avg_logprob": -0.1421234965324402, "compression_ratio": 1.6263736263736264, "no_speech_prob": 1.3845755347574595e-05}, {"id": 82, "seek": 57480, "start": 587.52, "end": 595.8399999999999, "text": " every model of this kind. And basically, surprise, surprise, it turns out that the answer is", "tokens": [633, 2316, 295, 341, 733, 13, 400, 1936, 11, 6365, 11, 6365, 11, 309, 4523, 484, 300, 264, 1867, 307], "temperature": 0.0, "avg_logprob": -0.1421234965324402, "compression_ratio": 1.6263736263736264, "no_speech_prob": 1.3845755347574595e-05}, {"id": 83, "seek": 57480, "start": 595.8399999999999, "end": 599.0999999999999, "text": " to use a neural network.", "tokens": [281, 764, 257, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.1421234965324402, "compression_ratio": 1.6263736263736264, "no_speech_prob": 1.3845755347574595e-05}, {"id": 84, "seek": 59910, "start": 599.1, "end": 608.48, "text": " So I need to warn you again, what I'm going to teach you here is very, very uncool. You'll", "tokens": [407, 286, 643, 281, 12286, 291, 797, 11, 437, 286, 478, 516, 281, 2924, 291, 510, 307, 588, 11, 588, 6219, 1092, 13, 509, 603], "temperature": 0.0, "avg_logprob": -0.11068538717321448, "compression_ratio": 1.4126984126984128, "no_speech_prob": 2.3551041522296146e-05}, {"id": 85, "seek": 59910, "start": 608.48, "end": 614.36, "text": " never read about it from DeepMind or OpenAI. It doesn't involve any robot arms, it doesn't", "tokens": [1128, 1401, 466, 309, 490, 14895, 44, 471, 420, 7238, 48698, 13, 467, 1177, 380, 9494, 604, 7881, 5812, 11, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.11068538717321448, "compression_ratio": 1.4126984126984128, "no_speech_prob": 2.3551041522296146e-05}, {"id": 86, "seek": 59910, "start": 614.36, "end": 622.12, "text": " involve thousands of GPUs. It's the kind of boring stuff that normal companies use to", "tokens": [9494, 5383, 295, 18407, 82, 13, 467, 311, 264, 733, 295, 9989, 1507, 300, 2710, 3431, 764, 281], "temperature": 0.0, "avg_logprob": -0.11068538717321448, "compression_ratio": 1.4126984126984128, "no_speech_prob": 2.3551041522296146e-05}, {"id": 87, "seek": 62212, "start": 622.12, "end": 629.32, "text": " make more money or spend less money or satisfy their customers. So I apologize deeply for", "tokens": [652, 544, 1460, 420, 3496, 1570, 1460, 420, 19319, 641, 4581, 13, 407, 286, 12328, 8760, 337], "temperature": 0.0, "avg_logprob": -0.1285106900712134, "compression_ratio": 1.4365482233502538, "no_speech_prob": 7.411154001601972e-06}, {"id": 88, "seek": 62212, "start": 629.32, "end": 633.36, "text": " that oversight.", "tokens": [300, 29146, 13], "temperature": 0.0, "avg_logprob": -0.1285106900712134, "compression_ratio": 1.4365482233502538, "no_speech_prob": 7.411154001601972e-06}, {"id": 89, "seek": 62212, "start": 633.36, "end": 641.84, "text": " Having said that, in the 25 years or more I've been doing machine learning work applied", "tokens": [10222, 848, 300, 11, 294, 264, 3552, 924, 420, 544, 286, 600, 668, 884, 3479, 2539, 589, 6456], "temperature": 0.0, "avg_logprob": -0.1285106900712134, "compression_ratio": 1.4365482233502538, "no_speech_prob": 7.411154001601972e-06}, {"id": 90, "seek": 62212, "start": 641.84, "end": 650.42, "text": " in the real world, 98% of it has been this kind of data. Whether it be when I was working", "tokens": [294, 264, 957, 1002, 11, 20860, 4, 295, 309, 575, 668, 341, 733, 295, 1412, 13, 8503, 309, 312, 562, 286, 390, 1364], "temperature": 0.0, "avg_logprob": -0.1285106900712134, "compression_ratio": 1.4365482233502538, "no_speech_prob": 7.411154001601972e-06}, {"id": 91, "seek": 65042, "start": 650.42, "end": 658.4799999999999, "text": " in agriculture, I've worked in wool, macadamia nuts, and rice, and we were figuring out how", "tokens": [294, 14837, 11, 286, 600, 2732, 294, 24181, 11, 7912, 345, 335, 654, 10483, 11, 293, 5090, 11, 293, 321, 645, 15213, 484, 577], "temperature": 0.0, "avg_logprob": -0.2041790622404252, "compression_ratio": 1.7380952380952381, "no_speech_prob": 3.219019708922133e-05}, {"id": 92, "seek": 65042, "start": 658.4799999999999, "end": 664.04, "text": " full our barrels were going to be, whether we needed more, we were figuring out how to", "tokens": [1577, 527, 33138, 645, 516, 281, 312, 11, 1968, 321, 2978, 544, 11, 321, 645, 15213, 484, 577, 281], "temperature": 0.0, "avg_logprob": -0.2041790622404252, "compression_ratio": 1.7380952380952381, "no_speech_prob": 3.219019708922133e-05}, {"id": 93, "seek": 65042, "start": 664.04, "end": 670.8399999999999, "text": " set futures markets prices for agricultural goods, whatever. I worked in mining and brewing,", "tokens": [992, 26071, 8383, 7901, 337, 19587, 10179, 11, 2035, 13, 286, 2732, 294, 15512, 293, 39019, 11], "temperature": 0.0, "avg_logprob": -0.2041790622404252, "compression_ratio": 1.7380952380952381, "no_speech_prob": 3.219019708922133e-05}, {"id": 94, "seek": 65042, "start": 670.8399999999999, "end": 678.3199999999999, "text": " which required analyzing all kinds of engineering data and sales data. I've worked in banking", "tokens": [597, 4739, 23663, 439, 3685, 295, 7043, 1412, 293, 5763, 1412, 13, 286, 600, 2732, 294, 18261], "temperature": 0.0, "avg_logprob": -0.2041790622404252, "compression_ratio": 1.7380952380952381, "no_speech_prob": 3.219019708922133e-05}, {"id": 95, "seek": 67832, "start": 678.32, "end": 685.08, "text": " that required looking at transaction account pricing and risk and fraud. All of these areas", "tokens": [300, 4739, 1237, 412, 14425, 2696, 17621, 293, 3148, 293, 14560, 13, 1057, 295, 613, 3179], "temperature": 0.0, "avg_logprob": -0.1429961257510715, "compression_ratio": 1.4829268292682927, "no_speech_prob": 2.2125004761619493e-05}, {"id": 96, "seek": 67832, "start": 685.08, "end": 689.44, "text": " basically involve this kind of data.", "tokens": [1936, 9494, 341, 733, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1429961257510715, "compression_ratio": 1.4829268292682927, "no_speech_prob": 2.2125004761619493e-05}, {"id": 97, "seek": 67832, "start": 689.44, "end": 697.36, "text": " So I think although no one publishes stuff about this, because anybody who comes out", "tokens": [407, 286, 519, 4878, 572, 472, 11374, 279, 1507, 466, 341, 11, 570, 4472, 567, 1487, 484], "temperature": 0.0, "avg_logprob": -0.1429961257510715, "compression_ratio": 1.4829268292682927, "no_speech_prob": 2.2125004761619493e-05}, {"id": 98, "seek": 67832, "start": 697.36, "end": 703.6, "text": " of a Stanford PhD and goes to Google doesn't know about any of those things, I guess, it's", "tokens": [295, 257, 20374, 14476, 293, 1709, 281, 3329, 1177, 380, 458, 466, 604, 295, 729, 721, 11, 286, 2041, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.1429961257510715, "compression_ratio": 1.4829268292682927, "no_speech_prob": 2.2125004761619493e-05}, {"id": 99, "seek": 70360, "start": 703.6, "end": 712.32, "text": " probably the most useful thing for the vast majority of people. And excitingly, it turns", "tokens": [1391, 264, 881, 4420, 551, 337, 264, 8369, 6286, 295, 561, 13, 400, 4670, 356, 11, 309, 4523], "temperature": 0.0, "avg_logprob": -0.12707688591697, "compression_ratio": 1.4864864864864864, "no_speech_prob": 4.860357421421213e-06}, {"id": 100, "seek": 70360, "start": 712.32, "end": 721.88, "text": " out that you don't need to learn any new techniques at all. In fact, the model that they got this", "tokens": [484, 300, 291, 500, 380, 643, 281, 1466, 604, 777, 7512, 412, 439, 13, 682, 1186, 11, 264, 2316, 300, 436, 658, 341], "temperature": 0.0, "avg_logprob": -0.12707688591697, "compression_ratio": 1.4864864864864864, "no_speech_prob": 4.860357421421213e-06}, {"id": 101, "seek": 70360, "start": 721.88, "end": 727.64, "text": " third place result with, a very very simple model, is basically one where each different", "tokens": [2636, 1081, 1874, 365, 11, 257, 588, 588, 2199, 2316, 11, 307, 1936, 472, 689, 1184, 819], "temperature": 0.0, "avg_logprob": -0.12707688591697, "compression_ratio": 1.4864864864864864, "no_speech_prob": 4.860357421421213e-06}, {"id": 102, "seek": 72764, "start": 727.64, "end": 734.4399999999999, "text": " categorical variable was one-hot encoded and chucked into an embedding layer. The embedding", "tokens": [19250, 804, 7006, 390, 472, 12, 12194, 2058, 12340, 293, 20870, 292, 666, 364, 12240, 3584, 4583, 13, 440, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.16910085012746412, "compression_ratio": 1.7277227722772277, "no_speech_prob": 2.144438258255832e-05}, {"id": 103, "seek": 72764, "start": 734.4399999999999, "end": 738.56, "text": " layers were concatenated and chucked through a dense layer, and then a second dense layer,", "tokens": [7914, 645, 1588, 7186, 770, 293, 20870, 292, 807, 257, 18011, 4583, 11, 293, 550, 257, 1150, 18011, 4583, 11], "temperature": 0.0, "avg_logprob": -0.16910085012746412, "compression_ratio": 1.7277227722772277, "no_speech_prob": 2.144438258255832e-05}, {"id": 104, "seek": 72764, "start": 738.56, "end": 748.16, "text": " and then went through a sigmoid function into an output layer. So very simple.", "tokens": [293, 550, 1437, 807, 257, 4556, 3280, 327, 2445, 666, 364, 5598, 4583, 13, 407, 588, 2199, 13], "temperature": 0.0, "avg_logprob": -0.16910085012746412, "compression_ratio": 1.7277227722772277, "no_speech_prob": 2.144438258255832e-05}, {"id": 105, "seek": 72764, "start": 748.16, "end": 752.84, "text": " The continuous variables, they haven't drawn here. All these pictures are going to come", "tokens": [440, 10957, 9102, 11, 436, 2378, 380, 10117, 510, 13, 1057, 613, 5242, 366, 516, 281, 808], "temperature": 0.0, "avg_logprob": -0.16910085012746412, "compression_ratio": 1.7277227722772277, "no_speech_prob": 2.144438258255832e-05}, {"id": 106, "seek": 75284, "start": 752.84, "end": 758.32, "text": " straight from this paper, which the folks that came third kindly actually wrote a paper", "tokens": [2997, 490, 341, 3035, 11, 597, 264, 4024, 300, 1361, 2636, 29736, 767, 4114, 257, 3035], "temperature": 0.0, "avg_logprob": -0.1717713584362621, "compression_ratio": 1.49, "no_speech_prob": 1.2805166079488117e-05}, {"id": 107, "seek": 75284, "start": 758.32, "end": 766.2, "text": " about this. The continuous variables basically get fed directly into the dense layer. So", "tokens": [466, 341, 13, 440, 10957, 9102, 1936, 483, 4636, 3838, 666, 264, 18011, 4583, 13, 407], "temperature": 0.0, "avg_logprob": -0.1717713584362621, "compression_ratio": 1.49, "no_speech_prob": 1.2805166079488117e-05}, {"id": 108, "seek": 75284, "start": 766.2, "end": 770.72, "text": " that's the structure of the model.", "tokens": [300, 311, 264, 3877, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1717713584362621, "compression_ratio": 1.49, "no_speech_prob": 1.2805166079488117e-05}, {"id": 109, "seek": 75284, "start": 770.72, "end": 777.2800000000001, "text": " How well does it work? So the short answer is, compared to k-nearest neighbors, random", "tokens": [1012, 731, 775, 309, 589, 30, 407, 264, 2099, 1867, 307, 11, 5347, 281, 350, 12, 716, 17363, 12512, 11, 4974], "temperature": 0.0, "avg_logprob": -0.1717713584362621, "compression_ratio": 1.49, "no_speech_prob": 1.2805166079488117e-05}, {"id": 110, "seek": 77728, "start": 777.28, "end": 786.36, "text": " forests and GBMs, just a simple neural network beats all of those approaches, just with standard", "tokens": [21700, 293, 460, 18345, 82, 11, 445, 257, 2199, 18161, 3209, 16447, 439, 295, 729, 11587, 11, 445, 365, 3832], "temperature": 0.0, "avg_logprob": -0.18281625600961537, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.6378122533496935e-06}, {"id": 111, "seek": 77728, "start": 786.36, "end": 794.76, "text": " one-hot encoding. But then the EE is entity embeddings. So adding in this idea of using", "tokens": [472, 12, 12194, 43430, 13, 583, 550, 264, 33685, 307, 13977, 12240, 29432, 13, 407, 5127, 294, 341, 1558, 295, 1228], "temperature": 0.0, "avg_logprob": -0.18281625600961537, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.6378122533496935e-06}, {"id": 112, "seek": 77728, "start": 794.76, "end": 800.9599999999999, "text": " embeddings, interestingly you can take the embeddings trained by a neural network and", "tokens": [12240, 29432, 11, 25873, 291, 393, 747, 264, 12240, 29432, 8895, 538, 257, 18161, 3209, 293], "temperature": 0.0, "avg_logprob": -0.18281625600961537, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.6378122533496935e-06}, {"id": 113, "seek": 80096, "start": 800.96, "end": 808.44, "text": " feed them into a KNN or a random forest or a GBM. In fact, using embeddings with every", "tokens": [3154, 552, 666, 257, 26967, 45, 420, 257, 4974, 6719, 420, 257, 460, 18345, 13, 682, 1186, 11, 1228, 12240, 29432, 365, 633], "temperature": 0.0, "avg_logprob": -0.1756401795607347, "compression_ratio": 1.6310160427807487, "no_speech_prob": 3.668840918180649e-06}, {"id": 114, "seek": 80096, "start": 808.44, "end": 815.2800000000001, "text": " one of those things is way better than all of those things are anything other than neural", "tokens": [472, 295, 729, 721, 307, 636, 1101, 813, 439, 295, 729, 721, 366, 1340, 661, 813, 18161], "temperature": 0.0, "avg_logprob": -0.1756401795607347, "compression_ratio": 1.6310160427807487, "no_speech_prob": 3.668840918180649e-06}, {"id": 115, "seek": 80096, "start": 815.2800000000001, "end": 820.36, "text": " networks. So that's pretty interesting. And then if you use the embeddings with a neural", "tokens": [9590, 13, 407, 300, 311, 1238, 1880, 13, 400, 550, 498, 291, 764, 264, 12240, 29432, 365, 257, 18161], "temperature": 0.0, "avg_logprob": -0.1756401795607347, "compression_ratio": 1.6310160427807487, "no_speech_prob": 3.668840918180649e-06}, {"id": 116, "seek": 80096, "start": 820.36, "end": 823.2800000000001, "text": " network, you get the best result still.", "tokens": [3209, 11, 291, 483, 264, 1151, 1874, 920, 13], "temperature": 0.0, "avg_logprob": -0.1756401795607347, "compression_ratio": 1.6310160427807487, "no_speech_prob": 3.668840918180649e-06}, {"id": 117, "seek": 82328, "start": 823.28, "end": 835.4, "text": " So this actually is kind of fascinating because training this neural network took me some", "tokens": [407, 341, 767, 307, 733, 295, 10343, 570, 3097, 341, 18161, 3209, 1890, 385, 512], "temperature": 0.0, "avg_logprob": -0.2438718208899865, "compression_ratio": 1.4802259887005649, "no_speech_prob": 2.5612707759137265e-06}, {"id": 118, "seek": 82328, "start": 835.4, "end": 846.68, "text": " hours on a Titan X, whereas training the GBM took I think less than a second. It was so", "tokens": [2496, 322, 257, 17731, 1783, 11, 9735, 3097, 264, 460, 18345, 1890, 286, 519, 1570, 813, 257, 1150, 13, 467, 390, 370], "temperature": 0.0, "avg_logprob": -0.2438718208899865, "compression_ratio": 1.4802259887005649, "no_speech_prob": 2.5612707759137265e-06}, {"id": 119, "seek": 82328, "start": 846.68, "end": 851.8399999999999, "text": " fast I thought I had screwed something up. And then I tried running it, it was like,", "tokens": [2370, 286, 1194, 286, 632, 20331, 746, 493, 13, 400, 550, 286, 3031, 2614, 309, 11, 309, 390, 411, 11], "temperature": 0.0, "avg_logprob": -0.2438718208899865, "compression_ratio": 1.4802259887005649, "no_speech_prob": 2.5612707759137265e-06}, {"id": 120, "seek": 85184, "start": 851.84, "end": 859.2800000000001, "text": " holy shit, it's giving accurate predictions. So GBMs and random forests are so fast. So", "tokens": [10622, 4611, 11, 309, 311, 2902, 8559, 21264, 13, 407, 460, 18345, 82, 293, 4974, 21700, 366, 370, 2370, 13, 407], "temperature": 0.0, "avg_logprob": -0.1363466436212713, "compression_ratio": 1.6103896103896105, "no_speech_prob": 1.4063843082112726e-05}, {"id": 121, "seek": 85184, "start": 859.2800000000001, "end": 865.0400000000001, "text": " in your organization, you could try taking everything that you could think of as a categorical", "tokens": [294, 428, 4475, 11, 291, 727, 853, 1940, 1203, 300, 291, 727, 519, 295, 382, 257, 19250, 804], "temperature": 0.0, "avg_logprob": -0.1363466436212713, "compression_ratio": 1.6103896103896105, "no_speech_prob": 1.4063843082112726e-05}, {"id": 122, "seek": 85184, "start": 865.0400000000001, "end": 873.36, "text": " variable and once a month train a neural net with embeddings and then store those embeddings", "tokens": [7006, 293, 1564, 257, 1618, 3847, 257, 18161, 2533, 365, 12240, 29432, 293, 550, 3531, 729, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.1363466436212713, "compression_ratio": 1.6103896103896105, "no_speech_prob": 1.4063843082112726e-05}, {"id": 123, "seek": 85184, "start": 873.36, "end": 880.6800000000001, "text": " in a database table and tell all of your business users, hey, anytime you want to create a model", "tokens": [294, 257, 8149, 3199, 293, 980, 439, 295, 428, 1606, 5022, 11, 4177, 11, 13038, 291, 528, 281, 1884, 257, 2316], "temperature": 0.0, "avg_logprob": -0.1363466436212713, "compression_ratio": 1.6103896103896105, "no_speech_prob": 1.4063843082112726e-05}, {"id": 124, "seek": 88068, "start": 880.68, "end": 889.4799999999999, "text": " that incorporates day of week or store ID or customer ID, you can go grab the embeddings.", "tokens": [300, 50193, 786, 295, 1243, 420, 3531, 7348, 420, 5474, 7348, 11, 291, 393, 352, 4444, 264, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.17842024394444056, "compression_ratio": 1.6089385474860336, "no_speech_prob": 1.6964149835985154e-05}, {"id": 125, "seek": 88068, "start": 889.4799999999999, "end": 894.3199999999999, "text": " So they're basically like word vectors, but they're customer vectors and store vectors", "tokens": [407, 436, 434, 1936, 411, 1349, 18875, 11, 457, 436, 434, 5474, 18875, 293, 3531, 18875], "temperature": 0.0, "avg_logprob": -0.17842024394444056, "compression_ratio": 1.6089385474860336, "no_speech_prob": 1.6964149835985154e-05}, {"id": 126, "seek": 88068, "start": 894.3199999999999, "end": 896.3199999999999, "text": " and product vectors.", "tokens": [293, 1674, 18875, 13], "temperature": 0.0, "avg_logprob": -0.17842024394444056, "compression_ratio": 1.6089385474860336, "no_speech_prob": 1.6964149835985154e-05}, {"id": 127, "seek": 88068, "start": 896.3199999999999, "end": 904.28, "text": " So I've never seen anybody write about this other than this paper. And even in this paper,", "tokens": [407, 286, 600, 1128, 1612, 4472, 2464, 466, 341, 661, 813, 341, 3035, 13, 400, 754, 294, 341, 3035, 11], "temperature": 0.0, "avg_logprob": -0.17842024394444056, "compression_ratio": 1.6089385474860336, "no_speech_prob": 1.6964149835985154e-05}, {"id": 128, "seek": 90428, "start": 904.28, "end": 913.36, "text": " they don't really get to this hugely important idea of what you could do with these embeddings.", "tokens": [436, 500, 380, 534, 483, 281, 341, 27417, 1021, 1558, 295, 437, 291, 727, 360, 365, 613, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.21744447343804862, "compression_ratio": 1.6179245283018868, "no_speech_prob": 4.264503877493553e-05}, {"id": 129, "seek": 90428, "start": 913.36, "end": 919.72, "text": " What's the difference between A and B and C? Is it different data types flowing in?", "tokens": [708, 311, 264, 2649, 1296, 316, 293, 363, 293, 383, 30, 1119, 309, 819, 1412, 3467, 13974, 294, 30], "temperature": 0.0, "avg_logprob": -0.21744447343804862, "compression_ratio": 1.6179245283018868, "no_speech_prob": 4.264503877493553e-05}, {"id": 130, "seek": 90428, "start": 919.72, "end": 924.88, "text": " We're going to get to that in a moment. Basically the different things are like A might be the", "tokens": [492, 434, 516, 281, 483, 281, 300, 294, 257, 1623, 13, 8537, 264, 819, 721, 366, 411, 316, 1062, 312, 264], "temperature": 0.0, "avg_logprob": -0.21744447343804862, "compression_ratio": 1.6179245283018868, "no_speech_prob": 4.264503877493553e-05}, {"id": 131, "seek": 90428, "start": 924.88, "end": 933.36, "text": " store ID, B might be the product ID, and C might be the day of week.", "tokens": [3531, 7348, 11, 363, 1062, 312, 264, 1674, 7348, 11, 293, 383, 1062, 312, 264, 786, 295, 1243, 13], "temperature": 0.0, "avg_logprob": -0.21744447343804862, "compression_ratio": 1.6179245283018868, "no_speech_prob": 4.264503877493553e-05}, {"id": 132, "seek": 93336, "start": 933.36, "end": 940.12, "text": " One of the really nice things that they did in this paper was to then draw some projections", "tokens": [1485, 295, 264, 534, 1481, 721, 300, 436, 630, 294, 341, 3035, 390, 281, 550, 2642, 512, 32371], "temperature": 0.0, "avg_logprob": -0.17814984164395176, "compression_ratio": 1.727699530516432, "no_speech_prob": 1.4738182471774053e-05}, {"id": 133, "seek": 93336, "start": 940.12, "end": 943.48, "text": " of some of these embeddings. They just used t-sneak, it doesn't really matter what the", "tokens": [295, 512, 295, 613, 12240, 29432, 13, 814, 445, 1143, 256, 12, 82, 716, 514, 11, 309, 1177, 380, 534, 1871, 437, 264], "temperature": 0.0, "avg_logprob": -0.17814984164395176, "compression_ratio": 1.727699530516432, "no_speech_prob": 1.4738182471774053e-05}, {"id": 134, "seek": 93336, "start": 943.48, "end": 953.96, "text": " projection method is. But here's some interesting ideas. They took each state of Germany, this", "tokens": [22743, 3170, 307, 13, 583, 510, 311, 512, 1880, 3487, 13, 814, 1890, 1184, 1785, 295, 7244, 11, 341], "temperature": 0.0, "avg_logprob": -0.17814984164395176, "compression_ratio": 1.727699530516432, "no_speech_prob": 1.4738182471774053e-05}, {"id": 135, "seek": 93336, "start": 953.96, "end": 961.48, "text": " is all based in Germany, and they did a projection of the embeddings from the state field. And", "tokens": [307, 439, 2361, 294, 7244, 11, 293, 436, 630, 257, 22743, 295, 264, 12240, 29432, 490, 264, 1785, 2519, 13, 400], "temperature": 0.0, "avg_logprob": -0.17814984164395176, "compression_ratio": 1.727699530516432, "no_speech_prob": 1.4738182471774053e-05}, {"id": 136, "seek": 96148, "start": 961.48, "end": 963.76, "text": " here are those projections.", "tokens": [510, 366, 729, 32371, 13], "temperature": 0.0, "avg_logprob": -0.14213111287071592, "compression_ratio": 1.7227272727272727, "no_speech_prob": 6.748009127477417e-06}, {"id": 137, "seek": 96148, "start": 963.76, "end": 968.6800000000001, "text": " And I've drawn around them different colored circles, and you might notice the different", "tokens": [400, 286, 600, 10117, 926, 552, 819, 14332, 13040, 11, 293, 291, 1062, 3449, 264, 819], "temperature": 0.0, "avg_logprob": -0.14213111287071592, "compression_ratio": 1.7227272727272727, "no_speech_prob": 6.748009127477417e-06}, {"id": 138, "seek": 96148, "start": 968.6800000000001, "end": 974.0, "text": " colored circles exactly correspond to the different colored circles on a map of Germany.", "tokens": [14332, 13040, 2293, 6805, 281, 264, 819, 14332, 13040, 322, 257, 4471, 295, 7244, 13], "temperature": 0.0, "avg_logprob": -0.14213111287071592, "compression_ratio": 1.7227272727272727, "no_speech_prob": 6.748009127477417e-06}, {"id": 139, "seek": 96148, "start": 974.0, "end": 982.6, "text": " Now these were just random embeddings trained with SGD trying to predict sales in stores", "tokens": [823, 613, 645, 445, 4974, 12240, 29432, 8895, 365, 34520, 35, 1382, 281, 6069, 5763, 294, 9512], "temperature": 0.0, "avg_logprob": -0.14213111287071592, "compression_ratio": 1.7227272727272727, "no_speech_prob": 6.748009127477417e-06}, {"id": 140, "seek": 96148, "start": 982.6, "end": 989.84, "text": " at Rossmann. And yet somehow they've drawn a map of Germany. So obviously the reason", "tokens": [412, 16140, 14912, 13, 400, 1939, 6063, 436, 600, 10117, 257, 4471, 295, 7244, 13, 407, 2745, 264, 1778], "temperature": 0.0, "avg_logprob": -0.14213111287071592, "compression_ratio": 1.7227272727272727, "no_speech_prob": 6.748009127477417e-06}, {"id": 141, "seek": 98984, "start": 989.84, "end": 995.84, "text": " why is because things close to each other in Germany have similar behaviors around how", "tokens": [983, 307, 570, 721, 1998, 281, 1184, 661, 294, 7244, 362, 2531, 15501, 926, 577], "temperature": 0.0, "avg_logprob": -0.1756595114003057, "compression_ratio": 1.4619289340101522, "no_speech_prob": 2.7108118956675753e-05}, {"id": 142, "seek": 98984, "start": 995.84, "end": 1002.36, "text": " they respond to events, who buys what kinds of products, and so on and so forth. So that's", "tokens": [436, 4196, 281, 3931, 11, 567, 28153, 437, 3685, 295, 3383, 11, 293, 370, 322, 293, 370, 5220, 13, 407, 300, 311], "temperature": 0.0, "avg_logprob": -0.1756595114003057, "compression_ratio": 1.4619289340101522, "no_speech_prob": 2.7108118956675753e-05}, {"id": 143, "seek": 98984, "start": 1002.36, "end": 1007.2, "text": " like crazy fascinating.", "tokens": [411, 3219, 10343, 13], "temperature": 0.0, "avg_logprob": -0.1756595114003057, "compression_ratio": 1.4619289340101522, "no_speech_prob": 2.7108118956675753e-05}, {"id": 144, "seek": 98984, "start": 1007.2, "end": 1015.5600000000001, "text": " Here's the kind of bigger picture. Every one of these dots is the distance between two", "tokens": [1692, 311, 264, 733, 295, 3801, 3036, 13, 2048, 472, 295, 613, 15026, 307, 264, 4560, 1296, 732], "temperature": 0.0, "avg_logprob": -0.1756595114003057, "compression_ratio": 1.4619289340101522, "no_speech_prob": 2.7108118956675753e-05}, {"id": 145, "seek": 101556, "start": 1015.56, "end": 1022.0799999999999, "text": " stores. And this shows the correlation between the distance in embedding space versus the", "tokens": [9512, 13, 400, 341, 3110, 264, 20009, 1296, 264, 4560, 294, 12240, 3584, 1901, 5717, 264], "temperature": 0.0, "avg_logprob": -0.165608980438926, "compression_ratio": 1.7788018433179724, "no_speech_prob": 1.241130939888535e-05}, {"id": 146, "seek": 101556, "start": 1022.0799999999999, "end": 1028.6799999999998, "text": " actual distance between the stores. So you can basically see that there's a strong correlation", "tokens": [3539, 4560, 1296, 264, 9512, 13, 407, 291, 393, 1936, 536, 300, 456, 311, 257, 2068, 20009], "temperature": 0.0, "avg_logprob": -0.165608980438926, "compression_ratio": 1.7788018433179724, "no_speech_prob": 1.241130939888535e-05}, {"id": 147, "seek": 101556, "start": 1028.6799999999998, "end": 1033.0, "text": " between things being close to each other in real life and close to each other in these", "tokens": [1296, 721, 885, 1998, 281, 1184, 661, 294, 957, 993, 293, 1998, 281, 1184, 661, 294, 613], "temperature": 0.0, "avg_logprob": -0.165608980438926, "compression_ratio": 1.7788018433179724, "no_speech_prob": 1.241130939888535e-05}, {"id": 148, "seek": 101556, "start": 1033.0, "end": 1035.52, "text": " SGD trained embeddings.", "tokens": [34520, 35, 8895, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.165608980438926, "compression_ratio": 1.7788018433179724, "no_speech_prob": 1.241130939888535e-05}, {"id": 149, "seek": 101556, "start": 1035.52, "end": 1040.6399999999999, "text": " Here's a couple more pictures. All the lines are drawn on top of mine, but everything else", "tokens": [1692, 311, 257, 1916, 544, 5242, 13, 1057, 264, 3876, 366, 10117, 322, 1192, 295, 3892, 11, 457, 1203, 1646], "temperature": 0.0, "avg_logprob": -0.165608980438926, "compression_ratio": 1.7788018433179724, "no_speech_prob": 1.241130939888535e-05}, {"id": 150, "seek": 104064, "start": 1040.64, "end": 1045.92, "text": " is just from the paper. On the left is days of the week embedding. And you can see the", "tokens": [307, 445, 490, 264, 3035, 13, 1282, 264, 1411, 307, 1708, 295, 264, 1243, 12240, 3584, 13, 400, 291, 393, 536, 264], "temperature": 0.0, "avg_logprob": -0.13282666887555802, "compression_ratio": 1.7884615384615385, "no_speech_prob": 3.555959438017453e-06}, {"id": 151, "seek": 104064, "start": 1045.92, "end": 1050.48, "text": " days of the week that are near each other have ended up embedded close together. On", "tokens": [1708, 295, 264, 1243, 300, 366, 2651, 1184, 661, 362, 4590, 493, 16741, 1998, 1214, 13, 1282], "temperature": 0.0, "avg_logprob": -0.13282666887555802, "compression_ratio": 1.7884615384615385, "no_speech_prob": 3.555959438017453e-06}, {"id": 152, "seek": 104064, "start": 1050.48, "end": 1056.2, "text": " the right is months of the year embedding. Again, same thing. And you can see that the", "tokens": [264, 558, 307, 2493, 295, 264, 1064, 12240, 3584, 13, 3764, 11, 912, 551, 13, 400, 291, 393, 536, 300, 264], "temperature": 0.0, "avg_logprob": -0.13282666887555802, "compression_ratio": 1.7884615384615385, "no_speech_prob": 3.555959438017453e-06}, {"id": 153, "seek": 104064, "start": 1056.2, "end": 1061.94, "text": " weekend is clearly separate.", "tokens": [6711, 307, 4448, 4994, 13], "temperature": 0.0, "avg_logprob": -0.13282666887555802, "compression_ratio": 1.7884615384615385, "no_speech_prob": 3.555959438017453e-06}, {"id": 154, "seek": 104064, "start": 1061.94, "end": 1069.92, "text": " So that's where we're going to get to. And I'm actually going to take you through the", "tokens": [407, 300, 311, 689, 321, 434, 516, 281, 483, 281, 13, 400, 286, 478, 767, 516, 281, 747, 291, 807, 264], "temperature": 0.0, "avg_logprob": -0.13282666887555802, "compression_ratio": 1.7884615384615385, "no_speech_prob": 3.555959438017453e-06}, {"id": 155, "seek": 106992, "start": 1069.92, "end": 1077.3600000000001, "text": " end-to-end process. I rebuilt the end-to-end process from scratch and tried to make it", "tokens": [917, 12, 1353, 12, 521, 1399, 13, 286, 38532, 264, 917, 12, 1353, 12, 521, 1399, 490, 8459, 293, 3031, 281, 652, 309], "temperature": 0.0, "avg_logprob": -0.12131222434665846, "compression_ratio": 1.5251396648044693, "no_speech_prob": 1.593622801010497e-05}, {"id": 156, "seek": 106992, "start": 1077.3600000000001, "end": 1084.8000000000002, "text": " in as few lines as code as possible because we just haven't really looked at any of these", "tokens": [294, 382, 1326, 3876, 382, 3089, 382, 1944, 570, 321, 445, 2378, 380, 534, 2956, 412, 604, 295, 613], "temperature": 0.0, "avg_logprob": -0.12131222434665846, "compression_ratio": 1.5251396648044693, "no_speech_prob": 1.593622801010497e-05}, {"id": 157, "seek": 106992, "start": 1084.8000000000002, "end": 1095.6000000000001, "text": " structured data type problems before. So it's kind of a very different process, even a different", "tokens": [18519, 1412, 2010, 2740, 949, 13, 407, 309, 311, 733, 295, 257, 588, 819, 1399, 11, 754, 257, 819], "temperature": 0.0, "avg_logprob": -0.12131222434665846, "compression_ratio": 1.5251396648044693, "no_speech_prob": 1.593622801010497e-05}, {"id": 158, "seek": 109560, "start": 1095.6, "end": 1102.24, "text": " set of techniques.", "tokens": [992, 295, 7512, 13], "temperature": 0.0, "avg_logprob": -0.18828166448152983, "compression_ratio": 1.3732394366197183, "no_speech_prob": 1.8630846170708537e-05}, {"id": 159, "seek": 109560, "start": 1102.24, "end": 1109.4399999999998, "text": " So we import the usual stuff. When you try to do this stuff yourself, you'll find 3 or", "tokens": [407, 321, 974, 264, 7713, 1507, 13, 1133, 291, 853, 281, 360, 341, 1507, 1803, 11, 291, 603, 915, 805, 420], "temperature": 0.0, "avg_logprob": -0.18828166448152983, "compression_ratio": 1.3732394366197183, "no_speech_prob": 1.8630846170708537e-05}, {"id": 160, "seek": 109560, "start": 1109.4399999999998, "end": 1117.8, "text": " 4 libraries we haven't used before. So when you hit something that says module not found,", "tokens": [1017, 15148, 321, 2378, 380, 1143, 949, 13, 407, 562, 291, 2045, 746, 300, 1619, 10088, 406, 1352, 11], "temperature": 0.0, "avg_logprob": -0.18828166448152983, "compression_ratio": 1.3732394366197183, "no_speech_prob": 1.8630846170708537e-05}, {"id": 161, "seek": 111780, "start": 1117.8, "end": 1125.8, "text": " you can just pip install all these things. They're pure Python. We'll talk about them", "tokens": [291, 393, 445, 8489, 3625, 439, 613, 721, 13, 814, 434, 6075, 15329, 13, 492, 603, 751, 466, 552], "temperature": 0.0, "avg_logprob": -0.12835812312300487, "compression_ratio": 1.576271186440678, "no_speech_prob": 1.3419744391285349e-05}, {"id": 162, "seek": 111780, "start": 1125.8, "end": 1126.8, "text": " as we get to them.", "tokens": [382, 321, 483, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.12835812312300487, "compression_ratio": 1.576271186440678, "no_speech_prob": 1.3419744391285349e-05}, {"id": 163, "seek": 111780, "start": 1126.8, "end": 1132.8, "text": " So the data that comes from Kaggle comes down as a bunch of CSV files. I wrote a quick thing", "tokens": [407, 264, 1412, 300, 1487, 490, 48751, 22631, 1487, 760, 382, 257, 3840, 295, 48814, 7098, 13, 286, 4114, 257, 1702, 551], "temperature": 0.0, "avg_logprob": -0.12835812312300487, "compression_ratio": 1.576271186440678, "no_speech_prob": 1.3419744391285349e-05}, {"id": 164, "seek": 111780, "start": 1132.8, "end": 1140.72, "text": " to combine some of those CSVs together. This was one of those competitions where people", "tokens": [281, 10432, 512, 295, 729, 48814, 82, 1214, 13, 639, 390, 472, 295, 729, 26185, 689, 561], "temperature": 0.0, "avg_logprob": -0.12835812312300487, "compression_ratio": 1.576271186440678, "no_speech_prob": 1.3419744391285349e-05}, {"id": 165, "seek": 111780, "start": 1140.72, "end": 1146.32, "text": " were allowed to use additional external data as long as they were shared on the forum.", "tokens": [645, 4350, 281, 764, 4497, 8320, 1412, 382, 938, 382, 436, 645, 5507, 322, 264, 17542, 13], "temperature": 0.0, "avg_logprob": -0.12835812312300487, "compression_ratio": 1.576271186440678, "no_speech_prob": 1.3419744391285349e-05}, {"id": 166, "seek": 114632, "start": 1146.32, "end": 1152.3999999999999, "text": " So the data I'll share with you, I'm going to combine it all into one place for you.", "tokens": [407, 264, 1412, 286, 603, 2073, 365, 291, 11, 286, 478, 516, 281, 10432, 309, 439, 666, 472, 1081, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.09253548602668607, "compression_ratio": 1.705069124423963, "no_speech_prob": 1.0782992831082083e-05}, {"id": 167, "seek": 114632, "start": 1152.3999999999999, "end": 1157.08, "text": " So I've commented these out because the stuff I'll give you will have already run this concatenation", "tokens": [407, 286, 600, 26940, 613, 484, 570, 264, 1507, 286, 603, 976, 291, 486, 362, 1217, 1190, 341, 1588, 7186, 399], "temperature": 0.0, "avg_logprob": -0.09253548602668607, "compression_ratio": 1.705069124423963, "no_speech_prob": 1.0782992831082083e-05}, {"id": 168, "seek": 114632, "start": 1157.08, "end": 1158.6799999999998, "text": " process.", "tokens": [1399, 13], "temperature": 0.0, "avg_logprob": -0.09253548602668607, "compression_ratio": 1.705069124423963, "no_speech_prob": 1.0782992831082083e-05}, {"id": 169, "seek": 114632, "start": 1158.6799999999998, "end": 1164.96, "text": " So the basic tables that you're going to get access to is the training set itself, a list", "tokens": [407, 264, 3875, 8020, 300, 291, 434, 516, 281, 483, 2105, 281, 307, 264, 3097, 992, 2564, 11, 257, 1329], "temperature": 0.0, "avg_logprob": -0.09253548602668607, "compression_ratio": 1.705069124423963, "no_speech_prob": 1.0782992831082083e-05}, {"id": 170, "seek": 114632, "start": 1164.96, "end": 1173.8, "text": " of stores, a list of which state each store is in, a list of the abbreviation and the", "tokens": [295, 9512, 11, 257, 1329, 295, 597, 1785, 1184, 3531, 307, 294, 11, 257, 1329, 295, 264, 35839, 399, 293, 264], "temperature": 0.0, "avg_logprob": -0.09253548602668607, "compression_ratio": 1.705069124423963, "no_speech_prob": 1.0782992831082083e-05}, {"id": 171, "seek": 117380, "start": 1173.8, "end": 1180.9199999999998, "text": " name of each state in Germany, a list of data from Google Trends. So if you've used Google", "tokens": [1315, 295, 1184, 1785, 294, 7244, 11, 257, 1329, 295, 1412, 490, 3329, 37417, 82, 13, 407, 498, 291, 600, 1143, 3329], "temperature": 0.0, "avg_logprob": -0.10877906757852306, "compression_ratio": 1.6565217391304348, "no_speech_prob": 1.2606413292814977e-05}, {"id": 172, "seek": 117380, "start": 1180.9199999999998, "end": 1187.56, "text": " Trends, you can basically see how particular keywords change over time. I don't actually", "tokens": [37417, 82, 11, 291, 393, 1936, 536, 577, 1729, 21009, 1319, 670, 565, 13, 286, 500, 380, 767], "temperature": 0.0, "avg_logprob": -0.10877906757852306, "compression_ratio": 1.6565217391304348, "no_speech_prob": 1.2606413292814977e-05}, {"id": 173, "seek": 117380, "start": 1187.56, "end": 1193.56, "text": " know which keywords they used, but somebody found that there were some Google Trends keywords", "tokens": [458, 597, 21009, 436, 1143, 11, 457, 2618, 1352, 300, 456, 645, 512, 3329, 37417, 82, 21009], "temperature": 0.0, "avg_logprob": -0.10877906757852306, "compression_ratio": 1.6565217391304348, "no_speech_prob": 1.2606413292814977e-05}, {"id": 174, "seek": 117380, "start": 1193.56, "end": 1199.04, "text": " that correlated well, so we've got access to those. Some information about the weather", "tokens": [300, 38574, 731, 11, 370, 321, 600, 658, 2105, 281, 729, 13, 2188, 1589, 466, 264, 5503], "temperature": 0.0, "avg_logprob": -0.10877906757852306, "compression_ratio": 1.6565217391304348, "no_speech_prob": 1.2606413292814977e-05}, {"id": 175, "seek": 117380, "start": 1199.04, "end": 1201.28, "text": " and then a test set.", "tokens": [293, 550, 257, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.10877906757852306, "compression_ratio": 1.6565217391304348, "no_speech_prob": 1.2606413292814977e-05}, {"id": 176, "seek": 120128, "start": 1201.28, "end": 1206.76, "text": " So I'm not sure that we've really used Pandas much if at all yet, so let's talk a bit about", "tokens": [407, 286, 478, 406, 988, 300, 321, 600, 534, 1143, 16995, 296, 709, 498, 412, 439, 1939, 11, 370, 718, 311, 751, 257, 857, 466], "temperature": 0.0, "avg_logprob": -0.17440115730717498, "compression_ratio": 1.6590909090909092, "no_speech_prob": 9.972787665901706e-06}, {"id": 177, "seek": 120128, "start": 1206.76, "end": 1213.48, "text": " Pandas. Pandas lets us take this kind of structured data and manipulate it in similar ways to", "tokens": [16995, 296, 13, 16995, 296, 6653, 505, 747, 341, 733, 295, 18519, 1412, 293, 20459, 309, 294, 2531, 2098, 281], "temperature": 0.0, "avg_logprob": -0.17440115730717498, "compression_ratio": 1.6590909090909092, "no_speech_prob": 9.972787665901706e-06}, {"id": 178, "seek": 120128, "start": 1213.48, "end": 1219.68, "text": " the way you would manipulate it in a database. So Pandas, just like NumPy, tends to become", "tokens": [264, 636, 291, 576, 20459, 309, 294, 257, 8149, 13, 407, 16995, 296, 11, 445, 411, 22592, 47, 88, 11, 12258, 281, 1813], "temperature": 0.0, "avg_logprob": -0.17440115730717498, "compression_ratio": 1.6590909090909092, "no_speech_prob": 9.972787665901706e-06}, {"id": 179, "seek": 120128, "start": 1219.68, "end": 1228.92, "text": " NP, Pandas tends to become PD. So pd.read.csv is going to return a data frame. So a data", "tokens": [38611, 11, 16995, 296, 12258, 281, 1813, 10464, 13, 407, 280, 67, 13, 2538, 13, 14368, 85, 307, 516, 281, 2736, 257, 1412, 3920, 13, 407, 257, 1412], "temperature": 0.0, "avg_logprob": -0.17440115730717498, "compression_ratio": 1.6590909090909092, "no_speech_prob": 9.972787665901706e-06}, {"id": 180, "seek": 122892, "start": 1228.92, "end": 1236.16, "text": " frame is like a database table. If you've used R, it's called the same thing. So this", "tokens": [3920, 307, 411, 257, 8149, 3199, 13, 759, 291, 600, 1143, 497, 11, 309, 311, 1219, 264, 912, 551, 13, 407, 341], "temperature": 0.0, "avg_logprob": -0.15545186996459961, "compression_ratio": 1.5307262569832403, "no_speech_prob": 6.27667930075404e-07}, {"id": 181, "seek": 122892, "start": 1236.16, "end": 1244.76, "text": " read.csv is going to return a data frame containing the information from this CSV file. We're", "tokens": [1401, 13, 14368, 85, 307, 516, 281, 2736, 257, 1412, 3920, 19273, 264, 1589, 490, 341, 48814, 3991, 13, 492, 434], "temperature": 0.0, "avg_logprob": -0.15545186996459961, "compression_ratio": 1.5307262569832403, "no_speech_prob": 6.27667930075404e-07}, {"id": 182, "seek": 122892, "start": 1244.76, "end": 1249.44, "text": " going to go through each one of those table names and read the CSV. So this list comprehension", "tokens": [516, 281, 352, 807, 1184, 472, 295, 729, 3199, 5288, 293, 1401, 264, 48814, 13, 407, 341, 1329, 44991], "temperature": 0.0, "avg_logprob": -0.15545186996459961, "compression_ratio": 1.5307262569832403, "no_speech_prob": 6.27667930075404e-07}, {"id": 183, "seek": 124944, "start": 1249.44, "end": 1261.3200000000002, "text": " is going to return a list of data frames. So I can now go ahead and display the head,", "tokens": [307, 516, 281, 2736, 257, 1329, 295, 1412, 12083, 13, 407, 286, 393, 586, 352, 2286, 293, 4674, 264, 1378, 11], "temperature": 0.0, "avg_logprob": -0.1536488099531694, "compression_ratio": 1.5680473372781065, "no_speech_prob": 7.811456157469365e-07}, {"id": 184, "seek": 124944, "start": 1261.3200000000002, "end": 1265.92, "text": " so the first 5 rows from each table. That's a good way to get a sense of what these tables", "tokens": [370, 264, 700, 1025, 13241, 490, 1184, 3199, 13, 663, 311, 257, 665, 636, 281, 483, 257, 2020, 295, 437, 613, 8020], "temperature": 0.0, "avg_logprob": -0.1536488099531694, "compression_ratio": 1.5680473372781065, "no_speech_prob": 7.811456157469365e-07}, {"id": 185, "seek": 124944, "start": 1265.92, "end": 1267.1200000000001, "text": " are.", "tokens": [366, 13], "temperature": 0.0, "avg_logprob": -0.1536488099531694, "compression_ratio": 1.5680473372781065, "no_speech_prob": 7.811456157469365e-07}, {"id": 186, "seek": 124944, "start": 1267.1200000000001, "end": 1278.56, "text": " So here's the first one, the trading set. So for some store, on some date, they had", "tokens": [407, 510, 311, 264, 700, 472, 11, 264, 9529, 992, 13, 407, 337, 512, 3531, 11, 322, 512, 4002, 11, 436, 632], "temperature": 0.0, "avg_logprob": -0.1536488099531694, "compression_ratio": 1.5680473372781065, "no_speech_prob": 7.811456157469365e-07}, {"id": 187, "seek": 127856, "start": 1278.56, "end": 1285.32, "text": " some level of sales to some number of customers. They were either open or closed, they either", "tokens": [512, 1496, 295, 5763, 281, 512, 1230, 295, 4581, 13, 814, 645, 2139, 1269, 420, 5395, 11, 436, 2139], "temperature": 0.0, "avg_logprob": -0.1551849102151805, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.5612707759137265e-06}, {"id": 188, "seek": 127856, "start": 1285.32, "end": 1290.04, "text": " had a promotion on or they didn't, it either was a holiday or it wasn't, first date and", "tokens": [632, 257, 15783, 322, 420, 436, 994, 380, 11, 309, 2139, 390, 257, 9960, 420, 309, 2067, 380, 11, 700, 4002, 293], "temperature": 0.0, "avg_logprob": -0.1551849102151805, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.5612707759137265e-06}, {"id": 189, "seek": 127856, "start": 1290.04, "end": 1296.12, "text": " school and then some additional information about the date. So that's the basic information", "tokens": [1395, 293, 550, 512, 4497, 1589, 466, 264, 4002, 13, 407, 300, 311, 264, 3875, 1589], "temperature": 0.0, "avg_logprob": -0.1551849102151805, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.5612707759137265e-06}, {"id": 190, "seek": 127856, "start": 1296.12, "end": 1304.6799999999998, "text": " we have. And then everything else we join onto that. So for example, for each store,", "tokens": [321, 362, 13, 400, 550, 1203, 1646, 321, 3917, 3911, 300, 13, 407, 337, 1365, 11, 337, 1184, 3531, 11], "temperature": 0.0, "avg_logprob": -0.1551849102151805, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.5612707759137265e-06}, {"id": 191, "seek": 130468, "start": 1304.68, "end": 1308.88, "text": " we can join up some information, some kind of categorical variable about what kind of", "tokens": [321, 393, 3917, 493, 512, 1589, 11, 512, 733, 295, 19250, 804, 7006, 466, 437, 733, 295], "temperature": 0.0, "avg_logprob": -0.14909447564019096, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.422172762337141e-06}, {"id": 192, "seek": 130468, "start": 1308.88, "end": 1314.4, "text": " store it is. I have no idea what this is, it might be a different brand or something.", "tokens": [3531, 309, 307, 13, 286, 362, 572, 1558, 437, 341, 307, 11, 309, 1062, 312, 257, 819, 3360, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.14909447564019096, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.422172762337141e-06}, {"id": 193, "seek": 130468, "start": 1314.4, "end": 1318.52, "text": " What kinds of products do they carry? Again, it's just a letter, I don't know what it means,", "tokens": [708, 3685, 295, 3383, 360, 436, 3985, 30, 3764, 11, 309, 311, 445, 257, 5063, 11, 286, 500, 380, 458, 437, 309, 1355, 11], "temperature": 0.0, "avg_logprob": -0.14909447564019096, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.422172762337141e-06}, {"id": 194, "seek": 130468, "start": 1318.52, "end": 1321.8, "text": " but maybe it's like some are electronics, maybe some are supermarkets, maybe some are", "tokens": [457, 1310, 309, 311, 411, 512, 366, 20611, 11, 1310, 512, 366, 1687, 48850, 11, 1310, 512, 366], "temperature": 0.0, "avg_logprob": -0.14909447564019096, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.422172762337141e-06}, {"id": 195, "seek": 130468, "start": 1321.8, "end": 1322.8, "text": " full-spectrum.", "tokens": [1577, 12, 82, 1043, 6247, 13], "temperature": 0.0, "avg_logprob": -0.14909447564019096, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.422172762337141e-06}, {"id": 196, "seek": 130468, "start": 1322.8, "end": 1332.96, "text": " How far away is the nearest competitor? And what year and month did the competitor open", "tokens": [1012, 1400, 1314, 307, 264, 23831, 27266, 30, 400, 437, 1064, 293, 1618, 630, 264, 27266, 1269], "temperature": 0.0, "avg_logprob": -0.14909447564019096, "compression_ratio": 1.696629213483146, "no_speech_prob": 5.422172762337141e-06}, {"id": 197, "seek": 133296, "start": 1332.96, "end": 1342.52, "text": " for business? Notice that sometimes the competitor opened for business quite late in the game,", "tokens": [337, 1606, 30, 13428, 300, 2171, 264, 27266, 5625, 337, 1606, 1596, 3469, 294, 264, 1216, 11], "temperature": 0.0, "avg_logprob": -0.2089554922921317, "compression_ratio": 1.6126126126126126, "no_speech_prob": 9.368603969051037e-06}, {"id": 198, "seek": 133296, "start": 1342.52, "end": 1348.0, "text": " like later than some of the data we're actually looking at. So that's going to be a little", "tokens": [411, 1780, 813, 512, 295, 264, 1412, 321, 434, 767, 1237, 412, 13, 407, 300, 311, 516, 281, 312, 257, 707], "temperature": 0.0, "avg_logprob": -0.2089554922921317, "compression_ratio": 1.6126126126126126, "no_speech_prob": 9.368603969051037e-06}, {"id": 199, "seek": 133296, "start": 1348.0, "end": 1351.72, "text": " bit confusing. And then this thing called promo2, which as far as I understand it is", "tokens": [857, 13181, 13, 400, 550, 341, 551, 1219, 26750, 17, 11, 597, 382, 1400, 382, 286, 1223, 309, 307], "temperature": 0.0, "avg_logprob": -0.2089554922921317, "compression_ratio": 1.6126126126126126, "no_speech_prob": 9.368603969051037e-06}, {"id": 200, "seek": 133296, "start": 1351.72, "end": 1358.8400000000001, "text": " basically is this a store which has some kind of standard promotion timing going on. So", "tokens": [1936, 307, 341, 257, 3531, 597, 575, 512, 733, 295, 3832, 15783, 10822, 516, 322, 13, 407], "temperature": 0.0, "avg_logprob": -0.2089554922921317, "compression_ratio": 1.6126126126126126, "no_speech_prob": 9.368603969051037e-06}, {"id": 201, "seek": 135884, "start": 1358.84, "end": 1365.76, "text": " you can see here that this store has standard promotions in January, April, July and October.", "tokens": [291, 393, 536, 510, 300, 341, 3531, 575, 3832, 42127, 294, 7061, 11, 6929, 11, 7370, 293, 7617, 13], "temperature": 0.0, "avg_logprob": -0.13587552757673366, "compression_ratio": 1.7037037037037037, "no_speech_prob": 8.664528650115244e-06}, {"id": 202, "seek": 135884, "start": 1365.76, "end": 1374.32, "text": " So that's the stores. We also know for each store what state they're in based on the abbreviation.", "tokens": [407, 300, 311, 264, 9512, 13, 492, 611, 458, 337, 1184, 3531, 437, 1785, 436, 434, 294, 2361, 322, 264, 35839, 399, 13], "temperature": 0.0, "avg_logprob": -0.13587552757673366, "compression_ratio": 1.7037037037037037, "no_speech_prob": 8.664528650115244e-06}, {"id": 203, "seek": 135884, "start": 1374.32, "end": 1380.36, "text": " And then we can find out for each state what is the name of that state. And then for each", "tokens": [400, 550, 321, 393, 915, 484, 337, 1184, 1785, 437, 307, 264, 1315, 295, 300, 1785, 13, 400, 550, 337, 1184], "temperature": 0.0, "avg_logprob": -0.13587552757673366, "compression_ratio": 1.7037037037037037, "no_speech_prob": 8.664528650115244e-06}, {"id": 204, "seek": 135884, "start": 1380.36, "end": 1384.24, "text": " state, this is slightly weird, this is the state abbreviation, the last 2 letters. In", "tokens": [1785, 11, 341, 307, 4748, 3657, 11, 341, 307, 264, 1785, 35839, 399, 11, 264, 1036, 568, 7825, 13, 682], "temperature": 0.0, "avg_logprob": -0.13587552757673366, "compression_ratio": 1.7037037037037037, "no_speech_prob": 8.664528650115244e-06}, {"id": 205, "seek": 138424, "start": 1384.24, "end": 1390.68, "text": " this state during this week, this was the Google Trend data for some keyword, I'm not", "tokens": [341, 1785, 1830, 341, 1243, 11, 341, 390, 264, 3329, 37417, 1412, 337, 512, 20428, 11, 286, 478, 406], "temperature": 0.0, "avg_logprob": -0.20988300323486328, "compression_ratio": 1.553072625698324, "no_speech_prob": 1.5294028798962245e-06}, {"id": 206, "seek": 138424, "start": 1390.68, "end": 1400.24, "text": " sure what keyword it was. For this state name on this date, here's the temperature, view", "tokens": [988, 437, 20428, 309, 390, 13, 1171, 341, 1785, 1315, 322, 341, 4002, 11, 510, 311, 264, 4292, 11, 1910], "temperature": 0.0, "avg_logprob": -0.20988300323486328, "compression_ratio": 1.553072625698324, "no_speech_prob": 1.5294028798962245e-06}, {"id": 207, "seek": 138424, "start": 1400.24, "end": 1404.56, "text": " point, so forth.", "tokens": [935, 11, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.20988300323486328, "compression_ratio": 1.553072625698324, "no_speech_prob": 1.5294028798962245e-06}, {"id": 208, "seek": 138424, "start": 1404.56, "end": 1408.76, "text": " And then finally here's the test set. It's identical to the training set, but we don't", "tokens": [400, 550, 2721, 510, 311, 264, 1500, 992, 13, 467, 311, 14800, 281, 264, 3097, 992, 11, 457, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.20988300323486328, "compression_ratio": 1.553072625698324, "no_speech_prob": 1.5294028798962245e-06}, {"id": 209, "seek": 140876, "start": 1408.76, "end": 1415.76, "text": " have the number of customers and we don't have the number of sales. So this is a pretty", "tokens": [362, 264, 1230, 295, 4581, 293, 321, 500, 380, 362, 264, 1230, 295, 5763, 13, 407, 341, 307, 257, 1238], "temperature": 0.0, "avg_logprob": -0.12428417912235966, "compression_ratio": 1.6336633663366336, "no_speech_prob": 1.1544557310116943e-06}, {"id": 210, "seek": 140876, "start": 1415.76, "end": 1423.12, "text": " standard kind of industry data set. We've got a central table, various tables related", "tokens": [3832, 733, 295, 3518, 1412, 992, 13, 492, 600, 658, 257, 5777, 3199, 11, 3683, 8020, 4077], "temperature": 0.0, "avg_logprob": -0.12428417912235966, "compression_ratio": 1.6336633663366336, "no_speech_prob": 1.1544557310116943e-06}, {"id": 211, "seek": 140876, "start": 1423.12, "end": 1430.4, "text": " to that, and some things representing time periods or time points.", "tokens": [281, 300, 11, 293, 512, 721, 13460, 565, 13804, 420, 565, 2793, 13], "temperature": 0.0, "avg_logprob": -0.12428417912235966, "compression_ratio": 1.6336633663366336, "no_speech_prob": 1.1544557310116943e-06}, {"id": 212, "seek": 140876, "start": 1430.4, "end": 1437.48, "text": " One of the nice things you can do in Pandas is to use this Pandas summary module and call", "tokens": [1485, 295, 264, 1481, 721, 291, 393, 360, 294, 16995, 296, 307, 281, 764, 341, 16995, 296, 12691, 10088, 293, 818], "temperature": 0.0, "avg_logprob": -0.12428417912235966, "compression_ratio": 1.6336633663366336, "no_speech_prob": 1.1544557310116943e-06}, {"id": 213, "seek": 143748, "start": 1437.48, "end": 1443.32, "text": " data frame summary for table.summary and that will return a whole bunch of information about", "tokens": [1412, 3920, 12691, 337, 3199, 13, 82, 40879, 822, 293, 300, 486, 2736, 257, 1379, 3840, 295, 1589, 466], "temperature": 0.0, "avg_logprob": -0.1872671227706106, "compression_ratio": 1.6205357142857142, "no_speech_prob": 4.785053079103818e-06}, {"id": 214, "seek": 143748, "start": 1443.32, "end": 1448.04, "text": " every field. So I'm not going to go through all of it in detail, but you can see for example", "tokens": [633, 2519, 13, 407, 286, 478, 406, 516, 281, 352, 807, 439, 295, 309, 294, 2607, 11, 457, 291, 393, 536, 337, 1365], "temperature": 0.0, "avg_logprob": -0.1872671227706106, "compression_ratio": 1.6205357142857142, "no_speech_prob": 4.785053079103818e-06}, {"id": 215, "seek": 143748, "start": 1448.04, "end": 1456.08, "text": " for the sales, on average 5800 sales, standard deviation of 3800, sometimes the sales goes", "tokens": [337, 264, 5763, 11, 322, 4274, 1025, 14423, 5763, 11, 3832, 25163, 295, 12843, 628, 11, 2171, 264, 5763, 1709], "temperature": 0.0, "avg_logprob": -0.1872671227706106, "compression_ratio": 1.6205357142857142, "no_speech_prob": 4.785053079103818e-06}, {"id": 216, "seek": 143748, "start": 1456.08, "end": 1462.56, "text": " all the way down to 0, sometimes all the way up to 41000. There's no missing to sales,", "tokens": [439, 264, 636, 760, 281, 1958, 11, 2171, 439, 264, 636, 493, 281, 18173, 1360, 13, 821, 311, 572, 5361, 281, 5763, 11], "temperature": 0.0, "avg_logprob": -0.1872671227706106, "compression_ratio": 1.6205357142857142, "no_speech_prob": 4.785053079103818e-06}, {"id": 217, "seek": 146256, "start": 1462.56, "end": 1470.24, "text": " that's good to know. So this is the kind of thing that's good to scroll through and identify.", "tokens": [300, 311, 665, 281, 458, 13, 407, 341, 307, 264, 733, 295, 551, 300, 311, 665, 281, 11369, 807, 293, 5876, 13], "temperature": 0.0, "avg_logprob": -0.1703103238886053, "compression_ratio": 1.5310734463276836, "no_speech_prob": 3.2887026009120746e-06}, {"id": 218, "seek": 146256, "start": 1470.24, "end": 1477.56, "text": " Competition open since month is missing about a third of the time, that's good to know.", "tokens": [43634, 1269, 1670, 1618, 307, 5361, 466, 257, 2636, 295, 264, 565, 11, 300, 311, 665, 281, 458, 13], "temperature": 0.0, "avg_logprob": -0.1703103238886053, "compression_ratio": 1.5310734463276836, "no_speech_prob": 3.2887026009120746e-06}, {"id": 219, "seek": 146256, "start": 1477.56, "end": 1485.3999999999999, "text": " There's 12 unique states, that might be worth checking because there's actually 16 things", "tokens": [821, 311, 2272, 3845, 4368, 11, 300, 1062, 312, 3163, 8568, 570, 456, 311, 767, 3165, 721], "temperature": 0.0, "avg_logprob": -0.1703103238886053, "compression_ratio": 1.5310734463276836, "no_speech_prob": 3.2887026009120746e-06}, {"id": 220, "seek": 148540, "start": 1485.4, "end": 1494.3200000000002, "text": " in our state table for some reason. Google Trend data is never missing, that's good.", "tokens": [294, 527, 1785, 3199, 337, 512, 1778, 13, 3329, 37417, 1412, 307, 1128, 5361, 11, 300, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.17728083880979623, "compression_ratio": 1.4914285714285713, "no_speech_prob": 5.4221654863795266e-06}, {"id": 221, "seek": 148540, "start": 1494.3200000000002, "end": 1507.0800000000002, "text": " The year goes from 2012 through 2015. The weather data is never missing. And then here's", "tokens": [440, 1064, 1709, 490, 9125, 807, 7546, 13, 440, 5503, 1412, 307, 1128, 5361, 13, 400, 550, 510, 311], "temperature": 0.0, "avg_logprob": -0.17728083880979623, "compression_ratio": 1.4914285714285713, "no_speech_prob": 5.4221654863795266e-06}, {"id": 222, "seek": 148540, "start": 1507.0800000000002, "end": 1512.0800000000002, "text": " our test set. This is the kind of thing that might screw up a model, it's like actually", "tokens": [527, 1500, 992, 13, 639, 307, 264, 733, 295, 551, 300, 1062, 5630, 493, 257, 2316, 11, 309, 311, 411, 767], "temperature": 0.0, "avg_logprob": -0.17728083880979623, "compression_ratio": 1.4914285714285713, "no_speech_prob": 5.4221654863795266e-06}, {"id": 223, "seek": 151208, "start": 1512.08, "end": 1519.84, "text": " sometimes the test set is missing the information about whether that store was open or not.", "tokens": [2171, 264, 1500, 992, 307, 5361, 264, 1589, 466, 1968, 300, 3531, 390, 1269, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.1351746212352406, "compression_ratio": 1.751219512195122, "no_speech_prob": 5.422182312031509e-06}, {"id": 224, "seek": 151208, "start": 1519.84, "end": 1524.0, "text": " So we can take that list of tables and just destructure it out into a whole bunch of different", "tokens": [407, 321, 393, 747, 300, 1329, 295, 8020, 293, 445, 2677, 2885, 309, 484, 666, 257, 1379, 3840, 295, 819], "temperature": 0.0, "avg_logprob": -0.1351746212352406, "compression_ratio": 1.751219512195122, "no_speech_prob": 5.422182312031509e-06}, {"id": 225, "seek": 151208, "start": 1524.0, "end": 1530.8799999999999, "text": " table names, find out how big the training set is, find out how big the test set is.", "tokens": [3199, 5288, 11, 915, 484, 577, 955, 264, 3097, 992, 307, 11, 915, 484, 577, 955, 264, 1500, 992, 307, 13], "temperature": 0.0, "avg_logprob": -0.1351746212352406, "compression_ratio": 1.751219512195122, "no_speech_prob": 5.422182312031509e-06}, {"id": 226, "seek": 151208, "start": 1530.8799999999999, "end": 1539.4399999999998, "text": " And then with this kind of problem, there's going to be a whole bunch of data cleaning,", "tokens": [400, 550, 365, 341, 733, 295, 1154, 11, 456, 311, 516, 281, 312, 257, 1379, 3840, 295, 1412, 8924, 11], "temperature": 0.0, "avg_logprob": -0.1351746212352406, "compression_ratio": 1.751219512195122, "no_speech_prob": 5.422182312031509e-06}, {"id": 227, "seek": 153944, "start": 1539.44, "end": 1546.72, "text": " and a whole bunch of feature engineering. And so neural nets don't make any of that", "tokens": [293, 257, 1379, 3840, 295, 4111, 7043, 13, 400, 370, 18161, 36170, 500, 380, 652, 604, 295, 300], "temperature": 0.0, "avg_logprob": -0.12506532669067383, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.7502621631138027e-05}, {"id": 228, "seek": 153944, "start": 1546.72, "end": 1552.68, "text": " go away, particularly because we're using this style of neural net where we're basically", "tokens": [352, 1314, 11, 4098, 570, 321, 434, 1228, 341, 3758, 295, 18161, 2533, 689, 321, 434, 1936], "temperature": 0.0, "avg_logprob": -0.12506532669067383, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.7502621631138027e-05}, {"id": 229, "seek": 153944, "start": 1552.68, "end": 1556.56, "text": " feeding in a whole bunch of separate continuous and categorical variables.", "tokens": [12919, 294, 257, 1379, 3840, 295, 4994, 10957, 293, 19250, 804, 9102, 13], "temperature": 0.0, "avg_logprob": -0.12506532669067383, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.7502621631138027e-05}, {"id": 230, "seek": 153944, "start": 1556.56, "end": 1564.44, "text": " So simplify things a bit, turn state holidays into Booleans. And then I'm going to join", "tokens": [407, 20460, 721, 257, 857, 11, 1261, 1785, 15734, 666, 23351, 24008, 13, 400, 550, 286, 478, 516, 281, 3917], "temperature": 0.0, "avg_logprob": -0.12506532669067383, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.7502621631138027e-05}, {"id": 231, "seek": 156444, "start": 1564.44, "end": 1574.44, "text": " all of these tables together. I always use a default join type of an outer join. So you", "tokens": [439, 295, 613, 8020, 1214, 13, 286, 1009, 764, 257, 7576, 3917, 2010, 295, 364, 10847, 3917, 13, 407, 291], "temperature": 0.0, "avg_logprob": -0.1712156867980957, "compression_ratio": 1.7303921568627452, "no_speech_prob": 1.5206696843961254e-05}, {"id": 232, "seek": 156444, "start": 1574.44, "end": 1582.56, "text": " can see here, this is how we join in pandas. We say table.merge table2, and then to make", "tokens": [393, 536, 510, 11, 341, 307, 577, 321, 3917, 294, 4565, 296, 13, 492, 584, 3199, 13, 936, 432, 3199, 17, 11, 293, 550, 281, 652], "temperature": 0.0, "avg_logprob": -0.1712156867980957, "compression_ratio": 1.7303921568627452, "no_speech_prob": 1.5206696843961254e-05}, {"id": 233, "seek": 156444, "start": 1582.56, "end": 1588.4, "text": " a left outer join, how equals left. And then you say, what's the name of the fields that", "tokens": [257, 1411, 10847, 3917, 11, 577, 6915, 1411, 13, 400, 550, 291, 584, 11, 437, 311, 264, 1315, 295, 264, 7909, 300], "temperature": 0.0, "avg_logprob": -0.1712156867980957, "compression_ratio": 1.7303921568627452, "no_speech_prob": 1.5206696843961254e-05}, {"id": 234, "seek": 156444, "start": 1588.4, "end": 1591.52, "text": " you're going to join on the left-hand side, what are the fields you're going to join on", "tokens": [291, 434, 516, 281, 3917, 322, 264, 1411, 12, 5543, 1252, 11, 437, 366, 264, 7909, 291, 434, 516, 281, 3917, 322], "temperature": 0.0, "avg_logprob": -0.1712156867980957, "compression_ratio": 1.7303921568627452, "no_speech_prob": 1.5206696843961254e-05}, {"id": 235, "seek": 159152, "start": 1591.52, "end": 1598.28, "text": " the right-hand side. And then if both tables have some fields with the same name, what", "tokens": [264, 558, 12, 5543, 1252, 13, 400, 550, 498, 1293, 8020, 362, 512, 7909, 365, 264, 912, 1315, 11, 437], "temperature": 0.0, "avg_logprob": -0.15858465993506277, "compression_ratio": 1.7584745762711864, "no_speech_prob": 5.682406481355429e-06}, {"id": 236, "seek": 159152, "start": 1598.28, "end": 1603.16, "text": " are you going to suffix those fields with. So on the left-hand side we're not going to", "tokens": [366, 291, 516, 281, 3889, 970, 729, 7909, 365, 13, 407, 322, 264, 1411, 12, 5543, 1252, 321, 434, 406, 516, 281], "temperature": 0.0, "avg_logprob": -0.15858465993506277, "compression_ratio": 1.7584745762711864, "no_speech_prob": 5.682406481355429e-06}, {"id": 237, "seek": 159152, "start": 1603.16, "end": 1607.4, "text": " add any suffix, on the right-hand side we'll put in underscore y.", "tokens": [909, 604, 3889, 970, 11, 322, 264, 558, 12, 5543, 1252, 321, 603, 829, 294, 37556, 288, 13], "temperature": 0.0, "avg_logprob": -0.15858465993506277, "compression_ratio": 1.7584745762711864, "no_speech_prob": 5.682406481355429e-06}, {"id": 238, "seek": 159152, "start": 1607.4, "end": 1612.6, "text": " So again, I try to refactor things as much as I can, so we're going to join lots of things.", "tokens": [407, 797, 11, 286, 853, 281, 1895, 15104, 721, 382, 709, 382, 286, 393, 11, 370, 321, 434, 516, 281, 3917, 3195, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.15858465993506277, "compression_ratio": 1.7584745762711864, "no_speech_prob": 5.682406481355429e-06}, {"id": 239, "seek": 159152, "start": 1612.6, "end": 1620.0, "text": " Let's create one function to do the joining, and then we can call it lots of times.", "tokens": [961, 311, 1884, 472, 2445, 281, 360, 264, 5549, 11, 293, 550, 321, 393, 818, 309, 3195, 295, 1413, 13], "temperature": 0.0, "avg_logprob": -0.15858465993506277, "compression_ratio": 1.7584745762711864, "no_speech_prob": 5.682406481355429e-06}, {"id": 240, "seek": 162000, "start": 1620.0, "end": 1626.68, "text": " Was there any fields referring to the same file but named differently?", "tokens": [3027, 456, 604, 7909, 13761, 281, 264, 912, 3991, 457, 4926, 7614, 30], "temperature": 0.0, "avg_logprob": -0.27351898305556355, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.2218895790283568e-05}, {"id": 241, "seek": 162000, "start": 1626.68, "end": 1635.4, "text": " Not that I saw, no. And it wouldn't matter too much if there were because when we run", "tokens": [1726, 300, 286, 1866, 11, 572, 13, 400, 309, 2759, 380, 1871, 886, 709, 498, 456, 645, 570, 562, 321, 1190], "temperature": 0.0, "avg_logprob": -0.27351898305556355, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.2218895790283568e-05}, {"id": 242, "seek": 162000, "start": 1635.4, "end": 1637.28, "text": " the model, no problem.", "tokens": [264, 2316, 11, 572, 1154, 13], "temperature": 0.0, "avg_logprob": -0.27351898305556355, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.2218895790283568e-05}, {"id": 243, "seek": 162000, "start": 1637.28, "end": 1644.84, "text": " Question from the forum. Would you liken the use of embeddings from a neural network to", "tokens": [14464, 490, 264, 17542, 13, 6068, 291, 36946, 264, 764, 295, 12240, 29432, 490, 257, 18161, 3209, 281], "temperature": 0.0, "avg_logprob": -0.27351898305556355, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.2218895790283568e-05}, {"id": 244, "seek": 164484, "start": 1644.84, "end": 1650.76, "text": " extraction of implicit features, or can we think of it more like what a PCA would do,", "tokens": [30197, 295, 26947, 4122, 11, 420, 393, 321, 519, 295, 309, 544, 411, 437, 257, 6465, 32, 576, 360, 11], "temperature": 0.0, "avg_logprob": -0.14040238698323568, "compression_ratio": 1.4923857868020305, "no_speech_prob": 4.35686615674058e-06}, {"id": 245, "seek": 164484, "start": 1650.76, "end": 1653.1999999999998, "text": " like dimensionality reduction?", "tokens": [411, 10139, 1860, 11004, 30], "temperature": 0.0, "avg_logprob": -0.14040238698323568, "compression_ratio": 1.4923857868020305, "no_speech_prob": 4.35686615674058e-06}, {"id": 246, "seek": 164484, "start": 1653.1999999999998, "end": 1662.76, "text": " Let's talk about it more when we get there. Basically, when you deal with categorical", "tokens": [961, 311, 751, 466, 309, 544, 562, 321, 483, 456, 13, 8537, 11, 562, 291, 2028, 365, 19250, 804], "temperature": 0.0, "avg_logprob": -0.14040238698323568, "compression_ratio": 1.4923857868020305, "no_speech_prob": 4.35686615674058e-06}, {"id": 247, "seek": 164484, "start": 1662.76, "end": 1671.8799999999999, "text": " variables in any kind of model, you have to decide what to do with them. One of my favorite", "tokens": [9102, 294, 604, 733, 295, 2316, 11, 291, 362, 281, 4536, 437, 281, 360, 365, 552, 13, 1485, 295, 452, 2954], "temperature": 0.0, "avg_logprob": -0.14040238698323568, "compression_ratio": 1.4923857868020305, "no_speech_prob": 4.35686615674058e-06}, {"id": 248, "seek": 167188, "start": 1671.88, "end": 1677.0400000000002, "text": " data scientists, or a pair of them actually, who are very nearly neighbors of Rachel and", "tokens": [1412, 7708, 11, 420, 257, 6119, 295, 552, 767, 11, 567, 366, 588, 6217, 12512, 295, 14246, 293], "temperature": 0.0, "avg_logprob": -0.21521245018910554, "compression_ratio": 1.4437869822485208, "no_speech_prob": 1.012989832815947e-05}, {"id": 249, "seek": 167188, "start": 1677.0400000000002, "end": 1689.2, "text": " mine have this fantastic R package called Vtreat which has a bunch of state-of-the-art", "tokens": [3892, 362, 341, 5456, 497, 7372, 1219, 691, 83, 620, 597, 575, 257, 3840, 295, 1785, 12, 2670, 12, 3322, 12, 446], "temperature": 0.0, "avg_logprob": -0.21521245018910554, "compression_ratio": 1.4437869822485208, "no_speech_prob": 1.012989832815947e-05}, {"id": 250, "seek": 167188, "start": 1689.2, "end": 1695.68, "text": " approaches to dealing with stuff like categorical variable encoding.", "tokens": [11587, 281, 6260, 365, 1507, 411, 19250, 804, 7006, 43430, 13], "temperature": 0.0, "avg_logprob": -0.21521245018910554, "compression_ratio": 1.4437869822485208, "no_speech_prob": 1.012989832815947e-05}, {"id": 251, "seek": 169568, "start": 1695.68, "end": 1706.68, "text": " And so the obvious way to do categorical variable encoding is to just do a one-hot encoding,", "tokens": [400, 370, 264, 6322, 636, 281, 360, 19250, 804, 7006, 43430, 307, 281, 445, 360, 257, 472, 12, 12194, 43430, 11], "temperature": 0.0, "avg_logprob": -0.13720998030442458, "compression_ratio": 1.5112359550561798, "no_speech_prob": 3.966955318901455e-06}, {"id": 252, "seek": 169568, "start": 1706.68, "end": 1711.28, "text": " and that's the way nearly everybody puts it into their gradient boosting machines or random", "tokens": [293, 300, 311, 264, 636, 6217, 2201, 8137, 309, 666, 641, 16235, 43117, 8379, 420, 4974], "temperature": 0.0, "avg_logprob": -0.13720998030442458, "compression_ratio": 1.5112359550561798, "no_speech_prob": 3.966955318901455e-06}, {"id": 253, "seek": 169568, "start": 1711.28, "end": 1719.16, "text": " forests or whatever. But one of the things that Vtreat does is it has some much more", "tokens": [21700, 420, 2035, 13, 583, 472, 295, 264, 721, 300, 691, 83, 620, 775, 307, 309, 575, 512, 709, 544], "temperature": 0.0, "avg_logprob": -0.13720998030442458, "compression_ratio": 1.5112359550561798, "no_speech_prob": 3.966955318901455e-06}, {"id": 254, "seek": 171916, "start": 1719.16, "end": 1733.68, "text": " interesting techniques. So for example, you could look at the univariate mean of sales", "tokens": [1880, 7512, 13, 407, 337, 1365, 11, 291, 727, 574, 412, 264, 517, 592, 3504, 473, 914, 295, 5763], "temperature": 0.0, "avg_logprob": -0.11640254534207857, "compression_ratio": 1.587878787878788, "no_speech_prob": 3.7266149774950463e-06}, {"id": 255, "seek": 171916, "start": 1733.68, "end": 1741.42, "text": " for each day of week, and you could encode day of week using a continuous variable which", "tokens": [337, 1184, 786, 295, 1243, 11, 293, 291, 727, 2058, 1429, 786, 295, 1243, 1228, 257, 10957, 7006, 597], "temperature": 0.0, "avg_logprob": -0.11640254534207857, "compression_ratio": 1.587878787878788, "no_speech_prob": 3.7266149774950463e-06}, {"id": 256, "seek": 171916, "start": 1741.42, "end": 1747.4, "text": " represents the mean of sales. But then you have to think about, would I take that mean", "tokens": [8855, 264, 914, 295, 5763, 13, 583, 550, 291, 362, 281, 519, 466, 11, 576, 286, 747, 300, 914], "temperature": 0.0, "avg_logprob": -0.11640254534207857, "compression_ratio": 1.587878787878788, "no_speech_prob": 3.7266149774950463e-06}, {"id": 257, "seek": 174740, "start": 1747.4, "end": 1752.52, "text": " from the training set or the test set or the validation set? How do I avoid overfitting?", "tokens": [490, 264, 3097, 992, 420, 264, 1500, 992, 420, 264, 24071, 992, 30, 1012, 360, 286, 5042, 670, 69, 2414, 30], "temperature": 0.0, "avg_logprob": -0.1627927819887797, "compression_ratio": 1.6317991631799162, "no_speech_prob": 7.411224487441359e-06}, {"id": 258, "seek": 174740, "start": 1752.52, "end": 1758.5600000000002, "text": " There's all kinds of complex statistical subtleties to think about that Vtreat handles all this", "tokens": [821, 311, 439, 3685, 295, 3997, 22820, 7257, 2631, 530, 281, 519, 466, 300, 691, 83, 620, 18722, 439, 341], "temperature": 0.0, "avg_logprob": -0.1627927819887797, "compression_ratio": 1.6317991631799162, "no_speech_prob": 7.411224487441359e-06}, {"id": 259, "seek": 174740, "start": 1758.5600000000002, "end": 1765.0400000000002, "text": " stuff automatically.", "tokens": [1507, 6772, 13], "temperature": 0.0, "avg_logprob": -0.1627927819887797, "compression_ratio": 1.6317991631799162, "no_speech_prob": 7.411224487441359e-06}, {"id": 260, "seek": 174740, "start": 1765.0400000000002, "end": 1769.2800000000002, "text": " There's a lot of great techniques, but they're kind of complicated and in the end they tend", "tokens": [821, 311, 257, 688, 295, 869, 7512, 11, 457, 436, 434, 733, 295, 6179, 293, 294, 264, 917, 436, 3928], "temperature": 0.0, "avg_logprob": -0.1627927819887797, "compression_ratio": 1.6317991631799162, "no_speech_prob": 7.411224487441359e-06}, {"id": 261, "seek": 174740, "start": 1769.2800000000002, "end": 1776.76, "text": " to make a whole bunch of assumptions about linearity or univariate correlations or whatever.", "tokens": [281, 652, 257, 1379, 3840, 295, 17695, 466, 8213, 507, 420, 517, 592, 3504, 473, 13983, 763, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.1627927819887797, "compression_ratio": 1.6317991631799162, "no_speech_prob": 7.411224487441359e-06}, {"id": 262, "seek": 177676, "start": 1776.76, "end": 1785.44, "text": " Whereas with embeddings, we're using SGD to learn how to deal with it. Just like we do", "tokens": [13813, 365, 12240, 29432, 11, 321, 434, 1228, 34520, 35, 281, 1466, 577, 281, 2028, 365, 309, 13, 1449, 411, 321, 360], "temperature": 0.0, "avg_logprob": -0.1640271246433258, "compression_ratio": 1.4555555555555555, "no_speech_prob": 2.078478064504452e-05}, {"id": 263, "seek": 177676, "start": 1785.44, "end": 1793.82, "text": " when we build an NLP model or a collaborative filtering model, we provide some initially", "tokens": [562, 321, 1322, 364, 426, 45196, 2316, 420, 257, 16555, 30822, 2316, 11, 321, 2893, 512, 9105], "temperature": 0.0, "avg_logprob": -0.1640271246433258, "compression_ratio": 1.4555555555555555, "no_speech_prob": 2.078478064504452e-05}, {"id": 264, "seek": 177676, "start": 1793.82, "end": 1801.32, "text": " random embeddings and the system learns how the movies vary and compare to each other,", "tokens": [4974, 12240, 29432, 293, 264, 1185, 27152, 577, 264, 6233, 10559, 293, 6794, 281, 1184, 661, 11], "temperature": 0.0, "avg_logprob": -0.1640271246433258, "compression_ratio": 1.4555555555555555, "no_speech_prob": 2.078478064504452e-05}, {"id": 265, "seek": 180132, "start": 1801.32, "end": 1809.96, "text": " or users vary or words vary or whatever. So this is to me the ultimate pure technique.", "tokens": [420, 5022, 10559, 420, 2283, 10559, 420, 2035, 13, 407, 341, 307, 281, 385, 264, 9705, 6075, 6532, 13], "temperature": 0.0, "avg_logprob": -0.1442028755365416, "compression_ratio": 1.6384976525821595, "no_speech_prob": 2.3320540094573516e-06}, {"id": 266, "seek": 180132, "start": 1809.96, "end": 1813.28, "text": " And of course the other nice thing about embeddings is we get to pick the dimensionality of the", "tokens": [400, 295, 1164, 264, 661, 1481, 551, 466, 12240, 29432, 307, 321, 483, 281, 1888, 264, 10139, 1860, 295, 264], "temperature": 0.0, "avg_logprob": -0.1442028755365416, "compression_ratio": 1.6384976525821595, "no_speech_prob": 2.3320540094573516e-06}, {"id": 267, "seek": 180132, "start": 1813.28, "end": 1819.08, "text": " embedding so we can decide how much complexity and how much learning are we going to put", "tokens": [12240, 3584, 370, 321, 393, 4536, 577, 709, 14024, 293, 577, 709, 2539, 366, 321, 516, 281, 829], "temperature": 0.0, "avg_logprob": -0.1442028755365416, "compression_ratio": 1.6384976525821595, "no_speech_prob": 2.3320540094573516e-06}, {"id": 268, "seek": 180132, "start": 1819.08, "end": 1829.02, "text": " into each of the categorical variables. We'll see how to do that in a moment.", "tokens": [666, 1184, 295, 264, 19250, 804, 9102, 13, 492, 603, 536, 577, 281, 360, 300, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.1442028755365416, "compression_ratio": 1.6384976525821595, "no_speech_prob": 2.3320540094573516e-06}, {"id": 269, "seek": 182902, "start": 1829.02, "end": 1842.8, "text": " So one complexity was that the weather uses the name of the state rather than the abbreviation", "tokens": [407, 472, 14024, 390, 300, 264, 5503, 4960, 264, 1315, 295, 264, 1785, 2831, 813, 264, 35839, 399], "temperature": 0.0, "avg_logprob": -0.11513491736518013, "compression_ratio": 1.5726495726495726, "no_speech_prob": 1.3630936336994637e-05}, {"id": 270, "seek": 182902, "start": 1842.8, "end": 1851.12, "text": " of the state, so we can just go ahead and join weather to states to get the abbreviation.", "tokens": [295, 264, 1785, 11, 370, 321, 393, 445, 352, 2286, 293, 3917, 5503, 281, 4368, 281, 483, 264, 35839, 399, 13], "temperature": 0.0, "avg_logprob": -0.11513491736518013, "compression_ratio": 1.5726495726495726, "no_speech_prob": 1.3630936336994637e-05}, {"id": 271, "seek": 185112, "start": 1851.12, "end": 1859.3999999999999, "text": " The Google Trend information about the week from A to B, we can split that apart. You", "tokens": [440, 3329, 37417, 1589, 466, 264, 1243, 490, 316, 281, 363, 11, 321, 393, 7472, 300, 4936, 13, 509], "temperature": 0.0, "avg_logprob": -0.16713694334030152, "compression_ratio": 1.664835164835165, "no_speech_prob": 5.682384198735235e-06}, {"id": 272, "seek": 185112, "start": 1859.3999999999999, "end": 1866.32, "text": " can see here one of the things that happens in the Google Trend data is that one of the", "tokens": [393, 536, 510, 472, 295, 264, 721, 300, 2314, 294, 264, 3329, 37417, 1412, 307, 300, 472, 295, 264], "temperature": 0.0, "avg_logprob": -0.16713694334030152, "compression_ratio": 1.664835164835165, "no_speech_prob": 5.682384198735235e-06}, {"id": 273, "seek": 185112, "start": 1866.32, "end": 1872.9199999999998, "text": " states is called ni, whereas the rest of the data is called hb, ni. So this is a good opportunity", "tokens": [4368, 307, 1219, 3867, 11, 9735, 264, 1472, 295, 264, 1412, 307, 1219, 276, 65, 11, 3867, 13, 407, 341, 307, 257, 665, 2650], "temperature": 0.0, "avg_logprob": -0.16713694334030152, "compression_ratio": 1.664835164835165, "no_speech_prob": 5.682384198735235e-06}, {"id": 274, "seek": 185112, "start": 1872.9199999999998, "end": 1875.2399999999998, "text": " to learn about pandas indexing.", "tokens": [281, 1466, 466, 4565, 296, 8186, 278, 13], "temperature": 0.0, "avg_logprob": -0.16713694334030152, "compression_ratio": 1.664835164835165, "no_speech_prob": 5.682384198735235e-06}, {"id": 275, "seek": 187524, "start": 1875.24, "end": 1884.6, "text": " So pandas indexing, most of the time you want to use this.ix method. And the.ix method", "tokens": [407, 4565, 296, 8186, 278, 11, 881, 295, 264, 565, 291, 528, 281, 764, 341, 2411, 970, 3170, 13, 400, 264, 2411, 970, 3170], "temperature": 0.0, "avg_logprob": -0.11918153467866563, "compression_ratio": 1.7073170731707317, "no_speech_prob": 2.6015914045274258e-06}, {"id": 276, "seek": 187524, "start": 1884.6, "end": 1890.2, "text": " is your general indexing method. It's going to take two things, a list of rows to select", "tokens": [307, 428, 2674, 8186, 278, 3170, 13, 467, 311, 516, 281, 747, 732, 721, 11, 257, 1329, 295, 13241, 281, 3048], "temperature": 0.0, "avg_logprob": -0.11918153467866563, "compression_ratio": 1.7073170731707317, "no_speech_prob": 2.6015914045274258e-06}, {"id": 277, "seek": 187524, "start": 1890.2, "end": 1894.96, "text": " and a list of columns to select. And you can use it in pretty standard intuitive ways.", "tokens": [293, 257, 1329, 295, 13766, 281, 3048, 13, 400, 291, 393, 764, 309, 294, 1238, 3832, 21769, 2098, 13], "temperature": 0.0, "avg_logprob": -0.11918153467866563, "compression_ratio": 1.7073170731707317, "no_speech_prob": 2.6015914045274258e-06}, {"id": 278, "seek": 187524, "start": 1894.96, "end": 1901.16, "text": " This is a lot like NumPy. This here is going to return a list of Booleans, which things", "tokens": [639, 307, 257, 688, 411, 22592, 47, 88, 13, 639, 510, 307, 516, 281, 2736, 257, 1329, 295, 23351, 24008, 11, 597, 721], "temperature": 0.0, "avg_logprob": -0.11918153467866563, "compression_ratio": 1.7073170731707317, "no_speech_prob": 2.6015914045274258e-06}, {"id": 279, "seek": 190116, "start": 1901.16, "end": 1907.0, "text": " are in this state. And if you pass the list of Booleans to the pandas row selector, it", "tokens": [366, 294, 341, 1785, 13, 400, 498, 291, 1320, 264, 1329, 295, 23351, 24008, 281, 264, 4565, 296, 5386, 23264, 1672, 11, 309], "temperature": 0.0, "avg_logprob": -0.14534237187936766, "compression_ratio": 1.7617021276595746, "no_speech_prob": 1.3211849363869987e-05}, {"id": 280, "seek": 190116, "start": 1907.0, "end": 1912.16, "text": " will just return the rows where that Boolean is true. So therefore this is just going to", "tokens": [486, 445, 2736, 264, 13241, 689, 300, 23351, 28499, 307, 2074, 13, 407, 4412, 341, 307, 445, 516, 281], "temperature": 0.0, "avg_logprob": -0.14534237187936766, "compression_ratio": 1.7617021276595746, "no_speech_prob": 1.3211849363869987e-05}, {"id": 281, "seek": 190116, "start": 1912.16, "end": 1917.16, "text": " return the rows from Google Trend where Google Trend.state is ni.", "tokens": [2736, 264, 13241, 490, 3329, 37417, 689, 3329, 37417, 13, 15406, 307, 3867, 13], "temperature": 0.0, "avg_logprob": -0.14534237187936766, "compression_ratio": 1.7617021276595746, "no_speech_prob": 1.3211849363869987e-05}, {"id": 282, "seek": 190116, "start": 1917.16, "end": 1921.76, "text": " And then the second thing we pass in is a list of columns. In this case we just got", "tokens": [400, 550, 264, 1150, 551, 321, 1320, 294, 307, 257, 1329, 295, 13766, 13, 682, 341, 1389, 321, 445, 658], "temperature": 0.0, "avg_logprob": -0.14534237187936766, "compression_ratio": 1.7617021276595746, "no_speech_prob": 1.3211849363869987e-05}, {"id": 283, "seek": 190116, "start": 1921.76, "end": 1927.0, "text": " one column. And one very important thing to remember, again just like NumPy, you can put", "tokens": [472, 7738, 13, 400, 472, 588, 1021, 551, 281, 1604, 11, 797, 445, 411, 22592, 47, 88, 11, 291, 393, 829], "temperature": 0.0, "avg_logprob": -0.14534237187936766, "compression_ratio": 1.7617021276595746, "no_speech_prob": 1.3211849363869987e-05}, {"id": 284, "seek": 192700, "start": 1927.0, "end": 1931.24, "text": " this kind of thing on the left-hand side of an equal sign. In computer science we call", "tokens": [341, 733, 295, 551, 322, 264, 1411, 12, 5543, 1252, 295, 364, 2681, 1465, 13, 682, 3820, 3497, 321, 818], "temperature": 0.0, "avg_logprob": -0.1861999920436314, "compression_ratio": 1.4914285714285713, "no_speech_prob": 6.339145784295397e-06}, {"id": 285, "seek": 192700, "start": 1931.24, "end": 1938.88, "text": " this an L-value. So we can take this state field, four things which are equal to ni,", "tokens": [341, 364, 441, 12, 29155, 13, 407, 321, 393, 747, 341, 1785, 2519, 11, 1451, 721, 597, 366, 2681, 281, 3867, 11], "temperature": 0.0, "avg_logprob": -0.1861999920436314, "compression_ratio": 1.4914285714285713, "no_speech_prob": 6.339145784295397e-06}, {"id": 286, "seek": 192700, "start": 1938.88, "end": 1946.2, "text": " and change their value to this. So this is like a very nice, simple technique that you'll", "tokens": [293, 1319, 641, 2158, 281, 341, 13, 407, 341, 307, 411, 257, 588, 1481, 11, 2199, 6532, 300, 291, 603], "temperature": 0.0, "avg_logprob": -0.1861999920436314, "compression_ratio": 1.4914285714285713, "no_speech_prob": 6.339145784295397e-06}, {"id": 287, "seek": 194620, "start": 1946.2, "end": 1957.0800000000002, "text": " use all the time in pandas, both for looking at things and for changing things.", "tokens": [764, 439, 264, 565, 294, 4565, 296, 11, 1293, 337, 1237, 412, 721, 293, 337, 4473, 721, 13], "temperature": 0.0, "avg_logprob": -0.2661272307573739, "compression_ratio": 1.4768211920529801, "no_speech_prob": 1.7502599803265184e-05}, {"id": 288, "seek": 194620, "start": 1957.0800000000002, "end": 1965.3600000000001, "text": " Question- In this particular example, do you think the granularity of the data matter,", "tokens": [14464, 12, 682, 341, 1729, 1365, 11, 360, 291, 519, 264, 39962, 507, 295, 264, 1412, 1871, 11], "temperature": 0.0, "avg_logprob": -0.2661272307573739, "compression_ratio": 1.4768211920529801, "no_speech_prob": 1.7502599803265184e-05}, {"id": 289, "seek": 194620, "start": 1965.3600000000001, "end": 1969.88, "text": " as in per day or per week? Is one better than the other?", "tokens": [382, 294, 680, 786, 420, 680, 1243, 30, 1119, 472, 1101, 813, 264, 661, 30], "temperature": 0.0, "avg_logprob": -0.2661272307573739, "compression_ratio": 1.4768211920529801, "no_speech_prob": 1.7502599803265184e-05}, {"id": 290, "seek": 196988, "start": 1969.88, "end": 1981.0800000000002, "text": " Answer. I would want to have the lower granularity so that I can capture that. Ideally you'd", "tokens": [24545, 13, 286, 576, 528, 281, 362, 264, 3126, 39962, 507, 370, 300, 286, 393, 7983, 300, 13, 40817, 291, 1116], "temperature": 0.0, "avg_logprob": -0.18625315030415854, "compression_ratio": 1.5418502202643172, "no_speech_prob": 1.5689314750488847e-05}, {"id": 291, "seek": 196988, "start": 1981.0800000000002, "end": 1988.72, "text": " want time as well. It kind of depends how the organization is going to use it. What", "tokens": [528, 565, 382, 731, 13, 467, 733, 295, 5946, 577, 264, 4475, 307, 516, 281, 764, 309, 13, 708], "temperature": 0.0, "avg_logprob": -0.18625315030415854, "compression_ratio": 1.5418502202643172, "no_speech_prob": 1.5689314750488847e-05}, {"id": 292, "seek": 196988, "start": 1988.72, "end": 1991.68, "text": " are they going to do with this information? It's probably for purchasing and stuff, so", "tokens": [366, 436, 516, 281, 360, 365, 341, 1589, 30, 467, 311, 1391, 337, 20906, 293, 1507, 11, 370], "temperature": 0.0, "avg_logprob": -0.18625315030415854, "compression_ratio": 1.5418502202643172, "no_speech_prob": 1.5689314750488847e-05}, {"id": 293, "seek": 196988, "start": 1991.68, "end": 1997.0800000000002, "text": " maybe they don't care about an hourly level. But clearly the difference between Sunday", "tokens": [1310, 436, 500, 380, 1127, 466, 364, 48364, 1496, 13, 583, 4448, 264, 2649, 1296, 7776], "temperature": 0.0, "avg_logprob": -0.18625315030415854, "compression_ratio": 1.5418502202643172, "no_speech_prob": 1.5689314750488847e-05}, {"id": 294, "seek": 199708, "start": 1997.08, "end": 2005.84, "text": " sales and Wednesday sales would be quite significant. So this is mainly a business context or domain", "tokens": [5763, 293, 10579, 5763, 576, 312, 1596, 4776, 13, 407, 341, 307, 8704, 257, 1606, 4319, 420, 9274], "temperature": 0.0, "avg_logprob": -0.18318610418410528, "compression_ratio": 1.6785714285714286, "no_speech_prob": 2.0462332031456754e-05}, {"id": 295, "seek": 199708, "start": 2005.84, "end": 2008.36, "text": " understanding question.", "tokens": [3701, 1168, 13], "temperature": 0.0, "avg_logprob": -0.18318610418410528, "compression_ratio": 1.6785714285714286, "no_speech_prob": 2.0462332031456754e-05}, {"id": 296, "seek": 199708, "start": 2008.36, "end": 2012.6, "text": " Question- Do you know if there's any work that compares for structured data supervised", "tokens": [14464, 12, 1144, 291, 458, 498, 456, 311, 604, 589, 300, 38334, 337, 18519, 1412, 46533], "temperature": 0.0, "avg_logprob": -0.18318610418410528, "compression_ratio": 1.6785714285714286, "no_speech_prob": 2.0462332031456754e-05}, {"id": 297, "seek": 199708, "start": 2012.6, "end": 2018.6, "text": " embeddings like these to embeddings that come from an unsupervised paradigm such as an auto-encoder?", "tokens": [12240, 29432, 411, 613, 281, 12240, 29432, 300, 808, 490, 364, 2693, 12879, 24420, 24709, 1270, 382, 364, 8399, 12, 22660, 19866, 30], "temperature": 0.0, "avg_logprob": -0.18318610418410528, "compression_ratio": 1.6785714285714286, "no_speech_prob": 2.0462332031456754e-05}, {"id": 298, "seek": 199708, "start": 2018.6, "end": 2023.52, "text": " It seems like you'd get more useful for prediction embeddings with the former case, but if you", "tokens": [467, 2544, 411, 291, 1116, 483, 544, 4420, 337, 17630, 12240, 29432, 365, 264, 5819, 1389, 11, 457, 498, 291], "temperature": 0.0, "avg_logprob": -0.18318610418410528, "compression_ratio": 1.6785714285714286, "no_speech_prob": 2.0462332031456754e-05}, {"id": 299, "seek": 199708, "start": 2023.52, "end": 2026.4399999999998, "text": " wanted general purpose embeddings you might prefer the latter.", "tokens": [1415, 2674, 4334, 12240, 29432, 291, 1062, 4382, 264, 18481, 13], "temperature": 0.0, "avg_logprob": -0.18318610418410528, "compression_ratio": 1.6785714285714286, "no_speech_prob": 2.0462332031456754e-05}, {"id": 300, "seek": 202644, "start": 2026.44, "end": 2031.3200000000002, "text": " Answer. I think you guys are aware of my feelings about auto-encoders. It's like giving up on", "tokens": [24545, 13, 286, 519, 291, 1074, 366, 3650, 295, 452, 6640, 466, 8399, 12, 22660, 378, 433, 13, 467, 311, 411, 2902, 493, 322], "temperature": 0.0, "avg_logprob": -0.13818984847884994, "compression_ratio": 1.7251908396946565, "no_speech_prob": 3.219171048840508e-05}, {"id": 301, "seek": 202644, "start": 2031.3200000000002, "end": 2037.56, "text": " life. You can always come up with a loss function that's more interesting than an auto-encoder", "tokens": [993, 13, 509, 393, 1009, 808, 493, 365, 257, 4470, 2445, 300, 311, 544, 1880, 813, 364, 8399, 12, 22660, 19866], "temperature": 0.0, "avg_logprob": -0.13818984847884994, "compression_ratio": 1.7251908396946565, "no_speech_prob": 3.219171048840508e-05}, {"id": 302, "seek": 202644, "start": 2037.56, "end": 2043.92, "text": " loss function. I would be very surprised if embeddings that came from a sales model were", "tokens": [4470, 2445, 13, 286, 576, 312, 588, 6100, 498, 12240, 29432, 300, 1361, 490, 257, 5763, 2316, 645], "temperature": 0.0, "avg_logprob": -0.13818984847884994, "compression_ratio": 1.7251908396946565, "no_speech_prob": 3.219171048840508e-05}, {"id": 303, "seek": 202644, "start": 2043.92, "end": 2047.68, "text": " not more useful for just about everything than something that came from an unsupervised", "tokens": [406, 544, 4420, 337, 445, 466, 1203, 813, 746, 300, 1361, 490, 364, 2693, 12879, 24420], "temperature": 0.0, "avg_logprob": -0.13818984847884994, "compression_ratio": 1.7251908396946565, "no_speech_prob": 3.219171048840508e-05}, {"id": 304, "seek": 202644, "start": 2047.68, "end": 2053.12, "text": " model. These things are easily tested, and if you do find a model that they don't work", "tokens": [2316, 13, 1981, 721, 366, 3612, 8246, 11, 293, 498, 291, 360, 915, 257, 2316, 300, 436, 500, 380, 589], "temperature": 0.0, "avg_logprob": -0.13818984847884994, "compression_ratio": 1.7251908396946565, "no_speech_prob": 3.219171048840508e-05}, {"id": 305, "seek": 205312, "start": 2053.12, "end": 2056.8399999999997, "text": " as well with, then you can come up with a different set of supervised embeddings for", "tokens": [382, 731, 365, 11, 550, 291, 393, 808, 493, 365, 257, 819, 992, 295, 46533, 12240, 29432, 337], "temperature": 0.0, "avg_logprob": -0.23900010413730266, "compression_ratio": 1.5375, "no_speech_prob": 2.0784849766641855e-05}, {"id": 306, "seek": 205312, "start": 2056.8399999999997, "end": 2057.8399999999997, "text": " that model.", "tokens": [300, 2316, 13], "temperature": 0.0, "avg_logprob": -0.23900010413730266, "compression_ratio": 1.5375, "no_speech_prob": 2.0784849766641855e-05}, {"id": 307, "seek": 205312, "start": 2057.8399999999997, "end": 2063.7599999999998, "text": " Question- There's also just a note that.ix is deprecated and we should use.loc instead.", "tokens": [14464, 12, 821, 311, 611, 445, 257, 3637, 300, 2411, 970, 307, 1367, 13867, 770, 293, 321, 820, 764, 2411, 5842, 2602, 13], "temperature": 0.0, "avg_logprob": -0.23900010413730266, "compression_ratio": 1.5375, "no_speech_prob": 2.0784849766641855e-05}, {"id": 308, "seek": 205312, "start": 2063.7599999999998, "end": 2070.52, "text": " Answer. I was going to mention Pandas is changing a lot. Because I've been running this course,", "tokens": [24545, 13, 286, 390, 516, 281, 2152, 16995, 296, 307, 4473, 257, 688, 13, 1436, 286, 600, 668, 2614, 341, 1164, 11], "temperature": 0.0, "avg_logprob": -0.23900010413730266, "compression_ratio": 1.5375, "no_speech_prob": 2.0784849766641855e-05}, {"id": 309, "seek": 205312, "start": 2070.52, "end": 2078.88, "text": " I have not been keeping track of the recent versions of Pandas. So that should be google", "tokens": [286, 362, 406, 668, 5145, 2837, 295, 264, 5162, 9606, 295, 16995, 296, 13, 407, 300, 820, 312, 20742], "temperature": 0.0, "avg_logprob": -0.23900010413730266, "compression_ratio": 1.5375, "no_speech_prob": 2.0784849766641855e-05}, {"id": 310, "seek": 207888, "start": 2078.88, "end": 2085.6800000000003, "text": " trend.log. In Pandas there's a whole page called Advanced Indexing Methods. I don't", "tokens": [6028, 13, 4987, 13, 682, 16995, 296, 456, 311, 257, 1379, 3028, 1219, 26951, 33552, 278, 25285, 82, 13, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.14190339048703512, "compression_ratio": 1.5276595744680852, "no_speech_prob": 1.7502641640021466e-05}, {"id": 311, "seek": 207888, "start": 2085.6800000000003, "end": 2089.76, "text": " find the Pandas documentation terribly clear, to be honest, but there is a fantastic book", "tokens": [915, 264, 16995, 296, 14333, 22903, 1850, 11, 281, 312, 3245, 11, 457, 456, 307, 257, 5456, 1446], "temperature": 0.0, "avg_logprob": -0.14190339048703512, "compression_ratio": 1.5276595744680852, "no_speech_prob": 1.7502641640021466e-05}, {"id": 312, "seek": 207888, "start": 2089.76, "end": 2096.48, "text": " by the author of Pandas called Python for Data Analysis. There is a new edition out,", "tokens": [538, 264, 3793, 295, 16995, 296, 1219, 15329, 337, 11888, 38172, 13, 821, 307, 257, 777, 11377, 484, 11], "temperature": 0.0, "avg_logprob": -0.14190339048703512, "compression_ratio": 1.5276595744680852, "no_speech_prob": 1.7502641640021466e-05}, {"id": 313, "seek": 207888, "start": 2096.48, "end": 2107.88, "text": " and it covers Pandas, NumPy, Matplotlib, whatever. That's the best way by far to actually understand", "tokens": [293, 309, 10538, 16995, 296, 11, 22592, 47, 88, 11, 6789, 564, 310, 38270, 11, 2035, 13, 663, 311, 264, 1151, 636, 538, 1400, 281, 767, 1223], "temperature": 0.0, "avg_logprob": -0.14190339048703512, "compression_ratio": 1.5276595744680852, "no_speech_prob": 1.7502641640021466e-05}, {"id": 314, "seek": 210788, "start": 2107.88, "end": 2112.28, "text": " Pandas, because the documentation is a bit of a nightmare and it keeps changing, so the", "tokens": [16995, 296, 11, 570, 264, 14333, 307, 257, 857, 295, 257, 18724, 293, 309, 5965, 4473, 11, 370, 264], "temperature": 0.0, "avg_logprob": -0.17314554850260416, "compression_ratio": 1.761061946902655, "no_speech_prob": 1.2411295756464824e-05}, {"id": 315, "seek": 210788, "start": 2112.28, "end": 2116.1600000000003, "text": " new version has all the new stuff in it.", "tokens": [777, 3037, 575, 439, 264, 777, 1507, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.17314554850260416, "compression_ratio": 1.761061946902655, "no_speech_prob": 1.2411295756464824e-05}, {"id": 316, "seek": 210788, "start": 2116.1600000000003, "end": 2122.2200000000003, "text": " With these kind of indexing methods, Pandas tries really hard to be intuitive, which means", "tokens": [2022, 613, 733, 295, 8186, 278, 7150, 11, 16995, 296, 9898, 534, 1152, 281, 312, 21769, 11, 597, 1355], "temperature": 0.0, "avg_logprob": -0.17314554850260416, "compression_ratio": 1.761061946902655, "no_speech_prob": 1.2411295756464824e-05}, {"id": 317, "seek": 210788, "start": 2122.2200000000003, "end": 2128.6400000000003, "text": " that quite often you'll read the documentation for these methods and it'll say, if you pass", "tokens": [300, 1596, 2049, 291, 603, 1401, 264, 14333, 337, 613, 7150, 293, 309, 603, 584, 11, 498, 291, 1320], "temperature": 0.0, "avg_logprob": -0.17314554850260416, "compression_ratio": 1.761061946902655, "no_speech_prob": 1.2411295756464824e-05}, {"id": 318, "seek": 210788, "start": 2128.6400000000003, "end": 2132.56, "text": " it a boolean, it'll behave in this way. If you pass it a float, it'll behave this way.", "tokens": [309, 257, 748, 4812, 282, 11, 309, 603, 15158, 294, 341, 636, 13, 759, 291, 1320, 309, 257, 15706, 11, 309, 603, 15158, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.17314554850260416, "compression_ratio": 1.761061946902655, "no_speech_prob": 1.2411295756464824e-05}, {"id": 319, "seek": 213256, "start": 2132.56, "end": 2139.52, "text": " If it's an index, it's this way, unless this other thing happens. I don't find it intuitive", "tokens": [759, 309, 311, 364, 8186, 11, 309, 311, 341, 636, 11, 5969, 341, 661, 551, 2314, 13, 286, 500, 380, 915, 309, 21769], "temperature": 0.0, "avg_logprob": -0.13920129793826666, "compression_ratio": 1.66015625, "no_speech_prob": 7.296277090063086e-06}, {"id": 320, "seek": 213256, "start": 2139.52, "end": 2143.2799999999997, "text": " at all because in the end I need to know how something works in order to use it correctly.", "tokens": [412, 439, 570, 294, 264, 917, 286, 643, 281, 458, 577, 746, 1985, 294, 1668, 281, 764, 309, 8944, 13], "temperature": 0.0, "avg_logprob": -0.13920129793826666, "compression_ratio": 1.66015625, "no_speech_prob": 7.296277090063086e-06}, {"id": 321, "seek": 213256, "start": 2143.2799999999997, "end": 2147.0, "text": " You end up having to remember this huge list of things.", "tokens": [509, 917, 493, 1419, 281, 1604, 341, 2603, 1329, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.13920129793826666, "compression_ratio": 1.66015625, "no_speech_prob": 7.296277090063086e-06}, {"id": 322, "seek": 213256, "start": 2147.0, "end": 2152.7599999999998, "text": " I think Pandas is great, but this is one thing to be very careful of, is to really make sure", "tokens": [286, 519, 16995, 296, 307, 869, 11, 457, 341, 307, 472, 551, 281, 312, 588, 5026, 295, 11, 307, 281, 534, 652, 988], "temperature": 0.0, "avg_logprob": -0.13920129793826666, "compression_ratio": 1.66015625, "no_speech_prob": 7.296277090063086e-06}, {"id": 323, "seek": 213256, "start": 2152.7599999999998, "end": 2157.04, "text": " you understand how all these indexing methods actually work. I know Rachel's laughing because", "tokens": [291, 1223, 577, 439, 613, 8186, 278, 7150, 767, 589, 13, 286, 458, 14246, 311, 5059, 570], "temperature": 0.0, "avg_logprob": -0.13920129793826666, "compression_ratio": 1.66015625, "no_speech_prob": 7.296277090063086e-06}, {"id": 324, "seek": 215704, "start": 2157.04, "end": 2163.32, "text": " she's been there and probably laughing in disgust at what we all have to go through.", "tokens": [750, 311, 668, 456, 293, 1391, 5059, 294, 14116, 381, 412, 437, 321, 439, 362, 281, 352, 807, 13], "temperature": 0.0, "avg_logprob": -0.22272495308307685, "compression_ratio": 1.682608695652174, "no_speech_prob": 1.280528522329405e-05}, {"id": 325, "seek": 215704, "start": 2163.32, "end": 2169.24, "text": " Question. When you use embeddings from a supervised model in another model, do you have to worry", "tokens": [14464, 13, 1133, 291, 764, 12240, 29432, 490, 257, 46533, 2316, 294, 1071, 2316, 11, 360, 291, 362, 281, 3292], "temperature": 0.0, "avg_logprob": -0.22272495308307685, "compression_ratio": 1.682608695652174, "no_speech_prob": 1.280528522329405e-05}, {"id": 326, "seek": 215704, "start": 2169.24, "end": 2172.4, "text": " about data leakage?", "tokens": [466, 1412, 47799, 30], "temperature": 0.0, "avg_logprob": -0.22272495308307685, "compression_ratio": 1.682608695652174, "no_speech_prob": 1.280528522329405e-05}, {"id": 327, "seek": 215704, "start": 2172.4, "end": 2181.92, "text": " Yes, you always have to worry about data leakage. I think that's a great point. I don't think", "tokens": [1079, 11, 291, 1009, 362, 281, 3292, 466, 1412, 47799, 13, 286, 519, 300, 311, 257, 869, 935, 13, 286, 500, 380, 519], "temperature": 0.0, "avg_logprob": -0.22272495308307685, "compression_ratio": 1.682608695652174, "no_speech_prob": 1.280528522329405e-05}, {"id": 328, "seek": 215704, "start": 2181.92, "end": 2185.96, "text": " I've got anything to add to that. You can figure out easily enough if there's data leakage.", "tokens": [286, 600, 658, 1340, 281, 909, 281, 300, 13, 509, 393, 2573, 484, 3612, 1547, 498, 456, 311, 1412, 47799, 13], "temperature": 0.0, "avg_logprob": -0.22272495308307685, "compression_ratio": 1.682608695652174, "no_speech_prob": 1.280528522329405e-05}, {"id": 329, "seek": 218596, "start": 2185.96, "end": 2199.92, "text": " That's a great question and definitely something to think about.", "tokens": [663, 311, 257, 869, 1168, 293, 2138, 746, 281, 519, 466, 13], "temperature": 0.0, "avg_logprob": -0.1647990412182278, "compression_ratio": 1.3166666666666667, "no_speech_prob": 3.611948613979621e-06}, {"id": 330, "seek": 218596, "start": 2199.92, "end": 2210.16, "text": " So there's this kind of standard set of steps that I take for every single structured machine", "tokens": [407, 456, 311, 341, 733, 295, 3832, 992, 295, 4439, 300, 286, 747, 337, 633, 2167, 18519, 3479], "temperature": 0.0, "avg_logprob": -0.1647990412182278, "compression_ratio": 1.3166666666666667, "no_speech_prob": 3.611948613979621e-06}, {"id": 331, "seek": 221016, "start": 2210.16, "end": 2218.48, "text": " learning model I do. One of those is every time I see a date, I always do this. I always", "tokens": [2539, 2316, 286, 360, 13, 1485, 295, 729, 307, 633, 565, 286, 536, 257, 4002, 11, 286, 1009, 360, 341, 13, 286, 1009], "temperature": 0.0, "avg_logprob": -0.139136929261057, "compression_ratio": 1.5284090909090908, "no_speech_prob": 2.3320524178416235e-06}, {"id": 332, "seek": 221016, "start": 2218.48, "end": 2226.92, "text": " create 4 more fields. The year, the month of year, the week of year, and the day of", "tokens": [1884, 1017, 544, 7909, 13, 440, 1064, 11, 264, 1618, 295, 1064, 11, 264, 1243, 295, 1064, 11, 293, 264, 786, 295], "temperature": 0.0, "avg_logprob": -0.139136929261057, "compression_ratio": 1.5284090909090908, "no_speech_prob": 2.3320524178416235e-06}, {"id": 333, "seek": 221016, "start": 2226.92, "end": 2228.64, "text": " week.", "tokens": [1243, 13], "temperature": 0.0, "avg_logprob": -0.139136929261057, "compression_ratio": 1.5284090909090908, "no_speech_prob": 2.3320524178416235e-06}, {"id": 334, "seek": 221016, "start": 2228.64, "end": 2238.24, "text": " This is something which should be automatically built into every data loader, I feel. It's", "tokens": [639, 307, 746, 597, 820, 312, 6772, 3094, 666, 633, 1412, 3677, 260, 11, 286, 841, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.139136929261057, "compression_ratio": 1.5284090909090908, "no_speech_prob": 2.3320524178416235e-06}, {"id": 335, "seek": 223824, "start": 2238.24, "end": 2243.9199999999996, "text": " so important because these are the kinds of structures that you see. Once every single", "tokens": [370, 1021, 570, 613, 366, 264, 3685, 295, 9227, 300, 291, 536, 13, 3443, 633, 2167], "temperature": 0.0, "avg_logprob": -0.13652833659997146, "compression_ratio": 1.6135265700483092, "no_speech_prob": 1.9033708440474584e-06}, {"id": 336, "seek": 223824, "start": 2243.9199999999996, "end": 2252.2799999999997, "text": " date has got this added to it, you're doing great. So you can see that I add that into", "tokens": [4002, 575, 658, 341, 3869, 281, 309, 11, 291, 434, 884, 869, 13, 407, 291, 393, 536, 300, 286, 909, 300, 666], "temperature": 0.0, "avg_logprob": -0.13652833659997146, "compression_ratio": 1.6135265700483092, "no_speech_prob": 1.9033708440474584e-06}, {"id": 337, "seek": 223824, "start": 2252.2799999999997, "end": 2262.4799999999996, "text": " all of my tables that have a date field. So we'll have that from now on.", "tokens": [439, 295, 452, 8020, 300, 362, 257, 4002, 2519, 13, 407, 321, 603, 362, 300, 490, 586, 322, 13], "temperature": 0.0, "avg_logprob": -0.13652833659997146, "compression_ratio": 1.6135265700483092, "no_speech_prob": 1.9033708440474584e-06}, {"id": 338, "seek": 223824, "start": 2262.4799999999996, "end": 2267.64, "text": " So now I go ahead and do all of these outer joins. You'll see that the first thing I do", "tokens": [407, 586, 286, 352, 2286, 293, 360, 439, 295, 613, 10847, 24397, 13, 509, 603, 536, 300, 264, 700, 551, 286, 360], "temperature": 0.0, "avg_logprob": -0.13652833659997146, "compression_ratio": 1.6135265700483092, "no_speech_prob": 1.9033708440474584e-06}, {"id": 339, "seek": 226764, "start": 2267.64, "end": 2278.2799999999997, "text": " after every outer join is check whether the thing I just joined with has any nulls. So", "tokens": [934, 633, 10847, 3917, 307, 1520, 1968, 264, 551, 286, 445, 6869, 365, 575, 604, 18184, 82, 13, 407], "temperature": 0.0, "avg_logprob": -0.15281870251610166, "compression_ratio": 1.6482412060301508, "no_speech_prob": 4.425463885127101e-06}, {"id": 340, "seek": 226764, "start": 2278.2799999999997, "end": 2284.72, "text": " even if you're sure that these things match perfectly, I would still never ever do an", "tokens": [754, 498, 291, 434, 988, 300, 613, 721, 2995, 6239, 11, 286, 576, 920, 1128, 1562, 360, 364], "temperature": 0.0, "avg_logprob": -0.15281870251610166, "compression_ratio": 1.6482412060301508, "no_speech_prob": 4.425463885127101e-06}, {"id": 341, "seek": 226764, "start": 2284.72, "end": 2290.6, "text": " inner join. Do the outer join and then check for nulls. That way if anything changes ever", "tokens": [7284, 3917, 13, 1144, 264, 10847, 3917, 293, 550, 1520, 337, 18184, 82, 13, 663, 636, 498, 1340, 2962, 1562], "temperature": 0.0, "avg_logprob": -0.15281870251610166, "compression_ratio": 1.6482412060301508, "no_speech_prob": 4.425463885127101e-06}, {"id": 342, "seek": 226764, "start": 2290.6, "end": 2296.8799999999997, "text": " or if you ever make a mistake, one of these things will not be 0.", "tokens": [420, 498, 291, 1562, 652, 257, 6146, 11, 472, 295, 613, 721, 486, 406, 312, 1958, 13], "temperature": 0.0, "avg_logprob": -0.15281870251610166, "compression_ratio": 1.6482412060301508, "no_speech_prob": 4.425463885127101e-06}, {"id": 343, "seek": 229688, "start": 2296.88, "end": 2302.78, "text": " If this was happening in a production process, this would be an assert. This would be emailing", "tokens": [759, 341, 390, 2737, 294, 257, 4265, 1399, 11, 341, 576, 312, 364, 19810, 13, 639, 576, 312, 3796, 278], "temperature": 0.0, "avg_logprob": -0.11281279443015516, "compression_ratio": 1.511111111111111, "no_speech_prob": 6.339145784295397e-06}, {"id": 344, "seek": 229688, "start": 2302.78, "end": 2309.56, "text": " Henry at 2am to say something you're relying on is not working the way it was meant to", "tokens": [11085, 412, 568, 335, 281, 584, 746, 291, 434, 24140, 322, 307, 406, 1364, 264, 636, 309, 390, 4140, 281], "temperature": 0.0, "avg_logprob": -0.11281279443015516, "compression_ratio": 1.511111111111111, "no_speech_prob": 6.339145784295397e-06}, {"id": 345, "seek": 229688, "start": 2309.56, "end": 2317.04, "text": " look out. So that's why I always do it this way. So you can see I'm just basically joining", "tokens": [574, 484, 13, 407, 300, 311, 983, 286, 1009, 360, 309, 341, 636, 13, 407, 291, 393, 536, 286, 478, 445, 1936, 5549], "temperature": 0.0, "avg_logprob": -0.11281279443015516, "compression_ratio": 1.511111111111111, "no_speech_prob": 6.339145784295397e-06}, {"id": 346, "seek": 231704, "start": 2317.04, "end": 2328.96, "text": " my training to everything else until it's all in there together in one big thing.", "tokens": [452, 3097, 281, 1203, 1646, 1826, 309, 311, 439, 294, 456, 1214, 294, 472, 955, 551, 13], "temperature": 0.0, "avg_logprob": -0.1756300449371338, "compression_ratio": 1.6682464454976302, "no_speech_prob": 1.0616036888677627e-05}, {"id": 347, "seek": 231704, "start": 2328.96, "end": 2334.5, "text": " So that table, everything joined together, is called joined. And then I do a whole bunch", "tokens": [407, 300, 3199, 11, 1203, 6869, 1214, 11, 307, 1219, 6869, 13, 400, 550, 286, 360, 257, 1379, 3840], "temperature": 0.0, "avg_logprob": -0.1756300449371338, "compression_ratio": 1.6682464454976302, "no_speech_prob": 1.0616036888677627e-05}, {"id": 348, "seek": 231704, "start": 2334.5, "end": 2340.88, "text": " more thinking about the people that won this competition, and then I replicated their results", "tokens": [544, 1953, 466, 264, 561, 300, 1582, 341, 6211, 11, 293, 550, 286, 46365, 641, 3542], "temperature": 0.0, "avg_logprob": -0.1756300449371338, "compression_ratio": 1.6682464454976302, "no_speech_prob": 1.0616036888677627e-05}, {"id": 349, "seek": 231704, "start": 2340.88, "end": 2346.08, "text": " from scratch. Think about what are all the other things you might want to do with these", "tokens": [490, 8459, 13, 6557, 466, 437, 366, 439, 264, 661, 721, 291, 1062, 528, 281, 360, 365, 613], "temperature": 0.0, "avg_logprob": -0.1756300449371338, "compression_ratio": 1.6682464454976302, "no_speech_prob": 1.0616036888677627e-05}, {"id": 350, "seek": 234608, "start": 2346.08, "end": 2347.4, "text": " dates.", "tokens": [11691, 13], "temperature": 0.0, "avg_logprob": -0.16422214772966173, "compression_ratio": 1.6184971098265897, "no_speech_prob": 1.0952987395285163e-05}, {"id": 351, "seek": 234608, "start": 2347.4, "end": 2353.7599999999998, "text": " So competition open, we noticed before, a third of the time they're empty. So we just", "tokens": [407, 6211, 1269, 11, 321, 5694, 949, 11, 257, 2636, 295, 264, 565, 436, 434, 6707, 13, 407, 321, 445], "temperature": 0.0, "avg_logprob": -0.16422214772966173, "compression_ratio": 1.6184971098265897, "no_speech_prob": 1.0952987395285163e-05}, {"id": 352, "seek": 234608, "start": 2353.7599999999998, "end": 2361.24, "text": " fill in the empties with some kind of sentinel value. Because a lot of machine learning systems", "tokens": [2836, 294, 264, 6113, 530, 365, 512, 733, 295, 2279, 40952, 2158, 13, 1436, 257, 688, 295, 3479, 2539, 3652], "temperature": 0.0, "avg_logprob": -0.16422214772966173, "compression_ratio": 1.6184971098265897, "no_speech_prob": 1.0952987395285163e-05}, {"id": 353, "seek": 234608, "start": 2361.24, "end": 2368.92, "text": " don't like missing values. Fill in the missing months with some sentinel value. Again, keep", "tokens": [500, 380, 411, 5361, 4190, 13, 25315, 294, 264, 5361, 2493, 365, 512, 2279, 40952, 2158, 13, 3764, 11, 1066], "temperature": 0.0, "avg_logprob": -0.16422214772966173, "compression_ratio": 1.6184971098265897, "no_speech_prob": 1.0952987395285163e-05}, {"id": 354, "seek": 236892, "start": 2368.92, "end": 2377.84, "text": " on filling in missing data. So fill in A is a really important thing to be aware of.", "tokens": [322, 10623, 294, 5361, 1412, 13, 407, 2836, 294, 316, 307, 257, 534, 1021, 551, 281, 312, 3650, 295, 13], "temperature": 0.0, "avg_logprob": -0.3307523934737496, "compression_ratio": 1.32, "no_speech_prob": 8.397850251640193e-06}, {"id": 355, "seek": 236892, "start": 2377.84, "end": 2392.48, "text": " Question. Is the filling in the month with 1 a real value, isn't that a problem?", "tokens": [14464, 13, 1119, 264, 10623, 294, 264, 1618, 365, 502, 257, 957, 2158, 11, 1943, 380, 300, 257, 1154, 30], "temperature": 0.0, "avg_logprob": -0.3307523934737496, "compression_ratio": 1.32, "no_speech_prob": 8.397850251640193e-06}, {"id": 356, "seek": 239248, "start": 2392.48, "end": 2399.28, "text": " I guess the answer is yes, it is a problem. But in this case I happen to know that every", "tokens": [286, 2041, 264, 1867, 307, 2086, 11, 309, 307, 257, 1154, 13, 583, 294, 341, 1389, 286, 1051, 281, 458, 300, 633], "temperature": 0.0, "avg_logprob": -0.156276145677888, "compression_ratio": 1.55, "no_speech_prob": 2.0904267330479342e-06}, {"id": 357, "seek": 239248, "start": 2399.28, "end": 2404.14, "text": " time year is empty, month is also empty, and we only ever use both of them together. So", "tokens": [565, 1064, 307, 6707, 11, 1618, 307, 611, 6707, 11, 293, 321, 787, 1562, 764, 1293, 295, 552, 1214, 13, 407], "temperature": 0.0, "avg_logprob": -0.156276145677888, "compression_ratio": 1.55, "no_speech_prob": 2.0904267330479342e-06}, {"id": 358, "seek": 239248, "start": 2404.14, "end": 2408.28, "text": " any model, whether it be tree-based or neural net-based or whatever, is going to take advantage", "tokens": [604, 2316, 11, 1968, 309, 312, 4230, 12, 6032, 420, 18161, 2533, 12, 6032, 420, 2035, 11, 307, 516, 281, 747, 5002], "temperature": 0.0, "avg_logprob": -0.156276145677888, "compression_ratio": 1.55, "no_speech_prob": 2.0904267330479342e-06}, {"id": 359, "seek": 239248, "start": 2408.28, "end": 2418.46, "text": " of that fact. Probably would have been safer to pick something else.", "tokens": [295, 300, 1186, 13, 9210, 576, 362, 668, 15856, 281, 1888, 746, 1646, 13], "temperature": 0.0, "avg_logprob": -0.156276145677888, "compression_ratio": 1.55, "no_speech_prob": 2.0904267330479342e-06}, {"id": 360, "seek": 241846, "start": 2418.46, "end": 2424.64, "text": " So we don't really care actually when the competition store was opened. What we really", "tokens": [407, 321, 500, 380, 534, 1127, 767, 562, 264, 6211, 3531, 390, 5625, 13, 708, 321, 534], "temperature": 0.0, "avg_logprob": -0.10929242769877116, "compression_ratio": 1.7424242424242424, "no_speech_prob": 6.240809852897655e-06}, {"id": 361, "seek": 241846, "start": 2424.64, "end": 2429.56, "text": " care about is how long is it between when they were opened and the particular row that", "tokens": [1127, 466, 307, 577, 938, 307, 309, 1296, 562, 436, 645, 5625, 293, 264, 1729, 5386, 300], "temperature": 0.0, "avg_logprob": -0.10929242769877116, "compression_ratio": 1.7424242424242424, "no_speech_prob": 6.240809852897655e-06}, {"id": 362, "seek": 241846, "start": 2429.56, "end": 2436.12, "text": " we're looking at. The sales on the 2nd of February 2014, how long was it between 2nd", "tokens": [321, 434, 1237, 412, 13, 440, 5763, 322, 264, 568, 273, 295, 8711, 8227, 11, 577, 938, 390, 309, 1296, 568, 273], "temperature": 0.0, "avg_logprob": -0.10929242769877116, "compression_ratio": 1.7424242424242424, "no_speech_prob": 6.240809852897655e-06}, {"id": 363, "seek": 241846, "start": 2436.12, "end": 2441.96, "text": " of February 2014 and when the competition opened. So you can see here we use this very", "tokens": [295, 8711, 8227, 293, 562, 264, 6211, 5625, 13, 407, 291, 393, 536, 510, 321, 764, 341, 588], "temperature": 0.0, "avg_logprob": -0.10929242769877116, "compression_ratio": 1.7424242424242424, "no_speech_prob": 6.240809852897655e-06}, {"id": 364, "seek": 244196, "start": 2441.96, "end": 2451.52, "text": " important.apply function which just runs a Python function on every row of a data frame.", "tokens": [1021, 2411, 1746, 356, 2445, 597, 445, 6676, 257, 15329, 2445, 322, 633, 5386, 295, 257, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.1970683918442837, "compression_ratio": 1.6764705882352942, "no_speech_prob": 7.296202966244891e-06}, {"id": 365, "seek": 244196, "start": 2451.52, "end": 2458.04, "text": " In this case the function is to create a new date from the open since year and the open", "tokens": [682, 341, 1389, 264, 2445, 307, 281, 1884, 257, 777, 4002, 490, 264, 1269, 1670, 1064, 293, 264, 1269], "temperature": 0.0, "avg_logprob": -0.1970683918442837, "compression_ratio": 1.6764705882352942, "no_speech_prob": 7.296202966244891e-06}, {"id": 366, "seek": 244196, "start": 2458.04, "end": 2463.2400000000002, "text": " since month. We're just going to assume that it's the middle of the month. That's our competition", "tokens": [1670, 1618, 13, 492, 434, 445, 516, 281, 6552, 300, 309, 311, 264, 2808, 295, 264, 1618, 13, 663, 311, 527, 6211], "temperature": 0.0, "avg_logprob": -0.1970683918442837, "compression_ratio": 1.6764705882352942, "no_speech_prob": 7.296202966244891e-06}, {"id": 367, "seek": 244196, "start": 2463.2400000000002, "end": 2470.64, "text": " open since. Then we can get our days open by just doing a subtract.", "tokens": [1269, 1670, 13, 1396, 321, 393, 483, 527, 1708, 1269, 538, 445, 884, 257, 16390, 13], "temperature": 0.0, "avg_logprob": -0.1970683918442837, "compression_ratio": 1.6764705882352942, "no_speech_prob": 7.296202966244891e-06}, {"id": 368, "seek": 247064, "start": 2470.64, "end": 2478.92, "text": " In Pandas, every date field has this special magical dt property, which is what all the", "tokens": [682, 16995, 296, 11, 633, 4002, 2519, 575, 341, 2121, 12066, 36423, 4707, 11, 597, 307, 437, 439, 264], "temperature": 0.0, "avg_logprob": -0.19771082401275636, "compression_ratio": 1.5475113122171946, "no_speech_prob": 3.905433459294727e-06}, {"id": 369, "seek": 247064, "start": 2478.92, "end": 2487.72, "text": " days, month, year, all that stuff sits inside this little dt property.", "tokens": [1708, 11, 1618, 11, 1064, 11, 439, 300, 1507, 12696, 1854, 341, 707, 36423, 4707, 13], "temperature": 0.0, "avg_logprob": -0.19771082401275636, "compression_ratio": 1.5475113122171946, "no_speech_prob": 3.905433459294727e-06}, {"id": 370, "seek": 247064, "start": 2487.72, "end": 2495.2, "text": " Now sometimes, as I mentioned, the competition actually opened later than the particular", "tokens": [823, 2171, 11, 382, 286, 2835, 11, 264, 6211, 767, 5625, 1780, 813, 264, 1729], "temperature": 0.0, "avg_logprob": -0.19771082401275636, "compression_ratio": 1.5475113122171946, "no_speech_prob": 3.905433459294727e-06}, {"id": 371, "seek": 247064, "start": 2495.2, "end": 2499.56, "text": " observation we're looking at. So that would give us a negative, so if we replace our negatives", "tokens": [14816, 321, 434, 1237, 412, 13, 407, 300, 576, 976, 505, 257, 3671, 11, 370, 498, 321, 7406, 527, 40019], "temperature": 0.0, "avg_logprob": -0.19771082401275636, "compression_ratio": 1.5475113122171946, "no_speech_prob": 3.905433459294727e-06}, {"id": 372, "seek": 249956, "start": 2499.56, "end": 2510.36, "text": " with 0. We're going to use an embedding for this, so that's why we replace days open with", "tokens": [365, 1958, 13, 492, 434, 516, 281, 764, 364, 12240, 3584, 337, 341, 11, 370, 300, 311, 983, 321, 7406, 1708, 1269, 365], "temperature": 0.0, "avg_logprob": -0.14294893699779845, "compression_ratio": 1.42, "no_speech_prob": 4.78505899081938e-06}, {"id": 373, "seek": 249956, "start": 2510.36, "end": 2518.7999999999997, "text": " months open, so we have less values.", "tokens": [2493, 1269, 11, 370, 321, 362, 1570, 4190, 13], "temperature": 0.0, "avg_logprob": -0.14294893699779845, "compression_ratio": 1.42, "no_speech_prob": 4.78505899081938e-06}, {"id": 374, "seek": 249956, "start": 2518.7999999999997, "end": 2523.64, "text": " I didn't actually try replacing this with a continuous variable. I suspect it wouldn't", "tokens": [286, 994, 380, 767, 853, 19139, 341, 365, 257, 10957, 7006, 13, 286, 9091, 309, 2759, 380], "temperature": 0.0, "avg_logprob": -0.14294893699779845, "compression_ratio": 1.42, "no_speech_prob": 4.78505899081938e-06}, {"id": 375, "seek": 252364, "start": 2523.64, "end": 2531.12, "text": " make too much difference, but this is what they did. In order to make the embedding again", "tokens": [652, 886, 709, 2649, 11, 457, 341, 307, 437, 436, 630, 13, 682, 1668, 281, 652, 264, 12240, 3584, 797], "temperature": 0.0, "avg_logprob": -0.10015858922685895, "compression_ratio": 1.6238095238095238, "no_speech_prob": 2.2959045509196585e-06}, {"id": 376, "seek": 252364, "start": 2531.12, "end": 2538.7599999999998, "text": " not too big, they replaced anything that was bigger than 2 years with 2 years.", "tokens": [406, 886, 955, 11, 436, 10772, 1340, 300, 390, 3801, 813, 568, 924, 365, 568, 924, 13], "temperature": 0.0, "avg_logprob": -0.10015858922685895, "compression_ratio": 1.6238095238095238, "no_speech_prob": 2.2959045509196585e-06}, {"id": 377, "seek": 252364, "start": 2538.7599999999998, "end": 2544.4, "text": " So there's our unique values. Every time you do something, print something out to make", "tokens": [407, 456, 311, 527, 3845, 4190, 13, 2048, 565, 291, 360, 746, 11, 4482, 746, 484, 281, 652], "temperature": 0.0, "avg_logprob": -0.10015858922685895, "compression_ratio": 1.6238095238095238, "no_speech_prob": 2.2959045509196585e-06}, {"id": 378, "seek": 252364, "start": 2544.4, "end": 2549.16, "text": " sure that the thing you thought you did is what you actually did. It's much easier if", "tokens": [988, 300, 264, 551, 291, 1194, 291, 630, 307, 437, 291, 767, 630, 13, 467, 311, 709, 3571, 498], "temperature": 0.0, "avg_logprob": -0.10015858922685895, "compression_ratio": 1.6238095238095238, "no_speech_prob": 2.2959045509196585e-06}, {"id": 379, "seek": 254916, "start": 2549.16, "end": 2554.8399999999997, "text": " we were using Excel because you see straight away what you're doing. In Python, this is", "tokens": [321, 645, 1228, 19060, 570, 291, 536, 2997, 1314, 437, 291, 434, 884, 13, 682, 15329, 11, 341, 307], "temperature": 0.0, "avg_logprob": -0.1970696176801409, "compression_ratio": 1.492063492063492, "no_speech_prob": 4.356839781394228e-06}, {"id": 380, "seek": 254916, "start": 2554.8399999999997, "end": 2559.8799999999997, "text": " the kind of stuff that you have to really be rigorous about checking your work at every", "tokens": [264, 733, 295, 1507, 300, 291, 362, 281, 534, 312, 29882, 466, 8568, 428, 589, 412, 633], "temperature": 0.0, "avg_logprob": -0.1970696176801409, "compression_ratio": 1.492063492063492, "no_speech_prob": 4.356839781394228e-06}, {"id": 381, "seek": 254916, "start": 2559.8799999999997, "end": 2570.92, "text": " step. When I build stuff like this, I generally make at least one error in every cell, so", "tokens": [1823, 13, 1133, 286, 1322, 1507, 411, 341, 11, 286, 5101, 652, 412, 1935, 472, 6713, 294, 633, 2815, 11, 370], "temperature": 0.0, "avg_logprob": -0.1970696176801409, "compression_ratio": 1.492063492063492, "no_speech_prob": 4.356839781394228e-06}, {"id": 382, "seek": 254916, "start": 2570.92, "end": 2571.92, "text": " check carefully.", "tokens": [1520, 7500, 13], "temperature": 0.0, "avg_logprob": -0.1970696176801409, "compression_ratio": 1.492063492063492, "no_speech_prob": 4.356839781394228e-06}, {"id": 383, "seek": 257192, "start": 2571.92, "end": 2582.6800000000003, "text": " Do the same thing for the promo days, turn those into weeks. So that's some basic preprocessing.", "tokens": [1144, 264, 912, 551, 337, 264, 26750, 1708, 11, 1261, 729, 666, 3259, 13, 407, 300, 311, 512, 3875, 2666, 340, 780, 278, 13], "temperature": 0.0, "avg_logprob": -0.18253650362529453, "compression_ratio": 1.4303030303030304, "no_speech_prob": 3.0415578748943517e-06}, {"id": 384, "seek": 257192, "start": 2582.6800000000003, "end": 2587.84, "text": " You get the idea of how Pandas works, hopefully.", "tokens": [509, 483, 264, 1558, 295, 577, 16995, 296, 1985, 11, 4696, 13], "temperature": 0.0, "avg_logprob": -0.18253650362529453, "compression_ratio": 1.4303030303030304, "no_speech_prob": 3.0415578748943517e-06}, {"id": 385, "seek": 257192, "start": 2587.84, "end": 2598.2000000000003, "text": " So the next thing that they did in the paper was a very common kind of time series feature", "tokens": [407, 264, 958, 551, 300, 436, 630, 294, 264, 3035, 390, 257, 588, 2689, 733, 295, 565, 2638, 4111], "temperature": 0.0, "avg_logprob": -0.18253650362529453, "compression_ratio": 1.4303030303030304, "no_speech_prob": 3.0415578748943517e-06}, {"id": 386, "seek": 259820, "start": 2598.2, "end": 2608.8399999999997, "text": " manipulation, one to be aware of. They basically wanted to say, every time there's a promotion,", "tokens": [26475, 11, 472, 281, 312, 3650, 295, 13, 814, 1936, 1415, 281, 584, 11, 633, 565, 456, 311, 257, 15783, 11], "temperature": 0.0, "avg_logprob": -0.1762344733528469, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.1573139242245816e-06}, {"id": 387, "seek": 259820, "start": 2608.8399999999997, "end": 2615.6, "text": " every time there's a holiday, I want to create some additional fields for every one of our", "tokens": [633, 565, 456, 311, 257, 9960, 11, 286, 528, 281, 1884, 512, 4497, 7909, 337, 633, 472, 295, 527], "temperature": 0.0, "avg_logprob": -0.1762344733528469, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.1573139242245816e-06}, {"id": 388, "seek": 259820, "start": 2615.6, "end": 2620.56, "text": " training set roads, which is on a particular date. On that date, how long is it until the", "tokens": [3097, 992, 11344, 11, 597, 307, 322, 257, 1729, 4002, 13, 1282, 300, 4002, 11, 577, 938, 307, 309, 1826, 264], "temperature": 0.0, "avg_logprob": -0.1762344733528469, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.1573139242245816e-06}, {"id": 389, "seek": 259820, "start": 2620.56, "end": 2626.9199999999996, "text": " next holiday? How long is it until the previous holiday? How long is it until the next promotion?", "tokens": [958, 9960, 30, 1012, 938, 307, 309, 1826, 264, 3894, 9960, 30, 1012, 938, 307, 309, 1826, 264, 958, 15783, 30], "temperature": 0.0, "avg_logprob": -0.1762344733528469, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.1573139242245816e-06}, {"id": 390, "seek": 262692, "start": 2626.92, "end": 2635.52, "text": " How long is it since the previous promotion? So if we basically create those fields, this", "tokens": [1012, 938, 307, 309, 1670, 264, 3894, 15783, 30, 407, 498, 321, 1936, 1884, 729, 7909, 11, 341], "temperature": 0.0, "avg_logprob": -0.14531564712524414, "compression_ratio": 1.5884955752212389, "no_speech_prob": 7.646471203770489e-06}, {"id": 391, "seek": 262692, "start": 2635.52, "end": 2641.6, "text": " is the kind of thing which is super difficult for any GBM or random forest or neural net", "tokens": [307, 264, 733, 295, 551, 597, 307, 1687, 2252, 337, 604, 460, 18345, 420, 4974, 6719, 420, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.14531564712524414, "compression_ratio": 1.5884955752212389, "no_speech_prob": 7.646471203770489e-06}, {"id": 392, "seek": 262692, "start": 2641.6, "end": 2649.2000000000003, "text": " to figure out how to calculate itself. There's no obvious kind of mathematical function that", "tokens": [281, 2573, 484, 577, 281, 8873, 2564, 13, 821, 311, 572, 6322, 733, 295, 18894, 2445, 300], "temperature": 0.0, "avg_logprob": -0.14531564712524414, "compression_ratio": 1.5884955752212389, "no_speech_prob": 7.646471203770489e-06}, {"id": 393, "seek": 262692, "start": 2649.2000000000003, "end": 2652.92, "text": " it's going to build on its own. So this is the kind of feature engineering that we have", "tokens": [309, 311, 516, 281, 1322, 322, 1080, 1065, 13, 407, 341, 307, 264, 733, 295, 4111, 7043, 300, 321, 362], "temperature": 0.0, "avg_logprob": -0.14531564712524414, "compression_ratio": 1.5884955752212389, "no_speech_prob": 7.646471203770489e-06}, {"id": 394, "seek": 265292, "start": 2652.92, "end": 2659.6800000000003, "text": " to do in order to allow us to use these kinds of techniques effectively on time series data.", "tokens": [281, 360, 294, 1668, 281, 2089, 505, 281, 764, 613, 3685, 295, 7512, 8659, 322, 565, 2638, 1412, 13], "temperature": 0.0, "avg_logprob": -0.10674818824319278, "compression_ratio": 1.5964125560538116, "no_speech_prob": 8.398020327149425e-06}, {"id": 395, "seek": 265292, "start": 2659.6800000000003, "end": 2667.36, "text": " So a lot of people who work with time series data, particularly in academia, outside of", "tokens": [407, 257, 688, 295, 561, 567, 589, 365, 565, 2638, 1412, 11, 4098, 294, 28937, 11, 2380, 295], "temperature": 0.0, "avg_logprob": -0.10674818824319278, "compression_ratio": 1.5964125560538116, "no_speech_prob": 8.398020327149425e-06}, {"id": 396, "seek": 265292, "start": 2667.36, "end": 2673.16, "text": " industry, they're just not aware of the fact that the state-of-the-art approaches really", "tokens": [3518, 11, 436, 434, 445, 406, 3650, 295, 264, 1186, 300, 264, 1785, 12, 2670, 12, 3322, 12, 446, 11587, 534], "temperature": 0.0, "avg_logprob": -0.10674818824319278, "compression_ratio": 1.5964125560538116, "no_speech_prob": 8.398020327149425e-06}, {"id": 397, "seek": 265292, "start": 2673.16, "end": 2680.32, "text": " involve all these heuristics. Separating out your dates into their components, turning", "tokens": [9494, 439, 613, 415, 374, 6006, 13, 43480, 990, 484, 428, 11691, 666, 641, 6677, 11, 6246], "temperature": 0.0, "avg_logprob": -0.10674818824319278, "compression_ratio": 1.5964125560538116, "no_speech_prob": 8.398020327149425e-06}, {"id": 398, "seek": 268032, "start": 2680.32, "end": 2687.0, "text": " everything you can into durations, both forwards and backwards, and also doing a bunch of,", "tokens": [1203, 291, 393, 666, 4861, 763, 11, 1293, 30126, 293, 12204, 11, 293, 611, 884, 257, 3840, 295, 11], "temperature": 0.0, "avg_logprob": -0.14897819665762094, "compression_ratio": 1.5897435897435896, "no_speech_prob": 3.288698735559592e-06}, {"id": 399, "seek": 268032, "start": 2687.0, "end": 2689.6800000000003, "text": " you'll see in a moment, running averages.", "tokens": [291, 603, 536, 294, 257, 1623, 11, 2614, 42257, 13], "temperature": 0.0, "avg_logprob": -0.14897819665762094, "compression_ratio": 1.5897435897435896, "no_speech_prob": 3.288698735559592e-06}, {"id": 400, "seek": 268032, "start": 2689.6800000000003, "end": 2699.56, "text": " So when I used to do a lot of this kind of work, I had a bunch of library functions that", "tokens": [407, 562, 286, 1143, 281, 360, 257, 688, 295, 341, 733, 295, 589, 11, 286, 632, 257, 3840, 295, 6405, 6828, 300], "temperature": 0.0, "avg_logprob": -0.14897819665762094, "compression_ratio": 1.5897435897435896, "no_speech_prob": 3.288698735559592e-06}, {"id": 401, "seek": 268032, "start": 2699.56, "end": 2704.2000000000003, "text": " I would run on every file that came in and would automatically do these things for every", "tokens": [286, 576, 1190, 322, 633, 3991, 300, 1361, 294, 293, 576, 6772, 360, 613, 721, 337, 633], "temperature": 0.0, "avg_logprob": -0.14897819665762094, "compression_ratio": 1.5897435897435896, "no_speech_prob": 3.288698735559592e-06}, {"id": 402, "seek": 270420, "start": 2704.2, "end": 2712.08, "text": " combination of dates. So this thing of how long until the next promotion, how long since", "tokens": [6562, 295, 11691, 13, 407, 341, 551, 295, 577, 938, 1826, 264, 958, 15783, 11, 577, 938, 1670], "temperature": 0.0, "avg_logprob": -0.1417175104588638, "compression_ratio": 1.6728971962616823, "no_speech_prob": 2.1567934709310066e-06}, {"id": 403, "seek": 270420, "start": 2712.08, "end": 2720.48, "text": " the previous promotion, is not easy to do in any database system pretty much, or indeed", "tokens": [264, 3894, 15783, 11, 307, 406, 1858, 281, 360, 294, 604, 8149, 1185, 1238, 709, 11, 420, 6451], "temperature": 0.0, "avg_logprob": -0.1417175104588638, "compression_ratio": 1.6728971962616823, "no_speech_prob": 2.1567934709310066e-06}, {"id": 404, "seek": 270420, "start": 2720.48, "end": 2725.12, "text": " in pandas. Because generally speaking, these kind of systems are looking for relationships", "tokens": [294, 4565, 296, 13, 1436, 5101, 4124, 11, 613, 733, 295, 3652, 366, 1237, 337, 6159], "temperature": 0.0, "avg_logprob": -0.1417175104588638, "compression_ratio": 1.6728971962616823, "no_speech_prob": 2.1567934709310066e-06}, {"id": 405, "seek": 270420, "start": 2725.12, "end": 2731.52, "text": " between tables, but we're trying to look at relationships between rows. So I had to create", "tokens": [1296, 8020, 11, 457, 321, 434, 1382, 281, 574, 412, 6159, 1296, 13241, 13, 407, 286, 632, 281, 1884], "temperature": 0.0, "avg_logprob": -0.1417175104588638, "compression_ratio": 1.6728971962616823, "no_speech_prob": 2.1567934709310066e-06}, {"id": 406, "seek": 273152, "start": 2731.52, "end": 2736.28, "text": " this tiny little simple little class to do this.", "tokens": [341, 5870, 707, 2199, 707, 1508, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.14168202309381395, "compression_ratio": 1.6102564102564103, "no_speech_prob": 9.972824955184478e-06}, {"id": 407, "seek": 273152, "start": 2736.28, "end": 2742.56, "text": " So basically what happens is, let's say I'm looking at school holiday. So I sort my data", "tokens": [407, 1936, 437, 2314, 307, 11, 718, 311, 584, 286, 478, 1237, 412, 1395, 9960, 13, 407, 286, 1333, 452, 1412], "temperature": 0.0, "avg_logprob": -0.14168202309381395, "compression_ratio": 1.6102564102564103, "no_speech_prob": 9.972824955184478e-06}, {"id": 408, "seek": 273152, "start": 2742.56, "end": 2750.84, "text": " frame by store and then by date, and I call this little function called add elapsed, school", "tokens": [3920, 538, 3531, 293, 550, 538, 4002, 11, 293, 286, 818, 341, 707, 2445, 1219, 909, 806, 2382, 292, 11, 1395], "temperature": 0.0, "avg_logprob": -0.14168202309381395, "compression_ratio": 1.6102564102564103, "no_speech_prob": 9.972824955184478e-06}, {"id": 409, "seek": 273152, "start": 2750.84, "end": 2756.98, "text": " holiday, after. What does add elapsed do? Add elapsed is going to create an instance", "tokens": [9960, 11, 934, 13, 708, 775, 909, 806, 2382, 292, 360, 30, 5349, 806, 2382, 292, 307, 516, 281, 1884, 364, 5197], "temperature": 0.0, "avg_logprob": -0.14168202309381395, "compression_ratio": 1.6102564102564103, "no_speech_prob": 9.972824955184478e-06}, {"id": 410, "seek": 275698, "start": 2756.98, "end": 2764.64, "text": " of this class called elapsed, and in this case it's going to be called with school holiday.", "tokens": [295, 341, 1508, 1219, 806, 2382, 292, 11, 293, 294, 341, 1389, 309, 311, 516, 281, 312, 1219, 365, 1395, 9960, 13], "temperature": 0.0, "avg_logprob": -0.1563467554526754, "compression_ratio": 1.9095744680851063, "no_speech_prob": 1.3497011650542845e-06}, {"id": 411, "seek": 275698, "start": 2764.64, "end": 2768.72, "text": " So what this class is going to do, we're going to be calling this apply function again, it's", "tokens": [407, 437, 341, 1508, 307, 516, 281, 360, 11, 321, 434, 516, 281, 312, 5141, 341, 3079, 2445, 797, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.1563467554526754, "compression_ratio": 1.9095744680851063, "no_speech_prob": 1.3497011650542845e-06}, {"id": 412, "seek": 275698, "start": 2768.72, "end": 2775.68, "text": " going to run on every single row, and it's going to call my elapsed class.det for every", "tokens": [516, 281, 1190, 322, 633, 2167, 5386, 11, 293, 309, 311, 516, 281, 818, 452, 806, 2382, 292, 1508, 13, 17863, 337, 633], "temperature": 0.0, "avg_logprob": -0.1563467554526754, "compression_ratio": 1.9095744680851063, "no_speech_prob": 1.3497011650542845e-06}, {"id": 413, "seek": 275698, "start": 2775.68, "end": 2781.84, "text": " row. So I'm going to go through every row in order of store, in order of date, and I'm", "tokens": [5386, 13, 407, 286, 478, 516, 281, 352, 807, 633, 5386, 294, 1668, 295, 3531, 11, 294, 1668, 295, 4002, 11, 293, 286, 478], "temperature": 0.0, "avg_logprob": -0.1563467554526754, "compression_ratio": 1.9095744680851063, "no_speech_prob": 1.3497011650542845e-06}, {"id": 414, "seek": 278184, "start": 2781.84, "end": 2788.8, "text": " going to try to find how long has it been since the last school holiday. So when I create", "tokens": [516, 281, 853, 281, 915, 577, 938, 575, 309, 668, 1670, 264, 1036, 1395, 9960, 13, 407, 562, 286, 1884], "temperature": 0.0, "avg_logprob": -0.15361894260753284, "compression_ratio": 1.6287128712871286, "no_speech_prob": 2.948004066638532e-06}, {"id": 415, "seek": 278184, "start": 2788.8, "end": 2793.2400000000002, "text": " this object, I just have to keep track of what field is it.", "tokens": [341, 2657, 11, 286, 445, 362, 281, 1066, 2837, 295, 437, 2519, 307, 309, 13], "temperature": 0.0, "avg_logprob": -0.15361894260753284, "compression_ratio": 1.6287128712871286, "no_speech_prob": 2.948004066638532e-06}, {"id": 416, "seek": 278184, "start": 2793.2400000000002, "end": 2800.2400000000002, "text": " School holiday, initialize. When was the last time we saw a school holiday? The answer is", "tokens": [5070, 9960, 11, 5883, 1125, 13, 1133, 390, 264, 1036, 565, 321, 1866, 257, 1395, 9960, 30, 440, 1867, 307], "temperature": 0.0, "avg_logprob": -0.15361894260753284, "compression_ratio": 1.6287128712871286, "no_speech_prob": 2.948004066638532e-06}, {"id": 417, "seek": 278184, "start": 2800.2400000000002, "end": 2807.52, "text": " we haven't, so initialize it to not a number. And we also have to know each time we cross", "tokens": [321, 2378, 380, 11, 370, 5883, 1125, 309, 281, 406, 257, 1230, 13, 400, 321, 611, 362, 281, 458, 1184, 565, 321, 3278], "temperature": 0.0, "avg_logprob": -0.15361894260753284, "compression_ratio": 1.6287128712871286, "no_speech_prob": 2.948004066638532e-06}, {"id": 418, "seek": 280752, "start": 2807.52, "end": 2811.88, "text": " over to a new store. When we cross over to a new store, we have to reinitialize. So the", "tokens": [670, 281, 257, 777, 3531, 13, 1133, 321, 3278, 670, 281, 257, 777, 3531, 11, 321, 362, 281, 6561, 270, 831, 1125, 13, 407, 264], "temperature": 0.0, "avg_logprob": -0.1420277673370984, "compression_ratio": 1.7777777777777777, "no_speech_prob": 4.495160737860715e-06}, {"id": 419, "seek": 280752, "start": 2811.88, "end": 2817.96, "text": " previous store was 0. So every time we call get, we basically check, have we crossed over", "tokens": [3894, 3531, 390, 1958, 13, 407, 633, 565, 321, 818, 483, 11, 321, 1936, 1520, 11, 362, 321, 14622, 670], "temperature": 0.0, "avg_logprob": -0.1420277673370984, "compression_ratio": 1.7777777777777777, "no_speech_prob": 4.495160737860715e-06}, {"id": 420, "seek": 280752, "start": 2817.96, "end": 2825.32, "text": " to a new store, and if so, just initialize both of those things back again. And then", "tokens": [281, 257, 777, 3531, 11, 293, 498, 370, 11, 445, 5883, 1125, 1293, 295, 729, 721, 646, 797, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.1420277673370984, "compression_ratio": 1.7777777777777777, "no_speech_prob": 4.495160737860715e-06}, {"id": 421, "seek": 280752, "start": 2825.32, "end": 2833.0, "text": " we just say, is this a school holiday? If so, then the last time you saw a school holiday", "tokens": [321, 445, 584, 11, 307, 341, 257, 1395, 9960, 30, 759, 370, 11, 550, 264, 1036, 565, 291, 1866, 257, 1395, 9960], "temperature": 0.0, "avg_logprob": -0.1420277673370984, "compression_ratio": 1.7777777777777777, "no_speech_prob": 4.495160737860715e-06}, {"id": 422, "seek": 283300, "start": 2833.0, "end": 2838.86, "text": " is today. And then finally return, how long is it between today and the last time you", "tokens": [307, 965, 13, 400, 550, 2721, 2736, 11, 577, 938, 307, 309, 1296, 965, 293, 264, 1036, 565, 291], "temperature": 0.0, "avg_logprob": -0.16621514161427817, "compression_ratio": 1.6894977168949772, "no_speech_prob": 1.5056954225656227e-06}, {"id": 423, "seek": 283300, "start": 2838.86, "end": 2840.68, "text": " saw a school holiday.", "tokens": [1866, 257, 1395, 9960, 13], "temperature": 0.0, "avg_logprob": -0.16621514161427817, "compression_ratio": 1.6894977168949772, "no_speech_prob": 1.5056954225656227e-06}, {"id": 424, "seek": 283300, "start": 2840.68, "end": 2848.68, "text": " So basically this class is a way of keeping track of some memory about when did I last", "tokens": [407, 1936, 341, 1508, 307, 257, 636, 295, 5145, 2837, 295, 512, 4675, 466, 562, 630, 286, 1036], "temperature": 0.0, "avg_logprob": -0.16621514161427817, "compression_ratio": 1.6894977168949772, "no_speech_prob": 1.5056954225656227e-06}, {"id": 425, "seek": 283300, "start": 2848.68, "end": 2855.52, "text": " see this observation. So then by just calling df.apply, it's going to keep track of this", "tokens": [536, 341, 14816, 13, 407, 550, 538, 445, 5141, 274, 69, 13, 1746, 356, 11, 309, 311, 516, 281, 1066, 2837, 295, 341], "temperature": 0.0, "avg_logprob": -0.16621514161427817, "compression_ratio": 1.6894977168949772, "no_speech_prob": 1.5056954225656227e-06}, {"id": 426, "seek": 283300, "start": 2855.52, "end": 2861.4, "text": " for every single row, and so then I can call that for school holiday after and before,", "tokens": [337, 633, 2167, 5386, 11, 293, 370, 550, 286, 393, 818, 300, 337, 1395, 9960, 934, 293, 949, 11], "temperature": 0.0, "avg_logprob": -0.16621514161427817, "compression_ratio": 1.6894977168949772, "no_speech_prob": 1.5056954225656227e-06}, {"id": 427, "seek": 286140, "start": 2861.4, "end": 2867.82, "text": " the only difference being that for before I just sort my dates in descending order.", "tokens": [264, 787, 2649, 885, 300, 337, 949, 286, 445, 1333, 452, 11691, 294, 40182, 1668, 13], "temperature": 0.0, "avg_logprob": -0.22147245407104493, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.1125338460260537e-05}, {"id": 428, "seek": 286140, "start": 2867.82, "end": 2875.7200000000003, "text": " State holiday and promo. So that's going to add in the end 6 fields, how long until and", "tokens": [4533, 9960, 293, 26750, 13, 407, 300, 311, 516, 281, 909, 294, 264, 917, 1386, 7909, 11, 577, 938, 1826, 293], "temperature": 0.0, "avg_logprob": -0.22147245407104493, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.1125338460260537e-05}, {"id": 429, "seek": 286140, "start": 2875.7200000000003, "end": 2881.0, "text": " how long since the last school holiday, state holiday, and promotion.", "tokens": [577, 938, 1670, 264, 1036, 1395, 9960, 11, 1785, 9960, 11, 293, 15783, 13], "temperature": 0.0, "avg_logprob": -0.22147245407104493, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.1125338460260537e-05}, {"id": 430, "seek": 286140, "start": 2881.0, "end": 2890.04, "text": " And then there's 2 questions. One asking, is this similar to a windowing function?", "tokens": [400, 550, 456, 311, 568, 1651, 13, 1485, 3365, 11, 307, 341, 2531, 281, 257, 4910, 278, 2445, 30], "temperature": 0.0, "avg_logprob": -0.22147245407104493, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.1125338460260537e-05}, {"id": 431, "seek": 289004, "start": 2890.04, "end": 2892.4, "text": " Not quite, we're about to do a windowing function.", "tokens": [1726, 1596, 11, 321, 434, 466, 281, 360, 257, 4910, 278, 2445, 13], "temperature": 0.0, "avg_logprob": -0.21614430251630765, "compression_ratio": 1.669683257918552, "no_speech_prob": 3.591073618736118e-05}, {"id": 432, "seek": 289004, "start": 2892.4, "end": 2896.24, "text": " And then is there a reason to think that the current approach would be problematic with", "tokens": [400, 550, 307, 456, 257, 1778, 281, 519, 300, 264, 2190, 3109, 576, 312, 19011, 365], "temperature": 0.0, "avg_logprob": -0.21614430251630765, "compression_ratio": 1.669683257918552, "no_speech_prob": 3.591073618736118e-05}, {"id": 433, "seek": 289004, "start": 2896.24, "end": 2897.24, "text": " sparse data?", "tokens": [637, 11668, 1412, 30], "temperature": 0.0, "avg_logprob": -0.21614430251630765, "compression_ratio": 1.669683257918552, "no_speech_prob": 3.591073618736118e-05}, {"id": 434, "seek": 289004, "start": 2897.24, "end": 2905.44, "text": " I don't see why, but I'm not sure I quite follow.", "tokens": [286, 500, 380, 536, 983, 11, 457, 286, 478, 406, 988, 286, 1596, 1524, 13], "temperature": 0.0, "avg_logprob": -0.21614430251630765, "compression_ratio": 1.669683257918552, "no_speech_prob": 3.591073618736118e-05}, {"id": 435, "seek": 289004, "start": 2905.44, "end": 2910.72, "text": " So we don't care about absolute dates, we care about time deltas between events.", "tokens": [407, 321, 500, 380, 1127, 466, 8236, 11691, 11, 321, 1127, 466, 565, 1103, 83, 296, 1296, 3931, 13], "temperature": 0.0, "avg_logprob": -0.21614430251630765, "compression_ratio": 1.669683257918552, "no_speech_prob": 3.591073618736118e-05}, {"id": 436, "seek": 289004, "start": 2910.72, "end": 2918.16, "text": " We care about 2 things. We do care about the dates, but we care about what year is it,", "tokens": [492, 1127, 466, 568, 721, 13, 492, 360, 1127, 466, 264, 11691, 11, 457, 321, 1127, 466, 437, 1064, 307, 309, 11], "temperature": 0.0, "avg_logprob": -0.21614430251630765, "compression_ratio": 1.669683257918552, "no_speech_prob": 3.591073618736118e-05}, {"id": 437, "seek": 291816, "start": 2918.16, "end": 2926.6, "text": " what day of the week it is. And we also care about the elapsed time between the date I'm", "tokens": [437, 786, 295, 264, 1243, 309, 307, 13, 400, 321, 611, 1127, 466, 264, 806, 2382, 292, 565, 1296, 264, 4002, 286, 478], "temperature": 0.0, "avg_logprob": -0.1670611472356887, "compression_ratio": 1.5911330049261083, "no_speech_prob": 4.495141638471978e-06}, {"id": 438, "seek": 291816, "start": 2926.6, "end": 2933.24, "text": " predicting sales for and the previous and next of various events.", "tokens": [32884, 5763, 337, 293, 264, 3894, 293, 958, 295, 3683, 3931, 13], "temperature": 0.0, "avg_logprob": -0.1670611472356887, "compression_ratio": 1.5911330049261083, "no_speech_prob": 4.495141638471978e-06}, {"id": 439, "seek": 291816, "start": 2933.24, "end": 2936.8399999999997, "text": " And then windowing functions.", "tokens": [400, 550, 4910, 278, 6828, 13], "temperature": 0.0, "avg_logprob": -0.1670611472356887, "compression_ratio": 1.5911330049261083, "no_speech_prob": 4.495141638471978e-06}, {"id": 440, "seek": 291816, "start": 2936.8399999999997, "end": 2942.3599999999997, "text": " For the features that are time until an event, how do you deal with that given that you might", "tokens": [1171, 264, 4122, 300, 366, 565, 1826, 364, 2280, 11, 577, 360, 291, 2028, 365, 300, 2212, 300, 291, 1062], "temperature": 0.0, "avg_logprob": -0.1670611472356887, "compression_ratio": 1.5911330049261083, "no_speech_prob": 4.495141638471978e-06}, {"id": 441, "seek": 291816, "start": 2942.3599999999997, "end": 2946.3599999999997, "text": " not know when the last event is in the data?", "tokens": [406, 458, 562, 264, 1036, 2280, 307, 294, 264, 1412, 30], "temperature": 0.0, "avg_logprob": -0.1670611472356887, "compression_ratio": 1.5911330049261083, "no_speech_prob": 4.495141638471978e-06}, {"id": 442, "seek": 294636, "start": 2946.36, "end": 2957.84, "text": " Well all I do is I've sorted descending, and then we initialize last with not a number.", "tokens": [1042, 439, 286, 360, 307, 286, 600, 25462, 40182, 11, 293, 550, 321, 5883, 1125, 1036, 365, 406, 257, 1230, 13], "temperature": 0.0, "avg_logprob": -0.16544896277828494, "compression_ratio": 1.5975609756097562, "no_speech_prob": 5.422142749011982e-06}, {"id": 443, "seek": 294636, "start": 2957.84, "end": 2964.0, "text": " So basically when we then go subtract, here we are, subtract, and it tries to subtract", "tokens": [407, 1936, 562, 321, 550, 352, 16390, 11, 510, 321, 366, 11, 16390, 11, 293, 309, 9898, 281, 16390], "temperature": 0.0, "avg_logprob": -0.16544896277828494, "compression_ratio": 1.5975609756097562, "no_speech_prob": 5.422142749011982e-06}, {"id": 444, "seek": 294636, "start": 2964.0, "end": 2970.6800000000003, "text": " not a number, we'll end up with a null. So basically anything that's at an unknown time", "tokens": [406, 257, 1230, 11, 321, 603, 917, 493, 365, 257, 18184, 13, 407, 1936, 1340, 300, 311, 412, 364, 9841, 565], "temperature": 0.0, "avg_logprob": -0.16544896277828494, "compression_ratio": 1.5975609756097562, "no_speech_prob": 5.422142749011982e-06}, {"id": 445, "seek": 297068, "start": 2970.68, "end": 2979.96, "text": " because it's at one end or the other is going to end up null, which is why we're going to", "tokens": [570, 309, 311, 412, 472, 917, 420, 264, 661, 307, 516, 281, 917, 493, 18184, 11, 597, 307, 983, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.17691991545937277, "compression_ratio": 1.4857142857142858, "no_speech_prob": 1.903372663036862e-06}, {"id": 446, "seek": 297068, "start": 2979.96, "end": 2985.48, "text": " replace those nulls with zeros.", "tokens": [7406, 729, 18184, 82, 365, 35193, 13], "temperature": 0.0, "avg_logprob": -0.17691991545937277, "compression_ratio": 1.4857142857142858, "no_speech_prob": 1.903372663036862e-06}, {"id": 447, "seek": 297068, "start": 2985.48, "end": 2990.44, "text": " Pandas has this slightly strange way of thinking about indexes, but once you get used to it", "tokens": [16995, 296, 575, 341, 4748, 5861, 636, 295, 1953, 466, 8186, 279, 11, 457, 1564, 291, 483, 1143, 281, 309], "temperature": 0.0, "avg_logprob": -0.17691991545937277, "compression_ratio": 1.4857142857142858, "no_speech_prob": 1.903372663036862e-06}, {"id": 448, "seek": 297068, "start": 2990.44, "end": 2999.44, "text": " it's fine. At any point you can call dataFrame.setIndex and pass in a field. You then have to just", "tokens": [309, 311, 2489, 13, 1711, 604, 935, 291, 393, 818, 1412, 40305, 529, 13, 3854, 21790, 3121, 293, 1320, 294, 257, 2519, 13, 509, 550, 362, 281, 445], "temperature": 0.0, "avg_logprob": -0.17691991545937277, "compression_ratio": 1.4857142857142858, "no_speech_prob": 1.903372663036862e-06}, {"id": 449, "seek": 299944, "start": 2999.44, "end": 3004.7200000000003, "text": " remember what field you have as the index because quite a few methods in pandas use", "tokens": [1604, 437, 2519, 291, 362, 382, 264, 8186, 570, 1596, 257, 1326, 7150, 294, 4565, 296, 764], "temperature": 0.0, "avg_logprob": -0.1361179659443517, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.7850594455667306e-06}, {"id": 450, "seek": 299944, "start": 3004.7200000000003, "end": 3010.16, "text": " the currently active index by default, and of course things will run faster when you", "tokens": [264, 4362, 4967, 8186, 538, 7576, 11, 293, 295, 1164, 721, 486, 1190, 4663, 562, 291], "temperature": 0.0, "avg_logprob": -0.1361179659443517, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.7850594455667306e-06}, {"id": 451, "seek": 299944, "start": 3010.16, "end": 3015.56, "text": " do stuff with the currently active index. And you can pass multiple fields, in which", "tokens": [360, 1507, 365, 264, 4362, 4967, 8186, 13, 400, 291, 393, 1320, 3866, 7909, 11, 294, 597], "temperature": 0.0, "avg_logprob": -0.1361179659443517, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.7850594455667306e-06}, {"id": 452, "seek": 299944, "start": 3015.56, "end": 3020.32, "text": " case you end up with a multiple key index.", "tokens": [1389, 291, 917, 493, 365, 257, 3866, 2141, 8186, 13], "temperature": 0.0, "avg_logprob": -0.1361179659443517, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.7850594455667306e-06}, {"id": 453, "seek": 299944, "start": 3020.32, "end": 3026.46, "text": " So the next thing we do is these windowing functions. So a windowing function in pandas,", "tokens": [407, 264, 958, 551, 321, 360, 307, 613, 4910, 278, 6828, 13, 407, 257, 4910, 278, 2445, 294, 4565, 296, 11], "temperature": 0.0, "avg_logprob": -0.1361179659443517, "compression_ratio": 1.8333333333333333, "no_speech_prob": 4.7850594455667306e-06}, {"id": 454, "seek": 302646, "start": 3026.46, "end": 3031.2400000000002, "text": " we can use this rolling. So this is like a rolling mean, rolling min, rolling max, whatever", "tokens": [321, 393, 764, 341, 9439, 13, 407, 341, 307, 411, 257, 9439, 914, 11, 9439, 923, 11, 9439, 11469, 11, 2035], "temperature": 0.0, "avg_logprob": -0.200456786941696, "compression_ratio": 1.7163461538461537, "no_speech_prob": 1.0783210200315807e-05}, {"id": 455, "seek": 302646, "start": 3031.2400000000002, "end": 3040.46, "text": " you like. So this basically says, alright, let's take our data frame with the columns", "tokens": [291, 411, 13, 407, 341, 1936, 1619, 11, 5845, 11, 718, 311, 747, 527, 1412, 3920, 365, 264, 13766], "temperature": 0.0, "avg_logprob": -0.200456786941696, "compression_ratio": 1.7163461538461537, "no_speech_prob": 1.0783210200315807e-05}, {"id": 456, "seek": 302646, "start": 3040.46, "end": 3045.56, "text": " we're interested in, school, holidays, holiday, and promo, and we're going to keep track of", "tokens": [321, 434, 3102, 294, 11, 1395, 11, 15734, 11, 9960, 11, 293, 26750, 11, 293, 321, 434, 516, 281, 1066, 2837, 295], "temperature": 0.0, "avg_logprob": -0.200456786941696, "compression_ratio": 1.7163461538461537, "no_speech_prob": 1.0783210200315807e-05}, {"id": 457, "seek": 302646, "start": 3045.56, "end": 3051.92, "text": " how many holidays are there in the next week and the previous week. How many promos are", "tokens": [577, 867, 15734, 366, 456, 294, 264, 958, 1243, 293, 264, 3894, 1243, 13, 1012, 867, 2234, 329, 366], "temperature": 0.0, "avg_logprob": -0.200456786941696, "compression_ratio": 1.7163461538461537, "no_speech_prob": 1.0783210200315807e-05}, {"id": 458, "seek": 305192, "start": 3051.92, "end": 3058.2000000000003, "text": " there in the next week and the previous week.", "tokens": [456, 294, 264, 958, 1243, 293, 264, 3894, 1243, 13], "temperature": 0.0, "avg_logprob": -0.20383516947428384, "compression_ratio": 1.5333333333333334, "no_speech_prob": 2.8573010695254197e-06}, {"id": 459, "seek": 305192, "start": 3058.2000000000003, "end": 3068.66, "text": " To do that we can sort by date, group by store, and then rolling will be applied to each group.", "tokens": [1407, 360, 300, 321, 393, 1333, 538, 4002, 11, 1594, 538, 3531, 11, 293, 550, 9439, 486, 312, 6456, 281, 1184, 1594, 13], "temperature": 0.0, "avg_logprob": -0.20383516947428384, "compression_ratio": 1.5333333333333334, "no_speech_prob": 2.8573010695254197e-06}, {"id": 460, "seek": 305192, "start": 3068.66, "end": 3081.2000000000003, "text": " So within each group, create a rolling 7-day sum. So it's kind of like, it's the kind of", "tokens": [407, 1951, 1184, 1594, 11, 1884, 257, 9439, 1614, 12, 810, 2408, 13, 407, 309, 311, 733, 295, 411, 11, 309, 311, 264, 733, 295], "temperature": 0.0, "avg_logprob": -0.20383516947428384, "compression_ratio": 1.5333333333333334, "no_speech_prob": 2.8573010695254197e-06}, {"id": 461, "seek": 308120, "start": 3081.2, "end": 3087.8799999999997, "text": " notation I'm never likely to remember, but you can just look it up. So this is how you", "tokens": [24657, 286, 478, 1128, 3700, 281, 1604, 11, 457, 291, 393, 445, 574, 309, 493, 13, 407, 341, 307, 577, 291], "temperature": 0.0, "avg_logprob": -0.21309491937810723, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.66594507190166e-06}, {"id": 462, "seek": 308120, "start": 3087.8799999999997, "end": 3096.3199999999997, "text": " do group by type stuff. Pandas actually has quite a lot of time series functions. This", "tokens": [360, 1594, 538, 2010, 1507, 13, 16995, 296, 767, 575, 1596, 257, 688, 295, 565, 2638, 6828, 13, 639], "temperature": 0.0, "avg_logprob": -0.21309491937810723, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.66594507190166e-06}, {"id": 463, "seek": 308120, "start": 3096.3199999999997, "end": 3101.08, "text": " rolling function is one of the most useful ones. Wes McKinney had a background as a", "tokens": [9439, 2445, 307, 472, 295, 264, 881, 4420, 2306, 13, 23843, 21765, 259, 2397, 632, 257, 3678, 382, 257], "temperature": 0.0, "avg_logprob": -0.21309491937810723, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.66594507190166e-06}, {"id": 464, "seek": 308120, "start": 3101.08, "end": 3106.16, "text": " quant with memory serves correctly, so quants love their time series functions, so I think", "tokens": [4426, 365, 4675, 13451, 8944, 11, 370, 421, 1719, 959, 641, 565, 2638, 6828, 11, 370, 286, 519], "temperature": 0.0, "avg_logprob": -0.21309491937810723, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.66594507190166e-06}, {"id": 465, "seek": 308120, "start": 3106.16, "end": 3110.64, "text": " that was a lot of the history of pandas. So if you're interested in time series stuff,", "tokens": [300, 390, 257, 688, 295, 264, 2503, 295, 4565, 296, 13, 407, 498, 291, 434, 3102, 294, 565, 2638, 1507, 11], "temperature": 0.0, "avg_logprob": -0.21309491937810723, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.66594507190166e-06}, {"id": 466, "seek": 311064, "start": 3110.64, "end": 3122.0, "text": " you'll find a lot of time series stuff in pandas.", "tokens": [291, 603, 915, 257, 688, 295, 565, 2638, 1507, 294, 4565, 296, 13], "temperature": 0.0, "avg_logprob": -0.11952065802239753, "compression_ratio": 1.6020408163265305, "no_speech_prob": 6.339153969747713e-06}, {"id": 467, "seek": 311064, "start": 3122.0, "end": 3127.8799999999997, "text": " One helpful parameter that sits inside a lot of methods is inplace equals true. That means", "tokens": [1485, 4961, 13075, 300, 12696, 1854, 257, 688, 295, 7150, 307, 294, 6742, 6915, 2074, 13, 663, 1355], "temperature": 0.0, "avg_logprob": -0.11952065802239753, "compression_ratio": 1.6020408163265305, "no_speech_prob": 6.339153969747713e-06}, {"id": 468, "seek": 311064, "start": 3127.8799999999997, "end": 3133.3199999999997, "text": " that rather than returning a new data frame with this change made, it changes the data", "tokens": [300, 2831, 813, 12678, 257, 777, 1412, 3920, 365, 341, 1319, 1027, 11, 309, 2962, 264, 1412], "temperature": 0.0, "avg_logprob": -0.11952065802239753, "compression_ratio": 1.6020408163265305, "no_speech_prob": 6.339153969747713e-06}, {"id": 469, "seek": 311064, "start": 3133.3199999999997, "end": 3137.64, "text": " frame you already have. And when your data frames are quite big, this is going to save", "tokens": [3920, 291, 1217, 362, 13, 400, 562, 428, 1412, 12083, 366, 1596, 955, 11, 341, 307, 516, 281, 3155], "temperature": 0.0, "avg_logprob": -0.11952065802239753, "compression_ratio": 1.6020408163265305, "no_speech_prob": 6.339153969747713e-06}, {"id": 470, "seek": 313764, "start": 3137.64, "end": 3143.16, "text": " a lot of time and memory. That's a good little trick to know about.", "tokens": [257, 688, 295, 565, 293, 4675, 13, 663, 311, 257, 665, 707, 4282, 281, 458, 466, 13], "temperature": 0.0, "avg_logprob": -0.13702382307786207, "compression_ratio": 1.5460122699386503, "no_speech_prob": 7.183196430560201e-06}, {"id": 471, "seek": 313764, "start": 3143.16, "end": 3148.3199999999997, "text": " So now we merge all these together and we can now see that we've got all these after-school", "tokens": [407, 586, 321, 22183, 439, 613, 1214, 293, 321, 393, 586, 536, 300, 321, 600, 658, 439, 613, 934, 12, 22779], "temperature": 0.0, "avg_logprob": -0.13702382307786207, "compression_ratio": 1.5460122699386503, "no_speech_prob": 7.183196430560201e-06}, {"id": 472, "seek": 313764, "start": 3148.3199999999997, "end": 3158.52, "text": " holiday, before-school holiday, and our backward and forward running means. And then we join", "tokens": [9960, 11, 949, 12, 22779, 9960, 11, 293, 527, 23897, 293, 2128, 2614, 1355, 13, 400, 550, 321, 3917], "temperature": 0.0, "avg_logprob": -0.13702382307786207, "compression_ratio": 1.5460122699386503, "no_speech_prob": 7.183196430560201e-06}, {"id": 473, "seek": 315852, "start": 3158.52, "end": 3169.44, "text": " that up to our original data frame, and here we have our final result. So there it is.", "tokens": [300, 493, 281, 527, 3380, 1412, 3920, 11, 293, 510, 321, 362, 527, 2572, 1874, 13, 407, 456, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.12261258810758591, "compression_ratio": 1.5406976744186047, "no_speech_prob": 4.9369373300578445e-06}, {"id": 474, "seek": 315852, "start": 3169.44, "end": 3174.24, "text": " We started out with a pretty small set of fields in the training set, but we've done", "tokens": [492, 1409, 484, 365, 257, 1238, 1359, 992, 295, 7909, 294, 264, 3097, 992, 11, 457, 321, 600, 1096], "temperature": 0.0, "avg_logprob": -0.12261258810758591, "compression_ratio": 1.5406976744186047, "no_speech_prob": 4.9369373300578445e-06}, {"id": 475, "seek": 315852, "start": 3174.24, "end": 3182.28, "text": " this feature engineering. This feature engineering is not arbitrary. Although I didn't create", "tokens": [341, 4111, 7043, 13, 639, 4111, 7043, 307, 406, 23211, 13, 5780, 286, 994, 380, 1884], "temperature": 0.0, "avg_logprob": -0.12261258810758591, "compression_ratio": 1.5406976744186047, "no_speech_prob": 4.9369373300578445e-06}, {"id": 476, "seek": 318228, "start": 3182.28, "end": 3188.6400000000003, "text": " this solution, I was just re-implementing the solution that came from the competition", "tokens": [341, 3827, 11, 286, 390, 445, 319, 12, 332, 43704, 278, 264, 3827, 300, 1361, 490, 264, 6211], "temperature": 0.0, "avg_logprob": -0.19310970828957755, "compression_ratio": 1.4805825242718447, "no_speech_prob": 8.93962806003401e-06}, {"id": 477, "seek": 318228, "start": 3188.6400000000003, "end": 3196.1200000000003, "text": " 3rd place getters. This is nearly exactly the set of feature engineering steps I would", "tokens": [805, 7800, 1081, 483, 1559, 13, 639, 307, 6217, 2293, 264, 992, 295, 4111, 7043, 4439, 286, 576], "temperature": 0.0, "avg_logprob": -0.19310970828957755, "compression_ratio": 1.4805825242718447, "no_speech_prob": 8.93962806003401e-06}, {"id": 478, "seek": 318228, "start": 3196.1200000000003, "end": 3201.92, "text": " have done. It's just a really standard way of thinking about a time series. So you can", "tokens": [362, 1096, 13, 467, 311, 445, 257, 534, 3832, 636, 295, 1953, 466, 257, 565, 2638, 13, 407, 291, 393], "temperature": 0.0, "avg_logprob": -0.19310970828957755, "compression_ratio": 1.4805825242718447, "no_speech_prob": 8.93962806003401e-06}, {"id": 479, "seek": 318228, "start": 3201.92, "end": 3212.2400000000002, "text": " definitely borrow these ideas pretty closely.", "tokens": [2138, 11172, 613, 3487, 1238, 8185, 13], "temperature": 0.0, "avg_logprob": -0.19310970828957755, "compression_ratio": 1.4805825242718447, "no_speech_prob": 8.93962806003401e-06}, {"id": 480, "seek": 321224, "start": 3212.24, "end": 3223.8799999999997, "text": " So now that we've got that, we've got this table, we've done our feature engineering,", "tokens": [407, 586, 300, 321, 600, 658, 300, 11, 321, 600, 658, 341, 3199, 11, 321, 600, 1096, 527, 4111, 7043, 11], "temperature": 0.0, "avg_logprob": -0.11683047307680731, "compression_ratio": 1.7777777777777777, "no_speech_prob": 9.81821085588308e-06}, {"id": 481, "seek": 321224, "start": 3223.8799999999997, "end": 3230.2, "text": " we now want to feed it into a neural network. So to feed it into a neural network, we have", "tokens": [321, 586, 528, 281, 3154, 309, 666, 257, 18161, 3209, 13, 407, 281, 3154, 309, 666, 257, 18161, 3209, 11, 321, 362], "temperature": 0.0, "avg_logprob": -0.11683047307680731, "compression_ratio": 1.7777777777777777, "no_speech_prob": 9.81821085588308e-06}, {"id": 482, "seek": 321224, "start": 3230.2, "end": 3239.04, "text": " to do a few things. The categorical variables have to be turned into one-hot encoded variables,", "tokens": [281, 360, 257, 1326, 721, 13, 440, 19250, 804, 9102, 362, 281, 312, 3574, 666, 472, 12, 12194, 2058, 12340, 9102, 11], "temperature": 0.0, "avg_logprob": -0.11683047307680731, "compression_ratio": 1.7777777777777777, "no_speech_prob": 9.81821085588308e-06}, {"id": 483, "seek": 323904, "start": 3239.04, "end": 3245.64, "text": " or at least into contiguous integers. And the continuous variables we probably want", "tokens": [420, 412, 1935, 666, 660, 30525, 41674, 13, 400, 264, 10957, 9102, 321, 1391, 528], "temperature": 0.0, "avg_logprob": -0.19566757296338494, "compression_ratio": 1.5048543689320388, "no_speech_prob": 1.5446152247022837e-05}, {"id": 484, "seek": 323904, "start": 3245.64, "end": 3249.92, "text": " to normalize to a zero-mean one-statter deviation.", "tokens": [281, 2710, 1125, 281, 257, 4018, 12, 1398, 282, 472, 12, 372, 1161, 25163, 13], "temperature": 0.0, "avg_logprob": -0.19566757296338494, "compression_ratio": 1.5048543689320388, "no_speech_prob": 1.5446152247022837e-05}, {"id": 485, "seek": 323904, "start": 3249.92, "end": 3257.2799999999997, "text": " There's a very little-known package called sklearnpandas, and actually I contributed", "tokens": [821, 311, 257, 588, 707, 12, 6861, 7372, 1219, 1110, 306, 1083, 79, 474, 296, 11, 293, 767, 286, 18434], "temperature": 0.0, "avg_logprob": -0.19566757296338494, "compression_ratio": 1.5048543689320388, "no_speech_prob": 1.5446152247022837e-05}, {"id": 486, "seek": 323904, "start": 3257.2799999999997, "end": 3262.72, "text": " some new stuff to it for this course to make this even easier to use. If you use this data", "tokens": [512, 777, 1507, 281, 309, 337, 341, 1164, 281, 652, 341, 754, 3571, 281, 764, 13, 759, 291, 764, 341, 1412], "temperature": 0.0, "avg_logprob": -0.19566757296338494, "compression_ratio": 1.5048543689320388, "no_speech_prob": 1.5446152247022837e-05}, {"id": 487, "seek": 326272, "start": 3262.72, "end": 3269.2799999999997, "text": " frame mapper from sklearnpandas, as you'll see, it makes life very easy. Without it,", "tokens": [3920, 463, 3717, 490, 1110, 306, 1083, 79, 474, 296, 11, 382, 291, 603, 536, 11, 309, 1669, 993, 588, 1858, 13, 9129, 309, 11], "temperature": 0.0, "avg_logprob": -0.1254872439200418, "compression_ratio": 1.7578125, "no_speech_prob": 6.144089184090262e-06}, {"id": 488, "seek": 326272, "start": 3269.2799999999997, "end": 3273.56, "text": " life is very hard. And because very few people know about it, the vast majority of code you'll", "tokens": [993, 307, 588, 1152, 13, 400, 570, 588, 1326, 561, 458, 466, 309, 11, 264, 8369, 6286, 295, 3089, 291, 603], "temperature": 0.0, "avg_logprob": -0.1254872439200418, "compression_ratio": 1.7578125, "no_speech_prob": 6.144089184090262e-06}, {"id": 489, "seek": 326272, "start": 3273.56, "end": 3282.9199999999996, "text": " find on the Internet makes life look very hard. So use this code, not the other code.", "tokens": [915, 322, 264, 7703, 1669, 993, 574, 588, 1152, 13, 407, 764, 341, 3089, 11, 406, 264, 661, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1254872439200418, "compression_ratio": 1.7578125, "no_speech_prob": 6.144089184090262e-06}, {"id": 490, "seek": 326272, "start": 3282.9199999999996, "end": 3285.64, "text": " Actually I was talking to some of the students the other day and they were saying for their", "tokens": [5135, 286, 390, 1417, 281, 512, 295, 264, 1731, 264, 661, 786, 293, 436, 645, 1566, 337, 641], "temperature": 0.0, "avg_logprob": -0.1254872439200418, "compression_ratio": 1.7578125, "no_speech_prob": 6.144089184090262e-06}, {"id": 491, "seek": 326272, "start": 3285.64, "end": 3290.64, "text": " project they were stealing lots of code from part 1 of the course because they just couldn't", "tokens": [1716, 436, 645, 19757, 3195, 295, 3089, 490, 644, 502, 295, 264, 1164, 570, 436, 445, 2809, 380], "temperature": 0.0, "avg_logprob": -0.1254872439200418, "compression_ratio": 1.7578125, "no_speech_prob": 6.144089184090262e-06}, {"id": 492, "seek": 329064, "start": 3290.64, "end": 3298.08, "text": " find anywhere else people writing any of the kinds of code that we've used.", "tokens": [915, 4992, 1646, 561, 3579, 604, 295, 264, 3685, 295, 3089, 300, 321, 600, 1143, 13], "temperature": 0.0, "avg_logprob": -0.14133329029324687, "compression_ratio": 1.6318407960199004, "no_speech_prob": 9.223340384778567e-06}, {"id": 493, "seek": 329064, "start": 3298.08, "end": 3304.24, "text": " The stuff that we've learned throughout this course is on the whole not code that lives", "tokens": [440, 1507, 300, 321, 600, 3264, 3710, 341, 1164, 307, 322, 264, 1379, 406, 3089, 300, 2909], "temperature": 0.0, "avg_logprob": -0.14133329029324687, "compression_ratio": 1.6318407960199004, "no_speech_prob": 9.223340384778567e-06}, {"id": 494, "seek": 329064, "start": 3304.24, "end": 3309.68, "text": " elsewhere very much at all. So feel free to use a lot of these functions in your own work,", "tokens": [14517, 588, 709, 412, 439, 13, 407, 841, 1737, 281, 764, 257, 688, 295, 613, 6828, 294, 428, 1065, 589, 11], "temperature": 0.0, "avg_logprob": -0.14133329029324687, "compression_ratio": 1.6318407960199004, "no_speech_prob": 9.223340384778567e-06}, {"id": 495, "seek": 329064, "start": 3309.68, "end": 3315.7999999999997, "text": " because I've really tried to make them the best version of that function.", "tokens": [570, 286, 600, 534, 3031, 281, 652, 552, 264, 1151, 3037, 295, 300, 2445, 13], "temperature": 0.0, "avg_logprob": -0.14133329029324687, "compression_ratio": 1.6318407960199004, "no_speech_prob": 9.223340384778567e-06}, {"id": 496, "seek": 331580, "start": 3315.8, "end": 3321.92, "text": " So one way to do the embeddings and the way that they did it in the paper is to basically", "tokens": [407, 472, 636, 281, 360, 264, 12240, 29432, 293, 264, 636, 300, 436, 630, 309, 294, 264, 3035, 307, 281, 1936], "temperature": 0.0, "avg_logprob": -0.14342110331465557, "compression_ratio": 1.6948356807511737, "no_speech_prob": 2.1443735022330657e-05}, {"id": 497, "seek": 331580, "start": 3321.92, "end": 3327.92, "text": " say for each categorical variable they just manually decided what embedding dimensionality", "tokens": [584, 337, 1184, 19250, 804, 7006, 436, 445, 16945, 3047, 437, 12240, 3584, 10139, 1860], "temperature": 0.0, "avg_logprob": -0.14342110331465557, "compression_ratio": 1.6948356807511737, "no_speech_prob": 2.1443735022330657e-05}, {"id": 498, "seek": 331580, "start": 3327.92, "end": 3333.04, "text": " to use. They don't say in the paper how they picked these dimensionalities. But generally", "tokens": [281, 764, 13, 814, 500, 380, 584, 294, 264, 3035, 577, 436, 6183, 613, 10139, 16110, 13, 583, 5101], "temperature": 0.0, "avg_logprob": -0.14342110331465557, "compression_ratio": 1.6948356807511737, "no_speech_prob": 2.1443735022330657e-05}, {"id": 499, "seek": 331580, "start": 3333.04, "end": 3339.6800000000003, "text": " speaking things with a larger number of separate levels tend to have more dimensions. So I", "tokens": [4124, 721, 365, 257, 4833, 1230, 295, 4994, 4358, 3928, 281, 362, 544, 12819, 13, 407, 286], "temperature": 0.0, "avg_logprob": -0.14342110331465557, "compression_ratio": 1.6948356807511737, "no_speech_prob": 2.1443735022330657e-05}, {"id": 500, "seek": 333968, "start": 3339.68, "end": 3346.08, "text": " think there's like 1000 stores, so that has a big embedding dimensionality. Where else", "tokens": [519, 456, 311, 411, 9714, 9512, 11, 370, 300, 575, 257, 955, 12240, 3584, 10139, 1860, 13, 2305, 1646], "temperature": 0.0, "avg_logprob": -0.18941202829050463, "compression_ratio": 1.6053811659192825, "no_speech_prob": 5.338104983820813e-06}, {"id": 501, "seek": 333968, "start": 3346.08, "end": 3352.9199999999996, "text": " obviously things like promo, forward and backward, or day of week or whatever have much smaller", "tokens": [2745, 721, 411, 26750, 11, 2128, 293, 23897, 11, 420, 786, 295, 1243, 420, 2035, 362, 709, 4356], "temperature": 0.0, "avg_logprob": -0.18941202829050463, "compression_ratio": 1.6053811659192825, "no_speech_prob": 5.338104983820813e-06}, {"id": 502, "seek": 333968, "start": 3352.9199999999996, "end": 3355.0, "text": " ones.", "tokens": [2306, 13], "temperature": 0.0, "avg_logprob": -0.18941202829050463, "compression_ratio": 1.6053811659192825, "no_speech_prob": 5.338104983820813e-06}, {"id": 503, "seek": 333968, "start": 3355.0, "end": 3358.7599999999998, "text": " So this is this dictionary I created that basically goes from the name of the field", "tokens": [407, 341, 307, 341, 25890, 286, 2942, 300, 1936, 1709, 490, 264, 1315, 295, 264, 2519], "temperature": 0.0, "avg_logprob": -0.18941202829050463, "compression_ratio": 1.6053811659192825, "no_speech_prob": 5.338104983820813e-06}, {"id": 504, "seek": 333968, "start": 3358.7599999999998, "end": 3362.12, "text": " to the embedding dimensionality. Again this is all code that you guys can use in your", "tokens": [281, 264, 12240, 3584, 10139, 1860, 13, 3764, 341, 307, 439, 3089, 300, 291, 1074, 393, 764, 294, 428], "temperature": 0.0, "avg_logprob": -0.18941202829050463, "compression_ratio": 1.6053811659192825, "no_speech_prob": 5.338104983820813e-06}, {"id": 505, "seek": 336212, "start": 3362.12, "end": 3373.2799999999997, "text": " models. So then all I do is I say, my categorical variables is go through my dictionary, sort", "tokens": [5245, 13, 407, 550, 439, 286, 360, 307, 286, 584, 11, 452, 19250, 804, 9102, 307, 352, 807, 452, 25890, 11, 1333], "temperature": 0.0, "avg_logprob": -0.14014773735633262, "compression_ratio": 1.5759493670886076, "no_speech_prob": 3.6688186355604557e-06}, {"id": 506, "seek": 336212, "start": 3373.2799999999997, "end": 3381.0, "text": " it in reverse order of the value, and then get the first thing from that. So that's going", "tokens": [309, 294, 9943, 1668, 295, 264, 2158, 11, 293, 550, 483, 264, 700, 551, 490, 300, 13, 407, 300, 311, 516], "temperature": 0.0, "avg_logprob": -0.14014773735633262, "compression_ratio": 1.5759493670886076, "no_speech_prob": 3.6688186355604557e-06}, {"id": 507, "seek": 336212, "start": 3381.0, "end": 3390.7599999999998, "text": " to give me the keys from this in reverse order of dimensionality.", "tokens": [281, 976, 385, 264, 9317, 490, 341, 294, 9943, 1668, 295, 10139, 1860, 13], "temperature": 0.0, "avg_logprob": -0.14014773735633262, "compression_ratio": 1.5759493670886076, "no_speech_prob": 3.6688186355604557e-06}, {"id": 508, "seek": 339076, "start": 3390.76, "end": 3399.28, "text": " Continuous variables, it's just a list. Just make sure that there's no nulls. So continuous", "tokens": [14674, 12549, 9102, 11, 309, 311, 445, 257, 1329, 13, 1449, 652, 988, 300, 456, 311, 572, 18184, 82, 13, 407, 10957], "temperature": 0.0, "avg_logprob": -0.1685631615774972, "compression_ratio": 1.8316326530612246, "no_speech_prob": 1.5446088582393713e-05}, {"id": 509, "seek": 339076, "start": 3399.28, "end": 3404.46, "text": " variables replace nulls with zeros. Categorical variables replace nulls with empties. And", "tokens": [9102, 7406, 18184, 82, 365, 35193, 13, 383, 2968, 284, 804, 9102, 7406, 18184, 82, 365, 6113, 530, 13, 400], "temperature": 0.0, "avg_logprob": -0.1685631615774972, "compression_ratio": 1.8316326530612246, "no_speech_prob": 1.5446088582393713e-05}, {"id": 510, "seek": 339076, "start": 3404.46, "end": 3412.5200000000004, "text": " then here's where we use the data frame mapper. A data frame mapper takes a list of tuples,", "tokens": [550, 510, 311, 689, 321, 764, 264, 1412, 3920, 463, 3717, 13, 316, 1412, 3920, 463, 3717, 2516, 257, 1329, 295, 2604, 2622, 11], "temperature": 0.0, "avg_logprob": -0.1685631615774972, "compression_ratio": 1.8316326530612246, "no_speech_prob": 1.5446088582393713e-05}, {"id": 511, "seek": 339076, "start": 3412.5200000000004, "end": 3417.4, "text": " a list of tuples with just 2 items in. The first item is the name of the variable. So", "tokens": [257, 1329, 295, 2604, 2622, 365, 445, 568, 4754, 294, 13, 440, 700, 3174, 307, 264, 1315, 295, 264, 7006, 13, 407], "temperature": 0.0, "avg_logprob": -0.1685631615774972, "compression_ratio": 1.8316326530612246, "no_speech_prob": 1.5446088582393713e-05}, {"id": 512, "seek": 341740, "start": 3417.4, "end": 3420.84, "text": " in this case I'm looping through each categorical variable name.", "tokens": [294, 341, 1389, 286, 478, 6367, 278, 807, 1184, 19250, 804, 7006, 1315, 13], "temperature": 0.0, "avg_logprob": -0.13351517803264115, "compression_ratio": 1.6746031746031746, "no_speech_prob": 3.9669776015216485e-06}, {"id": 513, "seek": 341740, "start": 3420.84, "end": 3427.12, "text": " The second thing in the tuple is a class, or actually an instance of a class, which", "tokens": [440, 1150, 551, 294, 264, 2604, 781, 307, 257, 1508, 11, 420, 767, 364, 5197, 295, 257, 1508, 11, 597], "temperature": 0.0, "avg_logprob": -0.13351517803264115, "compression_ratio": 1.6746031746031746, "no_speech_prob": 3.9669776015216485e-06}, {"id": 514, "seek": 341740, "start": 3427.12, "end": 3432.92, "text": " is going to do your preprocessing. And there's really just 2 that you're going to use almost", "tokens": [307, 516, 281, 360, 428, 2666, 340, 780, 278, 13, 400, 456, 311, 534, 445, 568, 300, 291, 434, 516, 281, 764, 1920], "temperature": 0.0, "avg_logprob": -0.13351517803264115, "compression_ratio": 1.6746031746031746, "no_speech_prob": 3.9669776015216485e-06}, {"id": 515, "seek": 341740, "start": 3432.92, "end": 3440.76, "text": " all the time. For categorical variables, sklearn comes with something called labelencoder.", "tokens": [439, 264, 565, 13, 1171, 19250, 804, 9102, 11, 1110, 306, 1083, 1487, 365, 746, 1219, 7645, 22660, 19866, 13], "temperature": 0.0, "avg_logprob": -0.13351517803264115, "compression_ratio": 1.6746031746031746, "no_speech_prob": 3.9669776015216485e-06}, {"id": 516, "seek": 341740, "start": 3440.76, "end": 3447.08, "text": " It's really badly documented, in fact really misleadingly documented. But this is exactly", "tokens": [467, 311, 534, 13425, 23007, 11, 294, 1186, 534, 36429, 356, 23007, 13, 583, 341, 307, 2293], "temperature": 0.0, "avg_logprob": -0.13351517803264115, "compression_ratio": 1.6746031746031746, "no_speech_prob": 3.9669776015216485e-06}, {"id": 517, "seek": 344708, "start": 3447.08, "end": 3452.2, "text": " what you want. It's something that takes a column, figures out what are all the unique", "tokens": [437, 291, 528, 13, 467, 311, 746, 300, 2516, 257, 7738, 11, 9624, 484, 437, 366, 439, 264, 3845], "temperature": 0.0, "avg_logprob": -0.11411504234586443, "compression_ratio": 1.6776556776556777, "no_speech_prob": 6.2408648773271125e-06}, {"id": 518, "seek": 344708, "start": 3452.2, "end": 3457.7999999999997, "text": " values that appear in that column, and replaces them with a set of contiguous integers. So", "tokens": [4190, 300, 4204, 294, 300, 7738, 11, 293, 46734, 552, 365, 257, 992, 295, 660, 30525, 41674, 13, 407], "temperature": 0.0, "avg_logprob": -0.11411504234586443, "compression_ratio": 1.6776556776556777, "no_speech_prob": 6.2408648773271125e-06}, {"id": 519, "seek": 344708, "start": 3457.7999999999997, "end": 3463.48, "text": " if you've got the days of the week, Monday through Sunday, it'll replace them with zeros", "tokens": [498, 291, 600, 658, 264, 1708, 295, 264, 1243, 11, 8138, 807, 7776, 11, 309, 603, 7406, 552, 365, 35193], "temperature": 0.0, "avg_logprob": -0.11411504234586443, "compression_ratio": 1.6776556776556777, "no_speech_prob": 6.2408648773271125e-06}, {"id": 520, "seek": 344708, "start": 3463.48, "end": 3465.36, "text": " through sevens.", "tokens": [807, 3407, 82, 13], "temperature": 0.0, "avg_logprob": -0.11411504234586443, "compression_ratio": 1.6776556776556777, "no_speech_prob": 6.2408648773271125e-06}, {"id": 521, "seek": 344708, "start": 3465.36, "end": 3470.56, "text": " And then very importantly, this is critically important, you need to make sure that the", "tokens": [400, 550, 588, 8906, 11, 341, 307, 22797, 1021, 11, 291, 643, 281, 652, 988, 300, 264], "temperature": 0.0, "avg_logprob": -0.11411504234586443, "compression_ratio": 1.6776556776556777, "no_speech_prob": 6.2408648773271125e-06}, {"id": 522, "seek": 344708, "start": 3470.56, "end": 3474.92, "text": " training set and the test set have the same codes. There's no point in having Sunday be", "tokens": [3097, 992, 293, 264, 1500, 992, 362, 264, 912, 14211, 13, 821, 311, 572, 935, 294, 1419, 7776, 312], "temperature": 0.0, "avg_logprob": -0.11411504234586443, "compression_ratio": 1.6776556776556777, "no_speech_prob": 6.2408648773271125e-06}, {"id": 523, "seek": 347492, "start": 3474.92, "end": 3481.56, "text": " zero in the training set and one in the test set. So because we're actually instantiating", "tokens": [4018, 294, 264, 3097, 992, 293, 472, 294, 264, 1500, 992, 13, 407, 570, 321, 434, 767, 9836, 72, 990], "temperature": 0.0, "avg_logprob": -0.1412789991923741, "compression_ratio": 1.6996197718631179, "no_speech_prob": 2.5215613277396187e-06}, {"id": 524, "seek": 347492, "start": 3481.56, "end": 3488.88, "text": " this class here, this object is going to actually keep track of which codes it's using. And", "tokens": [341, 1508, 510, 11, 341, 2657, 307, 516, 281, 767, 1066, 2837, 295, 597, 14211, 309, 311, 1228, 13, 400], "temperature": 0.0, "avg_logprob": -0.1412789991923741, "compression_ratio": 1.6996197718631179, "no_speech_prob": 2.5215613277396187e-06}, {"id": 525, "seek": 347492, "start": 3488.88, "end": 3495.44, "text": " then ditto for the continuous, we want to normalize them to a 0, 1 variable. But again", "tokens": [550, 274, 34924, 337, 264, 10957, 11, 321, 528, 281, 2710, 1125, 552, 281, 257, 1958, 11, 502, 7006, 13, 583, 797], "temperature": 0.0, "avg_logprob": -0.1412789991923741, "compression_ratio": 1.6996197718631179, "no_speech_prob": 2.5215613277396187e-06}, {"id": 526, "seek": 347492, "start": 3495.44, "end": 3499.56, "text": " we need to remember what was the mean that we subtracted, what was the standard deviation", "tokens": [321, 643, 281, 1604, 437, 390, 264, 914, 300, 321, 16390, 292, 11, 437, 390, 264, 3832, 25163], "temperature": 0.0, "avg_logprob": -0.1412789991923741, "compression_ratio": 1.6996197718631179, "no_speech_prob": 2.5215613277396187e-06}, {"id": 527, "seek": 347492, "start": 3499.56, "end": 3504.04, "text": " we divided by, so that we can do exactly the same thing to the test set. Otherwise again", "tokens": [321, 6666, 538, 11, 370, 300, 321, 393, 360, 2293, 264, 912, 551, 281, 264, 1500, 992, 13, 10328, 797], "temperature": 0.0, "avg_logprob": -0.1412789991923741, "compression_ratio": 1.6996197718631179, "no_speech_prob": 2.5215613277396187e-06}, {"id": 528, "seek": 350404, "start": 3504.04, "end": 3507.88, "text": " our models are going to be nearly totally useless. So the way the data frame mapper", "tokens": [527, 5245, 366, 516, 281, 312, 6217, 3879, 14115, 13, 407, 264, 636, 264, 1412, 3920, 463, 3717], "temperature": 0.0, "avg_logprob": -0.15318641455277152, "compression_ratio": 1.6904761904761905, "no_speech_prob": 9.818239959713537e-06}, {"id": 529, "seek": 350404, "start": 3507.88, "end": 3513.36, "text": " works is that it's using this instantiated object, it's going to keep track of this information.", "tokens": [1985, 307, 300, 309, 311, 1228, 341, 9836, 72, 770, 2657, 11, 309, 311, 516, 281, 1066, 2837, 295, 341, 1589, 13], "temperature": 0.0, "avg_logprob": -0.15318641455277152, "compression_ratio": 1.6904761904761905, "no_speech_prob": 9.818239959713537e-06}, {"id": 530, "seek": 350404, "start": 3513.36, "end": 3518.64, "text": " So this is basically code you can copy and paste in every one of your models. Once we've", "tokens": [407, 341, 307, 1936, 3089, 291, 393, 5055, 293, 9163, 294, 633, 472, 295, 428, 5245, 13, 3443, 321, 600], "temperature": 0.0, "avg_logprob": -0.15318641455277152, "compression_ratio": 1.6904761904761905, "no_speech_prob": 9.818239959713537e-06}, {"id": 531, "seek": 350404, "start": 3518.64, "end": 3526.84, "text": " got those mappings, you just pass those to a data frame mapper, and then you call.fit", "tokens": [658, 729, 463, 28968, 11, 291, 445, 1320, 729, 281, 257, 1412, 3920, 463, 3717, 11, 293, 550, 291, 818, 2411, 6845], "temperature": 0.0, "avg_logprob": -0.15318641455277152, "compression_ratio": 1.6904761904761905, "no_speech_prob": 9.818239959713537e-06}, {"id": 532, "seek": 352684, "start": 3526.84, "end": 3536.44, "text": " passing in your data set. So this thing now is a special object which has a.features", "tokens": [8437, 294, 428, 1412, 992, 13, 407, 341, 551, 586, 307, 257, 2121, 2657, 597, 575, 257, 2411, 2106, 3377], "temperature": 0.0, "avg_logprob": -0.19992753149757922, "compression_ratio": 1.6628571428571428, "no_speech_prob": 6.24087397227413e-06}, {"id": 533, "seek": 352684, "start": 3536.44, "end": 3542.04, "text": " property that's going to contain all of the features, all of the pre-processed features", "tokens": [4707, 300, 311, 516, 281, 5304, 439, 295, 264, 4122, 11, 439, 295, 264, 659, 12, 41075, 292, 4122], "temperature": 0.0, "avg_logprob": -0.19992753149757922, "compression_ratio": 1.6628571428571428, "no_speech_prob": 6.24087397227413e-06}, {"id": 534, "seek": 352684, "start": 3542.04, "end": 3548.76, "text": " that you want. So categorical columns contains the result of doing this mapping, basically", "tokens": [300, 291, 528, 13, 407, 19250, 804, 13766, 8306, 264, 1874, 295, 884, 341, 18350, 11, 1936], "temperature": 0.0, "avg_logprob": -0.19992753149757922, "compression_ratio": 1.6628571428571428, "no_speech_prob": 6.24087397227413e-06}, {"id": 535, "seek": 352684, "start": 3548.76, "end": 3551.96, "text": " doing this labeling coding.", "tokens": [884, 341, 40244, 17720, 13], "temperature": 0.0, "avg_logprob": -0.19992753149757922, "compression_ratio": 1.6628571428571428, "no_speech_prob": 6.24087397227413e-06}, {"id": 536, "seek": 355196, "start": 3551.96, "end": 3557.44, "text": " So in some ways the details of how this works doesn't matter too much because you can just", "tokens": [407, 294, 512, 2098, 264, 4365, 295, 577, 341, 1985, 1177, 380, 1871, 886, 709, 570, 291, 393, 445], "temperature": 0.0, "avg_logprob": -0.20205600782372485, "compression_ratio": 1.628440366972477, "no_speech_prob": 6.643383585469564e-06}, {"id": 537, "seek": 355196, "start": 3557.44, "end": 3562.88, "text": " use exactly this code in every one of your models. Same for continuous, it's exactly", "tokens": [764, 2293, 341, 3089, 294, 633, 472, 295, 428, 5245, 13, 10635, 337, 10957, 11, 309, 311, 2293], "temperature": 0.0, "avg_logprob": -0.20205600782372485, "compression_ratio": 1.628440366972477, "no_speech_prob": 6.643383585469564e-06}, {"id": 538, "seek": 355196, "start": 3562.88, "end": 3568.64, "text": " the same code. But of course for continuous, it's going to be using standard scalar, which", "tokens": [264, 912, 3089, 13, 583, 295, 1164, 337, 10957, 11, 309, 311, 516, 281, 312, 1228, 3832, 39684, 11, 597], "temperature": 0.0, "avg_logprob": -0.20205600782372485, "compression_ratio": 1.628440366972477, "no_speech_prob": 6.643383585469564e-06}, {"id": 539, "seek": 355196, "start": 3568.64, "end": 3575.32, "text": " is the Skykit learn thing that turns it into a 0 mean, 1 standard deviation variable. So", "tokens": [307, 264, 9879, 22681, 1466, 551, 300, 4523, 309, 666, 257, 1958, 914, 11, 502, 3832, 25163, 7006, 13, 407], "temperature": 0.0, "avg_logprob": -0.20205600782372485, "compression_ratio": 1.628440366972477, "no_speech_prob": 6.643383585469564e-06}, {"id": 540, "seek": 357532, "start": 3575.32, "end": 3583.48, "text": " we've now got continuous columns that have all been standardized. Here's an example of", "tokens": [321, 600, 586, 658, 10957, 13766, 300, 362, 439, 668, 31677, 13, 1692, 311, 364, 1365, 295], "temperature": 0.0, "avg_logprob": -0.1877336653452071, "compression_ratio": 1.592814371257485, "no_speech_prob": 4.222809820930706e-06}, {"id": 541, "seek": 357532, "start": 3583.48, "end": 3593.0800000000004, "text": " the first 5 rows from the 0th column for a categorical and then ditto for a continuous.", "tokens": [264, 700, 1025, 13241, 490, 264, 1958, 392, 7738, 337, 257, 19250, 804, 293, 550, 274, 34924, 337, 257, 10957, 13], "temperature": 0.0, "avg_logprob": -0.1877336653452071, "compression_ratio": 1.592814371257485, "no_speech_prob": 4.222809820930706e-06}, {"id": 542, "seek": 357532, "start": 3593.0800000000004, "end": 3600.44, "text": " So you can see these have been turned into integers and these have been turned into numbers", "tokens": [407, 291, 393, 536, 613, 362, 668, 3574, 666, 41674, 293, 613, 362, 668, 3574, 666, 3547], "temperature": 0.0, "avg_logprob": -0.1877336653452071, "compression_ratio": 1.592814371257485, "no_speech_prob": 4.222809820930706e-06}, {"id": 543, "seek": 360044, "start": 3600.44, "end": 3605.44, "text": " which are going to average to 0 and have a standard deviation of 1.", "tokens": [597, 366, 516, 281, 4274, 281, 1958, 293, 362, 257, 3832, 25163, 295, 502, 13], "temperature": 0.0, "avg_logprob": -0.12956740742637998, "compression_ratio": 1.6, "no_speech_prob": 4.495164375839522e-06}, {"id": 544, "seek": 360044, "start": 3605.44, "end": 3611.96, "text": " One of the nice things about this data frame mapper is that you can now take that object", "tokens": [1485, 295, 264, 1481, 721, 466, 341, 1412, 3920, 463, 3717, 307, 300, 291, 393, 586, 747, 300, 2657], "temperature": 0.0, "avg_logprob": -0.12956740742637998, "compression_ratio": 1.6, "no_speech_prob": 4.495164375839522e-06}, {"id": 545, "seek": 360044, "start": 3611.96, "end": 3619.4, "text": " and actually store it, pickle it, and so now you can use those categorical encodings and", "tokens": [293, 767, 3531, 309, 11, 31433, 309, 11, 293, 370, 586, 291, 393, 764, 729, 19250, 804, 2058, 378, 1109, 293], "temperature": 0.0, "avg_logprob": -0.12956740742637998, "compression_ratio": 1.6, "no_speech_prob": 4.495164375839522e-06}, {"id": 546, "seek": 360044, "start": 3619.4, "end": 3630.2000000000003, "text": " scaling parameters elsewhere. By just unpickling it, you've immediately got those same parameters.", "tokens": [21589, 9834, 14517, 13, 3146, 445, 20994, 618, 1688, 309, 11, 291, 600, 4258, 658, 729, 912, 9834, 13], "temperature": 0.0, "avg_logprob": -0.12956740742637998, "compression_ratio": 1.6, "no_speech_prob": 4.495164375839522e-06}, {"id": 547, "seek": 363020, "start": 3630.2, "end": 3637.56, "text": " For my categorical variables, you can see here the number of unique classes in every", "tokens": [1171, 452, 19250, 804, 9102, 11, 291, 393, 536, 510, 264, 1230, 295, 3845, 5359, 294, 633], "temperature": 0.0, "avg_logprob": -0.16195733923661082, "compression_ratio": 1.5170454545454546, "no_speech_prob": 7.5279340308043174e-06}, {"id": 548, "seek": 363020, "start": 3637.56, "end": 3645.0, "text": " one. So here's my 1100 stores and 31 days of the month and 7 days of the week and so", "tokens": [472, 13, 407, 510, 311, 452, 2975, 628, 9512, 293, 10353, 1708, 295, 264, 1618, 293, 1614, 1708, 295, 264, 1243, 293, 370], "temperature": 0.0, "avg_logprob": -0.16195733923661082, "compression_ratio": 1.5170454545454546, "no_speech_prob": 7.5279340308043174e-06}, {"id": 549, "seek": 363020, "start": 3645.0, "end": 3649.0, "text": " forth.", "tokens": [5220, 13], "temperature": 0.0, "avg_logprob": -0.16195733923661082, "compression_ratio": 1.5170454545454546, "no_speech_prob": 7.5279340308043174e-06}, {"id": 550, "seek": 363020, "start": 3649.0, "end": 3659.08, "text": " So that's the kind of key preprocessing that has to be done. So here is their big mistake.", "tokens": [407, 300, 311, 264, 733, 295, 2141, 2666, 340, 780, 278, 300, 575, 281, 312, 1096, 13, 407, 510, 307, 641, 955, 6146, 13], "temperature": 0.0, "avg_logprob": -0.16195733923661082, "compression_ratio": 1.5170454545454546, "no_speech_prob": 7.5279340308043174e-06}, {"id": 551, "seek": 365908, "start": 3659.08, "end": 3663.88, "text": " And I think if they didn't do this big mistake, they probably would have won. Their big mistake", "tokens": [400, 286, 519, 498, 436, 994, 380, 360, 341, 955, 6146, 11, 436, 1391, 576, 362, 1582, 13, 6710, 955, 6146], "temperature": 0.0, "avg_logprob": -0.11723399488893274, "compression_ratio": 1.6, "no_speech_prob": 8.315227546518145e-07}, {"id": 552, "seek": 365908, "start": 3663.88, "end": 3673.0, "text": " is that they went join.sales not equal to 0. So they've removed all of the rows with", "tokens": [307, 300, 436, 1437, 3917, 13, 82, 4229, 406, 2681, 281, 1958, 13, 407, 436, 600, 7261, 439, 295, 264, 13241, 365], "temperature": 0.0, "avg_logprob": -0.11723399488893274, "compression_ratio": 1.6, "no_speech_prob": 8.315227546518145e-07}, {"id": 553, "seek": 365908, "start": 3673.0, "end": 3681.52, "text": " no sales. Those are all of the rows where the store was closed. Why was this a big mistake?", "tokens": [572, 5763, 13, 3950, 366, 439, 295, 264, 13241, 689, 264, 3531, 390, 5395, 13, 1545, 390, 341, 257, 955, 6146, 30], "temperature": 0.0, "avg_logprob": -0.11723399488893274, "compression_ratio": 1.6, "no_speech_prob": 8.315227546518145e-07}, {"id": 554, "seek": 368152, "start": 3681.52, "end": 3689.36, "text": " Because if you go to the Rosman Store Sales Competition website and click on Kernels and", "tokens": [1436, 498, 291, 352, 281, 264, 11144, 1601, 17242, 23467, 43634, 3144, 293, 2052, 322, 40224, 1625, 293], "temperature": 0.0, "avg_logprob": -0.18695782555474175, "compression_ratio": 1.313868613138686, "no_speech_prob": 5.682396476913709e-06}, {"id": 555, "seek": 368152, "start": 3689.36, "end": 3705.28, "text": " look at the kernel that got the highest rating, Exploratory Data Analysis Rosman, I'll show", "tokens": [574, 412, 264, 28256, 300, 658, 264, 6343, 10990, 11, 12514, 284, 4745, 11888, 38172, 11144, 1601, 11, 286, 603, 855], "temperature": 0.0, "avg_logprob": -0.18695782555474175, "compression_ratio": 1.313868613138686, "no_speech_prob": 5.682396476913709e-06}, {"id": 556, "seek": 370528, "start": 3705.28, "end": 3712.0, "text": " you a couple of pictures.", "tokens": [291, 257, 1916, 295, 5242, 13], "temperature": 0.0, "avg_logprob": -0.12999224081272032, "compression_ratio": 1.5634517766497462, "no_speech_prob": 1.9033744820262655e-06}, {"id": 557, "seek": 370528, "start": 3712.0, "end": 3718.5600000000004, "text": " Here is an example of a store, store 708, and these are all from this kernel. Here is", "tokens": [1692, 307, 364, 1365, 295, 257, 3531, 11, 3531, 5285, 23, 11, 293, 613, 366, 439, 490, 341, 28256, 13, 1692, 307], "temperature": 0.0, "avg_logprob": -0.12999224081272032, "compression_ratio": 1.5634517766497462, "no_speech_prob": 1.9033744820262655e-06}, {"id": 558, "seek": 370528, "start": 3718.5600000000004, "end": 3725.48, "text": " a period of time where it was closed to refurbishment. This happens a lot in Rosman stores. You get", "tokens": [257, 2896, 295, 565, 689, 309, 390, 5395, 281, 1895, 16659, 30273, 13, 639, 2314, 257, 688, 294, 11144, 1601, 9512, 13, 509, 483], "temperature": 0.0, "avg_logprob": -0.12999224081272032, "compression_ratio": 1.5634517766497462, "no_speech_prob": 1.9033744820262655e-06}, {"id": 559, "seek": 370528, "start": 3725.48, "end": 3731.92, "text": " these periods of time when you get zeros for sales, lots in a row. Look what happens immediately", "tokens": [613, 13804, 295, 565, 562, 291, 483, 35193, 337, 5763, 11, 3195, 294, 257, 5386, 13, 2053, 437, 2314, 4258], "temperature": 0.0, "avg_logprob": -0.12999224081272032, "compression_ratio": 1.5634517766497462, "no_speech_prob": 1.9033744820262655e-06}, {"id": 560, "seek": 373192, "start": 3731.92, "end": 3740.28, "text": " before and after. So in the data set that we're looking at, our unfortunate 3rd place", "tokens": [949, 293, 934, 13, 407, 294, 264, 1412, 992, 300, 321, 434, 1237, 412, 11, 527, 17843, 805, 7800, 1081], "temperature": 0.0, "avg_logprob": -0.18699554715837752, "compression_ratio": 1.4748603351955307, "no_speech_prob": 2.3687819066253724e-06}, {"id": 561, "seek": 373192, "start": 3740.28, "end": 3746.7200000000003, "text": " winners deleted all of these. So they had no ability to build a feature that could find", "tokens": [17193, 22981, 439, 295, 613, 13, 407, 436, 632, 572, 3485, 281, 1322, 257, 4111, 300, 727, 915], "temperature": 0.0, "avg_logprob": -0.18699554715837752, "compression_ratio": 1.4748603351955307, "no_speech_prob": 2.3687819066253724e-06}, {"id": 562, "seek": 373192, "start": 3746.7200000000003, "end": 3755.2400000000002, "text": " this. So this store 708. Look, here's another one where it was closed. Same thing. So this", "tokens": [341, 13, 407, 341, 3531, 5285, 23, 13, 2053, 11, 510, 311, 1071, 472, 689, 309, 390, 5395, 13, 10635, 551, 13, 407, 341], "temperature": 0.0, "avg_logprob": -0.18699554715837752, "compression_ratio": 1.4748603351955307, "no_speech_prob": 2.3687819066253724e-06}, {"id": 563, "seek": 375524, "start": 3755.24, "end": 3763.72, "text": " turns out to be super common. And the 2nd place winner actually built a feature. It's", "tokens": [4523, 484, 281, 312, 1687, 2689, 13, 400, 264, 568, 273, 1081, 8507, 767, 3094, 257, 4111, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.16871302127838134, "compression_ratio": 1.5472636815920398, "no_speech_prob": 3.187549282301916e-06}, {"id": 564, "seek": 375524, "start": 3763.72, "end": 3768.9199999999996, "text": " going to be exactly the same feature we've seen before. How many days since the closing", "tokens": [516, 281, 312, 2293, 264, 912, 4111, 321, 600, 1612, 949, 13, 1012, 867, 1708, 1670, 264, 10377], "temperature": 0.0, "avg_logprob": -0.16871302127838134, "compression_ratio": 1.5472636815920398, "no_speech_prob": 3.187549282301916e-06}, {"id": 565, "seek": 375524, "start": 3768.9199999999996, "end": 3773.2799999999997, "text": " and how many days until the next closing. If they had just done that, I'm pretty sure", "tokens": [293, 577, 867, 1708, 1826, 264, 958, 10377, 13, 759, 436, 632, 445, 1096, 300, 11, 286, 478, 1238, 988], "temperature": 0.0, "avg_logprob": -0.16871302127838134, "compression_ratio": 1.5472636815920398, "no_speech_prob": 3.187549282301916e-06}, {"id": 566, "seek": 375524, "start": 3773.2799999999997, "end": 3780.0, "text": " they would have won. So that was their big mistake.", "tokens": [436, 576, 362, 1582, 13, 407, 300, 390, 641, 955, 6146, 13], "temperature": 0.0, "avg_logprob": -0.16871302127838134, "compression_ratio": 1.5472636815920398, "no_speech_prob": 3.187549282301916e-06}, {"id": 567, "seek": 378000, "start": 3780.0, "end": 3788.44, "text": " This kernel has a number of interesting analyses in it. Here's another one which I think our", "tokens": [639, 28256, 575, 257, 1230, 295, 1880, 37560, 294, 309, 13, 1692, 311, 1071, 472, 597, 286, 519, 527], "temperature": 0.0, "avg_logprob": -0.11429058955265925, "compression_ratio": 1.4754098360655739, "no_speech_prob": 9.368629434902687e-06}, {"id": 568, "seek": 378000, "start": 3788.44, "end": 3797.08, "text": " neural net can capture, although it might have been better to be explicit. Some stores", "tokens": [18161, 2533, 393, 7983, 11, 4878, 309, 1062, 362, 668, 1101, 281, 312, 13691, 13, 2188, 9512], "temperature": 0.0, "avg_logprob": -0.11429058955265925, "compression_ratio": 1.4754098360655739, "no_speech_prob": 9.368629434902687e-06}, {"id": 569, "seek": 378000, "start": 3797.08, "end": 3805.08, "text": " opened on Sundays. Most didn't, but some did. And for those stores that opened on Sundays,", "tokens": [5625, 322, 44857, 13, 4534, 994, 380, 11, 457, 512, 630, 13, 400, 337, 729, 9512, 300, 5625, 322, 44857, 11], "temperature": 0.0, "avg_logprob": -0.11429058955265925, "compression_ratio": 1.4754098360655739, "no_speech_prob": 9.368629434902687e-06}, {"id": 570, "seek": 380508, "start": 3805.08, "end": 3810.12, "text": " their sales on Sundays were far higher than on any other day. I guess that's because in", "tokens": [641, 5763, 322, 44857, 645, 1400, 2946, 813, 322, 604, 661, 786, 13, 286, 2041, 300, 311, 570, 294], "temperature": 0.0, "avg_logprob": -0.14478843616989423, "compression_ratio": 1.6425855513307985, "no_speech_prob": 7.453723469552642e-07}, {"id": 571, "seek": 380508, "start": 3810.12, "end": 3815.68, "text": " Germany not many shops open on Sundays. So something else that they didn't explicitly", "tokens": [7244, 406, 867, 14457, 1269, 322, 44857, 13, 407, 746, 1646, 300, 436, 994, 380, 20803], "temperature": 0.0, "avg_logprob": -0.14478843616989423, "compression_ratio": 1.6425855513307985, "no_speech_prob": 7.453723469552642e-07}, {"id": 572, "seek": 380508, "start": 3815.68, "end": 3823.4, "text": " do was create a like, is store open on Sunday field. Having said that, I think the neural", "tokens": [360, 390, 1884, 257, 411, 11, 307, 3531, 1269, 322, 7776, 2519, 13, 10222, 848, 300, 11, 286, 519, 264, 18161], "temperature": 0.0, "avg_logprob": -0.14478843616989423, "compression_ratio": 1.6425855513307985, "no_speech_prob": 7.453723469552642e-07}, {"id": 573, "seek": 380508, "start": 3823.4, "end": 3828.16, "text": " net may have been able to put that in the embedding. So if you're interested during", "tokens": [2533, 815, 362, 668, 1075, 281, 829, 300, 294, 264, 12240, 3584, 13, 407, 498, 291, 434, 3102, 1830], "temperature": 0.0, "avg_logprob": -0.14478843616989423, "compression_ratio": 1.6425855513307985, "no_speech_prob": 7.453723469552642e-07}, {"id": 574, "seek": 380508, "start": 3828.16, "end": 3831.96, "text": " the week, you could try adding this field and see if it actually improves it or not.", "tokens": [264, 1243, 11, 291, 727, 853, 5127, 341, 2519, 293, 536, 498, 309, 767, 24771, 309, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.14478843616989423, "compression_ratio": 1.6425855513307985, "no_speech_prob": 7.453723469552642e-07}, {"id": 575, "seek": 383196, "start": 3831.96, "end": 3836.6, "text": " It'd certainly be interesting to hear if you try adding this field. Do you find that you", "tokens": [467, 1116, 3297, 312, 1880, 281, 1568, 498, 291, 853, 5127, 341, 2519, 13, 1144, 291, 915, 300, 291], "temperature": 0.0, "avg_logprob": -0.2170102620862194, "compression_ratio": 1.5614754098360655, "no_speech_prob": 5.682402388629271e-06}, {"id": 576, "seek": 383196, "start": 3836.6, "end": 3841.52, "text": " actually would win the competition?", "tokens": [767, 576, 1942, 264, 6211, 30], "temperature": 0.0, "avg_logprob": -0.2170102620862194, "compression_ratio": 1.5614754098360655, "no_speech_prob": 5.682402388629271e-06}, {"id": 577, "seek": 383196, "start": 3841.52, "end": 3847.68, "text": " This Sunday thing, these are all from the same Kaggle kernel. Here's the day of week,", "tokens": [639, 7776, 551, 11, 613, 366, 439, 490, 264, 912, 48751, 22631, 28256, 13, 1692, 311, 264, 786, 295, 1243, 11], "temperature": 0.0, "avg_logprob": -0.2170102620862194, "compression_ratio": 1.5614754098360655, "no_speech_prob": 5.682402388629271e-06}, {"id": 578, "seek": 383196, "start": 3847.68, "end": 3854.56, "text": " and here's the sales as a box plot. You can see normally on a Sunday, it's not that the", "tokens": [293, 510, 311, 264, 5763, 382, 257, 2424, 7542, 13, 509, 393, 536, 5646, 322, 257, 7776, 11, 309, 311, 406, 300, 264], "temperature": 0.0, "avg_logprob": -0.2170102620862194, "compression_ratio": 1.5614754098360655, "no_speech_prob": 5.682402388629271e-06}, {"id": 579, "seek": 383196, "start": 3854.56, "end": 3861.92, "text": " sales are much higher. So it's really explicitly just for these particular stores.", "tokens": [5763, 366, 709, 2946, 13, 407, 309, 311, 534, 20803, 445, 337, 613, 1729, 9512, 13], "temperature": 0.0, "avg_logprob": -0.2170102620862194, "compression_ratio": 1.5614754098360655, "no_speech_prob": 5.682402388629271e-06}, {"id": 580, "seek": 386192, "start": 3861.92, "end": 3870.7200000000003, "text": " So that's the kind of visualization stuff which is really helpful to do as you work", "tokens": [407, 300, 311, 264, 733, 295, 25801, 1507, 597, 307, 534, 4961, 281, 360, 382, 291, 589], "temperature": 0.0, "avg_logprob": -0.1890540039330198, "compression_ratio": 1.4723926380368098, "no_speech_prob": 3.373668732820079e-05}, {"id": 581, "seek": 386192, "start": 3870.7200000000003, "end": 3875.92, "text": " through these kinds of problems. Just draw lots of pictures. And those pictures were", "tokens": [807, 613, 3685, 295, 2740, 13, 1449, 2642, 3195, 295, 5242, 13, 400, 729, 5242, 645], "temperature": 0.0, "avg_logprob": -0.1890540039330198, "compression_ratio": 1.4723926380368098, "no_speech_prob": 3.373668732820079e-05}, {"id": 582, "seek": 386192, "start": 3875.92, "end": 3882.44, "text": " drawn in R. R is actually pretty good for this kind of structured data.", "tokens": [10117, 294, 497, 13, 497, 307, 767, 1238, 665, 337, 341, 733, 295, 18519, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1890540039330198, "compression_ratio": 1.4723926380368098, "no_speech_prob": 3.373668732820079e-05}, {"id": 583, "seek": 388244, "start": 3882.44, "end": 3892.2400000000002, "text": " So for categorical fields, they're converted by the numbers, not with mean 0. They were", "tokens": [407, 337, 19250, 804, 7909, 11, 436, 434, 16424, 538, 264, 3547, 11, 406, 365, 914, 1958, 13, 814, 645], "temperature": 0.0, "avg_logprob": -0.31169410601054154, "compression_ratio": 1.4719101123595506, "no_speech_prob": 1.2805247024516575e-05}, {"id": 584, "seek": 388244, "start": 3892.2400000000002, "end": 3895.08, "text": " just Monday 0, Tuesday 1, whatever.", "tokens": [445, 8138, 1958, 11, 10017, 502, 11, 2035, 13], "temperature": 0.0, "avg_logprob": -0.31169410601054154, "compression_ratio": 1.4719101123595506, "no_speech_prob": 1.2805247024516575e-05}, {"id": 585, "seek": 388244, "start": 3895.08, "end": 3899.52, "text": " As ease, they will send to neural network.", "tokens": [1018, 12708, 11, 436, 486, 2845, 281, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.31169410601054154, "compression_ratio": 1.4719101123595506, "no_speech_prob": 1.2805247024516575e-05}, {"id": 586, "seek": 388244, "start": 3899.52, "end": 3907.0, "text": " We're going to get there. We're going to use embeddings. Just like we did with word embeddings,", "tokens": [492, 434, 516, 281, 483, 456, 13, 492, 434, 516, 281, 764, 12240, 29432, 13, 1449, 411, 321, 630, 365, 1349, 12240, 29432, 11], "temperature": 0.0, "avg_logprob": -0.31169410601054154, "compression_ratio": 1.4719101123595506, "no_speech_prob": 1.2805247024516575e-05}, {"id": 587, "seek": 390700, "start": 3907.0, "end": 3913.28, "text": " remember we turned every word into a word index. So our sentences rather than being", "tokens": [1604, 321, 3574, 633, 1349, 666, 257, 1349, 8186, 13, 407, 527, 16579, 2831, 813, 885], "temperature": 0.0, "avg_logprob": -0.1720002342672909, "compression_ratio": 1.5416666666666667, "no_speech_prob": 3.7266202070895815e-06}, {"id": 588, "seek": 390700, "start": 3913.28, "end": 3922.52, "text": " like the dog ate the beans, it would be 3, 6, 12, 2. We're going to do the same basic", "tokens": [411, 264, 3000, 8468, 264, 12010, 11, 309, 576, 312, 805, 11, 1386, 11, 2272, 11, 568, 13, 492, 434, 516, 281, 360, 264, 912, 3875], "temperature": 0.0, "avg_logprob": -0.1720002342672909, "compression_ratio": 1.5416666666666667, "no_speech_prob": 3.7266202070895815e-06}, {"id": 589, "seek": 390700, "start": 3922.52, "end": 3925.88, "text": " thing. We've done the same basic thing.", "tokens": [551, 13, 492, 600, 1096, 264, 912, 3875, 551, 13], "temperature": 0.0, "avg_logprob": -0.1720002342672909, "compression_ratio": 1.5416666666666667, "no_speech_prob": 3.7266202070895815e-06}, {"id": 590, "seek": 390700, "start": 3925.88, "end": 3934.68, "text": " So now that we've done our terrible mistake, we've still got 844,000 rows left. As per", "tokens": [407, 586, 300, 321, 600, 1096, 527, 6237, 6146, 11, 321, 600, 920, 658, 1649, 13912, 11, 1360, 13241, 1411, 13, 1018, 680], "temperature": 0.0, "avg_logprob": -0.1720002342672909, "compression_ratio": 1.5416666666666667, "no_speech_prob": 3.7266202070895815e-06}, {"id": 591, "seek": 393468, "start": 3934.68, "end": 3940.52, "text": " usual, I made it really easy for me to create a random sample and did most of my analysis", "tokens": [7713, 11, 286, 1027, 309, 534, 1858, 337, 385, 281, 1884, 257, 4974, 6889, 293, 630, 881, 295, 452, 5215], "temperature": 0.0, "avg_logprob": -0.12137735732878098, "compression_ratio": 1.7443946188340806, "no_speech_prob": 1.963795966730686e-06}, {"id": 592, "seek": 393468, "start": 3940.52, "end": 3946.56, "text": " with a random sample, but can just as easily not do the random sample. So now I've got", "tokens": [365, 257, 4974, 6889, 11, 457, 393, 445, 382, 3612, 406, 360, 264, 4974, 6889, 13, 407, 586, 286, 600, 658], "temperature": 0.0, "avg_logprob": -0.12137735732878098, "compression_ratio": 1.7443946188340806, "no_speech_prob": 1.963795966730686e-06}, {"id": 593, "seek": 393468, "start": 3946.56, "end": 3952.0, "text": " a separate sample version of it.", "tokens": [257, 4994, 6889, 3037, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.12137735732878098, "compression_ratio": 1.7443946188340806, "no_speech_prob": 1.963795966730686e-06}, {"id": 594, "seek": 393468, "start": 3952.0, "end": 3958.2, "text": " Split it into training and test. Now notice here, the way I split it into training and", "tokens": [45111, 309, 666, 3097, 293, 1500, 13, 823, 3449, 510, 11, 264, 636, 286, 7472, 309, 666, 3097, 293], "temperature": 0.0, "avg_logprob": -0.12137735732878098, "compression_ratio": 1.7443946188340806, "no_speech_prob": 1.963795966730686e-06}, {"id": 595, "seek": 393468, "start": 3958.2, "end": 3964.52, "text": " test is not randomly. And the reason it's not randomly is because in the Kaggle competition,", "tokens": [1500, 307, 406, 16979, 13, 400, 264, 1778, 309, 311, 406, 16979, 307, 570, 294, 264, 48751, 22631, 6211, 11], "temperature": 0.0, "avg_logprob": -0.12137735732878098, "compression_ratio": 1.7443946188340806, "no_speech_prob": 1.963795966730686e-06}, {"id": 596, "seek": 396452, "start": 3964.52, "end": 3971.44, "text": " they set it up the smart way. The smart way to set up a test set in a time series is to", "tokens": [436, 992, 309, 493, 264, 4069, 636, 13, 440, 4069, 636, 281, 992, 493, 257, 1500, 992, 294, 257, 565, 2638, 307, 281], "temperature": 0.0, "avg_logprob": -0.08197175250964218, "compression_ratio": 1.6255707762557077, "no_speech_prob": 1.2029442586936057e-05}, {"id": 597, "seek": 396452, "start": 3971.44, "end": 3978.04, "text": " make your test set the most recent period of time. If you choose random points, you've", "tokens": [652, 428, 1500, 992, 264, 881, 5162, 2896, 295, 565, 13, 759, 291, 2826, 4974, 2793, 11, 291, 600], "temperature": 0.0, "avg_logprob": -0.08197175250964218, "compression_ratio": 1.6255707762557077, "no_speech_prob": 1.2029442586936057e-05}, {"id": 598, "seek": 396452, "start": 3978.04, "end": 3983.16, "text": " got two problems. The first is you're predicting tomorrow's sales where you always have the", "tokens": [658, 732, 2740, 13, 440, 700, 307, 291, 434, 32884, 4153, 311, 5763, 689, 291, 1009, 362, 264], "temperature": 0.0, "avg_logprob": -0.08197175250964218, "compression_ratio": 1.6255707762557077, "no_speech_prob": 1.2029442586936057e-05}, {"id": 599, "seek": 396452, "start": 3983.16, "end": 3992.16, "text": " previous day's sales, which is very rarely the way things really work. And then secondly,", "tokens": [3894, 786, 311, 5763, 11, 597, 307, 588, 13752, 264, 636, 721, 534, 589, 13, 400, 550, 26246, 11], "temperature": 0.0, "avg_logprob": -0.08197175250964218, "compression_ratio": 1.6255707762557077, "no_speech_prob": 1.2029442586936057e-05}, {"id": 600, "seek": 399216, "start": 3992.16, "end": 3997.56, "text": " you're ignoring the fact that in the real world, you're always trying to model a few", "tokens": [291, 434, 26258, 264, 1186, 300, 294, 264, 957, 1002, 11, 291, 434, 1009, 1382, 281, 2316, 257, 1326], "temperature": 0.0, "avg_logprob": -0.16633552493471088, "compression_ratio": 1.8503649635036497, "no_speech_prob": 3.0894730116415303e-06}, {"id": 601, "seek": 399216, "start": 3997.56, "end": 4001.48, "text": " days or a few weeks or a few months in the future that haven't happened yet.", "tokens": [1708, 420, 257, 1326, 3259, 420, 257, 1326, 2493, 294, 264, 2027, 300, 2378, 380, 2011, 1939, 13], "temperature": 0.0, "avg_logprob": -0.16633552493471088, "compression_ratio": 1.8503649635036497, "no_speech_prob": 3.0894730116415303e-06}, {"id": 602, "seek": 399216, "start": 4001.48, "end": 4006.8999999999996, "text": " So the way you want to set up, if you were building, if you were setting up the data", "tokens": [407, 264, 636, 291, 528, 281, 992, 493, 11, 498, 291, 645, 2390, 11, 498, 291, 645, 3287, 493, 264, 1412], "temperature": 0.0, "avg_logprob": -0.16633552493471088, "compression_ratio": 1.8503649635036497, "no_speech_prob": 3.0894730116415303e-06}, {"id": 603, "seek": 399216, "start": 4006.8999999999996, "end": 4011.8799999999997, "text": " for such a model yourself, you would need to be deciding how often am I going to be", "tokens": [337, 1270, 257, 2316, 1803, 11, 291, 576, 643, 281, 312, 17990, 577, 2049, 669, 286, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.16633552493471088, "compression_ratio": 1.8503649635036497, "no_speech_prob": 3.0894730116415303e-06}, {"id": 604, "seek": 399216, "start": 4011.8799999999997, "end": 4016.3599999999997, "text": " rerunning this model, how long is it going to take for those model results to get into", "tokens": [43819, 25589, 341, 2316, 11, 577, 938, 307, 309, 516, 281, 747, 337, 729, 2316, 3542, 281, 483, 666], "temperature": 0.0, "avg_logprob": -0.16633552493471088, "compression_ratio": 1.8503649635036497, "no_speech_prob": 3.0894730116415303e-06}, {"id": 605, "seek": 399216, "start": 4016.3599999999997, "end": 4020.8799999999997, "text": " the field to be used and however they're being used. In this case, I guess they decide, I", "tokens": [264, 2519, 281, 312, 1143, 293, 4461, 436, 434, 885, 1143, 13, 682, 341, 1389, 11, 286, 2041, 436, 4536, 11, 286], "temperature": 0.0, "avg_logprob": -0.16633552493471088, "compression_ratio": 1.8503649635036497, "no_speech_prob": 3.0894730116415303e-06}, {"id": 606, "seek": 402088, "start": 4020.88, "end": 4026.52, "text": " can't remember, I think it's like a month or two. So in that case I should make sure", "tokens": [393, 380, 1604, 11, 286, 519, 309, 311, 411, 257, 1618, 420, 732, 13, 407, 294, 300, 1389, 286, 820, 652, 988], "temperature": 0.0, "avg_logprob": -0.16401414076487222, "compression_ratio": 1.6650485436893203, "no_speech_prob": 7.296213425433962e-06}, {"id": 607, "seek": 402088, "start": 4026.52, "end": 4034.76, "text": " there's a month or two gap or a month or two test set, which is the last bit.", "tokens": [456, 311, 257, 1618, 420, 732, 7417, 420, 257, 1618, 420, 732, 1500, 992, 11, 597, 307, 264, 1036, 857, 13], "temperature": 0.0, "avg_logprob": -0.16401414076487222, "compression_ratio": 1.6650485436893203, "no_speech_prob": 7.296213425433962e-06}, {"id": 608, "seek": 402088, "start": 4034.76, "end": 4041.0, "text": " So you can see here I've taken the last 10% of my validation set and it's literally just", "tokens": [407, 291, 393, 536, 510, 286, 600, 2726, 264, 1036, 1266, 4, 295, 452, 24071, 992, 293, 309, 311, 3736, 445], "temperature": 0.0, "avg_logprob": -0.16401414076487222, "compression_ratio": 1.6650485436893203, "no_speech_prob": 7.296213425433962e-06}, {"id": 609, "seek": 402088, "start": 4041.0, "end": 4049.4, "text": " here's the first bit and here's the last bit. And since it was already sorted by date, this", "tokens": [510, 311, 264, 700, 857, 293, 510, 311, 264, 1036, 857, 13, 400, 1670, 309, 390, 1217, 25462, 538, 4002, 11, 341], "temperature": 0.0, "avg_logprob": -0.16401414076487222, "compression_ratio": 1.6650485436893203, "no_speech_prob": 7.296213425433962e-06}, {"id": 610, "seek": 404940, "start": 4049.4, "end": 4068.6800000000003, "text": " ensures that I have it done the way I want.", "tokens": [28111, 300, 286, 362, 309, 1096, 264, 636, 286, 528, 13], "temperature": 0.0, "avg_logprob": -0.29683241973052155, "compression_ratio": 1.2285714285714286, "no_speech_prob": 1.0616011422825977e-05}, {"id": 611, "seek": 404940, "start": 4068.6800000000003, "end": 4074.44, "text": " This is how you take that data frame map or object we created earlier, we called.fit,", "tokens": [639, 307, 577, 291, 747, 300, 1412, 3920, 4471, 420, 2657, 321, 2942, 3071, 11, 321, 1219, 2411, 6845, 11], "temperature": 0.0, "avg_logprob": -0.29683241973052155, "compression_ratio": 1.2285714285714286, "no_speech_prob": 1.0616011422825977e-05}, {"id": 612, "seek": 407444, "start": 4074.44, "end": 4084.7200000000003, "text": " in order to learn the transformation parameters, you then call transform to actually do it.", "tokens": [294, 1668, 281, 1466, 264, 9887, 9834, 11, 291, 550, 818, 4088, 281, 767, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.10916131734848022, "compression_ratio": 1.878787878787879, "no_speech_prob": 6.747967290721135e-06}, {"id": 613, "seek": 407444, "start": 4084.7200000000003, "end": 4091.6, "text": " So take my training set and transform it to grab the categorical variables, and then", "tokens": [407, 747, 452, 3097, 992, 293, 4088, 309, 281, 4444, 264, 19250, 804, 9102, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.10916131734848022, "compression_ratio": 1.878787878787879, "no_speech_prob": 6.747967290721135e-06}, {"id": 614, "seek": 407444, "start": 4091.6, "end": 4097.28, "text": " the continuous preprocessing is the same thing for my continuous map. So preprocess my training", "tokens": [264, 10957, 2666, 340, 780, 278, 307, 264, 912, 551, 337, 452, 10957, 4471, 13, 407, 2666, 340, 780, 452, 3097], "temperature": 0.0, "avg_logprob": -0.10916131734848022, "compression_ratio": 1.878787878787879, "no_speech_prob": 6.747967290721135e-06}, {"id": 615, "seek": 407444, "start": 4097.28, "end": 4101.64, "text": " set and grab my continuous variables.", "tokens": [992, 293, 4444, 452, 10957, 9102, 13], "temperature": 0.0, "avg_logprob": -0.10916131734848022, "compression_ratio": 1.878787878787879, "no_speech_prob": 6.747967290721135e-06}, {"id": 616, "seek": 410164, "start": 4101.64, "end": 4115.04, "text": " So that's nearly done. The only final piece is in their solution, they modified their", "tokens": [407, 300, 311, 6217, 1096, 13, 440, 787, 2572, 2522, 307, 294, 641, 3827, 11, 436, 15873, 641], "temperature": 0.0, "avg_logprob": -0.17004773020744324, "compression_ratio": 1.7039473684210527, "no_speech_prob": 2.4439841581624933e-06}, {"id": 617, "seek": 410164, "start": 4115.04, "end": 4121.04, "text": " target, their sales value. And the way they modified it was that they found what is the", "tokens": [3779, 11, 641, 5763, 2158, 13, 400, 264, 636, 436, 15873, 309, 390, 300, 436, 1352, 437, 307, 264], "temperature": 0.0, "avg_logprob": -0.17004773020744324, "compression_ratio": 1.7039473684210527, "no_speech_prob": 2.4439841581624933e-06}, {"id": 618, "seek": 410164, "start": 4121.04, "end": 4129.400000000001, "text": " highest amount of sales, and they took the log of that, and then they modified all of", "tokens": [6343, 2372, 295, 5763, 11, 293, 436, 1890, 264, 3565, 295, 300, 11, 293, 550, 436, 15873, 439, 295], "temperature": 0.0, "avg_logprob": -0.17004773020744324, "compression_ratio": 1.7039473684210527, "no_speech_prob": 2.4439841581624933e-06}, {"id": 619, "seek": 412940, "start": 4129.4, "end": 4136.839999999999, "text": " their Y values to take the log of sales divided by the maximum log of sales. So what this", "tokens": [641, 398, 4190, 281, 747, 264, 3565, 295, 5763, 6666, 538, 264, 6674, 3565, 295, 5763, 13, 407, 437, 341], "temperature": 0.0, "avg_logprob": -0.1834868098912614, "compression_ratio": 1.5644444444444445, "no_speech_prob": 7.296347575902473e-06}, {"id": 620, "seek": 412940, "start": 4136.839999999999, "end": 4145.12, "text": " means is that the Y values are going to be no higher than 1. And furthermore, remember", "tokens": [1355, 307, 300, 264, 398, 4190, 366, 516, 281, 312, 572, 2946, 813, 502, 13, 400, 3052, 3138, 11, 1604], "temperature": 0.0, "avg_logprob": -0.1834868098912614, "compression_ratio": 1.5644444444444445, "no_speech_prob": 7.296347575902473e-06}, {"id": 621, "seek": 412940, "start": 4145.12, "end": 4149.28, "text": " how they had a long tail, the average was 5000, but the maximum was like 40-something", "tokens": [577, 436, 632, 257, 938, 6838, 11, 264, 4274, 390, 23777, 11, 457, 264, 6674, 390, 411, 3356, 12, 31681], "temperature": 0.0, "avg_logprob": -0.1834868098912614, "compression_ratio": 1.5644444444444445, "no_speech_prob": 7.296347575902473e-06}, {"id": 622, "seek": 412940, "start": 4149.28, "end": 4156.44, "text": " thousand. This is really common. Most financial data, sales data, so forth, generally has", "tokens": [4714, 13, 639, 307, 534, 2689, 13, 4534, 4669, 1412, 11, 5763, 1412, 11, 370, 5220, 11, 5101, 575], "temperature": 0.0, "avg_logprob": -0.1834868098912614, "compression_ratio": 1.5644444444444445, "no_speech_prob": 7.296347575902473e-06}, {"id": 623, "seek": 415644, "start": 4156.44, "end": 4162.32, "text": " a nicer shape when it's logged than it does not. So taking a log is a really good idea.", "tokens": [257, 22842, 3909, 562, 309, 311, 27231, 813, 309, 775, 406, 13, 407, 1940, 257, 3565, 307, 257, 534, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.1448913014263188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.013440492504742e-06}, {"id": 624, "seek": 415644, "start": 4162.32, "end": 4167.879999999999, "text": " The reason that as well as taking the log, they also did this division, is it means that", "tokens": [440, 1778, 300, 382, 731, 382, 1940, 264, 3565, 11, 436, 611, 630, 341, 10044, 11, 307, 309, 1355, 300], "temperature": 0.0, "avg_logprob": -0.1448913014263188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.013440492504742e-06}, {"id": 625, "seek": 415644, "start": 4167.879999999999, "end": 4173.719999999999, "text": " what we can now do is we can use an activation function in our neural net of a sigmoid, which", "tokens": [437, 321, 393, 586, 360, 307, 321, 393, 764, 364, 24433, 2445, 294, 527, 18161, 2533, 295, 257, 4556, 3280, 327, 11, 597], "temperature": 0.0, "avg_logprob": -0.1448913014263188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.013440492504742e-06}, {"id": 626, "seek": 415644, "start": 4173.719999999999, "end": 4180.12, "text": " goes between 0 and 1, and then just multiply by the maximum log. So that's basically going", "tokens": [1709, 1296, 1958, 293, 502, 11, 293, 550, 445, 12972, 538, 264, 6674, 3565, 13, 407, 300, 311, 1936, 516], "temperature": 0.0, "avg_logprob": -0.1448913014263188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.013440492504742e-06}, {"id": 627, "seek": 415644, "start": 4180.12, "end": 4183.879999999999, "text": " to ensure that the data is in the right scaling area.", "tokens": [281, 5586, 300, 264, 1412, 307, 294, 264, 558, 21589, 1859, 13], "temperature": 0.0, "avg_logprob": -0.1448913014263188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.013440492504742e-06}, {"id": 628, "seek": 418388, "start": 4183.88, "end": 4191.0, "text": " I actually tried taking this out, and this technique doesn't really seem to help. It", "tokens": [286, 767, 3031, 1940, 341, 484, 11, 293, 341, 6532, 1177, 380, 534, 1643, 281, 854, 13, 467], "temperature": 0.0, "avg_logprob": -0.19604450318871475, "compression_ratio": 1.51528384279476, "no_speech_prob": 8.664555025461596e-06}, {"id": 629, "seek": 418388, "start": 4191.0, "end": 4197.4800000000005, "text": " actually reminds me of the style transfer paper where they mentioned they originally", "tokens": [767, 12025, 385, 295, 264, 3758, 5003, 3035, 689, 436, 2835, 436, 7993], "temperature": 0.0, "avg_logprob": -0.19604450318871475, "compression_ratio": 1.51528384279476, "no_speech_prob": 8.664555025461596e-06}, {"id": 630, "seek": 418388, "start": 4197.4800000000005, "end": 4202.6, "text": " had a hyperbolic tan layer at the end for exactly the same reason, to make sure everything", "tokens": [632, 257, 9848, 65, 7940, 7603, 4583, 412, 264, 917, 337, 2293, 264, 912, 1778, 11, 281, 652, 988, 1203], "temperature": 0.0, "avg_logprob": -0.19604450318871475, "compression_ratio": 1.51528384279476, "no_speech_prob": 8.664555025461596e-06}, {"id": 631, "seek": 418388, "start": 4202.6, "end": 4206.84, "text": " was between 0 and 255. And it turns out if you just use a linear activation, it worked", "tokens": [390, 1296, 1958, 293, 3552, 20, 13, 400, 309, 4523, 484, 498, 291, 445, 764, 257, 8213, 24433, 11, 309, 2732], "temperature": 0.0, "avg_logprob": -0.19604450318871475, "compression_ratio": 1.51528384279476, "no_speech_prob": 8.664555025461596e-06}, {"id": 632, "seek": 420684, "start": 4206.84, "end": 4214.04, "text": " just as well. So interestingly, this idea of using sigmoids at the end in order to get", "tokens": [445, 382, 731, 13, 407, 25873, 11, 341, 1558, 295, 1228, 4556, 3280, 3742, 412, 264, 917, 294, 1668, 281, 483], "temperature": 0.0, "avg_logprob": -0.15360708144104596, "compression_ratio": 1.6872427983539096, "no_speech_prob": 2.443958464937168e-06}, {"id": 633, "seek": 420684, "start": 4214.04, "end": 4217.6, "text": " the right range doesn't seem to be that helpful.", "tokens": [264, 558, 3613, 1177, 380, 1643, 281, 312, 300, 4961, 13], "temperature": 0.0, "avg_logprob": -0.15360708144104596, "compression_ratio": 1.6872427983539096, "no_speech_prob": 2.443958464937168e-06}, {"id": 634, "seek": 420684, "start": 4217.6, "end": 4223.04, "text": " My guess is the reason why is because for a sigmoid it's really difficult to get the", "tokens": [1222, 2041, 307, 264, 1778, 983, 307, 570, 337, 257, 4556, 3280, 327, 309, 311, 534, 2252, 281, 483, 264], "temperature": 0.0, "avg_logprob": -0.15360708144104596, "compression_ratio": 1.6872427983539096, "no_speech_prob": 2.443958464937168e-06}, {"id": 635, "seek": 420684, "start": 4223.04, "end": 4228.84, "text": " maximum. And I think actually what they should have done is they probably should have, instead", "tokens": [6674, 13, 400, 286, 519, 767, 437, 436, 820, 362, 1096, 307, 436, 1391, 820, 362, 11, 2602], "temperature": 0.0, "avg_logprob": -0.15360708144104596, "compression_ratio": 1.6872427983539096, "no_speech_prob": 2.443958464937168e-06}, {"id": 636, "seek": 420684, "start": 4228.84, "end": 4235.8, "text": " of using maximum, they should have used maximum times 1.25, so that they never have to predict", "tokens": [295, 1228, 6674, 11, 436, 820, 362, 1143, 6674, 1413, 502, 13, 6074, 11, 370, 300, 436, 1128, 362, 281, 6069], "temperature": 0.0, "avg_logprob": -0.15360708144104596, "compression_ratio": 1.6872427983539096, "no_speech_prob": 2.443958464937168e-06}, {"id": 637, "seek": 423580, "start": 4235.8, "end": 4242.72, "text": " 1, because it's impossible to predict 1 because it's a sigmoid.", "tokens": [502, 11, 570, 309, 311, 6243, 281, 6069, 502, 570, 309, 311, 257, 4556, 3280, 327, 13], "temperature": 0.0, "avg_logprob": -0.2308633873261601, "compression_ratio": 1.53, "no_speech_prob": 3.763576751225628e-05}, {"id": 638, "seek": 423580, "start": 4242.72, "end": 4246.76, "text": " Someone asked, is there any issue in fitting the preprocessors on the full training and", "tokens": [8734, 2351, 11, 307, 456, 604, 2734, 294, 15669, 264, 2666, 340, 45700, 322, 264, 1577, 3097, 293], "temperature": 0.0, "avg_logprob": -0.2308633873261601, "compression_ratio": 1.53, "no_speech_prob": 3.763576751225628e-05}, {"id": 639, "seek": 423580, "start": 4246.76, "end": 4251.56, "text": " validation data, shouldn't they be fit only to the training set?", "tokens": [24071, 1412, 11, 4659, 380, 436, 312, 3318, 787, 281, 264, 3097, 992, 30], "temperature": 0.0, "avg_logprob": -0.2308633873261601, "compression_ratio": 1.53, "no_speech_prob": 3.763576751225628e-05}, {"id": 640, "seek": 423580, "start": 4251.56, "end": 4261.56, "text": " No, it's fine. In fact, for the categorical variables, if you don't include the test set,", "tokens": [883, 11, 309, 311, 2489, 13, 682, 1186, 11, 337, 264, 19250, 804, 9102, 11, 498, 291, 500, 380, 4090, 264, 1500, 992, 11], "temperature": 0.0, "avg_logprob": -0.2308633873261601, "compression_ratio": 1.53, "no_speech_prob": 3.763576751225628e-05}, {"id": 641, "seek": 426156, "start": 4261.56, "end": 4265.88, "text": " then you're going to have some codes that aren't there at all. Or else this way they're", "tokens": [550, 291, 434, 516, 281, 362, 512, 14211, 300, 3212, 380, 456, 412, 439, 13, 1610, 1646, 341, 636, 436, 434], "temperature": 0.0, "avg_logprob": -0.18085543702288373, "compression_ratio": 1.5167464114832536, "no_speech_prob": 4.2892902456515e-06}, {"id": 642, "seek": 426156, "start": 4265.88, "end": 4271.04, "text": " just going to be random, which is better than failing.", "tokens": [445, 516, 281, 312, 4974, 11, 597, 307, 1101, 813, 18223, 13], "temperature": 0.0, "avg_logprob": -0.18085543702288373, "compression_ratio": 1.5167464114832536, "no_speech_prob": 4.2892902456515e-06}, {"id": 643, "seek": 426156, "start": 4271.04, "end": 4276.160000000001, "text": " As for deciding what to divide and subtract in order to get a 0 or 1 random variable,", "tokens": [1018, 337, 17990, 437, 281, 9845, 293, 16390, 294, 1668, 281, 483, 257, 1958, 420, 502, 4974, 7006, 11], "temperature": 0.0, "avg_logprob": -0.18085543702288373, "compression_ratio": 1.5167464114832536, "no_speech_prob": 4.2892902456515e-06}, {"id": 644, "seek": 426156, "start": 4276.160000000001, "end": 4286.120000000001, "text": " it doesn't really matter. There's no leakage involved, that's what you're worried about.", "tokens": [309, 1177, 380, 534, 1871, 13, 821, 311, 572, 47799, 3288, 11, 300, 311, 437, 291, 434, 5804, 466, 13], "temperature": 0.0, "avg_logprob": -0.18085543702288373, "compression_ratio": 1.5167464114832536, "no_speech_prob": 4.2892902456515e-06}, {"id": 645, "seek": 428612, "start": 4286.12, "end": 4291.72, "text": " Crit mean squared percent error is what the Kaggle competition used as the official loss", "tokens": [23202, 914, 8889, 3043, 6713, 307, 437, 264, 48751, 22631, 6211, 1143, 382, 264, 4783, 4470], "temperature": 0.0, "avg_logprob": -0.24786393039197807, "compression_ratio": 1.572093023255814, "no_speech_prob": 1.6187286746571772e-05}, {"id": 646, "seek": 428612, "start": 4291.72, "end": 4294.68, "text": " function. So this is just calculating that.", "tokens": [2445, 13, 407, 341, 307, 445, 28258, 300, 13], "temperature": 0.0, "avg_logprob": -0.24786393039197807, "compression_ratio": 1.572093023255814, "no_speech_prob": 1.6187286746571772e-05}, {"id": 647, "seek": 428612, "start": 4294.68, "end": 4300.76, "text": " So before we take a break, we'll just finally take a look at the definition of the model.", "tokens": [407, 949, 321, 747, 257, 1821, 11, 321, 603, 445, 2721, 747, 257, 574, 412, 264, 7123, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.24786393039197807, "compression_ratio": 1.572093023255814, "no_speech_prob": 1.6187286746571772e-05}, {"id": 648, "seek": 428612, "start": 4300.76, "end": 4307.4, "text": " I'll kind of work backwards.", "tokens": [286, 603, 733, 295, 589, 12204, 13], "temperature": 0.0, "avg_logprob": -0.24786393039197807, "compression_ratio": 1.572093023255814, "no_speech_prob": 1.6187286746571772e-05}, {"id": 649, "seek": 428612, "start": 4307.4, "end": 4314.72, "text": " Here's the basic model. Get our embeddings, combine the embeddings with the continuous", "tokens": [1692, 311, 264, 3875, 2316, 13, 3240, 527, 12240, 29432, 11, 10432, 264, 12240, 29432, 365, 264, 10957], "temperature": 0.0, "avg_logprob": -0.24786393039197807, "compression_ratio": 1.572093023255814, "no_speech_prob": 1.6187286746571772e-05}, {"id": 650, "seek": 431472, "start": 4314.72, "end": 4321.0, "text": " variables, a tiny bit of dropout, 1 dense layer, 2 dense layers, more dropout, and then", "tokens": [9102, 11, 257, 5870, 857, 295, 3270, 346, 11, 502, 18011, 4583, 11, 568, 18011, 7914, 11, 544, 3270, 346, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.1767587848738128, "compression_ratio": 1.5853658536585367, "no_speech_prob": 2.7108406357001513e-05}, {"id": 651, "seek": 431472, "start": 4321.0, "end": 4325.08, "text": " the final sigmoid activation function.", "tokens": [264, 2572, 4556, 3280, 327, 24433, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1767587848738128, "compression_ratio": 1.5853658536585367, "no_speech_prob": 2.7108406357001513e-05}, {"id": 652, "seek": 431472, "start": 4325.08, "end": 4330.38, "text": " You'll see that I've got commented out stuff all over the place. This is because I had", "tokens": [509, 603, 536, 300, 286, 600, 658, 26940, 484, 1507, 439, 670, 264, 1081, 13, 639, 307, 570, 286, 632], "temperature": 0.0, "avg_logprob": -0.1767587848738128, "compression_ratio": 1.5853658536585367, "no_speech_prob": 2.7108406357001513e-05}, {"id": 653, "seek": 431472, "start": 4330.38, "end": 4339.12, "text": " a lot of questions about some of the details. Why did they do things certain ways? Some", "tokens": [257, 688, 295, 1651, 466, 512, 295, 264, 4365, 13, 1545, 630, 436, 360, 721, 1629, 2098, 30, 2188], "temperature": 0.0, "avg_logprob": -0.1767587848738128, "compression_ratio": 1.5853658536585367, "no_speech_prob": 2.7108406357001513e-05}, {"id": 654, "seek": 431472, "start": 4339.12, "end": 4342.76, "text": " of the things they did were so weird, I just thought they couldn't possibly be right. So", "tokens": [295, 264, 721, 436, 630, 645, 370, 3657, 11, 286, 445, 1194, 436, 2809, 380, 6264, 312, 558, 13, 407], "temperature": 0.0, "avg_logprob": -0.1767587848738128, "compression_ratio": 1.5853658536585367, "no_speech_prob": 2.7108406357001513e-05}, {"id": 655, "seek": 434276, "start": 4342.76, "end": 4347.08, "text": " I did some experimenting. We'll learn more about that in a moment.", "tokens": [286, 630, 512, 29070, 13, 492, 603, 1466, 544, 466, 300, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.12582243056524367, "compression_ratio": 1.66, "no_speech_prob": 3.7853030789847253e-06}, {"id": 656, "seek": 434276, "start": 4347.08, "end": 4354.8, "text": " So the embeddings, as per usual, I create a little function to create an embedding which", "tokens": [407, 264, 12240, 29432, 11, 382, 680, 7713, 11, 286, 1884, 257, 707, 2445, 281, 1884, 364, 12240, 3584, 597], "temperature": 0.0, "avg_logprob": -0.12582243056524367, "compression_ratio": 1.66, "no_speech_prob": 3.7853030789847253e-06}, {"id": 657, "seek": 434276, "start": 4354.8, "end": 4363.56, "text": " first of all creates my regular Keras input layer, and then it creates my embedding layer,", "tokens": [700, 295, 439, 7829, 452, 3890, 591, 6985, 4846, 4583, 11, 293, 550, 309, 7829, 452, 12240, 3584, 4583, 11], "temperature": 0.0, "avg_logprob": -0.12582243056524367, "compression_ratio": 1.66, "no_speech_prob": 3.7853030789847253e-06}, {"id": 658, "seek": 434276, "start": 4363.56, "end": 4368.52, "text": " and then how many embedding dimensions am I going to use? Sometimes I look them up in", "tokens": [293, 550, 577, 867, 12240, 3584, 12819, 669, 286, 516, 281, 764, 30, 4803, 286, 574, 552, 493, 294], "temperature": 0.0, "avg_logprob": -0.12582243056524367, "compression_ratio": 1.66, "no_speech_prob": 3.7853030789847253e-06}, {"id": 659, "seek": 436852, "start": 4368.52, "end": 4374.92, "text": " that dictionary I had earlier, and sometimes I calculated them using this simple approach", "tokens": [300, 25890, 286, 632, 3071, 11, 293, 2171, 286, 15598, 552, 1228, 341, 2199, 3109], "temperature": 0.0, "avg_logprob": -0.1811630931901343, "compression_ratio": 1.4786324786324787, "no_speech_prob": 4.78505899081938e-06}, {"id": 660, "seek": 436852, "start": 4374.92, "end": 4380.320000000001, "text": " of saying I will use however many levels there are in the categorical variable, divide it", "tokens": [295, 1566, 286, 486, 764, 4461, 867, 4358, 456, 366, 294, 264, 19250, 804, 7006, 11, 9845, 309], "temperature": 0.0, "avg_logprob": -0.1811630931901343, "compression_ratio": 1.4786324786324787, "no_speech_prob": 4.78505899081938e-06}, {"id": 661, "seek": 436852, "start": 4380.320000000001, "end": 4387.8, "text": " by 2 with a maximum of 50. These were 2 different techniques I was playing with.", "tokens": [538, 568, 365, 257, 6674, 295, 2625, 13, 1981, 645, 568, 819, 7512, 286, 390, 2433, 365, 13], "temperature": 0.0, "avg_logprob": -0.1811630931901343, "compression_ratio": 1.4786324786324787, "no_speech_prob": 4.78505899081938e-06}, {"id": 662, "seek": 436852, "start": 4387.8, "end": 4393.96, "text": " Normally with word embeddings, you have a whole sentence. So you've got to feed it to", "tokens": [17424, 365, 1349, 12240, 29432, 11, 291, 362, 257, 1379, 8174, 13, 407, 291, 600, 658, 281, 3154, 309, 281], "temperature": 0.0, "avg_logprob": -0.1811630931901343, "compression_ratio": 1.4786324786324787, "no_speech_prob": 4.78505899081938e-06}, {"id": 663, "seek": 439396, "start": 4393.96, "end": 4399.36, "text": " an RNN, and so you have time steps. So normally you have an input length equal to the length", "tokens": [364, 45702, 45, 11, 293, 370, 291, 362, 565, 4439, 13, 407, 5646, 291, 362, 364, 4846, 4641, 2681, 281, 264, 4641], "temperature": 0.0, "avg_logprob": -0.1122165602080676, "compression_ratio": 1.723809523809524, "no_speech_prob": 7.76688102632761e-06}, {"id": 664, "seek": 439396, "start": 4399.36, "end": 4405.52, "text": " of your sentence. This is the time steps for an RNN. We don't have an RNN. We don't have", "tokens": [295, 428, 8174, 13, 639, 307, 264, 565, 4439, 337, 364, 45702, 45, 13, 492, 500, 380, 362, 364, 45702, 45, 13, 492, 500, 380, 362], "temperature": 0.0, "avg_logprob": -0.1122165602080676, "compression_ratio": 1.723809523809524, "no_speech_prob": 7.76688102632761e-06}, {"id": 665, "seek": 439396, "start": 4405.52, "end": 4414.3, "text": " any time steps. We just have one element in one column. So therefore I have to pass flatten", "tokens": [604, 565, 4439, 13, 492, 445, 362, 472, 4478, 294, 472, 7738, 13, 407, 4412, 286, 362, 281, 1320, 24183], "temperature": 0.0, "avg_logprob": -0.1122165602080676, "compression_ratio": 1.723809523809524, "no_speech_prob": 7.76688102632761e-06}, {"id": 666, "seek": 439396, "start": 4414.3, "end": 4422.28, "text": " after this because it's going to have this redundant unit 1 time axis that I don't want.", "tokens": [934, 341, 570, 309, 311, 516, 281, 362, 341, 40997, 4985, 502, 565, 10298, 300, 286, 500, 380, 528, 13], "temperature": 0.0, "avg_logprob": -0.1122165602080676, "compression_ratio": 1.723809523809524, "no_speech_prob": 7.76688102632761e-06}, {"id": 667, "seek": 442228, "start": 4422.28, "end": 4427.0, "text": " So this is just because people don't normally do this kind of stuff with embeddings, so", "tokens": [407, 341, 307, 445, 570, 561, 500, 380, 5646, 360, 341, 733, 295, 1507, 365, 12240, 29432, 11, 370], "temperature": 0.0, "avg_logprob": -0.12375745586320466, "compression_ratio": 1.7802690582959642, "no_speech_prob": 3.6119520245847525e-06}, {"id": 668, "seek": 442228, "start": 4427.0, "end": 4431.16, "text": " they're assuming that you're going to want it in a format ready to go to an RNN, so this", "tokens": [436, 434, 11926, 300, 291, 434, 516, 281, 528, 309, 294, 257, 7877, 1919, 281, 352, 281, 364, 45702, 45, 11, 370, 341], "temperature": 0.0, "avg_logprob": -0.12375745586320466, "compression_ratio": 1.7802690582959642, "no_speech_prob": 3.6119520245847525e-06}, {"id": 669, "seek": 442228, "start": 4431.16, "end": 4434.48, "text": " is just turning it back into a normal format.", "tokens": [307, 445, 6246, 309, 646, 666, 257, 2710, 7877, 13], "temperature": 0.0, "avg_logprob": -0.12375745586320466, "compression_ratio": 1.7802690582959642, "no_speech_prob": 3.6119520245847525e-06}, {"id": 670, "seek": 442228, "start": 4434.48, "end": 4441.2, "text": " So we grab each embedding, we end up with a whole list of those. We then combine all", "tokens": [407, 321, 4444, 1184, 12240, 3584, 11, 321, 917, 493, 365, 257, 1379, 1329, 295, 729, 13, 492, 550, 10432, 439], "temperature": 0.0, "avg_logprob": -0.12375745586320466, "compression_ratio": 1.7802690582959642, "no_speech_prob": 3.6119520245847525e-06}, {"id": 671, "seek": 442228, "start": 4441.2, "end": 4446.8, "text": " of those embeddings with all of our continuous variables into a single list of variables.", "tokens": [295, 729, 12240, 29432, 365, 439, 295, 527, 10957, 9102, 666, 257, 2167, 1329, 295, 9102, 13], "temperature": 0.0, "avg_logprob": -0.12375745586320466, "compression_ratio": 1.7802690582959642, "no_speech_prob": 3.6119520245847525e-06}, {"id": 672, "seek": 444680, "start": 4446.8, "end": 4452.9800000000005, "text": " And so then our model is going to have all of those embedding inputs and all of our continuous", "tokens": [400, 370, 550, 527, 2316, 307, 516, 281, 362, 439, 295, 729, 12240, 3584, 15743, 293, 439, 295, 527, 10957], "temperature": 0.0, "avg_logprob": -0.1438278649982653, "compression_ratio": 1.361904761904762, "no_speech_prob": 5.09366373080411e-06}, {"id": 673, "seek": 444680, "start": 4452.9800000000005, "end": 4459.96, "text": " inputs, and then we can compile it and train it.", "tokens": [15743, 11, 293, 550, 321, 393, 31413, 309, 293, 3847, 309, 13], "temperature": 0.0, "avg_logprob": -0.1438278649982653, "compression_ratio": 1.361904761904762, "no_speech_prob": 5.09366373080411e-06}, {"id": 674, "seek": 445996, "start": 4459.96, "end": 4487.78, "text": " So let's take a break and see you back here at 5 past 8.", "tokens": [407, 718, 311, 747, 257, 1821, 293, 536, 291, 646, 510, 412, 1025, 1791, 1649, 13], "temperature": 0.0, "avg_logprob": -0.09086042642593384, "compression_ratio": 0.8888888888888888, "no_speech_prob": 4.198047463432886e-05}, {"id": 675, "seek": 448778, "start": 4487.78, "end": 4505.24, "text": " So we've got our neural net set up. We train it in the usual way, go.fit, and away we go.", "tokens": [407, 321, 600, 658, 527, 18161, 2533, 992, 493, 13, 492, 3847, 309, 294, 264, 7713, 636, 11, 352, 13, 6845, 11, 293, 1314, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.1659081059117471, "compression_ratio": 1.0853658536585367, "no_speech_prob": 3.269823719165288e-05}, {"id": 676, "seek": 450524, "start": 4505.24, "end": 4521.679999999999, "text": " So that's basically that. It trains reasonably quickly, 6 minutes in this case.", "tokens": [407, 300, 311, 1936, 300, 13, 467, 16329, 23551, 2661, 11, 1386, 2077, 294, 341, 1389, 13], "temperature": 0.0, "avg_logprob": -0.3156953454017639, "compression_ratio": 1.3642857142857143, "no_speech_prob": 1.7330291939288145e-06}, {"id": 677, "seek": 450524, "start": 4521.679999999999, "end": 4527.2, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.3156953454017639, "compression_ratio": 1.3642857142857143, "no_speech_prob": 1.7330291939288145e-06}, {"id": 678, "seek": 450524, "start": 4527.2, "end": 4531.639999999999, "text": " One of them is, for the normalization, is it possible to use another function other", "tokens": [1485, 295, 552, 307, 11, 337, 264, 2710, 2144, 11, 307, 309, 1944, 281, 764, 1071, 2445, 661], "temperature": 0.0, "avg_logprob": -0.3156953454017639, "compression_ratio": 1.3642857142857143, "no_speech_prob": 1.7330291939288145e-06}, {"id": 679, "seek": 453164, "start": 4531.64, "end": 4540.08, "text": " than log such as sigmoid?", "tokens": [813, 3565, 1270, 382, 4556, 3280, 327, 30], "temperature": 0.0, "avg_logprob": -0.16291298866271972, "compression_ratio": 1.530612244897959, "no_speech_prob": 1.863151373981964e-05}, {"id": 680, "seek": 453164, "start": 4540.08, "end": 4545.12, "text": " I don't think you'd want to use sigmoid. That kind of financial data and sales data and", "tokens": [286, 500, 380, 519, 291, 1116, 528, 281, 764, 4556, 3280, 327, 13, 663, 733, 295, 4669, 1412, 293, 5763, 1412, 293], "temperature": 0.0, "avg_logprob": -0.16291298866271972, "compression_ratio": 1.530612244897959, "no_speech_prob": 1.863151373981964e-05}, {"id": 681, "seek": 453164, "start": 4545.12, "end": 4551.04, "text": " stuff tends to be of a shape where log will make it more linear, which is generally what", "tokens": [1507, 12258, 281, 312, 295, 257, 3909, 689, 3565, 486, 652, 309, 544, 8213, 11, 597, 307, 5101, 437], "temperature": 0.0, "avg_logprob": -0.16291298866271972, "compression_ratio": 1.530612244897959, "no_speech_prob": 1.863151373981964e-05}, {"id": 682, "seek": 453164, "start": 4551.04, "end": 4552.12, "text": " you want.", "tokens": [291, 528, 13], "temperature": 0.0, "avg_logprob": -0.16291298866271972, "compression_ratio": 1.530612244897959, "no_speech_prob": 1.863151373981964e-05}, {"id": 683, "seek": 453164, "start": 4552.12, "end": 4557.56, "text": " And then when we log transform our target variable, we're also transforming the squared", "tokens": [400, 550, 562, 321, 3565, 4088, 527, 3779, 7006, 11, 321, 434, 611, 27210, 264, 8889], "temperature": 0.0, "avg_logprob": -0.16291298866271972, "compression_ratio": 1.530612244897959, "no_speech_prob": 1.863151373981964e-05}, {"id": 684, "seek": 455756, "start": 4557.56, "end": 4562.280000000001, "text": " error. Is this a problem or is it helping the model to find a better minimum error in", "tokens": [6713, 13, 1119, 341, 257, 1154, 420, 307, 309, 4315, 264, 2316, 281, 915, 257, 1101, 7285, 6713, 294], "temperature": 0.0, "avg_logprob": -0.2639715543357275, "compression_ratio": 1.5836909871244635, "no_speech_prob": 2.8856527933385223e-05}, {"id": 685, "seek": 455756, "start": 4562.280000000001, "end": 4564.320000000001, "text": " the untransformed space?", "tokens": [264, 1701, 25392, 22892, 1901, 30], "temperature": 0.0, "avg_logprob": -0.2639715543357275, "compression_ratio": 1.5836909871244635, "no_speech_prob": 2.8856527933385223e-05}, {"id": 686, "seek": 455756, "start": 4564.320000000001, "end": 4567.04, "text": " So you've got to be careful about what loss function you want. In this case, the Calc", "tokens": [407, 291, 600, 658, 281, 312, 5026, 466, 437, 4470, 2445, 291, 528, 13, 682, 341, 1389, 11, 264, 3511, 66], "temperature": 0.0, "avg_logprob": -0.2639715543357275, "compression_ratio": 1.5836909871244635, "no_speech_prob": 2.8856527933385223e-05}, {"id": 687, "seek": 455756, "start": 4567.04, "end": 4572.56, "text": " competition is trying to minimize root and mean squared percent error. So I actually", "tokens": [6211, 307, 1382, 281, 17522, 5593, 293, 914, 8889, 3043, 6713, 13, 407, 286, 767], "temperature": 0.0, "avg_logprob": -0.2639715543357275, "compression_ratio": 1.5836909871244635, "no_speech_prob": 2.8856527933385223e-05}, {"id": 688, "seek": 455756, "start": 4572.56, "end": 4581.72, "text": " then said, I want you to do mean absolute error. Because in log space, that's basically", "tokens": [550, 848, 11, 286, 528, 291, 281, 360, 914, 8236, 6713, 13, 1436, 294, 3565, 1901, 11, 300, 311, 1936], "temperature": 0.0, "avg_logprob": -0.2639715543357275, "compression_ratio": 1.5836909871244635, "no_speech_prob": 2.8856527933385223e-05}, {"id": 689, "seek": 458172, "start": 4581.72, "end": 4588.2, "text": " doing the same thing. The percent is a ratio, so this is the absolute error between two", "tokens": [884, 264, 912, 551, 13, 440, 3043, 307, 257, 8509, 11, 370, 341, 307, 264, 8236, 6713, 1296, 732], "temperature": 0.0, "avg_logprob": -0.19660170013840134, "compression_ratio": 1.5691489361702127, "no_speech_prob": 4.710863777290797e-06}, {"id": 690, "seek": 458172, "start": 4588.2, "end": 4592.6, "text": " logs which is basically the same as a ratio. So you need to make sure your loss function", "tokens": [20820, 597, 307, 1936, 264, 912, 382, 257, 8509, 13, 407, 291, 643, 281, 652, 988, 428, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.19660170013840134, "compression_ratio": 1.5691489361702127, "no_speech_prob": 4.710863777290797e-06}, {"id": 691, "seek": 458172, "start": 4592.6, "end": 4598.6, "text": " is appropriate in that space.", "tokens": [307, 6854, 294, 300, 1901, 13], "temperature": 0.0, "avg_logprob": -0.19660170013840134, "compression_ratio": 1.5691489361702127, "no_speech_prob": 4.710863777290797e-06}, {"id": 692, "seek": 458172, "start": 4598.6, "end": 4602.96, "text": " I think this is one of the things they didn't do in the original competition. As you can", "tokens": [286, 519, 341, 307, 472, 295, 264, 721, 436, 994, 380, 360, 294, 264, 3380, 6211, 13, 1018, 291, 393], "temperature": 0.0, "avg_logprob": -0.19660170013840134, "compression_ratio": 1.5691489361702127, "no_speech_prob": 4.710863777290797e-06}, {"id": 693, "seek": 460296, "start": 4602.96, "end": 4617.4800000000005, "text": " see I tried changing it and I think it helped. By the way, XGBoost is fantastic. Here is", "tokens": [536, 286, 3031, 4473, 309, 293, 286, 519, 309, 4254, 13, 3146, 264, 636, 11, 1783, 8769, 78, 555, 307, 5456, 13, 1692, 307], "temperature": 0.0, "avg_logprob": -0.1375248336791992, "compression_ratio": 1.4833333333333334, "no_speech_prob": 3.500809953038697e-06}, {"id": 694, "seek": 460296, "start": 4617.4800000000005, "end": 4625.0, "text": " the same series of steps to run this model with XGBoost. As you can see, I just concatenate", "tokens": [264, 912, 2638, 295, 4439, 281, 1190, 341, 2316, 365, 1783, 8769, 78, 555, 13, 1018, 291, 393, 536, 11, 286, 445, 1588, 7186, 473], "temperature": 0.0, "avg_logprob": -0.1375248336791992, "compression_ratio": 1.4833333333333334, "no_speech_prob": 3.500809953038697e-06}, {"id": 695, "seek": 460296, "start": 4625.0, "end": 4631.8, "text": " my categorical and continuous for training and for my validation set. Here is a set of", "tokens": [452, 19250, 804, 293, 10957, 337, 3097, 293, 337, 452, 24071, 992, 13, 1692, 307, 257, 992, 295], "temperature": 0.0, "avg_logprob": -0.1375248336791992, "compression_ratio": 1.4833333333333334, "no_speech_prob": 3.500809953038697e-06}, {"id": 696, "seek": 463180, "start": 4631.8, "end": 4641.72, "text": " parameters which tends to work pretty well. XGBoost has a data type called D matrix, a", "tokens": [9834, 597, 12258, 281, 589, 1238, 731, 13, 1783, 8769, 78, 555, 575, 257, 1412, 2010, 1219, 413, 8141, 11, 257], "temperature": 0.0, "avg_logprob": -0.19461741166956284, "compression_ratio": 1.5139664804469273, "no_speech_prob": 1.7778229448595084e-05}, {"id": 697, "seek": 463180, "start": 4641.72, "end": 4647.7, "text": " data matrix, which is basically a normal matrix but it keeps track of the names of the features", "tokens": [1412, 8141, 11, 597, 307, 1936, 257, 2710, 8141, 457, 309, 5965, 2837, 295, 264, 5288, 295, 264, 4122], "temperature": 0.0, "avg_logprob": -0.19461741166956284, "compression_ratio": 1.5139664804469273, "no_speech_prob": 1.7778229448595084e-05}, {"id": 698, "seek": 463180, "start": 4647.7, "end": 4656.12, "text": " so it prints out better information. Then you go.train and this takes less than a second", "tokens": [370, 309, 22305, 484, 1101, 1589, 13, 1396, 291, 352, 2411, 83, 7146, 293, 341, 2516, 1570, 813, 257, 1150], "temperature": 0.0, "avg_logprob": -0.19461741166956284, "compression_ratio": 1.5139664804469273, "no_speech_prob": 1.7778229448595084e-05}, {"id": 699, "seek": 465612, "start": 4656.12, "end": 4665.92, "text": " to run and it's not massively worse than our previous result. So this is a good way to", "tokens": [281, 1190, 293, 309, 311, 406, 29379, 5324, 813, 527, 3894, 1874, 13, 407, 341, 307, 257, 665, 636, 281], "temperature": 0.0, "avg_logprob": -0.15571769801053134, "compression_ratio": 1.609865470852018, "no_speech_prob": 7.411180376948323e-06}, {"id": 700, "seek": 465612, "start": 4665.92, "end": 4667.48, "text": " get started.", "tokens": [483, 1409, 13], "temperature": 0.0, "avg_logprob": -0.15571769801053134, "compression_ratio": 1.609865470852018, "no_speech_prob": 7.411180376948323e-06}, {"id": 701, "seek": 465612, "start": 4667.48, "end": 4673.24, "text": " The reason that XGBoost and Random Forest is particularly helpful is because it does", "tokens": [440, 1778, 300, 1783, 8769, 78, 555, 293, 37603, 18124, 307, 4098, 4961, 307, 570, 309, 775], "temperature": 0.0, "avg_logprob": -0.15571769801053134, "compression_ratio": 1.609865470852018, "no_speech_prob": 7.411180376948323e-06}, {"id": 702, "seek": 465612, "start": 4673.24, "end": 4678.08, "text": " something called variable importance. So this is how you get the variable importance for", "tokens": [746, 1219, 7006, 7379, 13, 407, 341, 307, 577, 291, 483, 264, 7006, 7379, 337], "temperature": 0.0, "avg_logprob": -0.15571769801053134, "compression_ratio": 1.609865470852018, "no_speech_prob": 7.411180376948323e-06}, {"id": 703, "seek": 465612, "start": 4678.08, "end": 4684.96, "text": " an XGBoost model. So it takes a second and suddenly here is the information you need.", "tokens": [364, 1783, 8769, 78, 555, 2316, 13, 407, 309, 2516, 257, 1150, 293, 5800, 510, 307, 264, 1589, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.15571769801053134, "compression_ratio": 1.609865470852018, "no_speech_prob": 7.411180376948323e-06}, {"id": 704, "seek": 468496, "start": 4684.96, "end": 4694.12, "text": " So when I was having trouble replicating the original results from the 3rd place winners,", "tokens": [407, 562, 286, 390, 1419, 5253, 3248, 30541, 264, 3380, 3542, 490, 264, 805, 7800, 1081, 17193, 11], "temperature": 0.0, "avg_logprob": -0.18972733580035928, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.1300639926048461e-05}, {"id": 705, "seek": 468496, "start": 4694.12, "end": 4698.4800000000005, "text": " one of the things that helped me a lot was to look at this feature importance plot and", "tokens": [472, 295, 264, 721, 300, 4254, 385, 257, 688, 390, 281, 574, 412, 341, 4111, 7379, 7542, 293], "temperature": 0.0, "avg_logprob": -0.18972733580035928, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.1300639926048461e-05}, {"id": 706, "seek": 468496, "start": 4698.4800000000005, "end": 4704.68, "text": " say, OK, competition distance, holy cow, that's really really important, let's make sure that", "tokens": [584, 11, 2264, 11, 6211, 4560, 11, 10622, 8408, 11, 300, 311, 534, 534, 1021, 11, 718, 311, 652, 988, 300], "temperature": 0.0, "avg_logprob": -0.18972733580035928, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.1300639926048461e-05}, {"id": 707, "seek": 468496, "start": 4704.68, "end": 4710.56, "text": " my competition distance pre-processing results really is exactly the same.", "tokens": [452, 6211, 4560, 659, 12, 41075, 278, 3542, 534, 307, 2293, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.18972733580035928, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.1300639926048461e-05}, {"id": 708, "seek": 471056, "start": 4710.56, "end": 4719.8, "text": " On the other hand, events doesn't really matter at all, so I'm not going to worry at all about", "tokens": [1282, 264, 661, 1011, 11, 3931, 1177, 380, 534, 1871, 412, 439, 11, 370, 286, 478, 406, 516, 281, 3292, 412, 439, 466], "temperature": 0.0, "avg_logprob": -0.16653105038315502, "compression_ratio": 1.4808743169398908, "no_speech_prob": 1.0616015060804784e-05}, {"id": 709, "seek": 471056, "start": 4719.8, "end": 4726.68, "text": " checking my events. This feature importance or variable importance plot, also as it's", "tokens": [8568, 452, 3931, 13, 639, 4111, 7379, 420, 7006, 7379, 7542, 11, 611, 382, 309, 311], "temperature": 0.0, "avg_logprob": -0.16653105038315502, "compression_ratio": 1.4808743169398908, "no_speech_prob": 1.0616015060804784e-05}, {"id": 710, "seek": 471056, "start": 4726.68, "end": 4733.280000000001, "text": " known, you can also create with a random forest. These are amazing. Because you're using a", "tokens": [2570, 11, 291, 393, 611, 1884, 365, 257, 4974, 6719, 13, 1981, 366, 2243, 13, 1436, 291, 434, 1228, 257], "temperature": 0.0, "avg_logprob": -0.16653105038315502, "compression_ratio": 1.4808743169398908, "no_speech_prob": 1.0616015060804784e-05}, {"id": 711, "seek": 473328, "start": 4733.28, "end": 4741.28, "text": " tree ensemble, it doesn't matter the shape of anything, it doesn't matter if you have", "tokens": [4230, 19492, 11, 309, 1177, 380, 1871, 264, 3909, 295, 1340, 11, 309, 1177, 380, 1871, 498, 291, 362], "temperature": 0.0, "avg_logprob": -0.1651925826340579, "compression_ratio": 1.661904761904762, "no_speech_prob": 4.936959612678038e-06}, {"id": 712, "seek": 473328, "start": 4741.28, "end": 4750.679999999999, "text": " or don't have interactions, this is all totally assumption-free. In real life, this is the", "tokens": [420, 500, 380, 362, 13280, 11, 341, 307, 439, 3879, 15302, 12, 10792, 13, 682, 957, 993, 11, 341, 307, 264], "temperature": 0.0, "avg_logprob": -0.1651925826340579, "compression_ratio": 1.661904761904762, "no_speech_prob": 4.936959612678038e-06}, {"id": 713, "seek": 473328, "start": 4750.679999999999, "end": 4756.48, "text": " first thing I do. The first thing I do is try to get a feature importance plot printed.", "tokens": [700, 551, 286, 360, 13, 440, 700, 551, 286, 360, 307, 853, 281, 483, 257, 4111, 7379, 7542, 13567, 13], "temperature": 0.0, "avg_logprob": -0.1651925826340579, "compression_ratio": 1.661904761904762, "no_speech_prob": 4.936959612678038e-06}, {"id": 714, "seek": 473328, "start": 4756.48, "end": 4761.599999999999, "text": " Because that tells me, often it turns out there's only 3 or 4 variables that matter.", "tokens": [1436, 300, 5112, 385, 11, 2049, 309, 4523, 484, 456, 311, 787, 805, 420, 1017, 9102, 300, 1871, 13], "temperature": 0.0, "avg_logprob": -0.1651925826340579, "compression_ratio": 1.661904761904762, "no_speech_prob": 4.936959612678038e-06}, {"id": 715, "seek": 476160, "start": 4761.6, "end": 4766.320000000001, "text": " Like if you've got 10,000 variables, so I worked on a big credit scoring problem a couple", "tokens": [1743, 498, 291, 600, 658, 1266, 11, 1360, 9102, 11, 370, 286, 2732, 322, 257, 955, 5397, 22358, 1154, 257, 1916], "temperature": 0.0, "avg_logprob": -0.13201108068790077, "compression_ratio": 1.746212121212121, "no_speech_prob": 1.9222579794586636e-05}, {"id": 716, "seek": 476160, "start": 4766.320000000001, "end": 4772.6, "text": " of years ago, I had 9,500 variables, it turned out that only 9 of them mattered. So the company", "tokens": [295, 924, 2057, 11, 286, 632, 1722, 11, 7526, 9102, 11, 309, 3574, 484, 300, 787, 1722, 295, 552, 44282, 13, 407, 264, 2237], "temperature": 0.0, "avg_logprob": -0.13201108068790077, "compression_ratio": 1.746212121212121, "no_speech_prob": 1.9222579794586636e-05}, {"id": 717, "seek": 476160, "start": 4772.6, "end": 4778.320000000001, "text": " I was working for, literally it spent something like $5 million on this big management consulting", "tokens": [286, 390, 1364, 337, 11, 3736, 309, 4418, 746, 411, 1848, 20, 2459, 322, 341, 955, 4592, 23682], "temperature": 0.0, "avg_logprob": -0.13201108068790077, "compression_ratio": 1.746212121212121, "no_speech_prob": 1.9222579794586636e-05}, {"id": 718, "seek": 476160, "start": 4778.320000000001, "end": 4782.64, "text": " project, and this big management consulting project had told them all these ways in which", "tokens": [1716, 11, 293, 341, 955, 4592, 23682, 1716, 632, 1907, 552, 439, 613, 2098, 294, 597], "temperature": 0.0, "avg_logprob": -0.13201108068790077, "compression_ratio": 1.746212121212121, "no_speech_prob": 1.9222579794586636e-05}, {"id": 719, "seek": 476160, "start": 4782.64, "end": 4787.400000000001, "text": " they can capture all this information in this really clean way for their credit scoring", "tokens": [436, 393, 7983, 439, 341, 1589, 294, 341, 534, 2541, 636, 337, 641, 5397, 22358], "temperature": 0.0, "avg_logprob": -0.13201108068790077, "compression_ratio": 1.746212121212121, "no_speech_prob": 1.9222579794586636e-05}, {"id": 720, "seek": 478740, "start": 4787.4, "end": 4792.48, "text": " models, and of course none of those things were in these 9 that mattered. So they could", "tokens": [5245, 11, 293, 295, 1164, 6022, 295, 729, 721, 645, 294, 613, 1722, 300, 44282, 13, 407, 436, 727], "temperature": 0.0, "avg_logprob": -0.17499123982020787, "compression_ratio": 1.4840425531914894, "no_speech_prob": 4.710880148195429e-06}, {"id": 721, "seek": 478740, "start": 4792.48, "end": 4799.16, "text": " have saved $5 million, but they didn't. Because management consulting companies don't use", "tokens": [362, 6624, 1848, 20, 2459, 11, 457, 436, 994, 380, 13, 1436, 4592, 23682, 3431, 500, 380, 764], "temperature": 0.0, "avg_logprob": -0.17499123982020787, "compression_ratio": 1.4840425531914894, "no_speech_prob": 4.710880148195429e-06}, {"id": 722, "seek": 478740, "start": 4799.16, "end": 4801.5599999999995, "text": " random forests.", "tokens": [4974, 21700, 13], "temperature": 0.0, "avg_logprob": -0.17499123982020787, "compression_ratio": 1.4840425531914894, "no_speech_prob": 4.710880148195429e-06}, {"id": 723, "seek": 478740, "start": 4801.5599999999995, "end": 4809.339999999999, "text": " So I can't overstate the importance of this plot, but this is a deep learning course,", "tokens": [407, 286, 393, 380, 670, 15406, 264, 7379, 295, 341, 7542, 11, 457, 341, 307, 257, 2452, 2539, 1164, 11], "temperature": 0.0, "avg_logprob": -0.17499123982020787, "compression_ratio": 1.4840425531914894, "no_speech_prob": 4.710880148195429e-06}, {"id": 724, "seek": 480934, "start": 4809.34, "end": 4817.9800000000005, "text": " so we're not really going to spend time talking about it. I mentioned that I had a whole bunch", "tokens": [370, 321, 434, 406, 534, 516, 281, 3496, 565, 1417, 466, 309, 13, 286, 2835, 300, 286, 632, 257, 1379, 3840], "temperature": 0.0, "avg_logprob": -0.20712110270624576, "compression_ratio": 1.5139664804469273, "no_speech_prob": 1.706172156445973e-06}, {"id": 725, "seek": 480934, "start": 4817.9800000000005, "end": 4826.2, "text": " of really, really, really weird things in the way that the competition, the place-getters,", "tokens": [295, 534, 11, 534, 11, 534, 3657, 721, 294, 264, 636, 300, 264, 6211, 11, 264, 1081, 12, 847, 1559, 11], "temperature": 0.0, "avg_logprob": -0.20712110270624576, "compression_ratio": 1.5139664804469273, "no_speech_prob": 1.706172156445973e-06}, {"id": 726, "seek": 480934, "start": 4826.2, "end": 4836.24, "text": " did things. For one, they didn't normalize their continuous variables. Who does that?", "tokens": [630, 721, 13, 1171, 472, 11, 436, 994, 380, 2710, 1125, 641, 10957, 9102, 13, 2102, 775, 300, 30], "temperature": 0.0, "avg_logprob": -0.20712110270624576, "compression_ratio": 1.5139664804469273, "no_speech_prob": 1.706172156445973e-06}, {"id": 727, "seek": 483624, "start": 4836.24, "end": 4847.34, "text": " But then when people do well in a competition, something's working. The ways in which they", "tokens": [583, 550, 562, 561, 360, 731, 294, 257, 6211, 11, 746, 311, 1364, 13, 440, 2098, 294, 597, 436], "temperature": 0.0, "avg_logprob": -0.18143033981323242, "compression_ratio": 1.3846153846153846, "no_speech_prob": 1.067700509338465e-06}, {"id": 728, "seek": 483624, "start": 4847.34, "end": 4853.92, "text": " initialized their embeddings were really, really weird. But all these things were really,", "tokens": [5883, 1602, 641, 12240, 29432, 645, 534, 11, 534, 3657, 13, 583, 439, 613, 721, 645, 534, 11], "temperature": 0.0, "avg_logprob": -0.18143033981323242, "compression_ratio": 1.3846153846153846, "no_speech_prob": 1.067700509338465e-06}, {"id": 729, "seek": 485392, "start": 4853.92, "end": 4870.8, "text": " really weird. So what I did was I wrote a little script, Rosman Experiments. What I", "tokens": [534, 3657, 13, 407, 437, 286, 630, 390, 286, 4114, 257, 707, 5755, 11, 11144, 1601, 12522, 8321, 13, 708, 286], "temperature": 0.0, "avg_logprob": -0.1945727242363824, "compression_ratio": 1.3307692307692307, "no_speech_prob": 1.653681692914688e-06}, {"id": 730, "seek": 485392, "start": 4870.8, "end": 4876.92, "text": " did was basically I copied and pasted all the important code out of my notebook. Remember", "tokens": [630, 390, 1936, 286, 25365, 293, 1791, 292, 439, 264, 1021, 3089, 484, 295, 452, 21060, 13, 5459], "temperature": 0.0, "avg_logprob": -0.1945727242363824, "compression_ratio": 1.3307692307692307, "no_speech_prob": 1.653681692914688e-06}, {"id": 731, "seek": 487692, "start": 4876.92, "end": 4884.2, "text": " I've already pickled the parameters for the labeling coder and the scalar, so I didn't", "tokens": [286, 600, 1217, 38076, 264, 9834, 337, 264, 40244, 17656, 260, 293, 264, 39684, 11, 370, 286, 994, 380], "temperature": 0.0, "avg_logprob": -0.21820788383483886, "compression_ratio": 1.5252525252525253, "no_speech_prob": 3.905453013430815e-06}, {"id": 732, "seek": 487692, "start": 4884.2, "end": 4888.2, "text": " have to worry about doing those again.", "tokens": [362, 281, 3292, 466, 884, 729, 797, 13], "temperature": 0.0, "avg_logprob": -0.21820788383483886, "compression_ratio": 1.5252525252525253, "no_speech_prob": 3.905453013430815e-06}, {"id": 733, "seek": 487692, "start": 4888.2, "end": 4892.96, "text": " Once I copied and pasted all that code in, so this is exactly all the code you just saw,", "tokens": [3443, 286, 25365, 293, 1791, 292, 439, 300, 3089, 294, 11, 370, 341, 307, 2293, 439, 264, 3089, 291, 445, 1866, 11], "temperature": 0.0, "avg_logprob": -0.21820788383483886, "compression_ratio": 1.5252525252525253, "no_speech_prob": 3.905453013430815e-06}, {"id": 734, "seek": 487692, "start": 4892.96, "end": 4906.76, "text": " I then had this bunch of for loops, pretty inelegant. But these are all the things that", "tokens": [286, 550, 632, 341, 3840, 295, 337, 16121, 11, 1238, 7167, 6363, 394, 13, 583, 613, 366, 439, 264, 721, 300], "temperature": 0.0, "avg_logprob": -0.21820788383483886, "compression_ratio": 1.5252525252525253, "no_speech_prob": 3.905453013430815e-06}, {"id": 735, "seek": 490676, "start": 4906.76, "end": 4915.96, "text": " I wanted to basically find out. Does it matter whether or not you use 1-0 scaling? Does it", "tokens": [286, 1415, 281, 1936, 915, 484, 13, 4402, 309, 1871, 1968, 420, 406, 291, 764, 502, 12, 15, 21589, 30, 4402, 309], "temperature": 0.0, "avg_logprob": -0.10441086931926448, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.507117278582882e-05}, {"id": 736, "seek": 490676, "start": 4915.96, "end": 4919.68, "text": " matter whether you use their weird approach to initializing embeddings? Does it matter", "tokens": [1871, 1968, 291, 764, 641, 3657, 3109, 281, 5883, 3319, 12240, 29432, 30, 4402, 309, 1871], "temperature": 0.0, "avg_logprob": -0.10441086931926448, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.507117278582882e-05}, {"id": 737, "seek": 490676, "start": 4919.68, "end": 4925.96, "text": " whether you use their particular dictionary of embedding dimensions or use my simple little", "tokens": [1968, 291, 764, 641, 1729, 25890, 295, 12240, 3584, 12819, 420, 764, 452, 2199, 707], "temperature": 0.0, "avg_logprob": -0.10441086931926448, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.507117278582882e-05}, {"id": 738, "seek": 490676, "start": 4925.96, "end": 4928.64, "text": " formula?", "tokens": [8513, 30], "temperature": 0.0, "avg_logprob": -0.10441086931926448, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.507117278582882e-05}, {"id": 739, "seek": 490676, "start": 4928.64, "end": 4933.04, "text": " Something else I tried is they basically took all their continuous variables and put them", "tokens": [6595, 1646, 286, 3031, 307, 436, 1936, 1890, 439, 641, 10957, 9102, 293, 829, 552], "temperature": 0.0, "avg_logprob": -0.10441086931926448, "compression_ratio": 1.7777777777777777, "no_speech_prob": 2.507117278582882e-05}, {"id": 740, "seek": 493304, "start": 4933.04, "end": 4937.64, "text": " through a separate little dense layer each. I was like, why don't we put them all together.", "tokens": [807, 257, 4994, 707, 18011, 4583, 1184, 13, 286, 390, 411, 11, 983, 500, 380, 321, 829, 552, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.19036032358805338, "compression_ratio": 1.5474137931034482, "no_speech_prob": 7.766918315610383e-06}, {"id": 741, "seek": 493304, "start": 4937.64, "end": 4943.72, "text": " I also tried some other things like using batch normalization. So I ran this and got", "tokens": [286, 611, 3031, 512, 661, 721, 411, 1228, 15245, 2710, 2144, 13, 407, 286, 5872, 341, 293, 658], "temperature": 0.0, "avg_logprob": -0.19036032358805338, "compression_ratio": 1.5474137931034482, "no_speech_prob": 7.766918315610383e-06}, {"id": 742, "seek": 493304, "start": 4943.72, "end": 4950.28, "text": " back every possible combination of these. This is where you want to be using the script.", "tokens": [646, 633, 1944, 6562, 295, 613, 13, 639, 307, 689, 291, 528, 281, 312, 1228, 264, 5755, 13], "temperature": 0.0, "avg_logprob": -0.19036032358805338, "compression_ratio": 1.5474137931034482, "no_speech_prob": 7.766918315610383e-06}, {"id": 743, "seek": 493304, "start": 4950.28, "end": 4957.36, "text": " I'm not going to tell you that I jumped straight to this. First of all, I spent days screwing", "tokens": [286, 478, 406, 516, 281, 980, 291, 300, 286, 13864, 2997, 281, 341, 13, 2386, 295, 439, 11, 286, 4418, 1708, 5630, 278], "temperature": 0.0, "avg_logprob": -0.19036032358805338, "compression_ratio": 1.5474137931034482, "no_speech_prob": 7.766918315610383e-06}, {"id": 744, "seek": 495736, "start": 4957.36, "end": 4964.2, "text": " around with experiments in a notebook by hand, continually forgetting what I had just done", "tokens": [926, 365, 12050, 294, 257, 21060, 538, 1011, 11, 22277, 25428, 437, 286, 632, 445, 1096], "temperature": 0.0, "avg_logprob": -0.19958378019787015, "compression_ratio": 1.5462555066079295, "no_speech_prob": 8.398012141697109e-06}, {"id": 745, "seek": 495736, "start": 4964.2, "end": 4972.679999999999, "text": " until eventually it took me like an hour to write this. And then of course I pasted it", "tokens": [1826, 4728, 309, 1890, 385, 411, 364, 1773, 281, 2464, 341, 13, 400, 550, 295, 1164, 286, 1791, 292, 309], "temperature": 0.0, "avg_logprob": -0.19958378019787015, "compression_ratio": 1.5462555066079295, "no_speech_prob": 8.398012141697109e-06}, {"id": 746, "seek": 495736, "start": 4972.679999999999, "end": 4982.24, "text": " into Excel. And here it is. Chucked it into a pivot table, used conditional formatting,", "tokens": [666, 19060, 13, 400, 510, 309, 307, 13, 21607, 292, 309, 666, 257, 14538, 3199, 11, 1143, 27708, 39366, 11], "temperature": 0.0, "avg_logprob": -0.19958378019787015, "compression_ratio": 1.5462555066079295, "no_speech_prob": 8.398012141697109e-06}, {"id": 747, "seek": 495736, "start": 4982.24, "end": 4985.92, "text": " and here's my results. So you can see all my different combinations, with and without", "tokens": [293, 510, 311, 452, 3542, 13, 407, 291, 393, 536, 439, 452, 819, 21267, 11, 365, 293, 1553], "temperature": 0.0, "avg_logprob": -0.19958378019787015, "compression_ratio": 1.5462555066079295, "no_speech_prob": 8.398012141697109e-06}, {"id": 748, "seek": 498592, "start": 4985.92, "end": 4993.76, "text": " normalization, with my special function versus their dictionary, using a single dense matrix", "tokens": [2710, 2144, 11, 365, 452, 2121, 2445, 5717, 641, 25890, 11, 1228, 257, 2167, 18011, 8141], "temperature": 0.0, "avg_logprob": -0.18278532643471995, "compression_ratio": 1.5297619047619047, "no_speech_prob": 2.212536128354259e-05}, {"id": 749, "seek": 498592, "start": 4993.76, "end": 5003.4, "text": " versus putting everything together, using my init versus their lack of init.", "tokens": [5717, 3372, 1203, 1214, 11, 1228, 452, 3157, 5717, 641, 5011, 295, 3157, 13], "temperature": 0.0, "avg_logprob": -0.18278532643471995, "compression_ratio": 1.5297619047619047, "no_speech_prob": 2.212536128354259e-05}, {"id": 750, "seek": 498592, "start": 5003.4, "end": 5014.92, "text": " And here is this dark blue here, is what they did. It's full of weird to me. But as you", "tokens": [400, 510, 307, 341, 2877, 3344, 510, 11, 307, 437, 436, 630, 13, 467, 311, 1577, 295, 3657, 281, 385, 13, 583, 382, 291], "temperature": 0.0, "avg_logprob": -0.18278532643471995, "compression_ratio": 1.5297619047619047, "no_speech_prob": 2.212536128354259e-05}, {"id": 751, "seek": 501492, "start": 5014.92, "end": 5020.38, "text": " can see, it's actually the darkest blue. It actually is the best. But then when you kind", "tokens": [393, 536, 11, 309, 311, 767, 264, 33460, 3344, 13, 467, 767, 307, 264, 1151, 13, 583, 550, 562, 291, 733], "temperature": 0.0, "avg_logprob": -0.17849679244192024, "compression_ratio": 1.5982142857142858, "no_speech_prob": 1.112545305659296e-05}, {"id": 752, "seek": 501492, "start": 5020.38, "end": 5027.64, "text": " of zoom out, you realize there's a whole corner over here that's got a couple of 8.6s, it's", "tokens": [295, 8863, 484, 11, 291, 4325, 456, 311, 257, 1379, 4538, 670, 510, 300, 311, 658, 257, 1916, 295, 1649, 13, 21, 82, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.17849679244192024, "compression_ratio": 1.5982142857142858, "no_speech_prob": 1.112545305659296e-05}, {"id": 753, "seek": 501492, "start": 5027.64, "end": 5036.84, "text": " nearly as good, but seems much more consistent. And also more consistent with sanity. Yes,", "tokens": [6217, 382, 665, 11, 457, 2544, 709, 544, 8398, 13, 400, 611, 544, 8398, 365, 47892, 13, 1079, 11], "temperature": 0.0, "avg_logprob": -0.17849679244192024, "compression_ratio": 1.5982142857142858, "no_speech_prob": 1.112545305659296e-05}, {"id": 754, "seek": 501492, "start": 5036.84, "end": 5042.04, "text": " do normalize your data, and yes, do use an appropriate initialization function. And if", "tokens": [360, 2710, 1125, 428, 1412, 11, 293, 2086, 11, 360, 764, 364, 6854, 5883, 2144, 2445, 13, 400, 498], "temperature": 0.0, "avg_logprob": -0.17849679244192024, "compression_ratio": 1.5982142857142858, "no_speech_prob": 1.112545305659296e-05}, {"id": 755, "seek": 504204, "start": 5042.04, "end": 5045.04, "text": " you do those two things, it doesn't really matter what else you do, it's all going to", "tokens": [291, 360, 729, 732, 721, 11, 309, 1177, 380, 534, 1871, 437, 1646, 291, 360, 11, 309, 311, 439, 516, 281], "temperature": 0.0, "avg_logprob": -0.16431834396806735, "compression_ratio": 1.6327433628318584, "no_speech_prob": 2.9944299058115575e-06}, {"id": 756, "seek": 504204, "start": 5045.04, "end": 5046.24, "text": " work fine.", "tokens": [589, 2489, 13], "temperature": 0.0, "avg_logprob": -0.16431834396806735, "compression_ratio": 1.6327433628318584, "no_speech_prob": 2.9944299058115575e-06}, {"id": 757, "seek": 504204, "start": 5046.24, "end": 5052.68, "text": " So what I then did was I created a little spark line in Excel for the actual training", "tokens": [407, 437, 286, 550, 630, 390, 286, 2942, 257, 707, 9908, 1622, 294, 19060, 337, 264, 3539, 3097], "temperature": 0.0, "avg_logprob": -0.16431834396806735, "compression_ratio": 1.6327433628318584, "no_speech_prob": 2.9944299058115575e-06}, {"id": 758, "seek": 504204, "start": 5052.68, "end": 5065.0, "text": " graphs. And so here's their winning one, again,.085. But here's the variance of getting there.", "tokens": [24877, 13, 400, 370, 510, 311, 641, 8224, 472, 11, 797, 11, 2411, 16133, 20, 13, 583, 510, 311, 264, 21977, 295, 1242, 456, 13], "temperature": 0.0, "avg_logprob": -0.16431834396806735, "compression_ratio": 1.6327433628318584, "no_speech_prob": 2.9944299058115575e-06}, {"id": 759, "seek": 504204, "start": 5065.0, "end": 5069.08, "text": " And as you can see, their approach was pretty bumpy, up and down, up and down, up and down.", "tokens": [400, 382, 291, 393, 536, 11, 641, 3109, 390, 1238, 49400, 11, 493, 293, 760, 11, 493, 293, 760, 11, 493, 293, 760, 13], "temperature": 0.0, "avg_logprob": -0.16431834396806735, "compression_ratio": 1.6327433628318584, "no_speech_prob": 2.9944299058115575e-06}, {"id": 760, "seek": 506908, "start": 5069.08, "end": 5078.6, "text": " The second best, on the other hand,.086 rather than.085, is going down very smoothly. And", "tokens": [440, 1150, 1151, 11, 322, 264, 661, 1011, 11, 2411, 16133, 21, 2831, 813, 2411, 16133, 20, 11, 307, 516, 760, 588, 19565, 13, 400], "temperature": 0.0, "avg_logprob": -0.12883109234749002, "compression_ratio": 1.6372093023255814, "no_speech_prob": 1.7061771586668328e-06}, {"id": 761, "seek": 506908, "start": 5078.6, "end": 5083.16, "text": " so that made me think, given that it's in this very stable part of the world, and given", "tokens": [370, 300, 1027, 385, 519, 11, 2212, 300, 309, 311, 294, 341, 588, 8351, 644, 295, 264, 1002, 11, 293, 2212], "temperature": 0.0, "avg_logprob": -0.12883109234749002, "compression_ratio": 1.6372093023255814, "no_speech_prob": 1.7061771586668328e-06}, {"id": 762, "seek": 506908, "start": 5083.16, "end": 5087.48, "text": " that it's training much better, I actually think this is just random chance. It just", "tokens": [300, 309, 311, 3097, 709, 1101, 11, 286, 767, 519, 341, 307, 445, 4974, 2931, 13, 467, 445], "temperature": 0.0, "avg_logprob": -0.12883109234749002, "compression_ratio": 1.6372093023255814, "no_speech_prob": 1.7061771586668328e-06}, {"id": 763, "seek": 506908, "start": 5087.48, "end": 5094.76, "text": " happened to be low in this point. I actually thought this is a better approach. It's more", "tokens": [2011, 281, 312, 2295, 294, 341, 935, 13, 286, 767, 1194, 341, 307, 257, 1101, 3109, 13, 467, 311, 544], "temperature": 0.0, "avg_logprob": -0.12883109234749002, "compression_ratio": 1.6372093023255814, "no_speech_prob": 1.7061771586668328e-06}, {"id": 764, "seek": 509476, "start": 5094.76, "end": 5099.04, "text": " sensible and it's more consistent.", "tokens": [25380, 293, 309, 311, 544, 8398, 13], "temperature": 0.0, "avg_logprob": -0.19272649915594803, "compression_ratio": 1.6, "no_speech_prob": 8.267751582025085e-06}, {"id": 765, "seek": 509476, "start": 5099.04, "end": 5107.64, "text": " So this kind of approach to running experiments, I thought I'd just show you. When you run", "tokens": [407, 341, 733, 295, 3109, 281, 2614, 12050, 11, 286, 1194, 286, 1116, 445, 855, 291, 13, 1133, 291, 1190], "temperature": 0.0, "avg_logprob": -0.19272649915594803, "compression_ratio": 1.6, "no_speech_prob": 8.267751582025085e-06}, {"id": 766, "seek": 509476, "start": 5107.64, "end": 5114.84, "text": " experiments, try and do it in a rigorous way and track both the stability of the approach", "tokens": [12050, 11, 853, 293, 360, 309, 294, 257, 29882, 636, 293, 2837, 1293, 264, 11826, 295, 264, 3109], "temperature": 0.0, "avg_logprob": -0.19272649915594803, "compression_ratio": 1.6, "no_speech_prob": 8.267751582025085e-06}, {"id": 767, "seek": 509476, "start": 5114.84, "end": 5119.8, "text": " as well as the actual result of the approach. So this one here makes so much sense. It's", "tokens": [382, 731, 382, 264, 3539, 1874, 295, 264, 3109, 13, 407, 341, 472, 510, 1669, 370, 709, 2020, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.19272649915594803, "compression_ratio": 1.6, "no_speech_prob": 8.267751582025085e-06}, {"id": 768, "seek": 511980, "start": 5119.8, "end": 5126.04, "text": " like, use my simple function rather than their weird dictionary, use normalization, use a", "tokens": [411, 11, 764, 452, 2199, 2445, 2831, 813, 641, 3657, 25890, 11, 764, 2710, 2144, 11, 764, 257], "temperature": 0.0, "avg_logprob": -0.19830720218611353, "compression_ratio": 1.5681818181818181, "no_speech_prob": 8.530222658009734e-06}, {"id": 769, "seek": 511980, "start": 5126.04, "end": 5130.4400000000005, "text": " single dense matrix and use a thoughtful initialization. And you do all of those things, you end up", "tokens": [2167, 18011, 8141, 293, 764, 257, 21566, 5883, 2144, 13, 400, 291, 360, 439, 295, 729, 721, 11, 291, 917, 493], "temperature": 0.0, "avg_logprob": -0.19830720218611353, "compression_ratio": 1.5681818181818181, "no_speech_prob": 8.530222658009734e-06}, {"id": 770, "seek": 511980, "start": 5130.4400000000005, "end": 5141.76, "text": " with something that basically is good and much more stable.", "tokens": [365, 746, 300, 1936, 307, 665, 293, 709, 544, 8351, 13], "temperature": 0.0, "avg_logprob": -0.19830720218611353, "compression_ratio": 1.5681818181818181, "no_speech_prob": 8.530222658009734e-06}, {"id": 771, "seek": 511980, "start": 5141.76, "end": 5147.56, "text": " That's all I wanted to say about Rosman. I'm going to very briefly mention another competition,", "tokens": [663, 311, 439, 286, 1415, 281, 584, 466, 11144, 1601, 13, 286, 478, 516, 281, 588, 10515, 2152, 1071, 6211, 11], "temperature": 0.0, "avg_logprob": -0.19830720218611353, "compression_ratio": 1.5681818181818181, "no_speech_prob": 8.530222658009734e-06}, {"id": 772, "seek": 514756, "start": 5147.56, "end": 5160.240000000001, "text": " which is the Kaggle taxi destination competition.", "tokens": [597, 307, 264, 48751, 22631, 18984, 12236, 6211, 13], "temperature": 0.0, "avg_logprob": -0.22625805650438582, "compression_ratio": 1.5657894736842106, "no_speech_prob": 7.253430521814153e-05}, {"id": 773, "seek": 514756, "start": 5160.240000000001, "end": 5167.52, "text": " You were saying that you did a couple of experiments. One, you figured out the embeddings and then", "tokens": [509, 645, 1566, 300, 291, 630, 257, 1916, 295, 12050, 13, 1485, 11, 291, 8932, 484, 264, 12240, 29432, 293, 550], "temperature": 0.0, "avg_logprob": -0.22625805650438582, "compression_ratio": 1.5657894736842106, "no_speech_prob": 7.253430521814153e-05}, {"id": 774, "seek": 514756, "start": 5167.52, "end": 5173.64, "text": " put the embeddings into random forest, and then put embeddings again into neural network.", "tokens": [829, 264, 12240, 29432, 666, 4974, 6719, 11, 293, 550, 829, 12240, 29432, 797, 666, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.22625805650438582, "compression_ratio": 1.5657894736842106, "no_speech_prob": 7.253430521814153e-05}, {"id": 775, "seek": 517364, "start": 5173.64, "end": 5182.8, "text": " I didn't do that, that was from the paper.", "tokens": [286, 994, 380, 360, 300, 11, 300, 390, 490, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.208012233060949, "compression_ratio": 1.5778894472361809, "no_speech_prob": 4.425338829605607e-06}, {"id": 776, "seek": 517364, "start": 5182.8, "end": 5187.8, "text": " So what they did was, for this one here, this 115, they trained the neural network I just", "tokens": [407, 437, 436, 630, 390, 11, 337, 341, 472, 510, 11, 341, 39436, 11, 436, 8895, 264, 18161, 3209, 286, 445], "temperature": 0.0, "avg_logprob": -0.208012233060949, "compression_ratio": 1.5778894472361809, "no_speech_prob": 4.425338829605607e-06}, {"id": 777, "seek": 517364, "start": 5187.8, "end": 5196.360000000001, "text": " showed you. They then threw away the neural network and trained a GBM model. But for the", "tokens": [4712, 291, 13, 814, 550, 11918, 1314, 264, 18161, 3209, 293, 8895, 257, 460, 18345, 2316, 13, 583, 337, 264], "temperature": 0.0, "avg_logprob": -0.208012233060949, "compression_ratio": 1.5778894472361809, "no_speech_prob": 4.425338829605607e-06}, {"id": 778, "seek": 517364, "start": 5196.360000000001, "end": 5200.96, "text": " categorical variables, rather than using one-hot encodings, they used the embeddings. That's", "tokens": [19250, 804, 9102, 11, 2831, 813, 1228, 472, 12, 12194, 2058, 378, 1109, 11, 436, 1143, 264, 12240, 29432, 13, 663, 311], "temperature": 0.0, "avg_logprob": -0.208012233060949, "compression_ratio": 1.5778894472361809, "no_speech_prob": 4.425338829605607e-06}, {"id": 779, "seek": 520096, "start": 5200.96, "end": 5205.4, "text": " all.", "tokens": [439, 13], "temperature": 0.0, "avg_logprob": -0.17122709123711838, "compression_ratio": 1.5, "no_speech_prob": 6.747972292941995e-06}, {"id": 780, "seek": 520096, "start": 5205.4, "end": 5215.2, "text": " The taxi competition was won by the team with this Unicode name, which is pretty cool. It's", "tokens": [440, 18984, 6211, 390, 1582, 538, 264, 1469, 365, 341, 1156, 299, 1429, 1315, 11, 597, 307, 1238, 1627, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.17122709123711838, "compression_ratio": 1.5, "no_speech_prob": 6.747972292941995e-06}, {"id": 781, "seek": 520096, "start": 5215.2, "end": 5221.28, "text": " actually turned out to be a team run by Yoshua Bengio, who's one of the people that stuck", "tokens": [767, 3574, 484, 281, 312, 257, 1469, 1190, 538, 38949, 4398, 29425, 1004, 11, 567, 311, 472, 295, 264, 561, 300, 5541], "temperature": 0.0, "avg_logprob": -0.17122709123711838, "compression_ratio": 1.5, "no_speech_prob": 6.747972292941995e-06}, {"id": 782, "seek": 520096, "start": 5221.28, "end": 5229.8, "text": " it out through the AI winter and is now one of the leading lights in deep learning. Interestingly,", "tokens": [309, 484, 807, 264, 7318, 6355, 293, 307, 586, 472, 295, 264, 5775, 5811, 294, 2452, 2539, 13, 30564, 11], "temperature": 0.0, "avg_logprob": -0.17122709123711838, "compression_ratio": 1.5, "no_speech_prob": 6.747972292941995e-06}, {"id": 783, "seek": 522980, "start": 5229.8, "end": 5235.84, "text": " the thing I just showed you, the Rossman competition, this paper they wrote in the Rossman competition,", "tokens": [264, 551, 286, 445, 4712, 291, 11, 264, 16140, 1601, 6211, 11, 341, 3035, 436, 4114, 294, 264, 16140, 1601, 6211, 11], "temperature": 0.0, "avg_logprob": -0.15214601782865303, "compression_ratio": 1.6411483253588517, "no_speech_prob": 3.3931103189388523e-06}, {"id": 784, "seek": 522980, "start": 5235.84, "end": 5241.96, "text": " they claimed to have invented this idea of categorical embeddings. But actually, Yoshua", "tokens": [436, 12941, 281, 362, 14479, 341, 1558, 295, 19250, 804, 12240, 29432, 13, 583, 767, 11, 38949, 4398], "temperature": 0.0, "avg_logprob": -0.15214601782865303, "compression_ratio": 1.6411483253588517, "no_speech_prob": 3.3931103189388523e-06}, {"id": 785, "seek": 522980, "start": 5241.96, "end": 5248.400000000001, "text": " Bengio's team won this competition a year earlier with his same technique. But again,", "tokens": [29425, 1004, 311, 1469, 1582, 341, 6211, 257, 1064, 3071, 365, 702, 912, 6532, 13, 583, 797, 11], "temperature": 0.0, "avg_logprob": -0.15214601782865303, "compression_ratio": 1.6411483253588517, "no_speech_prob": 3.3931103189388523e-06}, {"id": 786, "seek": 522980, "start": 5248.400000000001, "end": 5252.96, "text": " it's so uncool, nobody noticed, even though it was Yoshua Bengio.", "tokens": [309, 311, 370, 6219, 1092, 11, 5079, 5694, 11, 754, 1673, 309, 390, 38949, 4398, 29425, 1004, 13], "temperature": 0.0, "avg_logprob": -0.15214601782865303, "compression_ratio": 1.6411483253588517, "no_speech_prob": 3.3931103189388523e-06}, {"id": 787, "seek": 525296, "start": 5252.96, "end": 5262.04, "text": " So I want to quickly show you what they did. This is the paper they wrote. Their approach", "tokens": [407, 286, 528, 281, 2661, 855, 291, 437, 436, 630, 13, 639, 307, 264, 3035, 436, 4114, 13, 6710, 3109], "temperature": 0.0, "avg_logprob": -0.09697802861531575, "compression_ratio": 1.5406976744186047, "no_speech_prob": 1.5689236533944495e-05}, {"id": 788, "seek": 525296, "start": 5262.04, "end": 5272.6, "text": " to picking an embedding size was very simple. Use 10. So the data was, which customer is", "tokens": [281, 8867, 364, 12240, 3584, 2744, 390, 588, 2199, 13, 8278, 1266, 13, 407, 264, 1412, 390, 11, 597, 5474, 307], "temperature": 0.0, "avg_logprob": -0.09697802861531575, "compression_ratio": 1.5406976744186047, "no_speech_prob": 1.5689236533944495e-05}, {"id": 789, "seek": 525296, "start": 5272.6, "end": 5280.44, "text": " taking this taxi, which taxi are they in, which taxi stand did they get the taxi from,", "tokens": [1940, 341, 18984, 11, 597, 18984, 366, 436, 294, 11, 597, 18984, 1463, 630, 436, 483, 264, 18984, 490, 11], "temperature": 0.0, "avg_logprob": -0.09697802861531575, "compression_ratio": 1.5406976744186047, "no_speech_prob": 1.5689236533944495e-05}, {"id": 790, "seek": 528044, "start": 5280.44, "end": 5287.24, "text": " and then quarter hour of the day, day of the week, week of the year. And they didn't add", "tokens": [293, 550, 6555, 1773, 295, 264, 786, 11, 786, 295, 264, 1243, 11, 1243, 295, 264, 1064, 13, 400, 436, 994, 380, 909], "temperature": 0.0, "avg_logprob": -0.20593021761986516, "compression_ratio": 1.550660792951542, "no_speech_prob": 6.144071903690929e-06}, {"id": 791, "seek": 528044, "start": 5287.24, "end": 5293.719999999999, "text": " all kinds of other stuff, this is basically it. And so then they said, we're going to", "tokens": [439, 3685, 295, 661, 1507, 11, 341, 307, 1936, 309, 13, 400, 370, 550, 436, 848, 11, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.20593021761986516, "compression_ratio": 1.550660792951542, "no_speech_prob": 6.144071903690929e-06}, {"id": 792, "seek": 528044, "start": 5293.719999999999, "end": 5301.719999999999, "text": " learn embeddings inspired by NLP. So actually to my knowledge, this is the first time this", "tokens": [1466, 12240, 29432, 7547, 538, 426, 45196, 13, 407, 767, 281, 452, 3601, 11, 341, 307, 264, 700, 565, 341], "temperature": 0.0, "avg_logprob": -0.20593021761986516, "compression_ratio": 1.550660792951542, "no_speech_prob": 6.144071903690929e-06}, {"id": 793, "seek": 528044, "start": 5301.719999999999, "end": 5306.759999999999, "text": " appears in the literature. Having said that, I'm sure 1000 people have done it before,", "tokens": [7038, 294, 264, 10394, 13, 10222, 848, 300, 11, 286, 478, 988, 9714, 561, 362, 1096, 309, 949, 11], "temperature": 0.0, "avg_logprob": -0.20593021761986516, "compression_ratio": 1.550660792951542, "no_speech_prob": 6.144071903690929e-06}, {"id": 794, "seek": 530676, "start": 5306.76, "end": 5313.4800000000005, "text": " it's just not obvious to actually make it into a paper.", "tokens": [309, 311, 445, 406, 6322, 281, 767, 652, 309, 666, 257, 3035, 13], "temperature": 0.0, "avg_logprob": -0.24664481093243854, "compression_ratio": 1.4439024390243902, "no_speech_prob": 2.468135062372312e-05}, {"id": 795, "seek": 530676, "start": 5313.4800000000005, "end": 5322.320000000001, "text": " As a quick sanity check, if you have day of the week with 7 one-hot variable potentials,", "tokens": [1018, 257, 1702, 47892, 1520, 11, 498, 291, 362, 786, 295, 264, 1243, 365, 1614, 472, 12, 12194, 7006, 3995, 82, 11], "temperature": 0.0, "avg_logprob": -0.24664481093243854, "compression_ratio": 1.4439024390243902, "no_speech_prob": 2.468135062372312e-05}, {"id": 796, "seek": 530676, "start": 5322.320000000001, "end": 5327.72, "text": " and embedding size of 10, that doesn't make any sense, right?", "tokens": [293, 12240, 3584, 2744, 295, 1266, 11, 300, 1177, 380, 652, 604, 2020, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.24664481093243854, "compression_ratio": 1.4439024390243902, "no_speech_prob": 2.468135062372312e-05}, {"id": 797, "seek": 530676, "start": 5327.72, "end": 5335.68, "text": " I used to think that, but actually it does. I've in the last few months quite often ended", "tokens": [286, 1143, 281, 519, 300, 11, 457, 767, 309, 775, 13, 286, 600, 294, 264, 1036, 1326, 2493, 1596, 2049, 4590], "temperature": 0.0, "avg_logprob": -0.24664481093243854, "compression_ratio": 1.4439024390243902, "no_speech_prob": 2.468135062372312e-05}, {"id": 798, "seek": 533568, "start": 5335.68, "end": 5343.16, "text": " up with bigger embeddings than my original patternality. Often it does give better results.", "tokens": [493, 365, 3801, 12240, 29432, 813, 452, 3380, 5102, 1860, 13, 20043, 309, 775, 976, 1101, 3542, 13], "temperature": 0.0, "avg_logprob": -0.2078580061594645, "compression_ratio": 1.5836909871244635, "no_speech_prob": 1.3211659279477317e-05}, {"id": 799, "seek": 533568, "start": 5343.16, "end": 5348.72, "text": " And I think it's just like, when you realize it's just a dense layer on top of a one-hot", "tokens": [400, 286, 519, 309, 311, 445, 411, 11, 562, 291, 4325, 309, 311, 445, 257, 18011, 4583, 322, 1192, 295, 257, 472, 12, 12194], "temperature": 0.0, "avg_logprob": -0.2078580061594645, "compression_ratio": 1.5836909871244635, "no_speech_prob": 1.3211659279477317e-05}, {"id": 800, "seek": 533568, "start": 5348.72, "end": 5355.16, "text": " encoding, it's like, why shouldn't the dense layer have more information. I found it weird", "tokens": [43430, 11, 309, 311, 411, 11, 983, 4659, 380, 264, 18011, 4583, 362, 544, 1589, 13, 286, 1352, 309, 3657], "temperature": 0.0, "avg_logprob": -0.2078580061594645, "compression_ratio": 1.5836909871244635, "no_speech_prob": 1.3211659279477317e-05}, {"id": 801, "seek": 533568, "start": 5355.16, "end": 5359.320000000001, "text": " too, I still find it a little weird, but it definitely seems to be something that's quite", "tokens": [886, 11, 286, 920, 915, 309, 257, 707, 3657, 11, 457, 309, 2138, 2544, 281, 312, 746, 300, 311, 1596], "temperature": 0.0, "avg_logprob": -0.2078580061594645, "compression_ratio": 1.5836909871244635, "no_speech_prob": 1.3211659279477317e-05}, {"id": 802, "seek": 533568, "start": 5359.320000000001, "end": 5360.320000000001, "text": " useful.", "tokens": [4420, 13], "temperature": 0.0, "avg_logprob": -0.2078580061594645, "compression_ratio": 1.5836909871244635, "no_speech_prob": 1.3211659279477317e-05}, {"id": 803, "seek": 536032, "start": 5360.32, "end": 5380.2, "text": " I've absolutely found plenty of times now where I need a bigger embedding matrix dimensionality", "tokens": [286, 600, 3122, 1352, 7140, 295, 1413, 586, 689, 286, 643, 257, 3801, 12240, 3584, 8141, 10139, 1860], "temperature": 0.0, "avg_logprob": -0.2324941907610212, "compression_ratio": 1.2767857142857142, "no_speech_prob": 4.006080052931793e-05}, {"id": 804, "seek": 536032, "start": 5380.2, "end": 5386.679999999999, "text": " than my cardinality of my categorical variable.", "tokens": [813, 452, 2920, 259, 1860, 295, 452, 19250, 804, 7006, 13], "temperature": 0.0, "avg_logprob": -0.2324941907610212, "compression_ratio": 1.2767857142857142, "no_speech_prob": 4.006080052931793e-05}, {"id": 805, "seek": 538668, "start": 5386.68, "end": 5392.72, "text": " Now in this competition, again it's a time series competition really because the main", "tokens": [823, 294, 341, 6211, 11, 797, 309, 311, 257, 565, 2638, 6211, 534, 570, 264, 2135], "temperature": 0.0, "avg_logprob": -0.17300029184626436, "compression_ratio": 1.6431924882629108, "no_speech_prob": 3.70514317182824e-05}, {"id": 806, "seek": 538668, "start": 5392.72, "end": 5398.84, "text": " thing you're given, other than all this metadata, is a series of GPS points, which is every", "tokens": [551, 291, 434, 2212, 11, 661, 813, 439, 341, 26603, 11, 307, 257, 2638, 295, 19462, 2793, 11, 597, 307, 633], "temperature": 0.0, "avg_logprob": -0.17300029184626436, "compression_ratio": 1.6431924882629108, "no_speech_prob": 3.70514317182824e-05}, {"id": 807, "seek": 538668, "start": 5398.84, "end": 5405.360000000001, "text": " GPS point along a route. And at some point for the test set, the route is cut off and", "tokens": [19462, 935, 2051, 257, 7955, 13, 400, 412, 512, 935, 337, 264, 1500, 992, 11, 264, 7955, 307, 1723, 766, 293], "temperature": 0.0, "avg_logprob": -0.17300029184626436, "compression_ratio": 1.6431924882629108, "no_speech_prob": 3.70514317182824e-05}, {"id": 808, "seek": 538668, "start": 5405.360000000001, "end": 5413.76, "text": " you have to figure out what the final GPS point would have been, where are they going.", "tokens": [291, 362, 281, 2573, 484, 437, 264, 2572, 19462, 935, 576, 362, 668, 11, 689, 366, 436, 516, 13], "temperature": 0.0, "avg_logprob": -0.17300029184626436, "compression_ratio": 1.6431924882629108, "no_speech_prob": 3.70514317182824e-05}, {"id": 809, "seek": 541376, "start": 5413.76, "end": 5421.72, "text": " Here's the model that they won with. It turns out, again, to be very simple. You take all", "tokens": [1692, 311, 264, 2316, 300, 436, 1582, 365, 13, 467, 4523, 484, 11, 797, 11, 281, 312, 588, 2199, 13, 509, 747, 439], "temperature": 0.0, "avg_logprob": -0.16710500490097774, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.771856194769498e-06}, {"id": 810, "seek": 541376, "start": 5421.72, "end": 5428.4800000000005, "text": " the metadata we just saw and chuck it through the embeddings. You then take the first 5", "tokens": [264, 26603, 321, 445, 1866, 293, 20870, 309, 807, 264, 12240, 29432, 13, 509, 550, 747, 264, 700, 1025], "temperature": 0.0, "avg_logprob": -0.16710500490097774, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.771856194769498e-06}, {"id": 811, "seek": 541376, "start": 5428.4800000000005, "end": 5436.16, "text": " GPS points and the last 5 GPS points and concatenate them together with the embeddings, chuck them", "tokens": [19462, 2793, 293, 264, 1036, 1025, 19462, 2793, 293, 1588, 7186, 473, 552, 1214, 365, 264, 12240, 29432, 11, 20870, 552], "temperature": 0.0, "avg_logprob": -0.16710500490097774, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.771856194769498e-06}, {"id": 812, "seek": 541376, "start": 5436.16, "end": 5441.12, "text": " through a hidden layer, then through a softmax.", "tokens": [807, 257, 7633, 4583, 11, 550, 807, 257, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.16710500490097774, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.771856194769498e-06}, {"id": 813, "seek": 544112, "start": 5441.12, "end": 5448.599999999999, "text": " Now this is quite interesting. What they then do is they take the result of this softmax", "tokens": [823, 341, 307, 1596, 1880, 13, 708, 436, 550, 360, 307, 436, 747, 264, 1874, 295, 341, 2787, 41167], "temperature": 0.0, "avg_logprob": -0.14948562953783118, "compression_ratio": 1.849246231155779, "no_speech_prob": 6.144149210740579e-06}, {"id": 814, "seek": 544112, "start": 5448.599999999999, "end": 5455.5199999999995, "text": " and they combine it with clusters. Now what are these clusters? They used mean shift clustering.", "tokens": [293, 436, 10432, 309, 365, 23313, 13, 823, 437, 366, 613, 23313, 30, 814, 1143, 914, 5513, 596, 48673, 13], "temperature": 0.0, "avg_logprob": -0.14948562953783118, "compression_ratio": 1.849246231155779, "no_speech_prob": 6.144149210740579e-06}, {"id": 815, "seek": 544112, "start": 5455.5199999999995, "end": 5461.599999999999, "text": " And they used mean shift clustering to figure out where are the places people tend to go.", "tokens": [400, 436, 1143, 914, 5513, 596, 48673, 281, 2573, 484, 689, 366, 264, 3190, 561, 3928, 281, 352, 13], "temperature": 0.0, "avg_logprob": -0.14948562953783118, "compression_ratio": 1.849246231155779, "no_speech_prob": 6.144149210740579e-06}, {"id": 816, "seek": 544112, "start": 5461.599999999999, "end": 5466.0, "text": " So with taxis, people tend to go to the airport, or they tend to go to the hospital, or they", "tokens": [407, 365, 3366, 271, 11, 561, 3928, 281, 352, 281, 264, 10155, 11, 420, 436, 3928, 281, 352, 281, 264, 4530, 11, 420, 436], "temperature": 0.0, "avg_logprob": -0.14948562953783118, "compression_ratio": 1.849246231155779, "no_speech_prob": 6.144149210740579e-06}, {"id": 817, "seek": 546600, "start": 5466.0, "end": 5472.24, "text": " tend to go to the shopping strip. So using mean shift clustering, they came up with I", "tokens": [3928, 281, 352, 281, 264, 8688, 12828, 13, 407, 1228, 914, 5513, 596, 48673, 11, 436, 1361, 493, 365, 286], "temperature": 0.0, "avg_logprob": -0.16787326600816516, "compression_ratio": 1.617117117117117, "no_speech_prob": 1.4823546052866732e-06}, {"id": 818, "seek": 546600, "start": 5472.24, "end": 5483.04, "text": " think it was about 3000 clusters, x, y coordinates of places that people tended to go. However,", "tokens": [519, 309, 390, 466, 20984, 23313, 11, 2031, 11, 288, 21056, 295, 3190, 300, 561, 34732, 281, 352, 13, 2908, 11], "temperature": 0.0, "avg_logprob": -0.16787326600816516, "compression_ratio": 1.617117117117117, "no_speech_prob": 1.4823546052866732e-06}, {"id": 819, "seek": 546600, "start": 5483.04, "end": 5488.28, "text": " people don't always go to those 3000 places. So this is a really cool thing. By using a", "tokens": [561, 500, 380, 1009, 352, 281, 729, 20984, 3190, 13, 407, 341, 307, 257, 534, 1627, 551, 13, 3146, 1228, 257], "temperature": 0.0, "avg_logprob": -0.16787326600816516, "compression_ratio": 1.617117117117117, "no_speech_prob": 1.4823546052866732e-06}, {"id": 820, "seek": 546600, "start": 5488.28, "end": 5495.4, "text": " softmax and then they took the softmax and they multiplied it and took a weighted average", "tokens": [2787, 41167, 293, 550, 436, 1890, 264, 2787, 41167, 293, 436, 17207, 309, 293, 1890, 257, 32807, 4274], "temperature": 0.0, "avg_logprob": -0.16787326600816516, "compression_ratio": 1.617117117117117, "no_speech_prob": 1.4823546052866732e-06}, {"id": 821, "seek": 549540, "start": 5495.4, "end": 5500.759999999999, "text": " using the softmax as the weights and the cluster centers as the thing that you're taking the", "tokens": [1228, 264, 2787, 41167, 382, 264, 17443, 293, 264, 13630, 10898, 382, 264, 551, 300, 291, 434, 1940, 264], "temperature": 0.0, "avg_logprob": -0.14392561815222915, "compression_ratio": 1.8113207547169812, "no_speech_prob": 6.339184437820222e-06}, {"id": 822, "seek": 549540, "start": 5500.759999999999, "end": 5502.32, "text": " weighted average of.", "tokens": [32807, 4274, 295, 13], "temperature": 0.0, "avg_logprob": -0.14392561815222915, "compression_ratio": 1.8113207547169812, "no_speech_prob": 6.339184437820222e-06}, {"id": 823, "seek": 549540, "start": 5502.32, "end": 5507.54, "text": " So in other words, if they're going to the airport, for sure, the softmax will end up", "tokens": [407, 294, 661, 2283, 11, 498, 436, 434, 516, 281, 264, 10155, 11, 337, 988, 11, 264, 2787, 41167, 486, 917, 493], "temperature": 0.0, "avg_logprob": -0.14392561815222915, "compression_ratio": 1.8113207547169812, "no_speech_prob": 6.339184437820222e-06}, {"id": 824, "seek": 549540, "start": 5507.54, "end": 5513.32, "text": " giving a p of very close to 1 for the airport cluster. On the other hand, if it's not really", "tokens": [2902, 257, 280, 295, 588, 1998, 281, 502, 337, 264, 10155, 13630, 13, 1282, 264, 661, 1011, 11, 498, 309, 311, 406, 534], "temperature": 0.0, "avg_logprob": -0.14392561815222915, "compression_ratio": 1.8113207547169812, "no_speech_prob": 6.339184437820222e-06}, {"id": 825, "seek": 549540, "start": 5513.32, "end": 5521.16, "text": " that clear whether they're going to this shopping strip or this movie, then those 2 cluster", "tokens": [300, 1850, 1968, 436, 434, 516, 281, 341, 8688, 12828, 420, 341, 3169, 11, 550, 729, 568, 13630], "temperature": 0.0, "avg_logprob": -0.14392561815222915, "compression_ratio": 1.8113207547169812, "no_speech_prob": 6.339184437820222e-06}, {"id": 826, "seek": 552116, "start": 5521.16, "end": 5526.5199999999995, "text": " centers could both have a softmax of about 0.5, so it's going to end up predicting somewhere", "tokens": [10898, 727, 1293, 362, 257, 2787, 41167, 295, 466, 1958, 13, 20, 11, 370, 309, 311, 516, 281, 917, 493, 32884, 4079], "temperature": 0.0, "avg_logprob": -0.11744450887044271, "compression_ratio": 1.485, "no_speech_prob": 1.933346538862679e-06}, {"id": 827, "seek": 552116, "start": 5526.5199999999995, "end": 5529.12, "text": " halfway between the 2.", "tokens": [15461, 1296, 264, 568, 13], "temperature": 0.0, "avg_logprob": -0.11744450887044271, "compression_ratio": 1.485, "no_speech_prob": 1.933346538862679e-06}, {"id": 828, "seek": 552116, "start": 5529.12, "end": 5537.28, "text": " So this is really interesting. They've built a different kind of architecture to anything", "tokens": [407, 341, 307, 534, 1880, 13, 814, 600, 3094, 257, 819, 733, 295, 9482, 281, 1340], "temperature": 0.0, "avg_logprob": -0.11744450887044271, "compression_ratio": 1.485, "no_speech_prob": 1.933346538862679e-06}, {"id": 829, "seek": 552116, "start": 5537.28, "end": 5544.0199999999995, "text": " we've seen before where the softmax is not the last thing we do. It's being used to average", "tokens": [321, 600, 1612, 949, 689, 264, 2787, 41167, 307, 406, 264, 1036, 551, 321, 360, 13, 467, 311, 885, 1143, 281, 4274], "temperature": 0.0, "avg_logprob": -0.11744450887044271, "compression_ratio": 1.485, "no_speech_prob": 1.933346538862679e-06}, {"id": 830, "seek": 554402, "start": 5544.02, "end": 5553.040000000001, "text": " a bunch of clusters. So this is really smart because the softmax forces it to be easier", "tokens": [257, 3840, 295, 23313, 13, 407, 341, 307, 534, 4069, 570, 264, 2787, 41167, 5874, 309, 281, 312, 3571], "temperature": 0.0, "avg_logprob": -0.13385693014484562, "compression_ratio": 1.6386138613861385, "no_speech_prob": 4.289288881409448e-06}, {"id": 831, "seek": 554402, "start": 5553.040000000001, "end": 5558.200000000001, "text": " for it to pick a specific destination that's very common, but also makes it possible for", "tokens": [337, 309, 281, 1888, 257, 2685, 12236, 300, 311, 588, 2689, 11, 457, 611, 1669, 309, 1944, 337], "temperature": 0.0, "avg_logprob": -0.13385693014484562, "compression_ratio": 1.6386138613861385, "no_speech_prob": 4.289288881409448e-06}, {"id": 832, "seek": 554402, "start": 5558.200000000001, "end": 5564.240000000001, "text": " it to predict any destination anywhere by combining the average of a number of clusters", "tokens": [309, 281, 6069, 604, 12236, 4992, 538, 21928, 264, 4274, 295, 257, 1230, 295, 23313], "temperature": 0.0, "avg_logprob": -0.13385693014484562, "compression_ratio": 1.6386138613861385, "no_speech_prob": 4.289288881409448e-06}, {"id": 833, "seek": 554402, "start": 5564.240000000001, "end": 5571.400000000001, "text": " together. I think this is really elegant architecture engineering.", "tokens": [1214, 13, 286, 519, 341, 307, 534, 21117, 9482, 7043, 13], "temperature": 0.0, "avg_logprob": -0.13385693014484562, "compression_ratio": 1.6386138613861385, "no_speech_prob": 4.289288881409448e-06}, {"id": 834, "seek": 557140, "start": 5571.4, "end": 5580.2, "text": " So last 4 dots, isn't that what we're trying to predict?", "tokens": [407, 1036, 1017, 15026, 11, 1943, 380, 300, 437, 321, 434, 1382, 281, 6069, 30], "temperature": 0.0, "avg_logprob": -0.19552495744493273, "compression_ratio": 1.4591194968553458, "no_speech_prob": 5.422079084382858e-06}, {"id": 835, "seek": 557140, "start": 5580.2, "end": 5587.4, "text": " Last 5 GPS points that we're given. So to create the training set, what they did was", "tokens": [5264, 1025, 19462, 2793, 300, 321, 434, 2212, 13, 407, 281, 1884, 264, 3097, 992, 11, 437, 436, 630, 390], "temperature": 0.0, "avg_logprob": -0.19552495744493273, "compression_ratio": 1.4591194968553458, "no_speech_prob": 5.422079084382858e-06}, {"id": 836, "seek": 557140, "start": 5587.4, "end": 5595.12, "text": " they took all of the routes and they truncated them randomly basically. So every time they", "tokens": [436, 1890, 439, 295, 264, 18242, 293, 436, 504, 409, 66, 770, 552, 16979, 1936, 13, 407, 633, 565, 436], "temperature": 0.0, "avg_logprob": -0.19552495744493273, "compression_ratio": 1.4591194968553458, "no_speech_prob": 5.422079084382858e-06}, {"id": 837, "seek": 559512, "start": 5595.12, "end": 5602.12, "text": " sampled another route, think of the data generator, basically the data generator would randomly", "tokens": [3247, 15551, 1071, 7955, 11, 519, 295, 264, 1412, 19265, 11, 1936, 264, 1412, 19265, 576, 16979], "temperature": 0.0, "avg_logprob": -0.16477342831191197, "compression_ratio": 1.6069868995633187, "no_speech_prob": 6.276677595451474e-07}, {"id": 838, "seek": 559512, "start": 5602.12, "end": 5611.12, "text": " slice it off somewhere. So this was the last 5 points which we have access to and the first", "tokens": [13153, 309, 766, 4079, 13, 407, 341, 390, 264, 1036, 1025, 2793, 597, 321, 362, 2105, 281, 293, 264, 700], "temperature": 0.0, "avg_logprob": -0.16477342831191197, "compression_ratio": 1.6069868995633187, "no_speech_prob": 6.276677595451474e-07}, {"id": 839, "seek": 559512, "start": 5611.12, "end": 5618.36, "text": " 5 points. The reason it's not all the points is because they're using a standard multilayer", "tokens": [1025, 2793, 13, 440, 1778, 309, 311, 406, 439, 264, 2793, 307, 570, 436, 434, 1228, 257, 3832, 2120, 388, 11167], "temperature": 0.0, "avg_logprob": -0.16477342831191197, "compression_ratio": 1.6069868995633187, "no_speech_prob": 6.276677595451474e-07}, {"id": 840, "seek": 559512, "start": 5618.36, "end": 5624.599999999999, "text": " perceptron here. So it's a variable length, A, and also you don't want it to be too big.", "tokens": [43276, 2044, 510, 13, 407, 309, 311, 257, 7006, 4641, 11, 316, 11, 293, 611, 291, 500, 380, 528, 309, 281, 312, 886, 955, 13], "temperature": 0.0, "avg_logprob": -0.16477342831191197, "compression_ratio": 1.6069868995633187, "no_speech_prob": 6.276677595451474e-07}, {"id": 841, "seek": 562460, "start": 5624.6, "end": 5631.76, "text": " Question. So the prefix is not fed into an RNN, it's just fed into a dense layer?", "tokens": [14464, 13, 407, 264, 46969, 307, 406, 4636, 666, 364, 45702, 45, 11, 309, 311, 445, 4636, 666, 257, 18011, 4583, 30], "temperature": 0.0, "avg_logprob": -0.2357158660888672, "compression_ratio": 1.5294117647058822, "no_speech_prob": 7.411208571284078e-06}, {"id": 842, "seek": 562460, "start": 5631.76, "end": 5637.96, "text": " Correct. So we just get 10 points concatenated together into a dense layer. So surprisingly", "tokens": [12753, 13, 407, 321, 445, 483, 1266, 2793, 1588, 7186, 770, 1214, 666, 257, 18011, 4583, 13, 407, 17600], "temperature": 0.0, "avg_logprob": -0.2357158660888672, "compression_ratio": 1.5294117647058822, "no_speech_prob": 7.411208571284078e-06}, {"id": 843, "seek": 562460, "start": 5637.96, "end": 5652.92, "text": " simple. How good was it? Well, look at the results. 214, 214, 213, 213, 213, 213, 211,", "tokens": [2199, 13, 1012, 665, 390, 309, 30, 1042, 11, 574, 412, 264, 3542, 13, 5080, 19, 11, 5080, 19, 11, 5080, 18, 11, 5080, 18, 11, 5080, 18, 11, 5080, 18, 11, 5080, 16, 11], "temperature": 0.0, "avg_logprob": -0.2357158660888672, "compression_ratio": 1.5294117647058822, "no_speech_prob": 7.411208571284078e-06}, {"id": 844, "seek": 565292, "start": 5652.92, "end": 5659.2, "text": " 214, 212, everybody's clustered together. One person's a bit better at 208 and they're", "tokens": [5080, 19, 11, 5080, 17, 11, 2201, 311, 596, 38624, 1214, 13, 1485, 954, 311, 257, 857, 1101, 412, 945, 23, 293, 436, 434], "temperature": 0.0, "avg_logprob": -0.18803155722738313, "compression_ratio": 1.7596899224806202, "no_speech_prob": 4.092877134098671e-06}, {"id": 845, "seek": 565292, "start": 5659.2, "end": 5663.76, "text": " way better at 203. And they mention, by the way, in the paper, they didn't actually have", "tokens": [636, 1101, 412, 945, 18, 13, 400, 436, 2152, 11, 538, 264, 636, 11, 294, 264, 3035, 11, 436, 994, 380, 767, 362], "temperature": 0.0, "avg_logprob": -0.18803155722738313, "compression_ratio": 1.7596899224806202, "no_speech_prob": 4.092877134098671e-06}, {"id": 846, "seek": 565292, "start": 5663.76, "end": 5670.96, "text": " time to finish training. So when they actually finished training, it was actually 1.87. So", "tokens": [565, 281, 2413, 3097, 13, 407, 562, 436, 767, 4335, 3097, 11, 309, 390, 767, 502, 13, 23853, 13, 407], "temperature": 0.0, "avg_logprob": -0.18803155722738313, "compression_ratio": 1.7596899224806202, "no_speech_prob": 4.092877134098671e-06}, {"id": 847, "seek": 565292, "start": 5670.96, "end": 5676.8, "text": " they won so easily, it's not funny. And interestingly in the paper they actually mentioned the test", "tokens": [436, 1582, 370, 3612, 11, 309, 311, 406, 4074, 13, 400, 25873, 294, 264, 3035, 436, 767, 2835, 264, 1500], "temperature": 0.0, "avg_logprob": -0.18803155722738313, "compression_ratio": 1.7596899224806202, "no_speech_prob": 4.092877134098671e-06}, {"id": 848, "seek": 565292, "start": 5676.8, "end": 5681.92, "text": " set was so small that they knew that the only way they could be sure to win was to make", "tokens": [992, 390, 370, 1359, 300, 436, 2586, 300, 264, 787, 636, 436, 727, 312, 988, 281, 1942, 390, 281, 652], "temperature": 0.0, "avg_logprob": -0.18803155722738313, "compression_ratio": 1.7596899224806202, "no_speech_prob": 4.092877134098671e-06}, {"id": 849, "seek": 568192, "start": 5681.92, "end": 5685.16, "text": " sure they won easily.", "tokens": [988, 436, 1582, 3612, 13], "temperature": 0.0, "avg_logprob": -0.12315371796325013, "compression_ratio": 1.6954545454545455, "no_speech_prob": 5.9550975493039005e-06}, {"id": 850, "seek": 568192, "start": 5685.16, "end": 5690.16, "text": " Now because the test set was so small, the leaderboard's actually not statistically that", "tokens": [823, 570, 264, 1500, 992, 390, 370, 1359, 11, 264, 5263, 3787, 311, 767, 406, 36478, 300], "temperature": 0.0, "avg_logprob": -0.12315371796325013, "compression_ratio": 1.6954545454545455, "no_speech_prob": 5.9550975493039005e-06}, {"id": 851, "seek": 568192, "start": 5690.16, "end": 5695.84, "text": " great. So they created a custom test set and tried to see if they could find something", "tokens": [869, 13, 407, 436, 2942, 257, 2375, 1500, 992, 293, 3031, 281, 536, 498, 436, 727, 915, 746], "temperature": 0.0, "avg_logprob": -0.12315371796325013, "compression_ratio": 1.6954545454545455, "no_speech_prob": 5.9550975493039005e-06}, {"id": 852, "seek": 568192, "start": 5695.84, "end": 5700.76, "text": " that's even better still on the custom test set. And it turns out that actually an RNN", "tokens": [300, 311, 754, 1101, 920, 322, 264, 2375, 1500, 992, 13, 400, 309, 4523, 484, 300, 767, 364, 45702, 45], "temperature": 0.0, "avg_logprob": -0.12315371796325013, "compression_ratio": 1.6954545454545455, "no_speech_prob": 5.9550975493039005e-06}, {"id": 853, "seek": 568192, "start": 5700.76, "end": 5706.96, "text": " is better still. It still would have won the competition, but there's not enough data in", "tokens": [307, 1101, 920, 13, 467, 920, 576, 362, 1582, 264, 6211, 11, 457, 456, 311, 406, 1547, 1412, 294], "temperature": 0.0, "avg_logprob": -0.12315371796325013, "compression_ratio": 1.6954545454545455, "no_speech_prob": 5.9550975493039005e-06}, {"id": 854, "seek": 570696, "start": 5706.96, "end": 5712.52, "text": " the Kaggle test set that this is a statistically significant result. In this case it is statistically", "tokens": [264, 48751, 22631, 1500, 992, 300, 341, 307, 257, 36478, 4776, 1874, 13, 682, 341, 1389, 309, 307, 36478], "temperature": 0.0, "avg_logprob": -0.14624366760253907, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.3419768038147595e-05}, {"id": 855, "seek": 570696, "start": 5712.52, "end": 5719.52, "text": " significant. A regular RNN wasn't better. But what they did instead was they said, let's", "tokens": [4776, 13, 316, 3890, 45702, 45, 2067, 380, 1101, 13, 583, 437, 436, 630, 2602, 390, 436, 848, 11, 718, 311], "temperature": 0.0, "avg_logprob": -0.14624366760253907, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.3419768038147595e-05}, {"id": 856, "seek": 570696, "start": 5719.52, "end": 5729.08, "text": " take an RNN where we pass in 5 points at a time into the RNN. I think what probably would", "tokens": [747, 364, 45702, 45, 689, 321, 1320, 294, 1025, 2793, 412, 257, 565, 666, 264, 45702, 45, 13, 286, 519, 437, 1391, 576], "temperature": 0.0, "avg_logprob": -0.14624366760253907, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.3419768038147595e-05}, {"id": 857, "seek": 570696, "start": 5729.08, "end": 5732.92, "text": " have been even better would be to have had a convolutional layer first and then pass", "tokens": [362, 668, 754, 1101, 576, 312, 281, 362, 632, 257, 45216, 304, 4583, 700, 293, 550, 1320], "temperature": 0.0, "avg_logprob": -0.14624366760253907, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.3419768038147595e-05}, {"id": 858, "seek": 573292, "start": 5732.92, "end": 5740.76, "text": " that into an RNN. They didn't try it as far as I can see from the paper. And importantly", "tokens": [300, 666, 364, 45702, 45, 13, 814, 994, 380, 853, 309, 382, 1400, 382, 286, 393, 536, 490, 264, 3035, 13, 400, 8906], "temperature": 0.0, "avg_logprob": -0.14176828433305788, "compression_ratio": 1.5365853658536586, "no_speech_prob": 7.296281182789244e-06}, {"id": 859, "seek": 573292, "start": 5740.76, "end": 5746.92, "text": " a bidirectional RNN which ensures that the initial points and the last points tend to", "tokens": [257, 12957, 621, 41048, 45702, 45, 597, 28111, 300, 264, 5883, 2793, 293, 264, 1036, 2793, 3928, 281], "temperature": 0.0, "avg_logprob": -0.14176828433305788, "compression_ratio": 1.5365853658536586, "no_speech_prob": 7.296281182789244e-06}, {"id": 860, "seek": 573292, "start": 5746.92, "end": 5752.28, "text": " have more weight because we know that their state generally reflects things they've seen", "tokens": [362, 544, 3364, 570, 321, 458, 300, 641, 1785, 5101, 18926, 721, 436, 600, 1612], "temperature": 0.0, "avg_logprob": -0.14176828433305788, "compression_ratio": 1.5365853658536586, "no_speech_prob": 7.296281182789244e-06}, {"id": 861, "seek": 573292, "start": 5752.28, "end": 5758.28, "text": " more recently. So there's the result of this model.", "tokens": [544, 3938, 13, 407, 456, 311, 264, 1874, 295, 341, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14176828433305788, "compression_ratio": 1.5365853658536586, "no_speech_prob": 7.296281182789244e-06}, {"id": 862, "seek": 575828, "start": 5758.28, "end": 5764.08, "text": " So our poor long-suffering intern Brad has been trying to replicate this result. He has", "tokens": [407, 527, 4716, 938, 12, 82, 1245, 1794, 2154, 11895, 575, 668, 1382, 281, 25356, 341, 1874, 13, 634, 575], "temperature": 0.0, "avg_logprob": -0.15006208419799805, "compression_ratio": 1.5772357723577235, "no_speech_prob": 1.0952938282571267e-05}, {"id": 863, "seek": 575828, "start": 5764.08, "end": 5768.0, "text": " had at least 2 all-nighters in the last 2 weeks, but hasn't quite managed to yet. So", "tokens": [632, 412, 1935, 568, 439, 12, 6402, 433, 294, 264, 1036, 568, 3259, 11, 457, 6132, 380, 1596, 6453, 281, 1939, 13, 407], "temperature": 0.0, "avg_logprob": -0.15006208419799805, "compression_ratio": 1.5772357723577235, "no_speech_prob": 1.0952938282571267e-05}, {"id": 864, "seek": 575828, "start": 5768.0, "end": 5773.04, "text": " I'm not going to show you the code, but hopefully once Brad starts sleeping again, he'll be", "tokens": [286, 478, 406, 516, 281, 855, 291, 264, 3089, 11, 457, 4696, 1564, 11895, 3719, 8296, 797, 11, 415, 603, 312], "temperature": 0.0, "avg_logprob": -0.15006208419799805, "compression_ratio": 1.5772357723577235, "no_speech_prob": 1.0952938282571267e-05}, {"id": 865, "seek": 575828, "start": 5773.04, "end": 5778.42, "text": " able to finish it off and we can show you the notebook during the week on the forum", "tokens": [1075, 281, 2413, 309, 766, 293, 321, 393, 855, 291, 264, 21060, 1830, 264, 1243, 322, 264, 17542], "temperature": 0.0, "avg_logprob": -0.15006208419799805, "compression_ratio": 1.5772357723577235, "no_speech_prob": 1.0952938282571267e-05}, {"id": 866, "seek": 575828, "start": 5778.42, "end": 5781.88, "text": " that actually re-implements this thing.", "tokens": [300, 767, 319, 12, 332, 781, 1117, 341, 551, 13], "temperature": 0.0, "avg_logprob": -0.15006208419799805, "compression_ratio": 1.5772357723577235, "no_speech_prob": 1.0952938282571267e-05}, {"id": 867, "seek": 578188, "start": 5781.88, "end": 5790.4400000000005, "text": " It was an interesting process to watch Brad try to replicate this. The vast majority of", "tokens": [467, 390, 364, 1880, 1399, 281, 1159, 11895, 853, 281, 25356, 341, 13, 440, 8369, 6286, 295], "temperature": 0.0, "avg_logprob": -0.13459771593040395, "compression_ratio": 1.655430711610487, "no_speech_prob": 1.3211720215622336e-05}, {"id": 868, "seek": 578188, "start": 5790.4400000000005, "end": 5794.96, "text": " the time in my experience when people say they've tried a model and the model didn't", "tokens": [264, 565, 294, 452, 1752, 562, 561, 584, 436, 600, 3031, 257, 2316, 293, 264, 2316, 994, 380], "temperature": 0.0, "avg_logprob": -0.13459771593040395, "compression_ratio": 1.655430711610487, "no_speech_prob": 1.3211720215622336e-05}, {"id": 869, "seek": 578188, "start": 5794.96, "end": 5798.24, "text": " work out and they've given up on the model, it turns out that it's actually because they", "tokens": [589, 484, 293, 436, 600, 2212, 493, 322, 264, 2316, 11, 309, 4523, 484, 300, 309, 311, 767, 570, 436], "temperature": 0.0, "avg_logprob": -0.13459771593040395, "compression_ratio": 1.655430711610487, "no_speech_prob": 1.3211720215622336e-05}, {"id": 870, "seek": 578188, "start": 5798.24, "end": 5804.52, "text": " screwed something up, not because of the problem with the model. And if you weren't comparing", "tokens": [20331, 746, 493, 11, 406, 570, 295, 264, 1154, 365, 264, 2316, 13, 400, 498, 291, 4999, 380, 15763], "temperature": 0.0, "avg_logprob": -0.13459771593040395, "compression_ratio": 1.655430711610487, "no_speech_prob": 1.3211720215622336e-05}, {"id": 871, "seek": 578188, "start": 5804.52, "end": 5810.76, "text": " to Yoshua Bengio's team's result, knowing that you haven't replicated it yet, at which", "tokens": [281, 38949, 4398, 29425, 1004, 311, 1469, 311, 1874, 11, 5276, 300, 291, 2378, 380, 46365, 309, 1939, 11, 412, 597], "temperature": 0.0, "avg_logprob": -0.13459771593040395, "compression_ratio": 1.655430711610487, "no_speech_prob": 1.3211720215622336e-05}, {"id": 872, "seek": 581076, "start": 5810.76, "end": 5815.88, "text": " point do you give up and say, oh my model's not working, versus saying, no I've still", "tokens": [935, 360, 291, 976, 493, 293, 584, 11, 1954, 452, 2316, 311, 406, 1364, 11, 5717, 1566, 11, 572, 286, 600, 920], "temperature": 0.0, "avg_logprob": -0.21292808444000955, "compression_ratio": 1.5639810426540284, "no_speech_prob": 2.295880676683737e-06}, {"id": 873, "seek": 581076, "start": 5815.88, "end": 5823.400000000001, "text": " got bugs. It's very difficult to debug machine learning models.", "tokens": [658, 15120, 13, 467, 311, 588, 2252, 281, 24083, 3479, 2539, 5245, 13], "temperature": 0.0, "avg_logprob": -0.21292808444000955, "compression_ratio": 1.5639810426540284, "no_speech_prob": 2.295880676683737e-06}, {"id": 874, "seek": 581076, "start": 5823.400000000001, "end": 5829.320000000001, "text": " What Brad's actually had to end up doing is literally take the original Bengio team code,", "tokens": [708, 11895, 311, 767, 632, 281, 917, 493, 884, 307, 3736, 747, 264, 3380, 29425, 1004, 1469, 3089, 11], "temperature": 0.0, "avg_logprob": -0.21292808444000955, "compression_ratio": 1.5639810426540284, "no_speech_prob": 2.295880676683737e-06}, {"id": 875, "seek": 581076, "start": 5829.320000000001, "end": 5834.4400000000005, "text": " run it line by line, and then try to replicate it in Keras line by line, in like literally", "tokens": [1190, 309, 1622, 538, 1622, 11, 293, 550, 853, 281, 25356, 309, 294, 591, 6985, 1622, 538, 1622, 11, 294, 411, 3736], "temperature": 0.0, "avg_logprob": -0.21292808444000955, "compression_ratio": 1.5639810426540284, "no_speech_prob": 2.295880676683737e-06}, {"id": 876, "seek": 583444, "start": 5834.44, "end": 5841.719999999999, "text": " np.org clause every time. Because to build a model like this, it doesn't look that complex,", "tokens": [33808, 13, 4646, 25925, 633, 565, 13, 1436, 281, 1322, 257, 2316, 411, 341, 11, 309, 1177, 380, 574, 300, 3997, 11], "temperature": 0.0, "avg_logprob": -0.15169024063368974, "compression_ratio": 1.7843137254901962, "no_speech_prob": 6.438920536311343e-06}, {"id": 877, "seek": 583444, "start": 5841.719999999999, "end": 5848.48, "text": " but there's just so many places that you can make little mistakes. No normal person will", "tokens": [457, 456, 311, 445, 370, 867, 3190, 300, 291, 393, 652, 707, 8038, 13, 883, 2710, 954, 486], "temperature": 0.0, "avg_logprob": -0.15169024063368974, "compression_ratio": 1.7843137254901962, "no_speech_prob": 6.438920536311343e-06}, {"id": 878, "seek": 583444, "start": 5848.48, "end": 5854.5599999999995, "text": " make like zero mistakes. In fact, normal people like me will make dozens of mistakes. So when", "tokens": [652, 411, 4018, 8038, 13, 682, 1186, 11, 2710, 561, 411, 385, 486, 652, 18431, 295, 8038, 13, 407, 562], "temperature": 0.0, "avg_logprob": -0.15169024063368974, "compression_ratio": 1.7843137254901962, "no_speech_prob": 6.438920536311343e-06}, {"id": 879, "seek": 583444, "start": 5854.5599999999995, "end": 5860.16, "text": " you build a model like this, you need to find a way to test every single line of code. Any", "tokens": [291, 1322, 257, 2316, 411, 341, 11, 291, 643, 281, 915, 257, 636, 281, 1500, 633, 2167, 1622, 295, 3089, 13, 2639], "temperature": 0.0, "avg_logprob": -0.15169024063368974, "compression_ratio": 1.7843137254901962, "no_speech_prob": 6.438920536311343e-06}, {"id": 880, "seek": 583444, "start": 5860.16, "end": 5863.96, "text": " line of code you don't test, I guarantee you'll end up with a bug, and you won't know you", "tokens": [1622, 295, 3089, 291, 500, 380, 1500, 11, 286, 10815, 291, 603, 917, 493, 365, 257, 7426, 11, 293, 291, 1582, 380, 458, 291], "temperature": 0.0, "avg_logprob": -0.15169024063368974, "compression_ratio": 1.7843137254901962, "no_speech_prob": 6.438920536311343e-06}, {"id": 881, "seek": 586396, "start": 5863.96, "end": 5866.96, "text": " have a bug, and there's no way to ever find out you had a bug.", "tokens": [362, 257, 7426, 11, 293, 456, 311, 572, 636, 281, 1562, 915, 484, 291, 632, 257, 7426, 13], "temperature": 0.0, "avg_logprob": -0.31926670941439544, "compression_ratio": 1.5066666666666666, "no_speech_prob": 1.544617953186389e-05}, {"id": 882, "seek": 586396, "start": 5866.96, "end": 5877.92, "text": " Question. We have several questions. One is, note that PI times CI is very similar to what", "tokens": [14464, 13, 492, 362, 2940, 1651, 13, 1485, 307, 11, 3637, 300, 27176, 1413, 37777, 307, 588, 2531, 281, 437], "temperature": 0.0, "avg_logprob": -0.31926670941439544, "compression_ratio": 1.5066666666666666, "no_speech_prob": 1.544617953186389e-05}, {"id": 883, "seek": 586396, "start": 5877.92, "end": 5882.16, "text": " happens in the memory network paper. In that case the output embeddings are weighted by", "tokens": [2314, 294, 264, 4675, 3209, 3035, 13, 682, 300, 1389, 264, 5598, 12240, 29432, 366, 32807, 538], "temperature": 0.0, "avg_logprob": -0.31926670941439544, "compression_ratio": 1.5066666666666666, "no_speech_prob": 1.544617953186389e-05}, {"id": 884, "seek": 586396, "start": 5882.16, "end": 5883.52, "text": " the attention probability.", "tokens": [264, 3202, 8482, 13], "temperature": 0.0, "avg_logprob": -0.31926670941439544, "compression_ratio": 1.5066666666666666, "no_speech_prob": 1.544617953186389e-05}, {"id": 885, "seek": 586396, "start": 5883.52, "end": 5890.28, "text": " Answer. Yeah, or it's a lot like a regular attentional language model.", "tokens": [24545, 13, 865, 11, 420, 309, 311, 257, 688, 411, 257, 3890, 3202, 304, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.31926670941439544, "compression_ratio": 1.5066666666666666, "no_speech_prob": 1.544617953186389e-05}, {"id": 886, "seek": 589028, "start": 5890.28, "end": 5894.5199999999995, "text": " Question. Can you talk more about the idea you have about first having the convolutional", "tokens": [14464, 13, 1664, 291, 751, 544, 466, 264, 1558, 291, 362, 466, 700, 1419, 264, 45216, 304], "temperature": 0.0, "avg_logprob": -0.18963067959516478, "compression_ratio": 1.3097345132743363, "no_speech_prob": 2.3922922991914675e-05}, {"id": 887, "seek": 589028, "start": 5894.5199999999995, "end": 5899.32, "text": " layer and passing that to an RNN? What do you mean by that?", "tokens": [4583, 293, 8437, 300, 281, 364, 45702, 45, 30, 708, 360, 291, 914, 538, 300, 30], "temperature": 0.0, "avg_logprob": -0.18963067959516478, "compression_ratio": 1.3097345132743363, "no_speech_prob": 2.3922922991914675e-05}, {"id": 888, "seek": 589932, "start": 5899.32, "end": 5928.92, "text": " So here is a fantastic paper. We looked at these kind of sub-word encodings last week", "tokens": [407, 510, 307, 257, 5456, 3035, 13, 492, 2956, 412, 613, 733, 295, 1422, 12, 7462, 2058, 378, 1109, 1036, 1243], "temperature": 0.0, "avg_logprob": -0.17915424346923828, "compression_ratio": 1.0759493670886076, "no_speech_prob": 2.31862304644892e-05}, {"id": 889, "seek": 592892, "start": 5928.92, "end": 5933.24, "text": " for language models. I don't know if any of you thought about this and wondered what if", "tokens": [337, 2856, 5245, 13, 286, 500, 380, 458, 498, 604, 295, 291, 1194, 466, 341, 293, 17055, 437, 498], "temperature": 0.0, "avg_logprob": -0.14638417959213257, "compression_ratio": 1.5020746887966805, "no_speech_prob": 2.2827283828519285e-05}, {"id": 890, "seek": 592892, "start": 5933.24, "end": 5940.16, "text": " we just had individual characters. There's a really fascinating paper called Fully Character", "tokens": [321, 445, 632, 2609, 4342, 13, 821, 311, 257, 534, 10343, 3035, 1219, 479, 2150, 36786], "temperature": 0.0, "avg_logprob": -0.14638417959213257, "compression_ratio": 1.5020746887966805, "no_speech_prob": 2.2827283828519285e-05}, {"id": 891, "seek": 592892, "start": 5940.16, "end": 5948.12, "text": " Level Machine Translation with No Explicit Segmentation. It's from November of last year.", "tokens": [16872, 22155, 6531, 24278, 365, 883, 2111, 4770, 270, 1100, 10433, 399, 13, 467, 311, 490, 7674, 295, 1036, 1064, 13], "temperature": 0.0, "avg_logprob": -0.14638417959213257, "compression_ratio": 1.5020746887966805, "no_speech_prob": 2.2827283828519285e-05}, {"id": 892, "seek": 592892, "start": 5948.12, "end": 5957.12, "text": " They actually get fantastic results on just character level, beating pretty much everything", "tokens": [814, 767, 483, 5456, 3542, 322, 445, 2517, 1496, 11, 13497, 1238, 709, 1203], "temperature": 0.0, "avg_logprob": -0.14638417959213257, "compression_ratio": 1.5020746887966805, "no_speech_prob": 2.2827283828519285e-05}, {"id": 893, "seek": 595712, "start": 5957.12, "end": 5966.0, "text": " including the BPE approach we saw last time. So they looked at lots of different approaches", "tokens": [3009, 264, 363, 5208, 3109, 321, 1866, 1036, 565, 13, 407, 436, 2956, 412, 3195, 295, 819, 11587], "temperature": 0.0, "avg_logprob": -0.13768191509936228, "compression_ratio": 1.7009345794392523, "no_speech_prob": 6.339155788737116e-06}, {"id": 894, "seek": 595712, "start": 5966.0, "end": 5976.24, "text": " and comparing BPE to individual character, and most of the time they got the best results.", "tokens": [293, 15763, 363, 5208, 281, 2609, 2517, 11, 293, 881, 295, 264, 565, 436, 658, 264, 1151, 3542, 13], "temperature": 0.0, "avg_logprob": -0.13768191509936228, "compression_ratio": 1.7009345794392523, "no_speech_prob": 6.339155788737116e-06}, {"id": 895, "seek": 595712, "start": 5976.24, "end": 5982.48, "text": " Their model looks like this. They start out with every individual character. It goes through", "tokens": [6710, 2316, 1542, 411, 341, 13, 814, 722, 484, 365, 633, 2609, 2517, 13, 467, 1709, 807], "temperature": 0.0, "avg_logprob": -0.13768191509936228, "compression_ratio": 1.7009345794392523, "no_speech_prob": 6.339155788737116e-06}, {"id": 896, "seek": 595712, "start": 5982.48, "end": 5986.68, "text": " a character embedding, just like we've used character embeddings lots of times. Then you", "tokens": [257, 2517, 12240, 3584, 11, 445, 411, 321, 600, 1143, 2517, 12240, 29432, 3195, 295, 1413, 13, 1396, 291], "temperature": 0.0, "avg_logprob": -0.13768191509936228, "compression_ratio": 1.7009345794392523, "no_speech_prob": 6.339155788737116e-06}, {"id": 897, "seek": 598668, "start": 5986.68, "end": 5992.280000000001, "text": " take those character embeddings and you pass it through a one-dimensional convolution.", "tokens": [747, 729, 2517, 12240, 29432, 293, 291, 1320, 309, 807, 257, 472, 12, 18759, 45216, 13], "temperature": 0.0, "avg_logprob": -0.19096336364746094, "compression_ratio": 1.5675675675675675, "no_speech_prob": 1.6187303117476404e-05}, {"id": 898, "seek": 598668, "start": 5992.280000000001, "end": 5999.16, "text": " I don't know if you guys remember, but in part 1 of this course, Ben actually had a", "tokens": [286, 500, 380, 458, 498, 291, 1074, 1604, 11, 457, 294, 644, 502, 295, 341, 1164, 11, 3964, 767, 632, 257], "temperature": 0.0, "avg_logprob": -0.19096336364746094, "compression_ratio": 1.5675675675675675, "no_speech_prob": 1.6187303117476404e-05}, {"id": 899, "seek": 598668, "start": 5999.16, "end": 6004.8, "text": " blog post about showing how you can do multiple size convolutions and concatenate them all", "tokens": [6968, 2183, 466, 4099, 577, 291, 393, 360, 3866, 2744, 3754, 15892, 293, 1588, 7186, 473, 552, 439], "temperature": 0.0, "avg_logprob": -0.19096336364746094, "compression_ratio": 1.5675675675675675, "no_speech_prob": 1.6187303117476404e-05}, {"id": 900, "seek": 598668, "start": 6004.8, "end": 6009.88, "text": " together. So you could use that approach, or you could just pick a single size. So you", "tokens": [1214, 13, 407, 291, 727, 764, 300, 3109, 11, 420, 291, 727, 445, 1888, 257, 2167, 2744, 13, 407, 291], "temperature": 0.0, "avg_logprob": -0.19096336364746094, "compression_ratio": 1.5675675675675675, "no_speech_prob": 1.6187303117476404e-05}, {"id": 901, "seek": 600988, "start": 6009.88, "end": 6017.4400000000005, "text": " end up basically scrolling your convolutional window across your sets of characters. So", "tokens": [917, 493, 1936, 29053, 428, 45216, 304, 4910, 2108, 428, 6352, 295, 4342, 13, 407], "temperature": 0.0, "avg_logprob": -0.1879750765286959, "compression_ratio": 1.6504854368932038, "no_speech_prob": 1.0615872270136606e-05}, {"id": 902, "seek": 600988, "start": 6017.4400000000005, "end": 6023.52, "text": " you end up with the same number of convolution outputs as you started out with letters, but", "tokens": [291, 917, 493, 365, 264, 912, 1230, 295, 45216, 23930, 382, 291, 1409, 484, 365, 7825, 11, 457], "temperature": 0.0, "avg_logprob": -0.1879750765286959, "compression_ratio": 1.6504854368932038, "no_speech_prob": 1.0615872270136606e-05}, {"id": 903, "seek": 600988, "start": 6023.52, "end": 6030.4400000000005, "text": " they're now representing the information in a window around that letter.", "tokens": [436, 434, 586, 13460, 264, 1589, 294, 257, 4910, 926, 300, 5063, 13], "temperature": 0.0, "avg_logprob": -0.1879750765286959, "compression_ratio": 1.6504854368932038, "no_speech_prob": 1.0615872270136606e-05}, {"id": 904, "seek": 600988, "start": 6030.4400000000005, "end": 6038.92, "text": " In this case, they then did max-pulling. So they basically said, which window, assuming", "tokens": [682, 341, 1389, 11, 436, 550, 630, 11469, 12, 79, 858, 278, 13, 407, 436, 1936, 848, 11, 597, 4910, 11, 11926], "temperature": 0.0, "avg_logprob": -0.1879750765286959, "compression_ratio": 1.6504854368932038, "no_speech_prob": 1.0615872270136606e-05}, {"id": 905, "seek": 603892, "start": 6038.92, "end": 6048.4400000000005, "text": " that maybe we had a different size, size 4, size 3, or size 5, which bits seem to have", "tokens": [300, 1310, 321, 632, 257, 819, 2744, 11, 2744, 1017, 11, 2744, 805, 11, 420, 2744, 1025, 11, 597, 9239, 1643, 281, 362], "temperature": 0.0, "avg_logprob": -0.23265635750510477, "compression_ratio": 1.6470588235294117, "no_speech_prob": 8.800981049716938e-06}, {"id": 906, "seek": 603892, "start": 6048.4400000000005, "end": 6053.56, "text": " got the highest activations around here. And then they took those max-pulled things and", "tokens": [658, 264, 6343, 2430, 763, 926, 510, 13, 400, 550, 436, 1890, 729, 11469, 12, 79, 858, 292, 721, 293], "temperature": 0.0, "avg_logprob": -0.23265635750510477, "compression_ratio": 1.6470588235294117, "no_speech_prob": 8.800981049716938e-06}, {"id": 907, "seek": 603892, "start": 6053.56, "end": 6058.16, "text": " they put them through a second set of segment embeddings. They then put that through something", "tokens": [436, 829, 552, 807, 257, 1150, 992, 295, 9469, 12240, 29432, 13, 814, 550, 829, 300, 807, 746], "temperature": 0.0, "avg_logprob": -0.23265635750510477, "compression_ratio": 1.6470588235294117, "no_speech_prob": 8.800981049716938e-06}, {"id": 908, "seek": 603892, "start": 6058.16, "end": 6061.52, "text": " called a highway network, which the details don't matter too much. It's kind of something", "tokens": [1219, 257, 17205, 3209, 11, 597, 264, 4365, 500, 380, 1871, 886, 709, 13, 467, 311, 733, 295, 746], "temperature": 0.0, "avg_logprob": -0.23265635750510477, "compression_ratio": 1.6470588235294117, "no_speech_prob": 8.800981049716938e-06}, {"id": 909, "seek": 603892, "start": 6061.52, "end": 6067.56, "text": " like a dense net like we learned about last week. This is a slightly older approach than", "tokens": [411, 257, 18011, 2533, 411, 321, 3264, 466, 1036, 1243, 13, 639, 307, 257, 4748, 4906, 3109, 813], "temperature": 0.0, "avg_logprob": -0.23265635750510477, "compression_ratio": 1.6470588235294117, "no_speech_prob": 8.800981049716938e-06}, {"id": 910, "seek": 606756, "start": 6067.56, "end": 6073.160000000001, "text": " the dense net. And then finally after doing all that, stick that through an RNN.", "tokens": [264, 18011, 2533, 13, 400, 550, 2721, 934, 884, 439, 300, 11, 2897, 300, 807, 364, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.09772498791034405, "compression_ratio": 1.5669642857142858, "no_speech_prob": 1.2878864481535857e-06}, {"id": 911, "seek": 606756, "start": 6073.160000000001, "end": 6082.56, "text": " So the idea here in this model was they basically did as much learned preprocessing as possible", "tokens": [407, 264, 1558, 510, 294, 341, 2316, 390, 436, 1936, 630, 382, 709, 3264, 2666, 340, 780, 278, 382, 1944], "temperature": 0.0, "avg_logprob": -0.09772498791034405, "compression_ratio": 1.5669642857142858, "no_speech_prob": 1.2878864481535857e-06}, {"id": 912, "seek": 606756, "start": 6082.56, "end": 6088.160000000001, "text": " and then finally put that into an RNN. And because we've got these max-pulling layers,", "tokens": [293, 550, 2721, 829, 300, 666, 364, 45702, 45, 13, 400, 570, 321, 600, 658, 613, 11469, 12, 79, 858, 278, 7914, 11], "temperature": 0.0, "avg_logprob": -0.09772498791034405, "compression_ratio": 1.5669642857142858, "no_speech_prob": 1.2878864481535857e-06}, {"id": 913, "seek": 606756, "start": 6088.160000000001, "end": 6096.96, "text": " this RNN ends up with a lot less time points, which is really important to minimize the", "tokens": [341, 45702, 45, 5314, 493, 365, 257, 688, 1570, 565, 2793, 11, 597, 307, 534, 1021, 281, 17522, 264], "temperature": 0.0, "avg_logprob": -0.09772498791034405, "compression_ratio": 1.5669642857142858, "no_speech_prob": 1.2878864481535857e-06}, {"id": 914, "seek": 609696, "start": 6096.96, "end": 6102.92, "text": " amount of processing in the RNN. So I'm not going to go into detail on this, but check", "tokens": [2372, 295, 9007, 294, 264, 45702, 45, 13, 407, 286, 478, 406, 516, 281, 352, 666, 2607, 322, 341, 11, 457, 1520], "temperature": 0.0, "avg_logprob": -0.2825232823689779, "compression_ratio": 1.5520361990950227, "no_speech_prob": 4.83250551042147e-05}, {"id": 915, "seek": 609696, "start": 6102.92, "end": 6105.52, "text": " out this paper because it's really interesting.", "tokens": [484, 341, 3035, 570, 309, 311, 534, 1880, 13], "temperature": 0.0, "avg_logprob": -0.2825232823689779, "compression_ratio": 1.5520361990950227, "no_speech_prob": 4.83250551042147e-05}, {"id": 916, "seek": 609696, "start": 6105.52, "end": 6112.32, "text": " Question. So for the destinations, we would have more air for the peripheral points. Are", "tokens": [14464, 13, 407, 337, 264, 37787, 11, 321, 576, 362, 544, 1988, 337, 264, 40235, 2793, 13, 2014], "temperature": 0.0, "avg_logprob": -0.2825232823689779, "compression_ratio": 1.5520361990950227, "no_speech_prob": 4.83250551042147e-05}, {"id": 917, "seek": 609696, "start": 6112.32, "end": 6114.64, "text": " we taking a centroid of clusters?", "tokens": [321, 1940, 257, 1489, 6490, 295, 23313, 30], "temperature": 0.0, "avg_logprob": -0.2825232823689779, "compression_ratio": 1.5520361990950227, "no_speech_prob": 4.83250551042147e-05}, {"id": 918, "seek": 609696, "start": 6114.64, "end": 6123.04, "text": " Answer. I don't understand that, sorry. You understand that? All we're doing is we're", "tokens": [24545, 13, 286, 500, 380, 1223, 300, 11, 2597, 13, 509, 1223, 300, 30, 1057, 321, 434, 884, 307, 321, 434], "temperature": 0.0, "avg_logprob": -0.2825232823689779, "compression_ratio": 1.5520361990950227, "no_speech_prob": 4.83250551042147e-05}, {"id": 919, "seek": 612304, "start": 6123.04, "end": 6129.32, "text": " taking the softmax-p multiplied by the cluster C, multiply them and add them up.", "tokens": [1940, 264, 2787, 41167, 12, 79, 17207, 538, 264, 13630, 383, 11, 12972, 552, 293, 909, 552, 493, 13], "temperature": 0.0, "avg_logprob": -0.25754009203964406, "compression_ratio": 1.6431718061674008, "no_speech_prob": 1.2606818927451968e-05}, {"id": 920, "seek": 612304, "start": 6129.32, "end": 6136.08, "text": " Question. I thought the first part was asking that with destinations that are more peripheral,", "tokens": [14464, 13, 286, 1194, 264, 700, 644, 390, 3365, 300, 365, 37787, 300, 366, 544, 40235, 11], "temperature": 0.0, "avg_logprob": -0.25754009203964406, "compression_ratio": 1.6431718061674008, "no_speech_prob": 1.2606818927451968e-05}, {"id": 921, "seek": 612304, "start": 6136.08, "end": 6139.04, "text": " they would have higher air because they would be harder to predict this way.", "tokens": [436, 576, 362, 2946, 1988, 570, 436, 576, 312, 6081, 281, 6069, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.25754009203964406, "compression_ratio": 1.6431718061674008, "no_speech_prob": 1.2606818927451968e-05}, {"id": 922, "seek": 612304, "start": 6139.04, "end": 6143.2, "text": " Answer. Yeah, probably, which is fine because by definition they're not close to a cluster", "tokens": [24545, 13, 865, 11, 1391, 11, 597, 307, 2489, 570, 538, 7123, 436, 434, 406, 1998, 281, 257, 13630], "temperature": 0.0, "avg_logprob": -0.25754009203964406, "compression_ratio": 1.6431718061674008, "no_speech_prob": 1.2606818927451968e-05}, {"id": 923, "seek": 612304, "start": 6143.2, "end": 6145.2, "text": " center so they're not common.", "tokens": [3056, 370, 436, 434, 406, 2689, 13], "temperature": 0.0, "avg_logprob": -0.25754009203964406, "compression_ratio": 1.6431718061674008, "no_speech_prob": 1.2606818927451968e-05}, {"id": 924, "seek": 614520, "start": 6145.2, "end": 6153.16, "text": " Question. Going back, there was a question on the Rossmann example. What does MAPE with", "tokens": [14464, 13, 10963, 646, 11, 456, 390, 257, 1168, 322, 264, 16140, 14912, 1365, 13, 708, 775, 376, 4715, 36, 365], "temperature": 0.0, "avg_logprob": -0.23060830434163412, "compression_ratio": 1.3681318681318682, "no_speech_prob": 7.071854270179756e-06}, {"id": 925, "seek": 614520, "start": 6153.16, "end": 6157.599999999999, "text": " neural network mean? I would have expected that result to be the same. Why is it lower?", "tokens": [18161, 3209, 914, 30, 286, 576, 362, 5176, 300, 1874, 281, 312, 264, 912, 13, 1545, 307, 309, 3126, 30], "temperature": 0.0, "avg_logprob": -0.23060830434163412, "compression_ratio": 1.3681318681318682, "no_speech_prob": 7.071854270179756e-06}, {"id": 926, "seek": 614520, "start": 6157.599999999999, "end": 6164.32, "text": " Answer. This is just using a one-hot encoding without an embedding layer.", "tokens": [24545, 13, 639, 307, 445, 1228, 257, 472, 12, 12194, 43430, 1553, 364, 12240, 3584, 4583, 13], "temperature": 0.0, "avg_logprob": -0.23060830434163412, "compression_ratio": 1.3681318681318682, "no_speech_prob": 7.071854270179756e-06}, {"id": 927, "seek": 616432, "start": 6164.32, "end": 6176.16, "text": " We kind of ran out of time a bit quickly, but I really want to show you this. So quite", "tokens": [492, 733, 295, 5872, 484, 295, 565, 257, 857, 2661, 11, 457, 286, 534, 528, 281, 855, 291, 341, 13, 407, 1596], "temperature": 0.0, "avg_logprob": -0.16053889952984052, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.668841827675351e-06}, {"id": 928, "seek": 616432, "start": 6176.16, "end": 6182.92, "text": " a few of the students and I have been trying to get a new approach to segmentation working.", "tokens": [257, 1326, 295, 264, 1731, 293, 286, 362, 668, 1382, 281, 483, 257, 777, 3109, 281, 9469, 399, 1364, 13], "temperature": 0.0, "avg_logprob": -0.16053889952984052, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.668841827675351e-06}, {"id": 929, "seek": 616432, "start": 6182.92, "end": 6186.799999999999, "text": " I finally got it working in the last day or two and I really wanted to show it to you.", "tokens": [286, 2721, 658, 309, 1364, 294, 264, 1036, 786, 420, 732, 293, 286, 534, 1415, 281, 855, 309, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.16053889952984052, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.668841827675351e-06}, {"id": 930, "seek": 616432, "start": 6186.799999999999, "end": 6194.28, "text": " We talked last week about DenseNet and I mentioned that DenseNet is like ass-kickingly good at", "tokens": [492, 2825, 1036, 1243, 466, 413, 1288, 31890, 293, 286, 2835, 300, 413, 1288, 31890, 307, 411, 1256, 12, 74, 10401, 356, 665, 412], "temperature": 0.0, "avg_logprob": -0.16053889952984052, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.668841827675351e-06}, {"id": 931, "seek": 619428, "start": 6194.28, "end": 6202.599999999999, "text": " doing image classification with a small number of data points. Like crazily good.", "tokens": [884, 3256, 21538, 365, 257, 1359, 1230, 295, 1412, 2793, 13, 1743, 46348, 953, 665, 13], "temperature": 0.0, "avg_logprob": -0.14799287345971954, "compression_ratio": 1.6355140186915889, "no_speech_prob": 2.078476063616108e-05}, {"id": 932, "seek": 619428, "start": 6202.599999999999, "end": 6206.5599999999995, "text": " But I also mentioned that it's the basis of this thing called the 100 layers tiramisu,", "tokens": [583, 286, 611, 2835, 300, 309, 311, 264, 5143, 295, 341, 551, 1219, 264, 2319, 7914, 13807, 335, 25871, 11], "temperature": 0.0, "avg_logprob": -0.14799287345971954, "compression_ratio": 1.6355140186915889, "no_speech_prob": 2.078476063616108e-05}, {"id": 933, "seek": 619428, "start": 6206.5599999999995, "end": 6216.04, "text": " which is an approach to segmentation. So segmentation refers to taking a picture, an image, and", "tokens": [597, 307, 364, 3109, 281, 9469, 399, 13, 407, 9469, 399, 14942, 281, 1940, 257, 3036, 11, 364, 3256, 11, 293], "temperature": 0.0, "avg_logprob": -0.14799287345971954, "compression_ratio": 1.6355140186915889, "no_speech_prob": 2.078476063616108e-05}, {"id": 934, "seek": 619428, "start": 6216.04, "end": 6223.28, "text": " figuring out where's the tree, where's the dog, where's the bicycle, and so forth. So", "tokens": [15213, 484, 689, 311, 264, 4230, 11, 689, 311, 264, 3000, 11, 689, 311, 264, 20888, 11, 293, 370, 5220, 13, 407], "temperature": 0.0, "avg_logprob": -0.14799287345971954, "compression_ratio": 1.6355140186915889, "no_speech_prob": 2.078476063616108e-05}, {"id": 935, "seek": 622328, "start": 6223.28, "end": 6228.92, "text": " it seems like we're Yoshua Bengio fans today because this is one of his Crips papers as", "tokens": [309, 2544, 411, 321, 434, 38949, 4398, 29425, 1004, 4499, 965, 570, 341, 307, 472, 295, 702, 383, 470, 1878, 10577, 382], "temperature": 0.0, "avg_logprob": -0.22235249250363082, "compression_ratio": 1.4095744680851063, "no_speech_prob": 2.6273837647750042e-05}, {"id": 936, "seek": 622328, "start": 6228.92, "end": 6234.599999999999, "text": " well.", "tokens": [731, 13], "temperature": 0.0, "avg_logprob": -0.22235249250363082, "compression_ratio": 1.4095744680851063, "no_speech_prob": 2.6273837647750042e-05}, {"id": 937, "seek": 622328, "start": 6234.599999999999, "end": 6245.96, "text": " Let me set the scene. So Brendan, one of our students, who many of you have seen a lot", "tokens": [961, 385, 992, 264, 4145, 13, 407, 48484, 11, 472, 295, 527, 1731, 11, 567, 867, 295, 291, 362, 1612, 257, 688], "temperature": 0.0, "avg_logprob": -0.22235249250363082, "compression_ratio": 1.4095744680851063, "no_speech_prob": 2.6273837647750042e-05}, {"id": 938, "seek": 622328, "start": 6245.96, "end": 6250.679999999999, "text": " of his blog posts, he has successfully got a PyTorch of this working. So I've shared", "tokens": [295, 702, 6968, 12300, 11, 415, 575, 10727, 658, 257, 9953, 51, 284, 339, 295, 341, 1364, 13, 407, 286, 600, 5507], "temperature": 0.0, "avg_logprob": -0.22235249250363082, "compression_ratio": 1.4095744680851063, "no_speech_prob": 2.6273837647750042e-05}, {"id": 939, "seek": 625068, "start": 6250.68, "end": 6256.92, "text": " that on our files.fast.ai and I got the Keras version of it working. So I'll show you the", "tokens": [300, 322, 527, 7098, 13, 7011, 13, 1301, 293, 286, 658, 264, 591, 6985, 3037, 295, 309, 1364, 13, 407, 286, 603, 855, 291, 264], "temperature": 0.0, "avg_logprob": -0.14200103282928467, "compression_ratio": 1.4759358288770053, "no_speech_prob": 2.1111558453412727e-05}, {"id": 940, "seek": 625068, "start": 6256.92, "end": 6261.04, "text": " Keras version because I actually understand it. And if anybody's interested in asking", "tokens": [591, 6985, 3037, 570, 286, 767, 1223, 309, 13, 400, 498, 4472, 311, 3102, 294, 3365], "temperature": 0.0, "avg_logprob": -0.14200103282928467, "compression_ratio": 1.4759358288770053, "no_speech_prob": 2.1111558453412727e-05}, {"id": 941, "seek": 625068, "start": 6261.04, "end": 6265.52, "text": " questions about the PyTorch version, hopefully Brendan will be happy to answer them during", "tokens": [1651, 466, 264, 9953, 51, 284, 339, 3037, 11, 4696, 48484, 486, 312, 2055, 281, 1867, 552, 1830], "temperature": 0.0, "avg_logprob": -0.14200103282928467, "compression_ratio": 1.4759358288770053, "no_speech_prob": 2.1111558453412727e-05}, {"id": 942, "seek": 625068, "start": 6265.52, "end": 6267.92, "text": " the week.", "tokens": [264, 1243, 13], "temperature": 0.0, "avg_logprob": -0.14200103282928467, "compression_ratio": 1.4759358288770053, "no_speech_prob": 2.1111558453412727e-05}, {"id": 943, "seek": 626792, "start": 6267.92, "end": 6290.64, "text": " So the data looks like this. There's an image, and then there's labels. So that's basically", "tokens": [407, 264, 1412, 1542, 411, 341, 13, 821, 311, 364, 3256, 11, 293, 550, 456, 311, 16949, 13, 407, 300, 311, 1936], "temperature": 0.0, "avg_logprob": -0.12312362708297431, "compression_ratio": 1.5, "no_speech_prob": 8.013077604118735e-06}, {"id": 944, "seek": 626792, "start": 6290.64, "end": 6294.24, "text": " what it looks like. So you can see here you've got traffic lights, you've got poles, you've", "tokens": [437, 309, 1542, 411, 13, 407, 291, 393, 536, 510, 291, 600, 658, 6419, 5811, 11, 291, 600, 658, 24760, 11, 291, 600], "temperature": 0.0, "avg_logprob": -0.12312362708297431, "compression_ratio": 1.5, "no_speech_prob": 8.013077604118735e-06}, {"id": 945, "seek": 629424, "start": 6294.24, "end": 6303.0, "text": " got trees, buildings, roads. Interestingly, the dataset we're using is something called", "tokens": [658, 5852, 11, 7446, 11, 11344, 13, 30564, 11, 264, 28872, 321, 434, 1228, 307, 746, 1219], "temperature": 0.0, "avg_logprob": -0.23663990121138723, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.9943980734969955e-06}, {"id": 946, "seek": 629424, "start": 6303.0, "end": 6308.679999999999, "text": " Canvid and the dataset is actually frames from a video. So a lot of the frames look", "tokens": [1664, 6833, 293, 264, 28872, 307, 767, 12083, 490, 257, 960, 13, 407, 257, 688, 295, 264, 12083, 574], "temperature": 0.0, "avg_logprob": -0.23663990121138723, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.9943980734969955e-06}, {"id": 947, "seek": 629424, "start": 6308.679999999999, "end": 6316.679999999999, "text": " very similar to each other. And there's only like 600 frames in total, so there's very,", "tokens": [588, 2531, 281, 1184, 661, 13, 400, 456, 311, 787, 411, 11849, 12083, 294, 3217, 11, 370, 456, 311, 588, 11], "temperature": 0.0, "avg_logprob": -0.23663990121138723, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.9943980734969955e-06}, {"id": 948, "seek": 629424, "start": 6316.679999999999, "end": 6319.719999999999, "text": " very little data in this Canvid dataset.", "tokens": [588, 707, 1412, 294, 341, 1664, 6833, 28872, 13], "temperature": 0.0, "avg_logprob": -0.23663990121138723, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.9943980734969955e-06}, {"id": 949, "seek": 631972, "start": 6319.72, "end": 6326.6, "text": " Furthermore, we're not going to do any pre-training. So we're going to try and build a state-of-the-art", "tokens": [23999, 11, 321, 434, 406, 516, 281, 360, 604, 659, 12, 17227, 1760, 13, 407, 321, 434, 516, 281, 853, 293, 1322, 257, 1785, 12, 2670, 12, 3322, 12, 446], "temperature": 0.0, "avg_logprob": -0.1254126699347245, "compression_ratio": 1.5349794238683128, "no_speech_prob": 4.425463885127101e-06}, {"id": 950, "seek": 631972, "start": 6326.6, "end": 6332.240000000001, "text": " classification system on video, which is already much lower information content because most", "tokens": [21538, 1185, 322, 960, 11, 597, 307, 1217, 709, 3126, 1589, 2701, 570, 881], "temperature": 0.0, "avg_logprob": -0.1254126699347245, "compression_ratio": 1.5349794238683128, "no_speech_prob": 4.425463885127101e-06}, {"id": 951, "seek": 631972, "start": 6332.240000000001, "end": 6336.96, "text": " of the frames are pretty similar, using just 600 frames without pre-training. Now if you", "tokens": [295, 264, 12083, 366, 1238, 2531, 11, 1228, 445, 11849, 12083, 1553, 659, 12, 17227, 1760, 13, 823, 498, 291], "temperature": 0.0, "avg_logprob": -0.1254126699347245, "compression_ratio": 1.5349794238683128, "no_speech_prob": 4.425463885127101e-06}, {"id": 952, "seek": 631972, "start": 6336.96, "end": 6341.56, "text": " had asked me a month ago, I would have told you it's not possible. This just seems like", "tokens": [632, 2351, 385, 257, 1618, 2057, 11, 286, 576, 362, 1907, 291, 309, 311, 406, 1944, 13, 639, 445, 2544, 411], "temperature": 0.0, "avg_logprob": -0.1254126699347245, "compression_ratio": 1.5349794238683128, "no_speech_prob": 4.425463885127101e-06}, {"id": 953, "seek": 634156, "start": 6341.56, "end": 6349.64, "text": " an incredibly difficult thing to do. But just watch.", "tokens": [364, 6252, 2252, 551, 281, 360, 13, 583, 445, 1159, 13], "temperature": 0.0, "avg_logprob": -0.16158457271388318, "compression_ratio": 1.4713375796178343, "no_speech_prob": 9.36859032663051e-06}, {"id": 954, "seek": 634156, "start": 6349.64, "end": 6358.56, "text": " I'm going to skip to the answer first. Here's an example of a particular frame we're trying", "tokens": [286, 478, 516, 281, 10023, 281, 264, 1867, 700, 13, 1692, 311, 364, 1365, 295, 257, 1729, 3920, 321, 434, 1382], "temperature": 0.0, "avg_logprob": -0.16158457271388318, "compression_ratio": 1.4713375796178343, "no_speech_prob": 9.36859032663051e-06}, {"id": 955, "seek": 634156, "start": 6358.56, "end": 6367.96, "text": " to match. Here is the ground truth for that frame. You can see there's a tiny car here", "tokens": [281, 2995, 13, 1692, 307, 264, 2727, 3494, 337, 300, 3920, 13, 509, 393, 536, 456, 311, 257, 5870, 1032, 510], "temperature": 0.0, "avg_logprob": -0.16158457271388318, "compression_ratio": 1.4713375796178343, "no_speech_prob": 9.36859032663051e-06}, {"id": 956, "seek": 636796, "start": 6367.96, "end": 6377.4, "text": " and a little car here. There's a tree. Trees are really difficult. They're incredibly fine,", "tokens": [293, 257, 707, 1032, 510, 13, 821, 311, 257, 4230, 13, 314, 4856, 366, 534, 2252, 13, 814, 434, 6252, 2489, 11], "temperature": 0.0, "avg_logprob": -0.16647842407226562, "compression_ratio": 1.5542857142857143, "no_speech_prob": 5.338090431905584e-06}, {"id": 957, "seek": 636796, "start": 6377.4, "end": 6384.8, "text": " funny things. And here is my trained model. And as you can see, it's done really, really", "tokens": [4074, 721, 13, 400, 510, 307, 452, 8895, 2316, 13, 400, 382, 291, 393, 536, 11, 309, 311, 1096, 534, 11, 534], "temperature": 0.0, "avg_logprob": -0.16647842407226562, "compression_ratio": 1.5542857142857143, "no_speech_prob": 5.338090431905584e-06}, {"id": 958, "seek": 636796, "start": 6384.8, "end": 6387.28, "text": " well.", "tokens": [731, 13], "temperature": 0.0, "avg_logprob": -0.16647842407226562, "compression_ratio": 1.5542857142857143, "no_speech_prob": 5.338090431905584e-06}, {"id": 959, "seek": 636796, "start": 6387.28, "end": 6394.8, "text": " It's interesting to look at the mistakes it made. This little thing here is a person.", "tokens": [467, 311, 1880, 281, 574, 412, 264, 8038, 309, 1027, 13, 639, 707, 551, 510, 307, 257, 954, 13], "temperature": 0.0, "avg_logprob": -0.16647842407226562, "compression_ratio": 1.5542857142857143, "no_speech_prob": 5.338090431905584e-06}, {"id": 960, "seek": 639480, "start": 6394.8, "end": 6399.88, "text": " You can see that the person, their head looks a lot like traffic light and their jacket", "tokens": [509, 393, 536, 300, 264, 954, 11, 641, 1378, 1542, 257, 688, 411, 6419, 1442, 293, 641, 11781], "temperature": 0.0, "avg_logprob": -0.15895766773443112, "compression_ratio": 1.6820276497695852, "no_speech_prob": 4.157336661592126e-06}, {"id": 961, "seek": 639480, "start": 6399.88, "end": 6412.24, "text": " looks a lot like mailbox. Whereas these tiny little people here, it's done perfectly. Whereas", "tokens": [1542, 257, 688, 411, 43602, 13, 13813, 613, 5870, 707, 561, 510, 11, 309, 311, 1096, 6239, 13, 13813], "temperature": 0.0, "avg_logprob": -0.15895766773443112, "compression_ratio": 1.6820276497695852, "no_speech_prob": 4.157336661592126e-06}, {"id": 962, "seek": 639480, "start": 6412.24, "end": 6417.66, "text": " this person got a little bit confused. Another example of where it's gone wrong is this should", "tokens": [341, 954, 658, 257, 707, 857, 9019, 13, 3996, 1365, 295, 689, 309, 311, 2780, 2085, 307, 341, 820], "temperature": 0.0, "avg_logprob": -0.15895766773443112, "compression_ratio": 1.6820276497695852, "no_speech_prob": 4.157336661592126e-06}, {"id": 963, "seek": 639480, "start": 6417.66, "end": 6422.68, "text": " be a road, whereas it wasn't quite sure what was road and what was footpath. Which makes", "tokens": [312, 257, 3060, 11, 9735, 309, 2067, 380, 1596, 988, 437, 390, 3060, 293, 437, 390, 2671, 31852, 13, 3013, 1669], "temperature": 0.0, "avg_logprob": -0.15895766773443112, "compression_ratio": 1.6820276497695852, "no_speech_prob": 4.157336661592126e-06}, {"id": 964, "seek": 642268, "start": 6422.68, "end": 6429.88, "text": " sense, the colors do look very similar. But had we have pre-trained something, a pre-trained", "tokens": [2020, 11, 264, 4577, 360, 574, 588, 2531, 13, 583, 632, 321, 362, 659, 12, 17227, 2001, 746, 11, 257, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.16741117198815506, "compression_ratio": 1.6342592592592593, "no_speech_prob": 3.90546574635664e-06}, {"id": 965, "seek": 642268, "start": 6429.88, "end": 6435.4800000000005, "text": " network would have understood that crossroads tend to go straight across, they don't tend", "tokens": [3209, 576, 362, 7320, 300, 3278, 30342, 3928, 281, 352, 2997, 2108, 11, 436, 500, 380, 3928], "temperature": 0.0, "avg_logprob": -0.16741117198815506, "compression_ratio": 1.6342592592592593, "no_speech_prob": 3.90546574635664e-06}, {"id": 966, "seek": 642268, "start": 6435.4800000000005, "end": 6443.08, "text": " to look like that. So you can kind of see where the minor mistakes it made, it also", "tokens": [281, 574, 411, 300, 13, 407, 291, 393, 733, 295, 536, 689, 264, 6696, 8038, 309, 1027, 11, 309, 611], "temperature": 0.0, "avg_logprob": -0.16741117198815506, "compression_ratio": 1.6342592592592593, "no_speech_prob": 3.90546574635664e-06}, {"id": 967, "seek": 642268, "start": 6443.08, "end": 6448.08, "text": " would have learned, had it looked at more than a couple of hundred examples of people,", "tokens": [576, 362, 3264, 11, 632, 309, 2956, 412, 544, 813, 257, 1916, 295, 3262, 5110, 295, 561, 11], "temperature": 0.0, "avg_logprob": -0.16741117198815506, "compression_ratio": 1.6342592592592593, "no_speech_prob": 3.90546574635664e-06}, {"id": 968, "seek": 644808, "start": 6448.08, "end": 6452.8, "text": " that people generally are a particular shape. So there's just not enough data for it to", "tokens": [300, 561, 5101, 366, 257, 1729, 3909, 13, 407, 456, 311, 445, 406, 1547, 1412, 337, 309, 281], "temperature": 0.0, "avg_logprob": -0.20011877172133502, "compression_ratio": 1.5964125560538116, "no_speech_prob": 1.5534949397988385e-06}, {"id": 969, "seek": 644808, "start": 6452.8, "end": 6458.92, "text": " have learned some of these things. But nonetheless, it is extraordinarily effective. Look at this", "tokens": [362, 3264, 512, 295, 613, 721, 13, 583, 26756, 11, 309, 307, 34557, 4942, 13, 2053, 412, 341], "temperature": 0.0, "avg_logprob": -0.20011877172133502, "compression_ratio": 1.5964125560538116, "no_speech_prob": 1.5534949397988385e-06}, {"id": 970, "seek": 644808, "start": 6458.92, "end": 6465.16, "text": " traffic light, it's kind of surrounded by a sign. So the ground truth actually has the", "tokens": [6419, 1442, 11, 309, 311, 733, 295, 13221, 538, 257, 1465, 13, 407, 264, 2727, 3494, 767, 575, 264], "temperature": 0.0, "avg_logprob": -0.20011877172133502, "compression_ratio": 1.5964125560538116, "no_speech_prob": 1.5534949397988385e-06}, {"id": 971, "seek": 644808, "start": 6465.16, "end": 6470.72, "text": " traffic light and then a tiny little edge of sign, and it's even got that right. So", "tokens": [6419, 1442, 293, 550, 257, 5870, 707, 4691, 295, 1465, 11, 293, 309, 311, 754, 658, 300, 558, 13, 407], "temperature": 0.0, "avg_logprob": -0.20011877172133502, "compression_ratio": 1.5964125560538116, "no_speech_prob": 1.5534949397988385e-06}, {"id": 972, "seek": 647072, "start": 6470.72, "end": 6478.52, "text": " it's an incredibly accurate model. So how does it work? And in particular, how does", "tokens": [309, 311, 364, 6252, 8559, 2316, 13, 407, 577, 775, 309, 589, 30, 400, 294, 1729, 11, 577, 775], "temperature": 0.0, "avg_logprob": -0.19019080974437572, "compression_ratio": 1.3466666666666667, "no_speech_prob": 3.3405285648768768e-06}, {"id": 973, "seek": 647072, "start": 6478.52, "end": 6481.68, "text": " it do these amazing trees?", "tokens": [309, 360, 613, 2243, 5852, 30], "temperature": 0.0, "avg_logprob": -0.19019080974437572, "compression_ratio": 1.3466666666666667, "no_speech_prob": 3.3405285648768768e-06}, {"id": 974, "seek": 647072, "start": 6481.68, "end": 6494.2, "text": " So the answer is in this picture. Basically, until this is inspired by a model called UNET,", "tokens": [407, 264, 1867, 307, 294, 341, 3036, 13, 8537, 11, 1826, 341, 307, 7547, 538, 257, 2316, 1219, 8229, 4850, 11], "temperature": 0.0, "avg_logprob": -0.19019080974437572, "compression_ratio": 1.3466666666666667, "no_speech_prob": 3.3405285648768768e-06}, {"id": 975, "seek": 649420, "start": 6494.2, "end": 6501.0, "text": " until the UNET model came along, everybody was doing these kinds of segmentation models", "tokens": [1826, 264, 8229, 4850, 2316, 1361, 2051, 11, 2201, 390, 884, 613, 3685, 295, 9469, 399, 5245], "temperature": 0.0, "avg_logprob": -0.12130347693838724, "compression_ratio": 1.550660792951542, "no_speech_prob": 4.785029886988923e-06}, {"id": 976, "seek": 649420, "start": 6501.0, "end": 6506.88, "text": " using an approach just like what we did for style transfer, which is basically you have", "tokens": [1228, 364, 3109, 445, 411, 437, 321, 630, 337, 3758, 5003, 11, 597, 307, 1936, 291, 362], "temperature": 0.0, "avg_logprob": -0.12130347693838724, "compression_ratio": 1.550660792951542, "no_speech_prob": 4.785029886988923e-06}, {"id": 977, "seek": 649420, "start": 6506.88, "end": 6515.28, "text": " a number of convolutional layers with max pooling or with a stride of 2, which gradually", "tokens": [257, 1230, 295, 45216, 304, 7914, 365, 11469, 7005, 278, 420, 365, 257, 1056, 482, 295, 568, 11, 597, 13145], "temperature": 0.0, "avg_logprob": -0.12130347693838724, "compression_ratio": 1.550660792951542, "no_speech_prob": 4.785029886988923e-06}, {"id": 978, "seek": 649420, "start": 6515.28, "end": 6520.76, "text": " make the image smaller and smaller, bigger receptive field, and then you go back up the", "tokens": [652, 264, 3256, 4356, 293, 4356, 11, 3801, 45838, 2519, 11, 293, 550, 291, 352, 646, 493, 264], "temperature": 0.0, "avg_logprob": -0.12130347693838724, "compression_ratio": 1.550660792951542, "no_speech_prob": 4.785029886988923e-06}, {"id": 979, "seek": 652076, "start": 6520.76, "end": 6527.280000000001, "text": " other side using up-sampling or decompolutions until you get back to the original size, and", "tokens": [661, 1252, 1228, 493, 12, 19988, 11970, 420, 22867, 15892, 1826, 291, 483, 646, 281, 264, 3380, 2744, 11, 293], "temperature": 0.0, "avg_logprob": -0.13839506521457579, "compression_ratio": 1.5891089108910892, "no_speech_prob": 6.854257662780583e-06}, {"id": 980, "seek": 652076, "start": 6527.280000000001, "end": 6532.6, "text": " then your final layer is the same size as your starting layer and has a bunch of different", "tokens": [550, 428, 2572, 4583, 307, 264, 912, 2744, 382, 428, 2891, 4583, 293, 575, 257, 3840, 295, 819], "temperature": 0.0, "avg_logprob": -0.13839506521457579, "compression_ratio": 1.5891089108910892, "no_speech_prob": 6.854257662780583e-06}, {"id": 981, "seek": 652076, "start": 6532.6, "end": 6539.0, "text": " classes that you're trying to use in the softmax.", "tokens": [5359, 300, 291, 434, 1382, 281, 764, 294, 264, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.13839506521457579, "compression_ratio": 1.5891089108910892, "no_speech_prob": 6.854257662780583e-06}, {"id": 982, "seek": 652076, "start": 6539.0, "end": 6545.4400000000005, "text": " The problem with that is that you end up with, in fact I'll show you an example, there's", "tokens": [440, 1154, 365, 300, 307, 300, 291, 917, 493, 365, 11, 294, 1186, 286, 603, 855, 291, 364, 1365, 11, 456, 311], "temperature": 0.0, "avg_logprob": -0.13839506521457579, "compression_ratio": 1.5891089108910892, "no_speech_prob": 6.854257662780583e-06}, {"id": 983, "seek": 654544, "start": 6545.44, "end": 6554.2, "text": " a really nice paper called ENET. ENET is not only an incredibly accurate model for segmentation,", "tokens": [257, 534, 1481, 3035, 1219, 15244, 4850, 13, 15244, 4850, 307, 406, 787, 364, 6252, 8559, 2316, 337, 9469, 399, 11], "temperature": 0.0, "avg_logprob": -0.15726566314697266, "compression_ratio": 1.6143497757847534, "no_speech_prob": 5.955086180620128e-06}, {"id": 984, "seek": 654544, "start": 6554.2, "end": 6559.759999999999, "text": " but it's also incredibly fast. It actually can run in real-time. You can actually run", "tokens": [457, 309, 311, 611, 6252, 2370, 13, 467, 767, 393, 1190, 294, 957, 12, 3766, 13, 509, 393, 767, 1190], "temperature": 0.0, "avg_logprob": -0.15726566314697266, "compression_ratio": 1.6143497757847534, "no_speech_prob": 5.955086180620128e-06}, {"id": 985, "seek": 654544, "start": 6559.759999999999, "end": 6565.12, "text": " it on a video. But the mistakes it makes, look at this chair. This chair has a big gap", "tokens": [309, 322, 257, 960, 13, 583, 264, 8038, 309, 1669, 11, 574, 412, 341, 6090, 13, 639, 6090, 575, 257, 955, 7417], "temperature": 0.0, "avg_logprob": -0.15726566314697266, "compression_ratio": 1.6143497757847534, "no_speech_prob": 5.955086180620128e-06}, {"id": 986, "seek": 654544, "start": 6565.12, "end": 6571.879999999999, "text": " here and here and here, but ENET gets it totally wrong. And the reason why is because they", "tokens": [510, 293, 510, 293, 510, 11, 457, 15244, 4850, 2170, 309, 3879, 2085, 13, 400, 264, 1778, 983, 307, 570, 436], "temperature": 0.0, "avg_logprob": -0.15726566314697266, "compression_ratio": 1.6143497757847534, "no_speech_prob": 5.955086180620128e-06}, {"id": 987, "seek": 657188, "start": 6571.88, "end": 6577.92, "text": " use a very traditional down-sampling, up-sampling approach. And by the time they get to the", "tokens": [764, 257, 588, 5164, 760, 12, 19988, 11970, 11, 493, 12, 19988, 11970, 3109, 13, 400, 538, 264, 565, 436, 483, 281, 264], "temperature": 0.0, "avg_logprob": -0.11142479080751717, "compression_ratio": 1.5686274509803921, "no_speech_prob": 3.555971716195927e-06}, {"id": 988, "seek": 657188, "start": 6577.92, "end": 6582.96, "text": " bottom, they've just lost track of the fine detail.", "tokens": [2767, 11, 436, 600, 445, 2731, 2837, 295, 264, 2489, 2607, 13], "temperature": 0.0, "avg_logprob": -0.11142479080751717, "compression_ratio": 1.5686274509803921, "no_speech_prob": 3.555971716195927e-06}, {"id": 989, "seek": 657188, "start": 6582.96, "end": 6590.72, "text": " So the trick are these connections here. What we do is we start with our input, we do a", "tokens": [407, 264, 4282, 366, 613, 9271, 510, 13, 708, 321, 360, 307, 321, 722, 365, 527, 4846, 11, 321, 360, 257], "temperature": 0.0, "avg_logprob": -0.11142479080751717, "compression_ratio": 1.5686274509803921, "no_speech_prob": 3.555971716195927e-06}, {"id": 990, "seek": 657188, "start": 6590.72, "end": 6595.32, "text": " standard initial convolution, just like we did with style transfer. We then have a dense", "tokens": [3832, 5883, 45216, 11, 445, 411, 321, 630, 365, 3758, 5003, 13, 492, 550, 362, 257, 18011], "temperature": 0.0, "avg_logprob": -0.11142479080751717, "compression_ratio": 1.5686274509803921, "no_speech_prob": 3.555971716195927e-06}, {"id": 991, "seek": 659532, "start": 6595.32, "end": 6602.36, "text": " net block, which we learned about last week. And then that block, we keep going down, we", "tokens": [2533, 3461, 11, 597, 321, 3264, 466, 1036, 1243, 13, 400, 550, 300, 3461, 11, 321, 1066, 516, 760, 11, 321], "temperature": 0.0, "avg_logprob": -0.17385761410582298, "compression_ratio": 1.9782608695652173, "no_speech_prob": 1.172636189039622e-06}, {"id": 992, "seek": 659532, "start": 6602.36, "end": 6606.96, "text": " do a max-pooling type thing, another dense net block, max-pooling type thing, keep going", "tokens": [360, 257, 11469, 12, 17374, 278, 2010, 551, 11, 1071, 18011, 2533, 3461, 11, 11469, 12, 17374, 278, 2010, 551, 11, 1066, 516], "temperature": 0.0, "avg_logprob": -0.17385761410582298, "compression_ratio": 1.9782608695652173, "no_speech_prob": 1.172636189039622e-06}, {"id": 993, "seek": 659532, "start": 6606.96, "end": 6615.0, "text": " down. And then as we go up the other side, so we do a deconvolution, dense block, deconvolution,", "tokens": [760, 13, 400, 550, 382, 321, 352, 493, 264, 661, 1252, 11, 370, 321, 360, 257, 979, 266, 85, 3386, 11, 18011, 3461, 11, 979, 266, 85, 3386, 11], "temperature": 0.0, "avg_logprob": -0.17385761410582298, "compression_ratio": 1.9782608695652173, "no_speech_prob": 1.172636189039622e-06}, {"id": 994, "seek": 659532, "start": 6615.0, "end": 6622.12, "text": " dense block, we take the output from the dense block on the way down and we actually copy", "tokens": [18011, 3461, 11, 321, 747, 264, 5598, 490, 264, 18011, 3461, 322, 264, 636, 760, 293, 321, 767, 5055], "temperature": 0.0, "avg_logprob": -0.17385761410582298, "compression_ratio": 1.9782608695652173, "no_speech_prob": 1.172636189039622e-06}, {"id": 995, "seek": 662212, "start": 6622.12, "end": 6627.46, "text": " it over to here and concatenate the two together.", "tokens": [309, 670, 281, 510, 293, 1588, 7186, 473, 264, 732, 1214, 13], "temperature": 0.0, "avg_logprob": -0.23656120754423596, "compression_ratio": 1.5186915887850467, "no_speech_prob": 1.3419771676126402e-05}, {"id": 996, "seek": 662212, "start": 6627.46, "end": 6634.5599999999995, "text": " So actually, Brendan a few days ago actually drew this on our whiteboard when we were explaining", "tokens": [407, 767, 11, 48484, 257, 1326, 1708, 2057, 767, 12804, 341, 322, 527, 2418, 3787, 562, 321, 645, 13468], "temperature": 0.0, "avg_logprob": -0.23656120754423596, "compression_ratio": 1.5186915887850467, "no_speech_prob": 1.3419771676126402e-05}, {"id": 997, "seek": 662212, "start": 6634.5599999999995, "end": 6642.68, "text": " it to Melissa, and so he's shown us every stage here. We start out with a 224x224 input,", "tokens": [309, 281, 22844, 11, 293, 370, 415, 311, 4898, 505, 633, 3233, 510, 13, 492, 722, 484, 365, 257, 5853, 19, 87, 7490, 19, 4846, 11], "temperature": 0.0, "avg_logprob": -0.23656120754423596, "compression_ratio": 1.5186915887850467, "no_speech_prob": 1.3419771676126402e-05}, {"id": 998, "seek": 662212, "start": 6642.68, "end": 6650.5199999999995, "text": " goes through the convolutions with 48 filters, goes through our dense block, adds another", "tokens": [1709, 807, 264, 3754, 15892, 365, 11174, 15995, 11, 1709, 807, 527, 18011, 3461, 11, 10860, 1071], "temperature": 0.0, "avg_logprob": -0.23656120754423596, "compression_ratio": 1.5186915887850467, "no_speech_prob": 1.3419771676126402e-05}, {"id": 999, "seek": 665052, "start": 6650.52, "end": 6656.280000000001, "text": " 80 filters, and then goes throughout, they call it a transition down, so basically a", "tokens": [4688, 15995, 11, 293, 550, 1709, 3710, 11, 436, 818, 309, 257, 6034, 760, 11, 370, 1936, 257], "temperature": 0.0, "avg_logprob": -0.15440484519316772, "compression_ratio": 1.6411483253588517, "no_speech_prob": 8.013451406441163e-06}, {"id": 1000, "seek": 665052, "start": 6656.280000000001, "end": 6662.0, "text": " max-pooling, so it's now size 112. We keep doing that. Dense block, transition down,", "tokens": [11469, 12, 17374, 278, 11, 370, 309, 311, 586, 2744, 45835, 13, 492, 1066, 884, 300, 13, 413, 1288, 3461, 11, 6034, 760, 11], "temperature": 0.0, "avg_logprob": -0.15440484519316772, "compression_ratio": 1.6411483253588517, "no_speech_prob": 8.013451406441163e-06}, {"id": 1001, "seek": 665052, "start": 6662.0, "end": 6670.6, "text": " so it's now 56x56, 28x28, 14x14, 7x7, and then on the way up again, we go transition", "tokens": [370, 309, 311, 586, 19687, 87, 18317, 11, 7562, 87, 11205, 11, 3499, 87, 7271, 11, 1614, 87, 22, 11, 293, 550, 322, 264, 636, 493, 797, 11, 321, 352, 6034], "temperature": 0.0, "avg_logprob": -0.15440484519316772, "compression_ratio": 1.6411483253588517, "no_speech_prob": 8.013451406441163e-06}, {"id": 1002, "seek": 665052, "start": 6670.6, "end": 6678.4400000000005, "text": " up, it's now 14x14. We copy across the results of the 14x14 from the transition down and", "tokens": [493, 11, 309, 311, 586, 3499, 87, 7271, 13, 492, 5055, 2108, 264, 3542, 295, 264, 3499, 87, 7271, 490, 264, 6034, 760, 293], "temperature": 0.0, "avg_logprob": -0.15440484519316772, "compression_ratio": 1.6411483253588517, "no_speech_prob": 8.013451406441163e-06}, {"id": 1003, "seek": 667844, "start": 6678.44, "end": 6684.599999999999, "text": " concatenate together. Then we do a dense block, transition up, it's now 28x28, so we copy", "tokens": [1588, 7186, 473, 1214, 13, 1396, 321, 360, 257, 18011, 3461, 11, 6034, 493, 11, 309, 311, 586, 7562, 87, 11205, 11, 370, 321, 5055], "temperature": 0.0, "avg_logprob": -0.09823006132374638, "compression_ratio": 1.5305164319248827, "no_speech_prob": 1.308174660152872e-06}, {"id": 1004, "seek": 667844, "start": 6684.599999999999, "end": 6689.24, "text": " across our 28x28 from the transition down and so forth.", "tokens": [2108, 527, 7562, 87, 11205, 490, 264, 6034, 760, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.09823006132374638, "compression_ratio": 1.5305164319248827, "no_speech_prob": 1.308174660152872e-06}, {"id": 1005, "seek": 667844, "start": 6689.24, "end": 6695.799999999999, "text": " So by the time we get all the way back up here, we're actually copying across something", "tokens": [407, 538, 264, 565, 321, 483, 439, 264, 636, 646, 493, 510, 11, 321, 434, 767, 27976, 2108, 746], "temperature": 0.0, "avg_logprob": -0.09823006132374638, "compression_ratio": 1.5305164319248827, "no_speech_prob": 1.308174660152872e-06}, {"id": 1006, "seek": 667844, "start": 6695.799999999999, "end": 6704.639999999999, "text": " that was originally of size 224x224. It hadn't had much done to it. It had only been through", "tokens": [300, 390, 7993, 295, 2744, 5853, 19, 87, 7490, 19, 13, 467, 8782, 380, 632, 709, 1096, 281, 309, 13, 467, 632, 787, 668, 807], "temperature": 0.0, "avg_logprob": -0.09823006132374638, "compression_ratio": 1.5305164319248827, "no_speech_prob": 1.308174660152872e-06}, {"id": 1007, "seek": 670464, "start": 6704.64, "end": 6709.4400000000005, "text": " one convolutional layer and one dense block, so it hadn't really got much rich computation", "tokens": [472, 45216, 304, 4583, 293, 472, 18011, 3461, 11, 370, 309, 8782, 380, 534, 658, 709, 4593, 24903], "temperature": 0.0, "avg_logprob": -0.13353143557153566, "compression_ratio": 1.6946902654867257, "no_speech_prob": 8.398014870181214e-06}, {"id": 1008, "seek": 670464, "start": 6709.4400000000005, "end": 6716.360000000001, "text": " being done. But the thing is, by the time it gets back up all the way up here, the model", "tokens": [885, 1096, 13, 583, 264, 551, 307, 11, 538, 264, 565, 309, 2170, 646, 493, 439, 264, 636, 493, 510, 11, 264, 2316], "temperature": 0.0, "avg_logprob": -0.13353143557153566, "compression_ratio": 1.6946902654867257, "no_speech_prob": 8.398014870181214e-06}, {"id": 1009, "seek": 670464, "start": 6716.360000000001, "end": 6721.96, "text": " knows pretty much this is a tree, this is a person, and this is a house, and it just", "tokens": [3255, 1238, 709, 341, 307, 257, 4230, 11, 341, 307, 257, 954, 11, 293, 341, 307, 257, 1782, 11, 293, 309, 445], "temperature": 0.0, "avg_logprob": -0.13353143557153566, "compression_ratio": 1.6946902654867257, "no_speech_prob": 8.398014870181214e-06}, {"id": 1010, "seek": 670464, "start": 6721.96, "end": 6726.56, "text": " needs to get the fine little details. Where exactly does this leaf finish? Where exactly", "tokens": [2203, 281, 483, 264, 2489, 707, 4365, 13, 2305, 2293, 775, 341, 10871, 2413, 30, 2305, 2293], "temperature": 0.0, "avg_logprob": -0.13353143557153566, "compression_ratio": 1.6946902654867257, "no_speech_prob": 8.398014870181214e-06}, {"id": 1011, "seek": 670464, "start": 6726.56, "end": 6728.360000000001, "text": " does the person's hat finish?", "tokens": [775, 264, 954, 311, 2385, 2413, 30], "temperature": 0.0, "avg_logprob": -0.13353143557153566, "compression_ratio": 1.6946902654867257, "no_speech_prob": 8.398014870181214e-06}, {"id": 1012, "seek": 672836, "start": 6728.36, "end": 6735.24, "text": " So it's basically copying across something which is very high resolution but doesn't", "tokens": [407, 309, 311, 1936, 27976, 2108, 746, 597, 307, 588, 1090, 8669, 457, 1177, 380], "temperature": 0.0, "avg_logprob": -0.16372267807586283, "compression_ratio": 1.5244444444444445, "no_speech_prob": 2.482449644958251e-06}, {"id": 1013, "seek": 672836, "start": 6735.24, "end": 6739.5599999999995, "text": " have that much rich information, but that's fine because it really only needs to fill", "tokens": [362, 300, 709, 4593, 1589, 11, 457, 300, 311, 2489, 570, 309, 534, 787, 2203, 281, 2836], "temperature": 0.0, "avg_logprob": -0.16372267807586283, "compression_ratio": 1.5244444444444445, "no_speech_prob": 2.482449644958251e-06}, {"id": 1014, "seek": 672836, "start": 6739.5599999999995, "end": 6744.5199999999995, "text": " in the details. So these things here, they're called skip connections. They were really", "tokens": [294, 264, 4365, 13, 407, 613, 721, 510, 11, 436, 434, 1219, 10023, 9271, 13, 814, 645, 534], "temperature": 0.0, "avg_logprob": -0.16372267807586283, "compression_ratio": 1.5244444444444445, "no_speech_prob": 2.482449644958251e-06}, {"id": 1015, "seek": 672836, "start": 6744.5199999999995, "end": 6752.679999999999, "text": " inspired by this paper called UNET, which has won many Kaggle competitions. But it's", "tokens": [7547, 538, 341, 3035, 1219, 8229, 4850, 11, 597, 575, 1582, 867, 48751, 22631, 26185, 13, 583, 309, 311], "temperature": 0.0, "avg_logprob": -0.16372267807586283, "compression_ratio": 1.5244444444444445, "no_speech_prob": 2.482449644958251e-06}, {"id": 1016, "seek": 675268, "start": 6752.68, "end": 6758.52, "text": " using dense blocks rather than normal fully connected blocks.", "tokens": [1228, 18011, 8474, 2831, 813, 2710, 4498, 4582, 8474, 13], "temperature": 0.0, "avg_logprob": -0.1588382047765395, "compression_ratio": 1.569377990430622, "no_speech_prob": 6.854226739960723e-06}, {"id": 1017, "seek": 675268, "start": 6758.52, "end": 6765.96, "text": " So let me show you. We're not going to have time to go into this in detail, but I've done", "tokens": [407, 718, 385, 855, 291, 13, 492, 434, 406, 516, 281, 362, 565, 281, 352, 666, 341, 294, 2607, 11, 457, 286, 600, 1096], "temperature": 0.0, "avg_logprob": -0.1588382047765395, "compression_ratio": 1.569377990430622, "no_speech_prob": 6.854226739960723e-06}, {"id": 1018, "seek": 675268, "start": 6765.96, "end": 6771.240000000001, "text": " all this coding Keras from scratch. This is actually a fantastic fit for Keras. I didn't", "tokens": [439, 341, 17720, 591, 6985, 490, 8459, 13, 639, 307, 767, 257, 5456, 3318, 337, 591, 6985, 13, 286, 994, 380], "temperature": 0.0, "avg_logprob": -0.1588382047765395, "compression_ratio": 1.569377990430622, "no_speech_prob": 6.854226739960723e-06}, {"id": 1019, "seek": 675268, "start": 6771.240000000001, "end": 6776.76, "text": " have to create any custom layers, I didn't have to do anything weird at all, except for", "tokens": [362, 281, 1884, 604, 2375, 7914, 11, 286, 994, 380, 362, 281, 360, 1340, 3657, 412, 439, 11, 3993, 337], "temperature": 0.0, "avg_logprob": -0.1588382047765395, "compression_ratio": 1.569377990430622, "no_speech_prob": 6.854226739960723e-06}, {"id": 1020, "seek": 677676, "start": 6776.76, "end": 6787.56, "text": " one thing, data augmentation. So the data augmentation was we start with 480x360 images,", "tokens": [472, 551, 11, 1412, 14501, 19631, 13, 407, 264, 1412, 14501, 19631, 390, 321, 722, 365, 1017, 4702, 87, 34099, 5267, 11], "temperature": 0.0, "avg_logprob": -0.14333225303972272, "compression_ratio": 1.435483870967742, "no_speech_prob": 5.09365872858325e-06}, {"id": 1021, "seek": 677676, "start": 6787.56, "end": 6795.24, "text": " we randomly crop some 224x224 part, and also randomly we may flip it horizontally. That's", "tokens": [321, 16979, 9086, 512, 5853, 19, 87, 7490, 19, 644, 11, 293, 611, 16979, 321, 815, 7929, 309, 33796, 13, 663, 311], "temperature": 0.0, "avg_logprob": -0.14333225303972272, "compression_ratio": 1.435483870967742, "no_speech_prob": 5.09365872858325e-06}, {"id": 1022, "seek": 677676, "start": 6795.24, "end": 6801.72, "text": " all perfectly fine. Well, Keras doesn't really have the random crops, unfortunately. But", "tokens": [439, 6239, 2489, 13, 1042, 11, 591, 6985, 1177, 380, 534, 362, 264, 4974, 16829, 11, 7015, 13, 583], "temperature": 0.0, "avg_logprob": -0.14333225303972272, "compression_ratio": 1.435483870967742, "no_speech_prob": 5.09365872858325e-06}, {"id": 1023, "seek": 680172, "start": 6801.72, "end": 6807.8, "text": " more importantly, whatever we do to the input image, we also have to do to the target image.", "tokens": [544, 8906, 11, 2035, 321, 360, 281, 264, 4846, 3256, 11, 321, 611, 362, 281, 360, 281, 264, 3779, 3256, 13], "temperature": 0.0, "avg_logprob": -0.13421991597051205, "compression_ratio": 1.5029239766081872, "no_speech_prob": 2.225262960564578e-06}, {"id": 1024, "seek": 680172, "start": 6807.8, "end": 6813.08, "text": " We need to get the same 224x224 crop and we need to do the same horizontal flip. So I", "tokens": [492, 643, 281, 483, 264, 912, 5853, 19, 87, 7490, 19, 9086, 293, 321, 643, 281, 360, 264, 912, 12750, 7929, 13, 407, 286], "temperature": 0.0, "avg_logprob": -0.13421991597051205, "compression_ratio": 1.5029239766081872, "no_speech_prob": 2.225262960564578e-06}, {"id": 1025, "seek": 680172, "start": 6813.08, "end": 6824.76, "text": " had to write a data generator, which you guys may actually find useful anyway.", "tokens": [632, 281, 2464, 257, 1412, 19265, 11, 597, 291, 1074, 815, 767, 915, 4420, 4033, 13], "temperature": 0.0, "avg_logprob": -0.13421991597051205, "compression_ratio": 1.5029239766081872, "no_speech_prob": 2.225262960564578e-06}, {"id": 1026, "seek": 682476, "start": 6824.76, "end": 6832.8, "text": " So this is my data generator. Basically I call it a segment generator. It's just a standard", "tokens": [407, 341, 307, 452, 1412, 19265, 13, 8537, 286, 818, 309, 257, 9469, 19265, 13, 467, 311, 445, 257, 3832], "temperature": 0.0, "avg_logprob": -0.11369841126189835, "compression_ratio": 1.6529680365296804, "no_speech_prob": 7.766858288960066e-06}, {"id": 1027, "seek": 682476, "start": 6832.8, "end": 6838.24, "text": " generator, so it's got a next function. Each time you call next, it grabs some random bunch", "tokens": [19265, 11, 370, 309, 311, 658, 257, 958, 2445, 13, 6947, 565, 291, 818, 958, 11, 309, 30028, 512, 4974, 3840], "temperature": 0.0, "avg_logprob": -0.11369841126189835, "compression_ratio": 1.6529680365296804, "no_speech_prob": 7.766858288960066e-06}, {"id": 1028, "seek": 682476, "start": 6838.24, "end": 6846.26, "text": " of indexes, it goes through each one of those indexes and grabs the necessary item, grabbing", "tokens": [295, 8186, 279, 11, 309, 1709, 807, 1184, 472, 295, 729, 8186, 279, 293, 30028, 264, 4818, 3174, 11, 23771], "temperature": 0.0, "avg_logprob": -0.11369841126189835, "compression_ratio": 1.6529680365296804, "no_speech_prob": 7.766858288960066e-06}, {"id": 1029, "seek": 682476, "start": 6846.26, "end": 6851.92, "text": " a random slice, sometimes randomly flipping it horizontally, and then it's doing this", "tokens": [257, 4974, 13153, 11, 2171, 16979, 26886, 309, 33796, 11, 293, 550, 309, 311, 884, 341], "temperature": 0.0, "avg_logprob": -0.11369841126189835, "compression_ratio": 1.6529680365296804, "no_speech_prob": 7.766858288960066e-06}, {"id": 1030, "seek": 685192, "start": 6851.92, "end": 6859.28, "text": " to both the Xs and the Ys, returning them back.", "tokens": [281, 1293, 264, 1783, 82, 293, 264, 398, 82, 11, 12678, 552, 646, 13], "temperature": 0.0, "avg_logprob": -0.17679635784294032, "compression_ratio": 1.5858585858585859, "no_speech_prob": 8.53023630043026e-06}, {"id": 1031, "seek": 685192, "start": 6859.28, "end": 6866.26, "text": " Along with this segment generator, in order to randomly grab a batch of random indexes", "tokens": [17457, 365, 341, 9469, 19265, 11, 294, 1668, 281, 16979, 4444, 257, 15245, 295, 4974, 8186, 279], "temperature": 0.0, "avg_logprob": -0.17679635784294032, "compression_ratio": 1.5858585858585859, "no_speech_prob": 8.53023630043026e-06}, {"id": 1032, "seek": 685192, "start": 6866.26, "end": 6874.0, "text": " each time, I created this little class called batch indexes, which can basically do that.", "tokens": [1184, 565, 11, 286, 2942, 341, 707, 1508, 1219, 15245, 8186, 279, 11, 597, 393, 1936, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.17679635784294032, "compression_ratio": 1.5858585858585859, "no_speech_prob": 8.53023630043026e-06}, {"id": 1033, "seek": 685192, "start": 6874.0, "end": 6880.72, "text": " It can have either shuffle true or shuffle false. So this pair of classes, you guys might", "tokens": [467, 393, 362, 2139, 39426, 2074, 420, 39426, 7908, 13, 407, 341, 6119, 295, 5359, 11, 291, 1074, 1062], "temperature": 0.0, "avg_logprob": -0.17679635784294032, "compression_ratio": 1.5858585858585859, "no_speech_prob": 8.53023630043026e-06}, {"id": 1034, "seek": 688072, "start": 6880.72, "end": 6885.72, "text": " find really helpful for creating your own data generators. This batch indexes class", "tokens": [915, 534, 4961, 337, 4084, 428, 1065, 1412, 38662, 13, 639, 15245, 8186, 279, 1508], "temperature": 0.0, "avg_logprob": -0.16909191045868263, "compression_ratio": 1.5162790697674418, "no_speech_prob": 9.223426786775235e-06}, {"id": 1035, "seek": 688072, "start": 6885.72, "end": 6891.360000000001, "text": " in particular, now that I've written it, you can see how it works.", "tokens": [294, 1729, 11, 586, 300, 286, 600, 3720, 309, 11, 291, 393, 536, 577, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.16909191045868263, "compression_ratio": 1.5162790697674418, "no_speech_prob": 9.223426786775235e-06}, {"id": 1036, "seek": 688072, "start": 6891.360000000001, "end": 6898.16, "text": " If I say batch indexes from a data set of size 10, I want to grab 3 indexes at a time,", "tokens": [759, 286, 584, 15245, 8186, 279, 490, 257, 1412, 992, 295, 2744, 1266, 11, 286, 528, 281, 4444, 805, 8186, 279, 412, 257, 565, 11], "temperature": 0.0, "avg_logprob": -0.16909191045868263, "compression_ratio": 1.5162790697674418, "no_speech_prob": 9.223426786775235e-06}, {"id": 1037, "seek": 688072, "start": 6898.16, "end": 6904.08, "text": " so then let's grab 5 batches. Now in this case I've got by default shuffle equals false,", "tokens": [370, 550, 718, 311, 4444, 1025, 15245, 279, 13, 823, 294, 341, 1389, 286, 600, 658, 538, 7576, 39426, 6915, 7908, 11], "temperature": 0.0, "avg_logprob": -0.16909191045868263, "compression_ratio": 1.5162790697674418, "no_speech_prob": 9.223426786775235e-06}, {"id": 1038, "seek": 690408, "start": 6904.08, "end": 6912.88, "text": " so it just returns 012, 345, 678, 9, and finished. On the other hand, if I say shuffle equals", "tokens": [370, 309, 445, 11247, 1958, 4762, 11, 805, 8465, 11, 23879, 23, 11, 1722, 11, 293, 4335, 13, 1282, 264, 661, 1011, 11, 498, 286, 584, 39426, 6915], "temperature": 0.0, "avg_logprob": -0.17666823605456985, "compression_ratio": 1.5051020408163265, "no_speech_prob": 2.1568118881987175e-06}, {"id": 1039, "seek": 690408, "start": 6912.88, "end": 6918.76, "text": " true, it returns them in random order, but it still makes sure it captures all of them.", "tokens": [2074, 11, 309, 11247, 552, 294, 4974, 1668, 11, 457, 309, 920, 1669, 988, 309, 27986, 439, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.17666823605456985, "compression_ratio": 1.5051020408163265, "no_speech_prob": 2.1568118881987175e-06}, {"id": 1040, "seek": 690408, "start": 6918.76, "end": 6923.08, "text": " And then when we're done, it starts a new random order. So this makes it really easy", "tokens": [400, 550, 562, 321, 434, 1096, 11, 309, 3719, 257, 777, 4974, 1668, 13, 407, 341, 1669, 309, 534, 1858], "temperature": 0.0, "avg_logprob": -0.17666823605456985, "compression_ratio": 1.5051020408163265, "no_speech_prob": 2.1568118881987175e-06}, {"id": 1041, "seek": 690408, "start": 6923.08, "end": 6926.6, "text": " to create random generators.", "tokens": [281, 1884, 4974, 38662, 13], "temperature": 0.0, "avg_logprob": -0.17666823605456985, "compression_ratio": 1.5051020408163265, "no_speech_prob": 2.1568118881987175e-06}, {"id": 1042, "seek": 692660, "start": 6926.6, "end": 6934.360000000001, "text": " So that was the only thing I had to add to Keras to get this all to work. Other than", "tokens": [407, 300, 390, 264, 787, 551, 286, 632, 281, 909, 281, 591, 6985, 281, 483, 341, 439, 281, 589, 13, 5358, 813], "temperature": 0.0, "avg_logprob": -0.17882413130540115, "compression_ratio": 1.5542168674698795, "no_speech_prob": 3.785311719184392e-06}, {"id": 1043, "seek": 692660, "start": 6934.360000000001, "end": 6940.56, "text": " that, we wrote the tiramisu. The tiramisu looks very very similar to the DenseNet that", "tokens": [300, 11, 321, 4114, 264, 13807, 335, 25871, 13, 440, 13807, 335, 25871, 1542, 588, 588, 2531, 281, 264, 413, 1288, 31890, 300], "temperature": 0.0, "avg_logprob": -0.17882413130540115, "compression_ratio": 1.5542168674698795, "no_speech_prob": 3.785311719184392e-06}, {"id": 1044, "seek": 692660, "start": 6940.56, "end": 6950.04, "text": " we saw last week. We've got all our pieces, the relu, the dropout, the batch norm, the", "tokens": [321, 1866, 1036, 1243, 13, 492, 600, 658, 439, 527, 3755, 11, 264, 1039, 84, 11, 264, 3270, 346, 11, 264, 15245, 2026, 11, 264], "temperature": 0.0, "avg_logprob": -0.17882413130540115, "compression_ratio": 1.5542168674698795, "no_speech_prob": 3.785311719184392e-06}, {"id": 1045, "seek": 695004, "start": 6950.04, "end": 6957.48, "text": " relu on top of batch norm, a concat layer, so this is something I had to add, a convolution", "tokens": [1039, 84, 322, 1192, 295, 15245, 2026, 11, 257, 1588, 267, 4583, 11, 370, 341, 307, 746, 286, 632, 281, 909, 11, 257, 45216], "temperature": 0.0, "avg_logprob": -0.13280823254826093, "compression_ratio": 1.7393364928909953, "no_speech_prob": 4.3568588807829656e-06}, {"id": 1046, "seek": 695004, "start": 6957.48, "end": 6963.48, "text": " 2D followed by dropout, and then finally my batch norm followed by relu followed by convolution", "tokens": [568, 35, 6263, 538, 3270, 346, 11, 293, 550, 2721, 452, 15245, 2026, 6263, 538, 1039, 84, 6263, 538, 45216], "temperature": 0.0, "avg_logprob": -0.13280823254826093, "compression_ratio": 1.7393364928909953, "no_speech_prob": 4.3568588807829656e-06}, {"id": 1047, "seek": 695004, "start": 6963.48, "end": 6964.96, "text": " 2D.", "tokens": [568, 35, 13], "temperature": 0.0, "avg_logprob": -0.13280823254826093, "compression_ratio": 1.7393364928909953, "no_speech_prob": 4.3568588807829656e-06}, {"id": 1048, "seek": 695004, "start": 6964.96, "end": 6970.44, "text": " So this is just the dense block that we saw last week. So a dense block is something where", "tokens": [407, 341, 307, 445, 264, 18011, 3461, 300, 321, 1866, 1036, 1243, 13, 407, 257, 18011, 3461, 307, 746, 689], "temperature": 0.0, "avg_logprob": -0.13280823254826093, "compression_ratio": 1.7393364928909953, "no_speech_prob": 4.3568588807829656e-06}, {"id": 1049, "seek": 695004, "start": 6970.44, "end": 6976.84, "text": " we just keep grabbing 12 or 16 filters at a time, concatenating them to the last set", "tokens": [321, 445, 1066, 23771, 2272, 420, 3165, 15995, 412, 257, 565, 11, 1588, 7186, 990, 552, 281, 264, 1036, 992], "temperature": 0.0, "avg_logprob": -0.13280823254826093, "compression_ratio": 1.7393364928909953, "no_speech_prob": 4.3568588807829656e-06}, {"id": 1050, "seek": 697684, "start": 6976.84, "end": 6983.12, "text": " and doing that a few times. That's what a dense block is.", "tokens": [293, 884, 300, 257, 1326, 1413, 13, 663, 311, 437, 257, 18011, 3461, 307, 13], "temperature": 0.0, "avg_logprob": -0.17537876867478894, "compression_ratio": 1.4082840236686391, "no_speech_prob": 6.74798502586782e-06}, {"id": 1051, "seek": 697684, "start": 6983.12, "end": 6991.4800000000005, "text": " So here's something interesting. The original paper for its down sampling, they call it", "tokens": [407, 510, 311, 746, 1880, 13, 440, 3380, 3035, 337, 1080, 760, 21179, 11, 436, 818, 309], "temperature": 0.0, "avg_logprob": -0.17537876867478894, "compression_ratio": 1.4082840236686391, "no_speech_prob": 6.74798502586782e-06}, {"id": 1052, "seek": 697684, "start": 6991.4800000000005, "end": 7003.360000000001, "text": " transition down, did a 1x1 convolution followed by a max-pauling. I actually discovered that", "tokens": [6034, 760, 11, 630, 257, 502, 87, 16, 45216, 6263, 538, 257, 11469, 12, 4306, 425, 278, 13, 286, 767, 6941, 300], "temperature": 0.0, "avg_logprob": -0.17537876867478894, "compression_ratio": 1.4082840236686391, "no_speech_prob": 6.74798502586782e-06}, {"id": 1053, "seek": 700336, "start": 7003.36, "end": 7009.96, "text": " doing a stride-2 convolution gives better results. So you'll see I actually have not", "tokens": [884, 257, 1056, 482, 12, 17, 45216, 2709, 1101, 3542, 13, 407, 291, 603, 536, 286, 767, 362, 406], "temperature": 0.0, "avg_logprob": -0.18016179832252296, "compression_ratio": 1.555, "no_speech_prob": 5.9550816331466194e-06}, {"id": 1054, "seek": 700336, "start": 7009.96, "end": 7015.839999999999, "text": " followed the paper. The one that's commented out here is what the paper did, but actually", "tokens": [6263, 264, 3035, 13, 440, 472, 300, 311, 26940, 484, 510, 307, 437, 264, 3035, 630, 11, 457, 767], "temperature": 0.0, "avg_logprob": -0.18016179832252296, "compression_ratio": 1.555, "no_speech_prob": 5.9550816331466194e-06}, {"id": 1055, "seek": 700336, "start": 7015.839999999999, "end": 7021.0, "text": " this works better. So that was interesting.", "tokens": [341, 1985, 1101, 13, 407, 300, 390, 1880, 13], "temperature": 0.0, "avg_logprob": -0.18016179832252296, "compression_ratio": 1.555, "no_speech_prob": 5.9550816331466194e-06}, {"id": 1056, "seek": 700336, "start": 7021.0, "end": 7028.24, "text": " Interestingly though, on the transition up side, do you remember that checkerboard artifacts", "tokens": [30564, 1673, 11, 322, 264, 6034, 493, 1252, 11, 360, 291, 1604, 300, 1520, 260, 3787, 24617], "temperature": 0.0, "avg_logprob": -0.18016179832252296, "compression_ratio": 1.555, "no_speech_prob": 5.9550816331466194e-06}, {"id": 1057, "seek": 702824, "start": 7028.24, "end": 7033.88, "text": " blog post we saw that showed that upsampling 2D followed by a convolutional layer works", "tokens": [6968, 2183, 321, 1866, 300, 4712, 300, 15497, 335, 11970, 568, 35, 6263, 538, 257, 45216, 304, 4583, 1985], "temperature": 0.0, "avg_logprob": -0.1433756375553632, "compression_ratio": 1.7428571428571429, "no_speech_prob": 1.9333513137098635e-06}, {"id": 1058, "seek": 702824, "start": 7033.88, "end": 7041.04, "text": " better? It does not work better for this. In fact, a deconvolution works better for", "tokens": [1101, 30, 467, 775, 406, 589, 1101, 337, 341, 13, 682, 1186, 11, 257, 979, 266, 85, 3386, 1985, 1101, 337], "temperature": 0.0, "avg_logprob": -0.1433756375553632, "compression_ratio": 1.7428571428571429, "no_speech_prob": 1.9333513137098635e-06}, {"id": 1059, "seek": 702824, "start": 7041.04, "end": 7045.5599999999995, "text": " this. So that's why you can see I've got this deconvolution layer. So I thought that was", "tokens": [341, 13, 407, 300, 311, 983, 291, 393, 536, 286, 600, 658, 341, 979, 266, 85, 3386, 4583, 13, 407, 286, 1194, 300, 390], "temperature": 0.0, "avg_logprob": -0.1433756375553632, "compression_ratio": 1.7428571428571429, "no_speech_prob": 1.9333513137098635e-06}, {"id": 1060, "seek": 702824, "start": 7045.5599999999995, "end": 7049.719999999999, "text": " interesting.", "tokens": [1880, 13], "temperature": 0.0, "avg_logprob": -0.1433756375553632, "compression_ratio": 1.7428571428571429, "no_speech_prob": 1.9333513137098635e-06}, {"id": 1061, "seek": 702824, "start": 7049.719999999999, "end": 7056.0, "text": " So basically you can see when I go down sampling a bunch of times, it's basically do a dense", "tokens": [407, 1936, 291, 393, 536, 562, 286, 352, 760, 21179, 257, 3840, 295, 1413, 11, 309, 311, 1936, 360, 257, 18011], "temperature": 0.0, "avg_logprob": -0.1433756375553632, "compression_ratio": 1.7428571428571429, "no_speech_prob": 1.9333513137098635e-06}, {"id": 1062, "seek": 705600, "start": 7056.0, "end": 7061.96, "text": " block and then I have to keep track of my skip connections. So basically keep a list", "tokens": [3461, 293, 550, 286, 362, 281, 1066, 2837, 295, 452, 10023, 9271, 13, 407, 1936, 1066, 257, 1329], "temperature": 0.0, "avg_logprob": -0.15616789923773872, "compression_ratio": 1.7959183673469388, "no_speech_prob": 1.4970978554629255e-05}, {"id": 1063, "seek": 705600, "start": 7061.96, "end": 7067.32, "text": " of all of those skip connections. So I've got to hang on to all of these. So every one", "tokens": [295, 439, 295, 729, 10023, 9271, 13, 407, 286, 600, 658, 281, 3967, 322, 281, 439, 295, 613, 13, 407, 633, 472], "temperature": 0.0, "avg_logprob": -0.15616789923773872, "compression_ratio": 1.7959183673469388, "no_speech_prob": 1.4970978554629255e-05}, {"id": 1064, "seek": 705600, "start": 7067.32, "end": 7072.56, "text": " of these skip connections, I just stick in this little array, appending them after every", "tokens": [295, 613, 10023, 9271, 11, 286, 445, 2897, 294, 341, 707, 10225, 11, 724, 2029, 552, 934, 633], "temperature": 0.0, "avg_logprob": -0.15616789923773872, "compression_ratio": 1.7959183673469388, "no_speech_prob": 1.4970978554629255e-05}, {"id": 1065, "seek": 705600, "start": 7072.56, "end": 7081.2, "text": " dense block. So then I keep them all and then I pass them to my upward path. So I basically", "tokens": [18011, 3461, 13, 407, 550, 286, 1066, 552, 439, 293, 550, 286, 1320, 552, 281, 452, 23452, 3100, 13, 407, 286, 1936], "temperature": 0.0, "avg_logprob": -0.15616789923773872, "compression_ratio": 1.7959183673469388, "no_speech_prob": 1.4970978554629255e-05}, {"id": 1066, "seek": 708120, "start": 7081.2, "end": 7089.08, "text": " do my transition up and then I concatenate that with that skip connection.", "tokens": [360, 452, 6034, 493, 293, 550, 286, 1588, 7186, 473, 300, 365, 300, 10023, 4984, 13], "temperature": 0.0, "avg_logprob": -0.10244589266569717, "compression_ratio": 1.5089820359281436, "no_speech_prob": 1.7061776134141837e-06}, {"id": 1067, "seek": 708120, "start": 7089.08, "end": 7097.08, "text": " So that's the basic approach. So then the actual tiramisu model itself with those pieces", "tokens": [407, 300, 311, 264, 3875, 3109, 13, 407, 550, 264, 3539, 13807, 335, 25871, 2316, 2564, 365, 729, 3755], "temperature": 0.0, "avg_logprob": -0.10244589266569717, "compression_ratio": 1.5089820359281436, "no_speech_prob": 1.7061776134141837e-06}, {"id": 1068, "seek": 708120, "start": 7097.08, "end": 7104.16, "text": " is less than a screen of code. It's basically just do a 3x3 conv, do my down path, do my", "tokens": [307, 1570, 813, 257, 2568, 295, 3089, 13, 467, 311, 1936, 445, 360, 257, 805, 87, 18, 3754, 11, 360, 452, 760, 3100, 11, 360, 452], "temperature": 0.0, "avg_logprob": -0.10244589266569717, "compression_ratio": 1.5089820359281436, "no_speech_prob": 1.7061776134141837e-06}, {"id": 1069, "seek": 710416, "start": 7104.16, "end": 7119.639999999999, "text": " up path using those skip connections, then a 1x1 conv at the end, and a softmax.", "tokens": [493, 3100, 1228, 729, 10023, 9271, 11, 550, 257, 502, 87, 16, 3754, 412, 264, 917, 11, 293, 257, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.15648460388183594, "compression_ratio": 1.497142857142857, "no_speech_prob": 5.507578407559777e-06}, {"id": 1070, "seek": 710416, "start": 7119.639999999999, "end": 7128.28, "text": " So these dense nets, and indeed this fully convolutional dense net or this tiramisu model,", "tokens": [407, 613, 18011, 36170, 11, 293, 6451, 341, 4498, 45216, 304, 18011, 2533, 420, 341, 13807, 335, 25871, 2316, 11], "temperature": 0.0, "avg_logprob": -0.15648460388183594, "compression_ratio": 1.497142857142857, "no_speech_prob": 5.507578407559777e-06}, {"id": 1071, "seek": 710416, "start": 7128.28, "end": 7131.5199999999995, "text": " they actually take quite a long time to train. They don't have very many parameters, which", "tokens": [436, 767, 747, 1596, 257, 938, 565, 281, 3847, 13, 814, 500, 380, 362, 588, 867, 9834, 11, 597], "temperature": 0.0, "avg_logprob": -0.15648460388183594, "compression_ratio": 1.497142857142857, "no_speech_prob": 5.507578407559777e-06}, {"id": 1072, "seek": 713152, "start": 7131.52, "end": 7135.92, "text": " is why I think they work so well with these tiny datasets, but they do still take a long", "tokens": [307, 983, 286, 519, 436, 589, 370, 731, 365, 613, 5870, 42856, 11, 457, 436, 360, 920, 747, 257, 938], "temperature": 0.0, "avg_logprob": -0.16004636785486243, "compression_ratio": 1.5454545454545454, "no_speech_prob": 5.682398295903113e-06}, {"id": 1073, "seek": 713152, "start": 7135.92, "end": 7142.8, "text": " time to train. Each epoch took a couple of minutes, and in the end I had to do many hundreds", "tokens": [565, 281, 3847, 13, 6947, 30992, 339, 1890, 257, 1916, 295, 2077, 11, 293, 294, 264, 917, 286, 632, 281, 360, 867, 6779], "temperature": 0.0, "avg_logprob": -0.16004636785486243, "compression_ratio": 1.5454545454545454, "no_speech_prob": 5.682398295903113e-06}, {"id": 1074, "seek": 713152, "start": 7142.8, "end": 7150.160000000001, "text": " of epochs. I was also doing a bunch of learning rate annealing. So in the end this really", "tokens": [295, 30992, 28346, 13, 286, 390, 611, 884, 257, 3840, 295, 2539, 3314, 22256, 4270, 13, 407, 294, 264, 917, 341, 534], "temperature": 0.0, "avg_logprob": -0.16004636785486243, "compression_ratio": 1.5454545454545454, "no_speech_prob": 5.682398295903113e-06}, {"id": 1075, "seek": 713152, "start": 7150.160000000001, "end": 7156.200000000001, "text": " had to train overnight, even though I had only about 500-600 frames.", "tokens": [632, 281, 3847, 13935, 11, 754, 1673, 286, 632, 787, 466, 5923, 12, 15707, 12083, 13], "temperature": 0.0, "avg_logprob": -0.16004636785486243, "compression_ratio": 1.5454545454545454, "no_speech_prob": 5.682398295903113e-06}, {"id": 1076, "seek": 715620, "start": 7156.2, "end": 7178.2, "text": " But in the end, I got a really good result. I was a bit nervous at first, I was getting", "tokens": [583, 294, 264, 917, 11, 286, 658, 257, 534, 665, 1874, 13, 286, 390, 257, 857, 6296, 412, 700, 11, 286, 390, 1242], "temperature": 0.0, "avg_logprob": -0.2886078357696533, "compression_ratio": 1.3181818181818181, "no_speech_prob": 1.6700802007107995e-05}, {"id": 1077, "seek": 715620, "start": 7178.2, "end": 7186.12, "text": " this like 87.6% accuracy, but in the paper they were getting 90%+. It turned out to be", "tokens": [341, 411, 27990, 13, 21, 4, 14170, 11, 457, 294, 264, 3035, 436, 645, 1242, 4289, 4, 45585, 467, 3574, 484, 281, 312], "temperature": 0.0, "avg_logprob": -0.2886078357696533, "compression_ratio": 1.3181818181818181, "no_speech_prob": 1.6700802007107995e-05}, {"id": 1078, "seek": 718612, "start": 7186.12, "end": 7191.64, "text": " 3% of the pixels are marked as void. I don't know why they're marked as void, but in the", "tokens": [805, 4, 295, 264, 18668, 366, 12658, 382, 22009, 13, 286, 500, 380, 458, 983, 436, 434, 12658, 382, 22009, 11, 457, 294, 264], "temperature": 0.0, "avg_logprob": -0.14574085936254386, "compression_ratio": 1.5339366515837105, "no_speech_prob": 5.594256435870193e-06}, {"id": 1079, "seek": 718612, "start": 7191.64, "end": 7195.88, "text": " paper they actually remove them. So you'll see when you get to my results section, I've", "tokens": [3035, 436, 767, 4159, 552, 13, 407, 291, 603, 536, 562, 291, 483, 281, 452, 3542, 3541, 11, 286, 600], "temperature": 0.0, "avg_logprob": -0.14574085936254386, "compression_ratio": 1.5339366515837105, "no_speech_prob": 5.594256435870193e-06}, {"id": 1080, "seek": 718612, "start": 7195.88, "end": 7205.84, "text": " got this bit where I remove those void ones, and I ended up with 89.5%.", "tokens": [658, 341, 857, 689, 286, 4159, 729, 22009, 2306, 11, 293, 286, 4590, 493, 365, 31877, 13, 20, 6856], "temperature": 0.0, "avg_logprob": -0.14574085936254386, "compression_ratio": 1.5339366515837105, "no_speech_prob": 5.594256435870193e-06}, {"id": 1081, "seek": 718612, "start": 7205.84, "end": 7215.84, "text": " None of us in class managed to replicate the paper. The paper got 91.5% or 91.2%. We tried", "tokens": [14492, 295, 505, 294, 1508, 6453, 281, 25356, 264, 3035, 13, 440, 3035, 658, 31064, 13, 20, 4, 420, 31064, 13, 17, 6856, 492, 3031], "temperature": 0.0, "avg_logprob": -0.14574085936254386, "compression_ratio": 1.5339366515837105, "no_speech_prob": 5.594256435870193e-06}, {"id": 1082, "seek": 721584, "start": 7215.84, "end": 7223.16, "text": " the lasagna code they provided, we tried Brendan's PyTorch, we tried Mike Keras. Even though", "tokens": [264, 2439, 35697, 3089, 436, 5649, 11, 321, 3031, 48484, 311, 9953, 51, 284, 339, 11, 321, 3031, 6602, 591, 6985, 13, 2754, 1673], "temperature": 0.0, "avg_logprob": -0.2537329258062901, "compression_ratio": 1.4902912621359223, "no_speech_prob": 1.0348496743972646e-06}, {"id": 1083, "seek": 721584, "start": 7223.16, "end": 7228.2, "text": " we couldn't replicate their result, this is still better than any other result I've found.", "tokens": [321, 2809, 380, 25356, 641, 1874, 11, 341, 307, 920, 1101, 813, 604, 661, 1874, 286, 600, 1352, 13], "temperature": 0.0, "avg_logprob": -0.2537329258062901, "compression_ratio": 1.4902912621359223, "no_speech_prob": 1.0348496743972646e-06}, {"id": 1084, "seek": 721584, "start": 7228.2, "end": 7232.400000000001, "text": " So this is still super accurate.", "tokens": [407, 341, 307, 920, 1687, 8559, 13], "temperature": 0.0, "avg_logprob": -0.2537329258062901, "compression_ratio": 1.4902912621359223, "no_speech_prob": 1.0348496743972646e-06}, {"id": 1085, "seek": 721584, "start": 7232.400000000001, "end": 7241.04, "text": " A couple of quick notes about this. First is, they tried training also on something called", "tokens": [316, 1916, 295, 1702, 5570, 466, 341, 13, 2386, 307, 11, 436, 3031, 3097, 611, 322, 746, 1219], "temperature": 0.0, "avg_logprob": -0.2537329258062901, "compression_ratio": 1.4902912621359223, "no_speech_prob": 1.0348496743972646e-06}, {"id": 1086, "seek": 724104, "start": 7241.04, "end": 7247.2, "text": " the GATECH dataset, which is another video dataset. The degree to which this is an amazing", "tokens": [264, 460, 20047, 5462, 28872, 11, 597, 307, 1071, 960, 28872, 13, 440, 4314, 281, 597, 341, 307, 364, 2243], "temperature": 0.0, "avg_logprob": -0.13046779839888864, "compression_ratio": 1.63013698630137, "no_speech_prob": 1.147862622019602e-05}, {"id": 1087, "seek": 724104, "start": 7247.2, "end": 7255.64, "text": " model is really clear here. This 76% is from a model which is specifically built for video,", "tokens": [2316, 307, 534, 1850, 510, 13, 639, 24733, 4, 307, 490, 257, 2316, 597, 307, 4682, 3094, 337, 960, 11], "temperature": 0.0, "avg_logprob": -0.13046779839888864, "compression_ratio": 1.63013698630137, "no_speech_prob": 1.147862622019602e-05}, {"id": 1088, "seek": 724104, "start": 7255.64, "end": 7260.16, "text": " so it actually includes the time component, which is absolutely critical, and it uses", "tokens": [370, 309, 767, 5974, 264, 565, 6542, 11, 597, 307, 3122, 4924, 11, 293, 309, 4960], "temperature": 0.0, "avg_logprob": -0.13046779839888864, "compression_ratio": 1.63013698630137, "no_speech_prob": 1.147862622019602e-05}, {"id": 1089, "seek": 724104, "start": 7260.16, "end": 7264.8, "text": " a pre-trained network, so it's used like a million images to kind of pre-train, and it's", "tokens": [257, 659, 12, 17227, 2001, 3209, 11, 370, 309, 311, 1143, 411, 257, 2459, 5267, 281, 733, 295, 659, 12, 83, 7146, 11, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.13046779839888864, "compression_ratio": 1.63013698630137, "no_speech_prob": 1.147862622019602e-05}, {"id": 1090, "seek": 726480, "start": 7264.8, "end": 7272.64, "text": " still not as good as this model. So that is an extraordinary comparison.", "tokens": [920, 406, 382, 665, 382, 341, 2316, 13, 407, 300, 307, 364, 10581, 9660, 13], "temperature": 0.0, "avg_logprob": -0.18528370327419705, "compression_ratio": 1.5294117647058822, "no_speech_prob": 6.048846898920601e-06}, {"id": 1091, "seek": 726480, "start": 7272.64, "end": 7278.84, "text": " This is the CAMTEC comparison. Here's the model we were just looking at. And again, I actually", "tokens": [639, 307, 264, 27040, 51, 8140, 9660, 13, 1692, 311, 264, 2316, 321, 645, 445, 1237, 412, 13, 400, 797, 11, 286, 767], "temperature": 0.0, "avg_logprob": -0.18528370327419705, "compression_ratio": 1.5294117647058822, "no_speech_prob": 6.048846898920601e-06}, {"id": 1092, "seek": 726480, "start": 7278.84, "end": 7286.88, "text": " looked into this. I thought 91.5%, whereas this one here, 88%, wow, it actually looks", "tokens": [2956, 666, 341, 13, 286, 1194, 31064, 13, 20, 8923, 9735, 341, 472, 510, 11, 24587, 8923, 6076, 11, 309, 767, 1542], "temperature": 0.0, "avg_logprob": -0.18528370327419705, "compression_ratio": 1.5294117647058822, "no_speech_prob": 6.048846898920601e-06}, {"id": 1093, "seek": 726480, "start": 7286.88, "end": 7292.04, "text": " like it's not that much better. I'm really surprised. Even Tree, I really thought it", "tokens": [411, 309, 311, 406, 300, 709, 1101, 13, 286, 478, 534, 6100, 13, 2754, 22291, 11, 286, 534, 1194, 309], "temperature": 0.0, "avg_logprob": -0.18528370327419705, "compression_ratio": 1.5294117647058822, "no_speech_prob": 6.048846898920601e-06}, {"id": 1094, "seek": 729204, "start": 7292.04, "end": 7296.28, "text": " should win easily on Tree, but it doesn't win by very much.", "tokens": [820, 1942, 3612, 322, 22291, 11, 457, 309, 1177, 380, 1942, 538, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.14730617978157254, "compression_ratio": 1.6477732793522266, "no_speech_prob": 2.684185574253206e-06}, {"id": 1095, "seek": 729204, "start": 7296.28, "end": 7300.28, "text": " So I actually went back and looked at this paper, and it turns out that the authors of", "tokens": [407, 286, 767, 1437, 646, 293, 2956, 412, 341, 3035, 11, 293, 309, 4523, 484, 300, 264, 16552, 295], "temperature": 0.0, "avg_logprob": -0.14730617978157254, "compression_ratio": 1.6477732793522266, "no_speech_prob": 2.684185574253206e-06}, {"id": 1096, "seek": 729204, "start": 7300.28, "end": 7307.72, "text": " the DenseNet paper, this is the paper by the way, Modiscale, that they're comparing to.", "tokens": [264, 413, 1288, 31890, 3035, 11, 341, 307, 264, 3035, 538, 264, 636, 11, 6583, 5606, 1220, 11, 300, 436, 434, 15763, 281, 13], "temperature": 0.0, "avg_logprob": -0.14730617978157254, "compression_ratio": 1.6477732793522266, "no_speech_prob": 2.684185574253206e-06}, {"id": 1097, "seek": 729204, "start": 7307.72, "end": 7315.72, "text": " It turned out that they actually trained on crops of 852x852, so they actually used a", "tokens": [467, 3574, 484, 300, 436, 767, 8895, 322, 16829, 295, 14695, 17, 87, 19287, 17, 11, 370, 436, 767, 1143, 257], "temperature": 0.0, "avg_logprob": -0.14730617978157254, "compression_ratio": 1.6477732793522266, "no_speech_prob": 2.684185574253206e-06}, {"id": 1098, "seek": 729204, "start": 7315.72, "end": 7321.04, "text": " way higher resolution image to start with. So you've got to be really careful when you", "tokens": [636, 2946, 8669, 3256, 281, 722, 365, 13, 407, 291, 600, 658, 281, 312, 534, 5026, 562, 291], "temperature": 0.0, "avg_logprob": -0.14730617978157254, "compression_ratio": 1.6477732793522266, "no_speech_prob": 2.684185574253206e-06}, {"id": 1099, "seek": 732104, "start": 7321.04, "end": 7326.8, "text": " read these comparisons. Sometimes people actually shoot themselves in the foot. So these guys", "tokens": [1401, 613, 33157, 13, 4803, 561, 767, 3076, 2969, 294, 264, 2671, 13, 407, 613, 1074], "temperature": 0.0, "avg_logprob": -0.14939934866768972, "compression_ratio": 1.645631067961165, "no_speech_prob": 8.664569577376824e-06}, {"id": 1100, "seek": 732104, "start": 7326.8, "end": 7334.84, "text": " were comparing their result to another model that was using twice as big a picture. So", "tokens": [645, 15763, 641, 1874, 281, 1071, 2316, 300, 390, 1228, 6091, 382, 955, 257, 3036, 13, 407], "temperature": 0.0, "avg_logprob": -0.14939934866768972, "compression_ratio": 1.645631067961165, "no_speech_prob": 8.664569577376824e-06}, {"id": 1101, "seek": 732104, "start": 7334.84, "end": 7339.64, "text": " again, this is actually way better than they actually made it look like.", "tokens": [797, 11, 341, 307, 767, 636, 1101, 813, 436, 767, 1027, 309, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.14939934866768972, "compression_ratio": 1.645631067961165, "no_speech_prob": 8.664569577376824e-06}, {"id": 1102, "seek": 732104, "start": 7339.64, "end": 7344.36, "text": " Another one, this one here, this 88%, also looks impressive. But then I looked across", "tokens": [3996, 472, 11, 341, 472, 510, 11, 341, 24587, 8923, 611, 1542, 8992, 13, 583, 550, 286, 2956, 2108], "temperature": 0.0, "avg_logprob": -0.14939934866768972, "compression_ratio": 1.645631067961165, "no_speech_prob": 8.664569577376824e-06}, {"id": 1103, "seek": 734436, "start": 7344.36, "end": 7353.28, "text": " here and I noticed that the Dilution8 model is way better than this model on every single", "tokens": [510, 293, 286, 5694, 300, 264, 36475, 1448, 23, 2316, 307, 636, 1101, 813, 341, 2316, 322, 633, 2167], "temperature": 0.0, "avg_logprob": -0.18763815272938122, "compression_ratio": 1.6319018404907975, "no_speech_prob": 5.42217730981065e-06}, {"id": 1104, "seek": 734436, "start": 7353.28, "end": 7359.04, "text": " category, way better. And yet somehow the average is only.3 better, and then I realized", "tokens": [7719, 11, 636, 1101, 13, 400, 1939, 6063, 264, 4274, 307, 787, 2411, 18, 1101, 11, 293, 550, 286, 5334], "temperature": 0.0, "avg_logprob": -0.18763815272938122, "compression_ratio": 1.6319018404907975, "no_speech_prob": 5.42217730981065e-06}, {"id": 1105, "seek": 734436, "start": 7359.04, "end": 7368.639999999999, "text": " this actually has to be an error. So this model is actually a lot better than this table", "tokens": [341, 767, 575, 281, 312, 364, 6713, 13, 407, 341, 2316, 307, 767, 257, 688, 1101, 813, 341, 3199], "temperature": 0.0, "avg_logprob": -0.18763815272938122, "compression_ratio": 1.6319018404907975, "no_speech_prob": 5.42217730981065e-06}, {"id": 1106, "seek": 736864, "start": 7368.64, "end": 7376.240000000001, "text": " gives the impression.", "tokens": [2709, 264, 9995, 13], "temperature": 0.0, "avg_logprob": -0.16386575114970303, "compression_ratio": 1.3513513513513513, "no_speech_prob": 6.747994575562188e-06}, {"id": 1107, "seek": 736864, "start": 7376.240000000001, "end": 7382.04, "text": " I briefly mentioned that there's a model which doesn't have any skip connections called Enet,", "tokens": [286, 10515, 2835, 300, 456, 311, 257, 2316, 597, 1177, 380, 362, 604, 10023, 9271, 1219, 2193, 302, 11], "temperature": 0.0, "avg_logprob": -0.16386575114970303, "compression_ratio": 1.3513513513513513, "no_speech_prob": 6.747994575562188e-06}, {"id": 1108, "seek": 736864, "start": 7382.04, "end": 7388.4800000000005, "text": " which is actually better than the Tiramisu on everything except for tree. But on the", "tokens": [597, 307, 767, 1101, 813, 264, 314, 40359, 25871, 322, 1203, 3993, 337, 4230, 13, 583, 322, 264], "temperature": 0.0, "avg_logprob": -0.16386575114970303, "compression_ratio": 1.3513513513513513, "no_speech_prob": 6.747994575562188e-06}, {"id": 1109, "seek": 738848, "start": 7388.48, "end": 7407.639999999999, "text": " tree, it's terrible. It's 77.8 vs. 77.3. I'm sure it was less good than this model, but", "tokens": [4230, 11, 309, 311, 6237, 13, 467, 311, 25546, 13, 23, 12041, 13, 25546, 13, 18, 13, 286, 478, 988, 309, 390, 1570, 665, 813, 341, 2316, 11, 457], "temperature": 0.0, "avg_logprob": -0.27984372959580534, "compression_ratio": 1.1616161616161615, "no_speech_prob": 5.5943064580787905e-06}, {"id": 1110, "seek": 738848, "start": 7407.639999999999, "end": 7412.5199999999995, "text": " now I can't find that data.", "tokens": [586, 286, 393, 380, 915, 300, 1412, 13], "temperature": 0.0, "avg_logprob": -0.27984372959580534, "compression_ratio": 1.1616161616161615, "no_speech_prob": 5.5943064580787905e-06}, {"id": 1111, "seek": 741252, "start": 7412.52, "end": 7422.200000000001, "text": " The reason I wanted to mention this is that Eugenio is about to release a new model which", "tokens": [440, 1778, 286, 1415, 281, 2152, 341, 307, 300, 462, 27915, 1004, 307, 466, 281, 4374, 257, 777, 2316, 597], "temperature": 0.0, "avg_logprob": -0.2219892534716376, "compression_ratio": 1.3801169590643274, "no_speech_prob": 1.6442332707811147e-05}, {"id": 1112, "seek": 741252, "start": 7422.200000000001, "end": 7428.160000000001, "text": " combines these approaches with skip connections. It's called LinkNet. So keep an eye on the", "tokens": [29520, 613, 11587, 365, 10023, 9271, 13, 467, 311, 1219, 8466, 31890, 13, 407, 1066, 364, 3313, 322, 264], "temperature": 0.0, "avg_logprob": -0.2219892534716376, "compression_ratio": 1.3801169590643274, "no_speech_prob": 1.6442332707811147e-05}, {"id": 1113, "seek": 741252, "start": 7428.160000000001, "end": 7431.4800000000005, "text": " forum because I'll be looking into that quite shortly.", "tokens": [17542, 570, 286, 603, 312, 1237, 666, 300, 1596, 13392, 13], "temperature": 0.0, "avg_logprob": -0.2219892534716376, "compression_ratio": 1.3801169590643274, "no_speech_prob": 1.6442332707811147e-05}, {"id": 1114, "seek": 743148, "start": 7431.48, "end": 7456.759999999999, "text": " A lot of you have come up to me and been like, We're finishing, what do we do now? And the", "tokens": [316, 688, 295, 291, 362, 808, 493, 281, 385, 293, 668, 411, 11, 492, 434, 12693, 11, 437, 360, 321, 360, 586, 30, 400, 264], "temperature": 0.0, "avg_logprob": -0.23268254049893083, "compression_ratio": 1.0588235294117647, "no_speech_prob": 4.757471469929442e-05}, {"id": 1115, "seek": 745676, "start": 7456.76, "end": 7465.0, "text": " answer is, we have now created a community of all these people who have spent well over", "tokens": [1867, 307, 11, 321, 362, 586, 2942, 257, 1768, 295, 439, 613, 561, 567, 362, 4418, 731, 670], "temperature": 0.0, "avg_logprob": -0.18480820953845978, "compression_ratio": 1.5375722543352601, "no_speech_prob": 2.506964847270865e-05}, {"id": 1116, "seek": 745676, "start": 7465.0, "end": 7471.64, "text": " 100 hours working on deep learning for many, many months, and have built their own boxes,", "tokens": [2319, 2496, 1364, 322, 2452, 2539, 337, 867, 11, 867, 2493, 11, 293, 362, 3094, 641, 1065, 9002, 11], "temperature": 0.0, "avg_logprob": -0.18480820953845978, "compression_ratio": 1.5375722543352601, "no_speech_prob": 2.506964847270865e-05}, {"id": 1117, "seek": 745676, "start": 7471.64, "end": 7483.96, "text": " and written blog posts, and done all kinds of stuff, set up social impact talks, written", "tokens": [293, 3720, 6968, 12300, 11, 293, 1096, 439, 3685, 295, 1507, 11, 992, 493, 2093, 2712, 6686, 11, 3720], "temperature": 0.0, "avg_logprob": -0.18480820953845978, "compression_ratio": 1.5375722543352601, "no_speech_prob": 2.506964847270865e-05}, {"id": 1118, "seek": 748396, "start": 7483.96, "end": 7491.52, "text": " articles in Forbes. This community is happening. So it doesn't make any sense in my opinion", "tokens": [11290, 294, 45950, 13, 639, 1768, 307, 2737, 13, 407, 309, 1177, 380, 652, 604, 2020, 294, 452, 4800], "temperature": 0.0, "avg_logprob": -0.2390362156762017, "compression_ratio": 1.4615384615384615, "no_speech_prob": 2.1111320165800862e-05}, {"id": 1119, "seek": 748396, "start": 7491.52, "end": 7498.76, "text": " for Rachel and I to now be saying, Here's what happens next. So just like Elena has", "tokens": [337, 14246, 293, 286, 281, 586, 312, 1566, 11, 1692, 311, 437, 2314, 958, 13, 407, 445, 411, 39603, 575], "temperature": 0.0, "avg_logprob": -0.2390362156762017, "compression_ratio": 1.4615384615384615, "no_speech_prob": 2.1111320165800862e-05}, {"id": 1120, "seek": 748396, "start": 7498.76, "end": 7508.76, "text": " decided, I want a book club. So she talked to Mindy and we now have a book club. So what's", "tokens": [3047, 11, 286, 528, 257, 1446, 6482, 13, 407, 750, 2825, 281, 13719, 88, 293, 321, 586, 362, 257, 1446, 6482, 13, 407, 437, 311], "temperature": 0.0, "avg_logprob": -0.2390362156762017, "compression_ratio": 1.4615384615384615, "no_speech_prob": 2.1111320165800862e-05}, {"id": 1121, "seek": 750876, "start": 7508.76, "end": 7521.04, "text": " next? Well, the forums will continue forever. We all know each other. Let's do good shit.", "tokens": [958, 30, 1042, 11, 264, 26998, 486, 2354, 5680, 13, 492, 439, 458, 1184, 661, 13, 961, 311, 360, 665, 4611, 13], "temperature": 0.0, "avg_logprob": -0.16396801206800674, "compression_ratio": 1.288888888888889, "no_speech_prob": 2.3920676540001296e-05}, {"id": 1122, "seek": 750876, "start": 7521.04, "end": 7533.18, "text": " Most importantly, write code. Please write code. Build apps, take your work projects", "tokens": [4534, 8906, 11, 2464, 3089, 13, 2555, 2464, 3089, 13, 11875, 7733, 11, 747, 428, 589, 4455], "temperature": 0.0, "avg_logprob": -0.16396801206800674, "compression_ratio": 1.288888888888889, "no_speech_prob": 2.3920676540001296e-05}, {"id": 1123, "seek": 753318, "start": 7533.18, "end": 7539.04, "text": " and try doing them with deep learning. Build libraries to make things easier. Maybe go", "tokens": [293, 853, 884, 552, 365, 2452, 2539, 13, 11875, 15148, 281, 652, 721, 3571, 13, 2704, 352], "temperature": 0.0, "avg_logprob": -0.19012009953878012, "compression_ratio": 1.5021645021645023, "no_speech_prob": 2.282622517668642e-05}, {"id": 1124, "seek": 753318, "start": 7539.04, "end": 7543.64, "text": " back to stuff from part 1 of the course and look back and think, Why didn't we do it this", "tokens": [646, 281, 1507, 490, 644, 502, 295, 264, 1164, 293, 574, 646, 293, 519, 11, 1545, 994, 380, 321, 360, 309, 341], "temperature": 0.0, "avg_logprob": -0.19012009953878012, "compression_ratio": 1.5021645021645023, "no_speech_prob": 2.282622517668642e-05}, {"id": 1125, "seek": 753318, "start": 7543.64, "end": 7551.34, "text": " other way? Maybe I could make it simpler. Write papers. So I showed you that amazing", "tokens": [661, 636, 30, 2704, 286, 727, 652, 309, 18587, 13, 23499, 10577, 13, 407, 286, 4712, 291, 300, 2243], "temperature": 0.0, "avg_logprob": -0.19012009953878012, "compression_ratio": 1.5021645021645023, "no_speech_prob": 2.282622517668642e-05}, {"id": 1126, "seek": 753318, "start": 7551.34, "end": 7556.360000000001, "text": " result of the new style transfer approach from Vincent last week. Hopefully that will", "tokens": [1874, 295, 264, 777, 3758, 5003, 3109, 490, 28003, 1036, 1243, 13, 10429, 300, 486], "temperature": 0.0, "avg_logprob": -0.19012009953878012, "compression_ratio": 1.5021645021645023, "no_speech_prob": 2.282622517668642e-05}, {"id": 1127, "seek": 755636, "start": 7556.36, "end": 7565.4, "text": " get my tape into a paper. Write blog posts. In a few weeks' time, all the MOOC guys are", "tokens": [483, 452, 7314, 666, 257, 3035, 13, 23499, 6968, 12300, 13, 682, 257, 1326, 3259, 6, 565, 11, 439, 264, 49197, 34, 1074, 366], "temperature": 0.0, "avg_logprob": -0.16875921168797453, "compression_ratio": 1.4426229508196722, "no_speech_prob": 4.157293005846441e-06}, {"id": 1128, "seek": 755636, "start": 7565.4, "end": 7569.5199999999995, "text": " going to be coming through and doing part 2 of the course. So help them out on the forum.", "tokens": [516, 281, 312, 1348, 807, 293, 884, 644, 568, 295, 264, 1164, 13, 407, 854, 552, 484, 322, 264, 17542, 13], "temperature": 0.0, "avg_logprob": -0.16875921168797453, "compression_ratio": 1.4426229508196722, "no_speech_prob": 4.157293005846441e-06}, {"id": 1129, "seek": 755636, "start": 7569.5199999999995, "end": 7579.48, "text": " Teaching is the best way to learn yourself. I really want to hear the success stories.", "tokens": [34244, 307, 264, 1151, 636, 281, 1466, 1803, 13, 286, 534, 528, 281, 1568, 264, 2245, 3676, 13], "temperature": 0.0, "avg_logprob": -0.16875921168797453, "compression_ratio": 1.4426229508196722, "no_speech_prob": 4.157293005846441e-06}, {"id": 1130, "seek": 757948, "start": 7579.48, "end": 7587.5, "text": " People don't believe that what you've done is possible. I know that because as recently", "tokens": [3432, 500, 380, 1697, 300, 437, 291, 600, 1096, 307, 1944, 13, 286, 458, 300, 570, 382, 3938], "temperature": 0.0, "avg_logprob": -0.15123646599905832, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.0001334027328994125}, {"id": 1131, "seek": 757948, "start": 7587.5, "end": 7593.2, "text": " as yesterday, there was the highest ranked Hacker News comment on a story about deep", "tokens": [382, 5186, 11, 456, 390, 264, 6343, 20197, 389, 23599, 7987, 2871, 322, 257, 1657, 466, 2452], "temperature": 0.0, "avg_logprob": -0.15123646599905832, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.0001334027328994125}, {"id": 1132, "seek": 757948, "start": 7593.2, "end": 7598.32, "text": " learning. It's pointless trying to do deep learning unless you have years of mathematical", "tokens": [2539, 13, 467, 311, 32824, 1382, 281, 360, 2452, 2539, 5969, 291, 362, 924, 295, 18894], "temperature": 0.0, "avg_logprob": -0.15123646599905832, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.0001334027328994125}, {"id": 1133, "seek": 757948, "start": 7598.32, "end": 7603.799999999999, "text": " background and you know C++ and you're an expert in machine learning techniques across", "tokens": [3678, 293, 291, 458, 383, 25472, 293, 291, 434, 364, 5844, 294, 3479, 2539, 7512, 2108], "temperature": 0.0, "avg_logprob": -0.15123646599905832, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.0001334027328994125}, {"id": 1134, "seek": 757948, "start": 7603.799999999999, "end": 7607.599999999999, "text": " the board. Otherwise there's no way you're going to be able to do anything useful in", "tokens": [264, 3150, 13, 10328, 456, 311, 572, 636, 291, 434, 516, 281, 312, 1075, 281, 360, 1340, 4420, 294], "temperature": 0.0, "avg_logprob": -0.15123646599905832, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.0001334027328994125}, {"id": 1135, "seek": 760760, "start": 7607.6, "end": 7615.120000000001, "text": " the real world project. That today is what everybody believes. We now know that's not", "tokens": [264, 957, 1002, 1716, 13, 663, 965, 307, 437, 2201, 12307, 13, 492, 586, 458, 300, 311, 406], "temperature": 0.0, "avg_logprob": -0.16795919133328843, "compression_ratio": 1.5675675675675675, "no_speech_prob": 3.72658087144373e-06}, {"id": 1136, "seek": 760760, "start": 7615.120000000001, "end": 7626.4400000000005, "text": " true. So Rachel and I are going to start up a podcast where we're going to try to help", "tokens": [2074, 13, 407, 14246, 293, 286, 366, 516, 281, 722, 493, 257, 7367, 689, 321, 434, 516, 281, 853, 281, 854], "temperature": 0.0, "avg_logprob": -0.16795919133328843, "compression_ratio": 1.5675675675675675, "no_speech_prob": 3.72658087144373e-06}, {"id": 1137, "seek": 760760, "start": 7626.4400000000005, "end": 7632.240000000001, "text": " deep learning learners. But one of the key things we want to do is tell your stories.", "tokens": [2452, 2539, 23655, 13, 583, 472, 295, 264, 2141, 721, 321, 528, 281, 360, 307, 980, 428, 3676, 13], "temperature": 0.0, "avg_logprob": -0.16795919133328843, "compression_ratio": 1.5675675675675675, "no_speech_prob": 3.72658087144373e-06}, {"id": 1138, "seek": 760760, "start": 7632.240000000001, "end": 7637.4400000000005, "text": " So if you've done something interesting at work, or you've got an interesting new result,", "tokens": [407, 498, 291, 600, 1096, 746, 1880, 412, 589, 11, 420, 291, 600, 658, 364, 1880, 777, 1874, 11], "temperature": 0.0, "avg_logprob": -0.16795919133328843, "compression_ratio": 1.5675675675675675, "no_speech_prob": 3.72658087144373e-06}, {"id": 1139, "seek": 763744, "start": 7637.44, "end": 7642.799999999999, "text": " or you're just in the middle of a project that's kind of fun, please tell us. Either", "tokens": [420, 291, 434, 445, 294, 264, 2808, 295, 257, 1716, 300, 311, 733, 295, 1019, 11, 1767, 980, 505, 13, 13746], "temperature": 0.0, "avg_logprob": -0.18571764489878778, "compression_ratio": 1.5890410958904109, "no_speech_prob": 2.2125099349068478e-05}, {"id": 1140, "seek": 763744, "start": 7642.799999999999, "end": 7648.679999999999, "text": " on the forum or private message or whatever. Please tell us, because we really want to", "tokens": [322, 264, 17542, 420, 4551, 3636, 420, 2035, 13, 2555, 980, 505, 11, 570, 321, 534, 528, 281], "temperature": 0.0, "avg_logprob": -0.18571764489878778, "compression_ratio": 1.5890410958904109, "no_speech_prob": 2.2125099349068478e-05}, {"id": 1141, "seek": 763744, "start": 7648.679999999999, "end": 7656.12, "text": " share your story. And if it's not a story yet, tell us enough that we can help you,", "tokens": [2073, 428, 1657, 13, 400, 498, 309, 311, 406, 257, 1657, 1939, 11, 980, 505, 1547, 300, 321, 393, 854, 291, 11], "temperature": 0.0, "avg_logprob": -0.18571764489878778, "compression_ratio": 1.5890410958904109, "no_speech_prob": 2.2125099349068478e-05}, {"id": 1142, "seek": 763744, "start": 7656.12, "end": 7665.5599999999995, "text": " and that the community can help you. Get together. The book club, if you're watching this on", "tokens": [293, 300, 264, 1768, 393, 854, 291, 13, 3240, 1214, 13, 440, 1446, 6482, 11, 498, 291, 434, 1976, 341, 322], "temperature": 0.0, "avg_logprob": -0.18571764489878778, "compression_ratio": 1.5890410958904109, "no_speech_prob": 2.2125099349068478e-05}, {"id": 1143, "seek": 766556, "start": 7665.56, "end": 7674.160000000001, "text": " the MOOC, organize other people in your geography to get together and meet up at your workplace.", "tokens": [264, 49197, 34, 11, 13859, 661, 561, 294, 428, 26695, 281, 483, 1214, 293, 1677, 493, 412, 428, 15328, 13], "temperature": 0.0, "avg_logprob": -0.21805258230729538, "compression_ratio": 1.5344827586206897, "no_speech_prob": 3.4266147849848494e-05}, {"id": 1144, "seek": 766556, "start": 7674.160000000001, "end": 7679.320000000001, "text": " In this group here, I know we've got people from Apple and Uber and Airbnb who started", "tokens": [682, 341, 1594, 510, 11, 286, 458, 321, 600, 658, 561, 490, 6373, 293, 21839, 293, 38232, 567, 1409], "temperature": 0.0, "avg_logprob": -0.21805258230729538, "compression_ratio": 1.5344827586206897, "no_speech_prob": 3.4266147849848494e-05}, {"id": 1145, "seek": 766556, "start": 7679.320000000001, "end": 7686.64, "text": " doing this in lunchtime MOOC chats, and now they're here at this course. Yes Rachel?", "tokens": [884, 341, 294, 6349, 3766, 49197, 34, 38057, 11, 293, 586, 436, 434, 510, 412, 341, 1164, 13, 1079, 14246, 30], "temperature": 0.0, "avg_logprob": -0.21805258230729538, "compression_ratio": 1.5344827586206897, "no_speech_prob": 3.4266147849848494e-05}, {"id": 1146, "seek": 766556, "start": 7686.64, "end": 7692.240000000001, "text": " I also wanted to recommend it would be great to start meetups to help lead other people", "tokens": [286, 611, 1415, 281, 2748, 309, 576, 312, 869, 281, 722, 1677, 7528, 281, 854, 1477, 661, 561], "temperature": 0.0, "avg_logprob": -0.21805258230729538, "compression_ratio": 1.5344827586206897, "no_speech_prob": 3.4266147849848494e-05}, {"id": 1147, "seek": 769224, "start": 7692.24, "end": 7698.84, "text": " through part 1 of the course, assist them going through it.", "tokens": [807, 644, 502, 295, 264, 1164, 11, 4255, 552, 516, 807, 309, 13], "temperature": 0.0, "avg_logprob": -0.19025551478068034, "compression_ratio": 1.4352941176470588, "no_speech_prob": 2.5865416318993084e-05}, {"id": 1148, "seek": 769224, "start": 7698.84, "end": 7710.16, "text": " So Rachel and I really just want to spend the next 6-12 months focused on supporting", "tokens": [407, 14246, 293, 286, 534, 445, 528, 281, 3496, 264, 958, 1386, 12, 4762, 2493, 5178, 322, 7231], "temperature": 0.0, "avg_logprob": -0.19025551478068034, "compression_ratio": 1.4352941176470588, "no_speech_prob": 2.5865416318993084e-05}, {"id": 1149, "seek": 769224, "start": 7710.16, "end": 7721.24, "text": " your projects. So I'm very interested in working on this lung cancer stuff, but I'm also interested", "tokens": [428, 4455, 13, 407, 286, 478, 588, 3102, 294, 1364, 322, 341, 16730, 5592, 1507, 11, 457, 286, 478, 611, 3102], "temperature": 0.0, "avg_logprob": -0.19025551478068034, "compression_ratio": 1.4352941176470588, "no_speech_prob": 2.5865416318993084e-05}, {"id": 1150, "seek": 772124, "start": 7721.24, "end": 7726.24, "text": " in every project that you guys are working on, I want to help with that. I also want", "tokens": [294, 633, 1716, 300, 291, 1074, 366, 1364, 322, 11, 286, 528, 281, 854, 365, 300, 13, 286, 611, 528], "temperature": 0.0, "avg_logprob": -0.20559182568131207, "compression_ratio": 1.7622950819672132, "no_speech_prob": 6.920976738911122e-05}, {"id": 1151, "seek": 772124, "start": 7726.24, "end": 7732.92, "text": " to help people who want to teach this. So Yannett is going to go from being a student", "tokens": [281, 854, 561, 567, 528, 281, 2924, 341, 13, 407, 398, 969, 3093, 307, 516, 281, 352, 490, 885, 257, 3107], "temperature": 0.0, "avg_logprob": -0.20559182568131207, "compression_ratio": 1.7622950819672132, "no_speech_prob": 6.920976738911122e-05}, {"id": 1152, "seek": 772124, "start": 7732.92, "end": 7738.0, "text": " to teacher hopefully soon, and will be teaching USF students about deep learning, and hopefully", "tokens": [281, 5027, 4696, 2321, 11, 293, 486, 312, 4571, 2546, 37, 1731, 466, 2452, 2539, 11, 293, 4696], "temperature": 0.0, "avg_logprob": -0.20559182568131207, "compression_ratio": 1.7622950819672132, "no_speech_prob": 6.920976738911122e-05}, {"id": 1153, "seek": 772124, "start": 7738.0, "end": 7742.36, "text": " the next batch of people about deep learning. Anybody who's interested in teaching, let", "tokens": [264, 958, 15245, 295, 561, 466, 2452, 2539, 13, 19082, 567, 311, 3102, 294, 4571, 11, 718], "temperature": 0.0, "avg_logprob": -0.20559182568131207, "compression_ratio": 1.7622950819672132, "no_speech_prob": 6.920976738911122e-05}, {"id": 1154, "seek": 772124, "start": 7742.36, "end": 7750.84, "text": " us know. This is the best high-leverage activity, is to teach the teachers.", "tokens": [505, 458, 13, 639, 307, 264, 1151, 1090, 12, 306, 3623, 5191, 11, 307, 281, 2924, 264, 6023, 13], "temperature": 0.0, "avg_logprob": -0.20559182568131207, "compression_ratio": 1.7622950819672132, "no_speech_prob": 6.920976738911122e-05}, {"id": 1155, "seek": 775084, "start": 7750.84, "end": 7757.32, "text": " So I don't know where this is going to end up, but my hope is really that, basically", "tokens": [407, 286, 500, 380, 458, 689, 341, 307, 516, 281, 917, 493, 11, 457, 452, 1454, 307, 534, 300, 11, 1936], "temperature": 0.0, "avg_logprob": -0.16245256311753217, "compression_ratio": 1.6161137440758293, "no_speech_prob": 4.7573503252351657e-05}, {"id": 1156, "seek": 775084, "start": 7757.32, "end": 7763.32, "text": " I would say the experiment has worked. You guys are all here, you're reading papers,", "tokens": [286, 576, 584, 264, 5120, 575, 2732, 13, 509, 1074, 366, 439, 510, 11, 291, 434, 3760, 10577, 11], "temperature": 0.0, "avg_logprob": -0.16245256311753217, "compression_ratio": 1.6161137440758293, "no_speech_prob": 4.7573503252351657e-05}, {"id": 1157, "seek": 775084, "start": 7763.32, "end": 7769.02, "text": " you're writing code, you're understanding the most cutting-edge research-level deep", "tokens": [291, 434, 3579, 3089, 11, 291, 434, 3701, 264, 881, 6492, 12, 12203, 2132, 12, 12418, 2452], "temperature": 0.0, "avg_logprob": -0.16245256311753217, "compression_ratio": 1.6161137440758293, "no_speech_prob": 4.7573503252351657e-05}, {"id": 1158, "seek": 775084, "start": 7769.02, "end": 7774.84, "text": " learning that exists today. We've gone beyond some of the cutting-edge research in many", "tokens": [2539, 300, 8198, 965, 13, 492, 600, 2780, 4399, 512, 295, 264, 6492, 12, 12203, 2132, 294, 867], "temperature": 0.0, "avg_logprob": -0.16245256311753217, "compression_ratio": 1.6161137440758293, "no_speech_prob": 4.7573503252351657e-05}, {"id": 1159, "seek": 777484, "start": 7774.84, "end": 7782.400000000001, "text": " situations, some of you have gone beyond the cutting-edge research. So let's build from", "tokens": [6851, 11, 512, 295, 291, 362, 2780, 4399, 264, 6492, 12, 12203, 2132, 13, 407, 718, 311, 1322, 490], "temperature": 0.0, "avg_logprob": -0.17610701199235587, "compression_ratio": 1.608294930875576, "no_speech_prob": 7.410816579067614e-06}, {"id": 1160, "seek": 777484, "start": 7782.400000000001, "end": 7788.72, "text": " here as a community, and anything that Rachel and I can do to help, please tell us, because", "tokens": [510, 382, 257, 1768, 11, 293, 1340, 300, 14246, 293, 286, 393, 360, 281, 854, 11, 1767, 980, 505, 11, 570], "temperature": 0.0, "avg_logprob": -0.17610701199235587, "compression_ratio": 1.608294930875576, "no_speech_prob": 7.410816579067614e-06}, {"id": 1161, "seek": 777484, "start": 7788.72, "end": 7795.360000000001, "text": " we just want you to be successful, we want the community to be successful.", "tokens": [321, 445, 528, 291, 281, 312, 4406, 11, 321, 528, 264, 1768, 281, 312, 4406, 13], "temperature": 0.0, "avg_logprob": -0.17610701199235587, "compression_ratio": 1.608294930875576, "no_speech_prob": 7.410816579067614e-06}, {"id": 1162, "seek": 779536, "start": 7795.36, "end": 7805.639999999999, "text": " So will you be still active in the forums? My job is to make you guys successful.", "tokens": [407, 486, 291, 312, 920, 4967, 294, 264, 26998, 30, 1222, 1691, 307, 281, 652, 291, 1074, 4406, 13], "temperature": 0.0, "avg_logprob": -0.21424920558929444, "compression_ratio": 1.2857142857142858, "no_speech_prob": 0.00021975890558678657}, {"id": 1163, "seek": 780564, "start": 7805.64, "end": 7825.96, "text": " So thank you all so much for coming, and congratulations to all of you.", "tokens": [50364, 407, 1309, 291, 439, 370, 709, 337, 1348, 11, 293, 13568, 281, 439, 295, 291, 13, 51380], "temperature": 0.0, "avg_logprob": -0.3215107666818719, "compression_ratio": 0.9861111111111112, "no_speech_prob": 0.0004107078129891306}], "language": "en"}