{"text": " Welcome back lesson 6, so this is our penultimate lesson Believe it or not Couple of weeks ago in lesson 4 I mentioned I was going to share that lesson with this terrific NLP researcher Sebastian Ruda Which I did and he he said he loved it and he's gone on to Yesterday released this new post he called optimization for deep learning highlights in 2017 in which he covered Basically everything that we talked about in that lesson And with some very nice shout outs to some of the work that some of the students here have done including when he talked about this separation of Of the Separation of weight decay from the momentum term and so he actually mentions here the opportunities in terms of improved kind of software decoupling this allows and actually links to The commit from an answer ha actually showing how to implement this in fast AI so fast AI's Code is actually being used as a bit of a role model now He then covers some of these learning rate training techniques that we've talked about and This is the SGDR schedule it looks a bit different to what you're used to seeing this is on a log curve This is the way that they show it on the paper and For more information again links to two blog posts the one from a Vitali about this topic and And again an answer ha his Blog post on this topic, so it's great to see that some of the work from fast AI students is already Getting noticed and picked up and shared and this blog post went on to get on the front page of Hacker News So That's pretty cool, and hopefully more and more of this work will be picked up once this is released publicly So last week we were kind of doing a deep dive into collaborative filtering and Let's remind ourselves of kind of what our final model looked like So in the end we kind of ended up rebuilding the Model that's actually in the fast AI library Where we had An embedding so we had this little get embedding function that grabbed an embedding and randomly initialized the weights for the users And for the items that's the kind of generic term in that case the items are movies and the bias for the users the bias for the items And we had n factors Embedding size for each for each one of course the biases just had a single one And then we grabbed the users and item embeddings multiply them together Summed it up for each row and added on the bias terms Popped that through a sigmoid to put it into the range that we wanted so that was our model and One of you asked if we can kind of interpret this information in some way And I promise this week we would see how to do that so let's take a look so we're going to start with the Model we built here where we just used that fast AI library Collab for all that data set from CSV and then that dot get learner, and then we fitted it in three epochs 19 seconds We've got a pretty good result So what we can now do is to analyze that model So You may remember right back when we started we read in the movies dot CSV file But that's just a mapping from the ID of the movie to the name of the movie And so we're just going to use that for display purposes so we can see what we're doing Because not all of us have watched every movie. I'm just going to limit this to the top 500 Most popular sorry 3,000 most popular movies, so we might have more chance of recognizing the movies. We're looking at and then I'll go ahead and Change it from the movie IDs from movie lens to those unique IDs that we're using the contiguous IDs Because that's what our model has all right so inside the Learn object that we create inside a learner We can always grab the pie torch model itself just by saying learn dot model Right and like I'm going to kind of show you more and more of the code At the moment, so let's take a look at the definition of model And so model is a property So if you haven't seen a property before a property is just something in Python which looks like a method When you define it that you can call it without Parentheses as we do here right and so it kind of looks when you call it like it's a regular attribute But it looks like when you define it like it's a method so every time you call it it actually runs this code Okay, and so in this case. It's just a shortcut to grab something called dot models dot model So you may be interested to know what that looks like learn dot models and so this is there's a the fast AI Model type is a very thin wrapper for pie torch models, so we could take a look at this Collab filter model and see what that is It's Only one line of code okay, and Yeah, we'll talk more about these in part two right but basically that this is this very thin wrapper and the main thing one of The main things that fast AI does is we have this concept of layer groups where basically when you say here There are different learning rates, and they get applied to different sets of layers, and that's something that's not in pie torch So when you say I want to use this pie torch model This was one thing we have to do which is to say like I came in around layer groups Okay, so the details aren't terribly important But in general if you want to create a little wrapper for some other pie torch model you could just write something like this So to get to get inside that to grab the actual pie torch model itself its models dot Model that's the pie torch model and then the learn object has a shortcut to that okay? So we're going to set M to be the pie torch model and so when you Print out a pie torch model it prints it out Basically by listing out all of the layers that you created in the constructor It's quite. It's quite nifty actually when you kind of think about the way this works. Thanks to kind of some very Handy stuff in python we're actually able to use standard python. Oh to kind of define These modules and these layers and they basically automatically kind of register themselves with pie torch so back in our embedding dot bias We just had a bunch of things where we said okay each of these things are equal to these things And then it automatically knows how to represent that So you can see there's the name is you and so the name is just literally whatever we called it Yeah, you right And then the definition is it's this kind of layer, okay? So that's our pie torch model so we can Look inside that basically use that so if we say m dot ib then that's referring to the m dot ib embedding layer For an item which is the bias layer so an item bias in this case is the movie bias So each movie there are 9,000 of them has a single bias element Now the really nice thing about pie torch layers and Models is that they all look the same they basically cut to use them you call them as if they were a function So we can go m dot ib parentheses Right and that basically says I want you to return The value of that layer and that layer could be a full-on model right so to actually Get a prediction from a pie torch model. You just I would go M and pass in my variable Okay, and so in this case m dot ib and pass in my top movie indexes now models remember layers They require variables Not tensors because it needs to keep track of the derivatives Okay, and so we use this capital V to turn the tensor into a variable And was just announced this week that pie torch 0.4 Which is the version after the one that's just about to be released is going to get rid of variables And we'll actually be able to use tensors directly to keep track of derivatives So if you're watching this on the MOOC and you're looking at point four then you'll probably notice that the code doesn't have this V in it anymore So that would be pretty exciting when that happens but for now we have to remember if we're going to pass something into a model to turn it into a variable first and Remember a variable has a strict superset of the API of a tensor so anything you can do to a tensor They can do to a variable like add it up or take its log or whatever okay? So that's going to return a variable which consists of going through each of these movie IDs Putting it through this embedding layer to get its bias Okay, and that's going to return a Variable Let's take a look So before I press Shift enter here. You can have a think about what I'm going to have I've got a list of 3,000 movies going in Turning into a variable putting it through this embedding layer So just have a think about what you expect to come out Okay, and we have a variable of size 3,000 by 1 hopefully that doesn't surprise you we had 3,000 movies that we're looking up each one hadn't had a one long embedding okay, so there's our 3,000 long you'll notice It's a variable which is not surprising because we fed it a variable so we got a variable back And it's a variable that's on the GPU right dot Cuda okay, so We have a little shortcut in fast AI because we we very often want to take variables Turn them into tensors and move them back to the CPU so we can play with them more easily So 2 NP is is to numpy okay, and that does all of those things It works regardless of whether it's a tensor or a variable it works regardless of whether it's on the CPU or GPU It'll end up giving you a a numpy Array from that okay, so if we do that That gives us exactly the same thing as we just looked at but now in numpy form Okay, so that's a super handy thing to use when you're playing around with pipe watch my approach to things is I Try to use numpy for everything Except when I explicitly you need something to run on the GPU or I need its derivatives Right in which case I use pytorch because like numpy like I kind of find NumPy is often easier to work with it's been around many years longer than pytorch So you know and and lots of things like like the Python imaging library and open CV and lots and lots of stuff like Pandas it works with numpy so my approach is kind of like do as much as I can in numpy land Finally when I'm ready to do something on the GPU or take its derivative To pytorch and then as soon as I can I put it back in numpy and you'll see that the fast AI library Really works this way like all the transformations and stuff happen in numpy, which is different to most high torch Computer vision libraries which tend to do it all as much as possible in pytorch. I tried to do as much as possible in numpy So let's say we wanted to transfer build a model in the GPU with the GPU and train it and then we want to Bring this to production so would we call to numpy on the model itself? Or would we have to iterate through all the different layers and then call to NP? Yeah good question So it's very likely that you want to do inference on a CPU rather than a GPU. It's it's more scalable You don't have to worry about putting things in batches. You know so on and so forth So you can move a model? Onto the CPU just by typing m dot CPU And that model is now on the CPU and so therefore you can also then put your variable on the CPU By doing exactly the same thing so you can say Like so now having said that if your if your server doesn't have a GPU or CUDA GPU You don't have to do this because it won't put it on the GPU at all So if you're if if for inferencing on the server if you're running it on you know some T2 instance or something it'll work fine. It'll all run on the on the CPU automatically Quick follow-up, and if we train the model on the GPU, and then we save those embeddings and the weights Would we have to do anything special to load it on to you know you won't? We have something well it kind of depends how much of fast AI you're using So I'll show you how you can do that in case you have to do it manually One of the students figured this out, which is very handy when we There's a load model function And you'll see what it does, but it does torch dot load is it basically this is like some magic Incantation that like normally it has to load it onto the same GPU as saved on But this will like load it into whatever's whatever's available, so That was a handy discovery Thanks for the great questions And so to put that back on the GPU I'll need to say dot CUDA and Now there we go I can run it again, okay So it's really important to know about the zip function in Python which iterates through a number of lists At the same time so in this case. I want to grab each movie Along with its bias term so that I can just pop it into a list of tuples So if I just go zip like that that's going to iterate through each movie ID and each bias term And so then I can use that in a list comprehension to grab the name of each movie along with its bias Okay, so having done that I can then Sort and so here. I told you that John Travolta Scientology movie at the most negative of net quite by a lot if this is a Kaggle competition Battlefield Earth would have like won by miles look at this seven seven seven ninety six So here is the worst movie of all time according to IMDB and like it's interesting when you think about what this means right because this is like a much more authentic way to find out how bad this movie is because Like some people are just more negative about movies right and like if more of them watch your movie like you know Highly critical audience they're going to rate it badly, so if you take an average. It's not quite fair right and So what this is you know what this is doing is saying once we you know remove the fact that different people Have different overall positive or negative experiences and different people watch different kinds of movies, and we correct for all that And this is the worst movie of all time So that's a good thing to know So this is how we can yeah look inside our our model and and and interpret the bias vectors You'll see here. I've sorted by The zeroth element of each tuple by using a lambda Originally I used this special Item getter this is part of Python's operator library And this creates a function that returns the zeroth element of something In order to save time and then I actually realized that the lambda is only one more character To write than the item getter, so maybe we don't need to know this after all so Yeah, really useful to make sure you know how to write lambdas in python So this is this is a function okay? And so the sort is going to call this function every time it decides like is this thing higher or lower than that other thing and This is going to return the zeroth element, okay? so here's the same thing in item getter format, and here is the reverse and Shawshank redemption right at the top definitely agree with that Godfather usual suspects. Yeah, these are all pretty great movies 12 angry men absolutely So there you go There's how we can look at the bias So then the second piece to look at would be the the embeddings how can we look at the embeddings? So we can do the same thing so remember I was the item embeddings rather than IB with the item bias We can pass in our list of movies as a variable Turn it into numpy, and here's our movie embeddings so for each of the three thousand most popular movies Here are its 50 embeddings so It's very hard unless you're Jeffrey Hinton to visualize a 50 dimensional space So what we'll do is we'll turn it into a three dimensional space So we can compress High dimensional spaces down into lower dimensional spaces using lots of different techniques Perhaps one of the most common and popular is called PCA PCA stands for principal components analysis. It's a linear technique but linear techniques generally work fine for this kind of Embedding I'm not going to teach you about PCA now But I will say in Rachel's computational linear algebra class which you can get to from faster AI We cover PCA in a lot of detail And it's a really important technique it actually it turns out to be almost identical to something called singular value decomposition Which is a type of matrix decomposition which? Actually does turn up in deep learning a little bit from time to time So it's kind of somewhat worth knowing if you are going to dig more into linear algebra. You know SPD and PCA along with eigenvalues and eigenvectors which are all slightly different versions of this kind of the same thing Are all worth knowing But for now just know that you can grab PCA from sklearn.decomposition Say how much you want to reduce the dimensionality to so I want to find three Components and what this is going to do is it's going to find three linear combinations of the 50 dimensions Which capture as much as the variation as possible, but are as different to each other as possible, okay? So we would call this a lower rank approximation of our matrix all right So then we can grab the components so that's going to be the three Dimensions and so once we've done that we've now got three by three thousand And so we can now take a look at the first of them and we'll do the same thing of using zip To look at each one along with its movie and so here's the thing right we We don't know ahead of time what this PCA thing is it's just it's just a bunch of latent factors You know it's kind of the the main axis in this space of latent factors and so but what we can do is we can look at it and See if we can figure out What it's about right so given that? Police Academy for is high up here along with water world Where else bar go pulp fiction and Godfeather of high up here? I'm going to guess that a high value is not going to represent like critically acclaimed movies Or serious watching so I kind of what did I call this yeah, okay? I called this easy watching versus serious right, but like this is kind of how you have to interpret your embeddings is like take a look at what they seem to be showing and Decide what you think it means so this is the kind of the The principal axis in this set of embedding so we can look at the next one So do the same thing and look at the first index one embedding This one's a little bit harder to kind of figure out what's going on But with things like Mulholland Drive and purple rose of Cairo These look more kind of dialog e kind of ones or else things like Lord of the Rings and a Latin and Star Wars These look more like kind of modern CGI e kind of ones so you could kind of imagine that on that pair of dimensions it probably represents a lot of you know differences between how people rate movies you know some people like You know purple rise of Cairo type movies you know Woody Allen Kind of classic and some people like these you know big Hollywood spectacles Some people presumably like police academy for more than they like Fargo So yeah, so like you can kind of get the idea of what's happened it's it's done a you know for a model which was You know for a model which was literally multiply two things together and add them up It's learnt quite a lot you know which is kind of cool So that's what we can do with With that and then we could we could plot them if we wanted to I just grabbed a small subset To plot on those first two axes All right, so that's that so I wanted to next kind of dig in a layer deeper Into what actually happens? when we say Fit right so when we said Learn dot fit What's it doing? For something like the store model is it a way to interpret the embeddings For something like this the the Rossman one yeah, yeah, we'll see that in a moment. Well. Let's jump straight there. What the hell okay, so So for the Rossman how much are we going to sell at each store on each date? Model we This is from the paper gore and Burkhan. It's a great paper by the way Well worth you know like pretty accessible. I think Any of you would at this point be able to at least get the gist of it if you know and much of the detail As well particularly as you've also done the machine learning course And they actually make this point in the paper. This is in the paper that the equivalent of what they call entity embedding layers so an embedding of a categorical variable is identical to a one-hot encoding followed by a Matrix multiply right so they're basically saying if you've got three embeddings That's the same as doing three one-hot encodings putting each through one through a matrix multiply and then put that through a dense layer or what Pi torch would call a linear layer right One of the nice things here is because this is kind of like well. I thought it was the first paper It's actually the second. I think paper to show the idea of using categorical embeddings for This kind of data set they really go to quite a lot of detail to you know write back to The detailed stuff that we learned about so it's kind of a second you know a second cut at thinking about what embeddings are doing So One of the interesting things that they did was they said okay after we've trained a neural net with these embeddings What else could we do with it, so They got a winning result with a neural network with entity embeddings But then they said hey, you know what? We could take those empty embeddings and replace each categorical variable With the learned entity embeddings and then feed that into a GBM Right so in other words like it rather than passing into the GBM or one hot encoded version or an ordinal version Let's actually replace the categorical variable with its embedding for the appropriate level for that row right so it's actually a way of Create you know feature engineering and so the main average percent error without that for GBMs Using just one hot encodings was point one five, but with that it was point one one Right random forests without that was point one six with that point one oh eight nearly as good as the neural net Right so this is kind of an interesting technique because what it means is in your organization you can train a neural net that has an embedding of stores and an embedding of product types and an embedding of I Don't know whatever kind of high-catenality or even medium-catenality categorical variables you have and then everybody else in the organization can now like chuck those into their you know GBM or random forest or whatever and And use them and what this is saying is they won't get in fact you can even use k nearest neighbors with this technique and get nearly as good a result right so This is a good way of kind of giving the power of neural nets to everybody in your organization without having them do the Fastai deep learning course first you know they can just use whatever sk learn or R or whatever that they're used to and like those those Embeddings could literally be in a database table because if you think about an embedding is just an index lookup Right which is the same as an inner join in SQL right? So if you've got a table of each product along with its embedding vector Then you can literally do an inner join and now you have every row in your table along with its product embedding vector so that's a really this is this is a really useful idea and GBMs and random forests learn a lot quicker than neural nets do All right, so that's like even if you do know how to train neural nets. This is still potentially quite handy so here's what happened when they took the various different states of Germany and plotted the first two principal components of their embedding vectors and they basically here is where they were in that 2d space and Wackily enough I've circled in red Three cities and I've circled here the three cities in Germany and here I've circled in purple. Sorry blue Here are the blue is the green is the green so it's actually Drawn a map of Germany even though it never was told anything about How far these states are away from each other or the very concept of geography didn't exist? So that's pretty crazy So that was from their paper, so I went ahead and looked Well here's another thing. I think this is also from their paper. They took every pair of Places and they looked at how far away they are on a map Versus how far away are they in embedding space and they got this beautiful Correlation right so again it kind of Apparently you know stores that are nearby each other Physically have similar Characteristics in terms of when people buy more or less stuff from them So I looked at the same thing for days of the week right so here's an embedding of the days of the week From our model, and I just kind of joined up Monday Tuesday Wednesday Tuesday Thursday Friday Saturday Sunday I do the same thing for the months of the year All right again. You can say you know here's here's winter Here's summer So yeah, I think like Visualizing embeddings can be interesting like it's good to like first of all check You can see things you would expect to see you know And then you could like try and see like maybe things you didn't expect to see so you could try all kinds of Cluster rings or or whatever right? And This is not something which has been Widely studied at all right, so I'm not going to tell you what the limitations are of this technique or whatever Yes, so I've heard of other ways to generate embeddings like skip grams Uh-huh, so I mean if you could say is there one better than the other using neural networks or skip grams Um so script grams is quite specific to NLP right so like I'm not sure if we'll cover it in this course, but basically The the approach to original kind of word to vec approach to generating embeddings was to say You know what we actually don't have We don't actually have a Labeled data set you know they said all we have is like Google Books And so they have an unsupervised learning problem unlabeled problem And so the best way in my opinion to turn an unlabeled problem into a labeled problem is to kind of invent some labels And so what they did in the word to vec case was they said okay? Here's a sentence with 11 words in it Right and then they said okay. Let's delete the middle word and Replace it with a random word and so You know originally it said cat and they say no let's replace that with justice Right so before it said the cute little cat sat on the fuzzy mat and now it says the cute little justice Sat on the fuzzy mat right and what they do is they do that so they have one sentence Where they keep exactly as is Right and then they make a copy of it and they do the replacement and so then they have a label Where they say it's a one if it was unchanged it was the original and zero Otherwise right and so basically then you now have something you can build a machine learning model on And so they went and built a machine learning model on this so the model was like try and find the effect sentences Not because they were interested in a fake sentence finder But because as a result they now have embeddings that just like we discussed you can now use for other purposes And that became word to vec now it turns out that if you do this as just a kind of a Effectively like a single matrix multiply rather than make it a deep neural net you can train this super quickly And so that's basically what they did with they met there though they kind of decided we're going to make a Pretty crappy model like a shallow learning model rather than a deep model You know with the downside it's a less powerful model but a number of upsides the first thing we can train it on a really large data set and Then also really importantly we're going to end up with embeddings which have really Very linear characteristics, so we can like add them together and subtract them and stuff like that, right? So that So there's a lot of stuff we can learn about there from like for other types of embedding like categorical embeddings specifically if we want categorical embeddings which we can kind of Draw nicely and expect them to ask to be able to add and subtract them and behave linearly You know probably if we want to use them in K nearest neighbors and stuff We should probably use shallow learning If we want something that's going to be more predictive we probably want to use a neural net And so actually in NLP I Am really pushing the idea that we need to move past Word to back and glove these linear based methods because it turns out that those embeddings Way less predictive than embeddings learned from deep models And so the language model that we learned about which ended up getting a state-of-the-art on sentiment analysis Didn't use glove or word to back that instead we pre trained a deep recurrent neural network And we ended up with not just a pre trained word vectors, but a full pre trained model So it looks like to create embeddings for entities we need like a dummy task Not necessarily a dummy task like in this case. We had a real task right so we created the embeddings for Rossman by trying to predict store sales You only need this isn't just in this isn't just for Learning embeddings for learning any kind of feature space You either need labeled data or You need to invent some kind of fake task So does that task matter like if I choose a task and train embeddings if I choose another task and train embeddings Like which one is? It's a great question, and it's not something that's been studied nearly enough, right? I'm not sure that many people even quite understand that when they say Unsupervised learning now that nowadays they almost nearly always mean fake task Labeled learning and so the idea of like What makes a good fake task? I don't know that I've seen a paper on that right but intuitively you know We need something where the kinds of Relationships that's going to learn likely to be the kinds of relationships that you probably care about right so for example in In computer vision one kind of fake task people use is to say like let's take some images and use some kind of like Unreal and unreasonable data augmentation like like recolor them too much or whatever And then we'll ask the neural net to like predict which one was the augmented which one was not the augmented Yeah, so it's it I think it's a fascinating area and one which You know would be really interesting for people to you know Maybe some of the students here to look into you further is like take some interesting semi-supervised or unsupervised data sets and try and come up with some like more clever Fake tasks and see like does it matter you know how much does it matter? In general like if you can't come up with a fake task that you think seems great. I would say use it Use the best you can it's often surprising how? How little you need like the ultimately crappy fake task is called the auto encoder All right, and the auto encoder? Is the thing which which won the claims prediction competition that just finished on Kaggle? they had lots of examples of Insurance policies where we knew this was how much was claimed and then lots of examples of insurance policies where I guess they must have been still Still open we didn't yet know how much they claimed right and so what they did was they said okay? So for all of the ones so let's basically start off by grabbing every policy right and we'll take a single policy And we'll put it through a neural net right And we'll try and have it reconstruct itself But in these intermediate layers at least one of those intermediate layers will make sure there's less activations Than they were input so let's say if there was a hundred variables on the insurance policy You know we'll have something in the middle that only has like 20 activations Right and so when you basically are saying hey Reconstruct your own input like it's not a different kind of model doesn't require any special code It's literally just passing you can use any standard pytorch or fast AI learner You just say my output equals my input right and that's that's like the the most Uncreated you know invented task you can create and that's called an auto encoder and it works Surprisingly well in fact the point that it literally just won a Kaggle competition they took the features that it learnt and Chucked it into another neural net and Yeah, and one You know maybe if we have enough students taking an interest in this then You know we'll be able to cover cover unsupervised learning in more detail in part two especially given this Kaggle recent have a win I Think this may be related to the previous question when training language models Is the language model fix I'm a train on the archive data is that useful at all in the movie lens movie like the movie? I am DB data great question. You know I was just Talking to Sebastian about this best in route about this this week And we thought we'd try and do some research on this in January it's it's again. It's not well-known We know that in computer vision It's shockingly effective to train on cats and dogs and use that pre-trained network to do lung cancer diagnosis and CT scans in the NLP world Nobody much seems to have tried this the NLP researchers I've spoken to others than Sebastian about this assume that it wouldn't work, and they generally haven't bothered trying. I think it would work great So So since we're talking about Rossman I've just mentioned during the week. I was interested to see like how good this solution actually actually was Because I noticed that on the public leaderboard. It didn't look like it was going to be that great and I also thought it'd be good to see like What does it actually take to? Use a test set properly with this kind of structured data So if you have a look at Rossman now I've pushed some changes that actually run the test set through as well And so you can get a sense of how to do this so you'll see basically every line appears twice one for test and one for One for train when we get there yeah test train test train test train obviously you could do this in a lot fewer lines of code By putting all of the steps into a method and then pass either the train data set or the test data set data frame to it and in this case I wanted to Kind of for teaching purposes you'd be able to see each step and through to experiment to see what each step looks like But you could certainly simplify this code So yeah, so we do this for every data frame And then some of these you can see I kind of loop through the data frame in joined and for join test right train and test This whole thing about the Durations I basically put two lines here one that said data frame equals train columns one that says data frame equals test columns And so my you know basically idea is you'd run this line first And then you would skip the next one and you'd run everything beneath it And then you'd go back and run this line and then run everything beneath it So some people on the forum were asking how come this code wasn't working this week Which is a good reminder that the code is not designed to be code that you always run top to bottom without thinking right? You're meant to like think like what is this code here should I be running it right now? Okay, and so like the early lessons I tried to make it so you can run it top to bottom But increasingly as we go along I kind of make it more and more that like you actually have to think about what's going on So Jeremy you're talking about shallow learning and deep learning Could you define that a bit better by shallow learning? I think I just mean anything that doesn't have a hidden layer So something that's like a dot product a matrix multiplier basically Okay, so So we end up with a So we end up with a training and a test version and then everything else is basically the same And then everything else is basically the same One thing to note and a lot of these details of this we cover in the machine learning course by the way It's not really deep learning specific so check that out if you're interested in the details I Should mention you know we use apply cats rather than train cats to make sure that the test set and the training set have the same same Categorical codes that they join to We also need to make sure that we keep track of the mapper This is the thing which basically says what's the mean and standard deviation of each continuous column? And then apply that same mapper to the test set and So when we do all that that's basically it then the rest is easy We just have to pass in the test data frame in the usual way when we create our model data object and Then there's no changes through all here. We trained it in the same way and then once we finish training it We can then call predict As per usual passing in true to say this is the test set rather than the validation set and pass that off to kaggle, and so it was really interesting because This was my submission. They've got a public score of 103 Which would put us in About 300 and something's place Which looks awful right and our private score of? of 107 Need a board private Is about fifth right so like if you're competing in a kaggle competition and You don't haven't thoughtfully created a validation set of your own and you're relying on public leaderboard feedback This could totally happen to you, but the other way around you'll be like oh, I'm in the top ten I'm doing great and then oh for example at the moment the icebergs competition recognizing icebergs a very large percentage of the public leaderboard set is synthetically generated Data augmentation data like totally Meaningless and so your validation set is going to be much more helpful than the public leaderboard feedback right so Yeah, be very careful, so our final score here is kind of within statistical noise of the actual third place get us some pretty confident that we we've captured their approach and So that's that was pretty interesting Something to mention There's a nice kernel about the Rossman about quite a few nice kernels actually But you can go back and see like particularly if you're doing the groceries competition go and have a look at the Rossman kernels Because actually quite a few of them are higher quality than the ones for the Ecuadorian groceries competition One of them for example showed how on for particular stores like store 85 The sales for non Sundays and the sale for Sundays looked very different Where else there are some other stores where the sales on Sunday? Don't look any different and it can kind of like get a sense of why you need these kind of interactions The one I particularly wanted to point out is the one I think I briefly mentioned that the third place winners whose Approach we used they didn't notice is this one and here's a really cool visualization Here you can see that the store this store is closed Right and just after oh my god we run out of we've run out of eggs and just before oh my god Go and get the milk before the store closes Right and here again closed bang right so this third place winner actually Deleted all of the closed store rows before they started doing any analysis right so remember how we talked about like Don't touch your data unless you first of all Analyze to see whether that thing you're doing is actually okay No assumptions right so in this case I am sure like I haven't tried it But I'm sure they would have won otherwise right because like although they weren't actually any store closures to my knowledge In the test set period the problem is that their model was trying to fit To these like really extreme things and so and because it wasn't able to do it very well It was going to end up getting a little bit confused It's not going to break the model But it's definitely going to harm it because it's kind of trying to do computations to fit something which it literally doesn't have the data For your neck can you pass that back there? All right, so that Rossman model Again like it's nice to kind of look inside to see what's actually going on right and so that Rossman model Want to make sure you kind of know how to find your way around the code so you can answer these questions for yourself So it's inside columnar model data now We started out by kind of saying hey if you want to look at the code for something you can like go question mark question mark like this and Okay, I need to I haven't got this reading, but you can use question mark question mark to get the source code for something right? But obviously like that's not Really a great way because often you look at that source code and it turns out you need to look at something else Right and so for those of you that haven't done much coding you might not be aware that almost certainly the Editor you're using probably has the ability to both open up stuff directly off SSH and To navigate through it so you can jump straight from place to place right so I want to show you what I mean So if I want to find columnar model data, and I happen to be using Vim here. I can basically say tag Columnar model data, and it will jump straight to the definition of that class right and so then I notice here that like oh It's actually building up a data loader. That's interesting if I hit ctrl right square bracket It'll jump to the definition of the thing that was under my cursor and after I finished reading it for a while I can hit ctrl t to jump back up to where I came from Right and you kind of get the idea right or if I want to find every usage of this in this file of columnar model data I can hit star to jump to the next place It's used you know and so forth right so in this case Get learner was the thing which actually got the model we want to find out what kind of model it is and apparently it uses a Or not using collaborative filtering are we we're using columnar model data, sorry Columnar model data get learner Which uses and so here you can see mixed input model is the pie torch model and then it wraps it in the structured learner Which is the the fast AI learner type which wraps the data and the model together? So if we want to see the definition of this actual pie torch model I can go to control right square bracket to see it right and so here is the model right and Nearly all of this we can now understand right so we got past We got past a list of embedding sizes Sure there is In the mixed model that we saw does it always expect categorical and continuous together Yes, it does and the The model data behind the scenes if there are no none of the other type it creates a Column of ones or zeros or something Okay, so if it is null it can still work. Yeah, yeah, yeah It's kind of ugly and hacky and will you know hopefully improve it but but yeah, you can pass in an empty list of Categorical or continuous variables to the model data, and it will basically yeah, it'll basically pass and unused column of zeros to avoid things breaking And I'm I'm leaving fixing some of these slightly hacky edge cases because pie torch 0.4 As well as getting rid of variables they're going to also add rank zero tensors Which is to say if you grab a single thing out of like a rank one tensor rather than getting back at a number Which is like? Qualitatively different you're actually going to get back at a tensor that just happens to have no rank Now it turns out that a lot of this kind of code is going to be much easier to write then so And for now it's it's a little bit more hacky than it needs to be Jeremy you talk about this a little bit before where maybe it's a good time at some point to talk about How can we? Write something that is slightly different for worries of the library Yeah, I Think we'll cover that a little bit next week, but I'm mainly going to do that in part two like part two is going to cover a Quite a lot of stuff one of the main things we'll cover in part two is what it called generative models So things where the output is a whole sentence or a whole image, but you know I also dig into like how to really Either customize the fast AI library or use it on on more custom models But if we have time we'll touch on it a little bit next week Okay, so The the learner we were passing in a list of embedding sizes and as you can see that embedding sizes list was literally just the Number of rows and the number of columns in each embedding right and the number of code rows was just coming from Literally how many stores are there in the store? Category for example and the number of columns was just equal to that divided by two And a maximum of 50 so that thing that list of tuples was coming in and so you can see here How we use it right we go through each of those tuples grab the number of categories and the size of the embedding and construct an embedding Right and so that's a that's a list right one minor thing pie torch specific thing. We haven't talked about before is for it to be able to like register remember how we kind of said like it registers your parameters it registers your Your layers like so when we like listed the model it actually printed out the name of each embedding and each bias It can't do that if they're hidden inside a list right they have to be like a they have to be a An actual and end up module subclass, so there's a special thing called an NN dot module list Which takes a list and it basically says I want you to register everything in here as being part of this model Okay, so it's just a minor tweak So yeah, so our mixed input model has a list of embeddings And then I do the same thing for a list of linear layers right so when I said here 1000, 500 this is saying how many activations I wanted featured my linear layers Okay, and so here. I just go through that list and create a linear layer that goes from this size to the next size Okay, so you can see like how easy it is to kind of construct your own not just your own model But a kind of a model which you can pass parameters to have it constructed on the fly dynamically That's normal talk about next week This is initialization. We've mentioned timing her initialization before and we mentioned it last week And then Dropout same thing right we have here a list of how much dropout to apply to each layer All right, so again here. It's just like go through each thing in that list and create a dropout layer for it Okay, so this constructor we understand everything in it except for batch norm which we don't have to worry about for now So that's the constructor and so then the forward Also, you know all stuff We're aware of go through each of those embedding layers that we just saw and remember we just treated like as a function So call it with the Ith categorical variable and then concatenate them all together Put that through dropout And then go through each one of our linear layers and call it Apply relu to it Apply dropout to it right and then finally apply the final linear layer and the final linear layer has this as its size which is Here Yeah Right size one. There's a single unit sales Okay, so we're kind of getting to the point where oh and then of course at the end if this I mentioned We would come back to this if you passed in a y underscore range parameter Then we're going to do the thing we just learned about last week Which is to use a sigmoid right and this is a cool little trick to make your not just to make your collaborative filtering better but in this case my basic idea was You know sales are going to be greater than zero And probably less than the largest sale they've ever had So I just pass in That as y range and so we do a sigmoid and multiply with the sigmoid by the range that I passed it right and so Hopefully we can find that here Yeah, here. It is right, so I actually said hey, maybe the range is between zero and You know the highest times 1.2. You know because maybe Maybe the next two weeks we have one bigger But this is kind of like again trying to make it a little bit easier for it to give us the kind of results that it Thinks is right so like increasingly You know I'd love you all to kind of try to Not treat these learners and models as black boxes But to feel like you now have the information you need to look inside them and remember you could then copy and paste this plus paste it into a cell in Jupiter notebook and Start fiddling with it to create your own versions Okay I Think what I might do is we might take a bit of a early break because we've got a lot to cover and I want To do it all in one big go, so let's take a Let's take a break until 745 and then we're going to come back and talk about recurrent neural networks all right So we're going to talk about RNN's before we do we've got to kind of dig a little bit deeper into SGD Because I just want to make sure everybody's totally comfortable with with SGD and So what we're going to look at is we're going to look at a lesson 6 SGD notebook and we're going to look at a really simple example of Using SGD to learn y equals a x plus b and So what we're going to do here is we're going to create like the simplest possible model Y equals a x plus b okay, and then we're going to generate some random data That looks like so so here's our X and here's our Y women to predict Y from X and We passed in 3 and 8 as our a and b so we're going to kind of try and recover that Right and so the idea is that if we can solve something like this which has two parameters We can use the same technique to solve We can use the same technique to solve something with 100 million parameters right without any changes at all So in order to Find a and a b that fits this we need a loss function Okay, and this is a regression problem because we have a continuous output So for continuous output regression we tend to use means grid error right and obviously all of this stuff There's there's implementations in numpy. There's implementations in pytorch. We're just doing stuff by hand so you can see all the steps right So there's MSC okay y hat is what we often call our predictions y hat minus y squared mean There's our mean square error okay? So for example if we had 10 and 5 were a and b then there's our mean square error squared error 3.25 Okay, so if we've got an a and a b and we've got an X and a y then our mean square error loss is just the mean Square error of our linear That's our predictions and our way okay, so there's a loss for 10 5 x y Alright, so that's a loss function right and so when we talk about combining Linear layers and loss functions and optionally nonlinear layers This is all we're doing right is we're putting a function inside a function Okay, that's that's all like I know people draw these clever looking Dots and lines all over the screen when they're saying this is what a neural network is But it's just a it's just a function of a function of a function okay So here we've got a prediction function being a linear layer followed by a loss function being MSE And now we can say like oh well Let's just define this is MSE loss and we'll use that in the future okay, so there's our loss function which incorporates our prediction function So let's generate 10,000 items of fake data And let's turn them into variables so we can use them with pytorch because Jeremy doesn't like taking derivatives So we're going to use pytorch for that And let's create a random weight for a and for B. So a single random number And we want the gradients of these to be calculated as we start Computing with them because these are the actual things we need to update in our SGD okay, so here's our a and B 0.029 0.111 All right, so let's pick a learning rate okay, and then let's do 10,000 epochs of SGD in fact this isn't really SGD. It's not stochastic gradient descent. This is actually full gradient descent. We're going to each Each loop is going to look at all of the data Okay Stochastic gradient descent would be looking at a subset each time So to do gradient descent we basically calculate the loss right so remember we've started out with a random a and B Okay, and so this is going to compute some amount of loss, and then it's nice from time to time So one way of saying from time to time is if the epoch number mod a thousand is zero Right so every thousand epochs just print out the loss see how we're doing okay So now that we've computed the loss we can compute our gradients right and so you just remember this thing here is Both a number a single number that is our loss something we can print But it's also a variable because we passed variables into it and therefore it also has a method dot backward Which means calculate the gradients of everything that we asked it to everything where we said requires radicals true Okay, so at this point we now have a dot grad property inside a and inside B and Here they are here is that dot grad property Okay, so now that we've calculated the gradients for a and B We can update them by saying a is equal to whatever it used to be minus the learning rate times the gradient right dot data because a is a variable and a variable contains a tensor in its dot data property and We again this is going to disappear in pytorch point four but for now It's actually the tensor that we need to update okay, so update the tensor inside here with whatever it used to be Minus the learning rate times the gradient Okay, and that's basically it right that's basically all Gradient descent is okay, so it's it's as simple as we claimed There's one extra step in pytorch Which is that you might have like multiple different loss functions or like lots of lots of output layers all contributing to the gradient, and you like have to add them all together and so If you've got multiple loss functions You could be calling lost up backward on each of them and what it does is it adds it to the gradients right? And so you have to tell it when to set the gradients back to zero Okay, so that's where you just go okay set a to zero and Gradients and set B gradients to zero okay, and so this is wrapped up inside the You know optium dot SGD Class right so when we say optium dot SGD, and we just say you know dot step It's just doing these for us so when we say dot zero gradients. It's just doing this for us And this underscore here every Pretty much every function that applies to a tensor in pytorch If you stick an underscore on the end it means do it in place Okay, so this is actually going to not return a bunch of zeros, but it's going to change this in place to be a bunch of zeros So that's basically it We can look at the same thing without pytorch Which means we actually do have to do some calculus, so if we generate some fake data again We're just going to create 50 data points this time just to make this fast and easy to look at And so let's create a function called update right we're just going to use numpy no pytorch Okay, so our predictions is equal to again linear And in this case we're actually going to calculate the derivatives so the derivative of the square of the loss is just two times And then the derivative with respect to a is just that you can confirm that yourself if you want to and so here our we're going to update a minus equals learning rate times the derivative of loss with respect to a and for B it's Learning rate times derivative with respect to B. Okay, and so what we can do Let's just run all this So just for fun rather than looping through manually we can use the map plot map plot lib func animation command to run The animate function a bunch of times and the animate function is going to run 30 Epochs and at the end of each epoch it's going to print out On the plot where the line currently is and that creates this little movie okay, so you can actually see the Line moving into place right so if you want to play around with like understanding how pytorch gradients Actually work step-by-step. Here's like the world's simplest little example, okay? And you know it's kind of like It's kind of weird to say like that's that's it like when you're optimizing a hundred million parameters in a neural net It's doing the same thing, but it it actually is right you can actually look at the pytorch code and see it This is it right. There's no trick We well we learned a couple of minor tricks last time which was like momentum and Adam right, but If you can do it in Excel you can do it in Python so okay So let's now talk about RNN's so we're now in lesson 6 RNN notebook And We're going to study Nietzsche as you should So Nietzsche says Supposing that truth is a woman what then I love this apparently All philosophers have failed to understand women so apparently at the point that Nietzsche was alive There was no female philosophers or at least those that were around didn't understand women either so anyway, so this is the philosopher Apparently we've chosen to study This is actually much less worse than people think he is But it's a different era. I guess all right, so we're going to learn to write philosophy Like Nietzsche And so we're going to do it one character at a time So this is like the language model that we did in lesson 4 where we did it a word at the time But this time we're going to do it a character at a time And so the main thing I'm going to try and convince you is an RNN is no different to anything you've already learned Okay, and so to show you that we're going to build it from Plane pie torch layers all of which are extremely familiar already okay, and eventually we're going to use something really complex Which is a for loop okay? So that's when we're going to make it really sophisticated so the basic idea of RNN is that you want to keep track of The main thing is you want to keep track of kind of state over long-term dependencies So for example if you're trying to model something like this kind of Template language right then at the end of your percent comment do percent you need a percent comment end percent right and so somehow Your model needs to keep track of the fact that it's like inside a comment Over all of these different characters right and so this is this idea of state it needs kind of memory right and this is quite a difficult Thing to do with like just a conv net it turns out actually to be possible, but It's it's you know a little bit tricky Where else we're than RNN it turns out to be pretty straightforward right so these are the basic ideas if you want a stateful Representation we're going to keeping track of like where are we now have memory have long-term dependencies and potentially even have variable length Sequences these are all difficult things to do with comp nets They're very straightforward with RNN's so for example for example Swift key a year or so ago did a blog post about how they had a new language model where they basically This is from the blog post. They basically said like of course. This is what their neural net looks like Somehow they always look like this on the internet You know you've got a bunch of words And it's basically going to take your particular words in their particular orders and try and figure out what the next words going to Be which is to say they they built a language model They actually have a pretty good language model if you've used Swift key they seem to do better predictions than anybody else still Another cool example was on Drake apathy a couple of years ago Showed that he could use character level RNN to actually create an entire Latex document, so he didn't actually tell it in any way what latex looks like he just passed in Some latex text like this and said generate more latex text and it literally started writing something which Means about as much to me as most math papers do so Okay, so we're going to start with something that's not an RNN and I'm going to introduce Jeremy's patented Neural network notation involving boxes circles and trials So Let me explain what's going on a rectangle is an input an Arrow is a layer a Circle In fact every square is a bunch of activate. Sorry every shape is a bunch of activations right the rectangle is the input activations the circle is a hidden activations and a triangle is an output activations an Arrow is a layer operation right or possibly more than one right so here my rectangle is an input of number of rows equal to batch size and number of columns equal to the number of Number of inputs number of variables right and so my first arrow my first operation is going to represent a matrix product followed by a relu and That's going to generate a set of activations remember Activations an activation is a number That an activation is a number a number that's being calculated by a relu or a matrix product or whatever It's a number right so this circle here represents a matrix of activations All of the numbers that come out when we take the inputs we do a matrix product followed by a relu So we started with batch size by number of imports and so after we do this matrix operation we now have batch size by You know whatever the number of columns in our matrix product was by number of hidden units Okay, and so if we now take these activations, right? It's the matrix and we put it through another operation in this case another matrix product and a softmax We get a triangle. That's our output activations another matrix of activations And again number of roses batch size number of columns number is equal to the number of classes again however many Columns our matrix in this matrix product head so that's a That's a neural net right that's our basic kind of one hidden layer neural net and If you haven't written one of these from scratch try it you know and in fact in lessons 9 10 and 11 of the machine Learning course we do this right we create one of these from scratch So if you're not quite sure how to do it you can check out the machine learning course Now in general the machine learning course is much more like building stuff up from the foundations Where else this course is much more like best practices kind of top-down? All right, so if we were doing like a conv net with a single dense hidden layer our input would be equal to actually Number yeah, sorry in pie torch number of channels by height by width right and Notice that here batch size appeared every time so I'm not going to I'm not going to write it anymore Okay, so I've removed the batch size Also the activation function It's always basically value or something similar for all the hidden layers and softmax at the end for classification So I'm not going to write that either okay, so I'm kind of each picture I'm going to simplify it a little bit All right, so I'm not going to mention batch size is still there We're not going to mention really or softmax, but it's still there so here's our input and so in this case rather than a Matrix product we'll do a convolution a stride to convolution So we'll skip over every second one or could be a convolution followed by a max pool In either case we end up with something which is replaced number of channels with number of filters Right and we have now height divided by 2 and width divided by 2 ok and then We can flatten that out somehow We'll talk next week about the main way we do that nowadays which is basically to do something called an adaptive max pooling Where we basically get an average across the height and the width? And turn that into a vector anyway somehow we flatten it out into a vector we can do a matrix product Or a couple of matrix products we actually tend to do in fast AI So that'll be our fully connected layer with some number of activations Final matrix product give us some number of classes ok so this is our basic component remembering Rectangles input circle is hidden triangle is output all of the shapes represent a tensor of activations all of the arrows represent a operation a layer operation All right, so now let's go to jump to the one the first one that we're going to actually Try to try to create for NLP, and we're going to basically do exactly the same thing as here Right and we're going to try and predict the third character in a three character sequence based on the previous two characters so Now input and again remember we've removed the batch size Dimension, but we're not saying it, but it's still here, okay And also here. I've removed the names of the layer operations entirely okay, just keeping simplifying things so for example our first input would be the first character of each string in our many batch okay and Assuming this is one hot encoded then the width is just however many items there are in the vocabulary How many unique characters could we have okay? We probably won't really one hot encode it will feed it in as an integer and pretend It's one hot encoded by using an embedding layer. Which is mathematically identical okay, and then we That's going to give us some activations which we can stick through a fully connected layer Okay so We we put that through a flick through a fully connected layer to get some activations We can then put that through another fully connected layer And now we're going to bring in The input of character 2 right so the character 2 input will be exactly the same Dimensionality as the character 1 input and we now need to somehow combine these two arrows together So we could just add them up for instance right because remember this arrow here represents a Matrix product so this matrix product is going to spit out the same dimensionality as this matrix product So we could just add them up to create these activations And so now we can put that through another matrix product and of course remember all these matrix products have a value as well And this final one will have a softmax instead to create our predicted set of characters right so it's a standard You know to hidden layer I Guess it's actually three matrix products Neural net This first one is coming through an embedding layer The only difference is that we're also got a second input coming in here, but we're just adding in right But it's kind of conceptually identical So let's let's implement that For Nietzsche right so and I'm not going to use torch text I'm going to try not to use almost any fast AI so we can see it all kind of again from raw right so Here's the first 400 characters of the connected works Let's grab a set of all of the letters that we see there And sort them okay, and so a set creates all the unique letters So we've got 85 unique letters in our vocab Let's pop a it's nice to put an empty kind of a null or some kind of padding character in there for padding So we're going to put a padding character at the start right and so here is What our vocab looks like okay, so so cars is our vocab So as per usual we want some way to map Every character to a unique ID and every unique ID to a character And so now we can just go through our collected works of niche and Grab the index of each one of those characters so now We've just turned it into this right so rather than Quote P re we now have 40 42 29 Okay so So that's basically the first step and just to confirm we can now take each of those indexes and Turn them back into characters and join them together and yeah there it is Okay so From now on we're just going to work with this IDX list the list of Character numbers in the connected works of Nietzsche yes So Jeremy why are we doing like a model of characters and not a model of words? I just thought it seemed simpler. You know with a vocab of 80 ish items we can Kind of see it better Character level models Turn out to be potentially quite useful in a number of situations, but we'll cover that in part two the short answer is like You generally want to combine both the word level model and a character level model like if you're doing say translation It's a great way to deal with unknown like unusual words rather than treating it as unknown Anytime you see a word you haven't seen before you could use a character level model for that And there's actually something in between the two called a byte pair encoding BPE which basically looks at little n Grams of characters, but we'll cover all that in part two If you want to look at it right now Then part two of the existing course already has this stuff taught and part two of the Version one of this course all the LVL P stuff is in pie torch by the way, so you'll understand it straight away It was actually the thing that inspired us to move to pie torch because trying to do it in Keras turned out to be a nightmare All right, so let's create the Inputs to this we're actually going to do something slightly different what I said We're actually going to try and predict the fourth character The well actually the the fifth character using the first four so the index four character using the index zero one two and three All right, so we're gonna do exactly the same thing, but with just a couple more layers so that means that we need a list of Of the zeroth first second and third characters That's why I'm just cutting every character from the start from the one from two from three skipping over three at a time Okay, so Hmm this is I I said this wrong, so we're going to predict the third character the fourth character from the third from the first three Okay, the fourth character from the first three All right, so our inputs will be These three lists right so we can just use NP dot stack to pop them together right, so here's the zero one and two Characters that are going to feed into a model and then here is the next character in the list So for example X1 X2 X3 and Y All right, so you can see for example we start off the first the very first item would be 40 42 and 29 that's that's characters not one and two and then we'd be predicting 30 that's the fourth character which is The start of the next row right so then 30 25 27 we need to predict 29 Which is the start of the next row and so forth so we're always using three characters to predict the fourth So there are 200,000 of these That we're going to try and model right, so We're going to build this model which means we need to decide how many activations So I'm going to use 256 Okay, and we need to decide how big our embeddings are going to be and so I decided to use 42 so about half the number Of characters I have And you can play around with these see if you can come up with better numbers. It's just kind of experimental And now we're going to build our model Now I'm going to change my model slightly and so here is the full version So predicting character for using characters one two and three as you can see it's the same picture as the previous page But I put some very important colored arrows here All the arrows of the same color are going to use the same Matrix the same weight matrix right so all of our input embeddings are going to use the same matrix all of our Layers that go from one layer to the next are going to use the same Orange arrow weight matrix and then our output will have its own matrix So we're going to have one two three weight matrices Right and the idea here is the reason I'm not going to have a separate one for every everything here is that like Why would kind of semantically a character have a different meaning depending? If it was the first or the second or the third item in a sequence like it's not like we're even starting every sequence At the start of a sentence. We just arbitrarily chopped it into groups of three right so you would expect these to all have the same kind of Conceptual mapping and ditto like when we're moving from character naught to character one You know to kind of say build up some state here Why would that be any different kind of operation to moving from character one to character two? So that's the basic idea, so let's create a three character model and So we're going to create one linear layer for our green arrow One linear layer for orange arrow and one linear layer for our blue arrow and then also one embedding Okay, so the embedding is going to bring in something with of size whatever it was 84 I think vocab size and spit out something with a number of factors in the embedding We'll then put that through a linear layer And then we've got our hidden layers. We've got our output layer so when we call Forward we're going to be passing in one two three characters So for each one we'll stick it through an embedding We'll stick it through a linear layer, and we'll stick it through a value Yes, we do it for character one character two and character three Okay Then I'm going to create This circle of activations here, okay, and that matrix I'm going to call H Right and so it's going to be equal to my input activations Okay after going through the value and the linear layer and the embedding right and then I'm going to apply this L hidden so the orange arrow and that's going to get me to here Okay, so that's what this layer here does and then to get to the next one I need to apply the same thing and it's by the orange arrow to that okay, but I also have to add in The second input right so take my second input and add in Okay my previous layer Yanet could you pass that back through yours I? Don't really see how these dimensions are the same from H And I am too from which to which from yeah, okay? Let's go through it, so let's figure out the dimensions together, so Self dot e is going to be of length 42 Okay, and then it's going to go through L in just going to make it of size and hidden Okay And so then we're going to pass that which is now is size and hidden through this which is Also going to return something of size and hidden okay, so it's really important to notice that this is square This is a square weight matrix Okay, so we know I'll know that this is of size and hidden in In two is going to be exactly the same size as in one was which is n hidden so we can now sum together two sets of activations both of size and hidden passing it into here and Again, it returns something of size and hidden so basically the trick was to make this a square matrix and to make sure that it's square Matrix was the same size as the output of this hidden way. Thanks for the great question. Can you pass that back to you now? Jeremy is Summing the only thing people can do in these cases or we'll come back to that in a moment. That's a great point okay I don't like it when I have like Three bits of code that look identical and then three bits of code that look nearly identical But aren't quite because it's harder to refactor, so I'm going to put a Make H into a bunch of zeros so that I can then put H here and these are now identical Right so that the hugely complex trick that we're going to do very shortly is to replace these three things With a for loop okay, and it's going to loop Through one two and three and that's that's going to be the for loop or actually zero one and two okay at that point We'll be able to call it a recurrent neural network. Okay, so just to skip ahead a little bit alright, so we create that That model make sure I've run all these so we can actually run this thing Okay So we can now just use the same columnar model data class that we've used before and if we use from arrays Then it's basically it is going to spit back the exact arrays We gave it right so if we pass if we stack together those three arrays Then it's going to feed us those three things back to our forward method so if you want to like Play around with training models Using like you know as raw and approach as possible, but without writing lots of boilerplate This is kind of how to do it use columnar met model data from arrays And then if you pass in whatever you pass in here Right you're going to get back here Okay So I've passed in three things which means I'm going to get sent three things okay, so that's how that works I'm batch size 512 because this is you know this data is tiny so I can use a bigger batch size So I'm not using Really much fast AI stuff at all I'm using fast AI stuff just to save me fiddling around with data loaders and data sets and stuff But I'm actually going to create a standard pie torch model. I'm not going to create a learner Okay, so this is a standard pie torch model and because I'm using pie torch that means I have to remember to write kuda Okay, let's take it on the GPU So Here is how we can look inside at what's going on right so we can say it a MD dot train data loader to grab the iterator to iterate through the training set We can then call next on that to grab a mini batch, and that's going to return All of our X's and our Y tensor and so we can then take a look at You know here's our X's for example All right, and so you would expect have a think about what you would expect for this length Three not surprisingly because these are the three things okay, and so then excess Zero Not surprisingly okay is of length 512 And it's not actually one hot encoded because we use an embedding to pretend it is Okay, and so then we can use a model as if it's a function Okay by passing to it the variable eyes version of our tensors and So have a think about what you would expect to be returned here Okay, so not surprisingly we had a mini batch of 512 So we still have 512 and then 85 is the probability of each of the possible vocab items and of course We've got the log of them because that's kind of what we do in pytorch Okay, you can see here the softmax all right, so that's how you can look inside Right so you can see here how to do everything really very much by hand So we can create an optimizer Again using standard pytorch so with pytorch We use a pytorch optimizer you have to pass in a list of the things to optimize and so if you call M.parameters that will return that list for you And then we can fit And there it goes okay and So we don't have Learning rate finders and SGDR and all that stuff because we're not using a learner So we'll have to manually do learning rate annealing so set the learning rate a little bit lower and fit again okay? Okay, and so now we can write a little function to to test this thing out okay, so Here's something called get next Where we can pass in three characters Like y full stop space right and so I can then go through and turn that into a tensor with capital T of an array of The character index for each character in that list so basically turn those into the integers Turn those into variables pass that to our model Right and then we can do an arg max on that to grab which character number is it? And in order to do stuff in numpy land like I use to NP to turn that variable into a numpy array Right and then I can return that character and so for example a capital T Was what it thinks would be reasonable after seeing why full stop space that seems like a very reasonable way to start a sentence If it was PPL a that sounds reasonable space th e that's bounce reasonable a and D space that sounds reasonable So it seems to have like created something Sensible right so you know the important thing to note here is our Character model is a totally standard Fully connected model right the only slightly interesting thing we did was to kind of do this addition of each of the inputs one at a time Okay But there's nothing New conceptually here. We're training it in the usual way All right, let's now create an RNN So an RNN is when we do Exactly the same thing That we did here Right, but I could draw this more simply by saying you know what if we've got a green arrow going to a circle Let's not draw a green arrow going to a circle again and again and again But let's just draw it like this green arrow going to a circle right and rather than drawing an orange arrow going to a circle Let's just draw it like this Okay, so this is the same picture Exactly the same picture as this one All right And so you just have to say how many times to go around this circle right so in this case if we want to predict Character number n from characters 1 through n minus 1 then we can take the character 1 import get some activations Feed that to some new activations that go through remember orange is the hidden to hidden weight matrix Right and each time will also bring in the next character of input through its embeddings Okay, so that picture and that picture are two ways of writing the same thing But this one is more flexible because rather than me having to say hey, let's do it for eight I don't have to draw eight circles But I can just say oh just repeat this So I could simplify this a little bit further by saying you know what rather than having this thing as a special case Let's actually start out with a bunch of zeros right and then let's have all of our characters Inside here yes Yeah, so I was wondering if you can explain a little bit better. Why are you reusing those? Why use the same color arrows? They're same. Yeah, where are you you're kind of seem to be reusing the same same weight matrices weight matrices Yeah, maybe this is kind of similar to what we did in convolutional your nets like it's somehow No, I don't think so at least not that I can see so the idea is just kind of semantically speaking like this arrow here this this arrow here is saying take a character of import and represented as some So some set of features Right and this arrow is saying the same thing take some character and represent as a set of features and so is this one right so like Why would the three be represented with different weight matrices because it's all doing the same thing Right and this orange arrow is saying Kind of transition from character zeroes state to character one state characters to state Again, it's it's the same thing. It's like why would the transition from character zero to one be different to character from? Transition from one to two so the idea is like But is to like say hey if it's doing the same Conceptual thing let's use the exact same Weight matrix my comment on convolutional networks is that a filter or so this apply? Oh tomorrow places and we'll take this point of view yeah, I think so You're saying like a convolution is almost like a kind of a special dot product with shared weights. Yeah, no, that's okay That's very good point and in fact One of our students actually wrote a good blog post about that last year. We should dig that up, okay I totally see where you're coming from and I totally agree with you All right, so let's let's implement this version So This time we're going to do eight characters eight seas Okay, and so let's create a list of every eighth character From zero through seven and then our outputs will be the next character and so we can stack that together And so now we've got six hundred thousand by eight Eight so here's an example So for example after this series of eight characters Right so this is characters naught through eight. This is characters one through nine. This is two through ten. These are all overlapping Okay, so after characters one naught through eight. This is going to be the next one Okay, and then after these characters, this will be the next one. All right, so you can see that this one here has 43 is its y value right because after those the next one will be 43 okay, so So this is the first eight characters. This is two through nine Three through ten and so forth right so these are overlapping groups of eight characters, and then this is the the next one along Okay So let's Create that model Okay, so again we use from arrays to create a model data class And so you'll see here we have exactly the same code as we had before there's our embedding Linear hidden output these are literally identical Okay, and then we've replaced our our Reliou of the linear input of the embedding with something that's inside a loop Okay, and then we've replaced the self dot L hidden thing Okay, also inside the loop I just realized I didn't mention last time the use of the hyperbolic tan Hyperbolic tan looks like this Okay, so it's just a sigmoid that's offset right and it's very common to use a hyperbolic tan Inside this trend this state-to-state transition because it kind of stops it from flying off too high or too low You know it's nicely controlled Back in the old days we used to use hyperbolic tan or the equivalent Sigmoid a lot as most of our activation functions nowadays. We tend to use relu, but in these hidden state to here in the hidden state Transition weight matrices we still tend to use hyperbolic tan quite a lot, so you'll see I've done that also So yeah hyperbolic tan okay, so this is exactly the same as before But I've just replaced it with a for loop and then here's my output Yes, you know So does he have to do anything with convergence of these networks? Yeah, kind of well we'll talk about that a little bit over time Let's let's let's come back to that though for now. We're not really going to do anything special at all We don't recognizing. This is just a standard fully connected network You know interestingly it's quite a deep one right like because This is actually this but we've got eight of these things now We've now got a deep eight layer network, which is why units Starting suggests we should be concerned is you know as we get deeper and deeper networks that can be harder and harder to trade But let's try training this All right, so where it goes as before we've got a batch size of 512 We're using Adam And away it goes so we'll sit there watching it so we can then set the learning rate down back to 1 in egg 3 We can fit it again And yeah, it's actually it seems to be training fun, okay But we're going to try something else which is we're going to use the trick that Yannett rather hinted at before which is maybe we shouldn't be adding these things together and so the reason you might want to be feeling a little uncomfortable about adding these things together is that the input state and the hidden state a kind of qualitatively different kinds of things Right the input state is the is the encoding of this character Whereas H represents the encoding of the series of characters so far and so adding them together is Kind of potentially going to lose information So I think what your net was going to prefer that we might do is maybe to concatenate these instead of adding them So that's how good to you get it. She's not it okay, so Let's now make a copy of the previous cell all the same right but rather than using plus let's use Cat Right now if we can cat then we need to make sure now that our input layer is Not from n fact to hidden which is what we had before But because we're concatenating it needs to be n fact plus and hidden To end hidden okay, and so now that's going to make all the dimensions work nicely So this now is of size n fact plus n hidden This now makes it back to size n hidden again Okay, and then this is putting it through the same square matrix as before so it's still a size n hidden Okay, so this is like a good design heuristic if you're designing an architecture is if you've got Different types of information that you want to combine you generally want to concatenate it Right you know adding things together even if they're the same shape is losing Information okay, and so once you've concatenated things together you can always convert it back down to a Fixed size by just chucking it through a matrix product okay, so that's all we've done here again It's the same thing, but now we're concatenating instead and So we can fit that and so last time we got 1.72 This time we go at 1.68, so it's not setting the world on fire But it's an improvement and the improvements are good okay So we can now test that with get next and so now we can pass in eight things Right so it's now before those it looks good or part of that sounds good as well so Queens and that sounds good too alright, so great So that's enough manual hackery Let's see if pytorch can do some of this for us and so basically What pytorch will do for us is it will write this loop? Automatically okay, and it will create these linear input layers Automatically okay, and so to ask it to do that we can use the nn dot rnn plus So here's the exact same thing in less code By taking advantage of pytorch and again I'm not using a conceptual analogy to say pytorch is doing something like it I'm saying pytorch is doing it right this is just the code you just saw Wrapped up a little bit refactored a little bit for your convenience alright, so where we say we now want to create an RNN Called RNN Then what this does is it does that for loop now notice that our for loop? needed a starting point You remember why right because otherwise our for loop didn't quite work We couldn't quite refactor it out and because this is exactly the same this needs a starting point, too All right, so let's give it a starting point and so you have to pass in your initial hidden state Okay for reasons that will become apparent later on It turns out To be quite useful to be able to get back That hidden state at the end and just like we could here. We could actually keep track of the hidden state we Get back to things we get back both the output and the hidden state right so we pass in the input in the hidden state And we get back the output and the hidden state Yes, could you remind us what the hidden state represents the hidden state is H? so it's the It's the orange Circle ellipse of activations Okay, and so it is of size 256 Okay All right, so we can Okay, there's one other thing to know which is in our case we were replacing H with a new hidden state The one minor difference in pytorch is they append the new hidden state To a list or to a tensor which gets bigger and bigger so they actually give you back all of the hidden states So in other words rather than just giving you back the final Ellipse they give you back all the ellipses stacked on top of each other and so because we just want the final one I just got indexed into it with minus one Yeah, okay other than that. This is the same code as before put that through our output layer to get the correct vocab size And then we can train that Right so you can see here. I can do it manually I can create some hidden state I can pass it to that RNN and I can see the stuff I get back you'll see that the Dimensionality of H. It's actually a rank 3 tensor where else in my version it was a See it was a rank 2 tensor Okay, and the difference is here. We've got just a unit axis at the front We'll learn more about why that is later But basically it turns out you can have a second RNN that goes backwards Right one that goes forwards one that goes backwards and the idea is it can then it's going to be better at finding Relationships that kind of go backwards that's called a bi-directional RNN also it turns out you can have an RNN feed to an RNN That's got a multi-layer RNN So basically if you have those things you need an additional Axis on your tensor to keep track of those additional layers of hidden state But for now we'll always have a one Yeah, and we'll always also get back a one at the end Okay So if we go ahead and fit this now Let's actually train it for a bit longer Okay, so last time we only kind of did a couple of epochs this time. We'll do four epochs What do we set at one in egg three and? Then we'll do another two epochs at one in egg four And so we've now got our loss down to 1.5 So getting better and better So here's our get next again Okay, and you know it's just it was the same thing so what we can now do is we can loop through Like 40 times calling get next each time and then each time we'll replace that input by removing the first character and Adding the thing that we just predicted and so that way we can like feed in a new set of eight characters again and again and again And so that way we'll call that get next in so here are 40 characters that we've generated so we started out with for thos We got for those of the same to the same to the same You can probably guess what happens if you keep predicting the same to the same Right so it's you know it's doing okay We we now have something which You know We've basically built from scratch, and then we've said here's how Pie torch refactored it for us, so if you want to like have an interesting little homework assignment this week Try to write your own version of an iron in class Right like trying to like literally like create your like you know Jeremy's iron in and then like type in here Jeremy's iron in or in your case. Maybe your name's not Jeremy which is okay, too And then get it to run writing your implementation of that class from scratch without looking at the pie torch source code You know like basically it's just a case of like going up and seeing What we did back here right and like make sure you get the same answers and confirm that you do So that's kind of a good little test Simp very simple at all assignment, but I think you'll feel really good when you seem like oh I've just really implemented and end on our name All right, so I'm gonna do one other thing when I switched from this one when I've moved the car one input inside the dotted line Right this dotted rectangle represents the thing I'm repeating I Also, what's the triangle the output I? Move that inside as well Now that's a big difference Because now what I've actually done is I'm actually saying spit out an output after every one of these circles So spit out an output here and here and here Right so in other words if I have a three character input. I'm going to spit out a three character output I'm saying after character one this will be next after character two this would be next up to character three this will be next So again nothing different I And again this you know if you wanted to go a bit further with the assignment You could write this by hand as well But basically what we're saying is in the for loop would be saying like You know Results equals some empty list right and then we'd be going through and rather than returning that We'd instead be saying You know results dot append That right and then like return whatever torch dot stack Something like that right that it made me right I'm not quite sure So now you know we now have like Every step we've created an output Okay, so which is basically this picture and so the reason Was lots of reasons that's interesting But I think the main reason right now that's interesting is that you probably noticed This This approach to Dealing with our data seems terribly inefficient like we're grabbing the first eight Right, but then this next set All but one of them overlap the previous one Right so we're kind of like recalculating Calculating the exact same embeddings seven out of eight of them are going to be exact same embeddings Right exact same transitions It kind of seems weird to like do all this calculation To just predict one thing and then go back and recalculate seven out of eight of them and add one more to the end To calculate the next thing right so the basic idea then is to say well. Let's not do it that way Instead let's taking non overlapping sets of characters All right, so like so here is our first eight characters Here is the next eight characters here are the next eight characters so like if you read this Top left to bottom right that would be the whole Nietzsche Right and so then If these are the first eight characters then offset this by one starting here That's a list of outputs Right so after we see characters zero through seven we should predict characters one through eight That makes sense so after 40 should come 42 as it did after 42 should come 29 as it did Right and so now that can be our inputs and labels for that model And so it shouldn't be any More or less accurate It should just be the same right pretty much But it should allow us to do it more efficiently So let's try that all right So I mentioned last time that we had a Minus one index here because we just wanted to grab the last triangle Okay, so in this case. We're going to grab all the triangles, so this this is actually the way in n dot RNN creates things We we only kept the last one But this time we're going to keep all of them So we've made one change which is to remove that minus one other than that This is the exact same code as before Okay So There's nothing much to show you here. I mean except of course at this time if we look at the the labels it's now 512 by 8 that because we're trying to predict eight things every time through So there is one complexity here, which is that we want to use the the negative log likelihood loss function As before right but the leg of loss likelihood loss function just like RMSE expects to receive two Rank one tensors actually with the mini batch access to rank two tensors right so to two mini batches of vectors Problem is that we've got eight eight Time steps you know eight characters in an RNN. We call it a time step right we have eight time steps And then for each one we have 84 Probabilities we have the probability for every single One of those eight time steps, and then we have that for each of our 512 items in the mini-batch, so we have a rank three tensor Not a rank two tensor So that means that the negative log likelihood loss function is going to spit out an error Now frankly, I think this is kind of dumb. You know I think it would be better if Pi torch had written their loss functions in such a way that they didn't care at all about rank, and they just applied it To whatever rank you gave it, but for now at least it does care about Rick But the nice thing is I get to show you how to write a custom loss function Okay, so we're going to create a special negative log likelihood loss function for sequences Okay, and so it's going to take an input in the target, and it's going to call f dot negative log likelihood loss So the pi torch one all right But what we're going to do is we're going to Flatten our input and We're going to flatten our targets right and So and it turns out these are going to be The first two axes are going to have to be transposed so the way Pi torch handles RNN data by default is the first axis is the sequence length in this case eight right so the sequence length of an RNN Is how many time steps? So we have eight characters so a sequence length of eight the second axis is the batch size and Then as we would expect the third axis is the actual hidden state itself, okay, so this is going to be eight by five twelve by N hidden which I think was 256 Yeah, okay So we can grab the size and unpack it into each of these sequence length batch size num hidden Now target Yt dot Size Is 512 by 8 8 where else this one here was 8 by 512 so to make them match we're going to have to transpose the first two axes Okay Hi torch when you do something like transpose doesn't generally actually shuffle the memory order But instead it just kind of keeps some internal metadata to say like hey you should treat this as if it's transposed and and Some things in pie torch will give you an error if you try and use it when it has these like this internal state It'll basically say Error this tensor is not contiguous if you ever see that error at the word contiguous after it and it goes away So I don't know they can't do that for you apparently so in this particular case I got that error so I wrote the word contiguous after it okay And so then finally we need to flatten it out into a single vector and so we can just go dot view Which is the same as numpy dot reshape and minus 1 means as long as it needs to be Okay, and then the input again. We also reshape that right, but remember the input sorry the the the predictions Also have this axis of length 84 all of the predicted probabilities Okay, so so here's a custom Here's a custom loss function. That's it right so if you ever want to play around with your own loss functions you can just do that like so and then Pass that to fit right so it's important to remember that fit is this like Lowest level fast AI abstraction you know that sits that this is the thing that implements the training loop Okay, and so like you're the stuff you pass it in is all standard pie torch stuff except for this This is our model data object. This is the thing that wraps up the test set The training set and the validation set together, okay? You're not could you pass that back? So when we pull the triangle into the repetitive structure right? So the the first n minus 1 Iterations of the sequence length we don't see the whole sequence length Yeah, so does that mean that the batch size should be much bigger so that you get a triangular kind of the big careful You don't mean batch size you mean sequence length right because the batch size is like something else entirely yeah, okay, so yes Yes, if you have a short sequence length like eight Yeah, the first character Has nothing to go on it starts with an Empty hidden state of zeros All right, so what we're going to start with next week is we're going to learn how to avoid that problem, right? And so it's a really insightful Question or concern right and but if you think about it the basic idea is Why should we reset this to zero? Every time you know like if we can kind of line up these Many batches somehow so that the next mini batch Joins up correctly it represents like the next letter in Nietzsche's works Then we want to move this up into the constructor Right and then like pass that here and Then store it here Right and now We're not resetting the hidden state each time. We're actually We're actually keeping the hidden state from call to call and so the only Time that it would be failing to benefit from learning state would be like literally at the very start of the document So that's where we're that's where we're going to train ahead next week I Feel like this lesson every time I've got a punchline coming somebody asks me a question where I have to like do the punchline ahead of time Okay, so we can fit that and we can fit that And I want to show you something interesting and this is coming to the punchline that another punchline that you're net tried to spoil which is When we're You know remember. This is just doing a loop right applying the same matrix multiplier again and again If that matrix multiply tends to increase the Activations each time then effectively we're doing that to the power of eight right so it's going to like shoot off Really high or if it's decreasing it a little bit each time. It's going to shoot off really low That's what we call a gradient explosion right and so we really want to make sure That the initial h Not h the initial what did we call it? The initial L hidden That we create is is like of a size That's not going to cause our activations on average to increase or decrease Right and there's actually a very nice matrix That does exactly that Called the identity matrix So the identity matrix for those that don't quite remember their linear algebra is this This would be a size three identity matrix right and so The trick about an identity matrix is anything times an identity matrix is itself Right and so therefore you could multiply by this again and again and again and again and still end up with itself Right so there's no gradient explosion so What we could do is instead of using whatever the default Random in it is for this matrix We could instead after we create our RNN is we can go into that RNN right and notice this right we can go M.RNN Right and if we now go Like so we can get the docs for M.RNN Right and as well as the arguments for constructing it it also tells you the inputs and outputs for calling the layer And it also tells you the attributes and so it tells you there's something called weight HH and these are the learnable hidden to hidden weights. That's that square matrix Right so after we've constructed our M. We can just go in and say all right M.RNN.weightHHL Weight HHL Dot data that's the tensor dot copy underscore in place Torch dot I that is I for identity in case you are wondering So this is an identity matrix of size and hidden so this both puts into this weight matrix and returns the identity matrix and so this was like Actually a Jeffrey Hinton paper Was like hey, you know after what is this 2015 so after? Recurrent neural nets have been around for decades. He was like Hey gang Maybe we should just use the identity matrix to initialize this and like it actually turns out to work really well And so that was a 2015 paper believe it or not From the father of neural networks and so here is the here is our implementation of his paper And this is an important thing to note right when very famous people like Jeffrey Hinton write a paper Sometimes an entire implementation of that paper looks like one line of code Okay, so let's do it before we got point six one two five seven We'll fit it with exactly the same parameters and now we've got point five one and in fact We can keep training point five oh so like this tweak really really really helped Okay, and one of the nice things about this tweak was before I could only use a learning rate of one in egg three Before it started going crazy, but after I use the identity matrix I've found I could use one in egg two because it's you know it's better behaved weight initialization. I found I could use a higher learning rate Okay, and honestly these things You know increasingly we're trying to Incorporate into the defaults in fast AI you you know you don't want necessarily increasingly need to actually know them But you know at this point We're still at a point where you know most things in most libraries most of the time don't have great defaults It's good to know all these little tricks It's also nice to know if you want to improve something what kind of tricks people have used elsewhere because you can often borrow them yourself All right Well, that's the end of the lesson today, and so next week we will Look at this idea of a stateful RNN That's going to keep its hidden state around and then we're going to go back to looking at language models again And then finally we're going to go all the way back to computer vision and learn about things like resnets and batch norm And all the tricks that were in figured out in cats versus dogs see you then", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.24, "text": " Welcome back lesson 6, so this is our penultimate lesson", "tokens": [4027, 646, 6898, 1386, 11, 370, 341, 307, 527, 3435, 723, 2905, 6898], "temperature": 0.0, "avg_logprob": -0.3205244681414436, "compression_ratio": 1.446927374301676, "no_speech_prob": 0.0037646894343197346}, {"id": 1, "seek": 0, "start": 9.0, "end": 11.0, "text": " Believe it or not", "tokens": [21486, 309, 420, 406], "temperature": 0.0, "avg_logprob": -0.3205244681414436, "compression_ratio": 1.446927374301676, "no_speech_prob": 0.0037646894343197346}, {"id": 2, "seek": 0, "start": 13.92, "end": 21.44, "text": " Couple of weeks ago in lesson 4 I mentioned I was going to share that lesson with this terrific NLP researcher Sebastian Ruda", "tokens": [38266, 295, 3259, 2057, 294, 6898, 1017, 286, 2835, 286, 390, 516, 281, 2073, 300, 6898, 365, 341, 20899, 426, 45196, 21751, 31102, 497, 11152], "temperature": 0.0, "avg_logprob": -0.3205244681414436, "compression_ratio": 1.446927374301676, "no_speech_prob": 0.0037646894343197346}, {"id": 3, "seek": 0, "start": 21.8, "end": 25.98, "text": " Which I did and he he said he loved it and he's gone on to", "tokens": [3013, 286, 630, 293, 415, 415, 848, 415, 4333, 309, 293, 415, 311, 2780, 322, 281], "temperature": 0.0, "avg_logprob": -0.3205244681414436, "compression_ratio": 1.446927374301676, "no_speech_prob": 0.0037646894343197346}, {"id": 4, "seek": 2598, "start": 25.98, "end": 34.06, "text": " Yesterday released this new post he called optimization for deep learning highlights in 2017 in which he covered", "tokens": [19765, 4736, 341, 777, 2183, 415, 1219, 19618, 337, 2452, 2539, 14254, 294, 6591, 294, 597, 415, 5343], "temperature": 0.0, "avg_logprob": -0.2113386447613056, "compression_ratio": 1.6071428571428572, "no_speech_prob": 7.252866635099053e-05}, {"id": 5, "seek": 2598, "start": 35.14, "end": 37.14, "text": " Basically everything that we talked about in that lesson", "tokens": [8537, 1203, 300, 321, 2825, 466, 294, 300, 6898], "temperature": 0.0, "avg_logprob": -0.2113386447613056, "compression_ratio": 1.6071428571428572, "no_speech_prob": 7.252866635099053e-05}, {"id": 6, "seek": 2598, "start": 39.06, "end": 43.74, "text": " And with some very nice shout outs to some of the work that some of the students here have done", "tokens": [400, 365, 512, 588, 1481, 8043, 14758, 281, 512, 295, 264, 589, 300, 512, 295, 264, 1731, 510, 362, 1096], "temperature": 0.0, "avg_logprob": -0.2113386447613056, "compression_ratio": 1.6071428571428572, "no_speech_prob": 7.252866635099053e-05}, {"id": 7, "seek": 2598, "start": 44.7, "end": 47.980000000000004, "text": " including when he talked about this separation of", "tokens": [3009, 562, 415, 2825, 466, 341, 14634, 295], "temperature": 0.0, "avg_logprob": -0.2113386447613056, "compression_ratio": 1.6071428571428572, "no_speech_prob": 7.252866635099053e-05}, {"id": 8, "seek": 4798, "start": 47.98, "end": 51.98, "text": " Of the", "tokens": [2720, 264], "temperature": 0.0, "avg_logprob": -0.29831134911739465, "compression_ratio": 1.5666666666666667, "no_speech_prob": 1.2804432117263786e-05}, {"id": 9, "seek": 4798, "start": 52.9, "end": 59.3, "text": " Separation of weight decay from the momentum term and so he actually mentions here", "tokens": [43480, 399, 295, 3364, 21039, 490, 264, 11244, 1433, 293, 370, 415, 767, 23844, 510], "temperature": 0.0, "avg_logprob": -0.29831134911739465, "compression_ratio": 1.5666666666666667, "no_speech_prob": 1.2804432117263786e-05}, {"id": 10, "seek": 4798, "start": 60.339999999999996, "end": 62.5, "text": " the opportunities in terms of improved", "tokens": [264, 4786, 294, 2115, 295, 9689], "temperature": 0.0, "avg_logprob": -0.29831134911739465, "compression_ratio": 1.5666666666666667, "no_speech_prob": 1.2804432117263786e-05}, {"id": 11, "seek": 4798, "start": 63.26, "end": 66.6, "text": " kind of software decoupling this allows and actually links to", "tokens": [733, 295, 4722, 979, 263, 11970, 341, 4045, 293, 767, 6123, 281], "temperature": 0.0, "avg_logprob": -0.29831134911739465, "compression_ratio": 1.5666666666666667, "no_speech_prob": 1.2804432117263786e-05}, {"id": 12, "seek": 4798, "start": 70.25999999999999, "end": 75.42, "text": " The commit from an answer ha actually showing how to implement this in fast AI so fast AI's", "tokens": [440, 5599, 490, 364, 1867, 324, 767, 4099, 577, 281, 4445, 341, 294, 2370, 7318, 370, 2370, 7318, 311], "temperature": 0.0, "avg_logprob": -0.29831134911739465, "compression_ratio": 1.5666666666666667, "no_speech_prob": 1.2804432117263786e-05}, {"id": 13, "seek": 7542, "start": 75.42, "end": 78.5, "text": " Code is actually being used as a bit of a role model now", "tokens": [15549, 307, 767, 885, 1143, 382, 257, 857, 295, 257, 3090, 2316, 586], "temperature": 0.0, "avg_logprob": -0.2166276831375925, "compression_ratio": 1.5982532751091703, "no_speech_prob": 1.0952432603517082e-05}, {"id": 14, "seek": 7542, "start": 79.9, "end": 81.9, "text": " He then covers", "tokens": [634, 550, 10538], "temperature": 0.0, "avg_logprob": -0.2166276831375925, "compression_ratio": 1.5982532751091703, "no_speech_prob": 1.0952432603517082e-05}, {"id": 15, "seek": 7542, "start": 82.86, "end": 86.62, "text": " some of these learning rate training techniques that we've talked about and", "tokens": [512, 295, 613, 2539, 3314, 3097, 7512, 300, 321, 600, 2825, 466, 293], "temperature": 0.0, "avg_logprob": -0.2166276831375925, "compression_ratio": 1.5982532751091703, "no_speech_prob": 1.0952432603517082e-05}, {"id": 16, "seek": 7542, "start": 88.06, "end": 90.06, "text": " This is the", "tokens": [639, 307, 264], "temperature": 0.0, "avg_logprob": -0.2166276831375925, "compression_ratio": 1.5982532751091703, "no_speech_prob": 1.0952432603517082e-05}, {"id": 17, "seek": 7542, "start": 91.5, "end": 95.42, "text": " SGDR schedule it looks a bit different to what you're used to seeing this is on a log curve", "tokens": [34520, 9301, 7567, 309, 1542, 257, 857, 819, 281, 437, 291, 434, 1143, 281, 2577, 341, 307, 322, 257, 3565, 7605], "temperature": 0.0, "avg_logprob": -0.2166276831375925, "compression_ratio": 1.5982532751091703, "no_speech_prob": 1.0952432603517082e-05}, {"id": 18, "seek": 7542, "start": 95.42, "end": 97.42, "text": " This is the way that they show it on the paper", "tokens": [639, 307, 264, 636, 300, 436, 855, 309, 322, 264, 3035], "temperature": 0.0, "avg_logprob": -0.2166276831375925, "compression_ratio": 1.5982532751091703, "no_speech_prob": 1.0952432603517082e-05}, {"id": 19, "seek": 7542, "start": 98.14, "end": 99.3, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.2166276831375925, "compression_ratio": 1.5982532751091703, "no_speech_prob": 1.0952432603517082e-05}, {"id": 20, "seek": 7542, "start": 99.3, "end": 103.82000000000001, "text": " For more information again links to two blog posts the one from", "tokens": [1171, 544, 1589, 797, 6123, 281, 732, 6968, 12300, 264, 472, 490], "temperature": 0.0, "avg_logprob": -0.2166276831375925, "compression_ratio": 1.5982532751091703, "no_speech_prob": 1.0952432603517082e-05}, {"id": 21, "seek": 10382, "start": 103.82, "end": 105.82, "text": " a Vitali", "tokens": [257, 48307, 72], "temperature": 0.0, "avg_logprob": -0.28614455154261637, "compression_ratio": 1.6473214285714286, "no_speech_prob": 1.3419515198620502e-05}, {"id": 22, "seek": 10382, "start": 106.3, "end": 108.3, "text": " about this topic and", "tokens": [466, 341, 4829, 293], "temperature": 0.0, "avg_logprob": -0.28614455154261637, "compression_ratio": 1.6473214285714286, "no_speech_prob": 1.3419515198620502e-05}, {"id": 23, "seek": 10382, "start": 108.66, "end": 111.02, "text": " And again an answer ha his", "tokens": [400, 797, 364, 1867, 324, 702], "temperature": 0.0, "avg_logprob": -0.28614455154261637, "compression_ratio": 1.6473214285714286, "no_speech_prob": 1.3419515198620502e-05}, {"id": 24, "seek": 10382, "start": 111.82, "end": 117.82, "text": " Blog post on this topic, so it's great to see that some of the work from fast AI students is already", "tokens": [46693, 2183, 322, 341, 4829, 11, 370, 309, 311, 869, 281, 536, 300, 512, 295, 264, 589, 490, 2370, 7318, 1731, 307, 1217], "temperature": 0.0, "avg_logprob": -0.28614455154261637, "compression_ratio": 1.6473214285714286, "no_speech_prob": 1.3419515198620502e-05}, {"id": 25, "seek": 10382, "start": 118.82, "end": 123.66, "text": " Getting noticed and picked up and shared and this blog post went on to get on the front page of Hacker News", "tokens": [13674, 5694, 293, 6183, 493, 293, 5507, 293, 341, 6968, 2183, 1437, 322, 281, 483, 322, 264, 1868, 3028, 295, 389, 23599, 7987], "temperature": 0.0, "avg_logprob": -0.28614455154261637, "compression_ratio": 1.6473214285714286, "no_speech_prob": 1.3419515198620502e-05}, {"id": 26, "seek": 10382, "start": 125.13999999999999, "end": 126.46, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.28614455154261637, "compression_ratio": 1.6473214285714286, "no_speech_prob": 1.3419515198620502e-05}, {"id": 27, "seek": 10382, "start": 126.46, "end": 131.94, "text": " That's pretty cool, and hopefully more and more of this work will be picked up once this is released", "tokens": [663, 311, 1238, 1627, 11, 293, 4696, 544, 293, 544, 295, 341, 589, 486, 312, 6183, 493, 1564, 341, 307, 4736], "temperature": 0.0, "avg_logprob": -0.28614455154261637, "compression_ratio": 1.6473214285714286, "no_speech_prob": 1.3419515198620502e-05}, {"id": 28, "seek": 13194, "start": 131.94, "end": 133.94, "text": " publicly", "tokens": [14843], "temperature": 0.0, "avg_logprob": -0.1982860191195619, "compression_ratio": 1.4388489208633093, "no_speech_prob": 5.771832547907252e-06}, {"id": 29, "seek": 13194, "start": 135.66, "end": 137.66, "text": " So last week we were", "tokens": [407, 1036, 1243, 321, 645], "temperature": 0.0, "avg_logprob": -0.1982860191195619, "compression_ratio": 1.4388489208633093, "no_speech_prob": 5.771832547907252e-06}, {"id": 30, "seek": 13194, "start": 139.34, "end": 142.22, "text": " kind of doing a deep dive into collaborative filtering and", "tokens": [733, 295, 884, 257, 2452, 9192, 666, 16555, 30822, 293], "temperature": 0.0, "avg_logprob": -0.1982860191195619, "compression_ratio": 1.4388489208633093, "no_speech_prob": 5.771832547907252e-06}, {"id": 31, "seek": 13194, "start": 145.74, "end": 149.1, "text": " Let's remind ourselves of kind of what our final model looked like", "tokens": [961, 311, 4160, 4175, 295, 733, 295, 437, 527, 2572, 2316, 2956, 411], "temperature": 0.0, "avg_logprob": -0.1982860191195619, "compression_ratio": 1.4388489208633093, "no_speech_prob": 5.771832547907252e-06}, {"id": 32, "seek": 13194, "start": 154.78, "end": 157.57999999999998, "text": " So in the end we kind of ended up rebuilding", "tokens": [407, 294, 264, 917, 321, 733, 295, 4590, 493, 36717], "temperature": 0.0, "avg_logprob": -0.1982860191195619, "compression_ratio": 1.4388489208633093, "no_speech_prob": 5.771832547907252e-06}, {"id": 33, "seek": 15758, "start": 157.58, "end": 159.58, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.19725154166997866, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.2377288334828336e-06}, {"id": 34, "seek": 15758, "start": 161.06, "end": 163.34, "text": " Model that's actually in the fast AI library", "tokens": [17105, 300, 311, 767, 294, 264, 2370, 7318, 6405], "temperature": 0.0, "avg_logprob": -0.19725154166997866, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.2377288334828336e-06}, {"id": 35, "seek": 15758, "start": 165.18, "end": 167.18, "text": " Where we had", "tokens": [2305, 321, 632], "temperature": 0.0, "avg_logprob": -0.19725154166997866, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.2377288334828336e-06}, {"id": 36, "seek": 15758, "start": 167.22000000000003, "end": 172.98000000000002, "text": " An embedding so we had this little get embedding function that grabbed an embedding and randomly", "tokens": [1107, 12240, 3584, 370, 321, 632, 341, 707, 483, 12240, 3584, 2445, 300, 18607, 364, 12240, 3584, 293, 16979], "temperature": 0.0, "avg_logprob": -0.19725154166997866, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.2377288334828336e-06}, {"id": 37, "seek": 15758, "start": 173.78, "end": 175.78, "text": " initialized the weights", "tokens": [5883, 1602, 264, 17443], "temperature": 0.0, "avg_logprob": -0.19725154166997866, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.2377288334828336e-06}, {"id": 38, "seek": 15758, "start": 176.06, "end": 178.06, "text": " for the users", "tokens": [337, 264, 5022], "temperature": 0.0, "avg_logprob": -0.19725154166997866, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.2377288334828336e-06}, {"id": 39, "seek": 15758, "start": 178.14000000000001, "end": 184.62, "text": " And for the items that's the kind of generic term in that case the items are movies and the bias for the users the bias for", "tokens": [400, 337, 264, 4754, 300, 311, 264, 733, 295, 19577, 1433, 294, 300, 1389, 264, 4754, 366, 6233, 293, 264, 12577, 337, 264, 5022, 264, 12577, 337], "temperature": 0.0, "avg_logprob": -0.19725154166997866, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.2377288334828336e-06}, {"id": 40, "seek": 15758, "start": 184.62, "end": 186.38000000000002, "text": " the items", "tokens": [264, 4754], "temperature": 0.0, "avg_logprob": -0.19725154166997866, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.2377288334828336e-06}, {"id": 41, "seek": 18638, "start": 186.38, "end": 188.38, "text": " And we had n factors", "tokens": [400, 321, 632, 297, 6771], "temperature": 0.0, "avg_logprob": -0.15589902877807618, "compression_ratio": 1.7112068965517242, "no_speech_prob": 2.642566641952726e-06}, {"id": 42, "seek": 18638, "start": 188.74, "end": 192.82, "text": " Embedding size for each for each one of course the biases just had a single one", "tokens": [24234, 292, 3584, 2744, 337, 1184, 337, 1184, 472, 295, 1164, 264, 32152, 445, 632, 257, 2167, 472], "temperature": 0.0, "avg_logprob": -0.15589902877807618, "compression_ratio": 1.7112068965517242, "no_speech_prob": 2.642566641952726e-06}, {"id": 43, "seek": 18638, "start": 193.46, "end": 196.78, "text": " And then we grabbed the users and item embeddings multiply them together", "tokens": [400, 550, 321, 18607, 264, 5022, 293, 3174, 12240, 29432, 12972, 552, 1214], "temperature": 0.0, "avg_logprob": -0.15589902877807618, "compression_ratio": 1.7112068965517242, "no_speech_prob": 2.642566641952726e-06}, {"id": 44, "seek": 18638, "start": 197.38, "end": 201.26, "text": " Summed it up for each row and added on the bias terms", "tokens": [8626, 1912, 309, 493, 337, 1184, 5386, 293, 3869, 322, 264, 12577, 2115], "temperature": 0.0, "avg_logprob": -0.15589902877807618, "compression_ratio": 1.7112068965517242, "no_speech_prob": 2.642566641952726e-06}, {"id": 45, "seek": 18638, "start": 202.26, "end": 208.62, "text": " Popped that through a sigmoid to put it into the range that we wanted so that was our model and", "tokens": [10215, 3452, 300, 807, 257, 4556, 3280, 327, 281, 829, 309, 666, 264, 3613, 300, 321, 1415, 370, 300, 390, 527, 2316, 293], "temperature": 0.0, "avg_logprob": -0.15589902877807618, "compression_ratio": 1.7112068965517242, "no_speech_prob": 2.642566641952726e-06}, {"id": 46, "seek": 20862, "start": 208.62, "end": 215.74, "text": " One of you asked if we can kind of interpret this information in some way", "tokens": [1485, 295, 291, 2351, 498, 321, 393, 733, 295, 7302, 341, 1589, 294, 512, 636], "temperature": 0.0, "avg_logprob": -0.2318040598993716, "compression_ratio": 1.5256410256410255, "no_speech_prob": 4.425456154422136e-06}, {"id": 47, "seek": 20862, "start": 215.74, "end": 221.78, "text": " And I promise this week we would see how to do that so let's take a look so we're going to start with the", "tokens": [400, 286, 6228, 341, 1243, 321, 576, 536, 577, 281, 360, 300, 370, 718, 311, 747, 257, 574, 370, 321, 434, 516, 281, 722, 365, 264], "temperature": 0.0, "avg_logprob": -0.2318040598993716, "compression_ratio": 1.5256410256410255, "no_speech_prob": 4.425456154422136e-06}, {"id": 48, "seek": 20862, "start": 222.82, "end": 226.14000000000001, "text": " Model we built here where we just used that fast AI library", "tokens": [17105, 321, 3094, 510, 689, 321, 445, 1143, 300, 2370, 7318, 6405], "temperature": 0.0, "avg_logprob": -0.2318040598993716, "compression_ratio": 1.5256410256410255, "no_speech_prob": 4.425456154422136e-06}, {"id": 49, "seek": 20862, "start": 226.58, "end": 234.62, "text": " Collab for all that data set from CSV and then that dot get learner, and then we fitted it in three epochs", "tokens": [4586, 455, 337, 439, 300, 1412, 992, 490, 48814, 293, 550, 300, 5893, 483, 33347, 11, 293, 550, 321, 26321, 309, 294, 1045, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.2318040598993716, "compression_ratio": 1.5256410256410255, "no_speech_prob": 4.425456154422136e-06}, {"id": 50, "seek": 20862, "start": 235.74, "end": 237.74, "text": " 19 seconds", "tokens": [1294, 3949], "temperature": 0.0, "avg_logprob": -0.2318040598993716, "compression_ratio": 1.5256410256410255, "no_speech_prob": 4.425456154422136e-06}, {"id": 51, "seek": 23774, "start": 237.74, "end": 239.74, "text": " We've got a pretty good result", "tokens": [492, 600, 658, 257, 1238, 665, 1874], "temperature": 0.0, "avg_logprob": -0.13484037592169945, "compression_ratio": 1.5679611650485437, "no_speech_prob": 4.356859790277667e-06}, {"id": 52, "seek": 23774, "start": 240.98000000000002, "end": 243.34, "text": " So what we can now do is to analyze", "tokens": [407, 437, 321, 393, 586, 360, 307, 281, 12477], "temperature": 0.0, "avg_logprob": -0.13484037592169945, "compression_ratio": 1.5679611650485437, "no_speech_prob": 4.356859790277667e-06}, {"id": 53, "seek": 23774, "start": 244.54000000000002, "end": 246.54000000000002, "text": " that model", "tokens": [300, 2316], "temperature": 0.0, "avg_logprob": -0.13484037592169945, "compression_ratio": 1.5679611650485437, "no_speech_prob": 4.356859790277667e-06}, {"id": 54, "seek": 23774, "start": 247.26000000000002, "end": 248.54000000000002, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.13484037592169945, "compression_ratio": 1.5679611650485437, "no_speech_prob": 4.356859790277667e-06}, {"id": 55, "seek": 23774, "start": 248.54000000000002, "end": 254.38, "text": " You may remember right back when we started we read in the movies dot CSV file", "tokens": [509, 815, 1604, 558, 646, 562, 321, 1409, 321, 1401, 294, 264, 6233, 5893, 48814, 3991], "temperature": 0.0, "avg_logprob": -0.13484037592169945, "compression_ratio": 1.5679611650485437, "no_speech_prob": 4.356859790277667e-06}, {"id": 56, "seek": 23774, "start": 254.98000000000002, "end": 259.1, "text": " But that's just a mapping from the ID of the movie to the name of the movie", "tokens": [583, 300, 311, 445, 257, 18350, 490, 264, 7348, 295, 264, 3169, 281, 264, 1315, 295, 264, 3169], "temperature": 0.0, "avg_logprob": -0.13484037592169945, "compression_ratio": 1.5679611650485437, "no_speech_prob": 4.356859790277667e-06}, {"id": 57, "seek": 23774, "start": 259.1, "end": 262.5, "text": " And so we're just going to use that for display purposes so we can see what we're doing", "tokens": [400, 370, 321, 434, 445, 516, 281, 764, 300, 337, 4674, 9932, 370, 321, 393, 536, 437, 321, 434, 884], "temperature": 0.0, "avg_logprob": -0.13484037592169945, "compression_ratio": 1.5679611650485437, "no_speech_prob": 4.356859790277667e-06}, {"id": 58, "seek": 26250, "start": 262.5, "end": 269.5, "text": " Because not all of us have watched every movie. I'm just going to limit this to the top 500", "tokens": [1436, 406, 439, 295, 505, 362, 6337, 633, 3169, 13, 286, 478, 445, 516, 281, 4948, 341, 281, 264, 1192, 5923], "temperature": 0.0, "avg_logprob": -0.253241947719029, "compression_ratio": 1.6244725738396624, "no_speech_prob": 1.844814391915861e-06}, {"id": 59, "seek": 26250, "start": 271.14, "end": 276.34, "text": " Most popular sorry 3,000 most popular movies, so we might have more chance of recognizing the movies. We're looking at", "tokens": [4534, 3743, 2597, 805, 11, 1360, 881, 3743, 6233, 11, 370, 321, 1062, 362, 544, 2931, 295, 18538, 264, 6233, 13, 492, 434, 1237, 412], "temperature": 0.0, "avg_logprob": -0.253241947719029, "compression_ratio": 1.6244725738396624, "no_speech_prob": 1.844814391915861e-06}, {"id": 60, "seek": 26250, "start": 277.14, "end": 279.14, "text": " and then I'll go ahead and", "tokens": [293, 550, 286, 603, 352, 2286, 293], "temperature": 0.0, "avg_logprob": -0.253241947719029, "compression_ratio": 1.6244725738396624, "no_speech_prob": 1.844814391915861e-06}, {"id": 61, "seek": 26250, "start": 279.18, "end": 287.12, "text": " Change it from the movie IDs from movie lens to those unique IDs that we're using the contiguous IDs", "tokens": [15060, 309, 490, 264, 3169, 48212, 490, 3169, 6765, 281, 729, 3845, 48212, 300, 321, 434, 1228, 264, 660, 30525, 48212], "temperature": 0.0, "avg_logprob": -0.253241947719029, "compression_ratio": 1.6244725738396624, "no_speech_prob": 1.844814391915861e-06}, {"id": 62, "seek": 26250, "start": 287.12, "end": 289.22, "text": " Because that's what our model has all right", "tokens": [1436, 300, 311, 437, 527, 2316, 575, 439, 558], "temperature": 0.0, "avg_logprob": -0.253241947719029, "compression_ratio": 1.6244725738396624, "no_speech_prob": 1.844814391915861e-06}, {"id": 63, "seek": 26250, "start": 289.94, "end": 291.94, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.253241947719029, "compression_ratio": 1.6244725738396624, "no_speech_prob": 1.844814391915861e-06}, {"id": 64, "seek": 29194, "start": 291.94, "end": 293.94, "text": " inside", "tokens": [1854], "temperature": 0.0, "avg_logprob": -0.20882136385205766, "compression_ratio": 1.528735632183908, "no_speech_prob": 1.8448181435815059e-06}, {"id": 65, "seek": 29194, "start": 294.5, "end": 296.5, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.20882136385205766, "compression_ratio": 1.528735632183908, "no_speech_prob": 1.8448181435815059e-06}, {"id": 66, "seek": 29194, "start": 296.86, "end": 299.62, "text": " Learn object that we create inside a learner", "tokens": [17216, 2657, 300, 321, 1884, 1854, 257, 33347], "temperature": 0.0, "avg_logprob": -0.20882136385205766, "compression_ratio": 1.528735632183908, "no_speech_prob": 1.8448181435815059e-06}, {"id": 67, "seek": 29194, "start": 300.3, "end": 306.06, "text": " We can always grab the pie torch model itself just by saying learn dot model", "tokens": [492, 393, 1009, 4444, 264, 1730, 27822, 2316, 2564, 445, 538, 1566, 1466, 5893, 2316], "temperature": 0.0, "avg_logprob": -0.20882136385205766, "compression_ratio": 1.528735632183908, "no_speech_prob": 1.8448181435815059e-06}, {"id": 68, "seek": 29194, "start": 307.18, "end": 310.86, "text": " Right and like I'm going to kind of show you more and more of the code", "tokens": [1779, 293, 411, 286, 478, 516, 281, 733, 295, 855, 291, 544, 293, 544, 295, 264, 3089], "temperature": 0.0, "avg_logprob": -0.20882136385205766, "compression_ratio": 1.528735632183908, "no_speech_prob": 1.8448181435815059e-06}, {"id": 69, "seek": 29194, "start": 311.78, "end": 315.34, "text": " At the moment, so let's take a look at the definition of model", "tokens": [1711, 264, 1623, 11, 370, 718, 311, 747, 257, 574, 412, 264, 7123, 295, 2316], "temperature": 0.0, "avg_logprob": -0.20882136385205766, "compression_ratio": 1.528735632183908, "no_speech_prob": 1.8448181435815059e-06}, {"id": 70, "seek": 31534, "start": 315.34, "end": 319.97999999999996, "text": " And so model is a property", "tokens": [400, 370, 2316, 307, 257, 4707], "temperature": 0.0, "avg_logprob": -0.13805863731785825, "compression_ratio": 1.792, "no_speech_prob": 5.122890343045583e-07}, {"id": 71, "seek": 31534, "start": 319.97999999999996, "end": 325.41999999999996, "text": " So if you haven't seen a property before a property is just something in Python which looks like a method", "tokens": [407, 498, 291, 2378, 380, 1612, 257, 4707, 949, 257, 4707, 307, 445, 746, 294, 15329, 597, 1542, 411, 257, 3170], "temperature": 0.0, "avg_logprob": -0.13805863731785825, "compression_ratio": 1.792, "no_speech_prob": 5.122890343045583e-07}, {"id": 72, "seek": 31534, "start": 326.38, "end": 329.34, "text": " When you define it that you can call it without", "tokens": [1133, 291, 6964, 309, 300, 291, 393, 818, 309, 1553], "temperature": 0.0, "avg_logprob": -0.13805863731785825, "compression_ratio": 1.792, "no_speech_prob": 5.122890343045583e-07}, {"id": 73, "seek": 31534, "start": 329.97999999999996, "end": 335.73999999999995, "text": " Parentheses as we do here right and so it kind of looks when you call it like it's a regular attribute", "tokens": [430, 20616, 23639, 382, 321, 360, 510, 558, 293, 370, 309, 733, 295, 1542, 562, 291, 818, 309, 411, 309, 311, 257, 3890, 19667], "temperature": 0.0, "avg_logprob": -0.13805863731785825, "compression_ratio": 1.792, "no_speech_prob": 5.122890343045583e-07}, {"id": 74, "seek": 31534, "start": 335.73999999999995, "end": 341.26, "text": " But it looks like when you define it like it's a method so every time you call it it actually runs this code", "tokens": [583, 309, 1542, 411, 562, 291, 6964, 309, 411, 309, 311, 257, 3170, 370, 633, 565, 291, 818, 309, 309, 767, 6676, 341, 3089], "temperature": 0.0, "avg_logprob": -0.13805863731785825, "compression_ratio": 1.792, "no_speech_prob": 5.122890343045583e-07}, {"id": 75, "seek": 31534, "start": 341.38, "end": 344.35999999999996, "text": " Okay, and so in this case. It's just a shortcut to grab", "tokens": [1033, 11, 293, 370, 294, 341, 1389, 13, 467, 311, 445, 257, 24822, 281, 4444], "temperature": 0.0, "avg_logprob": -0.13805863731785825, "compression_ratio": 1.792, "no_speech_prob": 5.122890343045583e-07}, {"id": 76, "seek": 34436, "start": 344.36, "end": 347.32, "text": " something called dot models dot model", "tokens": [746, 1219, 5893, 5245, 5893, 2316], "temperature": 0.0, "avg_logprob": -0.22902534484863282, "compression_ratio": 1.5747126436781609, "no_speech_prob": 1.8448200762577471e-06}, {"id": 77, "seek": 34436, "start": 348.24, "end": 350.24, "text": " So you may be interested to know what that looks like", "tokens": [407, 291, 815, 312, 3102, 281, 458, 437, 300, 1542, 411], "temperature": 0.0, "avg_logprob": -0.22902534484863282, "compression_ratio": 1.5747126436781609, "no_speech_prob": 1.8448200762577471e-06}, {"id": 78, "seek": 34436, "start": 351.0, "end": 353.0, "text": " learn dot models", "tokens": [1466, 5893, 5245], "temperature": 0.0, "avg_logprob": -0.22902534484863282, "compression_ratio": 1.5747126436781609, "no_speech_prob": 1.8448200762577471e-06}, {"id": 79, "seek": 34436, "start": 353.40000000000003, "end": 355.40000000000003, "text": " and so this is there's a", "tokens": [293, 370, 341, 307, 456, 311, 257], "temperature": 0.0, "avg_logprob": -0.22902534484863282, "compression_ratio": 1.5747126436781609, "no_speech_prob": 1.8448200762577471e-06}, {"id": 80, "seek": 34436, "start": 356.52000000000004, "end": 358.32, "text": " the fast AI", "tokens": [264, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.22902534484863282, "compression_ratio": 1.5747126436781609, "no_speech_prob": 1.8448200762577471e-06}, {"id": 81, "seek": 34436, "start": 358.32, "end": 364.28000000000003, "text": " Model type is a very thin wrapper for pie torch models, so we could take a look at this", "tokens": [17105, 2010, 307, 257, 588, 5862, 46906, 337, 1730, 27822, 5245, 11, 370, 321, 727, 747, 257, 574, 412, 341], "temperature": 0.0, "avg_logprob": -0.22902534484863282, "compression_ratio": 1.5747126436781609, "no_speech_prob": 1.8448200762577471e-06}, {"id": 82, "seek": 34436, "start": 365.36, "end": 367.36, "text": " Collab filter model and see what that is", "tokens": [4586, 455, 6608, 2316, 293, 536, 437, 300, 307], "temperature": 0.0, "avg_logprob": -0.22902534484863282, "compression_ratio": 1.5747126436781609, "no_speech_prob": 1.8448200762577471e-06}, {"id": 83, "seek": 36736, "start": 367.36, "end": 369.36, "text": " It's", "tokens": [467, 311], "temperature": 0.0, "avg_logprob": -0.20023719469706217, "compression_ratio": 1.7296137339055795, "no_speech_prob": 1.994714011743781e-06}, {"id": 84, "seek": 36736, "start": 373.04, "end": 375.72, "text": " Only one line of code okay, and", "tokens": [5686, 472, 1622, 295, 3089, 1392, 11, 293], "temperature": 0.0, "avg_logprob": -0.20023719469706217, "compression_ratio": 1.7296137339055795, "no_speech_prob": 1.994714011743781e-06}, {"id": 85, "seek": 36736, "start": 378.40000000000003, "end": 385.16, "text": " Yeah, we'll talk more about these in part two right but basically that this is this very thin wrapper and the main thing one of", "tokens": [865, 11, 321, 603, 751, 544, 466, 613, 294, 644, 732, 558, 457, 1936, 300, 341, 307, 341, 588, 5862, 46906, 293, 264, 2135, 551, 472, 295], "temperature": 0.0, "avg_logprob": -0.20023719469706217, "compression_ratio": 1.7296137339055795, "no_speech_prob": 1.994714011743781e-06}, {"id": 86, "seek": 36736, "start": 385.16, "end": 390.04, "text": " The main things that fast AI does is we have this concept of layer groups where basically when you say here", "tokens": [440, 2135, 721, 300, 2370, 7318, 775, 307, 321, 362, 341, 3410, 295, 4583, 3935, 689, 1936, 562, 291, 584, 510], "temperature": 0.0, "avg_logprob": -0.20023719469706217, "compression_ratio": 1.7296137339055795, "no_speech_prob": 1.994714011743781e-06}, {"id": 87, "seek": 36736, "start": 390.04, "end": 395.76, "text": " There are different learning rates, and they get applied to different sets of layers, and that's something that's not in pie torch", "tokens": [821, 366, 819, 2539, 6846, 11, 293, 436, 483, 6456, 281, 819, 6352, 295, 7914, 11, 293, 300, 311, 746, 300, 311, 406, 294, 1730, 27822], "temperature": 0.0, "avg_logprob": -0.20023719469706217, "compression_ratio": 1.7296137339055795, "no_speech_prob": 1.994714011743781e-06}, {"id": 88, "seek": 39576, "start": 395.76, "end": 399.03999999999996, "text": " So when you say I want to use this pie torch model", "tokens": [407, 562, 291, 584, 286, 528, 281, 764, 341, 1730, 27822, 2316], "temperature": 0.0, "avg_logprob": -0.2121751866442092, "compression_ratio": 1.708695652173913, "no_speech_prob": 4.029433057439746e-06}, {"id": 89, "seek": 39576, "start": 399.59999999999997, "end": 403.15999999999997, "text": " This was one thing we have to do which is to say like I came in around layer groups", "tokens": [639, 390, 472, 551, 321, 362, 281, 360, 597, 307, 281, 584, 411, 286, 1361, 294, 926, 4583, 3935], "temperature": 0.0, "avg_logprob": -0.2121751866442092, "compression_ratio": 1.708695652173913, "no_speech_prob": 4.029433057439746e-06}, {"id": 90, "seek": 39576, "start": 403.15999999999997, "end": 405.24, "text": " Okay, so the details aren't terribly important", "tokens": [1033, 11, 370, 264, 4365, 3212, 380, 22903, 1021], "temperature": 0.0, "avg_logprob": -0.2121751866442092, "compression_ratio": 1.708695652173913, "no_speech_prob": 4.029433057439746e-06}, {"id": 91, "seek": 39576, "start": 406.03999999999996, "end": 412.32, "text": " But in general if you want to create a little wrapper for some other pie torch model you could just write something like this", "tokens": [583, 294, 2674, 498, 291, 528, 281, 1884, 257, 707, 46906, 337, 512, 661, 1730, 27822, 2316, 291, 727, 445, 2464, 746, 411, 341], "temperature": 0.0, "avg_logprob": -0.2121751866442092, "compression_ratio": 1.708695652173913, "no_speech_prob": 4.029433057439746e-06}, {"id": 92, "seek": 39576, "start": 415.15999999999997, "end": 421.03999999999996, "text": " So to get to get inside that to grab the actual pie torch model itself its models dot", "tokens": [407, 281, 483, 281, 483, 1854, 300, 281, 4444, 264, 3539, 1730, 27822, 2316, 2564, 1080, 5245, 5893], "temperature": 0.0, "avg_logprob": -0.2121751866442092, "compression_ratio": 1.708695652173913, "no_speech_prob": 4.029433057439746e-06}, {"id": 93, "seek": 42104, "start": 421.04, "end": 427.36, "text": " Model that's the pie torch model and then the learn object has a shortcut to that okay?", "tokens": [17105, 300, 311, 264, 1730, 27822, 2316, 293, 550, 264, 1466, 2657, 575, 257, 24822, 281, 300, 1392, 30], "temperature": 0.0, "avg_logprob": -0.16996813793571627, "compression_ratio": 1.751131221719457, "no_speech_prob": 2.68418489213218e-06}, {"id": 94, "seek": 42104, "start": 427.36, "end": 433.12, "text": " So we're going to set M to be the pie torch model and so when you", "tokens": [407, 321, 434, 516, 281, 992, 376, 281, 312, 264, 1730, 27822, 2316, 293, 370, 562, 291], "temperature": 0.0, "avg_logprob": -0.16996813793571627, "compression_ratio": 1.751131221719457, "no_speech_prob": 2.68418489213218e-06}, {"id": 95, "seek": 42104, "start": 433.64000000000004, "end": 436.0, "text": " Print out a pie torch model it prints it out", "tokens": [34439, 484, 257, 1730, 27822, 2316, 309, 22305, 309, 484], "temperature": 0.0, "avg_logprob": -0.16996813793571627, "compression_ratio": 1.751131221719457, "no_speech_prob": 2.68418489213218e-06}, {"id": 96, "seek": 42104, "start": 436.48, "end": 442.64000000000004, "text": " Basically by listing out all of the layers that you created in the constructor", "tokens": [8537, 538, 22161, 484, 439, 295, 264, 7914, 300, 291, 2942, 294, 264, 47479], "temperature": 0.0, "avg_logprob": -0.16996813793571627, "compression_ratio": 1.751131221719457, "no_speech_prob": 2.68418489213218e-06}, {"id": 97, "seek": 42104, "start": 443.04, "end": 449.08000000000004, "text": " It's quite. It's quite nifty actually when you kind of think about the way this works. Thanks to kind of some", "tokens": [467, 311, 1596, 13, 467, 311, 1596, 297, 37177, 767, 562, 291, 733, 295, 519, 466, 264, 636, 341, 1985, 13, 2561, 281, 733, 295, 512], "temperature": 0.0, "avg_logprob": -0.16996813793571627, "compression_ratio": 1.751131221719457, "no_speech_prob": 2.68418489213218e-06}, {"id": 98, "seek": 44908, "start": 449.08, "end": 450.28, "text": " very", "tokens": [588], "temperature": 0.0, "avg_logprob": -0.1851396047940818, "compression_ratio": 1.689795918367347, "no_speech_prob": 1.101592147279007e-06}, {"id": 99, "seek": 44908, "start": 450.28, "end": 456.59999999999997, "text": " Handy stuff in python we're actually able to use standard python. Oh to kind of define", "tokens": [47006, 1507, 294, 38797, 321, 434, 767, 1075, 281, 764, 3832, 38797, 13, 876, 281, 733, 295, 6964], "temperature": 0.0, "avg_logprob": -0.1851396047940818, "compression_ratio": 1.689795918367347, "no_speech_prob": 1.101592147279007e-06}, {"id": 100, "seek": 44908, "start": 458.08, "end": 466.68, "text": " These modules and these layers and they basically automatically kind of register themselves with pie torch so back in our embedding dot bias", "tokens": [1981, 16679, 293, 613, 7914, 293, 436, 1936, 6772, 733, 295, 7280, 2969, 365, 1730, 27822, 370, 646, 294, 527, 12240, 3584, 5893, 12577], "temperature": 0.0, "avg_logprob": -0.1851396047940818, "compression_ratio": 1.689795918367347, "no_speech_prob": 1.101592147279007e-06}, {"id": 101, "seek": 44908, "start": 467.56, "end": 473.4, "text": " We just had a bunch of things where we said okay each of these things are equal to these things", "tokens": [492, 445, 632, 257, 3840, 295, 721, 689, 321, 848, 1392, 1184, 295, 613, 721, 366, 2681, 281, 613, 721], "temperature": 0.0, "avg_logprob": -0.1851396047940818, "compression_ratio": 1.689795918367347, "no_speech_prob": 1.101592147279007e-06}, {"id": 102, "seek": 44908, "start": 473.4, "end": 476.0, "text": " And then it automatically knows how to represent that", "tokens": [400, 550, 309, 6772, 3255, 577, 281, 2906, 300], "temperature": 0.0, "avg_logprob": -0.1851396047940818, "compression_ratio": 1.689795918367347, "no_speech_prob": 1.101592147279007e-06}, {"id": 103, "seek": 47600, "start": 476.0, "end": 482.62, "text": " So you can see there's the name is you and so the name is just literally whatever we called it", "tokens": [407, 291, 393, 536, 456, 311, 264, 1315, 307, 291, 293, 370, 264, 1315, 307, 445, 3736, 2035, 321, 1219, 309], "temperature": 0.0, "avg_logprob": -0.18806559698922293, "compression_ratio": 1.5775401069518717, "no_speech_prob": 3.927856937480101e-07}, {"id": 104, "seek": 47600, "start": 482.84, "end": 484.84, "text": " Yeah, you right", "tokens": [865, 11, 291, 558], "temperature": 0.0, "avg_logprob": -0.18806559698922293, "compression_ratio": 1.5775401069518717, "no_speech_prob": 3.927856937480101e-07}, {"id": 105, "seek": 47600, "start": 485.04, "end": 489.44, "text": " And then the definition is it's this kind of layer, okay?", "tokens": [400, 550, 264, 7123, 307, 309, 311, 341, 733, 295, 4583, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.18806559698922293, "compression_ratio": 1.5775401069518717, "no_speech_prob": 3.927856937480101e-07}, {"id": 106, "seek": 47600, "start": 490.52, "end": 492.44, "text": " So that's our", "tokens": [407, 300, 311, 527], "temperature": 0.0, "avg_logprob": -0.18806559698922293, "compression_ratio": 1.5775401069518717, "no_speech_prob": 3.927856937480101e-07}, {"id": 107, "seek": 47600, "start": 492.44, "end": 494.36, "text": " pie torch model", "tokens": [1730, 27822, 2316], "temperature": 0.0, "avg_logprob": -0.18806559698922293, "compression_ratio": 1.5775401069518717, "no_speech_prob": 3.927856937480101e-07}, {"id": 108, "seek": 47600, "start": 494.36, "end": 496.36, "text": " so we can", "tokens": [370, 321, 393], "temperature": 0.0, "avg_logprob": -0.18806559698922293, "compression_ratio": 1.5775401069518717, "no_speech_prob": 3.927856937480101e-07}, {"id": 109, "seek": 47600, "start": 497.36, "end": 504.04, "text": " Look inside that basically use that so if we say m dot ib then that's referring to the", "tokens": [2053, 1854, 300, 1936, 764, 300, 370, 498, 321, 584, 275, 5893, 39073, 550, 300, 311, 13761, 281, 264], "temperature": 0.0, "avg_logprob": -0.18806559698922293, "compression_ratio": 1.5775401069518717, "no_speech_prob": 3.927856937480101e-07}, {"id": 110, "seek": 50404, "start": 504.04, "end": 505.16, "text": " m dot ib", "tokens": [275, 5893, 39073], "temperature": 0.0, "avg_logprob": -0.27593990287395437, "compression_ratio": 1.7033492822966507, "no_speech_prob": 6.681509034933697e-07}, {"id": 111, "seek": 50404, "start": 505.16, "end": 506.40000000000003, "text": " embedding layer", "tokens": [12240, 3584, 4583], "temperature": 0.0, "avg_logprob": -0.27593990287395437, "compression_ratio": 1.7033492822966507, "no_speech_prob": 6.681509034933697e-07}, {"id": 112, "seek": 50404, "start": 506.40000000000003, "end": 512.28, "text": " For an item which is the bias layer so an item bias in this case is the movie bias", "tokens": [1171, 364, 3174, 597, 307, 264, 12577, 4583, 370, 364, 3174, 12577, 294, 341, 1389, 307, 264, 3169, 12577], "temperature": 0.0, "avg_logprob": -0.27593990287395437, "compression_ratio": 1.7033492822966507, "no_speech_prob": 6.681509034933697e-07}, {"id": 113, "seek": 50404, "start": 512.52, "end": 518.1800000000001, "text": " So each movie there are 9,000 of them has a single bias element", "tokens": [407, 1184, 3169, 456, 366, 1722, 11, 1360, 295, 552, 575, 257, 2167, 12577, 4478], "temperature": 0.0, "avg_logprob": -0.27593990287395437, "compression_ratio": 1.7033492822966507, "no_speech_prob": 6.681509034933697e-07}, {"id": 114, "seek": 50404, "start": 519.16, "end": 521.96, "text": " Now the really nice thing about pie torch", "tokens": [823, 264, 534, 1481, 551, 466, 1730, 27822], "temperature": 0.0, "avg_logprob": -0.27593990287395437, "compression_ratio": 1.7033492822966507, "no_speech_prob": 6.681509034933697e-07}, {"id": 115, "seek": 50404, "start": 522.72, "end": 524.64, "text": " layers and", "tokens": [7914, 293], "temperature": 0.0, "avg_logprob": -0.27593990287395437, "compression_ratio": 1.7033492822966507, "no_speech_prob": 6.681509034933697e-07}, {"id": 116, "seek": 50404, "start": 524.64, "end": 531.0, "text": " Models is that they all look the same they basically cut to use them you call them as if they were a function", "tokens": [6583, 1625, 307, 300, 436, 439, 574, 264, 912, 436, 1936, 1723, 281, 764, 552, 291, 818, 552, 382, 498, 436, 645, 257, 2445], "temperature": 0.0, "avg_logprob": -0.27593990287395437, "compression_ratio": 1.7033492822966507, "no_speech_prob": 6.681509034933697e-07}, {"id": 117, "seek": 50404, "start": 531.04, "end": 533.04, "text": " So we can go m dot ib", "tokens": [407, 321, 393, 352, 275, 5893, 39073], "temperature": 0.0, "avg_logprob": -0.27593990287395437, "compression_ratio": 1.7033492822966507, "no_speech_prob": 6.681509034933697e-07}, {"id": 118, "seek": 53304, "start": 533.04, "end": 534.68, "text": " parentheses", "tokens": [34153], "temperature": 0.0, "avg_logprob": -0.20484531626981847, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.0845142242033035e-06}, {"id": 119, "seek": 53304, "start": 534.68, "end": 537.8399999999999, "text": " Right and that basically says I want you to return", "tokens": [1779, 293, 300, 1936, 1619, 286, 528, 291, 281, 2736], "temperature": 0.0, "avg_logprob": -0.20484531626981847, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.0845142242033035e-06}, {"id": 120, "seek": 53304, "start": 538.24, "end": 544.64, "text": " The value of that layer and that layer could be a full-on model right so to actually", "tokens": [440, 2158, 295, 300, 4583, 293, 300, 4583, 727, 312, 257, 1577, 12, 266, 2316, 558, 370, 281, 767], "temperature": 0.0, "avg_logprob": -0.20484531626981847, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.0845142242033035e-06}, {"id": 121, "seek": 53304, "start": 545.3199999999999, "end": 550.52, "text": " Get a prediction from a pie torch model. You just I would go M and pass in my variable", "tokens": [3240, 257, 17630, 490, 257, 1730, 27822, 2316, 13, 509, 445, 286, 576, 352, 376, 293, 1320, 294, 452, 7006], "temperature": 0.0, "avg_logprob": -0.20484531626981847, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.0845142242033035e-06}, {"id": 122, "seek": 53304, "start": 551.28, "end": 558.76, "text": " Okay, and so in this case m dot ib and pass in my top movie indexes", "tokens": [1033, 11, 293, 370, 294, 341, 1389, 275, 5893, 39073, 293, 1320, 294, 452, 1192, 3169, 8186, 279], "temperature": 0.0, "avg_logprob": -0.20484531626981847, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.0845142242033035e-06}, {"id": 123, "seek": 53304, "start": 559.4399999999999, "end": 561.4399999999999, "text": " now models remember", "tokens": [586, 5245, 1604], "temperature": 0.0, "avg_logprob": -0.20484531626981847, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.0845142242033035e-06}, {"id": 124, "seek": 56144, "start": 561.44, "end": 562.96, "text": " layers", "tokens": [7914], "temperature": 0.0, "avg_logprob": -0.19835881152784968, "compression_ratio": 1.5242718446601942, "no_speech_prob": 8.315267336911347e-07}, {"id": 125, "seek": 56144, "start": 562.96, "end": 564.6800000000001, "text": " They require", "tokens": [814, 3651], "temperature": 0.0, "avg_logprob": -0.19835881152784968, "compression_ratio": 1.5242718446601942, "no_speech_prob": 8.315267336911347e-07}, {"id": 126, "seek": 56144, "start": 564.6800000000001, "end": 565.8800000000001, "text": " variables", "tokens": [9102], "temperature": 0.0, "avg_logprob": -0.19835881152784968, "compression_ratio": 1.5242718446601942, "no_speech_prob": 8.315267336911347e-07}, {"id": 127, "seek": 56144, "start": 565.8800000000001, "end": 569.6, "text": " Not tensors because it needs to keep track of the derivatives", "tokens": [1726, 10688, 830, 570, 309, 2203, 281, 1066, 2837, 295, 264, 33733], "temperature": 0.0, "avg_logprob": -0.19835881152784968, "compression_ratio": 1.5242718446601942, "no_speech_prob": 8.315267336911347e-07}, {"id": 128, "seek": 56144, "start": 570.0400000000001, "end": 575.12, "text": " Okay, and so we use this capital V to turn the tensor into a variable", "tokens": [1033, 11, 293, 370, 321, 764, 341, 4238, 691, 281, 1261, 264, 40863, 666, 257, 7006], "temperature": 0.0, "avg_logprob": -0.19835881152784968, "compression_ratio": 1.5242718446601942, "no_speech_prob": 8.315267336911347e-07}, {"id": 129, "seek": 56144, "start": 576.24, "end": 579.7600000000001, "text": " And was just announced this week that pie torch", "tokens": [400, 390, 445, 7548, 341, 1243, 300, 1730, 27822], "temperature": 0.0, "avg_logprob": -0.19835881152784968, "compression_ratio": 1.5242718446601942, "no_speech_prob": 8.315267336911347e-07}, {"id": 130, "seek": 56144, "start": 580.48, "end": 581.44, "text": " 0.4", "tokens": [1958, 13, 19], "temperature": 0.0, "avg_logprob": -0.19835881152784968, "compression_ratio": 1.5242718446601942, "no_speech_prob": 8.315267336911347e-07}, {"id": 131, "seek": 56144, "start": 581.44, "end": 587.1, "text": " Which is the version after the one that's just about to be released is going to get rid of variables", "tokens": [3013, 307, 264, 3037, 934, 264, 472, 300, 311, 445, 466, 281, 312, 4736, 307, 516, 281, 483, 3973, 295, 9102], "temperature": 0.0, "avg_logprob": -0.19835881152784968, "compression_ratio": 1.5242718446601942, "no_speech_prob": 8.315267336911347e-07}, {"id": 132, "seek": 58710, "start": 587.1, "end": 592.1, "text": " And we'll actually be able to use tensors directly to keep track of derivatives", "tokens": [400, 321, 603, 767, 312, 1075, 281, 764, 10688, 830, 3838, 281, 1066, 2837, 295, 33733], "temperature": 0.0, "avg_logprob": -0.16558930853835674, "compression_ratio": 1.6883561643835616, "no_speech_prob": 2.902296046158881e-06}, {"id": 133, "seek": 58710, "start": 592.1, "end": 598.0600000000001, "text": " So if you're watching this on the MOOC and you're looking at point four then you'll probably notice that the code doesn't have this", "tokens": [407, 498, 291, 434, 1976, 341, 322, 264, 49197, 34, 293, 291, 434, 1237, 412, 935, 1451, 550, 291, 603, 1391, 3449, 300, 264, 3089, 1177, 380, 362, 341], "temperature": 0.0, "avg_logprob": -0.16558930853835674, "compression_ratio": 1.6883561643835616, "no_speech_prob": 2.902296046158881e-06}, {"id": 134, "seek": 58710, "start": 598.0600000000001, "end": 600.0600000000001, "text": " V in it anymore", "tokens": [691, 294, 309, 3602], "temperature": 0.0, "avg_logprob": -0.16558930853835674, "compression_ratio": 1.6883561643835616, "no_speech_prob": 2.902296046158881e-06}, {"id": 135, "seek": 58710, "start": 600.46, "end": 602.96, "text": " So that would be pretty exciting when that happens", "tokens": [407, 300, 576, 312, 1238, 4670, 562, 300, 2314], "temperature": 0.0, "avg_logprob": -0.16558930853835674, "compression_ratio": 1.6883561643835616, "no_speech_prob": 2.902296046158881e-06}, {"id": 136, "seek": 58710, "start": 602.96, "end": 607.82, "text": " but for now we have to remember if we're going to pass something into a model to turn it into a variable first and", "tokens": [457, 337, 586, 321, 362, 281, 1604, 498, 321, 434, 516, 281, 1320, 746, 666, 257, 2316, 281, 1261, 309, 666, 257, 7006, 700, 293], "temperature": 0.0, "avg_logprob": -0.16558930853835674, "compression_ratio": 1.6883561643835616, "no_speech_prob": 2.902296046158881e-06}, {"id": 137, "seek": 58710, "start": 608.0600000000001, "end": 614.58, "text": " Remember a variable has a strict superset of the API of a tensor so anything you can do to a tensor", "tokens": [5459, 257, 7006, 575, 257, 10910, 37906, 302, 295, 264, 9362, 295, 257, 40863, 370, 1340, 291, 393, 360, 281, 257, 40863], "temperature": 0.0, "avg_logprob": -0.16558930853835674, "compression_ratio": 1.6883561643835616, "no_speech_prob": 2.902296046158881e-06}, {"id": 138, "seek": 61458, "start": 614.58, "end": 619.14, "text": " They can do to a variable like add it up or take its log or whatever okay?", "tokens": [814, 393, 360, 281, 257, 7006, 411, 909, 309, 493, 420, 747, 1080, 3565, 420, 2035, 1392, 30], "temperature": 0.0, "avg_logprob": -0.1878022914979516, "compression_ratio": 1.5947368421052632, "no_speech_prob": 5.626390020552208e-07}, {"id": 139, "seek": 61458, "start": 619.6600000000001, "end": 625.46, "text": " So that's going to return a variable which consists of going through each of these movie IDs", "tokens": [407, 300, 311, 516, 281, 2736, 257, 7006, 597, 14689, 295, 516, 807, 1184, 295, 613, 3169, 48212], "temperature": 0.0, "avg_logprob": -0.1878022914979516, "compression_ratio": 1.5947368421052632, "no_speech_prob": 5.626390020552208e-07}, {"id": 140, "seek": 61458, "start": 626.0200000000001, "end": 629.82, "text": " Putting it through this embedding layer to get its bias", "tokens": [31367, 309, 807, 341, 12240, 3584, 4583, 281, 483, 1080, 12577], "temperature": 0.0, "avg_logprob": -0.1878022914979516, "compression_ratio": 1.5947368421052632, "no_speech_prob": 5.626390020552208e-07}, {"id": 141, "seek": 61458, "start": 630.26, "end": 632.5400000000001, "text": " Okay, and that's going to return a", "tokens": [1033, 11, 293, 300, 311, 516, 281, 2736, 257], "temperature": 0.0, "avg_logprob": -0.1878022914979516, "compression_ratio": 1.5947368421052632, "no_speech_prob": 5.626390020552208e-07}, {"id": 142, "seek": 61458, "start": 634.14, "end": 635.94, "text": " Variable", "tokens": [32511, 712], "temperature": 0.0, "avg_logprob": -0.1878022914979516, "compression_ratio": 1.5947368421052632, "no_speech_prob": 5.626390020552208e-07}, {"id": 143, "seek": 61458, "start": 635.94, "end": 637.94, "text": " Let's take a look", "tokens": [961, 311, 747, 257, 574], "temperature": 0.0, "avg_logprob": -0.1878022914979516, "compression_ratio": 1.5947368421052632, "no_speech_prob": 5.626390020552208e-07}, {"id": 144, "seek": 63794, "start": 637.94, "end": 643.86, "text": " So before I press", "tokens": [407, 949, 286, 1886], "temperature": 0.0, "avg_logprob": -0.18886579020639485, "compression_ratio": 1.5700934579439252, "no_speech_prob": 9.874619308902766e-07}, {"id": 145, "seek": 63794, "start": 645.3000000000001, "end": 650.98, "text": " Shift enter here. You can have a think about what I'm going to have I've got a list of 3,000 movies going in", "tokens": [28304, 3242, 510, 13, 509, 393, 362, 257, 519, 466, 437, 286, 478, 516, 281, 362, 286, 600, 658, 257, 1329, 295, 805, 11, 1360, 6233, 516, 294], "temperature": 0.0, "avg_logprob": -0.18886579020639485, "compression_ratio": 1.5700934579439252, "no_speech_prob": 9.874619308902766e-07}, {"id": 146, "seek": 63794, "start": 651.74, "end": 654.1400000000001, "text": " Turning into a variable putting it through this embedding layer", "tokens": [39660, 666, 257, 7006, 3372, 309, 807, 341, 12240, 3584, 4583], "temperature": 0.0, "avg_logprob": -0.18886579020639485, "compression_ratio": 1.5700934579439252, "no_speech_prob": 9.874619308902766e-07}, {"id": 147, "seek": 63794, "start": 655.1400000000001, "end": 657.3800000000001, "text": " So just have a think about what you expect to come out", "tokens": [407, 445, 362, 257, 519, 466, 437, 291, 2066, 281, 808, 484], "temperature": 0.0, "avg_logprob": -0.18886579020639485, "compression_ratio": 1.5700934579439252, "no_speech_prob": 9.874619308902766e-07}, {"id": 148, "seek": 63794, "start": 658.82, "end": 664.98, "text": " Okay, and we have a variable of size 3,000 by 1 hopefully that doesn't surprise you we had", "tokens": [1033, 11, 293, 321, 362, 257, 7006, 295, 2744, 805, 11, 1360, 538, 502, 4696, 300, 1177, 380, 6365, 291, 321, 632], "temperature": 0.0, "avg_logprob": -0.18886579020639485, "compression_ratio": 1.5700934579439252, "no_speech_prob": 9.874619308902766e-07}, {"id": 149, "seek": 66498, "start": 664.98, "end": 672.66, "text": " 3,000 movies that we're looking up each one hadn't had a one long embedding okay, so there's our 3,000 long you'll notice", "tokens": [805, 11, 1360, 6233, 300, 321, 434, 1237, 493, 1184, 472, 8782, 380, 632, 257, 472, 938, 12240, 3584, 1392, 11, 370, 456, 311, 527, 805, 11, 1360, 938, 291, 603, 3449], "temperature": 0.0, "avg_logprob": -0.1815434310395839, "compression_ratio": 1.6902985074626866, "no_speech_prob": 7.338182967941975e-07}, {"id": 150, "seek": 66498, "start": 672.66, "end": 677.08, "text": " It's a variable which is not surprising because we fed it a variable so we got a variable back", "tokens": [467, 311, 257, 7006, 597, 307, 406, 8830, 570, 321, 4636, 309, 257, 7006, 370, 321, 658, 257, 7006, 646], "temperature": 0.0, "avg_logprob": -0.1815434310395839, "compression_ratio": 1.6902985074626866, "no_speech_prob": 7.338182967941975e-07}, {"id": 151, "seek": 66498, "start": 677.08, "end": 682.74, "text": " And it's a variable that's on the GPU right dot Cuda okay, so", "tokens": [400, 309, 311, 257, 7006, 300, 311, 322, 264, 18407, 558, 5893, 383, 11152, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.1815434310395839, "compression_ratio": 1.6902985074626866, "no_speech_prob": 7.338182967941975e-07}, {"id": 152, "seek": 66498, "start": 683.54, "end": 689.0600000000001, "text": " We have a little shortcut in fast AI because we we very often want to take variables", "tokens": [492, 362, 257, 707, 24822, 294, 2370, 7318, 570, 321, 321, 588, 2049, 528, 281, 747, 9102], "temperature": 0.0, "avg_logprob": -0.1815434310395839, "compression_ratio": 1.6902985074626866, "no_speech_prob": 7.338182967941975e-07}, {"id": 153, "seek": 66498, "start": 689.38, "end": 693.66, "text": " Turn them into tensors and move them back to the CPU so we can play with them more easily", "tokens": [7956, 552, 666, 10688, 830, 293, 1286, 552, 646, 281, 264, 13199, 370, 321, 393, 862, 365, 552, 544, 3612], "temperature": 0.0, "avg_logprob": -0.1815434310395839, "compression_ratio": 1.6902985074626866, "no_speech_prob": 7.338182967941975e-07}, {"id": 154, "seek": 69366, "start": 693.66, "end": 698.78, "text": " So 2 NP is is to numpy okay, and that does all of those things", "tokens": [407, 568, 38611, 307, 307, 281, 1031, 8200, 1392, 11, 293, 300, 775, 439, 295, 729, 721], "temperature": 0.0, "avg_logprob": -0.21510821317149476, "compression_ratio": 1.6829268292682926, "no_speech_prob": 1.6028047866711859e-06}, {"id": 155, "seek": 69366, "start": 698.98, "end": 704.38, "text": " It works regardless of whether it's a tensor or a variable it works regardless of whether it's on the CPU or GPU", "tokens": [467, 1985, 10060, 295, 1968, 309, 311, 257, 40863, 420, 257, 7006, 309, 1985, 10060, 295, 1968, 309, 311, 322, 264, 13199, 420, 18407], "temperature": 0.0, "avg_logprob": -0.21510821317149476, "compression_ratio": 1.6829268292682926, "no_speech_prob": 1.6028047866711859e-06}, {"id": 156, "seek": 69366, "start": 705.02, "end": 707.4, "text": " It'll end up giving you a a numpy", "tokens": [467, 603, 917, 493, 2902, 291, 257, 257, 1031, 8200], "temperature": 0.0, "avg_logprob": -0.21510821317149476, "compression_ratio": 1.6829268292682926, "no_speech_prob": 1.6028047866711859e-06}, {"id": 157, "seek": 69366, "start": 707.98, "end": 711.02, "text": " Array from that okay, so if we do that", "tokens": [1587, 3458, 490, 300, 1392, 11, 370, 498, 321, 360, 300], "temperature": 0.0, "avg_logprob": -0.21510821317149476, "compression_ratio": 1.6829268292682926, "no_speech_prob": 1.6028047866711859e-06}, {"id": 158, "seek": 69366, "start": 712.3399999999999, "end": 717.42, "text": " That gives us exactly the same thing as we just looked at but now in numpy form", "tokens": [663, 2709, 505, 2293, 264, 912, 551, 382, 321, 445, 2956, 412, 457, 586, 294, 1031, 8200, 1254], "temperature": 0.0, "avg_logprob": -0.21510821317149476, "compression_ratio": 1.6829268292682926, "no_speech_prob": 1.6028047866711859e-06}, {"id": 159, "seek": 69366, "start": 717.9, "end": 722.62, "text": " Okay, so that's a super handy thing to use when you're playing around with pipe watch", "tokens": [1033, 11, 370, 300, 311, 257, 1687, 13239, 551, 281, 764, 562, 291, 434, 2433, 926, 365, 11240, 1159], "temperature": 0.0, "avg_logprob": -0.21510821317149476, "compression_ratio": 1.6829268292682926, "no_speech_prob": 1.6028047866711859e-06}, {"id": 160, "seek": 72262, "start": 722.62, "end": 724.98, "text": " my approach to things is", "tokens": [452, 3109, 281, 721, 307], "temperature": 0.0, "avg_logprob": -0.27430064703828544, "compression_ratio": 1.5405405405405406, "no_speech_prob": 2.9480006560334004e-06}, {"id": 161, "seek": 72262, "start": 725.86, "end": 727.18, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.27430064703828544, "compression_ratio": 1.5405405405405406, "no_speech_prob": 2.9480006560334004e-06}, {"id": 162, "seek": 72262, "start": 727.18, "end": 730.1, "text": " Try to use numpy for everything", "tokens": [6526, 281, 764, 1031, 8200, 337, 1203], "temperature": 0.0, "avg_logprob": -0.27430064703828544, "compression_ratio": 1.5405405405405406, "no_speech_prob": 2.9480006560334004e-06}, {"id": 163, "seek": 72262, "start": 731.02, "end": 735.9, "text": " Except when I explicitly you need something to run on the GPU or I need its derivatives", "tokens": [16192, 562, 286, 20803, 291, 643, 746, 281, 1190, 322, 264, 18407, 420, 286, 643, 1080, 33733], "temperature": 0.0, "avg_logprob": -0.27430064703828544, "compression_ratio": 1.5405405405405406, "no_speech_prob": 2.9480006560334004e-06}, {"id": 164, "seek": 72262, "start": 736.34, "end": 740.5, "text": " Right in which case I use pytorch because like numpy like I kind of find", "tokens": [1779, 294, 597, 1389, 286, 764, 25878, 284, 339, 570, 411, 1031, 8200, 411, 286, 733, 295, 915], "temperature": 0.0, "avg_logprob": -0.27430064703828544, "compression_ratio": 1.5405405405405406, "no_speech_prob": 2.9480006560334004e-06}, {"id": 165, "seek": 72262, "start": 741.1, "end": 745.9, "text": " NumPy is often easier to work with it's been around many years longer than pytorch", "tokens": [22592, 47, 88, 307, 2049, 3571, 281, 589, 365, 309, 311, 668, 926, 867, 924, 2854, 813, 25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.27430064703828544, "compression_ratio": 1.5405405405405406, "no_speech_prob": 2.9480006560334004e-06}, {"id": 166, "seek": 72262, "start": 747.74, "end": 750.82, "text": " So you know and and lots of things like", "tokens": [407, 291, 458, 293, 293, 3195, 295, 721, 411], "temperature": 0.0, "avg_logprob": -0.27430064703828544, "compression_ratio": 1.5405405405405406, "no_speech_prob": 2.9480006560334004e-06}, {"id": 167, "seek": 75082, "start": 750.82, "end": 756.0600000000001, "text": " like the Python imaging library and open CV and lots and lots of stuff like", "tokens": [411, 264, 15329, 25036, 6405, 293, 1269, 22995, 293, 3195, 293, 3195, 295, 1507, 411], "temperature": 0.0, "avg_logprob": -0.1925127699568465, "compression_ratio": 1.6946564885496183, "no_speech_prob": 1.0030117891801638e-06}, {"id": 168, "seek": 75082, "start": 756.82, "end": 763.5, "text": " Pandas it works with numpy so my approach is kind of like do as much as I can in numpy land", "tokens": [16995, 296, 309, 1985, 365, 1031, 8200, 370, 452, 3109, 307, 733, 295, 411, 360, 382, 709, 382, 286, 393, 294, 1031, 8200, 2117], "temperature": 0.0, "avg_logprob": -0.1925127699568465, "compression_ratio": 1.6946564885496183, "no_speech_prob": 1.0030117891801638e-06}, {"id": 169, "seek": 75082, "start": 764.98, "end": 767.6600000000001, "text": " Finally when I'm ready to do something on the GPU or take its derivative", "tokens": [6288, 562, 286, 478, 1919, 281, 360, 746, 322, 264, 18407, 420, 747, 1080, 13760], "temperature": 0.0, "avg_logprob": -0.1925127699568465, "compression_ratio": 1.6946564885496183, "no_speech_prob": 1.0030117891801638e-06}, {"id": 170, "seek": 75082, "start": 768.1400000000001, "end": 774.0200000000001, "text": " To pytorch and then as soon as I can I put it back in numpy and you'll see that the fast AI library", "tokens": [1407, 25878, 284, 339, 293, 550, 382, 2321, 382, 286, 393, 286, 829, 309, 646, 294, 1031, 8200, 293, 291, 603, 536, 300, 264, 2370, 7318, 6405], "temperature": 0.0, "avg_logprob": -0.1925127699568465, "compression_ratio": 1.6946564885496183, "no_speech_prob": 1.0030117891801638e-06}, {"id": 171, "seek": 77402, "start": 774.02, "end": 781.62, "text": " Really works this way like all the transformations and stuff happen in numpy, which is different to most high torch", "tokens": [4083, 1985, 341, 636, 411, 439, 264, 34852, 293, 1507, 1051, 294, 1031, 8200, 11, 597, 307, 819, 281, 881, 1090, 27822], "temperature": 0.0, "avg_logprob": -0.14774020512898764, "compression_ratio": 1.6398104265402844, "no_speech_prob": 1.7061761354852933e-06}, {"id": 172, "seek": 77402, "start": 782.26, "end": 789.02, "text": " Computer vision libraries which tend to do it all as much as possible in pytorch. I tried to do as much as possible in numpy", "tokens": [22289, 5201, 15148, 597, 3928, 281, 360, 309, 439, 382, 709, 382, 1944, 294, 25878, 284, 339, 13, 286, 3031, 281, 360, 382, 709, 382, 1944, 294, 1031, 8200], "temperature": 0.0, "avg_logprob": -0.14774020512898764, "compression_ratio": 1.6398104265402844, "no_speech_prob": 1.7061761354852933e-06}, {"id": 173, "seek": 77402, "start": 792.8199999999999, "end": 799.46, "text": " So let's say we wanted to transfer build a model in the GPU with the GPU and train it and then we want to", "tokens": [407, 718, 311, 584, 321, 1415, 281, 5003, 1322, 257, 2316, 294, 264, 18407, 365, 264, 18407, 293, 3847, 309, 293, 550, 321, 528, 281], "temperature": 0.0, "avg_logprob": -0.14774020512898764, "compression_ratio": 1.6398104265402844, "no_speech_prob": 1.7061761354852933e-06}, {"id": 174, "seek": 79946, "start": 799.46, "end": 804.9000000000001, "text": " Bring this to production so would we call to numpy on the model itself?", "tokens": [12842, 341, 281, 4265, 370, 576, 321, 818, 281, 1031, 8200, 322, 264, 2316, 2564, 30], "temperature": 0.0, "avg_logprob": -0.19342666625976562, "compression_ratio": 1.5609756097560976, "no_speech_prob": 4.139935683156182e-08}, {"id": 175, "seek": 79946, "start": 804.9000000000001, "end": 811.46, "text": " Or would we have to iterate through all the different layers and then call to NP? Yeah good question", "tokens": [1610, 576, 321, 362, 281, 44497, 807, 439, 264, 819, 7914, 293, 550, 818, 281, 38611, 30, 865, 665, 1168], "temperature": 0.0, "avg_logprob": -0.19342666625976562, "compression_ratio": 1.5609756097560976, "no_speech_prob": 4.139935683156182e-08}, {"id": 176, "seek": 79946, "start": 811.46, "end": 817.74, "text": " So it's very likely that you want to do inference on a CPU rather than a GPU. It's it's more scalable", "tokens": [407, 309, 311, 588, 3700, 300, 291, 528, 281, 360, 38253, 322, 257, 13199, 2831, 813, 257, 18407, 13, 467, 311, 309, 311, 544, 38481], "temperature": 0.0, "avg_logprob": -0.19342666625976562, "compression_ratio": 1.5609756097560976, "no_speech_prob": 4.139935683156182e-08}, {"id": 177, "seek": 79946, "start": 817.74, "end": 821.14, "text": " You don't have to worry about putting things in batches. You know so on and so forth", "tokens": [509, 500, 380, 362, 281, 3292, 466, 3372, 721, 294, 15245, 279, 13, 509, 458, 370, 322, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.19342666625976562, "compression_ratio": 1.5609756097560976, "no_speech_prob": 4.139935683156182e-08}, {"id": 178, "seek": 79946, "start": 822.0600000000001, "end": 824.82, "text": " So you can move a model?", "tokens": [407, 291, 393, 1286, 257, 2316, 30], "temperature": 0.0, "avg_logprob": -0.19342666625976562, "compression_ratio": 1.5609756097560976, "no_speech_prob": 4.139935683156182e-08}, {"id": 179, "seek": 82482, "start": 824.82, "end": 828.7800000000001, "text": " Onto the CPU just by typing m dot CPU", "tokens": [16980, 78, 264, 13199, 445, 538, 18444, 275, 5893, 13199], "temperature": 0.0, "avg_logprob": -0.19726346333821615, "compression_ratio": 1.6517412935323383, "no_speech_prob": 4.092872131877812e-06}, {"id": 180, "seek": 82482, "start": 829.4200000000001, "end": 837.5, "text": " And that model is now on the CPU and so therefore you can also then put your variable on the CPU", "tokens": [400, 300, 2316, 307, 586, 322, 264, 13199, 293, 370, 4412, 291, 393, 611, 550, 829, 428, 7006, 322, 264, 13199], "temperature": 0.0, "avg_logprob": -0.19726346333821615, "compression_ratio": 1.6517412935323383, "no_speech_prob": 4.092872131877812e-06}, {"id": 181, "seek": 82482, "start": 837.7800000000001, "end": 840.48, "text": " By doing exactly the same thing so you can say", "tokens": [3146, 884, 2293, 264, 912, 551, 370, 291, 393, 584], "temperature": 0.0, "avg_logprob": -0.19726346333821615, "compression_ratio": 1.6517412935323383, "no_speech_prob": 4.092872131877812e-06}, {"id": 182, "seek": 82482, "start": 844.2600000000001, "end": 850.4000000000001, "text": " Like so now having said that if your if your server doesn't have a GPU or CUDA GPU", "tokens": [1743, 370, 586, 1419, 848, 300, 498, 428, 498, 428, 7154, 1177, 380, 362, 257, 18407, 420, 29777, 7509, 18407], "temperature": 0.0, "avg_logprob": -0.19726346333821615, "compression_ratio": 1.6517412935323383, "no_speech_prob": 4.092872131877812e-06}, {"id": 183, "seek": 85040, "start": 850.4, "end": 854.56, "text": " You don't have to do this because it won't put it on the GPU at all", "tokens": [509, 500, 380, 362, 281, 360, 341, 570, 309, 1582, 380, 829, 309, 322, 264, 18407, 412, 439], "temperature": 0.0, "avg_logprob": -0.18469647566477457, "compression_ratio": 1.599078341013825, "no_speech_prob": 1.3081695442451746e-06}, {"id": 184, "seek": 85040, "start": 854.64, "end": 860.72, "text": " So if you're if if for inferencing on the server if you're running it on you know some", "tokens": [407, 498, 291, 434, 498, 498, 337, 13596, 13644, 322, 264, 7154, 498, 291, 434, 2614, 309, 322, 291, 458, 512], "temperature": 0.0, "avg_logprob": -0.18469647566477457, "compression_ratio": 1.599078341013825, "no_speech_prob": 1.3081695442451746e-06}, {"id": 185, "seek": 85040, "start": 861.48, "end": 866.56, "text": " T2 instance or something it'll work fine. It'll all run on the on the CPU automatically", "tokens": [314, 17, 5197, 420, 746, 309, 603, 589, 2489, 13, 467, 603, 439, 1190, 322, 264, 322, 264, 13199, 6772], "temperature": 0.0, "avg_logprob": -0.18469647566477457, "compression_ratio": 1.599078341013825, "no_speech_prob": 1.3081695442451746e-06}, {"id": 186, "seek": 85040, "start": 867.8, "end": 874.88, "text": " Quick follow-up, and if we train the model on the GPU, and then we save those embeddings and", "tokens": [12101, 1524, 12, 1010, 11, 293, 498, 321, 3847, 264, 2316, 322, 264, 18407, 11, 293, 550, 321, 3155, 729, 12240, 29432, 293], "temperature": 0.0, "avg_logprob": -0.18469647566477457, "compression_ratio": 1.599078341013825, "no_speech_prob": 1.3081695442451746e-06}, {"id": 187, "seek": 85040, "start": 875.68, "end": 877.0799999999999, "text": " the weights", "tokens": [264, 17443], "temperature": 0.0, "avg_logprob": -0.18469647566477457, "compression_ratio": 1.599078341013825, "no_speech_prob": 1.3081695442451746e-06}, {"id": 188, "seek": 87708, "start": 877.08, "end": 881.08, "text": " Would we have to do anything special to load it on to you know you won't?", "tokens": [6068, 321, 362, 281, 360, 1340, 2121, 281, 3677, 309, 322, 281, 291, 458, 291, 1582, 380, 30], "temperature": 0.0, "avg_logprob": -0.23673350311989008, "compression_ratio": 1.5566502463054188, "no_speech_prob": 4.356849331088597e-06}, {"id": 189, "seek": 87708, "start": 882.96, "end": 887.48, "text": " We have something well it kind of depends how much of fast AI you're using", "tokens": [492, 362, 746, 731, 309, 733, 295, 5946, 577, 709, 295, 2370, 7318, 291, 434, 1228], "temperature": 0.0, "avg_logprob": -0.23673350311989008, "compression_ratio": 1.5566502463054188, "no_speech_prob": 4.356849331088597e-06}, {"id": 190, "seek": 87708, "start": 887.6, "end": 891.2800000000001, "text": " So I'll show you how you can do that in case you have to do it manually", "tokens": [407, 286, 603, 855, 291, 577, 291, 393, 360, 300, 294, 1389, 291, 362, 281, 360, 309, 16945], "temperature": 0.0, "avg_logprob": -0.23673350311989008, "compression_ratio": 1.5566502463054188, "no_speech_prob": 4.356849331088597e-06}, {"id": 191, "seek": 87708, "start": 892.48, "end": 894.88, "text": " One of the students figured this out, which is very handy", "tokens": [1485, 295, 264, 1731, 8932, 341, 484, 11, 597, 307, 588, 13239], "temperature": 0.0, "avg_logprob": -0.23673350311989008, "compression_ratio": 1.5566502463054188, "no_speech_prob": 4.356849331088597e-06}, {"id": 192, "seek": 87708, "start": 895.8000000000001, "end": 897.8000000000001, "text": " when we", "tokens": [562, 321], "temperature": 0.0, "avg_logprob": -0.23673350311989008, "compression_ratio": 1.5566502463054188, "no_speech_prob": 4.356849331088597e-06}, {"id": 193, "seek": 87708, "start": 901.6, "end": 903.6, "text": " There's a load model function", "tokens": [821, 311, 257, 3677, 2316, 2445], "temperature": 0.0, "avg_logprob": -0.23673350311989008, "compression_ratio": 1.5566502463054188, "no_speech_prob": 4.356849331088597e-06}, {"id": 194, "seek": 90360, "start": 903.6, "end": 909.24, "text": " And you'll see what it does, but it does torch dot load is it basically this is like some magic", "tokens": [400, 291, 603, 536, 437, 309, 775, 11, 457, 309, 775, 27822, 5893, 3677, 307, 309, 1936, 341, 307, 411, 512, 5585], "temperature": 0.0, "avg_logprob": -0.22906995374102926, "compression_ratio": 1.5837320574162679, "no_speech_prob": 1.2098611250621616e-06}, {"id": 195, "seek": 90360, "start": 909.84, "end": 914.12, "text": " Incantation that like normally it has to load it onto the same GPU as saved on", "tokens": [7779, 394, 399, 300, 411, 5646, 309, 575, 281, 3677, 309, 3911, 264, 912, 18407, 382, 6624, 322], "temperature": 0.0, "avg_logprob": -0.22906995374102926, "compression_ratio": 1.5837320574162679, "no_speech_prob": 1.2098611250621616e-06}, {"id": 196, "seek": 90360, "start": 914.36, "end": 917.84, "text": " But this will like load it into whatever's whatever's available, so", "tokens": [583, 341, 486, 411, 3677, 309, 666, 2035, 311, 2035, 311, 2435, 11, 370], "temperature": 0.0, "avg_logprob": -0.22906995374102926, "compression_ratio": 1.5837320574162679, "no_speech_prob": 1.2098611250621616e-06}, {"id": 197, "seek": 90360, "start": 918.44, "end": 920.9200000000001, "text": " That was a handy discovery", "tokens": [663, 390, 257, 13239, 12114], "temperature": 0.0, "avg_logprob": -0.22906995374102926, "compression_ratio": 1.5837320574162679, "no_speech_prob": 1.2098611250621616e-06}, {"id": 198, "seek": 90360, "start": 923.36, "end": 925.36, "text": " Thanks for the great questions", "tokens": [2561, 337, 264, 869, 1651], "temperature": 0.0, "avg_logprob": -0.22906995374102926, "compression_ratio": 1.5837320574162679, "no_speech_prob": 1.2098611250621616e-06}, {"id": 199, "seek": 90360, "start": 927.72, "end": 930.88, "text": " And so to put that back on the", "tokens": [400, 370, 281, 829, 300, 646, 322, 264], "temperature": 0.0, "avg_logprob": -0.22906995374102926, "compression_ratio": 1.5837320574162679, "no_speech_prob": 1.2098611250621616e-06}, {"id": 200, "seek": 93088, "start": 930.88, "end": 933.96, "text": " GPU I'll need to say dot CUDA and", "tokens": [18407, 286, 603, 643, 281, 584, 5893, 29777, 7509, 293], "temperature": 0.0, "avg_logprob": -0.19246866511202407, "compression_ratio": 1.4403669724770642, "no_speech_prob": 1.5534915291937068e-06}, {"id": 201, "seek": 93088, "start": 935.56, "end": 938.28, "text": " Now there we go I can run it again, okay", "tokens": [823, 456, 321, 352, 286, 393, 1190, 309, 797, 11, 1392], "temperature": 0.0, "avg_logprob": -0.19246866511202407, "compression_ratio": 1.4403669724770642, "no_speech_prob": 1.5534915291937068e-06}, {"id": 202, "seek": 93088, "start": 939.6, "end": 948.2, "text": " So it's really important to know about the zip function in Python which iterates through a number of lists", "tokens": [407, 309, 311, 534, 1021, 281, 458, 466, 264, 20730, 2445, 294, 15329, 597, 17138, 1024, 807, 257, 1230, 295, 14511], "temperature": 0.0, "avg_logprob": -0.19246866511202407, "compression_ratio": 1.4403669724770642, "no_speech_prob": 1.5534915291937068e-06}, {"id": 203, "seek": 93088, "start": 948.76, "end": 952.74, "text": " At the same time so in this case. I want to grab each movie", "tokens": [1711, 264, 912, 565, 370, 294, 341, 1389, 13, 286, 528, 281, 4444, 1184, 3169], "temperature": 0.0, "avg_logprob": -0.19246866511202407, "compression_ratio": 1.4403669724770642, "no_speech_prob": 1.5534915291937068e-06}, {"id": 204, "seek": 93088, "start": 953.36, "end": 957.68, "text": " Along with its bias term so that I can just pop it into a list of tuples", "tokens": [17457, 365, 1080, 12577, 1433, 370, 300, 286, 393, 445, 1665, 309, 666, 257, 1329, 295, 2604, 2622], "temperature": 0.0, "avg_logprob": -0.19246866511202407, "compression_ratio": 1.4403669724770642, "no_speech_prob": 1.5534915291937068e-06}, {"id": 205, "seek": 95768, "start": 957.68, "end": 964.3599999999999, "text": " So if I just go zip like that that's going to iterate through each movie ID and each bias term", "tokens": [407, 498, 286, 445, 352, 20730, 411, 300, 300, 311, 516, 281, 44497, 807, 1184, 3169, 7348, 293, 1184, 12577, 1433], "temperature": 0.0, "avg_logprob": -0.17796198527018228, "compression_ratio": 1.543956043956044, "no_speech_prob": 9.132533591582614e-07}, {"id": 206, "seek": 95768, "start": 965.0799999999999, "end": 970.92, "text": " And so then I can use that in a list comprehension to grab the name of each movie along with its bias", "tokens": [400, 370, 550, 286, 393, 764, 300, 294, 257, 1329, 44991, 281, 4444, 264, 1315, 295, 1184, 3169, 2051, 365, 1080, 12577], "temperature": 0.0, "avg_logprob": -0.17796198527018228, "compression_ratio": 1.543956043956044, "no_speech_prob": 9.132533591582614e-07}, {"id": 207, "seek": 95768, "start": 971.8, "end": 975.04, "text": " Okay, so having done that I can then", "tokens": [1033, 11, 370, 1419, 1096, 300, 286, 393, 550], "temperature": 0.0, "avg_logprob": -0.17796198527018228, "compression_ratio": 1.543956043956044, "no_speech_prob": 9.132533591582614e-07}, {"id": 208, "seek": 95768, "start": 976.3599999999999, "end": 980.88, "text": " Sort and so here. I told you that John Travolta", "tokens": [26149, 293, 370, 510, 13, 286, 1907, 291, 300, 2619, 5403, 9646, 1328], "temperature": 0.0, "avg_logprob": -0.17796198527018228, "compression_ratio": 1.543956043956044, "no_speech_prob": 9.132533591582614e-07}, {"id": 209, "seek": 98088, "start": 980.88, "end": 987.8, "text": " Scientology movie at the most negative of net quite by a lot if this is a Kaggle competition", "tokens": [18944, 1793, 3169, 412, 264, 881, 3671, 295, 2533, 1596, 538, 257, 688, 498, 341, 307, 257, 48751, 22631, 6211], "temperature": 0.0, "avg_logprob": -0.22779208289252387, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.594307822320843e-06}, {"id": 210, "seek": 98088, "start": 988.24, "end": 993.16, "text": " Battlefield Earth would have like won by miles look at this seven seven seven ninety six", "tokens": [41091, 4755, 576, 362, 411, 1582, 538, 6193, 574, 412, 341, 3407, 3407, 3407, 25063, 2309], "temperature": 0.0, "avg_logprob": -0.22779208289252387, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.594307822320843e-06}, {"id": 211, "seek": 98088, "start": 994.4399999999999, "end": 999.6, "text": " So here is the worst movie of all time according to IMDB and like it's interesting", "tokens": [407, 510, 307, 264, 5855, 3169, 295, 439, 565, 4650, 281, 21463, 27735, 293, 411, 309, 311, 1880], "temperature": 0.0, "avg_logprob": -0.22779208289252387, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.594307822320843e-06}, {"id": 212, "seek": 98088, "start": 999.84, "end": 1005.74, "text": " when you think about what this means right because this is like a much more authentic way to find out how bad this movie is because", "tokens": [562, 291, 519, 466, 437, 341, 1355, 558, 570, 341, 307, 411, 257, 709, 544, 12466, 636, 281, 915, 484, 577, 1578, 341, 3169, 307, 570], "temperature": 0.0, "avg_logprob": -0.22779208289252387, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.594307822320843e-06}, {"id": 213, "seek": 100574, "start": 1005.74, "end": 1013.46, "text": " Like some people are just more negative about movies right and like if more of them watch your movie like you know", "tokens": [1743, 512, 561, 366, 445, 544, 3671, 466, 6233, 558, 293, 411, 498, 544, 295, 552, 1159, 428, 3169, 411, 291, 458], "temperature": 0.0, "avg_logprob": -0.16377966220562273, "compression_ratio": 1.833976833976834, "no_speech_prob": 5.989259648231382e-07}, {"id": 214, "seek": 100574, "start": 1013.46, "end": 1018.54, "text": " Highly critical audience they're going to rate it badly, so if you take an average. It's not quite fair right", "tokens": [5229, 356, 4924, 4034, 436, 434, 516, 281, 3314, 309, 13425, 11, 370, 498, 291, 747, 364, 4274, 13, 467, 311, 406, 1596, 3143, 558], "temperature": 0.0, "avg_logprob": -0.16377966220562273, "compression_ratio": 1.833976833976834, "no_speech_prob": 5.989259648231382e-07}, {"id": 215, "seek": 100574, "start": 1019.58, "end": 1020.82, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.16377966220562273, "compression_ratio": 1.833976833976834, "no_speech_prob": 5.989259648231382e-07}, {"id": 216, "seek": 100574, "start": 1020.82, "end": 1026.98, "text": " So what this is you know what this is doing is saying once we you know remove the fact that different people", "tokens": [407, 437, 341, 307, 291, 458, 437, 341, 307, 884, 307, 1566, 1564, 321, 291, 458, 4159, 264, 1186, 300, 819, 561], "temperature": 0.0, "avg_logprob": -0.16377966220562273, "compression_ratio": 1.833976833976834, "no_speech_prob": 5.989259648231382e-07}, {"id": 217, "seek": 100574, "start": 1027.34, "end": 1033.46, "text": " Have different overall positive or negative experiences and different people watch different kinds of movies, and we correct for all that", "tokens": [3560, 819, 4787, 3353, 420, 3671, 5235, 293, 819, 561, 1159, 819, 3685, 295, 6233, 11, 293, 321, 3006, 337, 439, 300], "temperature": 0.0, "avg_logprob": -0.16377966220562273, "compression_ratio": 1.833976833976834, "no_speech_prob": 5.989259648231382e-07}, {"id": 218, "seek": 103346, "start": 1033.46, "end": 1035.82, "text": " And this is the worst movie of all time", "tokens": [400, 341, 307, 264, 5855, 3169, 295, 439, 565], "temperature": 0.0, "avg_logprob": -0.24018064298127828, "compression_ratio": 1.4603174603174602, "no_speech_prob": 1.7603381365915993e-06}, {"id": 219, "seek": 103346, "start": 1036.78, "end": 1038.78, "text": " So that's a good thing to know", "tokens": [407, 300, 311, 257, 665, 551, 281, 458], "temperature": 0.0, "avg_logprob": -0.24018064298127828, "compression_ratio": 1.4603174603174602, "no_speech_prob": 1.7603381365915993e-06}, {"id": 220, "seek": 103346, "start": 1042.14, "end": 1049.92, "text": " So this is how we can yeah look inside our our model and and and interpret the bias vectors", "tokens": [407, 341, 307, 577, 321, 393, 1338, 574, 1854, 527, 527, 2316, 293, 293, 293, 7302, 264, 12577, 18875], "temperature": 0.0, "avg_logprob": -0.24018064298127828, "compression_ratio": 1.4603174603174602, "no_speech_prob": 1.7603381365915993e-06}, {"id": 221, "seek": 103346, "start": 1050.3, "end": 1052.98, "text": " You'll see here. I've sorted by", "tokens": [509, 603, 536, 510, 13, 286, 600, 25462, 538], "temperature": 0.0, "avg_logprob": -0.24018064298127828, "compression_ratio": 1.4603174603174602, "no_speech_prob": 1.7603381365915993e-06}, {"id": 222, "seek": 103346, "start": 1053.56, "end": 1057.42, "text": " The zeroth element of each tuple by using a lambda", "tokens": [440, 44746, 900, 4478, 295, 1184, 2604, 781, 538, 1228, 257, 13607], "temperature": 0.0, "avg_logprob": -0.24018064298127828, "compression_ratio": 1.4603174603174602, "no_speech_prob": 1.7603381365915993e-06}, {"id": 223, "seek": 103346, "start": 1058.3400000000001, "end": 1060.38, "text": " Originally I used this special", "tokens": [28696, 286, 1143, 341, 2121], "temperature": 0.0, "avg_logprob": -0.24018064298127828, "compression_ratio": 1.4603174603174602, "no_speech_prob": 1.7603381365915993e-06}, {"id": 224, "seek": 106038, "start": 1060.38, "end": 1064.8000000000002, "text": " Item getter this is part of Python's operator library", "tokens": [31066, 483, 391, 341, 307, 644, 295, 15329, 311, 12973, 6405], "temperature": 0.0, "avg_logprob": -0.18704971464553682, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.577957164045074e-06}, {"id": 225, "seek": 106038, "start": 1064.8000000000002, "end": 1069.0600000000002, "text": " And this creates a function that returns the zeroth element of something", "tokens": [400, 341, 7829, 257, 2445, 300, 11247, 264, 44746, 900, 4478, 295, 746], "temperature": 0.0, "avg_logprob": -0.18704971464553682, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.577957164045074e-06}, {"id": 226, "seek": 106038, "start": 1069.5, "end": 1075.3400000000001, "text": " In order to save time and then I actually realized that the lambda is only one more character", "tokens": [682, 1668, 281, 3155, 565, 293, 550, 286, 767, 5334, 300, 264, 13607, 307, 787, 472, 544, 2517], "temperature": 0.0, "avg_logprob": -0.18704971464553682, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.577957164045074e-06}, {"id": 227, "seek": 106038, "start": 1075.66, "end": 1079.72, "text": " To write than the item getter, so maybe we don't need to know this after all so", "tokens": [1407, 2464, 813, 264, 3174, 483, 391, 11, 370, 1310, 321, 500, 380, 643, 281, 458, 341, 934, 439, 370], "temperature": 0.0, "avg_logprob": -0.18704971464553682, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.577957164045074e-06}, {"id": 228, "seek": 106038, "start": 1080.46, "end": 1084.5600000000002, "text": " Yeah, really useful to make sure you know how to write lambdas in python", "tokens": [865, 11, 534, 4420, 281, 652, 988, 291, 458, 577, 281, 2464, 10097, 27476, 294, 38797], "temperature": 0.0, "avg_logprob": -0.18704971464553682, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.577957164045074e-06}, {"id": 229, "seek": 106038, "start": 1084.5600000000002, "end": 1087.0600000000002, "text": " So this is this is a function okay?", "tokens": [407, 341, 307, 341, 307, 257, 2445, 1392, 30], "temperature": 0.0, "avg_logprob": -0.18704971464553682, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.577957164045074e-06}, {"id": 230, "seek": 108706, "start": 1087.06, "end": 1094.2, "text": " And so the sort is going to call this function every time it decides like is this thing higher or lower than that other thing and", "tokens": [400, 370, 264, 1333, 307, 516, 281, 818, 341, 2445, 633, 565, 309, 14898, 411, 307, 341, 551, 2946, 420, 3126, 813, 300, 661, 551, 293], "temperature": 0.0, "avg_logprob": -0.2422441108939574, "compression_ratio": 1.6015625, "no_speech_prob": 1.1189395081601106e-06}, {"id": 231, "seek": 108706, "start": 1094.3799999999999, "end": 1098.3, "text": " This is going to return the zeroth element, okay?", "tokens": [639, 307, 516, 281, 2736, 264, 44746, 900, 4478, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.2422441108939574, "compression_ratio": 1.6015625, "no_speech_prob": 1.1189395081601106e-06}, {"id": 232, "seek": 108706, "start": 1099.58, "end": 1103.94, "text": " so here's the same thing in item getter format, and here is the reverse and", "tokens": [370, 510, 311, 264, 912, 551, 294, 3174, 483, 391, 7877, 11, 293, 510, 307, 264, 9943, 293], "temperature": 0.0, "avg_logprob": -0.2422441108939574, "compression_ratio": 1.6015625, "no_speech_prob": 1.1189395081601106e-06}, {"id": 233, "seek": 108706, "start": 1106.3799999999999, "end": 1112.1599999999999, "text": " Shawshank redemption right at the top definitely agree with that Godfather usual suspects. Yeah, these are all pretty great movies", "tokens": [27132, 2716, 657, 35644, 558, 412, 264, 1192, 2138, 3986, 365, 300, 1265, 11541, 7713, 35667, 13, 865, 11, 613, 366, 439, 1238, 869, 6233], "temperature": 0.0, "avg_logprob": -0.2422441108939574, "compression_ratio": 1.6015625, "no_speech_prob": 1.1189395081601106e-06}, {"id": 234, "seek": 108706, "start": 1113.8999999999999, "end": 1115.8999999999999, "text": " 12 angry men absolutely", "tokens": [2272, 6884, 1706, 3122], "temperature": 0.0, "avg_logprob": -0.2422441108939574, "compression_ratio": 1.6015625, "no_speech_prob": 1.1189395081601106e-06}, {"id": 235, "seek": 111590, "start": 1115.9, "end": 1117.9, "text": " So there you go", "tokens": [407, 456, 291, 352], "temperature": 0.0, "avg_logprob": -0.17496273818525296, "compression_ratio": 1.786046511627907, "no_speech_prob": 1.4367473113452434e-06}, {"id": 236, "seek": 111590, "start": 1119.22, "end": 1121.22, "text": " There's how we can look at the bias", "tokens": [821, 311, 577, 321, 393, 574, 412, 264, 12577], "temperature": 0.0, "avg_logprob": -0.17496273818525296, "compression_ratio": 1.786046511627907, "no_speech_prob": 1.4367473113452434e-06}, {"id": 237, "seek": 111590, "start": 1122.0600000000002, "end": 1127.68, "text": " So then the second piece to look at would be the the embeddings how can we look at the embeddings?", "tokens": [407, 550, 264, 1150, 2522, 281, 574, 412, 576, 312, 264, 264, 12240, 29432, 577, 393, 321, 574, 412, 264, 12240, 29432, 30], "temperature": 0.0, "avg_logprob": -0.17496273818525296, "compression_ratio": 1.786046511627907, "no_speech_prob": 1.4367473113452434e-06}, {"id": 238, "seek": 111590, "start": 1128.74, "end": 1134.94, "text": " So we can do the same thing so remember I was the item embeddings rather than IB with the item bias", "tokens": [407, 321, 393, 360, 264, 912, 551, 370, 1604, 286, 390, 264, 3174, 12240, 29432, 2831, 813, 40385, 365, 264, 3174, 12577], "temperature": 0.0, "avg_logprob": -0.17496273818525296, "compression_ratio": 1.786046511627907, "no_speech_prob": 1.4367473113452434e-06}, {"id": 239, "seek": 111590, "start": 1135.3000000000002, "end": 1138.1000000000001, "text": " We can pass in our list of movies as a variable", "tokens": [492, 393, 1320, 294, 527, 1329, 295, 6233, 382, 257, 7006], "temperature": 0.0, "avg_logprob": -0.17496273818525296, "compression_ratio": 1.786046511627907, "no_speech_prob": 1.4367473113452434e-06}, {"id": 240, "seek": 111590, "start": 1138.5400000000002, "end": 1143.3400000000001, "text": " Turn it into numpy, and here's our movie embeddings so for each of the three thousand", "tokens": [7956, 309, 666, 1031, 8200, 11, 293, 510, 311, 527, 3169, 12240, 29432, 370, 337, 1184, 295, 264, 1045, 4714], "temperature": 0.0, "avg_logprob": -0.17496273818525296, "compression_ratio": 1.786046511627907, "no_speech_prob": 1.4367473113452434e-06}, {"id": 241, "seek": 114334, "start": 1143.34, "end": 1145.34, "text": " most popular movies", "tokens": [881, 3743, 6233], "temperature": 0.0, "avg_logprob": -0.21194611896168103, "compression_ratio": 1.6396396396396395, "no_speech_prob": 9.874611350824125e-07}, {"id": 242, "seek": 114334, "start": 1146.34, "end": 1148.5, "text": " Here are its 50 embeddings", "tokens": [1692, 366, 1080, 2625, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.21194611896168103, "compression_ratio": 1.6396396396396395, "no_speech_prob": 9.874611350824125e-07}, {"id": 243, "seek": 114334, "start": 1149.74, "end": 1151.4199999999998, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.21194611896168103, "compression_ratio": 1.6396396396396395, "no_speech_prob": 9.874611350824125e-07}, {"id": 244, "seek": 114334, "start": 1151.4199999999998, "end": 1155.3799999999999, "text": " It's very hard unless you're Jeffrey Hinton to visualize a 50 dimensional space", "tokens": [467, 311, 588, 1152, 5969, 291, 434, 28721, 389, 12442, 281, 23273, 257, 2625, 18795, 1901], "temperature": 0.0, "avg_logprob": -0.21194611896168103, "compression_ratio": 1.6396396396396395, "no_speech_prob": 9.874611350824125e-07}, {"id": 245, "seek": 114334, "start": 1156.1399999999999, "end": 1159.58, "text": " So what we'll do is we'll turn it into a three dimensional space", "tokens": [407, 437, 321, 603, 360, 307, 321, 603, 1261, 309, 666, 257, 1045, 18795, 1901], "temperature": 0.0, "avg_logprob": -0.21194611896168103, "compression_ratio": 1.6396396396396395, "no_speech_prob": 9.874611350824125e-07}, {"id": 246, "seek": 114334, "start": 1160.54, "end": 1162.54, "text": " So we can compress", "tokens": [407, 321, 393, 14778], "temperature": 0.0, "avg_logprob": -0.21194611896168103, "compression_ratio": 1.6396396396396395, "no_speech_prob": 9.874611350824125e-07}, {"id": 247, "seek": 114334, "start": 1162.98, "end": 1167.78, "text": " High dimensional spaces down into lower dimensional spaces using lots of different techniques", "tokens": [5229, 18795, 7673, 760, 666, 3126, 18795, 7673, 1228, 3195, 295, 819, 7512], "temperature": 0.0, "avg_logprob": -0.21194611896168103, "compression_ratio": 1.6396396396396395, "no_speech_prob": 9.874611350824125e-07}, {"id": 248, "seek": 114334, "start": 1168.3, "end": 1171.34, "text": " Perhaps one of the most common and popular is called PCA", "tokens": [10517, 472, 295, 264, 881, 2689, 293, 3743, 307, 1219, 6465, 32], "temperature": 0.0, "avg_logprob": -0.21194611896168103, "compression_ratio": 1.6396396396396395, "no_speech_prob": 9.874611350824125e-07}, {"id": 249, "seek": 117134, "start": 1171.34, "end": 1176.86, "text": " PCA stands for principal components analysis. It's a linear technique", "tokens": [6465, 32, 7382, 337, 9716, 6677, 5215, 13, 467, 311, 257, 8213, 6532], "temperature": 0.0, "avg_logprob": -0.239039362212758, "compression_ratio": 1.5, "no_speech_prob": 2.6015984531113645e-06}, {"id": 250, "seek": 117134, "start": 1177.62, "end": 1178.9399999999998, "text": " but", "tokens": [457], "temperature": 0.0, "avg_logprob": -0.239039362212758, "compression_ratio": 1.5, "no_speech_prob": 2.6015984531113645e-06}, {"id": 251, "seek": 117134, "start": 1178.9399999999998, "end": 1182.34, "text": " linear techniques generally work fine for this kind of", "tokens": [8213, 7512, 5101, 589, 2489, 337, 341, 733, 295], "temperature": 0.0, "avg_logprob": -0.239039362212758, "compression_ratio": 1.5, "no_speech_prob": 2.6015984531113645e-06}, {"id": 252, "seek": 117134, "start": 1183.3, "end": 1185.9599999999998, "text": " Embedding I'm not going to teach you about PCA now", "tokens": [24234, 292, 3584, 286, 478, 406, 516, 281, 2924, 291, 466, 6465, 32, 586], "temperature": 0.0, "avg_logprob": -0.239039362212758, "compression_ratio": 1.5, "no_speech_prob": 2.6015984531113645e-06}, {"id": 253, "seek": 117134, "start": 1185.9599999999998, "end": 1191.4599999999998, "text": " But I will say in Rachel's computational linear algebra class which you can get to from faster AI", "tokens": [583, 286, 486, 584, 294, 14246, 311, 28270, 8213, 21989, 1508, 597, 291, 393, 483, 281, 490, 4663, 7318], "temperature": 0.0, "avg_logprob": -0.239039362212758, "compression_ratio": 1.5, "no_speech_prob": 2.6015984531113645e-06}, {"id": 254, "seek": 117134, "start": 1192.5, "end": 1194.1399999999999, "text": " We cover", "tokens": [492, 2060], "temperature": 0.0, "avg_logprob": -0.239039362212758, "compression_ratio": 1.5, "no_speech_prob": 2.6015984531113645e-06}, {"id": 255, "seek": 117134, "start": 1194.1399999999999, "end": 1196.1399999999999, "text": " PCA in a lot of detail", "tokens": [6465, 32, 294, 257, 688, 295, 2607], "temperature": 0.0, "avg_logprob": -0.239039362212758, "compression_ratio": 1.5, "no_speech_prob": 2.6015984531113645e-06}, {"id": 256, "seek": 119614, "start": 1196.14, "end": 1203.9, "text": " And it's a really important technique it actually it turns out to be almost identical to something called singular value decomposition", "tokens": [400, 309, 311, 257, 534, 1021, 6532, 309, 767, 309, 4523, 484, 281, 312, 1920, 14800, 281, 746, 1219, 20010, 2158, 48356], "temperature": 0.0, "avg_logprob": -0.2170811703330592, "compression_ratio": 1.5863636363636364, "no_speech_prob": 4.565938979794737e-06}, {"id": 257, "seek": 119614, "start": 1204.0200000000002, "end": 1206.6200000000001, "text": " Which is a type of matrix decomposition which?", "tokens": [3013, 307, 257, 2010, 295, 8141, 48356, 597, 30], "temperature": 0.0, "avg_logprob": -0.2170811703330592, "compression_ratio": 1.5863636363636364, "no_speech_prob": 4.565938979794737e-06}, {"id": 258, "seek": 119614, "start": 1207.3000000000002, "end": 1211.3400000000001, "text": " Actually does turn up in deep learning a little bit from time to time", "tokens": [5135, 775, 1261, 493, 294, 2452, 2539, 257, 707, 857, 490, 565, 281, 565], "temperature": 0.0, "avg_logprob": -0.2170811703330592, "compression_ratio": 1.5863636363636364, "no_speech_prob": 4.565938979794737e-06}, {"id": 259, "seek": 119614, "start": 1212.42, "end": 1219.46, "text": " So it's kind of somewhat worth knowing if you are going to dig more into linear algebra. You know", "tokens": [407, 309, 311, 733, 295, 8344, 3163, 5276, 498, 291, 366, 516, 281, 2528, 544, 666, 8213, 21989, 13, 509, 458], "temperature": 0.0, "avg_logprob": -0.2170811703330592, "compression_ratio": 1.5863636363636364, "no_speech_prob": 4.565938979794737e-06}, {"id": 260, "seek": 121946, "start": 1219.46, "end": 1226.8600000000001, "text": " SPD and PCA along with eigenvalues and eigenvectors which are all slightly different versions of this kind of the same thing", "tokens": [19572, 293, 6465, 32, 2051, 365, 10446, 46033, 293, 10446, 303, 5547, 597, 366, 439, 4748, 819, 9606, 295, 341, 733, 295, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.19327316661872487, "compression_ratio": 1.6489795918367347, "no_speech_prob": 3.2887128327274695e-06}, {"id": 261, "seek": 121946, "start": 1227.26, "end": 1229.1000000000001, "text": " Are all worth knowing", "tokens": [2014, 439, 3163, 5276], "temperature": 0.0, "avg_logprob": -0.19327316661872487, "compression_ratio": 1.6489795918367347, "no_speech_prob": 3.2887128327274695e-06}, {"id": 262, "seek": 121946, "start": 1229.1000000000001, "end": 1234.06, "text": " But for now just know that you can grab PCA from sklearn.decomposition", "tokens": [583, 337, 586, 445, 458, 300, 291, 393, 4444, 6465, 32, 490, 1110, 306, 1083, 13, 1479, 21541, 5830], "temperature": 0.0, "avg_logprob": -0.19327316661872487, "compression_ratio": 1.6489795918367347, "no_speech_prob": 3.2887128327274695e-06}, {"id": 263, "seek": 121946, "start": 1234.98, "end": 1239.6000000000001, "text": " Say how much you want to reduce the dimensionality to so I want to find three", "tokens": [6463, 577, 709, 291, 528, 281, 5407, 264, 10139, 1860, 281, 370, 286, 528, 281, 915, 1045], "temperature": 0.0, "avg_logprob": -0.19327316661872487, "compression_ratio": 1.6489795918367347, "no_speech_prob": 3.2887128327274695e-06}, {"id": 264, "seek": 121946, "start": 1240.42, "end": 1243.54, "text": " Components and what this is going to do is it's going to find three", "tokens": [6620, 40496, 293, 437, 341, 307, 516, 281, 360, 307, 309, 311, 516, 281, 915, 1045], "temperature": 0.0, "avg_logprob": -0.19327316661872487, "compression_ratio": 1.6489795918367347, "no_speech_prob": 3.2887128327274695e-06}, {"id": 265, "seek": 121946, "start": 1244.18, "end": 1247.5, "text": " linear combinations of the 50 dimensions", "tokens": [8213, 21267, 295, 264, 2625, 12819], "temperature": 0.0, "avg_logprob": -0.19327316661872487, "compression_ratio": 1.6489795918367347, "no_speech_prob": 3.2887128327274695e-06}, {"id": 266, "seek": 124750, "start": 1247.5, "end": 1254.48, "text": " Which capture as much as the variation as possible, but are as different to each other as possible, okay?", "tokens": [3013, 7983, 382, 709, 382, 264, 12990, 382, 1944, 11, 457, 366, 382, 819, 281, 1184, 661, 382, 1944, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.22808494567871093, "compression_ratio": 1.5940594059405941, "no_speech_prob": 9.874612487692502e-07}, {"id": 267, "seek": 124750, "start": 1255.38, "end": 1257.86, "text": " So we would call this a lower rank", "tokens": [407, 321, 576, 818, 341, 257, 3126, 6181], "temperature": 0.0, "avg_logprob": -0.22808494567871093, "compression_ratio": 1.5940594059405941, "no_speech_prob": 9.874612487692502e-07}, {"id": 268, "seek": 124750, "start": 1258.82, "end": 1261.6, "text": " approximation of our matrix all right", "tokens": [28023, 295, 527, 8141, 439, 558], "temperature": 0.0, "avg_logprob": -0.22808494567871093, "compression_ratio": 1.5940594059405941, "no_speech_prob": 9.874612487692502e-07}, {"id": 269, "seek": 124750, "start": 1262.58, "end": 1265.94, "text": " So then we can grab the components so that's going to be the three", "tokens": [407, 550, 321, 393, 4444, 264, 6677, 370, 300, 311, 516, 281, 312, 264, 1045], "temperature": 0.0, "avg_logprob": -0.22808494567871093, "compression_ratio": 1.5940594059405941, "no_speech_prob": 9.874612487692502e-07}, {"id": 270, "seek": 124750, "start": 1266.94, "end": 1270.52, "text": " Dimensions and so once we've done that we've now got three by three thousand", "tokens": [20975, 8302, 293, 370, 1564, 321, 600, 1096, 300, 321, 600, 586, 658, 1045, 538, 1045, 4714], "temperature": 0.0, "avg_logprob": -0.22808494567871093, "compression_ratio": 1.5940594059405941, "no_speech_prob": 9.874612487692502e-07}, {"id": 271, "seek": 127052, "start": 1270.52, "end": 1277.96, "text": " And so we can now take a look at the first of them and we'll do the same thing of using zip", "tokens": [400, 370, 321, 393, 586, 747, 257, 574, 412, 264, 700, 295, 552, 293, 321, 603, 360, 264, 912, 551, 295, 1228, 20730], "temperature": 0.0, "avg_logprob": -0.15315026250378838, "compression_ratio": 1.596774193548387, "no_speech_prob": 1.248264311470848e-06}, {"id": 272, "seek": 127052, "start": 1277.96, "end": 1279.96, "text": " To look at each one along with its movie", "tokens": [1407, 574, 412, 1184, 472, 2051, 365, 1080, 3169], "temperature": 0.0, "avg_logprob": -0.15315026250378838, "compression_ratio": 1.596774193548387, "no_speech_prob": 1.248264311470848e-06}, {"id": 273, "seek": 127052, "start": 1280.48, "end": 1282.48, "text": " and so here's the thing right we", "tokens": [293, 370, 510, 311, 264, 551, 558, 321], "temperature": 0.0, "avg_logprob": -0.15315026250378838, "compression_ratio": 1.596774193548387, "no_speech_prob": 1.248264311470848e-06}, {"id": 274, "seek": 127052, "start": 1283.24, "end": 1285.32, "text": " We don't know ahead of time what this", "tokens": [492, 500, 380, 458, 2286, 295, 565, 437, 341], "temperature": 0.0, "avg_logprob": -0.15315026250378838, "compression_ratio": 1.596774193548387, "no_speech_prob": 1.248264311470848e-06}, {"id": 275, "seek": 127052, "start": 1286.4, "end": 1291.84, "text": " PCA thing is it's just it's just a bunch of latent factors", "tokens": [6465, 32, 551, 307, 309, 311, 445, 309, 311, 445, 257, 3840, 295, 48994, 6771], "temperature": 0.0, "avg_logprob": -0.15315026250378838, "compression_ratio": 1.596774193548387, "no_speech_prob": 1.248264311470848e-06}, {"id": 276, "seek": 127052, "start": 1293.2, "end": 1295.8799999999999, "text": " You know it's kind of the the main", "tokens": [509, 458, 309, 311, 733, 295, 264, 264, 2135], "temperature": 0.0, "avg_logprob": -0.15315026250378838, "compression_ratio": 1.596774193548387, "no_speech_prob": 1.248264311470848e-06}, {"id": 277, "seek": 129588, "start": 1295.88, "end": 1301.5200000000002, "text": " axis in this space of latent factors and so but what we can do is we can look at it and", "tokens": [10298, 294, 341, 1901, 295, 48994, 6771, 293, 370, 457, 437, 321, 393, 360, 307, 321, 393, 574, 412, 309, 293], "temperature": 0.0, "avg_logprob": -0.25220216180860383, "compression_ratio": 1.5913043478260869, "no_speech_prob": 1.3925431403549737e-06}, {"id": 278, "seek": 129588, "start": 1302.2, "end": 1304.2, "text": " See if we can figure out", "tokens": [3008, 498, 321, 393, 2573, 484], "temperature": 0.0, "avg_logprob": -0.25220216180860383, "compression_ratio": 1.5913043478260869, "no_speech_prob": 1.3925431403549737e-06}, {"id": 279, "seek": 129588, "start": 1304.48, "end": 1307.2800000000002, "text": " What it's about right so given that?", "tokens": [708, 309, 311, 466, 558, 370, 2212, 300, 30], "temperature": 0.0, "avg_logprob": -0.25220216180860383, "compression_ratio": 1.5913043478260869, "no_speech_prob": 1.3925431403549737e-06}, {"id": 280, "seek": 129588, "start": 1308.16, "end": 1311.68, "text": " Police Academy for is high up here along with water world", "tokens": [11882, 11735, 337, 307, 1090, 493, 510, 2051, 365, 1281, 1002], "temperature": 0.0, "avg_logprob": -0.25220216180860383, "compression_ratio": 1.5913043478260869, "no_speech_prob": 1.3925431403549737e-06}, {"id": 281, "seek": 129588, "start": 1312.4, "end": 1316.64, "text": " Where else bar go pulp fiction and Godfeather of high up here?", "tokens": [2305, 1646, 2159, 352, 37489, 13266, 293, 1265, 2106, 1172, 295, 1090, 493, 510, 30], "temperature": 0.0, "avg_logprob": -0.25220216180860383, "compression_ratio": 1.5913043478260869, "no_speech_prob": 1.3925431403549737e-06}, {"id": 282, "seek": 129588, "start": 1316.64, "end": 1321.0400000000002, "text": " I'm going to guess that a high value is not going to represent like", "tokens": [286, 478, 516, 281, 2041, 300, 257, 1090, 2158, 307, 406, 516, 281, 2906, 411], "temperature": 0.0, "avg_logprob": -0.25220216180860383, "compression_ratio": 1.5913043478260869, "no_speech_prob": 1.3925431403549737e-06}, {"id": 283, "seek": 129588, "start": 1321.64, "end": 1323.5600000000002, "text": " critically acclaimed movies", "tokens": [22797, 1317, 22642, 6233], "temperature": 0.0, "avg_logprob": -0.25220216180860383, "compression_ratio": 1.5913043478260869, "no_speech_prob": 1.3925431403549737e-06}, {"id": 284, "seek": 132356, "start": 1323.56, "end": 1327.6, "text": " Or serious watching so I kind of what did I call this yeah, okay?", "tokens": [1610, 3156, 1976, 370, 286, 733, 295, 437, 630, 286, 818, 341, 1338, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.1761277688516153, "compression_ratio": 1.8083333333333333, "no_speech_prob": 2.4439782464469317e-06}, {"id": 285, "seek": 132356, "start": 1327.6, "end": 1330.08, "text": " I called this easy watching versus serious", "tokens": [286, 1219, 341, 1858, 1976, 5717, 3156], "temperature": 0.0, "avg_logprob": -0.1761277688516153, "compression_ratio": 1.8083333333333333, "no_speech_prob": 2.4439782464469317e-06}, {"id": 286, "seek": 132356, "start": 1330.52, "end": 1336.6799999999998, "text": " right, but like this is kind of how you have to interpret your embeddings is like take a look at what they seem to be showing and", "tokens": [558, 11, 457, 411, 341, 307, 733, 295, 577, 291, 362, 281, 7302, 428, 12240, 29432, 307, 411, 747, 257, 574, 412, 437, 436, 1643, 281, 312, 4099, 293], "temperature": 0.0, "avg_logprob": -0.1761277688516153, "compression_ratio": 1.8083333333333333, "no_speech_prob": 2.4439782464469317e-06}, {"id": 287, "seek": 132356, "start": 1337.96, "end": 1340.72, "text": " Decide what you think it means so this is the kind of the", "tokens": [12427, 482, 437, 291, 519, 309, 1355, 370, 341, 307, 264, 733, 295, 264], "temperature": 0.0, "avg_logprob": -0.1761277688516153, "compression_ratio": 1.8083333333333333, "no_speech_prob": 2.4439782464469317e-06}, {"id": 288, "seek": 132356, "start": 1341.52, "end": 1346.6, "text": " The principal axis in this set of embedding so we can look at the next one", "tokens": [440, 9716, 10298, 294, 341, 992, 295, 12240, 3584, 370, 321, 393, 574, 412, 264, 958, 472], "temperature": 0.0, "avg_logprob": -0.1761277688516153, "compression_ratio": 1.8083333333333333, "no_speech_prob": 2.4439782464469317e-06}, {"id": 289, "seek": 132356, "start": 1348.28, "end": 1352.44, "text": " So do the same thing and look at the first index one embedding", "tokens": [407, 360, 264, 912, 551, 293, 574, 412, 264, 700, 8186, 472, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.1761277688516153, "compression_ratio": 1.8083333333333333, "no_speech_prob": 2.4439782464469317e-06}, {"id": 290, "seek": 135244, "start": 1352.44, "end": 1356.28, "text": " This one's a little bit harder to kind of figure out what's going on", "tokens": [639, 472, 311, 257, 707, 857, 6081, 281, 733, 295, 2573, 484, 437, 311, 516, 322], "temperature": 0.0, "avg_logprob": -0.23958316150012318, "compression_ratio": 1.7660377358490567, "no_speech_prob": 1.3081727274766308e-06}, {"id": 291, "seek": 135244, "start": 1356.28, "end": 1359.72, "text": " But with things like Mulholland Drive and purple rose of Cairo", "tokens": [583, 365, 721, 411, 29960, 1289, 285, 474, 15622, 293, 9656, 10895, 295, 30983, 340], "temperature": 0.0, "avg_logprob": -0.23958316150012318, "compression_ratio": 1.7660377358490567, "no_speech_prob": 1.3081727274766308e-06}, {"id": 292, "seek": 135244, "start": 1360.44, "end": 1366.52, "text": " These look more kind of dialog e kind of ones or else things like Lord of the Rings and a Latin and Star Wars", "tokens": [1981, 574, 544, 733, 295, 19308, 308, 733, 295, 2306, 420, 1646, 721, 411, 3257, 295, 264, 38543, 293, 257, 10803, 293, 5705, 9818], "temperature": 0.0, "avg_logprob": -0.23958316150012318, "compression_ratio": 1.7660377358490567, "no_speech_prob": 1.3081727274766308e-06}, {"id": 293, "seek": 135244, "start": 1366.52, "end": 1371.26, "text": " These look more like kind of modern CGI e kind of ones so you could kind of imagine that on that", "tokens": [1981, 574, 544, 411, 733, 295, 4363, 48448, 308, 733, 295, 2306, 370, 291, 727, 733, 295, 3811, 300, 322, 300], "temperature": 0.0, "avg_logprob": -0.23958316150012318, "compression_ratio": 1.7660377358490567, "no_speech_prob": 1.3081727274766308e-06}, {"id": 294, "seek": 135244, "start": 1371.8400000000001, "end": 1375.96, "text": " pair of dimensions it probably represents a lot of you know", "tokens": [6119, 295, 12819, 309, 1391, 8855, 257, 688, 295, 291, 458], "temperature": 0.0, "avg_logprob": -0.23958316150012318, "compression_ratio": 1.7660377358490567, "no_speech_prob": 1.3081727274766308e-06}, {"id": 295, "seek": 137596, "start": 1375.96, "end": 1381.28, "text": " differences between how people rate movies you know some people like", "tokens": [7300, 1296, 577, 561, 3314, 6233, 291, 458, 512, 561, 411], "temperature": 0.0, "avg_logprob": -0.2440460549026239, "compression_ratio": 1.6627218934911243, "no_speech_prob": 5.25535824635881e-06}, {"id": 296, "seek": 137596, "start": 1382.8, "end": 1387.14, "text": " You know purple rise of Cairo type movies you know Woody Allen", "tokens": [509, 458, 9656, 6272, 295, 30983, 340, 2010, 6233, 291, 458, 40618, 17160], "temperature": 0.0, "avg_logprob": -0.2440460549026239, "compression_ratio": 1.6627218934911243, "no_speech_prob": 5.25535824635881e-06}, {"id": 297, "seek": 137596, "start": 1388.48, "end": 1393.32, "text": " Kind of classic and some people like these you know big Hollywood spectacles", "tokens": [9242, 295, 7230, 293, 512, 561, 411, 613, 291, 458, 955, 11628, 6177, 9918], "temperature": 0.0, "avg_logprob": -0.2440460549026239, "compression_ratio": 1.6627218934911243, "no_speech_prob": 5.25535824635881e-06}, {"id": 298, "seek": 137596, "start": 1394.8400000000001, "end": 1400.52, "text": " Some people presumably like police academy for more than they like Fargo", "tokens": [2188, 561, 26742, 411, 3804, 25525, 337, 544, 813, 436, 411, 9067, 1571], "temperature": 0.0, "avg_logprob": -0.2440460549026239, "compression_ratio": 1.6627218934911243, "no_speech_prob": 5.25535824635881e-06}, {"id": 299, "seek": 140052, "start": 1400.52, "end": 1408.2, "text": " So yeah, so like you can kind of get the idea of what's happened it's it's done a you know for a model which was", "tokens": [407, 1338, 11, 370, 411, 291, 393, 733, 295, 483, 264, 1558, 295, 437, 311, 2011, 309, 311, 309, 311, 1096, 257, 291, 458, 337, 257, 2316, 597, 390], "temperature": 0.0, "avg_logprob": -0.1889947764078776, "compression_ratio": 1.6647058823529413, "no_speech_prob": 4.710856046585832e-06}, {"id": 300, "seek": 140052, "start": 1413.36, "end": 1418.36, "text": " You know for a model which was literally multiply two things together and add them up", "tokens": [509, 458, 337, 257, 2316, 597, 390, 3736, 12972, 732, 721, 1214, 293, 909, 552, 493], "temperature": 0.0, "avg_logprob": -0.1889947764078776, "compression_ratio": 1.6647058823529413, "no_speech_prob": 4.710856046585832e-06}, {"id": 301, "seek": 140052, "start": 1420.92, "end": 1424.44, "text": " It's learnt quite a lot you know which is kind of cool", "tokens": [467, 311, 18991, 1596, 257, 688, 291, 458, 597, 307, 733, 295, 1627], "temperature": 0.0, "avg_logprob": -0.1889947764078776, "compression_ratio": 1.6647058823529413, "no_speech_prob": 4.710856046585832e-06}, {"id": 302, "seek": 142444, "start": 1424.44, "end": 1429.68, "text": " So that's what we can do with", "tokens": [407, 300, 311, 437, 321, 393, 360, 365], "temperature": 0.0, "avg_logprob": -0.17321467063796353, "compression_ratio": 1.5266272189349113, "no_speech_prob": 1.7061776134141837e-06}, {"id": 303, "seek": 142444, "start": 1431.56, "end": 1437.16, "text": " With that and then we could we could plot them if we wanted to I just grabbed a small subset", "tokens": [2022, 300, 293, 550, 321, 727, 321, 727, 7542, 552, 498, 321, 1415, 281, 286, 445, 18607, 257, 1359, 25993], "temperature": 0.0, "avg_logprob": -0.17321467063796353, "compression_ratio": 1.5266272189349113, "no_speech_prob": 1.7061776134141837e-06}, {"id": 304, "seek": 142444, "start": 1439.28, "end": 1441.28, "text": " To plot on those first two axes", "tokens": [1407, 7542, 322, 729, 700, 732, 35387], "temperature": 0.0, "avg_logprob": -0.17321467063796353, "compression_ratio": 1.5266272189349113, "no_speech_prob": 1.7061776134141837e-06}, {"id": 305, "seek": 142444, "start": 1442.96, "end": 1449.56, "text": " All right, so that's that so I wanted to next kind of dig in a layer deeper", "tokens": [1057, 558, 11, 370, 300, 311, 300, 370, 286, 1415, 281, 958, 733, 295, 2528, 294, 257, 4583, 7731], "temperature": 0.0, "avg_logprob": -0.17321467063796353, "compression_ratio": 1.5266272189349113, "no_speech_prob": 1.7061776134141837e-06}, {"id": 306, "seek": 142444, "start": 1450.3600000000001, "end": 1452.3600000000001, "text": " Into what actually happens?", "tokens": [23373, 437, 767, 2314, 30], "temperature": 0.0, "avg_logprob": -0.17321467063796353, "compression_ratio": 1.5266272189349113, "no_speech_prob": 1.7061776134141837e-06}, {"id": 307, "seek": 145236, "start": 1452.36, "end": 1454.36, "text": " when we say", "tokens": [562, 321, 584], "temperature": 0.0, "avg_logprob": -0.20977396965026857, "compression_ratio": 1.5810055865921788, "no_speech_prob": 6.3391435105586424e-06}, {"id": 308, "seek": 145236, "start": 1455.0, "end": 1457.52, "text": " Fit right so when we said", "tokens": [29263, 558, 370, 562, 321, 848], "temperature": 0.0, "avg_logprob": -0.20977396965026857, "compression_ratio": 1.5810055865921788, "no_speech_prob": 6.3391435105586424e-06}, {"id": 309, "seek": 145236, "start": 1458.8, "end": 1460.7199999999998, "text": " Learn dot fit", "tokens": [17216, 5893, 3318], "temperature": 0.0, "avg_logprob": -0.20977396965026857, "compression_ratio": 1.5810055865921788, "no_speech_prob": 6.3391435105586424e-06}, {"id": 310, "seek": 145236, "start": 1460.7199999999998, "end": 1462.7199999999998, "text": " What's it doing?", "tokens": [708, 311, 309, 884, 30], "temperature": 0.0, "avg_logprob": -0.20977396965026857, "compression_ratio": 1.5810055865921788, "no_speech_prob": 6.3391435105586424e-06}, {"id": 311, "seek": 145236, "start": 1464.8, "end": 1469.84, "text": " For something like the store model is it a way to interpret the embeddings", "tokens": [1171, 746, 411, 264, 3531, 2316, 307, 309, 257, 636, 281, 7302, 264, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.20977396965026857, "compression_ratio": 1.5810055865921788, "no_speech_prob": 6.3391435105586424e-06}, {"id": 312, "seek": 146984, "start": 1469.84, "end": 1484.08, "text": " For something like this the the Rossman one yeah, yeah, we'll see that in a moment. Well. Let's jump straight there. What the hell okay, so", "tokens": [1171, 746, 411, 341, 264, 264, 16140, 1601, 472, 1338, 11, 1338, 11, 321, 603, 536, 300, 294, 257, 1623, 13, 1042, 13, 961, 311, 3012, 2997, 456, 13, 708, 264, 4921, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.27203406148882053, "compression_ratio": 1.4423076923076923, "no_speech_prob": 1.4593567811971297e-06}, {"id": 313, "seek": 146984, "start": 1484.9599999999998, "end": 1486.9599999999998, "text": " So for the", "tokens": [407, 337, 264], "temperature": 0.0, "avg_logprob": -0.27203406148882053, "compression_ratio": 1.4423076923076923, "no_speech_prob": 1.4593567811971297e-06}, {"id": 314, "seek": 146984, "start": 1488.6, "end": 1493.28, "text": " Rossman how much are we going to sell at each store on each date?", "tokens": [16140, 1601, 577, 709, 366, 321, 516, 281, 3607, 412, 1184, 3531, 322, 1184, 4002, 30], "temperature": 0.0, "avg_logprob": -0.27203406148882053, "compression_ratio": 1.4423076923076923, "no_speech_prob": 1.4593567811971297e-06}, {"id": 315, "seek": 146984, "start": 1495.32, "end": 1497.32, "text": " Model", "tokens": [17105], "temperature": 0.0, "avg_logprob": -0.27203406148882053, "compression_ratio": 1.4423076923076923, "no_speech_prob": 1.4593567811971297e-06}, {"id": 316, "seek": 149732, "start": 1497.32, "end": 1499.32, "text": " we", "tokens": [321], "temperature": 0.0, "avg_logprob": -0.22238338354862097, "compression_ratio": 1.6577777777777778, "no_speech_prob": 6.540252798004076e-06}, {"id": 317, "seek": 149732, "start": 1503.08, "end": 1507.24, "text": " This is from the paper gore and Burkhan. It's a great paper by the way", "tokens": [639, 307, 490, 264, 3035, 352, 265, 293, 7031, 74, 3451, 13, 467, 311, 257, 869, 3035, 538, 264, 636], "temperature": 0.0, "avg_logprob": -0.22238338354862097, "compression_ratio": 1.6577777777777778, "no_speech_prob": 6.540252798004076e-06}, {"id": 318, "seek": 149732, "start": 1508.0, "end": 1510.84, "text": " Well worth you know like pretty accessible. I think", "tokens": [1042, 3163, 291, 458, 411, 1238, 9515, 13, 286, 519], "temperature": 0.0, "avg_logprob": -0.22238338354862097, "compression_ratio": 1.6577777777777778, "no_speech_prob": 6.540252798004076e-06}, {"id": 319, "seek": 149732, "start": 1511.76, "end": 1517.52, "text": " Any of you would at this point be able to at least get the gist of it if you know and much of the detail", "tokens": [2639, 295, 291, 576, 412, 341, 935, 312, 1075, 281, 412, 1935, 483, 264, 290, 468, 295, 309, 498, 291, 458, 293, 709, 295, 264, 2607], "temperature": 0.0, "avg_logprob": -0.22238338354862097, "compression_ratio": 1.6577777777777778, "no_speech_prob": 6.540252798004076e-06}, {"id": 320, "seek": 149732, "start": 1517.52, "end": 1520.6799999999998, "text": " As well particularly as you've also done the machine learning course", "tokens": [1018, 731, 4098, 382, 291, 600, 611, 1096, 264, 3479, 2539, 1164], "temperature": 0.0, "avg_logprob": -0.22238338354862097, "compression_ratio": 1.6577777777777778, "no_speech_prob": 6.540252798004076e-06}, {"id": 321, "seek": 149732, "start": 1521.2, "end": 1524.8799999999999, "text": " And they actually make this point in the paper. This is in the paper that", "tokens": [400, 436, 767, 652, 341, 935, 294, 264, 3035, 13, 639, 307, 294, 264, 3035, 300], "temperature": 0.0, "avg_logprob": -0.22238338354862097, "compression_ratio": 1.6577777777777778, "no_speech_prob": 6.540252798004076e-06}, {"id": 322, "seek": 152488, "start": 1524.88, "end": 1526.5200000000002, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.23763385026351266, "compression_ratio": 1.7735849056603774, "no_speech_prob": 3.041568334083422e-06}, {"id": 323, "seek": 152488, "start": 1526.5200000000002, "end": 1532.0, "text": " equivalent of what they call entity embedding layers so an embedding of a categorical variable is", "tokens": [10344, 295, 437, 436, 818, 13977, 12240, 3584, 7914, 370, 364, 12240, 3584, 295, 257, 19250, 804, 7006, 307], "temperature": 0.0, "avg_logprob": -0.23763385026351266, "compression_ratio": 1.7735849056603774, "no_speech_prob": 3.041568334083422e-06}, {"id": 324, "seek": 152488, "start": 1532.4, "end": 1534.4, "text": " identical to a one-hot encoding", "tokens": [14800, 281, 257, 472, 12, 12194, 43430], "temperature": 0.0, "avg_logprob": -0.23763385026351266, "compression_ratio": 1.7735849056603774, "no_speech_prob": 3.041568334083422e-06}, {"id": 325, "seek": 152488, "start": 1535.1200000000001, "end": 1537.1200000000001, "text": " followed by a", "tokens": [6263, 538, 257], "temperature": 0.0, "avg_logprob": -0.23763385026351266, "compression_ratio": 1.7735849056603774, "no_speech_prob": 3.041568334083422e-06}, {"id": 326, "seek": 152488, "start": 1537.3200000000002, "end": 1541.72, "text": " Matrix multiply right so they're basically saying if you've got three embeddings", "tokens": [36274, 12972, 558, 370, 436, 434, 1936, 1566, 498, 291, 600, 658, 1045, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.23763385026351266, "compression_ratio": 1.7735849056603774, "no_speech_prob": 3.041568334083422e-06}, {"id": 327, "seek": 152488, "start": 1541.72, "end": 1548.88, "text": " That's the same as doing three one-hot encodings putting each through one through a matrix multiply and then put that through a", "tokens": [663, 311, 264, 912, 382, 884, 1045, 472, 12, 12194, 2058, 378, 1109, 3372, 1184, 807, 472, 807, 257, 8141, 12972, 293, 550, 829, 300, 807, 257], "temperature": 0.0, "avg_logprob": -0.23763385026351266, "compression_ratio": 1.7735849056603774, "no_speech_prob": 3.041568334083422e-06}, {"id": 328, "seek": 152488, "start": 1549.64, "end": 1551.64, "text": " dense layer or what", "tokens": [18011, 4583, 420, 437], "temperature": 0.0, "avg_logprob": -0.23763385026351266, "compression_ratio": 1.7735849056603774, "no_speech_prob": 3.041568334083422e-06}, {"id": 329, "seek": 155164, "start": 1551.64, "end": 1555.0400000000002, "text": " Pi torch would call a linear layer right", "tokens": [17741, 27822, 576, 818, 257, 8213, 4583, 558], "temperature": 0.0, "avg_logprob": -0.19010370586990216, "compression_ratio": 1.7865612648221343, "no_speech_prob": 1.7330372656942927e-06}, {"id": 330, "seek": 155164, "start": 1556.5200000000002, "end": 1561.2, "text": " One of the nice things here is because this is kind of like well. I thought it was the first paper", "tokens": [1485, 295, 264, 1481, 721, 510, 307, 570, 341, 307, 733, 295, 411, 731, 13, 286, 1194, 309, 390, 264, 700, 3035], "temperature": 0.0, "avg_logprob": -0.19010370586990216, "compression_ratio": 1.7865612648221343, "no_speech_prob": 1.7330372656942927e-06}, {"id": 331, "seek": 155164, "start": 1561.2, "end": 1565.5200000000002, "text": " It's actually the second. I think paper to show the idea of using categorical embeddings for", "tokens": [467, 311, 767, 264, 1150, 13, 286, 519, 3035, 281, 855, 264, 1558, 295, 1228, 19250, 804, 12240, 29432, 337], "temperature": 0.0, "avg_logprob": -0.19010370586990216, "compression_ratio": 1.7865612648221343, "no_speech_prob": 1.7330372656942927e-06}, {"id": 332, "seek": 155164, "start": 1566.2, "end": 1571.44, "text": " This kind of data set they really go to quite a lot of detail to you know write back to", "tokens": [639, 733, 295, 1412, 992, 436, 534, 352, 281, 1596, 257, 688, 295, 2607, 281, 291, 458, 2464, 646, 281], "temperature": 0.0, "avg_logprob": -0.19010370586990216, "compression_ratio": 1.7865612648221343, "no_speech_prob": 1.7330372656942927e-06}, {"id": 333, "seek": 155164, "start": 1571.8400000000001, "end": 1578.24, "text": " The detailed stuff that we learned about so it's kind of a second you know a second cut at thinking about what embeddings are doing", "tokens": [440, 9942, 1507, 300, 321, 3264, 466, 370, 309, 311, 733, 295, 257, 1150, 291, 458, 257, 1150, 1723, 412, 1953, 466, 437, 12240, 29432, 366, 884], "temperature": 0.0, "avg_logprob": -0.19010370586990216, "compression_ratio": 1.7865612648221343, "no_speech_prob": 1.7330372656942927e-06}, {"id": 334, "seek": 157824, "start": 1578.24, "end": 1580.24, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.18334478355315795, "compression_ratio": 1.6699507389162562, "no_speech_prob": 1.0845116094060359e-06}, {"id": 335, "seek": 157824, "start": 1580.92, "end": 1587.84, "text": " One of the interesting things that they did was they said okay after we've trained a neural net with these embeddings", "tokens": [1485, 295, 264, 1880, 721, 300, 436, 630, 390, 436, 848, 1392, 934, 321, 600, 8895, 257, 18161, 2533, 365, 613, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.18334478355315795, "compression_ratio": 1.6699507389162562, "no_speech_prob": 1.0845116094060359e-06}, {"id": 336, "seek": 157824, "start": 1590.8, "end": 1592.8, "text": " What else could we do with it, so", "tokens": [708, 1646, 727, 321, 360, 365, 309, 11, 370], "temperature": 0.0, "avg_logprob": -0.18334478355315795, "compression_ratio": 1.6699507389162562, "no_speech_prob": 1.0845116094060359e-06}, {"id": 337, "seek": 157824, "start": 1594.68, "end": 1599.52, "text": " They got a winning result with a neural network with entity embeddings", "tokens": [814, 658, 257, 8224, 1874, 365, 257, 18161, 3209, 365, 13977, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.18334478355315795, "compression_ratio": 1.6699507389162562, "no_speech_prob": 1.0845116094060359e-06}, {"id": 338, "seek": 157824, "start": 1600.68, "end": 1602.68, "text": " But then they said hey, you know what?", "tokens": [583, 550, 436, 848, 4177, 11, 291, 458, 437, 30], "temperature": 0.0, "avg_logprob": -0.18334478355315795, "compression_ratio": 1.6699507389162562, "no_speech_prob": 1.0845116094060359e-06}, {"id": 339, "seek": 157824, "start": 1602.88, "end": 1607.16, "text": " We could take those empty embeddings and replace each categorical variable", "tokens": [492, 727, 747, 729, 6707, 12240, 29432, 293, 7406, 1184, 19250, 804, 7006], "temperature": 0.0, "avg_logprob": -0.18334478355315795, "compression_ratio": 1.6699507389162562, "no_speech_prob": 1.0845116094060359e-06}, {"id": 340, "seek": 160716, "start": 1607.16, "end": 1611.8400000000001, "text": " With the learned entity embeddings and then feed that into a GBM", "tokens": [2022, 264, 3264, 13977, 12240, 29432, 293, 550, 3154, 300, 666, 257, 460, 18345], "temperature": 0.0, "avg_logprob": -0.21316860423368567, "compression_ratio": 1.662037037037037, "no_speech_prob": 1.2878925872428226e-06}, {"id": 341, "seek": 160716, "start": 1613.3200000000002, "end": 1620.1200000000001, "text": " Right so in other words like it rather than passing into the GBM or one hot encoded version or an ordinal version", "tokens": [1779, 370, 294, 661, 2283, 411, 309, 2831, 813, 8437, 666, 264, 460, 18345, 420, 472, 2368, 2058, 12340, 3037, 420, 364, 4792, 2071, 3037], "temperature": 0.0, "avg_logprob": -0.21316860423368567, "compression_ratio": 1.662037037037037, "no_speech_prob": 1.2878925872428226e-06}, {"id": 342, "seek": 160716, "start": 1620.44, "end": 1625.44, "text": " Let's actually replace the categorical variable with its embedding", "tokens": [961, 311, 767, 7406, 264, 19250, 804, 7006, 365, 1080, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.21316860423368567, "compression_ratio": 1.662037037037037, "no_speech_prob": 1.2878925872428226e-06}, {"id": 343, "seek": 160716, "start": 1626.0400000000002, "end": 1630.48, "text": " for the appropriate level for that row right so it's actually a way of", "tokens": [337, 264, 6854, 1496, 337, 300, 5386, 558, 370, 309, 311, 767, 257, 636, 295], "temperature": 0.0, "avg_logprob": -0.21316860423368567, "compression_ratio": 1.662037037037037, "no_speech_prob": 1.2878925872428226e-06}, {"id": 344, "seek": 160716, "start": 1631.88, "end": 1634.52, "text": " Create you know feature engineering and so", "tokens": [20248, 291, 458, 4111, 7043, 293, 370], "temperature": 0.0, "avg_logprob": -0.21316860423368567, "compression_ratio": 1.662037037037037, "no_speech_prob": 1.2878925872428226e-06}, {"id": 345, "seek": 163452, "start": 1634.52, "end": 1636.52, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.23700673239571707, "compression_ratio": 1.7128712871287128, "no_speech_prob": 2.4966999490061426e-07}, {"id": 346, "seek": 163452, "start": 1636.6, "end": 1639.36, "text": " main average percent error without that", "tokens": [2135, 4274, 3043, 6713, 1553, 300], "temperature": 0.0, "avg_logprob": -0.23700673239571707, "compression_ratio": 1.7128712871287128, "no_speech_prob": 2.4966999490061426e-07}, {"id": 347, "seek": 163452, "start": 1640.28, "end": 1642.08, "text": " for GBMs", "tokens": [337, 460, 18345, 82], "temperature": 0.0, "avg_logprob": -0.23700673239571707, "compression_ratio": 1.7128712871287128, "no_speech_prob": 2.4966999490061426e-07}, {"id": 348, "seek": 163452, "start": 1642.08, "end": 1647.06, "text": " Using just one hot encodings was point one five, but with that it was point one one", "tokens": [11142, 445, 472, 2368, 2058, 378, 1109, 390, 935, 472, 1732, 11, 457, 365, 300, 309, 390, 935, 472, 472], "temperature": 0.0, "avg_logprob": -0.23700673239571707, "compression_ratio": 1.7128712871287128, "no_speech_prob": 2.4966999490061426e-07}, {"id": 349, "seek": 163452, "start": 1647.76, "end": 1655.28, "text": " Right random forests without that was point one six with that point one oh eight nearly as good as the neural net", "tokens": [1779, 4974, 21700, 1553, 300, 390, 935, 472, 2309, 365, 300, 935, 472, 1954, 3180, 6217, 382, 665, 382, 264, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.23700673239571707, "compression_ratio": 1.7128712871287128, "no_speech_prob": 2.4966999490061426e-07}, {"id": 350, "seek": 163452, "start": 1656.6, "end": 1662.98, "text": " Right so this is kind of an interesting technique because what it means is in your organization", "tokens": [1779, 370, 341, 307, 733, 295, 364, 1880, 6532, 570, 437, 309, 1355, 307, 294, 428, 4475], "temperature": 0.0, "avg_logprob": -0.23700673239571707, "compression_ratio": 1.7128712871287128, "no_speech_prob": 2.4966999490061426e-07}, {"id": 351, "seek": 166298, "start": 1662.98, "end": 1671.1, "text": " you can train a neural net that has an embedding of stores and an embedding of product types and an embedding of I", "tokens": [291, 393, 3847, 257, 18161, 2533, 300, 575, 364, 12240, 3584, 295, 9512, 293, 364, 12240, 3584, 295, 1674, 3467, 293, 364, 12240, 3584, 295, 286], "temperature": 0.0, "avg_logprob": -0.21763327411402053, "compression_ratio": 1.743083003952569, "no_speech_prob": 9.23744902792123e-09}, {"id": 352, "seek": 166298, "start": 1671.42, "end": 1674.42, "text": " Don't know whatever kind of high-catenality or even medium-catenality", "tokens": [1468, 380, 458, 2035, 733, 295, 1090, 12, 66, 7186, 1860, 420, 754, 6399, 12, 66, 7186, 1860], "temperature": 0.0, "avg_logprob": -0.21763327411402053, "compression_ratio": 1.743083003952569, "no_speech_prob": 9.23744902792123e-09}, {"id": 353, "seek": 166298, "start": 1675.1, "end": 1681.9, "text": " categorical variables you have and then everybody else in the organization can now like chuck those into their you know", "tokens": [19250, 804, 9102, 291, 362, 293, 550, 2201, 1646, 294, 264, 4475, 393, 586, 411, 20870, 729, 666, 641, 291, 458], "temperature": 0.0, "avg_logprob": -0.21763327411402053, "compression_ratio": 1.743083003952569, "no_speech_prob": 9.23744902792123e-09}, {"id": 354, "seek": 166298, "start": 1682.14, "end": 1684.66, "text": " GBM or random forest or whatever and", "tokens": [460, 18345, 420, 4974, 6719, 420, 2035, 293], "temperature": 0.0, "avg_logprob": -0.21763327411402053, "compression_ratio": 1.743083003952569, "no_speech_prob": 9.23744902792123e-09}, {"id": 355, "seek": 166298, "start": 1685.42, "end": 1691.78, "text": " And use them and what this is saying is they won't get in fact you can even use k nearest neighbors", "tokens": [400, 764, 552, 293, 437, 341, 307, 1566, 307, 436, 1582, 380, 483, 294, 1186, 291, 393, 754, 764, 350, 23831, 12512], "temperature": 0.0, "avg_logprob": -0.21763327411402053, "compression_ratio": 1.743083003952569, "no_speech_prob": 9.23744902792123e-09}, {"id": 356, "seek": 169178, "start": 1691.78, "end": 1695.7, "text": " with this technique and get nearly as good a result right so", "tokens": [365, 341, 6532, 293, 483, 6217, 382, 665, 257, 1874, 558, 370], "temperature": 0.0, "avg_logprob": -0.22071456909179688, "compression_ratio": 1.686046511627907, "no_speech_prob": 4.52093530611819e-07}, {"id": 357, "seek": 169178, "start": 1696.82, "end": 1701.5, "text": " This is a good way of kind of giving the power of neural nets to everybody in your organization", "tokens": [639, 307, 257, 665, 636, 295, 733, 295, 2902, 264, 1347, 295, 18161, 36170, 281, 2201, 294, 428, 4475], "temperature": 0.0, "avg_logprob": -0.22071456909179688, "compression_ratio": 1.686046511627907, "no_speech_prob": 4.52093530611819e-07}, {"id": 358, "seek": 169178, "start": 1702.46, "end": 1704.46, "text": " without having them do the", "tokens": [1553, 1419, 552, 360, 264], "temperature": 0.0, "avg_logprob": -0.22071456909179688, "compression_ratio": 1.686046511627907, "no_speech_prob": 4.52093530611819e-07}, {"id": 359, "seek": 169178, "start": 1704.58, "end": 1711.98, "text": " Fastai deep learning course first you know they can just use whatever sk learn or R or whatever that they're used to and like those those", "tokens": [15968, 1301, 2452, 2539, 1164, 700, 291, 458, 436, 393, 445, 764, 2035, 1110, 1466, 420, 497, 420, 2035, 300, 436, 434, 1143, 281, 293, 411, 729, 729], "temperature": 0.0, "avg_logprob": -0.22071456909179688, "compression_ratio": 1.686046511627907, "no_speech_prob": 4.52093530611819e-07}, {"id": 360, "seek": 169178, "start": 1712.58, "end": 1718.1, "text": " Embeddings could literally be in a database table because if you think about an embedding is just an index lookup", "tokens": [24234, 292, 29432, 727, 3736, 312, 294, 257, 8149, 3199, 570, 498, 291, 519, 466, 364, 12240, 3584, 307, 445, 364, 8186, 574, 1010], "temperature": 0.0, "avg_logprob": -0.22071456909179688, "compression_ratio": 1.686046511627907, "no_speech_prob": 4.52093530611819e-07}, {"id": 361, "seek": 171810, "start": 1718.1, "end": 1723.34, "text": " Right which is the same as an inner join in SQL right?", "tokens": [1779, 597, 307, 264, 912, 382, 364, 7284, 3917, 294, 19200, 558, 30], "temperature": 0.0, "avg_logprob": -0.16638384583175822, "compression_ratio": 1.690909090909091, "no_speech_prob": 1.172639827018429e-06}, {"id": 362, "seek": 171810, "start": 1723.34, "end": 1727.04, "text": " So if you've got a table of each product along with its embedding vector", "tokens": [407, 498, 291, 600, 658, 257, 3199, 295, 1184, 1674, 2051, 365, 1080, 12240, 3584, 8062], "temperature": 0.0, "avg_logprob": -0.16638384583175822, "compression_ratio": 1.690909090909091, "no_speech_prob": 1.172639827018429e-06}, {"id": 363, "seek": 171810, "start": 1727.54, "end": 1733.9199999999998, "text": " Then you can literally do an inner join and now you have every row in your table along with its product embedding vector", "tokens": [1396, 291, 393, 3736, 360, 364, 7284, 3917, 293, 586, 291, 362, 633, 5386, 294, 428, 3199, 2051, 365, 1080, 1674, 12240, 3584, 8062], "temperature": 0.0, "avg_logprob": -0.16638384583175822, "compression_ratio": 1.690909090909091, "no_speech_prob": 1.172639827018429e-06}, {"id": 364, "seek": 171810, "start": 1734.4599999999998, "end": 1738.3799999999999, "text": " so that's a really this is this is a really useful idea and", "tokens": [370, 300, 311, 257, 534, 341, 307, 341, 307, 257, 534, 4420, 1558, 293], "temperature": 0.0, "avg_logprob": -0.16638384583175822, "compression_ratio": 1.690909090909091, "no_speech_prob": 1.172639827018429e-06}, {"id": 365, "seek": 171810, "start": 1740.02, "end": 1744.32, "text": " GBMs and random forests learn a lot quicker than neural nets do", "tokens": [460, 18345, 82, 293, 4974, 21700, 1466, 257, 688, 16255, 813, 18161, 36170, 360], "temperature": 0.0, "avg_logprob": -0.16638384583175822, "compression_ratio": 1.690909090909091, "no_speech_prob": 1.172639827018429e-06}, {"id": 366, "seek": 174432, "start": 1744.32, "end": 1750.1599999999999, "text": " All right, so that's like even if you do know how to train neural nets. This is still potentially quite handy", "tokens": [1057, 558, 11, 370, 300, 311, 411, 754, 498, 291, 360, 458, 577, 281, 3847, 18161, 36170, 13, 639, 307, 920, 7263, 1596, 13239], "temperature": 0.0, "avg_logprob": -0.16707727644178602, "compression_ratio": 1.7272727272727273, "no_speech_prob": 7.112424782462767e-07}, {"id": 367, "seek": 174432, "start": 1751.6399999999999, "end": 1756.2, "text": " so here's what happened when they took the various different states of Germany and", "tokens": [370, 510, 311, 437, 2011, 562, 436, 1890, 264, 3683, 819, 4368, 295, 7244, 293], "temperature": 0.0, "avg_logprob": -0.16707727644178602, "compression_ratio": 1.7272727272727273, "no_speech_prob": 7.112424782462767e-07}, {"id": 368, "seek": 174432, "start": 1756.8, "end": 1760.52, "text": " plotted the first two principal components of their embedding vectors and", "tokens": [43288, 264, 700, 732, 9716, 6677, 295, 641, 12240, 3584, 18875, 293], "temperature": 0.0, "avg_logprob": -0.16707727644178602, "compression_ratio": 1.7272727272727273, "no_speech_prob": 7.112424782462767e-07}, {"id": 369, "seek": 174432, "start": 1760.8799999999999, "end": 1764.6, "text": " they basically here is where they were in that 2d space and", "tokens": [436, 1936, 510, 307, 689, 436, 645, 294, 300, 568, 67, 1901, 293], "temperature": 0.0, "avg_logprob": -0.16707727644178602, "compression_ratio": 1.7272727272727273, "no_speech_prob": 7.112424782462767e-07}, {"id": 370, "seek": 174432, "start": 1765.3999999999999, "end": 1767.96, "text": " Wackily enough I've circled in red", "tokens": [343, 501, 953, 1547, 286, 600, 3510, 1493, 294, 2182], "temperature": 0.0, "avg_logprob": -0.16707727644178602, "compression_ratio": 1.7272727272727273, "no_speech_prob": 7.112424782462767e-07}, {"id": 371, "seek": 176796, "start": 1767.96, "end": 1774.8, "text": " Three cities and I've circled here the three cities in Germany and here I've circled in purple. Sorry blue", "tokens": [6244, 6486, 293, 286, 600, 3510, 1493, 510, 264, 1045, 6486, 294, 7244, 293, 510, 286, 600, 3510, 1493, 294, 9656, 13, 4919, 3344], "temperature": 0.0, "avg_logprob": -0.20098758273654513, "compression_ratio": 1.6325581395348838, "no_speech_prob": 2.48246965384169e-06}, {"id": 372, "seek": 176796, "start": 1774.96, "end": 1779.44, "text": " Here are the blue is the green is the green so it's actually", "tokens": [1692, 366, 264, 3344, 307, 264, 3092, 307, 264, 3092, 370, 309, 311, 767], "temperature": 0.0, "avg_logprob": -0.20098758273654513, "compression_ratio": 1.6325581395348838, "no_speech_prob": 2.48246965384169e-06}, {"id": 373, "seek": 176796, "start": 1781.0, "end": 1785.74, "text": " Drawn a map of Germany even though it never was told anything about", "tokens": [15971, 895, 257, 4471, 295, 7244, 754, 1673, 309, 1128, 390, 1907, 1340, 466], "temperature": 0.0, "avg_logprob": -0.20098758273654513, "compression_ratio": 1.6325581395348838, "no_speech_prob": 2.48246965384169e-06}, {"id": 374, "seek": 176796, "start": 1786.48, "end": 1790.92, "text": " How far these states are away from each other or the very concept of geography didn't exist?", "tokens": [1012, 1400, 613, 4368, 366, 1314, 490, 1184, 661, 420, 264, 588, 3410, 295, 26695, 994, 380, 2514, 30], "temperature": 0.0, "avg_logprob": -0.20098758273654513, "compression_ratio": 1.6325581395348838, "no_speech_prob": 2.48246965384169e-06}, {"id": 375, "seek": 176796, "start": 1791.76, "end": 1793.6000000000001, "text": " So that's pretty", "tokens": [407, 300, 311, 1238], "temperature": 0.0, "avg_logprob": -0.20098758273654513, "compression_ratio": 1.6325581395348838, "no_speech_prob": 2.48246965384169e-06}, {"id": 376, "seek": 176796, "start": 1793.6000000000001, "end": 1795.6000000000001, "text": " crazy", "tokens": [3219], "temperature": 0.0, "avg_logprob": -0.20098758273654513, "compression_ratio": 1.6325581395348838, "no_speech_prob": 2.48246965384169e-06}, {"id": 377, "seek": 179560, "start": 1795.6, "end": 1799.9199999999998, "text": " So that was from their paper, so I went ahead and looked", "tokens": [407, 300, 390, 490, 641, 3035, 11, 370, 286, 1437, 2286, 293, 2956], "temperature": 0.0, "avg_logprob": -0.20324373245239258, "compression_ratio": 1.6091370558375635, "no_speech_prob": 3.446549271757249e-06}, {"id": 378, "seek": 179560, "start": 1801.12, "end": 1805.56, "text": " Well here's another thing. I think this is also from their paper. They took every pair of", "tokens": [1042, 510, 311, 1071, 551, 13, 286, 519, 341, 307, 611, 490, 641, 3035, 13, 814, 1890, 633, 6119, 295], "temperature": 0.0, "avg_logprob": -0.20324373245239258, "compression_ratio": 1.6091370558375635, "no_speech_prob": 3.446549271757249e-06}, {"id": 379, "seek": 179560, "start": 1807.32, "end": 1812.1599999999999, "text": " Places and they looked at how far away they are on a map", "tokens": [2149, 2116, 293, 436, 2956, 412, 577, 1400, 1314, 436, 366, 322, 257, 4471], "temperature": 0.0, "avg_logprob": -0.20324373245239258, "compression_ratio": 1.6091370558375635, "no_speech_prob": 3.446549271757249e-06}, {"id": 380, "seek": 179560, "start": 1812.8799999999999, "end": 1817.9199999999998, "text": " Versus how far away are they in embedding space and they got this beautiful", "tokens": [12226, 301, 577, 1400, 1314, 366, 436, 294, 12240, 3584, 1901, 293, 436, 658, 341, 2238], "temperature": 0.0, "avg_logprob": -0.20324373245239258, "compression_ratio": 1.6091370558375635, "no_speech_prob": 3.446549271757249e-06}, {"id": 381, "seek": 179560, "start": 1819.1999999999998, "end": 1821.54, "text": " Correlation right so again it kind of", "tokens": [3925, 4419, 399, 558, 370, 797, 309, 733, 295], "temperature": 0.0, "avg_logprob": -0.20324373245239258, "compression_ratio": 1.6091370558375635, "no_speech_prob": 3.446549271757249e-06}, {"id": 382, "seek": 182154, "start": 1821.54, "end": 1825.22, "text": " Apparently you know stores that are nearby each other", "tokens": [16755, 291, 458, 9512, 300, 366, 11184, 1184, 661], "temperature": 0.0, "avg_logprob": -0.20161078373591104, "compression_ratio": 1.657258064516129, "no_speech_prob": 2.443974381094449e-06}, {"id": 383, "seek": 182154, "start": 1827.54, "end": 1829.54, "text": " Physically have similar", "tokens": [15542, 984, 362, 2531], "temperature": 0.0, "avg_logprob": -0.20161078373591104, "compression_ratio": 1.657258064516129, "no_speech_prob": 2.443974381094449e-06}, {"id": 384, "seek": 182154, "start": 1830.86, "end": 1834.1599999999999, "text": " Characteristics in terms of when people buy more or less stuff from them", "tokens": [36786, 6006, 294, 2115, 295, 562, 561, 2256, 544, 420, 1570, 1507, 490, 552], "temperature": 0.0, "avg_logprob": -0.20161078373591104, "compression_ratio": 1.657258064516129, "no_speech_prob": 2.443974381094449e-06}, {"id": 385, "seek": 182154, "start": 1835.26, "end": 1841.82, "text": " So I looked at the same thing for days of the week right so here's an embedding of the days of the week", "tokens": [407, 286, 2956, 412, 264, 912, 551, 337, 1708, 295, 264, 1243, 558, 370, 510, 311, 364, 12240, 3584, 295, 264, 1708, 295, 264, 1243], "temperature": 0.0, "avg_logprob": -0.20161078373591104, "compression_ratio": 1.657258064516129, "no_speech_prob": 2.443974381094449e-06}, {"id": 386, "seek": 182154, "start": 1842.3799999999999, "end": 1847.1599999999999, "text": " From our model, and I just kind of joined up Monday Tuesday Wednesday Tuesday Thursday Friday Saturday Sunday", "tokens": [3358, 527, 2316, 11, 293, 286, 445, 733, 295, 6869, 493, 8138, 10017, 10579, 10017, 10383, 6984, 8803, 7776], "temperature": 0.0, "avg_logprob": -0.20161078373591104, "compression_ratio": 1.657258064516129, "no_speech_prob": 2.443974381094449e-06}, {"id": 387, "seek": 182154, "start": 1847.1599999999999, "end": 1849.58, "text": " I do the same thing for the months of the year", "tokens": [286, 360, 264, 912, 551, 337, 264, 2493, 295, 264, 1064], "temperature": 0.0, "avg_logprob": -0.20161078373591104, "compression_ratio": 1.657258064516129, "no_speech_prob": 2.443974381094449e-06}, {"id": 388, "seek": 184958, "start": 1849.58, "end": 1853.26, "text": " All right again. You can say you know here's here's winter", "tokens": [1057, 558, 797, 13, 509, 393, 584, 291, 458, 510, 311, 510, 311, 6355], "temperature": 0.0, "avg_logprob": -0.18873333178068463, "compression_ratio": 1.705607476635514, "no_speech_prob": 3.089476422246662e-06}, {"id": 389, "seek": 184958, "start": 1853.98, "end": 1855.98, "text": " Here's summer", "tokens": [1692, 311, 4266], "temperature": 0.0, "avg_logprob": -0.18873333178068463, "compression_ratio": 1.705607476635514, "no_speech_prob": 3.089476422246662e-06}, {"id": 390, "seek": 184958, "start": 1856.6999999999998, "end": 1859.04, "text": " So yeah, I think like", "tokens": [407, 1338, 11, 286, 519, 411], "temperature": 0.0, "avg_logprob": -0.18873333178068463, "compression_ratio": 1.705607476635514, "no_speech_prob": 3.089476422246662e-06}, {"id": 391, "seek": 184958, "start": 1860.6599999999999, "end": 1864.6999999999998, "text": " Visualizing embeddings can be interesting like it's good to like first of all check", "tokens": [23187, 3319, 12240, 29432, 393, 312, 1880, 411, 309, 311, 665, 281, 411, 700, 295, 439, 1520], "temperature": 0.0, "avg_logprob": -0.18873333178068463, "compression_ratio": 1.705607476635514, "no_speech_prob": 3.089476422246662e-06}, {"id": 392, "seek": 184958, "start": 1865.3799999999999, "end": 1868.46, "text": " You can see things you would expect to see you know", "tokens": [509, 393, 536, 721, 291, 576, 2066, 281, 536, 291, 458], "temperature": 0.0, "avg_logprob": -0.18873333178068463, "compression_ratio": 1.705607476635514, "no_speech_prob": 3.089476422246662e-06}, {"id": 393, "seek": 184958, "start": 1868.82, "end": 1874.1399999999999, "text": " And then you could like try and see like maybe things you didn't expect to see so you could try all kinds of", "tokens": [400, 550, 291, 727, 411, 853, 293, 536, 411, 1310, 721, 291, 994, 380, 2066, 281, 536, 370, 291, 727, 853, 439, 3685, 295], "temperature": 0.0, "avg_logprob": -0.18873333178068463, "compression_ratio": 1.705607476635514, "no_speech_prob": 3.089476422246662e-06}, {"id": 394, "seek": 187414, "start": 1874.14, "end": 1879.46, "text": " Cluster rings or or whatever right? And", "tokens": [2033, 8393, 11136, 420, 420, 2035, 558, 30, 400], "temperature": 0.0, "avg_logprob": -0.25935595968495245, "compression_ratio": 1.6167400881057268, "no_speech_prob": 2.2959061425353866e-06}, {"id": 395, "seek": 187414, "start": 1880.3000000000002, "end": 1882.3000000000002, "text": " This is not something which has been", "tokens": [639, 307, 406, 746, 597, 575, 668], "temperature": 0.0, "avg_logprob": -0.25935595968495245, "compression_ratio": 1.6167400881057268, "no_speech_prob": 2.2959061425353866e-06}, {"id": 396, "seek": 187414, "start": 1883.3000000000002, "end": 1889.0600000000002, "text": " Widely studied at all right, so I'm not going to tell you what the limitations are of this technique or whatever", "tokens": [28331, 736, 9454, 412, 439, 558, 11, 370, 286, 478, 406, 516, 281, 980, 291, 437, 264, 15705, 366, 295, 341, 6532, 420, 2035], "temperature": 0.0, "avg_logprob": -0.25935595968495245, "compression_ratio": 1.6167400881057268, "no_speech_prob": 2.2959061425353866e-06}, {"id": 397, "seek": 187414, "start": 1891.5400000000002, "end": 1896.42, "text": " Yes, so I've heard of other ways to generate embeddings like skip grams", "tokens": [1079, 11, 370, 286, 600, 2198, 295, 661, 2098, 281, 8460, 12240, 29432, 411, 10023, 11899], "temperature": 0.0, "avg_logprob": -0.25935595968495245, "compression_ratio": 1.6167400881057268, "no_speech_prob": 2.2959061425353866e-06}, {"id": 398, "seek": 187414, "start": 1896.42, "end": 1903.3400000000001, "text": " Uh-huh, so I mean if you could say is there one better than the other using neural networks or skip grams", "tokens": [4019, 12, 18710, 11, 370, 286, 914, 498, 291, 727, 584, 307, 456, 472, 1101, 813, 264, 661, 1228, 18161, 9590, 420, 10023, 11899], "temperature": 0.0, "avg_logprob": -0.25935595968495245, "compression_ratio": 1.6167400881057268, "no_speech_prob": 2.2959061425353866e-06}, {"id": 399, "seek": 190334, "start": 1903.34, "end": 1907.74, "text": " Um so script grams is quite specific to NLP right so like", "tokens": [3301, 370, 5755, 11899, 307, 1596, 2685, 281, 426, 45196, 558, 370, 411], "temperature": 0.0, "avg_logprob": -0.2282568613688151, "compression_ratio": 1.4731182795698925, "no_speech_prob": 1.8738688822850236e-06}, {"id": 400, "seek": 190334, "start": 1910.3799999999999, "end": 1913.22, "text": " I'm not sure if we'll cover it in this course, but basically", "tokens": [286, 478, 406, 988, 498, 321, 603, 2060, 309, 294, 341, 1164, 11, 457, 1936], "temperature": 0.0, "avg_logprob": -0.2282568613688151, "compression_ratio": 1.4731182795698925, "no_speech_prob": 1.8738688822850236e-06}, {"id": 401, "seek": 190334, "start": 1914.6599999999999, "end": 1920.6999999999998, "text": " The the approach to original kind of word to vec approach to generating embeddings was to say", "tokens": [440, 264, 3109, 281, 3380, 733, 295, 1349, 281, 42021, 3109, 281, 17746, 12240, 29432, 390, 281, 584], "temperature": 0.0, "avg_logprob": -0.2282568613688151, "compression_ratio": 1.4731182795698925, "no_speech_prob": 1.8738688822850236e-06}, {"id": 402, "seek": 190334, "start": 1921.8999999999999, "end": 1924.22, "text": " You know what we actually don't have", "tokens": [509, 458, 437, 321, 767, 500, 380, 362], "temperature": 0.0, "avg_logprob": -0.2282568613688151, "compression_ratio": 1.4731182795698925, "no_speech_prob": 1.8738688822850236e-06}, {"id": 403, "seek": 190334, "start": 1926.82, "end": 1928.82, "text": " We don't actually have a", "tokens": [492, 500, 380, 767, 362, 257], "temperature": 0.0, "avg_logprob": -0.2282568613688151, "compression_ratio": 1.4731182795698925, "no_speech_prob": 1.8738688822850236e-06}, {"id": 404, "seek": 192882, "start": 1928.82, "end": 1933.1399999999999, "text": " Labeled data set you know they said all we have is like Google Books", "tokens": [10137, 31689, 1412, 992, 291, 458, 436, 848, 439, 321, 362, 307, 411, 3329, 33843], "temperature": 0.0, "avg_logprob": -0.1724622090657552, "compression_ratio": 1.793991416309013, "no_speech_prob": 8.579201562497474e-07}, {"id": 405, "seek": 192882, "start": 1933.1399999999999, "end": 1937.7, "text": " And so they have an unsupervised learning problem unlabeled problem", "tokens": [400, 370, 436, 362, 364, 2693, 12879, 24420, 2539, 1154, 32118, 18657, 292, 1154], "temperature": 0.0, "avg_logprob": -0.1724622090657552, "compression_ratio": 1.793991416309013, "no_speech_prob": 8.579201562497474e-07}, {"id": 406, "seek": 192882, "start": 1937.7, "end": 1944.26, "text": " And so the best way in my opinion to turn an unlabeled problem into a labeled problem is to kind of invent some labels", "tokens": [400, 370, 264, 1151, 636, 294, 452, 4800, 281, 1261, 364, 32118, 18657, 292, 1154, 666, 257, 21335, 1154, 307, 281, 733, 295, 7962, 512, 16949], "temperature": 0.0, "avg_logprob": -0.1724622090657552, "compression_ratio": 1.793991416309013, "no_speech_prob": 8.579201562497474e-07}, {"id": 407, "seek": 192882, "start": 1944.26, "end": 1950.6599999999999, "text": " And so what they did in the word to vec case was they said okay? Here's a sentence with 11 words in it", "tokens": [400, 370, 437, 436, 630, 294, 264, 1349, 281, 42021, 1389, 390, 436, 848, 1392, 30, 1692, 311, 257, 8174, 365, 2975, 2283, 294, 309], "temperature": 0.0, "avg_logprob": -0.1724622090657552, "compression_ratio": 1.793991416309013, "no_speech_prob": 8.579201562497474e-07}, {"id": 408, "seek": 192882, "start": 1951.46, "end": 1954.98, "text": " Right and then they said okay. Let's delete the middle word", "tokens": [1779, 293, 550, 436, 848, 1392, 13, 961, 311, 12097, 264, 2808, 1349], "temperature": 0.0, "avg_logprob": -0.1724622090657552, "compression_ratio": 1.793991416309013, "no_speech_prob": 8.579201562497474e-07}, {"id": 409, "seek": 195498, "start": 1954.98, "end": 1956.98, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.24766815862348002, "compression_ratio": 1.5957446808510638, "no_speech_prob": 2.8130064038123237e-06}, {"id": 410, "seek": 195498, "start": 1958.6200000000001, "end": 1961.5, "text": " Replace it with a random word and", "tokens": [1300, 6742, 309, 365, 257, 4974, 1349, 293], "temperature": 0.0, "avg_logprob": -0.24766815862348002, "compression_ratio": 1.5957446808510638, "no_speech_prob": 2.8130064038123237e-06}, {"id": 411, "seek": 195498, "start": 1962.5, "end": 1963.8600000000001, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.24766815862348002, "compression_ratio": 1.5957446808510638, "no_speech_prob": 2.8130064038123237e-06}, {"id": 412, "seek": 195498, "start": 1963.8600000000001, "end": 1970.14, "text": " You know originally it said cat and they say no let's replace that with", "tokens": [509, 458, 7993, 309, 848, 3857, 293, 436, 584, 572, 718, 311, 7406, 300, 365], "temperature": 0.0, "avg_logprob": -0.24766815862348002, "compression_ratio": 1.5957446808510638, "no_speech_prob": 2.8130064038123237e-06}, {"id": 413, "seek": 195498, "start": 1971.14, "end": 1973.14, "text": " justice", "tokens": [6118], "temperature": 0.0, "avg_logprob": -0.24766815862348002, "compression_ratio": 1.5957446808510638, "no_speech_prob": 2.8130064038123237e-06}, {"id": 414, "seek": 195498, "start": 1973.14, "end": 1980.1200000000001, "text": " Right so before it said the cute little cat sat on the fuzzy mat and now it says the cute little justice", "tokens": [1779, 370, 949, 309, 848, 264, 4052, 707, 3857, 3227, 322, 264, 34710, 3803, 293, 586, 309, 1619, 264, 4052, 707, 6118], "temperature": 0.0, "avg_logprob": -0.24766815862348002, "compression_ratio": 1.5957446808510638, "no_speech_prob": 2.8130064038123237e-06}, {"id": 415, "seek": 198012, "start": 1980.12, "end": 1985.78, "text": " Sat on the fuzzy mat right and what they do is they do that so they have one sentence", "tokens": [5344, 322, 264, 34710, 3803, 558, 293, 437, 436, 360, 307, 436, 360, 300, 370, 436, 362, 472, 8174], "temperature": 0.0, "avg_logprob": -0.18215171604940336, "compression_ratio": 1.7546012269938651, "no_speech_prob": 6.083583912186441e-07}, {"id": 416, "seek": 198012, "start": 1986.04, "end": 1988.1799999999998, "text": " Where they keep exactly as is", "tokens": [2305, 436, 1066, 2293, 382, 307], "temperature": 0.0, "avg_logprob": -0.18215171604940336, "compression_ratio": 1.7546012269938651, "no_speech_prob": 6.083583912186441e-07}, {"id": 417, "seek": 198012, "start": 1991.4399999999998, "end": 1997.2399999999998, "text": " Right and then they make a copy of it and they do the replacement and so then they have a label", "tokens": [1779, 293, 550, 436, 652, 257, 5055, 295, 309, 293, 436, 360, 264, 14419, 293, 370, 550, 436, 362, 257, 7645], "temperature": 0.0, "avg_logprob": -0.18215171604940336, "compression_ratio": 1.7546012269938651, "no_speech_prob": 6.083583912186441e-07}, {"id": 418, "seek": 198012, "start": 1998.0, "end": 2003.3999999999999, "text": " Where they say it's a one if it was unchanged it was the original and zero", "tokens": [2305, 436, 584, 309, 311, 257, 472, 498, 309, 390, 44553, 309, 390, 264, 3380, 293, 4018], "temperature": 0.0, "avg_logprob": -0.18215171604940336, "compression_ratio": 1.7546012269938651, "no_speech_prob": 6.083583912186441e-07}, {"id": 419, "seek": 200340, "start": 2003.4, "end": 2010.64, "text": " Otherwise right and so basically then you now have something you can build a machine learning model on", "tokens": [10328, 558, 293, 370, 1936, 550, 291, 586, 362, 746, 291, 393, 1322, 257, 3479, 2539, 2316, 322], "temperature": 0.0, "avg_logprob": -0.19053109821520353, "compression_ratio": 1.8464912280701755, "no_speech_prob": 6.577918156835949e-07}, {"id": 420, "seek": 200340, "start": 2011.5600000000002, "end": 2016.5600000000002, "text": " And so they went and built a machine learning model on this so the model was like try and find the", "tokens": [400, 370, 436, 1437, 293, 3094, 257, 3479, 2539, 2316, 322, 341, 370, 264, 2316, 390, 411, 853, 293, 915, 264], "temperature": 0.0, "avg_logprob": -0.19053109821520353, "compression_ratio": 1.8464912280701755, "no_speech_prob": 6.577918156835949e-07}, {"id": 421, "seek": 200340, "start": 2017.44, "end": 2019.44, "text": " effect sentences", "tokens": [1802, 16579], "temperature": 0.0, "avg_logprob": -0.19053109821520353, "compression_ratio": 1.8464912280701755, "no_speech_prob": 6.577918156835949e-07}, {"id": 422, "seek": 200340, "start": 2020.2800000000002, "end": 2023.8400000000001, "text": " Not because they were interested in a fake sentence finder", "tokens": [1726, 570, 436, 645, 3102, 294, 257, 7592, 8174, 915, 260], "temperature": 0.0, "avg_logprob": -0.19053109821520353, "compression_ratio": 1.8464912280701755, "no_speech_prob": 6.577918156835949e-07}, {"id": 423, "seek": 200340, "start": 2023.8400000000001, "end": 2028.5600000000002, "text": " But because as a result they now have embeddings that just like we discussed you can now use for other purposes", "tokens": [583, 570, 382, 257, 1874, 436, 586, 362, 12240, 29432, 300, 445, 411, 321, 7152, 291, 393, 586, 764, 337, 661, 9932], "temperature": 0.0, "avg_logprob": -0.19053109821520353, "compression_ratio": 1.8464912280701755, "no_speech_prob": 6.577918156835949e-07}, {"id": 424, "seek": 200340, "start": 2028.8000000000002, "end": 2030.8000000000002, "text": " And that became word to vec now", "tokens": [400, 300, 3062, 1349, 281, 42021, 586], "temperature": 0.0, "avg_logprob": -0.19053109821520353, "compression_ratio": 1.8464912280701755, "no_speech_prob": 6.577918156835949e-07}, {"id": 425, "seek": 203080, "start": 2030.8, "end": 2035.68, "text": " it turns out that if you do this as just a kind of a", "tokens": [309, 4523, 484, 300, 498, 291, 360, 341, 382, 445, 257, 733, 295, 257], "temperature": 0.0, "avg_logprob": -0.20286217141658702, "compression_ratio": 1.7106382978723405, "no_speech_prob": 6.276691237872001e-07}, {"id": 426, "seek": 203080, "start": 2037.0, "end": 2042.04, "text": " Effectively like a single matrix multiply rather than make it a deep neural net you can train this super quickly", "tokens": [17764, 3413, 411, 257, 2167, 8141, 12972, 2831, 813, 652, 309, 257, 2452, 18161, 2533, 291, 393, 3847, 341, 1687, 2661], "temperature": 0.0, "avg_logprob": -0.20286217141658702, "compression_ratio": 1.7106382978723405, "no_speech_prob": 6.276691237872001e-07}, {"id": 427, "seek": 203080, "start": 2044.0, "end": 2048.48, "text": " And so that's basically what they did with they met there though they kind of decided we're going to make a", "tokens": [400, 370, 300, 311, 1936, 437, 436, 630, 365, 436, 1131, 456, 1673, 436, 733, 295, 3047, 321, 434, 516, 281, 652, 257], "temperature": 0.0, "avg_logprob": -0.20286217141658702, "compression_ratio": 1.7106382978723405, "no_speech_prob": 6.276691237872001e-07}, {"id": 428, "seek": 203080, "start": 2049.56, "end": 2053.44, "text": " Pretty crappy model like a shallow learning model rather than a deep model", "tokens": [10693, 36531, 2316, 411, 257, 20488, 2539, 2316, 2831, 813, 257, 2452, 2316], "temperature": 0.0, "avg_logprob": -0.20286217141658702, "compression_ratio": 1.7106382978723405, "no_speech_prob": 6.276691237872001e-07}, {"id": 429, "seek": 203080, "start": 2054.24, "end": 2056.74, "text": " You know with the downside it's a less powerful model", "tokens": [509, 458, 365, 264, 25060, 309, 311, 257, 1570, 4005, 2316], "temperature": 0.0, "avg_logprob": -0.20286217141658702, "compression_ratio": 1.7106382978723405, "no_speech_prob": 6.276691237872001e-07}, {"id": 430, "seek": 205674, "start": 2056.74, "end": 2061.6, "text": " but a number of upsides the first thing we can train it on a really large data set and", "tokens": [457, 257, 1230, 295, 15497, 1875, 264, 700, 551, 321, 393, 3847, 309, 322, 257, 534, 2416, 1412, 992, 293], "temperature": 0.0, "avg_logprob": -0.16908160845438638, "compression_ratio": 1.7307692307692308, "no_speech_prob": 6.681499371552491e-07}, {"id": 431, "seek": 205674, "start": 2062.08, "end": 2066.9599999999996, "text": " Then also really importantly we're going to end up with embeddings which have really", "tokens": [1396, 611, 534, 8906, 321, 434, 516, 281, 917, 493, 365, 12240, 29432, 597, 362, 534], "temperature": 0.0, "avg_logprob": -0.16908160845438638, "compression_ratio": 1.7307692307692308, "no_speech_prob": 6.681499371552491e-07}, {"id": 432, "seek": 205674, "start": 2067.72, "end": 2073.64, "text": " Very linear characteristics, so we can like add them together and subtract them and stuff like that, right?", "tokens": [4372, 8213, 10891, 11, 370, 321, 393, 411, 909, 552, 1214, 293, 16390, 552, 293, 1507, 411, 300, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16908160845438638, "compression_ratio": 1.7307692307692308, "no_speech_prob": 6.681499371552491e-07}, {"id": 433, "seek": 205674, "start": 2075.3999999999996, "end": 2076.72, "text": " So that", "tokens": [407, 300], "temperature": 0.0, "avg_logprob": -0.16908160845438638, "compression_ratio": 1.7307692307692308, "no_speech_prob": 6.681499371552491e-07}, {"id": 434, "seek": 205674, "start": 2076.72, "end": 2083.4199999999996, "text": " So there's a lot of stuff we can learn about there from like for other types of embedding like categorical embeddings", "tokens": [407, 456, 311, 257, 688, 295, 1507, 321, 393, 1466, 466, 456, 490, 411, 337, 661, 3467, 295, 12240, 3584, 411, 19250, 804, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.16908160845438638, "compression_ratio": 1.7307692307692308, "no_speech_prob": 6.681499371552491e-07}, {"id": 435, "seek": 208342, "start": 2083.42, "end": 2087.46, "text": " specifically if we want categorical embeddings which we can kind of", "tokens": [4682, 498, 321, 528, 19250, 804, 12240, 29432, 597, 321, 393, 733, 295], "temperature": 0.0, "avg_logprob": -0.20957699450817738, "compression_ratio": 1.6869565217391305, "no_speech_prob": 1.7061629478121176e-06}, {"id": 436, "seek": 208342, "start": 2088.3, "end": 2093.14, "text": " Draw nicely and expect them to ask to be able to add and subtract them and behave linearly", "tokens": [20386, 9594, 293, 2066, 552, 281, 1029, 281, 312, 1075, 281, 909, 293, 16390, 552, 293, 15158, 43586], "temperature": 0.0, "avg_logprob": -0.20957699450817738, "compression_ratio": 1.6869565217391305, "no_speech_prob": 1.7061629478121176e-06}, {"id": 437, "seek": 208342, "start": 2094.42, "end": 2098.02, "text": " You know probably if we want to use them in K nearest neighbors and stuff", "tokens": [509, 458, 1391, 498, 321, 528, 281, 764, 552, 294, 591, 23831, 12512, 293, 1507], "temperature": 0.0, "avg_logprob": -0.20957699450817738, "compression_ratio": 1.6869565217391305, "no_speech_prob": 1.7061629478121176e-06}, {"id": 438, "seek": 208342, "start": 2098.82, "end": 2100.82, "text": " We should probably use shallow learning", "tokens": [492, 820, 1391, 764, 20488, 2539], "temperature": 0.0, "avg_logprob": -0.20957699450817738, "compression_ratio": 1.6869565217391305, "no_speech_prob": 1.7061629478121176e-06}, {"id": 439, "seek": 208342, "start": 2102.02, "end": 2107.54, "text": " If we want something that's going to be more predictive we probably want to use a neural net", "tokens": [759, 321, 528, 746, 300, 311, 516, 281, 312, 544, 35521, 321, 1391, 528, 281, 764, 257, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.20957699450817738, "compression_ratio": 1.6869565217391305, "no_speech_prob": 1.7061629478121176e-06}, {"id": 440, "seek": 208342, "start": 2108.2200000000003, "end": 2110.2200000000003, "text": " And so actually in NLP", "tokens": [400, 370, 767, 294, 426, 45196], "temperature": 0.0, "avg_logprob": -0.20957699450817738, "compression_ratio": 1.6869565217391305, "no_speech_prob": 1.7061629478121176e-06}, {"id": 441, "seek": 211022, "start": 2110.22, "end": 2112.22, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.23472046852111816, "compression_ratio": 1.6788617886178863, "no_speech_prob": 2.857291519831051e-06}, {"id": 442, "seek": 211022, "start": 2112.3399999999997, "end": 2115.8599999999997, "text": " Am really pushing the idea that we need to move past", "tokens": [2012, 534, 7380, 264, 1558, 300, 321, 643, 281, 1286, 1791], "temperature": 0.0, "avg_logprob": -0.23472046852111816, "compression_ratio": 1.6788617886178863, "no_speech_prob": 2.857291519831051e-06}, {"id": 443, "seek": 211022, "start": 2116.18, "end": 2120.9399999999996, "text": " Word to back and glove these linear based methods because it turns out that those embeddings", "tokens": [8725, 281, 646, 293, 26928, 613, 8213, 2361, 7150, 570, 309, 4523, 484, 300, 729, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.23472046852111816, "compression_ratio": 1.6788617886178863, "no_speech_prob": 2.857291519831051e-06}, {"id": 444, "seek": 211022, "start": 2121.8199999999997, "end": 2125.62, "text": " Way less predictive than embeddings learned from deep models", "tokens": [9558, 1570, 35521, 813, 12240, 29432, 3264, 490, 2452, 5245], "temperature": 0.0, "avg_logprob": -0.23472046852111816, "compression_ratio": 1.6788617886178863, "no_speech_prob": 2.857291519831051e-06}, {"id": 445, "seek": 211022, "start": 2125.62, "end": 2130.7599999999998, "text": " And so the language model that we learned about which ended up getting a state-of-the-art on sentiment analysis", "tokens": [400, 370, 264, 2856, 2316, 300, 321, 3264, 466, 597, 4590, 493, 1242, 257, 1785, 12, 2670, 12, 3322, 12, 446, 322, 16149, 5215], "temperature": 0.0, "avg_logprob": -0.23472046852111816, "compression_ratio": 1.6788617886178863, "no_speech_prob": 2.857291519831051e-06}, {"id": 446, "seek": 211022, "start": 2130.9399999999996, "end": 2136.3799999999997, "text": " Didn't use glove or word to back that instead we pre trained a deep recurrent neural network", "tokens": [11151, 380, 764, 26928, 420, 1349, 281, 646, 300, 2602, 321, 659, 8895, 257, 2452, 18680, 1753, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.23472046852111816, "compression_ratio": 1.6788617886178863, "no_speech_prob": 2.857291519831051e-06}, {"id": 447, "seek": 213638, "start": 2136.38, "end": 2141.3, "text": " And we ended up with not just a pre trained word vectors, but a full pre trained", "tokens": [400, 321, 4590, 493, 365, 406, 445, 257, 659, 8895, 1349, 18875, 11, 457, 257, 1577, 659, 8895], "temperature": 0.0, "avg_logprob": -0.17230515838951194, "compression_ratio": 1.6883720930232557, "no_speech_prob": 6.083572543502669e-07}, {"id": 448, "seek": 213638, "start": 2142.3, "end": 2144.3, "text": " model", "tokens": [2316], "temperature": 0.0, "avg_logprob": -0.17230515838951194, "compression_ratio": 1.6883720930232557, "no_speech_prob": 6.083572543502669e-07}, {"id": 449, "seek": 213638, "start": 2144.98, "end": 2149.9, "text": " So it looks like to create embeddings for entities we need like a dummy task", "tokens": [407, 309, 1542, 411, 281, 1884, 12240, 29432, 337, 16667, 321, 643, 411, 257, 35064, 5633], "temperature": 0.0, "avg_logprob": -0.17230515838951194, "compression_ratio": 1.6883720930232557, "no_speech_prob": 6.083572543502669e-07}, {"id": 450, "seek": 213638, "start": 2150.62, "end": 2157.1800000000003, "text": " Not necessarily a dummy task like in this case. We had a real task right so we created the embeddings for Rossman by trying to predict", "tokens": [1726, 4725, 257, 35064, 5633, 411, 294, 341, 1389, 13, 492, 632, 257, 957, 5633, 558, 370, 321, 2942, 264, 12240, 29432, 337, 16140, 1601, 538, 1382, 281, 6069], "temperature": 0.0, "avg_logprob": -0.17230515838951194, "compression_ratio": 1.6883720930232557, "no_speech_prob": 6.083572543502669e-07}, {"id": 451, "seek": 213638, "start": 2157.7000000000003, "end": 2159.7000000000003, "text": " store sales", "tokens": [3531, 5763], "temperature": 0.0, "avg_logprob": -0.17230515838951194, "compression_ratio": 1.6883720930232557, "no_speech_prob": 6.083572543502669e-07}, {"id": 452, "seek": 213638, "start": 2159.7000000000003, "end": 2162.9, "text": " You only need this isn't just in this isn't just for", "tokens": [509, 787, 643, 341, 1943, 380, 445, 294, 341, 1943, 380, 445, 337], "temperature": 0.0, "avg_logprob": -0.17230515838951194, "compression_ratio": 1.6883720930232557, "no_speech_prob": 6.083572543502669e-07}, {"id": 453, "seek": 216290, "start": 2162.9, "end": 2166.94, "text": " Learning embeddings for learning any kind of feature space", "tokens": [15205, 12240, 29432, 337, 2539, 604, 733, 295, 4111, 1901], "temperature": 0.0, "avg_logprob": -0.168188983743841, "compression_ratio": 1.714975845410628, "no_speech_prob": 1.4367406038218178e-06}, {"id": 454, "seek": 216290, "start": 2168.86, "end": 2171.26, "text": " You either need labeled data or", "tokens": [509, 2139, 643, 21335, 1412, 420], "temperature": 0.0, "avg_logprob": -0.168188983743841, "compression_ratio": 1.714975845410628, "no_speech_prob": 1.4367406038218178e-06}, {"id": 455, "seek": 216290, "start": 2172.26, "end": 2176.02, "text": " You need to invent some kind of fake task", "tokens": [509, 643, 281, 7962, 512, 733, 295, 7592, 5633], "temperature": 0.0, "avg_logprob": -0.168188983743841, "compression_ratio": 1.714975845410628, "no_speech_prob": 1.4367406038218178e-06}, {"id": 456, "seek": 216290, "start": 2176.34, "end": 2182.34, "text": " So does that task matter like if I choose a task and train embeddings if I choose another task and train embeddings", "tokens": [407, 775, 300, 5633, 1871, 411, 498, 286, 2826, 257, 5633, 293, 3847, 12240, 29432, 498, 286, 2826, 1071, 5633, 293, 3847, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.168188983743841, "compression_ratio": 1.714975845410628, "no_speech_prob": 1.4367406038218178e-06}, {"id": 457, "seek": 216290, "start": 2183.06, "end": 2185.06, "text": " Like which one is?", "tokens": [1743, 597, 472, 307, 30], "temperature": 0.0, "avg_logprob": -0.168188983743841, "compression_ratio": 1.714975845410628, "no_speech_prob": 1.4367406038218178e-06}, {"id": 458, "seek": 216290, "start": 2185.6600000000003, "end": 2190.6600000000003, "text": " It's a great question, and it's not something that's been studied nearly enough, right?", "tokens": [467, 311, 257, 869, 1168, 11, 293, 309, 311, 406, 746, 300, 311, 668, 9454, 6217, 1547, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.168188983743841, "compression_ratio": 1.714975845410628, "no_speech_prob": 1.4367406038218178e-06}, {"id": 459, "seek": 219066, "start": 2190.66, "end": 2194.7, "text": " I'm not sure that many people even quite understand that when they say", "tokens": [286, 478, 406, 988, 300, 867, 561, 754, 1596, 1223, 300, 562, 436, 584], "temperature": 0.0, "avg_logprob": -0.198765686580113, "compression_ratio": 1.616504854368932, "no_speech_prob": 8.579220889259886e-07}, {"id": 460, "seek": 219066, "start": 2195.3799999999997, "end": 2199.66, "text": " Unsupervised learning now that nowadays they almost nearly always mean", "tokens": [25017, 12879, 24420, 2539, 586, 300, 13434, 436, 1920, 6217, 1009, 914], "temperature": 0.0, "avg_logprob": -0.198765686580113, "compression_ratio": 1.616504854368932, "no_speech_prob": 8.579220889259886e-07}, {"id": 461, "seek": 219066, "start": 2200.2999999999997, "end": 2202.2999999999997, "text": " fake task", "tokens": [7592, 5633], "temperature": 0.0, "avg_logprob": -0.198765686580113, "compression_ratio": 1.616504854368932, "no_speech_prob": 8.579220889259886e-07}, {"id": 462, "seek": 219066, "start": 2202.3799999999997, "end": 2205.46, "text": " Labeled learning and so the idea of like", "tokens": [10137, 31689, 2539, 293, 370, 264, 1558, 295, 411], "temperature": 0.0, "avg_logprob": -0.198765686580113, "compression_ratio": 1.616504854368932, "no_speech_prob": 8.579220889259886e-07}, {"id": 463, "seek": 219066, "start": 2206.1, "end": 2210.62, "text": " What makes a good fake task? I don't know that I've seen a paper on that right but", "tokens": [708, 1669, 257, 665, 7592, 5633, 30, 286, 500, 380, 458, 300, 286, 600, 1612, 257, 3035, 322, 300, 558, 457], "temperature": 0.0, "avg_logprob": -0.198765686580113, "compression_ratio": 1.616504854368932, "no_speech_prob": 8.579220889259886e-07}, {"id": 464, "seek": 219066, "start": 2211.2599999999998, "end": 2213.2599999999998, "text": " intuitively you know", "tokens": [46506, 291, 458], "temperature": 0.0, "avg_logprob": -0.198765686580113, "compression_ratio": 1.616504854368932, "no_speech_prob": 8.579220889259886e-07}, {"id": 465, "seek": 219066, "start": 2213.7, "end": 2216.2999999999997, "text": " We need something where the kinds of", "tokens": [492, 643, 746, 689, 264, 3685, 295], "temperature": 0.0, "avg_logprob": -0.198765686580113, "compression_ratio": 1.616504854368932, "no_speech_prob": 8.579220889259886e-07}, {"id": 466, "seek": 221630, "start": 2216.3, "end": 2223.54, "text": " Relationships that's going to learn likely to be the kinds of relationships that you probably care about right so", "tokens": [28663, 7640, 300, 311, 516, 281, 1466, 3700, 281, 312, 264, 3685, 295, 6159, 300, 291, 1391, 1127, 466, 558, 370], "temperature": 0.0, "avg_logprob": -0.1785244439777575, "compression_ratio": 1.6417910447761195, "no_speech_prob": 1.6028060372264008e-06}, {"id": 467, "seek": 221630, "start": 2224.5800000000004, "end": 2226.5800000000004, "text": " for example in", "tokens": [337, 1365, 294], "temperature": 0.0, "avg_logprob": -0.1785244439777575, "compression_ratio": 1.6417910447761195, "no_speech_prob": 1.6028060372264008e-06}, {"id": 468, "seek": 221630, "start": 2227.9, "end": 2233.1800000000003, "text": " In computer vision one kind of fake task people use is to say like", "tokens": [682, 3820, 5201, 472, 733, 295, 7592, 5633, 561, 764, 307, 281, 584, 411], "temperature": 0.0, "avg_logprob": -0.1785244439777575, "compression_ratio": 1.6417910447761195, "no_speech_prob": 1.6028060372264008e-06}, {"id": 469, "seek": 221630, "start": 2234.3, "end": 2237.0600000000004, "text": " let's take some images and use some kind of like", "tokens": [718, 311, 747, 512, 5267, 293, 764, 512, 733, 295, 411], "temperature": 0.0, "avg_logprob": -0.1785244439777575, "compression_ratio": 1.6417910447761195, "no_speech_prob": 1.6028060372264008e-06}, {"id": 470, "seek": 221630, "start": 2238.38, "end": 2244.02, "text": " Unreal and unreasonable data augmentation like like recolor them too much or whatever", "tokens": [34464, 293, 41730, 1412, 14501, 19631, 411, 411, 850, 36182, 552, 886, 709, 420, 2035], "temperature": 0.0, "avg_logprob": -0.1785244439777575, "compression_ratio": 1.6417910447761195, "no_speech_prob": 1.6028060372264008e-06}, {"id": 471, "seek": 224402, "start": 2244.02, "end": 2249.62, "text": " And then we'll ask the neural net to like predict which one was the augmented which one was not the augmented", "tokens": [400, 550, 321, 603, 1029, 264, 18161, 2533, 281, 411, 6069, 597, 472, 390, 264, 36155, 597, 472, 390, 406, 264, 36155], "temperature": 0.0, "avg_logprob": -0.17023063428474194, "compression_ratio": 1.7739130434782608, "no_speech_prob": 1.0845072893062024e-06}, {"id": 472, "seek": 224402, "start": 2254.38, "end": 2259.06, "text": " Yeah, so it's it I think it's a fascinating area and one which", "tokens": [865, 11, 370, 309, 311, 309, 286, 519, 309, 311, 257, 10343, 1859, 293, 472, 597], "temperature": 0.0, "avg_logprob": -0.17023063428474194, "compression_ratio": 1.7739130434782608, "no_speech_prob": 1.0845072893062024e-06}, {"id": 473, "seek": 224402, "start": 2260.02, "end": 2262.98, "text": " You know would be really interesting for people to you know", "tokens": [509, 458, 576, 312, 534, 1880, 337, 561, 281, 291, 458], "temperature": 0.0, "avg_logprob": -0.17023063428474194, "compression_ratio": 1.7739130434782608, "no_speech_prob": 1.0845072893062024e-06}, {"id": 474, "seek": 224402, "start": 2262.98, "end": 2266.08, "text": " Maybe some of the students here to look into you further is like take some interesting", "tokens": [2704, 512, 295, 264, 1731, 510, 281, 574, 666, 291, 3052, 307, 411, 747, 512, 1880], "temperature": 0.0, "avg_logprob": -0.17023063428474194, "compression_ratio": 1.7739130434782608, "no_speech_prob": 1.0845072893062024e-06}, {"id": 475, "seek": 224402, "start": 2266.58, "end": 2269.9, "text": " semi-supervised or unsupervised data sets and try and come up with some like", "tokens": [12909, 12, 48172, 24420, 420, 2693, 12879, 24420, 1412, 6352, 293, 853, 293, 808, 493, 365, 512, 411], "temperature": 0.0, "avg_logprob": -0.17023063428474194, "compression_ratio": 1.7739130434782608, "no_speech_prob": 1.0845072893062024e-06}, {"id": 476, "seek": 224402, "start": 2271.06, "end": 2272.98, "text": " more clever", "tokens": [544, 13494], "temperature": 0.0, "avg_logprob": -0.17023063428474194, "compression_ratio": 1.7739130434782608, "no_speech_prob": 1.0845072893062024e-06}, {"id": 477, "seek": 227298, "start": 2272.98, "end": 2277.36, "text": " Fake tasks and see like does it matter you know how much does it matter?", "tokens": [40469, 9608, 293, 536, 411, 775, 309, 1871, 291, 458, 577, 709, 775, 309, 1871, 30], "temperature": 0.0, "avg_logprob": -0.21433816000679942, "compression_ratio": 1.6901960784313725, "no_speech_prob": 2.9023019578744425e-06}, {"id": 478, "seek": 227298, "start": 2278.02, "end": 2284.0, "text": " In general like if you can't come up with a fake task that you think seems great. I would say use it", "tokens": [682, 2674, 411, 498, 291, 393, 380, 808, 493, 365, 257, 7592, 5633, 300, 291, 519, 2544, 869, 13, 286, 576, 584, 764, 309], "temperature": 0.0, "avg_logprob": -0.21433816000679942, "compression_ratio": 1.6901960784313725, "no_speech_prob": 2.9023019578744425e-06}, {"id": 479, "seek": 227298, "start": 2284.58, "end": 2287.48, "text": " Use the best you can it's often surprising how?", "tokens": [8278, 264, 1151, 291, 393, 309, 311, 2049, 8830, 577, 30], "temperature": 0.0, "avg_logprob": -0.21433816000679942, "compression_ratio": 1.6901960784313725, "no_speech_prob": 2.9023019578744425e-06}, {"id": 480, "seek": 227298, "start": 2288.3, "end": 2293.62, "text": " How little you need like the ultimately crappy fake task is called the auto encoder", "tokens": [1012, 707, 291, 643, 411, 264, 6284, 36531, 7592, 5633, 307, 1219, 264, 8399, 2058, 19866], "temperature": 0.0, "avg_logprob": -0.21433816000679942, "compression_ratio": 1.6901960784313725, "no_speech_prob": 2.9023019578744425e-06}, {"id": 481, "seek": 227298, "start": 2293.98, "end": 2295.98, "text": " All right, and the auto encoder?", "tokens": [1057, 558, 11, 293, 264, 8399, 2058, 19866, 30], "temperature": 0.0, "avg_logprob": -0.21433816000679942, "compression_ratio": 1.6901960784313725, "no_speech_prob": 2.9023019578744425e-06}, {"id": 482, "seek": 227298, "start": 2296.42, "end": 2301.86, "text": " Is the thing which which won the claims prediction competition that just finished on Kaggle?", "tokens": [1119, 264, 551, 597, 597, 1582, 264, 9441, 17630, 6211, 300, 445, 4335, 322, 48751, 22631, 30], "temperature": 0.0, "avg_logprob": -0.21433816000679942, "compression_ratio": 1.6901960784313725, "no_speech_prob": 2.9023019578744425e-06}, {"id": 483, "seek": 230186, "start": 2301.86, "end": 2304.6600000000003, "text": " they had lots of examples of", "tokens": [436, 632, 3195, 295, 5110, 295], "temperature": 0.0, "avg_logprob": -0.18326853761578552, "compression_ratio": 1.8166666666666667, "no_speech_prob": 6.786724497942487e-07}, {"id": 484, "seek": 230186, "start": 2305.58, "end": 2312.6600000000003, "text": " Insurance policies where we knew this was how much was claimed and then lots of examples of insurance policies where I guess they must have been still", "tokens": [39971, 7657, 689, 321, 2586, 341, 390, 577, 709, 390, 12941, 293, 550, 3195, 295, 5110, 295, 7214, 7657, 689, 286, 2041, 436, 1633, 362, 668, 920], "temperature": 0.0, "avg_logprob": -0.18326853761578552, "compression_ratio": 1.8166666666666667, "no_speech_prob": 6.786724497942487e-07}, {"id": 485, "seek": 230186, "start": 2313.1400000000003, "end": 2319.2200000000003, "text": " Still open we didn't yet know how much they claimed right and so what they did was they said okay?", "tokens": [8291, 1269, 321, 994, 380, 1939, 458, 577, 709, 436, 12941, 558, 293, 370, 437, 436, 630, 390, 436, 848, 1392, 30], "temperature": 0.0, "avg_logprob": -0.18326853761578552, "compression_ratio": 1.8166666666666667, "no_speech_prob": 6.786724497942487e-07}, {"id": 486, "seek": 230186, "start": 2319.2200000000003, "end": 2324.82, "text": " So for all of the ones so let's basically start off by grabbing every policy right and we'll take a single policy", "tokens": [407, 337, 439, 295, 264, 2306, 370, 718, 311, 1936, 722, 766, 538, 23771, 633, 3897, 558, 293, 321, 603, 747, 257, 2167, 3897], "temperature": 0.0, "avg_logprob": -0.18326853761578552, "compression_ratio": 1.8166666666666667, "no_speech_prob": 6.786724497942487e-07}, {"id": 487, "seek": 230186, "start": 2324.9, "end": 2328.7000000000003, "text": " And we'll put it through a neural net right", "tokens": [400, 321, 603, 829, 309, 807, 257, 18161, 2533, 558], "temperature": 0.0, "avg_logprob": -0.18326853761578552, "compression_ratio": 1.8166666666666667, "no_speech_prob": 6.786724497942487e-07}, {"id": 488, "seek": 232870, "start": 2328.7, "end": 2333.18, "text": " And we'll try and have it reconstruct itself", "tokens": [400, 321, 603, 853, 293, 362, 309, 31499, 2564], "temperature": 0.0, "avg_logprob": -0.19304956708635604, "compression_ratio": 1.644736842105263, "no_speech_prob": 1.4367467429110548e-06}, {"id": 489, "seek": 232870, "start": 2335.02, "end": 2341.18, "text": " But in these intermediate layers at least one of those intermediate layers will make sure there's less activations", "tokens": [583, 294, 613, 19376, 7914, 412, 1935, 472, 295, 729, 19376, 7914, 486, 652, 988, 456, 311, 1570, 2430, 763], "temperature": 0.0, "avg_logprob": -0.19304956708635604, "compression_ratio": 1.644736842105263, "no_speech_prob": 1.4367467429110548e-06}, {"id": 490, "seek": 232870, "start": 2341.58, "end": 2346.72, "text": " Than they were input so let's say if there was a hundred variables on the insurance policy", "tokens": [18289, 436, 645, 4846, 370, 718, 311, 584, 498, 456, 390, 257, 3262, 9102, 322, 264, 7214, 3897], "temperature": 0.0, "avg_logprob": -0.19304956708635604, "compression_ratio": 1.644736842105263, "no_speech_prob": 1.4367467429110548e-06}, {"id": 491, "seek": 232870, "start": 2347.06, "end": 2350.5, "text": " You know we'll have something in the middle that only has like 20 activations", "tokens": [509, 458, 321, 603, 362, 746, 294, 264, 2808, 300, 787, 575, 411, 945, 2430, 763], "temperature": 0.0, "avg_logprob": -0.19304956708635604, "compression_ratio": 1.644736842105263, "no_speech_prob": 1.4367467429110548e-06}, {"id": 492, "seek": 232870, "start": 2351.9399999999996, "end": 2355.24, "text": " Right and so when you basically are saying hey", "tokens": [1779, 293, 370, 562, 291, 1936, 366, 1566, 4177], "temperature": 0.0, "avg_logprob": -0.19304956708635604, "compression_ratio": 1.644736842105263, "no_speech_prob": 1.4367467429110548e-06}, {"id": 493, "seek": 235524, "start": 2355.24, "end": 2360.72, "text": " Reconstruct your own input like it's not a different kind of model doesn't require any special code", "tokens": [1300, 25279, 1757, 428, 1065, 4846, 411, 309, 311, 406, 257, 819, 733, 295, 2316, 1177, 380, 3651, 604, 2121, 3089], "temperature": 0.0, "avg_logprob": -0.19255406669016634, "compression_ratio": 1.6136363636363635, "no_speech_prob": 8.059425340434245e-07}, {"id": 494, "seek": 235524, "start": 2361.68, "end": 2366.7999999999997, "text": " It's literally just passing you can use any standard pytorch or fast AI learner", "tokens": [467, 311, 3736, 445, 8437, 291, 393, 764, 604, 3832, 25878, 284, 339, 420, 2370, 7318, 33347], "temperature": 0.0, "avg_logprob": -0.19255406669016634, "compression_ratio": 1.6136363636363635, "no_speech_prob": 8.059425340434245e-07}, {"id": 495, "seek": 235524, "start": 2367.0, "end": 2372.2999999999997, "text": " You just say my output equals my input right and that's that's like the", "tokens": [509, 445, 584, 452, 5598, 6915, 452, 4846, 558, 293, 300, 311, 300, 311, 411, 264], "temperature": 0.0, "avg_logprob": -0.19255406669016634, "compression_ratio": 1.6136363636363635, "no_speech_prob": 8.059425340434245e-07}, {"id": 496, "seek": 235524, "start": 2373.0, "end": 2374.4799999999996, "text": " the most", "tokens": [264, 881], "temperature": 0.0, "avg_logprob": -0.19255406669016634, "compression_ratio": 1.6136363636363635, "no_speech_prob": 8.059425340434245e-07}, {"id": 497, "seek": 235524, "start": 2374.4799999999996, "end": 2379.4799999999996, "text": " Uncreated you know invented task you can create and that's called an auto encoder and it works", "tokens": [1156, 66, 26559, 291, 458, 14479, 5633, 291, 393, 1884, 293, 300, 311, 1219, 364, 8399, 2058, 19866, 293, 309, 1985], "temperature": 0.0, "avg_logprob": -0.19255406669016634, "compression_ratio": 1.6136363636363635, "no_speech_prob": 8.059425340434245e-07}, {"id": 498, "seek": 237948, "start": 2379.48, "end": 2386.28, "text": " Surprisingly well in fact the point that it literally just won a Kaggle competition they took the features that it learnt and", "tokens": [49908, 731, 294, 1186, 264, 935, 300, 309, 3736, 445, 1582, 257, 48751, 22631, 6211, 436, 1890, 264, 4122, 300, 309, 18991, 293], "temperature": 0.0, "avg_logprob": -0.2520140150318975, "compression_ratio": 1.63135593220339, "no_speech_prob": 3.0894752853782848e-06}, {"id": 499, "seek": 237948, "start": 2386.56, "end": 2387.96, "text": " Chucked it into", "tokens": [21607, 292, 309, 666], "temperature": 0.0, "avg_logprob": -0.2520140150318975, "compression_ratio": 1.63135593220339, "no_speech_prob": 3.0894752853782848e-06}, {"id": 500, "seek": 237948, "start": 2387.96, "end": 2389.72, "text": " another neural net and", "tokens": [1071, 18161, 2533, 293], "temperature": 0.0, "avg_logprob": -0.2520140150318975, "compression_ratio": 1.63135593220339, "no_speech_prob": 3.0894752853782848e-06}, {"id": 501, "seek": 237948, "start": 2389.72, "end": 2391.72, "text": " Yeah, and one", "tokens": [865, 11, 293, 472], "temperature": 0.0, "avg_logprob": -0.2520140150318975, "compression_ratio": 1.63135593220339, "no_speech_prob": 3.0894752853782848e-06}, {"id": 502, "seek": 237948, "start": 2393.04, "end": 2397.0, "text": " You know maybe if we have enough students taking an interest in this then", "tokens": [509, 458, 1310, 498, 321, 362, 1547, 1731, 1940, 364, 1179, 294, 341, 550], "temperature": 0.0, "avg_logprob": -0.2520140150318975, "compression_ratio": 1.63135593220339, "no_speech_prob": 3.0894752853782848e-06}, {"id": 503, "seek": 237948, "start": 2398.32, "end": 2404.18, "text": " You know we'll be able to cover cover unsupervised learning in more detail in part two especially given this Kaggle", "tokens": [509, 458, 321, 603, 312, 1075, 281, 2060, 2060, 2693, 12879, 24420, 2539, 294, 544, 2607, 294, 644, 732, 2318, 2212, 341, 48751, 22631], "temperature": 0.0, "avg_logprob": -0.2520140150318975, "compression_ratio": 1.63135593220339, "no_speech_prob": 3.0894752853782848e-06}, {"id": 504, "seek": 240418, "start": 2404.18, "end": 2408.98, "text": " recent have a win I", "tokens": [5162, 362, 257, 1942, 286], "temperature": 0.0, "avg_logprob": -0.39545921043113424, "compression_ratio": 1.6231884057971016, "no_speech_prob": 4.63778496850864e-06}, {"id": 505, "seek": 240418, "start": 2410.2599999999998, "end": 2415.02, "text": " Think this may be related to the previous question when training language models", "tokens": [6557, 341, 815, 312, 4077, 281, 264, 3894, 1168, 562, 3097, 2856, 5245], "temperature": 0.0, "avg_logprob": -0.39545921043113424, "compression_ratio": 1.6231884057971016, "no_speech_prob": 4.63778496850864e-06}, {"id": 506, "seek": 240418, "start": 2416.2599999999998, "end": 2423.14, "text": " Is the language model fix I'm a train on the archive data is that useful at all in the movie lens movie like the movie?", "tokens": [1119, 264, 2856, 2316, 3191, 286, 478, 257, 3847, 322, 264, 23507, 1412, 307, 300, 4420, 412, 439, 294, 264, 3169, 6765, 3169, 411, 264, 3169, 30], "temperature": 0.0, "avg_logprob": -0.39545921043113424, "compression_ratio": 1.6231884057971016, "no_speech_prob": 4.63778496850864e-06}, {"id": 507, "seek": 240418, "start": 2423.14, "end": 2426.8199999999997, "text": " I am DB data great question. You know I was just", "tokens": [286, 669, 26754, 1412, 869, 1168, 13, 509, 458, 286, 390, 445], "temperature": 0.0, "avg_logprob": -0.39545921043113424, "compression_ratio": 1.6231884057971016, "no_speech_prob": 4.63778496850864e-06}, {"id": 508, "seek": 240418, "start": 2427.8199999999997, "end": 2430.96, "text": " Talking to Sebastian about this best in route about this this week", "tokens": [22445, 281, 31102, 466, 341, 1151, 294, 7955, 466, 341, 341, 1243], "temperature": 0.0, "avg_logprob": -0.39545921043113424, "compression_ratio": 1.6231884057971016, "no_speech_prob": 4.63778496850864e-06}, {"id": 509, "seek": 243096, "start": 2430.96, "end": 2436.54, "text": " And we thought we'd try and do some research on this in January it's it's again. It's not well-known", "tokens": [400, 321, 1194, 321, 1116, 853, 293, 360, 512, 2132, 322, 341, 294, 7061, 309, 311, 309, 311, 797, 13, 467, 311, 406, 731, 12, 6861], "temperature": 0.0, "avg_logprob": -0.1623906498461698, "compression_ratio": 1.641304347826087, "no_speech_prob": 2.8129950351285515e-06}, {"id": 510, "seek": 243096, "start": 2436.7, "end": 2438.7, "text": " We know that in computer vision", "tokens": [492, 458, 300, 294, 3820, 5201], "temperature": 0.0, "avg_logprob": -0.1623906498461698, "compression_ratio": 1.641304347826087, "no_speech_prob": 2.8129950351285515e-06}, {"id": 511, "seek": 243096, "start": 2438.94, "end": 2446.26, "text": " It's shockingly effective to train on cats and dogs and use that pre-trained network to do", "tokens": [467, 311, 5588, 12163, 4942, 281, 3847, 322, 11111, 293, 7197, 293, 764, 300, 659, 12, 17227, 2001, 3209, 281, 360], "temperature": 0.0, "avg_logprob": -0.1623906498461698, "compression_ratio": 1.641304347826087, "no_speech_prob": 2.8129950351285515e-06}, {"id": 512, "seek": 243096, "start": 2446.5, "end": 2448.66, "text": " lung cancer diagnosis and CT scans", "tokens": [16730, 5592, 15217, 293, 19529, 35116], "temperature": 0.0, "avg_logprob": -0.1623906498461698, "compression_ratio": 1.641304347826087, "no_speech_prob": 2.8129950351285515e-06}, {"id": 513, "seek": 243096, "start": 2449.26, "end": 2451.26, "text": " in the NLP world", "tokens": [294, 264, 426, 45196, 1002], "temperature": 0.0, "avg_logprob": -0.1623906498461698, "compression_ratio": 1.641304347826087, "no_speech_prob": 2.8129950351285515e-06}, {"id": 514, "seek": 243096, "start": 2451.5, "end": 2455.18, "text": " Nobody much seems to have tried this the NLP researchers", "tokens": [9297, 709, 2544, 281, 362, 3031, 341, 264, 426, 45196, 10309], "temperature": 0.0, "avg_logprob": -0.1623906498461698, "compression_ratio": 1.641304347826087, "no_speech_prob": 2.8129950351285515e-06}, {"id": 515, "seek": 245518, "start": 2455.18, "end": 2462.0, "text": " I've spoken to others than Sebastian about this assume that it wouldn't work, and they generally haven't bothered trying. I think it would work great", "tokens": [286, 600, 10759, 281, 2357, 813, 31102, 466, 341, 6552, 300, 309, 2759, 380, 589, 11, 293, 436, 5101, 2378, 380, 22996, 1382, 13, 286, 519, 309, 576, 589, 869], "temperature": 0.0, "avg_logprob": -0.20625258506612576, "compression_ratio": 1.6528925619834711, "no_speech_prob": 1.8162063497584313e-06}, {"id": 516, "seek": 245518, "start": 2463.98, "end": 2465.98, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.20625258506612576, "compression_ratio": 1.6528925619834711, "no_speech_prob": 1.8162063497584313e-06}, {"id": 517, "seek": 245518, "start": 2465.98, "end": 2467.98, "text": " So since we're talking about Rossman", "tokens": [407, 1670, 321, 434, 1417, 466, 16140, 1601], "temperature": 0.0, "avg_logprob": -0.20625258506612576, "compression_ratio": 1.6528925619834711, "no_speech_prob": 1.8162063497584313e-06}, {"id": 518, "seek": 245518, "start": 2469.7, "end": 2477.3999999999996, "text": " I've just mentioned during the week. I was interested to see like how good this solution actually actually was", "tokens": [286, 600, 445, 2835, 1830, 264, 1243, 13, 286, 390, 3102, 281, 536, 411, 577, 665, 341, 3827, 767, 767, 390], "temperature": 0.0, "avg_logprob": -0.20625258506612576, "compression_ratio": 1.6528925619834711, "no_speech_prob": 1.8162063497584313e-06}, {"id": 519, "seek": 245518, "start": 2478.3799999999997, "end": 2481.8599999999997, "text": " Because I noticed that on the public leaderboard. It didn't look like it was going to be that great", "tokens": [1436, 286, 5694, 300, 322, 264, 1908, 5263, 3787, 13, 467, 994, 380, 574, 411, 309, 390, 516, 281, 312, 300, 869], "temperature": 0.0, "avg_logprob": -0.20625258506612576, "compression_ratio": 1.6528925619834711, "no_speech_prob": 1.8162063497584313e-06}, {"id": 520, "seek": 248186, "start": 2481.86, "end": 2484.6600000000003, "text": " and I also thought it'd be good to see like", "tokens": [293, 286, 611, 1194, 309, 1116, 312, 665, 281, 536, 411], "temperature": 0.0, "avg_logprob": -0.1746948787144252, "compression_ratio": 1.7455197132616487, "no_speech_prob": 2.40608528656594e-06}, {"id": 521, "seek": 248186, "start": 2485.26, "end": 2487.26, "text": " What does it actually take to?", "tokens": [708, 775, 309, 767, 747, 281, 30], "temperature": 0.0, "avg_logprob": -0.1746948787144252, "compression_ratio": 1.7455197132616487, "no_speech_prob": 2.40608528656594e-06}, {"id": 522, "seek": 248186, "start": 2487.42, "end": 2490.86, "text": " Use a test set properly with this kind of structured data", "tokens": [8278, 257, 1500, 992, 6108, 365, 341, 733, 295, 18519, 1412], "temperature": 0.0, "avg_logprob": -0.1746948787144252, "compression_ratio": 1.7455197132616487, "no_speech_prob": 2.40608528656594e-06}, {"id": 523, "seek": 248186, "start": 2490.86, "end": 2492.6800000000003, "text": " So if you have a look at Rossman now", "tokens": [407, 498, 291, 362, 257, 574, 412, 16140, 1601, 586], "temperature": 0.0, "avg_logprob": -0.1746948787144252, "compression_ratio": 1.7455197132616487, "no_speech_prob": 2.40608528656594e-06}, {"id": 524, "seek": 248186, "start": 2492.6800000000003, "end": 2496.2200000000003, "text": " I've pushed some changes that actually run the test set through as well", "tokens": [286, 600, 9152, 512, 2962, 300, 767, 1190, 264, 1500, 992, 807, 382, 731], "temperature": 0.0, "avg_logprob": -0.1746948787144252, "compression_ratio": 1.7455197132616487, "no_speech_prob": 2.40608528656594e-06}, {"id": 525, "seek": 248186, "start": 2496.2200000000003, "end": 2498.2200000000003, "text": " And so you can get a sense of how to do this", "tokens": [400, 370, 291, 393, 483, 257, 2020, 295, 577, 281, 360, 341], "temperature": 0.0, "avg_logprob": -0.1746948787144252, "compression_ratio": 1.7455197132616487, "no_speech_prob": 2.40608528656594e-06}, {"id": 526, "seek": 248186, "start": 2498.3, "end": 2502.3, "text": " so you'll see basically every line appears twice one for test and", "tokens": [370, 291, 603, 536, 1936, 633, 1622, 7038, 6091, 472, 337, 1500, 293], "temperature": 0.0, "avg_logprob": -0.1746948787144252, "compression_ratio": 1.7455197132616487, "no_speech_prob": 2.40608528656594e-06}, {"id": 527, "seek": 248186, "start": 2502.98, "end": 2504.6200000000003, "text": " one for", "tokens": [472, 337], "temperature": 0.0, "avg_logprob": -0.1746948787144252, "compression_ratio": 1.7455197132616487, "no_speech_prob": 2.40608528656594e-06}, {"id": 528, "seek": 248186, "start": 2504.6200000000003, "end": 2511.1, "text": " One for train when we get there yeah test train test train test train obviously you could do this in a lot fewer lines of code", "tokens": [1485, 337, 3847, 562, 321, 483, 456, 1338, 1500, 3847, 1500, 3847, 1500, 3847, 2745, 291, 727, 360, 341, 294, 257, 688, 13366, 3876, 295, 3089], "temperature": 0.0, "avg_logprob": -0.1746948787144252, "compression_ratio": 1.7455197132616487, "no_speech_prob": 2.40608528656594e-06}, {"id": 529, "seek": 251110, "start": 2511.1, "end": 2518.38, "text": " By putting all of the steps into a method and then pass either the train data set or the test data set data frame to it", "tokens": [3146, 3372, 439, 295, 264, 4439, 666, 257, 3170, 293, 550, 1320, 2139, 264, 3847, 1412, 992, 420, 264, 1500, 1412, 992, 1412, 3920, 281, 309], "temperature": 0.0, "avg_logprob": -0.20048677599107897, "compression_ratio": 1.824, "no_speech_prob": 3.1381055123347323e-06}, {"id": 530, "seek": 251110, "start": 2518.38, "end": 2520.38, "text": " and in this case I wanted to", "tokens": [293, 294, 341, 1389, 286, 1415, 281], "temperature": 0.0, "avg_logprob": -0.20048677599107897, "compression_ratio": 1.824, "no_speech_prob": 3.1381055123347323e-06}, {"id": 531, "seek": 251110, "start": 2520.9, "end": 2526.3399999999997, "text": " Kind of for teaching purposes you'd be able to see each step and through to experiment to see what each step looks like", "tokens": [9242, 295, 337, 4571, 9932, 291, 1116, 312, 1075, 281, 536, 1184, 1823, 293, 807, 281, 5120, 281, 536, 437, 1184, 1823, 1542, 411], "temperature": 0.0, "avg_logprob": -0.20048677599107897, "compression_ratio": 1.824, "no_speech_prob": 3.1381055123347323e-06}, {"id": 532, "seek": 251110, "start": 2527.1, "end": 2529.1, "text": " But you could certainly simplify this code", "tokens": [583, 291, 727, 3297, 20460, 341, 3089], "temperature": 0.0, "avg_logprob": -0.20048677599107897, "compression_ratio": 1.824, "no_speech_prob": 3.1381055123347323e-06}, {"id": 533, "seek": 251110, "start": 2530.98, "end": 2533.92, "text": " So yeah, so we do this for every data frame", "tokens": [407, 1338, 11, 370, 321, 360, 341, 337, 633, 1412, 3920], "temperature": 0.0, "avg_logprob": -0.20048677599107897, "compression_ratio": 1.824, "no_speech_prob": 3.1381055123347323e-06}, {"id": 534, "seek": 251110, "start": 2534.94, "end": 2540.74, "text": " And then some of these you can see I kind of loop through the data frame in joined and for join test", "tokens": [400, 550, 512, 295, 613, 291, 393, 536, 286, 733, 295, 6367, 807, 264, 1412, 3920, 294, 6869, 293, 337, 3917, 1500], "temperature": 0.0, "avg_logprob": -0.20048677599107897, "compression_ratio": 1.824, "no_speech_prob": 3.1381055123347323e-06}, {"id": 535, "seek": 254074, "start": 2540.74, "end": 2542.74, "text": " right train and test", "tokens": [558, 3847, 293, 1500], "temperature": 0.0, "avg_logprob": -0.17346029883032446, "compression_ratio": 1.9308943089430894, "no_speech_prob": 2.482471700204769e-06}, {"id": 536, "seek": 254074, "start": 2544.02, "end": 2546.02, "text": " This whole thing about the", "tokens": [639, 1379, 551, 466, 264], "temperature": 0.0, "avg_logprob": -0.17346029883032446, "compression_ratio": 1.9308943089430894, "no_speech_prob": 2.482471700204769e-06}, {"id": 537, "seek": 254074, "start": 2546.68, "end": 2554.18, "text": " Durations I basically put two lines here one that said data frame equals train columns one that says data frame equals test columns", "tokens": [13710, 763, 286, 1936, 829, 732, 3876, 510, 472, 300, 848, 1412, 3920, 6915, 3847, 13766, 472, 300, 1619, 1412, 3920, 6915, 1500, 13766], "temperature": 0.0, "avg_logprob": -0.17346029883032446, "compression_ratio": 1.9308943089430894, "no_speech_prob": 2.482471700204769e-06}, {"id": 538, "seek": 254074, "start": 2554.18, "end": 2557.9799999999996, "text": " And so my you know basically idea is you'd run this line first", "tokens": [400, 370, 452, 291, 458, 1936, 1558, 307, 291, 1116, 1190, 341, 1622, 700], "temperature": 0.0, "avg_logprob": -0.17346029883032446, "compression_ratio": 1.9308943089430894, "no_speech_prob": 2.482471700204769e-06}, {"id": 539, "seek": 254074, "start": 2557.9799999999996, "end": 2561.62, "text": " And then you would skip the next one and you'd run everything beneath it", "tokens": [400, 550, 291, 576, 10023, 264, 958, 472, 293, 291, 1116, 1190, 1203, 17149, 309], "temperature": 0.0, "avg_logprob": -0.17346029883032446, "compression_ratio": 1.9308943089430894, "no_speech_prob": 2.482471700204769e-06}, {"id": 540, "seek": 254074, "start": 2561.74, "end": 2565.8199999999997, "text": " And then you'd go back and run this line and then run everything beneath it", "tokens": [400, 550, 291, 1116, 352, 646, 293, 1190, 341, 1622, 293, 550, 1190, 1203, 17149, 309], "temperature": 0.0, "avg_logprob": -0.17346029883032446, "compression_ratio": 1.9308943089430894, "no_speech_prob": 2.482471700204769e-06}, {"id": 541, "seek": 254074, "start": 2565.8199999999997, "end": 2570.2599999999998, "text": " So some people on the forum were asking how come this code wasn't working this week", "tokens": [407, 512, 561, 322, 264, 17542, 645, 3365, 577, 808, 341, 3089, 2067, 380, 1364, 341, 1243], "temperature": 0.0, "avg_logprob": -0.17346029883032446, "compression_ratio": 1.9308943089430894, "no_speech_prob": 2.482471700204769e-06}, {"id": 542, "seek": 257026, "start": 2570.26, "end": 2577.1800000000003, "text": " Which is a good reminder that the code is not designed to be code that you always run top to bottom without thinking right?", "tokens": [3013, 307, 257, 665, 13548, 300, 264, 3089, 307, 406, 4761, 281, 312, 3089, 300, 291, 1009, 1190, 1192, 281, 2767, 1553, 1953, 558, 30], "temperature": 0.0, "avg_logprob": -0.14748223011310285, "compression_ratio": 1.7154471544715446, "no_speech_prob": 6.577915314665006e-07}, {"id": 543, "seek": 257026, "start": 2577.1800000000003, "end": 2581.46, "text": " You're meant to like think like what is this code here should I be running it right now?", "tokens": [509, 434, 4140, 281, 411, 519, 411, 437, 307, 341, 3089, 510, 820, 286, 312, 2614, 309, 558, 586, 30], "temperature": 0.0, "avg_logprob": -0.14748223011310285, "compression_ratio": 1.7154471544715446, "no_speech_prob": 6.577915314665006e-07}, {"id": 544, "seek": 257026, "start": 2582.1000000000004, "end": 2585.0, "text": " Okay, and so like the early lessons", "tokens": [1033, 11, 293, 370, 411, 264, 2440, 8820], "temperature": 0.0, "avg_logprob": -0.14748223011310285, "compression_ratio": 1.7154471544715446, "no_speech_prob": 6.577915314665006e-07}, {"id": 545, "seek": 257026, "start": 2585.0, "end": 2587.0, "text": " I tried to make it so you can run it top to bottom", "tokens": [286, 3031, 281, 652, 309, 370, 291, 393, 1190, 309, 1192, 281, 2767], "temperature": 0.0, "avg_logprob": -0.14748223011310285, "compression_ratio": 1.7154471544715446, "no_speech_prob": 6.577915314665006e-07}, {"id": 546, "seek": 257026, "start": 2587.1000000000004, "end": 2592.2200000000003, "text": " But increasingly as we go along I kind of make it more and more that like you actually have to think about what's going on", "tokens": [583, 12980, 382, 321, 352, 2051, 286, 733, 295, 652, 309, 544, 293, 544, 300, 411, 291, 767, 362, 281, 519, 466, 437, 311, 516, 322], "temperature": 0.0, "avg_logprob": -0.14748223011310285, "compression_ratio": 1.7154471544715446, "no_speech_prob": 6.577915314665006e-07}, {"id": 547, "seek": 259222, "start": 2592.22, "end": 2597.54, "text": " So Jeremy you're talking about shallow learning and deep learning", "tokens": [407, 17809, 291, 434, 1417, 466, 20488, 2539, 293, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.545809689690085, "compression_ratio": 1.510752688172043, "no_speech_prob": 4.784994871442905e-06}, {"id": 548, "seek": 259222, "start": 2597.54, "end": 2601.3399999999997, "text": " Could you define that a bit better by shallow learning? I think I just mean anything that doesn't have a hidden layer", "tokens": [7497, 291, 6964, 300, 257, 857, 1101, 538, 20488, 2539, 30, 286, 519, 286, 445, 914, 1340, 300, 1177, 380, 362, 257, 7633, 4583], "temperature": 0.0, "avg_logprob": -0.545809689690085, "compression_ratio": 1.510752688172043, "no_speech_prob": 4.784994871442905e-06}, {"id": 549, "seek": 259222, "start": 2601.3399999999997, "end": 2606.7799999999997, "text": " So something that's like a dot product a matrix multiplier basically", "tokens": [407, 746, 300, 311, 411, 257, 5893, 1674, 257, 8141, 44106, 1936], "temperature": 0.0, "avg_logprob": -0.545809689690085, "compression_ratio": 1.510752688172043, "no_speech_prob": 4.784994871442905e-06}, {"id": 550, "seek": 259222, "start": 2608.2999999999997, "end": 2610.2999999999997, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.545809689690085, "compression_ratio": 1.510752688172043, "no_speech_prob": 4.784994871442905e-06}, {"id": 551, "seek": 259222, "start": 2615.06, "end": 2617.06, "text": " So we end up with a", "tokens": [407, 321, 917, 493, 365, 257], "temperature": 0.0, "avg_logprob": -0.545809689690085, "compression_ratio": 1.510752688172043, "no_speech_prob": 4.784994871442905e-06}, {"id": 552, "seek": 261706, "start": 2617.06, "end": 2624.22, "text": " So we end up with a training and a test version and then everything else is basically the same", "tokens": [407, 321, 917, 493, 365, 257, 3097, 293, 257, 1500, 3037, 293, 550, 1203, 1646, 307, 1936, 264, 912], "temperature": 0.0, "avg_logprob": -0.23005640948260273, "compression_ratio": 1.930327868852459, "no_speech_prob": 2.726435923250392e-06}, {"id": 553, "seek": 261706, "start": 2624.94, "end": 2627.1, "text": " And then everything else is basically the same", "tokens": [400, 550, 1203, 1646, 307, 1936, 264, 912], "temperature": 0.0, "avg_logprob": -0.23005640948260273, "compression_ratio": 1.930327868852459, "no_speech_prob": 2.726435923250392e-06}, {"id": 554, "seek": 261706, "start": 2628.7, "end": 2633.34, "text": " One thing to note and a lot of these details of this we cover in the machine learning course by the way", "tokens": [1485, 551, 281, 3637, 293, 257, 688, 295, 613, 4365, 295, 341, 321, 2060, 294, 264, 3479, 2539, 1164, 538, 264, 636], "temperature": 0.0, "avg_logprob": -0.23005640948260273, "compression_ratio": 1.930327868852459, "no_speech_prob": 2.726435923250392e-06}, {"id": 555, "seek": 261706, "start": 2633.34, "end": 2636.7, "text": " It's not really deep learning specific so check that out if you're interested in the details", "tokens": [467, 311, 406, 534, 2452, 2539, 2685, 370, 1520, 300, 484, 498, 291, 434, 3102, 294, 264, 4365], "temperature": 0.0, "avg_logprob": -0.23005640948260273, "compression_ratio": 1.930327868852459, "no_speech_prob": 2.726435923250392e-06}, {"id": 556, "seek": 261706, "start": 2637.22, "end": 2638.62, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.23005640948260273, "compression_ratio": 1.930327868852459, "no_speech_prob": 2.726435923250392e-06}, {"id": 557, "seek": 261706, "start": 2638.62, "end": 2645.18, "text": " Should mention you know we use apply cats rather than train cats to make sure that the test set and the training set have the same", "tokens": [6454, 2152, 291, 458, 321, 764, 3079, 11111, 2831, 813, 3847, 11111, 281, 652, 988, 300, 264, 1500, 992, 293, 264, 3097, 992, 362, 264, 912], "temperature": 0.0, "avg_logprob": -0.23005640948260273, "compression_ratio": 1.930327868852459, "no_speech_prob": 2.726435923250392e-06}, {"id": 558, "seek": 264518, "start": 2645.18, "end": 2647.18, "text": " same", "tokens": [912], "temperature": 0.0, "avg_logprob": -0.18547650879504635, "compression_ratio": 1.6754385964912282, "no_speech_prob": 2.406086423434317e-06}, {"id": 559, "seek": 264518, "start": 2647.74, "end": 2650.06, "text": " Categorical codes that they join to", "tokens": [383, 2968, 284, 804, 14211, 300, 436, 3917, 281], "temperature": 0.0, "avg_logprob": -0.18547650879504635, "compression_ratio": 1.6754385964912282, "no_speech_prob": 2.406086423434317e-06}, {"id": 560, "seek": 264518, "start": 2652.2599999999998, "end": 2655.46, "text": " We also need to make sure that we keep track of the mapper", "tokens": [492, 611, 643, 281, 652, 988, 300, 321, 1066, 2837, 295, 264, 463, 3717], "temperature": 0.0, "avg_logprob": -0.18547650879504635, "compression_ratio": 1.6754385964912282, "no_speech_prob": 2.406086423434317e-06}, {"id": 561, "seek": 264518, "start": 2655.46, "end": 2660.4199999999996, "text": " This is the thing which basically says what's the mean and standard deviation of each continuous column?", "tokens": [639, 307, 264, 551, 597, 1936, 1619, 437, 311, 264, 914, 293, 3832, 25163, 295, 1184, 10957, 7738, 30], "temperature": 0.0, "avg_logprob": -0.18547650879504635, "compression_ratio": 1.6754385964912282, "no_speech_prob": 2.406086423434317e-06}, {"id": 562, "seek": 264518, "start": 2661.02, "end": 2664.0, "text": " And then apply that same mapper to the test set", "tokens": [400, 550, 3079, 300, 912, 463, 3717, 281, 264, 1500, 992], "temperature": 0.0, "avg_logprob": -0.18547650879504635, "compression_ratio": 1.6754385964912282, "no_speech_prob": 2.406086423434317e-06}, {"id": 563, "seek": 264518, "start": 2664.8999999999996, "end": 2666.7, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.18547650879504635, "compression_ratio": 1.6754385964912282, "no_speech_prob": 2.406086423434317e-06}, {"id": 564, "seek": 264518, "start": 2666.7, "end": 2669.7799999999997, "text": " So when we do all that that's basically it then the rest is easy", "tokens": [407, 562, 321, 360, 439, 300, 300, 311, 1936, 309, 550, 264, 1472, 307, 1858], "temperature": 0.0, "avg_logprob": -0.18547650879504635, "compression_ratio": 1.6754385964912282, "no_speech_prob": 2.406086423434317e-06}, {"id": 565, "seek": 264518, "start": 2669.7799999999997, "end": 2673.08, "text": " We just have to pass in the test data frame in the usual way", "tokens": [492, 445, 362, 281, 1320, 294, 264, 1500, 1412, 3920, 294, 264, 7713, 636], "temperature": 0.0, "avg_logprob": -0.18547650879504635, "compression_ratio": 1.6754385964912282, "no_speech_prob": 2.406086423434317e-06}, {"id": 566, "seek": 267308, "start": 2673.08, "end": 2677.48, "text": " when we create our model data object and", "tokens": [562, 321, 1884, 527, 2316, 1412, 2657, 293], "temperature": 0.0, "avg_logprob": -0.22952471206437297, "compression_ratio": 1.5804597701149425, "no_speech_prob": 2.332064468646422e-06}, {"id": 567, "seek": 267308, "start": 2678.7999999999997, "end": 2685.0, "text": " Then there's no changes through all here. We trained it in the same way and then once we finish training it", "tokens": [1396, 456, 311, 572, 2962, 807, 439, 510, 13, 492, 8895, 309, 294, 264, 912, 636, 293, 550, 1564, 321, 2413, 3097, 309], "temperature": 0.0, "avg_logprob": -0.22952471206437297, "compression_ratio": 1.5804597701149425, "no_speech_prob": 2.332064468646422e-06}, {"id": 568, "seek": 267308, "start": 2689.6, "end": 2692.04, "text": " We can then call predict", "tokens": [492, 393, 550, 818, 6069], "temperature": 0.0, "avg_logprob": -0.22952471206437297, "compression_ratio": 1.5804597701149425, "no_speech_prob": 2.332064468646422e-06}, {"id": 569, "seek": 267308, "start": 2692.64, "end": 2699.6, "text": " As per usual passing in true to say this is the test set rather than the validation set and pass that", "tokens": [1018, 680, 7713, 8437, 294, 2074, 281, 584, 341, 307, 264, 1500, 992, 2831, 813, 264, 24071, 992, 293, 1320, 300], "temperature": 0.0, "avg_logprob": -0.22952471206437297, "compression_ratio": 1.5804597701149425, "no_speech_prob": 2.332064468646422e-06}, {"id": 570, "seek": 269960, "start": 2699.6, "end": 2704.24, "text": " off to kaggle, and so it was really interesting because", "tokens": [766, 281, 350, 559, 22631, 11, 293, 370, 309, 390, 534, 1880, 570], "temperature": 0.0, "avg_logprob": -0.283772736787796, "compression_ratio": 1.3641975308641976, "no_speech_prob": 1.0188061878579902e-06}, {"id": 571, "seek": 269960, "start": 2705.96, "end": 2709.92, "text": " This was my submission. They've got a public score of", "tokens": [639, 390, 452, 23689, 13, 814, 600, 658, 257, 1908, 6175, 295], "temperature": 0.0, "avg_logprob": -0.283772736787796, "compression_ratio": 1.3641975308641976, "no_speech_prob": 1.0188061878579902e-06}, {"id": 572, "seek": 269960, "start": 2711.3199999999997, "end": 2713.3199999999997, "text": " 103", "tokens": [48784], "temperature": 0.0, "avg_logprob": -0.283772736787796, "compression_ratio": 1.3641975308641976, "no_speech_prob": 1.0188061878579902e-06}, {"id": 573, "seek": 269960, "start": 2713.7999999999997, "end": 2715.7999999999997, "text": " Which would put us in", "tokens": [3013, 576, 829, 505, 294], "temperature": 0.0, "avg_logprob": -0.283772736787796, "compression_ratio": 1.3641975308641976, "no_speech_prob": 1.0188061878579902e-06}, {"id": 574, "seek": 269960, "start": 2717.3199999999997, "end": 2719.66, "text": " About 300 and something's place", "tokens": [7769, 6641, 293, 746, 311, 1081], "temperature": 0.0, "avg_logprob": -0.283772736787796, "compression_ratio": 1.3641975308641976, "no_speech_prob": 1.0188061878579902e-06}, {"id": 575, "seek": 269960, "start": 2720.6, "end": 2725.72, "text": " Which looks awful right and our private score of?", "tokens": [3013, 1542, 11232, 558, 293, 527, 4551, 6175, 295, 30], "temperature": 0.0, "avg_logprob": -0.283772736787796, "compression_ratio": 1.3641975308641976, "no_speech_prob": 1.0188061878579902e-06}, {"id": 576, "seek": 272572, "start": 2725.72, "end": 2727.8399999999997, "text": " of 107", "tokens": [295, 1266, 22], "temperature": 0.0, "avg_logprob": -0.23388101914349724, "compression_ratio": 1.519047619047619, "no_speech_prob": 3.0590166488764226e-07}, {"id": 577, "seek": 272572, "start": 2732.04, "end": 2734.04, "text": " Need a board private", "tokens": [16984, 257, 3150, 4551], "temperature": 0.0, "avg_logprob": -0.23388101914349724, "compression_ratio": 1.519047619047619, "no_speech_prob": 3.0590166488764226e-07}, {"id": 578, "seek": 272572, "start": 2737.08, "end": 2739.08, "text": " Is about fifth", "tokens": [1119, 466, 9266], "temperature": 0.0, "avg_logprob": -0.23388101914349724, "compression_ratio": 1.519047619047619, "no_speech_prob": 3.0590166488764226e-07}, {"id": 579, "seek": 272572, "start": 2739.3999999999996, "end": 2743.6, "text": " right so like if you're competing in a kaggle competition and", "tokens": [558, 370, 411, 498, 291, 434, 15439, 294, 257, 350, 559, 22631, 6211, 293], "temperature": 0.0, "avg_logprob": -0.23388101914349724, "compression_ratio": 1.519047619047619, "no_speech_prob": 3.0590166488764226e-07}, {"id": 580, "seek": 272572, "start": 2744.3999999999996, "end": 2750.62, "text": " You don't haven't thoughtfully created a validation set of your own and you're relying on public leaderboard feedback", "tokens": [509, 500, 380, 2378, 380, 1194, 2277, 2942, 257, 24071, 992, 295, 428, 1065, 293, 291, 434, 24140, 322, 1908, 5263, 3787, 5824], "temperature": 0.0, "avg_logprob": -0.23388101914349724, "compression_ratio": 1.519047619047619, "no_speech_prob": 3.0590166488764226e-07}, {"id": 581, "seek": 272572, "start": 2751.08, "end": 2755.48, "text": " This could totally happen to you, but the other way around you'll be like oh, I'm in the top ten", "tokens": [639, 727, 3879, 1051, 281, 291, 11, 457, 264, 661, 636, 926, 291, 603, 312, 411, 1954, 11, 286, 478, 294, 264, 1192, 2064], "temperature": 0.0, "avg_logprob": -0.23388101914349724, "compression_ratio": 1.519047619047619, "no_speech_prob": 3.0590166488764226e-07}, {"id": 582, "seek": 275548, "start": 2755.48, "end": 2762.68, "text": " I'm doing great and then oh for example at the moment the icebergs competition recognizing icebergs a", "tokens": [286, 478, 884, 869, 293, 550, 1954, 337, 1365, 412, 264, 1623, 264, 38880, 82, 6211, 18538, 38880, 82, 257], "temperature": 0.0, "avg_logprob": -0.2257040363468536, "compression_ratio": 1.645320197044335, "no_speech_prob": 4.49514800493489e-06}, {"id": 583, "seek": 275548, "start": 2763.36, "end": 2766.72, "text": " very large percentage of the public leaderboard set is", "tokens": [588, 2416, 9668, 295, 264, 1908, 5263, 3787, 992, 307], "temperature": 0.0, "avg_logprob": -0.2257040363468536, "compression_ratio": 1.645320197044335, "no_speech_prob": 4.49514800493489e-06}, {"id": 584, "seek": 275548, "start": 2767.2400000000002, "end": 2768.72, "text": " synthetically generated", "tokens": [10657, 22652, 10833], "temperature": 0.0, "avg_logprob": -0.2257040363468536, "compression_ratio": 1.645320197044335, "no_speech_prob": 4.49514800493489e-06}, {"id": 585, "seek": 275548, "start": 2768.72, "end": 2771.32, "text": " Data augmentation data like totally", "tokens": [11888, 14501, 19631, 1412, 411, 3879], "temperature": 0.0, "avg_logprob": -0.2257040363468536, "compression_ratio": 1.645320197044335, "no_speech_prob": 4.49514800493489e-06}, {"id": 586, "seek": 275548, "start": 2771.96, "end": 2778.84, "text": " Meaningless and so your validation set is going to be much more helpful than the public leaderboard feedback right so", "tokens": [19948, 1832, 293, 370, 428, 24071, 992, 307, 516, 281, 312, 709, 544, 4961, 813, 264, 1908, 5263, 3787, 5824, 558, 370], "temperature": 0.0, "avg_logprob": -0.2257040363468536, "compression_ratio": 1.645320197044335, "no_speech_prob": 4.49514800493489e-06}, {"id": 587, "seek": 277884, "start": 2778.84, "end": 2786.6800000000003, "text": " Yeah, be very careful, so our final score here is kind of within statistical noise of the actual third place", "tokens": [865, 11, 312, 588, 5026, 11, 370, 527, 2572, 6175, 510, 307, 733, 295, 1951, 22820, 5658, 295, 264, 3539, 2636, 1081], "temperature": 0.0, "avg_logprob": -0.28281245360503326, "compression_ratio": 1.5665024630541873, "no_speech_prob": 2.4824610136420233e-06}, {"id": 588, "seek": 277884, "start": 2787.04, "end": 2791.76, "text": " get us some pretty confident that we we've captured their approach and", "tokens": [483, 505, 512, 1238, 6679, 300, 321, 321, 600, 11828, 641, 3109, 293], "temperature": 0.0, "avg_logprob": -0.28281245360503326, "compression_ratio": 1.5665024630541873, "no_speech_prob": 2.4824610136420233e-06}, {"id": 589, "seek": 277884, "start": 2794.28, "end": 2797.2000000000003, "text": " So that's that was pretty interesting", "tokens": [407, 300, 311, 300, 390, 1238, 1880], "temperature": 0.0, "avg_logprob": -0.28281245360503326, "compression_ratio": 1.5665024630541873, "no_speech_prob": 2.4824610136420233e-06}, {"id": 590, "seek": 277884, "start": 2799.6400000000003, "end": 2801.1200000000003, "text": " Something to mention", "tokens": [6595, 281, 2152], "temperature": 0.0, "avg_logprob": -0.28281245360503326, "compression_ratio": 1.5665024630541873, "no_speech_prob": 2.4824610136420233e-06}, {"id": 591, "seek": 277884, "start": 2801.1200000000003, "end": 2804.94, "text": " There's a nice kernel about the Rossman about quite a few nice kernels actually", "tokens": [821, 311, 257, 1481, 28256, 466, 264, 16140, 1601, 466, 1596, 257, 1326, 1481, 23434, 1625, 767], "temperature": 0.0, "avg_logprob": -0.28281245360503326, "compression_ratio": 1.5665024630541873, "no_speech_prob": 2.4824610136420233e-06}, {"id": 592, "seek": 280494, "start": 2804.94, "end": 2809.64, "text": " But you can go back and see like particularly if you're doing the groceries competition go and have a look at the Rossman kernels", "tokens": [583, 291, 393, 352, 646, 293, 536, 411, 4098, 498, 291, 434, 884, 264, 31391, 6211, 352, 293, 362, 257, 574, 412, 264, 16140, 1601, 23434, 1625], "temperature": 0.0, "avg_logprob": -0.14956488901255083, "compression_ratio": 1.755813953488372, "no_speech_prob": 2.406086878181668e-06}, {"id": 593, "seek": 280494, "start": 2809.64, "end": 2814.64, "text": " Because actually quite a few of them are higher quality than the ones for the Ecuadorian groceries competition", "tokens": [1436, 767, 1596, 257, 1326, 295, 552, 366, 2946, 3125, 813, 264, 2306, 337, 264, 41558, 952, 31391, 6211], "temperature": 0.0, "avg_logprob": -0.14956488901255083, "compression_ratio": 1.755813953488372, "no_speech_prob": 2.406086878181668e-06}, {"id": 594, "seek": 280494, "start": 2815.2000000000003, "end": 2820.76, "text": " One of them for example showed how on for particular stores like store 85", "tokens": [1485, 295, 552, 337, 1365, 4712, 577, 322, 337, 1729, 9512, 411, 3531, 14695], "temperature": 0.0, "avg_logprob": -0.14956488901255083, "compression_ratio": 1.755813953488372, "no_speech_prob": 2.406086878181668e-06}, {"id": 595, "seek": 280494, "start": 2821.7200000000003, "end": 2827.2000000000003, "text": " The sales for non Sundays and the sale for Sundays looked very different", "tokens": [440, 5763, 337, 2107, 44857, 293, 264, 8680, 337, 44857, 2956, 588, 819], "temperature": 0.0, "avg_logprob": -0.14956488901255083, "compression_ratio": 1.755813953488372, "no_speech_prob": 2.406086878181668e-06}, {"id": 596, "seek": 280494, "start": 2827.92, "end": 2830.92, "text": " Where else there are some other stores where the sales on Sunday?", "tokens": [2305, 1646, 456, 366, 512, 661, 9512, 689, 264, 5763, 322, 7776, 30], "temperature": 0.0, "avg_logprob": -0.14956488901255083, "compression_ratio": 1.755813953488372, "no_speech_prob": 2.406086878181668e-06}, {"id": 597, "seek": 283092, "start": 2830.92, "end": 2835.26, "text": " Don't look any different and it can kind of like get a sense of why you need these kind of interactions", "tokens": [1468, 380, 574, 604, 819, 293, 309, 393, 733, 295, 411, 483, 257, 2020, 295, 983, 291, 643, 613, 733, 295, 13280], "temperature": 0.0, "avg_logprob": -0.21721924675835502, "compression_ratio": 1.7067669172932332, "no_speech_prob": 1.0348493333367514e-06}, {"id": 598, "seek": 283092, "start": 2835.36, "end": 2841.16, "text": " The one I particularly wanted to point out is the one I think I briefly mentioned that the third place winners whose", "tokens": [440, 472, 286, 4098, 1415, 281, 935, 484, 307, 264, 472, 286, 519, 286, 10515, 2835, 300, 264, 2636, 1081, 17193, 6104], "temperature": 0.0, "avg_logprob": -0.21721924675835502, "compression_ratio": 1.7067669172932332, "no_speech_prob": 1.0348493333367514e-06}, {"id": 599, "seek": 283092, "start": 2841.56, "end": 2846.64, "text": " Approach we used they didn't notice is this one and here's a really cool visualization", "tokens": [29551, 608, 321, 1143, 436, 994, 380, 3449, 307, 341, 472, 293, 510, 311, 257, 534, 1627, 25801], "temperature": 0.0, "avg_logprob": -0.21721924675835502, "compression_ratio": 1.7067669172932332, "no_speech_prob": 1.0348493333367514e-06}, {"id": 600, "seek": 283092, "start": 2848.4, "end": 2852.32, "text": " Here you can see that the store this store is closed", "tokens": [1692, 291, 393, 536, 300, 264, 3531, 341, 3531, 307, 5395], "temperature": 0.0, "avg_logprob": -0.21721924675835502, "compression_ratio": 1.7067669172932332, "no_speech_prob": 1.0348493333367514e-06}, {"id": 601, "seek": 285232, "start": 2852.32, "end": 2861.0, "text": " Right and just after oh my god we run out of we've run out of eggs and just before oh my god", "tokens": [1779, 293, 445, 934, 1954, 452, 3044, 321, 1190, 484, 295, 321, 600, 1190, 484, 295, 6466, 293, 445, 949, 1954, 452, 3044], "temperature": 0.0, "avg_logprob": -0.24462860900086242, "compression_ratio": 1.746031746031746, "no_speech_prob": 1.8448139371685102e-06}, {"id": 602, "seek": 285232, "start": 2861.0, "end": 2863.7200000000003, "text": " Go and get the milk before the store closes", "tokens": [1037, 293, 483, 264, 5392, 949, 264, 3531, 24157], "temperature": 0.0, "avg_logprob": -0.24462860900086242, "compression_ratio": 1.746031746031746, "no_speech_prob": 1.8448139371685102e-06}, {"id": 603, "seek": 285232, "start": 2864.28, "end": 2870.3, "text": " Right and here again closed bang right so this third place winner actually", "tokens": [1779, 293, 510, 797, 5395, 8550, 558, 370, 341, 2636, 1081, 8507, 767], "temperature": 0.0, "avg_logprob": -0.24462860900086242, "compression_ratio": 1.746031746031746, "no_speech_prob": 1.8448139371685102e-06}, {"id": 604, "seek": 285232, "start": 2870.8, "end": 2878.46, "text": " Deleted all of the closed store rows before they started doing any analysis right so remember how we talked about like", "tokens": [5831, 10993, 439, 295, 264, 5395, 3531, 13241, 949, 436, 1409, 884, 604, 5215, 558, 370, 1604, 577, 321, 2825, 466, 411], "temperature": 0.0, "avg_logprob": -0.24462860900086242, "compression_ratio": 1.746031746031746, "no_speech_prob": 1.8448139371685102e-06}, {"id": 605, "seek": 287846, "start": 2878.46, "end": 2882.06, "text": " Don't touch your data unless you first of all", "tokens": [1468, 380, 2557, 428, 1412, 5969, 291, 700, 295, 439], "temperature": 0.0, "avg_logprob": -0.16078278326219128, "compression_ratio": 1.6212765957446809, "no_speech_prob": 2.2732685067694547e-07}, {"id": 606, "seek": 287846, "start": 2882.9, "end": 2887.14, "text": " Analyze to see whether that thing you're doing is actually okay", "tokens": [1107, 5222, 1381, 281, 536, 1968, 300, 551, 291, 434, 884, 307, 767, 1392], "temperature": 0.0, "avg_logprob": -0.16078278326219128, "compression_ratio": 1.6212765957446809, "no_speech_prob": 2.2732685067694547e-07}, {"id": 607, "seek": 287846, "start": 2887.66, "end": 2890.34, "text": " No assumptions right so in this case", "tokens": [883, 17695, 558, 370, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.16078278326219128, "compression_ratio": 1.6212765957446809, "no_speech_prob": 2.2732685067694547e-07}, {"id": 608, "seek": 287846, "start": 2890.34, "end": 2892.28, "text": " I am sure like I haven't tried it", "tokens": [286, 669, 988, 411, 286, 2378, 380, 3031, 309], "temperature": 0.0, "avg_logprob": -0.16078278326219128, "compression_ratio": 1.6212765957446809, "no_speech_prob": 2.2732685067694547e-07}, {"id": 609, "seek": 287846, "start": 2892.28, "end": 2898.9, "text": " But I'm sure they would have won otherwise right because like although they weren't actually any store closures to my knowledge", "tokens": [583, 286, 478, 988, 436, 576, 362, 1582, 5911, 558, 570, 411, 4878, 436, 4999, 380, 767, 604, 3531, 2611, 1303, 281, 452, 3601], "temperature": 0.0, "avg_logprob": -0.16078278326219128, "compression_ratio": 1.6212765957446809, "no_speech_prob": 2.2732685067694547e-07}, {"id": 610, "seek": 287846, "start": 2899.06, "end": 2904.1, "text": " In the test set period the problem is that their model was trying to fit", "tokens": [682, 264, 1500, 992, 2896, 264, 1154, 307, 300, 641, 2316, 390, 1382, 281, 3318], "temperature": 0.0, "avg_logprob": -0.16078278326219128, "compression_ratio": 1.6212765957446809, "no_speech_prob": 2.2732685067694547e-07}, {"id": 611, "seek": 290410, "start": 2904.1, "end": 2908.94, "text": " To these like really extreme things and so and because it wasn't able to do it very well", "tokens": [1407, 613, 411, 534, 8084, 721, 293, 370, 293, 570, 309, 2067, 380, 1075, 281, 360, 309, 588, 731], "temperature": 0.0, "avg_logprob": -0.16524306866301208, "compression_ratio": 1.7028985507246377, "no_speech_prob": 1.9947217424487462e-06}, {"id": 612, "seek": 290410, "start": 2908.94, "end": 2911.5, "text": " It was going to end up getting a little bit confused", "tokens": [467, 390, 516, 281, 917, 493, 1242, 257, 707, 857, 9019], "temperature": 0.0, "avg_logprob": -0.16524306866301208, "compression_ratio": 1.7028985507246377, "no_speech_prob": 1.9947217424487462e-06}, {"id": 613, "seek": 290410, "start": 2911.7, "end": 2913.18, "text": " It's not going to break the model", "tokens": [467, 311, 406, 516, 281, 1821, 264, 2316], "temperature": 0.0, "avg_logprob": -0.16524306866301208, "compression_ratio": 1.7028985507246377, "no_speech_prob": 1.9947217424487462e-06}, {"id": 614, "seek": 290410, "start": 2913.18, "end": 2919.54, "text": " But it's definitely going to harm it because it's kind of trying to do computations to fit something which it literally doesn't have the data", "tokens": [583, 309, 311, 2138, 516, 281, 6491, 309, 570, 309, 311, 733, 295, 1382, 281, 360, 2807, 763, 281, 3318, 746, 597, 309, 3736, 1177, 380, 362, 264, 1412], "temperature": 0.0, "avg_logprob": -0.16524306866301208, "compression_ratio": 1.7028985507246377, "no_speech_prob": 1.9947217424487462e-06}, {"id": 615, "seek": 290410, "start": 2919.54, "end": 2922.02, "text": " For your neck can you pass that back there?", "tokens": [1171, 428, 6189, 393, 291, 1320, 300, 646, 456, 30], "temperature": 0.0, "avg_logprob": -0.16524306866301208, "compression_ratio": 1.7028985507246377, "no_speech_prob": 1.9947217424487462e-06}, {"id": 616, "seek": 290410, "start": 2923.38, "end": 2925.38, "text": " All right, so that", "tokens": [1057, 558, 11, 370, 300], "temperature": 0.0, "avg_logprob": -0.16524306866301208, "compression_ratio": 1.7028985507246377, "no_speech_prob": 1.9947217424487462e-06}, {"id": 617, "seek": 290410, "start": 2927.2999999999997, "end": 2929.2999999999997, "text": " Rossman model", "tokens": [16140, 1601, 2316], "temperature": 0.0, "avg_logprob": -0.16524306866301208, "compression_ratio": 1.7028985507246377, "no_speech_prob": 1.9947217424487462e-06}, {"id": 618, "seek": 292930, "start": 2929.3, "end": 2936.5800000000004, "text": " Again like it's nice to kind of look inside to see what's actually going on right and so that Rossman model", "tokens": [3764, 411, 309, 311, 1481, 281, 733, 295, 574, 1854, 281, 536, 437, 311, 767, 516, 322, 558, 293, 370, 300, 16140, 1601, 2316], "temperature": 0.0, "avg_logprob": -0.15075924632313487, "compression_ratio": 1.7089201877934272, "no_speech_prob": 1.6028054687922122e-06}, {"id": 619, "seek": 292930, "start": 2941.86, "end": 2946.1800000000003, "text": " Want to make sure you kind of know how to find your way around the code so you can answer these questions for yourself", "tokens": [11773, 281, 652, 988, 291, 733, 295, 458, 577, 281, 915, 428, 636, 926, 264, 3089, 370, 291, 393, 1867, 613, 1651, 337, 1803], "temperature": 0.0, "avg_logprob": -0.15075924632313487, "compression_ratio": 1.7089201877934272, "no_speech_prob": 1.6028054687922122e-06}, {"id": 620, "seek": 292930, "start": 2946.1800000000003, "end": 2948.1400000000003, "text": " So it's inside", "tokens": [407, 309, 311, 1854], "temperature": 0.0, "avg_logprob": -0.15075924632313487, "compression_ratio": 1.7089201877934272, "no_speech_prob": 1.6028054687922122e-06}, {"id": 621, "seek": 292930, "start": 2948.1400000000003, "end": 2950.1400000000003, "text": " columnar model data now", "tokens": [7738, 289, 2316, 1412, 586], "temperature": 0.0, "avg_logprob": -0.15075924632313487, "compression_ratio": 1.7089201877934272, "no_speech_prob": 1.6028054687922122e-06}, {"id": 622, "seek": 292930, "start": 2951.82, "end": 2956.98, "text": " We started out by kind of saying hey if you want to look at the code for something you can like go", "tokens": [492, 1409, 484, 538, 733, 295, 1566, 4177, 498, 291, 528, 281, 574, 412, 264, 3089, 337, 746, 291, 393, 411, 352], "temperature": 0.0, "avg_logprob": -0.15075924632313487, "compression_ratio": 1.7089201877934272, "no_speech_prob": 1.6028054687922122e-06}, {"id": 623, "seek": 295698, "start": 2956.98, "end": 2960.14, "text": " question mark question mark like this and", "tokens": [1168, 1491, 1168, 1491, 411, 341, 293], "temperature": 0.0, "avg_logprob": -0.1558684917411419, "compression_ratio": 1.8283261802575108, "no_speech_prob": 2.684174660316785e-06}, {"id": 624, "seek": 295698, "start": 2961.26, "end": 2968.58, "text": " Okay, I need to I haven't got this reading, but you can use question mark question mark to get the source code for something right?", "tokens": [1033, 11, 286, 643, 281, 286, 2378, 380, 658, 341, 3760, 11, 457, 291, 393, 764, 1168, 1491, 1168, 1491, 281, 483, 264, 4009, 3089, 337, 746, 558, 30], "temperature": 0.0, "avg_logprob": -0.1558684917411419, "compression_ratio": 1.8283261802575108, "no_speech_prob": 2.684174660316785e-06}, {"id": 625, "seek": 295698, "start": 2969.38, "end": 2971.38, "text": " But obviously like that's not", "tokens": [583, 2745, 411, 300, 311, 406], "temperature": 0.0, "avg_logprob": -0.1558684917411419, "compression_ratio": 1.8283261802575108, "no_speech_prob": 2.684174660316785e-06}, {"id": 626, "seek": 295698, "start": 2971.94, "end": 2976.26, "text": " Really a great way because often you look at that source code and it turns out you need to look at something else", "tokens": [4083, 257, 869, 636, 570, 2049, 291, 574, 412, 300, 4009, 3089, 293, 309, 4523, 484, 291, 643, 281, 574, 412, 746, 1646], "temperature": 0.0, "avg_logprob": -0.1558684917411419, "compression_ratio": 1.8283261802575108, "no_speech_prob": 2.684174660316785e-06}, {"id": 627, "seek": 295698, "start": 2976.38, "end": 2982.9, "text": " Right and so for those of you that haven't done much coding you might not be aware that almost certainly the", "tokens": [1779, 293, 370, 337, 729, 295, 291, 300, 2378, 380, 1096, 709, 17720, 291, 1062, 406, 312, 3650, 300, 1920, 3297, 264], "temperature": 0.0, "avg_logprob": -0.1558684917411419, "compression_ratio": 1.8283261802575108, "no_speech_prob": 2.684174660316785e-06}, {"id": 628, "seek": 298290, "start": 2982.9, "end": 2989.58, "text": " Editor you're using probably has the ability to both open up stuff directly off SSH and", "tokens": [24281, 291, 434, 1228, 1391, 575, 264, 3485, 281, 1293, 1269, 493, 1507, 3838, 766, 12238, 39, 293], "temperature": 0.0, "avg_logprob": -0.15300252804389367, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.1381312055600574e-06}, {"id": 629, "seek": 298290, "start": 2990.2200000000003, "end": 2995.2000000000003, "text": " To navigate through it so you can jump straight from place to place right so I want to show you what I mean", "tokens": [1407, 12350, 807, 309, 370, 291, 393, 3012, 2997, 490, 1081, 281, 1081, 558, 370, 286, 528, 281, 855, 291, 437, 286, 914], "temperature": 0.0, "avg_logprob": -0.15300252804389367, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.1381312055600574e-06}, {"id": 630, "seek": 298290, "start": 2995.2000000000003, "end": 3001.7400000000002, "text": " So if I want to find columnar model data, and I happen to be using Vim here. I can basically say tag", "tokens": [407, 498, 286, 528, 281, 915, 7738, 289, 2316, 1412, 11, 293, 286, 1051, 281, 312, 1228, 691, 332, 510, 13, 286, 393, 1936, 584, 6162], "temperature": 0.0, "avg_logprob": -0.15300252804389367, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.1381312055600574e-06}, {"id": 631, "seek": 298290, "start": 3002.54, "end": 3010.78, "text": " Columnar model data, and it will jump straight to the definition of that class right and so then I notice here that like oh", "tokens": [4004, 16449, 289, 2316, 1412, 11, 293, 309, 486, 3012, 2997, 281, 264, 7123, 295, 300, 1508, 558, 293, 370, 550, 286, 3449, 510, 300, 411, 1954], "temperature": 0.0, "avg_logprob": -0.15300252804389367, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.1381312055600574e-06}, {"id": 632, "seek": 301078, "start": 3010.78, "end": 3016.4, "text": " It's actually building up a data loader. That's interesting if I hit ctrl right square bracket", "tokens": [467, 311, 767, 2390, 493, 257, 1412, 3677, 260, 13, 663, 311, 1880, 498, 286, 2045, 269, 28269, 558, 3732, 16904], "temperature": 0.0, "avg_logprob": -0.15306472778320312, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.2959127363719745e-06}, {"id": 633, "seek": 301078, "start": 3016.6600000000003, "end": 3022.1000000000004, "text": " It'll jump to the definition of the thing that was under my cursor and after I finished reading it for a while", "tokens": [467, 603, 3012, 281, 264, 7123, 295, 264, 551, 300, 390, 833, 452, 28169, 293, 934, 286, 4335, 3760, 309, 337, 257, 1339], "temperature": 0.0, "avg_logprob": -0.15306472778320312, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.2959127363719745e-06}, {"id": 634, "seek": 301078, "start": 3022.1000000000004, "end": 3025.82, "text": " I can hit ctrl t to jump back up to where I came from", "tokens": [286, 393, 2045, 269, 28269, 256, 281, 3012, 646, 493, 281, 689, 286, 1361, 490], "temperature": 0.0, "avg_logprob": -0.15306472778320312, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.2959127363719745e-06}, {"id": 635, "seek": 301078, "start": 3026.42, "end": 3031.82, "text": " Right and you kind of get the idea right or if I want to find every usage of this in this file", "tokens": [1779, 293, 291, 733, 295, 483, 264, 1558, 558, 420, 498, 286, 528, 281, 915, 633, 14924, 295, 341, 294, 341, 3991], "temperature": 0.0, "avg_logprob": -0.15306472778320312, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.2959127363719745e-06}, {"id": 636, "seek": 301078, "start": 3032.38, "end": 3034.38, "text": " of columnar model data", "tokens": [295, 7738, 289, 2316, 1412], "temperature": 0.0, "avg_logprob": -0.15306472778320312, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.2959127363719745e-06}, {"id": 637, "seek": 301078, "start": 3035.2200000000003, "end": 3038.6200000000003, "text": " I can hit star to jump to the next place", "tokens": [286, 393, 2045, 3543, 281, 3012, 281, 264, 958, 1081], "temperature": 0.0, "avg_logprob": -0.15306472778320312, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.2959127363719745e-06}, {"id": 638, "seek": 303862, "start": 3038.62, "end": 3043.0, "text": " It's used you know and so forth right so in this case", "tokens": [467, 311, 1143, 291, 458, 293, 370, 5220, 558, 370, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.24372339884440103, "compression_ratio": 1.646067415730337, "no_speech_prob": 8.186347031369223e-07}, {"id": 639, "seek": 303862, "start": 3045.2999999999997, "end": 3048.58, "text": " Get learner was the thing which actually got the", "tokens": [3240, 33347, 390, 264, 551, 597, 767, 658, 264], "temperature": 0.0, "avg_logprob": -0.24372339884440103, "compression_ratio": 1.646067415730337, "no_speech_prob": 8.186347031369223e-07}, {"id": 640, "seek": 303862, "start": 3049.3399999999997, "end": 3052.68, "text": " model we want to find out what kind of model it is and", "tokens": [2316, 321, 528, 281, 915, 484, 437, 733, 295, 2316, 309, 307, 293], "temperature": 0.0, "avg_logprob": -0.24372339884440103, "compression_ratio": 1.646067415730337, "no_speech_prob": 8.186347031369223e-07}, {"id": 641, "seek": 303862, "start": 3053.3399999999997, "end": 3055.3399999999997, "text": " apparently it uses a", "tokens": [7970, 309, 4960, 257], "temperature": 0.0, "avg_logprob": -0.24372339884440103, "compression_ratio": 1.646067415730337, "no_speech_prob": 8.186347031369223e-07}, {"id": 642, "seek": 303862, "start": 3055.54, "end": 3059.7, "text": " Or not using collaborative filtering are we we're using columnar model data, sorry", "tokens": [1610, 406, 1228, 16555, 30822, 366, 321, 321, 434, 1228, 7738, 289, 2316, 1412, 11, 2597], "temperature": 0.0, "avg_logprob": -0.24372339884440103, "compression_ratio": 1.646067415730337, "no_speech_prob": 8.186347031369223e-07}, {"id": 643, "seek": 305970, "start": 3059.7, "end": 3065.7799999999997, "text": " Columnar model data", "tokens": [4004, 16449, 289, 2316, 1412], "temperature": 0.0, "avg_logprob": -0.28068736342133066, "compression_ratio": 1.6232876712328768, "no_speech_prob": 5.539162089007732e-07}, {"id": 644, "seek": 305970, "start": 3066.8599999999997, "end": 3068.58, "text": " get learner", "tokens": [483, 33347], "temperature": 0.0, "avg_logprob": -0.28068736342133066, "compression_ratio": 1.6232876712328768, "no_speech_prob": 5.539162089007732e-07}, {"id": 645, "seek": 305970, "start": 3068.58, "end": 3075.8599999999997, "text": " Which uses and so here you can see mixed input model is the pie torch model and then it wraps it in the", "tokens": [3013, 4960, 293, 370, 510, 291, 393, 536, 7467, 4846, 2316, 307, 264, 1730, 27822, 2316, 293, 550, 309, 25831, 309, 294, 264], "temperature": 0.0, "avg_logprob": -0.28068736342133066, "compression_ratio": 1.6232876712328768, "no_speech_prob": 5.539162089007732e-07}, {"id": 646, "seek": 305970, "start": 3076.46, "end": 3078.46, "text": " structured learner", "tokens": [18519, 33347], "temperature": 0.0, "avg_logprob": -0.28068736342133066, "compression_ratio": 1.6232876712328768, "no_speech_prob": 5.539162089007732e-07}, {"id": 647, "seek": 305970, "start": 3078.46, "end": 3084.14, "text": " Which is the the fast AI learner type which wraps the data and the model together?", "tokens": [3013, 307, 264, 264, 2370, 7318, 33347, 2010, 597, 25831, 264, 1412, 293, 264, 2316, 1214, 30], "temperature": 0.0, "avg_logprob": -0.28068736342133066, "compression_ratio": 1.6232876712328768, "no_speech_prob": 5.539162089007732e-07}, {"id": 648, "seek": 308414, "start": 3084.14, "end": 3090.2999999999997, "text": " So if we want to see the definition of this actual pie torch model I can go to control right square bracket", "tokens": [407, 498, 321, 528, 281, 536, 264, 7123, 295, 341, 3539, 1730, 27822, 2316, 286, 393, 352, 281, 1969, 558, 3732, 16904], "temperature": 0.0, "avg_logprob": -0.21594286676663071, "compression_ratio": 1.6125, "no_speech_prob": 3.9897122405818664e-07}, {"id": 649, "seek": 308414, "start": 3090.94, "end": 3094.98, "text": " to see it right and so here is the", "tokens": [281, 536, 309, 558, 293, 370, 510, 307, 264], "temperature": 0.0, "avg_logprob": -0.21594286676663071, "compression_ratio": 1.6125, "no_speech_prob": 3.9897122405818664e-07}, {"id": 650, "seek": 308414, "start": 3096.22, "end": 3098.22, "text": " model right and", "tokens": [2316, 558, 293], "temperature": 0.0, "avg_logprob": -0.21594286676663071, "compression_ratio": 1.6125, "no_speech_prob": 3.9897122405818664e-07}, {"id": 651, "seek": 308414, "start": 3098.8599999999997, "end": 3103.2599999999998, "text": " Nearly all of this we can now understand right so we got past", "tokens": [38000, 439, 295, 341, 321, 393, 586, 1223, 558, 370, 321, 658, 1791], "temperature": 0.0, "avg_logprob": -0.21594286676663071, "compression_ratio": 1.6125, "no_speech_prob": 3.9897122405818664e-07}, {"id": 652, "seek": 310326, "start": 3103.26, "end": 3112.5800000000004, "text": " We got past a list of embedding sizes", "tokens": [492, 658, 1791, 257, 1329, 295, 12240, 3584, 11602], "temperature": 0.0, "avg_logprob": -0.34546710283328325, "compression_ratio": 1.2612612612612613, "no_speech_prob": 4.133275433559902e-05}, {"id": 653, "seek": 310326, "start": 3115.5, "end": 3117.5, "text": " Sure there is", "tokens": [4894, 456, 307], "temperature": 0.0, "avg_logprob": -0.34546710283328325, "compression_ratio": 1.2612612612612613, "no_speech_prob": 4.133275433559902e-05}, {"id": 654, "seek": 310326, "start": 3122.1800000000003, "end": 3126.5800000000004, "text": " In the mixed model that we saw does it always expect", "tokens": [682, 264, 7467, 2316, 300, 321, 1866, 775, 309, 1009, 2066], "temperature": 0.0, "avg_logprob": -0.34546710283328325, "compression_ratio": 1.2612612612612613, "no_speech_prob": 4.133275433559902e-05}, {"id": 655, "seek": 312658, "start": 3126.58, "end": 3132.38, "text": " categorical and continuous together", "tokens": [19250, 804, 293, 10957, 1214], "temperature": 0.0, "avg_logprob": -0.24633430712150806, "compression_ratio": 1.490566037735849, "no_speech_prob": 6.276658268689062e-07}, {"id": 656, "seek": 312658, "start": 3133.2599999999998, "end": 3136.54, "text": " Yes, it does and the", "tokens": [1079, 11, 309, 775, 293, 264], "temperature": 0.0, "avg_logprob": -0.24633430712150806, "compression_ratio": 1.490566037735849, "no_speech_prob": 6.276658268689062e-07}, {"id": 657, "seek": 312658, "start": 3137.66, "end": 3143.22, "text": " The model data behind the scenes if there are no none of the other type it creates a", "tokens": [440, 2316, 1412, 2261, 264, 8026, 498, 456, 366, 572, 6022, 295, 264, 661, 2010, 309, 7829, 257], "temperature": 0.0, "avg_logprob": -0.24633430712150806, "compression_ratio": 1.490566037735849, "no_speech_prob": 6.276658268689062e-07}, {"id": 658, "seek": 312658, "start": 3144.2999999999997, "end": 3146.2999999999997, "text": " Column of ones or zeros or something", "tokens": [4004, 16449, 295, 2306, 420, 35193, 420, 746], "temperature": 0.0, "avg_logprob": -0.24633430712150806, "compression_ratio": 1.490566037735849, "no_speech_prob": 6.276658268689062e-07}, {"id": 659, "seek": 312658, "start": 3147.14, "end": 3152.74, "text": " Okay, so if it is null it can still work. Yeah, yeah, yeah", "tokens": [1033, 11, 370, 498, 309, 307, 18184, 309, 393, 920, 589, 13, 865, 11, 1338, 11, 1338], "temperature": 0.0, "avg_logprob": -0.24633430712150806, "compression_ratio": 1.490566037735849, "no_speech_prob": 6.276658268689062e-07}, {"id": 660, "seek": 315274, "start": 3152.74, "end": 3158.06, "text": " It's kind of ugly and hacky and will you know hopefully improve it", "tokens": [467, 311, 733, 295, 12246, 293, 10339, 88, 293, 486, 291, 458, 4696, 3470, 309], "temperature": 0.0, "avg_logprob": -0.2258416479760474, "compression_ratio": 1.5565217391304347, "no_speech_prob": 1.9333460841153283e-06}, {"id": 661, "seek": 315274, "start": 3158.06, "end": 3160.7, "text": " but but yeah, you can pass in an empty list of", "tokens": [457, 457, 1338, 11, 291, 393, 1320, 294, 364, 6707, 1329, 295], "temperature": 0.0, "avg_logprob": -0.2258416479760474, "compression_ratio": 1.5565217391304347, "no_speech_prob": 1.9333460841153283e-06}, {"id": 662, "seek": 315274, "start": 3161.4199999999996, "end": 3169.22, "text": " Categorical or continuous variables to the model data, and it will basically yeah, it'll basically pass and unused column of zeros", "tokens": [383, 2968, 284, 804, 420, 10957, 9102, 281, 264, 2316, 1412, 11, 293, 309, 486, 1936, 1338, 11, 309, 603, 1936, 1320, 293, 44383, 7738, 295, 35193], "temperature": 0.0, "avg_logprob": -0.2258416479760474, "compression_ratio": 1.5565217391304347, "no_speech_prob": 1.9333460841153283e-06}, {"id": 663, "seek": 315274, "start": 3170.22, "end": 3172.22, "text": " to avoid things breaking", "tokens": [281, 5042, 721, 7697], "temperature": 0.0, "avg_logprob": -0.2258416479760474, "compression_ratio": 1.5565217391304347, "no_speech_prob": 1.9333460841153283e-06}, {"id": 664, "seek": 315274, "start": 3173.14, "end": 3180.02, "text": " And I'm I'm leaving fixing some of these slightly hacky edge cases because pie torch 0.4", "tokens": [400, 286, 478, 286, 478, 5012, 19442, 512, 295, 613, 4748, 10339, 88, 4691, 3331, 570, 1730, 27822, 1958, 13, 19], "temperature": 0.0, "avg_logprob": -0.2258416479760474, "compression_ratio": 1.5565217391304347, "no_speech_prob": 1.9333460841153283e-06}, {"id": 665, "seek": 318002, "start": 3180.02, "end": 3185.82, "text": " As well as getting rid of variables they're going to also add rank zero tensors", "tokens": [1018, 731, 382, 1242, 3973, 295, 9102, 436, 434, 516, 281, 611, 909, 6181, 4018, 10688, 830], "temperature": 0.0, "avg_logprob": -0.1743492029481015, "compression_ratio": 1.7471698113207548, "no_speech_prob": 2.947998837043997e-06}, {"id": 666, "seek": 318002, "start": 3186.2599999999998, "end": 3192.94, "text": " Which is to say if you grab a single thing out of like a rank one tensor rather than getting back at a number", "tokens": [3013, 307, 281, 584, 498, 291, 4444, 257, 2167, 551, 484, 295, 411, 257, 6181, 472, 40863, 2831, 813, 1242, 646, 412, 257, 1230], "temperature": 0.0, "avg_logprob": -0.1743492029481015, "compression_ratio": 1.7471698113207548, "no_speech_prob": 2.947998837043997e-06}, {"id": 667, "seek": 318002, "start": 3193.38, "end": 3195.2599999999998, "text": " Which is like?", "tokens": [3013, 307, 411, 30], "temperature": 0.0, "avg_logprob": -0.1743492029481015, "compression_ratio": 1.7471698113207548, "no_speech_prob": 2.947998837043997e-06}, {"id": 668, "seek": 318002, "start": 3195.2599999999998, "end": 3199.86, "text": " Qualitatively different you're actually going to get back at a tensor that just happens to have no rank", "tokens": [13616, 14275, 356, 819, 291, 434, 767, 516, 281, 483, 646, 412, 257, 40863, 300, 445, 2314, 281, 362, 572, 6181], "temperature": 0.0, "avg_logprob": -0.1743492029481015, "compression_ratio": 1.7471698113207548, "no_speech_prob": 2.947998837043997e-06}, {"id": 669, "seek": 318002, "start": 3199.94, "end": 3204.06, "text": " Now it turns out that a lot of this kind of code is going to be much easier to write then so", "tokens": [823, 309, 4523, 484, 300, 257, 688, 295, 341, 733, 295, 3089, 307, 516, 281, 312, 709, 3571, 281, 2464, 550, 370], "temperature": 0.0, "avg_logprob": -0.1743492029481015, "compression_ratio": 1.7471698113207548, "no_speech_prob": 2.947998837043997e-06}, {"id": 670, "seek": 320406, "start": 3204.06, "end": 3210.14, "text": " And for now it's it's a little bit more hacky than it needs to be", "tokens": [400, 337, 586, 309, 311, 309, 311, 257, 707, 857, 544, 10339, 88, 813, 309, 2203, 281, 312], "temperature": 0.0, "avg_logprob": -0.21280751369967318, "compression_ratio": 1.6754385964912282, "no_speech_prob": 5.771741143689724e-06}, {"id": 671, "seek": 320406, "start": 3210.9, "end": 3215.9, "text": " Jeremy you talk about this a little bit before where maybe it's a good time at some point to talk about", "tokens": [17809, 291, 751, 466, 341, 257, 707, 857, 949, 689, 1310, 309, 311, 257, 665, 565, 412, 512, 935, 281, 751, 466], "temperature": 0.0, "avg_logprob": -0.21280751369967318, "compression_ratio": 1.6754385964912282, "no_speech_prob": 5.771741143689724e-06}, {"id": 672, "seek": 320406, "start": 3216.66, "end": 3218.46, "text": " How can we?", "tokens": [1012, 393, 321, 30], "temperature": 0.0, "avg_logprob": -0.21280751369967318, "compression_ratio": 1.6754385964912282, "no_speech_prob": 5.771741143689724e-06}, {"id": 673, "seek": 320406, "start": 3218.46, "end": 3222.02, "text": " Write something that is slightly different for worries of the library", "tokens": [23499, 746, 300, 307, 4748, 819, 337, 16340, 295, 264, 6405], "temperature": 0.0, "avg_logprob": -0.21280751369967318, "compression_ratio": 1.6754385964912282, "no_speech_prob": 5.771741143689724e-06}, {"id": 674, "seek": 320406, "start": 3222.54, "end": 3224.5, "text": " Yeah, I", "tokens": [865, 11, 286], "temperature": 0.0, "avg_logprob": -0.21280751369967318, "compression_ratio": 1.6754385964912282, "no_speech_prob": 5.771741143689724e-06}, {"id": 675, "seek": 320406, "start": 3224.5, "end": 3231.54, "text": " Think we'll cover that a little bit next week, but I'm mainly going to do that in part two like part two is going to cover", "tokens": [6557, 321, 603, 2060, 300, 257, 707, 857, 958, 1243, 11, 457, 286, 478, 8704, 516, 281, 360, 300, 294, 644, 732, 411, 644, 732, 307, 516, 281, 2060], "temperature": 0.0, "avg_logprob": -0.21280751369967318, "compression_ratio": 1.6754385964912282, "no_speech_prob": 5.771741143689724e-06}, {"id": 676, "seek": 323154, "start": 3231.54, "end": 3233.22, "text": " a", "tokens": [257], "temperature": 0.0, "avg_logprob": -0.15761370129055446, "compression_ratio": 1.5669642857142858, "no_speech_prob": 1.933344947246951e-06}, {"id": 677, "seek": 323154, "start": 3233.22, "end": 3238.06, "text": " Quite a lot of stuff one of the main things we'll cover in part two is what it called generative models", "tokens": [20464, 257, 688, 295, 1507, 472, 295, 264, 2135, 721, 321, 603, 2060, 294, 644, 732, 307, 437, 309, 1219, 1337, 1166, 5245], "temperature": 0.0, "avg_logprob": -0.15761370129055446, "compression_ratio": 1.5669642857142858, "no_speech_prob": 1.933344947246951e-06}, {"id": 678, "seek": 323154, "start": 3238.06, "end": 3244.66, "text": " So things where the output is a whole sentence or a whole image, but you know I also dig into like how to really", "tokens": [407, 721, 689, 264, 5598, 307, 257, 1379, 8174, 420, 257, 1379, 3256, 11, 457, 291, 458, 286, 611, 2528, 666, 411, 577, 281, 534], "temperature": 0.0, "avg_logprob": -0.15761370129055446, "compression_ratio": 1.5669642857142858, "no_speech_prob": 1.933344947246951e-06}, {"id": 679, "seek": 323154, "start": 3247.3, "end": 3252.7799999999997, "text": " Either customize the fast AI library or use it on on more custom models", "tokens": [13746, 19734, 264, 2370, 7318, 6405, 420, 764, 309, 322, 322, 544, 2375, 5245], "temperature": 0.0, "avg_logprob": -0.15761370129055446, "compression_ratio": 1.5669642857142858, "no_speech_prob": 1.933344947246951e-06}, {"id": 680, "seek": 323154, "start": 3254.34, "end": 3258.34, "text": " But if we have time we'll touch on it a little bit next week", "tokens": [583, 498, 321, 362, 565, 321, 603, 2557, 322, 309, 257, 707, 857, 958, 1243], "temperature": 0.0, "avg_logprob": -0.15761370129055446, "compression_ratio": 1.5669642857142858, "no_speech_prob": 1.933344947246951e-06}, {"id": 681, "seek": 325834, "start": 3258.34, "end": 3260.34, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.18670009530108908, "compression_ratio": 1.8468899521531101, "no_speech_prob": 1.7061673815987888e-06}, {"id": 682, "seek": 325834, "start": 3261.9, "end": 3269.1400000000003, "text": " The the learner we were passing in a list of embedding sizes and as you can see that embedding sizes list was literally just the", "tokens": [440, 264, 33347, 321, 645, 8437, 294, 257, 1329, 295, 12240, 3584, 11602, 293, 382, 291, 393, 536, 300, 12240, 3584, 11602, 1329, 390, 3736, 445, 264], "temperature": 0.0, "avg_logprob": -0.18670009530108908, "compression_ratio": 1.8468899521531101, "no_speech_prob": 1.7061673815987888e-06}, {"id": 683, "seek": 325834, "start": 3269.1400000000003, "end": 3274.98, "text": " Number of rows and the number of columns in each embedding right and the number of code rows was just coming from", "tokens": [5118, 295, 13241, 293, 264, 1230, 295, 13766, 294, 1184, 12240, 3584, 558, 293, 264, 1230, 295, 3089, 13241, 390, 445, 1348, 490], "temperature": 0.0, "avg_logprob": -0.18670009530108908, "compression_ratio": 1.8468899521531101, "no_speech_prob": 1.7061673815987888e-06}, {"id": 684, "seek": 325834, "start": 3276.06, "end": 3279.1800000000003, "text": " Literally how many stores are there in the store?", "tokens": [23768, 577, 867, 9512, 366, 456, 294, 264, 3531, 30], "temperature": 0.0, "avg_logprob": -0.18670009530108908, "compression_ratio": 1.8468899521531101, "no_speech_prob": 1.7061673815987888e-06}, {"id": 685, "seek": 325834, "start": 3280.02, "end": 3285.1800000000003, "text": " Category for example and the number of columns was just equal to that divided by two", "tokens": [383, 48701, 337, 1365, 293, 264, 1230, 295, 13766, 390, 445, 2681, 281, 300, 6666, 538, 732], "temperature": 0.0, "avg_logprob": -0.18670009530108908, "compression_ratio": 1.8468899521531101, "no_speech_prob": 1.7061673815987888e-06}, {"id": 686, "seek": 328518, "start": 3285.18, "end": 3291.2599999999998, "text": " And a maximum of 50 so that thing that list of tuples was coming in and so you can see here", "tokens": [400, 257, 6674, 295, 2625, 370, 300, 551, 300, 1329, 295, 2604, 2622, 390, 1348, 294, 293, 370, 291, 393, 536, 510], "temperature": 0.0, "avg_logprob": -0.16505753475686777, "compression_ratio": 1.6325581395348838, "no_speech_prob": 1.8162136257160455e-06}, {"id": 687, "seek": 328518, "start": 3291.2599999999998, "end": 3293.66, "text": " How we use it right we go through each of those tuples", "tokens": [1012, 321, 764, 309, 558, 321, 352, 807, 1184, 295, 729, 2604, 2622], "temperature": 0.0, "avg_logprob": -0.16505753475686777, "compression_ratio": 1.6325581395348838, "no_speech_prob": 1.8162136257160455e-06}, {"id": 688, "seek": 328518, "start": 3294.2599999999998, "end": 3295.8199999999997, "text": " grab the", "tokens": [4444, 264], "temperature": 0.0, "avg_logprob": -0.16505753475686777, "compression_ratio": 1.6325581395348838, "no_speech_prob": 1.8162136257160455e-06}, {"id": 689, "seek": 328518, "start": 3295.8199999999997, "end": 3298.8599999999997, "text": " number of categories and the size of the embedding and", "tokens": [1230, 295, 10479, 293, 264, 2744, 295, 264, 12240, 3584, 293], "temperature": 0.0, "avg_logprob": -0.16505753475686777, "compression_ratio": 1.6325581395348838, "no_speech_prob": 1.8162136257160455e-06}, {"id": 690, "seek": 328518, "start": 3299.74, "end": 3301.5, "text": " construct an embedding", "tokens": [7690, 364, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.16505753475686777, "compression_ratio": 1.6325581395348838, "no_speech_prob": 1.8162136257160455e-06}, {"id": 691, "seek": 328518, "start": 3301.5, "end": 3308.8199999999997, "text": " Right and so that's a that's a list right one minor thing pie torch specific thing. We haven't talked about before is", "tokens": [1779, 293, 370, 300, 311, 257, 300, 311, 257, 1329, 558, 472, 6696, 551, 1730, 27822, 2685, 551, 13, 492, 2378, 380, 2825, 466, 949, 307], "temperature": 0.0, "avg_logprob": -0.16505753475686777, "compression_ratio": 1.6325581395348838, "no_speech_prob": 1.8162136257160455e-06}, {"id": 692, "seek": 330882, "start": 3308.82, "end": 3316.34, "text": " for it to be able to like register remember how we kind of said like it registers your parameters it registers your", "tokens": [337, 309, 281, 312, 1075, 281, 411, 7280, 1604, 577, 321, 733, 295, 848, 411, 309, 38351, 428, 9834, 309, 38351, 428], "temperature": 0.0, "avg_logprob": -0.21291277668263653, "compression_ratio": 1.7796610169491525, "no_speech_prob": 1.933351541083539e-06}, {"id": 693, "seek": 330882, "start": 3316.6200000000003, "end": 3322.1000000000004, "text": " Your layers like so when we like listed the model it actually printed out the name of each embedding and each bias", "tokens": [2260, 7914, 411, 370, 562, 321, 411, 10052, 264, 2316, 309, 767, 13567, 484, 264, 1315, 295, 1184, 12240, 3584, 293, 1184, 12577], "temperature": 0.0, "avg_logprob": -0.21291277668263653, "compression_ratio": 1.7796610169491525, "no_speech_prob": 1.933351541083539e-06}, {"id": 694, "seek": 330882, "start": 3323.06, "end": 3328.46, "text": " It can't do that if they're hidden inside a list right they have to be like a they have to be a", "tokens": [467, 393, 380, 360, 300, 498, 436, 434, 7633, 1854, 257, 1329, 558, 436, 362, 281, 312, 411, 257, 436, 362, 281, 312, 257], "temperature": 0.0, "avg_logprob": -0.21291277668263653, "compression_ratio": 1.7796610169491525, "no_speech_prob": 1.933351541083539e-06}, {"id": 695, "seek": 330882, "start": 3328.98, "end": 3335.94, "text": " An actual and end up module subclass, so there's a special thing called an NN dot module list", "tokens": [1107, 3539, 293, 917, 493, 10088, 1422, 11665, 11, 370, 456, 311, 257, 2121, 551, 1219, 364, 426, 45, 5893, 10088, 1329], "temperature": 0.0, "avg_logprob": -0.21291277668263653, "compression_ratio": 1.7796610169491525, "no_speech_prob": 1.933351541083539e-06}, {"id": 696, "seek": 333594, "start": 3335.94, "end": 3343.6, "text": " Which takes a list and it basically says I want you to register everything in here as being part of this model", "tokens": [3013, 2516, 257, 1329, 293, 309, 1936, 1619, 286, 528, 291, 281, 7280, 1203, 294, 510, 382, 885, 644, 295, 341, 2316], "temperature": 0.0, "avg_logprob": -0.1820491971196355, "compression_ratio": 1.5159574468085106, "no_speech_prob": 1.8448167793394532e-06}, {"id": 697, "seek": 333594, "start": 3343.7000000000003, "end": 3345.7000000000003, "text": " Okay, so it's just a minor tweak", "tokens": [1033, 11, 370, 309, 311, 445, 257, 6696, 29879], "temperature": 0.0, "avg_logprob": -0.1820491971196355, "compression_ratio": 1.5159574468085106, "no_speech_prob": 1.8448167793394532e-06}, {"id": 698, "seek": 333594, "start": 3346.86, "end": 3349.9, "text": " So yeah, so our mixed input model has a list of embeddings", "tokens": [407, 1338, 11, 370, 527, 7467, 4846, 2316, 575, 257, 1329, 295, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.1820491971196355, "compression_ratio": 1.5159574468085106, "no_speech_prob": 1.8448167793394532e-06}, {"id": 699, "seek": 333594, "start": 3352.26, "end": 3359.14, "text": " And then I do the same thing for a list of linear layers right so when I said here", "tokens": [400, 550, 286, 360, 264, 912, 551, 337, 257, 1329, 295, 8213, 7914, 558, 370, 562, 286, 848, 510], "temperature": 0.0, "avg_logprob": -0.1820491971196355, "compression_ratio": 1.5159574468085106, "no_speech_prob": 1.8448167793394532e-06}, {"id": 700, "seek": 335914, "start": 3359.14, "end": 3367.7, "text": " 1000, 500 this is saying how many activations I wanted featured my linear layers", "tokens": [9714, 11, 5923, 341, 307, 1566, 577, 867, 2430, 763, 286, 1415, 13822, 452, 8213, 7914], "temperature": 0.0, "avg_logprob": -0.18237382012444572, "compression_ratio": 1.5508021390374331, "no_speech_prob": 3.52090921751369e-07}, {"id": 701, "seek": 335914, "start": 3368.2599999999998, "end": 3375.54, "text": " Okay, and so here. I just go through that list and create a linear layer that goes from this size", "tokens": [1033, 11, 293, 370, 510, 13, 286, 445, 352, 807, 300, 1329, 293, 1884, 257, 8213, 4583, 300, 1709, 490, 341, 2744], "temperature": 0.0, "avg_logprob": -0.18237382012444572, "compression_ratio": 1.5508021390374331, "no_speech_prob": 3.52090921751369e-07}, {"id": 702, "seek": 335914, "start": 3376.3399999999997, "end": 3378.2599999999998, "text": " to the next size", "tokens": [281, 264, 958, 2744], "temperature": 0.0, "avg_logprob": -0.18237382012444572, "compression_ratio": 1.5508021390374331, "no_speech_prob": 3.52090921751369e-07}, {"id": 703, "seek": 335914, "start": 3378.2599999999998, "end": 3383.2999999999997, "text": " Okay, so you can see like how easy it is to kind of construct your own not just your own model", "tokens": [1033, 11, 370, 291, 393, 536, 411, 577, 1858, 309, 307, 281, 733, 295, 7690, 428, 1065, 406, 445, 428, 1065, 2316], "temperature": 0.0, "avg_logprob": -0.18237382012444572, "compression_ratio": 1.5508021390374331, "no_speech_prob": 3.52090921751369e-07}, {"id": 704, "seek": 338330, "start": 3383.3, "end": 3389.1000000000004, "text": " But a kind of a model which you can pass parameters to have it constructed on the fly dynamically", "tokens": [583, 257, 733, 295, 257, 2316, 597, 291, 393, 1320, 9834, 281, 362, 309, 17083, 322, 264, 3603, 43492], "temperature": 0.0, "avg_logprob": -0.20256590541405015, "compression_ratio": 1.5660377358490567, "no_speech_prob": 1.5534930071225972e-06}, {"id": 705, "seek": 338330, "start": 3389.54, "end": 3391.54, "text": " That's normal talk about next week", "tokens": [663, 311, 2710, 751, 466, 958, 1243], "temperature": 0.0, "avg_logprob": -0.20256590541405015, "compression_ratio": 1.5660377358490567, "no_speech_prob": 1.5534930071225972e-06}, {"id": 706, "seek": 338330, "start": 3392.78, "end": 3397.78, "text": " This is initialization. We've mentioned timing her initialization before and we mentioned it last week", "tokens": [639, 307, 5883, 2144, 13, 492, 600, 2835, 10822, 720, 5883, 2144, 949, 293, 321, 2835, 309, 1036, 1243], "temperature": 0.0, "avg_logprob": -0.20256590541405015, "compression_ratio": 1.5660377358490567, "no_speech_prob": 1.5534930071225972e-06}, {"id": 707, "seek": 338330, "start": 3400.6200000000003, "end": 3402.6200000000003, "text": " And then", "tokens": [400, 550], "temperature": 0.0, "avg_logprob": -0.20256590541405015, "compression_ratio": 1.5660377358490567, "no_speech_prob": 1.5534930071225972e-06}, {"id": 708, "seek": 338330, "start": 3402.7400000000002, "end": 3407.1000000000004, "text": " Dropout same thing right we have here a list of how much dropout to apply to each layer", "tokens": [17675, 346, 912, 551, 558, 321, 362, 510, 257, 1329, 295, 577, 709, 3270, 346, 281, 3079, 281, 1184, 4583], "temperature": 0.0, "avg_logprob": -0.20256590541405015, "compression_ratio": 1.5660377358490567, "no_speech_prob": 1.5534930071225972e-06}, {"id": 709, "seek": 340710, "start": 3407.1, "end": 3413.38, "text": " All right, so again here. It's just like go through each thing in that list and create a dropout layer for it", "tokens": [1057, 558, 11, 370, 797, 510, 13, 467, 311, 445, 411, 352, 807, 1184, 551, 294, 300, 1329, 293, 1884, 257, 3270, 346, 4583, 337, 309], "temperature": 0.0, "avg_logprob": -0.14725287909646637, "compression_ratio": 1.6784313725490196, "no_speech_prob": 1.0030119028670015e-06}, {"id": 710, "seek": 340710, "start": 3413.5, "end": 3421.02, "text": " Okay, so this constructor we understand everything in it except for batch norm which we don't have to worry about for now", "tokens": [1033, 11, 370, 341, 47479, 321, 1223, 1203, 294, 309, 3993, 337, 15245, 2026, 597, 321, 500, 380, 362, 281, 3292, 466, 337, 586], "temperature": 0.0, "avg_logprob": -0.14725287909646637, "compression_ratio": 1.6784313725490196, "no_speech_prob": 1.0030119028670015e-06}, {"id": 711, "seek": 340710, "start": 3421.98, "end": 3424.92, "text": " So that's the constructor and so then the forward", "tokens": [407, 300, 311, 264, 47479, 293, 370, 550, 264, 2128], "temperature": 0.0, "avg_logprob": -0.14725287909646637, "compression_ratio": 1.6784313725490196, "no_speech_prob": 1.0030119028670015e-06}, {"id": 712, "seek": 340710, "start": 3427.02, "end": 3428.7799999999997, "text": " Also, you know all stuff", "tokens": [2743, 11, 291, 458, 439, 1507], "temperature": 0.0, "avg_logprob": -0.14725287909646637, "compression_ratio": 1.6784313725490196, "no_speech_prob": 1.0030119028670015e-06}, {"id": 713, "seek": 340710, "start": 3428.7799999999997, "end": 3434.9, "text": " We're aware of go through each of those embedding layers that we just saw and remember we just treated like as a function", "tokens": [492, 434, 3650, 295, 352, 807, 1184, 295, 729, 12240, 3584, 7914, 300, 321, 445, 1866, 293, 1604, 321, 445, 8668, 411, 382, 257, 2445], "temperature": 0.0, "avg_logprob": -0.14725287909646637, "compression_ratio": 1.6784313725490196, "no_speech_prob": 1.0030119028670015e-06}, {"id": 714, "seek": 343490, "start": 3434.9, "end": 3439.84, "text": " So call it with the Ith categorical variable and then concatenate them all together", "tokens": [407, 818, 309, 365, 264, 286, 392, 19250, 804, 7006, 293, 550, 1588, 7186, 473, 552, 439, 1214], "temperature": 0.0, "avg_logprob": -0.26370776141131363, "compression_ratio": 1.8372093023255813, "no_speech_prob": 1.3497012787411222e-06}, {"id": 715, "seek": 343490, "start": 3441.58, "end": 3443.58, "text": " Put that through dropout", "tokens": [4935, 300, 807, 3270, 346], "temperature": 0.0, "avg_logprob": -0.26370776141131363, "compression_ratio": 1.8372093023255813, "no_speech_prob": 1.3497012787411222e-06}, {"id": 716, "seek": 343490, "start": 3444.42, "end": 3449.42, "text": " And then go through each one of our linear layers and call it", "tokens": [400, 550, 352, 807, 1184, 472, 295, 527, 8213, 7914, 293, 818, 309], "temperature": 0.0, "avg_logprob": -0.26370776141131363, "compression_ratio": 1.8372093023255813, "no_speech_prob": 1.3497012787411222e-06}, {"id": 717, "seek": 343490, "start": 3450.6600000000003, "end": 3452.6600000000003, "text": " Apply relu to it", "tokens": [25264, 1039, 84, 281, 309], "temperature": 0.0, "avg_logprob": -0.26370776141131363, "compression_ratio": 1.8372093023255813, "no_speech_prob": 1.3497012787411222e-06}, {"id": 718, "seek": 343490, "start": 3452.7000000000003, "end": 3458.7000000000003, "text": " Apply dropout to it right and then finally apply the final linear layer and the final linear layer", "tokens": [25264, 3270, 346, 281, 309, 558, 293, 550, 2721, 3079, 264, 2572, 8213, 4583, 293, 264, 2572, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.26370776141131363, "compression_ratio": 1.8372093023255813, "no_speech_prob": 1.3497012787411222e-06}, {"id": 719, "seek": 343490, "start": 3459.6600000000003, "end": 3462.94, "text": " has this as its size which is", "tokens": [575, 341, 382, 1080, 2744, 597, 307], "temperature": 0.0, "avg_logprob": -0.26370776141131363, "compression_ratio": 1.8372093023255813, "no_speech_prob": 1.3497012787411222e-06}, {"id": 720, "seek": 346294, "start": 3462.94, "end": 3464.94, "text": " Here", "tokens": [1692], "temperature": 0.0, "avg_logprob": -0.18042869921083804, "compression_ratio": 1.6259541984732824, "no_speech_prob": 2.4060959731286857e-06}, {"id": 721, "seek": 346294, "start": 3465.2200000000003, "end": 3466.54, "text": " Yeah", "tokens": [865], "temperature": 0.0, "avg_logprob": -0.18042869921083804, "compression_ratio": 1.6259541984732824, "no_speech_prob": 2.4060959731286857e-06}, {"id": 722, "seek": 346294, "start": 3466.54, "end": 3469.86, "text": " Right size one. There's a single unit sales", "tokens": [1779, 2744, 472, 13, 821, 311, 257, 2167, 4985, 5763], "temperature": 0.0, "avg_logprob": -0.18042869921083804, "compression_ratio": 1.6259541984732824, "no_speech_prob": 2.4060959731286857e-06}, {"id": 723, "seek": 346294, "start": 3470.26, "end": 3476.46, "text": " Okay, so we're kind of getting to the point where oh and then of course at the end if this I mentioned", "tokens": [1033, 11, 370, 321, 434, 733, 295, 1242, 281, 264, 935, 689, 1954, 293, 550, 295, 1164, 412, 264, 917, 498, 341, 286, 2835], "temperature": 0.0, "avg_logprob": -0.18042869921083804, "compression_ratio": 1.6259541984732824, "no_speech_prob": 2.4060959731286857e-06}, {"id": 724, "seek": 346294, "start": 3476.46, "end": 3480.86, "text": " We would come back to this if you passed in a y underscore range parameter", "tokens": [492, 576, 808, 646, 281, 341, 498, 291, 4678, 294, 257, 288, 37556, 3613, 13075], "temperature": 0.0, "avg_logprob": -0.18042869921083804, "compression_ratio": 1.6259541984732824, "no_speech_prob": 2.4060959731286857e-06}, {"id": 725, "seek": 346294, "start": 3481.02, "end": 3483.58, "text": " Then we're going to do the thing we just learned about last week", "tokens": [1396, 321, 434, 516, 281, 360, 264, 551, 321, 445, 3264, 466, 1036, 1243], "temperature": 0.0, "avg_logprob": -0.18042869921083804, "compression_ratio": 1.6259541984732824, "no_speech_prob": 2.4060959731286857e-06}, {"id": 726, "seek": 346294, "start": 3483.66, "end": 3490.44, "text": " Which is to use a sigmoid right and this is a cool little trick to make your not just to make your collaborative filtering better", "tokens": [3013, 307, 281, 764, 257, 4556, 3280, 327, 558, 293, 341, 307, 257, 1627, 707, 4282, 281, 652, 428, 406, 445, 281, 652, 428, 16555, 30822, 1101], "temperature": 0.0, "avg_logprob": -0.18042869921083804, "compression_ratio": 1.6259541984732824, "no_speech_prob": 2.4060959731286857e-06}, {"id": 727, "seek": 349044, "start": 3490.44, "end": 3492.62, "text": " but in this case my basic idea was", "tokens": [457, 294, 341, 1389, 452, 3875, 1558, 390], "temperature": 0.0, "avg_logprob": -0.18113901528967433, "compression_ratio": 1.5487179487179488, "no_speech_prob": 1.7061792050299118e-06}, {"id": 728, "seek": 349044, "start": 3494.04, "end": 3497.76, "text": " You know sales are going to be greater than zero", "tokens": [509, 458, 5763, 366, 516, 281, 312, 5044, 813, 4018], "temperature": 0.0, "avg_logprob": -0.18113901528967433, "compression_ratio": 1.5487179487179488, "no_speech_prob": 1.7061792050299118e-06}, {"id": 729, "seek": 349044, "start": 3498.44, "end": 3501.98, "text": " And probably less than the largest sale they've ever had", "tokens": [400, 1391, 1570, 813, 264, 6443, 8680, 436, 600, 1562, 632], "temperature": 0.0, "avg_logprob": -0.18113901528967433, "compression_ratio": 1.5487179487179488, "no_speech_prob": 1.7061792050299118e-06}, {"id": 730, "seek": 349044, "start": 3502.7200000000003, "end": 3504.7200000000003, "text": " So I just pass in", "tokens": [407, 286, 445, 1320, 294], "temperature": 0.0, "avg_logprob": -0.18113901528967433, "compression_ratio": 1.5487179487179488, "no_speech_prob": 1.7061792050299118e-06}, {"id": 731, "seek": 349044, "start": 3505.52, "end": 3513.4, "text": " That as y range and so we do a sigmoid and multiply with the sigmoid by the range that I passed it right and so", "tokens": [663, 382, 288, 3613, 293, 370, 321, 360, 257, 4556, 3280, 327, 293, 12972, 365, 264, 4556, 3280, 327, 538, 264, 3613, 300, 286, 4678, 309, 558, 293, 370], "temperature": 0.0, "avg_logprob": -0.18113901528967433, "compression_ratio": 1.5487179487179488, "no_speech_prob": 1.7061792050299118e-06}, {"id": 732, "seek": 349044, "start": 3514.44, "end": 3516.44, "text": " Hopefully we can find that here", "tokens": [10429, 321, 393, 915, 300, 510], "temperature": 0.0, "avg_logprob": -0.18113901528967433, "compression_ratio": 1.5487179487179488, "no_speech_prob": 1.7061792050299118e-06}, {"id": 733, "seek": 351644, "start": 3516.44, "end": 3521.92, "text": " Yeah, here. It is right, so I actually said hey, maybe the range is between zero and", "tokens": [865, 11, 510, 13, 467, 307, 558, 11, 370, 286, 767, 848, 4177, 11, 1310, 264, 3613, 307, 1296, 4018, 293], "temperature": 0.0, "avg_logprob": -0.19413841687715971, "compression_ratio": 1.6196581196581197, "no_speech_prob": 1.2098630577384029e-06}, {"id": 734, "seek": 351644, "start": 3522.76, "end": 3526.04, "text": " You know the highest times 1.2. You know because maybe", "tokens": [509, 458, 264, 6343, 1413, 502, 13, 17, 13, 509, 458, 570, 1310], "temperature": 0.0, "avg_logprob": -0.19413841687715971, "compression_ratio": 1.6196581196581197, "no_speech_prob": 1.2098630577384029e-06}, {"id": 735, "seek": 351644, "start": 3526.76, "end": 3528.7200000000003, "text": " Maybe the next two weeks we have one bigger", "tokens": [2704, 264, 958, 732, 3259, 321, 362, 472, 3801], "temperature": 0.0, "avg_logprob": -0.19413841687715971, "compression_ratio": 1.6196581196581197, "no_speech_prob": 1.2098630577384029e-06}, {"id": 736, "seek": 351644, "start": 3528.7200000000003, "end": 3534.56, "text": " But this is kind of like again trying to make it a little bit easier for it to give us the kind of results that it", "tokens": [583, 341, 307, 733, 295, 411, 797, 1382, 281, 652, 309, 257, 707, 857, 3571, 337, 309, 281, 976, 505, 264, 733, 295, 3542, 300, 309], "temperature": 0.0, "avg_logprob": -0.19413841687715971, "compression_ratio": 1.6196581196581197, "no_speech_prob": 1.2098630577384029e-06}, {"id": 737, "seek": 351644, "start": 3534.56, "end": 3535.7200000000003, "text": " Thinks is right", "tokens": [6557, 82, 307, 558], "temperature": 0.0, "avg_logprob": -0.19413841687715971, "compression_ratio": 1.6196581196581197, "no_speech_prob": 1.2098630577384029e-06}, {"id": 738, "seek": 351644, "start": 3535.7200000000003, "end": 3537.7200000000003, "text": " so like increasingly", "tokens": [370, 411, 12980], "temperature": 0.0, "avg_logprob": -0.19413841687715971, "compression_ratio": 1.6196581196581197, "no_speech_prob": 1.2098630577384029e-06}, {"id": 739, "seek": 351644, "start": 3537.92, "end": 3540.7200000000003, "text": " You know I'd love you all to kind of try to", "tokens": [509, 458, 286, 1116, 959, 291, 439, 281, 733, 295, 853, 281], "temperature": 0.0, "avg_logprob": -0.19413841687715971, "compression_ratio": 1.6196581196581197, "no_speech_prob": 1.2098630577384029e-06}, {"id": 740, "seek": 354072, "start": 3540.72, "end": 3547.16, "text": " Not treat these learners and models as black boxes", "tokens": [1726, 2387, 613, 23655, 293, 5245, 382, 2211, 9002], "temperature": 0.0, "avg_logprob": -0.20523434791012088, "compression_ratio": 1.516304347826087, "no_speech_prob": 1.6280450836347882e-06}, {"id": 741, "seek": 354072, "start": 3547.16, "end": 3553.24, "text": " But to feel like you now have the information you need to look inside them and remember you could then copy and paste this plus", "tokens": [583, 281, 841, 411, 291, 586, 362, 264, 1589, 291, 643, 281, 574, 1854, 552, 293, 1604, 291, 727, 550, 5055, 293, 9163, 341, 1804], "temperature": 0.0, "avg_logprob": -0.20523434791012088, "compression_ratio": 1.516304347826087, "no_speech_prob": 1.6280450836347882e-06}, {"id": 742, "seek": 354072, "start": 3553.64, "end": 3555.64, "text": " paste it into a cell in", "tokens": [9163, 309, 666, 257, 2815, 294], "temperature": 0.0, "avg_logprob": -0.20523434791012088, "compression_ratio": 1.516304347826087, "no_speech_prob": 1.6280450836347882e-06}, {"id": 743, "seek": 354072, "start": 3556.56, "end": 3558.4399999999996, "text": " Jupiter notebook and", "tokens": [24567, 21060, 293], "temperature": 0.0, "avg_logprob": -0.20523434791012088, "compression_ratio": 1.516304347826087, "no_speech_prob": 1.6280450836347882e-06}, {"id": 744, "seek": 354072, "start": 3558.4399999999996, "end": 3561.4199999999996, "text": " Start fiddling with it to create your own versions", "tokens": [6481, 283, 14273, 1688, 365, 309, 281, 1884, 428, 1065, 9606], "temperature": 0.0, "avg_logprob": -0.20523434791012088, "compression_ratio": 1.516304347826087, "no_speech_prob": 1.6280450836347882e-06}, {"id": 745, "seek": 354072, "start": 3565.08, "end": 3567.08, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.20523434791012088, "compression_ratio": 1.516304347826087, "no_speech_prob": 1.6280450836347882e-06}, {"id": 746, "seek": 356708, "start": 3567.08, "end": 3569.08, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.15186044234263746, "compression_ratio": 1.5056818181818181, "no_speech_prob": 7.071754680509912e-06}, {"id": 747, "seek": 356708, "start": 3570.52, "end": 3576.68, "text": " Think what I might do is we might take a bit of a early break because we've got a lot to cover and I want", "tokens": [6557, 437, 286, 1062, 360, 307, 321, 1062, 747, 257, 857, 295, 257, 2440, 1821, 570, 321, 600, 658, 257, 688, 281, 2060, 293, 286, 528], "temperature": 0.0, "avg_logprob": -0.15186044234263746, "compression_ratio": 1.5056818181818181, "no_speech_prob": 7.071754680509912e-06}, {"id": 748, "seek": 356708, "start": 3576.68, "end": 3579.68, "text": " To do it all in one big go, so let's take a", "tokens": [1407, 360, 309, 439, 294, 472, 955, 352, 11, 370, 718, 311, 747, 257], "temperature": 0.0, "avg_logprob": -0.15186044234263746, "compression_ratio": 1.5056818181818181, "no_speech_prob": 7.071754680509912e-06}, {"id": 749, "seek": 356708, "start": 3581.48, "end": 3583.48, "text": " Let's take a break until", "tokens": [961, 311, 747, 257, 1821, 1826], "temperature": 0.0, "avg_logprob": -0.15186044234263746, "compression_ratio": 1.5056818181818181, "no_speech_prob": 7.071754680509912e-06}, {"id": 750, "seek": 356708, "start": 3584.44, "end": 3589.0, "text": " 745 and then we're going to come back and talk about recurrent neural networks", "tokens": [1614, 8465, 293, 550, 321, 434, 516, 281, 808, 646, 293, 751, 466, 18680, 1753, 18161, 9590], "temperature": 0.0, "avg_logprob": -0.15186044234263746, "compression_ratio": 1.5056818181818181, "no_speech_prob": 7.071754680509912e-06}, {"id": 751, "seek": 358900, "start": 3589.0, "end": 3594.16, "text": " all right", "tokens": [439, 558], "temperature": 0.0, "avg_logprob": -0.22171356803492495, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.954901553195668e-06}, {"id": 752, "seek": 358900, "start": 3597.16, "end": 3603.76, "text": " So we're going to talk about RNN's before we do we've got to kind of dig a little bit deeper into SGD", "tokens": [407, 321, 434, 516, 281, 751, 466, 45702, 45, 311, 949, 321, 360, 321, 600, 658, 281, 733, 295, 2528, 257, 707, 857, 7731, 666, 34520, 35], "temperature": 0.0, "avg_logprob": -0.22171356803492495, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.954901553195668e-06}, {"id": 753, "seek": 358900, "start": 3605.24, "end": 3609.68, "text": " Because I just want to make sure everybody's totally comfortable with with SGD and", "tokens": [1436, 286, 445, 528, 281, 652, 988, 2201, 311, 3879, 4619, 365, 365, 34520, 35, 293], "temperature": 0.0, "avg_logprob": -0.22171356803492495, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.954901553195668e-06}, {"id": 754, "seek": 358900, "start": 3611.04, "end": 3616.32, "text": " So what we're going to look at is we're going to look at a lesson 6 SGD notebook", "tokens": [407, 437, 321, 434, 516, 281, 574, 412, 307, 321, 434, 516, 281, 574, 412, 257, 6898, 1386, 34520, 35, 21060], "temperature": 0.0, "avg_logprob": -0.22171356803492495, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.954901553195668e-06}, {"id": 755, "seek": 361632, "start": 3616.32, "end": 3618.2400000000002, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.2306503692230621, "compression_ratio": 1.6838709677419355, "no_speech_prob": 6.681501645289245e-07}, {"id": 756, "seek": 361632, "start": 3618.2400000000002, "end": 3620.84, "text": " we're going to look at a really simple example of", "tokens": [321, 434, 516, 281, 574, 412, 257, 534, 2199, 1365, 295], "temperature": 0.0, "avg_logprob": -0.2306503692230621, "compression_ratio": 1.6838709677419355, "no_speech_prob": 6.681501645289245e-07}, {"id": 757, "seek": 361632, "start": 3623.1600000000003, "end": 3625.1600000000003, "text": " Using SGD to learn", "tokens": [11142, 34520, 35, 281, 1466], "temperature": 0.0, "avg_logprob": -0.2306503692230621, "compression_ratio": 1.6838709677419355, "no_speech_prob": 6.681501645289245e-07}, {"id": 758, "seek": 361632, "start": 3626.0, "end": 3628.0, "text": " y equals a x plus b and", "tokens": [288, 6915, 257, 2031, 1804, 272, 293], "temperature": 0.0, "avg_logprob": -0.2306503692230621, "compression_ratio": 1.6838709677419355, "no_speech_prob": 6.681501645289245e-07}, {"id": 759, "seek": 361632, "start": 3628.7200000000003, "end": 3631.48, "text": " So what we're going to do here is we're going to create like", "tokens": [407, 437, 321, 434, 516, 281, 360, 510, 307, 321, 434, 516, 281, 1884, 411], "temperature": 0.0, "avg_logprob": -0.2306503692230621, "compression_ratio": 1.6838709677419355, "no_speech_prob": 6.681501645289245e-07}, {"id": 760, "seek": 361632, "start": 3632.7200000000003, "end": 3634.7200000000003, "text": " the simplest possible model", "tokens": [264, 22811, 1944, 2316], "temperature": 0.0, "avg_logprob": -0.2306503692230621, "compression_ratio": 1.6838709677419355, "no_speech_prob": 6.681501645289245e-07}, {"id": 761, "seek": 361632, "start": 3635.8, "end": 3641.36, "text": " Y equals a x plus b okay, and then we're going to generate some random data", "tokens": [398, 6915, 257, 2031, 1804, 272, 1392, 11, 293, 550, 321, 434, 516, 281, 8460, 512, 4974, 1412], "temperature": 0.0, "avg_logprob": -0.2306503692230621, "compression_ratio": 1.6838709677419355, "no_speech_prob": 6.681501645289245e-07}, {"id": 762, "seek": 364136, "start": 3641.36, "end": 3648.44, "text": " That looks like so so here's our X and here's our Y women to predict Y from X and", "tokens": [663, 1542, 411, 370, 370, 510, 311, 527, 1783, 293, 510, 311, 527, 398, 2266, 281, 6069, 398, 490, 1783, 293], "temperature": 0.0, "avg_logprob": -0.2184465142745006, "compression_ratio": 1.5806451612903225, "no_speech_prob": 2.9944062589493115e-06}, {"id": 763, "seek": 364136, "start": 3652.28, "end": 3657.1600000000003, "text": " We passed in 3 and 8 as our a and b so we're going to kind of try and recover that", "tokens": [492, 4678, 294, 805, 293, 1649, 382, 527, 257, 293, 272, 370, 321, 434, 516, 281, 733, 295, 853, 293, 8114, 300], "temperature": 0.0, "avg_logprob": -0.2184465142745006, "compression_ratio": 1.5806451612903225, "no_speech_prob": 2.9944062589493115e-06}, {"id": 764, "seek": 364136, "start": 3657.52, "end": 3663.28, "text": " Right and so the idea is that if we can solve something like this which has two parameters", "tokens": [1779, 293, 370, 264, 1558, 307, 300, 498, 321, 393, 5039, 746, 411, 341, 597, 575, 732, 9834], "temperature": 0.0, "avg_logprob": -0.2184465142745006, "compression_ratio": 1.5806451612903225, "no_speech_prob": 2.9944062589493115e-06}, {"id": 765, "seek": 364136, "start": 3664.04, "end": 3666.04, "text": " We can use the same technique to solve", "tokens": [492, 393, 764, 264, 912, 6532, 281, 5039], "temperature": 0.0, "avg_logprob": -0.2184465142745006, "compression_ratio": 1.5806451612903225, "no_speech_prob": 2.9944062589493115e-06}, {"id": 766, "seek": 366604, "start": 3666.04, "end": 3675.4, "text": " We can use the same technique to solve something with 100 million parameters right without any changes at all", "tokens": [492, 393, 764, 264, 912, 6532, 281, 5039, 746, 365, 2319, 2459, 9834, 558, 1553, 604, 2962, 412, 439], "temperature": 0.0, "avg_logprob": -0.16649374549771534, "compression_ratio": 1.617117117117117, "no_speech_prob": 8.446199331046955e-07}, {"id": 767, "seek": 366604, "start": 3677.96, "end": 3679.96, "text": " So in order to", "tokens": [407, 294, 1668, 281], "temperature": 0.0, "avg_logprob": -0.16649374549771534, "compression_ratio": 1.617117117117117, "no_speech_prob": 8.446199331046955e-07}, {"id": 768, "seek": 366604, "start": 3680.84, "end": 3684.0, "text": " Find a and a b that fits this we need a loss function", "tokens": [11809, 257, 293, 257, 272, 300, 9001, 341, 321, 643, 257, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.16649374549771534, "compression_ratio": 1.617117117117117, "no_speech_prob": 8.446199331046955e-07}, {"id": 769, "seek": 366604, "start": 3684.36, "end": 3688.24, "text": " Okay, and this is a regression problem because we have a continuous output", "tokens": [1033, 11, 293, 341, 307, 257, 24590, 1154, 570, 321, 362, 257, 10957, 5598], "temperature": 0.0, "avg_logprob": -0.16649374549771534, "compression_ratio": 1.617117117117117, "no_speech_prob": 8.446199331046955e-07}, {"id": 770, "seek": 366604, "start": 3688.84, "end": 3694.2799999999997, "text": " So for continuous output regression we tend to use means grid error right and obviously all of this stuff", "tokens": [407, 337, 10957, 5598, 24590, 321, 3928, 281, 764, 1355, 10748, 6713, 558, 293, 2745, 439, 295, 341, 1507], "temperature": 0.0, "avg_logprob": -0.16649374549771534, "compression_ratio": 1.617117117117117, "no_speech_prob": 8.446199331046955e-07}, {"id": 771, "seek": 369428, "start": 3694.28, "end": 3700.5600000000004, "text": " There's there's implementations in numpy. There's implementations in pytorch. We're just doing stuff by hand so you can see all the steps right", "tokens": [821, 311, 456, 311, 4445, 763, 294, 1031, 8200, 13, 821, 311, 4445, 763, 294, 25878, 284, 339, 13, 492, 434, 445, 884, 1507, 538, 1011, 370, 291, 393, 536, 439, 264, 4439, 558], "temperature": 0.0, "avg_logprob": -0.25929848233559977, "compression_ratio": 1.9051383399209487, "no_speech_prob": 6.9620987233065534e-06}, {"id": 772, "seek": 369428, "start": 3700.6800000000003, "end": 3707.46, "text": " So there's MSC okay y hat is what we often call our predictions y hat minus y squared mean", "tokens": [407, 456, 311, 7395, 34, 1392, 288, 2385, 307, 437, 321, 2049, 818, 527, 21264, 288, 2385, 3175, 288, 8889, 914], "temperature": 0.0, "avg_logprob": -0.25929848233559977, "compression_ratio": 1.9051383399209487, "no_speech_prob": 6.9620987233065534e-06}, {"id": 773, "seek": 369428, "start": 3707.46, "end": 3709.28, "text": " There's our mean square error okay?", "tokens": [821, 311, 527, 914, 3732, 6713, 1392, 30], "temperature": 0.0, "avg_logprob": -0.25929848233559977, "compression_ratio": 1.9051383399209487, "no_speech_prob": 6.9620987233065534e-06}, {"id": 774, "seek": 369428, "start": 3709.28, "end": 3715.52, "text": " So for example if we had 10 and 5 were a and b then there's our mean square error squared error", "tokens": [407, 337, 1365, 498, 321, 632, 1266, 293, 1025, 645, 257, 293, 272, 550, 456, 311, 527, 914, 3732, 6713, 8889, 6713], "temperature": 0.0, "avg_logprob": -0.25929848233559977, "compression_ratio": 1.9051383399209487, "no_speech_prob": 6.9620987233065534e-06}, {"id": 775, "seek": 369428, "start": 3716.2400000000002, "end": 3717.48, "text": " 3.25", "tokens": [805, 13, 6074], "temperature": 0.0, "avg_logprob": -0.25929848233559977, "compression_ratio": 1.9051383399209487, "no_speech_prob": 6.9620987233065534e-06}, {"id": 776, "seek": 369428, "start": 3717.48, "end": 3722.6400000000003, "text": " Okay, so if we've got an a and a b and we've got an X and a y then our mean square error loss is just the mean", "tokens": [1033, 11, 370, 498, 321, 600, 658, 364, 257, 293, 257, 272, 293, 321, 600, 658, 364, 1783, 293, 257, 288, 550, 527, 914, 3732, 6713, 4470, 307, 445, 264, 914], "temperature": 0.0, "avg_logprob": -0.25929848233559977, "compression_ratio": 1.9051383399209487, "no_speech_prob": 6.9620987233065534e-06}, {"id": 777, "seek": 372264, "start": 3722.64, "end": 3724.64, "text": " Square error of our linear", "tokens": [16463, 6713, 295, 527, 8213], "temperature": 0.0, "avg_logprob": -0.22780759406812262, "compression_ratio": 1.712962962962963, "no_speech_prob": 1.553494712425163e-06}, {"id": 778, "seek": 372264, "start": 3725.24, "end": 3729.64, "text": " That's our predictions and our way okay, so there's a loss for 10 5 x y", "tokens": [663, 311, 527, 21264, 293, 527, 636, 1392, 11, 370, 456, 311, 257, 4470, 337, 1266, 1025, 2031, 288], "temperature": 0.0, "avg_logprob": -0.22780759406812262, "compression_ratio": 1.712962962962963, "no_speech_prob": 1.553494712425163e-06}, {"id": 779, "seek": 372264, "start": 3730.7999999999997, "end": 3735.08, "text": " Alright, so that's a loss function right and so when we", "tokens": [2798, 11, 370, 300, 311, 257, 4470, 2445, 558, 293, 370, 562, 321], "temperature": 0.0, "avg_logprob": -0.22780759406812262, "compression_ratio": 1.712962962962963, "no_speech_prob": 1.553494712425163e-06}, {"id": 780, "seek": 372264, "start": 3736.08, "end": 3738.08, "text": " talk about", "tokens": [751, 466], "temperature": 0.0, "avg_logprob": -0.22780759406812262, "compression_ratio": 1.712962962962963, "no_speech_prob": 1.553494712425163e-06}, {"id": 781, "seek": 372264, "start": 3738.12, "end": 3739.2799999999997, "text": " combining", "tokens": [21928], "temperature": 0.0, "avg_logprob": -0.22780759406812262, "compression_ratio": 1.712962962962963, "no_speech_prob": 1.553494712425163e-06}, {"id": 782, "seek": 372264, "start": 3739.2799999999997, "end": 3743.7599999999998, "text": " Linear layers and loss functions and optionally nonlinear layers", "tokens": [14670, 289, 7914, 293, 4470, 6828, 293, 3614, 379, 2107, 28263, 7914], "temperature": 0.0, "avg_logprob": -0.22780759406812262, "compression_ratio": 1.712962962962963, "no_speech_prob": 1.553494712425163e-06}, {"id": 783, "seek": 372264, "start": 3743.7599999999998, "end": 3748.4, "text": " This is all we're doing right is we're putting a function inside a function", "tokens": [639, 307, 439, 321, 434, 884, 558, 307, 321, 434, 3372, 257, 2445, 1854, 257, 2445], "temperature": 0.0, "avg_logprob": -0.22780759406812262, "compression_ratio": 1.712962962962963, "no_speech_prob": 1.553494712425163e-06}, {"id": 784, "seek": 372264, "start": 3748.8799999999997, "end": 3751.44, "text": " Okay, that's that's all like I know people draw these", "tokens": [1033, 11, 300, 311, 300, 311, 439, 411, 286, 458, 561, 2642, 613], "temperature": 0.0, "avg_logprob": -0.22780759406812262, "compression_ratio": 1.712962962962963, "no_speech_prob": 1.553494712425163e-06}, {"id": 785, "seek": 375144, "start": 3751.44, "end": 3753.44, "text": " clever looking", "tokens": [13494, 1237], "temperature": 0.0, "avg_logprob": -0.1721937908389704, "compression_ratio": 1.8464419475655431, "no_speech_prob": 2.7264647997071734e-06}, {"id": 786, "seek": 375144, "start": 3753.84, "end": 3757.52, "text": " Dots and lines all over the screen when they're saying this is what a neural network is", "tokens": [413, 1971, 293, 3876, 439, 670, 264, 2568, 562, 436, 434, 1566, 341, 307, 437, 257, 18161, 3209, 307], "temperature": 0.0, "avg_logprob": -0.1721937908389704, "compression_ratio": 1.8464419475655431, "no_speech_prob": 2.7264647997071734e-06}, {"id": 787, "seek": 375144, "start": 3757.52, "end": 3760.52, "text": " But it's just a it's just a function of a function of a function okay", "tokens": [583, 309, 311, 445, 257, 309, 311, 445, 257, 2445, 295, 257, 2445, 295, 257, 2445, 1392], "temperature": 0.0, "avg_logprob": -0.1721937908389704, "compression_ratio": 1.8464419475655431, "no_speech_prob": 2.7264647997071734e-06}, {"id": 788, "seek": 375144, "start": 3760.52, "end": 3765.68, "text": " So here we've got a prediction function being a linear layer followed by a loss function being MSE", "tokens": [407, 510, 321, 600, 658, 257, 17630, 2445, 885, 257, 8213, 4583, 6263, 538, 257, 4470, 2445, 885, 376, 5879], "temperature": 0.0, "avg_logprob": -0.1721937908389704, "compression_ratio": 1.8464419475655431, "no_speech_prob": 2.7264647997071734e-06}, {"id": 789, "seek": 375144, "start": 3765.68, "end": 3767.4, "text": " And now we can say like oh well", "tokens": [400, 586, 321, 393, 584, 411, 1954, 731], "temperature": 0.0, "avg_logprob": -0.1721937908389704, "compression_ratio": 1.8464419475655431, "no_speech_prob": 2.7264647997071734e-06}, {"id": 790, "seek": 375144, "start": 3767.4, "end": 3774.88, "text": " Let's just define this is MSE loss and we'll use that in the future okay, so there's our loss function which incorporates our prediction function", "tokens": [961, 311, 445, 6964, 341, 307, 376, 5879, 4470, 293, 321, 603, 764, 300, 294, 264, 2027, 1392, 11, 370, 456, 311, 527, 4470, 2445, 597, 50193, 527, 17630, 2445], "temperature": 0.0, "avg_logprob": -0.1721937908389704, "compression_ratio": 1.8464419475655431, "no_speech_prob": 2.7264647997071734e-06}, {"id": 791, "seek": 375144, "start": 3776.28, "end": 3779.12, "text": " So let's generate 10,000 items of fake data", "tokens": [407, 718, 311, 8460, 1266, 11, 1360, 4754, 295, 7592, 1412], "temperature": 0.0, "avg_logprob": -0.1721937908389704, "compression_ratio": 1.8464419475655431, "no_speech_prob": 2.7264647997071734e-06}, {"id": 792, "seek": 377912, "start": 3779.12, "end": 3785.4, "text": " And let's turn them into variables so we can use them with pytorch because Jeremy doesn't like taking derivatives", "tokens": [400, 718, 311, 1261, 552, 666, 9102, 370, 321, 393, 764, 552, 365, 25878, 284, 339, 570, 17809, 1177, 380, 411, 1940, 33733], "temperature": 0.0, "avg_logprob": -0.19331890240050198, "compression_ratio": 1.6436781609195403, "no_speech_prob": 8.18634418919828e-07}, {"id": 793, "seek": 377912, "start": 3785.4, "end": 3787.4, "text": " So we're going to use pytorch for that", "tokens": [407, 321, 434, 516, 281, 764, 25878, 284, 339, 337, 300], "temperature": 0.0, "avg_logprob": -0.19331890240050198, "compression_ratio": 1.6436781609195403, "no_speech_prob": 8.18634418919828e-07}, {"id": 794, "seek": 377912, "start": 3787.64, "end": 3792.72, "text": " And let's create a random weight for a and for B. So a single random number", "tokens": [400, 718, 311, 1884, 257, 4974, 3364, 337, 257, 293, 337, 363, 13, 407, 257, 2167, 4974, 1230], "temperature": 0.0, "avg_logprob": -0.19331890240050198, "compression_ratio": 1.6436781609195403, "no_speech_prob": 8.18634418919828e-07}, {"id": 795, "seek": 377912, "start": 3793.56, "end": 3797.08, "text": " And we want the gradients of these to be calculated as we start", "tokens": [400, 321, 528, 264, 2771, 2448, 295, 613, 281, 312, 15598, 382, 321, 722], "temperature": 0.0, "avg_logprob": -0.19331890240050198, "compression_ratio": 1.6436781609195403, "no_speech_prob": 8.18634418919828e-07}, {"id": 796, "seek": 377912, "start": 3797.7599999999998, "end": 3804.7999999999997, "text": " Computing with them because these are the actual things we need to update in our SGD okay, so here's our a and B", "tokens": [37804, 278, 365, 552, 570, 613, 366, 264, 3539, 721, 321, 643, 281, 5623, 294, 527, 34520, 35, 1392, 11, 370, 510, 311, 527, 257, 293, 363], "temperature": 0.0, "avg_logprob": -0.19331890240050198, "compression_ratio": 1.6436781609195403, "no_speech_prob": 8.18634418919828e-07}, {"id": 797, "seek": 380480, "start": 3804.8, "end": 3808.32, "text": " 0.029 0.111", "tokens": [1958, 13, 15, 11871, 1958, 13, 5348, 16], "temperature": 0.0, "avg_logprob": -0.23102855682373047, "compression_ratio": 1.5980861244019138, "no_speech_prob": 2.2603123852604767e-06}, {"id": 798, "seek": 380480, "start": 3809.8, "end": 3816.6200000000003, "text": " All right, so let's pick a learning rate okay, and then let's do 10,000 epochs of", "tokens": [1057, 558, 11, 370, 718, 311, 1888, 257, 2539, 3314, 1392, 11, 293, 550, 718, 311, 360, 1266, 11, 1360, 30992, 28346, 295], "temperature": 0.0, "avg_logprob": -0.23102855682373047, "compression_ratio": 1.5980861244019138, "no_speech_prob": 2.2603123852604767e-06}, {"id": 799, "seek": 380480, "start": 3818.44, "end": 3825.2400000000002, "text": " SGD in fact this isn't really SGD. It's not stochastic gradient descent. This is actually full gradient descent. We're going to each", "tokens": [34520, 35, 294, 1186, 341, 1943, 380, 534, 34520, 35, 13, 467, 311, 406, 342, 8997, 2750, 16235, 23475, 13, 639, 307, 767, 1577, 16235, 23475, 13, 492, 434, 516, 281, 1184], "temperature": 0.0, "avg_logprob": -0.23102855682373047, "compression_ratio": 1.5980861244019138, "no_speech_prob": 2.2603123852604767e-06}, {"id": 800, "seek": 380480, "start": 3826.04, "end": 3828.88, "text": " Each loop is going to look at all of the data", "tokens": [6947, 6367, 307, 516, 281, 574, 412, 439, 295, 264, 1412], "temperature": 0.0, "avg_logprob": -0.23102855682373047, "compression_ratio": 1.5980861244019138, "no_speech_prob": 2.2603123852604767e-06}, {"id": 801, "seek": 380480, "start": 3829.6000000000004, "end": 3831.52, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.23102855682373047, "compression_ratio": 1.5980861244019138, "no_speech_prob": 2.2603123852604767e-06}, {"id": 802, "seek": 380480, "start": 3831.52, "end": 3834.1200000000003, "text": " Stochastic gradient descent would be looking at a subset", "tokens": [745, 8997, 2750, 16235, 23475, 576, 312, 1237, 412, 257, 25993], "temperature": 0.0, "avg_logprob": -0.23102855682373047, "compression_ratio": 1.5980861244019138, "no_speech_prob": 2.2603123852604767e-06}, {"id": 803, "seek": 383412, "start": 3834.12, "end": 3836.12, "text": " each time", "tokens": [1184, 565], "temperature": 0.0, "avg_logprob": -0.16570981999033504, "compression_ratio": 1.6782608695652175, "no_speech_prob": 1.903382440104906e-06}, {"id": 804, "seek": 383412, "start": 3836.12, "end": 3842.72, "text": " So to do gradient descent we basically calculate the loss right so remember we've started out with a random a and B", "tokens": [407, 281, 360, 16235, 23475, 321, 1936, 8873, 264, 4470, 558, 370, 1604, 321, 600, 1409, 484, 365, 257, 4974, 257, 293, 363], "temperature": 0.0, "avg_logprob": -0.16570981999033504, "compression_ratio": 1.6782608695652175, "no_speech_prob": 1.903382440104906e-06}, {"id": 805, "seek": 383412, "start": 3843.2799999999997, "end": 3848.68, "text": " Okay, and so this is going to compute some amount of loss, and then it's nice from time to time", "tokens": [1033, 11, 293, 370, 341, 307, 516, 281, 14722, 512, 2372, 295, 4470, 11, 293, 550, 309, 311, 1481, 490, 565, 281, 565], "temperature": 0.0, "avg_logprob": -0.16570981999033504, "compression_ratio": 1.6782608695652175, "no_speech_prob": 1.903382440104906e-06}, {"id": 806, "seek": 383412, "start": 3848.96, "end": 3854.56, "text": " So one way of saying from time to time is if the epoch number mod a thousand is zero", "tokens": [407, 472, 636, 295, 1566, 490, 565, 281, 565, 307, 498, 264, 30992, 339, 1230, 1072, 257, 4714, 307, 4018], "temperature": 0.0, "avg_logprob": -0.16570981999033504, "compression_ratio": 1.6782608695652175, "no_speech_prob": 1.903382440104906e-06}, {"id": 807, "seek": 383412, "start": 3855.04, "end": 3859.4, "text": " Right so every thousand epochs just print out the loss see how we're doing okay", "tokens": [1779, 370, 633, 4714, 30992, 28346, 445, 4482, 484, 264, 4470, 536, 577, 321, 434, 884, 1392], "temperature": 0.0, "avg_logprob": -0.16570981999033504, "compression_ratio": 1.6782608695652175, "no_speech_prob": 1.903382440104906e-06}, {"id": 808, "seek": 385940, "start": 3859.4, "end": 3868.52, "text": " So now that we've computed the loss we can compute our gradients right and so you just remember this thing here is", "tokens": [407, 586, 300, 321, 600, 40610, 264, 4470, 321, 393, 14722, 527, 2771, 2448, 558, 293, 370, 291, 445, 1604, 341, 551, 510, 307], "temperature": 0.0, "avg_logprob": -0.15397552914089627, "compression_ratio": 1.7196652719665273, "no_speech_prob": 4.4508462337944366e-07}, {"id": 809, "seek": 385940, "start": 3869.32, "end": 3873.4, "text": " Both a number a single number that is our loss something we can print", "tokens": [6767, 257, 1230, 257, 2167, 1230, 300, 307, 527, 4470, 746, 321, 393, 4482], "temperature": 0.0, "avg_logprob": -0.15397552914089627, "compression_ratio": 1.7196652719665273, "no_speech_prob": 4.4508462337944366e-07}, {"id": 810, "seek": 385940, "start": 3873.4, "end": 3880.08, "text": " But it's also a variable because we passed variables into it and therefore it also has a method dot backward", "tokens": [583, 309, 311, 611, 257, 7006, 570, 321, 4678, 9102, 666, 309, 293, 4412, 309, 611, 575, 257, 3170, 5893, 23897], "temperature": 0.0, "avg_logprob": -0.15397552914089627, "compression_ratio": 1.7196652719665273, "no_speech_prob": 4.4508462337944366e-07}, {"id": 811, "seek": 385940, "start": 3880.32, "end": 3887.0, "text": " Which means calculate the gradients of everything that we asked it to everything where we said requires radicals true", "tokens": [3013, 1355, 8873, 264, 2771, 2448, 295, 1203, 300, 321, 2351, 309, 281, 1203, 689, 321, 848, 7029, 12001, 82, 2074], "temperature": 0.0, "avg_logprob": -0.15397552914089627, "compression_ratio": 1.7196652719665273, "no_speech_prob": 4.4508462337944366e-07}, {"id": 812, "seek": 388700, "start": 3887.0, "end": 3890.76, "text": " Okay, so at this point we now have a", "tokens": [1033, 11, 370, 412, 341, 935, 321, 586, 362, 257], "temperature": 0.0, "avg_logprob": -0.24227590994401413, "compression_ratio": 1.678391959798995, "no_speech_prob": 2.9189376959948277e-07}, {"id": 813, "seek": 388700, "start": 3892.08, "end": 3896.52, "text": " dot grad property inside a and inside B and", "tokens": [5893, 2771, 4707, 1854, 257, 293, 1854, 363, 293], "temperature": 0.0, "avg_logprob": -0.24227590994401413, "compression_ratio": 1.678391959798995, "no_speech_prob": 2.9189376959948277e-07}, {"id": 814, "seek": 388700, "start": 3897.12, "end": 3900.62, "text": " Here they are here is that dot grad property", "tokens": [1692, 436, 366, 510, 307, 300, 5893, 2771, 4707], "temperature": 0.0, "avg_logprob": -0.24227590994401413, "compression_ratio": 1.678391959798995, "no_speech_prob": 2.9189376959948277e-07}, {"id": 815, "seek": 388700, "start": 3900.62, "end": 3904.06, "text": " Okay, so now that we've calculated the gradients for a and B", "tokens": [1033, 11, 370, 586, 300, 321, 600, 15598, 264, 2771, 2448, 337, 257, 293, 363], "temperature": 0.0, "avg_logprob": -0.24227590994401413, "compression_ratio": 1.678391959798995, "no_speech_prob": 2.9189376959948277e-07}, {"id": 816, "seek": 388700, "start": 3904.08, "end": 3911.52, "text": " We can update them by saying a is equal to whatever it used to be minus the learning rate times the gradient", "tokens": [492, 393, 5623, 552, 538, 1566, 257, 307, 2681, 281, 2035, 309, 1143, 281, 312, 3175, 264, 2539, 3314, 1413, 264, 16235], "temperature": 0.0, "avg_logprob": -0.24227590994401413, "compression_ratio": 1.678391959798995, "no_speech_prob": 2.9189376959948277e-07}, {"id": 817, "seek": 388700, "start": 3912.64, "end": 3913.76, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.24227590994401413, "compression_ratio": 1.678391959798995, "no_speech_prob": 2.9189376959948277e-07}, {"id": 818, "seek": 391376, "start": 3913.76, "end": 3922.0800000000004, "text": " dot data because a is a variable and a variable contains a tensor in its dot data property and", "tokens": [5893, 1412, 570, 257, 307, 257, 7006, 293, 257, 7006, 8306, 257, 40863, 294, 1080, 5893, 1412, 4707, 293], "temperature": 0.0, "avg_logprob": -0.19718763330480554, "compression_ratio": 1.7465437788018434, "no_speech_prob": 1.2289162896195194e-06}, {"id": 819, "seek": 391376, "start": 3922.6000000000004, "end": 3927.0400000000004, "text": " We again this is going to disappear in pytorch point four but for now", "tokens": [492, 797, 341, 307, 516, 281, 11596, 294, 25878, 284, 339, 935, 1451, 457, 337, 586], "temperature": 0.0, "avg_logprob": -0.19718763330480554, "compression_ratio": 1.7465437788018434, "no_speech_prob": 1.2289162896195194e-06}, {"id": 820, "seek": 391376, "start": 3927.1200000000003, "end": 3933.5600000000004, "text": " It's actually the tensor that we need to update okay, so update the tensor inside here with whatever it used to be", "tokens": [467, 311, 767, 264, 40863, 300, 321, 643, 281, 5623, 1392, 11, 370, 5623, 264, 40863, 1854, 510, 365, 2035, 309, 1143, 281, 312], "temperature": 0.0, "avg_logprob": -0.19718763330480554, "compression_ratio": 1.7465437788018434, "no_speech_prob": 1.2289162896195194e-06}, {"id": 821, "seek": 391376, "start": 3934.2000000000003, "end": 3936.2000000000003, "text": " Minus the learning rate times the gradient", "tokens": [2829, 301, 264, 2539, 3314, 1413, 264, 16235], "temperature": 0.0, "avg_logprob": -0.19718763330480554, "compression_ratio": 1.7465437788018434, "no_speech_prob": 1.2289162896195194e-06}, {"id": 822, "seek": 391376, "start": 3937.5200000000004, "end": 3942.1200000000003, "text": " Okay, and that's basically it right that's basically all", "tokens": [1033, 11, 293, 300, 311, 1936, 309, 558, 300, 311, 1936, 439], "temperature": 0.0, "avg_logprob": -0.19718763330480554, "compression_ratio": 1.7465437788018434, "no_speech_prob": 1.2289162896195194e-06}, {"id": 823, "seek": 394212, "start": 3942.12, "end": 3947.0, "text": " Gradient descent is okay, so it's it's as simple as we claimed", "tokens": [16710, 1196, 23475, 307, 1392, 11, 370, 309, 311, 309, 311, 382, 2199, 382, 321, 12941], "temperature": 0.0, "avg_logprob": -0.1707105547468239, "compression_ratio": 1.704724409448819, "no_speech_prob": 1.8162188553105807e-06}, {"id": 824, "seek": 394212, "start": 3947.4, "end": 3949.98, "text": " There's one extra step in pytorch", "tokens": [821, 311, 472, 2857, 1823, 294, 25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.1707105547468239, "compression_ratio": 1.704724409448819, "no_speech_prob": 1.8162188553105807e-06}, {"id": 825, "seek": 394212, "start": 3950.2799999999997, "end": 3956.24, "text": " Which is that you might have like multiple different loss functions or like lots of lots of output layers", "tokens": [3013, 307, 300, 291, 1062, 362, 411, 3866, 819, 4470, 6828, 420, 411, 3195, 295, 3195, 295, 5598, 7914], "temperature": 0.0, "avg_logprob": -0.1707105547468239, "compression_ratio": 1.704724409448819, "no_speech_prob": 1.8162188553105807e-06}, {"id": 826, "seek": 394212, "start": 3957.08, "end": 3962.08, "text": " all contributing to the gradient, and you like have to add them all together and so", "tokens": [439, 19270, 281, 264, 16235, 11, 293, 291, 411, 362, 281, 909, 552, 439, 1214, 293, 370], "temperature": 0.0, "avg_logprob": -0.1707105547468239, "compression_ratio": 1.704724409448819, "no_speech_prob": 1.8162188553105807e-06}, {"id": 827, "seek": 394212, "start": 3962.92, "end": 3964.7599999999998, "text": " If you've got multiple loss functions", "tokens": [759, 291, 600, 658, 3866, 4470, 6828], "temperature": 0.0, "avg_logprob": -0.1707105547468239, "compression_ratio": 1.704724409448819, "no_speech_prob": 1.8162188553105807e-06}, {"id": 828, "seek": 394212, "start": 3964.7599999999998, "end": 3970.2799999999997, "text": " You could be calling lost up backward on each of them and what it does is it adds it to the gradients right?", "tokens": [509, 727, 312, 5141, 2731, 493, 23897, 322, 1184, 295, 552, 293, 437, 309, 775, 307, 309, 10860, 309, 281, 264, 2771, 2448, 558, 30], "temperature": 0.0, "avg_logprob": -0.1707105547468239, "compression_ratio": 1.704724409448819, "no_speech_prob": 1.8162188553105807e-06}, {"id": 829, "seek": 397028, "start": 3970.28, "end": 3973.48, "text": " And so you have to tell it when to set the gradients back to zero", "tokens": [400, 370, 291, 362, 281, 980, 309, 562, 281, 992, 264, 2771, 2448, 646, 281, 4018], "temperature": 0.0, "avg_logprob": -0.2075250389870633, "compression_ratio": 1.6703296703296704, "no_speech_prob": 1.7603356354811694e-06}, {"id": 830, "seek": 397028, "start": 3974.1200000000003, "end": 3978.44, "text": " Okay, so that's where you just go okay set a to zero and", "tokens": [1033, 11, 370, 300, 311, 689, 291, 445, 352, 1392, 992, 257, 281, 4018, 293], "temperature": 0.0, "avg_logprob": -0.2075250389870633, "compression_ratio": 1.6703296703296704, "no_speech_prob": 1.7603356354811694e-06}, {"id": 831, "seek": 397028, "start": 3979.32, "end": 3986.6000000000004, "text": " Gradients and set B gradients to zero okay, and so this is wrapped up inside the", "tokens": [16710, 2448, 293, 992, 363, 2771, 2448, 281, 4018, 1392, 11, 293, 370, 341, 307, 14226, 493, 1854, 264], "temperature": 0.0, "avg_logprob": -0.2075250389870633, "compression_ratio": 1.6703296703296704, "no_speech_prob": 1.7603356354811694e-06}, {"id": 832, "seek": 397028, "start": 3987.1600000000003, "end": 3989.1600000000003, "text": " You know optium dot SGD", "tokens": [509, 458, 2427, 2197, 5893, 34520, 35], "temperature": 0.0, "avg_logprob": -0.2075250389870633, "compression_ratio": 1.6703296703296704, "no_speech_prob": 1.7603356354811694e-06}, {"id": 833, "seek": 397028, "start": 3989.8, "end": 3995.6400000000003, "text": " Class right so when we say optium dot SGD, and we just say you know dot step", "tokens": [9471, 558, 370, 562, 321, 584, 2427, 2197, 5893, 34520, 35, 11, 293, 321, 445, 584, 291, 458, 5893, 1823], "temperature": 0.0, "avg_logprob": -0.2075250389870633, "compression_ratio": 1.6703296703296704, "no_speech_prob": 1.7603356354811694e-06}, {"id": 834, "seek": 399564, "start": 3995.64, "end": 4000.8799999999997, "text": " It's just doing these for us so when we say dot zero gradients. It's just doing this for us", "tokens": [467, 311, 445, 884, 613, 337, 505, 370, 562, 321, 584, 5893, 4018, 2771, 2448, 13, 467, 311, 445, 884, 341, 337, 505], "temperature": 0.0, "avg_logprob": -0.14972922676487974, "compression_ratio": 1.7387755102040816, "no_speech_prob": 1.436747083971568e-06}, {"id": 835, "seek": 399564, "start": 4001.4, "end": 4003.4, "text": " And this underscore here", "tokens": [400, 341, 37556, 510], "temperature": 0.0, "avg_logprob": -0.14972922676487974, "compression_ratio": 1.7387755102040816, "no_speech_prob": 1.436747083971568e-06}, {"id": 836, "seek": 399564, "start": 4004.3199999999997, "end": 4005.7599999999998, "text": " every", "tokens": [633], "temperature": 0.0, "avg_logprob": -0.14972922676487974, "compression_ratio": 1.7387755102040816, "no_speech_prob": 1.436747083971568e-06}, {"id": 837, "seek": 399564, "start": 4005.7599999999998, "end": 4009.2, "text": " Pretty much every function that applies to a tensor in pytorch", "tokens": [10693, 709, 633, 2445, 300, 13165, 281, 257, 40863, 294, 25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.14972922676487974, "compression_ratio": 1.7387755102040816, "no_speech_prob": 1.436747083971568e-06}, {"id": 838, "seek": 399564, "start": 4009.6, "end": 4012.68, "text": " If you stick an underscore on the end it means do it in place", "tokens": [759, 291, 2897, 364, 37556, 322, 264, 917, 309, 1355, 360, 309, 294, 1081], "temperature": 0.0, "avg_logprob": -0.14972922676487974, "compression_ratio": 1.7387755102040816, "no_speech_prob": 1.436747083971568e-06}, {"id": 839, "seek": 399564, "start": 4012.92, "end": 4018.6, "text": " Okay, so this is actually going to not return a bunch of zeros, but it's going to change this in place to be a bunch of zeros", "tokens": [1033, 11, 370, 341, 307, 767, 516, 281, 406, 2736, 257, 3840, 295, 35193, 11, 457, 309, 311, 516, 281, 1319, 341, 294, 1081, 281, 312, 257, 3840, 295, 35193], "temperature": 0.0, "avg_logprob": -0.14972922676487974, "compression_ratio": 1.7387755102040816, "no_speech_prob": 1.436747083971568e-06}, {"id": 840, "seek": 399564, "start": 4021.0, "end": 4023.0, "text": " So that's basically it", "tokens": [407, 300, 311, 1936, 309], "temperature": 0.0, "avg_logprob": -0.14972922676487974, "compression_ratio": 1.7387755102040816, "no_speech_prob": 1.436747083971568e-06}, {"id": 841, "seek": 402300, "start": 4023.0, "end": 4026.64, "text": " We can look at the same thing without pytorch", "tokens": [492, 393, 574, 412, 264, 912, 551, 1553, 25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.16521140650699012, "compression_ratio": 1.6622222222222223, "no_speech_prob": 3.1875517834123457e-06}, {"id": 842, "seek": 402300, "start": 4028.92, "end": 4033.92, "text": " Which means we actually do have to do some calculus, so if we generate some fake data again", "tokens": [3013, 1355, 321, 767, 360, 362, 281, 360, 512, 33400, 11, 370, 498, 321, 8460, 512, 7592, 1412, 797], "temperature": 0.0, "avg_logprob": -0.16521140650699012, "compression_ratio": 1.6622222222222223, "no_speech_prob": 3.1875517834123457e-06}, {"id": 843, "seek": 402300, "start": 4035.12, "end": 4039.22, "text": " We're just going to create 50 data points this time just to make this fast and easy to look at", "tokens": [492, 434, 445, 516, 281, 1884, 2625, 1412, 2793, 341, 565, 445, 281, 652, 341, 2370, 293, 1858, 281, 574, 412], "temperature": 0.0, "avg_logprob": -0.16521140650699012, "compression_ratio": 1.6622222222222223, "no_speech_prob": 3.1875517834123457e-06}, {"id": 844, "seek": 402300, "start": 4039.92, "end": 4045.74, "text": " And so let's create a function called update right we're just going to use numpy no pytorch", "tokens": [400, 370, 718, 311, 1884, 257, 2445, 1219, 5623, 558, 321, 434, 445, 516, 281, 764, 1031, 8200, 572, 25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.16521140650699012, "compression_ratio": 1.6622222222222223, "no_speech_prob": 3.1875517834123457e-06}, {"id": 845, "seek": 402300, "start": 4046.24, "end": 4049.22, "text": " Okay, so our predictions is equal to again linear", "tokens": [1033, 11, 370, 527, 21264, 307, 2681, 281, 797, 8213], "temperature": 0.0, "avg_logprob": -0.16521140650699012, "compression_ratio": 1.6622222222222223, "no_speech_prob": 3.1875517834123457e-06}, {"id": 846, "seek": 404922, "start": 4049.22, "end": 4056.14, "text": " And in this case we're actually going to calculate the derivatives so the derivative of the square of the loss is just two times", "tokens": [400, 294, 341, 1389, 321, 434, 767, 516, 281, 8873, 264, 33733, 370, 264, 13760, 295, 264, 3732, 295, 264, 4470, 307, 445, 732, 1413], "temperature": 0.0, "avg_logprob": -0.19379877576641008, "compression_ratio": 2.032710280373832, "no_speech_prob": 5.255365067569073e-06}, {"id": 847, "seek": 404922, "start": 4056.9399999999996, "end": 4063.24, "text": " And then the derivative with respect to a is just that you can confirm that yourself if you want to and so here our", "tokens": [400, 550, 264, 13760, 365, 3104, 281, 257, 307, 445, 300, 291, 393, 9064, 300, 1803, 498, 291, 528, 281, 293, 370, 510, 527], "temperature": 0.0, "avg_logprob": -0.19379877576641008, "compression_ratio": 2.032710280373832, "no_speech_prob": 5.255365067569073e-06}, {"id": 848, "seek": 404922, "start": 4063.54, "end": 4068.18, "text": " we're going to update a minus equals learning rate times the derivative of", "tokens": [321, 434, 516, 281, 5623, 257, 3175, 6915, 2539, 3314, 1413, 264, 13760, 295], "temperature": 0.0, "avg_logprob": -0.19379877576641008, "compression_ratio": 2.032710280373832, "no_speech_prob": 5.255365067569073e-06}, {"id": 849, "seek": 404922, "start": 4068.74, "end": 4071.58, "text": " loss with respect to a and for B it's", "tokens": [4470, 365, 3104, 281, 257, 293, 337, 363, 309, 311], "temperature": 0.0, "avg_logprob": -0.19379877576641008, "compression_ratio": 2.032710280373832, "no_speech_prob": 5.255365067569073e-06}, {"id": 850, "seek": 404922, "start": 4072.3799999999997, "end": 4077.62, "text": " Learning rate times derivative with respect to B. Okay, and so what we can do", "tokens": [15205, 3314, 1413, 13760, 365, 3104, 281, 363, 13, 1033, 11, 293, 370, 437, 321, 393, 360], "temperature": 0.0, "avg_logprob": -0.19379877576641008, "compression_ratio": 2.032710280373832, "no_speech_prob": 5.255365067569073e-06}, {"id": 851, "seek": 407762, "start": 4077.62, "end": 4079.62, "text": " Let's just run all this", "tokens": [961, 311, 445, 1190, 439, 341], "temperature": 0.0, "avg_logprob": -0.17325606244675656, "compression_ratio": 1.6990740740740742, "no_speech_prob": 2.368787363593583e-06}, {"id": 852, "seek": 407762, "start": 4081.22, "end": 4088.7, "text": " So just for fun rather than looping through manually we can use the map plot map plot lib func animation command", "tokens": [407, 445, 337, 1019, 2831, 813, 6367, 278, 807, 16945, 321, 393, 764, 264, 4471, 7542, 4471, 7542, 22854, 1019, 66, 9603, 5622], "temperature": 0.0, "avg_logprob": -0.17325606244675656, "compression_ratio": 1.6990740740740742, "no_speech_prob": 2.368787363593583e-06}, {"id": 853, "seek": 407762, "start": 4089.62, "end": 4091.38, "text": " to run", "tokens": [281, 1190], "temperature": 0.0, "avg_logprob": -0.17325606244675656, "compression_ratio": 1.6990740740740742, "no_speech_prob": 2.368787363593583e-06}, {"id": 854, "seek": 407762, "start": 4091.38, "end": 4096.78, "text": " The animate function a bunch of times and the animate function is going to run 30", "tokens": [440, 36439, 2445, 257, 3840, 295, 1413, 293, 264, 36439, 2445, 307, 516, 281, 1190, 2217], "temperature": 0.0, "avg_logprob": -0.17325606244675656, "compression_ratio": 1.6990740740740742, "no_speech_prob": 2.368787363593583e-06}, {"id": 855, "seek": 407762, "start": 4097.3, "end": 4101.16, "text": " Epochs and at the end of each epoch it's going to print out", "tokens": [462, 2259, 28346, 293, 412, 264, 917, 295, 1184, 30992, 339, 309, 311, 516, 281, 4482, 484], "temperature": 0.0, "avg_logprob": -0.17325606244675656, "compression_ratio": 1.6990740740740742, "no_speech_prob": 2.368787363593583e-06}, {"id": 856, "seek": 410116, "start": 4101.16, "end": 4108.96, "text": " On the plot where the line currently is and that creates this little movie okay, so you can actually see the", "tokens": [1282, 264, 7542, 689, 264, 1622, 4362, 307, 293, 300, 7829, 341, 707, 3169, 1392, 11, 370, 291, 393, 767, 536, 264], "temperature": 0.0, "avg_logprob": -0.20858169463743648, "compression_ratio": 1.5645933014354068, "no_speech_prob": 1.5294091326722992e-06}, {"id": 857, "seek": 410116, "start": 4109.599999999999, "end": 4116.26, "text": " Line moving into place right so if you want to play around with like understanding how", "tokens": [14670, 2684, 666, 1081, 558, 370, 498, 291, 528, 281, 862, 926, 365, 411, 3701, 577], "temperature": 0.0, "avg_logprob": -0.20858169463743648, "compression_ratio": 1.5645933014354068, "no_speech_prob": 1.5294091326722992e-06}, {"id": 858, "seek": 410116, "start": 4117.36, "end": 4119.28, "text": " pytorch gradients", "tokens": [25878, 284, 339, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.20858169463743648, "compression_ratio": 1.5645933014354068, "no_speech_prob": 1.5294091326722992e-06}, {"id": 859, "seek": 410116, "start": 4119.28, "end": 4124.32, "text": " Actually work step-by-step. Here's like the world's simplest little example, okay?", "tokens": [5135, 589, 1823, 12, 2322, 12, 16792, 13, 1692, 311, 411, 264, 1002, 311, 22811, 707, 1365, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.20858169463743648, "compression_ratio": 1.5645933014354068, "no_speech_prob": 1.5294091326722992e-06}, {"id": 860, "seek": 410116, "start": 4125.48, "end": 4127.92, "text": " And you know it's kind of like", "tokens": [400, 291, 458, 309, 311, 733, 295, 411], "temperature": 0.0, "avg_logprob": -0.20858169463743648, "compression_ratio": 1.5645933014354068, "no_speech_prob": 1.5294091326722992e-06}, {"id": 861, "seek": 412792, "start": 4127.92, "end": 4134.9, "text": " It's kind of weird to say like that's that's it like when you're optimizing a hundred million parameters in a neural net", "tokens": [467, 311, 733, 295, 3657, 281, 584, 411, 300, 311, 300, 311, 309, 411, 562, 291, 434, 40425, 257, 3262, 2459, 9834, 294, 257, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.19438922188498758, "compression_ratio": 1.6561264822134387, "no_speech_prob": 2.601606411190005e-06}, {"id": 862, "seek": 412792, "start": 4134.9, "end": 4140.4800000000005, "text": " It's doing the same thing, but it it actually is right you can actually look at the pytorch code and see it", "tokens": [467, 311, 884, 264, 912, 551, 11, 457, 309, 309, 767, 307, 558, 291, 393, 767, 574, 412, 264, 25878, 284, 339, 3089, 293, 536, 309], "temperature": 0.0, "avg_logprob": -0.19438922188498758, "compression_ratio": 1.6561264822134387, "no_speech_prob": 2.601606411190005e-06}, {"id": 863, "seek": 412792, "start": 4140.56, "end": 4142.56, "text": " This is it right. There's no", "tokens": [639, 307, 309, 558, 13, 821, 311, 572], "temperature": 0.0, "avg_logprob": -0.19438922188498758, "compression_ratio": 1.6561264822134387, "no_speech_prob": 2.601606411190005e-06}, {"id": 864, "seek": 412792, "start": 4142.84, "end": 4144.4400000000005, "text": " trick", "tokens": [4282], "temperature": 0.0, "avg_logprob": -0.19438922188498758, "compression_ratio": 1.6561264822134387, "no_speech_prob": 2.601606411190005e-06}, {"id": 865, "seek": 412792, "start": 4144.4400000000005, "end": 4151.32, "text": " We well we learned a couple of minor tricks last time which was like momentum and Adam right, but", "tokens": [492, 731, 321, 3264, 257, 1916, 295, 6696, 11733, 1036, 565, 597, 390, 411, 11244, 293, 7938, 558, 11, 457], "temperature": 0.0, "avg_logprob": -0.19438922188498758, "compression_ratio": 1.6561264822134387, "no_speech_prob": 2.601606411190005e-06}, {"id": 866, "seek": 412792, "start": 4152.24, "end": 4155.84, "text": " If you can do it in Excel you can do it in Python so okay", "tokens": [759, 291, 393, 360, 309, 294, 19060, 291, 393, 360, 309, 294, 15329, 370, 1392], "temperature": 0.0, "avg_logprob": -0.19438922188498758, "compression_ratio": 1.6561264822134387, "no_speech_prob": 2.601606411190005e-06}, {"id": 867, "seek": 415584, "start": 4155.84, "end": 4160.7, "text": " So let's now talk about RNN's so we're now in lesson 6 RNN", "tokens": [407, 718, 311, 586, 751, 466, 45702, 45, 311, 370, 321, 434, 586, 294, 6898, 1386, 45702, 45], "temperature": 0.0, "avg_logprob": -0.28570198645958533, "compression_ratio": 1.4014084507042253, "no_speech_prob": 1.0348489922762383e-06}, {"id": 868, "seek": 415584, "start": 4161.360000000001, "end": 4163.360000000001, "text": " notebook", "tokens": [21060], "temperature": 0.0, "avg_logprob": -0.28570198645958533, "compression_ratio": 1.4014084507042253, "no_speech_prob": 1.0348489922762383e-06}, {"id": 869, "seek": 415584, "start": 4164.2, "end": 4166.2, "text": " And", "tokens": [400], "temperature": 0.0, "avg_logprob": -0.28570198645958533, "compression_ratio": 1.4014084507042253, "no_speech_prob": 1.0348489922762383e-06}, {"id": 870, "seek": 415584, "start": 4167.2, "end": 4172.66, "text": " We're going to study Nietzsche as you should", "tokens": [492, 434, 516, 281, 2979, 36583, 89, 12287, 382, 291, 820], "temperature": 0.0, "avg_logprob": -0.28570198645958533, "compression_ratio": 1.4014084507042253, "no_speech_prob": 1.0348489922762383e-06}, {"id": 871, "seek": 415584, "start": 4175.04, "end": 4177.04, "text": " So Nietzsche says", "tokens": [407, 36583, 89, 12287, 1619], "temperature": 0.0, "avg_logprob": -0.28570198645958533, "compression_ratio": 1.4014084507042253, "no_speech_prob": 1.0348489922762383e-06}, {"id": 872, "seek": 415584, "start": 4177.96, "end": 4180.92, "text": " Supposing that truth is a woman what then I love this", "tokens": [9391, 6110, 300, 3494, 307, 257, 3059, 437, 550, 286, 959, 341], "temperature": 0.0, "avg_logprob": -0.28570198645958533, "compression_ratio": 1.4014084507042253, "no_speech_prob": 1.0348489922762383e-06}, {"id": 873, "seek": 415584, "start": 4182.4800000000005, "end": 4183.72, "text": " apparently", "tokens": [7970], "temperature": 0.0, "avg_logprob": -0.28570198645958533, "compression_ratio": 1.4014084507042253, "no_speech_prob": 1.0348489922762383e-06}, {"id": 874, "seek": 418372, "start": 4183.72, "end": 4189.06, "text": " All philosophers have failed to understand women so apparently at the point that Nietzsche was alive", "tokens": [1057, 36839, 362, 7612, 281, 1223, 2266, 370, 7970, 412, 264, 935, 300, 36583, 89, 12287, 390, 5465], "temperature": 0.0, "avg_logprob": -0.21131362915039062, "compression_ratio": 1.7261410788381744, "no_speech_prob": 1.1544553899511811e-06}, {"id": 875, "seek": 418372, "start": 4189.06, "end": 4195.8, "text": " There was no female philosophers or at least those that were around didn't understand women either so anyway, so this is the philosopher", "tokens": [821, 390, 572, 6556, 36839, 420, 412, 1935, 729, 300, 645, 926, 994, 380, 1223, 2266, 2139, 370, 4033, 11, 370, 341, 307, 264, 29805], "temperature": 0.0, "avg_logprob": -0.21131362915039062, "compression_ratio": 1.7261410788381744, "no_speech_prob": 1.1544553899511811e-06}, {"id": 876, "seek": 418372, "start": 4196.6, "end": 4198.8, "text": " Apparently we've chosen to study", "tokens": [16755, 321, 600, 8614, 281, 2979], "temperature": 0.0, "avg_logprob": -0.21131362915039062, "compression_ratio": 1.7261410788381744, "no_speech_prob": 1.1544553899511811e-06}, {"id": 877, "seek": 418372, "start": 4200.04, "end": 4202.860000000001, "text": " This is actually much less worse than people think he is", "tokens": [639, 307, 767, 709, 1570, 5324, 813, 561, 519, 415, 307], "temperature": 0.0, "avg_logprob": -0.21131362915039062, "compression_ratio": 1.7261410788381744, "no_speech_prob": 1.1544553899511811e-06}, {"id": 878, "seek": 418372, "start": 4204.12, "end": 4208.88, "text": " But it's a different era. I guess all right, so we're going to learn to write", "tokens": [583, 309, 311, 257, 819, 4249, 13, 286, 2041, 439, 558, 11, 370, 321, 434, 516, 281, 1466, 281, 2464], "temperature": 0.0, "avg_logprob": -0.21131362915039062, "compression_ratio": 1.7261410788381744, "no_speech_prob": 1.1544553899511811e-06}, {"id": 879, "seek": 418372, "start": 4210.400000000001, "end": 4211.740000000001, "text": " philosophy", "tokens": [10675], "temperature": 0.0, "avg_logprob": -0.21131362915039062, "compression_ratio": 1.7261410788381744, "no_speech_prob": 1.1544553899511811e-06}, {"id": 880, "seek": 421174, "start": 4211.74, "end": 4213.74, "text": " Like Nietzsche", "tokens": [1743, 36583, 89, 12287], "temperature": 0.0, "avg_logprob": -0.1720214323564009, "compression_ratio": 1.738938053097345, "no_speech_prob": 5.043464739173942e-07}, {"id": 881, "seek": 421174, "start": 4213.76, "end": 4217.2, "text": " And so we're going to do it one character at a time", "tokens": [400, 370, 321, 434, 516, 281, 360, 309, 472, 2517, 412, 257, 565], "temperature": 0.0, "avg_logprob": -0.1720214323564009, "compression_ratio": 1.738938053097345, "no_speech_prob": 5.043464739173942e-07}, {"id": 882, "seek": 421174, "start": 4217.2, "end": 4221.5599999999995, "text": " So this is like the language model that we did in lesson 4 where we did it a word at the time", "tokens": [407, 341, 307, 411, 264, 2856, 2316, 300, 321, 630, 294, 6898, 1017, 689, 321, 630, 309, 257, 1349, 412, 264, 565], "temperature": 0.0, "avg_logprob": -0.1720214323564009, "compression_ratio": 1.738938053097345, "no_speech_prob": 5.043464739173942e-07}, {"id": 883, "seek": 421174, "start": 4221.719999999999, "end": 4224.36, "text": " But this time we're going to do it a character at a time", "tokens": [583, 341, 565, 321, 434, 516, 281, 360, 309, 257, 2517, 412, 257, 565], "temperature": 0.0, "avg_logprob": -0.1720214323564009, "compression_ratio": 1.738938053097345, "no_speech_prob": 5.043464739173942e-07}, {"id": 884, "seek": 421174, "start": 4225.32, "end": 4232.12, "text": " And so the main thing I'm going to try and convince you is an RNN is no different to anything you've already learned", "tokens": [400, 370, 264, 2135, 551, 286, 478, 516, 281, 853, 293, 13447, 291, 307, 364, 45702, 45, 307, 572, 819, 281, 1340, 291, 600, 1217, 3264], "temperature": 0.0, "avg_logprob": -0.1720214323564009, "compression_ratio": 1.738938053097345, "no_speech_prob": 5.043464739173942e-07}, {"id": 885, "seek": 421174, "start": 4232.92, "end": 4236.44, "text": " Okay, and so to show you that we're going to build it", "tokens": [1033, 11, 293, 370, 281, 855, 291, 300, 321, 434, 516, 281, 1322, 309], "temperature": 0.0, "avg_logprob": -0.1720214323564009, "compression_ratio": 1.738938053097345, "no_speech_prob": 5.043464739173942e-07}, {"id": 886, "seek": 421174, "start": 4237.28, "end": 4238.32, "text": " from", "tokens": [490], "temperature": 0.0, "avg_logprob": -0.1720214323564009, "compression_ratio": 1.738938053097345, "no_speech_prob": 5.043464739173942e-07}, {"id": 887, "seek": 423832, "start": 4238.32, "end": 4244.92, "text": " Plane pie torch layers all of which are extremely familiar already okay, and eventually we're going to use something really complex", "tokens": [2149, 1929, 1730, 27822, 7914, 439, 295, 597, 366, 4664, 4963, 1217, 1392, 11, 293, 4728, 321, 434, 516, 281, 764, 746, 534, 3997], "temperature": 0.0, "avg_logprob": -0.17598438262939453, "compression_ratio": 1.7269076305220883, "no_speech_prob": 3.726590648511774e-06}, {"id": 888, "seek": 423832, "start": 4244.92, "end": 4249.04, "text": " Which is a for loop okay? So that's when we're going to make it really sophisticated", "tokens": [3013, 307, 257, 337, 6367, 1392, 30, 407, 300, 311, 562, 321, 434, 516, 281, 652, 309, 534, 16950], "temperature": 0.0, "avg_logprob": -0.17598438262939453, "compression_ratio": 1.7269076305220883, "no_speech_prob": 3.726590648511774e-06}, {"id": 889, "seek": 423832, "start": 4249.44, "end": 4255.5199999999995, "text": " so the basic idea of RNN is that you want to keep track of", "tokens": [370, 264, 3875, 1558, 295, 45702, 45, 307, 300, 291, 528, 281, 1066, 2837, 295], "temperature": 0.0, "avg_logprob": -0.17598438262939453, "compression_ratio": 1.7269076305220883, "no_speech_prob": 3.726590648511774e-06}, {"id": 890, "seek": 423832, "start": 4256.16, "end": 4260.48, "text": " The main thing is you want to keep track of kind of state over long-term dependencies", "tokens": [440, 2135, 551, 307, 291, 528, 281, 1066, 2837, 295, 733, 295, 1785, 670, 938, 12, 7039, 36606], "temperature": 0.0, "avg_logprob": -0.17598438262939453, "compression_ratio": 1.7269076305220883, "no_speech_prob": 3.726590648511774e-06}, {"id": 891, "seek": 423832, "start": 4260.48, "end": 4263.5199999999995, "text": " So for example if you're trying to model something like this", "tokens": [407, 337, 1365, 498, 291, 434, 1382, 281, 2316, 746, 411, 341], "temperature": 0.0, "avg_logprob": -0.17598438262939453, "compression_ratio": 1.7269076305220883, "no_speech_prob": 3.726590648511774e-06}, {"id": 892, "seek": 423832, "start": 4264.12, "end": 4266.12, "text": " kind of", "tokens": [733, 295], "temperature": 0.0, "avg_logprob": -0.17598438262939453, "compression_ratio": 1.7269076305220883, "no_speech_prob": 3.726590648511774e-06}, {"id": 893, "seek": 426612, "start": 4266.12, "end": 4274.24, "text": " Template language right then at the end of your percent comment do percent you need a percent comment end percent right and so somehow", "tokens": [39563, 473, 2856, 558, 550, 412, 264, 917, 295, 428, 3043, 2871, 360, 3043, 291, 643, 257, 3043, 2871, 917, 3043, 558, 293, 370, 6063], "temperature": 0.0, "avg_logprob": -0.22261843550095864, "compression_ratio": 1.803088803088803, "no_speech_prob": 4.356851604825351e-06}, {"id": 894, "seek": 426612, "start": 4274.5199999999995, "end": 4278.16, "text": " Your model needs to keep track of the fact that it's like inside a comment", "tokens": [2260, 2316, 2203, 281, 1066, 2837, 295, 264, 1186, 300, 309, 311, 411, 1854, 257, 2871], "temperature": 0.0, "avg_logprob": -0.22261843550095864, "compression_ratio": 1.803088803088803, "no_speech_prob": 4.356851604825351e-06}, {"id": 895, "seek": 426612, "start": 4278.72, "end": 4286.16, "text": " Over all of these different characters right and so this is this idea of state it needs kind of memory right and this is quite a difficult", "tokens": [4886, 439, 295, 613, 819, 4342, 558, 293, 370, 341, 307, 341, 1558, 295, 1785, 309, 2203, 733, 295, 4675, 558, 293, 341, 307, 1596, 257, 2252], "temperature": 0.0, "avg_logprob": -0.22261843550095864, "compression_ratio": 1.803088803088803, "no_speech_prob": 4.356851604825351e-06}, {"id": 896, "seek": 426612, "start": 4287.3, "end": 4292.62, "text": " Thing to do with like just a conv net it turns out actually to be possible, but", "tokens": [30902, 281, 360, 365, 411, 445, 257, 416, 85, 2533, 309, 4523, 484, 767, 281, 312, 1944, 11, 457], "temperature": 0.0, "avg_logprob": -0.22261843550095864, "compression_ratio": 1.803088803088803, "no_speech_prob": 4.356851604825351e-06}, {"id": 897, "seek": 426612, "start": 4293.48, "end": 4295.48, "text": " It's it's you know a little bit tricky", "tokens": [467, 311, 309, 311, 291, 458, 257, 707, 857, 12414], "temperature": 0.0, "avg_logprob": -0.22261843550095864, "compression_ratio": 1.803088803088803, "no_speech_prob": 4.356851604825351e-06}, {"id": 898, "seek": 429548, "start": 4295.48, "end": 4302.639999999999, "text": " Where else we're than RNN it turns out to be pretty straightforward right so these are the basic ideas if you want a stateful", "tokens": [2305, 1646, 321, 434, 813, 45702, 45, 309, 4523, 484, 281, 312, 1238, 15325, 558, 370, 613, 366, 264, 3875, 3487, 498, 291, 528, 257, 1785, 906], "temperature": 0.0, "avg_logprob": -0.25845536418344783, "compression_ratio": 1.617283950617284, "no_speech_prob": 1.436747425032081e-06}, {"id": 899, "seek": 429548, "start": 4303.04, "end": 4311.32, "text": " Representation we're going to keeping track of like where are we now have memory have long-term dependencies and potentially even have variable length", "tokens": [19945, 399, 321, 434, 516, 281, 5145, 2837, 295, 411, 689, 366, 321, 586, 362, 4675, 362, 938, 12, 7039, 36606, 293, 7263, 754, 362, 7006, 4641], "temperature": 0.0, "avg_logprob": -0.25845536418344783, "compression_ratio": 1.617283950617284, "no_speech_prob": 1.436747425032081e-06}, {"id": 900, "seek": 429548, "start": 4312.04, "end": 4315.0, "text": " Sequences these are all difficult things to do with comp nets", "tokens": [46859, 2667, 613, 366, 439, 2252, 721, 281, 360, 365, 715, 36170], "temperature": 0.0, "avg_logprob": -0.25845536418344783, "compression_ratio": 1.617283950617284, "no_speech_prob": 1.436747425032081e-06}, {"id": 901, "seek": 429548, "start": 4315.719999999999, "end": 4319.36, "text": " They're very straightforward with RNN's so for example", "tokens": [814, 434, 588, 15325, 365, 45702, 45, 311, 370, 337, 1365], "temperature": 0.0, "avg_logprob": -0.25845536418344783, "compression_ratio": 1.617283950617284, "no_speech_prob": 1.436747425032081e-06}, {"id": 902, "seek": 431936, "start": 4319.36, "end": 4327.08, "text": " for example Swift key a year or so ago did a blog post about how they had a new language model where they basically", "tokens": [337, 1365, 25539, 2141, 257, 1064, 420, 370, 2057, 630, 257, 6968, 2183, 466, 577, 436, 632, 257, 777, 2856, 2316, 689, 436, 1936], "temperature": 0.0, "avg_logprob": -0.1866321563720703, "compression_ratio": 1.8296296296296297, "no_speech_prob": 2.812999355228385e-06}, {"id": 903, "seek": 431936, "start": 4327.08, "end": 4332.08, "text": " This is from the blog post. They basically said like of course. This is what their neural net looks like", "tokens": [639, 307, 490, 264, 6968, 2183, 13, 814, 1936, 848, 411, 295, 1164, 13, 639, 307, 437, 641, 18161, 2533, 1542, 411], "temperature": 0.0, "avg_logprob": -0.1866321563720703, "compression_ratio": 1.8296296296296297, "no_speech_prob": 2.812999355228385e-06}, {"id": 904, "seek": 431936, "start": 4333.799999999999, "end": 4336.179999999999, "text": " Somehow they always look like this on the internet", "tokens": [28357, 436, 1009, 574, 411, 341, 322, 264, 4705], "temperature": 0.0, "avg_logprob": -0.1866321563720703, "compression_ratio": 1.8296296296296297, "no_speech_prob": 2.812999355228385e-06}, {"id": 905, "seek": 431936, "start": 4336.799999999999, "end": 4338.24, "text": " You know you've got a bunch of words", "tokens": [509, 458, 291, 600, 658, 257, 3840, 295, 2283], "temperature": 0.0, "avg_logprob": -0.1866321563720703, "compression_ratio": 1.8296296296296297, "no_speech_prob": 2.812999355228385e-06}, {"id": 906, "seek": 431936, "start": 4338.24, "end": 4344.28, "text": " And it's basically going to take your particular words in their particular orders and try and figure out what the next words going to", "tokens": [400, 309, 311, 1936, 516, 281, 747, 428, 1729, 2283, 294, 641, 1729, 9470, 293, 853, 293, 2573, 484, 437, 264, 958, 2283, 516, 281], "temperature": 0.0, "avg_logprob": -0.1866321563720703, "compression_ratio": 1.8296296296296297, "no_speech_prob": 2.812999355228385e-06}, {"id": 907, "seek": 431936, "start": 4344.28, "end": 4347.04, "text": " Be which is to say they they built a language model", "tokens": [879, 597, 307, 281, 584, 436, 436, 3094, 257, 2856, 2316], "temperature": 0.0, "avg_logprob": -0.1866321563720703, "compression_ratio": 1.8296296296296297, "no_speech_prob": 2.812999355228385e-06}, {"id": 908, "seek": 434704, "start": 4347.04, "end": 4352.68, "text": " They actually have a pretty good language model if you've used Swift key they seem to do better predictions than anybody else still", "tokens": [814, 767, 362, 257, 1238, 665, 2856, 2316, 498, 291, 600, 1143, 25539, 2141, 436, 1643, 281, 360, 1101, 21264, 813, 4472, 1646, 920], "temperature": 0.0, "avg_logprob": -0.19632735342349647, "compression_ratio": 1.6458333333333333, "no_speech_prob": 1.6536811244804994e-06}, {"id": 909, "seek": 434704, "start": 4353.96, "end": 4356.68, "text": " Another cool example was on Drake apathy a couple of years ago", "tokens": [3996, 1627, 1365, 390, 322, 27465, 1882, 9527, 257, 1916, 295, 924, 2057], "temperature": 0.0, "avg_logprob": -0.19632735342349647, "compression_ratio": 1.6458333333333333, "no_speech_prob": 1.6536811244804994e-06}, {"id": 910, "seek": 434704, "start": 4357.08, "end": 4362.2, "text": " Showed that he could use character level RNN to actually create an entire", "tokens": [6895, 292, 300, 415, 727, 764, 2517, 1496, 45702, 45, 281, 767, 1884, 364, 2302], "temperature": 0.0, "avg_logprob": -0.19632735342349647, "compression_ratio": 1.6458333333333333, "no_speech_prob": 1.6536811244804994e-06}, {"id": 911, "seek": 434704, "start": 4362.76, "end": 4369.44, "text": " Latex document, so he didn't actually tell it in any way what latex looks like he just passed in", "tokens": [31220, 87, 4166, 11, 370, 415, 994, 380, 767, 980, 309, 294, 604, 636, 437, 3469, 87, 1542, 411, 415, 445, 4678, 294], "temperature": 0.0, "avg_logprob": -0.19632735342349647, "compression_ratio": 1.6458333333333333, "no_speech_prob": 1.6536811244804994e-06}, {"id": 912, "seek": 436944, "start": 4369.44, "end": 4376.599999999999, "text": " Some latex text like this and said generate more latex text and it literally started writing something which", "tokens": [2188, 3469, 87, 2487, 411, 341, 293, 848, 8460, 544, 3469, 87, 2487, 293, 309, 3736, 1409, 3579, 746, 597], "temperature": 0.0, "avg_logprob": -0.23381901994536194, "compression_ratio": 1.5571428571428572, "no_speech_prob": 2.295908416272141e-06}, {"id": 913, "seek": 436944, "start": 4377.28, "end": 4380.08, "text": " Means about as much to me as most math papers do so", "tokens": [40290, 466, 382, 709, 281, 385, 382, 881, 5221, 10577, 360, 370], "temperature": 0.0, "avg_logprob": -0.23381901994536194, "compression_ratio": 1.5571428571428572, "no_speech_prob": 2.295908416272141e-06}, {"id": 914, "seek": 436944, "start": 4383.0, "end": 4389.0, "text": " Okay, so we're going to start with something that's not an RNN and I'm going to introduce", "tokens": [1033, 11, 370, 321, 434, 516, 281, 722, 365, 746, 300, 311, 406, 364, 45702, 45, 293, 286, 478, 516, 281, 5366], "temperature": 0.0, "avg_logprob": -0.23381901994536194, "compression_ratio": 1.5571428571428572, "no_speech_prob": 2.295908416272141e-06}, {"id": 915, "seek": 436944, "start": 4390.96, "end": 4392.96, "text": " Jeremy's patented", "tokens": [17809, 311, 1947, 6003], "temperature": 0.0, "avg_logprob": -0.23381901994536194, "compression_ratio": 1.5571428571428572, "no_speech_prob": 2.295908416272141e-06}, {"id": 916, "seek": 436944, "start": 4393.5599999999995, "end": 4397.62, "text": " Neural network notation involving boxes circles and trials", "tokens": [1734, 1807, 3209, 24657, 17030, 9002, 13040, 293, 12450], "temperature": 0.0, "avg_logprob": -0.23381901994536194, "compression_ratio": 1.5571428571428572, "no_speech_prob": 2.295908416272141e-06}, {"id": 917, "seek": 439762, "start": 4397.62, "end": 4399.62, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.3103670313738395, "compression_ratio": 1.6928104575163399, "no_speech_prob": 1.1911030242117704e-06}, {"id": 918, "seek": 439762, "start": 4400.78, "end": 4406.3, "text": " Let me explain what's going on a rectangle is an input an", "tokens": [961, 385, 2903, 437, 311, 516, 322, 257, 21930, 307, 364, 4846, 364], "temperature": 0.0, "avg_logprob": -0.3103670313738395, "compression_ratio": 1.6928104575163399, "no_speech_prob": 1.1911030242117704e-06}, {"id": 919, "seek": 439762, "start": 4407.94, "end": 4410.22, "text": " Arrow is a layer a", "tokens": [40269, 307, 257, 4583, 257], "temperature": 0.0, "avg_logprob": -0.3103670313738395, "compression_ratio": 1.6928104575163399, "no_speech_prob": 1.1911030242117704e-06}, {"id": 920, "seek": 439762, "start": 4412.38, "end": 4413.62, "text": " Circle", "tokens": [29381], "temperature": 0.0, "avg_logprob": -0.3103670313738395, "compression_ratio": 1.6928104575163399, "no_speech_prob": 1.1911030242117704e-06}, {"id": 921, "seek": 439762, "start": 4413.62, "end": 4419.7, "text": " In fact every square is a bunch of activate. Sorry every shape is a bunch of activations", "tokens": [682, 1186, 633, 3732, 307, 257, 3840, 295, 13615, 13, 4919, 633, 3909, 307, 257, 3840, 295, 2430, 763], "temperature": 0.0, "avg_logprob": -0.3103670313738395, "compression_ratio": 1.6928104575163399, "no_speech_prob": 1.1911030242117704e-06}, {"id": 922, "seek": 439762, "start": 4420.3, "end": 4426.66, "text": " right the rectangle is the input activations the circle is a hidden activations and", "tokens": [558, 264, 21930, 307, 264, 4846, 2430, 763, 264, 6329, 307, 257, 7633, 2430, 763, 293], "temperature": 0.0, "avg_logprob": -0.3103670313738395, "compression_ratio": 1.6928104575163399, "no_speech_prob": 1.1911030242117704e-06}, {"id": 923, "seek": 442666, "start": 4426.66, "end": 4428.46, "text": " a", "tokens": [257], "temperature": 0.0, "avg_logprob": -0.24943863261829724, "compression_ratio": 1.825, "no_speech_prob": 1.0677010777726537e-06}, {"id": 924, "seek": 442666, "start": 4428.46, "end": 4430.86, "text": " triangle is an output activations an", "tokens": [13369, 307, 364, 5598, 2430, 763, 364], "temperature": 0.0, "avg_logprob": -0.24943863261829724, "compression_ratio": 1.825, "no_speech_prob": 1.0677010777726537e-06}, {"id": 925, "seek": 442666, "start": 4431.82, "end": 4435.0199999999995, "text": " Arrow is a layer operation", "tokens": [40269, 307, 257, 4583, 6916], "temperature": 0.0, "avg_logprob": -0.24943863261829724, "compression_ratio": 1.825, "no_speech_prob": 1.0677010777726537e-06}, {"id": 926, "seek": 442666, "start": 4435.62, "end": 4440.74, "text": " right or possibly more than one right so here my rectangle is an input of", "tokens": [558, 420, 6264, 544, 813, 472, 558, 370, 510, 452, 21930, 307, 364, 4846, 295], "temperature": 0.0, "avg_logprob": -0.24943863261829724, "compression_ratio": 1.825, "no_speech_prob": 1.0677010777726537e-06}, {"id": 927, "seek": 442666, "start": 4441.78, "end": 4445.86, "text": " number of rows equal to batch size and number of columns equal to the number of", "tokens": [1230, 295, 13241, 2681, 281, 15245, 2744, 293, 1230, 295, 13766, 2681, 281, 264, 1230, 295], "temperature": 0.0, "avg_logprob": -0.24943863261829724, "compression_ratio": 1.825, "no_speech_prob": 1.0677010777726537e-06}, {"id": 928, "seek": 442666, "start": 4446.099999999999, "end": 4453.62, "text": " Number of inputs number of variables right and so my first arrow my first operation is going to represent a matrix product", "tokens": [5118, 295, 15743, 1230, 295, 9102, 558, 293, 370, 452, 700, 11610, 452, 700, 6916, 307, 516, 281, 2906, 257, 8141, 1674], "temperature": 0.0, "avg_logprob": -0.24943863261829724, "compression_ratio": 1.825, "no_speech_prob": 1.0677010777726537e-06}, {"id": 929, "seek": 442666, "start": 4454.0199999999995, "end": 4456.0199999999995, "text": " followed by a relu and", "tokens": [6263, 538, 257, 1039, 84, 293], "temperature": 0.0, "avg_logprob": -0.24943863261829724, "compression_ratio": 1.825, "no_speech_prob": 1.0677010777726537e-06}, {"id": 930, "seek": 445602, "start": 4456.02, "end": 4459.02, "text": " That's going to generate a set of activations", "tokens": [663, 311, 516, 281, 8460, 257, 992, 295, 2430, 763], "temperature": 0.0, "avg_logprob": -0.19090782417045846, "compression_ratio": 1.927461139896373, "no_speech_prob": 1.328774715148029e-06}, {"id": 931, "seek": 445602, "start": 4460.26, "end": 4461.5, "text": " remember", "tokens": [1604], "temperature": 0.0, "avg_logprob": -0.19090782417045846, "compression_ratio": 1.927461139896373, "no_speech_prob": 1.328774715148029e-06}, {"id": 932, "seek": 445602, "start": 4461.5, "end": 4464.580000000001, "text": " Activations an activation is a number", "tokens": [28550, 763, 364, 24433, 307, 257, 1230], "temperature": 0.0, "avg_logprob": -0.19090782417045846, "compression_ratio": 1.927461139896373, "no_speech_prob": 1.328774715148029e-06}, {"id": 933, "seek": 445602, "start": 4465.14, "end": 4472.700000000001, "text": " That an activation is a number a number that's being calculated by a relu or a matrix product or whatever", "tokens": [663, 364, 24433, 307, 257, 1230, 257, 1230, 300, 311, 885, 15598, 538, 257, 1039, 84, 420, 257, 8141, 1674, 420, 2035], "temperature": 0.0, "avg_logprob": -0.19090782417045846, "compression_ratio": 1.927461139896373, "no_speech_prob": 1.328774715148029e-06}, {"id": 934, "seek": 445602, "start": 4472.700000000001, "end": 4478.820000000001, "text": " It's a number right so this circle here represents a matrix of activations", "tokens": [467, 311, 257, 1230, 558, 370, 341, 6329, 510, 8855, 257, 8141, 295, 2430, 763], "temperature": 0.0, "avg_logprob": -0.19090782417045846, "compression_ratio": 1.927461139896373, "no_speech_prob": 1.328774715148029e-06}, {"id": 935, "seek": 445602, "start": 4478.9800000000005, "end": 4484.26, "text": " All of the numbers that come out when we take the inputs we do a matrix product followed by a relu", "tokens": [1057, 295, 264, 3547, 300, 808, 484, 562, 321, 747, 264, 15743, 321, 360, 257, 8141, 1674, 6263, 538, 257, 1039, 84], "temperature": 0.0, "avg_logprob": -0.19090782417045846, "compression_ratio": 1.927461139896373, "no_speech_prob": 1.328774715148029e-06}, {"id": 936, "seek": 448426, "start": 4484.26, "end": 4489.42, "text": " So we started with batch size by number of imports and so after we do this matrix operation", "tokens": [407, 321, 1409, 365, 15245, 2744, 538, 1230, 295, 41596, 293, 370, 934, 321, 360, 341, 8141, 6916], "temperature": 0.0, "avg_logprob": -0.18500687941065375, "compression_ratio": 1.8589211618257262, "no_speech_prob": 5.453283051792823e-07}, {"id": 937, "seek": 448426, "start": 4489.5, "end": 4491.900000000001, "text": " we now have batch size by", "tokens": [321, 586, 362, 15245, 2744, 538], "temperature": 0.0, "avg_logprob": -0.18500687941065375, "compression_ratio": 1.8589211618257262, "no_speech_prob": 5.453283051792823e-07}, {"id": 938, "seek": 448426, "start": 4493.06, "end": 4498.9400000000005, "text": " You know whatever the number of columns in our matrix product was by number of hidden units", "tokens": [509, 458, 2035, 264, 1230, 295, 13766, 294, 527, 8141, 1674, 390, 538, 1230, 295, 7633, 6815], "temperature": 0.0, "avg_logprob": -0.18500687941065375, "compression_ratio": 1.8589211618257262, "no_speech_prob": 5.453283051792823e-07}, {"id": 939, "seek": 448426, "start": 4499.34, "end": 4502.74, "text": " Okay, and so if we now take these activations, right?", "tokens": [1033, 11, 293, 370, 498, 321, 586, 747, 613, 2430, 763, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18500687941065375, "compression_ratio": 1.8589211618257262, "no_speech_prob": 5.453283051792823e-07}, {"id": 940, "seek": 448426, "start": 4502.74, "end": 4508.62, "text": " It's the matrix and we put it through another operation in this case another matrix product and a softmax", "tokens": [467, 311, 264, 8141, 293, 321, 829, 309, 807, 1071, 6916, 294, 341, 1389, 1071, 8141, 1674, 293, 257, 2787, 41167], "temperature": 0.0, "avg_logprob": -0.18500687941065375, "compression_ratio": 1.8589211618257262, "no_speech_prob": 5.453283051792823e-07}, {"id": 941, "seek": 448426, "start": 4508.9400000000005, "end": 4513.06, "text": " We get a triangle. That's our output activations another matrix of activations", "tokens": [492, 483, 257, 13369, 13, 663, 311, 527, 5598, 2430, 763, 1071, 8141, 295, 2430, 763], "temperature": 0.0, "avg_logprob": -0.18500687941065375, "compression_ratio": 1.8589211618257262, "no_speech_prob": 5.453283051792823e-07}, {"id": 942, "seek": 451306, "start": 4513.06, "end": 4519.620000000001, "text": " And again number of roses batch size number of columns number is equal to the number of classes again however many", "tokens": [400, 797, 1230, 295, 28620, 15245, 2744, 1230, 295, 13766, 1230, 307, 2681, 281, 264, 1230, 295, 5359, 797, 4461, 867], "temperature": 0.0, "avg_logprob": -0.20924262162093277, "compression_ratio": 1.6787330316742082, "no_speech_prob": 3.1875551940174773e-06}, {"id": 943, "seek": 451306, "start": 4519.9400000000005, "end": 4523.1, "text": " Columns our matrix in this matrix product head so that's a", "tokens": [4004, 449, 3695, 527, 8141, 294, 341, 8141, 1674, 1378, 370, 300, 311, 257], "temperature": 0.0, "avg_logprob": -0.20924262162093277, "compression_ratio": 1.6787330316742082, "no_speech_prob": 3.1875551940174773e-06}, {"id": 944, "seek": 451306, "start": 4526.740000000001, "end": 4530.34, "text": " That's a neural net right that's our basic kind of one hidden layer", "tokens": [663, 311, 257, 18161, 2533, 558, 300, 311, 527, 3875, 733, 295, 472, 7633, 4583], "temperature": 0.0, "avg_logprob": -0.20924262162093277, "compression_ratio": 1.6787330316742082, "no_speech_prob": 3.1875551940174773e-06}, {"id": 945, "seek": 451306, "start": 4531.3, "end": 4533.3, "text": " neural net and", "tokens": [18161, 2533, 293], "temperature": 0.0, "avg_logprob": -0.20924262162093277, "compression_ratio": 1.6787330316742082, "no_speech_prob": 3.1875551940174773e-06}, {"id": 946, "seek": 451306, "start": 4533.660000000001, "end": 4541.860000000001, "text": " If you haven't written one of these from scratch try it you know and in fact in lessons 9 10 and 11 of the machine", "tokens": [759, 291, 2378, 380, 3720, 472, 295, 613, 490, 8459, 853, 309, 291, 458, 293, 294, 1186, 294, 8820, 1722, 1266, 293, 2975, 295, 264, 3479], "temperature": 0.0, "avg_logprob": -0.20924262162093277, "compression_ratio": 1.6787330316742082, "no_speech_prob": 3.1875551940174773e-06}, {"id": 947, "seek": 454186, "start": 4541.86, "end": 4546.58, "text": " Learning course we do this right we create one of these from scratch", "tokens": [15205, 1164, 321, 360, 341, 558, 321, 1884, 472, 295, 613, 490, 8459], "temperature": 0.0, "avg_logprob": -0.15603718899264193, "compression_ratio": 1.748, "no_speech_prob": 1.816217945815879e-06}, {"id": 948, "seek": 454186, "start": 4546.58, "end": 4549.86, "text": " So if you're not quite sure how to do it you can check out the machine learning course", "tokens": [407, 498, 291, 434, 406, 1596, 988, 577, 281, 360, 309, 291, 393, 1520, 484, 264, 3479, 2539, 1164], "temperature": 0.0, "avg_logprob": -0.15603718899264193, "compression_ratio": 1.748, "no_speech_prob": 1.816217945815879e-06}, {"id": 949, "seek": 454186, "start": 4550.299999999999, "end": 4555.299999999999, "text": " Now in general the machine learning course is much more like building stuff up from the foundations", "tokens": [823, 294, 2674, 264, 3479, 2539, 1164, 307, 709, 544, 411, 2390, 1507, 493, 490, 264, 22467], "temperature": 0.0, "avg_logprob": -0.15603718899264193, "compression_ratio": 1.748, "no_speech_prob": 1.816217945815879e-06}, {"id": 950, "seek": 454186, "start": 4555.5, "end": 4559.78, "text": " Where else this course is much more like best practices kind of top-down?", "tokens": [2305, 1646, 341, 1164, 307, 709, 544, 411, 1151, 7525, 733, 295, 1192, 12, 5093, 30], "temperature": 0.0, "avg_logprob": -0.15603718899264193, "compression_ratio": 1.748, "no_speech_prob": 1.816217945815879e-06}, {"id": 951, "seek": 454186, "start": 4561.9, "end": 4569.339999999999, "text": " All right, so if we were doing like a conv net with a single dense hidden layer our input would be equal to", "tokens": [1057, 558, 11, 370, 498, 321, 645, 884, 411, 257, 3754, 2533, 365, 257, 2167, 18011, 7633, 4583, 527, 4846, 576, 312, 2681, 281], "temperature": 0.0, "avg_logprob": -0.15603718899264193, "compression_ratio": 1.748, "no_speech_prob": 1.816217945815879e-06}, {"id": 952, "seek": 456934, "start": 4569.34, "end": 4570.9400000000005, "text": " actually", "tokens": [767], "temperature": 0.0, "avg_logprob": -0.22076701242989355, "compression_ratio": 1.7315175097276265, "no_speech_prob": 7.690363759138563e-07}, {"id": 953, "seek": 456934, "start": 4570.9400000000005, "end": 4574.74, "text": " Number yeah, sorry in pie torch number of channels by height by width", "tokens": [5118, 1338, 11, 2597, 294, 1730, 27822, 1230, 295, 9235, 538, 6681, 538, 11402], "temperature": 0.0, "avg_logprob": -0.22076701242989355, "compression_ratio": 1.7315175097276265, "no_speech_prob": 7.690363759138563e-07}, {"id": 954, "seek": 456934, "start": 4575.26, "end": 4576.74, "text": " right and", "tokens": [558, 293], "temperature": 0.0, "avg_logprob": -0.22076701242989355, "compression_ratio": 1.7315175097276265, "no_speech_prob": 7.690363759138563e-07}, {"id": 955, "seek": 456934, "start": 4576.74, "end": 4581.42, "text": " Notice that here batch size appeared every time so I'm not going to I'm not going to write it anymore", "tokens": [13428, 300, 510, 15245, 2744, 8516, 633, 565, 370, 286, 478, 406, 516, 281, 286, 478, 406, 516, 281, 2464, 309, 3602], "temperature": 0.0, "avg_logprob": -0.22076701242989355, "compression_ratio": 1.7315175097276265, "no_speech_prob": 7.690363759138563e-07}, {"id": 956, "seek": 456934, "start": 4581.82, "end": 4585.06, "text": " Okay, so I've removed the batch size", "tokens": [1033, 11, 370, 286, 600, 7261, 264, 15245, 2744], "temperature": 0.0, "avg_logprob": -0.22076701242989355, "compression_ratio": 1.7315175097276265, "no_speech_prob": 7.690363759138563e-07}, {"id": 957, "seek": 456934, "start": 4585.74, "end": 4587.66, "text": " Also the activation function", "tokens": [2743, 264, 24433, 2445], "temperature": 0.0, "avg_logprob": -0.22076701242989355, "compression_ratio": 1.7315175097276265, "no_speech_prob": 7.690363759138563e-07}, {"id": 958, "seek": 456934, "start": 4587.66, "end": 4593.92, "text": " It's always basically value or something similar for all the hidden layers and softmax at the end for classification", "tokens": [467, 311, 1009, 1936, 2158, 420, 746, 2531, 337, 439, 264, 7633, 7914, 293, 2787, 41167, 412, 264, 917, 337, 21538], "temperature": 0.0, "avg_logprob": -0.22076701242989355, "compression_ratio": 1.7315175097276265, "no_speech_prob": 7.690363759138563e-07}, {"id": 959, "seek": 456934, "start": 4593.92, "end": 4597.5, "text": " So I'm not going to write that either okay, so I'm kind of each picture", "tokens": [407, 286, 478, 406, 516, 281, 2464, 300, 2139, 1392, 11, 370, 286, 478, 733, 295, 1184, 3036], "temperature": 0.0, "avg_logprob": -0.22076701242989355, "compression_ratio": 1.7315175097276265, "no_speech_prob": 7.690363759138563e-07}, {"id": 960, "seek": 459750, "start": 4597.5, "end": 4599.5, "text": " I'm going to simplify it a little bit", "tokens": [286, 478, 516, 281, 20460, 309, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.17981786313264267, "compression_ratio": 1.7602996254681649, "no_speech_prob": 3.089486199314706e-06}, {"id": 961, "seek": 459750, "start": 4599.98, "end": 4602.5, "text": " All right, so I'm not going to mention batch size is still there", "tokens": [1057, 558, 11, 370, 286, 478, 406, 516, 281, 2152, 15245, 2744, 307, 920, 456], "temperature": 0.0, "avg_logprob": -0.17981786313264267, "compression_ratio": 1.7602996254681649, "no_speech_prob": 3.089486199314706e-06}, {"id": 962, "seek": 459750, "start": 4602.54, "end": 4608.34, "text": " We're not going to mention really or softmax, but it's still there so here's our input and so in this case rather than a", "tokens": [492, 434, 406, 516, 281, 2152, 534, 420, 2787, 41167, 11, 457, 309, 311, 920, 456, 370, 510, 311, 527, 4846, 293, 370, 294, 341, 1389, 2831, 813, 257], "temperature": 0.0, "avg_logprob": -0.17981786313264267, "compression_ratio": 1.7602996254681649, "no_speech_prob": 3.089486199314706e-06}, {"id": 963, "seek": 459750, "start": 4609.18, "end": 4613.2, "text": " Matrix product we'll do a convolution a stride to convolution", "tokens": [36274, 1674, 321, 603, 360, 257, 45216, 257, 1056, 482, 281, 45216], "temperature": 0.0, "avg_logprob": -0.17981786313264267, "compression_ratio": 1.7602996254681649, "no_speech_prob": 3.089486199314706e-06}, {"id": 964, "seek": 459750, "start": 4613.2, "end": 4618.68, "text": " So we'll skip over every second one or could be a convolution followed by a max pool", "tokens": [407, 321, 603, 10023, 670, 633, 1150, 472, 420, 727, 312, 257, 45216, 6263, 538, 257, 11469, 7005], "temperature": 0.0, "avg_logprob": -0.17981786313264267, "compression_ratio": 1.7602996254681649, "no_speech_prob": 3.089486199314706e-06}, {"id": 965, "seek": 459750, "start": 4619.38, "end": 4624.82, "text": " In either case we end up with something which is replaced number of channels with number of filters", "tokens": [682, 2139, 1389, 321, 917, 493, 365, 746, 597, 307, 10772, 1230, 295, 9235, 365, 1230, 295, 15995], "temperature": 0.0, "avg_logprob": -0.17981786313264267, "compression_ratio": 1.7602996254681649, "no_speech_prob": 3.089486199314706e-06}, {"id": 966, "seek": 462482, "start": 4624.82, "end": 4630.82, "text": " Right and we have now height divided by 2 and width divided by 2 ok and then", "tokens": [1779, 293, 321, 362, 586, 6681, 6666, 538, 568, 293, 11402, 6666, 538, 568, 3133, 293, 550], "temperature": 0.0, "avg_logprob": -0.17234128586789396, "compression_ratio": 1.775330396475771, "no_speech_prob": 1.2098630577384029e-06}, {"id": 967, "seek": 462482, "start": 4631.46, "end": 4633.46, "text": " We can flatten that out somehow", "tokens": [492, 393, 24183, 300, 484, 6063], "temperature": 0.0, "avg_logprob": -0.17234128586789396, "compression_ratio": 1.775330396475771, "no_speech_prob": 1.2098630577384029e-06}, {"id": 968, "seek": 462482, "start": 4633.46, "end": 4639.82, "text": " We'll talk next week about the main way we do that nowadays which is basically to do something called an adaptive max pooling", "tokens": [492, 603, 751, 958, 1243, 466, 264, 2135, 636, 321, 360, 300, 13434, 597, 307, 1936, 281, 360, 746, 1219, 364, 27912, 11469, 7005, 278], "temperature": 0.0, "avg_logprob": -0.17234128586789396, "compression_ratio": 1.775330396475771, "no_speech_prob": 1.2098630577384029e-06}, {"id": 969, "seek": 462482, "start": 4640.34, "end": 4644.34, "text": " Where we basically get an average across the height and the width?", "tokens": [2305, 321, 1936, 483, 364, 4274, 2108, 264, 6681, 293, 264, 11402, 30], "temperature": 0.0, "avg_logprob": -0.17234128586789396, "compression_ratio": 1.775330396475771, "no_speech_prob": 1.2098630577384029e-06}, {"id": 970, "seek": 462482, "start": 4645.34, "end": 4651.24, "text": " And turn that into a vector anyway somehow we flatten it out into a vector we can do a matrix product", "tokens": [400, 1261, 300, 666, 257, 8062, 4033, 6063, 321, 24183, 309, 484, 666, 257, 8062, 321, 393, 360, 257, 8141, 1674], "temperature": 0.0, "avg_logprob": -0.17234128586789396, "compression_ratio": 1.775330396475771, "no_speech_prob": 1.2098630577384029e-06}, {"id": 971, "seek": 465124, "start": 4651.24, "end": 4656.2, "text": " Or a couple of matrix products we actually tend to do in fast AI", "tokens": [1610, 257, 1916, 295, 8141, 3383, 321, 767, 3928, 281, 360, 294, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.2326562377228134, "compression_ratio": 1.716279069767442, "no_speech_prob": 3.2887378438317683e-06}, {"id": 972, "seek": 465124, "start": 4657.639999999999, "end": 4661.32, "text": " So that'll be our fully connected layer with some number of activations", "tokens": [407, 300, 603, 312, 527, 4498, 4582, 4583, 365, 512, 1230, 295, 2430, 763], "temperature": 0.0, "avg_logprob": -0.2326562377228134, "compression_ratio": 1.716279069767442, "no_speech_prob": 3.2887378438317683e-06}, {"id": 973, "seek": 465124, "start": 4661.96, "end": 4668.66, "text": " Final matrix product give us some number of classes ok so this is our basic component remembering", "tokens": [13443, 8141, 1674, 976, 505, 512, 1230, 295, 5359, 3133, 370, 341, 307, 527, 3875, 6542, 20719], "temperature": 0.0, "avg_logprob": -0.2326562377228134, "compression_ratio": 1.716279069767442, "no_speech_prob": 3.2887378438317683e-06}, {"id": 974, "seek": 465124, "start": 4669.44, "end": 4671.44, "text": " Rectangles input circle is hidden", "tokens": [497, 557, 656, 904, 4846, 6329, 307, 7633], "temperature": 0.0, "avg_logprob": -0.2326562377228134, "compression_ratio": 1.716279069767442, "no_speech_prob": 3.2887378438317683e-06}, {"id": 975, "seek": 465124, "start": 4672.28, "end": 4675.679999999999, "text": " triangle is output all of the shapes represent a", "tokens": [13369, 307, 5598, 439, 295, 264, 10854, 2906, 257], "temperature": 0.0, "avg_logprob": -0.2326562377228134, "compression_ratio": 1.716279069767442, "no_speech_prob": 3.2887378438317683e-06}, {"id": 976, "seek": 465124, "start": 4676.5199999999995, "end": 4677.88, "text": " tensor of", "tokens": [40863, 295], "temperature": 0.0, "avg_logprob": -0.2326562377228134, "compression_ratio": 1.716279069767442, "no_speech_prob": 3.2887378438317683e-06}, {"id": 977, "seek": 467788, "start": 4677.88, "end": 4682.76, "text": " activations all of the arrows represent a operation a layer operation", "tokens": [2430, 763, 439, 295, 264, 19669, 2906, 257, 6916, 257, 4583, 6916], "temperature": 0.0, "avg_logprob": -0.1855025291442871, "compression_ratio": 1.7685185185185186, "no_speech_prob": 1.2878916777481209e-06}, {"id": 978, "seek": 467788, "start": 4683.76, "end": 4687.68, "text": " All right, so now let's go to jump to the one the first one that we're going to actually", "tokens": [1057, 558, 11, 370, 586, 718, 311, 352, 281, 3012, 281, 264, 472, 264, 700, 472, 300, 321, 434, 516, 281, 767], "temperature": 0.0, "avg_logprob": -0.1855025291442871, "compression_ratio": 1.7685185185185186, "no_speech_prob": 1.2878916777481209e-06}, {"id": 979, "seek": 467788, "start": 4689.28, "end": 4695.2, "text": " Try to try to create for NLP, and we're going to basically do exactly the same thing as here", "tokens": [6526, 281, 853, 281, 1884, 337, 426, 45196, 11, 293, 321, 434, 516, 281, 1936, 360, 2293, 264, 912, 551, 382, 510], "temperature": 0.0, "avg_logprob": -0.1855025291442871, "compression_ratio": 1.7685185185185186, "no_speech_prob": 1.2878916777481209e-06}, {"id": 980, "seek": 467788, "start": 4695.64, "end": 4703.18, "text": " Right and we're going to try and predict the third character in a three character sequence based on the previous two characters", "tokens": [1779, 293, 321, 434, 516, 281, 853, 293, 6069, 264, 2636, 2517, 294, 257, 1045, 2517, 8310, 2361, 322, 264, 3894, 732, 4342], "temperature": 0.0, "avg_logprob": -0.1855025291442871, "compression_ratio": 1.7685185185185186, "no_speech_prob": 1.2878916777481209e-06}, {"id": 981, "seek": 467788, "start": 4704.64, "end": 4706.12, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.1855025291442871, "compression_ratio": 1.7685185185185186, "no_speech_prob": 1.2878916777481209e-06}, {"id": 982, "seek": 470612, "start": 4706.12, "end": 4711.24, "text": " Now input and again remember we've removed the batch size", "tokens": [823, 4846, 293, 797, 1604, 321, 600, 7261, 264, 15245, 2744], "temperature": 0.0, "avg_logprob": -0.271214450698301, "compression_ratio": 1.5789473684210527, "no_speech_prob": 9.276322998630349e-07}, {"id": 983, "seek": 470612, "start": 4712.04, "end": 4715.68, "text": " Dimension, but we're not saying it, but it's still here, okay", "tokens": [20975, 3378, 11, 457, 321, 434, 406, 1566, 309, 11, 457, 309, 311, 920, 510, 11, 1392], "temperature": 0.0, "avg_logprob": -0.271214450698301, "compression_ratio": 1.5789473684210527, "no_speech_prob": 9.276322998630349e-07}, {"id": 984, "seek": 470612, "start": 4716.24, "end": 4722.76, "text": " And also here. I've removed the names of the layer operations entirely okay, just keeping simplifying things", "tokens": [400, 611, 510, 13, 286, 600, 7261, 264, 5288, 295, 264, 4583, 7705, 7696, 1392, 11, 445, 5145, 6883, 5489, 721], "temperature": 0.0, "avg_logprob": -0.271214450698301, "compression_ratio": 1.5789473684210527, "no_speech_prob": 9.276322998630349e-07}, {"id": 985, "seek": 470612, "start": 4723.2, "end": 4727.32, "text": " so for example our first input would be the first character of", "tokens": [370, 337, 1365, 527, 700, 4846, 576, 312, 264, 700, 2517, 295], "temperature": 0.0, "avg_logprob": -0.271214450698301, "compression_ratio": 1.5789473684210527, "no_speech_prob": 9.276322998630349e-07}, {"id": 986, "seek": 470612, "start": 4727.96, "end": 4729.96, "text": " each", "tokens": [1184], "temperature": 0.0, "avg_logprob": -0.271214450698301, "compression_ratio": 1.5789473684210527, "no_speech_prob": 9.276322998630349e-07}, {"id": 987, "seek": 470612, "start": 4730.04, "end": 4732.2, "text": " string in our many batch okay and", "tokens": [6798, 294, 527, 867, 15245, 1392, 293], "temperature": 0.0, "avg_logprob": -0.271214450698301, "compression_ratio": 1.5789473684210527, "no_speech_prob": 9.276322998630349e-07}, {"id": 988, "seek": 473220, "start": 4732.2, "end": 4739.96, "text": " Assuming this is one hot encoded then the width is just however many items there are in the vocabulary", "tokens": [6281, 24919, 341, 307, 472, 2368, 2058, 12340, 550, 264, 11402, 307, 445, 4461, 867, 4754, 456, 366, 294, 264, 19864], "temperature": 0.0, "avg_logprob": -0.19469653643094575, "compression_ratio": 1.6526717557251909, "no_speech_prob": 1.8738684275376727e-06}, {"id": 989, "seek": 473220, "start": 4739.96, "end": 4742.46, "text": " How many unique characters could we have okay?", "tokens": [1012, 867, 3845, 4342, 727, 321, 362, 1392, 30], "temperature": 0.0, "avg_logprob": -0.19469653643094575, "compression_ratio": 1.6526717557251909, "no_speech_prob": 1.8738684275376727e-06}, {"id": 990, "seek": 473220, "start": 4742.96, "end": 4747.96, "text": " We probably won't really one hot encode it will feed it in as an integer and pretend", "tokens": [492, 1391, 1582, 380, 534, 472, 2368, 2058, 1429, 309, 486, 3154, 309, 294, 382, 364, 24922, 293, 11865], "temperature": 0.0, "avg_logprob": -0.19469653643094575, "compression_ratio": 1.6526717557251909, "no_speech_prob": 1.8738684275376727e-06}, {"id": 991, "seek": 473220, "start": 4747.96, "end": 4753.26, "text": " It's one hot encoded by using an embedding layer. Which is mathematically identical okay, and then we", "tokens": [467, 311, 472, 2368, 2058, 12340, 538, 1228, 364, 12240, 3584, 4583, 13, 3013, 307, 44003, 14800, 1392, 11, 293, 550, 321], "temperature": 0.0, "avg_logprob": -0.19469653643094575, "compression_ratio": 1.6526717557251909, "no_speech_prob": 1.8738684275376727e-06}, {"id": 992, "seek": 473220, "start": 4753.88, "end": 4758.32, "text": " That's going to give us some activations which we can stick through a fully connected layer", "tokens": [663, 311, 516, 281, 976, 505, 512, 2430, 763, 597, 321, 393, 2897, 807, 257, 4498, 4582, 4583], "temperature": 0.0, "avg_logprob": -0.19469653643094575, "compression_ratio": 1.6526717557251909, "no_speech_prob": 1.8738684275376727e-06}, {"id": 993, "seek": 473220, "start": 4759.44, "end": 4761.44, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.19469653643094575, "compression_ratio": 1.6526717557251909, "no_speech_prob": 1.8738684275376727e-06}, {"id": 994, "seek": 476144, "start": 4761.44, "end": 4762.5199999999995, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.231428861618042, "compression_ratio": 1.8081632653061224, "no_speech_prob": 1.06770175989368e-06}, {"id": 995, "seek": 476144, "start": 4762.5199999999995, "end": 4767.04, "text": " We we put that through a flick through a fully connected layer to get some activations", "tokens": [492, 321, 829, 300, 807, 257, 22774, 807, 257, 4498, 4582, 4583, 281, 483, 512, 2430, 763], "temperature": 0.0, "avg_logprob": -0.231428861618042, "compression_ratio": 1.8081632653061224, "no_speech_prob": 1.06770175989368e-06}, {"id": 996, "seek": 476144, "start": 4767.32, "end": 4770.24, "text": " We can then put that through another fully connected layer", "tokens": [492, 393, 550, 829, 300, 807, 1071, 4498, 4582, 4583], "temperature": 0.0, "avg_logprob": -0.231428861618042, "compression_ratio": 1.8081632653061224, "no_speech_prob": 1.06770175989368e-06}, {"id": 997, "seek": 476144, "start": 4770.96, "end": 4772.96, "text": " And now we're going to bring in", "tokens": [400, 586, 321, 434, 516, 281, 1565, 294], "temperature": 0.0, "avg_logprob": -0.231428861618042, "compression_ratio": 1.8081632653061224, "no_speech_prob": 1.06770175989368e-06}, {"id": 998, "seek": 476144, "start": 4773.12, "end": 4777.86, "text": " The input of character 2 right so the character 2 input will be exactly the same", "tokens": [440, 4846, 295, 2517, 568, 558, 370, 264, 2517, 568, 4846, 486, 312, 2293, 264, 912], "temperature": 0.0, "avg_logprob": -0.231428861618042, "compression_ratio": 1.8081632653061224, "no_speech_prob": 1.06770175989368e-06}, {"id": 999, "seek": 476144, "start": 4778.12, "end": 4783.4, "text": " Dimensionality as the character 1 input and we now need to somehow combine these two arrows together", "tokens": [20975, 3378, 1860, 382, 264, 2517, 502, 4846, 293, 321, 586, 643, 281, 6063, 10432, 613, 732, 19669, 1214], "temperature": 0.0, "avg_logprob": -0.231428861618042, "compression_ratio": 1.8081632653061224, "no_speech_prob": 1.06770175989368e-06}, {"id": 1000, "seek": 476144, "start": 4784.12, "end": 4786.12, "text": " So we could just add them up", "tokens": [407, 321, 727, 445, 909, 552, 493], "temperature": 0.0, "avg_logprob": -0.231428861618042, "compression_ratio": 1.8081632653061224, "no_speech_prob": 1.06770175989368e-06}, {"id": 1001, "seek": 476144, "start": 4786.36, "end": 4789.919999999999, "text": " for instance right because remember this arrow here", "tokens": [337, 5197, 558, 570, 1604, 341, 11610, 510], "temperature": 0.0, "avg_logprob": -0.231428861618042, "compression_ratio": 1.8081632653061224, "no_speech_prob": 1.06770175989368e-06}, {"id": 1002, "seek": 478992, "start": 4789.92, "end": 4791.92, "text": " represents a", "tokens": [8855, 257], "temperature": 0.0, "avg_logprob": -0.18965692105500595, "compression_ratio": 1.8130434782608695, "no_speech_prob": 1.628048380553082e-06}, {"id": 1003, "seek": 478992, "start": 4792.0, "end": 4797.12, "text": " Matrix product so this matrix product is going to spit out the same dimensionality as this matrix product", "tokens": [36274, 1674, 370, 341, 8141, 1674, 307, 516, 281, 22127, 484, 264, 912, 10139, 1860, 382, 341, 8141, 1674], "temperature": 0.0, "avg_logprob": -0.18965692105500595, "compression_ratio": 1.8130434782608695, "no_speech_prob": 1.628048380553082e-06}, {"id": 1004, "seek": 478992, "start": 4797.4, "end": 4800.56, "text": " So we could just add them up to create these activations", "tokens": [407, 321, 727, 445, 909, 552, 493, 281, 1884, 613, 2430, 763], "temperature": 0.0, "avg_logprob": -0.18965692105500595, "compression_ratio": 1.8130434782608695, "no_speech_prob": 1.628048380553082e-06}, {"id": 1005, "seek": 478992, "start": 4801.4400000000005, "end": 4807.16, "text": " And so now we can put that through another matrix product and of course remember all these matrix products have a value as well", "tokens": [400, 370, 586, 321, 393, 829, 300, 807, 1071, 8141, 1674, 293, 295, 1164, 1604, 439, 613, 8141, 3383, 362, 257, 2158, 382, 731], "temperature": 0.0, "avg_logprob": -0.18965692105500595, "compression_ratio": 1.8130434782608695, "no_speech_prob": 1.628048380553082e-06}, {"id": 1006, "seek": 478992, "start": 4807.88, "end": 4816.38, "text": " And this final one will have a softmax instead to create our predicted set of characters right so it's a standard", "tokens": [400, 341, 2572, 472, 486, 362, 257, 2787, 41167, 2602, 281, 1884, 527, 19147, 992, 295, 4342, 558, 370, 309, 311, 257, 3832], "temperature": 0.0, "avg_logprob": -0.18965692105500595, "compression_ratio": 1.8130434782608695, "no_speech_prob": 1.628048380553082e-06}, {"id": 1007, "seek": 481638, "start": 4816.38, "end": 4819.78, "text": " You know to hidden layer I", "tokens": [509, 458, 281, 7633, 4583, 286], "temperature": 0.0, "avg_logprob": -0.20218279361724853, "compression_ratio": 1.5073891625615763, "no_speech_prob": 8.059424203565868e-07}, {"id": 1008, "seek": 481638, "start": 4821.58, "end": 4823.58, "text": " Guess it's actually three matrix products", "tokens": [17795, 309, 311, 767, 1045, 8141, 3383], "temperature": 0.0, "avg_logprob": -0.20218279361724853, "compression_ratio": 1.5073891625615763, "no_speech_prob": 8.059424203565868e-07}, {"id": 1009, "seek": 481638, "start": 4824.18, "end": 4825.74, "text": " Neural net", "tokens": [1734, 1807, 2533], "temperature": 0.0, "avg_logprob": -0.20218279361724853, "compression_ratio": 1.5073891625615763, "no_speech_prob": 8.059424203565868e-07}, {"id": 1010, "seek": 481638, "start": 4825.74, "end": 4828.3, "text": " This first one is coming through an embedding layer", "tokens": [639, 700, 472, 307, 1348, 807, 364, 12240, 3584, 4583], "temperature": 0.0, "avg_logprob": -0.20218279361724853, "compression_ratio": 1.5073891625615763, "no_speech_prob": 8.059424203565868e-07}, {"id": 1011, "seek": 481638, "start": 4828.78, "end": 4836.1, "text": " The only difference is that we're also got a second input coming in here, but we're just adding in right", "tokens": [440, 787, 2649, 307, 300, 321, 434, 611, 658, 257, 1150, 4846, 1348, 294, 510, 11, 457, 321, 434, 445, 5127, 294, 558], "temperature": 0.0, "avg_logprob": -0.20218279361724853, "compression_ratio": 1.5073891625615763, "no_speech_prob": 8.059424203565868e-07}, {"id": 1012, "seek": 481638, "start": 4836.1, "end": 4838.1, "text": " But it's kind of conceptually identical", "tokens": [583, 309, 311, 733, 295, 3410, 671, 14800], "temperature": 0.0, "avg_logprob": -0.20218279361724853, "compression_ratio": 1.5073891625615763, "no_speech_prob": 8.059424203565868e-07}, {"id": 1013, "seek": 481638, "start": 4838.54, "end": 4840.54, "text": " So let's let's implement that", "tokens": [407, 718, 311, 718, 311, 4445, 300], "temperature": 0.0, "avg_logprob": -0.20218279361724853, "compression_ratio": 1.5073891625615763, "no_speech_prob": 8.059424203565868e-07}, {"id": 1014, "seek": 484054, "start": 4840.54, "end": 4842.54, "text": " For", "tokens": [1171], "temperature": 0.0, "avg_logprob": -0.20354905335799509, "compression_ratio": 1.5829383886255923, "no_speech_prob": 1.1079026052129848e-07}, {"id": 1015, "seek": 484054, "start": 4844.74, "end": 4848.62, "text": " Nietzsche right so and I'm not going to use torch text", "tokens": [36583, 89, 12287, 558, 370, 293, 286, 478, 406, 516, 281, 764, 27822, 2487], "temperature": 0.0, "avg_logprob": -0.20354905335799509, "compression_ratio": 1.5829383886255923, "no_speech_prob": 1.1079026052129848e-07}, {"id": 1016, "seek": 484054, "start": 4848.62, "end": 4854.72, "text": " I'm going to try not to use almost any fast AI so we can see it all kind of again from raw right so", "tokens": [286, 478, 516, 281, 853, 406, 281, 764, 1920, 604, 2370, 7318, 370, 321, 393, 536, 309, 439, 733, 295, 797, 490, 8936, 558, 370], "temperature": 0.0, "avg_logprob": -0.20354905335799509, "compression_ratio": 1.5829383886255923, "no_speech_prob": 1.1079026052129848e-07}, {"id": 1017, "seek": 484054, "start": 4854.74, "end": 4857.66, "text": " Here's the first 400 characters of the connected works", "tokens": [1692, 311, 264, 700, 8423, 4342, 295, 264, 4582, 1985], "temperature": 0.0, "avg_logprob": -0.20354905335799509, "compression_ratio": 1.5829383886255923, "no_speech_prob": 1.1079026052129848e-07}, {"id": 1018, "seek": 484054, "start": 4858.7, "end": 4862.98, "text": " Let's grab a set of all of the letters that we see there", "tokens": [961, 311, 4444, 257, 992, 295, 439, 295, 264, 7825, 300, 321, 536, 456], "temperature": 0.0, "avg_logprob": -0.20354905335799509, "compression_ratio": 1.5829383886255923, "no_speech_prob": 1.1079026052129848e-07}, {"id": 1019, "seek": 484054, "start": 4863.58, "end": 4867.82, "text": " And sort them okay, and so a set creates all the unique letters", "tokens": [400, 1333, 552, 1392, 11, 293, 370, 257, 992, 7829, 439, 264, 3845, 7825], "temperature": 0.0, "avg_logprob": -0.20354905335799509, "compression_ratio": 1.5829383886255923, "no_speech_prob": 1.1079026052129848e-07}, {"id": 1020, "seek": 486782, "start": 4867.82, "end": 4871.099999999999, "text": " So we've got 85 unique letters in our vocab", "tokens": [407, 321, 600, 658, 14695, 3845, 7825, 294, 527, 2329, 455], "temperature": 0.0, "avg_logprob": -0.16098762088351778, "compression_ratio": 1.6395939086294415, "no_speech_prob": 1.7061800008377759e-06}, {"id": 1021, "seek": 486782, "start": 4872.98, "end": 4878.88, "text": " Let's pop a it's nice to put an empty kind of a null or some kind of padding character in there for padding", "tokens": [961, 311, 1665, 257, 309, 311, 1481, 281, 829, 364, 6707, 733, 295, 257, 18184, 420, 512, 733, 295, 39562, 2517, 294, 456, 337, 39562], "temperature": 0.0, "avg_logprob": -0.16098762088351778, "compression_ratio": 1.6395939086294415, "no_speech_prob": 1.7061800008377759e-06}, {"id": 1022, "seek": 486782, "start": 4878.88, "end": 4882.82, "text": " So we're going to put a padding character at the start right and so here is", "tokens": [407, 321, 434, 516, 281, 829, 257, 39562, 2517, 412, 264, 722, 558, 293, 370, 510, 307], "temperature": 0.0, "avg_logprob": -0.16098762088351778, "compression_ratio": 1.6395939086294415, "no_speech_prob": 1.7061800008377759e-06}, {"id": 1023, "seek": 486782, "start": 4884.0599999999995, "end": 4889.7, "text": " What our vocab looks like okay, so so cars is our vocab", "tokens": [708, 527, 2329, 455, 1542, 411, 1392, 11, 370, 370, 5163, 307, 527, 2329, 455], "temperature": 0.0, "avg_logprob": -0.16098762088351778, "compression_ratio": 1.6395939086294415, "no_speech_prob": 1.7061800008377759e-06}, {"id": 1024, "seek": 486782, "start": 4891.46, "end": 4894.0599999999995, "text": " So as per usual we want some way to map", "tokens": [407, 382, 680, 7713, 321, 528, 512, 636, 281, 4471], "temperature": 0.0, "avg_logprob": -0.16098762088351778, "compression_ratio": 1.6395939086294415, "no_speech_prob": 1.7061800008377759e-06}, {"id": 1025, "seek": 489406, "start": 4894.06, "end": 4899.900000000001, "text": " Every character to a unique ID and every unique ID to a character", "tokens": [2048, 2517, 281, 257, 3845, 7348, 293, 633, 3845, 7348, 281, 257, 2517], "temperature": 0.0, "avg_logprob": -0.24378157315188892, "compression_ratio": 1.542857142857143, "no_speech_prob": 9.570812835590914e-07}, {"id": 1026, "seek": 489406, "start": 4901.700000000001, "end": 4906.14, "text": " And so now we can just go through our collected works of niche and", "tokens": [400, 370, 586, 321, 393, 445, 352, 807, 527, 11087, 1985, 295, 19956, 293], "temperature": 0.0, "avg_logprob": -0.24378157315188892, "compression_ratio": 1.542857142857143, "no_speech_prob": 9.570812835590914e-07}, {"id": 1027, "seek": 489406, "start": 4907.700000000001, "end": 4911.34, "text": " Grab the index of each one of those characters so now", "tokens": [20357, 264, 8186, 295, 1184, 472, 295, 729, 4342, 370, 586], "temperature": 0.0, "avg_logprob": -0.24378157315188892, "compression_ratio": 1.542857142857143, "no_speech_prob": 9.570812835590914e-07}, {"id": 1028, "seek": 489406, "start": 4912.18, "end": 4914.18, "text": " We've just turned it into this", "tokens": [492, 600, 445, 3574, 309, 666, 341], "temperature": 0.0, "avg_logprob": -0.24378157315188892, "compression_ratio": 1.542857142857143, "no_speech_prob": 9.570812835590914e-07}, {"id": 1029, "seek": 489406, "start": 4914.18, "end": 4916.18, "text": " right so rather than", "tokens": [558, 370, 2831, 813], "temperature": 0.0, "avg_logprob": -0.24378157315188892, "compression_ratio": 1.542857142857143, "no_speech_prob": 9.570812835590914e-07}, {"id": 1030, "seek": 489406, "start": 4917.22, "end": 4922.14, "text": " Quote P re we now have 40 42 29", "tokens": [2326, 1370, 430, 319, 321, 586, 362, 3356, 14034, 9413], "temperature": 0.0, "avg_logprob": -0.24378157315188892, "compression_ratio": 1.542857142857143, "no_speech_prob": 9.570812835590914e-07}, {"id": 1031, "seek": 492214, "start": 4922.14, "end": 4923.900000000001, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.21763785867130056, "compression_ratio": 1.5572139303482586, "no_speech_prob": 1.0511446362215793e-06}, {"id": 1032, "seek": 492214, "start": 4923.900000000001, "end": 4925.900000000001, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.21763785867130056, "compression_ratio": 1.5572139303482586, "no_speech_prob": 1.0511446362215793e-06}, {"id": 1033, "seek": 492214, "start": 4926.780000000001, "end": 4932.54, "text": " So that's basically the first step and just to confirm we can now take each of those indexes and", "tokens": [407, 300, 311, 1936, 264, 700, 1823, 293, 445, 281, 9064, 321, 393, 586, 747, 1184, 295, 729, 8186, 279, 293], "temperature": 0.0, "avg_logprob": -0.21763785867130056, "compression_ratio": 1.5572139303482586, "no_speech_prob": 1.0511446362215793e-06}, {"id": 1034, "seek": 492214, "start": 4933.06, "end": 4937.38, "text": " Turn them back into characters and join them together and yeah there it is", "tokens": [7956, 552, 646, 666, 4342, 293, 3917, 552, 1214, 293, 1338, 456, 309, 307], "temperature": 0.0, "avg_logprob": -0.21763785867130056, "compression_ratio": 1.5572139303482586, "no_speech_prob": 1.0511446362215793e-06}, {"id": 1035, "seek": 492214, "start": 4938.06, "end": 4939.38, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.21763785867130056, "compression_ratio": 1.5572139303482586, "no_speech_prob": 1.0511446362215793e-06}, {"id": 1036, "seek": 492214, "start": 4939.38, "end": 4940.54, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.21763785867130056, "compression_ratio": 1.5572139303482586, "no_speech_prob": 1.0511446362215793e-06}, {"id": 1037, "seek": 492214, "start": 4940.54, "end": 4944.58, "text": " From now on we're just going to work with this IDX list the list of", "tokens": [3358, 586, 322, 321, 434, 445, 516, 281, 589, 365, 341, 7348, 55, 1329, 264, 1329, 295], "temperature": 0.0, "avg_logprob": -0.21763785867130056, "compression_ratio": 1.5572139303482586, "no_speech_prob": 1.0511446362215793e-06}, {"id": 1038, "seek": 492214, "start": 4945.14, "end": 4948.08, "text": " Character numbers in the connected works of Nietzsche yes", "tokens": [36786, 3547, 294, 264, 4582, 1985, 295, 36583, 89, 12287, 2086], "temperature": 0.0, "avg_logprob": -0.21763785867130056, "compression_ratio": 1.5572139303482586, "no_speech_prob": 1.0511446362215793e-06}, {"id": 1039, "seek": 494808, "start": 4948.08, "end": 4955.88, "text": " So Jeremy why are we doing like a model of characters and not a model of words?", "tokens": [407, 17809, 983, 366, 321, 884, 411, 257, 2316, 295, 4342, 293, 406, 257, 2316, 295, 2283, 30], "temperature": 0.0, "avg_logprob": -0.1627618767494379, "compression_ratio": 1.4774774774774775, "no_speech_prob": 1.4367443554874626e-06}, {"id": 1040, "seek": 494808, "start": 4955.88, "end": 4960.68, "text": " I just thought it seemed simpler. You know with a vocab of 80 ish items we can", "tokens": [286, 445, 1194, 309, 6576, 18587, 13, 509, 458, 365, 257, 2329, 455, 295, 4688, 307, 71, 4754, 321, 393], "temperature": 0.0, "avg_logprob": -0.1627618767494379, "compression_ratio": 1.4774774774774775, "no_speech_prob": 1.4367443554874626e-06}, {"id": 1041, "seek": 494808, "start": 4961.68, "end": 4963.68, "text": " Kind of see it better", "tokens": [9242, 295, 536, 309, 1101], "temperature": 0.0, "avg_logprob": -0.1627618767494379, "compression_ratio": 1.4774774774774775, "no_speech_prob": 1.4367443554874626e-06}, {"id": 1042, "seek": 494808, "start": 4965.2, "end": 4967.2, "text": " Character level models", "tokens": [36786, 1496, 5245], "temperature": 0.0, "avg_logprob": -0.1627618767494379, "compression_ratio": 1.4774774774774775, "no_speech_prob": 1.4367443554874626e-06}, {"id": 1043, "seek": 494808, "start": 4968.0, "end": 4972.4, "text": " Turn out to be potentially quite useful in a number of situations, but we'll cover that in part two", "tokens": [7956, 484, 281, 312, 7263, 1596, 4420, 294, 257, 1230, 295, 6851, 11, 457, 321, 603, 2060, 300, 294, 644, 732], "temperature": 0.0, "avg_logprob": -0.1627618767494379, "compression_ratio": 1.4774774774774775, "no_speech_prob": 1.4367443554874626e-06}, {"id": 1044, "seek": 494808, "start": 4973.24, "end": 4975.24, "text": " the short answer is like", "tokens": [264, 2099, 1867, 307, 411], "temperature": 0.0, "avg_logprob": -0.1627618767494379, "compression_ratio": 1.4774774774774775, "no_speech_prob": 1.4367443554874626e-06}, {"id": 1045, "seek": 497524, "start": 4975.24, "end": 4980.78, "text": " You generally want to combine both the word level model and a character level model like if you're doing say translation", "tokens": [509, 5101, 528, 281, 10432, 1293, 264, 1349, 1496, 2316, 293, 257, 2517, 1496, 2316, 411, 498, 291, 434, 884, 584, 12853], "temperature": 0.0, "avg_logprob": -0.18410406274310612, "compression_ratio": 1.7373737373737375, "no_speech_prob": 1.2878878123956383e-06}, {"id": 1046, "seek": 497524, "start": 4981.4, "end": 4985.5199999999995, "text": " It's a great way to deal with unknown like unusual words rather than treating it as unknown", "tokens": [467, 311, 257, 869, 636, 281, 2028, 365, 9841, 411, 10901, 2283, 2831, 813, 15083, 309, 382, 9841], "temperature": 0.0, "avg_logprob": -0.18410406274310612, "compression_ratio": 1.7373737373737375, "no_speech_prob": 1.2878878123956383e-06}, {"id": 1047, "seek": 497524, "start": 4985.8, "end": 4990.44, "text": " Anytime you see a word you haven't seen before you could use a character level model for that", "tokens": [39401, 291, 536, 257, 1349, 291, 2378, 380, 1612, 949, 291, 727, 764, 257, 2517, 1496, 2316, 337, 300], "temperature": 0.0, "avg_logprob": -0.18410406274310612, "compression_ratio": 1.7373737373737375, "no_speech_prob": 1.2878878123956383e-06}, {"id": 1048, "seek": 497524, "start": 4990.76, "end": 4996.8, "text": " And there's actually something in between the two called a byte pair encoding BPE which basically looks at little n", "tokens": [400, 456, 311, 767, 746, 294, 1296, 264, 732, 1219, 257, 40846, 6119, 43430, 363, 5208, 597, 1936, 1542, 412, 707, 297], "temperature": 0.0, "avg_logprob": -0.18410406274310612, "compression_ratio": 1.7373737373737375, "no_speech_prob": 1.2878878123956383e-06}, {"id": 1049, "seek": 497524, "start": 4997.36, "end": 5000.44, "text": " Grams of characters, but we'll cover all that in part two", "tokens": [22130, 82, 295, 4342, 11, 457, 321, 603, 2060, 439, 300, 294, 644, 732], "temperature": 0.0, "avg_logprob": -0.18410406274310612, "compression_ratio": 1.7373737373737375, "no_speech_prob": 1.2878878123956383e-06}, {"id": 1050, "seek": 497524, "start": 5002.36, "end": 5004.36, "text": " If you want to look at it right now", "tokens": [759, 291, 528, 281, 574, 412, 309, 558, 586], "temperature": 0.0, "avg_logprob": -0.18410406274310612, "compression_ratio": 1.7373737373737375, "no_speech_prob": 1.2878878123956383e-06}, {"id": 1051, "seek": 500436, "start": 5004.36, "end": 5008.32, "text": " Then part two of the existing course already has this stuff", "tokens": [1396, 644, 732, 295, 264, 6741, 1164, 1217, 575, 341, 1507], "temperature": 0.0, "avg_logprob": -0.22391180939726776, "compression_ratio": 1.6481481481481481, "no_speech_prob": 8.059416245487228e-07}, {"id": 1052, "seek": 500436, "start": 5009.04, "end": 5011.04, "text": " taught and part two of the", "tokens": [5928, 293, 644, 732, 295, 264], "temperature": 0.0, "avg_logprob": -0.22391180939726776, "compression_ratio": 1.6481481481481481, "no_speech_prob": 8.059416245487228e-07}, {"id": 1053, "seek": 500436, "start": 5012.24, "end": 5018.679999999999, "text": " Version one of this course all the LVL P stuff is in pie torch by the way, so you'll understand it straight away", "tokens": [35965, 472, 295, 341, 1164, 439, 264, 441, 53, 43, 430, 1507, 307, 294, 1730, 27822, 538, 264, 636, 11, 370, 291, 603, 1223, 309, 2997, 1314], "temperature": 0.0, "avg_logprob": -0.22391180939726776, "compression_ratio": 1.6481481481481481, "no_speech_prob": 8.059416245487228e-07}, {"id": 1054, "seek": 500436, "start": 5020.5599999999995, "end": 5026.0, "text": " It was actually the thing that inspired us to move to pie torch because trying to do it in Keras turned out to be a nightmare", "tokens": [467, 390, 767, 264, 551, 300, 7547, 505, 281, 1286, 281, 1730, 27822, 570, 1382, 281, 360, 309, 294, 591, 6985, 3574, 484, 281, 312, 257, 18724], "temperature": 0.0, "avg_logprob": -0.22391180939726776, "compression_ratio": 1.6481481481481481, "no_speech_prob": 8.059416245487228e-07}, {"id": 1055, "seek": 500436, "start": 5027.5599999999995, "end": 5029.5599999999995, "text": " All right, so let's create the", "tokens": [1057, 558, 11, 370, 718, 311, 1884, 264], "temperature": 0.0, "avg_logprob": -0.22391180939726776, "compression_ratio": 1.6481481481481481, "no_speech_prob": 8.059416245487228e-07}, {"id": 1056, "seek": 502956, "start": 5029.56, "end": 5034.4800000000005, "text": " Inputs to this we're actually going to do something slightly different what I said", "tokens": [682, 2582, 82, 281, 341, 321, 434, 767, 516, 281, 360, 746, 4748, 819, 437, 286, 848], "temperature": 0.0, "avg_logprob": -0.23076319950883106, "compression_ratio": 1.7927927927927927, "no_speech_prob": 3.138124839097145e-06}, {"id": 1057, "seek": 502956, "start": 5034.4800000000005, "end": 5037.200000000001, "text": " We're actually going to try and predict the fourth character", "tokens": [492, 434, 767, 516, 281, 853, 293, 6069, 264, 6409, 2517], "temperature": 0.0, "avg_logprob": -0.23076319950883106, "compression_ratio": 1.7927927927927927, "no_speech_prob": 3.138124839097145e-06}, {"id": 1058, "seek": 502956, "start": 5037.52, "end": 5044.92, "text": " The well actually the the fifth character using the first four so the index four character using the index zero one two and three", "tokens": [440, 731, 767, 264, 264, 9266, 2517, 1228, 264, 700, 1451, 370, 264, 8186, 1451, 2517, 1228, 264, 8186, 4018, 472, 732, 293, 1045], "temperature": 0.0, "avg_logprob": -0.23076319950883106, "compression_ratio": 1.7927927927927927, "no_speech_prob": 3.138124839097145e-06}, {"id": 1059, "seek": 502956, "start": 5044.92, "end": 5048.64, "text": " All right, so we're gonna do exactly the same thing, but with just a couple more layers", "tokens": [1057, 558, 11, 370, 321, 434, 799, 360, 2293, 264, 912, 551, 11, 457, 365, 445, 257, 1916, 544, 7914], "temperature": 0.0, "avg_logprob": -0.23076319950883106, "compression_ratio": 1.7927927927927927, "no_speech_prob": 3.138124839097145e-06}, {"id": 1060, "seek": 502956, "start": 5049.64, "end": 5051.88, "text": " so that means that we need a list of", "tokens": [370, 300, 1355, 300, 321, 643, 257, 1329, 295], "temperature": 0.0, "avg_logprob": -0.23076319950883106, "compression_ratio": 1.7927927927927927, "no_speech_prob": 3.138124839097145e-06}, {"id": 1061, "seek": 505188, "start": 5051.88, "end": 5058.4800000000005, "text": " Of the zeroth first second and third characters", "tokens": [2720, 264, 44746, 900, 700, 1150, 293, 2636, 4342], "temperature": 0.0, "avg_logprob": -0.20504104463677658, "compression_ratio": 1.747191011235955, "no_speech_prob": 3.2377442948927637e-06}, {"id": 1062, "seek": 505188, "start": 5058.6, "end": 5064.8, "text": " That's why I'm just cutting every character from the start from the one from two from three skipping over", "tokens": [663, 311, 983, 286, 478, 445, 6492, 633, 2517, 490, 264, 722, 490, 264, 472, 490, 732, 490, 1045, 31533, 670], "temperature": 0.0, "avg_logprob": -0.20504104463677658, "compression_ratio": 1.747191011235955, "no_speech_prob": 3.2377442948927637e-06}, {"id": 1063, "seek": 505188, "start": 5065.4800000000005, "end": 5067.4800000000005, "text": " three at a time", "tokens": [1045, 412, 257, 565], "temperature": 0.0, "avg_logprob": -0.20504104463677658, "compression_ratio": 1.747191011235955, "no_speech_prob": 3.2377442948927637e-06}, {"id": 1064, "seek": 505188, "start": 5067.92, "end": 5069.92, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.20504104463677658, "compression_ratio": 1.747191011235955, "no_speech_prob": 3.2377442948927637e-06}, {"id": 1065, "seek": 505188, "start": 5071.88, "end": 5074.08, "text": " Hmm this is I", "tokens": [8239, 341, 307, 286], "temperature": 0.0, "avg_logprob": -0.20504104463677658, "compression_ratio": 1.747191011235955, "no_speech_prob": 3.2377442948927637e-06}, {"id": 1066, "seek": 507408, "start": 5074.08, "end": 5081.64, "text": " I said this wrong, so we're going to predict the third character the fourth character from the third from the first three", "tokens": [286, 848, 341, 2085, 11, 370, 321, 434, 516, 281, 6069, 264, 2636, 2517, 264, 6409, 2517, 490, 264, 2636, 490, 264, 700, 1045], "temperature": 0.0, "avg_logprob": -0.23377661057460455, "compression_ratio": 1.7808988764044944, "no_speech_prob": 2.9022974104009336e-06}, {"id": 1067, "seek": 507408, "start": 5081.8, "end": 5084.42, "text": " Okay, the fourth character from the first three", "tokens": [1033, 11, 264, 6409, 2517, 490, 264, 700, 1045], "temperature": 0.0, "avg_logprob": -0.23377661057460455, "compression_ratio": 1.7808988764044944, "no_speech_prob": 2.9022974104009336e-06}, {"id": 1068, "seek": 507408, "start": 5086.6, "end": 5089.36, "text": " All right, so our inputs will be", "tokens": [1057, 558, 11, 370, 527, 15743, 486, 312], "temperature": 0.0, "avg_logprob": -0.23377661057460455, "compression_ratio": 1.7808988764044944, "no_speech_prob": 2.9022974104009336e-06}, {"id": 1069, "seek": 507408, "start": 5089.96, "end": 5096.5599999999995, "text": " These three lists right so we can just use NP dot stack to pop them together", "tokens": [1981, 1045, 14511, 558, 370, 321, 393, 445, 764, 38611, 5893, 8630, 281, 1665, 552, 1214], "temperature": 0.0, "avg_logprob": -0.23377661057460455, "compression_ratio": 1.7808988764044944, "no_speech_prob": 2.9022974104009336e-06}, {"id": 1070, "seek": 507408, "start": 5097.08, "end": 5099.08, "text": " right, so here's the", "tokens": [558, 11, 370, 510, 311, 264], "temperature": 0.0, "avg_logprob": -0.23377661057460455, "compression_ratio": 1.7808988764044944, "no_speech_prob": 2.9022974104009336e-06}, {"id": 1071, "seek": 507408, "start": 5099.96, "end": 5101.96, "text": " zero one and two", "tokens": [4018, 472, 293, 732], "temperature": 0.0, "avg_logprob": -0.23377661057460455, "compression_ratio": 1.7808988764044944, "no_speech_prob": 2.9022974104009336e-06}, {"id": 1072, "seek": 510196, "start": 5101.96, "end": 5106.76, "text": " Characters that are going to feed into a model and then here is the next character in the list", "tokens": [4327, 326, 1559, 300, 366, 516, 281, 3154, 666, 257, 2316, 293, 550, 510, 307, 264, 958, 2517, 294, 264, 1329], "temperature": 0.0, "avg_logprob": -0.23657217759352464, "compression_ratio": 1.4466666666666668, "no_speech_prob": 1.06770175989368e-06}, {"id": 1073, "seek": 510196, "start": 5108.68, "end": 5110.68, "text": " So for example", "tokens": [407, 337, 1365], "temperature": 0.0, "avg_logprob": -0.23657217759352464, "compression_ratio": 1.4466666666666668, "no_speech_prob": 1.06770175989368e-06}, {"id": 1074, "seek": 510196, "start": 5112.4800000000005, "end": 5119.1, "text": " X1 X2 X3 and Y", "tokens": [1783, 16, 1783, 17, 1783, 18, 293, 398], "temperature": 0.0, "avg_logprob": -0.23657217759352464, "compression_ratio": 1.4466666666666668, "no_speech_prob": 1.06770175989368e-06}, {"id": 1075, "seek": 510196, "start": 5121.0, "end": 5128.0, "text": " All right, so you can see for example we start off the first the very first item would be", "tokens": [1057, 558, 11, 370, 291, 393, 536, 337, 1365, 321, 722, 766, 264, 700, 264, 588, 700, 3174, 576, 312], "temperature": 0.0, "avg_logprob": -0.23657217759352464, "compression_ratio": 1.4466666666666668, "no_speech_prob": 1.06770175989368e-06}, {"id": 1076, "seek": 510196, "start": 5129.04, "end": 5131.04, "text": " 40", "tokens": [3356], "temperature": 0.0, "avg_logprob": -0.23657217759352464, "compression_ratio": 1.4466666666666668, "no_speech_prob": 1.06770175989368e-06}, {"id": 1077, "seek": 513104, "start": 5131.04, "end": 5133.04, "text": " 42 and", "tokens": [14034, 293], "temperature": 0.0, "avg_logprob": -0.22210723084288758, "compression_ratio": 1.8170731707317074, "no_speech_prob": 2.090455836878391e-06}, {"id": 1078, "seek": 513104, "start": 5133.24, "end": 5138.72, "text": " 29 that's that's characters not one and two and then we'd be predicting", "tokens": [9413, 300, 311, 300, 311, 4342, 406, 472, 293, 732, 293, 550, 321, 1116, 312, 32884], "temperature": 0.0, "avg_logprob": -0.22210723084288758, "compression_ratio": 1.8170731707317074, "no_speech_prob": 2.090455836878391e-06}, {"id": 1079, "seek": 513104, "start": 5139.56, "end": 5142.8, "text": " 30 that's the fourth character which is", "tokens": [2217, 300, 311, 264, 6409, 2517, 597, 307], "temperature": 0.0, "avg_logprob": -0.22210723084288758, "compression_ratio": 1.8170731707317074, "no_speech_prob": 2.090455836878391e-06}, {"id": 1080, "seek": 513104, "start": 5144.0, "end": 5150.68, "text": " The start of the next row right so then 30 25 27 we need to predict 29", "tokens": [440, 722, 295, 264, 958, 5386, 558, 370, 550, 2217, 3552, 7634, 321, 643, 281, 6069, 9413], "temperature": 0.0, "avg_logprob": -0.22210723084288758, "compression_ratio": 1.8170731707317074, "no_speech_prob": 2.090455836878391e-06}, {"id": 1081, "seek": 513104, "start": 5151.36, "end": 5157.16, "text": " Which is the start of the next row and so forth so we're always using three characters to predict the fourth", "tokens": [3013, 307, 264, 722, 295, 264, 958, 5386, 293, 370, 5220, 370, 321, 434, 1009, 1228, 1045, 4342, 281, 6069, 264, 6409], "temperature": 0.0, "avg_logprob": -0.22210723084288758, "compression_ratio": 1.8170731707317074, "no_speech_prob": 2.090455836878391e-06}, {"id": 1082, "seek": 515716, "start": 5157.16, "end": 5159.16, "text": " So there are", "tokens": [407, 456, 366], "temperature": 0.0, "avg_logprob": -0.1928791948544082, "compression_ratio": 1.6080402010050252, "no_speech_prob": 4.116356819849898e-07}, {"id": 1083, "seek": 515716, "start": 5161.84, "end": 5163.84, "text": " 200,000 of these", "tokens": [2331, 11, 1360, 295, 613], "temperature": 0.0, "avg_logprob": -0.1928791948544082, "compression_ratio": 1.6080402010050252, "no_speech_prob": 4.116356819849898e-07}, {"id": 1084, "seek": 515716, "start": 5164.599999999999, "end": 5166.599999999999, "text": " That we're going to try and model", "tokens": [663, 321, 434, 516, 281, 853, 293, 2316], "temperature": 0.0, "avg_logprob": -0.1928791948544082, "compression_ratio": 1.6080402010050252, "no_speech_prob": 4.116356819849898e-07}, {"id": 1085, "seek": 515716, "start": 5166.599999999999, "end": 5168.04, "text": " right, so", "tokens": [558, 11, 370], "temperature": 0.0, "avg_logprob": -0.1928791948544082, "compression_ratio": 1.6080402010050252, "no_speech_prob": 4.116356819849898e-07}, {"id": 1086, "seek": 515716, "start": 5168.04, "end": 5171.92, "text": " We're going to build this model which means we need to decide how many activations", "tokens": [492, 434, 516, 281, 1322, 341, 2316, 597, 1355, 321, 643, 281, 4536, 577, 867, 2430, 763], "temperature": 0.0, "avg_logprob": -0.1928791948544082, "compression_ratio": 1.6080402010050252, "no_speech_prob": 4.116356819849898e-07}, {"id": 1087, "seek": 515716, "start": 5175.32, "end": 5177.32, "text": " So I'm going to use 256", "tokens": [407, 286, 478, 516, 281, 764, 38882], "temperature": 0.0, "avg_logprob": -0.1928791948544082, "compression_ratio": 1.6080402010050252, "no_speech_prob": 4.116356819849898e-07}, {"id": 1088, "seek": 515716, "start": 5177.8, "end": 5184.24, "text": " Okay, and we need to decide how big our embeddings are going to be and so I decided to use 42 so about half the number", "tokens": [1033, 11, 293, 321, 643, 281, 4536, 577, 955, 527, 12240, 29432, 366, 516, 281, 312, 293, 370, 286, 3047, 281, 764, 14034, 370, 466, 1922, 264, 1230], "temperature": 0.0, "avg_logprob": -0.1928791948544082, "compression_ratio": 1.6080402010050252, "no_speech_prob": 4.116356819849898e-07}, {"id": 1089, "seek": 515716, "start": 5184.24, "end": 5186.24, "text": " Of characters I have", "tokens": [2720, 4342, 286, 362], "temperature": 0.0, "avg_logprob": -0.1928791948544082, "compression_ratio": 1.6080402010050252, "no_speech_prob": 4.116356819849898e-07}, {"id": 1090, "seek": 518624, "start": 5186.24, "end": 5190.719999999999, "text": " And you can play around with these see if you can come up with better numbers. It's just kind of experimental", "tokens": [400, 291, 393, 862, 926, 365, 613, 536, 498, 291, 393, 808, 493, 365, 1101, 3547, 13, 467, 311, 445, 733, 295, 17069], "temperature": 0.0, "avg_logprob": -0.15375099895156433, "compression_ratio": 1.7251908396946565, "no_speech_prob": 2.8573069812409813e-06}, {"id": 1091, "seek": 518624, "start": 5191.719999999999, "end": 5193.719999999999, "text": " And now we're going to build our model", "tokens": [400, 586, 321, 434, 516, 281, 1322, 527, 2316], "temperature": 0.0, "avg_logprob": -0.15375099895156433, "compression_ratio": 1.7251908396946565, "no_speech_prob": 2.8573069812409813e-06}, {"id": 1092, "seek": 518624, "start": 5194.32, "end": 5199.08, "text": " Now I'm going to change my model slightly and so here is the full version", "tokens": [823, 286, 478, 516, 281, 1319, 452, 2316, 4748, 293, 370, 510, 307, 264, 1577, 3037], "temperature": 0.0, "avg_logprob": -0.15375099895156433, "compression_ratio": 1.7251908396946565, "no_speech_prob": 2.8573069812409813e-06}, {"id": 1093, "seek": 518624, "start": 5199.5199999999995, "end": 5205.9, "text": " So predicting character for using characters one two and three as you can see it's the same picture as the previous page", "tokens": [407, 32884, 2517, 337, 1228, 4342, 472, 732, 293, 1045, 382, 291, 393, 536, 309, 311, 264, 912, 3036, 382, 264, 3894, 3028], "temperature": 0.0, "avg_logprob": -0.15375099895156433, "compression_ratio": 1.7251908396946565, "no_speech_prob": 2.8573069812409813e-06}, {"id": 1094, "seek": 518624, "start": 5206.24, "end": 5209.0, "text": " But I put some very important colored arrows here", "tokens": [583, 286, 829, 512, 588, 1021, 14332, 19669, 510], "temperature": 0.0, "avg_logprob": -0.15375099895156433, "compression_ratio": 1.7251908396946565, "no_speech_prob": 2.8573069812409813e-06}, {"id": 1095, "seek": 518624, "start": 5209.679999999999, "end": 5213.8, "text": " All the arrows of the same color are going to use the same", "tokens": [1057, 264, 19669, 295, 264, 912, 2017, 366, 516, 281, 764, 264, 912], "temperature": 0.0, "avg_logprob": -0.15375099895156433, "compression_ratio": 1.7251908396946565, "no_speech_prob": 2.8573069812409813e-06}, {"id": 1096, "seek": 521380, "start": 5213.8, "end": 5221.76, "text": " Matrix the same weight matrix right so all of our input embeddings are going to use the same matrix", "tokens": [36274, 264, 912, 3364, 8141, 558, 370, 439, 295, 527, 4846, 12240, 29432, 366, 516, 281, 764, 264, 912, 8141], "temperature": 0.0, "avg_logprob": -0.19883135477701822, "compression_ratio": 1.89375, "no_speech_prob": 2.4439841581624933e-06}, {"id": 1097, "seek": 521380, "start": 5222.72, "end": 5224.4800000000005, "text": " all of our", "tokens": [439, 295, 527], "temperature": 0.0, "avg_logprob": -0.19883135477701822, "compression_ratio": 1.89375, "no_speech_prob": 2.4439841581624933e-06}, {"id": 1098, "seek": 521380, "start": 5224.4800000000005, "end": 5228.26, "text": " Layers that go from one layer to the next are going to use the same", "tokens": [20084, 433, 300, 352, 490, 472, 4583, 281, 264, 958, 366, 516, 281, 764, 264, 912], "temperature": 0.0, "avg_logprob": -0.19883135477701822, "compression_ratio": 1.89375, "no_speech_prob": 2.4439841581624933e-06}, {"id": 1099, "seek": 521380, "start": 5228.8, "end": 5234.54, "text": " Orange arrow weight matrix and then our output will have its own matrix", "tokens": [17106, 11610, 3364, 8141, 293, 550, 527, 5598, 486, 362, 1080, 1065, 8141], "temperature": 0.0, "avg_logprob": -0.19883135477701822, "compression_ratio": 1.89375, "no_speech_prob": 2.4439841581624933e-06}, {"id": 1100, "seek": 521380, "start": 5234.56, "end": 5238.92, "text": " So we're going to have one two three weight matrices", "tokens": [407, 321, 434, 516, 281, 362, 472, 732, 1045, 3364, 32284], "temperature": 0.0, "avg_logprob": -0.19883135477701822, "compression_ratio": 1.89375, "no_speech_prob": 2.4439841581624933e-06}, {"id": 1101, "seek": 523892, "start": 5238.92, "end": 5245.4800000000005, "text": " Right and the idea here is the reason I'm not going to have a separate one for every everything here is that like", "tokens": [1779, 293, 264, 1558, 510, 307, 264, 1778, 286, 478, 406, 516, 281, 362, 257, 4994, 472, 337, 633, 1203, 510, 307, 300, 411], "temperature": 0.0, "avg_logprob": -0.1740001983642578, "compression_ratio": 1.7894736842105263, "no_speech_prob": 1.6280458794426522e-06}, {"id": 1102, "seek": 523892, "start": 5246.4800000000005, "end": 5250.64, "text": " Why would kind of semantically a character have a different meaning depending?", "tokens": [1545, 576, 733, 295, 4361, 49505, 257, 2517, 362, 257, 819, 3620, 5413, 30], "temperature": 0.0, "avg_logprob": -0.1740001983642578, "compression_ratio": 1.7894736842105263, "no_speech_prob": 1.6280458794426522e-06}, {"id": 1103, "seek": 523892, "start": 5250.64, "end": 5255.7, "text": " If it was the first or the second or the third item in a sequence like it's not like we're even starting every sequence", "tokens": [759, 309, 390, 264, 700, 420, 264, 1150, 420, 264, 2636, 3174, 294, 257, 8310, 411, 309, 311, 406, 411, 321, 434, 754, 2891, 633, 8310], "temperature": 0.0, "avg_logprob": -0.1740001983642578, "compression_ratio": 1.7894736842105263, "no_speech_prob": 1.6280458794426522e-06}, {"id": 1104, "seek": 523892, "start": 5255.72, "end": 5261.62, "text": " At the start of a sentence. We just arbitrarily chopped it into groups of three right so you would expect these to all have the same", "tokens": [1711, 264, 722, 295, 257, 8174, 13, 492, 445, 19071, 3289, 16497, 309, 666, 3935, 295, 1045, 558, 370, 291, 576, 2066, 613, 281, 439, 362, 264, 912], "temperature": 0.0, "avg_logprob": -0.1740001983642578, "compression_ratio": 1.7894736842105263, "no_speech_prob": 1.6280458794426522e-06}, {"id": 1105, "seek": 523892, "start": 5261.72, "end": 5263.12, "text": " kind of", "tokens": [733, 295], "temperature": 0.0, "avg_logprob": -0.1740001983642578, "compression_ratio": 1.7894736842105263, "no_speech_prob": 1.6280458794426522e-06}, {"id": 1106, "seek": 523892, "start": 5263.12, "end": 5267.08, "text": " Conceptual mapping and ditto like when we're moving from character naught to character one", "tokens": [47482, 901, 18350, 293, 274, 34924, 411, 562, 321, 434, 2684, 490, 2517, 13138, 281, 2517, 472], "temperature": 0.0, "avg_logprob": -0.1740001983642578, "compression_ratio": 1.7894736842105263, "no_speech_prob": 1.6280458794426522e-06}, {"id": 1107, "seek": 526708, "start": 5267.08, "end": 5270.5599999999995, "text": " You know to kind of say build up some state here", "tokens": [509, 458, 281, 733, 295, 584, 1322, 493, 512, 1785, 510], "temperature": 0.0, "avg_logprob": -0.1829475826687283, "compression_ratio": 1.7793427230046948, "no_speech_prob": 3.6898813959851395e-07}, {"id": 1108, "seek": 526708, "start": 5270.68, "end": 5274.6, "text": " Why would that be any different kind of operation to moving from character one to character two?", "tokens": [1545, 576, 300, 312, 604, 819, 733, 295, 6916, 281, 2684, 490, 2517, 472, 281, 2517, 732, 30], "temperature": 0.0, "avg_logprob": -0.1829475826687283, "compression_ratio": 1.7793427230046948, "no_speech_prob": 3.6898813959851395e-07}, {"id": 1109, "seek": 526708, "start": 5275.96, "end": 5281.2, "text": " So that's the basic idea, so let's create a three character model and", "tokens": [407, 300, 311, 264, 3875, 1558, 11, 370, 718, 311, 1884, 257, 1045, 2517, 2316, 293], "temperature": 0.0, "avg_logprob": -0.1829475826687283, "compression_ratio": 1.7793427230046948, "no_speech_prob": 3.6898813959851395e-07}, {"id": 1110, "seek": 526708, "start": 5282.2, "end": 5285.8, "text": " So we're going to create one linear layer for our green arrow", "tokens": [407, 321, 434, 516, 281, 1884, 472, 8213, 4583, 337, 527, 3092, 11610], "temperature": 0.0, "avg_logprob": -0.1829475826687283, "compression_ratio": 1.7793427230046948, "no_speech_prob": 3.6898813959851395e-07}, {"id": 1111, "seek": 526708, "start": 5286.44, "end": 5293.72, "text": " One linear layer for orange arrow and one linear layer for our blue arrow and then also one embedding", "tokens": [1485, 8213, 4583, 337, 7671, 11610, 293, 472, 8213, 4583, 337, 527, 3344, 11610, 293, 550, 611, 472, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.1829475826687283, "compression_ratio": 1.7793427230046948, "no_speech_prob": 3.6898813959851395e-07}, {"id": 1112, "seek": 529372, "start": 5293.72, "end": 5300.4400000000005, "text": " Okay, so the embedding is going to bring in something with of size whatever it was 84", "tokens": [1033, 11, 370, 264, 12240, 3584, 307, 516, 281, 1565, 294, 746, 365, 295, 2744, 2035, 309, 390, 29018], "temperature": 0.0, "avg_logprob": -0.19740831284295945, "compression_ratio": 1.7245762711864407, "no_speech_prob": 1.5534941439909744e-06}, {"id": 1113, "seek": 529372, "start": 5300.4400000000005, "end": 5304.16, "text": " I think vocab size and spit out something with a number of factors in the embedding", "tokens": [286, 519, 2329, 455, 2744, 293, 22127, 484, 746, 365, 257, 1230, 295, 6771, 294, 264, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.19740831284295945, "compression_ratio": 1.7245762711864407, "no_speech_prob": 1.5534941439909744e-06}, {"id": 1114, "seek": 529372, "start": 5305.6, "end": 5307.6, "text": " We'll then put that through a linear layer", "tokens": [492, 603, 550, 829, 300, 807, 257, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.19740831284295945, "compression_ratio": 1.7245762711864407, "no_speech_prob": 1.5534941439909744e-06}, {"id": 1115, "seek": 529372, "start": 5308.400000000001, "end": 5311.88, "text": " And then we've got our hidden layers. We've got our output layer so", "tokens": [400, 550, 321, 600, 658, 527, 7633, 7914, 13, 492, 600, 658, 527, 5598, 4583, 370], "temperature": 0.0, "avg_logprob": -0.19740831284295945, "compression_ratio": 1.7245762711864407, "no_speech_prob": 1.5534941439909744e-06}, {"id": 1116, "seek": 529372, "start": 5312.52, "end": 5314.16, "text": " when we call", "tokens": [562, 321, 818], "temperature": 0.0, "avg_logprob": -0.19740831284295945, "compression_ratio": 1.7245762711864407, "no_speech_prob": 1.5534941439909744e-06}, {"id": 1117, "seek": 529372, "start": 5314.16, "end": 5318.16, "text": " Forward we're going to be passing in one two three characters", "tokens": [35524, 321, 434, 516, 281, 312, 8437, 294, 472, 732, 1045, 4342], "temperature": 0.0, "avg_logprob": -0.19740831284295945, "compression_ratio": 1.7245762711864407, "no_speech_prob": 1.5534941439909744e-06}, {"id": 1118, "seek": 529372, "start": 5318.88, "end": 5321.16, "text": " So for each one we'll stick it through an embedding", "tokens": [407, 337, 1184, 472, 321, 603, 2897, 309, 807, 364, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.19740831284295945, "compression_ratio": 1.7245762711864407, "no_speech_prob": 1.5534941439909744e-06}, {"id": 1119, "seek": 532116, "start": 5321.16, "end": 5325.5599999999995, "text": " We'll stick it through a linear layer, and we'll stick it through a value", "tokens": [492, 603, 2897, 309, 807, 257, 8213, 4583, 11, 293, 321, 603, 2897, 309, 807, 257, 2158], "temperature": 0.0, "avg_logprob": -0.24832758696182913, "compression_ratio": 1.6375838926174497, "no_speech_prob": 2.918937127560639e-07}, {"id": 1120, "seek": 532116, "start": 5326.16, "end": 5329.24, "text": " Yes, we do it for character one character two and character three", "tokens": [1079, 11, 321, 360, 309, 337, 2517, 472, 2517, 732, 293, 2517, 1045], "temperature": 0.0, "avg_logprob": -0.24832758696182913, "compression_ratio": 1.6375838926174497, "no_speech_prob": 2.918937127560639e-07}, {"id": 1121, "seek": 532116, "start": 5330.2, "end": 5332.2, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.24832758696182913, "compression_ratio": 1.6375838926174497, "no_speech_prob": 2.918937127560639e-07}, {"id": 1122, "seek": 532116, "start": 5332.44, "end": 5334.44, "text": " Then", "tokens": [1396], "temperature": 0.0, "avg_logprob": -0.24832758696182913, "compression_ratio": 1.6375838926174497, "no_speech_prob": 2.918937127560639e-07}, {"id": 1123, "seek": 532116, "start": 5339.639999999999, "end": 5341.639999999999, "text": " I'm going to create", "tokens": [286, 478, 516, 281, 1884], "temperature": 0.0, "avg_logprob": -0.24832758696182913, "compression_ratio": 1.6375838926174497, "no_speech_prob": 2.918937127560639e-07}, {"id": 1124, "seek": 532116, "start": 5343.68, "end": 5348.44, "text": " This circle of activations here, okay, and that matrix I'm going to call H", "tokens": [639, 6329, 295, 2430, 763, 510, 11, 1392, 11, 293, 300, 8141, 286, 478, 516, 281, 818, 389], "temperature": 0.0, "avg_logprob": -0.24832758696182913, "compression_ratio": 1.6375838926174497, "no_speech_prob": 2.918937127560639e-07}, {"id": 1125, "seek": 534844, "start": 5348.44, "end": 5353.32, "text": " Right and so it's going to be equal to my input activations", "tokens": [1779, 293, 370, 309, 311, 516, 281, 312, 2681, 281, 452, 4846, 2430, 763], "temperature": 0.0, "avg_logprob": -0.1428449611471157, "compression_ratio": 1.8529411764705883, "no_speech_prob": 1.873870360213914e-06}, {"id": 1126, "seek": 534844, "start": 5355.719999999999, "end": 5361.879999999999, "text": " Okay after going through the value and the linear layer and the embedding right and then I'm going to apply", "tokens": [1033, 934, 516, 807, 264, 2158, 293, 264, 8213, 4583, 293, 264, 12240, 3584, 558, 293, 550, 286, 478, 516, 281, 3079], "temperature": 0.0, "avg_logprob": -0.1428449611471157, "compression_ratio": 1.8529411764705883, "no_speech_prob": 1.873870360213914e-06}, {"id": 1127, "seek": 534844, "start": 5363.0, "end": 5364.719999999999, "text": " this", "tokens": [341], "temperature": 0.0, "avg_logprob": -0.1428449611471157, "compression_ratio": 1.8529411764705883, "no_speech_prob": 1.873870360213914e-06}, {"id": 1128, "seek": 534844, "start": 5364.719999999999, "end": 5368.719999999999, "text": " L hidden so the orange arrow and that's going to get me to here", "tokens": [441, 7633, 370, 264, 7671, 11610, 293, 300, 311, 516, 281, 483, 385, 281, 510], "temperature": 0.0, "avg_logprob": -0.1428449611471157, "compression_ratio": 1.8529411764705883, "no_speech_prob": 1.873870360213914e-06}, {"id": 1129, "seek": 534844, "start": 5369.719999999999, "end": 5374.16, "text": " Okay, so that's what this layer here does and then to get to the next one", "tokens": [1033, 11, 370, 300, 311, 437, 341, 4583, 510, 775, 293, 550, 281, 483, 281, 264, 958, 472], "temperature": 0.0, "avg_logprob": -0.1428449611471157, "compression_ratio": 1.8529411764705883, "no_speech_prob": 1.873870360213914e-06}, {"id": 1130, "seek": 537416, "start": 5374.16, "end": 5381.08, "text": " I need to apply the same thing and it's by the orange arrow to that okay, but I also have to add in", "tokens": [286, 643, 281, 3079, 264, 912, 551, 293, 309, 311, 538, 264, 7671, 11610, 281, 300, 1392, 11, 457, 286, 611, 362, 281, 909, 294], "temperature": 0.0, "avg_logprob": -0.27538362334046185, "compression_ratio": 1.5544041450777202, "no_speech_prob": 1.028939732350409e-05}, {"id": 1131, "seek": 537416, "start": 5381.88, "end": 5386.4, "text": " The second input right so take my second input and add in", "tokens": [440, 1150, 4846, 558, 370, 747, 452, 1150, 4846, 293, 909, 294], "temperature": 0.0, "avg_logprob": -0.27538362334046185, "compression_ratio": 1.5544041450777202, "no_speech_prob": 1.028939732350409e-05}, {"id": 1132, "seek": 537416, "start": 5388.0, "end": 5390.54, "text": " Okay my previous layer", "tokens": [1033, 452, 3894, 4583], "temperature": 0.0, "avg_logprob": -0.27538362334046185, "compression_ratio": 1.5544041450777202, "no_speech_prob": 1.028939732350409e-05}, {"id": 1133, "seek": 537416, "start": 5391.8, "end": 5394.28, "text": " Yanet could you pass that back through yours I?", "tokens": [13633, 302, 727, 291, 1320, 300, 646, 807, 6342, 286, 30], "temperature": 0.0, "avg_logprob": -0.27538362334046185, "compression_ratio": 1.5544041450777202, "no_speech_prob": 1.028939732350409e-05}, {"id": 1134, "seek": 539428, "start": 5394.28, "end": 5402.8, "text": " Don't really see how these dimensions are the same from H", "tokens": [1468, 380, 534, 536, 577, 613, 12819, 366, 264, 912, 490, 389], "temperature": 0.0, "avg_logprob": -0.2369184261415063, "compression_ratio": 1.5759162303664922, "no_speech_prob": 3.668827503133798e-06}, {"id": 1135, "seek": 539428, "start": 5402.8, "end": 5409.719999999999, "text": " And I am too from which to which from yeah, okay? Let's go through it, so let's figure out the dimensions together, so", "tokens": [400, 286, 669, 886, 490, 597, 281, 597, 490, 1338, 11, 1392, 30, 961, 311, 352, 807, 309, 11, 370, 718, 311, 2573, 484, 264, 12819, 1214, 11, 370], "temperature": 0.0, "avg_logprob": -0.2369184261415063, "compression_ratio": 1.5759162303664922, "no_speech_prob": 3.668827503133798e-06}, {"id": 1136, "seek": 539428, "start": 5410.8, "end": 5414.759999999999, "text": " Self dot e is going to be of length 42", "tokens": [16348, 5893, 308, 307, 516, 281, 312, 295, 4641, 14034], "temperature": 0.0, "avg_logprob": -0.2369184261415063, "compression_ratio": 1.5759162303664922, "no_speech_prob": 3.668827503133798e-06}, {"id": 1137, "seek": 539428, "start": 5415.599999999999, "end": 5421.0, "text": " Okay, and then it's going to go through L in just going to make it of size and hidden", "tokens": [1033, 11, 293, 550, 309, 311, 516, 281, 352, 807, 441, 294, 445, 516, 281, 652, 309, 295, 2744, 293, 7633], "temperature": 0.0, "avg_logprob": -0.2369184261415063, "compression_ratio": 1.5759162303664922, "no_speech_prob": 3.668827503133798e-06}, {"id": 1138, "seek": 542100, "start": 5421.0, "end": 5423.0, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.27290254906763006, "compression_ratio": 1.7383720930232558, "no_speech_prob": 1.0188073247263674e-06}, {"id": 1139, "seek": 542100, "start": 5423.76, "end": 5428.76, "text": " And so then we're going to pass that which is now is size and hidden", "tokens": [400, 370, 550, 321, 434, 516, 281, 1320, 300, 597, 307, 586, 307, 2744, 293, 7633], "temperature": 0.0, "avg_logprob": -0.27290254906763006, "compression_ratio": 1.7383720930232558, "no_speech_prob": 1.0188073247263674e-06}, {"id": 1140, "seek": 542100, "start": 5429.8, "end": 5431.8, "text": " through this which is", "tokens": [807, 341, 597, 307], "temperature": 0.0, "avg_logprob": -0.27290254906763006, "compression_ratio": 1.7383720930232558, "no_speech_prob": 1.0188073247263674e-06}, {"id": 1141, "seek": 542100, "start": 5433.12, "end": 5438.64, "text": " Also going to return something of size and hidden okay, so it's really important to notice that this is square", "tokens": [2743, 516, 281, 2736, 746, 295, 2744, 293, 7633, 1392, 11, 370, 309, 311, 534, 1021, 281, 3449, 300, 341, 307, 3732], "temperature": 0.0, "avg_logprob": -0.27290254906763006, "compression_ratio": 1.7383720930232558, "no_speech_prob": 1.0188073247263674e-06}, {"id": 1142, "seek": 542100, "start": 5438.64, "end": 5440.64, "text": " This is a square weight matrix", "tokens": [639, 307, 257, 3732, 3364, 8141], "temperature": 0.0, "avg_logprob": -0.27290254906763006, "compression_ratio": 1.7383720930232558, "no_speech_prob": 1.0188073247263674e-06}, {"id": 1143, "seek": 542100, "start": 5442.0, "end": 5445.0, "text": " Okay, so we know I'll know that this is of size and hidden in", "tokens": [1033, 11, 370, 321, 458, 286, 603, 458, 300, 341, 307, 295, 2744, 293, 7633, 294], "temperature": 0.0, "avg_logprob": -0.27290254906763006, "compression_ratio": 1.7383720930232558, "no_speech_prob": 1.0188073247263674e-06}, {"id": 1144, "seek": 544500, "start": 5445.0, "end": 5451.44, "text": " In two is going to be exactly the same size as in one was which is n hidden so we can now sum together two", "tokens": [682, 732, 307, 516, 281, 312, 2293, 264, 912, 2744, 382, 294, 472, 390, 597, 307, 297, 7633, 370, 321, 393, 586, 2408, 1214, 732], "temperature": 0.0, "avg_logprob": -0.19351343838673718, "compression_ratio": 1.7410358565737052, "no_speech_prob": 3.0894773317413637e-06}, {"id": 1145, "seek": 544500, "start": 5452.48, "end": 5454.8, "text": " sets of activations both of size and hidden", "tokens": [6352, 295, 2430, 763, 1293, 295, 2744, 293, 7633], "temperature": 0.0, "avg_logprob": -0.19351343838673718, "compression_ratio": 1.7410358565737052, "no_speech_prob": 3.0894773317413637e-06}, {"id": 1146, "seek": 544500, "start": 5455.64, "end": 5457.64, "text": " passing it into here and", "tokens": [8437, 309, 666, 510, 293], "temperature": 0.0, "avg_logprob": -0.19351343838673718, "compression_ratio": 1.7410358565737052, "no_speech_prob": 3.0894773317413637e-06}, {"id": 1147, "seek": 544500, "start": 5457.92, "end": 5464.54, "text": " Again, it returns something of size and hidden so basically the trick was to make this a square matrix and to make sure that it's square", "tokens": [3764, 11, 309, 11247, 746, 295, 2744, 293, 7633, 370, 1936, 264, 4282, 390, 281, 652, 341, 257, 3732, 8141, 293, 281, 652, 988, 300, 309, 311, 3732], "temperature": 0.0, "avg_logprob": -0.19351343838673718, "compression_ratio": 1.7410358565737052, "no_speech_prob": 3.0894773317413637e-06}, {"id": 1148, "seek": 546454, "start": 5464.54, "end": 5475.22, "text": " Matrix was the same size as the output of this hidden way. Thanks for the great question. Can you pass that back to you now?", "tokens": [36274, 390, 264, 912, 2744, 382, 264, 5598, 295, 341, 7633, 636, 13, 2561, 337, 264, 869, 1168, 13, 1664, 291, 1320, 300, 646, 281, 291, 586, 30], "temperature": 0.0, "avg_logprob": -0.24358044068018594, "compression_ratio": 1.6550218340611353, "no_speech_prob": 7.183190973591991e-06}, {"id": 1149, "seek": 546454, "start": 5475.76, "end": 5477.24, "text": " Jeremy is", "tokens": [17809, 307], "temperature": 0.0, "avg_logprob": -0.24358044068018594, "compression_ratio": 1.6550218340611353, "no_speech_prob": 7.183190973591991e-06}, {"id": 1150, "seek": 546454, "start": 5477.24, "end": 5484.28, "text": " Summing the only thing people can do in these cases or we'll come back to that in a moment. That's a great point okay", "tokens": [8626, 2810, 264, 787, 551, 561, 393, 360, 294, 613, 3331, 420, 321, 603, 808, 646, 281, 300, 294, 257, 1623, 13, 663, 311, 257, 869, 935, 1392], "temperature": 0.0, "avg_logprob": -0.24358044068018594, "compression_ratio": 1.6550218340611353, "no_speech_prob": 7.183190973591991e-06}, {"id": 1151, "seek": 546454, "start": 5485.08, "end": 5487.32, "text": " I don't like it when I have like", "tokens": [286, 500, 380, 411, 309, 562, 286, 362, 411], "temperature": 0.0, "avg_logprob": -0.24358044068018594, "compression_ratio": 1.6550218340611353, "no_speech_prob": 7.183190973591991e-06}, {"id": 1152, "seek": 546454, "start": 5487.76, "end": 5491.6, "text": " Three bits of code that look identical and then three bits of code that look nearly identical", "tokens": [6244, 9239, 295, 3089, 300, 574, 14800, 293, 550, 1045, 9239, 295, 3089, 300, 574, 6217, 14800], "temperature": 0.0, "avg_logprob": -0.24358044068018594, "compression_ratio": 1.6550218340611353, "no_speech_prob": 7.183190973591991e-06}, {"id": 1153, "seek": 549160, "start": 5491.6, "end": 5496.72, "text": " But aren't quite because it's harder to refactor, so I'm going to put a", "tokens": [583, 3212, 380, 1596, 570, 309, 311, 6081, 281, 1895, 15104, 11, 370, 286, 478, 516, 281, 829, 257], "temperature": 0.0, "avg_logprob": -0.20888841719854445, "compression_ratio": 1.5396039603960396, "no_speech_prob": 1.1365613090674742e-06}, {"id": 1154, "seek": 549160, "start": 5498.5, "end": 5502.620000000001, "text": " Make H into a bunch of zeros so that I can then put", "tokens": [4387, 389, 666, 257, 3840, 295, 35193, 370, 300, 286, 393, 550, 829], "temperature": 0.0, "avg_logprob": -0.20888841719854445, "compression_ratio": 1.5396039603960396, "no_speech_prob": 1.1365613090674742e-06}, {"id": 1155, "seek": 549160, "start": 5504.3, "end": 5506.860000000001, "text": " H here and these are now identical", "tokens": [389, 510, 293, 613, 366, 586, 14800], "temperature": 0.0, "avg_logprob": -0.20888841719854445, "compression_ratio": 1.5396039603960396, "no_speech_prob": 1.1365613090674742e-06}, {"id": 1156, "seek": 549160, "start": 5507.54, "end": 5514.740000000001, "text": " Right so that the hugely complex trick that we're going to do very shortly is to replace these three things", "tokens": [1779, 370, 300, 264, 27417, 3997, 4282, 300, 321, 434, 516, 281, 360, 588, 13392, 307, 281, 7406, 613, 1045, 721], "temperature": 0.0, "avg_logprob": -0.20888841719854445, "compression_ratio": 1.5396039603960396, "no_speech_prob": 1.1365613090674742e-06}, {"id": 1157, "seek": 549160, "start": 5515.06, "end": 5518.38, "text": " With a for loop okay, and it's going to loop", "tokens": [2022, 257, 337, 6367, 1392, 11, 293, 309, 311, 516, 281, 6367], "temperature": 0.0, "avg_logprob": -0.20888841719854445, "compression_ratio": 1.5396039603960396, "no_speech_prob": 1.1365613090674742e-06}, {"id": 1158, "seek": 551838, "start": 5518.38, "end": 5525.62, "text": " Through one two and three and that's that's going to be the for loop or actually zero one and two okay at that point", "tokens": [8927, 472, 732, 293, 1045, 293, 300, 311, 300, 311, 516, 281, 312, 264, 337, 6367, 420, 767, 4018, 472, 293, 732, 1392, 412, 300, 935], "temperature": 0.0, "avg_logprob": -0.18400435674758184, "compression_ratio": 1.691358024691358, "no_speech_prob": 2.406092562523554e-06}, {"id": 1159, "seek": 551838, "start": 5525.62, "end": 5532.74, "text": " We'll be able to call it a recurrent neural network. Okay, so just to skip ahead a little bit alright, so we create that", "tokens": [492, 603, 312, 1075, 281, 818, 309, 257, 18680, 1753, 18161, 3209, 13, 1033, 11, 370, 445, 281, 10023, 2286, 257, 707, 857, 5845, 11, 370, 321, 1884, 300], "temperature": 0.0, "avg_logprob": -0.18400435674758184, "compression_ratio": 1.691358024691358, "no_speech_prob": 2.406092562523554e-06}, {"id": 1160, "seek": 551838, "start": 5534.26, "end": 5538.06, "text": " That model make sure I've run all these so we can actually run this thing", "tokens": [663, 2316, 652, 988, 286, 600, 1190, 439, 613, 370, 321, 393, 767, 1190, 341, 551], "temperature": 0.0, "avg_logprob": -0.18400435674758184, "compression_ratio": 1.691358024691358, "no_speech_prob": 2.406092562523554e-06}, {"id": 1161, "seek": 551838, "start": 5539.86, "end": 5541.86, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.18400435674758184, "compression_ratio": 1.691358024691358, "no_speech_prob": 2.406092562523554e-06}, {"id": 1162, "seek": 554186, "start": 5541.86, "end": 5549.0599999999995, "text": " So we can now just use the same columnar model data class that we've used before and if we use from arrays", "tokens": [407, 321, 393, 586, 445, 764, 264, 912, 7738, 289, 2316, 1412, 1508, 300, 321, 600, 1143, 949, 293, 498, 321, 764, 490, 41011], "temperature": 0.0, "avg_logprob": -0.1386513286166721, "compression_ratio": 1.7037037037037037, "no_speech_prob": 2.2603148863709066e-06}, {"id": 1163, "seek": 554186, "start": 5550.0199999999995, "end": 5552.96, "text": " Then it's basically it is going to spit back the exact arrays", "tokens": [1396, 309, 311, 1936, 309, 307, 516, 281, 22127, 646, 264, 1900, 41011], "temperature": 0.0, "avg_logprob": -0.1386513286166721, "compression_ratio": 1.7037037037037037, "no_speech_prob": 2.2603148863709066e-06}, {"id": 1164, "seek": 554186, "start": 5552.96, "end": 5557.259999999999, "text": " We gave it right so if we pass if we stack together those three arrays", "tokens": [492, 2729, 309, 558, 370, 498, 321, 1320, 498, 321, 8630, 1214, 729, 1045, 41011], "temperature": 0.0, "avg_logprob": -0.1386513286166721, "compression_ratio": 1.7037037037037037, "no_speech_prob": 2.2603148863709066e-06}, {"id": 1165, "seek": 554186, "start": 5557.62, "end": 5562.299999999999, "text": " Then it's going to feed us those three things back to our forward method so if you want to like", "tokens": [1396, 309, 311, 516, 281, 3154, 505, 729, 1045, 721, 646, 281, 527, 2128, 3170, 370, 498, 291, 528, 281, 411], "temperature": 0.0, "avg_logprob": -0.1386513286166721, "compression_ratio": 1.7037037037037037, "no_speech_prob": 2.2603148863709066e-06}, {"id": 1166, "seek": 554186, "start": 5563.94, "end": 5566.259999999999, "text": " Play around with training models", "tokens": [5506, 926, 365, 3097, 5245], "temperature": 0.0, "avg_logprob": -0.1386513286166721, "compression_ratio": 1.7037037037037037, "no_speech_prob": 2.2603148863709066e-06}, {"id": 1167, "seek": 556626, "start": 5566.26, "end": 5572.64, "text": " Using like you know as raw and approach as possible, but without writing lots of boilerplate", "tokens": [11142, 411, 291, 458, 382, 8936, 293, 3109, 382, 1944, 11, 457, 1553, 3579, 3195, 295, 39228, 37008], "temperature": 0.0, "avg_logprob": -0.17779382404528166, "compression_ratio": 1.6409090909090909, "no_speech_prob": 5.896409902561572e-07}, {"id": 1168, "seek": 556626, "start": 5572.64, "end": 5576.62, "text": " This is kind of how to do it use columnar met model data from arrays", "tokens": [639, 307, 733, 295, 577, 281, 360, 309, 764, 7738, 289, 1131, 2316, 1412, 490, 41011], "temperature": 0.0, "avg_logprob": -0.17779382404528166, "compression_ratio": 1.6409090909090909, "no_speech_prob": 5.896409902561572e-07}, {"id": 1169, "seek": 556626, "start": 5576.62, "end": 5579.96, "text": " And then if you pass in whatever you pass in here", "tokens": [400, 550, 498, 291, 1320, 294, 2035, 291, 1320, 294, 510], "temperature": 0.0, "avg_logprob": -0.17779382404528166, "compression_ratio": 1.6409090909090909, "no_speech_prob": 5.896409902561572e-07}, {"id": 1170, "seek": 556626, "start": 5580.54, "end": 5582.54, "text": " Right you're going to get back", "tokens": [1779, 291, 434, 516, 281, 483, 646], "temperature": 0.0, "avg_logprob": -0.17779382404528166, "compression_ratio": 1.6409090909090909, "no_speech_prob": 5.896409902561572e-07}, {"id": 1171, "seek": 556626, "start": 5582.9800000000005, "end": 5584.3, "text": " here", "tokens": [510], "temperature": 0.0, "avg_logprob": -0.17779382404528166, "compression_ratio": 1.6409090909090909, "no_speech_prob": 5.896409902561572e-07}, {"id": 1172, "seek": 556626, "start": 5584.3, "end": 5586.3, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.17779382404528166, "compression_ratio": 1.6409090909090909, "no_speech_prob": 5.896409902561572e-07}, {"id": 1173, "seek": 556626, "start": 5586.34, "end": 5592.1, "text": " So I've passed in three things which means I'm going to get sent three things okay, so that's how that works", "tokens": [407, 286, 600, 4678, 294, 1045, 721, 597, 1355, 286, 478, 516, 281, 483, 2279, 1045, 721, 1392, 11, 370, 300, 311, 577, 300, 1985], "temperature": 0.0, "avg_logprob": -0.17779382404528166, "compression_ratio": 1.6409090909090909, "no_speech_prob": 5.896409902561572e-07}, {"id": 1174, "seek": 559210, "start": 5592.1, "end": 5598.700000000001, "text": " I'm batch size 512 because this is you know this data is tiny so I can use a bigger batch size", "tokens": [286, 478, 15245, 2744, 1025, 4762, 570, 341, 307, 291, 458, 341, 1412, 307, 5870, 370, 286, 393, 764, 257, 3801, 15245, 2744], "temperature": 0.0, "avg_logprob": -0.16818561553955078, "compression_ratio": 1.8074074074074074, "no_speech_prob": 3.611961574279121e-06}, {"id": 1175, "seek": 559210, "start": 5599.9800000000005, "end": 5601.9800000000005, "text": " So I'm not using", "tokens": [407, 286, 478, 406, 1228], "temperature": 0.0, "avg_logprob": -0.16818561553955078, "compression_ratio": 1.8074074074074074, "no_speech_prob": 3.611961574279121e-06}, {"id": 1176, "seek": 559210, "start": 5602.620000000001, "end": 5604.42, "text": " Really much fast AI stuff at all", "tokens": [4083, 709, 2370, 7318, 1507, 412, 439], "temperature": 0.0, "avg_logprob": -0.16818561553955078, "compression_ratio": 1.8074074074074074, "no_speech_prob": 3.611961574279121e-06}, {"id": 1177, "seek": 559210, "start": 5604.42, "end": 5608.780000000001, "text": " I'm using fast AI stuff just to save me fiddling around with data loaders and data sets and stuff", "tokens": [286, 478, 1228, 2370, 7318, 1507, 445, 281, 3155, 385, 283, 14273, 1688, 926, 365, 1412, 3677, 433, 293, 1412, 6352, 293, 1507], "temperature": 0.0, "avg_logprob": -0.16818561553955078, "compression_ratio": 1.8074074074074074, "no_speech_prob": 3.611961574279121e-06}, {"id": 1178, "seek": 559210, "start": 5608.820000000001, "end": 5613.740000000001, "text": " But I'm actually going to create a standard pie torch model. I'm not going to create a learner", "tokens": [583, 286, 478, 767, 516, 281, 1884, 257, 3832, 1730, 27822, 2316, 13, 286, 478, 406, 516, 281, 1884, 257, 33347], "temperature": 0.0, "avg_logprob": -0.16818561553955078, "compression_ratio": 1.8074074074074074, "no_speech_prob": 3.611961574279121e-06}, {"id": 1179, "seek": 559210, "start": 5614.26, "end": 5619.14, "text": " Okay, so this is a standard pie torch model and because I'm using pie torch that means I have to remember to write kuda", "tokens": [1033, 11, 370, 341, 307, 257, 3832, 1730, 27822, 2316, 293, 570, 286, 478, 1228, 1730, 27822, 300, 1355, 286, 362, 281, 1604, 281, 2464, 350, 11152], "temperature": 0.0, "avg_logprob": -0.16818561553955078, "compression_ratio": 1.8074074074074074, "no_speech_prob": 3.611961574279121e-06}, {"id": 1180, "seek": 561914, "start": 5619.14, "end": 5621.26, "text": " Okay, let's take it on the GPU", "tokens": [1033, 11, 718, 311, 747, 309, 322, 264, 18407], "temperature": 0.0, "avg_logprob": -0.25289116692297237, "compression_ratio": 1.642156862745098, "no_speech_prob": 1.4593734931622748e-06}, {"id": 1181, "seek": 561914, "start": 5623.26, "end": 5625.26, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.25289116692297237, "compression_ratio": 1.642156862745098, "no_speech_prob": 1.4593734931622748e-06}, {"id": 1182, "seek": 561914, "start": 5627.660000000001, "end": 5631.700000000001, "text": " Here is how we can look inside at what's going on right so we can say it a", "tokens": [1692, 307, 577, 321, 393, 574, 1854, 412, 437, 311, 516, 322, 558, 370, 321, 393, 584, 309, 257], "temperature": 0.0, "avg_logprob": -0.25289116692297237, "compression_ratio": 1.642156862745098, "no_speech_prob": 1.4593734931622748e-06}, {"id": 1183, "seek": 561914, "start": 5631.9400000000005, "end": 5637.34, "text": " MD dot train data loader to grab the iterator to iterate through the training set", "tokens": [22521, 5893, 3847, 1412, 3677, 260, 281, 4444, 264, 17138, 1639, 281, 44497, 807, 264, 3097, 992], "temperature": 0.0, "avg_logprob": -0.25289116692297237, "compression_ratio": 1.642156862745098, "no_speech_prob": 1.4593734931622748e-06}, {"id": 1184, "seek": 561914, "start": 5637.780000000001, "end": 5642.200000000001, "text": " We can then call next on that to grab a mini batch, and that's going to return", "tokens": [492, 393, 550, 818, 958, 322, 300, 281, 4444, 257, 8382, 15245, 11, 293, 300, 311, 516, 281, 2736], "temperature": 0.0, "avg_logprob": -0.25289116692297237, "compression_ratio": 1.642156862745098, "no_speech_prob": 1.4593734931622748e-06}, {"id": 1185, "seek": 561914, "start": 5642.860000000001, "end": 5648.3, "text": " All of our X's and our Y tensor and so we can then take a look at", "tokens": [1057, 295, 527, 1783, 311, 293, 527, 398, 40863, 293, 370, 321, 393, 550, 747, 257, 574, 412], "temperature": 0.0, "avg_logprob": -0.25289116692297237, "compression_ratio": 1.642156862745098, "no_speech_prob": 1.4593734931622748e-06}, {"id": 1186, "seek": 564830, "start": 5648.3, "end": 5651.3, "text": " You know here's our X's for example", "tokens": [509, 458, 510, 311, 527, 1783, 311, 337, 1365], "temperature": 0.0, "avg_logprob": -0.22960278391838074, "compression_ratio": 1.5209580838323353, "no_speech_prob": 2.642580511746928e-06}, {"id": 1187, "seek": 564830, "start": 5653.22, "end": 5657.860000000001, "text": " All right, and so you would expect have a think about what you would expect for this length", "tokens": [1057, 558, 11, 293, 370, 291, 576, 2066, 362, 257, 519, 466, 437, 291, 576, 2066, 337, 341, 4641], "temperature": 0.0, "avg_logprob": -0.22960278391838074, "compression_ratio": 1.5209580838323353, "no_speech_prob": 2.642580511746928e-06}, {"id": 1188, "seek": 564830, "start": 5660.38, "end": 5666.06, "text": " Three not surprisingly because these are the three things okay, and so then excess", "tokens": [6244, 406, 17600, 570, 613, 366, 264, 1045, 721, 1392, 11, 293, 370, 550, 9310], "temperature": 0.0, "avg_logprob": -0.22960278391838074, "compression_ratio": 1.5209580838323353, "no_speech_prob": 2.642580511746928e-06}, {"id": 1189, "seek": 564830, "start": 5668.14, "end": 5670.14, "text": " Zero", "tokens": [17182], "temperature": 0.0, "avg_logprob": -0.22960278391838074, "compression_ratio": 1.5209580838323353, "no_speech_prob": 2.642580511746928e-06}, {"id": 1190, "seek": 564830, "start": 5670.860000000001, "end": 5674.1, "text": " Not surprisingly okay is of length 512", "tokens": [1726, 17600, 1392, 307, 295, 4641, 1025, 4762], "temperature": 0.0, "avg_logprob": -0.22960278391838074, "compression_ratio": 1.5209580838323353, "no_speech_prob": 2.642580511746928e-06}, {"id": 1191, "seek": 567410, "start": 5674.1, "end": 5679.54, "text": " And it's not actually one hot encoded because we use an embedding to pretend it is", "tokens": [400, 309, 311, 406, 767, 472, 2368, 2058, 12340, 570, 321, 764, 364, 12240, 3584, 281, 11865, 309, 307], "temperature": 0.0, "avg_logprob": -0.17538476793953542, "compression_ratio": 1.5116279069767442, "no_speech_prob": 5.539165499612864e-07}, {"id": 1192, "seek": 567410, "start": 5680.02, "end": 5683.38, "text": " Okay, and so then we can use a model as if it's a function", "tokens": [1033, 11, 293, 370, 550, 321, 393, 764, 257, 2316, 382, 498, 309, 311, 257, 2445], "temperature": 0.0, "avg_logprob": -0.17538476793953542, "compression_ratio": 1.5116279069767442, "no_speech_prob": 5.539165499612864e-07}, {"id": 1193, "seek": 567410, "start": 5684.02, "end": 5686.02, "text": " Okay by passing to it", "tokens": [1033, 538, 8437, 281, 309], "temperature": 0.0, "avg_logprob": -0.17538476793953542, "compression_ratio": 1.5116279069767442, "no_speech_prob": 5.539165499612864e-07}, {"id": 1194, "seek": 567410, "start": 5686.620000000001, "end": 5689.34, "text": " the variable eyes version of our tensors and", "tokens": [264, 7006, 2575, 3037, 295, 527, 10688, 830, 293], "temperature": 0.0, "avg_logprob": -0.17538476793953542, "compression_ratio": 1.5116279069767442, "no_speech_prob": 5.539165499612864e-07}, {"id": 1195, "seek": 567410, "start": 5690.46, "end": 5693.22, "text": " So have a think about what you would expect to be returned here", "tokens": [407, 362, 257, 519, 466, 437, 291, 576, 2066, 281, 312, 8752, 510], "temperature": 0.0, "avg_logprob": -0.17538476793953542, "compression_ratio": 1.5116279069767442, "no_speech_prob": 5.539165499612864e-07}, {"id": 1196, "seek": 567410, "start": 5694.900000000001, "end": 5698.34, "text": " Okay, so not surprisingly we had a mini batch of 512", "tokens": [1033, 11, 370, 406, 17600, 321, 632, 257, 8382, 15245, 295, 1025, 4762], "temperature": 0.0, "avg_logprob": -0.17538476793953542, "compression_ratio": 1.5116279069767442, "no_speech_prob": 5.539165499612864e-07}, {"id": 1197, "seek": 569834, "start": 5698.34, "end": 5705.18, "text": " So we still have 512 and then 85 is the probability of each of the possible vocab items and of course", "tokens": [407, 321, 920, 362, 1025, 4762, 293, 550, 14695, 307, 264, 8482, 295, 1184, 295, 264, 1944, 2329, 455, 4754, 293, 295, 1164], "temperature": 0.0, "avg_logprob": -0.15865682899405104, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.4465674616512842e-06}, {"id": 1198, "seek": 569834, "start": 5705.18, "end": 5708.66, "text": " We've got the log of them because that's kind of what we do in pytorch", "tokens": [492, 600, 658, 264, 3565, 295, 552, 570, 300, 311, 733, 295, 437, 321, 360, 294, 25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.15865682899405104, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.4465674616512842e-06}, {"id": 1199, "seek": 569834, "start": 5708.78, "end": 5712.900000000001, "text": " Okay, you can see here the softmax all right, so that's how you can look inside", "tokens": [1033, 11, 291, 393, 536, 510, 264, 2787, 41167, 439, 558, 11, 370, 300, 311, 577, 291, 393, 574, 1854], "temperature": 0.0, "avg_logprob": -0.15865682899405104, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.4465674616512842e-06}, {"id": 1200, "seek": 569834, "start": 5713.42, "end": 5717.06, "text": " Right so you can see here how to do everything really very much by hand", "tokens": [1779, 370, 291, 393, 536, 510, 577, 281, 360, 1203, 534, 588, 709, 538, 1011], "temperature": 0.0, "avg_logprob": -0.15865682899405104, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.4465674616512842e-06}, {"id": 1201, "seek": 569834, "start": 5718.62, "end": 5720.62, "text": " So we can create an optimizer", "tokens": [407, 321, 393, 1884, 364, 5028, 6545], "temperature": 0.0, "avg_logprob": -0.15865682899405104, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.4465674616512842e-06}, {"id": 1202, "seek": 569834, "start": 5720.78, "end": 5723.92, "text": " Again using standard pytorch so with pytorch", "tokens": [3764, 1228, 3832, 25878, 284, 339, 370, 365, 25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.15865682899405104, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.4465674616512842e-06}, {"id": 1203, "seek": 572392, "start": 5723.92, "end": 5730.08, "text": " We use a pytorch optimizer you have to pass in a list of the things to optimize and so if you call", "tokens": [492, 764, 257, 25878, 284, 339, 5028, 6545, 291, 362, 281, 1320, 294, 257, 1329, 295, 264, 721, 281, 19719, 293, 370, 498, 291, 818], "temperature": 0.0, "avg_logprob": -0.19840723329836185, "compression_ratio": 1.6945606694560669, "no_speech_prob": 2.684191713342443e-06}, {"id": 1204, "seek": 572392, "start": 5730.6, "end": 5732.84, "text": " M.parameters that will return that list for you", "tokens": [376, 13, 2181, 335, 6202, 300, 486, 2736, 300, 1329, 337, 291], "temperature": 0.0, "avg_logprob": -0.19840723329836185, "compression_ratio": 1.6945606694560669, "no_speech_prob": 2.684191713342443e-06}, {"id": 1205, "seek": 572392, "start": 5733.8, "end": 5735.8, "text": " And then we can fit", "tokens": [400, 550, 321, 393, 3318], "temperature": 0.0, "avg_logprob": -0.19840723329836185, "compression_ratio": 1.6945606694560669, "no_speech_prob": 2.684191713342443e-06}, {"id": 1206, "seek": 572392, "start": 5736.72, "end": 5738.72, "text": " And there it goes okay", "tokens": [400, 456, 309, 1709, 1392], "temperature": 0.0, "avg_logprob": -0.19840723329836185, "compression_ratio": 1.6945606694560669, "no_speech_prob": 2.684191713342443e-06}, {"id": 1207, "seek": 572392, "start": 5739.28, "end": 5740.8, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.19840723329836185, "compression_ratio": 1.6945606694560669, "no_speech_prob": 2.684191713342443e-06}, {"id": 1208, "seek": 572392, "start": 5740.8, "end": 5742.8, "text": " So we don't have", "tokens": [407, 321, 500, 380, 362], "temperature": 0.0, "avg_logprob": -0.19840723329836185, "compression_ratio": 1.6945606694560669, "no_speech_prob": 2.684191713342443e-06}, {"id": 1209, "seek": 572392, "start": 5743.24, "end": 5747.16, "text": " Learning rate finders and SGDR and all that stuff because we're not using a learner", "tokens": [15205, 3314, 915, 433, 293, 34520, 9301, 293, 439, 300, 1507, 570, 321, 434, 406, 1228, 257, 33347], "temperature": 0.0, "avg_logprob": -0.19840723329836185, "compression_ratio": 1.6945606694560669, "no_speech_prob": 2.684191713342443e-06}, {"id": 1210, "seek": 574716, "start": 5747.16, "end": 5754.32, "text": " So we'll have to manually do learning rate annealing so set the learning rate a little bit lower and fit again okay?", "tokens": [407, 321, 603, 362, 281, 16945, 360, 2539, 3314, 22256, 4270, 370, 992, 264, 2539, 3314, 257, 707, 857, 3126, 293, 3318, 797, 1392, 30], "temperature": 0.0, "avg_logprob": -0.20644375937325612, "compression_ratio": 1.542857142857143, "no_speech_prob": 9.874622719507897e-07}, {"id": 1211, "seek": 574716, "start": 5756.5199999999995, "end": 5764.4, "text": " Okay, and so now we can write a little function to to test this thing out okay, so", "tokens": [1033, 11, 293, 370, 586, 321, 393, 2464, 257, 707, 2445, 281, 281, 1500, 341, 551, 484, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.20644375937325612, "compression_ratio": 1.542857142857143, "no_speech_prob": 9.874622719507897e-07}, {"id": 1212, "seek": 574716, "start": 5767.0, "end": 5769.0, "text": " Here's something called get next", "tokens": [1692, 311, 746, 1219, 483, 958], "temperature": 0.0, "avg_logprob": -0.20644375937325612, "compression_ratio": 1.542857142857143, "no_speech_prob": 9.874622719507897e-07}, {"id": 1213, "seek": 574716, "start": 5770.12, "end": 5772.12, "text": " Where we can pass in", "tokens": [2305, 321, 393, 1320, 294], "temperature": 0.0, "avg_logprob": -0.20644375937325612, "compression_ratio": 1.542857142857143, "no_speech_prob": 9.874622719507897e-07}, {"id": 1214, "seek": 574716, "start": 5772.4, "end": 5774.0, "text": " three characters", "tokens": [1045, 4342], "temperature": 0.0, "avg_logprob": -0.20644375937325612, "compression_ratio": 1.542857142857143, "no_speech_prob": 9.874622719507897e-07}, {"id": 1215, "seek": 577400, "start": 5774.0, "end": 5780.68, "text": " Like y full stop space right and so I can then go through and turn that into a tensor with capital T", "tokens": [1743, 288, 1577, 1590, 1901, 558, 293, 370, 286, 393, 550, 352, 807, 293, 1261, 300, 666, 257, 40863, 365, 4238, 314], "temperature": 0.0, "avg_logprob": -0.1813068506194324, "compression_ratio": 1.7258883248730965, "no_speech_prob": 4.052542692534189e-07}, {"id": 1216, "seek": 577400, "start": 5781.24, "end": 5783.24, "text": " of an array of", "tokens": [295, 364, 10225, 295], "temperature": 0.0, "avg_logprob": -0.1813068506194324, "compression_ratio": 1.7258883248730965, "no_speech_prob": 4.052542692534189e-07}, {"id": 1217, "seek": 577400, "start": 5783.72, "end": 5788.76, "text": " The character index for each character in that list so basically turn those into the integers", "tokens": [440, 2517, 8186, 337, 1184, 2517, 294, 300, 1329, 370, 1936, 1261, 729, 666, 264, 41674], "temperature": 0.0, "avg_logprob": -0.1813068506194324, "compression_ratio": 1.7258883248730965, "no_speech_prob": 4.052542692534189e-07}, {"id": 1218, "seek": 577400, "start": 5789.36, "end": 5792.64, "text": " Turn those into variables pass that to our model", "tokens": [7956, 729, 666, 9102, 1320, 300, 281, 527, 2316], "temperature": 0.0, "avg_logprob": -0.1813068506194324, "compression_ratio": 1.7258883248730965, "no_speech_prob": 4.052542692534189e-07}, {"id": 1219, "seek": 577400, "start": 5793.52, "end": 5799.38, "text": " Right and then we can do an arg max on that to grab which character number is it?", "tokens": [1779, 293, 550, 321, 393, 360, 364, 3882, 11469, 322, 300, 281, 4444, 597, 2517, 1230, 307, 309, 30], "temperature": 0.0, "avg_logprob": -0.1813068506194324, "compression_ratio": 1.7258883248730965, "no_speech_prob": 4.052542692534189e-07}, {"id": 1220, "seek": 579938, "start": 5799.38, "end": 5805.74, "text": " And in order to do stuff in numpy land like I use to NP to turn that variable into a numpy array", "tokens": [400, 294, 1668, 281, 360, 1507, 294, 1031, 8200, 2117, 411, 286, 764, 281, 38611, 281, 1261, 300, 7006, 666, 257, 1031, 8200, 10225], "temperature": 0.0, "avg_logprob": -0.23809119371267465, "compression_ratio": 1.7836734693877552, "no_speech_prob": 1.9637986952147912e-06}, {"id": 1221, "seek": 579938, "start": 5805.86, "end": 5809.76, "text": " Right and then I can return that character and so for example a capital T", "tokens": [1779, 293, 550, 286, 393, 2736, 300, 2517, 293, 370, 337, 1365, 257, 4238, 314], "temperature": 0.0, "avg_logprob": -0.23809119371267465, "compression_ratio": 1.7836734693877552, "no_speech_prob": 1.9637986952147912e-06}, {"id": 1222, "seek": 579938, "start": 5809.9400000000005, "end": 5816.78, "text": " Was what it thinks would be reasonable after seeing why full stop space that seems like a very reasonable way to start a sentence", "tokens": [3027, 437, 309, 7309, 576, 312, 10585, 934, 2577, 983, 1577, 1590, 1901, 300, 2544, 411, 257, 588, 10585, 636, 281, 722, 257, 8174], "temperature": 0.0, "avg_logprob": -0.23809119371267465, "compression_ratio": 1.7836734693877552, "no_speech_prob": 1.9637986952147912e-06}, {"id": 1223, "seek": 579938, "start": 5817.9400000000005, "end": 5825.46, "text": " If it was PPL a that sounds reasonable space th e that's bounce reasonable a and D space that sounds reasonable", "tokens": [759, 309, 390, 430, 21593, 257, 300, 3263, 10585, 1901, 258, 308, 300, 311, 15894, 10585, 257, 293, 413, 1901, 300, 3263, 10585], "temperature": 0.0, "avg_logprob": -0.23809119371267465, "compression_ratio": 1.7836734693877552, "no_speech_prob": 1.9637986952147912e-06}, {"id": 1224, "seek": 579938, "start": 5825.58, "end": 5827.54, "text": " So it seems to have like", "tokens": [407, 309, 2544, 281, 362, 411], "temperature": 0.0, "avg_logprob": -0.23809119371267465, "compression_ratio": 1.7836734693877552, "no_speech_prob": 1.9637986952147912e-06}, {"id": 1225, "seek": 582754, "start": 5827.54, "end": 5829.46, "text": " created something", "tokens": [2942, 746], "temperature": 0.0, "avg_logprob": -0.22757840474446614, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.276698627516453e-07}, {"id": 1226, "seek": 582754, "start": 5829.46, "end": 5834.06, "text": " Sensible right so you know the important thing to note here is our", "tokens": [318, 30633, 558, 370, 291, 458, 264, 1021, 551, 281, 3637, 510, 307, 527], "temperature": 0.0, "avg_logprob": -0.22757840474446614, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.276698627516453e-07}, {"id": 1227, "seek": 582754, "start": 5835.5, "end": 5839.26, "text": " Character model is a totally standard", "tokens": [36786, 2316, 307, 257, 3879, 3832], "temperature": 0.0, "avg_logprob": -0.22757840474446614, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.276698627516453e-07}, {"id": 1228, "seek": 582754, "start": 5840.7, "end": 5847.54, "text": " Fully connected model right the only slightly interesting thing we did was to kind of do this", "tokens": [479, 2150, 4582, 2316, 558, 264, 787, 4748, 1880, 551, 321, 630, 390, 281, 733, 295, 360, 341], "temperature": 0.0, "avg_logprob": -0.22757840474446614, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.276698627516453e-07}, {"id": 1229, "seek": 582754, "start": 5848.38, "end": 5849.78, "text": " addition of", "tokens": [4500, 295], "temperature": 0.0, "avg_logprob": -0.22757840474446614, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.276698627516453e-07}, {"id": 1230, "seek": 582754, "start": 5849.78, "end": 5851.38, "text": " each of the", "tokens": [1184, 295, 264], "temperature": 0.0, "avg_logprob": -0.22757840474446614, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.276698627516453e-07}, {"id": 1231, "seek": 582754, "start": 5851.38, "end": 5853.38, "text": " inputs one at a time", "tokens": [15743, 472, 412, 257, 565], "temperature": 0.0, "avg_logprob": -0.22757840474446614, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.276698627516453e-07}, {"id": 1232, "seek": 582754, "start": 5853.94, "end": 5855.34, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.22757840474446614, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.276698627516453e-07}, {"id": 1233, "seek": 582754, "start": 5855.34, "end": 5857.0199999999995, "text": " But there's nothing", "tokens": [583, 456, 311, 1825], "temperature": 0.0, "avg_logprob": -0.22757840474446614, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.276698627516453e-07}, {"id": 1234, "seek": 585702, "start": 5857.02, "end": 5860.5, "text": " New conceptually here. We're training it in the usual way", "tokens": [1873, 3410, 671, 510, 13, 492, 434, 3097, 309, 294, 264, 7713, 636], "temperature": 0.0, "avg_logprob": -0.2016002243640376, "compression_ratio": 1.248, "no_speech_prob": 1.9333542695676442e-06}, {"id": 1235, "seek": 585702, "start": 5863.46, "end": 5867.900000000001, "text": " All right, let's now create an RNN", "tokens": [1057, 558, 11, 718, 311, 586, 1884, 364, 45702, 45], "temperature": 0.0, "avg_logprob": -0.2016002243640376, "compression_ratio": 1.248, "no_speech_prob": 1.9333542695676442e-06}, {"id": 1236, "seek": 585702, "start": 5869.9800000000005, "end": 5874.22, "text": " So an RNN is when we do", "tokens": [407, 364, 45702, 45, 307, 562, 321, 360], "temperature": 0.0, "avg_logprob": -0.2016002243640376, "compression_ratio": 1.248, "no_speech_prob": 1.9333542695676442e-06}, {"id": 1237, "seek": 585702, "start": 5878.820000000001, "end": 5880.820000000001, "text": " Exactly the same thing", "tokens": [7587, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.2016002243640376, "compression_ratio": 1.248, "no_speech_prob": 1.9333542695676442e-06}, {"id": 1238, "seek": 585702, "start": 5881.06, "end": 5882.820000000001, "text": " That we did here", "tokens": [663, 321, 630, 510], "temperature": 0.0, "avg_logprob": -0.2016002243640376, "compression_ratio": 1.248, "no_speech_prob": 1.9333542695676442e-06}, {"id": 1239, "seek": 588282, "start": 5882.82, "end": 5888.66, "text": " Right, but I could draw this more simply by saying you know what if we've got a green arrow going to a circle", "tokens": [1779, 11, 457, 286, 727, 2642, 341, 544, 2935, 538, 1566, 291, 458, 437, 498, 321, 600, 658, 257, 3092, 11610, 516, 281, 257, 6329], "temperature": 0.0, "avg_logprob": -0.15240812764584438, "compression_ratio": 2.0761421319796955, "no_speech_prob": 1.9637984678411158e-06}, {"id": 1240, "seek": 588282, "start": 5889.42, "end": 5893.219999999999, "text": " Let's not draw a green arrow going to a circle again and again and again", "tokens": [961, 311, 406, 2642, 257, 3092, 11610, 516, 281, 257, 6329, 797, 293, 797, 293, 797], "temperature": 0.0, "avg_logprob": -0.15240812764584438, "compression_ratio": 2.0761421319796955, "no_speech_prob": 1.9637984678411158e-06}, {"id": 1241, "seek": 588282, "start": 5893.86, "end": 5900.08, "text": " But let's just draw it like this green arrow going to a circle right and rather than drawing an orange arrow going to a circle", "tokens": [583, 718, 311, 445, 2642, 309, 411, 341, 3092, 11610, 516, 281, 257, 6329, 558, 293, 2831, 813, 6316, 364, 7671, 11610, 516, 281, 257, 6329], "temperature": 0.0, "avg_logprob": -0.15240812764584438, "compression_ratio": 2.0761421319796955, "no_speech_prob": 1.9637984678411158e-06}, {"id": 1242, "seek": 588282, "start": 5900.08, "end": 5902.08, "text": " Let's just draw it like this", "tokens": [961, 311, 445, 2642, 309, 411, 341], "temperature": 0.0, "avg_logprob": -0.15240812764584438, "compression_ratio": 2.0761421319796955, "no_speech_prob": 1.9637984678411158e-06}, {"id": 1243, "seek": 588282, "start": 5903.139999999999, "end": 5905.259999999999, "text": " Okay, so this is the same picture", "tokens": [1033, 11, 370, 341, 307, 264, 912, 3036], "temperature": 0.0, "avg_logprob": -0.15240812764584438, "compression_ratio": 2.0761421319796955, "no_speech_prob": 1.9637984678411158e-06}, {"id": 1244, "seek": 588282, "start": 5906.54, "end": 5909.259999999999, "text": " Exactly the same picture as this one", "tokens": [7587, 264, 912, 3036, 382, 341, 472], "temperature": 0.0, "avg_logprob": -0.15240812764584438, "compression_ratio": 2.0761421319796955, "no_speech_prob": 1.9637984678411158e-06}, {"id": 1245, "seek": 590926, "start": 5909.26, "end": 5911.780000000001, "text": " All right", "tokens": [1057, 558], "temperature": 0.0, "avg_logprob": -0.17680174050871858, "compression_ratio": 1.7663934426229508, "no_speech_prob": 8.059428182605188e-07}, {"id": 1246, "seek": 590926, "start": 5911.780000000001, "end": 5916.62, "text": " And so you just have to say how many times to go around this circle right so in this case if we want to predict", "tokens": [400, 370, 291, 445, 362, 281, 584, 577, 867, 1413, 281, 352, 926, 341, 6329, 558, 370, 294, 341, 1389, 498, 321, 528, 281, 6069], "temperature": 0.0, "avg_logprob": -0.17680174050871858, "compression_ratio": 1.7663934426229508, "no_speech_prob": 8.059428182605188e-07}, {"id": 1247, "seek": 590926, "start": 5916.62, "end": 5923.38, "text": " Character number n from characters 1 through n minus 1 then we can take the character 1 import get some activations", "tokens": [36786, 1230, 297, 490, 4342, 502, 807, 297, 3175, 502, 550, 321, 393, 747, 264, 2517, 502, 974, 483, 512, 2430, 763], "temperature": 0.0, "avg_logprob": -0.17680174050871858, "compression_ratio": 1.7663934426229508, "no_speech_prob": 8.059428182605188e-07}, {"id": 1248, "seek": 590926, "start": 5923.7, "end": 5929.74, "text": " Feed that to some new activations that go through remember orange is the hidden to hidden weight matrix", "tokens": [33720, 300, 281, 512, 777, 2430, 763, 300, 352, 807, 1604, 7671, 307, 264, 7633, 281, 7633, 3364, 8141], "temperature": 0.0, "avg_logprob": -0.17680174050871858, "compression_ratio": 1.7663934426229508, "no_speech_prob": 8.059428182605188e-07}, {"id": 1249, "seek": 590926, "start": 5930.22, "end": 5934.9400000000005, "text": " Right and each time will also bring in the next character of input through its embeddings", "tokens": [1779, 293, 1184, 565, 486, 611, 1565, 294, 264, 958, 2517, 295, 4846, 807, 1080, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.17680174050871858, "compression_ratio": 1.7663934426229508, "no_speech_prob": 8.059428182605188e-07}, {"id": 1250, "seek": 593494, "start": 5934.94, "end": 5942.339999999999, "text": " Okay, so that picture and that picture are two ways of writing the same thing", "tokens": [1033, 11, 370, 300, 3036, 293, 300, 3036, 366, 732, 2098, 295, 3579, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.14274952920635095, "compression_ratio": 1.665137614678899, "no_speech_prob": 1.3081744327791966e-06}, {"id": 1251, "seek": 593494, "start": 5943.419999999999, "end": 5947.599999999999, "text": " But this one is more flexible because rather than me having to say hey, let's do it for eight", "tokens": [583, 341, 472, 307, 544, 11358, 570, 2831, 813, 385, 1419, 281, 584, 4177, 11, 718, 311, 360, 309, 337, 3180], "temperature": 0.0, "avg_logprob": -0.14274952920635095, "compression_ratio": 1.665137614678899, "no_speech_prob": 1.3081744327791966e-06}, {"id": 1252, "seek": 593494, "start": 5947.599999999999, "end": 5949.599999999999, "text": " I don't have to draw eight circles", "tokens": [286, 500, 380, 362, 281, 2642, 3180, 13040], "temperature": 0.0, "avg_logprob": -0.14274952920635095, "compression_ratio": 1.665137614678899, "no_speech_prob": 1.3081744327791966e-06}, {"id": 1253, "seek": 593494, "start": 5949.82, "end": 5951.82, "text": " But I can just say oh just repeat this", "tokens": [583, 286, 393, 445, 584, 1954, 445, 7149, 341], "temperature": 0.0, "avg_logprob": -0.14274952920635095, "compression_ratio": 1.665137614678899, "no_speech_prob": 1.3081744327791966e-06}, {"id": 1254, "seek": 593494, "start": 5955.78, "end": 5962.94, "text": " So I could simplify this a little bit further by saying you know what rather than having this thing as a special case", "tokens": [407, 286, 727, 20460, 341, 257, 707, 857, 3052, 538, 1566, 291, 458, 437, 2831, 813, 1419, 341, 551, 382, 257, 2121, 1389], "temperature": 0.0, "avg_logprob": -0.14274952920635095, "compression_ratio": 1.665137614678899, "no_speech_prob": 1.3081744327791966e-06}, {"id": 1255, "seek": 596294, "start": 5962.94, "end": 5969.139999999999, "text": " Let's actually start out with a bunch of zeros right and then let's have all of our characters", "tokens": [961, 311, 767, 722, 484, 365, 257, 3840, 295, 35193, 558, 293, 550, 718, 311, 362, 439, 295, 527, 4342], "temperature": 0.0, "avg_logprob": -0.2762883284996296, "compression_ratio": 1.6064814814814814, "no_speech_prob": 6.643369033554336e-06}, {"id": 1256, "seek": 596294, "start": 5970.54, "end": 5972.54, "text": " Inside here yes", "tokens": [15123, 510, 2086], "temperature": 0.0, "avg_logprob": -0.2762883284996296, "compression_ratio": 1.6064814814814814, "no_speech_prob": 6.643369033554336e-06}, {"id": 1257, "seek": 596294, "start": 5976.74, "end": 5983.66, "text": " Yeah, so I was wondering if you can explain a little bit better. Why are you reusing those?", "tokens": [865, 11, 370, 286, 390, 6359, 498, 291, 393, 2903, 257, 707, 857, 1101, 13, 1545, 366, 291, 319, 7981, 729, 30], "temperature": 0.0, "avg_logprob": -0.2762883284996296, "compression_ratio": 1.6064814814814814, "no_speech_prob": 6.643369033554336e-06}, {"id": 1258, "seek": 596294, "start": 5984.78, "end": 5992.04, "text": " Why use the same color arrows? They're same. Yeah, where are you you're kind of seem to be reusing the same same weight matrices weight matrices", "tokens": [1545, 764, 264, 912, 2017, 19669, 30, 814, 434, 912, 13, 865, 11, 689, 366, 291, 291, 434, 733, 295, 1643, 281, 312, 319, 7981, 264, 912, 912, 3364, 32284, 3364, 32284], "temperature": 0.0, "avg_logprob": -0.2762883284996296, "compression_ratio": 1.6064814814814814, "no_speech_prob": 6.643369033554336e-06}, {"id": 1259, "seek": 599204, "start": 5992.04, "end": 5997.8, "text": " Yeah, maybe this is kind of similar to what we did in convolutional your nets like it's somehow", "tokens": [865, 11, 1310, 341, 307, 733, 295, 2531, 281, 437, 321, 630, 294, 45216, 304, 428, 36170, 411, 309, 311, 6063], "temperature": 0.0, "avg_logprob": -0.23753647577194942, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.495149369176943e-06}, {"id": 1260, "seek": 599204, "start": 5998.6, "end": 6002.92, "text": " No, I don't think so at least not that I can see so the idea is just", "tokens": [883, 11, 286, 500, 380, 519, 370, 412, 1935, 406, 300, 286, 393, 536, 370, 264, 1558, 307, 445], "temperature": 0.0, "avg_logprob": -0.23753647577194942, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.495149369176943e-06}, {"id": 1261, "seek": 599204, "start": 6003.88, "end": 6006.2, "text": " kind of semantically speaking like", "tokens": [733, 295, 4361, 49505, 4124, 411], "temperature": 0.0, "avg_logprob": -0.23753647577194942, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.495149369176943e-06}, {"id": 1262, "seek": 599204, "start": 6007.08, "end": 6009.08, "text": " this arrow here", "tokens": [341, 11610, 510], "temperature": 0.0, "avg_logprob": -0.23753647577194942, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.495149369176943e-06}, {"id": 1263, "seek": 599204, "start": 6010.12, "end": 6011.76, "text": " this", "tokens": [341], "temperature": 0.0, "avg_logprob": -0.23753647577194942, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.495149369176943e-06}, {"id": 1264, "seek": 599204, "start": 6011.76, "end": 6013.76, "text": " this arrow here is saying", "tokens": [341, 11610, 510, 307, 1566], "temperature": 0.0, "avg_logprob": -0.23753647577194942, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.495149369176943e-06}, {"id": 1265, "seek": 599204, "start": 6015.08, "end": 6016.88, "text": " take a", "tokens": [747, 257], "temperature": 0.0, "avg_logprob": -0.23753647577194942, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.495149369176943e-06}, {"id": 1266, "seek": 599204, "start": 6016.88, "end": 6018.88, "text": " character of import and", "tokens": [2517, 295, 974, 293], "temperature": 0.0, "avg_logprob": -0.23753647577194942, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.495149369176943e-06}, {"id": 1267, "seek": 599204, "start": 6019.16, "end": 6021.16, "text": " represented as some", "tokens": [10379, 382, 512], "temperature": 0.0, "avg_logprob": -0.23753647577194942, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.495149369176943e-06}, {"id": 1268, "seek": 602116, "start": 6021.16, "end": 6023.16, "text": " So some set of features", "tokens": [407, 512, 992, 295, 4122], "temperature": 0.0, "avg_logprob": -0.18987646768259447, "compression_ratio": 1.945, "no_speech_prob": 1.2289157211853308e-06}, {"id": 1269, "seek": 602116, "start": 6023.32, "end": 6028.4, "text": " Right and this arrow is saying the same thing take some character and represent as a set of features and so is this one", "tokens": [1779, 293, 341, 11610, 307, 1566, 264, 912, 551, 747, 512, 2517, 293, 2906, 382, 257, 992, 295, 4122, 293, 370, 307, 341, 472], "temperature": 0.0, "avg_logprob": -0.18987646768259447, "compression_ratio": 1.945, "no_speech_prob": 1.2289157211853308e-06}, {"id": 1270, "seek": 602116, "start": 6028.4, "end": 6030.4, "text": " right so like", "tokens": [558, 370, 411], "temperature": 0.0, "avg_logprob": -0.18987646768259447, "compression_ratio": 1.945, "no_speech_prob": 1.2289157211853308e-06}, {"id": 1271, "seek": 602116, "start": 6030.68, "end": 6035.16, "text": " Why would the three be represented with different weight matrices because it's all doing the same thing", "tokens": [1545, 576, 264, 1045, 312, 10379, 365, 819, 3364, 32284, 570, 309, 311, 439, 884, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.18987646768259447, "compression_ratio": 1.945, "no_speech_prob": 1.2289157211853308e-06}, {"id": 1272, "seek": 602116, "start": 6035.599999999999, "end": 6038.4, "text": " Right and this orange arrow is saying", "tokens": [1779, 293, 341, 7671, 11610, 307, 1566], "temperature": 0.0, "avg_logprob": -0.18987646768259447, "compression_ratio": 1.945, "no_speech_prob": 1.2289157211853308e-06}, {"id": 1273, "seek": 602116, "start": 6040.32, "end": 6046.5599999999995, "text": " Kind of transition from character zeroes state to character one state characters to state", "tokens": [9242, 295, 6034, 490, 2517, 4018, 279, 1785, 281, 2517, 472, 1785, 4342, 281, 1785], "temperature": 0.0, "avg_logprob": -0.18987646768259447, "compression_ratio": 1.945, "no_speech_prob": 1.2289157211853308e-06}, {"id": 1274, "seek": 604656, "start": 6046.56, "end": 6053.4400000000005, "text": " Again, it's it's the same thing. It's like why would the transition from character zero to one be different to character from?", "tokens": [3764, 11, 309, 311, 309, 311, 264, 912, 551, 13, 467, 311, 411, 983, 576, 264, 6034, 490, 2517, 4018, 281, 472, 312, 819, 281, 2517, 490, 30], "temperature": 0.0, "avg_logprob": -0.2872383460569917, "compression_ratio": 1.6445497630331753, "no_speech_prob": 1.8448104128765408e-06}, {"id": 1275, "seek": 604656, "start": 6053.6, "end": 6055.96, "text": " Transition from one to two so the idea is like", "tokens": [6531, 849, 490, 472, 281, 732, 370, 264, 1558, 307, 411], "temperature": 0.0, "avg_logprob": -0.2872383460569917, "compression_ratio": 1.6445497630331753, "no_speech_prob": 1.8448104128765408e-06}, {"id": 1276, "seek": 604656, "start": 6057.52, "end": 6060.9800000000005, "text": " But is to like say hey if it's doing the same", "tokens": [583, 307, 281, 411, 584, 4177, 498, 309, 311, 884, 264, 912], "temperature": 0.0, "avg_logprob": -0.2872383460569917, "compression_ratio": 1.6445497630331753, "no_speech_prob": 1.8448104128765408e-06}, {"id": 1277, "seek": 604656, "start": 6062.120000000001, "end": 6064.92, "text": " Conceptual thing let's use the exact same", "tokens": [47482, 901, 551, 718, 311, 764, 264, 1900, 912], "temperature": 0.0, "avg_logprob": -0.2872383460569917, "compression_ratio": 1.6445497630331753, "no_speech_prob": 1.8448104128765408e-06}, {"id": 1278, "seek": 604656, "start": 6065.4400000000005, "end": 6070.96, "text": " Weight matrix my comment on convolutional networks is that a filter or so this apply?", "tokens": [44464, 8141, 452, 2871, 322, 45216, 304, 9590, 307, 300, 257, 6608, 420, 370, 341, 3079, 30], "temperature": 0.0, "avg_logprob": -0.2872383460569917, "compression_ratio": 1.6445497630331753, "no_speech_prob": 1.8448104128765408e-06}, {"id": 1279, "seek": 607096, "start": 6070.96, "end": 6076.16, "text": " Oh tomorrow places and we'll take this point of view yeah, I think so", "tokens": [876, 4153, 3190, 293, 321, 603, 747, 341, 935, 295, 1910, 1338, 11, 286, 519, 370], "temperature": 0.0, "avg_logprob": -0.23503569194248744, "compression_ratio": 1.6101083032490975, "no_speech_prob": 2.9944098969281185e-06}, {"id": 1280, "seek": 607096, "start": 6076.16, "end": 6082.28, "text": " You're saying like a convolution is almost like a kind of a special dot product with shared weights. Yeah, no, that's okay", "tokens": [509, 434, 1566, 411, 257, 45216, 307, 1920, 411, 257, 733, 295, 257, 2121, 5893, 1674, 365, 5507, 17443, 13, 865, 11, 572, 11, 300, 311, 1392], "temperature": 0.0, "avg_logprob": -0.23503569194248744, "compression_ratio": 1.6101083032490975, "no_speech_prob": 2.9944098969281185e-06}, {"id": 1281, "seek": 607096, "start": 6082.52, "end": 6084.84, "text": " That's very good point and in fact", "tokens": [663, 311, 588, 665, 935, 293, 294, 1186], "temperature": 0.0, "avg_logprob": -0.23503569194248744, "compression_ratio": 1.6101083032490975, "no_speech_prob": 2.9944098969281185e-06}, {"id": 1282, "seek": 607096, "start": 6085.4800000000005, "end": 6090.32, "text": " One of our students actually wrote a good blog post about that last year. We should dig that up, okay", "tokens": [1485, 295, 527, 1731, 767, 4114, 257, 665, 6968, 2183, 466, 300, 1036, 1064, 13, 492, 820, 2528, 300, 493, 11, 1392], "temperature": 0.0, "avg_logprob": -0.23503569194248744, "compression_ratio": 1.6101083032490975, "no_speech_prob": 2.9944098969281185e-06}, {"id": 1283, "seek": 607096, "start": 6090.32, "end": 6092.8, "text": " I totally see where you're coming from and I totally agree with you", "tokens": [286, 3879, 536, 689, 291, 434, 1348, 490, 293, 286, 3879, 3986, 365, 291], "temperature": 0.0, "avg_logprob": -0.23503569194248744, "compression_ratio": 1.6101083032490975, "no_speech_prob": 2.9944098969281185e-06}, {"id": 1284, "seek": 607096, "start": 6094.96, "end": 6098.04, "text": " All right, so let's let's implement this version", "tokens": [1057, 558, 11, 370, 718, 311, 718, 311, 4445, 341, 3037], "temperature": 0.0, "avg_logprob": -0.23503569194248744, "compression_ratio": 1.6101083032490975, "no_speech_prob": 2.9944098969281185e-06}, {"id": 1285, "seek": 609804, "start": 6098.04, "end": 6100.04, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.25932992471231, "compression_ratio": 1.5364583333333333, "no_speech_prob": 3.1381357530335663e-06}, {"id": 1286, "seek": 609804, "start": 6100.76, "end": 6105.44, "text": " This time we're going to do eight characters eight seas", "tokens": [639, 565, 321, 434, 516, 281, 360, 3180, 4342, 3180, 22535], "temperature": 0.0, "avg_logprob": -0.25932992471231, "compression_ratio": 1.5364583333333333, "no_speech_prob": 3.1381357530335663e-06}, {"id": 1287, "seek": 609804, "start": 6106.56, "end": 6110.6, "text": " Okay, and so let's create a list of every eighth character", "tokens": [1033, 11, 293, 370, 718, 311, 1884, 257, 1329, 295, 633, 19495, 2517], "temperature": 0.0, "avg_logprob": -0.25932992471231, "compression_ratio": 1.5364583333333333, "no_speech_prob": 3.1381357530335663e-06}, {"id": 1288, "seek": 609804, "start": 6112.08, "end": 6118.16, "text": " From zero through seven and then our outputs will be the next character and so we can stack that together", "tokens": [3358, 4018, 807, 3407, 293, 550, 527, 23930, 486, 312, 264, 958, 2517, 293, 370, 321, 393, 8630, 300, 1214], "temperature": 0.0, "avg_logprob": -0.25932992471231, "compression_ratio": 1.5364583333333333, "no_speech_prob": 3.1381357530335663e-06}, {"id": 1289, "seek": 609804, "start": 6118.16, "end": 6122.16, "text": " And so now we've got six hundred thousand by eight", "tokens": [400, 370, 586, 321, 600, 658, 2309, 3262, 4714, 538, 3180], "temperature": 0.0, "avg_logprob": -0.25932992471231, "compression_ratio": 1.5364583333333333, "no_speech_prob": 3.1381357530335663e-06}, {"id": 1290, "seek": 612216, "start": 6122.16, "end": 6125.92, "text": " Eight so here's an example", "tokens": [17708, 370, 510, 311, 364, 1365], "temperature": 0.0, "avg_logprob": -0.2222922228384709, "compression_ratio": 1.9483870967741936, "no_speech_prob": 8.446214110335859e-07}, {"id": 1291, "seek": 612216, "start": 6128.32, "end": 6134.0, "text": " So for example after this series of eight characters", "tokens": [407, 337, 1365, 934, 341, 2638, 295, 3180, 4342], "temperature": 0.0, "avg_logprob": -0.2222922228384709, "compression_ratio": 1.9483870967741936, "no_speech_prob": 8.446214110335859e-07}, {"id": 1292, "seek": 612216, "start": 6134.92, "end": 6142.2, "text": " Right so this is characters naught through eight. This is characters one through nine. This is two through ten. These are all overlapping", "tokens": [1779, 370, 341, 307, 4342, 13138, 807, 3180, 13, 639, 307, 4342, 472, 807, 4949, 13, 639, 307, 732, 807, 2064, 13, 1981, 366, 439, 33535], "temperature": 0.0, "avg_logprob": -0.2222922228384709, "compression_ratio": 1.9483870967741936, "no_speech_prob": 8.446214110335859e-07}, {"id": 1293, "seek": 612216, "start": 6143.16, "end": 6147.68, "text": " Okay, so after characters one naught through eight. This is going to be the next one", "tokens": [1033, 11, 370, 934, 4342, 472, 13138, 807, 3180, 13, 639, 307, 516, 281, 312, 264, 958, 472], "temperature": 0.0, "avg_logprob": -0.2222922228384709, "compression_ratio": 1.9483870967741936, "no_speech_prob": 8.446214110335859e-07}, {"id": 1294, "seek": 614768, "start": 6147.68, "end": 6155.280000000001, "text": " Okay, and then after these characters, this will be the next one. All right, so you can see that this one here has", "tokens": [1033, 11, 293, 550, 934, 613, 4342, 11, 341, 486, 312, 264, 958, 472, 13, 1057, 558, 11, 370, 291, 393, 536, 300, 341, 472, 510, 575], "temperature": 0.0, "avg_logprob": -0.21425439934981497, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.3574329216226033e-07}, {"id": 1295, "seek": 614768, "start": 6156.8, "end": 6160.96, "text": " 43 is its y value right because after those the next one will be", "tokens": [17914, 307, 1080, 288, 2158, 558, 570, 934, 729, 264, 958, 472, 486, 312], "temperature": 0.0, "avg_logprob": -0.21425439934981497, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.3574329216226033e-07}, {"id": 1296, "seek": 614768, "start": 6161.72, "end": 6163.72, "text": " 43 okay, so", "tokens": [17914, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.21425439934981497, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.3574329216226033e-07}, {"id": 1297, "seek": 614768, "start": 6164.280000000001, "end": 6168.56, "text": " So this is the first eight characters. This is two through nine", "tokens": [407, 341, 307, 264, 700, 3180, 4342, 13, 639, 307, 732, 807, 4949], "temperature": 0.0, "avg_logprob": -0.21425439934981497, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.3574329216226033e-07}, {"id": 1298, "seek": 614768, "start": 6169.12, "end": 6175.96, "text": " Three through ten and so forth right so these are overlapping groups of eight characters, and then this is the the next one along", "tokens": [6244, 807, 2064, 293, 370, 5220, 558, 370, 613, 366, 33535, 3935, 295, 3180, 4342, 11, 293, 550, 341, 307, 264, 264, 958, 472, 2051], "temperature": 0.0, "avg_logprob": -0.21425439934981497, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.3574329216226033e-07}, {"id": 1299, "seek": 617596, "start": 6175.96, "end": 6177.96, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.21791290592502904, "compression_ratio": 1.4972375690607735, "no_speech_prob": 1.873872406576993e-06}, {"id": 1300, "seek": 617596, "start": 6179.0, "end": 6181.0, "text": " So let's", "tokens": [407, 718, 311], "temperature": 0.0, "avg_logprob": -0.21791290592502904, "compression_ratio": 1.4972375690607735, "no_speech_prob": 1.873872406576993e-06}, {"id": 1301, "seek": 617596, "start": 6183.72, "end": 6185.68, "text": " Create that model", "tokens": [20248, 300, 2316], "temperature": 0.0, "avg_logprob": -0.21791290592502904, "compression_ratio": 1.4972375690607735, "no_speech_prob": 1.873872406576993e-06}, {"id": 1302, "seek": 617596, "start": 6185.68, "end": 6189.6, "text": " Okay, so again we use from arrays to create a model data class", "tokens": [1033, 11, 370, 797, 321, 764, 490, 41011, 281, 1884, 257, 2316, 1412, 1508], "temperature": 0.0, "avg_logprob": -0.21791290592502904, "compression_ratio": 1.4972375690607735, "no_speech_prob": 1.873872406576993e-06}, {"id": 1303, "seek": 617596, "start": 6189.6, "end": 6195.72, "text": " And so you'll see here we have exactly the same code as we had before there's our embedding", "tokens": [400, 370, 291, 603, 536, 510, 321, 362, 2293, 264, 912, 3089, 382, 321, 632, 949, 456, 311, 527, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.21791290592502904, "compression_ratio": 1.4972375690607735, "no_speech_prob": 1.873872406576993e-06}, {"id": 1304, "seek": 617596, "start": 6196.76, "end": 6199.8, "text": " Linear hidden output these are literally identical", "tokens": [14670, 289, 7633, 5598, 613, 366, 3736, 14800], "temperature": 0.0, "avg_logprob": -0.21791290592502904, "compression_ratio": 1.4972375690607735, "no_speech_prob": 1.873872406576993e-06}, {"id": 1305, "seek": 617596, "start": 6200.76, "end": 6203.16, "text": " Okay, and then we've replaced our", "tokens": [1033, 11, 293, 550, 321, 600, 10772, 527], "temperature": 0.0, "avg_logprob": -0.21791290592502904, "compression_ratio": 1.4972375690607735, "no_speech_prob": 1.873872406576993e-06}, {"id": 1306, "seek": 620316, "start": 6203.16, "end": 6204.72, "text": " our", "tokens": [527], "temperature": 0.0, "avg_logprob": -0.263473563724094, "compression_ratio": 1.5421686746987953, "no_speech_prob": 3.4465633689251263e-06}, {"id": 1307, "seek": 620316, "start": 6204.72, "end": 6209.08, "text": " Reliou of the linear input of the embedding with something that's inside a loop", "tokens": [8738, 72, 263, 295, 264, 8213, 4846, 295, 264, 12240, 3584, 365, 746, 300, 311, 1854, 257, 6367], "temperature": 0.0, "avg_logprob": -0.263473563724094, "compression_ratio": 1.5421686746987953, "no_speech_prob": 3.4465633689251263e-06}, {"id": 1308, "seek": 620316, "start": 6210.0, "end": 6213.599999999999, "text": " Okay, and then we've replaced the self dot L hidden thing", "tokens": [1033, 11, 293, 550, 321, 600, 10772, 264, 2698, 5893, 441, 7633, 551], "temperature": 0.0, "avg_logprob": -0.263473563724094, "compression_ratio": 1.5421686746987953, "no_speech_prob": 3.4465633689251263e-06}, {"id": 1309, "seek": 620316, "start": 6214.36, "end": 6216.72, "text": " Okay, also inside the loop", "tokens": [1033, 11, 611, 1854, 264, 6367], "temperature": 0.0, "avg_logprob": -0.263473563724094, "compression_ratio": 1.5421686746987953, "no_speech_prob": 3.4465633689251263e-06}, {"id": 1310, "seek": 620316, "start": 6223.88, "end": 6227.0199999999995, "text": " I just realized I didn't mention last time the use of the hyperbolic tan", "tokens": [286, 445, 5334, 286, 994, 380, 2152, 1036, 565, 264, 764, 295, 264, 9848, 65, 7940, 7603], "temperature": 0.0, "avg_logprob": -0.263473563724094, "compression_ratio": 1.5421686746987953, "no_speech_prob": 3.4465633689251263e-06}, {"id": 1311, "seek": 622702, "start": 6227.02, "end": 6233.820000000001, "text": " Hyperbolic tan looks like this", "tokens": [29592, 65, 7940, 7603, 1542, 411, 341], "temperature": 0.0, "avg_logprob": -0.15645254770914713, "compression_ratio": 1.5284090909090908, "no_speech_prob": 1.6536864677618723e-06}, {"id": 1312, "seek": 622702, "start": 6236.660000000001, "end": 6242.780000000001, "text": " Okay, so it's just a sigmoid that's offset right and it's very common to use a hyperbolic tan", "tokens": [1033, 11, 370, 309, 311, 445, 257, 4556, 3280, 327, 300, 311, 18687, 558, 293, 309, 311, 588, 2689, 281, 764, 257, 9848, 65, 7940, 7603], "temperature": 0.0, "avg_logprob": -0.15645254770914713, "compression_ratio": 1.5284090909090908, "no_speech_prob": 1.6536864677618723e-06}, {"id": 1313, "seek": 622702, "start": 6243.22, "end": 6250.160000000001, "text": " Inside this trend this state-to-state transition because it kind of stops it from flying off too high or too low", "tokens": [15123, 341, 6028, 341, 1785, 12, 1353, 12, 15406, 6034, 570, 309, 733, 295, 10094, 309, 490, 7137, 766, 886, 1090, 420, 886, 2295], "temperature": 0.0, "avg_logprob": -0.15645254770914713, "compression_ratio": 1.5284090909090908, "no_speech_prob": 1.6536864677618723e-06}, {"id": 1314, "seek": 622702, "start": 6250.160000000001, "end": 6252.06, "text": " You know it's nicely", "tokens": [509, 458, 309, 311, 9594], "temperature": 0.0, "avg_logprob": -0.15645254770914713, "compression_ratio": 1.5284090909090908, "no_speech_prob": 1.6536864677618723e-06}, {"id": 1315, "seek": 622702, "start": 6252.06, "end": 6253.580000000001, "text": " controlled", "tokens": [10164], "temperature": 0.0, "avg_logprob": -0.15645254770914713, "compression_ratio": 1.5284090909090908, "no_speech_prob": 1.6536864677618723e-06}, {"id": 1316, "seek": 625358, "start": 6253.58, "end": 6258.62, "text": " Back in the old days we used to use hyperbolic tan or the equivalent", "tokens": [5833, 294, 264, 1331, 1708, 321, 1143, 281, 764, 9848, 65, 7940, 7603, 420, 264, 10344], "temperature": 0.0, "avg_logprob": -0.17052838888513036, "compression_ratio": 1.6473684210526316, "no_speech_prob": 1.8162181731895544e-06}, {"id": 1317, "seek": 625358, "start": 6259.86, "end": 6265.98, "text": " Sigmoid a lot as most of our activation functions nowadays. We tend to use relu, but in these", "tokens": [37763, 3280, 327, 257, 688, 382, 881, 295, 527, 24433, 6828, 13434, 13, 492, 3928, 281, 764, 1039, 84, 11, 457, 294, 613], "temperature": 0.0, "avg_logprob": -0.17052838888513036, "compression_ratio": 1.6473684210526316, "no_speech_prob": 1.8162181731895544e-06}, {"id": 1318, "seek": 625358, "start": 6267.22, "end": 6269.22, "text": " hidden state to here in the hidden state", "tokens": [7633, 1785, 281, 510, 294, 264, 7633, 1785], "temperature": 0.0, "avg_logprob": -0.17052838888513036, "compression_ratio": 1.6473684210526316, "no_speech_prob": 1.8162181731895544e-06}, {"id": 1319, "seek": 625358, "start": 6270.42, "end": 6276.86, "text": " Transition weight matrices we still tend to use hyperbolic tan quite a lot, so you'll see I've done that also", "tokens": [6531, 849, 3364, 32284, 321, 920, 3928, 281, 764, 9848, 65, 7940, 7603, 1596, 257, 688, 11, 370, 291, 603, 536, 286, 600, 1096, 300, 611], "temperature": 0.0, "avg_logprob": -0.17052838888513036, "compression_ratio": 1.6473684210526316, "no_speech_prob": 1.8162181731895544e-06}, {"id": 1320, "seek": 627686, "start": 6276.86, "end": 6283.0599999999995, "text": " So yeah hyperbolic tan okay, so this is exactly the same as before", "tokens": [407, 1338, 9848, 65, 7940, 7603, 1392, 11, 370, 341, 307, 2293, 264, 912, 382, 949], "temperature": 0.0, "avg_logprob": -0.2109097415961108, "compression_ratio": 1.5483870967741935, "no_speech_prob": 4.860414719587425e-06}, {"id": 1321, "seek": 627686, "start": 6283.0599999999995, "end": 6286.48, "text": " But I've just replaced it with a for loop and then here's my output", "tokens": [583, 286, 600, 445, 10772, 309, 365, 257, 337, 6367, 293, 550, 510, 311, 452, 5598], "temperature": 0.0, "avg_logprob": -0.2109097415961108, "compression_ratio": 1.5483870967741935, "no_speech_prob": 4.860414719587425e-06}, {"id": 1322, "seek": 627686, "start": 6287.259999999999, "end": 6289.259999999999, "text": " Yes, you know", "tokens": [1079, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.2109097415961108, "compression_ratio": 1.5483870967741935, "no_speech_prob": 4.860414719587425e-06}, {"id": 1323, "seek": 627686, "start": 6289.339999999999, "end": 6293.9, "text": " So does he have to do anything with convergence of these networks?", "tokens": [407, 775, 415, 362, 281, 360, 1340, 365, 32181, 295, 613, 9590, 30], "temperature": 0.0, "avg_logprob": -0.2109097415961108, "compression_ratio": 1.5483870967741935, "no_speech_prob": 4.860414719587425e-06}, {"id": 1324, "seek": 627686, "start": 6295.66, "end": 6298.5, "text": " Yeah, kind of well we'll talk about that a little bit over time", "tokens": [865, 11, 733, 295, 731, 321, 603, 751, 466, 300, 257, 707, 857, 670, 565], "temperature": 0.0, "avg_logprob": -0.2109097415961108, "compression_ratio": 1.5483870967741935, "no_speech_prob": 4.860414719587425e-06}, {"id": 1325, "seek": 627686, "start": 6299.66, "end": 6304.74, "text": " Let's let's let's come back to that though for now. We're not really going to do anything special at all", "tokens": [961, 311, 718, 311, 718, 311, 808, 646, 281, 300, 1673, 337, 586, 13, 492, 434, 406, 534, 516, 281, 360, 1340, 2121, 412, 439], "temperature": 0.0, "avg_logprob": -0.2109097415961108, "compression_ratio": 1.5483870967741935, "no_speech_prob": 4.860414719587425e-06}, {"id": 1326, "seek": 630474, "start": 6304.74, "end": 6309.62, "text": " We don't recognizing. This is just a standard fully connected network", "tokens": [492, 500, 380, 18538, 13, 639, 307, 445, 257, 3832, 4498, 4582, 3209], "temperature": 0.0, "avg_logprob": -0.20721170091137445, "compression_ratio": 1.6788617886178863, "no_speech_prob": 8.013444130483549e-06}, {"id": 1327, "seek": 630474, "start": 6310.34, "end": 6312.42, "text": " You know interestingly it's quite a deep one", "tokens": [509, 458, 25873, 309, 311, 1596, 257, 2452, 472], "temperature": 0.0, "avg_logprob": -0.20721170091137445, "compression_ratio": 1.6788617886178863, "no_speech_prob": 8.013444130483549e-06}, {"id": 1328, "seek": 630474, "start": 6313.219999999999, "end": 6315.219999999999, "text": " right like because", "tokens": [558, 411, 570], "temperature": 0.0, "avg_logprob": -0.20721170091137445, "compression_ratio": 1.6788617886178863, "no_speech_prob": 8.013444130483549e-06}, {"id": 1329, "seek": 630474, "start": 6315.5, "end": 6320.219999999999, "text": " This is actually this but we've got eight of these things now", "tokens": [639, 307, 767, 341, 457, 321, 600, 658, 3180, 295, 613, 721, 586], "temperature": 0.0, "avg_logprob": -0.20721170091137445, "compression_ratio": 1.6788617886178863, "no_speech_prob": 8.013444130483549e-06}, {"id": 1330, "seek": 630474, "start": 6320.34, "end": 6324.16, "text": " We've now got a deep eight layer network, which is why units", "tokens": [492, 600, 586, 658, 257, 2452, 3180, 4583, 3209, 11, 597, 307, 983, 6815], "temperature": 0.0, "avg_logprob": -0.20721170091137445, "compression_ratio": 1.6788617886178863, "no_speech_prob": 8.013444130483549e-06}, {"id": 1331, "seek": 630474, "start": 6324.9, "end": 6330.42, "text": " Starting suggests we should be concerned is you know as we get deeper and deeper networks that can be harder and harder to trade", "tokens": [16217, 13409, 321, 820, 312, 5922, 307, 291, 458, 382, 321, 483, 7731, 293, 7731, 9590, 300, 393, 312, 6081, 293, 6081, 281, 4923], "temperature": 0.0, "avg_logprob": -0.20721170091137445, "compression_ratio": 1.6788617886178863, "no_speech_prob": 8.013444130483549e-06}, {"id": 1332, "seek": 633042, "start": 6330.42, "end": 6337.42, "text": " But let's try training this", "tokens": [583, 718, 311, 853, 3097, 341], "temperature": 0.0, "avg_logprob": -0.2748829210308236, "compression_ratio": 1.4518072289156627, "no_speech_prob": 4.49515300715575e-06}, {"id": 1333, "seek": 633042, "start": 6337.78, "end": 6343.42, "text": " All right, so where it goes as before we've got a batch size of 512", "tokens": [1057, 558, 11, 370, 689, 309, 1709, 382, 949, 321, 600, 658, 257, 15245, 2744, 295, 1025, 4762], "temperature": 0.0, "avg_logprob": -0.2748829210308236, "compression_ratio": 1.4518072289156627, "no_speech_prob": 4.49515300715575e-06}, {"id": 1334, "seek": 633042, "start": 6344.54, "end": 6346.54, "text": " We're using Adam", "tokens": [492, 434, 1228, 7938], "temperature": 0.0, "avg_logprob": -0.2748829210308236, "compression_ratio": 1.4518072289156627, "no_speech_prob": 4.49515300715575e-06}, {"id": 1335, "seek": 633042, "start": 6347.58, "end": 6353.82, "text": " And away it goes so we'll sit there watching it so we can then set the learning rate down back to 1 in egg 3", "tokens": [400, 1314, 309, 1709, 370, 321, 603, 1394, 456, 1976, 309, 370, 321, 393, 550, 992, 264, 2539, 3314, 760, 646, 281, 502, 294, 3777, 805], "temperature": 0.0, "avg_logprob": -0.2748829210308236, "compression_ratio": 1.4518072289156627, "no_speech_prob": 4.49515300715575e-06}, {"id": 1336, "seek": 633042, "start": 6354.42, "end": 6356.42, "text": " We can fit it again", "tokens": [492, 393, 3318, 309, 797], "temperature": 0.0, "avg_logprob": -0.2748829210308236, "compression_ratio": 1.4518072289156627, "no_speech_prob": 4.49515300715575e-06}, {"id": 1337, "seek": 635642, "start": 6356.42, "end": 6359.9800000000005, "text": " And yeah, it's actually it seems to be", "tokens": [400, 1338, 11, 309, 311, 767, 309, 2544, 281, 312], "temperature": 0.0, "avg_logprob": -0.2217597042221621, "compression_ratio": 1.7323232323232323, "no_speech_prob": 1.0188057331106393e-06}, {"id": 1338, "seek": 635642, "start": 6361.02, "end": 6363.02, "text": " training fun, okay", "tokens": [3097, 1019, 11, 1392], "temperature": 0.0, "avg_logprob": -0.2217597042221621, "compression_ratio": 1.7323232323232323, "no_speech_prob": 1.0188057331106393e-06}, {"id": 1339, "seek": 635642, "start": 6363.9800000000005, "end": 6367.62, "text": " But we're going to try something else which is we're going to use the trick that", "tokens": [583, 321, 434, 516, 281, 853, 746, 1646, 597, 307, 321, 434, 516, 281, 764, 264, 4282, 300], "temperature": 0.0, "avg_logprob": -0.2217597042221621, "compression_ratio": 1.7323232323232323, "no_speech_prob": 1.0188057331106393e-06}, {"id": 1340, "seek": 635642, "start": 6368.1, "end": 6372.82, "text": " Yannett rather hinted at before which is maybe we shouldn't be adding these things together and", "tokens": [398, 969, 3093, 2831, 12075, 292, 412, 949, 597, 307, 1310, 321, 4659, 380, 312, 5127, 613, 721, 1214, 293], "temperature": 0.0, "avg_logprob": -0.2217597042221621, "compression_ratio": 1.7323232323232323, "no_speech_prob": 1.0188057331106393e-06}, {"id": 1341, "seek": 635642, "start": 6373.62, "end": 6379.9800000000005, "text": " so the reason you might want to be feeling a little uncomfortable about adding these things together is that", "tokens": [370, 264, 1778, 291, 1062, 528, 281, 312, 2633, 257, 707, 10532, 466, 5127, 613, 721, 1214, 307, 300], "temperature": 0.0, "avg_logprob": -0.2217597042221621, "compression_ratio": 1.7323232323232323, "no_speech_prob": 1.0188057331106393e-06}, {"id": 1342, "seek": 637998, "start": 6379.98, "end": 6387.299999999999, "text": " the input state and the hidden state a kind of qualitatively different kinds of things", "tokens": [264, 4846, 1785, 293, 264, 7633, 1785, 257, 733, 295, 31312, 356, 819, 3685, 295, 721], "temperature": 0.0, "avg_logprob": -0.20211012098524306, "compression_ratio": 1.8303571428571428, "no_speech_prob": 1.529405153632979e-06}, {"id": 1343, "seek": 637998, "start": 6387.66, "end": 6391.7, "text": " Right the input state is the is the encoding of this character", "tokens": [1779, 264, 4846, 1785, 307, 264, 307, 264, 43430, 295, 341, 2517], "temperature": 0.0, "avg_logprob": -0.20211012098524306, "compression_ratio": 1.8303571428571428, "no_speech_prob": 1.529405153632979e-06}, {"id": 1344, "seek": 637998, "start": 6392.219999999999, "end": 6398.98, "text": " Whereas H represents the encoding of the series of characters so far and so adding them together is", "tokens": [13813, 389, 8855, 264, 43430, 295, 264, 2638, 295, 4342, 370, 1400, 293, 370, 5127, 552, 1214, 307], "temperature": 0.0, "avg_logprob": -0.20211012098524306, "compression_ratio": 1.8303571428571428, "no_speech_prob": 1.529405153632979e-06}, {"id": 1345, "seek": 637998, "start": 6399.339999999999, "end": 6401.82, "text": " Kind of potentially going to lose information", "tokens": [9242, 295, 7263, 516, 281, 3624, 1589], "temperature": 0.0, "avg_logprob": -0.20211012098524306, "compression_ratio": 1.8303571428571428, "no_speech_prob": 1.529405153632979e-06}, {"id": 1346, "seek": 637998, "start": 6402.62, "end": 6407.98, "text": " So I think what your net was going to prefer that we might do is maybe to concatenate these instead of adding them", "tokens": [407, 286, 519, 437, 428, 2533, 390, 516, 281, 4382, 300, 321, 1062, 360, 307, 1310, 281, 1588, 7186, 473, 613, 2602, 295, 5127, 552], "temperature": 0.0, "avg_logprob": -0.20211012098524306, "compression_ratio": 1.8303571428571428, "no_speech_prob": 1.529405153632979e-06}, {"id": 1347, "seek": 640798, "start": 6407.98, "end": 6411.099999999999, "text": " So that's how good to you get it. She's not it okay, so", "tokens": [407, 300, 311, 577, 665, 281, 291, 483, 309, 13, 1240, 311, 406, 309, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.24255571365356446, "compression_ratio": 1.5859030837004404, "no_speech_prob": 1.2098615798095125e-06}, {"id": 1348, "seek": 640798, "start": 6411.7, "end": 6418.78, "text": " Let's now make a copy of the previous cell all the same right but rather than using plus let's use", "tokens": [961, 311, 586, 652, 257, 5055, 295, 264, 3894, 2815, 439, 264, 912, 558, 457, 2831, 813, 1228, 1804, 718, 311, 764], "temperature": 0.0, "avg_logprob": -0.24255571365356446, "compression_ratio": 1.5859030837004404, "no_speech_prob": 1.2098615798095125e-06}, {"id": 1349, "seek": 640798, "start": 6419.299999999999, "end": 6420.419999999999, "text": " Cat", "tokens": [9565], "temperature": 0.0, "avg_logprob": -0.24255571365356446, "compression_ratio": 1.5859030837004404, "no_speech_prob": 1.2098615798095125e-06}, {"id": 1350, "seek": 640798, "start": 6420.419999999999, "end": 6425.86, "text": " Right now if we can cat then we need to make sure now that our input layer is", "tokens": [1779, 586, 498, 321, 393, 3857, 550, 321, 643, 281, 652, 988, 586, 300, 527, 4846, 4583, 307], "temperature": 0.0, "avg_logprob": -0.24255571365356446, "compression_ratio": 1.5859030837004404, "no_speech_prob": 1.2098615798095125e-06}, {"id": 1351, "seek": 640798, "start": 6427.139999999999, "end": 6431.419999999999, "text": " Not from n fact to hidden which is what we had before", "tokens": [1726, 490, 297, 1186, 281, 7633, 597, 307, 437, 321, 632, 949], "temperature": 0.0, "avg_logprob": -0.24255571365356446, "compression_ratio": 1.5859030837004404, "no_speech_prob": 1.2098615798095125e-06}, {"id": 1352, "seek": 640798, "start": 6431.7, "end": 6435.82, "text": " But because we're concatenating it needs to be n fact plus and hidden", "tokens": [583, 570, 321, 434, 1588, 7186, 990, 309, 2203, 281, 312, 297, 1186, 1804, 293, 7633], "temperature": 0.0, "avg_logprob": -0.24255571365356446, "compression_ratio": 1.5859030837004404, "no_speech_prob": 1.2098615798095125e-06}, {"id": 1353, "seek": 643582, "start": 6435.82, "end": 6441.34, "text": " To end hidden okay, and so now that's going to make all the dimensions work nicely", "tokens": [1407, 917, 7633, 1392, 11, 293, 370, 586, 300, 311, 516, 281, 652, 439, 264, 12819, 589, 9594], "temperature": 0.0, "avg_logprob": -0.20690382980718847, "compression_ratio": 1.654054054054054, "no_speech_prob": 3.412584419493214e-07}, {"id": 1354, "seek": 643582, "start": 6443.299999999999, "end": 6447.78, "text": " So this now is of size n fact plus n hidden", "tokens": [407, 341, 586, 307, 295, 2744, 297, 1186, 1804, 297, 7633], "temperature": 0.0, "avg_logprob": -0.20690382980718847, "compression_ratio": 1.654054054054054, "no_speech_prob": 3.412584419493214e-07}, {"id": 1355, "seek": 643582, "start": 6448.9, "end": 6451.88, "text": " This now makes it back to size n hidden again", "tokens": [639, 586, 1669, 309, 646, 281, 2744, 297, 7633, 797], "temperature": 0.0, "avg_logprob": -0.20690382980718847, "compression_ratio": 1.654054054054054, "no_speech_prob": 3.412584419493214e-07}, {"id": 1356, "seek": 643582, "start": 6452.58, "end": 6458.0199999999995, "text": " Okay, and then this is putting it through the same square matrix as before so it's still a size n hidden", "tokens": [1033, 11, 293, 550, 341, 307, 3372, 309, 807, 264, 912, 3732, 8141, 382, 949, 370, 309, 311, 920, 257, 2744, 297, 7633], "temperature": 0.0, "avg_logprob": -0.20690382980718847, "compression_ratio": 1.654054054054054, "no_speech_prob": 3.412584419493214e-07}, {"id": 1357, "seek": 643582, "start": 6458.7, "end": 6462.0599999999995, "text": " Okay, so this is like a good", "tokens": [1033, 11, 370, 341, 307, 411, 257, 665], "temperature": 0.0, "avg_logprob": -0.20690382980718847, "compression_ratio": 1.654054054054054, "no_speech_prob": 3.412584419493214e-07}, {"id": 1358, "seek": 646206, "start": 6462.06, "end": 6466.820000000001, "text": " design heuristic if you're designing an architecture is if you've got", "tokens": [1715, 415, 374, 3142, 498, 291, 434, 14685, 364, 9482, 307, 498, 291, 600, 658], "temperature": 0.0, "avg_logprob": -0.15361967273786956, "compression_ratio": 1.7120622568093384, "no_speech_prob": 2.406096655249712e-06}, {"id": 1359, "seek": 646206, "start": 6467.46, "end": 6472.18, "text": " Different types of information that you want to combine you generally want to concatenate it", "tokens": [20825, 3467, 295, 1589, 300, 291, 528, 281, 10432, 291, 5101, 528, 281, 1588, 7186, 473, 309], "temperature": 0.0, "avg_logprob": -0.15361967273786956, "compression_ratio": 1.7120622568093384, "no_speech_prob": 2.406096655249712e-06}, {"id": 1360, "seek": 646206, "start": 6472.700000000001, "end": 6478.160000000001, "text": " Right you know adding things together even if they're the same shape is losing", "tokens": [1779, 291, 458, 5127, 721, 1214, 754, 498, 436, 434, 264, 912, 3909, 307, 7027], "temperature": 0.0, "avg_logprob": -0.15361967273786956, "compression_ratio": 1.7120622568093384, "no_speech_prob": 2.406096655249712e-06}, {"id": 1361, "seek": 646206, "start": 6479.06, "end": 6485.18, "text": " Information okay, and so once you've concatenated things together you can always convert it back down to a", "tokens": [15357, 1392, 11, 293, 370, 1564, 291, 600, 1588, 7186, 770, 721, 1214, 291, 393, 1009, 7620, 309, 646, 760, 281, 257], "temperature": 0.0, "avg_logprob": -0.15361967273786956, "compression_ratio": 1.7120622568093384, "no_speech_prob": 2.406096655249712e-06}, {"id": 1362, "seek": 648518, "start": 6485.18, "end": 6492.26, "text": " Fixed size by just chucking it through a matrix product okay, so that's all we've done here again", "tokens": [25538, 292, 2744, 538, 445, 20870, 278, 309, 807, 257, 8141, 1674, 1392, 11, 370, 300, 311, 439, 321, 600, 1096, 510, 797], "temperature": 0.0, "avg_logprob": -0.1877063225055563, "compression_ratio": 1.653061224489796, "no_speech_prob": 1.6028064919737517e-06}, {"id": 1363, "seek": 648518, "start": 6492.26, "end": 6494.26, "text": " It's the same thing, but now we're concatenating instead", "tokens": [467, 311, 264, 912, 551, 11, 457, 586, 321, 434, 1588, 7186, 990, 2602], "temperature": 0.0, "avg_logprob": -0.1877063225055563, "compression_ratio": 1.653061224489796, "no_speech_prob": 1.6028064919737517e-06}, {"id": 1364, "seek": 648518, "start": 6495.3, "end": 6496.9800000000005, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.1877063225055563, "compression_ratio": 1.653061224489796, "no_speech_prob": 1.6028064919737517e-06}, {"id": 1365, "seek": 648518, "start": 6496.9800000000005, "end": 6499.9400000000005, "text": " So we can fit that and so last time we got", "tokens": [407, 321, 393, 3318, 300, 293, 370, 1036, 565, 321, 658], "temperature": 0.0, "avg_logprob": -0.1877063225055563, "compression_ratio": 1.653061224489796, "no_speech_prob": 1.6028064919737517e-06}, {"id": 1366, "seek": 648518, "start": 6501.34, "end": 6502.54, "text": " 1.72", "tokens": [502, 13, 28890], "temperature": 0.0, "avg_logprob": -0.1877063225055563, "compression_ratio": 1.653061224489796, "no_speech_prob": 1.6028064919737517e-06}, {"id": 1367, "seek": 648518, "start": 6502.54, "end": 6506.02, "text": " This time we go at 1.68, so it's not setting the world on fire", "tokens": [639, 565, 321, 352, 412, 502, 13, 27102, 11, 370, 309, 311, 406, 3287, 264, 1002, 322, 2610], "temperature": 0.0, "avg_logprob": -0.1877063225055563, "compression_ratio": 1.653061224489796, "no_speech_prob": 1.6028064919737517e-06}, {"id": 1368, "seek": 648518, "start": 6506.02, "end": 6509.5, "text": " But it's an improvement and the improvements are good okay", "tokens": [583, 309, 311, 364, 10444, 293, 264, 13797, 366, 665, 1392], "temperature": 0.0, "avg_logprob": -0.1877063225055563, "compression_ratio": 1.653061224489796, "no_speech_prob": 1.6028064919737517e-06}, {"id": 1369, "seek": 650950, "start": 6509.5, "end": 6514.78, "text": " So we can now test that with get next and so now we can pass in eight things", "tokens": [407, 321, 393, 586, 1500, 300, 365, 483, 958, 293, 370, 586, 321, 393, 1320, 294, 3180, 721], "temperature": 0.0, "avg_logprob": -0.25313175425809975, "compression_ratio": 1.6505376344086022, "no_speech_prob": 2.6016045922006015e-06}, {"id": 1370, "seek": 650950, "start": 6515.5, "end": 6521.7, "text": " Right so it's now before those it looks good or part of that sounds good as well so", "tokens": [1779, 370, 309, 311, 586, 949, 729, 309, 1542, 665, 420, 644, 295, 300, 3263, 665, 382, 731, 370], "temperature": 0.0, "avg_logprob": -0.25313175425809975, "compression_ratio": 1.6505376344086022, "no_speech_prob": 2.6016045922006015e-06}, {"id": 1371, "seek": 650950, "start": 6522.3, "end": 6526.3, "text": " Queens and that sounds good too alright, so great", "tokens": [18414, 293, 300, 3263, 665, 886, 5845, 11, 370, 869], "temperature": 0.0, "avg_logprob": -0.25313175425809975, "compression_ratio": 1.6505376344086022, "no_speech_prob": 2.6016045922006015e-06}, {"id": 1372, "seek": 650950, "start": 6527.62, "end": 6529.62, "text": " So that's enough", "tokens": [407, 300, 311, 1547], "temperature": 0.0, "avg_logprob": -0.25313175425809975, "compression_ratio": 1.6505376344086022, "no_speech_prob": 2.6016045922006015e-06}, {"id": 1373, "seek": 650950, "start": 6529.94, "end": 6531.94, "text": " manual hackery", "tokens": [9688, 10339, 2109], "temperature": 0.0, "avg_logprob": -0.25313175425809975, "compression_ratio": 1.6505376344086022, "no_speech_prob": 2.6016045922006015e-06}, {"id": 1374, "seek": 650950, "start": 6532.18, "end": 6536.34, "text": " Let's see if pytorch can do some of this for us and so basically", "tokens": [961, 311, 536, 498, 25878, 284, 339, 393, 360, 512, 295, 341, 337, 505, 293, 370, 1936], "temperature": 0.0, "avg_logprob": -0.25313175425809975, "compression_ratio": 1.6505376344086022, "no_speech_prob": 2.6016045922006015e-06}, {"id": 1375, "seek": 653634, "start": 6536.34, "end": 6540.34, "text": " What pytorch will do for us is it will write this loop?", "tokens": [708, 25878, 284, 339, 486, 360, 337, 505, 307, 309, 486, 2464, 341, 6367, 30], "temperature": 0.0, "avg_logprob": -0.19176537919752668, "compression_ratio": 1.663594470046083, "no_speech_prob": 9.570808288117405e-07}, {"id": 1376, "seek": 653634, "start": 6541.58, "end": 6546.06, "text": " Automatically okay, and it will create these linear input layers", "tokens": [24619, 5030, 1392, 11, 293, 309, 486, 1884, 613, 8213, 4846, 7914], "temperature": 0.0, "avg_logprob": -0.19176537919752668, "compression_ratio": 1.663594470046083, "no_speech_prob": 9.570808288117405e-07}, {"id": 1377, "seek": 653634, "start": 6547.06, "end": 6553.22, "text": " Automatically okay, and so to ask it to do that we can use the nn dot rnn plus", "tokens": [24619, 5030, 1392, 11, 293, 370, 281, 1029, 309, 281, 360, 300, 321, 393, 764, 264, 297, 77, 5893, 367, 26384, 1804], "temperature": 0.0, "avg_logprob": -0.19176537919752668, "compression_ratio": 1.663594470046083, "no_speech_prob": 9.570808288117405e-07}, {"id": 1378, "seek": 653634, "start": 6554.3, "end": 6556.3, "text": " So here's the exact same thing", "tokens": [407, 510, 311, 264, 1900, 912, 551], "temperature": 0.0, "avg_logprob": -0.19176537919752668, "compression_ratio": 1.663594470046083, "no_speech_prob": 9.570808288117405e-07}, {"id": 1379, "seek": 653634, "start": 6556.74, "end": 6558.3, "text": " in less code", "tokens": [294, 1570, 3089], "temperature": 0.0, "avg_logprob": -0.19176537919752668, "compression_ratio": 1.663594470046083, "no_speech_prob": 9.570808288117405e-07}, {"id": 1380, "seek": 653634, "start": 6558.3, "end": 6560.9400000000005, "text": " By taking advantage of pytorch and again", "tokens": [3146, 1940, 5002, 295, 25878, 284, 339, 293, 797], "temperature": 0.0, "avg_logprob": -0.19176537919752668, "compression_ratio": 1.663594470046083, "no_speech_prob": 9.570808288117405e-07}, {"id": 1381, "seek": 653634, "start": 6560.9400000000005, "end": 6565.68, "text": " I'm not using a conceptual analogy to say pytorch is doing something like it", "tokens": [286, 478, 406, 1228, 257, 24106, 21663, 281, 584, 25878, 284, 339, 307, 884, 746, 411, 309], "temperature": 0.0, "avg_logprob": -0.19176537919752668, "compression_ratio": 1.663594470046083, "no_speech_prob": 9.570808288117405e-07}, {"id": 1382, "seek": 656568, "start": 6565.68, "end": 6570.58, "text": " I'm saying pytorch is doing it right this is just the code you just saw", "tokens": [286, 478, 1566, 25878, 284, 339, 307, 884, 309, 558, 341, 307, 445, 264, 3089, 291, 445, 1866], "temperature": 0.0, "avg_logprob": -0.1603237595221009, "compression_ratio": 1.6347826086956523, "no_speech_prob": 2.948006567748962e-06}, {"id": 1383, "seek": 656568, "start": 6571.1, "end": 6577.34, "text": " Wrapped up a little bit refactored a little bit for your convenience alright, so where we say we now want to create an RNN", "tokens": [343, 424, 3320, 493, 257, 707, 857, 1895, 578, 2769, 257, 707, 857, 337, 428, 19283, 5845, 11, 370, 689, 321, 584, 321, 586, 528, 281, 1884, 364, 45702, 45], "temperature": 0.0, "avg_logprob": -0.1603237595221009, "compression_ratio": 1.6347826086956523, "no_speech_prob": 2.948006567748962e-06}, {"id": 1384, "seek": 656568, "start": 6578.62, "end": 6580.42, "text": " Called RNN", "tokens": [45001, 45702, 45], "temperature": 0.0, "avg_logprob": -0.1603237595221009, "compression_ratio": 1.6347826086956523, "no_speech_prob": 2.948006567748962e-06}, {"id": 1385, "seek": 656568, "start": 6580.42, "end": 6586.200000000001, "text": " Then what this does is it does that for loop now notice that our for loop?", "tokens": [1396, 437, 341, 775, 307, 309, 775, 300, 337, 6367, 586, 3449, 300, 527, 337, 6367, 30], "temperature": 0.0, "avg_logprob": -0.1603237595221009, "compression_ratio": 1.6347826086956523, "no_speech_prob": 2.948006567748962e-06}, {"id": 1386, "seek": 656568, "start": 6587.26, "end": 6589.26, "text": " needed a starting point", "tokens": [2978, 257, 2891, 935], "temperature": 0.0, "avg_logprob": -0.1603237595221009, "compression_ratio": 1.6347826086956523, "no_speech_prob": 2.948006567748962e-06}, {"id": 1387, "seek": 656568, "start": 6590.26, "end": 6593.780000000001, "text": " You remember why right because otherwise our for loop didn't quite work", "tokens": [509, 1604, 983, 558, 570, 5911, 527, 337, 6367, 994, 380, 1596, 589], "temperature": 0.0, "avg_logprob": -0.1603237595221009, "compression_ratio": 1.6347826086956523, "no_speech_prob": 2.948006567748962e-06}, {"id": 1388, "seek": 659378, "start": 6593.78, "end": 6598.599999999999, "text": " We couldn't quite refactor it out and because this is exactly the same this needs a starting point, too", "tokens": [492, 2809, 380, 1596, 1895, 15104, 309, 484, 293, 570, 341, 307, 2293, 264, 912, 341, 2203, 257, 2891, 935, 11, 886], "temperature": 0.0, "avg_logprob": -0.16582853653851679, "compression_ratio": 1.680161943319838, "no_speech_prob": 5.98926476413908e-07}, {"id": 1389, "seek": 659378, "start": 6598.98, "end": 6603.82, "text": " All right, so let's give it a starting point and so you have to pass in your initial hidden state", "tokens": [1057, 558, 11, 370, 718, 311, 976, 309, 257, 2891, 935, 293, 370, 291, 362, 281, 1320, 294, 428, 5883, 7633, 1785], "temperature": 0.0, "avg_logprob": -0.16582853653851679, "compression_ratio": 1.680161943319838, "no_speech_prob": 5.98926476413908e-07}, {"id": 1390, "seek": 659378, "start": 6604.58, "end": 6608.179999999999, "text": " Okay for reasons that will become apparent later on", "tokens": [1033, 337, 4112, 300, 486, 1813, 18335, 1780, 322], "temperature": 0.0, "avg_logprob": -0.16582853653851679, "compression_ratio": 1.680161943319838, "no_speech_prob": 5.98926476413908e-07}, {"id": 1391, "seek": 659378, "start": 6609.9, "end": 6611.9, "text": " It turns out", "tokens": [467, 4523, 484], "temperature": 0.0, "avg_logprob": -0.16582853653851679, "compression_ratio": 1.680161943319838, "no_speech_prob": 5.98926476413908e-07}, {"id": 1392, "seek": 659378, "start": 6612.8, "end": 6616.0199999999995, "text": " To be quite useful to be able to get back", "tokens": [1407, 312, 1596, 4420, 281, 312, 1075, 281, 483, 646], "temperature": 0.0, "avg_logprob": -0.16582853653851679, "compression_ratio": 1.680161943319838, "no_speech_prob": 5.98926476413908e-07}, {"id": 1393, "seek": 659378, "start": 6616.74, "end": 6622.34, "text": " That hidden state at the end and just like we could here. We could actually keep track of the hidden state", "tokens": [663, 7633, 1785, 412, 264, 917, 293, 445, 411, 321, 727, 510, 13, 492, 727, 767, 1066, 2837, 295, 264, 7633, 1785], "temperature": 0.0, "avg_logprob": -0.16582853653851679, "compression_ratio": 1.680161943319838, "no_speech_prob": 5.98926476413908e-07}, {"id": 1394, "seek": 662234, "start": 6622.34, "end": 6623.9800000000005, "text": " we", "tokens": [321], "temperature": 0.0, "avg_logprob": -0.24257937971367893, "compression_ratio": 1.8950617283950617, "no_speech_prob": 3.844904313154984e-06}, {"id": 1395, "seek": 662234, "start": 6623.9800000000005, "end": 6631.1, "text": " Get back to things we get back both the output and the hidden state right so we pass in the input in the hidden state", "tokens": [3240, 646, 281, 721, 321, 483, 646, 1293, 264, 5598, 293, 264, 7633, 1785, 558, 370, 321, 1320, 294, 264, 4846, 294, 264, 7633, 1785], "temperature": 0.0, "avg_logprob": -0.24257937971367893, "compression_ratio": 1.8950617283950617, "no_speech_prob": 3.844904313154984e-06}, {"id": 1396, "seek": 662234, "start": 6631.1, "end": 6633.9800000000005, "text": " And we get back the output and the hidden state", "tokens": [400, 321, 483, 646, 264, 5598, 293, 264, 7633, 1785], "temperature": 0.0, "avg_logprob": -0.24257937971367893, "compression_ratio": 1.8950617283950617, "no_speech_prob": 3.844904313154984e-06}, {"id": 1397, "seek": 662234, "start": 6635.18, "end": 6641.14, "text": " Yes, could you remind us what the hidden state represents the hidden state is H?", "tokens": [1079, 11, 727, 291, 4160, 505, 437, 264, 7633, 1785, 8855, 264, 7633, 1785, 307, 389, 30], "temperature": 0.0, "avg_logprob": -0.24257937971367893, "compression_ratio": 1.8950617283950617, "no_speech_prob": 3.844904313154984e-06}, {"id": 1398, "seek": 662234, "start": 6642.06, "end": 6644.06, "text": " so it's the", "tokens": [370, 309, 311, 264], "temperature": 0.0, "avg_logprob": -0.24257937971367893, "compression_ratio": 1.8950617283950617, "no_speech_prob": 3.844904313154984e-06}, {"id": 1399, "seek": 662234, "start": 6645.62, "end": 6647.62, "text": " It's the orange", "tokens": [467, 311, 264, 7671], "temperature": 0.0, "avg_logprob": -0.24257937971367893, "compression_ratio": 1.8950617283950617, "no_speech_prob": 3.844904313154984e-06}, {"id": 1400, "seek": 662234, "start": 6648.46, "end": 6650.9800000000005, "text": " Circle ellipse of activations", "tokens": [29381, 8284, 48041, 295, 2430, 763], "temperature": 0.0, "avg_logprob": -0.24257937971367893, "compression_ratio": 1.8950617283950617, "no_speech_prob": 3.844904313154984e-06}, {"id": 1401, "seek": 665098, "start": 6650.98, "end": 6653.94, "text": " Okay, and so it is of size", "tokens": [1033, 11, 293, 370, 309, 307, 295, 2744], "temperature": 0.0, "avg_logprob": -0.24538522608139934, "compression_ratio": 1.282258064516129, "no_speech_prob": 3.059017217310611e-07}, {"id": 1402, "seek": 665098, "start": 6655.379999999999, "end": 6657.379999999999, "text": " 256", "tokens": [38882], "temperature": 0.0, "avg_logprob": -0.24538522608139934, "compression_ratio": 1.282258064516129, "no_speech_prob": 3.059017217310611e-07}, {"id": 1403, "seek": 665098, "start": 6657.58, "end": 6659.58, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.24538522608139934, "compression_ratio": 1.282258064516129, "no_speech_prob": 3.059017217310611e-07}, {"id": 1404, "seek": 665098, "start": 6660.259999999999, "end": 6662.259999999999, "text": " All right, so we can", "tokens": [1057, 558, 11, 370, 321, 393], "temperature": 0.0, "avg_logprob": -0.24538522608139934, "compression_ratio": 1.282258064516129, "no_speech_prob": 3.059017217310611e-07}, {"id": 1405, "seek": 665098, "start": 6665.54, "end": 6673.419999999999, "text": " Okay, there's one other thing to know which is in our case we were replacing H with a new hidden state", "tokens": [1033, 11, 456, 311, 472, 661, 551, 281, 458, 597, 307, 294, 527, 1389, 321, 645, 19139, 389, 365, 257, 777, 7633, 1785], "temperature": 0.0, "avg_logprob": -0.24538522608139934, "compression_ratio": 1.282258064516129, "no_speech_prob": 3.059017217310611e-07}, {"id": 1406, "seek": 667342, "start": 6673.42, "end": 6680.38, "text": " The one minor difference in pytorch is they append the new hidden state", "tokens": [440, 472, 6696, 2649, 294, 25878, 284, 339, 307, 436, 34116, 264, 777, 7633, 1785], "temperature": 0.0, "avg_logprob": -0.16741135655617229, "compression_ratio": 1.7733333333333334, "no_speech_prob": 1.1726378943421878e-06}, {"id": 1407, "seek": 667342, "start": 6680.7, "end": 6686.5, "text": " To a list or to a tensor which gets bigger and bigger so they actually give you back all of the hidden states", "tokens": [1407, 257, 1329, 420, 281, 257, 40863, 597, 2170, 3801, 293, 3801, 370, 436, 767, 976, 291, 646, 439, 295, 264, 7633, 4368], "temperature": 0.0, "avg_logprob": -0.16741135655617229, "compression_ratio": 1.7733333333333334, "no_speech_prob": 1.1726378943421878e-06}, {"id": 1408, "seek": 667342, "start": 6686.66, "end": 6689.26, "text": " So in other words rather than just giving you back the final", "tokens": [407, 294, 661, 2283, 2831, 813, 445, 2902, 291, 646, 264, 2572], "temperature": 0.0, "avg_logprob": -0.16741135655617229, "compression_ratio": 1.7733333333333334, "no_speech_prob": 1.1726378943421878e-06}, {"id": 1409, "seek": 667342, "start": 6689.9, "end": 6695.5, "text": " Ellipse they give you back all the ellipses stacked on top of each other and so because we just want the final one", "tokens": [8353, 48041, 436, 976, 291, 646, 439, 264, 8284, 2600, 279, 28867, 322, 1192, 295, 1184, 661, 293, 370, 570, 321, 445, 528, 264, 2572, 472], "temperature": 0.0, "avg_logprob": -0.16741135655617229, "compression_ratio": 1.7733333333333334, "no_speech_prob": 1.1726378943421878e-06}, {"id": 1410, "seek": 667342, "start": 6695.5, "end": 6698.34, "text": " I just got indexed into it with minus one", "tokens": [286, 445, 658, 8186, 292, 666, 309, 365, 3175, 472], "temperature": 0.0, "avg_logprob": -0.16741135655617229, "compression_ratio": 1.7733333333333334, "no_speech_prob": 1.1726378943421878e-06}, {"id": 1411, "seek": 669834, "start": 6698.34, "end": 6706.9800000000005, "text": " Yeah, okay other than that. This is the same code as before put that through our output layer to get the correct vocab size", "tokens": [865, 11, 1392, 661, 813, 300, 13, 639, 307, 264, 912, 3089, 382, 949, 829, 300, 807, 527, 5598, 4583, 281, 483, 264, 3006, 2329, 455, 2744], "temperature": 0.0, "avg_logprob": -0.17231829961140951, "compression_ratio": 1.5736040609137056, "no_speech_prob": 2.4824723823257955e-06}, {"id": 1412, "seek": 669834, "start": 6706.9800000000005, "end": 6708.9800000000005, "text": " And then we can train that", "tokens": [400, 550, 321, 393, 3847, 300], "temperature": 0.0, "avg_logprob": -0.17231829961140951, "compression_ratio": 1.5736040609137056, "no_speech_prob": 2.4824723823257955e-06}, {"id": 1413, "seek": 669834, "start": 6718.1, "end": 6721.22, "text": " Right so you can see here. I can do it manually I can create some hidden state", "tokens": [1779, 370, 291, 393, 536, 510, 13, 286, 393, 360, 309, 16945, 286, 393, 1884, 512, 7633, 1785], "temperature": 0.0, "avg_logprob": -0.17231829961140951, "compression_ratio": 1.5736040609137056, "no_speech_prob": 2.4824723823257955e-06}, {"id": 1414, "seek": 669834, "start": 6721.22, "end": 6724.5, "text": " I can pass it to that RNN and I can see the stuff I get back", "tokens": [286, 393, 1320, 309, 281, 300, 45702, 45, 293, 286, 393, 536, 264, 1507, 286, 483, 646], "temperature": 0.0, "avg_logprob": -0.17231829961140951, "compression_ratio": 1.5736040609137056, "no_speech_prob": 2.4824723823257955e-06}, {"id": 1415, "seek": 672450, "start": 6724.5, "end": 6727.5, "text": " you'll see that the", "tokens": [291, 603, 536, 300, 264], "temperature": 0.0, "avg_logprob": -0.2796991401248508, "compression_ratio": 1.430232558139535, "no_speech_prob": 1.844820531005098e-06}, {"id": 1416, "seek": 672450, "start": 6729.1, "end": 6736.22, "text": " Dimensionality of H. It's actually a rank 3 tensor where else in my version it was a", "tokens": [20975, 3378, 1860, 295, 389, 13, 467, 311, 767, 257, 6181, 805, 40863, 689, 1646, 294, 452, 3037, 309, 390, 257], "temperature": 0.0, "avg_logprob": -0.2796991401248508, "compression_ratio": 1.430232558139535, "no_speech_prob": 1.844820531005098e-06}, {"id": 1417, "seek": 672450, "start": 6739.54, "end": 6741.86, "text": " See it was a rank 2 tensor", "tokens": [3008, 309, 390, 257, 6181, 568, 40863], "temperature": 0.0, "avg_logprob": -0.2796991401248508, "compression_ratio": 1.430232558139535, "no_speech_prob": 1.844820531005098e-06}, {"id": 1418, "seek": 672450, "start": 6742.62, "end": 6746.86, "text": " Okay, and the difference is here. We've got just a unit axis at the front", "tokens": [1033, 11, 293, 264, 2649, 307, 510, 13, 492, 600, 658, 445, 257, 4985, 10298, 412, 264, 1868], "temperature": 0.0, "avg_logprob": -0.2796991401248508, "compression_ratio": 1.430232558139535, "no_speech_prob": 1.844820531005098e-06}, {"id": 1419, "seek": 672450, "start": 6747.98, "end": 6749.98, "text": " We'll learn more about why that is later", "tokens": [492, 603, 1466, 544, 466, 983, 300, 307, 1780], "temperature": 0.0, "avg_logprob": -0.2796991401248508, "compression_ratio": 1.430232558139535, "no_speech_prob": 1.844820531005098e-06}, {"id": 1420, "seek": 674998, "start": 6749.98, "end": 6754.82, "text": " But basically it turns out you can have a second RNN that goes backwards", "tokens": [583, 1936, 309, 4523, 484, 291, 393, 362, 257, 1150, 45702, 45, 300, 1709, 12204], "temperature": 0.0, "avg_logprob": -0.19529820276685983, "compression_ratio": 1.9090909090909092, "no_speech_prob": 2.0904483335471014e-06}, {"id": 1421, "seek": 674998, "start": 6755.299999999999, "end": 6760.58, "text": " Right one that goes forwards one that goes backwards and the idea is it can then it's going to be better at finding", "tokens": [1779, 472, 300, 1709, 30126, 472, 300, 1709, 12204, 293, 264, 1558, 307, 309, 393, 550, 309, 311, 516, 281, 312, 1101, 412, 5006], "temperature": 0.0, "avg_logprob": -0.19529820276685983, "compression_ratio": 1.9090909090909092, "no_speech_prob": 2.0904483335471014e-06}, {"id": 1422, "seek": 674998, "start": 6761.5, "end": 6768.139999999999, "text": " Relationships that kind of go backwards that's called a bi-directional RNN also it turns out you can have an RNN feed to an RNN", "tokens": [28663, 7640, 300, 733, 295, 352, 12204, 300, 311, 1219, 257, 3228, 12, 18267, 41048, 45702, 45, 611, 309, 4523, 484, 291, 393, 362, 364, 45702, 45, 3154, 281, 364, 45702, 45], "temperature": 0.0, "avg_logprob": -0.19529820276685983, "compression_ratio": 1.9090909090909092, "no_speech_prob": 2.0904483335471014e-06}, {"id": 1423, "seek": 674998, "start": 6768.139999999999, "end": 6769.66, "text": " That's got a multi-layer RNN", "tokens": [663, 311, 658, 257, 4825, 12, 8376, 260, 45702, 45], "temperature": 0.0, "avg_logprob": -0.19529820276685983, "compression_ratio": 1.9090909090909092, "no_speech_prob": 2.0904483335471014e-06}, {"id": 1424, "seek": 674998, "start": 6769.66, "end": 6773.82, "text": " So basically if you have those things you need an additional", "tokens": [407, 1936, 498, 291, 362, 729, 721, 291, 643, 364, 4497], "temperature": 0.0, "avg_logprob": -0.19529820276685983, "compression_ratio": 1.9090909090909092, "no_speech_prob": 2.0904483335471014e-06}, {"id": 1425, "seek": 674998, "start": 6774.459999999999, "end": 6778.419999999999, "text": " Axis on your tensor to keep track of those additional layers of hidden state", "tokens": [20118, 271, 322, 428, 40863, 281, 1066, 2837, 295, 729, 4497, 7914, 295, 7633, 1785], "temperature": 0.0, "avg_logprob": -0.19529820276685983, "compression_ratio": 1.9090909090909092, "no_speech_prob": 2.0904483335471014e-06}, {"id": 1426, "seek": 677842, "start": 6778.42, "end": 6780.74, "text": " But for now we'll always have a one", "tokens": [583, 337, 586, 321, 603, 1009, 362, 257, 472], "temperature": 0.0, "avg_logprob": -0.2477537585842994, "compression_ratio": 1.530612244897959, "no_speech_prob": 1.4144737860988243e-06}, {"id": 1427, "seek": 677842, "start": 6781.22, "end": 6785.9, "text": " Yeah, and we'll always also get back a one at the end", "tokens": [865, 11, 293, 321, 603, 1009, 611, 483, 646, 257, 472, 412, 264, 917], "temperature": 0.0, "avg_logprob": -0.2477537585842994, "compression_ratio": 1.530612244897959, "no_speech_prob": 1.4144737860988243e-06}, {"id": 1428, "seek": 677842, "start": 6787.1, "end": 6788.74, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2477537585842994, "compression_ratio": 1.530612244897959, "no_speech_prob": 1.4144737860988243e-06}, {"id": 1429, "seek": 677842, "start": 6788.74, "end": 6790.74, "text": " So if we go ahead and fit this now", "tokens": [407, 498, 321, 352, 2286, 293, 3318, 341, 586], "temperature": 0.0, "avg_logprob": -0.2477537585842994, "compression_ratio": 1.530612244897959, "no_speech_prob": 1.4144737860988243e-06}, {"id": 1430, "seek": 677842, "start": 6791.9800000000005, "end": 6794.22, "text": " Let's actually train it for a bit longer", "tokens": [961, 311, 767, 3847, 309, 337, 257, 857, 2854], "temperature": 0.0, "avg_logprob": -0.2477537585842994, "compression_ratio": 1.530612244897959, "no_speech_prob": 1.4144737860988243e-06}, {"id": 1431, "seek": 677842, "start": 6794.9800000000005, "end": 6801.02, "text": " Okay, so last time we only kind of did a couple of epochs this time. We'll do four epochs", "tokens": [1033, 11, 370, 1036, 565, 321, 787, 733, 295, 630, 257, 1916, 295, 30992, 28346, 341, 565, 13, 492, 603, 360, 1451, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.2477537585842994, "compression_ratio": 1.530612244897959, "no_speech_prob": 1.4144737860988243e-06}, {"id": 1432, "seek": 677842, "start": 6801.82, "end": 6804.62, "text": " What do we set at one in egg three and?", "tokens": [708, 360, 321, 992, 412, 472, 294, 3777, 1045, 293, 30], "temperature": 0.0, "avg_logprob": -0.2477537585842994, "compression_ratio": 1.530612244897959, "no_speech_prob": 1.4144737860988243e-06}, {"id": 1433, "seek": 680462, "start": 6804.62, "end": 6808.62, "text": " Then we'll do another two epochs at one in egg four", "tokens": [1396, 321, 603, 360, 1071, 732, 30992, 28346, 412, 472, 294, 3777, 1451], "temperature": 0.0, "avg_logprob": -0.1943252537701581, "compression_ratio": 1.4529411764705882, "no_speech_prob": 3.555972170943278e-06}, {"id": 1434, "seek": 680462, "start": 6809.54, "end": 6812.38, "text": " And so we've now got our loss down to 1.5", "tokens": [400, 370, 321, 600, 586, 658, 527, 4470, 760, 281, 502, 13, 20], "temperature": 0.0, "avg_logprob": -0.1943252537701581, "compression_ratio": 1.4529411764705882, "no_speech_prob": 3.555972170943278e-06}, {"id": 1435, "seek": 680462, "start": 6813.26, "end": 6815.26, "text": " So getting better and better", "tokens": [407, 1242, 1101, 293, 1101], "temperature": 0.0, "avg_logprob": -0.1943252537701581, "compression_ratio": 1.4529411764705882, "no_speech_prob": 3.555972170943278e-06}, {"id": 1436, "seek": 680462, "start": 6818.0599999999995, "end": 6820.3, "text": " So here's our get next again", "tokens": [407, 510, 311, 527, 483, 958, 797], "temperature": 0.0, "avg_logprob": -0.1943252537701581, "compression_ratio": 1.4529411764705882, "no_speech_prob": 3.555972170943278e-06}, {"id": 1437, "seek": 680462, "start": 6820.98, "end": 6826.94, "text": " Okay, and you know it's just it was the same thing so what we can now do is we can loop through", "tokens": [1033, 11, 293, 291, 458, 309, 311, 445, 309, 390, 264, 912, 551, 370, 437, 321, 393, 586, 360, 307, 321, 393, 6367, 807], "temperature": 0.0, "avg_logprob": -0.1943252537701581, "compression_ratio": 1.4529411764705882, "no_speech_prob": 3.555972170943278e-06}, {"id": 1438, "seek": 682694, "start": 6826.94, "end": 6835.54, "text": " Like 40 times calling get next each time and then each time we'll replace that input by removing the first character and", "tokens": [1743, 3356, 1413, 5141, 483, 958, 1184, 565, 293, 550, 1184, 565, 321, 603, 7406, 300, 4846, 538, 12720, 264, 700, 2517, 293], "temperature": 0.0, "avg_logprob": -0.20680186817946944, "compression_ratio": 1.9192825112107623, "no_speech_prob": 2.684190121726715e-06}, {"id": 1439, "seek": 682694, "start": 6835.86, "end": 6842.379999999999, "text": " Adding the thing that we just predicted and so that way we can like feed in a new set of eight characters again and again and again", "tokens": [31204, 264, 551, 300, 321, 445, 19147, 293, 370, 300, 636, 321, 393, 411, 3154, 294, 257, 777, 992, 295, 3180, 4342, 797, 293, 797, 293, 797], "temperature": 0.0, "avg_logprob": -0.20680186817946944, "compression_ratio": 1.9192825112107623, "no_speech_prob": 2.684190121726715e-06}, {"id": 1440, "seek": 682694, "start": 6843.139999999999, "end": 6846.099999999999, "text": " And so that way we'll call that get next in so here are", "tokens": [400, 370, 300, 636, 321, 603, 818, 300, 483, 958, 294, 370, 510, 366], "temperature": 0.0, "avg_logprob": -0.20680186817946944, "compression_ratio": 1.9192825112107623, "no_speech_prob": 2.684190121726715e-06}, {"id": 1441, "seek": 682694, "start": 6847.379999999999, "end": 6851.82, "text": " 40 characters that we've generated so we started out with for thos", "tokens": [3356, 4342, 300, 321, 600, 10833, 370, 321, 1409, 484, 365, 337, 258, 329], "temperature": 0.0, "avg_logprob": -0.20680186817946944, "compression_ratio": 1.9192825112107623, "no_speech_prob": 2.684190121726715e-06}, {"id": 1442, "seek": 682694, "start": 6851.82, "end": 6856.219999999999, "text": " We got for those of the same to the same to the same", "tokens": [492, 658, 337, 729, 295, 264, 912, 281, 264, 912, 281, 264, 912], "temperature": 0.0, "avg_logprob": -0.20680186817946944, "compression_ratio": 1.9192825112107623, "no_speech_prob": 2.684190121726715e-06}, {"id": 1443, "seek": 685622, "start": 6856.22, "end": 6859.46, "text": " You can probably guess what happens if you keep predicting the same to the same", "tokens": [509, 393, 1391, 2041, 437, 2314, 498, 291, 1066, 32884, 264, 912, 281, 264, 912], "temperature": 0.0, "avg_logprob": -0.2162573803430316, "compression_ratio": 1.574766355140187, "no_speech_prob": 1.9947249256802024e-06}, {"id": 1444, "seek": 685622, "start": 6860.14, "end": 6863.54, "text": " Right so it's you know it's doing okay", "tokens": [1779, 370, 309, 311, 291, 458, 309, 311, 884, 1392], "temperature": 0.0, "avg_logprob": -0.2162573803430316, "compression_ratio": 1.574766355140187, "no_speech_prob": 1.9947249256802024e-06}, {"id": 1445, "seek": 685622, "start": 6864.740000000001, "end": 6866.740000000001, "text": " We we now have something", "tokens": [492, 321, 586, 362, 746], "temperature": 0.0, "avg_logprob": -0.2162573803430316, "compression_ratio": 1.574766355140187, "no_speech_prob": 1.9947249256802024e-06}, {"id": 1446, "seek": 685622, "start": 6867.5, "end": 6869.22, "text": " which", "tokens": [597], "temperature": 0.0, "avg_logprob": -0.2162573803430316, "compression_ratio": 1.574766355140187, "no_speech_prob": 1.9947249256802024e-06}, {"id": 1447, "seek": 685622, "start": 6869.22, "end": 6871.18, "text": " You know", "tokens": [509, 458], "temperature": 0.0, "avg_logprob": -0.2162573803430316, "compression_ratio": 1.574766355140187, "no_speech_prob": 1.9947249256802024e-06}, {"id": 1448, "seek": 685622, "start": 6871.18, "end": 6875.4800000000005, "text": " We've basically built from scratch, and then we've said here's how", "tokens": [492, 600, 1936, 3094, 490, 8459, 11, 293, 550, 321, 600, 848, 510, 311, 577], "temperature": 0.0, "avg_logprob": -0.2162573803430316, "compression_ratio": 1.574766355140187, "no_speech_prob": 1.9947249256802024e-06}, {"id": 1449, "seek": 685622, "start": 6876.34, "end": 6882.22, "text": " Pie torch refactored it for us, so if you want to like have an interesting little homework assignment this week", "tokens": [22914, 27822, 1895, 578, 2769, 309, 337, 505, 11, 370, 498, 291, 528, 281, 411, 362, 364, 1880, 707, 14578, 15187, 341, 1243], "temperature": 0.0, "avg_logprob": -0.2162573803430316, "compression_ratio": 1.574766355140187, "no_speech_prob": 1.9947249256802024e-06}, {"id": 1450, "seek": 688222, "start": 6882.22, "end": 6887.14, "text": " Try to write your own version of an iron in class", "tokens": [6526, 281, 2464, 428, 1065, 3037, 295, 364, 6497, 294, 1508], "temperature": 0.0, "avg_logprob": -0.22381528966567096, "compression_ratio": 1.697674418604651, "no_speech_prob": 8.851546340338245e-07}, {"id": 1451, "seek": 688222, "start": 6887.66, "end": 6895.62, "text": " Right like trying to like literally like create your like you know Jeremy's iron in and then like type in here", "tokens": [1779, 411, 1382, 281, 411, 3736, 411, 1884, 428, 411, 291, 458, 17809, 311, 6497, 294, 293, 550, 411, 2010, 294, 510], "temperature": 0.0, "avg_logprob": -0.22381528966567096, "compression_ratio": 1.697674418604651, "no_speech_prob": 8.851546340338245e-07}, {"id": 1452, "seek": 688222, "start": 6896.18, "end": 6900.64, "text": " Jeremy's iron in or in your case. Maybe your name's not Jeremy which is okay, too", "tokens": [17809, 311, 6497, 294, 420, 294, 428, 1389, 13, 2704, 428, 1315, 311, 406, 17809, 597, 307, 1392, 11, 886], "temperature": 0.0, "avg_logprob": -0.22381528966567096, "compression_ratio": 1.697674418604651, "no_speech_prob": 8.851546340338245e-07}, {"id": 1453, "seek": 688222, "start": 6901.26, "end": 6908.0, "text": " And then get it to run writing your implementation of that class from scratch without looking at the pie torch source code", "tokens": [400, 550, 483, 309, 281, 1190, 3579, 428, 11420, 295, 300, 1508, 490, 8459, 1553, 1237, 412, 264, 1730, 27822, 4009, 3089], "temperature": 0.0, "avg_logprob": -0.22381528966567096, "compression_ratio": 1.697674418604651, "no_speech_prob": 8.851546340338245e-07}, {"id": 1454, "seek": 690800, "start": 6908.0, "end": 6911.98, "text": " You know like basically it's just a case of like going up and seeing", "tokens": [509, 458, 411, 1936, 309, 311, 445, 257, 1389, 295, 411, 516, 493, 293, 2577], "temperature": 0.0, "avg_logprob": -0.21061429669780116, "compression_ratio": 1.5982142857142858, "no_speech_prob": 1.994722651943448e-06}, {"id": 1455, "seek": 690800, "start": 6912.4, "end": 6918.12, "text": " What we did back here right and like make sure you get the same answers and confirm that you do", "tokens": [708, 321, 630, 646, 510, 558, 293, 411, 652, 988, 291, 483, 264, 912, 6338, 293, 9064, 300, 291, 360], "temperature": 0.0, "avg_logprob": -0.21061429669780116, "compression_ratio": 1.5982142857142858, "no_speech_prob": 1.994722651943448e-06}, {"id": 1456, "seek": 690800, "start": 6918.12, "end": 6920.12, "text": " So that's kind of a good little test", "tokens": [407, 300, 311, 733, 295, 257, 665, 707, 1500], "temperature": 0.0, "avg_logprob": -0.21061429669780116, "compression_ratio": 1.5982142857142858, "no_speech_prob": 1.994722651943448e-06}, {"id": 1457, "seek": 690800, "start": 6920.36, "end": 6925.48, "text": " Simp very simple at all assignment, but I think you'll feel really good when you seem like oh", "tokens": [3998, 79, 588, 2199, 412, 439, 15187, 11, 457, 286, 519, 291, 603, 841, 534, 665, 562, 291, 1643, 411, 1954], "temperature": 0.0, "avg_logprob": -0.21061429669780116, "compression_ratio": 1.5982142857142858, "no_speech_prob": 1.994722651943448e-06}, {"id": 1458, "seek": 690800, "start": 6925.48, "end": 6927.92, "text": " I've just really implemented and end on our name", "tokens": [286, 600, 445, 534, 12270, 293, 917, 322, 527, 1315], "temperature": 0.0, "avg_logprob": -0.21061429669780116, "compression_ratio": 1.5982142857142858, "no_speech_prob": 1.994722651943448e-06}, {"id": 1459, "seek": 690800, "start": 6930.8, "end": 6932.8, "text": " All right, so", "tokens": [1057, 558, 11, 370], "temperature": 0.0, "avg_logprob": -0.21061429669780116, "compression_ratio": 1.5982142857142858, "no_speech_prob": 1.994722651943448e-06}, {"id": 1460, "seek": 693280, "start": 6932.8, "end": 6940.56, "text": " I'm gonna do one other thing when I switched from this one when I've moved the car one input inside the dotted line", "tokens": [286, 478, 799, 360, 472, 661, 551, 562, 286, 16858, 490, 341, 472, 562, 286, 600, 4259, 264, 1032, 472, 4846, 1854, 264, 37459, 1622], "temperature": 0.0, "avg_logprob": -0.2495864300017661, "compression_ratio": 1.6768558951965065, "no_speech_prob": 3.7853058074688306e-06}, {"id": 1461, "seek": 693280, "start": 6940.56, "end": 6943.360000000001, "text": " Right this dotted rectangle represents the thing I'm repeating I", "tokens": [1779, 341, 37459, 21930, 8855, 264, 551, 286, 478, 18617, 286], "temperature": 0.0, "avg_logprob": -0.2495864300017661, "compression_ratio": 1.6768558951965065, "no_speech_prob": 3.7853058074688306e-06}, {"id": 1462, "seek": 693280, "start": 6944.4800000000005, "end": 6947.4400000000005, "text": " Also, what's the triangle the output I?", "tokens": [2743, 11, 437, 311, 264, 13369, 264, 5598, 286, 30], "temperature": 0.0, "avg_logprob": -0.2495864300017661, "compression_ratio": 1.6768558951965065, "no_speech_prob": 3.7853058074688306e-06}, {"id": 1463, "seek": 693280, "start": 6948.04, "end": 6950.04, "text": " Move that inside as well", "tokens": [10475, 300, 1854, 382, 731], "temperature": 0.0, "avg_logprob": -0.2495864300017661, "compression_ratio": 1.6768558951965065, "no_speech_prob": 3.7853058074688306e-06}, {"id": 1464, "seek": 693280, "start": 6950.26, "end": 6952.26, "text": " Now that's a big difference", "tokens": [823, 300, 311, 257, 955, 2649], "temperature": 0.0, "avg_logprob": -0.2495864300017661, "compression_ratio": 1.6768558951965065, "no_speech_prob": 3.7853058074688306e-06}, {"id": 1465, "seek": 693280, "start": 6952.4400000000005, "end": 6955.2, "text": " Because now what I've actually done is", "tokens": [1436, 586, 437, 286, 600, 767, 1096, 307], "temperature": 0.0, "avg_logprob": -0.2495864300017661, "compression_ratio": 1.6768558951965065, "no_speech_prob": 3.7853058074688306e-06}, {"id": 1466, "seek": 695520, "start": 6955.2, "end": 6962.72, "text": " I'm actually saying spit out an output after every one of these circles", "tokens": [286, 478, 767, 1566, 22127, 484, 364, 5598, 934, 633, 472, 295, 613, 13040], "temperature": 0.0, "avg_logprob": -0.22558989891639122, "compression_ratio": 2.015957446808511, "no_speech_prob": 1.5779568229845609e-06}, {"id": 1467, "seek": 695520, "start": 6962.96, "end": 6966.16, "text": " So spit out an output here and here and here", "tokens": [407, 22127, 484, 364, 5598, 510, 293, 510, 293, 510], "temperature": 0.0, "avg_logprob": -0.22558989891639122, "compression_ratio": 2.015957446808511, "no_speech_prob": 1.5779568229845609e-06}, {"id": 1468, "seek": 695520, "start": 6967.72, "end": 6973.44, "text": " Right so in other words if I have a three character input. I'm going to spit out a three character output", "tokens": [1779, 370, 294, 661, 2283, 498, 286, 362, 257, 1045, 2517, 4846, 13, 286, 478, 516, 281, 22127, 484, 257, 1045, 2517, 5598], "temperature": 0.0, "avg_logprob": -0.22558989891639122, "compression_ratio": 2.015957446808511, "no_speech_prob": 1.5779568229845609e-06}, {"id": 1469, "seek": 695520, "start": 6973.44, "end": 6979.08, "text": " I'm saying after character one this will be next after character two this would be next up to character three this will be next", "tokens": [286, 478, 1566, 934, 2517, 472, 341, 486, 312, 958, 934, 2517, 732, 341, 576, 312, 958, 493, 281, 2517, 1045, 341, 486, 312, 958], "temperature": 0.0, "avg_logprob": -0.22558989891639122, "compression_ratio": 2.015957446808511, "no_speech_prob": 1.5779568229845609e-06}, {"id": 1470, "seek": 695520, "start": 6981.12, "end": 6983.12, "text": " So again nothing", "tokens": [407, 797, 1825], "temperature": 0.0, "avg_logprob": -0.22558989891639122, "compression_ratio": 2.015957446808511, "no_speech_prob": 1.5779568229845609e-06}, {"id": 1471, "seek": 695520, "start": 6983.16, "end": 6984.84, "text": " different I", "tokens": [819, 286], "temperature": 0.0, "avg_logprob": -0.22558989891639122, "compression_ratio": 2.015957446808511, "no_speech_prob": 1.5779568229845609e-06}, {"id": 1472, "seek": 698484, "start": 6984.84, "end": 6987.8, "text": " And again this you know if you wanted to go a bit further with the assignment", "tokens": [400, 797, 341, 291, 458, 498, 291, 1415, 281, 352, 257, 857, 3052, 365, 264, 15187], "temperature": 0.0, "avg_logprob": -0.21992750601335007, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.505698151049728e-06}, {"id": 1473, "seek": 698484, "start": 6988.360000000001, "end": 6990.360000000001, "text": " You could write this by hand as well", "tokens": [509, 727, 2464, 341, 538, 1011, 382, 731], "temperature": 0.0, "avg_logprob": -0.21992750601335007, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.505698151049728e-06}, {"id": 1474, "seek": 698484, "start": 6991.0, "end": 6995.360000000001, "text": " But basically what we're saying is in the for loop would be saying like", "tokens": [583, 1936, 437, 321, 434, 1566, 307, 294, 264, 337, 6367, 576, 312, 1566, 411], "temperature": 0.0, "avg_logprob": -0.21992750601335007, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.505698151049728e-06}, {"id": 1475, "seek": 698484, "start": 6996.24, "end": 6998.08, "text": " You know", "tokens": [509, 458], "temperature": 0.0, "avg_logprob": -0.21992750601335007, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.505698151049728e-06}, {"id": 1476, "seek": 698484, "start": 6998.08, "end": 7004.2, "text": " Results equals some empty list right and then we'd be going through and rather than returning that", "tokens": [5015, 33361, 6915, 512, 6707, 1329, 558, 293, 550, 321, 1116, 312, 516, 807, 293, 2831, 813, 12678, 300], "temperature": 0.0, "avg_logprob": -0.21992750601335007, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.505698151049728e-06}, {"id": 1477, "seek": 698484, "start": 7004.72, "end": 7006.72, "text": " We'd instead be saying", "tokens": [492, 1116, 2602, 312, 1566], "temperature": 0.0, "avg_logprob": -0.21992750601335007, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.505698151049728e-06}, {"id": 1478, "seek": 698484, "start": 7006.84, "end": 7008.4400000000005, "text": " You know", "tokens": [509, 458], "temperature": 0.0, "avg_logprob": -0.21992750601335007, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.505698151049728e-06}, {"id": 1479, "seek": 698484, "start": 7008.4400000000005, "end": 7010.4400000000005, "text": " results dot append", "tokens": [3542, 5893, 34116], "temperature": 0.0, "avg_logprob": -0.21992750601335007, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.505698151049728e-06}, {"id": 1480, "seek": 701044, "start": 7010.44, "end": 7017.719999999999, "text": " That right and then like return whatever torch dot stack", "tokens": [663, 558, 293, 550, 411, 2736, 2035, 27822, 5893, 8630], "temperature": 0.0, "avg_logprob": -0.2818903923034668, "compression_ratio": 1.5052083333333333, "no_speech_prob": 1.4367457197295153e-06}, {"id": 1481, "seek": 701044, "start": 7020.2, "end": 7022.919999999999, "text": " Something like that right that it made me right I'm not quite sure", "tokens": [6595, 411, 300, 558, 300, 309, 1027, 385, 558, 286, 478, 406, 1596, 988], "temperature": 0.0, "avg_logprob": -0.2818903923034668, "compression_ratio": 1.5052083333333333, "no_speech_prob": 1.4367457197295153e-06}, {"id": 1482, "seek": 701044, "start": 7023.5599999999995, "end": 7026.0, "text": " So now you know we now have like", "tokens": [407, 586, 291, 458, 321, 586, 362, 411], "temperature": 0.0, "avg_logprob": -0.2818903923034668, "compression_ratio": 1.5052083333333333, "no_speech_prob": 1.4367457197295153e-06}, {"id": 1483, "seek": 701044, "start": 7026.719999999999, "end": 7029.48, "text": " Every step we've created an output", "tokens": [2048, 1823, 321, 600, 2942, 364, 5598], "temperature": 0.0, "avg_logprob": -0.2818903923034668, "compression_ratio": 1.5052083333333333, "no_speech_prob": 1.4367457197295153e-06}, {"id": 1484, "seek": 701044, "start": 7030.12, "end": 7034.839999999999, "text": " Okay, so which is basically this picture and so the reason", "tokens": [1033, 11, 370, 597, 307, 1936, 341, 3036, 293, 370, 264, 1778], "temperature": 0.0, "avg_logprob": -0.2818903923034668, "compression_ratio": 1.5052083333333333, "no_speech_prob": 1.4367457197295153e-06}, {"id": 1485, "seek": 701044, "start": 7035.5599999999995, "end": 7037.5599999999995, "text": " Was lots of reasons that's interesting", "tokens": [3027, 3195, 295, 4112, 300, 311, 1880], "temperature": 0.0, "avg_logprob": -0.2818903923034668, "compression_ratio": 1.5052083333333333, "no_speech_prob": 1.4367457197295153e-06}, {"id": 1486, "seek": 703756, "start": 7037.56, "end": 7043.240000000001, "text": " But I think the main reason right now that's interesting is that you probably noticed", "tokens": [583, 286, 519, 264, 2135, 1778, 558, 586, 300, 311, 1880, 307, 300, 291, 1391, 5694], "temperature": 0.0, "avg_logprob": -0.23264873027801514, "compression_ratio": 1.545, "no_speech_prob": 9.27634289382695e-07}, {"id": 1487, "seek": 703756, "start": 7045.4800000000005, "end": 7047.4800000000005, "text": " This", "tokens": [639], "temperature": 0.0, "avg_logprob": -0.23264873027801514, "compression_ratio": 1.545, "no_speech_prob": 9.27634289382695e-07}, {"id": 1488, "seek": 703756, "start": 7048.6, "end": 7050.6, "text": " This approach to", "tokens": [639, 3109, 281], "temperature": 0.0, "avg_logprob": -0.23264873027801514, "compression_ratio": 1.545, "no_speech_prob": 9.27634289382695e-07}, {"id": 1489, "seek": 703756, "start": 7050.6, "end": 7054.160000000001, "text": " Dealing with our data seems terribly inefficient like we're grabbing", "tokens": [1346, 4270, 365, 527, 1412, 2544, 22903, 43495, 411, 321, 434, 23771], "temperature": 0.0, "avg_logprob": -0.23264873027801514, "compression_ratio": 1.545, "no_speech_prob": 9.27634289382695e-07}, {"id": 1490, "seek": 703756, "start": 7055.0, "end": 7056.92, "text": " the first eight", "tokens": [264, 700, 3180], "temperature": 0.0, "avg_logprob": -0.23264873027801514, "compression_ratio": 1.545, "no_speech_prob": 9.27634289382695e-07}, {"id": 1491, "seek": 703756, "start": 7056.92, "end": 7059.080000000001, "text": " Right, but then this next set", "tokens": [1779, 11, 457, 550, 341, 958, 992], "temperature": 0.0, "avg_logprob": -0.23264873027801514, "compression_ratio": 1.545, "no_speech_prob": 9.27634289382695e-07}, {"id": 1492, "seek": 703756, "start": 7060.0, "end": 7062.54, "text": " All but one of them overlap the previous one", "tokens": [1057, 457, 472, 295, 552, 19959, 264, 3894, 472], "temperature": 0.0, "avg_logprob": -0.23264873027801514, "compression_ratio": 1.545, "no_speech_prob": 9.27634289382695e-07}, {"id": 1493, "seek": 703756, "start": 7063.72, "end": 7065.96, "text": " Right so we're kind of like recalculating", "tokens": [1779, 370, 321, 434, 733, 295, 411, 850, 304, 2444, 990], "temperature": 0.0, "avg_logprob": -0.23264873027801514, "compression_ratio": 1.545, "no_speech_prob": 9.27634289382695e-07}, {"id": 1494, "seek": 706596, "start": 7065.96, "end": 7071.52, "text": " Calculating the exact same embeddings seven out of eight of them are going to be exact same embeddings", "tokens": [3511, 2444, 990, 264, 1900, 912, 12240, 29432, 3407, 484, 295, 3180, 295, 552, 366, 516, 281, 312, 1900, 912, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.1894403515440045, "compression_ratio": 1.8425925925925926, "no_speech_prob": 1.7603351807338186e-06}, {"id": 1495, "seek": 706596, "start": 7072.2, "end": 7074.26, "text": " Right exact same transitions", "tokens": [1779, 1900, 912, 23767], "temperature": 0.0, "avg_logprob": -0.1894403515440045, "compression_ratio": 1.8425925925925926, "no_speech_prob": 1.7603351807338186e-06}, {"id": 1496, "seek": 706596, "start": 7075.0, "end": 7078.36, "text": " It kind of seems weird to like do all this calculation", "tokens": [467, 733, 295, 2544, 3657, 281, 411, 360, 439, 341, 17108], "temperature": 0.0, "avg_logprob": -0.1894403515440045, "compression_ratio": 1.8425925925925926, "no_speech_prob": 1.7603351807338186e-06}, {"id": 1497, "seek": 706596, "start": 7079.16, "end": 7084.46, "text": " To just predict one thing and then go back and recalculate seven out of eight of them and add one more to the end", "tokens": [1407, 445, 6069, 472, 551, 293, 550, 352, 646, 293, 850, 304, 2444, 473, 3407, 484, 295, 3180, 295, 552, 293, 909, 472, 544, 281, 264, 917], "temperature": 0.0, "avg_logprob": -0.1894403515440045, "compression_ratio": 1.8425925925925926, "no_speech_prob": 1.7603351807338186e-06}, {"id": 1498, "seek": 706596, "start": 7084.72, "end": 7090.36, "text": " To calculate the next thing right so the basic idea then is to say well. Let's not do it that way", "tokens": [1407, 8873, 264, 958, 551, 558, 370, 264, 3875, 1558, 550, 307, 281, 584, 731, 13, 961, 311, 406, 360, 309, 300, 636], "temperature": 0.0, "avg_logprob": -0.1894403515440045, "compression_ratio": 1.8425925925925926, "no_speech_prob": 1.7603351807338186e-06}, {"id": 1499, "seek": 709036, "start": 7090.36, "end": 7098.5199999999995, "text": " Instead let's taking non overlapping sets of characters", "tokens": [7156, 718, 311, 1940, 2107, 33535, 6352, 295, 4342], "temperature": 0.0, "avg_logprob": -0.27917064764560795, "compression_ratio": 1.8352272727272727, "no_speech_prob": 1.051145545716281e-06}, {"id": 1500, "seek": 709036, "start": 7099.719999999999, "end": 7103.92, "text": " All right, so like so here is our first eight characters", "tokens": [1057, 558, 11, 370, 411, 370, 510, 307, 527, 700, 3180, 4342], "temperature": 0.0, "avg_logprob": -0.27917064764560795, "compression_ratio": 1.8352272727272727, "no_speech_prob": 1.051145545716281e-06}, {"id": 1501, "seek": 709036, "start": 7104.5599999999995, "end": 7109.719999999999, "text": " Here is the next eight characters here are the next eight characters so like if you read this", "tokens": [1692, 307, 264, 958, 3180, 4342, 510, 366, 264, 958, 3180, 4342, 370, 411, 498, 291, 1401, 341], "temperature": 0.0, "avg_logprob": -0.27917064764560795, "compression_ratio": 1.8352272727272727, "no_speech_prob": 1.051145545716281e-06}, {"id": 1502, "seek": 709036, "start": 7110.36, "end": 7113.32, "text": " Top left to bottom right that would be the whole", "tokens": [8840, 1411, 281, 2767, 558, 300, 576, 312, 264, 1379], "temperature": 0.0, "avg_logprob": -0.27917064764560795, "compression_ratio": 1.8352272727272727, "no_speech_prob": 1.051145545716281e-06}, {"id": 1503, "seek": 709036, "start": 7114.04, "end": 7115.28, "text": " Nietzsche", "tokens": [36583, 89, 12287], "temperature": 0.0, "avg_logprob": -0.27917064764560795, "compression_ratio": 1.8352272727272727, "no_speech_prob": 1.051145545716281e-06}, {"id": 1504, "seek": 709036, "start": 7115.28, "end": 7117.28, "text": " Right and so then", "tokens": [1779, 293, 370, 550], "temperature": 0.0, "avg_logprob": -0.27917064764560795, "compression_ratio": 1.8352272727272727, "no_speech_prob": 1.051145545716281e-06}, {"id": 1505, "seek": 711728, "start": 7117.28, "end": 7123.92, "text": " If these are the first eight characters then offset this by one starting here", "tokens": [759, 613, 366, 264, 700, 3180, 4342, 550, 18687, 341, 538, 472, 2891, 510], "temperature": 0.0, "avg_logprob": -0.2170116901397705, "compression_ratio": 1.6900584795321638, "no_speech_prob": 1.4823539231656468e-06}, {"id": 1506, "seek": 711728, "start": 7124.36, "end": 7126.36, "text": " That's a list of outputs", "tokens": [663, 311, 257, 1329, 295, 23930], "temperature": 0.0, "avg_logprob": -0.2170116901397705, "compression_ratio": 1.6900584795321638, "no_speech_prob": 1.4823539231656468e-06}, {"id": 1507, "seek": 711728, "start": 7127.599999999999, "end": 7133.9, "text": " Right so after we see characters zero through seven we should predict characters one through eight", "tokens": [1779, 370, 934, 321, 536, 4342, 4018, 807, 3407, 321, 820, 6069, 4342, 472, 807, 3180], "temperature": 0.0, "avg_logprob": -0.2170116901397705, "compression_ratio": 1.6900584795321638, "no_speech_prob": 1.4823539231656468e-06}, {"id": 1508, "seek": 711728, "start": 7134.5599999999995, "end": 7141.5599999999995, "text": " That makes sense so after 40 should come 42 as it did after 42 should come 29", "tokens": [663, 1669, 2020, 370, 934, 3356, 820, 808, 14034, 382, 309, 630, 934, 14034, 820, 808, 9413], "temperature": 0.0, "avg_logprob": -0.2170116901397705, "compression_ratio": 1.6900584795321638, "no_speech_prob": 1.4823539231656468e-06}, {"id": 1509, "seek": 711728, "start": 7142.08, "end": 7143.599999999999, "text": " as it did", "tokens": [382, 309, 630], "temperature": 0.0, "avg_logprob": -0.2170116901397705, "compression_ratio": 1.6900584795321638, "no_speech_prob": 1.4823539231656468e-06}, {"id": 1510, "seek": 714360, "start": 7143.6, "end": 7149.22, "text": " Right and so now that can be our inputs and labels for that model", "tokens": [1779, 293, 370, 586, 300, 393, 312, 527, 15743, 293, 16949, 337, 300, 2316], "temperature": 0.0, "avg_logprob": -0.17323832358083419, "compression_ratio": 1.4966887417218544, "no_speech_prob": 6.475937084360339e-07}, {"id": 1511, "seek": 714360, "start": 7150.400000000001, "end": 7152.400000000001, "text": " And so it shouldn't be any", "tokens": [400, 370, 309, 4659, 380, 312, 604], "temperature": 0.0, "avg_logprob": -0.17323832358083419, "compression_ratio": 1.4966887417218544, "no_speech_prob": 6.475937084360339e-07}, {"id": 1512, "seek": 714360, "start": 7153.4400000000005, "end": 7155.4400000000005, "text": " More or less accurate", "tokens": [5048, 420, 1570, 8559], "temperature": 0.0, "avg_logprob": -0.17323832358083419, "compression_ratio": 1.4966887417218544, "no_speech_prob": 6.475937084360339e-07}, {"id": 1513, "seek": 714360, "start": 7155.76, "end": 7158.64, "text": " It should just be the same right pretty much", "tokens": [467, 820, 445, 312, 264, 912, 558, 1238, 709], "temperature": 0.0, "avg_logprob": -0.17323832358083419, "compression_ratio": 1.4966887417218544, "no_speech_prob": 6.475937084360339e-07}, {"id": 1514, "seek": 714360, "start": 7159.96, "end": 7161.96, "text": " But it should allow us to do it more efficiently", "tokens": [583, 309, 820, 2089, 505, 281, 360, 309, 544, 19621], "temperature": 0.0, "avg_logprob": -0.17323832358083419, "compression_ratio": 1.4966887417218544, "no_speech_prob": 6.475937084360339e-07}, {"id": 1515, "seek": 716196, "start": 7161.96, "end": 7169.96, "text": " So let's try that all right", "tokens": [407, 718, 311, 853, 300, 439, 558], "temperature": 0.0, "avg_logprob": -0.3399939775466919, "compression_ratio": 1.2710280373831775, "no_speech_prob": 6.786724497942487e-07}, {"id": 1516, "seek": 716196, "start": 7175.12, "end": 7177.78, "text": " So I mentioned last time that we had a", "tokens": [407, 286, 2835, 1036, 565, 300, 321, 632, 257], "temperature": 0.0, "avg_logprob": -0.3399939775466919, "compression_ratio": 1.2710280373831775, "no_speech_prob": 6.786724497942487e-07}, {"id": 1517, "seek": 716196, "start": 7181.04, "end": 7184.44, "text": " Minus one index here because we just wanted to grab the last", "tokens": [2829, 301, 472, 8186, 510, 570, 321, 445, 1415, 281, 4444, 264, 1036], "temperature": 0.0, "avg_logprob": -0.3399939775466919, "compression_ratio": 1.2710280373831775, "no_speech_prob": 6.786724497942487e-07}, {"id": 1518, "seek": 716196, "start": 7185.96, "end": 7187.24, "text": " triangle", "tokens": [13369], "temperature": 0.0, "avg_logprob": -0.3399939775466919, "compression_ratio": 1.2710280373831775, "no_speech_prob": 6.786724497942487e-07}, {"id": 1519, "seek": 718724, "start": 7187.24, "end": 7194.44, "text": " Okay, so in this case. We're going to grab all the triangles, so this this is actually the way in n dot RNN creates things", "tokens": [1033, 11, 370, 294, 341, 1389, 13, 492, 434, 516, 281, 4444, 439, 264, 29896, 11, 370, 341, 341, 307, 767, 264, 636, 294, 297, 5893, 45702, 45, 7829, 721], "temperature": 0.0, "avg_logprob": -0.19879720377367596, "compression_ratio": 1.5897435897435896, "no_speech_prob": 7.811472073626646e-07}, {"id": 1520, "seek": 718724, "start": 7195.08, "end": 7197.08, "text": " We we only kept the last one", "tokens": [492, 321, 787, 4305, 264, 1036, 472], "temperature": 0.0, "avg_logprob": -0.19879720377367596, "compression_ratio": 1.5897435897435896, "no_speech_prob": 7.811472073626646e-07}, {"id": 1521, "seek": 718724, "start": 7197.5599999999995, "end": 7199.5599999999995, "text": " But this time we're going to keep all of them", "tokens": [583, 341, 565, 321, 434, 516, 281, 1066, 439, 295, 552], "temperature": 0.0, "avg_logprob": -0.19879720377367596, "compression_ratio": 1.5897435897435896, "no_speech_prob": 7.811472073626646e-07}, {"id": 1522, "seek": 718724, "start": 7205.28, "end": 7210.36, "text": " So we've made one change which is to remove that minus one other than that", "tokens": [407, 321, 600, 1027, 472, 1319, 597, 307, 281, 4159, 300, 3175, 472, 661, 813, 300], "temperature": 0.0, "avg_logprob": -0.19879720377367596, "compression_ratio": 1.5897435897435896, "no_speech_prob": 7.811472073626646e-07}, {"id": 1523, "seek": 718724, "start": 7211.24, "end": 7213.24, "text": " This is the exact same", "tokens": [639, 307, 264, 1900, 912], "temperature": 0.0, "avg_logprob": -0.19879720377367596, "compression_ratio": 1.5897435897435896, "no_speech_prob": 7.811472073626646e-07}, {"id": 1524, "seek": 718724, "start": 7213.639999999999, "end": 7215.639999999999, "text": " code as before", "tokens": [3089, 382, 949], "temperature": 0.0, "avg_logprob": -0.19879720377367596, "compression_ratio": 1.5897435897435896, "no_speech_prob": 7.811472073626646e-07}, {"id": 1525, "seek": 721564, "start": 7215.64, "end": 7217.64, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.20413992140028211, "compression_ratio": 1.424731182795699, "no_speech_prob": 5.422196409199387e-06}, {"id": 1526, "seek": 721564, "start": 7219.84, "end": 7221.84, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.20413992140028211, "compression_ratio": 1.424731182795699, "no_speech_prob": 5.422196409199387e-06}, {"id": 1527, "seek": 721564, "start": 7222.52, "end": 7229.360000000001, "text": " There's nothing much to show you here. I mean except of course at this time if we look at the the labels it's now", "tokens": [821, 311, 1825, 709, 281, 855, 291, 510, 13, 286, 914, 3993, 295, 1164, 412, 341, 565, 498, 321, 574, 412, 264, 264, 16949, 309, 311, 586], "temperature": 0.0, "avg_logprob": -0.20413992140028211, "compression_ratio": 1.424731182795699, "no_speech_prob": 5.422196409199387e-06}, {"id": 1528, "seek": 721564, "start": 7230.360000000001, "end": 7235.8, "text": " 512 by 8 that because we're trying to predict eight things every time through", "tokens": [1025, 4762, 538, 1649, 300, 570, 321, 434, 1382, 281, 6069, 3180, 721, 633, 565, 807], "temperature": 0.0, "avg_logprob": -0.20413992140028211, "compression_ratio": 1.424731182795699, "no_speech_prob": 5.422196409199387e-06}, {"id": 1529, "seek": 721564, "start": 7237.8, "end": 7243.280000000001, "text": " So there is one complexity here, which is that we want to use the", "tokens": [407, 456, 307, 472, 14024, 510, 11, 597, 307, 300, 321, 528, 281, 764, 264], "temperature": 0.0, "avg_logprob": -0.20413992140028211, "compression_ratio": 1.424731182795699, "no_speech_prob": 5.422196409199387e-06}, {"id": 1530, "seek": 724328, "start": 7243.28, "end": 7244.639999999999, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.35899919933742946, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.7853053527214797e-06}, {"id": 1531, "seek": 724328, "start": 7244.639999999999, "end": 7246.639999999999, "text": " negative log likelihood", "tokens": [3671, 3565, 22119], "temperature": 0.0, "avg_logprob": -0.35899919933742946, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.7853053527214797e-06}, {"id": 1532, "seek": 724328, "start": 7247.16, "end": 7248.679999999999, "text": " loss function", "tokens": [4470, 2445], "temperature": 0.0, "avg_logprob": -0.35899919933742946, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.7853053527214797e-06}, {"id": 1533, "seek": 724328, "start": 7248.679999999999, "end": 7254.639999999999, "text": " As before right but the leg of loss likelihood loss function just like RMSE", "tokens": [1018, 949, 558, 457, 264, 1676, 295, 4470, 22119, 4470, 2445, 445, 411, 23790, 5879], "temperature": 0.0, "avg_logprob": -0.35899919933742946, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.7853053527214797e-06}, {"id": 1534, "seek": 724328, "start": 7255.28, "end": 7257.28, "text": " expects to receive two", "tokens": [33280, 281, 4774, 732], "temperature": 0.0, "avg_logprob": -0.35899919933742946, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.7853053527214797e-06}, {"id": 1535, "seek": 724328, "start": 7257.36, "end": 7265.96, "text": " Rank one tensors actually with the mini batch access to rank two tensors right so to two mini batches of vectors", "tokens": [35921, 472, 10688, 830, 767, 365, 264, 8382, 15245, 2105, 281, 6181, 732, 10688, 830, 558, 370, 281, 732, 8382, 15245, 279, 295, 18875], "temperature": 0.0, "avg_logprob": -0.35899919933742946, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.7853053527214797e-06}, {"id": 1536, "seek": 724328, "start": 7267.5599999999995, "end": 7271.32, "text": " Problem is that we've got eight", "tokens": [11676, 307, 300, 321, 600, 658, 3180], "temperature": 0.0, "avg_logprob": -0.35899919933742946, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.7853053527214797e-06}, {"id": 1537, "seek": 727132, "start": 7271.32, "end": 7272.88, "text": " eight", "tokens": [3180], "temperature": 0.0, "avg_logprob": -0.24354330364026522, "compression_ratio": 1.7653061224489797, "no_speech_prob": 6.681507329631131e-07}, {"id": 1538, "seek": 727132, "start": 7272.88, "end": 7279.4, "text": " Time steps you know eight characters in an RNN. We call it a time step right we have eight time steps", "tokens": [6161, 4439, 291, 458, 3180, 4342, 294, 364, 45702, 45, 13, 492, 818, 309, 257, 565, 1823, 558, 321, 362, 3180, 565, 4439], "temperature": 0.0, "avg_logprob": -0.24354330364026522, "compression_ratio": 1.7653061224489797, "no_speech_prob": 6.681507329631131e-07}, {"id": 1539, "seek": 727132, "start": 7280.2, "end": 7282.2, "text": " And then for each one we have 84", "tokens": [400, 550, 337, 1184, 472, 321, 362, 29018], "temperature": 0.0, "avg_logprob": -0.24354330364026522, "compression_ratio": 1.7653061224489797, "no_speech_prob": 6.681507329631131e-07}, {"id": 1540, "seek": 727132, "start": 7282.92, "end": 7287.04, "text": " Probabilities we have the probability for every single", "tokens": [8736, 6167, 321, 362, 264, 8482, 337, 633, 2167], "temperature": 0.0, "avg_logprob": -0.24354330364026522, "compression_ratio": 1.7653061224489797, "no_speech_prob": 6.681507329631131e-07}, {"id": 1541, "seek": 727132, "start": 7288.5199999999995, "end": 7292.92, "text": " One of those eight time steps, and then we have that for each of our", "tokens": [1485, 295, 729, 3180, 565, 4439, 11, 293, 550, 321, 362, 300, 337, 1184, 295, 527], "temperature": 0.0, "avg_logprob": -0.24354330364026522, "compression_ratio": 1.7653061224489797, "no_speech_prob": 6.681507329631131e-07}, {"id": 1542, "seek": 727132, "start": 7294.08, "end": 7297.84, "text": " 512 items in the mini-batch, so we have a rank three tensor", "tokens": [1025, 4762, 4754, 294, 264, 8382, 12, 65, 852, 11, 370, 321, 362, 257, 6181, 1045, 40863], "temperature": 0.0, "avg_logprob": -0.24354330364026522, "compression_ratio": 1.7653061224489797, "no_speech_prob": 6.681507329631131e-07}, {"id": 1543, "seek": 727132, "start": 7298.5599999999995, "end": 7300.5599999999995, "text": " Not a rank two tensor", "tokens": [1726, 257, 6181, 732, 40863], "temperature": 0.0, "avg_logprob": -0.24354330364026522, "compression_ratio": 1.7653061224489797, "no_speech_prob": 6.681507329631131e-07}, {"id": 1544, "seek": 730056, "start": 7300.56, "end": 7306.400000000001, "text": " So that means that the negative log likelihood loss function is going to spit out an error", "tokens": [407, 300, 1355, 300, 264, 3671, 3565, 22119, 4470, 2445, 307, 516, 281, 22127, 484, 364, 6713], "temperature": 0.0, "avg_logprob": -0.18033864082546408, "compression_ratio": 1.6481481481481481, "no_speech_prob": 2.601608457553084e-06}, {"id": 1545, "seek": 730056, "start": 7307.280000000001, "end": 7310.84, "text": " Now frankly, I think this is kind of dumb. You know I think it would be better if", "tokens": [823, 11939, 11, 286, 519, 341, 307, 733, 295, 10316, 13, 509, 458, 286, 519, 309, 576, 312, 1101, 498], "temperature": 0.0, "avg_logprob": -0.18033864082546408, "compression_ratio": 1.6481481481481481, "no_speech_prob": 2.601608457553084e-06}, {"id": 1546, "seek": 730056, "start": 7311.96, "end": 7319.72, "text": " Pi torch had written their loss functions in such a way that they didn't care at all about rank, and they just applied it", "tokens": [17741, 27822, 632, 3720, 641, 4470, 6828, 294, 1270, 257, 636, 300, 436, 994, 380, 1127, 412, 439, 466, 6181, 11, 293, 436, 445, 6456, 309], "temperature": 0.0, "avg_logprob": -0.18033864082546408, "compression_ratio": 1.6481481481481481, "no_speech_prob": 2.601608457553084e-06}, {"id": 1547, "seek": 730056, "start": 7320.400000000001, "end": 7325.6, "text": " To whatever rank you gave it, but for now at least it does care about Rick", "tokens": [1407, 2035, 6181, 291, 2729, 309, 11, 457, 337, 586, 412, 1935, 309, 775, 1127, 466, 11224], "temperature": 0.0, "avg_logprob": -0.18033864082546408, "compression_ratio": 1.6481481481481481, "no_speech_prob": 2.601608457553084e-06}, {"id": 1548, "seek": 730056, "start": 7326.080000000001, "end": 7329.320000000001, "text": " But the nice thing is I get to show you how to write a custom loss function", "tokens": [583, 264, 1481, 551, 307, 286, 483, 281, 855, 291, 577, 281, 2464, 257, 2375, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.18033864082546408, "compression_ratio": 1.6481481481481481, "no_speech_prob": 2.601608457553084e-06}, {"id": 1549, "seek": 732932, "start": 7329.32, "end": 7335.28, "text": " Okay, so we're going to create a special negative log likelihood loss function for sequences", "tokens": [1033, 11, 370, 321, 434, 516, 281, 1884, 257, 2121, 3671, 3565, 22119, 4470, 2445, 337, 22978], "temperature": 0.0, "avg_logprob": -0.2680985362259383, "compression_ratio": 1.891304347826087, "no_speech_prob": 1.228916403306357e-06}, {"id": 1550, "seek": 732932, "start": 7335.5, "end": 7341.719999999999, "text": " Okay, and so it's going to take an input in the target, and it's going to call f dot negative log likelihood loss", "tokens": [1033, 11, 293, 370, 309, 311, 516, 281, 747, 364, 4846, 294, 264, 3779, 11, 293, 309, 311, 516, 281, 818, 283, 5893, 3671, 3565, 22119, 4470], "temperature": 0.0, "avg_logprob": -0.2680985362259383, "compression_ratio": 1.891304347826087, "no_speech_prob": 1.228916403306357e-06}, {"id": 1551, "seek": 732932, "start": 7341.799999999999, "end": 7343.799999999999, "text": " So the pi torch one", "tokens": [407, 264, 3895, 27822, 472], "temperature": 0.0, "avg_logprob": -0.2680985362259383, "compression_ratio": 1.891304347826087, "no_speech_prob": 1.228916403306357e-06}, {"id": 1552, "seek": 732932, "start": 7343.88, "end": 7345.719999999999, "text": " all right", "tokens": [439, 558], "temperature": 0.0, "avg_logprob": -0.2680985362259383, "compression_ratio": 1.891304347826087, "no_speech_prob": 1.228916403306357e-06}, {"id": 1553, "seek": 732932, "start": 7345.719999999999, "end": 7348.48, "text": " But what we're going to do is we're going to", "tokens": [583, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.2680985362259383, "compression_ratio": 1.891304347826087, "no_speech_prob": 1.228916403306357e-06}, {"id": 1554, "seek": 732932, "start": 7349.4, "end": 7351.4, "text": " Flatten our input", "tokens": [3235, 32733, 527, 4846], "temperature": 0.0, "avg_logprob": -0.2680985362259383, "compression_ratio": 1.891304347826087, "no_speech_prob": 1.228916403306357e-06}, {"id": 1555, "seek": 732932, "start": 7351.679999999999, "end": 7352.679999999999, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.2680985362259383, "compression_ratio": 1.891304347826087, "no_speech_prob": 1.228916403306357e-06}, {"id": 1556, "seek": 732932, "start": 7352.679999999999, "end": 7354.719999999999, "text": " We're going to flatten our", "tokens": [492, 434, 516, 281, 24183, 527], "temperature": 0.0, "avg_logprob": -0.2680985362259383, "compression_ratio": 1.891304347826087, "no_speech_prob": 1.228916403306357e-06}, {"id": 1557, "seek": 732932, "start": 7355.48, "end": 7357.48, "text": " targets right and", "tokens": [12911, 558, 293], "temperature": 0.0, "avg_logprob": -0.2680985362259383, "compression_ratio": 1.891304347826087, "no_speech_prob": 1.228916403306357e-06}, {"id": 1558, "seek": 735748, "start": 7357.48, "end": 7360.44, "text": " So and it turns out these are going to be", "tokens": [407, 293, 309, 4523, 484, 613, 366, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.18985203334263392, "compression_ratio": 1.7635467980295567, "no_speech_prob": 2.4824732918204973e-06}, {"id": 1559, "seek": 735748, "start": 7362.639999999999, "end": 7367.139999999999, "text": " The first two axes are going to have to be transposed so the way", "tokens": [440, 700, 732, 35387, 366, 516, 281, 362, 281, 312, 7132, 1744, 370, 264, 636], "temperature": 0.0, "avg_logprob": -0.18985203334263392, "compression_ratio": 1.7635467980295567, "no_speech_prob": 2.4824732918204973e-06}, {"id": 1560, "seek": 735748, "start": 7368.679999999999, "end": 7370.679999999999, "text": " Pi torch handles", "tokens": [17741, 27822, 18722], "temperature": 0.0, "avg_logprob": -0.18985203334263392, "compression_ratio": 1.7635467980295567, "no_speech_prob": 2.4824732918204973e-06}, {"id": 1561, "seek": 735748, "start": 7371.28, "end": 7379.639999999999, "text": " RNN data by default is the first axis is the sequence length in this case eight right so the sequence length of an RNN", "tokens": [45702, 45, 1412, 538, 7576, 307, 264, 700, 10298, 307, 264, 8310, 4641, 294, 341, 1389, 3180, 558, 370, 264, 8310, 4641, 295, 364, 45702, 45], "temperature": 0.0, "avg_logprob": -0.18985203334263392, "compression_ratio": 1.7635467980295567, "no_speech_prob": 2.4824732918204973e-06}, {"id": 1562, "seek": 735748, "start": 7379.679999999999, "end": 7381.679999999999, "text": " Is how many time steps?", "tokens": [1119, 577, 867, 565, 4439, 30], "temperature": 0.0, "avg_logprob": -0.18985203334263392, "compression_ratio": 1.7635467980295567, "no_speech_prob": 2.4824732918204973e-06}, {"id": 1563, "seek": 738168, "start": 7381.68, "end": 7387.68, "text": " So we have eight characters so a sequence length of eight the second axis is the batch size and", "tokens": [407, 321, 362, 3180, 4342, 370, 257, 8310, 4641, 295, 3180, 264, 1150, 10298, 307, 264, 15245, 2744, 293], "temperature": 0.0, "avg_logprob": -0.2529430942258973, "compression_ratio": 1.4831460674157304, "no_speech_prob": 1.3925423445471097e-06}, {"id": 1564, "seek": 738168, "start": 7388.4400000000005, "end": 7394.6, "text": " Then as we would expect the third axis is the actual hidden state itself, okay, so this is going to be", "tokens": [1396, 382, 321, 576, 2066, 264, 2636, 10298, 307, 264, 3539, 7633, 1785, 2564, 11, 1392, 11, 370, 341, 307, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.2529430942258973, "compression_ratio": 1.4831460674157304, "no_speech_prob": 1.3925423445471097e-06}, {"id": 1565, "seek": 738168, "start": 7395.200000000001, "end": 7396.52, "text": " eight", "tokens": [3180], "temperature": 0.0, "avg_logprob": -0.2529430942258973, "compression_ratio": 1.4831460674157304, "no_speech_prob": 1.3925423445471097e-06}, {"id": 1566, "seek": 738168, "start": 7396.52, "end": 7401.68, "text": " by five twelve by N hidden which I think was 256", "tokens": [538, 1732, 14390, 538, 426, 7633, 597, 286, 519, 390, 38882], "temperature": 0.0, "avg_logprob": -0.2529430942258973, "compression_ratio": 1.4831460674157304, "no_speech_prob": 1.3925423445471097e-06}, {"id": 1567, "seek": 738168, "start": 7403.320000000001, "end": 7405.320000000001, "text": " Yeah, okay", "tokens": [865, 11, 1392], "temperature": 0.0, "avg_logprob": -0.2529430942258973, "compression_ratio": 1.4831460674157304, "no_speech_prob": 1.3925423445471097e-06}, {"id": 1568, "seek": 740532, "start": 7405.32, "end": 7412.599999999999, "text": " So we can grab the size and unpack it into each of these sequence length batch size num hidden", "tokens": [407, 321, 393, 4444, 264, 2744, 293, 26699, 309, 666, 1184, 295, 613, 8310, 4641, 15245, 2744, 1031, 7633], "temperature": 0.0, "avg_logprob": -0.41282254173642113, "compression_ratio": 1.18348623853211, "no_speech_prob": 6.179384968163504e-07}, {"id": 1569, "seek": 740532, "start": 7416.32, "end": 7418.32, "text": " Now target", "tokens": [823, 3779], "temperature": 0.0, "avg_logprob": -0.41282254173642113, "compression_ratio": 1.18348623853211, "no_speech_prob": 6.179384968163504e-07}, {"id": 1570, "seek": 740532, "start": 7420.599999999999, "end": 7422.599999999999, "text": " Yt dot", "tokens": [398, 83, 5893], "temperature": 0.0, "avg_logprob": -0.41282254173642113, "compression_ratio": 1.18348623853211, "no_speech_prob": 6.179384968163504e-07}, {"id": 1571, "seek": 740532, "start": 7423.44, "end": 7425.44, "text": " Size", "tokens": [35818], "temperature": 0.0, "avg_logprob": -0.41282254173642113, "compression_ratio": 1.18348623853211, "no_speech_prob": 6.179384968163504e-07}, {"id": 1572, "seek": 740532, "start": 7426.12, "end": 7428.12, "text": " Is 512 by 8", "tokens": [1119, 1025, 4762, 538, 1649], "temperature": 0.0, "avg_logprob": -0.41282254173642113, "compression_ratio": 1.18348623853211, "no_speech_prob": 6.179384968163504e-07}, {"id": 1573, "seek": 742812, "start": 7428.12, "end": 7435.599999999999, "text": " 8 where else this one here was 8 by 512 so to make them match we're going to have to transpose", "tokens": [1649, 689, 1646, 341, 472, 510, 390, 1649, 538, 1025, 4762, 370, 281, 652, 552, 2995, 321, 434, 516, 281, 362, 281, 25167], "temperature": 0.0, "avg_logprob": -0.2397519536765225, "compression_ratio": 1.5529953917050692, "no_speech_prob": 1.2482689726311946e-06}, {"id": 1574, "seek": 742812, "start": 7435.92, "end": 7437.92, "text": " the first two axes", "tokens": [264, 700, 732, 35387], "temperature": 0.0, "avg_logprob": -0.2397519536765225, "compression_ratio": 1.5529953917050692, "no_speech_prob": 1.2482689726311946e-06}, {"id": 1575, "seek": 742812, "start": 7438.32, "end": 7440.32, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2397519536765225, "compression_ratio": 1.5529953917050692, "no_speech_prob": 1.2482689726311946e-06}, {"id": 1576, "seek": 742812, "start": 7442.04, "end": 7448.599999999999, "text": " Hi torch when you do something like transpose doesn't generally actually shuffle the memory order", "tokens": [2421, 27822, 562, 291, 360, 746, 411, 25167, 1177, 380, 5101, 767, 39426, 264, 4675, 1668], "temperature": 0.0, "avg_logprob": -0.2397519536765225, "compression_ratio": 1.5529953917050692, "no_speech_prob": 1.2482689726311946e-06}, {"id": 1577, "seek": 742812, "start": 7448.599999999999, "end": 7451.0599999999995, "text": " But instead it just kind of keeps some internal", "tokens": [583, 2602, 309, 445, 733, 295, 5965, 512, 6920], "temperature": 0.0, "avg_logprob": -0.2397519536765225, "compression_ratio": 1.5529953917050692, "no_speech_prob": 1.2482689726311946e-06}, {"id": 1578, "seek": 742812, "start": 7452.0, "end": 7456.48, "text": " metadata to say like hey you should treat this as if it's transposed and", "tokens": [26603, 281, 584, 411, 4177, 291, 820, 2387, 341, 382, 498, 309, 311, 7132, 1744, 293], "temperature": 0.0, "avg_logprob": -0.2397519536765225, "compression_ratio": 1.5529953917050692, "no_speech_prob": 1.2482689726311946e-06}, {"id": 1579, "seek": 745648, "start": 7456.48, "end": 7458.32, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.16414857864379884, "compression_ratio": 1.7935779816513762, "no_speech_prob": 3.785304443226778e-06}, {"id": 1580, "seek": 745648, "start": 7458.32, "end": 7465.24, "text": " Some things in pie torch will give you an error if you try and use it when it has these like this internal state", "tokens": [2188, 721, 294, 1730, 27822, 486, 976, 291, 364, 6713, 498, 291, 853, 293, 764, 309, 562, 309, 575, 613, 411, 341, 6920, 1785], "temperature": 0.0, "avg_logprob": -0.16414857864379884, "compression_ratio": 1.7935779816513762, "no_speech_prob": 3.785304443226778e-06}, {"id": 1581, "seek": 745648, "start": 7465.759999999999, "end": 7467.759999999999, "text": " It'll basically say", "tokens": [467, 603, 1936, 584], "temperature": 0.0, "avg_logprob": -0.16414857864379884, "compression_ratio": 1.7935779816513762, "no_speech_prob": 3.785304443226778e-06}, {"id": 1582, "seek": 745648, "start": 7468.48, "end": 7475.759999999999, "text": " Error this tensor is not contiguous if you ever see that error at the word contiguous after it and it goes away", "tokens": [3300, 2874, 341, 40863, 307, 406, 660, 30525, 498, 291, 1562, 536, 300, 6713, 412, 264, 1349, 660, 30525, 934, 309, 293, 309, 1709, 1314], "temperature": 0.0, "avg_logprob": -0.16414857864379884, "compression_ratio": 1.7935779816513762, "no_speech_prob": 3.785304443226778e-06}, {"id": 1583, "seek": 745648, "start": 7476.28, "end": 7480.24, "text": " So I don't know they can't do that for you apparently so in this particular case", "tokens": [407, 286, 500, 380, 458, 436, 393, 380, 360, 300, 337, 291, 7970, 370, 294, 341, 1729, 1389], "temperature": 0.0, "avg_logprob": -0.16414857864379884, "compression_ratio": 1.7935779816513762, "no_speech_prob": 3.785304443226778e-06}, {"id": 1584, "seek": 745648, "start": 7480.24, "end": 7483.44, "text": " I got that error so I wrote the word contiguous after it okay", "tokens": [286, 658, 300, 6713, 370, 286, 4114, 264, 1349, 660, 30525, 934, 309, 1392], "temperature": 0.0, "avg_logprob": -0.16414857864379884, "compression_ratio": 1.7935779816513762, "no_speech_prob": 3.785304443226778e-06}, {"id": 1585, "seek": 748344, "start": 7483.44, "end": 7489.759999999999, "text": " And so then finally we need to flatten it out into a single vector and so we can just go dot view", "tokens": [400, 370, 550, 2721, 321, 643, 281, 24183, 309, 484, 666, 257, 2167, 8062, 293, 370, 321, 393, 445, 352, 5893, 1910], "temperature": 0.0, "avg_logprob": -0.19605779123830272, "compression_ratio": 1.5903083700440528, "no_speech_prob": 1.3081736369713326e-06}, {"id": 1586, "seek": 748344, "start": 7489.759999999999, "end": 7495.179999999999, "text": " Which is the same as numpy dot reshape and minus 1 means as long as it needs to be", "tokens": [3013, 307, 264, 912, 382, 1031, 8200, 5893, 725, 42406, 293, 3175, 502, 1355, 382, 938, 382, 309, 2203, 281, 312], "temperature": 0.0, "avg_logprob": -0.19605779123830272, "compression_ratio": 1.5903083700440528, "no_speech_prob": 1.3081736369713326e-06}, {"id": 1587, "seek": 748344, "start": 7496.48, "end": 7504.16, "text": " Okay, and then the input again. We also reshape that right, but remember the input sorry the", "tokens": [1033, 11, 293, 550, 264, 4846, 797, 13, 492, 611, 725, 42406, 300, 558, 11, 457, 1604, 264, 4846, 2597, 264], "temperature": 0.0, "avg_logprob": -0.19605779123830272, "compression_ratio": 1.5903083700440528, "no_speech_prob": 1.3081736369713326e-06}, {"id": 1588, "seek": 748344, "start": 7504.919999999999, "end": 7506.919999999999, "text": " the the predictions", "tokens": [264, 264, 21264], "temperature": 0.0, "avg_logprob": -0.19605779123830272, "compression_ratio": 1.5903083700440528, "no_speech_prob": 1.3081736369713326e-06}, {"id": 1589, "seek": 748344, "start": 7508.08, "end": 7512.4, "text": " Also have this axis of length 84 all of the predicted probabilities", "tokens": [2743, 362, 341, 10298, 295, 4641, 29018, 439, 295, 264, 19147, 33783], "temperature": 0.0, "avg_logprob": -0.19605779123830272, "compression_ratio": 1.5903083700440528, "no_speech_prob": 1.3081736369713326e-06}, {"id": 1590, "seek": 751240, "start": 7512.4, "end": 7515.679999999999, "text": " Okay, so so here's a custom", "tokens": [1033, 11, 370, 370, 510, 311, 257, 2375], "temperature": 0.0, "avg_logprob": -0.16817043092515732, "compression_ratio": 1.7142857142857142, "no_speech_prob": 6.786728476981807e-07}, {"id": 1591, "seek": 751240, "start": 7516.36, "end": 7522.24, "text": " Here's a custom loss function. That's it right so if you ever want to play around with your own loss functions", "tokens": [1692, 311, 257, 2375, 4470, 2445, 13, 663, 311, 309, 558, 370, 498, 291, 1562, 528, 281, 862, 926, 365, 428, 1065, 4470, 6828], "temperature": 0.0, "avg_logprob": -0.16817043092515732, "compression_ratio": 1.7142857142857142, "no_speech_prob": 6.786728476981807e-07}, {"id": 1592, "seek": 751240, "start": 7522.24, "end": 7525.28, "text": " you can just do that like so and then", "tokens": [291, 393, 445, 360, 300, 411, 370, 293, 550], "temperature": 0.0, "avg_logprob": -0.16817043092515732, "compression_ratio": 1.7142857142857142, "no_speech_prob": 6.786728476981807e-07}, {"id": 1593, "seek": 751240, "start": 7526.44, "end": 7532.96, "text": " Pass that to fit right so it's important to remember that fit is this like", "tokens": [10319, 300, 281, 3318, 558, 370, 309, 311, 1021, 281, 1604, 300, 3318, 307, 341, 411], "temperature": 0.0, "avg_logprob": -0.16817043092515732, "compression_ratio": 1.7142857142857142, "no_speech_prob": 6.786728476981807e-07}, {"id": 1594, "seek": 751240, "start": 7533.879999999999, "end": 7541.0, "text": " Lowest level fast AI abstraction you know that sits that this is the thing that implements the training loop", "tokens": [17078, 377, 1496, 2370, 7318, 37765, 291, 458, 300, 12696, 300, 341, 307, 264, 551, 300, 704, 17988, 264, 3097, 6367], "temperature": 0.0, "avg_logprob": -0.16817043092515732, "compression_ratio": 1.7142857142857142, "no_speech_prob": 6.786728476981807e-07}, {"id": 1595, "seek": 754100, "start": 7541.0, "end": 7544.84, "text": " Okay, and so like you're the stuff you pass it in is", "tokens": [1033, 11, 293, 370, 411, 291, 434, 264, 1507, 291, 1320, 309, 294, 307], "temperature": 0.0, "avg_logprob": -0.29751810534247036, "compression_ratio": 1.5613207547169812, "no_speech_prob": 1.1015923746526823e-06}, {"id": 1596, "seek": 754100, "start": 7545.88, "end": 7547.2, "text": " all", "tokens": [439], "temperature": 0.0, "avg_logprob": -0.29751810534247036, "compression_ratio": 1.5613207547169812, "no_speech_prob": 1.1015923746526823e-06}, {"id": 1597, "seek": 754100, "start": 7547.2, "end": 7549.2, "text": " standard pie torch stuff", "tokens": [3832, 1730, 27822, 1507], "temperature": 0.0, "avg_logprob": -0.29751810534247036, "compression_ratio": 1.5613207547169812, "no_speech_prob": 1.1015923746526823e-06}, {"id": 1598, "seek": 754100, "start": 7549.8, "end": 7551.36, "text": " except for this", "tokens": [3993, 337, 341], "temperature": 0.0, "avg_logprob": -0.29751810534247036, "compression_ratio": 1.5613207547169812, "no_speech_prob": 1.1015923746526823e-06}, {"id": 1599, "seek": 754100, "start": 7551.36, "end": 7555.64, "text": " This is our model data object. This is the thing that wraps up the test set", "tokens": [639, 307, 527, 2316, 1412, 2657, 13, 639, 307, 264, 551, 300, 25831, 493, 264, 1500, 992], "temperature": 0.0, "avg_logprob": -0.29751810534247036, "compression_ratio": 1.5613207547169812, "no_speech_prob": 1.1015923746526823e-06}, {"id": 1600, "seek": 754100, "start": 7556.52, "end": 7560.02, "text": " The training set and the validation set together, okay?", "tokens": [440, 3097, 992, 293, 264, 24071, 992, 1214, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.29751810534247036, "compression_ratio": 1.5613207547169812, "no_speech_prob": 1.1015923746526823e-06}, {"id": 1601, "seek": 754100, "start": 7560.52, "end": 7562.52, "text": " You're not could you pass that back?", "tokens": [509, 434, 406, 727, 291, 1320, 300, 646, 30], "temperature": 0.0, "avg_logprob": -0.29751810534247036, "compression_ratio": 1.5613207547169812, "no_speech_prob": 1.1015923746526823e-06}, {"id": 1602, "seek": 756252, "start": 7562.52, "end": 7570.080000000001, "text": " So when we pull the triangle into the repetitive structure right?", "tokens": [407, 562, 321, 2235, 264, 13369, 666, 264, 29404, 3877, 558, 30], "temperature": 0.0, "avg_logprob": -0.2418424505936472, "compression_ratio": 1.771551724137931, "no_speech_prob": 1.922238698170986e-05}, {"id": 1603, "seek": 756252, "start": 7570.92, "end": 7573.5, "text": " So the the first n minus 1", "tokens": [407, 264, 264, 700, 297, 3175, 502], "temperature": 0.0, "avg_logprob": -0.2418424505936472, "compression_ratio": 1.771551724137931, "no_speech_prob": 1.922238698170986e-05}, {"id": 1604, "seek": 756252, "start": 7574.280000000001, "end": 7577.56, "text": " Iterations of the sequence length we don't see the whole sequence length", "tokens": [286, 391, 763, 295, 264, 8310, 4641, 321, 500, 380, 536, 264, 1379, 8310, 4641], "temperature": 0.0, "avg_logprob": -0.2418424505936472, "compression_ratio": 1.771551724137931, "no_speech_prob": 1.922238698170986e-05}, {"id": 1605, "seek": 756252, "start": 7577.72, "end": 7584.8, "text": " Yeah, so does that mean that the batch size should be much bigger so that you get a triangular kind of the big careful", "tokens": [865, 11, 370, 775, 300, 914, 300, 264, 15245, 2744, 820, 312, 709, 3801, 370, 300, 291, 483, 257, 38190, 733, 295, 264, 955, 5026], "temperature": 0.0, "avg_logprob": -0.2418424505936472, "compression_ratio": 1.771551724137931, "no_speech_prob": 1.922238698170986e-05}, {"id": 1606, "seek": 758480, "start": 7584.8, "end": 7592.72, "text": " You don't mean batch size you mean sequence length right because the batch size is like something else entirely yeah, okay, so yes", "tokens": [509, 500, 380, 914, 15245, 2744, 291, 914, 8310, 4641, 558, 570, 264, 15245, 2744, 307, 411, 746, 1646, 7696, 1338, 11, 1392, 11, 370, 2086], "temperature": 0.0, "avg_logprob": -0.19866328336754624, "compression_ratio": 1.6943231441048034, "no_speech_prob": 3.2563082186243264e-07}, {"id": 1607, "seek": 758480, "start": 7593.04, "end": 7595.84, "text": " Yes, if you have a short sequence length like eight", "tokens": [1079, 11, 498, 291, 362, 257, 2099, 8310, 4641, 411, 3180], "temperature": 0.0, "avg_logprob": -0.19866328336754624, "compression_ratio": 1.6943231441048034, "no_speech_prob": 3.2563082186243264e-07}, {"id": 1608, "seek": 758480, "start": 7596.400000000001, "end": 7598.400000000001, "text": " Yeah, the first character", "tokens": [865, 11, 264, 700, 2517], "temperature": 0.0, "avg_logprob": -0.19866328336754624, "compression_ratio": 1.6943231441048034, "no_speech_prob": 3.2563082186243264e-07}, {"id": 1609, "seek": 758480, "start": 7599.4400000000005, "end": 7602.84, "text": " Has nothing to go on it starts with an", "tokens": [8646, 1825, 281, 352, 322, 309, 3719, 365, 364], "temperature": 0.0, "avg_logprob": -0.19866328336754624, "compression_ratio": 1.6943231441048034, "no_speech_prob": 3.2563082186243264e-07}, {"id": 1610, "seek": 758480, "start": 7605.24, "end": 7607.24, "text": " Empty hidden state of zeros", "tokens": [3968, 39420, 7633, 1785, 295, 35193], "temperature": 0.0, "avg_logprob": -0.19866328336754624, "compression_ratio": 1.6943231441048034, "no_speech_prob": 3.2563082186243264e-07}, {"id": 1611, "seek": 758480, "start": 7607.320000000001, "end": 7614.24, "text": " All right, so what we're going to start with next week is we're going to learn how to avoid that problem, right?", "tokens": [1057, 558, 11, 370, 437, 321, 434, 516, 281, 722, 365, 958, 1243, 307, 321, 434, 516, 281, 1466, 577, 281, 5042, 300, 1154, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19866328336754624, "compression_ratio": 1.6943231441048034, "no_speech_prob": 3.2563082186243264e-07}, {"id": 1612, "seek": 761424, "start": 7614.24, "end": 7616.24, "text": " And so it's a really insightful", "tokens": [400, 370, 309, 311, 257, 534, 46401], "temperature": 0.0, "avg_logprob": -0.18472315190912603, "compression_ratio": 1.5309734513274336, "no_speech_prob": 1.2098619208700256e-06}, {"id": 1613, "seek": 761424, "start": 7617.16, "end": 7622.08, "text": " Question or concern right and but if you think about it the basic idea is", "tokens": [14464, 420, 3136, 558, 293, 457, 498, 291, 519, 466, 309, 264, 3875, 1558, 307], "temperature": 0.0, "avg_logprob": -0.18472315190912603, "compression_ratio": 1.5309734513274336, "no_speech_prob": 1.2098619208700256e-06}, {"id": 1614, "seek": 761424, "start": 7623.0, "end": 7625.36, "text": " Why should we reset this to zero?", "tokens": [1545, 820, 321, 14322, 341, 281, 4018, 30], "temperature": 0.0, "avg_logprob": -0.18472315190912603, "compression_ratio": 1.5309734513274336, "no_speech_prob": 1.2098619208700256e-06}, {"id": 1615, "seek": 761424, "start": 7626.4, "end": 7630.96, "text": " Every time you know like if we can kind of line up these", "tokens": [2048, 565, 291, 458, 411, 498, 321, 393, 733, 295, 1622, 493, 613], "temperature": 0.0, "avg_logprob": -0.18472315190912603, "compression_ratio": 1.5309734513274336, "no_speech_prob": 1.2098619208700256e-06}, {"id": 1616, "seek": 761424, "start": 7631.76, "end": 7635.0, "text": " Many batches somehow so that the next mini batch", "tokens": [5126, 15245, 279, 6063, 370, 300, 264, 958, 8382, 15245], "temperature": 0.0, "avg_logprob": -0.18472315190912603, "compression_ratio": 1.5309734513274336, "no_speech_prob": 1.2098619208700256e-06}, {"id": 1617, "seek": 761424, "start": 7635.96, "end": 7639.86, "text": " Joins up correctly it represents like the next letter in Nietzsche's works", "tokens": [3139, 1292, 493, 8944, 309, 8855, 411, 264, 958, 5063, 294, 36583, 89, 12287, 311, 1985], "temperature": 0.0, "avg_logprob": -0.18472315190912603, "compression_ratio": 1.5309734513274336, "no_speech_prob": 1.2098619208700256e-06}, {"id": 1618, "seek": 761424, "start": 7640.92, "end": 7642.92, "text": " Then we want to move this", "tokens": [1396, 321, 528, 281, 1286, 341], "temperature": 0.0, "avg_logprob": -0.18472315190912603, "compression_ratio": 1.5309734513274336, "no_speech_prob": 1.2098619208700256e-06}, {"id": 1619, "seek": 764292, "start": 7642.92, "end": 7644.92, "text": " up into the constructor", "tokens": [493, 666, 264, 47479], "temperature": 0.0, "avg_logprob": -0.23186725185763452, "compression_ratio": 1.6934306569343065, "no_speech_prob": 1.5056963320603245e-06}, {"id": 1620, "seek": 764292, "start": 7647.36, "end": 7650.96, "text": " Right and then like pass that here and", "tokens": [1779, 293, 550, 411, 1320, 300, 510, 293], "temperature": 0.0, "avg_logprob": -0.23186725185763452, "compression_ratio": 1.6934306569343065, "no_speech_prob": 1.5056963320603245e-06}, {"id": 1621, "seek": 764292, "start": 7652.84, "end": 7654.84, "text": " Then store it here", "tokens": [1396, 3531, 309, 510], "temperature": 0.0, "avg_logprob": -0.23186725185763452, "compression_ratio": 1.6934306569343065, "no_speech_prob": 1.5056963320603245e-06}, {"id": 1622, "seek": 764292, "start": 7655.4, "end": 7657.4, "text": " Right and now", "tokens": [1779, 293, 586], "temperature": 0.0, "avg_logprob": -0.23186725185763452, "compression_ratio": 1.6934306569343065, "no_speech_prob": 1.5056963320603245e-06}, {"id": 1623, "seek": 764292, "start": 7657.52, "end": 7660.68, "text": " We're not resetting the hidden state each time. We're actually", "tokens": [492, 434, 406, 14322, 783, 264, 7633, 1785, 1184, 565, 13, 492, 434, 767], "temperature": 0.0, "avg_logprob": -0.23186725185763452, "compression_ratio": 1.6934306569343065, "no_speech_prob": 1.5056963320603245e-06}, {"id": 1624, "seek": 764292, "start": 7661.92, "end": 7666.24, "text": " We're actually keeping the hidden state from call to call and so the only", "tokens": [492, 434, 767, 5145, 264, 7633, 1785, 490, 818, 281, 818, 293, 370, 264, 787], "temperature": 0.0, "avg_logprob": -0.23186725185763452, "compression_ratio": 1.6934306569343065, "no_speech_prob": 1.5056963320603245e-06}, {"id": 1625, "seek": 766624, "start": 7666.24, "end": 7674.12, "text": " Time that it would be failing to benefit from learning state would be like literally at the very start of the document", "tokens": [6161, 300, 309, 576, 312, 18223, 281, 5121, 490, 2539, 1785, 576, 312, 411, 3736, 412, 264, 588, 722, 295, 264, 4166], "temperature": 0.0, "avg_logprob": -0.19374135645424448, "compression_ratio": 1.6818181818181819, "no_speech_prob": 6.276684985095926e-07}, {"id": 1626, "seek": 766624, "start": 7675.04, "end": 7677.04, "text": " So that's where we're that's where we're going to train ahead", "tokens": [407, 300, 311, 689, 321, 434, 300, 311, 689, 321, 434, 516, 281, 3847, 2286], "temperature": 0.0, "avg_logprob": -0.19374135645424448, "compression_ratio": 1.6818181818181819, "no_speech_prob": 6.276684985095926e-07}, {"id": 1627, "seek": 766624, "start": 7678.08, "end": 7680.08, "text": " next week I", "tokens": [958, 1243, 286], "temperature": 0.0, "avg_logprob": -0.19374135645424448, "compression_ratio": 1.6818181818181819, "no_speech_prob": 6.276684985095926e-07}, {"id": 1628, "seek": 766624, "start": 7687.16, "end": 7693.599999999999, "text": " Feel like this lesson every time I've got a punchline coming somebody asks me a question where I have to like do the punchline", "tokens": [14113, 411, 341, 6898, 633, 565, 286, 600, 658, 257, 8135, 1889, 1348, 2618, 8962, 385, 257, 1168, 689, 286, 362, 281, 411, 360, 264, 8135, 1889], "temperature": 0.0, "avg_logprob": -0.19374135645424448, "compression_ratio": 1.6818181818181819, "no_speech_prob": 6.276684985095926e-07}, {"id": 1629, "seek": 766624, "start": 7693.599999999999, "end": 7695.599999999999, "text": " ahead of time", "tokens": [2286, 295, 565], "temperature": 0.0, "avg_logprob": -0.19374135645424448, "compression_ratio": 1.6818181818181819, "no_speech_prob": 6.276684985095926e-07}, {"id": 1630, "seek": 769560, "start": 7695.6, "end": 7699.360000000001, "text": " Okay, so we can fit that and we can fit that", "tokens": [1033, 11, 370, 321, 393, 3318, 300, 293, 321, 393, 3318, 300], "temperature": 0.0, "avg_logprob": -0.2271248045421782, "compression_ratio": 1.6618357487922706, "no_speech_prob": 2.5215535970346536e-06}, {"id": 1631, "seek": 769560, "start": 7699.96, "end": 7703.68, "text": " And I want to show you something interesting and this is coming to", "tokens": [400, 286, 528, 281, 855, 291, 746, 1880, 293, 341, 307, 1348, 281], "temperature": 0.0, "avg_logprob": -0.2271248045421782, "compression_ratio": 1.6618357487922706, "no_speech_prob": 2.5215535970346536e-06}, {"id": 1632, "seek": 769560, "start": 7704.56, "end": 7708.92, "text": " the punchline that another punchline that you're net tried to spoil which is", "tokens": [264, 8135, 1889, 300, 1071, 8135, 1889, 300, 291, 434, 2533, 3031, 281, 18630, 597, 307], "temperature": 0.0, "avg_logprob": -0.2271248045421782, "compression_ratio": 1.6618357487922706, "no_speech_prob": 2.5215535970346536e-06}, {"id": 1633, "seek": 769560, "start": 7711.320000000001, "end": 7713.280000000001, "text": " When we're", "tokens": [1133, 321, 434], "temperature": 0.0, "avg_logprob": -0.2271248045421782, "compression_ratio": 1.6618357487922706, "no_speech_prob": 2.5215535970346536e-06}, {"id": 1634, "seek": 769560, "start": 7713.280000000001, "end": 7718.160000000001, "text": " You know remember. This is just doing a loop right applying the same matrix multiplier again and again", "tokens": [509, 458, 1604, 13, 639, 307, 445, 884, 257, 6367, 558, 9275, 264, 912, 8141, 44106, 797, 293, 797], "temperature": 0.0, "avg_logprob": -0.2271248045421782, "compression_ratio": 1.6618357487922706, "no_speech_prob": 2.5215535970346536e-06}, {"id": 1635, "seek": 769560, "start": 7719.88, "end": 7723.64, "text": " If that matrix multiply tends to increase", "tokens": [759, 300, 8141, 12972, 12258, 281, 3488], "temperature": 0.0, "avg_logprob": -0.2271248045421782, "compression_ratio": 1.6618357487922706, "no_speech_prob": 2.5215535970346536e-06}, {"id": 1636, "seek": 772364, "start": 7723.64, "end": 7725.320000000001, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.25576232827228046, "compression_ratio": 1.7067307692307692, "no_speech_prob": 5.896407628824818e-07}, {"id": 1637, "seek": 772364, "start": 7725.320000000001, "end": 7731.160000000001, "text": " Activations each time then effectively we're doing that to the power of eight right so it's going to like shoot off", "tokens": [28550, 763, 1184, 565, 550, 8659, 321, 434, 884, 300, 281, 264, 1347, 295, 3180, 558, 370, 309, 311, 516, 281, 411, 3076, 766], "temperature": 0.0, "avg_logprob": -0.25576232827228046, "compression_ratio": 1.7067307692307692, "no_speech_prob": 5.896407628824818e-07}, {"id": 1638, "seek": 772364, "start": 7731.64, "end": 7737.12, "text": " Really high or if it's decreasing it a little bit each time. It's going to shoot off really low", "tokens": [4083, 1090, 420, 498, 309, 311, 23223, 309, 257, 707, 857, 1184, 565, 13, 467, 311, 516, 281, 3076, 766, 534, 2295], "temperature": 0.0, "avg_logprob": -0.25576232827228046, "compression_ratio": 1.7067307692307692, "no_speech_prob": 5.896407628824818e-07}, {"id": 1639, "seek": 772364, "start": 7737.12, "end": 7741.8, "text": " That's what we call a gradient explosion right and so we really want to make sure", "tokens": [663, 311, 437, 321, 818, 257, 16235, 15673, 558, 293, 370, 321, 534, 528, 281, 652, 988], "temperature": 0.0, "avg_logprob": -0.25576232827228046, "compression_ratio": 1.7067307692307692, "no_speech_prob": 5.896407628824818e-07}, {"id": 1640, "seek": 772364, "start": 7742.92, "end": 7745.320000000001, "text": " That the initial h", "tokens": [663, 264, 5883, 276], "temperature": 0.0, "avg_logprob": -0.25576232827228046, "compression_ratio": 1.7067307692307692, "no_speech_prob": 5.896407628824818e-07}, {"id": 1641, "seek": 772364, "start": 7746.52, "end": 7748.88, "text": " Not h the initial what did we call it?", "tokens": [1726, 276, 264, 5883, 437, 630, 321, 818, 309, 30], "temperature": 0.0, "avg_logprob": -0.25576232827228046, "compression_ratio": 1.7067307692307692, "no_speech_prob": 5.896407628824818e-07}, {"id": 1642, "seek": 774888, "start": 7748.88, "end": 7753.14, "text": " The initial L hidden", "tokens": [440, 5883, 441, 7633], "temperature": 0.0, "avg_logprob": -0.24177409507132866, "compression_ratio": 1.6010362694300517, "no_speech_prob": 7.112426487765333e-07}, {"id": 1643, "seek": 774888, "start": 7754.08, "end": 7758.0, "text": " That we create is is like of a size", "tokens": [663, 321, 1884, 307, 307, 411, 295, 257, 2744], "temperature": 0.0, "avg_logprob": -0.24177409507132866, "compression_ratio": 1.6010362694300517, "no_speech_prob": 7.112426487765333e-07}, {"id": 1644, "seek": 774888, "start": 7758.4400000000005, "end": 7762.7, "text": " That's not going to cause our activations on average to increase or decrease", "tokens": [663, 311, 406, 516, 281, 3082, 527, 2430, 763, 322, 4274, 281, 3488, 420, 11514], "temperature": 0.0, "avg_logprob": -0.24177409507132866, "compression_ratio": 1.6010362694300517, "no_speech_prob": 7.112426487765333e-07}, {"id": 1645, "seek": 774888, "start": 7763.36, "end": 7767.24, "text": " Right and there's actually a very nice matrix", "tokens": [1779, 293, 456, 311, 767, 257, 588, 1481, 8141], "temperature": 0.0, "avg_logprob": -0.24177409507132866, "compression_ratio": 1.6010362694300517, "no_speech_prob": 7.112426487765333e-07}, {"id": 1646, "seek": 774888, "start": 7768.2, "end": 7770.2, "text": " That does exactly that", "tokens": [663, 775, 2293, 300], "temperature": 0.0, "avg_logprob": -0.24177409507132866, "compression_ratio": 1.6010362694300517, "no_speech_prob": 7.112426487765333e-07}, {"id": 1647, "seek": 774888, "start": 7770.400000000001, "end": 7772.400000000001, "text": " Called the identity matrix", "tokens": [45001, 264, 6575, 8141], "temperature": 0.0, "avg_logprob": -0.24177409507132866, "compression_ratio": 1.6010362694300517, "no_speech_prob": 7.112426487765333e-07}, {"id": 1648, "seek": 777240, "start": 7772.4, "end": 7779.32, "text": " So the identity matrix for those that don't quite remember their linear algebra is this", "tokens": [407, 264, 6575, 8141, 337, 729, 300, 500, 380, 1596, 1604, 641, 8213, 21989, 307, 341], "temperature": 0.0, "avg_logprob": -0.16682742890857516, "compression_ratio": 1.9637305699481866, "no_speech_prob": 1.8162176047553658e-06}, {"id": 1649, "seek": 777240, "start": 7780.48, "end": 7782.759999999999, "text": " This would be a size three identity matrix", "tokens": [639, 576, 312, 257, 2744, 1045, 6575, 8141], "temperature": 0.0, "avg_logprob": -0.16682742890857516, "compression_ratio": 1.9637305699481866, "no_speech_prob": 1.8162176047553658e-06}, {"id": 1650, "seek": 777240, "start": 7783.679999999999, "end": 7785.679999999999, "text": " right and so", "tokens": [558, 293, 370], "temperature": 0.0, "avg_logprob": -0.16682742890857516, "compression_ratio": 1.9637305699481866, "no_speech_prob": 1.8162176047553658e-06}, {"id": 1651, "seek": 777240, "start": 7786.0, "end": 7791.599999999999, "text": " The trick about an identity matrix is anything times an identity matrix is itself", "tokens": [440, 4282, 466, 364, 6575, 8141, 307, 1340, 1413, 364, 6575, 8141, 307, 2564], "temperature": 0.0, "avg_logprob": -0.16682742890857516, "compression_ratio": 1.9637305699481866, "no_speech_prob": 1.8162176047553658e-06}, {"id": 1652, "seek": 777240, "start": 7791.839999999999, "end": 7798.92, "text": " Right and so therefore you could multiply by this again and again and again and again and still end up with itself", "tokens": [1779, 293, 370, 4412, 291, 727, 12972, 538, 341, 797, 293, 797, 293, 797, 293, 797, 293, 920, 917, 493, 365, 2564], "temperature": 0.0, "avg_logprob": -0.16682742890857516, "compression_ratio": 1.9637305699481866, "no_speech_prob": 1.8162176047553658e-06}, {"id": 1653, "seek": 777240, "start": 7798.92, "end": 7800.92, "text": " Right so there's no gradient explosion", "tokens": [1779, 370, 456, 311, 572, 16235, 15673], "temperature": 0.0, "avg_logprob": -0.16682742890857516, "compression_ratio": 1.9637305699481866, "no_speech_prob": 1.8162176047553658e-06}, {"id": 1654, "seek": 780092, "start": 7800.92, "end": 7802.4, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.24390020824614025, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.7880578297990724e-06}, {"id": 1655, "seek": 780092, "start": 7802.4, "end": 7806.64, "text": " What we could do is instead of using whatever the default", "tokens": [708, 321, 727, 360, 307, 2602, 295, 1228, 2035, 264, 7576], "temperature": 0.0, "avg_logprob": -0.24390020824614025, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.7880578297990724e-06}, {"id": 1656, "seek": 780092, "start": 7807.4, "end": 7809.4, "text": " Random in it is for this matrix", "tokens": [37603, 294, 309, 307, 337, 341, 8141], "temperature": 0.0, "avg_logprob": -0.24390020824614025, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.7880578297990724e-06}, {"id": 1657, "seek": 780092, "start": 7810.08, "end": 7817.66, "text": " We could instead after we create our RNN is we can go into that RNN right and notice this right we can go", "tokens": [492, 727, 2602, 934, 321, 1884, 527, 45702, 45, 307, 321, 393, 352, 666, 300, 45702, 45, 558, 293, 3449, 341, 558, 321, 393, 352], "temperature": 0.0, "avg_logprob": -0.24390020824614025, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.7880578297990724e-06}, {"id": 1658, "seek": 780092, "start": 7818.68, "end": 7819.88, "text": " M.RNN", "tokens": [376, 13, 49, 45, 45], "temperature": 0.0, "avg_logprob": -0.24390020824614025, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.7880578297990724e-06}, {"id": 1659, "seek": 780092, "start": 7819.88, "end": 7821.96, "text": " Right and if we now go", "tokens": [1779, 293, 498, 321, 586, 352], "temperature": 0.0, "avg_logprob": -0.24390020824614025, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.7880578297990724e-06}, {"id": 1660, "seek": 780092, "start": 7823.28, "end": 7826.24, "text": " Like so we can get the docs for M.RNN", "tokens": [1743, 370, 321, 393, 483, 264, 45623, 337, 376, 13, 49, 45, 45], "temperature": 0.0, "avg_logprob": -0.24390020824614025, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.7880578297990724e-06}, {"id": 1661, "seek": 782624, "start": 7826.24, "end": 7834.0, "text": " Right and as well as the arguments for constructing it it also tells you the inputs and outputs for calling the layer", "tokens": [1779, 293, 382, 731, 382, 264, 12869, 337, 39969, 309, 309, 611, 5112, 291, 264, 15743, 293, 23930, 337, 5141, 264, 4583], "temperature": 0.0, "avg_logprob": -0.23506450653076172, "compression_ratio": 1.7008928571428572, "no_speech_prob": 1.963795511983335e-06}, {"id": 1662, "seek": 782624, "start": 7834.48, "end": 7839.44, "text": " And it also tells you the attributes and so it tells you there's something called weight", "tokens": [400, 309, 611, 5112, 291, 264, 17212, 293, 370, 309, 5112, 291, 456, 311, 746, 1219, 3364], "temperature": 0.0, "avg_logprob": -0.23506450653076172, "compression_ratio": 1.7008928571428572, "no_speech_prob": 1.963795511983335e-06}, {"id": 1663, "seek": 782624, "start": 7839.679999999999, "end": 7843.679999999999, "text": " HH and these are the learnable hidden to hidden weights. That's that square matrix", "tokens": [389, 39, 293, 613, 366, 264, 1466, 712, 7633, 281, 7633, 17443, 13, 663, 311, 300, 3732, 8141], "temperature": 0.0, "avg_logprob": -0.23506450653076172, "compression_ratio": 1.7008928571428572, "no_speech_prob": 1.963795511983335e-06}, {"id": 1664, "seek": 782624, "start": 7844.4, "end": 7849.3, "text": " Right so after we've constructed our M. We can just go in and say all right", "tokens": [1779, 370, 934, 321, 600, 17083, 527, 376, 13, 492, 393, 445, 352, 294, 293, 584, 439, 558], "temperature": 0.0, "avg_logprob": -0.23506450653076172, "compression_ratio": 1.7008928571428572, "no_speech_prob": 1.963795511983335e-06}, {"id": 1665, "seek": 782624, "start": 7849.719999999999, "end": 7851.719999999999, "text": " M.RNN.weightHHL", "tokens": [376, 13, 49, 45, 45, 13, 12329, 7499, 43], "temperature": 0.0, "avg_logprob": -0.23506450653076172, "compression_ratio": 1.7008928571428572, "no_speech_prob": 1.963795511983335e-06}, {"id": 1666, "seek": 785172, "start": 7851.72, "end": 7853.72, "text": " Weight HHL", "tokens": [44464, 389, 39, 43], "temperature": 0.0, "avg_logprob": -0.2905247211456299, "compression_ratio": 1.5822784810126582, "no_speech_prob": 5.539162657441921e-07}, {"id": 1667, "seek": 785172, "start": 7854.56, "end": 7860.56, "text": " Dot data that's the tensor dot copy underscore in place", "tokens": [38753, 1412, 300, 311, 264, 40863, 5893, 5055, 37556, 294, 1081], "temperature": 0.0, "avg_logprob": -0.2905247211456299, "compression_ratio": 1.5822784810126582, "no_speech_prob": 5.539162657441921e-07}, {"id": 1668, "seek": 785172, "start": 7861.64, "end": 7866.68, "text": " Torch dot I that is I for identity in case you are wondering", "tokens": [7160, 339, 5893, 286, 300, 307, 286, 337, 6575, 294, 1389, 291, 366, 6359], "temperature": 0.0, "avg_logprob": -0.2905247211456299, "compression_ratio": 1.5822784810126582, "no_speech_prob": 5.539162657441921e-07}, {"id": 1669, "seek": 785172, "start": 7868.4400000000005, "end": 7876.4800000000005, "text": " So this is an identity matrix of size and hidden so this both puts into this weight matrix and returns", "tokens": [407, 341, 307, 364, 6575, 8141, 295, 2744, 293, 7633, 370, 341, 1293, 8137, 666, 341, 3364, 8141, 293, 11247], "temperature": 0.0, "avg_logprob": -0.2905247211456299, "compression_ratio": 1.5822784810126582, "no_speech_prob": 5.539162657441921e-07}, {"id": 1670, "seek": 785172, "start": 7877.400000000001, "end": 7879.400000000001, "text": " the identity matrix", "tokens": [264, 6575, 8141], "temperature": 0.0, "avg_logprob": -0.2905247211456299, "compression_ratio": 1.5822784810126582, "no_speech_prob": 5.539162657441921e-07}, {"id": 1671, "seek": 787940, "start": 7879.4, "end": 7881.4, "text": " and so this was like", "tokens": [293, 370, 341, 390, 411], "temperature": 0.0, "avg_logprob": -0.24925276673870322, "compression_ratio": 1.4630541871921183, "no_speech_prob": 8.579219752391509e-07}, {"id": 1672, "seek": 787940, "start": 7882.92, "end": 7884.599999999999, "text": " Actually a", "tokens": [5135, 257], "temperature": 0.0, "avg_logprob": -0.24925276673870322, "compression_ratio": 1.4630541871921183, "no_speech_prob": 8.579219752391509e-07}, {"id": 1673, "seek": 787940, "start": 7884.599999999999, "end": 7886.04, "text": " Jeffrey Hinton", "tokens": [28721, 389, 12442], "temperature": 0.0, "avg_logprob": -0.24925276673870322, "compression_ratio": 1.4630541871921183, "no_speech_prob": 8.579219752391509e-07}, {"id": 1674, "seek": 787940, "start": 7886.04, "end": 7887.36, "text": " paper", "tokens": [3035], "temperature": 0.0, "avg_logprob": -0.24925276673870322, "compression_ratio": 1.4630541871921183, "no_speech_prob": 8.579219752391509e-07}, {"id": 1675, "seek": 787940, "start": 7887.36, "end": 7892.5599999999995, "text": " Was like hey, you know after what is this 2015 so after?", "tokens": [3027, 411, 4177, 11, 291, 458, 934, 437, 307, 341, 7546, 370, 934, 30], "temperature": 0.0, "avg_logprob": -0.24925276673870322, "compression_ratio": 1.4630541871921183, "no_speech_prob": 8.579219752391509e-07}, {"id": 1676, "seek": 787940, "start": 7893.24, "end": 7896.219999999999, "text": " Recurrent neural nets have been around for decades. He was like", "tokens": [9647, 374, 1753, 18161, 36170, 362, 668, 926, 337, 7878, 13, 634, 390, 411], "temperature": 0.0, "avg_logprob": -0.24925276673870322, "compression_ratio": 1.4630541871921183, "no_speech_prob": 8.579219752391509e-07}, {"id": 1677, "seek": 787940, "start": 7897.12, "end": 7898.679999999999, "text": " Hey gang", "tokens": [1911, 10145], "temperature": 0.0, "avg_logprob": -0.24925276673870322, "compression_ratio": 1.4630541871921183, "no_speech_prob": 8.579219752391509e-07}, {"id": 1678, "seek": 787940, "start": 7898.679999999999, "end": 7905.94, "text": " Maybe we should just use the identity matrix to initialize this and like it actually turns out to work really well", "tokens": [2704, 321, 820, 445, 764, 264, 6575, 8141, 281, 5883, 1125, 341, 293, 411, 309, 767, 4523, 484, 281, 589, 534, 731], "temperature": 0.0, "avg_logprob": -0.24925276673870322, "compression_ratio": 1.4630541871921183, "no_speech_prob": 8.579219752391509e-07}, {"id": 1679, "seek": 790594, "start": 7905.94, "end": 7909.98, "text": " And so that was a 2015 paper believe it or not", "tokens": [400, 370, 300, 390, 257, 7546, 3035, 1697, 309, 420, 406], "temperature": 0.0, "avg_logprob": -0.15713687472873264, "compression_ratio": 1.6134453781512605, "no_speech_prob": 5.04346530760813e-07}, {"id": 1680, "seek": 790594, "start": 7910.66, "end": 7915.7, "text": " From the father of neural networks and so here is the here is our implementation of his paper", "tokens": [3358, 264, 3086, 295, 18161, 9590, 293, 370, 510, 307, 264, 510, 307, 527, 11420, 295, 702, 3035], "temperature": 0.0, "avg_logprob": -0.15713687472873264, "compression_ratio": 1.6134453781512605, "no_speech_prob": 5.04346530760813e-07}, {"id": 1681, "seek": 790594, "start": 7915.7, "end": 7920.78, "text": " And this is an important thing to note right when very famous people like Jeffrey Hinton write a paper", "tokens": [400, 341, 307, 364, 1021, 551, 281, 3637, 558, 562, 588, 4618, 561, 411, 28721, 389, 12442, 2464, 257, 3035], "temperature": 0.0, "avg_logprob": -0.15713687472873264, "compression_ratio": 1.6134453781512605, "no_speech_prob": 5.04346530760813e-07}, {"id": 1682, "seek": 790594, "start": 7921.379999999999, "end": 7925.419999999999, "text": " Sometimes an entire implementation of that paper looks like one line of code", "tokens": [4803, 364, 2302, 11420, 295, 300, 3035, 1542, 411, 472, 1622, 295, 3089], "temperature": 0.0, "avg_logprob": -0.15713687472873264, "compression_ratio": 1.6134453781512605, "no_speech_prob": 5.04346530760813e-07}, {"id": 1683, "seek": 790594, "start": 7926.259999999999, "end": 7931.0599999999995, "text": " Okay, so let's do it before we got point six one two five seven", "tokens": [1033, 11, 370, 718, 311, 360, 309, 949, 321, 658, 935, 2309, 472, 732, 1732, 3407], "temperature": 0.0, "avg_logprob": -0.15713687472873264, "compression_ratio": 1.6134453781512605, "no_speech_prob": 5.04346530760813e-07}, {"id": 1684, "seek": 793106, "start": 7931.06, "end": 7936.820000000001, "text": " We'll fit it with exactly the same parameters and now we've got point five one and in fact", "tokens": [492, 603, 3318, 309, 365, 2293, 264, 912, 9834, 293, 586, 321, 600, 658, 935, 1732, 472, 293, 294, 1186], "temperature": 0.0, "avg_logprob": -0.17876681452212126, "compression_ratio": 1.7572463768115942, "no_speech_prob": 2.123369313267176e-06}, {"id": 1685, "seek": 793106, "start": 7936.820000000001, "end": 7941.400000000001, "text": " We can keep training point five oh so like this tweak really really really helped", "tokens": [492, 393, 1066, 3097, 935, 1732, 1954, 370, 411, 341, 29879, 534, 534, 534, 4254], "temperature": 0.0, "avg_logprob": -0.17876681452212126, "compression_ratio": 1.7572463768115942, "no_speech_prob": 2.123369313267176e-06}, {"id": 1686, "seek": 793106, "start": 7941.900000000001, "end": 7948.26, "text": " Okay, and one of the nice things about this tweak was before I could only use a learning rate of one in egg three", "tokens": [1033, 11, 293, 472, 295, 264, 1481, 721, 466, 341, 29879, 390, 949, 286, 727, 787, 764, 257, 2539, 3314, 295, 472, 294, 3777, 1045], "temperature": 0.0, "avg_logprob": -0.17876681452212126, "compression_ratio": 1.7572463768115942, "no_speech_prob": 2.123369313267176e-06}, {"id": 1687, "seek": 793106, "start": 7948.9400000000005, "end": 7953.02, "text": " Before it started going crazy, but after I use the identity matrix", "tokens": [4546, 309, 1409, 516, 3219, 11, 457, 934, 286, 764, 264, 6575, 8141], "temperature": 0.0, "avg_logprob": -0.17876681452212126, "compression_ratio": 1.7572463768115942, "no_speech_prob": 2.123369313267176e-06}, {"id": 1688, "seek": 793106, "start": 7953.02, "end": 7959.5, "text": " I've found I could use one in egg two because it's you know it's better behaved weight initialization. I found I could use a higher", "tokens": [286, 600, 1352, 286, 727, 764, 472, 294, 3777, 732, 570, 309, 311, 291, 458, 309, 311, 1101, 48249, 3364, 5883, 2144, 13, 286, 1352, 286, 727, 764, 257, 2946], "temperature": 0.0, "avg_logprob": -0.17876681452212126, "compression_ratio": 1.7572463768115942, "no_speech_prob": 2.123369313267176e-06}, {"id": 1689, "seek": 795950, "start": 7959.5, "end": 7961.34, "text": " learning rate", "tokens": [2539, 3314], "temperature": 0.0, "avg_logprob": -0.1920463277938518, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.2377358820667723e-06}, {"id": 1690, "seek": 795950, "start": 7961.34, "end": 7964.02, "text": " Okay, and honestly these things", "tokens": [1033, 11, 293, 6095, 613, 721], "temperature": 0.0, "avg_logprob": -0.1920463277938518, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.2377358820667723e-06}, {"id": 1691, "seek": 795950, "start": 7966.22, "end": 7968.34, "text": " You know increasingly we're trying to", "tokens": [509, 458, 12980, 321, 434, 1382, 281], "temperature": 0.0, "avg_logprob": -0.1920463277938518, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.2377358820667723e-06}, {"id": 1692, "seek": 795950, "start": 7968.94, "end": 7975.06, "text": " Incorporate into the defaults in fast AI you you know you don't want necessarily increasingly need to actually know them", "tokens": [39120, 2816, 473, 666, 264, 7576, 82, 294, 2370, 7318, 291, 291, 458, 291, 500, 380, 528, 4725, 12980, 643, 281, 767, 458, 552], "temperature": 0.0, "avg_logprob": -0.1920463277938518, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.2377358820667723e-06}, {"id": 1693, "seek": 795950, "start": 7975.26, "end": 7977.26, "text": " But you know at this point", "tokens": [583, 291, 458, 412, 341, 935], "temperature": 0.0, "avg_logprob": -0.1920463277938518, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.2377358820667723e-06}, {"id": 1694, "seek": 795950, "start": 7977.9, "end": 7983.1, "text": " We're still at a point where you know most things in most libraries most of the time don't have great defaults", "tokens": [492, 434, 920, 412, 257, 935, 689, 291, 458, 881, 721, 294, 881, 15148, 881, 295, 264, 565, 500, 380, 362, 869, 7576, 82], "temperature": 0.0, "avg_logprob": -0.1920463277938518, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.2377358820667723e-06}, {"id": 1695, "seek": 795950, "start": 7983.1, "end": 7984.62, "text": " It's good to know all these little tricks", "tokens": [467, 311, 665, 281, 458, 439, 613, 707, 11733], "temperature": 0.0, "avg_logprob": -0.1920463277938518, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.2377358820667723e-06}, {"id": 1696, "seek": 798462, "start": 7984.62, "end": 7991.94, "text": " It's also nice to know if you want to improve something what kind of tricks people have used elsewhere because you can often borrow them yourself", "tokens": [467, 311, 611, 1481, 281, 458, 498, 291, 528, 281, 3470, 746, 437, 733, 295, 11733, 561, 362, 1143, 14517, 570, 291, 393, 2049, 11172, 552, 1803], "temperature": 0.0, "avg_logprob": -0.1510381939030495, "compression_ratio": 1.6655405405405406, "no_speech_prob": 2.090448106173426e-06}, {"id": 1697, "seek": 798462, "start": 7992.94, "end": 7994.38, "text": " All right", "tokens": [1057, 558], "temperature": 0.0, "avg_logprob": -0.1510381939030495, "compression_ratio": 1.6655405405405406, "no_speech_prob": 2.090448106173426e-06}, {"id": 1698, "seek": 798462, "start": 7994.38, "end": 7998.18, "text": " Well, that's the end of the lesson today, and so next week we will", "tokens": [1042, 11, 300, 311, 264, 917, 295, 264, 6898, 965, 11, 293, 370, 958, 1243, 321, 486], "temperature": 0.0, "avg_logprob": -0.1510381939030495, "compression_ratio": 1.6655405405405406, "no_speech_prob": 2.090448106173426e-06}, {"id": 1699, "seek": 798462, "start": 7999.0199999999995, "end": 8001.22, "text": " Look at this idea of a stateful RNN", "tokens": [2053, 412, 341, 1558, 295, 257, 1785, 906, 45702, 45], "temperature": 0.0, "avg_logprob": -0.1510381939030495, "compression_ratio": 1.6655405405405406, "no_speech_prob": 2.090448106173426e-06}, {"id": 1700, "seek": 798462, "start": 8001.22, "end": 8007.08, "text": " That's going to keep its hidden state around and then we're going to go back to looking at language models again", "tokens": [663, 311, 516, 281, 1066, 1080, 7633, 1785, 926, 293, 550, 321, 434, 516, 281, 352, 646, 281, 1237, 412, 2856, 5245, 797], "temperature": 0.0, "avg_logprob": -0.1510381939030495, "compression_ratio": 1.6655405405405406, "no_speech_prob": 2.090448106173426e-06}, {"id": 1701, "seek": 798462, "start": 8007.08, "end": 8013.62, "text": " And then finally we're going to go all the way back to computer vision and learn about things like resnets and batch norm", "tokens": [400, 550, 2721, 321, 434, 516, 281, 352, 439, 264, 636, 646, 281, 3820, 5201, 293, 1466, 466, 721, 411, 725, 77, 1385, 293, 15245, 2026], "temperature": 0.0, "avg_logprob": -0.1510381939030495, "compression_ratio": 1.6655405405405406, "no_speech_prob": 2.090448106173426e-06}, {"id": 1702, "seek": 801362, "start": 8013.62, "end": 8018.68, "text": " And all the tricks that were in figured out in cats versus dogs see you then", "tokens": [50364, 400, 439, 264, 11733, 300, 645, 294, 8932, 484, 294, 11111, 5717, 7197, 536, 291, 550, 50617], "temperature": 0.0, "avg_logprob": -0.1842440304003264, "compression_ratio": 1.0555555555555556, "no_speech_prob": 1.9523653463693336e-05}], "language": "en"}