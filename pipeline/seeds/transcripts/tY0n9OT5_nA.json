{"text": " I want to start pointing out a couple of the many cool things that happened this week. One thing that I'm really excited about is we briefly talked about how Leslie Smith has a new paper out. And basically the paper takes these previous two key papers, Cyclical Learning Rates and Superconvergence, and builds on them with a number of experiments to show how you can achieve superconvergence. So superconvergence lets you train models five times faster than previous stepwise approaches. It's not five times faster than CLR, but it's faster than CLR as well. And the key is that superconvergence lets you get up to massively high learning rates, somewhere between 1 and 3, which is quite amazing. And so the interesting thing about superconvergence is that you actually train at those very high learning rates for quite a large percentage of your epochs. And during that time, the loss doesn't really improve very much, but the trick is like it's doing a lot of searching through the space to find really generalizable areas, it seems. So we kind of had a lot of what we needed in fast.ai to achieve this, but we were missing a couple of bits. And so Sylvain Gouga has done an amazing job of fleshing out the pieces that we're missing and then confirming that he has actually achieved superconvergence on training on SciFi 10. So I think this is the first time that this has been done that I've heard of outside of Lesley Smith himself. So he's got a great blog post up now on one cycle, which is what Lesley Smith called this approach. And this is actually, it turns out, what one cycle looks like. It's a single cyclical learning rate, but the key difference here is that the going-up bit is the same length as the going-down bit. So you go up really slowly. And then at the end, for like a tenth of the time, you then have this little bit where you go down even further. And it's interesting, obviously this is a very easy thing to show, a very easy thing to explain. Sylvain has added it to FastAI, temporarily it's called use CLR beta. By the time you watch this on the video, it'll probably be called one cycle or something like that. But you can use this right now. So that's one key piece to getting these massively high learning rates. And he shows a number of experiments when you do that. A second key piece is that as you do this to the learning rate, you do this to the momentum. So when the learning rate is low, it's fine to have a high momentum. But when the learning rate gets up really high, your momentum needs to be quite a bit lower. So this is also part of what he's added to the library, is this cyclical momentum. And so with these two things, you can train for about the fifth of the number of epochs with a stepwise learning rate schedule. Then you can drop your weight decay down by about two orders of magnitude. You can often remove most or all of your dropout, and so you end up with something that's trained faster and generalizes better. It actually turns out that Sylvain got quite a bit better accuracy than Leslie Smith's paper. His guess, I was pleased to see, is because our data augmentation defaults are better than Leslie's. I hope that's true. So check that out. Another cool thing, there's been so many cool things this week, I'm just going to pick two. Hamil Hussain, who works at GitHub, I just really like this. There's a fairly new project called Qplo, which is basically a TensorFlow for Kubernetes. Hamil wrote a very nice article about magical sequence-to-sequence models, building data products on that, using Kubernetes to put that in production and so forth. He said that the Google Qflow team created a demo based on what he wrote earlier this year, directly based on the skills learned in class AI, and I will be presenting this technique at KDD. KDD is one of the top academic conferences. So I wanted to share this as a motivation for folks to blog, which I think is a great point. Nobody who goes out and writes a blog thinks that probably none of us really think our blog is actually going to be very good, probably nobody is going to read it, and then when people actually do like it and read it, it's like with great surprise, you just go, oh, it's actually something people were interested to read. So here is the tool where you can summarize GitHub issues using this tool, which is now hosted by Google on the Qflow.org domain. So I think that's a great story of getting, if Hamil didn't put his work out there, none of this would have happened. You can check out his post that made it all happen as well. So talking of the magic of sequence-to-sequence models, let's build one. So we're going to be specifically working on machine translation. So machine translation is something that's been around for a long time, but specifically we're going to look at a protocol called neural translation, which is using neural networks for translation. And that wasn't really a thing in any kind of meaningful way until a couple of years ago. And so thanks to Chris Manning from Stanford for the next three slides. 2015, Chris pointed out that neural machine translation first appeared properly, and it was pretty crappy compared to the statistical machine translation approaches that use kind of classic feature engineering and standard MLP approaches of lots of stemming and fiddling around with word frequencies and n-grams and lots of stuff. By a year later, it was better than everything else. This is on a metric called BLUE. We're not going to discuss the metric because it's not a very good metric and it's not very interesting, but it's what everybody uses. So that was BLUE as of when Chris did this slide. As of now, it's about up here, it's about 30. So we're kind of seeing machine translation starting down the path that we saw starting computer vision object classification in 2012, I guess, which is we just surpassed the state of the art and now we're zipping past it at a great rate. It's very unlikely that anybody watching this is actually going to build a machine translation model, because you can go to translate.google.com and use theirs and it works quite well. So why are we learning about machine translation? The reason we're learning about machine translation is that the general idea of taking some kind of input like a sentence in French and transforming it into some other kind of output of arbitrary length such as a sentence in English is a really useful thing to do. For example, the thing that we just saw that Hamil at GitHub did takes GitHub issues and turns them into summaries. Other examples is taking videos and turning them into descriptions. Or taking basically anything where you're spitting out an arbitrary-sized output, very often that's a sentence, so maybe taking a CT scan and spitting out a radiology report, this is where you can use sequence to sequence. So the important thing about neural machine translation, these are more slides from Chris, and generally sequence to sequence models, is that there's no fussing around with heuristics and hacky feature engineering, whatever it is, end-to-end training. We're able to build these distributed representations which are shared by lots of concepts within a single network. We're able to use long-term state in the RNN, so use a lot more context than N-gram type approaches. In the end, the text we're generating uses an RNN as well so we can build something that's more fluid. We're going to use a bidirectional LSTM with attention. Actually, we're going to use a bidirectional GRU with attention, but basically the same thing. So you already know about bidirectional recurrent neural networks and attention we're going to add on top today. These general ideas you can use for lots of other things as well as Chris points out on this slide. So let's jump into the code, which is in the translate notebook, funnily enough. And so we're going to try to translate French into English. And so the basic idea is that we're going to try and make this look as much like a standard neural network approach as possible. So we're going to need three things, you'll remember the three things, data, a suitable architecture and a suitable loss function. Once you've got those three things, you run fit. And all things going well, you end up with something that solves your problem. So data, we generally need x, y pairs. Because we need something which we can feed it into the loss function and say I took my x value, which was my French sentence, and the loss function says it was meant to generate this English sentence, and then you had your predictions, which you would then compare and see how good it is. So therefore we need lots of these tuples of French sentences with their equivalent English sentence. That's called a parallel corpus. Obviously this is harder to find than a corpus for a language model, because for a language model we just need text in some language, which you can basically, for any living language of which the people that use that language, like use computers, there will be a few gigabytes at least of text floating around the internet for you to grab. So building a language model is only challenging corpus-wise for ancient languages. One of our students is trying to do a Sanskrit one, for example, at the moment, but that's very rarely a problem. For translation, there are actually some pretty good parallel corpus available for European languages. The European Parliament basically has every sentence in every European language. Anything that goes through the UN is translated to lots of languages. For French to English, we have a particularly nice thing, which is pretty much any semi-official Canadian website will have a French version and an English version. So this chap, Chris Callison Birch, did a cool thing, which is basically to try to transform French URLs into English URLs by replacing \"-fr with \"-en\", hoping that that retrieves the equivalent document, and then did that for lots and lots of websites, and ended up creating a huge corpus based on millions of web pages. So French to English, we have this particularly nice resource. So we're going to start out by talking about how to create the data, then we'll look at the architecture, and then we'll look at the loss function. And so for bounding boxes, all of the interesting stuff was in the loss function, but for new translation, all of the interesting stuff is going to be in the architecture. So let's zip through this pretty quickly. One of the things I want you to think about particularly is what are the relationships or similarities in terms of the task we're doing and how we do it between language modeling versus neural translation. So the basic approach here is that we're going to take a sentence, so this case the example is English to German, and this slide is from Stephen Meridy, we steal everything we can from Stephen. We start with some sentence in English, and the first step is to do basically the exact same thing we do in a language model, which is to chuck it through an RNN. Now with our language model, actually let's not even think about language model, let's start even easier, the classification model. So something that turns this sentence into positive or negative sentiment. We had a decoder, something that basically took the RNN output, and from our paper we grabbed 3 things. We took a max pool over all of the time steps, we took a mean pool over all the time steps, and we took the value of the RNN at the last time step, stuck all those together, and put it through a linear layer. Most people don't do that in most NLP stuff, I think it's something we invented. People pretty much always use the last time step, so all the stuff we'll be talking about today uses the last time step. So we start out by chucking this sentence through an RNN, and out of it comes some state. So some state meaning some hidden state, some vector that represents the output of an RNN that has encoded that sentence. You'll see the word that Stephen used here was encoder. We've tended to use the word backbone. So like when we've talked about adding a custom head to an existing model, like the existing pre-trained ImageNet model, for example, we kind of say that's our backbone, and then we stick on top of it some head that does the task we want. In sequence-to-sequence learning, they use the word encoder. But basically it's the same thing, it's some piece of a neural network architecture that takes the input and turns it into some representation, which we can then stick a few more layers on top of to grab something out of it, such as we did for the classifier where we stuck a linear layer on top of it to turn it into a sentiment, positive or negative. So this time though, we have something that's a little bit harder than just getting sentiment, which is I want to turn this state not into a positive or negative sentiment, but into a sequence of tokens where that sequence of tokens is the German sentence that we want. So this is sounding more like the language model than the classifier, because the language model had multiple tokens for every input word there was an output word. But the language model was also much easier, because the number of tokens in the language model output was the same length as the number of tokens in the language model input. And not only were they the same length, they exactly matched up. It's like after word 1 comes word 2, after word 2 comes word 3, and so forth. But for translating language, you don't necessarily know that the word he will be translated as the first word in the output, and that loved will be the second word in the output. In this particular case, unfortunately, they are the same. But very often the subject-object order will be different, or there will be some extra words inserted, or some pronouns will need to add some gendered article to it, or whatever. So this is the key issue we're going to have to deal with, is the fact that we have an arbitrary length output where the tokens in the output do not correspond to the same order of specific tokens in the input. But the general idea is the same. There's an RNN to encode the input, turns it into some hidden state, and then this is the new thing we're going to learn is generating a sequence output. So we already know sequence to class, that's IMDB classifier, we already know sequence to equal length sequence where it corresponds to the same items, that's the language model for example, but we don't know yet how to do a general purpose sequence to sequence, so that's the new thing today. Very little of this will make sense unless you really understand Lesson 6, how an RNN works. So if some of this lesson doesn't make sense to you and you find yourself wondering what does he mean by hidden state exactly, how is that working, go back and re-watch Lesson 6 to give you a very quick review. We learned that an RNN at its heart is a standard fully connected network, so here's one with one, two, three, four layers, takes an input and puts it through four layers, but then at the second layer it can just concatenate in a second input, third layer concatenate in a third input, but we actually wrote this in Python as just literally a four-layer neural network. There was nothing else we used other than linear layers and values. We used the same weight matrix every time an input came in, we used the same matrix every time we went from one of these states to the next, and that's why these arrows are the same color, and so we can redraw that previous thing like this. And so not only did we redraw it, but we took the four lines of linear, linear, linear, linear code in PyTorch and we replaced it with a for loop. So remember we had something that did exactly the same thing as this, but it just had four lines of code saying linear, linear, linear, linear, and we literally replaced it with a for loop because that's nice to refactor. So like literally that refactoring, which doesn't change any of the math, any of the ideas, any of the outputs, that refactoring is an RNF. It's turning a bunch of separate lines of code into a Python for loop. And so that's how we can draw it. We could take the output so that it's not outside the loop and put it inside the loop, like so, and if we do that, we're now going to generate a separate output for every input. So in this case, this particular one here, the hidden state gets replaced each time and we end up just spitting out the final hidden state. So this one is this example. But if instead we had something that said h's.append h and returned h's at the end, that would be this picture. And so go back and re-look at that notebook if this is unclear. I think the main thing to remember is when we say hidden state, we're referring to a vector. See here, here's the vector. h equals torch.zeros and hidden. Now of course it's a vector for each thing in the minibash. So it's a matrix. But generally when I speak about these things, I ignore the minibash piece and treat it as just a single item. So it's just a vector of this length. We also learnt that you can stack these layers on top of each other. So rather than this first RNN spitting out output, it could just spit out inputs into a second RNN. If you're thinking at this point, I think I understand this, but I'm not quite sure. If you're anything like that me, that means you don't understand this. And the only way you know and actually understand it is to go and write this from scratch in PyTorch or NumPy. And if you can't do that, then you know you don't understand it, and you can go back and re-watch lesson 6 and check out the notebook and copy some of the ideas until you can, it's really important that you can write that from scratch. It's less than a screen of code. So you want to make sure you can create a two-layer RNN. And this is what it looks like if you unroll it. So that's the goal, to get to a point where we first of all have these x, y pairs of sentences and we're going to do French to English. So we're going to start by downloading this dataset. Training a translation model takes a long time. Google's translation model has 8 layers of RNN stacked on top of each other. There's no conceptual difference between 8 layers and 2 layers. It's just like if you're Google and you have more GPUs or TPUs and you know what to do with, then you're fine doing that. Whereas in our case, it's pretty likely that the kind of sequence to sequence models we're building are not going to require that level of computation. So to keep things simple, let's do a cut-down thing where rather than learning how to translate French into English for any sentence, let's learn to translate French questions into English questions. Specifically questions that start with what, where, which, when. So you can see here I've got a regex that looks for things that start with wh and end with a question mark. So I just go through the corpus, open up each of the 2 files, each line is one parallel text, zip them together, grab the English question and the French question and check whether they match the regular expressions. So we now have 52,000 sentences and here are some examples of those sentence pairs. One nice thing about this is that what, who, where type questions tend to be fairly short, which is nice. But I would say the idea that we could learn from scratch with no previous understanding of the idea of language, let alone of English or of French, that we could create something that can translate one to the other for any arbitrary question with only 50,000 sentences, it sounds like a ludicrously difficult thing to ask this to do. So I will be impressed if we can make any progress whatsoever, because this is very little data to do a very complex exercise. So this contains the tuples of French and English. You can use this handy idiom to split them apart into a list of English questions and a list of French questions. And then we tokenize the English questions and we tokenize the French questions. So remember that just means splitting them up into separate words or word-like things. By default, the tokenizer that we have here, and remember this is a wrapper around the spaCy tokenizer, which is a fantastic tokenizer, this wrapper by default assumes English. So to ask for French, you just add an extra parameter. The first time you do this, you'll get an error saying that you don't have the spaCy French model installed, and you can Google to get the command something Python-m spaCy download French, or something like that, to grab the French model. I don't think any of you are going to have RAM problems here, because this is not particularly big corpus, but I know that some of you were trying to train new language models during the week and were having RAM problems. If you do, it's worth knowing what these functions are actually doing. So for example, these ones here is processing every sentence across multiple processes. That's what the MP means. And remember, fastai code is designed to be pretty easy to read. So here's the 3 lines of code to process all MP. Find out how many CPUs you have, divide by 2, because normally with hyperthreading, they don't actually all work in parallel. Then in parallel, run this process function. So that's going to spit out a whole separate Python process for every CPU you have. If you have a lot of cores, that's a lot of Python processes, everyone's going to load the whole data in, and that can potentially use up all your RAM. So you could replace that with just proc all, rather than proc all MP, to use less RAM. Or you could use less cores. So at the moment, we're calling this function partition by cores, which calls partition on a list, and asks to split it into a number of equal length things according to how many CPUs you have. So you could replace that, splitting it into a smaller list, and run it on less things. Yes, Rachel. Question. Was an intention layer tried in the language model? Do you think it would be a good idea to try and add one? We haven't learned about attention yet, so let's ask about things that we have got to, not things we haven't. The short answer is no, I haven't tried it properly. Yes, you should try it, because it might help. In general, there's going to be a lot of things that we covered today, which if you've done some sequence-to-sequence stuff before, you'll want to know about something we haven't covered yet. I'm going to cover all the sequence-to-sequence things. So at the end of this, if I haven't covered the thing you wanted to know about, please ask me then. If you ask me before, I'll be answering something based on something that I'm about to teach you. Okay, so having tokenized the English and French, you can see how it gets spit out. And you can see that tokenization for French is quite different looking, because French loves their apostrophes and their hyphens and stuff. So if you try to use an English tokenizer for a French sentence, you're going to get a pretty crappy outcome. So I don't find you need to know heaps of NLP ideas to use deep learning for NLP, but just some basic stuff like, use the right tokenizer for your language is important. And so some of the students this week in our study group have been trying to build language models for Chinese, for instance, which of course doesn't really have the concept of a tokenizer in the same way. So we've been starting to look at, briefly mentioned last week, this Google thing called sentence piece, which basically spits things into arbitrary subword units. And so when I say tokenize, if you're using a language that doesn't have spaces in, you should probably be checking out sentence piece or some other similar subword unit thing instead. And hopefully in the next week or two we'll be able to report back with some early results of these experiments with Chinese. So having tokenized it, we'll save that to disk. And then remember the next step after we create tokens is to turn them into numbers. And to turn them into numbers, we have two steps. The first is to get a list of all of the words that appear, and then we turn every word into the index, into that list. If there are more than 40,000 words that appear, then let's cut it off there so it doesn't get too crazy. And we insert a few extra tokens for beginning of stream, padding, end of stream, and unknown. So if we try to look up something that wasn't in the 40,000 most common, then we use a default dict to return 3, which is unknown. So now we can go ahead and turn every token into an id by putting it through the string to integer dictionary we just created. And then at the end of that, let's add the number 2, which is end of stream. And you'll see the code you see here is the code I write when I'm iterating and experimenting. Because 99% of the code I write when I'm iterating and experimenting turns out to be totally wrong or stupid or embarrassing and you don't get to see it. But there's no point refactoring that and making it beautiful when I'm writing it. I'm wanting you to see all the little shortcuts I have. So rather than doing this properly and actually having some constant or something for end of stream marker and using it, when I'm prototyping, I just do the easy stuff. I mean not so much that I end up with broken code, but I try to find some mid-ground between beautiful code and code that works. Just heard him mention that we divide number of CPUs by 2 because with hyperthreading, we don't get a speedup using all the hyperthreaded cores. Is this based on practical experience or is there some underlying reason why we wouldn't get additional speedup? Yeah, it's just practical experience. And it's like not all things kind of seem like this, but I definitely noticed with tokenization, hyperthreading seemed to slow things down a little bit. Also if I use all the cores, like often I want to do something else at the same time, I generally run some interactive notebook and I don't have any spare room to do that. It's a minor issue. So now for our English and our French, we can grab our list of IDs. And when we do that, of course, we need to make sure that we also store the vocabulary. There's no point having IDs if we don't know what the number 5 represents. There's no point having a number 5. So that's our vocabulary, and the reverse mapping string to int that we can use to convert more corpuses in the future. So just to confirm it's working, we can go through each ID, convert the int to a string and spit that out, and there we have our thing back, now with an end of stream marker at the end. Our English vocab is 17000, our French vocab is 25000. So there's not too big, not too complex a vocab that we're dealing with, which is nice to know. So we spent a lot of time on the forums during the week discussing how pointless word vectors are and how you should stop getting so excited about them. We're now going to use them. Why is that? Actually all the stuff we've been learning about using language models and pre-trained proper models rather than pre-trained linear single layers, which is what word vectors are, I think applies equally well to sequence to sequence, but I haven't tried it yet. So Sebastian and I are starting to look at that. I'm slightly distracted by preparing this class at the moment, but after this class is done. So there's a whole thing, for anybody interested in creating some genuinely new, highly publishable results, the entire area of sequence to sequence with pre-trained language models hasn't been touched yet, and I strongly believe it's going to be just as good as classification stuff. If you work on this and you get to the point where you have something that's looking exciting and you want help publishing it, I'm very happy to help co-author papers on stuff that's looking good. So feel free to reach out if and when you have some interesting results. So at this stage, we don't have any of that, so we're going to use very little fast AI actually and very little in terms of fast AI ideas. So all we've got is word vectors. Anyway, so let's at least use decent word vectors. So word2vec is very old word vectors. There are better word vectors now, and fast text is a pretty good source of word vectors. There's hundreds of languages available for them. Your language is likely to be represented. So to grab them, you can click on this link, download word vectors for a language that you're interested in, install the fast text Python library. It's not available on PyPy, but here's a handy trick. If there is a GitHub repo that has a setup.py in it and a requirements.txt in it, you can just chuck git plus at the start and then stick that in your pip install and it works. Like hardly anybody seems to know this. Like if you go to the fast text repo, they won't tell you this. They'll say you have to download it and cd into it, blah blah blah, but you don't. You can just run that. Which you can also use for the fast AI library, by the way. If you want to pip install the latest version of fast AI, you can totally do this. So you grab the library, import it, load the model, so here's my English model and here's my French model. So see there's a text version and a binary version, the binary version is a bit faster. We're going to use that. The text version is also a bit buggy. And then I'm going to convert it into a standard Python dictionary to make it a bit easier to work with. So this is just going to go through each word with a dictionary comprehension and save it as a Pickhold dictionary. So now we've got our Pickhold dictionary. We can go ahead and look up a word, for example, comma, and that will return a vector. The length of that vector is the dimensionality of this set of word vectors. So in this case we've got 300 dimensional English and French word vectors. For reasons that you'll see in a moment, I also want to find out what the mean of my vectors are and the standard deviation of my vectors are. So the mean is about 0 and the standard deviation is about 0.3. Often corpuses have a pretty long-tailed distribution of sequence length and it's the longest sequences that kind of tend to overwhelm how long things take and how much memory is used and stuff like that. So I'm going to grab, in this case, the 99th and 97th percentile of the English and French and truncate them to that amount. Originally I was using the 90th percentile, so these are poorly named variables. So that's just truncating them. So we're nearly there. We've got our tokenized, numericalized English and French data set. We've got some word vectors. So now we need to get it ready for PyTorch. So PyTorch expects a data set object and hopefully by now you all can tell me that a data set object requires two things, a length and an indexer. So I started out writing this and I was like, okay, I need a sec to sec data set. I started out writing it and I thought, okay, we're going to have to pass it our x's and our y's and store them away. And then my indexer is going to need to return a numpy array of the x's at that point and a numpy array of the y's at that point. And oh, that's it. So then after I wrote this, I realized I haven't really written a sec to sec data set. I've just written a totally generic data set. So here's the simplest possible data set that works for any pair of arrays. So it's now poorly named. It's much more general than a sec to sec data set, but that's what I needed it for. This a function, remember we've got v for variables, t for tensors, a for arrays. So this basically goes through each of the things you pass it. If it's not already a numpy array, it converts it into a numpy array and returns back a tuple of all of the things that you passed it, which are now guaranteed to be numpy arrays. So that's a, v, t, three very handy little functions. So that's it. That's our data set. So now we need to grab our English and French IDs and get a training set and a validation set. So one of the things which is pretty disappointing about a lot of code out there on the internet is that they don't follow some simple best practices. For example, if you go to the PyTorch website, they have an example section for sequence to sequence translation. Their example does not have a separate validation set. I tried it, training according to their settings, and I tested it with a validation set and it turned out that it overfit massively. So this is not just a theoretical problem. The actual PyTorch repo has the actual official sequence to sequence translation example, which does not check for overfitting and overfits horribly. Also it fails to use mini-batches, so it actually fails to utilize any of the efficiency of PyTorch whatsoever. So there's a lot of like, even if you find code in the official PyTorch repo, don't assume it's any good at all. The other thing you'll notice is that everybody, pretty much every other sequence to sequence model I've found in PyTorch anywhere on the internet has clearly copied from that shitty PyTorch repo because it all has the same variable names, it has the same problems, it has the same mistakes. Like another example, nearly every PyTorch convolutional neural network I've found does not use an adaptive pooling layer. So in other words, the final layer is always like average pool 7,7. So they assume that the previous layer is 7x7, and if you use any other size input, you get an exception. And therefore, nearly everybody I've spoken to that uses PyTorch thinks that there is a fundamental limitation of CNNs, that they are tied to the input size. That has not been true since VGG. So every time we grab a new model and stick it in the fast AI repo, I have to go in, search for pool, and add adaptive to the start and replace the 7 with a 1, and now it works on any sized object. So just be careful, it's still early days, and believe it or not, even though most of you have only started in the last year your deep learning journey, you know quite a lot more about a lot of the more important practical aspects than the vast majority of people that are publishing and writing stuff in official repos and stuff. So you kind of need to have a little more self-confidence than you might expect when it comes to reading other people's code. If you find yourself thinking, that looks odd, it's not necessarily you. It might well be them. So I would say like at least 90% of deep learning code that I start looking at turns out to have like deathly serious problems that make it completely unusable for anything. And so I've been telling people that I've been working with recently, if the repo you're looking at doesn't have a section on it saying here's the test we did where we got the same results as the paper that it's going to be implementing, that almost certainly means they haven't got the same results of the paper they're implementing, they probably haven't even checked. And if you run it, it definitely won't get those results. Because it's hard to get things right the first time. It takes me 12 goes, probably takes normal smarter people than me 6 goes, but if they haven't tested it once, it almost certainly won't work. So there's our sequence data set. Let's get the training and validation sets. Here's an easy way to do that. Grab a bunch of random numbers, one for each row of your data. See if they're bigger than 0.1 or not. That gets you a list of bools. Index into your array with that list of bools to grab a training set. Index into that array with the opposite of that list of bools to get your validation set. So it's a nice easy way. There's lots of ways of doing it. I just like to do different ways so you can see a few approaches. So now we can create our data set with our x's and our y's, French and English. If you want to translate instead English to French, switch these two around and you're done. Now we need to create data loaders. We can just grab our data loader and pass in our data set and batch size. We actually have to transpose the arrays. I'm not going to go into the details about why. We can talk about it during the week if you're interested, but have a think about why we might need to transpose their orientation. But there's a few more things I want to do. One is that since we've already done all the preprocessing, there's no point spawning off multiple workers to do augmentation or whatever because there's no work to do. So making num workers equals one will save you some time. We have to tell it what our padding index is. That's actually pretty important because what's going to happen is that we've got different length sentences and fastai, I think it's pretty much the only library that does this, fastai will just automatically stick them together and pad the shorter ones so they all end up equal length. Because remember a tensor has to be rectangular. In the decoder in particular, I actually want my padding to be at the end, not at the start. For a classifier, I want the padding at the start because I want that final token to represent the last word of the Moody review. But in the decoder, as you'll see, it actually is going to work out a bit better to have padding at the end. So I say prepad equals pos. And then finally, since we've got sentences of different lengths coming in and they all have to be put together in a mini-batch to be the same size by padding, we would much prefer that the sentence in a mini-batch are of similar sizes already because otherwise it's going to be as long as the longest sentence and that's going to end up wasting time and memory. So therefore I'm going to use the sampler trick that we learned last time, which is the validation set, we're going to ask it to sort everything by length first. And then for the training set, we're going to ask it to randomize the order of things but to roughly make it so that things of similar length are about in the same spot. So we've got our sort sampler and our sort-ish sampler. And then at that point, we can create a model data object. So a model data object really does one thing, which is it says I have a training set and a validation set and an optional test set and sticks them into a single object. We also have a path so that it has somewhere to store temporary files, models, stuff like that. So we're not using fast AI for very much at all in this example, just kind of a minimal set to show you how to get your model data object. In the end, once you've got a model data object, you can then create a learner and you can then call fit. So that's kind of like minimal amount of fast AI stuff here. This is a standard PyTorch compatible data set. This is a standard PyTorch compatible data loader. Behind the scenes, it's actually using the fast AI version because I do need to do this automatic padding for convenience. There's a few tweaks in our version that are a bit faster and a bit more convenient. The fast AI samplers we're using, but there's not too much going on here. So now we've got our model data object. We can basically tick off number one. So as I said, most of the work is in the architecture. And so the architecture is going to take our sequence of tokens, it's going to spit them into an encoder, or in computer vision terms, what we've been calling a backbone, something that's going to try and turn this into some kind of representation. So that's just going to be an RNN. That's going to spit out the final hidden state, which for each sentence is just a vector. And so that's all going to take, none of this is going to be new. That's all going to be using very direct simple techniques that we've already learnt. And then we're going to take that and we're going to spit it into a different RNN, which is a decoder. And that's going to have some new stuff because we need something that can go through one word at a time. And it's going to keep going until it thinks it's finished the sentence. It doesn't know how long the sentence is going to be ahead of time. Keeps going until it thinks it's finished the sentence and then it stops and returns the sentence. So let's start with the encoder. So in terms of variable naming here, there's basically identical variables for encoder and decoder, the encoder versions have enc, the decoder versions have dec. So for the encoder, here's our embeddings. And so I always try to mention what the mnemonics are rather than writing things out in too long hand. So just remember, enc is an encoder, dec is a decoder, env is an embedding. The final thing that comes out is out. The RNN in this case is a GRU, not an LSTM. They're nearly the same thing. So don't worry about the difference. You could replace it with an LSTM and you'll get basically the same results. To replace it with an LSTM, simply type lstm. So we need to create an embedding layer. Remember what we're being passed is the index of the words into a vocabulary, and we want to grab their fast text embedding. And then over time, we might want to also fine-tune to train that embedding end-to-end. So to create an embedding, we'll call create embedding up here. So we'll just say nn.embedding. So it's important that you know now how to set the rows and columns for your embedding. So the number of rows has to be equal to your vocabulary size. So each vocabulary item has a word vector. And how big is your embedding? Well, in this case, it was determined by fast text. And the fast text embeddings are size 300. So we have to use size 300 as well, otherwise we can't start out by using their embeddings. So what we want to do is this is initially going to give us a random set of embeddings. And so we're going to now go through each one of these, and if we find it in fast text, we'll replace it with a fast text embedding. So again, something that you should already know is that a PyTorch module that is learnable has a weight attribute, and the weight attribute is a variable, and the variables have a data attribute, and the data attribute is a tensor. Now, you'll notice very often today I'm saying here is something you should know, not so that you think, oh, I don't know that, I'm a bad person, but so that you think, okay, this is a concept that I haven't learned yet, and Jeremy thinks I ought to know about, and so I've got to write that down, and I'm going to go home, and I'm going to Google, because this is a normal PyTorch attribute in every single learnable PyTorch module. This is a normal PyTorch attribute in every single PyTorch variable. And so if you don't know how to grab the weights out of a module, or you don't know how to grab the tensor out of a variable, it's going to be hard for you to build new things, or to debug things, or maintain things, or whatever. So if I say, you ought to know this, and you're thinking, I don't know this, don't run away and hide, go home and learn the thing, and if you're having trouble learning the thing, because you can't find documentation about it, or you don't understand that documentation, or you don't know why Jeremy thought it was important you know it, jump on the forum, and say, please explain this thing, here's my best understanding of that thing, as I have it at the moment, here's the resources I've looked at, help fill me in. And normally, if I respond, it's very likely I will not tell you the answer, but I will instead give you something, a problem that you could solve, that if you solve it, will solve it for you, because I know that way it will be something you remember. So again, don't be put off if I'm like, okay, go read this link, try and summarize that thing, tell us what you think, I'm trying to be helpful, not unhelpful, and if you're still not following, just come back and say, I had a look, honestly, that link you sent, I don't know what it means, I don't know where to start, whatever. I'll keep trying to help you until you fully understand it. So now that we've got our weight tensor, we can just go through our vocabulary, and we can look up the word in our pre-trained vectors, and if we find it, we will replace the random weights with that pre-trained vector. The random weights have a standard deviation of 1, our pre-trained vectors, it turned out, had a standard deviation of about.3, so again, this is the kind of hacky thing I do when I'm prototyping stuff, I just modify it by 3. Obviously by the time you see the video of this, being able to put all this sequence-to-sequence stuff into the Fast.ai library, you won't find horrible hats like that in there, I sure hope, but hack away when you're prototyping. Some things won't be in Fast.txt, in which case we'll just keep track of it, and I've just added this print statement here just so that I can kind of see what's going, like why am I missing stuff, basically, I'll probably comment it out when I actually commit this to GitHub, but that's why that's there. So we create those embeddings, and so when we actually create the sequence-to-sequence RNN, it'll print out how many were missed, and so remember we had about 30,000 words, so we're not missing too many, and interesting, the things that are missing, well there's our special token for uppercase, not surprising that's missing. Also remember it's not token-to-vec, it's not token-text, it does words, so L apostrophe and D apostrophe and apostrophe S, they're not appearing either. So that's interesting, that does suggest that maybe we could have slightly better embeddings if we tried to find some which would have been tokenized the same way we tokenized, but that's okay. Do we just keep embedding vectors from training, why don't we keep all word embeddings in case you have new words in the test set? We're going to be fine-tuning them, and so, I don't know, it's an interesting idea, maybe that would work, I haven't tried it. Also you wouldn't, sorry can you use that? I asked the question, so you can also add random embedding to those, and at the beginning just keep them random, but you're going to have, it's going to make an effect in the sense that you're going to be using those words. I think it's an interesting line of inquiry, but I will say this, the vast majority of the time when you're kind of doing this in the real world, your vocabulary will be bigger than 40,000, and once your vocabulary is bigger than 40,000, using the standard techniques, the embedding layer gets so big that it takes up all your memory, it takes up all of the time in the back crop. There are tricks to dealing with very large vocabularies, I don't think we'll have time to handle them in this session, but you definitely would not want to have all three and a half million fast text vectors in an embedding layer. So I wonder, if you're not touching a word, it's not going to change, right? Like even if you're fine-tuning, you're not, you're hiding the memory. It's in GPU RAM, and you've got to remember, three and a half million times 300 times the size of a single precision floating point vector, plus all of the gradients for them, even if it's not touched. Without being very careful and adding a lot more code and stuff, it is slow and hard, and we wouldn't touch it for now. I think it's an interesting path of inquiry, but it's the kind of path of inquiry that leads to multiple academic papers, not something that you do on a weekend. I think it would be very interesting, maybe we can look at it sometime. And as I say, I have actually started doing some stuff around incorporating large vocabulary handling into FastAI. It's not finished, but hopefully by the time we get here, this kind of stuff will be possible. So we create our encoder embedding, add a bit of dropout, and then we create our RNN. The input to the RNN obviously is the size of the embedding by definition. Number of hidden is whatever we want, so we set it to 256 for now. However many layers we want, and some dropout inside the RNN as well. This is all standard PyTorch stuff, you could use LSTM here as well. And then finally we need to turn that into some output that we're going to feed to the decoder, so let's use a linear layer to convert the number of hidden into the decoder embedding size. So in the forward pass, here's how that's used. We first of all initialize our hidden state to a bunch of zeros. So we've now got a vector of zeros, and then we're going to take our input and put it through our embedding, we're going to put that through dropout, we then pass our currently zeros hidden state and our embeddings into our RNN, and it's going to spit out the usual stuff that RNN spit out, which includes the final hidden state. We're then going to take that final hidden state and stick it through that linear layer, so we now have something of the right size to feed to our decoder. So that's it. And again, this ought to be very familiar and very comfortable, it's like the most simple possible RNN, so if it's not, go back, check out lesson 6, make sure you can write it from scratch and that you understand what it does. But the key thing to know is that it takes our inputs and spits out a hidden vector that hopefully will learn to contain all of the information about what that sentence says and how it says it. Because if it can't do that, then we can't feed it into a decoder and hope it to spit out our sentence in a different language. So that's what we want it to learn to do. And we're not going to do anything special to make it learn to do that, we're just going to do the three things and cross our fingers, because that's what we do. So that's H is that S, it's the hidden state. I guess Stephen used S for state, I used H for hidden. You'd think the two Australians could agree on something like that, but apparently not. So how do we now do the new bit? And so the basic idea of the new bit is the same, we're going to do exactly the same thing, but we're going to write our own for loop. And so the for loop is going to do exactly what the for loop inside PyTorch does here, but we're going to do it manually. So we're going to go through the for loop. And how big is the for loop? It's an output sequence length. Well what is output sequence length? That's something that got passed to the constructor, and it is equal to the length of the largest English sentence. So we're going to do this for loop as long as the largest English sentence, because we're translating into English. So we can't possibly be longer than that. At least not in this corpus. If we then used it on some different corpus that was longer, this is going to fail. You could always pass in a different parameter, of course. So the basic idea is the same, we're going to go through and put it through the embedding, we're going to stick it through the RNN, we're going to stick it through dropout, and we're going to stick it through a linear layer. So the basic four steps are the same. And once we've done that, we're then going to append that output to a list, and then when we're going to finish, we're going to stack that list up into a single tensor and return it. That's the basic idea. Normally a recurrent neural network, here's our decoder recurrent neural network, works on a whole sequence at a time, but we've got a for loop to go through each part of the sequence separately, so we have to add a leading unit access to the start to basically say this is a sequence of length 1. So we're not really taking advantage of the recurrent net much at all. We could easily rewrite this with a linear layer actually. That would be an interesting experiment if you wanted to try it. So we basically take our input and we feed it into our embedding, and we add something to the front saying treat this as a sequence of length 1, and then we pass that to our RNN. We then get the output of that RNN, feed it into our dropout, and feed it into our linear layer. So there's two extra things to be aware of. Well I guess it's really one thing. The one thing is, what's this? What is the input to that embedding? And the answer is, it's the previous word that we translated. See how the input here is the previous word here. The input here is the previous word here. So the basic idea is, if you're trying to translate, if you're about to translate, tell me the fourth word of the new sentence, but you don't know what the third word you just said was, that's going to be really hard. So we're going to feed that in at each time step. Let's make it as easy as possible. And so what was the previous word at the start? Well there was none. So specifically, we're going to start out with a beginning of stream token. So the beginning of stream token is a 0. So let's start out our decoder with a beginning of stream token, which is 0. And of course we're doing a mini-batch, so we need batch size number of them. But let's just think about one part of that batch. So we start out with a 0. We look up that 0 in our embedding matrix to find out what the vector for the beginning of stream token is. We stick a unit axis on the front to say we have a single sequence length of beginning of stream token. We stick that through our RNN, which gets not only the fact that there's a 0 at the beginning of stream, but also the hidden state, which at this point is whatever came out of our encoder. So this now, its job is to try and figure out what is the first word. What's the first word to translate the sentence? Pop through some dropout, go through one linear layer in order to convert that into the correct size for our decoder embedding matrix. Append that to our list of translated words. And now we need to figure out what word that was because we need to feed it to the next time step. We need to feed it to the next time step. So remember what we actually output here, and don't forget, use a debugger. That pdb.setTrace, put it here, what is outp? Outp is a tensor. How big is the tensor? So before you look it up in the debugger, try and figure it out from first principles and check your right. So outp is a tensor whose length is equal to the number of words in our English vocabulary, and it contains the probability for every one of those words that it is that word. So then, if we now say outp.data.max that looks in its tensor to find out which word has the highest probability. And max in PyTorch returns two things. The first thing is what is that max probability, and the second is what is the index into the array of that max probability. And so we want that second item, index number 1, which is the word index with the largest thing. So now that contains the word, or the word index into our vocabulary of the word. If it's a 1, you might remember 1 was padding, then that means we're done. That means we've reached the end because we've finished with a bunch of padding. If it's not 1, let's go back and continue. Now decimp is whatever the highest probability word was. So we keep looping through either until we get to the largest length of a sentence, or until everything in our many batch is padding. And each time we've appended our outputs, not the word, but the probabilities, to this list, which we stack up into a tensor, and we can now go ahead and feed that to a loss function. So before we go to a break, since we've done 1 and 2, let's do 3, which is a loss function. The loss function is categorical cross-entropy loss. We've got a list of probabilities for each of our classes, where the classes are all the words in our English vocab, and we have a target which is the correct class, i.e. which is the correct word at this location. There's two tweaks, which is why we need to write our own little loss function, but you can see basically it's going to be cross-entropy loss. And the tweaks are as bullets. Tweak number 1 is we might have stopped a little bit early, and so the sequence length that we generated may be different to the sequence length of the target, in which case we need to add some padding. PyTorch padding function is weird. If you have a rank 3 tensor, which we do, we have sequence length by batch size by number of words in the vocab. A rank 3 tensor requires a 6 tuple. Each pair of things in that tuple is the padding before and then the padding after that dimension. So in this case, the first dimension has no padding, the second dimension has no padding, the third dimension has no padding on the left, and as match padding is required on the right. It's good to know how to use that function. Now that we've added any padding, that's necessary. The only other thing we need to do is cross-entropy loss expects a rank 2 tensor, but we've got sequence length by batch size. So let's just flatten out the sequence length and batch size into a, that's what that minus 1 in Vue does. So flatten out that for both of them, and now we can go ahead and call cross-entropy. That's it. So now we can just use standard approach. Here's our sequence to sequence RNN, that's this one here. So that is a standard PyTorch module. Stick it on the GPU. Only by now you've noticed you can call.cuda, but if you call to GPU, then it doesn't put it on the GPU if you don't have one. You can also set fastai.core.useGPU to false to force it to not use the GPU, and that can be super handy for debugging. We then need something that tells it how to handle learning rate groups. So there's a thing called single model that you can pass it to, which treats the whole thing as a single learning rate group. So this is like the easiest way to turn a PyTorch module into a fastai model. Here's the model data object we created before. We could then just call learner to turn that into a learner, but if we call RNN learner, RNN learner is a learner. It defines cross-entropy as the default criteria. In this case, we're overriding that anyway, so that's not what we care about. But it does add in these save encoder and load encoder things that can be handy sometimes. In this case, we really could have just said learner, but RNN learner also works. So here's how we turn our PyTorch module into a fastai model into a learner. And once we have a learner, give it our new loss function, and then we can call LR find and we can call fit, and it runs for a while, and we can save it. So all the normal learn stuff now works. Remember the model attribute of a learner is a standard PyTorch model, so we can pass that some x, which we can grab out of our validation set, or you could use learn.predict array or whatever you like to get some predictions. And then we can convert those predictions into words by going.max1 to grab the index of the highest probability words to get some predictions. And then we can go through a few examples and print out the French, the correct English, and the predicted English for things that are not padding. And here we go. So amazingly enough, this kind of like simplest possible written largely from scratch PyTorch module on only 50,000 sentences is sometimes capable on a validation set of giving you exactly the right answer, sometimes the right answer in slightly different wording, and sometimes sentences that aren't grammatically sensible or even have too many question marks. So we're well on the right track, I think you would agree. So even the simplest possible sec2sec trained for a very small number of epochs without any pre-training other than the use of word embeddings is surprisingly good. So I think the message here, and we're going to improve this in a moment after the break, but I think the message here is even sequence2sequence models that you think are simpler than could possibly work, even with less data than you think you could learn from can be surprisingly effective and in certain situations this may not even be enough for your needs. So we're going to learn a few tricks after the break which will make this much better. So let's come back at 7.50. So one question that came up during the break is that some of the tokens that are missing in fast text had a curly quote rather than a straight quote, for example, and the question was would it help to normalize punctuation? And the answer for this particular case is probably yes. The difference between curly quotes and straight quotes is rarely semantic. You do have to be very careful though because like it may turn out that people using beautiful curly quotes are like using more formal language and actually writing in a different way. So I generally, if you're going to do some kind of preprocessing like punctuation normalization, you should definitely check your results with and without because like nearly always that kind of preprocessing makes things worse even when I'm sure it won't. What may be some ways of regularizing these sequence2sequence models besides dropout and wait to get? Let me think about that during the week. Yeah, it's like, you know, AWD LSTM, which we've been relying on a lot has so many great, I mean it's all dropout, well not all dropout, there's dropout of many different kinds. And then there's the, we haven't talked about it much, but there's also a kind of regularization based on activations and stuff like that as well and on changes and whatever. I just haven't seen anybody put anything like that amount of work into regularization of sequence2sequence models and I think there's a huge opportunity for somebody to do like the AWD LSTM of sec2sec, which might be as simple as stealing all the ideas from AWD LSTM and using them directly in sec2sec, that'd be pretty easy to try, I think. And there's been an interesting paper that actually Stephen Meredith added in the last couple of weeks where he used an idea which I don't know if he stole it from me, but it was certainly something I had also recently done and talked about on Twitter. Either way, I'm thrilled that he's done it, which was to take all of those different AWD LSTM hyperparameters and train a bunch of different models and then use a random forest to find out with feature importance which ones actually matter the most and then figure out how to set them. So I think you could totally use this approach to figure out for sec2sec regularization approaches which ones are best and optimize them. That would be amazing. But at the moment, I don't know that there are additional ideas for sec2sec regularization that I can think of beyond what's in that paper for regular language model stuff and probably all those same approaches would work. So tricks. Trick number 1, go bidirectional. So for classification, my approach to bidirectional that I suggested you use is take all of your token sequences, spin them around and train a new language model and train a new classifier. I also mentioned the wiki text pre-trained model, if you replace FWD with BWD in the name, you'll get the pre-trained backward model I created for you. Get a set of predictions and then average the predictions just like a normal ensemble. And that's kind of how we do bidire for that kind of classification. There may be ways to do it end-to-end, but I haven't quite figured them out yet, they're not in Fast.ai yet, and I don't think anybody's written a paper about them yet, so if you figure it out, that's an interesting line of research. But because we're not doing massive documents where we have to kind of chunk it into separate bits and then pull over them and whatever, we can do bidire very easily in this case, which is literally as simple as adding bidirectional equals true to our encoder. People tend not to do bidirectional for the decoder, I think partly because it's kind of considered cheating, but I don't know, I was just talking to somebody at the break about it, maybe it can work in some situations, although it might need to be more of an ensembling approach in the decoder because it's a bit less obvious. The encoder is very, very simple, bidirectional equals true, and we now have, with bidirectional equals true, rather than just having an RNN which is going this direction, we have a second RNN that's going in this direction. And so that second RNN literally is visiting them, each token in the opposing order, so when we get the final hidden state, it's here, rather than here. But the hidden state is of the same size, so the final result is that we end up with a tensor that's got an extra 2 long axis. And depending on what library you use, often that will be then combined with the number of layers thing, so if you've got 2 layers and bidirectional, that tensor dimension is now of length 4. The pytorch, it kind of depends which bit of the process you're looking at as to whether you get a separate result for each layer and bidirectional bit and so forth. You have to look up the docs and it will tell you inputs, outputs, tensor sizes, appropriate for the number of layers, and whether you have bidirectional equals true. In this particular case, you'll basically see all the changes I've had to make. So for example, you'll see when I added bidirectional equals true, my linear layer now needs number of hidden times 2 to reflect the fact that we have that second direction in our hidden state now. You'll see in it hidden, it's now self.numberOfLayers times 2 here. So you'll see there's a few places where there's been an extra 2 that has to be thrown in. Why making a decoder bidirectional is considered cheating? It's not just that it's cheating, it's like we have this loop going on, you know. It's not as simple as just having 2 tensors. And then how do you turn those 2 separate loops into a final result? After talking about it during the break, I've kind of gone from like, hey everybody knows it doesn't work, to like, oh maybe it kind of could work, but it requires more thought. It's quite possible during the week I'll realize it's a dumb idea and I was being stupid, but we'll think about it. Another question people had, why do you need to have an end to that loop? Why do you need to have an end to that loop? You have like a range. When I start training, everything's random, so this will probably never be true. So later on, it will pretty much always break out eventually, but it's basically like we're going to go forever. It's really important to remember when you're designing an architecture that when you start, the model knows nothing about anything. So you kind of want to make sure it's doing something that's vaguely sensible. So bidirectional means we had, let's see how we go here, we got out to 358 cross entropy loss with a single direction, with bidirectional it's down to 351. So that improved it a bit, that's good. And as I say, it shouldn't really slow things down too much. So it does mean there's a little bit more sequential processing have to happen, but it's generally a good win. In the Google translation model of the 8 layers, only the first layer is bidirectional because it allows it to do more in parallel. So if you create really deep models, you may need to think about which one is bidirectional, otherwise you'll have performance issues. So 351. Now let's talk about teacher forcing. So teacher forcing is going to come back to this idea that when the model starts learning, it knows nothing about nothing. So when the model starts learning, it is not going to spit out uh at this point, it's going to spit out some random meaningless word. It doesn't know anything about German or about English or about the idea of language or anything. And it's going to feed it down here as an input and be totally unhelpful. And so that means that early learning is going to be very very difficult because it's feeding in an input that's stupid into a model that knows nothing and somehow it's going to get better. So that's, it's not asking too much, eventually it gets there, but it's definitely not as helpful as we can be. So what if instead of feeding in the thing I predicted just now, what if instead we feed in the actual correct word it was meant to be. Now we can't do that at inference time because by definition we don't know the correct word, we've been asked to translate it. We can't require a correct translation in order to do translation. So the way I've set this up is I've got this thing called PR force, which is probability of forcing. And if some random number is less than that probability, then I'm going to replace my decoder input with the actual correct thing. And if we've already gone too far, if it's already longer than the target sentence, I'm just going to stop because I can't give it the correct thing. So you can see how beautiful PyTorch is for this, because if you try to do this with some static graph thing like classic TensorFlow, well I try. One of the key reasons that we switched to PyTorch at this exact point in last year's class was because Jeremy tried to implement teacher forcing in Keras and TensorFlow and went even more insane than he started. It was weeks of getting nowhere. And then I literally on Twitter, I think it was on Dracapathy, I announced, said something about oh there's this thing called PyTorch that just came out and it's really cool. And I tried it that day. By the next day I had teacher forcing. And so I was like, oh my gosh. And all the stuff of trying to debug things, it was suddenly so much easier, and this kind of dynamic stuff is so much easier. So this is a great example of like, hey I get to use random numbers and if statements and stuff. So here's the basic idea, at the start of training, let's set PR force really high, so that nearly always it gets the actual correct previous word, and so it has a useful input. And then as I train a bit more, let's decrease PR force so that by the end PR force is 0 and it has to learn properly, which is fine because it's now actually feeding in sensible inputs most of the time anyway. So let's now write something such that in the training loop, it gradually decreases PR force. So how do you do that? Well one approach would be to write our own training loop. But let's not do that because we already have a training loop that has progress bars and uses exponential weighted averages to smooth out the losses and keeps track of metrics and you know it does a bunch of things which, they're not rocket science but they're kind of convenient, and they also kind of keep track of calling the reset for RNNs at the start of an epoch to make sure that the hidden state is set to zeros and you know little things like that. We'd rather not have to write that from scratch. So what we've tended to find is that as I start to write some new thing and I'm like oh I need to kind of replace some part of the code, I'll then kind of add some little hook so that we can all use that hook to make things easier. In this particular case, there's a hook that I've ended up using all the damn time now which is the hook called the stepper. And so if you look at our code, model.py is where our fit function lives, right? And so the fit function in model.py is kind of, we've seen it before, I think it's like the lowest level thing that doesn't require a learner, it doesn't really require anything much at all, it just requires a standard PyTorch model and a model data object. You just need to know how many epochs, a standard PyTorch optimizer, and a standard PyTorch loss function. So you can call, we've hardly ever used it in the class, we normally call learn.fit, but learn.fit calls this. This is our lowest level thing. But we've looked at the source code here sometimes, we've seen how it loops through each epoch and it loops through each thing in our batch and calls stepper.step. And so stepper.step is the thing that's responsible for calling the model, getting the loss, finding the loss function, and calling the optimizer. And so by default, stepper.step uses a particular class called stepper, which basically calls the model, so the model ends up inside m, zeros the gradients, calls the loss function, calls backwards, does gradient clipping if necessary, and then calls the optimizer. So you know, they're the basic steps that back when we looked at PyTorch from scratch we had to do. So the nice thing is, we can replace that with something else, rather than replacing the training loop. So if you inherit from stepper and then write your own version of step, you can just copy and paste the contents of step and add whatever you like. Or if it's something you're going to do before or afterwards, you could even call super.step. In this case, I rather suspect I've been unnecessarily complicated here. I probably could have commented out all of that and just said super.step x, y, epoch, because I think this is an exact copy of everything. But as I say, when I'm prototyping, I don't think carefully about how to minimize my code. I copied and pasted the contents of the code from step and I added a single line to the top which was to replace PR force in my module with something that gradually decreased linearly for the first 10 epochs and after 10 epochs, it was 0. So total hack, but good enough to try it out. And so the nice thing is that everything else is the same. I've added these 3 lines of code to my module. And the only thing I need to do that's different is when I call fit, I pass in my customized step of class. And so that's going to do teacher forcing. We don't have bidirectional, so we're just changing one thing at a time. So we should compare this to our unidirectional results, which was 3.58 and this is 3.49. So that was an improvement. So that's great. I needed to make sure I at least did 10 epochs because before that it was cheating by using the teacher forcing. So that's good, that's an improvement. So we've got another trick, and this next trick is a bigger trick. It's a pretty cool trick. And it's called attention. And the basic idea of attention is this, which is expecting the entirety of the sentence to be summarized into this single hidden vector is asking a lot. It has to know what was said and how it was said and everything necessary to create the sentence in German. And so the idea of attention is basically like maybe we're asking too much, particularly because we could use this form of model where we output every step of the loop to not just have a hidden state at the end, but to have a hidden state after every single word. And why not try and use that information? It's already there, and so far we've just been throwing it away. And not only that, but bidirectional, we've got every step, we've got two vectors of state that we can use. So how could we use this piece of state, this piece of state, this piece of state, this piece of state, and this piece of state rather than just the final state? And so the basic idea is, well, let's say I'm doing this word, translating this word right now. Which of these five pieces of state do I want? And of course the answer is, if I'm doing, well actually let's pick a more interesting word. Let's pick this one. So if I'm trying to do loved, then clearly the hidden state I want is this one, because this is the word. And then for this preposition, I probably would need this and this and this to make sure I've got the tense right and know that I actually need this part of the verb and so forth. So depending on which bit I'm translating, I'm going to need one or more bits of these various hidden states. And in fact, I probably want some weighting of them. So like what I'm doing here, I probably mainly want this state, but I maybe want a little bit of that one and a little bit of that one. So in other words, for these five pieces of hidden state, we want a weighted average. And we want it weighted by something that can figure out which bits of the sentence are most important right now. So how do we figure out something like which bits of the sentence are important right now? We create a neural net and we train the neural net to figure it out. When do we train that neural net? End to end. So let's now train two neural nets. Well we've actually already kind of got a bunch, right? We've got an RNN encoder, we've got an RNN decoder, we've got a couple of linear layers. What the hell? We've got to put the neural net into the mix. And this neural net is going to spit out a weight for every one of these things and we've got to take the weighted average at every step. And it's just another set of parameters that we learn all at the same time. And so that's called attention. So the idea is that once that attention has been learned, we can see this terrific demo from Chris Oller and Sean Carter, each different word is going to take a weighted average. See how the weights are different depending on which word is being translated? And you can see how it's kind of figuring out the color, the deepness of the blue is how much weight it's using. You can see that each word is basically, or here, which word are we translating from. So when we say European, we need to know that both of these two parts are going to be influenced. Or if we're doing economic, both of these three parts are going to be influenced, including the gender of the definite article, and so forth. So check out this distil.hub article. These things are all like little interactive diagrams. It basically shows you how attention works and what the actual attention looks like in a trained translation model. So let's try and implement attention. So with attention, it's basically, this is all identical, and the encoder is identical, and all of this bit of the decoder is identical. There's one difference, which is that we basically are going to take a weighted average. And the way that we're going to do the weighted average is we create a little neural net, which we're going to see here and here, and then we use softmax, because of course the nice thing about softmax is that we want to ensure that all of the weights that we're using add up to 1, and we also kind of expect that one of those weights should probably be quite a bit higher than the other ones. And so softmax gives us the guarantee that they add up to 1, and because it's the a to the in it, it tends to encourage one of the weights to be higher than the other ones. So let's see how this works. So what's going to happen is we're going to take the last layer's hidden state, and we're going to stick it into a linear layer, and then we're going to stick it into a nonlinear activation, and then we're going to do matrix multiply, and so if you think about it, linear layer, nonlinear activation, matrix multiply, that's a neural net. It's a neural net with one hidden layer. Stick it into a softmax, and then we can use that to weight our encoder outputs. So now, rather than just taking the last encoder output, we've got this is going to be the whole tensor of all of the encoder outputs, which I just weight by this little neural net that I created. And that's basically it. So what I'll do is I'll put on the wiki thread a couple of papers to check out. There was basically one amazing paper that really originally introduced this idea of attention. And I say amazing because it actually introduced a couple of key things which have really changed how people work in this field. This area of attention has been used not just for text, but for things like reading text out of pictures or doing various stuff with computer vision and stuff like that. And then there's a second paper which actually Jeffrey Hinton was involved in called Grammar as a Foreign Language, which used this idea of RNNs with attention to basically try to replace rules-based grammar with an RNN which automatically basically tagged the grammatical each word based on this grammar and turned out to do it better than any rules-based system. Which today actually kind of seems obvious. I think we're now used to the idea that neural nets do lots of this stuff better than rules-based systems, but at the time it was considered really surprising. One nice thing is that their summary of how attention works is really nice and concise. Let's go back and look at our original encoder. So an RNN spits out two things. It spits out a list of the state after every time step and it also tells you the state at the last time step. And we used the state at the last time step to create the input state for our decoder, which is what we see here, one vector. But we know that it's actually creating a vector at every time step, so wouldn't it be nice to use them all? But wouldn't it be nice to use the ones that are most relevant to translating the word I'm translating now? So wouldn't it be nice to be able to take a weighted average of the hidden state at each time step, weighted by whatever is the appropriate weight right now, which for example in this case, LBTA would definitely be time step number 2, because that's the word I'm translating. So how do we get a list of weights that is suitable for the word we're training right now, or the answer is by training a neural net to figure out the list of weights. And so any time we want to figure out how to train a little neural net that does any task, the easiest way normally always to do that is to include it in your module and train it in line with everything else. The minimal possible neural net is something that contains 2 layers and 1 non-linear activation function. So here is one linear layer, and in fact instead of a linear layer, we can even just grab a random matrix, if we don't care about bias. And so here's a random matrix, it's just a random tensor wrapped up in a parameter. A parameter remember is just a PyTorch variable, it's like identical to a variable, but it just tells PyTorch I want you to learn the weights for this. So here we've got a linear layer, here we've got a random matrix, and so here at this point where we start out our decoder, let's take that final, let's take the current hidden state of the decoder, put that into a linear layer, because what's the information we use to decide what words we should focus on next? The only information we have to go on is what the decoder's hidden state is now. So let's grab that, put it into the linear layer, put it through a non-linearity, put it through one more non-linear layer, this one actually doesn't have a bias in it, so it's actually just a matrix-multiply, put that into a softmax, and that's it, that's a little neural net. It doesn't do anything, it's just a neural net, no neural nets do anything, they're just linear layers with non-linear activations with random weights. But it starts to do something if we give it a job to do, and in this case the job we give it to do is to say don't just take the final state, but now let's use all of the encoder states, and let's take all of them and multiply them by the output of that little neural net. And so given that the things in this little neural net are learnable weights, hopefully it's going to learn to weight those encoder outputs, those encoder hidden states by something useful. That's all a neural net ever does, is we give it some random weights to start with and a job to do and hope that it learns to do the job. And it turns out that it does. So everything else in here is identical to what it was before. We've got teacher forcing, it's not bidirectional. So we can see how this goes. Teacher forcing had 3.49, and so now we've got nearly exactly the same thing, but we've got this little minimal neural net figuring out what weightings to give our inputs. Oh wow, now it's down to 3.37. Remember these things are logs, so e to the power of this is quite a significant change. So 3.37, let's try it out. Not bad, right? Where are they located? What are their skills? What do you do? They're still not perfect, why or why not, but quite a few of them are correct. And again, considering that we're asking it to learn about the very idea of language for two different languages and how to translate them between the two, and grammar and vocabulary, and we only have 50,000 sentences and a lot of the words only appear once, I would say this is actually pretty amazing. Yes, Jeanette? Why do we use tanh instead of relu for attention mininet? I don't quite remember. It's been a while since I looked at it. You should totally try using relu and see how it goes. Obviously tanh, the key difference is that it can go in each direction and it's limited both at the top and the bottom. I know very often, like for the gates inside RNNs and LSTMs and GRUs, tanh often works out better, but it's been about a year since I actually looked at that specific question, so I'll look at it during the week. The short answer is you should try a different activation function and see if you can get a better result. I'd be interested to hear what you find out. So what we can do also is we can actually grab the attentions out of the model. So I actually added this return attention equals true here. See here, my forward? Like forward, you can put anything you like in forward. So I added a return attention parameter, false by default, because obviously the training loop doesn't know anything about it. But then I just had something here saying if return attention, then stick the attentions on as well. The attentions is simply that value a, just check it in a list. So we can now call the model with return attention equals true and get back the probabilities and the attentions, which means as well as printing out these here, we can draw pictures at each time step of the attention. And so you can see at the start, the attentions are all on the first word, second word, third word, a couple of different words, and this is just for one particular sentence. So you can kind of see, this is the equivalent, when you're Chris Oller and Sean Carter, you make things that look like this. When you're Jeremy Howard, the exact same information looks like this. But it's the same thing, just pretend that it's beautiful. So you can see basically at each different time step, we've got a different attention. And it's really important when you try to build something like this, like you don't really know if it's not working. Because if it's not working, and as per usual, my first 12 attempts at this were broken, and they were broken in the sense that it wasn't really learning anything useful, and so therefore it was basically giving equal attention to everything, and therefore it wasn't worse. It just wasn't better. It wasn't much better. And so until you actually find ways to visualize the thing in a way that you know what it ought to look like ahead of time, you don't really know if it's working. So it's really important that you try to find ways to kind of check your intermediate steps and your outputs. Yes, Yonet? I think there is a little bit of a, so people are asking, what is the loss function for the attentional neural network? No, no, no loss function for the attentional neural network. It's trained end to end. So it's just sitting here inside our decoder loop. So the loss function for the decoder loop is that this result contains, it's exactly the same as before, just the outputs, the probabilities of the words. So like the loss function, it's the same loss function. So how come the little mini neural nets learning something? Well, because in order to make the outputs better and better, it would be great if it made the weights of this little weighted average better and better. So part of creating our output is to please do a good job of finding a good set of weights. And if it doesn't do a good job of finding a good set of weights, then the loss function won't improve from that bit. So like end to end learning means like you throw in everything that you can into one loss function, and the gradients of all the different parameters point in a direction that says basically, hey, if you put more weight over there, it would have been better. And thanks to the magic of the chain rule, it then knows, oh, it would have put more weight over there if you would change the parameter in this matrix, multiply a little bit over there. And so that's the magic of end to end learning. So it's a very understandable question of like, how did this little mini neural network? But you've got to realize, there's nothing particularly about this code that says, hey, this particular bit is a separate little mini neural network, any more than the GRU is a separate little neural network, or this linear layer is a separate little function. Like it all ends up pushed into one output, which is a bunch of probabilities, which ends up in one loss function that returns a single number that says this either was or wasn't a good translation. And so thanks to the magic of the chain rule, we then back propagate little updates to all the parameters to make them a little bit better. So this is a big, weird, counterintuitive idea, and it's totally okay if it's a bit mind-bending. And it's the bit where, even back to lesson one, it's like, how did we make it find dogs versus cats? We didn't. All we did was we said, this is our data, this is our architecture, this is our loss function, please back propagate into the weights to make them better. And after you've made them better a while, it'll start finding cats from dogs. It's just in this case, we haven't used somebody else's convolutional network architecture. We've said here's a custom architecture which we hope is going to be particularly good at this problem. And even without this custom architecture, it was still okay, but then when we kind of made it in a way that made more sense to what we think it ought to do, it worked even better. But at no point did we kind of do anything different other than say, here's a data, here's an architecture, here's a loss function, go and find the parameters please. And it did it, because that's what neural nets do. So that is sequence-to-sequence learning. And if you want to encode an image into using a CNN backbone of some kind and then pass that into a decoder, which is like a RNN with a tension, and you make your Y values the actual correct captions for each of those images, you will end up with an image caption generator. If you do the same thing with videos and captions, you'll end up with a video caption generator. If you do the same thing with 3D CT scans and radiology reports, you'll end up with a radiology report generator. If you do the same thing with GitHub issues and people's chosen summaries of them, you'll get a GitHub issue summary generator. Sector-sec, I agree, they're magical, but they work. And I don't feel like people have begun to scratch the surface of how to use sector-sec models in their own domains. So not being a GitHub person, it would never have occurred to me that it would be kind of cool to start with some issue and automatically create a summary. But now I'm like, of course, next time I go to GitHub, I want to see a summary written there for me. I don't want to write my own damn commit message through that. Why should I write my own summary of the code review when I finished adding comments to lots of clients? It should do that for me as well. Now I'm thinking, GitHub is so behind, it could be doing this stuff. So what are the things in your industry that you could start with a sequence and generate something from it? I can't begin to imagine. So again, it's a fairly new area. The tools for it are not easy to use. They're not even built into FastAI yet, as you can see. Hopefully they will be soon. I don't think anybody knows what the opportunities are. So I've got good news and bad news. The bad news is we have 20 minutes to cover a topic which in last year's course took a whole lesson. The good news is that when I went to rewrite this using FastAI and PyTorch, I ended up with almost no code. So all of the stuff that made it hard last year is basically gone now. So we're going to do something bringing together for the first time our two little worlds we focused on, text and images. We're going to try and bring them together. And so this idea came up really in a paper by this extraordinary deep learning practitioner and researcher named Andrea Fromm. And Andrea was at Google at the time. And her basic crazy idea was to say words can have a distributed representation, a space, which particularly at that time really was just word vectors. And images can be represented in a space. In the end, if we have a fully connected layer, they kind of ended up as a vector representation. Could we merge the two? Could we somehow encourage the vector space that the images end up with be the same vector space that the words are in? And if we could do that, what would that mean? What could we do with that? So what could we do with that covers things like, well, what if I'm wrong? What if I'm predicting that this image is a Beagle and I predict Jumbo Jet and Yannet's model predicts Corgi? The normal loss function says that Yannet and Jeremy's models are equally good, i.e. they're both wrong. But what if we could somehow actually say, Corgi's closer to Beagle than it is to Jumbo Jet. So Yannet's model is better than Jeremy's. And we should be able to do that, right, because in word vector space, Beagle and Corgi are pretty close together, but Jumbo Jet, not so much. So it would give us a nice situation where hopefully our inferences would be wrong in saner ways if they're wrong. It would also allow us to search for things that aren't in our ImageNet, like a category in ImageNet, like dog and cat. Why did I have to train a whole new model to find dogs vs. cats when we already had something that found Corgis and Tabbies? Why can't I just say find me dogs? Well, if I trained it in word vector space, I totally could, right, because there's now a word vector I can find things with the right image vector, and so forth. So we'll look at some cool things we can do with it in a moment, but first of all, let's train a model where this model is not learning a category, a one-hot encoded ID where every category is equally far from every other category. Let's instead train a model where we're finding the dependent variable, which is a word vector. So what word vector? Well, obviously the word vector for the word you want. So if it's Corgi, let's train it to create a word vector that's the Corgi word vector, and if it's a Jumbo Jet, let's train it with a dependent variable that says this is the word vector for a Jumbo Jet. So as I said, it's now shockingly easy. So let's grab the fast text word vectors again, load them in, we only need English this time, and so here's an example of the word vector for King. It's just 300 numbers. So for example, little j Jeremy and big J Jeremy have a correlation of.6, I don't like bananas at all, this is good, banana, and Jeremy,.14. So words that you would expect to be correlated are correlated in words that should be as far away from each other as possible. Unfortunately they're still slightly correlated, but not so much. So let's now grab all of the ImageNet classes, because we actually want to know which one's Corgi and which one's Jumbo Jet. So we've got a list of all of those up on files.fast.ai, we can grab them. And let's also grab a list of all of the nouns in English, which I've made available here as well. So here are the names of each of the 1000 ImageNet classes, and here are all of the nouns in English according to WordNet, which is a popular thing for kind of representing what words are and aren't. So we can now go ahead and load that list of nouns, load the list of ImageNet classes, turn that into a dictionary. So these are the class IDs for the 1000 ImageNet classes that are in the competition dataset. So here's an example, n01 is a tench, which apparently is a kind of fish. Let's do the same thing for all those WordNet nouns. And you can see, actually it turns out that ImageNet is using WordNet class names, so that makes it nice and easy to map between the two. And WordNet, the most basic thing is an entity, and then that includes an abstraction, or a physical entity can be an object, and so forth. So these are our two worlds, we've got the ImageNet 1000 and we've got the 82000, which are in WordNet. So we want to map the two together, which is as simple as creating a couple of dictionaries to map them based on the SYNSET ID or the WordNet ID. It turns out that 49469, so SYNSET to WordVector. So what I need to do now is grab the 82000 nouns in WordNet and try and look them up in FastText. And so I've managed to look up 49000 of them in FastText. So I've now got a dictionary that goes from SYNSET ID, which is what WordNet calls them, to WordVectors. So that's what this dictionary is, SYN to WordVector. And I've also got the same thing specifically for the 1000 WordNet classes. So save them away, that's fine. Now I grab all of ImageNet, which you can actually download from Kaggle now. If you look up the Kaggle ImageNet localization competition, that contains the entirety of the ImageNet classifications as well. It's got a validation set of 28650 items in it. And so I can basically just grab for every image in ImageNet, I can grab using that SYNSET to WordVector, grab its FastText WordVector, and I can now stick that into this ImageVectors array, stack that all up into a single matrix, and save that away. And so now what I've got is something for every ImageNet image. I've also got the FastText WordVector that it's associated with. Just by looking up the SYNSET ID, going to WordNet, then going to FastText, and grabbing the WordVector. And so here's a cool trick. I can now create a model data object, which specifically is an image classifier data object. And I've got this thing called FromNamesInArray. I'm not sure if you've used it before, but we can pass it a list of file names, and so these are all of the file names in ImageNet, and we can just pass it an array of our dependent files. And so this is all of the FastText WordVectors. And then I can pass in the validation indexes, which in this case is just all of the last IDs. I need to make sure that they're the same as ImageNet uses, otherwise I'll be cheating. And then I pass in continuous equals true, which means this puts a lie again to this image classifier data. It's now really an image regressor data. So continuous equals true means don't one-hot encode my outputs, but treat them just as continuous values. So now I've got a model data object that contains all of my file names, and for every file name, a continuous array representing the WordVector for that. So I have an X, I have a Y, so I have data. Now I need an architecture and a loss function. Once I've got that, I should be done. So let's create an architecture, and so we'll revise this next week. But basically we can use the tricks we've learned so far, but it's actually incredibly simple. FastAI has a ConvLearner builder, which is what when you say ConvLearner.pre-trained, it calls this. And you basically say, okay, what architecture do you want? So we're going to use ResNet-50. How many classes do you want? In this case, it's not really classes, it's how many outputs do you want, which is the length of the FastText WordVector, 300. Obviously it's not multi-class classification, it's not classification at all. Is it regression? Yes, it is regression. And then you can just say, all right, what fully connected layers do you want? So I'm just going to add one fully connected layer, hidden layer of length 1024. Why 1024? Well, I've got the last layer of ResNet-50 is I think is 1024 long. The final output I need is 300 long. I obviously need my penultimate layer to be longer than 300, otherwise there's not enough information. So I kind of just picked something a bit bigger. Maybe different numbers would be better, but this worked for me. How much dropout do you want? I found that the default dropout I was consistently underfitting, so I just decreased the dropout from 0.5 to 0.2. And so this is now a convolutional neural network that does not have any softmax or anything like that because it's regression, it's just a linear layer at the end. And that's basically it. That's my model. So I can create a conv learner from that model, give it an optimization function. So now all I need, I've got data, I've got an architecture. So the architecture, because I said I've got this many 300 outputs, it knows that there are 300 outputs because that's the size of this array. So now all I need is a loss function. Now the default loss function for regression is L1 loss, so the absolute differences. That's not bad, but unfortunately in really high dimensional spaces, anybody who's studied a bit of machine learning probably knows this, in really high dimensional spaces, in this case it's 300 dimensional, basically everything is on the outside. And when everything is on the outside, distance is, it's not meaningless, but it's a little bit awkward. Things tend to be close together or far away, it doesn't really mean much in these really high dimensional spaces where everything is on the edge. What does mean something though is that if one thing is on the edge over here, and one thing is on the edge over here, you can form an angle between those vectors. The angle is meaningful. And so that's why we use cosine similarity when we're basically looking for how close or far apart are things in high dimensional spaces. And if you haven't seen cosine similarity before, it's basically the same as Euclidean distance, but it's normalized to be basically a unit norm, so basically divide by the length. So we don't care about the length of the vector, we only care about its angle. So there's a bunch of stuff that you could easily learn in a couple of hours, but if you haven't seen it before, it's a bit mysterious. For now just know that plus functions in high dimensional spaces where you're trying to find similarity, you care about angle and you don't care about distance. If you didn't use this custom loss function, it would still work. I tried it. It's just a little bit less good. So we've got an architecture, we've got data, we've got a loss function, therefore we're done. So we can go ahead and fit. Now I'm training on all of ImageNet, that's going to take a long time, so pre-compute equals true is your friend. You remember pre-compute equals true? That's that thing we learned ages ago that caches the output of the final convolutional layer and just trains the fully connected bit. And even with pre-compute equals true, it takes like 3 minutes to train an epoch on all of ImageNet. So I trained it for a while longer, so that's like an hour's worth of training. It's pretty cool that with FastAI, we can train a new custom head on all of ImageNet for 40 epochs in an hour or so. And so at the end of all that, we can now say, let's grab the 1000 ImageNet classes and let's predict on a whole validation set. And let's just take a look at a few pictures. And because the validation set is ordered, all the stuff is the same type, it's in the same place. I don't know what this thing is. And what we can now do is we can now use nearest neighbors search. So nearest neighbors search means here's one 300-dimensional vector, here's a whole lot of other 3-dimensional vectors, which things is it closest to? And normally that takes a very long time because you have to look through every 300-dimensional vector, calculate its distance and find out how far away it is. But there's an amazing, almost unknown library called nmslib that does that incredibly fast. Like almost nobody's heard of it. Some of you may have tried other nearest neighbors libraries. I guarantee this is faster than what you're using. I can tell you that because it's been benchmarked by people who do this stuff for a living. This is by far the fastest on every possible dimension. So this is basically a super fast way. We basically look here, this is angular distance. So we want to create an index on angular distance and we're going to do it on all of our image net word vectors, add in a whole batch, create the index, and now I can query a bunch of vectors all at once, get their 10 nearest neighbors. Uses multi-threading, it's absolutely fantastic, this library. You can install it from pip, it just works. And it tells you how far away they are and their indexes. So we can now go through and print out the top 3. So it turns out that bird actually is a limkin. So here are, this is the top 3 for each one. Interestingly this one doesn't say it's a limkin, and I looked it up, it's the fourth one. I don't know much about birds, but everything else here is brown with white spots, that's not. So I don't know if that's actually a limkin or if it's a mislabel, but sure as hell it doesn't look like the other birds. So I thought that was pretty interesting. It's kind of saying I don't think it's that. Now this is not a particularly hard thing to do, because there's only 1000 image net classes and it's not doing anything new. But what if we now bring in the entirety of WordNet and we now say which of those 45,000 things is it closest to? Exactly the same. So it's now searching all of WordNet. So now let's do something a bit different, which is take all of our predictions, so basically take our whole validation set of images and create a k and n index of the image representations, because remember it's predicting things that are meant to be word vectors. And now let's grab the fast text vector for boat, and boat is not an image net concept. And yet I can now find all of the images in my predicted word vectors in my validation set that are closest to the word boat. And it works, even though it's not something that was ever trained on. What if we now take engine's vector and boat's vector and take their average? And what if we now look in our nearest neighbors for that, these are boats with engines. I mean yes, this is actually a boat with an engine. It just happens to have wings on as well. By the way, sail is not an image net thing. Boat is not an image net thing. Here's the average of two things that are not image net things. And yet with one exception, it's found me two sailboats. Let's do something else crazy. Let's open up an image in the validation set. Here it is. I don't know what it is. Let's call predict array on that image to get its kind of word vector-like thing. And let's do a nearest neighbor search on all the other images. And here's all the other images of whatever the hell that is. So you can see this is crazy. We've trained a thing on all of image net in an hour using a custom head that required basically two lines of code. And these things run in like 300 milliseconds to do these searches. I actually taught this basic idea last year as well, but it was in Keras and it was just like pages and pages and pages of code and everything took a long time and it was complicated. And back then I kind of said, I can't begin to think all the stuff you could do with this. I don't think anybody's really thought deeply about this yet, but I think it's fascinating. And so go back and read the device paper, because like Andrea had a whole bunch of other thoughts. And now that it's so easy to do, hopefully people will dig into this now, because I think it's crazy and amazing. Alright, thanks everybody. See you next week.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.08, "text": " I want to start pointing out a couple of the many cool things that happened this week.", "tokens": [286, 528, 281, 722, 12166, 484, 257, 1916, 295, 264, 867, 1627, 721, 300, 2011, 341, 1243, 13], "temperature": 0.0, "avg_logprob": -0.18999750222732772, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0031722092535346746}, {"id": 1, "seek": 0, "start": 7.08, "end": 12.82, "text": " One thing that I'm really excited about is we briefly talked about how Leslie Smith has", "tokens": [1485, 551, 300, 286, 478, 534, 2919, 466, 307, 321, 10515, 2825, 466, 577, 28140, 8538, 575], "temperature": 0.0, "avg_logprob": -0.18999750222732772, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0031722092535346746}, {"id": 2, "seek": 0, "start": 12.82, "end": 14.88, "text": " a new paper out.", "tokens": [257, 777, 3035, 484, 13], "temperature": 0.0, "avg_logprob": -0.18999750222732772, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0031722092535346746}, {"id": 3, "seek": 0, "start": 14.88, "end": 23.28, "text": " And basically the paper takes these previous two key papers, Cyclical Learning Rates and", "tokens": [400, 1936, 264, 3035, 2516, 613, 3894, 732, 2141, 10577, 11, 49173, 804, 15205, 497, 1024, 293], "temperature": 0.0, "avg_logprob": -0.18999750222732772, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0031722092535346746}, {"id": 4, "seek": 2328, "start": 23.28, "end": 30.880000000000003, "text": " Superconvergence, and builds on them with a number of experiments to show how you can", "tokens": [4548, 1671, 331, 15260, 11, 293, 15182, 322, 552, 365, 257, 1230, 295, 12050, 281, 855, 577, 291, 393], "temperature": 0.0, "avg_logprob": -0.15017832142032989, "compression_ratio": 1.6686746987951808, "no_speech_prob": 2.429897358524613e-05}, {"id": 5, "seek": 2328, "start": 30.880000000000003, "end": 32.88, "text": " achieve superconvergence.", "tokens": [4584, 1687, 1671, 331, 15260, 13], "temperature": 0.0, "avg_logprob": -0.15017832142032989, "compression_ratio": 1.6686746987951808, "no_speech_prob": 2.429897358524613e-05}, {"id": 6, "seek": 2328, "start": 32.88, "end": 41.88, "text": " So superconvergence lets you train models five times faster than previous stepwise approaches.", "tokens": [407, 1687, 1671, 331, 15260, 6653, 291, 3847, 5245, 1732, 1413, 4663, 813, 3894, 1823, 3711, 11587, 13], "temperature": 0.0, "avg_logprob": -0.15017832142032989, "compression_ratio": 1.6686746987951808, "no_speech_prob": 2.429897358524613e-05}, {"id": 7, "seek": 2328, "start": 41.88, "end": 46.44, "text": " It's not five times faster than CLR, but it's faster than CLR as well.", "tokens": [467, 311, 406, 1732, 1413, 4663, 813, 12855, 49, 11, 457, 309, 311, 4663, 813, 12855, 49, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.15017832142032989, "compression_ratio": 1.6686746987951808, "no_speech_prob": 2.429897358524613e-05}, {"id": 8, "seek": 4644, "start": 46.44, "end": 55.519999999999996, "text": " And the key is that superconvergence lets you get up to massively high learning rates,", "tokens": [400, 264, 2141, 307, 300, 1687, 1671, 331, 15260, 6653, 291, 483, 493, 281, 29379, 1090, 2539, 6846, 11], "temperature": 0.0, "avg_logprob": -0.10867754842194033, "compression_ratio": 1.6388888888888888, "no_speech_prob": 1.6442299966001883e-05}, {"id": 9, "seek": 4644, "start": 55.519999999999996, "end": 60.56, "text": " somewhere between 1 and 3, which is quite amazing.", "tokens": [4079, 1296, 502, 293, 805, 11, 597, 307, 1596, 2243, 13], "temperature": 0.0, "avg_logprob": -0.10867754842194033, "compression_ratio": 1.6388888888888888, "no_speech_prob": 1.6442299966001883e-05}, {"id": 10, "seek": 4644, "start": 60.56, "end": 70.44, "text": " And so the interesting thing about superconvergence is that you actually train at those very high", "tokens": [400, 370, 264, 1880, 551, 466, 1687, 1671, 331, 15260, 307, 300, 291, 767, 3847, 412, 729, 588, 1090], "temperature": 0.0, "avg_logprob": -0.10867754842194033, "compression_ratio": 1.6388888888888888, "no_speech_prob": 1.6442299966001883e-05}, {"id": 11, "seek": 4644, "start": 70.44, "end": 74.28, "text": " learning rates for quite a large percentage of your epochs.", "tokens": [2539, 6846, 337, 1596, 257, 2416, 9668, 295, 428, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.10867754842194033, "compression_ratio": 1.6388888888888888, "no_speech_prob": 1.6442299966001883e-05}, {"id": 12, "seek": 7428, "start": 74.28, "end": 80.6, "text": " And during that time, the loss doesn't really improve very much, but the trick is like it's", "tokens": [400, 1830, 300, 565, 11, 264, 4470, 1177, 380, 534, 3470, 588, 709, 11, 457, 264, 4282, 307, 411, 309, 311], "temperature": 0.0, "avg_logprob": -0.13504720891563637, "compression_ratio": 1.6127659574468085, "no_speech_prob": 8.267799785244279e-06}, {"id": 13, "seek": 7428, "start": 80.6, "end": 87.52, "text": " doing a lot of searching through the space to find really generalizable areas, it seems.", "tokens": [884, 257, 688, 295, 10808, 807, 264, 1901, 281, 915, 534, 2674, 22395, 3179, 11, 309, 2544, 13], "temperature": 0.0, "avg_logprob": -0.13504720891563637, "compression_ratio": 1.6127659574468085, "no_speech_prob": 8.267799785244279e-06}, {"id": 14, "seek": 7428, "start": 87.52, "end": 92.2, "text": " So we kind of had a lot of what we needed in fast.ai to achieve this, but we were missing", "tokens": [407, 321, 733, 295, 632, 257, 688, 295, 437, 321, 2978, 294, 2370, 13, 1301, 281, 4584, 341, 11, 457, 321, 645, 5361], "temperature": 0.0, "avg_logprob": -0.13504720891563637, "compression_ratio": 1.6127659574468085, "no_speech_prob": 8.267799785244279e-06}, {"id": 15, "seek": 7428, "start": 92.2, "end": 93.32, "text": " a couple of bits.", "tokens": [257, 1916, 295, 9239, 13], "temperature": 0.0, "avg_logprob": -0.13504720891563637, "compression_ratio": 1.6127659574468085, "no_speech_prob": 8.267799785244279e-06}, {"id": 16, "seek": 7428, "start": 93.32, "end": 99.38, "text": " And so Sylvain Gouga has done an amazing job of fleshing out the pieces that we're missing", "tokens": [400, 370, 3902, 14574, 491, 460, 513, 64, 575, 1096, 364, 2243, 1691, 295, 283, 904, 571, 484, 264, 3755, 300, 321, 434, 5361], "temperature": 0.0, "avg_logprob": -0.13504720891563637, "compression_ratio": 1.6127659574468085, "no_speech_prob": 8.267799785244279e-06}, {"id": 17, "seek": 9938, "start": 99.38, "end": 106.0, "text": " and then confirming that he has actually achieved superconvergence on training on SciFi 10.", "tokens": [293, 550, 42861, 300, 415, 575, 767, 11042, 1687, 1671, 331, 15260, 322, 3097, 322, 16942, 13229, 1266, 13], "temperature": 0.0, "avg_logprob": -0.15926400410760308, "compression_ratio": 1.5757575757575757, "no_speech_prob": 1.9222808987251483e-05}, {"id": 18, "seek": 9938, "start": 106.0, "end": 110.02, "text": " So I think this is the first time that this has been done that I've heard of outside of", "tokens": [407, 286, 519, 341, 307, 264, 700, 565, 300, 341, 575, 668, 1096, 300, 286, 600, 2198, 295, 2380, 295], "temperature": 0.0, "avg_logprob": -0.15926400410760308, "compression_ratio": 1.5757575757575757, "no_speech_prob": 1.9222808987251483e-05}, {"id": 19, "seek": 9938, "start": 110.02, "end": 111.96, "text": " Lesley Smith himself.", "tokens": [6965, 3420, 8538, 3647, 13], "temperature": 0.0, "avg_logprob": -0.15926400410760308, "compression_ratio": 1.5757575757575757, "no_speech_prob": 1.9222808987251483e-05}, {"id": 20, "seek": 9938, "start": 111.96, "end": 116.67999999999999, "text": " So he's got a great blog post up now on one cycle, which is what Lesley Smith called this", "tokens": [407, 415, 311, 658, 257, 869, 6968, 2183, 493, 586, 322, 472, 6586, 11, 597, 307, 437, 6965, 3420, 8538, 1219, 341], "temperature": 0.0, "avg_logprob": -0.15926400410760308, "compression_ratio": 1.5757575757575757, "no_speech_prob": 1.9222808987251483e-05}, {"id": 21, "seek": 9938, "start": 116.67999999999999, "end": 119.19999999999999, "text": " approach.", "tokens": [3109, 13], "temperature": 0.0, "avg_logprob": -0.15926400410760308, "compression_ratio": 1.5757575757575757, "no_speech_prob": 1.9222808987251483e-05}, {"id": 22, "seek": 9938, "start": 119.19999999999999, "end": 123.56, "text": " And this is actually, it turns out, what one cycle looks like.", "tokens": [400, 341, 307, 767, 11, 309, 4523, 484, 11, 437, 472, 6586, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.15926400410760308, "compression_ratio": 1.5757575757575757, "no_speech_prob": 1.9222808987251483e-05}, {"id": 23, "seek": 12356, "start": 123.56, "end": 130.68, "text": " It's a single cyclical learning rate, but the key difference here is that the going-up", "tokens": [467, 311, 257, 2167, 19474, 804, 2539, 3314, 11, 457, 264, 2141, 2649, 510, 307, 300, 264, 516, 12, 1010], "temperature": 0.0, "avg_logprob": -0.18184958971463716, "compression_ratio": 1.6591928251121075, "no_speech_prob": 2.1907599148107693e-06}, {"id": 24, "seek": 12356, "start": 130.68, "end": 133.76, "text": " bit is the same length as the going-down bit.", "tokens": [857, 307, 264, 912, 4641, 382, 264, 516, 12, 5093, 857, 13], "temperature": 0.0, "avg_logprob": -0.18184958971463716, "compression_ratio": 1.6591928251121075, "no_speech_prob": 2.1907599148107693e-06}, {"id": 25, "seek": 12356, "start": 133.76, "end": 136.1, "text": " So you go up really slowly.", "tokens": [407, 291, 352, 493, 534, 5692, 13], "temperature": 0.0, "avg_logprob": -0.18184958971463716, "compression_ratio": 1.6591928251121075, "no_speech_prob": 2.1907599148107693e-06}, {"id": 26, "seek": 12356, "start": 136.1, "end": 140.28, "text": " And then at the end, for like a tenth of the time, you then have this little bit where", "tokens": [400, 550, 412, 264, 917, 11, 337, 411, 257, 27269, 295, 264, 565, 11, 291, 550, 362, 341, 707, 857, 689], "temperature": 0.0, "avg_logprob": -0.18184958971463716, "compression_ratio": 1.6591928251121075, "no_speech_prob": 2.1907599148107693e-06}, {"id": 27, "seek": 12356, "start": 140.28, "end": 143.8, "text": " you go down even further.", "tokens": [291, 352, 760, 754, 3052, 13], "temperature": 0.0, "avg_logprob": -0.18184958971463716, "compression_ratio": 1.6591928251121075, "no_speech_prob": 2.1907599148107693e-06}, {"id": 28, "seek": 12356, "start": 143.8, "end": 147.28, "text": " And it's interesting, obviously this is a very easy thing to show, a very easy thing", "tokens": [400, 309, 311, 1880, 11, 2745, 341, 307, 257, 588, 1858, 551, 281, 855, 11, 257, 588, 1858, 551], "temperature": 0.0, "avg_logprob": -0.18184958971463716, "compression_ratio": 1.6591928251121075, "no_speech_prob": 2.1907599148107693e-06}, {"id": 29, "seek": 12356, "start": 147.28, "end": 149.32, "text": " to explain.", "tokens": [281, 2903, 13], "temperature": 0.0, "avg_logprob": -0.18184958971463716, "compression_ratio": 1.6591928251121075, "no_speech_prob": 2.1907599148107693e-06}, {"id": 30, "seek": 14932, "start": 149.32, "end": 155.16, "text": " Sylvain has added it to FastAI, temporarily it's called use CLR beta.", "tokens": [3902, 14574, 491, 575, 3869, 309, 281, 15968, 48698, 11, 23750, 309, 311, 1219, 764, 12855, 49, 9861, 13], "temperature": 0.0, "avg_logprob": -0.17175750394838046, "compression_ratio": 1.6640316205533596, "no_speech_prob": 1.3845787179889157e-05}, {"id": 31, "seek": 14932, "start": 155.16, "end": 160.6, "text": " By the time you watch this on the video, it'll probably be called one cycle or something", "tokens": [3146, 264, 565, 291, 1159, 341, 322, 264, 960, 11, 309, 603, 1391, 312, 1219, 472, 6586, 420, 746], "temperature": 0.0, "avg_logprob": -0.17175750394838046, "compression_ratio": 1.6640316205533596, "no_speech_prob": 1.3845787179889157e-05}, {"id": 32, "seek": 14932, "start": 160.6, "end": 163.64, "text": " like that.", "tokens": [411, 300, 13], "temperature": 0.0, "avg_logprob": -0.17175750394838046, "compression_ratio": 1.6640316205533596, "no_speech_prob": 1.3845787179889157e-05}, {"id": 33, "seek": 14932, "start": 163.64, "end": 165.95999999999998, "text": " But you can use this right now.", "tokens": [583, 291, 393, 764, 341, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.17175750394838046, "compression_ratio": 1.6640316205533596, "no_speech_prob": 1.3845787179889157e-05}, {"id": 34, "seek": 14932, "start": 165.95999999999998, "end": 169.82, "text": " So that's one key piece to getting these massively high learning rates.", "tokens": [407, 300, 311, 472, 2141, 2522, 281, 1242, 613, 29379, 1090, 2539, 6846, 13], "temperature": 0.0, "avg_logprob": -0.17175750394838046, "compression_ratio": 1.6640316205533596, "no_speech_prob": 1.3845787179889157e-05}, {"id": 35, "seek": 14932, "start": 169.82, "end": 172.24, "text": " And he shows a number of experiments when you do that.", "tokens": [400, 415, 3110, 257, 1230, 295, 12050, 562, 291, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.17175750394838046, "compression_ratio": 1.6640316205533596, "no_speech_prob": 1.3845787179889157e-05}, {"id": 36, "seek": 14932, "start": 172.24, "end": 179.12, "text": " A second key piece is that as you do this to the learning rate, you do this to the momentum.", "tokens": [316, 1150, 2141, 2522, 307, 300, 382, 291, 360, 341, 281, 264, 2539, 3314, 11, 291, 360, 341, 281, 264, 11244, 13], "temperature": 0.0, "avg_logprob": -0.17175750394838046, "compression_ratio": 1.6640316205533596, "no_speech_prob": 1.3845787179889157e-05}, {"id": 37, "seek": 17912, "start": 179.12, "end": 182.52, "text": " So when the learning rate is low, it's fine to have a high momentum.", "tokens": [407, 562, 264, 2539, 3314, 307, 2295, 11, 309, 311, 2489, 281, 362, 257, 1090, 11244, 13], "temperature": 0.0, "avg_logprob": -0.1377314205827384, "compression_ratio": 1.7392996108949417, "no_speech_prob": 5.9550493460847065e-06}, {"id": 38, "seek": 17912, "start": 182.52, "end": 188.52, "text": " But when the learning rate gets up really high, your momentum needs to be quite a bit", "tokens": [583, 562, 264, 2539, 3314, 2170, 493, 534, 1090, 11, 428, 11244, 2203, 281, 312, 1596, 257, 857], "temperature": 0.0, "avg_logprob": -0.1377314205827384, "compression_ratio": 1.7392996108949417, "no_speech_prob": 5.9550493460847065e-06}, {"id": 39, "seek": 17912, "start": 188.52, "end": 191.0, "text": " lower.", "tokens": [3126, 13], "temperature": 0.0, "avg_logprob": -0.1377314205827384, "compression_ratio": 1.7392996108949417, "no_speech_prob": 5.9550493460847065e-06}, {"id": 40, "seek": 17912, "start": 191.0, "end": 196.08, "text": " So this is also part of what he's added to the library, is this cyclical momentum.", "tokens": [407, 341, 307, 611, 644, 295, 437, 415, 311, 3869, 281, 264, 6405, 11, 307, 341, 19474, 804, 11244, 13], "temperature": 0.0, "avg_logprob": -0.1377314205827384, "compression_ratio": 1.7392996108949417, "no_speech_prob": 5.9550493460847065e-06}, {"id": 41, "seek": 17912, "start": 196.08, "end": 201.68, "text": " And so with these two things, you can train for about the fifth of the number of epochs", "tokens": [400, 370, 365, 613, 732, 721, 11, 291, 393, 3847, 337, 466, 264, 9266, 295, 264, 1230, 295, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.1377314205827384, "compression_ratio": 1.7392996108949417, "no_speech_prob": 5.9550493460847065e-06}, {"id": 42, "seek": 17912, "start": 201.68, "end": 204.84, "text": " with a stepwise learning rate schedule.", "tokens": [365, 257, 1823, 3711, 2539, 3314, 7567, 13], "temperature": 0.0, "avg_logprob": -0.1377314205827384, "compression_ratio": 1.7392996108949417, "no_speech_prob": 5.9550493460847065e-06}, {"id": 43, "seek": 17912, "start": 204.84, "end": 208.76, "text": " Then you can drop your weight decay down by about two orders of magnitude.", "tokens": [1396, 291, 393, 3270, 428, 3364, 21039, 760, 538, 466, 732, 9470, 295, 15668, 13], "temperature": 0.0, "avg_logprob": -0.1377314205827384, "compression_ratio": 1.7392996108949417, "no_speech_prob": 5.9550493460847065e-06}, {"id": 44, "seek": 20876, "start": 208.76, "end": 214.2, "text": " You can often remove most or all of your dropout, and so you end up with something that's trained", "tokens": [509, 393, 2049, 4159, 881, 420, 439, 295, 428, 3270, 346, 11, 293, 370, 291, 917, 493, 365, 746, 300, 311, 8895], "temperature": 0.0, "avg_logprob": -0.17904699325561524, "compression_ratio": 1.5739130434782609, "no_speech_prob": 2.2827673092251644e-05}, {"id": 45, "seek": 20876, "start": 214.2, "end": 217.72, "text": " faster and generalizes better.", "tokens": [4663, 293, 2674, 5660, 1101, 13], "temperature": 0.0, "avg_logprob": -0.17904699325561524, "compression_ratio": 1.5739130434782609, "no_speech_prob": 2.2827673092251644e-05}, {"id": 46, "seek": 20876, "start": 217.72, "end": 221.84, "text": " It actually turns out that Sylvain got quite a bit better accuracy than Leslie Smith's", "tokens": [467, 767, 4523, 484, 300, 3902, 14574, 491, 658, 1596, 257, 857, 1101, 14170, 813, 28140, 8538, 311], "temperature": 0.0, "avg_logprob": -0.17904699325561524, "compression_ratio": 1.5739130434782609, "no_speech_prob": 2.2827673092251644e-05}, {"id": 47, "seek": 20876, "start": 221.84, "end": 222.84, "text": " paper.", "tokens": [3035, 13], "temperature": 0.0, "avg_logprob": -0.17904699325561524, "compression_ratio": 1.5739130434782609, "no_speech_prob": 2.2827673092251644e-05}, {"id": 48, "seek": 20876, "start": 222.84, "end": 228.07999999999998, "text": " His guess, I was pleased to see, is because our data augmentation defaults are better", "tokens": [2812, 2041, 11, 286, 390, 10587, 281, 536, 11, 307, 570, 527, 1412, 14501, 19631, 7576, 82, 366, 1101], "temperature": 0.0, "avg_logprob": -0.17904699325561524, "compression_ratio": 1.5739130434782609, "no_speech_prob": 2.2827673092251644e-05}, {"id": 49, "seek": 20876, "start": 228.07999999999998, "end": 229.07999999999998, "text": " than Leslie's.", "tokens": [813, 28140, 311, 13], "temperature": 0.0, "avg_logprob": -0.17904699325561524, "compression_ratio": 1.5739130434782609, "no_speech_prob": 2.2827673092251644e-05}, {"id": 50, "seek": 20876, "start": 229.07999999999998, "end": 231.32, "text": " I hope that's true.", "tokens": [286, 1454, 300, 311, 2074, 13], "temperature": 0.0, "avg_logprob": -0.17904699325561524, "compression_ratio": 1.5739130434782609, "no_speech_prob": 2.2827673092251644e-05}, {"id": 51, "seek": 20876, "start": 231.32, "end": 232.32, "text": " So check that out.", "tokens": [407, 1520, 300, 484, 13], "temperature": 0.0, "avg_logprob": -0.17904699325561524, "compression_ratio": 1.5739130434782609, "no_speech_prob": 2.2827673092251644e-05}, {"id": 52, "seek": 23232, "start": 232.32, "end": 239.6, "text": " Another cool thing, there's been so many cool things this week, I'm just going to pick two.", "tokens": [3996, 1627, 551, 11, 456, 311, 668, 370, 867, 1627, 721, 341, 1243, 11, 286, 478, 445, 516, 281, 1888, 732, 13], "temperature": 0.0, "avg_logprob": -0.2447177532107331, "compression_ratio": 1.490990990990991, "no_speech_prob": 3.2191801437875256e-05}, {"id": 53, "seek": 23232, "start": 239.6, "end": 245.32, "text": " Hamil Hussain, who works at GitHub, I just really like this.", "tokens": [8234, 388, 389, 2023, 491, 11, 567, 1985, 412, 23331, 11, 286, 445, 534, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.2447177532107331, "compression_ratio": 1.490990990990991, "no_speech_prob": 3.2191801437875256e-05}, {"id": 54, "seek": 23232, "start": 245.32, "end": 252.0, "text": " There's a fairly new project called Qplo, which is basically a TensorFlow for Kubernetes.", "tokens": [821, 311, 257, 6457, 777, 1716, 1219, 1249, 21132, 11, 597, 307, 1936, 257, 37624, 337, 23145, 13], "temperature": 0.0, "avg_logprob": -0.2447177532107331, "compression_ratio": 1.490990990990991, "no_speech_prob": 3.2191801437875256e-05}, {"id": 55, "seek": 23232, "start": 252.0, "end": 259.8, "text": " Hamil wrote a very nice article about magical sequence-to-sequence models, building data", "tokens": [8234, 388, 4114, 257, 588, 1481, 7222, 466, 12066, 8310, 12, 1353, 12, 11834, 655, 5245, 11, 2390, 1412], "temperature": 0.0, "avg_logprob": -0.2447177532107331, "compression_ratio": 1.490990990990991, "no_speech_prob": 3.2191801437875256e-05}, {"id": 56, "seek": 25980, "start": 259.8, "end": 269.36, "text": " products on that, using Kubernetes to put that in production and so forth.", "tokens": [3383, 322, 300, 11, 1228, 23145, 281, 829, 300, 294, 4265, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1676317606216822, "compression_ratio": 1.4951456310679612, "no_speech_prob": 1.4285006727732252e-05}, {"id": 57, "seek": 25980, "start": 269.36, "end": 274.6, "text": " He said that the Google Qflow team created a demo based on what he wrote earlier this", "tokens": [634, 848, 300, 264, 3329, 1249, 10565, 1469, 2942, 257, 10723, 2361, 322, 437, 415, 4114, 3071, 341], "temperature": 0.0, "avg_logprob": -0.1676317606216822, "compression_ratio": 1.4951456310679612, "no_speech_prob": 1.4285006727732252e-05}, {"id": 58, "seek": 25980, "start": 274.6, "end": 279.76, "text": " year, directly based on the skills learned in class AI, and I will be presenting this", "tokens": [1064, 11, 3838, 2361, 322, 264, 3942, 3264, 294, 1508, 7318, 11, 293, 286, 486, 312, 15578, 341], "temperature": 0.0, "avg_logprob": -0.1676317606216822, "compression_ratio": 1.4951456310679612, "no_speech_prob": 1.4285006727732252e-05}, {"id": 59, "seek": 25980, "start": 279.76, "end": 280.76, "text": " technique at KDD.", "tokens": [6532, 412, 591, 20818, 13], "temperature": 0.0, "avg_logprob": -0.1676317606216822, "compression_ratio": 1.4951456310679612, "no_speech_prob": 1.4285006727732252e-05}, {"id": 60, "seek": 25980, "start": 280.76, "end": 285.72, "text": " KDD is one of the top academic conferences.", "tokens": [591, 20818, 307, 472, 295, 264, 1192, 7778, 22032, 13], "temperature": 0.0, "avg_logprob": -0.1676317606216822, "compression_ratio": 1.4951456310679612, "no_speech_prob": 1.4285006727732252e-05}, {"id": 61, "seek": 28572, "start": 285.72, "end": 290.6, "text": " So I wanted to share this as a motivation for folks to blog, which I think is a great", "tokens": [407, 286, 1415, 281, 2073, 341, 382, 257, 12335, 337, 4024, 281, 6968, 11, 597, 286, 519, 307, 257, 869], "temperature": 0.0, "avg_logprob": -0.19566984630766368, "compression_ratio": 1.7768240343347639, "no_speech_prob": 3.7634457839885727e-05}, {"id": 62, "seek": 28572, "start": 290.6, "end": 291.6, "text": " point.", "tokens": [935, 13], "temperature": 0.0, "avg_logprob": -0.19566984630766368, "compression_ratio": 1.7768240343347639, "no_speech_prob": 3.7634457839885727e-05}, {"id": 63, "seek": 28572, "start": 291.6, "end": 297.64000000000004, "text": " Nobody who goes out and writes a blog thinks that probably none of us really think our", "tokens": [9297, 567, 1709, 484, 293, 13657, 257, 6968, 7309, 300, 1391, 6022, 295, 505, 534, 519, 527], "temperature": 0.0, "avg_logprob": -0.19566984630766368, "compression_ratio": 1.7768240343347639, "no_speech_prob": 3.7634457839885727e-05}, {"id": 64, "seek": 28572, "start": 297.64000000000004, "end": 302.0, "text": " blog is actually going to be very good, probably nobody is going to read it, and then when", "tokens": [6968, 307, 767, 516, 281, 312, 588, 665, 11, 1391, 5079, 307, 516, 281, 1401, 309, 11, 293, 550, 562], "temperature": 0.0, "avg_logprob": -0.19566984630766368, "compression_ratio": 1.7768240343347639, "no_speech_prob": 3.7634457839885727e-05}, {"id": 65, "seek": 28572, "start": 302.0, "end": 305.68, "text": " people actually do like it and read it, it's like with great surprise, you just go, oh,", "tokens": [561, 767, 360, 411, 309, 293, 1401, 309, 11, 309, 311, 411, 365, 869, 6365, 11, 291, 445, 352, 11, 1954, 11], "temperature": 0.0, "avg_logprob": -0.19566984630766368, "compression_ratio": 1.7768240343347639, "no_speech_prob": 3.7634457839885727e-05}, {"id": 66, "seek": 28572, "start": 305.68, "end": 309.88000000000005, "text": " it's actually something people were interested to read.", "tokens": [309, 311, 767, 746, 561, 645, 3102, 281, 1401, 13], "temperature": 0.0, "avg_logprob": -0.19566984630766368, "compression_ratio": 1.7768240343347639, "no_speech_prob": 3.7634457839885727e-05}, {"id": 67, "seek": 30988, "start": 309.88, "end": 316.0, "text": " So here is the tool where you can summarize GitHub issues using this tool, which is now", "tokens": [407, 510, 307, 264, 2290, 689, 291, 393, 20858, 23331, 2663, 1228, 341, 2290, 11, 597, 307, 586], "temperature": 0.0, "avg_logprob": -0.1887314206077939, "compression_ratio": 1.497560975609756, "no_speech_prob": 1.4738685422344133e-05}, {"id": 68, "seek": 30988, "start": 316.0, "end": 319.6, "text": " hosted by Google on the Qflow.org domain.", "tokens": [19204, 538, 3329, 322, 264, 1249, 10565, 13, 4646, 9274, 13], "temperature": 0.0, "avg_logprob": -0.1887314206077939, "compression_ratio": 1.497560975609756, "no_speech_prob": 1.4738685422344133e-05}, {"id": 69, "seek": 30988, "start": 319.6, "end": 326.28, "text": " So I think that's a great story of getting, if Hamil didn't put his work out there, none", "tokens": [407, 286, 519, 300, 311, 257, 869, 1657, 295, 1242, 11, 498, 8234, 388, 994, 380, 829, 702, 589, 484, 456, 11, 6022], "temperature": 0.0, "avg_logprob": -0.1887314206077939, "compression_ratio": 1.497560975609756, "no_speech_prob": 1.4738685422344133e-05}, {"id": 70, "seek": 30988, "start": 326.28, "end": 328.4, "text": " of this would have happened.", "tokens": [295, 341, 576, 362, 2011, 13], "temperature": 0.0, "avg_logprob": -0.1887314206077939, "compression_ratio": 1.497560975609756, "no_speech_prob": 1.4738685422344133e-05}, {"id": 71, "seek": 30988, "start": 328.4, "end": 336.48, "text": " You can check out his post that made it all happen as well.", "tokens": [509, 393, 1520, 484, 702, 2183, 300, 1027, 309, 439, 1051, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1887314206077939, "compression_ratio": 1.497560975609756, "no_speech_prob": 1.4738685422344133e-05}, {"id": 72, "seek": 33648, "start": 336.48, "end": 344.64000000000004, "text": " So talking of the magic of sequence-to-sequence models, let's build one.", "tokens": [407, 1417, 295, 264, 5585, 295, 8310, 12, 1353, 12, 11834, 655, 5245, 11, 718, 311, 1322, 472, 13], "temperature": 0.0, "avg_logprob": -0.1428767216356495, "compression_ratio": 1.7872340425531914, "no_speech_prob": 9.368532118969597e-06}, {"id": 73, "seek": 33648, "start": 344.64000000000004, "end": 352.0, "text": " So we're going to be specifically working on machine translation.", "tokens": [407, 321, 434, 516, 281, 312, 4682, 1364, 322, 3479, 12853, 13], "temperature": 0.0, "avg_logprob": -0.1428767216356495, "compression_ratio": 1.7872340425531914, "no_speech_prob": 9.368532118969597e-06}, {"id": 74, "seek": 33648, "start": 352.0, "end": 358.06, "text": " So machine translation is something that's been around for a long time, but specifically", "tokens": [407, 3479, 12853, 307, 746, 300, 311, 668, 926, 337, 257, 938, 565, 11, 457, 4682], "temperature": 0.0, "avg_logprob": -0.1428767216356495, "compression_ratio": 1.7872340425531914, "no_speech_prob": 9.368532118969597e-06}, {"id": 75, "seek": 33648, "start": 358.06, "end": 362.04, "text": " we're going to look at a protocol called neural translation, which is using neural networks", "tokens": [321, 434, 516, 281, 574, 412, 257, 10336, 1219, 18161, 12853, 11, 597, 307, 1228, 18161, 9590], "temperature": 0.0, "avg_logprob": -0.1428767216356495, "compression_ratio": 1.7872340425531914, "no_speech_prob": 9.368532118969597e-06}, {"id": 76, "seek": 33648, "start": 362.04, "end": 364.88, "text": " for translation.", "tokens": [337, 12853, 13], "temperature": 0.0, "avg_logprob": -0.1428767216356495, "compression_ratio": 1.7872340425531914, "no_speech_prob": 9.368532118969597e-06}, {"id": 77, "seek": 36488, "start": 364.88, "end": 371.15999999999997, "text": " And that wasn't really a thing in any kind of meaningful way until a couple of years", "tokens": [400, 300, 2067, 380, 534, 257, 551, 294, 604, 733, 295, 10995, 636, 1826, 257, 1916, 295, 924], "temperature": 0.0, "avg_logprob": -0.1939510981241862, "compression_ratio": 1.5740740740740742, "no_speech_prob": 6.962177849345608e-06}, {"id": 78, "seek": 36488, "start": 371.15999999999997, "end": 373.36, "text": " ago.", "tokens": [2057, 13], "temperature": 0.0, "avg_logprob": -0.1939510981241862, "compression_ratio": 1.5740740740740742, "no_speech_prob": 6.962177849345608e-06}, {"id": 79, "seek": 36488, "start": 373.36, "end": 379.12, "text": " And so thanks to Chris Manning from Stanford for the next three slides.", "tokens": [400, 370, 3231, 281, 6688, 2458, 773, 490, 20374, 337, 264, 958, 1045, 9788, 13], "temperature": 0.0, "avg_logprob": -0.1939510981241862, "compression_ratio": 1.5740740740740742, "no_speech_prob": 6.962177849345608e-06}, {"id": 80, "seek": 36488, "start": 379.12, "end": 385.04, "text": " 2015, Chris pointed out that neural machine translation first appeared properly, and it", "tokens": [7546, 11, 6688, 10932, 484, 300, 18161, 3479, 12853, 700, 8516, 6108, 11, 293, 309], "temperature": 0.0, "avg_logprob": -0.1939510981241862, "compression_ratio": 1.5740740740740742, "no_speech_prob": 6.962177849345608e-06}, {"id": 81, "seek": 36488, "start": 385.04, "end": 388.88, "text": " was pretty crappy compared to the statistical machine translation approaches that use kind", "tokens": [390, 1238, 36531, 5347, 281, 264, 22820, 3479, 12853, 11587, 300, 764, 733], "temperature": 0.0, "avg_logprob": -0.1939510981241862, "compression_ratio": 1.5740740740740742, "no_speech_prob": 6.962177849345608e-06}, {"id": 82, "seek": 38888, "start": 388.88, "end": 396.36, "text": " of classic feature engineering and standard MLP approaches of lots of stemming and fiddling", "tokens": [295, 7230, 4111, 7043, 293, 3832, 21601, 47, 11587, 295, 3195, 295, 12312, 2810, 293, 283, 14273, 1688], "temperature": 0.0, "avg_logprob": -0.19460933549063547, "compression_ratio": 1.6293436293436294, "no_speech_prob": 1.3845768989995122e-05}, {"id": 83, "seek": 38888, "start": 396.36, "end": 402.04, "text": " around with word frequencies and n-grams and lots of stuff.", "tokens": [926, 365, 1349, 20250, 293, 297, 12, 1342, 82, 293, 3195, 295, 1507, 13], "temperature": 0.0, "avg_logprob": -0.19460933549063547, "compression_ratio": 1.6293436293436294, "no_speech_prob": 1.3845768989995122e-05}, {"id": 84, "seek": 38888, "start": 402.04, "end": 406.08, "text": " By a year later, it was better than everything else.", "tokens": [3146, 257, 1064, 1780, 11, 309, 390, 1101, 813, 1203, 1646, 13], "temperature": 0.0, "avg_logprob": -0.19460933549063547, "compression_ratio": 1.6293436293436294, "no_speech_prob": 1.3845768989995122e-05}, {"id": 85, "seek": 38888, "start": 406.08, "end": 407.52, "text": " This is on a metric called BLUE.", "tokens": [639, 307, 322, 257, 20678, 1219, 15132, 16309, 13], "temperature": 0.0, "avg_logprob": -0.19460933549063547, "compression_ratio": 1.6293436293436294, "no_speech_prob": 1.3845768989995122e-05}, {"id": 86, "seek": 38888, "start": 407.52, "end": 410.6, "text": " We're not going to discuss the metric because it's not a very good metric and it's not very", "tokens": [492, 434, 406, 516, 281, 2248, 264, 20678, 570, 309, 311, 406, 257, 588, 665, 20678, 293, 309, 311, 406, 588], "temperature": 0.0, "avg_logprob": -0.19460933549063547, "compression_ratio": 1.6293436293436294, "no_speech_prob": 1.3845768989995122e-05}, {"id": 87, "seek": 38888, "start": 410.6, "end": 414.24, "text": " interesting, but it's what everybody uses.", "tokens": [1880, 11, 457, 309, 311, 437, 2201, 4960, 13], "temperature": 0.0, "avg_logprob": -0.19460933549063547, "compression_ratio": 1.6293436293436294, "no_speech_prob": 1.3845768989995122e-05}, {"id": 88, "seek": 38888, "start": 414.24, "end": 417.36, "text": " So that was BLUE as of when Chris did this slide.", "tokens": [407, 300, 390, 15132, 16309, 382, 295, 562, 6688, 630, 341, 4137, 13], "temperature": 0.0, "avg_logprob": -0.19460933549063547, "compression_ratio": 1.6293436293436294, "no_speech_prob": 1.3845768989995122e-05}, {"id": 89, "seek": 41736, "start": 417.36, "end": 422.84000000000003, "text": " As of now, it's about up here, it's about 30.", "tokens": [1018, 295, 586, 11, 309, 311, 466, 493, 510, 11, 309, 311, 466, 2217, 13], "temperature": 0.0, "avg_logprob": -0.14634822845458983, "compression_ratio": 1.486910994764398, "no_speech_prob": 1.6964118913165294e-05}, {"id": 90, "seek": 41736, "start": 422.84000000000003, "end": 430.52000000000004, "text": " So we're kind of seeing machine translation starting down the path that we saw starting", "tokens": [407, 321, 434, 733, 295, 2577, 3479, 12853, 2891, 760, 264, 3100, 300, 321, 1866, 2891], "temperature": 0.0, "avg_logprob": -0.14634822845458983, "compression_ratio": 1.486910994764398, "no_speech_prob": 1.6964118913165294e-05}, {"id": 91, "seek": 41736, "start": 430.52000000000004, "end": 437.98, "text": " computer vision object classification in 2012, I guess, which is we just surpassed the state", "tokens": [3820, 5201, 2657, 21538, 294, 9125, 11, 286, 2041, 11, 597, 307, 321, 445, 27650, 292, 264, 1785], "temperature": 0.0, "avg_logprob": -0.14634822845458983, "compression_ratio": 1.486910994764398, "no_speech_prob": 1.6964118913165294e-05}, {"id": 92, "seek": 41736, "start": 437.98, "end": 444.2, "text": " of the art and now we're zipping past it at a great rate.", "tokens": [295, 264, 1523, 293, 586, 321, 434, 710, 6297, 1791, 309, 412, 257, 869, 3314, 13], "temperature": 0.0, "avg_logprob": -0.14634822845458983, "compression_ratio": 1.486910994764398, "no_speech_prob": 1.6964118913165294e-05}, {"id": 93, "seek": 44420, "start": 444.2, "end": 450.47999999999996, "text": " It's very unlikely that anybody watching this is actually going to build a machine translation", "tokens": [467, 311, 588, 17518, 300, 4472, 1976, 341, 307, 767, 516, 281, 1322, 257, 3479, 12853], "temperature": 0.0, "avg_logprob": -0.11513645090955368, "compression_ratio": 1.7622950819672132, "no_speech_prob": 1.2411312127369456e-05}, {"id": 94, "seek": 44420, "start": 450.47999999999996, "end": 457.32, "text": " model, because you can go to translate.google.com and use theirs and it works quite well.", "tokens": [2316, 11, 570, 291, 393, 352, 281, 13799, 13, 1571, 3127, 13, 1112, 293, 764, 22760, 293, 309, 1985, 1596, 731, 13], "temperature": 0.0, "avg_logprob": -0.11513645090955368, "compression_ratio": 1.7622950819672132, "no_speech_prob": 1.2411312127369456e-05}, {"id": 95, "seek": 44420, "start": 457.32, "end": 460.2, "text": " So why are we learning about machine translation?", "tokens": [407, 983, 366, 321, 2539, 466, 3479, 12853, 30], "temperature": 0.0, "avg_logprob": -0.11513645090955368, "compression_ratio": 1.7622950819672132, "no_speech_prob": 1.2411312127369456e-05}, {"id": 96, "seek": 44420, "start": 460.2, "end": 464.52, "text": " The reason we're learning about machine translation is that the general idea of taking some kind", "tokens": [440, 1778, 321, 434, 2539, 466, 3479, 12853, 307, 300, 264, 2674, 1558, 295, 1940, 512, 733], "temperature": 0.0, "avg_logprob": -0.11513645090955368, "compression_ratio": 1.7622950819672132, "no_speech_prob": 1.2411312127369456e-05}, {"id": 97, "seek": 44420, "start": 464.52, "end": 472.24, "text": " of input like a sentence in French and transforming it into some other kind of output of arbitrary", "tokens": [295, 4846, 411, 257, 8174, 294, 5522, 293, 27210, 309, 666, 512, 661, 733, 295, 5598, 295, 23211], "temperature": 0.0, "avg_logprob": -0.11513645090955368, "compression_ratio": 1.7622950819672132, "no_speech_prob": 1.2411312127369456e-05}, {"id": 98, "seek": 47224, "start": 472.24, "end": 479.12, "text": " length such as a sentence in English is a really useful thing to do.", "tokens": [4641, 1270, 382, 257, 8174, 294, 3669, 307, 257, 534, 4420, 551, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.202513225743028, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.5936198906274512e-05}, {"id": 99, "seek": 47224, "start": 479.12, "end": 485.6, "text": " For example, the thing that we just saw that Hamil at GitHub did takes GitHub issues and", "tokens": [1171, 1365, 11, 264, 551, 300, 321, 445, 1866, 300, 8234, 388, 412, 23331, 630, 2516, 23331, 2663, 293], "temperature": 0.0, "avg_logprob": -0.202513225743028, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.5936198906274512e-05}, {"id": 100, "seek": 47224, "start": 485.6, "end": 488.28000000000003, "text": " turns them into summaries.", "tokens": [4523, 552, 666, 8367, 4889, 13], "temperature": 0.0, "avg_logprob": -0.202513225743028, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.5936198906274512e-05}, {"id": 101, "seek": 47224, "start": 488.28000000000003, "end": 494.72, "text": " Other examples is taking videos and turning them into descriptions.", "tokens": [5358, 5110, 307, 1940, 2145, 293, 6246, 552, 666, 24406, 13], "temperature": 0.0, "avg_logprob": -0.202513225743028, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.5936198906274512e-05}, {"id": 102, "seek": 49472, "start": 494.72, "end": 504.16, "text": " Or taking basically anything where you're spitting out an arbitrary-sized output, very", "tokens": [1610, 1940, 1936, 1340, 689, 291, 434, 637, 2414, 484, 364, 23211, 12, 20614, 5598, 11, 588], "temperature": 0.0, "avg_logprob": -0.20837596484592982, "compression_ratio": 1.48, "no_speech_prob": 1.1842773346870672e-05}, {"id": 103, "seek": 49472, "start": 504.16, "end": 510.72, "text": " often that's a sentence, so maybe taking a CT scan and spitting out a radiology report,", "tokens": [2049, 300, 311, 257, 8174, 11, 370, 1310, 1940, 257, 19529, 11049, 293, 637, 2414, 484, 257, 16335, 1793, 2275, 11], "temperature": 0.0, "avg_logprob": -0.20837596484592982, "compression_ratio": 1.48, "no_speech_prob": 1.1842773346870672e-05}, {"id": 104, "seek": 49472, "start": 510.72, "end": 517.2, "text": " this is where you can use sequence to sequence.", "tokens": [341, 307, 689, 291, 393, 764, 8310, 281, 8310, 13], "temperature": 0.0, "avg_logprob": -0.20837596484592982, "compression_ratio": 1.48, "no_speech_prob": 1.1842773346870672e-05}, {"id": 105, "seek": 51720, "start": 517.2, "end": 525.2, "text": " So the important thing about neural machine translation, these are more slides from Chris,", "tokens": [407, 264, 1021, 551, 466, 18161, 3479, 12853, 11, 613, 366, 544, 9788, 490, 6688, 11], "temperature": 0.0, "avg_logprob": -0.1662695634932745, "compression_ratio": 1.5617021276595744, "no_speech_prob": 5.896391144233348e-07}, {"id": 106, "seek": 51720, "start": 525.2, "end": 533.5200000000001, "text": " and generally sequence to sequence models, is that there's no fussing around with heuristics", "tokens": [293, 5101, 8310, 281, 8310, 5245, 11, 307, 300, 456, 311, 572, 34792, 278, 926, 365, 415, 374, 6006], "temperature": 0.0, "avg_logprob": -0.1662695634932745, "compression_ratio": 1.5617021276595744, "no_speech_prob": 5.896391144233348e-07}, {"id": 107, "seek": 51720, "start": 533.5200000000001, "end": 538.72, "text": " and hacky feature engineering, whatever it is, end-to-end training.", "tokens": [293, 10339, 88, 4111, 7043, 11, 2035, 309, 307, 11, 917, 12, 1353, 12, 521, 3097, 13], "temperature": 0.0, "avg_logprob": -0.1662695634932745, "compression_ratio": 1.5617021276595744, "no_speech_prob": 5.896391144233348e-07}, {"id": 108, "seek": 51720, "start": 538.72, "end": 543.9200000000001, "text": " We're able to build these distributed representations which are shared by lots of concepts within", "tokens": [492, 434, 1075, 281, 1322, 613, 12631, 33358, 597, 366, 5507, 538, 3195, 295, 10392, 1951], "temperature": 0.0, "avg_logprob": -0.1662695634932745, "compression_ratio": 1.5617021276595744, "no_speech_prob": 5.896391144233348e-07}, {"id": 109, "seek": 51720, "start": 543.9200000000001, "end": 545.84, "text": " a single network.", "tokens": [257, 2167, 3209, 13], "temperature": 0.0, "avg_logprob": -0.1662695634932745, "compression_ratio": 1.5617021276595744, "no_speech_prob": 5.896391144233348e-07}, {"id": 110, "seek": 54584, "start": 545.84, "end": 551.6800000000001, "text": " We're able to use long-term state in the RNN, so use a lot more context than N-gram type", "tokens": [492, 434, 1075, 281, 764, 938, 12, 7039, 1785, 294, 264, 45702, 45, 11, 370, 764, 257, 688, 544, 4319, 813, 426, 12, 1342, 2010], "temperature": 0.0, "avg_logprob": -0.22180358825191374, "compression_ratio": 1.7490196078431373, "no_speech_prob": 4.860393346461933e-06}, {"id": 111, "seek": 54584, "start": 551.6800000000001, "end": 552.6800000000001, "text": " approaches.", "tokens": [11587, 13], "temperature": 0.0, "avg_logprob": -0.22180358825191374, "compression_ratio": 1.7490196078431373, "no_speech_prob": 4.860393346461933e-06}, {"id": 112, "seek": 54584, "start": 552.6800000000001, "end": 558.24, "text": " In the end, the text we're generating uses an RNN as well so we can build something that's", "tokens": [682, 264, 917, 11, 264, 2487, 321, 434, 17746, 4960, 364, 45702, 45, 382, 731, 370, 321, 393, 1322, 746, 300, 311], "temperature": 0.0, "avg_logprob": -0.22180358825191374, "compression_ratio": 1.7490196078431373, "no_speech_prob": 4.860393346461933e-06}, {"id": 113, "seek": 54584, "start": 558.24, "end": 559.24, "text": " more fluid.", "tokens": [544, 9113, 13], "temperature": 0.0, "avg_logprob": -0.22180358825191374, "compression_ratio": 1.7490196078431373, "no_speech_prob": 4.860393346461933e-06}, {"id": 114, "seek": 54584, "start": 559.24, "end": 564.24, "text": " We're going to use a bidirectional LSTM with attention.", "tokens": [492, 434, 516, 281, 764, 257, 12957, 621, 41048, 441, 6840, 44, 365, 3202, 13], "temperature": 0.0, "avg_logprob": -0.22180358825191374, "compression_ratio": 1.7490196078431373, "no_speech_prob": 4.860393346461933e-06}, {"id": 115, "seek": 54584, "start": 564.24, "end": 569.2, "text": " Actually, we're going to use a bidirectional GRU with attention, but basically the same", "tokens": [5135, 11, 321, 434, 516, 281, 764, 257, 12957, 621, 41048, 10903, 52, 365, 3202, 11, 457, 1936, 264, 912], "temperature": 0.0, "avg_logprob": -0.22180358825191374, "compression_ratio": 1.7490196078431373, "no_speech_prob": 4.860393346461933e-06}, {"id": 116, "seek": 54584, "start": 569.2, "end": 570.2, "text": " thing.", "tokens": [551, 13], "temperature": 0.0, "avg_logprob": -0.22180358825191374, "compression_ratio": 1.7490196078431373, "no_speech_prob": 4.860393346461933e-06}, {"id": 117, "seek": 54584, "start": 570.2, "end": 574.6, "text": " So you already know about bidirectional recurrent neural networks and attention we're going", "tokens": [407, 291, 1217, 458, 466, 12957, 621, 41048, 18680, 1753, 18161, 9590, 293, 3202, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.22180358825191374, "compression_ratio": 1.7490196078431373, "no_speech_prob": 4.860393346461933e-06}, {"id": 118, "seek": 57460, "start": 574.6, "end": 576.28, "text": " to add on top today.", "tokens": [281, 909, 322, 1192, 965, 13], "temperature": 0.0, "avg_logprob": -0.14587897585149398, "compression_ratio": 1.3673469387755102, "no_speech_prob": 3.844917046080809e-06}, {"id": 119, "seek": 57460, "start": 576.28, "end": 581.48, "text": " These general ideas you can use for lots of other things as well as Chris points out on", "tokens": [1981, 2674, 3487, 291, 393, 764, 337, 3195, 295, 661, 721, 382, 731, 382, 6688, 2793, 484, 322], "temperature": 0.0, "avg_logprob": -0.14587897585149398, "compression_ratio": 1.3673469387755102, "no_speech_prob": 3.844917046080809e-06}, {"id": 120, "seek": 57460, "start": 581.48, "end": 585.6, "text": " this slide.", "tokens": [341, 4137, 13], "temperature": 0.0, "avg_logprob": -0.14587897585149398, "compression_ratio": 1.3673469387755102, "no_speech_prob": 3.844917046080809e-06}, {"id": 121, "seek": 57460, "start": 585.6, "end": 604.0, "text": " So let's jump into the code, which is in the translate notebook, funnily enough.", "tokens": [407, 718, 311, 3012, 666, 264, 3089, 11, 597, 307, 294, 264, 13799, 21060, 11, 1019, 77, 953, 1547, 13], "temperature": 0.0, "avg_logprob": -0.14587897585149398, "compression_ratio": 1.3673469387755102, "no_speech_prob": 3.844917046080809e-06}, {"id": 122, "seek": 60400, "start": 604.0, "end": 609.88, "text": " And so we're going to try to translate French into English.", "tokens": [400, 370, 321, 434, 516, 281, 853, 281, 13799, 5522, 666, 3669, 13], "temperature": 0.0, "avg_logprob": -0.13002482547035701, "compression_ratio": 1.6789473684210525, "no_speech_prob": 3.089476422246662e-06}, {"id": 123, "seek": 60400, "start": 609.88, "end": 618.16, "text": " And so the basic idea is that we're going to try and make this look as much like a standard", "tokens": [400, 370, 264, 3875, 1558, 307, 300, 321, 434, 516, 281, 853, 293, 652, 341, 574, 382, 709, 411, 257, 3832], "temperature": 0.0, "avg_logprob": -0.13002482547035701, "compression_ratio": 1.6789473684210525, "no_speech_prob": 3.089476422246662e-06}, {"id": 124, "seek": 60400, "start": 618.16, "end": 621.14, "text": " neural network approach as possible.", "tokens": [18161, 3209, 3109, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.13002482547035701, "compression_ratio": 1.6789473684210525, "no_speech_prob": 3.089476422246662e-06}, {"id": 125, "seek": 60400, "start": 621.14, "end": 629.08, "text": " So we're going to need three things, you'll remember the three things, data, a suitable", "tokens": [407, 321, 434, 516, 281, 643, 1045, 721, 11, 291, 603, 1604, 264, 1045, 721, 11, 1412, 11, 257, 12873], "temperature": 0.0, "avg_logprob": -0.13002482547035701, "compression_ratio": 1.6789473684210525, "no_speech_prob": 3.089476422246662e-06}, {"id": 126, "seek": 60400, "start": 629.08, "end": 633.44, "text": " architecture and a suitable loss function.", "tokens": [9482, 293, 257, 12873, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.13002482547035701, "compression_ratio": 1.6789473684210525, "no_speech_prob": 3.089476422246662e-06}, {"id": 127, "seek": 63344, "start": 633.44, "end": 635.8000000000001, "text": " Once you've got those three things, you run fit.", "tokens": [3443, 291, 600, 658, 729, 1045, 721, 11, 291, 1190, 3318, 13], "temperature": 0.0, "avg_logprob": -0.16155377301302823, "compression_ratio": 1.619718309859155, "no_speech_prob": 1.0952981938316952e-05}, {"id": 128, "seek": 63344, "start": 635.8000000000001, "end": 641.96, "text": " And all things going well, you end up with something that solves your problem.", "tokens": [400, 439, 721, 516, 731, 11, 291, 917, 493, 365, 746, 300, 39890, 428, 1154, 13], "temperature": 0.0, "avg_logprob": -0.16155377301302823, "compression_ratio": 1.619718309859155, "no_speech_prob": 1.0952981938316952e-05}, {"id": 129, "seek": 63344, "start": 641.96, "end": 648.36, "text": " So data, we generally need x, y pairs.", "tokens": [407, 1412, 11, 321, 5101, 643, 2031, 11, 288, 15494, 13], "temperature": 0.0, "avg_logprob": -0.16155377301302823, "compression_ratio": 1.619718309859155, "no_speech_prob": 1.0952981938316952e-05}, {"id": 130, "seek": 63344, "start": 648.36, "end": 653.2, "text": " Because we need something which we can feed it into the loss function and say I took my", "tokens": [1436, 321, 643, 746, 597, 321, 393, 3154, 309, 666, 264, 4470, 2445, 293, 584, 286, 1890, 452], "temperature": 0.0, "avg_logprob": -0.16155377301302823, "compression_ratio": 1.619718309859155, "no_speech_prob": 1.0952981938316952e-05}, {"id": 131, "seek": 63344, "start": 653.2, "end": 661.0, "text": " x value, which was my French sentence, and the loss function says it was meant to generate", "tokens": [2031, 2158, 11, 597, 390, 452, 5522, 8174, 11, 293, 264, 4470, 2445, 1619, 309, 390, 4140, 281, 8460], "temperature": 0.0, "avg_logprob": -0.16155377301302823, "compression_ratio": 1.619718309859155, "no_speech_prob": 1.0952981938316952e-05}, {"id": 132, "seek": 66100, "start": 661.0, "end": 666.92, "text": " this English sentence, and then you had your predictions, which you would then compare", "tokens": [341, 3669, 8174, 11, 293, 550, 291, 632, 428, 21264, 11, 597, 291, 576, 550, 6794], "temperature": 0.0, "avg_logprob": -0.14392408510533775, "compression_ratio": 1.6201923076923077, "no_speech_prob": 1.1478617125249002e-05}, {"id": 133, "seek": 66100, "start": 666.92, "end": 668.76, "text": " and see how good it is.", "tokens": [293, 536, 577, 665, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.14392408510533775, "compression_ratio": 1.6201923076923077, "no_speech_prob": 1.1478617125249002e-05}, {"id": 134, "seek": 66100, "start": 668.76, "end": 674.86, "text": " So therefore we need lots of these tuples of French sentences with their equivalent", "tokens": [407, 4412, 321, 643, 3195, 295, 613, 2604, 2622, 295, 5522, 16579, 365, 641, 10344], "temperature": 0.0, "avg_logprob": -0.14392408510533775, "compression_ratio": 1.6201923076923077, "no_speech_prob": 1.1478617125249002e-05}, {"id": 135, "seek": 66100, "start": 674.86, "end": 676.6, "text": " English sentence.", "tokens": [3669, 8174, 13], "temperature": 0.0, "avg_logprob": -0.14392408510533775, "compression_ratio": 1.6201923076923077, "no_speech_prob": 1.1478617125249002e-05}, {"id": 136, "seek": 66100, "start": 676.6, "end": 678.88, "text": " That's called a parallel corpus.", "tokens": [663, 311, 1219, 257, 8952, 1181, 31624, 13], "temperature": 0.0, "avg_logprob": -0.14392408510533775, "compression_ratio": 1.6201923076923077, "no_speech_prob": 1.1478617125249002e-05}, {"id": 137, "seek": 66100, "start": 678.88, "end": 683.48, "text": " Obviously this is harder to find than a corpus for a language model, because for a language", "tokens": [7580, 341, 307, 6081, 281, 915, 813, 257, 1181, 31624, 337, 257, 2856, 2316, 11, 570, 337, 257, 2856], "temperature": 0.0, "avg_logprob": -0.14392408510533775, "compression_ratio": 1.6201923076923077, "no_speech_prob": 1.1478617125249002e-05}, {"id": 138, "seek": 68348, "start": 683.48, "end": 692.44, "text": " model we just need text in some language, which you can basically, for any living language", "tokens": [2316, 321, 445, 643, 2487, 294, 512, 2856, 11, 597, 291, 393, 1936, 11, 337, 604, 2647, 2856], "temperature": 0.0, "avg_logprob": -0.18532279918068334, "compression_ratio": 1.6633165829145728, "no_speech_prob": 6.643389042437775e-06}, {"id": 139, "seek": 68348, "start": 692.44, "end": 699.4, "text": " of which the people that use that language, like use computers, there will be a few gigabytes", "tokens": [295, 597, 264, 561, 300, 764, 300, 2856, 11, 411, 764, 10807, 11, 456, 486, 312, 257, 1326, 42741], "temperature": 0.0, "avg_logprob": -0.18532279918068334, "compression_ratio": 1.6633165829145728, "no_speech_prob": 6.643389042437775e-06}, {"id": 140, "seek": 68348, "start": 699.4, "end": 702.62, "text": " at least of text floating around the internet for you to grab.", "tokens": [412, 1935, 295, 2487, 12607, 926, 264, 4705, 337, 291, 281, 4444, 13], "temperature": 0.0, "avg_logprob": -0.18532279918068334, "compression_ratio": 1.6633165829145728, "no_speech_prob": 6.643389042437775e-06}, {"id": 141, "seek": 68348, "start": 702.62, "end": 709.8000000000001, "text": " So building a language model is only challenging corpus-wise for ancient languages.", "tokens": [407, 2390, 257, 2856, 2316, 307, 787, 7595, 1181, 31624, 12, 3711, 337, 7832, 8650, 13], "temperature": 0.0, "avg_logprob": -0.18532279918068334, "compression_ratio": 1.6633165829145728, "no_speech_prob": 6.643389042437775e-06}, {"id": 142, "seek": 70980, "start": 709.8, "end": 714.4799999999999, "text": " One of our students is trying to do a Sanskrit one, for example, at the moment, but that's", "tokens": [1485, 295, 527, 1731, 307, 1382, 281, 360, 257, 44392, 472, 11, 337, 1365, 11, 412, 264, 1623, 11, 457, 300, 311], "temperature": 0.0, "avg_logprob": -0.1894569167171616, "compression_ratio": 1.5887445887445888, "no_speech_prob": 1.0952890079352073e-05}, {"id": 143, "seek": 70980, "start": 714.4799999999999, "end": 716.76, "text": " very rarely a problem.", "tokens": [588, 13752, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.1894569167171616, "compression_ratio": 1.5887445887445888, "no_speech_prob": 1.0952890079352073e-05}, {"id": 144, "seek": 70980, "start": 716.76, "end": 722.64, "text": " For translation, there are actually some pretty good parallel corpus available for European", "tokens": [1171, 12853, 11, 456, 366, 767, 512, 1238, 665, 8952, 1181, 31624, 2435, 337, 6473], "temperature": 0.0, "avg_logprob": -0.1894569167171616, "compression_ratio": 1.5887445887445888, "no_speech_prob": 1.0952890079352073e-05}, {"id": 145, "seek": 70980, "start": 722.64, "end": 723.64, "text": " languages.", "tokens": [8650, 13], "temperature": 0.0, "avg_logprob": -0.1894569167171616, "compression_ratio": 1.5887445887445888, "no_speech_prob": 1.0952890079352073e-05}, {"id": 146, "seek": 70980, "start": 723.64, "end": 729.0, "text": " The European Parliament basically has every sentence in every European language.", "tokens": [440, 6473, 15538, 1936, 575, 633, 8174, 294, 633, 6473, 2856, 13], "temperature": 0.0, "avg_logprob": -0.1894569167171616, "compression_ratio": 1.5887445887445888, "no_speech_prob": 1.0952890079352073e-05}, {"id": 147, "seek": 70980, "start": 729.0, "end": 734.3599999999999, "text": " Anything that goes through the UN is translated to lots of languages.", "tokens": [11998, 300, 1709, 807, 264, 8229, 307, 16805, 281, 3195, 295, 8650, 13], "temperature": 0.0, "avg_logprob": -0.1894569167171616, "compression_ratio": 1.5887445887445888, "no_speech_prob": 1.0952890079352073e-05}, {"id": 148, "seek": 73436, "start": 734.36, "end": 741.44, "text": " For French to English, we have a particularly nice thing, which is pretty much any semi-official", "tokens": [1171, 5522, 281, 3669, 11, 321, 362, 257, 4098, 1481, 551, 11, 597, 307, 1238, 709, 604, 12909, 12, 78, 37661], "temperature": 0.0, "avg_logprob": -0.1867340156830937, "compression_ratio": 1.6074766355140186, "no_speech_prob": 1.952497950696852e-05}, {"id": 149, "seek": 73436, "start": 741.44, "end": 746.88, "text": " Canadian website will have a French version and an English version.", "tokens": [12641, 3144, 486, 362, 257, 5522, 3037, 293, 364, 3669, 3037, 13], "temperature": 0.0, "avg_logprob": -0.1867340156830937, "compression_ratio": 1.6074766355140186, "no_speech_prob": 1.952497950696852e-05}, {"id": 150, "seek": 73436, "start": 746.88, "end": 752.08, "text": " So this chap, Chris Callison Birch, did a cool thing, which is basically to try to transform", "tokens": [407, 341, 13223, 11, 6688, 7807, 2770, 7145, 339, 11, 630, 257, 1627, 551, 11, 597, 307, 1936, 281, 853, 281, 4088], "temperature": 0.0, "avg_logprob": -0.1867340156830937, "compression_ratio": 1.6074766355140186, "no_speech_prob": 1.952497950696852e-05}, {"id": 151, "seek": 73436, "start": 752.08, "end": 758.72, "text": " French URLs into English URLs by replacing \"-fr with \"-en\", hoping that that retrieves", "tokens": [5522, 43267, 666, 3669, 43267, 538, 19139, 31672, 5779, 365, 31672, 268, 2566, 7159, 300, 300, 19817, 977], "temperature": 0.0, "avg_logprob": -0.1867340156830937, "compression_ratio": 1.6074766355140186, "no_speech_prob": 1.952497950696852e-05}, {"id": 152, "seek": 75872, "start": 758.72, "end": 764.24, "text": " the equivalent document, and then did that for lots and lots of websites, and ended up", "tokens": [264, 10344, 4166, 11, 293, 550, 630, 300, 337, 3195, 293, 3195, 295, 12891, 11, 293, 4590, 493], "temperature": 0.0, "avg_logprob": -0.1487424523980768, "compression_ratio": 1.722007722007722, "no_speech_prob": 1.5445039025507867e-05}, {"id": 153, "seek": 75872, "start": 764.24, "end": 769.44, "text": " creating a huge corpus based on millions of web pages.", "tokens": [4084, 257, 2603, 1181, 31624, 2361, 322, 6803, 295, 3670, 7183, 13], "temperature": 0.0, "avg_logprob": -0.1487424523980768, "compression_ratio": 1.722007722007722, "no_speech_prob": 1.5445039025507867e-05}, {"id": 154, "seek": 75872, "start": 769.44, "end": 774.44, "text": " So French to English, we have this particularly nice resource.", "tokens": [407, 5522, 281, 3669, 11, 321, 362, 341, 4098, 1481, 7684, 13], "temperature": 0.0, "avg_logprob": -0.1487424523980768, "compression_ratio": 1.722007722007722, "no_speech_prob": 1.5445039025507867e-05}, {"id": 155, "seek": 75872, "start": 774.44, "end": 777.5600000000001, "text": " So we're going to start out by talking about how to create the data, then we'll look at", "tokens": [407, 321, 434, 516, 281, 722, 484, 538, 1417, 466, 577, 281, 1884, 264, 1412, 11, 550, 321, 603, 574, 412], "temperature": 0.0, "avg_logprob": -0.1487424523980768, "compression_ratio": 1.722007722007722, "no_speech_prob": 1.5445039025507867e-05}, {"id": 156, "seek": 75872, "start": 777.5600000000001, "end": 780.38, "text": " the architecture, and then we'll look at the loss function.", "tokens": [264, 9482, 11, 293, 550, 321, 603, 574, 412, 264, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1487424523980768, "compression_ratio": 1.722007722007722, "no_speech_prob": 1.5445039025507867e-05}, {"id": 157, "seek": 75872, "start": 780.38, "end": 788.62, "text": " And so for bounding boxes, all of the interesting stuff was in the loss function, but for new", "tokens": [400, 370, 337, 5472, 278, 9002, 11, 439, 295, 264, 1880, 1507, 390, 294, 264, 4470, 2445, 11, 457, 337, 777], "temperature": 0.0, "avg_logprob": -0.1487424523980768, "compression_ratio": 1.722007722007722, "no_speech_prob": 1.5445039025507867e-05}, {"id": 158, "seek": 78862, "start": 788.62, "end": 794.2, "text": " translation, all of the interesting stuff is going to be in the architecture.", "tokens": [12853, 11, 439, 295, 264, 1880, 1507, 307, 516, 281, 312, 294, 264, 9482, 13], "temperature": 0.0, "avg_logprob": -0.17932494910987648, "compression_ratio": 1.5625, "no_speech_prob": 3.4804757888196036e-05}, {"id": 159, "seek": 78862, "start": 794.2, "end": 797.8, "text": " So let's zip through this pretty quickly.", "tokens": [407, 718, 311, 20730, 807, 341, 1238, 2661, 13], "temperature": 0.0, "avg_logprob": -0.17932494910987648, "compression_ratio": 1.5625, "no_speech_prob": 3.4804757888196036e-05}, {"id": 160, "seek": 78862, "start": 797.8, "end": 802.44, "text": " One of the things I want you to think about particularly is what are the relationships", "tokens": [1485, 295, 264, 721, 286, 528, 291, 281, 519, 466, 4098, 307, 437, 366, 264, 6159], "temperature": 0.0, "avg_logprob": -0.17932494910987648, "compression_ratio": 1.5625, "no_speech_prob": 3.4804757888196036e-05}, {"id": 161, "seek": 78862, "start": 802.44, "end": 807.8, "text": " or similarities in terms of the task we're doing and how we do it between language modeling", "tokens": [420, 24197, 294, 2115, 295, 264, 5633, 321, 434, 884, 293, 577, 321, 360, 309, 1296, 2856, 15983], "temperature": 0.0, "avg_logprob": -0.17932494910987648, "compression_ratio": 1.5625, "no_speech_prob": 3.4804757888196036e-05}, {"id": 162, "seek": 78862, "start": 807.8, "end": 814.12, "text": " versus neural translation.", "tokens": [5717, 18161, 12853, 13], "temperature": 0.0, "avg_logprob": -0.17932494910987648, "compression_ratio": 1.5625, "no_speech_prob": 3.4804757888196036e-05}, {"id": 163, "seek": 81412, "start": 814.12, "end": 821.6, "text": " So the basic approach here is that we're going to take a sentence, so this case the example", "tokens": [407, 264, 3875, 3109, 510, 307, 300, 321, 434, 516, 281, 747, 257, 8174, 11, 370, 341, 1389, 264, 1365], "temperature": 0.0, "avg_logprob": -0.1903231484549386, "compression_ratio": 1.6330275229357798, "no_speech_prob": 9.51592119236011e-06}, {"id": 164, "seek": 81412, "start": 821.6, "end": 827.52, "text": " is English to German, and this slide is from Stephen Meridy, we steal everything we can", "tokens": [307, 3669, 281, 6521, 11, 293, 341, 4137, 307, 490, 13391, 6124, 38836, 11, 321, 11009, 1203, 321, 393], "temperature": 0.0, "avg_logprob": -0.1903231484549386, "compression_ratio": 1.6330275229357798, "no_speech_prob": 9.51592119236011e-06}, {"id": 165, "seek": 81412, "start": 827.52, "end": 829.76, "text": " from Stephen.", "tokens": [490, 13391, 13], "temperature": 0.0, "avg_logprob": -0.1903231484549386, "compression_ratio": 1.6330275229357798, "no_speech_prob": 9.51592119236011e-06}, {"id": 166, "seek": 81412, "start": 829.76, "end": 834.72, "text": " We start with some sentence in English, and the first step is to do basically the exact", "tokens": [492, 722, 365, 512, 8174, 294, 3669, 11, 293, 264, 700, 1823, 307, 281, 360, 1936, 264, 1900], "temperature": 0.0, "avg_logprob": -0.1903231484549386, "compression_ratio": 1.6330275229357798, "no_speech_prob": 9.51592119236011e-06}, {"id": 167, "seek": 81412, "start": 834.72, "end": 840.24, "text": " same thing we do in a language model, which is to chuck it through an RNN.", "tokens": [912, 551, 321, 360, 294, 257, 2856, 2316, 11, 597, 307, 281, 20870, 309, 807, 364, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.1903231484549386, "compression_ratio": 1.6330275229357798, "no_speech_prob": 9.51592119236011e-06}, {"id": 168, "seek": 84024, "start": 840.24, "end": 847.44, "text": " Now with our language model, actually let's not even think about language model, let's", "tokens": [823, 365, 527, 2856, 2316, 11, 767, 718, 311, 406, 754, 519, 466, 2856, 2316, 11, 718, 311], "temperature": 0.0, "avg_logprob": -0.18143722285395084, "compression_ratio": 1.4927536231884058, "no_speech_prob": 4.356862518761773e-06}, {"id": 169, "seek": 84024, "start": 847.44, "end": 850.24, "text": " start even easier, the classification model.", "tokens": [722, 754, 3571, 11, 264, 21538, 2316, 13], "temperature": 0.0, "avg_logprob": -0.18143722285395084, "compression_ratio": 1.4927536231884058, "no_speech_prob": 4.356862518761773e-06}, {"id": 170, "seek": 84024, "start": 850.24, "end": 856.84, "text": " So something that turns this sentence into positive or negative sentiment.", "tokens": [407, 746, 300, 4523, 341, 8174, 666, 3353, 420, 3671, 16149, 13], "temperature": 0.0, "avg_logprob": -0.18143722285395084, "compression_ratio": 1.4927536231884058, "no_speech_prob": 4.356862518761773e-06}, {"id": 171, "seek": 85684, "start": 856.84, "end": 872.52, "text": " We had a decoder, something that basically took the RNN output, and from our paper we", "tokens": [492, 632, 257, 979, 19866, 11, 746, 300, 1936, 1890, 264, 45702, 45, 5598, 11, 293, 490, 527, 3035, 321], "temperature": 0.0, "avg_logprob": -0.14412702487993845, "compression_ratio": 1.6863905325443787, "no_speech_prob": 1.4738804566150066e-05}, {"id": 172, "seek": 85684, "start": 872.52, "end": 873.52, "text": " grabbed 3 things.", "tokens": [18607, 805, 721, 13], "temperature": 0.0, "avg_logprob": -0.14412702487993845, "compression_ratio": 1.6863905325443787, "no_speech_prob": 1.4738804566150066e-05}, {"id": 173, "seek": 85684, "start": 873.52, "end": 878.84, "text": " We took a max pool over all of the time steps, we took a mean pool over all the time steps,", "tokens": [492, 1890, 257, 11469, 7005, 670, 439, 295, 264, 565, 4439, 11, 321, 1890, 257, 914, 7005, 670, 439, 264, 565, 4439, 11], "temperature": 0.0, "avg_logprob": -0.14412702487993845, "compression_ratio": 1.6863905325443787, "no_speech_prob": 1.4738804566150066e-05}, {"id": 174, "seek": 85684, "start": 878.84, "end": 884.24, "text": " and we took the value of the RNN at the last time step, stuck all those together, and put", "tokens": [293, 321, 1890, 264, 2158, 295, 264, 45702, 45, 412, 264, 1036, 565, 1823, 11, 5541, 439, 729, 1214, 11, 293, 829], "temperature": 0.0, "avg_logprob": -0.14412702487993845, "compression_ratio": 1.6863905325443787, "no_speech_prob": 1.4738804566150066e-05}, {"id": 175, "seek": 88424, "start": 884.24, "end": 886.96, "text": " it through a linear layer.", "tokens": [309, 807, 257, 8213, 4583, 13], "temperature": 0.0, "avg_logprob": -0.13089828712995663, "compression_ratio": 1.5588235294117647, "no_speech_prob": 1.4510338587570004e-05}, {"id": 176, "seek": 88424, "start": 886.96, "end": 894.08, "text": " Most people don't do that in most NLP stuff, I think it's something we invented.", "tokens": [4534, 561, 500, 380, 360, 300, 294, 881, 426, 45196, 1507, 11, 286, 519, 309, 311, 746, 321, 14479, 13], "temperature": 0.0, "avg_logprob": -0.13089828712995663, "compression_ratio": 1.5588235294117647, "no_speech_prob": 1.4510338587570004e-05}, {"id": 177, "seek": 88424, "start": 894.08, "end": 898.2, "text": " People pretty much always use the last time step, so all the stuff we'll be talking about", "tokens": [3432, 1238, 709, 1009, 764, 264, 1036, 565, 1823, 11, 370, 439, 264, 1507, 321, 603, 312, 1417, 466], "temperature": 0.0, "avg_logprob": -0.13089828712995663, "compression_ratio": 1.5588235294117647, "no_speech_prob": 1.4510338587570004e-05}, {"id": 178, "seek": 88424, "start": 898.2, "end": 901.36, "text": " today uses the last time step.", "tokens": [965, 4960, 264, 1036, 565, 1823, 13], "temperature": 0.0, "avg_logprob": -0.13089828712995663, "compression_ratio": 1.5588235294117647, "no_speech_prob": 1.4510338587570004e-05}, {"id": 179, "seek": 88424, "start": 901.36, "end": 909.52, "text": " So we start out by chucking this sentence through an RNN, and out of it comes some state.", "tokens": [407, 321, 722, 484, 538, 20870, 278, 341, 8174, 807, 364, 45702, 45, 11, 293, 484, 295, 309, 1487, 512, 1785, 13], "temperature": 0.0, "avg_logprob": -0.13089828712995663, "compression_ratio": 1.5588235294117647, "no_speech_prob": 1.4510338587570004e-05}, {"id": 180, "seek": 90952, "start": 909.52, "end": 916.64, "text": " So some state meaning some hidden state, some vector that represents the output of an RNN", "tokens": [407, 512, 1785, 3620, 512, 7633, 1785, 11, 512, 8062, 300, 8855, 264, 5598, 295, 364, 45702, 45], "temperature": 0.0, "avg_logprob": -0.19154582304113052, "compression_ratio": 1.7025862068965518, "no_speech_prob": 4.965270363754826e-07}, {"id": 181, "seek": 90952, "start": 916.64, "end": 918.88, "text": " that has encoded that sentence.", "tokens": [300, 575, 2058, 12340, 300, 8174, 13], "temperature": 0.0, "avg_logprob": -0.19154582304113052, "compression_ratio": 1.7025862068965518, "no_speech_prob": 4.965270363754826e-07}, {"id": 182, "seek": 90952, "start": 918.88, "end": 923.0799999999999, "text": " You'll see the word that Stephen used here was encoder.", "tokens": [509, 603, 536, 264, 1349, 300, 13391, 1143, 510, 390, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.19154582304113052, "compression_ratio": 1.7025862068965518, "no_speech_prob": 4.965270363754826e-07}, {"id": 183, "seek": 90952, "start": 923.0799999999999, "end": 925.72, "text": " We've tended to use the word backbone.", "tokens": [492, 600, 34732, 281, 764, 264, 1349, 34889, 13], "temperature": 0.0, "avg_logprob": -0.19154582304113052, "compression_ratio": 1.7025862068965518, "no_speech_prob": 4.965270363754826e-07}, {"id": 184, "seek": 90952, "start": 925.72, "end": 931.3199999999999, "text": " So like when we've talked about adding a custom head to an existing model, like the existing", "tokens": [407, 411, 562, 321, 600, 2825, 466, 5127, 257, 2375, 1378, 281, 364, 6741, 2316, 11, 411, 264, 6741], "temperature": 0.0, "avg_logprob": -0.19154582304113052, "compression_ratio": 1.7025862068965518, "no_speech_prob": 4.965270363754826e-07}, {"id": 185, "seek": 90952, "start": 931.3199999999999, "end": 935.1999999999999, "text": " pre-trained ImageNet model, for example, we kind of say that's our backbone, and then", "tokens": [659, 12, 17227, 2001, 29903, 31890, 2316, 11, 337, 1365, 11, 321, 733, 295, 584, 300, 311, 527, 34889, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.19154582304113052, "compression_ratio": 1.7025862068965518, "no_speech_prob": 4.965270363754826e-07}, {"id": 186, "seek": 93520, "start": 935.2, "end": 939.6800000000001, "text": " we stick on top of it some head that does the task we want.", "tokens": [321, 2897, 322, 1192, 295, 309, 512, 1378, 300, 775, 264, 5633, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.1115855604115099, "compression_ratio": 1.6485355648535565, "no_speech_prob": 3.4465674616512842e-06}, {"id": 187, "seek": 93520, "start": 939.6800000000001, "end": 944.72, "text": " In sequence-to-sequence learning, they use the word encoder.", "tokens": [682, 8310, 12, 1353, 12, 11834, 655, 2539, 11, 436, 764, 264, 1349, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.1115855604115099, "compression_ratio": 1.6485355648535565, "no_speech_prob": 3.4465674616512842e-06}, {"id": 188, "seek": 93520, "start": 944.72, "end": 949.1600000000001, "text": " But basically it's the same thing, it's some piece of a neural network architecture that", "tokens": [583, 1936, 309, 311, 264, 912, 551, 11, 309, 311, 512, 2522, 295, 257, 18161, 3209, 9482, 300], "temperature": 0.0, "avg_logprob": -0.1115855604115099, "compression_ratio": 1.6485355648535565, "no_speech_prob": 3.4465674616512842e-06}, {"id": 189, "seek": 93520, "start": 949.1600000000001, "end": 956.32, "text": " takes the input and turns it into some representation, which we can then stick a few more layers", "tokens": [2516, 264, 4846, 293, 4523, 309, 666, 512, 10290, 11, 597, 321, 393, 550, 2897, 257, 1326, 544, 7914], "temperature": 0.0, "avg_logprob": -0.1115855604115099, "compression_ratio": 1.6485355648535565, "no_speech_prob": 3.4465674616512842e-06}, {"id": 190, "seek": 93520, "start": 956.32, "end": 963.12, "text": " on top of to grab something out of it, such as we did for the classifier where we stuck", "tokens": [322, 1192, 295, 281, 4444, 746, 484, 295, 309, 11, 1270, 382, 321, 630, 337, 264, 1508, 9902, 689, 321, 5541], "temperature": 0.0, "avg_logprob": -0.1115855604115099, "compression_ratio": 1.6485355648535565, "no_speech_prob": 3.4465674616512842e-06}, {"id": 191, "seek": 96312, "start": 963.12, "end": 971.88, "text": " a linear layer on top of it to turn it into a sentiment, positive or negative.", "tokens": [257, 8213, 4583, 322, 1192, 295, 309, 281, 1261, 309, 666, 257, 16149, 11, 3353, 420, 3671, 13], "temperature": 0.0, "avg_logprob": -0.14460662516151987, "compression_ratio": 1.831578947368421, "no_speech_prob": 3.611958845795016e-06}, {"id": 192, "seek": 96312, "start": 971.88, "end": 980.4, "text": " So this time though, we have something that's a little bit harder than just getting sentiment,", "tokens": [407, 341, 565, 1673, 11, 321, 362, 746, 300, 311, 257, 707, 857, 6081, 813, 445, 1242, 16149, 11], "temperature": 0.0, "avg_logprob": -0.14460662516151987, "compression_ratio": 1.831578947368421, "no_speech_prob": 3.611958845795016e-06}, {"id": 193, "seek": 96312, "start": 980.4, "end": 984.88, "text": " which is I want to turn this state not into a positive or negative sentiment, but into", "tokens": [597, 307, 286, 528, 281, 1261, 341, 1785, 406, 666, 257, 3353, 420, 3671, 16149, 11, 457, 666], "temperature": 0.0, "avg_logprob": -0.14460662516151987, "compression_ratio": 1.831578947368421, "no_speech_prob": 3.611958845795016e-06}, {"id": 194, "seek": 96312, "start": 984.88, "end": 992.6, "text": " a sequence of tokens where that sequence of tokens is the German sentence that we want.", "tokens": [257, 8310, 295, 22667, 689, 300, 8310, 295, 22667, 307, 264, 6521, 8174, 300, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.14460662516151987, "compression_ratio": 1.831578947368421, "no_speech_prob": 3.611958845795016e-06}, {"id": 195, "seek": 99260, "start": 992.6, "end": 997.72, "text": " So this is sounding more like the language model than the classifier, because the language", "tokens": [407, 341, 307, 24931, 544, 411, 264, 2856, 2316, 813, 264, 1508, 9902, 11, 570, 264, 2856], "temperature": 0.0, "avg_logprob": -0.1881318144745879, "compression_ratio": 2.071794871794872, "no_speech_prob": 5.507575679075671e-06}, {"id": 196, "seek": 99260, "start": 997.72, "end": 1002.6, "text": " model had multiple tokens for every input word there was an output word.", "tokens": [2316, 632, 3866, 22667, 337, 633, 4846, 1349, 456, 390, 364, 5598, 1349, 13], "temperature": 0.0, "avg_logprob": -0.1881318144745879, "compression_ratio": 2.071794871794872, "no_speech_prob": 5.507575679075671e-06}, {"id": 197, "seek": 99260, "start": 1002.6, "end": 1008.72, "text": " But the language model was also much easier, because the number of tokens in the language", "tokens": [583, 264, 2856, 2316, 390, 611, 709, 3571, 11, 570, 264, 1230, 295, 22667, 294, 264, 2856], "temperature": 0.0, "avg_logprob": -0.1881318144745879, "compression_ratio": 2.071794871794872, "no_speech_prob": 5.507575679075671e-06}, {"id": 198, "seek": 99260, "start": 1008.72, "end": 1014.0400000000001, "text": " model output was the same length as the number of tokens in the language model input.", "tokens": [2316, 5598, 390, 264, 912, 4641, 382, 264, 1230, 295, 22667, 294, 264, 2856, 2316, 4846, 13], "temperature": 0.0, "avg_logprob": -0.1881318144745879, "compression_ratio": 2.071794871794872, "no_speech_prob": 5.507575679075671e-06}, {"id": 199, "seek": 99260, "start": 1014.0400000000001, "end": 1018.0, "text": " And not only were they the same length, they exactly matched up.", "tokens": [400, 406, 787, 645, 436, 264, 912, 4641, 11, 436, 2293, 21447, 493, 13], "temperature": 0.0, "avg_logprob": -0.1881318144745879, "compression_ratio": 2.071794871794872, "no_speech_prob": 5.507575679075671e-06}, {"id": 200, "seek": 101800, "start": 1018.0, "end": 1024.08, "text": " It's like after word 1 comes word 2, after word 2 comes word 3, and so forth.", "tokens": [467, 311, 411, 934, 1349, 502, 1487, 1349, 568, 11, 934, 1349, 568, 1487, 1349, 805, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.16502910614013672, "compression_ratio": 1.7675438596491229, "no_speech_prob": 3.321369513287209e-05}, {"id": 201, "seek": 101800, "start": 1024.08, "end": 1032.0, "text": " But for translating language, you don't necessarily know that the word he will be translated as", "tokens": [583, 337, 35030, 2856, 11, 291, 500, 380, 4725, 458, 300, 264, 1349, 415, 486, 312, 16805, 382], "temperature": 0.0, "avg_logprob": -0.16502910614013672, "compression_ratio": 1.7675438596491229, "no_speech_prob": 3.321369513287209e-05}, {"id": 202, "seek": 101800, "start": 1032.0, "end": 1035.68, "text": " the first word in the output, and that loved will be the second word in the output.", "tokens": [264, 700, 1349, 294, 264, 5598, 11, 293, 300, 4333, 486, 312, 264, 1150, 1349, 294, 264, 5598, 13], "temperature": 0.0, "avg_logprob": -0.16502910614013672, "compression_ratio": 1.7675438596491229, "no_speech_prob": 3.321369513287209e-05}, {"id": 203, "seek": 101800, "start": 1035.68, "end": 1039.0, "text": " In this particular case, unfortunately, they are the same.", "tokens": [682, 341, 1729, 1389, 11, 7015, 11, 436, 366, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.16502910614013672, "compression_ratio": 1.7675438596491229, "no_speech_prob": 3.321369513287209e-05}, {"id": 204, "seek": 101800, "start": 1039.0, "end": 1044.52, "text": " But very often the subject-object order will be different, or there will be some extra", "tokens": [583, 588, 2049, 264, 3983, 12, 41070, 1668, 486, 312, 819, 11, 420, 456, 486, 312, 512, 2857], "temperature": 0.0, "avg_logprob": -0.16502910614013672, "compression_ratio": 1.7675438596491229, "no_speech_prob": 3.321369513287209e-05}, {"id": 205, "seek": 104452, "start": 1044.52, "end": 1051.68, "text": " words inserted, or some pronouns will need to add some gendered article to it, or whatever.", "tokens": [2283, 27992, 11, 420, 512, 35883, 486, 643, 281, 909, 512, 7898, 292, 7222, 281, 309, 11, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.15153463027056527, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.2606838936335407e-05}, {"id": 206, "seek": 104452, "start": 1051.68, "end": 1056.68, "text": " So this is the key issue we're going to have to deal with, is the fact that we have an", "tokens": [407, 341, 307, 264, 2141, 2734, 321, 434, 516, 281, 362, 281, 2028, 365, 11, 307, 264, 1186, 300, 321, 362, 364], "temperature": 0.0, "avg_logprob": -0.15153463027056527, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.2606838936335407e-05}, {"id": 207, "seek": 104452, "start": 1056.68, "end": 1063.92, "text": " arbitrary length output where the tokens in the output do not correspond to the same order", "tokens": [23211, 4641, 5598, 689, 264, 22667, 294, 264, 5598, 360, 406, 6805, 281, 264, 912, 1668], "temperature": 0.0, "avg_logprob": -0.15153463027056527, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.2606838936335407e-05}, {"id": 208, "seek": 104452, "start": 1063.92, "end": 1068.04, "text": " of specific tokens in the input.", "tokens": [295, 2685, 22667, 294, 264, 4846, 13], "temperature": 0.0, "avg_logprob": -0.15153463027056527, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.2606838936335407e-05}, {"id": 209, "seek": 104452, "start": 1068.04, "end": 1069.6399999999999, "text": " But the general idea is the same.", "tokens": [583, 264, 2674, 1558, 307, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.15153463027056527, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.2606838936335407e-05}, {"id": 210, "seek": 106964, "start": 1069.64, "end": 1076.24, "text": " There's an RNN to encode the input, turns it into some hidden state, and then this is", "tokens": [821, 311, 364, 45702, 45, 281, 2058, 1429, 264, 4846, 11, 4523, 309, 666, 512, 7633, 1785, 11, 293, 550, 341, 307], "temperature": 0.0, "avg_logprob": -0.13649599760481454, "compression_ratio": 1.7405857740585775, "no_speech_prob": 5.507577952812426e-06}, {"id": 211, "seek": 106964, "start": 1076.24, "end": 1080.48, "text": " the new thing we're going to learn is generating a sequence output.", "tokens": [264, 777, 551, 321, 434, 516, 281, 1466, 307, 17746, 257, 8310, 5598, 13], "temperature": 0.0, "avg_logprob": -0.13649599760481454, "compression_ratio": 1.7405857740585775, "no_speech_prob": 5.507577952812426e-06}, {"id": 212, "seek": 106964, "start": 1080.48, "end": 1088.0600000000002, "text": " So we already know sequence to class, that's IMDB classifier, we already know sequence", "tokens": [407, 321, 1217, 458, 8310, 281, 1508, 11, 300, 311, 21463, 27735, 1508, 9902, 11, 321, 1217, 458, 8310], "temperature": 0.0, "avg_logprob": -0.13649599760481454, "compression_ratio": 1.7405857740585775, "no_speech_prob": 5.507577952812426e-06}, {"id": 213, "seek": 106964, "start": 1088.0600000000002, "end": 1093.8000000000002, "text": " to equal length sequence where it corresponds to the same items, that's the language model", "tokens": [281, 2681, 4641, 8310, 689, 309, 23249, 281, 264, 912, 4754, 11, 300, 311, 264, 2856, 2316], "temperature": 0.0, "avg_logprob": -0.13649599760481454, "compression_ratio": 1.7405857740585775, "no_speech_prob": 5.507577952812426e-06}, {"id": 214, "seek": 106964, "start": 1093.8000000000002, "end": 1098.48, "text": " for example, but we don't know yet how to do a general purpose sequence to sequence,", "tokens": [337, 1365, 11, 457, 321, 500, 380, 458, 1939, 577, 281, 360, 257, 2674, 4334, 8310, 281, 8310, 11], "temperature": 0.0, "avg_logprob": -0.13649599760481454, "compression_ratio": 1.7405857740585775, "no_speech_prob": 5.507577952812426e-06}, {"id": 215, "seek": 109848, "start": 1098.48, "end": 1101.28, "text": " so that's the new thing today.", "tokens": [370, 300, 311, 264, 777, 551, 965, 13], "temperature": 0.0, "avg_logprob": -0.17866052900041854, "compression_ratio": 1.5829383886255923, "no_speech_prob": 9.666038749855943e-06}, {"id": 216, "seek": 109848, "start": 1101.28, "end": 1110.2, "text": " Very little of this will make sense unless you really understand Lesson 6, how an RNN", "tokens": [4372, 707, 295, 341, 486, 652, 2020, 5969, 291, 534, 1223, 18649, 266, 1386, 11, 577, 364, 45702, 45], "temperature": 0.0, "avg_logprob": -0.17866052900041854, "compression_ratio": 1.5829383886255923, "no_speech_prob": 9.666038749855943e-06}, {"id": 217, "seek": 109848, "start": 1110.2, "end": 1111.2, "text": " works.", "tokens": [1985, 13], "temperature": 0.0, "avg_logprob": -0.17866052900041854, "compression_ratio": 1.5829383886255923, "no_speech_prob": 9.666038749855943e-06}, {"id": 218, "seek": 109848, "start": 1111.2, "end": 1116.56, "text": " So if some of this lesson doesn't make sense to you and you find yourself wondering what", "tokens": [407, 498, 512, 295, 341, 6898, 1177, 380, 652, 2020, 281, 291, 293, 291, 915, 1803, 6359, 437], "temperature": 0.0, "avg_logprob": -0.17866052900041854, "compression_ratio": 1.5829383886255923, "no_speech_prob": 9.666038749855943e-06}, {"id": 219, "seek": 109848, "start": 1116.56, "end": 1121.92, "text": " does he mean by hidden state exactly, how is that working, go back and re-watch Lesson", "tokens": [775, 415, 914, 538, 7633, 1785, 2293, 11, 577, 307, 300, 1364, 11, 352, 646, 293, 319, 12, 15219, 18649, 266], "temperature": 0.0, "avg_logprob": -0.17866052900041854, "compression_ratio": 1.5829383886255923, "no_speech_prob": 9.666038749855943e-06}, {"id": 220, "seek": 109848, "start": 1121.92, "end": 1125.4, "text": " 6 to give you a very quick review.", "tokens": [1386, 281, 976, 291, 257, 588, 1702, 3131, 13], "temperature": 0.0, "avg_logprob": -0.17866052900041854, "compression_ratio": 1.5829383886255923, "no_speech_prob": 9.666038749855943e-06}, {"id": 221, "seek": 112540, "start": 1125.4, "end": 1133.0800000000002, "text": " We learned that an RNN at its heart is a standard fully connected network, so here's one with", "tokens": [492, 3264, 300, 364, 45702, 45, 412, 1080, 1917, 307, 257, 3832, 4498, 4582, 3209, 11, 370, 510, 311, 472, 365], "temperature": 0.0, "avg_logprob": -0.1621335019770357, "compression_ratio": 1.7740384615384615, "no_speech_prob": 4.4000593334203586e-05}, {"id": 222, "seek": 112540, "start": 1133.0800000000002, "end": 1140.3200000000002, "text": " one, two, three, four layers, takes an input and puts it through four layers, but then", "tokens": [472, 11, 732, 11, 1045, 11, 1451, 7914, 11, 2516, 364, 4846, 293, 8137, 309, 807, 1451, 7914, 11, 457, 550], "temperature": 0.0, "avg_logprob": -0.1621335019770357, "compression_ratio": 1.7740384615384615, "no_speech_prob": 4.4000593334203586e-05}, {"id": 223, "seek": 112540, "start": 1140.3200000000002, "end": 1146.2, "text": " at the second layer it can just concatenate in a second input, third layer concatenate", "tokens": [412, 264, 1150, 4583, 309, 393, 445, 1588, 7186, 473, 294, 257, 1150, 4846, 11, 2636, 4583, 1588, 7186, 473], "temperature": 0.0, "avg_logprob": -0.1621335019770357, "compression_ratio": 1.7740384615384615, "no_speech_prob": 4.4000593334203586e-05}, {"id": 224, "seek": 112540, "start": 1146.2, "end": 1153.2, "text": " in a third input, but we actually wrote this in Python as just literally a four-layer neural", "tokens": [294, 257, 2636, 4846, 11, 457, 321, 767, 4114, 341, 294, 15329, 382, 445, 3736, 257, 1451, 12, 8376, 260, 18161], "temperature": 0.0, "avg_logprob": -0.1621335019770357, "compression_ratio": 1.7740384615384615, "no_speech_prob": 4.4000593334203586e-05}, {"id": 225, "seek": 112540, "start": 1153.2, "end": 1154.2, "text": " network.", "tokens": [3209, 13], "temperature": 0.0, "avg_logprob": -0.1621335019770357, "compression_ratio": 1.7740384615384615, "no_speech_prob": 4.4000593334203586e-05}, {"id": 226, "seek": 115420, "start": 1154.2, "end": 1160.52, "text": " There was nothing else we used other than linear layers and values.", "tokens": [821, 390, 1825, 1646, 321, 1143, 661, 813, 8213, 7914, 293, 4190, 13], "temperature": 0.0, "avg_logprob": -0.13226722268497243, "compression_ratio": 1.7678571428571428, "no_speech_prob": 2.5071227355510928e-05}, {"id": 227, "seek": 115420, "start": 1160.52, "end": 1164.24, "text": " We used the same weight matrix every time an input came in, we used the same matrix", "tokens": [492, 1143, 264, 912, 3364, 8141, 633, 565, 364, 4846, 1361, 294, 11, 321, 1143, 264, 912, 8141], "temperature": 0.0, "avg_logprob": -0.13226722268497243, "compression_ratio": 1.7678571428571428, "no_speech_prob": 2.5071227355510928e-05}, {"id": 228, "seek": 115420, "start": 1164.24, "end": 1168.4, "text": " every time we went from one of these states to the next, and that's why these arrows are", "tokens": [633, 565, 321, 1437, 490, 472, 295, 613, 4368, 281, 264, 958, 11, 293, 300, 311, 983, 613, 19669, 366], "temperature": 0.0, "avg_logprob": -0.13226722268497243, "compression_ratio": 1.7678571428571428, "no_speech_prob": 2.5071227355510928e-05}, {"id": 229, "seek": 115420, "start": 1168.4, "end": 1175.0, "text": " the same color, and so we can redraw that previous thing like this.", "tokens": [264, 912, 2017, 11, 293, 370, 321, 393, 2182, 5131, 300, 3894, 551, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.13226722268497243, "compression_ratio": 1.7678571428571428, "no_speech_prob": 2.5071227355510928e-05}, {"id": 230, "seek": 115420, "start": 1175.0, "end": 1182.92, "text": " And so not only did we redraw it, but we took the four lines of linear, linear, linear,", "tokens": [400, 370, 406, 787, 630, 321, 2182, 5131, 309, 11, 457, 321, 1890, 264, 1451, 3876, 295, 8213, 11, 8213, 11, 8213, 11], "temperature": 0.0, "avg_logprob": -0.13226722268497243, "compression_ratio": 1.7678571428571428, "no_speech_prob": 2.5071227355510928e-05}, {"id": 231, "seek": 118292, "start": 1182.92, "end": 1190.44, "text": " linear code in PyTorch and we replaced it with a for loop.", "tokens": [8213, 3089, 294, 9953, 51, 284, 339, 293, 321, 10772, 309, 365, 257, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.12173005938529968, "compression_ratio": 1.7980295566502462, "no_speech_prob": 5.014720045437571e-06}, {"id": 232, "seek": 118292, "start": 1190.44, "end": 1195.72, "text": " So remember we had something that did exactly the same thing as this, but it just had four", "tokens": [407, 1604, 321, 632, 746, 300, 630, 2293, 264, 912, 551, 382, 341, 11, 457, 309, 445, 632, 1451], "temperature": 0.0, "avg_logprob": -0.12173005938529968, "compression_ratio": 1.7980295566502462, "no_speech_prob": 5.014720045437571e-06}, {"id": 233, "seek": 118292, "start": 1195.72, "end": 1201.92, "text": " lines of code saying linear, linear, linear, linear, and we literally replaced it with", "tokens": [3876, 295, 3089, 1566, 8213, 11, 8213, 11, 8213, 11, 8213, 11, 293, 321, 3736, 10772, 309, 365], "temperature": 0.0, "avg_logprob": -0.12173005938529968, "compression_ratio": 1.7980295566502462, "no_speech_prob": 5.014720045437571e-06}, {"id": 234, "seek": 118292, "start": 1201.92, "end": 1205.42, "text": " a for loop because that's nice to refactor.", "tokens": [257, 337, 6367, 570, 300, 311, 1481, 281, 1895, 15104, 13], "temperature": 0.0, "avg_logprob": -0.12173005938529968, "compression_ratio": 1.7980295566502462, "no_speech_prob": 5.014720045437571e-06}, {"id": 235, "seek": 118292, "start": 1205.42, "end": 1211.48, "text": " So like literally that refactoring, which doesn't change any of the math, any of the", "tokens": [407, 411, 3736, 300, 1895, 578, 3662, 11, 597, 1177, 380, 1319, 604, 295, 264, 5221, 11, 604, 295, 264], "temperature": 0.0, "avg_logprob": -0.12173005938529968, "compression_ratio": 1.7980295566502462, "no_speech_prob": 5.014720045437571e-06}, {"id": 236, "seek": 121148, "start": 1211.48, "end": 1216.44, "text": " ideas, any of the outputs, that refactoring is an RNF.", "tokens": [3487, 11, 604, 295, 264, 23930, 11, 300, 1895, 578, 3662, 307, 364, 45702, 37, 13], "temperature": 0.0, "avg_logprob": -0.15253360072771707, "compression_ratio": 1.6201923076923077, "no_speech_prob": 6.643414508289425e-06}, {"id": 237, "seek": 121148, "start": 1216.44, "end": 1222.8, "text": " It's turning a bunch of separate lines of code into a Python for loop.", "tokens": [467, 311, 6246, 257, 3840, 295, 4994, 3876, 295, 3089, 666, 257, 15329, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.15253360072771707, "compression_ratio": 1.6201923076923077, "no_speech_prob": 6.643414508289425e-06}, {"id": 238, "seek": 121148, "start": 1222.8, "end": 1226.72, "text": " And so that's how we can draw it.", "tokens": [400, 370, 300, 311, 577, 321, 393, 2642, 309, 13], "temperature": 0.0, "avg_logprob": -0.15253360072771707, "compression_ratio": 1.6201923076923077, "no_speech_prob": 6.643414508289425e-06}, {"id": 239, "seek": 121148, "start": 1226.72, "end": 1232.76, "text": " We could take the output so that it's not outside the loop and put it inside the loop,", "tokens": [492, 727, 747, 264, 5598, 370, 300, 309, 311, 406, 2380, 264, 6367, 293, 829, 309, 1854, 264, 6367, 11], "temperature": 0.0, "avg_logprob": -0.15253360072771707, "compression_ratio": 1.6201923076923077, "no_speech_prob": 6.643414508289425e-06}, {"id": 240, "seek": 121148, "start": 1232.76, "end": 1240.6, "text": " like so, and if we do that, we're now going to generate a separate output for every input.", "tokens": [411, 370, 11, 293, 498, 321, 360, 300, 11, 321, 434, 586, 516, 281, 8460, 257, 4994, 5598, 337, 633, 4846, 13], "temperature": 0.0, "avg_logprob": -0.15253360072771707, "compression_ratio": 1.6201923076923077, "no_speech_prob": 6.643414508289425e-06}, {"id": 241, "seek": 124060, "start": 1240.6, "end": 1250.0, "text": " So in this case, this particular one here, the hidden state gets replaced each time and", "tokens": [407, 294, 341, 1389, 11, 341, 1729, 472, 510, 11, 264, 7633, 1785, 2170, 10772, 1184, 565, 293], "temperature": 0.0, "avg_logprob": -0.14433268757609577, "compression_ratio": 1.6, "no_speech_prob": 6.048898285371251e-06}, {"id": 242, "seek": 124060, "start": 1250.0, "end": 1252.9199999999998, "text": " we end up just spitting out the final hidden state.", "tokens": [321, 917, 493, 445, 637, 2414, 484, 264, 2572, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.14433268757609577, "compression_ratio": 1.6, "no_speech_prob": 6.048898285371251e-06}, {"id": 243, "seek": 124060, "start": 1252.9199999999998, "end": 1256.6399999999999, "text": " So this one is this example.", "tokens": [407, 341, 472, 307, 341, 1365, 13], "temperature": 0.0, "avg_logprob": -0.14433268757609577, "compression_ratio": 1.6, "no_speech_prob": 6.048898285371251e-06}, {"id": 244, "seek": 124060, "start": 1256.6399999999999, "end": 1265.48, "text": " But if instead we had something that said h's.append h and returned h's at the end,", "tokens": [583, 498, 2602, 321, 632, 746, 300, 848, 276, 311, 13, 1746, 521, 276, 293, 8752, 276, 311, 412, 264, 917, 11], "temperature": 0.0, "avg_logprob": -0.14433268757609577, "compression_ratio": 1.6, "no_speech_prob": 6.048898285371251e-06}, {"id": 245, "seek": 124060, "start": 1265.48, "end": 1268.1599999999999, "text": " that would be this picture.", "tokens": [300, 576, 312, 341, 3036, 13], "temperature": 0.0, "avg_logprob": -0.14433268757609577, "compression_ratio": 1.6, "no_speech_prob": 6.048898285371251e-06}, {"id": 246, "seek": 126816, "start": 1268.16, "end": 1270.72, "text": " And so go back and re-look at that notebook if this is unclear.", "tokens": [400, 370, 352, 646, 293, 319, 12, 12747, 412, 300, 21060, 498, 341, 307, 25636, 13], "temperature": 0.0, "avg_logprob": -0.20220914162880133, "compression_ratio": 1.6544715447154472, "no_speech_prob": 1.4970971278671641e-05}, {"id": 247, "seek": 126816, "start": 1270.72, "end": 1275.6000000000001, "text": " I think the main thing to remember is when we say hidden state, we're referring to a", "tokens": [286, 519, 264, 2135, 551, 281, 1604, 307, 562, 321, 584, 7633, 1785, 11, 321, 434, 13761, 281, 257], "temperature": 0.0, "avg_logprob": -0.20220914162880133, "compression_ratio": 1.6544715447154472, "no_speech_prob": 1.4970971278671641e-05}, {"id": 248, "seek": 126816, "start": 1275.6000000000001, "end": 1276.6000000000001, "text": " vector.", "tokens": [8062, 13], "temperature": 0.0, "avg_logprob": -0.20220914162880133, "compression_ratio": 1.6544715447154472, "no_speech_prob": 1.4970971278671641e-05}, {"id": 249, "seek": 126816, "start": 1276.6000000000001, "end": 1279.72, "text": " See here, here's the vector.", "tokens": [3008, 510, 11, 510, 311, 264, 8062, 13], "temperature": 0.0, "avg_logprob": -0.20220914162880133, "compression_ratio": 1.6544715447154472, "no_speech_prob": 1.4970971278671641e-05}, {"id": 250, "seek": 126816, "start": 1279.72, "end": 1284.92, "text": " h equals torch.zeros and hidden.", "tokens": [276, 6915, 27822, 13, 4527, 329, 293, 7633, 13], "temperature": 0.0, "avg_logprob": -0.20220914162880133, "compression_ratio": 1.6544715447154472, "no_speech_prob": 1.4970971278671641e-05}, {"id": 251, "seek": 126816, "start": 1284.92, "end": 1287.0800000000002, "text": " Now of course it's a vector for each thing in the minibash.", "tokens": [823, 295, 1164, 309, 311, 257, 8062, 337, 1184, 551, 294, 264, 923, 897, 1299, 13], "temperature": 0.0, "avg_logprob": -0.20220914162880133, "compression_ratio": 1.6544715447154472, "no_speech_prob": 1.4970971278671641e-05}, {"id": 252, "seek": 126816, "start": 1287.0800000000002, "end": 1289.76, "text": " So it's a matrix.", "tokens": [407, 309, 311, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.20220914162880133, "compression_ratio": 1.6544715447154472, "no_speech_prob": 1.4970971278671641e-05}, {"id": 253, "seek": 126816, "start": 1289.76, "end": 1294.3600000000001, "text": " But generally when I speak about these things, I ignore the minibash piece and treat it as", "tokens": [583, 5101, 562, 286, 1710, 466, 613, 721, 11, 286, 11200, 264, 923, 897, 1299, 2522, 293, 2387, 309, 382], "temperature": 0.0, "avg_logprob": -0.20220914162880133, "compression_ratio": 1.6544715447154472, "no_speech_prob": 1.4970971278671641e-05}, {"id": 254, "seek": 126816, "start": 1294.3600000000001, "end": 1295.72, "text": " just a single item.", "tokens": [445, 257, 2167, 3174, 13], "temperature": 0.0, "avg_logprob": -0.20220914162880133, "compression_ratio": 1.6544715447154472, "no_speech_prob": 1.4970971278671641e-05}, {"id": 255, "seek": 129572, "start": 1295.72, "end": 1302.08, "text": " So it's just a vector of this length.", "tokens": [407, 309, 311, 445, 257, 8062, 295, 341, 4641, 13], "temperature": 0.0, "avg_logprob": -0.12412699699401855, "compression_ratio": 1.6454545454545455, "no_speech_prob": 1.8738712697086157e-06}, {"id": 256, "seek": 129572, "start": 1302.08, "end": 1305.88, "text": " We also learnt that you can stack these layers on top of each other.", "tokens": [492, 611, 18991, 300, 291, 393, 8630, 613, 7914, 322, 1192, 295, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.12412699699401855, "compression_ratio": 1.6454545454545455, "no_speech_prob": 1.8738712697086157e-06}, {"id": 257, "seek": 129572, "start": 1305.88, "end": 1311.2, "text": " So rather than this first RNN spitting out output, it could just spit out inputs into", "tokens": [407, 2831, 813, 341, 700, 45702, 45, 637, 2414, 484, 5598, 11, 309, 727, 445, 22127, 484, 15743, 666], "temperature": 0.0, "avg_logprob": -0.12412699699401855, "compression_ratio": 1.6454545454545455, "no_speech_prob": 1.8738712697086157e-06}, {"id": 258, "seek": 129572, "start": 1311.2, "end": 1315.24, "text": " a second RNN.", "tokens": [257, 1150, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.12412699699401855, "compression_ratio": 1.6454545454545455, "no_speech_prob": 1.8738712697086157e-06}, {"id": 259, "seek": 129572, "start": 1315.24, "end": 1320.4, "text": " If you're thinking at this point, I think I understand this, but I'm not quite sure.", "tokens": [759, 291, 434, 1953, 412, 341, 935, 11, 286, 519, 286, 1223, 341, 11, 457, 286, 478, 406, 1596, 988, 13], "temperature": 0.0, "avg_logprob": -0.12412699699401855, "compression_ratio": 1.6454545454545455, "no_speech_prob": 1.8738712697086157e-06}, {"id": 260, "seek": 129572, "start": 1320.4, "end": 1324.2, "text": " If you're anything like that me, that means you don't understand this.", "tokens": [759, 291, 434, 1340, 411, 300, 385, 11, 300, 1355, 291, 500, 380, 1223, 341, 13], "temperature": 0.0, "avg_logprob": -0.12412699699401855, "compression_ratio": 1.6454545454545455, "no_speech_prob": 1.8738712697086157e-06}, {"id": 261, "seek": 132420, "start": 1324.2, "end": 1330.2, "text": " And the only way you know and actually understand it is to go and write this from scratch in", "tokens": [400, 264, 787, 636, 291, 458, 293, 767, 1223, 309, 307, 281, 352, 293, 2464, 341, 490, 8459, 294], "temperature": 0.0, "avg_logprob": -0.17125370788574218, "compression_ratio": 1.7580645161290323, "no_speech_prob": 7.527946763730142e-06}, {"id": 262, "seek": 132420, "start": 1330.2, "end": 1332.56, "text": " PyTorch or NumPy.", "tokens": [9953, 51, 284, 339, 420, 22592, 47, 88, 13], "temperature": 0.0, "avg_logprob": -0.17125370788574218, "compression_ratio": 1.7580645161290323, "no_speech_prob": 7.527946763730142e-06}, {"id": 263, "seek": 132420, "start": 1332.56, "end": 1336.8400000000001, "text": " And if you can't do that, then you know you don't understand it, and you can go back and", "tokens": [400, 498, 291, 393, 380, 360, 300, 11, 550, 291, 458, 291, 500, 380, 1223, 309, 11, 293, 291, 393, 352, 646, 293], "temperature": 0.0, "avg_logprob": -0.17125370788574218, "compression_ratio": 1.7580645161290323, "no_speech_prob": 7.527946763730142e-06}, {"id": 264, "seek": 132420, "start": 1336.8400000000001, "end": 1341.96, "text": " re-watch lesson 6 and check out the notebook and copy some of the ideas until you can,", "tokens": [319, 12, 15219, 6898, 1386, 293, 1520, 484, 264, 21060, 293, 5055, 512, 295, 264, 3487, 1826, 291, 393, 11], "temperature": 0.0, "avg_logprob": -0.17125370788574218, "compression_ratio": 1.7580645161290323, "no_speech_prob": 7.527946763730142e-06}, {"id": 265, "seek": 132420, "start": 1341.96, "end": 1344.68, "text": " it's really important that you can write that from scratch.", "tokens": [309, 311, 534, 1021, 300, 291, 393, 2464, 300, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.17125370788574218, "compression_ratio": 1.7580645161290323, "no_speech_prob": 7.527946763730142e-06}, {"id": 266, "seek": 132420, "start": 1344.68, "end": 1348.44, "text": " It's less than a screen of code.", "tokens": [467, 311, 1570, 813, 257, 2568, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.17125370788574218, "compression_ratio": 1.7580645161290323, "no_speech_prob": 7.527946763730142e-06}, {"id": 267, "seek": 132420, "start": 1348.44, "end": 1354.16, "text": " So you want to make sure you can create a two-layer RNN.", "tokens": [407, 291, 528, 281, 652, 988, 291, 393, 1884, 257, 732, 12, 8376, 260, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.17125370788574218, "compression_ratio": 1.7580645161290323, "no_speech_prob": 7.527946763730142e-06}, {"id": 268, "seek": 135416, "start": 1354.16, "end": 1358.92, "text": " And this is what it looks like if you unroll it.", "tokens": [400, 341, 307, 437, 309, 1542, 411, 498, 291, 517, 3970, 309, 13], "temperature": 0.0, "avg_logprob": -0.154263652100855, "compression_ratio": 1.6088888888888888, "no_speech_prob": 3.480739178485237e-05}, {"id": 269, "seek": 135416, "start": 1358.92, "end": 1365.5600000000002, "text": " So that's the goal, to get to a point where we first of all have these x, y pairs of sentences", "tokens": [407, 300, 311, 264, 3387, 11, 281, 483, 281, 257, 935, 689, 321, 700, 295, 439, 362, 613, 2031, 11, 288, 15494, 295, 16579], "temperature": 0.0, "avg_logprob": -0.154263652100855, "compression_ratio": 1.6088888888888888, "no_speech_prob": 3.480739178485237e-05}, {"id": 270, "seek": 135416, "start": 1365.5600000000002, "end": 1368.4, "text": " and we're going to do French to English.", "tokens": [293, 321, 434, 516, 281, 360, 5522, 281, 3669, 13], "temperature": 0.0, "avg_logprob": -0.154263652100855, "compression_ratio": 1.6088888888888888, "no_speech_prob": 3.480739178485237e-05}, {"id": 271, "seek": 135416, "start": 1368.4, "end": 1375.0400000000002, "text": " So we're going to start by downloading this dataset.", "tokens": [407, 321, 434, 516, 281, 722, 538, 32529, 341, 28872, 13], "temperature": 0.0, "avg_logprob": -0.154263652100855, "compression_ratio": 1.6088888888888888, "no_speech_prob": 3.480739178485237e-05}, {"id": 272, "seek": 135416, "start": 1375.0400000000002, "end": 1379.1200000000001, "text": " Training a translation model takes a long time.", "tokens": [20620, 257, 12853, 2316, 2516, 257, 938, 565, 13], "temperature": 0.0, "avg_logprob": -0.154263652100855, "compression_ratio": 1.6088888888888888, "no_speech_prob": 3.480739178485237e-05}, {"id": 273, "seek": 135416, "start": 1379.1200000000001, "end": 1383.96, "text": " Google's translation model has 8 layers of RNN stacked on top of each other.", "tokens": [3329, 311, 12853, 2316, 575, 1649, 7914, 295, 45702, 45, 28867, 322, 1192, 295, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.154263652100855, "compression_ratio": 1.6088888888888888, "no_speech_prob": 3.480739178485237e-05}, {"id": 274, "seek": 138396, "start": 1383.96, "end": 1388.88, "text": " There's no conceptual difference between 8 layers and 2 layers.", "tokens": [821, 311, 572, 24106, 2649, 1296, 1649, 7914, 293, 568, 7914, 13], "temperature": 0.0, "avg_logprob": -0.1525290161371231, "compression_ratio": 1.713355048859935, "no_speech_prob": 1.4510215805785265e-05}, {"id": 275, "seek": 138396, "start": 1388.88, "end": 1392.4, "text": " It's just like if you're Google and you have more GPUs or TPUs and you know what to do", "tokens": [467, 311, 445, 411, 498, 291, 434, 3329, 293, 291, 362, 544, 18407, 82, 420, 314, 8115, 82, 293, 291, 458, 437, 281, 360], "temperature": 0.0, "avg_logprob": -0.1525290161371231, "compression_ratio": 1.713355048859935, "no_speech_prob": 1.4510215805785265e-05}, {"id": 276, "seek": 138396, "start": 1392.4, "end": 1394.16, "text": " with, then you're fine doing that.", "tokens": [365, 11, 550, 291, 434, 2489, 884, 300, 13], "temperature": 0.0, "avg_logprob": -0.1525290161371231, "compression_ratio": 1.713355048859935, "no_speech_prob": 1.4510215805785265e-05}, {"id": 277, "seek": 138396, "start": 1394.16, "end": 1398.3600000000001, "text": " Whereas in our case, it's pretty likely that the kind of sequence to sequence models we're", "tokens": [13813, 294, 527, 1389, 11, 309, 311, 1238, 3700, 300, 264, 733, 295, 8310, 281, 8310, 5245, 321, 434], "temperature": 0.0, "avg_logprob": -0.1525290161371231, "compression_ratio": 1.713355048859935, "no_speech_prob": 1.4510215805785265e-05}, {"id": 278, "seek": 138396, "start": 1398.3600000000001, "end": 1401.92, "text": " building are not going to require that level of computation.", "tokens": [2390, 366, 406, 516, 281, 3651, 300, 1496, 295, 24903, 13], "temperature": 0.0, "avg_logprob": -0.1525290161371231, "compression_ratio": 1.713355048859935, "no_speech_prob": 1.4510215805785265e-05}, {"id": 279, "seek": 138396, "start": 1401.92, "end": 1408.04, "text": " So to keep things simple, let's do a cut-down thing where rather than learning how to translate", "tokens": [407, 281, 1066, 721, 2199, 11, 718, 311, 360, 257, 1723, 12, 5093, 551, 689, 2831, 813, 2539, 577, 281, 13799], "temperature": 0.0, "avg_logprob": -0.1525290161371231, "compression_ratio": 1.713355048859935, "no_speech_prob": 1.4510215805785265e-05}, {"id": 280, "seek": 138396, "start": 1408.04, "end": 1413.46, "text": " French into English for any sentence, let's learn to translate French questions into English", "tokens": [5522, 666, 3669, 337, 604, 8174, 11, 718, 311, 1466, 281, 13799, 5522, 1651, 666, 3669], "temperature": 0.0, "avg_logprob": -0.1525290161371231, "compression_ratio": 1.713355048859935, "no_speech_prob": 1.4510215805785265e-05}, {"id": 281, "seek": 141346, "start": 1413.46, "end": 1414.46, "text": " questions.", "tokens": [1651, 13], "temperature": 0.0, "avg_logprob": -0.20846068743363166, "compression_ratio": 1.702127659574468, "no_speech_prob": 2.355217293370515e-05}, {"id": 282, "seek": 141346, "start": 1414.46, "end": 1418.32, "text": " Specifically questions that start with what, where, which, when.", "tokens": [26058, 1651, 300, 722, 365, 437, 11, 689, 11, 597, 11, 562, 13], "temperature": 0.0, "avg_logprob": -0.20846068743363166, "compression_ratio": 1.702127659574468, "no_speech_prob": 2.355217293370515e-05}, {"id": 283, "seek": 141346, "start": 1418.32, "end": 1422.16, "text": " So you can see here I've got a regex that looks for things that start with wh and end", "tokens": [407, 291, 393, 536, 510, 286, 600, 658, 257, 319, 432, 87, 300, 1542, 337, 721, 300, 722, 365, 315, 293, 917], "temperature": 0.0, "avg_logprob": -0.20846068743363166, "compression_ratio": 1.702127659574468, "no_speech_prob": 2.355217293370515e-05}, {"id": 284, "seek": 141346, "start": 1422.16, "end": 1423.16, "text": " with a question mark.", "tokens": [365, 257, 1168, 1491, 13], "temperature": 0.0, "avg_logprob": -0.20846068743363166, "compression_ratio": 1.702127659574468, "no_speech_prob": 2.355217293370515e-05}, {"id": 285, "seek": 141346, "start": 1423.16, "end": 1429.4, "text": " So I just go through the corpus, open up each of the 2 files, each line is one parallel", "tokens": [407, 286, 445, 352, 807, 264, 1181, 31624, 11, 1269, 493, 1184, 295, 264, 568, 7098, 11, 1184, 1622, 307, 472, 8952], "temperature": 0.0, "avg_logprob": -0.20846068743363166, "compression_ratio": 1.702127659574468, "no_speech_prob": 2.355217293370515e-05}, {"id": 286, "seek": 141346, "start": 1429.4, "end": 1434.16, "text": " text, zip them together, grab the English question and the French question and check", "tokens": [2487, 11, 20730, 552, 1214, 11, 4444, 264, 3669, 1168, 293, 264, 5522, 1168, 293, 1520], "temperature": 0.0, "avg_logprob": -0.20846068743363166, "compression_ratio": 1.702127659574468, "no_speech_prob": 2.355217293370515e-05}, {"id": 287, "seek": 141346, "start": 1434.16, "end": 1437.8, "text": " whether they match the regular expressions.", "tokens": [1968, 436, 2995, 264, 3890, 15277, 13], "temperature": 0.0, "avg_logprob": -0.20846068743363166, "compression_ratio": 1.702127659574468, "no_speech_prob": 2.355217293370515e-05}, {"id": 288, "seek": 143780, "start": 1437.8, "end": 1449.52, "text": " So we now have 52,000 sentences and here are some examples of those sentence pairs.", "tokens": [407, 321, 586, 362, 18079, 11, 1360, 16579, 293, 510, 366, 512, 5110, 295, 729, 8174, 15494, 13], "temperature": 0.0, "avg_logprob": -0.20597328859217026, "compression_ratio": 1.455497382198953, "no_speech_prob": 8.530200830136891e-06}, {"id": 289, "seek": 143780, "start": 1449.52, "end": 1455.12, "text": " One nice thing about this is that what, who, where type questions tend to be fairly short,", "tokens": [1485, 1481, 551, 466, 341, 307, 300, 437, 11, 567, 11, 689, 2010, 1651, 3928, 281, 312, 6457, 2099, 11], "temperature": 0.0, "avg_logprob": -0.20597328859217026, "compression_ratio": 1.455497382198953, "no_speech_prob": 8.530200830136891e-06}, {"id": 290, "seek": 143780, "start": 1455.12, "end": 1458.0, "text": " which is nice.", "tokens": [597, 307, 1481, 13], "temperature": 0.0, "avg_logprob": -0.20597328859217026, "compression_ratio": 1.455497382198953, "no_speech_prob": 8.530200830136891e-06}, {"id": 291, "seek": 143780, "start": 1458.0, "end": 1464.2, "text": " But I would say the idea that we could learn from scratch with no previous understanding", "tokens": [583, 286, 576, 584, 264, 1558, 300, 321, 727, 1466, 490, 8459, 365, 572, 3894, 3701], "temperature": 0.0, "avg_logprob": -0.20597328859217026, "compression_ratio": 1.455497382198953, "no_speech_prob": 8.530200830136891e-06}, {"id": 292, "seek": 146420, "start": 1464.2, "end": 1468.88, "text": " of the idea of language, let alone of English or of French, that we could create something", "tokens": [295, 264, 1558, 295, 2856, 11, 718, 3312, 295, 3669, 420, 295, 5522, 11, 300, 321, 727, 1884, 746], "temperature": 0.0, "avg_logprob": -0.14616905725919282, "compression_ratio": 1.6022727272727273, "no_speech_prob": 2.6425616397318663e-06}, {"id": 293, "seek": 146420, "start": 1468.88, "end": 1474.1200000000001, "text": " that can translate one to the other for any arbitrary question with only 50,000 sentences,", "tokens": [300, 393, 13799, 472, 281, 264, 661, 337, 604, 23211, 1168, 365, 787, 2625, 11, 1360, 16579, 11], "temperature": 0.0, "avg_logprob": -0.14616905725919282, "compression_ratio": 1.6022727272727273, "no_speech_prob": 2.6425616397318663e-06}, {"id": 294, "seek": 146420, "start": 1474.1200000000001, "end": 1479.8400000000001, "text": " it sounds like a ludicrously difficult thing to ask this to do.", "tokens": [309, 3263, 411, 257, 15946, 299, 81, 5098, 2252, 551, 281, 1029, 341, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.14616905725919282, "compression_ratio": 1.6022727272727273, "no_speech_prob": 2.6425616397318663e-06}, {"id": 295, "seek": 146420, "start": 1479.8400000000001, "end": 1484.04, "text": " So I will be impressed if we can make any progress whatsoever, because this is very", "tokens": [407, 286, 486, 312, 11679, 498, 321, 393, 652, 604, 4205, 17076, 11, 570, 341, 307, 588], "temperature": 0.0, "avg_logprob": -0.14616905725919282, "compression_ratio": 1.6022727272727273, "no_speech_prob": 2.6425616397318663e-06}, {"id": 296, "seek": 146420, "start": 1484.04, "end": 1489.4, "text": " little data to do a very complex exercise.", "tokens": [707, 1412, 281, 360, 257, 588, 3997, 5380, 13], "temperature": 0.0, "avg_logprob": -0.14616905725919282, "compression_ratio": 1.6022727272727273, "no_speech_prob": 2.6425616397318663e-06}, {"id": 297, "seek": 146420, "start": 1489.4, "end": 1492.8400000000001, "text": " So this contains the tuples of French and English.", "tokens": [407, 341, 8306, 264, 2604, 2622, 295, 5522, 293, 3669, 13], "temperature": 0.0, "avg_logprob": -0.14616905725919282, "compression_ratio": 1.6022727272727273, "no_speech_prob": 2.6425616397318663e-06}, {"id": 298, "seek": 149284, "start": 1492.84, "end": 1497.52, "text": " You can use this handy idiom to split them apart into a list of English questions and", "tokens": [509, 393, 764, 341, 13239, 18014, 298, 281, 7472, 552, 4936, 666, 257, 1329, 295, 3669, 1651, 293], "temperature": 0.0, "avg_logprob": -0.1292170828038996, "compression_ratio": 1.803921568627451, "no_speech_prob": 3.882818418787792e-05}, {"id": 299, "seek": 149284, "start": 1497.52, "end": 1499.6, "text": " a list of French questions.", "tokens": [257, 1329, 295, 5522, 1651, 13], "temperature": 0.0, "avg_logprob": -0.1292170828038996, "compression_ratio": 1.803921568627451, "no_speech_prob": 3.882818418787792e-05}, {"id": 300, "seek": 149284, "start": 1499.6, "end": 1505.24, "text": " And then we tokenize the English questions and we tokenize the French questions.", "tokens": [400, 550, 321, 14862, 1125, 264, 3669, 1651, 293, 321, 14862, 1125, 264, 5522, 1651, 13], "temperature": 0.0, "avg_logprob": -0.1292170828038996, "compression_ratio": 1.803921568627451, "no_speech_prob": 3.882818418787792e-05}, {"id": 301, "seek": 149284, "start": 1505.24, "end": 1512.48, "text": " So remember that just means splitting them up into separate words or word-like things.", "tokens": [407, 1604, 300, 445, 1355, 30348, 552, 493, 666, 4994, 2283, 420, 1349, 12, 4092, 721, 13], "temperature": 0.0, "avg_logprob": -0.1292170828038996, "compression_ratio": 1.803921568627451, "no_speech_prob": 3.882818418787792e-05}, {"id": 302, "seek": 149284, "start": 1512.48, "end": 1517.9199999999998, "text": " By default, the tokenizer that we have here, and remember this is a wrapper around the", "tokens": [3146, 7576, 11, 264, 14862, 6545, 300, 321, 362, 510, 11, 293, 1604, 341, 307, 257, 46906, 926, 264], "temperature": 0.0, "avg_logprob": -0.1292170828038996, "compression_ratio": 1.803921568627451, "no_speech_prob": 3.882818418787792e-05}, {"id": 303, "seek": 151792, "start": 1517.92, "end": 1525.52, "text": " spaCy tokenizer, which is a fantastic tokenizer, this wrapper by default assumes English.", "tokens": [32543, 34, 88, 14862, 6545, 11, 597, 307, 257, 5456, 14862, 6545, 11, 341, 46906, 538, 7576, 37808, 3669, 13], "temperature": 0.0, "avg_logprob": -0.19197586059570312, "compression_ratio": 1.6508620689655173, "no_speech_prob": 3.1200637749861926e-05}, {"id": 304, "seek": 151792, "start": 1525.52, "end": 1528.3600000000001, "text": " So to ask for French, you just add an extra parameter.", "tokens": [407, 281, 1029, 337, 5522, 11, 291, 445, 909, 364, 2857, 13075, 13], "temperature": 0.0, "avg_logprob": -0.19197586059570312, "compression_ratio": 1.6508620689655173, "no_speech_prob": 3.1200637749861926e-05}, {"id": 305, "seek": 151792, "start": 1528.3600000000001, "end": 1531.64, "text": " The first time you do this, you'll get an error saying that you don't have the spaCy", "tokens": [440, 700, 565, 291, 360, 341, 11, 291, 603, 483, 364, 6713, 1566, 300, 291, 500, 380, 362, 264, 32543, 34, 88], "temperature": 0.0, "avg_logprob": -0.19197586059570312, "compression_ratio": 1.6508620689655173, "no_speech_prob": 3.1200637749861926e-05}, {"id": 306, "seek": 151792, "start": 1531.64, "end": 1537.64, "text": " French model installed, and you can Google to get the command something Python-m spaCy", "tokens": [5522, 2316, 8899, 11, 293, 291, 393, 3329, 281, 483, 264, 5622, 746, 15329, 12, 76, 32543, 34, 88], "temperature": 0.0, "avg_logprob": -0.19197586059570312, "compression_ratio": 1.6508620689655173, "no_speech_prob": 3.1200637749861926e-05}, {"id": 307, "seek": 151792, "start": 1537.64, "end": 1544.52, "text": " download French, or something like that, to grab the French model.", "tokens": [5484, 5522, 11, 420, 746, 411, 300, 11, 281, 4444, 264, 5522, 2316, 13], "temperature": 0.0, "avg_logprob": -0.19197586059570312, "compression_ratio": 1.6508620689655173, "no_speech_prob": 3.1200637749861926e-05}, {"id": 308, "seek": 154452, "start": 1544.52, "end": 1548.52, "text": " I don't think any of you are going to have RAM problems here, because this is not particularly", "tokens": [286, 500, 380, 519, 604, 295, 291, 366, 516, 281, 362, 14561, 2740, 510, 11, 570, 341, 307, 406, 4098], "temperature": 0.0, "avg_logprob": -0.16461528280506962, "compression_ratio": 1.6354166666666667, "no_speech_prob": 1.9832847101497464e-05}, {"id": 309, "seek": 154452, "start": 1548.52, "end": 1553.12, "text": " big corpus, but I know that some of you were trying to train new language models during", "tokens": [955, 1181, 31624, 11, 457, 286, 458, 300, 512, 295, 291, 645, 1382, 281, 3847, 777, 2856, 5245, 1830], "temperature": 0.0, "avg_logprob": -0.16461528280506962, "compression_ratio": 1.6354166666666667, "no_speech_prob": 1.9832847101497464e-05}, {"id": 310, "seek": 154452, "start": 1553.12, "end": 1555.3, "text": " the week and were having RAM problems.", "tokens": [264, 1243, 293, 645, 1419, 14561, 2740, 13], "temperature": 0.0, "avg_logprob": -0.16461528280506962, "compression_ratio": 1.6354166666666667, "no_speech_prob": 1.9832847101497464e-05}, {"id": 311, "seek": 154452, "start": 1555.3, "end": 1559.44, "text": " If you do, it's worth knowing what these functions are actually doing.", "tokens": [759, 291, 360, 11, 309, 311, 3163, 5276, 437, 613, 6828, 366, 767, 884, 13], "temperature": 0.0, "avg_logprob": -0.16461528280506962, "compression_ratio": 1.6354166666666667, "no_speech_prob": 1.9832847101497464e-05}, {"id": 312, "seek": 154452, "start": 1559.44, "end": 1563.92, "text": " So for example, these ones here is processing every sentence across multiple processes.", "tokens": [407, 337, 1365, 11, 613, 2306, 510, 307, 9007, 633, 8174, 2108, 3866, 7555, 13], "temperature": 0.0, "avg_logprob": -0.16461528280506962, "compression_ratio": 1.6354166666666667, "no_speech_prob": 1.9832847101497464e-05}, {"id": 313, "seek": 154452, "start": 1563.92, "end": 1565.58, "text": " That's what the MP means.", "tokens": [663, 311, 437, 264, 14146, 1355, 13], "temperature": 0.0, "avg_logprob": -0.16461528280506962, "compression_ratio": 1.6354166666666667, "no_speech_prob": 1.9832847101497464e-05}, {"id": 314, "seek": 154452, "start": 1565.58, "end": 1569.98, "text": " And remember, fastai code is designed to be pretty easy to read.", "tokens": [400, 1604, 11, 2370, 1301, 3089, 307, 4761, 281, 312, 1238, 1858, 281, 1401, 13], "temperature": 0.0, "avg_logprob": -0.16461528280506962, "compression_ratio": 1.6354166666666667, "no_speech_prob": 1.9832847101497464e-05}, {"id": 315, "seek": 156998, "start": 1569.98, "end": 1579.88, "text": " So here's the 3 lines of code to process all MP.", "tokens": [407, 510, 311, 264, 805, 3876, 295, 3089, 281, 1399, 439, 14146, 13], "temperature": 0.0, "avg_logprob": -0.16181572572684583, "compression_ratio": 1.4926108374384237, "no_speech_prob": 3.6119736250839196e-06}, {"id": 316, "seek": 156998, "start": 1579.88, "end": 1584.3600000000001, "text": " Find out how many CPUs you have, divide by 2, because normally with hyperthreading, they", "tokens": [11809, 484, 577, 867, 13199, 82, 291, 362, 11, 9845, 538, 568, 11, 570, 5646, 365, 9848, 392, 35908, 11, 436], "temperature": 0.0, "avg_logprob": -0.16181572572684583, "compression_ratio": 1.4926108374384237, "no_speech_prob": 3.6119736250839196e-06}, {"id": 317, "seek": 156998, "start": 1584.3600000000001, "end": 1587.64, "text": " don't actually all work in parallel.", "tokens": [500, 380, 767, 439, 589, 294, 8952, 13], "temperature": 0.0, "avg_logprob": -0.16181572572684583, "compression_ratio": 1.4926108374384237, "no_speech_prob": 3.6119736250839196e-06}, {"id": 318, "seek": 156998, "start": 1587.64, "end": 1592.72, "text": " Then in parallel, run this process function.", "tokens": [1396, 294, 8952, 11, 1190, 341, 1399, 2445, 13], "temperature": 0.0, "avg_logprob": -0.16181572572684583, "compression_ratio": 1.4926108374384237, "no_speech_prob": 3.6119736250839196e-06}, {"id": 319, "seek": 156998, "start": 1592.72, "end": 1597.74, "text": " So that's going to spit out a whole separate Python process for every CPU you have.", "tokens": [407, 300, 311, 516, 281, 22127, 484, 257, 1379, 4994, 15329, 1399, 337, 633, 13199, 291, 362, 13], "temperature": 0.0, "avg_logprob": -0.16181572572684583, "compression_ratio": 1.4926108374384237, "no_speech_prob": 3.6119736250839196e-06}, {"id": 320, "seek": 159774, "start": 1597.74, "end": 1601.44, "text": " If you have a lot of cores, that's a lot of Python processes, everyone's going to load", "tokens": [759, 291, 362, 257, 688, 295, 24826, 11, 300, 311, 257, 688, 295, 15329, 7555, 11, 1518, 311, 516, 281, 3677], "temperature": 0.0, "avg_logprob": -0.18310524314962406, "compression_ratio": 1.6635514018691588, "no_speech_prob": 1.3211887562647462e-05}, {"id": 321, "seek": 159774, "start": 1601.44, "end": 1606.74, "text": " the whole data in, and that can potentially use up all your RAM.", "tokens": [264, 1379, 1412, 294, 11, 293, 300, 393, 7263, 764, 493, 439, 428, 14561, 13], "temperature": 0.0, "avg_logprob": -0.18310524314962406, "compression_ratio": 1.6635514018691588, "no_speech_prob": 1.3211887562647462e-05}, {"id": 322, "seek": 159774, "start": 1606.74, "end": 1615.08, "text": " So you could replace that with just proc all, rather than proc all MP, to use less RAM.", "tokens": [407, 291, 727, 7406, 300, 365, 445, 9510, 439, 11, 2831, 813, 9510, 439, 14146, 11, 281, 764, 1570, 14561, 13], "temperature": 0.0, "avg_logprob": -0.18310524314962406, "compression_ratio": 1.6635514018691588, "no_speech_prob": 1.3211887562647462e-05}, {"id": 323, "seek": 159774, "start": 1615.08, "end": 1618.44, "text": " Or you could use less cores.", "tokens": [1610, 291, 727, 764, 1570, 24826, 13], "temperature": 0.0, "avg_logprob": -0.18310524314962406, "compression_ratio": 1.6635514018691588, "no_speech_prob": 1.3211887562647462e-05}, {"id": 324, "seek": 159774, "start": 1618.44, "end": 1624.96, "text": " So at the moment, we're calling this function partition by cores, which calls partition", "tokens": [407, 412, 264, 1623, 11, 321, 434, 5141, 341, 2445, 24808, 538, 24826, 11, 597, 5498, 24808], "temperature": 0.0, "avg_logprob": -0.18310524314962406, "compression_ratio": 1.6635514018691588, "no_speech_prob": 1.3211887562647462e-05}, {"id": 325, "seek": 162496, "start": 1624.96, "end": 1630.96, "text": " on a list, and asks to split it into a number of equal length things according to how many", "tokens": [322, 257, 1329, 11, 293, 8962, 281, 7472, 309, 666, 257, 1230, 295, 2681, 4641, 721, 4650, 281, 577, 867], "temperature": 0.0, "avg_logprob": -0.2377445502359359, "compression_ratio": 1.6730769230769231, "no_speech_prob": 3.785308535952936e-06}, {"id": 326, "seek": 162496, "start": 1630.96, "end": 1632.3600000000001, "text": " CPUs you have.", "tokens": [13199, 82, 291, 362, 13], "temperature": 0.0, "avg_logprob": -0.2377445502359359, "compression_ratio": 1.6730769230769231, "no_speech_prob": 3.785308535952936e-06}, {"id": 327, "seek": 162496, "start": 1632.3600000000001, "end": 1638.24, "text": " So you could replace that, splitting it into a smaller list, and run it on less things.", "tokens": [407, 291, 727, 7406, 300, 11, 30348, 309, 666, 257, 4356, 1329, 11, 293, 1190, 309, 322, 1570, 721, 13], "temperature": 0.0, "avg_logprob": -0.2377445502359359, "compression_ratio": 1.6730769230769231, "no_speech_prob": 3.785308535952936e-06}, {"id": 328, "seek": 162496, "start": 1638.24, "end": 1640.04, "text": " Yes, Rachel.", "tokens": [1079, 11, 14246, 13], "temperature": 0.0, "avg_logprob": -0.2377445502359359, "compression_ratio": 1.6730769230769231, "no_speech_prob": 3.785308535952936e-06}, {"id": 329, "seek": 162496, "start": 1640.04, "end": 1641.04, "text": " Question.", "tokens": [14464, 13], "temperature": 0.0, "avg_logprob": -0.2377445502359359, "compression_ratio": 1.6730769230769231, "no_speech_prob": 3.785308535952936e-06}, {"id": 330, "seek": 162496, "start": 1641.04, "end": 1644.3600000000001, "text": " Was an intention layer tried in the language model?", "tokens": [3027, 364, 7789, 4583, 3031, 294, 264, 2856, 2316, 30], "temperature": 0.0, "avg_logprob": -0.2377445502359359, "compression_ratio": 1.6730769230769231, "no_speech_prob": 3.785308535952936e-06}, {"id": 331, "seek": 162496, "start": 1644.3600000000001, "end": 1647.68, "text": " Do you think it would be a good idea to try and add one?", "tokens": [1144, 291, 519, 309, 576, 312, 257, 665, 1558, 281, 853, 293, 909, 472, 30], "temperature": 0.0, "avg_logprob": -0.2377445502359359, "compression_ratio": 1.6730769230769231, "no_speech_prob": 3.785308535952936e-06}, {"id": 332, "seek": 162496, "start": 1647.68, "end": 1651.72, "text": " We haven't learned about attention yet, so let's ask about things that we have got to,", "tokens": [492, 2378, 380, 3264, 466, 3202, 1939, 11, 370, 718, 311, 1029, 466, 721, 300, 321, 362, 658, 281, 11], "temperature": 0.0, "avg_logprob": -0.2377445502359359, "compression_ratio": 1.6730769230769231, "no_speech_prob": 3.785308535952936e-06}, {"id": 333, "seek": 162496, "start": 1651.72, "end": 1654.4, "text": " not things we haven't.", "tokens": [406, 721, 321, 2378, 380, 13], "temperature": 0.0, "avg_logprob": -0.2377445502359359, "compression_ratio": 1.6730769230769231, "no_speech_prob": 3.785308535952936e-06}, {"id": 334, "seek": 165440, "start": 1654.4, "end": 1656.8400000000001, "text": " The short answer is no, I haven't tried it properly.", "tokens": [440, 2099, 1867, 307, 572, 11, 286, 2378, 380, 3031, 309, 6108, 13], "temperature": 0.0, "avg_logprob": -0.15387361699884589, "compression_ratio": 1.8923611111111112, "no_speech_prob": 3.071784522035159e-05}, {"id": 335, "seek": 165440, "start": 1656.8400000000001, "end": 1662.0400000000002, "text": " Yes, you should try it, because it might help.", "tokens": [1079, 11, 291, 820, 853, 309, 11, 570, 309, 1062, 854, 13], "temperature": 0.0, "avg_logprob": -0.15387361699884589, "compression_ratio": 1.8923611111111112, "no_speech_prob": 3.071784522035159e-05}, {"id": 336, "seek": 165440, "start": 1662.0400000000002, "end": 1666.4, "text": " In general, there's going to be a lot of things that we covered today, which if you've done", "tokens": [682, 2674, 11, 456, 311, 516, 281, 312, 257, 688, 295, 721, 300, 321, 5343, 965, 11, 597, 498, 291, 600, 1096], "temperature": 0.0, "avg_logprob": -0.15387361699884589, "compression_ratio": 1.8923611111111112, "no_speech_prob": 3.071784522035159e-05}, {"id": 337, "seek": 165440, "start": 1666.4, "end": 1670.52, "text": " some sequence-to-sequence stuff before, you'll want to know about something we haven't covered", "tokens": [512, 8310, 12, 1353, 12, 11834, 655, 1507, 949, 11, 291, 603, 528, 281, 458, 466, 746, 321, 2378, 380, 5343], "temperature": 0.0, "avg_logprob": -0.15387361699884589, "compression_ratio": 1.8923611111111112, "no_speech_prob": 3.071784522035159e-05}, {"id": 338, "seek": 165440, "start": 1670.52, "end": 1671.52, "text": " yet.", "tokens": [1939, 13], "temperature": 0.0, "avg_logprob": -0.15387361699884589, "compression_ratio": 1.8923611111111112, "no_speech_prob": 3.071784522035159e-05}, {"id": 339, "seek": 165440, "start": 1671.52, "end": 1673.52, "text": " I'm going to cover all the sequence-to-sequence things.", "tokens": [286, 478, 516, 281, 2060, 439, 264, 8310, 12, 1353, 12, 11834, 655, 721, 13], "temperature": 0.0, "avg_logprob": -0.15387361699884589, "compression_ratio": 1.8923611111111112, "no_speech_prob": 3.071784522035159e-05}, {"id": 340, "seek": 165440, "start": 1673.52, "end": 1677.1200000000001, "text": " So at the end of this, if I haven't covered the thing you wanted to know about, please", "tokens": [407, 412, 264, 917, 295, 341, 11, 498, 286, 2378, 380, 5343, 264, 551, 291, 1415, 281, 458, 466, 11, 1767], "temperature": 0.0, "avg_logprob": -0.15387361699884589, "compression_ratio": 1.8923611111111112, "no_speech_prob": 3.071784522035159e-05}, {"id": 341, "seek": 165440, "start": 1677.1200000000001, "end": 1678.6200000000001, "text": " ask me then.", "tokens": [1029, 385, 550, 13], "temperature": 0.0, "avg_logprob": -0.15387361699884589, "compression_ratio": 1.8923611111111112, "no_speech_prob": 3.071784522035159e-05}, {"id": 342, "seek": 165440, "start": 1678.6200000000001, "end": 1682.16, "text": " If you ask me before, I'll be answering something based on something that I'm about to teach", "tokens": [759, 291, 1029, 385, 949, 11, 286, 603, 312, 13430, 746, 2361, 322, 746, 300, 286, 478, 466, 281, 2924], "temperature": 0.0, "avg_logprob": -0.15387361699884589, "compression_ratio": 1.8923611111111112, "no_speech_prob": 3.071784522035159e-05}, {"id": 343, "seek": 165440, "start": 1682.16, "end": 1683.16, "text": " you.", "tokens": [291, 13], "temperature": 0.0, "avg_logprob": -0.15387361699884589, "compression_ratio": 1.8923611111111112, "no_speech_prob": 3.071784522035159e-05}, {"id": 344, "seek": 168316, "start": 1683.16, "end": 1690.3600000000001, "text": " Okay, so having tokenized the English and French, you can see how it gets spit out.", "tokens": [1033, 11, 370, 1419, 14862, 1602, 264, 3669, 293, 5522, 11, 291, 393, 536, 577, 309, 2170, 22127, 484, 13], "temperature": 0.0, "avg_logprob": -0.14715301990509033, "compression_ratio": 1.6733067729083666, "no_speech_prob": 2.1111214664415456e-05}, {"id": 345, "seek": 168316, "start": 1690.3600000000001, "end": 1694.8000000000002, "text": " And you can see that tokenization for French is quite different looking, because French", "tokens": [400, 291, 393, 536, 300, 14862, 2144, 337, 5522, 307, 1596, 819, 1237, 11, 570, 5522], "temperature": 0.0, "avg_logprob": -0.14715301990509033, "compression_ratio": 1.6733067729083666, "no_speech_prob": 2.1111214664415456e-05}, {"id": 346, "seek": 168316, "start": 1694.8000000000002, "end": 1698.02, "text": " loves their apostrophes and their hyphens and stuff.", "tokens": [6752, 641, 19484, 11741, 279, 293, 641, 2477, 950, 694, 293, 1507, 13], "temperature": 0.0, "avg_logprob": -0.14715301990509033, "compression_ratio": 1.6733067729083666, "no_speech_prob": 2.1111214664415456e-05}, {"id": 347, "seek": 168316, "start": 1698.02, "end": 1701.3200000000002, "text": " So if you try to use an English tokenizer for a French sentence, you're going to get", "tokens": [407, 498, 291, 853, 281, 764, 364, 3669, 14862, 6545, 337, 257, 5522, 8174, 11, 291, 434, 516, 281, 483], "temperature": 0.0, "avg_logprob": -0.14715301990509033, "compression_ratio": 1.6733067729083666, "no_speech_prob": 2.1111214664415456e-05}, {"id": 348, "seek": 168316, "start": 1701.3200000000002, "end": 1705.16, "text": " a pretty crappy outcome.", "tokens": [257, 1238, 36531, 9700, 13], "temperature": 0.0, "avg_logprob": -0.14715301990509033, "compression_ratio": 1.6733067729083666, "no_speech_prob": 2.1111214664415456e-05}, {"id": 349, "seek": 168316, "start": 1705.16, "end": 1711.28, "text": " So I don't find you need to know heaps of NLP ideas to use deep learning for NLP, but", "tokens": [407, 286, 500, 380, 915, 291, 643, 281, 458, 415, 2382, 295, 426, 45196, 3487, 281, 764, 2452, 2539, 337, 426, 45196, 11, 457], "temperature": 0.0, "avg_logprob": -0.14715301990509033, "compression_ratio": 1.6733067729083666, "no_speech_prob": 2.1111214664415456e-05}, {"id": 350, "seek": 171128, "start": 1711.28, "end": 1718.08, "text": " just some basic stuff like, use the right tokenizer for your language is important.", "tokens": [445, 512, 3875, 1507, 411, 11, 764, 264, 558, 14862, 6545, 337, 428, 2856, 307, 1021, 13], "temperature": 0.0, "avg_logprob": -0.15244419309828017, "compression_ratio": 1.6085106382978724, "no_speech_prob": 1.2218786650919355e-05}, {"id": 351, "seek": 171128, "start": 1718.08, "end": 1722.12, "text": " And so some of the students this week in our study group have been trying to build language", "tokens": [400, 370, 512, 295, 264, 1731, 341, 1243, 294, 527, 2979, 1594, 362, 668, 1382, 281, 1322, 2856], "temperature": 0.0, "avg_logprob": -0.15244419309828017, "compression_ratio": 1.6085106382978724, "no_speech_prob": 1.2218786650919355e-05}, {"id": 352, "seek": 171128, "start": 1722.12, "end": 1728.44, "text": " models for Chinese, for instance, which of course doesn't really have the concept of", "tokens": [5245, 337, 4649, 11, 337, 5197, 11, 597, 295, 1164, 1177, 380, 534, 362, 264, 3410, 295], "temperature": 0.0, "avg_logprob": -0.15244419309828017, "compression_ratio": 1.6085106382978724, "no_speech_prob": 1.2218786650919355e-05}, {"id": 353, "seek": 171128, "start": 1728.44, "end": 1730.48, "text": " a tokenizer in the same way.", "tokens": [257, 14862, 6545, 294, 264, 912, 636, 13], "temperature": 0.0, "avg_logprob": -0.15244419309828017, "compression_ratio": 1.6085106382978724, "no_speech_prob": 1.2218786650919355e-05}, {"id": 354, "seek": 171128, "start": 1730.48, "end": 1735.08, "text": " So we've been starting to look at, briefly mentioned last week, this Google thing called", "tokens": [407, 321, 600, 668, 2891, 281, 574, 412, 11, 10515, 2835, 1036, 1243, 11, 341, 3329, 551, 1219], "temperature": 0.0, "avg_logprob": -0.15244419309828017, "compression_ratio": 1.6085106382978724, "no_speech_prob": 1.2218786650919355e-05}, {"id": 355, "seek": 173508, "start": 1735.08, "end": 1741.08, "text": " sentence piece, which basically spits things into arbitrary subword units.", "tokens": [8174, 2522, 11, 597, 1936, 637, 1208, 721, 666, 23211, 1422, 7462, 6815, 13], "temperature": 0.0, "avg_logprob": -0.14783227708604602, "compression_ratio": 1.5975103734439835, "no_speech_prob": 1.7777969333110377e-05}, {"id": 356, "seek": 173508, "start": 1741.08, "end": 1746.8799999999999, "text": " And so when I say tokenize, if you're using a language that doesn't have spaces in, you", "tokens": [400, 370, 562, 286, 584, 14862, 1125, 11, 498, 291, 434, 1228, 257, 2856, 300, 1177, 380, 362, 7673, 294, 11, 291], "temperature": 0.0, "avg_logprob": -0.14783227708604602, "compression_ratio": 1.5975103734439835, "no_speech_prob": 1.7777969333110377e-05}, {"id": 357, "seek": 173508, "start": 1746.8799999999999, "end": 1753.84, "text": " should probably be checking out sentence piece or some other similar subword unit thing instead.", "tokens": [820, 1391, 312, 8568, 484, 8174, 2522, 420, 512, 661, 2531, 1422, 7462, 4985, 551, 2602, 13], "temperature": 0.0, "avg_logprob": -0.14783227708604602, "compression_ratio": 1.5975103734439835, "no_speech_prob": 1.7777969333110377e-05}, {"id": 358, "seek": 173508, "start": 1753.84, "end": 1757.78, "text": " And hopefully in the next week or two we'll be able to report back with some early results", "tokens": [400, 4696, 294, 264, 958, 1243, 420, 732, 321, 603, 312, 1075, 281, 2275, 646, 365, 512, 2440, 3542], "temperature": 0.0, "avg_logprob": -0.14783227708604602, "compression_ratio": 1.5975103734439835, "no_speech_prob": 1.7777969333110377e-05}, {"id": 359, "seek": 173508, "start": 1757.78, "end": 1763.08, "text": " of these experiments with Chinese.", "tokens": [295, 613, 12050, 365, 4649, 13], "temperature": 0.0, "avg_logprob": -0.14783227708604602, "compression_ratio": 1.5975103734439835, "no_speech_prob": 1.7777969333110377e-05}, {"id": 360, "seek": 176308, "start": 1763.08, "end": 1767.96, "text": " So having tokenized it, we'll save that to disk.", "tokens": [407, 1419, 14862, 1602, 309, 11, 321, 603, 3155, 300, 281, 12355, 13], "temperature": 0.0, "avg_logprob": -0.15296767050759835, "compression_ratio": 1.7782608695652173, "no_speech_prob": 1.5206441275950056e-05}, {"id": 361, "seek": 176308, "start": 1767.96, "end": 1773.1999999999998, "text": " And then remember the next step after we create tokens is to turn them into numbers.", "tokens": [400, 550, 1604, 264, 958, 1823, 934, 321, 1884, 22667, 307, 281, 1261, 552, 666, 3547, 13], "temperature": 0.0, "avg_logprob": -0.15296767050759835, "compression_ratio": 1.7782608695652173, "no_speech_prob": 1.5206441275950056e-05}, {"id": 362, "seek": 176308, "start": 1773.1999999999998, "end": 1775.34, "text": " And to turn them into numbers, we have two steps.", "tokens": [400, 281, 1261, 552, 666, 3547, 11, 321, 362, 732, 4439, 13], "temperature": 0.0, "avg_logprob": -0.15296767050759835, "compression_ratio": 1.7782608695652173, "no_speech_prob": 1.5206441275950056e-05}, {"id": 363, "seek": 176308, "start": 1775.34, "end": 1780.96, "text": " The first is to get a list of all of the words that appear, and then we turn every word into", "tokens": [440, 700, 307, 281, 483, 257, 1329, 295, 439, 295, 264, 2283, 300, 4204, 11, 293, 550, 321, 1261, 633, 1349, 666], "temperature": 0.0, "avg_logprob": -0.15296767050759835, "compression_ratio": 1.7782608695652173, "no_speech_prob": 1.5206441275950056e-05}, {"id": 364, "seek": 176308, "start": 1780.96, "end": 1784.6399999999999, "text": " the index, into that list.", "tokens": [264, 8186, 11, 666, 300, 1329, 13], "temperature": 0.0, "avg_logprob": -0.15296767050759835, "compression_ratio": 1.7782608695652173, "no_speech_prob": 1.5206441275950056e-05}, {"id": 365, "seek": 176308, "start": 1784.6399999999999, "end": 1788.96, "text": " If there are more than 40,000 words that appear, then let's cut it off there so it doesn't", "tokens": [759, 456, 366, 544, 813, 3356, 11, 1360, 2283, 300, 4204, 11, 550, 718, 311, 1723, 309, 766, 456, 370, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.15296767050759835, "compression_ratio": 1.7782608695652173, "no_speech_prob": 1.5206441275950056e-05}, {"id": 366, "seek": 176308, "start": 1788.96, "end": 1791.32, "text": " get too crazy.", "tokens": [483, 886, 3219, 13], "temperature": 0.0, "avg_logprob": -0.15296767050759835, "compression_ratio": 1.7782608695652173, "no_speech_prob": 1.5206441275950056e-05}, {"id": 367, "seek": 179132, "start": 1791.32, "end": 1800.96, "text": " And we insert a few extra tokens for beginning of stream, padding, end of stream, and unknown.", "tokens": [400, 321, 8969, 257, 1326, 2857, 22667, 337, 2863, 295, 4309, 11, 39562, 11, 917, 295, 4309, 11, 293, 9841, 13], "temperature": 0.0, "avg_logprob": -0.08688444676606552, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.3405253816454206e-06}, {"id": 368, "seek": 179132, "start": 1800.96, "end": 1806.6799999999998, "text": " So if we try to look up something that wasn't in the 40,000 most common, then we use a default", "tokens": [407, 498, 321, 853, 281, 574, 493, 746, 300, 2067, 380, 294, 264, 3356, 11, 1360, 881, 2689, 11, 550, 321, 764, 257, 7576], "temperature": 0.0, "avg_logprob": -0.08688444676606552, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.3405253816454206e-06}, {"id": 369, "seek": 179132, "start": 1806.6799999999998, "end": 1811.6799999999998, "text": " dict to return 3, which is unknown.", "tokens": [12569, 281, 2736, 805, 11, 597, 307, 9841, 13], "temperature": 0.0, "avg_logprob": -0.08688444676606552, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.3405253816454206e-06}, {"id": 370, "seek": 179132, "start": 1811.6799999999998, "end": 1817.28, "text": " So now we can go ahead and turn every token into an id by putting it through the string", "tokens": [407, 586, 321, 393, 352, 2286, 293, 1261, 633, 14862, 666, 364, 4496, 538, 3372, 309, 807, 264, 6798], "temperature": 0.0, "avg_logprob": -0.08688444676606552, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.3405253816454206e-06}, {"id": 371, "seek": 179132, "start": 1817.28, "end": 1820.9199999999998, "text": " to integer dictionary we just created.", "tokens": [281, 24922, 25890, 321, 445, 2942, 13], "temperature": 0.0, "avg_logprob": -0.08688444676606552, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.3405253816454206e-06}, {"id": 372, "seek": 182092, "start": 1820.92, "end": 1825.8000000000002, "text": " And then at the end of that, let's add the number 2, which is end of stream.", "tokens": [400, 550, 412, 264, 917, 295, 300, 11, 718, 311, 909, 264, 1230, 568, 11, 597, 307, 917, 295, 4309, 13], "temperature": 0.0, "avg_logprob": -0.1468278655299434, "compression_ratio": 1.8214285714285714, "no_speech_prob": 2.3923288608784787e-05}, {"id": 373, "seek": 182092, "start": 1825.8000000000002, "end": 1833.64, "text": " And you'll see the code you see here is the code I write when I'm iterating and experimenting.", "tokens": [400, 291, 603, 536, 264, 3089, 291, 536, 510, 307, 264, 3089, 286, 2464, 562, 286, 478, 17138, 990, 293, 29070, 13], "temperature": 0.0, "avg_logprob": -0.1468278655299434, "compression_ratio": 1.8214285714285714, "no_speech_prob": 2.3923288608784787e-05}, {"id": 374, "seek": 182092, "start": 1833.64, "end": 1838.52, "text": " Because 99% of the code I write when I'm iterating and experimenting turns out to be totally", "tokens": [1436, 11803, 4, 295, 264, 3089, 286, 2464, 562, 286, 478, 17138, 990, 293, 29070, 4523, 484, 281, 312, 3879], "temperature": 0.0, "avg_logprob": -0.1468278655299434, "compression_ratio": 1.8214285714285714, "no_speech_prob": 2.3923288608784787e-05}, {"id": 375, "seek": 182092, "start": 1838.52, "end": 1842.68, "text": " wrong or stupid or embarrassing and you don't get to see it.", "tokens": [2085, 420, 6631, 420, 17299, 293, 291, 500, 380, 483, 281, 536, 309, 13], "temperature": 0.0, "avg_logprob": -0.1468278655299434, "compression_ratio": 1.8214285714285714, "no_speech_prob": 2.3923288608784787e-05}, {"id": 376, "seek": 182092, "start": 1842.68, "end": 1849.4, "text": " But there's no point refactoring that and making it beautiful when I'm writing it.", "tokens": [583, 456, 311, 572, 935, 1895, 578, 3662, 300, 293, 1455, 309, 2238, 562, 286, 478, 3579, 309, 13], "temperature": 0.0, "avg_logprob": -0.1468278655299434, "compression_ratio": 1.8214285714285714, "no_speech_prob": 2.3923288608784787e-05}, {"id": 377, "seek": 184940, "start": 1849.4, "end": 1852.6000000000001, "text": " I'm wanting you to see all the little shortcuts I have.", "tokens": [286, 478, 7935, 291, 281, 536, 439, 264, 707, 34620, 286, 362, 13], "temperature": 0.0, "avg_logprob": -0.17540802190333238, "compression_ratio": 1.5240384615384615, "no_speech_prob": 5.255370069789933e-06}, {"id": 378, "seek": 184940, "start": 1852.6000000000001, "end": 1857.2, "text": " So rather than doing this properly and actually having some constant or something for end", "tokens": [407, 2831, 813, 884, 341, 6108, 293, 767, 1419, 512, 5754, 420, 746, 337, 917], "temperature": 0.0, "avg_logprob": -0.17540802190333238, "compression_ratio": 1.5240384615384615, "no_speech_prob": 5.255370069789933e-06}, {"id": 379, "seek": 184940, "start": 1857.2, "end": 1864.5600000000002, "text": " of stream marker and using it, when I'm prototyping, I just do the easy stuff.", "tokens": [295, 4309, 15247, 293, 1228, 309, 11, 562, 286, 478, 46219, 3381, 11, 286, 445, 360, 264, 1858, 1507, 13], "temperature": 0.0, "avg_logprob": -0.17540802190333238, "compression_ratio": 1.5240384615384615, "no_speech_prob": 5.255370069789933e-06}, {"id": 380, "seek": 184940, "start": 1864.5600000000002, "end": 1874.8000000000002, "text": " I mean not so much that I end up with broken code, but I try to find some mid-ground between", "tokens": [286, 914, 406, 370, 709, 300, 286, 917, 493, 365, 5463, 3089, 11, 457, 286, 853, 281, 915, 512, 2062, 12, 2921, 1296], "temperature": 0.0, "avg_logprob": -0.17540802190333238, "compression_ratio": 1.5240384615384615, "no_speech_prob": 5.255370069789933e-06}, {"id": 381, "seek": 187480, "start": 1874.8, "end": 1880.04, "text": " beautiful code and code that works.", "tokens": [2238, 3089, 293, 3089, 300, 1985, 13], "temperature": 0.0, "avg_logprob": -0.22467305925157335, "compression_ratio": 1.6398467432950192, "no_speech_prob": 1.6187334040296264e-05}, {"id": 382, "seek": 187480, "start": 1880.04, "end": 1884.8, "text": " Just heard him mention that we divide number of CPUs by 2 because with hyperthreading, we", "tokens": [1449, 2198, 796, 2152, 300, 321, 9845, 1230, 295, 13199, 82, 538, 568, 570, 365, 9848, 392, 35908, 11, 321], "temperature": 0.0, "avg_logprob": -0.22467305925157335, "compression_ratio": 1.6398467432950192, "no_speech_prob": 1.6187334040296264e-05}, {"id": 383, "seek": 187480, "start": 1884.8, "end": 1887.82, "text": " don't get a speedup using all the hyperthreaded cores.", "tokens": [500, 380, 483, 257, 3073, 1010, 1228, 439, 264, 9848, 392, 2538, 292, 24826, 13], "temperature": 0.0, "avg_logprob": -0.22467305925157335, "compression_ratio": 1.6398467432950192, "no_speech_prob": 1.6187334040296264e-05}, {"id": 384, "seek": 187480, "start": 1887.82, "end": 1891.68, "text": " Is this based on practical experience or is there some underlying reason why we wouldn't", "tokens": [1119, 341, 2361, 322, 8496, 1752, 420, 307, 456, 512, 14217, 1778, 983, 321, 2759, 380], "temperature": 0.0, "avg_logprob": -0.22467305925157335, "compression_ratio": 1.6398467432950192, "no_speech_prob": 1.6187334040296264e-05}, {"id": 385, "seek": 187480, "start": 1891.68, "end": 1893.0, "text": " get additional speedup?", "tokens": [483, 4497, 3073, 1010, 30], "temperature": 0.0, "avg_logprob": -0.22467305925157335, "compression_ratio": 1.6398467432950192, "no_speech_prob": 1.6187334040296264e-05}, {"id": 386, "seek": 187480, "start": 1893.0, "end": 1895.58, "text": " Yeah, it's just practical experience.", "tokens": [865, 11, 309, 311, 445, 8496, 1752, 13], "temperature": 0.0, "avg_logprob": -0.22467305925157335, "compression_ratio": 1.6398467432950192, "no_speech_prob": 1.6187334040296264e-05}, {"id": 387, "seek": 187480, "start": 1895.58, "end": 1901.8, "text": " And it's like not all things kind of seem like this, but I definitely noticed with tokenization,", "tokens": [400, 309, 311, 411, 406, 439, 721, 733, 295, 1643, 411, 341, 11, 457, 286, 2138, 5694, 365, 14862, 2144, 11], "temperature": 0.0, "avg_logprob": -0.22467305925157335, "compression_ratio": 1.6398467432950192, "no_speech_prob": 1.6187334040296264e-05}, {"id": 388, "seek": 190180, "start": 1901.8, "end": 1905.04, "text": " hyperthreading seemed to slow things down a little bit.", "tokens": [9848, 392, 35908, 6576, 281, 2964, 721, 760, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.185035635571961, "compression_ratio": 1.592885375494071, "no_speech_prob": 1.952556885953527e-05}, {"id": 389, "seek": 190180, "start": 1905.04, "end": 1910.8, "text": " Also if I use all the cores, like often I want to do something else at the same time,", "tokens": [2743, 498, 286, 764, 439, 264, 24826, 11, 411, 2049, 286, 528, 281, 360, 746, 1646, 412, 264, 912, 565, 11], "temperature": 0.0, "avg_logprob": -0.185035635571961, "compression_ratio": 1.592885375494071, "no_speech_prob": 1.952556885953527e-05}, {"id": 390, "seek": 190180, "start": 1910.8, "end": 1917.12, "text": " I generally run some interactive notebook and I don't have any spare room to do that.", "tokens": [286, 5101, 1190, 512, 15141, 21060, 293, 286, 500, 380, 362, 604, 13798, 1808, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.185035635571961, "compression_ratio": 1.592885375494071, "no_speech_prob": 1.952556885953527e-05}, {"id": 391, "seek": 190180, "start": 1917.12, "end": 1923.1599999999999, "text": " It's a minor issue.", "tokens": [467, 311, 257, 6696, 2734, 13], "temperature": 0.0, "avg_logprob": -0.185035635571961, "compression_ratio": 1.592885375494071, "no_speech_prob": 1.952556885953527e-05}, {"id": 392, "seek": 190180, "start": 1923.1599999999999, "end": 1927.84, "text": " So now for our English and our French, we can grab our list of IDs.", "tokens": [407, 586, 337, 527, 3669, 293, 527, 5522, 11, 321, 393, 4444, 527, 1329, 295, 48212, 13], "temperature": 0.0, "avg_logprob": -0.185035635571961, "compression_ratio": 1.592885375494071, "no_speech_prob": 1.952556885953527e-05}, {"id": 393, "seek": 190180, "start": 1927.84, "end": 1931.6399999999999, "text": " And when we do that, of course, we need to make sure that we also store the vocabulary.", "tokens": [400, 562, 321, 360, 300, 11, 295, 1164, 11, 321, 643, 281, 652, 988, 300, 321, 611, 3531, 264, 19864, 13], "temperature": 0.0, "avg_logprob": -0.185035635571961, "compression_ratio": 1.592885375494071, "no_speech_prob": 1.952556885953527e-05}, {"id": 394, "seek": 193164, "start": 1931.64, "end": 1935.8400000000001, "text": " There's no point having IDs if we don't know what the number 5 represents.", "tokens": [821, 311, 572, 935, 1419, 48212, 498, 321, 500, 380, 458, 437, 264, 1230, 1025, 8855, 13], "temperature": 0.0, "avg_logprob": -0.20951890108878152, "compression_ratio": 1.7405857740585775, "no_speech_prob": 5.828847861266695e-05}, {"id": 395, "seek": 193164, "start": 1935.8400000000001, "end": 1937.64, "text": " There's no point having a number 5.", "tokens": [821, 311, 572, 935, 1419, 257, 1230, 1025, 13], "temperature": 0.0, "avg_logprob": -0.20951890108878152, "compression_ratio": 1.7405857740585775, "no_speech_prob": 5.828847861266695e-05}, {"id": 396, "seek": 193164, "start": 1937.64, "end": 1944.0, "text": " So that's our vocabulary, and the reverse mapping string to int that we can use to convert", "tokens": [407, 300, 311, 527, 19864, 11, 293, 264, 9943, 18350, 6798, 281, 560, 300, 321, 393, 764, 281, 7620], "temperature": 0.0, "avg_logprob": -0.20951890108878152, "compression_ratio": 1.7405857740585775, "no_speech_prob": 5.828847861266695e-05}, {"id": 397, "seek": 193164, "start": 1944.0, "end": 1948.38, "text": " more corpuses in the future.", "tokens": [544, 1181, 79, 8355, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.20951890108878152, "compression_ratio": 1.7405857740585775, "no_speech_prob": 5.828847861266695e-05}, {"id": 398, "seek": 193164, "start": 1948.38, "end": 1953.48, "text": " So just to confirm it's working, we can go through each ID, convert the int to a string", "tokens": [407, 445, 281, 9064, 309, 311, 1364, 11, 321, 393, 352, 807, 1184, 7348, 11, 7620, 264, 560, 281, 257, 6798], "temperature": 0.0, "avg_logprob": -0.20951890108878152, "compression_ratio": 1.7405857740585775, "no_speech_prob": 5.828847861266695e-05}, {"id": 399, "seek": 193164, "start": 1953.48, "end": 1957.96, "text": " and spit that out, and there we have our thing back, now with an end of stream marker at", "tokens": [293, 22127, 300, 484, 11, 293, 456, 321, 362, 527, 551, 646, 11, 586, 365, 364, 917, 295, 4309, 15247, 412], "temperature": 0.0, "avg_logprob": -0.20951890108878152, "compression_ratio": 1.7405857740585775, "no_speech_prob": 5.828847861266695e-05}, {"id": 400, "seek": 193164, "start": 1957.96, "end": 1960.4, "text": " the end.", "tokens": [264, 917, 13], "temperature": 0.0, "avg_logprob": -0.20951890108878152, "compression_ratio": 1.7405857740585775, "no_speech_prob": 5.828847861266695e-05}, {"id": 401, "seek": 196040, "start": 1960.4, "end": 1964.92, "text": " Our English vocab is 17000, our French vocab is 25000.", "tokens": [2621, 3669, 2329, 455, 307, 3282, 1360, 11, 527, 5522, 2329, 455, 307, 3552, 1360, 13], "temperature": 0.0, "avg_logprob": -0.19052769439389008, "compression_ratio": 1.5198237885462555, "no_speech_prob": 1.618737223907374e-05}, {"id": 402, "seek": 196040, "start": 1964.92, "end": 1971.5600000000002, "text": " So there's not too big, not too complex a vocab that we're dealing with, which is nice", "tokens": [407, 456, 311, 406, 886, 955, 11, 406, 886, 3997, 257, 2329, 455, 300, 321, 434, 6260, 365, 11, 597, 307, 1481], "temperature": 0.0, "avg_logprob": -0.19052769439389008, "compression_ratio": 1.5198237885462555, "no_speech_prob": 1.618737223907374e-05}, {"id": 403, "seek": 196040, "start": 1971.5600000000002, "end": 1974.88, "text": " to know.", "tokens": [281, 458, 13], "temperature": 0.0, "avg_logprob": -0.19052769439389008, "compression_ratio": 1.5198237885462555, "no_speech_prob": 1.618737223907374e-05}, {"id": 404, "seek": 196040, "start": 1974.88, "end": 1981.0800000000002, "text": " So we spent a lot of time on the forums during the week discussing how pointless word vectors", "tokens": [407, 321, 4418, 257, 688, 295, 565, 322, 264, 26998, 1830, 264, 1243, 10850, 577, 32824, 1349, 18875], "temperature": 0.0, "avg_logprob": -0.19052769439389008, "compression_ratio": 1.5198237885462555, "no_speech_prob": 1.618737223907374e-05}, {"id": 405, "seek": 196040, "start": 1981.0800000000002, "end": 1983.92, "text": " are and how you should stop getting so excited about them.", "tokens": [366, 293, 577, 291, 820, 1590, 1242, 370, 2919, 466, 552, 13], "temperature": 0.0, "avg_logprob": -0.19052769439389008, "compression_ratio": 1.5198237885462555, "no_speech_prob": 1.618737223907374e-05}, {"id": 406, "seek": 196040, "start": 1983.92, "end": 1986.48, "text": " We're now going to use them.", "tokens": [492, 434, 586, 516, 281, 764, 552, 13], "temperature": 0.0, "avg_logprob": -0.19052769439389008, "compression_ratio": 1.5198237885462555, "no_speech_prob": 1.618737223907374e-05}, {"id": 407, "seek": 196040, "start": 1986.48, "end": 1988.24, "text": " Why is that?", "tokens": [1545, 307, 300, 30], "temperature": 0.0, "avg_logprob": -0.19052769439389008, "compression_ratio": 1.5198237885462555, "no_speech_prob": 1.618737223907374e-05}, {"id": 408, "seek": 198824, "start": 1988.24, "end": 1993.44, "text": " Actually all the stuff we've been learning about using language models and pre-trained", "tokens": [5135, 439, 264, 1507, 321, 600, 668, 2539, 466, 1228, 2856, 5245, 293, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.18173943733682438, "compression_ratio": 1.6408163265306122, "no_speech_prob": 2.111170942953322e-05}, {"id": 409, "seek": 198824, "start": 1993.44, "end": 1998.24, "text": " proper models rather than pre-trained linear single layers, which is what word vectors", "tokens": [2296, 5245, 2831, 813, 659, 12, 17227, 2001, 8213, 2167, 7914, 11, 597, 307, 437, 1349, 18875], "temperature": 0.0, "avg_logprob": -0.18173943733682438, "compression_ratio": 1.6408163265306122, "no_speech_prob": 2.111170942953322e-05}, {"id": 410, "seek": 198824, "start": 1998.24, "end": 2005.56, "text": " are, I think applies equally well to sequence to sequence, but I haven't tried it yet.", "tokens": [366, 11, 286, 519, 13165, 12309, 731, 281, 8310, 281, 8310, 11, 457, 286, 2378, 380, 3031, 309, 1939, 13], "temperature": 0.0, "avg_logprob": -0.18173943733682438, "compression_ratio": 1.6408163265306122, "no_speech_prob": 2.111170942953322e-05}, {"id": 411, "seek": 198824, "start": 2005.56, "end": 2009.6200000000001, "text": " So Sebastian and I are starting to look at that.", "tokens": [407, 31102, 293, 286, 366, 2891, 281, 574, 412, 300, 13], "temperature": 0.0, "avg_logprob": -0.18173943733682438, "compression_ratio": 1.6408163265306122, "no_speech_prob": 2.111170942953322e-05}, {"id": 412, "seek": 198824, "start": 2009.6200000000001, "end": 2013.1200000000001, "text": " I'm slightly distracted by preparing this class at the moment, but after this class", "tokens": [286, 478, 4748, 21658, 538, 10075, 341, 1508, 412, 264, 1623, 11, 457, 934, 341, 1508], "temperature": 0.0, "avg_logprob": -0.18173943733682438, "compression_ratio": 1.6408163265306122, "no_speech_prob": 2.111170942953322e-05}, {"id": 413, "seek": 198824, "start": 2013.1200000000001, "end": 2014.1200000000001, "text": " is done.", "tokens": [307, 1096, 13], "temperature": 0.0, "avg_logprob": -0.18173943733682438, "compression_ratio": 1.6408163265306122, "no_speech_prob": 2.111170942953322e-05}, {"id": 414, "seek": 201412, "start": 2014.12, "end": 2021.52, "text": " So there's a whole thing, for anybody interested in creating some genuinely new, highly publishable", "tokens": [407, 456, 311, 257, 1379, 551, 11, 337, 4472, 3102, 294, 4084, 512, 17839, 777, 11, 5405, 11374, 712], "temperature": 0.0, "avg_logprob": -0.12323208742363509, "compression_ratio": 1.5684647302904564, "no_speech_prob": 2.2472744603874162e-05}, {"id": 415, "seek": 201412, "start": 2021.52, "end": 2028.1999999999998, "text": " results, the entire area of sequence to sequence with pre-trained language models hasn't been", "tokens": [3542, 11, 264, 2302, 1859, 295, 8310, 281, 8310, 365, 659, 12, 17227, 2001, 2856, 5245, 6132, 380, 668], "temperature": 0.0, "avg_logprob": -0.12323208742363509, "compression_ratio": 1.5684647302904564, "no_speech_prob": 2.2472744603874162e-05}, {"id": 416, "seek": 201412, "start": 2028.1999999999998, "end": 2034.84, "text": " touched yet, and I strongly believe it's going to be just as good as classification stuff.", "tokens": [9828, 1939, 11, 293, 286, 10613, 1697, 309, 311, 516, 281, 312, 445, 382, 665, 382, 21538, 1507, 13], "temperature": 0.0, "avg_logprob": -0.12323208742363509, "compression_ratio": 1.5684647302904564, "no_speech_prob": 2.2472744603874162e-05}, {"id": 417, "seek": 201412, "start": 2034.84, "end": 2040.9199999999998, "text": " If you work on this and you get to the point where you have something that's looking exciting", "tokens": [759, 291, 589, 322, 341, 293, 291, 483, 281, 264, 935, 689, 291, 362, 746, 300, 311, 1237, 4670], "temperature": 0.0, "avg_logprob": -0.12323208742363509, "compression_ratio": 1.5684647302904564, "no_speech_prob": 2.2472744603874162e-05}, {"id": 418, "seek": 204092, "start": 2040.92, "end": 2048.2000000000003, "text": " and you want help publishing it, I'm very happy to help co-author papers on stuff that's", "tokens": [293, 291, 528, 854, 17832, 309, 11, 286, 478, 588, 2055, 281, 854, 598, 12, 34224, 10577, 322, 1507, 300, 311], "temperature": 0.0, "avg_logprob": -0.140763978397145, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.169244882883504e-05}, {"id": 419, "seek": 204092, "start": 2048.2000000000003, "end": 2050.04, "text": " looking good.", "tokens": [1237, 665, 13], "temperature": 0.0, "avg_logprob": -0.140763978397145, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.169244882883504e-05}, {"id": 420, "seek": 204092, "start": 2050.04, "end": 2055.08, "text": " So feel free to reach out if and when you have some interesting results.", "tokens": [407, 841, 1737, 281, 2524, 484, 498, 293, 562, 291, 362, 512, 1880, 3542, 13], "temperature": 0.0, "avg_logprob": -0.140763978397145, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.169244882883504e-05}, {"id": 421, "seek": 204092, "start": 2055.08, "end": 2062.6, "text": " So at this stage, we don't have any of that, so we're going to use very little fast AI", "tokens": [407, 412, 341, 3233, 11, 321, 500, 380, 362, 604, 295, 300, 11, 370, 321, 434, 516, 281, 764, 588, 707, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.140763978397145, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.169244882883504e-05}, {"id": 422, "seek": 204092, "start": 2062.6, "end": 2067.34, "text": " actually and very little in terms of fast AI ideas.", "tokens": [767, 293, 588, 707, 294, 2115, 295, 2370, 7318, 3487, 13], "temperature": 0.0, "avg_logprob": -0.140763978397145, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.169244882883504e-05}, {"id": 423, "seek": 206734, "start": 2067.34, "end": 2071.28, "text": " So all we've got is word vectors.", "tokens": [407, 439, 321, 600, 658, 307, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.15208693383966834, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.2805165169993415e-05}, {"id": 424, "seek": 206734, "start": 2071.28, "end": 2073.92, "text": " Anyway, so let's at least use decent word vectors.", "tokens": [5684, 11, 370, 718, 311, 412, 1935, 764, 8681, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.15208693383966834, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.2805165169993415e-05}, {"id": 425, "seek": 206734, "start": 2073.92, "end": 2076.92, "text": " So word2vec is very old word vectors.", "tokens": [407, 1349, 17, 303, 66, 307, 588, 1331, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.15208693383966834, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.2805165169993415e-05}, {"id": 426, "seek": 206734, "start": 2076.92, "end": 2082.04, "text": " There are better word vectors now, and fast text is a pretty good source of word vectors.", "tokens": [821, 366, 1101, 1349, 18875, 586, 11, 293, 2370, 2487, 307, 257, 1238, 665, 4009, 295, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.15208693383966834, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.2805165169993415e-05}, {"id": 427, "seek": 206734, "start": 2082.04, "end": 2084.28, "text": " There's hundreds of languages available for them.", "tokens": [821, 311, 6779, 295, 8650, 2435, 337, 552, 13], "temperature": 0.0, "avg_logprob": -0.15208693383966834, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.2805165169993415e-05}, {"id": 428, "seek": 206734, "start": 2084.28, "end": 2087.1400000000003, "text": " Your language is likely to be represented.", "tokens": [2260, 2856, 307, 3700, 281, 312, 10379, 13], "temperature": 0.0, "avg_logprob": -0.15208693383966834, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.2805165169993415e-05}, {"id": 429, "seek": 206734, "start": 2087.1400000000003, "end": 2092.1200000000003, "text": " So to grab them, you can click on this link, download word vectors for a language that", "tokens": [407, 281, 4444, 552, 11, 291, 393, 2052, 322, 341, 2113, 11, 5484, 1349, 18875, 337, 257, 2856, 300], "temperature": 0.0, "avg_logprob": -0.15208693383966834, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.2805165169993415e-05}, {"id": 430, "seek": 209212, "start": 2092.12, "end": 2101.08, "text": " you're interested in, install the fast text Python library.", "tokens": [291, 434, 3102, 294, 11, 3625, 264, 2370, 2487, 15329, 6405, 13], "temperature": 0.0, "avg_logprob": -0.16451009889928306, "compression_ratio": 1.4948453608247423, "no_speech_prob": 5.594307822320843e-06}, {"id": 431, "seek": 209212, "start": 2101.08, "end": 2104.74, "text": " It's not available on PyPy, but here's a handy trick.", "tokens": [467, 311, 406, 2435, 322, 9953, 47, 88, 11, 457, 510, 311, 257, 13239, 4282, 13], "temperature": 0.0, "avg_logprob": -0.16451009889928306, "compression_ratio": 1.4948453608247423, "no_speech_prob": 5.594307822320843e-06}, {"id": 432, "seek": 209212, "start": 2104.74, "end": 2112.7599999999998, "text": " If there is a GitHub repo that has a setup.py in it and a requirements.txt in it, you can", "tokens": [759, 456, 307, 257, 23331, 49040, 300, 575, 257, 8657, 13, 8200, 294, 309, 293, 257, 7728, 13, 83, 734, 294, 309, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.16451009889928306, "compression_ratio": 1.4948453608247423, "no_speech_prob": 5.594307822320843e-06}, {"id": 433, "seek": 209212, "start": 2112.7599999999998, "end": 2120.04, "text": " just chuck git plus at the start and then stick that in your pip install and it works.", "tokens": [445, 20870, 18331, 1804, 412, 264, 722, 293, 550, 2897, 300, 294, 428, 8489, 3625, 293, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.16451009889928306, "compression_ratio": 1.4948453608247423, "no_speech_prob": 5.594307822320843e-06}, {"id": 434, "seek": 212004, "start": 2120.04, "end": 2122.8, "text": " Like hardly anybody seems to know this.", "tokens": [1743, 13572, 4472, 2544, 281, 458, 341, 13], "temperature": 0.0, "avg_logprob": -0.18505477905273438, "compression_ratio": 1.712686567164179, "no_speech_prob": 1.6964189853752032e-05}, {"id": 435, "seek": 212004, "start": 2122.8, "end": 2125.24, "text": " Like if you go to the fast text repo, they won't tell you this.", "tokens": [1743, 498, 291, 352, 281, 264, 2370, 2487, 49040, 11, 436, 1582, 380, 980, 291, 341, 13], "temperature": 0.0, "avg_logprob": -0.18505477905273438, "compression_ratio": 1.712686567164179, "no_speech_prob": 1.6964189853752032e-05}, {"id": 436, "seek": 212004, "start": 2125.24, "end": 2128.52, "text": " They'll say you have to download it and cd into it, blah blah blah, but you don't.", "tokens": [814, 603, 584, 291, 362, 281, 5484, 309, 293, 269, 67, 666, 309, 11, 12288, 12288, 12288, 11, 457, 291, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.18505477905273438, "compression_ratio": 1.712686567164179, "no_speech_prob": 1.6964189853752032e-05}, {"id": 437, "seek": 212004, "start": 2128.52, "end": 2130.72, "text": " You can just run that.", "tokens": [509, 393, 445, 1190, 300, 13], "temperature": 0.0, "avg_logprob": -0.18505477905273438, "compression_ratio": 1.712686567164179, "no_speech_prob": 1.6964189853752032e-05}, {"id": 438, "seek": 212004, "start": 2130.72, "end": 2133.68, "text": " Which you can also use for the fast AI library, by the way.", "tokens": [3013, 291, 393, 611, 764, 337, 264, 2370, 7318, 6405, 11, 538, 264, 636, 13], "temperature": 0.0, "avg_logprob": -0.18505477905273438, "compression_ratio": 1.712686567164179, "no_speech_prob": 1.6964189853752032e-05}, {"id": 439, "seek": 212004, "start": 2133.68, "end": 2139.12, "text": " If you want to pip install the latest version of fast AI, you can totally do this.", "tokens": [759, 291, 528, 281, 8489, 3625, 264, 6792, 3037, 295, 2370, 7318, 11, 291, 393, 3879, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.18505477905273438, "compression_ratio": 1.712686567164179, "no_speech_prob": 1.6964189853752032e-05}, {"id": 440, "seek": 212004, "start": 2139.12, "end": 2145.04, "text": " So you grab the library, import it, load the model, so here's my English model and here's", "tokens": [407, 291, 4444, 264, 6405, 11, 974, 309, 11, 3677, 264, 2316, 11, 370, 510, 311, 452, 3669, 2316, 293, 510, 311], "temperature": 0.0, "avg_logprob": -0.18505477905273438, "compression_ratio": 1.712686567164179, "no_speech_prob": 1.6964189853752032e-05}, {"id": 441, "seek": 212004, "start": 2145.04, "end": 2147.08, "text": " my French model.", "tokens": [452, 5522, 2316, 13], "temperature": 0.0, "avg_logprob": -0.18505477905273438, "compression_ratio": 1.712686567164179, "no_speech_prob": 1.6964189853752032e-05}, {"id": 442, "seek": 214708, "start": 2147.08, "end": 2150.72, "text": " So see there's a text version and a binary version, the binary version is a bit faster.", "tokens": [407, 536, 456, 311, 257, 2487, 3037, 293, 257, 17434, 3037, 11, 264, 17434, 3037, 307, 257, 857, 4663, 13], "temperature": 0.0, "avg_logprob": -0.18171726261173282, "compression_ratio": 1.8274336283185841, "no_speech_prob": 1.5206700481940061e-05}, {"id": 443, "seek": 214708, "start": 2150.72, "end": 2151.72, "text": " We're going to use that.", "tokens": [492, 434, 516, 281, 764, 300, 13], "temperature": 0.0, "avg_logprob": -0.18171726261173282, "compression_ratio": 1.8274336283185841, "no_speech_prob": 1.5206700481940061e-05}, {"id": 444, "seek": 214708, "start": 2151.72, "end": 2155.6, "text": " The text version is also a bit buggy.", "tokens": [440, 2487, 3037, 307, 611, 257, 857, 7426, 1480, 13], "temperature": 0.0, "avg_logprob": -0.18171726261173282, "compression_ratio": 1.8274336283185841, "no_speech_prob": 1.5206700481940061e-05}, {"id": 445, "seek": 214708, "start": 2155.6, "end": 2159.72, "text": " And then I'm going to convert it into a standard Python dictionary to make it a bit easier", "tokens": [400, 550, 286, 478, 516, 281, 7620, 309, 666, 257, 3832, 15329, 25890, 281, 652, 309, 257, 857, 3571], "temperature": 0.0, "avg_logprob": -0.18171726261173282, "compression_ratio": 1.8274336283185841, "no_speech_prob": 1.5206700481940061e-05}, {"id": 446, "seek": 214708, "start": 2159.72, "end": 2160.72, "text": " to work with.", "tokens": [281, 589, 365, 13], "temperature": 0.0, "avg_logprob": -0.18171726261173282, "compression_ratio": 1.8274336283185841, "no_speech_prob": 1.5206700481940061e-05}, {"id": 447, "seek": 214708, "start": 2160.72, "end": 2164.58, "text": " So this is just going to go through each word with a dictionary comprehension and save it", "tokens": [407, 341, 307, 445, 516, 281, 352, 807, 1184, 1349, 365, 257, 25890, 44991, 293, 3155, 309], "temperature": 0.0, "avg_logprob": -0.18171726261173282, "compression_ratio": 1.8274336283185841, "no_speech_prob": 1.5206700481940061e-05}, {"id": 448, "seek": 214708, "start": 2164.58, "end": 2167.84, "text": " as a Pickhold dictionary.", "tokens": [382, 257, 14129, 4104, 25890, 13], "temperature": 0.0, "avg_logprob": -0.18171726261173282, "compression_ratio": 1.8274336283185841, "no_speech_prob": 1.5206700481940061e-05}, {"id": 449, "seek": 214708, "start": 2167.84, "end": 2171.48, "text": " So now we've got our Pickhold dictionary.", "tokens": [407, 586, 321, 600, 658, 527, 14129, 4104, 25890, 13], "temperature": 0.0, "avg_logprob": -0.18171726261173282, "compression_ratio": 1.8274336283185841, "no_speech_prob": 1.5206700481940061e-05}, {"id": 450, "seek": 217148, "start": 2171.48, "end": 2177.68, "text": " We can go ahead and look up a word, for example, comma, and that will return a vector.", "tokens": [492, 393, 352, 2286, 293, 574, 493, 257, 1349, 11, 337, 1365, 11, 22117, 11, 293, 300, 486, 2736, 257, 8062, 13], "temperature": 0.0, "avg_logprob": -0.11748318819655586, "compression_ratio": 1.6933333333333334, "no_speech_prob": 1.11254739749711e-05}, {"id": 451, "seek": 217148, "start": 2177.68, "end": 2182.0, "text": " The length of that vector is the dimensionality of this set of word vectors.", "tokens": [440, 4641, 295, 300, 8062, 307, 264, 10139, 1860, 295, 341, 992, 295, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.11748318819655586, "compression_ratio": 1.6933333333333334, "no_speech_prob": 1.11254739749711e-05}, {"id": 452, "seek": 217148, "start": 2182.0, "end": 2192.0, "text": " So in this case we've got 300 dimensional English and French word vectors.", "tokens": [407, 294, 341, 1389, 321, 600, 658, 6641, 18795, 3669, 293, 5522, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.11748318819655586, "compression_ratio": 1.6933333333333334, "no_speech_prob": 1.11254739749711e-05}, {"id": 453, "seek": 217148, "start": 2192.0, "end": 2195.72, "text": " For reasons that you'll see in a moment, I also want to find out what the mean of my", "tokens": [1171, 4112, 300, 291, 603, 536, 294, 257, 1623, 11, 286, 611, 528, 281, 915, 484, 437, 264, 914, 295, 452], "temperature": 0.0, "avg_logprob": -0.11748318819655586, "compression_ratio": 1.6933333333333334, "no_speech_prob": 1.11254739749711e-05}, {"id": 454, "seek": 217148, "start": 2195.72, "end": 2198.72, "text": " vectors are and the standard deviation of my vectors are.", "tokens": [18875, 366, 293, 264, 3832, 25163, 295, 452, 18875, 366, 13], "temperature": 0.0, "avg_logprob": -0.11748318819655586, "compression_ratio": 1.6933333333333334, "no_speech_prob": 1.11254739749711e-05}, {"id": 455, "seek": 219872, "start": 2198.72, "end": 2209.08, "text": " So the mean is about 0 and the standard deviation is about 0.3.", "tokens": [407, 264, 914, 307, 466, 1958, 293, 264, 3832, 25163, 307, 466, 1958, 13, 18, 13], "temperature": 0.0, "avg_logprob": -0.15817720993705417, "compression_ratio": 1.5227272727272727, "no_speech_prob": 2.8573024337674724e-06}, {"id": 456, "seek": 219872, "start": 2209.08, "end": 2218.0, "text": " Often corpuses have a pretty long-tailed distribution of sequence length and it's the longest sequences", "tokens": [20043, 1181, 79, 8355, 362, 257, 1238, 938, 12, 14430, 292, 7316, 295, 8310, 4641, 293, 309, 311, 264, 15438, 22978], "temperature": 0.0, "avg_logprob": -0.15817720993705417, "compression_ratio": 1.5227272727272727, "no_speech_prob": 2.8573024337674724e-06}, {"id": 457, "seek": 219872, "start": 2218.0, "end": 2222.8799999999997, "text": " that kind of tend to overwhelm how long things take and how much memory is used and stuff", "tokens": [300, 733, 295, 3928, 281, 9103, 76, 577, 938, 721, 747, 293, 577, 709, 4675, 307, 1143, 293, 1507], "temperature": 0.0, "avg_logprob": -0.15817720993705417, "compression_ratio": 1.5227272727272727, "no_speech_prob": 2.8573024337674724e-06}, {"id": 458, "seek": 219872, "start": 2222.8799999999997, "end": 2224.3999999999996, "text": " like that.", "tokens": [411, 300, 13], "temperature": 0.0, "avg_logprob": -0.15817720993705417, "compression_ratio": 1.5227272727272727, "no_speech_prob": 2.8573024337674724e-06}, {"id": 459, "seek": 222440, "start": 2224.4, "end": 2232.0, "text": " So I'm going to grab, in this case, the 99th and 97th percentile of the English and French", "tokens": [407, 286, 478, 516, 281, 4444, 11, 294, 341, 1389, 11, 264, 11803, 392, 293, 23399, 392, 3043, 794, 295, 264, 3669, 293, 5522], "temperature": 0.0, "avg_logprob": -0.18486682573954263, "compression_ratio": 1.569377990430622, "no_speech_prob": 1.644231815589592e-05}, {"id": 460, "seek": 222440, "start": 2232.0, "end": 2236.0, "text": " and truncate them to that amount.", "tokens": [293, 504, 409, 66, 473, 552, 281, 300, 2372, 13], "temperature": 0.0, "avg_logprob": -0.18486682573954263, "compression_ratio": 1.569377990430622, "no_speech_prob": 1.644231815589592e-05}, {"id": 461, "seek": 222440, "start": 2236.0, "end": 2241.1600000000003, "text": " Originally I was using the 90th percentile, so these are poorly named variables.", "tokens": [28696, 286, 390, 1228, 264, 4289, 392, 3043, 794, 11, 370, 613, 366, 22271, 4926, 9102, 13], "temperature": 0.0, "avg_logprob": -0.18486682573954263, "compression_ratio": 1.569377990430622, "no_speech_prob": 1.644231815589592e-05}, {"id": 462, "seek": 222440, "start": 2241.1600000000003, "end": 2245.2000000000003, "text": " So that's just truncating them.", "tokens": [407, 300, 311, 445, 504, 409, 66, 990, 552, 13], "temperature": 0.0, "avg_logprob": -0.18486682573954263, "compression_ratio": 1.569377990430622, "no_speech_prob": 1.644231815589592e-05}, {"id": 463, "seek": 222440, "start": 2245.2000000000003, "end": 2246.36, "text": " So we're nearly there.", "tokens": [407, 321, 434, 6217, 456, 13], "temperature": 0.0, "avg_logprob": -0.18486682573954263, "compression_ratio": 1.569377990430622, "no_speech_prob": 1.644231815589592e-05}, {"id": 464, "seek": 222440, "start": 2246.36, "end": 2253.92, "text": " We've got our tokenized, numericalized English and French data set.", "tokens": [492, 600, 658, 527, 14862, 1602, 11, 29054, 1602, 3669, 293, 5522, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.18486682573954263, "compression_ratio": 1.569377990430622, "no_speech_prob": 1.644231815589592e-05}, {"id": 465, "seek": 225392, "start": 2253.92, "end": 2257.0, "text": " We've got some word vectors.", "tokens": [492, 600, 658, 512, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.16710311889648438, "compression_ratio": 1.7458333333333333, "no_speech_prob": 1.6187404980883002e-05}, {"id": 466, "seek": 225392, "start": 2257.0, "end": 2259.92, "text": " So now we need to get it ready for PyTorch.", "tokens": [407, 586, 321, 643, 281, 483, 309, 1919, 337, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.16710311889648438, "compression_ratio": 1.7458333333333333, "no_speech_prob": 1.6187404980883002e-05}, {"id": 467, "seek": 225392, "start": 2259.92, "end": 2267.76, "text": " So PyTorch expects a data set object and hopefully by now you all can tell me that a data set", "tokens": [407, 9953, 51, 284, 339, 33280, 257, 1412, 992, 2657, 293, 4696, 538, 586, 291, 439, 393, 980, 385, 300, 257, 1412, 992], "temperature": 0.0, "avg_logprob": -0.16710311889648438, "compression_ratio": 1.7458333333333333, "no_speech_prob": 1.6187404980883002e-05}, {"id": 468, "seek": 225392, "start": 2267.76, "end": 2273.3, "text": " object requires two things, a length and an indexer.", "tokens": [2657, 7029, 732, 721, 11, 257, 4641, 293, 364, 8186, 260, 13], "temperature": 0.0, "avg_logprob": -0.16710311889648438, "compression_ratio": 1.7458333333333333, "no_speech_prob": 1.6187404980883002e-05}, {"id": 469, "seek": 225392, "start": 2273.3, "end": 2276.36, "text": " So I started out writing this and I was like, okay, I need a sec to sec data set.", "tokens": [407, 286, 1409, 484, 3579, 341, 293, 286, 390, 411, 11, 1392, 11, 286, 643, 257, 907, 281, 907, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.16710311889648438, "compression_ratio": 1.7458333333333333, "no_speech_prob": 1.6187404980883002e-05}, {"id": 470, "seek": 225392, "start": 2276.36, "end": 2279.96, "text": " I started out writing it and I thought, okay, we're going to have to pass it our x's and", "tokens": [286, 1409, 484, 3579, 309, 293, 286, 1194, 11, 1392, 11, 321, 434, 516, 281, 362, 281, 1320, 309, 527, 2031, 311, 293], "temperature": 0.0, "avg_logprob": -0.16710311889648438, "compression_ratio": 1.7458333333333333, "no_speech_prob": 1.6187404980883002e-05}, {"id": 471, "seek": 225392, "start": 2279.96, "end": 2282.48, "text": " our y's and store them away.", "tokens": [527, 288, 311, 293, 3531, 552, 1314, 13], "temperature": 0.0, "avg_logprob": -0.16710311889648438, "compression_ratio": 1.7458333333333333, "no_speech_prob": 1.6187404980883002e-05}, {"id": 472, "seek": 228248, "start": 2282.48, "end": 2288.48, "text": " And then my indexer is going to need to return a numpy array of the x's at that point and", "tokens": [400, 550, 452, 8186, 260, 307, 516, 281, 643, 281, 2736, 257, 1031, 8200, 10225, 295, 264, 2031, 311, 412, 300, 935, 293], "temperature": 0.0, "avg_logprob": -0.1426900965826852, "compression_ratio": 1.7488584474885844, "no_speech_prob": 3.288738298579119e-06}, {"id": 473, "seek": 228248, "start": 2288.48, "end": 2291.0, "text": " a numpy array of the y's at that point.", "tokens": [257, 1031, 8200, 10225, 295, 264, 288, 311, 412, 300, 935, 13], "temperature": 0.0, "avg_logprob": -0.1426900965826852, "compression_ratio": 1.7488584474885844, "no_speech_prob": 3.288738298579119e-06}, {"id": 474, "seek": 228248, "start": 2291.0, "end": 2293.12, "text": " And oh, that's it.", "tokens": [400, 1954, 11, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.1426900965826852, "compression_ratio": 1.7488584474885844, "no_speech_prob": 3.288738298579119e-06}, {"id": 475, "seek": 228248, "start": 2293.12, "end": 2296.96, "text": " So then after I wrote this, I realized I haven't really written a sec to sec data set.", "tokens": [407, 550, 934, 286, 4114, 341, 11, 286, 5334, 286, 2378, 380, 534, 3720, 257, 907, 281, 907, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.1426900965826852, "compression_ratio": 1.7488584474885844, "no_speech_prob": 3.288738298579119e-06}, {"id": 476, "seek": 228248, "start": 2296.96, "end": 2300.22, "text": " I've just written a totally generic data set.", "tokens": [286, 600, 445, 3720, 257, 3879, 19577, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.1426900965826852, "compression_ratio": 1.7488584474885844, "no_speech_prob": 3.288738298579119e-06}, {"id": 477, "seek": 228248, "start": 2300.22, "end": 2306.5, "text": " So here's the simplest possible data set that works for any pair of arrays.", "tokens": [407, 510, 311, 264, 22811, 1944, 1412, 992, 300, 1985, 337, 604, 6119, 295, 41011, 13], "temperature": 0.0, "avg_logprob": -0.1426900965826852, "compression_ratio": 1.7488584474885844, "no_speech_prob": 3.288738298579119e-06}, {"id": 478, "seek": 228248, "start": 2306.5, "end": 2308.56, "text": " So it's now poorly named.", "tokens": [407, 309, 311, 586, 22271, 4926, 13], "temperature": 0.0, "avg_logprob": -0.1426900965826852, "compression_ratio": 1.7488584474885844, "no_speech_prob": 3.288738298579119e-06}, {"id": 479, "seek": 230856, "start": 2308.56, "end": 2313.6, "text": " It's much more general than a sec to sec data set, but that's what I needed it for.", "tokens": [467, 311, 709, 544, 2674, 813, 257, 907, 281, 907, 1412, 992, 11, 457, 300, 311, 437, 286, 2978, 309, 337, 13], "temperature": 0.0, "avg_logprob": -0.1073150634765625, "compression_ratio": 1.6790123456790123, "no_speech_prob": 3.2887221550481627e-06}, {"id": 480, "seek": 230856, "start": 2313.6, "end": 2319.7999999999997, "text": " This a function, remember we've got v for variables, t for tensors, a for arrays.", "tokens": [639, 257, 2445, 11, 1604, 321, 600, 658, 371, 337, 9102, 11, 256, 337, 10688, 830, 11, 257, 337, 41011, 13], "temperature": 0.0, "avg_logprob": -0.1073150634765625, "compression_ratio": 1.6790123456790123, "no_speech_prob": 3.2887221550481627e-06}, {"id": 481, "seek": 230856, "start": 2319.7999999999997, "end": 2322.4, "text": " So this basically goes through each of the things you pass it.", "tokens": [407, 341, 1936, 1709, 807, 1184, 295, 264, 721, 291, 1320, 309, 13], "temperature": 0.0, "avg_logprob": -0.1073150634765625, "compression_ratio": 1.6790123456790123, "no_speech_prob": 3.2887221550481627e-06}, {"id": 482, "seek": 230856, "start": 2322.4, "end": 2327.92, "text": " If it's not already a numpy array, it converts it into a numpy array and returns back a tuple", "tokens": [759, 309, 311, 406, 1217, 257, 1031, 8200, 10225, 11, 309, 38874, 309, 666, 257, 1031, 8200, 10225, 293, 11247, 646, 257, 2604, 781], "temperature": 0.0, "avg_logprob": -0.1073150634765625, "compression_ratio": 1.6790123456790123, "no_speech_prob": 3.2887221550481627e-06}, {"id": 483, "seek": 230856, "start": 2327.92, "end": 2332.44, "text": " of all of the things that you passed it, which are now guaranteed to be numpy arrays.", "tokens": [295, 439, 295, 264, 721, 300, 291, 4678, 309, 11, 597, 366, 586, 18031, 281, 312, 1031, 8200, 41011, 13], "temperature": 0.0, "avg_logprob": -0.1073150634765625, "compression_ratio": 1.6790123456790123, "no_speech_prob": 3.2887221550481627e-06}, {"id": 484, "seek": 233244, "start": 2332.44, "end": 2340.04, "text": " So that's a, v, t, three very handy little functions.", "tokens": [407, 300, 311, 257, 11, 371, 11, 256, 11, 1045, 588, 13239, 707, 6828, 13], "temperature": 0.0, "avg_logprob": -0.1383976577430643, "compression_ratio": 1.560747663551402, "no_speech_prob": 3.555969669832848e-06}, {"id": 485, "seek": 233244, "start": 2340.04, "end": 2341.4, "text": " So that's it.", "tokens": [407, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.1383976577430643, "compression_ratio": 1.560747663551402, "no_speech_prob": 3.555969669832848e-06}, {"id": 486, "seek": 233244, "start": 2341.4, "end": 2344.68, "text": " That's our data set.", "tokens": [663, 311, 527, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.1383976577430643, "compression_ratio": 1.560747663551402, "no_speech_prob": 3.555969669832848e-06}, {"id": 487, "seek": 233244, "start": 2344.68, "end": 2352.36, "text": " So now we need to grab our English and French IDs and get a training set and a validation", "tokens": [407, 586, 321, 643, 281, 4444, 527, 3669, 293, 5522, 48212, 293, 483, 257, 3097, 992, 293, 257, 24071], "temperature": 0.0, "avg_logprob": -0.1383976577430643, "compression_ratio": 1.560747663551402, "no_speech_prob": 3.555969669832848e-06}, {"id": 488, "seek": 233244, "start": 2352.36, "end": 2353.36, "text": " set.", "tokens": [992, 13], "temperature": 0.0, "avg_logprob": -0.1383976577430643, "compression_ratio": 1.560747663551402, "no_speech_prob": 3.555969669832848e-06}, {"id": 489, "seek": 233244, "start": 2353.36, "end": 2358.84, "text": " So one of the things which is pretty disappointing about a lot of code out there on the internet", "tokens": [407, 472, 295, 264, 721, 597, 307, 1238, 25054, 466, 257, 688, 295, 3089, 484, 456, 322, 264, 4705], "temperature": 0.0, "avg_logprob": -0.1383976577430643, "compression_ratio": 1.560747663551402, "no_speech_prob": 3.555969669832848e-06}, {"id": 490, "seek": 233244, "start": 2358.84, "end": 2362.28, "text": " is that they don't follow some simple best practices.", "tokens": [307, 300, 436, 500, 380, 1524, 512, 2199, 1151, 7525, 13], "temperature": 0.0, "avg_logprob": -0.1383976577430643, "compression_ratio": 1.560747663551402, "no_speech_prob": 3.555969669832848e-06}, {"id": 491, "seek": 236228, "start": 2362.28, "end": 2369.44, "text": " For example, if you go to the PyTorch website, they have an example section for sequence", "tokens": [1171, 1365, 11, 498, 291, 352, 281, 264, 9953, 51, 284, 339, 3144, 11, 436, 362, 364, 1365, 3541, 337, 8310], "temperature": 0.0, "avg_logprob": -0.14732891264415923, "compression_ratio": 1.8158995815899581, "no_speech_prob": 2.0784807929885574e-05}, {"id": 492, "seek": 236228, "start": 2369.44, "end": 2371.1600000000003, "text": " to sequence translation.", "tokens": [281, 8310, 12853, 13], "temperature": 0.0, "avg_logprob": -0.14732891264415923, "compression_ratio": 1.8158995815899581, "no_speech_prob": 2.0784807929885574e-05}, {"id": 493, "seek": 236228, "start": 2371.1600000000003, "end": 2374.4, "text": " Their example does not have a separate validation set.", "tokens": [6710, 1365, 775, 406, 362, 257, 4994, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.14732891264415923, "compression_ratio": 1.8158995815899581, "no_speech_prob": 2.0784807929885574e-05}, {"id": 494, "seek": 236228, "start": 2374.4, "end": 2379.1200000000003, "text": " I tried it, training according to their settings, and I tested it with a validation set and", "tokens": [286, 3031, 309, 11, 3097, 4650, 281, 641, 6257, 11, 293, 286, 8246, 309, 365, 257, 24071, 992, 293], "temperature": 0.0, "avg_logprob": -0.14732891264415923, "compression_ratio": 1.8158995815899581, "no_speech_prob": 2.0784807929885574e-05}, {"id": 495, "seek": 236228, "start": 2379.1200000000003, "end": 2381.84, "text": " it turned out that it overfit massively.", "tokens": [309, 3574, 484, 300, 309, 670, 6845, 29379, 13], "temperature": 0.0, "avg_logprob": -0.14732891264415923, "compression_ratio": 1.8158995815899581, "no_speech_prob": 2.0784807929885574e-05}, {"id": 496, "seek": 236228, "start": 2381.84, "end": 2383.7200000000003, "text": " So this is not just a theoretical problem.", "tokens": [407, 341, 307, 406, 445, 257, 20864, 1154, 13], "temperature": 0.0, "avg_logprob": -0.14732891264415923, "compression_ratio": 1.8158995815899581, "no_speech_prob": 2.0784807929885574e-05}, {"id": 497, "seek": 236228, "start": 2383.7200000000003, "end": 2390.0400000000004, "text": " The actual PyTorch repo has the actual official sequence to sequence translation example,", "tokens": [440, 3539, 9953, 51, 284, 339, 49040, 575, 264, 3539, 4783, 8310, 281, 8310, 12853, 1365, 11], "temperature": 0.0, "avg_logprob": -0.14732891264415923, "compression_ratio": 1.8158995815899581, "no_speech_prob": 2.0784807929885574e-05}, {"id": 498, "seek": 239004, "start": 2390.04, "end": 2394.24, "text": " which does not check for overfitting and overfits horribly.", "tokens": [597, 775, 406, 1520, 337, 670, 69, 2414, 293, 670, 13979, 45028, 13], "temperature": 0.0, "avg_logprob": -0.13191661467918983, "compression_ratio": 1.6016949152542372, "no_speech_prob": 1.6797249600131181e-06}, {"id": 499, "seek": 239004, "start": 2394.24, "end": 2400.16, "text": " Also it fails to use mini-batches, so it actually fails to utilize any of the efficiency of", "tokens": [2743, 309, 18199, 281, 764, 8382, 12, 65, 852, 279, 11, 370, 309, 767, 18199, 281, 16117, 604, 295, 264, 10493, 295], "temperature": 0.0, "avg_logprob": -0.13191661467918983, "compression_ratio": 1.6016949152542372, "no_speech_prob": 1.6797249600131181e-06}, {"id": 500, "seek": 239004, "start": 2400.16, "end": 2401.16, "text": " PyTorch whatsoever.", "tokens": [9953, 51, 284, 339, 17076, 13], "temperature": 0.0, "avg_logprob": -0.13191661467918983, "compression_ratio": 1.6016949152542372, "no_speech_prob": 1.6797249600131181e-06}, {"id": 501, "seek": 239004, "start": 2401.16, "end": 2407.44, "text": " So there's a lot of like, even if you find code in the official PyTorch repo, don't assume", "tokens": [407, 456, 311, 257, 688, 295, 411, 11, 754, 498, 291, 915, 3089, 294, 264, 4783, 9953, 51, 284, 339, 49040, 11, 500, 380, 6552], "temperature": 0.0, "avg_logprob": -0.13191661467918983, "compression_ratio": 1.6016949152542372, "no_speech_prob": 1.6797249600131181e-06}, {"id": 502, "seek": 239004, "start": 2407.44, "end": 2409.44, "text": " it's any good at all.", "tokens": [309, 311, 604, 665, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.13191661467918983, "compression_ratio": 1.6016949152542372, "no_speech_prob": 1.6797249600131181e-06}, {"id": 503, "seek": 239004, "start": 2409.44, "end": 2416.22, "text": " The other thing you'll notice is that everybody, pretty much every other sequence to sequence", "tokens": [440, 661, 551, 291, 603, 3449, 307, 300, 2201, 11, 1238, 709, 633, 661, 8310, 281, 8310], "temperature": 0.0, "avg_logprob": -0.13191661467918983, "compression_ratio": 1.6016949152542372, "no_speech_prob": 1.6797249600131181e-06}, {"id": 504, "seek": 241622, "start": 2416.22, "end": 2421.7599999999998, "text": " model I've found in PyTorch anywhere on the internet has clearly copied from that shitty", "tokens": [2316, 286, 600, 1352, 294, 9953, 51, 284, 339, 4992, 322, 264, 4705, 575, 4448, 25365, 490, 300, 30748], "temperature": 0.0, "avg_logprob": -0.13638844209558823, "compression_ratio": 1.6652360515021458, "no_speech_prob": 1.7061800008377759e-06}, {"id": 505, "seek": 241622, "start": 2421.7599999999998, "end": 2425.8399999999997, "text": " PyTorch repo because it all has the same variable names, it has the same problems, it has the", "tokens": [9953, 51, 284, 339, 49040, 570, 309, 439, 575, 264, 912, 7006, 5288, 11, 309, 575, 264, 912, 2740, 11, 309, 575, 264], "temperature": 0.0, "avg_logprob": -0.13638844209558823, "compression_ratio": 1.6652360515021458, "no_speech_prob": 1.7061800008377759e-06}, {"id": 506, "seek": 241622, "start": 2425.8399999999997, "end": 2428.48, "text": " same mistakes.", "tokens": [912, 8038, 13], "temperature": 0.0, "avg_logprob": -0.13638844209558823, "compression_ratio": 1.6652360515021458, "no_speech_prob": 1.7061800008377759e-06}, {"id": 507, "seek": 241622, "start": 2428.48, "end": 2433.52, "text": " Like another example, nearly every PyTorch convolutional neural network I've found does", "tokens": [1743, 1071, 1365, 11, 6217, 633, 9953, 51, 284, 339, 45216, 304, 18161, 3209, 286, 600, 1352, 775], "temperature": 0.0, "avg_logprob": -0.13638844209558823, "compression_ratio": 1.6652360515021458, "no_speech_prob": 1.7061800008377759e-06}, {"id": 508, "seek": 241622, "start": 2433.52, "end": 2436.16, "text": " not use an adaptive pooling layer.", "tokens": [406, 764, 364, 27912, 7005, 278, 4583, 13], "temperature": 0.0, "avg_logprob": -0.13638844209558823, "compression_ratio": 1.6652360515021458, "no_speech_prob": 1.7061800008377759e-06}, {"id": 509, "seek": 241622, "start": 2436.16, "end": 2441.9199999999996, "text": " So in other words, the final layer is always like average pool 7,7.", "tokens": [407, 294, 661, 2283, 11, 264, 2572, 4583, 307, 1009, 411, 4274, 7005, 1614, 11, 22, 13], "temperature": 0.0, "avg_logprob": -0.13638844209558823, "compression_ratio": 1.6652360515021458, "no_speech_prob": 1.7061800008377759e-06}, {"id": 510, "seek": 244192, "start": 2441.92, "end": 2447.88, "text": " So they assume that the previous layer is 7x7, and if you use any other size input,", "tokens": [407, 436, 6552, 300, 264, 3894, 4583, 307, 1614, 87, 22, 11, 293, 498, 291, 764, 604, 661, 2744, 4846, 11], "temperature": 0.0, "avg_logprob": -0.1354555505694765, "compression_ratio": 1.5953177257525084, "no_speech_prob": 1.5779580735397758e-06}, {"id": 511, "seek": 244192, "start": 2447.88, "end": 2449.2400000000002, "text": " you get an exception.", "tokens": [291, 483, 364, 11183, 13], "temperature": 0.0, "avg_logprob": -0.1354555505694765, "compression_ratio": 1.5953177257525084, "no_speech_prob": 1.5779580735397758e-06}, {"id": 512, "seek": 244192, "start": 2449.2400000000002, "end": 2453.04, "text": " And therefore, nearly everybody I've spoken to that uses PyTorch thinks that there is", "tokens": [400, 4412, 11, 6217, 2201, 286, 600, 10759, 281, 300, 4960, 9953, 51, 284, 339, 7309, 300, 456, 307], "temperature": 0.0, "avg_logprob": -0.1354555505694765, "compression_ratio": 1.5953177257525084, "no_speech_prob": 1.5779580735397758e-06}, {"id": 513, "seek": 244192, "start": 2453.04, "end": 2457.64, "text": " a fundamental limitation of CNNs, that they are tied to the input size.", "tokens": [257, 8088, 27432, 295, 24859, 82, 11, 300, 436, 366, 9601, 281, 264, 4846, 2744, 13], "temperature": 0.0, "avg_logprob": -0.1354555505694765, "compression_ratio": 1.5953177257525084, "no_speech_prob": 1.5779580735397758e-06}, {"id": 514, "seek": 244192, "start": 2457.64, "end": 2460.28, "text": " That has not been true since VGG.", "tokens": [663, 575, 406, 668, 2074, 1670, 691, 27561, 13], "temperature": 0.0, "avg_logprob": -0.1354555505694765, "compression_ratio": 1.5953177257525084, "no_speech_prob": 1.5779580735397758e-06}, {"id": 515, "seek": 244192, "start": 2460.28, "end": 2464.7200000000003, "text": " So every time we grab a new model and stick it in the fast AI repo, I have to go in, search", "tokens": [407, 633, 565, 321, 4444, 257, 777, 2316, 293, 2897, 309, 294, 264, 2370, 7318, 49040, 11, 286, 362, 281, 352, 294, 11, 3164], "temperature": 0.0, "avg_logprob": -0.1354555505694765, "compression_ratio": 1.5953177257525084, "no_speech_prob": 1.5779580735397758e-06}, {"id": 516, "seek": 244192, "start": 2464.7200000000003, "end": 2470.44, "text": " for pool, and add adaptive to the start and replace the 7 with a 1, and now it works on", "tokens": [337, 7005, 11, 293, 909, 27912, 281, 264, 722, 293, 7406, 264, 1614, 365, 257, 502, 11, 293, 586, 309, 1985, 322], "temperature": 0.0, "avg_logprob": -0.1354555505694765, "compression_ratio": 1.5953177257525084, "no_speech_prob": 1.5779580735397758e-06}, {"id": 517, "seek": 247044, "start": 2470.44, "end": 2472.44, "text": " any sized object.", "tokens": [604, 20004, 2657, 13], "temperature": 0.0, "avg_logprob": -0.15345541912576427, "compression_ratio": 1.674911660777385, "no_speech_prob": 7.766895578242838e-06}, {"id": 518, "seek": 247044, "start": 2472.44, "end": 2478.28, "text": " So just be careful, it's still early days, and believe it or not, even though most of", "tokens": [407, 445, 312, 5026, 11, 309, 311, 920, 2440, 1708, 11, 293, 1697, 309, 420, 406, 11, 754, 1673, 881, 295], "temperature": 0.0, "avg_logprob": -0.15345541912576427, "compression_ratio": 1.674911660777385, "no_speech_prob": 7.766895578242838e-06}, {"id": 519, "seek": 247044, "start": 2478.28, "end": 2484.04, "text": " you have only started in the last year your deep learning journey, you know quite a lot", "tokens": [291, 362, 787, 1409, 294, 264, 1036, 1064, 428, 2452, 2539, 4671, 11, 291, 458, 1596, 257, 688], "temperature": 0.0, "avg_logprob": -0.15345541912576427, "compression_ratio": 1.674911660777385, "no_speech_prob": 7.766895578242838e-06}, {"id": 520, "seek": 247044, "start": 2484.04, "end": 2488.52, "text": " more about a lot of the more important practical aspects than the vast majority of people that", "tokens": [544, 466, 257, 688, 295, 264, 544, 1021, 8496, 7270, 813, 264, 8369, 6286, 295, 561, 300], "temperature": 0.0, "avg_logprob": -0.15345541912576427, "compression_ratio": 1.674911660777385, "no_speech_prob": 7.766895578242838e-06}, {"id": 521, "seek": 247044, "start": 2488.52, "end": 2492.54, "text": " are publishing and writing stuff in official repos and stuff.", "tokens": [366, 17832, 293, 3579, 1507, 294, 4783, 1085, 329, 293, 1507, 13], "temperature": 0.0, "avg_logprob": -0.15345541912576427, "compression_ratio": 1.674911660777385, "no_speech_prob": 7.766895578242838e-06}, {"id": 522, "seek": 247044, "start": 2492.54, "end": 2497.76, "text": " So you kind of need to have a little more self-confidence than you might expect when", "tokens": [407, 291, 733, 295, 643, 281, 362, 257, 707, 544, 2698, 12, 47273, 813, 291, 1062, 2066, 562], "temperature": 0.0, "avg_logprob": -0.15345541912576427, "compression_ratio": 1.674911660777385, "no_speech_prob": 7.766895578242838e-06}, {"id": 523, "seek": 247044, "start": 2497.76, "end": 2499.28, "text": " it comes to reading other people's code.", "tokens": [309, 1487, 281, 3760, 661, 561, 311, 3089, 13], "temperature": 0.0, "avg_logprob": -0.15345541912576427, "compression_ratio": 1.674911660777385, "no_speech_prob": 7.766895578242838e-06}, {"id": 524, "seek": 249928, "start": 2499.28, "end": 2503.96, "text": " If you find yourself thinking, that looks odd, it's not necessarily you.", "tokens": [759, 291, 915, 1803, 1953, 11, 300, 1542, 7401, 11, 309, 311, 406, 4725, 291, 13], "temperature": 0.0, "avg_logprob": -0.19342931579141057, "compression_ratio": 1.4619565217391304, "no_speech_prob": 1.5936431736918166e-05}, {"id": 525, "seek": 249928, "start": 2503.96, "end": 2509.88, "text": " It might well be them.", "tokens": [467, 1062, 731, 312, 552, 13], "temperature": 0.0, "avg_logprob": -0.19342931579141057, "compression_ratio": 1.4619565217391304, "no_speech_prob": 1.5936431736918166e-05}, {"id": 526, "seek": 249928, "start": 2509.88, "end": 2519.44, "text": " So I would say like at least 90% of deep learning code that I start looking at turns out to", "tokens": [407, 286, 576, 584, 411, 412, 1935, 4289, 4, 295, 2452, 2539, 3089, 300, 286, 722, 1237, 412, 4523, 484, 281], "temperature": 0.0, "avg_logprob": -0.19342931579141057, "compression_ratio": 1.4619565217391304, "no_speech_prob": 1.5936431736918166e-05}, {"id": 527, "seek": 249928, "start": 2519.44, "end": 2528.0, "text": " have like deathly serious problems that make it completely unusable for anything.", "tokens": [362, 411, 2966, 356, 3156, 2740, 300, 652, 309, 2584, 10054, 712, 337, 1340, 13], "temperature": 0.0, "avg_logprob": -0.19342931579141057, "compression_ratio": 1.4619565217391304, "no_speech_prob": 1.5936431736918166e-05}, {"id": 528, "seek": 252800, "start": 2528.0, "end": 2533.96, "text": " And so I've been telling people that I've been working with recently, if the repo you're", "tokens": [400, 370, 286, 600, 668, 3585, 561, 300, 286, 600, 668, 1364, 365, 3938, 11, 498, 264, 49040, 291, 434], "temperature": 0.0, "avg_logprob": -0.17892603874206542, "compression_ratio": 1.8257575757575757, "no_speech_prob": 1.983294714591466e-05}, {"id": 529, "seek": 252800, "start": 2533.96, "end": 2538.16, "text": " looking at doesn't have a section on it saying here's the test we did where we got the same", "tokens": [1237, 412, 1177, 380, 362, 257, 3541, 322, 309, 1566, 510, 311, 264, 1500, 321, 630, 689, 321, 658, 264, 912], "temperature": 0.0, "avg_logprob": -0.17892603874206542, "compression_ratio": 1.8257575757575757, "no_speech_prob": 1.983294714591466e-05}, {"id": 530, "seek": 252800, "start": 2538.16, "end": 2542.0, "text": " results as the paper that it's going to be implementing, that almost certainly means", "tokens": [3542, 382, 264, 3035, 300, 309, 311, 516, 281, 312, 18114, 11, 300, 1920, 3297, 1355], "temperature": 0.0, "avg_logprob": -0.17892603874206542, "compression_ratio": 1.8257575757575757, "no_speech_prob": 1.983294714591466e-05}, {"id": 531, "seek": 252800, "start": 2542.0, "end": 2544.8, "text": " they haven't got the same results of the paper they're implementing, they probably haven't", "tokens": [436, 2378, 380, 658, 264, 912, 3542, 295, 264, 3035, 436, 434, 18114, 11, 436, 1391, 2378, 380], "temperature": 0.0, "avg_logprob": -0.17892603874206542, "compression_ratio": 1.8257575757575757, "no_speech_prob": 1.983294714591466e-05}, {"id": 532, "seek": 252800, "start": 2544.8, "end": 2546.48, "text": " even checked.", "tokens": [754, 10033, 13], "temperature": 0.0, "avg_logprob": -0.17892603874206542, "compression_ratio": 1.8257575757575757, "no_speech_prob": 1.983294714591466e-05}, {"id": 533, "seek": 252800, "start": 2546.48, "end": 2549.4, "text": " And if you run it, it definitely won't get those results.", "tokens": [400, 498, 291, 1190, 309, 11, 309, 2138, 1582, 380, 483, 729, 3542, 13], "temperature": 0.0, "avg_logprob": -0.17892603874206542, "compression_ratio": 1.8257575757575757, "no_speech_prob": 1.983294714591466e-05}, {"id": 534, "seek": 252800, "start": 2549.4, "end": 2552.04, "text": " Because it's hard to get things right the first time.", "tokens": [1436, 309, 311, 1152, 281, 483, 721, 558, 264, 700, 565, 13], "temperature": 0.0, "avg_logprob": -0.17892603874206542, "compression_ratio": 1.8257575757575757, "no_speech_prob": 1.983294714591466e-05}, {"id": 535, "seek": 255204, "start": 2552.04, "end": 2558.16, "text": " It takes me 12 goes, probably takes normal smarter people than me 6 goes, but if they", "tokens": [467, 2516, 385, 2272, 1709, 11, 1391, 2516, 2710, 20294, 561, 813, 385, 1386, 1709, 11, 457, 498, 436], "temperature": 0.0, "avg_logprob": -0.15192741980919472, "compression_ratio": 1.641304347826087, "no_speech_prob": 6.240899892873131e-06}, {"id": 536, "seek": 255204, "start": 2558.16, "end": 2563.68, "text": " haven't tested it once, it almost certainly won't work.", "tokens": [2378, 380, 8246, 309, 1564, 11, 309, 1920, 3297, 1582, 380, 589, 13], "temperature": 0.0, "avg_logprob": -0.15192741980919472, "compression_ratio": 1.641304347826087, "no_speech_prob": 6.240899892873131e-06}, {"id": 537, "seek": 255204, "start": 2563.68, "end": 2565.4, "text": " So there's our sequence data set.", "tokens": [407, 456, 311, 527, 8310, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.15192741980919472, "compression_ratio": 1.641304347826087, "no_speech_prob": 6.240899892873131e-06}, {"id": 538, "seek": 255204, "start": 2565.4, "end": 2568.08, "text": " Let's get the training and validation sets.", "tokens": [961, 311, 483, 264, 3097, 293, 24071, 6352, 13], "temperature": 0.0, "avg_logprob": -0.15192741980919472, "compression_ratio": 1.641304347826087, "no_speech_prob": 6.240899892873131e-06}, {"id": 539, "seek": 255204, "start": 2568.08, "end": 2569.08, "text": " Here's an easy way to do that.", "tokens": [1692, 311, 364, 1858, 636, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.15192741980919472, "compression_ratio": 1.641304347826087, "no_speech_prob": 6.240899892873131e-06}, {"id": 540, "seek": 255204, "start": 2569.08, "end": 2572.68, "text": " Grab a bunch of random numbers, one for each row of your data.", "tokens": [20357, 257, 3840, 295, 4974, 3547, 11, 472, 337, 1184, 5386, 295, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.15192741980919472, "compression_ratio": 1.641304347826087, "no_speech_prob": 6.240899892873131e-06}, {"id": 541, "seek": 255204, "start": 2572.68, "end": 2575.7599999999998, "text": " See if they're bigger than 0.1 or not.", "tokens": [3008, 498, 436, 434, 3801, 813, 1958, 13, 16, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.15192741980919472, "compression_ratio": 1.641304347826087, "no_speech_prob": 6.240899892873131e-06}, {"id": 542, "seek": 255204, "start": 2575.7599999999998, "end": 2577.8, "text": " That gets you a list of bools.", "tokens": [663, 2170, 291, 257, 1329, 295, 748, 19385, 13], "temperature": 0.0, "avg_logprob": -0.15192741980919472, "compression_ratio": 1.641304347826087, "no_speech_prob": 6.240899892873131e-06}, {"id": 543, "seek": 255204, "start": 2577.8, "end": 2581.5, "text": " Index into your array with that list of bools to grab a training set.", "tokens": [33552, 666, 428, 10225, 365, 300, 1329, 295, 748, 19385, 281, 4444, 257, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.15192741980919472, "compression_ratio": 1.641304347826087, "no_speech_prob": 6.240899892873131e-06}, {"id": 544, "seek": 258150, "start": 2581.5, "end": 2585.08, "text": " Index into that array with the opposite of that list of bools to get your validation", "tokens": [33552, 666, 300, 10225, 365, 264, 6182, 295, 300, 1329, 295, 748, 19385, 281, 483, 428, 24071], "temperature": 0.0, "avg_logprob": -0.18220828374226888, "compression_ratio": 1.6719367588932805, "no_speech_prob": 6.240898528631078e-06}, {"id": 545, "seek": 258150, "start": 2585.08, "end": 2586.08, "text": " set.", "tokens": [992, 13], "temperature": 0.0, "avg_logprob": -0.18220828374226888, "compression_ratio": 1.6719367588932805, "no_speech_prob": 6.240898528631078e-06}, {"id": 546, "seek": 258150, "start": 2586.08, "end": 2587.08, "text": " So it's a nice easy way.", "tokens": [407, 309, 311, 257, 1481, 1858, 636, 13], "temperature": 0.0, "avg_logprob": -0.18220828374226888, "compression_ratio": 1.6719367588932805, "no_speech_prob": 6.240898528631078e-06}, {"id": 547, "seek": 258150, "start": 2587.08, "end": 2588.24, "text": " There's lots of ways of doing it.", "tokens": [821, 311, 3195, 295, 2098, 295, 884, 309, 13], "temperature": 0.0, "avg_logprob": -0.18220828374226888, "compression_ratio": 1.6719367588932805, "no_speech_prob": 6.240898528631078e-06}, {"id": 548, "seek": 258150, "start": 2588.24, "end": 2592.96, "text": " I just like to do different ways so you can see a few approaches.", "tokens": [286, 445, 411, 281, 360, 819, 2098, 370, 291, 393, 536, 257, 1326, 11587, 13], "temperature": 0.0, "avg_logprob": -0.18220828374226888, "compression_ratio": 1.6719367588932805, "no_speech_prob": 6.240898528631078e-06}, {"id": 549, "seek": 258150, "start": 2592.96, "end": 2597.86, "text": " So now we can create our data set with our x's and our y's, French and English.", "tokens": [407, 586, 321, 393, 1884, 527, 1412, 992, 365, 527, 2031, 311, 293, 527, 288, 311, 11, 5522, 293, 3669, 13], "temperature": 0.0, "avg_logprob": -0.18220828374226888, "compression_ratio": 1.6719367588932805, "no_speech_prob": 6.240898528631078e-06}, {"id": 550, "seek": 258150, "start": 2597.86, "end": 2602.76, "text": " If you want to translate instead English to French, switch these two around and you're", "tokens": [759, 291, 528, 281, 13799, 2602, 3669, 281, 5522, 11, 3679, 613, 732, 926, 293, 291, 434], "temperature": 0.0, "avg_logprob": -0.18220828374226888, "compression_ratio": 1.6719367588932805, "no_speech_prob": 6.240898528631078e-06}, {"id": 551, "seek": 258150, "start": 2602.76, "end": 2603.76, "text": " done.", "tokens": [1096, 13], "temperature": 0.0, "avg_logprob": -0.18220828374226888, "compression_ratio": 1.6719367588932805, "no_speech_prob": 6.240898528631078e-06}, {"id": 552, "seek": 258150, "start": 2603.76, "end": 2606.72, "text": " Now we need to create data loaders.", "tokens": [823, 321, 643, 281, 1884, 1412, 3677, 433, 13], "temperature": 0.0, "avg_logprob": -0.18220828374226888, "compression_ratio": 1.6719367588932805, "no_speech_prob": 6.240898528631078e-06}, {"id": 553, "seek": 260672, "start": 2606.72, "end": 2615.24, "text": " We can just grab our data loader and pass in our data set and batch size.", "tokens": [492, 393, 445, 4444, 527, 1412, 3677, 260, 293, 1320, 294, 527, 1412, 992, 293, 15245, 2744, 13], "temperature": 0.0, "avg_logprob": -0.12918251701023267, "compression_ratio": 1.67578125, "no_speech_prob": 8.267828889074735e-06}, {"id": 554, "seek": 260672, "start": 2615.24, "end": 2618.16, "text": " We actually have to transpose the arrays.", "tokens": [492, 767, 362, 281, 25167, 264, 41011, 13], "temperature": 0.0, "avg_logprob": -0.12918251701023267, "compression_ratio": 1.67578125, "no_speech_prob": 8.267828889074735e-06}, {"id": 555, "seek": 260672, "start": 2618.16, "end": 2620.4399999999996, "text": " I'm not going to go into the details about why.", "tokens": [286, 478, 406, 516, 281, 352, 666, 264, 4365, 466, 983, 13], "temperature": 0.0, "avg_logprob": -0.12918251701023267, "compression_ratio": 1.67578125, "no_speech_prob": 8.267828889074735e-06}, {"id": 556, "seek": 260672, "start": 2620.4399999999996, "end": 2624.12, "text": " We can talk about it during the week if you're interested, but have a think about why we", "tokens": [492, 393, 751, 466, 309, 1830, 264, 1243, 498, 291, 434, 3102, 11, 457, 362, 257, 519, 466, 983, 321], "temperature": 0.0, "avg_logprob": -0.12918251701023267, "compression_ratio": 1.67578125, "no_speech_prob": 8.267828889074735e-06}, {"id": 557, "seek": 260672, "start": 2624.12, "end": 2629.16, "text": " might need to transpose their orientation.", "tokens": [1062, 643, 281, 25167, 641, 14764, 13], "temperature": 0.0, "avg_logprob": -0.12918251701023267, "compression_ratio": 1.67578125, "no_speech_prob": 8.267828889074735e-06}, {"id": 558, "seek": 260672, "start": 2629.16, "end": 2631.12, "text": " But there's a few more things I want to do.", "tokens": [583, 456, 311, 257, 1326, 544, 721, 286, 528, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.12918251701023267, "compression_ratio": 1.67578125, "no_speech_prob": 8.267828889074735e-06}, {"id": 559, "seek": 260672, "start": 2631.12, "end": 2635.8399999999997, "text": " One is that since we've already done all the preprocessing, there's no point spawning off", "tokens": [1485, 307, 300, 1670, 321, 600, 1217, 1096, 439, 264, 2666, 340, 780, 278, 11, 456, 311, 572, 935, 637, 35880, 766], "temperature": 0.0, "avg_logprob": -0.12918251701023267, "compression_ratio": 1.67578125, "no_speech_prob": 8.267828889074735e-06}, {"id": 560, "seek": 263584, "start": 2635.84, "end": 2640.7000000000003, "text": " multiple workers to do augmentation or whatever because there's no work to do.", "tokens": [3866, 5600, 281, 360, 14501, 19631, 420, 2035, 570, 456, 311, 572, 589, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.20893171855381557, "compression_ratio": 1.6725978647686832, "no_speech_prob": 5.626393999591528e-07}, {"id": 561, "seek": 263584, "start": 2640.7000000000003, "end": 2644.88, "text": " So making num workers equals one will save you some time.", "tokens": [407, 1455, 1031, 5600, 6915, 472, 486, 3155, 291, 512, 565, 13], "temperature": 0.0, "avg_logprob": -0.20893171855381557, "compression_ratio": 1.6725978647686832, "no_speech_prob": 5.626393999591528e-07}, {"id": 562, "seek": 263584, "start": 2644.88, "end": 2647.1600000000003, "text": " We have to tell it what our padding index is.", "tokens": [492, 362, 281, 980, 309, 437, 527, 39562, 8186, 307, 13], "temperature": 0.0, "avg_logprob": -0.20893171855381557, "compression_ratio": 1.6725978647686832, "no_speech_prob": 5.626393999591528e-07}, {"id": 563, "seek": 263584, "start": 2647.1600000000003, "end": 2652.48, "text": " That's actually pretty important because what's going to happen is that we've got different", "tokens": [663, 311, 767, 1238, 1021, 570, 437, 311, 516, 281, 1051, 307, 300, 321, 600, 658, 819], "temperature": 0.0, "avg_logprob": -0.20893171855381557, "compression_ratio": 1.6725978647686832, "no_speech_prob": 5.626393999591528e-07}, {"id": 564, "seek": 263584, "start": 2652.48, "end": 2657.88, "text": " length sentences and fastai, I think it's pretty much the only library that does this,", "tokens": [4641, 16579, 293, 2370, 1301, 11, 286, 519, 309, 311, 1238, 709, 264, 787, 6405, 300, 775, 341, 11], "temperature": 0.0, "avg_logprob": -0.20893171855381557, "compression_ratio": 1.6725978647686832, "no_speech_prob": 5.626393999591528e-07}, {"id": 565, "seek": 263584, "start": 2657.88, "end": 2662.96, "text": " fastai will just automatically stick them together and pad the shorter ones so they", "tokens": [2370, 1301, 486, 445, 6772, 2897, 552, 1214, 293, 6887, 264, 11639, 2306, 370, 436], "temperature": 0.0, "avg_logprob": -0.20893171855381557, "compression_ratio": 1.6725978647686832, "no_speech_prob": 5.626393999591528e-07}, {"id": 566, "seek": 263584, "start": 2662.96, "end": 2664.52, "text": " all end up equal length.", "tokens": [439, 917, 493, 2681, 4641, 13], "temperature": 0.0, "avg_logprob": -0.20893171855381557, "compression_ratio": 1.6725978647686832, "no_speech_prob": 5.626393999591528e-07}, {"id": 567, "seek": 266452, "start": 2664.52, "end": 2671.56, "text": " Because remember a tensor has to be rectangular.", "tokens": [1436, 1604, 257, 40863, 575, 281, 312, 31167, 13], "temperature": 0.0, "avg_logprob": -0.1469293948823372, "compression_ratio": 1.728813559322034, "no_speech_prob": 8.267809789685998e-06}, {"id": 568, "seek": 266452, "start": 2671.56, "end": 2678.84, "text": " In the decoder in particular, I actually want my padding to be at the end, not at the start.", "tokens": [682, 264, 979, 19866, 294, 1729, 11, 286, 767, 528, 452, 39562, 281, 312, 412, 264, 917, 11, 406, 412, 264, 722, 13], "temperature": 0.0, "avg_logprob": -0.1469293948823372, "compression_ratio": 1.728813559322034, "no_speech_prob": 8.267809789685998e-06}, {"id": 569, "seek": 266452, "start": 2678.84, "end": 2684.8, "text": " For a classifier, I want the padding at the start because I want that final token to represent", "tokens": [1171, 257, 1508, 9902, 11, 286, 528, 264, 39562, 412, 264, 722, 570, 286, 528, 300, 2572, 14862, 281, 2906], "temperature": 0.0, "avg_logprob": -0.1469293948823372, "compression_ratio": 1.728813559322034, "no_speech_prob": 8.267809789685998e-06}, {"id": 570, "seek": 266452, "start": 2684.8, "end": 2687.84, "text": " the last word of the Moody review.", "tokens": [264, 1036, 1349, 295, 264, 3335, 843, 3131, 13], "temperature": 0.0, "avg_logprob": -0.1469293948823372, "compression_ratio": 1.728813559322034, "no_speech_prob": 8.267809789685998e-06}, {"id": 571, "seek": 266452, "start": 2687.84, "end": 2691.28, "text": " But in the decoder, as you'll see, it actually is going to work out a bit better to have", "tokens": [583, 294, 264, 979, 19866, 11, 382, 291, 603, 536, 11, 309, 767, 307, 516, 281, 589, 484, 257, 857, 1101, 281, 362], "temperature": 0.0, "avg_logprob": -0.1469293948823372, "compression_ratio": 1.728813559322034, "no_speech_prob": 8.267809789685998e-06}, {"id": 572, "seek": 266452, "start": 2691.28, "end": 2692.28, "text": " padding at the end.", "tokens": [39562, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.1469293948823372, "compression_ratio": 1.728813559322034, "no_speech_prob": 8.267809789685998e-06}, {"id": 573, "seek": 266452, "start": 2692.28, "end": 2694.4, "text": " So I say prepad equals pos.", "tokens": [407, 286, 584, 2666, 345, 6915, 1366, 13], "temperature": 0.0, "avg_logprob": -0.1469293948823372, "compression_ratio": 1.728813559322034, "no_speech_prob": 8.267809789685998e-06}, {"id": 574, "seek": 269440, "start": 2694.4, "end": 2699.08, "text": " And then finally, since we've got sentences of different lengths coming in and they all", "tokens": [400, 550, 2721, 11, 1670, 321, 600, 658, 16579, 295, 819, 26329, 1348, 294, 293, 436, 439], "temperature": 0.0, "avg_logprob": -0.12796485424041748, "compression_ratio": 1.7768924302788844, "no_speech_prob": 8.139644705806859e-06}, {"id": 575, "seek": 269440, "start": 2699.08, "end": 2704.12, "text": " have to be put together in a mini-batch to be the same size by padding, we would much", "tokens": [362, 281, 312, 829, 1214, 294, 257, 8382, 12, 65, 852, 281, 312, 264, 912, 2744, 538, 39562, 11, 321, 576, 709], "temperature": 0.0, "avg_logprob": -0.12796485424041748, "compression_ratio": 1.7768924302788844, "no_speech_prob": 8.139644705806859e-06}, {"id": 576, "seek": 269440, "start": 2704.12, "end": 2710.2000000000003, "text": " prefer that the sentence in a mini-batch are of similar sizes already because otherwise", "tokens": [4382, 300, 264, 8174, 294, 257, 8382, 12, 65, 852, 366, 295, 2531, 11602, 1217, 570, 5911], "temperature": 0.0, "avg_logprob": -0.12796485424041748, "compression_ratio": 1.7768924302788844, "no_speech_prob": 8.139644705806859e-06}, {"id": 577, "seek": 269440, "start": 2710.2000000000003, "end": 2714.48, "text": " it's going to be as long as the longest sentence and that's going to end up wasting time and", "tokens": [309, 311, 516, 281, 312, 382, 938, 382, 264, 15438, 8174, 293, 300, 311, 516, 281, 917, 493, 20457, 565, 293], "temperature": 0.0, "avg_logprob": -0.12796485424041748, "compression_ratio": 1.7768924302788844, "no_speech_prob": 8.139644705806859e-06}, {"id": 578, "seek": 269440, "start": 2714.48, "end": 2716.32, "text": " memory.", "tokens": [4675, 13], "temperature": 0.0, "avg_logprob": -0.12796485424041748, "compression_ratio": 1.7768924302788844, "no_speech_prob": 8.139644705806859e-06}, {"id": 579, "seek": 269440, "start": 2716.32, "end": 2720.64, "text": " So therefore I'm going to use the sampler trick that we learned last time, which is", "tokens": [407, 4412, 286, 478, 516, 281, 764, 264, 3247, 22732, 4282, 300, 321, 3264, 1036, 565, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.12796485424041748, "compression_ratio": 1.7768924302788844, "no_speech_prob": 8.139644705806859e-06}, {"id": 580, "seek": 272064, "start": 2720.64, "end": 2727.4, "text": " the validation set, we're going to ask it to sort everything by length first.", "tokens": [264, 24071, 992, 11, 321, 434, 516, 281, 1029, 309, 281, 1333, 1203, 538, 4641, 700, 13], "temperature": 0.0, "avg_logprob": -0.13984379325945354, "compression_ratio": 1.7403846153846154, "no_speech_prob": 4.092895323992707e-06}, {"id": 581, "seek": 272064, "start": 2727.4, "end": 2732.48, "text": " And then for the training set, we're going to ask it to randomize the order of things", "tokens": [400, 550, 337, 264, 3097, 992, 11, 321, 434, 516, 281, 1029, 309, 281, 4974, 1125, 264, 1668, 295, 721], "temperature": 0.0, "avg_logprob": -0.13984379325945354, "compression_ratio": 1.7403846153846154, "no_speech_prob": 4.092895323992707e-06}, {"id": 582, "seek": 272064, "start": 2732.48, "end": 2737.52, "text": " but to roughly make it so that things of similar length are about in the same spot.", "tokens": [457, 281, 9810, 652, 309, 370, 300, 721, 295, 2531, 4641, 366, 466, 294, 264, 912, 4008, 13], "temperature": 0.0, "avg_logprob": -0.13984379325945354, "compression_ratio": 1.7403846153846154, "no_speech_prob": 4.092895323992707e-06}, {"id": 583, "seek": 272064, "start": 2737.52, "end": 2742.0, "text": " So we've got our sort sampler and our sort-ish sampler.", "tokens": [407, 321, 600, 658, 527, 1333, 3247, 22732, 293, 527, 1333, 12, 742, 3247, 22732, 13], "temperature": 0.0, "avg_logprob": -0.13984379325945354, "compression_ratio": 1.7403846153846154, "no_speech_prob": 4.092895323992707e-06}, {"id": 584, "seek": 272064, "start": 2742.0, "end": 2745.52, "text": " And then at that point, we can create a model data object.", "tokens": [400, 550, 412, 300, 935, 11, 321, 393, 1884, 257, 2316, 1412, 2657, 13], "temperature": 0.0, "avg_logprob": -0.13984379325945354, "compression_ratio": 1.7403846153846154, "no_speech_prob": 4.092895323992707e-06}, {"id": 585, "seek": 274552, "start": 2745.52, "end": 2750.8, "text": " So a model data object really does one thing, which is it says I have a training set and", "tokens": [407, 257, 2316, 1412, 2657, 534, 775, 472, 551, 11, 597, 307, 309, 1619, 286, 362, 257, 3097, 992, 293], "temperature": 0.0, "avg_logprob": -0.15180990431043836, "compression_ratio": 1.5784753363228698, "no_speech_prob": 1.7330486343780649e-06}, {"id": 586, "seek": 274552, "start": 2750.8, "end": 2755.52, "text": " a validation set and an optional test set and sticks them into a single object.", "tokens": [257, 24071, 992, 293, 364, 17312, 1500, 992, 293, 12518, 552, 666, 257, 2167, 2657, 13], "temperature": 0.0, "avg_logprob": -0.15180990431043836, "compression_ratio": 1.5784753363228698, "no_speech_prob": 1.7330486343780649e-06}, {"id": 587, "seek": 274552, "start": 2755.52, "end": 2760.44, "text": " We also have a path so that it has somewhere to store temporary files, models, stuff like", "tokens": [492, 611, 362, 257, 3100, 370, 300, 309, 575, 4079, 281, 3531, 13413, 7098, 11, 5245, 11, 1507, 411], "temperature": 0.0, "avg_logprob": -0.15180990431043836, "compression_ratio": 1.5784753363228698, "no_speech_prob": 1.7330486343780649e-06}, {"id": 588, "seek": 274552, "start": 2760.44, "end": 2761.44, "text": " that.", "tokens": [300, 13], "temperature": 0.0, "avg_logprob": -0.15180990431043836, "compression_ratio": 1.5784753363228698, "no_speech_prob": 1.7330486343780649e-06}, {"id": 589, "seek": 274552, "start": 2761.44, "end": 2768.72, "text": " So we're not using fast AI for very much at all in this example, just kind of a minimal", "tokens": [407, 321, 434, 406, 1228, 2370, 7318, 337, 588, 709, 412, 439, 294, 341, 1365, 11, 445, 733, 295, 257, 13206], "temperature": 0.0, "avg_logprob": -0.15180990431043836, "compression_ratio": 1.5784753363228698, "no_speech_prob": 1.7330486343780649e-06}, {"id": 590, "seek": 276872, "start": 2768.72, "end": 2776.3599999999997, "text": " set to show you how to get your model data object.", "tokens": [992, 281, 855, 291, 577, 281, 483, 428, 2316, 1412, 2657, 13], "temperature": 0.0, "avg_logprob": -0.14938601818713512, "compression_ratio": 1.7555555555555555, "no_speech_prob": 8.01344958745176e-06}, {"id": 591, "seek": 276872, "start": 2776.3599999999997, "end": 2780.3999999999996, "text": " In the end, once you've got a model data object, you can then create a learner and you can", "tokens": [682, 264, 917, 11, 1564, 291, 600, 658, 257, 2316, 1412, 2657, 11, 291, 393, 550, 1884, 257, 33347, 293, 291, 393], "temperature": 0.0, "avg_logprob": -0.14938601818713512, "compression_ratio": 1.7555555555555555, "no_speech_prob": 8.01344958745176e-06}, {"id": 592, "seek": 276872, "start": 2780.3999999999996, "end": 2782.12, "text": " then call fit.", "tokens": [550, 818, 3318, 13], "temperature": 0.0, "avg_logprob": -0.14938601818713512, "compression_ratio": 1.7555555555555555, "no_speech_prob": 8.01344958745176e-06}, {"id": 593, "seek": 276872, "start": 2782.12, "end": 2788.48, "text": " So that's kind of like minimal amount of fast AI stuff here.", "tokens": [407, 300, 311, 733, 295, 411, 13206, 2372, 295, 2370, 7318, 1507, 510, 13], "temperature": 0.0, "avg_logprob": -0.14938601818713512, "compression_ratio": 1.7555555555555555, "no_speech_prob": 8.01344958745176e-06}, {"id": 594, "seek": 276872, "start": 2788.48, "end": 2792.64, "text": " This is a standard PyTorch compatible data set.", "tokens": [639, 307, 257, 3832, 9953, 51, 284, 339, 18218, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.14938601818713512, "compression_ratio": 1.7555555555555555, "no_speech_prob": 8.01344958745176e-06}, {"id": 595, "seek": 276872, "start": 2792.64, "end": 2794.7599999999998, "text": " This is a standard PyTorch compatible data loader.", "tokens": [639, 307, 257, 3832, 9953, 51, 284, 339, 18218, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.14938601818713512, "compression_ratio": 1.7555555555555555, "no_speech_prob": 8.01344958745176e-06}, {"id": 596, "seek": 279476, "start": 2794.76, "end": 2799.4, "text": " Behind the scenes, it's actually using the fast AI version because I do need to do this", "tokens": [20475, 264, 8026, 11, 309, 311, 767, 1228, 264, 2370, 7318, 3037, 570, 286, 360, 643, 281, 360, 341], "temperature": 0.0, "avg_logprob": -0.19651574487084741, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.527936759288423e-06}, {"id": 597, "seek": 279476, "start": 2799.4, "end": 2800.92, "text": " automatic padding for convenience.", "tokens": [12509, 39562, 337, 19283, 13], "temperature": 0.0, "avg_logprob": -0.19651574487084741, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.527936759288423e-06}, {"id": 598, "seek": 279476, "start": 2800.92, "end": 2807.0800000000004, "text": " There's a few tweaks in our version that are a bit faster and a bit more convenient.", "tokens": [821, 311, 257, 1326, 46664, 294, 527, 3037, 300, 366, 257, 857, 4663, 293, 257, 857, 544, 10851, 13], "temperature": 0.0, "avg_logprob": -0.19651574487084741, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.527936759288423e-06}, {"id": 599, "seek": 279476, "start": 2807.0800000000004, "end": 2812.5200000000004, "text": " The fast AI samplers we're using, but there's not too much going on here.", "tokens": [440, 2370, 7318, 3247, 564, 433, 321, 434, 1228, 11, 457, 456, 311, 406, 886, 709, 516, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.19651574487084741, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.527936759288423e-06}, {"id": 600, "seek": 279476, "start": 2812.5200000000004, "end": 2815.2000000000003, "text": " So now we've got our model data object.", "tokens": [407, 586, 321, 600, 658, 527, 2316, 1412, 2657, 13], "temperature": 0.0, "avg_logprob": -0.19651574487084741, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.527936759288423e-06}, {"id": 601, "seek": 279476, "start": 2815.2000000000003, "end": 2819.82, "text": " We can basically tick off number one.", "tokens": [492, 393, 1936, 5204, 766, 1230, 472, 13], "temperature": 0.0, "avg_logprob": -0.19651574487084741, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.527936759288423e-06}, {"id": 602, "seek": 279476, "start": 2819.82, "end": 2824.0800000000004, "text": " So as I said, most of the work is in the architecture.", "tokens": [407, 382, 286, 848, 11, 881, 295, 264, 589, 307, 294, 264, 9482, 13], "temperature": 0.0, "avg_logprob": -0.19651574487084741, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.527936759288423e-06}, {"id": 603, "seek": 282408, "start": 2824.08, "end": 2835.4, "text": " And so the architecture is going to take our sequence of tokens, it's going to spit them", "tokens": [400, 370, 264, 9482, 307, 516, 281, 747, 527, 8310, 295, 22667, 11, 309, 311, 516, 281, 22127, 552], "temperature": 0.0, "avg_logprob": -0.16409662607553843, "compression_ratio": 1.5380434782608696, "no_speech_prob": 6.144151484477334e-06}, {"id": 604, "seek": 282408, "start": 2835.4, "end": 2844.84, "text": " into an encoder, or in computer vision terms, what we've been calling a backbone, something", "tokens": [666, 364, 2058, 19866, 11, 420, 294, 3820, 5201, 2115, 11, 437, 321, 600, 668, 5141, 257, 34889, 11, 746], "temperature": 0.0, "avg_logprob": -0.16409662607553843, "compression_ratio": 1.5380434782608696, "no_speech_prob": 6.144151484477334e-06}, {"id": 605, "seek": 282408, "start": 2844.84, "end": 2848.6, "text": " that's going to try and turn this into some kind of representation.", "tokens": [300, 311, 516, 281, 853, 293, 1261, 341, 666, 512, 733, 295, 10290, 13], "temperature": 0.0, "avg_logprob": -0.16409662607553843, "compression_ratio": 1.5380434782608696, "no_speech_prob": 6.144151484477334e-06}, {"id": 606, "seek": 282408, "start": 2848.6, "end": 2851.96, "text": " So that's just going to be an RNN.", "tokens": [407, 300, 311, 445, 516, 281, 312, 364, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.16409662607553843, "compression_ratio": 1.5380434782608696, "no_speech_prob": 6.144151484477334e-06}, {"id": 607, "seek": 285196, "start": 2851.96, "end": 2862.0, "text": " That's going to spit out the final hidden state, which for each sentence is just a vector.", "tokens": [663, 311, 516, 281, 22127, 484, 264, 2572, 7633, 1785, 11, 597, 337, 1184, 8174, 307, 445, 257, 8062, 13], "temperature": 0.0, "avg_logprob": -0.11990602392899363, "compression_ratio": 1.8049792531120332, "no_speech_prob": 4.092893505003303e-06}, {"id": 608, "seek": 285196, "start": 2862.0, "end": 2865.6, "text": " And so that's all going to take, none of this is going to be new.", "tokens": [400, 370, 300, 311, 439, 516, 281, 747, 11, 6022, 295, 341, 307, 516, 281, 312, 777, 13], "temperature": 0.0, "avg_logprob": -0.11990602392899363, "compression_ratio": 1.8049792531120332, "no_speech_prob": 4.092893505003303e-06}, {"id": 609, "seek": 285196, "start": 2865.6, "end": 2870.2400000000002, "text": " That's all going to be using very direct simple techniques that we've already learnt.", "tokens": [663, 311, 439, 516, 281, 312, 1228, 588, 2047, 2199, 7512, 300, 321, 600, 1217, 18991, 13], "temperature": 0.0, "avg_logprob": -0.11990602392899363, "compression_ratio": 1.8049792531120332, "no_speech_prob": 4.092893505003303e-06}, {"id": 610, "seek": 285196, "start": 2870.2400000000002, "end": 2875.0, "text": " And then we're going to take that and we're going to spit it into a different RNN, which", "tokens": [400, 550, 321, 434, 516, 281, 747, 300, 293, 321, 434, 516, 281, 22127, 309, 666, 257, 819, 45702, 45, 11, 597], "temperature": 0.0, "avg_logprob": -0.11990602392899363, "compression_ratio": 1.8049792531120332, "no_speech_prob": 4.092893505003303e-06}, {"id": 611, "seek": 285196, "start": 2875.0, "end": 2876.0, "text": " is a decoder.", "tokens": [307, 257, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.11990602392899363, "compression_ratio": 1.8049792531120332, "no_speech_prob": 4.092893505003303e-06}, {"id": 612, "seek": 285196, "start": 2876.0, "end": 2879.7400000000002, "text": " And that's going to have some new stuff because we need something that can go through one", "tokens": [400, 300, 311, 516, 281, 362, 512, 777, 1507, 570, 321, 643, 746, 300, 393, 352, 807, 472], "temperature": 0.0, "avg_logprob": -0.11990602392899363, "compression_ratio": 1.8049792531120332, "no_speech_prob": 4.092893505003303e-06}, {"id": 613, "seek": 287974, "start": 2879.74, "end": 2883.8799999999997, "text": " word at a time.", "tokens": [1349, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.16784123011997767, "compression_ratio": 1.9035532994923858, "no_speech_prob": 3.7266211165842833e-06}, {"id": 614, "seek": 287974, "start": 2883.8799999999997, "end": 2886.68, "text": " And it's going to keep going until it thinks it's finished the sentence.", "tokens": [400, 309, 311, 516, 281, 1066, 516, 1826, 309, 7309, 309, 311, 4335, 264, 8174, 13], "temperature": 0.0, "avg_logprob": -0.16784123011997767, "compression_ratio": 1.9035532994923858, "no_speech_prob": 3.7266211165842833e-06}, {"id": 615, "seek": 287974, "start": 2886.68, "end": 2890.68, "text": " It doesn't know how long the sentence is going to be ahead of time.", "tokens": [467, 1177, 380, 458, 577, 938, 264, 8174, 307, 516, 281, 312, 2286, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.16784123011997767, "compression_ratio": 1.9035532994923858, "no_speech_prob": 3.7266211165842833e-06}, {"id": 616, "seek": 287974, "start": 2890.68, "end": 2893.8399999999997, "text": " Keeps going until it thinks it's finished the sentence and then it stops and returns", "tokens": [5527, 82, 516, 1826, 309, 7309, 309, 311, 4335, 264, 8174, 293, 550, 309, 10094, 293, 11247], "temperature": 0.0, "avg_logprob": -0.16784123011997767, "compression_ratio": 1.9035532994923858, "no_speech_prob": 3.7266211165842833e-06}, {"id": 617, "seek": 287974, "start": 2893.8399999999997, "end": 2895.68, "text": " the sentence.", "tokens": [264, 8174, 13], "temperature": 0.0, "avg_logprob": -0.16784123011997767, "compression_ratio": 1.9035532994923858, "no_speech_prob": 3.7266211165842833e-06}, {"id": 618, "seek": 287974, "start": 2895.68, "end": 2899.8399999999997, "text": " So let's start with the encoder.", "tokens": [407, 718, 311, 722, 365, 264, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.16784123011997767, "compression_ratio": 1.9035532994923858, "no_speech_prob": 3.7266211165842833e-06}, {"id": 619, "seek": 287974, "start": 2899.8399999999997, "end": 2907.02, "text": " So in terms of variable naming here, there's basically identical variables for encoder", "tokens": [407, 294, 2115, 295, 7006, 25290, 510, 11, 456, 311, 1936, 14800, 9102, 337, 2058, 19866], "temperature": 0.0, "avg_logprob": -0.16784123011997767, "compression_ratio": 1.9035532994923858, "no_speech_prob": 3.7266211165842833e-06}, {"id": 620, "seek": 290702, "start": 2907.02, "end": 2914.24, "text": " and decoder, the encoder versions have enc, the decoder versions have dec.", "tokens": [293, 979, 19866, 11, 264, 2058, 19866, 9606, 362, 2058, 11, 264, 979, 19866, 9606, 362, 979, 13], "temperature": 0.0, "avg_logprob": -0.16291844844818115, "compression_ratio": 1.7309941520467835, "no_speech_prob": 3.844912953354651e-06}, {"id": 621, "seek": 290702, "start": 2914.24, "end": 2919.16, "text": " So for the encoder, here's our embeddings.", "tokens": [407, 337, 264, 2058, 19866, 11, 510, 311, 527, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.16291844844818115, "compression_ratio": 1.7309941520467835, "no_speech_prob": 3.844912953354651e-06}, {"id": 622, "seek": 290702, "start": 2919.16, "end": 2927.4, "text": " And so I always try to mention what the mnemonics are rather than writing things out in too", "tokens": [400, 370, 286, 1009, 853, 281, 2152, 437, 264, 275, 25989, 266, 1167, 366, 2831, 813, 3579, 721, 484, 294, 886], "temperature": 0.0, "avg_logprob": -0.16291844844818115, "compression_ratio": 1.7309941520467835, "no_speech_prob": 3.844912953354651e-06}, {"id": 623, "seek": 290702, "start": 2927.4, "end": 2928.96, "text": " long hand.", "tokens": [938, 1011, 13], "temperature": 0.0, "avg_logprob": -0.16291844844818115, "compression_ratio": 1.7309941520467835, "no_speech_prob": 3.844912953354651e-06}, {"id": 624, "seek": 290702, "start": 2928.96, "end": 2934.7599999999998, "text": " So just remember, enc is an encoder, dec is a decoder, env is an embedding.", "tokens": [407, 445, 1604, 11, 2058, 307, 364, 2058, 19866, 11, 979, 307, 257, 979, 19866, 11, 2267, 307, 364, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.16291844844818115, "compression_ratio": 1.7309941520467835, "no_speech_prob": 3.844912953354651e-06}, {"id": 625, "seek": 293476, "start": 2934.76, "end": 2937.7200000000003, "text": " The final thing that comes out is out.", "tokens": [440, 2572, 551, 300, 1487, 484, 307, 484, 13], "temperature": 0.0, "avg_logprob": -0.15359051704406737, "compression_ratio": 1.57, "no_speech_prob": 1.9033774378840462e-06}, {"id": 626, "seek": 293476, "start": 2937.7200000000003, "end": 2942.76, "text": " The RNN in this case is a GRU, not an LSTM.", "tokens": [440, 45702, 45, 294, 341, 1389, 307, 257, 10903, 52, 11, 406, 364, 441, 6840, 44, 13], "temperature": 0.0, "avg_logprob": -0.15359051704406737, "compression_ratio": 1.57, "no_speech_prob": 1.9033774378840462e-06}, {"id": 627, "seek": 293476, "start": 2942.76, "end": 2943.76, "text": " They're nearly the same thing.", "tokens": [814, 434, 6217, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.15359051704406737, "compression_ratio": 1.57, "no_speech_prob": 1.9033774378840462e-06}, {"id": 628, "seek": 293476, "start": 2943.76, "end": 2945.0400000000004, "text": " So don't worry about the difference.", "tokens": [407, 500, 380, 3292, 466, 264, 2649, 13], "temperature": 0.0, "avg_logprob": -0.15359051704406737, "compression_ratio": 1.57, "no_speech_prob": 1.9033774378840462e-06}, {"id": 629, "seek": 293476, "start": 2945.0400000000004, "end": 2948.32, "text": " You could replace it with an LSTM and you'll get basically the same results.", "tokens": [509, 727, 7406, 309, 365, 364, 441, 6840, 44, 293, 291, 603, 483, 1936, 264, 912, 3542, 13], "temperature": 0.0, "avg_logprob": -0.15359051704406737, "compression_ratio": 1.57, "no_speech_prob": 1.9033774378840462e-06}, {"id": 630, "seek": 293476, "start": 2948.32, "end": 2957.48, "text": " To replace it with an LSTM, simply type lstm.", "tokens": [1407, 7406, 309, 365, 364, 441, 6840, 44, 11, 2935, 2010, 287, 372, 76, 13], "temperature": 0.0, "avg_logprob": -0.15359051704406737, "compression_ratio": 1.57, "no_speech_prob": 1.9033774378840462e-06}, {"id": 631, "seek": 293476, "start": 2957.48, "end": 2962.92, "text": " So we need to create an embedding layer.", "tokens": [407, 321, 643, 281, 1884, 364, 12240, 3584, 4583, 13], "temperature": 0.0, "avg_logprob": -0.15359051704406737, "compression_ratio": 1.57, "no_speech_prob": 1.9033774378840462e-06}, {"id": 632, "seek": 296292, "start": 2962.92, "end": 2967.2400000000002, "text": " Remember what we're being passed is the index of the words into a vocabulary, and we want", "tokens": [5459, 437, 321, 434, 885, 4678, 307, 264, 8186, 295, 264, 2283, 666, 257, 19864, 11, 293, 321, 528], "temperature": 0.0, "avg_logprob": -0.12207242525540865, "compression_ratio": 1.7653846153846153, "no_speech_prob": 5.507576588570373e-06}, {"id": 633, "seek": 296292, "start": 2967.2400000000002, "end": 2970.28, "text": " to grab their fast text embedding.", "tokens": [281, 4444, 641, 2370, 2487, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.12207242525540865, "compression_ratio": 1.7653846153846153, "no_speech_prob": 5.507576588570373e-06}, {"id": 634, "seek": 296292, "start": 2970.28, "end": 2977.6800000000003, "text": " And then over time, we might want to also fine-tune to train that embedding end-to-end.", "tokens": [400, 550, 670, 565, 11, 321, 1062, 528, 281, 611, 2489, 12, 83, 2613, 281, 3847, 300, 12240, 3584, 917, 12, 1353, 12, 521, 13], "temperature": 0.0, "avg_logprob": -0.12207242525540865, "compression_ratio": 1.7653846153846153, "no_speech_prob": 5.507576588570373e-06}, {"id": 635, "seek": 296292, "start": 2977.6800000000003, "end": 2980.88, "text": " So to create an embedding, we'll call create embedding up here.", "tokens": [407, 281, 1884, 364, 12240, 3584, 11, 321, 603, 818, 1884, 12240, 3584, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.12207242525540865, "compression_ratio": 1.7653846153846153, "no_speech_prob": 5.507576588570373e-06}, {"id": 636, "seek": 296292, "start": 2980.88, "end": 2983.8, "text": " So we'll just say nn.embedding.", "tokens": [407, 321, 603, 445, 584, 297, 77, 13, 443, 2883, 3584, 13], "temperature": 0.0, "avg_logprob": -0.12207242525540865, "compression_ratio": 1.7653846153846153, "no_speech_prob": 5.507576588570373e-06}, {"id": 637, "seek": 296292, "start": 2983.8, "end": 2989.2000000000003, "text": " So it's important that you know now how to set the rows and columns for your embedding.", "tokens": [407, 309, 311, 1021, 300, 291, 458, 586, 577, 281, 992, 264, 13241, 293, 13766, 337, 428, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.12207242525540865, "compression_ratio": 1.7653846153846153, "no_speech_prob": 5.507576588570373e-06}, {"id": 638, "seek": 296292, "start": 2989.2000000000003, "end": 2992.76, "text": " So the number of rows has to be equal to your vocabulary size.", "tokens": [407, 264, 1230, 295, 13241, 575, 281, 312, 2681, 281, 428, 19864, 2744, 13], "temperature": 0.0, "avg_logprob": -0.12207242525540865, "compression_ratio": 1.7653846153846153, "no_speech_prob": 5.507576588570373e-06}, {"id": 639, "seek": 299276, "start": 2992.76, "end": 2996.32, "text": " So each vocabulary item has a word vector.", "tokens": [407, 1184, 19864, 3174, 575, 257, 1349, 8062, 13], "temperature": 0.0, "avg_logprob": -0.14452828082841696, "compression_ratio": 1.6338028169014085, "no_speech_prob": 3.6119708965998143e-06}, {"id": 640, "seek": 299276, "start": 2996.32, "end": 2998.0, "text": " And how big is your embedding?", "tokens": [400, 577, 955, 307, 428, 12240, 3584, 30], "temperature": 0.0, "avg_logprob": -0.14452828082841696, "compression_ratio": 1.6338028169014085, "no_speech_prob": 3.6119708965998143e-06}, {"id": 641, "seek": 299276, "start": 2998.0, "end": 3002.28, "text": " Well, in this case, it was determined by fast text.", "tokens": [1042, 11, 294, 341, 1389, 11, 309, 390, 9540, 538, 2370, 2487, 13], "temperature": 0.0, "avg_logprob": -0.14452828082841696, "compression_ratio": 1.6338028169014085, "no_speech_prob": 3.6119708965998143e-06}, {"id": 642, "seek": 299276, "start": 3002.28, "end": 3004.7200000000003, "text": " And the fast text embeddings are size 300.", "tokens": [400, 264, 2370, 2487, 12240, 29432, 366, 2744, 6641, 13], "temperature": 0.0, "avg_logprob": -0.14452828082841696, "compression_ratio": 1.6338028169014085, "no_speech_prob": 3.6119708965998143e-06}, {"id": 643, "seek": 299276, "start": 3004.7200000000003, "end": 3014.0400000000004, "text": " So we have to use size 300 as well, otherwise we can't start out by using their embeddings.", "tokens": [407, 321, 362, 281, 764, 2744, 6641, 382, 731, 11, 5911, 321, 393, 380, 722, 484, 538, 1228, 641, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.14452828082841696, "compression_ratio": 1.6338028169014085, "no_speech_prob": 3.6119708965998143e-06}, {"id": 644, "seek": 299276, "start": 3014.0400000000004, "end": 3018.8, "text": " So what we want to do is this is initially going to give us a random set of embeddings.", "tokens": [407, 437, 321, 528, 281, 360, 307, 341, 307, 9105, 516, 281, 976, 505, 257, 4974, 992, 295, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.14452828082841696, "compression_ratio": 1.6338028169014085, "no_speech_prob": 3.6119708965998143e-06}, {"id": 645, "seek": 301880, "start": 3018.8, "end": 3023.0, "text": " And so we're going to now go through each one of these, and if we find it in fast text,", "tokens": [400, 370, 321, 434, 516, 281, 586, 352, 807, 1184, 472, 295, 613, 11, 293, 498, 321, 915, 309, 294, 2370, 2487, 11], "temperature": 0.0, "avg_logprob": -0.12378383895098152, "compression_ratio": 1.823293172690763, "no_speech_prob": 4.860425178776495e-06}, {"id": 646, "seek": 301880, "start": 3023.0, "end": 3025.6400000000003, "text": " we'll replace it with a fast text embedding.", "tokens": [321, 603, 7406, 309, 365, 257, 2370, 2487, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.12378383895098152, "compression_ratio": 1.823293172690763, "no_speech_prob": 4.860425178776495e-06}, {"id": 647, "seek": 301880, "start": 3025.6400000000003, "end": 3034.1400000000003, "text": " So again, something that you should already know is that a PyTorch module that is learnable", "tokens": [407, 797, 11, 746, 300, 291, 820, 1217, 458, 307, 300, 257, 9953, 51, 284, 339, 10088, 300, 307, 1466, 712], "temperature": 0.0, "avg_logprob": -0.12378383895098152, "compression_ratio": 1.823293172690763, "no_speech_prob": 4.860425178776495e-06}, {"id": 648, "seek": 301880, "start": 3034.1400000000003, "end": 3040.96, "text": " has a weight attribute, and the weight attribute is a variable, and the variables have a data", "tokens": [575, 257, 3364, 19667, 11, 293, 264, 3364, 19667, 307, 257, 7006, 11, 293, 264, 9102, 362, 257, 1412], "temperature": 0.0, "avg_logprob": -0.12378383895098152, "compression_ratio": 1.823293172690763, "no_speech_prob": 4.860425178776495e-06}, {"id": 649, "seek": 301880, "start": 3040.96, "end": 3043.7200000000003, "text": " attribute, and the data attribute is a tensor.", "tokens": [19667, 11, 293, 264, 1412, 19667, 307, 257, 40863, 13], "temperature": 0.0, "avg_logprob": -0.12378383895098152, "compression_ratio": 1.823293172690763, "no_speech_prob": 4.860425178776495e-06}, {"id": 650, "seek": 301880, "start": 3043.7200000000003, "end": 3048.46, "text": " Now, you'll notice very often today I'm saying here is something you should know, not so", "tokens": [823, 11, 291, 603, 3449, 588, 2049, 965, 286, 478, 1566, 510, 307, 746, 291, 820, 458, 11, 406, 370], "temperature": 0.0, "avg_logprob": -0.12378383895098152, "compression_ratio": 1.823293172690763, "no_speech_prob": 4.860425178776495e-06}, {"id": 651, "seek": 304846, "start": 3048.46, "end": 3053.78, "text": " that you think, oh, I don't know that, I'm a bad person, but so that you think, okay,", "tokens": [300, 291, 519, 11, 1954, 11, 286, 500, 380, 458, 300, 11, 286, 478, 257, 1578, 954, 11, 457, 370, 300, 291, 519, 11, 1392, 11], "temperature": 0.0, "avg_logprob": -0.11196433520707928, "compression_ratio": 1.8944954128440368, "no_speech_prob": 9.516192221781239e-06}, {"id": 652, "seek": 304846, "start": 3053.78, "end": 3061.04, "text": " this is a concept that I haven't learned yet, and Jeremy thinks I ought to know about, and", "tokens": [341, 307, 257, 3410, 300, 286, 2378, 380, 3264, 1939, 11, 293, 17809, 7309, 286, 13416, 281, 458, 466, 11, 293], "temperature": 0.0, "avg_logprob": -0.11196433520707928, "compression_ratio": 1.8944954128440368, "no_speech_prob": 9.516192221781239e-06}, {"id": 653, "seek": 304846, "start": 3061.04, "end": 3065.64, "text": " so I've got to write that down, and I'm going to go home, and I'm going to Google, because", "tokens": [370, 286, 600, 658, 281, 2464, 300, 760, 11, 293, 286, 478, 516, 281, 352, 1280, 11, 293, 286, 478, 516, 281, 3329, 11, 570], "temperature": 0.0, "avg_logprob": -0.11196433520707928, "compression_ratio": 1.8944954128440368, "no_speech_prob": 9.516192221781239e-06}, {"id": 654, "seek": 304846, "start": 3065.64, "end": 3071.3, "text": " this is a normal PyTorch attribute in every single learnable PyTorch module.", "tokens": [341, 307, 257, 2710, 9953, 51, 284, 339, 19667, 294, 633, 2167, 1466, 712, 9953, 51, 284, 339, 10088, 13], "temperature": 0.0, "avg_logprob": -0.11196433520707928, "compression_ratio": 1.8944954128440368, "no_speech_prob": 9.516192221781239e-06}, {"id": 655, "seek": 304846, "start": 3071.3, "end": 3075.86, "text": " This is a normal PyTorch attribute in every single PyTorch variable.", "tokens": [639, 307, 257, 2710, 9953, 51, 284, 339, 19667, 294, 633, 2167, 9953, 51, 284, 339, 7006, 13], "temperature": 0.0, "avg_logprob": -0.11196433520707928, "compression_ratio": 1.8944954128440368, "no_speech_prob": 9.516192221781239e-06}, {"id": 656, "seek": 307586, "start": 3075.86, "end": 3079.32, "text": " And so if you don't know how to grab the weights out of a module, or you don't know how to", "tokens": [400, 370, 498, 291, 500, 380, 458, 577, 281, 4444, 264, 17443, 484, 295, 257, 10088, 11, 420, 291, 500, 380, 458, 577, 281], "temperature": 0.0, "avg_logprob": -0.10001797553820488, "compression_ratio": 2.013745704467354, "no_speech_prob": 1.4970953088777605e-05}, {"id": 657, "seek": 307586, "start": 3079.32, "end": 3083.8, "text": " grab the tensor out of a variable, it's going to be hard for you to build new things, or", "tokens": [4444, 264, 40863, 484, 295, 257, 7006, 11, 309, 311, 516, 281, 312, 1152, 337, 291, 281, 1322, 777, 721, 11, 420], "temperature": 0.0, "avg_logprob": -0.10001797553820488, "compression_ratio": 2.013745704467354, "no_speech_prob": 1.4970953088777605e-05}, {"id": 658, "seek": 307586, "start": 3083.8, "end": 3086.48, "text": " to debug things, or maintain things, or whatever.", "tokens": [281, 24083, 721, 11, 420, 6909, 721, 11, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.10001797553820488, "compression_ratio": 2.013745704467354, "no_speech_prob": 1.4970953088777605e-05}, {"id": 659, "seek": 307586, "start": 3086.48, "end": 3091.2400000000002, "text": " So if I say, you ought to know this, and you're thinking, I don't know this, don't run away", "tokens": [407, 498, 286, 584, 11, 291, 13416, 281, 458, 341, 11, 293, 291, 434, 1953, 11, 286, 500, 380, 458, 341, 11, 500, 380, 1190, 1314], "temperature": 0.0, "avg_logprob": -0.10001797553820488, "compression_ratio": 2.013745704467354, "no_speech_prob": 1.4970953088777605e-05}, {"id": 660, "seek": 307586, "start": 3091.2400000000002, "end": 3096.2400000000002, "text": " and hide, go home and learn the thing, and if you're having trouble learning the thing,", "tokens": [293, 6479, 11, 352, 1280, 293, 1466, 264, 551, 11, 293, 498, 291, 434, 1419, 5253, 2539, 264, 551, 11], "temperature": 0.0, "avg_logprob": -0.10001797553820488, "compression_ratio": 2.013745704467354, "no_speech_prob": 1.4970953088777605e-05}, {"id": 661, "seek": 307586, "start": 3096.2400000000002, "end": 3100.1200000000003, "text": " because you can't find documentation about it, or you don't understand that documentation,", "tokens": [570, 291, 393, 380, 915, 14333, 466, 309, 11, 420, 291, 500, 380, 1223, 300, 14333, 11], "temperature": 0.0, "avg_logprob": -0.10001797553820488, "compression_ratio": 2.013745704467354, "no_speech_prob": 1.4970953088777605e-05}, {"id": 662, "seek": 307586, "start": 3100.1200000000003, "end": 3104.1200000000003, "text": " or you don't know why Jeremy thought it was important you know it, jump on the forum,", "tokens": [420, 291, 500, 380, 458, 983, 17809, 1194, 309, 390, 1021, 291, 458, 309, 11, 3012, 322, 264, 17542, 11], "temperature": 0.0, "avg_logprob": -0.10001797553820488, "compression_ratio": 2.013745704467354, "no_speech_prob": 1.4970953088777605e-05}, {"id": 663, "seek": 310412, "start": 3104.12, "end": 3108.96, "text": " and say, please explain this thing, here's my best understanding of that thing, as I", "tokens": [293, 584, 11, 1767, 2903, 341, 551, 11, 510, 311, 452, 1151, 3701, 295, 300, 551, 11, 382, 286], "temperature": 0.0, "avg_logprob": -0.16503419788605575, "compression_ratio": 1.7136929460580912, "no_speech_prob": 7.646483936696313e-06}, {"id": 664, "seek": 310412, "start": 3108.96, "end": 3114.48, "text": " have it at the moment, here's the resources I've looked at, help fill me in.", "tokens": [362, 309, 412, 264, 1623, 11, 510, 311, 264, 3593, 286, 600, 2956, 412, 11, 854, 2836, 385, 294, 13], "temperature": 0.0, "avg_logprob": -0.16503419788605575, "compression_ratio": 1.7136929460580912, "no_speech_prob": 7.646483936696313e-06}, {"id": 665, "seek": 310412, "start": 3114.48, "end": 3120.56, "text": " And normally, if I respond, it's very likely I will not tell you the answer, but I will", "tokens": [400, 5646, 11, 498, 286, 4196, 11, 309, 311, 588, 3700, 286, 486, 406, 980, 291, 264, 1867, 11, 457, 286, 486], "temperature": 0.0, "avg_logprob": -0.16503419788605575, "compression_ratio": 1.7136929460580912, "no_speech_prob": 7.646483936696313e-06}, {"id": 666, "seek": 310412, "start": 3120.56, "end": 3125.96, "text": " instead give you something, a problem that you could solve, that if you solve it, will", "tokens": [2602, 976, 291, 746, 11, 257, 1154, 300, 291, 727, 5039, 11, 300, 498, 291, 5039, 309, 11, 486], "temperature": 0.0, "avg_logprob": -0.16503419788605575, "compression_ratio": 1.7136929460580912, "no_speech_prob": 7.646483936696313e-06}, {"id": 667, "seek": 310412, "start": 3125.96, "end": 3130.64, "text": " solve it for you, because I know that way it will be something you remember.", "tokens": [5039, 309, 337, 291, 11, 570, 286, 458, 300, 636, 309, 486, 312, 746, 291, 1604, 13], "temperature": 0.0, "avg_logprob": -0.16503419788605575, "compression_ratio": 1.7136929460580912, "no_speech_prob": 7.646483936696313e-06}, {"id": 668, "seek": 313064, "start": 3130.64, "end": 3135.12, "text": " So again, don't be put off if I'm like, okay, go read this link, try and summarize that", "tokens": [407, 797, 11, 500, 380, 312, 829, 766, 498, 286, 478, 411, 11, 1392, 11, 352, 1401, 341, 2113, 11, 853, 293, 20858, 300], "temperature": 0.0, "avg_logprob": -0.17637644404858616, "compression_ratio": 1.691304347826087, "no_speech_prob": 6.1441164689313155e-06}, {"id": 669, "seek": 313064, "start": 3135.12, "end": 3138.96, "text": " thing, tell us what you think, I'm trying to be helpful, not unhelpful, and if you're", "tokens": [551, 11, 980, 505, 437, 291, 519, 11, 286, 478, 1382, 281, 312, 4961, 11, 406, 517, 37451, 906, 11, 293, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.17637644404858616, "compression_ratio": 1.691304347826087, "no_speech_prob": 6.1441164689313155e-06}, {"id": 670, "seek": 313064, "start": 3138.96, "end": 3145.04, "text": " still not following, just come back and say, I had a look, honestly, that link you sent,", "tokens": [920, 406, 3480, 11, 445, 808, 646, 293, 584, 11, 286, 632, 257, 574, 11, 6095, 11, 300, 2113, 291, 2279, 11], "temperature": 0.0, "avg_logprob": -0.17637644404858616, "compression_ratio": 1.691304347826087, "no_speech_prob": 6.1441164689313155e-06}, {"id": 671, "seek": 313064, "start": 3145.04, "end": 3148.72, "text": " I don't know what it means, I don't know where to start, whatever.", "tokens": [286, 500, 380, 458, 437, 309, 1355, 11, 286, 500, 380, 458, 689, 281, 722, 11, 2035, 13], "temperature": 0.0, "avg_logprob": -0.17637644404858616, "compression_ratio": 1.691304347826087, "no_speech_prob": 6.1441164689313155e-06}, {"id": 672, "seek": 313064, "start": 3148.72, "end": 3155.72, "text": " I'll keep trying to help you until you fully understand it.", "tokens": [286, 603, 1066, 1382, 281, 854, 291, 1826, 291, 4498, 1223, 309, 13], "temperature": 0.0, "avg_logprob": -0.17637644404858616, "compression_ratio": 1.691304347826087, "no_speech_prob": 6.1441164689313155e-06}, {"id": 673, "seek": 315572, "start": 3155.72, "end": 3162.48, "text": " So now that we've got our weight tensor, we can just go through our vocabulary, and we", "tokens": [407, 586, 300, 321, 600, 658, 527, 3364, 40863, 11, 321, 393, 445, 352, 807, 527, 19864, 11, 293, 321], "temperature": 0.0, "avg_logprob": -0.11243585709038131, "compression_ratio": 1.821917808219178, "no_speech_prob": 4.539602741715498e-05}, {"id": 674, "seek": 315572, "start": 3162.48, "end": 3169.3599999999997, "text": " can look up the word in our pre-trained vectors, and if we find it, we will replace the random", "tokens": [393, 574, 493, 264, 1349, 294, 527, 659, 12, 17227, 2001, 18875, 11, 293, 498, 321, 915, 309, 11, 321, 486, 7406, 264, 4974], "temperature": 0.0, "avg_logprob": -0.11243585709038131, "compression_ratio": 1.821917808219178, "no_speech_prob": 4.539602741715498e-05}, {"id": 675, "seek": 315572, "start": 3169.3599999999997, "end": 3172.72, "text": " weights with that pre-trained vector.", "tokens": [17443, 365, 300, 659, 12, 17227, 2001, 8062, 13], "temperature": 0.0, "avg_logprob": -0.11243585709038131, "compression_ratio": 1.821917808219178, "no_speech_prob": 4.539602741715498e-05}, {"id": 676, "seek": 315572, "start": 3172.72, "end": 3179.08, "text": " The random weights have a standard deviation of 1, our pre-trained vectors, it turned out,", "tokens": [440, 4974, 17443, 362, 257, 3832, 25163, 295, 502, 11, 527, 659, 12, 17227, 2001, 18875, 11, 309, 3574, 484, 11], "temperature": 0.0, "avg_logprob": -0.11243585709038131, "compression_ratio": 1.821917808219178, "no_speech_prob": 4.539602741715498e-05}, {"id": 677, "seek": 315572, "start": 3179.08, "end": 3183.2799999999997, "text": " had a standard deviation of about.3, so again, this is the kind of hacky thing I do when", "tokens": [632, 257, 3832, 25163, 295, 466, 2411, 18, 11, 370, 797, 11, 341, 307, 264, 733, 295, 10339, 88, 551, 286, 360, 562], "temperature": 0.0, "avg_logprob": -0.11243585709038131, "compression_ratio": 1.821917808219178, "no_speech_prob": 4.539602741715498e-05}, {"id": 678, "seek": 318328, "start": 3183.28, "end": 3187.52, "text": " I'm prototyping stuff, I just modify it by 3.", "tokens": [286, 478, 46219, 3381, 1507, 11, 286, 445, 16927, 309, 538, 805, 13], "temperature": 0.0, "avg_logprob": -0.20362404414585658, "compression_ratio": 1.641025641025641, "no_speech_prob": 6.240906714083394e-06}, {"id": 679, "seek": 318328, "start": 3187.52, "end": 3192.1600000000003, "text": " Obviously by the time you see the video of this, being able to put all this sequence-to-sequence", "tokens": [7580, 538, 264, 565, 291, 536, 264, 960, 295, 341, 11, 885, 1075, 281, 829, 439, 341, 8310, 12, 1353, 12, 11834, 655], "temperature": 0.0, "avg_logprob": -0.20362404414585658, "compression_ratio": 1.641025641025641, "no_speech_prob": 6.240906714083394e-06}, {"id": 680, "seek": 318328, "start": 3192.1600000000003, "end": 3197.2400000000002, "text": " stuff into the Fast.ai library, you won't find horrible hats like that in there, I sure", "tokens": [1507, 666, 264, 15968, 13, 1301, 6405, 11, 291, 1582, 380, 915, 9263, 20549, 411, 300, 294, 456, 11, 286, 988], "temperature": 0.0, "avg_logprob": -0.20362404414585658, "compression_ratio": 1.641025641025641, "no_speech_prob": 6.240906714083394e-06}, {"id": 681, "seek": 318328, "start": 3197.2400000000002, "end": 3203.1200000000003, "text": " hope, but hack away when you're prototyping.", "tokens": [1454, 11, 457, 10339, 1314, 562, 291, 434, 46219, 3381, 13], "temperature": 0.0, "avg_logprob": -0.20362404414585658, "compression_ratio": 1.641025641025641, "no_speech_prob": 6.240906714083394e-06}, {"id": 682, "seek": 318328, "start": 3203.1200000000003, "end": 3207.44, "text": " Some things won't be in Fast.txt, in which case we'll just keep track of it, and I've", "tokens": [2188, 721, 1582, 380, 312, 294, 15968, 13, 83, 734, 11, 294, 597, 1389, 321, 603, 445, 1066, 2837, 295, 309, 11, 293, 286, 600], "temperature": 0.0, "avg_logprob": -0.20362404414585658, "compression_ratio": 1.641025641025641, "no_speech_prob": 6.240906714083394e-06}, {"id": 683, "seek": 318328, "start": 3207.44, "end": 3212.1200000000003, "text": " just added this print statement here just so that I can kind of see what's going, like", "tokens": [445, 3869, 341, 4482, 5629, 510, 445, 370, 300, 286, 393, 733, 295, 536, 437, 311, 516, 11, 411], "temperature": 0.0, "avg_logprob": -0.20362404414585658, "compression_ratio": 1.641025641025641, "no_speech_prob": 6.240906714083394e-06}, {"id": 684, "seek": 321212, "start": 3212.12, "end": 3218.56, "text": " why am I missing stuff, basically, I'll probably comment it out when I actually commit this", "tokens": [983, 669, 286, 5361, 1507, 11, 1936, 11, 286, 603, 1391, 2871, 309, 484, 562, 286, 767, 5599, 341], "temperature": 0.0, "avg_logprob": -0.15814841134207588, "compression_ratio": 1.6624472573839661, "no_speech_prob": 1.165941921499325e-05}, {"id": 685, "seek": 321212, "start": 3218.56, "end": 3223.2, "text": " to GitHub, but that's why that's there.", "tokens": [281, 23331, 11, 457, 300, 311, 983, 300, 311, 456, 13], "temperature": 0.0, "avg_logprob": -0.15814841134207588, "compression_ratio": 1.6624472573839661, "no_speech_prob": 1.165941921499325e-05}, {"id": 686, "seek": 321212, "start": 3223.2, "end": 3227.56, "text": " So we create those embeddings, and so when we actually create the sequence-to-sequence", "tokens": [407, 321, 1884, 729, 12240, 29432, 11, 293, 370, 562, 321, 767, 1884, 264, 8310, 12, 1353, 12, 11834, 655], "temperature": 0.0, "avg_logprob": -0.15814841134207588, "compression_ratio": 1.6624472573839661, "no_speech_prob": 1.165941921499325e-05}, {"id": 687, "seek": 321212, "start": 3227.56, "end": 3235.0, "text": " RNN, it'll print out how many were missed, and so remember we had about 30,000 words,", "tokens": [45702, 45, 11, 309, 603, 4482, 484, 577, 867, 645, 6721, 11, 293, 370, 1604, 321, 632, 466, 2217, 11, 1360, 2283, 11], "temperature": 0.0, "avg_logprob": -0.15814841134207588, "compression_ratio": 1.6624472573839661, "no_speech_prob": 1.165941921499325e-05}, {"id": 688, "seek": 321212, "start": 3235.0, "end": 3239.6, "text": " so we're not missing too many, and interesting, the things that are missing, well there's", "tokens": [370, 321, 434, 406, 5361, 886, 867, 11, 293, 1880, 11, 264, 721, 300, 366, 5361, 11, 731, 456, 311], "temperature": 0.0, "avg_logprob": -0.15814841134207588, "compression_ratio": 1.6624472573839661, "no_speech_prob": 1.165941921499325e-05}, {"id": 689, "seek": 323960, "start": 3239.6, "end": 3245.2, "text": " our special token for uppercase, not surprising that's missing.", "tokens": [527, 2121, 14862, 337, 11775, 2869, 651, 11, 406, 8830, 300, 311, 5361, 13], "temperature": 0.0, "avg_logprob": -0.2133632178779121, "compression_ratio": 1.7196652719665273, "no_speech_prob": 1.4063804883335251e-05}, {"id": 690, "seek": 323960, "start": 3245.2, "end": 3251.68, "text": " Also remember it's not token-to-vec, it's not token-text, it does words, so L apostrophe", "tokens": [2743, 1604, 309, 311, 406, 14862, 12, 1353, 12, 303, 66, 11, 309, 311, 406, 14862, 12, 25111, 11, 309, 775, 2283, 11, 370, 441, 19484, 27194], "temperature": 0.0, "avg_logprob": -0.2133632178779121, "compression_ratio": 1.7196652719665273, "no_speech_prob": 1.4063804883335251e-05}, {"id": 691, "seek": 323960, "start": 3251.68, "end": 3255.52, "text": " and D apostrophe and apostrophe S, they're not appearing either.", "tokens": [293, 413, 19484, 27194, 293, 19484, 27194, 318, 11, 436, 434, 406, 19870, 2139, 13], "temperature": 0.0, "avg_logprob": -0.2133632178779121, "compression_ratio": 1.7196652719665273, "no_speech_prob": 1.4063804883335251e-05}, {"id": 692, "seek": 323960, "start": 3255.52, "end": 3259.64, "text": " So that's interesting, that does suggest that maybe we could have slightly better embeddings", "tokens": [407, 300, 311, 1880, 11, 300, 775, 3402, 300, 1310, 321, 727, 362, 4748, 1101, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.2133632178779121, "compression_ratio": 1.7196652719665273, "no_speech_prob": 1.4063804883335251e-05}, {"id": 693, "seek": 323960, "start": 3259.64, "end": 3265.16, "text": " if we tried to find some which would have been tokenized the same way we tokenized,", "tokens": [498, 321, 3031, 281, 915, 512, 597, 576, 362, 668, 14862, 1602, 264, 912, 636, 321, 14862, 1602, 11], "temperature": 0.0, "avg_logprob": -0.2133632178779121, "compression_ratio": 1.7196652719665273, "no_speech_prob": 1.4063804883335251e-05}, {"id": 694, "seek": 323960, "start": 3265.16, "end": 3267.16, "text": " but that's okay.", "tokens": [457, 300, 311, 1392, 13], "temperature": 0.0, "avg_logprob": -0.2133632178779121, "compression_ratio": 1.7196652719665273, "no_speech_prob": 1.4063804883335251e-05}, {"id": 695, "seek": 326716, "start": 3267.16, "end": 3273.04, "text": " Do we just keep embedding vectors from training, why don't we keep all word embeddings in case", "tokens": [1144, 321, 445, 1066, 12240, 3584, 18875, 490, 3097, 11, 983, 500, 380, 321, 1066, 439, 1349, 12240, 29432, 294, 1389], "temperature": 0.0, "avg_logprob": -0.20367490617852463, "compression_ratio": 1.4855491329479769, "no_speech_prob": 7.368270598817617e-05}, {"id": 696, "seek": 326716, "start": 3273.04, "end": 3282.8399999999997, "text": " you have new words in the test set?", "tokens": [291, 362, 777, 2283, 294, 264, 1500, 992, 30], "temperature": 0.0, "avg_logprob": -0.20367490617852463, "compression_ratio": 1.4855491329479769, "no_speech_prob": 7.368270598817617e-05}, {"id": 697, "seek": 326716, "start": 3282.8399999999997, "end": 3290.44, "text": " We're going to be fine-tuning them, and so, I don't know, it's an interesting idea, maybe", "tokens": [492, 434, 516, 281, 312, 2489, 12, 83, 37726, 552, 11, 293, 370, 11, 286, 500, 380, 458, 11, 309, 311, 364, 1880, 1558, 11, 1310], "temperature": 0.0, "avg_logprob": -0.20367490617852463, "compression_ratio": 1.4855491329479769, "no_speech_prob": 7.368270598817617e-05}, {"id": 698, "seek": 326716, "start": 3290.44, "end": 3296.92, "text": " that would work, I haven't tried it.", "tokens": [300, 576, 589, 11, 286, 2378, 380, 3031, 309, 13], "temperature": 0.0, "avg_logprob": -0.20367490617852463, "compression_ratio": 1.4855491329479769, "no_speech_prob": 7.368270598817617e-05}, {"id": 699, "seek": 329692, "start": 3296.92, "end": 3302.04, "text": " Also you wouldn't, sorry can you use that?", "tokens": [2743, 291, 2759, 380, 11, 2597, 393, 291, 764, 300, 30], "temperature": 0.0, "avg_logprob": -0.22786676507247122, "compression_ratio": 1.6572769953051643, "no_speech_prob": 3.4266860893694684e-05}, {"id": 700, "seek": 329692, "start": 3302.04, "end": 3307.48, "text": " I asked the question, so you can also add random embedding to those, and at the beginning", "tokens": [286, 2351, 264, 1168, 11, 370, 291, 393, 611, 909, 4974, 12240, 3584, 281, 729, 11, 293, 412, 264, 2863], "temperature": 0.0, "avg_logprob": -0.22786676507247122, "compression_ratio": 1.6572769953051643, "no_speech_prob": 3.4266860893694684e-05}, {"id": 701, "seek": 329692, "start": 3307.48, "end": 3313.12, "text": " just keep them random, but you're going to have, it's going to make an effect in the", "tokens": [445, 1066, 552, 4974, 11, 457, 291, 434, 516, 281, 362, 11, 309, 311, 516, 281, 652, 364, 1802, 294, 264], "temperature": 0.0, "avg_logprob": -0.22786676507247122, "compression_ratio": 1.6572769953051643, "no_speech_prob": 3.4266860893694684e-05}, {"id": 702, "seek": 329692, "start": 3313.12, "end": 3318.04, "text": " sense that you're going to be using those words.", "tokens": [2020, 300, 291, 434, 516, 281, 312, 1228, 729, 2283, 13], "temperature": 0.0, "avg_logprob": -0.22786676507247122, "compression_ratio": 1.6572769953051643, "no_speech_prob": 3.4266860893694684e-05}, {"id": 703, "seek": 329692, "start": 3318.04, "end": 3322.96, "text": " I think it's an interesting line of inquiry, but I will say this, the vast majority of", "tokens": [286, 519, 309, 311, 364, 1880, 1622, 295, 25736, 11, 457, 286, 486, 584, 341, 11, 264, 8369, 6286, 295], "temperature": 0.0, "avg_logprob": -0.22786676507247122, "compression_ratio": 1.6572769953051643, "no_speech_prob": 3.4266860893694684e-05}, {"id": 704, "seek": 332296, "start": 3322.96, "end": 3328.32, "text": " the time when you're kind of doing this in the real world, your vocabulary will be bigger", "tokens": [264, 565, 562, 291, 434, 733, 295, 884, 341, 294, 264, 957, 1002, 11, 428, 19864, 486, 312, 3801], "temperature": 0.0, "avg_logprob": -0.1299366247458536, "compression_ratio": 1.7435897435897436, "no_speech_prob": 1.0783094694488682e-05}, {"id": 705, "seek": 332296, "start": 3328.32, "end": 3336.32, "text": " than 40,000, and once your vocabulary is bigger than 40,000, using the standard techniques,", "tokens": [813, 3356, 11, 1360, 11, 293, 1564, 428, 19864, 307, 3801, 813, 3356, 11, 1360, 11, 1228, 264, 3832, 7512, 11], "temperature": 0.0, "avg_logprob": -0.1299366247458536, "compression_ratio": 1.7435897435897436, "no_speech_prob": 1.0783094694488682e-05}, {"id": 706, "seek": 332296, "start": 3336.32, "end": 3340.04, "text": " the embedding layer gets so big that it takes up all your memory, it takes up all of the", "tokens": [264, 12240, 3584, 4583, 2170, 370, 955, 300, 309, 2516, 493, 439, 428, 4675, 11, 309, 2516, 493, 439, 295, 264], "temperature": 0.0, "avg_logprob": -0.1299366247458536, "compression_ratio": 1.7435897435897436, "no_speech_prob": 1.0783094694488682e-05}, {"id": 707, "seek": 332296, "start": 3340.04, "end": 3341.96, "text": " time in the back crop.", "tokens": [565, 294, 264, 646, 9086, 13], "temperature": 0.0, "avg_logprob": -0.1299366247458536, "compression_ratio": 1.7435897435897436, "no_speech_prob": 1.0783094694488682e-05}, {"id": 708, "seek": 332296, "start": 3341.96, "end": 3346.32, "text": " There are tricks to dealing with very large vocabularies, I don't think we'll have time", "tokens": [821, 366, 11733, 281, 6260, 365, 588, 2416, 2329, 455, 1040, 530, 11, 286, 500, 380, 519, 321, 603, 362, 565], "temperature": 0.0, "avg_logprob": -0.1299366247458536, "compression_ratio": 1.7435897435897436, "no_speech_prob": 1.0783094694488682e-05}, {"id": 709, "seek": 332296, "start": 3346.32, "end": 3352.8, "text": " to handle them in this session, but you definitely would not want to have all three and a half", "tokens": [281, 4813, 552, 294, 341, 5481, 11, 457, 291, 2138, 576, 406, 528, 281, 362, 439, 1045, 293, 257, 1922], "temperature": 0.0, "avg_logprob": -0.1299366247458536, "compression_ratio": 1.7435897435897436, "no_speech_prob": 1.0783094694488682e-05}, {"id": 710, "seek": 335280, "start": 3352.8, "end": 3358.76, "text": " million fast text vectors in an embedding layer.", "tokens": [2459, 2370, 2487, 18875, 294, 364, 12240, 3584, 4583, 13], "temperature": 0.0, "avg_logprob": -0.25824843101131106, "compression_ratio": 1.5630252100840336, "no_speech_prob": 2.0784686057595536e-05}, {"id": 711, "seek": 335280, "start": 3358.76, "end": 3364.44, "text": " So I wonder, if you're not touching a word, it's not going to change, right?", "tokens": [407, 286, 2441, 11, 498, 291, 434, 406, 11175, 257, 1349, 11, 309, 311, 406, 516, 281, 1319, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.25824843101131106, "compression_ratio": 1.5630252100840336, "no_speech_prob": 2.0784686057595536e-05}, {"id": 712, "seek": 335280, "start": 3364.44, "end": 3369.4, "text": " Like even if you're fine-tuning, you're not, you're hiding the memory.", "tokens": [1743, 754, 498, 291, 434, 2489, 12, 83, 37726, 11, 291, 434, 406, 11, 291, 434, 10596, 264, 4675, 13], "temperature": 0.0, "avg_logprob": -0.25824843101131106, "compression_ratio": 1.5630252100840336, "no_speech_prob": 2.0784686057595536e-05}, {"id": 713, "seek": 335280, "start": 3369.4, "end": 3375.5600000000004, "text": " It's in GPU RAM, and you've got to remember, three and a half million times 300 times the", "tokens": [467, 311, 294, 18407, 14561, 11, 293, 291, 600, 658, 281, 1604, 11, 1045, 293, 257, 1922, 2459, 1413, 6641, 1413, 264], "temperature": 0.0, "avg_logprob": -0.25824843101131106, "compression_ratio": 1.5630252100840336, "no_speech_prob": 2.0784686057595536e-05}, {"id": 714, "seek": 335280, "start": 3375.5600000000004, "end": 3380.5600000000004, "text": " size of a single precision floating point vector, plus all of the gradients for them,", "tokens": [2744, 295, 257, 2167, 18356, 12607, 935, 8062, 11, 1804, 439, 295, 264, 2771, 2448, 337, 552, 11], "temperature": 0.0, "avg_logprob": -0.25824843101131106, "compression_ratio": 1.5630252100840336, "no_speech_prob": 2.0784686057595536e-05}, {"id": 715, "seek": 338056, "start": 3380.56, "end": 3384.36, "text": " even if it's not touched.", "tokens": [754, 498, 309, 311, 406, 9828, 13], "temperature": 0.0, "avg_logprob": -0.16123297156357183, "compression_ratio": 1.5505050505050506, "no_speech_prob": 2.078491343127098e-05}, {"id": 716, "seek": 338056, "start": 3384.36, "end": 3391.68, "text": " Without being very careful and adding a lot more code and stuff, it is slow and hard,", "tokens": [9129, 885, 588, 5026, 293, 5127, 257, 688, 544, 3089, 293, 1507, 11, 309, 307, 2964, 293, 1152, 11], "temperature": 0.0, "avg_logprob": -0.16123297156357183, "compression_ratio": 1.5505050505050506, "no_speech_prob": 2.078491343127098e-05}, {"id": 717, "seek": 338056, "start": 3391.68, "end": 3395.52, "text": " and we wouldn't touch it for now.", "tokens": [293, 321, 2759, 380, 2557, 309, 337, 586, 13], "temperature": 0.0, "avg_logprob": -0.16123297156357183, "compression_ratio": 1.5505050505050506, "no_speech_prob": 2.078491343127098e-05}, {"id": 718, "seek": 338056, "start": 3395.52, "end": 3399.68, "text": " I think it's an interesting path of inquiry, but it's the kind of path of inquiry that", "tokens": [286, 519, 309, 311, 364, 1880, 3100, 295, 25736, 11, 457, 309, 311, 264, 733, 295, 3100, 295, 25736, 300], "temperature": 0.0, "avg_logprob": -0.16123297156357183, "compression_ratio": 1.5505050505050506, "no_speech_prob": 2.078491343127098e-05}, {"id": 719, "seek": 338056, "start": 3399.68, "end": 3405.44, "text": " leads to multiple academic papers, not something that you do on a weekend.", "tokens": [6689, 281, 3866, 7778, 10577, 11, 406, 746, 300, 291, 360, 322, 257, 6711, 13], "temperature": 0.0, "avg_logprob": -0.16123297156357183, "compression_ratio": 1.5505050505050506, "no_speech_prob": 2.078491343127098e-05}, {"id": 720, "seek": 340544, "start": 3405.44, "end": 3411.96, "text": " I think it would be very interesting, maybe we can look at it sometime.", "tokens": [286, 519, 309, 576, 312, 588, 1880, 11, 1310, 321, 393, 574, 412, 309, 15053, 13], "temperature": 0.0, "avg_logprob": -0.2332614401112432, "compression_ratio": 1.441025641025641, "no_speech_prob": 1.0783229299704544e-05}, {"id": 721, "seek": 340544, "start": 3411.96, "end": 3416.88, "text": " And as I say, I have actually started doing some stuff around incorporating large vocabulary", "tokens": [400, 382, 286, 584, 11, 286, 362, 767, 1409, 884, 512, 1507, 926, 33613, 2416, 19864], "temperature": 0.0, "avg_logprob": -0.2332614401112432, "compression_ratio": 1.441025641025641, "no_speech_prob": 1.0783229299704544e-05}, {"id": 722, "seek": 340544, "start": 3416.88, "end": 3418.48, "text": " handling into FastAI.", "tokens": [13175, 666, 15968, 48698, 13], "temperature": 0.0, "avg_logprob": -0.2332614401112432, "compression_ratio": 1.441025641025641, "no_speech_prob": 1.0783229299704544e-05}, {"id": 723, "seek": 340544, "start": 3418.48, "end": 3427.6, "text": " It's not finished, but hopefully by the time we get here, this kind of stuff will be possible.", "tokens": [467, 311, 406, 4335, 11, 457, 4696, 538, 264, 565, 321, 483, 510, 11, 341, 733, 295, 1507, 486, 312, 1944, 13], "temperature": 0.0, "avg_logprob": -0.2332614401112432, "compression_ratio": 1.441025641025641, "no_speech_prob": 1.0783229299704544e-05}, {"id": 724, "seek": 342760, "start": 3427.6, "end": 3435.7599999999998, "text": " So we create our encoder embedding, add a bit of dropout, and then we create our RNN.", "tokens": [407, 321, 1884, 527, 2058, 19866, 12240, 3584, 11, 909, 257, 857, 295, 3270, 346, 11, 293, 550, 321, 1884, 527, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.16964399034731856, "compression_ratio": 1.605263157894737, "no_speech_prob": 5.422191406978527e-06}, {"id": 725, "seek": 342760, "start": 3435.7599999999998, "end": 3441.2799999999997, "text": " The input to the RNN obviously is the size of the embedding by definition.", "tokens": [440, 4846, 281, 264, 45702, 45, 2745, 307, 264, 2744, 295, 264, 12240, 3584, 538, 7123, 13], "temperature": 0.0, "avg_logprob": -0.16964399034731856, "compression_ratio": 1.605263157894737, "no_speech_prob": 5.422191406978527e-06}, {"id": 726, "seek": 342760, "start": 3441.2799999999997, "end": 3445.36, "text": " Number of hidden is whatever we want, so we set it to 256 for now.", "tokens": [5118, 295, 7633, 307, 2035, 321, 528, 11, 370, 321, 992, 309, 281, 38882, 337, 586, 13], "temperature": 0.0, "avg_logprob": -0.16964399034731856, "compression_ratio": 1.605263157894737, "no_speech_prob": 5.422191406978527e-06}, {"id": 727, "seek": 342760, "start": 3445.36, "end": 3451.08, "text": " However many layers we want, and some dropout inside the RNN as well.", "tokens": [2908, 867, 7914, 321, 528, 11, 293, 512, 3270, 346, 1854, 264, 45702, 45, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.16964399034731856, "compression_ratio": 1.605263157894737, "no_speech_prob": 5.422191406978527e-06}, {"id": 728, "seek": 342760, "start": 3451.08, "end": 3455.18, "text": " This is all standard PyTorch stuff, you could use LSTM here as well.", "tokens": [639, 307, 439, 3832, 9953, 51, 284, 339, 1507, 11, 291, 727, 764, 441, 6840, 44, 510, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.16964399034731856, "compression_ratio": 1.605263157894737, "no_speech_prob": 5.422191406978527e-06}, {"id": 729, "seek": 345518, "start": 3455.18, "end": 3460.04, "text": " And then finally we need to turn that into some output that we're going to feed to the", "tokens": [400, 550, 2721, 321, 643, 281, 1261, 300, 666, 512, 5598, 300, 321, 434, 516, 281, 3154, 281, 264], "temperature": 0.0, "avg_logprob": -0.16542673685464515, "compression_ratio": 1.5677083333333333, "no_speech_prob": 1.6536854445803328e-06}, {"id": 730, "seek": 345518, "start": 3460.04, "end": 3466.3999999999996, "text": " decoder, so let's use a linear layer to convert the number of hidden into the decoder embedding", "tokens": [979, 19866, 11, 370, 718, 311, 764, 257, 8213, 4583, 281, 7620, 264, 1230, 295, 7633, 666, 264, 979, 19866, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.16542673685464515, "compression_ratio": 1.5677083333333333, "no_speech_prob": 1.6536854445803328e-06}, {"id": 731, "seek": 345518, "start": 3466.3999999999996, "end": 3467.3999999999996, "text": " size.", "tokens": [2744, 13], "temperature": 0.0, "avg_logprob": -0.16542673685464515, "compression_ratio": 1.5677083333333333, "no_speech_prob": 1.6536854445803328e-06}, {"id": 732, "seek": 345518, "start": 3467.3999999999996, "end": 3472.64, "text": " So in the forward pass, here's how that's used.", "tokens": [407, 294, 264, 2128, 1320, 11, 510, 311, 577, 300, 311, 1143, 13], "temperature": 0.0, "avg_logprob": -0.16542673685464515, "compression_ratio": 1.5677083333333333, "no_speech_prob": 1.6536854445803328e-06}, {"id": 733, "seek": 345518, "start": 3472.64, "end": 3478.7599999999998, "text": " We first of all initialize our hidden state to a bunch of zeros.", "tokens": [492, 700, 295, 439, 5883, 1125, 527, 7633, 1785, 281, 257, 3840, 295, 35193, 13], "temperature": 0.0, "avg_logprob": -0.16542673685464515, "compression_ratio": 1.5677083333333333, "no_speech_prob": 1.6536854445803328e-06}, {"id": 734, "seek": 347876, "start": 3478.76, "end": 3485.92, "text": " So we've now got a vector of zeros, and then we're going to take our input and put it through", "tokens": [407, 321, 600, 586, 658, 257, 8062, 295, 35193, 11, 293, 550, 321, 434, 516, 281, 747, 527, 4846, 293, 829, 309, 807], "temperature": 0.0, "avg_logprob": -0.15054438580041643, "compression_ratio": 1.7783783783783784, "no_speech_prob": 2.2603178422286874e-06}, {"id": 735, "seek": 347876, "start": 3485.92, "end": 3492.46, "text": " our embedding, we're going to put that through dropout, we then pass our currently zeros", "tokens": [527, 12240, 3584, 11, 321, 434, 516, 281, 829, 300, 807, 3270, 346, 11, 321, 550, 1320, 527, 4362, 35193], "temperature": 0.0, "avg_logprob": -0.15054438580041643, "compression_ratio": 1.7783783783783784, "no_speech_prob": 2.2603178422286874e-06}, {"id": 736, "seek": 347876, "start": 3492.46, "end": 3498.5600000000004, "text": " hidden state and our embeddings into our RNN, and it's going to spit out the usual stuff", "tokens": [7633, 1785, 293, 527, 12240, 29432, 666, 527, 45702, 45, 11, 293, 309, 311, 516, 281, 22127, 484, 264, 7713, 1507], "temperature": 0.0, "avg_logprob": -0.15054438580041643, "compression_ratio": 1.7783783783783784, "no_speech_prob": 2.2603178422286874e-06}, {"id": 737, "seek": 347876, "start": 3498.5600000000004, "end": 3505.32, "text": " that RNN spit out, which includes the final hidden state.", "tokens": [300, 45702, 45, 22127, 484, 11, 597, 5974, 264, 2572, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.15054438580041643, "compression_ratio": 1.7783783783783784, "no_speech_prob": 2.2603178422286874e-06}, {"id": 738, "seek": 350532, "start": 3505.32, "end": 3509.9, "text": " We're then going to take that final hidden state and stick it through that linear layer,", "tokens": [492, 434, 550, 516, 281, 747, 300, 2572, 7633, 1785, 293, 2897, 309, 807, 300, 8213, 4583, 11], "temperature": 0.0, "avg_logprob": -0.15730823660796545, "compression_ratio": 1.5590551181102361, "no_speech_prob": 2.2125568648334593e-05}, {"id": 739, "seek": 350532, "start": 3509.9, "end": 3513.0800000000004, "text": " so we now have something of the right size to feed to our decoder.", "tokens": [370, 321, 586, 362, 746, 295, 264, 558, 2744, 281, 3154, 281, 527, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.15730823660796545, "compression_ratio": 1.5590551181102361, "no_speech_prob": 2.2125568648334593e-05}, {"id": 740, "seek": 350532, "start": 3513.0800000000004, "end": 3515.2000000000003, "text": " So that's it.", "tokens": [407, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.15730823660796545, "compression_ratio": 1.5590551181102361, "no_speech_prob": 2.2125568648334593e-05}, {"id": 741, "seek": 350532, "start": 3515.2000000000003, "end": 3521.84, "text": " And again, this ought to be very familiar and very comfortable, it's like the most simple", "tokens": [400, 797, 11, 341, 13416, 281, 312, 588, 4963, 293, 588, 4619, 11, 309, 311, 411, 264, 881, 2199], "temperature": 0.0, "avg_logprob": -0.15730823660796545, "compression_ratio": 1.5590551181102361, "no_speech_prob": 2.2125568648334593e-05}, {"id": 742, "seek": 350532, "start": 3521.84, "end": 3526.6800000000003, "text": " possible RNN, so if it's not, go back, check out lesson 6, make sure you can write it from", "tokens": [1944, 45702, 45, 11, 370, 498, 309, 311, 406, 11, 352, 646, 11, 1520, 484, 6898, 1386, 11, 652, 988, 291, 393, 2464, 309, 490], "temperature": 0.0, "avg_logprob": -0.15730823660796545, "compression_ratio": 1.5590551181102361, "no_speech_prob": 2.2125568648334593e-05}, {"id": 743, "seek": 350532, "start": 3526.6800000000003, "end": 3529.6800000000003, "text": " scratch and that you understand what it does.", "tokens": [8459, 293, 300, 291, 1223, 437, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.15730823660796545, "compression_ratio": 1.5590551181102361, "no_speech_prob": 2.2125568648334593e-05}, {"id": 744, "seek": 352968, "start": 3529.68, "end": 3541.04, "text": " But the key thing to know is that it takes our inputs and spits out a hidden vector that", "tokens": [583, 264, 2141, 551, 281, 458, 307, 300, 309, 2516, 527, 15743, 293, 637, 1208, 484, 257, 7633, 8062, 300], "temperature": 0.0, "avg_logprob": -0.08721322814623515, "compression_ratio": 1.4621212121212122, "no_speech_prob": 3.844927050522529e-06}, {"id": 745, "seek": 352968, "start": 3541.04, "end": 3549.52, "text": " hopefully will learn to contain all of the information about what that sentence says", "tokens": [4696, 486, 1466, 281, 5304, 439, 295, 264, 1589, 466, 437, 300, 8174, 1619], "temperature": 0.0, "avg_logprob": -0.08721322814623515, "compression_ratio": 1.4621212121212122, "no_speech_prob": 3.844927050522529e-06}, {"id": 746, "seek": 352968, "start": 3549.52, "end": 3551.3599999999997, "text": " and how it says it.", "tokens": [293, 577, 309, 1619, 309, 13], "temperature": 0.0, "avg_logprob": -0.08721322814623515, "compression_ratio": 1.4621212121212122, "no_speech_prob": 3.844927050522529e-06}, {"id": 747, "seek": 355136, "start": 3551.36, "end": 3561.1200000000003, "text": " Because if it can't do that, then we can't feed it into a decoder and hope it to spit", "tokens": [1436, 498, 309, 393, 380, 360, 300, 11, 550, 321, 393, 380, 3154, 309, 666, 257, 979, 19866, 293, 1454, 309, 281, 22127], "temperature": 0.0, "avg_logprob": -0.12049106885028142, "compression_ratio": 1.7329842931937174, "no_speech_prob": 4.4254784370423295e-06}, {"id": 748, "seek": 355136, "start": 3561.1200000000003, "end": 3564.6800000000003, "text": " out our sentence in a different language.", "tokens": [484, 527, 8174, 294, 257, 819, 2856, 13], "temperature": 0.0, "avg_logprob": -0.12049106885028142, "compression_ratio": 1.7329842931937174, "no_speech_prob": 4.4254784370423295e-06}, {"id": 749, "seek": 355136, "start": 3564.6800000000003, "end": 3568.56, "text": " So that's what we want it to learn to do.", "tokens": [407, 300, 311, 437, 321, 528, 309, 281, 1466, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.12049106885028142, "compression_ratio": 1.7329842931937174, "no_speech_prob": 4.4254784370423295e-06}, {"id": 750, "seek": 355136, "start": 3568.56, "end": 3571.6, "text": " And we're not going to do anything special to make it learn to do that, we're just going", "tokens": [400, 321, 434, 406, 516, 281, 360, 1340, 2121, 281, 652, 309, 1466, 281, 360, 300, 11, 321, 434, 445, 516], "temperature": 0.0, "avg_logprob": -0.12049106885028142, "compression_ratio": 1.7329842931937174, "no_speech_prob": 4.4254784370423295e-06}, {"id": 751, "seek": 355136, "start": 3571.6, "end": 3580.76, "text": " to do the three things and cross our fingers, because that's what we do.", "tokens": [281, 360, 264, 1045, 721, 293, 3278, 527, 7350, 11, 570, 300, 311, 437, 321, 360, 13], "temperature": 0.0, "avg_logprob": -0.12049106885028142, "compression_ratio": 1.7329842931937174, "no_speech_prob": 4.4254784370423295e-06}, {"id": 752, "seek": 358076, "start": 3580.76, "end": 3588.76, "text": " So that's H is that S, it's the hidden state.", "tokens": [407, 300, 311, 389, 307, 300, 318, 11, 309, 311, 264, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.20820047590467666, "compression_ratio": 1.5808080808080809, "no_speech_prob": 7.296306648640893e-06}, {"id": 753, "seek": 358076, "start": 3588.76, "end": 3593.5200000000004, "text": " I guess Stephen used S for state, I used H for hidden.", "tokens": [286, 2041, 13391, 1143, 318, 337, 1785, 11, 286, 1143, 389, 337, 7633, 13], "temperature": 0.0, "avg_logprob": -0.20820047590467666, "compression_ratio": 1.5808080808080809, "no_speech_prob": 7.296306648640893e-06}, {"id": 754, "seek": 358076, "start": 3593.5200000000004, "end": 3599.1200000000003, "text": " You'd think the two Australians could agree on something like that, but apparently not.", "tokens": [509, 1116, 519, 264, 732, 38108, 727, 3986, 322, 746, 411, 300, 11, 457, 7970, 406, 13], "temperature": 0.0, "avg_logprob": -0.20820047590467666, "compression_ratio": 1.5808080808080809, "no_speech_prob": 7.296306648640893e-06}, {"id": 755, "seek": 358076, "start": 3599.1200000000003, "end": 3603.84, "text": " So how do we now do the new bit?", "tokens": [407, 577, 360, 321, 586, 360, 264, 777, 857, 30], "temperature": 0.0, "avg_logprob": -0.20820047590467666, "compression_ratio": 1.5808080808080809, "no_speech_prob": 7.296306648640893e-06}, {"id": 756, "seek": 358076, "start": 3603.84, "end": 3609.4, "text": " And so the basic idea of the new bit is the same, we're going to do exactly the same thing,", "tokens": [400, 370, 264, 3875, 1558, 295, 264, 777, 857, 307, 264, 912, 11, 321, 434, 516, 281, 360, 2293, 264, 912, 551, 11], "temperature": 0.0, "avg_logprob": -0.20820047590467666, "compression_ratio": 1.5808080808080809, "no_speech_prob": 7.296306648640893e-06}, {"id": 757, "seek": 360940, "start": 3609.4, "end": 3613.6, "text": " but we're going to write our own for loop.", "tokens": [457, 321, 434, 516, 281, 2464, 527, 1065, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.15221158593101838, "compression_ratio": 1.8571428571428572, "no_speech_prob": 8.801040166872554e-06}, {"id": 758, "seek": 360940, "start": 3613.6, "end": 3619.8, "text": " And so the for loop is going to do exactly what the for loop inside PyTorch does here,", "tokens": [400, 370, 264, 337, 6367, 307, 516, 281, 360, 2293, 437, 264, 337, 6367, 1854, 9953, 51, 284, 339, 775, 510, 11], "temperature": 0.0, "avg_logprob": -0.15221158593101838, "compression_ratio": 1.8571428571428572, "no_speech_prob": 8.801040166872554e-06}, {"id": 759, "seek": 360940, "start": 3619.8, "end": 3621.12, "text": " but we're going to do it manually.", "tokens": [457, 321, 434, 516, 281, 360, 309, 16945, 13], "temperature": 0.0, "avg_logprob": -0.15221158593101838, "compression_ratio": 1.8571428571428572, "no_speech_prob": 8.801040166872554e-06}, {"id": 760, "seek": 360940, "start": 3621.12, "end": 3623.04, "text": " So we're going to go through the for loop.", "tokens": [407, 321, 434, 516, 281, 352, 807, 264, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.15221158593101838, "compression_ratio": 1.8571428571428572, "no_speech_prob": 8.801040166872554e-06}, {"id": 761, "seek": 360940, "start": 3623.04, "end": 3625.76, "text": " And how big is the for loop?", "tokens": [400, 577, 955, 307, 264, 337, 6367, 30], "temperature": 0.0, "avg_logprob": -0.15221158593101838, "compression_ratio": 1.8571428571428572, "no_speech_prob": 8.801040166872554e-06}, {"id": 762, "seek": 360940, "start": 3625.76, "end": 3628.2400000000002, "text": " It's an output sequence length.", "tokens": [467, 311, 364, 5598, 8310, 4641, 13], "temperature": 0.0, "avg_logprob": -0.15221158593101838, "compression_ratio": 1.8571428571428572, "no_speech_prob": 8.801040166872554e-06}, {"id": 763, "seek": 360940, "start": 3628.2400000000002, "end": 3630.4, "text": " Well what is output sequence length?", "tokens": [1042, 437, 307, 5598, 8310, 4641, 30], "temperature": 0.0, "avg_logprob": -0.15221158593101838, "compression_ratio": 1.8571428571428572, "no_speech_prob": 8.801040166872554e-06}, {"id": 764, "seek": 360940, "start": 3630.4, "end": 3638.12, "text": " That's something that got passed to the constructor, and it is equal to the length of the largest", "tokens": [663, 311, 746, 300, 658, 4678, 281, 264, 47479, 11, 293, 309, 307, 2681, 281, 264, 4641, 295, 264, 6443], "temperature": 0.0, "avg_logprob": -0.15221158593101838, "compression_ratio": 1.8571428571428572, "no_speech_prob": 8.801040166872554e-06}, {"id": 765, "seek": 363812, "start": 3638.12, "end": 3640.12, "text": " English sentence.", "tokens": [3669, 8174, 13], "temperature": 0.0, "avg_logprob": -0.1413291808097593, "compression_ratio": 1.6682692307692308, "no_speech_prob": 4.0294444261235185e-06}, {"id": 766, "seek": 363812, "start": 3640.12, "end": 3645.7999999999997, "text": " So we're going to do this for loop as long as the largest English sentence, because we're", "tokens": [407, 321, 434, 516, 281, 360, 341, 337, 6367, 382, 938, 382, 264, 6443, 3669, 8174, 11, 570, 321, 434], "temperature": 0.0, "avg_logprob": -0.1413291808097593, "compression_ratio": 1.6682692307692308, "no_speech_prob": 4.0294444261235185e-06}, {"id": 767, "seek": 363812, "start": 3645.7999999999997, "end": 3647.24, "text": " translating into English.", "tokens": [35030, 666, 3669, 13], "temperature": 0.0, "avg_logprob": -0.1413291808097593, "compression_ratio": 1.6682692307692308, "no_speech_prob": 4.0294444261235185e-06}, {"id": 768, "seek": 363812, "start": 3647.24, "end": 3653.08, "text": " So we can't possibly be longer than that.", "tokens": [407, 321, 393, 380, 6264, 312, 2854, 813, 300, 13], "temperature": 0.0, "avg_logprob": -0.1413291808097593, "compression_ratio": 1.6682692307692308, "no_speech_prob": 4.0294444261235185e-06}, {"id": 769, "seek": 363812, "start": 3653.08, "end": 3655.12, "text": " At least not in this corpus.", "tokens": [1711, 1935, 406, 294, 341, 1181, 31624, 13], "temperature": 0.0, "avg_logprob": -0.1413291808097593, "compression_ratio": 1.6682692307692308, "no_speech_prob": 4.0294444261235185e-06}, {"id": 770, "seek": 363812, "start": 3655.12, "end": 3662.72, "text": " If we then used it on some different corpus that was longer, this is going to fail.", "tokens": [759, 321, 550, 1143, 309, 322, 512, 819, 1181, 31624, 300, 390, 2854, 11, 341, 307, 516, 281, 3061, 13], "temperature": 0.0, "avg_logprob": -0.1413291808097593, "compression_ratio": 1.6682692307692308, "no_speech_prob": 4.0294444261235185e-06}, {"id": 771, "seek": 363812, "start": 3662.72, "end": 3667.44, "text": " You could always pass in a different parameter, of course.", "tokens": [509, 727, 1009, 1320, 294, 257, 819, 13075, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.1413291808097593, "compression_ratio": 1.6682692307692308, "no_speech_prob": 4.0294444261235185e-06}, {"id": 772, "seek": 366744, "start": 3667.44, "end": 3672.6, "text": " So the basic idea is the same, we're going to go through and put it through the embedding,", "tokens": [407, 264, 3875, 1558, 307, 264, 912, 11, 321, 434, 516, 281, 352, 807, 293, 829, 309, 807, 264, 12240, 3584, 11], "temperature": 0.0, "avg_logprob": -0.10765585591716151, "compression_ratio": 2.139423076923077, "no_speech_prob": 4.289320258976659e-06}, {"id": 773, "seek": 366744, "start": 3672.6, "end": 3676.64, "text": " we're going to stick it through the RNN, we're going to stick it through dropout, and we're", "tokens": [321, 434, 516, 281, 2897, 309, 807, 264, 45702, 45, 11, 321, 434, 516, 281, 2897, 309, 807, 3270, 346, 11, 293, 321, 434], "temperature": 0.0, "avg_logprob": -0.10765585591716151, "compression_ratio": 2.139423076923077, "no_speech_prob": 4.289320258976659e-06}, {"id": 774, "seek": 366744, "start": 3676.64, "end": 3679.08, "text": " going to stick it through a linear layer.", "tokens": [516, 281, 2897, 309, 807, 257, 8213, 4583, 13], "temperature": 0.0, "avg_logprob": -0.10765585591716151, "compression_ratio": 2.139423076923077, "no_speech_prob": 4.289320258976659e-06}, {"id": 775, "seek": 366744, "start": 3679.08, "end": 3682.88, "text": " So the basic four steps are the same.", "tokens": [407, 264, 3875, 1451, 4439, 366, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.10765585591716151, "compression_ratio": 2.139423076923077, "no_speech_prob": 4.289320258976659e-06}, {"id": 776, "seek": 366744, "start": 3682.88, "end": 3689.84, "text": " And once we've done that, we're then going to append that output to a list, and then", "tokens": [400, 1564, 321, 600, 1096, 300, 11, 321, 434, 550, 516, 281, 34116, 300, 5598, 281, 257, 1329, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.10765585591716151, "compression_ratio": 2.139423076923077, "no_speech_prob": 4.289320258976659e-06}, {"id": 777, "seek": 366744, "start": 3689.84, "end": 3693.88, "text": " when we're going to finish, we're going to stack that list up into a single tensor and", "tokens": [562, 321, 434, 516, 281, 2413, 11, 321, 434, 516, 281, 8630, 300, 1329, 493, 666, 257, 2167, 40863, 293], "temperature": 0.0, "avg_logprob": -0.10765585591716151, "compression_ratio": 2.139423076923077, "no_speech_prob": 4.289320258976659e-06}, {"id": 778, "seek": 366744, "start": 3693.88, "end": 3695.44, "text": " return it.", "tokens": [2736, 309, 13], "temperature": 0.0, "avg_logprob": -0.10765585591716151, "compression_ratio": 2.139423076923077, "no_speech_prob": 4.289320258976659e-06}, {"id": 779, "seek": 369544, "start": 3695.44, "end": 3698.88, "text": " That's the basic idea.", "tokens": [663, 311, 264, 3875, 1558, 13], "temperature": 0.0, "avg_logprob": -0.14538894915113262, "compression_ratio": 1.6926406926406927, "no_speech_prob": 2.260319661218091e-06}, {"id": 780, "seek": 369544, "start": 3698.88, "end": 3705.44, "text": " Normally a recurrent neural network, here's our decoder recurrent neural network, works", "tokens": [17424, 257, 18680, 1753, 18161, 3209, 11, 510, 311, 527, 979, 19866, 18680, 1753, 18161, 3209, 11, 1985], "temperature": 0.0, "avg_logprob": -0.14538894915113262, "compression_ratio": 1.6926406926406927, "no_speech_prob": 2.260319661218091e-06}, {"id": 781, "seek": 369544, "start": 3705.44, "end": 3710.12, "text": " on a whole sequence at a time, but we've got a for loop to go through each part of the", "tokens": [322, 257, 1379, 8310, 412, 257, 565, 11, 457, 321, 600, 658, 257, 337, 6367, 281, 352, 807, 1184, 644, 295, 264], "temperature": 0.0, "avg_logprob": -0.14538894915113262, "compression_ratio": 1.6926406926406927, "no_speech_prob": 2.260319661218091e-06}, {"id": 782, "seek": 369544, "start": 3710.12, "end": 3717.8, "text": " sequence separately, so we have to add a leading unit access to the start to basically say", "tokens": [8310, 14759, 11, 370, 321, 362, 281, 909, 257, 5775, 4985, 2105, 281, 264, 722, 281, 1936, 584], "temperature": 0.0, "avg_logprob": -0.14538894915113262, "compression_ratio": 1.6926406926406927, "no_speech_prob": 2.260319661218091e-06}, {"id": 783, "seek": 369544, "start": 3717.8, "end": 3720.28, "text": " this is a sequence of length 1.", "tokens": [341, 307, 257, 8310, 295, 4641, 502, 13], "temperature": 0.0, "avg_logprob": -0.14538894915113262, "compression_ratio": 1.6926406926406927, "no_speech_prob": 2.260319661218091e-06}, {"id": 784, "seek": 369544, "start": 3720.28, "end": 3723.88, "text": " So we're not really taking advantage of the recurrent net much at all.", "tokens": [407, 321, 434, 406, 534, 1940, 5002, 295, 264, 18680, 1753, 2533, 709, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.14538894915113262, "compression_ratio": 1.6926406926406927, "no_speech_prob": 2.260319661218091e-06}, {"id": 785, "seek": 372388, "start": 3723.88, "end": 3726.6800000000003, "text": " We could easily rewrite this with a linear layer actually.", "tokens": [492, 727, 3612, 28132, 341, 365, 257, 8213, 4583, 767, 13], "temperature": 0.0, "avg_logprob": -0.16817555060753456, "compression_ratio": 1.5175879396984924, "no_speech_prob": 2.642574827405042e-06}, {"id": 786, "seek": 372388, "start": 3726.6800000000003, "end": 3731.32, "text": " That would be an interesting experiment if you wanted to try it.", "tokens": [663, 576, 312, 364, 1880, 5120, 498, 291, 1415, 281, 853, 309, 13], "temperature": 0.0, "avg_logprob": -0.16817555060753456, "compression_ratio": 1.5175879396984924, "no_speech_prob": 2.642574827405042e-06}, {"id": 787, "seek": 372388, "start": 3731.32, "end": 3739.32, "text": " So we basically take our input and we feed it into our embedding, and we add something", "tokens": [407, 321, 1936, 747, 527, 4846, 293, 321, 3154, 309, 666, 527, 12240, 3584, 11, 293, 321, 909, 746], "temperature": 0.0, "avg_logprob": -0.16817555060753456, "compression_ratio": 1.5175879396984924, "no_speech_prob": 2.642574827405042e-06}, {"id": 788, "seek": 372388, "start": 3739.32, "end": 3744.28, "text": " to the front saying treat this as a sequence of length 1, and then we pass that to our", "tokens": [281, 264, 1868, 1566, 2387, 341, 382, 257, 8310, 295, 4641, 502, 11, 293, 550, 321, 1320, 300, 281, 527], "temperature": 0.0, "avg_logprob": -0.16817555060753456, "compression_ratio": 1.5175879396984924, "no_speech_prob": 2.642574827405042e-06}, {"id": 789, "seek": 372388, "start": 3744.28, "end": 3746.84, "text": " RNN.", "tokens": [45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.16817555060753456, "compression_ratio": 1.5175879396984924, "no_speech_prob": 2.642574827405042e-06}, {"id": 790, "seek": 374684, "start": 3746.84, "end": 3754.36, "text": " We then get the output of that RNN, feed it into our dropout, and feed it into our linear", "tokens": [492, 550, 483, 264, 5598, 295, 300, 45702, 45, 11, 3154, 309, 666, 527, 3270, 346, 11, 293, 3154, 309, 666, 527, 8213], "temperature": 0.0, "avg_logprob": -0.1434814435131145, "compression_ratio": 1.6952380952380952, "no_speech_prob": 3.2377570278185885e-06}, {"id": 791, "seek": 374684, "start": 3754.36, "end": 3755.36, "text": " layer.", "tokens": [4583, 13], "temperature": 0.0, "avg_logprob": -0.1434814435131145, "compression_ratio": 1.6952380952380952, "no_speech_prob": 3.2377570278185885e-06}, {"id": 792, "seek": 374684, "start": 3755.36, "end": 3757.56, "text": " So there's two extra things to be aware of.", "tokens": [407, 456, 311, 732, 2857, 721, 281, 312, 3650, 295, 13], "temperature": 0.0, "avg_logprob": -0.1434814435131145, "compression_ratio": 1.6952380952380952, "no_speech_prob": 3.2377570278185885e-06}, {"id": 793, "seek": 374684, "start": 3757.56, "end": 3759.6800000000003, "text": " Well I guess it's really one thing.", "tokens": [1042, 286, 2041, 309, 311, 534, 472, 551, 13], "temperature": 0.0, "avg_logprob": -0.1434814435131145, "compression_ratio": 1.6952380952380952, "no_speech_prob": 3.2377570278185885e-06}, {"id": 794, "seek": 374684, "start": 3759.6800000000003, "end": 3763.56, "text": " The one thing is, what's this?", "tokens": [440, 472, 551, 307, 11, 437, 311, 341, 30], "temperature": 0.0, "avg_logprob": -0.1434814435131145, "compression_ratio": 1.6952380952380952, "no_speech_prob": 3.2377570278185885e-06}, {"id": 795, "seek": 374684, "start": 3763.56, "end": 3766.7200000000003, "text": " What is the input to that embedding?", "tokens": [708, 307, 264, 4846, 281, 300, 12240, 3584, 30], "temperature": 0.0, "avg_logprob": -0.1434814435131145, "compression_ratio": 1.6952380952380952, "no_speech_prob": 3.2377570278185885e-06}, {"id": 796, "seek": 374684, "start": 3766.7200000000003, "end": 3772.4, "text": " And the answer is, it's the previous word that we translated.", "tokens": [400, 264, 1867, 307, 11, 309, 311, 264, 3894, 1349, 300, 321, 16805, 13], "temperature": 0.0, "avg_logprob": -0.1434814435131145, "compression_ratio": 1.6952380952380952, "no_speech_prob": 3.2377570278185885e-06}, {"id": 797, "seek": 374684, "start": 3772.4, "end": 3775.7200000000003, "text": " See how the input here is the previous word here.", "tokens": [3008, 577, 264, 4846, 510, 307, 264, 3894, 1349, 510, 13], "temperature": 0.0, "avg_logprob": -0.1434814435131145, "compression_ratio": 1.6952380952380952, "no_speech_prob": 3.2377570278185885e-06}, {"id": 798, "seek": 377572, "start": 3775.72, "end": 3778.16, "text": " The input here is the previous word here.", "tokens": [440, 4846, 510, 307, 264, 3894, 1349, 510, 13], "temperature": 0.0, "avg_logprob": -0.13992366627750233, "compression_ratio": 1.7208333333333334, "no_speech_prob": 2.6841948965738993e-06}, {"id": 799, "seek": 377572, "start": 3778.16, "end": 3784.9599999999996, "text": " So the basic idea is, if you're trying to translate, if you're about to translate, tell", "tokens": [407, 264, 3875, 1558, 307, 11, 498, 291, 434, 1382, 281, 13799, 11, 498, 291, 434, 466, 281, 13799, 11, 980], "temperature": 0.0, "avg_logprob": -0.13992366627750233, "compression_ratio": 1.7208333333333334, "no_speech_prob": 2.6841948965738993e-06}, {"id": 800, "seek": 377572, "start": 3784.9599999999996, "end": 3789.56, "text": " me the fourth word of the new sentence, but you don't know what the third word you just", "tokens": [385, 264, 6409, 1349, 295, 264, 777, 8174, 11, 457, 291, 500, 380, 458, 437, 264, 2636, 1349, 291, 445], "temperature": 0.0, "avg_logprob": -0.13992366627750233, "compression_ratio": 1.7208333333333334, "no_speech_prob": 2.6841948965738993e-06}, {"id": 801, "seek": 377572, "start": 3789.56, "end": 3793.08, "text": " said was, that's going to be really hard.", "tokens": [848, 390, 11, 300, 311, 516, 281, 312, 534, 1152, 13], "temperature": 0.0, "avg_logprob": -0.13992366627750233, "compression_ratio": 1.7208333333333334, "no_speech_prob": 2.6841948965738993e-06}, {"id": 802, "seek": 377572, "start": 3793.08, "end": 3797.16, "text": " So we're going to feed that in at each time step.", "tokens": [407, 321, 434, 516, 281, 3154, 300, 294, 412, 1184, 565, 1823, 13], "temperature": 0.0, "avg_logprob": -0.13992366627750233, "compression_ratio": 1.7208333333333334, "no_speech_prob": 2.6841948965738993e-06}, {"id": 803, "seek": 377572, "start": 3797.16, "end": 3799.08, "text": " Let's make it as easy as possible.", "tokens": [961, 311, 652, 309, 382, 1858, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.13992366627750233, "compression_ratio": 1.7208333333333334, "no_speech_prob": 2.6841948965738993e-06}, {"id": 804, "seek": 377572, "start": 3799.08, "end": 3801.0, "text": " And so what was the previous word at the start?", "tokens": [400, 370, 437, 390, 264, 3894, 1349, 412, 264, 722, 30], "temperature": 0.0, "avg_logprob": -0.13992366627750233, "compression_ratio": 1.7208333333333334, "no_speech_prob": 2.6841948965738993e-06}, {"id": 805, "seek": 377572, "start": 3801.0, "end": 3803.3599999999997, "text": " Well there was none.", "tokens": [1042, 456, 390, 6022, 13], "temperature": 0.0, "avg_logprob": -0.13992366627750233, "compression_ratio": 1.7208333333333334, "no_speech_prob": 2.6841948965738993e-06}, {"id": 806, "seek": 380336, "start": 3803.36, "end": 3817.96, "text": " So specifically, we're going to start out with a beginning of stream token.", "tokens": [407, 4682, 11, 321, 434, 516, 281, 722, 484, 365, 257, 2863, 295, 4309, 14862, 13], "temperature": 0.0, "avg_logprob": -0.13898246693161298, "compression_ratio": 1.6929824561403508, "no_speech_prob": 3.6688518321170704e-06}, {"id": 807, "seek": 380336, "start": 3817.96, "end": 3822.08, "text": " So the beginning of stream token is a 0.", "tokens": [407, 264, 2863, 295, 4309, 14862, 307, 257, 1958, 13], "temperature": 0.0, "avg_logprob": -0.13898246693161298, "compression_ratio": 1.6929824561403508, "no_speech_prob": 3.6688518321170704e-06}, {"id": 808, "seek": 380336, "start": 3822.08, "end": 3829.92, "text": " So let's start out our decoder with a beginning of stream token, which is 0.", "tokens": [407, 718, 311, 722, 484, 527, 979, 19866, 365, 257, 2863, 295, 4309, 14862, 11, 597, 307, 1958, 13], "temperature": 0.0, "avg_logprob": -0.13898246693161298, "compression_ratio": 1.6929824561403508, "no_speech_prob": 3.6688518321170704e-06}, {"id": 809, "seek": 382992, "start": 3829.92, "end": 3833.52, "text": " And of course we're doing a mini-batch, so we need batch size number of them.", "tokens": [400, 295, 1164, 321, 434, 884, 257, 8382, 12, 65, 852, 11, 370, 321, 643, 15245, 2744, 1230, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.14440100533621653, "compression_ratio": 1.6917293233082706, "no_speech_prob": 9.51617766986601e-06}, {"id": 810, "seek": 382992, "start": 3833.52, "end": 3835.8, "text": " But let's just think about one part of that batch.", "tokens": [583, 718, 311, 445, 519, 466, 472, 644, 295, 300, 15245, 13], "temperature": 0.0, "avg_logprob": -0.14440100533621653, "compression_ratio": 1.6917293233082706, "no_speech_prob": 9.51617766986601e-06}, {"id": 811, "seek": 382992, "start": 3835.8, "end": 3838.52, "text": " So we start out with a 0.", "tokens": [407, 321, 722, 484, 365, 257, 1958, 13], "temperature": 0.0, "avg_logprob": -0.14440100533621653, "compression_ratio": 1.6917293233082706, "no_speech_prob": 9.51617766986601e-06}, {"id": 812, "seek": 382992, "start": 3838.52, "end": 3844.28, "text": " We look up that 0 in our embedding matrix to find out what the vector for the beginning", "tokens": [492, 574, 493, 300, 1958, 294, 527, 12240, 3584, 8141, 281, 915, 484, 437, 264, 8062, 337, 264, 2863], "temperature": 0.0, "avg_logprob": -0.14440100533621653, "compression_ratio": 1.6917293233082706, "no_speech_prob": 9.51617766986601e-06}, {"id": 813, "seek": 382992, "start": 3844.28, "end": 3846.2000000000003, "text": " of stream token is.", "tokens": [295, 4309, 14862, 307, 13], "temperature": 0.0, "avg_logprob": -0.14440100533621653, "compression_ratio": 1.6917293233082706, "no_speech_prob": 9.51617766986601e-06}, {"id": 814, "seek": 382992, "start": 3846.2000000000003, "end": 3850.76, "text": " We stick a unit axis on the front to say we have a single sequence length of beginning", "tokens": [492, 2897, 257, 4985, 10298, 322, 264, 1868, 281, 584, 321, 362, 257, 2167, 8310, 4641, 295, 2863], "temperature": 0.0, "avg_logprob": -0.14440100533621653, "compression_ratio": 1.6917293233082706, "no_speech_prob": 9.51617766986601e-06}, {"id": 815, "seek": 382992, "start": 3850.76, "end": 3852.52, "text": " of stream token.", "tokens": [295, 4309, 14862, 13], "temperature": 0.0, "avg_logprob": -0.14440100533621653, "compression_ratio": 1.6917293233082706, "no_speech_prob": 9.51617766986601e-06}, {"id": 816, "seek": 382992, "start": 3852.52, "end": 3858.76, "text": " We stick that through our RNN, which gets not only the fact that there's a 0 at the", "tokens": [492, 2897, 300, 807, 527, 45702, 45, 11, 597, 2170, 406, 787, 264, 1186, 300, 456, 311, 257, 1958, 412, 264], "temperature": 0.0, "avg_logprob": -0.14440100533621653, "compression_ratio": 1.6917293233082706, "no_speech_prob": 9.51617766986601e-06}, {"id": 817, "seek": 385876, "start": 3858.76, "end": 3865.2400000000002, "text": " beginning of stream, but also the hidden state, which at this point is whatever came out of", "tokens": [2863, 295, 4309, 11, 457, 611, 264, 7633, 1785, 11, 597, 412, 341, 935, 307, 2035, 1361, 484, 295], "temperature": 0.0, "avg_logprob": -0.15947618717100562, "compression_ratio": 1.5870646766169154, "no_speech_prob": 1.5534956219198648e-06}, {"id": 818, "seek": 385876, "start": 3865.2400000000002, "end": 3866.92, "text": " our encoder.", "tokens": [527, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.15947618717100562, "compression_ratio": 1.5870646766169154, "no_speech_prob": 1.5534956219198648e-06}, {"id": 819, "seek": 385876, "start": 3866.92, "end": 3876.5200000000004, "text": " So this now, its job is to try and figure out what is the first word.", "tokens": [407, 341, 586, 11, 1080, 1691, 307, 281, 853, 293, 2573, 484, 437, 307, 264, 700, 1349, 13], "temperature": 0.0, "avg_logprob": -0.15947618717100562, "compression_ratio": 1.5870646766169154, "no_speech_prob": 1.5534956219198648e-06}, {"id": 820, "seek": 385876, "start": 3876.5200000000004, "end": 3879.8, "text": " What's the first word to translate the sentence?", "tokens": [708, 311, 264, 700, 1349, 281, 13799, 264, 8174, 30], "temperature": 0.0, "avg_logprob": -0.15947618717100562, "compression_ratio": 1.5870646766169154, "no_speech_prob": 1.5534956219198648e-06}, {"id": 821, "seek": 385876, "start": 3879.8, "end": 3885.2000000000003, "text": " Pop through some dropout, go through one linear layer in order to convert that into the correct", "tokens": [10215, 807, 512, 3270, 346, 11, 352, 807, 472, 8213, 4583, 294, 1668, 281, 7620, 300, 666, 264, 3006], "temperature": 0.0, "avg_logprob": -0.15947618717100562, "compression_ratio": 1.5870646766169154, "no_speech_prob": 1.5534956219198648e-06}, {"id": 822, "seek": 388520, "start": 3885.2, "end": 3890.48, "text": " size for our decoder embedding matrix.", "tokens": [2744, 337, 527, 979, 19866, 12240, 3584, 8141, 13], "temperature": 0.0, "avg_logprob": -0.20681316151338466, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.4439902972517302e-06}, {"id": 823, "seek": 388520, "start": 3890.48, "end": 3894.72, "text": " Append that to our list of translated words.", "tokens": [3132, 521, 300, 281, 527, 1329, 295, 16805, 2283, 13], "temperature": 0.0, "avg_logprob": -0.20681316151338466, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.4439902972517302e-06}, {"id": 824, "seek": 388520, "start": 3894.72, "end": 3899.7599999999998, "text": " And now we need to figure out what word that was because we need to feed it to the next", "tokens": [400, 586, 321, 643, 281, 2573, 484, 437, 1349, 300, 390, 570, 321, 643, 281, 3154, 309, 281, 264, 958], "temperature": 0.0, "avg_logprob": -0.20681316151338466, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.4439902972517302e-06}, {"id": 825, "seek": 388520, "start": 3899.7599999999998, "end": 3902.4399999999996, "text": " time step.", "tokens": [565, 1823, 13], "temperature": 0.0, "avg_logprob": -0.20681316151338466, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.4439902972517302e-06}, {"id": 826, "seek": 388520, "start": 3902.4399999999996, "end": 3904.68, "text": " We need to feed it to the next time step.", "tokens": [492, 643, 281, 3154, 309, 281, 264, 958, 565, 1823, 13], "temperature": 0.0, "avg_logprob": -0.20681316151338466, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.4439902972517302e-06}, {"id": 827, "seek": 388520, "start": 3904.68, "end": 3911.2, "text": " So remember what we actually output here, and don't forget, use a debugger.", "tokens": [407, 1604, 437, 321, 767, 5598, 510, 11, 293, 500, 380, 2870, 11, 764, 257, 24083, 1321, 13], "temperature": 0.0, "avg_logprob": -0.20681316151338466, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.4439902972517302e-06}, {"id": 828, "seek": 391120, "start": 3911.2, "end": 3915.48, "text": " That pdb.setTrace, put it here, what is outp?", "tokens": [663, 280, 67, 65, 13, 3854, 14252, 617, 11, 829, 309, 510, 11, 437, 307, 484, 79, 30], "temperature": 0.0, "avg_logprob": -0.16592927736656687, "compression_ratio": 1.5905172413793103, "no_speech_prob": 5.255381438473705e-06}, {"id": 829, "seek": 391120, "start": 3915.48, "end": 3917.8399999999997, "text": " Outp is a tensor.", "tokens": [5925, 79, 307, 257, 40863, 13], "temperature": 0.0, "avg_logprob": -0.16592927736656687, "compression_ratio": 1.5905172413793103, "no_speech_prob": 5.255381438473705e-06}, {"id": 830, "seek": 391120, "start": 3917.8399999999997, "end": 3919.12, "text": " How big is the tensor?", "tokens": [1012, 955, 307, 264, 40863, 30], "temperature": 0.0, "avg_logprob": -0.16592927736656687, "compression_ratio": 1.5905172413793103, "no_speech_prob": 5.255381438473705e-06}, {"id": 831, "seek": 391120, "start": 3919.12, "end": 3923.04, "text": " So before you look it up in the debugger, try and figure it out from first principles", "tokens": [407, 949, 291, 574, 309, 493, 294, 264, 24083, 1321, 11, 853, 293, 2573, 309, 484, 490, 700, 9156], "temperature": 0.0, "avg_logprob": -0.16592927736656687, "compression_ratio": 1.5905172413793103, "no_speech_prob": 5.255381438473705e-06}, {"id": 832, "seek": 391120, "start": 3923.04, "end": 3924.4399999999996, "text": " and check your right.", "tokens": [293, 1520, 428, 558, 13], "temperature": 0.0, "avg_logprob": -0.16592927736656687, "compression_ratio": 1.5905172413793103, "no_speech_prob": 5.255381438473705e-06}, {"id": 833, "seek": 391120, "start": 3924.4399999999996, "end": 3932.06, "text": " So outp is a tensor whose length is equal to the number of words in our English vocabulary,", "tokens": [407, 484, 79, 307, 257, 40863, 6104, 4641, 307, 2681, 281, 264, 1230, 295, 2283, 294, 527, 3669, 19864, 11], "temperature": 0.0, "avg_logprob": -0.16592927736656687, "compression_ratio": 1.5905172413793103, "no_speech_prob": 5.255381438473705e-06}, {"id": 834, "seek": 391120, "start": 3932.06, "end": 3938.7999999999997, "text": " and it contains the probability for every one of those words that it is that word.", "tokens": [293, 309, 8306, 264, 8482, 337, 633, 472, 295, 729, 2283, 300, 309, 307, 300, 1349, 13], "temperature": 0.0, "avg_logprob": -0.16592927736656687, "compression_ratio": 1.5905172413793103, "no_speech_prob": 5.255381438473705e-06}, {"id": 835, "seek": 393880, "start": 3938.8, "end": 3948.32, "text": " So then, if we now say outp.data.max that looks in its tensor to find out which word", "tokens": [407, 550, 11, 498, 321, 586, 584, 484, 79, 13, 67, 3274, 13, 41167, 300, 1542, 294, 1080, 40863, 281, 915, 484, 597, 1349], "temperature": 0.0, "avg_logprob": -0.13769972324371338, "compression_ratio": 1.755868544600939, "no_speech_prob": 1.3631254660140257e-05}, {"id": 836, "seek": 393880, "start": 3948.32, "end": 3951.7200000000003, "text": " has the highest probability.", "tokens": [575, 264, 6343, 8482, 13], "temperature": 0.0, "avg_logprob": -0.13769972324371338, "compression_ratio": 1.755868544600939, "no_speech_prob": 1.3631254660140257e-05}, {"id": 837, "seek": 393880, "start": 3951.7200000000003, "end": 3954.0, "text": " And max in PyTorch returns two things.", "tokens": [400, 11469, 294, 9953, 51, 284, 339, 11247, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.13769972324371338, "compression_ratio": 1.755868544600939, "no_speech_prob": 1.3631254660140257e-05}, {"id": 838, "seek": 393880, "start": 3954.0, "end": 3958.92, "text": " The first thing is what is that max probability, and the second is what is the index into the", "tokens": [440, 700, 551, 307, 437, 307, 300, 11469, 8482, 11, 293, 264, 1150, 307, 437, 307, 264, 8186, 666, 264], "temperature": 0.0, "avg_logprob": -0.13769972324371338, "compression_ratio": 1.755868544600939, "no_speech_prob": 1.3631254660140257e-05}, {"id": 839, "seek": 393880, "start": 3958.92, "end": 3961.2000000000003, "text": " array of that max probability.", "tokens": [10225, 295, 300, 11469, 8482, 13], "temperature": 0.0, "avg_logprob": -0.13769972324371338, "compression_ratio": 1.755868544600939, "no_speech_prob": 1.3631254660140257e-05}, {"id": 840, "seek": 393880, "start": 3961.2000000000003, "end": 3966.76, "text": " And so we want that second item, index number 1, which is the word index with the largest", "tokens": [400, 370, 321, 528, 300, 1150, 3174, 11, 8186, 1230, 502, 11, 597, 307, 264, 1349, 8186, 365, 264, 6443], "temperature": 0.0, "avg_logprob": -0.13769972324371338, "compression_ratio": 1.755868544600939, "no_speech_prob": 1.3631254660140257e-05}, {"id": 841, "seek": 393880, "start": 3966.76, "end": 3968.2000000000003, "text": " thing.", "tokens": [551, 13], "temperature": 0.0, "avg_logprob": -0.13769972324371338, "compression_ratio": 1.755868544600939, "no_speech_prob": 1.3631254660140257e-05}, {"id": 842, "seek": 396820, "start": 3968.2, "end": 3977.16, "text": " So now that contains the word, or the word index into our vocabulary of the word.", "tokens": [407, 586, 300, 8306, 264, 1349, 11, 420, 264, 1349, 8186, 666, 527, 19864, 295, 264, 1349, 13], "temperature": 0.0, "avg_logprob": -0.14275850568498885, "compression_ratio": 1.5821596244131455, "no_speech_prob": 1.963802105819923e-06}, {"id": 843, "seek": 396820, "start": 3977.16, "end": 3982.7599999999998, "text": " If it's a 1, you might remember 1 was padding, then that means we're done.", "tokens": [759, 309, 311, 257, 502, 11, 291, 1062, 1604, 502, 390, 39562, 11, 550, 300, 1355, 321, 434, 1096, 13], "temperature": 0.0, "avg_logprob": -0.14275850568498885, "compression_ratio": 1.5821596244131455, "no_speech_prob": 1.963802105819923e-06}, {"id": 844, "seek": 396820, "start": 3982.7599999999998, "end": 3987.08, "text": " That means we've reached the end because we've finished with a bunch of padding.", "tokens": [663, 1355, 321, 600, 6488, 264, 917, 570, 321, 600, 4335, 365, 257, 3840, 295, 39562, 13], "temperature": 0.0, "avg_logprob": -0.14275850568498885, "compression_ratio": 1.5821596244131455, "no_speech_prob": 1.963802105819923e-06}, {"id": 845, "seek": 396820, "start": 3987.08, "end": 3990.48, "text": " If it's not 1, let's go back and continue.", "tokens": [759, 309, 311, 406, 502, 11, 718, 311, 352, 646, 293, 2354, 13], "temperature": 0.0, "avg_logprob": -0.14275850568498885, "compression_ratio": 1.5821596244131455, "no_speech_prob": 1.963802105819923e-06}, {"id": 846, "seek": 396820, "start": 3990.48, "end": 3997.9199999999996, "text": " Now decimp is whatever the highest probability word was.", "tokens": [823, 979, 8814, 307, 2035, 264, 6343, 8482, 1349, 390, 13], "temperature": 0.0, "avg_logprob": -0.14275850568498885, "compression_ratio": 1.5821596244131455, "no_speech_prob": 1.963802105819923e-06}, {"id": 847, "seek": 399792, "start": 3997.92, "end": 4004.88, "text": " So we keep looping through either until we get to the largest length of a sentence, or", "tokens": [407, 321, 1066, 6367, 278, 807, 2139, 1826, 321, 483, 281, 264, 6443, 4641, 295, 257, 8174, 11, 420], "temperature": 0.0, "avg_logprob": -0.16824130784897579, "compression_ratio": 1.5240384615384615, "no_speech_prob": 1.0289457350154407e-05}, {"id": 848, "seek": 399792, "start": 4004.88, "end": 4009.36, "text": " until everything in our many batch is padding.", "tokens": [1826, 1203, 294, 527, 867, 15245, 307, 39562, 13], "temperature": 0.0, "avg_logprob": -0.16824130784897579, "compression_ratio": 1.5240384615384615, "no_speech_prob": 1.0289457350154407e-05}, {"id": 849, "seek": 399792, "start": 4009.36, "end": 4017.16, "text": " And each time we've appended our outputs, not the word, but the probabilities, to this", "tokens": [400, 1184, 565, 321, 600, 724, 3502, 527, 23930, 11, 406, 264, 1349, 11, 457, 264, 33783, 11, 281, 341], "temperature": 0.0, "avg_logprob": -0.16824130784897579, "compression_ratio": 1.5240384615384615, "no_speech_prob": 1.0289457350154407e-05}, {"id": 850, "seek": 399792, "start": 4017.16, "end": 4022.7400000000002, "text": " list, which we stack up into a tensor, and we can now go ahead and feed that to a loss", "tokens": [1329, 11, 597, 321, 8630, 493, 666, 257, 40863, 11, 293, 321, 393, 586, 352, 2286, 293, 3154, 300, 281, 257, 4470], "temperature": 0.0, "avg_logprob": -0.16824130784897579, "compression_ratio": 1.5240384615384615, "no_speech_prob": 1.0289457350154407e-05}, {"id": 851, "seek": 399792, "start": 4022.7400000000002, "end": 4024.92, "text": " function.", "tokens": [2445, 13], "temperature": 0.0, "avg_logprob": -0.16824130784897579, "compression_ratio": 1.5240384615384615, "no_speech_prob": 1.0289457350154407e-05}, {"id": 852, "seek": 402492, "start": 4024.92, "end": 4033.64, "text": " So before we go to a break, since we've done 1 and 2, let's do 3, which is a loss function.", "tokens": [407, 949, 321, 352, 281, 257, 1821, 11, 1670, 321, 600, 1096, 502, 293, 568, 11, 718, 311, 360, 805, 11, 597, 307, 257, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.13906206943020963, "compression_ratio": 1.669767441860465, "no_speech_prob": 4.565958533930825e-06}, {"id": 853, "seek": 402492, "start": 4033.64, "end": 4038.32, "text": " The loss function is categorical cross-entropy loss.", "tokens": [440, 4470, 2445, 307, 19250, 804, 3278, 12, 317, 27514, 4470, 13], "temperature": 0.0, "avg_logprob": -0.13906206943020963, "compression_ratio": 1.669767441860465, "no_speech_prob": 4.565958533930825e-06}, {"id": 854, "seek": 402492, "start": 4038.32, "end": 4044.2400000000002, "text": " We've got a list of probabilities for each of our classes, where the classes are all", "tokens": [492, 600, 658, 257, 1329, 295, 33783, 337, 1184, 295, 527, 5359, 11, 689, 264, 5359, 366, 439], "temperature": 0.0, "avg_logprob": -0.13906206943020963, "compression_ratio": 1.669767441860465, "no_speech_prob": 4.565958533930825e-06}, {"id": 855, "seek": 402492, "start": 4044.2400000000002, "end": 4049.6, "text": " the words in our English vocab, and we have a target which is the correct class, i.e.", "tokens": [264, 2283, 294, 527, 3669, 2329, 455, 11, 293, 321, 362, 257, 3779, 597, 307, 264, 3006, 1508, 11, 741, 13, 68, 13], "temperature": 0.0, "avg_logprob": -0.13906206943020963, "compression_ratio": 1.669767441860465, "no_speech_prob": 4.565958533930825e-06}, {"id": 856, "seek": 402492, "start": 4049.6, "end": 4054.2400000000002, "text": " which is the correct word at this location.", "tokens": [597, 307, 264, 3006, 1349, 412, 341, 4914, 13], "temperature": 0.0, "avg_logprob": -0.13906206943020963, "compression_ratio": 1.669767441860465, "no_speech_prob": 4.565958533930825e-06}, {"id": 857, "seek": 405424, "start": 4054.24, "end": 4057.3999999999996, "text": " There's two tweaks, which is why we need to write our own little loss function, but you", "tokens": [821, 311, 732, 46664, 11, 597, 307, 983, 321, 643, 281, 2464, 527, 1065, 707, 4470, 2445, 11, 457, 291], "temperature": 0.0, "avg_logprob": -0.15204375380769783, "compression_ratio": 1.657258064516129, "no_speech_prob": 8.939662620832678e-06}, {"id": 858, "seek": 405424, "start": 4057.3999999999996, "end": 4060.68, "text": " can see basically it's going to be cross-entropy loss.", "tokens": [393, 536, 1936, 309, 311, 516, 281, 312, 3278, 12, 317, 27514, 4470, 13], "temperature": 0.0, "avg_logprob": -0.15204375380769783, "compression_ratio": 1.657258064516129, "no_speech_prob": 8.939662620832678e-06}, {"id": 859, "seek": 405424, "start": 4060.68, "end": 4062.3599999999997, "text": " And the tweaks are as bullets.", "tokens": [400, 264, 46664, 366, 382, 20132, 13], "temperature": 0.0, "avg_logprob": -0.15204375380769783, "compression_ratio": 1.657258064516129, "no_speech_prob": 8.939662620832678e-06}, {"id": 860, "seek": 405424, "start": 4062.3599999999997, "end": 4069.4799999999996, "text": " Tweak number 1 is we might have stopped a little bit early, and so the sequence length", "tokens": [47763, 514, 1230, 502, 307, 321, 1062, 362, 5936, 257, 707, 857, 2440, 11, 293, 370, 264, 8310, 4641], "temperature": 0.0, "avg_logprob": -0.15204375380769783, "compression_ratio": 1.657258064516129, "no_speech_prob": 8.939662620832678e-06}, {"id": 861, "seek": 405424, "start": 4069.4799999999996, "end": 4073.7599999999998, "text": " that we generated may be different to the sequence length of the target, in which case", "tokens": [300, 321, 10833, 815, 312, 819, 281, 264, 8310, 4641, 295, 264, 3779, 11, 294, 597, 1389], "temperature": 0.0, "avg_logprob": -0.15204375380769783, "compression_ratio": 1.657258064516129, "no_speech_prob": 8.939662620832678e-06}, {"id": 862, "seek": 405424, "start": 4073.7599999999998, "end": 4077.2, "text": " we need to add some padding.", "tokens": [321, 643, 281, 909, 512, 39562, 13], "temperature": 0.0, "avg_logprob": -0.15204375380769783, "compression_ratio": 1.657258064516129, "no_speech_prob": 8.939662620832678e-06}, {"id": 863, "seek": 405424, "start": 4077.2, "end": 4079.8199999999997, "text": " PyTorch padding function is weird.", "tokens": [9953, 51, 284, 339, 39562, 2445, 307, 3657, 13], "temperature": 0.0, "avg_logprob": -0.15204375380769783, "compression_ratio": 1.657258064516129, "no_speech_prob": 8.939662620832678e-06}, {"id": 864, "seek": 407982, "start": 4079.82, "end": 4092.44, "text": " If you have a rank 3 tensor, which we do, we have sequence length by batch size by number", "tokens": [759, 291, 362, 257, 6181, 805, 40863, 11, 597, 321, 360, 11, 321, 362, 8310, 4641, 538, 15245, 2744, 538, 1230], "temperature": 0.0, "avg_logprob": -0.20426220052382527, "compression_ratio": 1.5060975609756098, "no_speech_prob": 1.0129934707947541e-05}, {"id": 865, "seek": 407982, "start": 4092.44, "end": 4094.32, "text": " of words in the vocab.", "tokens": [295, 2283, 294, 264, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.20426220052382527, "compression_ratio": 1.5060975609756098, "no_speech_prob": 1.0129934707947541e-05}, {"id": 866, "seek": 407982, "start": 4094.32, "end": 4099.34, "text": " A rank 3 tensor requires a 6 tuple.", "tokens": [316, 6181, 805, 40863, 7029, 257, 1386, 2604, 781, 13], "temperature": 0.0, "avg_logprob": -0.20426220052382527, "compression_ratio": 1.5060975609756098, "no_speech_prob": 1.0129934707947541e-05}, {"id": 867, "seek": 407982, "start": 4099.34, "end": 4106.8, "text": " Each pair of things in that tuple is the padding before and then the padding after that dimension.", "tokens": [6947, 6119, 295, 721, 294, 300, 2604, 781, 307, 264, 39562, 949, 293, 550, 264, 39562, 934, 300, 10139, 13], "temperature": 0.0, "avg_logprob": -0.20426220052382527, "compression_ratio": 1.5060975609756098, "no_speech_prob": 1.0129934707947541e-05}, {"id": 868, "seek": 410680, "start": 4106.8, "end": 4111.320000000001, "text": " So in this case, the first dimension has no padding, the second dimension has no padding,", "tokens": [407, 294, 341, 1389, 11, 264, 700, 10139, 575, 572, 39562, 11, 264, 1150, 10139, 575, 572, 39562, 11], "temperature": 0.0, "avg_logprob": -0.13351574650517217, "compression_ratio": 1.7543103448275863, "no_speech_prob": 7.18320598025457e-06}, {"id": 869, "seek": 410680, "start": 4111.320000000001, "end": 4115.68, "text": " the third dimension has no padding on the left, and as match padding is required on", "tokens": [264, 2636, 10139, 575, 572, 39562, 322, 264, 1411, 11, 293, 382, 2995, 39562, 307, 4739, 322], "temperature": 0.0, "avg_logprob": -0.13351574650517217, "compression_ratio": 1.7543103448275863, "no_speech_prob": 7.18320598025457e-06}, {"id": 870, "seek": 410680, "start": 4115.68, "end": 4116.68, "text": " the right.", "tokens": [264, 558, 13], "temperature": 0.0, "avg_logprob": -0.13351574650517217, "compression_ratio": 1.7543103448275863, "no_speech_prob": 7.18320598025457e-06}, {"id": 871, "seek": 410680, "start": 4116.68, "end": 4120.76, "text": " It's good to know how to use that function.", "tokens": [467, 311, 665, 281, 458, 577, 281, 764, 300, 2445, 13], "temperature": 0.0, "avg_logprob": -0.13351574650517217, "compression_ratio": 1.7543103448275863, "no_speech_prob": 7.18320598025457e-06}, {"id": 872, "seek": 410680, "start": 4120.76, "end": 4124.0, "text": " Now that we've added any padding, that's necessary.", "tokens": [823, 300, 321, 600, 3869, 604, 39562, 11, 300, 311, 4818, 13], "temperature": 0.0, "avg_logprob": -0.13351574650517217, "compression_ratio": 1.7543103448275863, "no_speech_prob": 7.18320598025457e-06}, {"id": 873, "seek": 410680, "start": 4124.0, "end": 4131.52, "text": " The only other thing we need to do is cross-entropy loss expects a rank 2 tensor, but we've got", "tokens": [440, 787, 661, 551, 321, 643, 281, 360, 307, 3278, 12, 317, 27514, 4470, 33280, 257, 6181, 568, 40863, 11, 457, 321, 600, 658], "temperature": 0.0, "avg_logprob": -0.13351574650517217, "compression_ratio": 1.7543103448275863, "no_speech_prob": 7.18320598025457e-06}, {"id": 874, "seek": 410680, "start": 4131.52, "end": 4133.56, "text": " sequence length by batch size.", "tokens": [8310, 4641, 538, 15245, 2744, 13], "temperature": 0.0, "avg_logprob": -0.13351574650517217, "compression_ratio": 1.7543103448275863, "no_speech_prob": 7.18320598025457e-06}, {"id": 875, "seek": 413356, "start": 4133.56, "end": 4139.120000000001, "text": " So let's just flatten out the sequence length and batch size into a, that's what that minus", "tokens": [407, 718, 311, 445, 24183, 484, 264, 8310, 4641, 293, 15245, 2744, 666, 257, 11, 300, 311, 437, 300, 3175], "temperature": 0.0, "avg_logprob": -0.2181867822870478, "compression_ratio": 1.635135135135135, "no_speech_prob": 5.682421488018008e-06}, {"id": 876, "seek": 413356, "start": 4139.120000000001, "end": 4141.160000000001, "text": " 1 in Vue does.", "tokens": [502, 294, 691, 622, 775, 13], "temperature": 0.0, "avg_logprob": -0.2181867822870478, "compression_ratio": 1.635135135135135, "no_speech_prob": 5.682421488018008e-06}, {"id": 877, "seek": 413356, "start": 4141.160000000001, "end": 4146.84, "text": " So flatten out that for both of them, and now we can go ahead and call cross-entropy.", "tokens": [407, 24183, 484, 300, 337, 1293, 295, 552, 11, 293, 586, 321, 393, 352, 2286, 293, 818, 3278, 12, 317, 27514, 13], "temperature": 0.0, "avg_logprob": -0.2181867822870478, "compression_ratio": 1.635135135135135, "no_speech_prob": 5.682421488018008e-06}, {"id": 878, "seek": 413356, "start": 4146.84, "end": 4148.200000000001, "text": " That's it.", "tokens": [663, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.2181867822870478, "compression_ratio": 1.635135135135135, "no_speech_prob": 5.682421488018008e-06}, {"id": 879, "seek": 413356, "start": 4148.200000000001, "end": 4153.56, "text": " So now we can just use standard approach.", "tokens": [407, 586, 321, 393, 445, 764, 3832, 3109, 13], "temperature": 0.0, "avg_logprob": -0.2181867822870478, "compression_ratio": 1.635135135135135, "no_speech_prob": 5.682421488018008e-06}, {"id": 880, "seek": 413356, "start": 4153.56, "end": 4156.76, "text": " Here's our sequence to sequence RNN, that's this one here.", "tokens": [1692, 311, 527, 8310, 281, 8310, 45702, 45, 11, 300, 311, 341, 472, 510, 13], "temperature": 0.0, "avg_logprob": -0.2181867822870478, "compression_ratio": 1.635135135135135, "no_speech_prob": 5.682421488018008e-06}, {"id": 881, "seek": 413356, "start": 4156.76, "end": 4160.76, "text": " So that is a standard PyTorch module.", "tokens": [407, 300, 307, 257, 3832, 9953, 51, 284, 339, 10088, 13], "temperature": 0.0, "avg_logprob": -0.2181867822870478, "compression_ratio": 1.635135135135135, "no_speech_prob": 5.682421488018008e-06}, {"id": 882, "seek": 413356, "start": 4160.76, "end": 4163.320000000001, "text": " Stick it on the GPU.", "tokens": [22744, 309, 322, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.2181867822870478, "compression_ratio": 1.635135135135135, "no_speech_prob": 5.682421488018008e-06}, {"id": 883, "seek": 416332, "start": 4163.32, "end": 4170.04, "text": " Only by now you've noticed you can call.cuda, but if you call to GPU, then it doesn't put", "tokens": [5686, 538, 586, 291, 600, 5694, 291, 393, 818, 2411, 66, 11152, 11, 457, 498, 291, 818, 281, 18407, 11, 550, 309, 1177, 380, 829], "temperature": 0.0, "avg_logprob": -0.16069753895635192, "compression_ratio": 1.6639344262295082, "no_speech_prob": 1.321189211012097e-05}, {"id": 884, "seek": 416332, "start": 4170.04, "end": 4172.12, "text": " it on the GPU if you don't have one.", "tokens": [309, 322, 264, 18407, 498, 291, 500, 380, 362, 472, 13], "temperature": 0.0, "avg_logprob": -0.16069753895635192, "compression_ratio": 1.6639344262295082, "no_speech_prob": 1.321189211012097e-05}, {"id": 885, "seek": 416332, "start": 4172.12, "end": 4177.759999999999, "text": " You can also set fastai.core.useGPU to false to force it to not use the GPU, and that can", "tokens": [509, 393, 611, 992, 2370, 1301, 13, 12352, 13, 438, 38, 8115, 281, 7908, 281, 3464, 309, 281, 406, 764, 264, 18407, 11, 293, 300, 393], "temperature": 0.0, "avg_logprob": -0.16069753895635192, "compression_ratio": 1.6639344262295082, "no_speech_prob": 1.321189211012097e-05}, {"id": 886, "seek": 416332, "start": 4177.759999999999, "end": 4181.4, "text": " be super handy for debugging.", "tokens": [312, 1687, 13239, 337, 45592, 13], "temperature": 0.0, "avg_logprob": -0.16069753895635192, "compression_ratio": 1.6639344262295082, "no_speech_prob": 1.321189211012097e-05}, {"id": 887, "seek": 416332, "start": 4181.4, "end": 4187.82, "text": " We then need something that tells it how to handle learning rate groups.", "tokens": [492, 550, 643, 746, 300, 5112, 309, 577, 281, 4813, 2539, 3314, 3935, 13], "temperature": 0.0, "avg_logprob": -0.16069753895635192, "compression_ratio": 1.6639344262295082, "no_speech_prob": 1.321189211012097e-05}, {"id": 888, "seek": 416332, "start": 4187.82, "end": 4191.36, "text": " So there's a thing called single model that you can pass it to, which treats the whole", "tokens": [407, 456, 311, 257, 551, 1219, 2167, 2316, 300, 291, 393, 1320, 309, 281, 11, 597, 19566, 264, 1379], "temperature": 0.0, "avg_logprob": -0.16069753895635192, "compression_ratio": 1.6639344262295082, "no_speech_prob": 1.321189211012097e-05}, {"id": 889, "seek": 419136, "start": 4191.36, "end": 4193.96, "text": " thing as a single learning rate group.", "tokens": [551, 382, 257, 2167, 2539, 3314, 1594, 13], "temperature": 0.0, "avg_logprob": -0.17928830437038257, "compression_ratio": 1.6127450980392157, "no_speech_prob": 1.0677018735805177e-06}, {"id": 890, "seek": 419136, "start": 4193.96, "end": 4201.28, "text": " So this is like the easiest way to turn a PyTorch module into a fastai model.", "tokens": [407, 341, 307, 411, 264, 12889, 636, 281, 1261, 257, 9953, 51, 284, 339, 10088, 666, 257, 2370, 1301, 2316, 13], "temperature": 0.0, "avg_logprob": -0.17928830437038257, "compression_ratio": 1.6127450980392157, "no_speech_prob": 1.0677018735805177e-06}, {"id": 891, "seek": 419136, "start": 4201.28, "end": 4205.2, "text": " Here's the model data object we created before.", "tokens": [1692, 311, 264, 2316, 1412, 2657, 321, 2942, 949, 13], "temperature": 0.0, "avg_logprob": -0.17928830437038257, "compression_ratio": 1.6127450980392157, "no_speech_prob": 1.0677018735805177e-06}, {"id": 892, "seek": 419136, "start": 4205.2, "end": 4212.12, "text": " We could then just call learner to turn that into a learner, but if we call RNN learner,", "tokens": [492, 727, 550, 445, 818, 33347, 281, 1261, 300, 666, 257, 33347, 11, 457, 498, 321, 818, 45702, 45, 33347, 11], "temperature": 0.0, "avg_logprob": -0.17928830437038257, "compression_ratio": 1.6127450980392157, "no_speech_prob": 1.0677018735805177e-06}, {"id": 893, "seek": 419136, "start": 4212.12, "end": 4216.5199999999995, "text": " RNN learner is a learner.", "tokens": [45702, 45, 33347, 307, 257, 33347, 13], "temperature": 0.0, "avg_logprob": -0.17928830437038257, "compression_ratio": 1.6127450980392157, "no_speech_prob": 1.0677018735805177e-06}, {"id": 894, "seek": 419136, "start": 4216.5199999999995, "end": 4219.44, "text": " It defines cross-entropy as the default criteria.", "tokens": [467, 23122, 3278, 12, 317, 27514, 382, 264, 7576, 11101, 13], "temperature": 0.0, "avg_logprob": -0.17928830437038257, "compression_ratio": 1.6127450980392157, "no_speech_prob": 1.0677018735805177e-06}, {"id": 895, "seek": 421944, "start": 4219.44, "end": 4222.96, "text": " In this case, we're overriding that anyway, so that's not what we care about.", "tokens": [682, 341, 1389, 11, 321, 434, 670, 81, 2819, 300, 4033, 11, 370, 300, 311, 406, 437, 321, 1127, 466, 13], "temperature": 0.0, "avg_logprob": -0.13840613522372402, "compression_ratio": 1.5571428571428572, "no_speech_prob": 1.80583811015822e-05}, {"id": 896, "seek": 421944, "start": 4222.96, "end": 4232.24, "text": " But it does add in these save encoder and load encoder things that can be handy sometimes.", "tokens": [583, 309, 775, 909, 294, 613, 3155, 2058, 19866, 293, 3677, 2058, 19866, 721, 300, 393, 312, 13239, 2171, 13], "temperature": 0.0, "avg_logprob": -0.13840613522372402, "compression_ratio": 1.5571428571428572, "no_speech_prob": 1.80583811015822e-05}, {"id": 897, "seek": 421944, "start": 4232.24, "end": 4238.44, "text": " In this case, we really could have just said learner, but RNN learner also works.", "tokens": [682, 341, 1389, 11, 321, 534, 727, 362, 445, 848, 33347, 11, 457, 45702, 45, 33347, 611, 1985, 13], "temperature": 0.0, "avg_logprob": -0.13840613522372402, "compression_ratio": 1.5571428571428572, "no_speech_prob": 1.80583811015822e-05}, {"id": 898, "seek": 421944, "start": 4238.44, "end": 4246.16, "text": " So here's how we turn our PyTorch module into a fastai model into a learner.", "tokens": [407, 510, 311, 577, 321, 1261, 527, 9953, 51, 284, 339, 10088, 666, 257, 2370, 1301, 2316, 666, 257, 33347, 13], "temperature": 0.0, "avg_logprob": -0.13840613522372402, "compression_ratio": 1.5571428571428572, "no_speech_prob": 1.80583811015822e-05}, {"id": 899, "seek": 424616, "start": 4246.16, "end": 4253.44, "text": " And once we have a learner, give it our new loss function, and then we can call LR find", "tokens": [400, 1564, 321, 362, 257, 33347, 11, 976, 309, 527, 777, 4470, 2445, 11, 293, 550, 321, 393, 818, 441, 49, 915], "temperature": 0.0, "avg_logprob": -0.18334502440232497, "compression_ratio": 1.64, "no_speech_prob": 5.682417850039201e-06}, {"id": 900, "seek": 424616, "start": 4253.44, "end": 4259.2, "text": " and we can call fit, and it runs for a while, and we can save it.", "tokens": [293, 321, 393, 818, 3318, 11, 293, 309, 6676, 337, 257, 1339, 11, 293, 321, 393, 3155, 309, 13], "temperature": 0.0, "avg_logprob": -0.18334502440232497, "compression_ratio": 1.64, "no_speech_prob": 5.682417850039201e-06}, {"id": 901, "seek": 424616, "start": 4259.2, "end": 4262.8, "text": " So all the normal learn stuff now works.", "tokens": [407, 439, 264, 2710, 1466, 1507, 586, 1985, 13], "temperature": 0.0, "avg_logprob": -0.18334502440232497, "compression_ratio": 1.64, "no_speech_prob": 5.682417850039201e-06}, {"id": 902, "seek": 424616, "start": 4262.8, "end": 4267.68, "text": " Remember the model attribute of a learner is a standard PyTorch model, so we can pass", "tokens": [5459, 264, 2316, 19667, 295, 257, 33347, 307, 257, 3832, 9953, 51, 284, 339, 2316, 11, 370, 321, 393, 1320], "temperature": 0.0, "avg_logprob": -0.18334502440232497, "compression_ratio": 1.64, "no_speech_prob": 5.682417850039201e-06}, {"id": 903, "seek": 424616, "start": 4267.68, "end": 4274.4, "text": " that some x, which we can grab out of our validation set, or you could use learn.predict", "tokens": [300, 512, 2031, 11, 597, 321, 393, 4444, 484, 295, 527, 24071, 992, 11, 420, 291, 727, 764, 1466, 13, 79, 24945], "temperature": 0.0, "avg_logprob": -0.18334502440232497, "compression_ratio": 1.64, "no_speech_prob": 5.682417850039201e-06}, {"id": 904, "seek": 427440, "start": 4274.4, "end": 4279.879999999999, "text": " array or whatever you like to get some predictions.", "tokens": [10225, 420, 2035, 291, 411, 281, 483, 512, 21264, 13], "temperature": 0.0, "avg_logprob": -0.12302613258361816, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.165944013337139e-05}, {"id": 905, "seek": 427440, "start": 4279.879999999999, "end": 4286.08, "text": " And then we can convert those predictions into words by going.max1 to grab the index", "tokens": [400, 550, 321, 393, 7620, 729, 21264, 666, 2283, 538, 516, 2411, 41167, 16, 281, 4444, 264, 8186], "temperature": 0.0, "avg_logprob": -0.12302613258361816, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.165944013337139e-05}, {"id": 906, "seek": 427440, "start": 4286.08, "end": 4290.5199999999995, "text": " of the highest probability words to get some predictions.", "tokens": [295, 264, 6343, 8482, 2283, 281, 483, 512, 21264, 13], "temperature": 0.0, "avg_logprob": -0.12302613258361816, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.165944013337139e-05}, {"id": 907, "seek": 427440, "start": 4290.5199999999995, "end": 4297.759999999999, "text": " And then we can go through a few examples and print out the French, the correct English,", "tokens": [400, 550, 321, 393, 352, 807, 257, 1326, 5110, 293, 4482, 484, 264, 5522, 11, 264, 3006, 3669, 11], "temperature": 0.0, "avg_logprob": -0.12302613258361816, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.165944013337139e-05}, {"id": 908, "seek": 427440, "start": 4297.759999999999, "end": 4303.639999999999, "text": " and the predicted English for things that are not padding.", "tokens": [293, 264, 19147, 3669, 337, 721, 300, 366, 406, 39562, 13], "temperature": 0.0, "avg_logprob": -0.12302613258361816, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.165944013337139e-05}, {"id": 909, "seek": 430364, "start": 4303.64, "end": 4305.400000000001, "text": " And here we go.", "tokens": [400, 510, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.15756139021653395, "compression_ratio": 1.4639175257731958, "no_speech_prob": 1.0783239304146264e-05}, {"id": 910, "seek": 430364, "start": 4305.400000000001, "end": 4314.6, "text": " So amazingly enough, this kind of like simplest possible written largely from scratch PyTorch", "tokens": [407, 31762, 1547, 11, 341, 733, 295, 411, 22811, 1944, 3720, 11611, 490, 8459, 9953, 51, 284, 339], "temperature": 0.0, "avg_logprob": -0.15756139021653395, "compression_ratio": 1.4639175257731958, "no_speech_prob": 1.0783239304146264e-05}, {"id": 911, "seek": 430364, "start": 4314.6, "end": 4320.8, "text": " module on only 50,000 sentences is sometimes capable on a validation set of giving you", "tokens": [10088, 322, 787, 2625, 11, 1360, 16579, 307, 2171, 8189, 322, 257, 24071, 992, 295, 2902, 291], "temperature": 0.0, "avg_logprob": -0.15756139021653395, "compression_ratio": 1.4639175257731958, "no_speech_prob": 1.0783239304146264e-05}, {"id": 912, "seek": 430364, "start": 4320.8, "end": 4329.160000000001, "text": " exactly the right answer, sometimes the right answer in slightly different wording, and", "tokens": [2293, 264, 558, 1867, 11, 2171, 264, 558, 1867, 294, 4748, 819, 47602, 11, 293], "temperature": 0.0, "avg_logprob": -0.15756139021653395, "compression_ratio": 1.4639175257731958, "no_speech_prob": 1.0783239304146264e-05}, {"id": 913, "seek": 432916, "start": 4329.16, "end": 4334.16, "text": " sometimes sentences that aren't grammatically sensible or even have too many question marks.", "tokens": [2171, 16579, 300, 3212, 380, 17570, 5030, 25380, 420, 754, 362, 886, 867, 1168, 10640, 13], "temperature": 0.0, "avg_logprob": -0.13357210640955453, "compression_ratio": 1.5984251968503937, "no_speech_prob": 7.071817890391685e-06}, {"id": 914, "seek": 432916, "start": 4334.16, "end": 4338.44, "text": " So we're well on the right track, I think you would agree.", "tokens": [407, 321, 434, 731, 322, 264, 558, 2837, 11, 286, 519, 291, 576, 3986, 13], "temperature": 0.0, "avg_logprob": -0.13357210640955453, "compression_ratio": 1.5984251968503937, "no_speech_prob": 7.071817890391685e-06}, {"id": 915, "seek": 432916, "start": 4338.44, "end": 4345.8, "text": " So even the simplest possible sec2sec trained for a very small number of epochs without", "tokens": [407, 754, 264, 22811, 1944, 907, 17, 8159, 8895, 337, 257, 588, 1359, 1230, 295, 30992, 28346, 1553], "temperature": 0.0, "avg_logprob": -0.13357210640955453, "compression_ratio": 1.5984251968503937, "no_speech_prob": 7.071817890391685e-06}, {"id": 916, "seek": 432916, "start": 4345.8, "end": 4352.639999999999, "text": " any pre-training other than the use of word embeddings is surprisingly good.", "tokens": [604, 659, 12, 17227, 1760, 661, 813, 264, 764, 295, 1349, 12240, 29432, 307, 17600, 665, 13], "temperature": 0.0, "avg_logprob": -0.13357210640955453, "compression_ratio": 1.5984251968503937, "no_speech_prob": 7.071817890391685e-06}, {"id": 917, "seek": 432916, "start": 4352.639999999999, "end": 4356.4, "text": " So I think the message here, and we're going to improve this in a moment after the break,", "tokens": [407, 286, 519, 264, 3636, 510, 11, 293, 321, 434, 516, 281, 3470, 341, 294, 257, 1623, 934, 264, 1821, 11], "temperature": 0.0, "avg_logprob": -0.13357210640955453, "compression_ratio": 1.5984251968503937, "no_speech_prob": 7.071817890391685e-06}, {"id": 918, "seek": 435640, "start": 4356.4, "end": 4362.599999999999, "text": " but I think the message here is even sequence2sequence models that you think are simpler than could", "tokens": [457, 286, 519, 264, 3636, 510, 307, 754, 8310, 17, 11834, 655, 5245, 300, 291, 519, 366, 18587, 813, 727], "temperature": 0.0, "avg_logprob": -0.18620076591585888, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.777814031811431e-05}, {"id": 919, "seek": 435640, "start": 4362.599999999999, "end": 4368.5199999999995, "text": " possibly work, even with less data than you think you could learn from can be surprisingly", "tokens": [6264, 589, 11, 754, 365, 1570, 1412, 813, 291, 519, 291, 727, 1466, 490, 393, 312, 17600], "temperature": 0.0, "avg_logprob": -0.18620076591585888, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.777814031811431e-05}, {"id": 920, "seek": 435640, "start": 4368.5199999999995, "end": 4374.799999999999, "text": " effective and in certain situations this may not even be enough for your needs.", "tokens": [4942, 293, 294, 1629, 6851, 341, 815, 406, 754, 312, 1547, 337, 428, 2203, 13], "temperature": 0.0, "avg_logprob": -0.18620076591585888, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.777814031811431e-05}, {"id": 921, "seek": 435640, "start": 4374.799999999999, "end": 4383.179999999999, "text": " So we're going to learn a few tricks after the break which will make this much better.", "tokens": [407, 321, 434, 516, 281, 1466, 257, 1326, 11733, 934, 264, 1821, 597, 486, 652, 341, 709, 1101, 13], "temperature": 0.0, "avg_logprob": -0.18620076591585888, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.777814031811431e-05}, {"id": 922, "seek": 438318, "start": 4383.18, "end": 4390.76, "text": " So let's come back at 7.50.", "tokens": [407, 718, 311, 808, 646, 412, 1614, 13, 2803, 13], "temperature": 0.0, "avg_logprob": -0.10158903815529563, "compression_ratio": 1.4344827586206896, "no_speech_prob": 8.53015899338061e-06}, {"id": 923, "seek": 438318, "start": 4390.76, "end": 4400.84, "text": " So one question that came up during the break is that some of the tokens that are missing", "tokens": [407, 472, 1168, 300, 1361, 493, 1830, 264, 1821, 307, 300, 512, 295, 264, 22667, 300, 366, 5361], "temperature": 0.0, "avg_logprob": -0.10158903815529563, "compression_ratio": 1.4344827586206896, "no_speech_prob": 8.53015899338061e-06}, {"id": 924, "seek": 438318, "start": 4400.84, "end": 4408.72, "text": " in fast text had a curly quote rather than a straight quote, for example, and the question", "tokens": [294, 2370, 2487, 632, 257, 32066, 6513, 2831, 813, 257, 2997, 6513, 11, 337, 1365, 11, 293, 264, 1168], "temperature": 0.0, "avg_logprob": -0.10158903815529563, "compression_ratio": 1.4344827586206896, "no_speech_prob": 8.53015899338061e-06}, {"id": 925, "seek": 440872, "start": 4408.72, "end": 4415.76, "text": " was would it help to normalize punctuation?", "tokens": [390, 576, 309, 854, 281, 2710, 1125, 27006, 16073, 30], "temperature": 0.0, "avg_logprob": -0.17496211619316776, "compression_ratio": 1.5903083700440528, "no_speech_prob": 6.4389678300358355e-06}, {"id": 926, "seek": 440872, "start": 4415.76, "end": 4419.56, "text": " And the answer for this particular case is probably yes.", "tokens": [400, 264, 1867, 337, 341, 1729, 1389, 307, 1391, 2086, 13], "temperature": 0.0, "avg_logprob": -0.17496211619316776, "compression_ratio": 1.5903083700440528, "no_speech_prob": 6.4389678300358355e-06}, {"id": 927, "seek": 440872, "start": 4419.56, "end": 4424.360000000001, "text": " The difference between curly quotes and straight quotes is rarely semantic.", "tokens": [440, 2649, 1296, 32066, 19963, 293, 2997, 19963, 307, 13752, 47982, 13], "temperature": 0.0, "avg_logprob": -0.17496211619316776, "compression_ratio": 1.5903083700440528, "no_speech_prob": 6.4389678300358355e-06}, {"id": 928, "seek": 440872, "start": 4424.360000000001, "end": 4431.18, "text": " You do have to be very careful though because like it may turn out that people using beautiful", "tokens": [509, 360, 362, 281, 312, 588, 5026, 1673, 570, 411, 309, 815, 1261, 484, 300, 561, 1228, 2238], "temperature": 0.0, "avg_logprob": -0.17496211619316776, "compression_ratio": 1.5903083700440528, "no_speech_prob": 6.4389678300358355e-06}, {"id": 929, "seek": 440872, "start": 4431.18, "end": 4437.38, "text": " curly quotes are like using more formal language and actually writing in a different way.", "tokens": [32066, 19963, 366, 411, 1228, 544, 9860, 2856, 293, 767, 3579, 294, 257, 819, 636, 13], "temperature": 0.0, "avg_logprob": -0.17496211619316776, "compression_ratio": 1.5903083700440528, "no_speech_prob": 6.4389678300358355e-06}, {"id": 930, "seek": 443738, "start": 4437.38, "end": 4444.400000000001, "text": " So I generally, if you're going to do some kind of preprocessing like punctuation normalization,", "tokens": [407, 286, 5101, 11, 498, 291, 434, 516, 281, 360, 512, 733, 295, 2666, 340, 780, 278, 411, 27006, 16073, 2710, 2144, 11], "temperature": 0.0, "avg_logprob": -0.15821097270551934, "compression_ratio": 1.5681818181818181, "no_speech_prob": 8.939568942878395e-06}, {"id": 931, "seek": 443738, "start": 4444.400000000001, "end": 4449.56, "text": " you should definitely check your results with and without because like nearly always that", "tokens": [291, 820, 2138, 1520, 428, 3542, 365, 293, 1553, 570, 411, 6217, 1009, 300], "temperature": 0.0, "avg_logprob": -0.15821097270551934, "compression_ratio": 1.5681818181818181, "no_speech_prob": 8.939568942878395e-06}, {"id": 932, "seek": 443738, "start": 4449.56, "end": 4457.84, "text": " kind of preprocessing makes things worse even when I'm sure it won't.", "tokens": [733, 295, 2666, 340, 780, 278, 1669, 721, 5324, 754, 562, 286, 478, 988, 309, 1582, 380, 13], "temperature": 0.0, "avg_logprob": -0.15821097270551934, "compression_ratio": 1.5681818181818181, "no_speech_prob": 8.939568942878395e-06}, {"id": 933, "seek": 443738, "start": 4457.84, "end": 4463.04, "text": " What may be some ways of regularizing these sequence2sequence models besides dropout and", "tokens": [708, 815, 312, 512, 2098, 295, 3890, 3319, 613, 8310, 17, 11834, 655, 5245, 11868, 3270, 346, 293], "temperature": 0.0, "avg_logprob": -0.15821097270551934, "compression_ratio": 1.5681818181818181, "no_speech_prob": 8.939568942878395e-06}, {"id": 934, "seek": 446304, "start": 4463.04, "end": 4468.28, "text": " wait to get?", "tokens": [1699, 281, 483, 30], "temperature": 0.0, "avg_logprob": -0.24191661796184502, "compression_ratio": 1.4977168949771689, "no_speech_prob": 7.296305739146192e-06}, {"id": 935, "seek": 446304, "start": 4468.28, "end": 4470.4, "text": " Let me think about that during the week.", "tokens": [961, 385, 519, 466, 300, 1830, 264, 1243, 13], "temperature": 0.0, "avg_logprob": -0.24191661796184502, "compression_ratio": 1.4977168949771689, "no_speech_prob": 7.296305739146192e-06}, {"id": 936, "seek": 446304, "start": 4470.4, "end": 4480.96, "text": " Yeah, it's like, you know, AWD LSTM, which we've been relying on a lot has so many great,", "tokens": [865, 11, 309, 311, 411, 11, 291, 458, 11, 25815, 35, 441, 6840, 44, 11, 597, 321, 600, 668, 24140, 322, 257, 688, 575, 370, 867, 869, 11], "temperature": 0.0, "avg_logprob": -0.24191661796184502, "compression_ratio": 1.4977168949771689, "no_speech_prob": 7.296305739146192e-06}, {"id": 937, "seek": 446304, "start": 4480.96, "end": 4486.92, "text": " I mean it's all dropout, well not all dropout, there's dropout of many different kinds.", "tokens": [286, 914, 309, 311, 439, 3270, 346, 11, 731, 406, 439, 3270, 346, 11, 456, 311, 3270, 346, 295, 867, 819, 3685, 13], "temperature": 0.0, "avg_logprob": -0.24191661796184502, "compression_ratio": 1.4977168949771689, "no_speech_prob": 7.296305739146192e-06}, {"id": 938, "seek": 446304, "start": 4486.92, "end": 4492.08, "text": " And then there's the, we haven't talked about it much, but there's also a kind of regularization", "tokens": [400, 550, 456, 311, 264, 11, 321, 2378, 380, 2825, 466, 309, 709, 11, 457, 456, 311, 611, 257, 733, 295, 3890, 2144], "temperature": 0.0, "avg_logprob": -0.24191661796184502, "compression_ratio": 1.4977168949771689, "no_speech_prob": 7.296305739146192e-06}, {"id": 939, "seek": 449208, "start": 4492.08, "end": 4500.04, "text": " based on activations and stuff like that as well and on changes and whatever.", "tokens": [2361, 322, 2430, 763, 293, 1507, 411, 300, 382, 731, 293, 322, 2962, 293, 2035, 13], "temperature": 0.0, "avg_logprob": -0.13207749759449677, "compression_ratio": 1.5201793721973094, "no_speech_prob": 1.2411306670401245e-05}, {"id": 940, "seek": 449208, "start": 4500.04, "end": 4505.32, "text": " I just haven't seen anybody put anything like that amount of work into regularization of", "tokens": [286, 445, 2378, 380, 1612, 4472, 829, 1340, 411, 300, 2372, 295, 589, 666, 3890, 2144, 295], "temperature": 0.0, "avg_logprob": -0.13207749759449677, "compression_ratio": 1.5201793721973094, "no_speech_prob": 1.2411306670401245e-05}, {"id": 941, "seek": 449208, "start": 4505.32, "end": 4510.6, "text": " sequence2sequence models and I think there's a huge opportunity for somebody to do like", "tokens": [8310, 17, 11834, 655, 5245, 293, 286, 519, 456, 311, 257, 2603, 2650, 337, 2618, 281, 360, 411], "temperature": 0.0, "avg_logprob": -0.13207749759449677, "compression_ratio": 1.5201793721973094, "no_speech_prob": 1.2411306670401245e-05}, {"id": 942, "seek": 449208, "start": 4510.6, "end": 4517.36, "text": " the AWD LSTM of sec2sec, which might be as simple as stealing all the ideas from AWD", "tokens": [264, 25815, 35, 441, 6840, 44, 295, 907, 17, 8159, 11, 597, 1062, 312, 382, 2199, 382, 19757, 439, 264, 3487, 490, 25815, 35], "temperature": 0.0, "avg_logprob": -0.13207749759449677, "compression_ratio": 1.5201793721973094, "no_speech_prob": 1.2411306670401245e-05}, {"id": 943, "seek": 451736, "start": 4517.36, "end": 4525.759999999999, "text": " LSTM and using them directly in sec2sec, that'd be pretty easy to try, I think.", "tokens": [441, 6840, 44, 293, 1228, 552, 3838, 294, 907, 17, 8159, 11, 300, 1116, 312, 1238, 1858, 281, 853, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.19163952933417427, "compression_ratio": 1.5364963503649636, "no_speech_prob": 8.800994692137465e-06}, {"id": 944, "seek": 451736, "start": 4525.759999999999, "end": 4529.599999999999, "text": " And there's been an interesting paper that actually Stephen Meredith added in the last", "tokens": [400, 456, 311, 668, 364, 1880, 3035, 300, 767, 13391, 29737, 3869, 294, 264, 1036], "temperature": 0.0, "avg_logprob": -0.19163952933417427, "compression_ratio": 1.5364963503649636, "no_speech_prob": 8.800994692137465e-06}, {"id": 945, "seek": 451736, "start": 4529.599999999999, "end": 4535.48, "text": " couple of weeks where he used an idea which I don't know if he stole it from me, but it", "tokens": [1916, 295, 3259, 689, 415, 1143, 364, 1558, 597, 286, 500, 380, 458, 498, 415, 16326, 309, 490, 385, 11, 457, 309], "temperature": 0.0, "avg_logprob": -0.19163952933417427, "compression_ratio": 1.5364963503649636, "no_speech_prob": 8.800994692137465e-06}, {"id": 946, "seek": 451736, "start": 4535.48, "end": 4538.5599999999995, "text": " was certainly something I had also recently done and talked about on Twitter.", "tokens": [390, 3297, 746, 286, 632, 611, 3938, 1096, 293, 2825, 466, 322, 5794, 13], "temperature": 0.0, "avg_logprob": -0.19163952933417427, "compression_ratio": 1.5364963503649636, "no_speech_prob": 8.800994692137465e-06}, {"id": 947, "seek": 451736, "start": 4538.5599999999995, "end": 4545.28, "text": " Either way, I'm thrilled that he's done it, which was to take all of those different AWD", "tokens": [13746, 636, 11, 286, 478, 18744, 300, 415, 311, 1096, 309, 11, 597, 390, 281, 747, 439, 295, 729, 819, 25815, 35], "temperature": 0.0, "avg_logprob": -0.19163952933417427, "compression_ratio": 1.5364963503649636, "no_speech_prob": 8.800994692137465e-06}, {"id": 948, "seek": 454528, "start": 4545.28, "end": 4551.36, "text": " LSTM hyperparameters and train a bunch of different models and then use a random forest", "tokens": [441, 6840, 44, 9848, 2181, 335, 6202, 293, 3847, 257, 3840, 295, 819, 5245, 293, 550, 764, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.19038608156401535, "compression_ratio": 1.6227272727272728, "no_speech_prob": 6.540371941810008e-06}, {"id": 949, "seek": 454528, "start": 4551.36, "end": 4555.34, "text": " to find out with feature importance which ones actually matter the most and then figure", "tokens": [281, 915, 484, 365, 4111, 7379, 597, 2306, 767, 1871, 264, 881, 293, 550, 2573], "temperature": 0.0, "avg_logprob": -0.19038608156401535, "compression_ratio": 1.6227272727272728, "no_speech_prob": 6.540371941810008e-06}, {"id": 950, "seek": 454528, "start": 4555.34, "end": 4558.12, "text": " out how to set them.", "tokens": [484, 577, 281, 992, 552, 13], "temperature": 0.0, "avg_logprob": -0.19038608156401535, "compression_ratio": 1.6227272727272728, "no_speech_prob": 6.540371941810008e-06}, {"id": 951, "seek": 454528, "start": 4558.12, "end": 4566.96, "text": " So I think you could totally use this approach to figure out for sec2sec regularization approaches", "tokens": [407, 286, 519, 291, 727, 3879, 764, 341, 3109, 281, 2573, 484, 337, 907, 17, 8159, 3890, 2144, 11587], "temperature": 0.0, "avg_logprob": -0.19038608156401535, "compression_ratio": 1.6227272727272728, "no_speech_prob": 6.540371941810008e-06}, {"id": 952, "seek": 454528, "start": 4566.96, "end": 4570.5199999999995, "text": " which ones are best and optimize them.", "tokens": [597, 2306, 366, 1151, 293, 19719, 552, 13], "temperature": 0.0, "avg_logprob": -0.19038608156401535, "compression_ratio": 1.6227272727272728, "no_speech_prob": 6.540371941810008e-06}, {"id": 953, "seek": 454528, "start": 4570.5199999999995, "end": 4574.599999999999, "text": " That would be amazing.", "tokens": [663, 576, 312, 2243, 13], "temperature": 0.0, "avg_logprob": -0.19038608156401535, "compression_ratio": 1.6227272727272728, "no_speech_prob": 6.540371941810008e-06}, {"id": 954, "seek": 457460, "start": 4574.6, "end": 4580.08, "text": " But at the moment, I don't know that there are additional ideas for sec2sec regularization", "tokens": [583, 412, 264, 1623, 11, 286, 500, 380, 458, 300, 456, 366, 4497, 3487, 337, 907, 17, 8159, 3890, 2144], "temperature": 0.0, "avg_logprob": -0.1988117245660312, "compression_ratio": 1.459016393442623, "no_speech_prob": 3.668840918180649e-06}, {"id": 955, "seek": 457460, "start": 4580.08, "end": 4585.400000000001, "text": " that I can think of beyond what's in that paper for regular language model stuff and", "tokens": [300, 286, 393, 519, 295, 4399, 437, 311, 294, 300, 3035, 337, 3890, 2856, 2316, 1507, 293], "temperature": 0.0, "avg_logprob": -0.1988117245660312, "compression_ratio": 1.459016393442623, "no_speech_prob": 3.668840918180649e-06}, {"id": 956, "seek": 457460, "start": 4585.400000000001, "end": 4589.76, "text": " probably all those same approaches would work.", "tokens": [1391, 439, 729, 912, 11587, 576, 589, 13], "temperature": 0.0, "avg_logprob": -0.1988117245660312, "compression_ratio": 1.459016393442623, "no_speech_prob": 3.668840918180649e-06}, {"id": 957, "seek": 457460, "start": 4589.76, "end": 4593.56, "text": " So tricks.", "tokens": [407, 11733, 13], "temperature": 0.0, "avg_logprob": -0.1988117245660312, "compression_ratio": 1.459016393442623, "no_speech_prob": 3.668840918180649e-06}, {"id": 958, "seek": 457460, "start": 4593.56, "end": 4596.8, "text": " Trick number 1, go bidirectional.", "tokens": [43367, 1230, 502, 11, 352, 12957, 621, 41048, 13], "temperature": 0.0, "avg_logprob": -0.1988117245660312, "compression_ratio": 1.459016393442623, "no_speech_prob": 3.668840918180649e-06}, {"id": 959, "seek": 459680, "start": 4596.8, "end": 4607.4400000000005, "text": " So for classification, my approach to bidirectional that I suggested you use is take all of your", "tokens": [407, 337, 21538, 11, 452, 3109, 281, 12957, 621, 41048, 300, 286, 10945, 291, 764, 307, 747, 439, 295, 428], "temperature": 0.0, "avg_logprob": -0.17721863587697348, "compression_ratio": 1.5054945054945055, "no_speech_prob": 3.6688204545498593e-06}, {"id": 960, "seek": 459680, "start": 4607.4400000000005, "end": 4614.2, "text": " token sequences, spin them around and train a new language model and train a new classifier.", "tokens": [14862, 22978, 11, 6060, 552, 926, 293, 3847, 257, 777, 2856, 2316, 293, 3847, 257, 777, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.17721863587697348, "compression_ratio": 1.5054945054945055, "no_speech_prob": 3.6688204545498593e-06}, {"id": 961, "seek": 459680, "start": 4614.2, "end": 4621.24, "text": " I also mentioned the wiki text pre-trained model, if you replace FWD with BWD in the", "tokens": [286, 611, 2835, 264, 261, 9850, 2487, 659, 12, 17227, 2001, 2316, 11, 498, 291, 7406, 479, 54, 35, 365, 363, 54, 35, 294, 264], "temperature": 0.0, "avg_logprob": -0.17721863587697348, "compression_ratio": 1.5054945054945055, "no_speech_prob": 3.6688204545498593e-06}, {"id": 962, "seek": 462124, "start": 4621.24, "end": 4627.0, "text": " name, you'll get the pre-trained backward model I created for you.", "tokens": [1315, 11, 291, 603, 483, 264, 659, 12, 17227, 2001, 23897, 2316, 286, 2942, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.15813735615123403, "compression_ratio": 1.5849802371541502, "no_speech_prob": 4.5658807721338235e-06}, {"id": 963, "seek": 462124, "start": 4627.0, "end": 4632.12, "text": " Get a set of predictions and then average the predictions just like a normal ensemble.", "tokens": [3240, 257, 992, 295, 21264, 293, 550, 4274, 264, 21264, 445, 411, 257, 2710, 19492, 13], "temperature": 0.0, "avg_logprob": -0.15813735615123403, "compression_ratio": 1.5849802371541502, "no_speech_prob": 4.5658807721338235e-06}, {"id": 964, "seek": 462124, "start": 4632.12, "end": 4636.28, "text": " And that's kind of how we do bidire for that kind of classification.", "tokens": [400, 300, 311, 733, 295, 577, 321, 360, 12957, 621, 337, 300, 733, 295, 21538, 13], "temperature": 0.0, "avg_logprob": -0.15813735615123403, "compression_ratio": 1.5849802371541502, "no_speech_prob": 4.5658807721338235e-06}, {"id": 965, "seek": 462124, "start": 4636.28, "end": 4641.8, "text": " There may be ways to do it end-to-end, but I haven't quite figured them out yet, they're", "tokens": [821, 815, 312, 2098, 281, 360, 309, 917, 12, 1353, 12, 521, 11, 457, 286, 2378, 380, 1596, 8932, 552, 484, 1939, 11, 436, 434], "temperature": 0.0, "avg_logprob": -0.15813735615123403, "compression_ratio": 1.5849802371541502, "no_speech_prob": 4.5658807721338235e-06}, {"id": 966, "seek": 462124, "start": 4641.8, "end": 4646.0, "text": " not in Fast.ai yet, and I don't think anybody's written a paper about them yet, so if you", "tokens": [406, 294, 15968, 13, 1301, 1939, 11, 293, 286, 500, 380, 519, 4472, 311, 3720, 257, 3035, 466, 552, 1939, 11, 370, 498, 291], "temperature": 0.0, "avg_logprob": -0.15813735615123403, "compression_ratio": 1.5849802371541502, "no_speech_prob": 4.5658807721338235e-06}, {"id": 967, "seek": 464600, "start": 4646.0, "end": 4651.72, "text": " figure it out, that's an interesting line of research.", "tokens": [2573, 309, 484, 11, 300, 311, 364, 1880, 1622, 295, 2132, 13], "temperature": 0.0, "avg_logprob": -0.10278548203505479, "compression_ratio": 1.5594059405940595, "no_speech_prob": 2.642571644173586e-06}, {"id": 968, "seek": 464600, "start": 4651.72, "end": 4658.32, "text": " But because we're not doing massive documents where we have to kind of chunk it into separate", "tokens": [583, 570, 321, 434, 406, 884, 5994, 8512, 689, 321, 362, 281, 733, 295, 16635, 309, 666, 4994], "temperature": 0.0, "avg_logprob": -0.10278548203505479, "compression_ratio": 1.5594059405940595, "no_speech_prob": 2.642571644173586e-06}, {"id": 969, "seek": 464600, "start": 4658.32, "end": 4665.08, "text": " bits and then pull over them and whatever, we can do bidire very easily in this case,", "tokens": [9239, 293, 550, 2235, 670, 552, 293, 2035, 11, 321, 393, 360, 12957, 621, 588, 3612, 294, 341, 1389, 11], "temperature": 0.0, "avg_logprob": -0.10278548203505479, "compression_ratio": 1.5594059405940595, "no_speech_prob": 2.642571644173586e-06}, {"id": 970, "seek": 464600, "start": 4665.08, "end": 4674.0, "text": " which is literally as simple as adding bidirectional equals true to our encoder.", "tokens": [597, 307, 3736, 382, 2199, 382, 5127, 12957, 621, 41048, 6915, 2074, 281, 527, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.10278548203505479, "compression_ratio": 1.5594059405940595, "no_speech_prob": 2.642571644173586e-06}, {"id": 971, "seek": 467400, "start": 4674.0, "end": 4679.96, "text": " People tend not to do bidirectional for the decoder, I think partly because it's kind", "tokens": [3432, 3928, 406, 281, 360, 12957, 621, 41048, 337, 264, 979, 19866, 11, 286, 519, 17031, 570, 309, 311, 733], "temperature": 0.0, "avg_logprob": -0.12842795428107767, "compression_ratio": 1.5571428571428572, "no_speech_prob": 1.5936446288833395e-05}, {"id": 972, "seek": 467400, "start": 4679.96, "end": 4687.92, "text": " of considered cheating, but I don't know, I was just talking to somebody at the break", "tokens": [295, 4888, 18309, 11, 457, 286, 500, 380, 458, 11, 286, 390, 445, 1417, 281, 2618, 412, 264, 1821], "temperature": 0.0, "avg_logprob": -0.12842795428107767, "compression_ratio": 1.5571428571428572, "no_speech_prob": 1.5936446288833395e-05}, {"id": 973, "seek": 467400, "start": 4687.92, "end": 4695.88, "text": " about it, maybe it can work in some situations, although it might need to be more of an ensembling", "tokens": [466, 309, 11, 1310, 309, 393, 589, 294, 512, 6851, 11, 4878, 309, 1062, 643, 281, 312, 544, 295, 364, 12567, 2504, 1688], "temperature": 0.0, "avg_logprob": -0.12842795428107767, "compression_ratio": 1.5571428571428572, "no_speech_prob": 1.5936446288833395e-05}, {"id": 974, "seek": 467400, "start": 4695.88, "end": 4700.12, "text": " approach in the decoder because it's a bit less obvious.", "tokens": [3109, 294, 264, 979, 19866, 570, 309, 311, 257, 857, 1570, 6322, 13], "temperature": 0.0, "avg_logprob": -0.12842795428107767, "compression_ratio": 1.5571428571428572, "no_speech_prob": 1.5936446288833395e-05}, {"id": 975, "seek": 470012, "start": 4700.12, "end": 4709.36, "text": " The encoder is very, very simple, bidirectional equals true, and we now have, with bidirectional", "tokens": [440, 2058, 19866, 307, 588, 11, 588, 2199, 11, 12957, 621, 41048, 6915, 2074, 11, 293, 321, 586, 362, 11, 365, 12957, 621, 41048], "temperature": 0.0, "avg_logprob": -0.1444901605931724, "compression_ratio": 1.7049180327868851, "no_speech_prob": 5.0147164074587636e-06}, {"id": 976, "seek": 470012, "start": 4709.36, "end": 4716.16, "text": " equals true, rather than just having an RNN which is going this direction, we have a second", "tokens": [6915, 2074, 11, 2831, 813, 445, 1419, 364, 45702, 45, 597, 307, 516, 341, 3513, 11, 321, 362, 257, 1150], "temperature": 0.0, "avg_logprob": -0.1444901605931724, "compression_ratio": 1.7049180327868851, "no_speech_prob": 5.0147164074587636e-06}, {"id": 977, "seek": 470012, "start": 4716.16, "end": 4721.24, "text": " RNN that's going in this direction.", "tokens": [45702, 45, 300, 311, 516, 294, 341, 3513, 13], "temperature": 0.0, "avg_logprob": -0.1444901605931724, "compression_ratio": 1.7049180327868851, "no_speech_prob": 5.0147164074587636e-06}, {"id": 978, "seek": 470012, "start": 4721.24, "end": 4730.08, "text": " And so that second RNN literally is visiting them, each token in the opposing order, so", "tokens": [400, 370, 300, 1150, 45702, 45, 3736, 307, 11700, 552, 11, 1184, 14862, 294, 264, 27890, 1668, 11, 370], "temperature": 0.0, "avg_logprob": -0.1444901605931724, "compression_ratio": 1.7049180327868851, "no_speech_prob": 5.0147164074587636e-06}, {"id": 979, "seek": 473008, "start": 4730.08, "end": 4736.8, "text": " when we get the final hidden state, it's here, rather than here.", "tokens": [562, 321, 483, 264, 2572, 7633, 1785, 11, 309, 311, 510, 11, 2831, 813, 510, 13], "temperature": 0.0, "avg_logprob": -0.15975166987446907, "compression_ratio": 1.6652173913043478, "no_speech_prob": 1.4970943084335886e-05}, {"id": 980, "seek": 473008, "start": 4736.8, "end": 4742.16, "text": " But the hidden state is of the same size, so the final result is that we end up with", "tokens": [583, 264, 7633, 1785, 307, 295, 264, 912, 2744, 11, 370, 264, 2572, 1874, 307, 300, 321, 917, 493, 365], "temperature": 0.0, "avg_logprob": -0.15975166987446907, "compression_ratio": 1.6652173913043478, "no_speech_prob": 1.4970943084335886e-05}, {"id": 981, "seek": 473008, "start": 4742.16, "end": 4747.04, "text": " a tensor that's got an extra 2 long axis.", "tokens": [257, 40863, 300, 311, 658, 364, 2857, 568, 938, 10298, 13], "temperature": 0.0, "avg_logprob": -0.15975166987446907, "compression_ratio": 1.6652173913043478, "no_speech_prob": 1.4970943084335886e-05}, {"id": 982, "seek": 473008, "start": 4747.04, "end": 4751.44, "text": " And depending on what library you use, often that will be then combined with the number", "tokens": [400, 5413, 322, 437, 6405, 291, 764, 11, 2049, 300, 486, 312, 550, 9354, 365, 264, 1230], "temperature": 0.0, "avg_logprob": -0.15975166987446907, "compression_ratio": 1.6652173913043478, "no_speech_prob": 1.4970943084335886e-05}, {"id": 983, "seek": 473008, "start": 4751.44, "end": 4757.04, "text": " of layers thing, so if you've got 2 layers and bidirectional, that tensor dimension is", "tokens": [295, 7914, 551, 11, 370, 498, 291, 600, 658, 568, 7914, 293, 12957, 621, 41048, 11, 300, 40863, 10139, 307], "temperature": 0.0, "avg_logprob": -0.15975166987446907, "compression_ratio": 1.6652173913043478, "no_speech_prob": 1.4970943084335886e-05}, {"id": 984, "seek": 473008, "start": 4757.04, "end": 4760.04, "text": " now of length 4.", "tokens": [586, 295, 4641, 1017, 13], "temperature": 0.0, "avg_logprob": -0.15975166987446907, "compression_ratio": 1.6652173913043478, "no_speech_prob": 1.4970943084335886e-05}, {"id": 985, "seek": 476004, "start": 4760.04, "end": 4765.2, "text": " The pytorch, it kind of depends which bit of the process you're looking at as to whether", "tokens": [440, 25878, 284, 339, 11, 309, 733, 295, 5946, 597, 857, 295, 264, 1399, 291, 434, 1237, 412, 382, 281, 1968], "temperature": 0.0, "avg_logprob": -0.1808140644660363, "compression_ratio": 1.6414342629482073, "no_speech_prob": 5.25536324857967e-06}, {"id": 986, "seek": 476004, "start": 4765.2, "end": 4769.4, "text": " you get a separate result for each layer and bidirectional bit and so forth.", "tokens": [291, 483, 257, 4994, 1874, 337, 1184, 4583, 293, 12957, 621, 41048, 857, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1808140644660363, "compression_ratio": 1.6414342629482073, "no_speech_prob": 5.25536324857967e-06}, {"id": 987, "seek": 476004, "start": 4769.4, "end": 4775.08, "text": " You have to look up the docs and it will tell you inputs, outputs, tensor sizes, appropriate", "tokens": [509, 362, 281, 574, 493, 264, 45623, 293, 309, 486, 980, 291, 15743, 11, 23930, 11, 40863, 11602, 11, 6854], "temperature": 0.0, "avg_logprob": -0.1808140644660363, "compression_ratio": 1.6414342629482073, "no_speech_prob": 5.25536324857967e-06}, {"id": 988, "seek": 476004, "start": 4775.08, "end": 4778.96, "text": " for the number of layers, and whether you have bidirectional equals true.", "tokens": [337, 264, 1230, 295, 7914, 11, 293, 1968, 291, 362, 12957, 621, 41048, 6915, 2074, 13], "temperature": 0.0, "avg_logprob": -0.1808140644660363, "compression_ratio": 1.6414342629482073, "no_speech_prob": 5.25536324857967e-06}, {"id": 989, "seek": 476004, "start": 4778.96, "end": 4785.08, "text": " In this particular case, you'll basically see all the changes I've had to make.", "tokens": [682, 341, 1729, 1389, 11, 291, 603, 1936, 536, 439, 264, 2962, 286, 600, 632, 281, 652, 13], "temperature": 0.0, "avg_logprob": -0.1808140644660363, "compression_ratio": 1.6414342629482073, "no_speech_prob": 5.25536324857967e-06}, {"id": 990, "seek": 478508, "start": 4785.08, "end": 4790.68, "text": " So for example, you'll see when I added bidirectional equals true, my linear layer now needs number", "tokens": [407, 337, 1365, 11, 291, 603, 536, 562, 286, 3869, 12957, 621, 41048, 6915, 2074, 11, 452, 8213, 4583, 586, 2203, 1230], "temperature": 0.0, "avg_logprob": -0.16055635530121473, "compression_ratio": 1.640552995391705, "no_speech_prob": 7.527950401708949e-06}, {"id": 991, "seek": 478508, "start": 4790.68, "end": 4796.5199999999995, "text": " of hidden times 2 to reflect the fact that we have that second direction in our hidden", "tokens": [295, 7633, 1413, 568, 281, 5031, 264, 1186, 300, 321, 362, 300, 1150, 3513, 294, 527, 7633], "temperature": 0.0, "avg_logprob": -0.16055635530121473, "compression_ratio": 1.640552995391705, "no_speech_prob": 7.527950401708949e-06}, {"id": 992, "seek": 478508, "start": 4796.5199999999995, "end": 4798.64, "text": " state now.", "tokens": [1785, 586, 13], "temperature": 0.0, "avg_logprob": -0.16055635530121473, "compression_ratio": 1.640552995391705, "no_speech_prob": 7.527950401708949e-06}, {"id": 993, "seek": 478508, "start": 4798.64, "end": 4805.04, "text": " You'll see in it hidden, it's now self.numberOfLayers times 2 here.", "tokens": [509, 603, 536, 294, 309, 7633, 11, 309, 311, 586, 2698, 13, 41261, 23919, 43, 320, 433, 1413, 568, 510, 13], "temperature": 0.0, "avg_logprob": -0.16055635530121473, "compression_ratio": 1.640552995391705, "no_speech_prob": 7.527950401708949e-06}, {"id": 994, "seek": 478508, "start": 4805.04, "end": 4811.28, "text": " So you'll see there's a few places where there's been an extra 2 that has to be thrown in.", "tokens": [407, 291, 603, 536, 456, 311, 257, 1326, 3190, 689, 456, 311, 668, 364, 2857, 568, 300, 575, 281, 312, 11732, 294, 13], "temperature": 0.0, "avg_logprob": -0.16055635530121473, "compression_ratio": 1.640552995391705, "no_speech_prob": 7.527950401708949e-06}, {"id": 995, "seek": 481128, "start": 4811.28, "end": 4819.48, "text": " Why making a decoder bidirectional is considered cheating?", "tokens": [1545, 1455, 257, 979, 19866, 12957, 621, 41048, 307, 4888, 18309, 30], "temperature": 0.0, "avg_logprob": -0.261430440125642, "compression_ratio": 1.3703703703703705, "no_speech_prob": 1.300702933804132e-05}, {"id": 996, "seek": 481128, "start": 4819.48, "end": 4828.8, "text": " It's not just that it's cheating, it's like we have this loop going on, you know.", "tokens": [467, 311, 406, 445, 300, 309, 311, 18309, 11, 309, 311, 411, 321, 362, 341, 6367, 516, 322, 11, 291, 458, 13], "temperature": 0.0, "avg_logprob": -0.261430440125642, "compression_ratio": 1.3703703703703705, "no_speech_prob": 1.300702933804132e-05}, {"id": 997, "seek": 481128, "start": 4828.8, "end": 4834.44, "text": " It's not as simple as just having 2 tensors.", "tokens": [467, 311, 406, 382, 2199, 382, 445, 1419, 568, 10688, 830, 13], "temperature": 0.0, "avg_logprob": -0.261430440125642, "compression_ratio": 1.3703703703703705, "no_speech_prob": 1.300702933804132e-05}, {"id": 998, "seek": 483444, "start": 4834.44, "end": 4841.96, "text": " And then how do you turn those 2 separate loops into a final result?", "tokens": [400, 550, 577, 360, 291, 1261, 729, 568, 4994, 16121, 666, 257, 2572, 1874, 30], "temperature": 0.0, "avg_logprob": -0.16950148344039917, "compression_ratio": 1.5807860262008733, "no_speech_prob": 8.013442311494146e-06}, {"id": 999, "seek": 483444, "start": 4841.96, "end": 4847.4, "text": " After talking about it during the break, I've kind of gone from like, hey everybody knows", "tokens": [2381, 1417, 466, 309, 1830, 264, 1821, 11, 286, 600, 733, 295, 2780, 490, 411, 11, 4177, 2201, 3255], "temperature": 0.0, "avg_logprob": -0.16950148344039917, "compression_ratio": 1.5807860262008733, "no_speech_prob": 8.013442311494146e-06}, {"id": 1000, "seek": 483444, "start": 4847.4, "end": 4853.08, "text": " it doesn't work, to like, oh maybe it kind of could work, but it requires more thought.", "tokens": [309, 1177, 380, 589, 11, 281, 411, 11, 1954, 1310, 309, 733, 295, 727, 589, 11, 457, 309, 7029, 544, 1194, 13], "temperature": 0.0, "avg_logprob": -0.16950148344039917, "compression_ratio": 1.5807860262008733, "no_speech_prob": 8.013442311494146e-06}, {"id": 1001, "seek": 483444, "start": 4853.08, "end": 4857.96, "text": " It's quite possible during the week I'll realize it's a dumb idea and I was being stupid, but", "tokens": [467, 311, 1596, 1944, 1830, 264, 1243, 286, 603, 4325, 309, 311, 257, 10316, 1558, 293, 286, 390, 885, 6631, 11, 457], "temperature": 0.0, "avg_logprob": -0.16950148344039917, "compression_ratio": 1.5807860262008733, "no_speech_prob": 8.013442311494146e-06}, {"id": 1002, "seek": 483444, "start": 4857.96, "end": 4858.96, "text": " we'll think about it.", "tokens": [321, 603, 519, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.16950148344039917, "compression_ratio": 1.5807860262008733, "no_speech_prob": 8.013442311494146e-06}, {"id": 1003, "seek": 485896, "start": 4858.96, "end": 4866.12, "text": " Another question people had, why do you need to have an end to that loop?", "tokens": [3996, 1168, 561, 632, 11, 983, 360, 291, 643, 281, 362, 364, 917, 281, 300, 6367, 30], "temperature": 0.0, "avg_logprob": -0.29145157721734816, "compression_ratio": 1.574468085106383, "no_speech_prob": 8.939588042267133e-06}, {"id": 1004, "seek": 485896, "start": 4866.12, "end": 4869.28, "text": " Why do you need to have an end to that loop?", "tokens": [1545, 360, 291, 643, 281, 362, 364, 917, 281, 300, 6367, 30], "temperature": 0.0, "avg_logprob": -0.29145157721734816, "compression_ratio": 1.574468085106383, "no_speech_prob": 8.939588042267133e-06}, {"id": 1005, "seek": 485896, "start": 4869.28, "end": 4876.36, "text": " You have like a range.", "tokens": [509, 362, 411, 257, 3613, 13], "temperature": 0.0, "avg_logprob": -0.29145157721734816, "compression_ratio": 1.574468085106383, "no_speech_prob": 8.939588042267133e-06}, {"id": 1006, "seek": 485896, "start": 4876.36, "end": 4882.64, "text": " When I start training, everything's random, so this will probably never be true.", "tokens": [1133, 286, 722, 3097, 11, 1203, 311, 4974, 11, 370, 341, 486, 1391, 1128, 312, 2074, 13], "temperature": 0.0, "avg_logprob": -0.29145157721734816, "compression_ratio": 1.574468085106383, "no_speech_prob": 8.939588042267133e-06}, {"id": 1007, "seek": 488264, "start": 4882.64, "end": 4890.160000000001, "text": " So later on, it will pretty much always break out eventually, but it's basically like we're", "tokens": [407, 1780, 322, 11, 309, 486, 1238, 709, 1009, 1821, 484, 4728, 11, 457, 309, 311, 1936, 411, 321, 434], "temperature": 0.0, "avg_logprob": -0.18455032155483583, "compression_ratio": 1.5576923076923077, "no_speech_prob": 5.682403752871323e-06}, {"id": 1008, "seek": 488264, "start": 4890.160000000001, "end": 4893.68, "text": " going to go forever.", "tokens": [516, 281, 352, 5680, 13], "temperature": 0.0, "avg_logprob": -0.18455032155483583, "compression_ratio": 1.5576923076923077, "no_speech_prob": 5.682403752871323e-06}, {"id": 1009, "seek": 488264, "start": 4893.68, "end": 4898.240000000001, "text": " It's really important to remember when you're designing an architecture that when you start,", "tokens": [467, 311, 534, 1021, 281, 1604, 562, 291, 434, 14685, 364, 9482, 300, 562, 291, 722, 11], "temperature": 0.0, "avg_logprob": -0.18455032155483583, "compression_ratio": 1.5576923076923077, "no_speech_prob": 5.682403752871323e-06}, {"id": 1010, "seek": 488264, "start": 4898.240000000001, "end": 4901.12, "text": " the model knows nothing about anything.", "tokens": [264, 2316, 3255, 1825, 466, 1340, 13], "temperature": 0.0, "avg_logprob": -0.18455032155483583, "compression_ratio": 1.5576923076923077, "no_speech_prob": 5.682403752871323e-06}, {"id": 1011, "seek": 488264, "start": 4901.12, "end": 4906.4800000000005, "text": " So you kind of want to make sure it's doing something that's vaguely sensible.", "tokens": [407, 291, 733, 295, 528, 281, 652, 988, 309, 311, 884, 746, 300, 311, 13501, 48863, 25380, 13], "temperature": 0.0, "avg_logprob": -0.18455032155483583, "compression_ratio": 1.5576923076923077, "no_speech_prob": 5.682403752871323e-06}, {"id": 1012, "seek": 490648, "start": 4906.48, "end": 4916.799999999999, "text": " So bidirectional means we had, let's see how we go here, we got out to 358 cross entropy", "tokens": [407, 12957, 621, 41048, 1355, 321, 632, 11, 718, 311, 536, 577, 321, 352, 510, 11, 321, 658, 484, 281, 6976, 23, 3278, 30867], "temperature": 0.0, "avg_logprob": -0.18987281601150313, "compression_ratio": 1.5058823529411764, "no_speech_prob": 1.644238000153564e-05}, {"id": 1013, "seek": 490648, "start": 4916.799999999999, "end": 4923.0, "text": " loss with a single direction, with bidirectional it's down to 351.", "tokens": [4470, 365, 257, 2167, 3513, 11, 365, 12957, 621, 41048, 309, 311, 760, 281, 6976, 16, 13], "temperature": 0.0, "avg_logprob": -0.18987281601150313, "compression_ratio": 1.5058823529411764, "no_speech_prob": 1.644238000153564e-05}, {"id": 1014, "seek": 490648, "start": 4923.0, "end": 4925.04, "text": " So that improved it a bit, that's good.", "tokens": [407, 300, 9689, 309, 257, 857, 11, 300, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.18987281601150313, "compression_ratio": 1.5058823529411764, "no_speech_prob": 1.644238000153564e-05}, {"id": 1015, "seek": 490648, "start": 4925.04, "end": 4932.959999999999, "text": " And as I say, it shouldn't really slow things down too much.", "tokens": [400, 382, 286, 584, 11, 309, 4659, 380, 534, 2964, 721, 760, 886, 709, 13], "temperature": 0.0, "avg_logprob": -0.18987281601150313, "compression_ratio": 1.5058823529411764, "no_speech_prob": 1.644238000153564e-05}, {"id": 1016, "seek": 493296, "start": 4932.96, "end": 4938.4, "text": " So it does mean there's a little bit more sequential processing have to happen, but", "tokens": [407, 309, 775, 914, 456, 311, 257, 707, 857, 544, 42881, 9007, 362, 281, 1051, 11, 457], "temperature": 0.0, "avg_logprob": -0.23766734383322977, "compression_ratio": 1.6022727272727273, "no_speech_prob": 3.966964868595824e-06}, {"id": 1017, "seek": 493296, "start": 4938.4, "end": 4940.56, "text": " it's generally a good win.", "tokens": [309, 311, 5101, 257, 665, 1942, 13], "temperature": 0.0, "avg_logprob": -0.23766734383322977, "compression_ratio": 1.6022727272727273, "no_speech_prob": 3.966964868595824e-06}, {"id": 1018, "seek": 493296, "start": 4940.56, "end": 4947.44, "text": " In the Google translation model of the 8 layers, only the first layer is bidirectional because", "tokens": [682, 264, 3329, 12853, 2316, 295, 264, 1649, 7914, 11, 787, 264, 700, 4583, 307, 12957, 621, 41048, 570], "temperature": 0.0, "avg_logprob": -0.23766734383322977, "compression_ratio": 1.6022727272727273, "no_speech_prob": 3.966964868595824e-06}, {"id": 1019, "seek": 493296, "start": 4947.44, "end": 4949.92, "text": " it allows it to do more in parallel.", "tokens": [309, 4045, 309, 281, 360, 544, 294, 8952, 13], "temperature": 0.0, "avg_logprob": -0.23766734383322977, "compression_ratio": 1.6022727272727273, "no_speech_prob": 3.966964868595824e-06}, {"id": 1020, "seek": 493296, "start": 4949.92, "end": 4954.36, "text": " So if you create really deep models, you may need to think about which one is bidirectional,", "tokens": [407, 498, 291, 1884, 534, 2452, 5245, 11, 291, 815, 643, 281, 519, 466, 597, 472, 307, 12957, 621, 41048, 11], "temperature": 0.0, "avg_logprob": -0.23766734383322977, "compression_ratio": 1.6022727272727273, "no_speech_prob": 3.966964868595824e-06}, {"id": 1021, "seek": 493296, "start": 4954.36, "end": 4956.4, "text": " otherwise you'll have performance issues.", "tokens": [5911, 291, 603, 362, 3389, 2663, 13], "temperature": 0.0, "avg_logprob": -0.23766734383322977, "compression_ratio": 1.6022727272727273, "no_speech_prob": 3.966964868595824e-06}, {"id": 1022, "seek": 493296, "start": 4956.4, "end": 4959.4800000000005, "text": " So 351.", "tokens": [407, 6976, 16, 13], "temperature": 0.0, "avg_logprob": -0.23766734383322977, "compression_ratio": 1.6022727272727273, "no_speech_prob": 3.966964868595824e-06}, {"id": 1023, "seek": 493296, "start": 4959.4800000000005, "end": 4962.76, "text": " Now let's talk about teacher forcing.", "tokens": [823, 718, 311, 751, 466, 5027, 19030, 13], "temperature": 0.0, "avg_logprob": -0.23766734383322977, "compression_ratio": 1.6022727272727273, "no_speech_prob": 3.966964868595824e-06}, {"id": 1024, "seek": 496276, "start": 4962.76, "end": 4972.320000000001, "text": " So teacher forcing is going to come back to this idea that when the model starts learning,", "tokens": [407, 5027, 19030, 307, 516, 281, 808, 646, 281, 341, 1558, 300, 562, 264, 2316, 3719, 2539, 11], "temperature": 0.0, "avg_logprob": -0.15202750799790868, "compression_ratio": 1.8771929824561404, "no_speech_prob": 6.747980478394311e-06}, {"id": 1025, "seek": 496276, "start": 4972.320000000001, "end": 4974.4800000000005, "text": " it knows nothing about nothing.", "tokens": [309, 3255, 1825, 466, 1825, 13], "temperature": 0.0, "avg_logprob": -0.15202750799790868, "compression_ratio": 1.8771929824561404, "no_speech_prob": 6.747980478394311e-06}, {"id": 1026, "seek": 496276, "start": 4974.4800000000005, "end": 4980.34, "text": " So when the model starts learning, it is not going to spit out uh at this point, it's going", "tokens": [407, 562, 264, 2316, 3719, 2539, 11, 309, 307, 406, 516, 281, 22127, 484, 2232, 412, 341, 935, 11, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.15202750799790868, "compression_ratio": 1.8771929824561404, "no_speech_prob": 6.747980478394311e-06}, {"id": 1027, "seek": 496276, "start": 4980.34, "end": 4983.6, "text": " to spit out some random meaningless word.", "tokens": [281, 22127, 484, 512, 4974, 33232, 1349, 13], "temperature": 0.0, "avg_logprob": -0.15202750799790868, "compression_ratio": 1.8771929824561404, "no_speech_prob": 6.747980478394311e-06}, {"id": 1028, "seek": 496276, "start": 4983.6, "end": 4988.16, "text": " It doesn't know anything about German or about English or about the idea of language or anything.", "tokens": [467, 1177, 380, 458, 1340, 466, 6521, 420, 466, 3669, 420, 466, 264, 1558, 295, 2856, 420, 1340, 13], "temperature": 0.0, "avg_logprob": -0.15202750799790868, "compression_ratio": 1.8771929824561404, "no_speech_prob": 6.747980478394311e-06}, {"id": 1029, "seek": 496276, "start": 4988.16, "end": 4992.320000000001, "text": " And it's going to feed it down here as an input and be totally unhelpful.", "tokens": [400, 309, 311, 516, 281, 3154, 309, 760, 510, 382, 364, 4846, 293, 312, 3879, 517, 37451, 906, 13], "temperature": 0.0, "avg_logprob": -0.15202750799790868, "compression_ratio": 1.8771929824561404, "no_speech_prob": 6.747980478394311e-06}, {"id": 1030, "seek": 499232, "start": 4992.32, "end": 4996.98, "text": " And so that means that early learning is going to be very very difficult because it's feeding", "tokens": [400, 370, 300, 1355, 300, 2440, 2539, 307, 516, 281, 312, 588, 588, 2252, 570, 309, 311, 12919], "temperature": 0.0, "avg_logprob": -0.1689379969729653, "compression_ratio": 1.627027027027027, "no_speech_prob": 1.1659458323265426e-05}, {"id": 1031, "seek": 499232, "start": 4996.98, "end": 5002.5199999999995, "text": " in an input that's stupid into a model that knows nothing and somehow it's going to get", "tokens": [294, 364, 4846, 300, 311, 6631, 666, 257, 2316, 300, 3255, 1825, 293, 6063, 309, 311, 516, 281, 483], "temperature": 0.0, "avg_logprob": -0.1689379969729653, "compression_ratio": 1.627027027027027, "no_speech_prob": 1.1659458323265426e-05}, {"id": 1032, "seek": 499232, "start": 5002.5199999999995, "end": 5003.5199999999995, "text": " better.", "tokens": [1101, 13], "temperature": 0.0, "avg_logprob": -0.1689379969729653, "compression_ratio": 1.627027027027027, "no_speech_prob": 1.1659458323265426e-05}, {"id": 1033, "seek": 499232, "start": 5003.5199999999995, "end": 5008.54, "text": " So that's, it's not asking too much, eventually it gets there, but it's definitely not as", "tokens": [407, 300, 311, 11, 309, 311, 406, 3365, 886, 709, 11, 4728, 309, 2170, 456, 11, 457, 309, 311, 2138, 406, 382], "temperature": 0.0, "avg_logprob": -0.1689379969729653, "compression_ratio": 1.627027027027027, "no_speech_prob": 1.1659458323265426e-05}, {"id": 1034, "seek": 499232, "start": 5008.54, "end": 5010.639999999999, "text": " helpful as we can be.", "tokens": [4961, 382, 321, 393, 312, 13], "temperature": 0.0, "avg_logprob": -0.1689379969729653, "compression_ratio": 1.627027027027027, "no_speech_prob": 1.1659458323265426e-05}, {"id": 1035, "seek": 501064, "start": 5010.64, "end": 5024.88, "text": " So what if instead of feeding in the thing I predicted just now, what if instead we feed", "tokens": [407, 437, 498, 2602, 295, 12919, 294, 264, 551, 286, 19147, 445, 586, 11, 437, 498, 2602, 321, 3154], "temperature": 0.0, "avg_logprob": -0.17686214166529038, "compression_ratio": 1.5783132530120483, "no_speech_prob": 3.3405208341719117e-06}, {"id": 1036, "seek": 501064, "start": 5024.88, "end": 5029.92, "text": " in the actual correct word it was meant to be.", "tokens": [294, 264, 3539, 3006, 1349, 309, 390, 4140, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.17686214166529038, "compression_ratio": 1.5783132530120483, "no_speech_prob": 3.3405208341719117e-06}, {"id": 1037, "seek": 501064, "start": 5029.92, "end": 5034.72, "text": " Now we can't do that at inference time because by definition we don't know the correct word,", "tokens": [823, 321, 393, 380, 360, 300, 412, 38253, 565, 570, 538, 7123, 321, 500, 380, 458, 264, 3006, 1349, 11], "temperature": 0.0, "avg_logprob": -0.17686214166529038, "compression_ratio": 1.5783132530120483, "no_speech_prob": 3.3405208341719117e-06}, {"id": 1038, "seek": 501064, "start": 5034.72, "end": 5037.240000000001, "text": " we've been asked to translate it.", "tokens": [321, 600, 668, 2351, 281, 13799, 309, 13], "temperature": 0.0, "avg_logprob": -0.17686214166529038, "compression_ratio": 1.5783132530120483, "no_speech_prob": 3.3405208341719117e-06}, {"id": 1039, "seek": 503724, "start": 5037.24, "end": 5041.78, "text": " We can't require a correct translation in order to do translation.", "tokens": [492, 393, 380, 3651, 257, 3006, 12853, 294, 1668, 281, 360, 12853, 13], "temperature": 0.0, "avg_logprob": -0.13538649587920218, "compression_ratio": 1.6566523605150214, "no_speech_prob": 7.766890121274628e-06}, {"id": 1040, "seek": 503724, "start": 5041.78, "end": 5048.2, "text": " So the way I've set this up is I've got this thing called PR force, which is probability", "tokens": [407, 264, 636, 286, 600, 992, 341, 493, 307, 286, 600, 658, 341, 551, 1219, 11568, 3464, 11, 597, 307, 8482], "temperature": 0.0, "avg_logprob": -0.13538649587920218, "compression_ratio": 1.6566523605150214, "no_speech_prob": 7.766890121274628e-06}, {"id": 1041, "seek": 503724, "start": 5048.2, "end": 5049.54, "text": " of forcing.", "tokens": [295, 19030, 13], "temperature": 0.0, "avg_logprob": -0.13538649587920218, "compression_ratio": 1.6566523605150214, "no_speech_prob": 7.766890121274628e-06}, {"id": 1042, "seek": 503724, "start": 5049.54, "end": 5055.08, "text": " And if some random number is less than that probability, then I'm going to replace my", "tokens": [400, 498, 512, 4974, 1230, 307, 1570, 813, 300, 8482, 11, 550, 286, 478, 516, 281, 7406, 452], "temperature": 0.0, "avg_logprob": -0.13538649587920218, "compression_ratio": 1.6566523605150214, "no_speech_prob": 7.766890121274628e-06}, {"id": 1043, "seek": 503724, "start": 5055.08, "end": 5060.099999999999, "text": " decoder input with the actual correct thing.", "tokens": [979, 19866, 4846, 365, 264, 3539, 3006, 551, 13], "temperature": 0.0, "avg_logprob": -0.13538649587920218, "compression_ratio": 1.6566523605150214, "no_speech_prob": 7.766890121274628e-06}, {"id": 1044, "seek": 503724, "start": 5060.099999999999, "end": 5064.5199999999995, "text": " And if we've already gone too far, if it's already longer than the target sentence, I'm", "tokens": [400, 498, 321, 600, 1217, 2780, 886, 1400, 11, 498, 309, 311, 1217, 2854, 813, 264, 3779, 8174, 11, 286, 478], "temperature": 0.0, "avg_logprob": -0.13538649587920218, "compression_ratio": 1.6566523605150214, "no_speech_prob": 7.766890121274628e-06}, {"id": 1045, "seek": 506452, "start": 5064.52, "end": 5068.52, "text": " just going to stop because I can't give it the correct thing.", "tokens": [445, 516, 281, 1590, 570, 286, 393, 380, 976, 309, 264, 3006, 551, 13], "temperature": 0.0, "avg_logprob": -0.1651768774356482, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.9333524505782407e-06}, {"id": 1046, "seek": 506452, "start": 5068.52, "end": 5073.92, "text": " So you can see how beautiful PyTorch is for this, because if you try to do this with some", "tokens": [407, 291, 393, 536, 577, 2238, 9953, 51, 284, 339, 307, 337, 341, 11, 570, 498, 291, 853, 281, 360, 341, 365, 512], "temperature": 0.0, "avg_logprob": -0.1651768774356482, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.9333524505782407e-06}, {"id": 1047, "seek": 506452, "start": 5073.92, "end": 5080.64, "text": " static graph thing like classic TensorFlow, well I try.", "tokens": [13437, 4295, 551, 411, 7230, 37624, 11, 731, 286, 853, 13], "temperature": 0.0, "avg_logprob": -0.1651768774356482, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.9333524505782407e-06}, {"id": 1048, "seek": 506452, "start": 5080.64, "end": 5085.080000000001, "text": " One of the key reasons that we switched to PyTorch at this exact point in last year's", "tokens": [1485, 295, 264, 2141, 4112, 300, 321, 16858, 281, 9953, 51, 284, 339, 412, 341, 1900, 935, 294, 1036, 1064, 311], "temperature": 0.0, "avg_logprob": -0.1651768774356482, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.9333524505782407e-06}, {"id": 1049, "seek": 506452, "start": 5085.080000000001, "end": 5089.72, "text": " class was because Jeremy tried to implement teacher forcing in Keras and TensorFlow and", "tokens": [1508, 390, 570, 17809, 3031, 281, 4445, 5027, 19030, 294, 591, 6985, 293, 37624, 293], "temperature": 0.0, "avg_logprob": -0.1651768774356482, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.9333524505782407e-06}, {"id": 1050, "seek": 506452, "start": 5089.72, "end": 5094.080000000001, "text": " went even more insane than he started.", "tokens": [1437, 754, 544, 10838, 813, 415, 1409, 13], "temperature": 0.0, "avg_logprob": -0.1651768774356482, "compression_ratio": 1.6342412451361867, "no_speech_prob": 1.9333524505782407e-06}, {"id": 1051, "seek": 509408, "start": 5094.08, "end": 5097.32, "text": " It was weeks of getting nowhere.", "tokens": [467, 390, 3259, 295, 1242, 11159, 13], "temperature": 0.0, "avg_logprob": -0.2075465066092355, "compression_ratio": 1.610655737704918, "no_speech_prob": 2.468266939104069e-05}, {"id": 1052, "seek": 509408, "start": 5097.32, "end": 5105.68, "text": " And then I literally on Twitter, I think it was on Dracapathy, I announced, said something", "tokens": [400, 550, 286, 3736, 322, 5794, 11, 286, 519, 309, 390, 322, 2491, 326, 569, 9527, 11, 286, 7548, 11, 848, 746], "temperature": 0.0, "avg_logprob": -0.2075465066092355, "compression_ratio": 1.610655737704918, "no_speech_prob": 2.468266939104069e-05}, {"id": 1053, "seek": 509408, "start": 5105.68, "end": 5109.74, "text": " about oh there's this thing called PyTorch that just came out and it's really cool.", "tokens": [466, 1954, 456, 311, 341, 551, 1219, 9953, 51, 284, 339, 300, 445, 1361, 484, 293, 309, 311, 534, 1627, 13], "temperature": 0.0, "avg_logprob": -0.2075465066092355, "compression_ratio": 1.610655737704918, "no_speech_prob": 2.468266939104069e-05}, {"id": 1054, "seek": 509408, "start": 5109.74, "end": 5112.04, "text": " And I tried it that day.", "tokens": [400, 286, 3031, 309, 300, 786, 13], "temperature": 0.0, "avg_logprob": -0.2075465066092355, "compression_ratio": 1.610655737704918, "no_speech_prob": 2.468266939104069e-05}, {"id": 1055, "seek": 509408, "start": 5112.04, "end": 5114.96, "text": " By the next day I had teacher forcing.", "tokens": [3146, 264, 958, 786, 286, 632, 5027, 19030, 13], "temperature": 0.0, "avg_logprob": -0.2075465066092355, "compression_ratio": 1.610655737704918, "no_speech_prob": 2.468266939104069e-05}, {"id": 1056, "seek": 509408, "start": 5114.96, "end": 5117.5199999999995, "text": " And so I was like, oh my gosh.", "tokens": [400, 370, 286, 390, 411, 11, 1954, 452, 6502, 13], "temperature": 0.0, "avg_logprob": -0.2075465066092355, "compression_ratio": 1.610655737704918, "no_speech_prob": 2.468266939104069e-05}, {"id": 1057, "seek": 509408, "start": 5117.5199999999995, "end": 5122.08, "text": " And all the stuff of trying to debug things, it was suddenly so much easier, and this kind", "tokens": [400, 439, 264, 1507, 295, 1382, 281, 24083, 721, 11, 309, 390, 5800, 370, 709, 3571, 11, 293, 341, 733], "temperature": 0.0, "avg_logprob": -0.2075465066092355, "compression_ratio": 1.610655737704918, "no_speech_prob": 2.468266939104069e-05}, {"id": 1058, "seek": 512208, "start": 5122.08, "end": 5124.5199999999995, "text": " of dynamic stuff is so much easier.", "tokens": [295, 8546, 1507, 307, 370, 709, 3571, 13], "temperature": 0.0, "avg_logprob": -0.20958985478044992, "compression_ratio": 1.5121951219512195, "no_speech_prob": 5.422195954452036e-06}, {"id": 1059, "seek": 512208, "start": 5124.5199999999995, "end": 5129.16, "text": " So this is a great example of like, hey I get to use random numbers and if statements", "tokens": [407, 341, 307, 257, 869, 1365, 295, 411, 11, 4177, 286, 483, 281, 764, 4974, 3547, 293, 498, 12363], "temperature": 0.0, "avg_logprob": -0.20958985478044992, "compression_ratio": 1.5121951219512195, "no_speech_prob": 5.422195954452036e-06}, {"id": 1060, "seek": 512208, "start": 5129.16, "end": 5130.16, "text": " and stuff.", "tokens": [293, 1507, 13], "temperature": 0.0, "avg_logprob": -0.20958985478044992, "compression_ratio": 1.5121951219512195, "no_speech_prob": 5.422195954452036e-06}, {"id": 1061, "seek": 512208, "start": 5130.16, "end": 5139.64, "text": " So here's the basic idea, at the start of training, let's set PR force really high,", "tokens": [407, 510, 311, 264, 3875, 1558, 11, 412, 264, 722, 295, 3097, 11, 718, 311, 992, 11568, 3464, 534, 1090, 11], "temperature": 0.0, "avg_logprob": -0.20958985478044992, "compression_ratio": 1.5121951219512195, "no_speech_prob": 5.422195954452036e-06}, {"id": 1062, "seek": 512208, "start": 5139.64, "end": 5147.84, "text": " so that nearly always it gets the actual correct previous word, and so it has a useful input.", "tokens": [370, 300, 6217, 1009, 309, 2170, 264, 3539, 3006, 3894, 1349, 11, 293, 370, 309, 575, 257, 4420, 4846, 13], "temperature": 0.0, "avg_logprob": -0.20958985478044992, "compression_ratio": 1.5121951219512195, "no_speech_prob": 5.422195954452036e-06}, {"id": 1063, "seek": 514784, "start": 5147.84, "end": 5155.4800000000005, "text": " And then as I train a bit more, let's decrease PR force so that by the end PR force is 0", "tokens": [400, 550, 382, 286, 3847, 257, 857, 544, 11, 718, 311, 11514, 11568, 3464, 370, 300, 538, 264, 917, 11568, 3464, 307, 1958], "temperature": 0.0, "avg_logprob": -0.12297249924052846, "compression_ratio": 1.5721153846153846, "no_speech_prob": 6.681509603367886e-07}, {"id": 1064, "seek": 514784, "start": 5155.4800000000005, "end": 5160.64, "text": " and it has to learn properly, which is fine because it's now actually feeding in sensible", "tokens": [293, 309, 575, 281, 1466, 6108, 11, 597, 307, 2489, 570, 309, 311, 586, 767, 12919, 294, 25380], "temperature": 0.0, "avg_logprob": -0.12297249924052846, "compression_ratio": 1.5721153846153846, "no_speech_prob": 6.681509603367886e-07}, {"id": 1065, "seek": 514784, "start": 5160.64, "end": 5163.74, "text": " inputs most of the time anyway.", "tokens": [15743, 881, 295, 264, 565, 4033, 13], "temperature": 0.0, "avg_logprob": -0.12297249924052846, "compression_ratio": 1.5721153846153846, "no_speech_prob": 6.681509603367886e-07}, {"id": 1066, "seek": 514784, "start": 5163.74, "end": 5171.0, "text": " So let's now write something such that in the training loop, it gradually decreases", "tokens": [407, 718, 311, 586, 2464, 746, 1270, 300, 294, 264, 3097, 6367, 11, 309, 13145, 24108], "temperature": 0.0, "avg_logprob": -0.12297249924052846, "compression_ratio": 1.5721153846153846, "no_speech_prob": 6.681509603367886e-07}, {"id": 1067, "seek": 514784, "start": 5171.0, "end": 5173.4800000000005, "text": " PR force.", "tokens": [11568, 3464, 13], "temperature": 0.0, "avg_logprob": -0.12297249924052846, "compression_ratio": 1.5721153846153846, "no_speech_prob": 6.681509603367886e-07}, {"id": 1068, "seek": 514784, "start": 5173.4800000000005, "end": 5175.04, "text": " So how do you do that?", "tokens": [407, 577, 360, 291, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.12297249924052846, "compression_ratio": 1.5721153846153846, "no_speech_prob": 6.681509603367886e-07}, {"id": 1069, "seek": 517504, "start": 5175.04, "end": 5178.6, "text": " Well one approach would be to write our own training loop.", "tokens": [1042, 472, 3109, 576, 312, 281, 2464, 527, 1065, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.1926664844635994, "compression_ratio": 1.7432432432432432, "no_speech_prob": 9.51618221733952e-06}, {"id": 1070, "seek": 517504, "start": 5178.6, "end": 5183.88, "text": " But let's not do that because we already have a training loop that has progress bars and", "tokens": [583, 718, 311, 406, 360, 300, 570, 321, 1217, 362, 257, 3097, 6367, 300, 575, 4205, 10228, 293], "temperature": 0.0, "avg_logprob": -0.1926664844635994, "compression_ratio": 1.7432432432432432, "no_speech_prob": 9.51618221733952e-06}, {"id": 1071, "seek": 517504, "start": 5183.88, "end": 5188.28, "text": " uses exponential weighted averages to smooth out the losses and keeps track of metrics", "tokens": [4960, 21510, 32807, 42257, 281, 5508, 484, 264, 15352, 293, 5965, 2837, 295, 16367], "temperature": 0.0, "avg_logprob": -0.1926664844635994, "compression_ratio": 1.7432432432432432, "no_speech_prob": 9.51618221733952e-06}, {"id": 1072, "seek": 517504, "start": 5188.28, "end": 5191.96, "text": " and you know it does a bunch of things which, they're not rocket science but they're kind", "tokens": [293, 291, 458, 309, 775, 257, 3840, 295, 721, 597, 11, 436, 434, 406, 13012, 3497, 457, 436, 434, 733], "temperature": 0.0, "avg_logprob": -0.1926664844635994, "compression_ratio": 1.7432432432432432, "no_speech_prob": 9.51618221733952e-06}, {"id": 1073, "seek": 517504, "start": 5191.96, "end": 5197.72, "text": " of convenient, and they also kind of keep track of calling the reset for RNNs at the", "tokens": [295, 10851, 11, 293, 436, 611, 733, 295, 1066, 2837, 295, 5141, 264, 14322, 337, 45702, 45, 82, 412, 264], "temperature": 0.0, "avg_logprob": -0.1926664844635994, "compression_ratio": 1.7432432432432432, "no_speech_prob": 9.51618221733952e-06}, {"id": 1074, "seek": 517504, "start": 5197.72, "end": 5201.46, "text": " start of an epoch to make sure that the hidden state is set to zeros and you know little", "tokens": [722, 295, 364, 30992, 339, 281, 652, 988, 300, 264, 7633, 1785, 307, 992, 281, 35193, 293, 291, 458, 707], "temperature": 0.0, "avg_logprob": -0.1926664844635994, "compression_ratio": 1.7432432432432432, "no_speech_prob": 9.51618221733952e-06}, {"id": 1075, "seek": 517504, "start": 5201.46, "end": 5202.46, "text": " things like that.", "tokens": [721, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.1926664844635994, "compression_ratio": 1.7432432432432432, "no_speech_prob": 9.51618221733952e-06}, {"id": 1076, "seek": 520246, "start": 5202.46, "end": 5205.64, "text": " We'd rather not have to write that from scratch.", "tokens": [492, 1116, 2831, 406, 362, 281, 2464, 300, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.1246193452314897, "compression_ratio": 1.7608695652173914, "no_speech_prob": 4.637855909095379e-06}, {"id": 1077, "seek": 520246, "start": 5205.64, "end": 5213.4800000000005, "text": " So what we've tended to find is that as I start to write some new thing and I'm like", "tokens": [407, 437, 321, 600, 34732, 281, 915, 307, 300, 382, 286, 722, 281, 2464, 512, 777, 551, 293, 286, 478, 411], "temperature": 0.0, "avg_logprob": -0.1246193452314897, "compression_ratio": 1.7608695652173914, "no_speech_prob": 4.637855909095379e-06}, {"id": 1078, "seek": 520246, "start": 5213.4800000000005, "end": 5219.28, "text": " oh I need to kind of replace some part of the code, I'll then kind of add some little", "tokens": [1954, 286, 643, 281, 733, 295, 7406, 512, 644, 295, 264, 3089, 11, 286, 603, 550, 733, 295, 909, 512, 707], "temperature": 0.0, "avg_logprob": -0.1246193452314897, "compression_ratio": 1.7608695652173914, "no_speech_prob": 4.637855909095379e-06}, {"id": 1079, "seek": 520246, "start": 5219.28, "end": 5224.0, "text": " hook so that we can all use that hook to make things easier.", "tokens": [6328, 370, 300, 321, 393, 439, 764, 300, 6328, 281, 652, 721, 3571, 13], "temperature": 0.0, "avg_logprob": -0.1246193452314897, "compression_ratio": 1.7608695652173914, "no_speech_prob": 4.637855909095379e-06}, {"id": 1080, "seek": 520246, "start": 5224.0, "end": 5228.84, "text": " In this particular case, there's a hook that I've ended up using all the damn time now", "tokens": [682, 341, 1729, 1389, 11, 456, 311, 257, 6328, 300, 286, 600, 4590, 493, 1228, 439, 264, 8151, 565, 586], "temperature": 0.0, "avg_logprob": -0.1246193452314897, "compression_ratio": 1.7608695652173914, "no_speech_prob": 4.637855909095379e-06}, {"id": 1081, "seek": 520246, "start": 5228.84, "end": 5232.08, "text": " which is the hook called the stepper.", "tokens": [597, 307, 264, 6328, 1219, 264, 2126, 3717, 13], "temperature": 0.0, "avg_logprob": -0.1246193452314897, "compression_ratio": 1.7608695652173914, "no_speech_prob": 4.637855909095379e-06}, {"id": 1082, "seek": 523208, "start": 5232.08, "end": 5241.32, "text": " And so if you look at our code, model.py is where our fit function lives, right?", "tokens": [400, 370, 498, 291, 574, 412, 527, 3089, 11, 2316, 13, 8200, 307, 689, 527, 3318, 2445, 2909, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13387651443481446, "compression_ratio": 1.7622950819672132, "no_speech_prob": 7.296351668628631e-06}, {"id": 1083, "seek": 523208, "start": 5241.32, "end": 5246.2, "text": " And so the fit function in model.py is kind of, we've seen it before, I think it's like", "tokens": [400, 370, 264, 3318, 2445, 294, 2316, 13, 8200, 307, 733, 295, 11, 321, 600, 1612, 309, 949, 11, 286, 519, 309, 311, 411], "temperature": 0.0, "avg_logprob": -0.13387651443481446, "compression_ratio": 1.7622950819672132, "no_speech_prob": 7.296351668628631e-06}, {"id": 1084, "seek": 523208, "start": 5246.2, "end": 5250.88, "text": " the lowest level thing that doesn't require a learner, it doesn't really require anything", "tokens": [264, 12437, 1496, 551, 300, 1177, 380, 3651, 257, 33347, 11, 309, 1177, 380, 534, 3651, 1340], "temperature": 0.0, "avg_logprob": -0.13387651443481446, "compression_ratio": 1.7622950819672132, "no_speech_prob": 7.296351668628631e-06}, {"id": 1085, "seek": 523208, "start": 5250.88, "end": 5255.32, "text": " much at all, it just requires a standard PyTorch model and a model data object.", "tokens": [709, 412, 439, 11, 309, 445, 7029, 257, 3832, 9953, 51, 284, 339, 2316, 293, 257, 2316, 1412, 2657, 13], "temperature": 0.0, "avg_logprob": -0.13387651443481446, "compression_ratio": 1.7622950819672132, "no_speech_prob": 7.296351668628631e-06}, {"id": 1086, "seek": 523208, "start": 5255.32, "end": 5260.68, "text": " You just need to know how many epochs, a standard PyTorch optimizer, and a standard PyTorch", "tokens": [509, 445, 643, 281, 458, 577, 867, 30992, 28346, 11, 257, 3832, 9953, 51, 284, 339, 5028, 6545, 11, 293, 257, 3832, 9953, 51, 284, 339], "temperature": 0.0, "avg_logprob": -0.13387651443481446, "compression_ratio": 1.7622950819672132, "no_speech_prob": 7.296351668628631e-06}, {"id": 1087, "seek": 526068, "start": 5260.68, "end": 5262.16, "text": " loss function.", "tokens": [4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.16431881132579984, "compression_ratio": 1.8571428571428572, "no_speech_prob": 1.184307984658517e-05}, {"id": 1088, "seek": 526068, "start": 5262.16, "end": 5266.56, "text": " So you can call, we've hardly ever used it in the class, we normally call learn.fit,", "tokens": [407, 291, 393, 818, 11, 321, 600, 13572, 1562, 1143, 309, 294, 264, 1508, 11, 321, 5646, 818, 1466, 13, 6845, 11], "temperature": 0.0, "avg_logprob": -0.16431881132579984, "compression_ratio": 1.8571428571428572, "no_speech_prob": 1.184307984658517e-05}, {"id": 1089, "seek": 526068, "start": 5266.56, "end": 5268.360000000001, "text": " but learn.fit calls this.", "tokens": [457, 1466, 13, 6845, 5498, 341, 13], "temperature": 0.0, "avg_logprob": -0.16431881132579984, "compression_ratio": 1.8571428571428572, "no_speech_prob": 1.184307984658517e-05}, {"id": 1090, "seek": 526068, "start": 5268.360000000001, "end": 5270.200000000001, "text": " This is our lowest level thing.", "tokens": [639, 307, 527, 12437, 1496, 551, 13], "temperature": 0.0, "avg_logprob": -0.16431881132579984, "compression_ratio": 1.8571428571428572, "no_speech_prob": 1.184307984658517e-05}, {"id": 1091, "seek": 526068, "start": 5270.200000000001, "end": 5274.84, "text": " But we've looked at the source code here sometimes, we've seen how it loops through each epoch", "tokens": [583, 321, 600, 2956, 412, 264, 4009, 3089, 510, 2171, 11, 321, 600, 1612, 577, 309, 16121, 807, 1184, 30992, 339], "temperature": 0.0, "avg_logprob": -0.16431881132579984, "compression_ratio": 1.8571428571428572, "no_speech_prob": 1.184307984658517e-05}, {"id": 1092, "seek": 526068, "start": 5274.84, "end": 5281.6, "text": " and it loops through each thing in our batch and calls stepper.step.", "tokens": [293, 309, 16121, 807, 1184, 551, 294, 527, 15245, 293, 5498, 2126, 3717, 13, 16792, 13], "temperature": 0.0, "avg_logprob": -0.16431881132579984, "compression_ratio": 1.8571428571428572, "no_speech_prob": 1.184307984658517e-05}, {"id": 1093, "seek": 526068, "start": 5281.6, "end": 5286.68, "text": " And so stepper.step is the thing that's responsible for calling the model, getting the loss, finding", "tokens": [400, 370, 2126, 3717, 13, 16792, 307, 264, 551, 300, 311, 6250, 337, 5141, 264, 2316, 11, 1242, 264, 4470, 11, 5006], "temperature": 0.0, "avg_logprob": -0.16431881132579984, "compression_ratio": 1.8571428571428572, "no_speech_prob": 1.184307984658517e-05}, {"id": 1094, "seek": 526068, "start": 5286.68, "end": 5290.1, "text": " the loss function, and calling the optimizer.", "tokens": [264, 4470, 2445, 11, 293, 5141, 264, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.16431881132579984, "compression_ratio": 1.8571428571428572, "no_speech_prob": 1.184307984658517e-05}, {"id": 1095, "seek": 529010, "start": 5290.1, "end": 5300.280000000001, "text": " And so by default, stepper.step uses a particular class called stepper, which basically calls", "tokens": [400, 370, 538, 7576, 11, 2126, 3717, 13, 16792, 4960, 257, 1729, 1508, 1219, 2126, 3717, 11, 597, 1936, 5498], "temperature": 0.0, "avg_logprob": -0.1634595430814303, "compression_ratio": 1.6158536585365855, "no_speech_prob": 5.862792477273615e-06}, {"id": 1096, "seek": 529010, "start": 5300.280000000001, "end": 5311.120000000001, "text": " the model, so the model ends up inside m, zeros the gradients, calls the loss function,", "tokens": [264, 2316, 11, 370, 264, 2316, 5314, 493, 1854, 275, 11, 35193, 264, 2771, 2448, 11, 5498, 264, 4470, 2445, 11], "temperature": 0.0, "avg_logprob": -0.1634595430814303, "compression_ratio": 1.6158536585365855, "no_speech_prob": 5.862792477273615e-06}, {"id": 1097, "seek": 529010, "start": 5311.120000000001, "end": 5317.120000000001, "text": " calls backwards, does gradient clipping if necessary, and then calls the optimizer.", "tokens": [5498, 12204, 11, 775, 16235, 49320, 498, 4818, 11, 293, 550, 5498, 264, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.1634595430814303, "compression_ratio": 1.6158536585365855, "no_speech_prob": 5.862792477273615e-06}, {"id": 1098, "seek": 531712, "start": 5317.12, "end": 5323.3, "text": " So you know, they're the basic steps that back when we looked at PyTorch from scratch", "tokens": [407, 291, 458, 11, 436, 434, 264, 3875, 4439, 300, 646, 562, 321, 2956, 412, 9953, 51, 284, 339, 490, 8459], "temperature": 0.0, "avg_logprob": -0.1194253429289787, "compression_ratio": 1.6556603773584906, "no_speech_prob": 2.026138645305764e-06}, {"id": 1099, "seek": 531712, "start": 5323.3, "end": 5324.96, "text": " we had to do.", "tokens": [321, 632, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1194253429289787, "compression_ratio": 1.6556603773584906, "no_speech_prob": 2.026138645305764e-06}, {"id": 1100, "seek": 531712, "start": 5324.96, "end": 5332.8, "text": " So the nice thing is, we can replace that with something else, rather than replacing", "tokens": [407, 264, 1481, 551, 307, 11, 321, 393, 7406, 300, 365, 746, 1646, 11, 2831, 813, 19139], "temperature": 0.0, "avg_logprob": -0.1194253429289787, "compression_ratio": 1.6556603773584906, "no_speech_prob": 2.026138645305764e-06}, {"id": 1101, "seek": 531712, "start": 5332.8, "end": 5334.16, "text": " the training loop.", "tokens": [264, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.1194253429289787, "compression_ratio": 1.6556603773584906, "no_speech_prob": 2.026138645305764e-06}, {"id": 1102, "seek": 531712, "start": 5334.16, "end": 5341.5199999999995, "text": " So if you inherit from stepper and then write your own version of step, you can just copy", "tokens": [407, 498, 291, 21389, 490, 2126, 3717, 293, 550, 2464, 428, 1065, 3037, 295, 1823, 11, 291, 393, 445, 5055], "temperature": 0.0, "avg_logprob": -0.1194253429289787, "compression_ratio": 1.6556603773584906, "no_speech_prob": 2.026138645305764e-06}, {"id": 1103, "seek": 531712, "start": 5341.5199999999995, "end": 5346.2, "text": " and paste the contents of step and add whatever you like.", "tokens": [293, 9163, 264, 15768, 295, 1823, 293, 909, 2035, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.1194253429289787, "compression_ratio": 1.6556603773584906, "no_speech_prob": 2.026138645305764e-06}, {"id": 1104, "seek": 534620, "start": 5346.2, "end": 5352.679999999999, "text": " Or if it's something you're going to do before or afterwards, you could even call super.step.", "tokens": [1610, 498, 309, 311, 746, 291, 434, 516, 281, 360, 949, 420, 10543, 11, 291, 727, 754, 818, 1687, 13, 16792, 13], "temperature": 0.0, "avg_logprob": -0.21874608993530273, "compression_ratio": 1.4950980392156863, "no_speech_prob": 4.710893790615955e-06}, {"id": 1105, "seek": 534620, "start": 5352.679999999999, "end": 5358.36, "text": " In this case, I rather suspect I've been unnecessarily complicated here.", "tokens": [682, 341, 1389, 11, 286, 2831, 9091, 286, 600, 668, 16799, 3289, 6179, 510, 13], "temperature": 0.0, "avg_logprob": -0.21874608993530273, "compression_ratio": 1.4950980392156863, "no_speech_prob": 4.710893790615955e-06}, {"id": 1106, "seek": 534620, "start": 5358.36, "end": 5368.0, "text": " I probably could have commented out all of that and just said super.step x, y, epoch,", "tokens": [286, 1391, 727, 362, 26940, 484, 439, 295, 300, 293, 445, 848, 1687, 13, 16792, 2031, 11, 288, 11, 30992, 339, 11], "temperature": 0.0, "avg_logprob": -0.21874608993530273, "compression_ratio": 1.4950980392156863, "no_speech_prob": 4.710893790615955e-06}, {"id": 1107, "seek": 534620, "start": 5368.0, "end": 5372.24, "text": " because I think this is an exact copy of everything.", "tokens": [570, 286, 519, 341, 307, 364, 1900, 5055, 295, 1203, 13], "temperature": 0.0, "avg_logprob": -0.21874608993530273, "compression_ratio": 1.4950980392156863, "no_speech_prob": 4.710893790615955e-06}, {"id": 1108, "seek": 537224, "start": 5372.24, "end": 5377.599999999999, "text": " But as I say, when I'm prototyping, I don't think carefully about how to minimize my code.", "tokens": [583, 382, 286, 584, 11, 562, 286, 478, 46219, 3381, 11, 286, 500, 380, 519, 7500, 466, 577, 281, 17522, 452, 3089, 13], "temperature": 0.0, "avg_logprob": -0.13914423549876492, "compression_ratio": 1.5091743119266054, "no_speech_prob": 4.565959443425527e-06}, {"id": 1109, "seek": 537224, "start": 5377.599999999999, "end": 5382.32, "text": " I copied and pasted the contents of the code from step and I added a single line to the", "tokens": [286, 25365, 293, 1791, 292, 264, 15768, 295, 264, 3089, 490, 1823, 293, 286, 3869, 257, 2167, 1622, 281, 264], "temperature": 0.0, "avg_logprob": -0.13914423549876492, "compression_ratio": 1.5091743119266054, "no_speech_prob": 4.565959443425527e-06}, {"id": 1110, "seek": 537224, "start": 5382.32, "end": 5394.639999999999, "text": " top which was to replace PR force in my module with something that gradually decreased linearly", "tokens": [1192, 597, 390, 281, 7406, 11568, 3464, 294, 452, 10088, 365, 746, 300, 13145, 24436, 43586], "temperature": 0.0, "avg_logprob": -0.13914423549876492, "compression_ratio": 1.5091743119266054, "no_speech_prob": 4.565959443425527e-06}, {"id": 1111, "seek": 537224, "start": 5394.639999999999, "end": 5399.639999999999, "text": " for the first 10 epochs and after 10 epochs, it was 0.", "tokens": [337, 264, 700, 1266, 30992, 28346, 293, 934, 1266, 30992, 28346, 11, 309, 390, 1958, 13], "temperature": 0.0, "avg_logprob": -0.13914423549876492, "compression_ratio": 1.5091743119266054, "no_speech_prob": 4.565959443425527e-06}, {"id": 1112, "seek": 539964, "start": 5399.64, "end": 5404.84, "text": " So total hack, but good enough to try it out.", "tokens": [407, 3217, 10339, 11, 457, 665, 1547, 281, 853, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.24441703796386718, "compression_ratio": 1.48, "no_speech_prob": 9.818284524953924e-06}, {"id": 1113, "seek": 539964, "start": 5404.84, "end": 5411.84, "text": " And so the nice thing is that everything else is the same.", "tokens": [400, 370, 264, 1481, 551, 307, 300, 1203, 1646, 307, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.24441703796386718, "compression_ratio": 1.48, "no_speech_prob": 9.818284524953924e-06}, {"id": 1114, "seek": 539964, "start": 5411.84, "end": 5418.0, "text": " I've added these 3 lines of code to my module.", "tokens": [286, 600, 3869, 613, 805, 3876, 295, 3089, 281, 452, 10088, 13], "temperature": 0.0, "avg_logprob": -0.24441703796386718, "compression_ratio": 1.48, "no_speech_prob": 9.818284524953924e-06}, {"id": 1115, "seek": 539964, "start": 5418.0, "end": 5423.88, "text": " And the only thing I need to do that's different is when I call fit, I pass in my customized", "tokens": [400, 264, 787, 551, 286, 643, 281, 360, 300, 311, 819, 307, 562, 286, 818, 3318, 11, 286, 1320, 294, 452, 30581], "temperature": 0.0, "avg_logprob": -0.24441703796386718, "compression_ratio": 1.48, "no_speech_prob": 9.818284524953924e-06}, {"id": 1116, "seek": 539964, "start": 5423.88, "end": 5426.88, "text": " step of class.", "tokens": [1823, 295, 1508, 13], "temperature": 0.0, "avg_logprob": -0.24441703796386718, "compression_ratio": 1.48, "no_speech_prob": 9.818284524953924e-06}, {"id": 1117, "seek": 542688, "start": 5426.88, "end": 5430.16, "text": " And so that's going to do teacher forcing.", "tokens": [400, 370, 300, 311, 516, 281, 360, 5027, 19030, 13], "temperature": 0.0, "avg_logprob": -0.19147992374921086, "compression_ratio": 1.5336322869955157, "no_speech_prob": 9.818243597692344e-06}, {"id": 1118, "seek": 542688, "start": 5430.16, "end": 5434.88, "text": " We don't have bidirectional, so we're just changing one thing at a time.", "tokens": [492, 500, 380, 362, 12957, 621, 41048, 11, 370, 321, 434, 445, 4473, 472, 551, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.19147992374921086, "compression_ratio": 1.5336322869955157, "no_speech_prob": 9.818243597692344e-06}, {"id": 1119, "seek": 542688, "start": 5434.88, "end": 5445.4800000000005, "text": " So we should compare this to our unidirectional results, which was 3.58 and this is 3.49.", "tokens": [407, 321, 820, 6794, 341, 281, 527, 517, 327, 621, 41048, 3542, 11, 597, 390, 805, 13, 20419, 293, 341, 307, 805, 13, 14938, 13], "temperature": 0.0, "avg_logprob": -0.19147992374921086, "compression_ratio": 1.5336322869955157, "no_speech_prob": 9.818243597692344e-06}, {"id": 1120, "seek": 542688, "start": 5445.4800000000005, "end": 5447.84, "text": " So that was an improvement.", "tokens": [407, 300, 390, 364, 10444, 13], "temperature": 0.0, "avg_logprob": -0.19147992374921086, "compression_ratio": 1.5336322869955157, "no_speech_prob": 9.818243597692344e-06}, {"id": 1121, "seek": 542688, "start": 5447.84, "end": 5449.0, "text": " So that's great.", "tokens": [407, 300, 311, 869, 13], "temperature": 0.0, "avg_logprob": -0.19147992374921086, "compression_ratio": 1.5336322869955157, "no_speech_prob": 9.818243597692344e-06}, {"id": 1122, "seek": 542688, "start": 5449.0, "end": 5454.88, "text": " I needed to make sure I at least did 10 epochs because before that it was cheating by using", "tokens": [286, 2978, 281, 652, 988, 286, 412, 1935, 630, 1266, 30992, 28346, 570, 949, 300, 309, 390, 18309, 538, 1228], "temperature": 0.0, "avg_logprob": -0.19147992374921086, "compression_ratio": 1.5336322869955157, "no_speech_prob": 9.818243597692344e-06}, {"id": 1123, "seek": 545488, "start": 5454.88, "end": 5458.4400000000005, "text": " the teacher forcing.", "tokens": [264, 5027, 19030, 13], "temperature": 0.0, "avg_logprob": -0.19019411260431462, "compression_ratio": 1.4094488188976377, "no_speech_prob": 1.9033733451578883e-06}, {"id": 1124, "seek": 545488, "start": 5458.4400000000005, "end": 5461.4400000000005, "text": " So that's good, that's an improvement.", "tokens": [407, 300, 311, 665, 11, 300, 311, 364, 10444, 13], "temperature": 0.0, "avg_logprob": -0.19019411260431462, "compression_ratio": 1.4094488188976377, "no_speech_prob": 1.9033733451578883e-06}, {"id": 1125, "seek": 545488, "start": 5461.4400000000005, "end": 5467.88, "text": " So we've got another trick, and this next trick is a bigger trick.", "tokens": [407, 321, 600, 658, 1071, 4282, 11, 293, 341, 958, 4282, 307, 257, 3801, 4282, 13], "temperature": 0.0, "avg_logprob": -0.19019411260431462, "compression_ratio": 1.4094488188976377, "no_speech_prob": 1.9033733451578883e-06}, {"id": 1126, "seek": 545488, "start": 5467.88, "end": 5469.68, "text": " It's a pretty cool trick.", "tokens": [467, 311, 257, 1238, 1627, 4282, 13], "temperature": 0.0, "avg_logprob": -0.19019411260431462, "compression_ratio": 1.4094488188976377, "no_speech_prob": 1.9033733451578883e-06}, {"id": 1127, "seek": 545488, "start": 5469.68, "end": 5472.8, "text": " And it's called attention.", "tokens": [400, 309, 311, 1219, 3202, 13], "temperature": 0.0, "avg_logprob": -0.19019411260431462, "compression_ratio": 1.4094488188976377, "no_speech_prob": 1.9033733451578883e-06}, {"id": 1128, "seek": 547280, "start": 5472.8, "end": 5485.96, "text": " And the basic idea of attention is this, which is expecting the entirety of the sentence", "tokens": [400, 264, 3875, 1558, 295, 3202, 307, 341, 11, 597, 307, 9650, 264, 31557, 295, 264, 8174], "temperature": 0.0, "avg_logprob": -0.11860753619481647, "compression_ratio": 1.5818181818181818, "no_speech_prob": 5.682374649040867e-06}, {"id": 1129, "seek": 547280, "start": 5485.96, "end": 5492.400000000001, "text": " to be summarized into this single hidden vector is asking a lot.", "tokens": [281, 312, 14611, 1602, 666, 341, 2167, 7633, 8062, 307, 3365, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.11860753619481647, "compression_ratio": 1.5818181818181818, "no_speech_prob": 5.682374649040867e-06}, {"id": 1130, "seek": 547280, "start": 5492.400000000001, "end": 5498.88, "text": " It has to know what was said and how it was said and everything necessary to create the", "tokens": [467, 575, 281, 458, 437, 390, 848, 293, 577, 309, 390, 848, 293, 1203, 4818, 281, 1884, 264], "temperature": 0.0, "avg_logprob": -0.11860753619481647, "compression_ratio": 1.5818181818181818, "no_speech_prob": 5.682374649040867e-06}, {"id": 1131, "seek": 547280, "start": 5498.88, "end": 5501.28, "text": " sentence in German.", "tokens": [8174, 294, 6521, 13], "temperature": 0.0, "avg_logprob": -0.11860753619481647, "compression_ratio": 1.5818181818181818, "no_speech_prob": 5.682374649040867e-06}, {"id": 1132, "seek": 550128, "start": 5501.28, "end": 5506.8, "text": " And so the idea of attention is basically like maybe we're asking too much, particularly", "tokens": [400, 370, 264, 1558, 295, 3202, 307, 1936, 411, 1310, 321, 434, 3365, 886, 709, 11, 4098], "temperature": 0.0, "avg_logprob": -0.1396566726065971, "compression_ratio": 1.5968586387434556, "no_speech_prob": 1.0289100828231312e-05}, {"id": 1133, "seek": 550128, "start": 5506.8, "end": 5517.32, "text": " because we could use this form of model where we output every step of the loop to not just", "tokens": [570, 321, 727, 764, 341, 1254, 295, 2316, 689, 321, 5598, 633, 1823, 295, 264, 6367, 281, 406, 445], "temperature": 0.0, "avg_logprob": -0.1396566726065971, "compression_ratio": 1.5968586387434556, "no_speech_prob": 1.0289100828231312e-05}, {"id": 1134, "seek": 550128, "start": 5517.32, "end": 5522.719999999999, "text": " have a hidden state at the end, but to have a hidden state after every single word.", "tokens": [362, 257, 7633, 1785, 412, 264, 917, 11, 457, 281, 362, 257, 7633, 1785, 934, 633, 2167, 1349, 13], "temperature": 0.0, "avg_logprob": -0.1396566726065971, "compression_ratio": 1.5968586387434556, "no_speech_prob": 1.0289100828231312e-05}, {"id": 1135, "seek": 550128, "start": 5522.719999999999, "end": 5527.08, "text": " And why not try and use that information?", "tokens": [400, 983, 406, 853, 293, 764, 300, 1589, 30], "temperature": 0.0, "avg_logprob": -0.1396566726065971, "compression_ratio": 1.5968586387434556, "no_speech_prob": 1.0289100828231312e-05}, {"id": 1136, "seek": 552708, "start": 5527.08, "end": 5533.16, "text": " It's already there, and so far we've just been throwing it away.", "tokens": [467, 311, 1217, 456, 11, 293, 370, 1400, 321, 600, 445, 668, 10238, 309, 1314, 13], "temperature": 0.0, "avg_logprob": -0.13289395199027113, "compression_ratio": 1.9195402298850575, "no_speech_prob": 2.2125241230241954e-05}, {"id": 1137, "seek": 552708, "start": 5533.16, "end": 5542.44, "text": " And not only that, but bidirectional, we've got every step, we've got two vectors of state", "tokens": [400, 406, 787, 300, 11, 457, 12957, 621, 41048, 11, 321, 600, 658, 633, 1823, 11, 321, 600, 658, 732, 18875, 295, 1785], "temperature": 0.0, "avg_logprob": -0.13289395199027113, "compression_ratio": 1.9195402298850575, "no_speech_prob": 2.2125241230241954e-05}, {"id": 1138, "seek": 552708, "start": 5542.44, "end": 5544.28, "text": " that we can use.", "tokens": [300, 321, 393, 764, 13], "temperature": 0.0, "avg_logprob": -0.13289395199027113, "compression_ratio": 1.9195402298850575, "no_speech_prob": 2.2125241230241954e-05}, {"id": 1139, "seek": 552708, "start": 5544.28, "end": 5548.88, "text": " So how could we use this piece of state, this piece of state, this piece of state, this", "tokens": [407, 577, 727, 321, 764, 341, 2522, 295, 1785, 11, 341, 2522, 295, 1785, 11, 341, 2522, 295, 1785, 11, 341], "temperature": 0.0, "avg_logprob": -0.13289395199027113, "compression_ratio": 1.9195402298850575, "no_speech_prob": 2.2125241230241954e-05}, {"id": 1140, "seek": 552708, "start": 5548.88, "end": 5554.08, "text": " piece of state, and this piece of state rather than just the final state?", "tokens": [2522, 295, 1785, 11, 293, 341, 2522, 295, 1785, 2831, 813, 445, 264, 2572, 1785, 30], "temperature": 0.0, "avg_logprob": -0.13289395199027113, "compression_ratio": 1.9195402298850575, "no_speech_prob": 2.2125241230241954e-05}, {"id": 1141, "seek": 555408, "start": 5554.08, "end": 5560.32, "text": " And so the basic idea is, well, let's say I'm doing this word, translating this word", "tokens": [400, 370, 264, 3875, 1558, 307, 11, 731, 11, 718, 311, 584, 286, 478, 884, 341, 1349, 11, 35030, 341, 1349], "temperature": 0.0, "avg_logprob": -0.16579829420998832, "compression_ratio": 1.6837209302325582, "no_speech_prob": 2.840932211256586e-05}, {"id": 1142, "seek": 555408, "start": 5560.32, "end": 5562.16, "text": " right now.", "tokens": [558, 586, 13], "temperature": 0.0, "avg_logprob": -0.16579829420998832, "compression_ratio": 1.6837209302325582, "no_speech_prob": 2.840932211256586e-05}, {"id": 1143, "seek": 555408, "start": 5562.16, "end": 5566.08, "text": " Which of these five pieces of state do I want?", "tokens": [3013, 295, 613, 1732, 3755, 295, 1785, 360, 286, 528, 30], "temperature": 0.0, "avg_logprob": -0.16579829420998832, "compression_ratio": 1.6837209302325582, "no_speech_prob": 2.840932211256586e-05}, {"id": 1144, "seek": 555408, "start": 5566.08, "end": 5571.68, "text": " And of course the answer is, if I'm doing, well actually let's pick a more interesting", "tokens": [400, 295, 1164, 264, 1867, 307, 11, 498, 286, 478, 884, 11, 731, 767, 718, 311, 1888, 257, 544, 1880], "temperature": 0.0, "avg_logprob": -0.16579829420998832, "compression_ratio": 1.6837209302325582, "no_speech_prob": 2.840932211256586e-05}, {"id": 1145, "seek": 555408, "start": 5571.68, "end": 5572.68, "text": " word.", "tokens": [1349, 13], "temperature": 0.0, "avg_logprob": -0.16579829420998832, "compression_ratio": 1.6837209302325582, "no_speech_prob": 2.840932211256586e-05}, {"id": 1146, "seek": 555408, "start": 5572.68, "end": 5573.68, "text": " Let's pick this one.", "tokens": [961, 311, 1888, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.16579829420998832, "compression_ratio": 1.6837209302325582, "no_speech_prob": 2.840932211256586e-05}, {"id": 1147, "seek": 555408, "start": 5573.68, "end": 5579.84, "text": " So if I'm trying to do loved, then clearly the hidden state I want is this one, because", "tokens": [407, 498, 286, 478, 1382, 281, 360, 4333, 11, 550, 4448, 264, 7633, 1785, 286, 528, 307, 341, 472, 11, 570], "temperature": 0.0, "avg_logprob": -0.16579829420998832, "compression_ratio": 1.6837209302325582, "no_speech_prob": 2.840932211256586e-05}, {"id": 1148, "seek": 555408, "start": 5579.84, "end": 5583.4, "text": " this is the word.", "tokens": [341, 307, 264, 1349, 13], "temperature": 0.0, "avg_logprob": -0.16579829420998832, "compression_ratio": 1.6837209302325582, "no_speech_prob": 2.840932211256586e-05}, {"id": 1149, "seek": 558340, "start": 5583.4, "end": 5597.5599999999995, "text": " And then for this preposition, I probably would need this and this and this to make", "tokens": [400, 550, 337, 341, 2666, 5830, 11, 286, 1391, 576, 643, 341, 293, 341, 293, 341, 281, 652], "temperature": 0.0, "avg_logprob": -0.16711710521153042, "compression_ratio": 1.5739644970414202, "no_speech_prob": 1.7778322217054665e-05}, {"id": 1150, "seek": 558340, "start": 5597.5599999999995, "end": 5601.92, "text": " sure I've got the tense right and know that I actually need this part of the verb and", "tokens": [988, 286, 600, 658, 264, 18760, 558, 293, 458, 300, 286, 767, 643, 341, 644, 295, 264, 9595, 293], "temperature": 0.0, "avg_logprob": -0.16711710521153042, "compression_ratio": 1.5739644970414202, "no_speech_prob": 1.7778322217054665e-05}, {"id": 1151, "seek": 558340, "start": 5601.92, "end": 5603.799999999999, "text": " so forth.", "tokens": [370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.16711710521153042, "compression_ratio": 1.5739644970414202, "no_speech_prob": 1.7778322217054665e-05}, {"id": 1152, "seek": 558340, "start": 5603.799999999999, "end": 5612.24, "text": " So depending on which bit I'm translating, I'm going to need one or more bits of these", "tokens": [407, 5413, 322, 597, 857, 286, 478, 35030, 11, 286, 478, 516, 281, 643, 472, 420, 544, 9239, 295, 613], "temperature": 0.0, "avg_logprob": -0.16711710521153042, "compression_ratio": 1.5739644970414202, "no_speech_prob": 1.7778322217054665e-05}, {"id": 1153, "seek": 561224, "start": 5612.24, "end": 5614.0, "text": " various hidden states.", "tokens": [3683, 7633, 4368, 13], "temperature": 0.0, "avg_logprob": -0.1357998221811622, "compression_ratio": 1.780373831775701, "no_speech_prob": 9.08044330572011e-06}, {"id": 1154, "seek": 561224, "start": 5614.0, "end": 5618.639999999999, "text": " And in fact, I probably want some weighting of them.", "tokens": [400, 294, 1186, 11, 286, 1391, 528, 512, 3364, 278, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.1357998221811622, "compression_ratio": 1.780373831775701, "no_speech_prob": 9.08044330572011e-06}, {"id": 1155, "seek": 561224, "start": 5618.639999999999, "end": 5624.48, "text": " So like what I'm doing here, I probably mainly want this state, but I maybe want a little", "tokens": [407, 411, 437, 286, 478, 884, 510, 11, 286, 1391, 8704, 528, 341, 1785, 11, 457, 286, 1310, 528, 257, 707], "temperature": 0.0, "avg_logprob": -0.1357998221811622, "compression_ratio": 1.780373831775701, "no_speech_prob": 9.08044330572011e-06}, {"id": 1156, "seek": 561224, "start": 5624.48, "end": 5627.82, "text": " bit of that one and a little bit of that one.", "tokens": [857, 295, 300, 472, 293, 257, 707, 857, 295, 300, 472, 13], "temperature": 0.0, "avg_logprob": -0.1357998221811622, "compression_ratio": 1.780373831775701, "no_speech_prob": 9.08044330572011e-06}, {"id": 1157, "seek": 561224, "start": 5627.82, "end": 5634.44, "text": " So in other words, for these five pieces of hidden state, we want a weighted average.", "tokens": [407, 294, 661, 2283, 11, 337, 613, 1732, 3755, 295, 7633, 1785, 11, 321, 528, 257, 32807, 4274, 13], "temperature": 0.0, "avg_logprob": -0.1357998221811622, "compression_ratio": 1.780373831775701, "no_speech_prob": 9.08044330572011e-06}, {"id": 1158, "seek": 561224, "start": 5634.44, "end": 5640.639999999999, "text": " And we want it weighted by something that can figure out which bits of the sentence", "tokens": [400, 321, 528, 309, 32807, 538, 746, 300, 393, 2573, 484, 597, 9239, 295, 264, 8174], "temperature": 0.0, "avg_logprob": -0.1357998221811622, "compression_ratio": 1.780373831775701, "no_speech_prob": 9.08044330572011e-06}, {"id": 1159, "seek": 564064, "start": 5640.64, "end": 5643.12, "text": " are most important right now.", "tokens": [366, 881, 1021, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.15458633422851562, "compression_ratio": 1.8185654008438819, "no_speech_prob": 3.6119752166996477e-06}, {"id": 1160, "seek": 564064, "start": 5643.12, "end": 5649.72, "text": " So how do we figure out something like which bits of the sentence are important right now?", "tokens": [407, 577, 360, 321, 2573, 484, 746, 411, 597, 9239, 295, 264, 8174, 366, 1021, 558, 586, 30], "temperature": 0.0, "avg_logprob": -0.15458633422851562, "compression_ratio": 1.8185654008438819, "no_speech_prob": 3.6119752166996477e-06}, {"id": 1161, "seek": 564064, "start": 5649.72, "end": 5655.4400000000005, "text": " We create a neural net and we train the neural net to figure it out.", "tokens": [492, 1884, 257, 18161, 2533, 293, 321, 3847, 264, 18161, 2533, 281, 2573, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.15458633422851562, "compression_ratio": 1.8185654008438819, "no_speech_prob": 3.6119752166996477e-06}, {"id": 1162, "seek": 564064, "start": 5655.4400000000005, "end": 5657.72, "text": " When do we train that neural net?", "tokens": [1133, 360, 321, 3847, 300, 18161, 2533, 30], "temperature": 0.0, "avg_logprob": -0.15458633422851562, "compression_ratio": 1.8185654008438819, "no_speech_prob": 3.6119752166996477e-06}, {"id": 1163, "seek": 564064, "start": 5657.72, "end": 5659.46, "text": " End to end.", "tokens": [6967, 281, 917, 13], "temperature": 0.0, "avg_logprob": -0.15458633422851562, "compression_ratio": 1.8185654008438819, "no_speech_prob": 3.6119752166996477e-06}, {"id": 1164, "seek": 564064, "start": 5659.46, "end": 5661.8, "text": " So let's now train two neural nets.", "tokens": [407, 718, 311, 586, 3847, 732, 18161, 36170, 13], "temperature": 0.0, "avg_logprob": -0.15458633422851562, "compression_ratio": 1.8185654008438819, "no_speech_prob": 3.6119752166996477e-06}, {"id": 1165, "seek": 564064, "start": 5661.8, "end": 5662.8, "text": " Well we've actually already kind of got a bunch, right?", "tokens": [1042, 321, 600, 767, 1217, 733, 295, 658, 257, 3840, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15458633422851562, "compression_ratio": 1.8185654008438819, "no_speech_prob": 3.6119752166996477e-06}, {"id": 1166, "seek": 564064, "start": 5662.8, "end": 5669.08, "text": " We've got an RNN encoder, we've got an RNN decoder, we've got a couple of linear layers.", "tokens": [492, 600, 658, 364, 45702, 45, 2058, 19866, 11, 321, 600, 658, 364, 45702, 45, 979, 19866, 11, 321, 600, 658, 257, 1916, 295, 8213, 7914, 13], "temperature": 0.0, "avg_logprob": -0.15458633422851562, "compression_ratio": 1.8185654008438819, "no_speech_prob": 3.6119752166996477e-06}, {"id": 1167, "seek": 564064, "start": 5669.08, "end": 5670.08, "text": " What the hell?", "tokens": [708, 264, 4921, 30], "temperature": 0.0, "avg_logprob": -0.15458633422851562, "compression_ratio": 1.8185654008438819, "no_speech_prob": 3.6119752166996477e-06}, {"id": 1168, "seek": 567008, "start": 5670.08, "end": 5672.8, "text": " We've got to put the neural net into the mix.", "tokens": [492, 600, 658, 281, 829, 264, 18161, 2533, 666, 264, 2890, 13], "temperature": 0.0, "avg_logprob": -0.18161689991853674, "compression_ratio": 1.730593607305936, "no_speech_prob": 4.1573489397706e-06}, {"id": 1169, "seek": 567008, "start": 5672.8, "end": 5678.84, "text": " And this neural net is going to spit out a weight for every one of these things and we've", "tokens": [400, 341, 18161, 2533, 307, 516, 281, 22127, 484, 257, 3364, 337, 633, 472, 295, 613, 721, 293, 321, 600], "temperature": 0.0, "avg_logprob": -0.18161689991853674, "compression_ratio": 1.730593607305936, "no_speech_prob": 4.1573489397706e-06}, {"id": 1170, "seek": 567008, "start": 5678.84, "end": 5681.2, "text": " got to take the weighted average at every step.", "tokens": [658, 281, 747, 264, 32807, 4274, 412, 633, 1823, 13], "temperature": 0.0, "avg_logprob": -0.18161689991853674, "compression_ratio": 1.730593607305936, "no_speech_prob": 4.1573489397706e-06}, {"id": 1171, "seek": 567008, "start": 5681.2, "end": 5686.76, "text": " And it's just another set of parameters that we learn all at the same time.", "tokens": [400, 309, 311, 445, 1071, 992, 295, 9834, 300, 321, 1466, 439, 412, 264, 912, 565, 13], "temperature": 0.0, "avg_logprob": -0.18161689991853674, "compression_ratio": 1.730593607305936, "no_speech_prob": 4.1573489397706e-06}, {"id": 1172, "seek": 567008, "start": 5686.76, "end": 5690.2, "text": " And so that's called attention.", "tokens": [400, 370, 300, 311, 1219, 3202, 13], "temperature": 0.0, "avg_logprob": -0.18161689991853674, "compression_ratio": 1.730593607305936, "no_speech_prob": 4.1573489397706e-06}, {"id": 1173, "seek": 567008, "start": 5690.2, "end": 5696.08, "text": " So the idea is that once that attention has been learned, we can see this terrific demo", "tokens": [407, 264, 1558, 307, 300, 1564, 300, 3202, 575, 668, 3264, 11, 321, 393, 536, 341, 20899, 10723], "temperature": 0.0, "avg_logprob": -0.18161689991853674, "compression_ratio": 1.730593607305936, "no_speech_prob": 4.1573489397706e-06}, {"id": 1174, "seek": 569608, "start": 5696.08, "end": 5701.5599999999995, "text": " from Chris Oller and Sean Carter, each different word is going to take a weighted average.", "tokens": [490, 6688, 422, 4658, 293, 14839, 21622, 11, 1184, 819, 1349, 307, 516, 281, 747, 257, 32807, 4274, 13], "temperature": 0.0, "avg_logprob": -0.18266377135784956, "compression_ratio": 1.8662207357859533, "no_speech_prob": 1.1478614396764897e-05}, {"id": 1175, "seek": 569608, "start": 5701.5599999999995, "end": 5706.96, "text": " See how the weights are different depending on which word is being translated?", "tokens": [3008, 577, 264, 17443, 366, 819, 5413, 322, 597, 1349, 307, 885, 16805, 30], "temperature": 0.0, "avg_logprob": -0.18266377135784956, "compression_ratio": 1.8662207357859533, "no_speech_prob": 1.1478614396764897e-05}, {"id": 1176, "seek": 569608, "start": 5706.96, "end": 5710.36, "text": " And you can see how it's kind of figuring out the color, the deepness of the blue is", "tokens": [400, 291, 393, 536, 577, 309, 311, 733, 295, 15213, 484, 264, 2017, 11, 264, 2452, 1287, 295, 264, 3344, 307], "temperature": 0.0, "avg_logprob": -0.18266377135784956, "compression_ratio": 1.8662207357859533, "no_speech_prob": 1.1478614396764897e-05}, {"id": 1177, "seek": 569608, "start": 5710.36, "end": 5712.12, "text": " how much weight it's using.", "tokens": [577, 709, 3364, 309, 311, 1228, 13], "temperature": 0.0, "avg_logprob": -0.18266377135784956, "compression_ratio": 1.8662207357859533, "no_speech_prob": 1.1478614396764897e-05}, {"id": 1178, "seek": 569608, "start": 5712.12, "end": 5716.5199999999995, "text": " You can see that each word is basically, or here, which word are we translating from.", "tokens": [509, 393, 536, 300, 1184, 1349, 307, 1936, 11, 420, 510, 11, 597, 1349, 366, 321, 35030, 490, 13], "temperature": 0.0, "avg_logprob": -0.18266377135784956, "compression_ratio": 1.8662207357859533, "no_speech_prob": 1.1478614396764897e-05}, {"id": 1179, "seek": 569608, "start": 5716.5199999999995, "end": 5720.5599999999995, "text": " So when we say European, we need to know that both of these two parts are going to be influenced.", "tokens": [407, 562, 321, 584, 6473, 11, 321, 643, 281, 458, 300, 1293, 295, 613, 732, 3166, 366, 516, 281, 312, 15269, 13], "temperature": 0.0, "avg_logprob": -0.18266377135784956, "compression_ratio": 1.8662207357859533, "no_speech_prob": 1.1478614396764897e-05}, {"id": 1180, "seek": 569608, "start": 5720.5599999999995, "end": 5723.92, "text": " Or if we're doing economic, both of these three parts are going to be influenced, including", "tokens": [1610, 498, 321, 434, 884, 4836, 11, 1293, 295, 613, 1045, 3166, 366, 516, 281, 312, 15269, 11, 3009], "temperature": 0.0, "avg_logprob": -0.18266377135784956, "compression_ratio": 1.8662207357859533, "no_speech_prob": 1.1478614396764897e-05}, {"id": 1181, "seek": 572392, "start": 5723.92, "end": 5728.0, "text": " the gender of the definite article, and so forth.", "tokens": [264, 7898, 295, 264, 25131, 7222, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.20959275389370852, "compression_ratio": 1.566137566137566, "no_speech_prob": 1.4738860954821575e-05}, {"id": 1182, "seek": 572392, "start": 5728.0, "end": 5733.38, "text": " So check out this distil.hub article.", "tokens": [407, 1520, 484, 341, 1483, 388, 13, 71, 836, 7222, 13], "temperature": 0.0, "avg_logprob": -0.20959275389370852, "compression_ratio": 1.566137566137566, "no_speech_prob": 1.4738860954821575e-05}, {"id": 1183, "seek": 572392, "start": 5733.38, "end": 5737.88, "text": " These things are all like little interactive diagrams.", "tokens": [1981, 721, 366, 439, 411, 707, 15141, 36709, 13], "temperature": 0.0, "avg_logprob": -0.20959275389370852, "compression_ratio": 1.566137566137566, "no_speech_prob": 1.4738860954821575e-05}, {"id": 1184, "seek": 572392, "start": 5737.88, "end": 5744.36, "text": " It basically shows you how attention works and what the actual attention looks like in", "tokens": [467, 1936, 3110, 291, 577, 3202, 1985, 293, 437, 264, 3539, 3202, 1542, 411, 294], "temperature": 0.0, "avg_logprob": -0.20959275389370852, "compression_ratio": 1.566137566137566, "no_speech_prob": 1.4738860954821575e-05}, {"id": 1185, "seek": 572392, "start": 5744.36, "end": 5748.08, "text": " a trained translation model.", "tokens": [257, 8895, 12853, 2316, 13], "temperature": 0.0, "avg_logprob": -0.20959275389370852, "compression_ratio": 1.566137566137566, "no_speech_prob": 1.4738860954821575e-05}, {"id": 1186, "seek": 572392, "start": 5748.08, "end": 5753.18, "text": " So let's try and implement attention.", "tokens": [407, 718, 311, 853, 293, 4445, 3202, 13], "temperature": 0.0, "avg_logprob": -0.20959275389370852, "compression_ratio": 1.566137566137566, "no_speech_prob": 1.4738860954821575e-05}, {"id": 1187, "seek": 575318, "start": 5753.18, "end": 5768.400000000001, "text": " So with attention, it's basically, this is all identical, and the encoder is identical,", "tokens": [407, 365, 3202, 11, 309, 311, 1936, 11, 341, 307, 439, 14800, 11, 293, 264, 2058, 19866, 307, 14800, 11], "temperature": 0.0, "avg_logprob": -0.1833752330980803, "compression_ratio": 1.511111111111111, "no_speech_prob": 4.356864792498527e-06}, {"id": 1188, "seek": 575318, "start": 5768.400000000001, "end": 5772.280000000001, "text": " and all of this bit of the decoder is identical.", "tokens": [293, 439, 295, 341, 857, 295, 264, 979, 19866, 307, 14800, 13], "temperature": 0.0, "avg_logprob": -0.1833752330980803, "compression_ratio": 1.511111111111111, "no_speech_prob": 4.356864792498527e-06}, {"id": 1189, "seek": 577228, "start": 5772.28, "end": 5790.88, "text": " There's one difference, which is that we basically are going to take a weighted average.", "tokens": [821, 311, 472, 2649, 11, 597, 307, 300, 321, 1936, 366, 516, 281, 747, 257, 32807, 4274, 13], "temperature": 0.0, "avg_logprob": -0.14924598807719217, "compression_ratio": 1.65625, "no_speech_prob": 6.438999207603047e-06}, {"id": 1190, "seek": 577228, "start": 5790.88, "end": 5795.32, "text": " And the way that we're going to do the weighted average is we create a little neural net,", "tokens": [400, 264, 636, 300, 321, 434, 516, 281, 360, 264, 32807, 4274, 307, 321, 1884, 257, 707, 18161, 2533, 11], "temperature": 0.0, "avg_logprob": -0.14924598807719217, "compression_ratio": 1.65625, "no_speech_prob": 6.438999207603047e-06}, {"id": 1191, "seek": 577228, "start": 5795.32, "end": 5801.62, "text": " which we're going to see here and here, and then we use softmax, because of course the", "tokens": [597, 321, 434, 516, 281, 536, 510, 293, 510, 11, 293, 550, 321, 764, 2787, 41167, 11, 570, 295, 1164, 264], "temperature": 0.0, "avg_logprob": -0.14924598807719217, "compression_ratio": 1.65625, "no_speech_prob": 6.438999207603047e-06}, {"id": 1192, "seek": 580162, "start": 5801.62, "end": 5807.44, "text": " nice thing about softmax is that we want to ensure that all of the weights that we're", "tokens": [1481, 551, 466, 2787, 41167, 307, 300, 321, 528, 281, 5586, 300, 439, 295, 264, 17443, 300, 321, 434], "temperature": 0.0, "avg_logprob": -0.13433337679096297, "compression_ratio": 1.861244019138756, "no_speech_prob": 3.4465592761989683e-06}, {"id": 1193, "seek": 580162, "start": 5807.44, "end": 5813.74, "text": " using add up to 1, and we also kind of expect that one of those weights should probably", "tokens": [1228, 909, 493, 281, 502, 11, 293, 321, 611, 733, 295, 2066, 300, 472, 295, 729, 17443, 820, 1391], "temperature": 0.0, "avg_logprob": -0.13433337679096297, "compression_ratio": 1.861244019138756, "no_speech_prob": 3.4465592761989683e-06}, {"id": 1194, "seek": 580162, "start": 5813.74, "end": 5816.2, "text": " be quite a bit higher than the other ones.", "tokens": [312, 1596, 257, 857, 2946, 813, 264, 661, 2306, 13], "temperature": 0.0, "avg_logprob": -0.13433337679096297, "compression_ratio": 1.861244019138756, "no_speech_prob": 3.4465592761989683e-06}, {"id": 1195, "seek": 580162, "start": 5816.2, "end": 5823.4, "text": " And so softmax gives us the guarantee that they add up to 1, and because it's the a to", "tokens": [400, 370, 2787, 41167, 2709, 505, 264, 10815, 300, 436, 909, 493, 281, 502, 11, 293, 570, 309, 311, 264, 257, 281], "temperature": 0.0, "avg_logprob": -0.13433337679096297, "compression_ratio": 1.861244019138756, "no_speech_prob": 3.4465592761989683e-06}, {"id": 1196, "seek": 580162, "start": 5823.4, "end": 5829.5199999999995, "text": " the in it, it tends to encourage one of the weights to be higher than the other ones.", "tokens": [264, 294, 309, 11, 309, 12258, 281, 5373, 472, 295, 264, 17443, 281, 312, 2946, 813, 264, 661, 2306, 13], "temperature": 0.0, "avg_logprob": -0.13433337679096297, "compression_ratio": 1.861244019138756, "no_speech_prob": 3.4465592761989683e-06}, {"id": 1197, "seek": 582952, "start": 5829.52, "end": 5831.740000000001, "text": " So let's see how this works.", "tokens": [407, 718, 311, 536, 577, 341, 1985, 13], "temperature": 0.0, "avg_logprob": -0.12256454839939024, "compression_ratio": 1.8633540372670807, "no_speech_prob": 6.1441401157935616e-06}, {"id": 1198, "seek": 582952, "start": 5831.740000000001, "end": 5840.3, "text": " So what's going to happen is we're going to take the last layer's hidden state, and we're", "tokens": [407, 437, 311, 516, 281, 1051, 307, 321, 434, 516, 281, 747, 264, 1036, 4583, 311, 7633, 1785, 11, 293, 321, 434], "temperature": 0.0, "avg_logprob": -0.12256454839939024, "compression_ratio": 1.8633540372670807, "no_speech_prob": 6.1441401157935616e-06}, {"id": 1199, "seek": 582952, "start": 5840.3, "end": 5847.120000000001, "text": " going to stick it into a linear layer, and then we're going to stick it into a nonlinear", "tokens": [516, 281, 2897, 309, 666, 257, 8213, 4583, 11, 293, 550, 321, 434, 516, 281, 2897, 309, 666, 257, 2107, 28263], "temperature": 0.0, "avg_logprob": -0.12256454839939024, "compression_ratio": 1.8633540372670807, "no_speech_prob": 6.1441401157935616e-06}, {"id": 1200, "seek": 582952, "start": 5847.120000000001, "end": 5856.240000000001, "text": " activation, and then we're going to do matrix multiply, and so if you think about it, linear", "tokens": [24433, 11, 293, 550, 321, 434, 516, 281, 360, 8141, 12972, 11, 293, 370, 498, 291, 519, 466, 309, 11, 8213], "temperature": 0.0, "avg_logprob": -0.12256454839939024, "compression_ratio": 1.8633540372670807, "no_speech_prob": 6.1441401157935616e-06}, {"id": 1201, "seek": 585624, "start": 5856.24, "end": 5861.2, "text": " layer, nonlinear activation, matrix multiply, that's a neural net.", "tokens": [4583, 11, 2107, 28263, 24433, 11, 8141, 12972, 11, 300, 311, 257, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.15312032575731155, "compression_ratio": 1.558659217877095, "no_speech_prob": 9.276357673115854e-07}, {"id": 1202, "seek": 585624, "start": 5861.2, "end": 5865.679999999999, "text": " It's a neural net with one hidden layer.", "tokens": [467, 311, 257, 18161, 2533, 365, 472, 7633, 4583, 13], "temperature": 0.0, "avg_logprob": -0.15312032575731155, "compression_ratio": 1.558659217877095, "no_speech_prob": 9.276357673115854e-07}, {"id": 1203, "seek": 585624, "start": 5865.679999999999, "end": 5876.62, "text": " Stick it into a softmax, and then we can use that to weight our encoder outputs.", "tokens": [22744, 309, 666, 257, 2787, 41167, 11, 293, 550, 321, 393, 764, 300, 281, 3364, 527, 2058, 19866, 23930, 13], "temperature": 0.0, "avg_logprob": -0.15312032575731155, "compression_ratio": 1.558659217877095, "no_speech_prob": 9.276357673115854e-07}, {"id": 1204, "seek": 585624, "start": 5876.62, "end": 5882.28, "text": " So now, rather than just taking the last encoder output, we've got this is going to be the", "tokens": [407, 586, 11, 2831, 813, 445, 1940, 264, 1036, 2058, 19866, 5598, 11, 321, 600, 658, 341, 307, 516, 281, 312, 264], "temperature": 0.0, "avg_logprob": -0.15312032575731155, "compression_ratio": 1.558659217877095, "no_speech_prob": 9.276357673115854e-07}, {"id": 1205, "seek": 588228, "start": 5882.28, "end": 5888.84, "text": " whole tensor of all of the encoder outputs, which I just weight by this little neural", "tokens": [1379, 40863, 295, 439, 295, 264, 2058, 19866, 23930, 11, 597, 286, 445, 3364, 538, 341, 707, 18161], "temperature": 0.0, "avg_logprob": -0.18271957459996957, "compression_ratio": 1.4, "no_speech_prob": 4.8604356379655655e-06}, {"id": 1206, "seek": 588228, "start": 5888.84, "end": 5890.84, "text": " net that I created.", "tokens": [2533, 300, 286, 2942, 13], "temperature": 0.0, "avg_logprob": -0.18271957459996957, "compression_ratio": 1.4, "no_speech_prob": 4.8604356379655655e-06}, {"id": 1207, "seek": 588228, "start": 5890.84, "end": 5898.2, "text": " And that's basically it.", "tokens": [400, 300, 311, 1936, 309, 13], "temperature": 0.0, "avg_logprob": -0.18271957459996957, "compression_ratio": 1.4, "no_speech_prob": 4.8604356379655655e-06}, {"id": 1208, "seek": 588228, "start": 5898.2, "end": 5908.2, "text": " So what I'll do is I'll put on the wiki thread a couple of papers to check out.", "tokens": [407, 437, 286, 603, 360, 307, 286, 603, 829, 322, 264, 261, 9850, 7207, 257, 1916, 295, 10577, 281, 1520, 484, 13], "temperature": 0.0, "avg_logprob": -0.18271957459996957, "compression_ratio": 1.4, "no_speech_prob": 4.8604356379655655e-06}, {"id": 1209, "seek": 590820, "start": 5908.2, "end": 5913.72, "text": " There was basically one amazing paper that really originally introduced this idea of", "tokens": [821, 390, 1936, 472, 2243, 3035, 300, 534, 7993, 7268, 341, 1558, 295], "temperature": 0.0, "avg_logprob": -0.17684866034466287, "compression_ratio": 1.638743455497382, "no_speech_prob": 1.130064447352197e-05}, {"id": 1210, "seek": 590820, "start": 5913.72, "end": 5915.16, "text": " attention.", "tokens": [3202, 13], "temperature": 0.0, "avg_logprob": -0.17684866034466287, "compression_ratio": 1.638743455497382, "no_speech_prob": 1.130064447352197e-05}, {"id": 1211, "seek": 590820, "start": 5915.16, "end": 5920.54, "text": " And I say amazing because it actually introduced a couple of key things which have really changed", "tokens": [400, 286, 584, 2243, 570, 309, 767, 7268, 257, 1916, 295, 2141, 721, 597, 362, 534, 3105], "temperature": 0.0, "avg_logprob": -0.17684866034466287, "compression_ratio": 1.638743455497382, "no_speech_prob": 1.130064447352197e-05}, {"id": 1212, "seek": 590820, "start": 5920.54, "end": 5923.679999999999, "text": " how people work in this field.", "tokens": [577, 561, 589, 294, 341, 2519, 13], "temperature": 0.0, "avg_logprob": -0.17684866034466287, "compression_ratio": 1.638743455497382, "no_speech_prob": 1.130064447352197e-05}, {"id": 1213, "seek": 590820, "start": 5923.679999999999, "end": 5933.44, "text": " This area of attention has been used not just for text, but for things like reading text", "tokens": [639, 1859, 295, 3202, 575, 668, 1143, 406, 445, 337, 2487, 11, 457, 337, 721, 411, 3760, 2487], "temperature": 0.0, "avg_logprob": -0.17684866034466287, "compression_ratio": 1.638743455497382, "no_speech_prob": 1.130064447352197e-05}, {"id": 1214, "seek": 593344, "start": 5933.44, "end": 5939.32, "text": " out of pictures or doing various stuff with computer vision and stuff like that.", "tokens": [484, 295, 5242, 420, 884, 3683, 1507, 365, 3820, 5201, 293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.14179070388214499, "compression_ratio": 1.5972850678733033, "no_speech_prob": 5.4221618484007195e-06}, {"id": 1215, "seek": 593344, "start": 5939.32, "end": 5945.2, "text": " And then there's a second paper which actually Jeffrey Hinton was involved in called Grammar", "tokens": [400, 550, 456, 311, 257, 1150, 3035, 597, 767, 28721, 389, 12442, 390, 3288, 294, 1219, 22130, 6209], "temperature": 0.0, "avg_logprob": -0.14179070388214499, "compression_ratio": 1.5972850678733033, "no_speech_prob": 5.4221618484007195e-06}, {"id": 1216, "seek": 593344, "start": 5945.2, "end": 5950.799999999999, "text": " as a Foreign Language, which used this idea of RNNs with attention to basically try to", "tokens": [382, 257, 20430, 24445, 11, 597, 1143, 341, 1558, 295, 45702, 45, 82, 365, 3202, 281, 1936, 853, 281], "temperature": 0.0, "avg_logprob": -0.14179070388214499, "compression_ratio": 1.5972850678733033, "no_speech_prob": 5.4221618484007195e-06}, {"id": 1217, "seek": 593344, "start": 5950.799999999999, "end": 5961.5199999999995, "text": " replace rules-based grammar with an RNN which automatically basically tagged the grammatical", "tokens": [7406, 4474, 12, 6032, 22317, 365, 364, 45702, 45, 597, 6772, 1936, 40239, 264, 17570, 267, 804], "temperature": 0.0, "avg_logprob": -0.14179070388214499, "compression_ratio": 1.5972850678733033, "no_speech_prob": 5.4221618484007195e-06}, {"id": 1218, "seek": 596152, "start": 5961.52, "end": 5966.38, "text": " each word based on this grammar and turned out to do it better than any rules-based system.", "tokens": [1184, 1349, 2361, 322, 341, 22317, 293, 3574, 484, 281, 360, 309, 1101, 813, 604, 4474, 12, 6032, 1185, 13], "temperature": 0.0, "avg_logprob": -0.21051857497665907, "compression_ratio": 1.6652173913043478, "no_speech_prob": 1.618696296645794e-05}, {"id": 1219, "seek": 596152, "start": 5966.38, "end": 5969.56, "text": " Which today actually kind of seems obvious.", "tokens": [3013, 965, 767, 733, 295, 2544, 6322, 13], "temperature": 0.0, "avg_logprob": -0.21051857497665907, "compression_ratio": 1.6652173913043478, "no_speech_prob": 1.618696296645794e-05}, {"id": 1220, "seek": 596152, "start": 5969.56, "end": 5975.92, "text": " I think we're now used to the idea that neural nets do lots of this stuff better than rules-based", "tokens": [286, 519, 321, 434, 586, 1143, 281, 264, 1558, 300, 18161, 36170, 360, 3195, 295, 341, 1507, 1101, 813, 4474, 12, 6032], "temperature": 0.0, "avg_logprob": -0.21051857497665907, "compression_ratio": 1.6652173913043478, "no_speech_prob": 1.618696296645794e-05}, {"id": 1221, "seek": 596152, "start": 5975.92, "end": 5979.84, "text": " systems, but at the time it was considered really surprising.", "tokens": [3652, 11, 457, 412, 264, 565, 309, 390, 4888, 534, 8830, 13], "temperature": 0.0, "avg_logprob": -0.21051857497665907, "compression_ratio": 1.6652173913043478, "no_speech_prob": 1.618696296645794e-05}, {"id": 1222, "seek": 596152, "start": 5979.84, "end": 5985.4400000000005, "text": " One nice thing is that their summary of how attention works is really nice and concise.", "tokens": [1485, 1481, 551, 307, 300, 641, 12691, 295, 577, 3202, 1985, 307, 534, 1481, 293, 44882, 13], "temperature": 0.0, "avg_logprob": -0.21051857497665907, "compression_ratio": 1.6652173913043478, "no_speech_prob": 1.618696296645794e-05}, {"id": 1223, "seek": 598544, "start": 5985.44, "end": 6012.08, "text": " Let's go back and look at our original encoder.", "tokens": [961, 311, 352, 646, 293, 574, 412, 527, 3380, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.22471373421805246, "compression_ratio": 1.0128205128205128, "no_speech_prob": 1.3211457371653523e-05}, {"id": 1224, "seek": 598544, "start": 6012.08, "end": 6015.0, "text": " So an RNN spits out two things.", "tokens": [407, 364, 45702, 45, 637, 1208, 484, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.22471373421805246, "compression_ratio": 1.0128205128205128, "no_speech_prob": 1.3211457371653523e-05}, {"id": 1225, "seek": 601500, "start": 6015.0, "end": 6023.76, "text": " It spits out a list of the state after every time step and it also tells you the state", "tokens": [467, 637, 1208, 484, 257, 1329, 295, 264, 1785, 934, 633, 565, 1823, 293, 309, 611, 5112, 291, 264, 1785], "temperature": 0.0, "avg_logprob": -0.12517063706009476, "compression_ratio": 1.6610169491525424, "no_speech_prob": 9.080349627765827e-06}, {"id": 1226, "seek": 601500, "start": 6023.76, "end": 6025.76, "text": " at the last time step.", "tokens": [412, 264, 1036, 565, 1823, 13], "temperature": 0.0, "avg_logprob": -0.12517063706009476, "compression_ratio": 1.6610169491525424, "no_speech_prob": 9.080349627765827e-06}, {"id": 1227, "seek": 601500, "start": 6025.76, "end": 6034.72, "text": " And we used the state at the last time step to create the input state for our decoder,", "tokens": [400, 321, 1143, 264, 1785, 412, 264, 1036, 565, 1823, 281, 1884, 264, 4846, 1785, 337, 527, 979, 19866, 11], "temperature": 0.0, "avg_logprob": -0.12517063706009476, "compression_ratio": 1.6610169491525424, "no_speech_prob": 9.080349627765827e-06}, {"id": 1228, "seek": 603472, "start": 6034.72, "end": 6045.92, "text": " which is what we see here, one vector.", "tokens": [597, 307, 437, 321, 536, 510, 11, 472, 8062, 13], "temperature": 0.0, "avg_logprob": -0.11132461923948477, "compression_ratio": 1.6346153846153846, "no_speech_prob": 1.3496959354597493e-06}, {"id": 1229, "seek": 603472, "start": 6045.92, "end": 6050.52, "text": " But we know that it's actually creating a vector at every time step, so wouldn't it", "tokens": [583, 321, 458, 300, 309, 311, 767, 4084, 257, 8062, 412, 633, 565, 1823, 11, 370, 2759, 380, 309], "temperature": 0.0, "avg_logprob": -0.11132461923948477, "compression_ratio": 1.6346153846153846, "no_speech_prob": 1.3496959354597493e-06}, {"id": 1230, "seek": 603472, "start": 6050.52, "end": 6053.4800000000005, "text": " be nice to use them all?", "tokens": [312, 1481, 281, 764, 552, 439, 30], "temperature": 0.0, "avg_logprob": -0.11132461923948477, "compression_ratio": 1.6346153846153846, "no_speech_prob": 1.3496959354597493e-06}, {"id": 1231, "seek": 603472, "start": 6053.4800000000005, "end": 6059.6, "text": " But wouldn't it be nice to use the ones that are most relevant to translating the word", "tokens": [583, 2759, 380, 309, 312, 1481, 281, 764, 264, 2306, 300, 366, 881, 7340, 281, 35030, 264, 1349], "temperature": 0.0, "avg_logprob": -0.11132461923948477, "compression_ratio": 1.6346153846153846, "no_speech_prob": 1.3496959354597493e-06}, {"id": 1232, "seek": 603472, "start": 6059.6, "end": 6062.72, "text": " I'm translating now?", "tokens": [286, 478, 35030, 586, 30], "temperature": 0.0, "avg_logprob": -0.11132461923948477, "compression_ratio": 1.6346153846153846, "no_speech_prob": 1.3496959354597493e-06}, {"id": 1233, "seek": 606272, "start": 6062.72, "end": 6066.4800000000005, "text": " So wouldn't it be nice to be able to take a weighted average of the hidden state at", "tokens": [407, 2759, 380, 309, 312, 1481, 281, 312, 1075, 281, 747, 257, 32807, 4274, 295, 264, 7633, 1785, 412], "temperature": 0.0, "avg_logprob": -0.214646245097066, "compression_ratio": 1.5903083700440528, "no_speech_prob": 5.771884843852604e-06}, {"id": 1234, "seek": 606272, "start": 6066.4800000000005, "end": 6072.96, "text": " each time step, weighted by whatever is the appropriate weight right now, which for example", "tokens": [1184, 565, 1823, 11, 32807, 538, 2035, 307, 264, 6854, 3364, 558, 586, 11, 597, 337, 1365], "temperature": 0.0, "avg_logprob": -0.214646245097066, "compression_ratio": 1.5903083700440528, "no_speech_prob": 5.771884843852604e-06}, {"id": 1235, "seek": 606272, "start": 6072.96, "end": 6079.8, "text": " in this case, LBTA would definitely be time step number 2, because that's the word I'm", "tokens": [294, 341, 1389, 11, 441, 33, 8241, 576, 2138, 312, 565, 1823, 1230, 568, 11, 570, 300, 311, 264, 1349, 286, 478], "temperature": 0.0, "avg_logprob": -0.214646245097066, "compression_ratio": 1.5903083700440528, "no_speech_prob": 5.771884843852604e-06}, {"id": 1236, "seek": 606272, "start": 6079.8, "end": 6081.76, "text": " translating.", "tokens": [35030, 13], "temperature": 0.0, "avg_logprob": -0.214646245097066, "compression_ratio": 1.5903083700440528, "no_speech_prob": 5.771884843852604e-06}, {"id": 1237, "seek": 606272, "start": 6081.76, "end": 6090.68, "text": " So how do we get a list of weights that is suitable for the word we're training right", "tokens": [407, 577, 360, 321, 483, 257, 1329, 295, 17443, 300, 307, 12873, 337, 264, 1349, 321, 434, 3097, 558], "temperature": 0.0, "avg_logprob": -0.214646245097066, "compression_ratio": 1.5903083700440528, "no_speech_prob": 5.771884843852604e-06}, {"id": 1238, "seek": 609068, "start": 6090.68, "end": 6097.08, "text": " now, or the answer is by training a neural net to figure out the list of weights.", "tokens": [586, 11, 420, 264, 1867, 307, 538, 3097, 257, 18161, 2533, 281, 2573, 484, 264, 1329, 295, 17443, 13], "temperature": 0.0, "avg_logprob": -0.13005022100500158, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.507582500285935e-06}, {"id": 1239, "seek": 609068, "start": 6097.08, "end": 6102.16, "text": " And so any time we want to figure out how to train a little neural net that does any", "tokens": [400, 370, 604, 565, 321, 528, 281, 2573, 484, 577, 281, 3847, 257, 707, 18161, 2533, 300, 775, 604], "temperature": 0.0, "avg_logprob": -0.13005022100500158, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.507582500285935e-06}, {"id": 1240, "seek": 609068, "start": 6102.16, "end": 6108.92, "text": " task, the easiest way normally always to do that is to include it in your module and train", "tokens": [5633, 11, 264, 12889, 636, 5646, 1009, 281, 360, 300, 307, 281, 4090, 309, 294, 428, 10088, 293, 3847], "temperature": 0.0, "avg_logprob": -0.13005022100500158, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.507582500285935e-06}, {"id": 1241, "seek": 609068, "start": 6108.92, "end": 6113.52, "text": " it in line with everything else.", "tokens": [309, 294, 1622, 365, 1203, 1646, 13], "temperature": 0.0, "avg_logprob": -0.13005022100500158, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.507582500285935e-06}, {"id": 1242, "seek": 611352, "start": 6113.52, "end": 6125.64, "text": " The minimal possible neural net is something that contains 2 layers and 1 non-linear activation", "tokens": [440, 13206, 1944, 18161, 2533, 307, 746, 300, 8306, 568, 7914, 293, 502, 2107, 12, 28263, 24433], "temperature": 0.0, "avg_logprob": -0.1984120583047672, "compression_ratio": 1.4485294117647058, "no_speech_prob": 1.6028066056605894e-06}, {"id": 1243, "seek": 611352, "start": 6125.64, "end": 6129.68, "text": " function.", "tokens": [2445, 13], "temperature": 0.0, "avg_logprob": -0.1984120583047672, "compression_ratio": 1.4485294117647058, "no_speech_prob": 1.6028066056605894e-06}, {"id": 1244, "seek": 611352, "start": 6129.68, "end": 6142.8, "text": " So here is one linear layer, and in fact instead of a linear layer, we can even just grab a", "tokens": [407, 510, 307, 472, 8213, 4583, 11, 293, 294, 1186, 2602, 295, 257, 8213, 4583, 11, 321, 393, 754, 445, 4444, 257], "temperature": 0.0, "avg_logprob": -0.1984120583047672, "compression_ratio": 1.4485294117647058, "no_speech_prob": 1.6028066056605894e-06}, {"id": 1245, "seek": 614280, "start": 6142.8, "end": 6149.4400000000005, "text": " random matrix, if we don't care about bias.", "tokens": [4974, 8141, 11, 498, 321, 500, 380, 1127, 466, 12577, 13], "temperature": 0.0, "avg_logprob": -0.1470804335195807, "compression_ratio": 1.6127167630057804, "no_speech_prob": 9.080380550585687e-06}, {"id": 1246, "seek": 614280, "start": 6149.4400000000005, "end": 6156.04, "text": " And so here's a random matrix, it's just a random tensor wrapped up in a parameter.", "tokens": [400, 370, 510, 311, 257, 4974, 8141, 11, 309, 311, 445, 257, 4974, 40863, 14226, 493, 294, 257, 13075, 13], "temperature": 0.0, "avg_logprob": -0.1470804335195807, "compression_ratio": 1.6127167630057804, "no_speech_prob": 9.080380550585687e-06}, {"id": 1247, "seek": 614280, "start": 6156.04, "end": 6162.64, "text": " A parameter remember is just a PyTorch variable, it's like identical to a variable, but it", "tokens": [316, 13075, 1604, 307, 445, 257, 9953, 51, 284, 339, 7006, 11, 309, 311, 411, 14800, 281, 257, 7006, 11, 457, 309], "temperature": 0.0, "avg_logprob": -0.1470804335195807, "compression_ratio": 1.6127167630057804, "no_speech_prob": 9.080380550585687e-06}, {"id": 1248, "seek": 614280, "start": 6162.64, "end": 6168.46, "text": " just tells PyTorch I want you to learn the weights for this.", "tokens": [445, 5112, 9953, 51, 284, 339, 286, 528, 291, 281, 1466, 264, 17443, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.1470804335195807, "compression_ratio": 1.6127167630057804, "no_speech_prob": 9.080380550585687e-06}, {"id": 1249, "seek": 616846, "start": 6168.46, "end": 6176.8, "text": " So here we've got a linear layer, here we've got a random matrix, and so here at this point", "tokens": [407, 510, 321, 600, 658, 257, 8213, 4583, 11, 510, 321, 600, 658, 257, 4974, 8141, 11, 293, 370, 510, 412, 341, 935], "temperature": 0.0, "avg_logprob": -0.0916833541762661, "compression_ratio": 1.75, "no_speech_prob": 6.643406322837109e-06}, {"id": 1250, "seek": 616846, "start": 6176.8, "end": 6186.04, "text": " where we start out our decoder, let's take that final, let's take the current hidden", "tokens": [689, 321, 722, 484, 527, 979, 19866, 11, 718, 311, 747, 300, 2572, 11, 718, 311, 747, 264, 2190, 7633], "temperature": 0.0, "avg_logprob": -0.0916833541762661, "compression_ratio": 1.75, "no_speech_prob": 6.643406322837109e-06}, {"id": 1251, "seek": 616846, "start": 6186.04, "end": 6195.82, "text": " state of the decoder, put that into a linear layer, because what's the information we use", "tokens": [1785, 295, 264, 979, 19866, 11, 829, 300, 666, 257, 8213, 4583, 11, 570, 437, 311, 264, 1589, 321, 764], "temperature": 0.0, "avg_logprob": -0.0916833541762661, "compression_ratio": 1.75, "no_speech_prob": 6.643406322837109e-06}, {"id": 1252, "seek": 619582, "start": 6195.82, "end": 6200.679999999999, "text": " to decide what words we should focus on next?", "tokens": [281, 4536, 437, 2283, 321, 820, 1879, 322, 958, 30], "temperature": 0.0, "avg_logprob": -0.13419708045753273, "compression_ratio": 1.7022222222222223, "no_speech_prob": 1.670132769504562e-05}, {"id": 1253, "seek": 619582, "start": 6200.679999999999, "end": 6205.04, "text": " The only information we have to go on is what the decoder's hidden state is now.", "tokens": [440, 787, 1589, 321, 362, 281, 352, 322, 307, 437, 264, 979, 19866, 311, 7633, 1785, 307, 586, 13], "temperature": 0.0, "avg_logprob": -0.13419708045753273, "compression_ratio": 1.7022222222222223, "no_speech_prob": 1.670132769504562e-05}, {"id": 1254, "seek": 619582, "start": 6205.04, "end": 6214.42, "text": " So let's grab that, put it into the linear layer, put it through a non-linearity, put", "tokens": [407, 718, 311, 4444, 300, 11, 829, 309, 666, 264, 8213, 4583, 11, 829, 309, 807, 257, 2107, 12, 1889, 17409, 11, 829], "temperature": 0.0, "avg_logprob": -0.13419708045753273, "compression_ratio": 1.7022222222222223, "no_speech_prob": 1.670132769504562e-05}, {"id": 1255, "seek": 619582, "start": 6214.42, "end": 6218.679999999999, "text": " it through one more non-linear layer, this one actually doesn't have a bias in it, so", "tokens": [309, 807, 472, 544, 2107, 12, 28263, 4583, 11, 341, 472, 767, 1177, 380, 362, 257, 12577, 294, 309, 11, 370], "temperature": 0.0, "avg_logprob": -0.13419708045753273, "compression_ratio": 1.7022222222222223, "no_speech_prob": 1.670132769504562e-05}, {"id": 1256, "seek": 619582, "start": 6218.679999999999, "end": 6224.48, "text": " it's actually just a matrix-multiply, put that into a softmax, and that's it, that's", "tokens": [309, 311, 767, 445, 257, 8141, 12, 76, 723, 647, 356, 11, 829, 300, 666, 257, 2787, 41167, 11, 293, 300, 311, 309, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.13419708045753273, "compression_ratio": 1.7022222222222223, "no_speech_prob": 1.670132769504562e-05}, {"id": 1257, "seek": 622448, "start": 6224.48, "end": 6228.0599999999995, "text": " a little neural net.", "tokens": [257, 707, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.1369891847882952, "compression_ratio": 1.7376237623762376, "no_speech_prob": 5.014723683416378e-06}, {"id": 1258, "seek": 622448, "start": 6228.0599999999995, "end": 6233.879999999999, "text": " It doesn't do anything, it's just a neural net, no neural nets do anything, they're", "tokens": [467, 1177, 380, 360, 1340, 11, 309, 311, 445, 257, 18161, 2533, 11, 572, 18161, 36170, 360, 1340, 11, 436, 434], "temperature": 0.0, "avg_logprob": -0.1369891847882952, "compression_ratio": 1.7376237623762376, "no_speech_prob": 5.014723683416378e-06}, {"id": 1259, "seek": 622448, "start": 6233.879999999999, "end": 6238.32, "text": " just linear layers with non-linear activations with random weights.", "tokens": [445, 8213, 7914, 365, 2107, 12, 28263, 2430, 763, 365, 4974, 17443, 13], "temperature": 0.0, "avg_logprob": -0.1369891847882952, "compression_ratio": 1.7376237623762376, "no_speech_prob": 5.014723683416378e-06}, {"id": 1260, "seek": 622448, "start": 6238.32, "end": 6244.5599999999995, "text": " But it starts to do something if we give it a job to do, and in this case the job we give", "tokens": [583, 309, 3719, 281, 360, 746, 498, 321, 976, 309, 257, 1691, 281, 360, 11, 293, 294, 341, 1389, 264, 1691, 321, 976], "temperature": 0.0, "avg_logprob": -0.1369891847882952, "compression_ratio": 1.7376237623762376, "no_speech_prob": 5.014723683416378e-06}, {"id": 1261, "seek": 622448, "start": 6244.5599999999995, "end": 6252.719999999999, "text": " it to do is to say don't just take the final state, but now let's use all of the encoder", "tokens": [309, 281, 360, 307, 281, 584, 500, 380, 445, 747, 264, 2572, 1785, 11, 457, 586, 718, 311, 764, 439, 295, 264, 2058, 19866], "temperature": 0.0, "avg_logprob": -0.1369891847882952, "compression_ratio": 1.7376237623762376, "no_speech_prob": 5.014723683416378e-06}, {"id": 1262, "seek": 625272, "start": 6252.72, "end": 6262.320000000001, "text": " states, and let's take all of them and multiply them by the output of that little neural net.", "tokens": [4368, 11, 293, 718, 311, 747, 439, 295, 552, 293, 12972, 552, 538, 264, 5598, 295, 300, 707, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.11589482556218686, "compression_ratio": 1.7799043062200957, "no_speech_prob": 2.2959111447562464e-06}, {"id": 1263, "seek": 625272, "start": 6262.320000000001, "end": 6268.68, "text": " And so given that the things in this little neural net are learnable weights, hopefully", "tokens": [400, 370, 2212, 300, 264, 721, 294, 341, 707, 18161, 2533, 366, 1466, 712, 17443, 11, 4696], "temperature": 0.0, "avg_logprob": -0.11589482556218686, "compression_ratio": 1.7799043062200957, "no_speech_prob": 2.2959111447562464e-06}, {"id": 1264, "seek": 625272, "start": 6268.68, "end": 6274.2, "text": " it's going to learn to weight those encoder outputs, those encoder hidden states by something", "tokens": [309, 311, 516, 281, 1466, 281, 3364, 729, 2058, 19866, 23930, 11, 729, 2058, 19866, 7633, 4368, 538, 746], "temperature": 0.0, "avg_logprob": -0.11589482556218686, "compression_ratio": 1.7799043062200957, "no_speech_prob": 2.2959111447562464e-06}, {"id": 1265, "seek": 625272, "start": 6274.2, "end": 6275.2, "text": " useful.", "tokens": [4420, 13], "temperature": 0.0, "avg_logprob": -0.11589482556218686, "compression_ratio": 1.7799043062200957, "no_speech_prob": 2.2959111447562464e-06}, {"id": 1266, "seek": 625272, "start": 6275.2, "end": 6281.52, "text": " That's all a neural net ever does, is we give it some random weights to start with and a", "tokens": [663, 311, 439, 257, 18161, 2533, 1562, 775, 11, 307, 321, 976, 309, 512, 4974, 17443, 281, 722, 365, 293, 257], "temperature": 0.0, "avg_logprob": -0.11589482556218686, "compression_ratio": 1.7799043062200957, "no_speech_prob": 2.2959111447562464e-06}, {"id": 1267, "seek": 628152, "start": 6281.52, "end": 6285.52, "text": " job to do and hope that it learns to do the job.", "tokens": [1691, 281, 360, 293, 1454, 300, 309, 27152, 281, 360, 264, 1691, 13], "temperature": 0.0, "avg_logprob": -0.20065731788749125, "compression_ratio": 1.4415584415584415, "no_speech_prob": 1.3631250112666748e-05}, {"id": 1268, "seek": 628152, "start": 6285.52, "end": 6289.26, "text": " And it turns out that it does.", "tokens": [400, 309, 4523, 484, 300, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.20065731788749125, "compression_ratio": 1.4415584415584415, "no_speech_prob": 1.3631250112666748e-05}, {"id": 1269, "seek": 628152, "start": 6289.26, "end": 6294.72, "text": " So everything else in here is identical to what it was before.", "tokens": [407, 1203, 1646, 294, 510, 307, 14800, 281, 437, 309, 390, 949, 13], "temperature": 0.0, "avg_logprob": -0.20065731788749125, "compression_ratio": 1.4415584415584415, "no_speech_prob": 1.3631250112666748e-05}, {"id": 1270, "seek": 628152, "start": 6294.72, "end": 6298.92, "text": " We've got teacher forcing, it's not bidirectional.", "tokens": [492, 600, 658, 5027, 19030, 11, 309, 311, 406, 12957, 621, 41048, 13], "temperature": 0.0, "avg_logprob": -0.20065731788749125, "compression_ratio": 1.4415584415584415, "no_speech_prob": 1.3631250112666748e-05}, {"id": 1271, "seek": 628152, "start": 6298.92, "end": 6310.96, "text": " So we can see how this goes.", "tokens": [407, 321, 393, 536, 577, 341, 1709, 13], "temperature": 0.0, "avg_logprob": -0.20065731788749125, "compression_ratio": 1.4415584415584415, "no_speech_prob": 1.3631250112666748e-05}, {"id": 1272, "seek": 631096, "start": 6310.96, "end": 6318.96, "text": " Teacher forcing had 3.49, and so now we've got nearly exactly the same thing, but we've", "tokens": [19745, 19030, 632, 805, 13, 14938, 11, 293, 370, 586, 321, 600, 658, 6217, 2293, 264, 912, 551, 11, 457, 321, 600], "temperature": 0.0, "avg_logprob": -0.19272858583474461, "compression_ratio": 1.4477611940298507, "no_speech_prob": 6.339175797620555e-06}, {"id": 1273, "seek": 631096, "start": 6318.96, "end": 6324.96, "text": " got this little minimal neural net figuring out what weightings to give our inputs.", "tokens": [658, 341, 707, 13206, 18161, 2533, 15213, 484, 437, 3364, 1109, 281, 976, 527, 15743, 13], "temperature": 0.0, "avg_logprob": -0.19272858583474461, "compression_ratio": 1.4477611940298507, "no_speech_prob": 6.339175797620555e-06}, {"id": 1274, "seek": 631096, "start": 6324.96, "end": 6330.12, "text": " Oh wow, now it's down to 3.37.", "tokens": [876, 6076, 11, 586, 309, 311, 760, 281, 805, 13, 12851, 13], "temperature": 0.0, "avg_logprob": -0.19272858583474461, "compression_ratio": 1.4477611940298507, "no_speech_prob": 6.339175797620555e-06}, {"id": 1275, "seek": 631096, "start": 6330.12, "end": 6337.56, "text": " Remember these things are logs, so e to the power of this is quite a significant change.", "tokens": [5459, 613, 721, 366, 20820, 11, 370, 308, 281, 264, 1347, 295, 341, 307, 1596, 257, 4776, 1319, 13], "temperature": 0.0, "avg_logprob": -0.19272858583474461, "compression_ratio": 1.4477611940298507, "no_speech_prob": 6.339175797620555e-06}, {"id": 1276, "seek": 633756, "start": 6337.56, "end": 6345.360000000001, "text": " So 3.37, let's try it out.", "tokens": [407, 805, 13, 12851, 11, 718, 311, 853, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.17876730526194853, "compression_ratio": 1.4270833333333333, "no_speech_prob": 4.289322532713413e-06}, {"id": 1277, "seek": 633756, "start": 6345.360000000001, "end": 6346.360000000001, "text": " Not bad, right?", "tokens": [1726, 1578, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17876730526194853, "compression_ratio": 1.4270833333333333, "no_speech_prob": 4.289322532713413e-06}, {"id": 1278, "seek": 633756, "start": 6346.360000000001, "end": 6347.360000000001, "text": " Where are they located?", "tokens": [2305, 366, 436, 6870, 30], "temperature": 0.0, "avg_logprob": -0.17876730526194853, "compression_ratio": 1.4270833333333333, "no_speech_prob": 4.289322532713413e-06}, {"id": 1279, "seek": 633756, "start": 6347.360000000001, "end": 6348.84, "text": " What are their skills?", "tokens": [708, 366, 641, 3942, 30], "temperature": 0.0, "avg_logprob": -0.17876730526194853, "compression_ratio": 1.4270833333333333, "no_speech_prob": 4.289322532713413e-06}, {"id": 1280, "seek": 633756, "start": 6348.84, "end": 6351.240000000001, "text": " What do you do?", "tokens": [708, 360, 291, 360, 30], "temperature": 0.0, "avg_logprob": -0.17876730526194853, "compression_ratio": 1.4270833333333333, "no_speech_prob": 4.289322532713413e-06}, {"id": 1281, "seek": 633756, "start": 6351.240000000001, "end": 6362.04, "text": " They're still not perfect, why or why not, but quite a few of them are correct.", "tokens": [814, 434, 920, 406, 2176, 11, 983, 420, 983, 406, 11, 457, 1596, 257, 1326, 295, 552, 366, 3006, 13], "temperature": 0.0, "avg_logprob": -0.17876730526194853, "compression_ratio": 1.4270833333333333, "no_speech_prob": 4.289322532713413e-06}, {"id": 1282, "seek": 633756, "start": 6362.04, "end": 6366.68, "text": " And again, considering that we're asking it to learn about the very idea of language for", "tokens": [400, 797, 11, 8079, 300, 321, 434, 3365, 309, 281, 1466, 466, 264, 588, 1558, 295, 2856, 337], "temperature": 0.0, "avg_logprob": -0.17876730526194853, "compression_ratio": 1.4270833333333333, "no_speech_prob": 4.289322532713413e-06}, {"id": 1283, "seek": 636668, "start": 6366.68, "end": 6371.84, "text": " two different languages and how to translate them between the two, and grammar and vocabulary,", "tokens": [732, 819, 8650, 293, 577, 281, 13799, 552, 1296, 264, 732, 11, 293, 22317, 293, 19864, 11], "temperature": 0.0, "avg_logprob": -0.27994385219755624, "compression_ratio": 1.4465116279069767, "no_speech_prob": 1.983243419090286e-05}, {"id": 1284, "seek": 636668, "start": 6371.84, "end": 6377.52, "text": " and we only have 50,000 sentences and a lot of the words only appear once, I would say", "tokens": [293, 321, 787, 362, 2625, 11, 1360, 16579, 293, 257, 688, 295, 264, 2283, 787, 4204, 1564, 11, 286, 576, 584], "temperature": 0.0, "avg_logprob": -0.27994385219755624, "compression_ratio": 1.4465116279069767, "no_speech_prob": 1.983243419090286e-05}, {"id": 1285, "seek": 636668, "start": 6377.52, "end": 6380.92, "text": " this is actually pretty amazing.", "tokens": [341, 307, 767, 1238, 2243, 13], "temperature": 0.0, "avg_logprob": -0.27994385219755624, "compression_ratio": 1.4465116279069767, "no_speech_prob": 1.983243419090286e-05}, {"id": 1286, "seek": 636668, "start": 6380.92, "end": 6384.400000000001, "text": " Yes, Jeanette?", "tokens": [1079, 11, 13854, 3007, 30], "temperature": 0.0, "avg_logprob": -0.27994385219755624, "compression_ratio": 1.4465116279069767, "no_speech_prob": 1.983243419090286e-05}, {"id": 1287, "seek": 636668, "start": 6384.400000000001, "end": 6393.0, "text": " Why do we use tanh instead of relu for attention mininet?", "tokens": [1545, 360, 321, 764, 7603, 71, 2602, 295, 1039, 84, 337, 3202, 923, 21370, 30], "temperature": 0.0, "avg_logprob": -0.27994385219755624, "compression_ratio": 1.4465116279069767, "no_speech_prob": 1.983243419090286e-05}, {"id": 1288, "seek": 636668, "start": 6393.0, "end": 6394.0, "text": " I don't quite remember.", "tokens": [286, 500, 380, 1596, 1604, 13], "temperature": 0.0, "avg_logprob": -0.27994385219755624, "compression_ratio": 1.4465116279069767, "no_speech_prob": 1.983243419090286e-05}, {"id": 1289, "seek": 639400, "start": 6394.0, "end": 6397.2, "text": " It's been a while since I looked at it.", "tokens": [467, 311, 668, 257, 1339, 1670, 286, 2956, 412, 309, 13], "temperature": 0.0, "avg_logprob": -0.13709860110501632, "compression_ratio": 1.570281124497992, "no_speech_prob": 3.76343923562672e-05}, {"id": 1290, "seek": 639400, "start": 6397.2, "end": 6400.68, "text": " You should totally try using relu and see how it goes.", "tokens": [509, 820, 3879, 853, 1228, 1039, 84, 293, 536, 577, 309, 1709, 13], "temperature": 0.0, "avg_logprob": -0.13709860110501632, "compression_ratio": 1.570281124497992, "no_speech_prob": 3.76343923562672e-05}, {"id": 1291, "seek": 639400, "start": 6400.68, "end": 6408.28, "text": " Obviously tanh, the key difference is that it can go in each direction and it's limited", "tokens": [7580, 7603, 71, 11, 264, 2141, 2649, 307, 300, 309, 393, 352, 294, 1184, 3513, 293, 309, 311, 5567], "temperature": 0.0, "avg_logprob": -0.13709860110501632, "compression_ratio": 1.570281124497992, "no_speech_prob": 3.76343923562672e-05}, {"id": 1292, "seek": 639400, "start": 6408.28, "end": 6411.08, "text": " both at the top and the bottom.", "tokens": [1293, 412, 264, 1192, 293, 264, 2767, 13], "temperature": 0.0, "avg_logprob": -0.13709860110501632, "compression_ratio": 1.570281124497992, "no_speech_prob": 3.76343923562672e-05}, {"id": 1293, "seek": 639400, "start": 6411.08, "end": 6417.72, "text": " I know very often, like for the gates inside RNNs and LSTMs and GRUs, tanh often works", "tokens": [286, 458, 588, 2049, 11, 411, 337, 264, 19792, 1854, 45702, 45, 82, 293, 441, 6840, 26386, 293, 10903, 29211, 11, 7603, 71, 2049, 1985], "temperature": 0.0, "avg_logprob": -0.13709860110501632, "compression_ratio": 1.570281124497992, "no_speech_prob": 3.76343923562672e-05}, {"id": 1294, "seek": 639400, "start": 6417.72, "end": 6422.74, "text": " out better, but it's been about a year since I actually looked at that specific question,", "tokens": [484, 1101, 11, 457, 309, 311, 668, 466, 257, 1064, 1670, 286, 767, 2956, 412, 300, 2685, 1168, 11], "temperature": 0.0, "avg_logprob": -0.13709860110501632, "compression_ratio": 1.570281124497992, "no_speech_prob": 3.76343923562672e-05}, {"id": 1295, "seek": 642274, "start": 6422.74, "end": 6425.139999999999, "text": " so I'll look at it during the week.", "tokens": [370, 286, 603, 574, 412, 309, 1830, 264, 1243, 13], "temperature": 0.0, "avg_logprob": -0.16172207726372612, "compression_ratio": 1.6804979253112033, "no_speech_prob": 2.3551414415123872e-05}, {"id": 1296, "seek": 642274, "start": 6425.139999999999, "end": 6429.12, "text": " The short answer is you should try a different activation function and see if you can get", "tokens": [440, 2099, 1867, 307, 291, 820, 853, 257, 819, 24433, 2445, 293, 536, 498, 291, 393, 483], "temperature": 0.0, "avg_logprob": -0.16172207726372612, "compression_ratio": 1.6804979253112033, "no_speech_prob": 2.3551414415123872e-05}, {"id": 1297, "seek": 642274, "start": 6429.12, "end": 6430.12, "text": " a better result.", "tokens": [257, 1101, 1874, 13], "temperature": 0.0, "avg_logprob": -0.16172207726372612, "compression_ratio": 1.6804979253112033, "no_speech_prob": 2.3551414415123872e-05}, {"id": 1298, "seek": 642274, "start": 6430.12, "end": 6433.679999999999, "text": " I'd be interested to hear what you find out.", "tokens": [286, 1116, 312, 3102, 281, 1568, 437, 291, 915, 484, 13], "temperature": 0.0, "avg_logprob": -0.16172207726372612, "compression_ratio": 1.6804979253112033, "no_speech_prob": 2.3551414415123872e-05}, {"id": 1299, "seek": 642274, "start": 6433.679999999999, "end": 6440.46, "text": " So what we can do also is we can actually grab the attentions out of the model.", "tokens": [407, 437, 321, 393, 360, 611, 307, 321, 393, 767, 4444, 264, 30980, 626, 484, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16172207726372612, "compression_ratio": 1.6804979253112033, "no_speech_prob": 2.3551414415123872e-05}, {"id": 1300, "seek": 642274, "start": 6440.46, "end": 6448.36, "text": " So I actually added this return attention equals true here.", "tokens": [407, 286, 767, 3869, 341, 2736, 3202, 6915, 2074, 510, 13], "temperature": 0.0, "avg_logprob": -0.16172207726372612, "compression_ratio": 1.6804979253112033, "no_speech_prob": 2.3551414415123872e-05}, {"id": 1301, "seek": 642274, "start": 6448.36, "end": 6449.36, "text": " See here, my forward?", "tokens": [3008, 510, 11, 452, 2128, 30], "temperature": 0.0, "avg_logprob": -0.16172207726372612, "compression_ratio": 1.6804979253112033, "no_speech_prob": 2.3551414415123872e-05}, {"id": 1302, "seek": 642274, "start": 6449.36, "end": 6451.88, "text": " Like forward, you can put anything you like in forward.", "tokens": [1743, 2128, 11, 291, 393, 829, 1340, 291, 411, 294, 2128, 13], "temperature": 0.0, "avg_logprob": -0.16172207726372612, "compression_ratio": 1.6804979253112033, "no_speech_prob": 2.3551414415123872e-05}, {"id": 1303, "seek": 645188, "start": 6451.88, "end": 6457.2, "text": " So I added a return attention parameter, false by default, because obviously the training", "tokens": [407, 286, 3869, 257, 2736, 3202, 13075, 11, 7908, 538, 7576, 11, 570, 2745, 264, 3097], "temperature": 0.0, "avg_logprob": -0.1816563606262207, "compression_ratio": 1.6812227074235808, "no_speech_prob": 4.495153916650452e-06}, {"id": 1304, "seek": 645188, "start": 6457.2, "end": 6460.64, "text": " loop doesn't know anything about it.", "tokens": [6367, 1177, 380, 458, 1340, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.1816563606262207, "compression_ratio": 1.6812227074235808, "no_speech_prob": 4.495153916650452e-06}, {"id": 1305, "seek": 645188, "start": 6460.64, "end": 6464.76, "text": " But then I just had something here saying if return attention, then stick the attentions", "tokens": [583, 550, 286, 445, 632, 746, 510, 1566, 498, 2736, 3202, 11, 550, 2897, 264, 30980, 626], "temperature": 0.0, "avg_logprob": -0.1816563606262207, "compression_ratio": 1.6812227074235808, "no_speech_prob": 4.495153916650452e-06}, {"id": 1306, "seek": 645188, "start": 6464.76, "end": 6466.8, "text": " on as well.", "tokens": [322, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1816563606262207, "compression_ratio": 1.6812227074235808, "no_speech_prob": 4.495153916650452e-06}, {"id": 1307, "seek": 645188, "start": 6466.8, "end": 6472.72, "text": " The attentions is simply that value a, just check it in a list.", "tokens": [440, 30980, 626, 307, 2935, 300, 2158, 257, 11, 445, 1520, 309, 294, 257, 1329, 13], "temperature": 0.0, "avg_logprob": -0.1816563606262207, "compression_ratio": 1.6812227074235808, "no_speech_prob": 4.495153916650452e-06}, {"id": 1308, "seek": 645188, "start": 6472.72, "end": 6478.34, "text": " So we can now call the model with return attention equals true and get back the probabilities", "tokens": [407, 321, 393, 586, 818, 264, 2316, 365, 2736, 3202, 6915, 2074, 293, 483, 646, 264, 33783], "temperature": 0.0, "avg_logprob": -0.1816563606262207, "compression_ratio": 1.6812227074235808, "no_speech_prob": 4.495153916650452e-06}, {"id": 1309, "seek": 647834, "start": 6478.34, "end": 6485.96, "text": " and the attentions, which means as well as printing out these here, we can draw pictures", "tokens": [293, 264, 30980, 626, 11, 597, 1355, 382, 731, 382, 14699, 484, 613, 510, 11, 321, 393, 2642, 5242], "temperature": 0.0, "avg_logprob": -0.2155426570347377, "compression_ratio": 1.6519337016574585, "no_speech_prob": 1.221899947267957e-05}, {"id": 1310, "seek": 647834, "start": 6485.96, "end": 6488.72, "text": " at each time step of the attention.", "tokens": [412, 1184, 565, 1823, 295, 264, 3202, 13], "temperature": 0.0, "avg_logprob": -0.2155426570347377, "compression_ratio": 1.6519337016574585, "no_speech_prob": 1.221899947267957e-05}, {"id": 1311, "seek": 647834, "start": 6488.72, "end": 6492.64, "text": " And so you can see at the start, the attentions are all on the first word, second word, third", "tokens": [400, 370, 291, 393, 536, 412, 264, 722, 11, 264, 30980, 626, 366, 439, 322, 264, 700, 1349, 11, 1150, 1349, 11, 2636], "temperature": 0.0, "avg_logprob": -0.2155426570347377, "compression_ratio": 1.6519337016574585, "no_speech_prob": 1.221899947267957e-05}, {"id": 1312, "seek": 647834, "start": 6492.64, "end": 6500.360000000001, "text": " word, a couple of different words, and this is just for one particular sentence.", "tokens": [1349, 11, 257, 1916, 295, 819, 2283, 11, 293, 341, 307, 445, 337, 472, 1729, 8174, 13], "temperature": 0.0, "avg_logprob": -0.2155426570347377, "compression_ratio": 1.6519337016574585, "no_speech_prob": 1.221899947267957e-05}, {"id": 1313, "seek": 650036, "start": 6500.36, "end": 6510.2, "text": " So you can kind of see, this is the equivalent, when you're Chris Oller and Sean Carter, you", "tokens": [407, 291, 393, 733, 295, 536, 11, 341, 307, 264, 10344, 11, 562, 291, 434, 6688, 422, 4658, 293, 14839, 21622, 11, 291], "temperature": 0.0, "avg_logprob": -0.20305035331032492, "compression_ratio": 1.5981308411214954, "no_speech_prob": 1.112546306103468e-05}, {"id": 1314, "seek": 650036, "start": 6510.2, "end": 6512.44, "text": " make things that look like this.", "tokens": [652, 721, 300, 574, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.20305035331032492, "compression_ratio": 1.5981308411214954, "no_speech_prob": 1.112546306103468e-05}, {"id": 1315, "seek": 650036, "start": 6512.44, "end": 6515.599999999999, "text": " When you're Jeremy Howard, the exact same information looks like this.", "tokens": [1133, 291, 434, 17809, 17626, 11, 264, 1900, 912, 1589, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.20305035331032492, "compression_ratio": 1.5981308411214954, "no_speech_prob": 1.112546306103468e-05}, {"id": 1316, "seek": 650036, "start": 6515.599999999999, "end": 6521.88, "text": " But it's the same thing, just pretend that it's beautiful.", "tokens": [583, 309, 311, 264, 912, 551, 11, 445, 11865, 300, 309, 311, 2238, 13], "temperature": 0.0, "avg_logprob": -0.20305035331032492, "compression_ratio": 1.5981308411214954, "no_speech_prob": 1.112546306103468e-05}, {"id": 1317, "seek": 650036, "start": 6521.88, "end": 6527.38, "text": " So you can see basically at each different time step, we've got a different attention.", "tokens": [407, 291, 393, 536, 1936, 412, 1184, 819, 565, 1823, 11, 321, 600, 658, 257, 819, 3202, 13], "temperature": 0.0, "avg_logprob": -0.20305035331032492, "compression_ratio": 1.5981308411214954, "no_speech_prob": 1.112546306103468e-05}, {"id": 1318, "seek": 652738, "start": 6527.38, "end": 6534.6, "text": " And it's really important when you try to build something like this, like you don't", "tokens": [400, 309, 311, 534, 1021, 562, 291, 853, 281, 1322, 746, 411, 341, 11, 411, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.18818509578704834, "compression_ratio": 1.8319327731092436, "no_speech_prob": 7.646504855074454e-06}, {"id": 1319, "seek": 652738, "start": 6534.6, "end": 6537.0, "text": " really know if it's not working.", "tokens": [534, 458, 498, 309, 311, 406, 1364, 13], "temperature": 0.0, "avg_logprob": -0.18818509578704834, "compression_ratio": 1.8319327731092436, "no_speech_prob": 7.646504855074454e-06}, {"id": 1320, "seek": 652738, "start": 6537.0, "end": 6543.32, "text": " Because if it's not working, and as per usual, my first 12 attempts at this were broken,", "tokens": [1436, 498, 309, 311, 406, 1364, 11, 293, 382, 680, 7713, 11, 452, 700, 2272, 15257, 412, 341, 645, 5463, 11], "temperature": 0.0, "avg_logprob": -0.18818509578704834, "compression_ratio": 1.8319327731092436, "no_speech_prob": 7.646504855074454e-06}, {"id": 1321, "seek": 652738, "start": 6543.32, "end": 6546.96, "text": " and they were broken in the sense that it wasn't really learning anything useful, and", "tokens": [293, 436, 645, 5463, 294, 264, 2020, 300, 309, 2067, 380, 534, 2539, 1340, 4420, 11, 293], "temperature": 0.0, "avg_logprob": -0.18818509578704834, "compression_ratio": 1.8319327731092436, "no_speech_prob": 7.646504855074454e-06}, {"id": 1322, "seek": 652738, "start": 6546.96, "end": 6550.2, "text": " so therefore it was basically giving equal attention to everything, and therefore it", "tokens": [370, 4412, 309, 390, 1936, 2902, 2681, 3202, 281, 1203, 11, 293, 4412, 309], "temperature": 0.0, "avg_logprob": -0.18818509578704834, "compression_ratio": 1.8319327731092436, "no_speech_prob": 7.646504855074454e-06}, {"id": 1323, "seek": 652738, "start": 6550.2, "end": 6551.2, "text": " wasn't worse.", "tokens": [2067, 380, 5324, 13], "temperature": 0.0, "avg_logprob": -0.18818509578704834, "compression_ratio": 1.8319327731092436, "no_speech_prob": 7.646504855074454e-06}, {"id": 1324, "seek": 652738, "start": 6551.2, "end": 6552.68, "text": " It just wasn't better.", "tokens": [467, 445, 2067, 380, 1101, 13], "temperature": 0.0, "avg_logprob": -0.18818509578704834, "compression_ratio": 1.8319327731092436, "no_speech_prob": 7.646504855074454e-06}, {"id": 1325, "seek": 652738, "start": 6552.68, "end": 6555.0, "text": " It wasn't much better.", "tokens": [467, 2067, 380, 709, 1101, 13], "temperature": 0.0, "avg_logprob": -0.18818509578704834, "compression_ratio": 1.8319327731092436, "no_speech_prob": 7.646504855074454e-06}, {"id": 1326, "seek": 655500, "start": 6555.0, "end": 6562.28, "text": " And so until you actually find ways to visualize the thing in a way that you know what it ought", "tokens": [400, 370, 1826, 291, 767, 915, 2098, 281, 23273, 264, 551, 294, 257, 636, 300, 291, 458, 437, 309, 13416], "temperature": 0.0, "avg_logprob": -0.19362525653122062, "compression_ratio": 1.8421052631578947, "no_speech_prob": 4.495111170399468e-06}, {"id": 1327, "seek": 655500, "start": 6562.28, "end": 6564.86, "text": " to look like ahead of time, you don't really know if it's working.", "tokens": [281, 574, 411, 2286, 295, 565, 11, 291, 500, 380, 534, 458, 498, 309, 311, 1364, 13], "temperature": 0.0, "avg_logprob": -0.19362525653122062, "compression_ratio": 1.8421052631578947, "no_speech_prob": 4.495111170399468e-06}, {"id": 1328, "seek": 655500, "start": 6564.86, "end": 6569.68, "text": " So it's really important that you try to find ways to kind of check your intermediate steps", "tokens": [407, 309, 311, 534, 1021, 300, 291, 853, 281, 915, 2098, 281, 733, 295, 1520, 428, 19376, 4439], "temperature": 0.0, "avg_logprob": -0.19362525653122062, "compression_ratio": 1.8421052631578947, "no_speech_prob": 4.495111170399468e-06}, {"id": 1329, "seek": 655500, "start": 6569.68, "end": 6570.68, "text": " and your outputs.", "tokens": [293, 428, 23930, 13], "temperature": 0.0, "avg_logprob": -0.19362525653122062, "compression_ratio": 1.8421052631578947, "no_speech_prob": 4.495111170399468e-06}, {"id": 1330, "seek": 655500, "start": 6570.68, "end": 6571.68, "text": " Yes, Yonet?", "tokens": [1079, 11, 398, 266, 302, 30], "temperature": 0.0, "avg_logprob": -0.19362525653122062, "compression_ratio": 1.8421052631578947, "no_speech_prob": 4.495111170399468e-06}, {"id": 1331, "seek": 655500, "start": 6571.68, "end": 6576.36, "text": " I think there is a little bit of a, so people are asking, what is the loss function for", "tokens": [286, 519, 456, 307, 257, 707, 857, 295, 257, 11, 370, 561, 366, 3365, 11, 437, 307, 264, 4470, 2445, 337], "temperature": 0.0, "avg_logprob": -0.19362525653122062, "compression_ratio": 1.8421052631578947, "no_speech_prob": 4.495111170399468e-06}, {"id": 1332, "seek": 655500, "start": 6576.36, "end": 6577.84, "text": " the attentional neural network?", "tokens": [264, 3202, 304, 18161, 3209, 30], "temperature": 0.0, "avg_logprob": -0.19362525653122062, "compression_ratio": 1.8421052631578947, "no_speech_prob": 4.495111170399468e-06}, {"id": 1333, "seek": 655500, "start": 6577.84, "end": 6581.44, "text": " No, no, no loss function for the attentional neural network.", "tokens": [883, 11, 572, 11, 572, 4470, 2445, 337, 264, 3202, 304, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.19362525653122062, "compression_ratio": 1.8421052631578947, "no_speech_prob": 4.495111170399468e-06}, {"id": 1334, "seek": 655500, "start": 6581.44, "end": 6583.2, "text": " It's trained end to end.", "tokens": [467, 311, 8895, 917, 281, 917, 13], "temperature": 0.0, "avg_logprob": -0.19362525653122062, "compression_ratio": 1.8421052631578947, "no_speech_prob": 4.495111170399468e-06}, {"id": 1335, "seek": 658320, "start": 6583.2, "end": 6588.139999999999, "text": " So it's just sitting here inside our decoder loop.", "tokens": [407, 309, 311, 445, 3798, 510, 1854, 527, 979, 19866, 6367, 13], "temperature": 0.0, "avg_logprob": -0.15159055921766493, "compression_ratio": 1.7074468085106382, "no_speech_prob": 5.862775651621632e-06}, {"id": 1336, "seek": 658320, "start": 6588.139999999999, "end": 6594.28, "text": " So the loss function for the decoder loop is that this result contains, it's exactly", "tokens": [407, 264, 4470, 2445, 337, 264, 979, 19866, 6367, 307, 300, 341, 1874, 8306, 11, 309, 311, 2293], "temperature": 0.0, "avg_logprob": -0.15159055921766493, "compression_ratio": 1.7074468085106382, "no_speech_prob": 5.862775651621632e-06}, {"id": 1337, "seek": 658320, "start": 6594.28, "end": 6599.72, "text": " the same as before, just the outputs, the probabilities of the words.", "tokens": [264, 912, 382, 949, 11, 445, 264, 23930, 11, 264, 33783, 295, 264, 2283, 13], "temperature": 0.0, "avg_logprob": -0.15159055921766493, "compression_ratio": 1.7074468085106382, "no_speech_prob": 5.862775651621632e-06}, {"id": 1338, "seek": 658320, "start": 6599.72, "end": 6605.48, "text": " So like the loss function, it's the same loss function.", "tokens": [407, 411, 264, 4470, 2445, 11, 309, 311, 264, 912, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.15159055921766493, "compression_ratio": 1.7074468085106382, "no_speech_prob": 5.862775651621632e-06}, {"id": 1339, "seek": 658320, "start": 6605.48, "end": 6610.84, "text": " So how come the little mini neural nets learning something?", "tokens": [407, 577, 808, 264, 707, 8382, 18161, 36170, 2539, 746, 30], "temperature": 0.0, "avg_logprob": -0.15159055921766493, "compression_ratio": 1.7074468085106382, "no_speech_prob": 5.862775651621632e-06}, {"id": 1340, "seek": 661084, "start": 6610.84, "end": 6617.6, "text": " Well, because in order to make the outputs better and better, it would be great if it", "tokens": [1042, 11, 570, 294, 1668, 281, 652, 264, 23930, 1101, 293, 1101, 11, 309, 576, 312, 869, 498, 309], "temperature": 0.0, "avg_logprob": -0.11442257477356507, "compression_ratio": 1.8744769874476988, "no_speech_prob": 3.13813006869168e-06}, {"id": 1341, "seek": 661084, "start": 6617.6, "end": 6621.32, "text": " made the weights of this little weighted average better and better.", "tokens": [1027, 264, 17443, 295, 341, 707, 32807, 4274, 1101, 293, 1101, 13], "temperature": 0.0, "avg_logprob": -0.11442257477356507, "compression_ratio": 1.8744769874476988, "no_speech_prob": 3.13813006869168e-06}, {"id": 1342, "seek": 661084, "start": 6621.32, "end": 6626.04, "text": " So part of creating our output is to please do a good job of finding a good set of weights.", "tokens": [407, 644, 295, 4084, 527, 5598, 307, 281, 1767, 360, 257, 665, 1691, 295, 5006, 257, 665, 992, 295, 17443, 13], "temperature": 0.0, "avg_logprob": -0.11442257477356507, "compression_ratio": 1.8744769874476988, "no_speech_prob": 3.13813006869168e-06}, {"id": 1343, "seek": 661084, "start": 6626.04, "end": 6629.64, "text": " And if it doesn't do a good job of finding a good set of weights, then the loss function", "tokens": [400, 498, 309, 1177, 380, 360, 257, 665, 1691, 295, 5006, 257, 665, 992, 295, 17443, 11, 550, 264, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.11442257477356507, "compression_ratio": 1.8744769874476988, "no_speech_prob": 3.13813006869168e-06}, {"id": 1344, "seek": 661084, "start": 6629.64, "end": 6631.74, "text": " won't improve from that bit.", "tokens": [1582, 380, 3470, 490, 300, 857, 13], "temperature": 0.0, "avg_logprob": -0.11442257477356507, "compression_ratio": 1.8744769874476988, "no_speech_prob": 3.13813006869168e-06}, {"id": 1345, "seek": 661084, "start": 6631.74, "end": 6639.16, "text": " So like end to end learning means like you throw in everything that you can into one", "tokens": [407, 411, 917, 281, 917, 2539, 1355, 411, 291, 3507, 294, 1203, 300, 291, 393, 666, 472], "temperature": 0.0, "avg_logprob": -0.11442257477356507, "compression_ratio": 1.8744769874476988, "no_speech_prob": 3.13813006869168e-06}, {"id": 1346, "seek": 663916, "start": 6639.16, "end": 6646.68, "text": " loss function, and the gradients of all the different parameters point in a direction", "tokens": [4470, 2445, 11, 293, 264, 2771, 2448, 295, 439, 264, 819, 9834, 935, 294, 257, 3513], "temperature": 0.0, "avg_logprob": -0.1856796672043291, "compression_ratio": 1.8214285714285714, "no_speech_prob": 8.139647434290964e-06}, {"id": 1347, "seek": 663916, "start": 6646.68, "end": 6652.599999999999, "text": " that says basically, hey, if you put more weight over there, it would have been better.", "tokens": [300, 1619, 1936, 11, 4177, 11, 498, 291, 829, 544, 3364, 670, 456, 11, 309, 576, 362, 668, 1101, 13], "temperature": 0.0, "avg_logprob": -0.1856796672043291, "compression_ratio": 1.8214285714285714, "no_speech_prob": 8.139647434290964e-06}, {"id": 1348, "seek": 663916, "start": 6652.599999999999, "end": 6655.84, "text": " And thanks to the magic of the chain rule, it then knows, oh, it would have put more", "tokens": [400, 3231, 281, 264, 5585, 295, 264, 5021, 4978, 11, 309, 550, 3255, 11, 1954, 11, 309, 576, 362, 829, 544], "temperature": 0.0, "avg_logprob": -0.1856796672043291, "compression_ratio": 1.8214285714285714, "no_speech_prob": 8.139647434290964e-06}, {"id": 1349, "seek": 663916, "start": 6655.84, "end": 6660.24, "text": " weight over there if you would change the parameter in this matrix, multiply a little", "tokens": [3364, 670, 456, 498, 291, 576, 1319, 264, 13075, 294, 341, 8141, 11, 12972, 257, 707], "temperature": 0.0, "avg_logprob": -0.1856796672043291, "compression_ratio": 1.8214285714285714, "no_speech_prob": 8.139647434290964e-06}, {"id": 1350, "seek": 663916, "start": 6660.24, "end": 6662.38, "text": " bit over there.", "tokens": [857, 670, 456, 13], "temperature": 0.0, "avg_logprob": -0.1856796672043291, "compression_ratio": 1.8214285714285714, "no_speech_prob": 8.139647434290964e-06}, {"id": 1351, "seek": 663916, "start": 6662.38, "end": 6667.96, "text": " And so that's the magic of end to end learning.", "tokens": [400, 370, 300, 311, 264, 5585, 295, 917, 281, 917, 2539, 13], "temperature": 0.0, "avg_logprob": -0.1856796672043291, "compression_ratio": 1.8214285714285714, "no_speech_prob": 8.139647434290964e-06}, {"id": 1352, "seek": 666796, "start": 6667.96, "end": 6677.08, "text": " So it's a very understandable question of like, how did this little mini neural network?", "tokens": [407, 309, 311, 257, 588, 25648, 1168, 295, 411, 11, 577, 630, 341, 707, 8382, 18161, 3209, 30], "temperature": 0.0, "avg_logprob": -0.17920762521249275, "compression_ratio": 1.780612244897959, "no_speech_prob": 7.183186880865833e-06}, {"id": 1353, "seek": 666796, "start": 6677.08, "end": 6682.32, "text": " But you've got to realize, there's nothing particularly about this code that says, hey,", "tokens": [583, 291, 600, 658, 281, 4325, 11, 456, 311, 1825, 4098, 466, 341, 3089, 300, 1619, 11, 4177, 11], "temperature": 0.0, "avg_logprob": -0.17920762521249275, "compression_ratio": 1.780612244897959, "no_speech_prob": 7.183186880865833e-06}, {"id": 1354, "seek": 666796, "start": 6682.32, "end": 6686.4800000000005, "text": " this particular bit is a separate little mini neural network, any more than the GRU is a", "tokens": [341, 1729, 857, 307, 257, 4994, 707, 8382, 18161, 3209, 11, 604, 544, 813, 264, 10903, 52, 307, 257], "temperature": 0.0, "avg_logprob": -0.17920762521249275, "compression_ratio": 1.780612244897959, "no_speech_prob": 7.183186880865833e-06}, {"id": 1355, "seek": 666796, "start": 6686.4800000000005, "end": 6690.88, "text": " separate little neural network, or this linear layer is a separate little function.", "tokens": [4994, 707, 18161, 3209, 11, 420, 341, 8213, 4583, 307, 257, 4994, 707, 2445, 13], "temperature": 0.0, "avg_logprob": -0.17920762521249275, "compression_ratio": 1.780612244897959, "no_speech_prob": 7.183186880865833e-06}, {"id": 1356, "seek": 669088, "start": 6690.88, "end": 6699.12, "text": " Like it all ends up pushed into one output, which is a bunch of probabilities, which ends", "tokens": [1743, 309, 439, 5314, 493, 9152, 666, 472, 5598, 11, 597, 307, 257, 3840, 295, 33783, 11, 597, 5314], "temperature": 0.0, "avg_logprob": -0.09312687269071253, "compression_ratio": 1.6095238095238096, "no_speech_prob": 1.8162193100579316e-06}, {"id": 1357, "seek": 669088, "start": 6699.12, "end": 6704.04, "text": " up in one loss function that returns a single number that says this either was or wasn't", "tokens": [493, 294, 472, 4470, 2445, 300, 11247, 257, 2167, 1230, 300, 1619, 341, 2139, 390, 420, 2067, 380], "temperature": 0.0, "avg_logprob": -0.09312687269071253, "compression_ratio": 1.6095238095238096, "no_speech_prob": 1.8162193100579316e-06}, {"id": 1358, "seek": 669088, "start": 6704.04, "end": 6706.2, "text": " a good translation.", "tokens": [257, 665, 12853, 13], "temperature": 0.0, "avg_logprob": -0.09312687269071253, "compression_ratio": 1.6095238095238096, "no_speech_prob": 1.8162193100579316e-06}, {"id": 1359, "seek": 669088, "start": 6706.2, "end": 6712.36, "text": " And so thanks to the magic of the chain rule, we then back propagate little updates to all", "tokens": [400, 370, 3231, 281, 264, 5585, 295, 264, 5021, 4978, 11, 321, 550, 646, 48256, 707, 9205, 281, 439], "temperature": 0.0, "avg_logprob": -0.09312687269071253, "compression_ratio": 1.6095238095238096, "no_speech_prob": 1.8162193100579316e-06}, {"id": 1360, "seek": 669088, "start": 6712.36, "end": 6715.4400000000005, "text": " the parameters to make them a little bit better.", "tokens": [264, 9834, 281, 652, 552, 257, 707, 857, 1101, 13], "temperature": 0.0, "avg_logprob": -0.09312687269071253, "compression_ratio": 1.6095238095238096, "no_speech_prob": 1.8162193100579316e-06}, {"id": 1361, "seek": 671544, "start": 6715.44, "end": 6725.879999999999, "text": " So this is a big, weird, counterintuitive idea, and it's totally okay if it's a bit", "tokens": [407, 341, 307, 257, 955, 11, 3657, 11, 5682, 686, 48314, 1558, 11, 293, 309, 311, 3879, 1392, 498, 309, 311, 257, 857], "temperature": 0.0, "avg_logprob": -0.1942360434733646, "compression_ratio": 1.3636363636363635, "no_speech_prob": 6.540390131704044e-06}, {"id": 1362, "seek": 671544, "start": 6725.879999999999, "end": 6726.879999999999, "text": " mind-bending.", "tokens": [1575, 12, 65, 2029, 13], "temperature": 0.0, "avg_logprob": -0.1942360434733646, "compression_ratio": 1.3636363636363635, "no_speech_prob": 6.540390131704044e-06}, {"id": 1363, "seek": 671544, "start": 6726.879999999999, "end": 6735.36, "text": " And it's the bit where, even back to lesson one, it's like, how did we make it find dogs", "tokens": [400, 309, 311, 264, 857, 689, 11, 754, 646, 281, 6898, 472, 11, 309, 311, 411, 11, 577, 630, 321, 652, 309, 915, 7197], "temperature": 0.0, "avg_logprob": -0.1942360434733646, "compression_ratio": 1.3636363636363635, "no_speech_prob": 6.540390131704044e-06}, {"id": 1364, "seek": 671544, "start": 6735.36, "end": 6737.5599999999995, "text": " versus cats?", "tokens": [5717, 11111, 30], "temperature": 0.0, "avg_logprob": -0.1942360434733646, "compression_ratio": 1.3636363636363635, "no_speech_prob": 6.540390131704044e-06}, {"id": 1365, "seek": 671544, "start": 6737.5599999999995, "end": 6739.32, "text": " We didn't.", "tokens": [492, 994, 380, 13], "temperature": 0.0, "avg_logprob": -0.1942360434733646, "compression_ratio": 1.3636363636363635, "no_speech_prob": 6.540390131704044e-06}, {"id": 1366, "seek": 673932, "start": 6739.32, "end": 6745.5599999999995, "text": " All we did was we said, this is our data, this is our architecture, this is our loss", "tokens": [1057, 321, 630, 390, 321, 848, 11, 341, 307, 527, 1412, 11, 341, 307, 527, 9482, 11, 341, 307, 527, 4470], "temperature": 0.0, "avg_logprob": -0.17775544580423608, "compression_ratio": 1.6956521739130435, "no_speech_prob": 3.5008388294954784e-06}, {"id": 1367, "seek": 673932, "start": 6745.5599999999995, "end": 6749.24, "text": " function, please back propagate into the weights to make them better.", "tokens": [2445, 11, 1767, 646, 48256, 666, 264, 17443, 281, 652, 552, 1101, 13], "temperature": 0.0, "avg_logprob": -0.17775544580423608, "compression_ratio": 1.6956521739130435, "no_speech_prob": 3.5008388294954784e-06}, {"id": 1368, "seek": 673932, "start": 6749.24, "end": 6753.32, "text": " And after you've made them better a while, it'll start finding cats from dogs.", "tokens": [400, 934, 291, 600, 1027, 552, 1101, 257, 1339, 11, 309, 603, 722, 5006, 11111, 490, 7197, 13], "temperature": 0.0, "avg_logprob": -0.17775544580423608, "compression_ratio": 1.6956521739130435, "no_speech_prob": 3.5008388294954784e-06}, {"id": 1369, "seek": 673932, "start": 6753.32, "end": 6759.28, "text": " It's just in this case, we haven't used somebody else's convolutional network architecture.", "tokens": [467, 311, 445, 294, 341, 1389, 11, 321, 2378, 380, 1143, 2618, 1646, 311, 45216, 304, 3209, 9482, 13], "temperature": 0.0, "avg_logprob": -0.17775544580423608, "compression_ratio": 1.6956521739130435, "no_speech_prob": 3.5008388294954784e-06}, {"id": 1370, "seek": 673932, "start": 6759.28, "end": 6763.719999999999, "text": " We've said here's a custom architecture which we hope is going to be particularly good at", "tokens": [492, 600, 848, 510, 311, 257, 2375, 9482, 597, 321, 1454, 307, 516, 281, 312, 4098, 665, 412], "temperature": 0.0, "avg_logprob": -0.17775544580423608, "compression_ratio": 1.6956521739130435, "no_speech_prob": 3.5008388294954784e-06}, {"id": 1371, "seek": 673932, "start": 6763.719999999999, "end": 6765.219999999999, "text": " this problem.", "tokens": [341, 1154, 13], "temperature": 0.0, "avg_logprob": -0.17775544580423608, "compression_ratio": 1.6956521739130435, "no_speech_prob": 3.5008388294954784e-06}, {"id": 1372, "seek": 676522, "start": 6765.22, "end": 6770.820000000001, "text": " And even without this custom architecture, it was still okay, but then when we kind of", "tokens": [400, 754, 1553, 341, 2375, 9482, 11, 309, 390, 920, 1392, 11, 457, 550, 562, 321, 733, 295], "temperature": 0.0, "avg_logprob": -0.15611366743452093, "compression_ratio": 1.6926829268292682, "no_speech_prob": 7.296331659745192e-06}, {"id": 1373, "seek": 676522, "start": 6770.820000000001, "end": 6776.76, "text": " made it in a way that made more sense to what we think it ought to do, it worked even better.", "tokens": [1027, 309, 294, 257, 636, 300, 1027, 544, 2020, 281, 437, 321, 519, 309, 13416, 281, 360, 11, 309, 2732, 754, 1101, 13], "temperature": 0.0, "avg_logprob": -0.15611366743452093, "compression_ratio": 1.6926829268292682, "no_speech_prob": 7.296331659745192e-06}, {"id": 1374, "seek": 676522, "start": 6776.76, "end": 6784.04, "text": " But at no point did we kind of do anything different other than say, here's a data, here's", "tokens": [583, 412, 572, 935, 630, 321, 733, 295, 360, 1340, 819, 661, 813, 584, 11, 510, 311, 257, 1412, 11, 510, 311], "temperature": 0.0, "avg_logprob": -0.15611366743452093, "compression_ratio": 1.6926829268292682, "no_speech_prob": 7.296331659745192e-06}, {"id": 1375, "seek": 676522, "start": 6784.04, "end": 6789.66, "text": " an architecture, here's a loss function, go and find the parameters please.", "tokens": [364, 9482, 11, 510, 311, 257, 4470, 2445, 11, 352, 293, 915, 264, 9834, 1767, 13], "temperature": 0.0, "avg_logprob": -0.15611366743452093, "compression_ratio": 1.6926829268292682, "no_speech_prob": 7.296331659745192e-06}, {"id": 1376, "seek": 678966, "start": 6789.66, "end": 6798.0, "text": " And it did it, because that's what neural nets do.", "tokens": [400, 309, 630, 309, 11, 570, 300, 311, 437, 18161, 36170, 360, 13], "temperature": 0.0, "avg_logprob": -0.17580699920654297, "compression_ratio": 1.3432835820895523, "no_speech_prob": 5.0936841944349e-06}, {"id": 1377, "seek": 678966, "start": 6798.0, "end": 6803.28, "text": " So that is sequence-to-sequence learning.", "tokens": [407, 300, 307, 8310, 12, 1353, 12, 11834, 655, 2539, 13], "temperature": 0.0, "avg_logprob": -0.17580699920654297, "compression_ratio": 1.3432835820895523, "no_speech_prob": 5.0936841944349e-06}, {"id": 1378, "seek": 678966, "start": 6803.28, "end": 6814.139999999999, "text": " And if you want to encode an image into using a CNN backbone of some kind and then pass", "tokens": [400, 498, 291, 528, 281, 2058, 1429, 364, 3256, 666, 1228, 257, 24859, 34889, 295, 512, 733, 293, 550, 1320], "temperature": 0.0, "avg_logprob": -0.17580699920654297, "compression_ratio": 1.3432835820895523, "no_speech_prob": 5.0936841944349e-06}, {"id": 1379, "seek": 681414, "start": 6814.14, "end": 6820.9800000000005, "text": " that into a decoder, which is like a RNN with a tension, and you make your Y values the", "tokens": [300, 666, 257, 979, 19866, 11, 597, 307, 411, 257, 45702, 45, 365, 257, 8980, 11, 293, 291, 652, 428, 398, 4190, 264], "temperature": 0.0, "avg_logprob": -0.1275838063313411, "compression_ratio": 1.985, "no_speech_prob": 1.2218774827488232e-05}, {"id": 1380, "seek": 681414, "start": 6820.9800000000005, "end": 6825.72, "text": " actual correct captions for each of those images, you will end up with an image caption", "tokens": [3539, 3006, 44832, 337, 1184, 295, 729, 5267, 11, 291, 486, 917, 493, 365, 364, 3256, 31974], "temperature": 0.0, "avg_logprob": -0.1275838063313411, "compression_ratio": 1.985, "no_speech_prob": 1.2218774827488232e-05}, {"id": 1381, "seek": 681414, "start": 6825.72, "end": 6826.88, "text": " generator.", "tokens": [19265, 13], "temperature": 0.0, "avg_logprob": -0.1275838063313411, "compression_ratio": 1.985, "no_speech_prob": 1.2218774827488232e-05}, {"id": 1382, "seek": 681414, "start": 6826.88, "end": 6830.46, "text": " If you do the same thing with videos and captions, you'll end up with a video caption generator.", "tokens": [759, 291, 360, 264, 912, 551, 365, 2145, 293, 44832, 11, 291, 603, 917, 493, 365, 257, 960, 31974, 19265, 13], "temperature": 0.0, "avg_logprob": -0.1275838063313411, "compression_ratio": 1.985, "no_speech_prob": 1.2218774827488232e-05}, {"id": 1383, "seek": 681414, "start": 6830.46, "end": 6835.320000000001, "text": " If you do the same thing with 3D CT scans and radiology reports, you'll end up with", "tokens": [759, 291, 360, 264, 912, 551, 365, 805, 35, 19529, 35116, 293, 16335, 1793, 7122, 11, 291, 603, 917, 493, 365], "temperature": 0.0, "avg_logprob": -0.1275838063313411, "compression_ratio": 1.985, "no_speech_prob": 1.2218774827488232e-05}, {"id": 1384, "seek": 681414, "start": 6835.320000000001, "end": 6837.160000000001, "text": " a radiology report generator.", "tokens": [257, 16335, 1793, 2275, 19265, 13], "temperature": 0.0, "avg_logprob": -0.1275838063313411, "compression_ratio": 1.985, "no_speech_prob": 1.2218774827488232e-05}, {"id": 1385, "seek": 683716, "start": 6837.16, "end": 6844.3, "text": " If you do the same thing with GitHub issues and people's chosen summaries of them, you'll", "tokens": [759, 291, 360, 264, 912, 551, 365, 23331, 2663, 293, 561, 311, 8614, 8367, 4889, 295, 552, 11, 291, 603], "temperature": 0.0, "avg_logprob": -0.21041335588620033, "compression_ratio": 1.5, "no_speech_prob": 9.874615898297634e-07}, {"id": 1386, "seek": 683716, "start": 6844.3, "end": 6848.08, "text": " get a GitHub issue summary generator.", "tokens": [483, 257, 23331, 2734, 12691, 19265, 13], "temperature": 0.0, "avg_logprob": -0.21041335588620033, "compression_ratio": 1.5, "no_speech_prob": 9.874615898297634e-07}, {"id": 1387, "seek": 683716, "start": 6848.08, "end": 6858.58, "text": " Sector-sec, I agree, they're magical, but they work.", "tokens": [318, 20814, 12, 8159, 11, 286, 3986, 11, 436, 434, 12066, 11, 457, 436, 589, 13], "temperature": 0.0, "avg_logprob": -0.21041335588620033, "compression_ratio": 1.5, "no_speech_prob": 9.874615898297634e-07}, {"id": 1388, "seek": 683716, "start": 6858.58, "end": 6863.26, "text": " And I don't feel like people have begun to scratch the surface of how to use sector-sec", "tokens": [400, 286, 500, 380, 841, 411, 561, 362, 16009, 281, 8459, 264, 3753, 295, 577, 281, 764, 6977, 12, 8159], "temperature": 0.0, "avg_logprob": -0.21041335588620033, "compression_ratio": 1.5, "no_speech_prob": 9.874615898297634e-07}, {"id": 1389, "seek": 683716, "start": 6863.26, "end": 6865.34, "text": " models in their own domains.", "tokens": [5245, 294, 641, 1065, 25514, 13], "temperature": 0.0, "avg_logprob": -0.21041335588620033, "compression_ratio": 1.5, "no_speech_prob": 9.874615898297634e-07}, {"id": 1390, "seek": 686534, "start": 6865.34, "end": 6870.3, "text": " So not being a GitHub person, it would never have occurred to me that it would be kind", "tokens": [407, 406, 885, 257, 23331, 954, 11, 309, 576, 1128, 362, 11068, 281, 385, 300, 309, 576, 312, 733], "temperature": 0.0, "avg_logprob": -0.12747897786542403, "compression_ratio": 1.6838235294117647, "no_speech_prob": 1.9222779883421026e-05}, {"id": 1391, "seek": 686534, "start": 6870.3, "end": 6874.66, "text": " of cool to start with some issue and automatically create a summary.", "tokens": [295, 1627, 281, 722, 365, 512, 2734, 293, 6772, 1884, 257, 12691, 13], "temperature": 0.0, "avg_logprob": -0.12747897786542403, "compression_ratio": 1.6838235294117647, "no_speech_prob": 1.9222779883421026e-05}, {"id": 1392, "seek": 686534, "start": 6874.66, "end": 6880.52, "text": " But now I'm like, of course, next time I go to GitHub, I want to see a summary written", "tokens": [583, 586, 286, 478, 411, 11, 295, 1164, 11, 958, 565, 286, 352, 281, 23331, 11, 286, 528, 281, 536, 257, 12691, 3720], "temperature": 0.0, "avg_logprob": -0.12747897786542403, "compression_ratio": 1.6838235294117647, "no_speech_prob": 1.9222779883421026e-05}, {"id": 1393, "seek": 686534, "start": 6880.52, "end": 6881.52, "text": " there for me.", "tokens": [456, 337, 385, 13], "temperature": 0.0, "avg_logprob": -0.12747897786542403, "compression_ratio": 1.6838235294117647, "no_speech_prob": 1.9222779883421026e-05}, {"id": 1394, "seek": 686534, "start": 6881.52, "end": 6884.82, "text": " I don't want to write my own damn commit message through that.", "tokens": [286, 500, 380, 528, 281, 2464, 452, 1065, 8151, 5599, 3636, 807, 300, 13], "temperature": 0.0, "avg_logprob": -0.12747897786542403, "compression_ratio": 1.6838235294117647, "no_speech_prob": 1.9222779883421026e-05}, {"id": 1395, "seek": 686534, "start": 6884.82, "end": 6889.14, "text": " Why should I write my own summary of the code review when I finished adding comments to", "tokens": [1545, 820, 286, 2464, 452, 1065, 12691, 295, 264, 3089, 3131, 562, 286, 4335, 5127, 3053, 281], "temperature": 0.0, "avg_logprob": -0.12747897786542403, "compression_ratio": 1.6838235294117647, "no_speech_prob": 1.9222779883421026e-05}, {"id": 1396, "seek": 686534, "start": 6889.14, "end": 6890.14, "text": " lots of clients?", "tokens": [3195, 295, 6982, 30], "temperature": 0.0, "avg_logprob": -0.12747897786542403, "compression_ratio": 1.6838235294117647, "no_speech_prob": 1.9222779883421026e-05}, {"id": 1397, "seek": 686534, "start": 6890.14, "end": 6891.26, "text": " It should do that for me as well.", "tokens": [467, 820, 360, 300, 337, 385, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.12747897786542403, "compression_ratio": 1.6838235294117647, "no_speech_prob": 1.9222779883421026e-05}, {"id": 1398, "seek": 689126, "start": 6891.26, "end": 6895.54, "text": " Now I'm thinking, GitHub is so behind, it could be doing this stuff.", "tokens": [823, 286, 478, 1953, 11, 23331, 307, 370, 2261, 11, 309, 727, 312, 884, 341, 1507, 13], "temperature": 0.0, "avg_logprob": -0.1901723604935866, "compression_ratio": 1.518987341772152, "no_speech_prob": 9.97283132164739e-06}, {"id": 1399, "seek": 689126, "start": 6895.54, "end": 6900.58, "text": " So what are the things in your industry that you could start with a sequence and generate", "tokens": [407, 437, 366, 264, 721, 294, 428, 3518, 300, 291, 727, 722, 365, 257, 8310, 293, 8460], "temperature": 0.0, "avg_logprob": -0.1901723604935866, "compression_ratio": 1.518987341772152, "no_speech_prob": 9.97283132164739e-06}, {"id": 1400, "seek": 689126, "start": 6900.58, "end": 6901.58, "text": " something from it?", "tokens": [746, 490, 309, 30], "temperature": 0.0, "avg_logprob": -0.1901723604935866, "compression_ratio": 1.518987341772152, "no_speech_prob": 9.97283132164739e-06}, {"id": 1401, "seek": 689126, "start": 6901.58, "end": 6904.900000000001, "text": " I can't begin to imagine.", "tokens": [286, 393, 380, 1841, 281, 3811, 13], "temperature": 0.0, "avg_logprob": -0.1901723604935866, "compression_ratio": 1.518987341772152, "no_speech_prob": 9.97283132164739e-06}, {"id": 1402, "seek": 689126, "start": 6904.900000000001, "end": 6908.9800000000005, "text": " So again, it's a fairly new area.", "tokens": [407, 797, 11, 309, 311, 257, 6457, 777, 1859, 13], "temperature": 0.0, "avg_logprob": -0.1901723604935866, "compression_ratio": 1.518987341772152, "no_speech_prob": 9.97283132164739e-06}, {"id": 1403, "seek": 689126, "start": 6908.9800000000005, "end": 6912.14, "text": " The tools for it are not easy to use.", "tokens": [440, 3873, 337, 309, 366, 406, 1858, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.1901723604935866, "compression_ratio": 1.518987341772152, "no_speech_prob": 9.97283132164739e-06}, {"id": 1404, "seek": 689126, "start": 6912.14, "end": 6915.66, "text": " They're not even built into FastAI yet, as you can see.", "tokens": [814, 434, 406, 754, 3094, 666, 15968, 48698, 1939, 11, 382, 291, 393, 536, 13], "temperature": 0.0, "avg_logprob": -0.1901723604935866, "compression_ratio": 1.518987341772152, "no_speech_prob": 9.97283132164739e-06}, {"id": 1405, "seek": 689126, "start": 6915.66, "end": 6919.46, "text": " Hopefully they will be soon.", "tokens": [10429, 436, 486, 312, 2321, 13], "temperature": 0.0, "avg_logprob": -0.1901723604935866, "compression_ratio": 1.518987341772152, "no_speech_prob": 9.97283132164739e-06}, {"id": 1406, "seek": 691946, "start": 6919.46, "end": 6924.46, "text": " I don't think anybody knows what the opportunities are.", "tokens": [286, 500, 380, 519, 4472, 3255, 437, 264, 4786, 366, 13], "temperature": 0.0, "avg_logprob": -0.14495116610859715, "compression_ratio": 1.4705882352941178, "no_speech_prob": 4.936935965815792e-06}, {"id": 1407, "seek": 691946, "start": 6924.46, "end": 6927.94, "text": " So I've got good news and bad news.", "tokens": [407, 286, 600, 658, 665, 2583, 293, 1578, 2583, 13], "temperature": 0.0, "avg_logprob": -0.14495116610859715, "compression_ratio": 1.4705882352941178, "no_speech_prob": 4.936935965815792e-06}, {"id": 1408, "seek": 691946, "start": 6927.94, "end": 6934.9800000000005, "text": " The bad news is we have 20 minutes to cover a topic which in last year's course took a", "tokens": [440, 1578, 2583, 307, 321, 362, 945, 2077, 281, 2060, 257, 4829, 597, 294, 1036, 1064, 311, 1164, 1890, 257], "temperature": 0.0, "avg_logprob": -0.14495116610859715, "compression_ratio": 1.4705882352941178, "no_speech_prob": 4.936935965815792e-06}, {"id": 1409, "seek": 691946, "start": 6934.9800000000005, "end": 6937.62, "text": " whole lesson.", "tokens": [1379, 6898, 13], "temperature": 0.0, "avg_logprob": -0.14495116610859715, "compression_ratio": 1.4705882352941178, "no_speech_prob": 4.936935965815792e-06}, {"id": 1410, "seek": 691946, "start": 6937.62, "end": 6942.06, "text": " The good news is that when I went to rewrite this using FastAI and PyTorch, I ended up", "tokens": [440, 665, 2583, 307, 300, 562, 286, 1437, 281, 28132, 341, 1228, 15968, 48698, 293, 9953, 51, 284, 339, 11, 286, 4590, 493], "temperature": 0.0, "avg_logprob": -0.14495116610859715, "compression_ratio": 1.4705882352941178, "no_speech_prob": 4.936935965815792e-06}, {"id": 1411, "seek": 691946, "start": 6942.06, "end": 6944.86, "text": " with almost no code.", "tokens": [365, 1920, 572, 3089, 13], "temperature": 0.0, "avg_logprob": -0.14495116610859715, "compression_ratio": 1.4705882352941178, "no_speech_prob": 4.936935965815792e-06}, {"id": 1412, "seek": 694486, "start": 6944.86, "end": 6949.38, "text": " So all of the stuff that made it hard last year is basically gone now.", "tokens": [407, 439, 295, 264, 1507, 300, 1027, 309, 1152, 1036, 1064, 307, 1936, 2780, 586, 13], "temperature": 0.0, "avg_logprob": -0.16435774889859286, "compression_ratio": 1.5991189427312775, "no_speech_prob": 6.438881428039167e-06}, {"id": 1413, "seek": 694486, "start": 6949.38, "end": 6955.5199999999995, "text": " So we're going to do something bringing together for the first time our two little worlds we", "tokens": [407, 321, 434, 516, 281, 360, 746, 5062, 1214, 337, 264, 700, 565, 527, 732, 707, 13401, 321], "temperature": 0.0, "avg_logprob": -0.16435774889859286, "compression_ratio": 1.5991189427312775, "no_speech_prob": 6.438881428039167e-06}, {"id": 1414, "seek": 694486, "start": 6955.5199999999995, "end": 6958.0199999999995, "text": " focused on, text and images.", "tokens": [5178, 322, 11, 2487, 293, 5267, 13], "temperature": 0.0, "avg_logprob": -0.16435774889859286, "compression_ratio": 1.5991189427312775, "no_speech_prob": 6.438881428039167e-06}, {"id": 1415, "seek": 694486, "start": 6958.0199999999995, "end": 6961.259999999999, "text": " We're going to try and bring them together.", "tokens": [492, 434, 516, 281, 853, 293, 1565, 552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.16435774889859286, "compression_ratio": 1.5991189427312775, "no_speech_prob": 6.438881428039167e-06}, {"id": 1416, "seek": 694486, "start": 6961.259999999999, "end": 6968.339999999999, "text": " And so this idea came up really in a paper by this extraordinary deep learning practitioner", "tokens": [400, 370, 341, 1558, 1361, 493, 534, 294, 257, 3035, 538, 341, 10581, 2452, 2539, 32125], "temperature": 0.0, "avg_logprob": -0.16435774889859286, "compression_ratio": 1.5991189427312775, "no_speech_prob": 6.438881428039167e-06}, {"id": 1417, "seek": 694486, "start": 6968.339999999999, "end": 6971.179999999999, "text": " and researcher named Andrea Fromm.", "tokens": [293, 21751, 4926, 24215, 1526, 1204, 13], "temperature": 0.0, "avg_logprob": -0.16435774889859286, "compression_ratio": 1.5991189427312775, "no_speech_prob": 6.438881428039167e-06}, {"id": 1418, "seek": 697118, "start": 6971.18, "end": 6975.22, "text": " And Andrea was at Google at the time.", "tokens": [400, 24215, 390, 412, 3329, 412, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.17472632137345678, "compression_ratio": 1.6287128712871286, "no_speech_prob": 5.422129106591456e-06}, {"id": 1419, "seek": 697118, "start": 6975.22, "end": 6986.1, "text": " And her basic crazy idea was to say words can have a distributed representation, a space,", "tokens": [400, 720, 3875, 3219, 1558, 390, 281, 584, 2283, 393, 362, 257, 12631, 10290, 11, 257, 1901, 11], "temperature": 0.0, "avg_logprob": -0.17472632137345678, "compression_ratio": 1.6287128712871286, "no_speech_prob": 5.422129106591456e-06}, {"id": 1420, "seek": 697118, "start": 6986.1, "end": 6991.3, "text": " which particularly at that time really was just word vectors.", "tokens": [597, 4098, 412, 300, 565, 534, 390, 445, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.17472632137345678, "compression_ratio": 1.6287128712871286, "no_speech_prob": 5.422129106591456e-06}, {"id": 1421, "seek": 697118, "start": 6991.3, "end": 6994.42, "text": " And images can be represented in a space.", "tokens": [400, 5267, 393, 312, 10379, 294, 257, 1901, 13], "temperature": 0.0, "avg_logprob": -0.17472632137345678, "compression_ratio": 1.6287128712871286, "no_speech_prob": 5.422129106591456e-06}, {"id": 1422, "seek": 697118, "start": 6994.42, "end": 7001.1, "text": " In the end, if we have a fully connected layer, they kind of ended up as a vector representation.", "tokens": [682, 264, 917, 11, 498, 321, 362, 257, 4498, 4582, 4583, 11, 436, 733, 295, 4590, 493, 382, 257, 8062, 10290, 13], "temperature": 0.0, "avg_logprob": -0.17472632137345678, "compression_ratio": 1.6287128712871286, "no_speech_prob": 5.422129106591456e-06}, {"id": 1423, "seek": 700110, "start": 7001.1, "end": 7003.700000000001, "text": " Could we merge the two?", "tokens": [7497, 321, 22183, 264, 732, 30], "temperature": 0.0, "avg_logprob": -0.13119503346885122, "compression_ratio": 1.7865853658536586, "no_speech_prob": 5.682377377524972e-06}, {"id": 1424, "seek": 700110, "start": 7003.700000000001, "end": 7010.18, "text": " Could we somehow encourage the vector space that the images end up with be the same vector", "tokens": [7497, 321, 6063, 5373, 264, 8062, 1901, 300, 264, 5267, 917, 493, 365, 312, 264, 912, 8062], "temperature": 0.0, "avg_logprob": -0.13119503346885122, "compression_ratio": 1.7865853658536586, "no_speech_prob": 5.682377377524972e-06}, {"id": 1425, "seek": 700110, "start": 7010.18, "end": 7012.14, "text": " space that the words are in?", "tokens": [1901, 300, 264, 2283, 366, 294, 30], "temperature": 0.0, "avg_logprob": -0.13119503346885122, "compression_ratio": 1.7865853658536586, "no_speech_prob": 5.682377377524972e-06}, {"id": 1426, "seek": 700110, "start": 7012.14, "end": 7014.820000000001, "text": " And if we could do that, what would that mean?", "tokens": [400, 498, 321, 727, 360, 300, 11, 437, 576, 300, 914, 30], "temperature": 0.0, "avg_logprob": -0.13119503346885122, "compression_ratio": 1.7865853658536586, "no_speech_prob": 5.682377377524972e-06}, {"id": 1427, "seek": 700110, "start": 7014.820000000001, "end": 7017.42, "text": " What could we do with that?", "tokens": [708, 727, 321, 360, 365, 300, 30], "temperature": 0.0, "avg_logprob": -0.13119503346885122, "compression_ratio": 1.7865853658536586, "no_speech_prob": 5.682377377524972e-06}, {"id": 1428, "seek": 700110, "start": 7017.42, "end": 7025.34, "text": " So what could we do with that covers things like, well, what if I'm wrong?", "tokens": [407, 437, 727, 321, 360, 365, 300, 10538, 721, 411, 11, 731, 11, 437, 498, 286, 478, 2085, 30], "temperature": 0.0, "avg_logprob": -0.13119503346885122, "compression_ratio": 1.7865853658536586, "no_speech_prob": 5.682377377524972e-06}, {"id": 1429, "seek": 702534, "start": 7025.34, "end": 7036.9400000000005, "text": " What if I'm predicting that this image is a Beagle and I predict Jumbo Jet and Yannet's", "tokens": [708, 498, 286, 478, 32884, 300, 341, 3256, 307, 257, 879, 15088, 293, 286, 6069, 508, 449, 1763, 28730, 293, 398, 969, 302, 311], "temperature": 0.0, "avg_logprob": -0.18963880037006578, "compression_ratio": 1.5505050505050506, "no_speech_prob": 5.2553400564647745e-06}, {"id": 1430, "seek": 702534, "start": 7036.9400000000005, "end": 7040.26, "text": " model predicts Corgi?", "tokens": [2316, 6069, 82, 3925, 7834, 30], "temperature": 0.0, "avg_logprob": -0.18963880037006578, "compression_ratio": 1.5505050505050506, "no_speech_prob": 5.2553400564647745e-06}, {"id": 1431, "seek": 702534, "start": 7040.26, "end": 7044.860000000001, "text": " The normal loss function says that Yannet and Jeremy's models are equally good, i.e.", "tokens": [440, 2710, 4470, 2445, 1619, 300, 398, 969, 302, 293, 17809, 311, 5245, 366, 12309, 665, 11, 741, 13, 68, 13], "temperature": 0.0, "avg_logprob": -0.18963880037006578, "compression_ratio": 1.5505050505050506, "no_speech_prob": 5.2553400564647745e-06}, {"id": 1432, "seek": 702534, "start": 7044.860000000001, "end": 7047.74, "text": " they're both wrong.", "tokens": [436, 434, 1293, 2085, 13], "temperature": 0.0, "avg_logprob": -0.18963880037006578, "compression_ratio": 1.5505050505050506, "no_speech_prob": 5.2553400564647745e-06}, {"id": 1433, "seek": 702534, "start": 7047.74, "end": 7053.7, "text": " But what if we could somehow actually say, Corgi's closer to Beagle than it is to Jumbo", "tokens": [583, 437, 498, 321, 727, 6063, 767, 584, 11, 3925, 7834, 311, 4966, 281, 879, 15088, 813, 309, 307, 281, 508, 449, 1763], "temperature": 0.0, "avg_logprob": -0.18963880037006578, "compression_ratio": 1.5505050505050506, "no_speech_prob": 5.2553400564647745e-06}, {"id": 1434, "seek": 702534, "start": 7053.7, "end": 7054.7, "text": " Jet.", "tokens": [28730, 13], "temperature": 0.0, "avg_logprob": -0.18963880037006578, "compression_ratio": 1.5505050505050506, "no_speech_prob": 5.2553400564647745e-06}, {"id": 1435, "seek": 705470, "start": 7054.7, "end": 7056.9, "text": " So Yannet's model is better than Jeremy's.", "tokens": [407, 398, 969, 302, 311, 2316, 307, 1101, 813, 17809, 311, 13], "temperature": 0.0, "avg_logprob": -0.19534527978231742, "compression_ratio": 1.4492753623188406, "no_speech_prob": 6.748011401214171e-06}, {"id": 1436, "seek": 705470, "start": 7056.9, "end": 7063.22, "text": " And we should be able to do that, right, because in word vector space, Beagle and Corgi are", "tokens": [400, 321, 820, 312, 1075, 281, 360, 300, 11, 558, 11, 570, 294, 1349, 8062, 1901, 11, 879, 15088, 293, 3925, 7834, 366], "temperature": 0.0, "avg_logprob": -0.19534527978231742, "compression_ratio": 1.4492753623188406, "no_speech_prob": 6.748011401214171e-06}, {"id": 1437, "seek": 705470, "start": 7063.22, "end": 7068.5, "text": " pretty close together, but Jumbo Jet, not so much.", "tokens": [1238, 1998, 1214, 11, 457, 508, 449, 1763, 28730, 11, 406, 370, 709, 13], "temperature": 0.0, "avg_logprob": -0.19534527978231742, "compression_ratio": 1.4492753623188406, "no_speech_prob": 6.748011401214171e-06}, {"id": 1438, "seek": 705470, "start": 7068.5, "end": 7075.38, "text": " So it would give us a nice situation where hopefully our inferences would be wrong in", "tokens": [407, 309, 576, 976, 505, 257, 1481, 2590, 689, 4696, 527, 13596, 2667, 576, 312, 2085, 294], "temperature": 0.0, "avg_logprob": -0.19534527978231742, "compression_ratio": 1.4492753623188406, "no_speech_prob": 6.748011401214171e-06}, {"id": 1439, "seek": 705470, "start": 7075.38, "end": 7077.66, "text": " saner ways if they're wrong.", "tokens": [6645, 260, 2098, 498, 436, 434, 2085, 13], "temperature": 0.0, "avg_logprob": -0.19534527978231742, "compression_ratio": 1.4492753623188406, "no_speech_prob": 6.748011401214171e-06}, {"id": 1440, "seek": 707766, "start": 7077.66, "end": 7088.0199999999995, "text": " It would also allow us to search for things that aren't in our ImageNet, like a category", "tokens": [467, 576, 611, 2089, 505, 281, 3164, 337, 721, 300, 3212, 380, 294, 527, 29903, 31890, 11, 411, 257, 7719], "temperature": 0.0, "avg_logprob": -0.21228900956518856, "compression_ratio": 1.4840425531914894, "no_speech_prob": 9.516183126834221e-06}, {"id": 1441, "seek": 707766, "start": 7088.0199999999995, "end": 7091.74, "text": " in ImageNet, like dog and cat.", "tokens": [294, 29903, 31890, 11, 411, 3000, 293, 3857, 13], "temperature": 0.0, "avg_logprob": -0.21228900956518856, "compression_ratio": 1.4840425531914894, "no_speech_prob": 9.516183126834221e-06}, {"id": 1442, "seek": 707766, "start": 7091.74, "end": 7095.38, "text": " Why did I have to train a whole new model to find dogs vs. cats when we already had", "tokens": [1545, 630, 286, 362, 281, 3847, 257, 1379, 777, 2316, 281, 915, 7197, 12041, 13, 11111, 562, 321, 1217, 632], "temperature": 0.0, "avg_logprob": -0.21228900956518856, "compression_ratio": 1.4840425531914894, "no_speech_prob": 9.516183126834221e-06}, {"id": 1443, "seek": 707766, "start": 7095.38, "end": 7100.0, "text": " something that found Corgis and Tabbies?", "tokens": [746, 300, 1352, 3925, 70, 271, 293, 14106, 23177, 30], "temperature": 0.0, "avg_logprob": -0.21228900956518856, "compression_ratio": 1.4840425531914894, "no_speech_prob": 9.516183126834221e-06}, {"id": 1444, "seek": 707766, "start": 7100.0, "end": 7101.54, "text": " Why can't I just say find me dogs?", "tokens": [1545, 393, 380, 286, 445, 584, 915, 385, 7197, 30], "temperature": 0.0, "avg_logprob": -0.21228900956518856, "compression_ratio": 1.4840425531914894, "no_speech_prob": 9.516183126834221e-06}, {"id": 1445, "seek": 710154, "start": 7101.54, "end": 7108.1, "text": " Well, if I trained it in word vector space, I totally could, right, because there's now", "tokens": [1042, 11, 498, 286, 8895, 309, 294, 1349, 8062, 1901, 11, 286, 3879, 727, 11, 558, 11, 570, 456, 311, 586], "temperature": 0.0, "avg_logprob": -0.14188105326432449, "compression_ratio": 1.720524017467249, "no_speech_prob": 3.8449193198175635e-06}, {"id": 1446, "seek": 710154, "start": 7108.1, "end": 7114.78, "text": " a word vector I can find things with the right image vector, and so forth.", "tokens": [257, 1349, 8062, 286, 393, 915, 721, 365, 264, 558, 3256, 8062, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.14188105326432449, "compression_ratio": 1.720524017467249, "no_speech_prob": 3.8449193198175635e-06}, {"id": 1447, "seek": 710154, "start": 7114.78, "end": 7118.18, "text": " So we'll look at some cool things we can do with it in a moment, but first of all, let's", "tokens": [407, 321, 603, 574, 412, 512, 1627, 721, 321, 393, 360, 365, 309, 294, 257, 1623, 11, 457, 700, 295, 439, 11, 718, 311], "temperature": 0.0, "avg_logprob": -0.14188105326432449, "compression_ratio": 1.720524017467249, "no_speech_prob": 3.8449193198175635e-06}, {"id": 1448, "seek": 710154, "start": 7118.18, "end": 7127.74, "text": " train a model where this model is not learning a category, a one-hot encoded ID where every", "tokens": [3847, 257, 2316, 689, 341, 2316, 307, 406, 2539, 257, 7719, 11, 257, 472, 12, 12194, 2058, 12340, 7348, 689, 633], "temperature": 0.0, "avg_logprob": -0.14188105326432449, "compression_ratio": 1.720524017467249, "no_speech_prob": 3.8449193198175635e-06}, {"id": 1449, "seek": 710154, "start": 7127.74, "end": 7131.38, "text": " category is equally far from every other category.", "tokens": [7719, 307, 12309, 1400, 490, 633, 661, 7719, 13], "temperature": 0.0, "avg_logprob": -0.14188105326432449, "compression_ratio": 1.720524017467249, "no_speech_prob": 3.8449193198175635e-06}, {"id": 1450, "seek": 713138, "start": 7131.38, "end": 7139.5, "text": " Let's instead train a model where we're finding the dependent variable, which is a word vector.", "tokens": [961, 311, 2602, 3847, 257, 2316, 689, 321, 434, 5006, 264, 12334, 7006, 11, 597, 307, 257, 1349, 8062, 13], "temperature": 0.0, "avg_logprob": -0.13956836038384557, "compression_ratio": 1.8949771689497716, "no_speech_prob": 1.1659411939035635e-05}, {"id": 1451, "seek": 713138, "start": 7139.5, "end": 7140.5, "text": " So what word vector?", "tokens": [407, 437, 1349, 8062, 30], "temperature": 0.0, "avg_logprob": -0.13956836038384557, "compression_ratio": 1.8949771689497716, "no_speech_prob": 1.1659411939035635e-05}, {"id": 1452, "seek": 713138, "start": 7140.5, "end": 7143.78, "text": " Well, obviously the word vector for the word you want.", "tokens": [1042, 11, 2745, 264, 1349, 8062, 337, 264, 1349, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.13956836038384557, "compression_ratio": 1.8949771689497716, "no_speech_prob": 1.1659411939035635e-05}, {"id": 1453, "seek": 713138, "start": 7143.78, "end": 7150.06, "text": " So if it's Corgi, let's train it to create a word vector that's the Corgi word vector,", "tokens": [407, 498, 309, 311, 3925, 7834, 11, 718, 311, 3847, 309, 281, 1884, 257, 1349, 8062, 300, 311, 264, 3925, 7834, 1349, 8062, 11], "temperature": 0.0, "avg_logprob": -0.13956836038384557, "compression_ratio": 1.8949771689497716, "no_speech_prob": 1.1659411939035635e-05}, {"id": 1454, "seek": 713138, "start": 7150.06, "end": 7154.06, "text": " and if it's a Jumbo Jet, let's train it with a dependent variable that says this is the", "tokens": [293, 498, 309, 311, 257, 508, 449, 1763, 28730, 11, 718, 311, 3847, 309, 365, 257, 12334, 7006, 300, 1619, 341, 307, 264], "temperature": 0.0, "avg_logprob": -0.13956836038384557, "compression_ratio": 1.8949771689497716, "no_speech_prob": 1.1659411939035635e-05}, {"id": 1455, "seek": 713138, "start": 7154.06, "end": 7156.36, "text": " word vector for a Jumbo Jet.", "tokens": [1349, 8062, 337, 257, 508, 449, 1763, 28730, 13], "temperature": 0.0, "avg_logprob": -0.13956836038384557, "compression_ratio": 1.8949771689497716, "no_speech_prob": 1.1659411939035635e-05}, {"id": 1456, "seek": 713138, "start": 7156.36, "end": 7160.58, "text": " So as I said, it's now shockingly easy.", "tokens": [407, 382, 286, 848, 11, 309, 311, 586, 5588, 12163, 1858, 13], "temperature": 0.0, "avg_logprob": -0.13956836038384557, "compression_ratio": 1.8949771689497716, "no_speech_prob": 1.1659411939035635e-05}, {"id": 1457, "seek": 716058, "start": 7160.58, "end": 7168.82, "text": " So let's grab the fast text word vectors again, load them in, we only need English this time,", "tokens": [407, 718, 311, 4444, 264, 2370, 2487, 1349, 18875, 797, 11, 3677, 552, 294, 11, 321, 787, 643, 3669, 341, 565, 11], "temperature": 0.0, "avg_logprob": -0.1695553706242488, "compression_ratio": 1.4880382775119618, "no_speech_prob": 1.6964042515610345e-05}, {"id": 1458, "seek": 716058, "start": 7168.82, "end": 7172.58, "text": " and so here's an example of the word vector for King.", "tokens": [293, 370, 510, 311, 364, 1365, 295, 264, 1349, 8062, 337, 3819, 13], "temperature": 0.0, "avg_logprob": -0.1695553706242488, "compression_ratio": 1.4880382775119618, "no_speech_prob": 1.6964042515610345e-05}, {"id": 1459, "seek": 716058, "start": 7172.58, "end": 7176.28, "text": " It's just 300 numbers.", "tokens": [467, 311, 445, 6641, 3547, 13], "temperature": 0.0, "avg_logprob": -0.1695553706242488, "compression_ratio": 1.4880382775119618, "no_speech_prob": 1.6964042515610345e-05}, {"id": 1460, "seek": 716058, "start": 7176.28, "end": 7182.46, "text": " So for example, little j Jeremy and big J Jeremy have a correlation of.6, I don't like", "tokens": [407, 337, 1365, 11, 707, 361, 17809, 293, 955, 508, 17809, 362, 257, 20009, 295, 2411, 21, 11, 286, 500, 380, 411], "temperature": 0.0, "avg_logprob": -0.1695553706242488, "compression_ratio": 1.4880382775119618, "no_speech_prob": 1.6964042515610345e-05}, {"id": 1461, "seek": 716058, "start": 7182.46, "end": 7186.66, "text": " bananas at all, this is good, banana, and Jeremy,.14.", "tokens": [22742, 412, 439, 11, 341, 307, 665, 11, 14194, 11, 293, 17809, 11, 2411, 7271, 13], "temperature": 0.0, "avg_logprob": -0.1695553706242488, "compression_ratio": 1.4880382775119618, "no_speech_prob": 1.6964042515610345e-05}, {"id": 1462, "seek": 718666, "start": 7186.66, "end": 7192.139999999999, "text": " So words that you would expect to be correlated are correlated in words that should be as", "tokens": [407, 2283, 300, 291, 576, 2066, 281, 312, 38574, 366, 38574, 294, 2283, 300, 820, 312, 382], "temperature": 0.0, "avg_logprob": -0.15197969366002967, "compression_ratio": 1.5742574257425743, "no_speech_prob": 5.255354153632652e-06}, {"id": 1463, "seek": 718666, "start": 7192.139999999999, "end": 7193.94, "text": " far away from each other as possible.", "tokens": [1400, 1314, 490, 1184, 661, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.15197969366002967, "compression_ratio": 1.5742574257425743, "no_speech_prob": 5.255354153632652e-06}, {"id": 1464, "seek": 718666, "start": 7193.94, "end": 7197.18, "text": " Unfortunately they're still slightly correlated, but not so much.", "tokens": [8590, 436, 434, 920, 4748, 38574, 11, 457, 406, 370, 709, 13], "temperature": 0.0, "avg_logprob": -0.15197969366002967, "compression_ratio": 1.5742574257425743, "no_speech_prob": 5.255354153632652e-06}, {"id": 1465, "seek": 718666, "start": 7197.18, "end": 7208.0199999999995, "text": " So let's now grab all of the ImageNet classes, because we actually want to know which one's", "tokens": [407, 718, 311, 586, 4444, 439, 295, 264, 29903, 31890, 5359, 11, 570, 321, 767, 528, 281, 458, 597, 472, 311], "temperature": 0.0, "avg_logprob": -0.15197969366002967, "compression_ratio": 1.5742574257425743, "no_speech_prob": 5.255354153632652e-06}, {"id": 1466, "seek": 718666, "start": 7208.0199999999995, "end": 7210.78, "text": " Corgi and which one's Jumbo Jet.", "tokens": [3925, 7834, 293, 597, 472, 311, 508, 449, 1763, 28730, 13], "temperature": 0.0, "avg_logprob": -0.15197969366002967, "compression_ratio": 1.5742574257425743, "no_speech_prob": 5.255354153632652e-06}, {"id": 1467, "seek": 721078, "start": 7210.78, "end": 7217.62, "text": " So we've got a list of all of those up on files.fast.ai, we can grab them.", "tokens": [407, 321, 600, 658, 257, 1329, 295, 439, 295, 729, 493, 322, 7098, 13, 7011, 13, 1301, 11, 321, 393, 4444, 552, 13], "temperature": 0.0, "avg_logprob": -0.08511301875114441, "compression_ratio": 1.6634615384615385, "no_speech_prob": 7.411215847241692e-06}, {"id": 1468, "seek": 721078, "start": 7217.62, "end": 7223.219999999999, "text": " And let's also grab a list of all of the nouns in English, which I've made available here", "tokens": [400, 718, 311, 611, 4444, 257, 1329, 295, 439, 295, 264, 48184, 294, 3669, 11, 597, 286, 600, 1027, 2435, 510], "temperature": 0.0, "avg_logprob": -0.08511301875114441, "compression_ratio": 1.6634615384615385, "no_speech_prob": 7.411215847241692e-06}, {"id": 1469, "seek": 721078, "start": 7223.219999999999, "end": 7224.66, "text": " as well.", "tokens": [382, 731, 13], "temperature": 0.0, "avg_logprob": -0.08511301875114441, "compression_ratio": 1.6634615384615385, "no_speech_prob": 7.411215847241692e-06}, {"id": 1470, "seek": 721078, "start": 7224.66, "end": 7230.3, "text": " So here are the names of each of the 1000 ImageNet classes, and here are all of the", "tokens": [407, 510, 366, 264, 5288, 295, 1184, 295, 264, 9714, 29903, 31890, 5359, 11, 293, 510, 366, 439, 295, 264], "temperature": 0.0, "avg_logprob": -0.08511301875114441, "compression_ratio": 1.6634615384615385, "no_speech_prob": 7.411215847241692e-06}, {"id": 1471, "seek": 721078, "start": 7230.3, "end": 7239.34, "text": " nouns in English according to WordNet, which is a popular thing for kind of representing", "tokens": [48184, 294, 3669, 4650, 281, 8725, 31890, 11, 597, 307, 257, 3743, 551, 337, 733, 295, 13460], "temperature": 0.0, "avg_logprob": -0.08511301875114441, "compression_ratio": 1.6634615384615385, "no_speech_prob": 7.411215847241692e-06}, {"id": 1472, "seek": 723934, "start": 7239.34, "end": 7241.5, "text": " what words are and aren't.", "tokens": [437, 2283, 366, 293, 3212, 380, 13], "temperature": 0.0, "avg_logprob": -0.19145375206356957, "compression_ratio": 1.5733333333333333, "no_speech_prob": 2.014536948991008e-05}, {"id": 1473, "seek": 723934, "start": 7241.5, "end": 7254.34, "text": " So we can now go ahead and load that list of nouns, load the list of ImageNet classes,", "tokens": [407, 321, 393, 586, 352, 2286, 293, 3677, 300, 1329, 295, 48184, 11, 3677, 264, 1329, 295, 29903, 31890, 5359, 11], "temperature": 0.0, "avg_logprob": -0.19145375206356957, "compression_ratio": 1.5733333333333333, "no_speech_prob": 2.014536948991008e-05}, {"id": 1474, "seek": 723934, "start": 7254.34, "end": 7255.860000000001, "text": " turn that into a dictionary.", "tokens": [1261, 300, 666, 257, 25890, 13], "temperature": 0.0, "avg_logprob": -0.19145375206356957, "compression_ratio": 1.5733333333333333, "no_speech_prob": 2.014536948991008e-05}, {"id": 1475, "seek": 723934, "start": 7255.860000000001, "end": 7263.66, "text": " So these are the class IDs for the 1000 ImageNet classes that are in the competition dataset.", "tokens": [407, 613, 366, 264, 1508, 48212, 337, 264, 9714, 29903, 31890, 5359, 300, 366, 294, 264, 6211, 28872, 13], "temperature": 0.0, "avg_logprob": -0.19145375206356957, "compression_ratio": 1.5733333333333333, "no_speech_prob": 2.014536948991008e-05}, {"id": 1476, "seek": 726366, "start": 7263.66, "end": 7272.0199999999995, "text": " So here's an example, n01 is a tench, which apparently is a kind of fish.", "tokens": [407, 510, 311, 364, 1365, 11, 297, 10607, 307, 257, 2064, 339, 11, 597, 7970, 307, 257, 733, 295, 3506, 13], "temperature": 0.0, "avg_logprob": -0.2177167036095444, "compression_ratio": 1.5594713656387664, "no_speech_prob": 1.6442270862171426e-05}, {"id": 1477, "seek": 726366, "start": 7272.0199999999995, "end": 7274.86, "text": " Let's do the same thing for all those WordNet nouns.", "tokens": [961, 311, 360, 264, 912, 551, 337, 439, 729, 8725, 31890, 48184, 13], "temperature": 0.0, "avg_logprob": -0.2177167036095444, "compression_ratio": 1.5594713656387664, "no_speech_prob": 1.6442270862171426e-05}, {"id": 1478, "seek": 726366, "start": 7274.86, "end": 7281.18, "text": " And you can see, actually it turns out that ImageNet is using WordNet class names, so", "tokens": [400, 291, 393, 536, 11, 767, 309, 4523, 484, 300, 29903, 31890, 307, 1228, 8725, 31890, 1508, 5288, 11, 370], "temperature": 0.0, "avg_logprob": -0.2177167036095444, "compression_ratio": 1.5594713656387664, "no_speech_prob": 1.6442270862171426e-05}, {"id": 1479, "seek": 726366, "start": 7281.18, "end": 7285.62, "text": " that makes it nice and easy to map between the two.", "tokens": [300, 1669, 309, 1481, 293, 1858, 281, 4471, 1296, 264, 732, 13], "temperature": 0.0, "avg_logprob": -0.2177167036095444, "compression_ratio": 1.5594713656387664, "no_speech_prob": 1.6442270862171426e-05}, {"id": 1480, "seek": 726366, "start": 7285.62, "end": 7291.74, "text": " And WordNet, the most basic thing is an entity, and then that includes an abstraction, or", "tokens": [400, 8725, 31890, 11, 264, 881, 3875, 551, 307, 364, 13977, 11, 293, 550, 300, 5974, 364, 37765, 11, 420], "temperature": 0.0, "avg_logprob": -0.2177167036095444, "compression_ratio": 1.5594713656387664, "no_speech_prob": 1.6442270862171426e-05}, {"id": 1481, "seek": 729174, "start": 7291.74, "end": 7295.099999999999, "text": " a physical entity can be an object, and so forth.", "tokens": [257, 4001, 13977, 393, 312, 364, 2657, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.17744810125801, "compression_ratio": 1.5025125628140703, "no_speech_prob": 4.710836037702393e-06}, {"id": 1482, "seek": 729174, "start": 7295.099999999999, "end": 7300.86, "text": " So these are our two worlds, we've got the ImageNet 1000 and we've got the 82000, which", "tokens": [407, 613, 366, 527, 732, 13401, 11, 321, 600, 658, 264, 29903, 31890, 9714, 293, 321, 600, 658, 264, 29097, 1360, 11, 597], "temperature": 0.0, "avg_logprob": -0.17744810125801, "compression_ratio": 1.5025125628140703, "no_speech_prob": 4.710836037702393e-06}, {"id": 1483, "seek": 729174, "start": 7300.86, "end": 7303.5599999999995, "text": " are in WordNet.", "tokens": [366, 294, 8725, 31890, 13], "temperature": 0.0, "avg_logprob": -0.17744810125801, "compression_ratio": 1.5025125628140703, "no_speech_prob": 4.710836037702393e-06}, {"id": 1484, "seek": 729174, "start": 7303.5599999999995, "end": 7307.3, "text": " So we want to map the two together, which is as simple as creating a couple of dictionaries", "tokens": [407, 321, 528, 281, 4471, 264, 732, 1214, 11, 597, 307, 382, 2199, 382, 4084, 257, 1916, 295, 22352, 4889], "temperature": 0.0, "avg_logprob": -0.17744810125801, "compression_ratio": 1.5025125628140703, "no_speech_prob": 4.710836037702393e-06}, {"id": 1485, "seek": 729174, "start": 7307.3, "end": 7312.139999999999, "text": " to map them based on the SYNSET ID or the WordNet ID.", "tokens": [281, 4471, 552, 2361, 322, 264, 318, 22315, 50, 4850, 7348, 420, 264, 8725, 31890, 7348, 13], "temperature": 0.0, "avg_logprob": -0.17744810125801, "compression_ratio": 1.5025125628140703, "no_speech_prob": 4.710836037702393e-06}, {"id": 1486, "seek": 731214, "start": 7312.14, "end": 7333.18, "text": " It turns out that 49469, so SYNSET to WordVector.", "tokens": [467, 4523, 484, 300, 16513, 16169, 24, 11, 370, 318, 22315, 50, 4850, 281, 8725, 53, 20814, 13], "temperature": 0.0, "avg_logprob": -0.19804175003715183, "compression_ratio": 1.1260504201680672, "no_speech_prob": 2.5465926228207536e-05}, {"id": 1487, "seek": 731214, "start": 7333.18, "end": 7340.9400000000005, "text": " So what I need to do now is grab the 82000 nouns in WordNet and try and look them up", "tokens": [407, 437, 286, 643, 281, 360, 586, 307, 4444, 264, 29097, 1360, 48184, 294, 8725, 31890, 293, 853, 293, 574, 552, 493], "temperature": 0.0, "avg_logprob": -0.19804175003715183, "compression_ratio": 1.1260504201680672, "no_speech_prob": 2.5465926228207536e-05}, {"id": 1488, "seek": 734094, "start": 7340.94, "end": 7343.179999999999, "text": " in FastText.", "tokens": [294, 15968, 50198, 13], "temperature": 0.0, "avg_logprob": -0.15443663163618607, "compression_ratio": 1.440251572327044, "no_speech_prob": 1.618737223907374e-05}, {"id": 1489, "seek": 734094, "start": 7343.179999999999, "end": 7347.94, "text": " And so I've managed to look up 49000 of them in FastText.", "tokens": [400, 370, 286, 600, 6453, 281, 574, 493, 16513, 1360, 295, 552, 294, 15968, 50198, 13], "temperature": 0.0, "avg_logprob": -0.15443663163618607, "compression_ratio": 1.440251572327044, "no_speech_prob": 1.618737223907374e-05}, {"id": 1490, "seek": 734094, "start": 7347.94, "end": 7353.599999999999, "text": " So I've now got a dictionary that goes from SYNSET ID, which is what WordNet calls them,", "tokens": [407, 286, 600, 586, 658, 257, 25890, 300, 1709, 490, 318, 22315, 50, 4850, 7348, 11, 597, 307, 437, 8725, 31890, 5498, 552, 11], "temperature": 0.0, "avg_logprob": -0.15443663163618607, "compression_ratio": 1.440251572327044, "no_speech_prob": 1.618737223907374e-05}, {"id": 1491, "seek": 734094, "start": 7353.599999999999, "end": 7354.599999999999, "text": " to WordVectors.", "tokens": [281, 8725, 53, 557, 830, 13], "temperature": 0.0, "avg_logprob": -0.15443663163618607, "compression_ratio": 1.440251572327044, "no_speech_prob": 1.618737223907374e-05}, {"id": 1492, "seek": 734094, "start": 7354.599999999999, "end": 7361.78, "text": " So that's what this dictionary is, SYN to WordVector.", "tokens": [407, 300, 311, 437, 341, 25890, 307, 11, 318, 22315, 281, 8725, 53, 20814, 13], "temperature": 0.0, "avg_logprob": -0.15443663163618607, "compression_ratio": 1.440251572327044, "no_speech_prob": 1.618737223907374e-05}, {"id": 1493, "seek": 736178, "start": 7361.78, "end": 7370.98, "text": " And I've also got the same thing specifically for the 1000 WordNet classes.", "tokens": [400, 286, 600, 611, 658, 264, 912, 551, 4682, 337, 264, 9714, 8725, 31890, 5359, 13], "temperature": 0.0, "avg_logprob": -0.1563246691668475, "compression_ratio": 1.4976076555023923, "no_speech_prob": 5.338093615137041e-06}, {"id": 1494, "seek": 736178, "start": 7370.98, "end": 7374.62, "text": " So save them away, that's fine.", "tokens": [407, 3155, 552, 1314, 11, 300, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.1563246691668475, "compression_ratio": 1.4976076555023923, "no_speech_prob": 5.338093615137041e-06}, {"id": 1495, "seek": 736178, "start": 7374.62, "end": 7380.38, "text": " Now I grab all of ImageNet, which you can actually download from Kaggle now.", "tokens": [823, 286, 4444, 439, 295, 29903, 31890, 11, 597, 291, 393, 767, 5484, 490, 48751, 22631, 586, 13], "temperature": 0.0, "avg_logprob": -0.1563246691668475, "compression_ratio": 1.4976076555023923, "no_speech_prob": 5.338093615137041e-06}, {"id": 1496, "seek": 736178, "start": 7380.38, "end": 7385.9, "text": " If you look up the Kaggle ImageNet localization competition, that contains the entirety of", "tokens": [759, 291, 574, 493, 264, 48751, 22631, 29903, 31890, 2654, 2144, 6211, 11, 300, 8306, 264, 31557, 295], "temperature": 0.0, "avg_logprob": -0.1563246691668475, "compression_ratio": 1.4976076555023923, "no_speech_prob": 5.338093615137041e-06}, {"id": 1497, "seek": 736178, "start": 7385.9, "end": 7388.219999999999, "text": " the ImageNet classifications as well.", "tokens": [264, 29903, 31890, 1508, 7833, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1563246691668475, "compression_ratio": 1.4976076555023923, "no_speech_prob": 5.338093615137041e-06}, {"id": 1498, "seek": 738822, "start": 7388.22, "end": 7395.820000000001, "text": " It's got a validation set of 28650 items in it.", "tokens": [467, 311, 658, 257, 24071, 992, 295, 7562, 21, 2803, 4754, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.12955131530761718, "compression_ratio": 1.4, "no_speech_prob": 3.7852980767638655e-06}, {"id": 1499, "seek": 738822, "start": 7395.820000000001, "end": 7403.46, "text": " And so I can basically just grab for every image in ImageNet, I can grab using that SYNSET", "tokens": [400, 370, 286, 393, 1936, 445, 4444, 337, 633, 3256, 294, 29903, 31890, 11, 286, 393, 4444, 1228, 300, 318, 22315, 50, 4850], "temperature": 0.0, "avg_logprob": -0.12955131530761718, "compression_ratio": 1.4, "no_speech_prob": 3.7852980767638655e-06}, {"id": 1500, "seek": 738822, "start": 7403.46, "end": 7415.240000000001, "text": " to WordVector, grab its FastText WordVector, and I can now stick that into this ImageVectors", "tokens": [281, 8725, 53, 20814, 11, 4444, 1080, 15968, 50198, 8725, 53, 20814, 11, 293, 286, 393, 586, 2897, 300, 666, 341, 29903, 53, 557, 830], "temperature": 0.0, "avg_logprob": -0.12955131530761718, "compression_ratio": 1.4, "no_speech_prob": 3.7852980767638655e-06}, {"id": 1501, "seek": 741524, "start": 7415.24, "end": 7423.3, "text": " array, stack that all up into a single matrix, and save that away.", "tokens": [10225, 11, 8630, 300, 439, 493, 666, 257, 2167, 8141, 11, 293, 3155, 300, 1314, 13], "temperature": 0.0, "avg_logprob": -0.10124681025375555, "compression_ratio": 1.4489795918367347, "no_speech_prob": 5.338132723409217e-06}, {"id": 1502, "seek": 741524, "start": 7423.3, "end": 7429.7, "text": " And so now what I've got is something for every ImageNet image.", "tokens": [400, 370, 586, 437, 286, 600, 658, 307, 746, 337, 633, 29903, 31890, 3256, 13], "temperature": 0.0, "avg_logprob": -0.10124681025375555, "compression_ratio": 1.4489795918367347, "no_speech_prob": 5.338132723409217e-06}, {"id": 1503, "seek": 741524, "start": 7429.7, "end": 7435.54, "text": " I've also got the FastText WordVector that it's associated with.", "tokens": [286, 600, 611, 658, 264, 15968, 50198, 8725, 53, 20814, 300, 309, 311, 6615, 365, 13], "temperature": 0.0, "avg_logprob": -0.10124681025375555, "compression_ratio": 1.4489795918367347, "no_speech_prob": 5.338132723409217e-06}, {"id": 1504, "seek": 741524, "start": 7435.54, "end": 7443.34, "text": " Just by looking up the SYNSET ID, going to WordNet, then going to FastText, and grabbing", "tokens": [1449, 538, 1237, 493, 264, 318, 22315, 50, 4850, 7348, 11, 516, 281, 8725, 31890, 11, 550, 516, 281, 15968, 50198, 11, 293, 23771], "temperature": 0.0, "avg_logprob": -0.10124681025375555, "compression_ratio": 1.4489795918367347, "no_speech_prob": 5.338132723409217e-06}, {"id": 1505, "seek": 744334, "start": 7443.34, "end": 7446.62, "text": " the WordVector.", "tokens": [264, 8725, 53, 20814, 13], "temperature": 0.0, "avg_logprob": -0.18857990120941737, "compression_ratio": 1.581896551724138, "no_speech_prob": 3.1875533750280738e-06}, {"id": 1506, "seek": 744334, "start": 7446.62, "end": 7450.38, "text": " And so here's a cool trick.", "tokens": [400, 370, 510, 311, 257, 1627, 4282, 13], "temperature": 0.0, "avg_logprob": -0.18857990120941737, "compression_ratio": 1.581896551724138, "no_speech_prob": 3.1875533750280738e-06}, {"id": 1507, "seek": 744334, "start": 7450.38, "end": 7457.9800000000005, "text": " I can now create a model data object, which specifically is an image classifier data object.", "tokens": [286, 393, 586, 1884, 257, 2316, 1412, 2657, 11, 597, 4682, 307, 364, 3256, 1508, 9902, 1412, 2657, 13], "temperature": 0.0, "avg_logprob": -0.18857990120941737, "compression_ratio": 1.581896551724138, "no_speech_prob": 3.1875533750280738e-06}, {"id": 1508, "seek": 744334, "start": 7457.9800000000005, "end": 7459.78, "text": " And I've got this thing called FromNamesInArray.", "tokens": [400, 286, 600, 658, 341, 551, 1219, 3358, 45, 1632, 4575, 10683, 3458, 13], "temperature": 0.0, "avg_logprob": -0.18857990120941737, "compression_ratio": 1.581896551724138, "no_speech_prob": 3.1875533750280738e-06}, {"id": 1509, "seek": 744334, "start": 7459.78, "end": 7464.42, "text": " I'm not sure if you've used it before, but we can pass it a list of file names, and so", "tokens": [286, 478, 406, 988, 498, 291, 600, 1143, 309, 949, 11, 457, 321, 393, 1320, 309, 257, 1329, 295, 3991, 5288, 11, 293, 370], "temperature": 0.0, "avg_logprob": -0.18857990120941737, "compression_ratio": 1.581896551724138, "no_speech_prob": 3.1875533750280738e-06}, {"id": 1510, "seek": 744334, "start": 7464.42, "end": 7472.34, "text": " these are all of the file names in ImageNet, and we can just pass it an array of our dependent", "tokens": [613, 366, 439, 295, 264, 3991, 5288, 294, 29903, 31890, 11, 293, 321, 393, 445, 1320, 309, 364, 10225, 295, 527, 12334], "temperature": 0.0, "avg_logprob": -0.18857990120941737, "compression_ratio": 1.581896551724138, "no_speech_prob": 3.1875533750280738e-06}, {"id": 1511, "seek": 747234, "start": 7472.34, "end": 7473.34, "text": " files.", "tokens": [7098, 13], "temperature": 0.0, "avg_logprob": -0.1427859834262303, "compression_ratio": 1.628691983122363, "no_speech_prob": 1.9637961941043613e-06}, {"id": 1512, "seek": 747234, "start": 7473.34, "end": 7478.18, "text": " And so this is all of the FastText WordVectors.", "tokens": [400, 370, 341, 307, 439, 295, 264, 15968, 50198, 8725, 53, 557, 830, 13], "temperature": 0.0, "avg_logprob": -0.1427859834262303, "compression_ratio": 1.628691983122363, "no_speech_prob": 1.9637961941043613e-06}, {"id": 1513, "seek": 747234, "start": 7478.18, "end": 7483.82, "text": " And then I can pass in the validation indexes, which in this case is just all of the last", "tokens": [400, 550, 286, 393, 1320, 294, 264, 24071, 8186, 279, 11, 597, 294, 341, 1389, 307, 445, 439, 295, 264, 1036], "temperature": 0.0, "avg_logprob": -0.1427859834262303, "compression_ratio": 1.628691983122363, "no_speech_prob": 1.9637961941043613e-06}, {"id": 1514, "seek": 747234, "start": 7483.82, "end": 7484.82, "text": " IDs.", "tokens": [48212, 13], "temperature": 0.0, "avg_logprob": -0.1427859834262303, "compression_ratio": 1.628691983122363, "no_speech_prob": 1.9637961941043613e-06}, {"id": 1515, "seek": 747234, "start": 7484.82, "end": 7490.14, "text": " I need to make sure that they're the same as ImageNet uses, otherwise I'll be cheating.", "tokens": [286, 643, 281, 652, 988, 300, 436, 434, 264, 912, 382, 29903, 31890, 4960, 11, 5911, 286, 603, 312, 18309, 13], "temperature": 0.0, "avg_logprob": -0.1427859834262303, "compression_ratio": 1.628691983122363, "no_speech_prob": 1.9637961941043613e-06}, {"id": 1516, "seek": 747234, "start": 7490.14, "end": 7495.54, "text": " And then I pass in continuous equals true, which means this puts a lie again to this", "tokens": [400, 550, 286, 1320, 294, 10957, 6915, 2074, 11, 597, 1355, 341, 8137, 257, 4544, 797, 281, 341], "temperature": 0.0, "avg_logprob": -0.1427859834262303, "compression_ratio": 1.628691983122363, "no_speech_prob": 1.9637961941043613e-06}, {"id": 1517, "seek": 747234, "start": 7495.54, "end": 7496.54, "text": " image classifier data.", "tokens": [3256, 1508, 9902, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1427859834262303, "compression_ratio": 1.628691983122363, "no_speech_prob": 1.9637961941043613e-06}, {"id": 1518, "seek": 747234, "start": 7496.54, "end": 7499.08, "text": " It's now really an image regressor data.", "tokens": [467, 311, 586, 534, 364, 3256, 1121, 735, 284, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1427859834262303, "compression_ratio": 1.628691983122363, "no_speech_prob": 1.9637961941043613e-06}, {"id": 1519, "seek": 749908, "start": 7499.08, "end": 7504.78, "text": " So continuous equals true means don't one-hot encode my outputs, but treat them just as", "tokens": [407, 10957, 6915, 2074, 1355, 500, 380, 472, 12, 12194, 2058, 1429, 452, 23930, 11, 457, 2387, 552, 445, 382], "temperature": 0.0, "avg_logprob": -0.11719566692005504, "compression_ratio": 1.6595744680851063, "no_speech_prob": 6.893605473123898e-07}, {"id": 1520, "seek": 749908, "start": 7504.78, "end": 7506.6, "text": " continuous values.", "tokens": [10957, 4190, 13], "temperature": 0.0, "avg_logprob": -0.11719566692005504, "compression_ratio": 1.6595744680851063, "no_speech_prob": 6.893605473123898e-07}, {"id": 1521, "seek": 749908, "start": 7506.6, "end": 7512.94, "text": " So now I've got a model data object that contains all of my file names, and for every file name,", "tokens": [407, 586, 286, 600, 658, 257, 2316, 1412, 2657, 300, 8306, 439, 295, 452, 3991, 5288, 11, 293, 337, 633, 3991, 1315, 11], "temperature": 0.0, "avg_logprob": -0.11719566692005504, "compression_ratio": 1.6595744680851063, "no_speech_prob": 6.893605473123898e-07}, {"id": 1522, "seek": 749908, "start": 7512.94, "end": 7517.54, "text": " a continuous array representing the WordVector for that.", "tokens": [257, 10957, 10225, 13460, 264, 8725, 53, 20814, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.11719566692005504, "compression_ratio": 1.6595744680851063, "no_speech_prob": 6.893605473123898e-07}, {"id": 1523, "seek": 749908, "start": 7517.54, "end": 7521.12, "text": " So I have an X, I have a Y, so I have data.", "tokens": [407, 286, 362, 364, 1783, 11, 286, 362, 257, 398, 11, 370, 286, 362, 1412, 13], "temperature": 0.0, "avg_logprob": -0.11719566692005504, "compression_ratio": 1.6595744680851063, "no_speech_prob": 6.893605473123898e-07}, {"id": 1524, "seek": 749908, "start": 7521.12, "end": 7523.98, "text": " Now I need an architecture and a loss function.", "tokens": [823, 286, 643, 364, 9482, 293, 257, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.11719566692005504, "compression_ratio": 1.6595744680851063, "no_speech_prob": 6.893605473123898e-07}, {"id": 1525, "seek": 749908, "start": 7523.98, "end": 7526.42, "text": " Once I've got that, I should be done.", "tokens": [3443, 286, 600, 658, 300, 11, 286, 820, 312, 1096, 13], "temperature": 0.0, "avg_logprob": -0.11719566692005504, "compression_ratio": 1.6595744680851063, "no_speech_prob": 6.893605473123898e-07}, {"id": 1526, "seek": 752642, "start": 7526.42, "end": 7532.74, "text": " So let's create an architecture, and so we'll revise this next week.", "tokens": [407, 718, 311, 1884, 364, 9482, 11, 293, 370, 321, 603, 44252, 341, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.23352060732634172, "compression_ratio": 1.5991735537190082, "no_speech_prob": 9.080410563910846e-06}, {"id": 1527, "seek": 752642, "start": 7532.74, "end": 7537.14, "text": " But basically we can use the tricks we've learned so far, but it's actually incredibly", "tokens": [583, 1936, 321, 393, 764, 264, 11733, 321, 600, 3264, 370, 1400, 11, 457, 309, 311, 767, 6252], "temperature": 0.0, "avg_logprob": -0.23352060732634172, "compression_ratio": 1.5991735537190082, "no_speech_prob": 9.080410563910846e-06}, {"id": 1528, "seek": 752642, "start": 7537.14, "end": 7538.14, "text": " simple.", "tokens": [2199, 13], "temperature": 0.0, "avg_logprob": -0.23352060732634172, "compression_ratio": 1.5991735537190082, "no_speech_prob": 9.080410563910846e-06}, {"id": 1529, "seek": 752642, "start": 7538.14, "end": 7546.54, "text": " FastAI has a ConvLearner builder, which is what when you say ConvLearner.pre-trained,", "tokens": [15968, 48698, 575, 257, 2656, 85, 11020, 22916, 27377, 11, 597, 307, 437, 562, 291, 584, 2656, 85, 11020, 22916, 13, 3712, 12, 17227, 2001, 11], "temperature": 0.0, "avg_logprob": -0.23352060732634172, "compression_ratio": 1.5991735537190082, "no_speech_prob": 9.080410563910846e-06}, {"id": 1530, "seek": 752642, "start": 7546.54, "end": 7548.34, "text": " it calls this.", "tokens": [309, 5498, 341, 13], "temperature": 0.0, "avg_logprob": -0.23352060732634172, "compression_ratio": 1.5991735537190082, "no_speech_prob": 9.080410563910846e-06}, {"id": 1531, "seek": 752642, "start": 7548.34, "end": 7551.06, "text": " And you basically say, okay, what architecture do you want?", "tokens": [400, 291, 1936, 584, 11, 1392, 11, 437, 9482, 360, 291, 528, 30], "temperature": 0.0, "avg_logprob": -0.23352060732634172, "compression_ratio": 1.5991735537190082, "no_speech_prob": 9.080410563910846e-06}, {"id": 1532, "seek": 752642, "start": 7551.06, "end": 7554.5, "text": " So we're going to use ResNet-50.", "tokens": [407, 321, 434, 516, 281, 764, 5015, 31890, 12, 2803, 13], "temperature": 0.0, "avg_logprob": -0.23352060732634172, "compression_ratio": 1.5991735537190082, "no_speech_prob": 9.080410563910846e-06}, {"id": 1533, "seek": 752642, "start": 7554.5, "end": 7556.06, "text": " How many classes do you want?", "tokens": [1012, 867, 5359, 360, 291, 528, 30], "temperature": 0.0, "avg_logprob": -0.23352060732634172, "compression_ratio": 1.5991735537190082, "no_speech_prob": 9.080410563910846e-06}, {"id": 1534, "seek": 755606, "start": 7556.06, "end": 7560.34, "text": " In this case, it's not really classes, it's how many outputs do you want, which is the", "tokens": [682, 341, 1389, 11, 309, 311, 406, 534, 5359, 11, 309, 311, 577, 867, 23930, 360, 291, 528, 11, 597, 307, 264], "temperature": 0.0, "avg_logprob": -0.19027393147096797, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.6701287677278742e-05}, {"id": 1535, "seek": 755606, "start": 7560.34, "end": 7564.820000000001, "text": " length of the FastText WordVector, 300.", "tokens": [4641, 295, 264, 15968, 50198, 8725, 53, 20814, 11, 6641, 13], "temperature": 0.0, "avg_logprob": -0.19027393147096797, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.6701287677278742e-05}, {"id": 1536, "seek": 755606, "start": 7564.820000000001, "end": 7568.3, "text": " Obviously it's not multi-class classification, it's not classification at all.", "tokens": [7580, 309, 311, 406, 4825, 12, 11665, 21538, 11, 309, 311, 406, 21538, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.19027393147096797, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.6701287677278742e-05}, {"id": 1537, "seek": 755606, "start": 7568.3, "end": 7569.3, "text": " Is it regression?", "tokens": [1119, 309, 24590, 30], "temperature": 0.0, "avg_logprob": -0.19027393147096797, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.6701287677278742e-05}, {"id": 1538, "seek": 755606, "start": 7569.3, "end": 7572.54, "text": " Yes, it is regression.", "tokens": [1079, 11, 309, 307, 24590, 13], "temperature": 0.0, "avg_logprob": -0.19027393147096797, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.6701287677278742e-05}, {"id": 1539, "seek": 755606, "start": 7572.54, "end": 7576.900000000001, "text": " And then you can just say, all right, what fully connected layers do you want?", "tokens": [400, 550, 291, 393, 445, 584, 11, 439, 558, 11, 437, 4498, 4582, 7914, 360, 291, 528, 30], "temperature": 0.0, "avg_logprob": -0.19027393147096797, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.6701287677278742e-05}, {"id": 1540, "seek": 755606, "start": 7576.900000000001, "end": 7581.780000000001, "text": " So I'm just going to add one fully connected layer, hidden layer of length 1024.", "tokens": [407, 286, 478, 445, 516, 281, 909, 472, 4498, 4582, 4583, 11, 7633, 4583, 295, 4641, 1266, 7911, 13], "temperature": 0.0, "avg_logprob": -0.19027393147096797, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.6701287677278742e-05}, {"id": 1541, "seek": 755606, "start": 7581.780000000001, "end": 7582.780000000001, "text": " Why 1024?", "tokens": [1545, 1266, 7911, 30], "temperature": 0.0, "avg_logprob": -0.19027393147096797, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.6701287677278742e-05}, {"id": 1542, "seek": 758278, "start": 7582.78, "end": 7591.86, "text": " Well, I've got the last layer of ResNet-50 is I think is 1024 long.", "tokens": [1042, 11, 286, 600, 658, 264, 1036, 4583, 295, 5015, 31890, 12, 2803, 307, 286, 519, 307, 1266, 7911, 938, 13], "temperature": 0.0, "avg_logprob": -0.1541187787296796, "compression_ratio": 1.4444444444444444, "no_speech_prob": 4.565957624436123e-06}, {"id": 1543, "seek": 758278, "start": 7591.86, "end": 7594.86, "text": " The final output I need is 300 long.", "tokens": [440, 2572, 5598, 286, 643, 307, 6641, 938, 13], "temperature": 0.0, "avg_logprob": -0.1541187787296796, "compression_ratio": 1.4444444444444444, "no_speech_prob": 4.565957624436123e-06}, {"id": 1544, "seek": 758278, "start": 7594.86, "end": 7599.179999999999, "text": " I obviously need my penultimate layer to be longer than 300, otherwise there's not enough", "tokens": [286, 2745, 643, 452, 3435, 723, 2905, 4583, 281, 312, 2854, 813, 6641, 11, 5911, 456, 311, 406, 1547], "temperature": 0.0, "avg_logprob": -0.1541187787296796, "compression_ratio": 1.4444444444444444, "no_speech_prob": 4.565957624436123e-06}, {"id": 1545, "seek": 758278, "start": 7599.179999999999, "end": 7600.34, "text": " information.", "tokens": [1589, 13], "temperature": 0.0, "avg_logprob": -0.1541187787296796, "compression_ratio": 1.4444444444444444, "no_speech_prob": 4.565957624436123e-06}, {"id": 1546, "seek": 758278, "start": 7600.34, "end": 7602.78, "text": " So I kind of just picked something a bit bigger.", "tokens": [407, 286, 733, 295, 445, 6183, 746, 257, 857, 3801, 13], "temperature": 0.0, "avg_logprob": -0.1541187787296796, "compression_ratio": 1.4444444444444444, "no_speech_prob": 4.565957624436123e-06}, {"id": 1547, "seek": 758278, "start": 7602.78, "end": 7607.259999999999, "text": " Maybe different numbers would be better, but this worked for me.", "tokens": [2704, 819, 3547, 576, 312, 1101, 11, 457, 341, 2732, 337, 385, 13], "temperature": 0.0, "avg_logprob": -0.1541187787296796, "compression_ratio": 1.4444444444444444, "no_speech_prob": 4.565957624436123e-06}, {"id": 1548, "seek": 758278, "start": 7607.259999999999, "end": 7608.259999999999, "text": " How much dropout do you want?", "tokens": [1012, 709, 3270, 346, 360, 291, 528, 30], "temperature": 0.0, "avg_logprob": -0.1541187787296796, "compression_ratio": 1.4444444444444444, "no_speech_prob": 4.565957624436123e-06}, {"id": 1549, "seek": 760826, "start": 7608.26, "end": 7613.46, "text": " I found that the default dropout I was consistently underfitting, so I just decreased the dropout", "tokens": [286, 1352, 300, 264, 7576, 3270, 346, 286, 390, 14961, 833, 69, 2414, 11, 370, 286, 445, 24436, 264, 3270, 346], "temperature": 0.0, "avg_logprob": -0.1634180152809227, "compression_ratio": 1.5285714285714285, "no_speech_prob": 3.0894848350726534e-06}, {"id": 1550, "seek": 760826, "start": 7613.46, "end": 7616.54, "text": " from 0.5 to 0.2.", "tokens": [490, 1958, 13, 20, 281, 1958, 13, 17, 13], "temperature": 0.0, "avg_logprob": -0.1634180152809227, "compression_ratio": 1.5285714285714285, "no_speech_prob": 3.0894848350726534e-06}, {"id": 1551, "seek": 760826, "start": 7616.54, "end": 7623.06, "text": " And so this is now a convolutional neural network that does not have any softmax or", "tokens": [400, 370, 341, 307, 586, 257, 45216, 304, 18161, 3209, 300, 775, 406, 362, 604, 2787, 41167, 420], "temperature": 0.0, "avg_logprob": -0.1634180152809227, "compression_ratio": 1.5285714285714285, "no_speech_prob": 3.0894848350726534e-06}, {"id": 1552, "seek": 760826, "start": 7623.06, "end": 7628.1, "text": " anything like that because it's regression, it's just a linear layer at the end.", "tokens": [1340, 411, 300, 570, 309, 311, 24590, 11, 309, 311, 445, 257, 8213, 4583, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.1634180152809227, "compression_ratio": 1.5285714285714285, "no_speech_prob": 3.0894848350726534e-06}, {"id": 1553, "seek": 760826, "start": 7628.1, "end": 7631.780000000001, "text": " And that's basically it.", "tokens": [400, 300, 311, 1936, 309, 13], "temperature": 0.0, "avg_logprob": -0.1634180152809227, "compression_ratio": 1.5285714285714285, "no_speech_prob": 3.0894848350726534e-06}, {"id": 1554, "seek": 760826, "start": 7631.780000000001, "end": 7634.72, "text": " That's my model.", "tokens": [663, 311, 452, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1634180152809227, "compression_ratio": 1.5285714285714285, "no_speech_prob": 3.0894848350726534e-06}, {"id": 1555, "seek": 763472, "start": 7634.72, "end": 7641.5, "text": " So I can create a conv learner from that model, give it an optimization function.", "tokens": [407, 286, 393, 1884, 257, 3754, 33347, 490, 300, 2316, 11, 976, 309, 364, 19618, 2445, 13], "temperature": 0.0, "avg_logprob": -0.12421609067368782, "compression_ratio": 1.7311827956989247, "no_speech_prob": 6.240911716304254e-06}, {"id": 1556, "seek": 763472, "start": 7641.5, "end": 7645.42, "text": " So now all I need, I've got data, I've got an architecture.", "tokens": [407, 586, 439, 286, 643, 11, 286, 600, 658, 1412, 11, 286, 600, 658, 364, 9482, 13], "temperature": 0.0, "avg_logprob": -0.12421609067368782, "compression_ratio": 1.7311827956989247, "no_speech_prob": 6.240911716304254e-06}, {"id": 1557, "seek": 763472, "start": 7645.42, "end": 7650.820000000001, "text": " So the architecture, because I said I've got this many 300 outputs, it knows that there", "tokens": [407, 264, 9482, 11, 570, 286, 848, 286, 600, 658, 341, 867, 6641, 23930, 11, 309, 3255, 300, 456], "temperature": 0.0, "avg_logprob": -0.12421609067368782, "compression_ratio": 1.7311827956989247, "no_speech_prob": 6.240911716304254e-06}, {"id": 1558, "seek": 763472, "start": 7650.820000000001, "end": 7656.8, "text": " are 300 outputs because that's the size of this array.", "tokens": [366, 6641, 23930, 570, 300, 311, 264, 2744, 295, 341, 10225, 13], "temperature": 0.0, "avg_logprob": -0.12421609067368782, "compression_ratio": 1.7311827956989247, "no_speech_prob": 6.240911716304254e-06}, {"id": 1559, "seek": 763472, "start": 7656.8, "end": 7659.18, "text": " So now all I need is a loss function.", "tokens": [407, 586, 439, 286, 643, 307, 257, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.12421609067368782, "compression_ratio": 1.7311827956989247, "no_speech_prob": 6.240911716304254e-06}, {"id": 1560, "seek": 765918, "start": 7659.18, "end": 7666.3, "text": " Now the default loss function for regression is L1 loss, so the absolute differences.", "tokens": [823, 264, 7576, 4470, 2445, 337, 24590, 307, 441, 16, 4470, 11, 370, 264, 8236, 7300, 13], "temperature": 0.0, "avg_logprob": -0.1331358729182063, "compression_ratio": 1.6243902439024391, "no_speech_prob": 2.9023065053479513e-06}, {"id": 1561, "seek": 765918, "start": 7666.3, "end": 7675.26, "text": " That's not bad, but unfortunately in really high dimensional spaces, anybody who's studied", "tokens": [663, 311, 406, 1578, 11, 457, 7015, 294, 534, 1090, 18795, 7673, 11, 4472, 567, 311, 9454], "temperature": 0.0, "avg_logprob": -0.1331358729182063, "compression_ratio": 1.6243902439024391, "no_speech_prob": 2.9023065053479513e-06}, {"id": 1562, "seek": 765918, "start": 7675.26, "end": 7678.700000000001, "text": " a bit of machine learning probably knows this, in really high dimensional spaces, in this", "tokens": [257, 857, 295, 3479, 2539, 1391, 3255, 341, 11, 294, 534, 1090, 18795, 7673, 11, 294, 341], "temperature": 0.0, "avg_logprob": -0.1331358729182063, "compression_ratio": 1.6243902439024391, "no_speech_prob": 2.9023065053479513e-06}, {"id": 1563, "seek": 765918, "start": 7678.700000000001, "end": 7684.1, "text": " case it's 300 dimensional, basically everything is on the outside.", "tokens": [1389, 309, 311, 6641, 18795, 11, 1936, 1203, 307, 322, 264, 2380, 13], "temperature": 0.0, "avg_logprob": -0.1331358729182063, "compression_ratio": 1.6243902439024391, "no_speech_prob": 2.9023065053479513e-06}, {"id": 1564, "seek": 768410, "start": 7684.1, "end": 7692.02, "text": " And when everything is on the outside, distance is, it's not meaningless, but it's a little", "tokens": [400, 562, 1203, 307, 322, 264, 2380, 11, 4560, 307, 11, 309, 311, 406, 33232, 11, 457, 309, 311, 257, 707], "temperature": 0.0, "avg_logprob": -0.1436327752612886, "compression_ratio": 1.6984924623115578, "no_speech_prob": 3.5008383747481275e-06}, {"id": 1565, "seek": 768410, "start": 7692.02, "end": 7695.02, "text": " bit awkward.", "tokens": [857, 11411, 13], "temperature": 0.0, "avg_logprob": -0.1436327752612886, "compression_ratio": 1.6984924623115578, "no_speech_prob": 3.5008383747481275e-06}, {"id": 1566, "seek": 768410, "start": 7695.02, "end": 7700.02, "text": " Things tend to be close together or far away, it doesn't really mean much in these really", "tokens": [9514, 3928, 281, 312, 1998, 1214, 420, 1400, 1314, 11, 309, 1177, 380, 534, 914, 709, 294, 613, 534], "temperature": 0.0, "avg_logprob": -0.1436327752612886, "compression_ratio": 1.6984924623115578, "no_speech_prob": 3.5008383747481275e-06}, {"id": 1567, "seek": 768410, "start": 7700.02, "end": 7704.4800000000005, "text": " high dimensional spaces where everything is on the edge.", "tokens": [1090, 18795, 7673, 689, 1203, 307, 322, 264, 4691, 13], "temperature": 0.0, "avg_logprob": -0.1436327752612886, "compression_ratio": 1.6984924623115578, "no_speech_prob": 3.5008383747481275e-06}, {"id": 1568, "seek": 768410, "start": 7704.4800000000005, "end": 7709.22, "text": " What does mean something though is that if one thing is on the edge over here, and one", "tokens": [708, 775, 914, 746, 1673, 307, 300, 498, 472, 551, 307, 322, 264, 4691, 670, 510, 11, 293, 472], "temperature": 0.0, "avg_logprob": -0.1436327752612886, "compression_ratio": 1.6984924623115578, "no_speech_prob": 3.5008383747481275e-06}, {"id": 1569, "seek": 770922, "start": 7709.22, "end": 7714.34, "text": " thing is on the edge over here, you can form an angle between those vectors.", "tokens": [551, 307, 322, 264, 4691, 670, 510, 11, 291, 393, 1254, 364, 5802, 1296, 729, 18875, 13], "temperature": 0.0, "avg_logprob": -0.13553106048960745, "compression_ratio": 1.5523809523809524, "no_speech_prob": 6.339099854812957e-06}, {"id": 1570, "seek": 770922, "start": 7714.34, "end": 7716.9400000000005, "text": " The angle is meaningful.", "tokens": [440, 5802, 307, 10995, 13], "temperature": 0.0, "avg_logprob": -0.13553106048960745, "compression_ratio": 1.5523809523809524, "no_speech_prob": 6.339099854812957e-06}, {"id": 1571, "seek": 770922, "start": 7716.9400000000005, "end": 7723.740000000001, "text": " And so that's why we use cosine similarity when we're basically looking for how close", "tokens": [400, 370, 300, 311, 983, 321, 764, 23565, 32194, 562, 321, 434, 1936, 1237, 337, 577, 1998], "temperature": 0.0, "avg_logprob": -0.13553106048960745, "compression_ratio": 1.5523809523809524, "no_speech_prob": 6.339099854812957e-06}, {"id": 1572, "seek": 770922, "start": 7723.740000000001, "end": 7727.58, "text": " or far apart are things in high dimensional spaces.", "tokens": [420, 1400, 4936, 366, 721, 294, 1090, 18795, 7673, 13], "temperature": 0.0, "avg_logprob": -0.13553106048960745, "compression_ratio": 1.5523809523809524, "no_speech_prob": 6.339099854812957e-06}, {"id": 1573, "seek": 770922, "start": 7727.58, "end": 7731.54, "text": " And if you haven't seen cosine similarity before, it's basically the same as Euclidean", "tokens": [400, 498, 291, 2378, 380, 1612, 23565, 32194, 949, 11, 309, 311, 1936, 264, 912, 382, 462, 1311, 31264, 282], "temperature": 0.0, "avg_logprob": -0.13553106048960745, "compression_ratio": 1.5523809523809524, "no_speech_prob": 6.339099854812957e-06}, {"id": 1574, "seek": 773154, "start": 7731.54, "end": 7741.0199999999995, "text": " distance, but it's normalized to be basically a unit norm, so basically divide by the length.", "tokens": [4560, 11, 457, 309, 311, 48704, 281, 312, 1936, 257, 4985, 2026, 11, 370, 1936, 9845, 538, 264, 4641, 13], "temperature": 0.0, "avg_logprob": -0.1383335304260254, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.889155313023366e-06}, {"id": 1575, "seek": 773154, "start": 7741.0199999999995, "end": 7745.3, "text": " So we don't care about the length of the vector, we only care about its angle.", "tokens": [407, 321, 500, 380, 1127, 466, 264, 4641, 295, 264, 8062, 11, 321, 787, 1127, 466, 1080, 5802, 13], "temperature": 0.0, "avg_logprob": -0.1383335304260254, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.889155313023366e-06}, {"id": 1576, "seek": 773154, "start": 7745.3, "end": 7751.62, "text": " So there's a bunch of stuff that you could easily learn in a couple of hours, but if", "tokens": [407, 456, 311, 257, 3840, 295, 1507, 300, 291, 727, 3612, 1466, 294, 257, 1916, 295, 2496, 11, 457, 498], "temperature": 0.0, "avg_logprob": -0.1383335304260254, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.889155313023366e-06}, {"id": 1577, "seek": 773154, "start": 7751.62, "end": 7754.66, "text": " you haven't seen it before, it's a bit mysterious.", "tokens": [291, 2378, 380, 1612, 309, 949, 11, 309, 311, 257, 857, 13831, 13], "temperature": 0.0, "avg_logprob": -0.1383335304260254, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.889155313023366e-06}, {"id": 1578, "seek": 773154, "start": 7754.66, "end": 7758.9, "text": " For now just know that plus functions in high dimensional spaces where you're trying to", "tokens": [1171, 586, 445, 458, 300, 1804, 6828, 294, 1090, 18795, 7673, 689, 291, 434, 1382, 281], "temperature": 0.0, "avg_logprob": -0.1383335304260254, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.889155313023366e-06}, {"id": 1579, "seek": 775890, "start": 7758.9, "end": 7766.379999999999, "text": " find similarity, you care about angle and you don't care about distance.", "tokens": [915, 32194, 11, 291, 1127, 466, 5802, 293, 291, 500, 380, 1127, 466, 4560, 13], "temperature": 0.0, "avg_logprob": -0.12062758258265308, "compression_ratio": 1.6794871794871795, "no_speech_prob": 1.3845902685716283e-05}, {"id": 1580, "seek": 775890, "start": 7766.379999999999, "end": 7769.259999999999, "text": " If you didn't use this custom loss function, it would still work.", "tokens": [759, 291, 994, 380, 764, 341, 2375, 4470, 2445, 11, 309, 576, 920, 589, 13], "temperature": 0.0, "avg_logprob": -0.12062758258265308, "compression_ratio": 1.6794871794871795, "no_speech_prob": 1.3845902685716283e-05}, {"id": 1581, "seek": 775890, "start": 7769.259999999999, "end": 7770.259999999999, "text": " I tried it.", "tokens": [286, 3031, 309, 13], "temperature": 0.0, "avg_logprob": -0.12062758258265308, "compression_ratio": 1.6794871794871795, "no_speech_prob": 1.3845902685716283e-05}, {"id": 1582, "seek": 775890, "start": 7770.259999999999, "end": 7772.5, "text": " It's just a little bit less good.", "tokens": [467, 311, 445, 257, 707, 857, 1570, 665, 13], "temperature": 0.0, "avg_logprob": -0.12062758258265308, "compression_ratio": 1.6794871794871795, "no_speech_prob": 1.3845902685716283e-05}, {"id": 1583, "seek": 775890, "start": 7772.5, "end": 7777.7, "text": " So we've got an architecture, we've got data, we've got a loss function, therefore we're", "tokens": [407, 321, 600, 658, 364, 9482, 11, 321, 600, 658, 1412, 11, 321, 600, 658, 257, 4470, 2445, 11, 4412, 321, 434], "temperature": 0.0, "avg_logprob": -0.12062758258265308, "compression_ratio": 1.6794871794871795, "no_speech_prob": 1.3845902685716283e-05}, {"id": 1584, "seek": 775890, "start": 7777.7, "end": 7779.42, "text": " done.", "tokens": [1096, 13], "temperature": 0.0, "avg_logprob": -0.12062758258265308, "compression_ratio": 1.6794871794871795, "no_speech_prob": 1.3845902685716283e-05}, {"id": 1585, "seek": 775890, "start": 7779.42, "end": 7781.62, "text": " So we can go ahead and fit.", "tokens": [407, 321, 393, 352, 2286, 293, 3318, 13], "temperature": 0.0, "avg_logprob": -0.12062758258265308, "compression_ratio": 1.6794871794871795, "no_speech_prob": 1.3845902685716283e-05}, {"id": 1586, "seek": 775890, "start": 7781.62, "end": 7786.94, "text": " Now I'm training on all of ImageNet, that's going to take a long time, so pre-compute", "tokens": [823, 286, 478, 3097, 322, 439, 295, 29903, 31890, 11, 300, 311, 516, 281, 747, 257, 938, 565, 11, 370, 659, 12, 21541, 1169], "temperature": 0.0, "avg_logprob": -0.12062758258265308, "compression_ratio": 1.6794871794871795, "no_speech_prob": 1.3845902685716283e-05}, {"id": 1587, "seek": 778694, "start": 7786.94, "end": 7788.94, "text": " equals true is your friend.", "tokens": [6915, 2074, 307, 428, 1277, 13], "temperature": 0.0, "avg_logprob": -0.1668999745295598, "compression_ratio": 1.6144067796610169, "no_speech_prob": 4.2228093661833555e-06}, {"id": 1588, "seek": 778694, "start": 7788.94, "end": 7790.46, "text": " You remember pre-compute equals true?", "tokens": [509, 1604, 659, 12, 21541, 1169, 6915, 2074, 30], "temperature": 0.0, "avg_logprob": -0.1668999745295598, "compression_ratio": 1.6144067796610169, "no_speech_prob": 4.2228093661833555e-06}, {"id": 1589, "seek": 778694, "start": 7790.46, "end": 7794.54, "text": " That's that thing we learned ages ago that caches the output of the final convolutional", "tokens": [663, 311, 300, 551, 321, 3264, 12357, 2057, 300, 269, 13272, 264, 5598, 295, 264, 2572, 45216, 304], "temperature": 0.0, "avg_logprob": -0.1668999745295598, "compression_ratio": 1.6144067796610169, "no_speech_prob": 4.2228093661833555e-06}, {"id": 1590, "seek": 778694, "start": 7794.54, "end": 7800.219999999999, "text": " layer and just trains the fully connected bit.", "tokens": [4583, 293, 445, 16329, 264, 4498, 4582, 857, 13], "temperature": 0.0, "avg_logprob": -0.1668999745295598, "compression_ratio": 1.6144067796610169, "no_speech_prob": 4.2228093661833555e-06}, {"id": 1591, "seek": 778694, "start": 7800.219999999999, "end": 7806.58, "text": " And even with pre-compute equals true, it takes like 3 minutes to train an epoch on", "tokens": [400, 754, 365, 659, 12, 21541, 1169, 6915, 2074, 11, 309, 2516, 411, 805, 2077, 281, 3847, 364, 30992, 339, 322], "temperature": 0.0, "avg_logprob": -0.1668999745295598, "compression_ratio": 1.6144067796610169, "no_speech_prob": 4.2228093661833555e-06}, {"id": 1592, "seek": 778694, "start": 7806.58, "end": 7808.66, "text": " all of ImageNet.", "tokens": [439, 295, 29903, 31890, 13], "temperature": 0.0, "avg_logprob": -0.1668999745295598, "compression_ratio": 1.6144067796610169, "no_speech_prob": 4.2228093661833555e-06}, {"id": 1593, "seek": 778694, "start": 7808.66, "end": 7814.139999999999, "text": " So I trained it for a while longer, so that's like an hour's worth of training.", "tokens": [407, 286, 8895, 309, 337, 257, 1339, 2854, 11, 370, 300, 311, 411, 364, 1773, 311, 3163, 295, 3097, 13], "temperature": 0.0, "avg_logprob": -0.1668999745295598, "compression_ratio": 1.6144067796610169, "no_speech_prob": 4.2228093661833555e-06}, {"id": 1594, "seek": 781414, "start": 7814.14, "end": 7820.820000000001, "text": " It's pretty cool that with FastAI, we can train a new custom head on all of ImageNet", "tokens": [467, 311, 1238, 1627, 300, 365, 15968, 48698, 11, 321, 393, 3847, 257, 777, 2375, 1378, 322, 439, 295, 29903, 31890], "temperature": 0.0, "avg_logprob": -0.19552002214405634, "compression_ratio": 1.4411764705882353, "no_speech_prob": 3.393132146811695e-06}, {"id": 1595, "seek": 781414, "start": 7820.820000000001, "end": 7826.3, "text": " for 40 epochs in an hour or so.", "tokens": [337, 3356, 30992, 28346, 294, 364, 1773, 420, 370, 13], "temperature": 0.0, "avg_logprob": -0.19552002214405634, "compression_ratio": 1.4411764705882353, "no_speech_prob": 3.393132146811695e-06}, {"id": 1596, "seek": 781414, "start": 7826.3, "end": 7836.34, "text": " And so at the end of all that, we can now say, let's grab the 1000 ImageNet classes", "tokens": [400, 370, 412, 264, 917, 295, 439, 300, 11, 321, 393, 586, 584, 11, 718, 311, 4444, 264, 9714, 29903, 31890, 5359], "temperature": 0.0, "avg_logprob": -0.19552002214405634, "compression_ratio": 1.4411764705882353, "no_speech_prob": 3.393132146811695e-06}, {"id": 1597, "seek": 781414, "start": 7836.34, "end": 7840.62, "text": " and let's predict on a whole validation set.", "tokens": [293, 718, 311, 6069, 322, 257, 1379, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.19552002214405634, "compression_ratio": 1.4411764705882353, "no_speech_prob": 3.393132146811695e-06}, {"id": 1598, "seek": 784062, "start": 7840.62, "end": 7846.34, "text": " And let's just take a look at a few pictures.", "tokens": [400, 718, 311, 445, 747, 257, 574, 412, 257, 1326, 5242, 13], "temperature": 0.0, "avg_logprob": -0.16227736428519277, "compression_ratio": 1.6952789699570816, "no_speech_prob": 1.0129869224329013e-05}, {"id": 1599, "seek": 784062, "start": 7846.34, "end": 7851.14, "text": " And because the validation set is ordered, all the stuff is the same type, it's in the", "tokens": [400, 570, 264, 24071, 992, 307, 8866, 11, 439, 264, 1507, 307, 264, 912, 2010, 11, 309, 311, 294, 264], "temperature": 0.0, "avg_logprob": -0.16227736428519277, "compression_ratio": 1.6952789699570816, "no_speech_prob": 1.0129869224329013e-05}, {"id": 1600, "seek": 784062, "start": 7851.14, "end": 7852.14, "text": " same place.", "tokens": [912, 1081, 13], "temperature": 0.0, "avg_logprob": -0.16227736428519277, "compression_ratio": 1.6952789699570816, "no_speech_prob": 1.0129869224329013e-05}, {"id": 1601, "seek": 784062, "start": 7852.14, "end": 7855.98, "text": " I don't know what this thing is.", "tokens": [286, 500, 380, 458, 437, 341, 551, 307, 13], "temperature": 0.0, "avg_logprob": -0.16227736428519277, "compression_ratio": 1.6952789699570816, "no_speech_prob": 1.0129869224329013e-05}, {"id": 1602, "seek": 784062, "start": 7855.98, "end": 7860.78, "text": " And what we can now do is we can now use nearest neighbors search.", "tokens": [400, 437, 321, 393, 586, 360, 307, 321, 393, 586, 764, 23831, 12512, 3164, 13], "temperature": 0.0, "avg_logprob": -0.16227736428519277, "compression_ratio": 1.6952789699570816, "no_speech_prob": 1.0129869224329013e-05}, {"id": 1603, "seek": 784062, "start": 7860.78, "end": 7866.3, "text": " So nearest neighbors search means here's one 300-dimensional vector, here's a whole lot", "tokens": [407, 23831, 12512, 3164, 1355, 510, 311, 472, 6641, 12, 18759, 8062, 11, 510, 311, 257, 1379, 688], "temperature": 0.0, "avg_logprob": -0.16227736428519277, "compression_ratio": 1.6952789699570816, "no_speech_prob": 1.0129869224329013e-05}, {"id": 1604, "seek": 784062, "start": 7866.3, "end": 7870.46, "text": " of other 3-dimensional vectors, which things is it closest to?", "tokens": [295, 661, 805, 12, 18759, 18875, 11, 597, 721, 307, 309, 13699, 281, 30], "temperature": 0.0, "avg_logprob": -0.16227736428519277, "compression_ratio": 1.6952789699570816, "no_speech_prob": 1.0129869224329013e-05}, {"id": 1605, "seek": 787046, "start": 7870.46, "end": 7873.46, "text": " And normally that takes a very long time because you have to look through every 300-dimensional", "tokens": [400, 5646, 300, 2516, 257, 588, 938, 565, 570, 291, 362, 281, 574, 807, 633, 6641, 12, 18759], "temperature": 0.0, "avg_logprob": -0.13333298796314305, "compression_ratio": 1.6045751633986929, "no_speech_prob": 3.02407679555472e-05}, {"id": 1606, "seek": 787046, "start": 7873.46, "end": 7878.42, "text": " vector, calculate its distance and find out how far away it is.", "tokens": [8062, 11, 8873, 1080, 4560, 293, 915, 484, 577, 1400, 1314, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.13333298796314305, "compression_ratio": 1.6045751633986929, "no_speech_prob": 3.02407679555472e-05}, {"id": 1607, "seek": 787046, "start": 7878.42, "end": 7886.7, "text": " But there's an amazing, almost unknown library called nmslib that does that incredibly fast.", "tokens": [583, 456, 311, 364, 2243, 11, 1920, 9841, 6405, 1219, 297, 2592, 38270, 300, 775, 300, 6252, 2370, 13], "temperature": 0.0, "avg_logprob": -0.13333298796314305, "compression_ratio": 1.6045751633986929, "no_speech_prob": 3.02407679555472e-05}, {"id": 1608, "seek": 787046, "start": 7886.7, "end": 7889.14, "text": " Like almost nobody's heard of it.", "tokens": [1743, 1920, 5079, 311, 2198, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.13333298796314305, "compression_ratio": 1.6045751633986929, "no_speech_prob": 3.02407679555472e-05}, {"id": 1609, "seek": 787046, "start": 7889.14, "end": 7891.78, "text": " Some of you may have tried other nearest neighbors libraries.", "tokens": [2188, 295, 291, 815, 362, 3031, 661, 23831, 12512, 15148, 13], "temperature": 0.0, "avg_logprob": -0.13333298796314305, "compression_ratio": 1.6045751633986929, "no_speech_prob": 3.02407679555472e-05}, {"id": 1610, "seek": 787046, "start": 7891.78, "end": 7894.58, "text": " I guarantee this is faster than what you're using.", "tokens": [286, 10815, 341, 307, 4663, 813, 437, 291, 434, 1228, 13], "temperature": 0.0, "avg_logprob": -0.13333298796314305, "compression_ratio": 1.6045751633986929, "no_speech_prob": 3.02407679555472e-05}, {"id": 1611, "seek": 787046, "start": 7894.58, "end": 7900.16, "text": " I can tell you that because it's been benchmarked by people who do this stuff for a living.", "tokens": [286, 393, 980, 291, 300, 570, 309, 311, 668, 18927, 292, 538, 561, 567, 360, 341, 1507, 337, 257, 2647, 13], "temperature": 0.0, "avg_logprob": -0.13333298796314305, "compression_ratio": 1.6045751633986929, "no_speech_prob": 3.02407679555472e-05}, {"id": 1612, "seek": 790016, "start": 7900.16, "end": 7904.62, "text": " This is by far the fastest on every possible dimension.", "tokens": [639, 307, 538, 1400, 264, 14573, 322, 633, 1944, 10139, 13], "temperature": 0.0, "avg_logprob": -0.1746161723959035, "compression_ratio": 1.6505576208178439, "no_speech_prob": 9.666029654908925e-06}, {"id": 1613, "seek": 790016, "start": 7904.62, "end": 7907.0, "text": " So this is basically a super fast way.", "tokens": [407, 341, 307, 1936, 257, 1687, 2370, 636, 13], "temperature": 0.0, "avg_logprob": -0.1746161723959035, "compression_ratio": 1.6505576208178439, "no_speech_prob": 9.666029654908925e-06}, {"id": 1614, "seek": 790016, "start": 7907.0, "end": 7909.94, "text": " We basically look here, this is angular distance.", "tokens": [492, 1936, 574, 510, 11, 341, 307, 24413, 4560, 13], "temperature": 0.0, "avg_logprob": -0.1746161723959035, "compression_ratio": 1.6505576208178439, "no_speech_prob": 9.666029654908925e-06}, {"id": 1615, "seek": 790016, "start": 7909.94, "end": 7916.26, "text": " So we want to create an index on angular distance and we're going to do it on all of our image", "tokens": [407, 321, 528, 281, 1884, 364, 8186, 322, 24413, 4560, 293, 321, 434, 516, 281, 360, 309, 322, 439, 295, 527, 3256], "temperature": 0.0, "avg_logprob": -0.1746161723959035, "compression_ratio": 1.6505576208178439, "no_speech_prob": 9.666029654908925e-06}, {"id": 1616, "seek": 790016, "start": 7916.26, "end": 7921.82, "text": " net word vectors, add in a whole batch, create the index, and now I can query a bunch of", "tokens": [2533, 1349, 18875, 11, 909, 294, 257, 1379, 15245, 11, 1884, 264, 8186, 11, 293, 586, 286, 393, 14581, 257, 3840, 295], "temperature": 0.0, "avg_logprob": -0.1746161723959035, "compression_ratio": 1.6505576208178439, "no_speech_prob": 9.666029654908925e-06}, {"id": 1617, "seek": 790016, "start": 7921.82, "end": 7924.98, "text": " vectors all at once, get their 10 nearest neighbors.", "tokens": [18875, 439, 412, 1564, 11, 483, 641, 1266, 23831, 12512, 13], "temperature": 0.0, "avg_logprob": -0.1746161723959035, "compression_ratio": 1.6505576208178439, "no_speech_prob": 9.666029654908925e-06}, {"id": 1618, "seek": 790016, "start": 7924.98, "end": 7929.32, "text": " Uses multi-threading, it's absolutely fantastic, this library.", "tokens": [4958, 279, 4825, 12, 392, 35908, 11, 309, 311, 3122, 5456, 11, 341, 6405, 13], "temperature": 0.0, "avg_logprob": -0.1746161723959035, "compression_ratio": 1.6505576208178439, "no_speech_prob": 9.666029654908925e-06}, {"id": 1619, "seek": 792932, "start": 7929.32, "end": 7933.219999999999, "text": " You can install it from pip, it just works.", "tokens": [509, 393, 3625, 309, 490, 8489, 11, 309, 445, 1985, 13], "temperature": 0.0, "avg_logprob": -0.20987416239618098, "compression_ratio": 1.580188679245283, "no_speech_prob": 6.339194897009293e-06}, {"id": 1620, "seek": 792932, "start": 7933.219999999999, "end": 7937.299999999999, "text": " And it tells you how far away they are and their indexes.", "tokens": [400, 309, 5112, 291, 577, 1400, 1314, 436, 366, 293, 641, 8186, 279, 13], "temperature": 0.0, "avg_logprob": -0.20987416239618098, "compression_ratio": 1.580188679245283, "no_speech_prob": 6.339194897009293e-06}, {"id": 1621, "seek": 792932, "start": 7937.299999999999, "end": 7941.66, "text": " So we can now go through and print out the top 3.", "tokens": [407, 321, 393, 586, 352, 807, 293, 4482, 484, 264, 1192, 805, 13], "temperature": 0.0, "avg_logprob": -0.20987416239618098, "compression_ratio": 1.580188679245283, "no_speech_prob": 6.339194897009293e-06}, {"id": 1622, "seek": 792932, "start": 7941.66, "end": 7945.08, "text": " So it turns out that bird actually is a limkin.", "tokens": [407, 309, 4523, 484, 300, 5255, 767, 307, 257, 2364, 5843, 13], "temperature": 0.0, "avg_logprob": -0.20987416239618098, "compression_ratio": 1.580188679245283, "no_speech_prob": 6.339194897009293e-06}, {"id": 1623, "seek": 792932, "start": 7945.08, "end": 7949.7, "text": " So here are, this is the top 3 for each one.", "tokens": [407, 510, 366, 11, 341, 307, 264, 1192, 805, 337, 1184, 472, 13], "temperature": 0.0, "avg_logprob": -0.20987416239618098, "compression_ratio": 1.580188679245283, "no_speech_prob": 6.339194897009293e-06}, {"id": 1624, "seek": 792932, "start": 7949.7, "end": 7953.299999999999, "text": " Interestingly this one doesn't say it's a limkin, and I looked it up, it's the fourth", "tokens": [30564, 341, 472, 1177, 380, 584, 309, 311, 257, 2364, 5843, 11, 293, 286, 2956, 309, 493, 11, 309, 311, 264, 6409], "temperature": 0.0, "avg_logprob": -0.20987416239618098, "compression_ratio": 1.580188679245283, "no_speech_prob": 6.339194897009293e-06}, {"id": 1625, "seek": 792932, "start": 7953.299999999999, "end": 7954.299999999999, "text": " one.", "tokens": [472, 13], "temperature": 0.0, "avg_logprob": -0.20987416239618098, "compression_ratio": 1.580188679245283, "no_speech_prob": 6.339194897009293e-06}, {"id": 1626, "seek": 795430, "start": 7954.3, "end": 7959.78, "text": " I don't know much about birds, but everything else here is brown with white spots, that's", "tokens": [286, 500, 380, 458, 709, 466, 9009, 11, 457, 1203, 1646, 510, 307, 6292, 365, 2418, 10681, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.22281862477787207, "compression_ratio": 1.6436781609195403, "no_speech_prob": 9.516195859760046e-06}, {"id": 1627, "seek": 795430, "start": 7959.78, "end": 7961.5, "text": " not.", "tokens": [406, 13], "temperature": 0.0, "avg_logprob": -0.22281862477787207, "compression_ratio": 1.6436781609195403, "no_speech_prob": 9.516195859760046e-06}, {"id": 1628, "seek": 795430, "start": 7961.5, "end": 7966.74, "text": " So I don't know if that's actually a limkin or if it's a mislabel, but sure as hell it", "tokens": [407, 286, 500, 380, 458, 498, 300, 311, 767, 257, 2364, 5843, 420, 498, 309, 311, 257, 3346, 75, 18657, 11, 457, 988, 382, 4921, 309], "temperature": 0.0, "avg_logprob": -0.22281862477787207, "compression_ratio": 1.6436781609195403, "no_speech_prob": 9.516195859760046e-06}, {"id": 1629, "seek": 795430, "start": 7966.74, "end": 7968.900000000001, "text": " doesn't look like the other birds.", "tokens": [1177, 380, 574, 411, 264, 661, 9009, 13], "temperature": 0.0, "avg_logprob": -0.22281862477787207, "compression_ratio": 1.6436781609195403, "no_speech_prob": 9.516195859760046e-06}, {"id": 1630, "seek": 795430, "start": 7968.900000000001, "end": 7974.02, "text": " So I thought that was pretty interesting.", "tokens": [407, 286, 1194, 300, 390, 1238, 1880, 13], "temperature": 0.0, "avg_logprob": -0.22281862477787207, "compression_ratio": 1.6436781609195403, "no_speech_prob": 9.516195859760046e-06}, {"id": 1631, "seek": 795430, "start": 7974.02, "end": 7975.78, "text": " It's kind of saying I don't think it's that.", "tokens": [467, 311, 733, 295, 1566, 286, 500, 380, 519, 309, 311, 300, 13], "temperature": 0.0, "avg_logprob": -0.22281862477787207, "compression_ratio": 1.6436781609195403, "no_speech_prob": 9.516195859760046e-06}, {"id": 1632, "seek": 795430, "start": 7975.78, "end": 7979.5, "text": " Now this is not a particularly hard thing to do, because there's only 1000 image net", "tokens": [823, 341, 307, 406, 257, 4098, 1152, 551, 281, 360, 11, 570, 456, 311, 787, 9714, 3256, 2533], "temperature": 0.0, "avg_logprob": -0.22281862477787207, "compression_ratio": 1.6436781609195403, "no_speech_prob": 9.516195859760046e-06}, {"id": 1633, "seek": 795430, "start": 7979.5, "end": 7982.5, "text": " classes and it's not doing anything new.", "tokens": [5359, 293, 309, 311, 406, 884, 1340, 777, 13], "temperature": 0.0, "avg_logprob": -0.22281862477787207, "compression_ratio": 1.6436781609195403, "no_speech_prob": 9.516195859760046e-06}, {"id": 1634, "seek": 798250, "start": 7982.5, "end": 7988.94, "text": " But what if we now bring in the entirety of WordNet and we now say which of those 45,000", "tokens": [583, 437, 498, 321, 586, 1565, 294, 264, 31557, 295, 8725, 31890, 293, 321, 586, 584, 597, 295, 729, 6905, 11, 1360], "temperature": 0.0, "avg_logprob": -0.1789547043877679, "compression_ratio": 1.4692737430167597, "no_speech_prob": 2.6841912585950922e-06}, {"id": 1635, "seek": 798250, "start": 7988.94, "end": 7991.94, "text": " things is it closest to?", "tokens": [721, 307, 309, 13699, 281, 30], "temperature": 0.0, "avg_logprob": -0.1789547043877679, "compression_ratio": 1.4692737430167597, "no_speech_prob": 2.6841912585950922e-06}, {"id": 1636, "seek": 798250, "start": 7991.94, "end": 7993.82, "text": " Exactly the same.", "tokens": [7587, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.1789547043877679, "compression_ratio": 1.4692737430167597, "no_speech_prob": 2.6841912585950922e-06}, {"id": 1637, "seek": 798250, "start": 7993.82, "end": 7996.5, "text": " So it's now searching all of WordNet.", "tokens": [407, 309, 311, 586, 10808, 439, 295, 8725, 31890, 13], "temperature": 0.0, "avg_logprob": -0.1789547043877679, "compression_ratio": 1.4692737430167597, "no_speech_prob": 2.6841912585950922e-06}, {"id": 1638, "seek": 798250, "start": 7996.5, "end": 8002.22, "text": " So now let's do something a bit different, which is take all of our predictions, so basically", "tokens": [407, 586, 718, 311, 360, 746, 257, 857, 819, 11, 597, 307, 747, 439, 295, 527, 21264, 11, 370, 1936], "temperature": 0.0, "avg_logprob": -0.1789547043877679, "compression_ratio": 1.4692737430167597, "no_speech_prob": 2.6841912585950922e-06}, {"id": 1639, "seek": 800222, "start": 8002.22, "end": 8013.22, "text": " take our whole validation set of images and create a k and n index of the image representations,", "tokens": [747, 527, 1379, 24071, 992, 295, 5267, 293, 1884, 257, 350, 293, 297, 8186, 295, 264, 3256, 33358, 11], "temperature": 0.0, "avg_logprob": -0.16691100212835497, "compression_ratio": 1.5416666666666667, "no_speech_prob": 1.6280442878269241e-06}, {"id": 1640, "seek": 800222, "start": 8013.22, "end": 8017.3, "text": " because remember it's predicting things that are meant to be word vectors.", "tokens": [570, 1604, 309, 311, 32884, 721, 300, 366, 4140, 281, 312, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.16691100212835497, "compression_ratio": 1.5416666666666667, "no_speech_prob": 1.6280442878269241e-06}, {"id": 1641, "seek": 800222, "start": 8017.3, "end": 8029.42, "text": " And now let's grab the fast text vector for boat, and boat is not an image net concept.", "tokens": [400, 586, 718, 311, 4444, 264, 2370, 2487, 8062, 337, 6582, 11, 293, 6582, 307, 406, 364, 3256, 2533, 3410, 13], "temperature": 0.0, "avg_logprob": -0.16691100212835497, "compression_ratio": 1.5416666666666667, "no_speech_prob": 1.6280442878269241e-06}, {"id": 1642, "seek": 802942, "start": 8029.42, "end": 8036.02, "text": " And yet I can now find all of the images in my predicted word vectors in my validation", "tokens": [400, 1939, 286, 393, 586, 915, 439, 295, 264, 5267, 294, 452, 19147, 1349, 18875, 294, 452, 24071], "temperature": 0.0, "avg_logprob": -0.1386147085225807, "compression_ratio": 1.7291666666666667, "no_speech_prob": 3.555916237019119e-06}, {"id": 1643, "seek": 802942, "start": 8036.02, "end": 8039.34, "text": " set that are closest to the word boat.", "tokens": [992, 300, 366, 13699, 281, 264, 1349, 6582, 13], "temperature": 0.0, "avg_logprob": -0.1386147085225807, "compression_ratio": 1.7291666666666667, "no_speech_prob": 3.555916237019119e-06}, {"id": 1644, "seek": 802942, "start": 8039.34, "end": 8044.38, "text": " And it works, even though it's not something that was ever trained on.", "tokens": [400, 309, 1985, 11, 754, 1673, 309, 311, 406, 746, 300, 390, 1562, 8895, 322, 13], "temperature": 0.0, "avg_logprob": -0.1386147085225807, "compression_ratio": 1.7291666666666667, "no_speech_prob": 3.555916237019119e-06}, {"id": 1645, "seek": 802942, "start": 8044.38, "end": 8050.18, "text": " What if we now take engine's vector and boat's vector and take their average?", "tokens": [708, 498, 321, 586, 747, 2848, 311, 8062, 293, 6582, 311, 8062, 293, 747, 641, 4274, 30], "temperature": 0.0, "avg_logprob": -0.1386147085225807, "compression_ratio": 1.7291666666666667, "no_speech_prob": 3.555916237019119e-06}, {"id": 1646, "seek": 802942, "start": 8050.18, "end": 8054.9, "text": " And what if we now look in our nearest neighbors for that, these are boats with engines.", "tokens": [400, 437, 498, 321, 586, 574, 294, 527, 23831, 12512, 337, 300, 11, 613, 366, 17772, 365, 12982, 13], "temperature": 0.0, "avg_logprob": -0.1386147085225807, "compression_ratio": 1.7291666666666667, "no_speech_prob": 3.555916237019119e-06}, {"id": 1647, "seek": 802942, "start": 8054.9, "end": 8058.86, "text": " I mean yes, this is actually a boat with an engine.", "tokens": [286, 914, 2086, 11, 341, 307, 767, 257, 6582, 365, 364, 2848, 13], "temperature": 0.0, "avg_logprob": -0.1386147085225807, "compression_ratio": 1.7291666666666667, "no_speech_prob": 3.555916237019119e-06}, {"id": 1648, "seek": 805886, "start": 8058.86, "end": 8062.139999999999, "text": " It just happens to have wings on as well.", "tokens": [467, 445, 2314, 281, 362, 11405, 322, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.13453496586192737, "compression_ratio": 1.7087378640776698, "no_speech_prob": 8.267838893516455e-06}, {"id": 1649, "seek": 805886, "start": 8062.139999999999, "end": 8065.94, "text": " By the way, sail is not an image net thing.", "tokens": [3146, 264, 636, 11, 15758, 307, 406, 364, 3256, 2533, 551, 13], "temperature": 0.0, "avg_logprob": -0.13453496586192737, "compression_ratio": 1.7087378640776698, "no_speech_prob": 8.267838893516455e-06}, {"id": 1650, "seek": 805886, "start": 8065.94, "end": 8067.62, "text": " Boat is not an image net thing.", "tokens": [3286, 267, 307, 406, 364, 3256, 2533, 551, 13], "temperature": 0.0, "avg_logprob": -0.13453496586192737, "compression_ratio": 1.7087378640776698, "no_speech_prob": 8.267838893516455e-06}, {"id": 1651, "seek": 805886, "start": 8067.62, "end": 8071.099999999999, "text": " Here's the average of two things that are not image net things.", "tokens": [1692, 311, 264, 4274, 295, 732, 721, 300, 366, 406, 3256, 2533, 721, 13], "temperature": 0.0, "avg_logprob": -0.13453496586192737, "compression_ratio": 1.7087378640776698, "no_speech_prob": 8.267838893516455e-06}, {"id": 1652, "seek": 805886, "start": 8071.099999999999, "end": 8076.7, "text": " And yet with one exception, it's found me two sailboats.", "tokens": [400, 1939, 365, 472, 11183, 11, 309, 311, 1352, 385, 732, 15758, 1763, 1720, 13], "temperature": 0.0, "avg_logprob": -0.13453496586192737, "compression_ratio": 1.7087378640776698, "no_speech_prob": 8.267838893516455e-06}, {"id": 1653, "seek": 805886, "start": 8076.7, "end": 8078.38, "text": " Let's do something else crazy.", "tokens": [961, 311, 360, 746, 1646, 3219, 13], "temperature": 0.0, "avg_logprob": -0.13453496586192737, "compression_ratio": 1.7087378640776698, "no_speech_prob": 8.267838893516455e-06}, {"id": 1654, "seek": 805886, "start": 8078.38, "end": 8082.42, "text": " Let's open up an image in the validation set.", "tokens": [961, 311, 1269, 493, 364, 3256, 294, 264, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.13453496586192737, "compression_ratio": 1.7087378640776698, "no_speech_prob": 8.267838893516455e-06}, {"id": 1655, "seek": 805886, "start": 8082.42, "end": 8083.42, "text": " Here it is.", "tokens": [1692, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.13453496586192737, "compression_ratio": 1.7087378640776698, "no_speech_prob": 8.267838893516455e-06}, {"id": 1656, "seek": 805886, "start": 8083.42, "end": 8086.82, "text": " I don't know what it is.", "tokens": [286, 500, 380, 458, 437, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.13453496586192737, "compression_ratio": 1.7087378640776698, "no_speech_prob": 8.267838893516455e-06}, {"id": 1657, "seek": 808682, "start": 8086.82, "end": 8093.42, "text": " Let's call predict array on that image to get its kind of word vector-like thing.", "tokens": [961, 311, 818, 6069, 10225, 322, 300, 3256, 281, 483, 1080, 733, 295, 1349, 8062, 12, 4092, 551, 13], "temperature": 0.0, "avg_logprob": -0.15361259338703562, "compression_ratio": 1.6495327102803738, "no_speech_prob": 5.338114533515181e-06}, {"id": 1658, "seek": 808682, "start": 8093.42, "end": 8097.46, "text": " And let's do a nearest neighbor search on all the other images.", "tokens": [400, 718, 311, 360, 257, 23831, 5987, 3164, 322, 439, 264, 661, 5267, 13], "temperature": 0.0, "avg_logprob": -0.15361259338703562, "compression_ratio": 1.6495327102803738, "no_speech_prob": 5.338114533515181e-06}, {"id": 1659, "seek": 808682, "start": 8097.46, "end": 8101.7, "text": " And here's all the other images of whatever the hell that is.", "tokens": [400, 510, 311, 439, 264, 661, 5267, 295, 2035, 264, 4921, 300, 307, 13], "temperature": 0.0, "avg_logprob": -0.15361259338703562, "compression_ratio": 1.6495327102803738, "no_speech_prob": 5.338114533515181e-06}, {"id": 1660, "seek": 808682, "start": 8101.7, "end": 8104.98, "text": " So you can see this is crazy.", "tokens": [407, 291, 393, 536, 341, 307, 3219, 13], "temperature": 0.0, "avg_logprob": -0.15361259338703562, "compression_ratio": 1.6495327102803738, "no_speech_prob": 5.338114533515181e-06}, {"id": 1661, "seek": 808682, "start": 8104.98, "end": 8109.78, "text": " We've trained a thing on all of image net in an hour using a custom head that required", "tokens": [492, 600, 8895, 257, 551, 322, 439, 295, 3256, 2533, 294, 364, 1773, 1228, 257, 2375, 1378, 300, 4739], "temperature": 0.0, "avg_logprob": -0.15361259338703562, "compression_ratio": 1.6495327102803738, "no_speech_prob": 5.338114533515181e-06}, {"id": 1662, "seek": 808682, "start": 8109.78, "end": 8112.42, "text": " basically two lines of code.", "tokens": [1936, 732, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.15361259338703562, "compression_ratio": 1.6495327102803738, "no_speech_prob": 5.338114533515181e-06}, {"id": 1663, "seek": 811242, "start": 8112.42, "end": 8118.38, "text": " And these things run in like 300 milliseconds to do these searches.", "tokens": [400, 613, 721, 1190, 294, 411, 6641, 34184, 281, 360, 613, 26701, 13], "temperature": 0.0, "avg_logprob": -0.16494095972342085, "compression_ratio": 1.7375415282392026, "no_speech_prob": 1.221891761815641e-05}, {"id": 1664, "seek": 811242, "start": 8118.38, "end": 8122.18, "text": " I actually taught this basic idea last year as well, but it was in Keras and it was just", "tokens": [286, 767, 5928, 341, 3875, 1558, 1036, 1064, 382, 731, 11, 457, 309, 390, 294, 591, 6985, 293, 309, 390, 445], "temperature": 0.0, "avg_logprob": -0.16494095972342085, "compression_ratio": 1.7375415282392026, "no_speech_prob": 1.221891761815641e-05}, {"id": 1665, "seek": 811242, "start": 8122.18, "end": 8126.9400000000005, "text": " like pages and pages and pages of code and everything took a long time and it was complicated.", "tokens": [411, 7183, 293, 7183, 293, 7183, 295, 3089, 293, 1203, 1890, 257, 938, 565, 293, 309, 390, 6179, 13], "temperature": 0.0, "avg_logprob": -0.16494095972342085, "compression_ratio": 1.7375415282392026, "no_speech_prob": 1.221891761815641e-05}, {"id": 1666, "seek": 811242, "start": 8126.9400000000005, "end": 8131.46, "text": " And back then I kind of said, I can't begin to think all the stuff you could do with this.", "tokens": [400, 646, 550, 286, 733, 295, 848, 11, 286, 393, 380, 1841, 281, 519, 439, 264, 1507, 291, 727, 360, 365, 341, 13], "temperature": 0.0, "avg_logprob": -0.16494095972342085, "compression_ratio": 1.7375415282392026, "no_speech_prob": 1.221891761815641e-05}, {"id": 1667, "seek": 811242, "start": 8131.46, "end": 8136.9, "text": " I don't think anybody's really thought deeply about this yet, but I think it's fascinating.", "tokens": [286, 500, 380, 519, 4472, 311, 534, 1194, 8760, 466, 341, 1939, 11, 457, 286, 519, 309, 311, 10343, 13], "temperature": 0.0, "avg_logprob": -0.16494095972342085, "compression_ratio": 1.7375415282392026, "no_speech_prob": 1.221891761815641e-05}, {"id": 1668, "seek": 811242, "start": 8136.9, "end": 8142.22, "text": " And so go back and read the device paper, because like Andrea had a whole bunch of other", "tokens": [400, 370, 352, 646, 293, 1401, 264, 4302, 3035, 11, 570, 411, 24215, 632, 257, 1379, 3840, 295, 661], "temperature": 0.0, "avg_logprob": -0.16494095972342085, "compression_ratio": 1.7375415282392026, "no_speech_prob": 1.221891761815641e-05}, {"id": 1669, "seek": 814222, "start": 8142.22, "end": 8143.46, "text": " thoughts.", "tokens": [4598, 13], "temperature": 0.0, "avg_logprob": -0.2536440658569336, "compression_ratio": 1.2611940298507462, "no_speech_prob": 2.88563114736462e-05}, {"id": 1670, "seek": 814222, "start": 8143.46, "end": 8149.26, "text": " And now that it's so easy to do, hopefully people will dig into this now, because I think", "tokens": [400, 586, 300, 309, 311, 370, 1858, 281, 360, 11, 4696, 561, 486, 2528, 666, 341, 586, 11, 570, 286, 519], "temperature": 0.0, "avg_logprob": -0.2536440658569336, "compression_ratio": 1.2611940298507462, "no_speech_prob": 2.88563114736462e-05}, {"id": 1671, "seek": 814222, "start": 8149.26, "end": 8150.9800000000005, "text": " it's crazy and amazing.", "tokens": [309, 311, 3219, 293, 2243, 13], "temperature": 0.0, "avg_logprob": -0.2536440658569336, "compression_ratio": 1.2611940298507462, "no_speech_prob": 2.88563114736462e-05}, {"id": 1672, "seek": 814222, "start": 8150.9800000000005, "end": 8153.860000000001, "text": " Alright, thanks everybody.", "tokens": [2798, 11, 3231, 2201, 13], "temperature": 0.0, "avg_logprob": -0.2536440658569336, "compression_ratio": 1.2611940298507462, "no_speech_prob": 2.88563114736462e-05}, {"id": 1673, "seek": 815386, "start": 8153.86, "end": 8174.259999999999, "text": " See you next week.", "tokens": [50364, 3008, 291, 958, 1243, 13, 51384], "temperature": 0.0, "avg_logprob": -0.7506746649742126, "compression_ratio": 0.6923076923076923, "no_speech_prob": 0.00012368892203085124}], "language": "en"}