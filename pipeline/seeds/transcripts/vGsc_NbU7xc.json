{"text": " Hi everybody and welcome to lesson 17 of practical deep learning for coders. Really excited about what we're gonna look at over the next lesson or two. It's actually been turning out really well, much better than I could have hoped. So I can't wait to dive in. Before I do, I'm just gonna mention a couple of minor changes that I made to our mini AI library this week. One was I went back to our callback class in the learner notebook and I did decide in the end to add a dunder getattr to it that just adds these four attributes. And for these four attributes, it passes it down to self.learn. So in a callback, you'll be able to refer to model to get self.learn.model, opt will be self.learn.opt, batch will be self.learn.batch, epoch will be self.learn.epoch. You can change these, you know, you could subclass the callback and add your own to underscore forward or you could remove things from underscore forward or whatever. But I felt like these four things I access a lot and I was sick of typing self.learn. And then I added one more property which is, in a callback, there'll be a self.training which saves me from typing self.learn.model.training. Since we have model, you can get rid of the learn, but still, I mean, you so often have to check the training now you can just go self.training in a callback. So that was one change I made. The second change I made was I found myself getting a bit bored of adding train cb every time. So what I did was I took the four training methods from the momentum learner subclass and I've moved them into a train learner subclass along with zero grad. So now momentum learner actually inherits from train learner and just adds momentum. It's kind of a quirky momentum method and changes zero grad to do the momentum thing. So yeah, so we'll be using train learner quite a bit over the next lesson or two. So train learner is just a learner which has the usual training. It's exactly the same that fast.ai2 has, or you'd have in most PyTorch training loops. And obviously by using this you lose the ability to change these with a callback. So it's a little bit less flexible. Okay, so those are little changes. And then I made some changes to what we looked at last week, which is the activations notebook. And specifically... Okay, so I added a hooks callback. So previously we had a hooks class and it didn't really require too much ceremony to use, but I thought we could make it even simpler and a bit more fast.ai-ish or mini.ai-ish by putting hooks into a callback. So this callback, as usual, you pass a function that's going to be called for your hook. And you can optionally pass it a filter as to what modules you want to hook. And then in before fit, it will filter the modules in the learner. And so this is one of these things we can now get rid of. We don't need the .learn here because model is one of the four things we have a shortcut to. And then here we're going to create the hooks object and put it in hooks. And so one thing that's convenient here is the hook function. Now you don't have to worry, and we can get rid of learn.model, you don't have to worry about checking in your hook functions whether in training or not. It always checks whether you're in training, and if so it calls that hook function you passed in. And after it finishes it removes the hooks. And you can iterate through the hooks and get the length of the hooks, because it just passes these iterators and length down to self.hooks. So to show you how this works, we can create a hooks callback. We can use the same append stats, and then we can run the model. And so as it's training, what we're going to be able to do is, yeah, we can now then, here we go. So we just added that as an extra callback to our fit function. I don't remember if we had the extra callbacks before, I'm not sure we did. So just to explain, it's just, I just added extra callbacks here in the fit function, and we're just adding any extra callbacks here. So then now that we've got that callback that we created, because we can get it iterate through it and so forth, we can just iterate it through that callback, as if it's hooks and plot in the usual way. So that's a convenient little thing, I think it's a convenient thing I added. And then I took our colorful dimension stuff, which Stefano and I came up with a few years ago, and decided to wrap all that up in a callback as well. So I've actually subclassed here our hooks callback to create an activation stats. And what that's going to do is it's going to use this append stats, which appends the means, the standard deviations, and the histograms. And I changed that very slightly also, the thing which creates these kind of dead plots, I changed it to just get the ratio of the very first, the very smallest histogram bin to the rest of the bins. So these are really kind of more like very dead at this point. So these graphs look a little bit different. Okay, so yeah, so I subclassed the hooks callback, and added the colorful dimension method, a dead chart method, and a plot stats method. So to see them at work, if we want to get the activations on all of the cons, then we train our model, and then we can just call, and so we've added, created our activation stats, we've added that as an extra callback, and then, and then yeah, then we can call colored in to get that plot, dead chart to get that plot, and plot stats to get that chart plot. So now we have absolutely no excuse for not getting all of these really fantastic, informative visualizations of what's going on inside our model, because it's literally as easy as adding one line of code, and just putting that in your callbacks. So I really think that couldn't be easier, and so I hope you're, even for models you thought you know a training really well, why don't you try using this, because you might be surprised to discover that they're not. Okay so those are some changes, pretty minor, but hopefully useful. And so today, and over the next lesson or two, we're going to look at trying to get to a important milestone, which is to try to get fashion MNIST training to an accuracy of 90% or more, which is certainly not the end of the road, but it's not bad. If we look at papers with code, there's, so 90% would be a 10% error, so there's folks that have got down to 3 or 4% error in the very best, which is very impressive. But you know, 10% error wouldn't be way off what's in this paper leaderboard. I don't know how far we'll get eventually, but without using even any architectural changes, no resnets or anything, we're going to try to get into the 10% error. All right, so the first few cells are just copied from earlier, and so here's our ridiculously simple model. All I did here was I said, okay, well the very first convolution is taking a 9x9x1 channel input, so we should compress it at least a little bit, so I made it 8 channels output for the convolution, and then I just doubled it to 16, doubled it to 32, doubled it to 64, and so that's going to get to a, that will be as I say, 14x14 image, 7x7, a 4x4, a 2x2, and then this one gets us to a 1x1, so of course we get the 10 digits. So there was no thought at all behind really this architecture, this pure convolutional architecture. And remember this flatten at the end is necessary to get rid of the unit axes that we end up with, because this is a 1x1. Okay, so let's do a learning rate finder on this very simple model, and what I found was that this model is, and you know, this situation is so bad that when I tried to use the learning rate finder kind of in the usual way, which would be just to say, you know, start at 1e neg 5 or 1e neg 4, say, and then run it, it kind of looks ridiculous. It's impossible to see what's going on. So if you remember, we added that multiplier, we called it LRMult or gamma is what they called it in PyTorch, so we ended up calling it gamma. So I dialed that way down to make it much more gradual, which means I have to dial up the starting learning rate, and only then did I manage even to get the learning rate finder to tell us anything useful. Okay, so there we are. So that's our learning rate finder. Actually, I'm just going to come back to these three later. So I tried using a learning rate of 0.2, and after trying a few different values, 0.4, 0.1, 0.2 seems about the highest we can get up to. Even this actually is too high, I found. Much lower, and it didn't train much at all. You can see what happens if I do, it starts training and then it kind of, yeah, we lose it, which is unfortunate. And you can see that in the colorful dimension plot, we get this classic, you know, getting activations crashing, getting activations crashing. And you can kind of see the key problem here really is that we don't have zero mean standard deviation one layers at the start. So we certainly don't keep them throughout. And this is this is a problem. Now just something I'm going to mention, by the way, is when you're training stuff in Jupyter Notebooks, this is just a new thing we've just added. If you get, you can easily run out of memory, GPU memory. And there's two reasons it turns out why you can particularly run out of GPU memory if you run a few cells in a Jupyter Notebook. The first is that, kind of for your convenience, Jupyter Notebook, you might, may or may not know this, actually stores the results of your previous few evaluations. If you just type underscore, it tells you the very last thing you evaluated. And you can do more underscores to go backwards further in time. Or you can also use, oh, you can also use numbers to get the out 16, for example, would be underscore 16. Now the reason this is an issue is that if one of your outputs is a big CUDA tensor, and you've shown it in a cell, that's going to keep that GPU memory basically forever. And so that's a bit of a problem. So if you are running out of memory, one thing you'd want to do is clean out all of those underscore blah things. I found that there's actually some function that nearly does that in the IPython source code, so I copied the important bits out of it and put it in here. So if you call clean IPython history, it will, don't worry about the lines of code at all, this is just a thing that you can use to get back that GPU memory. The second thing, which Peter figured out in the last week or so, is that you also have, if you have a CUDA error at any point, or even any kind of exception at any point, then the exception object is actually stored by Python, and any tensors that were allocated anywhere in that trace, in that traceback, will stay allocated basically forever. And again, that's a big problem. So I created this clean traceback function based on Peter's code, which gets rid of that. So this is particularly problematic because if you have a CUDA out of memory error, and then you try to rerun it, you'll still have a CUDA out of memory error, because all the memory that was allocated before is now in that traceback. So basically any time you get a CUDA out of memory error, or any kind of error with memory, you can call cleanmem, and that will clean the memory in your traceback. It will clean the memory used in your Jupyter history, do a garbage collect, empty the CUDA cache, and that will basically, should give you a totally clean GPU. You don't have to restart your notebook. So Sam asked a very good question in the chat. So just to remind you guys, yes we did start, he's asking, I thought we were training an autoencoder, or are we training a classifier, or what? So we started doing this autoencoder back in notebook 8, and we decided, oh, we don't have the tools to make this work yet, so let's go back and create the tools and then come back to it. So in creating the tools, we're doing a classifier, we're trying to make a really good fashion MNIST classifier, well we're trying to create tools which hopefully have a side effect we'll find of giving us a really good classifier, and then using those tools, we hope that will allow us to create a really good autoencoder. So yes, we're kind of like gradually unwinding, and we'll come back to where we were actually trying to get to. So that's why we're doing this classifier, the techniques and library pieces we're building will be all very necessary. Okay, so why do we need a zero mean, one standard deviation, why do we need that? And B, how do we get it? So first of all, on the way. So if you think about what a neural net does, a deep learning net specifically, it takes an input and it puts it through a whole bunch of matrix multiplications, and of course there are activation functions sandwiched in there. Don't worry about the activation functions, that doesn't change the argument. So let's just imagine we start with some matrix, right. Imagine a 50 deep neural net. So a 50 deep neural net basically, if we ignore the activation functions, is taking the previous input and doing a matrix multiply by some, initially some random weights. So these are all, yeah, these are just a bunch of random weights. And these are actually, rand n is mean zero, variance one. And if we run this, after 50 times of multiplying by a matrix, by a matrix, by a matrix, by a matrix, we end up with NANDs. That's no good. So that might be that our matrix, the numbers in our matrix are too big. So each time we multiply, the numbers are getting bigger and bigger and bigger. So maybe we should make them a bit smaller. Okay, so let's try using, in the matrix we're multiplying by, let's try multiplying by 0.01. And we multiply that lots of times. Oh, now we've got zeros. Now of course, mathematically speaking, this isn't actually NAND, it's actually some really big number. Mathematically speaking, this isn't really zero, it's some really small number. But computers can't handle really, really small numbers, or really, really big numbers. So really, really big numbers eventually just get called NAND, and really, really small numbers eventually just get called zero. So basically, they get washed out. And in fact, even if you don't get a NAND, or even if you don't quite get a zero, for numbers that are extremely big, the internal representation has no ability to discriminate between even slightly similar numbers. Basically in the way a floating point is stored, the further you get away from zero, the less accurate the numbers are. So yeah, this is a problem. So we have to scale our weight matrices exactly right. We have to scale them in such a way that the standard deviation at every point stays at one, and the mean stays at zero. So there's actually a paper that describes how to do this for multiplying lots of matrices together. And this paper basically just went through. It's actually pretty simple math. Actually, let's see. What did they do? All right. Yeah, so they looked at gradients and the propagation of gradients. And they came up with a particular weight initialization of using a uniform with 1 over root N as the bounds of that uniform. And they studied basically what happened with various different activation functions. And as a result, we now have this way of initializing neural networks, which is called either Gloro initialization or Xavier initialization. And yeah, this is the amount that we scale our initialization, our random numbers by, where N in is the number of inputs. So in our case, we have 100 inputs. And so root 100 is 10. So 1 over 10 is 0.1. And so if we actually run that, if we start with our random numbers, and then we multiply by random numbers times 0.1, which is, this is the Gloro initialization, you can see we do end up with numbers that are actually reasonable. So that's pretty cool. So just some background in case you're not familiar with some of these details. What exactly do we mean by variance? So if we take a tensor, let's call it T, and just put 1, 2, 4, 18 in it, the mean of that is simply the sum divided by the count. So that's 6.25. Now we want to know, basically, we want to come up with a measure of how far away each data point is from the mean. That tells you how much variation there is. If all the data points are very similar to each other, right, so if you've got kind of like a whole bunch of data points, and they're all pretty similar to each other, right, then the mean would be about here, right? And the average distance away of each point from the mean is not very far. Where else if you had dots which were very widely spread all over the place, right, then you might end up with the same mean, but the distance from each point to the mean is now quite a long way. So that's what we want. We want some measure of kind of how far away the points are on average from the mean. So here we could do that. We can take our tensor, we can subtract the mean, and then take the mean of that. Ah, well that doesn't work. Because we've got some numbers that are bigger than the mean and some that are smaller than the mean, and so if you average them all out, then by definition you actually get zero. So instead, you could either square those differences, and that will give you something, and you could also take the square root of that if you wanted to, to get it back to the same kind of area. Or you could take the absolute differences. Okay, so actually I'm doing this in two steps here. So for the first one, here it is on a different scale, and then add square root, get it on the same scale. So 6.87 and 5.88 are quite similar, right? But they're mathematically not quite the same. But they're both similar ideas. So this is the mean absolute difference, and this is called the standard deviation, and this is called the variance. So the reason that the standard deviation is bigger than the mean absolute difference is because in our original data, one of the numbers is much bigger than the others. And so when we square it, that number ends up having an outsized influence. And so that's a bit of an issue in general with standard deviation and variance, is that outliers like this have an outsized influence. So you've got to be a bit careful. Okay, so here's the formula for the standard deviation. It's normally written as sigma. Okay, so it's just going to be each of our data points minus the mean squared, plus the next data point minus the mean squared, so forth, for all the data points. And then divide that by the number of data points in square root. And okay, so one thing I point out here is that the mean absolute deviation isn't used as much as the standard deviation because mathematicians find it difficult to use. But we're not mathematicians, we have computers, so we can use it. Okay, now variance we can calculate like this, as we said. The mean of the square of the differences. And if you feel like doing some math, you could discover that actually this is exactly the same, as you can see. And this is actually nice because this is showing that the mean of the squared data points minus the square of the mean of the data points is also the variance. And this is very helpful because it means you actually never have to calculate this. You can just calculate the mean. So with just the data points on their own, you can actually calculate the variance. This is a really nice shortcut. This is how we normally calculate variance. And so there is the LaTeX version, which of course I didn't write myself. I stole from the Wikipedia LaTeX because I'm lazy. Now there's a very, very similar idea, which is covariance. And it's already come up a little bit in the first lesson or two. And particularly the extra math lesson that Basim and Dinesh did. And it's, yeah, so covariance tells you how much two things vary, not just on their own but together. And there's a definition here in math, but I like code so we'll see the code. So here's our tensor again. Now we're going to want to have two things. So let's create something called u, which is just two times our tensor with a bit of randomness. So here it is. Now you can see that u and t are very closely correlated here. But they're not perfectly correlated. So the covariance tells us, yeah, how they vary together and separately. So we can take the, you can see this is exactly the same thing we had before. Each data point minus its mean. But now we've got two different tensors. So we're also going to do it for the other one, the other data points minus their mean. And we multiply them together. So it's actually the same thing as standard deviation. But in standard deviation, it's kind of like the covariance with itself in a sense, right? And so that's a product we can calculate. And then what we then do is we take the mean of that. And that gives us the covariance between those two tensors. And you can see that's quite a high number. And if we compare it to two things that aren't very related at all, so let's create a totally random tensor, v. So this is not related to t. And we do exactly the same thing. So take the difference of t to its means and v to its means and take the mean of that. That's a very small number. And so you can see covariance is basically telling us how related are these two tensors. So covariance and variance are basically the same thing. But you can think of variance as being covariance with itself. And you can change this mathematical version, which is the one we just created in code, to this version, just like we have for variance. It's the easier to calculate version, which as you can see, gives exactly the same answer. Okay, so if you haven't done stuff with covariance much before, you should experiment a bit with it by creating a few different plots and experimenting with those. And finally, the Pearson correlation coefficient, which is normally called R or rho, is just the covariance divided by the product of the standard deviations. So you've probably seen that number many times. There's just a scaled version of the same thing. Okay, so with that in mind, here is how Xavier in it or Gloro in it is derived. So when you do a matrix multiplication, for each of the y i's, we're adding together all of these products. So we've got a i, 0 times x 0 plus a i, 1 times x 1, etc. And we can write that in sigma notation. So we're adding up together all of the a i k's with all of the x k's. This is the stuff that we did in our first lesson of part two. And so here it is in pure Python code, and here it is in NumPy code. Now at the very beginning, our vector has a mean of about 0 and a standard deviation of about 1, because that's what we asked for, to remind you. That's what we asked for. That's a standard deviation of 1, mean of 0. That's what rand n is. So let's create some random numbers and we can confirm, yeah, they have a mean of about 0 and a standard deviation of about 1. So if we chose weights for a that have a mean of 0, we can compute the standard deviation quite easily. So let's do that. So 100 times, let's try creating our x, and let's try creating something to multiply it by. And we'll do the matrix multiplication. And we're gonna get the mean and mean of the squares. And so that is very close to our matrix. So I won't go into, I mean, you can look at it if you like, but basically as long as the elements in a and x are independent, which obviously they are because they're random. Then we're gonna end up with a mean of 0 and a standard deviation of 1 for these products. And so we can try it if we create a random number, a normally distributed random number, and then a second random number. Multiply them together and then do it a bunch of times. And you can see here we've got our 0, 1. So that's the reason why we need this math.square root 100. We don't normally worry about the mathematical reasons why things are exactly, but yeah, I thought I would just dive into this one, cuz sometimes it's fun to go through it. And so you can check out the paper if you wanna look at that in more detail or experiment with these little simulations. Now the problem is that that doesn't work. It doesn't work for us because we use rectified linear units, which is not something that Xavier Gloreau looked at. Let's take a look. Let's create a couple of matrices. This is 200 by 100. This is just a matrix and a vector. This is 200. And then let's create a couple of weight matrices, two weight matrices and two bias vectors. Okay, so we've got some input data, x's and y's. And we've got some weight matrices and bias vectors. So let's create a linear layer function, which we've done lots of times before. And let's start going through a little neural net. I'm mentioning this is the forward pass of our neural net. So we're gonna apply our linear layer to the x's with our first set of weights and our first set of biases and see what the mean and standard deviation is. Okay, it's about 0 and about 1, so that's good news. And the reason why is because we have 100 inputs and we divided it by square root of 100, just like Gloreau told us to. And our second one has 50 inputs and we divide by square root of 50. And so this all ought to work, right? And so far it is, but now we're gonna mess everything up by doing ReLU. So ReLU, after we do a ReLU, look, we don't have a 0 mean or a 1 standard deviation anymore. So if we go through that and create a deep neural network with Gloreau initialization, but with a ReLU, dear, it's disappeared. It's all gone to 0. And you can see why, right? After a matrix multiply and a ReLU, our means and variances are going down. And of course they're going down because a ReLU squishes it. So I'm not gonna worry about the math of why, but a very important paper indeed called Delving Deep Indirectifiers, Surpassing Human-Level Performance on ImageNet Classification, by Kaiming He et al, came up with a new init, which is just like Gloreau initialization. But you multiply, remember the Gloreau initialization was 1 over root n. This one is root 2 over n. And again, n is the number of inputs. So let's try it. So we've got 100 inputs. So we have to multiply it by root 2 over 100. And there we go. You can see we are, in fact, getting some non-zero numbers. So it's very encouraging even after going through 50 layers of depth. So that's good news. So this is called Kaiming, it's either called Kaiming initialization or called He initialization. And notice it looks like it's spelt he, but it's a Chinese surname. So it's actually pronounced He. Okay, maybe that's why a lot of people increasingly call it Kaiming initialization. I don't have to say his surname, which is a little bit harder to pronounce. All right, so how on earth do we actually use this? Now that we know what initialization function to use for a deep neural network with a ReLU activation function. The trick is to use a method called apply, which all nn.modules have. So if we grab our model, we can apply any function we like. For example, let's apply the function print the name of the type. So here you can see it's going through and it's printing out all of the modules that are inside our model. And notice that our model has modules inside modules. It's a conv in a sequential, in a sequential. But model.apply goes through all of them regardless of their depth. So we can apply an init function. So we can apply the init function which simply does. Multiply, random numbers, multiply, normally distributed random numbers times square root of 2 over the number of inputs. That's such an easy thing, it's not even worth writing. So that's already been written, but that's all it does. It just does that one thing. It's called init.kymingNormal. As we've seen before, if there's an underscore at the end of a PyTorch method name, that means that it changes something in place. So init.kymingNormal underscore will modify this weight matrix so that it has been initialized with normally distributed random numbers based on root of 2 divided by the number of inputs. Now, you can't do that to a sequential layer, or a ReLU layer, or a flattened layer. So we should check that the module is a conv or linear layer. And then we can just say model.apply the function. And so if we do that, and now I can use our learning rate finder callbacks that we created earlier. And this time I don't have to worry about, actually we can create our own ones because we don't need to use even the weird gamma thing anymore. So let's go back and copy that. Oopsie daisy. Let's get rid of this gamma equals 1.1, it shouldn't be necessary anymore. And we can probably make that 4 now. I should have, need to recreate the model. There we go. Okay, so that's looking much more sensible. So at least we've got to a point where the learning rate finder works, that's a good sign. So now when we create our learner, we're going to use our momentum learner still. After we get the model, we will apply initWeights. And apply also returns the model, so this is actually going to return the model with the initialization applied. While I wait, I will answer questions. Okay, so Fabrizio asks, why do we double the number of filters in successive convolutions? So what's happening is in each stride two convolution, These are all stride two convolutions. So this is changing the grid size from 28 by 28 to 14 by 14. So it's reducing the size of the grid by a factor of 4 in total. So basically, so as we go from 1 to 8, from this one to this one, same deal, we're going from 14 by 14 to 7 by 7. So if we reduce the grid size by 4. We want it to learn something. And if you give it exactly the same kind of number of units or activations, it's not really forcing it to learn things as much. So ideally, as we decrease the grid size, we want to have enough channels that you end up with a few less activations than before, but not too many less. So if we double the number of channels, then that means we've decreased the grid size by a model of 4. Increase the channel count by a model of 2. So overall, the number of activations has decreased by a factor of 2. And so that's what we want. We want to be kind of forcing it to find ways of compressing the information intelligently as it goes down. Also, we kind of want to be having a roughly similar amount of compute, roughly similar amount, through the neural net. So as we decrease the grid size, we can add more channels. Because decreasing the grid size decreases the amount of compute. Increasing the channels then gives it more things to compute. So we're kind of getting this nice compromise between, yeah, between the kind of amount of compute that it's doing, but also giving it some kind of compression work to do. That's the kind of the basic idea. Well, still not able to train. Well, okay, if we leave it for a while. Okay, it's not great, but it is actually starting to train. That's encouraging. And we got up to a 77% accuracy. So we can see, yeah, not surprisingly, we're getting these spikes and spikes. And so in the statistics, you can see that, well, it didn't quite work. We don't have a mean of 0. We don't have a standard deviation of 1, even at the start. Why is that? Well, it's because we forgot something critical. If you go back to our original point, even when we had our, well, let's go to the timing version. Even when we had the correctly normalized matrix that we're multiplying by, well, you also have to have a correctly normalized input matrix. And we never did anything to normalize our inputs. So our inputs actually, if we just get the first x mini-batch, I get its mean and standard deviation. It has a mean of 0.28 and a standard deviation of 0.35. So we actually didn't even start with a 0, 1 input. And so we started with a mean above 0 and a standard deviation beneath 1. So that was very hard for it. So using the init helped, at least we're able to train a little bit. But it's not quite what we want. We actually need to modify our inputs so they have a mean of 0 and a standard deviation of 1. So we could create a callback to do that. So a callback, let's create a batch transform callback. And so we're gonna pass in a function that's gonna transform every batch. And so just in the before batch, we will set the batch to be equal to the function applied to the batch. Now I can, note by the way, we don't need self.learn.batch here. Because we can read any, cuz it's one of the four things that we kind of proxy down to the learner automatically. But we do need it on the left hand side. Cuz it's only in the getAttra, remember. So be very careful. So I might just leave it the same on both sides, just so that people don't get confused. Okay, so let's create a function, underscore norm, that subtracts the mean and divides by the standard deviation. And so remember a batch has an x and a y. So it's the x part where we subtract the mean and divide by the standard deviation. And so the new batch will be that as the x and the y will be exactly the same as it was before. So let's create a instance of the normalization of the batch transform callback, which is gonna do the normalization function. And we'll call it norm. So we can pass that as an additional callback to our learner. And now, that's looking a lot better. So you can see here, all we had to do was check that our input matrix was 0, 1, mean standard deviation. And all of our weight matrices was 0, 1 standard deviation. And we didn't have to use any tricks at all. It was able to train and got it to an accuracy of 85%. And so if we look at the color dim and stats, look at this, it looks beautiful. Now, this is layer one, this is layer two, three, four. It's still not perfect. I mean, there's some randomness, right? And we've got, what is it, like seven or eight layers. So that randomness does kind of, as you go through the layers, by the last one, it still gets a bit ugly. And you can kind of see it bouncing around here as a result. And you can see that also in the means and standard deviations. There's some other reasons this is happening, we'll see in a moment. But this is the first time we've really got our even somewhat deep convolutional model to train. And so this is a really exciting step. We have, from scratch, in a sequence of 11 notebooks, managed to create a real convolutional neural network that is training properly. So I think that's pretty amazing. Now, we don't have to use a callback for this. The other thing we could do to modify the input data, of course, is to use the with transform method from the HuggingFaceDatasets library. So we could modify our transform i to subtract the mean and divide by the standard deviation. And then recreate our data loaders. And if we now get a batch out of that and check it, it's now got, yep, the mean is 0 and the standard deviation of 1. So we could also do it this way. So generally speaking, for stuff that needs to kind of dynamically modify the batch, you can often do it either in your data processing code or you can do it in a callback. And neither's right or wrong. They both work well, and you can see whichever one works best for you. Okay, now I'm gonna show you something amazing. Okay, so it's great this is training well, but when you look at our stats, despite what we did with the normalized input and the normalized weight matrices, we don't have a mean of 0 and we don't have a standard deviation of 1. Even from the start. So why is that? Well, the problem is that we were putting our data through a ReLU. And our activation stats are looking at the output of those ReLU blocks, cuz that's kind of the end of each. You know, that's the activation of each combination of matrix multiplication and activation function. And since a ReLU removes all of the negative numbers, it's impossible for the output of a ReLU to have a mean of 0, unless literally every single number is 0. Perhaps it's got no negatives. So ReLU seems to me to be fundamentally incompatible with the idea of a correctly calibrated bunch of layers in a neural net. So I came up with this idea of saying, well, why don't we take our normal ReLU and have the ability to subtract something from it? And so we just take the result of our ReLU and subtract, so sub minus. I mean, I can write this in a more obvious way. It's exactly the same as just minus equals. I'm gonna just do that. We'll subtract something from our ReLU. That will allow us to pull the whole thing down so that the bottom of our ReLU is underneath the x-axis, and it has negatives. And that would allow us to have a mean of 0. And while we're there, let's also do something that's existed for a while. I didn't come up with this idea, which is to do a leaky ReLU, which is where we say, let's not have the negative speed totally flat, just truncated. But instead, let's just have those numbers decreased by some constant amount. Let me show you what that looks like. So those two together, I'm gonna call general ReLU, which is where we do this thing called leaky ReLU, which is where we make it so it's not flat under 0, but instead just less sloped. And we also subtract something from it. For example, I've created a little function here for plotting a function. So let's plot the general ReLU function with a leakiness of 0.1. So that will mean there's a 0.1 slope under 0, and we'll subtract 0.4. And so you can see, above 0, it's just a normal y equals x line, but it's been pushed down by 0.4. And then when it's less than 0, it's not flat anymore, but it's just got a slope of one-tenth. And so this is now something which, if you find the right amount to subtract for each amount of leakiness, you can make a mean of 0. And I actually found that this particular combination gives us a mean of 0, or thereabouts. So let's now create a new convolution function where we can actually change what activation function is used. That gives us the ability to change the activation functions in our neural nets. Let's change getModel to allow it to take an activation function which is passed into the layers. And while we're there, let's also make it easy to change the number of filters. So we're gonna pass in a list of the number of filters in each layer, and we will default it to the numbers in each layer that we've discussed. And so we're just gonna go through in a list comprehension, creating a convolution from the previous number of filters, this number of filters to the next number of filters. And we'll pop that all into a sequential along with a flatten at the end. And while we're there, we also then need to be careful about initWaits, because this is something that people tend to forget. Which is that, init, which is the timing initialization, the default only applies at all to layers that have a value activation function. We don't have value anymore, we actually have leaky value. The fact that we're subtracting a bit from it doesn't change things, but the fact that it's leaky does. Now luckily, a lot of people don't know this, but actually PyTorch's timing normal has an adjustment for leaky values. Weirdly enough, they just call it A. So if you pass into the timing normal initialization how your leaky values leaky factor as A, then you'll get the correct initialization for a leaky value. So we need to change initWaits now to pass in the leakiness. All right, so let's put all this together. So our general value activation function is general value with a leak of 0.1 and a subtract of 0.4. So we use partial to create a function that has those built-in parameters. For activation stats, we need to update it now to look for general values, not nn.values. Okay, and then our initWaits function, we're gonna have a partial with leaky equals 0.1. So we'll call that our initWaits. Great, so now we'll get our model using that new activation function and that new initWaits. And we'll fit that. That's encouraging. An accuracy of 8.45, which is about as high as we got to at the end previously. Wow, look at that. So we're up to an accuracy of 87%. And let's take a look. Yeah, I mean, look, we still got a little bit of a spike, but it's almost smooth and flat. And let's have a look here. Look at that. Our mean is starting at about 0. Standard deviation. No, standard deviation's still a bit low, but it's coming up around 1. It's not too bad, generally around 0.8. So it's all looking pretty encouraging, I think. And yeah, look, the percentage of dead units in each layer is very small. So finally, we've really got some very nice looking training graphs here. And yeah, it's interesting that we had to literally invent our own activation function to make this work. And I think that gives you a sense of how few people actually care about this, which is crazy, because as you can see, in some ways, it's the only thing that matters. And it's not at all mathematically difficult to make it all work. And it's not at all computationally difficult to see whether it's working. But other frameworks don't even let you plot these kinds of things. So nobody even knows that they've completely messed up their initialization. So yeah, now you know. Now some very nice news. Well, so the first thing to be aware of, which is tricky, is a lot of models use more complicated activation functions nowadays, rather than ReLU or leaky ReLU or even this general version. You need to initialize your neural network correctly, and most people don't. And sometimes nobody's even figured out or bothered to try to figure out what the correct initialization to use is. But there's actually a very cool trick which almost nobody knows about, which is a paper called All You Need is a Good Init, which Dmitry Mishkin wrote a few years ago. And what Dmitry showed is that there's actually a completely general way of initializing any neural network correctly, regardless of what activation functions are in it. And it uses a very, very simple idea. And the idea is create your model, initialize it however you like, and then go through and put a single batch of data through. And look at the first layer, see what the mean standard deviation through the first layer is. And if the mean, if the standard deviation is too big, divide the weight matrix by a bit. If the mean's too high, subtract a bit off the weight matrix. And do that repeatedly for the first layer until you get the correct mean and standard deviation. And then go to the second layer, do the same thing. Third layer, do the same thing, and so forth. So we can do that using hooks, right? So we could create a little, so this is called layer-wise sequential unit variance, LSUV. We can create a little LSUV stats that will grab the mean of the activations of a layer and the standard deviation of the activations of a layer. And we will create a hook with that function. And what it's gonna do is after we've run that hook to find out the mean and standard deviation of the layer, we will go through and run the model, get the standard deviation and mean. See if the standard deviation is not 1, see if the mean is not 0. And we will subtract the mean from the bias, and we will divide the weight matrix by the standard deviation. And we will keep doing that until we get a standard deviation of 1 and a mean of 0. And so by making that a hook, what we will do is we will grab all the values and all the coms, right? And so just to show you what happens there, once I've got all the values and all the coms, I can use zip. So zip in Python takes a bunch of lists and creates a list of the items, the first items, the second items, the third items, and so forth. So if I go through the zip of values and coms and just print them out, you can see it prints out the value and the first com. The second value, the second com, the second value, sorry, the third value, the third com, and so forth. We use zip all the time in Python, so it's a really important thing to be aware of. So we could go through the values and the coms and call layer-wise sequential unit variance in it, passing in those module pairs. Sorry, passing in, yeah, passing in the relu and the conv. And then for each one, and we're gonna do that on the batch. And of course, we need to put the batch on the correct device for our model. And so now that I've done that, We now have, it ran almost instantly. It's now made all the biases and weights correct, give us 0, 1. And now if I train it, there it is. So we didn't do any initialization at all of the model, other than just call lsuv init. And this time we've got an accuracy of 0.86, versus previously it's 0.87. So pretty much the same thing, close enough. And actually, If you wanna actually see that happening, I guess what we could do, I mean, it's gonna be pretty obvious. After we've run this, we could say print h.mean, h.standard deviation. Actually, we could do it before and afterwards, right? So we could say, right, before and after. There we go. Yeah, so the first layer started at a mean of negative 0.13 and a variance of 0.46. And it kept doing the divide, subtract, divide, subtract, divide, subtract until eventually it got to mean is 0, standard deviation of 1. And then it went to the next layer, and it kept going, going, going until that was 0, 1, and then the third layer, and then the fourth layer. And so at that point, all of the layers had a mean of 0 and a standard deviation of 1. So I guess one thing with LSUV, it's kind of very mathematically convenient. We don't have to spend any time thinking about if we've invented a new activation function or we're using some activation function where nobody seems to have figured out the correct initialization for it, we can just use LSUV. It did require a little bit more fiddling around with hooks and stuff to get it to work, and I haven't even put this into a callback or anything. So if you decide you want to try using this in some of your models, it might be a good idea. And it'd actually be good homework to see if you can come up with a callback that does LSUV initialization for you. That would be pretty cool, wouldn't it, in before fit, I guess it would be. You'd have to be a bit careful because if you ran fit multiple times, it would actually initialize it each time. So that would be one issue with that to think about. Okay, so something which is quite similar to LSUV is batch normalization. So we're gonna have a seven minute break, and then we're gonna come back and we're gonna talk about batch normalization. See you in seven minutes. Okay, hi, let's do this, batch normalization. Batch normalization was such an important paper. I remember when it came out, I was at Analytic, my medical startup. And I think that's right. And everybody was talking about it. And in particular, They were talking about this graph that basically showed what it used to be like until batch norm to train a model on ImageNet. How many training steps you'd have to do to get to a certain accuracy. And then they showed what you could do with batch norm. So much faster, it was amazing. And we all thought, that can't be true. But it was true. So basically, the key idea of batch norm is that with LSUV and input normalization and timing in it, we are normalizing each layer's inputs before training. But the distribution of each layer's inputs changes during training. And that's a problem. So you end up having to decrease your learning rates. And as we've seen, you'd have to be very careful about parameter initialization. So the fact that the layers inputs change during training, they call internal covariate shift. Which for some reason, a lot of people tend to find a confusing statement or confusing name, but it's very clear to me. And you can fix it by normalizing layer inputs during training. So you're making the normalization a part of the model architecture. And you perform the normalization for each mini-batch. Now, I'm actually not gonna start with batch normalization. I'm gonna start with something that came out one year later, called layer normalization, because layer normalization is simpler. Let's do the simpler one first. So layer normalization came out as this group of fellows, the last of whom I'm sure you've heard of. And it's probably easiest to explain by showing you the code. So if you're thinking layer normalization, wow, it's a whole paper, a Geoffrey Hinton paper, must be complicated. No, the whole thing is this code. What is layer normalization? Well, we can create a module. And we're going to pass in, we don't really need to pass in anything, actually. You can totally ignore the parameters for now. In fact, what we're gonna do is we're gonna have a single number called mult, for the multiplier, and a single number called add, that's the thing we're gonna add, and we're gonna start off by multiplying things by one and adding zero. So we're gonna start off by doing nothing at all. Okay, this is the layer, it has a forward function. And in the forward function, so remember that by default we have n, c, h, w. We have batch by channel by height by width. We're gonna take the mean over the channel, height, and width. So we're just gonna find the mean activation for each input in the mini-batch. And when I say input, though, remember that this is gonna be, this is a layer, right? So we can put this layer anywhere we like, so it's the input to that layer. And we'll do the same thing for finding the variance. Okay, and then we're going to normalize our data by subtracting the mean. And dividing by the square root of the variance, which of course is the standard deviation. We're going to add a very small number by default, 1 eneg 5, to the denominator. Just in case the variance is 0 or ridiculously small, this will keep the number from going giant, just if we happen to get something with a very small variance. This idea of an epsilon as being something we add to a divisor is really, really common. And in general, you should not assume that the defaults are correct. Very often, the defaults are too small for algorithms that use an epsilon. Okay, so here we are, as you can see, we are normalizing the, The batch. I mean, I can call it a batch, but just remember, this isn't necessarily the first layer, right? So it's whichever layer we decide to put this in, so we normalize it. Now the thing is, maybe we don't want it to be normalized. Maybe we want it to have something other than unit variance and something other than 0 mean. Well, what we do is we then multiply it back by self.mult and add self.add. Now remember, self.mult was 1 and self.add is 0. So at first, that does nothing at all. So at first, this is just normalizing the data. So that's good. But because these are parameters, these two numbers are learnable. That means that the SGD algorithm can change them. So there's a very subtle thing going on here, which is that in fact, this might not be normalizing the data at all or normalizing the inputs to the next layer at all. Because self.mult and self.add could be anything. So I tend to think that when people think about these kind of things, like layer normalization and batch normalization, thinking of this as normalization in some ways is not the right way to think of it. It's actually doing something, I think, to really, well, it's normalizing it for the initial layers. And we don't really need LSUV anymore if we have this in here, cuz it's gonna normalize it automatically. So that's handy. But after a few batches, it's not really normalizing at all. But what it is doing is previously this idea of how big are the numbers overall, and how much variation do they have overall, was kind of built into every single number in the weight matrix and in the bias vector. This way, those two things have been turned into just two numbers. And I think this makes training a lot easier for it, basically, to just have just two numbers that it can focus on to change this overall positioning and variation. So there's something very subtle going on here, cuz it's not just doing normalization. At least not after the first few batches are complete, because it can learn to create any distribution of outputs it want. So there's our layer. So we're gonna need to change our con function let again. Previously we changed it to add activation function to be modifiable. Now we're gonna also change it to allow us to add normalization layers to the end. So our basic layers, well, we'll start off by adding our conf2d as usual. And then if you're doing normalization, we will append the normalization layer with this many inputs. Now in fact, layer norm doesn't care how many inputs, so I just ignore it. But you'll see batch normal care. If you've got an activation function, add it. And so our convolutional layer is actually a sequential bunch of layers. Now one thing that's interesting, I think, is that for bias in the conv, if you're using, well, this isn't quite true, is it? I was gonna say if you're using layer norm, you don't need bias. But actually, you kind of do. So maybe we should actually change that. For batch norm, we won't need bias. But actually, for this one, we do. So, let me put this back. Bias equals true. Bias equals bias. Okay. So then these initial layers, right, yes. So they all have bias, and then we've got bias equals false. Okay, so now in our model, we're gonna add layer normalization to every layer except for the last one. And let's see how we go. Nice, 873. Okay, 860 and 872. So just, we've just got our best by a little bit. So that's cool. So the thing about these normalization layers is though that they do cause a lot of challenges in models. And generally speaking, ever since patch normalization, ever since patch norm appeared, well, there's been this kind of like, big change of view towards it. At first, people were like, my God, batch norm is our savior. And it kind of was, it let us train much deeper models and get great results and train quickly. But then increasingly, people realized it also added a lot of complexity. These learnable parameters turned out to create all kind of complexity. And in particular, batch norm, which we'll see in a minute, created all kinds of complexity. So there has been a tendency in recent years to be trying to get rid of or at least reduce the use of these kinds of layers. So knowing how to actually initialize your models correctly at first is becoming increasingly important as people are trying to move away from these normalization layers increasingly. So I will say that. So they're still very helpful, but they're not a silver bullet, as it turns out. All right, so now let's look at batch norm. So batch norm is still not huge, but it's a little bit bigger than layer norm. And you'll see that we've got the mult and add as before. But it's not just one number to add or one number to multiply, but actually we've got a whole bunch of them. And the reason is that we're gonna have one for every channel. And so now when we take the mean and the variance, we're actually taking it over the batch dimension and the height and width dimensions. So we're ending up with one mean per channel and one variance per channel. So just like before, once we get our means and variances, we subtract them out and divide them by the epsilon modified variance. And just like before, we then multiply by mult and add add. But now we're actually multiplying by a vector of mults and we're adding a vector of adds. And that's why we have to pass in the number of filters, because we have to know how many ones and how many zeros we have in our initial mults and adds. So that's the main difference in a sense is that we have one per channel. And that we're also taking the average across all of the things in the batch. Whereas in layer norm, we didn't. Each thing in the batch had its own separate normalization it was doing. Then there's something else in batch norm which is a bit tricky. Which is that during training, we are not just subtracting the mean and the variance. But instead, we're getting an exponentially weighted moving average of the means and the variances of the last few batches. That's what this is doing. So we start out, so we basically create something called vars and something called means. And initially the variances are all one and the means are all zero. And there's one per channel, just like before, or one per filter. This is number of filters, same idea. I guess filters we tend to actually use inside the model and channels we tend to use as the first input. So I should probably say filters. Either works though. So we get out, let's for example, we get our mean per filter. And then what we do is we use this thing called lerp. And lerp is simply saying, Yes, that's what it's done. So what lerp does is it takes two numbers, in this case I'm gonna take 5 and 15, or two tensors, they could be vectors or matrices. And it creates a weighted average of them. And the amount of weight it uses is this number here. Let me explain. In this case, if I put 0.5, it's gonna take half of this number plus half of this number, so we end up with just the mean. But what if we used 0.75? Then that's gonna take 0.75 times this number plus 0.25 of this number. So it's basically kind of allows it to be on like a sliding scale. One extreme would be to take all of the second number, so that would be lerp with one there. And the other extreme would be all of the first number. And then you can slide anywhere between them, like so. So that's exactly the same as saying 5 times 0.9 plus 15 times 0.1. So this number here is how much of the second number do we have? And 1 minus that is how much of this number do we have? And you can also move this, as you can with most PyTorch things, you can move the first parameter into there and get exactly the same result. So that's what lerp is. So what we're doing here is we're doing an in-place lerp. So we're replacing self.means with 1- momentum of self.means, and plus self.momentum times this particular mini-batch's mean. So this is basically doing momentum again, which is why we indeed are calling the parameter mom for momentum. So with a mom of 0.1, which I kind of think is the opposite of what I'd expect momentum to mean, I'd expect it to be 0.9. But with a mom of 0.1, it's saying that each mini-batch, self.means will be 0.1 of this particular mini-batch's mean, and 0.9 of the previous one, the previous sequence, in fact. And that ends up giving us what's called an exponentially weighted moving average. And we do the same thing for variances. Okay, so that's only updated during training. Okay, and then during inference, we just use the saved means and variances. So this, and then why do we have buffers? What does that mean? These buffers mean that these means and variances will be actually saved as part of the model. So it's important to understand that this information about the means and variances that your model saw are saved in the model. And this is the key thing which makes batch norm very tricky to deal with, and particularly tricky, as we'll see in later lessons, with transfer learning. But what this does do is that it means that we're gonna get something that's much smoother. A single, weird mini-batch shouldn't screw things around too much. And because we're averaging across the mini-batch, it's also gonna make things smoother. So this whole thing should lead to a pretty nice, smooth training. So we can train this. So this time we're gonna use our batch norm layer for norm. Actually, we need to put the bias thing. Is that right? No, it's, no, that's fine. Don't need to change that. Okay, and one interesting thing I found here is I was able to now finally increase the learning rate up to 0.4 for the first time. So each time I was really trying to see if I can push the learning rate. And I'm now able to double the learning rate. And still, as you can see, it's training very smoothly, which is really cool. So there's actually a number of different types of layer-based normalization we can use. In this lesson, we specifically seen batch norm and layer norm. I wanted to mention that there's also instance norm and group norm. And this picture from the group norm paper explains what happens. What it's showing is that we've got here the n, c, h, w. And so they've kind of concatenated, flattened h, w into a single axis, since they can't draw 4D cubes. And what they're saying is in batch norm, all this blue stuff is what we average over. So we average across the batch and across the height and width. And we end up with one, therefore, normalization number per channel. Right, so you can kind of slide these blue blocks across. So batch norm is averaging over the batch and height and width. Layer norm, as we learned, averages over the channel and the height and the width. And it has a separate one per item in the mini-batch. I mean, kind of, it's a bit subtle, right? Because remember the overall molten add, it just had literally a single number for each, right? So it's not quite as simple as this, but that's a general idea. Instance norm, which we're not looking at today, only averages across height and width. So there's gonna be a separate one for every channel and every element of the mini-batch. And then finally, group norm, which I'm quite fond of, is like instance norm, but it arbitrarily basically groups a bunch of channels together. And you can decide how many groups of channels there are and averages over them. Group norm tends to be a bit slow, unfortunately, because the way these things are implemented is a bit tricky. But group norm does allow you to, yeah, avoid some of the challenges of some of the other methods. So it's worth trying if you can. And of course, batch norm has the additional thing of the kind of momentum-based statistics. But in general, the idea of do you use momentum-based statistics? Do you store things per channel or a single mean and variance in your buffers or whatever? All that kind of stuff along with what you average over. They're all somewhat independent choices you can make and particular combinations of those have been given particular names. And so there we go. Okay, so we've got some good initialization methods here. Let's try putting them all together. And one other thing we can do is, we've been using a batch size of 1,020 for speed purposes. If we drop it down a bit to 256, it's gonna mean that it's gonna get to see more mini-batches. So that should improve performance. And so we're trying to get to 90%, remember. So let's, yeah, do all this. This time we'll use PyTorch as its own batch norm. We'll just use PyTorches. There's nothing wrong with ours, but we try to switch to PyTorches when something we've recreated exists there. We'll use our momentum learner. And we'll fit for three epochs. And so as you can see, it's going a little bit more slowly now. And then the other thing I'm gonna do is I'm going to decrease the learning rate and keep the existing model. And then train for a little bit longer. The idea being that as it's kind of getting close to a pretty good answer, maybe it just wants to be able to fine-tune that a little bit. And so by decreasing the learning rate, we give it a chance to fine-tune a little bit. So let's see, how are we going? So we got to 87.8% accuracy after three epochs, which is an improvement, I guess, basically thanks to using this smaller mini-batch size. Now with a smaller mini-batch size, you do have to decrease the learning rate. So I found I could still get away with 0.2, which is pretty cool. And look at this, after just one more epoch, by decreasing the learning rate, we've got up to 89.7. We didn't make it, 89.9, so towards 90%, but not quite 90%, 89.9. So we're gonna have to do some more work to get up to our magical 90% number, but we are getting pretty close. All right, so that is the end of initialization, an incredibly important topic, as hopefully you've seen. Accelerated SGD, let's see if we can use this to get us up to 90%. Above 90%. So let's do our normal imports and data setup as usual. And so just to summarize what we've got, we've got our metrics callback. We've got our activation stats on the general value. So our callbacks are gonna be the device callback, put it on CUDA or whatever, the metrics, the progress bar, the activation stats. Our activation function is gonna be our general value with 0.1 leakiness and 0.4 subtraction. And we've got the in it weights, which we need to tell it about how leaky they are. And then if we're doing a learning rate finder, we've got a different set of callbacks. So there's no real reason to have a progress bar callback for the learning rate finder, I guess, it's pretty short anyway. Which reminds me, there was one little thing I didn't mention in initializing. Which is a fun trick you might wanna play around with. And in fact, Sam Watkins asked a question earlier in the chat and I didn't answer it because it's actually exactly here. In general value, I added a second thing you might have seen, which is the maximum value. And if the maximum value is set, then I clamp The value to be no more than the maximum. So basically as a result, let's say you set it to three, then the line would go up to here like it does here. And then it would go up to three like it does here, and then it would be flat. And using that can be a nice way, I mean, that'd probably go higher up to about six. But that can be a nice way to avoid numbers getting too big. And maybe if you really wanted to have fun, you could do kind of like a leaky maximum, which I haven't tried yet. Where maybe at the top it kind of goes like ten times smaller, kind of just exactly like the leaky could be. So anyway, if you do that, you'd need to make sure that you're still getting zero, one layers with your initialization. But that would be something you could consider playing with. Okay, so Let's create our own little SGD class. So an SGD class is going to need to know what parameters to optimize. And if you remember the module.parameters method returns a generator. So we use a list to turn, we want to turn that into a list so it's kind of forced to be a particular, not something that's going to change. We're going to need to know the learning rate. We're going to need to know the weight decay, which we'll look at a bit in a moment. And for reasons we'll discuss later, we also want to keep track of what batch number are we up to. So an optimizer basically has two things, a step and a zero grid. So what step's going to do is, obviously with no grad, because this is not part of the thing that we're optimizing, this is the optimization itself. We go through each tensor of parameters and we do a step of the optimizer. And we'll come back to this in a moment, we do a step of the regularizer. And we keep track of what batch number we're up to. And so what does SGD do in a step of the optimizer? It subtracts out from the parameter its gradient times the learning rate. So that's an SGD optimization step. And to zero the gradients, we go through each parameter and we zero it. And that's in torch.no grad, so. I guess it's not, okay. So use .data that way. If you use .data, then you don't need to say the no grad. It's just a little typing saver. Okay, so let's create a train learner. So it's a learner with a training callback kind of built in. And we're gonna set the optimization function to be this SGD we just wrote. And we'll use the batch norm model with the weight initialization we've used before. And if we train it, then this should give us basically the same results we've had before. While this is training, I'm gonna talk about regularization. Hopefully you remember from part one of this course or from your other learning what weight decay is. And so just to remind you, weight decay or L2 regularization are kind of the same thing. And basically what we're doing is we're saying, let's add the square of the weights to the loss function. Now, if we add the square of the weights to the loss function, so whatever our loss function is, so we'll just call it loss. We're adding plus the sum of the square of the weights. So that's our L. And so the only thing we actually care about is the derivative of that. And the derivative of that is equal to the derivative of, let's try to write that a little bit better. Is the derivative of the loss plus the derivative of this, which is just the sum of 2w. And then what we do is we multiply this bit here by some constant, which is the weight decay, so we call that weight decay. And so since the weight decay could directly incorporate the number, the two, we can actually just delete that entirely and just. Time, weight decay, do that. I'm doing this very quickly because we have already covered it in part one, so this is hopefully something that you've all seen before. So we can do weight decay by taking our gradients and adding on the weight decay times the weights. And so as a result, then in SGD, because that's part of the gradient. Man, I got it the wrong way around. Need to do that first. I guess, well, whatever, okay. So since that's part of the gradient, then in the optimization step, that's using the gradient. And it's subtracting out gradient times learning rate. But what you could do is because we're just ending up doing p.grad times self.lr, and the p.grad update is just to add in wt times weight. We could simply skip updating the gradients and instead directly update the weights to subtract out the learning rate times the wd times weight. So they would be mathematically identical. And that is what we've done here. In the regularization step, we basically say if you've got weight decay, then just take p times equals 1 minus the learning rate times the weight decay, which is mathematically the same as this, because we've got weight on both sides. So that's why the regularization is here inside our SGD. And yeah, so it's finished running, that's good, we've got an 85% accuracy. That all looks fine. And we're able to train at a high learning rate of 0.4, so that's pretty cool. So now let's add momentum. Now we had a kind of a hacky momentum learner before, but we've actually, momentum should be in an optimizer really. And so let's talk a bit about what momentum actually is. So let's just create some data. So our x's are just gonna be equally spaced numbers from minus 4 to 4, 100 of them. And our y's are just gonna be our x's divided by 3 squared, 1 minus that, plus some randomization. And so these dots here is our random data. I'm gonna show you what momentum is by example. And this is something that Sylvain Grugier helped build, so thank you Sylvain for our book actually, if memory serves correctly. Actually, it might even be the course before that. What we're gonna do is we're gonna show you what momentum looks like for a range of different levels of momentum. These are the different levels we're gonna use. So let's take a beta of 0.5, so that's gonna be our first one. So we're gonna just do a scatter plot of our x's and y's, it's the blue dots. And then we're gonna go through each of the y's. And we're gonna do, this hopefully looks familiar, this is doing a lerp. We're gonna take our previous average, which we'll start at 0, times beta, which is 0.5 plus 1 minus beta, that's 0.5, times our new average. And then we'll append that to this red line. And we'll do that for all the data points and then plot them. And you can see what happens when we do that is that the red line becomes less bumpy, right? Because each one is half, it's this exact dot, and half of whatever the red line previously was. So again, this is an exponentially weighted moving average. And so we could have implemented this using lerp. So as the beta gets higher, it's saying, do more of just be wherever the red line used to be, and less of where this particular data point is. And so that means when we have these kind of outliers, the red line doesn't jump around as much, as you see. But if your momentum gets too high, then it doesn't follow what's going on at all. And in fact, it's way behind, right? When you're using momentum, it's always going to be partially responding to how things were many batches ago. And so even at beta of 0.9 here, the red line is offset to the right. Because again, it's taking it a while for it to recognize that, things have changed. Because each time, it's 0.9 of it is where the red line used to be. And only 0.1 of it is what this data point say. So that's what momentum does. So the reason that momentum is useful is because when you have a loss function that's actually kind of like very, very bumpy, like that, right? You want to be able to follow the actual curve, right? So using momentum, you don't quite get that, but you get a kind of a version of that that's offset to the right a little bit. But still, hopefully, spending a lot more time, you don't really want to be heading off in this direction, which you would if you followed the line. And then this direction, which you would if you followed the line. You really want to be following the average of those directions. And that's what momentum lets you do. So to use momentum, we will inherit from SGD, and we will override the definition of the optimization step. Remember, there was two things that step called. It called the regularization step and the optimization step. So we're going to modify the optimization step. We're not just going to do minus equals grad times self.lr. But instead, when we create our momentum object, we will tell it what momentum we want, or default to 0.9, store that away. And then in the optimization step, for each parameter, because remember the optimization step is being called for each parameter in our model. So that's each layer's weights and each layer's biases, for example. We'll find out for that parameter, have we ever stored away its moving average of gradients before? And if we haven't, then we'll set them to 0 initially, just like we did here. And then we will do our lerp, right? So we're going to say the exponentially weighted moving average of gradients is equal to whatever it used to be times the momentum plus this actual new batch's gradients times 1 minus momentum. So that's just doing the lerp, as we discussed. And so then we're just going to do exactly the same as the SGD update step. But instead of multiplying by p.grad, we're multiplying it by p.grad average. So there's a cool little trick here, right? Which is that we are basically inventing a brand new attribute, putting it inside the parameter tensor. And that attribute is where we're storing away the exponentially weighted moving average of gradients for that particular parameter. So as we loop through the parameters, we don't have to do any special work to get access to that. So I think that's pretty handy. All right, so one interesting thing, very interesting here, I found is I could really hike the learning rate way up to 1.5. And the reason why is because we're not getting these huge bumps anymore. And so by getting rid of the huge bumps, the whole thing's just a whole lot smoother. So previously we got up to 85% because we've gone back to our 102 for batch size and just three epochs and a constant learning rate. And look at that, we've gone up to 87.6%. So it's really improved things. The loss function is nice and smooth, as you can see. Okay, and so then in our color dim plot, you can see this is actually, that's really the smoothest we've seen. And it's a bit different to the momentum learner, cuz the momentum learner didn't have this one minus part, right? It wasn't lerping, it was basically always including all of the grad plus a bit of the momentum part. So this is a different, better approach, I think. And yeah, we've got a really nice, smooth result. And the person's asking, don't we get a similar effect, I think, in terms of the smoothness if we increase the batch size? Which we do, but if you just increase the batch size, you're giving it less opportunities to update. So having a really big batch size is actually not great. Jan Lekun, who created the first really successful ConvNets, including LearnNet 5, says he thinks the ideal batch size, if you can get away with it, is one, but it's just slow. You want it to have as many opportunities to update as possible. There's this weird thing recently where people seem to be trying to create really large batch sizes, which to me is, yeah, doesn't make any sense. We want the smallest batch size we can get away with, generally speaking, to give it the most chances to update. So this has done a great job of that, and we're getting very good results, despite using only three epochs of very large batch size. Okay, so that's called momentum. Now something that was announced in a Coursera course back in maybe 2012, 2013 by Jeffrey Hinton, and it's never been published, is called rmsprop. I'll just have it running while we talk about it. rmsprop is gonna update the optimization step using something very similar to momentum, but rather than lerping on the p.grad, we're going to lerp on p.grad squared. And just to keep it kind of consistent, we won't call it mom, we'll call it square mom, but this is just the multiplier. And what are we doing with the grad squared? Well, the idea is that a large grad squared indicates a large variance of gradients. So what we're then gonna do is divide by the square root of that plus epsilon. Now, you'll see I've actually been a bit all over the place here. With my batch norm, I put the epsilon inside the square root. In this case, I'm putting the epsilon outside the square root. It does make a difference, and so be careful as to how your epsilon is being interpreted. Generally speaking, I can't remember if I've been exactly right, but I've tried to be consistent with the papers or normal implementations. This is a very common cause of confusion and errors, though. So what we're doing here is we're dividing the gradient by the amount of variation, so the square root of the moving average of gradient squared. And so the idea here is that if the gradient has been moving around all over the place, then we don't really know what it is, right? So we shouldn't do a very big update. If the gradient is very, very much the same all the time, then we're very confident about it, so we do want to be a big update. I have no idea why we're doing this in two steps. Let's just pop this over here. Now, because we are dividing our gradient by this generally, possibly rather small number, we generally have to decrease the learning rate. So we bring the learning rate back to 0.01. And as you see, it's training. It's not amazing, but it's training okay. So rmsprop can be quite nice. It's a bit bumpy there, isn't it? I mean, I could try decreasing it a little bit, maybe down to 3e neg 3 instead. That's a little bit better and a bit smoother, so that's probably good. Let's see what the colorful dimension plot looks like too, shall we? Again, it's very nice, isn't it? That's great. Now, one thing I did, which I don't think I've seen done before, I don't remember people talking about, is I actually decided not to do the normal thing of initializing to 0s. Because if I initialize to 0s, then my initial denominator here will basically be 0 plus epsilon, which will mean my initial learning rate will be very, very high, which I certainly don't want. So I actually initialized it at first to just whatever the first many batches gradient is, squared. And I think this is a really useful little trick for using rmsprop. Momentum can be a bit aggressive sometimes for some really finicky learning methods, finicky architectures. And so rmsprop can be a good way to get reasonably fast optimization of a very finicky architectures. And in particular, EfficientNet is an architecture which people have generally trained best with rmsprop. So you don't see it a whole lot, but in some ways it's just historical interest, but you see it a bit. But I mean, the thing we really want to look at is rmsprop plus momentum together. And rmsprop plus momentum together exists, it has a name. You all have heard the name many times, name is Adam. Adam is literally just rmsprop and momentum. So we rather annoyingly call them beta1 and beta2. They should be called momentum and square momentum, or momentum of squares, I suppose. So beta1 is just the momentum from the momentum optimizer. And beta2 is just the momentum for the squares from the rmsprop optimizer. So we'll store those away, and just like rmsprop, we need the epsilon. So I'm going to, as before, store away the gradient average and the square average. And then we're gonna do our lerping. But there's a nice little trick here, which is in order to avoid doing this where we just put the initial batch gradients as our starting values. We're gonna use zeros as our starting values, and then we're going to unbiased them. So basically the idea is that for the very first mini batch, Here, being lerped with the gradient, then the first mini batch will obviously be closer to zero than it should be. But we know exactly how much closer it should be to zero, which is just, it's gonna be self.beta1 times closer, at least in the first mini batch, cuz that's what we've lerped with. And then in the second mini batch, it'll be self.beta1 squared. And so in the third mini batch, it'll be self.beta1 cubed, and so forth. And that's why we had this self.i back in our SGD, which was keeping track of what mini batch were up to. So we need that in order to do this unbiasing of the average. Dear, I'm not unbiasing the square of the average. Am I? No, I'm not. Whoops. So we need to do that here as well. Wonder if this is gonna help things a little bit. Unbiased square average is going to be p.squareAverage. And that will be beta2. And so we will use those unbiased versions. So this unbiasing only matters for the first few mini batches, where otherwise it would be closer to zero than it should be. Right, so we'll run that. And so again, you would expect the learning rate to be similar to what RMSProp needs, because we're doing that same division. So we actually do have the same learning rate here. And yeah, so we're up to 86.5% accuracy. So that's pretty good. I think, Yeah, it's actually a bit less good than momentum, which is fine. Obviously, you can fiddle around. For momentum, we had 0.9. Yeah, so you could fiddle around with different values of beta2, beta1, see if you can beat the momentum version. I suspect you probably can. Okay. We're a bit out of time, aren't we? All right, I'm excited about the next bit. But I wanted to spend time doing it properly, so I won't rush through it now. But instead, we're gonna do it next time. So I will, yes, I will give you a hint that in our next lesson, we will in fact get above 90%. And it's got some very cool stuff to show you. I can't wait to show you that then. But I think in the meantime, let's give ourselves a pat on the back that we have successfully implemented. I mean, think about all this stuff we've got running and happening, and we've done the whole thing from scratch using nothing but what's in the Python standard library. We've re-implemented everything, and we understand exactly what's going on. So I think this is really quite terrifically cool, personally. I hope you feel the same way, and look forward to seeing you in the next lesson. Thanks, bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.76, "text": " Hi everybody and welcome to lesson 17 of practical deep learning for coders.", "tokens": [50364, 2421, 2201, 293, 2928, 281, 6898, 3282, 295, 8496, 2452, 2539, 337, 17656, 433, 13, 50752], "temperature": 0.0, "avg_logprob": -0.3040382464726766, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.00015354125935118645}, {"id": 1, "seek": 0, "start": 7.76, "end": 13.46, "text": " Really excited about what we're gonna look at over the next lesson or two.", "tokens": [50752, 4083, 2919, 466, 437, 321, 434, 799, 574, 412, 670, 264, 958, 6898, 420, 732, 13, 51037], "temperature": 0.0, "avg_logprob": -0.3040382464726766, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.00015354125935118645}, {"id": 2, "seek": 0, "start": 13.46, "end": 16.86, "text": " It's actually been turning out really well, much better than I could have hoped.", "tokens": [51037, 467, 311, 767, 668, 6246, 484, 534, 731, 11, 709, 1101, 813, 286, 727, 362, 19737, 13, 51207], "temperature": 0.0, "avg_logprob": -0.3040382464726766, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.00015354125935118645}, {"id": 3, "seek": 0, "start": 16.86, "end": 18.48, "text": " So I can't wait to dive in.", "tokens": [51207, 407, 286, 393, 380, 1699, 281, 9192, 294, 13, 51288], "temperature": 0.0, "avg_logprob": -0.3040382464726766, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.00015354125935118645}, {"id": 4, "seek": 0, "start": 18.48, "end": 24.04, "text": " Before I do, I'm just gonna mention a couple of minor changes that I made to our mini AI", "tokens": [51288, 4546, 286, 360, 11, 286, 478, 445, 799, 2152, 257, 1916, 295, 6696, 2962, 300, 286, 1027, 281, 527, 8382, 7318, 51566], "temperature": 0.0, "avg_logprob": -0.3040382464726766, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.00015354125935118645}, {"id": 5, "seek": 0, "start": 24.04, "end": 25.580000000000002, "text": " library this week.", "tokens": [51566, 6405, 341, 1243, 13, 51643], "temperature": 0.0, "avg_logprob": -0.3040382464726766, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.00015354125935118645}, {"id": 6, "seek": 2558, "start": 25.58, "end": 31.02, "text": " One was I went back to our callback class in the learner notebook and I did decide in", "tokens": [50364, 1485, 390, 286, 1437, 646, 281, 527, 818, 3207, 1508, 294, 264, 33347, 21060, 293, 286, 630, 4536, 294, 50636], "temperature": 0.0, "avg_logprob": -0.27305745018853084, "compression_ratio": 1.6577540106951871, "no_speech_prob": 0.0006166258826851845}, {"id": 7, "seek": 2558, "start": 31.02, "end": 41.379999999999995, "text": " the end to add a dunder getattr to it that just adds these four attributes.", "tokens": [50636, 264, 917, 281, 909, 257, 274, 6617, 483, 1591, 81, 281, 309, 300, 445, 10860, 613, 1451, 17212, 13, 51154], "temperature": 0.0, "avg_logprob": -0.27305745018853084, "compression_ratio": 1.6577540106951871, "no_speech_prob": 0.0006166258826851845}, {"id": 8, "seek": 2558, "start": 41.379999999999995, "end": 45.739999999999995, "text": " And for these four attributes, it passes it down to self.learn.", "tokens": [51154, 400, 337, 613, 1451, 17212, 11, 309, 11335, 309, 760, 281, 2698, 13, 306, 1083, 13, 51372], "temperature": 0.0, "avg_logprob": -0.27305745018853084, "compression_ratio": 1.6577540106951871, "no_speech_prob": 0.0006166258826851845}, {"id": 9, "seek": 2558, "start": 45.739999999999995, "end": 50.34, "text": " So in a callback, you'll be able to refer to model to get self.learn.model, opt will", "tokens": [51372, 407, 294, 257, 818, 3207, 11, 291, 603, 312, 1075, 281, 2864, 281, 2316, 281, 483, 2698, 13, 306, 1083, 13, 8014, 338, 11, 2427, 486, 51602], "temperature": 0.0, "avg_logprob": -0.27305745018853084, "compression_ratio": 1.6577540106951871, "no_speech_prob": 0.0006166258826851845}, {"id": 10, "seek": 5034, "start": 50.34, "end": 56.580000000000005, "text": " be self.learn.opt, batch will be self.learn.batch, epoch will be self.learn.epoch.", "tokens": [50364, 312, 2698, 13, 306, 1083, 13, 5747, 11, 15245, 486, 312, 2698, 13, 306, 1083, 13, 65, 852, 11, 30992, 339, 486, 312, 2698, 13, 306, 1083, 13, 595, 8997, 13, 50676], "temperature": 0.0, "avg_logprob": -0.24369517239657315, "compression_ratio": 1.815450643776824, "no_speech_prob": 0.003483374835923314}, {"id": 11, "seek": 5034, "start": 56.580000000000005, "end": 61.34, "text": " You can change these, you know, you could subclass the callback and add your own to", "tokens": [50676, 509, 393, 1319, 613, 11, 291, 458, 11, 291, 727, 1422, 11665, 264, 818, 3207, 293, 909, 428, 1065, 281, 50914], "temperature": 0.0, "avg_logprob": -0.24369517239657315, "compression_ratio": 1.815450643776824, "no_speech_prob": 0.003483374835923314}, {"id": 12, "seek": 5034, "start": 61.34, "end": 65.14, "text": " underscore forward or you could remove things from underscore forward or whatever.", "tokens": [50914, 37556, 2128, 420, 291, 727, 4159, 721, 490, 37556, 2128, 420, 2035, 13, 51104], "temperature": 0.0, "avg_logprob": -0.24369517239657315, "compression_ratio": 1.815450643776824, "no_speech_prob": 0.003483374835923314}, {"id": 13, "seek": 5034, "start": 65.14, "end": 71.18, "text": " But I felt like these four things I access a lot and I was sick of typing self.learn.", "tokens": [51104, 583, 286, 2762, 411, 613, 1451, 721, 286, 2105, 257, 688, 293, 286, 390, 4998, 295, 18444, 2698, 13, 306, 1083, 13, 51406], "temperature": 0.0, "avg_logprob": -0.24369517239657315, "compression_ratio": 1.815450643776824, "no_speech_prob": 0.003483374835923314}, {"id": 14, "seek": 5034, "start": 71.18, "end": 77.7, "text": " And then I added one more property which is, in a callback, there'll be a self.training", "tokens": [51406, 400, 550, 286, 3869, 472, 544, 4707, 597, 307, 11, 294, 257, 818, 3207, 11, 456, 603, 312, 257, 2698, 13, 17227, 1760, 51732], "temperature": 0.0, "avg_logprob": -0.24369517239657315, "compression_ratio": 1.815450643776824, "no_speech_prob": 0.003483374835923314}, {"id": 15, "seek": 7770, "start": 77.7, "end": 82.94, "text": " which saves me from typing self.learn.model.training.", "tokens": [50364, 597, 19155, 385, 490, 18444, 2698, 13, 306, 1083, 13, 8014, 338, 13, 17227, 1760, 13, 50626], "temperature": 0.0, "avg_logprob": -0.2986859703063965, "compression_ratio": 1.6650246305418719, "no_speech_prob": 0.00016603809490334243}, {"id": 16, "seek": 7770, "start": 82.94, "end": 86.78, "text": " Since we have model, you can get rid of the learn, but still, I mean, you so often have", "tokens": [50626, 4162, 321, 362, 2316, 11, 291, 393, 483, 3973, 295, 264, 1466, 11, 457, 920, 11, 286, 914, 11, 291, 370, 2049, 362, 50818], "temperature": 0.0, "avg_logprob": -0.2986859703063965, "compression_ratio": 1.6650246305418719, "no_speech_prob": 0.00016603809490334243}, {"id": 17, "seek": 7770, "start": 86.78, "end": 91.86, "text": " to check the training now you can just go self.training in a callback.", "tokens": [50818, 281, 1520, 264, 3097, 586, 291, 393, 445, 352, 2698, 13, 17227, 1760, 294, 257, 818, 3207, 13, 51072], "temperature": 0.0, "avg_logprob": -0.2986859703063965, "compression_ratio": 1.6650246305418719, "no_speech_prob": 0.00016603809490334243}, {"id": 18, "seek": 7770, "start": 91.86, "end": 94.7, "text": " So that was one change I made.", "tokens": [51072, 407, 300, 390, 472, 1319, 286, 1027, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2986859703063965, "compression_ratio": 1.6650246305418719, "no_speech_prob": 0.00016603809490334243}, {"id": 19, "seek": 7770, "start": 94.7, "end": 105.06, "text": " The second change I made was I found myself getting a bit bored of adding train cb every", "tokens": [51214, 440, 1150, 1319, 286, 1027, 390, 286, 1352, 2059, 1242, 257, 857, 13521, 295, 5127, 3847, 269, 65, 633, 51732], "temperature": 0.0, "avg_logprob": -0.2986859703063965, "compression_ratio": 1.6650246305418719, "no_speech_prob": 0.00016603809490334243}, {"id": 20, "seek": 7770, "start": 105.06, "end": 106.46000000000001, "text": " time.", "tokens": [51732, 565, 13, 51802], "temperature": 0.0, "avg_logprob": -0.2986859703063965, "compression_ratio": 1.6650246305418719, "no_speech_prob": 0.00016603809490334243}, {"id": 21, "seek": 10646, "start": 106.46, "end": 114.97999999999999, "text": " So what I did was I took the four training methods from the momentum learner subclass", "tokens": [50364, 407, 437, 286, 630, 390, 286, 1890, 264, 1451, 3097, 7150, 490, 264, 11244, 33347, 1422, 11665, 50790], "temperature": 0.0, "avg_logprob": -0.28404007459941666, "compression_ratio": 1.7826086956521738, "no_speech_prob": 1.0129963811777998e-05}, {"id": 22, "seek": 10646, "start": 114.97999999999999, "end": 120.86, "text": " and I've moved them into a train learner subclass along with zero grad.", "tokens": [50790, 293, 286, 600, 4259, 552, 666, 257, 3847, 33347, 1422, 11665, 2051, 365, 4018, 2771, 13, 51084], "temperature": 0.0, "avg_logprob": -0.28404007459941666, "compression_ratio": 1.7826086956521738, "no_speech_prob": 1.0129963811777998e-05}, {"id": 23, "seek": 10646, "start": 120.86, "end": 127.41999999999999, "text": " So now momentum learner actually inherits from train learner and just adds momentum.", "tokens": [51084, 407, 586, 11244, 33347, 767, 9484, 1208, 490, 3847, 33347, 293, 445, 10860, 11244, 13, 51412], "temperature": 0.0, "avg_logprob": -0.28404007459941666, "compression_ratio": 1.7826086956521738, "no_speech_prob": 1.0129963811777998e-05}, {"id": 24, "seek": 10646, "start": 127.41999999999999, "end": 134.5, "text": " It's kind of a quirky momentum method and changes zero grad to do the momentum thing.", "tokens": [51412, 467, 311, 733, 295, 257, 49515, 11244, 3170, 293, 2962, 4018, 2771, 281, 360, 264, 11244, 551, 13, 51766], "temperature": 0.0, "avg_logprob": -0.28404007459941666, "compression_ratio": 1.7826086956521738, "no_speech_prob": 1.0129963811777998e-05}, {"id": 25, "seek": 13450, "start": 134.54, "end": 139.7, "text": " So yeah, so we'll be using train learner quite a bit over the next lesson or two.", "tokens": [50366, 407, 1338, 11, 370, 321, 603, 312, 1228, 3847, 33347, 1596, 257, 857, 670, 264, 958, 6898, 420, 732, 13, 50624], "temperature": 0.0, "avg_logprob": -0.3179213007291158, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.00037998269544914365}, {"id": 26, "seek": 13450, "start": 139.7, "end": 145.22, "text": " So train learner is just a learner which has the usual training.", "tokens": [50624, 407, 3847, 33347, 307, 445, 257, 33347, 597, 575, 264, 7713, 3097, 13, 50900], "temperature": 0.0, "avg_logprob": -0.3179213007291158, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.00037998269544914365}, {"id": 27, "seek": 13450, "start": 145.22, "end": 153.22, "text": " It's exactly the same that fast.ai2 has, or you'd have in most PyTorch training loops.", "tokens": [50900, 467, 311, 2293, 264, 912, 300, 2370, 13, 1301, 17, 575, 11, 420, 291, 1116, 362, 294, 881, 9953, 51, 284, 339, 3097, 16121, 13, 51300], "temperature": 0.0, "avg_logprob": -0.3179213007291158, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.00037998269544914365}, {"id": 28, "seek": 13450, "start": 153.22, "end": 157.34, "text": " And obviously by using this you lose the ability to change these with a callback.", "tokens": [51300, 400, 2745, 538, 1228, 341, 291, 3624, 264, 3485, 281, 1319, 613, 365, 257, 818, 3207, 13, 51506], "temperature": 0.0, "avg_logprob": -0.3179213007291158, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.00037998269544914365}, {"id": 29, "seek": 13450, "start": 157.34, "end": 160.62, "text": " So it's a little bit less flexible.", "tokens": [51506, 407, 309, 311, 257, 707, 857, 1570, 11358, 13, 51670], "temperature": 0.0, "avg_logprob": -0.3179213007291158, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.00037998269544914365}, {"id": 30, "seek": 16062, "start": 161.62, "end": 165.74, "text": " Okay, so those are little changes.", "tokens": [50414, 1033, 11, 370, 729, 366, 707, 2962, 13, 50620], "temperature": 0.0, "avg_logprob": -0.3567774866668271, "compression_ratio": 1.5, "no_speech_prob": 0.0003150369448121637}, {"id": 31, "seek": 16062, "start": 165.74, "end": 172.22, "text": " And then I made some changes to what we looked at last week, which is the activations notebook.", "tokens": [50620, 400, 550, 286, 1027, 512, 2962, 281, 437, 321, 2956, 412, 1036, 1243, 11, 597, 307, 264, 2430, 763, 21060, 13, 50944], "temperature": 0.0, "avg_logprob": -0.3567774866668271, "compression_ratio": 1.5, "no_speech_prob": 0.0003150369448121637}, {"id": 32, "seek": 16062, "start": 172.22, "end": 174.62, "text": " And specifically...", "tokens": [50944, 400, 4682, 485, 51064], "temperature": 0.0, "avg_logprob": -0.3567774866668271, "compression_ratio": 1.5, "no_speech_prob": 0.0003150369448121637}, {"id": 33, "seek": 16062, "start": 174.62, "end": 184.14000000000001, "text": " Okay, so I added a hooks callback.", "tokens": [51064, 1033, 11, 370, 286, 3869, 257, 26485, 818, 3207, 13, 51540], "temperature": 0.0, "avg_logprob": -0.3567774866668271, "compression_ratio": 1.5, "no_speech_prob": 0.0003150369448121637}, {"id": 34, "seek": 16062, "start": 184.14000000000001, "end": 190.22, "text": " So previously we had a hooks class and it didn't really require too much ceremony to", "tokens": [51540, 407, 8046, 321, 632, 257, 26485, 1508, 293, 309, 994, 380, 534, 3651, 886, 709, 12813, 281, 51844], "temperature": 0.0, "avg_logprob": -0.3567774866668271, "compression_ratio": 1.5, "no_speech_prob": 0.0003150369448121637}, {"id": 35, "seek": 19022, "start": 190.22, "end": 195.9, "text": " use, but I thought we could make it even simpler and a bit more fast.ai-ish or mini.ai-ish", "tokens": [50364, 764, 11, 457, 286, 1194, 321, 727, 652, 309, 754, 18587, 293, 257, 857, 544, 2370, 13, 1301, 12, 742, 420, 8382, 13, 1301, 12, 742, 50648], "temperature": 0.0, "avg_logprob": -0.2184882845197405, "compression_ratio": 1.5263157894736843, "no_speech_prob": 5.173917998035904e-06}, {"id": 36, "seek": 19022, "start": 195.9, "end": 199.9, "text": " by putting hooks into a callback.", "tokens": [50648, 538, 3372, 26485, 666, 257, 818, 3207, 13, 50848], "temperature": 0.0, "avg_logprob": -0.2184882845197405, "compression_ratio": 1.5263157894736843, "no_speech_prob": 5.173917998035904e-06}, {"id": 37, "seek": 19022, "start": 199.9, "end": 209.18, "text": " So this callback, as usual, you pass a function that's going to be called for your hook.", "tokens": [50848, 407, 341, 818, 3207, 11, 382, 7713, 11, 291, 1320, 257, 2445, 300, 311, 516, 281, 312, 1219, 337, 428, 6328, 13, 51312], "temperature": 0.0, "avg_logprob": -0.2184882845197405, "compression_ratio": 1.5263157894736843, "no_speech_prob": 5.173917998035904e-06}, {"id": 38, "seek": 19022, "start": 209.18, "end": 214.42, "text": " And you can optionally pass it a filter as to what modules you want to hook.", "tokens": [51312, 400, 291, 393, 3614, 379, 1320, 309, 257, 6608, 382, 281, 437, 16679, 291, 528, 281, 6328, 13, 51574], "temperature": 0.0, "avg_logprob": -0.2184882845197405, "compression_ratio": 1.5263157894736843, "no_speech_prob": 5.173917998035904e-06}, {"id": 39, "seek": 21442, "start": 214.42, "end": 225.61999999999998, "text": " And then in before fit, it will filter the modules in the learner.", "tokens": [50364, 400, 550, 294, 949, 3318, 11, 309, 486, 6608, 264, 16679, 294, 264, 33347, 13, 50924], "temperature": 0.0, "avg_logprob": -0.2515045834570816, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0013670171611011028}, {"id": 40, "seek": 21442, "start": 225.61999999999998, "end": 227.45999999999998, "text": " And so this is one of these things we can now get rid of.", "tokens": [50924, 400, 370, 341, 307, 472, 295, 613, 721, 321, 393, 586, 483, 3973, 295, 13, 51016], "temperature": 0.0, "avg_logprob": -0.2515045834570816, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0013670171611011028}, {"id": 41, "seek": 21442, "start": 227.45999999999998, "end": 231.29999999999998, "text": " We don't need the .learn here because model is one of the four things we have a shortcut", "tokens": [51016, 492, 500, 380, 643, 264, 2411, 306, 1083, 510, 570, 2316, 307, 472, 295, 264, 1451, 721, 321, 362, 257, 24822, 51208], "temperature": 0.0, "avg_logprob": -0.2515045834570816, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0013670171611011028}, {"id": 42, "seek": 21442, "start": 231.29999999999998, "end": 232.29999999999998, "text": " to.", "tokens": [51208, 281, 13, 51258], "temperature": 0.0, "avg_logprob": -0.2515045834570816, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0013670171611011028}, {"id": 43, "seek": 21442, "start": 232.29999999999998, "end": 240.17999999999998, "text": " And then here we're going to create the hooks object and put it in hooks.", "tokens": [51258, 400, 550, 510, 321, 434, 516, 281, 1884, 264, 26485, 2657, 293, 829, 309, 294, 26485, 13, 51652], "temperature": 0.0, "avg_logprob": -0.2515045834570816, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0013670171611011028}, {"id": 44, "seek": 21442, "start": 240.17999999999998, "end": 242.61999999999998, "text": " And so one thing that's convenient here is the hook function.", "tokens": [51652, 400, 370, 472, 551, 300, 311, 10851, 510, 307, 264, 6328, 2445, 13, 51774], "temperature": 0.0, "avg_logprob": -0.2515045834570816, "compression_ratio": 1.7135922330097086, "no_speech_prob": 0.0013670171611011028}, {"id": 45, "seek": 24262, "start": 242.82, "end": 245.98000000000002, "text": " Now you don't have to worry, and we can get rid of learn.model, you don't have to worry", "tokens": [50374, 823, 291, 500, 380, 362, 281, 3292, 11, 293, 321, 393, 483, 3973, 295, 1466, 13, 8014, 338, 11, 291, 500, 380, 362, 281, 3292, 50532], "temperature": 0.0, "avg_logprob": -0.23518593867022292, "compression_ratio": 1.897338403041825, "no_speech_prob": 0.0011879028752446175}, {"id": 46, "seek": 24262, "start": 245.98000000000002, "end": 249.42000000000002, "text": " about checking in your hook functions whether in training or not.", "tokens": [50532, 466, 8568, 294, 428, 6328, 6828, 1968, 294, 3097, 420, 406, 13, 50704], "temperature": 0.0, "avg_logprob": -0.23518593867022292, "compression_ratio": 1.897338403041825, "no_speech_prob": 0.0011879028752446175}, {"id": 47, "seek": 24262, "start": 249.42000000000002, "end": 252.46, "text": " It always checks whether you're in training, and if so it calls that hook function you", "tokens": [50704, 467, 1009, 13834, 1968, 291, 434, 294, 3097, 11, 293, 498, 370, 309, 5498, 300, 6328, 2445, 291, 50856], "temperature": 0.0, "avg_logprob": -0.23518593867022292, "compression_ratio": 1.897338403041825, "no_speech_prob": 0.0011879028752446175}, {"id": 48, "seek": 24262, "start": 252.46, "end": 253.74, "text": " passed in.", "tokens": [50856, 4678, 294, 13, 50920], "temperature": 0.0, "avg_logprob": -0.23518593867022292, "compression_ratio": 1.897338403041825, "no_speech_prob": 0.0011879028752446175}, {"id": 49, "seek": 24262, "start": 253.74, "end": 256.98, "text": " And after it finishes it removes the hooks.", "tokens": [50920, 400, 934, 309, 23615, 309, 30445, 264, 26485, 13, 51082], "temperature": 0.0, "avg_logprob": -0.23518593867022292, "compression_ratio": 1.897338403041825, "no_speech_prob": 0.0011879028752446175}, {"id": 50, "seek": 24262, "start": 256.98, "end": 260.22, "text": " And you can iterate through the hooks and get the length of the hooks, because it just", "tokens": [51082, 400, 291, 393, 44497, 807, 264, 26485, 293, 483, 264, 4641, 295, 264, 26485, 11, 570, 309, 445, 51244], "temperature": 0.0, "avg_logprob": -0.23518593867022292, "compression_ratio": 1.897338403041825, "no_speech_prob": 0.0011879028752446175}, {"id": 51, "seek": 24262, "start": 260.22, "end": 264.34000000000003, "text": " passes these iterators and length down to self.hooks.", "tokens": [51244, 11335, 613, 17138, 3391, 293, 4641, 760, 281, 2698, 13, 71, 1212, 82, 13, 51450], "temperature": 0.0, "avg_logprob": -0.23518593867022292, "compression_ratio": 1.897338403041825, "no_speech_prob": 0.0011879028752446175}, {"id": 52, "seek": 24262, "start": 264.34000000000003, "end": 268.38, "text": " So to show you how this works, we can create a hooks callback.", "tokens": [51450, 407, 281, 855, 291, 577, 341, 1985, 11, 321, 393, 1884, 257, 26485, 818, 3207, 13, 51652], "temperature": 0.0, "avg_logprob": -0.23518593867022292, "compression_ratio": 1.897338403041825, "no_speech_prob": 0.0011879028752446175}, {"id": 53, "seek": 26838, "start": 268.38, "end": 284.82, "text": " We can use the same append stats, and then we can run the model.", "tokens": [50364, 492, 393, 764, 264, 912, 34116, 18152, 11, 293, 550, 321, 393, 1190, 264, 2316, 13, 51186], "temperature": 0.0, "avg_logprob": -0.28740618569510323, "compression_ratio": 1.4394904458598725, "no_speech_prob": 0.0006771923508495092}, {"id": 54, "seek": 26838, "start": 284.82, "end": 289.58, "text": " And so as it's training, what we're going to be able to do is, yeah, we can now then,", "tokens": [51186, 400, 370, 382, 309, 311, 3097, 11, 437, 321, 434, 516, 281, 312, 1075, 281, 360, 307, 11, 1338, 11, 321, 393, 586, 550, 11, 51424], "temperature": 0.0, "avg_logprob": -0.28740618569510323, "compression_ratio": 1.4394904458598725, "no_speech_prob": 0.0006771923508495092}, {"id": 55, "seek": 26838, "start": 289.58, "end": 291.3, "text": " here we go.", "tokens": [51424, 510, 321, 352, 13, 51510], "temperature": 0.0, "avg_logprob": -0.28740618569510323, "compression_ratio": 1.4394904458598725, "no_speech_prob": 0.0006771923508495092}, {"id": 56, "seek": 26838, "start": 291.3, "end": 296.06, "text": " So we just added that as an extra callback to our fit function.", "tokens": [51510, 407, 321, 445, 3869, 300, 382, 364, 2857, 818, 3207, 281, 527, 3318, 2445, 13, 51748], "temperature": 0.0, "avg_logprob": -0.28740618569510323, "compression_ratio": 1.4394904458598725, "no_speech_prob": 0.0006771923508495092}, {"id": 57, "seek": 29606, "start": 296.06, "end": 298.82, "text": " I don't remember if we had the extra callbacks before, I'm not sure we did.", "tokens": [50364, 286, 500, 380, 1604, 498, 321, 632, 264, 2857, 818, 17758, 949, 11, 286, 478, 406, 988, 321, 630, 13, 50502], "temperature": 0.0, "avg_logprob": -0.2523738316127232, "compression_ratio": 1.8246445497630333, "no_speech_prob": 2.2474057914223522e-05}, {"id": 58, "seek": 29606, "start": 298.82, "end": 306.3, "text": " So just to explain, it's just, I just added extra callbacks here in the fit function,", "tokens": [50502, 407, 445, 281, 2903, 11, 309, 311, 445, 11, 286, 445, 3869, 2857, 818, 17758, 510, 294, 264, 3318, 2445, 11, 50876], "temperature": 0.0, "avg_logprob": -0.2523738316127232, "compression_ratio": 1.8246445497630333, "no_speech_prob": 2.2474057914223522e-05}, {"id": 59, "seek": 29606, "start": 306.3, "end": 316.3, "text": " and we're just adding any extra callbacks here.", "tokens": [50876, 293, 321, 434, 445, 5127, 604, 2857, 818, 17758, 510, 13, 51376], "temperature": 0.0, "avg_logprob": -0.2523738316127232, "compression_ratio": 1.8246445497630333, "no_speech_prob": 2.2474057914223522e-05}, {"id": 60, "seek": 29606, "start": 316.3, "end": 320.22, "text": " So then now that we've got that callback that we created, because we can get it iterate", "tokens": [51376, 407, 550, 586, 300, 321, 600, 658, 300, 818, 3207, 300, 321, 2942, 11, 570, 321, 393, 483, 309, 44497, 51572], "temperature": 0.0, "avg_logprob": -0.2523738316127232, "compression_ratio": 1.8246445497630333, "no_speech_prob": 2.2474057914223522e-05}, {"id": 61, "seek": 29606, "start": 320.22, "end": 325.16, "text": " through it and so forth, we can just iterate it through that callback, as if it's hooks", "tokens": [51572, 807, 309, 293, 370, 5220, 11, 321, 393, 445, 44497, 309, 807, 300, 818, 3207, 11, 382, 498, 309, 311, 26485, 51819], "temperature": 0.0, "avg_logprob": -0.2523738316127232, "compression_ratio": 1.8246445497630333, "no_speech_prob": 2.2474057914223522e-05}, {"id": 62, "seek": 32516, "start": 325.26000000000005, "end": 327.0, "text": " and plot in the usual way.", "tokens": [50369, 293, 7542, 294, 264, 7713, 636, 13, 50456], "temperature": 0.0, "avg_logprob": -0.2263253180535285, "compression_ratio": 1.5794392523364487, "no_speech_prob": 2.046292502200231e-05}, {"id": 63, "seek": 32516, "start": 327.0, "end": 333.88000000000005, "text": " So that's a convenient little thing, I think it's a convenient thing I added.", "tokens": [50456, 407, 300, 311, 257, 10851, 707, 551, 11, 286, 519, 309, 311, 257, 10851, 551, 286, 3869, 13, 50800], "temperature": 0.0, "avg_logprob": -0.2263253180535285, "compression_ratio": 1.5794392523364487, "no_speech_prob": 2.046292502200231e-05}, {"id": 64, "seek": 32516, "start": 333.88000000000005, "end": 343.36, "text": " And then I took our colorful dimension stuff, which Stefano and I came up with a few years", "tokens": [50800, 400, 550, 286, 1890, 527, 18506, 10139, 1507, 11, 597, 43421, 3730, 293, 286, 1361, 493, 365, 257, 1326, 924, 51274], "temperature": 0.0, "avg_logprob": -0.2263253180535285, "compression_ratio": 1.5794392523364487, "no_speech_prob": 2.046292502200231e-05}, {"id": 65, "seek": 32516, "start": 343.36, "end": 347.0, "text": " ago, and decided to wrap all that up in a callback as well.", "tokens": [51274, 2057, 11, 293, 3047, 281, 7019, 439, 300, 493, 294, 257, 818, 3207, 382, 731, 13, 51456], "temperature": 0.0, "avg_logprob": -0.2263253180535285, "compression_ratio": 1.5794392523364487, "no_speech_prob": 2.046292502200231e-05}, {"id": 66, "seek": 32516, "start": 347.0, "end": 352.68, "text": " So I've actually subclassed here our hooks callback to create an activation stats.", "tokens": [51456, 407, 286, 600, 767, 1422, 11665, 292, 510, 527, 26485, 818, 3207, 281, 1884, 364, 24433, 18152, 13, 51740], "temperature": 0.0, "avg_logprob": -0.2263253180535285, "compression_ratio": 1.5794392523364487, "no_speech_prob": 2.046292502200231e-05}, {"id": 67, "seek": 35268, "start": 352.72, "end": 357.12, "text": " And what that's going to do is it's going to use this append stats, which appends the", "tokens": [50366, 400, 437, 300, 311, 516, 281, 360, 307, 309, 311, 516, 281, 764, 341, 34116, 18152, 11, 597, 724, 2581, 264, 50586], "temperature": 0.0, "avg_logprob": -0.3007150780070912, "compression_ratio": 1.7552083333333333, "no_speech_prob": 9.761563705978915e-05}, {"id": 68, "seek": 35268, "start": 357.12, "end": 364.12, "text": " means, the standard deviations, and the histograms.", "tokens": [50586, 1355, 11, 264, 3832, 31219, 763, 11, 293, 264, 49816, 82, 13, 50936], "temperature": 0.0, "avg_logprob": -0.3007150780070912, "compression_ratio": 1.7552083333333333, "no_speech_prob": 9.761563705978915e-05}, {"id": 69, "seek": 35268, "start": 364.12, "end": 370.84000000000003, "text": " And I changed that very slightly also, the thing which creates these kind of dead plots,", "tokens": [50936, 400, 286, 3105, 300, 588, 4748, 611, 11, 264, 551, 597, 7829, 613, 733, 295, 3116, 28609, 11, 51272], "temperature": 0.0, "avg_logprob": -0.3007150780070912, "compression_ratio": 1.7552083333333333, "no_speech_prob": 9.761563705978915e-05}, {"id": 70, "seek": 35268, "start": 370.84000000000003, "end": 378.78000000000003, "text": " I changed it to just get the ratio of the very first, the very smallest histogram bin", "tokens": [51272, 286, 3105, 309, 281, 445, 483, 264, 8509, 295, 264, 588, 700, 11, 264, 588, 16998, 49816, 5171, 51669], "temperature": 0.0, "avg_logprob": -0.3007150780070912, "compression_ratio": 1.7552083333333333, "no_speech_prob": 9.761563705978915e-05}, {"id": 71, "seek": 35268, "start": 378.78000000000003, "end": 381.12, "text": " to the rest of the bins.", "tokens": [51669, 281, 264, 1472, 295, 264, 41275, 13, 51786], "temperature": 0.0, "avg_logprob": -0.3007150780070912, "compression_ratio": 1.7552083333333333, "no_speech_prob": 9.761563705978915e-05}, {"id": 72, "seek": 38112, "start": 381.12, "end": 385.04, "text": " So these are really kind of more like very dead at this point.", "tokens": [50364, 407, 613, 366, 534, 733, 295, 544, 411, 588, 3116, 412, 341, 935, 13, 50560], "temperature": 0.0, "avg_logprob": -0.3144836635380001, "compression_ratio": 1.6067961165048543, "no_speech_prob": 2.355258220632095e-05}, {"id": 73, "seek": 38112, "start": 385.04, "end": 387.0, "text": " So these graphs look a little bit different.", "tokens": [50560, 407, 613, 24877, 574, 257, 707, 857, 819, 13, 50658], "temperature": 0.0, "avg_logprob": -0.3144836635380001, "compression_ratio": 1.6067961165048543, "no_speech_prob": 2.355258220632095e-05}, {"id": 74, "seek": 38112, "start": 387.0, "end": 397.16, "text": " Okay, so yeah, so I subclassed the hooks callback, and added the colorful dimension method, a", "tokens": [50658, 1033, 11, 370, 1338, 11, 370, 286, 1422, 11665, 292, 264, 26485, 818, 3207, 11, 293, 3869, 264, 18506, 10139, 3170, 11, 257, 51166], "temperature": 0.0, "avg_logprob": -0.3144836635380001, "compression_ratio": 1.6067961165048543, "no_speech_prob": 2.355258220632095e-05}, {"id": 75, "seek": 38112, "start": 397.16, "end": 400.28000000000003, "text": " dead chart method, and a plot stats method.", "tokens": [51166, 3116, 6927, 3170, 11, 293, 257, 7542, 18152, 3170, 13, 51322], "temperature": 0.0, "avg_logprob": -0.3144836635380001, "compression_ratio": 1.6067961165048543, "no_speech_prob": 2.355258220632095e-05}, {"id": 76, "seek": 38112, "start": 400.28000000000003, "end": 408.56, "text": " So to see them at work, if we want to get the activations on all of the cons, then we", "tokens": [51322, 407, 281, 536, 552, 412, 589, 11, 498, 321, 528, 281, 483, 264, 2430, 763, 322, 439, 295, 264, 1014, 11, 550, 321, 51736], "temperature": 0.0, "avg_logprob": -0.3144836635380001, "compression_ratio": 1.6067961165048543, "no_speech_prob": 2.355258220632095e-05}, {"id": 77, "seek": 40856, "start": 408.64, "end": 415.16, "text": " train our model, and then we can just call, and so we've added, created our activation", "tokens": [50368, 3847, 527, 2316, 11, 293, 550, 321, 393, 445, 818, 11, 293, 370, 321, 600, 3869, 11, 2942, 527, 24433, 50694], "temperature": 0.0, "avg_logprob": -0.3288476341649106, "compression_ratio": 1.8689655172413793, "no_speech_prob": 0.006488216575235128}, {"id": 78, "seek": 40856, "start": 415.16, "end": 424.12, "text": " stats, we've added that as an extra callback, and then, and then yeah, then we can call", "tokens": [50694, 18152, 11, 321, 600, 3869, 300, 382, 364, 2857, 818, 3207, 11, 293, 550, 11, 293, 550, 1338, 11, 550, 321, 393, 818, 51142], "temperature": 0.0, "avg_logprob": -0.3288476341649106, "compression_ratio": 1.8689655172413793, "no_speech_prob": 0.006488216575235128}, {"id": 79, "seek": 40856, "start": 424.12, "end": 429.0, "text": " colored in to get that plot, dead chart to get that plot, and plot stats to get that", "tokens": [51142, 14332, 294, 281, 483, 300, 7542, 11, 3116, 6927, 281, 483, 300, 7542, 11, 293, 7542, 18152, 281, 483, 300, 51386], "temperature": 0.0, "avg_logprob": -0.3288476341649106, "compression_ratio": 1.8689655172413793, "no_speech_prob": 0.006488216575235128}, {"id": 80, "seek": 40856, "start": 429.0, "end": 430.6, "text": " chart plot.", "tokens": [51386, 6927, 7542, 13, 51466], "temperature": 0.0, "avg_logprob": -0.3288476341649106, "compression_ratio": 1.8689655172413793, "no_speech_prob": 0.006488216575235128}, {"id": 81, "seek": 43060, "start": 430.6, "end": 439.68, "text": " So now we have absolutely no excuse for not getting all of these really fantastic, informative", "tokens": [50364, 407, 586, 321, 362, 3122, 572, 8960, 337, 406, 1242, 439, 295, 613, 534, 5456, 11, 27759, 50818], "temperature": 0.0, "avg_logprob": -0.21317760617125267, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0013670147163793445}, {"id": 82, "seek": 43060, "start": 439.68, "end": 445.0, "text": " visualizations of what's going on inside our model, because it's literally as easy as adding", "tokens": [50818, 5056, 14455, 295, 437, 311, 516, 322, 1854, 527, 2316, 11, 570, 309, 311, 3736, 382, 1858, 382, 5127, 51084], "temperature": 0.0, "avg_logprob": -0.21317760617125267, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0013670147163793445}, {"id": 83, "seek": 43060, "start": 445.0, "end": 449.48, "text": " one line of code, and just putting that in your callbacks.", "tokens": [51084, 472, 1622, 295, 3089, 11, 293, 445, 3372, 300, 294, 428, 818, 17758, 13, 51308], "temperature": 0.0, "avg_logprob": -0.21317760617125267, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0013670147163793445}, {"id": 84, "seek": 43060, "start": 449.48, "end": 453.68, "text": " So I really think that couldn't be easier, and so I hope you're, even for models you", "tokens": [51308, 407, 286, 534, 519, 300, 2809, 380, 312, 3571, 11, 293, 370, 286, 1454, 291, 434, 11, 754, 337, 5245, 291, 51518], "temperature": 0.0, "avg_logprob": -0.21317760617125267, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0013670147163793445}, {"id": 85, "seek": 43060, "start": 453.68, "end": 458.76000000000005, "text": " thought you know a training really well, why don't you try using this, because you might", "tokens": [51518, 1194, 291, 458, 257, 3097, 534, 731, 11, 983, 500, 380, 291, 853, 1228, 341, 11, 570, 291, 1062, 51772], "temperature": 0.0, "avg_logprob": -0.21317760617125267, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0013670147163793445}, {"id": 86, "seek": 45876, "start": 458.76, "end": 462.4, "text": " be surprised to discover that they're not.", "tokens": [50364, 312, 6100, 281, 4411, 300, 436, 434, 406, 13, 50546], "temperature": 0.0, "avg_logprob": -0.2631371903092894, "compression_ratio": 1.4635416666666667, "no_speech_prob": 0.0016229422762989998}, {"id": 87, "seek": 45876, "start": 462.4, "end": 472.2, "text": " Okay so those are some changes, pretty minor, but hopefully useful.", "tokens": [50546, 1033, 370, 729, 366, 512, 2962, 11, 1238, 6696, 11, 457, 4696, 4420, 13, 51036], "temperature": 0.0, "avg_logprob": -0.2631371903092894, "compression_ratio": 1.4635416666666667, "no_speech_prob": 0.0016229422762989998}, {"id": 88, "seek": 45876, "start": 472.2, "end": 477.92, "text": " And so today, and over the next lesson or two, we're going to look at trying to get", "tokens": [51036, 400, 370, 965, 11, 293, 670, 264, 958, 6898, 420, 732, 11, 321, 434, 516, 281, 574, 412, 1382, 281, 483, 51322], "temperature": 0.0, "avg_logprob": -0.2631371903092894, "compression_ratio": 1.4635416666666667, "no_speech_prob": 0.0016229422762989998}, {"id": 89, "seek": 45876, "start": 477.92, "end": 485.88, "text": " to a important milestone, which is to try to get fashion MNIST training to an accuracy", "tokens": [51322, 281, 257, 1021, 28048, 11, 597, 307, 281, 853, 281, 483, 6700, 376, 45, 19756, 3097, 281, 364, 14170, 51720], "temperature": 0.0, "avg_logprob": -0.2631371903092894, "compression_ratio": 1.4635416666666667, "no_speech_prob": 0.0016229422762989998}, {"id": 90, "seek": 48588, "start": 485.88, "end": 493.48, "text": " of 90% or more, which is certainly not the end of the road, but it's not bad.", "tokens": [50364, 295, 4289, 4, 420, 544, 11, 597, 307, 3297, 406, 264, 917, 295, 264, 3060, 11, 457, 309, 311, 406, 1578, 13, 50744], "temperature": 0.0, "avg_logprob": -0.24927514852936736, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.013636688701808453}, {"id": 91, "seek": 48588, "start": 493.48, "end": 501.96, "text": " If we look at papers with code, there's, so 90% would be a 10% error, so there's folks", "tokens": [50744, 759, 321, 574, 412, 10577, 365, 3089, 11, 456, 311, 11, 370, 4289, 4, 576, 312, 257, 1266, 4, 6713, 11, 370, 456, 311, 4024, 51168], "temperature": 0.0, "avg_logprob": -0.24927514852936736, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.013636688701808453}, {"id": 92, "seek": 48588, "start": 501.96, "end": 507.68, "text": " that have got down to 3 or 4% error in the very best, which is very impressive.", "tokens": [51168, 300, 362, 658, 760, 281, 805, 420, 1017, 4, 6713, 294, 264, 588, 1151, 11, 597, 307, 588, 8992, 13, 51454], "temperature": 0.0, "avg_logprob": -0.24927514852936736, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.013636688701808453}, {"id": 93, "seek": 48588, "start": 507.68, "end": 514.44, "text": " But you know, 10% error wouldn't be way off what's in this paper leaderboard.", "tokens": [51454, 583, 291, 458, 11, 1266, 4, 6713, 2759, 380, 312, 636, 766, 437, 311, 294, 341, 3035, 5263, 3787, 13, 51792], "temperature": 0.0, "avg_logprob": -0.24927514852936736, "compression_ratio": 1.5631067961165048, "no_speech_prob": 0.013636688701808453}, {"id": 94, "seek": 51444, "start": 514.44, "end": 521.4000000000001, "text": " I don't know how far we'll get eventually, but without using even any architectural changes,", "tokens": [50364, 286, 500, 380, 458, 577, 1400, 321, 603, 483, 4728, 11, 457, 1553, 1228, 754, 604, 26621, 2962, 11, 50712], "temperature": 0.0, "avg_logprob": -0.31720561451382107, "compression_ratio": 1.4414893617021276, "no_speech_prob": 7.843787170713767e-05}, {"id": 95, "seek": 51444, "start": 521.4000000000001, "end": 529.6400000000001, "text": " no resnets or anything, we're going to try to get into the 10% error.", "tokens": [50712, 572, 725, 77, 1385, 420, 1340, 11, 321, 434, 516, 281, 853, 281, 483, 666, 264, 1266, 4, 6713, 13, 51124], "temperature": 0.0, "avg_logprob": -0.31720561451382107, "compression_ratio": 1.4414893617021276, "no_speech_prob": 7.843787170713767e-05}, {"id": 96, "seek": 51444, "start": 529.6400000000001, "end": 540.08, "text": " All right, so the first few cells are just copied from earlier, and so here's our ridiculously", "tokens": [51124, 1057, 558, 11, 370, 264, 700, 1326, 5438, 366, 445, 25365, 490, 3071, 11, 293, 370, 510, 311, 527, 41358, 51646], "temperature": 0.0, "avg_logprob": -0.31720561451382107, "compression_ratio": 1.4414893617021276, "no_speech_prob": 7.843787170713767e-05}, {"id": 97, "seek": 51444, "start": 540.08, "end": 541.08, "text": " simple model.", "tokens": [51646, 2199, 2316, 13, 51696], "temperature": 0.0, "avg_logprob": -0.31720561451382107, "compression_ratio": 1.4414893617021276, "no_speech_prob": 7.843787170713767e-05}, {"id": 98, "seek": 54108, "start": 541.12, "end": 547.48, "text": " All I did here was I said, okay, well the very first convolution is taking a 9x9x1 channel", "tokens": [50366, 1057, 286, 630, 510, 390, 286, 848, 11, 1392, 11, 731, 264, 588, 700, 45216, 307, 1940, 257, 1722, 87, 24, 87, 16, 2269, 50684], "temperature": 0.0, "avg_logprob": -0.30523448520236546, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.0021489167120307684}, {"id": 99, "seek": 54108, "start": 547.48, "end": 553.32, "text": " input, so we should compress it at least a little bit, so I made it 8 channels output", "tokens": [50684, 4846, 11, 370, 321, 820, 14778, 309, 412, 1935, 257, 707, 857, 11, 370, 286, 1027, 309, 1649, 9235, 5598, 50976], "temperature": 0.0, "avg_logprob": -0.30523448520236546, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.0021489167120307684}, {"id": 100, "seek": 54108, "start": 553.32, "end": 559.0, "text": " for the convolution, and then I just doubled it to 16, doubled it to 32, doubled it to", "tokens": [50976, 337, 264, 45216, 11, 293, 550, 286, 445, 24405, 309, 281, 3165, 11, 24405, 309, 281, 8858, 11, 24405, 309, 281, 51260], "temperature": 0.0, "avg_logprob": -0.30523448520236546, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.0021489167120307684}, {"id": 101, "seek": 54108, "start": 559.0, "end": 568.12, "text": " 64, and so that's going to get to a, that will be as I say, 14x14 image, 7x7, a 4x4,", "tokens": [51260, 12145, 11, 293, 370, 300, 311, 516, 281, 483, 281, 257, 11, 300, 486, 312, 382, 286, 584, 11, 3499, 87, 7271, 3256, 11, 1614, 87, 22, 11, 257, 1017, 87, 19, 11, 51716], "temperature": 0.0, "avg_logprob": -0.30523448520236546, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.0021489167120307684}, {"id": 102, "seek": 56812, "start": 568.16, "end": 574.12, "text": " a 2x2, and then this one gets us to a 1x1, so of course we get the 10 digits.", "tokens": [50366, 257, 568, 87, 17, 11, 293, 550, 341, 472, 2170, 505, 281, 257, 502, 87, 16, 11, 370, 295, 1164, 321, 483, 264, 1266, 27011, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2739537845958363, "compression_ratio": 1.6049382716049383, "no_speech_prob": 7.254358206409961e-05}, {"id": 103, "seek": 56812, "start": 574.12, "end": 580.96, "text": " So there was no thought at all behind really this architecture, this pure convolutional", "tokens": [50664, 407, 456, 390, 572, 1194, 412, 439, 2261, 534, 341, 9482, 11, 341, 6075, 45216, 304, 51006], "temperature": 0.0, "avg_logprob": -0.2739537845958363, "compression_ratio": 1.6049382716049383, "no_speech_prob": 7.254358206409961e-05}, {"id": 104, "seek": 56812, "start": 580.96, "end": 584.0, "text": " architecture.", "tokens": [51006, 9482, 13, 51158], "temperature": 0.0, "avg_logprob": -0.2739537845958363, "compression_ratio": 1.6049382716049383, "no_speech_prob": 7.254358206409961e-05}, {"id": 105, "seek": 56812, "start": 584.0, "end": 589.0, "text": " And remember this flatten at the end is necessary to get rid of the unit axes that we end up", "tokens": [51158, 400, 1604, 341, 24183, 412, 264, 917, 307, 4818, 281, 483, 3973, 295, 264, 4985, 35387, 300, 321, 917, 493, 51408], "temperature": 0.0, "avg_logprob": -0.2739537845958363, "compression_ratio": 1.6049382716049383, "no_speech_prob": 7.254358206409961e-05}, {"id": 106, "seek": 56812, "start": 589.0, "end": 591.44, "text": " with, because this is a 1x1.", "tokens": [51408, 365, 11, 570, 341, 307, 257, 502, 87, 16, 13, 51530], "temperature": 0.0, "avg_logprob": -0.2739537845958363, "compression_ratio": 1.6049382716049383, "no_speech_prob": 7.254358206409961e-05}, {"id": 107, "seek": 56812, "start": 591.44, "end": 597.52, "text": " Okay, so let's do a learning rate finder on this very simple model, and what I found was", "tokens": [51530, 1033, 11, 370, 718, 311, 360, 257, 2539, 3314, 915, 260, 322, 341, 588, 2199, 2316, 11, 293, 437, 286, 1352, 390, 51834], "temperature": 0.0, "avg_logprob": -0.2739537845958363, "compression_ratio": 1.6049382716049383, "no_speech_prob": 7.254358206409961e-05}, {"id": 108, "seek": 59752, "start": 597.52, "end": 604.16, "text": " that this model is, and you know, this situation is so bad that when I tried to use the learning", "tokens": [50364, 300, 341, 2316, 307, 11, 293, 291, 458, 11, 341, 2590, 307, 370, 1578, 300, 562, 286, 3031, 281, 764, 264, 2539, 50696], "temperature": 0.0, "avg_logprob": -0.24427907220248518, "compression_ratio": 1.5473684210526315, "no_speech_prob": 5.771918495156569e-06}, {"id": 109, "seek": 59752, "start": 604.16, "end": 610.76, "text": " rate finder kind of in the usual way, which would be just to say, you know, start at 1e", "tokens": [50696, 3314, 915, 260, 733, 295, 294, 264, 7713, 636, 11, 597, 576, 312, 445, 281, 584, 11, 291, 458, 11, 722, 412, 502, 68, 51026], "temperature": 0.0, "avg_logprob": -0.24427907220248518, "compression_ratio": 1.5473684210526315, "no_speech_prob": 5.771918495156569e-06}, {"id": 110, "seek": 59752, "start": 610.76, "end": 620.8, "text": " neg 5 or 1e neg 4, say, and then run it, it kind of looks ridiculous.", "tokens": [51026, 2485, 1025, 420, 502, 68, 2485, 1017, 11, 584, 11, 293, 550, 1190, 309, 11, 309, 733, 295, 1542, 11083, 13, 51528], "temperature": 0.0, "avg_logprob": -0.24427907220248518, "compression_ratio": 1.5473684210526315, "no_speech_prob": 5.771918495156569e-06}, {"id": 111, "seek": 59752, "start": 620.8, "end": 623.28, "text": " It's impossible to see what's going on.", "tokens": [51528, 467, 311, 6243, 281, 536, 437, 311, 516, 322, 13, 51652], "temperature": 0.0, "avg_logprob": -0.24427907220248518, "compression_ratio": 1.5473684210526315, "no_speech_prob": 5.771918495156569e-06}, {"id": 112, "seek": 62328, "start": 623.28, "end": 630.76, "text": " So if you remember, we added that multiplier, we called it LRMult or gamma is what they", "tokens": [50364, 407, 498, 291, 1604, 11, 321, 3869, 300, 44106, 11, 321, 1219, 309, 441, 49, 44, 723, 420, 15546, 307, 437, 436, 50738], "temperature": 0.0, "avg_logprob": -0.27371539161318825, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0012643997324630618}, {"id": 113, "seek": 62328, "start": 630.76, "end": 633.3199999999999, "text": " called it in PyTorch, so we ended up calling it gamma.", "tokens": [50738, 1219, 309, 294, 9953, 51, 284, 339, 11, 370, 321, 4590, 493, 5141, 309, 15546, 13, 50866], "temperature": 0.0, "avg_logprob": -0.27371539161318825, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0012643997324630618}, {"id": 114, "seek": 62328, "start": 633.3199999999999, "end": 637.56, "text": " So I dialed that way down to make it much more gradual, which means I have to dial up", "tokens": [50866, 407, 286, 5502, 292, 300, 636, 760, 281, 652, 309, 709, 544, 32890, 11, 597, 1355, 286, 362, 281, 5502, 493, 51078], "temperature": 0.0, "avg_logprob": -0.27371539161318825, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0012643997324630618}, {"id": 115, "seek": 62328, "start": 637.56, "end": 642.4399999999999, "text": " the starting learning rate, and only then did I manage even to get the learning rate", "tokens": [51078, 264, 2891, 2539, 3314, 11, 293, 787, 550, 630, 286, 3067, 754, 281, 483, 264, 2539, 3314, 51322], "temperature": 0.0, "avg_logprob": -0.27371539161318825, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0012643997324630618}, {"id": 116, "seek": 62328, "start": 642.4399999999999, "end": 644.72, "text": " finder to tell us anything useful.", "tokens": [51322, 915, 260, 281, 980, 505, 1340, 4420, 13, 51436], "temperature": 0.0, "avg_logprob": -0.27371539161318825, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0012643997324630618}, {"id": 117, "seek": 62328, "start": 644.72, "end": 649.6, "text": " Okay, so there we are.", "tokens": [51436, 1033, 11, 370, 456, 321, 366, 13, 51680], "temperature": 0.0, "avg_logprob": -0.27371539161318825, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0012643997324630618}, {"id": 118, "seek": 64960, "start": 649.6, "end": 652.0400000000001, "text": " So that's our learning rate finder.", "tokens": [50364, 407, 300, 311, 527, 2539, 3314, 915, 260, 13, 50486], "temperature": 0.0, "avg_logprob": -0.2680550193786621, "compression_ratio": 1.4906542056074767, "no_speech_prob": 0.00019411118410062045}, {"id": 119, "seek": 64960, "start": 652.0400000000001, "end": 661.64, "text": " Actually, I'm just going to come back to these three later.", "tokens": [50486, 5135, 11, 286, 478, 445, 516, 281, 808, 646, 281, 613, 1045, 1780, 13, 50966], "temperature": 0.0, "avg_logprob": -0.2680550193786621, "compression_ratio": 1.4906542056074767, "no_speech_prob": 0.00019411118410062045}, {"id": 120, "seek": 64960, "start": 661.64, "end": 666.24, "text": " So I tried using a learning rate of 0.2, and after trying a few different values, 0.4,", "tokens": [50966, 407, 286, 3031, 1228, 257, 2539, 3314, 295, 1958, 13, 17, 11, 293, 934, 1382, 257, 1326, 819, 4190, 11, 1958, 13, 19, 11, 51196], "temperature": 0.0, "avg_logprob": -0.2680550193786621, "compression_ratio": 1.4906542056074767, "no_speech_prob": 0.00019411118410062045}, {"id": 121, "seek": 64960, "start": 666.24, "end": 669.96, "text": " 0.1, 0.2 seems about the highest we can get up to.", "tokens": [51196, 1958, 13, 16, 11, 1958, 13, 17, 2544, 466, 264, 6343, 321, 393, 483, 493, 281, 13, 51382], "temperature": 0.0, "avg_logprob": -0.2680550193786621, "compression_ratio": 1.4906542056074767, "no_speech_prob": 0.00019411118410062045}, {"id": 122, "seek": 64960, "start": 669.96, "end": 673.24, "text": " Even this actually is too high, I found.", "tokens": [51382, 2754, 341, 767, 307, 886, 1090, 11, 286, 1352, 13, 51546], "temperature": 0.0, "avg_logprob": -0.2680550193786621, "compression_ratio": 1.4906542056074767, "no_speech_prob": 0.00019411118410062045}, {"id": 123, "seek": 64960, "start": 673.24, "end": 675.96, "text": " Much lower, and it didn't train much at all.", "tokens": [51546, 12313, 3126, 11, 293, 309, 994, 380, 3847, 709, 412, 439, 13, 51682], "temperature": 0.0, "avg_logprob": -0.2680550193786621, "compression_ratio": 1.4906542056074767, "no_speech_prob": 0.00019411118410062045}, {"id": 124, "seek": 67596, "start": 676.0400000000001, "end": 681.1600000000001, "text": " You can see what happens if I do, it starts training and then it kind of, yeah, we lose", "tokens": [50368, 509, 393, 536, 437, 2314, 498, 286, 360, 11, 309, 3719, 3097, 293, 550, 309, 733, 295, 11, 1338, 11, 321, 3624, 50624], "temperature": 0.0, "avg_logprob": -0.27405986007379024, "compression_ratio": 1.7342342342342343, "no_speech_prob": 0.002287183655425906}, {"id": 125, "seek": 67596, "start": 681.1600000000001, "end": 684.0, "text": " it, which is unfortunate.", "tokens": [50624, 309, 11, 597, 307, 17843, 13, 50766], "temperature": 0.0, "avg_logprob": -0.27405986007379024, "compression_ratio": 1.7342342342342343, "no_speech_prob": 0.002287183655425906}, {"id": 126, "seek": 67596, "start": 684.0, "end": 690.48, "text": " And you can see that in the colorful dimension plot, we get this classic, you know, getting", "tokens": [50766, 400, 291, 393, 536, 300, 294, 264, 18506, 10139, 7542, 11, 321, 483, 341, 7230, 11, 291, 458, 11, 1242, 51090], "temperature": 0.0, "avg_logprob": -0.27405986007379024, "compression_ratio": 1.7342342342342343, "no_speech_prob": 0.002287183655425906}, {"id": 127, "seek": 67596, "start": 690.48, "end": 695.12, "text": " activations crashing, getting activations crashing.", "tokens": [51090, 2430, 763, 26900, 11, 1242, 2430, 763, 26900, 13, 51322], "temperature": 0.0, "avg_logprob": -0.27405986007379024, "compression_ratio": 1.7342342342342343, "no_speech_prob": 0.002287183655425906}, {"id": 128, "seek": 67596, "start": 695.12, "end": 701.12, "text": " And you can kind of see the key problem here really is that we don't have zero mean standard", "tokens": [51322, 400, 291, 393, 733, 295, 536, 264, 2141, 1154, 510, 534, 307, 300, 321, 500, 380, 362, 4018, 914, 3832, 51622], "temperature": 0.0, "avg_logprob": -0.27405986007379024, "compression_ratio": 1.7342342342342343, "no_speech_prob": 0.002287183655425906}, {"id": 129, "seek": 67596, "start": 701.12, "end": 705.6, "text": " deviation one layers at the start.", "tokens": [51622, 25163, 472, 7914, 412, 264, 722, 13, 51846], "temperature": 0.0, "avg_logprob": -0.27405986007379024, "compression_ratio": 1.7342342342342343, "no_speech_prob": 0.002287183655425906}, {"id": 130, "seek": 70560, "start": 706.24, "end": 709.48, "text": " So we certainly don't keep them throughout.", "tokens": [50396, 407, 321, 3297, 500, 380, 1066, 552, 3710, 13, 50558], "temperature": 0.0, "avg_logprob": -0.29330244182068627, "compression_ratio": 1.4583333333333333, "no_speech_prob": 6.7480464167601895e-06}, {"id": 131, "seek": 70560, "start": 709.48, "end": 711.48, "text": " And this is this is a problem.", "tokens": [50558, 400, 341, 307, 341, 307, 257, 1154, 13, 50658], "temperature": 0.0, "avg_logprob": -0.29330244182068627, "compression_ratio": 1.4583333333333333, "no_speech_prob": 6.7480464167601895e-06}, {"id": 132, "seek": 70560, "start": 711.48, "end": 719.76, "text": " Now just something I'm going to mention, by the way, is when you're training stuff in", "tokens": [50658, 823, 445, 746, 286, 478, 516, 281, 2152, 11, 538, 264, 636, 11, 307, 562, 291, 434, 3097, 1507, 294, 51072], "temperature": 0.0, "avg_logprob": -0.29330244182068627, "compression_ratio": 1.4583333333333333, "no_speech_prob": 6.7480464167601895e-06}, {"id": 133, "seek": 70560, "start": 719.76, "end": 725.24, "text": " Jupyter Notebooks, this is just a new thing we've just added.", "tokens": [51072, 22125, 88, 391, 11633, 15170, 11, 341, 307, 445, 257, 777, 551, 321, 600, 445, 3869, 13, 51346], "temperature": 0.0, "avg_logprob": -0.29330244182068627, "compression_ratio": 1.4583333333333333, "no_speech_prob": 6.7480464167601895e-06}, {"id": 134, "seek": 70560, "start": 725.24, "end": 730.36, "text": " If you get, you can easily run out of memory, GPU memory.", "tokens": [51346, 759, 291, 483, 11, 291, 393, 3612, 1190, 484, 295, 4675, 11, 18407, 4675, 13, 51602], "temperature": 0.0, "avg_logprob": -0.29330244182068627, "compression_ratio": 1.4583333333333333, "no_speech_prob": 6.7480464167601895e-06}, {"id": 135, "seek": 73036, "start": 730.36, "end": 736.52, "text": " And there's two reasons it turns out why you can particularly run out of GPU memory", "tokens": [50364, 400, 456, 311, 732, 4112, 309, 4523, 484, 983, 291, 393, 4098, 1190, 484, 295, 18407, 4675, 50672], "temperature": 0.0, "avg_logprob": -0.241699341015938, "compression_ratio": 1.4898989898989898, "no_speech_prob": 0.004538399167358875}, {"id": 136, "seek": 73036, "start": 736.52, "end": 739.84, "text": " if you run a few cells in a Jupyter Notebook.", "tokens": [50672, 498, 291, 1190, 257, 1326, 5438, 294, 257, 22125, 88, 391, 11633, 2939, 13, 50838], "temperature": 0.0, "avg_logprob": -0.241699341015938, "compression_ratio": 1.4898989898989898, "no_speech_prob": 0.004538399167358875}, {"id": 137, "seek": 73036, "start": 739.84, "end": 748.48, "text": " The first is that, kind of for your convenience, Jupyter Notebook, you might, may or may not", "tokens": [50838, 440, 700, 307, 300, 11, 733, 295, 337, 428, 19283, 11, 22125, 88, 391, 11633, 2939, 11, 291, 1062, 11, 815, 420, 815, 406, 51270], "temperature": 0.0, "avg_logprob": -0.241699341015938, "compression_ratio": 1.4898989898989898, "no_speech_prob": 0.004538399167358875}, {"id": 138, "seek": 73036, "start": 748.48, "end": 756.28, "text": " know this, actually stores the results of your previous few evaluations.", "tokens": [51270, 458, 341, 11, 767, 9512, 264, 3542, 295, 428, 3894, 1326, 43085, 13, 51660], "temperature": 0.0, "avg_logprob": -0.241699341015938, "compression_ratio": 1.4898989898989898, "no_speech_prob": 0.004538399167358875}, {"id": 139, "seek": 75628, "start": 756.28, "end": 761.6, "text": " If you just type underscore, it tells you the very last thing you evaluated.", "tokens": [50364, 759, 291, 445, 2010, 37556, 11, 309, 5112, 291, 264, 588, 1036, 551, 291, 25509, 13, 50630], "temperature": 0.0, "avg_logprob": -0.2513840853512942, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.0016743952874094248}, {"id": 140, "seek": 75628, "start": 761.6, "end": 765.28, "text": " And you can do more underscores to go backwards further in time.", "tokens": [50630, 400, 291, 393, 360, 544, 16692, 66, 2706, 281, 352, 12204, 3052, 294, 565, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2513840853512942, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.0016743952874094248}, {"id": 141, "seek": 75628, "start": 765.28, "end": 772.72, "text": " Or you can also use, oh, you can also use numbers to get the out 16, for example, would", "tokens": [50814, 1610, 291, 393, 611, 764, 11, 1954, 11, 291, 393, 611, 764, 3547, 281, 483, 264, 484, 3165, 11, 337, 1365, 11, 576, 51186], "temperature": 0.0, "avg_logprob": -0.2513840853512942, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.0016743952874094248}, {"id": 142, "seek": 75628, "start": 772.72, "end": 774.3199999999999, "text": " be underscore 16.", "tokens": [51186, 312, 37556, 3165, 13, 51266], "temperature": 0.0, "avg_logprob": -0.2513840853512942, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.0016743952874094248}, {"id": 143, "seek": 75628, "start": 774.3199999999999, "end": 782.52, "text": " Now the reason this is an issue is that if one of your outputs is a big CUDA tensor,", "tokens": [51266, 823, 264, 1778, 341, 307, 364, 2734, 307, 300, 498, 472, 295, 428, 23930, 307, 257, 955, 29777, 7509, 40863, 11, 51676], "temperature": 0.0, "avg_logprob": -0.2513840853512942, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.0016743952874094248}, {"id": 144, "seek": 78252, "start": 782.52, "end": 789.1999999999999, "text": " and you've shown it in a cell, that's going to keep that GPU memory basically forever.", "tokens": [50364, 293, 291, 600, 4898, 309, 294, 257, 2815, 11, 300, 311, 516, 281, 1066, 300, 18407, 4675, 1936, 5680, 13, 50698], "temperature": 0.0, "avg_logprob": -0.21055286952427454, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.00023050638264976442}, {"id": 145, "seek": 78252, "start": 789.1999999999999, "end": 792.84, "text": " And so that's a bit of a problem.", "tokens": [50698, 400, 370, 300, 311, 257, 857, 295, 257, 1154, 13, 50880], "temperature": 0.0, "avg_logprob": -0.21055286952427454, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.00023050638264976442}, {"id": 146, "seek": 78252, "start": 792.84, "end": 797.8, "text": " So if you are running out of memory, one thing you'd want to do is clean out all of those", "tokens": [50880, 407, 498, 291, 366, 2614, 484, 295, 4675, 11, 472, 551, 291, 1116, 528, 281, 360, 307, 2541, 484, 439, 295, 729, 51128], "temperature": 0.0, "avg_logprob": -0.21055286952427454, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.00023050638264976442}, {"id": 147, "seek": 78252, "start": 797.8, "end": 800.0799999999999, "text": " underscore blah things.", "tokens": [51128, 37556, 12288, 721, 13, 51242], "temperature": 0.0, "avg_logprob": -0.21055286952427454, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.00023050638264976442}, {"id": 148, "seek": 78252, "start": 800.0799999999999, "end": 805.0, "text": " I found that there's actually some function that nearly does that in the IPython source", "tokens": [51242, 286, 1352, 300, 456, 311, 767, 512, 2445, 300, 6217, 775, 300, 294, 264, 8671, 88, 11943, 4009, 51488], "temperature": 0.0, "avg_logprob": -0.21055286952427454, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.00023050638264976442}, {"id": 149, "seek": 78252, "start": 805.0, "end": 808.48, "text": " code, so I copied the important bits out of it and put it in here.", "tokens": [51488, 3089, 11, 370, 286, 25365, 264, 1021, 9239, 484, 295, 309, 293, 829, 309, 294, 510, 13, 51662], "temperature": 0.0, "avg_logprob": -0.21055286952427454, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.00023050638264976442}, {"id": 150, "seek": 80848, "start": 808.48, "end": 813.6, "text": " So if you call clean IPython history, it will, don't worry about the lines of code", "tokens": [50364, 407, 498, 291, 818, 2541, 8671, 88, 11943, 2503, 11, 309, 486, 11, 500, 380, 3292, 466, 264, 3876, 295, 3089, 50620], "temperature": 0.0, "avg_logprob": -0.22189720728064097, "compression_ratio": 1.5321100917431192, "no_speech_prob": 2.710874287004117e-05}, {"id": 151, "seek": 80848, "start": 813.6, "end": 820.0, "text": " at all, this is just a thing that you can use to get back that GPU memory.", "tokens": [50620, 412, 439, 11, 341, 307, 445, 257, 551, 300, 291, 393, 764, 281, 483, 646, 300, 18407, 4675, 13, 50940], "temperature": 0.0, "avg_logprob": -0.22189720728064097, "compression_ratio": 1.5321100917431192, "no_speech_prob": 2.710874287004117e-05}, {"id": 152, "seek": 80848, "start": 820.0, "end": 827.52, "text": " The second thing, which Peter figured out in the last week or so, is that you also have,", "tokens": [50940, 440, 1150, 551, 11, 597, 6508, 8932, 484, 294, 264, 1036, 1243, 420, 370, 11, 307, 300, 291, 611, 362, 11, 51316], "temperature": 0.0, "avg_logprob": -0.22189720728064097, "compression_ratio": 1.5321100917431192, "no_speech_prob": 2.710874287004117e-05}, {"id": 153, "seek": 80848, "start": 827.52, "end": 834.9200000000001, "text": " if you have a CUDA error at any point, or even any kind of exception at any point, then", "tokens": [51316, 498, 291, 362, 257, 29777, 7509, 6713, 412, 604, 935, 11, 420, 754, 604, 733, 295, 11183, 412, 604, 935, 11, 550, 51686], "temperature": 0.0, "avg_logprob": -0.22189720728064097, "compression_ratio": 1.5321100917431192, "no_speech_prob": 2.710874287004117e-05}, {"id": 154, "seek": 83492, "start": 835.0799999999999, "end": 844.0799999999999, "text": " the exception object is actually stored by Python, and any tensors that were allocated", "tokens": [50372, 264, 11183, 2657, 307, 767, 12187, 538, 15329, 11, 293, 604, 10688, 830, 300, 645, 29772, 50822], "temperature": 0.0, "avg_logprob": -0.23773972417267275, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.00021318580547813326}, {"id": 155, "seek": 83492, "start": 844.0799999999999, "end": 851.24, "text": " anywhere in that trace, in that traceback, will stay allocated basically forever.", "tokens": [50822, 4992, 294, 300, 13508, 11, 294, 300, 13508, 3207, 11, 486, 1754, 29772, 1936, 5680, 13, 51180], "temperature": 0.0, "avg_logprob": -0.23773972417267275, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.00021318580547813326}, {"id": 156, "seek": 83492, "start": 851.24, "end": 853.8, "text": " And again, that's a big problem.", "tokens": [51180, 400, 797, 11, 300, 311, 257, 955, 1154, 13, 51308], "temperature": 0.0, "avg_logprob": -0.23773972417267275, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.00021318580547813326}, {"id": 157, "seek": 83492, "start": 853.8, "end": 862.4, "text": " So I created this clean traceback function based on Peter's code, which gets rid of that.", "tokens": [51308, 407, 286, 2942, 341, 2541, 13508, 3207, 2445, 2361, 322, 6508, 311, 3089, 11, 597, 2170, 3973, 295, 300, 13, 51738], "temperature": 0.0, "avg_logprob": -0.23773972417267275, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.00021318580547813326}, {"id": 158, "seek": 86240, "start": 862.4, "end": 866.6, "text": " So this is particularly problematic because if you have a CUDA out of memory error, and", "tokens": [50364, 407, 341, 307, 4098, 19011, 570, 498, 291, 362, 257, 29777, 7509, 484, 295, 4675, 6713, 11, 293, 50574], "temperature": 0.0, "avg_logprob": -0.23980146646499634, "compression_ratio": 1.9878542510121457, "no_speech_prob": 0.0001334198605036363}, {"id": 159, "seek": 86240, "start": 866.6, "end": 871.0, "text": " then you try to rerun it, you'll still have a CUDA out of memory error, because all the", "tokens": [50574, 550, 291, 853, 281, 43819, 409, 309, 11, 291, 603, 920, 362, 257, 29777, 7509, 484, 295, 4675, 6713, 11, 570, 439, 264, 50794], "temperature": 0.0, "avg_logprob": -0.23980146646499634, "compression_ratio": 1.9878542510121457, "no_speech_prob": 0.0001334198605036363}, {"id": 160, "seek": 86240, "start": 871.0, "end": 874.1999999999999, "text": " memory that was allocated before is now in that traceback.", "tokens": [50794, 4675, 300, 390, 29772, 949, 307, 586, 294, 300, 13508, 3207, 13, 50954], "temperature": 0.0, "avg_logprob": -0.23980146646499634, "compression_ratio": 1.9878542510121457, "no_speech_prob": 0.0001334198605036363}, {"id": 161, "seek": 86240, "start": 874.1999999999999, "end": 879.88, "text": " So basically any time you get a CUDA out of memory error, or any kind of error with memory,", "tokens": [50954, 407, 1936, 604, 565, 291, 483, 257, 29777, 7509, 484, 295, 4675, 6713, 11, 420, 604, 733, 295, 6713, 365, 4675, 11, 51238], "temperature": 0.0, "avg_logprob": -0.23980146646499634, "compression_ratio": 1.9878542510121457, "no_speech_prob": 0.0001334198605036363}, {"id": 162, "seek": 86240, "start": 879.88, "end": 883.52, "text": " you can call cleanmem, and that will clean the memory in your traceback.", "tokens": [51238, 291, 393, 818, 2541, 17886, 11, 293, 300, 486, 2541, 264, 4675, 294, 428, 13508, 3207, 13, 51420], "temperature": 0.0, "avg_logprob": -0.23980146646499634, "compression_ratio": 1.9878542510121457, "no_speech_prob": 0.0001334198605036363}, {"id": 163, "seek": 86240, "start": 883.52, "end": 890.4, "text": " It will clean the memory used in your Jupyter history, do a garbage collect, empty the CUDA", "tokens": [51420, 467, 486, 2541, 264, 4675, 1143, 294, 428, 22125, 88, 391, 2503, 11, 360, 257, 14150, 2500, 11, 6707, 264, 29777, 7509, 51764], "temperature": 0.0, "avg_logprob": -0.23980146646499634, "compression_ratio": 1.9878542510121457, "no_speech_prob": 0.0001334198605036363}, {"id": 164, "seek": 89040, "start": 890.4, "end": 897.16, "text": " cache, and that will basically, should give you a totally clean GPU.", "tokens": [50364, 19459, 11, 293, 300, 486, 1936, 11, 820, 976, 291, 257, 3879, 2541, 18407, 13, 50702], "temperature": 0.0, "avg_logprob": -0.2714406296058937, "compression_ratio": 1.6016597510373445, "no_speech_prob": 9.761542605701834e-05}, {"id": 165, "seek": 89040, "start": 897.16, "end": 900.8, "text": " You don't have to restart your notebook.", "tokens": [50702, 509, 500, 380, 362, 281, 21022, 428, 21060, 13, 50884], "temperature": 0.0, "avg_logprob": -0.2714406296058937, "compression_ratio": 1.6016597510373445, "no_speech_prob": 9.761542605701834e-05}, {"id": 166, "seek": 89040, "start": 900.8, "end": 903.52, "text": " So Sam asked a very good question in the chat.", "tokens": [50884, 407, 4832, 2351, 257, 588, 665, 1168, 294, 264, 5081, 13, 51020], "temperature": 0.0, "avg_logprob": -0.2714406296058937, "compression_ratio": 1.6016597510373445, "no_speech_prob": 9.761542605701834e-05}, {"id": 167, "seek": 89040, "start": 903.52, "end": 908.6999999999999, "text": " So just to remind you guys, yes we did start, he's asking, I thought we were training an", "tokens": [51020, 407, 445, 281, 4160, 291, 1074, 11, 2086, 321, 630, 722, 11, 415, 311, 3365, 11, 286, 1194, 321, 645, 3097, 364, 51279], "temperature": 0.0, "avg_logprob": -0.2714406296058937, "compression_ratio": 1.6016597510373445, "no_speech_prob": 9.761542605701834e-05}, {"id": 168, "seek": 89040, "start": 908.6999999999999, "end": 912.28, "text": " autoencoder, or are we training a classifier, or what?", "tokens": [51279, 8399, 22660, 19866, 11, 420, 366, 321, 3097, 257, 1508, 9902, 11, 420, 437, 30, 51458], "temperature": 0.0, "avg_logprob": -0.2714406296058937, "compression_ratio": 1.6016597510373445, "no_speech_prob": 9.761542605701834e-05}, {"id": 169, "seek": 89040, "start": 912.28, "end": 918.52, "text": " So we started doing this autoencoder back in notebook 8, and we decided, oh, we don't", "tokens": [51458, 407, 321, 1409, 884, 341, 8399, 22660, 19866, 646, 294, 21060, 1649, 11, 293, 321, 3047, 11, 1954, 11, 321, 500, 380, 51770], "temperature": 0.0, "avg_logprob": -0.2714406296058937, "compression_ratio": 1.6016597510373445, "no_speech_prob": 9.761542605701834e-05}, {"id": 170, "seek": 91852, "start": 918.52, "end": 923.6, "text": " have the tools to make this work yet, so let's go back and create the tools and then", "tokens": [50364, 362, 264, 3873, 281, 652, 341, 589, 1939, 11, 370, 718, 311, 352, 646, 293, 1884, 264, 3873, 293, 550, 50618], "temperature": 0.0, "avg_logprob": -0.24271797922860205, "compression_ratio": 1.9406392694063928, "no_speech_prob": 0.00018522562459111214}, {"id": 171, "seek": 91852, "start": 923.6, "end": 926.04, "text": " come back to it.", "tokens": [50618, 808, 646, 281, 309, 13, 50740], "temperature": 0.0, "avg_logprob": -0.24271797922860205, "compression_ratio": 1.9406392694063928, "no_speech_prob": 0.00018522562459111214}, {"id": 172, "seek": 91852, "start": 926.04, "end": 931.68, "text": " So in creating the tools, we're doing a classifier, we're trying to make a really good fashion", "tokens": [50740, 407, 294, 4084, 264, 3873, 11, 321, 434, 884, 257, 1508, 9902, 11, 321, 434, 1382, 281, 652, 257, 534, 665, 6700, 51022], "temperature": 0.0, "avg_logprob": -0.24271797922860205, "compression_ratio": 1.9406392694063928, "no_speech_prob": 0.00018522562459111214}, {"id": 173, "seek": 91852, "start": 931.68, "end": 936.68, "text": " MNIST classifier, well we're trying to create tools which hopefully have a side effect we'll", "tokens": [51022, 376, 45, 19756, 1508, 9902, 11, 731, 321, 434, 1382, 281, 1884, 3873, 597, 4696, 362, 257, 1252, 1802, 321, 603, 51272], "temperature": 0.0, "avg_logprob": -0.24271797922860205, "compression_ratio": 1.9406392694063928, "no_speech_prob": 0.00018522562459111214}, {"id": 174, "seek": 91852, "start": 936.68, "end": 941.52, "text": " find of giving us a really good classifier, and then using those tools, we hope that will", "tokens": [51272, 915, 295, 2902, 505, 257, 534, 665, 1508, 9902, 11, 293, 550, 1228, 729, 3873, 11, 321, 1454, 300, 486, 51514], "temperature": 0.0, "avg_logprob": -0.24271797922860205, "compression_ratio": 1.9406392694063928, "no_speech_prob": 0.00018522562459111214}, {"id": 175, "seek": 91852, "start": 941.52, "end": 946.12, "text": " allow us to create a really good autoencoder.", "tokens": [51514, 2089, 505, 281, 1884, 257, 534, 665, 8399, 22660, 19866, 13, 51744], "temperature": 0.0, "avg_logprob": -0.24271797922860205, "compression_ratio": 1.9406392694063928, "no_speech_prob": 0.00018522562459111214}, {"id": 176, "seek": 94612, "start": 946.12, "end": 953.28, "text": " So yes, we're kind of like gradually unwinding, and we'll come back to where we were actually", "tokens": [50364, 407, 2086, 11, 321, 434, 733, 295, 411, 13145, 14853, 9245, 11, 293, 321, 603, 808, 646, 281, 689, 321, 645, 767, 50722], "temperature": 0.0, "avg_logprob": -0.27231443068560435, "compression_ratio": 1.5495049504950495, "no_speech_prob": 0.00012931477976962924}, {"id": 177, "seek": 94612, "start": 953.28, "end": 955.96, "text": " trying to get to.", "tokens": [50722, 1382, 281, 483, 281, 13, 50856], "temperature": 0.0, "avg_logprob": -0.27231443068560435, "compression_ratio": 1.5495049504950495, "no_speech_prob": 0.00012931477976962924}, {"id": 178, "seek": 94612, "start": 955.96, "end": 962.28, "text": " So that's why we're doing this classifier, the techniques and library pieces we're building", "tokens": [50856, 407, 300, 311, 983, 321, 434, 884, 341, 1508, 9902, 11, 264, 7512, 293, 6405, 3755, 321, 434, 2390, 51172], "temperature": 0.0, "avg_logprob": -0.27231443068560435, "compression_ratio": 1.5495049504950495, "no_speech_prob": 0.00012931477976962924}, {"id": 179, "seek": 94612, "start": 962.28, "end": 965.16, "text": " will be all very necessary.", "tokens": [51172, 486, 312, 439, 588, 4818, 13, 51316], "temperature": 0.0, "avg_logprob": -0.27231443068560435, "compression_ratio": 1.5495049504950495, "no_speech_prob": 0.00012931477976962924}, {"id": 180, "seek": 94612, "start": 965.16, "end": 974.96, "text": " Okay, so why do we need a zero mean, one standard deviation, why do we need that?", "tokens": [51316, 1033, 11, 370, 983, 360, 321, 643, 257, 4018, 914, 11, 472, 3832, 25163, 11, 983, 360, 321, 643, 300, 30, 51806], "temperature": 0.0, "avg_logprob": -0.27231443068560435, "compression_ratio": 1.5495049504950495, "no_speech_prob": 0.00012931477976962924}, {"id": 181, "seek": 97496, "start": 974.96, "end": 976.36, "text": " And B, how do we get it?", "tokens": [50364, 400, 363, 11, 577, 360, 321, 483, 309, 30, 50434], "temperature": 0.0, "avg_logprob": -0.24589198122742356, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.0003920410817954689}, {"id": 182, "seek": 97496, "start": 976.36, "end": 979.0400000000001, "text": " So first of all, on the way.", "tokens": [50434, 407, 700, 295, 439, 11, 322, 264, 636, 13, 50568], "temperature": 0.0, "avg_logprob": -0.24589198122742356, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.0003920410817954689}, {"id": 183, "seek": 97496, "start": 979.0400000000001, "end": 987.48, "text": " So if you think about what a neural net does, a deep learning net specifically, it takes", "tokens": [50568, 407, 498, 291, 519, 466, 437, 257, 18161, 2533, 775, 11, 257, 2452, 2539, 2533, 4682, 11, 309, 2516, 50990], "temperature": 0.0, "avg_logprob": -0.24589198122742356, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.0003920410817954689}, {"id": 184, "seek": 97496, "start": 987.48, "end": 992.36, "text": " an input and it puts it through a whole bunch of matrix multiplications, and of course there", "tokens": [50990, 364, 4846, 293, 309, 8137, 309, 807, 257, 1379, 3840, 295, 8141, 17596, 763, 11, 293, 295, 1164, 456, 51234], "temperature": 0.0, "avg_logprob": -0.24589198122742356, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.0003920410817954689}, {"id": 185, "seek": 97496, "start": 992.36, "end": 995.76, "text": " are activation functions sandwiched in there.", "tokens": [51234, 366, 24433, 6828, 11141, 292, 294, 456, 13, 51404], "temperature": 0.0, "avg_logprob": -0.24589198122742356, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.0003920410817954689}, {"id": 186, "seek": 97496, "start": 995.76, "end": 999.76, "text": " Don't worry about the activation functions, that doesn't change the argument.", "tokens": [51404, 1468, 380, 3292, 466, 264, 24433, 6828, 11, 300, 1177, 380, 1319, 264, 6770, 13, 51604], "temperature": 0.0, "avg_logprob": -0.24589198122742356, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.0003920410817954689}, {"id": 187, "seek": 99976, "start": 999.76, "end": 1008.2, "text": " So let's just imagine we start with some matrix, right.", "tokens": [50364, 407, 718, 311, 445, 3811, 321, 722, 365, 512, 8141, 11, 558, 13, 50786], "temperature": 0.0, "avg_logprob": -0.2890349023136092, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.00022693371283821762}, {"id": 188, "seek": 99976, "start": 1008.2, "end": 1011.08, "text": " Imagine a 50 deep neural net.", "tokens": [50786, 11739, 257, 2625, 2452, 18161, 2533, 13, 50930], "temperature": 0.0, "avg_logprob": -0.2890349023136092, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.00022693371283821762}, {"id": 189, "seek": 99976, "start": 1011.08, "end": 1017.04, "text": " So a 50 deep neural net basically, if we ignore the activation functions, is taking the previous", "tokens": [50930, 407, 257, 2625, 2452, 18161, 2533, 1936, 11, 498, 321, 11200, 264, 24433, 6828, 11, 307, 1940, 264, 3894, 51228], "temperature": 0.0, "avg_logprob": -0.2890349023136092, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.00022693371283821762}, {"id": 190, "seek": 99976, "start": 1017.04, "end": 1022.12, "text": " input and doing a matrix multiply by some, initially some random weights.", "tokens": [51228, 4846, 293, 884, 257, 8141, 12972, 538, 512, 11, 9105, 512, 4974, 17443, 13, 51482], "temperature": 0.0, "avg_logprob": -0.2890349023136092, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.00022693371283821762}, {"id": 191, "seek": 99976, "start": 1022.12, "end": 1027.24, "text": " So these are all, yeah, these are just a bunch of random weights.", "tokens": [51482, 407, 613, 366, 439, 11, 1338, 11, 613, 366, 445, 257, 3840, 295, 4974, 17443, 13, 51738], "temperature": 0.0, "avg_logprob": -0.2890349023136092, "compression_ratio": 1.634517766497462, "no_speech_prob": 0.00022693371283821762}, {"id": 192, "seek": 102724, "start": 1027.24, "end": 1035.94, "text": " And these are actually, rand n is mean zero, variance one.", "tokens": [50364, 400, 613, 366, 767, 11, 367, 474, 297, 307, 914, 4018, 11, 21977, 472, 13, 50799], "temperature": 0.0, "avg_logprob": -0.32816123962402344, "compression_ratio": 1.4452554744525548, "no_speech_prob": 0.000511257559992373}, {"id": 193, "seek": 102724, "start": 1035.94, "end": 1045.28, "text": " And if we run this, after 50 times of multiplying by a matrix, by a matrix, by a matrix, by", "tokens": [50799, 400, 498, 321, 1190, 341, 11, 934, 2625, 1413, 295, 30955, 538, 257, 8141, 11, 538, 257, 8141, 11, 538, 257, 8141, 11, 538, 51266], "temperature": 0.0, "avg_logprob": -0.32816123962402344, "compression_ratio": 1.4452554744525548, "no_speech_prob": 0.000511257559992373}, {"id": 194, "seek": 102724, "start": 1045.28, "end": 1051.88, "text": " a matrix, we end up with NANDs.", "tokens": [51266, 257, 8141, 11, 321, 917, 493, 365, 426, 8070, 82, 13, 51596], "temperature": 0.0, "avg_logprob": -0.32816123962402344, "compression_ratio": 1.4452554744525548, "no_speech_prob": 0.000511257559992373}, {"id": 195, "seek": 102724, "start": 1051.88, "end": 1053.6, "text": " That's no good.", "tokens": [51596, 663, 311, 572, 665, 13, 51682], "temperature": 0.0, "avg_logprob": -0.32816123962402344, "compression_ratio": 1.4452554744525548, "no_speech_prob": 0.000511257559992373}, {"id": 196, "seek": 105360, "start": 1053.6, "end": 1059.6, "text": " So that might be that our matrix, the numbers in our matrix are too big.", "tokens": [50364, 407, 300, 1062, 312, 300, 527, 8141, 11, 264, 3547, 294, 527, 8141, 366, 886, 955, 13, 50664], "temperature": 0.0, "avg_logprob": -0.27400364595301013, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.006097482051700354}, {"id": 197, "seek": 105360, "start": 1059.6, "end": 1065.04, "text": " So each time we multiply, the numbers are getting bigger and bigger and bigger.", "tokens": [50664, 407, 1184, 565, 321, 12972, 11, 264, 3547, 366, 1242, 3801, 293, 3801, 293, 3801, 13, 50936], "temperature": 0.0, "avg_logprob": -0.27400364595301013, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.006097482051700354}, {"id": 198, "seek": 105360, "start": 1065.04, "end": 1066.6399999999999, "text": " So maybe we should make them a bit smaller.", "tokens": [50936, 407, 1310, 321, 820, 652, 552, 257, 857, 4356, 13, 51016], "temperature": 0.0, "avg_logprob": -0.27400364595301013, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.006097482051700354}, {"id": 199, "seek": 105360, "start": 1066.6399999999999, "end": 1074.28, "text": " Okay, so let's try using, in the matrix we're multiplying by, let's try multiplying by 0.01.", "tokens": [51016, 1033, 11, 370, 718, 311, 853, 1228, 11, 294, 264, 8141, 321, 434, 30955, 538, 11, 718, 311, 853, 30955, 538, 1958, 13, 10607, 13, 51398], "temperature": 0.0, "avg_logprob": -0.27400364595301013, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.006097482051700354}, {"id": 200, "seek": 105360, "start": 1074.28, "end": 1075.3999999999999, "text": " And we multiply that lots of times.", "tokens": [51398, 400, 321, 12972, 300, 3195, 295, 1413, 13, 51454], "temperature": 0.0, "avg_logprob": -0.27400364595301013, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.006097482051700354}, {"id": 201, "seek": 105360, "start": 1075.3999999999999, "end": 1077.6, "text": " Oh, now we've got zeros.", "tokens": [51454, 876, 11, 586, 321, 600, 658, 35193, 13, 51564], "temperature": 0.0, "avg_logprob": -0.27400364595301013, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.006097482051700354}, {"id": 202, "seek": 105360, "start": 1077.6, "end": 1082.9599999999998, "text": " Now of course, mathematically speaking, this isn't actually NAND, it's actually some really", "tokens": [51564, 823, 295, 1164, 11, 44003, 4124, 11, 341, 1943, 380, 767, 426, 8070, 11, 309, 311, 767, 512, 534, 51832], "temperature": 0.0, "avg_logprob": -0.27400364595301013, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.006097482051700354}, {"id": 203, "seek": 108296, "start": 1083.1200000000001, "end": 1084.1200000000001, "text": " big number.", "tokens": [50372, 955, 1230, 13, 50422], "temperature": 0.0, "avg_logprob": -0.2586009439113921, "compression_ratio": 2.0926829268292684, "no_speech_prob": 0.0004373333649709821}, {"id": 204, "seek": 108296, "start": 1084.1200000000001, "end": 1087.24, "text": " Mathematically speaking, this isn't really zero, it's some really small number.", "tokens": [50422, 15776, 40197, 4124, 11, 341, 1943, 380, 534, 4018, 11, 309, 311, 512, 534, 1359, 1230, 13, 50578], "temperature": 0.0, "avg_logprob": -0.2586009439113921, "compression_ratio": 2.0926829268292684, "no_speech_prob": 0.0004373333649709821}, {"id": 205, "seek": 108296, "start": 1087.24, "end": 1091.52, "text": " But computers can't handle really, really small numbers, or really, really big numbers.", "tokens": [50578, 583, 10807, 393, 380, 4813, 534, 11, 534, 1359, 3547, 11, 420, 534, 11, 534, 955, 3547, 13, 50792], "temperature": 0.0, "avg_logprob": -0.2586009439113921, "compression_ratio": 2.0926829268292684, "no_speech_prob": 0.0004373333649709821}, {"id": 206, "seek": 108296, "start": 1091.52, "end": 1094.8400000000001, "text": " So really, really big numbers eventually just get called NAND, and really, really small", "tokens": [50792, 407, 534, 11, 534, 955, 3547, 4728, 445, 483, 1219, 426, 8070, 11, 293, 534, 11, 534, 1359, 50958], "temperature": 0.0, "avg_logprob": -0.2586009439113921, "compression_ratio": 2.0926829268292684, "no_speech_prob": 0.0004373333649709821}, {"id": 207, "seek": 108296, "start": 1094.8400000000001, "end": 1097.44, "text": " numbers eventually just get called zero.", "tokens": [50958, 3547, 4728, 445, 483, 1219, 4018, 13, 51088], "temperature": 0.0, "avg_logprob": -0.2586009439113921, "compression_ratio": 2.0926829268292684, "no_speech_prob": 0.0004373333649709821}, {"id": 208, "seek": 108296, "start": 1097.44, "end": 1101.2, "text": " So basically, they get washed out.", "tokens": [51088, 407, 1936, 11, 436, 483, 16300, 484, 13, 51276], "temperature": 0.0, "avg_logprob": -0.2586009439113921, "compression_ratio": 2.0926829268292684, "no_speech_prob": 0.0004373333649709821}, {"id": 209, "seek": 108296, "start": 1101.2, "end": 1107.44, "text": " And in fact, even if you don't get a NAND, or even if you don't quite get a zero, for", "tokens": [51276, 400, 294, 1186, 11, 754, 498, 291, 500, 380, 483, 257, 426, 8070, 11, 420, 754, 498, 291, 500, 380, 1596, 483, 257, 4018, 11, 337, 51588], "temperature": 0.0, "avg_logprob": -0.2586009439113921, "compression_ratio": 2.0926829268292684, "no_speech_prob": 0.0004373333649709821}, {"id": 210, "seek": 110744, "start": 1107.44, "end": 1115.6000000000001, "text": " numbers that are extremely big, the internal representation has no ability to discriminate", "tokens": [50364, 3547, 300, 366, 4664, 955, 11, 264, 6920, 10290, 575, 572, 3485, 281, 47833, 50772], "temperature": 0.0, "avg_logprob": -0.2622833496485001, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.00025315614766441286}, {"id": 211, "seek": 110744, "start": 1115.6000000000001, "end": 1119.3600000000001, "text": " between even slightly similar numbers.", "tokens": [50772, 1296, 754, 4748, 2531, 3547, 13, 50960], "temperature": 0.0, "avg_logprob": -0.2622833496485001, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.00025315614766441286}, {"id": 212, "seek": 110744, "start": 1119.3600000000001, "end": 1124.68, "text": " Basically in the way a floating point is stored, the further you get away from zero, the less", "tokens": [50960, 8537, 294, 264, 636, 257, 12607, 935, 307, 12187, 11, 264, 3052, 291, 483, 1314, 490, 4018, 11, 264, 1570, 51226], "temperature": 0.0, "avg_logprob": -0.2622833496485001, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.00025315614766441286}, {"id": 213, "seek": 110744, "start": 1124.68, "end": 1129.0, "text": " accurate the numbers are.", "tokens": [51226, 8559, 264, 3547, 366, 13, 51442], "temperature": 0.0, "avg_logprob": -0.2622833496485001, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.00025315614766441286}, {"id": 214, "seek": 110744, "start": 1129.0, "end": 1131.1200000000001, "text": " So yeah, this is a problem.", "tokens": [51442, 407, 1338, 11, 341, 307, 257, 1154, 13, 51548], "temperature": 0.0, "avg_logprob": -0.2622833496485001, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.00025315614766441286}, {"id": 215, "seek": 110744, "start": 1131.1200000000001, "end": 1135.68, "text": " So we have to scale our weight matrices exactly right.", "tokens": [51548, 407, 321, 362, 281, 4373, 527, 3364, 32284, 2293, 558, 13, 51776], "temperature": 0.0, "avg_logprob": -0.2622833496485001, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.00025315614766441286}, {"id": 216, "seek": 113568, "start": 1135.68, "end": 1139.8400000000001, "text": " We have to scale them in such a way that the standard deviation at every point stays", "tokens": [50364, 492, 362, 281, 4373, 552, 294, 1270, 257, 636, 300, 264, 3832, 25163, 412, 633, 935, 10834, 50572], "temperature": 0.0, "avg_logprob": -0.25341322927763965, "compression_ratio": 1.5056818181818181, "no_speech_prob": 5.3381540965347085e-06}, {"id": 217, "seek": 113568, "start": 1139.8400000000001, "end": 1144.72, "text": " at one, and the mean stays at zero.", "tokens": [50572, 412, 472, 11, 293, 264, 914, 10834, 412, 4018, 13, 50816], "temperature": 0.0, "avg_logprob": -0.25341322927763965, "compression_ratio": 1.5056818181818181, "no_speech_prob": 5.3381540965347085e-06}, {"id": 218, "seek": 113568, "start": 1144.72, "end": 1152.6000000000001, "text": " So there's actually a paper that describes how to do this for multiplying lots of matrices", "tokens": [50816, 407, 456, 311, 767, 257, 3035, 300, 15626, 577, 281, 360, 341, 337, 30955, 3195, 295, 32284, 51210], "temperature": 0.0, "avg_logprob": -0.25341322927763965, "compression_ratio": 1.5056818181818181, "no_speech_prob": 5.3381540965347085e-06}, {"id": 219, "seek": 113568, "start": 1152.6000000000001, "end": 1156.3200000000002, "text": " together.", "tokens": [51210, 1214, 13, 51396], "temperature": 0.0, "avg_logprob": -0.25341322927763965, "compression_ratio": 1.5056818181818181, "no_speech_prob": 5.3381540965347085e-06}, {"id": 220, "seek": 113568, "start": 1156.3200000000002, "end": 1160.16, "text": " And this paper basically just went through.", "tokens": [51396, 400, 341, 3035, 1936, 445, 1437, 807, 13, 51588], "temperature": 0.0, "avg_logprob": -0.25341322927763965, "compression_ratio": 1.5056818181818181, "no_speech_prob": 5.3381540965347085e-06}, {"id": 221, "seek": 116016, "start": 1160.16, "end": 1162.96, "text": " It's actually pretty simple math.", "tokens": [50364, 467, 311, 767, 1238, 2199, 5221, 13, 50504], "temperature": 0.0, "avg_logprob": -0.44226136848108094, "compression_ratio": 1.4753086419753085, "no_speech_prob": 0.001548748230561614}, {"id": 222, "seek": 116016, "start": 1162.96, "end": 1166.24, "text": " Actually, let's see.", "tokens": [50504, 5135, 11, 718, 311, 536, 13, 50668], "temperature": 0.0, "avg_logprob": -0.44226136848108094, "compression_ratio": 1.4753086419753085, "no_speech_prob": 0.001548748230561614}, {"id": 223, "seek": 116016, "start": 1166.24, "end": 1170.0, "text": " What did they do?", "tokens": [50668, 708, 630, 436, 360, 30, 50856], "temperature": 0.0, "avg_logprob": -0.44226136848108094, "compression_ratio": 1.4753086419753085, "no_speech_prob": 0.001548748230561614}, {"id": 224, "seek": 116016, "start": 1170.0, "end": 1172.0, "text": " All right.", "tokens": [50856, 1057, 558, 13, 50956], "temperature": 0.0, "avg_logprob": -0.44226136848108094, "compression_ratio": 1.4753086419753085, "no_speech_prob": 0.001548748230561614}, {"id": 225, "seek": 116016, "start": 1172.0, "end": 1178.8400000000001, "text": " Yeah, so they looked at gradients and the propagation of gradients.", "tokens": [50956, 865, 11, 370, 436, 2956, 412, 2771, 2448, 293, 264, 38377, 295, 2771, 2448, 13, 51298], "temperature": 0.0, "avg_logprob": -0.44226136848108094, "compression_ratio": 1.4753086419753085, "no_speech_prob": 0.001548748230561614}, {"id": 226, "seek": 116016, "start": 1178.8400000000001, "end": 1188.4, "text": " And they came up with a particular weight initialization of using a uniform with 1 over", "tokens": [51298, 400, 436, 1361, 493, 365, 257, 1729, 3364, 5883, 2144, 295, 1228, 257, 9452, 365, 502, 670, 51776], "temperature": 0.0, "avg_logprob": -0.44226136848108094, "compression_ratio": 1.4753086419753085, "no_speech_prob": 0.001548748230561614}, {"id": 227, "seek": 118840, "start": 1188.4, "end": 1193.2, "text": " root N as the bounds of that uniform.", "tokens": [50364, 5593, 426, 382, 264, 29905, 295, 300, 9452, 13, 50604], "temperature": 0.0, "avg_logprob": -0.34203798630658316, "compression_ratio": 1.3850931677018634, "no_speech_prob": 0.00026119023095816374}, {"id": 228, "seek": 118840, "start": 1193.2, "end": 1200.44, "text": " And they studied basically what happened with various different activation functions.", "tokens": [50604, 400, 436, 9454, 1936, 437, 2011, 365, 3683, 819, 24433, 6828, 13, 50966], "temperature": 0.0, "avg_logprob": -0.34203798630658316, "compression_ratio": 1.3850931677018634, "no_speech_prob": 0.00026119023095816374}, {"id": 229, "seek": 118840, "start": 1200.44, "end": 1211.16, "text": " And as a result, we now have this way of initializing neural networks, which is called either Gloro", "tokens": [50966, 400, 382, 257, 1874, 11, 321, 586, 362, 341, 636, 295, 5883, 3319, 18161, 9590, 11, 597, 307, 1219, 2139, 5209, 10780, 51502], "temperature": 0.0, "avg_logprob": -0.34203798630658316, "compression_ratio": 1.3850931677018634, "no_speech_prob": 0.00026119023095816374}, {"id": 230, "seek": 121116, "start": 1211.16, "end": 1218.3200000000002, "text": " initialization or Xavier initialization.", "tokens": [50364, 5883, 2144, 420, 44653, 5883, 2144, 13, 50722], "temperature": 0.0, "avg_logprob": -0.3040624884671943, "compression_ratio": 1.4086956521739131, "no_speech_prob": 0.0006462003220804036}, {"id": 231, "seek": 121116, "start": 1218.3200000000002, "end": 1229.28, "text": " And yeah, this is the amount that we scale our initialization, our random numbers by,", "tokens": [50722, 400, 1338, 11, 341, 307, 264, 2372, 300, 321, 4373, 527, 5883, 2144, 11, 527, 4974, 3547, 538, 11, 51270], "temperature": 0.0, "avg_logprob": -0.3040624884671943, "compression_ratio": 1.4086956521739131, "no_speech_prob": 0.0006462003220804036}, {"id": 232, "seek": 121116, "start": 1229.28, "end": 1233.0800000000002, "text": " where N in is the number of inputs.", "tokens": [51270, 689, 426, 294, 307, 264, 1230, 295, 15743, 13, 51460], "temperature": 0.0, "avg_logprob": -0.3040624884671943, "compression_ratio": 1.4086956521739131, "no_speech_prob": 0.0006462003220804036}, {"id": 233, "seek": 123308, "start": 1233.08, "end": 1238.8799999999999, "text": " So in our case, we have 100 inputs.", "tokens": [50364, 407, 294, 527, 1389, 11, 321, 362, 2319, 15743, 13, 50654], "temperature": 0.0, "avg_logprob": -0.2277907618769893, "compression_ratio": 1.494186046511628, "no_speech_prob": 0.00021654364536516368}, {"id": 234, "seek": 123308, "start": 1238.8799999999999, "end": 1242.8799999999999, "text": " And so root 100 is 10.", "tokens": [50654, 400, 370, 5593, 2319, 307, 1266, 13, 50854], "temperature": 0.0, "avg_logprob": -0.2277907618769893, "compression_ratio": 1.494186046511628, "no_speech_prob": 0.00021654364536516368}, {"id": 235, "seek": 123308, "start": 1242.8799999999999, "end": 1245.6, "text": " So 1 over 10 is 0.1.", "tokens": [50854, 407, 502, 670, 1266, 307, 1958, 13, 16, 13, 50990], "temperature": 0.0, "avg_logprob": -0.2277907618769893, "compression_ratio": 1.494186046511628, "no_speech_prob": 0.00021654364536516368}, {"id": 236, "seek": 123308, "start": 1245.6, "end": 1251.04, "text": " And so if we actually run that, if we start with our random numbers, and then we multiply", "tokens": [50990, 400, 370, 498, 321, 767, 1190, 300, 11, 498, 321, 722, 365, 527, 4974, 3547, 11, 293, 550, 321, 12972, 51262], "temperature": 0.0, "avg_logprob": -0.2277907618769893, "compression_ratio": 1.494186046511628, "no_speech_prob": 0.00021654364536516368}, {"id": 237, "seek": 123308, "start": 1251.04, "end": 1259.6399999999999, "text": " by random numbers times 0.1, which is, this is the Gloro initialization, you can see we", "tokens": [51262, 538, 4974, 3547, 1413, 1958, 13, 16, 11, 597, 307, 11, 341, 307, 264, 5209, 10780, 5883, 2144, 11, 291, 393, 536, 321, 51692], "temperature": 0.0, "avg_logprob": -0.2277907618769893, "compression_ratio": 1.494186046511628, "no_speech_prob": 0.00021654364536516368}, {"id": 238, "seek": 125964, "start": 1259.64, "end": 1264.1200000000001, "text": " do end up with numbers that are actually reasonable.", "tokens": [50364, 360, 917, 493, 365, 3547, 300, 366, 767, 10585, 13, 50588], "temperature": 0.0, "avg_logprob": -0.25330949783325196, "compression_ratio": 1.352112676056338, "no_speech_prob": 0.00014425932022277266}, {"id": 239, "seek": 125964, "start": 1264.1200000000001, "end": 1272.1200000000001, "text": " So that's pretty cool.", "tokens": [50588, 407, 300, 311, 1238, 1627, 13, 50988], "temperature": 0.0, "avg_logprob": -0.25330949783325196, "compression_ratio": 1.352112676056338, "no_speech_prob": 0.00014425932022277266}, {"id": 240, "seek": 125964, "start": 1272.1200000000001, "end": 1281.5200000000002, "text": " So just some background in case you're not familiar with some of these details.", "tokens": [50988, 407, 445, 512, 3678, 294, 1389, 291, 434, 406, 4963, 365, 512, 295, 613, 4365, 13, 51458], "temperature": 0.0, "avg_logprob": -0.25330949783325196, "compression_ratio": 1.352112676056338, "no_speech_prob": 0.00014425932022277266}, {"id": 241, "seek": 125964, "start": 1281.5200000000002, "end": 1284.2, "text": " What exactly do we mean by variance?", "tokens": [51458, 708, 2293, 360, 321, 914, 538, 21977, 30, 51592], "temperature": 0.0, "avg_logprob": -0.25330949783325196, "compression_ratio": 1.352112676056338, "no_speech_prob": 0.00014425932022277266}, {"id": 242, "seek": 128420, "start": 1284.2, "end": 1291.3600000000001, "text": " So if we take a tensor, let's call it T, and just put 1, 2, 4, 18 in it, the mean of", "tokens": [50364, 407, 498, 321, 747, 257, 40863, 11, 718, 311, 818, 309, 314, 11, 293, 445, 829, 502, 11, 568, 11, 1017, 11, 2443, 294, 309, 11, 264, 914, 295, 50722], "temperature": 0.0, "avg_logprob": -0.2385982631408062, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.0008167363121174276}, {"id": 243, "seek": 128420, "start": 1291.3600000000001, "end": 1294.8, "text": " that is simply the sum divided by the count.", "tokens": [50722, 300, 307, 2935, 264, 2408, 6666, 538, 264, 1207, 13, 50894], "temperature": 0.0, "avg_logprob": -0.2385982631408062, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.0008167363121174276}, {"id": 244, "seek": 128420, "start": 1294.8, "end": 1296.8400000000001, "text": " So that's 6.25.", "tokens": [50894, 407, 300, 311, 1386, 13, 6074, 13, 50996], "temperature": 0.0, "avg_logprob": -0.2385982631408062, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.0008167363121174276}, {"id": 245, "seek": 128420, "start": 1296.8400000000001, "end": 1303.1200000000001, "text": " Now we want to know, basically, we want to come up with a measure of how far away each", "tokens": [50996, 823, 321, 528, 281, 458, 11, 1936, 11, 321, 528, 281, 808, 493, 365, 257, 3481, 295, 577, 1400, 1314, 1184, 51310], "temperature": 0.0, "avg_logprob": -0.2385982631408062, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.0008167363121174276}, {"id": 246, "seek": 128420, "start": 1303.1200000000001, "end": 1304.72, "text": " data point is from the mean.", "tokens": [51310, 1412, 935, 307, 490, 264, 914, 13, 51390], "temperature": 0.0, "avg_logprob": -0.2385982631408062, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.0008167363121174276}, {"id": 247, "seek": 128420, "start": 1304.72, "end": 1307.4, "text": " That tells you how much variation there is.", "tokens": [51390, 663, 5112, 291, 577, 709, 12990, 456, 307, 13, 51524], "temperature": 0.0, "avg_logprob": -0.2385982631408062, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.0008167363121174276}, {"id": 248, "seek": 130740, "start": 1307.4, "end": 1315.4, "text": " If all the data points are very similar to each other, right, so if you've got kind of", "tokens": [50364, 759, 439, 264, 1412, 2793, 366, 588, 2531, 281, 1184, 661, 11, 558, 11, 370, 498, 291, 600, 658, 733, 295, 50764], "temperature": 0.0, "avg_logprob": -0.26346912631740815, "compression_ratio": 1.691860465116279, "no_speech_prob": 0.00039204093627631664}, {"id": 249, "seek": 130740, "start": 1315.4, "end": 1323.2800000000002, "text": " like a whole bunch of data points, and they're all pretty similar to each other, right, then", "tokens": [50764, 411, 257, 1379, 3840, 295, 1412, 2793, 11, 293, 436, 434, 439, 1238, 2531, 281, 1184, 661, 11, 558, 11, 550, 51158], "temperature": 0.0, "avg_logprob": -0.26346912631740815, "compression_ratio": 1.691860465116279, "no_speech_prob": 0.00039204093627631664}, {"id": 250, "seek": 130740, "start": 1323.2800000000002, "end": 1327.3600000000001, "text": " the mean would be about here, right?", "tokens": [51158, 264, 914, 576, 312, 466, 510, 11, 558, 30, 51362], "temperature": 0.0, "avg_logprob": -0.26346912631740815, "compression_ratio": 1.691860465116279, "no_speech_prob": 0.00039204093627631664}, {"id": 251, "seek": 130740, "start": 1327.3600000000001, "end": 1334.16, "text": " And the average distance away of each point from the mean is not very far.", "tokens": [51362, 400, 264, 4274, 4560, 1314, 295, 1184, 935, 490, 264, 914, 307, 406, 588, 1400, 13, 51702], "temperature": 0.0, "avg_logprob": -0.26346912631740815, "compression_ratio": 1.691860465116279, "no_speech_prob": 0.00039204093627631664}, {"id": 252, "seek": 133416, "start": 1334.16, "end": 1340.0, "text": " Where else if you had dots which were very widely spread all over the place, right, then", "tokens": [50364, 2305, 1646, 498, 291, 632, 15026, 597, 645, 588, 13371, 3974, 439, 670, 264, 1081, 11, 558, 11, 550, 50656], "temperature": 0.0, "avg_logprob": -0.2158506833589994, "compression_ratio": 1.6305418719211822, "no_speech_prob": 0.00015598000027239323}, {"id": 253, "seek": 133416, "start": 1340.0, "end": 1346.72, "text": " you might end up with the same mean, but the distance from each point to the mean is now", "tokens": [50656, 291, 1062, 917, 493, 365, 264, 912, 914, 11, 457, 264, 4560, 490, 1184, 935, 281, 264, 914, 307, 586, 50992], "temperature": 0.0, "avg_logprob": -0.2158506833589994, "compression_ratio": 1.6305418719211822, "no_speech_prob": 0.00015598000027239323}, {"id": 254, "seek": 133416, "start": 1346.72, "end": 1349.3600000000001, "text": " quite a long way.", "tokens": [50992, 1596, 257, 938, 636, 13, 51124], "temperature": 0.0, "avg_logprob": -0.2158506833589994, "compression_ratio": 1.6305418719211822, "no_speech_prob": 0.00015598000027239323}, {"id": 255, "seek": 133416, "start": 1349.3600000000001, "end": 1350.3600000000001, "text": " So that's what we want.", "tokens": [51124, 407, 300, 311, 437, 321, 528, 13, 51174], "temperature": 0.0, "avg_logprob": -0.2158506833589994, "compression_ratio": 1.6305418719211822, "no_speech_prob": 0.00015598000027239323}, {"id": 256, "seek": 133416, "start": 1350.3600000000001, "end": 1359.6000000000001, "text": " We want some measure of kind of how far away the points are on average from the mean.", "tokens": [51174, 492, 528, 512, 3481, 295, 733, 295, 577, 1400, 1314, 264, 2793, 366, 322, 4274, 490, 264, 914, 13, 51636], "temperature": 0.0, "avg_logprob": -0.2158506833589994, "compression_ratio": 1.6305418719211822, "no_speech_prob": 0.00015598000027239323}, {"id": 257, "seek": 133416, "start": 1359.6000000000001, "end": 1360.6000000000001, "text": " So here we could do that.", "tokens": [51636, 407, 510, 321, 727, 360, 300, 13, 51686], "temperature": 0.0, "avg_logprob": -0.2158506833589994, "compression_ratio": 1.6305418719211822, "no_speech_prob": 0.00015598000027239323}, {"id": 258, "seek": 136060, "start": 1360.6, "end": 1365.8799999999999, "text": " We can take our tensor, we can subtract the mean, and then take the mean of that.", "tokens": [50364, 492, 393, 747, 527, 40863, 11, 321, 393, 16390, 264, 914, 11, 293, 550, 747, 264, 914, 295, 300, 13, 50628], "temperature": 0.0, "avg_logprob": -0.2766688095895868, "compression_ratio": 1.6875, "no_speech_prob": 0.05500204116106033}, {"id": 259, "seek": 136060, "start": 1365.8799999999999, "end": 1368.9599999999998, "text": " Ah, well that doesn't work.", "tokens": [50628, 2438, 11, 731, 300, 1177, 380, 589, 13, 50782], "temperature": 0.0, "avg_logprob": -0.2766688095895868, "compression_ratio": 1.6875, "no_speech_prob": 0.05500204116106033}, {"id": 260, "seek": 136060, "start": 1368.9599999999998, "end": 1372.7199999999998, "text": " Because we've got some numbers that are bigger than the mean and some that are smaller than", "tokens": [50782, 1436, 321, 600, 658, 512, 3547, 300, 366, 3801, 813, 264, 914, 293, 512, 300, 366, 4356, 813, 50970], "temperature": 0.0, "avg_logprob": -0.2766688095895868, "compression_ratio": 1.6875, "no_speech_prob": 0.05500204116106033}, {"id": 261, "seek": 136060, "start": 1372.7199999999998, "end": 1378.6399999999999, "text": " the mean, and so if you average them all out, then by definition you actually get zero.", "tokens": [50970, 264, 914, 11, 293, 370, 498, 291, 4274, 552, 439, 484, 11, 550, 538, 7123, 291, 767, 483, 4018, 13, 51266], "temperature": 0.0, "avg_logprob": -0.2766688095895868, "compression_ratio": 1.6875, "no_speech_prob": 0.05500204116106033}, {"id": 262, "seek": 136060, "start": 1378.6399999999999, "end": 1386.84, "text": " So instead, you could either square those differences, and that will give you something,", "tokens": [51266, 407, 2602, 11, 291, 727, 2139, 3732, 729, 7300, 11, 293, 300, 486, 976, 291, 746, 11, 51676], "temperature": 0.0, "avg_logprob": -0.2766688095895868, "compression_ratio": 1.6875, "no_speech_prob": 0.05500204116106033}, {"id": 263, "seek": 138684, "start": 1386.84, "end": 1389.56, "text": " and you could also take the square root of that if you wanted to, to get it back to the", "tokens": [50364, 293, 291, 727, 611, 747, 264, 3732, 5593, 295, 300, 498, 291, 1415, 281, 11, 281, 483, 309, 646, 281, 264, 50500], "temperature": 0.0, "avg_logprob": -0.26591051778485697, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.023688985034823418}, {"id": 264, "seek": 138684, "start": 1389.56, "end": 1393.1599999999999, "text": " same kind of area.", "tokens": [50500, 912, 733, 295, 1859, 13, 50680], "temperature": 0.0, "avg_logprob": -0.26591051778485697, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.023688985034823418}, {"id": 265, "seek": 138684, "start": 1393.1599999999999, "end": 1395.4399999999998, "text": " Or you could take the absolute differences.", "tokens": [50680, 1610, 291, 727, 747, 264, 8236, 7300, 13, 50794], "temperature": 0.0, "avg_logprob": -0.26591051778485697, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.023688985034823418}, {"id": 266, "seek": 138684, "start": 1395.4399999999998, "end": 1401.52, "text": " Okay, so actually I'm doing this in two steps here.", "tokens": [50794, 1033, 11, 370, 767, 286, 478, 884, 341, 294, 732, 4439, 510, 13, 51098], "temperature": 0.0, "avg_logprob": -0.26591051778485697, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.023688985034823418}, {"id": 267, "seek": 138684, "start": 1401.52, "end": 1406.12, "text": " So for the first one, here it is on a different scale, and then add square root, get it on", "tokens": [51098, 407, 337, 264, 700, 472, 11, 510, 309, 307, 322, 257, 819, 4373, 11, 293, 550, 909, 3732, 5593, 11, 483, 309, 322, 51328], "temperature": 0.0, "avg_logprob": -0.26591051778485697, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.023688985034823418}, {"id": 268, "seek": 138684, "start": 1406.12, "end": 1407.1599999999999, "text": " the same scale.", "tokens": [51328, 264, 912, 4373, 13, 51380], "temperature": 0.0, "avg_logprob": -0.26591051778485697, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.023688985034823418}, {"id": 269, "seek": 138684, "start": 1407.1599999999999, "end": 1411.28, "text": " So 6.87 and 5.88 are quite similar, right?", "tokens": [51380, 407, 1386, 13, 23853, 293, 1025, 13, 16919, 366, 1596, 2531, 11, 558, 30, 51586], "temperature": 0.0, "avg_logprob": -0.26591051778485697, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.023688985034823418}, {"id": 270, "seek": 138684, "start": 1411.28, "end": 1413.72, "text": " But they're mathematically not quite the same.", "tokens": [51586, 583, 436, 434, 44003, 406, 1596, 264, 912, 13, 51708], "temperature": 0.0, "avg_logprob": -0.26591051778485697, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.023688985034823418}, {"id": 271, "seek": 138684, "start": 1413.72, "end": 1415.9599999999998, "text": " But they're both similar ideas.", "tokens": [51708, 583, 436, 434, 1293, 2531, 3487, 13, 51820], "temperature": 0.0, "avg_logprob": -0.26591051778485697, "compression_ratio": 1.7171314741035857, "no_speech_prob": 0.023688985034823418}, {"id": 272, "seek": 141596, "start": 1416.08, "end": 1422.04, "text": " So this is the mean absolute difference, and this is called the standard deviation, and", "tokens": [50370, 407, 341, 307, 264, 914, 8236, 2649, 11, 293, 341, 307, 1219, 264, 3832, 25163, 11, 293, 50668], "temperature": 0.0, "avg_logprob": -0.21552937371390207, "compression_ratio": 1.8952879581151831, "no_speech_prob": 1.6187561413971707e-05}, {"id": 273, "seek": 141596, "start": 1422.04, "end": 1427.08, "text": " this is called the variance.", "tokens": [50668, 341, 307, 1219, 264, 21977, 13, 50920], "temperature": 0.0, "avg_logprob": -0.21552937371390207, "compression_ratio": 1.8952879581151831, "no_speech_prob": 1.6187561413971707e-05}, {"id": 274, "seek": 141596, "start": 1427.08, "end": 1433.4, "text": " So the reason that the standard deviation is bigger than the mean absolute difference", "tokens": [50920, 407, 264, 1778, 300, 264, 3832, 25163, 307, 3801, 813, 264, 914, 8236, 2649, 51236], "temperature": 0.0, "avg_logprob": -0.21552937371390207, "compression_ratio": 1.8952879581151831, "no_speech_prob": 1.6187561413971707e-05}, {"id": 275, "seek": 141596, "start": 1433.4, "end": 1438.96, "text": " is because in our original data, one of the numbers is much bigger than the others.", "tokens": [51236, 307, 570, 294, 527, 3380, 1412, 11, 472, 295, 264, 3547, 307, 709, 3801, 813, 264, 2357, 13, 51514], "temperature": 0.0, "avg_logprob": -0.21552937371390207, "compression_ratio": 1.8952879581151831, "no_speech_prob": 1.6187561413971707e-05}, {"id": 276, "seek": 141596, "start": 1438.96, "end": 1444.9, "text": " And so when we square it, that number ends up having an outsized influence.", "tokens": [51514, 400, 370, 562, 321, 3732, 309, 11, 300, 1230, 5314, 493, 1419, 364, 14758, 1602, 6503, 13, 51811], "temperature": 0.0, "avg_logprob": -0.21552937371390207, "compression_ratio": 1.8952879581151831, "no_speech_prob": 1.6187561413971707e-05}, {"id": 277, "seek": 144490, "start": 1444.9, "end": 1450.22, "text": " And so that's a bit of an issue in general with standard deviation and variance, is that", "tokens": [50364, 400, 370, 300, 311, 257, 857, 295, 364, 2734, 294, 2674, 365, 3832, 25163, 293, 21977, 11, 307, 300, 50630], "temperature": 0.0, "avg_logprob": -0.32024952343532015, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.3007128472963814e-05}, {"id": 278, "seek": 144490, "start": 1450.22, "end": 1452.6200000000001, "text": " outliers like this have an outsized influence.", "tokens": [50630, 484, 23646, 411, 341, 362, 364, 14758, 1602, 6503, 13, 50750], "temperature": 0.0, "avg_logprob": -0.32024952343532015, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.3007128472963814e-05}, {"id": 279, "seek": 144490, "start": 1452.6200000000001, "end": 1458.74, "text": " So you've got to be a bit careful.", "tokens": [50750, 407, 291, 600, 658, 281, 312, 257, 857, 5026, 13, 51056], "temperature": 0.0, "avg_logprob": -0.32024952343532015, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.3007128472963814e-05}, {"id": 280, "seek": 144490, "start": 1458.74, "end": 1463.5, "text": " Okay, so here's the formula for the standard deviation.", "tokens": [51056, 1033, 11, 370, 510, 311, 264, 8513, 337, 264, 3832, 25163, 13, 51294], "temperature": 0.0, "avg_logprob": -0.32024952343532015, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.3007128472963814e-05}, {"id": 281, "seek": 144490, "start": 1463.5, "end": 1464.74, "text": " It's normally written as sigma.", "tokens": [51294, 467, 311, 5646, 3720, 382, 12771, 13, 51356], "temperature": 0.0, "avg_logprob": -0.32024952343532015, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.3007128472963814e-05}, {"id": 282, "seek": 144490, "start": 1464.74, "end": 1470.7800000000002, "text": " Okay, so it's just going to be each of our data points minus the mean squared, plus the", "tokens": [51356, 1033, 11, 370, 309, 311, 445, 516, 281, 312, 1184, 295, 527, 1412, 2793, 3175, 264, 914, 8889, 11, 1804, 264, 51658], "temperature": 0.0, "avg_logprob": -0.32024952343532015, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.3007128472963814e-05}, {"id": 283, "seek": 144490, "start": 1470.7800000000002, "end": 1474.3400000000001, "text": " next data point minus the mean squared, so forth, for all the data points.", "tokens": [51658, 958, 1412, 935, 3175, 264, 914, 8889, 11, 370, 5220, 11, 337, 439, 264, 1412, 2793, 13, 51836], "temperature": 0.0, "avg_logprob": -0.32024952343532015, "compression_ratio": 1.7615062761506277, "no_speech_prob": 1.3007128472963814e-05}, {"id": 284, "seek": 147434, "start": 1474.34, "end": 1479.22, "text": " And then divide that by the number of data points in square root.", "tokens": [50364, 400, 550, 9845, 300, 538, 264, 1230, 295, 1412, 2793, 294, 3732, 5593, 13, 50608], "temperature": 0.0, "avg_logprob": -0.27707422696627104, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.002019004663452506}, {"id": 285, "seek": 147434, "start": 1479.22, "end": 1484.4599999999998, "text": " And okay, so one thing I point out here is that the mean absolute deviation isn't used", "tokens": [50608, 400, 1392, 11, 370, 472, 551, 286, 935, 484, 510, 307, 300, 264, 914, 8236, 25163, 1943, 380, 1143, 50870], "temperature": 0.0, "avg_logprob": -0.27707422696627104, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.002019004663452506}, {"id": 286, "seek": 147434, "start": 1484.4599999999998, "end": 1492.1799999999998, "text": " as much as the standard deviation because mathematicians find it difficult to use.", "tokens": [50870, 382, 709, 382, 264, 3832, 25163, 570, 32811, 2567, 915, 309, 2252, 281, 764, 13, 51256], "temperature": 0.0, "avg_logprob": -0.27707422696627104, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.002019004663452506}, {"id": 287, "seek": 147434, "start": 1492.1799999999998, "end": 1496.4199999999998, "text": " But we're not mathematicians, we have computers, so we can use it.", "tokens": [51256, 583, 321, 434, 406, 32811, 2567, 11, 321, 362, 10807, 11, 370, 321, 393, 764, 309, 13, 51468], "temperature": 0.0, "avg_logprob": -0.27707422696627104, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.002019004663452506}, {"id": 288, "seek": 147434, "start": 1496.4199999999998, "end": 1501.9399999999998, "text": " Okay, now variance we can calculate like this, as we said.", "tokens": [51468, 1033, 11, 586, 21977, 321, 393, 8873, 411, 341, 11, 382, 321, 848, 13, 51744], "temperature": 0.0, "avg_logprob": -0.27707422696627104, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.002019004663452506}, {"id": 289, "seek": 150194, "start": 1501.94, "end": 1506.3, "text": " The mean of the square of the differences.", "tokens": [50364, 440, 914, 295, 264, 3732, 295, 264, 7300, 13, 50582], "temperature": 0.0, "avg_logprob": -0.2716239782480093, "compression_ratio": 1.8372093023255813, "no_speech_prob": 0.02262832410633564}, {"id": 290, "seek": 150194, "start": 1506.3, "end": 1510.18, "text": " And if you feel like doing some math, you could discover that actually this is exactly", "tokens": [50582, 400, 498, 291, 841, 411, 884, 512, 5221, 11, 291, 727, 4411, 300, 767, 341, 307, 2293, 50776], "temperature": 0.0, "avg_logprob": -0.2716239782480093, "compression_ratio": 1.8372093023255813, "no_speech_prob": 0.02262832410633564}, {"id": 291, "seek": 150194, "start": 1510.18, "end": 1513.8600000000001, "text": " the same, as you can see.", "tokens": [50776, 264, 912, 11, 382, 291, 393, 536, 13, 50960], "temperature": 0.0, "avg_logprob": -0.2716239782480093, "compression_ratio": 1.8372093023255813, "no_speech_prob": 0.02262832410633564}, {"id": 292, "seek": 150194, "start": 1513.8600000000001, "end": 1521.8600000000001, "text": " And this is actually nice because this is showing that the mean of the squared data", "tokens": [50960, 400, 341, 307, 767, 1481, 570, 341, 307, 4099, 300, 264, 914, 295, 264, 8889, 1412, 51360], "temperature": 0.0, "avg_logprob": -0.2716239782480093, "compression_ratio": 1.8372093023255813, "no_speech_prob": 0.02262832410633564}, {"id": 293, "seek": 150194, "start": 1521.8600000000001, "end": 1528.78, "text": " points minus the square of the mean of the data points is also the variance.", "tokens": [51360, 2793, 3175, 264, 3732, 295, 264, 914, 295, 264, 1412, 2793, 307, 611, 264, 21977, 13, 51706], "temperature": 0.0, "avg_logprob": -0.2716239782480093, "compression_ratio": 1.8372093023255813, "no_speech_prob": 0.02262832410633564}, {"id": 294, "seek": 152878, "start": 1528.78, "end": 1533.74, "text": " And this is very helpful because it means you actually never have to calculate this.", "tokens": [50364, 400, 341, 307, 588, 4961, 570, 309, 1355, 291, 767, 1128, 362, 281, 8873, 341, 13, 50612], "temperature": 0.0, "avg_logprob": -0.23891017506423506, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.0736929252743721}, {"id": 295, "seek": 152878, "start": 1533.74, "end": 1536.3799999999999, "text": " You can just calculate the mean.", "tokens": [50612, 509, 393, 445, 8873, 264, 914, 13, 50744], "temperature": 0.0, "avg_logprob": -0.23891017506423506, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.0736929252743721}, {"id": 296, "seek": 152878, "start": 1536.3799999999999, "end": 1539.42, "text": " So with just the data points on their own, you can actually calculate the variance.", "tokens": [50744, 407, 365, 445, 264, 1412, 2793, 322, 641, 1065, 11, 291, 393, 767, 8873, 264, 21977, 13, 50896], "temperature": 0.0, "avg_logprob": -0.23891017506423506, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.0736929252743721}, {"id": 297, "seek": 152878, "start": 1539.42, "end": 1541.62, "text": " This is a really nice shortcut.", "tokens": [50896, 639, 307, 257, 534, 1481, 24822, 13, 51006], "temperature": 0.0, "avg_logprob": -0.23891017506423506, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.0736929252743721}, {"id": 298, "seek": 152878, "start": 1541.62, "end": 1545.5, "text": " This is how we normally calculate variance.", "tokens": [51006, 639, 307, 577, 321, 5646, 8873, 21977, 13, 51200], "temperature": 0.0, "avg_logprob": -0.23891017506423506, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.0736929252743721}, {"id": 299, "seek": 152878, "start": 1545.5, "end": 1550.22, "text": " And so there is the LaTeX version, which of course I didn't write myself.", "tokens": [51200, 400, 370, 456, 307, 264, 2369, 14233, 55, 3037, 11, 597, 295, 1164, 286, 994, 380, 2464, 2059, 13, 51436], "temperature": 0.0, "avg_logprob": -0.23891017506423506, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.0736929252743721}, {"id": 300, "seek": 152878, "start": 1550.22, "end": 1556.18, "text": " I stole from the Wikipedia LaTeX because I'm lazy.", "tokens": [51436, 286, 16326, 490, 264, 28999, 2369, 14233, 55, 570, 286, 478, 14847, 13, 51734], "temperature": 0.0, "avg_logprob": -0.23891017506423506, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.0736929252743721}, {"id": 301, "seek": 155618, "start": 1556.18, "end": 1563.3400000000001, "text": " Now there's a very, very similar idea, which is covariance.", "tokens": [50364, 823, 456, 311, 257, 588, 11, 588, 2531, 1558, 11, 597, 307, 49851, 719, 13, 50722], "temperature": 0.0, "avg_logprob": -0.31904760613498917, "compression_ratio": 1.5181347150259068, "no_speech_prob": 0.0006878016865812242}, {"id": 302, "seek": 155618, "start": 1563.3400000000001, "end": 1567.6200000000001, "text": " And it's already come up a little bit in the first lesson or two.", "tokens": [50722, 400, 309, 311, 1217, 808, 493, 257, 707, 857, 294, 264, 700, 6898, 420, 732, 13, 50936], "temperature": 0.0, "avg_logprob": -0.31904760613498917, "compression_ratio": 1.5181347150259068, "no_speech_prob": 0.0006878016865812242}, {"id": 303, "seek": 155618, "start": 1567.6200000000001, "end": 1575.26, "text": " And particularly the extra math lesson that Basim and Dinesh did.", "tokens": [50936, 400, 4098, 264, 2857, 5221, 6898, 300, 5859, 332, 293, 413, 1652, 71, 630, 13, 51318], "temperature": 0.0, "avg_logprob": -0.31904760613498917, "compression_ratio": 1.5181347150259068, "no_speech_prob": 0.0006878016865812242}, {"id": 304, "seek": 155618, "start": 1575.26, "end": 1583.74, "text": " And it's, yeah, so covariance tells you how much two things vary, not just on their own", "tokens": [51318, 400, 309, 311, 11, 1338, 11, 370, 49851, 719, 5112, 291, 577, 709, 732, 721, 10559, 11, 406, 445, 322, 641, 1065, 51742], "temperature": 0.0, "avg_logprob": -0.31904760613498917, "compression_ratio": 1.5181347150259068, "no_speech_prob": 0.0006878016865812242}, {"id": 305, "seek": 155618, "start": 1583.74, "end": 1586.1000000000001, "text": " but together.", "tokens": [51742, 457, 1214, 13, 51860], "temperature": 0.0, "avg_logprob": -0.31904760613498917, "compression_ratio": 1.5181347150259068, "no_speech_prob": 0.0006878016865812242}, {"id": 306, "seek": 158610, "start": 1587.02, "end": 1589.62, "text": " And there's a definition here in math, but I like code so we'll see the code.", "tokens": [50410, 400, 456, 311, 257, 7123, 510, 294, 5221, 11, 457, 286, 411, 3089, 370, 321, 603, 536, 264, 3089, 13, 50540], "temperature": 0.0, "avg_logprob": -0.24183264145484337, "compression_ratio": 1.6306306306306306, "no_speech_prob": 5.0644655857468024e-05}, {"id": 307, "seek": 158610, "start": 1589.62, "end": 1593.34, "text": " So here's our tensor again.", "tokens": [50540, 407, 510, 311, 527, 40863, 797, 13, 50726], "temperature": 0.0, "avg_logprob": -0.24183264145484337, "compression_ratio": 1.6306306306306306, "no_speech_prob": 5.0644655857468024e-05}, {"id": 308, "seek": 158610, "start": 1593.34, "end": 1595.1399999999999, "text": " Now we're going to want to have two things.", "tokens": [50726, 823, 321, 434, 516, 281, 528, 281, 362, 732, 721, 13, 50816], "temperature": 0.0, "avg_logprob": -0.24183264145484337, "compression_ratio": 1.6306306306306306, "no_speech_prob": 5.0644655857468024e-05}, {"id": 309, "seek": 158610, "start": 1595.1399999999999, "end": 1600.8999999999999, "text": " So let's create something called u, which is just two times our tensor with a bit of", "tokens": [50816, 407, 718, 311, 1884, 746, 1219, 344, 11, 597, 307, 445, 732, 1413, 527, 40863, 365, 257, 857, 295, 51104], "temperature": 0.0, "avg_logprob": -0.24183264145484337, "compression_ratio": 1.6306306306306306, "no_speech_prob": 5.0644655857468024e-05}, {"id": 310, "seek": 158610, "start": 1600.8999999999999, "end": 1603.1799999999998, "text": " randomness.", "tokens": [51104, 4974, 1287, 13, 51218], "temperature": 0.0, "avg_logprob": -0.24183264145484337, "compression_ratio": 1.6306306306306306, "no_speech_prob": 5.0644655857468024e-05}, {"id": 311, "seek": 158610, "start": 1603.1799999999998, "end": 1604.1799999999998, "text": " So here it is.", "tokens": [51218, 407, 510, 309, 307, 13, 51268], "temperature": 0.0, "avg_logprob": -0.24183264145484337, "compression_ratio": 1.6306306306306306, "no_speech_prob": 5.0644655857468024e-05}, {"id": 312, "seek": 158610, "start": 1604.1799999999998, "end": 1612.1799999999998, "text": " Now you can see that u and t are very closely correlated here.", "tokens": [51268, 823, 291, 393, 536, 300, 344, 293, 256, 366, 588, 8185, 38574, 510, 13, 51668], "temperature": 0.0, "avg_logprob": -0.24183264145484337, "compression_ratio": 1.6306306306306306, "no_speech_prob": 5.0644655857468024e-05}, {"id": 313, "seek": 158610, "start": 1612.1799999999998, "end": 1615.3, "text": " But they're not perfectly correlated.", "tokens": [51668, 583, 436, 434, 406, 6239, 38574, 13, 51824], "temperature": 0.0, "avg_logprob": -0.24183264145484337, "compression_ratio": 1.6306306306306306, "no_speech_prob": 5.0644655857468024e-05}, {"id": 314, "seek": 161530, "start": 1615.5, "end": 1621.06, "text": " So the covariance tells us, yeah, how they vary together and separately.", "tokens": [50374, 407, 264, 49851, 719, 5112, 505, 11, 1338, 11, 577, 436, 10559, 1214, 293, 14759, 13, 50652], "temperature": 0.0, "avg_logprob": -0.2654622995628501, "compression_ratio": 1.7112068965517242, "no_speech_prob": 6.24093036094564e-06}, {"id": 315, "seek": 161530, "start": 1621.06, "end": 1627.02, "text": " So we can take the, you can see this is exactly the same thing we had before.", "tokens": [50652, 407, 321, 393, 747, 264, 11, 291, 393, 536, 341, 307, 2293, 264, 912, 551, 321, 632, 949, 13, 50950], "temperature": 0.0, "avg_logprob": -0.2654622995628501, "compression_ratio": 1.7112068965517242, "no_speech_prob": 6.24093036094564e-06}, {"id": 316, "seek": 161530, "start": 1627.02, "end": 1630.62, "text": " Each data point minus its mean.", "tokens": [50950, 6947, 1412, 935, 3175, 1080, 914, 13, 51130], "temperature": 0.0, "avg_logprob": -0.2654622995628501, "compression_ratio": 1.7112068965517242, "no_speech_prob": 6.24093036094564e-06}, {"id": 317, "seek": 161530, "start": 1630.62, "end": 1632.46, "text": " But now we've got two different tensors.", "tokens": [51130, 583, 586, 321, 600, 658, 732, 819, 10688, 830, 13, 51222], "temperature": 0.0, "avg_logprob": -0.2654622995628501, "compression_ratio": 1.7112068965517242, "no_speech_prob": 6.24093036094564e-06}, {"id": 318, "seek": 161530, "start": 1632.46, "end": 1636.98, "text": " So we're also going to do it for the other one, the other data points minus their mean.", "tokens": [51222, 407, 321, 434, 611, 516, 281, 360, 309, 337, 264, 661, 472, 11, 264, 661, 1412, 2793, 3175, 641, 914, 13, 51448], "temperature": 0.0, "avg_logprob": -0.2654622995628501, "compression_ratio": 1.7112068965517242, "no_speech_prob": 6.24093036094564e-06}, {"id": 319, "seek": 161530, "start": 1636.98, "end": 1638.5, "text": " And we multiply them together.", "tokens": [51448, 400, 321, 12972, 552, 1214, 13, 51524], "temperature": 0.0, "avg_logprob": -0.2654622995628501, "compression_ratio": 1.7112068965517242, "no_speech_prob": 6.24093036094564e-06}, {"id": 320, "seek": 161530, "start": 1638.5, "end": 1641.8999999999999, "text": " So it's actually the same thing as standard deviation.", "tokens": [51524, 407, 309, 311, 767, 264, 912, 551, 382, 3832, 25163, 13, 51694], "temperature": 0.0, "avg_logprob": -0.2654622995628501, "compression_ratio": 1.7112068965517242, "no_speech_prob": 6.24093036094564e-06}, {"id": 321, "seek": 164190, "start": 1641.94, "end": 1651.14, "text": " But in standard deviation, it's kind of like the covariance with itself in a sense, right?", "tokens": [50366, 583, 294, 3832, 25163, 11, 309, 311, 733, 295, 411, 264, 49851, 719, 365, 2564, 294, 257, 2020, 11, 558, 30, 50826], "temperature": 0.0, "avg_logprob": -0.23020496087915757, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.00019716907991096377}, {"id": 322, "seek": 164190, "start": 1651.14, "end": 1654.5, "text": " And so that's a product we can calculate.", "tokens": [50826, 400, 370, 300, 311, 257, 1674, 321, 393, 8873, 13, 50994], "temperature": 0.0, "avg_logprob": -0.23020496087915757, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.00019716907991096377}, {"id": 323, "seek": 164190, "start": 1654.5, "end": 1664.0600000000002, "text": " And then what we then do is we take the mean of that.", "tokens": [50994, 400, 550, 437, 321, 550, 360, 307, 321, 747, 264, 914, 295, 300, 13, 51472], "temperature": 0.0, "avg_logprob": -0.23020496087915757, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.00019716907991096377}, {"id": 324, "seek": 164190, "start": 1664.0600000000002, "end": 1670.7, "text": " And that gives us the covariance between those two tensors.", "tokens": [51472, 400, 300, 2709, 505, 264, 49851, 719, 1296, 729, 732, 10688, 830, 13, 51804], "temperature": 0.0, "avg_logprob": -0.23020496087915757, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.00019716907991096377}, {"id": 325, "seek": 167070, "start": 1670.7, "end": 1672.98, "text": " And you can see that's quite a high number.", "tokens": [50364, 400, 291, 393, 536, 300, 311, 1596, 257, 1090, 1230, 13, 50478], "temperature": 0.0, "avg_logprob": -0.2545112103832011, "compression_ratio": 1.6818181818181819, "no_speech_prob": 7.96729073044844e-05}, {"id": 326, "seek": 167070, "start": 1672.98, "end": 1677.94, "text": " And if we compare it to two things that aren't very related at all, so let's create a totally", "tokens": [50478, 400, 498, 321, 6794, 309, 281, 732, 721, 300, 3212, 380, 588, 4077, 412, 439, 11, 370, 718, 311, 1884, 257, 3879, 50726], "temperature": 0.0, "avg_logprob": -0.2545112103832011, "compression_ratio": 1.6818181818181819, "no_speech_prob": 7.96729073044844e-05}, {"id": 327, "seek": 167070, "start": 1677.94, "end": 1681.82, "text": " random tensor, v.", "tokens": [50726, 4974, 40863, 11, 371, 13, 50920], "temperature": 0.0, "avg_logprob": -0.2545112103832011, "compression_ratio": 1.6818181818181819, "no_speech_prob": 7.96729073044844e-05}, {"id": 328, "seek": 167070, "start": 1681.82, "end": 1683.02, "text": " So this is not related to t.", "tokens": [50920, 407, 341, 307, 406, 4077, 281, 256, 13, 50980], "temperature": 0.0, "avg_logprob": -0.2545112103832011, "compression_ratio": 1.6818181818181819, "no_speech_prob": 7.96729073044844e-05}, {"id": 329, "seek": 167070, "start": 1683.02, "end": 1688.94, "text": " And we do exactly the same thing.", "tokens": [50980, 400, 321, 360, 2293, 264, 912, 551, 13, 51276], "temperature": 0.0, "avg_logprob": -0.2545112103832011, "compression_ratio": 1.6818181818181819, "no_speech_prob": 7.96729073044844e-05}, {"id": 330, "seek": 167070, "start": 1688.94, "end": 1693.78, "text": " So take the difference of t to its means and v to its means and take the mean of that.", "tokens": [51276, 407, 747, 264, 2649, 295, 256, 281, 1080, 1355, 293, 371, 281, 1080, 1355, 293, 747, 264, 914, 295, 300, 13, 51518], "temperature": 0.0, "avg_logprob": -0.2545112103832011, "compression_ratio": 1.6818181818181819, "no_speech_prob": 7.96729073044844e-05}, {"id": 331, "seek": 167070, "start": 1693.78, "end": 1695.6200000000001, "text": " That's a very small number.", "tokens": [51518, 663, 311, 257, 588, 1359, 1230, 13, 51610], "temperature": 0.0, "avg_logprob": -0.2545112103832011, "compression_ratio": 1.6818181818181819, "no_speech_prob": 7.96729073044844e-05}, {"id": 332, "seek": 169562, "start": 1695.6599999999999, "end": 1705.02, "text": " And so you can see covariance is basically telling us how related are these two tensors.", "tokens": [50366, 400, 370, 291, 393, 536, 49851, 719, 307, 1936, 3585, 505, 577, 4077, 366, 613, 732, 10688, 830, 13, 50834], "temperature": 0.0, "avg_logprob": -0.2807440984816778, "compression_ratio": 1.796875, "no_speech_prob": 0.00047285176697187126}, {"id": 333, "seek": 169562, "start": 1705.02, "end": 1707.62, "text": " So covariance and variance are basically the same thing.", "tokens": [50834, 407, 49851, 719, 293, 21977, 366, 1936, 264, 912, 551, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2807440984816778, "compression_ratio": 1.796875, "no_speech_prob": 0.00047285176697187126}, {"id": 334, "seek": 169562, "start": 1707.62, "end": 1714.9799999999998, "text": " But you can think of variance as being covariance with itself.", "tokens": [50964, 583, 291, 393, 519, 295, 21977, 382, 885, 49851, 719, 365, 2564, 13, 51332], "temperature": 0.0, "avg_logprob": -0.2807440984816778, "compression_ratio": 1.796875, "no_speech_prob": 0.00047285176697187126}, {"id": 335, "seek": 169562, "start": 1714.9799999999998, "end": 1719.8999999999999, "text": " And you can change this mathematical version, which is the one we just created in code,", "tokens": [51332, 400, 291, 393, 1319, 341, 18894, 3037, 11, 597, 307, 264, 472, 321, 445, 2942, 294, 3089, 11, 51578], "temperature": 0.0, "avg_logprob": -0.2807440984816778, "compression_ratio": 1.796875, "no_speech_prob": 0.00047285176697187126}, {"id": 336, "seek": 169562, "start": 1719.8999999999999, "end": 1722.78, "text": " to this version, just like we have for variance.", "tokens": [51578, 281, 341, 3037, 11, 445, 411, 321, 362, 337, 21977, 13, 51722], "temperature": 0.0, "avg_logprob": -0.2807440984816778, "compression_ratio": 1.796875, "no_speech_prob": 0.00047285176697187126}, {"id": 337, "seek": 172278, "start": 1722.78, "end": 1733.82, "text": " It's the easier to calculate version, which as you can see, gives exactly the same answer.", "tokens": [50364, 467, 311, 264, 3571, 281, 8873, 3037, 11, 597, 382, 291, 393, 536, 11, 2709, 2293, 264, 912, 1867, 13, 50916], "temperature": 0.0, "avg_logprob": -0.36813845399950373, "compression_ratio": 1.4682080924855492, "no_speech_prob": 0.0003799831902142614}, {"id": 338, "seek": 172278, "start": 1733.82, "end": 1741.58, "text": " Okay, so if you haven't done stuff with covariance much before, you should experiment a bit with", "tokens": [50916, 1033, 11, 370, 498, 291, 2378, 380, 1096, 1507, 365, 49851, 719, 709, 949, 11, 291, 820, 5120, 257, 857, 365, 51304], "temperature": 0.0, "avg_logprob": -0.36813845399950373, "compression_ratio": 1.4682080924855492, "no_speech_prob": 0.0003799831902142614}, {"id": 339, "seek": 172278, "start": 1741.58, "end": 1751.18, "text": " it by creating a few different plots and experimenting with those.", "tokens": [51304, 309, 538, 4084, 257, 1326, 819, 28609, 293, 29070, 365, 729, 13, 51784], "temperature": 0.0, "avg_logprob": -0.36813845399950373, "compression_ratio": 1.4682080924855492, "no_speech_prob": 0.0003799831902142614}, {"id": 340, "seek": 175118, "start": 1751.18, "end": 1758.9, "text": " And finally, the Pearson correlation coefficient, which is normally called R or rho, is just", "tokens": [50364, 400, 2721, 11, 264, 39041, 20009, 17619, 11, 597, 307, 5646, 1219, 497, 420, 20293, 11, 307, 445, 50750], "temperature": 0.0, "avg_logprob": -0.34402508889475175, "compression_ratio": 1.4488636363636365, "no_speech_prob": 0.0012065867194905877}, {"id": 341, "seek": 175118, "start": 1758.9, "end": 1764.8600000000001, "text": " the covariance divided by the product of the standard deviations.", "tokens": [50750, 264, 49851, 719, 6666, 538, 264, 1674, 295, 264, 3832, 31219, 763, 13, 51048], "temperature": 0.0, "avg_logprob": -0.34402508889475175, "compression_ratio": 1.4488636363636365, "no_speech_prob": 0.0012065867194905877}, {"id": 342, "seek": 175118, "start": 1764.8600000000001, "end": 1767.98, "text": " So you've probably seen that number many times.", "tokens": [51048, 407, 291, 600, 1391, 1612, 300, 1230, 867, 1413, 13, 51204], "temperature": 0.0, "avg_logprob": -0.34402508889475175, "compression_ratio": 1.4488636363636365, "no_speech_prob": 0.0012065867194905877}, {"id": 343, "seek": 175118, "start": 1767.98, "end": 1773.42, "text": " There's just a scaled version of the same thing.", "tokens": [51204, 821, 311, 445, 257, 36039, 3037, 295, 264, 912, 551, 13, 51476], "temperature": 0.0, "avg_logprob": -0.34402508889475175, "compression_ratio": 1.4488636363636365, "no_speech_prob": 0.0012065867194905877}, {"id": 344, "seek": 177342, "start": 1773.46, "end": 1790.02, "text": " Okay, so with that in mind, here is how Xavier in it or Gloro in it is derived.", "tokens": [50366, 1033, 11, 370, 365, 300, 294, 1575, 11, 510, 307, 577, 44653, 294, 309, 420, 5209, 10780, 294, 309, 307, 18949, 13, 51194], "temperature": 0.0, "avg_logprob": -0.42198545282537286, "compression_ratio": 1.2136752136752136, "no_speech_prob": 0.0003150379052385688}, {"id": 345, "seek": 177342, "start": 1790.02, "end": 1800.1000000000001, "text": " So when you do a matrix multiplication, for each of the y i's,", "tokens": [51194, 407, 562, 291, 360, 257, 8141, 27290, 11, 337, 1184, 295, 264, 288, 741, 311, 11, 51698], "temperature": 0.0, "avg_logprob": -0.42198545282537286, "compression_ratio": 1.2136752136752136, "no_speech_prob": 0.0003150379052385688}, {"id": 346, "seek": 180010, "start": 1800.1399999999999, "end": 1804.6999999999998, "text": " we're adding together all of these products.", "tokens": [50366, 321, 434, 5127, 1214, 439, 295, 613, 3383, 13, 50594], "temperature": 0.0, "avg_logprob": -0.25865515795621, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00014883837138768286}, {"id": 347, "seek": 180010, "start": 1804.6999999999998, "end": 1814.5, "text": " So we've got a i, 0 times x 0 plus a i, 1 times x 1, etc.", "tokens": [50594, 407, 321, 600, 658, 257, 741, 11, 1958, 1413, 2031, 1958, 1804, 257, 741, 11, 502, 1413, 2031, 502, 11, 5183, 13, 51084], "temperature": 0.0, "avg_logprob": -0.25865515795621, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00014883837138768286}, {"id": 348, "seek": 180010, "start": 1814.5, "end": 1816.4199999999998, "text": " And we can write that in sigma notation.", "tokens": [51084, 400, 321, 393, 2464, 300, 294, 12771, 24657, 13, 51180], "temperature": 0.0, "avg_logprob": -0.25865515795621, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00014883837138768286}, {"id": 349, "seek": 180010, "start": 1816.4199999999998, "end": 1823.1399999999999, "text": " So we're adding up together all of the a i k's with all of the x k's.", "tokens": [51180, 407, 321, 434, 5127, 493, 1214, 439, 295, 264, 257, 741, 350, 311, 365, 439, 295, 264, 2031, 350, 311, 13, 51516], "temperature": 0.0, "avg_logprob": -0.25865515795621, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00014883837138768286}, {"id": 350, "seek": 180010, "start": 1823.1399999999999, "end": 1827.4199999999998, "text": " This is the stuff that we did in our first lesson of part two.", "tokens": [51516, 639, 307, 264, 1507, 300, 321, 630, 294, 527, 700, 6898, 295, 644, 732, 13, 51730], "temperature": 0.0, "avg_logprob": -0.25865515795621, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00014883837138768286}, {"id": 351, "seek": 182742, "start": 1827.42, "end": 1834.5800000000002, "text": " And so here it is in pure Python code, and here it is in NumPy code.", "tokens": [50364, 400, 370, 510, 309, 307, 294, 6075, 15329, 3089, 11, 293, 510, 309, 307, 294, 22592, 47, 88, 3089, 13, 50722], "temperature": 0.0, "avg_logprob": -0.3060018639815481, "compression_ratio": 1.7803468208092486, "no_speech_prob": 0.0019267314346507192}, {"id": 352, "seek": 182742, "start": 1834.5800000000002, "end": 1838.38, "text": " Now at the very beginning, our vector has a mean of about 0 and a standard deviation", "tokens": [50722, 823, 412, 264, 588, 2863, 11, 527, 8062, 575, 257, 914, 295, 466, 1958, 293, 257, 3832, 25163, 50912], "temperature": 0.0, "avg_logprob": -0.3060018639815481, "compression_ratio": 1.7803468208092486, "no_speech_prob": 0.0019267314346507192}, {"id": 353, "seek": 182742, "start": 1838.38, "end": 1844.8600000000001, "text": " of about 1, because that's what we asked for, to remind you.", "tokens": [50912, 295, 466, 502, 11, 570, 300, 311, 437, 321, 2351, 337, 11, 281, 4160, 291, 13, 51236], "temperature": 0.0, "avg_logprob": -0.3060018639815481, "compression_ratio": 1.7803468208092486, "no_speech_prob": 0.0019267314346507192}, {"id": 354, "seek": 182742, "start": 1844.8600000000001, "end": 1846.38, "text": " That's what we asked for.", "tokens": [51236, 663, 311, 437, 321, 2351, 337, 13, 51312], "temperature": 0.0, "avg_logprob": -0.3060018639815481, "compression_ratio": 1.7803468208092486, "no_speech_prob": 0.0019267314346507192}, {"id": 355, "seek": 182742, "start": 1846.38, "end": 1851.78, "text": " That's a standard deviation of 1, mean of 0.", "tokens": [51312, 663, 311, 257, 3832, 25163, 295, 502, 11, 914, 295, 1958, 13, 51582], "temperature": 0.0, "avg_logprob": -0.3060018639815481, "compression_ratio": 1.7803468208092486, "no_speech_prob": 0.0019267314346507192}, {"id": 356, "seek": 182742, "start": 1851.78, "end": 1855.8200000000002, "text": " That's what rand n is.", "tokens": [51582, 663, 311, 437, 367, 474, 297, 307, 13, 51784], "temperature": 0.0, "avg_logprob": -0.3060018639815481, "compression_ratio": 1.7803468208092486, "no_speech_prob": 0.0019267314346507192}, {"id": 357, "seek": 185582, "start": 1855.82, "end": 1860.46, "text": " So let's create some random numbers and we can confirm, yeah,", "tokens": [50364, 407, 718, 311, 1884, 512, 4974, 3547, 293, 321, 393, 9064, 11, 1338, 11, 50596], "temperature": 0.0, "avg_logprob": -0.42216388157435825, "compression_ratio": 1.6533333333333333, "no_speech_prob": 6.814862717874348e-05}, {"id": 358, "seek": 185582, "start": 1860.46, "end": 1863.98, "text": " they have a mean of about 0 and a standard deviation of about 1.", "tokens": [50596, 436, 362, 257, 914, 295, 466, 1958, 293, 257, 3832, 25163, 295, 466, 502, 13, 50772], "temperature": 0.0, "avg_logprob": -0.42216388157435825, "compression_ratio": 1.6533333333333333, "no_speech_prob": 6.814862717874348e-05}, {"id": 359, "seek": 185582, "start": 1868.1, "end": 1873.3799999999999, "text": " So if we chose weights for a that have a mean of 0,", "tokens": [50978, 407, 498, 321, 5111, 17443, 337, 257, 300, 362, 257, 914, 295, 1958, 11, 51242], "temperature": 0.0, "avg_logprob": -0.42216388157435825, "compression_ratio": 1.6533333333333333, "no_speech_prob": 6.814862717874348e-05}, {"id": 360, "seek": 185582, "start": 1873.3799999999999, "end": 1877.1399999999999, "text": " we can compute the standard deviation quite easily.", "tokens": [51242, 321, 393, 14722, 264, 3832, 25163, 1596, 3612, 13, 51430], "temperature": 0.0, "avg_logprob": -0.42216388157435825, "compression_ratio": 1.6533333333333333, "no_speech_prob": 6.814862717874348e-05}, {"id": 361, "seek": 185582, "start": 1882.3799999999999, "end": 1884.3799999999999, "text": " So let's do that.", "tokens": [51692, 407, 718, 311, 360, 300, 13, 51792], "temperature": 0.0, "avg_logprob": -0.42216388157435825, "compression_ratio": 1.6533333333333333, "no_speech_prob": 6.814862717874348e-05}, {"id": 362, "seek": 188438, "start": 1884.38, "end": 1888.98, "text": " So 100 times, let's try creating our x, and", "tokens": [50364, 407, 2319, 1413, 11, 718, 311, 853, 4084, 527, 2031, 11, 293, 50594], "temperature": 0.0, "avg_logprob": -0.5312151128595526, "compression_ratio": 1.4566929133858268, "no_speech_prob": 1.8448231458023656e-06}, {"id": 363, "seek": 188438, "start": 1888.98, "end": 1894.3400000000001, "text": " let's try creating something to multiply it by.", "tokens": [50594, 718, 311, 853, 4084, 746, 281, 12972, 309, 538, 13, 50862], "temperature": 0.0, "avg_logprob": -0.5312151128595526, "compression_ratio": 1.4566929133858268, "no_speech_prob": 1.8448231458023656e-06}, {"id": 364, "seek": 188438, "start": 1894.3400000000001, "end": 1895.94, "text": " And we'll do the matrix multiplication.", "tokens": [50862, 400, 321, 603, 360, 264, 8141, 27290, 13, 50942], "temperature": 0.0, "avg_logprob": -0.5312151128595526, "compression_ratio": 1.4566929133858268, "no_speech_prob": 1.8448231458023656e-06}, {"id": 365, "seek": 188438, "start": 1897.6200000000001, "end": 1903.66, "text": " And we're gonna get the mean and", "tokens": [51026, 400, 321, 434, 799, 483, 264, 914, 293, 51328], "temperature": 0.0, "avg_logprob": -0.5312151128595526, "compression_ratio": 1.4566929133858268, "no_speech_prob": 1.8448231458023656e-06}, {"id": 366, "seek": 188438, "start": 1903.66, "end": 1907.3400000000001, "text": " mean of the squares.", "tokens": [51328, 914, 295, 264, 19368, 13, 51512], "temperature": 0.0, "avg_logprob": -0.5312151128595526, "compression_ratio": 1.4566929133858268, "no_speech_prob": 1.8448231458023656e-06}, {"id": 367, "seek": 190734, "start": 1908.34, "end": 1916.34, "text": " And so that is very close to our matrix.", "tokens": [50414, 400, 370, 300, 307, 588, 1998, 281, 527, 8141, 13, 50814], "temperature": 0.0, "avg_logprob": -0.43950877651091547, "compression_ratio": 1.3782051282051282, "no_speech_prob": 7.64653395890491e-06}, {"id": 368, "seek": 190734, "start": 1923.62, "end": 1927.02, "text": " So I won't go into, I mean, you can look at it if you like, but", "tokens": [51178, 407, 286, 1582, 380, 352, 666, 11, 286, 914, 11, 291, 393, 574, 412, 309, 498, 291, 411, 11, 457, 51348], "temperature": 0.0, "avg_logprob": -0.43950877651091547, "compression_ratio": 1.3782051282051282, "no_speech_prob": 7.64653395890491e-06}, {"id": 369, "seek": 190734, "start": 1927.02, "end": 1930.54, "text": " basically as long as the elements in a and x are independent,", "tokens": [51348, 1936, 382, 938, 382, 264, 4959, 294, 257, 293, 2031, 366, 6695, 11, 51524], "temperature": 0.0, "avg_logprob": -0.43950877651091547, "compression_ratio": 1.3782051282051282, "no_speech_prob": 7.64653395890491e-06}, {"id": 370, "seek": 190734, "start": 1930.54, "end": 1933.58, "text": " which obviously they are because they're random.", "tokens": [51524, 597, 2745, 436, 366, 570, 436, 434, 4974, 13, 51676], "temperature": 0.0, "avg_logprob": -0.43950877651091547, "compression_ratio": 1.3782051282051282, "no_speech_prob": 7.64653395890491e-06}, {"id": 371, "seek": 193358, "start": 1933.8999999999999, "end": 1938.3799999999999, "text": " Then we're gonna end up with a mean of 0 and", "tokens": [50380, 1396, 321, 434, 799, 917, 493, 365, 257, 914, 295, 1958, 293, 50604], "temperature": 0.0, "avg_logprob": -0.33783697527508405, "compression_ratio": 1.5846153846153845, "no_speech_prob": 1.844823373176041e-06}, {"id": 372, "seek": 193358, "start": 1938.3799999999999, "end": 1941.6999999999998, "text": " a standard deviation of 1 for these products.", "tokens": [50604, 257, 3832, 25163, 295, 502, 337, 613, 3383, 13, 50770], "temperature": 0.0, "avg_logprob": -0.33783697527508405, "compression_ratio": 1.5846153846153845, "no_speech_prob": 1.844823373176041e-06}, {"id": 373, "seek": 193358, "start": 1943.4199999999998, "end": 1949.6599999999999, "text": " And so we can try it if we create a random number,", "tokens": [50856, 400, 370, 321, 393, 853, 309, 498, 321, 1884, 257, 4974, 1230, 11, 51168], "temperature": 0.0, "avg_logprob": -0.33783697527508405, "compression_ratio": 1.5846153846153845, "no_speech_prob": 1.844823373176041e-06}, {"id": 374, "seek": 193358, "start": 1949.6599999999999, "end": 1953.1, "text": " a normally distributed random number, and then a second random number.", "tokens": [51168, 257, 5646, 12631, 4974, 1230, 11, 293, 550, 257, 1150, 4974, 1230, 13, 51340], "temperature": 0.0, "avg_logprob": -0.33783697527508405, "compression_ratio": 1.5846153846153845, "no_speech_prob": 1.844823373176041e-06}, {"id": 375, "seek": 193358, "start": 1953.1, "end": 1957.1399999999999, "text": " Multiply them together and then do it a bunch of times.", "tokens": [51340, 31150, 356, 552, 1214, 293, 550, 360, 309, 257, 3840, 295, 1413, 13, 51542], "temperature": 0.0, "avg_logprob": -0.33783697527508405, "compression_ratio": 1.5846153846153845, "no_speech_prob": 1.844823373176041e-06}, {"id": 376, "seek": 193358, "start": 1957.1399999999999, "end": 1959.6999999999998, "text": " And you can see here we've got our 0, 1.", "tokens": [51542, 400, 291, 393, 536, 510, 321, 600, 658, 527, 1958, 11, 502, 13, 51670], "temperature": 0.0, "avg_logprob": -0.33783697527508405, "compression_ratio": 1.5846153846153845, "no_speech_prob": 1.844823373176041e-06}, {"id": 377, "seek": 196358, "start": 1964.58, "end": 1970.54, "text": " So that's the reason why we need this math.square root 100.", "tokens": [50414, 407, 300, 311, 264, 1778, 983, 321, 643, 341, 5221, 13, 33292, 543, 5593, 2319, 13, 50712], "temperature": 0.0, "avg_logprob": -0.3465061397342892, "compression_ratio": 1.5150214592274678, "no_speech_prob": 6.854314960946795e-06}, {"id": 378, "seek": 196358, "start": 1970.54, "end": 1975.62, "text": " We don't normally worry about the mathematical reasons why things are", "tokens": [50712, 492, 500, 380, 5646, 3292, 466, 264, 18894, 4112, 983, 721, 366, 50966], "temperature": 0.0, "avg_logprob": -0.3465061397342892, "compression_ratio": 1.5150214592274678, "no_speech_prob": 6.854314960946795e-06}, {"id": 379, "seek": 196358, "start": 1975.62, "end": 1979.5, "text": " exactly, but yeah, I thought I would just dive into this one,", "tokens": [50966, 2293, 11, 457, 1338, 11, 286, 1194, 286, 576, 445, 9192, 666, 341, 472, 11, 51160], "temperature": 0.0, "avg_logprob": -0.3465061397342892, "compression_ratio": 1.5150214592274678, "no_speech_prob": 6.854314960946795e-06}, {"id": 380, "seek": 196358, "start": 1979.5, "end": 1982.3799999999999, "text": " cuz sometimes it's fun to go through it.", "tokens": [51160, 11910, 2171, 309, 311, 1019, 281, 352, 807, 309, 13, 51304], "temperature": 0.0, "avg_logprob": -0.3465061397342892, "compression_ratio": 1.5150214592274678, "no_speech_prob": 6.854314960946795e-06}, {"id": 381, "seek": 196358, "start": 1982.3799999999999, "end": 1985.4199999999998, "text": " And so you can check out the paper if you wanna look at that in more detail or", "tokens": [51304, 400, 370, 291, 393, 1520, 484, 264, 3035, 498, 291, 1948, 574, 412, 300, 294, 544, 2607, 420, 51456], "temperature": 0.0, "avg_logprob": -0.3465061397342892, "compression_ratio": 1.5150214592274678, "no_speech_prob": 6.854314960946795e-06}, {"id": 382, "seek": 196358, "start": 1985.4199999999998, "end": 1989.5, "text": " experiment with these little simulations.", "tokens": [51456, 5120, 365, 613, 707, 35138, 13, 51660], "temperature": 0.0, "avg_logprob": -0.3465061397342892, "compression_ratio": 1.5150214592274678, "no_speech_prob": 6.854314960946795e-06}, {"id": 383, "seek": 198950, "start": 1990.5, "end": 1996.06, "text": " Now the problem is that that doesn't work.", "tokens": [50414, 823, 264, 1154, 307, 300, 300, 1177, 380, 589, 13, 50692], "temperature": 0.0, "avg_logprob": -0.4537743977137974, "compression_ratio": 1.406060606060606, "no_speech_prob": 1.6280514500977006e-06}, {"id": 384, "seek": 198950, "start": 1996.06, "end": 2003.14, "text": " It doesn't work for us because we use rectified linear units,", "tokens": [50692, 467, 1177, 380, 589, 337, 505, 570, 321, 764, 11048, 2587, 8213, 6815, 11, 51046], "temperature": 0.0, "avg_logprob": -0.4537743977137974, "compression_ratio": 1.406060606060606, "no_speech_prob": 1.6280514500977006e-06}, {"id": 385, "seek": 198950, "start": 2003.14, "end": 2010.1, "text": " which is not something that Xavier Gloreau looked at.", "tokens": [51046, 597, 307, 406, 746, 300, 44653, 5209, 418, 1459, 2956, 412, 13, 51394], "temperature": 0.0, "avg_logprob": -0.4537743977137974, "compression_ratio": 1.406060606060606, "no_speech_prob": 1.6280514500977006e-06}, {"id": 386, "seek": 198950, "start": 2010.1, "end": 2012.38, "text": " Let's take a look.", "tokens": [51394, 961, 311, 747, 257, 574, 13, 51508], "temperature": 0.0, "avg_logprob": -0.4537743977137974, "compression_ratio": 1.406060606060606, "no_speech_prob": 1.6280514500977006e-06}, {"id": 387, "seek": 198950, "start": 2012.38, "end": 2014.86, "text": " Let's create a couple of matrices.", "tokens": [51508, 961, 311, 1884, 257, 1916, 295, 32284, 13, 51632], "temperature": 0.0, "avg_logprob": -0.4537743977137974, "compression_ratio": 1.406060606060606, "no_speech_prob": 1.6280514500977006e-06}, {"id": 388, "seek": 198950, "start": 2014.86, "end": 2016.66, "text": " This is 200 by 100.", "tokens": [51632, 639, 307, 2331, 538, 2319, 13, 51722], "temperature": 0.0, "avg_logprob": -0.4537743977137974, "compression_ratio": 1.406060606060606, "no_speech_prob": 1.6280514500977006e-06}, {"id": 389, "seek": 201666, "start": 2016.66, "end": 2019.5400000000002, "text": " This is just a matrix and a vector.", "tokens": [50364, 639, 307, 445, 257, 8141, 293, 257, 8062, 13, 50508], "temperature": 0.0, "avg_logprob": -0.33434832993374075, "compression_ratio": 1.7459459459459459, "no_speech_prob": 1.7330513628621702e-06}, {"id": 390, "seek": 201666, "start": 2019.5400000000002, "end": 2022.8600000000001, "text": " This is 200.", "tokens": [50508, 639, 307, 2331, 13, 50674], "temperature": 0.0, "avg_logprob": -0.33434832993374075, "compression_ratio": 1.7459459459459459, "no_speech_prob": 1.7330513628621702e-06}, {"id": 391, "seek": 201666, "start": 2022.8600000000001, "end": 2027.18, "text": " And then let's create a couple of weight matrices,", "tokens": [50674, 400, 550, 718, 311, 1884, 257, 1916, 295, 3364, 32284, 11, 50890], "temperature": 0.0, "avg_logprob": -0.33434832993374075, "compression_ratio": 1.7459459459459459, "no_speech_prob": 1.7330513628621702e-06}, {"id": 392, "seek": 201666, "start": 2027.18, "end": 2030.9, "text": " two weight matrices and two bias vectors.", "tokens": [50890, 732, 3364, 32284, 293, 732, 12577, 18875, 13, 51076], "temperature": 0.0, "avg_logprob": -0.33434832993374075, "compression_ratio": 1.7459459459459459, "no_speech_prob": 1.7330513628621702e-06}, {"id": 393, "seek": 201666, "start": 2030.9, "end": 2034.38, "text": " Okay, so we've got some input data, x's and y's.", "tokens": [51076, 1033, 11, 370, 321, 600, 658, 512, 4846, 1412, 11, 2031, 311, 293, 288, 311, 13, 51250], "temperature": 0.0, "avg_logprob": -0.33434832993374075, "compression_ratio": 1.7459459459459459, "no_speech_prob": 1.7330513628621702e-06}, {"id": 394, "seek": 201666, "start": 2034.38, "end": 2038.42, "text": " And we've got some weight matrices and bias vectors.", "tokens": [51250, 400, 321, 600, 658, 512, 3364, 32284, 293, 12577, 18875, 13, 51452], "temperature": 0.0, "avg_logprob": -0.33434832993374075, "compression_ratio": 1.7459459459459459, "no_speech_prob": 1.7330513628621702e-06}, {"id": 395, "seek": 201666, "start": 2038.42, "end": 2043.8200000000002, "text": " So let's create a linear layer function, which we've done lots of times before.", "tokens": [51452, 407, 718, 311, 1884, 257, 8213, 4583, 2445, 11, 597, 321, 600, 1096, 3195, 295, 1413, 949, 13, 51722], "temperature": 0.0, "avg_logprob": -0.33434832993374075, "compression_ratio": 1.7459459459459459, "no_speech_prob": 1.7330513628621702e-06}, {"id": 396, "seek": 204382, "start": 2043.86, "end": 2046.98, "text": " And let's start going through a little neural net.", "tokens": [50366, 400, 718, 311, 722, 516, 807, 257, 707, 18161, 2533, 13, 50522], "temperature": 0.0, "avg_logprob": -0.2745317208646524, "compression_ratio": 1.6200873362445414, "no_speech_prob": 1.018809143715771e-06}, {"id": 397, "seek": 204382, "start": 2046.98, "end": 2050.34, "text": " I'm mentioning this is the forward pass of our neural net.", "tokens": [50522, 286, 478, 18315, 341, 307, 264, 2128, 1320, 295, 527, 18161, 2533, 13, 50690], "temperature": 0.0, "avg_logprob": -0.2745317208646524, "compression_ratio": 1.6200873362445414, "no_speech_prob": 1.018809143715771e-06}, {"id": 398, "seek": 204382, "start": 2050.34, "end": 2055.02, "text": " So we're gonna apply our linear layer to the x's with our first set of weights and", "tokens": [50690, 407, 321, 434, 799, 3079, 527, 8213, 4583, 281, 264, 2031, 311, 365, 527, 700, 992, 295, 17443, 293, 50924], "temperature": 0.0, "avg_logprob": -0.2745317208646524, "compression_ratio": 1.6200873362445414, "no_speech_prob": 1.018809143715771e-06}, {"id": 399, "seek": 204382, "start": 2055.02, "end": 2058.7799999999997, "text": " our first set of biases and see what the mean and standard deviation is.", "tokens": [50924, 527, 700, 992, 295, 32152, 293, 536, 437, 264, 914, 293, 3832, 25163, 307, 13, 51112], "temperature": 0.0, "avg_logprob": -0.2745317208646524, "compression_ratio": 1.6200873362445414, "no_speech_prob": 1.018809143715771e-06}, {"id": 400, "seek": 204382, "start": 2058.7799999999997, "end": 2065.14, "text": " Okay, it's about 0 and about 1, so that's good news.", "tokens": [51112, 1033, 11, 309, 311, 466, 1958, 293, 466, 502, 11, 370, 300, 311, 665, 2583, 13, 51430], "temperature": 0.0, "avg_logprob": -0.2745317208646524, "compression_ratio": 1.6200873362445414, "no_speech_prob": 1.018809143715771e-06}, {"id": 401, "seek": 204382, "start": 2066.34, "end": 2070.98, "text": " And the reason why is because we have 100 inputs and", "tokens": [51490, 400, 264, 1778, 983, 307, 570, 321, 362, 2319, 15743, 293, 51722], "temperature": 0.0, "avg_logprob": -0.2745317208646524, "compression_ratio": 1.6200873362445414, "no_speech_prob": 1.018809143715771e-06}, {"id": 402, "seek": 207098, "start": 2070.98, "end": 2074.78, "text": " we divided it by square root of 100, just like Gloreau told us to.", "tokens": [50364, 321, 6666, 309, 538, 3732, 5593, 295, 2319, 11, 445, 411, 5209, 418, 1459, 1907, 505, 281, 13, 50554], "temperature": 0.0, "avg_logprob": -0.24678283762709002, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00020342688367236406}, {"id": 403, "seek": 207098, "start": 2074.78, "end": 2079.42, "text": " And our second one has 50 inputs and we divide by square root of 50.", "tokens": [50554, 400, 527, 1150, 472, 575, 2625, 15743, 293, 321, 9845, 538, 3732, 5593, 295, 2625, 13, 50786], "temperature": 0.0, "avg_logprob": -0.24678283762709002, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00020342688367236406}, {"id": 404, "seek": 207098, "start": 2079.42, "end": 2082.26, "text": " And so this all ought to work, right?", "tokens": [50786, 400, 370, 341, 439, 13416, 281, 589, 11, 558, 30, 50928], "temperature": 0.0, "avg_logprob": -0.24678283762709002, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00020342688367236406}, {"id": 405, "seek": 207098, "start": 2082.26, "end": 2087.02, "text": " And so far it is, but now we're gonna mess everything up by doing ReLU.", "tokens": [50928, 400, 370, 1400, 309, 307, 11, 457, 586, 321, 434, 799, 2082, 1203, 493, 538, 884, 1300, 43, 52, 13, 51166], "temperature": 0.0, "avg_logprob": -0.24678283762709002, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00020342688367236406}, {"id": 406, "seek": 207098, "start": 2087.02, "end": 2090.42, "text": " So ReLU, after we do a ReLU, look,", "tokens": [51166, 407, 1300, 43, 52, 11, 934, 321, 360, 257, 1300, 43, 52, 11, 574, 11, 51336], "temperature": 0.0, "avg_logprob": -0.24678283762709002, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00020342688367236406}, {"id": 407, "seek": 207098, "start": 2090.42, "end": 2095.54, "text": " we don't have a 0 mean or a 1 standard deviation anymore.", "tokens": [51336, 321, 500, 380, 362, 257, 1958, 914, 420, 257, 502, 3832, 25163, 3602, 13, 51592], "temperature": 0.0, "avg_logprob": -0.24678283762709002, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.00020342688367236406}, {"id": 408, "seek": 209554, "start": 2096.5, "end": 2102.1, "text": " So if we go through that and create a deep neural network with Gloreau", "tokens": [50412, 407, 498, 321, 352, 807, 300, 293, 1884, 257, 2452, 18161, 3209, 365, 5209, 418, 1459, 50692], "temperature": 0.0, "avg_logprob": -0.341695805599815, "compression_ratio": 1.471698113207547, "no_speech_prob": 5.422216418082826e-06}, {"id": 409, "seek": 209554, "start": 2102.1, "end": 2109.74, "text": " initialization, but with a ReLU, dear, it's disappeared.", "tokens": [50692, 5883, 2144, 11, 457, 365, 257, 1300, 43, 52, 11, 6875, 11, 309, 311, 13954, 13, 51074], "temperature": 0.0, "avg_logprob": -0.341695805599815, "compression_ratio": 1.471698113207547, "no_speech_prob": 5.422216418082826e-06}, {"id": 410, "seek": 209554, "start": 2109.74, "end": 2111.14, "text": " It's all gone to 0.", "tokens": [51074, 467, 311, 439, 2780, 281, 1958, 13, 51144], "temperature": 0.0, "avg_logprob": -0.341695805599815, "compression_ratio": 1.471698113207547, "no_speech_prob": 5.422216418082826e-06}, {"id": 411, "seek": 209554, "start": 2111.14, "end": 2112.5, "text": " And you can see why, right?", "tokens": [51144, 400, 291, 393, 536, 983, 11, 558, 30, 51212], "temperature": 0.0, "avg_logprob": -0.341695805599815, "compression_ratio": 1.471698113207547, "no_speech_prob": 5.422216418082826e-06}, {"id": 412, "seek": 209554, "start": 2112.5, "end": 2118.86, "text": " After a matrix multiply and a ReLU, our means and variances are going down.", "tokens": [51212, 2381, 257, 8141, 12972, 293, 257, 1300, 43, 52, 11, 527, 1355, 293, 1374, 21518, 366, 516, 760, 13, 51530], "temperature": 0.0, "avg_logprob": -0.341695805599815, "compression_ratio": 1.471698113207547, "no_speech_prob": 5.422216418082826e-06}, {"id": 413, "seek": 209554, "start": 2120.2599999999998, "end": 2123.58, "text": " And of course they're going down because a ReLU squishes it.", "tokens": [51600, 400, 295, 1164, 436, 434, 516, 760, 570, 257, 1300, 43, 52, 2339, 16423, 309, 13, 51766], "temperature": 0.0, "avg_logprob": -0.341695805599815, "compression_ratio": 1.471698113207547, "no_speech_prob": 5.422216418082826e-06}, {"id": 414, "seek": 212554, "start": 2126.54, "end": 2130.7799999999997, "text": " So I'm not gonna worry about the math of why, but", "tokens": [50414, 407, 286, 478, 406, 799, 3292, 466, 264, 5221, 295, 983, 11, 457, 50626], "temperature": 0.0, "avg_logprob": -0.4620611017400568, "compression_ratio": 1.3692307692307693, "no_speech_prob": 2.699577521525498e-07}, {"id": 415, "seek": 212554, "start": 2130.7799999999997, "end": 2136.82, "text": " a very important paper indeed called Delving Deep Indirectifiers,", "tokens": [50626, 257, 588, 1021, 3035, 6451, 1219, 5831, 798, 14895, 2333, 11890, 23463, 11, 50928], "temperature": 0.0, "avg_logprob": -0.4620611017400568, "compression_ratio": 1.3692307692307693, "no_speech_prob": 2.699577521525498e-07}, {"id": 416, "seek": 212554, "start": 2136.82, "end": 2143.34, "text": " Surpassing Human-Level Performance on ImageNet Classification,", "tokens": [50928, 6732, 9216, 278, 10294, 12, 11020, 779, 25047, 322, 29903, 31890, 9471, 3774, 11, 51254], "temperature": 0.0, "avg_logprob": -0.4620611017400568, "compression_ratio": 1.3692307692307693, "no_speech_prob": 2.699577521525498e-07}, {"id": 417, "seek": 212554, "start": 2143.34, "end": 2147.3, "text": " by Kaiming He et al, came up with a new init,", "tokens": [51254, 538, 10988, 332, 278, 634, 1030, 419, 11, 1361, 493, 365, 257, 777, 3157, 11, 51452], "temperature": 0.0, "avg_logprob": -0.4620611017400568, "compression_ratio": 1.3692307692307693, "no_speech_prob": 2.699577521525498e-07}, {"id": 418, "seek": 212554, "start": 2147.3, "end": 2152.94, "text": " which is just like Gloreau initialization.", "tokens": [51452, 597, 307, 445, 411, 5209, 418, 1459, 5883, 2144, 13, 51734], "temperature": 0.0, "avg_logprob": -0.4620611017400568, "compression_ratio": 1.3692307692307693, "no_speech_prob": 2.699577521525498e-07}, {"id": 419, "seek": 215294, "start": 2152.94, "end": 2159.78, "text": " But you multiply, remember the Gloreau initialization was 1 over root n.", "tokens": [50364, 583, 291, 12972, 11, 1604, 264, 5209, 418, 1459, 5883, 2144, 390, 502, 670, 5593, 297, 13, 50706], "temperature": 0.0, "avg_logprob": -0.29338985019259983, "compression_ratio": 1.532994923857868, "no_speech_prob": 1.8448236005497165e-06}, {"id": 420, "seek": 215294, "start": 2159.78, "end": 2161.94, "text": " This one is root 2 over n.", "tokens": [50706, 639, 472, 307, 5593, 568, 670, 297, 13, 50814], "temperature": 0.0, "avg_logprob": -0.29338985019259983, "compression_ratio": 1.532994923857868, "no_speech_prob": 1.8448236005497165e-06}, {"id": 421, "seek": 215294, "start": 2161.94, "end": 2163.66, "text": " And again, n is the number of inputs.", "tokens": [50814, 400, 797, 11, 297, 307, 264, 1230, 295, 15743, 13, 50900], "temperature": 0.0, "avg_logprob": -0.29338985019259983, "compression_ratio": 1.532994923857868, "no_speech_prob": 1.8448236005497165e-06}, {"id": 422, "seek": 215294, "start": 2164.94, "end": 2166.98, "text": " So let's try it.", "tokens": [50964, 407, 718, 311, 853, 309, 13, 51066], "temperature": 0.0, "avg_logprob": -0.29338985019259983, "compression_ratio": 1.532994923857868, "no_speech_prob": 1.8448236005497165e-06}, {"id": 423, "seek": 215294, "start": 2166.98, "end": 2169.42, "text": " So we've got 100 inputs.", "tokens": [51066, 407, 321, 600, 658, 2319, 15743, 13, 51188], "temperature": 0.0, "avg_logprob": -0.29338985019259983, "compression_ratio": 1.532994923857868, "no_speech_prob": 1.8448236005497165e-06}, {"id": 424, "seek": 215294, "start": 2169.42, "end": 2171.82, "text": " So we have to multiply it by root 2 over 100.", "tokens": [51188, 407, 321, 362, 281, 12972, 309, 538, 5593, 568, 670, 2319, 13, 51308], "temperature": 0.0, "avg_logprob": -0.29338985019259983, "compression_ratio": 1.532994923857868, "no_speech_prob": 1.8448236005497165e-06}, {"id": 425, "seek": 215294, "start": 2174.1, "end": 2176.78, "text": " And there we go.", "tokens": [51422, 400, 456, 321, 352, 13, 51556], "temperature": 0.0, "avg_logprob": -0.29338985019259983, "compression_ratio": 1.532994923857868, "no_speech_prob": 1.8448236005497165e-06}, {"id": 426, "seek": 215294, "start": 2176.78, "end": 2180.14, "text": " You can see we are, in fact, getting some non-zero numbers.", "tokens": [51556, 509, 393, 536, 321, 366, 11, 294, 1186, 11, 1242, 512, 2107, 12, 32226, 3547, 13, 51724], "temperature": 0.0, "avg_logprob": -0.29338985019259983, "compression_ratio": 1.532994923857868, "no_speech_prob": 1.8448236005497165e-06}, {"id": 427, "seek": 218014, "start": 2180.14, "end": 2184.1, "text": " So it's very encouraging even after going through 50 layers of depth.", "tokens": [50364, 407, 309, 311, 588, 14580, 754, 934, 516, 807, 2625, 7914, 295, 7161, 13, 50562], "temperature": 0.0, "avg_logprob": -0.32568359375, "compression_ratio": 1.6607142857142858, "no_speech_prob": 2.6274752599420026e-05}, {"id": 428, "seek": 218014, "start": 2186.3799999999997, "end": 2187.22, "text": " So that's good news.", "tokens": [50676, 407, 300, 311, 665, 2583, 13, 50718], "temperature": 0.0, "avg_logprob": -0.32568359375, "compression_ratio": 1.6607142857142858, "no_speech_prob": 2.6274752599420026e-05}, {"id": 429, "seek": 218014, "start": 2188.9, "end": 2193.18, "text": " So this is called Kaiming, it's either called Kaiming initialization or", "tokens": [50802, 407, 341, 307, 1219, 10988, 332, 278, 11, 309, 311, 2139, 1219, 10988, 332, 278, 5883, 2144, 420, 51016], "temperature": 0.0, "avg_logprob": -0.32568359375, "compression_ratio": 1.6607142857142858, "no_speech_prob": 2.6274752599420026e-05}, {"id": 430, "seek": 218014, "start": 2193.18, "end": 2195.02, "text": " called He initialization.", "tokens": [51016, 1219, 634, 5883, 2144, 13, 51108], "temperature": 0.0, "avg_logprob": -0.32568359375, "compression_ratio": 1.6607142857142858, "no_speech_prob": 2.6274752599420026e-05}, {"id": 431, "seek": 218014, "start": 2195.02, "end": 2198.3399999999997, "text": " And notice it looks like it's spelt he, but it's a Chinese surname.", "tokens": [51108, 400, 3449, 309, 1542, 411, 309, 311, 637, 2018, 415, 11, 457, 309, 311, 257, 4649, 50152, 13, 51274], "temperature": 0.0, "avg_logprob": -0.32568359375, "compression_ratio": 1.6607142857142858, "no_speech_prob": 2.6274752599420026e-05}, {"id": 432, "seek": 218014, "start": 2198.3399999999997, "end": 2200.14, "text": " So it's actually pronounced He.", "tokens": [51274, 407, 309, 311, 767, 23155, 634, 13, 51364], "temperature": 0.0, "avg_logprob": -0.32568359375, "compression_ratio": 1.6607142857142858, "no_speech_prob": 2.6274752599420026e-05}, {"id": 433, "seek": 218014, "start": 2202.8199999999997, "end": 2206.98, "text": " Okay, maybe that's why a lot of people increasingly call it Kaiming", "tokens": [51498, 1033, 11, 1310, 300, 311, 983, 257, 688, 295, 561, 12980, 818, 309, 10988, 332, 278, 51706], "temperature": 0.0, "avg_logprob": -0.32568359375, "compression_ratio": 1.6607142857142858, "no_speech_prob": 2.6274752599420026e-05}, {"id": 434, "seek": 218014, "start": 2206.98, "end": 2207.94, "text": " initialization.", "tokens": [51706, 5883, 2144, 13, 51754], "temperature": 0.0, "avg_logprob": -0.32568359375, "compression_ratio": 1.6607142857142858, "no_speech_prob": 2.6274752599420026e-05}, {"id": 435, "seek": 220794, "start": 2207.98, "end": 2210.94, "text": " I don't have to say his surname, which is a little bit harder to pronounce.", "tokens": [50366, 286, 500, 380, 362, 281, 584, 702, 50152, 11, 597, 307, 257, 707, 857, 6081, 281, 19567, 13, 50514], "temperature": 0.0, "avg_logprob": -0.28349751117182714, "compression_ratio": 1.5569620253164558, "no_speech_prob": 7.527988145739073e-06}, {"id": 436, "seek": 220794, "start": 2212.66, "end": 2214.62, "text": " All right, so how on earth do we actually use this?", "tokens": [50600, 1057, 558, 11, 370, 577, 322, 4120, 360, 321, 767, 764, 341, 30, 50698], "temperature": 0.0, "avg_logprob": -0.28349751117182714, "compression_ratio": 1.5569620253164558, "no_speech_prob": 7.527988145739073e-06}, {"id": 437, "seek": 220794, "start": 2214.62, "end": 2218.26, "text": " Now that we know what initialization function to use for", "tokens": [50698, 823, 300, 321, 458, 437, 5883, 2144, 2445, 281, 764, 337, 50880], "temperature": 0.0, "avg_logprob": -0.28349751117182714, "compression_ratio": 1.5569620253164558, "no_speech_prob": 7.527988145739073e-06}, {"id": 438, "seek": 220794, "start": 2218.26, "end": 2222.3, "text": " a deep neural network with a ReLU activation function.", "tokens": [50880, 257, 2452, 18161, 3209, 365, 257, 1300, 43, 52, 24433, 2445, 13, 51082], "temperature": 0.0, "avg_logprob": -0.28349751117182714, "compression_ratio": 1.5569620253164558, "no_speech_prob": 7.527988145739073e-06}, {"id": 439, "seek": 220794, "start": 2224.18, "end": 2231.42, "text": " The trick is to use a method called apply, which all nn.modules have.", "tokens": [51176, 440, 4282, 307, 281, 764, 257, 3170, 1219, 3079, 11, 597, 439, 297, 77, 13, 8014, 3473, 362, 13, 51538], "temperature": 0.0, "avg_logprob": -0.28349751117182714, "compression_ratio": 1.5569620253164558, "no_speech_prob": 7.527988145739073e-06}, {"id": 440, "seek": 220794, "start": 2231.42, "end": 2235.54, "text": " So if we grab our model, we can apply any function we like.", "tokens": [51538, 407, 498, 321, 4444, 527, 2316, 11, 321, 393, 3079, 604, 2445, 321, 411, 13, 51744], "temperature": 0.0, "avg_logprob": -0.28349751117182714, "compression_ratio": 1.5569620253164558, "no_speech_prob": 7.527988145739073e-06}, {"id": 441, "seek": 223554, "start": 2235.54, "end": 2240.46, "text": " For example, let's apply the function print the name of the type.", "tokens": [50364, 1171, 1365, 11, 718, 311, 3079, 264, 2445, 4482, 264, 1315, 295, 264, 2010, 13, 50610], "temperature": 0.0, "avg_logprob": -0.2635975955577379, "compression_ratio": 1.7346938775510203, "no_speech_prob": 9.570828751748195e-07}, {"id": 442, "seek": 223554, "start": 2241.86, "end": 2244.58, "text": " So here you can see it's going through and", "tokens": [50680, 407, 510, 291, 393, 536, 309, 311, 516, 807, 293, 50816], "temperature": 0.0, "avg_logprob": -0.2635975955577379, "compression_ratio": 1.7346938775510203, "no_speech_prob": 9.570828751748195e-07}, {"id": 443, "seek": 223554, "start": 2244.58, "end": 2250.58, "text": " it's printing out all of the modules that are inside our model.", "tokens": [50816, 309, 311, 14699, 484, 439, 295, 264, 16679, 300, 366, 1854, 527, 2316, 13, 51116], "temperature": 0.0, "avg_logprob": -0.2635975955577379, "compression_ratio": 1.7346938775510203, "no_speech_prob": 9.570828751748195e-07}, {"id": 444, "seek": 223554, "start": 2252.22, "end": 2256.62, "text": " And notice that our model has modules inside modules.", "tokens": [51198, 400, 3449, 300, 527, 2316, 575, 16679, 1854, 16679, 13, 51418], "temperature": 0.0, "avg_logprob": -0.2635975955577379, "compression_ratio": 1.7346938775510203, "no_speech_prob": 9.570828751748195e-07}, {"id": 445, "seek": 223554, "start": 2256.62, "end": 2260.58, "text": " It's a conv in a sequential, in a sequential.", "tokens": [51418, 467, 311, 257, 3754, 294, 257, 42881, 11, 294, 257, 42881, 13, 51616], "temperature": 0.0, "avg_logprob": -0.2635975955577379, "compression_ratio": 1.7346938775510203, "no_speech_prob": 9.570828751748195e-07}, {"id": 446, "seek": 223554, "start": 2260.58, "end": 2264.58, "text": " But model.apply goes through all of them regardless of their depth.", "tokens": [51616, 583, 2316, 13, 1746, 356, 1709, 807, 439, 295, 552, 10060, 295, 641, 7161, 13, 51816], "temperature": 0.0, "avg_logprob": -0.2635975955577379, "compression_ratio": 1.7346938775510203, "no_speech_prob": 9.570828751748195e-07}, {"id": 447, "seek": 226554, "start": 2266.54, "end": 2269.62, "text": " So we can apply an init function.", "tokens": [50414, 407, 321, 393, 3079, 364, 3157, 2445, 13, 50568], "temperature": 0.0, "avg_logprob": -0.4498588057125316, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.602810471013072e-06}, {"id": 448, "seek": 226554, "start": 2270.9, "end": 2276.22, "text": " So we can apply the init function which simply does.", "tokens": [50632, 407, 321, 393, 3079, 264, 3157, 2445, 597, 2935, 775, 13, 50898], "temperature": 0.0, "avg_logprob": -0.4498588057125316, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.602810471013072e-06}, {"id": 449, "seek": 226554, "start": 2279.58, "end": 2285.06, "text": " Multiply, random numbers, multiply, normally distributed random numbers", "tokens": [51066, 31150, 356, 11, 4974, 3547, 11, 12972, 11, 5646, 12631, 4974, 3547, 51340], "temperature": 0.0, "avg_logprob": -0.4498588057125316, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.602810471013072e-06}, {"id": 450, "seek": 226554, "start": 2285.06, "end": 2288.38, "text": " times square root of 2 over the number of inputs.", "tokens": [51340, 1413, 3732, 5593, 295, 568, 670, 264, 1230, 295, 15743, 13, 51506], "temperature": 0.0, "avg_logprob": -0.4498588057125316, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.602810471013072e-06}, {"id": 451, "seek": 226554, "start": 2290.14, "end": 2293.14, "text": " That's such an easy thing, it's not even worth writing.", "tokens": [51594, 663, 311, 1270, 364, 1858, 551, 11, 309, 311, 406, 754, 3163, 3579, 13, 51744], "temperature": 0.0, "avg_logprob": -0.4498588057125316, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.602810471013072e-06}, {"id": 452, "seek": 229314, "start": 2293.18, "end": 2295.66, "text": " So that's already been written, but that's all it does.", "tokens": [50366, 407, 300, 311, 1217, 668, 3720, 11, 457, 300, 311, 439, 309, 775, 13, 50490], "temperature": 0.0, "avg_logprob": -0.2721219023397146, "compression_ratio": 1.6564885496183206, "no_speech_prob": 1.9947294731537113e-06}, {"id": 453, "seek": 229314, "start": 2295.66, "end": 2296.66, "text": " It just does that one thing.", "tokens": [50490, 467, 445, 775, 300, 472, 551, 13, 50540], "temperature": 0.0, "avg_logprob": -0.2721219023397146, "compression_ratio": 1.6564885496183206, "no_speech_prob": 1.9947294731537113e-06}, {"id": 454, "seek": 229314, "start": 2296.66, "end": 2298.18, "text": " It's called init.kymingNormal.", "tokens": [50540, 467, 311, 1219, 3157, 13, 74, 88, 2810, 45, 24440, 13, 50616], "temperature": 0.0, "avg_logprob": -0.2721219023397146, "compression_ratio": 1.6564885496183206, "no_speech_prob": 1.9947294731537113e-06}, {"id": 455, "seek": 229314, "start": 2299.3799999999997, "end": 2300.54, "text": " As we've seen before,", "tokens": [50676, 1018, 321, 600, 1612, 949, 11, 50734], "temperature": 0.0, "avg_logprob": -0.2721219023397146, "compression_ratio": 1.6564885496183206, "no_speech_prob": 1.9947294731537113e-06}, {"id": 456, "seek": 229314, "start": 2300.54, "end": 2303.66, "text": " if there's an underscore at the end of a PyTorch method name,", "tokens": [50734, 498, 456, 311, 364, 37556, 412, 264, 917, 295, 257, 9953, 51, 284, 339, 3170, 1315, 11, 50890], "temperature": 0.0, "avg_logprob": -0.2721219023397146, "compression_ratio": 1.6564885496183206, "no_speech_prob": 1.9947294731537113e-06}, {"id": 457, "seek": 229314, "start": 2303.66, "end": 2306.3399999999997, "text": " that means that it changes something in place.", "tokens": [50890, 300, 1355, 300, 309, 2962, 746, 294, 1081, 13, 51024], "temperature": 0.0, "avg_logprob": -0.2721219023397146, "compression_ratio": 1.6564885496183206, "no_speech_prob": 1.9947294731537113e-06}, {"id": 458, "seek": 229314, "start": 2306.3399999999997, "end": 2312.18, "text": " So init.kymingNormal underscore will modify this weight matrix so", "tokens": [51024, 407, 3157, 13, 4133, 2810, 45, 24440, 37556, 486, 16927, 341, 3364, 8141, 370, 51316], "temperature": 0.0, "avg_logprob": -0.2721219023397146, "compression_ratio": 1.6564885496183206, "no_speech_prob": 1.9947294731537113e-06}, {"id": 459, "seek": 229314, "start": 2312.18, "end": 2316.98, "text": " that it has been initialized with normally distributed random numbers", "tokens": [51316, 300, 309, 575, 668, 5883, 1602, 365, 5646, 12631, 4974, 3547, 51556], "temperature": 0.0, "avg_logprob": -0.2721219023397146, "compression_ratio": 1.6564885496183206, "no_speech_prob": 1.9947294731537113e-06}, {"id": 460, "seek": 229314, "start": 2316.98, "end": 2321.42, "text": " based on root of 2 divided by the number of inputs.", "tokens": [51556, 2361, 322, 5593, 295, 568, 6666, 538, 264, 1230, 295, 15743, 13, 51778], "temperature": 0.0, "avg_logprob": -0.2721219023397146, "compression_ratio": 1.6564885496183206, "no_speech_prob": 1.9947294731537113e-06}, {"id": 461, "seek": 232142, "start": 2322.42, "end": 2326.5, "text": " Now, you can't do that to a sequential layer, or a ReLU layer, or", "tokens": [50414, 823, 11, 291, 393, 380, 360, 300, 281, 257, 42881, 4583, 11, 420, 257, 1300, 43, 52, 4583, 11, 420, 50618], "temperature": 0.0, "avg_logprob": -0.32072457207573785, "compression_ratio": 1.538860103626943, "no_speech_prob": 1.1726421007551835e-06}, {"id": 462, "seek": 232142, "start": 2326.5, "end": 2327.7400000000002, "text": " a flattened layer.", "tokens": [50618, 257, 24183, 292, 4583, 13, 50680], "temperature": 0.0, "avg_logprob": -0.32072457207573785, "compression_ratio": 1.538860103626943, "no_speech_prob": 1.1726421007551835e-06}, {"id": 463, "seek": 232142, "start": 2327.7400000000002, "end": 2332.9, "text": " So we should check that the module is a conv or linear layer.", "tokens": [50680, 407, 321, 820, 1520, 300, 264, 10088, 307, 257, 3754, 420, 8213, 4583, 13, 50938], "temperature": 0.0, "avg_logprob": -0.32072457207573785, "compression_ratio": 1.538860103626943, "no_speech_prob": 1.1726421007551835e-06}, {"id": 464, "seek": 232142, "start": 2334.54, "end": 2337.7000000000003, "text": " And then we can just say model.apply the function.", "tokens": [51020, 400, 550, 321, 393, 445, 584, 2316, 13, 1746, 356, 264, 2445, 13, 51178], "temperature": 0.0, "avg_logprob": -0.32072457207573785, "compression_ratio": 1.538860103626943, "no_speech_prob": 1.1726421007551835e-06}, {"id": 465, "seek": 232142, "start": 2339.14, "end": 2346.2200000000003, "text": " And so if we do that, and now I can use our learning rate finder callbacks", "tokens": [51250, 400, 370, 498, 321, 360, 300, 11, 293, 586, 286, 393, 764, 527, 2539, 3314, 915, 260, 818, 17758, 51604], "temperature": 0.0, "avg_logprob": -0.32072457207573785, "compression_ratio": 1.538860103626943, "no_speech_prob": 1.1726421007551835e-06}, {"id": 466, "seek": 232142, "start": 2346.2200000000003, "end": 2347.7400000000002, "text": " that we created earlier.", "tokens": [51604, 300, 321, 2942, 3071, 13, 51680], "temperature": 0.0, "avg_logprob": -0.32072457207573785, "compression_ratio": 1.538860103626943, "no_speech_prob": 1.1726421007551835e-06}, {"id": 467, "seek": 234774, "start": 2347.74, "end": 2353.62, "text": " And this time I don't have to worry about, actually we can create our own ones", "tokens": [50364, 400, 341, 565, 286, 500, 380, 362, 281, 3292, 466, 11, 767, 321, 393, 1884, 527, 1065, 2306, 50658], "temperature": 0.0, "avg_logprob": -0.3752107744093065, "compression_ratio": 1.4426229508196722, "no_speech_prob": 8.22018482722342e-05}, {"id": 468, "seek": 234774, "start": 2353.62, "end": 2357.62, "text": " because we don't need to use even the weird gamma thing anymore.", "tokens": [50658, 570, 321, 500, 380, 643, 281, 764, 754, 264, 3657, 15546, 551, 3602, 13, 50858], "temperature": 0.0, "avg_logprob": -0.3752107744093065, "compression_ratio": 1.4426229508196722, "no_speech_prob": 8.22018482722342e-05}, {"id": 469, "seek": 234774, "start": 2357.62, "end": 2362.8599999999997, "text": " So let's go back and copy that.", "tokens": [50858, 407, 718, 311, 352, 646, 293, 5055, 300, 13, 51120], "temperature": 0.0, "avg_logprob": -0.3752107744093065, "compression_ratio": 1.4426229508196722, "no_speech_prob": 8.22018482722342e-05}, {"id": 470, "seek": 234774, "start": 2362.8599999999997, "end": 2363.4199999999996, "text": " Oopsie daisy.", "tokens": [51120, 21726, 414, 1120, 14169, 13, 51148], "temperature": 0.0, "avg_logprob": -0.3752107744093065, "compression_ratio": 1.4426229508196722, "no_speech_prob": 8.22018482722342e-05}, {"id": 471, "seek": 234774, "start": 2367.18, "end": 2370.66, "text": " Let's get rid of this gamma equals 1.1, it shouldn't be necessary anymore.", "tokens": [51336, 961, 311, 483, 3973, 295, 341, 15546, 6915, 502, 13, 16, 11, 309, 4659, 380, 312, 4818, 3602, 13, 51510], "temperature": 0.0, "avg_logprob": -0.3752107744093065, "compression_ratio": 1.4426229508196722, "no_speech_prob": 8.22018482722342e-05}, {"id": 472, "seek": 237066, "start": 2370.66, "end": 2378.98, "text": " And we can probably make that 4 now.", "tokens": [50364, 400, 321, 393, 1391, 652, 300, 1017, 586, 13, 50780], "temperature": 0.0, "avg_logprob": -0.3816431681315104, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.75029163074214e-05}, {"id": 473, "seek": 237066, "start": 2378.98, "end": 2382.18, "text": " I should have, need to recreate the model.", "tokens": [50780, 286, 820, 362, 11, 643, 281, 25833, 264, 2316, 13, 50940], "temperature": 0.0, "avg_logprob": -0.3816431681315104, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.75029163074214e-05}, {"id": 474, "seek": 237066, "start": 2382.18, "end": 2386.14, "text": " There we go.", "tokens": [50940, 821, 321, 352, 13, 51138], "temperature": 0.0, "avg_logprob": -0.3816431681315104, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.75029163074214e-05}, {"id": 475, "seek": 237066, "start": 2386.14, "end": 2388.8199999999997, "text": " Okay, so that's looking much more sensible.", "tokens": [51138, 1033, 11, 370, 300, 311, 1237, 709, 544, 25380, 13, 51272], "temperature": 0.0, "avg_logprob": -0.3816431681315104, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.75029163074214e-05}, {"id": 476, "seek": 237066, "start": 2388.8199999999997, "end": 2391.5, "text": " So at least we've got to a point where the learning rate finder works,", "tokens": [51272, 407, 412, 1935, 321, 600, 658, 281, 257, 935, 689, 264, 2539, 3314, 915, 260, 1985, 11, 51406], "temperature": 0.0, "avg_logprob": -0.3816431681315104, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.75029163074214e-05}, {"id": 477, "seek": 237066, "start": 2391.5, "end": 2392.22, "text": " that's a good sign.", "tokens": [51406, 300, 311, 257, 665, 1465, 13, 51442], "temperature": 0.0, "avg_logprob": -0.3816431681315104, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.75029163074214e-05}, {"id": 478, "seek": 237066, "start": 2394.54, "end": 2398.8599999999997, "text": " So now when we create our learner, we're going to use our momentum learner still.", "tokens": [51558, 407, 586, 562, 321, 1884, 527, 33347, 11, 321, 434, 516, 281, 764, 527, 11244, 33347, 920, 13, 51774], "temperature": 0.0, "avg_logprob": -0.3816431681315104, "compression_ratio": 1.5527638190954773, "no_speech_prob": 1.75029163074214e-05}, {"id": 479, "seek": 239886, "start": 2398.86, "end": 2402.7000000000003, "text": " After we get the model, we will apply initWeights.", "tokens": [50364, 2381, 321, 483, 264, 2316, 11, 321, 486, 3079, 3157, 4360, 5761, 13, 50556], "temperature": 0.0, "avg_logprob": -0.34434482325678284, "compression_ratio": 1.5818181818181818, "no_speech_prob": 3.2377620300394483e-06}, {"id": 480, "seek": 239886, "start": 2402.7000000000003, "end": 2405.38, "text": " And apply also returns the model, so", "tokens": [50556, 400, 3079, 611, 11247, 264, 2316, 11, 370, 50690], "temperature": 0.0, "avg_logprob": -0.34434482325678284, "compression_ratio": 1.5818181818181818, "no_speech_prob": 3.2377620300394483e-06}, {"id": 481, "seek": 239886, "start": 2405.38, "end": 2408.86, "text": " this is actually going to return the model with the initialization applied.", "tokens": [50690, 341, 307, 767, 516, 281, 2736, 264, 2316, 365, 264, 5883, 2144, 6456, 13, 50864], "temperature": 0.0, "avg_logprob": -0.34434482325678284, "compression_ratio": 1.5818181818181818, "no_speech_prob": 3.2377620300394483e-06}, {"id": 482, "seek": 239886, "start": 2410.7400000000002, "end": 2412.78, "text": " While I wait, I will answer questions.", "tokens": [50958, 3987, 286, 1699, 11, 286, 486, 1867, 1651, 13, 51060], "temperature": 0.0, "avg_logprob": -0.34434482325678284, "compression_ratio": 1.5818181818181818, "no_speech_prob": 3.2377620300394483e-06}, {"id": 483, "seek": 239886, "start": 2414.7400000000002, "end": 2417.06, "text": " Okay, so Fabrizio asks,", "tokens": [51158, 1033, 11, 370, 17440, 24959, 1004, 8962, 11, 51274], "temperature": 0.0, "avg_logprob": -0.34434482325678284, "compression_ratio": 1.5818181818181818, "no_speech_prob": 3.2377620300394483e-06}, {"id": 484, "seek": 239886, "start": 2417.06, "end": 2420.78, "text": " why do we double the number of filters in successive convolutions?", "tokens": [51274, 983, 360, 321, 3834, 264, 1230, 295, 15995, 294, 48043, 3754, 15892, 30, 51460], "temperature": 0.0, "avg_logprob": -0.34434482325678284, "compression_ratio": 1.5818181818181818, "no_speech_prob": 3.2377620300394483e-06}, {"id": 485, "seek": 239886, "start": 2424.34, "end": 2427.6600000000003, "text": " So what's happening is in each stride two convolution,", "tokens": [51638, 407, 437, 311, 2737, 307, 294, 1184, 1056, 482, 732, 45216, 11, 51804], "temperature": 0.0, "avg_logprob": -0.34434482325678284, "compression_ratio": 1.5818181818181818, "no_speech_prob": 3.2377620300394483e-06}, {"id": 486, "seek": 242886, "start": 2429.86, "end": 2433.38, "text": " These are all stride two convolutions.", "tokens": [50414, 1981, 366, 439, 1056, 482, 732, 3754, 15892, 13, 50590], "temperature": 0.0, "avg_logprob": -0.3409489822387695, "compression_ratio": 1.6702127659574468, "no_speech_prob": 1.1365635828042286e-06}, {"id": 487, "seek": 242886, "start": 2433.38, "end": 2437.86, "text": " So this is changing the grid size from 28 by 28 to 14 by 14.", "tokens": [50590, 407, 341, 307, 4473, 264, 10748, 2744, 490, 7562, 538, 7562, 281, 3499, 538, 3499, 13, 50814], "temperature": 0.0, "avg_logprob": -0.3409489822387695, "compression_ratio": 1.6702127659574468, "no_speech_prob": 1.1365635828042286e-06}, {"id": 488, "seek": 242886, "start": 2437.86, "end": 2441.7400000000002, "text": " So it's reducing the size of the grid by a factor of 4 in total.", "tokens": [50814, 407, 309, 311, 12245, 264, 2744, 295, 264, 10748, 538, 257, 5952, 295, 1017, 294, 3217, 13, 51008], "temperature": 0.0, "avg_logprob": -0.3409489822387695, "compression_ratio": 1.6702127659574468, "no_speech_prob": 1.1365635828042286e-06}, {"id": 489, "seek": 242886, "start": 2443.58, "end": 2449.2200000000003, "text": " So basically, so as we go from 1 to 8, from this one to this one,", "tokens": [51100, 407, 1936, 11, 370, 382, 321, 352, 490, 502, 281, 1649, 11, 490, 341, 472, 281, 341, 472, 11, 51382], "temperature": 0.0, "avg_logprob": -0.3409489822387695, "compression_ratio": 1.6702127659574468, "no_speech_prob": 1.1365635828042286e-06}, {"id": 490, "seek": 242886, "start": 2449.2200000000003, "end": 2453.42, "text": " same deal, we're going from 14 by 14 to 7 by 7.", "tokens": [51382, 912, 2028, 11, 321, 434, 516, 490, 3499, 538, 3499, 281, 1614, 538, 1614, 13, 51592], "temperature": 0.0, "avg_logprob": -0.3409489822387695, "compression_ratio": 1.6702127659574468, "no_speech_prob": 1.1365635828042286e-06}, {"id": 491, "seek": 242886, "start": 2453.42, "end": 2457.6200000000003, "text": " So if we reduce the grid size by 4.", "tokens": [51592, 407, 498, 321, 5407, 264, 10748, 2744, 538, 1017, 13, 51802], "temperature": 0.0, "avg_logprob": -0.3409489822387695, "compression_ratio": 1.6702127659574468, "no_speech_prob": 1.1365635828042286e-06}, {"id": 492, "seek": 245762, "start": 2457.66, "end": 2460.54, "text": " We want it to learn something.", "tokens": [50366, 492, 528, 309, 281, 1466, 746, 13, 50510], "temperature": 0.0, "avg_logprob": -0.2658689835492302, "compression_ratio": 1.5757575757575757, "no_speech_prob": 1.5057013342811842e-06}, {"id": 493, "seek": 245762, "start": 2461.8599999999997, "end": 2468.7, "text": " And if you give it exactly the same kind of number of units or", "tokens": [50576, 400, 498, 291, 976, 309, 2293, 264, 912, 733, 295, 1230, 295, 6815, 420, 50918], "temperature": 0.0, "avg_logprob": -0.2658689835492302, "compression_ratio": 1.5757575757575757, "no_speech_prob": 1.5057013342811842e-06}, {"id": 494, "seek": 245762, "start": 2468.7, "end": 2473.98, "text": " activations, it's not really forcing it to learn things as much.", "tokens": [50918, 2430, 763, 11, 309, 311, 406, 534, 19030, 309, 281, 1466, 721, 382, 709, 13, 51182], "temperature": 0.0, "avg_logprob": -0.2658689835492302, "compression_ratio": 1.5757575757575757, "no_speech_prob": 1.5057013342811842e-06}, {"id": 495, "seek": 245762, "start": 2473.98, "end": 2479.8599999999997, "text": " So ideally, as we decrease the grid size, we want to", "tokens": [51182, 407, 22915, 11, 382, 321, 11514, 264, 10748, 2744, 11, 321, 528, 281, 51476], "temperature": 0.0, "avg_logprob": -0.2658689835492302, "compression_ratio": 1.5757575757575757, "no_speech_prob": 1.5057013342811842e-06}, {"id": 496, "seek": 245762, "start": 2479.8599999999997, "end": 2484.22, "text": " have enough channels that you end up with a few less activations than before,", "tokens": [51476, 362, 1547, 9235, 300, 291, 917, 493, 365, 257, 1326, 1570, 2430, 763, 813, 949, 11, 51694], "temperature": 0.0, "avg_logprob": -0.2658689835492302, "compression_ratio": 1.5757575757575757, "no_speech_prob": 1.5057013342811842e-06}, {"id": 497, "seek": 245762, "start": 2484.22, "end": 2485.58, "text": " but not too many less.", "tokens": [51694, 457, 406, 886, 867, 1570, 13, 51762], "temperature": 0.0, "avg_logprob": -0.2658689835492302, "compression_ratio": 1.5757575757575757, "no_speech_prob": 1.5057013342811842e-06}, {"id": 498, "seek": 248558, "start": 2485.58, "end": 2487.9, "text": " So if we double the number of channels,", "tokens": [50364, 407, 498, 321, 3834, 264, 1230, 295, 9235, 11, 50480], "temperature": 0.0, "avg_logprob": -0.2353733654679923, "compression_ratio": 1.7377049180327868, "no_speech_prob": 3.747992138869449e-07}, {"id": 499, "seek": 248558, "start": 2487.9, "end": 2490.98, "text": " then that means we've decreased the grid size by a model of 4.", "tokens": [50480, 550, 300, 1355, 321, 600, 24436, 264, 10748, 2744, 538, 257, 2316, 295, 1017, 13, 50634], "temperature": 0.0, "avg_logprob": -0.2353733654679923, "compression_ratio": 1.7377049180327868, "no_speech_prob": 3.747992138869449e-07}, {"id": 500, "seek": 248558, "start": 2490.98, "end": 2493.2599999999998, "text": " Increase the channel count by a model of 2.", "tokens": [50634, 30367, 651, 264, 2269, 1207, 538, 257, 2316, 295, 568, 13, 50748], "temperature": 0.0, "avg_logprob": -0.2353733654679923, "compression_ratio": 1.7377049180327868, "no_speech_prob": 3.747992138869449e-07}, {"id": 501, "seek": 248558, "start": 2493.2599999999998, "end": 2497.14, "text": " So overall, the number of activations has decreased by a factor of 2.", "tokens": [50748, 407, 4787, 11, 264, 1230, 295, 2430, 763, 575, 24436, 538, 257, 5952, 295, 568, 13, 50942], "temperature": 0.0, "avg_logprob": -0.2353733654679923, "compression_ratio": 1.7377049180327868, "no_speech_prob": 3.747992138869449e-07}, {"id": 502, "seek": 248558, "start": 2498.2599999999998, "end": 2499.7799999999997, "text": " And so that's what we want.", "tokens": [50998, 400, 370, 300, 311, 437, 321, 528, 13, 51074], "temperature": 0.0, "avg_logprob": -0.2353733654679923, "compression_ratio": 1.7377049180327868, "no_speech_prob": 3.747992138869449e-07}, {"id": 503, "seek": 248558, "start": 2499.7799999999997, "end": 2503.8199999999997, "text": " We want to be kind of forcing it to find ways of compressing", "tokens": [51074, 492, 528, 281, 312, 733, 295, 19030, 309, 281, 915, 2098, 295, 14778, 278, 51276], "temperature": 0.0, "avg_logprob": -0.2353733654679923, "compression_ratio": 1.7377049180327868, "no_speech_prob": 3.747992138869449e-07}, {"id": 504, "seek": 248558, "start": 2503.8199999999997, "end": 2509.2599999999998, "text": " the information intelligently as it goes down.", "tokens": [51276, 264, 1589, 5613, 2276, 382, 309, 1709, 760, 13, 51548], "temperature": 0.0, "avg_logprob": -0.2353733654679923, "compression_ratio": 1.7377049180327868, "no_speech_prob": 3.747992138869449e-07}, {"id": 505, "seek": 248558, "start": 2509.2599999999998, "end": 2514.06, "text": " Also, we kind of want to be having a roughly similar amount of compute,", "tokens": [51548, 2743, 11, 321, 733, 295, 528, 281, 312, 1419, 257, 9810, 2531, 2372, 295, 14722, 11, 51788], "temperature": 0.0, "avg_logprob": -0.2353733654679923, "compression_ratio": 1.7377049180327868, "no_speech_prob": 3.747992138869449e-07}, {"id": 506, "seek": 251406, "start": 2514.06, "end": 2517.82, "text": " roughly similar amount, through the neural net.", "tokens": [50364, 9810, 2531, 2372, 11, 807, 264, 18161, 2533, 13, 50552], "temperature": 0.0, "avg_logprob": -0.2831076525766915, "compression_ratio": 1.8846153846153846, "no_speech_prob": 1.0348530850023963e-06}, {"id": 507, "seek": 251406, "start": 2517.82, "end": 2521.7, "text": " So as we decrease the grid size, we can add more channels.", "tokens": [50552, 407, 382, 321, 11514, 264, 10748, 2744, 11, 321, 393, 909, 544, 9235, 13, 50746], "temperature": 0.0, "avg_logprob": -0.2831076525766915, "compression_ratio": 1.8846153846153846, "no_speech_prob": 1.0348530850023963e-06}, {"id": 508, "seek": 251406, "start": 2522.94, "end": 2525.9, "text": " Because decreasing the grid size decreases the amount of compute.", "tokens": [50808, 1436, 23223, 264, 10748, 2744, 24108, 264, 2372, 295, 14722, 13, 50956], "temperature": 0.0, "avg_logprob": -0.2831076525766915, "compression_ratio": 1.8846153846153846, "no_speech_prob": 1.0348530850023963e-06}, {"id": 509, "seek": 251406, "start": 2525.9, "end": 2529.14, "text": " Increasing the channels then gives it more things to compute.", "tokens": [50956, 30367, 3349, 264, 9235, 550, 2709, 309, 544, 721, 281, 14722, 13, 51118], "temperature": 0.0, "avg_logprob": -0.2831076525766915, "compression_ratio": 1.8846153846153846, "no_speech_prob": 1.0348530850023963e-06}, {"id": 510, "seek": 251406, "start": 2529.14, "end": 2533.9, "text": " So we're kind of getting this nice compromise between, yeah,", "tokens": [51118, 407, 321, 434, 733, 295, 1242, 341, 1481, 18577, 1296, 11, 1338, 11, 51356], "temperature": 0.0, "avg_logprob": -0.2831076525766915, "compression_ratio": 1.8846153846153846, "no_speech_prob": 1.0348530850023963e-06}, {"id": 511, "seek": 251406, "start": 2533.9, "end": 2537.06, "text": " between the kind of amount of compute that it's doing, but", "tokens": [51356, 1296, 264, 733, 295, 2372, 295, 14722, 300, 309, 311, 884, 11, 457, 51514], "temperature": 0.0, "avg_logprob": -0.2831076525766915, "compression_ratio": 1.8846153846153846, "no_speech_prob": 1.0348530850023963e-06}, {"id": 512, "seek": 251406, "start": 2537.06, "end": 2540.06, "text": " also giving it some kind of compression work to do.", "tokens": [51514, 611, 2902, 309, 512, 733, 295, 19355, 589, 281, 360, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2831076525766915, "compression_ratio": 1.8846153846153846, "no_speech_prob": 1.0348530850023963e-06}, {"id": 513, "seek": 251406, "start": 2541.34, "end": 2542.86, "text": " That's the kind of the basic idea.", "tokens": [51728, 663, 311, 264, 733, 295, 264, 3875, 1558, 13, 51804], "temperature": 0.0, "avg_logprob": -0.2831076525766915, "compression_ratio": 1.8846153846153846, "no_speech_prob": 1.0348530850023963e-06}, {"id": 514, "seek": 254406, "start": 2545.06, "end": 2555.06, "text": " Well, still not able to train.", "tokens": [50414, 1042, 11, 920, 406, 1075, 281, 3847, 13, 50914], "temperature": 0.0, "avg_logprob": -0.3078221215142144, "compression_ratio": 1.458100558659218, "no_speech_prob": 7.527989055233775e-06}, {"id": 515, "seek": 254406, "start": 2555.06, "end": 2556.98, "text": " Well, okay, if we leave it for a while.", "tokens": [50914, 1042, 11, 1392, 11, 498, 321, 1856, 309, 337, 257, 1339, 13, 51010], "temperature": 0.0, "avg_logprob": -0.3078221215142144, "compression_ratio": 1.458100558659218, "no_speech_prob": 7.527989055233775e-06}, {"id": 516, "seek": 254406, "start": 2558.98, "end": 2561.18, "text": " Okay, it's not great, but it is actually starting to train.", "tokens": [51110, 1033, 11, 309, 311, 406, 869, 11, 457, 309, 307, 767, 2891, 281, 3847, 13, 51220], "temperature": 0.0, "avg_logprob": -0.3078221215142144, "compression_ratio": 1.458100558659218, "no_speech_prob": 7.527989055233775e-06}, {"id": 517, "seek": 254406, "start": 2561.18, "end": 2562.02, "text": " That's encouraging.", "tokens": [51220, 663, 311, 14580, 13, 51262], "temperature": 0.0, "avg_logprob": -0.3078221215142144, "compression_ratio": 1.458100558659218, "no_speech_prob": 7.527989055233775e-06}, {"id": 518, "seek": 254406, "start": 2563.74, "end": 2565.82, "text": " And we got up to a 77% accuracy.", "tokens": [51348, 400, 321, 658, 493, 281, 257, 25546, 4, 14170, 13, 51452], "temperature": 0.0, "avg_logprob": -0.3078221215142144, "compression_ratio": 1.458100558659218, "no_speech_prob": 7.527989055233775e-06}, {"id": 519, "seek": 254406, "start": 2565.82, "end": 2572.86, "text": " So we can see, yeah, not surprisingly, we're getting these spikes and spikes.", "tokens": [51452, 407, 321, 393, 536, 11, 1338, 11, 406, 17600, 11, 321, 434, 1242, 613, 28997, 293, 28997, 13, 51804], "temperature": 0.0, "avg_logprob": -0.3078221215142144, "compression_ratio": 1.458100558659218, "no_speech_prob": 7.527989055233775e-06}, {"id": 520, "seek": 257286, "start": 2572.86, "end": 2578.82, "text": " And so in the statistics, you can see that, well, it didn't quite work.", "tokens": [50364, 400, 370, 294, 264, 12523, 11, 291, 393, 536, 300, 11, 731, 11, 309, 994, 380, 1596, 589, 13, 50662], "temperature": 0.0, "avg_logprob": -0.3001724943822744, "compression_ratio": 1.5588235294117647, "no_speech_prob": 3.905481662513921e-06}, {"id": 521, "seek": 257286, "start": 2578.82, "end": 2580.6200000000003, "text": " We don't have a mean of 0.", "tokens": [50662, 492, 500, 380, 362, 257, 914, 295, 1958, 13, 50752], "temperature": 0.0, "avg_logprob": -0.3001724943822744, "compression_ratio": 1.5588235294117647, "no_speech_prob": 3.905481662513921e-06}, {"id": 522, "seek": 257286, "start": 2580.6200000000003, "end": 2583.38, "text": " We don't have a standard deviation of 1, even at the start.", "tokens": [50752, 492, 500, 380, 362, 257, 3832, 25163, 295, 502, 11, 754, 412, 264, 722, 13, 50890], "temperature": 0.0, "avg_logprob": -0.3001724943822744, "compression_ratio": 1.5588235294117647, "no_speech_prob": 3.905481662513921e-06}, {"id": 523, "seek": 257286, "start": 2584.98, "end": 2585.6600000000003, "text": " Why is that?", "tokens": [50970, 1545, 307, 300, 30, 51004], "temperature": 0.0, "avg_logprob": -0.3001724943822744, "compression_ratio": 1.5588235294117647, "no_speech_prob": 3.905481662513921e-06}, {"id": 524, "seek": 257286, "start": 2587.98, "end": 2591.98, "text": " Well, it's because we forgot something critical.", "tokens": [51120, 1042, 11, 309, 311, 570, 321, 5298, 746, 4924, 13, 51320], "temperature": 0.0, "avg_logprob": -0.3001724943822744, "compression_ratio": 1.5588235294117647, "no_speech_prob": 3.905481662513921e-06}, {"id": 525, "seek": 257286, "start": 2591.98, "end": 2597.1, "text": " If you go back to our original point, even when we had our,", "tokens": [51320, 759, 291, 352, 646, 281, 527, 3380, 935, 11, 754, 562, 321, 632, 527, 11, 51576], "temperature": 0.0, "avg_logprob": -0.3001724943822744, "compression_ratio": 1.5588235294117647, "no_speech_prob": 3.905481662513921e-06}, {"id": 526, "seek": 257286, "start": 2597.1, "end": 2598.9, "text": " well, let's go to the timing version.", "tokens": [51576, 731, 11, 718, 311, 352, 281, 264, 10822, 3037, 13, 51666], "temperature": 0.0, "avg_logprob": -0.3001724943822744, "compression_ratio": 1.5588235294117647, "no_speech_prob": 3.905481662513921e-06}, {"id": 527, "seek": 259890, "start": 2599.86, "end": 2605.1800000000003, "text": " Even when we had the correctly normalized matrix that we're multiplying by,", "tokens": [50412, 2754, 562, 321, 632, 264, 8944, 48704, 8141, 300, 321, 434, 30955, 538, 11, 50678], "temperature": 0.0, "avg_logprob": -0.3116365392157372, "compression_ratio": 1.6462264150943395, "no_speech_prob": 5.715004363082699e-07}, {"id": 528, "seek": 259890, "start": 2605.1800000000003, "end": 2609.94, "text": " well, you also have to have a correctly normalized input matrix.", "tokens": [50678, 731, 11, 291, 611, 362, 281, 362, 257, 8944, 48704, 4846, 8141, 13, 50916], "temperature": 0.0, "avg_logprob": -0.3116365392157372, "compression_ratio": 1.6462264150943395, "no_speech_prob": 5.715004363082699e-07}, {"id": 529, "seek": 259890, "start": 2609.94, "end": 2613.42, "text": " And we never did anything to normalize our inputs.", "tokens": [50916, 400, 321, 1128, 630, 1340, 281, 2710, 1125, 527, 15743, 13, 51090], "temperature": 0.0, "avg_logprob": -0.3116365392157372, "compression_ratio": 1.6462264150943395, "no_speech_prob": 5.715004363082699e-07}, {"id": 530, "seek": 259890, "start": 2613.42, "end": 2620.46, "text": " So our inputs actually, if we just get the first x mini-batch,", "tokens": [51090, 407, 527, 15743, 767, 11, 498, 321, 445, 483, 264, 700, 2031, 8382, 12, 65, 852, 11, 51442], "temperature": 0.0, "avg_logprob": -0.3116365392157372, "compression_ratio": 1.6462264150943395, "no_speech_prob": 5.715004363082699e-07}, {"id": 531, "seek": 259890, "start": 2620.46, "end": 2622.06, "text": " I get its mean and standard deviation.", "tokens": [51442, 286, 483, 1080, 914, 293, 3832, 25163, 13, 51522], "temperature": 0.0, "avg_logprob": -0.3116365392157372, "compression_ratio": 1.6462264150943395, "no_speech_prob": 5.715004363082699e-07}, {"id": 532, "seek": 259890, "start": 2622.06, "end": 2626.58, "text": " It has a mean of 0.28 and a standard deviation of 0.35.", "tokens": [51522, 467, 575, 257, 914, 295, 1958, 13, 11205, 293, 257, 3832, 25163, 295, 1958, 13, 8794, 13, 51748], "temperature": 0.0, "avg_logprob": -0.3116365392157372, "compression_ratio": 1.6462264150943395, "no_speech_prob": 5.715004363082699e-07}, {"id": 533, "seek": 262658, "start": 2626.58, "end": 2631.9, "text": " So we actually didn't even start with a 0, 1 input.", "tokens": [50364, 407, 321, 767, 994, 380, 754, 722, 365, 257, 1958, 11, 502, 4846, 13, 50630], "temperature": 0.0, "avg_logprob": -0.26476216870684954, "compression_ratio": 1.5126903553299493, "no_speech_prob": 2.785270964977826e-07}, {"id": 534, "seek": 262658, "start": 2633.22, "end": 2641.2999999999997, "text": " And so we started with a mean above 0 and a standard deviation beneath 1.", "tokens": [50696, 400, 370, 321, 1409, 365, 257, 914, 3673, 1958, 293, 257, 3832, 25163, 17149, 502, 13, 51100], "temperature": 0.0, "avg_logprob": -0.26476216870684954, "compression_ratio": 1.5126903553299493, "no_speech_prob": 2.785270964977826e-07}, {"id": 535, "seek": 262658, "start": 2641.2999999999997, "end": 2644.46, "text": " So that was very hard for it.", "tokens": [51100, 407, 300, 390, 588, 1152, 337, 309, 13, 51258], "temperature": 0.0, "avg_logprob": -0.26476216870684954, "compression_ratio": 1.5126903553299493, "no_speech_prob": 2.785270964977826e-07}, {"id": 536, "seek": 262658, "start": 2644.46, "end": 2648.58, "text": " So using the init helped, at least we're able to train a little bit.", "tokens": [51258, 407, 1228, 264, 3157, 4254, 11, 412, 1935, 321, 434, 1075, 281, 3847, 257, 707, 857, 13, 51464], "temperature": 0.0, "avg_logprob": -0.26476216870684954, "compression_ratio": 1.5126903553299493, "no_speech_prob": 2.785270964977826e-07}, {"id": 537, "seek": 262658, "start": 2648.58, "end": 2650.5, "text": " But it's not quite what we want.", "tokens": [51464, 583, 309, 311, 406, 1596, 437, 321, 528, 13, 51560], "temperature": 0.0, "avg_logprob": -0.26476216870684954, "compression_ratio": 1.5126903553299493, "no_speech_prob": 2.785270964977826e-07}, {"id": 538, "seek": 262658, "start": 2650.5, "end": 2654.2999999999997, "text": " We actually need to modify our inputs so", "tokens": [51560, 492, 767, 643, 281, 16927, 527, 15743, 370, 51750], "temperature": 0.0, "avg_logprob": -0.26476216870684954, "compression_ratio": 1.5126903553299493, "no_speech_prob": 2.785270964977826e-07}, {"id": 539, "seek": 265430, "start": 2654.3, "end": 2658.94, "text": " they have a mean of 0 and a standard deviation of 1.", "tokens": [50364, 436, 362, 257, 914, 295, 1958, 293, 257, 3832, 25163, 295, 502, 13, 50596], "temperature": 0.0, "avg_logprob": -0.30354655307272205, "compression_ratio": 1.7783783783783784, "no_speech_prob": 7.811483442310418e-07}, {"id": 540, "seek": 265430, "start": 2660.5800000000004, "end": 2662.1400000000003, "text": " So we could create a callback to do that.", "tokens": [50678, 407, 321, 727, 1884, 257, 818, 3207, 281, 360, 300, 13, 50756], "temperature": 0.0, "avg_logprob": -0.30354655307272205, "compression_ratio": 1.7783783783783784, "no_speech_prob": 7.811483442310418e-07}, {"id": 541, "seek": 265430, "start": 2664.26, "end": 2667.0600000000004, "text": " So a callback, let's create a batch transform callback.", "tokens": [50862, 407, 257, 818, 3207, 11, 718, 311, 1884, 257, 15245, 4088, 818, 3207, 13, 51002], "temperature": 0.0, "avg_logprob": -0.30354655307272205, "compression_ratio": 1.7783783783783784, "no_speech_prob": 7.811483442310418e-07}, {"id": 542, "seek": 265430, "start": 2667.0600000000004, "end": 2670.02, "text": " And so we're gonna pass in a function that's gonna transform every batch.", "tokens": [51002, 400, 370, 321, 434, 799, 1320, 294, 257, 2445, 300, 311, 799, 4088, 633, 15245, 13, 51150], "temperature": 0.0, "avg_logprob": -0.30354655307272205, "compression_ratio": 1.7783783783783784, "no_speech_prob": 7.811483442310418e-07}, {"id": 543, "seek": 265430, "start": 2671.86, "end": 2675.38, "text": " And so just in the before batch, we will set", "tokens": [51242, 400, 370, 445, 294, 264, 949, 15245, 11, 321, 486, 992, 51418], "temperature": 0.0, "avg_logprob": -0.30354655307272205, "compression_ratio": 1.7783783783783784, "no_speech_prob": 7.811483442310418e-07}, {"id": 544, "seek": 265430, "start": 2676.78, "end": 2681.5800000000004, "text": " the batch to be equal to the function applied to the batch.", "tokens": [51488, 264, 15245, 281, 312, 2681, 281, 264, 2445, 6456, 281, 264, 15245, 13, 51728], "temperature": 0.0, "avg_logprob": -0.30354655307272205, "compression_ratio": 1.7783783783783784, "no_speech_prob": 7.811483442310418e-07}, {"id": 545, "seek": 268158, "start": 2682.5, "end": 2685.98, "text": " Now I can, note by the way, we don't need self.learn.batch here.", "tokens": [50410, 823, 286, 393, 11, 3637, 538, 264, 636, 11, 321, 500, 380, 643, 2698, 13, 306, 1083, 13, 65, 852, 510, 13, 50584], "temperature": 0.0, "avg_logprob": -0.35924031441671805, "compression_ratio": 1.5702127659574467, "no_speech_prob": 1.451046955480706e-05}, {"id": 546, "seek": 268158, "start": 2687.54, "end": 2691.54, "text": " Because we can read any, cuz it's one of the four things that we", "tokens": [50662, 1436, 321, 393, 1401, 604, 11, 11910, 309, 311, 472, 295, 264, 1451, 721, 300, 321, 50862], "temperature": 0.0, "avg_logprob": -0.35924031441671805, "compression_ratio": 1.5702127659574467, "no_speech_prob": 1.451046955480706e-05}, {"id": 547, "seek": 268158, "start": 2692.54, "end": 2695.02, "text": " kind of proxy down to the learner automatically.", "tokens": [50912, 733, 295, 29690, 760, 281, 264, 33347, 6772, 13, 51036], "temperature": 0.0, "avg_logprob": -0.35924031441671805, "compression_ratio": 1.5702127659574467, "no_speech_prob": 1.451046955480706e-05}, {"id": 548, "seek": 268158, "start": 2695.02, "end": 2697.46, "text": " But we do need it on the left hand side.", "tokens": [51036, 583, 321, 360, 643, 309, 322, 264, 1411, 1011, 1252, 13, 51158], "temperature": 0.0, "avg_logprob": -0.35924031441671805, "compression_ratio": 1.5702127659574467, "no_speech_prob": 1.451046955480706e-05}, {"id": 549, "seek": 268158, "start": 2697.46, "end": 2699.38, "text": " Cuz it's only in the getAttra, remember.", "tokens": [51158, 27017, 309, 311, 787, 294, 264, 483, 18684, 17227, 11, 1604, 13, 51254], "temperature": 0.0, "avg_logprob": -0.35924031441671805, "compression_ratio": 1.5702127659574467, "no_speech_prob": 1.451046955480706e-05}, {"id": 550, "seek": 268158, "start": 2700.46, "end": 2701.7, "text": " So be very careful.", "tokens": [51308, 407, 312, 588, 5026, 13, 51370], "temperature": 0.0, "avg_logprob": -0.35924031441671805, "compression_ratio": 1.5702127659574467, "no_speech_prob": 1.451046955480706e-05}, {"id": 551, "seek": 268158, "start": 2701.7, "end": 2704.38, "text": " So I might just leave it the same on both sides,", "tokens": [51370, 407, 286, 1062, 445, 1856, 309, 264, 912, 322, 1293, 4881, 11, 51504], "temperature": 0.0, "avg_logprob": -0.35924031441671805, "compression_ratio": 1.5702127659574467, "no_speech_prob": 1.451046955480706e-05}, {"id": 552, "seek": 268158, "start": 2704.38, "end": 2705.9, "text": " just so that people don't get confused.", "tokens": [51504, 445, 370, 300, 561, 500, 380, 483, 9019, 13, 51580], "temperature": 0.0, "avg_logprob": -0.35924031441671805, "compression_ratio": 1.5702127659574467, "no_speech_prob": 1.451046955480706e-05}, {"id": 553, "seek": 270590, "start": 2706.42, "end": 2709.7400000000002, "text": " Okay, so let's create a function,", "tokens": [50390, 1033, 11, 370, 718, 311, 1884, 257, 2445, 11, 50556], "temperature": 0.0, "avg_logprob": -0.31371601422627765, "compression_ratio": 1.7819148936170213, "no_speech_prob": 2.225281377832289e-06}, {"id": 554, "seek": 270590, "start": 2709.7400000000002, "end": 2713.7000000000003, "text": " underscore norm, that subtracts the mean and", "tokens": [50556, 37556, 2026, 11, 300, 16390, 82, 264, 914, 293, 50754], "temperature": 0.0, "avg_logprob": -0.31371601422627765, "compression_ratio": 1.7819148936170213, "no_speech_prob": 2.225281377832289e-06}, {"id": 555, "seek": 270590, "start": 2713.7000000000003, "end": 2716.42, "text": " divides by the standard deviation.", "tokens": [50754, 41347, 538, 264, 3832, 25163, 13, 50890], "temperature": 0.0, "avg_logprob": -0.31371601422627765, "compression_ratio": 1.7819148936170213, "no_speech_prob": 2.225281377832289e-06}, {"id": 556, "seek": 270590, "start": 2716.42, "end": 2720.06, "text": " And so remember a batch has an x and a y.", "tokens": [50890, 400, 370, 1604, 257, 15245, 575, 364, 2031, 293, 257, 288, 13, 51072], "temperature": 0.0, "avg_logprob": -0.31371601422627765, "compression_ratio": 1.7819148936170213, "no_speech_prob": 2.225281377832289e-06}, {"id": 557, "seek": 270590, "start": 2720.06, "end": 2722.82, "text": " So it's the x part where we subtract the mean and", "tokens": [51072, 407, 309, 311, 264, 2031, 644, 689, 321, 16390, 264, 914, 293, 51210], "temperature": 0.0, "avg_logprob": -0.31371601422627765, "compression_ratio": 1.7819148936170213, "no_speech_prob": 2.225281377832289e-06}, {"id": 558, "seek": 270590, "start": 2722.82, "end": 2724.7400000000002, "text": " divide by the standard deviation.", "tokens": [51210, 9845, 538, 264, 3832, 25163, 13, 51306], "temperature": 0.0, "avg_logprob": -0.31371601422627765, "compression_ratio": 1.7819148936170213, "no_speech_prob": 2.225281377832289e-06}, {"id": 559, "seek": 270590, "start": 2724.7400000000002, "end": 2728.34, "text": " And so the new batch will be that as the x and", "tokens": [51306, 400, 370, 264, 777, 15245, 486, 312, 300, 382, 264, 2031, 293, 51486], "temperature": 0.0, "avg_logprob": -0.31371601422627765, "compression_ratio": 1.7819148936170213, "no_speech_prob": 2.225281377832289e-06}, {"id": 560, "seek": 270590, "start": 2728.34, "end": 2730.3, "text": " the y will be exactly the same as it was before.", "tokens": [51486, 264, 288, 486, 312, 2293, 264, 912, 382, 309, 390, 949, 13, 51584], "temperature": 0.0, "avg_logprob": -0.31371601422627765, "compression_ratio": 1.7819148936170213, "no_speech_prob": 2.225281377832289e-06}, {"id": 561, "seek": 273030, "start": 2731.3, "end": 2735.42, "text": " So let's create a instance of the normalization of the batch transform", "tokens": [50414, 407, 718, 311, 1884, 257, 5197, 295, 264, 2710, 2144, 295, 264, 15245, 4088, 50620], "temperature": 0.0, "avg_logprob": -0.3959968921750091, "compression_ratio": 1.6134020618556701, "no_speech_prob": 4.785094461112749e-06}, {"id": 562, "seek": 273030, "start": 2735.42, "end": 2738.78, "text": " callback, which is gonna do the normalization function.", "tokens": [50620, 818, 3207, 11, 597, 307, 799, 360, 264, 2710, 2144, 2445, 13, 50788], "temperature": 0.0, "avg_logprob": -0.3959968921750091, "compression_ratio": 1.6134020618556701, "no_speech_prob": 4.785094461112749e-06}, {"id": 563, "seek": 273030, "start": 2738.78, "end": 2740.6200000000003, "text": " And we'll call it norm.", "tokens": [50788, 400, 321, 603, 818, 309, 2026, 13, 50880], "temperature": 0.0, "avg_logprob": -0.3959968921750091, "compression_ratio": 1.6134020618556701, "no_speech_prob": 4.785094461112749e-06}, {"id": 564, "seek": 273030, "start": 2740.6200000000003, "end": 2744.9, "text": " So we can pass that as an additional callback to our learner.", "tokens": [50880, 407, 321, 393, 1320, 300, 382, 364, 4497, 818, 3207, 281, 527, 33347, 13, 51094], "temperature": 0.0, "avg_logprob": -0.3959968921750091, "compression_ratio": 1.6134020618556701, "no_speech_prob": 4.785094461112749e-06}, {"id": 565, "seek": 273030, "start": 2748.5, "end": 2752.38, "text": " And now, that's looking a lot better.", "tokens": [51274, 400, 586, 11, 300, 311, 1237, 257, 688, 1101, 13, 51468], "temperature": 0.0, "avg_logprob": -0.3959968921750091, "compression_ratio": 1.6134020618556701, "no_speech_prob": 4.785094461112749e-06}, {"id": 566, "seek": 273030, "start": 2752.38, "end": 2758.94, "text": " So you can see here, all we had to do was check that our input", "tokens": [51468, 407, 291, 393, 536, 510, 11, 439, 321, 632, 281, 360, 390, 1520, 300, 527, 4846, 51796], "temperature": 0.0, "avg_logprob": -0.3959968921750091, "compression_ratio": 1.6134020618556701, "no_speech_prob": 4.785094461112749e-06}, {"id": 567, "seek": 276030, "start": 2761.1000000000004, "end": 2766.46, "text": " matrix was 0, 1, mean standard deviation.", "tokens": [50404, 8141, 390, 1958, 11, 502, 11, 914, 3832, 25163, 13, 50672], "temperature": 0.0, "avg_logprob": -0.29848076502482096, "compression_ratio": 1.6778242677824269, "no_speech_prob": 7.527986326749669e-06}, {"id": 568, "seek": 276030, "start": 2766.46, "end": 2769.6600000000003, "text": " And all of our weight matrices was 0, 1 standard deviation.", "tokens": [50672, 400, 439, 295, 527, 3364, 32284, 390, 1958, 11, 502, 3832, 25163, 13, 50832], "temperature": 0.0, "avg_logprob": -0.29848076502482096, "compression_ratio": 1.6778242677824269, "no_speech_prob": 7.527986326749669e-06}, {"id": 569, "seek": 276030, "start": 2769.6600000000003, "end": 2772.5800000000004, "text": " And we didn't have to use any tricks at all.", "tokens": [50832, 400, 321, 994, 380, 362, 281, 764, 604, 11733, 412, 439, 13, 50978], "temperature": 0.0, "avg_logprob": -0.29848076502482096, "compression_ratio": 1.6778242677824269, "no_speech_prob": 7.527986326749669e-06}, {"id": 570, "seek": 276030, "start": 2772.5800000000004, "end": 2777.26, "text": " It was able to train and got it to an accuracy of 85%.", "tokens": [50978, 467, 390, 1075, 281, 3847, 293, 658, 309, 281, 364, 14170, 295, 14695, 6856, 51212], "temperature": 0.0, "avg_logprob": -0.29848076502482096, "compression_ratio": 1.6778242677824269, "no_speech_prob": 7.527986326749669e-06}, {"id": 571, "seek": 276030, "start": 2777.26, "end": 2782.82, "text": " And so if we look at the color dim and stats, look at this, it looks beautiful.", "tokens": [51212, 400, 370, 498, 321, 574, 412, 264, 2017, 5013, 293, 18152, 11, 574, 412, 341, 11, 309, 1542, 2238, 13, 51490], "temperature": 0.0, "avg_logprob": -0.29848076502482096, "compression_ratio": 1.6778242677824269, "no_speech_prob": 7.527986326749669e-06}, {"id": 572, "seek": 276030, "start": 2782.82, "end": 2787.02, "text": " Now, this is layer one, this is layer two, three, four.", "tokens": [51490, 823, 11, 341, 307, 4583, 472, 11, 341, 307, 4583, 732, 11, 1045, 11, 1451, 13, 51700], "temperature": 0.0, "avg_logprob": -0.29848076502482096, "compression_ratio": 1.6778242677824269, "no_speech_prob": 7.527986326749669e-06}, {"id": 573, "seek": 276030, "start": 2787.02, "end": 2788.2200000000003, "text": " It's still not perfect.", "tokens": [51700, 467, 311, 920, 406, 2176, 13, 51760], "temperature": 0.0, "avg_logprob": -0.29848076502482096, "compression_ratio": 1.6778242677824269, "no_speech_prob": 7.527986326749669e-06}, {"id": 574, "seek": 276030, "start": 2788.2200000000003, "end": 2789.82, "text": " I mean, there's some randomness, right?", "tokens": [51760, 286, 914, 11, 456, 311, 512, 4974, 1287, 11, 558, 30, 51840], "temperature": 0.0, "avg_logprob": -0.29848076502482096, "compression_ratio": 1.6778242677824269, "no_speech_prob": 7.527986326749669e-06}, {"id": 575, "seek": 278982, "start": 2790.34, "end": 2793.2200000000003, "text": " And we've got, what is it, like seven or eight layers.", "tokens": [50390, 400, 321, 600, 658, 11, 437, 307, 309, 11, 411, 3407, 420, 3180, 7914, 13, 50534], "temperature": 0.0, "avg_logprob": -0.3292727279663086, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.1233781808405183e-06}, {"id": 576, "seek": 278982, "start": 2793.2200000000003, "end": 2799.1800000000003, "text": " So that randomness does kind of, as you go through the layers,", "tokens": [50534, 407, 300, 4974, 1287, 775, 733, 295, 11, 382, 291, 352, 807, 264, 7914, 11, 50832], "temperature": 0.0, "avg_logprob": -0.3292727279663086, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.1233781808405183e-06}, {"id": 577, "seek": 278982, "start": 2799.1800000000003, "end": 2803.6200000000003, "text": " by the last one, it still gets a bit ugly.", "tokens": [50832, 538, 264, 1036, 472, 11, 309, 920, 2170, 257, 857, 12246, 13, 51054], "temperature": 0.0, "avg_logprob": -0.3292727279663086, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.1233781808405183e-06}, {"id": 578, "seek": 278982, "start": 2803.6200000000003, "end": 2806.54, "text": " And you can kind of see it bouncing around here as a result.", "tokens": [51054, 400, 291, 393, 733, 295, 536, 309, 27380, 926, 510, 382, 257, 1874, 13, 51200], "temperature": 0.0, "avg_logprob": -0.3292727279663086, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.1233781808405183e-06}, {"id": 579, "seek": 278982, "start": 2808.42, "end": 2812.2200000000003, "text": " And you can see that also in the means and standard deviations.", "tokens": [51294, 400, 291, 393, 536, 300, 611, 294, 264, 1355, 293, 3832, 31219, 763, 13, 51484], "temperature": 0.0, "avg_logprob": -0.3292727279663086, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.1233781808405183e-06}, {"id": 580, "seek": 278982, "start": 2814.2200000000003, "end": 2816.6200000000003, "text": " There's some other reasons this is happening, we'll see in a moment.", "tokens": [51584, 821, 311, 512, 661, 4112, 341, 307, 2737, 11, 321, 603, 536, 294, 257, 1623, 13, 51704], "temperature": 0.0, "avg_logprob": -0.3292727279663086, "compression_ratio": 1.6018099547511313, "no_speech_prob": 2.1233781808405183e-06}, {"id": 581, "seek": 281662, "start": 2816.62, "end": 2821.66, "text": " But this is the first time we've really got our even somewhat deep", "tokens": [50364, 583, 341, 307, 264, 700, 565, 321, 600, 534, 658, 527, 754, 8344, 2452, 50616], "temperature": 0.0, "avg_logprob": -0.30544195419702774, "compression_ratio": 1.4901960784313726, "no_speech_prob": 9.570828751748195e-07}, {"id": 582, "seek": 281662, "start": 2821.66, "end": 2822.94, "text": " convolutional model to train.", "tokens": [50616, 45216, 304, 2316, 281, 3847, 13, 50680], "temperature": 0.0, "avg_logprob": -0.30544195419702774, "compression_ratio": 1.4901960784313726, "no_speech_prob": 9.570828751748195e-07}, {"id": 583, "seek": 281662, "start": 2824.2599999999998, "end": 2826.14, "text": " And so this is a really exciting step.", "tokens": [50746, 400, 370, 341, 307, 257, 534, 4670, 1823, 13, 50840], "temperature": 0.0, "avg_logprob": -0.30544195419702774, "compression_ratio": 1.4901960784313726, "no_speech_prob": 9.570828751748195e-07}, {"id": 584, "seek": 281662, "start": 2826.14, "end": 2832.18, "text": " We have, from scratch, in a sequence of 11 notebooks,", "tokens": [50840, 492, 362, 11, 490, 8459, 11, 294, 257, 8310, 295, 2975, 43782, 11, 51142], "temperature": 0.0, "avg_logprob": -0.30544195419702774, "compression_ratio": 1.4901960784313726, "no_speech_prob": 9.570828751748195e-07}, {"id": 585, "seek": 281662, "start": 2832.18, "end": 2842.2599999999998, "text": " managed to create a real convolutional neural network that is training properly.", "tokens": [51142, 6453, 281, 1884, 257, 957, 45216, 304, 18161, 3209, 300, 307, 3097, 6108, 13, 51646], "temperature": 0.0, "avg_logprob": -0.30544195419702774, "compression_ratio": 1.4901960784313726, "no_speech_prob": 9.570828751748195e-07}, {"id": 586, "seek": 281662, "start": 2842.2599999999998, "end": 2843.58, "text": " So I think that's pretty amazing.", "tokens": [51646, 407, 286, 519, 300, 311, 1238, 2243, 13, 51712], "temperature": 0.0, "avg_logprob": -0.30544195419702774, "compression_ratio": 1.4901960784313726, "no_speech_prob": 9.570828751748195e-07}, {"id": 587, "seek": 284358, "start": 2844.2999999999997, "end": 2850.02, "text": " Now, we don't have to use a callback for this.", "tokens": [50400, 823, 11, 321, 500, 380, 362, 281, 764, 257, 818, 3207, 337, 341, 13, 50686], "temperature": 0.0, "avg_logprob": -0.29231692227450284, "compression_ratio": 1.6083333333333334, "no_speech_prob": 2.7694061373040313e-06}, {"id": 588, "seek": 284358, "start": 2850.02, "end": 2853.2599999999998, "text": " The other thing we could do to modify the input data, of course,", "tokens": [50686, 440, 661, 551, 321, 727, 360, 281, 16927, 264, 4846, 1412, 11, 295, 1164, 11, 50848], "temperature": 0.0, "avg_logprob": -0.29231692227450284, "compression_ratio": 1.6083333333333334, "no_speech_prob": 2.7694061373040313e-06}, {"id": 589, "seek": 284358, "start": 2853.2599999999998, "end": 2859.02, "text": " is to use the with transform method from the HuggingFaceDatasets library.", "tokens": [50848, 307, 281, 764, 264, 365, 4088, 3170, 490, 264, 46892, 3249, 37, 617, 35, 37892, 1385, 6405, 13, 51136], "temperature": 0.0, "avg_logprob": -0.29231692227450284, "compression_ratio": 1.6083333333333334, "no_speech_prob": 2.7694061373040313e-06}, {"id": 590, "seek": 284358, "start": 2859.02, "end": 2863.54, "text": " So we could modify our transform i to subtract the mean and", "tokens": [51136, 407, 321, 727, 16927, 527, 4088, 741, 281, 16390, 264, 914, 293, 51362], "temperature": 0.0, "avg_logprob": -0.29231692227450284, "compression_ratio": 1.6083333333333334, "no_speech_prob": 2.7694061373040313e-06}, {"id": 591, "seek": 284358, "start": 2863.54, "end": 2865.02, "text": " divide by the standard deviation.", "tokens": [51362, 9845, 538, 264, 3832, 25163, 13, 51436], "temperature": 0.0, "avg_logprob": -0.29231692227450284, "compression_ratio": 1.6083333333333334, "no_speech_prob": 2.7694061373040313e-06}, {"id": 592, "seek": 284358, "start": 2866.7, "end": 2868.86, "text": " And then recreate our data loaders.", "tokens": [51520, 400, 550, 25833, 527, 1412, 3677, 433, 13, 51628], "temperature": 0.0, "avg_logprob": -0.29231692227450284, "compression_ratio": 1.6083333333333334, "no_speech_prob": 2.7694061373040313e-06}, {"id": 593, "seek": 284358, "start": 2868.86, "end": 2873.46, "text": " And if we now get a batch out of that and check it, it's now got, yep,", "tokens": [51628, 400, 498, 321, 586, 483, 257, 15245, 484, 295, 300, 293, 1520, 309, 11, 309, 311, 586, 658, 11, 18633, 11, 51858], "temperature": 0.0, "avg_logprob": -0.29231692227450284, "compression_ratio": 1.6083333333333334, "no_speech_prob": 2.7694061373040313e-06}, {"id": 594, "seek": 287346, "start": 2874.34, "end": 2876.42, "text": " the mean is 0 and the standard deviation of 1.", "tokens": [50408, 264, 914, 307, 1958, 293, 264, 3832, 25163, 295, 502, 13, 50512], "temperature": 0.0, "avg_logprob": -0.2975760919076425, "compression_ratio": 1.5849802371541502, "no_speech_prob": 1.7330513628621702e-06}, {"id": 595, "seek": 287346, "start": 2876.42, "end": 2878.2200000000003, "text": " So we could also do it this way.", "tokens": [50512, 407, 321, 727, 611, 360, 309, 341, 636, 13, 50602], "temperature": 0.0, "avg_logprob": -0.2975760919076425, "compression_ratio": 1.5849802371541502, "no_speech_prob": 1.7330513628621702e-06}, {"id": 596, "seek": 287346, "start": 2878.2200000000003, "end": 2881.18, "text": " So generally speaking, for", "tokens": [50602, 407, 5101, 4124, 11, 337, 50750], "temperature": 0.0, "avg_logprob": -0.2975760919076425, "compression_ratio": 1.5849802371541502, "no_speech_prob": 1.7330513628621702e-06}, {"id": 597, "seek": 287346, "start": 2881.18, "end": 2886.7, "text": " stuff that needs to kind of dynamically modify the batch, you can often do it", "tokens": [50750, 1507, 300, 2203, 281, 733, 295, 43492, 16927, 264, 15245, 11, 291, 393, 2049, 360, 309, 51026], "temperature": 0.0, "avg_logprob": -0.2975760919076425, "compression_ratio": 1.5849802371541502, "no_speech_prob": 1.7330513628621702e-06}, {"id": 598, "seek": 287346, "start": 2886.7, "end": 2891.94, "text": " either in your data processing code or you can do it in a callback.", "tokens": [51026, 2139, 294, 428, 1412, 9007, 3089, 420, 291, 393, 360, 309, 294, 257, 818, 3207, 13, 51288], "temperature": 0.0, "avg_logprob": -0.2975760919076425, "compression_ratio": 1.5849802371541502, "no_speech_prob": 1.7330513628621702e-06}, {"id": 599, "seek": 287346, "start": 2891.94, "end": 2893.58, "text": " And neither's right or wrong.", "tokens": [51288, 400, 9662, 311, 558, 420, 2085, 13, 51370], "temperature": 0.0, "avg_logprob": -0.2975760919076425, "compression_ratio": 1.5849802371541502, "no_speech_prob": 1.7330513628621702e-06}, {"id": 600, "seek": 287346, "start": 2893.58, "end": 2897.14, "text": " They both work well, and you can see whichever one works best for you.", "tokens": [51370, 814, 1293, 589, 731, 11, 293, 291, 393, 536, 24123, 472, 1985, 1151, 337, 291, 13, 51548], "temperature": 0.0, "avg_logprob": -0.2975760919076425, "compression_ratio": 1.5849802371541502, "no_speech_prob": 1.7330513628621702e-06}, {"id": 601, "seek": 287346, "start": 2898.9, "end": 2901.3, "text": " Okay, now I'm gonna show you something amazing.", "tokens": [51636, 1033, 11, 586, 286, 478, 799, 855, 291, 746, 2243, 13, 51756], "temperature": 0.0, "avg_logprob": -0.2975760919076425, "compression_ratio": 1.5849802371541502, "no_speech_prob": 1.7330513628621702e-06}, {"id": 602, "seek": 290346, "start": 2904.46, "end": 2910.02, "text": " Okay, so it's great this is training well, but", "tokens": [50414, 1033, 11, 370, 309, 311, 869, 341, 307, 3097, 731, 11, 457, 50692], "temperature": 0.0, "avg_logprob": -0.4464522298177083, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.4285514225775842e-05}, {"id": 603, "seek": 290346, "start": 2910.02, "end": 2915.78, "text": " when you look at our stats, despite what we did with", "tokens": [50692, 562, 291, 574, 412, 527, 18152, 11, 7228, 437, 321, 630, 365, 50980], "temperature": 0.0, "avg_logprob": -0.4464522298177083, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.4285514225775842e-05}, {"id": 604, "seek": 290346, "start": 2915.78, "end": 2922.38, "text": " the normalized input and the normalized weight matrices,", "tokens": [50980, 264, 48704, 4846, 293, 264, 48704, 3364, 32284, 11, 51310], "temperature": 0.0, "avg_logprob": -0.4464522298177083, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.4285514225775842e-05}, {"id": 605, "seek": 290346, "start": 2922.38, "end": 2929.86, "text": " we don't have a mean of 0 and we don't have a standard deviation of 1.", "tokens": [51310, 321, 500, 380, 362, 257, 914, 295, 1958, 293, 321, 500, 380, 362, 257, 3832, 25163, 295, 502, 13, 51684], "temperature": 0.0, "avg_logprob": -0.4464522298177083, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.4285514225775842e-05}, {"id": 606, "seek": 290346, "start": 2929.86, "end": 2930.7, "text": " Even from the start.", "tokens": [51684, 2754, 490, 264, 722, 13, 51726], "temperature": 0.0, "avg_logprob": -0.4464522298177083, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.4285514225775842e-05}, {"id": 607, "seek": 290346, "start": 2932.3, "end": 2933.1, "text": " So why is that?", "tokens": [51806, 407, 983, 307, 300, 30, 51846], "temperature": 0.0, "avg_logprob": -0.4464522298177083, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.4285514225775842e-05}, {"id": 608, "seek": 293346, "start": 2934.46, "end": 2943.3, "text": " Well, the problem is that we were putting our data through a ReLU.", "tokens": [50414, 1042, 11, 264, 1154, 307, 300, 321, 645, 3372, 527, 1412, 807, 257, 1300, 43, 52, 13, 50856], "temperature": 0.0, "avg_logprob": -0.43240270018577576, "compression_ratio": 1.513157894736842, "no_speech_prob": 2.6995778057425923e-07}, {"id": 609, "seek": 293346, "start": 2944.42, "end": 2950.02, "text": " And our activation stats are looking at", "tokens": [50912, 400, 527, 24433, 18152, 366, 1237, 412, 51192], "temperature": 0.0, "avg_logprob": -0.43240270018577576, "compression_ratio": 1.513157894736842, "no_speech_prob": 2.6995778057425923e-07}, {"id": 610, "seek": 293346, "start": 2952.02, "end": 2955.34, "text": " the output of those ReLU blocks, cuz that's kind of the end of each.", "tokens": [51292, 264, 5598, 295, 729, 1300, 43, 52, 8474, 11, 11910, 300, 311, 733, 295, 264, 917, 295, 1184, 13, 51458], "temperature": 0.0, "avg_logprob": -0.43240270018577576, "compression_ratio": 1.513157894736842, "no_speech_prob": 2.6995778057425923e-07}, {"id": 611, "seek": 293346, "start": 2956.34, "end": 2960.62, "text": " You know, that's the activation of each combination of", "tokens": [51508, 509, 458, 11, 300, 311, 264, 24433, 295, 1184, 6562, 295, 51722], "temperature": 0.0, "avg_logprob": -0.43240270018577576, "compression_ratio": 1.513157894736842, "no_speech_prob": 2.6995778057425923e-07}, {"id": 612, "seek": 296062, "start": 2961.5, "end": 2965.14, "text": " matrix multiplication and activation function.", "tokens": [50408, 8141, 27290, 293, 24433, 2445, 13, 50590], "temperature": 0.0, "avg_logprob": -0.30461660184358297, "compression_ratio": 1.491891891891892, "no_speech_prob": 1.7603429114387836e-06}, {"id": 613, "seek": 296062, "start": 2965.14, "end": 2970.06, "text": " And since a ReLU removes all of the negative numbers,", "tokens": [50590, 400, 1670, 257, 1300, 43, 52, 30445, 439, 295, 264, 3671, 3547, 11, 50836], "temperature": 0.0, "avg_logprob": -0.30461660184358297, "compression_ratio": 1.491891891891892, "no_speech_prob": 1.7603429114387836e-06}, {"id": 614, "seek": 296062, "start": 2970.06, "end": 2975.8199999999997, "text": " it's impossible for the output of a ReLU to have a mean of 0,", "tokens": [50836, 309, 311, 6243, 337, 264, 5598, 295, 257, 1300, 43, 52, 281, 362, 257, 914, 295, 1958, 11, 51124], "temperature": 0.0, "avg_logprob": -0.30461660184358297, "compression_ratio": 1.491891891891892, "no_speech_prob": 1.7603429114387836e-06}, {"id": 615, "seek": 296062, "start": 2975.8199999999997, "end": 2978.54, "text": " unless literally every single number is 0.", "tokens": [51124, 5969, 3736, 633, 2167, 1230, 307, 1958, 13, 51260], "temperature": 0.0, "avg_logprob": -0.30461660184358297, "compression_ratio": 1.491891891891892, "no_speech_prob": 1.7603429114387836e-06}, {"id": 616, "seek": 296062, "start": 2978.54, "end": 2979.9, "text": " Perhaps it's got no negatives.", "tokens": [51260, 10517, 309, 311, 658, 572, 40019, 13, 51328], "temperature": 0.0, "avg_logprob": -0.30461660184358297, "compression_ratio": 1.491891891891892, "no_speech_prob": 1.7603429114387836e-06}, {"id": 617, "seek": 296062, "start": 2981.58, "end": 2987.74, "text": " So ReLU seems to me to be fundamentally", "tokens": [51412, 407, 1300, 43, 52, 2544, 281, 385, 281, 312, 17879, 51720], "temperature": 0.0, "avg_logprob": -0.30461660184358297, "compression_ratio": 1.491891891891892, "no_speech_prob": 1.7603429114387836e-06}, {"id": 618, "seek": 298774, "start": 2987.74, "end": 2992.9399999999996, "text": " incompatible with the idea of a correctly calibrated bunch of layers in", "tokens": [50364, 40393, 267, 964, 365, 264, 1558, 295, 257, 8944, 21583, 5468, 3840, 295, 7914, 294, 50624], "temperature": 0.0, "avg_logprob": -0.2888068452887579, "compression_ratio": 1.5791666666666666, "no_speech_prob": 4.006368908449076e-05}, {"id": 619, "seek": 298774, "start": 2992.9399999999996, "end": 2994.4199999999996, "text": " a neural net.", "tokens": [50624, 257, 18161, 2533, 13, 50698], "temperature": 0.0, "avg_logprob": -0.2888068452887579, "compression_ratio": 1.5791666666666666, "no_speech_prob": 4.006368908449076e-05}, {"id": 620, "seek": 298774, "start": 2994.4199999999996, "end": 3001.54, "text": " So I came up with this idea of saying, well, why don't we take our normal ReLU", "tokens": [50698, 407, 286, 1361, 493, 365, 341, 1558, 295, 1566, 11, 731, 11, 983, 500, 380, 321, 747, 527, 2710, 1300, 43, 52, 51054], "temperature": 0.0, "avg_logprob": -0.2888068452887579, "compression_ratio": 1.5791666666666666, "no_speech_prob": 4.006368908449076e-05}, {"id": 621, "seek": 298774, "start": 3002.7, "end": 3006.4599999999996, "text": " and have the ability to subtract something from it?", "tokens": [51112, 293, 362, 264, 3485, 281, 16390, 746, 490, 309, 30, 51300], "temperature": 0.0, "avg_logprob": -0.2888068452887579, "compression_ratio": 1.5791666666666666, "no_speech_prob": 4.006368908449076e-05}, {"id": 622, "seek": 298774, "start": 3006.4599999999996, "end": 3012.2999999999997, "text": " And so we just take the result of our ReLU and subtract, so sub minus.", "tokens": [51300, 400, 370, 321, 445, 747, 264, 1874, 295, 527, 1300, 43, 52, 293, 16390, 11, 370, 1422, 3175, 13, 51592], "temperature": 0.0, "avg_logprob": -0.2888068452887579, "compression_ratio": 1.5791666666666666, "no_speech_prob": 4.006368908449076e-05}, {"id": 623, "seek": 298774, "start": 3012.2999999999997, "end": 3014.74, "text": " I mean, I can write this in a more obvious way.", "tokens": [51592, 286, 914, 11, 286, 393, 2464, 341, 294, 257, 544, 6322, 636, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2888068452887579, "compression_ratio": 1.5791666666666666, "no_speech_prob": 4.006368908449076e-05}, {"id": 624, "seek": 298774, "start": 3014.74, "end": 3017.18, "text": " It's exactly the same as just minus equals.", "tokens": [51714, 467, 311, 2293, 264, 912, 382, 445, 3175, 6915, 13, 51836], "temperature": 0.0, "avg_logprob": -0.2888068452887579, "compression_ratio": 1.5791666666666666, "no_speech_prob": 4.006368908449076e-05}, {"id": 625, "seek": 301718, "start": 3017.18, "end": 3017.8999999999996, "text": " I'm gonna just do that.", "tokens": [50364, 286, 478, 799, 445, 360, 300, 13, 50400], "temperature": 0.0, "avg_logprob": -0.2693714509930527, "compression_ratio": 1.5897435897435896, "no_speech_prob": 2.11124679481145e-05}, {"id": 626, "seek": 301718, "start": 3019.54, "end": 3023.62, "text": " We'll subtract something from our ReLU.", "tokens": [50482, 492, 603, 16390, 746, 490, 527, 1300, 43, 52, 13, 50686], "temperature": 0.0, "avg_logprob": -0.2693714509930527, "compression_ratio": 1.5897435897435896, "no_speech_prob": 2.11124679481145e-05}, {"id": 627, "seek": 301718, "start": 3025.58, "end": 3028.58, "text": " That will allow us to pull the whole thing down so", "tokens": [50784, 663, 486, 2089, 505, 281, 2235, 264, 1379, 551, 760, 370, 50934], "temperature": 0.0, "avg_logprob": -0.2693714509930527, "compression_ratio": 1.5897435897435896, "no_speech_prob": 2.11124679481145e-05}, {"id": 628, "seek": 301718, "start": 3028.58, "end": 3035.18, "text": " that the bottom of our ReLU is underneath the x-axis, and it has negatives.", "tokens": [50934, 300, 264, 2767, 295, 527, 1300, 43, 52, 307, 7223, 264, 2031, 12, 24633, 11, 293, 309, 575, 40019, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2693714509930527, "compression_ratio": 1.5897435897435896, "no_speech_prob": 2.11124679481145e-05}, {"id": 629, "seek": 301718, "start": 3035.18, "end": 3038.02, "text": " And that would allow us to have a mean of 0.", "tokens": [51264, 400, 300, 576, 2089, 505, 281, 362, 257, 914, 295, 1958, 13, 51406], "temperature": 0.0, "avg_logprob": -0.2693714509930527, "compression_ratio": 1.5897435897435896, "no_speech_prob": 2.11124679481145e-05}, {"id": 630, "seek": 301718, "start": 3038.02, "end": 3041.7799999999997, "text": " And while we're there, let's also do something that's existed for a while.", "tokens": [51406, 400, 1339, 321, 434, 456, 11, 718, 311, 611, 360, 746, 300, 311, 13135, 337, 257, 1339, 13, 51594], "temperature": 0.0, "avg_logprob": -0.2693714509930527, "compression_ratio": 1.5897435897435896, "no_speech_prob": 2.11124679481145e-05}, {"id": 631, "seek": 301718, "start": 3041.7799999999997, "end": 3045.4199999999996, "text": " I didn't come up with this idea, which is to do a leaky ReLU,", "tokens": [51594, 286, 994, 380, 808, 493, 365, 341, 1558, 11, 597, 307, 281, 360, 257, 476, 15681, 1300, 43, 52, 11, 51776], "temperature": 0.0, "avg_logprob": -0.2693714509930527, "compression_ratio": 1.5897435897435896, "no_speech_prob": 2.11124679481145e-05}, {"id": 632, "seek": 304542, "start": 3045.42, "end": 3050.1800000000003, "text": " which is where we say, let's not have the negative speed totally flat,", "tokens": [50364, 597, 307, 689, 321, 584, 11, 718, 311, 406, 362, 264, 3671, 3073, 3879, 4962, 11, 50602], "temperature": 0.0, "avg_logprob": -0.20857740039667808, "compression_ratio": 1.6811023622047243, "no_speech_prob": 5.539171752388938e-07}, {"id": 633, "seek": 304542, "start": 3050.1800000000003, "end": 3051.42, "text": " just truncated.", "tokens": [50602, 445, 504, 409, 66, 770, 13, 50664], "temperature": 0.0, "avg_logprob": -0.20857740039667808, "compression_ratio": 1.6811023622047243, "no_speech_prob": 5.539171752388938e-07}, {"id": 634, "seek": 304542, "start": 3051.42, "end": 3057.06, "text": " But instead, let's just have those numbers decreased by some constant amount.", "tokens": [50664, 583, 2602, 11, 718, 311, 445, 362, 729, 3547, 24436, 538, 512, 5754, 2372, 13, 50946], "temperature": 0.0, "avg_logprob": -0.20857740039667808, "compression_ratio": 1.6811023622047243, "no_speech_prob": 5.539171752388938e-07}, {"id": 635, "seek": 304542, "start": 3058.82, "end": 3060.1800000000003, "text": " Let me show you what that looks like.", "tokens": [51034, 961, 385, 855, 291, 437, 300, 1542, 411, 13, 51102], "temperature": 0.0, "avg_logprob": -0.20857740039667808, "compression_ratio": 1.6811023622047243, "no_speech_prob": 5.539171752388938e-07}, {"id": 636, "seek": 304542, "start": 3060.1800000000003, "end": 3063.46, "text": " So those two together, I'm gonna call general ReLU,", "tokens": [51102, 407, 729, 732, 1214, 11, 286, 478, 799, 818, 2674, 1300, 43, 52, 11, 51266], "temperature": 0.0, "avg_logprob": -0.20857740039667808, "compression_ratio": 1.6811023622047243, "no_speech_prob": 5.539171752388938e-07}, {"id": 637, "seek": 304542, "start": 3063.46, "end": 3067.1, "text": " which is where we do this thing called leaky ReLU, which is where we make it so", "tokens": [51266, 597, 307, 689, 321, 360, 341, 551, 1219, 476, 15681, 1300, 43, 52, 11, 597, 307, 689, 321, 652, 309, 370, 51448], "temperature": 0.0, "avg_logprob": -0.20857740039667808, "compression_ratio": 1.6811023622047243, "no_speech_prob": 5.539171752388938e-07}, {"id": 638, "seek": 304542, "start": 3067.1, "end": 3072.02, "text": " it's not flat under 0, but instead just less sloped.", "tokens": [51448, 309, 311, 406, 4962, 833, 1958, 11, 457, 2602, 445, 1570, 21254, 292, 13, 51694], "temperature": 0.0, "avg_logprob": -0.20857740039667808, "compression_ratio": 1.6811023622047243, "no_speech_prob": 5.539171752388938e-07}, {"id": 639, "seek": 304542, "start": 3072.02, "end": 3074.86, "text": " And we also subtract something from it.", "tokens": [51694, 400, 321, 611, 16390, 746, 490, 309, 13, 51836], "temperature": 0.0, "avg_logprob": -0.20857740039667808, "compression_ratio": 1.6811023622047243, "no_speech_prob": 5.539171752388938e-07}, {"id": 640, "seek": 307486, "start": 3075.3, "end": 3079.94, "text": " For example, I've created a little function here for plotting a function.", "tokens": [50386, 1171, 1365, 11, 286, 600, 2942, 257, 707, 2445, 510, 337, 41178, 257, 2445, 13, 50618], "temperature": 0.0, "avg_logprob": -0.25170606530230977, "compression_ratio": 1.5252100840336134, "no_speech_prob": 7.85622091825644e-08}, {"id": 641, "seek": 307486, "start": 3079.94, "end": 3084.98, "text": " So let's plot the general ReLU function with a leakiness of 0.1.", "tokens": [50618, 407, 718, 311, 7542, 264, 2674, 1300, 43, 52, 2445, 365, 257, 17143, 1324, 295, 1958, 13, 16, 13, 50870], "temperature": 0.0, "avg_logprob": -0.25170606530230977, "compression_ratio": 1.5252100840336134, "no_speech_prob": 7.85622091825644e-08}, {"id": 642, "seek": 307486, "start": 3084.98, "end": 3091.06, "text": " So that will mean there's a 0.1 slope under 0, and we'll subtract 0.4.", "tokens": [50870, 407, 300, 486, 914, 456, 311, 257, 1958, 13, 16, 13525, 833, 1958, 11, 293, 321, 603, 16390, 1958, 13, 19, 13, 51174], "temperature": 0.0, "avg_logprob": -0.25170606530230977, "compression_ratio": 1.5252100840336134, "no_speech_prob": 7.85622091825644e-08}, {"id": 643, "seek": 307486, "start": 3092.7000000000003, "end": 3098.38, "text": " And so you can see, above 0, it's just a normal y equals x line,", "tokens": [51256, 400, 370, 291, 393, 536, 11, 3673, 1958, 11, 309, 311, 445, 257, 2710, 288, 6915, 2031, 1622, 11, 51540], "temperature": 0.0, "avg_logprob": -0.25170606530230977, "compression_ratio": 1.5252100840336134, "no_speech_prob": 7.85622091825644e-08}, {"id": 644, "seek": 307486, "start": 3098.38, "end": 3100.06, "text": " but it's been pushed down by 0.4.", "tokens": [51540, 457, 309, 311, 668, 9152, 760, 538, 1958, 13, 19, 13, 51624], "temperature": 0.0, "avg_logprob": -0.25170606530230977, "compression_ratio": 1.5252100840336134, "no_speech_prob": 7.85622091825644e-08}, {"id": 645, "seek": 307486, "start": 3101.38, "end": 3104.78, "text": " And then when it's less than 0, it's not flat anymore,", "tokens": [51690, 400, 550, 562, 309, 311, 1570, 813, 1958, 11, 309, 311, 406, 4962, 3602, 11, 51860], "temperature": 0.0, "avg_logprob": -0.25170606530230977, "compression_ratio": 1.5252100840336134, "no_speech_prob": 7.85622091825644e-08}, {"id": 646, "seek": 310478, "start": 3105.7000000000003, "end": 3108.7400000000002, "text": " but it's just got a slope of one-tenth.", "tokens": [50410, 457, 309, 311, 445, 658, 257, 13525, 295, 472, 12, 83, 17966, 13, 50562], "temperature": 0.0, "avg_logprob": -0.3118946576359296, "compression_ratio": 1.626086956521739, "no_speech_prob": 6.179393494676333e-07}, {"id": 647, "seek": 310478, "start": 3108.7400000000002, "end": 3115.86, "text": " And so this is now something which, if you find the right amount to subtract for", "tokens": [50562, 400, 370, 341, 307, 586, 746, 597, 11, 498, 291, 915, 264, 558, 2372, 281, 16390, 337, 50918], "temperature": 0.0, "avg_logprob": -0.3118946576359296, "compression_ratio": 1.626086956521739, "no_speech_prob": 6.179393494676333e-07}, {"id": 648, "seek": 310478, "start": 3115.86, "end": 3118.5400000000004, "text": " each amount of leakiness, you can make a mean of 0.", "tokens": [50918, 1184, 2372, 295, 17143, 1324, 11, 291, 393, 652, 257, 914, 295, 1958, 13, 51052], "temperature": 0.0, "avg_logprob": -0.3118946576359296, "compression_ratio": 1.626086956521739, "no_speech_prob": 6.179393494676333e-07}, {"id": 649, "seek": 310478, "start": 3118.5400000000004, "end": 3123.1400000000003, "text": " And I actually found that this particular combination gives us a mean of 0,", "tokens": [51052, 400, 286, 767, 1352, 300, 341, 1729, 6562, 2709, 505, 257, 914, 295, 1958, 11, 51282], "temperature": 0.0, "avg_logprob": -0.3118946576359296, "compression_ratio": 1.626086956521739, "no_speech_prob": 6.179393494676333e-07}, {"id": 650, "seek": 310478, "start": 3123.1400000000003, "end": 3123.98, "text": " or thereabouts.", "tokens": [51282, 420, 456, 41620, 13, 51324], "temperature": 0.0, "avg_logprob": -0.3118946576359296, "compression_ratio": 1.626086956521739, "no_speech_prob": 6.179393494676333e-07}, {"id": 651, "seek": 310478, "start": 3126.1800000000003, "end": 3131.86, "text": " So let's now create a new convolution function where we can actually", "tokens": [51434, 407, 718, 311, 586, 1884, 257, 777, 45216, 2445, 689, 321, 393, 767, 51718], "temperature": 0.0, "avg_logprob": -0.3118946576359296, "compression_ratio": 1.626086956521739, "no_speech_prob": 6.179393494676333e-07}, {"id": 652, "seek": 310478, "start": 3131.86, "end": 3134.26, "text": " change what activation function is used.", "tokens": [51718, 1319, 437, 24433, 2445, 307, 1143, 13, 51838], "temperature": 0.0, "avg_logprob": -0.3118946576359296, "compression_ratio": 1.626086956521739, "no_speech_prob": 6.179393494676333e-07}, {"id": 653, "seek": 313426, "start": 3134.26, "end": 3137.7000000000003, "text": " That gives us the ability to change the activation functions in our neural nets.", "tokens": [50364, 663, 2709, 505, 264, 3485, 281, 1319, 264, 24433, 6828, 294, 527, 18161, 36170, 13, 50536], "temperature": 0.0, "avg_logprob": -0.21076886794146368, "compression_ratio": 1.8857142857142857, "no_speech_prob": 1.1911083674931433e-06}, {"id": 654, "seek": 313426, "start": 3139.2200000000003, "end": 3143.5, "text": " Let's change getModel to allow it to take an activation function", "tokens": [50612, 961, 311, 1319, 483, 44, 41147, 281, 2089, 309, 281, 747, 364, 24433, 2445, 50826], "temperature": 0.0, "avg_logprob": -0.21076886794146368, "compression_ratio": 1.8857142857142857, "no_speech_prob": 1.1911083674931433e-06}, {"id": 655, "seek": 313426, "start": 3143.5, "end": 3145.82, "text": " which is passed into the layers.", "tokens": [50826, 597, 307, 4678, 666, 264, 7914, 13, 50942], "temperature": 0.0, "avg_logprob": -0.21076886794146368, "compression_ratio": 1.8857142857142857, "no_speech_prob": 1.1911083674931433e-06}, {"id": 656, "seek": 313426, "start": 3145.82, "end": 3146.98, "text": " And while we're there,", "tokens": [50942, 400, 1339, 321, 434, 456, 11, 51000], "temperature": 0.0, "avg_logprob": -0.21076886794146368, "compression_ratio": 1.8857142857142857, "no_speech_prob": 1.1911083674931433e-06}, {"id": 657, "seek": 313426, "start": 3146.98, "end": 3149.42, "text": " let's also make it easy to change the number of filters.", "tokens": [51000, 718, 311, 611, 652, 309, 1858, 281, 1319, 264, 1230, 295, 15995, 13, 51122], "temperature": 0.0, "avg_logprob": -0.21076886794146368, "compression_ratio": 1.8857142857142857, "no_speech_prob": 1.1911083674931433e-06}, {"id": 658, "seek": 313426, "start": 3149.42, "end": 3153.3, "text": " So we're gonna pass in a list of the number of filters in each layer, and", "tokens": [51122, 407, 321, 434, 799, 1320, 294, 257, 1329, 295, 264, 1230, 295, 15995, 294, 1184, 4583, 11, 293, 51316], "temperature": 0.0, "avg_logprob": -0.21076886794146368, "compression_ratio": 1.8857142857142857, "no_speech_prob": 1.1911083674931433e-06}, {"id": 659, "seek": 313426, "start": 3153.3, "end": 3156.94, "text": " we will default it to the numbers in each layer that we've discussed.", "tokens": [51316, 321, 486, 7576, 309, 281, 264, 3547, 294, 1184, 4583, 300, 321, 600, 7152, 13, 51498], "temperature": 0.0, "avg_logprob": -0.21076886794146368, "compression_ratio": 1.8857142857142857, "no_speech_prob": 1.1911083674931433e-06}, {"id": 660, "seek": 313426, "start": 3156.94, "end": 3161.6200000000003, "text": " And so we're just gonna go through in a list comprehension,", "tokens": [51498, 400, 370, 321, 434, 445, 799, 352, 807, 294, 257, 1329, 44991, 11, 51732], "temperature": 0.0, "avg_logprob": -0.21076886794146368, "compression_ratio": 1.8857142857142857, "no_speech_prob": 1.1911083674931433e-06}, {"id": 661, "seek": 316162, "start": 3161.62, "end": 3165.66, "text": " creating a convolution from the previous number of filters,", "tokens": [50364, 4084, 257, 45216, 490, 264, 3894, 1230, 295, 15995, 11, 50566], "temperature": 0.0, "avg_logprob": -0.3827499945958455, "compression_ratio": 1.705069124423963, "no_speech_prob": 6.540436061186483e-06}, {"id": 662, "seek": 316162, "start": 3165.66, "end": 3167.66, "text": " this number of filters to the next number of filters.", "tokens": [50566, 341, 1230, 295, 15995, 281, 264, 958, 1230, 295, 15995, 13, 50666], "temperature": 0.0, "avg_logprob": -0.3827499945958455, "compression_ratio": 1.705069124423963, "no_speech_prob": 6.540436061186483e-06}, {"id": 663, "seek": 316162, "start": 3169.02, "end": 3172.2999999999997, "text": " And we'll pop that all into a sequential along with a flatten at the end.", "tokens": [50734, 400, 321, 603, 1665, 300, 439, 666, 257, 42881, 2051, 365, 257, 24183, 412, 264, 917, 13, 50898], "temperature": 0.0, "avg_logprob": -0.3827499945958455, "compression_ratio": 1.705069124423963, "no_speech_prob": 6.540436061186483e-06}, {"id": 664, "seek": 316162, "start": 3173.54, "end": 3178.2999999999997, "text": " And while we're there, we also then need to be careful about initWaits,", "tokens": [50960, 400, 1339, 321, 434, 456, 11, 321, 611, 550, 643, 281, 312, 5026, 466, 294, 270, 54, 64, 1208, 11, 51198], "temperature": 0.0, "avg_logprob": -0.3827499945958455, "compression_ratio": 1.705069124423963, "no_speech_prob": 6.540436061186483e-06}, {"id": 665, "seek": 316162, "start": 3178.2999999999997, "end": 3182.2599999999998, "text": " because this is something that people tend to forget.", "tokens": [51198, 570, 341, 307, 746, 300, 561, 3928, 281, 2870, 13, 51396], "temperature": 0.0, "avg_logprob": -0.3827499945958455, "compression_ratio": 1.705069124423963, "no_speech_prob": 6.540436061186483e-06}, {"id": 666, "seek": 316162, "start": 3184.38, "end": 3191.54, "text": " Which is that, init, which is the timing initialization,", "tokens": [51502, 3013, 307, 300, 11, 294, 270, 11, 597, 307, 264, 10822, 5883, 2144, 11, 51860], "temperature": 0.0, "avg_logprob": -0.3827499945958455, "compression_ratio": 1.705069124423963, "no_speech_prob": 6.540436061186483e-06}, {"id": 667, "seek": 319154, "start": 3192.46, "end": 3201.7, "text": " the default only applies at all to layers that have a value activation function.", "tokens": [50410, 264, 7576, 787, 13165, 412, 439, 281, 7914, 300, 362, 257, 2158, 24433, 2445, 13, 50872], "temperature": 0.0, "avg_logprob": -0.32264563502097615, "compression_ratio": 1.6454545454545455, "no_speech_prob": 8.446221499980311e-07}, {"id": 668, "seek": 319154, "start": 3203.14, "end": 3207.06, "text": " We don't have value anymore, we actually have leaky value.", "tokens": [50944, 492, 500, 380, 362, 2158, 3602, 11, 321, 767, 362, 476, 15681, 2158, 13, 51140], "temperature": 0.0, "avg_logprob": -0.32264563502097615, "compression_ratio": 1.6454545454545455, "no_speech_prob": 8.446221499980311e-07}, {"id": 669, "seek": 319154, "start": 3208.7, "end": 3211.42, "text": " The fact that we're subtracting a bit from it doesn't change things, but", "tokens": [51222, 440, 1186, 300, 321, 434, 16390, 278, 257, 857, 490, 309, 1177, 380, 1319, 721, 11, 457, 51358], "temperature": 0.0, "avg_logprob": -0.32264563502097615, "compression_ratio": 1.6454545454545455, "no_speech_prob": 8.446221499980311e-07}, {"id": 670, "seek": 319154, "start": 3211.42, "end": 3212.7799999999997, "text": " the fact that it's leaky does.", "tokens": [51358, 264, 1186, 300, 309, 311, 476, 15681, 775, 13, 51426], "temperature": 0.0, "avg_logprob": -0.32264563502097615, "compression_ratio": 1.6454545454545455, "no_speech_prob": 8.446221499980311e-07}, {"id": 671, "seek": 319154, "start": 3212.7799999999997, "end": 3216.06, "text": " Now luckily, a lot of people don't know this, but", "tokens": [51426, 823, 22880, 11, 257, 688, 295, 561, 500, 380, 458, 341, 11, 457, 51590], "temperature": 0.0, "avg_logprob": -0.32264563502097615, "compression_ratio": 1.6454545454545455, "no_speech_prob": 8.446221499980311e-07}, {"id": 672, "seek": 319154, "start": 3216.06, "end": 3221.38, "text": " actually PyTorch's timing normal has an adjustment for leaky values.", "tokens": [51590, 767, 9953, 51, 284, 339, 311, 10822, 2710, 575, 364, 17132, 337, 476, 15681, 4190, 13, 51856], "temperature": 0.0, "avg_logprob": -0.32264563502097615, "compression_ratio": 1.6454545454545455, "no_speech_prob": 8.446221499980311e-07}, {"id": 673, "seek": 322138, "start": 3222.2200000000003, "end": 3223.98, "text": " Weirdly enough, they just call it A.", "tokens": [50406, 32033, 356, 1547, 11, 436, 445, 818, 309, 316, 13, 50494], "temperature": 0.0, "avg_logprob": -0.2980452075447004, "compression_ratio": 1.6585365853658536, "no_speech_prob": 1.7603429114387836e-06}, {"id": 674, "seek": 322138, "start": 3223.98, "end": 3228.9, "text": " So if you pass into the timing normal initialization how your leaky values", "tokens": [50494, 407, 498, 291, 1320, 666, 264, 10822, 2710, 5883, 2144, 577, 428, 476, 15681, 4190, 50740], "temperature": 0.0, "avg_logprob": -0.2980452075447004, "compression_ratio": 1.6585365853658536, "no_speech_prob": 1.7603429114387836e-06}, {"id": 675, "seek": 322138, "start": 3228.9, "end": 3233.42, "text": " leaky factor as A,", "tokens": [50740, 476, 15681, 5952, 382, 316, 11, 50966], "temperature": 0.0, "avg_logprob": -0.2980452075447004, "compression_ratio": 1.6585365853658536, "no_speech_prob": 1.7603429114387836e-06}, {"id": 676, "seek": 322138, "start": 3233.42, "end": 3236.7000000000003, "text": " then you'll get the correct initialization for a leaky value.", "tokens": [50966, 550, 291, 603, 483, 264, 3006, 5883, 2144, 337, 257, 476, 15681, 2158, 13, 51130], "temperature": 0.0, "avg_logprob": -0.2980452075447004, "compression_ratio": 1.6585365853658536, "no_speech_prob": 1.7603429114387836e-06}, {"id": 677, "seek": 322138, "start": 3236.7000000000003, "end": 3241.1400000000003, "text": " So we need to change initWaits now to pass in the leakiness.", "tokens": [51130, 407, 321, 643, 281, 1319, 294, 270, 54, 64, 1208, 586, 281, 1320, 294, 264, 17143, 1324, 13, 51352], "temperature": 0.0, "avg_logprob": -0.2980452075447004, "compression_ratio": 1.6585365853658536, "no_speech_prob": 1.7603429114387836e-06}, {"id": 678, "seek": 322138, "start": 3241.1400000000003, "end": 3242.94, "text": " All right, so let's put all this together.", "tokens": [51352, 1057, 558, 11, 370, 718, 311, 829, 439, 341, 1214, 13, 51442], "temperature": 0.0, "avg_logprob": -0.2980452075447004, "compression_ratio": 1.6585365853658536, "no_speech_prob": 1.7603429114387836e-06}, {"id": 679, "seek": 322138, "start": 3242.94, "end": 3246.7000000000003, "text": " So our general value activation function is", "tokens": [51442, 407, 527, 2674, 2158, 24433, 2445, 307, 51630], "temperature": 0.0, "avg_logprob": -0.2980452075447004, "compression_ratio": 1.6585365853658536, "no_speech_prob": 1.7603429114387836e-06}, {"id": 680, "seek": 324670, "start": 3247.7, "end": 3253.58, "text": " general value with a leak of 0.1 and a subtract of 0.4.", "tokens": [50414, 2674, 2158, 365, 257, 17143, 295, 1958, 13, 16, 293, 257, 16390, 295, 1958, 13, 19, 13, 50708], "temperature": 0.0, "avg_logprob": -0.3328711660284745, "compression_ratio": 1.5297029702970297, "no_speech_prob": 2.2827982320450246e-05}, {"id": 681, "seek": 324670, "start": 3253.58, "end": 3259.02, "text": " So we use partial to create a function that has those built-in parameters.", "tokens": [50708, 407, 321, 764, 14641, 281, 1884, 257, 2445, 300, 575, 729, 3094, 12, 259, 9834, 13, 50980], "temperature": 0.0, "avg_logprob": -0.3328711660284745, "compression_ratio": 1.5297029702970297, "no_speech_prob": 2.2827982320450246e-05}, {"id": 682, "seek": 324670, "start": 3261.58, "end": 3264.1, "text": " For activation stats, we need to update it now to look for", "tokens": [51108, 1171, 24433, 18152, 11, 321, 643, 281, 5623, 309, 586, 281, 574, 337, 51234], "temperature": 0.0, "avg_logprob": -0.3328711660284745, "compression_ratio": 1.5297029702970297, "no_speech_prob": 2.2827982320450246e-05}, {"id": 683, "seek": 324670, "start": 3264.1, "end": 3266.74, "text": " general values, not nn.values.", "tokens": [51234, 2674, 4190, 11, 406, 297, 77, 13, 46033, 13, 51366], "temperature": 0.0, "avg_logprob": -0.3328711660284745, "compression_ratio": 1.5297029702970297, "no_speech_prob": 2.2827982320450246e-05}, {"id": 684, "seek": 324670, "start": 3269.5, "end": 3272.8599999999997, "text": " Okay, and then our initWaits function,", "tokens": [51504, 1033, 11, 293, 550, 527, 294, 270, 54, 1001, 82, 2445, 11, 51672], "temperature": 0.0, "avg_logprob": -0.3328711660284745, "compression_ratio": 1.5297029702970297, "no_speech_prob": 2.2827982320450246e-05}, {"id": 685, "seek": 324670, "start": 3272.8599999999997, "end": 3275.5, "text": " we're gonna have a partial with leaky equals 0.1.", "tokens": [51672, 321, 434, 799, 362, 257, 14641, 365, 476, 15681, 6915, 1958, 13, 16, 13, 51804], "temperature": 0.0, "avg_logprob": -0.3328711660284745, "compression_ratio": 1.5297029702970297, "no_speech_prob": 2.2827982320450246e-05}, {"id": 686, "seek": 327550, "start": 3275.5, "end": 3277.86, "text": " So we'll call that our initWaits.", "tokens": [50364, 407, 321, 603, 818, 300, 527, 294, 270, 54, 1001, 82, 13, 50482], "temperature": 0.0, "avg_logprob": -0.470292610034608, "compression_ratio": 1.4067796610169492, "no_speech_prob": 7.112436719580728e-07}, {"id": 687, "seek": 327550, "start": 3281.86, "end": 3286.06, "text": " Great, so now we'll get our model using that new activation function and", "tokens": [50682, 3769, 11, 370, 586, 321, 603, 483, 527, 2316, 1228, 300, 777, 24433, 2445, 293, 50892], "temperature": 0.0, "avg_logprob": -0.470292610034608, "compression_ratio": 1.4067796610169492, "no_speech_prob": 7.112436719580728e-07}, {"id": 688, "seek": 327550, "start": 3286.06, "end": 3287.26, "text": " that new initWaits.", "tokens": [50892, 300, 777, 294, 270, 54, 1001, 82, 13, 50952], "temperature": 0.0, "avg_logprob": -0.470292610034608, "compression_ratio": 1.4067796610169492, "no_speech_prob": 7.112436719580728e-07}, {"id": 689, "seek": 327550, "start": 3289.66, "end": 3291.14, "text": " And we'll fit that.", "tokens": [51072, 400, 321, 603, 3318, 300, 13, 51146], "temperature": 0.0, "avg_logprob": -0.470292610034608, "compression_ratio": 1.4067796610169492, "no_speech_prob": 7.112436719580728e-07}, {"id": 690, "seek": 327550, "start": 3299.02, "end": 3300.66, "text": " That's encouraging.", "tokens": [51540, 663, 311, 14580, 13, 51622], "temperature": 0.0, "avg_logprob": -0.470292610034608, "compression_ratio": 1.4067796610169492, "no_speech_prob": 7.112436719580728e-07}, {"id": 691, "seek": 330066, "start": 3300.66, "end": 3307.3799999999997, "text": " An accuracy of 8.45, which is about as high as we got to at the end previously.", "tokens": [50364, 1107, 14170, 295, 1649, 13, 8465, 11, 597, 307, 466, 382, 1090, 382, 321, 658, 281, 412, 264, 917, 8046, 13, 50700], "temperature": 0.0, "avg_logprob": -0.3422332532478101, "compression_ratio": 1.5372340425531914, "no_speech_prob": 4.3319003452779725e-05}, {"id": 692, "seek": 330066, "start": 3309.66, "end": 3311.5, "text": " Wow, look at that.", "tokens": [50814, 3153, 11, 574, 412, 300, 13, 50906], "temperature": 0.0, "avg_logprob": -0.3422332532478101, "compression_ratio": 1.5372340425531914, "no_speech_prob": 4.3319003452779725e-05}, {"id": 693, "seek": 330066, "start": 3311.5, "end": 3315.3799999999997, "text": " So we're up to an accuracy of 87%.", "tokens": [50906, 407, 321, 434, 493, 281, 364, 14170, 295, 27990, 6856, 51100], "temperature": 0.0, "avg_logprob": -0.3422332532478101, "compression_ratio": 1.5372340425531914, "no_speech_prob": 4.3319003452779725e-05}, {"id": 694, "seek": 330066, "start": 3315.3799999999997, "end": 3316.66, "text": " And let's take a look.", "tokens": [51100, 400, 718, 311, 747, 257, 574, 13, 51164], "temperature": 0.0, "avg_logprob": -0.3422332532478101, "compression_ratio": 1.5372340425531914, "no_speech_prob": 4.3319003452779725e-05}, {"id": 695, "seek": 330066, "start": 3317.74, "end": 3321.22, "text": " Yeah, I mean, look, we still got a little bit of a spike, but", "tokens": [51218, 865, 11, 286, 914, 11, 574, 11, 321, 920, 658, 257, 707, 857, 295, 257, 21053, 11, 457, 51392], "temperature": 0.0, "avg_logprob": -0.3422332532478101, "compression_ratio": 1.5372340425531914, "no_speech_prob": 4.3319003452779725e-05}, {"id": 696, "seek": 330066, "start": 3321.22, "end": 3323.74, "text": " it's almost smooth and flat.", "tokens": [51392, 309, 311, 1920, 5508, 293, 4962, 13, 51518], "temperature": 0.0, "avg_logprob": -0.3422332532478101, "compression_ratio": 1.5372340425531914, "no_speech_prob": 4.3319003452779725e-05}, {"id": 697, "seek": 330066, "start": 3325.22, "end": 3326.18, "text": " And let's have a look here.", "tokens": [51592, 400, 718, 311, 362, 257, 574, 510, 13, 51640], "temperature": 0.0, "avg_logprob": -0.3422332532478101, "compression_ratio": 1.5372340425531914, "no_speech_prob": 4.3319003452779725e-05}, {"id": 698, "seek": 330066, "start": 3326.18, "end": 3327.1, "text": " Look at that.", "tokens": [51640, 2053, 412, 300, 13, 51686], "temperature": 0.0, "avg_logprob": -0.3422332532478101, "compression_ratio": 1.5372340425531914, "no_speech_prob": 4.3319003452779725e-05}, {"id": 699, "seek": 332710, "start": 3327.1, "end": 3329.1, "text": " Our mean is starting at about 0.", "tokens": [50364, 2621, 914, 307, 2891, 412, 466, 1958, 13, 50464], "temperature": 0.0, "avg_logprob": -0.3608061599731445, "compression_ratio": 1.5584415584415585, "no_speech_prob": 4.31391242727841e-07}, {"id": 700, "seek": 332710, "start": 3330.18, "end": 3331.2999999999997, "text": " Standard deviation.", "tokens": [50518, 21298, 25163, 13, 50574], "temperature": 0.0, "avg_logprob": -0.3608061599731445, "compression_ratio": 1.5584415584415585, "no_speech_prob": 4.31391242727841e-07}, {"id": 701, "seek": 332710, "start": 3332.98, "end": 3336.2999999999997, "text": " No, standard deviation's still a bit low, but it's coming up around 1.", "tokens": [50658, 883, 11, 3832, 25163, 311, 920, 257, 857, 2295, 11, 457, 309, 311, 1348, 493, 926, 502, 13, 50824], "temperature": 0.0, "avg_logprob": -0.3608061599731445, "compression_ratio": 1.5584415584415585, "no_speech_prob": 4.31391242727841e-07}, {"id": 702, "seek": 332710, "start": 3336.2999999999997, "end": 3338.22, "text": " It's not too bad, generally around 0.8.", "tokens": [50824, 467, 311, 406, 886, 1578, 11, 5101, 926, 1958, 13, 23, 13, 50920], "temperature": 0.0, "avg_logprob": -0.3608061599731445, "compression_ratio": 1.5584415584415585, "no_speech_prob": 4.31391242727841e-07}, {"id": 703, "seek": 332710, "start": 3338.22, "end": 3342.02, "text": " So it's all looking pretty encouraging, I think.", "tokens": [50920, 407, 309, 311, 439, 1237, 1238, 14580, 11, 286, 519, 13, 51110], "temperature": 0.0, "avg_logprob": -0.3608061599731445, "compression_ratio": 1.5584415584415585, "no_speech_prob": 4.31391242727841e-07}, {"id": 704, "seek": 332710, "start": 3342.02, "end": 3347.74, "text": " And yeah, look, the percentage of dead units in each layer is very small.", "tokens": [51110, 400, 1338, 11, 574, 11, 264, 9668, 295, 3116, 6815, 294, 1184, 4583, 307, 588, 1359, 13, 51396], "temperature": 0.0, "avg_logprob": -0.3608061599731445, "compression_ratio": 1.5584415584415585, "no_speech_prob": 4.31391242727841e-07}, {"id": 705, "seek": 332710, "start": 3349.02, "end": 3356.54, "text": " So finally, we've really got some very nice looking training graphs here.", "tokens": [51460, 407, 2721, 11, 321, 600, 534, 658, 512, 588, 1481, 1237, 3097, 24877, 510, 13, 51836], "temperature": 0.0, "avg_logprob": -0.3608061599731445, "compression_ratio": 1.5584415584415585, "no_speech_prob": 4.31391242727841e-07}, {"id": 706, "seek": 335654, "start": 3356.54, "end": 3362.02, "text": " And yeah, it's interesting that we had to literally invent our", "tokens": [50364, 400, 1338, 11, 309, 311, 1880, 300, 321, 632, 281, 3736, 7962, 527, 50638], "temperature": 0.0, "avg_logprob": -0.25823079622708833, "compression_ratio": 1.7574468085106383, "no_speech_prob": 4.565970357361948e-06}, {"id": 707, "seek": 335654, "start": 3362.02, "end": 3364.86, "text": " own activation function to make this work.", "tokens": [50638, 1065, 24433, 2445, 281, 652, 341, 589, 13, 50780], "temperature": 0.0, "avg_logprob": -0.25823079622708833, "compression_ratio": 1.7574468085106383, "no_speech_prob": 4.565970357361948e-06}, {"id": 708, "seek": 335654, "start": 3364.86, "end": 3371.14, "text": " And I think that gives you a sense of how few people actually care about this,", "tokens": [50780, 400, 286, 519, 300, 2709, 291, 257, 2020, 295, 577, 1326, 561, 767, 1127, 466, 341, 11, 51094], "temperature": 0.0, "avg_logprob": -0.25823079622708833, "compression_ratio": 1.7574468085106383, "no_speech_prob": 4.565970357361948e-06}, {"id": 709, "seek": 335654, "start": 3371.14, "end": 3375.46, "text": " which is crazy, because as you can see, in some ways, it's the only thing that matters.", "tokens": [51094, 597, 307, 3219, 11, 570, 382, 291, 393, 536, 11, 294, 512, 2098, 11, 309, 311, 264, 787, 551, 300, 7001, 13, 51310], "temperature": 0.0, "avg_logprob": -0.25823079622708833, "compression_ratio": 1.7574468085106383, "no_speech_prob": 4.565970357361948e-06}, {"id": 710, "seek": 335654, "start": 3375.46, "end": 3381.2599999999998, "text": " And it's not at all mathematically difficult to make it all work.", "tokens": [51310, 400, 309, 311, 406, 412, 439, 44003, 2252, 281, 652, 309, 439, 589, 13, 51600], "temperature": 0.0, "avg_logprob": -0.25823079622708833, "compression_ratio": 1.7574468085106383, "no_speech_prob": 4.565970357361948e-06}, {"id": 711, "seek": 335654, "start": 3381.2599999999998, "end": 3385.46, "text": " And it's not at all computationally difficult to see whether it's working.", "tokens": [51600, 400, 309, 311, 406, 412, 439, 24903, 379, 2252, 281, 536, 1968, 309, 311, 1364, 13, 51810], "temperature": 0.0, "avg_logprob": -0.25823079622708833, "compression_ratio": 1.7574468085106383, "no_speech_prob": 4.565970357361948e-06}, {"id": 712, "seek": 338546, "start": 3385.46, "end": 3389.7400000000002, "text": " But other frameworks don't even let you plot these kinds of things.", "tokens": [50364, 583, 661, 29834, 500, 380, 754, 718, 291, 7542, 613, 3685, 295, 721, 13, 50578], "temperature": 0.0, "avg_logprob": -0.35143180120558964, "compression_ratio": 1.5070422535211268, "no_speech_prob": 2.726466163949226e-06}, {"id": 713, "seek": 338546, "start": 3389.7400000000002, "end": 3392.9, "text": " So nobody even knows that they've completely messed up their", "tokens": [50578, 407, 5079, 754, 3255, 300, 436, 600, 2584, 16507, 493, 641, 50736], "temperature": 0.0, "avg_logprob": -0.35143180120558964, "compression_ratio": 1.5070422535211268, "no_speech_prob": 2.726466163949226e-06}, {"id": 714, "seek": 338546, "start": 3392.9, "end": 3393.7, "text": " initialization.", "tokens": [50736, 5883, 2144, 13, 50776], "temperature": 0.0, "avg_logprob": -0.35143180120558964, "compression_ratio": 1.5070422535211268, "no_speech_prob": 2.726466163949226e-06}, {"id": 715, "seek": 338546, "start": 3396.2200000000003, "end": 3397.78, "text": " So yeah, now you know.", "tokens": [50902, 407, 1338, 11, 586, 291, 458, 13, 50980], "temperature": 0.0, "avg_logprob": -0.35143180120558964, "compression_ratio": 1.5070422535211268, "no_speech_prob": 2.726466163949226e-06}, {"id": 716, "seek": 338546, "start": 3399.66, "end": 3401.86, "text": " Now some very nice news.", "tokens": [51074, 823, 512, 588, 1481, 2583, 13, 51184], "temperature": 0.0, "avg_logprob": -0.35143180120558964, "compression_ratio": 1.5070422535211268, "no_speech_prob": 2.726466163949226e-06}, {"id": 717, "seek": 338546, "start": 3401.86, "end": 3407.54, "text": " Well, so the first thing to be aware of, which is tricky, is a lot of", "tokens": [51184, 1042, 11, 370, 264, 700, 551, 281, 312, 3650, 295, 11, 597, 307, 12414, 11, 307, 257, 688, 295, 51468], "temperature": 0.0, "avg_logprob": -0.35143180120558964, "compression_ratio": 1.5070422535211268, "no_speech_prob": 2.726466163949226e-06}, {"id": 718, "seek": 338546, "start": 3407.54, "end": 3413.2200000000003, "text": " models use more complicated activation functions nowadays,", "tokens": [51468, 5245, 764, 544, 6179, 24433, 6828, 13434, 11, 51752], "temperature": 0.0, "avg_logprob": -0.35143180120558964, "compression_ratio": 1.5070422535211268, "no_speech_prob": 2.726466163949226e-06}, {"id": 719, "seek": 341322, "start": 3413.22, "end": 3416.18, "text": " rather than ReLU or leaky ReLU or even this general version.", "tokens": [50364, 2831, 813, 1300, 43, 52, 420, 476, 15681, 1300, 43, 52, 420, 754, 341, 2674, 3037, 13, 50512], "temperature": 0.0, "avg_logprob": -0.3490253076320741, "compression_ratio": 1.555023923444976, "no_speech_prob": 2.1444928279379383e-05}, {"id": 720, "seek": 341322, "start": 3419.9399999999996, "end": 3423.06, "text": " You need to initialize your neural network correctly, and most people don't.", "tokens": [50700, 509, 643, 281, 5883, 1125, 428, 18161, 3209, 8944, 11, 293, 881, 561, 500, 380, 13, 50856], "temperature": 0.0, "avg_logprob": -0.3490253076320741, "compression_ratio": 1.555023923444976, "no_speech_prob": 2.1444928279379383e-05}, {"id": 721, "seek": 341322, "start": 3425.54, "end": 3429.8999999999996, "text": " And sometimes nobody's even figured out or bothered to try to figure out", "tokens": [50980, 400, 2171, 5079, 311, 754, 8932, 484, 420, 22996, 281, 853, 281, 2573, 484, 51198], "temperature": 0.0, "avg_logprob": -0.3490253076320741, "compression_ratio": 1.555023923444976, "no_speech_prob": 2.1444928279379383e-05}, {"id": 722, "seek": 341322, "start": 3432.4199999999996, "end": 3436.2599999999998, "text": " what the correct initialization to use is.", "tokens": [51324, 437, 264, 3006, 5883, 2144, 281, 764, 307, 13, 51516], "temperature": 0.0, "avg_logprob": -0.3490253076320741, "compression_ratio": 1.555023923444976, "no_speech_prob": 2.1444928279379383e-05}, {"id": 723, "seek": 341322, "start": 3436.2599999999998, "end": 3439.62, "text": " But there's actually a very cool trick which almost nobody knows about,", "tokens": [51516, 583, 456, 311, 767, 257, 588, 1627, 4282, 597, 1920, 5079, 3255, 466, 11, 51684], "temperature": 0.0, "avg_logprob": -0.3490253076320741, "compression_ratio": 1.555023923444976, "no_speech_prob": 2.1444928279379383e-05}, {"id": 724, "seek": 343962, "start": 3440.62, "end": 3443.38, "text": " which is a paper called All You Need is a Good Init,", "tokens": [50414, 597, 307, 257, 3035, 1219, 1057, 509, 16984, 307, 257, 2205, 22937, 11, 50552], "temperature": 0.0, "avg_logprob": -0.3642224448067801, "compression_ratio": 1.4224598930481283, "no_speech_prob": 2.521573378544417e-06}, {"id": 725, "seek": 343962, "start": 3447.2599999999998, "end": 3450.14, "text": " which Dmitry Mishkin wrote a few years ago.", "tokens": [50746, 597, 413, 3508, 627, 376, 742, 5843, 4114, 257, 1326, 924, 2057, 13, 50890], "temperature": 0.0, "avg_logprob": -0.3642224448067801, "compression_ratio": 1.4224598930481283, "no_speech_prob": 2.521573378544417e-06}, {"id": 726, "seek": 343962, "start": 3451.3399999999997, "end": 3457.66, "text": " And what Dmitry showed is that there's actually a completely general way", "tokens": [50950, 400, 437, 413, 3508, 627, 4712, 307, 300, 456, 311, 767, 257, 2584, 2674, 636, 51266], "temperature": 0.0, "avg_logprob": -0.3642224448067801, "compression_ratio": 1.4224598930481283, "no_speech_prob": 2.521573378544417e-06}, {"id": 727, "seek": 343962, "start": 3457.66, "end": 3462.46, "text": " of initializing any neural network correctly,", "tokens": [51266, 295, 5883, 3319, 604, 18161, 3209, 8944, 11, 51506], "temperature": 0.0, "avg_logprob": -0.3642224448067801, "compression_ratio": 1.4224598930481283, "no_speech_prob": 2.521573378544417e-06}, {"id": 728, "seek": 343962, "start": 3462.46, "end": 3467.1, "text": " regardless of what activation functions are in it.", "tokens": [51506, 10060, 295, 437, 24433, 6828, 366, 294, 309, 13, 51738], "temperature": 0.0, "avg_logprob": -0.3642224448067801, "compression_ratio": 1.4224598930481283, "no_speech_prob": 2.521573378544417e-06}, {"id": 729, "seek": 346710, "start": 3467.14, "end": 3470.2599999999998, "text": " And it uses a very, very simple idea.", "tokens": [50366, 400, 309, 4960, 257, 588, 11, 588, 2199, 1558, 13, 50522], "temperature": 0.0, "avg_logprob": -0.2650629335695559, "compression_ratio": 1.8385650224215246, "no_speech_prob": 2.4824801130307605e-06}, {"id": 730, "seek": 346710, "start": 3470.2599999999998, "end": 3475.9, "text": " And the idea is create your model, initialize it however you like,", "tokens": [50522, 400, 264, 1558, 307, 1884, 428, 2316, 11, 5883, 1125, 309, 4461, 291, 411, 11, 50804], "temperature": 0.0, "avg_logprob": -0.2650629335695559, "compression_ratio": 1.8385650224215246, "no_speech_prob": 2.4824801130307605e-06}, {"id": 731, "seek": 346710, "start": 3477.18, "end": 3479.94, "text": " and then go through and put a single batch of data through.", "tokens": [50868, 293, 550, 352, 807, 293, 829, 257, 2167, 15245, 295, 1412, 807, 13, 51006], "temperature": 0.0, "avg_logprob": -0.2650629335695559, "compression_ratio": 1.8385650224215246, "no_speech_prob": 2.4824801130307605e-06}, {"id": 732, "seek": 346710, "start": 3481.86, "end": 3483.3399999999997, "text": " And look at the first layer,", "tokens": [51102, 400, 574, 412, 264, 700, 4583, 11, 51176], "temperature": 0.0, "avg_logprob": -0.2650629335695559, "compression_ratio": 1.8385650224215246, "no_speech_prob": 2.4824801130307605e-06}, {"id": 733, "seek": 346710, "start": 3484.38, "end": 3487.58, "text": " see what the mean standard deviation through the first layer is.", "tokens": [51228, 536, 437, 264, 914, 3832, 25163, 807, 264, 700, 4583, 307, 13, 51388], "temperature": 0.0, "avg_logprob": -0.2650629335695559, "compression_ratio": 1.8385650224215246, "no_speech_prob": 2.4824801130307605e-06}, {"id": 734, "seek": 346710, "start": 3487.58, "end": 3490.42, "text": " And if the mean, if the standard deviation is too big,", "tokens": [51388, 400, 498, 264, 914, 11, 498, 264, 3832, 25163, 307, 886, 955, 11, 51530], "temperature": 0.0, "avg_logprob": -0.2650629335695559, "compression_ratio": 1.8385650224215246, "no_speech_prob": 2.4824801130307605e-06}, {"id": 735, "seek": 346710, "start": 3490.42, "end": 3491.94, "text": " divide the weight matrix by a bit.", "tokens": [51530, 9845, 264, 3364, 8141, 538, 257, 857, 13, 51606], "temperature": 0.0, "avg_logprob": -0.2650629335695559, "compression_ratio": 1.8385650224215246, "no_speech_prob": 2.4824801130307605e-06}, {"id": 736, "seek": 346710, "start": 3491.94, "end": 3495.54, "text": " If the mean's too high, subtract a bit off the weight matrix.", "tokens": [51606, 759, 264, 914, 311, 886, 1090, 11, 16390, 257, 857, 766, 264, 3364, 8141, 13, 51786], "temperature": 0.0, "avg_logprob": -0.2650629335695559, "compression_ratio": 1.8385650224215246, "no_speech_prob": 2.4824801130307605e-06}, {"id": 737, "seek": 349554, "start": 3495.54, "end": 3499.86, "text": " And do that repeatedly for the first layer until you get the correct mean and", "tokens": [50364, 400, 360, 300, 18227, 337, 264, 700, 4583, 1826, 291, 483, 264, 3006, 914, 293, 50580], "temperature": 0.0, "avg_logprob": -0.25772859369005474, "compression_ratio": 1.7296137339055795, "no_speech_prob": 6.540436970681185e-06}, {"id": 738, "seek": 349554, "start": 3499.86, "end": 3500.94, "text": " standard deviation.", "tokens": [50580, 3832, 25163, 13, 50634], "temperature": 0.0, "avg_logprob": -0.25772859369005474, "compression_ratio": 1.7296137339055795, "no_speech_prob": 6.540436970681185e-06}, {"id": 739, "seek": 349554, "start": 3500.94, "end": 3503.42, "text": " And then go to the second layer, do the same thing.", "tokens": [50634, 400, 550, 352, 281, 264, 1150, 4583, 11, 360, 264, 912, 551, 13, 50758], "temperature": 0.0, "avg_logprob": -0.25772859369005474, "compression_ratio": 1.7296137339055795, "no_speech_prob": 6.540436970681185e-06}, {"id": 740, "seek": 349554, "start": 3503.42, "end": 3505.66, "text": " Third layer, do the same thing, and so forth.", "tokens": [50758, 12548, 4583, 11, 360, 264, 912, 551, 11, 293, 370, 5220, 13, 50870], "temperature": 0.0, "avg_logprob": -0.25772859369005474, "compression_ratio": 1.7296137339055795, "no_speech_prob": 6.540436970681185e-06}, {"id": 741, "seek": 349554, "start": 3507.1, "end": 3510.58, "text": " So we can do that using hooks, right?", "tokens": [50942, 407, 321, 393, 360, 300, 1228, 26485, 11, 558, 30, 51116], "temperature": 0.0, "avg_logprob": -0.25772859369005474, "compression_ratio": 1.7296137339055795, "no_speech_prob": 6.540436970681185e-06}, {"id": 742, "seek": 349554, "start": 3510.58, "end": 3513.3, "text": " So we could create a little, so", "tokens": [51116, 407, 321, 727, 1884, 257, 707, 11, 370, 51252], "temperature": 0.0, "avg_logprob": -0.25772859369005474, "compression_ratio": 1.7296137339055795, "no_speech_prob": 6.540436970681185e-06}, {"id": 743, "seek": 349554, "start": 3513.3, "end": 3516.66, "text": " this is called layer-wise sequential unit variance, LSUV.", "tokens": [51252, 341, 307, 1219, 4583, 12, 3711, 42881, 4985, 21977, 11, 441, 20214, 53, 13, 51420], "temperature": 0.0, "avg_logprob": -0.25772859369005474, "compression_ratio": 1.7296137339055795, "no_speech_prob": 6.540436970681185e-06}, {"id": 744, "seek": 349554, "start": 3516.66, "end": 3521.9, "text": " We can create a little LSUV stats that will grab the mean of the activations of", "tokens": [51420, 492, 393, 1884, 257, 707, 441, 20214, 53, 18152, 300, 486, 4444, 264, 914, 295, 264, 2430, 763, 295, 51682], "temperature": 0.0, "avg_logprob": -0.25772859369005474, "compression_ratio": 1.7296137339055795, "no_speech_prob": 6.540436970681185e-06}, {"id": 745, "seek": 352190, "start": 3521.9, "end": 3524.86, "text": " a layer and the standard deviation of the activations of a layer.", "tokens": [50364, 257, 4583, 293, 264, 3832, 25163, 295, 264, 2430, 763, 295, 257, 4583, 13, 50512], "temperature": 0.0, "avg_logprob": -0.30703859205369827, "compression_ratio": 1.8616352201257862, "no_speech_prob": 7.14190027792938e-05}, {"id": 746, "seek": 352190, "start": 3525.98, "end": 3530.38, "text": " And we will create a hook with that function.", "tokens": [50568, 400, 321, 486, 1884, 257, 6328, 365, 300, 2445, 13, 50788], "temperature": 0.0, "avg_logprob": -0.30703859205369827, "compression_ratio": 1.8616352201257862, "no_speech_prob": 7.14190027792938e-05}, {"id": 747, "seek": 352190, "start": 3530.38, "end": 3536.82, "text": " And what it's gonna do is after we've run that hook to find out the mean and", "tokens": [50788, 400, 437, 309, 311, 799, 360, 307, 934, 321, 600, 1190, 300, 6328, 281, 915, 484, 264, 914, 293, 51110], "temperature": 0.0, "avg_logprob": -0.30703859205369827, "compression_ratio": 1.8616352201257862, "no_speech_prob": 7.14190027792938e-05}, {"id": 748, "seek": 352190, "start": 3536.82, "end": 3542.34, "text": " standard deviation of the layer, we will go through and", "tokens": [51110, 3832, 25163, 295, 264, 4583, 11, 321, 486, 352, 807, 293, 51386], "temperature": 0.0, "avg_logprob": -0.30703859205369827, "compression_ratio": 1.8616352201257862, "no_speech_prob": 7.14190027792938e-05}, {"id": 749, "seek": 352190, "start": 3544.82, "end": 3549.7400000000002, "text": " run the model, get the standard deviation and mean.", "tokens": [51510, 1190, 264, 2316, 11, 483, 264, 3832, 25163, 293, 914, 13, 51756], "temperature": 0.0, "avg_logprob": -0.30703859205369827, "compression_ratio": 1.8616352201257862, "no_speech_prob": 7.14190027792938e-05}, {"id": 750, "seek": 354974, "start": 3549.74, "end": 3555.06, "text": " See if the standard deviation is not 1, see if the mean is not 0.", "tokens": [50364, 3008, 498, 264, 3832, 25163, 307, 406, 502, 11, 536, 498, 264, 914, 307, 406, 1958, 13, 50630], "temperature": 0.0, "avg_logprob": -0.30650806427001953, "compression_ratio": 1.9444444444444444, "no_speech_prob": 5.28551311163028e-07}, {"id": 751, "seek": 354974, "start": 3555.06, "end": 3558.62, "text": " And we will subtract the mean from the bias, and", "tokens": [50630, 400, 321, 486, 16390, 264, 914, 490, 264, 12577, 11, 293, 50808], "temperature": 0.0, "avg_logprob": -0.30650806427001953, "compression_ratio": 1.9444444444444444, "no_speech_prob": 5.28551311163028e-07}, {"id": 752, "seek": 354974, "start": 3558.62, "end": 3564.54, "text": " we will divide the weight matrix by the standard deviation.", "tokens": [50808, 321, 486, 9845, 264, 3364, 8141, 538, 264, 3832, 25163, 13, 51104], "temperature": 0.0, "avg_logprob": -0.30650806427001953, "compression_ratio": 1.9444444444444444, "no_speech_prob": 5.28551311163028e-07}, {"id": 753, "seek": 354974, "start": 3564.54, "end": 3572.14, "text": " And we will keep doing that until we get a standard deviation of 1 and a mean of 0.", "tokens": [51104, 400, 321, 486, 1066, 884, 300, 1826, 321, 483, 257, 3832, 25163, 295, 502, 293, 257, 914, 295, 1958, 13, 51484], "temperature": 0.0, "avg_logprob": -0.30650806427001953, "compression_ratio": 1.9444444444444444, "no_speech_prob": 5.28551311163028e-07}, {"id": 754, "seek": 354974, "start": 3572.14, "end": 3577.02, "text": " And so by making that a hook, what we will do is we will", "tokens": [51484, 400, 370, 538, 1455, 300, 257, 6328, 11, 437, 321, 486, 360, 307, 321, 486, 51728], "temperature": 0.0, "avg_logprob": -0.30650806427001953, "compression_ratio": 1.9444444444444444, "no_speech_prob": 5.28551311163028e-07}, {"id": 755, "seek": 357702, "start": 3578.02, "end": 3586.06, "text": " grab all the values and all the coms, right?", "tokens": [50414, 4444, 439, 264, 4190, 293, 439, 264, 395, 82, 11, 558, 30, 50816], "temperature": 0.0, "avg_logprob": -0.2841971624465216, "compression_ratio": 1.8, "no_speech_prob": 1.2878952020400902e-06}, {"id": 756, "seek": 357702, "start": 3586.06, "end": 3588.54, "text": " And so just to show you what happens there,", "tokens": [50816, 400, 370, 445, 281, 855, 291, 437, 2314, 456, 11, 50940], "temperature": 0.0, "avg_logprob": -0.2841971624465216, "compression_ratio": 1.8, "no_speech_prob": 1.2878952020400902e-06}, {"id": 757, "seek": 357702, "start": 3588.54, "end": 3591.54, "text": " once I've got all the values and all the coms, I can use zip.", "tokens": [50940, 1564, 286, 600, 658, 439, 264, 4190, 293, 439, 264, 395, 82, 11, 286, 393, 764, 20730, 13, 51090], "temperature": 0.0, "avg_logprob": -0.2841971624465216, "compression_ratio": 1.8, "no_speech_prob": 1.2878952020400902e-06}, {"id": 758, "seek": 357702, "start": 3591.54, "end": 3599.62, "text": " So zip in Python takes a bunch of lists and creates a list of the items,", "tokens": [51090, 407, 20730, 294, 15329, 2516, 257, 3840, 295, 14511, 293, 7829, 257, 1329, 295, 264, 4754, 11, 51494], "temperature": 0.0, "avg_logprob": -0.2841971624465216, "compression_ratio": 1.8, "no_speech_prob": 1.2878952020400902e-06}, {"id": 759, "seek": 357702, "start": 3599.62, "end": 3602.62, "text": " the first items, the second items, the third items, and so forth.", "tokens": [51494, 264, 700, 4754, 11, 264, 1150, 4754, 11, 264, 2636, 4754, 11, 293, 370, 5220, 13, 51644], "temperature": 0.0, "avg_logprob": -0.2841971624465216, "compression_ratio": 1.8, "no_speech_prob": 1.2878952020400902e-06}, {"id": 760, "seek": 357702, "start": 3602.62, "end": 3606.38, "text": " So if I go through the zip of values and coms and just print them out,", "tokens": [51644, 407, 498, 286, 352, 807, 264, 20730, 295, 4190, 293, 395, 82, 293, 445, 4482, 552, 484, 11, 51832], "temperature": 0.0, "avg_logprob": -0.2841971624465216, "compression_ratio": 1.8, "no_speech_prob": 1.2878952020400902e-06}, {"id": 761, "seek": 360638, "start": 3606.38, "end": 3609.38, "text": " you can see it prints out the value and the first com.", "tokens": [50364, 291, 393, 536, 309, 22305, 484, 264, 2158, 293, 264, 700, 395, 13, 50514], "temperature": 0.0, "avg_logprob": -0.34941586244453504, "compression_ratio": 1.7188940092165899, "no_speech_prob": 6.9622547016479075e-06}, {"id": 762, "seek": 360638, "start": 3609.38, "end": 3612.1800000000003, "text": " The second value, the second com, the second value, sorry,", "tokens": [50514, 440, 1150, 2158, 11, 264, 1150, 395, 11, 264, 1150, 2158, 11, 2597, 11, 50654], "temperature": 0.0, "avg_logprob": -0.34941586244453504, "compression_ratio": 1.7188940092165899, "no_speech_prob": 6.9622547016479075e-06}, {"id": 763, "seek": 360638, "start": 3612.1800000000003, "end": 3614.5, "text": " the third value, the third com, and so forth.", "tokens": [50654, 264, 2636, 2158, 11, 264, 2636, 395, 11, 293, 370, 5220, 13, 50770], "temperature": 0.0, "avg_logprob": -0.34941586244453504, "compression_ratio": 1.7188940092165899, "no_speech_prob": 6.9622547016479075e-06}, {"id": 764, "seek": 360638, "start": 3614.5, "end": 3618.6600000000003, "text": " We use zip all the time in Python, so it's a really important thing to be aware of.", "tokens": [50770, 492, 764, 20730, 439, 264, 565, 294, 15329, 11, 370, 309, 311, 257, 534, 1021, 551, 281, 312, 3650, 295, 13, 50978], "temperature": 0.0, "avg_logprob": -0.34941586244453504, "compression_ratio": 1.7188940092165899, "no_speech_prob": 6.9622547016479075e-06}, {"id": 765, "seek": 360638, "start": 3621.5, "end": 3625.26, "text": " So we could go through the values and the coms and", "tokens": [51120, 407, 321, 727, 352, 807, 264, 4190, 293, 264, 395, 82, 293, 51308], "temperature": 0.0, "avg_logprob": -0.34941586244453504, "compression_ratio": 1.7188940092165899, "no_speech_prob": 6.9622547016479075e-06}, {"id": 766, "seek": 360638, "start": 3625.26, "end": 3628.98, "text": " call layer-wise sequential unit variance in it,", "tokens": [51308, 818, 4583, 12, 3711, 42881, 4985, 21977, 294, 309, 11, 51494], "temperature": 0.0, "avg_logprob": -0.34941586244453504, "compression_ratio": 1.7188940092165899, "no_speech_prob": 6.9622547016479075e-06}, {"id": 767, "seek": 360638, "start": 3628.98, "end": 3634.5, "text": " passing in those module pairs.", "tokens": [51494, 8437, 294, 729, 10088, 15494, 13, 51770], "temperature": 0.0, "avg_logprob": -0.34941586244453504, "compression_ratio": 1.7188940092165899, "no_speech_prob": 6.9622547016479075e-06}, {"id": 768, "seek": 363638, "start": 3636.78, "end": 3641.26, "text": " Sorry, passing in, yeah, passing in the relu and the conv.", "tokens": [50384, 4919, 11, 8437, 294, 11, 1338, 11, 8437, 294, 264, 1039, 84, 293, 264, 3754, 13, 50608], "temperature": 0.0, "avg_logprob": -0.42655306277067767, "compression_ratio": 1.5724137931034483, "no_speech_prob": 3.822904545813799e-05}, {"id": 769, "seek": 363638, "start": 3641.26, "end": 3651.02, "text": " And then for each one, and we're gonna do that on the batch.", "tokens": [50608, 400, 550, 337, 1184, 472, 11, 293, 321, 434, 799, 360, 300, 322, 264, 15245, 13, 51096], "temperature": 0.0, "avg_logprob": -0.42655306277067767, "compression_ratio": 1.5724137931034483, "no_speech_prob": 3.822904545813799e-05}, {"id": 770, "seek": 363638, "start": 3651.02, "end": 3654.06, "text": " And of course, we need to put the batch on the correct device for our model.", "tokens": [51096, 400, 295, 1164, 11, 321, 643, 281, 829, 264, 15245, 322, 264, 3006, 4302, 337, 527, 2316, 13, 51248], "temperature": 0.0, "avg_logprob": -0.42655306277067767, "compression_ratio": 1.5724137931034483, "no_speech_prob": 3.822904545813799e-05}, {"id": 771, "seek": 363638, "start": 3657.6600000000003, "end": 3659.1800000000003, "text": " And so now that I've done that,", "tokens": [51428, 400, 370, 586, 300, 286, 600, 1096, 300, 11, 51504], "temperature": 0.0, "avg_logprob": -0.42655306277067767, "compression_ratio": 1.5724137931034483, "no_speech_prob": 3.822904545813799e-05}, {"id": 772, "seek": 365918, "start": 3659.18, "end": 3666.7, "text": " We now have, it ran almost instantly.", "tokens": [50364, 492, 586, 362, 11, 309, 5872, 1920, 13518, 13, 50740], "temperature": 0.0, "avg_logprob": -0.38000215424431694, "compression_ratio": 1.3888888888888888, "no_speech_prob": 3.8449352359748445e-06}, {"id": 773, "seek": 365918, "start": 3666.7, "end": 3670.98, "text": " It's now made all the biases and weights correct, give us 0, 1.", "tokens": [50740, 467, 311, 586, 1027, 439, 264, 32152, 293, 17443, 3006, 11, 976, 505, 1958, 11, 502, 13, 50954], "temperature": 0.0, "avg_logprob": -0.38000215424431694, "compression_ratio": 1.3888888888888888, "no_speech_prob": 3.8449352359748445e-06}, {"id": 774, "seek": 365918, "start": 3672.58, "end": 3675.74, "text": " And now if I train it, there it is.", "tokens": [51034, 400, 586, 498, 286, 3847, 309, 11, 456, 309, 307, 13, 51192], "temperature": 0.0, "avg_logprob": -0.38000215424431694, "compression_ratio": 1.3888888888888888, "no_speech_prob": 3.8449352359748445e-06}, {"id": 775, "seek": 365918, "start": 3675.74, "end": 3680.58, "text": " So we didn't do any initialization at all of the model,", "tokens": [51192, 407, 321, 994, 380, 360, 604, 5883, 2144, 412, 439, 295, 264, 2316, 11, 51434], "temperature": 0.0, "avg_logprob": -0.38000215424431694, "compression_ratio": 1.3888888888888888, "no_speech_prob": 3.8449352359748445e-06}, {"id": 776, "seek": 365918, "start": 3680.58, "end": 3682.5, "text": " other than just call lsuv init.", "tokens": [51434, 661, 813, 445, 818, 287, 82, 9350, 3157, 13, 51530], "temperature": 0.0, "avg_logprob": -0.38000215424431694, "compression_ratio": 1.3888888888888888, "no_speech_prob": 3.8449352359748445e-06}, {"id": 777, "seek": 368250, "start": 3682.5, "end": 3687.78, "text": " And this time we've got an accuracy of 0.86,", "tokens": [50364, 400, 341, 565, 321, 600, 658, 364, 14170, 295, 1958, 13, 22193, 11, 50628], "temperature": 0.0, "avg_logprob": -0.537090197001418, "compression_ratio": 1.4251497005988023, "no_speech_prob": 5.255395535641583e-06}, {"id": 778, "seek": 368250, "start": 3687.78, "end": 3692.26, "text": " versus previously it's 0.87.", "tokens": [50628, 5717, 8046, 309, 311, 1958, 13, 23853, 13, 50852], "temperature": 0.0, "avg_logprob": -0.537090197001418, "compression_ratio": 1.4251497005988023, "no_speech_prob": 5.255395535641583e-06}, {"id": 779, "seek": 368250, "start": 3692.26, "end": 3694.58, "text": " So pretty much the same thing, close enough.", "tokens": [50852, 407, 1238, 709, 264, 912, 551, 11, 1998, 1547, 13, 50968], "temperature": 0.0, "avg_logprob": -0.537090197001418, "compression_ratio": 1.4251497005988023, "no_speech_prob": 5.255395535641583e-06}, {"id": 780, "seek": 368250, "start": 3694.58, "end": 3696.98, "text": " And actually,", "tokens": [50968, 400, 767, 11, 51088], "temperature": 0.0, "avg_logprob": -0.537090197001418, "compression_ratio": 1.4251497005988023, "no_speech_prob": 5.255395535641583e-06}, {"id": 781, "seek": 368250, "start": 3704.18, "end": 3707.18, "text": " If you wanna actually see that happening, I guess what we could do,", "tokens": [51448, 759, 291, 1948, 767, 536, 300, 2737, 11, 286, 2041, 437, 321, 727, 360, 11, 51598], "temperature": 0.0, "avg_logprob": -0.537090197001418, "compression_ratio": 1.4251497005988023, "no_speech_prob": 5.255395535641583e-06}, {"id": 782, "seek": 368250, "start": 3707.18, "end": 3709.3, "text": " I mean, it's gonna be pretty obvious.", "tokens": [51598, 286, 914, 11, 309, 311, 799, 312, 1238, 6322, 13, 51704], "temperature": 0.0, "avg_logprob": -0.537090197001418, "compression_ratio": 1.4251497005988023, "no_speech_prob": 5.255395535641583e-06}, {"id": 783, "seek": 370930, "start": 3709.34, "end": 3717.3, "text": " After we've run this, we could say print h.mean, h.standard deviation.", "tokens": [50366, 2381, 321, 600, 1190, 341, 11, 321, 727, 584, 4482, 276, 13, 1398, 282, 11, 276, 13, 1115, 515, 25163, 13, 50764], "temperature": 0.0, "avg_logprob": -0.5487560807612905, "compression_ratio": 1.44, "no_speech_prob": 0.0003682943060994148}, {"id": 784, "seek": 370930, "start": 3719.82, "end": 3722.6200000000003, "text": " Actually, we could do it before and afterwards, right?", "tokens": [50890, 5135, 11, 321, 727, 360, 309, 949, 293, 10543, 11, 558, 30, 51030], "temperature": 0.0, "avg_logprob": -0.5487560807612905, "compression_ratio": 1.44, "no_speech_prob": 0.0003682943060994148}, {"id": 785, "seek": 370930, "start": 3722.6200000000003, "end": 3731.46, "text": " So we could say, right, before and after.", "tokens": [51030, 407, 321, 727, 584, 11, 558, 11, 949, 293, 934, 13, 51472], "temperature": 0.0, "avg_logprob": -0.5487560807612905, "compression_ratio": 1.44, "no_speech_prob": 0.0003682943060994148}, {"id": 786, "seek": 370930, "start": 3737.46, "end": 3738.1800000000003, "text": " There we go.", "tokens": [51772, 821, 321, 352, 13, 51808], "temperature": 0.0, "avg_logprob": -0.5487560807612905, "compression_ratio": 1.44, "no_speech_prob": 0.0003682943060994148}, {"id": 787, "seek": 373930, "start": 3740.3, "end": 3746.1800000000003, "text": " Yeah, so the first layer started at a mean of negative 0.13 and", "tokens": [50414, 865, 11, 370, 264, 700, 4583, 1409, 412, 257, 914, 295, 3671, 1958, 13, 7668, 293, 50708], "temperature": 0.0, "avg_logprob": -0.39420429711202976, "compression_ratio": 1.797029702970297, "no_speech_prob": 1.3007113011553884e-05}, {"id": 788, "seek": 373930, "start": 3746.1800000000003, "end": 3748.2200000000003, "text": " a variance of 0.46.", "tokens": [50708, 257, 21977, 295, 1958, 13, 16169, 13, 50810], "temperature": 0.0, "avg_logprob": -0.39420429711202976, "compression_ratio": 1.797029702970297, "no_speech_prob": 1.3007113011553884e-05}, {"id": 789, "seek": 373930, "start": 3748.2200000000003, "end": 3752.34, "text": " And it kept doing the divide, subtract, divide, subtract,", "tokens": [50810, 400, 309, 4305, 884, 264, 9845, 11, 16390, 11, 9845, 11, 16390, 11, 51016], "temperature": 0.0, "avg_logprob": -0.39420429711202976, "compression_ratio": 1.797029702970297, "no_speech_prob": 1.3007113011553884e-05}, {"id": 790, "seek": 373930, "start": 3752.34, "end": 3758.1400000000003, "text": " divide, subtract until eventually it got to mean is 0, standard deviation of 1.", "tokens": [51016, 9845, 11, 16390, 1826, 4728, 309, 658, 281, 914, 307, 1958, 11, 3832, 25163, 295, 502, 13, 51306], "temperature": 0.0, "avg_logprob": -0.39420429711202976, "compression_ratio": 1.797029702970297, "no_speech_prob": 1.3007113011553884e-05}, {"id": 791, "seek": 373930, "start": 3758.1400000000003, "end": 3762.5, "text": " And then it went to the next layer, and it kept going, going, going until that was 0,", "tokens": [51306, 400, 550, 309, 1437, 281, 264, 958, 4583, 11, 293, 309, 4305, 516, 11, 516, 11, 516, 1826, 300, 390, 1958, 11, 51524], "temperature": 0.0, "avg_logprob": -0.39420429711202976, "compression_ratio": 1.797029702970297, "no_speech_prob": 1.3007113011553884e-05}, {"id": 792, "seek": 373930, "start": 3762.5, "end": 3767.1000000000004, "text": " 1, and then the third layer, and then the fourth layer.", "tokens": [51524, 502, 11, 293, 550, 264, 2636, 4583, 11, 293, 550, 264, 6409, 4583, 13, 51754], "temperature": 0.0, "avg_logprob": -0.39420429711202976, "compression_ratio": 1.797029702970297, "no_speech_prob": 1.3007113011553884e-05}, {"id": 793, "seek": 376710, "start": 3767.1, "end": 3769.8199999999997, "text": " And so at that point, all of the layers had a mean of 0 and", "tokens": [50364, 400, 370, 412, 300, 935, 11, 439, 295, 264, 7914, 632, 257, 914, 295, 1958, 293, 50500], "temperature": 0.0, "avg_logprob": -0.29871683781689934, "compression_ratio": 1.5918367346938775, "no_speech_prob": 5.95513847656548e-06}, {"id": 794, "seek": 376710, "start": 3769.8199999999997, "end": 3770.86, "text": " a standard deviation of 1.", "tokens": [50500, 257, 3832, 25163, 295, 502, 13, 50552], "temperature": 0.0, "avg_logprob": -0.29871683781689934, "compression_ratio": 1.5918367346938775, "no_speech_prob": 5.95513847656548e-06}, {"id": 795, "seek": 376710, "start": 3776.58, "end": 3782.5, "text": " So I guess one thing with LSUV, it's kind of very mathematically convenient.", "tokens": [50838, 407, 286, 2041, 472, 551, 365, 441, 20214, 53, 11, 309, 311, 733, 295, 588, 44003, 10851, 13, 51134], "temperature": 0.0, "avg_logprob": -0.29871683781689934, "compression_ratio": 1.5918367346938775, "no_speech_prob": 5.95513847656548e-06}, {"id": 796, "seek": 376710, "start": 3782.5, "end": 3786.38, "text": " We don't have to spend any time thinking about if we've invented a new activation", "tokens": [51134, 492, 500, 380, 362, 281, 3496, 604, 565, 1953, 466, 498, 321, 600, 14479, 257, 777, 24433, 51328], "temperature": 0.0, "avg_logprob": -0.29871683781689934, "compression_ratio": 1.5918367346938775, "no_speech_prob": 5.95513847656548e-06}, {"id": 797, "seek": 376710, "start": 3786.38, "end": 3789.66, "text": " function or we're using some activation function where nobody seems to have", "tokens": [51328, 2445, 420, 321, 434, 1228, 512, 24433, 2445, 689, 5079, 2544, 281, 362, 51492], "temperature": 0.0, "avg_logprob": -0.29871683781689934, "compression_ratio": 1.5918367346938775, "no_speech_prob": 5.95513847656548e-06}, {"id": 798, "seek": 376710, "start": 3789.66, "end": 3793.1, "text": " figured out the correct initialization for it, we can just use LSUV.", "tokens": [51492, 8932, 484, 264, 3006, 5883, 2144, 337, 309, 11, 321, 393, 445, 764, 441, 20214, 53, 13, 51664], "temperature": 0.0, "avg_logprob": -0.29871683781689934, "compression_ratio": 1.5918367346938775, "no_speech_prob": 5.95513847656548e-06}, {"id": 799, "seek": 379310, "start": 3793.14, "end": 3797.5, "text": " It did require a little bit more fiddling around with hooks and", "tokens": [50366, 467, 630, 3651, 257, 707, 857, 544, 283, 14273, 1688, 926, 365, 26485, 293, 50584], "temperature": 0.0, "avg_logprob": -0.2719924863704965, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.00040447976789437234}, {"id": 800, "seek": 379310, "start": 3797.5, "end": 3801.7, "text": " stuff to get it to work, and I haven't even put this into a callback or anything.", "tokens": [50584, 1507, 281, 483, 309, 281, 589, 11, 293, 286, 2378, 380, 754, 829, 341, 666, 257, 818, 3207, 420, 1340, 13, 50794], "temperature": 0.0, "avg_logprob": -0.2719924863704965, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.00040447976789437234}, {"id": 801, "seek": 379310, "start": 3803.14, "end": 3807.62, "text": " So if you decide you want to try using this in some of your models,", "tokens": [50866, 407, 498, 291, 4536, 291, 528, 281, 853, 1228, 341, 294, 512, 295, 428, 5245, 11, 51090], "temperature": 0.0, "avg_logprob": -0.2719924863704965, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.00040447976789437234}, {"id": 802, "seek": 379310, "start": 3807.62, "end": 3809.18, "text": " it might be a good idea.", "tokens": [51090, 309, 1062, 312, 257, 665, 1558, 13, 51168], "temperature": 0.0, "avg_logprob": -0.2719924863704965, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.00040447976789437234}, {"id": 803, "seek": 379310, "start": 3809.18, "end": 3812.94, "text": " And it'd actually be good homework to see if you can come up with", "tokens": [51168, 400, 309, 1116, 767, 312, 665, 14578, 281, 536, 498, 291, 393, 808, 493, 365, 51356], "temperature": 0.0, "avg_logprob": -0.2719924863704965, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.00040447976789437234}, {"id": 804, "seek": 379310, "start": 3812.94, "end": 3817.2599999999998, "text": " a callback that does LSUV initialization for you.", "tokens": [51356, 257, 818, 3207, 300, 775, 441, 20214, 53, 5883, 2144, 337, 291, 13, 51572], "temperature": 0.0, "avg_logprob": -0.2719924863704965, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.00040447976789437234}, {"id": 805, "seek": 379310, "start": 3817.2599999999998, "end": 3822.14, "text": " That would be pretty cool, wouldn't it, in before fit, I guess it would be.", "tokens": [51572, 663, 576, 312, 1238, 1627, 11, 2759, 380, 309, 11, 294, 949, 3318, 11, 286, 2041, 309, 576, 312, 13, 51816], "temperature": 0.0, "avg_logprob": -0.2719924863704965, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.00040447976789437234}, {"id": 806, "seek": 382310, "start": 3824.1, "end": 3828.66, "text": " You'd have to be a bit careful because if you ran fit multiple times,", "tokens": [50414, 509, 1116, 362, 281, 312, 257, 857, 5026, 570, 498, 291, 5872, 3318, 3866, 1413, 11, 50642], "temperature": 0.0, "avg_logprob": -0.3036014155337685, "compression_ratio": 1.651376146788991, "no_speech_prob": 6.681515287709772e-07}, {"id": 807, "seek": 382310, "start": 3828.66, "end": 3831.7, "text": " it would actually initialize it each time.", "tokens": [50642, 309, 576, 767, 5883, 1125, 309, 1184, 565, 13, 50794], "temperature": 0.0, "avg_logprob": -0.3036014155337685, "compression_ratio": 1.651376146788991, "no_speech_prob": 6.681515287709772e-07}, {"id": 808, "seek": 382310, "start": 3831.7, "end": 3834.2599999999998, "text": " So that would be one issue with that to think about.", "tokens": [50794, 407, 300, 576, 312, 472, 2734, 365, 300, 281, 519, 466, 13, 50922], "temperature": 0.0, "avg_logprob": -0.3036014155337685, "compression_ratio": 1.651376146788991, "no_speech_prob": 6.681515287709772e-07}, {"id": 809, "seek": 382310, "start": 3835.74, "end": 3842.7, "text": " Okay, so something which is quite similar to LSUV is batch normalization.", "tokens": [50996, 1033, 11, 370, 746, 597, 307, 1596, 2531, 281, 441, 20214, 53, 307, 15245, 2710, 2144, 13, 51344], "temperature": 0.0, "avg_logprob": -0.3036014155337685, "compression_ratio": 1.651376146788991, "no_speech_prob": 6.681515287709772e-07}, {"id": 810, "seek": 382310, "start": 3842.7, "end": 3847.66, "text": " So we're gonna have a seven minute break, and", "tokens": [51344, 407, 321, 434, 799, 362, 257, 3407, 3456, 1821, 11, 293, 51592], "temperature": 0.0, "avg_logprob": -0.3036014155337685, "compression_ratio": 1.651376146788991, "no_speech_prob": 6.681515287709772e-07}, {"id": 811, "seek": 382310, "start": 3847.66, "end": 3851.74, "text": " then we're gonna come back and we're gonna talk about batch normalization.", "tokens": [51592, 550, 321, 434, 799, 808, 646, 293, 321, 434, 799, 751, 466, 15245, 2710, 2144, 13, 51796], "temperature": 0.0, "avg_logprob": -0.3036014155337685, "compression_ratio": 1.651376146788991, "no_speech_prob": 6.681515287709772e-07}, {"id": 812, "seek": 385174, "start": 3851.8199999999997, "end": 3853.3799999999997, "text": " See you in seven minutes.", "tokens": [50368, 3008, 291, 294, 3407, 2077, 13, 50446], "temperature": 0.0, "avg_logprob": -0.42215229914738583, "compression_ratio": 1.3896103896103895, "no_speech_prob": 0.0001442588254576549}, {"id": 813, "seek": 385174, "start": 3856.8999999999996, "end": 3860.4599999999996, "text": " Okay, hi, let's do this, batch normalization.", "tokens": [50622, 1033, 11, 4879, 11, 718, 311, 360, 341, 11, 15245, 2710, 2144, 13, 50800], "temperature": 0.0, "avg_logprob": -0.42215229914738583, "compression_ratio": 1.3896103896103895, "no_speech_prob": 0.0001442588254576549}, {"id": 814, "seek": 385174, "start": 3861.9799999999996, "end": 3865.5, "text": " Batch normalization was such an important paper.", "tokens": [50876, 363, 852, 2710, 2144, 390, 1270, 364, 1021, 3035, 13, 51052], "temperature": 0.0, "avg_logprob": -0.42215229914738583, "compression_ratio": 1.3896103896103895, "no_speech_prob": 0.0001442588254576549}, {"id": 815, "seek": 385174, "start": 3867.3399999999997, "end": 3873.4599999999996, "text": " I remember when it came out, I was at Analytic, my medical startup.", "tokens": [51144, 286, 1604, 562, 309, 1361, 484, 11, 286, 390, 412, 23688, 299, 11, 452, 4625, 18578, 13, 51450], "temperature": 0.0, "avg_logprob": -0.42215229914738583, "compression_ratio": 1.3896103896103895, "no_speech_prob": 0.0001442588254576549}, {"id": 816, "seek": 385174, "start": 3873.4599999999996, "end": 3876.5, "text": " And I think that's right.", "tokens": [51450, 400, 286, 519, 300, 311, 558, 13, 51602], "temperature": 0.0, "avg_logprob": -0.42215229914738583, "compression_ratio": 1.3896103896103895, "no_speech_prob": 0.0001442588254576549}, {"id": 817, "seek": 387650, "start": 3877.5, "end": 3882.18, "text": " And everybody was talking about it.", "tokens": [50414, 400, 2201, 390, 1417, 466, 309, 13, 50648], "temperature": 0.0, "avg_logprob": -0.6430760241569357, "compression_ratio": 1.348148148148148, "no_speech_prob": 3.288748530394514e-06}, {"id": 818, "seek": 387650, "start": 3882.18, "end": 3884.86, "text": " And in particular,", "tokens": [50648, 400, 294, 1729, 11, 50782], "temperature": 0.0, "avg_logprob": -0.6430760241569357, "compression_ratio": 1.348148148148148, "no_speech_prob": 3.288748530394514e-06}, {"id": 819, "seek": 387650, "start": 3889.46, "end": 3895.5, "text": " They were talking about this graph that basically showed", "tokens": [51012, 814, 645, 1417, 466, 341, 4295, 300, 1936, 4712, 51314], "temperature": 0.0, "avg_logprob": -0.6430760241569357, "compression_ratio": 1.348148148148148, "no_speech_prob": 3.288748530394514e-06}, {"id": 820, "seek": 387650, "start": 3895.5, "end": 3902.7, "text": " what it used to be like until batch norm to train a model on ImageNet.", "tokens": [51314, 437, 309, 1143, 281, 312, 411, 1826, 15245, 2026, 281, 3847, 257, 2316, 322, 29903, 31890, 13, 51674], "temperature": 0.0, "avg_logprob": -0.6430760241569357, "compression_ratio": 1.348148148148148, "no_speech_prob": 3.288748530394514e-06}, {"id": 821, "seek": 390270, "start": 3902.7, "end": 3909.66, "text": " How many training steps you'd have to do to get to a certain accuracy.", "tokens": [50364, 1012, 867, 3097, 4439, 291, 1116, 362, 281, 360, 281, 483, 281, 257, 1629, 14170, 13, 50712], "temperature": 0.0, "avg_logprob": -0.3823990225791931, "compression_ratio": 1.3870967741935485, "no_speech_prob": 1.0783307516248897e-05}, {"id": 822, "seek": 390270, "start": 3912.18, "end": 3916.8199999999997, "text": " And then they showed what you could do with batch norm.", "tokens": [50838, 400, 550, 436, 4712, 437, 291, 727, 360, 365, 15245, 2026, 13, 51070], "temperature": 0.0, "avg_logprob": -0.3823990225791931, "compression_ratio": 1.3870967741935485, "no_speech_prob": 1.0783307516248897e-05}, {"id": 823, "seek": 390270, "start": 3920.1, "end": 3922.9399999999996, "text": " So much faster, it was amazing.", "tokens": [51234, 407, 709, 4663, 11, 309, 390, 2243, 13, 51376], "temperature": 0.0, "avg_logprob": -0.3823990225791931, "compression_ratio": 1.3870967741935485, "no_speech_prob": 1.0783307516248897e-05}, {"id": 824, "seek": 390270, "start": 3922.9399999999996, "end": 3925.7799999999997, "text": " And we all thought, that can't be true.", "tokens": [51376, 400, 321, 439, 1194, 11, 300, 393, 380, 312, 2074, 13, 51518], "temperature": 0.0, "avg_logprob": -0.3823990225791931, "compression_ratio": 1.3870967741935485, "no_speech_prob": 1.0783307516248897e-05}, {"id": 825, "seek": 390270, "start": 3925.7799999999997, "end": 3926.3799999999997, "text": " But it was true.", "tokens": [51518, 583, 309, 390, 2074, 13, 51548], "temperature": 0.0, "avg_logprob": -0.3823990225791931, "compression_ratio": 1.3870967741935485, "no_speech_prob": 1.0783307516248897e-05}, {"id": 826, "seek": 392638, "start": 3927.38, "end": 3934.2200000000003, "text": " So basically, the key idea of batch norm is that with LSUV and", "tokens": [50414, 407, 1936, 11, 264, 2141, 1558, 295, 15245, 2026, 307, 300, 365, 441, 20214, 53, 293, 50756], "temperature": 0.0, "avg_logprob": -0.5371938902756264, "compression_ratio": 1.530612244897959, "no_speech_prob": 5.422216418082826e-06}, {"id": 827, "seek": 392638, "start": 3934.2200000000003, "end": 3938.42, "text": " input normalization and timing in it,", "tokens": [50756, 294, 34859, 83, 2710, 2144, 293, 10822, 294, 309, 11, 50966], "temperature": 0.0, "avg_logprob": -0.5371938902756264, "compression_ratio": 1.530612244897959, "no_speech_prob": 5.422216418082826e-06}, {"id": 828, "seek": 392638, "start": 3938.42, "end": 3945.54, "text": " we are normalizing each layer's inputs before training.", "tokens": [50966, 321, 366, 2710, 3319, 1184, 4583, 311, 15743, 949, 3097, 13, 51322], "temperature": 0.0, "avg_logprob": -0.5371938902756264, "compression_ratio": 1.530612244897959, "no_speech_prob": 5.422216418082826e-06}, {"id": 829, "seek": 392638, "start": 3945.54, "end": 3953.02, "text": " But the distribution of each layer's inputs changes during training.", "tokens": [51322, 583, 264, 7316, 295, 1184, 4583, 311, 15743, 2962, 1830, 3097, 13, 51696], "temperature": 0.0, "avg_logprob": -0.5371938902756264, "compression_ratio": 1.530612244897959, "no_speech_prob": 5.422216418082826e-06}, {"id": 830, "seek": 395638, "start": 3957.38, "end": 3959.46, "text": " And that's a problem.", "tokens": [50414, 400, 300, 311, 257, 1154, 13, 50518], "temperature": 0.0, "avg_logprob": -0.3190901080767314, "compression_ratio": 1.5854700854700854, "no_speech_prob": 1.497107314207824e-05}, {"id": 831, "seek": 395638, "start": 3961.1, "end": 3964.42, "text": " So you end up having to decrease your learning rates.", "tokens": [50600, 407, 291, 917, 493, 1419, 281, 11514, 428, 2539, 6846, 13, 50766], "temperature": 0.0, "avg_logprob": -0.3190901080767314, "compression_ratio": 1.5854700854700854, "no_speech_prob": 1.497107314207824e-05}, {"id": 832, "seek": 395638, "start": 3964.42, "end": 3967.6600000000003, "text": " And as we've seen, you'd have to be very careful about parameter initialization.", "tokens": [50766, 400, 382, 321, 600, 1612, 11, 291, 1116, 362, 281, 312, 588, 5026, 466, 13075, 5883, 2144, 13, 50928], "temperature": 0.0, "avg_logprob": -0.3190901080767314, "compression_ratio": 1.5854700854700854, "no_speech_prob": 1.497107314207824e-05}, {"id": 833, "seek": 395638, "start": 3969.54, "end": 3973.82, "text": " So the fact that the layers inputs change during training,", "tokens": [51022, 407, 264, 1186, 300, 264, 7914, 15743, 1319, 1830, 3097, 11, 51236], "temperature": 0.0, "avg_logprob": -0.3190901080767314, "compression_ratio": 1.5854700854700854, "no_speech_prob": 1.497107314207824e-05}, {"id": 834, "seek": 395638, "start": 3973.82, "end": 3975.82, "text": " they call internal covariate shift.", "tokens": [51236, 436, 818, 6920, 49851, 473, 5513, 13, 51336], "temperature": 0.0, "avg_logprob": -0.3190901080767314, "compression_ratio": 1.5854700854700854, "no_speech_prob": 1.497107314207824e-05}, {"id": 835, "seek": 395638, "start": 3975.82, "end": 3979.26, "text": " Which for some reason, a lot of people tend to find a confusing statement or", "tokens": [51336, 3013, 337, 512, 1778, 11, 257, 688, 295, 561, 3928, 281, 915, 257, 13181, 5629, 420, 51508], "temperature": 0.0, "avg_logprob": -0.3190901080767314, "compression_ratio": 1.5854700854700854, "no_speech_prob": 1.497107314207824e-05}, {"id": 836, "seek": 395638, "start": 3979.26, "end": 3983.3, "text": " confusing name, but it's very clear to me.", "tokens": [51508, 13181, 1315, 11, 457, 309, 311, 588, 1850, 281, 385, 13, 51710], "temperature": 0.0, "avg_logprob": -0.3190901080767314, "compression_ratio": 1.5854700854700854, "no_speech_prob": 1.497107314207824e-05}, {"id": 837, "seek": 398330, "start": 3983.3, "end": 3988.0600000000004, "text": " And you can fix it by normalizing layer inputs during training.", "tokens": [50364, 400, 291, 393, 3191, 309, 538, 2710, 3319, 4583, 15743, 1830, 3097, 13, 50602], "temperature": 0.0, "avg_logprob": -0.2516460044711244, "compression_ratio": 1.8133333333333332, "no_speech_prob": 3.7853246794838924e-06}, {"id": 838, "seek": 398330, "start": 3989.82, "end": 3993.1000000000004, "text": " So you're making the normalization a part of the model architecture.", "tokens": [50690, 407, 291, 434, 1455, 264, 2710, 2144, 257, 644, 295, 264, 2316, 9482, 13, 50854], "temperature": 0.0, "avg_logprob": -0.2516460044711244, "compression_ratio": 1.8133333333333332, "no_speech_prob": 3.7853246794838924e-06}, {"id": 839, "seek": 398330, "start": 3993.1000000000004, "end": 3996.5800000000004, "text": " And you perform the normalization for each mini-batch.", "tokens": [50854, 400, 291, 2042, 264, 2710, 2144, 337, 1184, 8382, 12, 65, 852, 13, 51028], "temperature": 0.0, "avg_logprob": -0.2516460044711244, "compression_ratio": 1.8133333333333332, "no_speech_prob": 3.7853246794838924e-06}, {"id": 840, "seek": 398330, "start": 3996.5800000000004, "end": 3999.38, "text": " Now, I'm actually not gonna start with batch normalization.", "tokens": [51028, 823, 11, 286, 478, 767, 406, 799, 722, 365, 15245, 2710, 2144, 13, 51168], "temperature": 0.0, "avg_logprob": -0.2516460044711244, "compression_ratio": 1.8133333333333332, "no_speech_prob": 3.7853246794838924e-06}, {"id": 841, "seek": 398330, "start": 3999.38, "end": 4002.1000000000004, "text": " I'm gonna start with something that came out one year later,", "tokens": [51168, 286, 478, 799, 722, 365, 746, 300, 1361, 484, 472, 1064, 1780, 11, 51304], "temperature": 0.0, "avg_logprob": -0.2516460044711244, "compression_ratio": 1.8133333333333332, "no_speech_prob": 3.7853246794838924e-06}, {"id": 842, "seek": 398330, "start": 4002.1000000000004, "end": 4006.34, "text": " called layer normalization, because layer normalization is simpler.", "tokens": [51304, 1219, 4583, 2710, 2144, 11, 570, 4583, 2710, 2144, 307, 18587, 13, 51516], "temperature": 0.0, "avg_logprob": -0.2516460044711244, "compression_ratio": 1.8133333333333332, "no_speech_prob": 3.7853246794838924e-06}, {"id": 843, "seek": 398330, "start": 4006.34, "end": 4007.6600000000003, "text": " Let's do the simpler one first.", "tokens": [51516, 961, 311, 360, 264, 18587, 472, 700, 13, 51582], "temperature": 0.0, "avg_logprob": -0.2516460044711244, "compression_ratio": 1.8133333333333332, "no_speech_prob": 3.7853246794838924e-06}, {"id": 844, "seek": 400766, "start": 4008.66, "end": 4013.7, "text": " So layer normalization came out as this group of fellows,", "tokens": [50414, 407, 4583, 2710, 2144, 1361, 484, 382, 341, 1594, 295, 35595, 11, 50666], "temperature": 0.0, "avg_logprob": -0.3960016965866089, "compression_ratio": 1.6066350710900474, "no_speech_prob": 4.28933117291308e-06}, {"id": 845, "seek": 400766, "start": 4013.7, "end": 4017.2599999999998, "text": " the last of whom I'm sure you've heard of.", "tokens": [50666, 264, 1036, 295, 7101, 286, 478, 988, 291, 600, 2198, 295, 13, 50844], "temperature": 0.0, "avg_logprob": -0.3960016965866089, "compression_ratio": 1.6066350710900474, "no_speech_prob": 4.28933117291308e-06}, {"id": 846, "seek": 400766, "start": 4017.2599999999998, "end": 4023.54, "text": " And it's probably easiest to explain by showing you the code.", "tokens": [50844, 400, 309, 311, 1391, 12889, 281, 2903, 538, 4099, 291, 264, 3089, 13, 51158], "temperature": 0.0, "avg_logprob": -0.3960016965866089, "compression_ratio": 1.6066350710900474, "no_speech_prob": 4.28933117291308e-06}, {"id": 847, "seek": 400766, "start": 4023.54, "end": 4027.2599999999998, "text": " So if you're thinking layer normalization, wow,", "tokens": [51158, 407, 498, 291, 434, 1953, 4583, 2710, 2144, 11, 6076, 11, 51344], "temperature": 0.0, "avg_logprob": -0.3960016965866089, "compression_ratio": 1.6066350710900474, "no_speech_prob": 4.28933117291308e-06}, {"id": 848, "seek": 400766, "start": 4027.2599999999998, "end": 4032.8999999999996, "text": " it's a whole paper, a Geoffrey Hinton paper, must be complicated.", "tokens": [51344, 309, 311, 257, 1379, 3035, 11, 257, 26119, 7950, 389, 12442, 3035, 11, 1633, 312, 6179, 13, 51626], "temperature": 0.0, "avg_logprob": -0.3960016965866089, "compression_ratio": 1.6066350710900474, "no_speech_prob": 4.28933117291308e-06}, {"id": 849, "seek": 400766, "start": 4032.8999999999996, "end": 4034.18, "text": " No, the whole thing is this code.", "tokens": [51626, 883, 11, 264, 1379, 551, 307, 341, 3089, 13, 51690], "temperature": 0.0, "avg_logprob": -0.3960016965866089, "compression_ratio": 1.6066350710900474, "no_speech_prob": 4.28933117291308e-06}, {"id": 850, "seek": 400766, "start": 4035.2999999999997, "end": 4036.74, "text": " What is layer normalization?", "tokens": [51746, 708, 307, 4583, 2710, 2144, 30, 51818], "temperature": 0.0, "avg_logprob": -0.3960016965866089, "compression_ratio": 1.6066350710900474, "no_speech_prob": 4.28933117291308e-06}, {"id": 851, "seek": 403674, "start": 4036.74, "end": 4037.8199999999997, "text": " Well, we can create a module.", "tokens": [50364, 1042, 11, 321, 393, 1884, 257, 10088, 13, 50418], "temperature": 0.0, "avg_logprob": -0.3228865694408574, "compression_ratio": 1.9051724137931034, "no_speech_prob": 3.9669853322266135e-06}, {"id": 852, "seek": 403674, "start": 4039.66, "end": 4043.4199999999996, "text": " And we're going to pass in,", "tokens": [50510, 400, 321, 434, 516, 281, 1320, 294, 11, 50698], "temperature": 0.0, "avg_logprob": -0.3228865694408574, "compression_ratio": 1.9051724137931034, "no_speech_prob": 3.9669853322266135e-06}, {"id": 853, "seek": 403674, "start": 4043.4199999999996, "end": 4046.58, "text": " we don't really need to pass in anything, actually.", "tokens": [50698, 321, 500, 380, 534, 643, 281, 1320, 294, 1340, 11, 767, 13, 50856], "temperature": 0.0, "avg_logprob": -0.3228865694408574, "compression_ratio": 1.9051724137931034, "no_speech_prob": 3.9669853322266135e-06}, {"id": 854, "seek": 403674, "start": 4046.58, "end": 4048.5, "text": " You can totally ignore the parameters for now.", "tokens": [50856, 509, 393, 3879, 11200, 264, 9834, 337, 586, 13, 50952], "temperature": 0.0, "avg_logprob": -0.3228865694408574, "compression_ratio": 1.9051724137931034, "no_speech_prob": 3.9669853322266135e-06}, {"id": 855, "seek": 403674, "start": 4048.5, "end": 4053.3399999999997, "text": " In fact, what we're gonna do is we're gonna have a single number called mult,", "tokens": [50952, 682, 1186, 11, 437, 321, 434, 799, 360, 307, 321, 434, 799, 362, 257, 2167, 1230, 1219, 2120, 11, 51194], "temperature": 0.0, "avg_logprob": -0.3228865694408574, "compression_ratio": 1.9051724137931034, "no_speech_prob": 3.9669853322266135e-06}, {"id": 856, "seek": 403674, "start": 4053.3399999999997, "end": 4057.18, "text": " for the multiplier, and a single number called add, that's the thing we're gonna", "tokens": [51194, 337, 264, 44106, 11, 293, 257, 2167, 1230, 1219, 909, 11, 300, 311, 264, 551, 321, 434, 799, 51386], "temperature": 0.0, "avg_logprob": -0.3228865694408574, "compression_ratio": 1.9051724137931034, "no_speech_prob": 3.9669853322266135e-06}, {"id": 857, "seek": 403674, "start": 4057.18, "end": 4061.58, "text": " add, and we're gonna start off by multiplying things by one and adding zero.", "tokens": [51386, 909, 11, 293, 321, 434, 799, 722, 766, 538, 30955, 721, 538, 472, 293, 5127, 4018, 13, 51606], "temperature": 0.0, "avg_logprob": -0.3228865694408574, "compression_ratio": 1.9051724137931034, "no_speech_prob": 3.9669853322266135e-06}, {"id": 858, "seek": 403674, "start": 4061.58, "end": 4064.4199999999996, "text": " So we're gonna start off by doing nothing at all.", "tokens": [51606, 407, 321, 434, 799, 722, 766, 538, 884, 1825, 412, 439, 13, 51748], "temperature": 0.0, "avg_logprob": -0.3228865694408574, "compression_ratio": 1.9051724137931034, "no_speech_prob": 3.9669853322266135e-06}, {"id": 859, "seek": 406442, "start": 4065.42, "end": 4068.66, "text": " Okay, this is the layer, it has a forward function.", "tokens": [50414, 1033, 11, 341, 307, 264, 4583, 11, 309, 575, 257, 2128, 2445, 13, 50576], "temperature": 0.0, "avg_logprob": -0.37954556102484044, "compression_ratio": 1.5324675324675325, "no_speech_prob": 1.5057016753416974e-06}, {"id": 860, "seek": 406442, "start": 4068.66, "end": 4073.42, "text": " And in the forward function, so", "tokens": [50576, 400, 294, 264, 2128, 2445, 11, 370, 50814], "temperature": 0.0, "avg_logprob": -0.37954556102484044, "compression_ratio": 1.5324675324675325, "no_speech_prob": 1.5057016753416974e-06}, {"id": 861, "seek": 406442, "start": 4073.42, "end": 4081.42, "text": " remember that by default we have n, c, h, w.", "tokens": [50814, 1604, 300, 538, 7576, 321, 362, 297, 11, 269, 11, 276, 11, 261, 13, 51214], "temperature": 0.0, "avg_logprob": -0.37954556102484044, "compression_ratio": 1.5324675324675325, "no_speech_prob": 1.5057016753416974e-06}, {"id": 862, "seek": 406442, "start": 4081.42, "end": 4086.66, "text": " We have batch by channel by height by width.", "tokens": [51214, 492, 362, 15245, 538, 2269, 538, 6681, 538, 11402, 13, 51476], "temperature": 0.0, "avg_logprob": -0.37954556102484044, "compression_ratio": 1.5324675324675325, "no_speech_prob": 1.5057016753416974e-06}, {"id": 863, "seek": 406442, "start": 4086.66, "end": 4092.9, "text": " We're gonna take the mean over the channel, height, and width.", "tokens": [51476, 492, 434, 799, 747, 264, 914, 670, 264, 2269, 11, 6681, 11, 293, 11402, 13, 51788], "temperature": 0.0, "avg_logprob": -0.37954556102484044, "compression_ratio": 1.5324675324675325, "no_speech_prob": 1.5057016753416974e-06}, {"id": 864, "seek": 409290, "start": 4092.9, "end": 4099.9800000000005, "text": " So we're just gonna find the mean activation for each input in the mini-batch.", "tokens": [50364, 407, 321, 434, 445, 799, 915, 264, 914, 24433, 337, 1184, 4846, 294, 264, 8382, 12, 65, 852, 13, 50718], "temperature": 0.0, "avg_logprob": -0.2967503371747952, "compression_ratio": 1.6211453744493391, "no_speech_prob": 5.36874949830235e-07}, {"id": 865, "seek": 409290, "start": 4099.9800000000005, "end": 4103.5, "text": " And when I say input, though, remember that this is gonna be,", "tokens": [50718, 400, 562, 286, 584, 4846, 11, 1673, 11, 1604, 300, 341, 307, 799, 312, 11, 50894], "temperature": 0.0, "avg_logprob": -0.2967503371747952, "compression_ratio": 1.6211453744493391, "no_speech_prob": 5.36874949830235e-07}, {"id": 866, "seek": 409290, "start": 4103.5, "end": 4104.54, "text": " this is a layer, right?", "tokens": [50894, 341, 307, 257, 4583, 11, 558, 30, 50946], "temperature": 0.0, "avg_logprob": -0.2967503371747952, "compression_ratio": 1.6211453744493391, "no_speech_prob": 5.36874949830235e-07}, {"id": 867, "seek": 409290, "start": 4104.54, "end": 4108.54, "text": " So we can put this layer anywhere we like, so it's the input to that layer.", "tokens": [50946, 407, 321, 393, 829, 341, 4583, 4992, 321, 411, 11, 370, 309, 311, 264, 4846, 281, 300, 4583, 13, 51146], "temperature": 0.0, "avg_logprob": -0.2967503371747952, "compression_ratio": 1.6211453744493391, "no_speech_prob": 5.36874949830235e-07}, {"id": 868, "seek": 409290, "start": 4108.54, "end": 4112.9400000000005, "text": " And we'll do the same thing for finding the variance.", "tokens": [51146, 400, 321, 603, 360, 264, 912, 551, 337, 5006, 264, 21977, 13, 51366], "temperature": 0.0, "avg_logprob": -0.2967503371747952, "compression_ratio": 1.6211453744493391, "no_speech_prob": 5.36874949830235e-07}, {"id": 869, "seek": 409290, "start": 4115.9400000000005, "end": 4122.46, "text": " Okay, and then we're going to normalize our data by subtracting the mean.", "tokens": [51516, 1033, 11, 293, 550, 321, 434, 516, 281, 2710, 1125, 527, 1412, 538, 16390, 278, 264, 914, 13, 51842], "temperature": 0.0, "avg_logprob": -0.2967503371747952, "compression_ratio": 1.6211453744493391, "no_speech_prob": 5.36874949830235e-07}, {"id": 870, "seek": 412290, "start": 4123.58, "end": 4126.98, "text": " And dividing by the square root of the variance,", "tokens": [50398, 400, 26764, 538, 264, 3732, 5593, 295, 264, 21977, 11, 50568], "temperature": 0.0, "avg_logprob": -0.3405676109846248, "compression_ratio": 1.5933014354066986, "no_speech_prob": 2.9944410471216543e-06}, {"id": 871, "seek": 412290, "start": 4126.98, "end": 4130.62, "text": " which of course is the standard deviation.", "tokens": [50568, 597, 295, 1164, 307, 264, 3832, 25163, 13, 50750], "temperature": 0.0, "avg_logprob": -0.3405676109846248, "compression_ratio": 1.5933014354066986, "no_speech_prob": 2.9944410471216543e-06}, {"id": 872, "seek": 412290, "start": 4132.66, "end": 4139.94, "text": " We're going to add a very small number by default, 1 eneg 5, to the denominator.", "tokens": [50852, 492, 434, 516, 281, 909, 257, 588, 1359, 1230, 538, 7576, 11, 502, 465, 1146, 1025, 11, 281, 264, 20687, 13, 51216], "temperature": 0.0, "avg_logprob": -0.3405676109846248, "compression_ratio": 1.5933014354066986, "no_speech_prob": 2.9944410471216543e-06}, {"id": 873, "seek": 412290, "start": 4139.94, "end": 4143.98, "text": " Just in case the variance is 0 or ridiculously small,", "tokens": [51216, 1449, 294, 1389, 264, 21977, 307, 1958, 420, 41358, 1359, 11, 51418], "temperature": 0.0, "avg_logprob": -0.3405676109846248, "compression_ratio": 1.5933014354066986, "no_speech_prob": 2.9944410471216543e-06}, {"id": 874, "seek": 412290, "start": 4143.98, "end": 4146.379999999999, "text": " this will keep the number from going giant,", "tokens": [51418, 341, 486, 1066, 264, 1230, 490, 516, 7410, 11, 51538], "temperature": 0.0, "avg_logprob": -0.3405676109846248, "compression_ratio": 1.5933014354066986, "no_speech_prob": 2.9944410471216543e-06}, {"id": 875, "seek": 412290, "start": 4146.379999999999, "end": 4149.219999999999, "text": " just if we happen to get something with a very small variance.", "tokens": [51538, 445, 498, 321, 1051, 281, 483, 746, 365, 257, 588, 1359, 21977, 13, 51680], "temperature": 0.0, "avg_logprob": -0.3405676109846248, "compression_ratio": 1.5933014354066986, "no_speech_prob": 2.9944410471216543e-06}, {"id": 876, "seek": 414922, "start": 4150.22, "end": 4155.02, "text": " This idea of an epsilon as being something we add to a divisor is really,", "tokens": [50414, 639, 1558, 295, 364, 17889, 382, 885, 746, 321, 909, 281, 257, 25974, 284, 307, 534, 11, 50654], "temperature": 0.0, "avg_logprob": -0.40595766393149774, "compression_ratio": 1.5555555555555556, "no_speech_prob": 3.576358835744031e-07}, {"id": 877, "seek": 414922, "start": 4155.02, "end": 4157.06, "text": " really common.", "tokens": [50654, 534, 2689, 13, 50756], "temperature": 0.0, "avg_logprob": -0.40595766393149774, "compression_ratio": 1.5555555555555556, "no_speech_prob": 3.576358835744031e-07}, {"id": 878, "seek": 414922, "start": 4157.06, "end": 4159.9400000000005, "text": " And in general, you should not assume that the defaults are correct.", "tokens": [50756, 400, 294, 2674, 11, 291, 820, 406, 6552, 300, 264, 7576, 82, 366, 3006, 13, 50900], "temperature": 0.0, "avg_logprob": -0.40595766393149774, "compression_ratio": 1.5555555555555556, "no_speech_prob": 3.576358835744031e-07}, {"id": 879, "seek": 414922, "start": 4159.9400000000005, "end": 4165.740000000001, "text": " Very often, the defaults are too small for algorithms that use an epsilon.", "tokens": [50900, 4372, 2049, 11, 264, 7576, 82, 366, 886, 1359, 337, 14642, 300, 764, 364, 17889, 13, 51190], "temperature": 0.0, "avg_logprob": -0.40595766393149774, "compression_ratio": 1.5555555555555556, "no_speech_prob": 3.576358835744031e-07}, {"id": 880, "seek": 414922, "start": 4167.9400000000005, "end": 4172.7, "text": " Okay, so here we are, as you can see,", "tokens": [51300, 1033, 11, 370, 510, 321, 366, 11, 382, 291, 393, 536, 11, 51538], "temperature": 0.0, "avg_logprob": -0.40595766393149774, "compression_ratio": 1.5555555555555556, "no_speech_prob": 3.576358835744031e-07}, {"id": 881, "seek": 414922, "start": 4172.7, "end": 4178.02, "text": " we are normalizing the,", "tokens": [51538, 321, 366, 2710, 3319, 264, 11, 51804], "temperature": 0.0, "avg_logprob": -0.40595766393149774, "compression_ratio": 1.5555555555555556, "no_speech_prob": 3.576358835744031e-07}, {"id": 882, "seek": 417922, "start": 4180.22, "end": 4182.06, "text": " The batch.", "tokens": [50414, 440, 15245, 13, 50506], "temperature": 0.0, "avg_logprob": -0.3113142378786777, "compression_ratio": 1.6127450980392157, "no_speech_prob": 4.539784276857972e-05}, {"id": 883, "seek": 417922, "start": 4182.06, "end": 4185.22, "text": " I mean, I can call it a batch, but just remember,", "tokens": [50506, 286, 914, 11, 286, 393, 818, 309, 257, 15245, 11, 457, 445, 1604, 11, 50664], "temperature": 0.0, "avg_logprob": -0.3113142378786777, "compression_ratio": 1.6127450980392157, "no_speech_prob": 4.539784276857972e-05}, {"id": 884, "seek": 417922, "start": 4185.22, "end": 4187.7, "text": " this isn't necessarily the first layer, right?", "tokens": [50664, 341, 1943, 380, 4725, 264, 700, 4583, 11, 558, 30, 50788], "temperature": 0.0, "avg_logprob": -0.3113142378786777, "compression_ratio": 1.6127450980392157, "no_speech_prob": 4.539784276857972e-05}, {"id": 885, "seek": 417922, "start": 4187.7, "end": 4192.14, "text": " So it's whichever layer we decide to put this in, so we normalize it.", "tokens": [50788, 407, 309, 311, 24123, 4583, 321, 4536, 281, 829, 341, 294, 11, 370, 321, 2710, 1125, 309, 13, 51010], "temperature": 0.0, "avg_logprob": -0.3113142378786777, "compression_ratio": 1.6127450980392157, "no_speech_prob": 4.539784276857972e-05}, {"id": 886, "seek": 417922, "start": 4192.14, "end": 4199.42, "text": " Now the thing is, maybe we don't want it to be normalized.", "tokens": [51010, 823, 264, 551, 307, 11, 1310, 321, 500, 380, 528, 309, 281, 312, 48704, 13, 51374], "temperature": 0.0, "avg_logprob": -0.3113142378786777, "compression_ratio": 1.6127450980392157, "no_speech_prob": 4.539784276857972e-05}, {"id": 887, "seek": 417922, "start": 4199.42, "end": 4203.46, "text": " Maybe we want it to have something other than unit variance and", "tokens": [51374, 2704, 321, 528, 309, 281, 362, 746, 661, 813, 4985, 21977, 293, 51576], "temperature": 0.0, "avg_logprob": -0.3113142378786777, "compression_ratio": 1.6127450980392157, "no_speech_prob": 4.539784276857972e-05}, {"id": 888, "seek": 417922, "start": 4203.46, "end": 4204.7, "text": " something other than 0 mean.", "tokens": [51576, 746, 661, 813, 1958, 914, 13, 51638], "temperature": 0.0, "avg_logprob": -0.3113142378786777, "compression_ratio": 1.6127450980392157, "no_speech_prob": 4.539784276857972e-05}, {"id": 889, "seek": 420470, "start": 4205.7, "end": 4211.46, "text": " Well, what we do is we then multiply it back by self.mult and add self.add.", "tokens": [50414, 1042, 11, 437, 321, 360, 307, 321, 550, 12972, 309, 646, 538, 2698, 13, 76, 723, 293, 909, 2698, 13, 25224, 13, 50702], "temperature": 0.0, "avg_logprob": -0.2720955879457535, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.288748985141865e-06}, {"id": 890, "seek": 420470, "start": 4211.46, "end": 4215.74, "text": " Now remember, self.mult was 1 and self.add is 0.", "tokens": [50702, 823, 1604, 11, 2698, 13, 76, 723, 390, 502, 293, 2698, 13, 25224, 307, 1958, 13, 50916], "temperature": 0.0, "avg_logprob": -0.2720955879457535, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.288748985141865e-06}, {"id": 891, "seek": 420470, "start": 4215.74, "end": 4217.74, "text": " So at first, that does nothing at all.", "tokens": [50916, 407, 412, 700, 11, 300, 775, 1825, 412, 439, 13, 51016], "temperature": 0.0, "avg_logprob": -0.2720955879457535, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.288748985141865e-06}, {"id": 892, "seek": 420470, "start": 4217.74, "end": 4220.26, "text": " So at first, this is just normalizing the data.", "tokens": [51016, 407, 412, 700, 11, 341, 307, 445, 2710, 3319, 264, 1412, 13, 51142], "temperature": 0.0, "avg_logprob": -0.2720955879457535, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.288748985141865e-06}, {"id": 893, "seek": 420470, "start": 4220.26, "end": 4220.9, "text": " So that's good.", "tokens": [51142, 407, 300, 311, 665, 13, 51174], "temperature": 0.0, "avg_logprob": -0.2720955879457535, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.288748985141865e-06}, {"id": 894, "seek": 420470, "start": 4222.9, "end": 4226.86, "text": " But because these are parameters, these two numbers are learnable.", "tokens": [51274, 583, 570, 613, 366, 9834, 11, 613, 732, 3547, 366, 1466, 712, 13, 51472], "temperature": 0.0, "avg_logprob": -0.2720955879457535, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.288748985141865e-06}, {"id": 895, "seek": 420470, "start": 4226.86, "end": 4230.22, "text": " That means that the SGD algorithm can change them.", "tokens": [51472, 663, 1355, 300, 264, 34520, 35, 9284, 393, 1319, 552, 13, 51640], "temperature": 0.0, "avg_logprob": -0.2720955879457535, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.288748985141865e-06}, {"id": 896, "seek": 420470, "start": 4230.22, "end": 4234.54, "text": " So there's a very subtle thing going on here, which is that in fact,", "tokens": [51640, 407, 456, 311, 257, 588, 13743, 551, 516, 322, 510, 11, 597, 307, 300, 294, 1186, 11, 51856], "temperature": 0.0, "avg_logprob": -0.2720955879457535, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.288748985141865e-06}, {"id": 897, "seek": 423454, "start": 4235.38, "end": 4238.3, "text": " this might not be normalizing the data at all or", "tokens": [50406, 341, 1062, 406, 312, 2710, 3319, 264, 1412, 412, 439, 420, 50552], "temperature": 0.0, "avg_logprob": -0.3511059019300673, "compression_ratio": 1.7816593886462881, "no_speech_prob": 9.570827614879818e-07}, {"id": 898, "seek": 423454, "start": 4238.3, "end": 4241.38, "text": " normalizing the inputs to the next layer at all.", "tokens": [50552, 2710, 3319, 264, 15743, 281, 264, 958, 4583, 412, 439, 13, 50706], "temperature": 0.0, "avg_logprob": -0.3511059019300673, "compression_ratio": 1.7816593886462881, "no_speech_prob": 9.570827614879818e-07}, {"id": 899, "seek": 423454, "start": 4241.38, "end": 4243.7, "text": " Because self.mult and self.add could be anything.", "tokens": [50706, 1436, 2698, 13, 76, 723, 293, 2698, 13, 25224, 727, 312, 1340, 13, 50822], "temperature": 0.0, "avg_logprob": -0.3511059019300673, "compression_ratio": 1.7816593886462881, "no_speech_prob": 9.570827614879818e-07}, {"id": 900, "seek": 423454, "start": 4245.14, "end": 4249.18, "text": " So I tend to think that when people think about these kind of things,", "tokens": [50894, 407, 286, 3928, 281, 519, 300, 562, 561, 519, 466, 613, 733, 295, 721, 11, 51096], "temperature": 0.0, "avg_logprob": -0.3511059019300673, "compression_ratio": 1.7816593886462881, "no_speech_prob": 9.570827614879818e-07}, {"id": 901, "seek": 423454, "start": 4249.18, "end": 4251.3, "text": " like layer normalization and batch normalization,", "tokens": [51096, 411, 4583, 2710, 2144, 293, 15245, 2710, 2144, 11, 51202], "temperature": 0.0, "avg_logprob": -0.3511059019300673, "compression_ratio": 1.7816593886462881, "no_speech_prob": 9.570827614879818e-07}, {"id": 902, "seek": 423454, "start": 4251.3, "end": 4256.78, "text": " thinking of this as normalization in some ways is not the right way to think of it.", "tokens": [51202, 1953, 295, 341, 382, 2710, 2144, 294, 512, 2098, 307, 406, 264, 558, 636, 281, 519, 295, 309, 13, 51476], "temperature": 0.0, "avg_logprob": -0.3511059019300673, "compression_ratio": 1.7816593886462881, "no_speech_prob": 9.570827614879818e-07}, {"id": 903, "seek": 423454, "start": 4259.1, "end": 4261.94, "text": " It's actually doing something, I think, to really, well,", "tokens": [51592, 467, 311, 767, 884, 746, 11, 286, 519, 11, 281, 534, 11, 731, 11, 51734], "temperature": 0.0, "avg_logprob": -0.3511059019300673, "compression_ratio": 1.7816593886462881, "no_speech_prob": 9.570827614879818e-07}, {"id": 904, "seek": 426194, "start": 4261.94, "end": 4264.46, "text": " it's normalizing it for the initial layers.", "tokens": [50364, 309, 311, 2710, 3319, 309, 337, 264, 5883, 7914, 13, 50490], "temperature": 0.0, "avg_logprob": -0.35600970489810213, "compression_ratio": 1.6118721461187215, "no_speech_prob": 1.5689549400121905e-05}, {"id": 905, "seek": 426194, "start": 4264.46, "end": 4267.82, "text": " And we don't really need LSUV anymore if we have this in here,", "tokens": [50490, 400, 321, 500, 380, 534, 643, 441, 20214, 53, 3602, 498, 321, 362, 341, 294, 510, 11, 50658], "temperature": 0.0, "avg_logprob": -0.35600970489810213, "compression_ratio": 1.6118721461187215, "no_speech_prob": 1.5689549400121905e-05}, {"id": 906, "seek": 426194, "start": 4267.82, "end": 4269.74, "text": " cuz it's gonna normalize it automatically.", "tokens": [50658, 11910, 309, 311, 799, 2710, 1125, 309, 6772, 13, 50754], "temperature": 0.0, "avg_logprob": -0.35600970489810213, "compression_ratio": 1.6118721461187215, "no_speech_prob": 1.5689549400121905e-05}, {"id": 907, "seek": 426194, "start": 4271.179999999999, "end": 4272.0199999999995, "text": " So that's handy.", "tokens": [50826, 407, 300, 311, 13239, 13, 50868], "temperature": 0.0, "avg_logprob": -0.35600970489810213, "compression_ratio": 1.6118721461187215, "no_speech_prob": 1.5689549400121905e-05}, {"id": 908, "seek": 426194, "start": 4273.62, "end": 4277.379999999999, "text": " But after a few batches, it's not really normalizing at all.", "tokens": [50948, 583, 934, 257, 1326, 15245, 279, 11, 309, 311, 406, 534, 2710, 3319, 412, 439, 13, 51136], "temperature": 0.0, "avg_logprob": -0.35600970489810213, "compression_ratio": 1.6118721461187215, "no_speech_prob": 1.5689549400121905e-05}, {"id": 909, "seek": 426194, "start": 4278.66, "end": 4286.259999999999, "text": " But what it is doing is previously this idea of how big are the numbers overall,", "tokens": [51200, 583, 437, 309, 307, 884, 307, 8046, 341, 1558, 295, 577, 955, 366, 264, 3547, 4787, 11, 51580], "temperature": 0.0, "avg_logprob": -0.35600970489810213, "compression_ratio": 1.6118721461187215, "no_speech_prob": 1.5689549400121905e-05}, {"id": 910, "seek": 426194, "start": 4286.259999999999, "end": 4289.58, "text": " and how much variation do they have overall,", "tokens": [51580, 293, 577, 709, 12990, 360, 436, 362, 4787, 11, 51746], "temperature": 0.0, "avg_logprob": -0.35600970489810213, "compression_ratio": 1.6118721461187215, "no_speech_prob": 1.5689549400121905e-05}, {"id": 911, "seek": 428958, "start": 4289.58, "end": 4294.26, "text": " was kind of built into every single number in the weight matrix and", "tokens": [50364, 390, 733, 295, 3094, 666, 633, 2167, 1230, 294, 264, 3364, 8141, 293, 50598], "temperature": 0.0, "avg_logprob": -0.3121878949901726, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.6280511090371874e-06}, {"id": 912, "seek": 428958, "start": 4294.26, "end": 4296.26, "text": " in the bias vector.", "tokens": [50598, 294, 264, 12577, 8062, 13, 50698], "temperature": 0.0, "avg_logprob": -0.3121878949901726, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.6280511090371874e-06}, {"id": 913, "seek": 428958, "start": 4298.26, "end": 4302.7, "text": " This way, those two things have been turned into just two numbers.", "tokens": [50798, 639, 636, 11, 729, 732, 721, 362, 668, 3574, 666, 445, 732, 3547, 13, 51020], "temperature": 0.0, "avg_logprob": -0.3121878949901726, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.6280511090371874e-06}, {"id": 914, "seek": 428958, "start": 4303.9, "end": 4308.0599999999995, "text": " And I think this makes training a lot easier for it, basically,", "tokens": [51080, 400, 286, 519, 341, 1669, 3097, 257, 688, 3571, 337, 309, 11, 1936, 11, 51288], "temperature": 0.0, "avg_logprob": -0.3121878949901726, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.6280511090371874e-06}, {"id": 915, "seek": 428958, "start": 4308.0599999999995, "end": 4312.66, "text": " to just have just two numbers that it can focus on to change this overall", "tokens": [51288, 281, 445, 362, 445, 732, 3547, 300, 309, 393, 1879, 322, 281, 1319, 341, 4787, 51518], "temperature": 0.0, "avg_logprob": -0.3121878949901726, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.6280511090371874e-06}, {"id": 916, "seek": 428958, "start": 4312.66, "end": 4314.26, "text": " positioning and variation.", "tokens": [51518, 26381, 293, 12990, 13, 51598], "temperature": 0.0, "avg_logprob": -0.3121878949901726, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.6280511090371874e-06}, {"id": 917, "seek": 431426, "start": 4315.22, "end": 4318.18, "text": " So there's something very subtle going on here,", "tokens": [50412, 407, 456, 311, 746, 588, 13743, 516, 322, 510, 11, 50560], "temperature": 0.0, "avg_logprob": -0.3680370164954144, "compression_ratio": 1.6026785714285714, "no_speech_prob": 4.0294567043019924e-06}, {"id": 918, "seek": 431426, "start": 4318.18, "end": 4321.14, "text": " cuz it's not just doing normalization.", "tokens": [50560, 11910, 309, 311, 406, 445, 884, 2710, 2144, 13, 50708], "temperature": 0.0, "avg_logprob": -0.3680370164954144, "compression_ratio": 1.6026785714285714, "no_speech_prob": 4.0294567043019924e-06}, {"id": 919, "seek": 431426, "start": 4321.14, "end": 4324.5, "text": " At least not after the first few batches are complete,", "tokens": [50708, 1711, 1935, 406, 934, 264, 700, 1326, 15245, 279, 366, 3566, 11, 50876], "temperature": 0.0, "avg_logprob": -0.3680370164954144, "compression_ratio": 1.6026785714285714, "no_speech_prob": 4.0294567043019924e-06}, {"id": 920, "seek": 431426, "start": 4324.5, "end": 4328.9400000000005, "text": " because it can learn to create any distribution of outputs it want.", "tokens": [50876, 570, 309, 393, 1466, 281, 1884, 604, 7316, 295, 23930, 309, 528, 13, 51098], "temperature": 0.0, "avg_logprob": -0.3680370164954144, "compression_ratio": 1.6026785714285714, "no_speech_prob": 4.0294567043019924e-06}, {"id": 921, "seek": 431426, "start": 4330.780000000001, "end": 4332.1, "text": " So there's our layer.", "tokens": [51190, 407, 456, 311, 527, 4583, 13, 51256], "temperature": 0.0, "avg_logprob": -0.3680370164954144, "compression_ratio": 1.6026785714285714, "no_speech_prob": 4.0294567043019924e-06}, {"id": 922, "seek": 431426, "start": 4332.1, "end": 4335.34, "text": " So we're gonna need to change our con function let again.", "tokens": [51256, 407, 321, 434, 799, 643, 281, 1319, 527, 416, 2445, 718, 797, 13, 51418], "temperature": 0.0, "avg_logprob": -0.3680370164954144, "compression_ratio": 1.6026785714285714, "no_speech_prob": 4.0294567043019924e-06}, {"id": 923, "seek": 431426, "start": 4335.34, "end": 4341.54, "text": " Previously we changed it to add activation function to be modifiable.", "tokens": [51418, 33606, 321, 3105, 309, 281, 909, 24433, 2445, 281, 312, 1072, 30876, 13, 51728], "temperature": 0.0, "avg_logprob": -0.3680370164954144, "compression_ratio": 1.6026785714285714, "no_speech_prob": 4.0294567043019924e-06}, {"id": 924, "seek": 434154, "start": 4341.78, "end": 4345.5, "text": " Now we're gonna also change it to allow us to add normalization layers to the end.", "tokens": [50376, 823, 321, 434, 799, 611, 1319, 309, 281, 2089, 505, 281, 909, 2710, 2144, 7914, 281, 264, 917, 13, 50562], "temperature": 0.0, "avg_logprob": -0.2792038499263295, "compression_ratio": 1.7136929460580912, "no_speech_prob": 5.955138021818129e-06}, {"id": 925, "seek": 434154, "start": 4346.78, "end": 4352.34, "text": " So our basic layers, well, we'll start off by adding our conf2d as usual.", "tokens": [50626, 407, 527, 3875, 7914, 11, 731, 11, 321, 603, 722, 766, 538, 5127, 527, 1497, 17, 67, 382, 7713, 13, 50904], "temperature": 0.0, "avg_logprob": -0.2792038499263295, "compression_ratio": 1.7136929460580912, "no_speech_prob": 5.955138021818129e-06}, {"id": 926, "seek": 434154, "start": 4352.34, "end": 4354.54, "text": " And then if you're doing normalization,", "tokens": [50904, 400, 550, 498, 291, 434, 884, 2710, 2144, 11, 51014], "temperature": 0.0, "avg_logprob": -0.2792038499263295, "compression_ratio": 1.7136929460580912, "no_speech_prob": 5.955138021818129e-06}, {"id": 927, "seek": 434154, "start": 4354.54, "end": 4360.74, "text": " we will append the normalization layer with this many inputs.", "tokens": [51014, 321, 486, 34116, 264, 2710, 2144, 4583, 365, 341, 867, 15743, 13, 51324], "temperature": 0.0, "avg_logprob": -0.2792038499263295, "compression_ratio": 1.7136929460580912, "no_speech_prob": 5.955138021818129e-06}, {"id": 928, "seek": 434154, "start": 4360.74, "end": 4364.26, "text": " Now in fact, layer norm doesn't care how many inputs, so I just ignore it.", "tokens": [51324, 823, 294, 1186, 11, 4583, 2026, 1177, 380, 1127, 577, 867, 15743, 11, 370, 286, 445, 11200, 309, 13, 51500], "temperature": 0.0, "avg_logprob": -0.2792038499263295, "compression_ratio": 1.7136929460580912, "no_speech_prob": 5.955138021818129e-06}, {"id": 929, "seek": 434154, "start": 4364.26, "end": 4365.62, "text": " But you'll see batch normal care.", "tokens": [51500, 583, 291, 603, 536, 15245, 2710, 1127, 13, 51568], "temperature": 0.0, "avg_logprob": -0.2792038499263295, "compression_ratio": 1.7136929460580912, "no_speech_prob": 5.955138021818129e-06}, {"id": 930, "seek": 434154, "start": 4366.66, "end": 4369.1, "text": " If you've got an activation function, add it.", "tokens": [51620, 759, 291, 600, 658, 364, 24433, 2445, 11, 909, 309, 13, 51742], "temperature": 0.0, "avg_logprob": -0.2792038499263295, "compression_ratio": 1.7136929460580912, "no_speech_prob": 5.955138021818129e-06}, {"id": 931, "seek": 436910, "start": 4369.1, "end": 4372.3, "text": " And so our convolutional layer is actually a sequential bunch of layers.", "tokens": [50364, 400, 370, 527, 45216, 304, 4583, 307, 767, 257, 42881, 3840, 295, 7914, 13, 50524], "temperature": 0.0, "avg_logprob": -0.3656125118857936, "compression_ratio": 1.5904761904761904, "no_speech_prob": 7.00216901350359e-07}, {"id": 932, "seek": 436910, "start": 4377.9800000000005, "end": 4382.820000000001, "text": " Now one thing that's interesting, I think, is that for bias in the conv,", "tokens": [50808, 823, 472, 551, 300, 311, 1880, 11, 286, 519, 11, 307, 300, 337, 12577, 294, 264, 3754, 11, 51050], "temperature": 0.0, "avg_logprob": -0.3656125118857936, "compression_ratio": 1.5904761904761904, "no_speech_prob": 7.00216901350359e-07}, {"id": 933, "seek": 436910, "start": 4382.820000000001, "end": 4388.14, "text": " if you're using, well, this isn't quite true, is it?", "tokens": [51050, 498, 291, 434, 1228, 11, 731, 11, 341, 1943, 380, 1596, 2074, 11, 307, 309, 30, 51316], "temperature": 0.0, "avg_logprob": -0.3656125118857936, "compression_ratio": 1.5904761904761904, "no_speech_prob": 7.00216901350359e-07}, {"id": 934, "seek": 436910, "start": 4388.14, "end": 4390.900000000001, "text": " I was gonna say if you're using layer norm, you don't need bias.", "tokens": [51316, 286, 390, 799, 584, 498, 291, 434, 1228, 4583, 2026, 11, 291, 500, 380, 643, 12577, 13, 51454], "temperature": 0.0, "avg_logprob": -0.3656125118857936, "compression_ratio": 1.5904761904761904, "no_speech_prob": 7.00216901350359e-07}, {"id": 935, "seek": 436910, "start": 4390.900000000001, "end": 4392.700000000001, "text": " But actually, you kind of do.", "tokens": [51454, 583, 767, 11, 291, 733, 295, 360, 13, 51544], "temperature": 0.0, "avg_logprob": -0.3656125118857936, "compression_ratio": 1.5904761904761904, "no_speech_prob": 7.00216901350359e-07}, {"id": 936, "seek": 436910, "start": 4394.26, "end": 4396.660000000001, "text": " So maybe we should actually change that.", "tokens": [51622, 407, 1310, 321, 820, 767, 1319, 300, 13, 51742], "temperature": 0.0, "avg_logprob": -0.3656125118857936, "compression_ratio": 1.5904761904761904, "no_speech_prob": 7.00216901350359e-07}, {"id": 937, "seek": 439666, "start": 4397.66, "end": 4400.54, "text": " For batch norm, we won't need bias.", "tokens": [50414, 1171, 15245, 2026, 11, 321, 1582, 380, 643, 12577, 13, 50558], "temperature": 0.0, "avg_logprob": -0.4896297920040968, "compression_ratio": 1.5157232704402517, "no_speech_prob": 9.818325452215504e-06}, {"id": 938, "seek": 439666, "start": 4400.54, "end": 4402.18, "text": " But actually, for this one, we do.", "tokens": [50558, 583, 767, 11, 337, 341, 472, 11, 321, 360, 13, 50640], "temperature": 0.0, "avg_logprob": -0.4896297920040968, "compression_ratio": 1.5157232704402517, "no_speech_prob": 9.818325452215504e-06}, {"id": 939, "seek": 439666, "start": 4402.18, "end": 4404.0199999999995, "text": " So, let me put this back.", "tokens": [50640, 407, 11, 718, 385, 829, 341, 646, 13, 50732], "temperature": 0.0, "avg_logprob": -0.4896297920040968, "compression_ratio": 1.5157232704402517, "no_speech_prob": 9.818325452215504e-06}, {"id": 940, "seek": 439666, "start": 4405.26, "end": 4407.38, "text": " Bias equals true.", "tokens": [50794, 363, 4609, 6915, 2074, 13, 50900], "temperature": 0.0, "avg_logprob": -0.4896297920040968, "compression_ratio": 1.5157232704402517, "no_speech_prob": 9.818325452215504e-06}, {"id": 941, "seek": 439666, "start": 4408.66, "end": 4410.74, "text": " Bias equals bias.", "tokens": [50964, 363, 4609, 6915, 12577, 13, 51068], "temperature": 0.0, "avg_logprob": -0.4896297920040968, "compression_ratio": 1.5157232704402517, "no_speech_prob": 9.818325452215504e-06}, {"id": 942, "seek": 439666, "start": 4412.3, "end": 4412.78, "text": " Okay.", "tokens": [51146, 1033, 13, 51170], "temperature": 0.0, "avg_logprob": -0.4896297920040968, "compression_ratio": 1.5157232704402517, "no_speech_prob": 9.818325452215504e-06}, {"id": 943, "seek": 439666, "start": 4415.18, "end": 4418.38, "text": " So then these initial layers, right, yes.", "tokens": [51290, 407, 550, 613, 5883, 7914, 11, 558, 11, 2086, 13, 51450], "temperature": 0.0, "avg_logprob": -0.4896297920040968, "compression_ratio": 1.5157232704402517, "no_speech_prob": 9.818325452215504e-06}, {"id": 944, "seek": 439666, "start": 4418.38, "end": 4424.62, "text": " So they all have bias, and then we've got bias equals false.", "tokens": [51450, 407, 436, 439, 362, 12577, 11, 293, 550, 321, 600, 658, 12577, 6915, 7908, 13, 51762], "temperature": 0.0, "avg_logprob": -0.4896297920040968, "compression_ratio": 1.5157232704402517, "no_speech_prob": 9.818325452215504e-06}, {"id": 945, "seek": 442666, "start": 4426.66, "end": 4431.94, "text": " Okay, so now in our model,", "tokens": [50364, 1033, 11, 370, 586, 294, 527, 2316, 11, 50628], "temperature": 0.0, "avg_logprob": -0.6453448704310826, "compression_ratio": 1.1794871794871795, "no_speech_prob": 3.5209131965530105e-07}, {"id": 946, "seek": 442666, "start": 4431.94, "end": 4437.34, "text": " we're gonna add layer normalization", "tokens": [50628, 321, 434, 799, 909, 4583, 2710, 2144, 50898], "temperature": 0.0, "avg_logprob": -0.6453448704310826, "compression_ratio": 1.1794871794871795, "no_speech_prob": 3.5209131965530105e-07}, {"id": 947, "seek": 442666, "start": 4437.34, "end": 4442.0199999999995, "text": " to every layer except for the last one.", "tokens": [50898, 281, 633, 4583, 3993, 337, 264, 1036, 472, 13, 51132], "temperature": 0.0, "avg_logprob": -0.6453448704310826, "compression_ratio": 1.1794871794871795, "no_speech_prob": 3.5209131965530105e-07}, {"id": 948, "seek": 442666, "start": 4444.42, "end": 4447.7, "text": " And let's see how we go.", "tokens": [51252, 400, 718, 311, 536, 577, 321, 352, 13, 51416], "temperature": 0.0, "avg_logprob": -0.6453448704310826, "compression_ratio": 1.1794871794871795, "no_speech_prob": 3.5209131965530105e-07}, {"id": 949, "seek": 442666, "start": 4450.7, "end": 4453.98, "text": " Nice, 873.", "tokens": [51566, 5490, 11, 27990, 18, 13, 51730], "temperature": 0.0, "avg_logprob": -0.6453448704310826, "compression_ratio": 1.1794871794871795, "no_speech_prob": 3.5209131965530105e-07}, {"id": 950, "seek": 445398, "start": 4454.0199999999995, "end": 4457.86, "text": " Okay, 860 and 872.", "tokens": [50366, 1033, 11, 1649, 4550, 293, 27990, 17, 13, 50558], "temperature": 0.0, "avg_logprob": -0.4811522165934245, "compression_ratio": 1.4137931034482758, "no_speech_prob": 6.577917019967572e-07}, {"id": 951, "seek": 445398, "start": 4457.86, "end": 4463.259999999999, "text": " So just, we've just got our best by a little bit.", "tokens": [50558, 407, 445, 11, 321, 600, 445, 658, 527, 1151, 538, 257, 707, 857, 13, 50828], "temperature": 0.0, "avg_logprob": -0.4811522165934245, "compression_ratio": 1.4137931034482758, "no_speech_prob": 6.577917019967572e-07}, {"id": 952, "seek": 445398, "start": 4463.259999999999, "end": 4464.099999999999, "text": " So that's cool.", "tokens": [50828, 407, 300, 311, 1627, 13, 50870], "temperature": 0.0, "avg_logprob": -0.4811522165934245, "compression_ratio": 1.4137931034482758, "no_speech_prob": 6.577917019967572e-07}, {"id": 953, "seek": 445398, "start": 4467.099999999999, "end": 4472.66, "text": " So the thing about these normalization layers is though that", "tokens": [51020, 407, 264, 551, 466, 613, 2710, 2144, 7914, 307, 1673, 300, 51298], "temperature": 0.0, "avg_logprob": -0.4811522165934245, "compression_ratio": 1.4137931034482758, "no_speech_prob": 6.577917019967572e-07}, {"id": 954, "seek": 445398, "start": 4472.66, "end": 4476.74, "text": " they do cause a lot of challenges in models.", "tokens": [51298, 436, 360, 3082, 257, 688, 295, 4759, 294, 5245, 13, 51502], "temperature": 0.0, "avg_logprob": -0.4811522165934245, "compression_ratio": 1.4137931034482758, "no_speech_prob": 6.577917019967572e-07}, {"id": 955, "seek": 445398, "start": 4476.74, "end": 4482.379999999999, "text": " And generally speaking, ever since patch normalization,", "tokens": [51502, 400, 5101, 4124, 11, 1562, 1670, 9972, 2710, 2144, 11, 51784], "temperature": 0.0, "avg_logprob": -0.4811522165934245, "compression_ratio": 1.4137931034482758, "no_speech_prob": 6.577917019967572e-07}, {"id": 956, "seek": 448238, "start": 4482.82, "end": 4486.86, "text": " ever since patch norm appeared, well, there's been this kind of like,", "tokens": [50386, 1562, 1670, 9972, 2026, 8516, 11, 731, 11, 456, 311, 668, 341, 733, 295, 411, 11, 50588], "temperature": 0.0, "avg_logprob": -0.3095856030782064, "compression_ratio": 1.6934306569343065, "no_speech_prob": 5.173917088541202e-06}, {"id": 957, "seek": 448238, "start": 4486.86, "end": 4489.82, "text": " big change of view towards it.", "tokens": [50588, 955, 1319, 295, 1910, 3030, 309, 13, 50736], "temperature": 0.0, "avg_logprob": -0.3095856030782064, "compression_ratio": 1.6934306569343065, "no_speech_prob": 5.173917088541202e-06}, {"id": 958, "seek": 448238, "start": 4489.82, "end": 4492.900000000001, "text": " At first, people were like, my God, batch norm is our savior.", "tokens": [50736, 1711, 700, 11, 561, 645, 411, 11, 452, 1265, 11, 15245, 2026, 307, 527, 41327, 13, 50890], "temperature": 0.0, "avg_logprob": -0.3095856030782064, "compression_ratio": 1.6934306569343065, "no_speech_prob": 5.173917088541202e-06}, {"id": 959, "seek": 448238, "start": 4492.900000000001, "end": 4496.86, "text": " And it kind of was, it let us train much deeper models and", "tokens": [50890, 400, 309, 733, 295, 390, 11, 309, 718, 505, 3847, 709, 7731, 5245, 293, 51088], "temperature": 0.0, "avg_logprob": -0.3095856030782064, "compression_ratio": 1.6934306569343065, "no_speech_prob": 5.173917088541202e-06}, {"id": 960, "seek": 448238, "start": 4496.86, "end": 4499.62, "text": " get great results and train quickly.", "tokens": [51088, 483, 869, 3542, 293, 3847, 2661, 13, 51226], "temperature": 0.0, "avg_logprob": -0.3095856030782064, "compression_ratio": 1.6934306569343065, "no_speech_prob": 5.173917088541202e-06}, {"id": 961, "seek": 448238, "start": 4499.62, "end": 4503.46, "text": " But then increasingly, people realized it also added a lot of complexity.", "tokens": [51226, 583, 550, 12980, 11, 561, 5334, 309, 611, 3869, 257, 688, 295, 14024, 13, 51418], "temperature": 0.0, "avg_logprob": -0.3095856030782064, "compression_ratio": 1.6934306569343065, "no_speech_prob": 5.173917088541202e-06}, {"id": 962, "seek": 448238, "start": 4504.46, "end": 4508.38, "text": " These learnable parameters turned out to create all kind of complexity.", "tokens": [51468, 1981, 1466, 712, 9834, 3574, 484, 281, 1884, 439, 733, 295, 14024, 13, 51664], "temperature": 0.0, "avg_logprob": -0.3095856030782064, "compression_ratio": 1.6934306569343065, "no_speech_prob": 5.173917088541202e-06}, {"id": 963, "seek": 448238, "start": 4508.38, "end": 4510.78, "text": " And in particular, batch norm, which we'll see in a minute,", "tokens": [51664, 400, 294, 1729, 11, 15245, 2026, 11, 597, 321, 603, 536, 294, 257, 3456, 11, 51784], "temperature": 0.0, "avg_logprob": -0.3095856030782064, "compression_ratio": 1.6934306569343065, "no_speech_prob": 5.173917088541202e-06}, {"id": 964, "seek": 451078, "start": 4510.78, "end": 4512.139999999999, "text": " created all kinds of complexity.", "tokens": [50364, 2942, 439, 3685, 295, 14024, 13, 50432], "temperature": 0.0, "avg_logprob": -0.26442694109539655, "compression_ratio": 1.663594470046083, "no_speech_prob": 1.2606943528226111e-05}, {"id": 965, "seek": 451078, "start": 4513.7, "end": 4518.179999999999, "text": " So there has been a tendency in recent years to be trying to get rid of or", "tokens": [50510, 407, 456, 575, 668, 257, 18187, 294, 5162, 924, 281, 312, 1382, 281, 483, 3973, 295, 420, 50734], "temperature": 0.0, "avg_logprob": -0.26442694109539655, "compression_ratio": 1.663594470046083, "no_speech_prob": 1.2606943528226111e-05}, {"id": 966, "seek": 451078, "start": 4518.179999999999, "end": 4521.099999999999, "text": " at least reduce the use of these kinds of layers.", "tokens": [50734, 412, 1935, 5407, 264, 764, 295, 613, 3685, 295, 7914, 13, 50880], "temperature": 0.0, "avg_logprob": -0.26442694109539655, "compression_ratio": 1.663594470046083, "no_speech_prob": 1.2606943528226111e-05}, {"id": 967, "seek": 451078, "start": 4521.099999999999, "end": 4526.259999999999, "text": " So knowing how to actually initialize your models correctly", "tokens": [50880, 407, 5276, 577, 281, 767, 5883, 1125, 428, 5245, 8944, 51138], "temperature": 0.0, "avg_logprob": -0.26442694109539655, "compression_ratio": 1.663594470046083, "no_speech_prob": 1.2606943528226111e-05}, {"id": 968, "seek": 451078, "start": 4528.34, "end": 4532.98, "text": " at first is becoming increasingly important as people are trying to move", "tokens": [51242, 412, 700, 307, 5617, 12980, 1021, 382, 561, 366, 1382, 281, 1286, 51474], "temperature": 0.0, "avg_logprob": -0.26442694109539655, "compression_ratio": 1.663594470046083, "no_speech_prob": 1.2606943528226111e-05}, {"id": 969, "seek": 451078, "start": 4532.98, "end": 4536.5, "text": " away from these normalization layers increasingly.", "tokens": [51474, 1314, 490, 613, 2710, 2144, 7914, 12980, 13, 51650], "temperature": 0.0, "avg_logprob": -0.26442694109539655, "compression_ratio": 1.663594470046083, "no_speech_prob": 1.2606943528226111e-05}, {"id": 970, "seek": 451078, "start": 4538.139999999999, "end": 4539.7, "text": " So I will say that.", "tokens": [51732, 407, 286, 486, 584, 300, 13, 51810], "temperature": 0.0, "avg_logprob": -0.26442694109539655, "compression_ratio": 1.663594470046083, "no_speech_prob": 1.2606943528226111e-05}, {"id": 971, "seek": 453970, "start": 4539.7, "end": 4546.66, "text": " So they're still very helpful, but they're not a silver bullet, as it turns out.", "tokens": [50364, 407, 436, 434, 920, 588, 4961, 11, 457, 436, 434, 406, 257, 8753, 11632, 11, 382, 309, 4523, 484, 13, 50712], "temperature": 0.0, "avg_logprob": -0.2763685616113806, "compression_ratio": 1.62, "no_speech_prob": 6.681515287709772e-07}, {"id": 972, "seek": 453970, "start": 4548.099999999999, "end": 4550.38, "text": " All right, so now let's look at batch norm.", "tokens": [50784, 1057, 558, 11, 370, 586, 718, 311, 574, 412, 15245, 2026, 13, 50898], "temperature": 0.0, "avg_logprob": -0.2763685616113806, "compression_ratio": 1.62, "no_speech_prob": 6.681515287709772e-07}, {"id": 973, "seek": 453970, "start": 4550.38, "end": 4555.7, "text": " So batch norm is still not huge, but it's a little bit bigger than layer norm.", "tokens": [50898, 407, 15245, 2026, 307, 920, 406, 2603, 11, 457, 309, 311, 257, 707, 857, 3801, 813, 4583, 2026, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2763685616113806, "compression_ratio": 1.62, "no_speech_prob": 6.681515287709772e-07}, {"id": 974, "seek": 453970, "start": 4557.139999999999, "end": 4562.3, "text": " And you'll see that we've got the mult and add as before.", "tokens": [51236, 400, 291, 603, 536, 300, 321, 600, 658, 264, 2120, 293, 909, 382, 949, 13, 51494], "temperature": 0.0, "avg_logprob": -0.2763685616113806, "compression_ratio": 1.62, "no_speech_prob": 6.681515287709772e-07}, {"id": 975, "seek": 453970, "start": 4563.86, "end": 4569.38, "text": " But it's not just one number to add or one number to multiply,", "tokens": [51572, 583, 309, 311, 406, 445, 472, 1230, 281, 909, 420, 472, 1230, 281, 12972, 11, 51848], "temperature": 0.0, "avg_logprob": -0.2763685616113806, "compression_ratio": 1.62, "no_speech_prob": 6.681515287709772e-07}, {"id": 976, "seek": 456938, "start": 4570.06, "end": 4571.5, "text": " but actually we've got a whole bunch of them.", "tokens": [50398, 457, 767, 321, 600, 658, 257, 1379, 3840, 295, 552, 13, 50470], "temperature": 0.0, "avg_logprob": -0.30580690682652484, "compression_ratio": 1.7637362637362637, "no_speech_prob": 8.059445235630847e-07}, {"id": 977, "seek": 456938, "start": 4571.5, "end": 4575.1, "text": " And the reason is that we're gonna have one for every channel.", "tokens": [50470, 400, 264, 1778, 307, 300, 321, 434, 799, 362, 472, 337, 633, 2269, 13, 50650], "temperature": 0.0, "avg_logprob": -0.30580690682652484, "compression_ratio": 1.7637362637362637, "no_speech_prob": 8.059445235630847e-07}, {"id": 978, "seek": 456938, "start": 4575.1, "end": 4580.1, "text": " And so now when we take the mean and the variance,", "tokens": [50650, 400, 370, 586, 562, 321, 747, 264, 914, 293, 264, 21977, 11, 50900], "temperature": 0.0, "avg_logprob": -0.30580690682652484, "compression_ratio": 1.7637362637362637, "no_speech_prob": 8.059445235630847e-07}, {"id": 979, "seek": 456938, "start": 4580.1, "end": 4584.46, "text": " we're actually taking it over the batch dimension and the height and", "tokens": [50900, 321, 434, 767, 1940, 309, 670, 264, 15245, 10139, 293, 264, 6681, 293, 51118], "temperature": 0.0, "avg_logprob": -0.30580690682652484, "compression_ratio": 1.7637362637362637, "no_speech_prob": 8.059445235630847e-07}, {"id": 980, "seek": 456938, "start": 4584.46, "end": 4585.58, "text": " width dimensions.", "tokens": [51118, 11402, 12819, 13, 51174], "temperature": 0.0, "avg_logprob": -0.30580690682652484, "compression_ratio": 1.7637362637362637, "no_speech_prob": 8.059445235630847e-07}, {"id": 981, "seek": 456938, "start": 4586.62, "end": 4592.14, "text": " So we're ending up with one mean per channel and one variance per channel.", "tokens": [51226, 407, 321, 434, 8121, 493, 365, 472, 914, 680, 2269, 293, 472, 21977, 680, 2269, 13, 51502], "temperature": 0.0, "avg_logprob": -0.30580690682652484, "compression_ratio": 1.7637362637362637, "no_speech_prob": 8.059445235630847e-07}, {"id": 982, "seek": 459214, "start": 4593.14, "end": 4597.700000000001, "text": " So just like before, once we get our means and", "tokens": [50414, 407, 445, 411, 949, 11, 1564, 321, 483, 527, 1355, 293, 50642], "temperature": 0.0, "avg_logprob": -0.3342815689418627, "compression_ratio": 1.6884422110552764, "no_speech_prob": 8.446225479019631e-07}, {"id": 983, "seek": 459214, "start": 4597.700000000001, "end": 4601.54, "text": " variances, we subtract them out and", "tokens": [50642, 1374, 21518, 11, 321, 16390, 552, 484, 293, 50834], "temperature": 0.0, "avg_logprob": -0.3342815689418627, "compression_ratio": 1.6884422110552764, "no_speech_prob": 8.446225479019631e-07}, {"id": 984, "seek": 459214, "start": 4601.54, "end": 4609.26, "text": " divide them by the epsilon modified variance.", "tokens": [50834, 9845, 552, 538, 264, 17889, 15873, 21977, 13, 51220], "temperature": 0.0, "avg_logprob": -0.3342815689418627, "compression_ratio": 1.6884422110552764, "no_speech_prob": 8.446225479019631e-07}, {"id": 985, "seek": 459214, "start": 4609.26, "end": 4613.02, "text": " And just like before, we then multiply by mult and add add.", "tokens": [51220, 400, 445, 411, 949, 11, 321, 550, 12972, 538, 2120, 293, 909, 909, 13, 51408], "temperature": 0.0, "avg_logprob": -0.3342815689418627, "compression_ratio": 1.6884422110552764, "no_speech_prob": 8.446225479019631e-07}, {"id": 986, "seek": 459214, "start": 4613.02, "end": 4615.740000000001, "text": " But now we're actually multiplying by a vector of mults and", "tokens": [51408, 583, 586, 321, 434, 767, 30955, 538, 257, 8062, 295, 2120, 82, 293, 51544], "temperature": 0.0, "avg_logprob": -0.3342815689418627, "compression_ratio": 1.6884422110552764, "no_speech_prob": 8.446225479019631e-07}, {"id": 987, "seek": 459214, "start": 4615.740000000001, "end": 4617.54, "text": " we're adding a vector of adds.", "tokens": [51544, 321, 434, 5127, 257, 8062, 295, 10860, 13, 51634], "temperature": 0.0, "avg_logprob": -0.3342815689418627, "compression_ratio": 1.6884422110552764, "no_speech_prob": 8.446225479019631e-07}, {"id": 988, "seek": 459214, "start": 4617.54, "end": 4620.900000000001, "text": " And that's why we have to pass in the number of filters,", "tokens": [51634, 400, 300, 311, 983, 321, 362, 281, 1320, 294, 264, 1230, 295, 15995, 11, 51802], "temperature": 0.0, "avg_logprob": -0.3342815689418627, "compression_ratio": 1.6884422110552764, "no_speech_prob": 8.446225479019631e-07}, {"id": 989, "seek": 462090, "start": 4620.94, "end": 4624.66, "text": " because we have to know how many ones and", "tokens": [50366, 570, 321, 362, 281, 458, 577, 867, 2306, 293, 50552], "temperature": 0.0, "avg_logprob": -0.27922620401754006, "compression_ratio": 1.5875706214689265, "no_speech_prob": 3.2563121976636467e-07}, {"id": 990, "seek": 462090, "start": 4624.66, "end": 4628.46, "text": " how many zeros we have in our initial mults and adds.", "tokens": [50552, 577, 867, 35193, 321, 362, 294, 527, 5883, 2120, 82, 293, 10860, 13, 50742], "temperature": 0.0, "avg_logprob": -0.27922620401754006, "compression_ratio": 1.5875706214689265, "no_speech_prob": 3.2563121976636467e-07}, {"id": 991, "seek": 462090, "start": 4630.46, "end": 4636.7, "text": " So that's the main difference in a sense is that we have one per channel.", "tokens": [50842, 407, 300, 311, 264, 2135, 2649, 294, 257, 2020, 307, 300, 321, 362, 472, 680, 2269, 13, 51154], "temperature": 0.0, "avg_logprob": -0.27922620401754006, "compression_ratio": 1.5875706214689265, "no_speech_prob": 3.2563121976636467e-07}, {"id": 992, "seek": 462090, "start": 4636.7, "end": 4644.74, "text": " And that we're also taking the average across all of the things in the batch.", "tokens": [51154, 400, 300, 321, 434, 611, 1940, 264, 4274, 2108, 439, 295, 264, 721, 294, 264, 15245, 13, 51556], "temperature": 0.0, "avg_logprob": -0.27922620401754006, "compression_ratio": 1.5875706214689265, "no_speech_prob": 3.2563121976636467e-07}, {"id": 993, "seek": 462090, "start": 4644.74, "end": 4647.82, "text": " Whereas in layer norm, we didn't.", "tokens": [51556, 13813, 294, 4583, 2026, 11, 321, 994, 380, 13, 51710], "temperature": 0.0, "avg_logprob": -0.27922620401754006, "compression_ratio": 1.5875706214689265, "no_speech_prob": 3.2563121976636467e-07}, {"id": 994, "seek": 464782, "start": 4647.82, "end": 4654.219999999999, "text": " Each thing in the batch had its own separate normalization it was doing.", "tokens": [50364, 6947, 551, 294, 264, 15245, 632, 1080, 1065, 4994, 2710, 2144, 309, 390, 884, 13, 50684], "temperature": 0.0, "avg_logprob": -0.33634534563337054, "compression_ratio": 1.515625, "no_speech_prob": 1.4144758324619033e-06}, {"id": 995, "seek": 464782, "start": 4659.299999999999, "end": 4662.219999999999, "text": " Then there's something else in batch norm which is a bit tricky.", "tokens": [50938, 1396, 456, 311, 746, 1646, 294, 15245, 2026, 597, 307, 257, 857, 12414, 13, 51084], "temperature": 0.0, "avg_logprob": -0.33634534563337054, "compression_ratio": 1.515625, "no_speech_prob": 1.4144758324619033e-06}, {"id": 996, "seek": 464782, "start": 4663.219999999999, "end": 4666.42, "text": " Which is that during training,", "tokens": [51134, 3013, 307, 300, 1830, 3097, 11, 51294], "temperature": 0.0, "avg_logprob": -0.33634534563337054, "compression_ratio": 1.515625, "no_speech_prob": 1.4144758324619033e-06}, {"id": 997, "seek": 464782, "start": 4666.42, "end": 4672.54, "text": " we are not just subtracting the mean and the variance.", "tokens": [51294, 321, 366, 406, 445, 16390, 278, 264, 914, 293, 264, 21977, 13, 51600], "temperature": 0.0, "avg_logprob": -0.33634534563337054, "compression_ratio": 1.515625, "no_speech_prob": 1.4144758324619033e-06}, {"id": 998, "seek": 464782, "start": 4672.54, "end": 4677.38, "text": " But instead, we're getting an exponentially weighted moving average", "tokens": [51600, 583, 2602, 11, 321, 434, 1242, 364, 37330, 32807, 2684, 4274, 51842], "temperature": 0.0, "avg_logprob": -0.33634534563337054, "compression_ratio": 1.515625, "no_speech_prob": 1.4144758324619033e-06}, {"id": 999, "seek": 467738, "start": 4677.46, "end": 4684.9800000000005, "text": " of the means and the variances of the last few batches.", "tokens": [50368, 295, 264, 1355, 293, 264, 1374, 21518, 295, 264, 1036, 1326, 15245, 279, 13, 50744], "temperature": 0.0, "avg_logprob": -0.29650501494712017, "compression_ratio": 1.7114427860696517, "no_speech_prob": 1.1015954441973008e-06}, {"id": 1000, "seek": 467738, "start": 4685.9800000000005, "end": 4686.78, "text": " That's what this is doing.", "tokens": [50794, 663, 311, 437, 341, 307, 884, 13, 50834], "temperature": 0.0, "avg_logprob": -0.29650501494712017, "compression_ratio": 1.7114427860696517, "no_speech_prob": 1.1015954441973008e-06}, {"id": 1001, "seek": 467738, "start": 4687.78, "end": 4692.34, "text": " So we start out, so we basically create something called vars and", "tokens": [50884, 407, 321, 722, 484, 11, 370, 321, 1936, 1884, 746, 1219, 46130, 293, 51112], "temperature": 0.0, "avg_logprob": -0.29650501494712017, "compression_ratio": 1.7114427860696517, "no_speech_prob": 1.1015954441973008e-06}, {"id": 1002, "seek": 467738, "start": 4692.34, "end": 4693.46, "text": " something called means.", "tokens": [51112, 746, 1219, 1355, 13, 51168], "temperature": 0.0, "avg_logprob": -0.29650501494712017, "compression_ratio": 1.7114427860696517, "no_speech_prob": 1.1015954441973008e-06}, {"id": 1003, "seek": 467738, "start": 4693.46, "end": 4698.62, "text": " And initially the variances are all one and the means are all zero.", "tokens": [51168, 400, 9105, 264, 1374, 21518, 366, 439, 472, 293, 264, 1355, 366, 439, 4018, 13, 51426], "temperature": 0.0, "avg_logprob": -0.29650501494712017, "compression_ratio": 1.7114427860696517, "no_speech_prob": 1.1015954441973008e-06}, {"id": 1004, "seek": 467738, "start": 4698.62, "end": 4701.66, "text": " And there's one per channel, just like before, or one per filter.", "tokens": [51426, 400, 456, 311, 472, 680, 2269, 11, 445, 411, 949, 11, 420, 472, 680, 6608, 13, 51578], "temperature": 0.0, "avg_logprob": -0.29650501494712017, "compression_ratio": 1.7114427860696517, "no_speech_prob": 1.1015954441973008e-06}, {"id": 1005, "seek": 467738, "start": 4701.66, "end": 4704.18, "text": " This is number of filters, same idea.", "tokens": [51578, 639, 307, 1230, 295, 15995, 11, 912, 1558, 13, 51704], "temperature": 0.0, "avg_logprob": -0.29650501494712017, "compression_ratio": 1.7114427860696517, "no_speech_prob": 1.1015954441973008e-06}, {"id": 1006, "seek": 470418, "start": 4704.18, "end": 4708.54, "text": " I guess filters we tend to actually use inside the model and", "tokens": [50364, 286, 2041, 15995, 321, 3928, 281, 767, 764, 1854, 264, 2316, 293, 50582], "temperature": 0.0, "avg_logprob": -0.38147731210993624, "compression_ratio": 1.5729166666666667, "no_speech_prob": 4.469403575058095e-05}, {"id": 1007, "seek": 470418, "start": 4708.54, "end": 4710.42, "text": " channels we tend to use as the first input.", "tokens": [50582, 9235, 321, 3928, 281, 764, 382, 264, 700, 4846, 13, 50676], "temperature": 0.0, "avg_logprob": -0.38147731210993624, "compression_ratio": 1.5729166666666667, "no_speech_prob": 4.469403575058095e-05}, {"id": 1008, "seek": 470418, "start": 4710.42, "end": 4711.740000000001, "text": " So I should probably say filters.", "tokens": [50676, 407, 286, 820, 1391, 584, 15995, 13, 50742], "temperature": 0.0, "avg_logprob": -0.38147731210993624, "compression_ratio": 1.5729166666666667, "no_speech_prob": 4.469403575058095e-05}, {"id": 1009, "seek": 470418, "start": 4712.860000000001, "end": 4713.740000000001, "text": " Either works though.", "tokens": [50798, 13746, 1985, 1673, 13, 50842], "temperature": 0.0, "avg_logprob": -0.38147731210993624, "compression_ratio": 1.5729166666666667, "no_speech_prob": 4.469403575058095e-05}, {"id": 1010, "seek": 470418, "start": 4716.740000000001, "end": 4720.54, "text": " So we get out, let's for example, we get our mean per filter.", "tokens": [50992, 407, 321, 483, 484, 11, 718, 311, 337, 1365, 11, 321, 483, 527, 914, 680, 6608, 13, 51182], "temperature": 0.0, "avg_logprob": -0.38147731210993624, "compression_ratio": 1.5729166666666667, "no_speech_prob": 4.469403575058095e-05}, {"id": 1011, "seek": 470418, "start": 4720.54, "end": 4723.1, "text": " And then what we do is we use this thing called lerp.", "tokens": [51182, 400, 550, 437, 321, 360, 307, 321, 764, 341, 551, 1219, 32068, 79, 13, 51310], "temperature": 0.0, "avg_logprob": -0.38147731210993624, "compression_ratio": 1.5729166666666667, "no_speech_prob": 4.469403575058095e-05}, {"id": 1012, "seek": 470418, "start": 4723.1, "end": 4725.62, "text": " And lerp is simply saying,", "tokens": [51310, 400, 32068, 79, 307, 2935, 1566, 11, 51436], "temperature": 0.0, "avg_logprob": -0.38147731210993624, "compression_ratio": 1.5729166666666667, "no_speech_prob": 4.469403575058095e-05}, {"id": 1013, "seek": 472562, "start": 4726.18, "end": 4733.58, "text": " Yes, that's what it's done.", "tokens": [50392, 1079, 11, 300, 311, 437, 309, 311, 1096, 13, 50762], "temperature": 0.0, "avg_logprob": -0.38207468233610453, "compression_ratio": 1.514792899408284, "no_speech_prob": 1.7778551409719512e-05}, {"id": 1014, "seek": 472562, "start": 4737.14, "end": 4743.3, "text": " So what lerp does is it takes two numbers, in this case I'm gonna take 5 and", "tokens": [50940, 407, 437, 32068, 79, 775, 307, 309, 2516, 732, 3547, 11, 294, 341, 1389, 286, 478, 799, 747, 1025, 293, 51248], "temperature": 0.0, "avg_logprob": -0.38207468233610453, "compression_ratio": 1.514792899408284, "no_speech_prob": 1.7778551409719512e-05}, {"id": 1015, "seek": 472562, "start": 4743.3, "end": 4748.9, "text": " 15, or two tensors, they could be vectors or matrices.", "tokens": [51248, 2119, 11, 420, 732, 10688, 830, 11, 436, 727, 312, 18875, 420, 32284, 13, 51528], "temperature": 0.0, "avg_logprob": -0.38207468233610453, "compression_ratio": 1.514792899408284, "no_speech_prob": 1.7778551409719512e-05}, {"id": 1016, "seek": 472562, "start": 4748.9, "end": 4751.34, "text": " And it creates a weighted average of them.", "tokens": [51528, 400, 309, 7829, 257, 32807, 4274, 295, 552, 13, 51650], "temperature": 0.0, "avg_logprob": -0.38207468233610453, "compression_ratio": 1.514792899408284, "no_speech_prob": 1.7778551409719512e-05}, {"id": 1017, "seek": 472562, "start": 4751.34, "end": 4755.38, "text": " And the amount of weight it uses is this number here.", "tokens": [51650, 400, 264, 2372, 295, 3364, 309, 4960, 307, 341, 1230, 510, 13, 51852], "temperature": 0.0, "avg_logprob": -0.38207468233610453, "compression_ratio": 1.514792899408284, "no_speech_prob": 1.7778551409719512e-05}, {"id": 1018, "seek": 475538, "start": 4756.18, "end": 4757.18, "text": " Let me explain.", "tokens": [50404, 961, 385, 2903, 13, 50454], "temperature": 0.0, "avg_logprob": -0.36432870229085285, "compression_ratio": 1.6467391304347827, "no_speech_prob": 9.184851990085008e-08}, {"id": 1019, "seek": 475538, "start": 4757.18, "end": 4762.26, "text": " In this case, if I put 0.5, it's gonna take half of this number plus half of", "tokens": [50454, 682, 341, 1389, 11, 498, 286, 829, 1958, 13, 20, 11, 309, 311, 799, 747, 1922, 295, 341, 1230, 1804, 1922, 295, 50708], "temperature": 0.0, "avg_logprob": -0.36432870229085285, "compression_ratio": 1.6467391304347827, "no_speech_prob": 9.184851990085008e-08}, {"id": 1020, "seek": 475538, "start": 4762.26, "end": 4764.1, "text": " this number, so we end up with just the mean.", "tokens": [50708, 341, 1230, 11, 370, 321, 917, 493, 365, 445, 264, 914, 13, 50800], "temperature": 0.0, "avg_logprob": -0.36432870229085285, "compression_ratio": 1.6467391304347827, "no_speech_prob": 9.184851990085008e-08}, {"id": 1021, "seek": 475538, "start": 4766.02, "end": 4767.82, "text": " But what if we used 0.75?", "tokens": [50896, 583, 437, 498, 321, 1143, 1958, 13, 11901, 30, 50986], "temperature": 0.0, "avg_logprob": -0.36432870229085285, "compression_ratio": 1.6467391304347827, "no_speech_prob": 9.184851990085008e-08}, {"id": 1022, "seek": 475538, "start": 4770.46, "end": 4779.62, "text": " Then that's gonna take 0.75 times this number plus 0.25 of this number.", "tokens": [51118, 1396, 300, 311, 799, 747, 1958, 13, 11901, 1413, 341, 1230, 1804, 1958, 13, 6074, 295, 341, 1230, 13, 51576], "temperature": 0.0, "avg_logprob": -0.36432870229085285, "compression_ratio": 1.6467391304347827, "no_speech_prob": 9.184851990085008e-08}, {"id": 1023, "seek": 475538, "start": 4780.9400000000005, "end": 4784.78, "text": " So it's basically kind of allows it to be on like a sliding scale.", "tokens": [51642, 407, 309, 311, 1936, 733, 295, 4045, 309, 281, 312, 322, 411, 257, 21169, 4373, 13, 51834], "temperature": 0.0, "avg_logprob": -0.36432870229085285, "compression_ratio": 1.6467391304347827, "no_speech_prob": 9.184851990085008e-08}, {"id": 1024, "seek": 478478, "start": 4785.46, "end": 4788.099999999999, "text": " One extreme would be to take all of the second number, so", "tokens": [50398, 1485, 8084, 576, 312, 281, 747, 439, 295, 264, 1150, 1230, 11, 370, 50530], "temperature": 0.0, "avg_logprob": -0.370548939704895, "compression_ratio": 1.5941176470588236, "no_speech_prob": 2.1024332852448424e-07}, {"id": 1025, "seek": 478478, "start": 4788.099999999999, "end": 4790.5, "text": " that would be lerp with one there.", "tokens": [50530, 300, 576, 312, 32068, 79, 365, 472, 456, 13, 50650], "temperature": 0.0, "avg_logprob": -0.370548939704895, "compression_ratio": 1.5941176470588236, "no_speech_prob": 2.1024332852448424e-07}, {"id": 1026, "seek": 478478, "start": 4790.5, "end": 4792.82, "text": " And the other extreme would be all of the first number.", "tokens": [50650, 400, 264, 661, 8084, 576, 312, 439, 295, 264, 700, 1230, 13, 50766], "temperature": 0.0, "avg_logprob": -0.370548939704895, "compression_ratio": 1.5941176470588236, "no_speech_prob": 2.1024332852448424e-07}, {"id": 1027, "seek": 478478, "start": 4793.94, "end": 4799.34, "text": " And then you can slide anywhere between them, like so.", "tokens": [50822, 400, 550, 291, 393, 4137, 4992, 1296, 552, 11, 411, 370, 13, 51092], "temperature": 0.0, "avg_logprob": -0.370548939704895, "compression_ratio": 1.5941176470588236, "no_speech_prob": 2.1024332852448424e-07}, {"id": 1028, "seek": 478478, "start": 4799.34, "end": 4805.82, "text": " So that's exactly the same as saying 5 times 0.9", "tokens": [51092, 407, 300, 311, 2293, 264, 912, 382, 1566, 1025, 1413, 1958, 13, 24, 51416], "temperature": 0.0, "avg_logprob": -0.370548939704895, "compression_ratio": 1.5941176470588236, "no_speech_prob": 2.1024332852448424e-07}, {"id": 1029, "seek": 478478, "start": 4805.82, "end": 4812.139999999999, "text": " plus 15 times 0.1.", "tokens": [51416, 1804, 2119, 1413, 1958, 13, 16, 13, 51732], "temperature": 0.0, "avg_logprob": -0.370548939704895, "compression_ratio": 1.5941176470588236, "no_speech_prob": 2.1024332852448424e-07}, {"id": 1030, "seek": 481214, "start": 4812.14, "end": 4819.06, "text": " So this number here is how much of the second number do we have?", "tokens": [50364, 407, 341, 1230, 510, 307, 577, 709, 295, 264, 1150, 1230, 360, 321, 362, 30, 50710], "temperature": 0.0, "avg_logprob": -0.23112502900680693, "compression_ratio": 1.7461928934010151, "no_speech_prob": 7.411280421365518e-06}, {"id": 1031, "seek": 481214, "start": 4819.06, "end": 4822.14, "text": " And 1 minus that is how much of this number do we have?", "tokens": [50710, 400, 502, 3175, 300, 307, 577, 709, 295, 341, 1230, 360, 321, 362, 30, 50864], "temperature": 0.0, "avg_logprob": -0.23112502900680693, "compression_ratio": 1.7461928934010151, "no_speech_prob": 7.411280421365518e-06}, {"id": 1032, "seek": 481214, "start": 4822.14, "end": 4825.9400000000005, "text": " And you can also move this, as you can with most PyTorch things,", "tokens": [50864, 400, 291, 393, 611, 1286, 341, 11, 382, 291, 393, 365, 881, 9953, 51, 284, 339, 721, 11, 51054], "temperature": 0.0, "avg_logprob": -0.23112502900680693, "compression_ratio": 1.7461928934010151, "no_speech_prob": 7.411280421365518e-06}, {"id": 1033, "seek": 481214, "start": 4825.9400000000005, "end": 4832.740000000001, "text": " you can move the first parameter into there and get exactly the same result.", "tokens": [51054, 291, 393, 1286, 264, 700, 13075, 666, 456, 293, 483, 2293, 264, 912, 1874, 13, 51394], "temperature": 0.0, "avg_logprob": -0.23112502900680693, "compression_ratio": 1.7461928934010151, "no_speech_prob": 7.411280421365518e-06}, {"id": 1034, "seek": 481214, "start": 4832.740000000001, "end": 4835.22, "text": " So that's what lerp is.", "tokens": [51394, 407, 300, 311, 437, 32068, 79, 307, 13, 51518], "temperature": 0.0, "avg_logprob": -0.23112502900680693, "compression_ratio": 1.7461928934010151, "no_speech_prob": 7.411280421365518e-06}, {"id": 1035, "seek": 481214, "start": 4836.3, "end": 4840.5, "text": " So what we're doing here is we're doing an in-place lerp.", "tokens": [51572, 407, 437, 321, 434, 884, 510, 307, 321, 434, 884, 364, 294, 12, 6742, 32068, 79, 13, 51782], "temperature": 0.0, "avg_logprob": -0.23112502900680693, "compression_ratio": 1.7461928934010151, "no_speech_prob": 7.411280421365518e-06}, {"id": 1036, "seek": 484050, "start": 4840.5, "end": 4850.18, "text": " So we're replacing self.means with 1- momentum of self.means,", "tokens": [50364, 407, 321, 434, 19139, 2698, 13, 1398, 599, 365, 502, 12, 11244, 295, 2698, 13, 1398, 599, 11, 50848], "temperature": 0.0, "avg_logprob": -0.3175027442700935, "compression_ratio": 1.525974025974026, "no_speech_prob": 1.5143261578032252e-07}, {"id": 1037, "seek": 484050, "start": 4850.18, "end": 4856.58, "text": " and plus self.momentum times this particular mini-batch's mean.", "tokens": [50848, 293, 1804, 2698, 13, 42544, 317, 449, 1413, 341, 1729, 8382, 12, 65, 852, 311, 914, 13, 51168], "temperature": 0.0, "avg_logprob": -0.3175027442700935, "compression_ratio": 1.525974025974026, "no_speech_prob": 1.5143261578032252e-07}, {"id": 1038, "seek": 484050, "start": 4856.58, "end": 4859.66, "text": " So this is basically doing momentum again,", "tokens": [51168, 407, 341, 307, 1936, 884, 11244, 797, 11, 51322], "temperature": 0.0, "avg_logprob": -0.3175027442700935, "compression_ratio": 1.525974025974026, "no_speech_prob": 1.5143261578032252e-07}, {"id": 1039, "seek": 484050, "start": 4859.66, "end": 4863.3, "text": " which is why we indeed are calling the parameter mom for momentum.", "tokens": [51322, 597, 307, 983, 321, 6451, 366, 5141, 264, 13075, 1225, 337, 11244, 13, 51504], "temperature": 0.0, "avg_logprob": -0.3175027442700935, "compression_ratio": 1.525974025974026, "no_speech_prob": 1.5143261578032252e-07}, {"id": 1040, "seek": 486330, "start": 4864.3, "end": 4869.7, "text": " So with a mom of 0.1, which I kind of think is the opposite of what I'd", "tokens": [50414, 407, 365, 257, 1225, 295, 1958, 13, 16, 11, 597, 286, 733, 295, 519, 307, 264, 6182, 295, 437, 286, 1116, 50684], "temperature": 0.0, "avg_logprob": -0.3300687004538143, "compression_ratio": 1.6129032258064515, "no_speech_prob": 7.646535777894314e-06}, {"id": 1041, "seek": 486330, "start": 4869.7, "end": 4873.46, "text": " expect momentum to mean, I'd expect it to be 0.9.", "tokens": [50684, 2066, 11244, 281, 914, 11, 286, 1116, 2066, 309, 281, 312, 1958, 13, 24, 13, 50872], "temperature": 0.0, "avg_logprob": -0.3300687004538143, "compression_ratio": 1.6129032258064515, "no_speech_prob": 7.646535777894314e-06}, {"id": 1042, "seek": 486330, "start": 4873.46, "end": 4879.14, "text": " But with a mom of 0.1, it's saying that each mini-batch,", "tokens": [50872, 583, 365, 257, 1225, 295, 1958, 13, 16, 11, 309, 311, 1566, 300, 1184, 8382, 12, 65, 852, 11, 51156], "temperature": 0.0, "avg_logprob": -0.3300687004538143, "compression_ratio": 1.6129032258064515, "no_speech_prob": 7.646535777894314e-06}, {"id": 1043, "seek": 486330, "start": 4879.14, "end": 4885.58, "text": " self.means will be 0.1 of this particular mini-batch's mean,", "tokens": [51156, 2698, 13, 1398, 599, 486, 312, 1958, 13, 16, 295, 341, 1729, 8382, 12, 65, 852, 311, 914, 11, 51478], "temperature": 0.0, "avg_logprob": -0.3300687004538143, "compression_ratio": 1.6129032258064515, "no_speech_prob": 7.646535777894314e-06}, {"id": 1044, "seek": 486330, "start": 4885.58, "end": 4891.38, "text": " and 0.9 of the previous one, the previous sequence, in fact.", "tokens": [51478, 293, 1958, 13, 24, 295, 264, 3894, 472, 11, 264, 3894, 8310, 11, 294, 1186, 13, 51768], "temperature": 0.0, "avg_logprob": -0.3300687004538143, "compression_ratio": 1.6129032258064515, "no_speech_prob": 7.646535777894314e-06}, {"id": 1045, "seek": 489138, "start": 4892.38, "end": 4896.22, "text": " And that ends up giving us what's called an exponentially weighted", "tokens": [50414, 400, 300, 5314, 493, 2902, 505, 437, 311, 1219, 364, 37330, 32807, 50606], "temperature": 0.0, "avg_logprob": -0.39378774306353403, "compression_ratio": 1.5743589743589743, "no_speech_prob": 9.132546665568952e-07}, {"id": 1046, "seek": 489138, "start": 4896.22, "end": 4897.02, "text": " moving average.", "tokens": [50606, 2684, 4274, 13, 50646], "temperature": 0.0, "avg_logprob": -0.39378774306353403, "compression_ratio": 1.5743589743589743, "no_speech_prob": 9.132546665568952e-07}, {"id": 1047, "seek": 489138, "start": 4898.9400000000005, "end": 4900.58, "text": " And we do the same thing for variances.", "tokens": [50742, 400, 321, 360, 264, 912, 551, 337, 1374, 21518, 13, 50824], "temperature": 0.0, "avg_logprob": -0.39378774306353403, "compression_ratio": 1.5743589743589743, "no_speech_prob": 9.132546665568952e-07}, {"id": 1048, "seek": 489138, "start": 4903.78, "end": 4907.22, "text": " Okay, so that's only updated during training.", "tokens": [50984, 1033, 11, 370, 300, 311, 787, 10588, 1830, 3097, 13, 51156], "temperature": 0.0, "avg_logprob": -0.39378774306353403, "compression_ratio": 1.5743589743589743, "no_speech_prob": 9.132546665568952e-07}, {"id": 1049, "seek": 489138, "start": 4908.58, "end": 4915.58, "text": " Okay, and then during inference, we just use the saved means and variances.", "tokens": [51224, 1033, 11, 293, 550, 1830, 38253, 11, 321, 445, 764, 264, 6624, 1355, 293, 1374, 21518, 13, 51574], "temperature": 0.0, "avg_logprob": -0.39378774306353403, "compression_ratio": 1.5743589743589743, "no_speech_prob": 9.132546665568952e-07}, {"id": 1050, "seek": 489138, "start": 4916.9400000000005, "end": 4919.54, "text": " So this, and then why do we have buffers?", "tokens": [51642, 407, 341, 11, 293, 550, 983, 360, 321, 362, 9204, 433, 30, 51772], "temperature": 0.0, "avg_logprob": -0.39378774306353403, "compression_ratio": 1.5743589743589743, "no_speech_prob": 9.132546665568952e-07}, {"id": 1051, "seek": 489138, "start": 4919.54, "end": 4920.14, "text": " What does that mean?", "tokens": [51772, 708, 775, 300, 914, 30, 51802], "temperature": 0.0, "avg_logprob": -0.39378774306353403, "compression_ratio": 1.5743589743589743, "no_speech_prob": 9.132546665568952e-07}, {"id": 1052, "seek": 492014, "start": 4921.14, "end": 4923.820000000001, "text": " These buffers mean that these means and", "tokens": [50414, 1981, 9204, 433, 914, 300, 613, 1355, 293, 50548], "temperature": 0.0, "avg_logprob": -0.30154599200238236, "compression_ratio": 1.6755555555555555, "no_speech_prob": 5.539171752388938e-07}, {"id": 1053, "seek": 492014, "start": 4923.820000000001, "end": 4928.22, "text": " variances will be actually saved as part of the model.", "tokens": [50548, 1374, 21518, 486, 312, 767, 6624, 382, 644, 295, 264, 2316, 13, 50768], "temperature": 0.0, "avg_logprob": -0.30154599200238236, "compression_ratio": 1.6755555555555555, "no_speech_prob": 5.539171752388938e-07}, {"id": 1054, "seek": 492014, "start": 4928.22, "end": 4933.54, "text": " So it's important to understand that this information about the means and", "tokens": [50768, 407, 309, 311, 1021, 281, 1223, 300, 341, 1589, 466, 264, 1355, 293, 51034], "temperature": 0.0, "avg_logprob": -0.30154599200238236, "compression_ratio": 1.6755555555555555, "no_speech_prob": 5.539171752388938e-07}, {"id": 1055, "seek": 492014, "start": 4933.54, "end": 4938.34, "text": " variances that your model saw are saved in the model.", "tokens": [51034, 1374, 21518, 300, 428, 2316, 1866, 366, 6624, 294, 264, 2316, 13, 51274], "temperature": 0.0, "avg_logprob": -0.30154599200238236, "compression_ratio": 1.6755555555555555, "no_speech_prob": 5.539171752388938e-07}, {"id": 1056, "seek": 492014, "start": 4938.34, "end": 4943.820000000001, "text": " And this is the key thing which makes batch norm very tricky to deal with,", "tokens": [51274, 400, 341, 307, 264, 2141, 551, 597, 1669, 15245, 2026, 588, 12414, 281, 2028, 365, 11, 51548], "temperature": 0.0, "avg_logprob": -0.30154599200238236, "compression_ratio": 1.6755555555555555, "no_speech_prob": 5.539171752388938e-07}, {"id": 1057, "seek": 492014, "start": 4943.820000000001, "end": 4947.660000000001, "text": " and particularly tricky, as we'll see in later lessons, with transfer learning.", "tokens": [51548, 293, 4098, 12414, 11, 382, 321, 603, 536, 294, 1780, 8820, 11, 365, 5003, 2539, 13, 51740], "temperature": 0.0, "avg_logprob": -0.30154599200238236, "compression_ratio": 1.6755555555555555, "no_speech_prob": 5.539171752388938e-07}, {"id": 1058, "seek": 494766, "start": 4948.66, "end": 4953.18, "text": " But what this does do is that it means that we're gonna get something that's", "tokens": [50414, 583, 437, 341, 775, 360, 307, 300, 309, 1355, 300, 321, 434, 799, 483, 746, 300, 311, 50640], "temperature": 0.0, "avg_logprob": -0.2945792763321488, "compression_ratio": 1.7918552036199096, "no_speech_prob": 4.0525461031393206e-07}, {"id": 1059, "seek": 494766, "start": 4953.18, "end": 4954.34, "text": " much smoother.", "tokens": [50640, 709, 28640, 13, 50698], "temperature": 0.0, "avg_logprob": -0.2945792763321488, "compression_ratio": 1.7918552036199096, "no_speech_prob": 4.0525461031393206e-07}, {"id": 1060, "seek": 494766, "start": 4954.34, "end": 4960.0599999999995, "text": " A single, weird mini-batch shouldn't screw things around too much.", "tokens": [50698, 316, 2167, 11, 3657, 8382, 12, 65, 852, 4659, 380, 5630, 721, 926, 886, 709, 13, 50984], "temperature": 0.0, "avg_logprob": -0.2945792763321488, "compression_ratio": 1.7918552036199096, "no_speech_prob": 4.0525461031393206e-07}, {"id": 1061, "seek": 494766, "start": 4960.0599999999995, "end": 4962.0599999999995, "text": " And because we're averaging across the mini-batch,", "tokens": [50984, 400, 570, 321, 434, 47308, 2108, 264, 8382, 12, 65, 852, 11, 51084], "temperature": 0.0, "avg_logprob": -0.2945792763321488, "compression_ratio": 1.7918552036199096, "no_speech_prob": 4.0525461031393206e-07}, {"id": 1062, "seek": 494766, "start": 4962.0599999999995, "end": 4963.38, "text": " it's also gonna make things smoother.", "tokens": [51084, 309, 311, 611, 799, 652, 721, 28640, 13, 51150], "temperature": 0.0, "avg_logprob": -0.2945792763321488, "compression_ratio": 1.7918552036199096, "no_speech_prob": 4.0525461031393206e-07}, {"id": 1063, "seek": 494766, "start": 4963.38, "end": 4967.5, "text": " So this whole thing should lead to a pretty nice, smooth training.", "tokens": [51150, 407, 341, 1379, 551, 820, 1477, 281, 257, 1238, 1481, 11, 5508, 3097, 13, 51356], "temperature": 0.0, "avg_logprob": -0.2945792763321488, "compression_ratio": 1.7918552036199096, "no_speech_prob": 4.0525461031393206e-07}, {"id": 1064, "seek": 494766, "start": 4967.5, "end": 4970.66, "text": " So we can train this.", "tokens": [51356, 407, 321, 393, 3847, 341, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2945792763321488, "compression_ratio": 1.7918552036199096, "no_speech_prob": 4.0525461031393206e-07}, {"id": 1065, "seek": 494766, "start": 4970.66, "end": 4974.58, "text": " So this time we're gonna use our batch norm layer for norm.", "tokens": [51514, 407, 341, 565, 321, 434, 799, 764, 527, 15245, 2026, 4583, 337, 2026, 13, 51710], "temperature": 0.0, "avg_logprob": -0.2945792763321488, "compression_ratio": 1.7918552036199096, "no_speech_prob": 4.0525461031393206e-07}, {"id": 1066, "seek": 497458, "start": 4974.62, "end": 4976.54, "text": " Actually, we need to put the bias thing.", "tokens": [50366, 5135, 11, 321, 643, 281, 829, 264, 12577, 551, 13, 50462], "temperature": 0.0, "avg_logprob": -0.3715199717768916, "compression_ratio": 1.6543778801843319, "no_speech_prob": 1.952587626874447e-05}, {"id": 1067, "seek": 497458, "start": 4976.54, "end": 4978.74, "text": " Is that right?", "tokens": [50462, 1119, 300, 558, 30, 50572], "temperature": 0.0, "avg_logprob": -0.3715199717768916, "compression_ratio": 1.6543778801843319, "no_speech_prob": 1.952587626874447e-05}, {"id": 1068, "seek": 497458, "start": 4978.74, "end": 4981.74, "text": " No, it's, no, that's fine.", "tokens": [50572, 883, 11, 309, 311, 11, 572, 11, 300, 311, 2489, 13, 50722], "temperature": 0.0, "avg_logprob": -0.3715199717768916, "compression_ratio": 1.6543778801843319, "no_speech_prob": 1.952587626874447e-05}, {"id": 1069, "seek": 497458, "start": 4981.74, "end": 4982.34, "text": " Don't need to change that.", "tokens": [50722, 1468, 380, 643, 281, 1319, 300, 13, 50752], "temperature": 0.0, "avg_logprob": -0.3715199717768916, "compression_ratio": 1.6543778801843319, "no_speech_prob": 1.952587626874447e-05}, {"id": 1070, "seek": 497458, "start": 4986.78, "end": 4992.18, "text": " Okay, and one interesting thing I found here is I was able to now finally", "tokens": [50974, 1033, 11, 293, 472, 1880, 551, 286, 1352, 510, 307, 286, 390, 1075, 281, 586, 2721, 51244], "temperature": 0.0, "avg_logprob": -0.3715199717768916, "compression_ratio": 1.6543778801843319, "no_speech_prob": 1.952587626874447e-05}, {"id": 1071, "seek": 497458, "start": 4992.18, "end": 4996.94, "text": " increase the learning rate up to 0.4 for the first time.", "tokens": [51244, 3488, 264, 2539, 3314, 493, 281, 1958, 13, 19, 337, 264, 700, 565, 13, 51482], "temperature": 0.0, "avg_logprob": -0.3715199717768916, "compression_ratio": 1.6543778801843319, "no_speech_prob": 1.952587626874447e-05}, {"id": 1072, "seek": 497458, "start": 4996.94, "end": 5000.3, "text": " So each time I was really trying to see if I can push the learning rate.", "tokens": [51482, 407, 1184, 565, 286, 390, 534, 1382, 281, 536, 498, 286, 393, 2944, 264, 2539, 3314, 13, 51650], "temperature": 0.0, "avg_logprob": -0.3715199717768916, "compression_ratio": 1.6543778801843319, "no_speech_prob": 1.952587626874447e-05}, {"id": 1073, "seek": 497458, "start": 5000.3, "end": 5002.74, "text": " And I'm now able to double the learning rate.", "tokens": [51650, 400, 286, 478, 586, 1075, 281, 3834, 264, 2539, 3314, 13, 51772], "temperature": 0.0, "avg_logprob": -0.3715199717768916, "compression_ratio": 1.6543778801843319, "no_speech_prob": 1.952587626874447e-05}, {"id": 1074, "seek": 500274, "start": 5002.74, "end": 5006.54, "text": " And still, as you can see, it's training very smoothly, which is really cool.", "tokens": [50364, 400, 920, 11, 382, 291, 393, 536, 11, 309, 311, 3097, 588, 19565, 11, 597, 307, 534, 1627, 13, 50554], "temperature": 0.0, "avg_logprob": -0.24759181908198766, "compression_ratio": 1.6381322957198443, "no_speech_prob": 4.936988261761144e-06}, {"id": 1075, "seek": 500274, "start": 5008.0599999999995, "end": 5012.9, "text": " So there's actually a number of different types of layer-based normalization", "tokens": [50630, 407, 456, 311, 767, 257, 1230, 295, 819, 3467, 295, 4583, 12, 6032, 2710, 2144, 50872], "temperature": 0.0, "avg_logprob": -0.24759181908198766, "compression_ratio": 1.6381322957198443, "no_speech_prob": 4.936988261761144e-06}, {"id": 1076, "seek": 500274, "start": 5012.9, "end": 5014.219999999999, "text": " we can use.", "tokens": [50872, 321, 393, 764, 13, 50938], "temperature": 0.0, "avg_logprob": -0.24759181908198766, "compression_ratio": 1.6381322957198443, "no_speech_prob": 4.936988261761144e-06}, {"id": 1077, "seek": 500274, "start": 5014.219999999999, "end": 5018.58, "text": " In this lesson, we specifically seen batch norm and layer norm.", "tokens": [50938, 682, 341, 6898, 11, 321, 4682, 1612, 15245, 2026, 293, 4583, 2026, 13, 51156], "temperature": 0.0, "avg_logprob": -0.24759181908198766, "compression_ratio": 1.6381322957198443, "no_speech_prob": 4.936988261761144e-06}, {"id": 1078, "seek": 500274, "start": 5018.58, "end": 5021.54, "text": " I wanted to mention that there's also instance norm and group norm.", "tokens": [51156, 286, 1415, 281, 2152, 300, 456, 311, 611, 5197, 2026, 293, 1594, 2026, 13, 51304], "temperature": 0.0, "avg_logprob": -0.24759181908198766, "compression_ratio": 1.6381322957198443, "no_speech_prob": 4.936988261761144e-06}, {"id": 1079, "seek": 500274, "start": 5021.54, "end": 5024.5, "text": " And this picture from the group norm paper explains what happens.", "tokens": [51304, 400, 341, 3036, 490, 264, 1594, 2026, 3035, 13948, 437, 2314, 13, 51452], "temperature": 0.0, "avg_logprob": -0.24759181908198766, "compression_ratio": 1.6381322957198443, "no_speech_prob": 4.936988261761144e-06}, {"id": 1080, "seek": 500274, "start": 5026.179999999999, "end": 5030.7, "text": " What it's showing is that we've got here the n, c, h, w.", "tokens": [51536, 708, 309, 311, 4099, 307, 300, 321, 600, 658, 510, 264, 297, 11, 269, 11, 276, 11, 261, 13, 51762], "temperature": 0.0, "avg_logprob": -0.24759181908198766, "compression_ratio": 1.6381322957198443, "no_speech_prob": 4.936988261761144e-06}, {"id": 1081, "seek": 503070, "start": 5030.7, "end": 5034.26, "text": " And so they've kind of concatenated, flattened h, w into a single axis,", "tokens": [50364, 400, 370, 436, 600, 733, 295, 1588, 7186, 770, 11, 24183, 292, 276, 11, 261, 666, 257, 2167, 10298, 11, 50542], "temperature": 0.0, "avg_logprob": -0.2813741510564631, "compression_ratio": 1.7729083665338645, "no_speech_prob": 8.990961077870452e-07}, {"id": 1082, "seek": 503070, "start": 5034.26, "end": 5037.179999999999, "text": " since they can't draw 4D cubes.", "tokens": [50542, 1670, 436, 393, 380, 2642, 1017, 35, 25415, 13, 50688], "temperature": 0.0, "avg_logprob": -0.2813741510564631, "compression_ratio": 1.7729083665338645, "no_speech_prob": 8.990961077870452e-07}, {"id": 1083, "seek": 503070, "start": 5037.179999999999, "end": 5039.54, "text": " And what they're saying is in batch norm,", "tokens": [50688, 400, 437, 436, 434, 1566, 307, 294, 15245, 2026, 11, 50806], "temperature": 0.0, "avg_logprob": -0.2813741510564631, "compression_ratio": 1.7729083665338645, "no_speech_prob": 8.990961077870452e-07}, {"id": 1084, "seek": 503070, "start": 5041.26, "end": 5043.46, "text": " all this blue stuff is what we average over.", "tokens": [50892, 439, 341, 3344, 1507, 307, 437, 321, 4274, 670, 13, 51002], "temperature": 0.0, "avg_logprob": -0.2813741510564631, "compression_ratio": 1.7729083665338645, "no_speech_prob": 8.990961077870452e-07}, {"id": 1085, "seek": 503070, "start": 5043.46, "end": 5048.099999999999, "text": " So we average across the batch and across the height and width.", "tokens": [51002, 407, 321, 4274, 2108, 264, 15245, 293, 2108, 264, 6681, 293, 11402, 13, 51234], "temperature": 0.0, "avg_logprob": -0.2813741510564631, "compression_ratio": 1.7729083665338645, "no_speech_prob": 8.990961077870452e-07}, {"id": 1086, "seek": 503070, "start": 5048.099999999999, "end": 5053.78, "text": " And we end up with one, therefore, normalization number per channel.", "tokens": [51234, 400, 321, 917, 493, 365, 472, 11, 4412, 11, 2710, 2144, 1230, 680, 2269, 13, 51518], "temperature": 0.0, "avg_logprob": -0.2813741510564631, "compression_ratio": 1.7729083665338645, "no_speech_prob": 8.990961077870452e-07}, {"id": 1087, "seek": 503070, "start": 5053.78, "end": 5056.42, "text": " Right, so you can kind of slide these blue blocks across.", "tokens": [51518, 1779, 11, 370, 291, 393, 733, 295, 4137, 613, 3344, 8474, 2108, 13, 51650], "temperature": 0.0, "avg_logprob": -0.2813741510564631, "compression_ratio": 1.7729083665338645, "no_speech_prob": 8.990961077870452e-07}, {"id": 1088, "seek": 503070, "start": 5056.42, "end": 5060.5, "text": " So batch norm is averaging over the batch and height and width.", "tokens": [51650, 407, 15245, 2026, 307, 47308, 670, 264, 15245, 293, 6681, 293, 11402, 13, 51854], "temperature": 0.0, "avg_logprob": -0.2813741510564631, "compression_ratio": 1.7729083665338645, "no_speech_prob": 8.990961077870452e-07}, {"id": 1089, "seek": 506050, "start": 5061.3, "end": 5064.66, "text": " Layer norm, as we learned, averages over the channel and the height and", "tokens": [50404, 35166, 2026, 11, 382, 321, 3264, 11, 42257, 670, 264, 2269, 293, 264, 6681, 293, 50572], "temperature": 0.0, "avg_logprob": -0.29506203622529004, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.933358362293802e-06}, {"id": 1090, "seek": 506050, "start": 5064.66, "end": 5065.22, "text": " the width.", "tokens": [50572, 264, 11402, 13, 50600], "temperature": 0.0, "avg_logprob": -0.29506203622529004, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.933358362293802e-06}, {"id": 1091, "seek": 506050, "start": 5065.22, "end": 5069.82, "text": " And it has a separate one per item in the mini-batch.", "tokens": [50600, 400, 309, 575, 257, 4994, 472, 680, 3174, 294, 264, 8382, 12, 65, 852, 13, 50830], "temperature": 0.0, "avg_logprob": -0.29506203622529004, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.933358362293802e-06}, {"id": 1092, "seek": 506050, "start": 5073.38, "end": 5076.18, "text": " I mean, kind of, it's a bit subtle, right?", "tokens": [51008, 286, 914, 11, 733, 295, 11, 309, 311, 257, 857, 13743, 11, 558, 30, 51148], "temperature": 0.0, "avg_logprob": -0.29506203622529004, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.933358362293802e-06}, {"id": 1093, "seek": 506050, "start": 5076.18, "end": 5080.54, "text": " Because remember the overall molten add,", "tokens": [51148, 1436, 1604, 264, 4787, 44845, 909, 11, 51366], "temperature": 0.0, "avg_logprob": -0.29506203622529004, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.933358362293802e-06}, {"id": 1094, "seek": 506050, "start": 5080.54, "end": 5083.26, "text": " it just had literally a single number for each, right?", "tokens": [51366, 309, 445, 632, 3736, 257, 2167, 1230, 337, 1184, 11, 558, 30, 51502], "temperature": 0.0, "avg_logprob": -0.29506203622529004, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.933358362293802e-06}, {"id": 1095, "seek": 506050, "start": 5083.26, "end": 5086.9, "text": " So it's not quite as simple as this, but that's a general idea.", "tokens": [51502, 407, 309, 311, 406, 1596, 382, 2199, 382, 341, 11, 457, 300, 311, 257, 2674, 1558, 13, 51684], "temperature": 0.0, "avg_logprob": -0.29506203622529004, "compression_ratio": 1.5694444444444444, "no_speech_prob": 1.933358362293802e-06}, {"id": 1096, "seek": 508690, "start": 5086.9, "end": 5088.98, "text": " Instance norm, which we're not looking at today,", "tokens": [50364, 2730, 719, 2026, 11, 597, 321, 434, 406, 1237, 412, 965, 11, 50468], "temperature": 0.0, "avg_logprob": -0.2787616911388579, "compression_ratio": 1.6890756302521008, "no_speech_prob": 4.092911694897339e-06}, {"id": 1097, "seek": 508690, "start": 5090.62, "end": 5094.379999999999, "text": " only averages across height and width.", "tokens": [50550, 787, 42257, 2108, 6681, 293, 11402, 13, 50738], "temperature": 0.0, "avg_logprob": -0.2787616911388579, "compression_ratio": 1.6890756302521008, "no_speech_prob": 4.092911694897339e-06}, {"id": 1098, "seek": 508690, "start": 5094.379999999999, "end": 5096.86, "text": " So there's gonna be a separate one for every channel and", "tokens": [50738, 407, 456, 311, 799, 312, 257, 4994, 472, 337, 633, 2269, 293, 50862], "temperature": 0.0, "avg_logprob": -0.2787616911388579, "compression_ratio": 1.6890756302521008, "no_speech_prob": 4.092911694897339e-06}, {"id": 1099, "seek": 508690, "start": 5096.86, "end": 5098.299999999999, "text": " every element of the mini-batch.", "tokens": [50862, 633, 4478, 295, 264, 8382, 12, 65, 852, 13, 50934], "temperature": 0.0, "avg_logprob": -0.2787616911388579, "compression_ratio": 1.6890756302521008, "no_speech_prob": 4.092911694897339e-06}, {"id": 1100, "seek": 508690, "start": 5100.139999999999, "end": 5105.259999999999, "text": " And then finally, group norm, which I'm quite fond of, is like instance norm, but", "tokens": [51026, 400, 550, 2721, 11, 1594, 2026, 11, 597, 286, 478, 1596, 9557, 295, 11, 307, 411, 5197, 2026, 11, 457, 51282], "temperature": 0.0, "avg_logprob": -0.2787616911388579, "compression_ratio": 1.6890756302521008, "no_speech_prob": 4.092911694897339e-06}, {"id": 1101, "seek": 508690, "start": 5105.259999999999, "end": 5108.66, "text": " it arbitrarily basically groups a bunch of channels together.", "tokens": [51282, 309, 19071, 3289, 1936, 3935, 257, 3840, 295, 9235, 1214, 13, 51452], "temperature": 0.0, "avg_logprob": -0.2787616911388579, "compression_ratio": 1.6890756302521008, "no_speech_prob": 4.092911694897339e-06}, {"id": 1102, "seek": 508690, "start": 5110.339999999999, "end": 5113.0599999999995, "text": " And you can decide how many groups of channels there are and", "tokens": [51536, 400, 291, 393, 4536, 577, 867, 3935, 295, 9235, 456, 366, 293, 51672], "temperature": 0.0, "avg_logprob": -0.2787616911388579, "compression_ratio": 1.6890756302521008, "no_speech_prob": 4.092911694897339e-06}, {"id": 1103, "seek": 508690, "start": 5113.0599999999995, "end": 5114.339999999999, "text": " averages over them.", "tokens": [51672, 42257, 670, 552, 13, 51736], "temperature": 0.0, "avg_logprob": -0.2787616911388579, "compression_ratio": 1.6890756302521008, "no_speech_prob": 4.092911694897339e-06}, {"id": 1104, "seek": 511434, "start": 5114.34, "end": 5116.66, "text": " Group norm tends to be a bit slow, unfortunately,", "tokens": [50364, 10500, 2026, 12258, 281, 312, 257, 857, 2964, 11, 7015, 11, 50480], "temperature": 0.0, "avg_logprob": -0.29601984345511106, "compression_ratio": 1.5774647887323943, "no_speech_prob": 2.769405455183005e-06}, {"id": 1105, "seek": 511434, "start": 5116.66, "end": 5119.66, "text": " because the way these things are implemented is a bit tricky.", "tokens": [50480, 570, 264, 636, 613, 721, 366, 12270, 307, 257, 857, 12414, 13, 50630], "temperature": 0.0, "avg_logprob": -0.29601984345511106, "compression_ratio": 1.5774647887323943, "no_speech_prob": 2.769405455183005e-06}, {"id": 1106, "seek": 511434, "start": 5119.66, "end": 5124.34, "text": " But group norm does allow you to, yeah,", "tokens": [50630, 583, 1594, 2026, 775, 2089, 291, 281, 11, 1338, 11, 50864], "temperature": 0.0, "avg_logprob": -0.29601984345511106, "compression_ratio": 1.5774647887323943, "no_speech_prob": 2.769405455183005e-06}, {"id": 1107, "seek": 511434, "start": 5124.34, "end": 5128.14, "text": " avoid some of the challenges of some of the other methods.", "tokens": [50864, 5042, 512, 295, 264, 4759, 295, 512, 295, 264, 661, 7150, 13, 51054], "temperature": 0.0, "avg_logprob": -0.29601984345511106, "compression_ratio": 1.5774647887323943, "no_speech_prob": 2.769405455183005e-06}, {"id": 1108, "seek": 511434, "start": 5129.62, "end": 5132.22, "text": " So it's worth trying if you can.", "tokens": [51128, 407, 309, 311, 3163, 1382, 498, 291, 393, 13, 51258], "temperature": 0.0, "avg_logprob": -0.29601984345511106, "compression_ratio": 1.5774647887323943, "no_speech_prob": 2.769405455183005e-06}, {"id": 1109, "seek": 511434, "start": 5133.78, "end": 5137.3, "text": " And of course, batch norm has the additional thing of the kind of", "tokens": [51336, 400, 295, 1164, 11, 15245, 2026, 575, 264, 4497, 551, 295, 264, 733, 295, 51512], "temperature": 0.0, "avg_logprob": -0.29601984345511106, "compression_ratio": 1.5774647887323943, "no_speech_prob": 2.769405455183005e-06}, {"id": 1110, "seek": 511434, "start": 5137.3, "end": 5139.26, "text": " momentum-based statistics.", "tokens": [51512, 11244, 12, 6032, 12523, 13, 51610], "temperature": 0.0, "avg_logprob": -0.29601984345511106, "compression_ratio": 1.5774647887323943, "no_speech_prob": 2.769405455183005e-06}, {"id": 1111, "seek": 513926, "start": 5139.3, "end": 5145.3, "text": " But in general, the idea of do you use momentum-based statistics?", "tokens": [50366, 583, 294, 2674, 11, 264, 1558, 295, 360, 291, 764, 11244, 12, 6032, 12523, 30, 50666], "temperature": 0.0, "avg_logprob": -0.37983985380692914, "compression_ratio": 1.543103448275862, "no_speech_prob": 4.289331627660431e-06}, {"id": 1112, "seek": 513926, "start": 5145.3, "end": 5149.1, "text": " Do you store things per channel or a single mean and", "tokens": [50666, 1144, 291, 3531, 721, 680, 2269, 420, 257, 2167, 914, 293, 50856], "temperature": 0.0, "avg_logprob": -0.37983985380692914, "compression_ratio": 1.543103448275862, "no_speech_prob": 4.289331627660431e-06}, {"id": 1113, "seek": 513926, "start": 5149.1, "end": 5151.46, "text": " variance in your buffers or whatever?", "tokens": [50856, 21977, 294, 428, 9204, 433, 420, 2035, 30, 50974], "temperature": 0.0, "avg_logprob": -0.37983985380692914, "compression_ratio": 1.543103448275862, "no_speech_prob": 4.289331627660431e-06}, {"id": 1114, "seek": 513926, "start": 5153.58, "end": 5156.22, "text": " All that kind of stuff along with what you average over.", "tokens": [51080, 1057, 300, 733, 295, 1507, 2051, 365, 437, 291, 4274, 670, 13, 51212], "temperature": 0.0, "avg_logprob": -0.37983985380692914, "compression_ratio": 1.543103448275862, "no_speech_prob": 4.289331627660431e-06}, {"id": 1115, "seek": 513926, "start": 5156.22, "end": 5158.9400000000005, "text": " They're all somewhat independent choices you can make and", "tokens": [51212, 814, 434, 439, 8344, 6695, 7994, 291, 393, 652, 293, 51348], "temperature": 0.0, "avg_logprob": -0.37983985380692914, "compression_ratio": 1.543103448275862, "no_speech_prob": 4.289331627660431e-06}, {"id": 1116, "seek": 513926, "start": 5158.9400000000005, "end": 5162.34, "text": " particular combinations of those have been given particular names.", "tokens": [51348, 1729, 21267, 295, 729, 362, 668, 2212, 1729, 5288, 13, 51518], "temperature": 0.0, "avg_logprob": -0.37983985380692914, "compression_ratio": 1.543103448275862, "no_speech_prob": 4.289331627660431e-06}, {"id": 1117, "seek": 513926, "start": 5162.34, "end": 5164.26, "text": " And so there we go.", "tokens": [51518, 400, 370, 456, 321, 352, 13, 51614], "temperature": 0.0, "avg_logprob": -0.37983985380692914, "compression_ratio": 1.543103448275862, "no_speech_prob": 4.289331627660431e-06}, {"id": 1118, "seek": 516426, "start": 5165.26, "end": 5171.5, "text": " Okay, so we've got some good initialization methods here.", "tokens": [50414, 1033, 11, 370, 321, 600, 658, 512, 665, 5883, 2144, 7150, 510, 13, 50726], "temperature": 0.0, "avg_logprob": -0.36187907059987384, "compression_ratio": 1.4654377880184333, "no_speech_prob": 3.340538341944921e-06}, {"id": 1119, "seek": 516426, "start": 5171.5, "end": 5173.74, "text": " Let's try putting them all together.", "tokens": [50726, 961, 311, 853, 3372, 552, 439, 1214, 13, 50838], "temperature": 0.0, "avg_logprob": -0.36187907059987384, "compression_ratio": 1.4654377880184333, "no_speech_prob": 3.340538341944921e-06}, {"id": 1120, "seek": 516426, "start": 5173.74, "end": 5178.02, "text": " And one other thing we can do is,", "tokens": [50838, 400, 472, 661, 551, 321, 393, 360, 307, 11, 51052], "temperature": 0.0, "avg_logprob": -0.36187907059987384, "compression_ratio": 1.4654377880184333, "no_speech_prob": 3.340538341944921e-06}, {"id": 1121, "seek": 516426, "start": 5178.02, "end": 5183.26, "text": " we've been using a batch size of 1,020 for speed purposes.", "tokens": [51052, 321, 600, 668, 1228, 257, 15245, 2744, 295, 502, 11, 15, 2009, 337, 3073, 9932, 13, 51314], "temperature": 0.0, "avg_logprob": -0.36187907059987384, "compression_ratio": 1.4654377880184333, "no_speech_prob": 3.340538341944921e-06}, {"id": 1122, "seek": 516426, "start": 5183.26, "end": 5185.34, "text": " If we drop it down a bit to 256,", "tokens": [51314, 759, 321, 3270, 309, 760, 257, 857, 281, 38882, 11, 51418], "temperature": 0.0, "avg_logprob": -0.36187907059987384, "compression_ratio": 1.4654377880184333, "no_speech_prob": 3.340538341944921e-06}, {"id": 1123, "seek": 516426, "start": 5185.34, "end": 5189.18, "text": " it's gonna mean that it's gonna get to see more mini-batches.", "tokens": [51418, 309, 311, 799, 914, 300, 309, 311, 799, 483, 281, 536, 544, 8382, 12, 65, 852, 279, 13, 51610], "temperature": 0.0, "avg_logprob": -0.36187907059987384, "compression_ratio": 1.4654377880184333, "no_speech_prob": 3.340538341944921e-06}, {"id": 1124, "seek": 516426, "start": 5189.18, "end": 5191.62, "text": " So that should improve performance.", "tokens": [51610, 407, 300, 820, 3470, 3389, 13, 51732], "temperature": 0.0, "avg_logprob": -0.36187907059987384, "compression_ratio": 1.4654377880184333, "no_speech_prob": 3.340538341944921e-06}, {"id": 1125, "seek": 519162, "start": 5191.9, "end": 5195.099999999999, "text": " And so we're trying to get to 90%, remember.", "tokens": [50378, 400, 370, 321, 434, 1382, 281, 483, 281, 4289, 8923, 1604, 13, 50538], "temperature": 0.0, "avg_logprob": -0.3312837852621978, "compression_ratio": 1.5305164319248827, "no_speech_prob": 2.6425859687151387e-06}, {"id": 1126, "seek": 519162, "start": 5195.099999999999, "end": 5199.7, "text": " So let's, yeah, do all this.", "tokens": [50538, 407, 718, 311, 11, 1338, 11, 360, 439, 341, 13, 50768], "temperature": 0.0, "avg_logprob": -0.3312837852621978, "compression_ratio": 1.5305164319248827, "no_speech_prob": 2.6425859687151387e-06}, {"id": 1127, "seek": 519162, "start": 5199.7, "end": 5202.26, "text": " This time we'll use PyTorch as its own batch norm.", "tokens": [50768, 639, 565, 321, 603, 764, 9953, 51, 284, 339, 382, 1080, 1065, 15245, 2026, 13, 50896], "temperature": 0.0, "avg_logprob": -0.3312837852621978, "compression_ratio": 1.5305164319248827, "no_speech_prob": 2.6425859687151387e-06}, {"id": 1128, "seek": 519162, "start": 5202.26, "end": 5203.62, "text": " We'll just use PyTorches.", "tokens": [50896, 492, 603, 445, 764, 9953, 51, 284, 3781, 13, 50964], "temperature": 0.0, "avg_logprob": -0.3312837852621978, "compression_ratio": 1.5305164319248827, "no_speech_prob": 2.6425859687151387e-06}, {"id": 1129, "seek": 519162, "start": 5203.62, "end": 5208.18, "text": " There's nothing wrong with ours, but we try to switch to PyTorches when", "tokens": [50964, 821, 311, 1825, 2085, 365, 11896, 11, 457, 321, 853, 281, 3679, 281, 9953, 51, 284, 3781, 562, 51192], "temperature": 0.0, "avg_logprob": -0.3312837852621978, "compression_ratio": 1.5305164319248827, "no_speech_prob": 2.6425859687151387e-06}, {"id": 1130, "seek": 519162, "start": 5208.18, "end": 5210.7, "text": " something we've recreated exists there.", "tokens": [51192, 746, 321, 600, 850, 26559, 8198, 456, 13, 51318], "temperature": 0.0, "avg_logprob": -0.3312837852621978, "compression_ratio": 1.5305164319248827, "no_speech_prob": 2.6425859687151387e-06}, {"id": 1131, "seek": 519162, "start": 5210.7, "end": 5212.3, "text": " We'll use our momentum learner.", "tokens": [51318, 492, 603, 764, 527, 11244, 33347, 13, 51398], "temperature": 0.0, "avg_logprob": -0.3312837852621978, "compression_ratio": 1.5305164319248827, "no_speech_prob": 2.6425859687151387e-06}, {"id": 1132, "seek": 519162, "start": 5215.34, "end": 5218.58, "text": " And we'll fit for three epochs.", "tokens": [51550, 400, 321, 603, 3318, 337, 1045, 30992, 28346, 13, 51712], "temperature": 0.0, "avg_logprob": -0.3312837852621978, "compression_ratio": 1.5305164319248827, "no_speech_prob": 2.6425859687151387e-06}, {"id": 1133, "seek": 521858, "start": 5218.58, "end": 5223.26, "text": " And so as you can see, it's going a little bit more slowly now.", "tokens": [50364, 400, 370, 382, 291, 393, 536, 11, 309, 311, 516, 257, 707, 857, 544, 5692, 586, 13, 50598], "temperature": 0.0, "avg_logprob": -0.34680108179019975, "compression_ratio": 1.5824175824175823, "no_speech_prob": 2.8291330522733915e-07}, {"id": 1134, "seek": 521858, "start": 5223.26, "end": 5227.54, "text": " And then the other thing I'm gonna do is I'm going to", "tokens": [50598, 400, 550, 264, 661, 551, 286, 478, 799, 360, 307, 286, 478, 516, 281, 50812], "temperature": 0.0, "avg_logprob": -0.34680108179019975, "compression_ratio": 1.5824175824175823, "no_speech_prob": 2.8291330522733915e-07}, {"id": 1135, "seek": 521858, "start": 5228.7, "end": 5232.0199999999995, "text": " decrease the learning rate and keep the existing model.", "tokens": [50870, 11514, 264, 2539, 3314, 293, 1066, 264, 6741, 2316, 13, 51036], "temperature": 0.0, "avg_logprob": -0.34680108179019975, "compression_ratio": 1.5824175824175823, "no_speech_prob": 2.8291330522733915e-07}, {"id": 1136, "seek": 521858, "start": 5233.58, "end": 5236.46, "text": " And then train for a little bit longer.", "tokens": [51114, 400, 550, 3847, 337, 257, 707, 857, 2854, 13, 51258], "temperature": 0.0, "avg_logprob": -0.34680108179019975, "compression_ratio": 1.5824175824175823, "no_speech_prob": 2.8291330522733915e-07}, {"id": 1137, "seek": 521858, "start": 5237.5, "end": 5243.98, "text": " The idea being that as it's kind of getting close to a pretty good answer,", "tokens": [51310, 440, 1558, 885, 300, 382, 309, 311, 733, 295, 1242, 1998, 281, 257, 1238, 665, 1867, 11, 51634], "temperature": 0.0, "avg_logprob": -0.34680108179019975, "compression_ratio": 1.5824175824175823, "no_speech_prob": 2.8291330522733915e-07}, {"id": 1138, "seek": 524398, "start": 5244.0199999999995, "end": 5249.62, "text": " maybe it just wants to be able to fine-tune that a little bit.", "tokens": [50366, 1310, 309, 445, 2738, 281, 312, 1075, 281, 2489, 12, 83, 2613, 300, 257, 707, 857, 13, 50646], "temperature": 0.0, "avg_logprob": -0.3191285974839154, "compression_ratio": 1.4943181818181819, "no_speech_prob": 1.300711119256448e-05}, {"id": 1139, "seek": 524398, "start": 5249.62, "end": 5252.299999999999, "text": " And so by decreasing the learning rate,", "tokens": [50646, 400, 370, 538, 23223, 264, 2539, 3314, 11, 50780], "temperature": 0.0, "avg_logprob": -0.3191285974839154, "compression_ratio": 1.4943181818181819, "no_speech_prob": 1.300711119256448e-05}, {"id": 1140, "seek": 524398, "start": 5252.299999999999, "end": 5255.82, "text": " we give it a chance to fine-tune a little bit.", "tokens": [50780, 321, 976, 309, 257, 2931, 281, 2489, 12, 83, 2613, 257, 707, 857, 13, 50956], "temperature": 0.0, "avg_logprob": -0.3191285974839154, "compression_ratio": 1.4943181818181819, "no_speech_prob": 1.300711119256448e-05}, {"id": 1141, "seek": 524398, "start": 5257.219999999999, "end": 5259.82, "text": " So let's see, how are we going?", "tokens": [51026, 407, 718, 311, 536, 11, 577, 366, 321, 516, 30, 51156], "temperature": 0.0, "avg_logprob": -0.3191285974839154, "compression_ratio": 1.4943181818181819, "no_speech_prob": 1.300711119256448e-05}, {"id": 1142, "seek": 524398, "start": 5259.82, "end": 5264.259999999999, "text": " So we got to 87.8% accuracy after three epochs,", "tokens": [51156, 407, 321, 658, 281, 27990, 13, 23, 4, 14170, 934, 1045, 30992, 28346, 11, 51378], "temperature": 0.0, "avg_logprob": -0.3191285974839154, "compression_ratio": 1.4943181818181819, "no_speech_prob": 1.300711119256448e-05}, {"id": 1143, "seek": 524398, "start": 5264.259999999999, "end": 5270.339999999999, "text": " which is an improvement, I guess,", "tokens": [51378, 597, 307, 364, 10444, 11, 286, 2041, 11, 51682], "temperature": 0.0, "avg_logprob": -0.3191285974839154, "compression_ratio": 1.4943181818181819, "no_speech_prob": 1.300711119256448e-05}, {"id": 1144, "seek": 527034, "start": 5271.3, "end": 5276.02, "text": " basically thanks to using this smaller mini-batch size.", "tokens": [50412, 1936, 3231, 281, 1228, 341, 4356, 8382, 12, 65, 852, 2744, 13, 50648], "temperature": 0.0, "avg_logprob": -0.2967285223766766, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.00024156425206456333}, {"id": 1145, "seek": 527034, "start": 5276.02, "end": 5279.7, "text": " Now with a smaller mini-batch size, you do have to decrease the learning rate.", "tokens": [50648, 823, 365, 257, 4356, 8382, 12, 65, 852, 2744, 11, 291, 360, 362, 281, 11514, 264, 2539, 3314, 13, 50832], "temperature": 0.0, "avg_logprob": -0.2967285223766766, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.00024156425206456333}, {"id": 1146, "seek": 527034, "start": 5279.7, "end": 5282.42, "text": " So I found I could still get away with 0.2, which is pretty cool.", "tokens": [50832, 407, 286, 1352, 286, 727, 920, 483, 1314, 365, 1958, 13, 17, 11, 597, 307, 1238, 1627, 13, 50968], "temperature": 0.0, "avg_logprob": -0.2967285223766766, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.00024156425206456333}, {"id": 1147, "seek": 527034, "start": 5283.900000000001, "end": 5287.14, "text": " And look at this, after just one more epoch, by decreasing the learning rate,", "tokens": [51042, 400, 574, 412, 341, 11, 934, 445, 472, 544, 30992, 339, 11, 538, 23223, 264, 2539, 3314, 11, 51204], "temperature": 0.0, "avg_logprob": -0.2967285223766766, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.00024156425206456333}, {"id": 1148, "seek": 527034, "start": 5287.14, "end": 5290.42, "text": " we've got up to 89.7.", "tokens": [51204, 321, 600, 658, 493, 281, 31877, 13, 22, 13, 51368], "temperature": 0.0, "avg_logprob": -0.2967285223766766, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.00024156425206456333}, {"id": 1149, "seek": 527034, "start": 5290.42, "end": 5298.1, "text": " We didn't make it, 89.9, so towards 90%, but not quite 90%, 89.9.", "tokens": [51368, 492, 994, 380, 652, 309, 11, 31877, 13, 24, 11, 370, 3030, 4289, 8923, 457, 406, 1596, 4289, 8923, 31877, 13, 24, 13, 51752], "temperature": 0.0, "avg_logprob": -0.2967285223766766, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.00024156425206456333}, {"id": 1150, "seek": 529810, "start": 5298.1, "end": 5302.700000000001, "text": " So we're gonna have to do some more work to get up to our magical 90% number,", "tokens": [50364, 407, 321, 434, 799, 362, 281, 360, 512, 544, 589, 281, 483, 493, 281, 527, 12066, 4289, 4, 1230, 11, 50594], "temperature": 0.0, "avg_logprob": -0.4185542595095751, "compression_ratio": 1.415, "no_speech_prob": 7.071892923704581e-06}, {"id": 1151, "seek": 529810, "start": 5302.700000000001, "end": 5304.46, "text": " but we are getting pretty close.", "tokens": [50594, 457, 321, 366, 1242, 1238, 1998, 13, 50682], "temperature": 0.0, "avg_logprob": -0.4185542595095751, "compression_ratio": 1.415, "no_speech_prob": 7.071892923704581e-06}, {"id": 1152, "seek": 529810, "start": 5307.46, "end": 5312.3, "text": " All right, so that is the end of initialization,", "tokens": [50832, 1057, 558, 11, 370, 300, 307, 264, 917, 295, 5883, 2144, 11, 51074], "temperature": 0.0, "avg_logprob": -0.4185542595095751, "compression_ratio": 1.415, "no_speech_prob": 7.071892923704581e-06}, {"id": 1153, "seek": 529810, "start": 5312.3, "end": 5318.900000000001, "text": " an incredibly important topic, as hopefully you've seen.", "tokens": [51074, 364, 6252, 1021, 4829, 11, 382, 4696, 291, 600, 1612, 13, 51404], "temperature": 0.0, "avg_logprob": -0.4185542595095751, "compression_ratio": 1.415, "no_speech_prob": 7.071892923704581e-06}, {"id": 1154, "seek": 529810, "start": 5323.18, "end": 5328.06, "text": " Accelerated SGD, let's see if we can use this to get us up to 90%.", "tokens": [51618, 5725, 6185, 770, 34520, 35, 11, 718, 311, 536, 498, 321, 393, 764, 341, 281, 483, 505, 493, 281, 4289, 6856, 51862], "temperature": 0.0, "avg_logprob": -0.4185542595095751, "compression_ratio": 1.415, "no_speech_prob": 7.071892923704581e-06}, {"id": 1155, "seek": 532810, "start": 5329.06, "end": 5331.54, "text": " Above 90%.", "tokens": [50412, 32691, 4289, 6856, 50536], "temperature": 0.0, "avg_logprob": -0.34898976926450376, "compression_ratio": 1.7359307359307359, "no_speech_prob": 8.530310878995806e-06}, {"id": 1156, "seek": 532810, "start": 5331.54, "end": 5335.5, "text": " So let's do our normal imports and data setup as usual.", "tokens": [50536, 407, 718, 311, 360, 527, 2710, 41596, 293, 1412, 8657, 382, 7713, 13, 50734], "temperature": 0.0, "avg_logprob": -0.34898976926450376, "compression_ratio": 1.7359307359307359, "no_speech_prob": 8.530310878995806e-06}, {"id": 1157, "seek": 532810, "start": 5336.5, "end": 5339.900000000001, "text": " And so just to summarize what we've got, we've got our metrics callback.", "tokens": [50784, 400, 370, 445, 281, 20858, 437, 321, 600, 658, 11, 321, 600, 658, 527, 16367, 818, 3207, 13, 50954], "temperature": 0.0, "avg_logprob": -0.34898976926450376, "compression_ratio": 1.7359307359307359, "no_speech_prob": 8.530310878995806e-06}, {"id": 1158, "seek": 532810, "start": 5340.9400000000005, "end": 5344.58, "text": " We've got our activation stats on the general value.", "tokens": [51006, 492, 600, 658, 527, 24433, 18152, 322, 264, 2674, 2158, 13, 51188], "temperature": 0.0, "avg_logprob": -0.34898976926450376, "compression_ratio": 1.7359307359307359, "no_speech_prob": 8.530310878995806e-06}, {"id": 1159, "seek": 532810, "start": 5344.58, "end": 5347.3, "text": " So our callbacks are gonna be the device callback, put it on CUDA or", "tokens": [51188, 407, 527, 818, 17758, 366, 799, 312, 264, 4302, 818, 3207, 11, 829, 309, 322, 29777, 7509, 420, 51324], "temperature": 0.0, "avg_logprob": -0.34898976926450376, "compression_ratio": 1.7359307359307359, "no_speech_prob": 8.530310878995806e-06}, {"id": 1160, "seek": 532810, "start": 5347.3, "end": 5351.06, "text": " whatever, the metrics, the progress bar, the activation stats.", "tokens": [51324, 2035, 11, 264, 16367, 11, 264, 4205, 2159, 11, 264, 24433, 18152, 13, 51512], "temperature": 0.0, "avg_logprob": -0.34898976926450376, "compression_ratio": 1.7359307359307359, "no_speech_prob": 8.530310878995806e-06}, {"id": 1161, "seek": 532810, "start": 5352.14, "end": 5356.860000000001, "text": " Our activation function is gonna be our general value with 0.1 leakiness and", "tokens": [51566, 2621, 24433, 2445, 307, 799, 312, 527, 2674, 2158, 365, 1958, 13, 16, 17143, 1324, 293, 51802], "temperature": 0.0, "avg_logprob": -0.34898976926450376, "compression_ratio": 1.7359307359307359, "no_speech_prob": 8.530310878995806e-06}, {"id": 1162, "seek": 535686, "start": 5356.94, "end": 5358.299999999999, "text": " 0.4 subtraction.", "tokens": [50368, 1958, 13, 19, 16390, 313, 13, 50436], "temperature": 0.0, "avg_logprob": -0.32812442098345074, "compression_ratio": 1.6182572614107884, "no_speech_prob": 1.4510467735817656e-05}, {"id": 1163, "seek": 535686, "start": 5359.86, "end": 5365.339999999999, "text": " And we've got the in it weights, which we need to tell it about how leaky they are.", "tokens": [50514, 400, 321, 600, 658, 264, 294, 309, 17443, 11, 597, 321, 643, 281, 980, 309, 466, 577, 476, 15681, 436, 366, 13, 50788], "temperature": 0.0, "avg_logprob": -0.32812442098345074, "compression_ratio": 1.6182572614107884, "no_speech_prob": 1.4510467735817656e-05}, {"id": 1164, "seek": 535686, "start": 5365.339999999999, "end": 5367.299999999999, "text": " And then if we're doing a learning rate finder,", "tokens": [50788, 400, 550, 498, 321, 434, 884, 257, 2539, 3314, 915, 260, 11, 50886], "temperature": 0.0, "avg_logprob": -0.32812442098345074, "compression_ratio": 1.6182572614107884, "no_speech_prob": 1.4510467735817656e-05}, {"id": 1165, "seek": 535686, "start": 5367.299999999999, "end": 5370.339999999999, "text": " we've got a different set of callbacks.", "tokens": [50886, 321, 600, 658, 257, 819, 992, 295, 818, 17758, 13, 51038], "temperature": 0.0, "avg_logprob": -0.32812442098345074, "compression_ratio": 1.6182572614107884, "no_speech_prob": 1.4510467735817656e-05}, {"id": 1166, "seek": 535686, "start": 5370.339999999999, "end": 5373.54, "text": " So there's no real reason to have a progress bar callback for", "tokens": [51038, 407, 456, 311, 572, 957, 1778, 281, 362, 257, 4205, 2159, 818, 3207, 337, 51198], "temperature": 0.0, "avg_logprob": -0.32812442098345074, "compression_ratio": 1.6182572614107884, "no_speech_prob": 1.4510467735817656e-05}, {"id": 1167, "seek": 535686, "start": 5373.54, "end": 5376.339999999999, "text": " the learning rate finder, I guess, it's pretty short anyway.", "tokens": [51198, 264, 2539, 3314, 915, 260, 11, 286, 2041, 11, 309, 311, 1238, 2099, 4033, 13, 51338], "temperature": 0.0, "avg_logprob": -0.32812442098345074, "compression_ratio": 1.6182572614107884, "no_speech_prob": 1.4510467735817656e-05}, {"id": 1168, "seek": 535686, "start": 5377.78, "end": 5382.94, "text": " Which reminds me, there was one little thing I didn't mention in initializing.", "tokens": [51410, 3013, 12025, 385, 11, 456, 390, 472, 707, 551, 286, 994, 380, 2152, 294, 5883, 3319, 13, 51668], "temperature": 0.0, "avg_logprob": -0.32812442098345074, "compression_ratio": 1.6182572614107884, "no_speech_prob": 1.4510467735817656e-05}, {"id": 1169, "seek": 538294, "start": 5383.94, "end": 5389.099999999999, "text": " Which is a fun trick you might wanna play around with.", "tokens": [50414, 3013, 307, 257, 1019, 4282, 291, 1062, 1948, 862, 926, 365, 13, 50672], "temperature": 0.0, "avg_logprob": -0.33845370156424387, "compression_ratio": 1.555, "no_speech_prob": 4.936988261761144e-06}, {"id": 1170, "seek": 538294, "start": 5390.419999999999, "end": 5394.82, "text": " And in fact, Sam Watkins asked a question earlier in the chat and", "tokens": [50738, 400, 294, 1186, 11, 4832, 12593, 10277, 2351, 257, 1168, 3071, 294, 264, 5081, 293, 50958], "temperature": 0.0, "avg_logprob": -0.33845370156424387, "compression_ratio": 1.555, "no_speech_prob": 4.936988261761144e-06}, {"id": 1171, "seek": 538294, "start": 5394.82, "end": 5398.0199999999995, "text": " I didn't answer it because it's actually exactly here.", "tokens": [50958, 286, 994, 380, 1867, 309, 570, 309, 311, 767, 2293, 510, 13, 51118], "temperature": 0.0, "avg_logprob": -0.33845370156424387, "compression_ratio": 1.555, "no_speech_prob": 4.936988261761144e-06}, {"id": 1172, "seek": 538294, "start": 5399.419999999999, "end": 5402.78, "text": " In general value, I added a second thing you might have seen,", "tokens": [51188, 682, 2674, 2158, 11, 286, 3869, 257, 1150, 551, 291, 1062, 362, 1612, 11, 51356], "temperature": 0.0, "avg_logprob": -0.33845370156424387, "compression_ratio": 1.555, "no_speech_prob": 4.936988261761144e-06}, {"id": 1173, "seek": 538294, "start": 5402.78, "end": 5404.139999999999, "text": " which is the maximum value.", "tokens": [51356, 597, 307, 264, 6674, 2158, 13, 51424], "temperature": 0.0, "avg_logprob": -0.33845370156424387, "compression_ratio": 1.555, "no_speech_prob": 4.936988261761144e-06}, {"id": 1174, "seek": 538294, "start": 5405.339999999999, "end": 5409.82, "text": " And if the maximum value is set, then I clamp", "tokens": [51484, 400, 498, 264, 6674, 2158, 307, 992, 11, 550, 286, 17690, 51708], "temperature": 0.0, "avg_logprob": -0.33845370156424387, "compression_ratio": 1.555, "no_speech_prob": 4.936988261761144e-06}, {"id": 1175, "seek": 540982, "start": 5410.82, "end": 5414.219999999999, "text": " The value to be no more than the maximum.", "tokens": [50414, 440, 2158, 281, 312, 572, 544, 813, 264, 6674, 13, 50584], "temperature": 0.0, "avg_logprob": -0.3403120214288885, "compression_ratio": 1.7735849056603774, "no_speech_prob": 1.1300799997115973e-05}, {"id": 1176, "seek": 540982, "start": 5414.219999999999, "end": 5417.82, "text": " So basically as a result, let's say you set it to three,", "tokens": [50584, 407, 1936, 382, 257, 1874, 11, 718, 311, 584, 291, 992, 309, 281, 1045, 11, 50764], "temperature": 0.0, "avg_logprob": -0.3403120214288885, "compression_ratio": 1.7735849056603774, "no_speech_prob": 1.1300799997115973e-05}, {"id": 1177, "seek": 540982, "start": 5417.82, "end": 5420.46, "text": " then the line would go up to here like it does here.", "tokens": [50764, 550, 264, 1622, 576, 352, 493, 281, 510, 411, 309, 775, 510, 13, 50896], "temperature": 0.0, "avg_logprob": -0.3403120214288885, "compression_ratio": 1.7735849056603774, "no_speech_prob": 1.1300799997115973e-05}, {"id": 1178, "seek": 540982, "start": 5420.46, "end": 5423.0199999999995, "text": " And then it would go up to three like it does here, and then it would be flat.", "tokens": [50896, 400, 550, 309, 576, 352, 493, 281, 1045, 411, 309, 775, 510, 11, 293, 550, 309, 576, 312, 4962, 13, 51024], "temperature": 0.0, "avg_logprob": -0.3403120214288885, "compression_ratio": 1.7735849056603774, "no_speech_prob": 1.1300799997115973e-05}, {"id": 1179, "seek": 540982, "start": 5424.179999999999, "end": 5428.74, "text": " And using that can be a nice way, I mean,", "tokens": [51082, 400, 1228, 300, 393, 312, 257, 1481, 636, 11, 286, 914, 11, 51310], "temperature": 0.0, "avg_logprob": -0.3403120214288885, "compression_ratio": 1.7735849056603774, "no_speech_prob": 1.1300799997115973e-05}, {"id": 1180, "seek": 540982, "start": 5428.74, "end": 5430.94, "text": " that'd probably go higher up to about six.", "tokens": [51310, 300, 1116, 1391, 352, 2946, 493, 281, 466, 2309, 13, 51420], "temperature": 0.0, "avg_logprob": -0.3403120214288885, "compression_ratio": 1.7735849056603774, "no_speech_prob": 1.1300799997115973e-05}, {"id": 1181, "seek": 540982, "start": 5430.94, "end": 5434.94, "text": " But that can be a nice way to avoid numbers getting too big.", "tokens": [51420, 583, 300, 393, 312, 257, 1481, 636, 281, 5042, 3547, 1242, 886, 955, 13, 51620], "temperature": 0.0, "avg_logprob": -0.3403120214288885, "compression_ratio": 1.7735849056603774, "no_speech_prob": 1.1300799997115973e-05}, {"id": 1182, "seek": 543494, "start": 5435.46, "end": 5438.46, "text": " And maybe if you really wanted to have fun,", "tokens": [50390, 400, 1310, 498, 291, 534, 1415, 281, 362, 1019, 11, 50540], "temperature": 0.0, "avg_logprob": -0.3505223710960317, "compression_ratio": 1.665289256198347, "no_speech_prob": 3.37372075591702e-05}, {"id": 1183, "seek": 543494, "start": 5438.46, "end": 5441.9, "text": " you could do kind of like a leaky maximum, which I haven't tried yet.", "tokens": [50540, 291, 727, 360, 733, 295, 411, 257, 476, 15681, 6674, 11, 597, 286, 2378, 380, 3031, 1939, 13, 50712], "temperature": 0.0, "avg_logprob": -0.3505223710960317, "compression_ratio": 1.665289256198347, "no_speech_prob": 3.37372075591702e-05}, {"id": 1184, "seek": 543494, "start": 5441.9, "end": 5446.7, "text": " Where maybe at the top it kind of goes like ten times smaller,", "tokens": [50712, 2305, 1310, 412, 264, 1192, 309, 733, 295, 1709, 411, 2064, 1413, 4356, 11, 50952], "temperature": 0.0, "avg_logprob": -0.3505223710960317, "compression_ratio": 1.665289256198347, "no_speech_prob": 3.37372075591702e-05}, {"id": 1185, "seek": 543494, "start": 5446.7, "end": 5448.98, "text": " kind of just exactly like the leaky could be.", "tokens": [50952, 733, 295, 445, 2293, 411, 264, 476, 15681, 727, 312, 13, 51066], "temperature": 0.0, "avg_logprob": -0.3505223710960317, "compression_ratio": 1.665289256198347, "no_speech_prob": 3.37372075591702e-05}, {"id": 1186, "seek": 543494, "start": 5450.9, "end": 5453.66, "text": " So anyway, if you do that, you'd need to make sure that", "tokens": [51162, 407, 4033, 11, 498, 291, 360, 300, 11, 291, 1116, 643, 281, 652, 988, 300, 51300], "temperature": 0.0, "avg_logprob": -0.3505223710960317, "compression_ratio": 1.665289256198347, "no_speech_prob": 3.37372075591702e-05}, {"id": 1187, "seek": 543494, "start": 5453.66, "end": 5461.419999999999, "text": " you're still getting zero, one layers with your initialization.", "tokens": [51300, 291, 434, 920, 1242, 4018, 11, 472, 7914, 365, 428, 5883, 2144, 13, 51688], "temperature": 0.0, "avg_logprob": -0.3505223710960317, "compression_ratio": 1.665289256198347, "no_speech_prob": 3.37372075591702e-05}, {"id": 1188, "seek": 543494, "start": 5461.419999999999, "end": 5464.7, "text": " But that would be something you could consider playing with.", "tokens": [51688, 583, 300, 576, 312, 746, 291, 727, 1949, 2433, 365, 13, 51852], "temperature": 0.0, "avg_logprob": -0.3505223710960317, "compression_ratio": 1.665289256198347, "no_speech_prob": 3.37372075591702e-05}, {"id": 1189, "seek": 546494, "start": 5464.94, "end": 5469.98, "text": " Okay, so", "tokens": [50364, 1033, 11, 370, 50616], "temperature": 0.0, "avg_logprob": -0.4708156938906069, "compression_ratio": 1.3453237410071943, "no_speech_prob": 6.577929525519721e-07}, {"id": 1190, "seek": 546494, "start": 5469.98, "end": 5479.98, "text": " Let's create our own little SGD class.", "tokens": [50616, 961, 311, 1884, 527, 1065, 707, 34520, 35, 1508, 13, 51116], "temperature": 0.0, "avg_logprob": -0.4708156938906069, "compression_ratio": 1.3453237410071943, "no_speech_prob": 6.577929525519721e-07}, {"id": 1191, "seek": 546494, "start": 5479.98, "end": 5486.66, "text": " So an SGD class is going to need to know what parameters to optimize.", "tokens": [51116, 407, 364, 34520, 35, 1508, 307, 516, 281, 643, 281, 458, 437, 9834, 281, 19719, 13, 51450], "temperature": 0.0, "avg_logprob": -0.4708156938906069, "compression_ratio": 1.3453237410071943, "no_speech_prob": 6.577929525519721e-07}, {"id": 1192, "seek": 546494, "start": 5486.66, "end": 5491.74, "text": " And if you remember the module.parameters method returns a generator.", "tokens": [51450, 400, 498, 291, 1604, 264, 10088, 13, 2181, 335, 6202, 3170, 11247, 257, 19265, 13, 51704], "temperature": 0.0, "avg_logprob": -0.4708156938906069, "compression_ratio": 1.3453237410071943, "no_speech_prob": 6.577929525519721e-07}, {"id": 1193, "seek": 549174, "start": 5491.78, "end": 5496.66, "text": " So we use a list to turn, we want to turn that into a list so", "tokens": [50366, 407, 321, 764, 257, 1329, 281, 1261, 11, 321, 528, 281, 1261, 300, 666, 257, 1329, 370, 50610], "temperature": 0.0, "avg_logprob": -0.29533053588867186, "compression_ratio": 1.7370517928286853, "no_speech_prob": 2.02614182853722e-06}, {"id": 1194, "seek": 549174, "start": 5496.66, "end": 5500.42, "text": " it's kind of forced to be a particular, not something that's going to change.", "tokens": [50610, 309, 311, 733, 295, 7579, 281, 312, 257, 1729, 11, 406, 746, 300, 311, 516, 281, 1319, 13, 50798], "temperature": 0.0, "avg_logprob": -0.29533053588867186, "compression_ratio": 1.7370517928286853, "no_speech_prob": 2.02614182853722e-06}, {"id": 1195, "seek": 549174, "start": 5502.099999999999, "end": 5504.179999999999, "text": " We're going to need to know the learning rate.", "tokens": [50882, 492, 434, 516, 281, 643, 281, 458, 264, 2539, 3314, 13, 50986], "temperature": 0.0, "avg_logprob": -0.29533053588867186, "compression_ratio": 1.7370517928286853, "no_speech_prob": 2.02614182853722e-06}, {"id": 1196, "seek": 549174, "start": 5504.179999999999, "end": 5506.219999999999, "text": " We're going to need to know the weight decay,", "tokens": [50986, 492, 434, 516, 281, 643, 281, 458, 264, 3364, 21039, 11, 51088], "temperature": 0.0, "avg_logprob": -0.29533053588867186, "compression_ratio": 1.7370517928286853, "no_speech_prob": 2.02614182853722e-06}, {"id": 1197, "seek": 549174, "start": 5506.219999999999, "end": 5507.7, "text": " which we'll look at a bit in a moment.", "tokens": [51088, 597, 321, 603, 574, 412, 257, 857, 294, 257, 1623, 13, 51162], "temperature": 0.0, "avg_logprob": -0.29533053588867186, "compression_ratio": 1.7370517928286853, "no_speech_prob": 2.02614182853722e-06}, {"id": 1198, "seek": 549174, "start": 5509.38, "end": 5511.34, "text": " And for reasons we'll discuss later,", "tokens": [51246, 400, 337, 4112, 321, 603, 2248, 1780, 11, 51344], "temperature": 0.0, "avg_logprob": -0.29533053588867186, "compression_ratio": 1.7370517928286853, "no_speech_prob": 2.02614182853722e-06}, {"id": 1199, "seek": 549174, "start": 5511.34, "end": 5514.66, "text": " we also want to keep track of what batch number are we up to.", "tokens": [51344, 321, 611, 528, 281, 1066, 2837, 295, 437, 15245, 1230, 366, 321, 493, 281, 13, 51510], "temperature": 0.0, "avg_logprob": -0.29533053588867186, "compression_ratio": 1.7370517928286853, "no_speech_prob": 2.02614182853722e-06}, {"id": 1200, "seek": 549174, "start": 5516.0199999999995, "end": 5519.9, "text": " So an optimizer basically has two things, a step and a zero grid.", "tokens": [51578, 407, 364, 5028, 6545, 1936, 575, 732, 721, 11, 257, 1823, 293, 257, 4018, 10748, 13, 51772], "temperature": 0.0, "avg_logprob": -0.29533053588867186, "compression_ratio": 1.7370517928286853, "no_speech_prob": 2.02614182853722e-06}, {"id": 1201, "seek": 551990, "start": 5520.9, "end": 5525.42, "text": " So what step's going to do is, obviously with no grad,", "tokens": [50414, 407, 437, 1823, 311, 516, 281, 360, 307, 11, 2745, 365, 572, 2771, 11, 50640], "temperature": 0.0, "avg_logprob": -0.26724489111649363, "compression_ratio": 1.7822222222222222, "no_speech_prob": 1.5779618252054206e-06}, {"id": 1202, "seek": 551990, "start": 5525.42, "end": 5529.0599999999995, "text": " because this is not part of the thing that we're optimizing,", "tokens": [50640, 570, 341, 307, 406, 644, 295, 264, 551, 300, 321, 434, 40425, 11, 50822], "temperature": 0.0, "avg_logprob": -0.26724489111649363, "compression_ratio": 1.7822222222222222, "no_speech_prob": 1.5779618252054206e-06}, {"id": 1203, "seek": 551990, "start": 5529.0599999999995, "end": 5530.9, "text": " this is the optimization itself.", "tokens": [50822, 341, 307, 264, 19618, 2564, 13, 50914], "temperature": 0.0, "avg_logprob": -0.26724489111649363, "compression_ratio": 1.7822222222222222, "no_speech_prob": 1.5779618252054206e-06}, {"id": 1204, "seek": 551990, "start": 5530.9, "end": 5535.94, "text": " We go through each tensor of parameters and we do a step of the optimizer.", "tokens": [50914, 492, 352, 807, 1184, 40863, 295, 9834, 293, 321, 360, 257, 1823, 295, 264, 5028, 6545, 13, 51166], "temperature": 0.0, "avg_logprob": -0.26724489111649363, "compression_ratio": 1.7822222222222222, "no_speech_prob": 1.5779618252054206e-06}, {"id": 1205, "seek": 551990, "start": 5535.94, "end": 5539.58, "text": " And we'll come back to this in a moment, we do a step of the regularizer.", "tokens": [51166, 400, 321, 603, 808, 646, 281, 341, 294, 257, 1623, 11, 321, 360, 257, 1823, 295, 264, 3890, 6545, 13, 51348], "temperature": 0.0, "avg_logprob": -0.26724489111649363, "compression_ratio": 1.7822222222222222, "no_speech_prob": 1.5779618252054206e-06}, {"id": 1206, "seek": 551990, "start": 5539.58, "end": 5541.339999999999, "text": " And we keep track of what batch number we're up to.", "tokens": [51348, 400, 321, 1066, 2837, 295, 437, 15245, 1230, 321, 434, 493, 281, 13, 51436], "temperature": 0.0, "avg_logprob": -0.26724489111649363, "compression_ratio": 1.7822222222222222, "no_speech_prob": 1.5779618252054206e-06}, {"id": 1207, "seek": 551990, "start": 5542.58, "end": 5546.379999999999, "text": " And so what does SGD do in a step of the optimizer?", "tokens": [51498, 400, 370, 437, 775, 34520, 35, 360, 294, 257, 1823, 295, 264, 5028, 6545, 30, 51688], "temperature": 0.0, "avg_logprob": -0.26724489111649363, "compression_ratio": 1.7822222222222222, "no_speech_prob": 1.5779618252054206e-06}, {"id": 1208, "seek": 554638, "start": 5546.38, "end": 5553.14, "text": " It subtracts out from the parameter its gradient times the learning rate.", "tokens": [50364, 467, 16390, 82, 484, 490, 264, 13075, 1080, 16235, 1413, 264, 2539, 3314, 13, 50702], "temperature": 0.0, "avg_logprob": -0.38669528961181643, "compression_ratio": 1.4782608695652173, "no_speech_prob": 2.8130170903750695e-06}, {"id": 1209, "seek": 554638, "start": 5553.14, "end": 5556.22, "text": " So that's an SGD optimization step.", "tokens": [50702, 407, 300, 311, 364, 34520, 35, 19618, 1823, 13, 50856], "temperature": 0.0, "avg_logprob": -0.38669528961181643, "compression_ratio": 1.4782608695652173, "no_speech_prob": 2.8130170903750695e-06}, {"id": 1210, "seek": 554638, "start": 5556.22, "end": 5562.26, "text": " And to zero the gradients, we go through each parameter and we zero it.", "tokens": [50856, 400, 281, 4018, 264, 2771, 2448, 11, 321, 352, 807, 1184, 13075, 293, 321, 4018, 309, 13, 51158], "temperature": 0.0, "avg_logprob": -0.38669528961181643, "compression_ratio": 1.4782608695652173, "no_speech_prob": 2.8130170903750695e-06}, {"id": 1211, "seek": 554638, "start": 5567.54, "end": 5570.46, "text": " And that's in torch.no grad, so.", "tokens": [51422, 400, 300, 311, 294, 27822, 13, 1771, 2771, 11, 370, 13, 51568], "temperature": 0.0, "avg_logprob": -0.38669528961181643, "compression_ratio": 1.4782608695652173, "no_speech_prob": 2.8130170903750695e-06}, {"id": 1212, "seek": 554638, "start": 5573.54, "end": 5574.9800000000005, "text": " I guess it's not, okay.", "tokens": [51722, 286, 2041, 309, 311, 406, 11, 1392, 13, 51794], "temperature": 0.0, "avg_logprob": -0.38669528961181643, "compression_ratio": 1.4782608695652173, "no_speech_prob": 2.8130170903750695e-06}, {"id": 1213, "seek": 557638, "start": 5577.38, "end": 5579.66, "text": " So use .data that way.", "tokens": [50414, 407, 764, 2411, 67, 3274, 300, 636, 13, 50528], "temperature": 0.0, "avg_logprob": -0.33818650679154827, "compression_ratio": 1.6120689655172413, "no_speech_prob": 2.5612819172238233e-06}, {"id": 1214, "seek": 557638, "start": 5579.66, "end": 5583.02, "text": " If you use .data, then you don't need to say the no grad.", "tokens": [50528, 759, 291, 764, 2411, 67, 3274, 11, 550, 291, 500, 380, 643, 281, 584, 264, 572, 2771, 13, 50696], "temperature": 0.0, "avg_logprob": -0.33818650679154827, "compression_ratio": 1.6120689655172413, "no_speech_prob": 2.5612819172238233e-06}, {"id": 1215, "seek": 557638, "start": 5583.02, "end": 5586.34, "text": " It's just a little typing saver.", "tokens": [50696, 467, 311, 445, 257, 707, 18444, 601, 331, 13, 50862], "temperature": 0.0, "avg_logprob": -0.33818650679154827, "compression_ratio": 1.6120689655172413, "no_speech_prob": 2.5612819172238233e-06}, {"id": 1216, "seek": 557638, "start": 5588.900000000001, "end": 5592.62, "text": " Okay, so let's create a train learner.", "tokens": [50990, 1033, 11, 370, 718, 311, 1884, 257, 3847, 33347, 13, 51176], "temperature": 0.0, "avg_logprob": -0.33818650679154827, "compression_ratio": 1.6120689655172413, "no_speech_prob": 2.5612819172238233e-06}, {"id": 1217, "seek": 557638, "start": 5592.62, "end": 5595.66, "text": " So it's a learner with a training callback kind of built in.", "tokens": [51176, 407, 309, 311, 257, 33347, 365, 257, 3097, 818, 3207, 733, 295, 3094, 294, 13, 51328], "temperature": 0.0, "avg_logprob": -0.33818650679154827, "compression_ratio": 1.6120689655172413, "no_speech_prob": 2.5612819172238233e-06}, {"id": 1218, "seek": 557638, "start": 5595.66, "end": 5600.02, "text": " And we're gonna set the optimization function to be this SGD we just wrote.", "tokens": [51328, 400, 321, 434, 799, 992, 264, 19618, 2445, 281, 312, 341, 34520, 35, 321, 445, 4114, 13, 51546], "temperature": 0.0, "avg_logprob": -0.33818650679154827, "compression_ratio": 1.6120689655172413, "no_speech_prob": 2.5612819172238233e-06}, {"id": 1219, "seek": 557638, "start": 5600.02, "end": 5604.9400000000005, "text": " And we'll use the batch norm model with the weight initialization we've used before.", "tokens": [51546, 400, 321, 603, 764, 264, 15245, 2026, 2316, 365, 264, 3364, 5883, 2144, 321, 600, 1143, 949, 13, 51792], "temperature": 0.0, "avg_logprob": -0.33818650679154827, "compression_ratio": 1.6120689655172413, "no_speech_prob": 2.5612819172238233e-06}, {"id": 1220, "seek": 560638, "start": 5606.74, "end": 5607.74, "text": " And if we train it,", "tokens": [50382, 400, 498, 321, 3847, 309, 11, 50432], "temperature": 0.0, "avg_logprob": -0.33006778516267476, "compression_ratio": 1.4824120603015076, "no_speech_prob": 1.8162219248551992e-06}, {"id": 1221, "seek": 560638, "start": 5607.74, "end": 5612.26, "text": " then this should give us basically the same results we've had before.", "tokens": [50432, 550, 341, 820, 976, 505, 1936, 264, 912, 3542, 321, 600, 632, 949, 13, 50658], "temperature": 0.0, "avg_logprob": -0.33006778516267476, "compression_ratio": 1.4824120603015076, "no_speech_prob": 1.8162219248551992e-06}, {"id": 1222, "seek": 560638, "start": 5612.26, "end": 5616.5, "text": " While this is training, I'm gonna talk about regularization.", "tokens": [50658, 3987, 341, 307, 3097, 11, 286, 478, 799, 751, 466, 3890, 2144, 13, 50870], "temperature": 0.0, "avg_logprob": -0.33006778516267476, "compression_ratio": 1.4824120603015076, "no_speech_prob": 1.8162219248551992e-06}, {"id": 1223, "seek": 560638, "start": 5618.38, "end": 5623.38, "text": " Hopefully you remember from part one of this course or", "tokens": [50964, 10429, 291, 1604, 490, 644, 472, 295, 341, 1164, 420, 51214], "temperature": 0.0, "avg_logprob": -0.33006778516267476, "compression_ratio": 1.4824120603015076, "no_speech_prob": 1.8162219248551992e-06}, {"id": 1224, "seek": 560638, "start": 5623.38, "end": 5628.62, "text": " from your other learning what weight decay is.", "tokens": [51214, 490, 428, 661, 2539, 437, 3364, 21039, 307, 13, 51476], "temperature": 0.0, "avg_logprob": -0.33006778516267476, "compression_ratio": 1.4824120603015076, "no_speech_prob": 1.8162219248551992e-06}, {"id": 1225, "seek": 560638, "start": 5628.62, "end": 5635.58, "text": " And so just to remind you, weight decay or", "tokens": [51476, 400, 370, 445, 281, 4160, 291, 11, 3364, 21039, 420, 51824], "temperature": 0.0, "avg_logprob": -0.33006778516267476, "compression_ratio": 1.4824120603015076, "no_speech_prob": 1.8162219248551992e-06}, {"id": 1226, "seek": 563558, "start": 5635.58, "end": 5640.7, "text": " L2 regularization are kind of the same thing.", "tokens": [50364, 441, 17, 3890, 2144, 366, 733, 295, 264, 912, 551, 13, 50620], "temperature": 0.0, "avg_logprob": -0.3217385279667842, "compression_ratio": 1.7806451612903227, "no_speech_prob": 1.2482707916205982e-06}, {"id": 1227, "seek": 563558, "start": 5640.7, "end": 5644.38, "text": " And basically what we're doing is we're saying,", "tokens": [50620, 400, 1936, 437, 321, 434, 884, 307, 321, 434, 1566, 11, 50804], "temperature": 0.0, "avg_logprob": -0.3217385279667842, "compression_ratio": 1.7806451612903227, "no_speech_prob": 1.2482707916205982e-06}, {"id": 1228, "seek": 563558, "start": 5645.54, "end": 5651.0599999999995, "text": " let's add the square of the weights to the loss function.", "tokens": [50862, 718, 311, 909, 264, 3732, 295, 264, 17443, 281, 264, 4470, 2445, 13, 51138], "temperature": 0.0, "avg_logprob": -0.3217385279667842, "compression_ratio": 1.7806451612903227, "no_speech_prob": 1.2482707916205982e-06}, {"id": 1229, "seek": 563558, "start": 5652.58, "end": 5658.5, "text": " Now, if we add the square of the weights to the loss function, so", "tokens": [51214, 823, 11, 498, 321, 909, 264, 3732, 295, 264, 17443, 281, 264, 4470, 2445, 11, 370, 51510], "temperature": 0.0, "avg_logprob": -0.3217385279667842, "compression_ratio": 1.7806451612903227, "no_speech_prob": 1.2482707916205982e-06}, {"id": 1230, "seek": 563558, "start": 5658.5, "end": 5665.54, "text": " whatever our loss function is, so we'll just call it loss.", "tokens": [51510, 2035, 527, 4470, 2445, 307, 11, 370, 321, 603, 445, 818, 309, 4470, 13, 51862], "temperature": 0.0, "avg_logprob": -0.3217385279667842, "compression_ratio": 1.7806451612903227, "no_speech_prob": 1.2482707916205982e-06}, {"id": 1231, "seek": 566554, "start": 5666.5, "end": 5673.14, "text": " We're adding plus the sum of the square of the weights.", "tokens": [50412, 492, 434, 5127, 1804, 264, 2408, 295, 264, 3732, 295, 264, 17443, 13, 50744], "temperature": 0.0, "avg_logprob": -0.5557464054652623, "compression_ratio": 1.6689189189189189, "no_speech_prob": 1.6797313264760305e-06}, {"id": 1232, "seek": 566554, "start": 5673.14, "end": 5675.06, "text": " So that's our L.", "tokens": [50744, 407, 300, 311, 527, 441, 13, 50840], "temperature": 0.0, "avg_logprob": -0.5557464054652623, "compression_ratio": 1.6689189189189189, "no_speech_prob": 1.6797313264760305e-06}, {"id": 1233, "seek": 566554, "start": 5675.06, "end": 5680.1, "text": " And so the only thing we actually care about is the derivative of that.", "tokens": [50840, 400, 370, 264, 787, 551, 321, 767, 1127, 466, 307, 264, 13760, 295, 300, 13, 51092], "temperature": 0.0, "avg_logprob": -0.5557464054652623, "compression_ratio": 1.6689189189189189, "no_speech_prob": 1.6797313264760305e-06}, {"id": 1234, "seek": 566554, "start": 5680.1, "end": 5683.86, "text": " And the derivative of that", "tokens": [51092, 400, 264, 13760, 295, 300, 51280], "temperature": 0.0, "avg_logprob": -0.5557464054652623, "compression_ratio": 1.6689189189189189, "no_speech_prob": 1.6797313264760305e-06}, {"id": 1235, "seek": 566554, "start": 5683.86, "end": 5689.1, "text": " is equal to the derivative of,", "tokens": [51280, 307, 2681, 281, 264, 13760, 295, 11, 51542], "temperature": 0.0, "avg_logprob": -0.5557464054652623, "compression_ratio": 1.6689189189189189, "no_speech_prob": 1.6797313264760305e-06}, {"id": 1236, "seek": 566554, "start": 5689.1, "end": 5694.9, "text": " let's try to write that a little bit better.", "tokens": [51542, 718, 311, 853, 281, 2464, 300, 257, 707, 857, 1101, 13, 51832], "temperature": 0.0, "avg_logprob": -0.5557464054652623, "compression_ratio": 1.6689189189189189, "no_speech_prob": 1.6797313264760305e-06}, {"id": 1237, "seek": 569554, "start": 5696.38, "end": 5701.9, "text": " Is the derivative of the loss plus", "tokens": [50406, 1119, 264, 13760, 295, 264, 4470, 1804, 50682], "temperature": 0.0, "avg_logprob": -0.36791861547182686, "compression_ratio": 1.7073170731707317, "no_speech_prob": 3.9897170722724695e-07}, {"id": 1238, "seek": 569554, "start": 5701.9, "end": 5709.98, "text": " the derivative of this, which is just the sum of 2w.", "tokens": [50682, 264, 13760, 295, 341, 11, 597, 307, 445, 264, 2408, 295, 568, 86, 13, 51086], "temperature": 0.0, "avg_logprob": -0.36791861547182686, "compression_ratio": 1.7073170731707317, "no_speech_prob": 3.9897170722724695e-07}, {"id": 1239, "seek": 569554, "start": 5711.62, "end": 5717.1, "text": " And then what we do is we multiply this bit here by some constant,", "tokens": [51168, 400, 550, 437, 321, 360, 307, 321, 12972, 341, 857, 510, 538, 512, 5754, 11, 51442], "temperature": 0.0, "avg_logprob": -0.36791861547182686, "compression_ratio": 1.7073170731707317, "no_speech_prob": 3.9897170722724695e-07}, {"id": 1240, "seek": 569554, "start": 5717.1, "end": 5719.58, "text": " which is the weight decay, so we call that weight decay.", "tokens": [51442, 597, 307, 264, 3364, 21039, 11, 370, 321, 818, 300, 3364, 21039, 13, 51566], "temperature": 0.0, "avg_logprob": -0.36791861547182686, "compression_ratio": 1.7073170731707317, "no_speech_prob": 3.9897170722724695e-07}, {"id": 1241, "seek": 569554, "start": 5719.58, "end": 5723.46, "text": " And so since the weight decay could directly incorporate the number,", "tokens": [51566, 400, 370, 1670, 264, 3364, 21039, 727, 3838, 16091, 264, 1230, 11, 51760], "temperature": 0.0, "avg_logprob": -0.36791861547182686, "compression_ratio": 1.7073170731707317, "no_speech_prob": 3.9897170722724695e-07}, {"id": 1242, "seek": 572346, "start": 5723.46, "end": 5727.74, "text": " the two, we can actually just delete that entirely and just.", "tokens": [50364, 264, 732, 11, 321, 393, 767, 445, 12097, 300, 7696, 293, 445, 13, 50578], "temperature": 0.0, "avg_logprob": -0.45255933489118305, "compression_ratio": 1.497175141242938, "no_speech_prob": 0.00012533697008620948}, {"id": 1243, "seek": 572346, "start": 5735.74, "end": 5738.14, "text": " Time, weight decay, do that.", "tokens": [50978, 6161, 11, 3364, 21039, 11, 360, 300, 13, 51098], "temperature": 0.0, "avg_logprob": -0.45255933489118305, "compression_ratio": 1.497175141242938, "no_speech_prob": 0.00012533697008620948}, {"id": 1244, "seek": 572346, "start": 5738.14, "end": 5742.7, "text": " I'm doing this very quickly because we have already covered it in part one, so", "tokens": [51098, 286, 478, 884, 341, 588, 2661, 570, 321, 362, 1217, 5343, 309, 294, 644, 472, 11, 370, 51326], "temperature": 0.0, "avg_logprob": -0.45255933489118305, "compression_ratio": 1.497175141242938, "no_speech_prob": 0.00012533697008620948}, {"id": 1245, "seek": 572346, "start": 5742.7, "end": 5744.78, "text": " this is hopefully something that you've all seen before.", "tokens": [51326, 341, 307, 4696, 746, 300, 291, 600, 439, 1612, 949, 13, 51430], "temperature": 0.0, "avg_logprob": -0.45255933489118305, "compression_ratio": 1.497175141242938, "no_speech_prob": 0.00012533697008620948}, {"id": 1246, "seek": 572346, "start": 5746.62, "end": 5753.38, "text": " So we can do weight decay by taking our", "tokens": [51522, 407, 321, 393, 360, 3364, 21039, 538, 1940, 527, 51860], "temperature": 0.0, "avg_logprob": -0.45255933489118305, "compression_ratio": 1.497175141242938, "no_speech_prob": 0.00012533697008620948}, {"id": 1247, "seek": 575338, "start": 5754.26, "end": 5760.9800000000005, "text": " gradients and adding on the weight decay times the weights.", "tokens": [50408, 2771, 2448, 293, 5127, 322, 264, 3364, 21039, 1413, 264, 17443, 13, 50744], "temperature": 0.0, "avg_logprob": -0.5056702912743412, "compression_ratio": 1.394904458598726, "no_speech_prob": 1.1843128959299065e-05}, {"id": 1248, "seek": 575338, "start": 5765.66, "end": 5770.900000000001, "text": " And so as a result, then in SGD, because that's part of the gradient.", "tokens": [50978, 400, 370, 382, 257, 1874, 11, 550, 294, 34520, 35, 11, 570, 300, 311, 644, 295, 264, 16235, 13, 51240], "temperature": 0.0, "avg_logprob": -0.5056702912743412, "compression_ratio": 1.394904458598726, "no_speech_prob": 1.1843128959299065e-05}, {"id": 1249, "seek": 575338, "start": 5773.58, "end": 5774.78, "text": " Man, I got it the wrong way around.", "tokens": [51374, 2458, 11, 286, 658, 309, 264, 2085, 636, 926, 13, 51434], "temperature": 0.0, "avg_logprob": -0.5056702912743412, "compression_ratio": 1.394904458598726, "no_speech_prob": 1.1843128959299065e-05}, {"id": 1250, "seek": 575338, "start": 5776.5, "end": 5779.38, "text": " Need to do that first.", "tokens": [51520, 16984, 281, 360, 300, 700, 13, 51664], "temperature": 0.0, "avg_logprob": -0.5056702912743412, "compression_ratio": 1.394904458598726, "no_speech_prob": 1.1843128959299065e-05}, {"id": 1251, "seek": 575338, "start": 5779.38, "end": 5782.38, "text": " I guess, well, whatever, okay.", "tokens": [51664, 286, 2041, 11, 731, 11, 2035, 11, 1392, 13, 51814], "temperature": 0.0, "avg_logprob": -0.5056702912743412, "compression_ratio": 1.394904458598726, "no_speech_prob": 1.1843128959299065e-05}, {"id": 1252, "seek": 578338, "start": 5783.38, "end": 5788.66, "text": " So since that's part of the gradient,", "tokens": [50364, 407, 1670, 300, 311, 644, 295, 264, 16235, 11, 50628], "temperature": 0.0, "avg_logprob": -0.3857534170150757, "compression_ratio": 1.5966850828729282, "no_speech_prob": 6.375547627612832e-07}, {"id": 1253, "seek": 578338, "start": 5788.66, "end": 5794.18, "text": " then in the optimization step, that's using the gradient.", "tokens": [50628, 550, 294, 264, 19618, 1823, 11, 300, 311, 1228, 264, 16235, 13, 50904], "temperature": 0.0, "avg_logprob": -0.3857534170150757, "compression_ratio": 1.5966850828729282, "no_speech_prob": 6.375547627612832e-07}, {"id": 1254, "seek": 578338, "start": 5794.18, "end": 5796.9400000000005, "text": " And it's subtracting out gradient times learning rate.", "tokens": [50904, 400, 309, 311, 16390, 278, 484, 16235, 1413, 2539, 3314, 13, 51042], "temperature": 0.0, "avg_logprob": -0.3857534170150757, "compression_ratio": 1.5966850828729282, "no_speech_prob": 6.375547627612832e-07}, {"id": 1255, "seek": 578338, "start": 5799.9800000000005, "end": 5806.34, "text": " But what you could do is because we're just ending up doing p.grad times self.lr,", "tokens": [51194, 583, 437, 291, 727, 360, 307, 570, 321, 434, 445, 8121, 493, 884, 280, 13, 7165, 1413, 2698, 13, 40987, 11, 51512], "temperature": 0.0, "avg_logprob": -0.3857534170150757, "compression_ratio": 1.5966850828729282, "no_speech_prob": 6.375547627612832e-07}, {"id": 1256, "seek": 578338, "start": 5806.34, "end": 5811.3, "text": " and the p.grad update is just to add in wt times weight.", "tokens": [51512, 293, 264, 280, 13, 7165, 5623, 307, 445, 281, 909, 294, 23105, 1413, 3364, 13, 51760], "temperature": 0.0, "avg_logprob": -0.3857534170150757, "compression_ratio": 1.5966850828729282, "no_speech_prob": 6.375547627612832e-07}, {"id": 1257, "seek": 581130, "start": 5811.3, "end": 5815.22, "text": " We could simply skip updating the gradients and", "tokens": [50364, 492, 727, 2935, 10023, 25113, 264, 2771, 2448, 293, 50560], "temperature": 0.0, "avg_logprob": -0.3079764519208743, "compression_ratio": 1.5728155339805825, "no_speech_prob": 2.496701370091614e-07}, {"id": 1258, "seek": 581130, "start": 5815.22, "end": 5818.9800000000005, "text": " instead directly update the weights to subtract out", "tokens": [50560, 2602, 3838, 5623, 264, 17443, 281, 16390, 484, 50748], "temperature": 0.0, "avg_logprob": -0.3079764519208743, "compression_ratio": 1.5728155339805825, "no_speech_prob": 2.496701370091614e-07}, {"id": 1259, "seek": 581130, "start": 5818.9800000000005, "end": 5822.62, "text": " the learning rate times the wd times weight.", "tokens": [50748, 264, 2539, 3314, 1413, 264, 261, 67, 1413, 3364, 13, 50930], "temperature": 0.0, "avg_logprob": -0.3079764519208743, "compression_ratio": 1.5728155339805825, "no_speech_prob": 2.496701370091614e-07}, {"id": 1260, "seek": 581130, "start": 5822.62, "end": 5825.5, "text": " So they would be mathematically identical.", "tokens": [50930, 407, 436, 576, 312, 44003, 14800, 13, 51074], "temperature": 0.0, "avg_logprob": -0.3079764519208743, "compression_ratio": 1.5728155339805825, "no_speech_prob": 2.496701370091614e-07}, {"id": 1261, "seek": 581130, "start": 5825.5, "end": 5827.22, "text": " And that is what we've done here.", "tokens": [51074, 400, 300, 307, 437, 321, 600, 1096, 510, 13, 51160], "temperature": 0.0, "avg_logprob": -0.3079764519208743, "compression_ratio": 1.5728155339805825, "no_speech_prob": 2.496701370091614e-07}, {"id": 1262, "seek": 581130, "start": 5827.22, "end": 5831.34, "text": " In the regularization step, we basically say if you've got weight decay,", "tokens": [51160, 682, 264, 3890, 2144, 1823, 11, 321, 1936, 584, 498, 291, 600, 658, 3364, 21039, 11, 51366], "temperature": 0.0, "avg_logprob": -0.3079764519208743, "compression_ratio": 1.5728155339805825, "no_speech_prob": 2.496701370091614e-07}, {"id": 1263, "seek": 581130, "start": 5832.900000000001, "end": 5837.5, "text": " then just take p times equals", "tokens": [51444, 550, 445, 747, 280, 1413, 6915, 51674], "temperature": 0.0, "avg_logprob": -0.3079764519208743, "compression_ratio": 1.5728155339805825, "no_speech_prob": 2.496701370091614e-07}, {"id": 1264, "seek": 583750, "start": 5838.5, "end": 5842.46, "text": " 1 minus the learning rate times the weight decay,", "tokens": [50414, 502, 3175, 264, 2539, 3314, 1413, 264, 3364, 21039, 11, 50612], "temperature": 0.0, "avg_logprob": -0.38696851485814804, "compression_ratio": 1.4615384615384615, "no_speech_prob": 2.642585513967788e-06}, {"id": 1265, "seek": 583750, "start": 5842.46, "end": 5847.66, "text": " which is mathematically the same as this, because we've got weight on both sides.", "tokens": [50612, 597, 307, 44003, 264, 912, 382, 341, 11, 570, 321, 600, 658, 3364, 322, 1293, 4881, 13, 50872], "temperature": 0.0, "avg_logprob": -0.38696851485814804, "compression_ratio": 1.4615384615384615, "no_speech_prob": 2.642585513967788e-06}, {"id": 1266, "seek": 583750, "start": 5851.18, "end": 5855.3, "text": " So that's why the regularization is here inside our SGD.", "tokens": [51048, 407, 300, 311, 983, 264, 3890, 2144, 307, 510, 1854, 527, 34520, 35, 13, 51254], "temperature": 0.0, "avg_logprob": -0.38696851485814804, "compression_ratio": 1.4615384615384615, "no_speech_prob": 2.642585513967788e-06}, {"id": 1267, "seek": 583750, "start": 5857.62, "end": 5861.5, "text": " And yeah, so it's finished running, that's good, we've got an 85% accuracy.", "tokens": [51370, 400, 1338, 11, 370, 309, 311, 4335, 2614, 11, 300, 311, 665, 11, 321, 600, 658, 364, 14695, 4, 14170, 13, 51564], "temperature": 0.0, "avg_logprob": -0.38696851485814804, "compression_ratio": 1.4615384615384615, "no_speech_prob": 2.642585513967788e-06}, {"id": 1268, "seek": 583750, "start": 5861.5, "end": 5862.42, "text": " That all looks fine.", "tokens": [51564, 663, 439, 1542, 2489, 13, 51610], "temperature": 0.0, "avg_logprob": -0.38696851485814804, "compression_ratio": 1.4615384615384615, "no_speech_prob": 2.642585513967788e-06}, {"id": 1269, "seek": 586750, "start": 5868.34, "end": 5871.9, "text": " And we're able to train at a high learning rate of 0.4, so that's pretty cool.", "tokens": [50406, 400, 321, 434, 1075, 281, 3847, 412, 257, 1090, 2539, 3314, 295, 1958, 13, 19, 11, 370, 300, 311, 1238, 1627, 13, 50584], "temperature": 0.0, "avg_logprob": -0.3121718044938712, "compression_ratio": 1.5991902834008098, "no_speech_prob": 2.2732722015916806e-07}, {"id": 1270, "seek": 586750, "start": 5873.5, "end": 5875.86, "text": " So now let's add momentum.", "tokens": [50664, 407, 586, 718, 311, 909, 11244, 13, 50782], "temperature": 0.0, "avg_logprob": -0.3121718044938712, "compression_ratio": 1.5991902834008098, "no_speech_prob": 2.2732722015916806e-07}, {"id": 1271, "seek": 586750, "start": 5875.86, "end": 5878.66, "text": " Now we had a kind of a hacky momentum learner before, but", "tokens": [50782, 823, 321, 632, 257, 733, 295, 257, 10339, 88, 11244, 33347, 949, 11, 457, 50922], "temperature": 0.0, "avg_logprob": -0.3121718044938712, "compression_ratio": 1.5991902834008098, "no_speech_prob": 2.2732722015916806e-07}, {"id": 1272, "seek": 586750, "start": 5878.66, "end": 5881.38, "text": " we've actually, momentum should be in an optimizer really.", "tokens": [50922, 321, 600, 767, 11, 11244, 820, 312, 294, 364, 5028, 6545, 534, 13, 51058], "temperature": 0.0, "avg_logprob": -0.3121718044938712, "compression_ratio": 1.5991902834008098, "no_speech_prob": 2.2732722015916806e-07}, {"id": 1273, "seek": 586750, "start": 5881.38, "end": 5885.86, "text": " And so let's talk a bit about what momentum actually is.", "tokens": [51058, 400, 370, 718, 311, 751, 257, 857, 466, 437, 11244, 767, 307, 13, 51282], "temperature": 0.0, "avg_logprob": -0.3121718044938712, "compression_ratio": 1.5991902834008098, "no_speech_prob": 2.2732722015916806e-07}, {"id": 1274, "seek": 586750, "start": 5886.94, "end": 5890.18, "text": " So let's just create some data.", "tokens": [51336, 407, 718, 311, 445, 1884, 512, 1412, 13, 51498], "temperature": 0.0, "avg_logprob": -0.3121718044938712, "compression_ratio": 1.5991902834008098, "no_speech_prob": 2.2732722015916806e-07}, {"id": 1275, "seek": 586750, "start": 5890.18, "end": 5895.42, "text": " So our x's are just gonna be equally spaced numbers from minus 4 to 4,", "tokens": [51498, 407, 527, 2031, 311, 366, 445, 799, 312, 12309, 43766, 3547, 490, 3175, 1017, 281, 1017, 11, 51760], "temperature": 0.0, "avg_logprob": -0.3121718044938712, "compression_ratio": 1.5991902834008098, "no_speech_prob": 2.2732722015916806e-07}, {"id": 1276, "seek": 586750, "start": 5895.42, "end": 5897.02, "text": " 100 of them.", "tokens": [51760, 2319, 295, 552, 13, 51840], "temperature": 0.0, "avg_logprob": -0.3121718044938712, "compression_ratio": 1.5991902834008098, "no_speech_prob": 2.2732722015916806e-07}, {"id": 1277, "seek": 589702, "start": 5897.02, "end": 5903.3, "text": " And our y's are just gonna be our x's divided by 3 squared,", "tokens": [50364, 400, 527, 288, 311, 366, 445, 799, 312, 527, 2031, 311, 6666, 538, 805, 8889, 11, 50678], "temperature": 0.0, "avg_logprob": -0.3990908728705512, "compression_ratio": 1.4929577464788732, "no_speech_prob": 1.7880641962619848e-06}, {"id": 1278, "seek": 589702, "start": 5903.3, "end": 5906.9800000000005, "text": " 1 minus that, plus some randomization.", "tokens": [50678, 502, 3175, 300, 11, 1804, 512, 4974, 2144, 13, 50862], "temperature": 0.0, "avg_logprob": -0.3990908728705512, "compression_ratio": 1.4929577464788732, "no_speech_prob": 1.7880641962619848e-06}, {"id": 1279, "seek": 589702, "start": 5908.14, "end": 5910.620000000001, "text": " And so these dots here is our random data.", "tokens": [50920, 400, 370, 613, 15026, 510, 307, 527, 4974, 1412, 13, 51044], "temperature": 0.0, "avg_logprob": -0.3990908728705512, "compression_ratio": 1.4929577464788732, "no_speech_prob": 1.7880641962619848e-06}, {"id": 1280, "seek": 589702, "start": 5912.14, "end": 5914.9400000000005, "text": " I'm gonna show you what momentum is by example.", "tokens": [51120, 286, 478, 799, 855, 291, 437, 11244, 307, 538, 1365, 13, 51260], "temperature": 0.0, "avg_logprob": -0.3990908728705512, "compression_ratio": 1.4929577464788732, "no_speech_prob": 1.7880641962619848e-06}, {"id": 1281, "seek": 589702, "start": 5916.22, "end": 5919.900000000001, "text": " And this is something that Sylvain Grugier helped build, so", "tokens": [51324, 400, 341, 307, 746, 300, 3902, 14574, 491, 2606, 697, 811, 4254, 1322, 11, 370, 51508], "temperature": 0.0, "avg_logprob": -0.3990908728705512, "compression_ratio": 1.4929577464788732, "no_speech_prob": 1.7880641962619848e-06}, {"id": 1282, "seek": 589702, "start": 5919.900000000001, "end": 5924.740000000001, "text": " thank you Sylvain for our book actually, if memory serves correctly.", "tokens": [51508, 1309, 291, 3902, 14574, 491, 337, 527, 1446, 767, 11, 498, 4675, 13451, 8944, 13, 51750], "temperature": 0.0, "avg_logprob": -0.3990908728705512, "compression_ratio": 1.4929577464788732, "no_speech_prob": 1.7880641962619848e-06}, {"id": 1283, "seek": 592474, "start": 5925.26, "end": 5929.139999999999, "text": " Actually, it might even be the course before that.", "tokens": [50390, 5135, 11, 309, 1062, 754, 312, 264, 1164, 949, 300, 13, 50584], "temperature": 0.0, "avg_logprob": -0.2624881493784215, "compression_ratio": 1.7924528301886793, "no_speech_prob": 9.422444691153942e-07}, {"id": 1284, "seek": 592474, "start": 5929.139999999999, "end": 5933.099999999999, "text": " What we're gonna do is we're gonna show you what momentum looks like for", "tokens": [50584, 708, 321, 434, 799, 360, 307, 321, 434, 799, 855, 291, 437, 11244, 1542, 411, 337, 50782], "temperature": 0.0, "avg_logprob": -0.2624881493784215, "compression_ratio": 1.7924528301886793, "no_speech_prob": 9.422444691153942e-07}, {"id": 1285, "seek": 592474, "start": 5933.099999999999, "end": 5934.7, "text": " a range of different levels of momentum.", "tokens": [50782, 257, 3613, 295, 819, 4358, 295, 11244, 13, 50862], "temperature": 0.0, "avg_logprob": -0.2624881493784215, "compression_ratio": 1.7924528301886793, "no_speech_prob": 9.422444691153942e-07}, {"id": 1286, "seek": 592474, "start": 5934.7, "end": 5936.42, "text": " These are the different levels we're gonna use.", "tokens": [50862, 1981, 366, 264, 819, 4358, 321, 434, 799, 764, 13, 50948], "temperature": 0.0, "avg_logprob": -0.2624881493784215, "compression_ratio": 1.7924528301886793, "no_speech_prob": 9.422444691153942e-07}, {"id": 1287, "seek": 592474, "start": 5937.86, "end": 5941.139999999999, "text": " So let's take a beta of 0.5, so that's gonna be our first one.", "tokens": [51020, 407, 718, 311, 747, 257, 9861, 295, 1958, 13, 20, 11, 370, 300, 311, 799, 312, 527, 700, 472, 13, 51184], "temperature": 0.0, "avg_logprob": -0.2624881493784215, "compression_ratio": 1.7924528301886793, "no_speech_prob": 9.422444691153942e-07}, {"id": 1288, "seek": 592474, "start": 5941.139999999999, "end": 5945.26, "text": " So we're gonna just do a scatter plot of our x's and y's, it's the blue dots.", "tokens": [51184, 407, 321, 434, 799, 445, 360, 257, 34951, 7542, 295, 527, 2031, 311, 293, 288, 311, 11, 309, 311, 264, 3344, 15026, 13, 51390], "temperature": 0.0, "avg_logprob": -0.2624881493784215, "compression_ratio": 1.7924528301886793, "no_speech_prob": 9.422444691153942e-07}, {"id": 1289, "seek": 592474, "start": 5945.26, "end": 5948.38, "text": " And then we're gonna go through each of the y's.", "tokens": [51390, 400, 550, 321, 434, 799, 352, 807, 1184, 295, 264, 288, 311, 13, 51546], "temperature": 0.0, "avg_logprob": -0.2624881493784215, "compression_ratio": 1.7924528301886793, "no_speech_prob": 9.422444691153942e-07}, {"id": 1290, "seek": 592474, "start": 5948.38, "end": 5952.74, "text": " And we're gonna do, this hopefully looks familiar, this is doing a lerp.", "tokens": [51546, 400, 321, 434, 799, 360, 11, 341, 4696, 1542, 4963, 11, 341, 307, 884, 257, 32068, 79, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2624881493784215, "compression_ratio": 1.7924528301886793, "no_speech_prob": 9.422444691153942e-07}, {"id": 1291, "seek": 595274, "start": 5952.74, "end": 5956.82, "text": " We're gonna take our previous average,", "tokens": [50364, 492, 434, 799, 747, 527, 3894, 4274, 11, 50568], "temperature": 0.0, "avg_logprob": -0.2842732218952922, "compression_ratio": 1.5897435897435896, "no_speech_prob": 1.6119918200274697e-07}, {"id": 1292, "seek": 595274, "start": 5956.82, "end": 5961.219999999999, "text": " which we'll start at 0, times beta,", "tokens": [50568, 597, 321, 603, 722, 412, 1958, 11, 1413, 9861, 11, 50788], "temperature": 0.0, "avg_logprob": -0.2842732218952922, "compression_ratio": 1.5897435897435896, "no_speech_prob": 1.6119918200274697e-07}, {"id": 1293, "seek": 595274, "start": 5961.219999999999, "end": 5969.34, "text": " which is 0.5 plus 1 minus beta, that's 0.5, times our new average.", "tokens": [50788, 597, 307, 1958, 13, 20, 1804, 502, 3175, 9861, 11, 300, 311, 1958, 13, 20, 11, 1413, 527, 777, 4274, 13, 51194], "temperature": 0.0, "avg_logprob": -0.2842732218952922, "compression_ratio": 1.5897435897435896, "no_speech_prob": 1.6119918200274697e-07}, {"id": 1294, "seek": 595274, "start": 5969.34, "end": 5973.179999999999, "text": " And then we'll append that to this red line.", "tokens": [51194, 400, 550, 321, 603, 34116, 300, 281, 341, 2182, 1622, 13, 51386], "temperature": 0.0, "avg_logprob": -0.2842732218952922, "compression_ratio": 1.5897435897435896, "no_speech_prob": 1.6119918200274697e-07}, {"id": 1295, "seek": 595274, "start": 5977.66, "end": 5981.46, "text": " And we'll do that for all the data points and then plot them.", "tokens": [51610, 400, 321, 603, 360, 300, 337, 439, 264, 1412, 2793, 293, 550, 7542, 552, 13, 51800], "temperature": 0.0, "avg_logprob": -0.2842732218952922, "compression_ratio": 1.5897435897435896, "no_speech_prob": 1.6119918200274697e-07}, {"id": 1296, "seek": 598146, "start": 5981.46, "end": 5986.02, "text": " And you can see what happens when we do that is that the red line", "tokens": [50364, 400, 291, 393, 536, 437, 2314, 562, 321, 360, 300, 307, 300, 264, 2182, 1622, 50592], "temperature": 0.0, "avg_logprob": -0.2773784347202467, "compression_ratio": 1.5740740740740742, "no_speech_prob": 3.747992138869449e-07}, {"id": 1297, "seek": 598146, "start": 5986.02, "end": 5988.94, "text": " becomes less bumpy, right?", "tokens": [50592, 3643, 1570, 49400, 11, 558, 30, 50738], "temperature": 0.0, "avg_logprob": -0.2773784347202467, "compression_ratio": 1.5740740740740742, "no_speech_prob": 3.747992138869449e-07}, {"id": 1298, "seek": 598146, "start": 5988.94, "end": 5992.78, "text": " Because each one is half, it's this exact dot, and", "tokens": [50738, 1436, 1184, 472, 307, 1922, 11, 309, 311, 341, 1900, 5893, 11, 293, 50930], "temperature": 0.0, "avg_logprob": -0.2773784347202467, "compression_ratio": 1.5740740740740742, "no_speech_prob": 3.747992138869449e-07}, {"id": 1299, "seek": 598146, "start": 5992.78, "end": 5995.62, "text": " half of whatever the red line previously was.", "tokens": [50930, 1922, 295, 2035, 264, 2182, 1622, 8046, 390, 13, 51072], "temperature": 0.0, "avg_logprob": -0.2773784347202467, "compression_ratio": 1.5740740740740742, "no_speech_prob": 3.747992138869449e-07}, {"id": 1300, "seek": 598146, "start": 5995.62, "end": 5999.22, "text": " So again, this is an exponentially weighted moving average.", "tokens": [51072, 407, 797, 11, 341, 307, 364, 37330, 32807, 2684, 4274, 13, 51252], "temperature": 0.0, "avg_logprob": -0.2773784347202467, "compression_ratio": 1.5740740740740742, "no_speech_prob": 3.747992138869449e-07}, {"id": 1301, "seek": 598146, "start": 5999.22, "end": 6002.5, "text": " And so we could have implemented this using lerp.", "tokens": [51252, 400, 370, 321, 727, 362, 12270, 341, 1228, 32068, 79, 13, 51416], "temperature": 0.0, "avg_logprob": -0.2773784347202467, "compression_ratio": 1.5740740740740742, "no_speech_prob": 3.747992138869449e-07}, {"id": 1302, "seek": 598146, "start": 6005.58, "end": 6010.34, "text": " So as the beta gets higher, it's saying,", "tokens": [51570, 407, 382, 264, 9861, 2170, 2946, 11, 309, 311, 1566, 11, 51808], "temperature": 0.0, "avg_logprob": -0.2773784347202467, "compression_ratio": 1.5740740740740742, "no_speech_prob": 3.747992138869449e-07}, {"id": 1303, "seek": 601034, "start": 6010.34, "end": 6015.02, "text": " do more of just be wherever the red line used to be, and", "tokens": [50364, 360, 544, 295, 445, 312, 8660, 264, 2182, 1622, 1143, 281, 312, 11, 293, 50598], "temperature": 0.0, "avg_logprob": -0.23281776026675577, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.1726421007551835e-06}, {"id": 1304, "seek": 601034, "start": 6015.02, "end": 6018.06, "text": " less of where this particular data point is.", "tokens": [50598, 1570, 295, 689, 341, 1729, 1412, 935, 307, 13, 50750], "temperature": 0.0, "avg_logprob": -0.23281776026675577, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.1726421007551835e-06}, {"id": 1305, "seek": 601034, "start": 6018.06, "end": 6021.38, "text": " And so that means when we have these kind of outliers,", "tokens": [50750, 400, 370, 300, 1355, 562, 321, 362, 613, 733, 295, 484, 23646, 11, 50916], "temperature": 0.0, "avg_logprob": -0.23281776026675577, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.1726421007551835e-06}, {"id": 1306, "seek": 601034, "start": 6021.38, "end": 6025.42, "text": " the red line doesn't jump around as much, as you see.", "tokens": [50916, 264, 2182, 1622, 1177, 380, 3012, 926, 382, 709, 11, 382, 291, 536, 13, 51118], "temperature": 0.0, "avg_logprob": -0.23281776026675577, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.1726421007551835e-06}, {"id": 1307, "seek": 601034, "start": 6026.42, "end": 6033.14, "text": " But if your momentum gets too high, then it doesn't follow what's going on at all.", "tokens": [51168, 583, 498, 428, 11244, 2170, 886, 1090, 11, 550, 309, 1177, 380, 1524, 437, 311, 516, 322, 412, 439, 13, 51504], "temperature": 0.0, "avg_logprob": -0.23281776026675577, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.1726421007551835e-06}, {"id": 1308, "seek": 601034, "start": 6033.14, "end": 6035.34, "text": " And in fact, it's way behind, right?", "tokens": [51504, 400, 294, 1186, 11, 309, 311, 636, 2261, 11, 558, 30, 51614], "temperature": 0.0, "avg_logprob": -0.23281776026675577, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.1726421007551835e-06}, {"id": 1309, "seek": 603534, "start": 6035.34, "end": 6040.46, "text": " When you're using momentum, it's always going to be partially responding", "tokens": [50364, 1133, 291, 434, 1228, 11244, 11, 309, 311, 1009, 516, 281, 312, 18886, 16670, 50620], "temperature": 0.0, "avg_logprob": -0.2781247099240621, "compression_ratio": 1.5961538461538463, "no_speech_prob": 0.00042388378642499447}, {"id": 1310, "seek": 603534, "start": 6040.46, "end": 6043.66, "text": " to how things were many batches ago.", "tokens": [50620, 281, 577, 721, 645, 867, 15245, 279, 2057, 13, 50780], "temperature": 0.0, "avg_logprob": -0.2781247099240621, "compression_ratio": 1.5961538461538463, "no_speech_prob": 0.00042388378642499447}, {"id": 1311, "seek": 603534, "start": 6045.78, "end": 6052.1, "text": " And so even at beta of 0.9 here, the red line is offset to the right.", "tokens": [50886, 400, 370, 754, 412, 9861, 295, 1958, 13, 24, 510, 11, 264, 2182, 1622, 307, 18687, 281, 264, 558, 13, 51202], "temperature": 0.0, "avg_logprob": -0.2781247099240621, "compression_ratio": 1.5961538461538463, "no_speech_prob": 0.00042388378642499447}, {"id": 1312, "seek": 603534, "start": 6052.1, "end": 6055.900000000001, "text": " Because again, it's taking it a while for it to recognize that,", "tokens": [51202, 1436, 797, 11, 309, 311, 1940, 309, 257, 1339, 337, 309, 281, 5521, 300, 11, 51392], "temperature": 0.0, "avg_logprob": -0.2781247099240621, "compression_ratio": 1.5961538461538463, "no_speech_prob": 0.00042388378642499447}, {"id": 1313, "seek": 603534, "start": 6055.900000000001, "end": 6057.34, "text": " things have changed.", "tokens": [51392, 721, 362, 3105, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2781247099240621, "compression_ratio": 1.5961538461538463, "no_speech_prob": 0.00042388378642499447}, {"id": 1314, "seek": 603534, "start": 6057.34, "end": 6062.58, "text": " Because each time, it's 0.9 of it is where the red line used to be.", "tokens": [51464, 1436, 1184, 565, 11, 309, 311, 1958, 13, 24, 295, 309, 307, 689, 264, 2182, 1622, 1143, 281, 312, 13, 51726], "temperature": 0.0, "avg_logprob": -0.2781247099240621, "compression_ratio": 1.5961538461538463, "no_speech_prob": 0.00042388378642499447}, {"id": 1315, "seek": 606258, "start": 6062.58, "end": 6066.0199999999995, "text": " And only 0.1 of it is what this data point say.", "tokens": [50364, 400, 787, 1958, 13, 16, 295, 309, 307, 437, 341, 1412, 935, 584, 13, 50536], "temperature": 0.0, "avg_logprob": -0.46038907766342163, "compression_ratio": 1.4539473684210527, "no_speech_prob": 3.205827852070797e-07}, {"id": 1316, "seek": 606258, "start": 6067.14, "end": 6069.66, "text": " So that's what momentum does.", "tokens": [50592, 407, 300, 311, 437, 11244, 775, 13, 50718], "temperature": 0.0, "avg_logprob": -0.46038907766342163, "compression_ratio": 1.4539473684210527, "no_speech_prob": 3.205827852070797e-07}, {"id": 1317, "seek": 606258, "start": 6070.86, "end": 6076.78, "text": " So the reason that momentum is useful is because when you have", "tokens": [50778, 407, 264, 1778, 300, 11244, 307, 4420, 307, 570, 562, 291, 362, 51074], "temperature": 0.0, "avg_logprob": -0.46038907766342163, "compression_ratio": 1.4539473684210527, "no_speech_prob": 3.205827852070797e-07}, {"id": 1318, "seek": 606258, "start": 6076.78, "end": 6082.7, "text": " a loss function that's actually kind of like very,", "tokens": [51074, 257, 4470, 2445, 300, 311, 767, 733, 295, 411, 588, 11, 51370], "temperature": 0.0, "avg_logprob": -0.46038907766342163, "compression_ratio": 1.4539473684210527, "no_speech_prob": 3.205827852070797e-07}, {"id": 1319, "seek": 606258, "start": 6082.7, "end": 6089.7, "text": " very bumpy, like that, right?", "tokens": [51370, 588, 49400, 11, 411, 300, 11, 558, 30, 51720], "temperature": 0.0, "avg_logprob": -0.46038907766342163, "compression_ratio": 1.4539473684210527, "no_speech_prob": 3.205827852070797e-07}, {"id": 1320, "seek": 608970, "start": 6089.7, "end": 6096.66, "text": " You want to be able to follow the actual curve, right?", "tokens": [50364, 509, 528, 281, 312, 1075, 281, 1524, 264, 3539, 7605, 11, 558, 30, 50712], "temperature": 0.0, "avg_logprob": -0.2365115483601888, "compression_ratio": 1.9288702928870294, "no_speech_prob": 2.2603242086915998e-06}, {"id": 1321, "seek": 608970, "start": 6096.66, "end": 6099.66, "text": " So using momentum, you don't quite get that, but", "tokens": [50712, 407, 1228, 11244, 11, 291, 500, 380, 1596, 483, 300, 11, 457, 50862], "temperature": 0.0, "avg_logprob": -0.2365115483601888, "compression_ratio": 1.9288702928870294, "no_speech_prob": 2.2603242086915998e-06}, {"id": 1322, "seek": 608970, "start": 6099.66, "end": 6104.86, "text": " you get a kind of a version of that that's offset to the right a little bit.", "tokens": [50862, 291, 483, 257, 733, 295, 257, 3037, 295, 300, 300, 311, 18687, 281, 264, 558, 257, 707, 857, 13, 51122], "temperature": 0.0, "avg_logprob": -0.2365115483601888, "compression_ratio": 1.9288702928870294, "no_speech_prob": 2.2603242086915998e-06}, {"id": 1323, "seek": 608970, "start": 6104.86, "end": 6108.46, "text": " But still, hopefully, spending a lot more time,", "tokens": [51122, 583, 920, 11, 4696, 11, 6434, 257, 688, 544, 565, 11, 51302], "temperature": 0.0, "avg_logprob": -0.2365115483601888, "compression_ratio": 1.9288702928870294, "no_speech_prob": 2.2603242086915998e-06}, {"id": 1324, "seek": 608970, "start": 6108.46, "end": 6111.0199999999995, "text": " you don't really want to be heading off in this direction,", "tokens": [51302, 291, 500, 380, 534, 528, 281, 312, 9864, 766, 294, 341, 3513, 11, 51430], "temperature": 0.0, "avg_logprob": -0.2365115483601888, "compression_ratio": 1.9288702928870294, "no_speech_prob": 2.2603242086915998e-06}, {"id": 1325, "seek": 608970, "start": 6111.0199999999995, "end": 6112.38, "text": " which you would if you followed the line.", "tokens": [51430, 597, 291, 576, 498, 291, 6263, 264, 1622, 13, 51498], "temperature": 0.0, "avg_logprob": -0.2365115483601888, "compression_ratio": 1.9288702928870294, "no_speech_prob": 2.2603242086915998e-06}, {"id": 1326, "seek": 608970, "start": 6112.38, "end": 6115.0599999999995, "text": " And then this direction, which you would if you followed the line.", "tokens": [51498, 400, 550, 341, 3513, 11, 597, 291, 576, 498, 291, 6263, 264, 1622, 13, 51632], "temperature": 0.0, "avg_logprob": -0.2365115483601888, "compression_ratio": 1.9288702928870294, "no_speech_prob": 2.2603242086915998e-06}, {"id": 1327, "seek": 608970, "start": 6115.0599999999995, "end": 6118.38, "text": " You really want to be following the average of those directions.", "tokens": [51632, 509, 534, 528, 281, 312, 3480, 264, 4274, 295, 729, 11095, 13, 51798], "temperature": 0.0, "avg_logprob": -0.2365115483601888, "compression_ratio": 1.9288702928870294, "no_speech_prob": 2.2603242086915998e-06}, {"id": 1328, "seek": 611970, "start": 6119.7, "end": 6121.139999999999, "text": " And that's what momentum lets you do.", "tokens": [50364, 400, 300, 311, 437, 11244, 6653, 291, 360, 13, 50436], "temperature": 0.0, "avg_logprob": -0.37596239362444195, "compression_ratio": 1.702247191011236, "no_speech_prob": 9.422445828022319e-07}, {"id": 1329, "seek": 611970, "start": 6128.22, "end": 6131.82, "text": " So to use momentum, we will inherit from SGD, and", "tokens": [50790, 407, 281, 764, 11244, 11, 321, 486, 21389, 490, 34520, 35, 11, 293, 50970], "temperature": 0.0, "avg_logprob": -0.37596239362444195, "compression_ratio": 1.702247191011236, "no_speech_prob": 9.422445828022319e-07}, {"id": 1330, "seek": 611970, "start": 6131.82, "end": 6137.0599999999995, "text": " we will override the definition of the optimization step.", "tokens": [50970, 321, 486, 42321, 264, 7123, 295, 264, 19618, 1823, 13, 51232], "temperature": 0.0, "avg_logprob": -0.37596239362444195, "compression_ratio": 1.702247191011236, "no_speech_prob": 9.422445828022319e-07}, {"id": 1331, "seek": 611970, "start": 6137.0599999999995, "end": 6139.62, "text": " Remember, there was two things that step called.", "tokens": [51232, 5459, 11, 456, 390, 732, 721, 300, 1823, 1219, 13, 51360], "temperature": 0.0, "avg_logprob": -0.37596239362444195, "compression_ratio": 1.702247191011236, "no_speech_prob": 9.422445828022319e-07}, {"id": 1332, "seek": 611970, "start": 6139.62, "end": 6142.3, "text": " It called the regularization step and the optimization step.", "tokens": [51360, 467, 1219, 264, 3890, 2144, 1823, 293, 264, 19618, 1823, 13, 51494], "temperature": 0.0, "avg_logprob": -0.37596239362444195, "compression_ratio": 1.702247191011236, "no_speech_prob": 9.422445828022319e-07}, {"id": 1333, "seek": 611970, "start": 6143.54, "end": 6146.179999999999, "text": " So we're going to modify the optimization step.", "tokens": [51556, 407, 321, 434, 516, 281, 16927, 264, 19618, 1823, 13, 51688], "temperature": 0.0, "avg_logprob": -0.37596239362444195, "compression_ratio": 1.702247191011236, "no_speech_prob": 9.422445828022319e-07}, {"id": 1334, "seek": 614618, "start": 6146.18, "end": 6150.1, "text": " We're not just going to do minus equals grad times self.lr.", "tokens": [50364, 492, 434, 406, 445, 516, 281, 360, 3175, 6915, 2771, 1413, 2698, 13, 40987, 13, 50560], "temperature": 0.0, "avg_logprob": -0.319667703965131, "compression_ratio": 1.6218905472636815, "no_speech_prob": 3.089494384767022e-06}, {"id": 1335, "seek": 614618, "start": 6150.1, "end": 6154.3, "text": " But instead, when we create our momentum object,", "tokens": [50560, 583, 2602, 11, 562, 321, 1884, 527, 11244, 2657, 11, 50770], "temperature": 0.0, "avg_logprob": -0.319667703965131, "compression_ratio": 1.6218905472636815, "no_speech_prob": 3.089494384767022e-06}, {"id": 1336, "seek": 614618, "start": 6155.5, "end": 6162.14, "text": " we will tell it what momentum we want, or default to 0.9, store that away.", "tokens": [50830, 321, 486, 980, 309, 437, 11244, 321, 528, 11, 420, 7576, 281, 1958, 13, 24, 11, 3531, 300, 1314, 13, 51162], "temperature": 0.0, "avg_logprob": -0.319667703965131, "compression_ratio": 1.6218905472636815, "no_speech_prob": 3.089494384767022e-06}, {"id": 1337, "seek": 614618, "start": 6162.14, "end": 6166.54, "text": " And then in the optimization step, for each parameter,", "tokens": [51162, 400, 550, 294, 264, 19618, 1823, 11, 337, 1184, 13075, 11, 51382], "temperature": 0.0, "avg_logprob": -0.319667703965131, "compression_ratio": 1.6218905472636815, "no_speech_prob": 3.089494384767022e-06}, {"id": 1338, "seek": 614618, "start": 6166.54, "end": 6169.780000000001, "text": " because remember the optimization step is being called for", "tokens": [51382, 570, 1604, 264, 19618, 1823, 307, 885, 1219, 337, 51544], "temperature": 0.0, "avg_logprob": -0.319667703965131, "compression_ratio": 1.6218905472636815, "no_speech_prob": 3.089494384767022e-06}, {"id": 1339, "seek": 614618, "start": 6169.780000000001, "end": 6172.1, "text": " each parameter in our model.", "tokens": [51544, 1184, 13075, 294, 527, 2316, 13, 51660], "temperature": 0.0, "avg_logprob": -0.319667703965131, "compression_ratio": 1.6218905472636815, "no_speech_prob": 3.089494384767022e-06}, {"id": 1340, "seek": 617210, "start": 6172.1, "end": 6176.58, "text": " So that's each layer's weights and each layer's biases, for example.", "tokens": [50364, 407, 300, 311, 1184, 4583, 311, 17443, 293, 1184, 4583, 311, 32152, 11, 337, 1365, 13, 50588], "temperature": 0.0, "avg_logprob": -0.29400496580162827, "compression_ratio": 1.6575342465753424, "no_speech_prob": 7.766951966914348e-06}, {"id": 1341, "seek": 617210, "start": 6176.58, "end": 6178.38, "text": " We'll find out for that parameter,", "tokens": [50588, 492, 603, 915, 484, 337, 300, 13075, 11, 50678], "temperature": 0.0, "avg_logprob": -0.29400496580162827, "compression_ratio": 1.6575342465753424, "no_speech_prob": 7.766951966914348e-06}, {"id": 1342, "seek": 617210, "start": 6178.38, "end": 6181.38, "text": " have we ever stored away its moving average of gradients before?", "tokens": [50678, 362, 321, 1562, 12187, 1314, 1080, 2684, 4274, 295, 2771, 2448, 949, 30, 50828], "temperature": 0.0, "avg_logprob": -0.29400496580162827, "compression_ratio": 1.6575342465753424, "no_speech_prob": 7.766951966914348e-06}, {"id": 1343, "seek": 617210, "start": 6182.54, "end": 6189.5, "text": " And if we haven't, then we'll set them to 0 initially, just like we did here.", "tokens": [50886, 400, 498, 321, 2378, 380, 11, 550, 321, 603, 992, 552, 281, 1958, 9105, 11, 445, 411, 321, 630, 510, 13, 51234], "temperature": 0.0, "avg_logprob": -0.29400496580162827, "compression_ratio": 1.6575342465753424, "no_speech_prob": 7.766951966914348e-06}, {"id": 1344, "seek": 617210, "start": 6191.9400000000005, "end": 6196.42, "text": " And then we will do our lerp, right?", "tokens": [51356, 400, 550, 321, 486, 360, 527, 32068, 79, 11, 558, 30, 51580], "temperature": 0.0, "avg_logprob": -0.29400496580162827, "compression_ratio": 1.6575342465753424, "no_speech_prob": 7.766951966914348e-06}, {"id": 1345, "seek": 617210, "start": 6196.42, "end": 6200.46, "text": " So we're going to say the exponentially weighted moving average of gradients is", "tokens": [51580, 407, 321, 434, 516, 281, 584, 264, 37330, 32807, 2684, 4274, 295, 2771, 2448, 307, 51782], "temperature": 0.0, "avg_logprob": -0.29400496580162827, "compression_ratio": 1.6575342465753424, "no_speech_prob": 7.766951966914348e-06}, {"id": 1346, "seek": 620046, "start": 6200.46, "end": 6206.74, "text": " equal to whatever it used to be times the momentum plus", "tokens": [50364, 2681, 281, 2035, 309, 1143, 281, 312, 1413, 264, 11244, 1804, 50678], "temperature": 0.0, "avg_logprob": -0.2529170103747435, "compression_ratio": 1.5884955752212389, "no_speech_prob": 1.1015954441973008e-06}, {"id": 1347, "seek": 620046, "start": 6208.58, "end": 6213.46, "text": " this actual new batch's gradients times 1 minus momentum.", "tokens": [50770, 341, 3539, 777, 15245, 311, 2771, 2448, 1413, 502, 3175, 11244, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2529170103747435, "compression_ratio": 1.5884955752212389, "no_speech_prob": 1.1015954441973008e-06}, {"id": 1348, "seek": 620046, "start": 6213.46, "end": 6216.02, "text": " So that's just doing the lerp, as we discussed.", "tokens": [51014, 407, 300, 311, 445, 884, 264, 32068, 79, 11, 382, 321, 7152, 13, 51142], "temperature": 0.0, "avg_logprob": -0.2529170103747435, "compression_ratio": 1.5884955752212389, "no_speech_prob": 1.1015954441973008e-06}, {"id": 1349, "seek": 620046, "start": 6216.02, "end": 6219.9800000000005, "text": " And so then we're just going to do exactly the same as the SGD update step.", "tokens": [51142, 400, 370, 550, 321, 434, 445, 516, 281, 360, 2293, 264, 912, 382, 264, 34520, 35, 5623, 1823, 13, 51340], "temperature": 0.0, "avg_logprob": -0.2529170103747435, "compression_ratio": 1.5884955752212389, "no_speech_prob": 1.1015954441973008e-06}, {"id": 1350, "seek": 620046, "start": 6219.9800000000005, "end": 6224.7, "text": " But instead of multiplying by p.grad, we're multiplying it by p.grad average.", "tokens": [51340, 583, 2602, 295, 30955, 538, 280, 13, 7165, 11, 321, 434, 30955, 309, 538, 280, 13, 7165, 4274, 13, 51576], "temperature": 0.0, "avg_logprob": -0.2529170103747435, "compression_ratio": 1.5884955752212389, "no_speech_prob": 1.1015954441973008e-06}, {"id": 1351, "seek": 620046, "start": 6224.7, "end": 6226.5, "text": " So there's a cool little trick here, right?", "tokens": [51576, 407, 456, 311, 257, 1627, 707, 4282, 510, 11, 558, 30, 51666], "temperature": 0.0, "avg_logprob": -0.2529170103747435, "compression_ratio": 1.5884955752212389, "no_speech_prob": 1.1015954441973008e-06}, {"id": 1352, "seek": 622650, "start": 6226.5, "end": 6230.54, "text": " Which is that we are basically inventing a brand new attribute,", "tokens": [50364, 3013, 307, 300, 321, 366, 1936, 7962, 278, 257, 3360, 777, 19667, 11, 50566], "temperature": 0.0, "avg_logprob": -0.2700644187556887, "compression_ratio": 1.683794466403162, "no_speech_prob": 3.763640779652633e-05}, {"id": 1353, "seek": 622650, "start": 6230.54, "end": 6232.18, "text": " putting it inside the parameter tensor.", "tokens": [50566, 3372, 309, 1854, 264, 13075, 40863, 13, 50648], "temperature": 0.0, "avg_logprob": -0.2700644187556887, "compression_ratio": 1.683794466403162, "no_speech_prob": 3.763640779652633e-05}, {"id": 1354, "seek": 622650, "start": 6233.86, "end": 6240.02, "text": " And that attribute is where we're storing away the exponentially", "tokens": [50732, 400, 300, 19667, 307, 689, 321, 434, 26085, 1314, 264, 37330, 51040], "temperature": 0.0, "avg_logprob": -0.2700644187556887, "compression_ratio": 1.683794466403162, "no_speech_prob": 3.763640779652633e-05}, {"id": 1355, "seek": 622650, "start": 6240.02, "end": 6243.38, "text": " weighted moving average of gradients for that particular parameter.", "tokens": [51040, 32807, 2684, 4274, 295, 2771, 2448, 337, 300, 1729, 13075, 13, 51208], "temperature": 0.0, "avg_logprob": -0.2700644187556887, "compression_ratio": 1.683794466403162, "no_speech_prob": 3.763640779652633e-05}, {"id": 1356, "seek": 622650, "start": 6243.38, "end": 6244.82, "text": " So as we loop through the parameters,", "tokens": [51208, 407, 382, 321, 6367, 807, 264, 9834, 11, 51280], "temperature": 0.0, "avg_logprob": -0.2700644187556887, "compression_ratio": 1.683794466403162, "no_speech_prob": 3.763640779652633e-05}, {"id": 1357, "seek": 622650, "start": 6244.82, "end": 6247.78, "text": " we don't have to do any special work to get access to that.", "tokens": [51280, 321, 500, 380, 362, 281, 360, 604, 2121, 589, 281, 483, 2105, 281, 300, 13, 51428], "temperature": 0.0, "avg_logprob": -0.2700644187556887, "compression_ratio": 1.683794466403162, "no_speech_prob": 3.763640779652633e-05}, {"id": 1358, "seek": 622650, "start": 6247.78, "end": 6248.82, "text": " So I think that's pretty handy.", "tokens": [51428, 407, 286, 519, 300, 311, 1238, 13239, 13, 51480], "temperature": 0.0, "avg_logprob": -0.2700644187556887, "compression_ratio": 1.683794466403162, "no_speech_prob": 3.763640779652633e-05}, {"id": 1359, "seek": 622650, "start": 6252.46, "end": 6255.86, "text": " All right, so one interesting thing, very interesting here,", "tokens": [51662, 1057, 558, 11, 370, 472, 1880, 551, 11, 588, 1880, 510, 11, 51832], "temperature": 0.0, "avg_logprob": -0.2700644187556887, "compression_ratio": 1.683794466403162, "no_speech_prob": 3.763640779652633e-05}, {"id": 1360, "seek": 625586, "start": 6255.86, "end": 6260.0199999999995, "text": " I found is I could really hike the learning rate way up to 1.5.", "tokens": [50364, 286, 1352, 307, 286, 727, 534, 23282, 264, 2539, 3314, 636, 493, 281, 502, 13, 20, 13, 50572], "temperature": 0.0, "avg_logprob": -0.3168912168409004, "compression_ratio": 1.6862745098039216, "no_speech_prob": 5.391081504058093e-05}, {"id": 1361, "seek": 625586, "start": 6260.0199999999995, "end": 6263.54, "text": " And the reason why is because we're not getting these huge bumps anymore.", "tokens": [50572, 400, 264, 1778, 983, 307, 570, 321, 434, 406, 1242, 613, 2603, 27719, 3602, 13, 50748], "temperature": 0.0, "avg_logprob": -0.3168912168409004, "compression_ratio": 1.6862745098039216, "no_speech_prob": 5.391081504058093e-05}, {"id": 1362, "seek": 625586, "start": 6263.54, "end": 6265.38, "text": " And so by getting rid of the huge bumps,", "tokens": [50748, 400, 370, 538, 1242, 3973, 295, 264, 2603, 27719, 11, 50840], "temperature": 0.0, "avg_logprob": -0.3168912168409004, "compression_ratio": 1.6862745098039216, "no_speech_prob": 5.391081504058093e-05}, {"id": 1363, "seek": 625586, "start": 6265.38, "end": 6267.219999999999, "text": " the whole thing's just a whole lot smoother.", "tokens": [50840, 264, 1379, 551, 311, 445, 257, 1379, 688, 28640, 13, 50932], "temperature": 0.0, "avg_logprob": -0.3168912168409004, "compression_ratio": 1.6862745098039216, "no_speech_prob": 5.391081504058093e-05}, {"id": 1364, "seek": 625586, "start": 6268.78, "end": 6274.42, "text": " So previously we got up to 85% because we've gone back to our", "tokens": [51010, 407, 8046, 321, 658, 493, 281, 14695, 4, 570, 321, 600, 2780, 646, 281, 527, 51292], "temperature": 0.0, "avg_logprob": -0.3168912168409004, "compression_ratio": 1.6862745098039216, "no_speech_prob": 5.391081504058093e-05}, {"id": 1365, "seek": 625586, "start": 6274.42, "end": 6280.339999999999, "text": " 102 for batch size and just three epochs and a constant learning rate.", "tokens": [51292, 502, 12756, 337, 15245, 2744, 293, 445, 1045, 30992, 28346, 293, 257, 5754, 2539, 3314, 13, 51588], "temperature": 0.0, "avg_logprob": -0.3168912168409004, "compression_ratio": 1.6862745098039216, "no_speech_prob": 5.391081504058093e-05}, {"id": 1366, "seek": 625586, "start": 6280.339999999999, "end": 6283.219999999999, "text": " And look at that, we've gone up to 87.6%.", "tokens": [51588, 400, 574, 412, 300, 11, 321, 600, 2780, 493, 281, 27990, 13, 21, 6856, 51732], "temperature": 0.0, "avg_logprob": -0.3168912168409004, "compression_ratio": 1.6862745098039216, "no_speech_prob": 5.391081504058093e-05}, {"id": 1367, "seek": 625586, "start": 6283.219999999999, "end": 6285.139999999999, "text": " So it's really improved things.", "tokens": [51732, 407, 309, 311, 534, 9689, 721, 13, 51828], "temperature": 0.0, "avg_logprob": -0.3168912168409004, "compression_ratio": 1.6862745098039216, "no_speech_prob": 5.391081504058093e-05}, {"id": 1368, "seek": 628514, "start": 6285.780000000001, "end": 6290.38, "text": " The loss function is nice and smooth, as you can see.", "tokens": [50396, 440, 4470, 2445, 307, 1481, 293, 5508, 11, 382, 291, 393, 536, 13, 50626], "temperature": 0.0, "avg_logprob": -0.4281515077103016, "compression_ratio": 1.568421052631579, "no_speech_prob": 2.616520475839934e-07}, {"id": 1369, "seek": 628514, "start": 6293.660000000001, "end": 6298.42, "text": " Okay, and so then in our color dim plot, you can see this is actually,", "tokens": [50790, 1033, 11, 293, 370, 550, 294, 527, 2017, 5013, 7542, 11, 291, 393, 536, 341, 307, 767, 11, 51028], "temperature": 0.0, "avg_logprob": -0.4281515077103016, "compression_ratio": 1.568421052631579, "no_speech_prob": 2.616520475839934e-07}, {"id": 1370, "seek": 628514, "start": 6298.42, "end": 6302.14, "text": " that's really the smoothest we've seen.", "tokens": [51028, 300, 311, 534, 264, 5508, 377, 321, 600, 1612, 13, 51214], "temperature": 0.0, "avg_logprob": -0.4281515077103016, "compression_ratio": 1.568421052631579, "no_speech_prob": 2.616520475839934e-07}, {"id": 1371, "seek": 628514, "start": 6302.14, "end": 6304.62, "text": " And it's a bit different to the momentum learner,", "tokens": [51214, 400, 309, 311, 257, 857, 819, 281, 264, 11244, 33347, 11, 51338], "temperature": 0.0, "avg_logprob": -0.4281515077103016, "compression_ratio": 1.568421052631579, "no_speech_prob": 2.616520475839934e-07}, {"id": 1372, "seek": 628514, "start": 6304.62, "end": 6308.58, "text": " cuz the momentum learner didn't have this one minus part, right?", "tokens": [51338, 11910, 264, 11244, 33347, 994, 380, 362, 341, 472, 3175, 644, 11, 558, 30, 51536], "temperature": 0.0, "avg_logprob": -0.4281515077103016, "compression_ratio": 1.568421052631579, "no_speech_prob": 2.616520475839934e-07}, {"id": 1373, "seek": 628514, "start": 6310.34, "end": 6311.34, "text": " It wasn't lerping,", "tokens": [51624, 467, 2067, 380, 32068, 3381, 11, 51674], "temperature": 0.0, "avg_logprob": -0.4281515077103016, "compression_ratio": 1.568421052631579, "no_speech_prob": 2.616520475839934e-07}, {"id": 1374, "seek": 631134, "start": 6311.34, "end": 6317.14, "text": " it was basically always including all of the grad plus a bit of the momentum part.", "tokens": [50364, 309, 390, 1936, 1009, 3009, 439, 295, 264, 2771, 1804, 257, 857, 295, 264, 11244, 644, 13, 50654], "temperature": 0.0, "avg_logprob": -0.33081833521525067, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.1843138054246083e-05}, {"id": 1375, "seek": 631134, "start": 6317.14, "end": 6324.38, "text": " So this is a different, better approach, I think.", "tokens": [50654, 407, 341, 307, 257, 819, 11, 1101, 3109, 11, 286, 519, 13, 51016], "temperature": 0.0, "avg_logprob": -0.33081833521525067, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.1843138054246083e-05}, {"id": 1376, "seek": 631134, "start": 6326.7, "end": 6330.5, "text": " And yeah, we've got a really nice, smooth result.", "tokens": [51132, 400, 1338, 11, 321, 600, 658, 257, 534, 1481, 11, 5508, 1874, 13, 51322], "temperature": 0.0, "avg_logprob": -0.33081833521525067, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.1843138054246083e-05}, {"id": 1377, "seek": 631134, "start": 6331.5, "end": 6334.7, "text": " And the person's asking, don't we get a similar effect,", "tokens": [51372, 400, 264, 954, 311, 3365, 11, 500, 380, 321, 483, 257, 2531, 1802, 11, 51532], "temperature": 0.0, "avg_logprob": -0.33081833521525067, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.1843138054246083e-05}, {"id": 1378, "seek": 631134, "start": 6334.7, "end": 6337.78, "text": " I think, in terms of the smoothness if we increase the batch size?", "tokens": [51532, 286, 519, 11, 294, 2115, 295, 264, 5508, 1287, 498, 321, 3488, 264, 15245, 2744, 30, 51686], "temperature": 0.0, "avg_logprob": -0.33081833521525067, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.1843138054246083e-05}, {"id": 1379, "seek": 633778, "start": 6337.78, "end": 6340.5, "text": " Which we do, but if you just increase the batch size,", "tokens": [50364, 3013, 321, 360, 11, 457, 498, 291, 445, 3488, 264, 15245, 2744, 11, 50500], "temperature": 0.0, "avg_logprob": -0.32148322709109806, "compression_ratio": 1.6352459016393444, "no_speech_prob": 7.48460297472775e-05}, {"id": 1380, "seek": 633778, "start": 6340.5, "end": 6344.0199999999995, "text": " you're giving it less opportunities to update.", "tokens": [50500, 291, 434, 2902, 309, 1570, 4786, 281, 5623, 13, 50676], "temperature": 0.0, "avg_logprob": -0.32148322709109806, "compression_ratio": 1.6352459016393444, "no_speech_prob": 7.48460297472775e-05}, {"id": 1381, "seek": 633778, "start": 6344.0199999999995, "end": 6347.139999999999, "text": " So having a really big batch size is actually not great.", "tokens": [50676, 407, 1419, 257, 534, 955, 15245, 2744, 307, 767, 406, 869, 13, 50832], "temperature": 0.0, "avg_logprob": -0.32148322709109806, "compression_ratio": 1.6352459016393444, "no_speech_prob": 7.48460297472775e-05}, {"id": 1382, "seek": 633778, "start": 6348.139999999999, "end": 6352.42, "text": " Jan Lekun, who created the first really successful ConvNets,", "tokens": [50882, 4956, 441, 916, 409, 11, 567, 2942, 264, 700, 534, 4406, 2656, 85, 45, 1385, 11, 51096], "temperature": 0.0, "avg_logprob": -0.32148322709109806, "compression_ratio": 1.6352459016393444, "no_speech_prob": 7.48460297472775e-05}, {"id": 1383, "seek": 633778, "start": 6352.42, "end": 6356.54, "text": " including LearnNet 5, says he thinks the ideal batch size,", "tokens": [51096, 3009, 17216, 31890, 1025, 11, 1619, 415, 7309, 264, 7157, 15245, 2744, 11, 51302], "temperature": 0.0, "avg_logprob": -0.32148322709109806, "compression_ratio": 1.6352459016393444, "no_speech_prob": 7.48460297472775e-05}, {"id": 1384, "seek": 633778, "start": 6356.54, "end": 6360.98, "text": " if you can get away with it, is one, but it's just slow.", "tokens": [51302, 498, 291, 393, 483, 1314, 365, 309, 11, 307, 472, 11, 457, 309, 311, 445, 2964, 13, 51524], "temperature": 0.0, "avg_logprob": -0.32148322709109806, "compression_ratio": 1.6352459016393444, "no_speech_prob": 7.48460297472775e-05}, {"id": 1385, "seek": 633778, "start": 6362.46, "end": 6365.54, "text": " You want it to have as many opportunities to update as possible.", "tokens": [51598, 509, 528, 309, 281, 362, 382, 867, 4786, 281, 5623, 382, 1944, 13, 51752], "temperature": 0.0, "avg_logprob": -0.32148322709109806, "compression_ratio": 1.6352459016393444, "no_speech_prob": 7.48460297472775e-05}, {"id": 1386, "seek": 636554, "start": 6365.54, "end": 6368.7, "text": " There's this weird thing recently where people seem to be trying to create really", "tokens": [50364, 821, 311, 341, 3657, 551, 3938, 689, 561, 1643, 281, 312, 1382, 281, 1884, 534, 50522], "temperature": 0.0, "avg_logprob": -0.2941262626647949, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.0616131476126611e-05}, {"id": 1387, "seek": 636554, "start": 6368.7, "end": 6375.42, "text": " large batch sizes, which to me is, yeah, doesn't make any sense.", "tokens": [50522, 2416, 15245, 11602, 11, 597, 281, 385, 307, 11, 1338, 11, 1177, 380, 652, 604, 2020, 13, 50858], "temperature": 0.0, "avg_logprob": -0.2941262626647949, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.0616131476126611e-05}, {"id": 1388, "seek": 636554, "start": 6379.46, "end": 6381.54, "text": " We want the smallest batch size we can get away with,", "tokens": [51060, 492, 528, 264, 16998, 15245, 2744, 321, 393, 483, 1314, 365, 11, 51164], "temperature": 0.0, "avg_logprob": -0.2941262626647949, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.0616131476126611e-05}, {"id": 1389, "seek": 636554, "start": 6381.54, "end": 6384.34, "text": " generally speaking, to give it the most chances to update.", "tokens": [51164, 5101, 4124, 11, 281, 976, 309, 264, 881, 10486, 281, 5623, 13, 51304], "temperature": 0.0, "avg_logprob": -0.2941262626647949, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.0616131476126611e-05}, {"id": 1390, "seek": 636554, "start": 6384.34, "end": 6388.1, "text": " So this has done a great job of that, and we're getting very good results,", "tokens": [51304, 407, 341, 575, 1096, 257, 869, 1691, 295, 300, 11, 293, 321, 434, 1242, 588, 665, 3542, 11, 51492], "temperature": 0.0, "avg_logprob": -0.2941262626647949, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.0616131476126611e-05}, {"id": 1391, "seek": 636554, "start": 6388.1, "end": 6392.82, "text": " despite using only three epochs of very large batch size.", "tokens": [51492, 7228, 1228, 787, 1045, 30992, 28346, 295, 588, 2416, 15245, 2744, 13, 51728], "temperature": 0.0, "avg_logprob": -0.2941262626647949, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.0616131476126611e-05}, {"id": 1392, "seek": 639282, "start": 6393.82, "end": 6395.5, "text": " Okay, so that's called momentum.", "tokens": [50414, 1033, 11, 370, 300, 311, 1219, 11244, 13, 50498], "temperature": 0.0, "avg_logprob": -0.4099357884104659, "compression_ratio": 1.4454976303317535, "no_speech_prob": 7.338209115914651e-07}, {"id": 1393, "seek": 639282, "start": 6396.82, "end": 6402.82, "text": " Now something that was announced in a Coursera course back in maybe 2012,", "tokens": [50564, 823, 746, 300, 390, 7548, 294, 257, 383, 5067, 1663, 1164, 646, 294, 1310, 9125, 11, 50864], "temperature": 0.0, "avg_logprob": -0.4099357884104659, "compression_ratio": 1.4454976303317535, "no_speech_prob": 7.338209115914651e-07}, {"id": 1394, "seek": 639282, "start": 6402.82, "end": 6408.34, "text": " 2013 by Jeffrey Hinton, and it's never been published, is called rmsprop.", "tokens": [50864, 9012, 538, 28721, 389, 12442, 11, 293, 309, 311, 1128, 668, 6572, 11, 307, 1219, 367, 2592, 79, 1513, 13, 51140], "temperature": 0.0, "avg_logprob": -0.4099357884104659, "compression_ratio": 1.4454976303317535, "no_speech_prob": 7.338209115914651e-07}, {"id": 1395, "seek": 639282, "start": 6408.34, "end": 6411.82, "text": " I'll just have it running while we talk about it.", "tokens": [51140, 286, 603, 445, 362, 309, 2614, 1339, 321, 751, 466, 309, 13, 51314], "temperature": 0.0, "avg_logprob": -0.4099357884104659, "compression_ratio": 1.4454976303317535, "no_speech_prob": 7.338209115914651e-07}, {"id": 1396, "seek": 639282, "start": 6411.82, "end": 6417.86, "text": " rmsprop is gonna update the optimization step using something very similar", "tokens": [51314, 367, 2592, 79, 1513, 307, 799, 5623, 264, 19618, 1823, 1228, 746, 588, 2531, 51616], "temperature": 0.0, "avg_logprob": -0.4099357884104659, "compression_ratio": 1.4454976303317535, "no_speech_prob": 7.338209115914651e-07}, {"id": 1397, "seek": 641786, "start": 6417.9, "end": 6424.58, "text": " to momentum, but rather than lerping on the p.grad,", "tokens": [50366, 281, 11244, 11, 457, 2831, 813, 32068, 3381, 322, 264, 280, 13, 7165, 11, 50700], "temperature": 0.0, "avg_logprob": -0.3591680037669646, "compression_ratio": 1.6, "no_speech_prob": 5.014734142605448e-06}, {"id": 1398, "seek": 641786, "start": 6424.58, "end": 6433.219999999999, "text": " we're going to lerp on p.grad squared.", "tokens": [50700, 321, 434, 516, 281, 32068, 79, 322, 280, 13, 7165, 8889, 13, 51132], "temperature": 0.0, "avg_logprob": -0.3591680037669646, "compression_ratio": 1.6, "no_speech_prob": 5.014734142605448e-06}, {"id": 1399, "seek": 641786, "start": 6435.42, "end": 6440.38, "text": " And just to keep it kind of consistent, we won't call it mom,", "tokens": [51242, 400, 445, 281, 1066, 309, 733, 295, 8398, 11, 321, 1582, 380, 818, 309, 1225, 11, 51490], "temperature": 0.0, "avg_logprob": -0.3591680037669646, "compression_ratio": 1.6, "no_speech_prob": 5.014734142605448e-06}, {"id": 1400, "seek": 641786, "start": 6440.38, "end": 6443.82, "text": " we'll call it square mom, but this is just the multiplier.", "tokens": [51490, 321, 603, 818, 309, 3732, 1225, 11, 457, 341, 307, 445, 264, 44106, 13, 51662], "temperature": 0.0, "avg_logprob": -0.3591680037669646, "compression_ratio": 1.6, "no_speech_prob": 5.014734142605448e-06}, {"id": 1401, "seek": 641786, "start": 6443.82, "end": 6446.299999999999, "text": " And what are we doing with the grad squared?", "tokens": [51662, 400, 437, 366, 321, 884, 365, 264, 2771, 8889, 30, 51786], "temperature": 0.0, "avg_logprob": -0.3591680037669646, "compression_ratio": 1.6, "no_speech_prob": 5.014734142605448e-06}, {"id": 1402, "seek": 644630, "start": 6446.34, "end": 6450.62, "text": " Well, the idea is that a large grad squared", "tokens": [50366, 1042, 11, 264, 1558, 307, 300, 257, 2416, 2771, 8889, 50580], "temperature": 0.0, "avg_logprob": -0.23633611654933495, "compression_ratio": 1.5396825396825398, "no_speech_prob": 1.255420443158073e-07}, {"id": 1403, "seek": 644630, "start": 6450.62, "end": 6455.66, "text": " indicates a large variance of gradients.", "tokens": [50580, 16203, 257, 2416, 21977, 295, 2771, 2448, 13, 50832], "temperature": 0.0, "avg_logprob": -0.23633611654933495, "compression_ratio": 1.5396825396825398, "no_speech_prob": 1.255420443158073e-07}, {"id": 1404, "seek": 644630, "start": 6455.66, "end": 6464.58, "text": " So what we're then gonna do is divide by the square root of that plus epsilon.", "tokens": [50832, 407, 437, 321, 434, 550, 799, 360, 307, 9845, 538, 264, 3732, 5593, 295, 300, 1804, 17889, 13, 51278], "temperature": 0.0, "avg_logprob": -0.23633611654933495, "compression_ratio": 1.5396825396825398, "no_speech_prob": 1.255420443158073e-07}, {"id": 1405, "seek": 644630, "start": 6464.58, "end": 6467.820000000001, "text": " Now, you'll see I've actually been a bit all over the place here.", "tokens": [51278, 823, 11, 291, 603, 536, 286, 600, 767, 668, 257, 857, 439, 670, 264, 1081, 510, 13, 51440], "temperature": 0.0, "avg_logprob": -0.23633611654933495, "compression_ratio": 1.5396825396825398, "no_speech_prob": 1.255420443158073e-07}, {"id": 1406, "seek": 644630, "start": 6467.820000000001, "end": 6473.34, "text": " With my batch norm, I put the epsilon inside the square root.", "tokens": [51440, 2022, 452, 15245, 2026, 11, 286, 829, 264, 17889, 1854, 264, 3732, 5593, 13, 51716], "temperature": 0.0, "avg_logprob": -0.23633611654933495, "compression_ratio": 1.5396825396825398, "no_speech_prob": 1.255420443158073e-07}, {"id": 1407, "seek": 647334, "start": 6473.34, "end": 6477.26, "text": " In this case, I'm putting the epsilon outside the square root.", "tokens": [50364, 682, 341, 1389, 11, 286, 478, 3372, 264, 17889, 2380, 264, 3732, 5593, 13, 50560], "temperature": 0.0, "avg_logprob": -0.318724734111897, "compression_ratio": 1.5725190839694656, "no_speech_prob": 5.53917118395475e-07}, {"id": 1408, "seek": 647334, "start": 6477.26, "end": 6479.66, "text": " It does make a difference, and so", "tokens": [50560, 467, 775, 652, 257, 2649, 11, 293, 370, 50680], "temperature": 0.0, "avg_logprob": -0.318724734111897, "compression_ratio": 1.5725190839694656, "no_speech_prob": 5.53917118395475e-07}, {"id": 1409, "seek": 647334, "start": 6479.66, "end": 6484.06, "text": " be careful as to how your epsilon is being interpreted.", "tokens": [50680, 312, 5026, 382, 281, 577, 428, 17889, 307, 885, 26749, 13, 50900], "temperature": 0.0, "avg_logprob": -0.318724734111897, "compression_ratio": 1.5725190839694656, "no_speech_prob": 5.53917118395475e-07}, {"id": 1410, "seek": 647334, "start": 6485.34, "end": 6487.5, "text": " Generally speaking, I can't remember if I've been exactly right, but", "tokens": [50964, 21082, 4124, 11, 286, 393, 380, 1604, 498, 286, 600, 668, 2293, 558, 11, 457, 51072], "temperature": 0.0, "avg_logprob": -0.318724734111897, "compression_ratio": 1.5725190839694656, "no_speech_prob": 5.53917118395475e-07}, {"id": 1411, "seek": 647334, "start": 6487.5, "end": 6491.74, "text": " I've tried to be consistent with the papers or normal implementations.", "tokens": [51072, 286, 600, 3031, 281, 312, 8398, 365, 264, 10577, 420, 2710, 4445, 763, 13, 51284], "temperature": 0.0, "avg_logprob": -0.318724734111897, "compression_ratio": 1.5725190839694656, "no_speech_prob": 5.53917118395475e-07}, {"id": 1412, "seek": 647334, "start": 6491.74, "end": 6495.78, "text": " This is a very common cause of confusion and errors, though.", "tokens": [51284, 639, 307, 257, 588, 2689, 3082, 295, 15075, 293, 13603, 11, 1673, 13, 51486], "temperature": 0.0, "avg_logprob": -0.318724734111897, "compression_ratio": 1.5725190839694656, "no_speech_prob": 5.53917118395475e-07}, {"id": 1413, "seek": 647334, "start": 6497.9800000000005, "end": 6501.9400000000005, "text": " So what we're doing here is we're dividing the gradient by", "tokens": [51596, 407, 437, 321, 434, 884, 510, 307, 321, 434, 26764, 264, 16235, 538, 51794], "temperature": 0.0, "avg_logprob": -0.318724734111897, "compression_ratio": 1.5725190839694656, "no_speech_prob": 5.53917118395475e-07}, {"id": 1414, "seek": 650334, "start": 6503.54, "end": 6506.46, "text": " the amount of variation, so", "tokens": [50374, 264, 2372, 295, 12990, 11, 370, 50520], "temperature": 0.0, "avg_logprob": -0.29100806214088615, "compression_ratio": 1.6321243523316062, "no_speech_prob": 5.594337835646002e-06}, {"id": 1415, "seek": 650334, "start": 6506.46, "end": 6511.34, "text": " the square root of the moving average of gradient squared.", "tokens": [50520, 264, 3732, 5593, 295, 264, 2684, 4274, 295, 16235, 8889, 13, 50764], "temperature": 0.0, "avg_logprob": -0.29100806214088615, "compression_ratio": 1.6321243523316062, "no_speech_prob": 5.594337835646002e-06}, {"id": 1416, "seek": 650334, "start": 6511.34, "end": 6518.5, "text": " And so the idea here is that if the gradient has been moving around all over", "tokens": [50764, 400, 370, 264, 1558, 510, 307, 300, 498, 264, 16235, 575, 668, 2684, 926, 439, 670, 51122], "temperature": 0.0, "avg_logprob": -0.29100806214088615, "compression_ratio": 1.6321243523316062, "no_speech_prob": 5.594337835646002e-06}, {"id": 1417, "seek": 650334, "start": 6518.5, "end": 6522.38, "text": " the place, then we don't really know what it is, right?", "tokens": [51122, 264, 1081, 11, 550, 321, 500, 380, 534, 458, 437, 309, 307, 11, 558, 30, 51316], "temperature": 0.0, "avg_logprob": -0.29100806214088615, "compression_ratio": 1.6321243523316062, "no_speech_prob": 5.594337835646002e-06}, {"id": 1418, "seek": 650334, "start": 6522.38, "end": 6524.58, "text": " So we shouldn't do a very big update.", "tokens": [51316, 407, 321, 4659, 380, 360, 257, 588, 955, 5623, 13, 51426], "temperature": 0.0, "avg_logprob": -0.29100806214088615, "compression_ratio": 1.6321243523316062, "no_speech_prob": 5.594337835646002e-06}, {"id": 1419, "seek": 650334, "start": 6526.66, "end": 6532.66, "text": " If the gradient is very, very much the same all the time,", "tokens": [51530, 759, 264, 16235, 307, 588, 11, 588, 709, 264, 912, 439, 264, 565, 11, 51830], "temperature": 0.0, "avg_logprob": -0.29100806214088615, "compression_ratio": 1.6321243523316062, "no_speech_prob": 5.594337835646002e-06}, {"id": 1420, "seek": 653266, "start": 6532.66, "end": 6535.94, "text": " then we're very confident about it, so we do want to be a big update.", "tokens": [50364, 550, 321, 434, 588, 6679, 466, 309, 11, 370, 321, 360, 528, 281, 312, 257, 955, 5623, 13, 50528], "temperature": 0.0, "avg_logprob": -0.3107205372230679, "compression_ratio": 1.5665236051502145, "no_speech_prob": 2.078516990877688e-05}, {"id": 1421, "seek": 653266, "start": 6535.94, "end": 6537.82, "text": " I have no idea why we're doing this in two steps.", "tokens": [50528, 286, 362, 572, 1558, 983, 321, 434, 884, 341, 294, 732, 4439, 13, 50622], "temperature": 0.0, "avg_logprob": -0.3107205372230679, "compression_ratio": 1.5665236051502145, "no_speech_prob": 2.078516990877688e-05}, {"id": 1422, "seek": 653266, "start": 6537.82, "end": 6540.42, "text": " Let's just pop this over here.", "tokens": [50622, 961, 311, 445, 1665, 341, 670, 510, 13, 50752], "temperature": 0.0, "avg_logprob": -0.3107205372230679, "compression_ratio": 1.5665236051502145, "no_speech_prob": 2.078516990877688e-05}, {"id": 1423, "seek": 653266, "start": 6544.78, "end": 6551.62, "text": " Now, because we are dividing our gradient by this generally,", "tokens": [50970, 823, 11, 570, 321, 366, 26764, 527, 16235, 538, 341, 5101, 11, 51312], "temperature": 0.0, "avg_logprob": -0.3107205372230679, "compression_ratio": 1.5665236051502145, "no_speech_prob": 2.078516990877688e-05}, {"id": 1424, "seek": 653266, "start": 6551.62, "end": 6555.98, "text": " possibly rather small number, we generally have to decrease the learning rate.", "tokens": [51312, 6264, 2831, 1359, 1230, 11, 321, 5101, 362, 281, 11514, 264, 2539, 3314, 13, 51530], "temperature": 0.0, "avg_logprob": -0.3107205372230679, "compression_ratio": 1.5665236051502145, "no_speech_prob": 2.078516990877688e-05}, {"id": 1425, "seek": 653266, "start": 6555.98, "end": 6558.74, "text": " So we bring the learning rate back to 0.01.", "tokens": [51530, 407, 321, 1565, 264, 2539, 3314, 646, 281, 1958, 13, 10607, 13, 51668], "temperature": 0.0, "avg_logprob": -0.3107205372230679, "compression_ratio": 1.5665236051502145, "no_speech_prob": 2.078516990877688e-05}, {"id": 1426, "seek": 653266, "start": 6558.74, "end": 6561.66, "text": " And as you see, it's training.", "tokens": [51668, 400, 382, 291, 536, 11, 309, 311, 3097, 13, 51814], "temperature": 0.0, "avg_logprob": -0.3107205372230679, "compression_ratio": 1.5665236051502145, "no_speech_prob": 2.078516990877688e-05}, {"id": 1427, "seek": 656166, "start": 6561.66, "end": 6563.22, "text": " It's not amazing, but it's training okay.", "tokens": [50364, 467, 311, 406, 2243, 11, 457, 309, 311, 3097, 1392, 13, 50442], "temperature": 0.0, "avg_logprob": -0.45966705909142125, "compression_ratio": 1.3028169014084507, "no_speech_prob": 7.25434219930321e-05}, {"id": 1428, "seek": 656166, "start": 6567.0599999999995, "end": 6571.46, "text": " So rmsprop can be quite nice.", "tokens": [50634, 407, 367, 2592, 79, 1513, 393, 312, 1596, 1481, 13, 50854], "temperature": 0.0, "avg_logprob": -0.45966705909142125, "compression_ratio": 1.3028169014084507, "no_speech_prob": 7.25434219930321e-05}, {"id": 1429, "seek": 656166, "start": 6573.94, "end": 6575.139999999999, "text": " It's a bit bumpy there, isn't it?", "tokens": [50978, 467, 311, 257, 857, 49400, 456, 11, 1943, 380, 309, 30, 51038], "temperature": 0.0, "avg_logprob": -0.45966705909142125, "compression_ratio": 1.3028169014084507, "no_speech_prob": 7.25434219930321e-05}, {"id": 1430, "seek": 656166, "start": 6575.139999999999, "end": 6577.18, "text": " I mean, I could try decreasing it a little bit,", "tokens": [51038, 286, 914, 11, 286, 727, 853, 23223, 309, 257, 707, 857, 11, 51140], "temperature": 0.0, "avg_logprob": -0.45966705909142125, "compression_ratio": 1.3028169014084507, "no_speech_prob": 7.25434219930321e-05}, {"id": 1431, "seek": 656166, "start": 6577.18, "end": 6581.34, "text": " maybe down to 3e neg 3 instead.", "tokens": [51140, 1310, 760, 281, 805, 68, 2485, 805, 2602, 13, 51348], "temperature": 0.0, "avg_logprob": -0.45966705909142125, "compression_ratio": 1.3028169014084507, "no_speech_prob": 7.25434219930321e-05}, {"id": 1432, "seek": 658134, "start": 6582.34, "end": 6590.34, "text": " That's a little bit better and a bit smoother, so that's probably good.", "tokens": [50414, 663, 311, 257, 707, 857, 1101, 293, 257, 857, 28640, 11, 370, 300, 311, 1391, 665, 13, 50814], "temperature": 0.0, "avg_logprob": -0.43913158617521586, "compression_ratio": 1.3453237410071943, "no_speech_prob": 0.00010071269207401201}, {"id": 1433, "seek": 658134, "start": 6598.34, "end": 6602.18, "text": " Let's see what the colorful dimension plot looks like too, shall we?", "tokens": [51214, 961, 311, 536, 437, 264, 18506, 10139, 7542, 1542, 411, 886, 11, 4393, 321, 30, 51406], "temperature": 0.0, "avg_logprob": -0.43913158617521586, "compression_ratio": 1.3453237410071943, "no_speech_prob": 0.00010071269207401201}, {"id": 1434, "seek": 658134, "start": 6603.38, "end": 6604.900000000001, "text": " Again, it's very nice, isn't it?", "tokens": [51466, 3764, 11, 309, 311, 588, 1481, 11, 1943, 380, 309, 30, 51542], "temperature": 0.0, "avg_logprob": -0.43913158617521586, "compression_ratio": 1.3453237410071943, "no_speech_prob": 0.00010071269207401201}, {"id": 1435, "seek": 658134, "start": 6604.900000000001, "end": 6605.42, "text": " That's great.", "tokens": [51542, 663, 311, 869, 13, 51568], "temperature": 0.0, "avg_logprob": -0.43913158617521586, "compression_ratio": 1.3453237410071943, "no_speech_prob": 0.00010071269207401201}, {"id": 1436, "seek": 660542, "start": 6606.42, "end": 6611.02, "text": " Now, one thing I did, which I don't think I've seen done before,", "tokens": [50414, 823, 11, 472, 551, 286, 630, 11, 597, 286, 500, 380, 519, 286, 600, 1612, 1096, 949, 11, 50644], "temperature": 0.0, "avg_logprob": -0.29350698112261175, "compression_ratio": 1.662162162162162, "no_speech_prob": 2.058048721664818e-06}, {"id": 1437, "seek": 660542, "start": 6611.02, "end": 6613.9800000000005, "text": " I don't remember people talking about,", "tokens": [50644, 286, 500, 380, 1604, 561, 1417, 466, 11, 50792], "temperature": 0.0, "avg_logprob": -0.29350698112261175, "compression_ratio": 1.662162162162162, "no_speech_prob": 2.058048721664818e-06}, {"id": 1438, "seek": 660542, "start": 6613.9800000000005, "end": 6621.02, "text": " is I actually decided not to do the normal thing of initializing to 0s.", "tokens": [50792, 307, 286, 767, 3047, 406, 281, 360, 264, 2710, 551, 295, 5883, 3319, 281, 1958, 82, 13, 51144], "temperature": 0.0, "avg_logprob": -0.29350698112261175, "compression_ratio": 1.662162162162162, "no_speech_prob": 2.058048721664818e-06}, {"id": 1439, "seek": 660542, "start": 6621.02, "end": 6623.14, "text": " Because if I initialize to 0s,", "tokens": [51144, 1436, 498, 286, 5883, 1125, 281, 1958, 82, 11, 51250], "temperature": 0.0, "avg_logprob": -0.29350698112261175, "compression_ratio": 1.662162162162162, "no_speech_prob": 2.058048721664818e-06}, {"id": 1440, "seek": 660542, "start": 6623.14, "end": 6629.74, "text": " then my initial denominator here will basically be 0 plus epsilon,", "tokens": [51250, 550, 452, 5883, 20687, 510, 486, 1936, 312, 1958, 1804, 17889, 11, 51580], "temperature": 0.0, "avg_logprob": -0.29350698112261175, "compression_ratio": 1.662162162162162, "no_speech_prob": 2.058048721664818e-06}, {"id": 1441, "seek": 660542, "start": 6629.74, "end": 6632.82, "text": " which will mean my initial learning rate will be very, very high,", "tokens": [51580, 597, 486, 914, 452, 5883, 2539, 3314, 486, 312, 588, 11, 588, 1090, 11, 51734], "temperature": 0.0, "avg_logprob": -0.29350698112261175, "compression_ratio": 1.662162162162162, "no_speech_prob": 2.058048721664818e-06}, {"id": 1442, "seek": 660542, "start": 6632.82, "end": 6634.5, "text": " which I certainly don't want.", "tokens": [51734, 597, 286, 3297, 500, 380, 528, 13, 51818], "temperature": 0.0, "avg_logprob": -0.29350698112261175, "compression_ratio": 1.662162162162162, "no_speech_prob": 2.058048721664818e-06}, {"id": 1443, "seek": 663450, "start": 6634.5, "end": 6638.06, "text": " So I actually initialized it at first to just whatever the first", "tokens": [50364, 407, 286, 767, 5883, 1602, 309, 412, 700, 281, 445, 2035, 264, 700, 50542], "temperature": 0.0, "avg_logprob": -0.3185545424340476, "compression_ratio": 1.4627659574468086, "no_speech_prob": 1.2289178812352475e-06}, {"id": 1444, "seek": 663450, "start": 6638.06, "end": 6641.46, "text": " many batches gradient is, squared.", "tokens": [50542, 867, 15245, 279, 16235, 307, 11, 8889, 13, 50712], "temperature": 0.0, "avg_logprob": -0.3185545424340476, "compression_ratio": 1.4627659574468086, "no_speech_prob": 1.2289178812352475e-06}, {"id": 1445, "seek": 663450, "start": 6641.46, "end": 6646.26, "text": " And I think this is a really useful little trick for using rmsprop.", "tokens": [50712, 400, 286, 519, 341, 307, 257, 534, 4420, 707, 4282, 337, 1228, 367, 2592, 79, 1513, 13, 50952], "temperature": 0.0, "avg_logprob": -0.3185545424340476, "compression_ratio": 1.4627659574468086, "no_speech_prob": 1.2289178812352475e-06}, {"id": 1446, "seek": 663450, "start": 6649.86, "end": 6655.46, "text": " Momentum can be a bit aggressive sometimes for", "tokens": [51132, 19093, 449, 393, 312, 257, 857, 10762, 2171, 337, 51412], "temperature": 0.0, "avg_logprob": -0.3185545424340476, "compression_ratio": 1.4627659574468086, "no_speech_prob": 1.2289178812352475e-06}, {"id": 1447, "seek": 663450, "start": 6655.46, "end": 6663.74, "text": " some really finicky learning methods, finicky architectures.", "tokens": [51412, 512, 534, 962, 20539, 2539, 7150, 11, 962, 20539, 6331, 1303, 13, 51826], "temperature": 0.0, "avg_logprob": -0.3185545424340476, "compression_ratio": 1.4627659574468086, "no_speech_prob": 1.2289178812352475e-06}, {"id": 1448, "seek": 666374, "start": 6663.74, "end": 6669.58, "text": " And so rmsprop can be a good way to get reasonably fast", "tokens": [50364, 400, 370, 367, 2592, 79, 1513, 393, 312, 257, 665, 636, 281, 483, 23551, 2370, 50656], "temperature": 0.0, "avg_logprob": -0.27036732760342685, "compression_ratio": 1.547244094488189, "no_speech_prob": 6.439037861127872e-06}, {"id": 1449, "seek": 666374, "start": 6669.58, "end": 6672.82, "text": " optimization of a very finicky architectures.", "tokens": [50656, 19618, 295, 257, 588, 962, 20539, 6331, 1303, 13, 50818], "temperature": 0.0, "avg_logprob": -0.27036732760342685, "compression_ratio": 1.547244094488189, "no_speech_prob": 6.439037861127872e-06}, {"id": 1450, "seek": 666374, "start": 6672.82, "end": 6677.179999999999, "text": " And in particular, EfficientNet is an architecture which people", "tokens": [50818, 400, 294, 1729, 11, 462, 7816, 31890, 307, 364, 9482, 597, 561, 51036], "temperature": 0.0, "avg_logprob": -0.27036732760342685, "compression_ratio": 1.547244094488189, "no_speech_prob": 6.439037861127872e-06}, {"id": 1451, "seek": 666374, "start": 6677.179999999999, "end": 6679.54, "text": " have generally trained best with rmsprop.", "tokens": [51036, 362, 5101, 8895, 1151, 365, 367, 2592, 79, 1513, 13, 51154], "temperature": 0.0, "avg_logprob": -0.27036732760342685, "compression_ratio": 1.547244094488189, "no_speech_prob": 6.439037861127872e-06}, {"id": 1452, "seek": 666374, "start": 6680.82, "end": 6685.98, "text": " So you don't see it a whole lot, but in some ways it's just historical interest,", "tokens": [51218, 407, 291, 500, 380, 536, 309, 257, 1379, 688, 11, 457, 294, 512, 2098, 309, 311, 445, 8584, 1179, 11, 51476], "temperature": 0.0, "avg_logprob": -0.27036732760342685, "compression_ratio": 1.547244094488189, "no_speech_prob": 6.439037861127872e-06}, {"id": 1453, "seek": 666374, "start": 6685.98, "end": 6687.46, "text": " but you see it a bit.", "tokens": [51476, 457, 291, 536, 309, 257, 857, 13, 51550], "temperature": 0.0, "avg_logprob": -0.27036732760342685, "compression_ratio": 1.547244094488189, "no_speech_prob": 6.439037861127872e-06}, {"id": 1454, "seek": 666374, "start": 6687.46, "end": 6693.54, "text": " But I mean, the thing we really want to look at is rmsprop plus momentum together.", "tokens": [51550, 583, 286, 914, 11, 264, 551, 321, 534, 528, 281, 574, 412, 307, 367, 2592, 79, 1513, 1804, 11244, 1214, 13, 51854], "temperature": 0.0, "avg_logprob": -0.27036732760342685, "compression_ratio": 1.547244094488189, "no_speech_prob": 6.439037861127872e-06}, {"id": 1455, "seek": 669354, "start": 6694.34, "end": 6697.22, "text": " And rmsprop plus momentum together exists, it has a name.", "tokens": [50404, 400, 367, 2592, 79, 1513, 1804, 11244, 1214, 8198, 11, 309, 575, 257, 1315, 13, 50548], "temperature": 0.0, "avg_logprob": -0.3400933401925223, "compression_ratio": 1.721951219512195, "no_speech_prob": 3.9669853322266135e-06}, {"id": 1456, "seek": 669354, "start": 6697.22, "end": 6700.26, "text": " You all have heard the name many times, name is Adam.", "tokens": [50548, 509, 439, 362, 2198, 264, 1315, 867, 1413, 11, 1315, 307, 7938, 13, 50700], "temperature": 0.0, "avg_logprob": -0.3400933401925223, "compression_ratio": 1.721951219512195, "no_speech_prob": 3.9669853322266135e-06}, {"id": 1457, "seek": 669354, "start": 6700.26, "end": 6705.42, "text": " Adam is literally just rmsprop and momentum.", "tokens": [50700, 7938, 307, 3736, 445, 367, 2592, 79, 1513, 293, 11244, 13, 50958], "temperature": 0.0, "avg_logprob": -0.3400933401925223, "compression_ratio": 1.721951219512195, "no_speech_prob": 3.9669853322266135e-06}, {"id": 1458, "seek": 669354, "start": 6705.42, "end": 6710.98, "text": " So we rather annoyingly call them beta1 and beta2.", "tokens": [50958, 407, 321, 2831, 11304, 356, 818, 552, 9861, 16, 293, 9861, 17, 13, 51236], "temperature": 0.0, "avg_logprob": -0.3400933401925223, "compression_ratio": 1.721951219512195, "no_speech_prob": 3.9669853322266135e-06}, {"id": 1459, "seek": 669354, "start": 6710.98, "end": 6715.42, "text": " They should be called momentum and square momentum, or", "tokens": [51236, 814, 820, 312, 1219, 11244, 293, 3732, 11244, 11, 420, 51458], "temperature": 0.0, "avg_logprob": -0.3400933401925223, "compression_ratio": 1.721951219512195, "no_speech_prob": 3.9669853322266135e-06}, {"id": 1460, "seek": 669354, "start": 6715.42, "end": 6717.9, "text": " momentum of squares, I suppose.", "tokens": [51458, 11244, 295, 19368, 11, 286, 7297, 13, 51582], "temperature": 0.0, "avg_logprob": -0.3400933401925223, "compression_ratio": 1.721951219512195, "no_speech_prob": 3.9669853322266135e-06}, {"id": 1461, "seek": 669354, "start": 6717.9, "end": 6721.78, "text": " So beta1 is just the momentum from the momentum optimizer.", "tokens": [51582, 407, 9861, 16, 307, 445, 264, 11244, 490, 264, 11244, 5028, 6545, 13, 51776], "temperature": 0.0, "avg_logprob": -0.3400933401925223, "compression_ratio": 1.721951219512195, "no_speech_prob": 3.9669853322266135e-06}, {"id": 1462, "seek": 672178, "start": 6721.78, "end": 6728.0599999999995, "text": " And beta2 is just the momentum for the squares from the rmsprop optimizer.", "tokens": [50364, 400, 9861, 17, 307, 445, 264, 11244, 337, 264, 19368, 490, 264, 367, 2592, 79, 1513, 5028, 6545, 13, 50678], "temperature": 0.0, "avg_logprob": -0.34804415702819824, "compression_ratio": 1.5647058823529412, "no_speech_prob": 1.1911082538063056e-06}, {"id": 1463, "seek": 672178, "start": 6729.7, "end": 6734.099999999999, "text": " So we'll store those away, and just like rmsprop, we need the epsilon.", "tokens": [50760, 407, 321, 603, 3531, 729, 1314, 11, 293, 445, 411, 367, 2592, 79, 1513, 11, 321, 643, 264, 17889, 13, 50980], "temperature": 0.0, "avg_logprob": -0.34804415702819824, "compression_ratio": 1.5647058823529412, "no_speech_prob": 1.1911082538063056e-06}, {"id": 1464, "seek": 672178, "start": 6736.66, "end": 6741.5, "text": " So I'm going to, as before, store away the gradient average and", "tokens": [51108, 407, 286, 478, 516, 281, 11, 382, 949, 11, 3531, 1314, 264, 16235, 4274, 293, 51350], "temperature": 0.0, "avg_logprob": -0.34804415702819824, "compression_ratio": 1.5647058823529412, "no_speech_prob": 1.1911082538063056e-06}, {"id": 1465, "seek": 672178, "start": 6741.5, "end": 6742.58, "text": " the square average.", "tokens": [51350, 264, 3732, 4274, 13, 51404], "temperature": 0.0, "avg_logprob": -0.34804415702819824, "compression_ratio": 1.5647058823529412, "no_speech_prob": 1.1911082538063056e-06}, {"id": 1466, "seek": 672178, "start": 6744.259999999999, "end": 6746.98, "text": " And then we're gonna do our lerping.", "tokens": [51488, 400, 550, 321, 434, 799, 360, 527, 32068, 3381, 13, 51624], "temperature": 0.0, "avg_logprob": -0.34804415702819824, "compression_ratio": 1.5647058823529412, "no_speech_prob": 1.1911082538063056e-06}, {"id": 1467, "seek": 674698, "start": 6747.94, "end": 6753.62, "text": " But there's a nice little trick here, which is in order to avoid doing this", "tokens": [50412, 583, 456, 311, 257, 1481, 707, 4282, 510, 11, 597, 307, 294, 1668, 281, 5042, 884, 341, 50696], "temperature": 0.0, "avg_logprob": -0.3502255972329672, "compression_ratio": 1.598901098901099, "no_speech_prob": 2.1568159809248755e-06}, {"id": 1468, "seek": 674698, "start": 6753.62, "end": 6761.5, "text": " where we just put the initial batch gradients as our starting values.", "tokens": [50696, 689, 321, 445, 829, 264, 5883, 15245, 2771, 2448, 382, 527, 2891, 4190, 13, 51090], "temperature": 0.0, "avg_logprob": -0.3502255972329672, "compression_ratio": 1.598901098901099, "no_speech_prob": 2.1568159809248755e-06}, {"id": 1469, "seek": 674698, "start": 6761.5, "end": 6765.9, "text": " We're gonna use zeros as our starting values, and", "tokens": [51090, 492, 434, 799, 764, 35193, 382, 527, 2891, 4190, 11, 293, 51310], "temperature": 0.0, "avg_logprob": -0.3502255972329672, "compression_ratio": 1.598901098901099, "no_speech_prob": 2.1568159809248755e-06}, {"id": 1470, "seek": 674698, "start": 6765.9, "end": 6768.58, "text": " then we're going to unbiased them.", "tokens": [51310, 550, 321, 434, 516, 281, 517, 5614, 1937, 552, 13, 51444], "temperature": 0.0, "avg_logprob": -0.3502255972329672, "compression_ratio": 1.598901098901099, "no_speech_prob": 2.1568159809248755e-06}, {"id": 1471, "seek": 674698, "start": 6768.58, "end": 6774.219999999999, "text": " So basically the idea is that for the very first mini batch,", "tokens": [51444, 407, 1936, 264, 1558, 307, 300, 337, 264, 588, 700, 8382, 15245, 11, 51726], "temperature": 0.0, "avg_logprob": -0.3502255972329672, "compression_ratio": 1.598901098901099, "no_speech_prob": 2.1568159809248755e-06}, {"id": 1472, "seek": 677422, "start": 6774.740000000001, "end": 6778.58, "text": " Here, being lerped with the gradient,", "tokens": [50390, 1692, 11, 885, 32068, 3452, 365, 264, 16235, 11, 50582], "temperature": 0.0, "avg_logprob": -0.35229143729576695, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.15736440118053e-06}, {"id": 1473, "seek": 677422, "start": 6778.58, "end": 6785.42, "text": " then the first mini batch will obviously be closer to zero than it should be.", "tokens": [50582, 550, 264, 700, 8382, 15245, 486, 2745, 312, 4966, 281, 4018, 813, 309, 820, 312, 13, 50924], "temperature": 0.0, "avg_logprob": -0.35229143729576695, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.15736440118053e-06}, {"id": 1474, "seek": 677422, "start": 6785.42, "end": 6790.5, "text": " But we know exactly how much closer it should be to zero,", "tokens": [50924, 583, 321, 458, 2293, 577, 709, 4966, 309, 820, 312, 281, 4018, 11, 51178], "temperature": 0.0, "avg_logprob": -0.35229143729576695, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.15736440118053e-06}, {"id": 1475, "seek": 677422, "start": 6790.5, "end": 6795.26, "text": " which is just, it's gonna be self.beta1 times closer,", "tokens": [51178, 597, 307, 445, 11, 309, 311, 799, 312, 2698, 13, 65, 7664, 16, 1413, 4966, 11, 51416], "temperature": 0.0, "avg_logprob": -0.35229143729576695, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.15736440118053e-06}, {"id": 1476, "seek": 677422, "start": 6795.26, "end": 6799.9800000000005, "text": " at least in the first mini batch, cuz that's what we've lerped with.", "tokens": [51416, 412, 1935, 294, 264, 700, 8382, 15245, 11, 11910, 300, 311, 437, 321, 600, 32068, 3452, 365, 13, 51652], "temperature": 0.0, "avg_logprob": -0.35229143729576695, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.15736440118053e-06}, {"id": 1477, "seek": 677422, "start": 6799.9800000000005, "end": 6802.46, "text": " And then in the second mini batch, it'll be self.beta1 squared.", "tokens": [51652, 400, 550, 294, 264, 1150, 8382, 15245, 11, 309, 603, 312, 2698, 13, 65, 7664, 16, 8889, 13, 51776], "temperature": 0.0, "avg_logprob": -0.35229143729576695, "compression_ratio": 1.7142857142857142, "no_speech_prob": 4.15736440118053e-06}, {"id": 1478, "seek": 680246, "start": 6802.5, "end": 6806.9, "text": " And so in the third mini batch, it'll be self.beta1 cubed, and so forth.", "tokens": [50366, 400, 370, 294, 264, 2636, 8382, 15245, 11, 309, 603, 312, 2698, 13, 65, 7664, 16, 36510, 11, 293, 370, 5220, 13, 50586], "temperature": 0.0, "avg_logprob": -0.3468752640944261, "compression_ratio": 1.564516129032258, "no_speech_prob": 6.240931270440342e-06}, {"id": 1479, "seek": 680246, "start": 6806.9, "end": 6811.58, "text": " And that's why we had this self.i back in our SGD,", "tokens": [50586, 400, 300, 311, 983, 321, 632, 341, 2698, 13, 72, 646, 294, 527, 34520, 35, 11, 50820], "temperature": 0.0, "avg_logprob": -0.3468752640944261, "compression_ratio": 1.564516129032258, "no_speech_prob": 6.240931270440342e-06}, {"id": 1480, "seek": 680246, "start": 6813.94, "end": 6818.14, "text": " which was keeping track of what mini batch were up to.", "tokens": [50938, 597, 390, 5145, 2837, 295, 437, 8382, 15245, 645, 493, 281, 13, 51148], "temperature": 0.0, "avg_logprob": -0.3468752640944261, "compression_ratio": 1.564516129032258, "no_speech_prob": 6.240931270440342e-06}, {"id": 1481, "seek": 680246, "start": 6818.14, "end": 6824.9, "text": " So we need that in order to do this unbiasing of the average.", "tokens": [51148, 407, 321, 643, 300, 294, 1668, 281, 360, 341, 517, 5614, 3349, 295, 264, 4274, 13, 51486], "temperature": 0.0, "avg_logprob": -0.3468752640944261, "compression_ratio": 1.564516129032258, "no_speech_prob": 6.240931270440342e-06}, {"id": 1482, "seek": 680246, "start": 6827.9, "end": 6830.9800000000005, "text": " Dear, I'm not unbiasing the square of the average.", "tokens": [51636, 14383, 11, 286, 478, 406, 517, 5614, 3349, 264, 3732, 295, 264, 4274, 13, 51790], "temperature": 0.0, "avg_logprob": -0.3468752640944261, "compression_ratio": 1.564516129032258, "no_speech_prob": 6.240931270440342e-06}, {"id": 1483, "seek": 683246, "start": 6832.58, "end": 6833.08, "text": " Am I?", "tokens": [50370, 2012, 286, 30, 50395], "temperature": 0.0, "avg_logprob": -0.4426547459193638, "compression_ratio": 1.4658385093167703, "no_speech_prob": 9.972939551516902e-06}, {"id": 1484, "seek": 683246, "start": 6834.3, "end": 6835.02, "text": " No, I'm not.", "tokens": [50456, 883, 11, 286, 478, 406, 13, 50492], "temperature": 0.0, "avg_logprob": -0.4426547459193638, "compression_ratio": 1.4658385093167703, "no_speech_prob": 9.972939551516902e-06}, {"id": 1485, "seek": 683246, "start": 6836.26, "end": 6837.26, "text": " Whoops.", "tokens": [50554, 45263, 13, 50604], "temperature": 0.0, "avg_logprob": -0.4426547459193638, "compression_ratio": 1.4658385093167703, "no_speech_prob": 9.972939551516902e-06}, {"id": 1486, "seek": 683246, "start": 6837.26, "end": 6839.78, "text": " So we need to do that here as well.", "tokens": [50604, 407, 321, 643, 281, 360, 300, 510, 382, 731, 13, 50730], "temperature": 0.0, "avg_logprob": -0.4426547459193638, "compression_ratio": 1.4658385093167703, "no_speech_prob": 9.972939551516902e-06}, {"id": 1487, "seek": 683246, "start": 6839.78, "end": 6841.58, "text": " Wonder if this is gonna help things a little bit.", "tokens": [50730, 13224, 498, 341, 307, 799, 854, 721, 257, 707, 857, 13, 50820], "temperature": 0.0, "avg_logprob": -0.4426547459193638, "compression_ratio": 1.4658385093167703, "no_speech_prob": 9.972939551516902e-06}, {"id": 1488, "seek": 683246, "start": 6841.58, "end": 6851.06, "text": " Unbiased square average is going to be p.squareAverage.", "tokens": [50820, 1156, 5614, 1937, 3732, 4274, 307, 516, 281, 312, 280, 13, 33292, 543, 32, 3623, 13, 51294], "temperature": 0.0, "avg_logprob": -0.4426547459193638, "compression_ratio": 1.4658385093167703, "no_speech_prob": 9.972939551516902e-06}, {"id": 1489, "seek": 683246, "start": 6852.82, "end": 6854.62, "text": " And that will be beta2.", "tokens": [51382, 400, 300, 486, 312, 9861, 17, 13, 51472], "temperature": 0.0, "avg_logprob": -0.4426547459193638, "compression_ratio": 1.4658385093167703, "no_speech_prob": 9.972939551516902e-06}, {"id": 1490, "seek": 683246, "start": 6855.9800000000005, "end": 6859.7, "text": " And so we will use those unbiased versions.", "tokens": [51540, 400, 370, 321, 486, 764, 729, 517, 5614, 1937, 9606, 13, 51726], "temperature": 0.0, "avg_logprob": -0.4426547459193638, "compression_ratio": 1.4658385093167703, "no_speech_prob": 9.972939551516902e-06}, {"id": 1491, "seek": 685970, "start": 6859.7, "end": 6863.179999999999, "text": " So this unbiasing only matters for the first few mini batches,", "tokens": [50364, 407, 341, 517, 5614, 3349, 787, 7001, 337, 264, 700, 1326, 8382, 15245, 279, 11, 50538], "temperature": 0.0, "avg_logprob": -0.4033594255323534, "compression_ratio": 1.4603174603174602, "no_speech_prob": 3.905483481503325e-06}, {"id": 1492, "seek": 685970, "start": 6863.179999999999, "end": 6867.66, "text": " where otherwise it would be closer to zero than it should be.", "tokens": [50538, 689, 5911, 309, 576, 312, 4966, 281, 4018, 813, 309, 820, 312, 13, 50762], "temperature": 0.0, "avg_logprob": -0.4033594255323534, "compression_ratio": 1.4603174603174602, "no_speech_prob": 3.905483481503325e-06}, {"id": 1493, "seek": 685970, "start": 6871.9, "end": 6874.94, "text": " Right, so we'll run that.", "tokens": [50974, 1779, 11, 370, 321, 603, 1190, 300, 13, 51126], "temperature": 0.0, "avg_logprob": -0.4033594255323534, "compression_ratio": 1.4603174603174602, "no_speech_prob": 3.905483481503325e-06}, {"id": 1494, "seek": 685970, "start": 6879.66, "end": 6884.179999999999, "text": " And so again, you would expect the learning rate to be similar to what", "tokens": [51362, 400, 370, 797, 11, 291, 576, 2066, 264, 2539, 3314, 281, 312, 2531, 281, 437, 51588], "temperature": 0.0, "avg_logprob": -0.4033594255323534, "compression_ratio": 1.4603174603174602, "no_speech_prob": 3.905483481503325e-06}, {"id": 1495, "seek": 685970, "start": 6884.179999999999, "end": 6888.46, "text": " RMSProp needs, because we're doing that same division.", "tokens": [51588, 497, 10288, 47, 1513, 2203, 11, 570, 321, 434, 884, 300, 912, 10044, 13, 51802], "temperature": 0.0, "avg_logprob": -0.4033594255323534, "compression_ratio": 1.4603174603174602, "no_speech_prob": 3.905483481503325e-06}, {"id": 1496, "seek": 688846, "start": 6888.5, "end": 6891.26, "text": " So we actually do have the same learning rate here.", "tokens": [50366, 407, 321, 767, 360, 362, 264, 912, 2539, 3314, 510, 13, 50504], "temperature": 0.0, "avg_logprob": -0.3849495310842255, "compression_ratio": 1.3513513513513513, "no_speech_prob": 1.6280514500977006e-06}, {"id": 1497, "seek": 688846, "start": 6893.34, "end": 6898.06, "text": " And yeah, so we're up to 86.5% accuracy.", "tokens": [50608, 400, 1338, 11, 370, 321, 434, 493, 281, 26687, 13, 20, 4, 14170, 13, 50844], "temperature": 0.0, "avg_logprob": -0.3849495310842255, "compression_ratio": 1.3513513513513513, "no_speech_prob": 1.6280514500977006e-06}, {"id": 1498, "seek": 688846, "start": 6898.06, "end": 6900.02, "text": " So that's pretty good.", "tokens": [50844, 407, 300, 311, 1238, 665, 13, 50942], "temperature": 0.0, "avg_logprob": -0.3849495310842255, "compression_ratio": 1.3513513513513513, "no_speech_prob": 1.6280514500977006e-06}, {"id": 1499, "seek": 688846, "start": 6900.02, "end": 6906.1, "text": " I think, Yeah, it's actually a bit less good than momentum, which is fine.", "tokens": [50942, 286, 519, 11, 865, 11, 309, 311, 767, 257, 857, 1570, 665, 813, 11244, 11, 597, 307, 2489, 13, 51246], "temperature": 0.0, "avg_logprob": -0.3849495310842255, "compression_ratio": 1.3513513513513513, "no_speech_prob": 1.6280514500977006e-06}, {"id": 1500, "seek": 688846, "start": 6906.1, "end": 6908.38, "text": " Obviously, you can fiddle around.", "tokens": [51246, 7580, 11, 291, 393, 24553, 2285, 926, 13, 51360], "temperature": 0.0, "avg_logprob": -0.3849495310842255, "compression_ratio": 1.3513513513513513, "no_speech_prob": 1.6280514500977006e-06}, {"id": 1501, "seek": 688846, "start": 6909.54, "end": 6914.06, "text": " For momentum, we had 0.9.", "tokens": [51418, 1171, 11244, 11, 321, 632, 1958, 13, 24, 13, 51644], "temperature": 0.0, "avg_logprob": -0.3849495310842255, "compression_ratio": 1.3513513513513513, "no_speech_prob": 1.6280514500977006e-06}, {"id": 1502, "seek": 691406, "start": 6914.1, "end": 6917.860000000001, "text": " Yeah, so you could fiddle around with different values of beta2, beta1,", "tokens": [50366, 865, 11, 370, 291, 727, 24553, 2285, 926, 365, 819, 4190, 295, 9861, 17, 11, 9861, 16, 11, 50554], "temperature": 0.0, "avg_logprob": -0.33693130061311544, "compression_ratio": 1.4913793103448276, "no_speech_prob": 0.00016603760013822466}, {"id": 1503, "seek": 691406, "start": 6917.860000000001, "end": 6920.42, "text": " see if you can beat the momentum version.", "tokens": [50554, 536, 498, 291, 393, 4224, 264, 11244, 3037, 13, 50682], "temperature": 0.0, "avg_logprob": -0.33693130061311544, "compression_ratio": 1.4913793103448276, "no_speech_prob": 0.00016603760013822466}, {"id": 1504, "seek": 691406, "start": 6920.42, "end": 6921.5, "text": " I suspect you probably can.", "tokens": [50682, 286, 9091, 291, 1391, 393, 13, 50736], "temperature": 0.0, "avg_logprob": -0.33693130061311544, "compression_ratio": 1.4913793103448276, "no_speech_prob": 0.00016603760013822466}, {"id": 1505, "seek": 691406, "start": 6926.06, "end": 6926.56, "text": " Okay.", "tokens": [50964, 1033, 13, 50989], "temperature": 0.0, "avg_logprob": -0.33693130061311544, "compression_ratio": 1.4913793103448276, "no_speech_prob": 0.00016603760013822466}, {"id": 1506, "seek": 691406, "start": 6930.5, "end": 6932.06, "text": " We're a bit out of time, aren't we?", "tokens": [51186, 492, 434, 257, 857, 484, 295, 565, 11, 3212, 380, 321, 30, 51264], "temperature": 0.0, "avg_logprob": -0.33693130061311544, "compression_ratio": 1.4913793103448276, "no_speech_prob": 0.00016603760013822466}, {"id": 1507, "seek": 691406, "start": 6932.06, "end": 6935.740000000001, "text": " All right, I'm excited about the next bit.", "tokens": [51264, 1057, 558, 11, 286, 478, 2919, 466, 264, 958, 857, 13, 51448], "temperature": 0.0, "avg_logprob": -0.33693130061311544, "compression_ratio": 1.4913793103448276, "no_speech_prob": 0.00016603760013822466}, {"id": 1508, "seek": 691406, "start": 6935.740000000001, "end": 6939.38, "text": " But I wanted to spend time doing it properly, so I won't rush through it now.", "tokens": [51448, 583, 286, 1415, 281, 3496, 565, 884, 309, 6108, 11, 370, 286, 1582, 380, 9300, 807, 309, 586, 13, 51630], "temperature": 0.0, "avg_logprob": -0.33693130061311544, "compression_ratio": 1.4913793103448276, "no_speech_prob": 0.00016603760013822466}, {"id": 1509, "seek": 691406, "start": 6939.38, "end": 6941.14, "text": " But instead, we're gonna do it next time.", "tokens": [51630, 583, 2602, 11, 321, 434, 799, 360, 309, 958, 565, 13, 51718], "temperature": 0.0, "avg_logprob": -0.33693130061311544, "compression_ratio": 1.4913793103448276, "no_speech_prob": 0.00016603760013822466}, {"id": 1510, "seek": 694114, "start": 6941.14, "end": 6947.820000000001, "text": " So I will, yes, I will give you a hint that in our next lesson,", "tokens": [50364, 407, 286, 486, 11, 2086, 11, 286, 486, 976, 291, 257, 12075, 300, 294, 527, 958, 6898, 11, 50698], "temperature": 0.0, "avg_logprob": -0.3326263053744447, "compression_ratio": 1.575221238938053, "no_speech_prob": 1.7778471374185756e-05}, {"id": 1511, "seek": 694114, "start": 6947.820000000001, "end": 6951.780000000001, "text": " we will in fact get above 90%.", "tokens": [50698, 321, 486, 294, 1186, 483, 3673, 4289, 6856, 50896], "temperature": 0.0, "avg_logprob": -0.3326263053744447, "compression_ratio": 1.575221238938053, "no_speech_prob": 1.7778471374185756e-05}, {"id": 1512, "seek": 694114, "start": 6951.780000000001, "end": 6954.9800000000005, "text": " And it's got some very cool stuff to show you.", "tokens": [50896, 400, 309, 311, 658, 512, 588, 1627, 1507, 281, 855, 291, 13, 51056], "temperature": 0.0, "avg_logprob": -0.3326263053744447, "compression_ratio": 1.575221238938053, "no_speech_prob": 1.7778471374185756e-05}, {"id": 1513, "seek": 694114, "start": 6954.9800000000005, "end": 6956.18, "text": " I can't wait to show you that then.", "tokens": [51056, 286, 393, 380, 1699, 281, 855, 291, 300, 550, 13, 51116], "temperature": 0.0, "avg_logprob": -0.3326263053744447, "compression_ratio": 1.575221238938053, "no_speech_prob": 1.7778471374185756e-05}, {"id": 1514, "seek": 694114, "start": 6957.820000000001, "end": 6961.740000000001, "text": " But I think in the meantime, let's give ourselves a pat on the back", "tokens": [51198, 583, 286, 519, 294, 264, 14991, 11, 718, 311, 976, 4175, 257, 1947, 322, 264, 646, 51394], "temperature": 0.0, "avg_logprob": -0.3326263053744447, "compression_ratio": 1.575221238938053, "no_speech_prob": 1.7778471374185756e-05}, {"id": 1515, "seek": 694114, "start": 6961.740000000001, "end": 6964.780000000001, "text": " that we have successfully implemented.", "tokens": [51394, 300, 321, 362, 10727, 12270, 13, 51546], "temperature": 0.0, "avg_logprob": -0.3326263053744447, "compression_ratio": 1.575221238938053, "no_speech_prob": 1.7778471374185756e-05}, {"id": 1516, "seek": 694114, "start": 6964.780000000001, "end": 6968.02, "text": " I mean, think about all this stuff we've got running and happening, and", "tokens": [51546, 286, 914, 11, 519, 466, 439, 341, 1507, 321, 600, 658, 2614, 293, 2737, 11, 293, 51708], "temperature": 0.0, "avg_logprob": -0.3326263053744447, "compression_ratio": 1.575221238938053, "no_speech_prob": 1.7778471374185756e-05}, {"id": 1517, "seek": 696802, "start": 6968.02, "end": 6972.620000000001, "text": " we've done the whole thing from scratch using nothing but", "tokens": [50364, 321, 600, 1096, 264, 1379, 551, 490, 8459, 1228, 1825, 457, 50594], "temperature": 0.0, "avg_logprob": -0.2561162682466729, "compression_ratio": 1.5446009389671362, "no_speech_prob": 0.00018814142094925046}, {"id": 1518, "seek": 696802, "start": 6972.620000000001, "end": 6974.38, "text": " what's in the Python standard library.", "tokens": [50594, 437, 311, 294, 264, 15329, 3832, 6405, 13, 50682], "temperature": 0.0, "avg_logprob": -0.2561162682466729, "compression_ratio": 1.5446009389671362, "no_speech_prob": 0.00018814142094925046}, {"id": 1519, "seek": 696802, "start": 6974.38, "end": 6979.5, "text": " We've re-implemented everything, and we understand exactly what's going on.", "tokens": [50682, 492, 600, 319, 12, 332, 781, 14684, 1203, 11, 293, 321, 1223, 2293, 437, 311, 516, 322, 13, 50938], "temperature": 0.0, "avg_logprob": -0.2561162682466729, "compression_ratio": 1.5446009389671362, "no_speech_prob": 0.00018814142094925046}, {"id": 1520, "seek": 696802, "start": 6979.5, "end": 6983.900000000001, "text": " So I think this is really quite terrifically cool, personally.", "tokens": [50938, 407, 286, 519, 341, 307, 534, 1596, 7245, 4278, 1627, 11, 5665, 13, 51158], "temperature": 0.0, "avg_logprob": -0.2561162682466729, "compression_ratio": 1.5446009389671362, "no_speech_prob": 0.00018814142094925046}, {"id": 1521, "seek": 696802, "start": 6983.900000000001, "end": 6990.1, "text": " I hope you feel the same way, and look forward to seeing you in the next lesson.", "tokens": [51158, 286, 1454, 291, 841, 264, 912, 636, 11, 293, 574, 2128, 281, 2577, 291, 294, 264, 958, 6898, 13, 51468], "temperature": 0.0, "avg_logprob": -0.2561162682466729, "compression_ratio": 1.5446009389671362, "no_speech_prob": 0.00018814142094925046}, {"id": 1522, "seek": 696802, "start": 6990.1, "end": 6991.18, "text": " Thanks, bye.", "tokens": [51468, 2561, 11, 6543, 13, 51522], "temperature": 0.0, "avg_logprob": -0.2561162682466729, "compression_ratio": 1.5446009389671362, "no_speech_prob": 0.00018814142094925046}], "language": "en"}