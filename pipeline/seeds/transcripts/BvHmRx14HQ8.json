{"text": " So, hello everybody and welcome back to Practical Deep Learning for Coders. This is lesson two. And in the last lesson, we started training our first models. We didn't really have any idea how that training was really working, but we were looking at a high level at what was going on. And we learned about what is machine learning and how does that work. And we realized that based on how machine learning worked, that there are some fundamental limitations on what it can do. And we talked about some of those limitations. And we also talked about how after you've trained a machine learning model, you end up with a program which behaves much like a normal program, something with inputs and a thing in the middle and outputs. So today we're going to finish up talking about talking about that. And we're going to then look at how we get those models into production and what some of the issues with doing that might be. I wanted to remind you that there are two sets of notebooks available to you. One is the Fastbook repo, the full actual notebooks containing all the text of the O'Reilly book. And so this lets you see everything that I'm telling you in much more detail. And then as well as that, there's the course v4 repo, which contains exactly the same notebooks, but with all the pros stripped away to help you study. So that's where you really want to be doing your experiment and your practice. And so maybe as you listen to the video, you can kind of switch back and forth between the video and reading or do one and then the other and then put it away and have a look at the course v4 notebooks and try to remember like, okay, what was this section about and run the code and see what happens and change it and so forth. So we were looking at this line of code where we looked at how we created our data by passing in information, perhaps most importantly, some way to label the data. And we talked about the importance of labeling. And in this case, this particular data set, whether it's a cat or a dog, you can tell by whether it's an uppercase or a lowercase letter in the first position. That's just how this data set, if they tell you in the readme works. And we also looked particularly at this idea of a valid percent equals 0.2 and like, what does that mean? It creates a validation set. And that was something I wanted to talk more about. The first thing I want to do though is point out that this particular labeling function returns something that's either true or false. And actually this data set, as we'll see later, also tells, also contains the actual breed of 37 different cat and dog breeds. So you can, you can also grab that from the file name. In each of those two cases, we're trying to predict a category. Is it a cat or is it a dog? Or is it a German shepherd or a beagle or a ragdoll cat or whatever. When you're trying to predict a category, so when the label is a category, we call that a classification model. On the other hand, you might try to predict how old is the animal or how tall is it? Or something like that, which is like a continuous number that could be like 13.2 or 26.5 or whatever. Anytime you're trying to predict a number, the label is a number, you call that regression. Okay, so those are the two main types of model, classification and regression. So this is very important jargon to know about. So the regression model attempts to predict one or more numeric quantities such as temperature or location or whatever. This is a bit confusing because sometimes people use the word regression as a shortcut to a particular, like an abbreviation for a particular kind of model called linear regression. That's super confusing because that's not what regression means. Linear regression is just a particular kind of regression. But I just wanted to warn you of that. When you start talking about regression, a lot of people will assume you're talking about linear regression, even though that's not what the word means. All right, so I wanted to talk about this valid percent 0.2 thing. So as we described, valid percent grabs in this case, 20% of the data, it's 0.2 and puts it aside like in a separate bucket. And then when you train your model, your model doesn't get to look at that data at all. That data is only used to decide, to show you how accurate your model is. So if you train for too long and or with not enough data and or a model with too many parameters, after a while the accuracy of your model will actually get worse. And this is called overfitting, right? So we use the validation set to ensure that we're not overfitting. The next line of code that we looked at is this one, where we created something called a learner. We'll be learning a lot more about that. But a learner is basically or is something which contains your data and your architecture. That is the mathematical function that you're optimizing. And so a learner is the thing that tries to figure out what are the parameters which best cause this function to match the labels in this data. So we're talking a lot more about that. But basically this particular function, ResNet34, is the name of a particular architecture, which is just very good for computer vision problems. In fact, the name really is ResNet, and then 34 tells you how many layers there are. So you can use ones with bigger numbers here to get more parameters that will take longer to train, take more memory, more likely to overfit, but could also create more complex models. Right now, though, I wanted to focus on this part here, which is metrics equals error rate. This is where you list the functions that you want to be called with your data, with your validation data, and print it out after each epoch. And epoch is what we call it when you look at every single image in the data set once. And so after you've looked at every image in the data set once, we print out some information about how you're doing. And the most important thing we print out is the result of calling these metrics. So error rate is the name of a metric, and it's a function that just prints out what percent of the validation set are being incorrectly classified by your model. So a metrics of function that measures the quality of the predictions using the validation set. So error rate is 1. Another common metric is accuracy, which is just 1 minus error rate. So very important to remember from last week, we talked about loss. Arthur Samuel had this important idea in machine learning that we need some way to figure out how good our, how well our model is doing so that when we change the parameters, we can figure out which set of parameters make that performance measurement get better or worse. That performance measurement is called the loss. The loss is not necessarily the same as your metric. The reason why is a bit subtle, and we'll be seeing it in a lot of detail once we delve into the math in the coming lessons. But basically you need a function. You need a loss function where if you change the parameters by just a little bit up or just a little bit down, you can see if the loss gets a little bit better or a little bit worse. And it turns out that error rate and accuracy doesn't tell you that at all because you might change the parameters by such a small amount that none of your dog's predictions start becoming cats and none of your cat predictions start becoming dogs. So like your predictions don't change and your error rate doesn't change. So loss and metric are closely related, but the metric is the thing that you care about. The loss is the thing which your computer is using as the measurement of performance to decide how to update your parameters. So we measure overfitting by looking at the metrics on the validation set. So Fast.ai always uses the validation set to print out your metrics. And overfitting is like the key thing that machine learning is about. It's all about how do we find a model which fits the data, not just for the data that we're training with, but for data that the training algorithm hasn't seen before. So overfitting results when our model is basically cheating. Our model can cheat by saying, oh, I've seen this exact picture before and I remember that that's a picture of a cat. So it might not have learned what cats look like in general. It just remembers, you know, that images one, four, and eight are cats and two and three and five are dogs and learns nothing actually about what they really look like. So that's the kind of cheating that we're trying to avoid. We don't want it to memorize our particular data set. So we split off our validation data and most of these words you're seeing on the screen are from the book. Okay, so I just copied and pasted them. So if we split off our validation data and make sure that our model sees it during training, it's completely untainted by it, so we can't possibly cheat. Not quite true. We can cheat. The way we could cheat is we could run, we could fit a model, look at the result in the validation set, change something a little bit, fit another model, look at the validation set, change something a little bit. We could do that like a hundred times until we find something where the validation set looks the best. But now we might have fit to the validation set, right? So if you want to be really rigorous about this, you should actually set aside a third bit of data called the test set that is not used for training and it's not used for your metrics. It's actually, you don't look at it until the whole project's finished. And this is what's used on competition platforms like Kaggle. On Kaggle, after the competition finishes, your performance will be measured against a data set that you have never seen. And so that's a really helpful approach. And it's actually a great idea to do that, like even if you're not doing the modeling yourself. So if you're, if you're looking at vendors and you're just trying to decide, should I go with IBM or Google or Microsoft and they're all showing you how great their models are. What you should do is you should say, okay, you go and build your models and I am going to hang on to 10% of my data and I'm not going to let you see it at all. And when you're all finished, come back and then I'll run your model on the 10% of data you've never seen. Now pulling out your validation and test sets is a bit subtle though. Here's an example of a simple little data set. And this comes from a fantastic blog post that Rachel wrote that we will link to about creating effective validation sets. And you can see basically you have some kind of seasonal data set here. Now if you just say, okay, fast AI, I want to model that. I want to create my data loader using a valid percent of 0.2. It would do this. It would delete randomly some of the dots, right? Now this isn't very helpful because it's, we can still cheat because these dots are right in the middle of other dots. And this isn't what would happen in practice. What would happen in practice is we would want to predict this as sales by date, right? We want to predict the sales for next week, not the sales for 14 days ago, 18 days ago, and 29 days ago, right? So what you actually need to do to create an effective validation set here is not do it randomly, but instead chop off the end, right? And so this is what happens in all Kaggle competitions pretty much that involve time, for instance, is the thing that you have to predict is the next like two weeks or so after the last data point that they give you. And this is what you should do also for your test set. So again, if you've got vendors that you're looking at, you should say to them, okay, after you're all done modeling, we're going to check your model against data that is one week later than you've ever seen before and you won't be able to retrain or anything because that's what happens in practice, right? Okay. There's a question. I've heard people describe overfitting as training error being below validation error. Does this rule of thumb end up being roughly the same as yours? Okay, so that's a great question. So I think what they mean there is training loss versus validation loss because we don't print training error. So we do print at the end of each epoch the value of your loss function for the training set and the value of the loss function for the validation set. And if you train for long enough, so if it's training nicely, your training loss will go down and your validation loss will go down because by definition loss function is defined such as a lower loss function is a better model. If you start overfitting, your training loss will keep going down, right? Because like, why wouldn't that, you know, you're getting better and better parameters. But your validation loss will start to go up because actually you've started fitting to the specific data points in the training set and so it's not going to actually get better. It's going to get, it's not going to get better for the validation set. It'll start to get worse. However, that does not necessarily mean that you're overfitting or at least not overfitting in a bad way. As we'll see, it's actually possible to be at a point where the validation loss is getting worse, but the validation accuracy or error or metric is still improving. So I'm not going to describe how that would happen mathematically yet because we need to learn more about loss functions, but we will. But for now, just realize that the important thing to look at is your metric getting worse, not your loss function getting worse. Thank you for that fantastic question. The next important thing we need to learn about is called transfer learning. So the next line of code said learn.fineTune. Why does it say learn.fineTune? FineTune is what we do when we are transfer learning. So transfer learning is using a pre-trained model for a task that is different to what it was originally trained for. So more jargon to understand our jargon. Let's look at that. What's a pre-trained model? So what happens is remember I told you the architecture we're using is called ResNet-34. So when we take that ResNet-34, that's just a, it's just a mathematical function, okay, with lots of parameters that we're going to fit using machine learning. There's a big data set called ImageNet that contains 1.3 million pictures of a thousand different types of thing, whether it be mushrooms or animals or airplanes or hammers or whatever. There's a competition, or there used to be a competition that runs every year to see who could get the best accuracy on the ImageNet competition. And the models that did really well, people would take those specific values of those parameters and they would make them available on the internet for anybody to download. So if you download that, you don't just have an architecture now, you have a trained model. You have a model that can recognize a thousand categories of thing in images, which probably isn't very useful unless you happen to want something that recognizes those exact thousand categories of thing. But it turns out you can, rather, you can start with those weights in your model and then train some more epochs on your data and you'll end up with a far, far more accurate model than you would if you didn't start with that pre-trained model. And we'll see why in just a moment, right? But this idea of transfer learning, it's kind of, it makes intuitive sense, right? ImageNet already has some cats and some dogs in it, and it's, you know, it can say this is a cat and this is a dog, but you want to maybe do something that recognizes lots of breeds that aren't in ImageNet. Well, for it to be able to recognize cats versus dogs versus airplanes versus hammers, it has to understand things like what does metal look like? What does fur look like? What do ears look like? You know, so it can say like, oh, this breed of animal, this breed of dog has pointy ears and oh, this thing is metal, so it can't be a dog. So all these kinds of concepts get implicitly learnt by a pre-trained model. So if you start with a pre-trained model, then you don't, you don't have to learn all these features from scratch. And so transfer learning is the single most important thing for being able to use less data and less compute and get better accuracy. So that's a key focus for the Fast.ai library and a key focus for this course. There's a question. I am a bit confused on the differences between loss, error and metric. Sure. So error is just one kind of metric. So there's lots of different possible errors or labels you could have. Let's say you're trying to create a model which could predict how old a cat or a dog is. So the metric you might use is on average, how many years were you off by? So that would be a metric. On the other hand, if you're trying to predict whether this is a cat or a dog, your metric would be what percentage of the time am I wrong? So that latter metric is called the error rate. Okay, so error is one particular metric. It's a thing that measures how well you're doing. And it's like it should be the thing that you most care about. So you write a function or use one of Fast.ai's predefined ones, which measures how well you're doing. Loss is the thing that we talked about in lesson one. So I'll give a quick summary, but go back to lesson one if you don't remember. Arthur Samuel talked about how a machine learning model needs some measure of performance, which we can look at when we adjust our parameters up or down. Does that measure of performance get better or worse? And as I mentioned earlier, some metrics possibly won't change at all if you move the parameters up and down just a little bit. So they can't be used for this purpose of adjusting the parameters to find a better measure of performance. So quite often we need to use a different function. We call this the loss function. The loss function is the measure of performance that the algorithm uses to try to make the parameters better. And it's something which should kind of track pretty closely to the metric you care about. But it's something which as you change the parameters a bit, the loss should always change a bit. And so there's a lot of hand waving there because we need to look at some of the math of how that works. And we'll be doing that in the next couple of lessons. Thanks for the great questions. Okay, so fine-tuning is a particular transfer learning technique where the... Oh, and you're still showing your picture and not the slides. So fine-tuning is a transfer learning technique where the weights, this is not quite the right word, we should say the parameters, where the parameters of a pre-trained model are updated by training for additional epochs using a different task to that used for pre-training. So pre-training the task might have been ImageNet classification and then our different task might be recognizing cats versus dogs. So the way by default Fast.ai does fine-tuning is that we use one epoch, which remember is one looking at every image in the data set once, one epoch to fit just those parts of the model necessary to get the particular part of the model that's specially for your data set working. And then we use as many epochs as you ask for to fit the whole model. And so this is more for those people who might be a bit more advanced. We'll see exactly how this works later on in the lessons. So why does transfer learning work and why does it work so well? The best way in my opinion to look at this is to see this paper by Zyler and Fergus who were actually 2012 ImageNet winners and interestingly their key insights came from their ability to visualize what's going on inside a model. So visualization very often turns out to be super important to getting great results. What they were able to do was they looked, remember I told you like a ResNet 34 has 34 layers, they looked at something called AlexNet which was the previous winner of the competition which only had seven layers. At the time that was considered huge and so they took the seven layer model and they said what is the first layer of parameters look like and they figured it out how to draw a picture of them. Right and so the first layer had lots and lots of features but here are nine of them, 1, 2, 3, 4, 5, 6, 7, 8, 9. And here's what nine of those features look like. One of them was something that could recognize diagonal lines from top left to bottom right. One of them could find diagonal lines from bottom left to top right. One of them could find gradients that went from the top of orange to the bottom of blue. Some of them were able, you know, one of them was specifically for finding things that were green and so forth. Right so for each of these nine they're called filters or features. So then something really interesting they did was they looked at for each one of these, each one of these filters, each one of these features, and we'll learn kind of mathematically about what these actually mean in the coming lessons but for now let's just recognize them as saying oh there's something that looks at diagonal lines and something that looks at gradients and they found in the actual images in ImageNet specific examples of parts of photos that match that filter. So for this top left filter here are nine actual patches of real photos that match that filter and as you can see they're all diagonal lines. And so here's the for the green one here's parts of actual photos that match the green one. So layer one is super super simple and one of the interesting things to note here is that something that can recognize gradients and patches of color and lines is likely to be useful for lots of other tasks as well, not just ImageNet. So you can kind of see how something that can do this might also be good at many many other computer vision tasks as well. This is layer two. Layer two takes the features of layer one and combines them so it can not just find edges but can find corners or repeating curving patterns or semi-circles or full circles and so you can see for example here's a it's kind of hard to exactly visualize these layers after layer one. You kind of have to show examples of what the filters look like but here you can see examples of parts of photos that these this layer two circular filter has activated on and as you can see it's found things with circles. So interestingly this one which is this kind of blotchy gradient seems to be very good at finding sunsets and this repeating vertical pattern is very good at finding like curtains and wheat fields and stuff. So the further we get layer three then gets to combine all the kinds of features in layer two and remember we're only seeing so we're only seeing here are 12 of the features but actually there's probably hundreds of them I don't remember exactly in AlexNet but there's lots but by the time we get to layer three by combining features from layer two it already has something which is finding text. So this is a feature which can find bits of image that contain text. It's already got something which can find repeating geometric patterns and you see this is not just like a matching specific pixel patterns this is like a semantic concept it can find repeating circles or repeating squares or repeating hexagons right. So it's really like computing it's not just matching a template and remember we know that neural networks can solve any possible computable function so it can certainly do that. So layer four gets to combine all the filters from layer three anyway it wants and so by layer four we have something that can find dog faces for instance. So you can kind of see how each layer we get like more applicatively more sophisticated features and so that's why these deep neural networks can be so incredibly powerful. It's also why transfer learning can work so well because like if we wanted something that can find books and I don't think there's a book category in ImageNet well it's actually already got something that can find text as an earlier filter which I guess it must be using to find maybe there's a category for library or something or a bookshelf. So when you use transfer learning you can take advantage of all of these pre-learned features to find things that are just combinations of these existing features. That's why transfer learning can be done so much more quickly and so much less data than traditional approaches. One important thing to realize then is that these techniques for computer vision are not just good at recognizing photos. There's all kinds of things you can turn into pictures. For example these are example these are sounds that have been turned into pictures by representing their frequencies over time and it turns out that if you convert a sound into these kinds of pictures you can get basically state-of-the-art results at sound detection just by using the exact same ResNet learner that we've already seen. I wanted to highlight that it's 9 45 so if you want to take a break soon. A really cool example from I think this is our very first year of running fast AI. One of our students created pictures they worked at Splunk in anti-fraud and they created pictures of users moving their mouse and if I remember correctly as they moved their mouse he basically drew a picture of where the mouse moved and the color depended on how fast they moved and these circular blobs is where they clicked the left or the right mouse button. And at Splunk they then well he what he did actually for the for the course as a project for the course is he tried to see whether he could use this these pictures with exactly the same approach we saw in lesson one to create an anti-fraud model and it worked so well that Splunk ended up patenting a new product based on this technique and you can actually check it out there's a blog post about it on the internet where they describe this breakthrough anti-fraud approach which literally came from one of our really amazing and brilliant and creative students after lesson one of the course. Another cool example of this is looking at different viruses and again turning them into pictures and you can kind of see how they've got here this is from a paper check out the book for the citation they've got three examples of a particular virus called VB.AT and another example of a particular virus called Fakrian and you can see each case the pictures all look kind of similar and that's why again they can get state-of-the-art results in in virus detection by turning the kind of program signatures into pictures and putting it through image recognition. So in the book you'll find a list of all of the terms all of the most important terms we've seen by so far and what they mean I'm not going to read through them but I want you to please because these are the these are the terms that we're going to be using from now on and you've got to know what they mean because if you don't you're going to be really confused because I'll be talking about labels and architectures and models and parameters and they have very specific exact meanings and they'll be using those exact meanings so please review this. So to remind you this is where we got to we we ended up with Arthur Samuel's overall approach and we replaced his terms with our terms so we have an architecture which contains parameters as inputs and we have more parameters and the data as inputs so that the architecture plus the parameters of the model with the inputs they use to calculate predictions they are compared to the labels with a loss function and that loss function is used to update the parameters many many times to make them better and better until the loss gets nice and super low. So this is the end of chapter one of the book. It's really important to look at the questionnaire because the questionnaire is the thing where you can check whether you have taken away from this book of this chapter the stuff that we hope you have. So go through it and anything that you're not sure about the tech the answer is in the text so just go back to earlier in the book and you will in the chapter and you will find the answers. There's also a further research section after each questionnaire for the first couple of chapters they're actually pretty simple hopefully they're pretty fun and interesting they're things where to answer the question it's not enough to just look in the chapter you actually have to go and do your own thinking and experimenting and googling and so forth. In later chapters some of these further research things are pretty significant projects that might take a few days or even weeks and so yeah you know check them out because hopefully there'll be a great way to expand your understanding of the material. So something that Sylvain points out in the book is that if you really want to make the most of this then after each chapter please take the time to experiment with your own project and with the notebooks you provide we provide and then see if you can redo the notebooks on a new data set. Perhaps for chapter one that might be a bit hard because we haven't really shown how to change things but for chapter two which we're going to start next you'll absolutely be able to do that. Okay so let's take a five minute break and we'll come back at 955 San Francisco time. Okay so welcome back everybody and I think we've got a couple of questions to start with so Rachel please take it away. Sure. Are filters independent? By that I mean if filters are pre-tried might they become less good in detecting features of previous images when fine-tuned? Oh that is a great question. So assuming I understand the question correctly if you start with say an ImageNet model and then you you fine-tune it on dogs versus cats for a few epochs and you get something that's very good at recognizing dogs versus cats it's going to be much less good as an ImageNet model after that so it's not going to be very good at recognizing airplanes or hammers or whatever. This is called catastrophic forgetting in the literature the idea that as you like see more images about different things to what you saw earlier that you start to forget about the things you saw earlier. So if you want to fine-tune something which is good at a new task but also continues to be good at the previous task you need to keep putting in examples of the previous task as well. And what are the example what are the differences between parameters and hyperparameters? If I am feeding an image of a dog as an input and then changing the hyperparameters of batch size in the model what would be an example of a parameter? So the parameters are the things that are described in lesson one that Arthur Samuel described as being the things which change what the model does what the architecture does. So we start with this infinitely flexible function the thing called a neural network that can do anything at all and the the way you get it to do one thing versus another thing is by changing its parameters there they are the numbers that you pass into that function. So there's two types of numbers you pass into the function there's the numbers that represent your input like the pixels of your dog and there's the numbers that represent the learnt parameters. So in the example of something that's not a neural net but like a checkers playing program like Arthur Samuel might have used back in the early 60s and late 50s those parameters may have been things like if there is a opportunity to take a piece versus an opportunity to get to the end of a board how much more value should I consider one versus the other you know it's twice as important or it's three times as important that two versus three that would be an example of a parameter. In a neural network parameters are a much more abstract concept and so a detailed understanding of what they are will come in the next lesson or two but it's the same basic idea they're the numbers which change what the model does to be something that recognizes malignant tumors versus cats versus dogs versus colorizes black and white pictures. Whereas the hyperparameter is the choices about what what numbers do you pass to the function when you act the actual fitting function to decide how that fitting process happens. There's a question I'm curious about the pacing of this course I'm concerned that all the material may not be covered. Depends what you mean by all the material we certainly won't cover everything in the world so yeah we'll cover what we can we'll cover what we can in seven lessons. We're certainly not covering the whole book if that's what you're wondering the whole book will be covered in either two or three courses. In the past it's generally been two courses to cover about the amount of stuff in the book but we'll see how it goes because the books pretty big 500 pages. When you say two courses you mean 14 lessons. 14 lessons yeah so it'd be like 14 or 21 lessons to get through the whole book. Although having said that by the end of the first lesson hopefully there'll be kind of like enough momentum and understanding that the reading the book independently will be more useful and you'll have also kind of gained a community of folks on the forums that you can hang out with and ask questions of and so forth. So in in the second part of the course we're going to be talking about putting stuff in production and we're so to do that we need to understand like what are the capabilities and limitations of of deep learning. What are the kinds of projects that even make sense to try to put in production and you know one of the key things I should mention in in the Balkan in this course is that the first two or three lessons and chapters there's a lot of stuff which is designed not just for the coders but for everybody. There's lots of information about like what are the practical things you need to know to make deep learning work and so one of the things you need to know is like well what's deep learning actually good at at the moment. So I'll summarize what the book says about this but there are the kind of four key areas that we have as applications in fast AI computer vision text tabular and what I've called here RECSIS. This stands for Recommendation Systems and specifically a technique called Collaborative Filtering which we briefly saw last week. Sorry another question. Is are there any pre-trained weights available other than the ones from ImageNet that we can use? If yes when should we use others in one ImageNet? Oh that's a really great question. So yes there are a lot of pre-trained models and one way to find them. And also you're currently just showing that. Okay great. One great way to find them is you can look up modelzoo which is a common name for like places that have lots of different models and so here's lots of modelzoo's or you can look for pre-trained models. And so yeah there's quite a few. Unfortunately not as wide a variety as I would like. Most are still on ImageNet or similar kinds of general photos. For example medical imaging there's hardly any. There's a lot of opportunities for people to create domain specific pre-trained models. It's still an area that's really under done because not enough people are working on transfer learning. Okay so as I was mentioning we've kind of got these four applications that we've talked about a bit. And deep learning is pretty you know pretty good at all of those. Tabular data like spreadsheets and database tables is an area where deep learning is not always the best choice but it's particularly good for things involving high cardinality variables. That means variables that have like lots and lots of discrete levels like zip code or product ID or something like that. Deep learning is really pretty great for those in particular. For text it's pretty great at things like classification and translation. It's actually terrible for conversation. So that's been something that's been a huge disappointment for a lot of companies. They tried to create these like conversation bots. But actually deep learning isn't good at providing accurate information. It's good at providing things that sound accurate and sound compelling but we don't really have great ways yet of actually making sure it's correct. One big issue for recommendation systems, collaborative filtering, is that deep learning is focused on making predictions which don't necessarily actually mean creating useful recommendations. We'll see what that means in a moment. Deep learning is also good at multimodal. That means things where you've got multiple different types of data. So you might have some tabular data including a text column and an image and some collaborative filtering data. And combining that all together is something that deep learning is really good at. So for example putting captions on photos is something which deep learning is pretty good at. Although again it's not very good at being accurate. So you know it might say this is a picture of two birds when it's actually a picture of three birds. And then this other category there's lots and lots of things that you can do with deep learning by being creative about the use of these kinds of other application based approaches. For example an approach that we developed for natural language processing called ULMFIT that we're learning in the course. It turns out that it's also fantastic at doing protein analysis. If you think of the different proteins as being different words and they're in a sequence which has some kind of state and meaning it turns out that ULMFIT works really well for protein analysis. So often it's about kind of being being creative. So to decide like for the product that you're trying to build is deep learning going to work well for it. In the end you kind of just have to try it and see. But if you if you do a search you know hopefully you can find examples about the people that have tried something similar. Even if you can't that doesn't mean it's not going to work. So for example I mentioned the collaborative filtering issue where a recommendation and a prediction are not necessarily the same thing. You can see this on Amazon for example quite often. So I bought a Terry Pratchett book and then Amazon tried for months to get me to buy more Terry Pratchett books. Now that must be because their predictive model said that people who bought one particular Terry Pratchett book are likely to also buy other Terry Pratchett books. But from the point of view of like well is this going to change my buying behavior? Probably not right. Like if I liked that book I already know I like that author and I already know that like they probably wrote other things so I'll go and buy it anyway. So this would be an example of like Amazon probably not being very smart here. They're actually showing me collaborative filtering predictions rather than actually figuring out how to optimize your recommendation. So an optimized recommendation would be something more like your local human bookseller might do where they might say oh you like Terry Pratchett well let me tell you about other kind of comedy fantasy sci-fi writers on the similar vein who you might not have heard about before. So the difference between recommendations and predictions is super important. So I wanted to talk about a really important issue around interpreting models and for a case study for this I thought we let's pick something that's actually super important right now which is a model in this paper. One of the things we're going to try and do in this course is learn how to read papers. So here is a paper which you would love for everybody to read called High Temperature and High Humidity Reduce the Transmission of COVID-19. Now this is a very important issue because if the claim of this paper is true then that would mean that this is going to be a seasonal disease and if this is a seasonal disease and it's going to have massive policy implications. So let's try and find out how this was modeled and understand how to interpret this model. So this is a key picture from the paper and what they've done here is they've taken a hundred cities in China and they've plotted the temperature on one axis in Celsius and R on the other axis where R is a measure of transmissibility. It says for each person that has this disease how many people on average will they infect. So if R is under 1 then the disease will not spread. If R is higher than like 2 it's going to spread incredibly quickly and basically R is going to you know any high R is going to create an exponential transmission impact. And you can see in this case they have plotted a best fit line through here and then they've made a claim that there's some particular relationship in terms of a formula that R is 1.99 minus 0.023 times temperature. So a very obvious concern I would have looking at this picture is that this might just be random maybe there's no relationship at all but just if you picked a hundred cities at random perhaps they would sometimes show this level of relationship. So one simple way to kind of see that would be to actually do it in a spreadsheet. So here's here is a spreadsheet where what I did was I kind of eyeballed this data and I guessed about what is the mean degree centigrade I think it's about 5 and that's about the standard deviation of centigrade I think it's probably about 5 as well. And then I did the same thing for R I think the mean R looks like it's about 1.9 to me and it looks like the standard deviation of R is probably about 0.5. So what I then did was I just jumped over here and I created a random normal value so a random value from a normal distribution from a normal distribution so a bell curve with that particular mean and standard deviation of temperature and that particular mean and standard deviation of R. And so this would be an example of a city that might be in this data set of a hundred cities something with 9 degrees Celsius and an R of 1.1 so that would be 9 degrees Celsius and an R of 1.1 so something about here. And so then I just copied that formula down 100 times. So here are a hundred cities that could be in China right where this is assuming that there is no relationship between temperature and R right they're just random numbers. And so each time I recalculate that so if I hit ctrl equals it will just recalculate it right I get different numbers okay because they're random and so you can see at the top here I've then got the average of all of the temperatures and the average of all of the Rs and the average of all of the temperatures varies and the average of all of the Rs varies as well. So then I what I did was I copied those random numbers over here. Let's actually do it so I'll go copy these 100 random numbers and paste them here here here here here and so now I've got 1 2 3 4 5 6 I've got 6 kind of groups of a hundred cities right and so let's stop those from randomly changing anymore by just fixing them in stone there. Okay so now that I've pasted them in I've got 6 examples of what a hundred cities might look like if there was no relationship at all between temperature and R and I've got their mean temperature and R in each of those 6 examples and what I've done is you can see here at least for the first one is I've plotted it right and you can see in this case there's actually a slight positive slope and I've actually calculated the slope for each just by using the slope function in Microsoft Excel and you can see that actually in this particular case it's just random 5 times it's been negative and it's even more negative than their point 0 2 3 and so you can like it's kind of matching our intuition here which is that this the slope of the line that we have here is something that absolutely can often happen totally by chance it doesn't seem to be indicating any kind of real relationship at all. If we wanted that slope to be like more confident we would need to look at more cities so like here I've got 3000 randomly generated numbers and you can see here the slope is point 0 0 0 2 right it's almost exactly 0 which is what we'd expect right when there's actually no relationship between C and R and in this case there isn't they're all random then if we look at lots and lots of randomly generated cities then we can say oh yeah this there's no slope but when you only look at a hundred as we did here you're going to see relationships totally coincidentally very very often right so that's something that we need to be able to measure and so one way to measure that is we use something called a p-value so a p-value here's how a p-value works we start out with something called a null hypothesis and the null hypothesis is basically what's what's our starting point assumption so our starting point assumption might be oh there's no relationship between temperature and R and then we gather some data and have you explained what R is I have yes R is the transmissibility of the virus so then we gather data of independent independent variables so in this case the independent variable is the thing that we think might cause a dependent variable so here the independent variable would be temperature the dependent variable would be R so here we've gathered data there's the data that was gathered in this example and then we say what percentage of the time would we see this amount of relationship which is a slope of 0.023 by chance and as we've seen one way to do that is by what we would call a simulation which is by generating random number a hundred set pairs of random numbers a bunch of times and seeing how often you see this this relationship we don't actually have to do it that though there's actually a simple equation we can use to jump straight to this number which is what percent of the time would we see that relationship by chance and this is basically what that looks like we have the most likely observation which in this case would be if there is no relationship between temperature and R then the most likely slope would be zero and sometimes you get positive slopes by chance and sometimes you get pretty small slopes and sometimes you get large negative slopes by chance and so the you know the larger the number the less likely it is to happen whether it be on the positive side or the negative side and so in our case our question was how often are we going to get less than negative 0.023 so it would actually be somewhere down here and I actually copy this from Wikipedia where they were looking for positive numbers and so they've colored in this area above the number so this is the p-value and so you can we don't care about the math but there's a simple little equation you can use to directly figure out this number the p-value from the data so this is kind of how nearly all kind of medical research results tend to be shown and folks really focus on this idea of p-values and indeed in this particular study as we'll see in a moment they reported p-values so probably a lot of you have seen p-values in your previous lives they come up in a lot of different domains here's the thing they are terrible you almost always shouldn't be using them don't just trust me trust the American Statistical Association they point out six things about p-values and those include p-values do not measure the probability that the hypothesis is true or the probability that the data were produced by random choice alone now we know this because we just saw that if we use more data right so if we sample 3,000 random cities rather than a hundred we get a much smaller value right so p-values don't just tell you about how big a relationship is but they actually tell you about a combination of that and how much data did you collect right so so they don't measure the probability that the hypothesis is true so therefore conclusions and policy decisions should not be based on whether a p-value passes some threshold p-value does not measure the importance of a result right because again it could just tell you that you collected lots of data which doesn't tell you that the results actually of any practical import and so by itself it does not provide a good measure of evidence so Frank Harrell who is somebody who I read his book and it's a really important part of my learning he's a professor of biostatistics has a number of great articles about this he says null hypothesis testing and p-values have done significant harm to science and he wrote another piece called null hypothesis significance testing never worked so I've shown you what p-values are so that you know why they don't work not so that you can use them right but they're a super important part of machine learning because they come up all the time in making this you know when people saying this is how we decide whether your drug worked or whether there is a epidemiological relationship or whatever and indeed p-values appear in this paper so in the paper they show the results of a multiple linear regression and they put three stars next to any relationship which has a p-value of 0.01 or less so there is something useful to say about a small p-value like 0.01 or less which is that the thing that we're looking at did not probably did not happen by chance right the biggest statistical error people make all the time is that they see that a p-value is not less than 0.05 and then they make the erroneous conclusion that no relationship exists right which doesn't make any sense because like let's say you only had like three data points then you almost certainly won't have enough data to have a p-value of less than 0.05 for any hypothesis so like the way to check is to go back and say what if I picked the exact opposite null hypothesis what if my null hypothesis was there is a relationship between temperature and R then do I have enough data to reject that null hypothesis right and if the answer is no then you just don't have enough data to make any conclusions at all right so in this case they do have enough data to be confident that there is a relationship between temperature and R now that's weird because we just looked at the graph and we did a little bit of a back of the envelope in Excel and we thought this is could could well be random so here's where the issue is the graph shows what we call a univariate relationship a univariate relationship shows the relationship between one independent variable and one dependent variable and that's what you can normally show in a graph but in this case they did a multivariate variant model in which they looked at temperature and humidity and GDP per capita and population density and when you put all of those things into the model then you end up with statistically significant results for temperature and humidity why does that happen well the reason that happens is because all these variation in the blue dots is not random there's a reason they're different right and the reasons include denser cities are going to have higher transmission for instance and probably more humid will have less transmission so when you do a multivariate model it actually allows you to be more confident of your results right but the p-value as noted by the American Statistical Association does not tell us whether this is a practical importance the thing that tells us is this is a practical as important as the actual slope that's found and so in this case the equation they come up with is that R equals 3.968 minus 3.038 by temperature minus 0.024 by relative humidity this is this equation is this practically important well we can again do a little back of the envelope here by just putting that into Excel let's say there was one place that had a temperature of 10 centigrade and a humidity of 40 then if this equation is correct I would be about 2.7 somewhere with a temperature of 35 centigrade and a humidity of 80 I would be about 0.8 so is this practically important oh my god yes right two different cities with different climates can be if they're the same in every other way and this model is correct then one city would have no spread of disease because R is less than one one would have massive exponential explosion so we can see from this model that if the modeling is correct then this is a highly practically significant result so this is how you determine practical significance of your models it's not with p-values but with looking at kind of actual outcomes so how do you think about the practical importance of a model and how do you turn a predictive model into something useful in production so I spent many many years thinking about this and I actually created a with some other great folks actually created a paper about it designing great data products and this is largely based on 10 years of work I did at a company I founded called optimal decisions group and optimal decisions group was focused on the question of helping insurance companies figure out what prices to set and insurance companies up until that point had focused on predictive modeling actuaries in particular spent their time trying to figure out how likely is it that you're going to crash your car and if you do how much damage might you have and then based on that try to figure out what price they should set for your policy so for this company what we did was we decided to use a different approach which I ended up calling the drivetrain approach which is described here to to set insurance prices and indeed to do all kinds of other things and so for the insurance example the objective would be for an insurance company would be how do I maximize my let's say five-year profit and then what inputs can we control can we control which I call levers so in this case it would be what price can I set and then data is data which can tell you as you change your levers how does that change your objective so if I start increasing my price to people who are likely to crash their car then we'll get less of them which means we have less costs but at the same time we'll also have less revenue coming in for example so to link up the kind of the levers to the objective via the data we collect we build models that described how the levers influence the objective and this is all like it seems pretty obvious when you say it like this but when we started work with optimal decisions in 1999 nobody was doing this in insurance everybody in insurance was simply doing a predictive model to guess how likely people were to crash their car and then pricing was set by like adding 20% or whatever it was just done in a very kind of naive way so what I did is I you know over many years took this basic process and tried to help lots of companies figure out how to use it to turn predictive models into actions so the starting point in like actually getting value in a predictive model is thinking about what is it you're trying to do and you know what are the sources of value in that thing you're trying to do the levers what are the things you can change like what's the point of a predictive model if you can't do anything about it right figuring out ways to find what data you you don't have which one's suitable what's available then think about what approaches to analytics you can then take and then super important like well can you actually implement you know those changes and super super important how do you actually change things as the environment changes and you know interestingly a lot of these things areas where there's not very much academic research there's a little bit and some of the papers that have been particularly around maintenance of like how do you decide when your machine learning model is kind of still okay how do you update it over time have had like many many many many citations but they don't pop up very often because a lot of folks are so focused on the math you know and then there's the whole question of like what constraints are in place across this whole thing so what you'll find in the book is there is a whole appendix which actually goes through every one of these six things and has a whole list of examples so this is an example of how to like think about value and lots of questions that companies and organizations can use to try and think about you know all of these different pieces of the actual puzzle of getting stuff into production and actually into an effective product. We have a question. Sure just a moment so I say so do check out this appendix because it actually originally appeared as a blog post and I think except for my COVID-19 posts that I did with Rachel it's actually the most popular blog post I've ever written it's had hundreds of thousands of views and it kind of represents like 20 years of hard-won insights about like how you actually get value from machine learning and practice and what you actually have to ask so please check it out because hopefully you'll find it helpful. So when we think about like think about this for the question of how should people think about the relationship between seasonality and transmissibility of COVID-19 you kind of need to dig really deeply into the questions about like oh not just what what's that what are those numbers in the data but what does it really look like right so one of the things in the paper that they show is actual maps right of temperature and humidity and R right and you can see like not surprisingly that humidity and temperature in China are what we would call autocorrelated which is to say that places that are close to each other in this case geographically have similar temperatures and similar humidities and so like this actually puts into the question the a lot the p-values that they have right because you you can't really think of these as a hundred totally separate cities because the ones that are close to each other probably have very close behavior so maybe you should think of them as like a small number of sets of cities you know of kind of larger geographies. So these are the kinds of things that when you look actually into a model you need to like think about what are the what are the limitations but then to decide like well what does that mean what do I what do I do about that you you need to think of it from this kind of utility point of view this kind of end-to-end what are the actions I can take what are the results point of view not just null hypothesis testing so in this case for example there are basically four possible key ways this could end up it could end up that there really is a relationship between temperature and R or so that's what the right hand side is or there is no real relationship between temperature and R and we might act on the assumption that there is a relationship or we might act on the assumption that there isn't a relationship and so you kind of want to look at each of these four possibilities and say like well what would be the economic and societal consequences and you know there's going to be a huge difference in lives lost and you know economies crashing and whatever else to you know for each of these four. The paper actually you know has shown if their model is correct what's the likely R value in March for like every city in the world and the likely R value in July for every city in the world and so for example if you look at kind of New England and New York the prediction here is and also West Coast the very coast of the West Coast is that in July the disease will stop spreading now you know if that happens if they're right then that's going to be a disaster because I think it's very likely in America and also the UK that people will say oh turns out this disease is not a problem you know it didn't really take off at all the scientists were wrong people will go back to their previous day-to-day life and we could see what happened in 1918 flu virus of like the second go-around when winter hits could be much worse than than the start right so like there's these kind of like huge potential policy impacts depending on whether this is true or false and so to think about it to yes I also just wanted to say that it would be it would be very irresponsible to to think oh summer is going to solve it we don't need to act now I'm just in that this is something growing exponentially and could do a huge huge amount of damage yeah yeah so it could already has done exactly either way if you assume that there will be seasonality and that summer will fix things then it could lead you to be apathetic now if you assume there's no seasonality and then there is then you could end up kind of creating a larger level of expectation of distraction than actually happens and end up with your population being even more apathetic you know so that they're you know being wrong in any direction of your problem so one of the ways we tend to deal with this with with this kind of modeling is we try to think about priors so priors are basically things where we you know rather than just having a null hypothesis we try and start with a guess as to like well what's what's more likely right so in this case if memory says correctly I think we know that like flu viruses become inactive at 27 centigrade we know that like cold the cold coronaviruses are seasonal the 1918 oopsie daisy the 1918 flu epidemic pandemic was seasonal in every country and city that's been studied so far there's been quite a few studies like this they've always found climate relationships so far so maybe we'd say well prior belief is that this thing is probably seasonal and so then we'd say well this particular paper adds some evidence to that so like it shows like how incredibly complex it is to use a model in practice for in this case policy discussions but also for like organizational decisions because you know there's always complexities there's always uncertainties and so you actually have to think about the the utilities you know and your best guesses and try to combine everything together as best as you can okay so with all that said it's still nice to be able to get our our models up and running even if you know even just a predictive model is sometimes useful of its own sometimes it's useful to prototype something and sometimes it's just it's going to be part of some bigger picture so rather than try to create some huge end-to-end model here we thought we would just show you how to get your your pytorch fast AI model up and running in as raw a form as possible so that from there you can kind of build on top of it as you like so to do that we are going to download and curate our own data set and you're going to do the same thing you've got to train your own model on that data set and then you're going to create an application and then you're going to host it right now there's lots of ways to create a curate an image data set you might have some photos on your own computer there might be stuff at work you can use one of the easiest though is just to download stuff off the internet there's lots of services for downloading stuff off the internet we're going to be using Bing image search here because they're super easy to use a lot of the other kind of easy to use things require breaking the terms of service of websites so like we're not going to show you how to do that but there's lots of examples that do show you how to do that so you can check them out as well if you if you want to Bing image search is actually pretty great at least at the moment these things change a lot so keep an eye on our website to see if we've changed our recommendation the biggest problem with Bing image search is that the signup process is a nightmare at least at the moment like one of the hardest parts of this book is just signing up to their damn API which requires going through Azure it's called cognitive services Azure cognitive services so we'll make sure that all that information is on the website for you to follow through just how to sign up so we're going to start from the assumption that you've already signed up but you can find it just go Bing Bing image search API and at the moment they give you seven days with a pretty high pretty high quota for free and then after that you can keep using it as long as you like but they kind of limit it to like three transactions per second or something which is still plenty you can still do thousands for free so it's at the moment it's pretty great even for free so what will happen is when you sign up for Bing image search or any of these kind of services they'll give you an API key so just replace the xxx here with the API key that they give you okay so that's now going to be called key in fact let's do it over here okay so you'll put in your key and then there's a function we've created called search images being which is just a super tiny little function as you can see it's just two lines of code I was just trying to save a little bit of time which will take some take your API key and some search term and return a list of URLs that match that search term as you can see for using this particular service you have to install a particular package so we show you how to do that on the site as well so once you've done so you'll be able to run this and that will return by default I think 150 URLs okay so fast AI comes with a download URL function so you let's just download one of those images just to check and open it up and so what I did was I searched for grizzly bear and here I have a grizzly bear so then what I did was I said okay let's try and create a model that can recognize grizzly bears versus black bears versus teddy bears so that way I can find out I could set up some video recognition system near our campsite when we're out camping that gives me bear warnings but if it's a teddy bear coming then it doesn't warn me and wake me up because that would not be scary at all so then I just go through each of those three bear types create a directory with the name of grizzly or black or teddy bear search being for that particular search term along with bear and download and so download images is a fast AI function as well so after that I can call get image files which is a fast AI function that will just return recursively all of the image files inside this path and you can see it's given me bears slash black slash and then lots of numbers so one of the things you have to be careful of is that a lot of the stuff you download will turn out to be like not images at all and will break so you can call verify images to check that all of these file names are actual images and in this case I didn't have any failed so this it's empty but if you did have some then you would call path.unlink.unlink path.unlink is part of the Python standard library and it deletes a file and map is something that we'll call this function for every element of this collection. This is part of a special fast AI class called L it's basically it's kind of a mix between the Python standard library list class and a NumPy array class and we'll be learning more about it later in this course but it basically tries to make it super easy to do kind of more functional style programming in Python. So in this case it's going to unlink everything that's in the failed list which is probably what we want because there are all the images that failed to verify. All right so we've now got a path that contains a whole bunch of images and they're classified according to black, grizzly or teddy based on what folder they're in and so to create so we're going to create a model and so to create a model the first thing we need to do is to tell fast AI what kind of data we have and how it's structured. Now in part in lesson one of the course we did that by using what we call a factory method which is we just said image data loaders dot from name and it did it all for us. Those factory methods are fine for beginners but now we're into lesson two we're not quite beginners anymore so we're going to show you the super super flexible way to use data in whatever format you like and it's called the data block API and so the data block API looks like this. Here's the data block API. You tell fast AI what your independent variable is and what your dependent variable is so what your labels are and what your input data is. So in this case our input data images and our labels are categories so category is going to be either grizzly or black or teddy so that's the first thing you tell it that that's the block's parameter and then you tell it how do you get a list of all of the in this case file names right and we just saw how to do that because we just called the function ourselves the function is called get image files so we tell it what function to use to get that list of items and then you tell it how do you split the data into a validation set and a training set and so we're going to use something called a random splitter which just splits it randomly and we're going to put 30% of it into the validation set. We're also going to set the random seed which ensures that every time we run this the validation set will be the same and then you say okay how do you label the data and this is the name of a function called parent label and so that's going to look for each item at the name of the parent so this this particular one would become a black bear and this is like the most common way for image data sets to be represented is that they get put the different images get the files get put into folder according to their label and then finally here we've got something called item transforms we'll be learning a lot more about transforms in a moment that these are basically functions that get applied to each image and so each image is going to be resized to 128 by 128 square. So we're going to be learning more about data block API soon but basically the process is going to be it's going to call whatever is get items which is a list of image files it's then I'm going to call get x get y so in this case there's no get x but there is a get y so it's just parent label and then it's going to call the create method for each of these two things it's going to create an image and it's going to create a category it's then going to call the item transforms which is resize and then the next thing it does is it puts it into something called a data loader a data loader is something that grabs a few images at a time I think by default at 64 and puts them all into a single it's got a batch it just grabs 64 images and sticks them all together and the reason it does that is it's then puts them all onto the GPU at once so it can pass them all to the model through the GPU in one go and that's going to let the GPU go much faster as we'll be learning about and then finally we don't use any here we can have something called batch transforms which we will talk about later and then somewhere in the middle about here conceptually is the splitter which is the thing that splits into the training set and the validation set so this is a super flexible way to tell fastai how to work with your data and so at the end of that it returns an object of type data loaders that's why we always call these things DLs right so data loaders has a validation and a training data loader and a data loader as I just mentioned is something that grabs a batch of a few items at a time and puts it on the GPU for you so this is basically the entire code of data loaders so the details don't matter I just wanted to point out that like a lot of these concepts in fastai when you actually look at what they are they're incredibly simple little things it's literally something that you just pass in a few data loaders to and it's still a similar attribute and pass and gives you the first one back as dot train and the second one back as dot valid so we can create our data loaders by first of all creating the data block and then we call the data loaders passing in our path to create DLs and then you can call show batch on that you can call show batch on pretty much anything in fastai to see your data and look we've got some grizzlies we've got a teddy we've got a grizzly so you get the idea right I'm going to look at these different I'm going to look at data augmentation next week so I'm going to skip over data augmentation and let's just jump straight into training your model so once we've got DLs we can just like in lesson one call CNN learner to create a resnet we're going to create a smaller resnet this time a resnet 18 again asking for error rate we can then call dot fine tune again so you see it's all the same lines of code we've already seen and you can see our error rate goes down from 9 to 1 so we've got 1% error and after training for about 25 seconds so you can see you know we've only got 450 images we've trained for well less than a minute and we only have let's look at the confusion matrix so we can say I want to create a classification interpretation class I want to look at the confusion matrix and the confusion matrix as you can see it's something that says for things that are actually black bears how many are predicted to be black bears versus grizzly bears versus teddy bears so the diagonal are the ones that are all correct and so it looks like we've got two errors we've got one grizzly that was predicted to be black one black that was predicted to be grizzly super super useful method is plot top losses and that'll actually show me what my errors actually look like so this one here was predicted to be a grizzly bear but the label was black bear this one was the one that's predicted to be a black bear and the label was grizzly bear these ones here are not actually wrong there this is predicted to be black and it's actually black but the reason they appear in this is because these are the ones that the model was the least confident about okay so we're going to look at image classifier cleaner next week let's focus on how we then get this into production so to get it into production we need to export the model so what exporting the model does is it creates a new file which by default is called export.pickle which contains the architecture and all of the parameters of the model so that is now something that you can copy over to a server somewhere and treat it as a predefined program right so then so the the process of using your trained model on new data kind of in production is called inference so here I've created an inference learner by loading that learner back again right and so obviously it doesn't make sense to do it right next to after I've saved it in in a notebook but I'm just showing you how it would work right so this is something that you would do on your server in inference and remember that once you have trained a model you can just treat it as a program you can pass inputs to it so this is now our our program this is our bear predictor so I can now call predict on it and I can pass it an image and it will tell me here is it is 99.999% sure that this is a grizzly so I think what we're going to do here is we're going to wrap it up here and next week we'll finish off by creating an actual GUI for our bear classifier we will show how to run it for free on a service called binder and yeah and then I think we'll be ready to dive into some of the some of the details of what's going on behind the scenes any questions or anything else before we wrap up Rachel now okay great all right thanks everybody so we hopefully yeah I think from here on we've covered you know most of the key kind of underlying foundational stuff from a machine learning point of view that we're going to need to cover so we'll be able to ready to dive into lower level details of how deep learning works behind the scenes and I think that'll be starting from next week so see you then.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.28, "text": " So, hello everybody and welcome back to Practical Deep Learning for Coders. This is lesson two.", "tokens": [407, 11, 7751, 2201, 293, 2928, 646, 281, 19170, 804, 14895, 15205, 337, 383, 378, 433, 13, 639, 307, 6898, 732, 13], "temperature": 0.0, "avg_logprob": -0.1580750906645362, "compression_ratio": 1.4361702127659575, "no_speech_prob": 0.0012445882894098759}, {"id": 1, "seek": 0, "start": 11.28, "end": 17.72, "text": " And in the last lesson, we started training our first models. We didn't really have any", "tokens": [400, 294, 264, 1036, 6898, 11, 321, 1409, 3097, 527, 700, 5245, 13, 492, 994, 380, 534, 362, 604], "temperature": 0.0, "avg_logprob": -0.1580750906645362, "compression_ratio": 1.4361702127659575, "no_speech_prob": 0.0012445882894098759}, {"id": 2, "seek": 0, "start": 17.72, "end": 21.88, "text": " idea how that training was really working, but we were looking at a high level at what", "tokens": [1558, 577, 300, 3097, 390, 534, 1364, 11, 457, 321, 645, 1237, 412, 257, 1090, 1496, 412, 437], "temperature": 0.0, "avg_logprob": -0.1580750906645362, "compression_ratio": 1.4361702127659575, "no_speech_prob": 0.0012445882894098759}, {"id": 3, "seek": 2188, "start": 21.88, "end": 34.64, "text": " was going on. And we learned about what is machine learning and how does that work. And", "tokens": [390, 516, 322, 13, 400, 321, 3264, 466, 437, 307, 3479, 2539, 293, 577, 775, 300, 589, 13, 400], "temperature": 0.0, "avg_logprob": -0.16220530252608042, "compression_ratio": 1.8223684210526316, "no_speech_prob": 5.225044151302427e-05}, {"id": 4, "seek": 2188, "start": 34.64, "end": 41.64, "text": " we realized that based on how machine learning worked, that there are some fundamental limitations", "tokens": [321, 5334, 300, 2361, 322, 577, 3479, 2539, 2732, 11, 300, 456, 366, 512, 8088, 15705], "temperature": 0.0, "avg_logprob": -0.16220530252608042, "compression_ratio": 1.8223684210526316, "no_speech_prob": 5.225044151302427e-05}, {"id": 5, "seek": 2188, "start": 41.64, "end": 47.0, "text": " on what it can do. And we talked about some of those limitations. And we also talked about", "tokens": [322, 437, 309, 393, 360, 13, 400, 321, 2825, 466, 512, 295, 729, 15705, 13, 400, 321, 611, 2825, 466], "temperature": 0.0, "avg_logprob": -0.16220530252608042, "compression_ratio": 1.8223684210526316, "no_speech_prob": 5.225044151302427e-05}, {"id": 6, "seek": 4700, "start": 47.0, "end": 52.68, "text": " how after you've trained a machine learning model, you end up with a program which behaves", "tokens": [577, 934, 291, 600, 8895, 257, 3479, 2539, 2316, 11, 291, 917, 493, 365, 257, 1461, 597, 36896], "temperature": 0.0, "avg_logprob": -0.10718749209148128, "compression_ratio": 1.6682464454976302, "no_speech_prob": 1.2411081115715206e-05}, {"id": 7, "seek": 4700, "start": 52.68, "end": 58.92, "text": " much like a normal program, something with inputs and a thing in the middle and outputs.", "tokens": [709, 411, 257, 2710, 1461, 11, 746, 365, 15743, 293, 257, 551, 294, 264, 2808, 293, 23930, 13], "temperature": 0.0, "avg_logprob": -0.10718749209148128, "compression_ratio": 1.6682464454976302, "no_speech_prob": 1.2411081115715206e-05}, {"id": 8, "seek": 4700, "start": 58.92, "end": 64.68, "text": " So today we're going to finish up talking about talking about that. And we're going", "tokens": [407, 965, 321, 434, 516, 281, 2413, 493, 1417, 466, 1417, 466, 300, 13, 400, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.10718749209148128, "compression_ratio": 1.6682464454976302, "no_speech_prob": 1.2411081115715206e-05}, {"id": 9, "seek": 4700, "start": 64.68, "end": 68.64, "text": " to then look at how we get those models into production and what some of the issues with", "tokens": [281, 550, 574, 412, 577, 321, 483, 729, 5245, 666, 4265, 293, 437, 512, 295, 264, 2663, 365], "temperature": 0.0, "avg_logprob": -0.10718749209148128, "compression_ratio": 1.6682464454976302, "no_speech_prob": 1.2411081115715206e-05}, {"id": 10, "seek": 6864, "start": 68.64, "end": 79.12, "text": " doing that might be. I wanted to remind you that there are two sets of notebooks available", "tokens": [884, 300, 1062, 312, 13, 286, 1415, 281, 4160, 291, 300, 456, 366, 732, 6352, 295, 43782, 2435], "temperature": 0.0, "avg_logprob": -0.11410410012771834, "compression_ratio": 1.4860335195530727, "no_speech_prob": 5.01464273838792e-06}, {"id": 11, "seek": 6864, "start": 79.12, "end": 88.04, "text": " to you. One is the Fastbook repo, the full actual notebooks containing all the text of", "tokens": [281, 291, 13, 1485, 307, 264, 15968, 2939, 49040, 11, 264, 1577, 3539, 43782, 19273, 439, 264, 2487, 295], "temperature": 0.0, "avg_logprob": -0.11410410012771834, "compression_ratio": 1.4860335195530727, "no_speech_prob": 5.01464273838792e-06}, {"id": 12, "seek": 6864, "start": 88.04, "end": 95.68, "text": " the O'Reilly book. And so this lets you see everything that I'm telling you in much more", "tokens": [264, 422, 6, 8524, 6917, 1446, 13, 400, 370, 341, 6653, 291, 536, 1203, 300, 286, 478, 3585, 291, 294, 709, 544], "temperature": 0.0, "avg_logprob": -0.11410410012771834, "compression_ratio": 1.4860335195530727, "no_speech_prob": 5.01464273838792e-06}, {"id": 13, "seek": 9568, "start": 95.68, "end": 102.52000000000001, "text": " detail. And then as well as that, there's the course v4 repo, which contains exactly", "tokens": [2607, 13, 400, 550, 382, 731, 382, 300, 11, 456, 311, 264, 1164, 371, 19, 49040, 11, 597, 8306, 2293], "temperature": 0.0, "avg_logprob": -0.0896447032963464, "compression_ratio": 1.71875, "no_speech_prob": 1.2098590786990826e-06}, {"id": 14, "seek": 9568, "start": 102.52000000000001, "end": 107.84, "text": " the same notebooks, but with all the pros stripped away to help you study. So that's", "tokens": [264, 912, 43782, 11, 457, 365, 439, 264, 6267, 33221, 1314, 281, 854, 291, 2979, 13, 407, 300, 311], "temperature": 0.0, "avg_logprob": -0.0896447032963464, "compression_ratio": 1.71875, "no_speech_prob": 1.2098590786990826e-06}, {"id": 15, "seek": 9568, "start": 107.84, "end": 113.28, "text": " where you really want to be doing your experiment and your practice. And so maybe as you listen", "tokens": [689, 291, 534, 528, 281, 312, 884, 428, 5120, 293, 428, 3124, 13, 400, 370, 1310, 382, 291, 2140], "temperature": 0.0, "avg_logprob": -0.0896447032963464, "compression_ratio": 1.71875, "no_speech_prob": 1.2098590786990826e-06}, {"id": 16, "seek": 9568, "start": 113.28, "end": 118.02000000000001, "text": " to the video, you can kind of switch back and forth between the video and reading or", "tokens": [281, 264, 960, 11, 291, 393, 733, 295, 3679, 646, 293, 5220, 1296, 264, 960, 293, 3760, 420], "temperature": 0.0, "avg_logprob": -0.0896447032963464, "compression_ratio": 1.71875, "no_speech_prob": 1.2098590786990826e-06}, {"id": 17, "seek": 9568, "start": 118.02000000000001, "end": 122.96000000000001, "text": " do one and then the other and then put it away and have a look at the course v4 notebooks", "tokens": [360, 472, 293, 550, 264, 661, 293, 550, 829, 309, 1314, 293, 362, 257, 574, 412, 264, 1164, 371, 19, 43782], "temperature": 0.0, "avg_logprob": -0.0896447032963464, "compression_ratio": 1.71875, "no_speech_prob": 1.2098590786990826e-06}, {"id": 18, "seek": 12296, "start": 122.96, "end": 127.33999999999999, "text": " and try to remember like, okay, what was this section about and run the code and see what", "tokens": [293, 853, 281, 1604, 411, 11, 1392, 11, 437, 390, 341, 3541, 466, 293, 1190, 264, 3089, 293, 536, 437], "temperature": 0.0, "avg_logprob": -0.10549563453311012, "compression_ratio": 1.6859903381642511, "no_speech_prob": 2.0904483335471014e-06}, {"id": 19, "seek": 12296, "start": 127.33999999999999, "end": 138.79999999999998, "text": " happens and change it and so forth. So we were looking at this line of code where we", "tokens": [2314, 293, 1319, 309, 293, 370, 5220, 13, 407, 321, 645, 1237, 412, 341, 1622, 295, 3089, 689, 321], "temperature": 0.0, "avg_logprob": -0.10549563453311012, "compression_ratio": 1.6859903381642511, "no_speech_prob": 2.0904483335471014e-06}, {"id": 20, "seek": 12296, "start": 138.79999999999998, "end": 147.9, "text": " looked at how we created our data by passing in information, perhaps most importantly,", "tokens": [2956, 412, 577, 321, 2942, 527, 1412, 538, 8437, 294, 1589, 11, 4317, 881, 8906, 11], "temperature": 0.0, "avg_logprob": -0.10549563453311012, "compression_ratio": 1.6859903381642511, "no_speech_prob": 2.0904483335471014e-06}, {"id": 21, "seek": 12296, "start": 147.9, "end": 152.06, "text": " some way to label the data. And we talked about the importance of labeling. And in this", "tokens": [512, 636, 281, 7645, 264, 1412, 13, 400, 321, 2825, 466, 264, 7379, 295, 40244, 13, 400, 294, 341], "temperature": 0.0, "avg_logprob": -0.10549563453311012, "compression_ratio": 1.6859903381642511, "no_speech_prob": 2.0904483335471014e-06}, {"id": 22, "seek": 15206, "start": 152.06, "end": 156.9, "text": " case, this particular data set, whether it's a cat or a dog, you can tell by whether it's", "tokens": [1389, 11, 341, 1729, 1412, 992, 11, 1968, 309, 311, 257, 3857, 420, 257, 3000, 11, 291, 393, 980, 538, 1968, 309, 311], "temperature": 0.0, "avg_logprob": -0.13097810745239258, "compression_ratio": 1.71484375, "no_speech_prob": 1.0845133147086017e-06}, {"id": 23, "seek": 15206, "start": 156.9, "end": 161.56, "text": " an uppercase or a lowercase letter in the first position. That's just how this data", "tokens": [364, 11775, 2869, 651, 420, 257, 3126, 9765, 5063, 294, 264, 700, 2535, 13, 663, 311, 445, 577, 341, 1412], "temperature": 0.0, "avg_logprob": -0.13097810745239258, "compression_ratio": 1.71484375, "no_speech_prob": 1.0845133147086017e-06}, {"id": 24, "seek": 15206, "start": 161.56, "end": 166.92000000000002, "text": " set, if they tell you in the readme works. And we also looked particularly at this idea", "tokens": [992, 11, 498, 436, 980, 291, 294, 264, 1401, 1398, 1985, 13, 400, 321, 611, 2956, 4098, 412, 341, 1558], "temperature": 0.0, "avg_logprob": -0.13097810745239258, "compression_ratio": 1.71484375, "no_speech_prob": 1.0845133147086017e-06}, {"id": 25, "seek": 15206, "start": 166.92000000000002, "end": 173.36, "text": " of a valid percent equals 0.2 and like, what does that mean? It creates a validation set.", "tokens": [295, 257, 7363, 3043, 6915, 1958, 13, 17, 293, 411, 11, 437, 775, 300, 914, 30, 467, 7829, 257, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.13097810745239258, "compression_ratio": 1.71484375, "no_speech_prob": 1.0845133147086017e-06}, {"id": 26, "seek": 15206, "start": 173.36, "end": 178.98000000000002, "text": " And that was something I wanted to talk more about. The first thing I want to do though", "tokens": [400, 300, 390, 746, 286, 1415, 281, 751, 544, 466, 13, 440, 700, 551, 286, 528, 281, 360, 1673], "temperature": 0.0, "avg_logprob": -0.13097810745239258, "compression_ratio": 1.71484375, "no_speech_prob": 1.0845133147086017e-06}, {"id": 27, "seek": 17898, "start": 178.98, "end": 187.14, "text": " is point out that this particular labeling function returns something that's either true", "tokens": [307, 935, 484, 300, 341, 1729, 40244, 2445, 11247, 746, 300, 311, 2139, 2074], "temperature": 0.0, "avg_logprob": -0.11640457673506303, "compression_ratio": 1.5964125560538116, "no_speech_prob": 2.2959106900088955e-06}, {"id": 28, "seek": 17898, "start": 187.14, "end": 194.56, "text": " or false. And actually this data set, as we'll see later, also tells, also contains the actual", "tokens": [420, 7908, 13, 400, 767, 341, 1412, 992, 11, 382, 321, 603, 536, 1780, 11, 611, 5112, 11, 611, 8306, 264, 3539], "temperature": 0.0, "avg_logprob": -0.11640457673506303, "compression_ratio": 1.5964125560538116, "no_speech_prob": 2.2959106900088955e-06}, {"id": 29, "seek": 17898, "start": 194.56, "end": 200.01999999999998, "text": " breed of 37 different cat and dog breeds. So you can, you can also grab that from the", "tokens": [18971, 295, 13435, 819, 3857, 293, 3000, 41609, 13, 407, 291, 393, 11, 291, 393, 611, 4444, 300, 490, 264], "temperature": 0.0, "avg_logprob": -0.11640457673506303, "compression_ratio": 1.5964125560538116, "no_speech_prob": 2.2959106900088955e-06}, {"id": 30, "seek": 17898, "start": 200.01999999999998, "end": 207.29999999999998, "text": " file name. In each of those two cases, we're trying to predict a category. Is it a cat", "tokens": [3991, 1315, 13, 682, 1184, 295, 729, 732, 3331, 11, 321, 434, 1382, 281, 6069, 257, 7719, 13, 1119, 309, 257, 3857], "temperature": 0.0, "avg_logprob": -0.11640457673506303, "compression_ratio": 1.5964125560538116, "no_speech_prob": 2.2959106900088955e-06}, {"id": 31, "seek": 20730, "start": 207.3, "end": 215.96, "text": " or is it a dog? Or is it a German shepherd or a beagle or a ragdoll cat or whatever.", "tokens": [420, 307, 309, 257, 3000, 30, 1610, 307, 309, 257, 6521, 40317, 420, 257, 312, 15088, 420, 257, 17539, 67, 1833, 3857, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.10678373357301117, "compression_ratio": 1.6431924882629108, "no_speech_prob": 6.893606041558087e-07}, {"id": 32, "seek": 20730, "start": 215.96, "end": 220.76000000000002, "text": " When you're trying to predict a category, so when the label is a category, we call that", "tokens": [1133, 291, 434, 1382, 281, 6069, 257, 7719, 11, 370, 562, 264, 7645, 307, 257, 7719, 11, 321, 818, 300], "temperature": 0.0, "avg_logprob": -0.10678373357301117, "compression_ratio": 1.6431924882629108, "no_speech_prob": 6.893606041558087e-07}, {"id": 33, "seek": 20730, "start": 220.76000000000002, "end": 229.8, "text": " a classification model. On the other hand, you might try to predict how old is the animal", "tokens": [257, 21538, 2316, 13, 1282, 264, 661, 1011, 11, 291, 1062, 853, 281, 6069, 577, 1331, 307, 264, 5496], "temperature": 0.0, "avg_logprob": -0.10678373357301117, "compression_ratio": 1.6431924882629108, "no_speech_prob": 6.893606041558087e-07}, {"id": 34, "seek": 20730, "start": 229.8, "end": 235.70000000000002, "text": " or how tall is it? Or something like that, which is like a continuous number that could", "tokens": [420, 577, 6764, 307, 309, 30, 1610, 746, 411, 300, 11, 597, 307, 411, 257, 10957, 1230, 300, 727], "temperature": 0.0, "avg_logprob": -0.10678373357301117, "compression_ratio": 1.6431924882629108, "no_speech_prob": 6.893606041558087e-07}, {"id": 35, "seek": 23570, "start": 235.7, "end": 242.33999999999997, "text": " be like 13.2 or 26.5 or whatever. Anytime you're trying to predict a number, the label", "tokens": [312, 411, 3705, 13, 17, 420, 7551, 13, 20, 420, 2035, 13, 39401, 291, 434, 1382, 281, 6069, 257, 1230, 11, 264, 7645], "temperature": 0.0, "avg_logprob": -0.12654641958383414, "compression_ratio": 1.6984732824427482, "no_speech_prob": 4.181183612672612e-07}, {"id": 36, "seek": 23570, "start": 242.33999999999997, "end": 248.94, "text": " is a number, you call that regression. Okay, so those are the two main types of model,", "tokens": [307, 257, 1230, 11, 291, 818, 300, 24590, 13, 1033, 11, 370, 729, 366, 264, 732, 2135, 3467, 295, 2316, 11], "temperature": 0.0, "avg_logprob": -0.12654641958383414, "compression_ratio": 1.6984732824427482, "no_speech_prob": 4.181183612672612e-07}, {"id": 37, "seek": 23570, "start": 248.94, "end": 253.45999999999998, "text": " classification and regression. So this is very important jargon to know about. So the", "tokens": [21538, 293, 24590, 13, 407, 341, 307, 588, 1021, 15181, 10660, 281, 458, 466, 13, 407, 264], "temperature": 0.0, "avg_logprob": -0.12654641958383414, "compression_ratio": 1.6984732824427482, "no_speech_prob": 4.181183612672612e-07}, {"id": 38, "seek": 23570, "start": 253.45999999999998, "end": 258.14, "text": " regression model attempts to predict one or more numeric quantities such as temperature", "tokens": [24590, 2316, 15257, 281, 6069, 472, 420, 544, 7866, 299, 22927, 1270, 382, 4292], "temperature": 0.0, "avg_logprob": -0.12654641958383414, "compression_ratio": 1.6984732824427482, "no_speech_prob": 4.181183612672612e-07}, {"id": 39, "seek": 23570, "start": 258.14, "end": 264.76, "text": " or location or whatever. This is a bit confusing because sometimes people use the word regression", "tokens": [420, 4914, 420, 2035, 13, 639, 307, 257, 857, 13181, 570, 2171, 561, 764, 264, 1349, 24590], "temperature": 0.0, "avg_logprob": -0.12654641958383414, "compression_ratio": 1.6984732824427482, "no_speech_prob": 4.181183612672612e-07}, {"id": 40, "seek": 26476, "start": 264.76, "end": 270.82, "text": " as a shortcut to a particular, like an abbreviation for a particular kind of model called linear", "tokens": [382, 257, 24822, 281, 257, 1729, 11, 411, 364, 35839, 399, 337, 257, 1729, 733, 295, 2316, 1219, 8213], "temperature": 0.0, "avg_logprob": -0.12693968359029517, "compression_ratio": 1.933609958506224, "no_speech_prob": 3.0894814244675217e-06}, {"id": 41, "seek": 26476, "start": 270.82, "end": 277.18, "text": " regression. That's super confusing because that's not what regression means. Linear regression", "tokens": [24590, 13, 663, 311, 1687, 13181, 570, 300, 311, 406, 437, 24590, 1355, 13, 14670, 289, 24590], "temperature": 0.0, "avg_logprob": -0.12693968359029517, "compression_ratio": 1.933609958506224, "no_speech_prob": 3.0894814244675217e-06}, {"id": 42, "seek": 26476, "start": 277.18, "end": 281.38, "text": " is just a particular kind of regression. But I just wanted to warn you of that. When you", "tokens": [307, 445, 257, 1729, 733, 295, 24590, 13, 583, 286, 445, 1415, 281, 12286, 291, 295, 300, 13, 1133, 291], "temperature": 0.0, "avg_logprob": -0.12693968359029517, "compression_ratio": 1.933609958506224, "no_speech_prob": 3.0894814244675217e-06}, {"id": 43, "seek": 26476, "start": 281.38, "end": 287.42, "text": " start talking about regression, a lot of people will assume you're talking about linear regression,", "tokens": [722, 1417, 466, 24590, 11, 257, 688, 295, 561, 486, 6552, 291, 434, 1417, 466, 8213, 24590, 11], "temperature": 0.0, "avg_logprob": -0.12693968359029517, "compression_ratio": 1.933609958506224, "no_speech_prob": 3.0894814244675217e-06}, {"id": 44, "seek": 26476, "start": 287.42, "end": 292.82, "text": " even though that's not what the word means. All right, so I wanted to talk about this", "tokens": [754, 1673, 300, 311, 406, 437, 264, 1349, 1355, 13, 1057, 558, 11, 370, 286, 1415, 281, 751, 466, 341], "temperature": 0.0, "avg_logprob": -0.12693968359029517, "compression_ratio": 1.933609958506224, "no_speech_prob": 3.0894814244675217e-06}, {"id": 45, "seek": 29282, "start": 292.82, "end": 300.65999999999997, "text": " valid percent 0.2 thing. So as we described, valid percent grabs in this case, 20% of the", "tokens": [7363, 3043, 1958, 13, 17, 551, 13, 407, 382, 321, 7619, 11, 7363, 3043, 30028, 294, 341, 1389, 11, 945, 4, 295, 264], "temperature": 0.0, "avg_logprob": -0.1710834379320021, "compression_ratio": 1.5224719101123596, "no_speech_prob": 4.785080363944871e-06}, {"id": 46, "seek": 29282, "start": 300.65999999999997, "end": 306.5, "text": " data, it's 0.2 and puts it aside like in a separate bucket. And then when you train your", "tokens": [1412, 11, 309, 311, 1958, 13, 17, 293, 8137, 309, 7359, 411, 294, 257, 4994, 13058, 13, 400, 550, 562, 291, 3847, 428], "temperature": 0.0, "avg_logprob": -0.1710834379320021, "compression_ratio": 1.5224719101123596, "no_speech_prob": 4.785080363944871e-06}, {"id": 47, "seek": 29282, "start": 306.5, "end": 314.94, "text": " model, your model doesn't get to look at that data at all. That data is only used to decide,", "tokens": [2316, 11, 428, 2316, 1177, 380, 483, 281, 574, 412, 300, 1412, 412, 439, 13, 663, 1412, 307, 787, 1143, 281, 4536, 11], "temperature": 0.0, "avg_logprob": -0.1710834379320021, "compression_ratio": 1.5224719101123596, "no_speech_prob": 4.785080363944871e-06}, {"id": 48, "seek": 31494, "start": 314.94, "end": 324.42, "text": " to show you how accurate your model is. So if you train for too long and or with not", "tokens": [281, 855, 291, 577, 8559, 428, 2316, 307, 13, 407, 498, 291, 3847, 337, 886, 938, 293, 420, 365, 406], "temperature": 0.0, "avg_logprob": -0.09664748055594308, "compression_ratio": 1.5611111111111111, "no_speech_prob": 5.043470991950016e-07}, {"id": 49, "seek": 31494, "start": 324.42, "end": 330.7, "text": " enough data and or a model with too many parameters, after a while the accuracy of your model will", "tokens": [1547, 1412, 293, 420, 257, 2316, 365, 886, 867, 9834, 11, 934, 257, 1339, 264, 14170, 295, 428, 2316, 486], "temperature": 0.0, "avg_logprob": -0.09664748055594308, "compression_ratio": 1.5611111111111111, "no_speech_prob": 5.043470991950016e-07}, {"id": 50, "seek": 31494, "start": 330.7, "end": 337.9, "text": " actually get worse. And this is called overfitting, right? So we use the validation set to ensure", "tokens": [767, 483, 5324, 13, 400, 341, 307, 1219, 670, 69, 2414, 11, 558, 30, 407, 321, 764, 264, 24071, 992, 281, 5586], "temperature": 0.0, "avg_logprob": -0.09664748055594308, "compression_ratio": 1.5611111111111111, "no_speech_prob": 5.043470991950016e-07}, {"id": 51, "seek": 33790, "start": 337.9, "end": 346.29999999999995, "text": " that we're not overfitting. The next line of code that we looked at is this one, where", "tokens": [300, 321, 434, 406, 670, 69, 2414, 13, 440, 958, 1622, 295, 3089, 300, 321, 2956, 412, 307, 341, 472, 11, 689], "temperature": 0.0, "avg_logprob": -0.10851920632755055, "compression_ratio": 1.6854460093896713, "no_speech_prob": 6.577925546480401e-07}, {"id": 52, "seek": 33790, "start": 346.29999999999995, "end": 350.26, "text": " we created something called a learner. We'll be learning a lot more about that. But a learner", "tokens": [321, 2942, 746, 1219, 257, 33347, 13, 492, 603, 312, 2539, 257, 688, 544, 466, 300, 13, 583, 257, 33347], "temperature": 0.0, "avg_logprob": -0.10851920632755055, "compression_ratio": 1.6854460093896713, "no_speech_prob": 6.577925546480401e-07}, {"id": 53, "seek": 33790, "start": 350.26, "end": 357.53999999999996, "text": " is basically or is something which contains your data and your architecture. That is the", "tokens": [307, 1936, 420, 307, 746, 597, 8306, 428, 1412, 293, 428, 9482, 13, 663, 307, 264], "temperature": 0.0, "avg_logprob": -0.10851920632755055, "compression_ratio": 1.6854460093896713, "no_speech_prob": 6.577925546480401e-07}, {"id": 54, "seek": 33790, "start": 357.53999999999996, "end": 363.59999999999997, "text": " mathematical function that you're optimizing. And so a learner is the thing that tries to", "tokens": [18894, 2445, 300, 291, 434, 40425, 13, 400, 370, 257, 33347, 307, 264, 551, 300, 9898, 281], "temperature": 0.0, "avg_logprob": -0.10851920632755055, "compression_ratio": 1.6854460093896713, "no_speech_prob": 6.577925546480401e-07}, {"id": 55, "seek": 36360, "start": 363.6, "end": 370.90000000000003, "text": " figure out what are the parameters which best cause this function to match the labels in", "tokens": [2573, 484, 437, 366, 264, 9834, 597, 1151, 3082, 341, 2445, 281, 2995, 264, 16949, 294], "temperature": 0.0, "avg_logprob": -0.1599695637540997, "compression_ratio": 1.6472727272727272, "no_speech_prob": 1.5056972415550263e-06}, {"id": 56, "seek": 36360, "start": 370.90000000000003, "end": 376.78000000000003, "text": " this data. So we're talking a lot more about that. But basically this particular function,", "tokens": [341, 1412, 13, 407, 321, 434, 1417, 257, 688, 544, 466, 300, 13, 583, 1936, 341, 1729, 2445, 11], "temperature": 0.0, "avg_logprob": -0.1599695637540997, "compression_ratio": 1.6472727272727272, "no_speech_prob": 1.5056972415550263e-06}, {"id": 57, "seek": 36360, "start": 376.78000000000003, "end": 383.1, "text": " ResNet34, is the name of a particular architecture, which is just very good for computer vision", "tokens": [5015, 31890, 12249, 11, 307, 264, 1315, 295, 257, 1729, 9482, 11, 597, 307, 445, 588, 665, 337, 3820, 5201], "temperature": 0.0, "avg_logprob": -0.1599695637540997, "compression_ratio": 1.6472727272727272, "no_speech_prob": 1.5056972415550263e-06}, {"id": 58, "seek": 36360, "start": 383.1, "end": 389.18, "text": " problems. In fact, the name really is ResNet, and then 34 tells you how many layers there", "tokens": [2740, 13, 682, 1186, 11, 264, 1315, 534, 307, 5015, 31890, 11, 293, 550, 12790, 5112, 291, 577, 867, 7914, 456], "temperature": 0.0, "avg_logprob": -0.1599695637540997, "compression_ratio": 1.6472727272727272, "no_speech_prob": 1.5056972415550263e-06}, {"id": 59, "seek": 36360, "start": 389.18, "end": 393.42, "text": " are. So you can use ones with bigger numbers here to get more parameters that will take", "tokens": [366, 13, 407, 291, 393, 764, 2306, 365, 3801, 3547, 510, 281, 483, 544, 9834, 300, 486, 747], "temperature": 0.0, "avg_logprob": -0.1599695637540997, "compression_ratio": 1.6472727272727272, "no_speech_prob": 1.5056972415550263e-06}, {"id": 60, "seek": 39342, "start": 393.42, "end": 401.42, "text": " longer to train, take more memory, more likely to overfit, but could also create more complex", "tokens": [2854, 281, 3847, 11, 747, 544, 4675, 11, 544, 3700, 281, 670, 6845, 11, 457, 727, 611, 1884, 544, 3997], "temperature": 0.0, "avg_logprob": -0.11044731768932971, "compression_ratio": 1.5903083700440528, "no_speech_prob": 1.2878925872428226e-06}, {"id": 61, "seek": 39342, "start": 401.42, "end": 406.34000000000003, "text": " models. Right now, though, I wanted to focus on this part here, which is metrics equals", "tokens": [5245, 13, 1779, 586, 11, 1673, 11, 286, 1415, 281, 1879, 322, 341, 644, 510, 11, 597, 307, 16367, 6915], "temperature": 0.0, "avg_logprob": -0.11044731768932971, "compression_ratio": 1.5903083700440528, "no_speech_prob": 1.2878925872428226e-06}, {"id": 62, "seek": 39342, "start": 406.34000000000003, "end": 414.1, "text": " error rate. This is where you list the functions that you want to be called with your data,", "tokens": [6713, 3314, 13, 639, 307, 689, 291, 1329, 264, 6828, 300, 291, 528, 281, 312, 1219, 365, 428, 1412, 11], "temperature": 0.0, "avg_logprob": -0.11044731768932971, "compression_ratio": 1.5903083700440528, "no_speech_prob": 1.2878925872428226e-06}, {"id": 63, "seek": 39342, "start": 414.1, "end": 421.52000000000004, "text": " with your validation data, and print it out after each epoch. And epoch is what we call", "tokens": [365, 428, 24071, 1412, 11, 293, 4482, 309, 484, 934, 1184, 30992, 339, 13, 400, 30992, 339, 307, 437, 321, 818], "temperature": 0.0, "avg_logprob": -0.11044731768932971, "compression_ratio": 1.5903083700440528, "no_speech_prob": 1.2878925872428226e-06}, {"id": 64, "seek": 42152, "start": 421.52, "end": 427.26, "text": " it when you look at every single image in the data set once. And so after you've looked", "tokens": [309, 562, 291, 574, 412, 633, 2167, 3256, 294, 264, 1412, 992, 1564, 13, 400, 370, 934, 291, 600, 2956], "temperature": 0.0, "avg_logprob": -0.08237519945417132, "compression_ratio": 1.777327935222672, "no_speech_prob": 1.3497000281859073e-06}, {"id": 65, "seek": 42152, "start": 427.26, "end": 432.9, "text": " at every image in the data set once, we print out some information about how you're doing.", "tokens": [412, 633, 3256, 294, 264, 1412, 992, 1564, 11, 321, 4482, 484, 512, 1589, 466, 577, 291, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.08237519945417132, "compression_ratio": 1.777327935222672, "no_speech_prob": 1.3497000281859073e-06}, {"id": 66, "seek": 42152, "start": 432.9, "end": 438.18, "text": " And the most important thing we print out is the result of calling these metrics. So", "tokens": [400, 264, 881, 1021, 551, 321, 4482, 484, 307, 264, 1874, 295, 5141, 613, 16367, 13, 407], "temperature": 0.0, "avg_logprob": -0.08237519945417132, "compression_ratio": 1.777327935222672, "no_speech_prob": 1.3497000281859073e-06}, {"id": 67, "seek": 42152, "start": 438.18, "end": 442.79999999999995, "text": " error rate is the name of a metric, and it's a function that just prints out what percent", "tokens": [6713, 3314, 307, 264, 1315, 295, 257, 20678, 11, 293, 309, 311, 257, 2445, 300, 445, 22305, 484, 437, 3043], "temperature": 0.0, "avg_logprob": -0.08237519945417132, "compression_ratio": 1.777327935222672, "no_speech_prob": 1.3497000281859073e-06}, {"id": 68, "seek": 42152, "start": 442.79999999999995, "end": 450.78, "text": " of the validation set are being incorrectly classified by your model. So a metrics of", "tokens": [295, 264, 24071, 992, 366, 885, 42892, 20627, 538, 428, 2316, 13, 407, 257, 16367, 295], "temperature": 0.0, "avg_logprob": -0.08237519945417132, "compression_ratio": 1.777327935222672, "no_speech_prob": 1.3497000281859073e-06}, {"id": 69, "seek": 45078, "start": 450.78, "end": 456.26, "text": " function that measures the quality of the predictions using the validation set. So error", "tokens": [2445, 300, 8000, 264, 3125, 295, 264, 21264, 1228, 264, 24071, 992, 13, 407, 6713], "temperature": 0.0, "avg_logprob": -0.11121753410056785, "compression_ratio": 1.59375, "no_speech_prob": 3.2058264309853257e-07}, {"id": 70, "seek": 45078, "start": 456.26, "end": 462.17999999999995, "text": " rate is 1. Another common metric is accuracy, which is just 1 minus error rate. So very", "tokens": [3314, 307, 502, 13, 3996, 2689, 20678, 307, 14170, 11, 597, 307, 445, 502, 3175, 6713, 3314, 13, 407, 588], "temperature": 0.0, "avg_logprob": -0.11121753410056785, "compression_ratio": 1.59375, "no_speech_prob": 3.2058264309853257e-07}, {"id": 71, "seek": 45078, "start": 462.17999999999995, "end": 469.41999999999996, "text": " important to remember from last week, we talked about loss. Arthur Samuel had this important", "tokens": [1021, 281, 1604, 490, 1036, 1243, 11, 321, 2825, 466, 4470, 13, 19624, 23036, 632, 341, 1021], "temperature": 0.0, "avg_logprob": -0.11121753410056785, "compression_ratio": 1.59375, "no_speech_prob": 3.2058264309853257e-07}, {"id": 72, "seek": 45078, "start": 469.41999999999996, "end": 476.53999999999996, "text": " idea in machine learning that we need some way to figure out how good our, how well our", "tokens": [1558, 294, 3479, 2539, 300, 321, 643, 512, 636, 281, 2573, 484, 577, 665, 527, 11, 577, 731, 527], "temperature": 0.0, "avg_logprob": -0.11121753410056785, "compression_ratio": 1.59375, "no_speech_prob": 3.2058264309853257e-07}, {"id": 73, "seek": 47654, "start": 476.54, "end": 480.90000000000003, "text": " model is doing so that when we change the parameters, we can figure out which set of", "tokens": [2316, 307, 884, 370, 300, 562, 321, 1319, 264, 9834, 11, 321, 393, 2573, 484, 597, 992, 295], "temperature": 0.0, "avg_logprob": -0.09415172367561155, "compression_ratio": 1.710144927536232, "no_speech_prob": 8.446214110335859e-07}, {"id": 74, "seek": 47654, "start": 480.90000000000003, "end": 486.62, "text": " parameters make that performance measurement get better or worse. That performance measurement", "tokens": [9834, 652, 300, 3389, 13160, 483, 1101, 420, 5324, 13, 663, 3389, 13160], "temperature": 0.0, "avg_logprob": -0.09415172367561155, "compression_ratio": 1.710144927536232, "no_speech_prob": 8.446214110335859e-07}, {"id": 75, "seek": 47654, "start": 486.62, "end": 496.22, "text": " is called the loss. The loss is not necessarily the same as your metric. The reason why is", "tokens": [307, 1219, 264, 4470, 13, 440, 4470, 307, 406, 4725, 264, 912, 382, 428, 20678, 13, 440, 1778, 983, 307], "temperature": 0.0, "avg_logprob": -0.09415172367561155, "compression_ratio": 1.710144927536232, "no_speech_prob": 8.446214110335859e-07}, {"id": 76, "seek": 47654, "start": 496.22, "end": 499.70000000000005, "text": " a bit subtle, and we'll be seeing it in a lot of detail once we delve into the math", "tokens": [257, 857, 13743, 11, 293, 321, 603, 312, 2577, 309, 294, 257, 688, 295, 2607, 1564, 321, 43098, 666, 264, 5221], "temperature": 0.0, "avg_logprob": -0.09415172367561155, "compression_ratio": 1.710144927536232, "no_speech_prob": 8.446214110335859e-07}, {"id": 77, "seek": 49970, "start": 499.7, "end": 508.42, "text": " in the coming lessons. But basically you need a function. You need a loss function where", "tokens": [294, 264, 1348, 8820, 13, 583, 1936, 291, 643, 257, 2445, 13, 509, 643, 257, 4470, 2445, 689], "temperature": 0.0, "avg_logprob": -0.10332183837890625, "compression_ratio": 1.8898305084745763, "no_speech_prob": 1.9333544969413197e-06}, {"id": 78, "seek": 49970, "start": 508.42, "end": 512.5, "text": " if you change the parameters by just a little bit up or just a little bit down, you can", "tokens": [498, 291, 1319, 264, 9834, 538, 445, 257, 707, 857, 493, 420, 445, 257, 707, 857, 760, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.10332183837890625, "compression_ratio": 1.8898305084745763, "no_speech_prob": 1.9333544969413197e-06}, {"id": 79, "seek": 49970, "start": 512.5, "end": 516.42, "text": " see if the loss gets a little bit better or a little bit worse. And it turns out that", "tokens": [536, 498, 264, 4470, 2170, 257, 707, 857, 1101, 420, 257, 707, 857, 5324, 13, 400, 309, 4523, 484, 300], "temperature": 0.0, "avg_logprob": -0.10332183837890625, "compression_ratio": 1.8898305084745763, "no_speech_prob": 1.9333544969413197e-06}, {"id": 80, "seek": 49970, "start": 516.42, "end": 521.98, "text": " error rate and accuracy doesn't tell you that at all because you might change the parameters", "tokens": [6713, 3314, 293, 14170, 1177, 380, 980, 291, 300, 412, 439, 570, 291, 1062, 1319, 264, 9834], "temperature": 0.0, "avg_logprob": -0.10332183837890625, "compression_ratio": 1.8898305084745763, "no_speech_prob": 1.9333544969413197e-06}, {"id": 81, "seek": 49970, "start": 521.98, "end": 527.84, "text": " by such a small amount that none of your dog's predictions start becoming cats and none of", "tokens": [538, 1270, 257, 1359, 2372, 300, 6022, 295, 428, 3000, 311, 21264, 722, 5617, 11111, 293, 6022, 295], "temperature": 0.0, "avg_logprob": -0.10332183837890625, "compression_ratio": 1.8898305084745763, "no_speech_prob": 1.9333544969413197e-06}, {"id": 82, "seek": 52784, "start": 527.84, "end": 531.72, "text": " your cat predictions start becoming dogs. So like your predictions don't change and", "tokens": [428, 3857, 21264, 722, 5617, 7197, 13, 407, 411, 428, 21264, 500, 380, 1319, 293], "temperature": 0.0, "avg_logprob": -0.08417764267364106, "compression_ratio": 1.7373737373737375, "no_speech_prob": 3.7479867387446575e-07}, {"id": 83, "seek": 52784, "start": 531.72, "end": 537.5, "text": " your error rate doesn't change. So loss and metric are closely related, but the metric", "tokens": [428, 6713, 3314, 1177, 380, 1319, 13, 407, 4470, 293, 20678, 366, 8185, 4077, 11, 457, 264, 20678], "temperature": 0.0, "avg_logprob": -0.08417764267364106, "compression_ratio": 1.7373737373737375, "no_speech_prob": 3.7479867387446575e-07}, {"id": 84, "seek": 52784, "start": 537.5, "end": 543.26, "text": " is the thing that you care about. The loss is the thing which your computer is using", "tokens": [307, 264, 551, 300, 291, 1127, 466, 13, 440, 4470, 307, 264, 551, 597, 428, 3820, 307, 1228], "temperature": 0.0, "avg_logprob": -0.08417764267364106, "compression_ratio": 1.7373737373737375, "no_speech_prob": 3.7479867387446575e-07}, {"id": 85, "seek": 52784, "start": 543.26, "end": 552.0600000000001, "text": " as the measurement of performance to decide how to update your parameters. So we measure", "tokens": [382, 264, 13160, 295, 3389, 281, 4536, 577, 281, 5623, 428, 9834, 13, 407, 321, 3481], "temperature": 0.0, "avg_logprob": -0.08417764267364106, "compression_ratio": 1.7373737373737375, "no_speech_prob": 3.7479867387446575e-07}, {"id": 86, "seek": 55206, "start": 552.06, "end": 558.9399999999999, "text": " overfitting by looking at the metrics on the validation set. So Fast.ai always uses the", "tokens": [670, 69, 2414, 538, 1237, 412, 264, 16367, 322, 264, 24071, 992, 13, 407, 15968, 13, 1301, 1009, 4960, 264], "temperature": 0.0, "avg_logprob": -0.09745207498239916, "compression_ratio": 1.70935960591133, "no_speech_prob": 7.338193768191559e-07}, {"id": 87, "seek": 55206, "start": 558.9399999999999, "end": 565.3, "text": " validation set to print out your metrics. And overfitting is like the key thing that", "tokens": [24071, 992, 281, 4482, 484, 428, 16367, 13, 400, 670, 69, 2414, 307, 411, 264, 2141, 551, 300], "temperature": 0.0, "avg_logprob": -0.09745207498239916, "compression_ratio": 1.70935960591133, "no_speech_prob": 7.338193768191559e-07}, {"id": 88, "seek": 55206, "start": 565.3, "end": 572.4, "text": " machine learning is about. It's all about how do we find a model which fits the data,", "tokens": [3479, 2539, 307, 466, 13, 467, 311, 439, 466, 577, 360, 321, 915, 257, 2316, 597, 9001, 264, 1412, 11], "temperature": 0.0, "avg_logprob": -0.09745207498239916, "compression_ratio": 1.70935960591133, "no_speech_prob": 7.338193768191559e-07}, {"id": 89, "seek": 55206, "start": 572.4, "end": 577.3199999999999, "text": " not just for the data that we're training with, but for data that the training algorithm", "tokens": [406, 445, 337, 264, 1412, 300, 321, 434, 3097, 365, 11, 457, 337, 1412, 300, 264, 3097, 9284], "temperature": 0.0, "avg_logprob": -0.09745207498239916, "compression_ratio": 1.70935960591133, "no_speech_prob": 7.338193768191559e-07}, {"id": 90, "seek": 57732, "start": 577.32, "end": 589.22, "text": " hasn't seen before. So overfitting results when our model is basically cheating. Our", "tokens": [6132, 380, 1612, 949, 13, 407, 670, 69, 2414, 3542, 562, 527, 2316, 307, 1936, 18309, 13, 2621], "temperature": 0.0, "avg_logprob": -0.12816309247698102, "compression_ratio": 1.5274725274725274, "no_speech_prob": 1.8738708149612648e-06}, {"id": 91, "seek": 57732, "start": 589.22, "end": 596.1800000000001, "text": " model can cheat by saying, oh, I've seen this exact picture before and I remember that that's", "tokens": [2316, 393, 17470, 538, 1566, 11, 1954, 11, 286, 600, 1612, 341, 1900, 3036, 949, 293, 286, 1604, 300, 300, 311], "temperature": 0.0, "avg_logprob": -0.12816309247698102, "compression_ratio": 1.5274725274725274, "no_speech_prob": 1.8738708149612648e-06}, {"id": 92, "seek": 57732, "start": 596.1800000000001, "end": 601.9000000000001, "text": " a picture of a cat. So it might not have learned what cats look like in general. It just remembers,", "tokens": [257, 3036, 295, 257, 3857, 13, 407, 309, 1062, 406, 362, 3264, 437, 11111, 574, 411, 294, 2674, 13, 467, 445, 26228, 11], "temperature": 0.0, "avg_logprob": -0.12816309247698102, "compression_ratio": 1.5274725274725274, "no_speech_prob": 1.8738708149612648e-06}, {"id": 93, "seek": 60190, "start": 601.9, "end": 607.42, "text": " you know, that images one, four, and eight are cats and two and three and five are dogs", "tokens": [291, 458, 11, 300, 5267, 472, 11, 1451, 11, 293, 3180, 366, 11111, 293, 732, 293, 1045, 293, 1732, 366, 7197], "temperature": 0.0, "avg_logprob": -0.11553179251181113, "compression_ratio": 1.7170542635658914, "no_speech_prob": 3.927856084828818e-07}, {"id": 94, "seek": 60190, "start": 607.42, "end": 612.14, "text": " and learns nothing actually about what they really look like. So that's the kind of cheating", "tokens": [293, 27152, 1825, 767, 466, 437, 436, 534, 574, 411, 13, 407, 300, 311, 264, 733, 295, 18309], "temperature": 0.0, "avg_logprob": -0.11553179251181113, "compression_ratio": 1.7170542635658914, "no_speech_prob": 3.927856084828818e-07}, {"id": 95, "seek": 60190, "start": 612.14, "end": 618.5799999999999, "text": " that we're trying to avoid. We don't want it to memorize our particular data set. So", "tokens": [300, 321, 434, 1382, 281, 5042, 13, 492, 500, 380, 528, 309, 281, 27478, 527, 1729, 1412, 992, 13, 407], "temperature": 0.0, "avg_logprob": -0.11553179251181113, "compression_ratio": 1.7170542635658914, "no_speech_prob": 3.927856084828818e-07}, {"id": 96, "seek": 60190, "start": 618.5799999999999, "end": 623.18, "text": " we split off our validation data and most of these words you're seeing on the screen", "tokens": [321, 7472, 766, 527, 24071, 1412, 293, 881, 295, 613, 2283, 291, 434, 2577, 322, 264, 2568], "temperature": 0.0, "avg_logprob": -0.11553179251181113, "compression_ratio": 1.7170542635658914, "no_speech_prob": 3.927856084828818e-07}, {"id": 97, "seek": 60190, "start": 623.18, "end": 628.86, "text": " are from the book. Okay, so I just copied and pasted them. So if we split off our validation", "tokens": [366, 490, 264, 1446, 13, 1033, 11, 370, 286, 445, 25365, 293, 1791, 292, 552, 13, 407, 498, 321, 7472, 766, 527, 24071], "temperature": 0.0, "avg_logprob": -0.11553179251181113, "compression_ratio": 1.7170542635658914, "no_speech_prob": 3.927856084828818e-07}, {"id": 98, "seek": 62886, "start": 628.86, "end": 634.02, "text": " data and make sure that our model sees it during training, it's completely untainted", "tokens": [1412, 293, 652, 988, 300, 527, 2316, 8194, 309, 1830, 3097, 11, 309, 311, 2584, 1701, 26278], "temperature": 0.0, "avg_logprob": -0.08248334412181049, "compression_ratio": 1.909871244635193, "no_speech_prob": 4.936975074087968e-06}, {"id": 99, "seek": 62886, "start": 634.02, "end": 641.42, "text": " by it, so we can't possibly cheat. Not quite true. We can cheat. The way we could cheat", "tokens": [538, 309, 11, 370, 321, 393, 380, 6264, 17470, 13, 1726, 1596, 2074, 13, 492, 393, 17470, 13, 440, 636, 321, 727, 17470], "temperature": 0.0, "avg_logprob": -0.08248334412181049, "compression_ratio": 1.909871244635193, "no_speech_prob": 4.936975074087968e-06}, {"id": 100, "seek": 62886, "start": 641.42, "end": 646.66, "text": " is we could run, we could fit a model, look at the result in the validation set, change", "tokens": [307, 321, 727, 1190, 11, 321, 727, 3318, 257, 2316, 11, 574, 412, 264, 1874, 294, 264, 24071, 992, 11, 1319], "temperature": 0.0, "avg_logprob": -0.08248334412181049, "compression_ratio": 1.909871244635193, "no_speech_prob": 4.936975074087968e-06}, {"id": 101, "seek": 62886, "start": 646.66, "end": 650.54, "text": " something a little bit, fit another model, look at the validation set, change something", "tokens": [746, 257, 707, 857, 11, 3318, 1071, 2316, 11, 574, 412, 264, 24071, 992, 11, 1319, 746], "temperature": 0.0, "avg_logprob": -0.08248334412181049, "compression_ratio": 1.909871244635193, "no_speech_prob": 4.936975074087968e-06}, {"id": 102, "seek": 62886, "start": 650.54, "end": 654.98, "text": " a little bit. We could do that like a hundred times until we find something where the validation", "tokens": [257, 707, 857, 13, 492, 727, 360, 300, 411, 257, 3262, 1413, 1826, 321, 915, 746, 689, 264, 24071], "temperature": 0.0, "avg_logprob": -0.08248334412181049, "compression_ratio": 1.909871244635193, "no_speech_prob": 4.936975074087968e-06}, {"id": 103, "seek": 65498, "start": 654.98, "end": 661.46, "text": " set looks the best. But now we might have fit to the validation set, right? So if you", "tokens": [992, 1542, 264, 1151, 13, 583, 586, 321, 1062, 362, 3318, 281, 264, 24071, 992, 11, 558, 30, 407, 498, 291], "temperature": 0.0, "avg_logprob": -0.08821892481978222, "compression_ratio": 1.6415929203539823, "no_speech_prob": 2.7264643449598225e-06}, {"id": 104, "seek": 65498, "start": 661.46, "end": 666.62, "text": " want to be really rigorous about this, you should actually set aside a third bit of data", "tokens": [528, 281, 312, 534, 29882, 466, 341, 11, 291, 820, 767, 992, 7359, 257, 2636, 857, 295, 1412], "temperature": 0.0, "avg_logprob": -0.08821892481978222, "compression_ratio": 1.6415929203539823, "no_speech_prob": 2.7264643449598225e-06}, {"id": 105, "seek": 65498, "start": 666.62, "end": 673.34, "text": " called the test set that is not used for training and it's not used for your metrics. It's actually,", "tokens": [1219, 264, 1500, 992, 300, 307, 406, 1143, 337, 3097, 293, 309, 311, 406, 1143, 337, 428, 16367, 13, 467, 311, 767, 11], "temperature": 0.0, "avg_logprob": -0.08821892481978222, "compression_ratio": 1.6415929203539823, "no_speech_prob": 2.7264643449598225e-06}, {"id": 106, "seek": 65498, "start": 673.34, "end": 677.98, "text": " you don't look at it until the whole project's finished. And this is what's used on competition", "tokens": [291, 500, 380, 574, 412, 309, 1826, 264, 1379, 1716, 311, 4335, 13, 400, 341, 307, 437, 311, 1143, 322, 6211], "temperature": 0.0, "avg_logprob": -0.08821892481978222, "compression_ratio": 1.6415929203539823, "no_speech_prob": 2.7264643449598225e-06}, {"id": 107, "seek": 67798, "start": 677.98, "end": 685.74, "text": " platforms like Kaggle. On Kaggle, after the competition finishes, your performance will", "tokens": [9473, 411, 48751, 22631, 13, 1282, 48751, 22631, 11, 934, 264, 6211, 23615, 11, 428, 3389, 486], "temperature": 0.0, "avg_logprob": -0.1124030490254247, "compression_ratio": 1.599078341013825, "no_speech_prob": 1.3574322110798676e-07}, {"id": 108, "seek": 67798, "start": 685.74, "end": 694.9, "text": " be measured against a data set that you have never seen. And so that's a really helpful", "tokens": [312, 12690, 1970, 257, 1412, 992, 300, 291, 362, 1128, 1612, 13, 400, 370, 300, 311, 257, 534, 4961], "temperature": 0.0, "avg_logprob": -0.1124030490254247, "compression_ratio": 1.599078341013825, "no_speech_prob": 1.3574322110798676e-07}, {"id": 109, "seek": 67798, "start": 694.9, "end": 699.52, "text": " approach. And it's actually a great idea to do that, like even if you're not doing the", "tokens": [3109, 13, 400, 309, 311, 767, 257, 869, 1558, 281, 360, 300, 11, 411, 754, 498, 291, 434, 406, 884, 264], "temperature": 0.0, "avg_logprob": -0.1124030490254247, "compression_ratio": 1.599078341013825, "no_speech_prob": 1.3574322110798676e-07}, {"id": 110, "seek": 67798, "start": 699.52, "end": 705.94, "text": " modeling yourself. So if you're, if you're looking at vendors and you're just trying", "tokens": [15983, 1803, 13, 407, 498, 291, 434, 11, 498, 291, 434, 1237, 412, 22056, 293, 291, 434, 445, 1382], "temperature": 0.0, "avg_logprob": -0.1124030490254247, "compression_ratio": 1.599078341013825, "no_speech_prob": 1.3574322110798676e-07}, {"id": 111, "seek": 70594, "start": 705.94, "end": 710.6, "text": " to decide, should I go with IBM or Google or Microsoft and they're all showing you how", "tokens": [281, 4536, 11, 820, 286, 352, 365, 23487, 420, 3329, 420, 8116, 293, 436, 434, 439, 4099, 291, 577], "temperature": 0.0, "avg_logprob": -0.10503661755434017, "compression_ratio": 1.6635071090047393, "no_speech_prob": 3.96696304960642e-06}, {"id": 112, "seek": 70594, "start": 710.6, "end": 715.98, "text": " great their models are. What you should do is you should say, okay, you go and build", "tokens": [869, 641, 5245, 366, 13, 708, 291, 820, 360, 307, 291, 820, 584, 11, 1392, 11, 291, 352, 293, 1322], "temperature": 0.0, "avg_logprob": -0.10503661755434017, "compression_ratio": 1.6635071090047393, "no_speech_prob": 3.96696304960642e-06}, {"id": 113, "seek": 70594, "start": 715.98, "end": 721.32, "text": " your models and I am going to hang on to 10% of my data and I'm not going to let you see", "tokens": [428, 5245, 293, 286, 669, 516, 281, 3967, 322, 281, 1266, 4, 295, 452, 1412, 293, 286, 478, 406, 516, 281, 718, 291, 536], "temperature": 0.0, "avg_logprob": -0.10503661755434017, "compression_ratio": 1.6635071090047393, "no_speech_prob": 3.96696304960642e-06}, {"id": 114, "seek": 70594, "start": 721.32, "end": 727.1800000000001, "text": " it at all. And when you're all finished, come back and then I'll run your model on the 10%", "tokens": [309, 412, 439, 13, 400, 562, 291, 434, 439, 4335, 11, 808, 646, 293, 550, 286, 603, 1190, 428, 2316, 322, 264, 1266, 4], "temperature": 0.0, "avg_logprob": -0.10503661755434017, "compression_ratio": 1.6635071090047393, "no_speech_prob": 3.96696304960642e-06}, {"id": 115, "seek": 72718, "start": 727.18, "end": 736.78, "text": " of data you've never seen. Now pulling out your validation and test sets is a bit subtle", "tokens": [295, 1412, 291, 600, 1128, 1612, 13, 823, 8407, 484, 428, 24071, 293, 1500, 6352, 307, 257, 857, 13743], "temperature": 0.0, "avg_logprob": -0.10814810373696936, "compression_ratio": 1.5803571428571428, "no_speech_prob": 2.699572689834895e-07}, {"id": 116, "seek": 72718, "start": 736.78, "end": 743.4599999999999, "text": " though. Here's an example of a simple little data set. And this comes from a fantastic", "tokens": [1673, 13, 1692, 311, 364, 1365, 295, 257, 2199, 707, 1412, 992, 13, 400, 341, 1487, 490, 257, 5456], "temperature": 0.0, "avg_logprob": -0.10814810373696936, "compression_ratio": 1.5803571428571428, "no_speech_prob": 2.699572689834895e-07}, {"id": 117, "seek": 72718, "start": 743.4599999999999, "end": 749.9799999999999, "text": " blog post that Rachel wrote that we will link to about creating effective validation sets.", "tokens": [6968, 2183, 300, 14246, 4114, 300, 321, 486, 2113, 281, 466, 4084, 4942, 24071, 6352, 13], "temperature": 0.0, "avg_logprob": -0.10814810373696936, "compression_ratio": 1.5803571428571428, "no_speech_prob": 2.699572689834895e-07}, {"id": 118, "seek": 72718, "start": 749.9799999999999, "end": 754.8599999999999, "text": " And you can see basically you have some kind of seasonal data set here. Now if you just", "tokens": [400, 291, 393, 536, 1936, 291, 362, 512, 733, 295, 27421, 1412, 992, 510, 13, 823, 498, 291, 445], "temperature": 0.0, "avg_logprob": -0.10814810373696936, "compression_ratio": 1.5803571428571428, "no_speech_prob": 2.699572689834895e-07}, {"id": 119, "seek": 75486, "start": 754.86, "end": 762.14, "text": " say, okay, fast AI, I want to model that. I want to create my data loader using a valid", "tokens": [584, 11, 1392, 11, 2370, 7318, 11, 286, 528, 281, 2316, 300, 13, 286, 528, 281, 1884, 452, 1412, 3677, 260, 1228, 257, 7363], "temperature": 0.0, "avg_logprob": -0.12859006179006477, "compression_ratio": 1.6388888888888888, "no_speech_prob": 3.689881964419328e-07}, {"id": 120, "seek": 75486, "start": 762.14, "end": 771.82, "text": " percent of 0.2. It would do this. It would delete randomly some of the dots, right? Now", "tokens": [3043, 295, 1958, 13, 17, 13, 467, 576, 360, 341, 13, 467, 576, 12097, 16979, 512, 295, 264, 15026, 11, 558, 30, 823], "temperature": 0.0, "avg_logprob": -0.12859006179006477, "compression_ratio": 1.6388888888888888, "no_speech_prob": 3.689881964419328e-07}, {"id": 121, "seek": 75486, "start": 771.82, "end": 777.58, "text": " this isn't very helpful because it's, we can still cheat because these dots are right in", "tokens": [341, 1943, 380, 588, 4961, 570, 309, 311, 11, 321, 393, 920, 17470, 570, 613, 15026, 366, 558, 294], "temperature": 0.0, "avg_logprob": -0.12859006179006477, "compression_ratio": 1.6388888888888888, "no_speech_prob": 3.689881964419328e-07}, {"id": 122, "seek": 75486, "start": 777.58, "end": 781.32, "text": " the middle of other dots. And this isn't what would happen in practice. What would happen", "tokens": [264, 2808, 295, 661, 15026, 13, 400, 341, 1943, 380, 437, 576, 1051, 294, 3124, 13, 708, 576, 1051], "temperature": 0.0, "avg_logprob": -0.12859006179006477, "compression_ratio": 1.6388888888888888, "no_speech_prob": 3.689881964419328e-07}, {"id": 123, "seek": 78132, "start": 781.32, "end": 785.98, "text": " in practice is we would want to predict this as sales by date, right? We want to predict", "tokens": [294, 3124, 307, 321, 576, 528, 281, 6069, 341, 382, 5763, 538, 4002, 11, 558, 30, 492, 528, 281, 6069], "temperature": 0.0, "avg_logprob": -0.11646362682720562, "compression_ratio": 1.697674418604651, "no_speech_prob": 7.112424782462767e-07}, {"id": 124, "seek": 78132, "start": 785.98, "end": 792.9000000000001, "text": " the sales for next week, not the sales for 14 days ago, 18 days ago, and 29 days ago,", "tokens": [264, 5763, 337, 958, 1243, 11, 406, 264, 5763, 337, 3499, 1708, 2057, 11, 2443, 1708, 2057, 11, 293, 9413, 1708, 2057, 11], "temperature": 0.0, "avg_logprob": -0.11646362682720562, "compression_ratio": 1.697674418604651, "no_speech_prob": 7.112424782462767e-07}, {"id": 125, "seek": 78132, "start": 792.9000000000001, "end": 796.6600000000001, "text": " right? So what you actually need to do to create an effective validation set here is", "tokens": [558, 30, 407, 437, 291, 767, 643, 281, 360, 281, 1884, 364, 4942, 24071, 992, 510, 307], "temperature": 0.0, "avg_logprob": -0.11646362682720562, "compression_ratio": 1.697674418604651, "no_speech_prob": 7.112424782462767e-07}, {"id": 126, "seek": 78132, "start": 796.6600000000001, "end": 804.1400000000001, "text": " not do it randomly, but instead chop off the end, right? And so this is what happens in", "tokens": [406, 360, 309, 16979, 11, 457, 2602, 7931, 766, 264, 917, 11, 558, 30, 400, 370, 341, 307, 437, 2314, 294], "temperature": 0.0, "avg_logprob": -0.11646362682720562, "compression_ratio": 1.697674418604651, "no_speech_prob": 7.112424782462767e-07}, {"id": 127, "seek": 78132, "start": 804.1400000000001, "end": 809.2600000000001, "text": " all Kaggle competitions pretty much that involve time, for instance, is the thing that you", "tokens": [439, 48751, 22631, 26185, 1238, 709, 300, 9494, 565, 11, 337, 5197, 11, 307, 264, 551, 300, 291], "temperature": 0.0, "avg_logprob": -0.11646362682720562, "compression_ratio": 1.697674418604651, "no_speech_prob": 7.112424782462767e-07}, {"id": 128, "seek": 80926, "start": 809.26, "end": 814.58, "text": " have to predict is the next like two weeks or so after the last data point that they", "tokens": [362, 281, 6069, 307, 264, 958, 411, 732, 3259, 420, 370, 934, 264, 1036, 1412, 935, 300, 436], "temperature": 0.0, "avg_logprob": -0.1463675340016683, "compression_ratio": 1.6836363636363636, "no_speech_prob": 3.668841372928e-06}, {"id": 129, "seek": 80926, "start": 814.58, "end": 820.02, "text": " give you. And this is what you should do also for your test set. So again, if you've got", "tokens": [976, 291, 13, 400, 341, 307, 437, 291, 820, 360, 611, 337, 428, 1500, 992, 13, 407, 797, 11, 498, 291, 600, 658], "temperature": 0.0, "avg_logprob": -0.1463675340016683, "compression_ratio": 1.6836363636363636, "no_speech_prob": 3.668841372928e-06}, {"id": 130, "seek": 80926, "start": 820.02, "end": 825.02, "text": " vendors that you're looking at, you should say to them, okay, after you're all done modeling,", "tokens": [22056, 300, 291, 434, 1237, 412, 11, 291, 820, 584, 281, 552, 11, 1392, 11, 934, 291, 434, 439, 1096, 15983, 11], "temperature": 0.0, "avg_logprob": -0.1463675340016683, "compression_ratio": 1.6836363636363636, "no_speech_prob": 3.668841372928e-06}, {"id": 131, "seek": 80926, "start": 825.02, "end": 830.18, "text": " we're going to check your model against data that is one week later than you've ever seen", "tokens": [321, 434, 516, 281, 1520, 428, 2316, 1970, 1412, 300, 307, 472, 1243, 1780, 813, 291, 600, 1562, 1612], "temperature": 0.0, "avg_logprob": -0.1463675340016683, "compression_ratio": 1.6836363636363636, "no_speech_prob": 3.668841372928e-06}, {"id": 132, "seek": 80926, "start": 830.18, "end": 835.14, "text": " before and you won't be able to retrain or anything because that's what happens in practice,", "tokens": [949, 293, 291, 1582, 380, 312, 1075, 281, 1533, 7146, 420, 1340, 570, 300, 311, 437, 2314, 294, 3124, 11], "temperature": 0.0, "avg_logprob": -0.1463675340016683, "compression_ratio": 1.6836363636363636, "no_speech_prob": 3.668841372928e-06}, {"id": 133, "seek": 80926, "start": 835.14, "end": 836.14, "text": " right? Okay.", "tokens": [558, 30, 1033, 13], "temperature": 0.0, "avg_logprob": -0.1463675340016683, "compression_ratio": 1.6836363636363636, "no_speech_prob": 3.668841372928e-06}, {"id": 134, "seek": 83614, "start": 836.14, "end": 842.74, "text": " There's a question. I've heard people describe overfitting as training error being below", "tokens": [821, 311, 257, 1168, 13, 286, 600, 2198, 561, 6786, 670, 69, 2414, 382, 3097, 6713, 885, 2507], "temperature": 0.0, "avg_logprob": -0.09367219457086527, "compression_ratio": 1.8319672131147542, "no_speech_prob": 2.1568077954725595e-06}, {"id": 135, "seek": 83614, "start": 842.74, "end": 847.74, "text": " validation error. Does this rule of thumb end up being roughly the same as yours?", "tokens": [24071, 6713, 13, 4402, 341, 4978, 295, 9298, 917, 493, 885, 9810, 264, 912, 382, 6342, 30], "temperature": 0.0, "avg_logprob": -0.09367219457086527, "compression_ratio": 1.8319672131147542, "no_speech_prob": 2.1568077954725595e-06}, {"id": 136, "seek": 83614, "start": 847.74, "end": 853.58, "text": " Okay, so that's a great question. So I think what they mean there is training loss versus", "tokens": [1033, 11, 370, 300, 311, 257, 869, 1168, 13, 407, 286, 519, 437, 436, 914, 456, 307, 3097, 4470, 5717], "temperature": 0.0, "avg_logprob": -0.09367219457086527, "compression_ratio": 1.8319672131147542, "no_speech_prob": 2.1568077954725595e-06}, {"id": 137, "seek": 83614, "start": 853.58, "end": 861.06, "text": " validation loss because we don't print training error. So we do print at the end of each epoch", "tokens": [24071, 4470, 570, 321, 500, 380, 4482, 3097, 6713, 13, 407, 321, 360, 4482, 412, 264, 917, 295, 1184, 30992, 339], "temperature": 0.0, "avg_logprob": -0.09367219457086527, "compression_ratio": 1.8319672131147542, "no_speech_prob": 2.1568077954725595e-06}, {"id": 138, "seek": 83614, "start": 861.06, "end": 865.38, "text": " the value of your loss function for the training set and the value of the loss function for", "tokens": [264, 2158, 295, 428, 4470, 2445, 337, 264, 3097, 992, 293, 264, 2158, 295, 264, 4470, 2445, 337], "temperature": 0.0, "avg_logprob": -0.09367219457086527, "compression_ratio": 1.8319672131147542, "no_speech_prob": 2.1568077954725595e-06}, {"id": 139, "seek": 86538, "start": 865.38, "end": 874.02, "text": " the validation set. And if you train for long enough, so if it's training nicely, your training", "tokens": [264, 24071, 992, 13, 400, 498, 291, 3847, 337, 938, 1547, 11, 370, 498, 309, 311, 3097, 9594, 11, 428, 3097], "temperature": 0.0, "avg_logprob": -0.11651999300176447, "compression_ratio": 1.829145728643216, "no_speech_prob": 3.866962288157083e-07}, {"id": 140, "seek": 86538, "start": 874.02, "end": 882.54, "text": " loss will go down and your validation loss will go down because by definition loss function", "tokens": [4470, 486, 352, 760, 293, 428, 24071, 4470, 486, 352, 760, 570, 538, 7123, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.11651999300176447, "compression_ratio": 1.829145728643216, "no_speech_prob": 3.866962288157083e-07}, {"id": 141, "seek": 86538, "start": 882.54, "end": 888.9, "text": " is defined such as a lower loss function is a better model. If you start overfitting,", "tokens": [307, 7642, 1270, 382, 257, 3126, 4470, 2445, 307, 257, 1101, 2316, 13, 759, 291, 722, 670, 69, 2414, 11], "temperature": 0.0, "avg_logprob": -0.11651999300176447, "compression_ratio": 1.829145728643216, "no_speech_prob": 3.866962288157083e-07}, {"id": 142, "seek": 86538, "start": 888.9, "end": 894.74, "text": " your training loss will keep going down, right? Because like, why wouldn't that, you know,", "tokens": [428, 3097, 4470, 486, 1066, 516, 760, 11, 558, 30, 1436, 411, 11, 983, 2759, 380, 300, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.11651999300176447, "compression_ratio": 1.829145728643216, "no_speech_prob": 3.866962288157083e-07}, {"id": 143, "seek": 89474, "start": 894.74, "end": 900.1800000000001, "text": " you're getting better and better parameters. But your validation loss will start to go", "tokens": [291, 434, 1242, 1101, 293, 1101, 9834, 13, 583, 428, 24071, 4470, 486, 722, 281, 352], "temperature": 0.0, "avg_logprob": -0.11468834247229234, "compression_ratio": 1.8695652173913044, "no_speech_prob": 6.577920999006892e-07}, {"id": 144, "seek": 89474, "start": 900.1800000000001, "end": 904.7, "text": " up because actually you've started fitting to the specific data points in the training", "tokens": [493, 570, 767, 291, 600, 1409, 15669, 281, 264, 2685, 1412, 2793, 294, 264, 3097], "temperature": 0.0, "avg_logprob": -0.11468834247229234, "compression_ratio": 1.8695652173913044, "no_speech_prob": 6.577920999006892e-07}, {"id": 145, "seek": 89474, "start": 904.7, "end": 908.34, "text": " set and so it's not going to actually get better. It's going to get, it's not going", "tokens": [992, 293, 370, 309, 311, 406, 516, 281, 767, 483, 1101, 13, 467, 311, 516, 281, 483, 11, 309, 311, 406, 516], "temperature": 0.0, "avg_logprob": -0.11468834247229234, "compression_ratio": 1.8695652173913044, "no_speech_prob": 6.577920999006892e-07}, {"id": 146, "seek": 89474, "start": 908.34, "end": 915.0600000000001, "text": " to get better for the validation set. It'll start to get worse. However, that does not", "tokens": [281, 483, 1101, 337, 264, 24071, 992, 13, 467, 603, 722, 281, 483, 5324, 13, 2908, 11, 300, 775, 406], "temperature": 0.0, "avg_logprob": -0.11468834247229234, "compression_ratio": 1.8695652173913044, "no_speech_prob": 6.577920999006892e-07}, {"id": 147, "seek": 89474, "start": 915.0600000000001, "end": 919.58, "text": " necessarily mean that you're overfitting or at least not overfitting in a bad way. As", "tokens": [4725, 914, 300, 291, 434, 670, 69, 2414, 420, 412, 1935, 406, 670, 69, 2414, 294, 257, 1578, 636, 13, 1018], "temperature": 0.0, "avg_logprob": -0.11468834247229234, "compression_ratio": 1.8695652173913044, "no_speech_prob": 6.577920999006892e-07}, {"id": 148, "seek": 91958, "start": 919.58, "end": 925.46, "text": " we'll see, it's actually possible to be at a point where the validation loss is getting", "tokens": [321, 603, 536, 11, 309, 311, 767, 1944, 281, 312, 412, 257, 935, 689, 264, 24071, 4470, 307, 1242], "temperature": 0.0, "avg_logprob": -0.11588479237384107, "compression_ratio": 1.5633187772925765, "no_speech_prob": 4.81250935990829e-07}, {"id": 149, "seek": 91958, "start": 925.46, "end": 932.0600000000001, "text": " worse, but the validation accuracy or error or metric is still improving. So I'm not going", "tokens": [5324, 11, 457, 264, 24071, 14170, 420, 6713, 420, 20678, 307, 920, 11470, 13, 407, 286, 478, 406, 516], "temperature": 0.0, "avg_logprob": -0.11588479237384107, "compression_ratio": 1.5633187772925765, "no_speech_prob": 4.81250935990829e-07}, {"id": 150, "seek": 91958, "start": 932.0600000000001, "end": 937.26, "text": " to describe how that would happen mathematically yet because we need to learn more about loss", "tokens": [281, 6786, 577, 300, 576, 1051, 44003, 1939, 570, 321, 643, 281, 1466, 544, 466, 4470], "temperature": 0.0, "avg_logprob": -0.11588479237384107, "compression_ratio": 1.5633187772925765, "no_speech_prob": 4.81250935990829e-07}, {"id": 151, "seek": 91958, "start": 937.26, "end": 941.7, "text": " functions, but we will. But for now, just realize that the important thing to look at", "tokens": [6828, 11, 457, 321, 486, 13, 583, 337, 586, 11, 445, 4325, 300, 264, 1021, 551, 281, 574, 412], "temperature": 0.0, "avg_logprob": -0.11588479237384107, "compression_ratio": 1.5633187772925765, "no_speech_prob": 4.81250935990829e-07}, {"id": 152, "seek": 94170, "start": 941.7, "end": 950.7, "text": " is your metric getting worse, not your loss function getting worse. Thank you for that", "tokens": [307, 428, 20678, 1242, 5324, 11, 406, 428, 4470, 2445, 1242, 5324, 13, 1044, 291, 337, 300], "temperature": 0.0, "avg_logprob": -0.15029807829521072, "compression_ratio": 1.625, "no_speech_prob": 5.715000952477567e-07}, {"id": 153, "seek": 94170, "start": 950.7, "end": 959.22, "text": " fantastic question. The next important thing we need to learn about is called transfer", "tokens": [5456, 1168, 13, 440, 958, 1021, 551, 321, 643, 281, 1466, 466, 307, 1219, 5003], "temperature": 0.0, "avg_logprob": -0.15029807829521072, "compression_ratio": 1.625, "no_speech_prob": 5.715000952477567e-07}, {"id": 154, "seek": 94170, "start": 959.22, "end": 967.5, "text": " learning. So the next line of code said learn.fineTune. Why does it say learn.fineTune? FineTune is", "tokens": [2539, 13, 407, 264, 958, 1622, 295, 3089, 848, 1466, 13, 69, 533, 51, 2613, 13, 1545, 775, 309, 584, 1466, 13, 69, 533, 51, 2613, 30, 12024, 51, 2613, 307], "temperature": 0.0, "avg_logprob": -0.15029807829521072, "compression_ratio": 1.625, "no_speech_prob": 5.715000952477567e-07}, {"id": 155, "seek": 96750, "start": 967.5, "end": 972.98, "text": " what we do when we are transfer learning. So transfer learning is using a pre-trained", "tokens": [437, 321, 360, 562, 321, 366, 5003, 2539, 13, 407, 5003, 2539, 307, 1228, 257, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.0984028557599601, "compression_ratio": 1.7176470588235293, "no_speech_prob": 5.368740971789521e-07}, {"id": 156, "seek": 96750, "start": 972.98, "end": 978.42, "text": " model for a task that is different to what it was originally trained for. So more jargon", "tokens": [2316, 337, 257, 5633, 300, 307, 819, 281, 437, 309, 390, 7993, 8895, 337, 13, 407, 544, 15181, 10660], "temperature": 0.0, "avg_logprob": -0.0984028557599601, "compression_ratio": 1.7176470588235293, "no_speech_prob": 5.368740971789521e-07}, {"id": 157, "seek": 96750, "start": 978.42, "end": 983.28, "text": " to understand our jargon. Let's look at that. What's a pre-trained model? So what happens", "tokens": [281, 1223, 527, 15181, 10660, 13, 961, 311, 574, 412, 300, 13, 708, 311, 257, 659, 12, 17227, 2001, 2316, 30, 407, 437, 2314], "temperature": 0.0, "avg_logprob": -0.0984028557599601, "compression_ratio": 1.7176470588235293, "no_speech_prob": 5.368740971789521e-07}, {"id": 158, "seek": 96750, "start": 983.28, "end": 989.06, "text": " is remember I told you the architecture we're using is called ResNet-34. So when we take", "tokens": [307, 1604, 286, 1907, 291, 264, 9482, 321, 434, 1228, 307, 1219, 5015, 31890, 12, 12249, 13, 407, 562, 321, 747], "temperature": 0.0, "avg_logprob": -0.0984028557599601, "compression_ratio": 1.7176470588235293, "no_speech_prob": 5.368740971789521e-07}, {"id": 159, "seek": 96750, "start": 989.06, "end": 994.36, "text": " that ResNet-34, that's just a, it's just a mathematical function, okay, with lots of", "tokens": [300, 5015, 31890, 12, 12249, 11, 300, 311, 445, 257, 11, 309, 311, 445, 257, 18894, 2445, 11, 1392, 11, 365, 3195, 295], "temperature": 0.0, "avg_logprob": -0.0984028557599601, "compression_ratio": 1.7176470588235293, "no_speech_prob": 5.368740971789521e-07}, {"id": 160, "seek": 99436, "start": 994.36, "end": 1002.46, "text": " parameters that we're going to fit using machine learning. There's a big data set called ImageNet", "tokens": [9834, 300, 321, 434, 516, 281, 3318, 1228, 3479, 2539, 13, 821, 311, 257, 955, 1412, 992, 1219, 29903, 31890], "temperature": 0.0, "avg_logprob": -0.10183086614499147, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.4823528999841074e-06}, {"id": 161, "seek": 99436, "start": 1002.46, "end": 1008.62, "text": " that contains 1.3 million pictures of a thousand different types of thing, whether it be mushrooms", "tokens": [300, 8306, 502, 13, 18, 2459, 5242, 295, 257, 4714, 819, 3467, 295, 551, 11, 1968, 309, 312, 17973], "temperature": 0.0, "avg_logprob": -0.10183086614499147, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.4823528999841074e-06}, {"id": 162, "seek": 99436, "start": 1008.62, "end": 1016.34, "text": " or animals or airplanes or hammers or whatever. There's a competition, or there used to be", "tokens": [420, 4882, 420, 32947, 420, 36600, 433, 420, 2035, 13, 821, 311, 257, 6211, 11, 420, 456, 1143, 281, 312], "temperature": 0.0, "avg_logprob": -0.10183086614499147, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.4823528999841074e-06}, {"id": 163, "seek": 99436, "start": 1016.34, "end": 1020.78, "text": " a competition that runs every year to see who could get the best accuracy on the ImageNet", "tokens": [257, 6211, 300, 6676, 633, 1064, 281, 536, 567, 727, 483, 264, 1151, 14170, 322, 264, 29903, 31890], "temperature": 0.0, "avg_logprob": -0.10183086614499147, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.4823528999841074e-06}, {"id": 164, "seek": 102078, "start": 1020.78, "end": 1027.58, "text": " competition. And the models that did really well, people would take those specific values", "tokens": [6211, 13, 400, 264, 5245, 300, 630, 534, 731, 11, 561, 576, 747, 729, 2685, 4190], "temperature": 0.0, "avg_logprob": -0.0760696810119006, "compression_ratio": 1.739622641509434, "no_speech_prob": 2.918935990692262e-07}, {"id": 165, "seek": 102078, "start": 1027.58, "end": 1033.1399999999999, "text": " of those parameters and they would make them available on the internet for anybody to download.", "tokens": [295, 729, 9834, 293, 436, 576, 652, 552, 2435, 322, 264, 4705, 337, 4472, 281, 5484, 13], "temperature": 0.0, "avg_logprob": -0.0760696810119006, "compression_ratio": 1.739622641509434, "no_speech_prob": 2.918935990692262e-07}, {"id": 166, "seek": 102078, "start": 1033.1399999999999, "end": 1037.76, "text": " So if you download that, you don't just have an architecture now, you have a trained model.", "tokens": [407, 498, 291, 5484, 300, 11, 291, 500, 380, 445, 362, 364, 9482, 586, 11, 291, 362, 257, 8895, 2316, 13], "temperature": 0.0, "avg_logprob": -0.0760696810119006, "compression_ratio": 1.739622641509434, "no_speech_prob": 2.918935990692262e-07}, {"id": 167, "seek": 102078, "start": 1037.76, "end": 1046.6, "text": " You have a model that can recognize a thousand categories of thing in images, which probably", "tokens": [509, 362, 257, 2316, 300, 393, 5521, 257, 4714, 10479, 295, 551, 294, 5267, 11, 597, 1391], "temperature": 0.0, "avg_logprob": -0.0760696810119006, "compression_ratio": 1.739622641509434, "no_speech_prob": 2.918935990692262e-07}, {"id": 168, "seek": 102078, "start": 1046.6, "end": 1050.48, "text": " isn't very useful unless you happen to want something that recognizes those exact thousand", "tokens": [1943, 380, 588, 4420, 5969, 291, 1051, 281, 528, 746, 300, 26564, 729, 1900, 4714], "temperature": 0.0, "avg_logprob": -0.0760696810119006, "compression_ratio": 1.739622641509434, "no_speech_prob": 2.918935990692262e-07}, {"id": 169, "seek": 105048, "start": 1050.48, "end": 1058.98, "text": " categories of thing. But it turns out you can, rather, you can start with those weights", "tokens": [10479, 295, 551, 13, 583, 309, 4523, 484, 291, 393, 11, 2831, 11, 291, 393, 722, 365, 729, 17443], "temperature": 0.0, "avg_logprob": -0.10817393436226794, "compression_ratio": 1.6418604651162791, "no_speech_prob": 9.132533591582614e-07}, {"id": 170, "seek": 105048, "start": 1058.98, "end": 1066.78, "text": " in your model and then train some more epochs on your data and you'll end up with a far,", "tokens": [294, 428, 2316, 293, 550, 3847, 512, 544, 30992, 28346, 322, 428, 1412, 293, 291, 603, 917, 493, 365, 257, 1400, 11], "temperature": 0.0, "avg_logprob": -0.10817393436226794, "compression_ratio": 1.6418604651162791, "no_speech_prob": 9.132533591582614e-07}, {"id": 171, "seek": 105048, "start": 1066.78, "end": 1072.02, "text": " far more accurate model than you would if you didn't start with that pre-trained model.", "tokens": [1400, 544, 8559, 2316, 813, 291, 576, 498, 291, 994, 380, 722, 365, 300, 659, 12, 17227, 2001, 2316, 13], "temperature": 0.0, "avg_logprob": -0.10817393436226794, "compression_ratio": 1.6418604651162791, "no_speech_prob": 9.132533591582614e-07}, {"id": 172, "seek": 105048, "start": 1072.02, "end": 1077.3, "text": " And we'll see why in just a moment, right? But this idea of transfer learning, it's kind", "tokens": [400, 321, 603, 536, 983, 294, 445, 257, 1623, 11, 558, 30, 583, 341, 1558, 295, 5003, 2539, 11, 309, 311, 733], "temperature": 0.0, "avg_logprob": -0.10817393436226794, "compression_ratio": 1.6418604651162791, "no_speech_prob": 9.132533591582614e-07}, {"id": 173, "seek": 107730, "start": 1077.3, "end": 1084.26, "text": " of, it makes intuitive sense, right? ImageNet already has some cats and some dogs in it,", "tokens": [295, 11, 309, 1669, 21769, 2020, 11, 558, 30, 29903, 31890, 1217, 575, 512, 11111, 293, 512, 7197, 294, 309, 11], "temperature": 0.0, "avg_logprob": -0.15468162868333898, "compression_ratio": 1.7935222672064777, "no_speech_prob": 3.1381173357658554e-06}, {"id": 174, "seek": 107730, "start": 1084.26, "end": 1087.86, "text": " and it's, you know, it can say this is a cat and this is a dog, but you want to maybe do", "tokens": [293, 309, 311, 11, 291, 458, 11, 309, 393, 584, 341, 307, 257, 3857, 293, 341, 307, 257, 3000, 11, 457, 291, 528, 281, 1310, 360], "temperature": 0.0, "avg_logprob": -0.15468162868333898, "compression_ratio": 1.7935222672064777, "no_speech_prob": 3.1381173357658554e-06}, {"id": 175, "seek": 107730, "start": 1087.86, "end": 1092.62, "text": " something that recognizes lots of breeds that aren't in ImageNet. Well, for it to be able", "tokens": [746, 300, 26564, 3195, 295, 41609, 300, 3212, 380, 294, 29903, 31890, 13, 1042, 11, 337, 309, 281, 312, 1075], "temperature": 0.0, "avg_logprob": -0.15468162868333898, "compression_ratio": 1.7935222672064777, "no_speech_prob": 3.1381173357658554e-06}, {"id": 176, "seek": 107730, "start": 1092.62, "end": 1098.78, "text": " to recognize cats versus dogs versus airplanes versus hammers, it has to understand things", "tokens": [281, 5521, 11111, 5717, 7197, 5717, 32947, 5717, 36600, 433, 11, 309, 575, 281, 1223, 721], "temperature": 0.0, "avg_logprob": -0.15468162868333898, "compression_ratio": 1.7935222672064777, "no_speech_prob": 3.1381173357658554e-06}, {"id": 177, "seek": 107730, "start": 1098.78, "end": 1104.74, "text": " like what does metal look like? What does fur look like? What do ears look like? You", "tokens": [411, 437, 775, 5760, 574, 411, 30, 708, 775, 2687, 574, 411, 30, 708, 360, 8798, 574, 411, 30, 509], "temperature": 0.0, "avg_logprob": -0.15468162868333898, "compression_ratio": 1.7935222672064777, "no_speech_prob": 3.1381173357658554e-06}, {"id": 178, "seek": 110474, "start": 1104.74, "end": 1109.42, "text": " know, so it can say like, oh, this breed of animal, this breed of dog has pointy ears", "tokens": [458, 11, 370, 309, 393, 584, 411, 11, 1954, 11, 341, 18971, 295, 5496, 11, 341, 18971, 295, 3000, 575, 935, 88, 8798], "temperature": 0.0, "avg_logprob": -0.09760322409161067, "compression_ratio": 1.7983539094650205, "no_speech_prob": 7.57113809868315e-07}, {"id": 179, "seek": 110474, "start": 1109.42, "end": 1114.78, "text": " and oh, this thing is metal, so it can't be a dog. So all these kinds of concepts get", "tokens": [293, 1954, 11, 341, 551, 307, 5760, 11, 370, 309, 393, 380, 312, 257, 3000, 13, 407, 439, 613, 3685, 295, 10392, 483], "temperature": 0.0, "avg_logprob": -0.09760322409161067, "compression_ratio": 1.7983539094650205, "no_speech_prob": 7.57113809868315e-07}, {"id": 180, "seek": 110474, "start": 1114.78, "end": 1120.84, "text": " implicitly learnt by a pre-trained model. So if you start with a pre-trained model,", "tokens": [26947, 356, 18991, 538, 257, 659, 12, 17227, 2001, 2316, 13, 407, 498, 291, 722, 365, 257, 659, 12, 17227, 2001, 2316, 11], "temperature": 0.0, "avg_logprob": -0.09760322409161067, "compression_ratio": 1.7983539094650205, "no_speech_prob": 7.57113809868315e-07}, {"id": 181, "seek": 110474, "start": 1120.84, "end": 1126.5, "text": " then you don't, you don't have to learn all these features from scratch. And so transfer", "tokens": [550, 291, 500, 380, 11, 291, 500, 380, 362, 281, 1466, 439, 613, 4122, 490, 8459, 13, 400, 370, 5003], "temperature": 0.0, "avg_logprob": -0.09760322409161067, "compression_ratio": 1.7983539094650205, "no_speech_prob": 7.57113809868315e-07}, {"id": 182, "seek": 110474, "start": 1126.5, "end": 1134.6200000000001, "text": " learning is the single most important thing for being able to use less data and less compute", "tokens": [2539, 307, 264, 2167, 881, 1021, 551, 337, 885, 1075, 281, 764, 1570, 1412, 293, 1570, 14722], "temperature": 0.0, "avg_logprob": -0.09760322409161067, "compression_ratio": 1.7983539094650205, "no_speech_prob": 7.57113809868315e-07}, {"id": 183, "seek": 113462, "start": 1134.62, "end": 1141.1, "text": " and get better accuracy. So that's a key focus for the Fast.ai library and a key focus for", "tokens": [293, 483, 1101, 14170, 13, 407, 300, 311, 257, 2141, 1879, 337, 264, 15968, 13, 1301, 6405, 293, 257, 2141, 1879, 337], "temperature": 0.0, "avg_logprob": -0.27887367557834936, "compression_ratio": 1.5754189944134078, "no_speech_prob": 4.356846147857141e-06}, {"id": 184, "seek": 113462, "start": 1141.1, "end": 1142.34, "text": " this course.", "tokens": [341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.27887367557834936, "compression_ratio": 1.5754189944134078, "no_speech_prob": 4.356846147857141e-06}, {"id": 185, "seek": 113462, "start": 1142.34, "end": 1152.62, "text": " There's a question. I am a bit confused on the differences between loss, error and metric.", "tokens": [821, 311, 257, 1168, 13, 286, 669, 257, 857, 9019, 322, 264, 7300, 1296, 4470, 11, 6713, 293, 20678, 13], "temperature": 0.0, "avg_logprob": -0.27887367557834936, "compression_ratio": 1.5754189944134078, "no_speech_prob": 4.356846147857141e-06}, {"id": 186, "seek": 113462, "start": 1152.62, "end": 1164.58, "text": " Sure. So error is just one kind of metric. So there's lots of different possible errors", "tokens": [4894, 13, 407, 6713, 307, 445, 472, 733, 295, 20678, 13, 407, 456, 311, 3195, 295, 819, 1944, 13603], "temperature": 0.0, "avg_logprob": -0.27887367557834936, "compression_ratio": 1.5754189944134078, "no_speech_prob": 4.356846147857141e-06}, {"id": 187, "seek": 116458, "start": 1164.58, "end": 1169.5, "text": " or labels you could have. Let's say you're trying to create a model which could predict", "tokens": [420, 16949, 291, 727, 362, 13, 961, 311, 584, 291, 434, 1382, 281, 1884, 257, 2316, 597, 727, 6069], "temperature": 0.0, "avg_logprob": -0.10111064099250956, "compression_ratio": 1.6778846153846154, "no_speech_prob": 2.684181481527048e-06}, {"id": 188, "seek": 116458, "start": 1169.5, "end": 1179.02, "text": " how old a cat or a dog is. So the metric you might use is on average, how many years were", "tokens": [577, 1331, 257, 3857, 420, 257, 3000, 307, 13, 407, 264, 20678, 291, 1062, 764, 307, 322, 4274, 11, 577, 867, 924, 645], "temperature": 0.0, "avg_logprob": -0.10111064099250956, "compression_ratio": 1.6778846153846154, "no_speech_prob": 2.684181481527048e-06}, {"id": 189, "seek": 116458, "start": 1179.02, "end": 1186.8999999999999, "text": " you off by? So that would be a metric. On the other hand, if you're trying to predict", "tokens": [291, 766, 538, 30, 407, 300, 576, 312, 257, 20678, 13, 1282, 264, 661, 1011, 11, 498, 291, 434, 1382, 281, 6069], "temperature": 0.0, "avg_logprob": -0.10111064099250956, "compression_ratio": 1.6778846153846154, "no_speech_prob": 2.684181481527048e-06}, {"id": 190, "seek": 116458, "start": 1186.8999999999999, "end": 1192.1399999999999, "text": " whether this is a cat or a dog, your metric would be what percentage of the time am I", "tokens": [1968, 341, 307, 257, 3857, 420, 257, 3000, 11, 428, 20678, 576, 312, 437, 9668, 295, 264, 565, 669, 286], "temperature": 0.0, "avg_logprob": -0.10111064099250956, "compression_ratio": 1.6778846153846154, "no_speech_prob": 2.684181481527048e-06}, {"id": 191, "seek": 119214, "start": 1192.14, "end": 1198.7, "text": " wrong? So that latter metric is called the error rate. Okay, so error is one particular", "tokens": [2085, 30, 407, 300, 18481, 20678, 307, 1219, 264, 6713, 3314, 13, 1033, 11, 370, 6713, 307, 472, 1729], "temperature": 0.0, "avg_logprob": -0.12010397394019437, "compression_ratio": 1.6030927835051547, "no_speech_prob": 4.52093530611819e-07}, {"id": 192, "seek": 119214, "start": 1198.7, "end": 1204.3400000000001, "text": " metric. It's a thing that measures how well you're doing. And it's like it should be the", "tokens": [20678, 13, 467, 311, 257, 551, 300, 8000, 577, 731, 291, 434, 884, 13, 400, 309, 311, 411, 309, 820, 312, 264], "temperature": 0.0, "avg_logprob": -0.12010397394019437, "compression_ratio": 1.6030927835051547, "no_speech_prob": 4.52093530611819e-07}, {"id": 193, "seek": 119214, "start": 1204.3400000000001, "end": 1209.8600000000001, "text": " thing that you most care about. So you write a function or use one of Fast.ai's predefined", "tokens": [551, 300, 291, 881, 1127, 466, 13, 407, 291, 2464, 257, 2445, 420, 764, 472, 295, 15968, 13, 1301, 311, 659, 37716], "temperature": 0.0, "avg_logprob": -0.12010397394019437, "compression_ratio": 1.6030927835051547, "no_speech_prob": 4.52093530611819e-07}, {"id": 194, "seek": 119214, "start": 1209.8600000000001, "end": 1216.48, "text": " ones, which measures how well you're doing.", "tokens": [2306, 11, 597, 8000, 577, 731, 291, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.12010397394019437, "compression_ratio": 1.6030927835051547, "no_speech_prob": 4.52093530611819e-07}, {"id": 195, "seek": 121648, "start": 1216.48, "end": 1222.38, "text": " Loss is the thing that we talked about in lesson one. So I'll give a quick summary,", "tokens": [441, 772, 307, 264, 551, 300, 321, 2825, 466, 294, 6898, 472, 13, 407, 286, 603, 976, 257, 1702, 12691, 11], "temperature": 0.0, "avg_logprob": -0.09780227054249156, "compression_ratio": 1.5726495726495726, "no_speech_prob": 6.179379852255806e-07}, {"id": 196, "seek": 121648, "start": 1222.38, "end": 1228.08, "text": " but go back to lesson one if you don't remember. Arthur Samuel talked about how a machine learning", "tokens": [457, 352, 646, 281, 6898, 472, 498, 291, 500, 380, 1604, 13, 19624, 23036, 2825, 466, 577, 257, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.09780227054249156, "compression_ratio": 1.5726495726495726, "no_speech_prob": 6.179379852255806e-07}, {"id": 197, "seek": 121648, "start": 1228.08, "end": 1235.1, "text": " model needs some measure of performance, which we can look at when we adjust our parameters", "tokens": [2316, 2203, 512, 3481, 295, 3389, 11, 597, 321, 393, 574, 412, 562, 321, 4369, 527, 9834], "temperature": 0.0, "avg_logprob": -0.09780227054249156, "compression_ratio": 1.5726495726495726, "no_speech_prob": 6.179379852255806e-07}, {"id": 198, "seek": 121648, "start": 1235.1, "end": 1243.06, "text": " up or down. Does that measure of performance get better or worse? And as I mentioned earlier,", "tokens": [493, 420, 760, 13, 4402, 300, 3481, 295, 3389, 483, 1101, 420, 5324, 30, 400, 382, 286, 2835, 3071, 11], "temperature": 0.0, "avg_logprob": -0.09780227054249156, "compression_ratio": 1.5726495726495726, "no_speech_prob": 6.179379852255806e-07}, {"id": 199, "seek": 124306, "start": 1243.06, "end": 1249.1, "text": " some metrics possibly won't change at all if you move the parameters up and down just", "tokens": [512, 16367, 6264, 1582, 380, 1319, 412, 439, 498, 291, 1286, 264, 9834, 493, 293, 760, 445], "temperature": 0.0, "avg_logprob": -0.05929810930006575, "compression_ratio": 1.788, "no_speech_prob": 1.0511470236451714e-06}, {"id": 200, "seek": 124306, "start": 1249.1, "end": 1254.44, "text": " a little bit. So they can't be used for this purpose of adjusting the parameters to find", "tokens": [257, 707, 857, 13, 407, 436, 393, 380, 312, 1143, 337, 341, 4334, 295, 23559, 264, 9834, 281, 915], "temperature": 0.0, "avg_logprob": -0.05929810930006575, "compression_ratio": 1.788, "no_speech_prob": 1.0511470236451714e-06}, {"id": 201, "seek": 124306, "start": 1254.44, "end": 1259.7, "text": " a better measure of performance. So quite often we need to use a different function.", "tokens": [257, 1101, 3481, 295, 3389, 13, 407, 1596, 2049, 321, 643, 281, 764, 257, 819, 2445, 13], "temperature": 0.0, "avg_logprob": -0.05929810930006575, "compression_ratio": 1.788, "no_speech_prob": 1.0511470236451714e-06}, {"id": 202, "seek": 124306, "start": 1259.7, "end": 1264.82, "text": " We call this the loss function. The loss function is the measure of performance that the algorithm", "tokens": [492, 818, 341, 264, 4470, 2445, 13, 440, 4470, 2445, 307, 264, 3481, 295, 3389, 300, 264, 9284], "temperature": 0.0, "avg_logprob": -0.05929810930006575, "compression_ratio": 1.788, "no_speech_prob": 1.0511470236451714e-06}, {"id": 203, "seek": 124306, "start": 1264.82, "end": 1270.26, "text": " uses to try to make the parameters better. And it's something which should kind of track", "tokens": [4960, 281, 853, 281, 652, 264, 9834, 1101, 13, 400, 309, 311, 746, 597, 820, 733, 295, 2837], "temperature": 0.0, "avg_logprob": -0.05929810930006575, "compression_ratio": 1.788, "no_speech_prob": 1.0511470236451714e-06}, {"id": 204, "seek": 127026, "start": 1270.26, "end": 1276.02, "text": " pretty closely to the metric you care about. But it's something which as you change the", "tokens": [1238, 8185, 281, 264, 20678, 291, 1127, 466, 13, 583, 309, 311, 746, 597, 382, 291, 1319, 264], "temperature": 0.0, "avg_logprob": -0.1320254575638544, "compression_ratio": 1.6213592233009708, "no_speech_prob": 9.874623856376274e-07}, {"id": 205, "seek": 127026, "start": 1276.02, "end": 1282.46, "text": " parameters a bit, the loss should always change a bit. And so there's a lot of hand waving", "tokens": [9834, 257, 857, 11, 264, 4470, 820, 1009, 1319, 257, 857, 13, 400, 370, 456, 311, 257, 688, 295, 1011, 35347], "temperature": 0.0, "avg_logprob": -0.1320254575638544, "compression_ratio": 1.6213592233009708, "no_speech_prob": 9.874623856376274e-07}, {"id": 206, "seek": 127026, "start": 1282.46, "end": 1285.98, "text": " there because we need to look at some of the math of how that works. And we'll be doing", "tokens": [456, 570, 321, 643, 281, 574, 412, 512, 295, 264, 5221, 295, 577, 300, 1985, 13, 400, 321, 603, 312, 884], "temperature": 0.0, "avg_logprob": -0.1320254575638544, "compression_ratio": 1.6213592233009708, "no_speech_prob": 9.874623856376274e-07}, {"id": 207, "seek": 127026, "start": 1285.98, "end": 1294.74, "text": " that in the next couple of lessons. Thanks for the great questions.", "tokens": [300, 294, 264, 958, 1916, 295, 8820, 13, 2561, 337, 264, 869, 1651, 13], "temperature": 0.0, "avg_logprob": -0.1320254575638544, "compression_ratio": 1.6213592233009708, "no_speech_prob": 9.874623856376274e-07}, {"id": 208, "seek": 129474, "start": 1294.74, "end": 1302.14, "text": " Okay, so fine-tuning is a particular transfer learning technique where the...", "tokens": [1033, 11, 370, 2489, 12, 83, 37726, 307, 257, 1729, 5003, 2539, 6532, 689, 264, 485], "temperature": 0.0, "avg_logprob": -0.16768682455714745, "compression_ratio": 1.776536312849162, "no_speech_prob": 8.059417382355605e-07}, {"id": 209, "seek": 129474, "start": 1302.14, "end": 1311.58, "text": " Oh, and you're still showing your picture and not the slides.", "tokens": [876, 11, 293, 291, 434, 920, 4099, 428, 3036, 293, 406, 264, 9788, 13], "temperature": 0.0, "avg_logprob": -0.16768682455714745, "compression_ratio": 1.776536312849162, "no_speech_prob": 8.059417382355605e-07}, {"id": 210, "seek": 129474, "start": 1311.58, "end": 1316.66, "text": " So fine-tuning is a transfer learning technique where the weights, this is not quite the right", "tokens": [407, 2489, 12, 83, 37726, 307, 257, 5003, 2539, 6532, 689, 264, 17443, 11, 341, 307, 406, 1596, 264, 558], "temperature": 0.0, "avg_logprob": -0.16768682455714745, "compression_ratio": 1.776536312849162, "no_speech_prob": 8.059417382355605e-07}, {"id": 211, "seek": 129474, "start": 1316.66, "end": 1320.54, "text": " word, we should say the parameters, where the parameters of a pre-trained model are", "tokens": [1349, 11, 321, 820, 584, 264, 9834, 11, 689, 264, 9834, 295, 257, 659, 12, 17227, 2001, 2316, 366], "temperature": 0.0, "avg_logprob": -0.16768682455714745, "compression_ratio": 1.776536312849162, "no_speech_prob": 8.059417382355605e-07}, {"id": 212, "seek": 132054, "start": 1320.54, "end": 1326.22, "text": " updated by training for additional epochs using a different task to that used for pre-training.", "tokens": [10588, 538, 3097, 337, 4497, 30992, 28346, 1228, 257, 819, 5633, 281, 300, 1143, 337, 659, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.14348465746099298, "compression_ratio": 1.6484018264840183, "no_speech_prob": 3.156119703362492e-07}, {"id": 213, "seek": 132054, "start": 1326.22, "end": 1331.78, "text": " So pre-training the task might have been ImageNet classification and then our different task", "tokens": [407, 659, 12, 17227, 1760, 264, 5633, 1062, 362, 668, 29903, 31890, 21538, 293, 550, 527, 819, 5633], "temperature": 0.0, "avg_logprob": -0.14348465746099298, "compression_ratio": 1.6484018264840183, "no_speech_prob": 3.156119703362492e-07}, {"id": 214, "seek": 132054, "start": 1331.78, "end": 1342.02, "text": " might be recognizing cats versus dogs. So the way by default Fast.ai does fine-tuning", "tokens": [1062, 312, 18538, 11111, 5717, 7197, 13, 407, 264, 636, 538, 7576, 15968, 13, 1301, 775, 2489, 12, 83, 37726], "temperature": 0.0, "avg_logprob": -0.14348465746099298, "compression_ratio": 1.6484018264840183, "no_speech_prob": 3.156119703362492e-07}, {"id": 215, "seek": 132054, "start": 1342.02, "end": 1347.94, "text": " is that we use one epoch, which remember is one looking at every image in the data set", "tokens": [307, 300, 321, 764, 472, 30992, 339, 11, 597, 1604, 307, 472, 1237, 412, 633, 3256, 294, 264, 1412, 992], "temperature": 0.0, "avg_logprob": -0.14348465746099298, "compression_ratio": 1.6484018264840183, "no_speech_prob": 3.156119703362492e-07}, {"id": 216, "seek": 134794, "start": 1347.94, "end": 1355.1000000000001, "text": " once, one epoch to fit just those parts of the model necessary to get the particular", "tokens": [1564, 11, 472, 30992, 339, 281, 3318, 445, 729, 3166, 295, 264, 2316, 4818, 281, 483, 264, 1729], "temperature": 0.0, "avg_logprob": -0.08417728814211758, "compression_ratio": 1.6507177033492824, "no_speech_prob": 9.874615898297634e-07}, {"id": 217, "seek": 134794, "start": 1355.1000000000001, "end": 1362.5, "text": " part of the model that's specially for your data set working. And then we use as many", "tokens": [644, 295, 264, 2316, 300, 311, 22549, 337, 428, 1412, 992, 1364, 13, 400, 550, 321, 764, 382, 867], "temperature": 0.0, "avg_logprob": -0.08417728814211758, "compression_ratio": 1.6507177033492824, "no_speech_prob": 9.874615898297634e-07}, {"id": 218, "seek": 134794, "start": 1362.5, "end": 1367.54, "text": " epochs as you ask for to fit the whole model. And so this is more for those people who might", "tokens": [30992, 28346, 382, 291, 1029, 337, 281, 3318, 264, 1379, 2316, 13, 400, 370, 341, 307, 544, 337, 729, 561, 567, 1062], "temperature": 0.0, "avg_logprob": -0.08417728814211758, "compression_ratio": 1.6507177033492824, "no_speech_prob": 9.874615898297634e-07}, {"id": 219, "seek": 134794, "start": 1367.54, "end": 1374.3, "text": " be a bit more advanced. We'll see exactly how this works later on in the lessons.", "tokens": [312, 257, 857, 544, 7339, 13, 492, 603, 536, 2293, 577, 341, 1985, 1780, 322, 294, 264, 8820, 13], "temperature": 0.0, "avg_logprob": -0.08417728814211758, "compression_ratio": 1.6507177033492824, "no_speech_prob": 9.874615898297634e-07}, {"id": 220, "seek": 137430, "start": 1374.3, "end": 1379.5, "text": " So why does transfer learning work and why does it work so well? The best way in my opinion", "tokens": [407, 983, 775, 5003, 2539, 589, 293, 983, 775, 309, 589, 370, 731, 30, 440, 1151, 636, 294, 452, 4800], "temperature": 0.0, "avg_logprob": -0.11486154444077436, "compression_ratio": 1.605072463768116, "no_speech_prob": 2.3320637865253957e-06}, {"id": 221, "seek": 137430, "start": 1379.5, "end": 1386.94, "text": " to look at this is to see this paper by Zyler and Fergus who were actually 2012 ImageNet", "tokens": [281, 574, 412, 341, 307, 281, 536, 341, 3035, 538, 1176, 88, 1918, 293, 36790, 567, 645, 767, 9125, 29903, 31890], "temperature": 0.0, "avg_logprob": -0.11486154444077436, "compression_ratio": 1.605072463768116, "no_speech_prob": 2.3320637865253957e-06}, {"id": 222, "seek": 137430, "start": 1386.94, "end": 1393.54, "text": " winners and interestingly their key insights came from their ability to visualize what's", "tokens": [17193, 293, 25873, 641, 2141, 14310, 1361, 490, 641, 3485, 281, 23273, 437, 311], "temperature": 0.0, "avg_logprob": -0.11486154444077436, "compression_ratio": 1.605072463768116, "no_speech_prob": 2.3320637865253957e-06}, {"id": 223, "seek": 137430, "start": 1393.54, "end": 1398.82, "text": " going on inside a model. So visualization very often turns out to be super important", "tokens": [516, 322, 1854, 257, 2316, 13, 407, 25801, 588, 2049, 4523, 484, 281, 312, 1687, 1021], "temperature": 0.0, "avg_logprob": -0.11486154444077436, "compression_ratio": 1.605072463768116, "no_speech_prob": 2.3320637865253957e-06}, {"id": 224, "seek": 137430, "start": 1398.82, "end": 1403.3799999999999, "text": " to getting great results. What they were able to do was they looked, remember I told you", "tokens": [281, 1242, 869, 3542, 13, 708, 436, 645, 1075, 281, 360, 390, 436, 2956, 11, 1604, 286, 1907, 291], "temperature": 0.0, "avg_logprob": -0.11486154444077436, "compression_ratio": 1.605072463768116, "no_speech_prob": 2.3320637865253957e-06}, {"id": 225, "seek": 140338, "start": 1403.38, "end": 1410.46, "text": " like a ResNet 34 has 34 layers, they looked at something called AlexNet which was the", "tokens": [411, 257, 5015, 31890, 12790, 575, 12790, 7914, 11, 436, 2956, 412, 746, 1219, 5202, 31890, 597, 390, 264], "temperature": 0.0, "avg_logprob": -0.19705029655905332, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.6425741452840157e-06}, {"id": 226, "seek": 140338, "start": 1410.46, "end": 1414.6200000000001, "text": " previous winner of the competition which only had seven layers. At the time that was considered", "tokens": [3894, 8507, 295, 264, 6211, 597, 787, 632, 3407, 7914, 13, 1711, 264, 565, 300, 390, 4888], "temperature": 0.0, "avg_logprob": -0.19705029655905332, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.6425741452840157e-06}, {"id": 227, "seek": 140338, "start": 1414.6200000000001, "end": 1422.0200000000002, "text": " huge and so they took the seven layer model and they said what is the first layer of parameters", "tokens": [2603, 293, 370, 436, 1890, 264, 3407, 4583, 2316, 293, 436, 848, 437, 307, 264, 700, 4583, 295, 9834], "temperature": 0.0, "avg_logprob": -0.19705029655905332, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.6425741452840157e-06}, {"id": 228, "seek": 140338, "start": 1422.0200000000002, "end": 1428.1200000000001, "text": " look like and they figured it out how to draw a picture of them. Right and so the first", "tokens": [574, 411, 293, 436, 8932, 309, 484, 577, 281, 2642, 257, 3036, 295, 552, 13, 1779, 293, 370, 264, 700], "temperature": 0.0, "avg_logprob": -0.19705029655905332, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.6425741452840157e-06}, {"id": 229, "seek": 142812, "start": 1428.12, "end": 1437.3799999999999, "text": " layer had lots and lots of features but here are nine of them, 1, 2, 3, 4, 5, 6, 7, 8,", "tokens": [4583, 632, 3195, 293, 3195, 295, 4122, 457, 510, 366, 4949, 295, 552, 11, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 1386, 11, 1614, 11, 1649, 11], "temperature": 0.0, "avg_logprob": -0.11530590858780035, "compression_ratio": 1.888412017167382, "no_speech_prob": 1.8738686549113481e-06}, {"id": 230, "seek": 142812, "start": 1437.3799999999999, "end": 1441.6999999999998, "text": " 9. And here's what nine of those features look like. One of them was something that", "tokens": [1722, 13, 400, 510, 311, 437, 4949, 295, 729, 4122, 574, 411, 13, 1485, 295, 552, 390, 746, 300], "temperature": 0.0, "avg_logprob": -0.11530590858780035, "compression_ratio": 1.888412017167382, "no_speech_prob": 1.8738686549113481e-06}, {"id": 231, "seek": 142812, "start": 1441.6999999999998, "end": 1446.58, "text": " could recognize diagonal lines from top left to bottom right. One of them could find diagonal", "tokens": [727, 5521, 21539, 3876, 490, 1192, 1411, 281, 2767, 558, 13, 1485, 295, 552, 727, 915, 21539], "temperature": 0.0, "avg_logprob": -0.11530590858780035, "compression_ratio": 1.888412017167382, "no_speech_prob": 1.8738686549113481e-06}, {"id": 232, "seek": 142812, "start": 1446.58, "end": 1450.78, "text": " lines from bottom left to top right. One of them could find gradients that went from the", "tokens": [3876, 490, 2767, 1411, 281, 1192, 558, 13, 1485, 295, 552, 727, 915, 2771, 2448, 300, 1437, 490, 264], "temperature": 0.0, "avg_logprob": -0.11530590858780035, "compression_ratio": 1.888412017167382, "no_speech_prob": 1.8738686549113481e-06}, {"id": 233, "seek": 142812, "start": 1450.78, "end": 1456.4599999999998, "text": " top of orange to the bottom of blue. Some of them were able, you know, one of them was", "tokens": [1192, 295, 7671, 281, 264, 2767, 295, 3344, 13, 2188, 295, 552, 645, 1075, 11, 291, 458, 11, 472, 295, 552, 390], "temperature": 0.0, "avg_logprob": -0.11530590858780035, "compression_ratio": 1.888412017167382, "no_speech_prob": 1.8738686549113481e-06}, {"id": 234, "seek": 145646, "start": 1456.46, "end": 1462.22, "text": " specifically for finding things that were green and so forth. Right so for each of these", "tokens": [4682, 337, 5006, 721, 300, 645, 3092, 293, 370, 5220, 13, 1779, 370, 337, 1184, 295, 613], "temperature": 0.0, "avg_logprob": -0.14011450203097597, "compression_ratio": 1.85, "no_speech_prob": 7.571136961814773e-07}, {"id": 235, "seek": 145646, "start": 1462.22, "end": 1470.6200000000001, "text": " nine they're called filters or features. So then something really interesting they did", "tokens": [4949, 436, 434, 1219, 15995, 420, 4122, 13, 407, 550, 746, 534, 1880, 436, 630], "temperature": 0.0, "avg_logprob": -0.14011450203097597, "compression_ratio": 1.85, "no_speech_prob": 7.571136961814773e-07}, {"id": 236, "seek": 145646, "start": 1470.6200000000001, "end": 1475.1000000000001, "text": " was they looked at for each one of these, each one of these filters, each one of these", "tokens": [390, 436, 2956, 412, 337, 1184, 472, 295, 613, 11, 1184, 472, 295, 613, 15995, 11, 1184, 472, 295, 613], "temperature": 0.0, "avg_logprob": -0.14011450203097597, "compression_ratio": 1.85, "no_speech_prob": 7.571136961814773e-07}, {"id": 237, "seek": 145646, "start": 1475.1000000000001, "end": 1480.66, "text": " features, and we'll learn kind of mathematically about what these actually mean in the coming", "tokens": [4122, 11, 293, 321, 603, 1466, 733, 295, 44003, 466, 437, 613, 767, 914, 294, 264, 1348], "temperature": 0.0, "avg_logprob": -0.14011450203097597, "compression_ratio": 1.85, "no_speech_prob": 7.571136961814773e-07}, {"id": 238, "seek": 145646, "start": 1480.66, "end": 1484.22, "text": " lessons but for now let's just recognize them as saying oh there's something that looks", "tokens": [8820, 457, 337, 586, 718, 311, 445, 5521, 552, 382, 1566, 1954, 456, 311, 746, 300, 1542], "temperature": 0.0, "avg_logprob": -0.14011450203097597, "compression_ratio": 1.85, "no_speech_prob": 7.571136961814773e-07}, {"id": 239, "seek": 148422, "start": 1484.22, "end": 1489.02, "text": " at diagonal lines and something that looks at gradients and they found in the actual", "tokens": [412, 21539, 3876, 293, 746, 300, 1542, 412, 2771, 2448, 293, 436, 1352, 294, 264, 3539], "temperature": 0.0, "avg_logprob": -0.12077279738437982, "compression_ratio": 1.7897435897435898, "no_speech_prob": 1.8553906500073936e-07}, {"id": 240, "seek": 148422, "start": 1489.02, "end": 1497.5, "text": " images in ImageNet specific examples of parts of photos that match that filter. So for this", "tokens": [5267, 294, 29903, 31890, 2685, 5110, 295, 3166, 295, 5787, 300, 2995, 300, 6608, 13, 407, 337, 341], "temperature": 0.0, "avg_logprob": -0.12077279738437982, "compression_ratio": 1.7897435897435898, "no_speech_prob": 1.8553906500073936e-07}, {"id": 241, "seek": 148422, "start": 1497.5, "end": 1504.22, "text": " top left filter here are nine actual patches of real photos that match that filter and", "tokens": [1192, 1411, 6608, 510, 366, 4949, 3539, 26531, 295, 957, 5787, 300, 2995, 300, 6608, 293], "temperature": 0.0, "avg_logprob": -0.12077279738437982, "compression_ratio": 1.7897435897435898, "no_speech_prob": 1.8553906500073936e-07}, {"id": 242, "seek": 148422, "start": 1504.22, "end": 1508.58, "text": " as you can see they're all diagonal lines. And so here's the for the green one here's", "tokens": [382, 291, 393, 536, 436, 434, 439, 21539, 3876, 13, 400, 370, 510, 311, 264, 337, 264, 3092, 472, 510, 311], "temperature": 0.0, "avg_logprob": -0.12077279738437982, "compression_ratio": 1.7897435897435898, "no_speech_prob": 1.8553906500073936e-07}, {"id": 243, "seek": 150858, "start": 1508.58, "end": 1515.1799999999998, "text": " parts of actual photos that match the green one. So layer one is super super simple and", "tokens": [3166, 295, 3539, 5787, 300, 2995, 264, 3092, 472, 13, 407, 4583, 472, 307, 1687, 1687, 2199, 293], "temperature": 0.0, "avg_logprob": -0.11301934140399822, "compression_ratio": 1.7540322580645162, "no_speech_prob": 1.05716978282544e-07}, {"id": 244, "seek": 150858, "start": 1515.1799999999998, "end": 1519.22, "text": " one of the interesting things to note here is that something that can recognize gradients", "tokens": [472, 295, 264, 1880, 721, 281, 3637, 510, 307, 300, 746, 300, 393, 5521, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.11301934140399822, "compression_ratio": 1.7540322580645162, "no_speech_prob": 1.05716978282544e-07}, {"id": 245, "seek": 150858, "start": 1519.22, "end": 1524.48, "text": " and patches of color and lines is likely to be useful for lots of other tasks as well,", "tokens": [293, 26531, 295, 2017, 293, 3876, 307, 3700, 281, 312, 4420, 337, 3195, 295, 661, 9608, 382, 731, 11], "temperature": 0.0, "avg_logprob": -0.11301934140399822, "compression_ratio": 1.7540322580645162, "no_speech_prob": 1.05716978282544e-07}, {"id": 246, "seek": 150858, "start": 1524.48, "end": 1529.9199999999998, "text": " not just ImageNet. So you can kind of see how something that can do this might also", "tokens": [406, 445, 29903, 31890, 13, 407, 291, 393, 733, 295, 536, 577, 746, 300, 393, 360, 341, 1062, 611], "temperature": 0.0, "avg_logprob": -0.11301934140399822, "compression_ratio": 1.7540322580645162, "no_speech_prob": 1.05716978282544e-07}, {"id": 247, "seek": 150858, "start": 1529.9199999999998, "end": 1537.82, "text": " be good at many many other computer vision tasks as well. This is layer two. Layer two", "tokens": [312, 665, 412, 867, 867, 661, 3820, 5201, 9608, 382, 731, 13, 639, 307, 4583, 732, 13, 35166, 732], "temperature": 0.0, "avg_logprob": -0.11301934140399822, "compression_ratio": 1.7540322580645162, "no_speech_prob": 1.05716978282544e-07}, {"id": 248, "seek": 153782, "start": 1537.82, "end": 1546.1399999999999, "text": " takes the features of layer one and combines them so it can not just find edges but can", "tokens": [2516, 264, 4122, 295, 4583, 472, 293, 29520, 552, 370, 309, 393, 406, 445, 915, 8819, 457, 393], "temperature": 0.0, "avg_logprob": -0.13505134274882655, "compression_ratio": 1.5568862275449102, "no_speech_prob": 9.87462158263952e-07}, {"id": 249, "seek": 153782, "start": 1546.1399999999999, "end": 1556.02, "text": " find corners or repeating curving patterns or semi-circles or full circles and so you", "tokens": [915, 12413, 420, 18617, 1262, 798, 8294, 420, 12909, 12, 23568, 6520, 420, 1577, 13040, 293, 370, 291], "temperature": 0.0, "avg_logprob": -0.13505134274882655, "compression_ratio": 1.5568862275449102, "no_speech_prob": 9.87462158263952e-07}, {"id": 250, "seek": 153782, "start": 1556.02, "end": 1565.58, "text": " can see for example here's a it's kind of hard to exactly visualize these layers after", "tokens": [393, 536, 337, 1365, 510, 311, 257, 309, 311, 733, 295, 1152, 281, 2293, 23273, 613, 7914, 934], "temperature": 0.0, "avg_logprob": -0.13505134274882655, "compression_ratio": 1.5568862275449102, "no_speech_prob": 9.87462158263952e-07}, {"id": 251, "seek": 156558, "start": 1565.58, "end": 1571.1, "text": " layer one. You kind of have to show examples of what the filters look like but here you", "tokens": [4583, 472, 13, 509, 733, 295, 362, 281, 855, 5110, 295, 437, 264, 15995, 574, 411, 457, 510, 291], "temperature": 0.0, "avg_logprob": -0.11080773671468098, "compression_ratio": 1.6728971962616823, "no_speech_prob": 4.075753068377708e-08}, {"id": 252, "seek": 156558, "start": 1571.1, "end": 1578.3, "text": " can see examples of parts of photos that these this layer two circular filter has activated", "tokens": [393, 536, 5110, 295, 3166, 295, 5787, 300, 613, 341, 4583, 732, 16476, 6608, 575, 18157], "temperature": 0.0, "avg_logprob": -0.11080773671468098, "compression_ratio": 1.6728971962616823, "no_speech_prob": 4.075753068377708e-08}, {"id": 253, "seek": 156558, "start": 1578.3, "end": 1585.26, "text": " on and as you can see it's found things with circles. So interestingly this one which is", "tokens": [322, 293, 382, 291, 393, 536, 309, 311, 1352, 721, 365, 13040, 13, 407, 25873, 341, 472, 597, 307], "temperature": 0.0, "avg_logprob": -0.11080773671468098, "compression_ratio": 1.6728971962616823, "no_speech_prob": 4.075753068377708e-08}, {"id": 254, "seek": 156558, "start": 1585.26, "end": 1591.84, "text": " this kind of blotchy gradient seems to be very good at finding sunsets and this repeating", "tokens": [341, 733, 295, 888, 310, 28629, 16235, 2544, 281, 312, 588, 665, 412, 5006, 3295, 82, 1385, 293, 341, 18617], "temperature": 0.0, "avg_logprob": -0.11080773671468098, "compression_ratio": 1.6728971962616823, "no_speech_prob": 4.075753068377708e-08}, {"id": 255, "seek": 159184, "start": 1591.84, "end": 1598.9399999999998, "text": " vertical pattern is very good at finding like curtains and wheat fields and stuff. So the", "tokens": [9429, 5102, 307, 588, 665, 412, 5006, 411, 36539, 293, 16691, 7909, 293, 1507, 13, 407, 264], "temperature": 0.0, "avg_logprob": -0.10782844880047966, "compression_ratio": 1.78125, "no_speech_prob": 4.888299258709594e-07}, {"id": 256, "seek": 159184, "start": 1598.9399999999998, "end": 1605.74, "text": " further we get layer three then gets to combine all the kinds of features in layer two and", "tokens": [3052, 321, 483, 4583, 1045, 550, 2170, 281, 10432, 439, 264, 3685, 295, 4122, 294, 4583, 732, 293], "temperature": 0.0, "avg_logprob": -0.10782844880047966, "compression_ratio": 1.78125, "no_speech_prob": 4.888299258709594e-07}, {"id": 257, "seek": 159184, "start": 1605.74, "end": 1609.82, "text": " remember we're only seeing so we're only seeing here are 12 of the features but actually there's", "tokens": [1604, 321, 434, 787, 2577, 370, 321, 434, 787, 2577, 510, 366, 2272, 295, 264, 4122, 457, 767, 456, 311], "temperature": 0.0, "avg_logprob": -0.10782844880047966, "compression_ratio": 1.78125, "no_speech_prob": 4.888299258709594e-07}, {"id": 258, "seek": 159184, "start": 1609.82, "end": 1614.4199999999998, "text": " probably hundreds of them I don't remember exactly in AlexNet but there's lots but by", "tokens": [1391, 6779, 295, 552, 286, 500, 380, 1604, 2293, 294, 5202, 31890, 457, 456, 311, 3195, 457, 538], "temperature": 0.0, "avg_logprob": -0.10782844880047966, "compression_ratio": 1.78125, "no_speech_prob": 4.888299258709594e-07}, {"id": 259, "seek": 159184, "start": 1614.4199999999998, "end": 1619.8999999999999, "text": " the time we get to layer three by combining features from layer two it already has something", "tokens": [264, 565, 321, 483, 281, 4583, 1045, 538, 21928, 4122, 490, 4583, 732, 309, 1217, 575, 746], "temperature": 0.0, "avg_logprob": -0.10782844880047966, "compression_ratio": 1.78125, "no_speech_prob": 4.888299258709594e-07}, {"id": 260, "seek": 161990, "start": 1619.9, "end": 1625.7800000000002, "text": " which is finding text. So this is a feature which can find bits of image that contain", "tokens": [597, 307, 5006, 2487, 13, 407, 341, 307, 257, 4111, 597, 393, 915, 9239, 295, 3256, 300, 5304], "temperature": 0.0, "avg_logprob": -0.12208368839361729, "compression_ratio": 1.828125, "no_speech_prob": 8.579223162996641e-07}, {"id": 261, "seek": 161990, "start": 1625.7800000000002, "end": 1632.5400000000002, "text": " text. It's already got something which can find repeating geometric patterns and you", "tokens": [2487, 13, 467, 311, 1217, 658, 746, 597, 393, 915, 18617, 33246, 8294, 293, 291], "temperature": 0.0, "avg_logprob": -0.12208368839361729, "compression_ratio": 1.828125, "no_speech_prob": 8.579223162996641e-07}, {"id": 262, "seek": 161990, "start": 1632.5400000000002, "end": 1640.3000000000002, "text": " see this is not just like a matching specific pixel patterns this is like a semantic concept", "tokens": [536, 341, 307, 406, 445, 411, 257, 14324, 2685, 19261, 8294, 341, 307, 411, 257, 47982, 3410], "temperature": 0.0, "avg_logprob": -0.12208368839361729, "compression_ratio": 1.828125, "no_speech_prob": 8.579223162996641e-07}, {"id": 263, "seek": 161990, "start": 1640.3000000000002, "end": 1646.26, "text": " it can find repeating circles or repeating squares or repeating hexagons right. So it's", "tokens": [309, 393, 915, 18617, 13040, 420, 18617, 19368, 420, 18617, 23291, 559, 892, 558, 13, 407, 309, 311], "temperature": 0.0, "avg_logprob": -0.12208368839361729, "compression_ratio": 1.828125, "no_speech_prob": 8.579223162996641e-07}, {"id": 264, "seek": 164626, "start": 1646.26, "end": 1652.86, "text": " really like computing it's not just matching a template and remember we know that neural", "tokens": [534, 411, 15866, 309, 311, 406, 445, 14324, 257, 12379, 293, 1604, 321, 458, 300, 18161], "temperature": 0.0, "avg_logprob": -0.09292318820953369, "compression_ratio": 1.6376146788990826, "no_speech_prob": 1.5869990477312967e-07}, {"id": 265, "seek": 164626, "start": 1652.86, "end": 1660.42, "text": " networks can solve any possible computable function so it can certainly do that. So layer", "tokens": [9590, 393, 5039, 604, 1944, 2807, 712, 2445, 370, 309, 393, 3297, 360, 300, 13, 407, 4583], "temperature": 0.0, "avg_logprob": -0.09292318820953369, "compression_ratio": 1.6376146788990826, "no_speech_prob": 1.5869990477312967e-07}, {"id": 266, "seek": 164626, "start": 1660.42, "end": 1665.58, "text": " four gets to combine all the filters from layer three anyway it wants and so by layer", "tokens": [1451, 2170, 281, 10432, 439, 264, 15995, 490, 4583, 1045, 4033, 309, 2738, 293, 370, 538, 4583], "temperature": 0.0, "avg_logprob": -0.09292318820953369, "compression_ratio": 1.6376146788990826, "no_speech_prob": 1.5869990477312967e-07}, {"id": 267, "seek": 164626, "start": 1665.58, "end": 1675.06, "text": " four we have something that can find dog faces for instance. So you can kind of see how each", "tokens": [1451, 321, 362, 746, 300, 393, 915, 3000, 8475, 337, 5197, 13, 407, 291, 393, 733, 295, 536, 577, 1184], "temperature": 0.0, "avg_logprob": -0.09292318820953369, "compression_ratio": 1.6376146788990826, "no_speech_prob": 1.5869990477312967e-07}, {"id": 268, "seek": 167506, "start": 1675.06, "end": 1681.7, "text": " layer we get like more applicatively more sophisticated features and so that's why these", "tokens": [4583, 321, 483, 411, 544, 2580, 19020, 544, 16950, 4122, 293, 370, 300, 311, 983, 613], "temperature": 0.0, "avg_logprob": -0.08991777282400229, "compression_ratio": 1.732, "no_speech_prob": 6.681509603367886e-07}, {"id": 269, "seek": 167506, "start": 1681.7, "end": 1687.7, "text": " deep neural networks can be so incredibly powerful. It's also why transfer learning", "tokens": [2452, 18161, 9590, 393, 312, 370, 6252, 4005, 13, 467, 311, 611, 983, 5003, 2539], "temperature": 0.0, "avg_logprob": -0.08991777282400229, "compression_ratio": 1.732, "no_speech_prob": 6.681509603367886e-07}, {"id": 270, "seek": 167506, "start": 1687.7, "end": 1693.5, "text": " can work so well because like if we wanted something that can find books and I don't", "tokens": [393, 589, 370, 731, 570, 411, 498, 321, 1415, 746, 300, 393, 915, 3642, 293, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.08991777282400229, "compression_ratio": 1.732, "no_speech_prob": 6.681509603367886e-07}, {"id": 271, "seek": 167506, "start": 1693.5, "end": 1697.52, "text": " think there's a book category in ImageNet well it's actually already got something that", "tokens": [519, 456, 311, 257, 1446, 7719, 294, 29903, 31890, 731, 309, 311, 767, 1217, 658, 746, 300], "temperature": 0.0, "avg_logprob": -0.08991777282400229, "compression_ratio": 1.732, "no_speech_prob": 6.681509603367886e-07}, {"id": 272, "seek": 167506, "start": 1697.52, "end": 1702.3, "text": " can find text as an earlier filter which I guess it must be using to find maybe there's", "tokens": [393, 915, 2487, 382, 364, 3071, 6608, 597, 286, 2041, 309, 1633, 312, 1228, 281, 915, 1310, 456, 311], "temperature": 0.0, "avg_logprob": -0.08991777282400229, "compression_ratio": 1.732, "no_speech_prob": 6.681509603367886e-07}, {"id": 273, "seek": 170230, "start": 1702.3, "end": 1709.46, "text": " a category for library or something or a bookshelf. So when you use transfer learning you can", "tokens": [257, 7719, 337, 6405, 420, 746, 420, 257, 1446, 46626, 13, 407, 562, 291, 764, 5003, 2539, 291, 393], "temperature": 0.0, "avg_logprob": -0.08064063583932272, "compression_ratio": 1.6533333333333333, "no_speech_prob": 7.33819717879669e-07}, {"id": 274, "seek": 170230, "start": 1709.46, "end": 1716.3799999999999, "text": " take advantage of all of these pre-learned features to find things that are just combinations", "tokens": [747, 5002, 295, 439, 295, 613, 659, 12, 306, 1083, 292, 4122, 281, 915, 721, 300, 366, 445, 21267], "temperature": 0.0, "avg_logprob": -0.08064063583932272, "compression_ratio": 1.6533333333333333, "no_speech_prob": 7.33819717879669e-07}, {"id": 275, "seek": 170230, "start": 1716.3799999999999, "end": 1722.3, "text": " of these existing features. That's why transfer learning can be done so much more quickly", "tokens": [295, 613, 6741, 4122, 13, 663, 311, 983, 5003, 2539, 393, 312, 1096, 370, 709, 544, 2661], "temperature": 0.0, "avg_logprob": -0.08064063583932272, "compression_ratio": 1.6533333333333333, "no_speech_prob": 7.33819717879669e-07}, {"id": 276, "seek": 170230, "start": 1722.3, "end": 1729.6599999999999, "text": " and so much less data than traditional approaches. One important thing to realize then is that", "tokens": [293, 370, 709, 1570, 1412, 813, 5164, 11587, 13, 1485, 1021, 551, 281, 4325, 550, 307, 300], "temperature": 0.0, "avg_logprob": -0.08064063583932272, "compression_ratio": 1.6533333333333333, "no_speech_prob": 7.33819717879669e-07}, {"id": 277, "seek": 172966, "start": 1729.66, "end": 1735.8600000000001, "text": " these techniques for computer vision are not just good at recognizing photos. There's all", "tokens": [613, 7512, 337, 3820, 5201, 366, 406, 445, 665, 412, 18538, 5787, 13, 821, 311, 439], "temperature": 0.0, "avg_logprob": -0.08946814949129835, "compression_ratio": 1.7110091743119267, "no_speech_prob": 1.2482672673286288e-06}, {"id": 278, "seek": 172966, "start": 1735.8600000000001, "end": 1743.6200000000001, "text": " kinds of things you can turn into pictures. For example these are example these are sounds", "tokens": [3685, 295, 721, 291, 393, 1261, 666, 5242, 13, 1171, 1365, 613, 366, 1365, 613, 366, 3263], "temperature": 0.0, "avg_logprob": -0.08946814949129835, "compression_ratio": 1.7110091743119267, "no_speech_prob": 1.2482672673286288e-06}, {"id": 279, "seek": 172966, "start": 1743.6200000000001, "end": 1750.14, "text": " that have been turned into pictures by representing their frequencies over time and it turns out", "tokens": [300, 362, 668, 3574, 666, 5242, 538, 13460, 641, 20250, 670, 565, 293, 309, 4523, 484], "temperature": 0.0, "avg_logprob": -0.08946814949129835, "compression_ratio": 1.7110091743119267, "no_speech_prob": 1.2482672673286288e-06}, {"id": 280, "seek": 172966, "start": 1750.14, "end": 1756.78, "text": " that if you convert a sound into these kinds of pictures you can get basically state-of-the-art", "tokens": [300, 498, 291, 7620, 257, 1626, 666, 613, 3685, 295, 5242, 291, 393, 483, 1936, 1785, 12, 2670, 12, 3322, 12, 446], "temperature": 0.0, "avg_logprob": -0.08946814949129835, "compression_ratio": 1.7110091743119267, "no_speech_prob": 1.2482672673286288e-06}, {"id": 281, "seek": 175678, "start": 1756.78, "end": 1764.42, "text": " results at sound detection just by using the exact same ResNet learner that we've already", "tokens": [3542, 412, 1626, 17784, 445, 538, 1228, 264, 1900, 912, 5015, 31890, 33347, 300, 321, 600, 1217], "temperature": 0.0, "avg_logprob": -0.14630297097292813, "compression_ratio": 1.5128205128205128, "no_speech_prob": 6.786723929508298e-07}, {"id": 282, "seek": 175678, "start": 1764.42, "end": 1771.86, "text": " seen. I wanted to highlight that it's 9 45 so if you want to take a break soon. A really", "tokens": [1612, 13, 286, 1415, 281, 5078, 300, 309, 311, 1722, 6905, 370, 498, 291, 528, 281, 747, 257, 1821, 2321, 13, 316, 534], "temperature": 0.0, "avg_logprob": -0.14630297097292813, "compression_ratio": 1.5128205128205128, "no_speech_prob": 6.786723929508298e-07}, {"id": 283, "seek": 175678, "start": 1771.86, "end": 1777.06, "text": " cool example from I think this is our very first year of running fast AI. One of our", "tokens": [1627, 1365, 490, 286, 519, 341, 307, 527, 588, 700, 1064, 295, 2614, 2370, 7318, 13, 1485, 295, 527], "temperature": 0.0, "avg_logprob": -0.14630297097292813, "compression_ratio": 1.5128205128205128, "no_speech_prob": 6.786723929508298e-07}, {"id": 284, "seek": 175678, "start": 1777.06, "end": 1784.3, "text": " students created pictures they worked at Splunk in anti-fraud and they created pictures of", "tokens": [1731, 2942, 5242, 436, 2732, 412, 19788, 3197, 294, 6061, 12, 69, 424, 532, 293, 436, 2942, 5242, 295], "temperature": 0.0, "avg_logprob": -0.14630297097292813, "compression_ratio": 1.5128205128205128, "no_speech_prob": 6.786723929508298e-07}, {"id": 285, "seek": 178430, "start": 1784.3, "end": 1789.58, "text": " users moving their mouse and if I remember correctly as they moved their mouse he basically", "tokens": [5022, 2684, 641, 9719, 293, 498, 286, 1604, 8944, 382, 436, 4259, 641, 9719, 415, 1936], "temperature": 0.0, "avg_logprob": -0.11276066536996879, "compression_ratio": 1.8353909465020577, "no_speech_prob": 2.4439775643259054e-06}, {"id": 286, "seek": 178430, "start": 1789.58, "end": 1794.8999999999999, "text": " drew a picture of where the mouse moved and the color depended on how fast they moved", "tokens": [12804, 257, 3036, 295, 689, 264, 9719, 4259, 293, 264, 2017, 1367, 3502, 322, 577, 2370, 436, 4259], "temperature": 0.0, "avg_logprob": -0.11276066536996879, "compression_ratio": 1.8353909465020577, "no_speech_prob": 2.4439775643259054e-06}, {"id": 287, "seek": 178430, "start": 1794.8999999999999, "end": 1800.98, "text": " and these circular blobs is where they clicked the left or the right mouse button. And at", "tokens": [293, 613, 16476, 1749, 929, 307, 689, 436, 23370, 264, 1411, 420, 264, 558, 9719, 2960, 13, 400, 412], "temperature": 0.0, "avg_logprob": -0.11276066536996879, "compression_ratio": 1.8353909465020577, "no_speech_prob": 2.4439775643259054e-06}, {"id": 288, "seek": 178430, "start": 1800.98, "end": 1807.06, "text": " Splunk they then well he what he did actually for the for the course as a project for the", "tokens": [19788, 3197, 436, 550, 731, 415, 437, 415, 630, 767, 337, 264, 337, 264, 1164, 382, 257, 1716, 337, 264], "temperature": 0.0, "avg_logprob": -0.11276066536996879, "compression_ratio": 1.8353909465020577, "no_speech_prob": 2.4439775643259054e-06}, {"id": 289, "seek": 178430, "start": 1807.06, "end": 1812.4199999999998, "text": " course is he tried to see whether he could use this these pictures with exactly the same", "tokens": [1164, 307, 415, 3031, 281, 536, 1968, 415, 727, 764, 341, 613, 5242, 365, 2293, 264, 912], "temperature": 0.0, "avg_logprob": -0.11276066536996879, "compression_ratio": 1.8353909465020577, "no_speech_prob": 2.4439775643259054e-06}, {"id": 290, "seek": 181242, "start": 1812.42, "end": 1819.26, "text": " approach we saw in lesson one to create an anti-fraud model and it worked so well that", "tokens": [3109, 321, 1866, 294, 6898, 472, 281, 1884, 364, 6061, 12, 69, 424, 532, 2316, 293, 309, 2732, 370, 731, 300], "temperature": 0.0, "avg_logprob": -0.07896066179462508, "compression_ratio": 1.6741573033707866, "no_speech_prob": 5.804988632007735e-07}, {"id": 291, "seek": 181242, "start": 1819.26, "end": 1824.38, "text": " Splunk ended up patenting a new product based on this technique and you can actually check", "tokens": [19788, 3197, 4590, 493, 20495, 278, 257, 777, 1674, 2361, 322, 341, 6532, 293, 291, 393, 767, 1520], "temperature": 0.0, "avg_logprob": -0.07896066179462508, "compression_ratio": 1.6741573033707866, "no_speech_prob": 5.804988632007735e-07}, {"id": 292, "seek": 181242, "start": 1824.38, "end": 1828.5600000000002, "text": " it out there's a blog post about it on the internet where they describe this breakthrough", "tokens": [309, 484, 456, 311, 257, 6968, 2183, 466, 309, 322, 264, 4705, 689, 436, 6786, 341, 22397], "temperature": 0.0, "avg_logprob": -0.07896066179462508, "compression_ratio": 1.6741573033707866, "no_speech_prob": 5.804988632007735e-07}, {"id": 293, "seek": 181242, "start": 1828.5600000000002, "end": 1834.42, "text": " anti-fraud approach which literally came from one of our really amazing and brilliant and", "tokens": [6061, 12, 69, 424, 532, 3109, 597, 3736, 1361, 490, 472, 295, 527, 534, 2243, 293, 10248, 293], "temperature": 0.0, "avg_logprob": -0.07896066179462508, "compression_ratio": 1.6741573033707866, "no_speech_prob": 5.804988632007735e-07}, {"id": 294, "seek": 181242, "start": 1834.42, "end": 1842.4, "text": " creative students after lesson one of the course. Another cool example of this is looking", "tokens": [5880, 1731, 934, 6898, 472, 295, 264, 1164, 13, 3996, 1627, 1365, 295, 341, 307, 1237], "temperature": 0.0, "avg_logprob": -0.07896066179462508, "compression_ratio": 1.6741573033707866, "no_speech_prob": 5.804988632007735e-07}, {"id": 295, "seek": 184240, "start": 1842.4, "end": 1849.7, "text": " at different viruses and again turning them into pictures and you can kind of see how", "tokens": [412, 819, 21785, 293, 797, 6246, 552, 666, 5242, 293, 291, 393, 733, 295, 536, 577], "temperature": 0.0, "avg_logprob": -0.12165250324067615, "compression_ratio": 1.8099173553719008, "no_speech_prob": 1.4823491483184625e-06}, {"id": 296, "seek": 184240, "start": 1849.7, "end": 1854.94, "text": " they've got here this is from a paper check out the book for the citation they've got", "tokens": [436, 600, 658, 510, 341, 307, 490, 257, 3035, 1520, 484, 264, 1446, 337, 264, 45590, 436, 600, 658], "temperature": 0.0, "avg_logprob": -0.12165250324067615, "compression_ratio": 1.8099173553719008, "no_speech_prob": 1.4823491483184625e-06}, {"id": 297, "seek": 184240, "start": 1854.94, "end": 1860.1000000000001, "text": " three examples of a particular virus called VB.AT and another example of a particular", "tokens": [1045, 5110, 295, 257, 1729, 5752, 1219, 691, 33, 13, 2218, 293, 1071, 1365, 295, 257, 1729], "temperature": 0.0, "avg_logprob": -0.12165250324067615, "compression_ratio": 1.8099173553719008, "no_speech_prob": 1.4823491483184625e-06}, {"id": 298, "seek": 184240, "start": 1860.1000000000001, "end": 1865.7800000000002, "text": " virus called Fakrian and you can see each case the pictures all look kind of similar", "tokens": [5752, 1219, 479, 514, 5501, 293, 291, 393, 536, 1184, 1389, 264, 5242, 439, 574, 733, 295, 2531], "temperature": 0.0, "avg_logprob": -0.12165250324067615, "compression_ratio": 1.8099173553719008, "no_speech_prob": 1.4823491483184625e-06}, {"id": 299, "seek": 184240, "start": 1865.7800000000002, "end": 1871.8600000000001, "text": " and that's why again they can get state-of-the-art results in in virus detection by turning the", "tokens": [293, 300, 311, 983, 797, 436, 393, 483, 1785, 12, 2670, 12, 3322, 12, 446, 3542, 294, 294, 5752, 17784, 538, 6246, 264], "temperature": 0.0, "avg_logprob": -0.12165250324067615, "compression_ratio": 1.8099173553719008, "no_speech_prob": 1.4823491483184625e-06}, {"id": 300, "seek": 187186, "start": 1871.86, "end": 1881.62, "text": " kind of program signatures into pictures and putting it through image recognition. So in", "tokens": [733, 295, 1461, 32322, 666, 5242, 293, 3372, 309, 807, 3256, 11150, 13, 407, 294], "temperature": 0.0, "avg_logprob": -0.06543923761243019, "compression_ratio": 1.847457627118644, "no_speech_prob": 1.328773464592814e-06}, {"id": 301, "seek": 187186, "start": 1881.62, "end": 1886.02, "text": " the book you'll find a list of all of the terms all of the most important terms we've", "tokens": [264, 1446, 291, 603, 915, 257, 1329, 295, 439, 295, 264, 2115, 439, 295, 264, 881, 1021, 2115, 321, 600], "temperature": 0.0, "avg_logprob": -0.06543923761243019, "compression_ratio": 1.847457627118644, "no_speech_prob": 1.328773464592814e-06}, {"id": 302, "seek": 187186, "start": 1886.02, "end": 1890.06, "text": " seen by so far and what they mean I'm not going to read through them but I want you", "tokens": [1612, 538, 370, 1400, 293, 437, 436, 914, 286, 478, 406, 516, 281, 1401, 807, 552, 457, 286, 528, 291], "temperature": 0.0, "avg_logprob": -0.06543923761243019, "compression_ratio": 1.847457627118644, "no_speech_prob": 1.328773464592814e-06}, {"id": 303, "seek": 187186, "start": 1890.06, "end": 1894.62, "text": " to please because these are the these are the terms that we're going to be using from", "tokens": [281, 1767, 570, 613, 366, 264, 613, 366, 264, 2115, 300, 321, 434, 516, 281, 312, 1228, 490], "temperature": 0.0, "avg_logprob": -0.06543923761243019, "compression_ratio": 1.847457627118644, "no_speech_prob": 1.328773464592814e-06}, {"id": 304, "seek": 187186, "start": 1894.62, "end": 1899.9799999999998, "text": " now on and you've got to know what they mean because if you don't you're going to be really", "tokens": [586, 322, 293, 291, 600, 658, 281, 458, 437, 436, 914, 570, 498, 291, 500, 380, 291, 434, 516, 281, 312, 534], "temperature": 0.0, "avg_logprob": -0.06543923761243019, "compression_ratio": 1.847457627118644, "no_speech_prob": 1.328773464592814e-06}, {"id": 305, "seek": 189998, "start": 1899.98, "end": 1904.3, "text": " confused because I'll be talking about labels and architectures and models and parameters", "tokens": [9019, 570, 286, 603, 312, 1417, 466, 16949, 293, 6331, 1303, 293, 5245, 293, 9834], "temperature": 0.0, "avg_logprob": -0.10454064149122971, "compression_ratio": 1.6854460093896713, "no_speech_prob": 3.412578450934234e-07}, {"id": 306, "seek": 189998, "start": 1904.3, "end": 1909.42, "text": " and they have very specific exact meanings and they'll be using those exact meanings", "tokens": [293, 436, 362, 588, 2685, 1900, 28138, 293, 436, 603, 312, 1228, 729, 1900, 28138], "temperature": 0.0, "avg_logprob": -0.10454064149122971, "compression_ratio": 1.6854460093896713, "no_speech_prob": 3.412578450934234e-07}, {"id": 307, "seek": 189998, "start": 1909.42, "end": 1917.38, "text": " so please review this. So to remind you this is where we got to we we ended up with Arthur", "tokens": [370, 1767, 3131, 341, 13, 407, 281, 4160, 291, 341, 307, 689, 321, 658, 281, 321, 321, 4590, 493, 365, 19624], "temperature": 0.0, "avg_logprob": -0.10454064149122971, "compression_ratio": 1.6854460093896713, "no_speech_prob": 3.412578450934234e-07}, {"id": 308, "seek": 189998, "start": 1917.38, "end": 1926.34, "text": " Samuel's overall approach and we replaced his terms with our terms so we have an architecture", "tokens": [23036, 311, 4787, 3109, 293, 321, 10772, 702, 2115, 365, 527, 2115, 370, 321, 362, 364, 9482], "temperature": 0.0, "avg_logprob": -0.10454064149122971, "compression_ratio": 1.6854460093896713, "no_speech_prob": 3.412578450934234e-07}, {"id": 309, "seek": 192634, "start": 1926.34, "end": 1933.6999999999998, "text": " which contains parameters as inputs and we have more parameters and the data as inputs", "tokens": [597, 8306, 9834, 382, 15743, 293, 321, 362, 544, 9834, 293, 264, 1412, 382, 15743], "temperature": 0.0, "avg_logprob": -0.12201962405688142, "compression_ratio": 1.9777777777777779, "no_speech_prob": 2.3823665173949848e-07}, {"id": 310, "seek": 192634, "start": 1933.6999999999998, "end": 1939.98, "text": " so that the architecture plus the parameters of the model with the inputs they use to calculate", "tokens": [370, 300, 264, 9482, 1804, 264, 9834, 295, 264, 2316, 365, 264, 15743, 436, 764, 281, 8873], "temperature": 0.0, "avg_logprob": -0.12201962405688142, "compression_ratio": 1.9777777777777779, "no_speech_prob": 2.3823665173949848e-07}, {"id": 311, "seek": 192634, "start": 1939.98, "end": 1945.62, "text": " predictions they are compared to the labels with a loss function and that loss function", "tokens": [21264, 436, 366, 5347, 281, 264, 16949, 365, 257, 4470, 2445, 293, 300, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.12201962405688142, "compression_ratio": 1.9777777777777779, "no_speech_prob": 2.3823665173949848e-07}, {"id": 312, "seek": 192634, "start": 1945.62, "end": 1950.82, "text": " is used to update the parameters many many times to make them better and better until", "tokens": [307, 1143, 281, 5623, 264, 9834, 867, 867, 1413, 281, 652, 552, 1101, 293, 1101, 1826], "temperature": 0.0, "avg_logprob": -0.12201962405688142, "compression_ratio": 1.9777777777777779, "no_speech_prob": 2.3823665173949848e-07}, {"id": 313, "seek": 195082, "start": 1950.82, "end": 1958.54, "text": " the loss gets nice and super low. So this is the end of chapter one of the book. It's", "tokens": [264, 4470, 2170, 1481, 293, 1687, 2295, 13, 407, 341, 307, 264, 917, 295, 7187, 472, 295, 264, 1446, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.10269775844755627, "compression_ratio": 1.7095238095238094, "no_speech_prob": 4.7378901513184246e-07}, {"id": 314, "seek": 195082, "start": 1958.54, "end": 1962.82, "text": " really important to look at the questionnaire because the questionnaire is the thing where", "tokens": [534, 1021, 281, 574, 412, 264, 44702, 570, 264, 44702, 307, 264, 551, 689], "temperature": 0.0, "avg_logprob": -0.10269775844755627, "compression_ratio": 1.7095238095238094, "no_speech_prob": 4.7378901513184246e-07}, {"id": 315, "seek": 195082, "start": 1962.82, "end": 1969.26, "text": " you can check whether you have taken away from this book of this chapter the stuff that", "tokens": [291, 393, 1520, 1968, 291, 362, 2726, 1314, 490, 341, 1446, 295, 341, 7187, 264, 1507, 300], "temperature": 0.0, "avg_logprob": -0.10269775844755627, "compression_ratio": 1.7095238095238094, "no_speech_prob": 4.7378901513184246e-07}, {"id": 316, "seek": 195082, "start": 1969.26, "end": 1976.76, "text": " we hope you have. So go through it and anything that you're not sure about the tech the answer", "tokens": [321, 1454, 291, 362, 13, 407, 352, 807, 309, 293, 1340, 300, 291, 434, 406, 988, 466, 264, 7553, 264, 1867], "temperature": 0.0, "avg_logprob": -0.10269775844755627, "compression_ratio": 1.7095238095238094, "no_speech_prob": 4.7378901513184246e-07}, {"id": 317, "seek": 197676, "start": 1976.76, "end": 1981.7, "text": " is in the text so just go back to earlier in the book and you will in the chapter and", "tokens": [307, 294, 264, 2487, 370, 445, 352, 646, 281, 3071, 294, 264, 1446, 293, 291, 486, 294, 264, 7187, 293], "temperature": 0.0, "avg_logprob": -0.08774241221319769, "compression_ratio": 1.8776371308016877, "no_speech_prob": 1.602798988642462e-06}, {"id": 318, "seek": 197676, "start": 1981.7, "end": 1988.5, "text": " you will find the answers. There's also a further research section after each questionnaire", "tokens": [291, 486, 915, 264, 6338, 13, 821, 311, 611, 257, 3052, 2132, 3541, 934, 1184, 44702], "temperature": 0.0, "avg_logprob": -0.08774241221319769, "compression_ratio": 1.8776371308016877, "no_speech_prob": 1.602798988642462e-06}, {"id": 319, "seek": 197676, "start": 1988.5, "end": 1992.1, "text": " for the first couple of chapters they're actually pretty simple hopefully they're pretty fun", "tokens": [337, 264, 700, 1916, 295, 20013, 436, 434, 767, 1238, 2199, 4696, 436, 434, 1238, 1019], "temperature": 0.0, "avg_logprob": -0.08774241221319769, "compression_ratio": 1.8776371308016877, "no_speech_prob": 1.602798988642462e-06}, {"id": 320, "seek": 197676, "start": 1992.1, "end": 1995.82, "text": " and interesting they're things where to answer the question it's not enough to just look", "tokens": [293, 1880, 436, 434, 721, 689, 281, 1867, 264, 1168, 309, 311, 406, 1547, 281, 445, 574], "temperature": 0.0, "avg_logprob": -0.08774241221319769, "compression_ratio": 1.8776371308016877, "no_speech_prob": 1.602798988642462e-06}, {"id": 321, "seek": 197676, "start": 1995.82, "end": 2001.34, "text": " in the chapter you actually have to go and do your own thinking and experimenting and", "tokens": [294, 264, 7187, 291, 767, 362, 281, 352, 293, 360, 428, 1065, 1953, 293, 29070, 293], "temperature": 0.0, "avg_logprob": -0.08774241221319769, "compression_ratio": 1.8776371308016877, "no_speech_prob": 1.602798988642462e-06}, {"id": 322, "seek": 200134, "start": 2001.34, "end": 2007.6399999999999, "text": " googling and so forth. In later chapters some of these further research things are pretty", "tokens": [50061, 1688, 293, 370, 5220, 13, 682, 1780, 20013, 512, 295, 613, 3052, 2132, 721, 366, 1238], "temperature": 0.0, "avg_logprob": -0.07442140102386474, "compression_ratio": 1.6943396226415095, "no_speech_prob": 2.419878342152515e-07}, {"id": 323, "seek": 200134, "start": 2007.6399999999999, "end": 2014.1399999999999, "text": " significant projects that might take a few days or even weeks and so yeah you know check", "tokens": [4776, 4455, 300, 1062, 747, 257, 1326, 1708, 420, 754, 3259, 293, 370, 1338, 291, 458, 1520], "temperature": 0.0, "avg_logprob": -0.07442140102386474, "compression_ratio": 1.6943396226415095, "no_speech_prob": 2.419878342152515e-07}, {"id": 324, "seek": 200134, "start": 2014.1399999999999, "end": 2021.6999999999998, "text": " them out because hopefully there'll be a great way to expand your understanding of the material.", "tokens": [552, 484, 570, 4696, 456, 603, 312, 257, 869, 636, 281, 5268, 428, 3701, 295, 264, 2527, 13], "temperature": 0.0, "avg_logprob": -0.07442140102386474, "compression_ratio": 1.6943396226415095, "no_speech_prob": 2.419878342152515e-07}, {"id": 325, "seek": 200134, "start": 2021.6999999999998, "end": 2025.3799999999999, "text": " So something that Sylvain points out in the book is that if you really want to make the", "tokens": [407, 746, 300, 3902, 14574, 491, 2793, 484, 294, 264, 1446, 307, 300, 498, 291, 534, 528, 281, 652, 264], "temperature": 0.0, "avg_logprob": -0.07442140102386474, "compression_ratio": 1.6943396226415095, "no_speech_prob": 2.419878342152515e-07}, {"id": 326, "seek": 200134, "start": 2025.3799999999999, "end": 2030.12, "text": " most of this then after each chapter please take the time to experiment with your own", "tokens": [881, 295, 341, 550, 934, 1184, 7187, 1767, 747, 264, 565, 281, 5120, 365, 428, 1065], "temperature": 0.0, "avg_logprob": -0.07442140102386474, "compression_ratio": 1.6943396226415095, "no_speech_prob": 2.419878342152515e-07}, {"id": 327, "seek": 203012, "start": 2030.12, "end": 2036.7399999999998, "text": " project and with the notebooks you provide we provide and then see if you can redo the", "tokens": [1716, 293, 365, 264, 43782, 291, 2893, 321, 2893, 293, 550, 536, 498, 291, 393, 29956, 264], "temperature": 0.0, "avg_logprob": -0.11489566536836845, "compression_ratio": 1.6481481481481481, "no_speech_prob": 7.071684649417875e-06}, {"id": 328, "seek": 203012, "start": 2036.7399999999998, "end": 2041.1799999999998, "text": " notebooks on a new data set. Perhaps for chapter one that might be a bit hard because we haven't", "tokens": [43782, 322, 257, 777, 1412, 992, 13, 10517, 337, 7187, 472, 300, 1062, 312, 257, 857, 1152, 570, 321, 2378, 380], "temperature": 0.0, "avg_logprob": -0.11489566536836845, "compression_ratio": 1.6481481481481481, "no_speech_prob": 7.071684649417875e-06}, {"id": 329, "seek": 203012, "start": 2041.1799999999998, "end": 2046.1, "text": " really shown how to change things but for chapter two which we're going to start next", "tokens": [534, 4898, 577, 281, 1319, 721, 457, 337, 7187, 732, 597, 321, 434, 516, 281, 722, 958], "temperature": 0.0, "avg_logprob": -0.11489566536836845, "compression_ratio": 1.6481481481481481, "no_speech_prob": 7.071684649417875e-06}, {"id": 330, "seek": 203012, "start": 2046.1, "end": 2053.1, "text": " you'll absolutely be able to do that. Okay so let's take a five minute break and we'll", "tokens": [291, 603, 3122, 312, 1075, 281, 360, 300, 13, 1033, 370, 718, 311, 747, 257, 1732, 3456, 1821, 293, 321, 603], "temperature": 0.0, "avg_logprob": -0.11489566536836845, "compression_ratio": 1.6481481481481481, "no_speech_prob": 7.071684649417875e-06}, {"id": 331, "seek": 205310, "start": 2053.1, "end": 2061.94, "text": " come back at 955 San Francisco time. Okay so welcome back everybody and I think we've", "tokens": [808, 646, 412, 1722, 13622, 5271, 12279, 565, 13, 1033, 370, 2928, 646, 2201, 293, 286, 519, 321, 600], "temperature": 0.0, "avg_logprob": -0.1548634334044023, "compression_ratio": 1.4895397489539748, "no_speech_prob": 1.370942072753678e-06}, {"id": 332, "seek": 205310, "start": 2061.94, "end": 2067.58, "text": " got a couple of questions to start with so Rachel please take it away. Sure. Are filters", "tokens": [658, 257, 1916, 295, 1651, 281, 722, 365, 370, 14246, 1767, 747, 309, 1314, 13, 4894, 13, 2014, 15995], "temperature": 0.0, "avg_logprob": -0.1548634334044023, "compression_ratio": 1.4895397489539748, "no_speech_prob": 1.370942072753678e-06}, {"id": 333, "seek": 205310, "start": 2067.58, "end": 2072.66, "text": " independent? By that I mean if filters are pre-tried might they become less good in detecting", "tokens": [6695, 30, 3146, 300, 286, 914, 498, 15995, 366, 659, 12, 83, 2428, 1062, 436, 1813, 1570, 665, 294, 40237], "temperature": 0.0, "avg_logprob": -0.1548634334044023, "compression_ratio": 1.4895397489539748, "no_speech_prob": 1.370942072753678e-06}, {"id": 334, "seek": 205310, "start": 2072.66, "end": 2080.9, "text": " features of previous images when fine-tuned? Oh that is a great question. So assuming I", "tokens": [4122, 295, 3894, 5267, 562, 2489, 12, 83, 43703, 30, 876, 300, 307, 257, 869, 1168, 13, 407, 11926, 286], "temperature": 0.0, "avg_logprob": -0.1548634334044023, "compression_ratio": 1.4895397489539748, "no_speech_prob": 1.370942072753678e-06}, {"id": 335, "seek": 208090, "start": 2080.9, "end": 2086.34, "text": " understand the question correctly if you start with say an ImageNet model and then you you", "tokens": [1223, 264, 1168, 8944, 498, 291, 722, 365, 584, 364, 29903, 31890, 2316, 293, 550, 291, 291], "temperature": 0.0, "avg_logprob": -0.09852195608204808, "compression_ratio": 1.8265306122448979, "no_speech_prob": 2.2603051093028625e-06}, {"id": 336, "seek": 208090, "start": 2086.34, "end": 2092.38, "text": " fine-tune it on dogs versus cats for a few epochs and you get something that's very good", "tokens": [2489, 12, 83, 2613, 309, 322, 7197, 5717, 11111, 337, 257, 1326, 30992, 28346, 293, 291, 483, 746, 300, 311, 588, 665], "temperature": 0.0, "avg_logprob": -0.09852195608204808, "compression_ratio": 1.8265306122448979, "no_speech_prob": 2.2603051093028625e-06}, {"id": 337, "seek": 208090, "start": 2092.38, "end": 2098.38, "text": " at recognizing dogs versus cats it's going to be much less good as an ImageNet model", "tokens": [412, 18538, 7197, 5717, 11111, 309, 311, 516, 281, 312, 709, 1570, 665, 382, 364, 29903, 31890, 2316], "temperature": 0.0, "avg_logprob": -0.09852195608204808, "compression_ratio": 1.8265306122448979, "no_speech_prob": 2.2603051093028625e-06}, {"id": 338, "seek": 208090, "start": 2098.38, "end": 2106.34, "text": " after that so it's not going to be very good at recognizing airplanes or hammers or whatever.", "tokens": [934, 300, 370, 309, 311, 406, 516, 281, 312, 588, 665, 412, 18538, 32947, 420, 36600, 433, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.09852195608204808, "compression_ratio": 1.8265306122448979, "no_speech_prob": 2.2603051093028625e-06}, {"id": 339, "seek": 210634, "start": 2106.34, "end": 2111.58, "text": " This is called catastrophic forgetting in the literature the idea that as you like see", "tokens": [639, 307, 1219, 34915, 25428, 294, 264, 10394, 264, 1558, 300, 382, 291, 411, 536], "temperature": 0.0, "avg_logprob": -0.07581858091716524, "compression_ratio": 1.7128712871287128, "no_speech_prob": 4.3138939531672804e-07}, {"id": 340, "seek": 210634, "start": 2111.58, "end": 2116.1400000000003, "text": " more images about different things to what you saw earlier that you start to forget about", "tokens": [544, 5267, 466, 819, 721, 281, 437, 291, 1866, 3071, 300, 291, 722, 281, 2870, 466], "temperature": 0.0, "avg_logprob": -0.07581858091716524, "compression_ratio": 1.7128712871287128, "no_speech_prob": 4.3138939531672804e-07}, {"id": 341, "seek": 210634, "start": 2116.1400000000003, "end": 2123.76, "text": " the things you saw earlier. So if you want to fine-tune something which is good at a", "tokens": [264, 721, 291, 1866, 3071, 13, 407, 498, 291, 528, 281, 2489, 12, 83, 2613, 746, 597, 307, 665, 412, 257], "temperature": 0.0, "avg_logprob": -0.07581858091716524, "compression_ratio": 1.7128712871287128, "no_speech_prob": 4.3138939531672804e-07}, {"id": 342, "seek": 210634, "start": 2123.76, "end": 2128.2200000000003, "text": " new task but also continues to be good at the previous task you need to keep putting", "tokens": [777, 5633, 457, 611, 6515, 281, 312, 665, 412, 264, 3894, 5633, 291, 643, 281, 1066, 3372], "temperature": 0.0, "avg_logprob": -0.07581858091716524, "compression_ratio": 1.7128712871287128, "no_speech_prob": 4.3138939531672804e-07}, {"id": 343, "seek": 212822, "start": 2128.22, "end": 2136.8999999999996, "text": " in examples of the previous task as well. And what are the example what are the differences", "tokens": [294, 5110, 295, 264, 3894, 5633, 382, 731, 13, 400, 437, 366, 264, 1365, 437, 366, 264, 7300], "temperature": 0.0, "avg_logprob": -0.11853213537307013, "compression_ratio": 1.7969543147208122, "no_speech_prob": 5.368738698052766e-07}, {"id": 344, "seek": 212822, "start": 2136.8999999999996, "end": 2142.8999999999996, "text": " between parameters and hyperparameters? If I am feeding an image of a dog as an input", "tokens": [1296, 9834, 293, 9848, 2181, 335, 6202, 30, 759, 286, 669, 12919, 364, 3256, 295, 257, 3000, 382, 364, 4846], "temperature": 0.0, "avg_logprob": -0.11853213537307013, "compression_ratio": 1.7969543147208122, "no_speech_prob": 5.368738698052766e-07}, {"id": 345, "seek": 212822, "start": 2142.8999999999996, "end": 2147.58, "text": " and then changing the hyperparameters of batch size in the model what would be an example", "tokens": [293, 550, 4473, 264, 9848, 2181, 335, 6202, 295, 15245, 2744, 294, 264, 2316, 437, 576, 312, 364, 1365], "temperature": 0.0, "avg_logprob": -0.11853213537307013, "compression_ratio": 1.7969543147208122, "no_speech_prob": 5.368738698052766e-07}, {"id": 346, "seek": 212822, "start": 2147.58, "end": 2155.66, "text": " of a parameter? So the parameters are the things that are described in lesson one that", "tokens": [295, 257, 13075, 30, 407, 264, 9834, 366, 264, 721, 300, 366, 7619, 294, 6898, 472, 300], "temperature": 0.0, "avg_logprob": -0.11853213537307013, "compression_ratio": 1.7969543147208122, "no_speech_prob": 5.368738698052766e-07}, {"id": 347, "seek": 215566, "start": 2155.66, "end": 2164.7799999999997, "text": " Arthur Samuel described as being the things which change what the model does what the", "tokens": [19624, 23036, 7619, 382, 885, 264, 721, 597, 1319, 437, 264, 2316, 775, 437, 264], "temperature": 0.0, "avg_logprob": -0.09065197016063489, "compression_ratio": 1.6919431279620853, "no_speech_prob": 7.811465252416383e-07}, {"id": 348, "seek": 215566, "start": 2164.7799999999997, "end": 2169.94, "text": " architecture does. So we start with this infinitely flexible function the thing called a neural", "tokens": [9482, 775, 13, 407, 321, 722, 365, 341, 36227, 11358, 2445, 264, 551, 1219, 257, 18161], "temperature": 0.0, "avg_logprob": -0.09065197016063489, "compression_ratio": 1.6919431279620853, "no_speech_prob": 7.811465252416383e-07}, {"id": 349, "seek": 215566, "start": 2169.94, "end": 2178.18, "text": " network that can do anything at all and the the way you get it to do one thing versus", "tokens": [3209, 300, 393, 360, 1340, 412, 439, 293, 264, 264, 636, 291, 483, 309, 281, 360, 472, 551, 5717], "temperature": 0.0, "avg_logprob": -0.09065197016063489, "compression_ratio": 1.6919431279620853, "no_speech_prob": 7.811465252416383e-07}, {"id": 350, "seek": 215566, "start": 2178.18, "end": 2183.42, "text": " another thing is by changing its parameters there they are the numbers that you pass into", "tokens": [1071, 551, 307, 538, 4473, 1080, 9834, 456, 436, 366, 264, 3547, 300, 291, 1320, 666], "temperature": 0.0, "avg_logprob": -0.09065197016063489, "compression_ratio": 1.6919431279620853, "no_speech_prob": 7.811465252416383e-07}, {"id": 351, "seek": 218342, "start": 2183.42, "end": 2187.64, "text": " that function. So there's two types of numbers you pass into the function there's the numbers", "tokens": [300, 2445, 13, 407, 456, 311, 732, 3467, 295, 3547, 291, 1320, 666, 264, 2445, 456, 311, 264, 3547], "temperature": 0.0, "avg_logprob": -0.1286248462956126, "compression_ratio": 1.805, "no_speech_prob": 1.6373753908283106e-07}, {"id": 352, "seek": 218342, "start": 2187.64, "end": 2196.2200000000003, "text": " that represent your input like the pixels of your dog and there's the numbers that represent", "tokens": [300, 2906, 428, 4846, 411, 264, 18668, 295, 428, 3000, 293, 456, 311, 264, 3547, 300, 2906], "temperature": 0.0, "avg_logprob": -0.1286248462956126, "compression_ratio": 1.805, "no_speech_prob": 1.6373753908283106e-07}, {"id": 353, "seek": 218342, "start": 2196.2200000000003, "end": 2201.14, "text": " the learnt parameters. So in the example of something that's not a neural net but like", "tokens": [264, 18991, 9834, 13, 407, 294, 264, 1365, 295, 746, 300, 311, 406, 257, 18161, 2533, 457, 411], "temperature": 0.0, "avg_logprob": -0.1286248462956126, "compression_ratio": 1.805, "no_speech_prob": 1.6373753908283106e-07}, {"id": 354, "seek": 218342, "start": 2201.14, "end": 2206.54, "text": " a checkers playing program like Arthur Samuel might have used back in the early 60s and", "tokens": [257, 1520, 433, 2433, 1461, 411, 19624, 23036, 1062, 362, 1143, 646, 294, 264, 2440, 4060, 82, 293], "temperature": 0.0, "avg_logprob": -0.1286248462956126, "compression_ratio": 1.805, "no_speech_prob": 1.6373753908283106e-07}, {"id": 355, "seek": 220654, "start": 2206.54, "end": 2214.94, "text": " late 50s those parameters may have been things like if there is a opportunity to take a piece", "tokens": [3469, 2625, 82, 729, 9834, 815, 362, 668, 721, 411, 498, 456, 307, 257, 2650, 281, 747, 257, 2522], "temperature": 0.0, "avg_logprob": -0.079439594084958, "compression_ratio": 1.7203791469194314, "no_speech_prob": 3.689880827550951e-07}, {"id": 356, "seek": 220654, "start": 2214.94, "end": 2220.84, "text": " versus an opportunity to get to the end of a board how much more value should I consider", "tokens": [5717, 364, 2650, 281, 483, 281, 264, 917, 295, 257, 3150, 577, 709, 544, 2158, 820, 286, 1949], "temperature": 0.0, "avg_logprob": -0.079439594084958, "compression_ratio": 1.7203791469194314, "no_speech_prob": 3.689880827550951e-07}, {"id": 357, "seek": 220654, "start": 2220.84, "end": 2224.82, "text": " one versus the other you know it's twice as important or it's three times as important", "tokens": [472, 5717, 264, 661, 291, 458, 309, 311, 6091, 382, 1021, 420, 309, 311, 1045, 1413, 382, 1021], "temperature": 0.0, "avg_logprob": -0.079439594084958, "compression_ratio": 1.7203791469194314, "no_speech_prob": 3.689880827550951e-07}, {"id": 358, "seek": 220654, "start": 2224.82, "end": 2232.56, "text": " that two versus three that would be an example of a parameter. In a neural network parameters", "tokens": [300, 732, 5717, 1045, 300, 576, 312, 364, 1365, 295, 257, 13075, 13, 682, 257, 18161, 3209, 9834], "temperature": 0.0, "avg_logprob": -0.079439594084958, "compression_ratio": 1.7203791469194314, "no_speech_prob": 3.689880827550951e-07}, {"id": 359, "seek": 223256, "start": 2232.56, "end": 2236.9, "text": " are a much more abstract concept and so a detailed understanding of what they are will", "tokens": [366, 257, 709, 544, 12649, 3410, 293, 370, 257, 9942, 3701, 295, 437, 436, 366, 486], "temperature": 0.0, "avg_logprob": -0.08677294926765637, "compression_ratio": 1.6384976525821595, "no_speech_prob": 4.664445043545129e-07}, {"id": 360, "seek": 223256, "start": 2236.9, "end": 2243.44, "text": " come in the next lesson or two but it's the same basic idea they're the numbers which", "tokens": [808, 294, 264, 958, 6898, 420, 732, 457, 309, 311, 264, 912, 3875, 1558, 436, 434, 264, 3547, 597], "temperature": 0.0, "avg_logprob": -0.08677294926765637, "compression_ratio": 1.6384976525821595, "no_speech_prob": 4.664445043545129e-07}, {"id": 361, "seek": 223256, "start": 2243.44, "end": 2252.0, "text": " change what the model does to be something that recognizes malignant tumors versus cats", "tokens": [1319, 437, 264, 2316, 775, 281, 312, 746, 300, 26564, 2806, 36818, 38466, 5717, 11111], "temperature": 0.0, "avg_logprob": -0.08677294926765637, "compression_ratio": 1.6384976525821595, "no_speech_prob": 4.664445043545129e-07}, {"id": 362, "seek": 223256, "start": 2252.0, "end": 2260.2799999999997, "text": " versus dogs versus colorizes black and white pictures. Whereas the hyperparameter is the", "tokens": [5717, 7197, 5717, 2017, 5660, 2211, 293, 2418, 5242, 13, 13813, 264, 9848, 2181, 335, 2398, 307, 264], "temperature": 0.0, "avg_logprob": -0.08677294926765637, "compression_ratio": 1.6384976525821595, "no_speech_prob": 4.664445043545129e-07}, {"id": 363, "seek": 226028, "start": 2260.28, "end": 2266.88, "text": " choices about what what numbers do you pass to the function when you act the actual fitting", "tokens": [7994, 466, 437, 437, 3547, 360, 291, 1320, 281, 264, 2445, 562, 291, 605, 264, 3539, 15669], "temperature": 0.0, "avg_logprob": -0.1473411296276336, "compression_ratio": 1.7885462555066078, "no_speech_prob": 1.0188051646764507e-06}, {"id": 364, "seek": 226028, "start": 2266.88, "end": 2270.2400000000002, "text": " function to decide how that fitting process happens.", "tokens": [2445, 281, 4536, 577, 300, 15669, 1399, 2314, 13], "temperature": 0.0, "avg_logprob": -0.1473411296276336, "compression_ratio": 1.7885462555066078, "no_speech_prob": 1.0188051646764507e-06}, {"id": 365, "seek": 226028, "start": 2270.2400000000002, "end": 2276.0800000000004, "text": " There's a question I'm curious about the pacing of this course I'm concerned that all the", "tokens": [821, 311, 257, 1168, 286, 478, 6369, 466, 264, 43285, 295, 341, 1164, 286, 478, 5922, 300, 439, 264], "temperature": 0.0, "avg_logprob": -0.1473411296276336, "compression_ratio": 1.7885462555066078, "no_speech_prob": 1.0188051646764507e-06}, {"id": 366, "seek": 226028, "start": 2276.0800000000004, "end": 2281.28, "text": " material may not be covered. Depends what you mean by all the material we certainly", "tokens": [2527, 815, 406, 312, 5343, 13, 4056, 2581, 437, 291, 914, 538, 439, 264, 2527, 321, 3297], "temperature": 0.0, "avg_logprob": -0.1473411296276336, "compression_ratio": 1.7885462555066078, "no_speech_prob": 1.0188051646764507e-06}, {"id": 367, "seek": 226028, "start": 2281.28, "end": 2289.5600000000004, "text": " won't cover everything in the world so yeah we'll cover what we can we'll cover what we", "tokens": [1582, 380, 2060, 1203, 294, 264, 1002, 370, 1338, 321, 603, 2060, 437, 321, 393, 321, 603, 2060, 437, 321], "temperature": 0.0, "avg_logprob": -0.1473411296276336, "compression_ratio": 1.7885462555066078, "no_speech_prob": 1.0188051646764507e-06}, {"id": 368, "seek": 228956, "start": 2289.56, "end": 2294.24, "text": " can in seven lessons. We're certainly not covering the whole book if that's what you're", "tokens": [393, 294, 3407, 8820, 13, 492, 434, 3297, 406, 10322, 264, 1379, 1446, 498, 300, 311, 437, 291, 434], "temperature": 0.0, "avg_logprob": -0.1820755364759913, "compression_ratio": 1.7620967741935485, "no_speech_prob": 1.0676999409042764e-06}, {"id": 369, "seek": 228956, "start": 2294.24, "end": 2300.7999999999997, "text": " wondering the whole book will be covered in either two or three courses. In the past it's", "tokens": [6359, 264, 1379, 1446, 486, 312, 5343, 294, 2139, 732, 420, 1045, 7712, 13, 682, 264, 1791, 309, 311], "temperature": 0.0, "avg_logprob": -0.1820755364759913, "compression_ratio": 1.7620967741935485, "no_speech_prob": 1.0676999409042764e-06}, {"id": 370, "seek": 228956, "start": 2300.7999999999997, "end": 2305.72, "text": " generally been two courses to cover about the amount of stuff in the book but we'll", "tokens": [5101, 668, 732, 7712, 281, 2060, 466, 264, 2372, 295, 1507, 294, 264, 1446, 457, 321, 603], "temperature": 0.0, "avg_logprob": -0.1820755364759913, "compression_ratio": 1.7620967741935485, "no_speech_prob": 1.0676999409042764e-06}, {"id": 371, "seek": 228956, "start": 2305.72, "end": 2311.32, "text": " see how it goes because the books pretty big 500 pages. When you say two courses you mean", "tokens": [536, 577, 309, 1709, 570, 264, 3642, 1238, 955, 5923, 7183, 13, 1133, 291, 584, 732, 7712, 291, 914], "temperature": 0.0, "avg_logprob": -0.1820755364759913, "compression_ratio": 1.7620967741935485, "no_speech_prob": 1.0676999409042764e-06}, {"id": 372, "seek": 228956, "start": 2311.32, "end": 2315.72, "text": " 14 lessons. 14 lessons yeah so it'd be like 14 or 21 lessons to get through the whole", "tokens": [3499, 8820, 13, 3499, 8820, 1338, 370, 309, 1116, 312, 411, 3499, 420, 5080, 8820, 281, 483, 807, 264, 1379], "temperature": 0.0, "avg_logprob": -0.1820755364759913, "compression_ratio": 1.7620967741935485, "no_speech_prob": 1.0676999409042764e-06}, {"id": 373, "seek": 231572, "start": 2315.72, "end": 2320.64, "text": " book. Although having said that by the end of the first lesson hopefully there'll be", "tokens": [1446, 13, 5780, 1419, 848, 300, 538, 264, 917, 295, 264, 700, 6898, 4696, 456, 603, 312], "temperature": 0.0, "avg_logprob": -0.08845293672778938, "compression_ratio": 1.6462264150943395, "no_speech_prob": 3.041578111151466e-06}, {"id": 374, "seek": 231572, "start": 2320.64, "end": 2324.9599999999996, "text": " kind of like enough momentum and understanding that the reading the book independently will", "tokens": [733, 295, 411, 1547, 11244, 293, 3701, 300, 264, 3760, 264, 1446, 21761, 486], "temperature": 0.0, "avg_logprob": -0.08845293672778938, "compression_ratio": 1.6462264150943395, "no_speech_prob": 3.041578111151466e-06}, {"id": 375, "seek": 231572, "start": 2324.9599999999996, "end": 2331.3199999999997, "text": " be more useful and you'll have also kind of gained a community of folks on the forums", "tokens": [312, 544, 4420, 293, 291, 603, 362, 611, 733, 295, 12634, 257, 1768, 295, 4024, 322, 264, 26998], "temperature": 0.0, "avg_logprob": -0.08845293672778938, "compression_ratio": 1.6462264150943395, "no_speech_prob": 3.041578111151466e-06}, {"id": 376, "seek": 231572, "start": 2331.3199999999997, "end": 2339.52, "text": " that you can hang out with and ask questions of and so forth. So in in the second part", "tokens": [300, 291, 393, 3967, 484, 365, 293, 1029, 1651, 295, 293, 370, 5220, 13, 407, 294, 294, 264, 1150, 644], "temperature": 0.0, "avg_logprob": -0.08845293672778938, "compression_ratio": 1.6462264150943395, "no_speech_prob": 3.041578111151466e-06}, {"id": 377, "seek": 233952, "start": 2339.52, "end": 2346.2, "text": " of the course we're going to be talking about putting stuff in production and we're so to", "tokens": [295, 264, 1164, 321, 434, 516, 281, 312, 1417, 466, 3372, 1507, 294, 4265, 293, 321, 434, 370, 281], "temperature": 0.0, "avg_logprob": -0.10126688847175011, "compression_ratio": 1.763779527559055, "no_speech_prob": 1.184273151011439e-05}, {"id": 378, "seek": 233952, "start": 2346.2, "end": 2351.64, "text": " do that we need to understand like what are the capabilities and limitations of of deep", "tokens": [360, 300, 321, 643, 281, 1223, 411, 437, 366, 264, 10862, 293, 15705, 295, 295, 2452], "temperature": 0.0, "avg_logprob": -0.10126688847175011, "compression_ratio": 1.763779527559055, "no_speech_prob": 1.184273151011439e-05}, {"id": 379, "seek": 233952, "start": 2351.64, "end": 2357.96, "text": " learning. What are the kinds of projects that even make sense to try to put in production", "tokens": [2539, 13, 708, 366, 264, 3685, 295, 4455, 300, 754, 652, 2020, 281, 853, 281, 829, 294, 4265], "temperature": 0.0, "avg_logprob": -0.10126688847175011, "compression_ratio": 1.763779527559055, "no_speech_prob": 1.184273151011439e-05}, {"id": 380, "seek": 233952, "start": 2357.96, "end": 2361.24, "text": " and you know one of the key things I should mention in in the Balkan in this course is", "tokens": [293, 291, 458, 472, 295, 264, 2141, 721, 286, 820, 2152, 294, 294, 264, 36289, 282, 294, 341, 1164, 307], "temperature": 0.0, "avg_logprob": -0.10126688847175011, "compression_ratio": 1.763779527559055, "no_speech_prob": 1.184273151011439e-05}, {"id": 381, "seek": 233952, "start": 2361.24, "end": 2367.0, "text": " that the first two or three lessons and chapters there's a lot of stuff which is designed not", "tokens": [300, 264, 700, 732, 420, 1045, 8820, 293, 20013, 456, 311, 257, 688, 295, 1507, 597, 307, 4761, 406], "temperature": 0.0, "avg_logprob": -0.10126688847175011, "compression_ratio": 1.763779527559055, "no_speech_prob": 1.184273151011439e-05}, {"id": 382, "seek": 236700, "start": 2367.0, "end": 2374.92, "text": " just for the coders but for everybody. There's lots of information about like what are the", "tokens": [445, 337, 264, 17656, 433, 457, 337, 2201, 13, 821, 311, 3195, 295, 1589, 466, 411, 437, 366, 264], "temperature": 0.0, "avg_logprob": -0.1430351367363563, "compression_ratio": 1.7038461538461538, "no_speech_prob": 1.4823366427663132e-06}, {"id": 383, "seek": 236700, "start": 2374.92, "end": 2378.76, "text": " practical things you need to know to make deep learning work and so one of the things", "tokens": [8496, 721, 291, 643, 281, 458, 281, 652, 2452, 2539, 589, 293, 370, 472, 295, 264, 721], "temperature": 0.0, "avg_logprob": -0.1430351367363563, "compression_ratio": 1.7038461538461538, "no_speech_prob": 1.4823366427663132e-06}, {"id": 384, "seek": 236700, "start": 2378.76, "end": 2384.44, "text": " you need to know is like well what's deep learning actually good at at the moment. So", "tokens": [291, 643, 281, 458, 307, 411, 731, 437, 311, 2452, 2539, 767, 665, 412, 412, 264, 1623, 13, 407], "temperature": 0.0, "avg_logprob": -0.1430351367363563, "compression_ratio": 1.7038461538461538, "no_speech_prob": 1.4823366427663132e-06}, {"id": 385, "seek": 236700, "start": 2384.44, "end": 2390.28, "text": " I'll summarize what the book says about this but there are the kind of four key areas that", "tokens": [286, 603, 20858, 437, 264, 1446, 1619, 466, 341, 457, 456, 366, 264, 733, 295, 1451, 2141, 3179, 300], "temperature": 0.0, "avg_logprob": -0.1430351367363563, "compression_ratio": 1.7038461538461538, "no_speech_prob": 1.4823366427663132e-06}, {"id": 386, "seek": 236700, "start": 2390.28, "end": 2396.36, "text": " we have as applications in fast AI computer vision text tabular and what I've called here", "tokens": [321, 362, 382, 5821, 294, 2370, 7318, 3820, 5201, 2487, 4421, 1040, 293, 437, 286, 600, 1219, 510], "temperature": 0.0, "avg_logprob": -0.1430351367363563, "compression_ratio": 1.7038461538461538, "no_speech_prob": 1.4823366427663132e-06}, {"id": 387, "seek": 239636, "start": 2396.36, "end": 2400.88, "text": " RECSIS. This stands for Recommendation Systems and specifically a technique called Collaborative", "tokens": [497, 8140, 50, 2343, 13, 639, 7382, 337, 49545, 521, 399, 27059, 293, 4682, 257, 6532, 1219, 44483, 1166], "temperature": 0.0, "avg_logprob": -0.16724394048963273, "compression_ratio": 1.602112676056338, "no_speech_prob": 2.7693399715644773e-06}, {"id": 388, "seek": 239636, "start": 2400.88, "end": 2407.0, "text": " Filtering which we briefly saw last week. Sorry another question. Is are there any pre-trained", "tokens": [7905, 34200, 597, 321, 10515, 1866, 1036, 1243, 13, 4919, 1071, 1168, 13, 1119, 366, 456, 604, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.16724394048963273, "compression_ratio": 1.602112676056338, "no_speech_prob": 2.7693399715644773e-06}, {"id": 389, "seek": 239636, "start": 2407.0, "end": 2411.8, "text": " weights available other than the ones from ImageNet that we can use? If yes when should", "tokens": [17443, 2435, 661, 813, 264, 2306, 490, 29903, 31890, 300, 321, 393, 764, 30, 759, 2086, 562, 820], "temperature": 0.0, "avg_logprob": -0.16724394048963273, "compression_ratio": 1.602112676056338, "no_speech_prob": 2.7693399715644773e-06}, {"id": 390, "seek": 239636, "start": 2411.8, "end": 2418.7200000000003, "text": " we use others in one ImageNet? Oh that's a really great question. So yes there are a", "tokens": [321, 764, 2357, 294, 472, 29903, 31890, 30, 876, 300, 311, 257, 534, 869, 1168, 13, 407, 2086, 456, 366, 257], "temperature": 0.0, "avg_logprob": -0.16724394048963273, "compression_ratio": 1.602112676056338, "no_speech_prob": 2.7693399715644773e-06}, {"id": 391, "seek": 239636, "start": 2418.7200000000003, "end": 2424.52, "text": " lot of pre-trained models and one way to find them. And also you're currently just showing", "tokens": [688, 295, 659, 12, 17227, 2001, 5245, 293, 472, 636, 281, 915, 552, 13, 400, 611, 291, 434, 4362, 445, 4099], "temperature": 0.0, "avg_logprob": -0.16724394048963273, "compression_ratio": 1.602112676056338, "no_speech_prob": 2.7693399715644773e-06}, {"id": 392, "seek": 242452, "start": 2424.52, "end": 2434.6, "text": " that. Okay great. One great way to find them is you can look up modelzoo which is a common", "tokens": [300, 13, 1033, 869, 13, 1485, 869, 636, 281, 915, 552, 307, 291, 393, 574, 493, 2316, 89, 1986, 597, 307, 257, 2689], "temperature": 0.0, "avg_logprob": -0.23747323989868163, "compression_ratio": 1.4330708661417322, "no_speech_prob": 1.4144713986752322e-06}, {"id": 393, "seek": 242452, "start": 2434.6, "end": 2442.8, "text": " name for like places that have lots of different models and so here's lots of modelzoo's or", "tokens": [1315, 337, 411, 3190, 300, 362, 3195, 295, 819, 5245, 293, 370, 510, 311, 3195, 295, 2316, 89, 1986, 311, 420], "temperature": 0.0, "avg_logprob": -0.23747323989868163, "compression_ratio": 1.4330708661417322, "no_speech_prob": 1.4144713986752322e-06}, {"id": 394, "seek": 244280, "start": 2442.8, "end": 2456.4, "text": " you can look for pre-trained models. And so yeah there's quite a few. Unfortunately not", "tokens": [291, 393, 574, 337, 659, 12, 17227, 2001, 5245, 13, 400, 370, 1338, 456, 311, 1596, 257, 1326, 13, 8590, 406], "temperature": 0.0, "avg_logprob": -0.12950735381155304, "compression_ratio": 1.411764705882353, "no_speech_prob": 7.734416129778765e-08}, {"id": 395, "seek": 244280, "start": 2456.4, "end": 2463.1600000000003, "text": " as wide a variety as I would like. Most are still on ImageNet or similar kinds of general", "tokens": [382, 4874, 257, 5673, 382, 286, 576, 411, 13, 4534, 366, 920, 322, 29903, 31890, 420, 2531, 3685, 295, 2674], "temperature": 0.0, "avg_logprob": -0.12950735381155304, "compression_ratio": 1.411764705882353, "no_speech_prob": 7.734416129778765e-08}, {"id": 396, "seek": 244280, "start": 2463.1600000000003, "end": 2470.52, "text": " photos. For example medical imaging there's hardly any. There's a lot of opportunities", "tokens": [5787, 13, 1171, 1365, 4625, 25036, 456, 311, 13572, 604, 13, 821, 311, 257, 688, 295, 4786], "temperature": 0.0, "avg_logprob": -0.12950735381155304, "compression_ratio": 1.411764705882353, "no_speech_prob": 7.734416129778765e-08}, {"id": 397, "seek": 247052, "start": 2470.52, "end": 2474.92, "text": " for people to create domain specific pre-trained models. It's still an area that's really under", "tokens": [337, 561, 281, 1884, 9274, 2685, 659, 12, 17227, 2001, 5245, 13, 467, 311, 920, 364, 1859, 300, 311, 534, 833], "temperature": 0.0, "avg_logprob": -0.08507513999938965, "compression_ratio": 1.5720338983050848, "no_speech_prob": 7.002141728662536e-07}, {"id": 398, "seek": 247052, "start": 2474.92, "end": 2483.08, "text": " done because not enough people are working on transfer learning. Okay so as I was mentioning", "tokens": [1096, 570, 406, 1547, 561, 366, 1364, 322, 5003, 2539, 13, 1033, 370, 382, 286, 390, 18315], "temperature": 0.0, "avg_logprob": -0.08507513999938965, "compression_ratio": 1.5720338983050848, "no_speech_prob": 7.002141728662536e-07}, {"id": 399, "seek": 247052, "start": 2483.08, "end": 2491.4, "text": " we've kind of got these four applications that we've talked about a bit. And deep learning", "tokens": [321, 600, 733, 295, 658, 613, 1451, 5821, 300, 321, 600, 2825, 466, 257, 857, 13, 400, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.08507513999938965, "compression_ratio": 1.5720338983050848, "no_speech_prob": 7.002141728662536e-07}, {"id": 400, "seek": 247052, "start": 2491.4, "end": 2499.18, "text": " is pretty you know pretty good at all of those. Tabular data like spreadsheets and database", "tokens": [307, 1238, 291, 458, 1238, 665, 412, 439, 295, 729, 13, 14106, 1040, 1412, 411, 23651, 1385, 293, 8149], "temperature": 0.0, "avg_logprob": -0.08507513999938965, "compression_ratio": 1.5720338983050848, "no_speech_prob": 7.002141728662536e-07}, {"id": 401, "seek": 249918, "start": 2499.18, "end": 2504.64, "text": " tables is an area where deep learning is not always the best choice but it's particularly", "tokens": [8020, 307, 364, 1859, 689, 2452, 2539, 307, 406, 1009, 264, 1151, 3922, 457, 309, 311, 4098], "temperature": 0.0, "avg_logprob": -0.09313029128235656, "compression_ratio": 1.6650943396226414, "no_speech_prob": 9.721510423332802e-07}, {"id": 402, "seek": 249918, "start": 2504.64, "end": 2509.08, "text": " good for things involving high cardinality variables. That means variables that have", "tokens": [665, 337, 721, 17030, 1090, 2920, 259, 1860, 9102, 13, 663, 1355, 9102, 300, 362], "temperature": 0.0, "avg_logprob": -0.09313029128235656, "compression_ratio": 1.6650943396226414, "no_speech_prob": 9.721510423332802e-07}, {"id": 403, "seek": 249918, "start": 2509.08, "end": 2516.3999999999996, "text": " like lots and lots of discrete levels like zip code or product ID or something like that.", "tokens": [411, 3195, 293, 3195, 295, 27706, 4358, 411, 20730, 3089, 420, 1674, 7348, 420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.09313029128235656, "compression_ratio": 1.6650943396226414, "no_speech_prob": 9.721510423332802e-07}, {"id": 404, "seek": 249918, "start": 2516.3999999999996, "end": 2525.18, "text": " Deep learning is really pretty great for those in particular. For text it's pretty great", "tokens": [14895, 2539, 307, 534, 1238, 869, 337, 729, 294, 1729, 13, 1171, 2487, 309, 311, 1238, 869], "temperature": 0.0, "avg_logprob": -0.09313029128235656, "compression_ratio": 1.6650943396226414, "no_speech_prob": 9.721510423332802e-07}, {"id": 405, "seek": 252518, "start": 2525.18, "end": 2531.6, "text": " at things like classification and translation. It's actually terrible for conversation. So", "tokens": [412, 721, 411, 21538, 293, 12853, 13, 467, 311, 767, 6237, 337, 3761, 13, 407], "temperature": 0.0, "avg_logprob": -0.1168263818799835, "compression_ratio": 1.80078125, "no_speech_prob": 4.592120603774674e-07}, {"id": 406, "seek": 252518, "start": 2531.6, "end": 2534.96, "text": " that's been something that's been a huge disappointment for a lot of companies. They tried to create", "tokens": [300, 311, 668, 746, 300, 311, 668, 257, 2603, 28175, 337, 257, 688, 295, 3431, 13, 814, 3031, 281, 1884], "temperature": 0.0, "avg_logprob": -0.1168263818799835, "compression_ratio": 1.80078125, "no_speech_prob": 4.592120603774674e-07}, {"id": 407, "seek": 252518, "start": 2534.96, "end": 2541.52, "text": " these like conversation bots. But actually deep learning isn't good at providing accurate", "tokens": [613, 411, 3761, 35410, 13, 583, 767, 2452, 2539, 1943, 380, 665, 412, 6530, 8559], "temperature": 0.0, "avg_logprob": -0.1168263818799835, "compression_ratio": 1.80078125, "no_speech_prob": 4.592120603774674e-07}, {"id": 408, "seek": 252518, "start": 2541.52, "end": 2547.0, "text": " information. It's good at providing things that sound accurate and sound compelling but", "tokens": [1589, 13, 467, 311, 665, 412, 6530, 721, 300, 1626, 8559, 293, 1626, 20050, 457], "temperature": 0.0, "avg_logprob": -0.1168263818799835, "compression_ratio": 1.80078125, "no_speech_prob": 4.592120603774674e-07}, {"id": 409, "seek": 252518, "start": 2547.0, "end": 2554.7999999999997, "text": " we don't really have great ways yet of actually making sure it's correct. One big issue for", "tokens": [321, 500, 380, 534, 362, 869, 2098, 1939, 295, 767, 1455, 988, 309, 311, 3006, 13, 1485, 955, 2734, 337], "temperature": 0.0, "avg_logprob": -0.1168263818799835, "compression_ratio": 1.80078125, "no_speech_prob": 4.592120603774674e-07}, {"id": 410, "seek": 255480, "start": 2554.8, "end": 2560.6800000000003, "text": " recommendation systems, collaborative filtering, is that deep learning is focused on making", "tokens": [11879, 3652, 11, 16555, 30822, 11, 307, 300, 2452, 2539, 307, 5178, 322, 1455], "temperature": 0.0, "avg_logprob": -0.07052617951443321, "compression_ratio": 1.6216216216216217, "no_speech_prob": 5.714990720662172e-07}, {"id": 411, "seek": 255480, "start": 2560.6800000000003, "end": 2567.36, "text": " predictions which don't necessarily actually mean creating useful recommendations. We'll", "tokens": [21264, 597, 500, 380, 4725, 767, 914, 4084, 4420, 10434, 13, 492, 603], "temperature": 0.0, "avg_logprob": -0.07052617951443321, "compression_ratio": 1.6216216216216217, "no_speech_prob": 5.714990720662172e-07}, {"id": 412, "seek": 255480, "start": 2567.36, "end": 2574.48, "text": " see what that means in a moment. Deep learning is also good at multimodal. That means things", "tokens": [536, 437, 300, 1355, 294, 257, 1623, 13, 14895, 2539, 307, 611, 665, 412, 32972, 378, 304, 13, 663, 1355, 721], "temperature": 0.0, "avg_logprob": -0.07052617951443321, "compression_ratio": 1.6216216216216217, "no_speech_prob": 5.714990720662172e-07}, {"id": 413, "seek": 255480, "start": 2574.48, "end": 2579.36, "text": " where you've got multiple different types of data. So you might have some tabular data", "tokens": [689, 291, 600, 658, 3866, 819, 3467, 295, 1412, 13, 407, 291, 1062, 362, 512, 4421, 1040, 1412], "temperature": 0.0, "avg_logprob": -0.07052617951443321, "compression_ratio": 1.6216216216216217, "no_speech_prob": 5.714990720662172e-07}, {"id": 414, "seek": 257936, "start": 2579.36, "end": 2587.36, "text": " including a text column and an image and some collaborative filtering data. And combining", "tokens": [3009, 257, 2487, 7738, 293, 364, 3256, 293, 512, 16555, 30822, 1412, 13, 400, 21928], "temperature": 0.0, "avg_logprob": -0.12250402646187024, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.1079039552441827e-07}, {"id": 415, "seek": 257936, "start": 2587.36, "end": 2593.96, "text": " that all together is something that deep learning is really good at. So for example putting", "tokens": [300, 439, 1214, 307, 746, 300, 2452, 2539, 307, 534, 665, 412, 13, 407, 337, 1365, 3372], "temperature": 0.0, "avg_logprob": -0.12250402646187024, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.1079039552441827e-07}, {"id": 416, "seek": 257936, "start": 2593.96, "end": 2599.58, "text": " captions on photos is something which deep learning is pretty good at. Although again", "tokens": [44832, 322, 5787, 307, 746, 597, 2452, 2539, 307, 1238, 665, 412, 13, 5780, 797], "temperature": 0.0, "avg_logprob": -0.12250402646187024, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.1079039552441827e-07}, {"id": 417, "seek": 257936, "start": 2599.58, "end": 2604.2400000000002, "text": " it's not very good at being accurate. So you know it might say this is a picture of two", "tokens": [309, 311, 406, 588, 665, 412, 885, 8559, 13, 407, 291, 458, 309, 1062, 584, 341, 307, 257, 3036, 295, 732], "temperature": 0.0, "avg_logprob": -0.12250402646187024, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.1079039552441827e-07}, {"id": 418, "seek": 260424, "start": 2604.24, "end": 2612.2, "text": " birds when it's actually a picture of three birds. And then this other category there's", "tokens": [9009, 562, 309, 311, 767, 257, 3036, 295, 1045, 9009, 13, 400, 550, 341, 661, 7719, 456, 311], "temperature": 0.0, "avg_logprob": -0.1160484540580523, "compression_ratio": 1.6544117647058822, "no_speech_prob": 9.874599982140353e-07}, {"id": 419, "seek": 260424, "start": 2612.2, "end": 2616.9199999999996, "text": " lots and lots of things that you can do with deep learning by being creative about the", "tokens": [3195, 293, 3195, 295, 721, 300, 291, 393, 360, 365, 2452, 2539, 538, 885, 5880, 466, 264], "temperature": 0.0, "avg_logprob": -0.1160484540580523, "compression_ratio": 1.6544117647058822, "no_speech_prob": 9.874599982140353e-07}, {"id": 420, "seek": 260424, "start": 2616.9199999999996, "end": 2623.14, "text": " use of these kinds of other application based approaches. For example an approach that we", "tokens": [764, 295, 613, 3685, 295, 661, 3861, 2361, 11587, 13, 1171, 1365, 364, 3109, 300, 321], "temperature": 0.0, "avg_logprob": -0.1160484540580523, "compression_ratio": 1.6544117647058822, "no_speech_prob": 9.874599982140353e-07}, {"id": 421, "seek": 260424, "start": 2623.14, "end": 2629.16, "text": " developed for natural language processing called ULMFIT that we're learning in the course.", "tokens": [4743, 337, 3303, 2856, 9007, 1219, 624, 43, 44, 37, 3927, 300, 321, 434, 2539, 294, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.1160484540580523, "compression_ratio": 1.6544117647058822, "no_speech_prob": 9.874599982140353e-07}, {"id": 422, "seek": 260424, "start": 2629.16, "end": 2633.52, "text": " It turns out that it's also fantastic at doing protein analysis. If you think of the different", "tokens": [467, 4523, 484, 300, 309, 311, 611, 5456, 412, 884, 7944, 5215, 13, 759, 291, 519, 295, 264, 819], "temperature": 0.0, "avg_logprob": -0.1160484540580523, "compression_ratio": 1.6544117647058822, "no_speech_prob": 9.874599982140353e-07}, {"id": 423, "seek": 263352, "start": 2633.52, "end": 2639.56, "text": " proteins as being different words and they're in a sequence which has some kind of state", "tokens": [15577, 382, 885, 819, 2283, 293, 436, 434, 294, 257, 8310, 597, 575, 512, 733, 295, 1785], "temperature": 0.0, "avg_logprob": -0.08148808811986169, "compression_ratio": 1.608294930875576, "no_speech_prob": 3.52090722799403e-07}, {"id": 424, "seek": 263352, "start": 2639.56, "end": 2645.14, "text": " and meaning it turns out that ULMFIT works really well for protein analysis. So often", "tokens": [293, 3620, 309, 4523, 484, 300, 624, 43, 44, 37, 3927, 1985, 534, 731, 337, 7944, 5215, 13, 407, 2049], "temperature": 0.0, "avg_logprob": -0.08148808811986169, "compression_ratio": 1.608294930875576, "no_speech_prob": 3.52090722799403e-07}, {"id": 425, "seek": 263352, "start": 2645.14, "end": 2651.7, "text": " it's about kind of being being creative. So to decide like for the product that you're", "tokens": [309, 311, 466, 733, 295, 885, 885, 5880, 13, 407, 281, 4536, 411, 337, 264, 1674, 300, 291, 434], "temperature": 0.0, "avg_logprob": -0.08148808811986169, "compression_ratio": 1.608294930875576, "no_speech_prob": 3.52090722799403e-07}, {"id": 426, "seek": 263352, "start": 2651.7, "end": 2657.2, "text": " trying to build is deep learning going to work well for it. In the end you kind of just", "tokens": [1382, 281, 1322, 307, 2452, 2539, 516, 281, 589, 731, 337, 309, 13, 682, 264, 917, 291, 733, 295, 445], "temperature": 0.0, "avg_logprob": -0.08148808811986169, "compression_ratio": 1.608294930875576, "no_speech_prob": 3.52090722799403e-07}, {"id": 427, "seek": 265720, "start": 2657.2, "end": 2664.6, "text": " have to try it and see. But if you if you do a search you know hopefully you can find", "tokens": [362, 281, 853, 309, 293, 536, 13, 583, 498, 291, 498, 291, 360, 257, 3164, 291, 458, 4696, 291, 393, 915], "temperature": 0.0, "avg_logprob": -0.09860206344752635, "compression_ratio": 1.6380597014925373, "no_speech_prob": 9.132479021900508e-07}, {"id": 428, "seek": 265720, "start": 2664.6, "end": 2668.96, "text": " examples about the people that have tried something similar. Even if you can't that", "tokens": [5110, 466, 264, 561, 300, 362, 3031, 746, 2531, 13, 2754, 498, 291, 393, 380, 300], "temperature": 0.0, "avg_logprob": -0.09860206344752635, "compression_ratio": 1.6380597014925373, "no_speech_prob": 9.132479021900508e-07}, {"id": 429, "seek": 265720, "start": 2668.96, "end": 2674.4399999999996, "text": " doesn't mean it's not going to work. So for example I mentioned the collaborative filtering", "tokens": [1177, 380, 914, 309, 311, 406, 516, 281, 589, 13, 407, 337, 1365, 286, 2835, 264, 16555, 30822], "temperature": 0.0, "avg_logprob": -0.09860206344752635, "compression_ratio": 1.6380597014925373, "no_speech_prob": 9.132479021900508e-07}, {"id": 430, "seek": 265720, "start": 2674.4399999999996, "end": 2680.0, "text": " issue where a recommendation and a prediction are not necessarily the same thing. You can", "tokens": [2734, 689, 257, 11879, 293, 257, 17630, 366, 406, 4725, 264, 912, 551, 13, 509, 393], "temperature": 0.0, "avg_logprob": -0.09860206344752635, "compression_ratio": 1.6380597014925373, "no_speech_prob": 9.132479021900508e-07}, {"id": 431, "seek": 265720, "start": 2680.0, "end": 2686.7599999999998, "text": " see this on Amazon for example quite often. So I bought a Terry Pratchett book and then", "tokens": [536, 341, 322, 6795, 337, 1365, 1596, 2049, 13, 407, 286, 4243, 257, 21983, 2114, 852, 3093, 1446, 293, 550], "temperature": 0.0, "avg_logprob": -0.09860206344752635, "compression_ratio": 1.6380597014925373, "no_speech_prob": 9.132479021900508e-07}, {"id": 432, "seek": 268676, "start": 2686.76, "end": 2692.2000000000003, "text": " Amazon tried for months to get me to buy more Terry Pratchett books. Now that must be because", "tokens": [6795, 3031, 337, 2493, 281, 483, 385, 281, 2256, 544, 21983, 2114, 852, 3093, 3642, 13, 823, 300, 1633, 312, 570], "temperature": 0.0, "avg_logprob": -0.07502413245866883, "compression_ratio": 1.7615384615384615, "no_speech_prob": 1.0348502428314532e-06}, {"id": 433, "seek": 268676, "start": 2692.2000000000003, "end": 2697.48, "text": " their predictive model said that people who bought one particular Terry Pratchett book", "tokens": [641, 35521, 2316, 848, 300, 561, 567, 4243, 472, 1729, 21983, 2114, 852, 3093, 1446], "temperature": 0.0, "avg_logprob": -0.07502413245866883, "compression_ratio": 1.7615384615384615, "no_speech_prob": 1.0348502428314532e-06}, {"id": 434, "seek": 268676, "start": 2697.48, "end": 2701.96, "text": " are likely to also buy other Terry Pratchett books. But from the point of view of like", "tokens": [366, 3700, 281, 611, 2256, 661, 21983, 2114, 852, 3093, 3642, 13, 583, 490, 264, 935, 295, 1910, 295, 411], "temperature": 0.0, "avg_logprob": -0.07502413245866883, "compression_ratio": 1.7615384615384615, "no_speech_prob": 1.0348502428314532e-06}, {"id": 435, "seek": 268676, "start": 2701.96, "end": 2707.92, "text": " well is this going to change my buying behavior? Probably not right. Like if I liked that book", "tokens": [731, 307, 341, 516, 281, 1319, 452, 6382, 5223, 30, 9210, 406, 558, 13, 1743, 498, 286, 4501, 300, 1446], "temperature": 0.0, "avg_logprob": -0.07502413245866883, "compression_ratio": 1.7615384615384615, "no_speech_prob": 1.0348502428314532e-06}, {"id": 436, "seek": 268676, "start": 2707.92, "end": 2712.1200000000003, "text": " I already know I like that author and I already know that like they probably wrote other things", "tokens": [286, 1217, 458, 286, 411, 300, 3793, 293, 286, 1217, 458, 300, 411, 436, 1391, 4114, 661, 721], "temperature": 0.0, "avg_logprob": -0.07502413245866883, "compression_ratio": 1.7615384615384615, "no_speech_prob": 1.0348502428314532e-06}, {"id": 437, "seek": 271212, "start": 2712.12, "end": 2717.12, "text": " so I'll go and buy it anyway. So this would be an example of like Amazon probably not", "tokens": [370, 286, 603, 352, 293, 2256, 309, 4033, 13, 407, 341, 576, 312, 364, 1365, 295, 411, 6795, 1391, 406], "temperature": 0.0, "avg_logprob": -0.11600483680257992, "compression_ratio": 1.6376811594202898, "no_speech_prob": 6.312648537232235e-08}, {"id": 438, "seek": 271212, "start": 2717.12, "end": 2723.12, "text": " being very smart here. They're actually showing me collaborative filtering predictions rather", "tokens": [885, 588, 4069, 510, 13, 814, 434, 767, 4099, 385, 16555, 30822, 21264, 2831], "temperature": 0.0, "avg_logprob": -0.11600483680257992, "compression_ratio": 1.6376811594202898, "no_speech_prob": 6.312648537232235e-08}, {"id": 439, "seek": 271212, "start": 2723.12, "end": 2728.3199999999997, "text": " than actually figuring out how to optimize your recommendation. So an optimized recommendation", "tokens": [813, 767, 15213, 484, 577, 281, 19719, 428, 11879, 13, 407, 364, 26941, 11879], "temperature": 0.0, "avg_logprob": -0.11600483680257992, "compression_ratio": 1.6376811594202898, "no_speech_prob": 6.312648537232235e-08}, {"id": 440, "seek": 271212, "start": 2728.3199999999997, "end": 2734.52, "text": " would be something more like your local human bookseller might do where they might say oh", "tokens": [576, 312, 746, 544, 411, 428, 2654, 1952, 1446, 405, 4658, 1062, 360, 689, 436, 1062, 584, 1954], "temperature": 0.0, "avg_logprob": -0.11600483680257992, "compression_ratio": 1.6376811594202898, "no_speech_prob": 6.312648537232235e-08}, {"id": 441, "seek": 271212, "start": 2734.52, "end": 2739.64, "text": " you like Terry Pratchett well let me tell you about other kind of comedy fantasy sci-fi", "tokens": [291, 411, 21983, 2114, 852, 3093, 731, 718, 385, 980, 291, 466, 661, 733, 295, 13394, 13861, 2180, 12, 13325], "temperature": 0.0, "avg_logprob": -0.11600483680257992, "compression_ratio": 1.6376811594202898, "no_speech_prob": 6.312648537232235e-08}, {"id": 442, "seek": 273964, "start": 2739.64, "end": 2745.04, "text": " writers on the similar vein who you might not have heard about before. So the difference", "tokens": [13491, 322, 264, 2531, 30669, 567, 291, 1062, 406, 362, 2198, 466, 949, 13, 407, 264, 2649], "temperature": 0.0, "avg_logprob": -0.1018256849172164, "compression_ratio": 1.7011494252873562, "no_speech_prob": 2.3823642436582304e-07}, {"id": 443, "seek": 273964, "start": 2745.04, "end": 2752.64, "text": " between recommendations and predictions is super important. So I wanted to talk about", "tokens": [1296, 10434, 293, 21264, 307, 1687, 1021, 13, 407, 286, 1415, 281, 751, 466], "temperature": 0.0, "avg_logprob": -0.1018256849172164, "compression_ratio": 1.7011494252873562, "no_speech_prob": 2.3823642436582304e-07}, {"id": 444, "seek": 273964, "start": 2752.64, "end": 2758.44, "text": " a really important issue around interpreting models and for a case study for this I thought", "tokens": [257, 534, 1021, 2734, 926, 37395, 5245, 293, 337, 257, 1389, 2979, 337, 341, 286, 1194], "temperature": 0.0, "avg_logprob": -0.1018256849172164, "compression_ratio": 1.7011494252873562, "no_speech_prob": 2.3823642436582304e-07}, {"id": 445, "seek": 273964, "start": 2758.44, "end": 2763.48, "text": " we let's pick something that's actually super important right now which is a model in this", "tokens": [321, 718, 311, 1888, 746, 300, 311, 767, 1687, 1021, 558, 586, 597, 307, 257, 2316, 294, 341], "temperature": 0.0, "avg_logprob": -0.1018256849172164, "compression_ratio": 1.7011494252873562, "no_speech_prob": 2.3823642436582304e-07}, {"id": 446, "seek": 273964, "start": 2763.48, "end": 2766.6, "text": " paper. One of the things we're going to try and do in this course is learn how to read", "tokens": [3035, 13, 1485, 295, 264, 721, 321, 434, 516, 281, 853, 293, 360, 294, 341, 1164, 307, 1466, 577, 281, 1401], "temperature": 0.0, "avg_logprob": -0.1018256849172164, "compression_ratio": 1.7011494252873562, "no_speech_prob": 2.3823642436582304e-07}, {"id": 447, "seek": 276660, "start": 2766.6, "end": 2773.56, "text": " papers. So here is a paper which you would love for everybody to read called High Temperature", "tokens": [10577, 13, 407, 510, 307, 257, 3035, 597, 291, 576, 959, 337, 2201, 281, 1401, 1219, 5229, 34864, 1503], "temperature": 0.0, "avg_logprob": -0.09528445306225358, "compression_ratio": 1.7078651685393258, "no_speech_prob": 7.571133551209641e-07}, {"id": 448, "seek": 276660, "start": 2773.56, "end": 2778.8399999999997, "text": " and High Humidity Reduce the Transmission of COVID-19. Now this is a very important issue", "tokens": [293, 5229, 12877, 17711, 4477, 4176, 264, 6531, 29797, 295, 4566, 12, 3405, 13, 823, 341, 307, 257, 588, 1021, 2734], "temperature": 0.0, "avg_logprob": -0.09528445306225358, "compression_ratio": 1.7078651685393258, "no_speech_prob": 7.571133551209641e-07}, {"id": 449, "seek": 276660, "start": 2778.8399999999997, "end": 2783.2799999999997, "text": " because if the claim of this paper is true then that would mean that this is going to", "tokens": [570, 498, 264, 3932, 295, 341, 3035, 307, 2074, 550, 300, 576, 914, 300, 341, 307, 516, 281], "temperature": 0.0, "avg_logprob": -0.09528445306225358, "compression_ratio": 1.7078651685393258, "no_speech_prob": 7.571133551209641e-07}, {"id": 450, "seek": 276660, "start": 2783.2799999999997, "end": 2788.44, "text": " be a seasonal disease and if this is a seasonal disease and it's going to have massive policy", "tokens": [312, 257, 27421, 4752, 293, 498, 341, 307, 257, 27421, 4752, 293, 309, 311, 516, 281, 362, 5994, 3897], "temperature": 0.0, "avg_logprob": -0.09528445306225358, "compression_ratio": 1.7078651685393258, "no_speech_prob": 7.571133551209641e-07}, {"id": 451, "seek": 276660, "start": 2788.44, "end": 2795.68, "text": " implications. So let's try and find out how this was modeled and understand how to interpret", "tokens": [16602, 13, 407, 718, 311, 853, 293, 915, 484, 577, 341, 390, 37140, 293, 1223, 577, 281, 7302], "temperature": 0.0, "avg_logprob": -0.09528445306225358, "compression_ratio": 1.7078651685393258, "no_speech_prob": 7.571133551209641e-07}, {"id": 452, "seek": 279568, "start": 2795.68, "end": 2806.6, "text": " this model. So this is a key picture from the paper and what they've done here is they've", "tokens": [341, 2316, 13, 407, 341, 307, 257, 2141, 3036, 490, 264, 3035, 293, 437, 436, 600, 1096, 510, 307, 436, 600], "temperature": 0.0, "avg_logprob": -0.0952298320941071, "compression_ratio": 1.542857142857143, "no_speech_prob": 1.9033728904105374e-06}, {"id": 453, "seek": 279568, "start": 2806.6, "end": 2812.3999999999996, "text": " taken a hundred cities in China and they've plotted the temperature on one axis in Celsius", "tokens": [2726, 257, 3262, 6486, 294, 3533, 293, 436, 600, 43288, 264, 4292, 322, 472, 10298, 294, 22658], "temperature": 0.0, "avg_logprob": -0.0952298320941071, "compression_ratio": 1.542857142857143, "no_speech_prob": 1.9033728904105374e-06}, {"id": 454, "seek": 279568, "start": 2812.3999999999996, "end": 2818.48, "text": " and R on the other axis where R is a measure of transmissibility. It says for each person", "tokens": [293, 497, 322, 264, 661, 10298, 689, 497, 307, 257, 3481, 295, 7715, 891, 2841, 13, 467, 1619, 337, 1184, 954], "temperature": 0.0, "avg_logprob": -0.0952298320941071, "compression_ratio": 1.542857142857143, "no_speech_prob": 1.9033728904105374e-06}, {"id": 455, "seek": 281848, "start": 2818.48, "end": 2827.0, "text": " that has this disease how many people on average will they infect. So if R is under 1 then", "tokens": [300, 575, 341, 4752, 577, 867, 561, 322, 4274, 486, 436, 5888, 13, 407, 498, 497, 307, 833, 502, 550], "temperature": 0.0, "avg_logprob": -0.10648602878346163, "compression_ratio": 1.6071428571428572, "no_speech_prob": 3.1561236824018124e-07}, {"id": 456, "seek": 281848, "start": 2827.0, "end": 2833.76, "text": " the disease will not spread. If R is higher than like 2 it's going to spread incredibly", "tokens": [264, 4752, 486, 406, 3974, 13, 759, 497, 307, 2946, 813, 411, 568, 309, 311, 516, 281, 3974, 6252], "temperature": 0.0, "avg_logprob": -0.10648602878346163, "compression_ratio": 1.6071428571428572, "no_speech_prob": 3.1561236824018124e-07}, {"id": 457, "seek": 281848, "start": 2833.76, "end": 2839.56, "text": " quickly and basically R is going to you know any high R is going to create an exponential", "tokens": [2661, 293, 1936, 497, 307, 516, 281, 291, 458, 604, 1090, 497, 307, 516, 281, 1884, 364, 21510], "temperature": 0.0, "avg_logprob": -0.10648602878346163, "compression_ratio": 1.6071428571428572, "no_speech_prob": 3.1561236824018124e-07}, {"id": 458, "seek": 281848, "start": 2839.56, "end": 2846.84, "text": " transmission impact. And you can see in this case they have plotted a best fit line through", "tokens": [11574, 2712, 13, 400, 291, 393, 536, 294, 341, 1389, 436, 362, 43288, 257, 1151, 3318, 1622, 807], "temperature": 0.0, "avg_logprob": -0.10648602878346163, "compression_ratio": 1.6071428571428572, "no_speech_prob": 3.1561236824018124e-07}, {"id": 459, "seek": 284684, "start": 2846.84, "end": 2853.8, "text": " here and then they've made a claim that there's some particular relationship in terms of a", "tokens": [510, 293, 550, 436, 600, 1027, 257, 3932, 300, 456, 311, 512, 1729, 2480, 294, 2115, 295, 257], "temperature": 0.0, "avg_logprob": -0.10226789873037766, "compression_ratio": 1.470899470899471, "no_speech_prob": 2.8453344569356886e-08}, {"id": 460, "seek": 284684, "start": 2853.8, "end": 2864.0, "text": " formula that R is 1.99 minus 0.023 times temperature. So a very obvious concern I would have looking", "tokens": [8513, 300, 497, 307, 502, 13, 8494, 3175, 1958, 13, 15, 9356, 1413, 4292, 13, 407, 257, 588, 6322, 3136, 286, 576, 362, 1237], "temperature": 0.0, "avg_logprob": -0.10226789873037766, "compression_ratio": 1.470899470899471, "no_speech_prob": 2.8453344569356886e-08}, {"id": 461, "seek": 284684, "start": 2864.0, "end": 2871.76, "text": " at this picture is that this might just be random maybe there's no relationship at all", "tokens": [412, 341, 3036, 307, 300, 341, 1062, 445, 312, 4974, 1310, 456, 311, 572, 2480, 412, 439], "temperature": 0.0, "avg_logprob": -0.10226789873037766, "compression_ratio": 1.470899470899471, "no_speech_prob": 2.8453344569356886e-08}, {"id": 462, "seek": 287176, "start": 2871.76, "end": 2877.6800000000003, "text": " but just if you picked a hundred cities at random perhaps they would sometimes show this", "tokens": [457, 445, 498, 291, 6183, 257, 3262, 6486, 412, 4974, 4317, 436, 576, 2171, 855, 341], "temperature": 0.0, "avg_logprob": -0.09511416940128102, "compression_ratio": 1.6081081081081081, "no_speech_prob": 1.197930146190629e-07}, {"id": 463, "seek": 287176, "start": 2877.6800000000003, "end": 2886.0400000000004, "text": " level of relationship. So one simple way to kind of see that would be to actually do it", "tokens": [1496, 295, 2480, 13, 407, 472, 2199, 636, 281, 733, 295, 536, 300, 576, 312, 281, 767, 360, 309], "temperature": 0.0, "avg_logprob": -0.09511416940128102, "compression_ratio": 1.6081081081081081, "no_speech_prob": 1.197930146190629e-07}, {"id": 464, "seek": 287176, "start": 2886.0400000000004, "end": 2893.76, "text": " in a spreadsheet. So here's here is a spreadsheet where what I did was I kind of eyeballed this", "tokens": [294, 257, 27733, 13, 407, 510, 311, 510, 307, 257, 27733, 689, 437, 286, 630, 390, 286, 733, 295, 38868, 292, 341], "temperature": 0.0, "avg_logprob": -0.09511416940128102, "compression_ratio": 1.6081081081081081, "no_speech_prob": 1.197930146190629e-07}, {"id": 465, "seek": 287176, "start": 2893.76, "end": 2899.96, "text": " data and I guessed about what is the mean degree centigrade I think it's about 5 and", "tokens": [1412, 293, 286, 21852, 466, 437, 307, 264, 914, 4314, 44731, 286, 519, 309, 311, 466, 1025, 293], "temperature": 0.0, "avg_logprob": -0.09511416940128102, "compression_ratio": 1.6081081081081081, "no_speech_prob": 1.197930146190629e-07}, {"id": 466, "seek": 289996, "start": 2899.96, "end": 2905.7200000000003, "text": " that's about the standard deviation of centigrade I think it's probably about 5 as well. And", "tokens": [300, 311, 466, 264, 3832, 25163, 295, 44731, 286, 519, 309, 311, 1391, 466, 1025, 382, 731, 13, 400], "temperature": 0.0, "avg_logprob": -0.09121017665653439, "compression_ratio": 1.7241379310344827, "no_speech_prob": 3.632673895026528e-07}, {"id": 467, "seek": 289996, "start": 2905.7200000000003, "end": 2911.32, "text": " then I did the same thing for R I think the mean R looks like it's about 1.9 to me and", "tokens": [550, 286, 630, 264, 912, 551, 337, 497, 286, 519, 264, 914, 497, 1542, 411, 309, 311, 466, 502, 13, 24, 281, 385, 293], "temperature": 0.0, "avg_logprob": -0.09121017665653439, "compression_ratio": 1.7241379310344827, "no_speech_prob": 3.632673895026528e-07}, {"id": 468, "seek": 289996, "start": 2911.32, "end": 2917.28, "text": " it looks like the standard deviation of R is probably about 0.5. So what I then did", "tokens": [309, 1542, 411, 264, 3832, 25163, 295, 497, 307, 1391, 466, 1958, 13, 20, 13, 407, 437, 286, 550, 630], "temperature": 0.0, "avg_logprob": -0.09121017665653439, "compression_ratio": 1.7241379310344827, "no_speech_prob": 3.632673895026528e-07}, {"id": 469, "seek": 289996, "start": 2917.28, "end": 2925.68, "text": " was I just jumped over here and I created a random normal value so a random value from", "tokens": [390, 286, 445, 13864, 670, 510, 293, 286, 2942, 257, 4974, 2710, 2158, 370, 257, 4974, 2158, 490], "temperature": 0.0, "avg_logprob": -0.09121017665653439, "compression_ratio": 1.7241379310344827, "no_speech_prob": 3.632673895026528e-07}, {"id": 470, "seek": 292568, "start": 2925.68, "end": 2930.9199999999996, "text": " a normal distribution from a normal distribution so a bell curve with that particular mean", "tokens": [257, 2710, 7316, 490, 257, 2710, 7316, 370, 257, 4549, 7605, 365, 300, 1729, 914], "temperature": 0.0, "avg_logprob": -0.0948266327381134, "compression_ratio": 1.9095744680851063, "no_speech_prob": 1.4367477660925942e-06}, {"id": 471, "seek": 292568, "start": 2930.9199999999996, "end": 2937.24, "text": " and standard deviation of temperature and that particular mean and standard deviation", "tokens": [293, 3832, 25163, 295, 4292, 293, 300, 1729, 914, 293, 3832, 25163], "temperature": 0.0, "avg_logprob": -0.0948266327381134, "compression_ratio": 1.9095744680851063, "no_speech_prob": 1.4367477660925942e-06}, {"id": 472, "seek": 292568, "start": 2937.24, "end": 2943.96, "text": " of R. And so this would be an example of a city that might be in this data set of a hundred", "tokens": [295, 497, 13, 400, 370, 341, 576, 312, 364, 1365, 295, 257, 2307, 300, 1062, 312, 294, 341, 1412, 992, 295, 257, 3262], "temperature": 0.0, "avg_logprob": -0.0948266327381134, "compression_ratio": 1.9095744680851063, "no_speech_prob": 1.4367477660925942e-06}, {"id": 473, "seek": 292568, "start": 2943.96, "end": 2950.16, "text": " cities something with 9 degrees Celsius and an R of 1.1 so that would be 9 degrees Celsius", "tokens": [6486, 746, 365, 1722, 5310, 22658, 293, 364, 497, 295, 502, 13, 16, 370, 300, 576, 312, 1722, 5310, 22658], "temperature": 0.0, "avg_logprob": -0.0948266327381134, "compression_ratio": 1.9095744680851063, "no_speech_prob": 1.4367477660925942e-06}, {"id": 474, "seek": 295016, "start": 2950.16, "end": 2960.8799999999997, "text": " and an R of 1.1 so something about here. And so then I just copied that formula down 100", "tokens": [293, 364, 497, 295, 502, 13, 16, 370, 746, 466, 510, 13, 400, 370, 550, 286, 445, 25365, 300, 8513, 760, 2319], "temperature": 0.0, "avg_logprob": -0.1359020526592548, "compression_ratio": 1.4808743169398908, "no_speech_prob": 2.1691710117011098e-07}, {"id": 475, "seek": 295016, "start": 2960.8799999999997, "end": 2970.44, "text": " times. So here are a hundred cities that could be in China right where this is assuming that", "tokens": [1413, 13, 407, 510, 366, 257, 3262, 6486, 300, 727, 312, 294, 3533, 558, 689, 341, 307, 11926, 300], "temperature": 0.0, "avg_logprob": -0.1359020526592548, "compression_ratio": 1.4808743169398908, "no_speech_prob": 2.1691710117011098e-07}, {"id": 476, "seek": 295016, "start": 2970.44, "end": 2978.0, "text": " there is no relationship between temperature and R right they're just random numbers. And", "tokens": [456, 307, 572, 2480, 1296, 4292, 293, 497, 558, 436, 434, 445, 4974, 3547, 13, 400], "temperature": 0.0, "avg_logprob": -0.1359020526592548, "compression_ratio": 1.4808743169398908, "no_speech_prob": 2.1691710117011098e-07}, {"id": 477, "seek": 297800, "start": 2978.0, "end": 2985.16, "text": " so each time I recalculate that so if I hit ctrl equals it will just recalculate it right", "tokens": [370, 1184, 565, 286, 850, 304, 2444, 473, 300, 370, 498, 286, 2045, 269, 28269, 6915, 309, 486, 445, 850, 304, 2444, 473, 309, 558], "temperature": 0.0, "avg_logprob": -0.12849350178495367, "compression_ratio": 1.989071038251366, "no_speech_prob": 3.8070129448897205e-07}, {"id": 478, "seek": 297800, "start": 2985.16, "end": 2991.2, "text": " I get different numbers okay because they're random and so you can see at the top here", "tokens": [286, 483, 819, 3547, 1392, 570, 436, 434, 4974, 293, 370, 291, 393, 536, 412, 264, 1192, 510], "temperature": 0.0, "avg_logprob": -0.12849350178495367, "compression_ratio": 1.989071038251366, "no_speech_prob": 3.8070129448897205e-07}, {"id": 479, "seek": 297800, "start": 2991.2, "end": 2998.56, "text": " I've then got the average of all of the temperatures and the average of all of the Rs and the average", "tokens": [286, 600, 550, 658, 264, 4274, 295, 439, 295, 264, 12633, 293, 264, 4274, 295, 439, 295, 264, 21643, 293, 264, 4274], "temperature": 0.0, "avg_logprob": -0.12849350178495367, "compression_ratio": 1.989071038251366, "no_speech_prob": 3.8070129448897205e-07}, {"id": 480, "seek": 297800, "start": 2998.56, "end": 3006.64, "text": " of all of the temperatures varies and the average of all of the Rs varies as well. So", "tokens": [295, 439, 295, 264, 12633, 21716, 293, 264, 4274, 295, 439, 295, 264, 21643, 21716, 382, 731, 13, 407], "temperature": 0.0, "avg_logprob": -0.12849350178495367, "compression_ratio": 1.989071038251366, "no_speech_prob": 3.8070129448897205e-07}, {"id": 481, "seek": 300664, "start": 3006.64, "end": 3018.52, "text": " then I what I did was I copied those random numbers over here. Let's actually do it so", "tokens": [550, 286, 437, 286, 630, 390, 286, 25365, 729, 4974, 3547, 670, 510, 13, 961, 311, 767, 360, 309, 370], "temperature": 0.0, "avg_logprob": -0.1501467227935791, "compression_ratio": 1.4741379310344827, "no_speech_prob": 6.083583912186441e-07}, {"id": 482, "seek": 300664, "start": 3018.52, "end": 3032.72, "text": " I'll go copy these 100 random numbers and paste them here here here here here and so", "tokens": [286, 603, 352, 5055, 613, 2319, 4974, 3547, 293, 9163, 552, 510, 510, 510, 510, 510, 293, 370], "temperature": 0.0, "avg_logprob": -0.1501467227935791, "compression_ratio": 1.4741379310344827, "no_speech_prob": 6.083583912186441e-07}, {"id": 483, "seek": 303272, "start": 3032.72, "end": 3045.72, "text": " now I've got 1 2 3 4 5 6 I've got 6 kind of groups of a hundred cities right and so let's", "tokens": [586, 286, 600, 658, 502, 568, 805, 1017, 1025, 1386, 286, 600, 658, 1386, 733, 295, 3935, 295, 257, 3262, 6486, 558, 293, 370, 718, 311], "temperature": 0.0, "avg_logprob": -0.1463885804017385, "compression_ratio": 1.3059701492537314, "no_speech_prob": 3.689881964419328e-07}, {"id": 484, "seek": 303272, "start": 3045.72, "end": 3058.48, "text": " stop those from randomly changing anymore by just fixing them in stone there. Okay so", "tokens": [1590, 729, 490, 16979, 4473, 3602, 538, 445, 19442, 552, 294, 7581, 456, 13, 1033, 370], "temperature": 0.0, "avg_logprob": -0.1463885804017385, "compression_ratio": 1.3059701492537314, "no_speech_prob": 3.689881964419328e-07}, {"id": 485, "seek": 305848, "start": 3058.48, "end": 3065.2, "text": " now that I've pasted them in I've got 6 examples of what a hundred cities might look like if", "tokens": [586, 300, 286, 600, 1791, 292, 552, 294, 286, 600, 658, 1386, 5110, 295, 437, 257, 3262, 6486, 1062, 574, 411, 498], "temperature": 0.0, "avg_logprob": -0.12061104077971384, "compression_ratio": 1.7378640776699028, "no_speech_prob": 1.1979298619735346e-07}, {"id": 486, "seek": 305848, "start": 3065.2, "end": 3071.2, "text": " there was no relationship at all between temperature and R and I've got their mean temperature", "tokens": [456, 390, 572, 2480, 412, 439, 1296, 4292, 293, 497, 293, 286, 600, 658, 641, 914, 4292], "temperature": 0.0, "avg_logprob": -0.12061104077971384, "compression_ratio": 1.7378640776699028, "no_speech_prob": 1.1979298619735346e-07}, {"id": 487, "seek": 305848, "start": 3071.2, "end": 3077.16, "text": " and R in each of those 6 examples and what I've done is you can see here at least for", "tokens": [293, 497, 294, 1184, 295, 729, 1386, 5110, 293, 437, 286, 600, 1096, 307, 291, 393, 536, 510, 412, 1935, 337], "temperature": 0.0, "avg_logprob": -0.12061104077971384, "compression_ratio": 1.7378640776699028, "no_speech_prob": 1.1979298619735346e-07}, {"id": 488, "seek": 305848, "start": 3077.16, "end": 3082.44, "text": " the first one is I've plotted it right and you can see in this case there's actually", "tokens": [264, 700, 472, 307, 286, 600, 43288, 309, 558, 293, 291, 393, 536, 294, 341, 1389, 456, 311, 767], "temperature": 0.0, "avg_logprob": -0.12061104077971384, "compression_ratio": 1.7378640776699028, "no_speech_prob": 1.1979298619735346e-07}, {"id": 489, "seek": 308244, "start": 3082.44, "end": 3094.32, "text": " a slight positive slope and I've actually calculated the slope for each just by using", "tokens": [257, 4036, 3353, 13525, 293, 286, 600, 767, 15598, 264, 13525, 337, 1184, 445, 538, 1228], "temperature": 0.0, "avg_logprob": -0.09137916564941406, "compression_ratio": 1.6024096385542168, "no_speech_prob": 2.1024321483764652e-07}, {"id": 490, "seek": 308244, "start": 3094.32, "end": 3100.56, "text": " the slope function in Microsoft Excel and you can see that actually in this particular", "tokens": [264, 13525, 2445, 294, 8116, 19060, 293, 291, 393, 536, 300, 767, 294, 341, 1729], "temperature": 0.0, "avg_logprob": -0.09137916564941406, "compression_ratio": 1.6024096385542168, "no_speech_prob": 2.1024321483764652e-07}, {"id": 491, "seek": 308244, "start": 3100.56, "end": 3108.6, "text": " case it's just random 5 times it's been negative and it's even more negative than their point", "tokens": [1389, 309, 311, 445, 4974, 1025, 1413, 309, 311, 668, 3671, 293, 309, 311, 754, 544, 3671, 813, 641, 935], "temperature": 0.0, "avg_logprob": -0.09137916564941406, "compression_ratio": 1.6024096385542168, "no_speech_prob": 2.1024321483764652e-07}, {"id": 492, "seek": 310860, "start": 3108.6, "end": 3116.4, "text": " 0 2 3 and so you can like it's kind of matching our intuition here which is that this the", "tokens": [1958, 568, 805, 293, 370, 291, 393, 411, 309, 311, 733, 295, 14324, 527, 24002, 510, 597, 307, 300, 341, 264], "temperature": 0.0, "avg_logprob": -0.08821141582795944, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.08358448062063e-07}, {"id": 493, "seek": 310860, "start": 3116.4, "end": 3122.6, "text": " slope of the line that we have here is something that absolutely can often happen totally by", "tokens": [13525, 295, 264, 1622, 300, 321, 362, 510, 307, 746, 300, 3122, 393, 2049, 1051, 3879, 538], "temperature": 0.0, "avg_logprob": -0.08821141582795944, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.08358448062063e-07}, {"id": 494, "seek": 310860, "start": 3122.6, "end": 3129.36, "text": " chance it doesn't seem to be indicating any kind of real relationship at all. If we wanted", "tokens": [2931, 309, 1177, 380, 1643, 281, 312, 25604, 604, 733, 295, 957, 2480, 412, 439, 13, 759, 321, 1415], "temperature": 0.0, "avg_logprob": -0.08821141582795944, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.08358448062063e-07}, {"id": 495, "seek": 310860, "start": 3129.36, "end": 3138.48, "text": " that slope to be like more confident we would need to look at more cities so like here I've", "tokens": [300, 13525, 281, 312, 411, 544, 6679, 321, 576, 643, 281, 574, 412, 544, 6486, 370, 411, 510, 286, 600], "temperature": 0.0, "avg_logprob": -0.08821141582795944, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.08358448062063e-07}, {"id": 496, "seek": 313848, "start": 3138.48, "end": 3150.0, "text": " got 3000 randomly generated numbers and you can see here the slope is point 0 0 0 2 right", "tokens": [658, 20984, 16979, 10833, 3547, 293, 291, 393, 536, 510, 264, 13525, 307, 935, 1958, 1958, 1958, 568, 558], "temperature": 0.0, "avg_logprob": -0.13831998026648232, "compression_ratio": 1.6682242990654206, "no_speech_prob": 8.990946298581548e-07}, {"id": 497, "seek": 313848, "start": 3150.0, "end": 3154.8, "text": " it's almost exactly 0 which is what we'd expect right when there's actually no relationship", "tokens": [309, 311, 1920, 2293, 1958, 597, 307, 437, 321, 1116, 2066, 558, 562, 456, 311, 767, 572, 2480], "temperature": 0.0, "avg_logprob": -0.13831998026648232, "compression_ratio": 1.6682242990654206, "no_speech_prob": 8.990946298581548e-07}, {"id": 498, "seek": 313848, "start": 3154.8, "end": 3160.08, "text": " between C and R and in this case there isn't they're all random then if we look at lots", "tokens": [1296, 383, 293, 497, 293, 294, 341, 1389, 456, 1943, 380, 436, 434, 439, 4974, 550, 498, 321, 574, 412, 3195], "temperature": 0.0, "avg_logprob": -0.13831998026648232, "compression_ratio": 1.6682242990654206, "no_speech_prob": 8.990946298581548e-07}, {"id": 499, "seek": 313848, "start": 3160.08, "end": 3165.16, "text": " and lots of randomly generated cities then we can say oh yeah this there's no slope but", "tokens": [293, 3195, 295, 16979, 10833, 6486, 550, 321, 393, 584, 1954, 1338, 341, 456, 311, 572, 13525, 457], "temperature": 0.0, "avg_logprob": -0.13831998026648232, "compression_ratio": 1.6682242990654206, "no_speech_prob": 8.990946298581548e-07}, {"id": 500, "seek": 316516, "start": 3165.16, "end": 3171.5, "text": " when you only look at a hundred as we did here you're going to see relationships totally", "tokens": [562, 291, 787, 574, 412, 257, 3262, 382, 321, 630, 510, 291, 434, 516, 281, 536, 6159, 3879], "temperature": 0.0, "avg_logprob": -0.11216252500360663, "compression_ratio": 1.8717948717948718, "no_speech_prob": 1.5869986214056553e-07}, {"id": 501, "seek": 316516, "start": 3171.5, "end": 3179.04, "text": " coincidentally very very often right so that's something that we need to be able to measure", "tokens": [13001, 36578, 588, 588, 2049, 558, 370, 300, 311, 746, 300, 321, 643, 281, 312, 1075, 281, 3481], "temperature": 0.0, "avg_logprob": -0.11216252500360663, "compression_ratio": 1.8717948717948718, "no_speech_prob": 1.5869986214056553e-07}, {"id": 502, "seek": 316516, "start": 3179.04, "end": 3184.56, "text": " and so one way to measure that is we use something called a p-value so a p-value here's how a", "tokens": [293, 370, 472, 636, 281, 3481, 300, 307, 321, 764, 746, 1219, 257, 280, 12, 29155, 370, 257, 280, 12, 29155, 510, 311, 577, 257], "temperature": 0.0, "avg_logprob": -0.11216252500360663, "compression_ratio": 1.8717948717948718, "no_speech_prob": 1.5869986214056553e-07}, {"id": 503, "seek": 316516, "start": 3184.56, "end": 3190.04, "text": " p-value works we start out with something called a null hypothesis and the null hypothesis", "tokens": [280, 12, 29155, 1985, 321, 722, 484, 365, 746, 1219, 257, 18184, 17291, 293, 264, 18184, 17291], "temperature": 0.0, "avg_logprob": -0.11216252500360663, "compression_ratio": 1.8717948717948718, "no_speech_prob": 1.5869986214056553e-07}, {"id": 504, "seek": 319004, "start": 3190.04, "end": 3196.24, "text": " is basically what's what's our starting point assumption so our starting point assumption", "tokens": [307, 1936, 437, 311, 437, 311, 527, 2891, 935, 15302, 370, 527, 2891, 935, 15302], "temperature": 0.0, "avg_logprob": -0.09492048304131691, "compression_ratio": 2.036363636363636, "no_speech_prob": 2.0377470377752616e-07}, {"id": 505, "seek": 319004, "start": 3196.24, "end": 3201.32, "text": " might be oh there's no relationship between temperature and R and then we gather some", "tokens": [1062, 312, 1954, 456, 311, 572, 2480, 1296, 4292, 293, 497, 293, 550, 321, 5448, 512], "temperature": 0.0, "avg_logprob": -0.09492048304131691, "compression_ratio": 2.036363636363636, "no_speech_prob": 2.0377470377752616e-07}, {"id": 506, "seek": 319004, "start": 3201.32, "end": 3208.68, "text": " data and have you explained what R is I have yes R is the transmissibility of the virus", "tokens": [1412, 293, 362, 291, 8825, 437, 497, 307, 286, 362, 2086, 497, 307, 264, 7715, 891, 2841, 295, 264, 5752], "temperature": 0.0, "avg_logprob": -0.09492048304131691, "compression_ratio": 2.036363636363636, "no_speech_prob": 2.0377470377752616e-07}, {"id": 507, "seek": 319004, "start": 3208.68, "end": 3213.48, "text": " so then we gather data of independent independent variables so in this case the independent", "tokens": [370, 550, 321, 5448, 1412, 295, 6695, 6695, 9102, 370, 294, 341, 1389, 264, 6695], "temperature": 0.0, "avg_logprob": -0.09492048304131691, "compression_ratio": 2.036363636363636, "no_speech_prob": 2.0377470377752616e-07}, {"id": 508, "seek": 319004, "start": 3213.48, "end": 3218.96, "text": " variable is the thing that we think might cause a dependent variable so here the independent", "tokens": [7006, 307, 264, 551, 300, 321, 519, 1062, 3082, 257, 12334, 7006, 370, 510, 264, 6695], "temperature": 0.0, "avg_logprob": -0.09492048304131691, "compression_ratio": 2.036363636363636, "no_speech_prob": 2.0377470377752616e-07}, {"id": 509, "seek": 321896, "start": 3218.96, "end": 3223.12, "text": " variable would be temperature the dependent variable would be R so here we've gathered", "tokens": [7006, 576, 312, 4292, 264, 12334, 7006, 576, 312, 497, 370, 510, 321, 600, 13032], "temperature": 0.0, "avg_logprob": -0.07274356713661781, "compression_ratio": 1.8185483870967742, "no_speech_prob": 1.7159553067358502e-07}, {"id": 510, "seek": 321896, "start": 3223.12, "end": 3228.94, "text": " data there's the data that was gathered in this example and then we say what percentage", "tokens": [1412, 456, 311, 264, 1412, 300, 390, 13032, 294, 341, 1365, 293, 550, 321, 584, 437, 9668], "temperature": 0.0, "avg_logprob": -0.07274356713661781, "compression_ratio": 1.8185483870967742, "no_speech_prob": 1.7159553067358502e-07}, {"id": 511, "seek": 321896, "start": 3228.94, "end": 3236.76, "text": " of the time would we see this amount of relationship which is a slope of 0.023 by chance and as", "tokens": [295, 264, 565, 576, 321, 536, 341, 2372, 295, 2480, 597, 307, 257, 13525, 295, 1958, 13, 15, 9356, 538, 2931, 293, 382], "temperature": 0.0, "avg_logprob": -0.07274356713661781, "compression_ratio": 1.8185483870967742, "no_speech_prob": 1.7159553067358502e-07}, {"id": 512, "seek": 321896, "start": 3236.76, "end": 3242.08, "text": " we've seen one way to do that is by what we would call a simulation which is by generating", "tokens": [321, 600, 1612, 472, 636, 281, 360, 300, 307, 538, 437, 321, 576, 818, 257, 16575, 597, 307, 538, 17746], "temperature": 0.0, "avg_logprob": -0.07274356713661781, "compression_ratio": 1.8185483870967742, "no_speech_prob": 1.7159553067358502e-07}, {"id": 513, "seek": 321896, "start": 3242.08, "end": 3247.84, "text": " random number a hundred set pairs of random numbers a bunch of times and seeing how often", "tokens": [4974, 1230, 257, 3262, 992, 15494, 295, 4974, 3547, 257, 3840, 295, 1413, 293, 2577, 577, 2049], "temperature": 0.0, "avg_logprob": -0.07274356713661781, "compression_ratio": 1.8185483870967742, "no_speech_prob": 1.7159553067358502e-07}, {"id": 514, "seek": 324784, "start": 3247.84, "end": 3253.6000000000004, "text": " you see this this relationship we don't actually have to do it that though there's actually", "tokens": [291, 536, 341, 341, 2480, 321, 500, 380, 767, 362, 281, 360, 309, 300, 1673, 456, 311, 767], "temperature": 0.0, "avg_logprob": -0.06749183316774006, "compression_ratio": 1.81, "no_speech_prob": 9.04245140986859e-08}, {"id": 515, "seek": 324784, "start": 3253.6000000000004, "end": 3260.56, "text": " a simple equation we can use to jump straight to this number which is what percent of the", "tokens": [257, 2199, 5367, 321, 393, 764, 281, 3012, 2997, 281, 341, 1230, 597, 307, 437, 3043, 295, 264], "temperature": 0.0, "avg_logprob": -0.06749183316774006, "compression_ratio": 1.81, "no_speech_prob": 9.04245140986859e-08}, {"id": 516, "seek": 324784, "start": 3260.56, "end": 3269.88, "text": " time would we see that relationship by chance and this is basically what that looks like", "tokens": [565, 576, 321, 536, 300, 2480, 538, 2931, 293, 341, 307, 1936, 437, 300, 1542, 411], "temperature": 0.0, "avg_logprob": -0.06749183316774006, "compression_ratio": 1.81, "no_speech_prob": 9.04245140986859e-08}, {"id": 517, "seek": 324784, "start": 3269.88, "end": 3276.6800000000003, "text": " we have the most likely observation which in this case would be if there is no relationship", "tokens": [321, 362, 264, 881, 3700, 14816, 597, 294, 341, 1389, 576, 312, 498, 456, 307, 572, 2480], "temperature": 0.0, "avg_logprob": -0.06749183316774006, "compression_ratio": 1.81, "no_speech_prob": 9.04245140986859e-08}, {"id": 518, "seek": 327668, "start": 3276.68, "end": 3283.98, "text": " between temperature and R then the most likely slope would be zero and sometimes you get", "tokens": [1296, 4292, 293, 497, 550, 264, 881, 3700, 13525, 576, 312, 4018, 293, 2171, 291, 483], "temperature": 0.0, "avg_logprob": -0.07432909647623698, "compression_ratio": 2.0654761904761907, "no_speech_prob": 2.0061553129835374e-07}, {"id": 519, "seek": 327668, "start": 3283.98, "end": 3290.08, "text": " positive slopes by chance and sometimes you get pretty small slopes and sometimes you", "tokens": [3353, 37725, 538, 2931, 293, 2171, 291, 483, 1238, 1359, 37725, 293, 2171, 291], "temperature": 0.0, "avg_logprob": -0.07432909647623698, "compression_ratio": 2.0654761904761907, "no_speech_prob": 2.0061553129835374e-07}, {"id": 520, "seek": 327668, "start": 3290.08, "end": 3296.8199999999997, "text": " get large negative slopes by chance and so the you know the larger the number the less", "tokens": [483, 2416, 3671, 37725, 538, 2931, 293, 370, 264, 291, 458, 264, 4833, 264, 1230, 264, 1570], "temperature": 0.0, "avg_logprob": -0.07432909647623698, "compression_ratio": 2.0654761904761907, "no_speech_prob": 2.0061553129835374e-07}, {"id": 521, "seek": 327668, "start": 3296.8199999999997, "end": 3301.44, "text": " likely it is to happen whether it be on the positive side or the negative side and so", "tokens": [3700, 309, 307, 281, 1051, 1968, 309, 312, 322, 264, 3353, 1252, 420, 264, 3671, 1252, 293, 370], "temperature": 0.0, "avg_logprob": -0.07432909647623698, "compression_ratio": 2.0654761904761907, "no_speech_prob": 2.0061553129835374e-07}, {"id": 522, "seek": 330144, "start": 3301.44, "end": 3309.44, "text": " in our case our question was how often are we going to get less than negative 0.023 so", "tokens": [294, 527, 1389, 527, 1168, 390, 577, 2049, 366, 321, 516, 281, 483, 1570, 813, 3671, 1958, 13, 15, 9356, 370], "temperature": 0.0, "avg_logprob": -0.12948350345387177, "compression_ratio": 1.6018518518518519, "no_speech_prob": 4.737899814699631e-07}, {"id": 523, "seek": 330144, "start": 3309.44, "end": 3313.4, "text": " it would actually be somewhere down here and I actually copy this from Wikipedia where", "tokens": [309, 576, 767, 312, 4079, 760, 510, 293, 286, 767, 5055, 341, 490, 28999, 689], "temperature": 0.0, "avg_logprob": -0.12948350345387177, "compression_ratio": 1.6018518518518519, "no_speech_prob": 4.737899814699631e-07}, {"id": 524, "seek": 330144, "start": 3313.4, "end": 3317.96, "text": " they were looking for positive numbers and so they've colored in this area above the", "tokens": [436, 645, 1237, 337, 3353, 3547, 293, 370, 436, 600, 14332, 294, 341, 1859, 3673, 264], "temperature": 0.0, "avg_logprob": -0.12948350345387177, "compression_ratio": 1.6018518518518519, "no_speech_prob": 4.737899814699631e-07}, {"id": 525, "seek": 330144, "start": 3317.96, "end": 3322.68, "text": " number so this is the p-value and so you can we don't care about the math but there's a", "tokens": [1230, 370, 341, 307, 264, 280, 12, 29155, 293, 370, 291, 393, 321, 500, 380, 1127, 466, 264, 5221, 457, 456, 311, 257], "temperature": 0.0, "avg_logprob": -0.12948350345387177, "compression_ratio": 1.6018518518518519, "no_speech_prob": 4.737899814699631e-07}, {"id": 526, "seek": 332268, "start": 3322.68, "end": 3332.3199999999997, "text": " simple little equation you can use to directly figure out this number the p-value from the", "tokens": [2199, 707, 5367, 291, 393, 764, 281, 3838, 2573, 484, 341, 1230, 264, 280, 12, 29155, 490, 264], "temperature": 0.0, "avg_logprob": -0.0823841542005539, "compression_ratio": 1.5813953488372092, "no_speech_prob": 1.7159553067358502e-07}, {"id": 527, "seek": 332268, "start": 3332.3199999999997, "end": 3342.08, "text": " data so this is kind of how nearly all kind of medical research results tend to be shown", "tokens": [1412, 370, 341, 307, 733, 295, 577, 6217, 439, 733, 295, 4625, 2132, 3542, 3928, 281, 312, 4898], "temperature": 0.0, "avg_logprob": -0.0823841542005539, "compression_ratio": 1.5813953488372092, "no_speech_prob": 1.7159553067358502e-07}, {"id": 528, "seek": 332268, "start": 3342.08, "end": 3348.9199999999996, "text": " and folks really focus on this idea of p-values and indeed in this particular study as we'll", "tokens": [293, 4024, 534, 1879, 322, 341, 1558, 295, 280, 12, 46033, 293, 6451, 294, 341, 1729, 2979, 382, 321, 603], "temperature": 0.0, "avg_logprob": -0.0823841542005539, "compression_ratio": 1.5813953488372092, "no_speech_prob": 1.7159553067358502e-07}, {"id": 529, "seek": 334892, "start": 3348.92, "end": 3354.7200000000003, "text": " see in a moment they reported p-values so probably a lot of you have seen p-values in", "tokens": [536, 294, 257, 1623, 436, 7055, 280, 12, 46033, 370, 1391, 257, 688, 295, 291, 362, 1612, 280, 12, 46033, 294], "temperature": 0.0, "avg_logprob": -0.052313952218918575, "compression_ratio": 1.6952380952380952, "no_speech_prob": 3.632673895026528e-07}, {"id": 530, "seek": 334892, "start": 3354.7200000000003, "end": 3361.64, "text": " your previous lives they come up in a lot of different domains here's the thing they", "tokens": [428, 3894, 2909, 436, 808, 493, 294, 257, 688, 295, 819, 25514, 510, 311, 264, 551, 436], "temperature": 0.0, "avg_logprob": -0.052313952218918575, "compression_ratio": 1.6952380952380952, "no_speech_prob": 3.632673895026528e-07}, {"id": 531, "seek": 334892, "start": 3361.64, "end": 3368.36, "text": " are terrible you almost always shouldn't be using them don't just trust me trust the American", "tokens": [366, 6237, 291, 1920, 1009, 4659, 380, 312, 1228, 552, 500, 380, 445, 3361, 385, 3361, 264, 2665], "temperature": 0.0, "avg_logprob": -0.052313952218918575, "compression_ratio": 1.6952380952380952, "no_speech_prob": 3.632673895026528e-07}, {"id": 532, "seek": 334892, "start": 3368.36, "end": 3376.36, "text": " Statistical Association they point out six things about p-values and those include p-values", "tokens": [16249, 42686, 10734, 436, 935, 484, 2309, 721, 466, 280, 12, 46033, 293, 729, 4090, 280, 12, 46033], "temperature": 0.0, "avg_logprob": -0.052313952218918575, "compression_ratio": 1.6952380952380952, "no_speech_prob": 3.632673895026528e-07}, {"id": 533, "seek": 337636, "start": 3376.36, "end": 3382.1600000000003, "text": " do not measure the probability that the hypothesis is true or the probability that the data were", "tokens": [360, 406, 3481, 264, 8482, 300, 264, 17291, 307, 2074, 420, 264, 8482, 300, 264, 1412, 645], "temperature": 0.0, "avg_logprob": -0.07074341493494371, "compression_ratio": 1.712962962962963, "no_speech_prob": 1.0677006230253028e-06}, {"id": 534, "seek": 337636, "start": 3382.1600000000003, "end": 3389.2000000000003, "text": " produced by random choice alone now we know this because we just saw that if we use more", "tokens": [7126, 538, 4974, 3922, 3312, 586, 321, 458, 341, 570, 321, 445, 1866, 300, 498, 321, 764, 544], "temperature": 0.0, "avg_logprob": -0.07074341493494371, "compression_ratio": 1.712962962962963, "no_speech_prob": 1.0677006230253028e-06}, {"id": 535, "seek": 337636, "start": 3389.2000000000003, "end": 3398.6400000000003, "text": " data right so if we sample 3,000 random cities rather than a hundred we get a much smaller", "tokens": [1412, 558, 370, 498, 321, 6889, 805, 11, 1360, 4974, 6486, 2831, 813, 257, 3262, 321, 483, 257, 709, 4356], "temperature": 0.0, "avg_logprob": -0.07074341493494371, "compression_ratio": 1.712962962962963, "no_speech_prob": 1.0677006230253028e-06}, {"id": 536, "seek": 337636, "start": 3398.6400000000003, "end": 3404.1200000000003, "text": " value right so p-values don't just tell you about how big a relationship is but they actually", "tokens": [2158, 558, 370, 280, 12, 46033, 500, 380, 445, 980, 291, 466, 577, 955, 257, 2480, 307, 457, 436, 767], "temperature": 0.0, "avg_logprob": -0.07074341493494371, "compression_ratio": 1.712962962962963, "no_speech_prob": 1.0677006230253028e-06}, {"id": 537, "seek": 340412, "start": 3404.12, "end": 3409.64, "text": " tell you about a combination of that and how much data did you collect right so so they", "tokens": [980, 291, 466, 257, 6562, 295, 300, 293, 577, 709, 1412, 630, 291, 2500, 558, 370, 370, 436], "temperature": 0.0, "avg_logprob": -0.0767244150241216, "compression_ratio": 1.7951807228915662, "no_speech_prob": 1.9947237888118252e-06}, {"id": 538, "seek": 340412, "start": 3409.64, "end": 3415.56, "text": " don't measure the probability that the hypothesis is true so therefore conclusions and policy", "tokens": [500, 380, 3481, 264, 8482, 300, 264, 17291, 307, 2074, 370, 4412, 22865, 293, 3897], "temperature": 0.0, "avg_logprob": -0.0767244150241216, "compression_ratio": 1.7951807228915662, "no_speech_prob": 1.9947237888118252e-06}, {"id": 539, "seek": 340412, "start": 3415.56, "end": 3423.3599999999997, "text": " decisions should not be based on whether a p-value passes some threshold p-value does", "tokens": [5327, 820, 406, 312, 2361, 322, 1968, 257, 280, 12, 29155, 11335, 512, 14678, 280, 12, 29155, 775], "temperature": 0.0, "avg_logprob": -0.0767244150241216, "compression_ratio": 1.7951807228915662, "no_speech_prob": 1.9947237888118252e-06}, {"id": 540, "seek": 340412, "start": 3423.3599999999997, "end": 3429.2, "text": " not measure the importance of a result right because again it could just tell you that", "tokens": [406, 3481, 264, 7379, 295, 257, 1874, 558, 570, 797, 309, 727, 445, 980, 291, 300], "temperature": 0.0, "avg_logprob": -0.0767244150241216, "compression_ratio": 1.7951807228915662, "no_speech_prob": 1.9947237888118252e-06}, {"id": 541, "seek": 340412, "start": 3429.2, "end": 3433.88, "text": " you collected lots of data which doesn't tell you that the results actually of any practical", "tokens": [291, 11087, 3195, 295, 1412, 597, 1177, 380, 980, 291, 300, 264, 3542, 767, 295, 604, 8496], "temperature": 0.0, "avg_logprob": -0.0767244150241216, "compression_ratio": 1.7951807228915662, "no_speech_prob": 1.9947237888118252e-06}, {"id": 542, "seek": 343388, "start": 3433.88, "end": 3441.96, "text": " import and so by itself it does not provide a good measure of evidence so Frank Harrell", "tokens": [974, 293, 370, 538, 2564, 309, 775, 406, 2893, 257, 665, 3481, 295, 4467, 370, 6823, 3653, 19771], "temperature": 0.0, "avg_logprob": -0.08079771936675649, "compression_ratio": 1.5810810810810811, "no_speech_prob": 1.118940986089001e-06}, {"id": 543, "seek": 343388, "start": 3441.96, "end": 3448.34, "text": " who is somebody who I read his book and it's a really important part of my learning he's", "tokens": [567, 307, 2618, 567, 286, 1401, 702, 1446, 293, 309, 311, 257, 534, 1021, 644, 295, 452, 2539, 415, 311], "temperature": 0.0, "avg_logprob": -0.08079771936675649, "compression_ratio": 1.5810810810810811, "no_speech_prob": 1.118940986089001e-06}, {"id": 544, "seek": 343388, "start": 3448.34, "end": 3455.36, "text": " a professor of biostatistics has a number of great articles about this he says null", "tokens": [257, 8304, 295, 3228, 39036, 6006, 575, 257, 1230, 295, 869, 11290, 466, 341, 415, 1619, 18184], "temperature": 0.0, "avg_logprob": -0.08079771936675649, "compression_ratio": 1.5810810810810811, "no_speech_prob": 1.118940986089001e-06}, {"id": 545, "seek": 343388, "start": 3455.36, "end": 3461.02, "text": " hypothesis testing and p-values have done significant harm to science and he wrote another", "tokens": [17291, 4997, 293, 280, 12, 46033, 362, 1096, 4776, 6491, 281, 3497, 293, 415, 4114, 1071], "temperature": 0.0, "avg_logprob": -0.08079771936675649, "compression_ratio": 1.5810810810810811, "no_speech_prob": 1.118940986089001e-06}, {"id": 546, "seek": 346102, "start": 3461.02, "end": 3468.7599999999998, "text": " piece called null hypothesis significance testing never worked so I've shown you what", "tokens": [2522, 1219, 18184, 17291, 17687, 4997, 1128, 2732, 370, 286, 600, 4898, 291, 437], "temperature": 0.0, "avg_logprob": -0.10963508750818952, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.829127083714411e-07}, {"id": 547, "seek": 346102, "start": 3468.7599999999998, "end": 3474.36, "text": " p-values are so that you know why they don't work not so that you can use them right but", "tokens": [280, 12, 46033, 366, 370, 300, 291, 458, 983, 436, 500, 380, 589, 406, 370, 300, 291, 393, 764, 552, 558, 457], "temperature": 0.0, "avg_logprob": -0.10963508750818952, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.829127083714411e-07}, {"id": 548, "seek": 346102, "start": 3474.36, "end": 3480.3, "text": " they're a super important part of machine learning because they come up all the time", "tokens": [436, 434, 257, 1687, 1021, 644, 295, 3479, 2539, 570, 436, 808, 493, 439, 264, 565], "temperature": 0.0, "avg_logprob": -0.10963508750818952, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.829127083714411e-07}, {"id": 549, "seek": 346102, "start": 3480.3, "end": 3485.6, "text": " in making this you know when people saying this is how we decide whether your drug worked", "tokens": [294, 1455, 341, 291, 458, 562, 561, 1566, 341, 307, 577, 321, 4536, 1968, 428, 4110, 2732], "temperature": 0.0, "avg_logprob": -0.10963508750818952, "compression_ratio": 1.6462264150943395, "no_speech_prob": 2.829127083714411e-07}, {"id": 550, "seek": 348560, "start": 3485.6, "end": 3493.48, "text": " or whether there is a epidemiological relationship or whatever and indeed p-values appear in", "tokens": [420, 1968, 456, 307, 257, 35761, 4383, 2480, 420, 2035, 293, 6451, 280, 12, 46033, 4204, 294], "temperature": 0.0, "avg_logprob": -0.09590809047222137, "compression_ratio": 1.6325301204819278, "no_speech_prob": 2.4579870228080836e-07}, {"id": 551, "seek": 348560, "start": 3493.48, "end": 3502.88, "text": " this paper so in the paper they show the results of a multiple linear regression and they put", "tokens": [341, 3035, 370, 294, 264, 3035, 436, 855, 264, 3542, 295, 257, 3866, 8213, 24590, 293, 436, 829], "temperature": 0.0, "avg_logprob": -0.09590809047222137, "compression_ratio": 1.6325301204819278, "no_speech_prob": 2.4579870228080836e-07}, {"id": 552, "seek": 348560, "start": 3502.88, "end": 3515.16, "text": " three stars next to any relationship which has a p-value of 0.01 or less so there is", "tokens": [1045, 6105, 958, 281, 604, 2480, 597, 575, 257, 280, 12, 29155, 295, 1958, 13, 10607, 420, 1570, 370, 456, 307], "temperature": 0.0, "avg_logprob": -0.09590809047222137, "compression_ratio": 1.6325301204819278, "no_speech_prob": 2.4579870228080836e-07}, {"id": 553, "seek": 351516, "start": 3515.16, "end": 3521.48, "text": " something useful to say about a small p-value like 0.01 or less which is that the thing", "tokens": [746, 4420, 281, 584, 466, 257, 1359, 280, 12, 29155, 411, 1958, 13, 10607, 420, 1570, 597, 307, 300, 264, 551], "temperature": 0.0, "avg_logprob": -0.06153978150466393, "compression_ratio": 1.6728971962616823, "no_speech_prob": 1.4225759059627308e-07}, {"id": 554, "seek": 351516, "start": 3521.48, "end": 3526.8799999999997, "text": " that we're looking at did not probably did not happen by chance right the biggest statistical", "tokens": [300, 321, 434, 1237, 412, 630, 406, 1391, 630, 406, 1051, 538, 2931, 558, 264, 3880, 22820], "temperature": 0.0, "avg_logprob": -0.06153978150466393, "compression_ratio": 1.6728971962616823, "no_speech_prob": 1.4225759059627308e-07}, {"id": 555, "seek": 351516, "start": 3526.8799999999997, "end": 3534.0, "text": " error people make all the time is that they see that a p-value is not less than 0.05 and", "tokens": [6713, 561, 652, 439, 264, 565, 307, 300, 436, 536, 300, 257, 280, 12, 29155, 307, 406, 1570, 813, 1958, 13, 13328, 293], "temperature": 0.0, "avg_logprob": -0.06153978150466393, "compression_ratio": 1.6728971962616823, "no_speech_prob": 1.4225759059627308e-07}, {"id": 556, "seek": 351516, "start": 3534.0, "end": 3541.8999999999996, "text": " then they make the erroneous conclusion that no relationship exists right which doesn't", "tokens": [550, 436, 652, 264, 1189, 26446, 563, 10063, 300, 572, 2480, 8198, 558, 597, 1177, 380], "temperature": 0.0, "avg_logprob": -0.06153978150466393, "compression_ratio": 1.6728971962616823, "no_speech_prob": 1.4225759059627308e-07}, {"id": 557, "seek": 354190, "start": 3541.9, "end": 3547.0, "text": " make any sense because like let's say you only had like three data points then you almost", "tokens": [652, 604, 2020, 570, 411, 718, 311, 584, 291, 787, 632, 411, 1045, 1412, 2793, 550, 291, 1920], "temperature": 0.0, "avg_logprob": -0.0778419389957335, "compression_ratio": 1.6556603773584906, "no_speech_prob": 1.8553899394646578e-07}, {"id": 558, "seek": 354190, "start": 3547.0, "end": 3554.1600000000003, "text": " certainly won't have enough data to have a p-value of less than 0.05 for any hypothesis", "tokens": [3297, 1582, 380, 362, 1547, 1412, 281, 362, 257, 280, 12, 29155, 295, 1570, 813, 1958, 13, 13328, 337, 604, 17291], "temperature": 0.0, "avg_logprob": -0.0778419389957335, "compression_ratio": 1.6556603773584906, "no_speech_prob": 1.8553899394646578e-07}, {"id": 559, "seek": 354190, "start": 3554.1600000000003, "end": 3558.96, "text": " so like the way to check is to go back and say what if I picked the exact opposite null", "tokens": [370, 411, 264, 636, 281, 1520, 307, 281, 352, 646, 293, 584, 437, 498, 286, 6183, 264, 1900, 6182, 18184], "temperature": 0.0, "avg_logprob": -0.0778419389957335, "compression_ratio": 1.6556603773584906, "no_speech_prob": 1.8553899394646578e-07}, {"id": 560, "seek": 354190, "start": 3558.96, "end": 3563.92, "text": " hypothesis what if my null hypothesis was there is a relationship between temperature", "tokens": [17291, 437, 498, 452, 18184, 17291, 390, 456, 307, 257, 2480, 1296, 4292], "temperature": 0.0, "avg_logprob": -0.0778419389957335, "compression_ratio": 1.6556603773584906, "no_speech_prob": 1.8553899394646578e-07}, {"id": 561, "seek": 356392, "start": 3563.92, "end": 3572.36, "text": " and R then do I have enough data to reject that null hypothesis right and if the answer", "tokens": [293, 497, 550, 360, 286, 362, 1547, 1412, 281, 8248, 300, 18184, 17291, 558, 293, 498, 264, 1867], "temperature": 0.0, "avg_logprob": -0.09830321143655216, "compression_ratio": 1.7524271844660195, "no_speech_prob": 1.3363877826577664e-07}, {"id": 562, "seek": 356392, "start": 3572.36, "end": 3579.48, "text": " is no then you just don't have enough data to make any conclusions at all right so in", "tokens": [307, 572, 550, 291, 445, 500, 380, 362, 1547, 1412, 281, 652, 604, 22865, 412, 439, 558, 370, 294], "temperature": 0.0, "avg_logprob": -0.09830321143655216, "compression_ratio": 1.7524271844660195, "no_speech_prob": 1.3363877826577664e-07}, {"id": 563, "seek": 356392, "start": 3579.48, "end": 3587.4, "text": " this case they do have enough data to be confident that there is a relationship between temperature", "tokens": [341, 1389, 436, 360, 362, 1547, 1412, 281, 312, 6679, 300, 456, 307, 257, 2480, 1296, 4292], "temperature": 0.0, "avg_logprob": -0.09830321143655216, "compression_ratio": 1.7524271844660195, "no_speech_prob": 1.3363877826577664e-07}, {"id": 564, "seek": 356392, "start": 3587.4, "end": 3593.28, "text": " and R now that's weird because we just looked at the graph and we did a little bit of a", "tokens": [293, 497, 586, 300, 311, 3657, 570, 321, 445, 2956, 412, 264, 4295, 293, 321, 630, 257, 707, 857, 295, 257], "temperature": 0.0, "avg_logprob": -0.09830321143655216, "compression_ratio": 1.7524271844660195, "no_speech_prob": 1.3363877826577664e-07}, {"id": 565, "seek": 359328, "start": 3593.28, "end": 3600.5600000000004, "text": " back of the envelope in Excel and we thought this is could could well be random so here's", "tokens": [646, 295, 264, 19989, 294, 19060, 293, 321, 1194, 341, 307, 727, 727, 731, 312, 4974, 370, 510, 311], "temperature": 0.0, "avg_logprob": -0.09146604732591279, "compression_ratio": 1.882608695652174, "no_speech_prob": 5.285512543196091e-07}, {"id": 566, "seek": 359328, "start": 3600.5600000000004, "end": 3606.6800000000003, "text": " where the issue is the graph shows what we call a univariate relationship a univariate", "tokens": [689, 264, 2734, 307, 264, 4295, 3110, 437, 321, 818, 257, 517, 592, 3504, 473, 2480, 257, 517, 592, 3504, 473], "temperature": 0.0, "avg_logprob": -0.09146604732591279, "compression_ratio": 1.882608695652174, "no_speech_prob": 5.285512543196091e-07}, {"id": 567, "seek": 359328, "start": 3606.6800000000003, "end": 3611.32, "text": " relationship shows the relationship between one independent variable and one dependent", "tokens": [2480, 3110, 264, 2480, 1296, 472, 6695, 7006, 293, 472, 12334], "temperature": 0.0, "avg_logprob": -0.09146604732591279, "compression_ratio": 1.882608695652174, "no_speech_prob": 5.285512543196091e-07}, {"id": 568, "seek": 359328, "start": 3611.32, "end": 3616.1600000000003, "text": " variable and that's what you can normally show in a graph but in this case they did", "tokens": [7006, 293, 300, 311, 437, 291, 393, 5646, 855, 294, 257, 4295, 457, 294, 341, 1389, 436, 630], "temperature": 0.0, "avg_logprob": -0.09146604732591279, "compression_ratio": 1.882608695652174, "no_speech_prob": 5.285512543196091e-07}, {"id": 569, "seek": 359328, "start": 3616.1600000000003, "end": 3622.0800000000004, "text": " a multivariate variant model in which they looked at temperature and humidity and GDP", "tokens": [257, 2120, 592, 3504, 473, 17501, 2316, 294, 597, 436, 2956, 412, 4292, 293, 24751, 293, 19599], "temperature": 0.0, "avg_logprob": -0.09146604732591279, "compression_ratio": 1.882608695652174, "no_speech_prob": 5.285512543196091e-07}, {"id": 570, "seek": 362208, "start": 3622.08, "end": 3628.16, "text": " per capita and population density and when you put all of those things into the model", "tokens": [680, 39727, 293, 4415, 10305, 293, 562, 291, 829, 439, 295, 729, 721, 666, 264, 2316], "temperature": 0.0, "avg_logprob": -0.0501270358626907, "compression_ratio": 1.722488038277512, "no_speech_prob": 3.466325040335505e-07}, {"id": 571, "seek": 362208, "start": 3628.16, "end": 3634.4, "text": " then you end up with statistically significant results for temperature and humidity why does", "tokens": [550, 291, 917, 493, 365, 36478, 4776, 3542, 337, 4292, 293, 24751, 983, 775], "temperature": 0.0, "avg_logprob": -0.0501270358626907, "compression_ratio": 1.722488038277512, "no_speech_prob": 3.466325040335505e-07}, {"id": 572, "seek": 362208, "start": 3634.4, "end": 3642.06, "text": " that happen well the reason that happens is because all these variation in the blue dots", "tokens": [300, 1051, 731, 264, 1778, 300, 2314, 307, 570, 439, 613, 12990, 294, 264, 3344, 15026], "temperature": 0.0, "avg_logprob": -0.0501270358626907, "compression_ratio": 1.722488038277512, "no_speech_prob": 3.466325040335505e-07}, {"id": 573, "seek": 362208, "start": 3642.06, "end": 3648.92, "text": " is not random there's a reason they're different right and the reasons include denser cities", "tokens": [307, 406, 4974, 456, 311, 257, 1778, 436, 434, 819, 558, 293, 264, 4112, 4090, 24505, 260, 6486], "temperature": 0.0, "avg_logprob": -0.0501270358626907, "compression_ratio": 1.722488038277512, "no_speech_prob": 3.466325040335505e-07}, {"id": 574, "seek": 364892, "start": 3648.92, "end": 3654.84, "text": " are going to have higher transmission for instance and probably more humid will have", "tokens": [366, 516, 281, 362, 2946, 11574, 337, 5197, 293, 1391, 544, 34649, 486, 362], "temperature": 0.0, "avg_logprob": -0.10187660028905045, "compression_ratio": 1.6986301369863013, "no_speech_prob": 7.856218786628233e-08}, {"id": 575, "seek": 364892, "start": 3654.84, "end": 3664.4, "text": " less transmission so when you do a multivariate model it actually allows you to be more confident", "tokens": [1570, 11574, 370, 562, 291, 360, 257, 2120, 592, 3504, 473, 2316, 309, 767, 4045, 291, 281, 312, 544, 6679], "temperature": 0.0, "avg_logprob": -0.10187660028905045, "compression_ratio": 1.6986301369863013, "no_speech_prob": 7.856218786628233e-08}, {"id": 576, "seek": 364892, "start": 3664.4, "end": 3673.54, "text": " of your results right but the p-value as noted by the American Statistical Association does", "tokens": [295, 428, 3542, 558, 457, 264, 280, 12, 29155, 382, 12964, 538, 264, 2665, 16249, 42686, 10734, 775], "temperature": 0.0, "avg_logprob": -0.10187660028905045, "compression_ratio": 1.6986301369863013, "no_speech_prob": 7.856218786628233e-08}, {"id": 577, "seek": 364892, "start": 3673.54, "end": 3678.88, "text": " not tell us whether this is a practical importance the thing that tells us is this is a practical", "tokens": [406, 980, 505, 1968, 341, 307, 257, 8496, 7379, 264, 551, 300, 5112, 505, 307, 341, 307, 257, 8496], "temperature": 0.0, "avg_logprob": -0.10187660028905045, "compression_ratio": 1.6986301369863013, "no_speech_prob": 7.856218786628233e-08}, {"id": 578, "seek": 367888, "start": 3678.88, "end": 3687.4, "text": " as important as the actual slope that's found and so in this case the equation they come", "tokens": [382, 1021, 382, 264, 3539, 13525, 300, 311, 1352, 293, 370, 294, 341, 1389, 264, 5367, 436, 808], "temperature": 0.0, "avg_logprob": -0.09882099948712249, "compression_ratio": 1.502824858757062, "no_speech_prob": 5.043469855081639e-07}, {"id": 579, "seek": 367888, "start": 3687.4, "end": 3698.6, "text": " up with is that R equals 3.968 minus 3.038 by temperature minus 0.024 by relative humidity", "tokens": [493, 365, 307, 300, 497, 6915, 805, 13, 22962, 23, 3175, 805, 13, 11592, 23, 538, 4292, 3175, 1958, 13, 15, 7911, 538, 4972, 24751], "temperature": 0.0, "avg_logprob": -0.09882099948712249, "compression_ratio": 1.502824858757062, "no_speech_prob": 5.043469855081639e-07}, {"id": 580, "seek": 367888, "start": 3698.6, "end": 3703.88, "text": " this is this equation is this practically important well we can again do a little back", "tokens": [341, 307, 341, 5367, 307, 341, 15667, 1021, 731, 321, 393, 797, 360, 257, 707, 646], "temperature": 0.0, "avg_logprob": -0.09882099948712249, "compression_ratio": 1.502824858757062, "no_speech_prob": 5.043469855081639e-07}, {"id": 581, "seek": 370388, "start": 3703.88, "end": 3713.12, "text": " of the envelope here by just putting that into Excel let's say there was one place that", "tokens": [295, 264, 19989, 510, 538, 445, 3372, 300, 666, 19060, 718, 311, 584, 456, 390, 472, 1081, 300], "temperature": 0.0, "avg_logprob": -0.1128482073545456, "compression_ratio": 1.590643274853801, "no_speech_prob": 1.742977531193901e-07}, {"id": 582, "seek": 370388, "start": 3713.12, "end": 3719.6800000000003, "text": " had a temperature of 10 centigrade and a humidity of 40 then if this equation is correct I would", "tokens": [632, 257, 4292, 295, 1266, 44731, 293, 257, 24751, 295, 3356, 550, 498, 341, 5367, 307, 3006, 286, 576], "temperature": 0.0, "avg_logprob": -0.1128482073545456, "compression_ratio": 1.590643274853801, "no_speech_prob": 1.742977531193901e-07}, {"id": 583, "seek": 370388, "start": 3719.6800000000003, "end": 3726.6, "text": " be about 2.7 somewhere with a temperature of 35 centigrade and a humidity of 80 I would", "tokens": [312, 466, 568, 13, 22, 4079, 365, 257, 4292, 295, 6976, 44731, 293, 257, 24751, 295, 4688, 286, 576], "temperature": 0.0, "avg_logprob": -0.1128482073545456, "compression_ratio": 1.590643274853801, "no_speech_prob": 1.742977531193901e-07}, {"id": 584, "seek": 372660, "start": 3726.6, "end": 3735.4, "text": " be about 0.8 so is this practically important oh my god yes right two different cities with", "tokens": [312, 466, 1958, 13, 23, 370, 307, 341, 15667, 1021, 1954, 452, 3044, 2086, 558, 732, 819, 6486, 365], "temperature": 0.0, "avg_logprob": -0.0817841029748684, "compression_ratio": 1.699530516431925, "no_speech_prob": 1.25541959050679e-07}, {"id": 585, "seek": 372660, "start": 3735.4, "end": 3741.4, "text": " different climates can be if they're the same in every other way and this model is correct", "tokens": [819, 5644, 1024, 393, 312, 498, 436, 434, 264, 912, 294, 633, 661, 636, 293, 341, 2316, 307, 3006], "temperature": 0.0, "avg_logprob": -0.0817841029748684, "compression_ratio": 1.699530516431925, "no_speech_prob": 1.25541959050679e-07}, {"id": 586, "seek": 372660, "start": 3741.4, "end": 3746.7599999999998, "text": " then one city would have no spread of disease because R is less than one one would have", "tokens": [550, 472, 2307, 576, 362, 572, 3974, 295, 4752, 570, 497, 307, 1570, 813, 472, 472, 576, 362], "temperature": 0.0, "avg_logprob": -0.0817841029748684, "compression_ratio": 1.699530516431925, "no_speech_prob": 1.25541959050679e-07}, {"id": 587, "seek": 372660, "start": 3746.7599999999998, "end": 3755.52, "text": " massive exponential explosion so we can see from this model that if the modeling is correct", "tokens": [5994, 21510, 15673, 370, 321, 393, 536, 490, 341, 2316, 300, 498, 264, 15983, 307, 3006], "temperature": 0.0, "avg_logprob": -0.0817841029748684, "compression_ratio": 1.699530516431925, "no_speech_prob": 1.25541959050679e-07}, {"id": 588, "seek": 375552, "start": 3755.52, "end": 3760.2, "text": " then this is a highly practically significant result so this is how you determine practical", "tokens": [550, 341, 307, 257, 5405, 15667, 4776, 1874, 370, 341, 307, 577, 291, 6997, 8496], "temperature": 0.0, "avg_logprob": -0.06433694613607306, "compression_ratio": 1.751219512195122, "no_speech_prob": 9.57081056185416e-07}, {"id": 589, "seek": 375552, "start": 3760.2, "end": 3767.2, "text": " significance of your models it's not with p-values but with looking at kind of actual", "tokens": [17687, 295, 428, 5245, 309, 311, 406, 365, 280, 12, 46033, 457, 365, 1237, 412, 733, 295, 3539], "temperature": 0.0, "avg_logprob": -0.06433694613607306, "compression_ratio": 1.751219512195122, "no_speech_prob": 9.57081056185416e-07}, {"id": 590, "seek": 375552, "start": 3767.2, "end": 3777.32, "text": " outcomes so how do you think about the practical importance of a model and how do you turn", "tokens": [10070, 370, 577, 360, 291, 519, 466, 264, 8496, 7379, 295, 257, 2316, 293, 577, 360, 291, 1261], "temperature": 0.0, "avg_logprob": -0.06433694613607306, "compression_ratio": 1.751219512195122, "no_speech_prob": 9.57081056185416e-07}, {"id": 591, "seek": 375552, "start": 3777.32, "end": 3784.06, "text": " a predictive model into something useful in production so I spent many many years thinking", "tokens": [257, 35521, 2316, 666, 746, 4420, 294, 4265, 370, 286, 4418, 867, 867, 924, 1953], "temperature": 0.0, "avg_logprob": -0.06433694613607306, "compression_ratio": 1.751219512195122, "no_speech_prob": 9.57081056185416e-07}, {"id": 592, "seek": 378406, "start": 3784.06, "end": 3790.7599999999998, "text": " about this and I actually created a with some other great folks actually created a paper", "tokens": [466, 341, 293, 286, 767, 2942, 257, 365, 512, 661, 869, 4024, 767, 2942, 257, 3035], "temperature": 0.0, "avg_logprob": -0.09269016439264471, "compression_ratio": 1.6838709677419355, "no_speech_prob": 1.663160418274856e-07}, {"id": 593, "seek": 378406, "start": 3790.7599999999998, "end": 3803.44, "text": " about it designing great data products and this is largely based on 10 years of work", "tokens": [466, 309, 14685, 869, 1412, 3383, 293, 341, 307, 11611, 2361, 322, 1266, 924, 295, 589], "temperature": 0.0, "avg_logprob": -0.09269016439264471, "compression_ratio": 1.6838709677419355, "no_speech_prob": 1.663160418274856e-07}, {"id": 594, "seek": 378406, "start": 3803.44, "end": 3808.72, "text": " I did at a company I founded called optimal decisions group and optimal decisions group", "tokens": [286, 630, 412, 257, 2237, 286, 13234, 1219, 16252, 5327, 1594, 293, 16252, 5327, 1594], "temperature": 0.0, "avg_logprob": -0.09269016439264471, "compression_ratio": 1.6838709677419355, "no_speech_prob": 1.663160418274856e-07}, {"id": 595, "seek": 380872, "start": 3808.72, "end": 3816.16, "text": " was focused on the question of helping insurance companies figure out what prices to set and", "tokens": [390, 5178, 322, 264, 1168, 295, 4315, 7214, 3431, 2573, 484, 437, 7901, 281, 992, 293], "temperature": 0.0, "avg_logprob": -0.09148797863408138, "compression_ratio": 1.7524752475247525, "no_speech_prob": 4.7378861722791044e-07}, {"id": 596, "seek": 380872, "start": 3816.16, "end": 3821.64, "text": " insurance companies up until that point had focused on predictive modeling actuaries in", "tokens": [7214, 3431, 493, 1826, 300, 935, 632, 5178, 322, 35521, 15983, 34964, 4889, 294], "temperature": 0.0, "avg_logprob": -0.09148797863408138, "compression_ratio": 1.7524752475247525, "no_speech_prob": 4.7378861722791044e-07}, {"id": 597, "seek": 380872, "start": 3821.64, "end": 3827.7599999999998, "text": " particular spent their time trying to figure out how likely is it that you're going to", "tokens": [1729, 4418, 641, 565, 1382, 281, 2573, 484, 577, 3700, 307, 309, 300, 291, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.09148797863408138, "compression_ratio": 1.7524752475247525, "no_speech_prob": 4.7378861722791044e-07}, {"id": 598, "seek": 380872, "start": 3827.7599999999998, "end": 3833.04, "text": " crash your car and if you do how much damage might you have and then based on that try", "tokens": [8252, 428, 1032, 293, 498, 291, 360, 577, 709, 4344, 1062, 291, 362, 293, 550, 2361, 322, 300, 853], "temperature": 0.0, "avg_logprob": -0.09148797863408138, "compression_ratio": 1.7524752475247525, "no_speech_prob": 4.7378861722791044e-07}, {"id": 599, "seek": 383304, "start": 3833.04, "end": 3839.12, "text": " to figure out what price they should set for your policy so for this company what we did", "tokens": [281, 2573, 484, 437, 3218, 436, 820, 992, 337, 428, 3897, 370, 337, 341, 2237, 437, 321, 630], "temperature": 0.0, "avg_logprob": -0.0841818956228403, "compression_ratio": 1.748768472906404, "no_speech_prob": 1.7704233812310122e-07}, {"id": 600, "seek": 383304, "start": 3839.12, "end": 3844.96, "text": " was we decided to use a different approach which I ended up calling the drivetrain approach", "tokens": [390, 321, 3047, 281, 764, 257, 819, 3109, 597, 286, 4590, 493, 5141, 264, 1630, 9771, 7146, 3109], "temperature": 0.0, "avg_logprob": -0.0841818956228403, "compression_ratio": 1.748768472906404, "no_speech_prob": 1.7704233812310122e-07}, {"id": 601, "seek": 383304, "start": 3844.96, "end": 3851.36, "text": " which is described here to to set insurance prices and indeed to do all kinds of other", "tokens": [597, 307, 7619, 510, 281, 281, 992, 7214, 7901, 293, 6451, 281, 360, 439, 3685, 295, 661], "temperature": 0.0, "avg_logprob": -0.0841818956228403, "compression_ratio": 1.748768472906404, "no_speech_prob": 1.7704233812310122e-07}, {"id": 602, "seek": 383304, "start": 3851.36, "end": 3857.36, "text": " things and so for the insurance example the objective would be for an insurance company", "tokens": [721, 293, 370, 337, 264, 7214, 1365, 264, 10024, 576, 312, 337, 364, 7214, 2237], "temperature": 0.0, "avg_logprob": -0.0841818956228403, "compression_ratio": 1.748768472906404, "no_speech_prob": 1.7704233812310122e-07}, {"id": 603, "seek": 385736, "start": 3857.36, "end": 3866.6, "text": " would be how do I maximize my let's say five-year profit and then what inputs can we control", "tokens": [576, 312, 577, 360, 286, 19874, 452, 718, 311, 584, 1732, 12, 5294, 7475, 293, 550, 437, 15743, 393, 321, 1969], "temperature": 0.0, "avg_logprob": -0.07243369346441225, "compression_ratio": 1.7246376811594204, "no_speech_prob": 3.011592752955039e-07}, {"id": 604, "seek": 385736, "start": 3866.6, "end": 3873.1600000000003, "text": " can we control which I call levers so in this case it would be what price can I set and", "tokens": [393, 321, 1969, 597, 286, 818, 45571, 370, 294, 341, 1389, 309, 576, 312, 437, 3218, 393, 286, 992, 293], "temperature": 0.0, "avg_logprob": -0.07243369346441225, "compression_ratio": 1.7246376811594204, "no_speech_prob": 3.011592752955039e-07}, {"id": 605, "seek": 385736, "start": 3873.1600000000003, "end": 3878.92, "text": " then data is data which can tell you as you change your levers how does that change your", "tokens": [550, 1412, 307, 1412, 597, 393, 980, 291, 382, 291, 1319, 428, 45571, 577, 775, 300, 1319, 428], "temperature": 0.0, "avg_logprob": -0.07243369346441225, "compression_ratio": 1.7246376811594204, "no_speech_prob": 3.011592752955039e-07}, {"id": 606, "seek": 385736, "start": 3878.92, "end": 3885.1600000000003, "text": " objective so if I start increasing my price to people who are likely to crash their car", "tokens": [10024, 370, 498, 286, 722, 5662, 452, 3218, 281, 561, 567, 366, 3700, 281, 8252, 641, 1032], "temperature": 0.0, "avg_logprob": -0.07243369346441225, "compression_ratio": 1.7246376811594204, "no_speech_prob": 3.011592752955039e-07}, {"id": 607, "seek": 388516, "start": 3885.16, "end": 3889.48, "text": " then we'll get less of them which means we have less costs but at the same time we'll", "tokens": [550, 321, 603, 483, 1570, 295, 552, 597, 1355, 321, 362, 1570, 5497, 457, 412, 264, 912, 565, 321, 603], "temperature": 0.0, "avg_logprob": -0.10036925113562381, "compression_ratio": 1.71875, "no_speech_prob": 1.5869990477312967e-07}, {"id": 608, "seek": 388516, "start": 3889.48, "end": 3895.72, "text": " also have less revenue coming in for example so to link up the kind of the levers to the", "tokens": [611, 362, 1570, 9324, 1348, 294, 337, 1365, 370, 281, 2113, 493, 264, 733, 295, 264, 45571, 281, 264], "temperature": 0.0, "avg_logprob": -0.10036925113562381, "compression_ratio": 1.71875, "no_speech_prob": 1.5869990477312967e-07}, {"id": 609, "seek": 388516, "start": 3895.72, "end": 3901.6, "text": " objective via the data we collect we build models that described how the levers influence", "tokens": [10024, 5766, 264, 1412, 321, 2500, 321, 1322, 5245, 300, 7619, 577, 264, 45571, 6503], "temperature": 0.0, "avg_logprob": -0.10036925113562381, "compression_ratio": 1.71875, "no_speech_prob": 1.5869990477312967e-07}, {"id": 610, "seek": 388516, "start": 3901.6, "end": 3907.48, "text": " the objective and this is all like it seems pretty obvious when you say it like this but", "tokens": [264, 10024, 293, 341, 307, 439, 411, 309, 2544, 1238, 6322, 562, 291, 584, 309, 411, 341, 457], "temperature": 0.0, "avg_logprob": -0.10036925113562381, "compression_ratio": 1.71875, "no_speech_prob": 1.5869990477312967e-07}, {"id": 611, "seek": 388516, "start": 3907.48, "end": 3913.5, "text": " when we started work with optimal decisions in 1999 nobody was doing this in insurance", "tokens": [562, 321, 1409, 589, 365, 16252, 5327, 294, 19952, 5079, 390, 884, 341, 294, 7214], "temperature": 0.0, "avg_logprob": -0.10036925113562381, "compression_ratio": 1.71875, "no_speech_prob": 1.5869990477312967e-07}, {"id": 612, "seek": 391350, "start": 3913.5, "end": 3919.86, "text": " everybody in insurance was simply doing a predictive model to guess how likely people", "tokens": [2201, 294, 7214, 390, 2935, 884, 257, 35521, 2316, 281, 2041, 577, 3700, 561], "temperature": 0.0, "avg_logprob": -0.06704817622540946, "compression_ratio": 1.5982142857142858, "no_speech_prob": 2.829129925885354e-07}, {"id": 613, "seek": 391350, "start": 3919.86, "end": 3925.96, "text": " were to crash their car and then pricing was set by like adding 20% or whatever it was", "tokens": [645, 281, 8252, 641, 1032, 293, 550, 17621, 390, 992, 538, 411, 5127, 945, 4, 420, 2035, 309, 390], "temperature": 0.0, "avg_logprob": -0.06704817622540946, "compression_ratio": 1.5982142857142858, "no_speech_prob": 2.829129925885354e-07}, {"id": 614, "seek": 391350, "start": 3925.96, "end": 3934.64, "text": " just done in a very kind of naive way so what I did is I you know over many years took this", "tokens": [445, 1096, 294, 257, 588, 733, 295, 29052, 636, 370, 437, 286, 630, 307, 286, 291, 458, 670, 867, 924, 1890, 341], "temperature": 0.0, "avg_logprob": -0.06704817622540946, "compression_ratio": 1.5982142857142858, "no_speech_prob": 2.829129925885354e-07}, {"id": 615, "seek": 391350, "start": 3934.64, "end": 3940.52, "text": " basic process and tried to help lots of companies figure out how to use it to turn predictive", "tokens": [3875, 1399, 293, 3031, 281, 854, 3195, 295, 3431, 2573, 484, 577, 281, 764, 309, 281, 1261, 35521], "temperature": 0.0, "avg_logprob": -0.06704817622540946, "compression_ratio": 1.5982142857142858, "no_speech_prob": 2.829129925885354e-07}, {"id": 616, "seek": 394052, "start": 3940.52, "end": 3947.92, "text": " models into actions so the starting point in like actually getting value in a predictive", "tokens": [5245, 666, 5909, 370, 264, 2891, 935, 294, 411, 767, 1242, 2158, 294, 257, 35521], "temperature": 0.0, "avg_logprob": -0.07872957396275788, "compression_ratio": 1.9823008849557522, "no_speech_prob": 4.381839460165793e-07}, {"id": 617, "seek": 394052, "start": 3947.92, "end": 3952.0, "text": " model is thinking about what is it you're trying to do and you know what are the sources", "tokens": [2316, 307, 1953, 466, 437, 307, 309, 291, 434, 1382, 281, 360, 293, 291, 458, 437, 366, 264, 7139], "temperature": 0.0, "avg_logprob": -0.07872957396275788, "compression_ratio": 1.9823008849557522, "no_speech_prob": 4.381839460165793e-07}, {"id": 618, "seek": 394052, "start": 3952.0, "end": 3957.12, "text": " of value in that thing you're trying to do the levers what are the things you can change", "tokens": [295, 2158, 294, 300, 551, 291, 434, 1382, 281, 360, 264, 45571, 437, 366, 264, 721, 291, 393, 1319], "temperature": 0.0, "avg_logprob": -0.07872957396275788, "compression_ratio": 1.9823008849557522, "no_speech_prob": 4.381839460165793e-07}, {"id": 619, "seek": 394052, "start": 3957.12, "end": 3962.68, "text": " like what's the point of a predictive model if you can't do anything about it right figuring", "tokens": [411, 437, 311, 264, 935, 295, 257, 35521, 2316, 498, 291, 393, 380, 360, 1340, 466, 309, 558, 15213], "temperature": 0.0, "avg_logprob": -0.07872957396275788, "compression_ratio": 1.9823008849557522, "no_speech_prob": 4.381839460165793e-07}, {"id": 620, "seek": 394052, "start": 3962.68, "end": 3966.96, "text": " out ways to find what data you you don't have which one's suitable what's available then", "tokens": [484, 2098, 281, 915, 437, 1412, 291, 291, 500, 380, 362, 597, 472, 311, 12873, 437, 311, 2435, 550], "temperature": 0.0, "avg_logprob": -0.07872957396275788, "compression_ratio": 1.9823008849557522, "no_speech_prob": 4.381839460165793e-07}, {"id": 621, "seek": 396696, "start": 3966.96, "end": 3973.04, "text": " think about what approaches to analytics you can then take and then super important like", "tokens": [519, 466, 437, 11587, 281, 15370, 291, 393, 550, 747, 293, 550, 1687, 1021, 411], "temperature": 0.0, "avg_logprob": -0.08925862842135959, "compression_ratio": 1.8823529411764706, "no_speech_prob": 8.315265631608781e-07}, {"id": 622, "seek": 396696, "start": 3973.04, "end": 3979.64, "text": " well can you actually implement you know those changes and super super important how do you", "tokens": [731, 393, 291, 767, 4445, 291, 458, 729, 2962, 293, 1687, 1687, 1021, 577, 360, 291], "temperature": 0.0, "avg_logprob": -0.08925862842135959, "compression_ratio": 1.8823529411764706, "no_speech_prob": 8.315265631608781e-07}, {"id": 623, "seek": 396696, "start": 3979.64, "end": 3984.32, "text": " actually change things as the environment changes and you know interestingly a lot of", "tokens": [767, 1319, 721, 382, 264, 2823, 2962, 293, 291, 458, 25873, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.08925862842135959, "compression_ratio": 1.8823529411764706, "no_speech_prob": 8.315265631608781e-07}, {"id": 624, "seek": 396696, "start": 3984.32, "end": 3989.68, "text": " these things areas where there's not very much academic research there's a little bit", "tokens": [613, 721, 3179, 689, 456, 311, 406, 588, 709, 7778, 2132, 456, 311, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.08925862842135959, "compression_ratio": 1.8823529411764706, "no_speech_prob": 8.315265631608781e-07}, {"id": 625, "seek": 396696, "start": 3989.68, "end": 3995.44, "text": " and some of the papers that have been particularly around maintenance of like how do you decide", "tokens": [293, 512, 295, 264, 10577, 300, 362, 668, 4098, 926, 11258, 295, 411, 577, 360, 291, 4536], "temperature": 0.0, "avg_logprob": -0.08925862842135959, "compression_ratio": 1.8823529411764706, "no_speech_prob": 8.315265631608781e-07}, {"id": 626, "seek": 399544, "start": 3995.44, "end": 4001.12, "text": " when your machine learning model is kind of still okay how do you update it over time", "tokens": [562, 428, 3479, 2539, 2316, 307, 733, 295, 920, 1392, 577, 360, 291, 5623, 309, 670, 565], "temperature": 0.0, "avg_logprob": -0.0949252913979923, "compression_ratio": 1.7312252964426877, "no_speech_prob": 8.714308705748408e-07}, {"id": 627, "seek": 399544, "start": 4001.12, "end": 4007.8, "text": " have had like many many many many citations but they don't pop up very often because a", "tokens": [362, 632, 411, 867, 867, 867, 867, 4814, 763, 457, 436, 500, 380, 1665, 493, 588, 2049, 570, 257], "temperature": 0.0, "avg_logprob": -0.0949252913979923, "compression_ratio": 1.7312252964426877, "no_speech_prob": 8.714308705748408e-07}, {"id": 628, "seek": 399544, "start": 4007.8, "end": 4012.56, "text": " lot of folks are so focused on the math you know and then there's the whole question of", "tokens": [688, 295, 4024, 366, 370, 5178, 322, 264, 5221, 291, 458, 293, 550, 456, 311, 264, 1379, 1168, 295], "temperature": 0.0, "avg_logprob": -0.0949252913979923, "compression_ratio": 1.7312252964426877, "no_speech_prob": 8.714308705748408e-07}, {"id": 629, "seek": 399544, "start": 4012.56, "end": 4017.04, "text": " like what constraints are in place across this whole thing so what you'll find in the", "tokens": [411, 437, 18491, 366, 294, 1081, 2108, 341, 1379, 551, 370, 437, 291, 603, 915, 294, 264], "temperature": 0.0, "avg_logprob": -0.0949252913979923, "compression_ratio": 1.7312252964426877, "no_speech_prob": 8.714308705748408e-07}, {"id": 630, "seek": 399544, "start": 4017.04, "end": 4023.12, "text": " book is there is a whole appendix which actually goes through every one of these six things", "tokens": [1446, 307, 456, 307, 257, 1379, 34116, 970, 597, 767, 1709, 807, 633, 472, 295, 613, 2309, 721], "temperature": 0.0, "avg_logprob": -0.0949252913979923, "compression_ratio": 1.7312252964426877, "no_speech_prob": 8.714308705748408e-07}, {"id": 631, "seek": 402312, "start": 4023.12, "end": 4033.3199999999997, "text": " and has a whole list of examples so this is an example of how to like think about value", "tokens": [293, 575, 257, 1379, 1329, 295, 5110, 370, 341, 307, 364, 1365, 295, 577, 281, 411, 519, 466, 2158], "temperature": 0.0, "avg_logprob": -0.1438805090414511, "compression_ratio": 1.6804123711340206, "no_speech_prob": 1.1911001820408273e-06}, {"id": 632, "seek": 402312, "start": 4033.3199999999997, "end": 4041.2, "text": " and lots of questions that companies and organizations can use to try and think about you know all", "tokens": [293, 3195, 295, 1651, 300, 3431, 293, 6150, 393, 764, 281, 853, 293, 519, 466, 291, 458, 439], "temperature": 0.0, "avg_logprob": -0.1438805090414511, "compression_ratio": 1.6804123711340206, "no_speech_prob": 1.1911001820408273e-06}, {"id": 633, "seek": 402312, "start": 4041.2, "end": 4046.7999999999997, "text": " of these different pieces of the actual puzzle of getting stuff into production and actually", "tokens": [295, 613, 819, 3755, 295, 264, 3539, 12805, 295, 1242, 1507, 666, 4265, 293, 767], "temperature": 0.0, "avg_logprob": -0.1438805090414511, "compression_ratio": 1.6804123711340206, "no_speech_prob": 1.1911001820408273e-06}, {"id": 634, "seek": 402312, "start": 4046.7999999999997, "end": 4048.0, "text": " into an effective product.", "tokens": [666, 364, 4942, 1674, 13], "temperature": 0.0, "avg_logprob": -0.1438805090414511, "compression_ratio": 1.6804123711340206, "no_speech_prob": 1.1911001820408273e-06}, {"id": 635, "seek": 402312, "start": 4048.0, "end": 4049.0, "text": " We have a question.", "tokens": [492, 362, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1438805090414511, "compression_ratio": 1.6804123711340206, "no_speech_prob": 1.1911001820408273e-06}, {"id": 636, "seek": 404900, "start": 4049.0, "end": 4054.36, "text": " Sure just a moment so I say so do check out this appendix because it actually originally", "tokens": [4894, 445, 257, 1623, 370, 286, 584, 370, 360, 1520, 484, 341, 34116, 970, 570, 309, 767, 7993], "temperature": 0.0, "avg_logprob": -0.11186043578799408, "compression_ratio": 1.6199261992619927, "no_speech_prob": 9.132497780228732e-07}, {"id": 637, "seek": 404900, "start": 4054.36, "end": 4060.56, "text": " appeared as a blog post and I think except for my COVID-19 posts that I did with Rachel", "tokens": [8516, 382, 257, 6968, 2183, 293, 286, 519, 3993, 337, 452, 4566, 12, 3405, 12300, 300, 286, 630, 365, 14246], "temperature": 0.0, "avg_logprob": -0.11186043578799408, "compression_ratio": 1.6199261992619927, "no_speech_prob": 9.132497780228732e-07}, {"id": 638, "seek": 404900, "start": 4060.56, "end": 4064.88, "text": " it's actually the most popular blog post I've ever written it's had hundreds of thousands", "tokens": [309, 311, 767, 264, 881, 3743, 6968, 2183, 286, 600, 1562, 3720, 309, 311, 632, 6779, 295, 5383], "temperature": 0.0, "avg_logprob": -0.11186043578799408, "compression_ratio": 1.6199261992619927, "no_speech_prob": 9.132497780228732e-07}, {"id": 639, "seek": 404900, "start": 4064.88, "end": 4072.08, "text": " of views and it kind of represents like 20 years of hard-won insights about like how", "tokens": [295, 6809, 293, 309, 733, 295, 8855, 411, 945, 924, 295, 1152, 12, 14693, 14310, 466, 411, 577], "temperature": 0.0, "avg_logprob": -0.11186043578799408, "compression_ratio": 1.6199261992619927, "no_speech_prob": 9.132497780228732e-07}, {"id": 640, "seek": 404900, "start": 4072.08, "end": 4076.44, "text": " you actually get value from machine learning and practice and what you actually have to", "tokens": [291, 767, 483, 2158, 490, 3479, 2539, 293, 3124, 293, 437, 291, 767, 362, 281], "temperature": 0.0, "avg_logprob": -0.11186043578799408, "compression_ratio": 1.6199261992619927, "no_speech_prob": 9.132497780228732e-07}, {"id": 641, "seek": 407644, "start": 4076.44, "end": 4080.42, "text": " ask so please check it out because hopefully you'll find it helpful.", "tokens": [1029, 370, 1767, 1520, 309, 484, 570, 4696, 291, 603, 915, 309, 4961, 13], "temperature": 0.0, "avg_logprob": -0.08265154140511739, "compression_ratio": 1.683794466403162, "no_speech_prob": 3.011593605606322e-07}, {"id": 642, "seek": 407644, "start": 4080.42, "end": 4086.32, "text": " So when we think about like think about this for the question of how should people think", "tokens": [407, 562, 321, 519, 466, 411, 519, 466, 341, 337, 264, 1168, 295, 577, 820, 561, 519], "temperature": 0.0, "avg_logprob": -0.08265154140511739, "compression_ratio": 1.683794466403162, "no_speech_prob": 3.011593605606322e-07}, {"id": 643, "seek": 407644, "start": 4086.32, "end": 4093.84, "text": " about the relationship between seasonality and transmissibility of COVID-19 you kind", "tokens": [466, 264, 2480, 1296, 3196, 1860, 293, 7715, 891, 2841, 295, 4566, 12, 3405, 291, 733], "temperature": 0.0, "avg_logprob": -0.08265154140511739, "compression_ratio": 1.683794466403162, "no_speech_prob": 3.011593605606322e-07}, {"id": 644, "seek": 407644, "start": 4093.84, "end": 4099.92, "text": " of need to dig really deeply into the questions about like oh not just what what's that what", "tokens": [295, 643, 281, 2528, 534, 8760, 666, 264, 1651, 466, 411, 1954, 406, 445, 437, 437, 311, 300, 437], "temperature": 0.0, "avg_logprob": -0.08265154140511739, "compression_ratio": 1.683794466403162, "no_speech_prob": 3.011593605606322e-07}, {"id": 645, "seek": 407644, "start": 4099.92, "end": 4103.92, "text": " are those numbers in the data but what does it really look like right so one of the things", "tokens": [366, 729, 3547, 294, 264, 1412, 457, 437, 775, 309, 534, 574, 411, 558, 370, 472, 295, 264, 721], "temperature": 0.0, "avg_logprob": -0.08265154140511739, "compression_ratio": 1.683794466403162, "no_speech_prob": 3.011593605606322e-07}, {"id": 646, "seek": 410392, "start": 4103.92, "end": 4113.92, "text": " in the paper that they show is actual maps right of temperature and humidity and R right", "tokens": [294, 264, 3035, 300, 436, 855, 307, 3539, 11317, 558, 295, 4292, 293, 24751, 293, 497, 558], "temperature": 0.0, "avg_logprob": -0.11683418224384258, "compression_ratio": 1.815, "no_speech_prob": 2.069836426699112e-07}, {"id": 647, "seek": 410392, "start": 4113.92, "end": 4121.12, "text": " and you can see like not surprisingly that humidity and temperature in China are what", "tokens": [293, 291, 393, 536, 411, 406, 17600, 300, 24751, 293, 4292, 294, 3533, 366, 437], "temperature": 0.0, "avg_logprob": -0.11683418224384258, "compression_ratio": 1.815, "no_speech_prob": 2.069836426699112e-07}, {"id": 648, "seek": 410392, "start": 4121.12, "end": 4126.2, "text": " we would call autocorrelated which is to say that places that are close to each other in", "tokens": [321, 576, 818, 45833, 284, 12004, 597, 307, 281, 584, 300, 3190, 300, 366, 1998, 281, 1184, 661, 294], "temperature": 0.0, "avg_logprob": -0.11683418224384258, "compression_ratio": 1.815, "no_speech_prob": 2.069836426699112e-07}, {"id": 649, "seek": 410392, "start": 4126.2, "end": 4133.88, "text": " this case geographically have similar temperatures and similar humidities and so like this actually", "tokens": [341, 1389, 25435, 984, 362, 2531, 12633, 293, 2531, 34649, 1088, 293, 370, 411, 341, 767], "temperature": 0.0, "avg_logprob": -0.11683418224384258, "compression_ratio": 1.815, "no_speech_prob": 2.069836426699112e-07}, {"id": 650, "seek": 413388, "start": 4133.88, "end": 4141.16, "text": " puts into the question the a lot the p-values that they have right because you you can't", "tokens": [8137, 666, 264, 1168, 264, 257, 688, 264, 280, 12, 46033, 300, 436, 362, 558, 570, 291, 291, 393, 380], "temperature": 0.0, "avg_logprob": -0.09707910364324396, "compression_ratio": 1.7759336099585061, "no_speech_prob": 1.067699713530601e-06}, {"id": 651, "seek": 413388, "start": 4141.16, "end": 4145.0, "text": " really think of these as a hundred totally separate cities because the ones that are", "tokens": [534, 519, 295, 613, 382, 257, 3262, 3879, 4994, 6486, 570, 264, 2306, 300, 366], "temperature": 0.0, "avg_logprob": -0.09707910364324396, "compression_ratio": 1.7759336099585061, "no_speech_prob": 1.067699713530601e-06}, {"id": 652, "seek": 413388, "start": 4145.0, "end": 4148.72, "text": " close to each other probably have very close behavior so maybe you should think of them", "tokens": [1998, 281, 1184, 661, 1391, 362, 588, 1998, 5223, 370, 1310, 291, 820, 519, 295, 552], "temperature": 0.0, "avg_logprob": -0.09707910364324396, "compression_ratio": 1.7759336099585061, "no_speech_prob": 1.067699713530601e-06}, {"id": 653, "seek": 413388, "start": 4148.72, "end": 4156.72, "text": " as like a small number of sets of cities you know of kind of larger geographies.", "tokens": [382, 411, 257, 1359, 1230, 295, 6352, 295, 6486, 291, 458, 295, 733, 295, 4833, 25435, 530, 13], "temperature": 0.0, "avg_logprob": -0.09707910364324396, "compression_ratio": 1.7759336099585061, "no_speech_prob": 1.067699713530601e-06}, {"id": 654, "seek": 413388, "start": 4156.72, "end": 4160.6, "text": " So these are the kinds of things that when you look actually into a model you need to", "tokens": [407, 613, 366, 264, 3685, 295, 721, 300, 562, 291, 574, 767, 666, 257, 2316, 291, 643, 281], "temperature": 0.0, "avg_logprob": -0.09707910364324396, "compression_ratio": 1.7759336099585061, "no_speech_prob": 1.067699713530601e-06}, {"id": 655, "seek": 416060, "start": 4160.6, "end": 4165.400000000001, "text": " like think about what are the what are the limitations but then to decide like well what", "tokens": [411, 519, 466, 437, 366, 264, 437, 366, 264, 15705, 457, 550, 281, 4536, 411, 731, 437], "temperature": 0.0, "avg_logprob": -0.08448989446773085, "compression_ratio": 1.9175824175824177, "no_speech_prob": 6.179384399729315e-07}, {"id": 656, "seek": 416060, "start": 4165.400000000001, "end": 4173.0, "text": " does that mean what do I what do I do about that you you need to think of it from this", "tokens": [775, 300, 914, 437, 360, 286, 437, 360, 286, 360, 466, 300, 291, 291, 643, 281, 519, 295, 309, 490, 341], "temperature": 0.0, "avg_logprob": -0.08448989446773085, "compression_ratio": 1.9175824175824177, "no_speech_prob": 6.179384399729315e-07}, {"id": 657, "seek": 416060, "start": 4173.0, "end": 4178.72, "text": " kind of utility point of view this kind of end-to-end what are the actions I can take", "tokens": [733, 295, 14877, 935, 295, 1910, 341, 733, 295, 917, 12, 1353, 12, 521, 437, 366, 264, 5909, 286, 393, 747], "temperature": 0.0, "avg_logprob": -0.08448989446773085, "compression_ratio": 1.9175824175824177, "no_speech_prob": 6.179384399729315e-07}, {"id": 658, "seek": 416060, "start": 4178.72, "end": 4184.400000000001, "text": " what are the results point of view not just null hypothesis testing so in this case for", "tokens": [437, 366, 264, 3542, 935, 295, 1910, 406, 445, 18184, 17291, 4997, 370, 294, 341, 1389, 337], "temperature": 0.0, "avg_logprob": -0.08448989446773085, "compression_ratio": 1.9175824175824177, "no_speech_prob": 6.179384399729315e-07}, {"id": 659, "seek": 418440, "start": 4184.4, "end": 4192.799999999999, "text": " example there are basically four possible key ways this could end up it could end up", "tokens": [1365, 456, 366, 1936, 1451, 1944, 2141, 2098, 341, 727, 917, 493, 309, 727, 917, 493], "temperature": 0.0, "avg_logprob": -0.06352177533236417, "compression_ratio": 2.1646341463414633, "no_speech_prob": 2.419883173843118e-07}, {"id": 660, "seek": 418440, "start": 4192.799999999999, "end": 4201.16, "text": " that there really is a relationship between temperature and R or so that's what the right", "tokens": [300, 456, 534, 307, 257, 2480, 1296, 4292, 293, 497, 420, 370, 300, 311, 437, 264, 558], "temperature": 0.0, "avg_logprob": -0.06352177533236417, "compression_ratio": 2.1646341463414633, "no_speech_prob": 2.419883173843118e-07}, {"id": 661, "seek": 418440, "start": 4201.16, "end": 4208.04, "text": " hand side is or there is no real relationship between temperature and R and we might act", "tokens": [1011, 1252, 307, 420, 456, 307, 572, 957, 2480, 1296, 4292, 293, 497, 293, 321, 1062, 605], "temperature": 0.0, "avg_logprob": -0.06352177533236417, "compression_ratio": 2.1646341463414633, "no_speech_prob": 2.419883173843118e-07}, {"id": 662, "seek": 418440, "start": 4208.04, "end": 4212.4, "text": " on the assumption that there is a relationship or we might act on the assumption that there", "tokens": [322, 264, 15302, 300, 456, 307, 257, 2480, 420, 321, 1062, 605, 322, 264, 15302, 300, 456], "temperature": 0.0, "avg_logprob": -0.06352177533236417, "compression_ratio": 2.1646341463414633, "no_speech_prob": 2.419883173843118e-07}, {"id": 663, "seek": 421240, "start": 4212.4, "end": 4216.86, "text": " isn't a relationship and so you kind of want to look at each of these four possibilities", "tokens": [1943, 380, 257, 2480, 293, 370, 291, 733, 295, 528, 281, 574, 412, 1184, 295, 613, 1451, 12178], "temperature": 0.0, "avg_logprob": -0.09549983118621397, "compression_ratio": 1.712707182320442, "no_speech_prob": 6.475939926531282e-07}, {"id": 664, "seek": 421240, "start": 4216.86, "end": 4223.92, "text": " and say like well what would be the economic and societal consequences and you know there's", "tokens": [293, 584, 411, 731, 437, 576, 312, 264, 4836, 293, 33472, 10098, 293, 291, 458, 456, 311], "temperature": 0.0, "avg_logprob": -0.09549983118621397, "compression_ratio": 1.712707182320442, "no_speech_prob": 6.475939926531282e-07}, {"id": 665, "seek": 421240, "start": 4223.92, "end": 4229.48, "text": " going to be a huge difference in lives lost and you know economies crashing and whatever", "tokens": [516, 281, 312, 257, 2603, 2649, 294, 2909, 2731, 293, 291, 458, 23158, 26900, 293, 2035], "temperature": 0.0, "avg_logprob": -0.09549983118621397, "compression_ratio": 1.712707182320442, "no_speech_prob": 6.475939926531282e-07}, {"id": 666, "seek": 421240, "start": 4229.48, "end": 4236.16, "text": " else to you know for each of these four.", "tokens": [1646, 281, 291, 458, 337, 1184, 295, 613, 1451, 13], "temperature": 0.0, "avg_logprob": -0.09549983118621397, "compression_ratio": 1.712707182320442, "no_speech_prob": 6.475939926531282e-07}, {"id": 667, "seek": 423616, "start": 4236.16, "end": 4242.84, "text": " The paper actually you know has shown if their model is correct what's the likely R value", "tokens": [440, 3035, 767, 291, 458, 575, 4898, 498, 641, 2316, 307, 3006, 437, 311, 264, 3700, 497, 2158], "temperature": 0.0, "avg_logprob": -0.08082888292711835, "compression_ratio": 1.7783251231527093, "no_speech_prob": 4.965274342794146e-07}, {"id": 668, "seek": 423616, "start": 4242.84, "end": 4249.92, "text": " in March for like every city in the world and the likely R value in July for every city", "tokens": [294, 6129, 337, 411, 633, 2307, 294, 264, 1002, 293, 264, 3700, 497, 2158, 294, 7370, 337, 633, 2307], "temperature": 0.0, "avg_logprob": -0.08082888292711835, "compression_ratio": 1.7783251231527093, "no_speech_prob": 4.965274342794146e-07}, {"id": 669, "seek": 423616, "start": 4249.92, "end": 4255.5199999999995, "text": " in the world and so for example if you look at kind of New England and New York the prediction", "tokens": [294, 264, 1002, 293, 370, 337, 1365, 498, 291, 574, 412, 733, 295, 1873, 8196, 293, 1873, 3609, 264, 17630], "temperature": 0.0, "avg_logprob": -0.08082888292711835, "compression_ratio": 1.7783251231527093, "no_speech_prob": 4.965274342794146e-07}, {"id": 670, "seek": 423616, "start": 4255.5199999999995, "end": 4262.5, "text": " here is and also West Coast the very coast of the West Coast is that in July the disease", "tokens": [510, 307, 293, 611, 4055, 14960, 264, 588, 8684, 295, 264, 4055, 14960, 307, 300, 294, 7370, 264, 4752], "temperature": 0.0, "avg_logprob": -0.08082888292711835, "compression_ratio": 1.7783251231527093, "no_speech_prob": 4.965274342794146e-07}, {"id": 671, "seek": 426250, "start": 4262.5, "end": 4268.92, "text": " will stop spreading now you know if that happens if they're right then that's going to be a", "tokens": [486, 1590, 15232, 586, 291, 458, 498, 300, 2314, 498, 436, 434, 558, 550, 300, 311, 516, 281, 312, 257], "temperature": 0.0, "avg_logprob": -0.07430376403633206, "compression_ratio": 1.609865470852018, "no_speech_prob": 7.81146638928476e-07}, {"id": 672, "seek": 426250, "start": 4268.92, "end": 4274.8, "text": " disaster because I think it's very likely in America and also the UK that people will", "tokens": [11293, 570, 286, 519, 309, 311, 588, 3700, 294, 3374, 293, 611, 264, 7051, 300, 561, 486], "temperature": 0.0, "avg_logprob": -0.07430376403633206, "compression_ratio": 1.609865470852018, "no_speech_prob": 7.81146638928476e-07}, {"id": 673, "seek": 426250, "start": 4274.8, "end": 4280.48, "text": " say oh turns out this disease is not a problem you know it didn't really take off at all", "tokens": [584, 1954, 4523, 484, 341, 4752, 307, 406, 257, 1154, 291, 458, 309, 994, 380, 534, 747, 766, 412, 439], "temperature": 0.0, "avg_logprob": -0.07430376403633206, "compression_ratio": 1.609865470852018, "no_speech_prob": 7.81146638928476e-07}, {"id": 674, "seek": 426250, "start": 4280.48, "end": 4286.52, "text": " the scientists were wrong people will go back to their previous day-to-day life and we could", "tokens": [264, 7708, 645, 2085, 561, 486, 352, 646, 281, 641, 3894, 786, 12, 1353, 12, 810, 993, 293, 321, 727], "temperature": 0.0, "avg_logprob": -0.07430376403633206, "compression_ratio": 1.609865470852018, "no_speech_prob": 7.81146638928476e-07}, {"id": 675, "seek": 428652, "start": 4286.52, "end": 4294.240000000001, "text": " see what happened in 1918 flu virus of like the second go-around when winter hits could", "tokens": [536, 437, 2011, 294, 36588, 5029, 5752, 295, 411, 264, 1150, 352, 12, 25762, 562, 6355, 8664, 727], "temperature": 0.0, "avg_logprob": -0.12580230759411323, "compression_ratio": 1.6192660550458715, "no_speech_prob": 1.7603252899789368e-06}, {"id": 676, "seek": 428652, "start": 4294.240000000001, "end": 4302.280000000001, "text": " be much worse than than the start right so like there's these kind of like huge potential", "tokens": [312, 709, 5324, 813, 813, 264, 722, 558, 370, 411, 456, 311, 613, 733, 295, 411, 2603, 3995], "temperature": 0.0, "avg_logprob": -0.12580230759411323, "compression_ratio": 1.6192660550458715, "no_speech_prob": 1.7603252899789368e-06}, {"id": 677, "seek": 428652, "start": 4302.280000000001, "end": 4309.64, "text": " policy impacts depending on whether this is true or false and so to think about it to", "tokens": [3897, 11606, 5413, 322, 1968, 341, 307, 2074, 420, 7908, 293, 370, 281, 519, 466, 309, 281], "temperature": 0.0, "avg_logprob": -0.12580230759411323, "compression_ratio": 1.6192660550458715, "no_speech_prob": 1.7603252899789368e-06}, {"id": 678, "seek": 428652, "start": 4309.64, "end": 4315.040000000001, "text": " yes I also just wanted to say that it would be it would be very irresponsible to to think", "tokens": [2086, 286, 611, 445, 1415, 281, 584, 300, 309, 576, 312, 309, 576, 312, 588, 46320, 281, 281, 519], "temperature": 0.0, "avg_logprob": -0.12580230759411323, "compression_ratio": 1.6192660550458715, "no_speech_prob": 1.7603252899789368e-06}, {"id": 679, "seek": 431504, "start": 4315.04, "end": 4320.88, "text": " oh summer is going to solve it we don't need to act now I'm just in that this is something", "tokens": [1954, 4266, 307, 516, 281, 5039, 309, 321, 500, 380, 643, 281, 605, 586, 286, 478, 445, 294, 300, 341, 307, 746], "temperature": 0.0, "avg_logprob": -0.1468369043790377, "compression_ratio": 1.8354430379746836, "no_speech_prob": 2.6425432224641554e-06}, {"id": 680, "seek": 431504, "start": 4320.88, "end": 4325.0, "text": " growing exponentially and could do a huge huge amount of damage yeah yeah so it could", "tokens": [4194, 37330, 293, 727, 360, 257, 2603, 2603, 2372, 295, 4344, 1338, 1338, 370, 309, 727], "temperature": 0.0, "avg_logprob": -0.1468369043790377, "compression_ratio": 1.8354430379746836, "no_speech_prob": 2.6425432224641554e-06}, {"id": 681, "seek": 431504, "start": 4325.0, "end": 4331.36, "text": " already has done exactly either way if you assume that there will be seasonality and", "tokens": [1217, 575, 1096, 2293, 2139, 636, 498, 291, 6552, 300, 456, 486, 312, 3196, 1860, 293], "temperature": 0.0, "avg_logprob": -0.1468369043790377, "compression_ratio": 1.8354430379746836, "no_speech_prob": 2.6425432224641554e-06}, {"id": 682, "seek": 431504, "start": 4331.36, "end": 4336.36, "text": " that summer will fix things then it could lead you to be apathetic now if you assume", "tokens": [300, 4266, 486, 3191, 721, 550, 309, 727, 1477, 291, 281, 312, 1882, 998, 3532, 586, 498, 291, 6552], "temperature": 0.0, "avg_logprob": -0.1468369043790377, "compression_ratio": 1.8354430379746836, "no_speech_prob": 2.6425432224641554e-06}, {"id": 683, "seek": 431504, "start": 4336.36, "end": 4343.5199999999995, "text": " there's no seasonality and then there is then you could end up kind of creating a larger", "tokens": [456, 311, 572, 3196, 1860, 293, 550, 456, 307, 550, 291, 727, 917, 493, 733, 295, 4084, 257, 4833], "temperature": 0.0, "avg_logprob": -0.1468369043790377, "compression_ratio": 1.8354430379746836, "no_speech_prob": 2.6425432224641554e-06}, {"id": 684, "seek": 434352, "start": 4343.52, "end": 4348.9400000000005, "text": " level of expectation of distraction than actually happens and end up with your population being", "tokens": [1496, 295, 14334, 295, 30217, 813, 767, 2314, 293, 917, 493, 365, 428, 4415, 885], "temperature": 0.0, "avg_logprob": -0.08881930008675289, "compression_ratio": 1.8395061728395061, "no_speech_prob": 9.132508580478316e-07}, {"id": 685, "seek": 434352, "start": 4348.9400000000005, "end": 4353.68, "text": " even more apathetic you know so that they're you know being wrong in any direction of your", "tokens": [754, 544, 1882, 998, 3532, 291, 458, 370, 300, 436, 434, 291, 458, 885, 2085, 294, 604, 3513, 295, 428], "temperature": 0.0, "avg_logprob": -0.08881930008675289, "compression_ratio": 1.8395061728395061, "no_speech_prob": 9.132508580478316e-07}, {"id": 686, "seek": 434352, "start": 4353.68, "end": 4358.9400000000005, "text": " problem so one of the ways we tend to deal with this with with this kind of modeling", "tokens": [1154, 370, 472, 295, 264, 2098, 321, 3928, 281, 2028, 365, 341, 365, 365, 341, 733, 295, 15983], "temperature": 0.0, "avg_logprob": -0.08881930008675289, "compression_ratio": 1.8395061728395061, "no_speech_prob": 9.132508580478316e-07}, {"id": 687, "seek": 434352, "start": 4358.9400000000005, "end": 4364.360000000001, "text": " is we try to think about priors so priors are basically things where we you know rather", "tokens": [307, 321, 853, 281, 519, 466, 1790, 830, 370, 1790, 830, 366, 1936, 721, 689, 321, 291, 458, 2831], "temperature": 0.0, "avg_logprob": -0.08881930008675289, "compression_ratio": 1.8395061728395061, "no_speech_prob": 9.132508580478316e-07}, {"id": 688, "seek": 434352, "start": 4364.360000000001, "end": 4370.200000000001, "text": " than just having a null hypothesis we try and start with a guess as to like well what's", "tokens": [813, 445, 1419, 257, 18184, 17291, 321, 853, 293, 722, 365, 257, 2041, 382, 281, 411, 731, 437, 311], "temperature": 0.0, "avg_logprob": -0.08881930008675289, "compression_ratio": 1.8395061728395061, "no_speech_prob": 9.132508580478316e-07}, {"id": 689, "seek": 437020, "start": 4370.2, "end": 4376.48, "text": " what's more likely right so in this case if memory says correctly I think we know that", "tokens": [437, 311, 544, 3700, 558, 370, 294, 341, 1389, 498, 4675, 1619, 8944, 286, 519, 321, 458, 300], "temperature": 0.0, "avg_logprob": -0.11305669991366835, "compression_ratio": 1.6839622641509433, "no_speech_prob": 2.4439802928100107e-06}, {"id": 690, "seek": 437020, "start": 4376.48, "end": 4385.08, "text": " like flu viruses become inactive at 27 centigrade we know that like cold the cold coronaviruses", "tokens": [411, 5029, 21785, 1813, 294, 12596, 412, 7634, 44731, 321, 458, 300, 411, 3554, 264, 3554, 10451, 706, 347, 8355], "temperature": 0.0, "avg_logprob": -0.11305669991366835, "compression_ratio": 1.6839622641509433, "no_speech_prob": 2.4439802928100107e-06}, {"id": 691, "seek": 437020, "start": 4385.08, "end": 4393.5199999999995, "text": " are seasonal the 1918 oopsie daisy the 1918 flu epidemic pandemic was seasonal in every", "tokens": [366, 27421, 264, 36588, 34166, 414, 1120, 14169, 264, 36588, 5029, 20982, 5388, 390, 27421, 294, 633], "temperature": 0.0, "avg_logprob": -0.11305669991366835, "compression_ratio": 1.6839622641509433, "no_speech_prob": 2.4439802928100107e-06}, {"id": 692, "seek": 437020, "start": 4393.5199999999995, "end": 4397.96, "text": " country and city that's been studied so far there's been quite a few studies like this", "tokens": [1941, 293, 2307, 300, 311, 668, 9454, 370, 1400, 456, 311, 668, 1596, 257, 1326, 5313, 411, 341], "temperature": 0.0, "avg_logprob": -0.11305669991366835, "compression_ratio": 1.6839622641509433, "no_speech_prob": 2.4439802928100107e-06}, {"id": 693, "seek": 439796, "start": 4397.96, "end": 4404.44, "text": " they've always found climate relationships so far so maybe we'd say well prior belief", "tokens": [436, 600, 1009, 1352, 5659, 6159, 370, 1400, 370, 1310, 321, 1116, 584, 731, 4059, 7107], "temperature": 0.0, "avg_logprob": -0.10530690511067708, "compression_ratio": 1.6523809523809523, "no_speech_prob": 2.3320676518778782e-06}, {"id": 694, "seek": 439796, "start": 4404.44, "end": 4408.92, "text": " is that this thing is probably seasonal and so then we'd say well this particular paper", "tokens": [307, 300, 341, 551, 307, 1391, 27421, 293, 370, 550, 321, 1116, 584, 731, 341, 1729, 3035], "temperature": 0.0, "avg_logprob": -0.10530690511067708, "compression_ratio": 1.6523809523809523, "no_speech_prob": 2.3320676518778782e-06}, {"id": 695, "seek": 439796, "start": 4408.92, "end": 4419.72, "text": " adds some evidence to that so like it shows like how incredibly complex it is to use a", "tokens": [10860, 512, 4467, 281, 300, 370, 411, 309, 3110, 411, 577, 6252, 3997, 309, 307, 281, 764, 257], "temperature": 0.0, "avg_logprob": -0.10530690511067708, "compression_ratio": 1.6523809523809523, "no_speech_prob": 2.3320676518778782e-06}, {"id": 696, "seek": 439796, "start": 4419.72, "end": 4426.12, "text": " model in practice for in this case policy discussions but also for like organizational", "tokens": [2316, 294, 3124, 337, 294, 341, 1389, 3897, 11088, 457, 611, 337, 411, 24730], "temperature": 0.0, "avg_logprob": -0.10530690511067708, "compression_ratio": 1.6523809523809523, "no_speech_prob": 2.3320676518778782e-06}, {"id": 697, "seek": 442612, "start": 4426.12, "end": 4431.9, "text": " decisions because you know there's always complexities there's always uncertainties", "tokens": [5327, 570, 291, 458, 456, 311, 1009, 48705, 456, 311, 1009, 11308, 6097], "temperature": 0.0, "avg_logprob": -0.06414573223559887, "compression_ratio": 1.758974358974359, "no_speech_prob": 4.495153916650452e-06}, {"id": 698, "seek": 442612, "start": 4431.9, "end": 4437.599999999999, "text": " and so you actually have to think about the the utilities you know and your best guesses", "tokens": [293, 370, 291, 767, 362, 281, 519, 466, 264, 264, 30482, 291, 458, 293, 428, 1151, 42703], "temperature": 0.0, "avg_logprob": -0.06414573223559887, "compression_ratio": 1.758974358974359, "no_speech_prob": 4.495153916650452e-06}, {"id": 699, "seek": 442612, "start": 4437.599999999999, "end": 4448.2, "text": " and try to combine everything together as best as you can okay so with all that said", "tokens": [293, 853, 281, 10432, 1203, 1214, 382, 1151, 382, 291, 393, 1392, 370, 365, 439, 300, 848], "temperature": 0.0, "avg_logprob": -0.06414573223559887, "compression_ratio": 1.758974358974359, "no_speech_prob": 4.495153916650452e-06}, {"id": 700, "seek": 442612, "start": 4448.2, "end": 4455.099999999999, "text": " it's still nice to be able to get our our models up and running even if you know even", "tokens": [309, 311, 920, 1481, 281, 312, 1075, 281, 483, 527, 527, 5245, 493, 293, 2614, 754, 498, 291, 458, 754], "temperature": 0.0, "avg_logprob": -0.06414573223559887, "compression_ratio": 1.758974358974359, "no_speech_prob": 4.495153916650452e-06}, {"id": 701, "seek": 445510, "start": 4455.1, "end": 4460.900000000001, "text": " just a predictive model is sometimes useful of its own sometimes it's useful to prototype", "tokens": [445, 257, 35521, 2316, 307, 2171, 4420, 295, 1080, 1065, 2171, 309, 311, 4420, 281, 19475], "temperature": 0.0, "avg_logprob": -0.0771849931672562, "compression_ratio": 1.7135922330097086, "no_speech_prob": 5.014693215343868e-06}, {"id": 702, "seek": 445510, "start": 4460.900000000001, "end": 4466.360000000001, "text": " something and sometimes it's just it's going to be part of some bigger picture so rather", "tokens": [746, 293, 2171, 309, 311, 445, 309, 311, 516, 281, 312, 644, 295, 512, 3801, 3036, 370, 2831], "temperature": 0.0, "avg_logprob": -0.0771849931672562, "compression_ratio": 1.7135922330097086, "no_speech_prob": 5.014693215343868e-06}, {"id": 703, "seek": 445510, "start": 4466.360000000001, "end": 4470.56, "text": " than try to create some huge end-to-end model here we thought we would just show you how", "tokens": [813, 853, 281, 1884, 512, 2603, 917, 12, 1353, 12, 521, 2316, 510, 321, 1194, 321, 576, 445, 855, 291, 577], "temperature": 0.0, "avg_logprob": -0.0771849931672562, "compression_ratio": 1.7135922330097086, "no_speech_prob": 5.014693215343868e-06}, {"id": 704, "seek": 445510, "start": 4470.56, "end": 4481.76, "text": " to get your your pytorch fast AI model up and running in as raw a form as possible so", "tokens": [281, 483, 428, 428, 25878, 284, 339, 2370, 7318, 2316, 493, 293, 2614, 294, 382, 8936, 257, 1254, 382, 1944, 370], "temperature": 0.0, "avg_logprob": -0.0771849931672562, "compression_ratio": 1.7135922330097086, "no_speech_prob": 5.014693215343868e-06}, {"id": 705, "seek": 448176, "start": 4481.76, "end": 4488.68, "text": " that from there you can kind of build on top of it as you like so to do that we are going", "tokens": [300, 490, 456, 291, 393, 733, 295, 1322, 322, 1192, 295, 309, 382, 291, 411, 370, 281, 360, 300, 321, 366, 516], "temperature": 0.0, "avg_logprob": -0.07620475603186566, "compression_ratio": 1.9447513812154695, "no_speech_prob": 6.276690101003624e-07}, {"id": 706, "seek": 448176, "start": 4488.68, "end": 4494.92, "text": " to download and curate our own data set and you're going to do the same thing you've got", "tokens": [281, 5484, 293, 1262, 473, 527, 1065, 1412, 992, 293, 291, 434, 516, 281, 360, 264, 912, 551, 291, 600, 658], "temperature": 0.0, "avg_logprob": -0.07620475603186566, "compression_ratio": 1.9447513812154695, "no_speech_prob": 6.276690101003624e-07}, {"id": 707, "seek": 448176, "start": 4494.92, "end": 4500.8, "text": " to train your own model on that data set and then you're going to create an application", "tokens": [281, 3847, 428, 1065, 2316, 322, 300, 1412, 992, 293, 550, 291, 434, 516, 281, 1884, 364, 3861], "temperature": 0.0, "avg_logprob": -0.07620475603186566, "compression_ratio": 1.9447513812154695, "no_speech_prob": 6.276690101003624e-07}, {"id": 708, "seek": 448176, "start": 4500.8, "end": 4507.4800000000005, "text": " and then you're going to host it right now there's lots of ways to create a curate an", "tokens": [293, 550, 291, 434, 516, 281, 3975, 309, 558, 586, 456, 311, 3195, 295, 2098, 281, 1884, 257, 1262, 473, 364], "temperature": 0.0, "avg_logprob": -0.07620475603186566, "compression_ratio": 1.9447513812154695, "no_speech_prob": 6.276690101003624e-07}, {"id": 709, "seek": 450748, "start": 4507.48, "end": 4512.08, "text": " image data set you might have some photos on your own computer there might be stuff", "tokens": [3256, 1412, 992, 291, 1062, 362, 512, 5787, 322, 428, 1065, 3820, 456, 1062, 312, 1507], "temperature": 0.0, "avg_logprob": -0.09675849310242303, "compression_ratio": 1.8697478991596639, "no_speech_prob": 7.571124456262623e-07}, {"id": 710, "seek": 450748, "start": 4512.08, "end": 4518.5199999999995, "text": " at work you can use one of the easiest though is just to download stuff off the internet", "tokens": [412, 589, 291, 393, 764, 472, 295, 264, 12889, 1673, 307, 445, 281, 5484, 1507, 766, 264, 4705], "temperature": 0.0, "avg_logprob": -0.09675849310242303, "compression_ratio": 1.8697478991596639, "no_speech_prob": 7.571124456262623e-07}, {"id": 711, "seek": 450748, "start": 4518.5199999999995, "end": 4523.5199999999995, "text": " there's lots of services for downloading stuff off the internet we're going to be using Bing", "tokens": [456, 311, 3195, 295, 3328, 337, 32529, 1507, 766, 264, 4705, 321, 434, 516, 281, 312, 1228, 30755], "temperature": 0.0, "avg_logprob": -0.09675849310242303, "compression_ratio": 1.8697478991596639, "no_speech_prob": 7.571124456262623e-07}, {"id": 712, "seek": 450748, "start": 4523.5199999999995, "end": 4529.4, "text": " image search here because they're super easy to use a lot of the other kind of easy to", "tokens": [3256, 3164, 510, 570, 436, 434, 1687, 1858, 281, 764, 257, 688, 295, 264, 661, 733, 295, 1858, 281], "temperature": 0.0, "avg_logprob": -0.09675849310242303, "compression_ratio": 1.8697478991596639, "no_speech_prob": 7.571124456262623e-07}, {"id": 713, "seek": 450748, "start": 4529.4, "end": 4535.799999999999, "text": " use things require breaking the terms of service of websites so like we're not going to show", "tokens": [764, 721, 3651, 7697, 264, 2115, 295, 2643, 295, 12891, 370, 411, 321, 434, 406, 516, 281, 855], "temperature": 0.0, "avg_logprob": -0.09675849310242303, "compression_ratio": 1.8697478991596639, "no_speech_prob": 7.571124456262623e-07}, {"id": 714, "seek": 453580, "start": 4535.8, "end": 4541.6, "text": " you how to do that but there's lots of examples that do show you how to do that so you can", "tokens": [291, 577, 281, 360, 300, 457, 456, 311, 3195, 295, 5110, 300, 360, 855, 291, 577, 281, 360, 300, 370, 291, 393], "temperature": 0.0, "avg_logprob": -0.06746149063110352, "compression_ratio": 1.8846153846153846, "no_speech_prob": 3.500812908896478e-06}, {"id": 715, "seek": 453580, "start": 4541.6, "end": 4545.68, "text": " check them out as well if you if you want to Bing image search is actually pretty great", "tokens": [1520, 552, 484, 382, 731, 498, 291, 498, 291, 528, 281, 30755, 3256, 3164, 307, 767, 1238, 869], "temperature": 0.0, "avg_logprob": -0.06746149063110352, "compression_ratio": 1.8846153846153846, "no_speech_prob": 3.500812908896478e-06}, {"id": 716, "seek": 453580, "start": 4545.68, "end": 4552.02, "text": " at least at the moment these things change a lot so keep an eye on our website to see", "tokens": [412, 1935, 412, 264, 1623, 613, 721, 1319, 257, 688, 370, 1066, 364, 3313, 322, 527, 3144, 281, 536], "temperature": 0.0, "avg_logprob": -0.06746149063110352, "compression_ratio": 1.8846153846153846, "no_speech_prob": 3.500812908896478e-06}, {"id": 717, "seek": 453580, "start": 4552.02, "end": 4558.04, "text": " if we've changed our recommendation the biggest problem with Bing image search is that the", "tokens": [498, 321, 600, 3105, 527, 11879, 264, 3880, 1154, 365, 30755, 3256, 3164, 307, 300, 264], "temperature": 0.0, "avg_logprob": -0.06746149063110352, "compression_ratio": 1.8846153846153846, "no_speech_prob": 3.500812908896478e-06}, {"id": 718, "seek": 453580, "start": 4558.04, "end": 4564.16, "text": " signup process is a nightmare at least at the moment like one of the hardest parts of", "tokens": [1465, 1010, 1399, 307, 257, 18724, 412, 1935, 412, 264, 1623, 411, 472, 295, 264, 13158, 3166, 295], "temperature": 0.0, "avg_logprob": -0.06746149063110352, "compression_ratio": 1.8846153846153846, "no_speech_prob": 3.500812908896478e-06}, {"id": 719, "seek": 456416, "start": 4564.16, "end": 4569.54, "text": " this book is just signing up to their damn API which requires going through Azure it's", "tokens": [341, 1446, 307, 445, 13393, 493, 281, 641, 8151, 9362, 597, 7029, 516, 807, 11969, 309, 311], "temperature": 0.0, "avg_logprob": -0.09204334094200606, "compression_ratio": 1.6851851851851851, "no_speech_prob": 5.453283620227012e-07}, {"id": 720, "seek": 456416, "start": 4569.54, "end": 4574.28, "text": " called cognitive services Azure cognitive services so we'll make sure that all that information", "tokens": [1219, 15605, 3328, 11969, 15605, 3328, 370, 321, 603, 652, 988, 300, 439, 300, 1589], "temperature": 0.0, "avg_logprob": -0.09204334094200606, "compression_ratio": 1.6851851851851851, "no_speech_prob": 5.453283620227012e-07}, {"id": 721, "seek": 456416, "start": 4574.28, "end": 4578.96, "text": " is on the website for you to follow through just how to sign up so we're going to start", "tokens": [307, 322, 264, 3144, 337, 291, 281, 1524, 807, 445, 577, 281, 1465, 493, 370, 321, 434, 516, 281, 722], "temperature": 0.0, "avg_logprob": -0.09204334094200606, "compression_ratio": 1.6851851851851851, "no_speech_prob": 5.453283620227012e-07}, {"id": 722, "seek": 456416, "start": 4578.96, "end": 4589.599999999999, "text": " from the assumption that you've already signed up but you can find it just go Bing Bing image", "tokens": [490, 264, 15302, 300, 291, 600, 1217, 8175, 493, 457, 291, 393, 915, 309, 445, 352, 30755, 30755, 3256], "temperature": 0.0, "avg_logprob": -0.09204334094200606, "compression_ratio": 1.6851851851851851, "no_speech_prob": 5.453283620227012e-07}, {"id": 723, "seek": 458960, "start": 4589.6, "end": 4598.860000000001, "text": " search API and at the moment they give you seven days with a pretty high pretty high", "tokens": [3164, 9362, 293, 412, 264, 1623, 436, 976, 291, 3407, 1708, 365, 257, 1238, 1090, 1238, 1090], "temperature": 0.0, "avg_logprob": -0.08352070319943311, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.3363867878979363e-07}, {"id": 724, "seek": 458960, "start": 4598.860000000001, "end": 4607.52, "text": " quota for free and then after that you can keep using it as long as you like but they", "tokens": [45171, 337, 1737, 293, 550, 934, 300, 291, 393, 1066, 1228, 309, 382, 938, 382, 291, 411, 457, 436], "temperature": 0.0, "avg_logprob": -0.08352070319943311, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.3363867878979363e-07}, {"id": 725, "seek": 458960, "start": 4607.52, "end": 4611.96, "text": " kind of limit it to like three transactions per second or something which is still plenty", "tokens": [733, 295, 4948, 309, 281, 411, 1045, 16856, 680, 1150, 420, 746, 597, 307, 920, 7140], "temperature": 0.0, "avg_logprob": -0.08352070319943311, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.3363867878979363e-07}, {"id": 726, "seek": 458960, "start": 4611.96, "end": 4619.0, "text": " you can still do thousands for free so it's at the moment it's pretty great even for free", "tokens": [291, 393, 920, 360, 5383, 337, 1737, 370, 309, 311, 412, 264, 1623, 309, 311, 1238, 869, 754, 337, 1737], "temperature": 0.0, "avg_logprob": -0.08352070319943311, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.3363867878979363e-07}, {"id": 727, "seek": 461900, "start": 4619.0, "end": 4624.56, "text": " so what will happen is when you sign up for Bing image search or any of these kind of", "tokens": [370, 437, 486, 1051, 307, 562, 291, 1465, 493, 337, 30755, 3256, 3164, 420, 604, 295, 613, 733, 295], "temperature": 0.0, "avg_logprob": -0.09436765246921115, "compression_ratio": 1.6572769953051643, "no_speech_prob": 2.616515644149331e-07}, {"id": 728, "seek": 461900, "start": 4624.56, "end": 4631.2, "text": " services they'll give you an API key so just replace the xxx here with the API key that", "tokens": [3328, 436, 603, 976, 291, 364, 9362, 2141, 370, 445, 7406, 264, 2031, 30569, 510, 365, 264, 9362, 2141, 300], "temperature": 0.0, "avg_logprob": -0.09436765246921115, "compression_ratio": 1.6572769953051643, "no_speech_prob": 2.616515644149331e-07}, {"id": 729, "seek": 461900, "start": 4631.2, "end": 4638.12, "text": " they give you okay so that's now going to be called key in fact let's do it over here", "tokens": [436, 976, 291, 1392, 370, 300, 311, 586, 516, 281, 312, 1219, 2141, 294, 1186, 718, 311, 360, 309, 670, 510], "temperature": 0.0, "avg_logprob": -0.09436765246921115, "compression_ratio": 1.6572769953051643, "no_speech_prob": 2.616515644149331e-07}, {"id": 730, "seek": 461900, "start": 4638.12, "end": 4647.52, "text": " okay so you'll put in your key and then there's a function we've created called search images", "tokens": [1392, 370, 291, 603, 829, 294, 428, 2141, 293, 550, 456, 311, 257, 2445, 321, 600, 2942, 1219, 3164, 5267], "temperature": 0.0, "avg_logprob": -0.09436765246921115, "compression_ratio": 1.6572769953051643, "no_speech_prob": 2.616515644149331e-07}, {"id": 731, "seek": 464752, "start": 4647.52, "end": 4653.92, "text": " being which is just a super tiny little function as you can see it's just two lines of code", "tokens": [885, 597, 307, 445, 257, 1687, 5870, 707, 2445, 382, 291, 393, 536, 309, 311, 445, 732, 3876, 295, 3089], "temperature": 0.0, "avg_logprob": -0.10936631422776442, "compression_ratio": 1.6134969325153374, "no_speech_prob": 5.203560249356087e-07}, {"id": 732, "seek": 464752, "start": 4653.92, "end": 4660.76, "text": " I was just trying to save a little bit of time which will take some take your API key", "tokens": [286, 390, 445, 1382, 281, 3155, 257, 707, 857, 295, 565, 597, 486, 747, 512, 747, 428, 9362, 2141], "temperature": 0.0, "avg_logprob": -0.10936631422776442, "compression_ratio": 1.6134969325153374, "no_speech_prob": 5.203560249356087e-07}, {"id": 733, "seek": 464752, "start": 4660.76, "end": 4667.360000000001, "text": " and some search term and return a list of URLs that match that search term as you can", "tokens": [293, 512, 3164, 1433, 293, 2736, 257, 1329, 295, 43267, 300, 2995, 300, 3164, 1433, 382, 291, 393], "temperature": 0.0, "avg_logprob": -0.10936631422776442, "compression_ratio": 1.6134969325153374, "no_speech_prob": 5.203560249356087e-07}, {"id": 734, "seek": 466736, "start": 4667.36, "end": 4677.639999999999, "text": " see for using this particular service you have to install a particular package so we", "tokens": [536, 337, 1228, 341, 1729, 2643, 291, 362, 281, 3625, 257, 1729, 7372, 370, 321], "temperature": 0.0, "avg_logprob": -0.10276387288020207, "compression_ratio": 1.4804469273743017, "no_speech_prob": 6.681504487460188e-07}, {"id": 735, "seek": 466736, "start": 4677.639999999999, "end": 4682.08, "text": " show you how to do that on the site as well so once you've done so you'll be able to run", "tokens": [855, 291, 577, 281, 360, 300, 322, 264, 3621, 382, 731, 370, 1564, 291, 600, 1096, 370, 291, 603, 312, 1075, 281, 1190], "temperature": 0.0, "avg_logprob": -0.10276387288020207, "compression_ratio": 1.4804469273743017, "no_speech_prob": 6.681504487460188e-07}, {"id": 736, "seek": 466736, "start": 4682.08, "end": 4692.08, "text": " this and that will return by default I think 150 URLs okay so fast AI comes with a download", "tokens": [341, 293, 300, 486, 2736, 538, 7576, 286, 519, 8451, 43267, 1392, 370, 2370, 7318, 1487, 365, 257, 5484], "temperature": 0.0, "avg_logprob": -0.10276387288020207, "compression_ratio": 1.4804469273743017, "no_speech_prob": 6.681504487460188e-07}, {"id": 737, "seek": 469208, "start": 4692.08, "end": 4697.32, "text": " URL function so you let's just download one of those images just to check and open it", "tokens": [12905, 2445, 370, 291, 718, 311, 445, 5484, 472, 295, 729, 5267, 445, 281, 1520, 293, 1269, 309], "temperature": 0.0, "avg_logprob": -0.06432465796775007, "compression_ratio": 1.7488151658767772, "no_speech_prob": 7.224428486551915e-07}, {"id": 738, "seek": 469208, "start": 4697.32, "end": 4704.84, "text": " up and so what I did was I searched for grizzly bear and here I have a grizzly bear so then", "tokens": [493, 293, 370, 437, 286, 630, 390, 286, 22961, 337, 17865, 4313, 356, 6155, 293, 510, 286, 362, 257, 17865, 4313, 356, 6155, 370, 550], "temperature": 0.0, "avg_logprob": -0.06432465796775007, "compression_ratio": 1.7488151658767772, "no_speech_prob": 7.224428486551915e-07}, {"id": 739, "seek": 469208, "start": 4704.84, "end": 4710.68, "text": " what I did was I said okay let's try and create a model that can recognize grizzly bears versus", "tokens": [437, 286, 630, 390, 286, 848, 1392, 718, 311, 853, 293, 1884, 257, 2316, 300, 393, 5521, 17865, 4313, 356, 17276, 5717], "temperature": 0.0, "avg_logprob": -0.06432465796775007, "compression_ratio": 1.7488151658767772, "no_speech_prob": 7.224428486551915e-07}, {"id": 740, "seek": 469208, "start": 4710.68, "end": 4718.76, "text": " black bears versus teddy bears so that way I can find out I could set up some video recognition", "tokens": [2211, 17276, 5717, 45116, 17276, 370, 300, 636, 286, 393, 915, 484, 286, 727, 992, 493, 512, 960, 11150], "temperature": 0.0, "avg_logprob": -0.06432465796775007, "compression_ratio": 1.7488151658767772, "no_speech_prob": 7.224428486551915e-07}, {"id": 741, "seek": 471876, "start": 4718.76, "end": 4724.400000000001, "text": " system near our campsite when we're out camping that gives me bear warnings but if it's a", "tokens": [1185, 2651, 527, 16573, 642, 562, 321, 434, 484, 19470, 300, 2709, 385, 6155, 30009, 457, 498, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.06419263884078624, "compression_ratio": 1.6744186046511629, "no_speech_prob": 2.496697391052294e-07}, {"id": 742, "seek": 471876, "start": 4724.400000000001, "end": 4728.320000000001, "text": " teddy bear coming then it doesn't warn me and wake me up because that would not be scary", "tokens": [45116, 6155, 1348, 550, 309, 1177, 380, 12286, 385, 293, 6634, 385, 493, 570, 300, 576, 406, 312, 6958], "temperature": 0.0, "avg_logprob": -0.06419263884078624, "compression_ratio": 1.6744186046511629, "no_speech_prob": 2.496697391052294e-07}, {"id": 743, "seek": 471876, "start": 4728.320000000001, "end": 4735.56, "text": " at all so then I just go through each of those three bear types create a directory with the", "tokens": [412, 439, 370, 550, 286, 445, 352, 807, 1184, 295, 729, 1045, 6155, 3467, 1884, 257, 21120, 365, 264], "temperature": 0.0, "avg_logprob": -0.06419263884078624, "compression_ratio": 1.6744186046511629, "no_speech_prob": 2.496697391052294e-07}, {"id": 744, "seek": 471876, "start": 4735.56, "end": 4743.64, "text": " name of grizzly or black or teddy bear search being for that particular search term along", "tokens": [1315, 295, 17865, 4313, 356, 420, 2211, 420, 45116, 6155, 3164, 885, 337, 300, 1729, 3164, 1433, 2051], "temperature": 0.0, "avg_logprob": -0.06419263884078624, "compression_ratio": 1.6744186046511629, "no_speech_prob": 2.496697391052294e-07}, {"id": 745, "seek": 474364, "start": 4743.64, "end": 4751.92, "text": " with bear and download and so download images is a fast AI function as well so after that", "tokens": [365, 6155, 293, 5484, 293, 370, 5484, 5267, 307, 257, 2370, 7318, 2445, 382, 731, 370, 934, 300], "temperature": 0.0, "avg_logprob": -0.06963986228494083, "compression_ratio": 1.7524752475247525, "no_speech_prob": 5.368738129618578e-07}, {"id": 746, "seek": 474364, "start": 4751.92, "end": 4757.700000000001, "text": " I can call get image files which is a fast AI function that will just return recursively", "tokens": [286, 393, 818, 483, 3256, 7098, 597, 307, 257, 2370, 7318, 2445, 300, 486, 445, 2736, 20560, 3413], "temperature": 0.0, "avg_logprob": -0.06963986228494083, "compression_ratio": 1.7524752475247525, "no_speech_prob": 5.368738129618578e-07}, {"id": 747, "seek": 474364, "start": 4757.700000000001, "end": 4763.08, "text": " all of the image files inside this path and you can see it's given me bears slash black", "tokens": [439, 295, 264, 3256, 7098, 1854, 341, 3100, 293, 291, 393, 536, 309, 311, 2212, 385, 17276, 17330, 2211], "temperature": 0.0, "avg_logprob": -0.06963986228494083, "compression_ratio": 1.7524752475247525, "no_speech_prob": 5.368738129618578e-07}, {"id": 748, "seek": 474364, "start": 4763.08, "end": 4771.12, "text": " slash and then lots of numbers so one of the things you have to be careful of is that a", "tokens": [17330, 293, 550, 3195, 295, 3547, 370, 472, 295, 264, 721, 291, 362, 281, 312, 5026, 295, 307, 300, 257], "temperature": 0.0, "avg_logprob": -0.06963986228494083, "compression_ratio": 1.7524752475247525, "no_speech_prob": 5.368738129618578e-07}, {"id": 749, "seek": 477112, "start": 4771.12, "end": 4775.599999999999, "text": " lot of the stuff you download will turn out to be like not images at all and will break", "tokens": [688, 295, 264, 1507, 291, 5484, 486, 1261, 484, 281, 312, 411, 406, 5267, 412, 439, 293, 486, 1821], "temperature": 0.0, "avg_logprob": -0.10855239232381185, "compression_ratio": 1.680952380952381, "no_speech_prob": 5.399494185098774e-08}, {"id": 750, "seek": 477112, "start": 4775.599999999999, "end": 4783.5599999999995, "text": " so you can call verify images to check that all of these file names are actual images", "tokens": [370, 291, 393, 818, 16888, 5267, 281, 1520, 300, 439, 295, 613, 3991, 5288, 366, 3539, 5267], "temperature": 0.0, "avg_logprob": -0.10855239232381185, "compression_ratio": 1.680952380952381, "no_speech_prob": 5.399494185098774e-08}, {"id": 751, "seek": 477112, "start": 4783.5599999999995, "end": 4789.88, "text": " and in this case I didn't have any failed so this it's empty but if you did have some", "tokens": [293, 294, 341, 1389, 286, 994, 380, 362, 604, 7612, 370, 341, 309, 311, 6707, 457, 498, 291, 630, 362, 512], "temperature": 0.0, "avg_logprob": -0.10855239232381185, "compression_ratio": 1.680952380952381, "no_speech_prob": 5.399494185098774e-08}, {"id": 752, "seek": 477112, "start": 4789.88, "end": 4797.12, "text": " then you would call path.unlink.unlink path.unlink is part of the Python standard library and", "tokens": [550, 291, 576, 818, 3100, 13, 409, 22473, 13, 409, 22473, 3100, 13, 409, 22473, 307, 644, 295, 264, 15329, 3832, 6405, 293], "temperature": 0.0, "avg_logprob": -0.10855239232381185, "compression_ratio": 1.680952380952381, "no_speech_prob": 5.399494185098774e-08}, {"id": 753, "seek": 479712, "start": 4797.12, "end": 4804.599999999999, "text": " it deletes a file and map is something that we'll call this function for every element", "tokens": [309, 1103, 37996, 257, 3991, 293, 4471, 307, 746, 300, 321, 603, 818, 341, 2445, 337, 633, 4478], "temperature": 0.0, "avg_logprob": -0.10019075302850633, "compression_ratio": 1.5963302752293578, "no_speech_prob": 3.927857790131384e-07}, {"id": 754, "seek": 479712, "start": 4804.599999999999, "end": 4814.72, "text": " of this collection. This is part of a special fast AI class called L it's basically it's", "tokens": [295, 341, 5765, 13, 639, 307, 644, 295, 257, 2121, 2370, 7318, 1508, 1219, 441, 309, 311, 1936, 309, 311], "temperature": 0.0, "avg_logprob": -0.10019075302850633, "compression_ratio": 1.5963302752293578, "no_speech_prob": 3.927857790131384e-07}, {"id": 755, "seek": 479712, "start": 4814.72, "end": 4821.16, "text": " kind of a mix between the Python standard library list class and a NumPy array class", "tokens": [733, 295, 257, 2890, 1296, 264, 15329, 3832, 6405, 1329, 1508, 293, 257, 22592, 47, 88, 10225, 1508], "temperature": 0.0, "avg_logprob": -0.10019075302850633, "compression_ratio": 1.5963302752293578, "no_speech_prob": 3.927857790131384e-07}, {"id": 756, "seek": 479712, "start": 4821.16, "end": 4825.16, "text": " and we'll be learning more about it later in this course but it basically tries to make", "tokens": [293, 321, 603, 312, 2539, 544, 466, 309, 1780, 294, 341, 1164, 457, 309, 1936, 9898, 281, 652], "temperature": 0.0, "avg_logprob": -0.10019075302850633, "compression_ratio": 1.5963302752293578, "no_speech_prob": 3.927857790131384e-07}, {"id": 757, "seek": 482516, "start": 4825.16, "end": 4831.76, "text": " it super easy to do kind of more functional style programming in Python. So in this case", "tokens": [309, 1687, 1858, 281, 360, 733, 295, 544, 11745, 3758, 9410, 294, 15329, 13, 407, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.16728881510292612, "compression_ratio": 1.5466666666666666, "no_speech_prob": 2.419884026494401e-07}, {"id": 758, "seek": 482516, "start": 4831.76, "end": 4837.12, "text": " it's going to unlink everything that's in the failed list which is probably what we", "tokens": [309, 311, 516, 281, 517, 22473, 1203, 300, 311, 294, 264, 7612, 1329, 597, 307, 1391, 437, 321], "temperature": 0.0, "avg_logprob": -0.16728881510292612, "compression_ratio": 1.5466666666666666, "no_speech_prob": 2.419884026494401e-07}, {"id": 759, "seek": 482516, "start": 4837.12, "end": 4843.08, "text": " want because there are all the images that failed to verify. All right so we've now got", "tokens": [528, 570, 456, 366, 439, 264, 5267, 300, 7612, 281, 16888, 13, 1057, 558, 370, 321, 600, 586, 658], "temperature": 0.0, "avg_logprob": -0.16728881510292612, "compression_ratio": 1.5466666666666666, "no_speech_prob": 2.419884026494401e-07}, {"id": 760, "seek": 482516, "start": 4843.08, "end": 4851.4, "text": " a path that contains a whole bunch of images and they're classified according to black,", "tokens": [257, 3100, 300, 8306, 257, 1379, 3840, 295, 5267, 293, 436, 434, 20627, 4650, 281, 2211, 11], "temperature": 0.0, "avg_logprob": -0.16728881510292612, "compression_ratio": 1.5466666666666666, "no_speech_prob": 2.419884026494401e-07}, {"id": 761, "seek": 485140, "start": 4851.4, "end": 4856.4, "text": " grizzly or teddy based on what folder they're in and so to create so we're going to create", "tokens": [17865, 4313, 356, 420, 45116, 2361, 322, 437, 10820, 436, 434, 294, 293, 370, 281, 1884, 370, 321, 434, 516, 281, 1884], "temperature": 0.0, "avg_logprob": -0.10743779324470683, "compression_ratio": 1.646788990825688, "no_speech_prob": 4.181184465323895e-07}, {"id": 762, "seek": 485140, "start": 4856.4, "end": 4864.839999999999, "text": " a model and so to create a model the first thing we need to do is to tell fast AI what", "tokens": [257, 2316, 293, 370, 281, 1884, 257, 2316, 264, 700, 551, 321, 643, 281, 360, 307, 281, 980, 2370, 7318, 437], "temperature": 0.0, "avg_logprob": -0.10743779324470683, "compression_ratio": 1.646788990825688, "no_speech_prob": 4.181184465323895e-07}, {"id": 763, "seek": 485140, "start": 4864.839999999999, "end": 4872.5599999999995, "text": " kind of data we have and how it's structured. Now in part in lesson one of the course we", "tokens": [733, 295, 1412, 321, 362, 293, 577, 309, 311, 18519, 13, 823, 294, 644, 294, 6898, 472, 295, 264, 1164, 321], "temperature": 0.0, "avg_logprob": -0.10743779324470683, "compression_ratio": 1.646788990825688, "no_speech_prob": 4.181184465323895e-07}, {"id": 764, "seek": 485140, "start": 4872.5599999999995, "end": 4879.599999999999, "text": " did that by using what we call a factory method which is we just said image data loaders dot", "tokens": [630, 300, 538, 1228, 437, 321, 818, 257, 9265, 3170, 597, 307, 321, 445, 848, 3256, 1412, 3677, 433, 5893], "temperature": 0.0, "avg_logprob": -0.10743779324470683, "compression_ratio": 1.646788990825688, "no_speech_prob": 4.181184465323895e-07}, {"id": 765, "seek": 487960, "start": 4879.6, "end": 4887.96, "text": " from name and it did it all for us. Those factory methods are fine for beginners but", "tokens": [490, 1315, 293, 309, 630, 309, 439, 337, 505, 13, 3950, 9265, 7150, 366, 2489, 337, 26992, 457], "temperature": 0.0, "avg_logprob": -0.08645951558673193, "compression_ratio": 1.5568862275449102, "no_speech_prob": 6.37553625892906e-07}, {"id": 766, "seek": 487960, "start": 4887.96, "end": 4891.280000000001, "text": " now we're into lesson two we're not quite beginners anymore so we're going to show you", "tokens": [586, 321, 434, 666, 6898, 732, 321, 434, 406, 1596, 26992, 3602, 370, 321, 434, 516, 281, 855, 291], "temperature": 0.0, "avg_logprob": -0.08645951558673193, "compression_ratio": 1.5568862275449102, "no_speech_prob": 6.37553625892906e-07}, {"id": 767, "seek": 487960, "start": 4891.280000000001, "end": 4897.120000000001, "text": " the super super flexible way to use data in whatever format you like and it's called the", "tokens": [264, 1687, 1687, 11358, 636, 281, 764, 1412, 294, 2035, 7877, 291, 411, 293, 309, 311, 1219, 264], "temperature": 0.0, "avg_logprob": -0.08645951558673193, "compression_ratio": 1.5568862275449102, "no_speech_prob": 6.37553625892906e-07}, {"id": 768, "seek": 489712, "start": 4897.12, "end": 4910.32, "text": " data block API and so the data block API looks like this. Here's the data block API. You", "tokens": [1412, 3461, 9362, 293, 370, 264, 1412, 3461, 9362, 1542, 411, 341, 13, 1692, 311, 264, 1412, 3461, 9362, 13, 509], "temperature": 0.0, "avg_logprob": -0.13191783238971044, "compression_ratio": 1.8450704225352113, "no_speech_prob": 4.381840312817076e-07}, {"id": 769, "seek": 489712, "start": 4910.32, "end": 4915.8, "text": " tell fast AI what your independent variable is and what your dependent variable is so", "tokens": [980, 2370, 7318, 437, 428, 6695, 7006, 307, 293, 437, 428, 12334, 7006, 307, 370], "temperature": 0.0, "avg_logprob": -0.13191783238971044, "compression_ratio": 1.8450704225352113, "no_speech_prob": 4.381840312817076e-07}, {"id": 770, "seek": 489712, "start": 4915.8, "end": 4923.76, "text": " what your labels are and what your input data is. So in this case our input data images", "tokens": [437, 428, 16949, 366, 293, 437, 428, 4846, 1412, 307, 13, 407, 294, 341, 1389, 527, 4846, 1412, 5267], "temperature": 0.0, "avg_logprob": -0.13191783238971044, "compression_ratio": 1.8450704225352113, "no_speech_prob": 4.381840312817076e-07}, {"id": 771, "seek": 492376, "start": 4923.76, "end": 4929.84, "text": " and our labels are categories so category is going to be either grizzly or black or", "tokens": [293, 527, 16949, 366, 10479, 370, 7719, 307, 516, 281, 312, 2139, 17865, 4313, 356, 420, 2211, 420], "temperature": 0.0, "avg_logprob": -0.10812171000354695, "compression_ratio": 1.920353982300885, "no_speech_prob": 2.8291310627537314e-07}, {"id": 772, "seek": 492376, "start": 4929.84, "end": 4935.52, "text": " teddy so that's the first thing you tell it that that's the block's parameter and then", "tokens": [45116, 370, 300, 311, 264, 700, 551, 291, 980, 309, 300, 300, 311, 264, 3461, 311, 13075, 293, 550], "temperature": 0.0, "avg_logprob": -0.10812171000354695, "compression_ratio": 1.920353982300885, "no_speech_prob": 2.8291310627537314e-07}, {"id": 773, "seek": 492376, "start": 4935.52, "end": 4940.56, "text": " you tell it how do you get a list of all of the in this case file names right and we just", "tokens": [291, 980, 309, 577, 360, 291, 483, 257, 1329, 295, 439, 295, 264, 294, 341, 1389, 3991, 5288, 558, 293, 321, 445], "temperature": 0.0, "avg_logprob": -0.10812171000354695, "compression_ratio": 1.920353982300885, "no_speech_prob": 2.8291310627537314e-07}, {"id": 774, "seek": 492376, "start": 4940.56, "end": 4944.04, "text": " saw how to do that because we just called the function ourselves the function is called", "tokens": [1866, 577, 281, 360, 300, 570, 321, 445, 1219, 264, 2445, 4175, 264, 2445, 307, 1219], "temperature": 0.0, "avg_logprob": -0.10812171000354695, "compression_ratio": 1.920353982300885, "no_speech_prob": 2.8291310627537314e-07}, {"id": 775, "seek": 492376, "start": 4944.04, "end": 4949.64, "text": " get image files so we tell it what function to use to get that list of items and then", "tokens": [483, 3256, 7098, 370, 321, 980, 309, 437, 2445, 281, 764, 281, 483, 300, 1329, 295, 4754, 293, 550], "temperature": 0.0, "avg_logprob": -0.10812171000354695, "compression_ratio": 1.920353982300885, "no_speech_prob": 2.8291310627537314e-07}, {"id": 776, "seek": 494964, "start": 4949.64, "end": 4956.12, "text": " you tell it how do you split the data into a validation set and a training set and so", "tokens": [291, 980, 309, 577, 360, 291, 7472, 264, 1412, 666, 257, 24071, 992, 293, 257, 3097, 992, 293, 370], "temperature": 0.0, "avg_logprob": -0.09373934023848204, "compression_ratio": 1.9217391304347826, "no_speech_prob": 2.964902421354054e-07}, {"id": 777, "seek": 494964, "start": 4956.12, "end": 4959.56, "text": " we're going to use something called a random splitter which just splits it randomly and", "tokens": [321, 434, 516, 281, 764, 746, 1219, 257, 4974, 4732, 3904, 597, 445, 37741, 309, 16979, 293], "temperature": 0.0, "avg_logprob": -0.09373934023848204, "compression_ratio": 1.9217391304347826, "no_speech_prob": 2.964902421354054e-07}, {"id": 778, "seek": 494964, "start": 4959.56, "end": 4964.84, "text": " we're going to put 30% of it into the validation set. We're also going to set the random seed", "tokens": [321, 434, 516, 281, 829, 2217, 4, 295, 309, 666, 264, 24071, 992, 13, 492, 434, 611, 516, 281, 992, 264, 4974, 8871], "temperature": 0.0, "avg_logprob": -0.09373934023848204, "compression_ratio": 1.9217391304347826, "no_speech_prob": 2.964902421354054e-07}, {"id": 779, "seek": 494964, "start": 4964.84, "end": 4970.280000000001, "text": " which ensures that every time we run this the validation set will be the same and then", "tokens": [597, 28111, 300, 633, 565, 321, 1190, 341, 264, 24071, 992, 486, 312, 264, 912, 293, 550], "temperature": 0.0, "avg_logprob": -0.09373934023848204, "compression_ratio": 1.9217391304347826, "no_speech_prob": 2.964902421354054e-07}, {"id": 780, "seek": 494964, "start": 4970.280000000001, "end": 4975.200000000001, "text": " you say okay how do you label the data and this is the name of a function called parent", "tokens": [291, 584, 1392, 577, 360, 291, 7645, 264, 1412, 293, 341, 307, 264, 1315, 295, 257, 2445, 1219, 2596], "temperature": 0.0, "avg_logprob": -0.09373934023848204, "compression_ratio": 1.9217391304347826, "no_speech_prob": 2.964902421354054e-07}, {"id": 781, "seek": 497520, "start": 4975.2, "end": 4983.12, "text": " label and so that's going to look for each item at the name of the parent so this this", "tokens": [7645, 293, 370, 300, 311, 516, 281, 574, 337, 1184, 3174, 412, 264, 1315, 295, 264, 2596, 370, 341, 341], "temperature": 0.0, "avg_logprob": -0.07744120588206281, "compression_ratio": 1.8, "no_speech_prob": 2.4337433401910857e-08}, {"id": 782, "seek": 497520, "start": 4983.12, "end": 4989.96, "text": " particular one would become a black bear and this is like the most common way for image", "tokens": [1729, 472, 576, 1813, 257, 2211, 6155, 293, 341, 307, 411, 264, 881, 2689, 636, 337, 3256], "temperature": 0.0, "avg_logprob": -0.07744120588206281, "compression_ratio": 1.8, "no_speech_prob": 2.4337433401910857e-08}, {"id": 783, "seek": 497520, "start": 4989.96, "end": 4994.36, "text": " data sets to be represented is that they get put the different images get the files get", "tokens": [1412, 6352, 281, 312, 10379, 307, 300, 436, 483, 829, 264, 819, 5267, 483, 264, 7098, 483], "temperature": 0.0, "avg_logprob": -0.07744120588206281, "compression_ratio": 1.8, "no_speech_prob": 2.4337433401910857e-08}, {"id": 784, "seek": 497520, "start": 4994.36, "end": 5000.5599999999995, "text": " put into folder according to their label and then finally here we've got something called", "tokens": [829, 666, 10820, 4650, 281, 641, 7645, 293, 550, 2721, 510, 321, 600, 658, 746, 1219], "temperature": 0.0, "avg_logprob": -0.07744120588206281, "compression_ratio": 1.8, "no_speech_prob": 2.4337433401910857e-08}, {"id": 785, "seek": 497520, "start": 5000.5599999999995, "end": 5004.92, "text": " item transforms we'll be learning a lot more about transforms in a moment that these are", "tokens": [3174, 35592, 321, 603, 312, 2539, 257, 688, 544, 466, 35592, 294, 257, 1623, 300, 613, 366], "temperature": 0.0, "avg_logprob": -0.07744120588206281, "compression_ratio": 1.8, "no_speech_prob": 2.4337433401910857e-08}, {"id": 786, "seek": 500492, "start": 5004.92, "end": 5010.9400000000005, "text": " basically functions that get applied to each image and so each image is going to be resized", "tokens": [1936, 6828, 300, 483, 6456, 281, 1184, 3256, 293, 370, 1184, 3256, 307, 516, 281, 312, 725, 1602], "temperature": 0.0, "avg_logprob": -0.09471035840218528, "compression_ratio": 1.8680851063829786, "no_speech_prob": 1.107904239461277e-07}, {"id": 787, "seek": 500492, "start": 5010.9400000000005, "end": 5019.68, "text": " to 128 by 128 square. So we're going to be learning more about data block API soon but", "tokens": [281, 29810, 538, 29810, 3732, 13, 407, 321, 434, 516, 281, 312, 2539, 544, 466, 1412, 3461, 9362, 2321, 457], "temperature": 0.0, "avg_logprob": -0.09471035840218528, "compression_ratio": 1.8680851063829786, "no_speech_prob": 1.107904239461277e-07}, {"id": 788, "seek": 500492, "start": 5019.68, "end": 5023.28, "text": " basically the process is going to be it's going to call whatever is get items which", "tokens": [1936, 264, 1399, 307, 516, 281, 312, 309, 311, 516, 281, 818, 2035, 307, 483, 4754, 597], "temperature": 0.0, "avg_logprob": -0.09471035840218528, "compression_ratio": 1.8680851063829786, "no_speech_prob": 1.107904239461277e-07}, {"id": 789, "seek": 500492, "start": 5023.28, "end": 5028.92, "text": " is a list of image files it's then I'm going to call get x get y so in this case there's", "tokens": [307, 257, 1329, 295, 3256, 7098, 309, 311, 550, 286, 478, 516, 281, 818, 483, 2031, 483, 288, 370, 294, 341, 1389, 456, 311], "temperature": 0.0, "avg_logprob": -0.09471035840218528, "compression_ratio": 1.8680851063829786, "no_speech_prob": 1.107904239461277e-07}, {"id": 790, "seek": 500492, "start": 5028.92, "end": 5033.6, "text": " no get x but there is a get y so it's just parent label and then it's going to call the", "tokens": [572, 483, 2031, 457, 456, 307, 257, 483, 288, 370, 309, 311, 445, 2596, 7645, 293, 550, 309, 311, 516, 281, 818, 264], "temperature": 0.0, "avg_logprob": -0.09471035840218528, "compression_ratio": 1.8680851063829786, "no_speech_prob": 1.107904239461277e-07}, {"id": 791, "seek": 503360, "start": 5033.6, "end": 5037.160000000001, "text": " create method for each of these two things it's going to create an image and it's going", "tokens": [1884, 3170, 337, 1184, 295, 613, 732, 721, 309, 311, 516, 281, 1884, 364, 3256, 293, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.09660581784827688, "compression_ratio": 1.9126637554585153, "no_speech_prob": 8.186351010408544e-07}, {"id": 792, "seek": 503360, "start": 5037.160000000001, "end": 5043.64, "text": " to create a category it's then going to call the item transforms which is resize and then", "tokens": [281, 1884, 257, 7719, 309, 311, 550, 516, 281, 818, 264, 3174, 35592, 597, 307, 50069, 293, 550], "temperature": 0.0, "avg_logprob": -0.09660581784827688, "compression_ratio": 1.9126637554585153, "no_speech_prob": 8.186351010408544e-07}, {"id": 793, "seek": 503360, "start": 5043.64, "end": 5047.68, "text": " the next thing it does is it puts it into something called a data loader a data loader", "tokens": [264, 958, 551, 309, 775, 307, 309, 8137, 309, 666, 746, 1219, 257, 1412, 3677, 260, 257, 1412, 3677, 260], "temperature": 0.0, "avg_logprob": -0.09660581784827688, "compression_ratio": 1.9126637554585153, "no_speech_prob": 8.186351010408544e-07}, {"id": 794, "seek": 503360, "start": 5047.68, "end": 5054.160000000001, "text": " is something that grabs a few images at a time I think by default at 64 and puts them", "tokens": [307, 746, 300, 30028, 257, 1326, 5267, 412, 257, 565, 286, 519, 538, 7576, 412, 12145, 293, 8137, 552], "temperature": 0.0, "avg_logprob": -0.09660581784827688, "compression_ratio": 1.9126637554585153, "no_speech_prob": 8.186351010408544e-07}, {"id": 795, "seek": 503360, "start": 5054.160000000001, "end": 5059.88, "text": " all into a single it's got a batch it just grabs 64 images and sticks them all together", "tokens": [439, 666, 257, 2167, 309, 311, 658, 257, 15245, 309, 445, 30028, 12145, 5267, 293, 12518, 552, 439, 1214], "temperature": 0.0, "avg_logprob": -0.09660581784827688, "compression_ratio": 1.9126637554585153, "no_speech_prob": 8.186351010408544e-07}, {"id": 796, "seek": 505988, "start": 5059.88, "end": 5065.64, "text": " and the reason it does that is it's then puts them all onto the GPU at once so it can pass", "tokens": [293, 264, 1778, 309, 775, 300, 307, 309, 311, 550, 8137, 552, 439, 3911, 264, 18407, 412, 1564, 370, 309, 393, 1320], "temperature": 0.0, "avg_logprob": -0.058914202735537574, "compression_ratio": 1.8361344537815125, "no_speech_prob": 3.3075886562983214e-07}, {"id": 797, "seek": 505988, "start": 5065.64, "end": 5072.0, "text": " them all to the model through the GPU in one go and that's going to let the GPU go much", "tokens": [552, 439, 281, 264, 2316, 807, 264, 18407, 294, 472, 352, 293, 300, 311, 516, 281, 718, 264, 18407, 352, 709], "temperature": 0.0, "avg_logprob": -0.058914202735537574, "compression_ratio": 1.8361344537815125, "no_speech_prob": 3.3075886562983214e-07}, {"id": 798, "seek": 505988, "start": 5072.0, "end": 5078.04, "text": " faster as we'll be learning about and then finally we don't use any here we can have", "tokens": [4663, 382, 321, 603, 312, 2539, 466, 293, 550, 2721, 321, 500, 380, 764, 604, 510, 321, 393, 362], "temperature": 0.0, "avg_logprob": -0.058914202735537574, "compression_ratio": 1.8361344537815125, "no_speech_prob": 3.3075886562983214e-07}, {"id": 799, "seek": 505988, "start": 5078.04, "end": 5082.92, "text": " something called batch transforms which we will talk about later and then somewhere in", "tokens": [746, 1219, 15245, 35592, 597, 321, 486, 751, 466, 1780, 293, 550, 4079, 294], "temperature": 0.0, "avg_logprob": -0.058914202735537574, "compression_ratio": 1.8361344537815125, "no_speech_prob": 3.3075886562983214e-07}, {"id": 800, "seek": 505988, "start": 5082.92, "end": 5088.36, "text": " the middle about here conceptually is the splitter which is the thing that splits into", "tokens": [264, 2808, 466, 510, 3410, 671, 307, 264, 4732, 3904, 597, 307, 264, 551, 300, 37741, 666], "temperature": 0.0, "avg_logprob": -0.058914202735537574, "compression_ratio": 1.8361344537815125, "no_speech_prob": 3.3075886562983214e-07}, {"id": 801, "seek": 508836, "start": 5088.36, "end": 5097.0, "text": " the training set and the validation set so this is a super flexible way to tell fastai", "tokens": [264, 3097, 992, 293, 264, 24071, 992, 370, 341, 307, 257, 1687, 11358, 636, 281, 980, 2370, 1301], "temperature": 0.0, "avg_logprob": -0.11632016572085294, "compression_ratio": 1.76, "no_speech_prob": 3.20582472568276e-07}, {"id": 802, "seek": 508836, "start": 5097.0, "end": 5105.12, "text": " how to work with your data and so at the end of that it returns an object of type data", "tokens": [577, 281, 589, 365, 428, 1412, 293, 370, 412, 264, 917, 295, 300, 309, 11247, 364, 2657, 295, 2010, 1412], "temperature": 0.0, "avg_logprob": -0.11632016572085294, "compression_ratio": 1.76, "no_speech_prob": 3.20582472568276e-07}, {"id": 803, "seek": 508836, "start": 5105.12, "end": 5112.12, "text": " loaders that's why we always call these things DLs right so data loaders has a validation", "tokens": [3677, 433, 300, 311, 983, 321, 1009, 818, 613, 721, 413, 43, 82, 558, 370, 1412, 3677, 433, 575, 257, 24071], "temperature": 0.0, "avg_logprob": -0.11632016572085294, "compression_ratio": 1.76, "no_speech_prob": 3.20582472568276e-07}, {"id": 804, "seek": 508836, "start": 5112.12, "end": 5117.28, "text": " and a training data loader and a data loader as I just mentioned is something that grabs", "tokens": [293, 257, 3097, 1412, 3677, 260, 293, 257, 1412, 3677, 260, 382, 286, 445, 2835, 307, 746, 300, 30028], "temperature": 0.0, "avg_logprob": -0.11632016572085294, "compression_ratio": 1.76, "no_speech_prob": 3.20582472568276e-07}, {"id": 805, "seek": 511728, "start": 5117.28, "end": 5124.12, "text": " a batch of a few items at a time and puts it on the GPU for you so this is basically", "tokens": [257, 15245, 295, 257, 1326, 4754, 412, 257, 565, 293, 8137, 309, 322, 264, 18407, 337, 291, 370, 341, 307, 1936], "temperature": 0.0, "avg_logprob": -0.10126103196188668, "compression_ratio": 1.7027027027027026, "no_speech_prob": 2.699575532005838e-07}, {"id": 806, "seek": 511728, "start": 5124.12, "end": 5130.639999999999, "text": " the entire code of data loaders so the details don't matter I just wanted to point out that", "tokens": [264, 2302, 3089, 295, 1412, 3677, 433, 370, 264, 4365, 500, 380, 1871, 286, 445, 1415, 281, 935, 484, 300], "temperature": 0.0, "avg_logprob": -0.10126103196188668, "compression_ratio": 1.7027027027027026, "no_speech_prob": 2.699575532005838e-07}, {"id": 807, "seek": 511728, "start": 5130.639999999999, "end": 5135.16, "text": " like a lot of these concepts in fastai when you actually look at what they are they're", "tokens": [411, 257, 688, 295, 613, 10392, 294, 2370, 1301, 562, 291, 767, 574, 412, 437, 436, 366, 436, 434], "temperature": 0.0, "avg_logprob": -0.10126103196188668, "compression_ratio": 1.7027027027027026, "no_speech_prob": 2.699575532005838e-07}, {"id": 808, "seek": 511728, "start": 5135.16, "end": 5139.8, "text": " incredibly simple little things it's literally something that you just pass in a few data", "tokens": [6252, 2199, 707, 721, 309, 311, 3736, 746, 300, 291, 445, 1320, 294, 257, 1326, 1412], "temperature": 0.0, "avg_logprob": -0.10126103196188668, "compression_ratio": 1.7027027027027026, "no_speech_prob": 2.699575532005838e-07}, {"id": 809, "seek": 511728, "start": 5139.8, "end": 5144.599999999999, "text": " loaders to and it's still a similar attribute and pass and gives you the first one back", "tokens": [3677, 433, 281, 293, 309, 311, 920, 257, 2531, 19667, 293, 1320, 293, 2709, 291, 264, 700, 472, 646], "temperature": 0.0, "avg_logprob": -0.10126103196188668, "compression_ratio": 1.7027027027027026, "no_speech_prob": 2.699575532005838e-07}, {"id": 810, "seek": 514460, "start": 5144.6, "end": 5156.400000000001, "text": " as dot train and the second one back as dot valid so we can create our data loaders by", "tokens": [382, 5893, 3847, 293, 264, 1150, 472, 646, 382, 5893, 7363, 370, 321, 393, 1884, 527, 1412, 3677, 433, 538], "temperature": 0.0, "avg_logprob": -0.08366833044135052, "compression_ratio": 1.8385416666666667, "no_speech_prob": 5.203564228395408e-07}, {"id": 811, "seek": 514460, "start": 5156.400000000001, "end": 5161.240000000001, "text": " first of all creating the data block and then we call the data loaders passing in our path", "tokens": [700, 295, 439, 4084, 264, 1412, 3461, 293, 550, 321, 818, 264, 1412, 3677, 433, 8437, 294, 527, 3100], "temperature": 0.0, "avg_logprob": -0.08366833044135052, "compression_ratio": 1.8385416666666667, "no_speech_prob": 5.203564228395408e-07}, {"id": 812, "seek": 514460, "start": 5161.240000000001, "end": 5166.240000000001, "text": " to create DLs and then you can call show batch on that you can call show batch on pretty", "tokens": [281, 1884, 413, 43, 82, 293, 550, 291, 393, 818, 855, 15245, 322, 300, 291, 393, 818, 855, 15245, 322, 1238], "temperature": 0.0, "avg_logprob": -0.08366833044135052, "compression_ratio": 1.8385416666666667, "no_speech_prob": 5.203564228395408e-07}, {"id": 813, "seek": 514460, "start": 5166.240000000001, "end": 5170.72, "text": " much anything in fastai to see your data and look we've got some grizzlies we've got a", "tokens": [709, 1340, 294, 2370, 1301, 281, 536, 428, 1412, 293, 574, 321, 600, 658, 512, 17865, 4313, 24119, 321, 600, 658, 257], "temperature": 0.0, "avg_logprob": -0.08366833044135052, "compression_ratio": 1.8385416666666667, "no_speech_prob": 5.203564228395408e-07}, {"id": 814, "seek": 517072, "start": 5170.72, "end": 5180.4800000000005, "text": " teddy we've got a grizzly so you get the idea right I'm going to look at these different", "tokens": [45116, 321, 600, 658, 257, 17865, 4313, 356, 370, 291, 483, 264, 1558, 558, 286, 478, 516, 281, 574, 412, 613, 819], "temperature": 0.0, "avg_logprob": -0.0930125978257921, "compression_ratio": 1.6918238993710693, "no_speech_prob": 3.927856084828818e-07}, {"id": 815, "seek": 517072, "start": 5180.4800000000005, "end": 5185.400000000001, "text": " I'm going to look at data augmentation next week so I'm going to skip over data augmentation", "tokens": [286, 478, 516, 281, 574, 412, 1412, 14501, 19631, 958, 1243, 370, 286, 478, 516, 281, 10023, 670, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.0930125978257921, "compression_ratio": 1.6918238993710693, "no_speech_prob": 3.927856084828818e-07}, {"id": 816, "seek": 517072, "start": 5185.400000000001, "end": 5194.76, "text": " and let's just jump straight into training your model so once we've got DLs we can just", "tokens": [293, 718, 311, 445, 3012, 2997, 666, 3097, 428, 2316, 370, 1564, 321, 600, 658, 413, 43, 82, 321, 393, 445], "temperature": 0.0, "avg_logprob": -0.0930125978257921, "compression_ratio": 1.6918238993710693, "no_speech_prob": 3.927856084828818e-07}, {"id": 817, "seek": 519476, "start": 5194.76, "end": 5201.64, "text": " like in lesson one call CNN learner to create a resnet we're going to create a smaller resnet", "tokens": [411, 294, 6898, 472, 818, 24859, 33347, 281, 1884, 257, 725, 7129, 321, 434, 516, 281, 1884, 257, 4356, 725, 7129], "temperature": 0.0, "avg_logprob": -0.11850869137307872, "compression_ratio": 1.7, "no_speech_prob": 7.338189789152239e-07}, {"id": 818, "seek": 519476, "start": 5201.64, "end": 5207.64, "text": " this time a resnet 18 again asking for error rate we can then call dot fine tune again", "tokens": [341, 565, 257, 725, 7129, 2443, 797, 3365, 337, 6713, 3314, 321, 393, 550, 818, 5893, 2489, 10864, 797], "temperature": 0.0, "avg_logprob": -0.11850869137307872, "compression_ratio": 1.7, "no_speech_prob": 7.338189789152239e-07}, {"id": 819, "seek": 519476, "start": 5207.64, "end": 5212.16, "text": " so you see it's all the same lines of code we've already seen and you can see our error", "tokens": [370, 291, 536, 309, 311, 439, 264, 912, 3876, 295, 3089, 321, 600, 1217, 1612, 293, 291, 393, 536, 527, 6713], "temperature": 0.0, "avg_logprob": -0.11850869137307872, "compression_ratio": 1.7, "no_speech_prob": 7.338189789152239e-07}, {"id": 820, "seek": 519476, "start": 5212.16, "end": 5219.360000000001, "text": " rate goes down from 9 to 1 so we've got 1% error and after training for about 25 seconds", "tokens": [3314, 1709, 760, 490, 1722, 281, 502, 370, 321, 600, 658, 502, 4, 6713, 293, 934, 3097, 337, 466, 3552, 3949], "temperature": 0.0, "avg_logprob": -0.11850869137307872, "compression_ratio": 1.7, "no_speech_prob": 7.338189789152239e-07}, {"id": 821, "seek": 521936, "start": 5219.36, "end": 5225.5199999999995, "text": " so you can see you know we've only got 450 images we've trained for well less than a", "tokens": [370, 291, 393, 536, 291, 458, 321, 600, 787, 658, 26034, 5267, 321, 600, 8895, 337, 731, 1570, 813, 257], "temperature": 0.0, "avg_logprob": -0.05865414549664753, "compression_ratio": 1.8402061855670102, "no_speech_prob": 9.57079805630201e-07}, {"id": 822, "seek": 521936, "start": 5225.5199999999995, "end": 5232.12, "text": " minute and we only have let's look at the confusion matrix so we can say I want to create", "tokens": [3456, 293, 321, 787, 362, 718, 311, 574, 412, 264, 15075, 8141, 370, 321, 393, 584, 286, 528, 281, 1884], "temperature": 0.0, "avg_logprob": -0.05865414549664753, "compression_ratio": 1.8402061855670102, "no_speech_prob": 9.57079805630201e-07}, {"id": 823, "seek": 521936, "start": 5232.12, "end": 5237.28, "text": " a classification interpretation class I want to look at the confusion matrix and the confusion", "tokens": [257, 21538, 14174, 1508, 286, 528, 281, 574, 412, 264, 15075, 8141, 293, 264, 15075], "temperature": 0.0, "avg_logprob": -0.05865414549664753, "compression_ratio": 1.8402061855670102, "no_speech_prob": 9.57079805630201e-07}, {"id": 824, "seek": 521936, "start": 5237.28, "end": 5243.0, "text": " matrix as you can see it's something that says for things that are actually black bears", "tokens": [8141, 382, 291, 393, 536, 309, 311, 746, 300, 1619, 337, 721, 300, 366, 767, 2211, 17276], "temperature": 0.0, "avg_logprob": -0.05865414549664753, "compression_ratio": 1.8402061855670102, "no_speech_prob": 9.57079805630201e-07}, {"id": 825, "seek": 524300, "start": 5243.0, "end": 5250.88, "text": " how many are predicted to be black bears versus grizzly bears versus teddy bears so the diagonal", "tokens": [577, 867, 366, 19147, 281, 312, 2211, 17276, 5717, 17865, 4313, 356, 17276, 5717, 45116, 17276, 370, 264, 21539], "temperature": 0.0, "avg_logprob": -0.10777474558630655, "compression_ratio": 1.9193548387096775, "no_speech_prob": 8.362898995528667e-08}, {"id": 826, "seek": 524300, "start": 5250.88, "end": 5254.36, "text": " are the ones that are all correct and so it looks like we've got two errors we've got", "tokens": [366, 264, 2306, 300, 366, 439, 3006, 293, 370, 309, 1542, 411, 321, 600, 658, 732, 13603, 321, 600, 658], "temperature": 0.0, "avg_logprob": -0.10777474558630655, "compression_ratio": 1.9193548387096775, "no_speech_prob": 8.362898995528667e-08}, {"id": 827, "seek": 524300, "start": 5254.36, "end": 5261.36, "text": " one grizzly that was predicted to be black one black that was predicted to be grizzly", "tokens": [472, 17865, 4313, 356, 300, 390, 19147, 281, 312, 2211, 472, 2211, 300, 390, 19147, 281, 312, 17865, 4313, 356], "temperature": 0.0, "avg_logprob": -0.10777474558630655, "compression_ratio": 1.9193548387096775, "no_speech_prob": 8.362898995528667e-08}, {"id": 828, "seek": 524300, "start": 5261.36, "end": 5269.88, "text": " super super useful method is plot top losses and that'll actually show me what my errors", "tokens": [1687, 1687, 4420, 3170, 307, 7542, 1192, 15352, 293, 300, 603, 767, 855, 385, 437, 452, 13603], "temperature": 0.0, "avg_logprob": -0.10777474558630655, "compression_ratio": 1.9193548387096775, "no_speech_prob": 8.362898995528667e-08}, {"id": 829, "seek": 526988, "start": 5269.88, "end": 5275.84, "text": " actually look like so this one here was predicted to be a grizzly bear but the label was black", "tokens": [767, 574, 411, 370, 341, 472, 510, 390, 19147, 281, 312, 257, 17865, 4313, 356, 6155, 457, 264, 7645, 390, 2211], "temperature": 0.0, "avg_logprob": -0.07557997613582972, "compression_ratio": 2.0651162790697675, "no_speech_prob": 5.233369293478063e-08}, {"id": 830, "seek": 526988, "start": 5275.84, "end": 5280.24, "text": " bear this one was the one that's predicted to be a black bear and the label was grizzly", "tokens": [6155, 341, 472, 390, 264, 472, 300, 311, 19147, 281, 312, 257, 2211, 6155, 293, 264, 7645, 390, 17865, 4313, 356], "temperature": 0.0, "avg_logprob": -0.07557997613582972, "compression_ratio": 2.0651162790697675, "no_speech_prob": 5.233369293478063e-08}, {"id": 831, "seek": 526988, "start": 5280.24, "end": 5286.24, "text": " bear these ones here are not actually wrong there this is predicted to be black and it's", "tokens": [6155, 613, 2306, 510, 366, 406, 767, 2085, 456, 341, 307, 19147, 281, 312, 2211, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.07557997613582972, "compression_ratio": 2.0651162790697675, "no_speech_prob": 5.233369293478063e-08}, {"id": 832, "seek": 526988, "start": 5286.24, "end": 5290.92, "text": " actually black but the reason they appear in this is because these are the ones that", "tokens": [767, 2211, 457, 264, 1778, 436, 4204, 294, 341, 307, 570, 613, 366, 264, 2306, 300], "temperature": 0.0, "avg_logprob": -0.07557997613582972, "compression_ratio": 2.0651162790697675, "no_speech_prob": 5.233369293478063e-08}, {"id": 833, "seek": 526988, "start": 5290.92, "end": 5299.0, "text": " the model was the least confident about okay so we're going to look at image classifier", "tokens": [264, 2316, 390, 264, 1935, 6679, 466, 1392, 370, 321, 434, 516, 281, 574, 412, 3256, 1508, 9902], "temperature": 0.0, "avg_logprob": -0.07557997613582972, "compression_ratio": 2.0651162790697675, "no_speech_prob": 5.233369293478063e-08}, {"id": 834, "seek": 529900, "start": 5299.0, "end": 5306.88, "text": " cleaner next week let's focus on how we then get this into production so to get it into", "tokens": [16532, 958, 1243, 718, 311, 1879, 322, 577, 321, 550, 483, 341, 666, 4265, 370, 281, 483, 309, 666], "temperature": 0.0, "avg_logprob": -0.07865511424957759, "compression_ratio": 1.654320987654321, "no_speech_prob": 1.1015933978342218e-06}, {"id": 835, "seek": 529900, "start": 5306.88, "end": 5315.36, "text": " production we need to export the model so what exporting the model does is it creates", "tokens": [4265, 321, 643, 281, 10725, 264, 2316, 370, 437, 44686, 264, 2316, 775, 307, 309, 7829], "temperature": 0.0, "avg_logprob": -0.07865511424957759, "compression_ratio": 1.654320987654321, "no_speech_prob": 1.1015933978342218e-06}, {"id": 836, "seek": 529900, "start": 5315.36, "end": 5323.08, "text": " a new file which by default is called export.pickle which contains the architecture and all of", "tokens": [257, 777, 3991, 597, 538, 7576, 307, 1219, 10725, 13, 79, 618, 306, 597, 8306, 264, 9482, 293, 439, 295], "temperature": 0.0, "avg_logprob": -0.07865511424957759, "compression_ratio": 1.654320987654321, "no_speech_prob": 1.1015933978342218e-06}, {"id": 837, "seek": 532308, "start": 5323.08, "end": 5329.48, "text": " the parameters of the model so that is now something that you can copy over to a server", "tokens": [264, 9834, 295, 264, 2316, 370, 300, 307, 586, 746, 300, 291, 393, 5055, 670, 281, 257, 7154], "temperature": 0.0, "avg_logprob": -0.07119323386520636, "compression_ratio": 1.5975609756097562, "no_speech_prob": 3.9897133774502436e-07}, {"id": 838, "seek": 532308, "start": 5329.48, "end": 5338.5599999999995, "text": " somewhere and treat it as a predefined program right so then so the the process of using", "tokens": [4079, 293, 2387, 309, 382, 257, 659, 37716, 1461, 558, 370, 550, 370, 264, 264, 1399, 295, 1228], "temperature": 0.0, "avg_logprob": -0.07119323386520636, "compression_ratio": 1.5975609756097562, "no_speech_prob": 3.9897133774502436e-07}, {"id": 839, "seek": 532308, "start": 5338.5599999999995, "end": 5346.36, "text": " your trained model on new data kind of in production is called inference so here I've", "tokens": [428, 8895, 2316, 322, 777, 1412, 733, 295, 294, 4265, 307, 1219, 38253, 370, 510, 286, 600], "temperature": 0.0, "avg_logprob": -0.07119323386520636, "compression_ratio": 1.5975609756097562, "no_speech_prob": 3.9897133774502436e-07}, {"id": 840, "seek": 534636, "start": 5346.36, "end": 5353.04, "text": " created an inference learner by loading that learner back again right and so obviously", "tokens": [2942, 364, 38253, 33347, 538, 15114, 300, 33347, 646, 797, 558, 293, 370, 2745], "temperature": 0.0, "avg_logprob": -0.07430959928153764, "compression_ratio": 1.794979079497908, "no_speech_prob": 2.496699664789048e-07}, {"id": 841, "seek": 534636, "start": 5353.04, "end": 5358.12, "text": " it doesn't make sense to do it right next to after I've saved it in in a notebook but", "tokens": [309, 1177, 380, 652, 2020, 281, 360, 309, 558, 958, 281, 934, 286, 600, 6624, 309, 294, 294, 257, 21060, 457], "temperature": 0.0, "avg_logprob": -0.07430959928153764, "compression_ratio": 1.794979079497908, "no_speech_prob": 2.496699664789048e-07}, {"id": 842, "seek": 534636, "start": 5358.12, "end": 5362.0, "text": " I'm just showing you how it would work right so this is something that you would do on", "tokens": [286, 478, 445, 4099, 291, 577, 309, 576, 589, 558, 370, 341, 307, 746, 300, 291, 576, 360, 322], "temperature": 0.0, "avg_logprob": -0.07430959928153764, "compression_ratio": 1.794979079497908, "no_speech_prob": 2.496699664789048e-07}, {"id": 843, "seek": 534636, "start": 5362.0, "end": 5368.48, "text": " your server in inference and remember that once you have trained a model you can just", "tokens": [428, 7154, 294, 38253, 293, 1604, 300, 1564, 291, 362, 8895, 257, 2316, 291, 393, 445], "temperature": 0.0, "avg_logprob": -0.07430959928153764, "compression_ratio": 1.794979079497908, "no_speech_prob": 2.496699664789048e-07}, {"id": 844, "seek": 534636, "start": 5368.48, "end": 5374.28, "text": " treat it as a program you can pass inputs to it so this is now our our program this", "tokens": [2387, 309, 382, 257, 1461, 291, 393, 1320, 15743, 281, 309, 370, 341, 307, 586, 527, 527, 1461, 341], "temperature": 0.0, "avg_logprob": -0.07430959928153764, "compression_ratio": 1.794979079497908, "no_speech_prob": 2.496699664789048e-07}, {"id": 845, "seek": 537428, "start": 5374.28, "end": 5381.84, "text": " is our bear predictor so I can now call predict on it and I can pass it an image and it will", "tokens": [307, 527, 6155, 6069, 284, 370, 286, 393, 586, 818, 6069, 322, 309, 293, 286, 393, 1320, 309, 364, 3256, 293, 309, 486], "temperature": 0.0, "avg_logprob": -0.07475983012806285, "compression_ratio": 1.6024096385542168, "no_speech_prob": 7.453762691511656e-07}, {"id": 846, "seek": 537428, "start": 5381.84, "end": 5391.5199999999995, "text": " tell me here is it is 99.999% sure that this is a grizzly so I think what we're going to", "tokens": [980, 385, 510, 307, 309, 307, 11803, 13, 49017, 4, 988, 300, 341, 307, 257, 17865, 4313, 356, 370, 286, 519, 437, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.07475983012806285, "compression_ratio": 1.6024096385542168, "no_speech_prob": 7.453762691511656e-07}, {"id": 847, "seek": 537428, "start": 5391.5199999999995, "end": 5399.4, "text": " do here is we're going to wrap it up here and next week we'll finish off by creating", "tokens": [360, 510, 307, 321, 434, 516, 281, 7019, 309, 493, 510, 293, 958, 1243, 321, 603, 2413, 766, 538, 4084], "temperature": 0.0, "avg_logprob": -0.07475983012806285, "compression_ratio": 1.6024096385542168, "no_speech_prob": 7.453762691511656e-07}, {"id": 848, "seek": 539940, "start": 5399.4, "end": 5410.24, "text": " an actual GUI for our bear classifier we will show how to run it for free on a service called", "tokens": [364, 3539, 17917, 40, 337, 527, 6155, 1508, 9902, 321, 486, 855, 577, 281, 1190, 309, 337, 1737, 322, 257, 2643, 1219], "temperature": 0.0, "avg_logprob": -0.13201584033112027, "compression_ratio": 1.5284090909090908, "no_speech_prob": 3.187521087966161e-06}, {"id": 849, "seek": 539940, "start": 5410.24, "end": 5421.679999999999, "text": " binder and yeah and then I think we'll be ready to dive into some of the some of the", "tokens": [45630, 293, 1338, 293, 550, 286, 519, 321, 603, 312, 1919, 281, 9192, 666, 512, 295, 264, 512, 295, 264], "temperature": 0.0, "avg_logprob": -0.13201584033112027, "compression_ratio": 1.5284090909090908, "no_speech_prob": 3.187521087966161e-06}, {"id": 850, "seek": 539940, "start": 5421.679999999999, "end": 5426.799999999999, "text": " details of what's going on behind the scenes any questions or anything else before we wrap", "tokens": [4365, 295, 437, 311, 516, 322, 2261, 264, 8026, 604, 1651, 420, 1340, 1646, 949, 321, 7019], "temperature": 0.0, "avg_logprob": -0.13201584033112027, "compression_ratio": 1.5284090909090908, "no_speech_prob": 3.187521087966161e-06}, {"id": 851, "seek": 542680, "start": 5426.8, "end": 5439.12, "text": " up Rachel now okay great all right thanks everybody so we hopefully yeah I think from", "tokens": [493, 14246, 586, 1392, 869, 439, 558, 3231, 2201, 370, 321, 4696, 1338, 286, 519, 490], "temperature": 0.0, "avg_logprob": -0.08237292766571044, "compression_ratio": 1.4971098265895955, "no_speech_prob": 2.3687748580414336e-06}, {"id": 852, "seek": 542680, "start": 5439.12, "end": 5446.68, "text": " here on we've covered you know most of the key kind of underlying foundational stuff", "tokens": [510, 322, 321, 600, 5343, 291, 458, 881, 295, 264, 2141, 733, 295, 14217, 32195, 1507], "temperature": 0.0, "avg_logprob": -0.08237292766571044, "compression_ratio": 1.4971098265895955, "no_speech_prob": 2.3687748580414336e-06}, {"id": 853, "seek": 542680, "start": 5446.68, "end": 5452.08, "text": " from a machine learning point of view that we're going to need to cover so we'll be able", "tokens": [490, 257, 3479, 2539, 935, 295, 1910, 300, 321, 434, 516, 281, 643, 281, 2060, 370, 321, 603, 312, 1075], "temperature": 0.0, "avg_logprob": -0.08237292766571044, "compression_ratio": 1.4971098265895955, "no_speech_prob": 2.3687748580414336e-06}, {"id": 854, "seek": 545208, "start": 5452.08, "end": 5461.2, "text": " to ready to dive into lower level details of how deep learning works behind the scenes", "tokens": [281, 1919, 281, 9192, 666, 3126, 1496, 4365, 295, 577, 2452, 2539, 1985, 2261, 264, 8026], "temperature": 0.0, "avg_logprob": -0.1232497215270996, "compression_ratio": 1.3545454545454545, "no_speech_prob": 3.446454002187238e-06}, {"id": 855, "seek": 546120, "start": 5461.2, "end": 5485.599999999999, "text": " and I think that'll be starting from next week so see you then.", "tokens": [50364, 293, 286, 519, 300, 603, 312, 2891, 490, 958, 1243, 370, 536, 291, 550, 13, 51584], "temperature": 0.0, "avg_logprob": -0.4121692975362142, "compression_ratio": 0.9402985074626866, "no_speech_prob": 0.00023352738935500383}], "language": "en"}